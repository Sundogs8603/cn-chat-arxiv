# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Instruction Tuning with Human Curriculum.](http://arxiv.org/abs/2310.09518) | 本文探讨了在大型语言模型中应用结构化认知学习方法进行指令调整的潜在好处，并提出了一个高度结构化的合成数据集，结果表明该方法优于传统的随机化方法，提高了指令调整的性能。 |
| [^2] | [Efficient Link Prediction via GNN Layers Induced by Negative Sampling.](http://arxiv.org/abs/2310.09516) | 本研究提出了一种新颖的GNN架构，通过负采样诱导了正边和负边的正向传递，以更加灵活而稳定地进行链接预测。 |
| [^3] | [Online Parameter Identification of Generalized Non-cooperative Game.](http://arxiv.org/abs/2310.09511) | 本文研究了广义非合作博弈的在线参数估计问题，使用观测到的数据识别未知参数，并提出了一种新颖的在线参数识别算法。 |
| [^4] | [Towards Semantic Communication Protocols for 6G: From Protocol Learning to Language-Oriented Approaches.](http://arxiv.org/abs/2310.09506) | 本文提出了一种新颖的数据驱动MAC协议分类，包括三个级别：面向任务的神经协议，神经网络导向的符号协议和面向语言的语义协议。该分类旨在探索每个级别的机遇和挑战，并深入研究它们的基础技术。 |
| [^5] | [Advancing Test-Time Adaptation for Acoustic Foundation Models in Open-World Shifts.](http://arxiv.org/abs/2310.09505) | 本文提出了一种针对声学基础模型的测试时间自适应方法，以解决开放世界数据转换中的分布变化问题。研究发现，噪声较大的语音帧包含重要的语义内容。 |
| [^6] | [Learning In-between Imagery Dynamics via Physical Latent Spaces.](http://arxiv.org/abs/2310.09495) | 本文提出了一个学习图像动态的框架，通过潜在动态估计图像演变的中间阶段，从而实现解释性，并保留与图像的空间相关性。该方法通过使用遵循物理模型的潜在变量，确保了学习模型的可解释性，并在地球科学图像数据上展示了其鲁棒性和有效性。 |
| [^7] | [ARM: Refining Multivariate Forecasting with Adaptive Temporal-Contextual Learning.](http://arxiv.org/abs/2310.09488) | 本研究提出了ARM，一种多变量的时间-上下文自适应学习方法，用于优化长期时间序列预测。ARM通过采用自适应单变量效应学习、随机丢弃训练策略和多核局部平滑，能更好地处理时间模式和学习系列之间的依赖关系。在多个基准测试中，ARM展示了卓越的性能，而计算成本相对较低。 |
| [^8] | [Mirage: Model-Agnostic Graph Distillation for Graph Classification.](http://arxiv.org/abs/2310.09486) | Mirage是一种模型无关的图蒸馏算法，旨在构建一个更小的合成训练集，以在资源有限的环境中实现图分类，并克服了现有算法对完整数据集的依赖性和对建模流程变化的敏感性。 |
| [^9] | [Applying Bayesian Ridge Regression AI Modeling in Virus Severity Prediction.](http://arxiv.org/abs/2310.09485) | 本论文研究了贝叶斯岭回归在病毒严重程度预测中的应用，该模型具有较高准确性，但在数据组织方面有改进的空间，严重程度指数有助于获得患者护理需求的广泛概述。 |
| [^10] | [Exploring the Design Space of Diffusion Autoencoders for Face Morphing.](http://arxiv.org/abs/2310.09484) | 这项研究探索了面向人脸变形的扩散自编码器的设计空间，研究了采样算法、逆向DDIM求解器和部分采样的方法。 |
| [^11] | [Can CNNs Accurately Classify Human Emotions? A Deep-Learning Facial Expression Recognition Study.](http://arxiv.org/abs/2310.09473) | 这项研究调查了卷积神经网络模型识别和分类人类面部表情的能力，并且证明模型在分类三种情绪类别时表现出了优于随机猜测的准确性。 |
| [^12] | [Randomized Benchmarking of Local Zeroth-Order Optimizers for Variational Quantum Systems.](http://arxiv.org/abs/2310.09468) | 本研究通过随机基准测试比较了局部零阶优化器在量子优化问题上的性能，为未来改进这些优化器提供了一些见解。 |
| [^13] | [A Framework for Empowering Reinforcement Learning Agents with Causal Analysis: Enhancing Automated Cryptocurrency Trading.](http://arxiv.org/abs/2310.09462) | 本研究提出了一个基于强化学习的自动交易系统框架CausalReinforceNet，通过因果分析增强了强化学习代理的能力，以提高对加密货币市场的交易能力。 |
| [^14] | [LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents.](http://arxiv.org/abs/2310.09454) | 本研究提出了一种名为LgTS的新方法，利用LLM的规划能力，为无法访问环境转换动力学的RL代理提供子目标的图形表示。这个方法可以教导RL代理学习一组成功的策略来达到目标状态。 |
| [^15] | [Pairwise Similarity Learning is SimPLE.](http://arxiv.org/abs/2310.09449) | 本文提出了一个名为SimPLE的简单的无代理方法，用于解决双向相似性学习(PSL)问题，该方法无需额外的规范化和边距设置，并在开放集识别中表现良好。 |
| [^16] | [G10: Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations.](http://arxiv.org/abs/2310.09443) | G10是一个统一的GPU内存和存储架构，通过智能张量迁移来实现高效的扩展GPU内存容量并满足深度学习工作负载的可扩展性要求。 |
| [^17] | [Target Variable Engineering.](http://arxiv.org/abs/2310.09440) | 本研究通过比较回归和分类的预测性能，发现回归模型需要更多计算工作才能达到最佳性能，并且对训练过程中的随机性和启发式选择更为敏感。 |
| [^18] | [Sub-network Discovery and Soft-masking for Continual Learning of Mixed Tasks.](http://arxiv.org/abs/2310.09436) | 该论文提出了一种新的连续学习方法，通过发现每个任务的子网络和软掩蔽机制来克服灾难性遗忘和促进知识传递。实验证明该方法在多个任务的情况下都表现优异。 |
| [^19] | [Learning nonlinear integral operators via Recurrent Neural Networks and its application in solving Integro-Differential Equations.](http://arxiv.org/abs/2310.09434) | 本文提出使用LSTM-RNN学习非线性积分算子，并将其应用于求解积分微分方程。使用LSTM-RNN表示非线性积分算子可将非线性积分微分方程转化为普通微分方程，从而提高求解效率。通过模型问题展示了该方法的效率和稳定性，并验证了学习到的积分算子的普适性。 |
| [^20] | [Effects of cavity nonlinearities and linear losses on silicon microring-based reservoir computing.](http://arxiv.org/abs/2310.09433) | 本文研究了微环谐振器中腔体非线性和线性损耗对库仑计算性能的影响，并发现了三个区域，其中一个区域在低输入功率和节点数下具有非常低的时间序列预测误差。这项研究对于改进时间延迟库仑计算的预测性能有着重要的设计和优化指导作用。 |
| [^21] | [Enhancing BERT-Based Visual Question Answering through Keyword-Driven Sentence Selection.](http://arxiv.org/abs/2310.09432) | 本文提出了一种通过关键词驱动的句子选择策略来增强基于BERT的视觉问答。通过利用掩码语言建模技术微调BERT模型，并重点关注带有敏感关键词的句子，本方法能够在文档中识别出回答问题的相关元素，并在视觉问答任务中取得高性能。 |
| [^22] | [Offline Reinforcement Learning for Optimizing Production Bidding Policies.](http://arxiv.org/abs/2310.09426) | 该论文介绍了一种使用离线强化学习方法来优化生产环境中竞标策略的通用方法，该方法可以优化任何可微分的基础策略，只需要使用基础策略生成的数据。论文提出了一种混合代理架构，将基础策略与强化学习模块相结合。 |
| [^23] | [ZeroSwap: Data-driven Optimal Market Making in DeFi.](http://arxiv.org/abs/2310.09413) | ZeroSwap 是第一个基于数据驱动算法的 DeFi 市场做市方案，在保持市场做市商零利润的情况下，通过适应交易者行为来解决了流动性提供者遭受套利损失的问题。 |
| [^24] | [Hybrid Reinforcement Learning for Optimizing Pump Sustainability in Real-World Water Distribution Networks.](http://arxiv.org/abs/2310.09412) | 本文研究了基于混合强化学习的泵站调度优化问题，旨在提高实时控制的同时降低能源消耗和运营成本。传统的优化技术由于缺乏收敛性保证表现不佳，而强化学习能够适应不确定性并实现实时响应。然而，准确的水力系统模拟模型对于实施强化学习至关重要，以前的应用受到模拟训练数据误差的限制。 |
| [^25] | [Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review.](http://arxiv.org/abs/2310.09411) | 深度学习在自然语言处理中的应用已经取得了显著的成果，并且在文本摘要领域也表现出了巨大的潜力。 |
| [^26] | [Identifiability of Product of Experts Models.](http://arxiv.org/abs/2310.09397) | 本文研究了专家模型的可识别性问题，发现当潜变量均匀分布时，使用与参数数量相等的可观测量可以完全识别模型；而在一般情况下，可识别性仍然成立，但需要更多的观测数据。 |
| [^27] | [Semantics Alignment via Split Learning for Resilient Multi-User Semantic Communication.](http://arxiv.org/abs/2310.09394) | 本文提出了一种基于分割学习的方法，用于在多个神经收发器之间对齐语义。该方法使用局部微调技术，可以有效地控制计算和通信成本。 |
| [^28] | [Machine Learning Estimation of Maximum Vertical Velocity from Radar.](http://arxiv.org/abs/2310.09392) | 本研究利用机器学习模型U-Nets，通过3D雷达反射率，成功地估计了最大垂直速度及其面积范围，并采用Sinh-arcsinh-normal（SHASH）分布参数回归技术进行了确定性和概率预测。 |
| [^29] | [CORN: Co-Trained Full-Reference And No-Reference Audio Metrics.](http://arxiv.org/abs/2310.09388) | CORN是一个新颖的框架，将全参考和非参考音频度量结合起来，并尝试在训练时同时训练这两种模型。CORN FR模式同时具备全参考和非参考度量的性能。 |
| [^30] | [LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations.](http://arxiv.org/abs/2310.09382) | 本文介绍了一种名为LL-VQ-VAE的学习可学习的格子向量量化方法，通过替换向量量化层来实现高效表示。与传统方法相比，该方法在相同训练条件下具有更低的重构误差，训练时间更短，参数数量恒定。 |
| [^31] | [Identifying and examining machine learning biases on Adult dataset.](http://arxiv.org/abs/2310.09373) | 该研究通过集成学习探讨了减少机器学习模型偏见的方法，揭示了性别属性偏见对工资预测的重要影响。研究结果强调了在数据驱动的社会中实施混合模型的必要性，以实现公正和包容性。 |
| [^32] | [From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment Technique.](http://arxiv.org/abs/2310.09362) | 这项研究开发了一个能够发出声音的波斯语聊天机器人，用于指导用户进行基于依恋理论的自我依恋技术。通过使用规则和分类模块，聊天机器人可以理解用户输入并推荐适当的自我依恋练习。该研究还开发了一种准确率超过92%的情感分析模块，以识别用户情感。这项工作有助于在后疫情时代提供数字心理疗法的替代方案。 |
| [^33] | [Is Certifying $\ell_p$ Robustness Still Worthwhile?.](http://arxiv.org/abs/2310.09361) | 本文重新审视并回答了三个问题：我们为什么关心鲁棒性研究？我们为什么关心$\ell_p$有界的威胁模型？我们为什么关心认证而不是经验性防御？ |
| [^34] | [Exact Verification of ReLU Neural Control Barrier Functions.](http://arxiv.org/abs/2310.09360) | 本文提出了一种用于验证具有ReLU激活函数的前馈神经控制屏障函数的安全性的新精确条件和算法，克服了传统安全验证方法中ReLU函数的不可微性质的挑战。 |
| [^35] | [When are Bandits Robust to Misspecification?.](http://arxiv.org/abs/2310.09358) | 该论文研究了参数化的强盗算法和情境化的强盗算法在真实奖励与模型之间存在误差的情况下的稳定性，并找到了依赖于问题实例和模型类的充分条件，使得经典算法如ε-贪心和LinUCB能够在时间范围内保持次线性的遗憾保障。 |
| [^36] | [Uncertainty Quantification using Generative Approach.](http://arxiv.org/abs/2310.09338) | 这篇论文介绍了一种使用深度生成方法的增量生成蒙特卡洛(IGMC)方法，用于测量深度神经网络中的不确定性。通过迭代训练生成模型并将其输出添加到数据集中，IGMC能计算随机变量期望的后验分布，并且具有理论保证的收敛速度。该方法适用于神经网络分类和回归任务，并在MNIST数字分类任务上进行了实证研究。 |
| [^37] | [Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task.](http://arxiv.org/abs/2310.09336) | 组合能力以乘法方式出现：研究了条件扩散模型在合成任务中的组合泛化能力，结果显示这种能力受到底层数据生成过程的结构影响，且模型在学习到更高级的组合时存在困难。 |
| [^38] | [Statistical guarantees for stochastic Metropolis-Hastings.](http://arxiv.org/abs/2310.09335) | 该论文研究了针对随机Metropolis-Hastings算法的统计保证。通过引入简单的修正项，该方法可以避免计算成本上的损失，并通过分析非参数回归情景和深度神经网络回归的数值实例来证明了其在采样和可信区间方面的优势。 |
| [^39] | [Topological Data Analysis in smart manufacturing processes -- A survey on the state of the art.](http://arxiv.org/abs/2310.09319) | 本次调查总结了拓扑数据分析在智能制造和产业4.0背景下的最新进展，突出了其在工业生产和制造领域的关键优势和挑战，并讨论了未充分利用的TDA方法和已识别的应用类型，以推动更多的相关研究。 |
| [^40] | [Eliciting Model Steering Interactions from Users via Data and Visual Design Probes.](http://arxiv.org/abs/2310.09314) | 本研究使用数据和可视化设计探针来研究领域专家如何通过语义交互更新分类模型，发现许多语义交互的目标是增强模型的训练数据，而不是直接调整模型参数。 |
| [^41] | [Digital Twin Assisted Deep Reinforcement Learning for Online Optimization of Network Slicing Admission Control.](http://arxiv.org/abs/2310.09299) | 这项工作提出了一种数字孪生辅助的深度强化学习解决方案来解决网络切片入场控制中深度强化学习模型初始不稳定性的问题。 |
| [^42] | [ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection.](http://arxiv.org/abs/2310.09298) | ByteStack-ID是一种基于灰度图像和负载字节频率的集成堆叠模型，用于数据包级入侵检测。它能迅速准确地识别网络流量中的各种攻击类型，并与传统方法有所不同。 |
| [^43] | [Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms.](http://arxiv.org/abs/2310.09297) | 该论文提出了一个受人类记忆机制启发的神经模块，模拟人类和机器如何对当前输入进行关联推理和问答，并将其与过去的记忆结合起来。通过感知、记忆和推理组件，该模块实现了感知更新、记忆融合和信息检索的功能。 |
| [^44] | [Generative Entropic Neural Optimal Transport To Map Within and Across Spaces.](http://arxiv.org/abs/2310.09254) | 该论文介绍了生成熵神经最优传输在测度到测度映射中的应用，解决了处理非平方欧氏距离成本、确定性蒙格映射、映射跨不可比较空间和质量守恒约束等实际挑战。 |
| [^45] | [Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning.](http://arxiv.org/abs/2310.08782) | 本论文提出了一种解决迁移学习中数据集修剪的方法，通过集成数据集修剪和迁移学习的观点，发现现有的方法不适用于迁移学习范式，并提出了标签映射和特征映射这两种新的数据集修剪方法。 |
| [^46] | [Deep Reinforcement Learning for Autonomous Vehicle Intersection Navigation.](http://arxiv.org/abs/2310.08595) | 本研究通过使用基于TD3算法的单智能体方法，在CARLA模拟平台中展示了在复杂T型路口导航中稳定收敛和改进安全性能，并在行程延误、碰撞减少和总体成本等方面优于先前的方法。 |
| [^47] | [Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models.](http://arxiv.org/abs/2310.08577) | 本文研究了视觉语言模型对视觉数据类型的理解能力，发现虽然在某些样式化数据类型上表现良好，但在基本操作引起的简单数据类型上表现困难。 |
| [^48] | [A Survey of Heterogeneous Transfer Learning.](http://arxiv.org/abs/2310.08459) | 异构迁移学习适用于源领域和目标领域具有不同特征、数据分布和标签空间的情况，通过处理这些差异来增强模型性能。 |
| [^49] | [Jailbreaking Black Box Large Language Models in Twenty Queries.](http://arxiv.org/abs/2310.08419) | 这项研究提出了一个名为PAIR的算法，可以在只能黑盒访问大语言模型的情况下生成破解，无需人工干预。实证表明，PAIR通常只需要少于二十个查询来生成破解，是现有算法的数个数量级更高效。 |
| [^50] | [The Expresssive Power of Transformers with Chain of Thought.](http://arxiv.org/abs/2310.07923) | 本论文研究基于思维链的Transformer的表达能力，通过允许使用中间生成的方式提高了Transformer的推理能力，并发现线性数量的解码步骤在标准计算复杂度下增加了明显的新能力。 |
| [^51] | [Measuring Feature Sparsity in Language Models.](http://arxiv.org/abs/2310.07837) | 这项研究开发了度量方法来评估语言模型中特征稀疏性的成功，并测试了线性性和稀疏性假设的有效性。研究结果表明，语言模型的激活可以准确地建模为特征的稀疏线性组合，并且在第一层和最后一层中呈最稀疏状态。 |
| [^52] | [Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling.](http://arxiv.org/abs/2310.07786) | 本文介绍了一种新颖的非稳态情境赌博算法，通过将可扩展的基于深度神经网络的架构与精心设计的探索机制相结合，在非稳态环境中优先收集持久价值信息，从而显著提高了性能。 |
| [^53] | [Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM.](http://arxiv.org/abs/2310.07678) | 本文提出了可解释的图像相似性的方法，整合了Siamese网络和Grad-CAM，能够提供相似性分数以及视觉上的事实和反事实解释，有潜力提高可解释性和可信度。 |
| [^54] | [Human-Centered Evaluation of XAI Methods.](http://arxiv.org/abs/2310.07534) | 在人工智能领域中，解释深度学习黑盒子的决策过程是一个关键挑战。本研究以用户为中心，客观评估了三种领先的解释方法的可解释性，并发现它们都提供了可解释的结果。 |
| [^55] | [A Branched Deep Convolutional Network for Forecasting the Occurrence of Hazes in Paris using Meteorological Maps with Different Characteristic Spatial Scales.](http://arxiv.org/abs/2310.07437) | 该论文开发了一种基于分支的深度卷积网络，用于预测巴黎雾霾的发生。该网络利用气象图的不同特征空间尺度来训练，通过使用多年期的气象变量和地面能见度观测数据进行训练和验证。这些新架构提高了网络性能，在未使用训练数据的情况下进行盲预测，具有合理的预测分数。 |
| [^56] | [An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l.](http://arxiv.org/abs/2310.07325) | 在gelu-4l中，我们提供了证据表明内存管理对于transformer模型至关重要，并说明了Direct Logit Attribution技术的不准确之处。 |
| [^57] | [Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset.](http://arxiv.org/abs/2310.07250) | 本文提出了一种利用生成对抗网络从已有的模态生成缺失的MRI序列的方法，在BraTS数据集上取得了有希望的结果。 |
| [^58] | [Off-Policy Evaluation for Human Feedback.](http://arxiv.org/abs/2310.07123) | 本论文介绍了一个用于人类反馈的非策略评估（OPEHF）框架，可以准确评估人类反馈信号。这个框架解决了现有OPE方法在估计人类反馈信号上的不足。 |
| [^59] | [Scalable Semantic Non-Markovian Simulation Proxy for Reinforcement Learning.](http://arxiv.org/abs/2310.06835) | 本文提出了一种基于语义的仿真代理方法，通过引入时间扩展和注释逻辑，解决了强化学习中可扩展性、可解释性和马尔可夫假设等问题。与高保真度的模拟器相比，该方法在加速训练过程三个数量级的同时保持了策略质量，并且能够建模和利用非马尔可夫动力学和即时动作，并提供了可解释的代理行为结果痕迹。 |
| [^60] | [Self-Supervised Dataset Distillation for Transfer Learning.](http://arxiv.org/abs/2310.06511) | 本文提出了一种自监督数据集蒸馏方法，用于将无标签数据集转化为小型合成样本，以支持高效的自监督学习。通过最小化模型对合成样本的表示和可学习目标特征表示之间的均方误差，解决了合成样本梯度偏差的问题。 |
| [^61] | [Rule Mining for Correcting Classification Models.](http://arxiv.org/abs/2310.06446) | 本研究提出了一种用于修正分类模型的规则挖掘方法，通过挖掘不准确子集和对其进行修正的规则列表，以提高模型的预测准确性。 |
| [^62] | [DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models.](http://arxiv.org/abs/2310.05793) | 本文提出了DiffuSeq-v2模型，通过将离散和连续文本空间连接起来实现了Seq2Seq扩散模型的加速。在训练中引入软吸收态，提高离散突变重构能力；在采样阶段使用ODE求解器加快采样速度。实验结果表明，训练收敛速度提高4倍，生成样本速度提高800倍，更适用于实际应用。 |
| [^63] | [Transformer Fusion with Optimal Transport.](http://arxiv.org/abs/2310.05719) | 本文介绍了一种使用最优输运来融合基于Transformer的网络的方法，可以对齐各种架构组件并允许不同大小的模型的融合，提供了一种新的高效压缩Transformer的方式。 |
| [^64] | [RetSeg: Retention-based Colorectal Polyps Segmentation Network.](http://arxiv.org/abs/2310.05446) | 本研究探索将保留机制整合到结直肠息肉分割中，解决了视觉变换器在资源受限设备上实时疾病检测中的内存和并行性挑战。 |
| [^65] | [Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis.](http://arxiv.org/abs/2310.05374) | 本论文提出了一种名为LaSyn的文本数据利用框架，通过将文本数据转换为中间潜变表示来增强端到端语音处理模型的训练。在低资源环境下的语音识别和口语理解任务中，LaSyn相对词错误率减少了22.3%，绝对意图分类准确率提高了4.1%。 |
| [^66] | [Generalized Neural Collapse for a Large Number of Classes.](http://arxiv.org/abs/2310.05351) | 本论文将神经崩溃概念扩展到类别数远大于特征空间维度的情况，并展示了广义神经崩溃现象的最小边界值被最大化。 |
| [^67] | [Federated Learning: A Cutting-Edge Survey of the Latest Advancements and Applications.](http://arxiv.org/abs/2310.05269) | 联邦学习是一种安全分布式机器学习方法，通过整合云基础设施和区块链技术，实现了隐私安全和经济有效的通信。它通过本地训练模型并将结果上传到云端进行整体模型聚合，避免了直接暴露原始数据，具有高效、可伸缩和保护隐私的优势。 |
| [^68] | [Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification.](http://arxiv.org/abs/2310.05128) | 提出了一种层次感知联合监督对比学习方法（HJCL），用于层次化多标签文本分类。该方法通过使用实例级和标签级对比学习技术，以及精心构建批次来处理标签层次结构，解决了在HMTC中使用监督对比学习的问题。 |
| [^69] | [EMOFM: Ensemble MLP mOdel with Feature-based Mixers for Click-Through Rate Prediction.](http://arxiv.org/abs/2310.04482) | EMOFM是一个集成MLP模型，通过使用基于特征的混合器实现了字段和类型特征的融合，从而提升了点击率预测的性能。 |
| [^70] | [C(NN)FD -- deep learning predictions of tip clearance variations on multi-stage axial compressors aerodynamic performance.](http://arxiv.org/abs/2310.04264) | 本文展示了一种用于实时预测多级轴向压缩机在燃气轮机中尖间隙变化对气动性能影响的深度学习框架，可与CFD基准相媲美的实时准确性，方便集成到燃气轮机的制造和构建过程中进行性能评估。 |
| [^71] | [PyDCM: Custom Data Center Models with Reinforcement Learning for Sustainability.](http://arxiv.org/abs/2310.03906) | PyDCM是一个用强化学习实现的可定制的数据中心模型，通过使用自定义配置和向量化的热计算，实现了对数据中心的优化，具有较高的效率。 |
| [^72] | [Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video Sequences Using Swin Transformer-Enhanced UNet.](http://arxiv.org/abs/2310.03365) | Swin-Tempo是一个创新模型，使用了Swin Transformer-Enhanced UNet将CT扫描中的肺结节检测作为视频序列进行时间感知。它克服了现有网络的计算复杂性问题，提高了肺结节检测的准确率。 |
| [^73] | [Raze to the Ground: Query-Efficient Adversarial HTML Attacks on Machine-Learning Phishing Webpage Detectors.](http://arxiv.org/abs/2310.03166) | 本文提出了一种新颖的查询高效对抗HTML攻击方法，通过细粒度篡改来修改钓鱼网页的HTML代码，同时保持其恶意性和视觉外观不变。实验表明，这种方法能够将目前最先进的机器学习防钓鱼网页检测器的性能摧毁，并且只需要30个查询。 |
| [^74] | [Credit card score prediction using machine learning models: A new dataset.](http://arxiv.org/abs/2310.02956) | 本研究探索了利用机器学习模型对信用卡违约进行预测的方法，并提出了一个新的信用卡评分数据集。实验结果表明，多层感知器（MLP）模型在预测性能上表现最佳。 |
| [^75] | [Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness.](http://arxiv.org/abs/2310.02832) | 本文提出了一种通过利用神经网络中间层变换的平滑性来检测带外数据的方法(BLOOD),该方法适用于没有训练数据访问权限的预训练模型，并在Transformer网络上的文本分类任务中取得了良好的效果。 |
| [^76] | [Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems.](http://arxiv.org/abs/2310.02299) | 本文介绍了一种用于建模3D物理系统的松弛八面体群卷积技术，它可以在保持数据一致的最高等变性水平的同时，发现物理系统中微妙的对称性破缺因素。 |
| [^77] | [Why do autoencoders work?.](http://arxiv.org/abs/2310.02250) | 自编码器是一种深度神经网络模型，通过调整参数实现输入数据和重构输出之间的最小差异，用于识别数据在高维空间中的内在维度，并且对于某些拓扑结构，存在难以找到完美重构网络的限制。 |
| [^78] | [MIS-AVoiDD: Modality Invariant and Specific Representation for Audio-Visual Deepfake Detection.](http://arxiv.org/abs/2310.02234) | 本论文提出了一种名为MIS-AVoiDD的模态不变和特定表示方法，用于音视深度伪造检测。通过充分关注音频和视觉数据，并解决音频和视觉信号的异质性问题，该方法在多模态操作检测中表现出良好的性能。 |
| [^79] | [Probabilistically Rewired Message-Passing Neural Networks.](http://arxiv.org/abs/2310.02156) | PR-MPNNs通过概率重连学习加入相关边，并省略对预测任务没有帮助的边，从而增强了表达能力。 |
| [^80] | [SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data.](http://arxiv.org/abs/2310.00270) | 这篇论文提出了一种名为SpatialRank的新颖空间事件排名方法，通过基于时空数据的NDCG优化来解决城市事件排名问题。 |
| [^81] | [Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation.](http://arxiv.org/abs/2310.00029) | 本论文开发了一种新型框架，结合人类风险认知来生成对手驾驶行为，用于评估自动驾驶车辆的有效性和弱点。 |
| [^82] | [Towards Free Data Selection with General-Purpose Models.](http://arxiv.org/abs/2309.17342) | 本文提出了一种新的数据选择流程，利用通用模型在单次推理中选择来自不同数据集的数据，无需额外的训练或监督。通过定义和利用语义模式提取微妙的局部信息，我们实现了对所有数据样本的选择。 |
| [^83] | [Reliability Quantification of Deep Reinforcement Learning-based Control.](http://arxiv.org/abs/2309.16977) | 本论文提出了一种用于量化深度强化学习控制系统可靠性的方法，通过使用两个神经网络来评估控制的可靠性差异。 |
| [^84] | [Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words.](http://arxiv.org/abs/2309.16108) | 本文提出了ChannelViT模型，通过对ViT架构的修改和引入分层通道采样技术，增强了对多通道图像的推理能力和鲁棒性，适用于显微镜和卫星成像等领域。 |
| [^85] | [Learning the Efficient Frontier.](http://arxiv.org/abs/2309.15775) | 本文引入了NeuralEF，一个快速的神经逼近框架，能够鲁棒地预测高效前沿问题的解，同时处理异构线性约束和可变数量的优化输入。 |
| [^86] | [Latent Graph Powered Semi-Supervised Learning on Biomedical Tabular Data.](http://arxiv.org/abs/2309.15757) | 本文提出了一种基于潜在图的半监督学习方法，通过利用图的表示来捕捉数据之间的关系，并实现了全局和局部知识的有效融合。在生物医学数据集上的评估中，我们的方法表现出了最先进的结果。 |
| [^87] | [HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models.](http://arxiv.org/abs/2309.15701) | 本文引入了第一个开源基准测试，利用大型语言模型进行自动语音识别错误修正，实现了与人类水平相当的性能，具有重要的实际应用价值。 |
| [^88] | [Efficient tensor network simulation of IBM's largest quantum processors.](http://arxiv.org/abs/2309.15642) | 本文展示了如何使用量子启发的二维张量网络高效模拟IBM最大的量子处理器，通过简单的张量更新实现前所未有的准确度和极低的计算资源消耗，并为最新的IBM量子机器设立了基准。 |
| [^89] | [Doubly Robust Proximal Causal Learning for Continuous Treatments.](http://arxiv.org/abs/2309.12819) | 本文提出了一种基于核函数的双重稳健近端因果学习方法，用于处理连续治疗，并提出了一种高效求解干扰函数的新方法。 |
| [^90] | [How Robust is Google's Bard to Adversarial Image Attacks?.](http://arxiv.org/abs/2309.11751) | 本文研究了Google的Bard在对抗图像攻击方面的鲁棒性，并发现它可以被攻击以输出错误的图像描述。这一攻击还可以对其他多模态语言模型产生影响。研究还发现了Bard的两种防御机制。 |
| [^91] | [Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion.](http://arxiv.org/abs/2309.11044) | 提出了一种基于贝叶斯信息准则的中间全局模型Clustered FedStack。通过将本地客户端的模型预测和输出层权重发送到服务器，构建一个强大的全局模型，并使用聚类机制对本地客户进行聚类。 |
| [^92] | [Replication: Contrastive Learning and Data Augmentation in Traffic Classification Using a Flowpic Input Representation.](http://arxiv.org/abs/2309.09733) | 这项研究复制并重现了一项先前的流量分类研究，该研究使用了对比学习和数据增强的深度学习方法，证明了使用Flowpic输入表示能够在仅有100个样本的情况下实现高准确性。 |
| [^93] | [Generalization error bounds for iterative learning algorithms with bounded updates.](http://arxiv.org/abs/2309.05077) | 本文研究了具有有界更新的迭代学习算法在非凸损失函数上的泛化特性，提出了一种新颖的泛化误差界限，利用了信息论技术。研究表明，在模型维度和训练数据样本数量相等的情况下，界限得到了改善。 |
| [^94] | [A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining.](http://arxiv.org/abs/2309.04761) | 本调研综合审查了在教育数据挖掘中深度学习技术的最新研究进展，包括对知识跟踪、学生不良行为检测、性能预测和个性化推荐等典型教育场景的应用。同时提供了公共数据集和处理工具的综合概述，并指出了未来的研究方向。 |
| [^95] | [RLSynC: Offline-Online Reinforcement Learning for Synthon Completion.](http://arxiv.org/abs/2309.02671) | RLSynC是一种离线-在线强化学习方法，用于半模板化逆向合成中的合成物补全。它使用多个代理同时完成合成物的补全，并通过正向合成模型评估反应物的合成能力来指导行动搜索。 |
| [^96] | [Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning.](http://arxiv.org/abs/2309.01289) | 本研究提出了一种名为联邦正交训练（FOT）的方法，用于解决连续联邦学习中的全局灾难性遗忘问题，该方法克服了现有方法对过去数据的不切实际假设和隐私原则的违反。 |
| [^97] | [Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation.](http://arxiv.org/abs/2309.00987) | 本研究提出了一种基于强化学习的通用系统，名为顺序灵巧性，用于解决由一系列互不相同的子任务组成的长程操作问题。该系统通过串联多个熟练策略来实现长程任务目标，并具备自主策略切换和绕过多余阶段的能力。实验结果表明，该系统在真实世界中表现出了良好的泛化性能。 |
| [^98] | [On a Connection between Differential Games, Optimal Control, and Energy-based Models for Multi-Agent Interactions.](http://arxiv.org/abs/2308.16539) | 本论文研究了微分博弈、最优控制和基于能量的模型之间的联系，并提出了基于能量的潜在博弈的新的端到端学习应用，通过神经网络和可微分的博弈论优化层的组合来提高预测性能。 |
| [^99] | [DECODE: DilatEd COnvolutional neural network for Detecting Extreme-mass-ratio inspirals.](http://arxiv.org/abs/2308.16422) | 该论文介绍了一种名为DECODE的扩张卷积神经网络模型，用于检测极端质量比激发的信号。该模型通过在频域进行序列建模，并考虑时间延迟干涉仪以处理多通道TDI数据。 |
| [^100] | [A correlation-based fuzzy cluster validity index with secondary options detector.](http://arxiv.org/abs/2308.14785) | 本研究提出了一种基于相关性的模糊聚类有效性指标，该指标考虑了在聚类数量选择时可能存在的多个选项，并通过评估在多种数据集上的性能，与现有指标进行比较。 |
| [^101] | [A small vocabulary database of ultrasound image sequences of vocal tract dynamics.](http://arxiv.org/abs/2308.13941) | 本文介绍了一个新的数据库，包含了连续的发声器官和声学语音数据，用超声图像和声音数据来研究言语生成过程中舌头上轮廓的可视化。这个数据库是由哥伦比亚圣坦德区的17名年轻被试完成的。 |
| [^102] | [LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors.](http://arxiv.org/abs/2308.13904) | LMSanitator是一种新颖的方法，用于检测和消除Transformer模型中的任务不可知后门。与传统方法不同，LMSanitator通过逆转预定义的攻击向量而不是触发器，实现更好的收敛性能和后门检测精确度。 |
| [^103] | [System Identification for Continuous-time Linear Dynamical Systems.](http://arxiv.org/abs/2308.11933) | 本文解决了在连续时间下观测不规则采样的情况下，Kalman滤波器的系统识别问题。通过引入连续时间Ito随机微分方程来推广Kalman滤波器的学习，并提供一个新颖的两滤波器的后验计算方法，通过贝叶斯派生获得的解析形式的后验计算方法可以高效地估计SDE的参数。 |
| [^104] | [Integrating Large Language Models into the Debugging C Compiler for generating contextual error explanations.](http://arxiv.org/abs/2308.11873) | 本文介绍了一种利用大型语言模型将调试C编译器的错误解释改进的方法，并通过专家评估证明其在编译时和运行时错误解释的准确性方面的有效性。 |
| [^105] | [GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning.](http://arxiv.org/abs/2308.10279) | 这里是中文总结出的一句话要点：GPFL是一种新的个性化联邦学习方法，它能够同时学习全局和个性化的特征信息，在效果、可扩展性、公平性、稳定性和隐私性方面优于其他方法，并减轻了过拟合现象，提升了准确度。 |
| [^106] | [A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection.](http://arxiv.org/abs/2308.07774) | 本文提出了一种无监督图编码解码模型，用于检测图中的异常节点。在编码阶段，通过设计一种新的池化机制，该模型能够根据节点的异常程度对节点进行排序。模型的池化过程具有较低的计算复杂度和更高的可解释性。 |
| [^107] | [SynJax: Structured Probability Distributions for JAX.](http://arxiv.org/abs/2308.03291) | SynJax是一个针对JAX的结构化概率分布库，通过提供高效的向量化实现解决了对于结构化对象的难以实现的问题。 |
| [^108] | [Efficient Model Adaptation for Continual Learning at the Edge.](http://arxiv.org/abs/2308.02084) | 这篇论文提出了一个名为Encoder-Adaptor-Reconfigurator（EAR）框架，用于在领域漂移下进行高效的持续学习。该框架使用了固定的深度神经网络（DNN）特征编码器，并在编码器之上训练浅层网络来处理新数据。通过结合DNN和超维计算（HDC），该框架能够检测新数据是否属于分布之外（OOD），并能够识别出... (摘要内容省略) |
| [^109] | [Training Data Protection with Compositional Diffusion Models.](http://arxiv.org/abs/2308.01937) | 使用分区扩散模型（CDM）训练不同的扩散模型，并在推断时任意组合它们，实现了训练数据保护和选择性遗忘，同时还可以根据用户访问权限提供定制模型。 |
| [^110] | [Explainable Deep Learning for Tumor Dynamic Modeling and Overall Survival Prediction using Neural-ODE.](http://arxiv.org/abs/2308.01362) | 该论文介绍了一种可解释的深度学习方法，使用神经-ODE进行肿瘤动力建模和整体生存预测。该方法能够从截断数据中进行无偏预测，并提供了一种融合多模态数据的有原则的方式。 |
| [^111] | [Discovering Adaptable Symbolic Algorithms from Scratch.](http://arxiv.org/abs/2307.16890) | 提出了一种从零开始发现零-shot可适应策略的方法，通过演化模块化策略来构建具有线性寄存器机的控制算法，并根据环境变化即时调整模型参数和推理算法。在逼真的仿真四足机器人上演示了该方法的有效性，并在名为Cataclysmic Cartpole的非稳态控制任务上进行了详细分析。 |
| [^112] | [Med-HALT: Medical Domain Hallucination Test for Large Language Models.](http://arxiv.org/abs/2307.15343) | Med-HALT是一个新的基准和数据集，用于评估和减少大规模语言模型中医疗领域的幻觉问题。这个数据集包括多种创新的测试模式，并评估了领先的LLMs在性能上的差异。 |
| [^113] | [Neural Schr\"odinger Bridge with Sinkhorn Losses: Application to Data-driven Minimum Effort Control of Colloidal Self-assembly.](http://arxiv.org/abs/2307.14442) | 本文介绍了一种基于神经网络的Schr\"odinger桥接算法，应用于胶体自组装的最小工作控制问题中。与现有方法相比，该方法考虑了胶体自组装中的非线性控制系数，并提出了一种数据驱动的学习和控制框架。 |
| [^114] | [QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum-Classical Neural Network.](http://arxiv.org/abs/2307.12906) | QAmplifyNet是一种使用量子启发技术的混合量子-经典神经网络，能够高效预测供应链缺货。它在处理短时间、不平衡数据集方面表现出色，并具有良好的可解释性。 |
| [^115] | [An Empirical Evaluation of Temporal Graph Benchmark.](http://arxiv.org/abs/2307.12510) | 本文对Temporal Graph Benchmark进行了实证评估，通过扩展动态图库(DyGLib)到TGB，并使用十一种动态图学习方法进行全面比较。实验发现不同模型在不同数据集上表现出不同的性能，一些基线方法的性能可以通过使用DyGLib显著提高。 |
| [^116] | [Ensemble learning for blending gridded satellite and gauge-measured precipitation data.](http://arxiv.org/abs/2307.06840) | 本研究填补论文领域中的空白，提出了11个新的集成学习算法并对其进行了广泛的比较，旨在改进卫星降水产品的准确性。 |
| [^117] | [Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection.](http://arxiv.org/abs/2307.06422) | 本论文提出了一种差分私有的解耦图卷积方法，用于多粒度拓扑保护。引入了图差分隐私框架，可以确保模型参数和预测的私密性。 |
| [^118] | [Variational quantum regression algorithm with encoded data structure.](http://arxiv.org/abs/2307.03334) | 本文介绍了一个具有编码数据结构的变分量子回归算法，在量子机器学习中具有模型解释性，并能有效地处理互连度较高的量子比特。算法通过压缩编码和数字-模拟门操作，大大提高了在噪声中尺度量子计算机上的运行时间复杂度。 |
| [^119] | [AutoML in Heavily Constrained Applications.](http://arxiv.org/abs/2306.16913) | 本文提出了Caml，一种在严格约束的应用中使用元学习的AutoML方法。Caml能够自动适应特定任务的AutoML参数，并考虑用户定义的约束，生成满足约束且具有高预测性能的流程。 |
| [^120] | [CLUE: Calibrated Latent Guidance for Offline Reinforcement Learning.](http://arxiv.org/abs/2306.13412) | CLUE使用条件可变自编码器实现专家数据的内在奖励，消除了离线强化学习中其它繁重的外在奖励工作。 |
| [^121] | [A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design.](http://arxiv.org/abs/2306.11768) | 本文在系统回顾几何深度学习在结构药物设计中的最新进展，分别讨论了不同任务并按不同的几何深度学习方法进行组织。该领域的前景看好，但仍存在挑战。 |
| [^122] | [Regularized Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy Gradient, and Sample Complexity.](http://arxiv.org/abs/2306.11626) | 本论文研究了正则化鲁棒MDP问题和风险敏感MDP问题的相关性，并提出了有效的学习算法和样本复杂度分析。 |
| [^123] | [Beyond Normal: On the Evaluation of Mutual Information Estimators.](http://arxiv.org/abs/2306.11078) | 本文提出了一种语言无关的互信息估计基准平台，并讨论了经典和神经估计器在处理高维数据、长尾分布和高互信息时的普适性和局限性。 |
| [^124] | [NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning.](http://arxiv.org/abs/2306.10792) | 本文重新思考了Transformer和图神经网络在网络表示学习中的不同特性，并提出了一个修改后的基于Transformer的通用神经网络表示学习模型NAR-Former V2。 |
| [^125] | [DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks.](http://arxiv.org/abs/2306.09124) | DIFFender是一种基于扩散的对抗性防御方法，通过定位和恢复两个阶段的操作，利用文本引导的扩散模型来防御对抗性Patch，从而提高其整体防御性能。 |
| [^126] | [On Certified Generalization in Structured Prediction.](http://arxiv.org/abs/2306.09112) | 该论文提出了一种新的结构化预测PAC-Bayesian风险界限，它可以随着结构化示例的数量和大小的变化而进行泛化，为使用生成模型建立结构化预测的泛化界限迈出了第一步。 |
| [^127] | [Realising Synthetic Active Inference Agents, Part I: Epistemic Objectives and Graphical Specification Language.](http://arxiv.org/abs/2306.08014) | 本文介绍了自由能原理和主动推理的理论框架，并推导了适用于任意图形模型的主动推理版本。同时引入了一种新的图形说明语言（GSL）来明确规定系统目标。 |
| [^128] | [One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning.](http://arxiv.org/abs/2306.07967) | 本论文提出了一种通用的参数高效微调算法——GLoRA，该算法通过广义提示模块、模块化的适配器层和可扩展的结构搜索具有了对不同任务和数据集的更高灵活性和适应性，并在各类基准测试中表现出了优异的精度。 |
| [^129] | [How to Learn and Generalize From Three Minutes of Data: Physics-Constrained and Uncertainty-Aware Neural Stochastic Differential Equations.](http://arxiv.org/abs/2306.06335) | 本文提出了一种使用神经随机微分方程学习控制动力学模型的方法，其中利用了物理知识作为归纳偏置，并在设计中利用距离感知的估计不确定性，在小数据集上进行训练，同时可以在长时间范围内进行准确预测，可用于模型预测控制和模型基础增强学习。 |
| [^130] | [Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards.](http://arxiv.org/abs/2306.04488) | 本文提出了 rewarded soup 方法，通过结合多种代理奖励，实现微调权重插值，从而在整个偏好空间中实现帕累托最优广义化。该方法在强化学习任务上具有有效性。 |
| [^131] | [Computation with Sequences in a Model of the Brain.](http://arxiv.org/abs/2306.03812) | 这篇论文在大脑模型中展示了如何通过捕获时间序列来进行一系列关于神经元集群的计算。 |
| [^132] | [Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?.](http://arxiv.org/abs/2306.01323) | 本研究证明GNN在同构图中的同构节点和异构图中的异构节点上表现良好，而在另一组节点上表现不佳。该研究提出了一种新框架，通过使用加权聚合提高GNN在不同结构模式节点上的表现，有效解决结构差异性问题。 |
| [^133] | [SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds.](http://arxiv.org/abs/2306.00980) | 本文提出了一种通用方法，首次在移动设备上运行文本到图像扩散模型不到2秒，这是通过引入高效的网络架构和改进步骤蒸馏来实现的。 |
| [^134] | [Going Deeper with Spectral Embeddings.](http://arxiv.org/abs/2306.00742) | 本文提出两种新的谱嵌入方法，一种基于函数分析原理和核方法，另一种基于深度网络优化损失，提供理论保证和实际有效的算法，并提供新的采样算法。 |
| [^135] | [FedCSD: A Federated Learning Based Approach for Code-Smell Detection.](http://arxiv.org/abs/2306.00038) | 本文提出了一种名为FedCSD的基于联邦学习的代码异味检测方法，可以在保护数据隐私的同时，让组织协作训练联邦学习模型。该方法在三个实验中分别使用不同的数据集，实现了高精度的检测效果，并且比集中式和交叉验证方法具有更好的性能表现。 |
| [^136] | [Evaluating geospatial context information for travel mode detection.](http://arxiv.org/abs/2305.19428) | 本研究确定了与相关工作有关的背景表示法，并基于随机森林模型和SHAP方法评估了地理空间背景信息在出行方式检测中的贡献，实验结果表明描述与基础设施网络关系的特征对预测有显着贡献。 |
| [^137] | [Sharp Bounds for Generalized Causal Sensitivity Analysis.](http://arxiv.org/abs/2305.16988) | 本文提出了一个统一的框架用于广义因果敏感性分析，通过提出一个灵活的边际敏感性模型，推导出各种因果效应的尖锐界限。该框架适用于多种设置，包括离散、连续和时变的治疗。 |
| [^138] | [Parallel Sampling of Diffusion Models.](http://arxiv.org/abs/2305.16317) | 本文提出了一种新方法，ParaDiGMS，可以通过并行处理多个步骤来加速预训练扩散模型的采样。ParaDiGMS是第一个使计算速度和采样效率实现平衡的扩散采样方法，并与现有方法兼容。 |
| [^139] | [Adversarial Demonstration Attacks on Large Language Models.](http://arxiv.org/abs/2305.14950) | 本文研究了对大型语言模型进行对抗性示范攻击的安全问题，并提出了一种新的攻击方法advICL，通过改变示范而不改变输入来误导模型。实验结果表明，随着示范数量的增加，上下文学习的鲁棒性降低。 |
| [^140] | [Finding the Pillars of Strength for Multi-Head Attention.](http://arxiv.org/abs/2305.14380) | 本研究提出聚合头注意力(Grouped Head Attention)，使用自监督组约束进行训练，为注意力头进行分组，其中每个组专注于一个重要而独特的特征子集。此方法可以缓解MHA的冗余性和过度参数化问题，并导致更有效和高效的MHA，进而在基准测试中取得了性能提升。 |
| [^141] | [VIP5: Towards Multimodal Foundation Models for Recommendation.](http://arxiv.org/abs/2305.14302) | VIP5是一个多模态基础模型，通过统一图像、文本和个性化模态，实现了多模态的共享架构，提高了推荐系统的效果。 |
| [^142] | [Amortized Variational Inference with Coverage Guarantees.](http://arxiv.org/abs/2305.14275) | 提出了一种称为CANVI的方法，通过构建一致化预测器并使用预测效率进行比较，来提供具有保证的后验近似结果。该方法可以快速计算，易于实现，并且对于候选近似器的设计决策无需关注。此外，CANVI能够在无似然的情况下使用。 |
| [^143] | [Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning.](http://arxiv.org/abs/2305.14203) | 本论文提出了一种基于度量学习的方法，以缩小视觉语音识别中正常和无声语音之间的差距。通过利用正常和无声语音之间的共享内容，我们的模型能够有效地学习和预测Viseme身份，从而提高了无声VSR的准确性。 |
| [^144] | [ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding.](http://arxiv.org/abs/2305.14196) | ZeroSCROLLS是一个用于长文本自然语言理解的零Shot基准测试，包括六个任务和四个数据集，能够评估大型语言模型的性能。当前，GPT-4的平均得分最高，但在聚合任务等多个挑战上，仍有改进的空间。 |
| [^145] | [The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning.](http://arxiv.org/abs/2305.14045) | 本文通过思维链条微调（CoT fine-tuning）来提高小型语言模型在零样本和少样本学习中的能力，并引入了CoT Collection数据集来增强模型的逐步推理能力。实验结果表明，在未见任务和4个特定领域任务上，通过CoT fine-tuning可以显著提升模型的准确度和学习能力。 |
| [^146] | [Statistical Guarantees of Group-Invariant GANs.](http://arxiv.org/abs/2305.13517) | 本研究提出了群不变GAN的统计保证，发现当学习群不变分布时，群不变GAN所需样本数会按群体大小的幂比例减少。 |
| [^147] | [The Mean Squared Error of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors.](http://arxiv.org/abs/2305.12883) | 该论文研究了基于一般回归误差假设的无噪声回归最小二乘估计值的均方误差，并发现包含大量不重要的参数可以有效地降低估计器的均方误差。 |
| [^148] | [Relabel Minimal Training Subset to Flip a Prediction.](http://arxiv.org/abs/2305.12809) | 本文利用扩展影响函数提出了一种有效的识别和重新标记最小训练子集的方法，并证明其始终能够成功翻转测试结果，同时还提供了挑战模型预测、评估模型鲁棒性和洞察训练集偏差等多重作用。 |
| [^149] | [Are Your Explanations Reliable? Investigating the Stability of LIME in Explaining Text Classifiers by Marrying XAI and Adversarial Attack.](http://arxiv.org/abs/2305.12351) | 本文研究了解释AI中常用的工具LIME在文本数据上的稳定性，并提出了一种新算法XAIFooler来扰动文本输入并操纵解释，以解决这个问题。 |
| [^150] | [Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability.](http://arxiv.org/abs/2305.11788) | 本文研究了逻辑回归常数步长梯度下降在稳定性边缘的收敛性和隐式偏差，证明了逻辑损失可以通过任何常数步长的梯度下降进行最小化，同时也发现了指数损失下的发散性问题，强调了稳定性边缘下梯度下降的不稳定性。 |
| [^151] | [LLM Itself Can Read and Generate CXR Images.](http://arxiv.org/abs/2305.11490) | 该论文提出了一种新方法，可以在不需要进行结构更改、额外训练、或训练专门网络的情况下，通过微调预先训练的LLM来读取和生成像文本一样的图像，并应用于胸部X线（CXR）图像的生成任务中。 |
| [^152] | [Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence.](http://arxiv.org/abs/2305.11420) | 本文介绍了一种新型拓扑——基础$(k+1)$图，其中节点在有限的迭代次数后能达到确切的共识，具有快速共识率和小的最大度数，从而可以用于分散式SGD。 |
| [^153] | [On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation.](http://arxiv.org/abs/2305.11283) | 本文研究了一般函数逼近下的均场控制(MFC)和均场博弈(MFG)中的强化学习的统计效率，提出了基于乐观最大似然估计的算法，并仅对转移动力学具有Lipschitz连续性的假设，最后建立了一个指数级的下界支持MFC设置。 |
| [^154] | [Annotating 8,000 Abdominal CT Volumes for Multi-Organ Segmentation in Three Weeks.](http://arxiv.org/abs/2305.09666) | 本文提出了一种高效方法在短时间内标记8000个腹部CT扫描中的8个器官，建立了迄今为止最大的多器官数据集。 |
| [^155] | [DPMLBench: Holistic Evaluation of Differentially Private Machine Learning.](http://arxiv.org/abs/2305.05900) | 本文提出了DPMLBench框架，通过在图像分类任务上综合衡量加强DP-SGD的DPML算法的实用性和防御能力，填补了比较DPML算法改进表现的空白，提高了DPML算法的性能。 |
| [^156] | [Empowering AI drug discovery with explicit and implicit knowledge.](http://arxiv.org/abs/2305.01523) | DeepEIK是一个统一的深度学习框架，利用显式和隐式知识进行AI药物发现，通过特征融合和注意力机制来提高模型预测准确性，并在多个任务上显著优于现有方法。 |
| [^157] | [Energy-Based Sliced Wasserstein Distance.](http://arxiv.org/abs/2304.13586) | 本文提出了一种能量为基础的切片Wasserstein距离，并将其参数化，以克服传统方法中的固定先验分布缺乏信息和优化最佳分布昂贵不稳定的局限。 |
| [^158] | [QuMoS: A Framework for Preserving Security of Quantum Machine Learning Model.](http://arxiv.org/abs/2304.11511) | QuMoS是一个保护 QML 模型安全的框架，通过经典加密、量子混淆和诱饵样本等多种技术来保护模型免受窃取攻击，并具有较高的分类准确性。 |
| [^159] | [Digital Twins in Wind Energy: Emerging Technologies and Industry-Informed Future Directions.](http://arxiv.org/abs/2304.11405) | 本文综合介绍了数字孪生技术及其在风能行业中的应用，并从产业角度识别了未来的研究需求和挑战，最终提供数字孪生和其在风能应用领域未来研究和发展的路线图。 |
| [^160] | [Bridging Discrete and Backpropagation: Straight-Through and Beyond.](http://arxiv.org/abs/2304.08612) | 本文提出了一种新方法来逼近生成离散潜变量的参数的梯度，其中包括了一些数值方法，实现了二阶精度，取得了实验上的持续改进。 |
| [^161] | [Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models.](http://arxiv.org/abs/2304.01046) | 本文研究了一种训练阅读理解和逻辑推理模型的反向方法，利用相对准确性的策略来训练模型，通过Polytuplet Loss函数来确保优先学习答案选择的相对正确性，获得了不错的成果，提出了具有一般性的训练方法和模型架构。 |
| [^162] | [TPMCF: Temporal QoS Prediction using Multi-Source Collaborative Features.](http://arxiv.org/abs/2303.18201) | 本文提出了一种新的方法TPMCF，利用多源特征进行QoS预测。该方法利用带有注意力机制的编码器-解码器架构，并使用协作特征捕捉用户和服务之间的关系，有效地处理数据稀疏和异常值。 |
| [^163] | [Operator learning with PCA-Net: upper and lower complexity bounds.](http://arxiv.org/abs/2303.16317) | 本文发展了PCA-Net的近似理论，得出了通用逼近结果，并识别出了使用PCA-Net进行高效操作学习的潜在障碍：输出分布的复杂性和算子空间的内在复杂性。 |
| [^164] | [Structured Dynamic Pricing: Optimal Regret in a Global Shrinkage Model.](http://arxiv.org/abs/2303.15652) | 本文提出了一种在流式纵向数据设置中的动态定价策略，该策略基于全局收缩结构和PSGD方法，并明确地将遗憾作为时间、模型参数和数据集规模的函数。 |
| [^165] | [The effectiveness of MAE pre-pretraining for billion-scale pretraining.](http://arxiv.org/abs/2303.13496) | 本文在计算机视觉领域提出了一种自我监督的MAE技术预前置训练方法，该方法适用于亿级预训练规模，并可显著提高模型收敛性和下游转移性能。 |
| [^166] | [The Scope of In-Context Learning for the Extraction of Medical Temporal Constraints.](http://arxiv.org/abs/2303.09366) | 本研究定义了一种MTC分类，开发了一种基于CFG模型的抽取方法，并通过ICL自动提取和标准化DUGs中的MTC，有望通过定义安全的患者活动模式来推进以患者为中心的医疗应用。 |
| [^167] | [Do Transformers Parse while Predicting the Masked Word?.](http://arxiv.org/abs/2303.08117) | 本文探讨了预训练语言模型是否实际上进行解析以及为什么能捕捉解析结构，证明了类似于BERT或RoBERTa这样的掩码语言模型可以近似执行英语PCFG的Inside-Outside算法。 |
| [^168] | [Root Cause Identification for Collective Anomalies in Time Series given an Acyclic Summary Causal Graph with Loops.](http://arxiv.org/abs/2303.04038) | 本文提出了一种方法来识别给定时间序列和非循环摘要因果图中集体异常的根本原因，并通过将问题划分为独立子问题和比较直接效应等方式来解决该问题 |
| [^169] | [Environment Transformer and Policy Optimization for Model-Based Offline Reinforcement Learning.](http://arxiv.org/abs/2303.03811) | 论文提出了一种称为环境转换器的不确定性感知序列建模架构，用于解决基于模型的离线强化学习中训练时间和计算资源需求增加的问题，并减少环境动力学模型累积误差的干扰。 |
| [^170] | [Auxiliary Functions as Koopman Observables: Data-Driven Polynomial Optimization for Dynamical Systems.](http://arxiv.org/abs/2303.01483) | 提出了一种基于数据的动力系统分析方法，使用辅助函数作为Koopman可观测量，不需要明确的模型发现，可以适用于确定性和随机动力学，具有收敛性和性能优势。 |
| [^171] | [Localizing Moments in Long Video Via Multimodal Guidance.](http://arxiv.org/abs/2302.13372) | 本文提出了一种通过多模态引导方法，在长视频中提高自然语言 grounding 性能的方法，通过识别和修剪不可描述的窗口，实验证明这种方法优于其他方法。 |
| [^172] | [Counterfactual Situation Testing: Uncovering Discrimination under Fairness given the Difference.](http://arxiv.org/abs/2302.11944) | 我们提出了一种反事实场景测试框架，通过比较数据集中类似的保护和非保护实例来检测分类器中的歧视，通过比较组间决策结果差异，来发现个人歧视。该框架可以更好地对「给定差异的公平原则」进行操作，以揭示在公平原则下的歧视差异。 |
| [^173] | [One Fits All:Power General Time Series Analysis by Pretrained LM.](http://arxiv.org/abs/2302.11939) | 本论文提出了一种称为 Frozen Pretrained Transformer (FPT) 的预训练模型，利用从数十亿标记训练出来的语言或 CV 模型进行时间序列分析的所有主要类型任务的微调，进而使其在所有任务中都具备着最先进的性能和泛化能力。 |
| [^174] | [Neural Attention Memory.](http://arxiv.org/abs/2302.09422) | 神经注意力记忆（NAM）是一种记忆架构，通过可微分线性代数操作可被读写，可应用于记忆增强型神经网络、小样本学习和高效的长程关注。实验证明NAM在各方面均具有优越性能。 |
| [^175] | [Beyond Distribution Shift: Spurious Features Through the Lens of Training Dynamics.](http://arxiv.org/abs/2302.09344) | 本文通过观察训练过程中的学习动态，研究了深度神经网络中虚假特征的影响，发现并非所有虚假特征都是有害的。我们提出了一种模型和数据集相关的定义来区分虚假特征的有害程度，并利用示例困难度方法来量化模型的易学性和识别虚假特征。 |
| [^176] | [Beyond Statistical Similarity: Rethinking Metrics for Deep Generative Models in Engineering Design.](http://arxiv.org/abs/2302.02913) | 本文提供了一篇深度学习在工程设计中度量方法的综述和指南。传统的基于似然性的统计度量方法在对工程应用的要求上可能无法充分捕捉，因此本文编辑了一组全面的新度量标准，旨在解决传统度量标准的缺点，并更好地与工程设计的需求相一致。通过案例研究，本文展示了这些度量标准如何应用于评估深度生成模型在工程设计中的性能，并发现这些度量标准在捕捉设计的重要细微差别方面表现优于传统的统计度量标准。 |
| [^177] | [A Survey on Deep Learning based Time Series Analysis with Frequency Transformation.](http://arxiv.org/abs/2302.02173) | 近期，频率变换（FT）在深度学习时间序列分析中得到广泛应用，显著提高了准确性和效率。本文系统回顾和总结了基于FT的深度学习时间序列模型的研究进展，并探讨了其优势、限制以及主要方法。 |
| [^178] | [Defensive ML: Defending Architectural Side-channels with Adversarial Obfuscation.](http://arxiv.org/abs/2302.01474) | 本论文提出了一种防御性机器学习的方法，通过对抗性混淆来抵御计算机架构中的侧信道攻击。该方法可以设计、实现、训练和部署不同环境下的防御器，有效解决了使用机器学习进行信号分析的安全威胁。 |
| [^179] | [Identifying the Hazard Boundary of ML-enabled Autonomous Systems Using Cooperative Co-Evolutionary Search.](http://arxiv.org/abs/2301.13807) | 本研究致力于通过合作协同进化搜索，确定机器学习自主系统中的ML组件的危险边界。这种边界可以用于构建安全监视器，并在达到危险边界时采取预定义的回退机制。 |
| [^180] | [Towards interpretable quantum machine learning via single-photon quantum walks.](http://arxiv.org/abs/2301.13669) | 通过单光子量子行走实现可解释的量子机器学习，使用变分量子算法量子化了项目模拟模型，实现了超越经典模型能力的量子干涉，为解释性量子机器学习的实现铺平了道路。 |
| [^181] | [DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising Diffusion Models.](http://arxiv.org/abs/2301.13629) | 本论文提出了一种新的方法DiffSTG，该方法结合了STGNN的时空学习能力和扩散模型的不确定性测量，可以有效减小STG预测中的排名概率分数和均方根误差。 |
| [^182] | [Neural networks learn to magnify areas near decision boundaries.](http://arxiv.org/abs/2301.11375) | 神经网络训练能够放大决策边界附近的局部区域，改善整个系统的泛化能力。 |
| [^183] | [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models.](http://arxiv.org/abs/2301.10472) | XLM-V通过克服多语言掩码语言模型中的词汇瓶颈，引入了一种新方法。使用一个一百万标记词汇表，XLM-V在各项任务中均优于XLM-R。 |
| [^184] | [Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models.](http://arxiv.org/abs/2301.04213) | 本文研究了语言模型中本地化与编辑之间的关系，发现将事实本地化到特定模型参数并不能提供编辑指导。因果追踪方法并不能指导编辑哪个模型层来覆盖存储的事实。 |
| [^185] | [Markovian Sliced Wasserstein Distances: Beyond Independent Projections.](http://arxiv.org/abs/2301.03749) | 马尔可夫切片Wasserstein（MSW）距离是一种新的SW距离家族，通过在投影方向上施加一阶马尔可夫结构，解决了切片Wasserstein（SW）距离中独立投影导致的冗余投影的问题，并且具有较低的计算复杂度。（found in translation） |
| [^186] | [MixupE: Understanding and Improving Mixup from Directional Derivative Perspective.](http://arxiv.org/abs/2212.13381) | 本文从方向导数的角度分析了深度神经网络中常用的数据增强技术Mixup，发现它隐含地对所有阶数的无限多个方向导数进行了正则化。基于这一思路，作者提出了改进版的Mixup，理论上证明具有更好的泛化性能，并在各种领域的数据集上进行了验证，表现出比Mixup更好的效果。 |
| [^187] | [Refiner: Data Refining against Gradient Leakage Attacks in Federated Learning.](http://arxiv.org/abs/2212.02042) | Refiner提出了一种创新的防御范式，通过构建与原始数据具有低语义相似性的健壮数据，有效地混淆梯度泄漏攻击者，从而提高联邦学习系统的隐私保护能力。 |
| [^188] | [Data-driven multinomial random forest: A new random forest variant with strong consistency.](http://arxiv.org/abs/2211.15154) | 本研究改进了先前弱一致性随机森林变体的证明方法，提出了一种数据驱动的多项式随机森林（DMRF），并表明DMRF在分类和回归问题中具有更好的性能，并且实现了概率1的强一致性。 |
| [^189] | [Powderworld: A Platform for Understanding Generalization via Rich Task Distributions.](http://arxiv.org/abs/2211.13051) | Powderworld是一个直接在GPU上运行的轻量级但表现力强的模拟环境，用于提供泛化性的研究平台，包括世界建模和强化学习。实验表明，增加环境的复杂性可以改善世界模型和某些强化学习代理的泛化性能。 |
| [^190] | [Data-Driven Network Neuroscience: On Data Collection and Benchmark.](http://arxiv.org/abs/2211.12421) | 本文提供了一个全面且高质量的人脑功能网络数据集，用于神经科学、机器学习和图分析的交叉研究。它利用解剖学和功能性磁共振成像来理解人脑的功能连接，并具有识别潜在神经退行性疾病的重要性。通过利用机器学习和图分析研究大脑的网络形式，能够预测这些疾病的早期发生。脑网络以图形方式表示，保留了丰富的结构和位置信息，传统的检查方法无法捕捉到这些信息。然而，缺乏公开可访问的脑网络数据限制了数据驱动的研究。 |
| [^191] | [Latent Iterative Refinement for Modular Source Separation.](http://arxiv.org/abs/2211.11917) | 通过将深度神经网络模型的训练和推理过程重新定义为潜在信号表示的迭代映射，能够显著提高模型的资源利用效率。 |
| [^192] | [Efficient Estimation for Longitudinal Network via Adaptive Merging.](http://arxiv.org/abs/2211.07866) | 本文提出了一个有效的纵向网络估计框架，利用自适应合并、张量分解和点过程等方法来减少估计偏差和方差。 |
| [^193] | [Efficient Domain Coverage for Vehicles with Second-Order Dynamics via Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2211.05952) | 本研究提出了一种通过多智能体强化学习解决具有二阶动力学的车辆高效域覆盖问题的方法，网络架构采用了LSTM和自注意力技术，训练好的策略在性能上明显超过了传统的经典控制策略。 |
| [^194] | [Generating counterfactual explanations of tumor spatial proteomes to discover effective strategies for enhancing immune infiltration.](http://arxiv.org/abs/2211.04020) | 通过使用空间组学技术和反事实优化策略，该论文提出了一种设计肿瘤扰动以增强T细胞浸润的方法，为提高实体肿瘤的免疫治疗效果提供了一种新思路。 |
| [^195] | [Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces.](http://arxiv.org/abs/2211.03536) | 本文从表示空间的角度对知识图谱嵌入技术进行了综述，通过分类和讨论不同的数学角度和方法，介绍了KGE模型及其优势。 |
| [^196] | [Concentration inequalities for leave-one-out cross validation.](http://arxiv.org/abs/2211.02478) | 本文证明了估计器的稳定性足以说明留一法交叉验证是可靠的，并通过提供集中界限超出Lipschitz连续性假设的损失函数或估计器，为我们提供了一个相对丰富的分布类。 |
| [^197] | [Deep Learning for Global Wildfire Forecasting.](http://arxiv.org/abs/2211.00534) | 本研究创建了一个全球火灾数据集，并使用分割深度学习模型预测全球烧毁面积，为半季度尺度提供了高效准确的预测能力。 |
| [^198] | [Occam learning.](http://arxiv.org/abs/2210.13179) | 本文讨论了一种具有固定隐藏层分布的概率神经网络模型，该模型选择简单、易解释，不需要过度参数化，同时训练有效。模型的隐藏单元为二元变量时具有以特征为基础的自然解释。作者认为隐藏变量的分布应该遵循最大关联度原则，并介绍了分层特征模型（HFM）作为满足这一原则并对特征空间进行中性先验组织的模型。 |
| [^199] | [Optimal AdaBoost Converges.](http://arxiv.org/abs/2210.07808) | 本研究通过形式证明，展示了最优AdaBoost算法的分类器和边缘的收敛性质，结果与几十年的研究相一致。 |
| [^200] | [Predicting fluid-structure interaction with graph neural networks.](http://arxiv.org/abs/2210.04193) | 本文提出了一种使用图神经网络预测流体-结构相互作用的降阶建模框架，通过准单体的方法将系统状态在时间上演变为两个子网络，并采用超图神经网络预测流体状态的演变，具有很好的效果。 |
| [^201] | [Universal Prompt Tuning for Graph Neural Networks.](http://arxiv.org/abs/2209.15240) | 本文介绍了一种名为Graph Prompt Feature（GPF）的新方法，可通用地调整预先训练过的图神经网络模型，操作于输入的特征空间，能够对应任何形式的Prompt函数。 |
| [^202] | [Pareto Actor-Critic for Equilibrium Selection in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2209.14344) | 本文提出了Pareto Actor-Critic（Pareto-AC）算法来解决多智能体强化学习中的均衡选择问题，该算法利用无冲突游戏的性质，即Pareto最优均衡最大化了所有智能体的回报。实验结果显示Pareto-AC相比其他七种最先进的算法更能收敛到更高的回合回报。 |
| [^203] | [Model Predictive Robustness of Signal Temporal Logic Predicates.](http://arxiv.org/abs/2209.07881) | 本论文提出了一种新的模型预测鲁棒性的概念来更系统地评估信号时间逻辑中底层谓词的鲁棒性，并成功验证了该方法在自动驾驶中的应用。 |
| [^204] | [5q032e@SMM4H'22: Transformer-based classification of premise in tweets related to COVID-19.](http://arxiv.org/abs/2209.03851) | 本文提出了一个基于Transformer的预测模型，用于在Twitter文本中分类前提。实验结果表明，RoBERTa模型在前提预测任务中表现出竞争性的性能。 |
| [^205] | [Attacking the Spike: On the Transferability and Security of Spiking Neural Networks to Adversarial Examples.](http://arxiv.org/abs/2209.03358) | 这项研究主要关注于脉冲神经网络(SNNs)对抗性样本的鲁棒性和转移性。研究发现，成功的白盒对抗攻击SNNs在很大程度上依赖于替代梯度技术，并且非SNN架构创建的对抗样本往往不被SNNs误分类。 |
| [^206] | [White-Box Adversarial Policies in Deep Reinforcement Learning.](http://arxiv.org/abs/2209.02167) | 本文研究了在强化学习中训练对抗策略的方法，提出了基于白盒攻击的策略，能够访问目标代理的内部状态，从而识别其漏洞，攻击成功率更高。 |
| [^207] | [Pipeline-Invariant Representation Learning for Neuroimaging.](http://arxiv.org/abs/2208.12909) | 本研究评估了预处理管线对神经影像的影响，并提出了两种管线不变表示学习方法，用于提高分类性能的鲁棒性和捕捉相似神经网络表示。实验证明，这些模型具有独特和共享优势。 |
| [^208] | [Community Detection in the Hypergraph SBM: Optimal Recovery Given the Similarity Matrix.](http://arxiv.org/abs/2208.12227) | 本文研究了超图随机块模型中的社区发现问题，通过分析相似性矩阵和设计谱算法，实现了在不同密度条件下的精确恢复和高效计算。 |
| [^209] | [Signed Network Embedding with Application to Simultaneous Detection of Communities and Anomalies.](http://arxiv.org/abs/2207.09324) | 本文开发了一个统一的嵌入模型，用于解决签名网络中的平衡结构和异常效应，并在社区检测、异常检测和网络推断等任务中取得了良好表现。 |
| [^210] | [Diagnostic Tool for Out-of-Sample Model Evaluation.](http://arxiv.org/abs/2206.10982) | 本文提出了一种用于外样本模型评估的诊断工具，可以通过有限的校准数据集来表征模型在未来外样本上的损失，并提供了简单易用且易于解释的方法。该工具可以量化分布转变的影响，促进回归分析，帮助实现模型选择和超参数调优。 |
| [^211] | [A Machine Learning Data Fusion Model for Soil Moisture Retrieval.](http://arxiv.org/abs/2206.09649) | 本研究开发了一种深度学习模型，利用多种传感器数据和地球物理变量，可以准确估算土壤顶部的体积湿度含量。该模型在全球范围内的1300个传感器数据上进行训练和评估，表现出较高的相关性和较小的误差，并可用于生成高分辨率的土壤湿度图。 |
| [^212] | [TFLEX: Temporal Feature-Logic Embedding Framework for Complex Reasoning over Temporal Knowledge Graph.](http://arxiv.org/abs/2205.14307) | 本文提出了第一个用于时间知识图谱的复杂查询嵌入方法TFLEX，能够自然地建模所有一阶逻辑（FOL）运算，同时扩展了向量逻辑以处理三个额外的时间运算符。 |
| [^213] | [Speculative Decoding: Lossless Speedup of Autoregressive Translation.](http://arxiv.org/abs/2203.16487) | Speculative Decoding是一种新型解码范式，结合了自回归翻译（AT）和非自回归翻译（NAT）的优势，提供了无损加速的翻译方法。在每个解码步骤中，它推测性地预测下一个标记，并使用验证模型确保翻译结果与AT完全相同。通过推测解码和验证的协作，实现了更快的解码速度，同时保持翻译质量不变。实验证明，原始的SpecDec与AT贪婪解码的结果完全相同。 |
| [^214] | [Dealing with Sparse Rewards Using Graph Neural Networks.](http://arxiv.org/abs/2203.13424) | 本研究提出了基于图神经网络的奖励塑造方法的两种修改，一种采用先进的聚合函数，另一种利用注意力机制。实验证实了这些解决方案在三维环境导航任务中的有效性。 |
| [^215] | [Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation.](http://arxiv.org/abs/2203.11740) | 该论文提出基于星形细胞作用的神经网络，通过突触的竞争和强度平衡实现现有和记忆性的大脑可塑性和突触形成，并探讨了与关键期相关的神经紊乱和负面和正面记忆的持久性对突触激活的影响。 |
| [^216] | [Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs.](http://arxiv.org/abs/2202.03583) | 本研究提出了一种基于密集卷积神经网络和GRADCAM的胸部X光疾病多标签诊断模型，获得了在Cardiomegaly条件下最高的AUC得分0.896，并使用热图提高了模型的可解释性。 |
| [^217] | [Counterfactual Memorization in Neural Language Models.](http://arxiv.org/abs/2112.12938) | 本研究提出了一种反事实记忆的概念，用于描述神经语言模型在训练中忽略了哪些特定文档的预测变化。通过研究标准文本数据集中的反事实记忆训练样本，我们可以估计每个记忆样本对验证集和生成文本的影响，并直接提供记忆来源的证据。 |
| [^218] | [High-order Tensor Pooling with Attention for Action Recognition.](http://arxiv.org/abs/2110.05216) | 本论文提出了一种利用注意机制进行高阶张量池化的方法，通过引入特征值幂归一化（EPN）来防止爆发现象并提高动作识别的准确性。 |
| [^219] | [Label Noise in Adversarial Training: A Novel Perspective to Study Robust Overfitting.](http://arxiv.org/abs/2110.03135) | 该论文发现了对抗训练中存在的标签噪声，并解释了其对鲁棒过度拟合的普遍存在以及扰动半径和数据质量的依赖性。通过该论文提出的方法，可以自动校准标签以应对标签噪声和鲁棒过度拟合。 |
| [^220] | [Manifold-Aware Deep Clustering: Maximizing Angles between Embedding Vectors Based on Regular Simplex.](http://arxiv.org/abs/2106.02331) | 本文提出了一种名为流形感知深度聚类（M-DC）的新方法，它通过基于正则简单形体的唯一损失函数，最大化超空间中目标角度，从而在提高聚类性能时更有效地增强了超空间利用率。 |
| [^221] | [A study on the plasticity of neural networks.](http://arxiv.org/abs/2106.00042) | 本研究探讨了神经网络的可塑性问题，发现在利用先前获得知识进行微调时，预训练模型可能无法达到相同的泛化能力，这对于持续学习具有重要影响。 |
| [^222] | [PDE-constrained Models with Neural Network Terms: Optimization and Global Convergence.](http://arxiv.org/abs/2105.08633) | 本研究严格研究了带有神经网络项的线性椭圆PDE模型的优化问题，并使用梯度下降和伴随PDE方法证明了神经网络-PDE优化过程的全局收敛。在流体力学中的实际应用中，我们使用了这个方法来训练神经网络模型作为一个闭环。 |
| [^223] | [BEAUTY Powered BEAST.](http://arxiv.org/abs/2103.00674) | 本文研究了使用BEAUTY方法进行分布无关的拟合优度检验。该方法通过二进制展开逼近特征函数，并将许多重要的独立性检验统一起来。使用数据自适应权重的BEAST检验提供了稳健的功效，同时提出了一个可行功效的参考。 |
| [^224] | [Graph Fairing Convolutional Networks for Anomaly Detection.](http://arxiv.org/abs/2010.10274) | 本文引入了一种图形平滑卷积网络用于半监督异常检测，通过使用跳跃连接和图结构信息来学习有区分性的节点表示。 |
| [^225] | [Fast Adaptive Non-Monotone Submodular Maximization Subject to a Knapsack Constraint.](http://arxiv.org/abs/2007.05014) | 本研究提出了一种针对非单调子模最大化问题的快速适应性算法，能够在背包约束下以O(log n)的复杂度接近最优解，并可以通过修改将函数值询问的数量减少到O(n)而保持较低的适应性复杂度。 |
| [^226] | [Ansor: Generating High-Performance Tensor Programs for Deep Learning.](http://arxiv.org/abs/2006.06762) | Ansor是一个针对深度学习应用的张量程序生成框架，通过采样程序和使用进化搜索和学习的成本模型进行微调，能够高效地找到高性能的张量程序。 |
| [^227] | [Asymptotic Analysis of Conditioned Stochastic Gradient Descent.](http://arxiv.org/abs/2006.02745) | 本文研究了一类称为条件随机梯度下降算法的通用类别，在进行梯度迭代时对梯度方向进行预条件。使用离散时间方法和鞅工具，在较弱的假设下证明了迭代序列的重新缩放收敛性，适用于包括随机一阶和二阶方法在内的广泛条件矩阵类别。同时，还介绍了具有独立兴趣的几乎肯定收敛的结果。 |
| [^228] | [Temporal Convolutional Attention-based Network For Sequence Modeling.](http://arxiv.org/abs/2002.12530) | 我们提出了一种叫做时间卷积注意力网络（TCAN）的架构，它结合了时间卷积网络和注意机制，既能替代循环网络，又能吸收前馈模型的优势。在实验中，我们改进了最新的困惑度结果到30.28（基于单词的PTB），1.092（基于字符的PTB）。 |
| [^229] | [Active Inverse Reward Design.](http://arxiv.org/abs/1809.03060) | 这篇论文提出了一种主动逆向奖励设计的方法，通过询问用户比较不同的奖励函数来选择具有最大信息量的查询，从而达到了在训练环境中保证良好行为的目的。与其他方法相比，该方法可以通过引起子优行为的偏好来收集额外信息，同时在测试环境中取得了明显的优势。 |

# 详细

[^1]: 人类课程指导下的指令调整

    Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])

    [http://arxiv.org/abs/2310.09518](http://arxiv.org/abs/2310.09518)

    本文探讨了在大型语言模型中应用结构化认知学习方法进行指令调整的潜在好处，并提出了一个高度结构化的合成数据集，结果表明该方法优于传统的随机化方法，提高了指令调整的性能。

    

    指令调整的主流范式是随机洗牌训练最大多样化指令-响应对。本文探讨了在当代大型语言模型如ChatGPT和GPT-4中应用结构化认知学习方法进行指令调整的潜在好处。与以往传统的随机化指令数据集不同，我们提出了一个高度结构化的合成数据集，模拟了人类教育的渐进性和有组织性。我们通过将数据集与教育框架对齐来策划我们的数据集，为每个样本包括主题和认知严谨程度等元信息。我们的数据集涵盖了从中学到研究生阶段的全面细粒度主题，每个主题都有各种问题，以利用布鲁姆的认知分级法提高概念深度，该分级法用于区分每个概念的不同人类认知水平。结果表明，这种认知学习方法优于传统的随机化方法，提高了指令调整的性能。

    The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
    
[^2]: 通过负采样诱导的GNN层进行高效的链接预测

    Efficient Link Prediction via GNN Layers Induced by Negative Sampling. (arXiv:2310.09516v1 [cs.LG])

    [http://arxiv.org/abs/2310.09516](http://arxiv.org/abs/2310.09516)

    本研究提出了一种新颖的GNN架构，通过负采样诱导了正边和负边的正向传递，以更加灵活而稳定地进行链接预测。

    

    链接预测中的图神经网络(GNN)可以大致分为两大类。第一类是基于节点的结构，为每个节点预先计算个体嵌入，并通过简单的解码器进行组合以进行预测。尽管在推理时非常高效（因为节点嵌入只计算一次并反复重用），但模型表达能力有限，导致无法区分对候选边有贡献的同构节点，从而影响准确性。与之相反，第二类方法则依赖于形成针对每个边的子图嵌入，以丰富两两关系的表示，从而消除同构节点，提高准确性，但代价是增加了模型复杂度。为了更好地权衡这个取舍，我们提出了一种新颖的GNN架构，其中的正向传递明确依赖于正边（通常情况下）和负边（我们方法的独特之处），以提供更灵活但仍稳定的信号。

    Graph neural networks (GNNs) for link prediction can loosely be divided into two broad categories. First, \emph{node-wise} architectures pre-compute individual embeddings for each node that are later combined by a simple decoder to make predictions. While extremely efficient at inference time (since node embeddings are only computed once and repeatedly reused), model expressiveness is limited such that isomorphic nodes contributing to candidate edges may not be distinguishable, compromising accuracy. In contrast, \emph{edge-wise} methods rely on the formation of edge-specific subgraph embeddings to enrich the representation of pair-wise relationships, disambiguating isomorphic nodes to improve accuracy, but with the cost of increased model complexity. To better navigate this trade-off, we propose a novel GNN architecture whereby the \emph{forward pass} explicitly depends on \emph{both} positive (as is typical) and negative (unique to our approach) edges to inform more flexible, yet sti
    
[^3]: 广义非合作博弈的在线参数估计

    Online Parameter Identification of Generalized Non-cooperative Game. (arXiv:2310.09511v1 [cs.GT])

    [http://arxiv.org/abs/2310.09511](http://arxiv.org/abs/2310.09511)

    本文研究了广义非合作博弈的在线参数估计问题，使用观测到的数据识别未知参数，并提出了一种新颖的在线参数识别算法。

    

    本文研究了广义非合作博弈的参数估计问题，其中每个玩家的成本函数受可观测信号和一些未知参数的影响。我们考虑了在某些可观测信号上的平衡可以被带有噪声的观测到的情况下，我们的目标是通过观测数据识别未知参数。假设可观测信号和相应的带有噪声的平衡是按顺序获得的，我们将这个参数识别问题构造为在线优化，并引入了一种新颖的在线参数识别算法。具体来说，我们构造了一个平衡了保守性和正确性的正则化损失函数，其中保守性项确保新估计值与当前估计值的偏差不大，而正确性项由Karush-Kuhn-Tucker条件捕捉。然后我们证明了当玩家的成本函数...

    This work studies the parameter identification problem of a generalized non-cooperative game, where each player's cost function is influenced by an observable signal and some unknown parameters. We consider the scenario where equilibrium of the game at some observable signals can be observed with noises, whereas our goal is to identify the unknown parameters with the observed data. Assuming that the observable signals and the corresponding noise-corrupted equilibriums are acquired sequentially, we construct this parameter identification problem as online optimization and introduce a novel online parameter identification algorithm. To be specific, we construct a regularized loss function that balances conservativeness and correctiveness, where the conservativeness term ensures that the new estimates do not deviate significantly from the current estimates, while the correctiveness term is captured by the Karush-Kuhn-Tucker conditions. We then prove that when the players' cost functions a
    
[^4]: 面向6G的语义通信协议：从协议学习到面向语言的方法

    Towards Semantic Communication Protocols for 6G: From Protocol Learning to Language-Oriented Approaches. (arXiv:2310.09506v1 [cs.IT])

    [http://arxiv.org/abs/2310.09506](http://arxiv.org/abs/2310.09506)

    本文提出了一种新颖的数据驱动MAC协议分类，包括三个级别：面向任务的神经协议，神经网络导向的符号协议和面向语言的语义协议。该分类旨在探索每个级别的机遇和挑战，并深入研究它们的基础技术。

    

    预计即将到来的6G系统将面临各种非静态任务的挑战。这给传统的媒介访问控制（MAC）协议带来了困扰，因为MAC协议是静态和预定义的。为了应对这个挑战，最近出现了数据驱动的MAC协议，可以根据特定任务定制信令消息。本文将这些数据驱动的MAC协议分为三个级别进行了新颖的分类：第1级MAC：使用多智能体深度强化学习（MADRL）构建的面向任务的神经协议；第2级MAC：通过将第1级MAC的输出转换为显式符号开发的神经网络导向的符号协议；第3级MAC：利用大型语言模型（LLMs）和生成模型的面向语言的语义协议。通过这种分类，我们旨在深入探讨每个级别的机遇和挑战，并探索它们的基础技术。从信息论和相关原理以及选定的案例中汲取经验，

    The forthcoming 6G systems are expected to address a wide range of non-stationary tasks. This poses challenges to traditional medium access control (MAC) protocols that are static and predefined. In response, data-driven MAC protocols have recently emerged, offering ability to tailor their signaling messages for specific tasks. This article presents a novel categorization of these data-driven MAC protocols into three levels: Level 1 MAC. task-oriented neural protocols constructed using multi-agent deep reinforcement learning (MADRL); Level 2 MAC. neural network-oriented symbolic protocols developed by converting Level 1 MAC outputs into explicit symbols; and Level 3 MAC. language-oriented semantic protocols harnessing large language models (LLMs) and generative models. With this categorization, we aim to explore the opportunities and challenges of each level by delving into their foundational techniques. Drawing from information theory and associated principles as well as selected case
    
[^5]: 在开放世界转换中推进声学基础模型的测试时间自适应

    Advancing Test-Time Adaptation for Acoustic Foundation Models in Open-World Shifts. (arXiv:2310.09505v1 [cs.SD])

    [http://arxiv.org/abs/2310.09505](http://arxiv.org/abs/2310.09505)

    本文提出了一种针对声学基础模型的测试时间自适应方法，以解决开放世界数据转换中的分布变化问题。研究发现，噪声较大的语音帧包含重要的语义内容。

    

    测试时间自适应（TTA）是在推理过程中解决分布转换问题的关键方法，特别是在视觉识别任务中。然而，虽然声学模型在测试时间的语音分布转换中面临相似的挑战，但针对声学建模在开放世界数据转换环境下的TTA技术仍然很少见。考虑到声学基础模型的特点：1）它们主要是基于具有层归一化的变压器架构构建的；2）它们以一种非静态的方式处理长度不同的测试时间语音数据。这些因素使得在视觉聚焦的TTA方法的直接应用变得不可行，这些方法大多依赖于批归一化并假设独立样本。在本文中，我们深入研究了面临开放世界数据转换的预训练声学模型的TTA方法。我们发现，噪声较大、熵较高的语音帧通常带有关键的语义内容。传统的视觉TTA方法的直接应用在声学建模中并不可行。

    Test-Time Adaptation (TTA) is a critical paradigm for tackling distribution shifts during inference, especially in visual recognition tasks. However, while acoustic models face similar challenges due to distribution shifts in test-time speech, TTA techniques specifically designed for acoustic modeling in the context of open-world data shifts remain scarce. This gap is further exacerbated when considering the unique characteristics of acoustic foundation models: 1) they are primarily built on transformer architectures with layer normalization and 2) they deal with test-time speech data of varying lengths in a non-stationary manner. These aspects make the direct application of vision-focused TTA methods, which are mostly reliant on batch normalization and assume independent samples, infeasible. In this paper, we delve into TTA for pre-trained acoustic models facing open-world data shifts. We find that noisy, high-entropy speech frames, often non-silent, carry key semantic content. Tradit
    
[^6]: 通过物理潜在空间学习图像动态

    Learning In-between Imagery Dynamics via Physical Latent Spaces. (arXiv:2310.09495v1 [cs.LG])

    [http://arxiv.org/abs/2310.09495](http://arxiv.org/abs/2310.09495)

    本文提出了一个学习图像动态的框架，通过潜在动态估计图像演变的中间阶段，从而实现解释性，并保留与图像的空间相关性。该方法通过使用遵循物理模型的潜在变量，确保了学习模型的可解释性，并在地球科学图像数据上展示了其鲁棒性和有效性。

    

    我们提出了一个框架，旨在学习在连续时间步骤中观察到的两个图像之间的底层动态。图像数据的复杂性和缺乏时间信息导致在捕捉独特的演变模式时存在重大挑战。我们提出的方法专注于估计图像演变的中间阶段，通过潜在动态实现可解释性，同时保留与图像的空间相关性。通过将遵循偏微分方程（PDEs）的物理模型表达的潜在变量纳入我们的方法中，我们的方法确保了学习模型的可解释性，并提供了对应的图像动态的洞察力。我们通过一系列使用地球科学图像数据的数值测试证明了我们学习框架的稳健性和有效性。

    We present a framework designed to learn the underlying dynamics between two images observed at consecutive time steps. The complex nature of image data and the lack of temporal information pose significant challenges in capturing the unique evolving patterns. Our proposed method focuses on estimating the intermediary stages of image evolution, allowing for interpretability through latent dynamics while preserving spatial correlations with the image. By incorporating a latent variable that follows a physical model expressed in partial differential equations (PDEs), our approach ensures the interpretability of the learned model and provides insight into corresponding image dynamics. We demonstrate the robustness and effectiveness of our learning framework through a series of numerical tests using geoscientific imagery data.
    
[^7]: 使用自适应时间-上下文学习优化多变量预测

    ARM: Refining Multivariate Forecasting with Adaptive Temporal-Contextual Learning. (arXiv:2310.09488v1 [stat.ML])

    [http://arxiv.org/abs/2310.09488](http://arxiv.org/abs/2310.09488)

    本研究提出了ARM，一种多变量的时间-上下文自适应学习方法，用于优化长期时间序列预测。ARM通过采用自适应单变量效应学习、随机丢弃训练策略和多核局部平滑，能更好地处理时间模式和学习系列之间的依赖关系。在多个基准测试中，ARM展示了卓越的性能，而计算成本相对较低。

    

    长期时间序列预测（LTSF）在各个领域中都很重要，但在处理复杂的时间-上下文关系方面面临挑战。由于多变量输入模型表现不如最近的一些单变量模型，我们认为问题在于现有的多变量LTSF变压器模型无法高效地建模系列之间的关系：往往不能正确地捕捉到系列之间的特征差异。为了解决这个问题，我们引入了ARM：一种多变量的时间-上下文自适应学习方法，它是专门为多变量LTSF建模而设计的增强型架构。ARM采用自适应单变量效应学习（AUEL）、随机丢弃（RD）训练策略和多核局部平滑（MKLS）来更好地处理单个系列的时间模式并正确学习系列之间的依赖关系。ARM在多个基准测试上展示了卓越的性能，而与现有方法相比并没有显著增加计算成本。

    Long-term time series forecasting (LTSF) is important for various domains but is confronted by challenges in handling the complex temporal-contextual relationships. As multivariate input models underperforming some recent univariate counterparts, we posit that the issue lies in the inefficiency of existing multivariate LTSF Transformers to model series-wise relationships: the characteristic differences between series are often captured incorrectly. To address this, we introduce ARM: a multivariate temporal-contextual adaptive learning method, which is an enhanced architecture specifically designed for multivariate LTSF modelling. ARM employs Adaptive Univariate Effect Learning (AUEL), Random Dropping (RD) training strategy, and Multi-kernel Local Smoothing (MKLS), to better handle individual series temporal patterns and correctly learn inter-series dependencies. ARM demonstrates superior performance on multiple benchmarks without significantly increasing computational costs compared to
    
[^8]: Mirage: 图分类的模型无关图蒸馏

    Mirage: Model-Agnostic Graph Distillation for Graph Classification. (arXiv:2310.09486v1 [cs.LG])

    [http://arxiv.org/abs/2310.09486](http://arxiv.org/abs/2310.09486)

    Mirage是一种模型无关的图蒸馏算法，旨在构建一个更小的合成训练集，以在资源有限的环境中实现图分类，并克服了现有算法对完整数据集的依赖性和对建模流程变化的敏感性。

    

    GNNs和其他深度学习模型一样，对数据和计算需求量很大。急需在大型数据集上扩展GNN的训练，以便在资源有限的环境中使用它们。图蒸馏是为此目的而努力，旨在从原始训练数据构建一个更小的合成训练集，而不会显著影响模型性能。虽然初步工作取得了一些进展，但这项工作基于两个关键观察：(1)现有的图蒸馏算法本身依赖于使用完整数据集进行训练，这就破坏了图蒸馏的前提。(2)蒸馏过程对目标GNN架构和超参数具有特异性，因此对建模流程的变化不具备鲁棒性。我们通过设计一种名为Mirage的图分类蒸馏算法来避免这些限制。Mirage建立在一个洞察的基础上，即一个消息传递的GNN将输入图分解为计算的多重集合。

    GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called Mirage for graph classification. Mirage is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation 
    
[^9]: 应用贝叶斯岭回归人工智能模型在病毒严重程度预测中的应用

    Applying Bayesian Ridge Regression AI Modeling in Virus Severity Prediction. (arXiv:2310.09485v1 [cs.LG])

    [http://arxiv.org/abs/2310.09485](http://arxiv.org/abs/2310.09485)

    本论文研究了贝叶斯岭回归在病毒严重程度预测中的应用，该模型具有较高准确性，但在数据组织方面有改进的空间，严重程度指数有助于获得患者护理需求的广泛概述。

    

    人工智能是重塑医疗系统的强大工具。在医疗领域，人工智能的数据处理能力无价，可以实现更准确和快速的诊断，从而减轻医疗专业人员的工作负担。因此，人工智能在各个行业都被证明是一种强大的工具，简化了复杂的任务和模式识别，这些对人类或传统的计算机算法来说本来是难以承受的。本文回顾了贝叶斯岭回归的优点和缺点，这是一种可以将尖端病毒分析带给全球医疗专业人员的人工智能模型。模型的准确性评估显示出有希望的结果，但数据组织方面还有改进的空间。此外，严重程度指数作为一种有价值的工具，可以获得关于患者护理需求的广泛概述，符合医疗专业人员对更广泛分类的偏好。

    Artificial intelligence (AI) is a powerful tool for reshaping healthcare systems. In healthcare, AI is invaluable for its capacity to manage vast amounts of data, which can lead to more accurate and speedy diagnoses, ultimately easing the workload on healthcare professionals. As a result, AI has proven itself to be a power tool across various industries, simplifying complex tasks and pattern recognition that would otherwise be overwhelming for humans or traditional computer algorithms. In this paper, we review the strengths and weaknesses of Bayesian Ridge Regression, an AI model that can be used to bring cutting edge virus analysis to healthcare professionals around the world. The model's accuracy assessment revealed promising results, with room for improvement primarily related to data organization. In addition, the severity index serves as a valuable tool to gain a broad overview of patient care needs, aligning with healthcare professionals' preference for broader categorizations.
    
[^10]: 探索面向人脸变形的扩散自编码器的设计空间

    Exploring the Design Space of Diffusion Autoencoders for Face Morphing. (arXiv:2310.09484v1 [cs.CV])

    [http://arxiv.org/abs/2310.09484](http://arxiv.org/abs/2310.09484)

    这项研究探索了面向人脸变形的扩散自编码器的设计空间，研究了采样算法、逆向DDIM求解器和部分采样的方法。

    

    通过扩散自编码器创建的人脸变形是一种最近的创新，而这种方法的设计空间尚未得到充分探索。我们探索了设计空间的三个方面，即1）采样算法，2）逆向DDIM求解器，以及3）通过添加少量噪声进行部分采样。

    Face morphs created by Diffusion Autoencoders are a recent innovation and the design space of such an approach has not been well explored. We explore three axes of the design space, i.e., 1) sampling algorithms, 2) the reverse DDIM solver, and 3) partial sampling through small amounts of added noise.
    
[^11]: 能否准确分类人类情绪？一个深度学习的面部表情识别研究

    Can CNNs Accurately Classify Human Emotions? A Deep-Learning Facial Expression Recognition Study. (arXiv:2310.09473v1 [cs.LG])

    [http://arxiv.org/abs/2310.09473](http://arxiv.org/abs/2310.09473)

    这项研究调查了卷积神经网络模型识别和分类人类面部表情的能力，并且证明模型在分类三种情绪类别时表现出了优于随机猜测的准确性。

    

    情绪人工智能当前是人工智能领域最受期待的发展之一。如果成功，这些人工智能将被归类为最复杂、最智能的非人类实体，因为它们将具备感知能力，这是区分生物人类和机械机器人的主要因素之一。为了被归类为“有情感”的人工智能，它们应该能够与他人产生共情并分类他们的情绪，因为没有这些能力，它们无法正常与人类互动。本研究调查了卷积神经网络模型识别和分类人类面部表情（积极、中性、消极）的能力。本研究使用Python编写了CNN模型，并使用芝加哥面部数据库的预处理数据进行训练。模型故意设计得较简单，以进一步研究其能力。我们假设该模型在分类输入数据的每个情感类别时将比随机猜测（33.3%）表现更好。模型的准确性通过n进行测试。

    Emotional Artificial Intelligences are currently one of the most anticipated developments of AI. If successful, these AIs will be classified as one of the most complex, intelligent nonhuman entities as they will possess sentience, the primary factor that distinguishes living humans and mechanical machines. For AIs to be classified as "emotional," they should be able to empathize with others and classify their emotions because without such abilities they cannot normally interact with humans. This study investigates the CNN model's ability to recognize and classify human facial expressions (positive, neutral, negative). The CNN model made for this study is programmed in Python and trained with preprocessed data from the Chicago Face Database. The model is intentionally designed with less complexity to further investigate its ability. We hypothesized that the model will perform better than chance (33.3%) in classifying each emotion class of input data. The model accuracy was tested with n
    
[^12]: 用于变分量子系统的局部零阶优化器的随机基准测试

    Randomized Benchmarking of Local Zeroth-Order Optimizers for Variational Quantum Systems. (arXiv:2310.09468v1 [quant-ph])

    [http://arxiv.org/abs/2310.09468](http://arxiv.org/abs/2310.09468)

    本研究通过随机基准测试比较了局部零阶优化器在量子优化问题上的性能，为未来改进这些优化器提供了一些见解。

    

    在量子信息领域中，经典优化器起着重要的作用。从优化物理设备的实验者到探索变分量子算法的理论家，量子信息的许多方面都需要使用经典优化器。因此，有许多论文对不同优化器在特定量子优化任务和参数化算法选择的有效性进行基准测试。然而，对于探索新算法或物理设备的研究人员来说，这些研究的见解不一定适用。为了解决这个问题，我们通过一系列部分随机化的任务来比较经典优化器的性能，以更广泛地抽样量子优化问题的空间。我们重点关注局部零阶优化器，因为它们在量子系统上通常具有良好的性能和查询效率。我们讨论了这些实验的见解，可以帮助激发未来的工作来改进这些优化器。

    In the field of quantum information, classical optimizers play an important role. From experimentalists optimizing their physical devices to theorists exploring variational quantum algorithms, many aspects of quantum information require the use of a classical optimizer. For this reason, there are many papers that benchmark the effectiveness of different optimizers for specific quantum optimization tasks and choices of parameterized algorithms. However, for researchers exploring new algorithms or physical devices, the insights from these studies don't necessarily translate. To address this concern, we compare the performance of classical optimizers across a series of partially-randomized tasks to more broadly sample the space of quantum optimization problems. We focus on local zeroth-order optimizers due to their generally favorable performance and query-efficiency on quantum systems. We discuss insights from these experiments that can help motivate future works to improve these optimiz
    
[^13]: 一个赋予因果分析能力的增强学习智能代理框架：增强自动加密货币交易

    A Framework for Empowering Reinforcement Learning Agents with Causal Analysis: Enhancing Automated Cryptocurrency Trading. (arXiv:2310.09462v1 [cs.AI])

    [http://arxiv.org/abs/2310.09462](http://arxiv.org/abs/2310.09462)

    本研究提出了一个基于强化学习的自动交易系统框架CausalReinforceNet，通过因果分析增强了强化学习代理的能力，以提高对加密货币市场的交易能力。

    

    尽管人工智能增强交易方法取得了一定进展，但在快速发展的加密货币市场中开发盈利的自动交易系统仍然具有挑战性。本研究旨在通过开发基于强化学习的自动交易系统来解决这些挑战，针对五种热门的替代加密货币（即比特币以外的加密货币）：币安币、以太坊、莱特币、瑞波币和泰达币。为此，我们提出了CausalReinforceNet，一个被构建为决策支持系统的框架。作为交易系统的基础架构，CausalReinforceNet框架通过因果分析增强了强化学习代理的能力。在该框架内，我们在特征工程过程中使用贝叶斯网络来识别影响加密货币价格变动的具有因果关系的最相关特征。此外，我们还通过动态贝叶斯网络将概率性价格方向信号纳入框架中，以增强交易系统的功能。

    Despite advances in artificial intelligence-enhanced trading methods, developing a profitable automated trading system remains challenging in the rapidly evolving cryptocurrency market. This study aims to address these challenges by developing a reinforcement learning-based automated trading system for five popular altcoins~(cryptocurrencies other than Bitcoin): Binance Coin, Ethereum, Litecoin, Ripple, and Tether. To this end, we present CausalReinforceNet, a framework framed as a decision support system. Designed as the foundational architecture of the trading system, the CausalReinforceNet framework enhances the capabilities of the reinforcement learning agent through causal analysis. Within this framework, we use Bayesian networks in the feature engineering process to identify the most relevant features with causal relationships that influence cryptocurrency price movements. Additionally, we incorporate probabilistic price direction signals from dynamic Bayesian networks to enhance
    
[^14]: LgTS：使用LLM生成的子目标进行动态任务采样，用于增强学习代理

    LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents. (arXiv:2310.09454v1 [cs.AI])

    [http://arxiv.org/abs/2310.09454](http://arxiv.org/abs/2310.09454)

    本研究提出了一种名为LgTS的新方法，利用LLM的规划能力，为无法访问环境转换动力学的RL代理提供子目标的图形表示。这个方法可以教导RL代理学习一组成功的策略来达到目标状态。

    

    最近，大型语言模型（LLM）在机器人和人工智能代理需要高级规划的问题中的推理能力方面取得了重大进展。然而，目前利用LLM进行此类规划任务的技术存在一些关键假设，比如需要访问允许微调的数据集，需要精心设计的提示信息仅向LLM提供相关且必要的信息，以及必须采用确定性方法执行LLM的响应，例如使用现有策略或计划操作。在本研究中，我们提出了LgTS（LLM引导的师生学习）这一全新的方法，它探索了LLM的规划能力，为无法访问环境转换动力学的增强学习（RL）代理提供了子目标的图形表示。RL代理利用师生学习算法学习一组成功达到目标状态的策略。

    Recent advancements in reasoning abilities of Large Language Models (LLM) has promoted their usage in problems that require high-level planning for robots and artificial agents. However, current techniques that utilize LLMs for such planning tasks make certain key assumptions such as, access to datasets that permit finetuning, meticulously engineered prompts that only provide relevant and essential information to the LLM, and most importantly, a deterministic approach to allow execution of the LLM responses either in the form of existing policies or plan operators. In this work, we propose LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment. The RL agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the st
    
[^15]: 双向相似性学习(SimPLE)是简单的。

    Pairwise Similarity Learning is SimPLE. (arXiv:2310.09449v1 [cs.CV])

    [http://arxiv.org/abs/2310.09449](http://arxiv.org/abs/2310.09449)

    本文提出了一个名为SimPLE的简单的无代理方法，用于解决双向相似性学习(PSL)问题，该方法无需额外的规范化和边距设置，并在开放集识别中表现良好。

    

    本文关注于一种普遍而重要的学习问题，即双向相似性学习(PSL)。PSL涵盖了一系列重要应用，如开放式人脸识别、说话人验证、图像检索和人物再识别。PSL的目标是学习一个双向相似性函数，为正样本对（即具有相同标签的样本对）赋予更高的相似性得分，而对于负样本对（即具有不同标签的样本对）赋予较低的相似性得分。我们首先确定了PSL的一个关键要求，然后讨论了现有方法如何实现这个要求。然后，我们提出了一种令人惊讶的简单的无代理方法——SimPLE，它既不需要特征/代理规范化，也不需要角度边距，但在开放集识别中能够很好地泛化。我们将所提出的方法应用于三个具有挑战性的PSL任务：开放式人脸识别、图像检索和说话人验证。基于大规模基准的全面实验结果证明了该方法的有效性。

    In this paper, we focus on a general yet important learning problem, pairwise similarity learning (PSL). PSL subsumes a wide range of important applications, such as open-set face recognition, speaker verification, image retrieval and person re-identification. The goal of PSL is to learn a pairwise similarity function assigning a higher similarity score to positive pairs (i.e., a pair of samples with the same label) than to negative pairs (i.e., a pair of samples with different label). We start by identifying a key desideratum for PSL, and then discuss how existing methods can achieve this desideratum. We then propose a surprisingly simple proxy-free method, called SimPLE, which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition. We apply the proposed method to three challenging PSL tasks: open-set face recognition, image retrieval and speaker verification. Comprehensive experimental results on large-scale benchmar
    
[^16]: G10：通过智能张量迁移实现高效的统一GPU内存和存储架构

    G10: Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations. (arXiv:2310.09443v1 [cs.AR])

    [http://arxiv.org/abs/2310.09443](http://arxiv.org/abs/2310.09443)

    G10是一个统一的GPU内存和存储架构，通过智能张量迁移来实现高效的扩展GPU内存容量并满足深度学习工作负载的可扩展性要求。

    

    为了突破GPU内存墙，以扩展深度学习工作负载规模，近期提出了各种架构和系统技术。它们的典型方法包括使用闪存扩展内存和直接存储访问。然而，这些技术仍然存在性能不佳以及引入GPU内存管理复杂性的问题，这使得它们难以满足当今深度学习工作负载的可扩展性要求。在本文中，我们提出了一种名为G10的统一GPU内存和存储架构，该架构受到深度学习工作负载的张量行为高度可预测的事实推动。G10将主机内存、GPU内存和闪存内存集成到统一的内存空间中，以扩展GPU内存容量并实现透明的数据迁移。基于这种统一的GPU内存和存储架构，G10利用编译器技术对深度学习工作负载中的张量行为进行特征化。因此，它可以提前调度数据迁移。

    To break the GPU memory wall for scaling deep learning workloads, a variety of architecture and system techniques have been proposed recently. Their typical approaches include memory extension with flash memory and direct storage access. However, these techniques still suffer from suboptimal performance and introduce complexity to the GPU memory management, making them hard to meet the scalability requirement of deep learning workloads today. In this paper, we present a unified GPU memory and storage architecture named G10 driven by the fact that the tensor behaviors of deep learning workloads are highly predictable. G10 integrates the host memory, GPU memory, and flash memory into a unified memory space, to scale the GPU memory capacity while enabling transparent data migrations. Based on this unified GPU memory and storage architecture, G10 utilizes compiler techniques to characterize the tensor behaviors in deep learning workloads. Therefore, it can schedule data migrations in advan
    
[^17]: 目标变量工程

    Target Variable Engineering. (arXiv:2310.09440v1 [cs.LG])

    [http://arxiv.org/abs/2310.09440](http://arxiv.org/abs/2310.09440)

    本研究通过比较回归和分类的预测性能，发现回归模型需要更多计算工作才能达到最佳性能，并且对训练过程中的随机性和启发式选择更为敏感。

    

    目标变量的设计如何影响机器学习流程中的性能？本研究的实验通过将数值目标与阈值进行比较来进行二元化处理。我们比较了针对数值目标进行训练的回归模型与针对其二元化版本进行训练的分类器的预测性能。具体而言，我们在随机超参数优化搜索的每个点上进行比较，以了解计算资源预算对两者之间权衡的影响。我们发现，回归模型需要更多的计算工作才能收敛到最佳性能，并且对训练过程中的随机性和启发式选择更为敏感。虽然分类也可以通过系统化的超参数调整和模型选择进行改进，但改进程度远不及回归。本研究是对回归和分类进行系统比较的第一项工作。

    How does the formulation of a target variable affect performance within the ML pipeline? The experiments in this study examine numeric targets that have been binarized by comparing against a threshold. We compare the predictive performance of regression models trained to predict the numeric targets vs. classifiers trained to predict their binarized counterparts. Specifically, we make this comparison at every point of a randomized hyperparameter optimization search to understand the effect of computational resource budget on the tradeoff between the two. We find that regression requires significantly more computational effort to converge upon the optimal performance, and is more sensitive to both randomness and heuristic choices in the training process. Although classification can and does benefit from systematic hyperparameter tuning and model selection, the improvements are much less than for regression. This work comprises the first systematic comparison of regression and classificat
    
[^18]: 混合任务的连续学习中的子网络发现和软掩蔽

    Sub-network Discovery and Soft-masking for Continual Learning of Mixed Tasks. (arXiv:2310.09436v1 [cs.CL])

    [http://arxiv.org/abs/2310.09436](http://arxiv.org/abs/2310.09436)

    该论文提出了一种新的连续学习方法，通过发现每个任务的子网络和软掩蔽机制来克服灾难性遗忘和促进知识传递。实验证明该方法在多个任务的情况下都表现优异。

    

    连续学习有两个主要目标: 防止灾难性遗忘和促进知识传递。现有文献主要关注于克服灾难性遗忘。一些工作也针对任务相似时的知识传递进行了研究。据我们所知，只有一种方法提出了连续学习混合任务序列的方法。然而，这些技术仍然存在灾难性遗忘和/或有限的知识传递问题。本文提出了一种新的连续学习方法来实现两者。它通过发现每个任务的子网络来克服灾难性遗忘。还提出了一种软掩蔽机制来保留先前的知识，并使新任务能够借助过去的知识实现知识传递。使用分类、生成、信息提取及其混合 (即异构任务) 进行的实验表明，所提出的方法始终优于强基准模型。

    Continual learning (CL) has two main objectives: preventing catastrophic forgetting (CF) and encouraging knowledge transfer (KT). The existing literature mainly focused on overcoming CF. Some work has also been done on KT when the tasks are similar. To our knowledge, only one method has been proposed to learn a sequence of mixed tasks. However, these techniques still suffer from CF and/or limited KT. This paper proposes a new CL method to achieve both. It overcomes CF by isolating the knowledge of each task via discovering a subnetwork for it. A soft-masking mechanism is also proposed to preserve the previous knowledge and to enable the new task to leverage the past knowledge to achieve KT. Experiments using classification, generation, information extraction, and their mixture (i.e., heterogeneous tasks) show that the proposed method consistently outperforms strong baselines.
    
[^19]: 通过循环神经网络学习非线性积分算子及其在求解积分微分方程中的应用

    Learning nonlinear integral operators via Recurrent Neural Networks and its application in solving Integro-Differential Equations. (arXiv:2310.09434v1 [cs.LG])

    [http://arxiv.org/abs/2310.09434](http://arxiv.org/abs/2310.09434)

    本文提出使用LSTM-RNN学习非线性积分算子，并将其应用于求解积分微分方程。使用LSTM-RNN表示非线性积分算子可将非线性积分微分方程转化为普通微分方程，从而提高求解效率。通过模型问题展示了该方法的效率和稳定性，并验证了学习到的积分算子的普适性。

    

    本文提出使用LSTM-RNN（长短期记忆循环神经网络）来学习和表示出现在非线性积分微分方程中的非线性积分算子。非线性积分算子的LSTM-RNN表示使得我们可以将非线性积分微分方程的系统转化为普通微分方程的系统，从而可以使用很多高效的求解器。此外，由于在积分微分方程中使用LSTM-RNN表示的非线性积分算子不需要在每个数值时间演化步骤中进行数值积分，LSTM-RNN基于的积分微分方程求解器的总时间成本可以从$O(n_T^2)$降低到$O(n_T)$，其中$n_T$为需要计算的步长。我们通过一个模型问题展示了这种LSTM-RNN基于的积分微分方程求解器的效率和稳定性。此外，我们通过将其应用于积分微分方程演示了学习到的积分算子的普适性。

    In this paper, we propose using LSTM-RNNs (Long Short-Term Memory-Recurrent Neural Networks) to learn and represent nonlinear integral operators that appear in nonlinear integro-differential equations (IDEs). The LSTM-RNN representation of the nonlinear integral operator allows us to turn a system of nonlinear integro-differential equations into a system of ordinary differential equations for which many efficient solvers are available. Furthermore, because the use of LSTM-RNN representation of the nonlinear integral operator in an IDE eliminates the need to perform a numerical integration in each numerical time evolution step, the overall temporal cost of the LSTM-RNN-based IDE solver can be reduced to $O(n_T)$ from $O(n_T^2)$ if a $n_T$-step trajectory is to be computed. We illustrate the efficiency and robustness of this LSTM-RNN-based numerical IDE solver with a model problem. Additionally, we highlight the generalizability of the learned integral operator by applying it to IDEs dri
    
[^20]: 基于硅微环的库仑计算中腔体非线性和线性损耗的影响

    Effects of cavity nonlinearities and linear losses on silicon microring-based reservoir computing. (arXiv:2310.09433v1 [physics.optics])

    [http://arxiv.org/abs/2310.09433](http://arxiv.org/abs/2310.09433)

    本文研究了微环谐振器中腔体非线性和线性损耗对库仑计算性能的影响，并发现了三个区域，其中一个区域在低输入功率和节点数下具有非常低的时间序列预测误差。这项研究对于改进时间延迟库仑计算的预测性能有着重要的设计和优化指导作用。

    

    微环谐振器是时间延迟光子库仑计算的有希望的器件，但是微环谐振器中不同物理效应对库仑计算性能的影响尚未完全理解。我们通过数值分析线性损耗、热光和自由载流子效应松弛时间对时间序列任务 NARMA-10 的预测误差的影响。我们证明了存在三个区域，由输入功率和光源与微环谐振之间的频率失谐定义，这些区域揭示了腔体从线性到非线性状态的转变。其中一个区域在相对较低的输入功率和节点数下提供了非常低的时间序列预测误差，而其他区域要么缺乏非线性，要么变得不稳定。这项研究为微环谐振器的设计和优化其物理特性以提高时间延迟库仑计算的预测性能提供了洞察。

    Microring resonators (MRRs) are promising devices for time-delay photonic reservoir computing, but the impact of the different physical effects taking place in the MRRs on the reservoir computing performance is yet to be fully understood. We numerically analyze the impact of linear losses as well as thermo-optic and free-carrier effects relaxation times on the prediction error of the time-series task NARMA-10. We demonstrate the existence of three regions, defined by the input power and the frequency detuning between the optical source and the microring resonance, that reveal the cavity transition from linear to nonlinear regimes. One of these regions offers very low error in time-series prediction under relatively low input power and number of nodes while the other regions either lack nonlinearity or become unstable. This study provides insight into the design of the MRR and the optimization of its physical properties for improving the prediction performance of time-delay reservoir co
    
[^21]: 通过关键词驱动的句子选择增强基于BERT的视觉问答

    Enhancing BERT-Based Visual Question Answering through Keyword-Driven Sentence Selection. (arXiv:2310.09432v1 [cs.CL])

    [http://arxiv.org/abs/2310.09432](http://arxiv.org/abs/2310.09432)

    本文提出了一种通过关键词驱动的句子选择策略来增强基于BERT的视觉问答。通过利用掩码语言建模技术微调BERT模型，并重点关注带有敏感关键词的句子，本方法能够在文档中识别出回答问题的相关元素，并在视觉问答任务中取得高性能。

    

    基于文档的视觉问答竞赛解决了在多页文档中自动检测元素之间的父子关系的问题。目标是识别回答自然语言提出的特定问题的文档元素。本文描述了PoliTo针对此任务的方法，具体而言，我们的最佳解决方案探索了一种仅使用文本的方法，利用了一种特制的抽样策略。具体而言，我们的方法利用了掩码语言建模技术来微调BERT模型，重点关注包含敏感关键词且与问题中出现的关键词相同的句子，例如对表格或图像的引用。由于这种方法的有效性，我们能够与基线相比取得较高的性能，展示了我们的解决方案对此任务的积极贡献。

    The Document-based Visual Question Answering competition addresses the automatic detection of parent-child relationships between elements in multi-page documents. The goal is to identify the document elements that answer a specific question posed in natural language. This paper describes the PoliTo's approach to addressing this task, in particular, our best solution explores a text-only approach, leveraging an ad hoc sampling strategy. Specifically, our approach leverages the Masked Language Modeling technique to fine-tune a BERT model, focusing on sentences containing sensitive keywords that also occur in the questions, such as references to tables or images. Thanks to the effectiveness of this approach, we are able to achieve high performance compared to baselines, demonstrating how our solution contributes positively to this task.
    
[^22]: 线下强化学习用于优化生产竞标策略

    Offline Reinforcement Learning for Optimizing Production Bidding Policies. (arXiv:2310.09426v1 [cs.LG])

    [http://arxiv.org/abs/2310.09426](http://arxiv.org/abs/2310.09426)

    该论文介绍了一种使用离线强化学习方法来优化生产环境中竞标策略的通用方法，该方法可以优化任何可微分的基础策略，只需要使用基础策略生成的数据。论文提出了一种混合代理架构，将基础策略与强化学习模块相结合。

    

    在线广告市场每秒进行数千次拍卖，对于希望在预算限制下优化支出的广告商来说，这是一个艰巨的挑战。因此，广告平台通常为他们的客户提供自动化代理人，代表他们实时大规模竞标。由于这些代理人由平台拥有但使用广告商的资金进行操作，因此在平衡代理人的可靠性和可解释性与优化性能方面存在着强烈的实际需求。我们提出了一种通过离线强化学习从真实数据中学习的方法，以优化生产环境中的竞标策略。这种方法可以用于优化任何可微分的基础策略（实际上是基于广告商能够轻松理解的原则的启发式策略），并且只需要由基础策略本身生成的数据。我们使用混合代理架构，将任意基础策略与强化学习模块相结合。

    The online advertising market, with its thousands of auctions run per second, presents a daunting challenge for advertisers who wish to optimize their spend under a budget constraint. Thus, advertising platforms typically provide automated agents to their customers, which act on their behalf to bid for impression opportunities in real time at scale. Because these proxy agents are owned by the platform but use advertiser funds to operate, there is a strong practical need to balance reliability and explainability of the agent with optimizing power. We propose a generalizable approach to optimizing bidding policies in production environments by learning from real data using offline reinforcement learning. This approach can be used to optimize any differentiable base policy (practically, a heuristic policy based on principles which the advertiser can easily understand), and only requires data generated by the base policy itself. We use a hybrid agent architecture that combines arbitrary ba
    
[^23]: ZeroSwap: 基于数据驱动的 DeFi 中的最优市场做市

    ZeroSwap: Data-driven Optimal Market Making in DeFi. (arXiv:2310.09413v1 [cs.LG])

    [http://arxiv.org/abs/2310.09413](http://arxiv.org/abs/2310.09413)

    ZeroSwap 是第一个基于数据驱动算法的 DeFi 市场做市方案，在保持市场做市商零利润的情况下，通过适应交易者行为来解决了流动性提供者遭受套利损失的问题。

    

    自动做市商 (AMMs) 是去中心化金融中匹配流动性供给和需求的主要中心。它们的功能主要依赖于流动性提供者 (LPs) 将其资产投资于流动性池。然而，池中资产交易的价格通常比集中化和更流动的交易所价格延迟更多。这导致流动性提供者遭受套利损失。我们通过采用 Glosten 和 Milgrom 的经典市场微观结构模型，将市场价格适应于交易者行为，从而解决了这个问题。在本文中，我们提出了第一个最优贝叶斯和第一个无模型数据驱动算法来最优地跟踪资产的外部价格。我们使用的最优性概念在市场做市商的价格上强制执行了零利润条件，因此取名为 ZeroSwap。这确保了市场做市商在损失知情交易者的同时从噪声交易者那里获得利润。

    Automated Market Makers (AMMs) are major centers of matching liquidity supply and demand in Decentralized Finance. Their functioning relies primarily on the presence of liquidity providers (LPs) incentivized to invest their assets into a liquidity pool. However, the prices at which a pooled asset is traded is often more stale than the prices on centralized and more liquid exchanges. This leads to the LPs suffering losses to arbitrage. This problem is addressed by adapting market prices to trader behavior, captured via the classical market microstructure model of Glosten and Milgrom. In this paper, we propose the first optimal Bayesian and the first model-free data-driven algorithm to optimally track the external price of the asset. The notion of optimality that we use enforces a zero-profit condition on the prices of the market maker, hence the name ZeroSwap. This ensures that the market maker balances losses to informed traders with profits from noise traders. The key property of our 
    
[^24]: 基于混合强化学习优化实际水力系统中泵站的可持续性

    Hybrid Reinforcement Learning for Optimizing Pump Sustainability in Real-World Water Distribution Networks. (arXiv:2310.09412v1 [cs.AI])

    [http://arxiv.org/abs/2310.09412](http://arxiv.org/abs/2310.09412)

    本文研究了基于混合强化学习的泵站调度优化问题，旨在提高实时控制的同时降低能源消耗和运营成本。传统的优化技术由于缺乏收敛性保证表现不佳，而强化学习能够适应不确定性并实现实时响应。然而，准确的水力系统模拟模型对于实施强化学习至关重要，以前的应用受到模拟训练数据误差的限制。

    

    本文针对实际水力系统中泵站调度优化问题进行研究，旨在提高实时控制的同时遵守物理运行约束，减少能源消耗和运营成本。传统的优化技术，如基于进化和遗传算法，往往由于缺乏收敛性保证而表现不佳。相反，强化学习在适应不确定性和减少推断时间方面具有突出优势，实现了实时响应。然而，有效实施强化学习依赖于准确的水力系统模拟模型，以前的应用受到模拟训练数据误差的限制。这些误差可能导致强化学习代理学习到误导性的模式和行为，并推荐次优的运行策略。为了克服这些挑战，我们提出了一种改进的“混合强化学习”方法。

    This article addresses the pump-scheduling optimization problem to enhance real-time control of real-world water distribution networks (WDNs). Our primary objectives are to adhere to physical operational constraints while reducing energy consumption and operational costs. Traditional optimization techniques, such as evolution-based and genetic algorithms, often fall short due to their lack of convergence guarantees. Conversely, reinforcement learning (RL) stands out for its adaptability to uncertainties and reduced inference time, enabling real-time responsiveness. However, the effective implementation of RL is contingent on building accurate simulation models for WDNs, and prior applications have been limited by errors in simulation training data. These errors can potentially cause the RL agent to learn misleading patterns and actions and recommend suboptimal operational strategies. To overcome these challenges, we present an improved "hybrid RL" methodology. This method integrates th
    
[^25]: 用深度学习调查文本摘要的现状：一项综述研究

    Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review. (arXiv:2310.09411v1 [cs.CL])

    [http://arxiv.org/abs/2310.09411](http://arxiv.org/abs/2310.09411)

    深度学习在自然语言处理中的应用已经取得了显著的成果，并且在文本摘要领域也表现出了巨大的潜力。

    

    近年来，深度学习通过使模型能够学习语言数据的复杂表示，彻底改变了自然语言处理（NLP），从而在各种NLP任务的性能方面取得了显著的改进。与传统的NLP方法相反，深度学习NLP模型通常使用大量数据来训练深度神经网络，使其能够学习语言数据中的模式和关系。深度神经网络能够学习语言数据的层次表示，处理可变长度的输入序列，并在大型数据集上表现良好，这使它们非常适合NLP应用。由于文本数据的指数增长和对简洁、连贯和信息丰富的摘要的需求不断增加，文本摘要已成为NLP领域的一个关键研究领域。将深度学习应用于文本摘要

    In recent years, deep learning has revolutionized natural language processing (NLP) by enabling the development of models that can learn complex representations of language data, leading to significant improvements in performance across a wide range of NLP tasks. Deep learning models for NLP typically use large amounts of data to train deep neural networks, allowing them to learn the patterns and relationships in language data. This is in contrast to traditional NLP approaches, which rely on hand-engineered features and rules to perform NLP tasks. The ability of deep neural networks to learn hierarchical representations of language data, handle variable-length input sequences, and perform well on large datasets makes them well-suited for NLP applications. Driven by the exponential growth of textual data and the increasing demand for condensed, coherent, and informative summaries, text summarization has been a critical research area in the field of NLP. Applying deep learning to text su
    
[^26]: 专家模型的可识别性

    Identifiability of Product of Experts Models. (arXiv:2310.09397v1 [cs.LG])

    [http://arxiv.org/abs/2310.09397](http://arxiv.org/abs/2310.09397)

    本文研究了专家模型的可识别性问题，发现当潜变量均匀分布时，使用与参数数量相等的可观测量可以完全识别模型；而在一般情况下，可识别性仍然成立，但需要更多的观测数据。

    

    专家模型是分层网络，每个节点的值是其输入值（可能取反）的乘积。这些模型被引入为一种神经网络结构，可以有效地学习生成满足许多低维约束的高维数据 - 从而允许每个专家执行简单的任务。专家模型在学习中有各种应用。本文研究了一种具有二进制潜变量层和条件独立于潜变量的二进制可观测层的专家模型的可识别性问题。先前的最优上界表明，识别该模型所需的可观测量数目随参数数量呈指数增长。我们证明：（a）当潜变量均匀分布时，模型可通过与参数数量相等的可观测量来识别（从而达到最佳可识别性）。（b）在更一般的情况下，当潜变量服从任意分布时，模型的可识别性成立。

    Product of experts (PoE) are layered networks in which the value at each node is an AND (or product) of the values (possibly negated) at its inputs. These were introduced as a neural network architecture that can efficiently learn to generate high-dimensional data which satisfy many low-dimensional constraints -- thereby allowing each individual expert to perform a simple task. PoEs have found a variety of applications in learning.  We study the problem of identifiability of a product of experts model having a layer of binary latent variables, and a layer of binary observables that are iid conditional on the latents. The previous best upper bound on the number of observables needed to identify the model was exponential in the number of parameters. We show: (a) When the latents are uniformly distributed, the model is identifiable with a number of observables equal to the number of parameters (and hence best possible). (b) In the more general case of arbitrarily distributed latents, the 
    
[^27]: 基于分割学习的语义对齐用于弹性多用户语义通信

    Semantics Alignment via Split Learning for Resilient Multi-User Semantic Communication. (arXiv:2310.09394v1 [cs.LG])

    [http://arxiv.org/abs/2310.09394](http://arxiv.org/abs/2310.09394)

    本文提出了一种基于分割学习的方法，用于在多个神经收发器之间对齐语义。该方法使用局部微调技术，可以有效地控制计算和通信成本。

    

    最近对语义通信的研究常常依赖于基于神经网络的收发器，如深度联合源和信道编码（DeepJSCC）。与传统的收发器不同，这些神经收发器可以使用实际的源数据和信道进行训练，使其能够提取和传递语义信息。然而，每个神经收发器固有地偏向于特定的源数据和信道，使得不同的收发器很难理解预期的语义，尤其是在初始遇到时。为了在多个神经收发器之间对齐语义，我们提出了一种基于分布式学习的解决方案，利用分割学习（SL）和局部神经网络微调技术。在这种称为具有层冻结的SL（SLF）的方法中，每个编码器下载一个不对齐的解码器，并在本地微调这些编码器-解码器神经网络层的一部分。通过调整这个比例，SLF可以控制计算和通信成本。仿真结果证实了该方法的有效性。

    Recent studies on semantic communication commonly rely on neural network (NN) based transceivers such as deep joint source and channel coding (DeepJSCC). Unlike traditional transceivers, these neural transceivers are trainable using actual source data and channels, enabling them to extract and communicate semantics. On the flip side, each neural transceiver is inherently biased towards specific source data and channels, making different transceivers difficult to understand intended semantics, particularly upon their initial encounter. To align semantics over multiple neural transceivers, we propose a distributed learning based solution, which leverages split learning (SL) and partial NN fine-tuning techniques. In this method, referred to as SL with layer freezing (SLF), each encoder downloads a misaligned decoder, and locally fine-tunes a fraction of these encoder-decoder NN layers. By adjusting this fraction, SLF controls computing and communication costs. Simulation results confirm t
    
[^28]: 从雷达中利用机器学习估计最大垂直速度

    Machine Learning Estimation of Maximum Vertical Velocity from Radar. (arXiv:2310.09392v1 [cs.LG])

    [http://arxiv.org/abs/2310.09392](http://arxiv.org/abs/2310.09392)

    本研究利用机器学习模型U-Nets，通过3D雷达反射率，成功地估计了最大垂直速度及其面积范围，并采用Sinh-arcsinh-normal（SHASH）分布参数回归技术进行了确定性和概率预测。

    

    尽管是严重天气灾害的源头，但对快速上升气流（即上升气流）的量化仍无法用于操作预测。像卫星图像中的透顶区域这样的上升气流代理物已被与严重天气灾害联系起来，但只与总体风暴上升气流的一小部分有关。本研究调查了一个机器学习模型，即U-Nets，是否能够仅利用三维（3D）格网雷达反射率，精确地提取最大垂直速度及其面积范围。该机器学习模型使用模拟雷达反射率和垂直速度训练于国家严重风暴实验室的预测预警系统（WoFS）。采用Sinh-arcsinh-normal（SHASH）分布的参数回归技术来适应UNets，允许对最大垂直速度进行确定性和概率预测。经过超参数搜索后，选出了最佳模型。

    Despite being the source region of severe weather hazards, the quantification of the fast current of upward moving air (i.e., updraft) remains unavailable for operational forecasting. Updraft proxies, like overshooting top area from satellite images, have been linked to severe weather hazards but only relate to a limited portion of the total storm updraft. This study investigates if a machine learning model, namely U-Nets, can skillfully retrieve maximum vertical velocity and its areal extent from 3-dimensional (3D) gridded radar reflectivity alone. The machine learning model is trained using simulated radar reflectivity and vertical velocity from the National Severe Storm Laboratory's convection permitting Warn on Forecast System (WoFS). A parametric regression technique using the Sinh-arcsinh-normal (SHASH) distribution is adapted to run with UNets, allowing for both deterministic and probabilistic predictions of maximum vertical velocity. The best models after hyperparameter search 
    
[^29]: CORN: 全参考和非参考音频度量的共训练模型

    CORN: Co-Trained Full-Reference And No-Reference Audio Metrics. (arXiv:2310.09388v1 [eess.AS])

    [http://arxiv.org/abs/2310.09388](http://arxiv.org/abs/2310.09388)

    CORN是一个新颖的框架，将全参考和非参考音频度量结合起来，并尝试在训练时同时训练这两种模型。CORN FR模式同时具备全参考和非参考度量的性能。

    

    感知评估是各种音频处理任务中至关重要的方面。全参考（FR）或基于相似性的度量依赖于高质量的参考录音，将其与录音的低质量或损坏版本进行比较以进行评估。相反，非参考（NR）度量评估录音而不依赖参考。FR和NR两种方法相对于彼此都具有优势和缺点。本文中，我们提出了一种新颖的框架称为CORN，将这两种方法结合起来，同时训练FR和NR模型。训练完成后，可以独立应用这些模型。我们通过预测几个常见的客观度量指标以及在两种不同架构上进行评估CORN。使用CORN训练的NR模型在训练期间可以访问参考录音，因此可以预期，它始终优于独立训练的基线NR模型。更令人印象深刻的是CORN FR模式可以同时提供全参考和非参考度量的性能。

    Perceptual evaluation constitutes a crucial aspect of various audio-processing tasks. Full reference (FR) or similarity-based metrics rely on high-quality reference recordings, to which lower-quality or corrupted versions of the recording may be compared for evaluation. In contrast, no-reference (NR) metrics evaluate a recording without relying on a reference. Both the FR and NR approaches exhibit advantages and drawbacks relative to each other. In this paper, we present a novel framework called CORN that amalgamates these dual approaches, concurrently training both FR and NR models together. After training, the models can be applied independently. We evaluate CORN by predicting several common objective metrics and across two different architectures. The NR model trained using CORN has access to a reference recording during training, and thus, as one would expect, it consistently outperforms baseline NR models trained independently. Perhaps even more remarkable is that the CORN FR mode
    
[^30]: LL-VQ-VAE: 学习可学习的格子向量量化以提高表示效率

    LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations. (arXiv:2310.09382v1 [cs.LG])

    [http://arxiv.org/abs/2310.09382](http://arxiv.org/abs/2310.09382)

    本文介绍了一种名为LL-VQ-VAE的学习可学习的格子向量量化方法，通过替换向量量化层来实现高效表示。与传统方法相比，该方法在相同训练条件下具有更低的重构误差，训练时间更短，参数数量恒定。

    

    本文介绍了可学习的格子向量量化方法(LL-VQ-VAE)，并展示了它在学习离散表示方面的有效性。我们的方法将VQ-VAE中的向量量化层替换为基于格子的离散化方法。可学习的格子对所有离散嵌入施加一种结构，防止码本崩溃，从而实现了高码本利用率。与VQ-VAE相比，我们的方法在相同的训练条件下得到了更低的重构误差，训练时间仅为一小部分，并且具有恒定数量的参数（等于嵌入维度D），使其成为一种非常可扩展的方法。我们在FFHQ-1024数据集上展示了这些结果，并包括了FashionMNIST和Celeb-A数据集。

    In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.
    
[^31]: 识别并研究成人数据集上的机器学习偏见

    Identifying and examining machine learning biases on Adult dataset. (arXiv:2310.09373v1 [cs.CY])

    [http://arxiv.org/abs/2310.09373](http://arxiv.org/abs/2310.09373)

    该研究通过集成学习探讨了减少机器学习模型偏见的方法，揭示了性别属性偏见对工资预测的重要影响。研究结果强调了在数据驱动的社会中实施混合模型的必要性，以实现公正和包容性。

    

    本研究通过集成学习深入探讨了减少机器学习模型偏见的方法。我们采用严格的方法全面评估了各个分类变量上的偏见，最终揭示了明显的性别属性偏见。实证证据揭示了显著的基于性别的工资预测差距：在将性别属性改为女性时，男性的预测工资从初始的902.91美元大幅降至774.31美元。值得注意的是，Kullback-Leibler散度得分指出了性别偏见，值超过0.13，主要集中在基于树的模型中。采用集成学习有助于追求公平和透明。有趣的是，我们的研究结果表明堆叠模型与各个单独模型一致，确认了模型偏见的弹性。本研究强调了道德考虑，并主张在以数据驱动的社会中实施混合模型，以追求公正和包容性。

    This research delves into the reduction of machine learning model bias through Ensemble Learning. Our rigorous methodology comprehensively assesses bias across various categorical variables, ultimately revealing a pronounced gender attribute bias. The empirical evidence unveils a substantial gender-based wage prediction disparity: wages predicted for males, initially at \$902.91, significantly decrease to \$774.31 when the gender attribute is alternated to females. Notably, Kullback-Leibler divergence scores point to gender bias, with values exceeding 0.13, predominantly within tree-based models. Employing Ensemble Learning elucidates the quest for fairness and transparency. Intriguingly, our findings reveal that the stacked model aligns with individual models, confirming the resilience of model bias. This study underscores ethical considerations and advocates the implementation of hybrid models for a data-driven society marked by impartiality and inclusivity.
    
[^32]: 从词语和练习到健康：用于自我依恋技术的波斯语聊天机器人

    From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment Technique. (arXiv:2310.09362v1 [cs.HC])

    [http://arxiv.org/abs/2310.09362](http://arxiv.org/abs/2310.09362)

    这项研究开发了一个能够发出声音的波斯语聊天机器人，用于指导用户进行基于依恋理论的自我依恋技术。通过使用规则和分类模块，聊天机器人可以理解用户输入并推荐适当的自我依恋练习。该研究还开发了一种准确率超过92%的情感分析模块，以识别用户情感。这项工作有助于在后疫情时代提供数字心理疗法的替代方案。

    

    在后疫情时代，社交孤立和抑郁焦虑症的患病率攀升的背景下，基于数字心理疗法的对话代理相对于传统疗法会发挥重要的影响。本文中，我们开发了一个能够发出声音的波斯语聊天机器人，指导用户进行自我依恋(Self-Attachment, SAT)技术，这是一种基于依恋理论的新型、自我管理、全面的心理技术。我们的聊天机器人使用一系列基于规则和分类的模块来理解用户在对话中的输入，并相应地导航对话流程图，根据用户的情感和心理状态推荐适当的SAT练习。具体而言，我们收集了超过6,000次话语的数据集，并开发了一种新颖的情感分析模块，可以将用户的情感分为12个类别，准确率超过92%。为了保持对话的新颖和吸引力，聊天机器人的回答是从大量话语数据集中检索得到的。

    In the wake of the post-pandemic era, marked by social isolation and surging rates of depression and anxiety, conversational agents based on digital psychotherapy can play an influential role compared to traditional therapy sessions. In this work, we develop a voice-capable chatbot in Farsi to guide users through Self-Attachment (SAT), a novel, self-administered, holistic psychological technique based on attachment theory. Our chatbot uses a dynamic array of rule-based and classification-based modules to comprehend user input throughout the conversation and navigates a dialogue flowchart accordingly, recommending appropriate SAT exercises that depend on the user's emotional and mental state. In particular, we collect a dataset of over 6,000 utterances and develop a novel sentiment-analysis module that classifies user sentiment into 12 classes, with accuracy above 92%. To keep the conversation novel and engaging, the chatbot's responses are retrieved from a large dataset of utterances c
    
[^33]: 是否仍然值得对$\ell_p$的鲁棒性进行认证?

    Is Certifying $\ell_p$ Robustness Still Worthwhile?. (arXiv:2310.09361v1 [cs.LG])

    [http://arxiv.org/abs/2310.09361](http://arxiv.org/abs/2310.09361)

    本文重新审视并回答了三个问题：我们为什么关心鲁棒性研究？我们为什么关心$\ell_p$有界的威胁模型？我们为什么关心认证而不是经验性防御？

    

    多年来，研究人员开发了各种攻击方法，利用了对抗性示例的普及性，以及旨在防范此类攻击所带来的安全漏洞的防御方法。本文主要关注提供对$\ell_p$有界攻击提供可证明保证的防御。认证防御取得了显著进展，将鲁棒性认证从玩具模型和数据集扩展到像ImageNet分类这样的大规模问题。虽然这无疑是一个有趣的学术问题，但随着领域的成熟，其在实践中的影响尚不清楚，因此我们发现重新审视继续进行这一研究线路的动机是有意义的。我们在本文中解决了三个层次的问题：(1)我们为什么关心鲁棒性研究？(2)我们为什么关心$\ell_p$有界的威胁模型？(3)我们为什么关心认证而不是经验性防御？简而言之，我们持以下立场：

    Over the years, researchers have developed myriad attacks that exploit the ubiquity of adversarial examples, as well as defenses that aim to guard against the security vulnerabilities posed by such attacks. Of particular interest to this paper are defenses that provide provable guarantees against the class of $\ell_p$-bounded attacks. Certified defenses have made significant progress, taking robustness certification from toy models and datasets to large-scale problems like ImageNet classification. While this is undoubtedly an interesting academic problem, as the field has matured, its impact in practice remains unclear, thus we find it useful to revisit the motivation for continuing this line of research. There are three layers to this inquiry, which we address in this paper: (1) why do we care about robustness research? (2) why do we care about the $\ell_p$-bounded threat model? And (3) why do we care about certification as opposed to empirical defenses? In brief, we take the position
    
[^34]: ReLU神经控制屏障函数的精确验证

    Exact Verification of ReLU Neural Control Barrier Functions. (arXiv:2310.09360v1 [cs.LG])

    [http://arxiv.org/abs/2310.09360](http://arxiv.org/abs/2310.09360)

    本文提出了一种用于验证具有ReLU激活函数的前馈神经控制屏障函数的安全性的新精确条件和算法，克服了传统安全验证方法中ReLU函数的不可微性质的挑战。

    

    控制屏障函数(CBFs)是一种用于非线性系统安全控制的常用方法。在基于CBF的控制中，系统的期望安全性质被映射到CBF的非负性，控制输入被选择以确保CBF始终保持非负。最近，将CBFs表示为神经网络(神经控制屏障函数，或NCBFs)的机器学习方法由于神经网络的普适表示能力而显示出巨大的潜力。然而，验证学习到的CBF是否能保证安全性仍然是一个具有挑战性的研究问题。本文提出了一种针对具有ReLU激活函数的前馈NCBFs的精确条件和算法来验证安全性。其中的关键挑战在于，由于ReLU函数的分段线性性质，在某些点处NCBF将是不可微的，因此无效地使传统的安全验证方法无法适用，这些方法假设一个光滑的屏障函数。我们通过利用g函数解决了这个问题。

    Control Barrier Functions (CBFs) are a popular approach for safe control of nonlinear systems. In CBF-based control, the desired safety properties of the system are mapped to nonnegativity of a CBF, and the control input is chosen to ensure that the CBF remains nonnegative for all time. Recently, machine learning methods that represent CBFs as neural networks (neural control barrier functions, or NCBFs) have shown great promise due to the universal representability of neural networks. However, verifying that a learned CBF guarantees safety remains a challenging research problem. This paper presents novel exact conditions and algorithms for verifying safety of feedforward NCBFs with ReLU activation functions. The key challenge in doing so is that, due to the piecewise linearity of the ReLU function, the NCBF will be nondifferentiable at certain points, thus invalidating traditional safety verification methods that assume a smooth barrier function. We resolve this issue by leveraging a g
    
[^35]: 何时才能使剧本在错误规范下保持稳定? (arXiv:2310.09358v1 [cs.LG])

    When are Bandits Robust to Misspecification?. (arXiv:2310.09358v1 [cs.LG])

    [http://arxiv.org/abs/2310.09358](http://arxiv.org/abs/2310.09358)

    该论文研究了参数化的强盗算法和情境化的强盗算法在真实奖励与模型之间存在误差的情况下的稳定性，并找到了依赖于问题实例和模型类的充分条件，使得经典算法如ε-贪心和LinUCB能够在时间范围内保持次线性的遗憾保障。

    

    参数特征为基础的奖励模型广泛应用于决策问题，如强盗算法和情境化的强盗算法。通常的假设是可行性，即行为的真实奖励完全由某个参数化模型解释。然而，我们关注的是真实奖励与模型类之间存在（可能显著）的误差的情况。对于参数化的强盗和情境化的强盗，我们识别出依赖问题实例和模型类的充分条件，使得经典算法如ε-贪心和LinUCB在即使奖励存在严重误差的情况下，也能够在时间范围内保证次线性（次于时间范围）的遗憾保障。这与现有的针对错误规范的最坏情况结果形成对比，后者显示遗憾边界随时间成线性比例增长，并且说明存在一个相当大的强盗问题实例集合在错误规范下仍然稳定。

    Parametric feature-based reward models are widely employed by algorithms for decision making settings such as bandits and contextual bandits. The typical assumption under which they are analysed is realizability, i.e., that the true rewards of actions are perfectly explained by some parametric model in the class. We are, however, interested in the situation where the true rewards are (potentially significantly) misspecified with respect to the model class. For parameterized bandits and contextual bandits, we identify sufficient conditions, depending on the problem instance and model class, under which classic algorithms such as $\epsilon$-greedy and LinUCB enjoy sublinear (in the time horizon) regret guarantees under even grossly misspecified rewards. This is in contrast to existing worst-case results for misspecified bandits which show regret bounds that scale linearly with time, and shows that there can be a nontrivially large set of bandit instances that are robust to misspecificati
    
[^36]: 使用生成方法的不确定性量化

    Uncertainty Quantification using Generative Approach. (arXiv:2310.09338v1 [cs.LG])

    [http://arxiv.org/abs/2310.09338](http://arxiv.org/abs/2310.09338)

    这篇论文介绍了一种使用深度生成方法的增量生成蒙特卡洛(IGMC)方法，用于测量深度神经网络中的不确定性。通过迭代训练生成模型并将其输出添加到数据集中，IGMC能计算随机变量期望的后验分布，并且具有理论保证的收敛速度。该方法适用于神经网络分类和回归任务，并在MNIST数字分类任务上进行了实证研究。

    

    我们提出了增量生成Monte Carlo (IGMC) 方法，该方法使用深度生成方法来测量深度神经网络中的不确定性。IGMC通过迭代训练生成模型，并将其输出添加到数据集中，以计算随机变量期望的后验分布。我们提供了IGMC相对于样本大小和抽样深度的收敛速度的理论保证。由于其与深度生成方法的兼容性，IGMC适用于神经网络分类和回归任务。我们在MNIST数字分类任务上对IGMC的行为进行了实证研究。

    We present the Incremental Generative Monte Carlo (IGMC) method, designed to measure uncertainty in deep neural networks using deep generative approaches. IGMC iteratively trains generative models, adding their output to the dataset, to compute the posterior distribution of the expectation of a random variable. We provide a theoretical guarantee of the convergence rate of IGMC relative to the sample size and sampling depth. Due to its compatibility with deep generative approaches, IGMC is adaptable to both neural network classification and regression tasks. We empirically study the behavior of IGMC on the MNIST digit classification task.
    
[^37]: 组合能力以乘法方式出现：在合成任务中探索扩散模型

    Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task. (arXiv:2310.09336v1 [cs.LG])

    [http://arxiv.org/abs/2310.09336](http://arxiv.org/abs/2310.09336)

    组合能力以乘法方式出现：研究了条件扩散模型在合成任务中的组合泛化能力，结果显示这种能力受到底层数据生成过程的结构影响，且模型在学习到更高级的组合时存在困难。

    

    现代生成模型展示出了产生极为逼真数据的前所未有的能力。然而，考虑到现实世界的自然组合性，这些模型在实际应用中可靠使用需要展示出能够组合新的概念集合以生成训练数据集中未见的输出的能力。先前的研究表明，最近的扩散模型确实表现出了有趣的组合泛化能力，但它们也会出现无法预测的失败。受此启发，我们在合成环境中进行了有控制性的研究，以了解条件扩散模型的组合泛化能力，我们变化了训练数据的不同属性并测量了模型生成越界样本的能力。我们的结果显示：（i）从一个概念生成样本的能力和将它们组合起来的能力的出现顺序受到了底层数据生成过程的结构的影响；（ii）在组合任务上的表现表明模型在学习到更高级的组合时存在困难。

    Modern generative models exhibit unprecedented capabilities to generate extremely realistic data. However, given the inherent compositionality of the real world, reliable use of these models in practical applications requires that they exhibit the capability to compose a novel set of concepts to generate outputs not seen in the training data set. Prior work demonstrates that recent diffusion models do exhibit intriguing compositional generalization abilities, but also fail unpredictably. Motivated by this, we perform a controlled study for understanding compositional generalization in conditional diffusion models in a synthetic setting, varying different attributes of the training data and measuring the model's ability to generate samples out-of-distribution. Our results show: (i) the order in which the ability to generate samples from a concept and compose them emerges is governed by the structure of the underlying data-generating process; (ii) performance on compositional tasks exhib
    
[^38]: 针对随机Metropolis-Hastings算法的统计保证

    Statistical guarantees for stochastic Metropolis-Hastings. (arXiv:2310.09335v1 [stat.ML])

    [http://arxiv.org/abs/2310.09335](http://arxiv.org/abs/2310.09335)

    该论文研究了针对随机Metropolis-Hastings算法的统计保证。通过引入简单的修正项，该方法可以避免计算成本上的损失，并通过分析非参数回归情景和深度神经网络回归的数值实例来证明了其在采样和可信区间方面的优势。

    

    Metropolis-Hastings步骤被广泛应用于基于梯度的马尔可夫链蒙特卡洛方法中的不确定性量化中。通过对批次计算接受概率，随机Metropolis-Hastings步骤节省了计算成本，但降低了有效样本量。我们展示了通过简单的修正项可以避免这个障碍。我们研究了如果在非参数回归设置中应用改进的随机Metropolis-Hastings方法从Gibbs后验分布中采样，则链的结果稳态分布的统计属性。针对深度神经网络回归，我们证明了PAC-Bayes预言不等式，它提供了最优的收缩速率，并分析了结果可信区间的直径和高置信概率。通过在高维参数空间中的数值实例，我们说明了随机Metropolis-Hastings算法的可信区间和收缩速率确实表现出类似的行为。

    A Metropolis-Hastings step is widely used for gradient-based Markov chain Monte Carlo methods in uncertainty quantification. By calculating acceptance probabilities on batches, a stochastic Metropolis-Hastings step saves computational costs, but reduces the effective sample size. We show that this obstacle can be avoided by a simple correction term. We study statistical properties of the resulting stationary distribution of the chain if the corrected stochastic Metropolis-Hastings approach is applied to sample from a Gibbs posterior distribution in a nonparametric regression setting. Focusing on deep neural network regression, we prove a PAC-Bayes oracle inequality which yields optimal contraction rates and we analyze the diameter and show high coverage probability of the resulting credible sets. With a numerical example in a high-dimensional parameter space, we illustrate that credible sets and contraction rates of the stochastic Metropolis-Hastings algorithm indeed behave similar to 
    
[^39]: 智能制造过程中的拓扑数据分析--关于现状的一项调查

    Topological Data Analysis in smart manufacturing processes -- A survey on the state of the art. (arXiv:2310.09319v1 [cs.LG])

    [http://arxiv.org/abs/2310.09319](http://arxiv.org/abs/2310.09319)

    本次调查总结了拓扑数据分析在智能制造和产业4.0背景下的最新进展，突出了其在工业生产和制造领域的关键优势和挑战，并讨论了未充分利用的TDA方法和已识别的应用类型，以推动更多的相关研究。

    

    拓扑数据分析（TDA）是一种使用拓扑学技术对复杂的多维数据进行分析的数学方法，已经在医学、材料科学、生物学等多个领域被广泛而成功地应用。本调查总结了TDA在另一个应用领域中的最新进展：工业制造和产业4.0背景下的生产。我们对工业生产和制造领域中TDA应用进行了严谨可重复的文献搜索。通过对结果进行聚类和分析，基于其在制造过程中的应用领域和输入数据类型进行论述。我们突出了TDA在这一领域的关键优势及其工具，并描述了它的挑战以及未来的潜力。最后，我们讨论了在（特定领域的）工业中未充分利用的TDA方法和已识别的应用类型，旨在促进更多的研究在当前领域中的开展。

    Topological Data Analysis (TDA) is a mathematical method using techniques from topology for the analysis of complex, multi-dimensional data that has been widely and successfully applied in several fields such as medicine, material science, biology, and others. This survey summarizes the state of the art of TDA in yet another application area: industrial manufacturing and production in the context of Industry 4.0. We perform a rigorous and reproducible literature search of applications of TDA on the setting of industrial production and manufacturing. The resulting works are clustered and analyzed based on their application area within the manufacturing process and their input data type. We highlight the key benefits of TDA and their tools in this area and describe its challenges, as well as future potential. Finally, we discuss which TDA methods are underutilized in (the specific area of) industry and the identified types of application, with the goal of prompting more research in this 
    
[^40]: 通过数据和可视化设计探针从用户那里引出模型调整交互

    Eliciting Model Steering Interactions from Users via Data and Visual Design Probes. (arXiv:2310.09314v1 [cs.HC])

    [http://arxiv.org/abs/2310.09314](http://arxiv.org/abs/2310.09314)

    本研究使用数据和可视化设计探针来研究领域专家如何通过语义交互更新分类模型，发现许多语义交互的目标是增强模型的训练数据，而不是直接调整模型参数。

    

    领域专家越来越多地使用自动化数据科学工具将机器学习（ML）模型纳入工作中，但在这些模型错误时很难进行“调试”。对于这些专家来说，语义交互可以提供一种可访问的途径，以引导和完善ML模型，而无需深入其技术细节。在本研究中，我们使用数据和可视化设计探针进行调查研究，以检查具有各种ML专业知识的专家如何使用语义交互来更新简单的分类模型。我们使用设计探针与20个参与者进行交互式对话，并将其交互行为编码为一组目标-交互对。有趣的是，我们的研究结果显示，许多语义交互的目标并不直接映射到ML模型参数，而是旨在增强模型用于训练的数据。我们还确定了参与者对于与ML模型进行交互犹豫的原因，包括...

    Domain experts increasingly use automated data science tools to incorporate machine learning (ML) models in their work but struggle to "debug" these models when they are incorrect. For these experts, semantic interactions can provide an accessible avenue to guide and refine ML models without having to programmatically dive into its technical details. In this research, we conduct an elicitation study using data and visual design probes to examine if and how experts with a spectrum of ML expertise use semantic interactions to update a simple classification model. We use our design probes to facilitate an interactive dialogue with 20 participants and codify their interactions as a set of target-interaction pairs. Interestingly, our findings revealed that many targets of semantic interactions do not directly map to ML model parameters, but instead aim to augment the data a model uses for training. We also identify reasons that participants would hesitate to interact with ML models, includi
    
[^41]: 数字孪生辅助的深度强化学习用于网络切片入场控制的在线优化

    Digital Twin Assisted Deep Reinforcement Learning for Online Optimization of Network Slicing Admission Control. (arXiv:2310.09299v1 [cs.LG])

    [http://arxiv.org/abs/2310.09299](http://arxiv.org/abs/2310.09299)

    这项工作提出了一种数字孪生辅助的深度强化学习解决方案来解决网络切片入场控制中深度强化学习模型初始不稳定性的问题。

    

    5G及以上网络中多样化的网络服务的普及导致了网络切片技术的出现。在其中，入场控制通过选择性接受服务请求来实现特定的优化目标起着关键作用。尽管深度强化学习(DRL)在许多入场控制方法中起着基础和灵活性的作用，但DRL模型的初始不稳定性阻碍了它们在实际网络中的实际部署。在这项工作中，我们提出了一种数字孪生(DT)辅助的DRL解决方案来解决这个问题。具体而言，我们首先将入场决策过程形式化为半马尔可夫决策过程，随后简化为等价的离散时间马尔可夫决策过程，以便实施DRL方法。DT是通过监督学习建立的，并用于辅助DRL模型的训练阶段。广泛的模拟表明，DT作为一种辅助手段可以显著提高DRL的性能和稳定性。

    The proliferation of diverse network services in 5G and beyond networks has led to the emergence of network slicing technologies. Among these, admission control plays a crucial role in achieving specific optimization goals through the selective acceptance of service requests. Although Deep Reinforcement Learning (DRL) forms the foundation in many admission control approaches for its effectiveness and flexibility, the initial instability of DRL models hinders their practical deployment in real-world networks. In this work, we propose a digital twin (DT) assisted DRL solution to address this issue. Specifically, we first formulate the admission decision-making process as a semi-Markov decision process, which is subsequently simplified into an equivalent discrete-time Markov decision process to facilitate the implementation of DRL methods. The DT is established through supervised learning and employed to assist the training phase of the DRL model. Extensive simulations show that the DT-as
    
[^42]: ByteStack-ID: 基于灰度图像的网络入侵检测的集成堆叠模型，利用负载字节频率

    ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection. (arXiv:2310.09298v1 [cs.CR])

    [http://arxiv.org/abs/2310.09298](http://arxiv.org/abs/2310.09298)

    ByteStack-ID是一种基于灰度图像和负载字节频率的集成堆叠模型，用于数据包级入侵检测。它能迅速准确地识别网络流量中的各种攻击类型，并与传统方法有所不同。

    

    在不断发展的网络安全领域中，迅速准确地识别网络流量中的各种攻击类型至关重要。本文介绍了"ByteStack-ID"，一种专为数据包级入侵检测而设计的创新方法。ByteStack-ID核心是利用从负载数据的频率分布生成的灰度图像，这是一种突破性的技术，极大地提高了模型识别复杂数据模式的能力。值得注意的是，我们的方法完全基于数据包级信息，与传统的基于流量数据的网络入侵检测系统（NIDS）有所不同。在基本堆叠方法的基础上，ByteStack-ID与传统的堆叠方法不同。它将附加的元学习器层无缝集成到连接的基础学习器中，创建了一个高度优化的统一模型。

    In the ever-evolving realm of network security, the swift and accurate identification of diverse attack classes within network traffic is of paramount importance. This paper introduces "ByteStack-ID," a pioneering approach tailored for packet-level intrusion detection. At its core, ByteStack-ID leverages grayscale images generated from the frequency distributions of payload data, a groundbreaking technique that greatly enhances the model's ability to discern intricate data patterns. Notably, our approach is exclusively grounded in packet-level information, a departure from conventional Network Intrusion Detection Systems (NIDS) that predominantly rely on flow-based data. While building upon the fundamental concept of stacking methodology, ByteStack-ID diverges from traditional stacking approaches. It seamlessly integrates additional meta learner layers into the concatenated base learners, creating a highly optimized, unified model. Empirical results unequivocally confirm the outstandin
    
[^43]: 理解人工智能认知：受人类记忆机制启发的推理神经模块

    Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms. (arXiv:2310.09297v1 [cs.LG])

    [http://arxiv.org/abs/2310.09297](http://arxiv.org/abs/2310.09297)

    该论文提出了一个受人类记忆机制启发的神经模块，模拟人类和机器如何对当前输入进行关联推理和问答，并将其与过去的记忆结合起来。通过感知、记忆和推理组件，该模块实现了感知更新、记忆融合和信息检索的功能。

    

    人类和机器如何将当前的输入与过去的记忆结合起来，进行关联推理和问答，并将感知到的信息置于上下文中，这是认知科学和人工智能中的一个具有挑战性的谜题。受到人脑记忆系统和认知结构的启发，我们提出了一个包含感知、记忆和推理组件的PMI框架。特别地，记忆模块包括工作记忆和长期记忆，其中后者具有更高阶的结构来保留更多的累积知识和经验。通过可区分的竞争写入访问，当前的感知更新工作记忆，之后通过外积关联与长期记忆融合，避免内存溢出并最小化信息冲突。在推理模块中，相关信息从两个单独的记忆源检索并结合，以获得更全面和精确的解释。

    How humans and machines make sense of current inputs for relation reasoning and question-answering while putting the perceived information into context of our past memories, has been a challenging conundrum in cognitive science and artificial intelligence. Inspired by human brain's memory system and cognitive architectures, we propose a PMI framework that consists of perception, memory and inference components. Notably, the memory module comprises working and long-term memory, with the latter endowed with a higher-order structure to retain more accumulated knowledge and experiences. Through a differentiable competitive write access, current perceptions update working memory, which is later merged with long-term memory via outer product associations, averting memory overflow and minimizing information conflicts. In the inference module, relevant information is retrieved from two separate memory origins and associatively integrated to attain a more comprehensive and precise interpretatio
    
[^44]: 生成熵神经最优传输在空间内外映射中的应用

    Generative Entropic Neural Optimal Transport To Map Within and Across Spaces. (arXiv:2310.09254v1 [stat.ML])

    [http://arxiv.org/abs/2310.09254](http://arxiv.org/abs/2310.09254)

    该论文介绍了生成熵神经最优传输在测度到测度映射中的应用，解决了处理非平方欧氏距离成本、确定性蒙格映射、映射跨不可比较空间和质量守恒约束等实际挑战。

    

    学习测度到测度的映射是机器学习中的一个关键任务，尤其在生成建模中占据重要地位。近年来，受最优传输理论启发的技术不断涌现。结合神经网络模型，这些方法统称为"神经最优传输"，将最优传输作为归纳偏好：这些映射应该针对给定的成本函数是最优的，能以节约的方式（通过最小化位移）在空间内或空间间移动点。这一原则在直观上是合理的，但往往面临几个实际挑战，需要调整最优传输工具箱：处理其他非平方欧氏距离成本的挑战，确定性状况下的蒙格映射公式会限制灵活性，映射在不可比较的空间中会带来多个挑战，最优传输固有的质量守恒约束可能对异常数据给予过多的重视。

    Learning measure-to-measure mappings is a crucial task in machine learning, featured prominently in generative modeling. Recent years have witnessed a surge of techniques that draw inspiration from optimal transport (OT) theory. Combined with neural network models, these methods collectively known as \textit{Neural OT} use optimal transport as an inductive bias: such mappings should be optimal w.r.t. a given cost function, in the sense that they are able to move points in a thrifty way, within (by minimizing displacements) or across spaces (by being isometric). This principle, while intuitive, is often confronted with several practical challenges that require adapting the OT toolbox: cost functions other than the squared-Euclidean cost can be challenging to handle, the deterministic formulation of Monge maps leaves little flexibility, mapping across incomparable spaces raises multiple challenges, while the mass conservation constraint inherent to OT can provide too much credit to outli
    
[^45]: 选择性驱动生产力：增强迁移学习的高效数据集修剪

    Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning. (arXiv:2310.08782v1 [cs.LG])

    [http://arxiv.org/abs/2310.08782](http://arxiv.org/abs/2310.08782)

    本论文提出了一种解决迁移学习中数据集修剪的方法，通过集成数据集修剪和迁移学习的观点，发现现有的方法不适用于迁移学习范式，并提出了标签映射和特征映射这两种新的数据集修剪方法。

    

    大规模数据通常被认为是深度学习应用的必要条件，但同时也会带来巨大的计算和基础设施成本。因此，数据集修剪（DP）作为一种有效的方法出现，通过识别和删除冗余的训练样本来提高数据效率，而不会影响性能。在这项工作中，我们旨在解决迁移学习中的DP问题，即如何在下游目标任务中提高预训练效率和完整微调准确性的同时修剪源数据集。据我们所知，迁移学习的DP问题仍然未解决，因为先前的研究主要将DP和迁移学习视为独立的问题。相反，我们建立了一个统一的视角，将DP与迁移学习相结合，并发现现有的DP方法不适用于迁移学习范式。然后，我们提出了两种新的DP方法，即标签映射和特征映射，用于监督和自监督的预训练设置。

    Massive data is often considered essential for deep learning applications, but it also incurs significant computational and infrastructural costs. Therefore, dataset pruning (DP) has emerged as an effective way to improve data efficiency by identifying and removing redundant training samples without sacrificing performance. In this work, we aim to address the problem of DP for transfer learning, i.e., how to prune a source dataset for improved pretraining efficiency and lossless finetuning accuracy on downstream target tasks. To our best knowledge, the problem of DP for transfer learning remains open, as previous studies have primarily addressed DP and transfer learning as separate problems. By contrast, we establish a unified viewpoint to integrate DP with transfer learning and find that existing DP methods are not suitable for the transfer learning paradigm. We then propose two new DP methods, label mapping and feature mapping, for supervised and self-supervised pretraining settings 
    
[^46]: 自主驾驶车辆交叉路口导航的深度强化学习

    Deep Reinforcement Learning for Autonomous Vehicle Intersection Navigation. (arXiv:2310.08595v1 [cs.RO])

    [http://arxiv.org/abs/2310.08595](http://arxiv.org/abs/2310.08595)

    本研究通过使用基于TD3算法的单智能体方法，在CARLA模拟平台中展示了在复杂T型路口导航中稳定收敛和改进安全性能，并在行程延误、碰撞减少和总体成本等方面优于先前的方法。

    

    本文探讨了在密集交通场景中，自主驾驶车辆（AVs）在复杂T型路口导航中面临的挑战。强化学习算法已经成为一种有希望的方法，可以通过实时地使AVs做出安全高效的决策来应对这些挑战。在本文中，我们使用一种基于双延迟深度确定性策略梯度（TD3）强化学习算法的低成本、单智能体方法来解决在T型路口上的高效安全导航问题。我们展示了当我们在CARLA模拟平台上对我们的TD3方法进行训练和测试时，该方法呈现出稳定的收敛性和改进的安全性能，适用于各种交通密度。我们的结果表明，所提出的方法使AV能够有效地导航T型路口，在行程延误、碰撞减少和总体成本方面优于先前的方法。本研究对强化学习在自主驾驶车辆领域的应用贡献了新的知识。

    In this paper, we explore the challenges associated with navigating complex T-intersections in dense traffic scenarios for autonomous vehicles (AVs). Reinforcement learning algorithms have emerged as a promising approach to address these challenges by enabling AVs to make safe and efficient decisions in real-time. Here, we address the problem of efficiently and safely navigating T-intersections using a lower-cost, single-agent approach based on the Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning algorithm. We show that our TD3-based method, when trained and tested in the CARLA simulation platform, demonstrates stable convergence and improved safety performance in various traffic densities. Our results reveal that the proposed approach enables the AV to effectively navigate T-intersections, outperforming previous methods in terms of travel delays, collision minimization, and overall cost. This study contributes to the growing body of knowledge on reinforceme
    
[^47]: 视觉数据类型理解并非源自扩展视觉语言模型

    Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models. (arXiv:2310.08577v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.08577](http://arxiv.org/abs/2310.08577)

    本文研究了视觉语言模型对视觉数据类型的理解能力，发现虽然在某些样式化数据类型上表现良好，但在基本操作引起的简单数据类型上表现困难。

    

    最近对视觉语言模型（VLMs）的发展取得了显著进展，取得了较好的视觉语义内容识别效果，包括出色的复合图像理解实例。本文介绍了一项新的任务，即视觉数据类型识别，这是一项基本的感知技能，对数据整理（例如从大型数据集中去除噪声数据，领域特定的检索）和自主视觉（例如区分不同的天气变化和相机镜头污染）具有重要意义。我们构建了两个数据集，其中包含经过27种视觉数据类型的动物图像的修改，涵盖了四个广泛的类别。对39个参数范围从100M到80B的VLMs进行了广泛的零样本评估，结果显示了一个细致的性能景观。虽然VLMs在识别某些样式化的数据类型（例如卡通和草图）方面表现良好，但在基本操作（例如图像旋转或添加噪声）引起的简单数据类型上表现出困难。

    Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of Visual Data-Type Identification, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual data-types, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic \textit{data-types}, such as cartoons and sketches, they struggle with simpler data-types arising from basic manipulations like image rotations or additive noise.
    
[^48]: 异构迁移学习综述

    A Survey of Heterogeneous Transfer Learning. (arXiv:2310.08459v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.08459](http://arxiv.org/abs/2310.08459)

    异构迁移学习适用于源领域和目标领域具有不同特征、数据分布和标签空间的情况，通过处理这些差异来增强模型性能。

    

    近年来，迁移学习的应用在很多实际场景中得到了广泛的应用，它利用源领域的知识来增强目标领域模型的性能。其成功的关键在于源领域和目标领域之间的共享知识，这是大多数迁移学习方法的前提条件。然而，这些方法通常假设两个领域具有相同的特征空间和标签空间，即同质迁移学习，但这并不总是现实合理的假设。通常，源领域和目标领域在特征空间、数据分布和标签空间上存在差异，这使得获取具有与目标领域相同特征和标签空间的源领域数据变得具有挑战性或昂贵。对这些差异进行随意的消除并不总是可行或最优的。因此，异构迁移学习作为一种应对这种差异的方法已经崭露头角，并在各种任务中显示出了巨大的潜力。

    The application of transfer learning, an approach utilizing knowledge from a source domain to enhance model performance in a target domain, has seen a tremendous rise in recent years, underpinning many real-world scenarios. The key to its success lies in the shared common knowledge between the domains, a prerequisite in most transfer learning methodologies. These methods typically presuppose identical feature spaces and label spaces in both domains, known as homogeneous transfer learning, which, however, is not always a practical assumption. Oftentimes, the source and target domains vary in feature spaces, data distributions, and label spaces, making it challenging or costly to secure source domain data with identical feature and label spaces as the target domain. Arbitrary elimination of these differences is not always feasible or optimal. Thus, heterogeneous transfer learning, acknowledging and dealing with such disparities, has emerged as a promising approach for a variety of tasks.
    
[^49]: 在二十个查询中破解黑盒大语言模型

    Jailbreaking Black Box Large Language Models in Twenty Queries. (arXiv:2310.08419v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.08419](http://arxiv.org/abs/2310.08419)

    这项研究提出了一个名为PAIR的算法，可以在只能黑盒访问大语言模型的情况下生成破解，无需人工干预。实证表明，PAIR通常只需要少于二十个查询来生成破解，是现有算法的数个数量级更高效。

    

    大语言模型（LLMs）与人类价值的一致性越来越受关注。然而，这类模型的一致性容易受到对抗性破解的影响，从而迫使LLMs超越其安全防护措施。因此，识别这些漏洞对于理解固有弱点并防止未来的不当使用至关重要。为此，我们提出了Prompt Automatic Iterative Refinement（PAIR），这是一种仅通过对LLM进行黑盒访问的算法生成语义破解。PAIR受到社会工程攻击的启发，使用攻击者LLM自动生成针对目标LLM的破解，无需人工干预。攻击者LLM通过迭代查询目标LLM来更新和改进候选破解。在实证上，PAIR通常只需要少于二十个查询来生成破解，这比现有算法高效数个数量级。PAIR还实现了有竞争力的破解效果。

    There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbrea
    
[^50]: 基于思维链的Transformer的表达能力

    The Expresssive Power of Transformers with Chain of Thought. (arXiv:2310.07923v1 [cs.LG])

    [http://arxiv.org/abs/2310.07923](http://arxiv.org/abs/2310.07923)

    本论文研究基于思维链的Transformer的表达能力，通过允许使用中间生成的方式提高了Transformer的推理能力，并发现线性数量的解码步骤在标准计算复杂度下增加了明显的新能力。

    

    最近的理论研究发现了一些出人意料地简单的推理问题，例如检查图中是否存在连接的两个节点，或模拟有限状态机，这些问题被证明无法由立即读取输入后回答的标准Transformer解决。然而，在实践中，通过允许Transformer使用“思维链”或“草稿纸”，即在回答之前生成并依赖一系列中间token，可以改善其推理能力。基于此，我们问：这种中间生成是否从根本上扩展了仅有解码器的Transformer的计算能力？我们表明答案是肯定的，但增加的程度关键取决于中间生成的数量。例如，我们发现相对于输入长度来说，具有对数级解码步骤的Transformer解码器仅略微推动了标准Transformer的极限，而线性数量的解码步骤则增加了明显的新能力（在标准计算复杂度下）。

    Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a "chain of thought" or "scratchpad", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard compl
    
[^51]: 在语言模型中衡量特征稀疏性

    Measuring Feature Sparsity in Language Models. (arXiv:2310.07837v1 [cs.LG])

    [http://arxiv.org/abs/2310.07837](http://arxiv.org/abs/2310.07837)

    这项研究开发了度量方法来评估语言模型中特征稀疏性的成功，并测试了线性性和稀疏性假设的有效性。研究结果表明，语言模型的激活可以准确地建模为特征的稀疏线性组合，并且在第一层和最后一层中呈最稀疏状态。

    

    最近的研究提出，语言模型中的激活可以被建模为与输入文本特征对应的向量的稀疏线性组合。在这个假设下，这些研究旨在使用稀疏编码重构特征方向。我们开发了度量方法来评估这些稀疏编码技术的成功，并测试线性性和稀疏性假设的有效性。我们展示了我们的度量方法可以预测合成稀疏线性激活的稀疏程度，并能够区分稀疏线性数据和其他几种分布。我们使用我们的度量方法来衡量几个语言模型的稀疏程度。我们发现，与对照数据集相比，语言模型的激活可以准确地建模为特征的稀疏线性组合，并且在第一层和最后一层中呈最稀疏状态。

    Recent works have proposed that activations in language models can be modelled as sparse linear combinations of vectors corresponding to features of input text. Under this assumption, these works aimed to reconstruct feature directions using sparse coding. We develop metrics to assess the success of these sparse coding techniques and test the validity of the linearity and sparsity assumptions. We show our metrics can predict the level of sparsity on synthetic sparse linear activations, and can distinguish between sparse linear data and several other distributions. We use our metrics to measure levels of sparsity in several language models. We find evidence that language model activations can be accurately modelled by sparse linear combinations of features, significantly more so than control datasets. We also show that model activations appear to be sparsest in the first and final layers.
    
[^52]: 非稳态环境下基于神经预测集成抽样的情境赌博学习

    Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling. (arXiv:2310.07786v1 [cs.LG])

    [http://arxiv.org/abs/2310.07786](http://arxiv.org/abs/2310.07786)

    本文介绍了一种新颖的非稳态情境赌博算法，通过将可扩展的基于深度神经网络的架构与精心设计的探索机制相结合，在非稳态环境中优先收集持久价值信息，从而显著提高了性能。

    

    实际世界中的情境赌博应用常常因季节性、偶然性和不断变化的社交趋势而呈非稳态。尽管文献中已提出了许多非稳态情境赌博学习算法，但由于缺乏对持久价值信息的优先考虑，这些算法在探索时过度，或者设计方式难以在具有高维用户特定特征和大规模动作集的现代应用中扩展，或者两者都有。在本文中，我们介绍了一种新颖的非稳态情境赌博算法，它解决了这些问题。它将可扩展的基于深度神经网络的架构与一个精心设计的探索机制相结合，在非稳态环境中战略性地优先收集具有最持久价值的信息。通过在展示明显非稳态的两个实际推荐数据集上进行实证评估，我们证明了我们的方法显著胜过现有的算法。

    Real-world applications of contextual bandits often exhibit non-stationarity due to seasonality, serendipity, and evolving social trends. While a number of non-stationary contextual bandit learning algorithms have been proposed in the literature, they excessively explore due to a lack of prioritization for information of enduring value, or are designed in ways that do not scale in modern applications with high-dimensional user-specific features and large action set, or both. In this paper, we introduce a novel non-stationary contextual bandit algorithm that addresses these concerns. It combines a scalable, deep-neural-network-based architecture with a carefully designed exploration mechanism that strategically prioritizes collecting information with the most lasting value in a non-stationary environment. Through empirical evaluations on two real-world recommendation datasets, which exhibit pronounced non-stationarity, we demonstrate that our approach significantly outperforms the state
    
[^53]: 可解释的图像相似性:整合Siamese网络和Grad-CAM

    Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM. (arXiv:2310.07678v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.07678](http://arxiv.org/abs/2310.07678)

    本文提出了可解释的图像相似性的方法，整合了Siamese网络和Grad-CAM，能够提供相似性分数以及视觉上的事实和反事实解释，有潜力提高可解释性和可信度。

    

    随着图像应用在各个领域的普及，准确且可解释的图像相似性度量的需求变得越来越关键。现有的图像相似性模型常常缺乏透明度，难以理解为何认为两个图像相似。在本文中，我们提出了可解释的图像相似性的概念，目标是开发一种方法，能够提供相似性分数以及视觉上的事实和反事实解释。在这方面，我们提出了一个新的框架，将Siamese网络和Grad-CAM整合起来，用于提供可解释的图像相似性，并讨论了采用这种方法的潜在利益和挑战。此外，我们对所提出框架提供的事实和反事实解释进行了全面的讨论，以辅助决策。所提出的方法有潜力提高可解释性和可信度。

    With the proliferation of image-based applications in various domains, the need for accurate and interpretable image similarity measures has become increasingly critical. Existing image similarity models often lack transparency, making it challenging to understand the reasons why two images are considered similar. In this paper, we propose the concept of explainable image similarity, where the goal is the development of an approach, which is capable of providing similarity scores along with visual factual and counterfactual explanations. Along this line, we present a new framework, which integrates Siamese Networks and Grad-CAM for providing explainable image similarity and discuss the potential benefits and challenges of adopting this approach. In addition, we provide a comprehensive discussion about factual and counterfactual explanations provided by the proposed framework for assisting decision making. The proposed approach has the potential to enhance the interpretability, trustwor
    
[^54]: XAI方法的以人为中心的评估

    Human-Centered Evaluation of XAI Methods. (arXiv:2310.07534v1 [cs.AI])

    [http://arxiv.org/abs/2310.07534](http://arxiv.org/abs/2310.07534)

    在人工智能领域中，解释深度学习黑盒子的决策过程是一个关键挑战。本研究以用户为中心，客观评估了三种领先的解释方法的可解释性，并发现它们都提供了可解释的结果。

    

    在不断发展的人工智能领域中，一个关键的挑战是解析深度学习中所谓的“黑盒子”中的决策过程。近年来，出现了许多方法，专门用于解释各种任务的决策。特别是在图像分类等任务中，这些方法通常会识别并强调对分类器预测影响最大的关键像素。有趣的是，这种方法与人类行为相似：当我们被要求解释分类图像的理由时，我们通常会指出最显著的特征或方面。利用这种类似性，我们的研究进行了以用户为中心的研究。我们试图客观地评估三种领先的解释方法的可解释性：（1）典型局部网络、（2）遮挡和（3）层次相关传播。有趣的是，我们的结果表明，尽管这些方法所突出的区域可能差异很大，但它们都提供了可解释的结果。

    In the ever-evolving field of Artificial Intelligence, a critical challenge has been to decipher the decision-making processes within the so-called "black boxes" in deep learning. Over recent years, a plethora of methods have emerged, dedicated to explaining decisions across diverse tasks. Particularly in tasks like image classification, these methods typically identify and emphasize the pivotal pixels that most influence a classifier's prediction. Interestingly, this approach mirrors human behavior: when asked to explain our rationale for classifying an image, we often point to the most salient features or aspects. Capitalizing on this parallel, our research embarked on a user-centric study. We sought to objectively measure the interpretability of three leading explanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3) Layer-wise Relevance Propagation. Intriguingly, our results highlight that while the regions spotlighted by these methods can vary widely, they all offe
    
[^55]: 一种基于分支的深度卷积网络用于使用具有不同特征空间尺度的气象图预测巴黎雾霾的发生

    A Branched Deep Convolutional Network for Forecasting the Occurrence of Hazes in Paris using Meteorological Maps with Different Characteristic Spatial Scales. (arXiv:2310.07437v1 [physics.ao-ph])

    [http://arxiv.org/abs/2310.07437](http://arxiv.org/abs/2310.07437)

    该论文开发了一种基于分支的深度卷积网络，用于预测巴黎雾霾的发生。该网络利用气象图的不同特征空间尺度来训练，通过使用多年期的气象变量和地面能见度观测数据进行训练和验证。这些新架构提高了网络性能，在未使用训练数据的情况下进行盲预测，具有合理的预测分数。

    

    开发了一个深度学习平台，用于预测低能见度事件或雾霾的发生。它通过使用多年期的各种气象和水文变量的区域日常图作为输入特征，并将地面能见度观测作为目标来进行训练。为了更好地保留不同输入特征的特征空间信息进行训练，最近开发了两种针对巴黎雾霾的分支架构。这些新架构提高了网络的性能，在验证和盲预测评估中产生了合理的分数，使用了2021年和2022年的数据，这些数据没有在训练和验证中使用。

    A deep learning platform has been developed to forecast the occurrence of the low visibility events or hazes. It is trained by using multi-decadal daily regional maps of various meteorological and hydrological variables as input features and surface visibility observations as the targets. To better preserve the characteristic spatial information of different input features for training, two branched architectures have recently been developed for the case of Paris hazes. These new architectures have improved the performance of the network, producing reasonable scores in both validation and a blind forecasting evaluation using the data of 2021 and 2022 that have not been used in the training and validation.
    
[^56]: 直接逻辑属性的对抗性样本：gelu-4l中的内存管理

    An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l. (arXiv:2310.07325v1 [cs.LG])

    [http://arxiv.org/abs/2310.07325](http://arxiv.org/abs/2310.07325)

    在gelu-4l中，我们提供了证据表明内存管理对于transformer模型至关重要，并说明了Direct Logit Attribution技术的不准确之处。

    

    我们提供了一个4层transformer中内存管理的具体证据。具体来说，我们发现在前向传播过程中，模型组件一致地移除前面组件的输出，这是一种清理行为。我们的研究结果表明，解释性技术Direct Logit Attribution提供了误导性结果。我们展示了明确的例子，证明这种技术是不准确的，因为它没有考虑到清理行为。

    We provide concrete evidence for memory management in a 4-layer transformer. Specifically, we identify clean-up behavior, in which model components consistently remove the output of preceeding components during a forward pass. Our findings suggest that the interpretability technique Direct Logit Attribution provides misleading results. We show explicit examples where this technique is inaccurate, as it does not account for clean-up behavior.
    
[^57]: 在BraTS数据集中使用生成对抗网络从已有的模态生成缺失的MRI序列

    Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset. (arXiv:2310.07250v1 [q-bio.QM])

    [http://arxiv.org/abs/2310.07250](http://arxiv.org/abs/2310.07250)

    本文提出了一种利用生成对抗网络从已有的模态生成缺失的MRI序列的方法，在BraTS数据集上取得了有希望的结果。

    

    高度侵袭性和致命的胶质母细胞瘤是一种常见的脑癌。磁共振成像（MRI）由于其无创和无辐射性质，在胶质母细胞瘤患者的诊断、治疗计划和随访中发挥着重要作用。国际脑肿瘤分割（BraTS）挑战为利用四种结构性MRI扫描（T1、T1Gd、T2、T2-FLAIR）准确高效地分割胶质母细胞瘤亚区域提供了许多人工智能算法。然而，这四个MRI序列不总是可用的。为解决这个问题，可以利用生成对抗网络（GANs）合成缺失的MRI序列。在本文中，我们实现并利用开源的GAN方法，以任三个MRI序列作为输入生成缺失的第四个结构序列。我们的方法贡献给了社区驱动的通用深度学习框架（GaNDLF），并在合成缺失的MRI序列方面取得了有希望的结果。

    Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic resonance imaging (MRI) plays a significant role in the diagnosis, treatment planning, and follow-up of glioblastoma patients due to its non-invasive and radiation-free nature. The International Brain Tumor Segmentation (BraTS) challenge has contributed to generating numerous AI algorithms to accurately and efficiently segment glioblastoma sub-compartments using four structural (T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not always be available. To address this issue, Generative Adversarial Networks (GANs) can be used to synthesize the missing MRI sequences. In this paper, we implement and utilize an open-source GAN approach that takes any three MRI sequences as input to generate the missing fourth structural sequence. Our proposed approach is contributed to the community-driven generally nuanced deep learning framework (GaNDLF) and demonstrates promising results in synthesizing 
    
[^58]: 用于人类反馈的非策略评估

    Off-Policy Evaluation for Human Feedback. (arXiv:2310.07123v1 [cs.LG])

    [http://arxiv.org/abs/2310.07123](http://arxiv.org/abs/2310.07123)

    本论文介绍了一个用于人类反馈的非策略评估（OPEHF）框架，可以准确评估人类反馈信号。这个框架解决了现有OPE方法在估计人类反馈信号上的不足。

    

    非策略评估（OPE）对于强化学习（RL）的离线训练和评估之间的差距的缩小非常重要，它通过仅使用离线轨迹估计目标（评估）策略的性能和/或排名。它可以提高数据收集和策略测试过程的安全性和效率，在在线部署成本较高的情况下，如医疗保健中。然而，现有的OPE方法在估计人类反馈（HF）信号方面存在不足，因为HF可能会受到多个潜在因素的影响，而且只是稀疏可用的；而不同于代理定义的环境奖励（用于策略优化），环境奖励通常是在参数函数或分布上决定的。因此，由于HF信号的性质，准确地推断OPE估计是具有挑战性的。为了解决这个问题，我们引入了一个用于HF的OPE框架，它重新使用现有的OPE方法，以准确评估HF信号。具体而言，我们开发了一个方法来

    Off-policy evaluation (OPE) is important for closing the gap between offline training and evaluation of reinforcement learning (RL), by estimating performance and/or rank of target (evaluation) policies using offline trajectories only. It can improve the safety and efficiency of data collection and policy testing procedures in situations where online deployments are expensive, such as healthcare. However, existing OPE methods fall short in estimating human feedback (HF) signals, as HF may be conditioned over multiple underlying factors and is only sparsely available; as opposed to the agent-defined environmental rewards (used in policy optimization), which are usually determined over parametric functions or distributions. Consequently, the nature of HF signals makes extrapolating accurate OPE estimations to be challenging. To resolve this, we introduce an OPE for HF (OPEHF) framework that revives existing OPE methods in order to accurately evaluate the HF signals. Specifically, we deve
    
[^59]: 可扩展的语义非马尔可夫仿真代理用于强化学习

    Scalable Semantic Non-Markovian Simulation Proxy for Reinforcement Learning. (arXiv:2310.06835v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.06835](http://arxiv.org/abs/2310.06835)

    本文提出了一种基于语义的仿真代理方法，通过引入时间扩展和注释逻辑，解决了强化学习中可扩展性、可解释性和马尔可夫假设等问题。与高保真度的模拟器相比，该方法在加速训练过程三个数量级的同时保持了策略质量，并且能够建模和利用非马尔可夫动力学和即时动作，并提供了可解释的代理行为结果痕迹。

    

    最近强化学习（RL）的进展在各种应用领域展现出了很多潜力。然而，可扩展性、可解释性和马尔可夫假设等问题限制了其在某些领域的适用性。我们观察到这些缺点大多来自于模拟器而不是RL训练算法本身。因此，我们提出了一种基于标注逻辑的时间扩展的语义代理来进行仿真。与两个高保真度的模拟器相比，我们展示了三个数量级的加速，并且保持了学习的策略质量。此外，我们展示了能够建模和利用非马尔可夫动力学和即时动作，并提供了能够解释代理行为结果的痕迹。

    Recent advances in reinforcement learning (RL) have shown much promise across a variety of applications. However, issues such as scalability, explainability, and Markovian assumptions limit its applicability in certain domains. We observe that many of these shortcomings emanate from the simulator as opposed to the RL training algorithms themselves. As such, we propose a semantic proxy for simulation based on a temporal extension to annotated logic. In comparison with two high-fidelity simulators, we show up to three orders of magnitude speed-up while preserving the quality of policy learned. In addition, we show the ability to model and leverage non-Markovian dynamics and instantaneous actions while providing an explainable trace describing the outcomes of the agent actions.
    
[^60]: 自监督数据集蒸馏用于迁移学习

    Self-Supervised Dataset Distillation for Transfer Learning. (arXiv:2310.06511v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.06511](http://arxiv.org/abs/2310.06511)

    本文提出了一种自监督数据集蒸馏方法，用于将无标签数据集转化为小型合成样本，以支持高效的自监督学习。通过最小化模型对合成样本的表示和可学习目标特征表示之间的均方误差，解决了合成样本梯度偏差的问题。

    

    数据集蒸馏方法在将大型数据集转化为少量具有代表性的样本方面取得了显著成功。然而，它们并不被设计用于产生一个适用于促进自监督预训练的蒸馏数据集。为此，我们提出了一种将无标签数据集蒸馏为一组小型合成样本以用于高效的自监督学习（SSL）的新问题。我们首先证明了在朴素双层优化中，合成样本相对于自监督目标的梯度是“有偏”的，这是由于数据增强或遮蔽引起的随机性。为了解决这个问题，我们提出了最小化模型对合成样本的表示和相应的可学习目标特征表示之间的均方误差（MSE）作为内部目标，这不引入任何随机性。我们的主要动机是通过提出的内部优化获得的模型可以模仿...

    Dataset distillation methods have achieved remarkable success in distilling a large dataset into a small set of representative samples. However, they are not designed to produce a distilled dataset that can be effectively used for facilitating self-supervised pre-training. To this end, we propose a novel problem of distilling an unlabeled dataset into a set of small synthetic samples for efficient self-supervised learning (SSL). We first prove that a gradient of synthetic samples with respect to a SSL objective in naive bilevel optimization is \textit{biased} due to the randomness originating from data augmentations or masking. To address this issue, we propose to minimize the mean squared error (MSE) between a model's representations of the synthetic examples and their corresponding learnable target feature representations for the inner objective, which does not introduce any randomness. Our primary motivation is that the model obtained by the proposed inner optimization can mimic the
    
[^61]: 用于修正分类模型的规则挖掘

    Rule Mining for Correcting Classification Models. (arXiv:2310.06446v1 [cs.SE])

    [http://arxiv.org/abs/2310.06446](http://arxiv.org/abs/2310.06446)

    本研究提出了一种用于修正分类模型的规则挖掘方法，通过挖掘不准确子集和对其进行修正的规则列表，以提高模型的预测准确性。

    

    机器学习模型需要不断更新或修正，以确保预测准确性始终保持高水平。本研究考虑了模型修正可能改变预测结果的场景，例如模型是复杂系统或软件的一部分。在这种场景中，开发人员希望能够控制修正的规范。为了实现这一点，开发人员需要了解哪些输入的子集会导致模型的预测不准确。因此，我们提出了修正规则挖掘方法，以获取描述不准确子集和如何进行修正的全面规则列表。我们还开发了一种高效的修正规则挖掘算法，该算法结合了频繁项集挖掘和独特的修正规则修剪技术。我们观察到，该算法找到了各种规则，有助于收集不充分学习的数据，直接修正模型输出。

    Machine learning models need to be continually updated or corrected to ensure that the prediction accuracy remains consistently high. In this study, we consider scenarios where developers should be careful to change the prediction results by the model correction, such as when the model is part of a complex system or software. In such scenarios, the developers want to control the specification of the corrections. To achieve this, the developers need to understand which subpopulations of the inputs get inaccurate predictions by the model. Therefore, we propose correction rule mining to acquire a comprehensive list of rules that describe inaccurate subpopulations and how to correct them. We also develop an efficient correction rule mining algorithm that is a combination of frequent itemset mining and a unique pruning technique for correction rules. We observed that the proposed algorithm found various rules which help to collect data insufficiently learned, directly correct model outputs,
    
[^62]: DiffuSeq-v2：将离散和连续文本空间连接起来以加速Seq2Seq扩散模型

    DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models. (arXiv:2310.05793v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05793](http://arxiv.org/abs/2310.05793)

    本文提出了DiffuSeq-v2模型，通过将离散和连续文本空间连接起来实现了Seq2Seq扩散模型的加速。在训练中引入软吸收态，提高离散突变重构能力；在采样阶段使用ODE求解器加快采样速度。实验结果表明，训练收敛速度提高4倍，生成样本速度提高800倍，更适用于实际应用。

    

    扩散模型在生成高质量的文本序列方面是很有潜力的。然而，目前的方法主要是利用连续的扩散空间表示离散文本，在训练过程中产生了大量的计算开销，导致采样速度变慢。本文引入了一个软吸收态，帮助扩散模型学习基于底层高斯空间的离散突变重构，从而增强其恢复条件信号的能力。在采样阶段，我们使用了最先进的连续空间ODE求解器来加快采样过程。广泛的实验评估表明，我们提出的方法有效地将训练收敛速度提高了4倍，并以800倍的速度生成相近质量的样本，使其更接近实际应用。

    Diffusion models have gained prominence in generating high-quality sequences of text. Nevertheless, current approaches predominantly represent discrete text within a continuous diffusion space, which incurs substantial computational overhead during training and results in slower sampling speeds. In this paper, we introduce a soft absorbing state that facilitates the diffusion model in learning to reconstruct discrete mutations based on the underlying Gaussian space, thereby enhancing its capacity to recover conditional signals. During the sampling phase, we employ state-of-the-art ODE solvers within the continuous space to expedite the sampling process. Comprehensive experimental evaluations reveal that our proposed method effectively accelerates the training convergence by 4x and generates samples of similar quality 800x faster, rendering it significantly closer to practical application. \footnote{The code is released at \url{https://github.com/Shark-NLP/DiffuSeq}
    
[^63]: 使用最优输运器合并Transformer

    Transformer Fusion with Optimal Transport. (arXiv:2310.05719v1 [cs.LG])

    [http://arxiv.org/abs/2310.05719](http://arxiv.org/abs/2310.05719)

    本文介绍了一种使用最优输运来融合基于Transformer的网络的方法，可以对齐各种架构组件并允许不同大小的模型的融合，提供了一种新的高效压缩Transformer的方式。

    

    融合是一种将多个独立训练的神经网络合并以结合它们的能力的技术。过去的尝试仅限于全连接、卷积和残差网络的情况。本文提出了一种系统的方法，利用最优输运来融合两个或多个基于Transformer的网络，以（软）对齐各种架构组件。我们详细描述了一种层对齐的抽象方法，可以推广到任意架构，例如多头自注意力、层归一化和残差连接。我们通过各种消融研究讨论了如何处理这些架构组件。此外，我们的方法允许不同大小的模型进行融合（异构融合），为Transformer的压缩提供了一种新的高效方法。我们通过Vision Transformer进行图像分类任务以及自然语言

    Fusion is a technique for merging multiple independently-trained neural networks in order to combine their capabilities. Past attempts have been restricted to the case of fully-connected, convolutional, and residual networks. In this paper, we present a systematic approach for fusing two or more transformer-based networks exploiting Optimal Transport to (soft-)align the various architectural components. We flesh out an abstraction for layer alignment, that can generalize to arbitrary architectures -- in principle -and we apply this to the key ingredients of Transformers such as multi-head self-attention, layer-normalization, and residual connections, and we discuss how to handle them via various ablation studies. Furthermore, our method allows the fusion of models of different sizes (heterogeneous fusion), providing a new and efficient way for compression of Transformers. The proposed approach is evaluated on both image classification tasks via Vision Transformer and natural language
    
[^64]: RetSeg: 基于保留机制的结直肠息肉分割网络

    RetSeg: Retention-based Colorectal Polyps Segmentation Network. (arXiv:2310.05446v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2310.05446](http://arxiv.org/abs/2310.05446)

    本研究探索将保留机制整合到结直肠息肉分割中，解决了视觉变换器在资源受限设备上实时疾病检测中的内存和并行性挑战。

    

    视觉变换器（ViTs）在医学图像分析方面取得了重大突破，与传统的卷积神经网络（CNNs）相比，在息肉分类、检测和分割等关键任务中展现出了更高的效能。通过利用注意机制聚焦于特定图像区域，ViTs在处理视觉数据时表现出上下文感知能力，从而在处理复杂医学图像时实现了强大且精确的预测。此外，变换器中固有的自注意机制适应了不同的输入尺寸和分辨率，为传统的CNNs所不具备的提供了前所未有的灵活性。然而，变换器由于自注意机制而面临着过多的内存使用和有限的训练并行性等挑战，从而使其在资源受限设备上实时疾病检测变得不切实际。在本研究中，我们通过探究将最近引入的保留机制整合到息肉分割中，来解决这些难题。

    Vision Transformers (ViTs) have revolutionized medical imaging analysis, showcasing superior efficacy compared to conventional Convolutional Neural Networks (CNNs) in vital tasks such as polyp classification, detection, and segmentation. Leveraging attention mechanisms to focus on specific image regions, ViTs exhibit contextual awareness in processing visual data, culminating in robust and precise predictions, even for intricate medical images. Moreover, the inherent self-attention mechanism in Transformers accommodates varying input sizes and resolutions, granting an unprecedented flexibility absent in traditional CNNs. However, Transformers grapple with challenges like excessive memory usage and limited training parallelism due to self-attention, rendering them impractical for real-time disease detection on resource-constrained devices. In this study, we address these hurdles by investigating the integration of the recently introduced retention mechanism into polyp segmentation, intr
    
[^65]: 通过有效利用文本数据合成提高端到端语音处理的效率

    Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis. (arXiv:2310.05374v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05374](http://arxiv.org/abs/2310.05374)

    本论文提出了一种名为LaSyn的文本数据利用框架，通过将文本数据转换为中间潜变表示来增强端到端语音处理模型的训练。在低资源环境下的语音识别和口语理解任务中，LaSyn相对词错误率减少了22.3%，绝对意图分类准确率提高了4.1%。

    

    在数据中心的人工智能时代，培训高性能的端到端语音处理模型需要大量标记的语音数据。然而，与文本数据相比，标记的语音数据通常更加稀缺和昂贵。我们提出了一种名为LaSyn的有效的文本数据利用框架，用于端到端语音处理模型。我们训练一个潜变合成器将文本数据转换为预训练语音模型的中间潜变表示。这些伪声学表示用于增强模型训练的声学数据。我们在低资源的自动语音识别（ASR）和口语理解（SLU）任务上评估了LaSyn。对于ASR，LaSyn改进了在LibriSpeech train-clean-100上训练的E2E基线，在不同的测试集上相对词错误率减少了22.3%。对于SLU，LaSyn改进了我们的E2E基线，绝对意图分类准确率提高了4.1%。

    Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accurac
    
[^66]: 大规模类别下的广义神经崩溃

    Generalized Neural Collapse for a Large Number of Classes. (arXiv:2310.05351v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05351](http://arxiv.org/abs/2310.05351)

    本论文将神经崩溃概念扩展到类别数远大于特征空间维度的情况，并展示了广义神经崩溃现象的最小边界值被最大化。

    

    神经崩溃提供了深度分类模型中学习的最后一层表示（即特征）和分类器权重的优雅数学描述。这种结果不仅提供了洞察力，还激发了改进实际深度模型的新技术。然而，大多数关于神经崩溃的现有经验和理论研究都集中于类别数相对于特征空间维度较小的情况。本文将神经崩溃扩展到类别数远大于特征空间维度的情况，这在语言模型、检索系统和人脸识别应用中广泛出现。我们展示了特征和分类器展现出了广义神经崩溃现象，其中最小的一对其他类别间边界值被最大化。我们进行了实证研究以验证实际深度神经网络中广义神经崩溃的发生。此外，我们提供了理论研究，以表明….

    Neural collapse provides an elegant mathematical characterization of learned last layer representations (a.k.a. features) and classifier weights in deep classification models. Such results not only provide insights but also motivate new techniques for improving practical deep models. However, most of the existing empirical and theoretical studies in neural collapse focus on the case that the number of classes is small relative to the dimension of the feature space. This paper extends neural collapse to cases where the number of classes are much larger than the dimension of feature space, which broadly occur for language models, retrieval systems, and face recognition applications. We show that the features and classifier exhibit a generalized neural collapse phenomenon, where the minimum one-vs-rest margins is maximized.We provide empirical study to verify the occurrence of generalized neural collapse in practical deep neural networks. Moreover, we provide theoretical study to show tha
    
[^67]: 联邦学习：最新进展和应用的前沿综述

    Federated Learning: A Cutting-Edge Survey of the Latest Advancements and Applications. (arXiv:2310.05269v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05269](http://arxiv.org/abs/2310.05269)

    联邦学习是一种安全分布式机器学习方法，通过整合云基础设施和区块链技术，实现了隐私安全和经济有效的通信。它通过本地训练模型并将结果上传到云端进行整体模型聚合，避免了直接暴露原始数据，具有高效、可伸缩和保护隐私的优势。

    

    在具有客户和主机连接的机器学习（ML）系统领域，通过联邦学习（FL）作为安全分布式ML方法，可以有效地提高隐私安全性。FL通过使用区块链技术将ML模型传输到边缘服务器上，有效地将云基础设施与中心化和分散化系统的流程处理和数据存储需求相结合，注重可伸缩性、隐私考虑和经济高效的通信。在当前的FL实现中，数据所有者在本地训练模型，然后以权重、梯度和参数的形式将结果上传到云端进行整体模型聚合。这种创新消除了与物联网（IoT）客户和参与者直接与云中心通信原始和潜在机密数据的必要性。这不仅降低了与与传统云中心通信相关的费用，还保护了用户的隐私。

    In the realm of machine learning (ML) systems featuring client-host connections, the enhancement of privacy security can be effectively achieved through federated learning (FL) as a secure distributed ML methodology. FL effectively integrates cloud infrastructure to transfer ML models onto edge servers using blockchain technology. Through this mechanism, it guarantees the streamlined processing and data storage requirements of both centralized and decentralized systems, with an emphasis on scalability, privacy considerations, and cost-effective communication. In current FL implementations, data owners locally train their models, and subsequently upload the outcomes in the form of weights, gradients, and parameters to the cloud for overall model aggregation. This innovation obviates the necessity of engaging Internet of Things (IoT) clients and participants to communicate raw and potentially confidential data directly with a cloud center. This not only reduces the costs associated with 
    
[^68]: 实例和标签: 针对层次化多标签文本分类的层次感知联合监督对比学习

    Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification. (arXiv:2310.05128v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05128](http://arxiv.org/abs/2310.05128)

    提出了一种层次感知联合监督对比学习方法（HJCL），用于层次化多标签文本分类。该方法通过使用实例级和标签级对比学习技术，以及精心构建批次来处理标签层次结构，解决了在HMTC中使用监督对比学习的问题。

    

    层次化多标签文本分类（HMTC）旨在利用标签层次结构进行多标签分类。近期关于HMTC的方法采用对比学习在生成的样本上以半监督的方式将文本和标签嵌入接近，从而解决了对输出空间施加过度约束的问题。然而，样本的生成往往引入噪声，因为它忽略了同一批次中相似样本之间的相关性。解决这个问题的一个方法是使用监督对比学习，但由于其复杂的结构化标签，这仍然是一个未被充分研究的领域。为了克服这一挑战，我们提出了一种称为$\textbf{HJCL}$的层次感知联合监督对比学习方法，用于填补监督对比学习和HMTC之间的差距。具体而言，我们采用实例级和标签级对比学习技术，并仔细构造批次来满足标签层次结构的要求。

    Hierarchical multi-label text classification (HMTC) aims at utilizing a label hierarchy in multi-label classification. Recent approaches to HMTC deal with the problem of imposing an over-constrained premise on the output space by using contrastive learning on generated samples in a semi-supervised manner to bring text and label embeddings closer. However, the generation of samples tends to introduce noise as it ignores the correlation between similar samples in the same batch. One solution to this issue is supervised contrastive learning, but it remains an underexplored topic in HMTC due to its complex structured labels. To overcome this challenge, we propose $\textbf{HJCL}$, a $\textbf{H}$ierarchy-aware $\textbf{J}$oint Supervised $\textbf{C}$ontrastive $\textbf{L}$earning method that bridges the gap between supervised contrastive learning and HMTC. Specifically, we employ both instance-wise and label-wise contrastive learning techniques and carefully construct batches to fulfill the 
    
[^69]: EMOFM: 带有基于特征的混合器的集成MLP模型用于点击率预测

    EMOFM: Ensemble MLP mOdel with Feature-based Mixers for Click-Through Rate Prediction. (arXiv:2310.04482v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04482](http://arxiv.org/abs/2310.04482)

    EMOFM是一个集成MLP模型，通过使用基于特征的混合器实现了字段和类型特征的融合，从而提升了点击率预测的性能。

    

    轨道一的CTI竞赛关注的是点击率（CTR）预测。数据集包含数百万条记录，每条记录中的每个字段特征都由哈希整数组成，以保护隐私。对于这个任务，网络方法的关键可能是按类型提取特征和跨不同字段进行信息融合。多层感知器（MLP）能够提取字段特征，但无法有效地融合特征。受到交叉注意力的自然融合特性和基于Transformer结构的高效性的启发，我们提出了简单的插件混合器用于字段/类型特征融合，并构建了一个字段和类型混合的集成模型，即EMOFM（带有基于特征的混合器的集成MLP模型）。在实验中，我们对该模型在数据集上进行了评估，可视化了优化过程并进行了消融研究。结果显示，EMOFM优于比较基线。最后，我们讨论了未来的工作。

    Track one of CTI competition is on click-through rate (CTR) prediction. The dataset contains millions of records and each field-wise feature in a record consists of hashed integers for privacy. For this task, the keys of network-based methods might be type-wise feature extraction and information fusion across different fields. Multi-layer perceptrons (MLPs) are able to extract field feature, but could not efficiently fuse features. Motivated by the natural fusion characteristic of cross attention and the efficiency of transformer-based structures, we propose simple plug-in mixers for field/type-wise feature fusion, and thus construct an field&type-wise ensemble model, namely EMOFM (Ensemble MLP mOdel with Feature-based Mixers). In the experiments, the proposed model is evaluated on the dataset, the optimization process is visualized and ablation studies are explored. It is shown that EMOFM outperforms compared baselines. In the end, we discuss on future work. WARNING: The comparison mi
    
[^70]: C(NN)FD -- 多级轴向压缩机气动性能中尖间隙变化的深度学习预测

    C(NN)FD -- deep learning predictions of tip clearance variations on multi-stage axial compressors aerodynamic performance. (arXiv:2310.04264v1 [cs.LG])

    [http://arxiv.org/abs/2310.04264](http://arxiv.org/abs/2310.04264)

    本文展示了一种用于实时预测多级轴向压缩机在燃气轮机中尖间隙变化对气动性能影响的深度学习框架，可与CFD基准相媲美的实时准确性，方便集成到燃气轮机的制造和构建过程中进行性能评估。

    

    迄今为止，将深度学习方法应用于诸如CFD（计算流体力学）等物理模拟在工业上的重要性有限。本文展示了一种用于多级轴向压缩机在燃气轮机中尖间隙变化对气动性能的实时预测的深度学习框架的开发和应用。所提出的C(NN)FD架构经证明可扩展至工业应用，并达到与CFD基准相媲美的实时准确性。部署的模型可轻松集成到燃气轮机的制造和构建过程中，从而提供了分析评估性能影响并潜在减少昂贵物理测试要求的机会。

    Application of deep learning methods to physical simulations such as CFD (Computational Fluid Dynamics), have been so far of limited industrial relevance. This paper demonstrates the development and application of a deep learning framework for real-time predictions of the impact of tip clearance variations on the aerodynamic performance of multi-stage axial compressors in gas turbines. The proposed C(NN)FD architecture is proven to be scalable to industrial applications, and achieves in real-time accuracy comparable to the CFD benchmark. The deployed model, is readily integrated within the manufacturing and build process of gas turbines, thus providing the opportunity to analytically assess the impact on performance and potentially reduce requirements for expensive physical tests.
    
[^71]: 用强化学习实现的PyDCM：为可持续性定制数据中心模型

    PyDCM: Custom Data Center Models with Reinforcement Learning for Sustainability. (arXiv:2310.03906v1 [cs.LG])

    [http://arxiv.org/abs/2310.03906](http://arxiv.org/abs/2310.03906)

    PyDCM是一个用强化学习实现的可定制的数据中心模型，通过使用自定义配置和向量化的热计算，实现了对数据中心的优化，具有较高的效率。

    

    全球对可持续性和减少碳排放的强调日益增加，促使政府和企业重新思考数据中心的设计和运营方法。鉴于数据中心的高能耗和指数级计算工作量，优化能耗特别是在冷却和IT能源使用方面，数据中心是优化电力消耗的理想候选。解决这个问题的一个重要挑战是缺乏可配置和可扩展的热数据中心模型，它提供了一个端到端的管道。数据中心由多个IT组件组成，其几何配置和散热使得热建模变得困难。本文介绍了PyDCM，这是一个用Python实现的可定制的数据中心模型，用户可以使用自定义的服务器规格和IT机柜的几何布置创建独特的配置。使用向量化的热计算使得PyDCM比当前方法快了数个数量级（30倍）。

    The increasing global emphasis on sustainability and reducing carbon emissions is pushing governments and corporations to rethink their approach to data center design and operation. Given their high energy consumption and exponentially large computational workloads, data centers are prime candidates for optimizing power consumption, especially in areas such as cooling and IT energy usage. A significant challenge in this pursuit is the lack of a configurable and scalable thermal data center model that offers an end-to-end pipeline. Data centers consist of multiple IT components whose geometric configuration and heat dissipation make thermal modeling difficult. This paper presents PyDCM, a customizable Data Center Model implemented in Python, that allows users to create unique configurations of IT equipment with custom server specifications and geometric arrangements of IT cabinets. The use of vectorized thermal calculations makes PyDCM orders of magnitude faster (30 times) than current 
    
[^72]: Swin-Tempo: 使用Swin Transformer-Enhanced UNet将CT扫描中的肺结节检测作为视频序列进行时间感知

    Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video Sequences Using Swin Transformer-Enhanced UNet. (arXiv:2310.03365v1 [eess.IV])

    [http://arxiv.org/abs/2310.03365](http://arxiv.org/abs/2310.03365)

    Swin-Tempo是一个创新模型，使用了Swin Transformer-Enhanced UNet将CT扫描中的肺结节检测作为视频序列进行时间感知。它克服了现有网络的计算复杂性问题，提高了肺结节检测的准确率。

    

    肺癌具有极高的致死率，早期检测对于防治非常重要。然而，对于放射科医生而言，识别肺结节存在重大挑战，他们往往依赖自己的专业知识和经验来进行准确的诊断。为解决这个问题，基于机器学习技术的计算机辅助诊断系统已经出现，帮助医生从计算机断层扫描（CT）图像中识别肺结节。然而，现有的网络往往存在计算复杂性问题，导致误报和漏报率较高，限制了它们的有效性。为应对这些挑战，我们提出了一种创新模型，结合了卷积神经网络和视觉Transformer的优势。受视频中的目标检测启发，我们将每个3D CT图像视为一个视频，将每个切片视为帧，将肺结节视为目标，实现一个时序应用。我们的工作的主要目标是克服硬件限制。

    Lung cancer is highly lethal, emphasizing the critical need for early detection. However, identifying lung nodules poses significant challenges for radiologists, who rely heavily on their expertise and experience for accurate diagnosis. To address this issue, computer-aided diagnosis systems based on machine learning techniques have emerged to assist doctors in identifying lung nodules from computed tomography (CT) scans. Unfortunately, existing networks in this domain often suffer from computational complexity, leading to high rates of false negatives and false positives, limiting their effectiveness. To address these challenges, we present an innovative model that harnesses the strengths of both convolutional neural networks and vision transformers. Inspired by object detection in videos, we treat each 3D CT image as a video, individual slices as frames, and lung nodules as objects, enabling a time-series application. The primary objective of our work is to overcome hardware limitati
    
[^73]: 破坏到底：对机器学习防钓鱼网页检测器的查询高效对抗HTML攻击

    Raze to the Ground: Query-Efficient Adversarial HTML Attacks on Machine-Learning Phishing Webpage Detectors. (arXiv:2310.03166v1 [cs.CR])

    [http://arxiv.org/abs/2310.03166](http://arxiv.org/abs/2310.03166)

    本文提出了一种新颖的查询高效对抗HTML攻击方法，通过细粒度篡改来修改钓鱼网页的HTML代码，同时保持其恶意性和视觉外观不变。实验表明，这种方法能够将目前最先进的机器学习防钓鱼网页检测器的性能摧毁，并且只需要30个查询。

    

    机器学习防钓鱼网页检测器（ML-PWD）已被证明容易受到输入网页HTML代码的对抗性篡改。然而，最近提出的攻击由于缺乏优化所采用的篡改的使用以及仅关注HTML代码的特定元素而表现出有限的有效性。在这项工作中，我们通过首先设计一组新颖的细粒度篡改来克服这些限制，这些篡改允许修改输入钓鱼网页的HTML代码，而无需 compromiser其恶意性和视觉外观，即篡改在设计上是功能和渲染保持不变的。然后，我们使用一个查询高效的黑盒优化算法选择需要应用哪些篡改以绕过目标检测器。我们的实验表明，我们的攻击只需要30个查询即可摧毁目前最先进的ML-PWD的性能，从而克服了较弱的攻击。

    Machine-learning phishing webpage detectors (ML-PWD) have been shown to suffer from adversarial manipulations of the HTML code of the input webpage. Nevertheless, the attacks recently proposed have demonstrated limited effectiveness due to their lack of optimizing the usage of the adopted manipulations, and they focus solely on specific elements of the HTML code. In this work, we overcome these limitations by first designing a novel set of fine-grained manipulations which allow to modify the HTML code of the input phishing webpage without compromising its maliciousness and visual appearance, i.e., the manipulations are functionality- and rendering-preserving by design. We then select which manipulations should be applied to bypass the target detector by a query-efficient black-box optimization algorithm. Our experiments show that our attacks are able to raze to the ground the performance of current state-of-the-art ML-PWD using just 30 queries, thus overcoming the weaker attacks develo
    
[^74]: 使用机器学习模型进行信用卡评分预测：一个新数据集

    Credit card score prediction using machine learning models: A new dataset. (arXiv:2310.02956v1 [cs.LG])

    [http://arxiv.org/abs/2310.02956](http://arxiv.org/abs/2310.02956)

    本研究探索了利用机器学习模型对信用卡违约进行预测的方法，并提出了一个新的信用卡评分数据集。实验结果表明，多层感知器（MLP）模型在预测性能上表现最佳。

    

    近年来，信用卡的使用量不断增加，为了最小化潜在风险，急需信用卡评估方法。本研究调查了利用机器学习模型进行信用卡违约预测系统的应用。主要目标是研究在新提出的信用卡评分数据集上表现最佳的机器学习模型。这个新数据集包括信用卡交易历史和客户档案，并使用了多种机器学习算法进行了测试，包括逻辑回归、决策树、随机森林、多层感知器（MLP）神经网络、XGBoost和LightGBM。为了准备机器学习模型的数据，我们进行了数据预处理、特征提取、特征选择和数据平衡技术。实验结果表明，在真正阳性率方面，MLP在预测性能上优于逻辑回归、决策树、随机森林、LightGBM和XGBoost，实现了最佳表现。

    The use of credit cards has recently increased, creating an essential need for credit card assessment methods to minimize potential risks. This study investigates the utilization of machine learning (ML) models for credit card default prediction system. The main goal here is to investigate the best-performing ML model for new proposed credit card scoring dataset. This new dataset includes credit card transaction histories and customer profiles, is proposed and tested using a variety of machine learning algorithms, including logistic regression, decision trees, random forests, multi layer perceptron (MLP) neural network, XGBoost, and LightGBM. To prepare the data for machine learning models, we perform data pre-proccessing, feature extraction, feature selection, and data balancing techniques. Experimental results demonstrate that MLP outperforms logistic regression, decision trees, random forests, LightGBM, and XGBoost in terms of predictive performance in true positive rate, achieving 
    
[^75]: 通过利用层间变换的平滑性进行带外分布检测

    Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness. (arXiv:2310.02832v1 [cs.LG])

    [http://arxiv.org/abs/2310.02832](http://arxiv.org/abs/2310.02832)

    本文提出了一种通过利用神经网络中间层变换的平滑性来检测带外数据的方法(BLOOD),该方法适用于没有训练数据访问权限的预训练模型，并在Transformer网络上的文本分类任务中取得了良好的效果。

    

    有效的带外分布检测对于可靠的机器学习模型至关重要，然而大多数当前方法由于需要访问训练数据或者干预训练而在实际应用中受到限制。我们提出了一种新的方法，通过网络中间层的变换平滑性来检测深度神经网络中的带外数据（BLOOD），该方法适用于没有训练数据访问权限的预训练模型。BLOOD利用内分布（ID）数据的层间表示变换相较于带外数据的变换更平滑的倾向，这也是我们在Transformer网络中经验证明的一个特性。我们在几个文本分类任务上评估了BLOOD与Transformer网络，并证明其在资源需求相当的方法上性能更好。我们的分析还表明，当学习更简单的任务时，带外数据的变换会保持其原始的锐度，而锐度会随着任务的增加而增加。

    Effective OOD detection is crucial for reliable machine learning models, yet most current methods are limited in practical use due to requirements like access to training data or intervention in training. We present a novel method for detecting OOD data in deep neural networks based on transformation smoothness between intermediate layers of a network (BLOOD), which is applicable to pre-trained models without access to training data. BLOOD utilizes the tendency of between-layer representation transformations of in-distribution (ID) data to be smoother than the corresponding transformations of OOD data, a property that we also demonstrate empirically for Transformer networks. We evaluate BLOOD on several text classification tasks with Transformer networks and demonstrate that it outperforms methods with comparable resource requirements. Our analysis also suggests that when learning simpler tasks, OOD data transformations maintain their original sharpness, whereas sharpness increases wit
    
[^76]: 3D物理系统中学习对称性破缺的松弛八面体群卷积

    Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems. (arXiv:2310.02299v1 [cs.LG])

    [http://arxiv.org/abs/2310.02299](http://arxiv.org/abs/2310.02299)

    本文介绍了一种用于建模3D物理系统的松弛八面体群卷积技术，它可以在保持数据一致的最高等变性水平的同时，发现物理系统中微妙的对称性破缺因素。

    

    深度等价模型利用对称性提高样本效率和泛化性能。然而，在许多这些模型中，完美对称性的假设有时可能会限制性能，特别是当数据与这些对称性不完全一致时。因此，我们在本文中引入了用于建模3D物理系统的松弛八面体群卷积。这种灵活的卷积技术能够在保持与数据一致的最高等变性水平的同时，发现物理系统中微妙的对称性破缺因素。实证结果验证了我们的方法不仅可以揭示相变中的对称性破缺因素，还可以在流体超分辨率任务中实现卓越性能。

    Deep equivariant models use symmetries to improve sample efficiency and generalization. However, the assumption of perfect symmetry in many of these models can sometimes be restrictive, especially when the data does not perfectly align with such symmetries. Thus, we introduce relaxed octahedral group convolution for modeling 3D physical systems in this paper. This flexible convolution technique provably allows the model to both maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in the physical systems. Empirical results validate that our approach can not only provide insights into the symmetry-breaking factors in phase transitions but also achieves superior performance in fluid super-resolution tasks.
    
[^77]: 为什么自编码器起作用？

    Why do autoencoders work?. (arXiv:2310.02250v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.02250](http://arxiv.org/abs/2310.02250)

    自编码器是一种深度神经网络模型，通过调整参数实现输入数据和重构输出之间的最小差异，用于识别数据在高维空间中的内在维度，并且对于某些拓扑结构，存在难以找到完美重构网络的限制。

    

    深度神经网络自编码器被广泛用于模型压缩。它们可以识别数据在输入的欧几里德空间R^n中，位于k维子集K中的内在维度。其基本思想是获得一个将R^n映射为R^k的编码层（称为瓶颈层或潜变量空间），以及一个将R^k映射回R^n的解码层，使得在组合这两个映射时可以恢复来自集合K的输入数据。这通过调整网络中的参数（权重）来最小化输入和重构输出之间的差异来实现。由于神经网络（具有连续激活函数）计算连续映射，实现完美重构的网络的存在将意味着K在R^k中是一个k维子集的同胚，因此明显存在拓扑障碍来寻找这样的网络。

    Deep neural network autoencoders are routinely used computationally for model reduction. They allow recognizing the intrinsic dimension of data that lie in a $k$-dimensional subset $K$ of an input Euclidean space $\mathbb{R}^n$. The underlying idea is to obtain both an encoding layer that maps $\mathbb{R}^n$ into $\mathbb{R}^k$ (called the bottleneck layer or the space of latent variables) and a decoding layer that maps $\mathbb{R}^k$ back into $\mathbb{R}^n$, in such a way that the input data from the set $K$ is recovered when composing the two maps. This is achieved by adjusting parameters (weights) in the network to minimize the discrepancy between the input and the reconstructed output. Since neural networks (with continuous activation functions) compute continuous maps, the existence of a network that achieves perfect reconstruction would imply that $K$ is homeomorphic to a $k$-dimensional subset of $\mathbb{R}^k$, so clearly there are topological obstructions to finding such a ne
    
[^78]: MIS-AVoiDD:音视深度伪造检测的模态不变和特定表示

    MIS-AVoiDD: Modality Invariant and Specific Representation for Audio-Visual Deepfake Detection. (arXiv:2310.02234v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.02234](http://arxiv.org/abs/2310.02234)

    本论文提出了一种名为MIS-AVoiDD的模态不变和特定表示方法，用于音视深度伪造检测。通过充分关注音频和视觉数据，并解决音频和视觉信号的异质性问题，该方法在多模态操作检测中表现出良好的性能。

    

    深度伪造是使用深度生成算法生成的合成媒体，对社会和政治构成了严重威胁。除了面部操作和合成语音，最近还出现了一种新型的深度伪造，其中音频或视觉模态被操作。为此，正在研究一种新一代的多模态音视深度伪造检测器，共同关注音频和视觉数据进行多模态操作检测。现有的多模态（音视）深度伪造检测器通常基于从视频中融合音频和视觉流。现有研究表明，这些多模态检测器通常具有与单模态音频和视觉深度伪造检测器相当的性能。我们推测，音频和视觉信号的异质性本质造成了模态分布差距，并对有效融合和高效性能提出了重大挑战。本文从表示层面解决了这个问题。

    Deepfakes are synthetic media generated using deep generative algorithms and have posed a severe societal and political threat. Apart from facial manipulation and synthetic voice, recently, a novel kind of deepfakes has emerged with either audio or visual modalities manipulated. In this regard, a new generation of multimodal audio-visual deepfake detectors is being investigated to collectively focus on audio and visual data for multimodal manipulation detection. Existing multimodal (audio-visual) deepfake detectors are often based on the fusion of the audio and visual streams from the video. Existing studies suggest that these multimodal detectors often obtain equivalent performances with unimodal audio and visual deepfake detectors. We conjecture that the heterogeneous nature of the audio and visual signals creates distributional modality gaps and poses a significant challenge to effective fusion and efficient performance. In this paper, we tackle the problem at the representation lev
    
[^79]: 概率重连的消息传递神经网络

    Probabilistically Rewired Message-Passing Neural Networks. (arXiv:2310.02156v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.02156](http://arxiv.org/abs/2310.02156)

    PR-MPNNs通过概率重连学习加入相关边，并省略对预测任务没有帮助的边，从而增强了表达能力。

    

    消息传递图神经网络（MPNN）作为处理图结构输入的强大工具而出现。然而，它们在固定的输入图结构上操作，忽略了潜在的噪声和缺失信息。此外，它们的局部聚合机制可能导致问题，如过度压缩和在捕捉相关图结构方面的有限表达能力。现有的解决这些挑战的方法主要依赖于启发式方法，往往忽视了底层数据分布。因此，设计了一种原则性的方法，用于学习推断与给定预测任务相关的图结构，仍然是一个未解决的挑战。在这项工作中，利用了最近在精确和可微分的k-子集采样方面的进展，我们设计了概率重连的MPNN (PR-MPNN)，它们学习在省略对预测任务没有帮助的边的同时添加相关的边。我们的理论分析首次探索了PR-MPNN如何增强表达能力，并且我们确定了确切的条件。

    Message-passing graph neural networks (MPNNs) emerged as powerful tools for processing graph-structured input. However, they operate on a fixed input graph structure, ignoring potential noise and missing information. Furthermore, their local aggregation mechanism can lead to problems such as over-squashing and limited expressive power in capturing relevant graph structures. Existing solutions to these challenges have primarily relied on heuristic methods, often disregarding the underlying data distribution. Hence, devising principled approaches for learning to infer graph structures relevant to the given prediction task remains an open challenge. In this work, leveraging recent progress in exact and differentiable $k$-subset sampling, we devise probabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges while omitting less beneficial ones. For the first time, our theoretical analysis explores how PR-MPNNs enhance expressive power, and we identify precise conditions un
    
[^80]: SpatialRank: 基于时空数据的城市事件排名与NDCG优化

    SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data. (arXiv:2310.00270v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00270](http://arxiv.org/abs/2310.00270)

    这篇论文提出了一种名为SpatialRank的新颖空间事件排名方法，通过基于时空数据的NDCG优化来解决城市事件排名问题。

    

    城市事件排名问题旨在预测未来事件（如交通事故和犯罪事件）的风险最高的前k个地点。这个问题对公共安全和城市管理非常重要，尤其是在资源有限的情况下。然而，由于地点之间复杂而动态的时空相关性，空间中城市事件的不均匀分布，以及正确对具有相似特征的附近地点进行排名的困难，这个问题很具挑战性。前人的研究主要旨在准确预测所有地点的实际风险得分或事件计数。由于预测错误，由此得到的排名通常质量较低。学习排序方法直接优化诸如标准化折扣累积增益（NDCG）之类的指标，但不能处理地点之间存在的时空自相关性。在本文中，我们通过提出一种名为SpatialRank的新颖空间事件排名方法来弥合这一差距。

    The problem of urban event ranking aims at predicting the top-k most risky locations of future events such as traffic accidents and crimes. This problem is of fundamental importance to public safety and urban administration especially when limited resources are available. The problem is, however, challenging due to complex and dynamic spatio-temporal correlations between locations, uneven distribution of urban events in space, and the difficulty to correctly rank nearby locations with similar features. Prior works on event forecasting mostly aim at accurately predicting the actual risk score or counts of events for all the locations. Rankings obtained as such usually have low quality due to prediction errors. Learning-to-rank methods directly optimize measures such as Normalized Discounted Cumulative Gain (NDCG), but cannot handle the spatiotemporal autocorrelation existing among locations. In this paper, we bridge the gap by proposing a novel spatial event ranking approach named Spati
    
[^81]: 融入人类风险认知的对抗驾驶行为生成技术用于自动驾驶车辆评估

    Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation. (arXiv:2310.00029v1 [cs.AI])

    [http://arxiv.org/abs/2310.00029](http://arxiv.org/abs/2310.00029)

    本论文开发了一种新型框架，结合人类风险认知来生成对手驾驶行为，用于评估自动驾驶车辆的有效性和弱点。

    

    这篇论文关注于开发一种用于生成对手驾驶行为的新型框架，以暴露出自动驾驶车辆面对的有效和合理的风险事件。具体而言，采用强化学习与累积前景理论相结合的方法来学习对手行为，累积前景理论能够表示人类的风险认知。然后，提出了扩展版本的深度确定性策略梯度技术，用于训练对手策略，同时保证了训练的稳定性。在高保真的硬件在环（HiL）平台上进行了基于并线情景的对比案例研究，结果证明了对手的有效性，可以推断出被测试自动驾驶车辆的弱点。

    Autonomous vehicle (AV) evaluation has been the subject of increased interest in recent years both in industry and in academia. This paper focuses on the development of a novel framework for generating adversarial driving behavior of background vehicle interfering against the AV to expose effective and rational risky events. Specifically, the adversarial behavior is learned by a reinforcement learning (RL) approach incorporated with the cumulative prospect theory (CPT) which allows representation of human risk cognition. Then, the extended version of deep deterministic policy gradient (DDPG) technique is proposed for training the adversarial policy while ensuring training stability as the CPT action-value function is leveraged. A comparative case study regarding the cut-in scenario is conducted on a high fidelity Hardware-in-the-Loop (HiL) platform and the results demonstrate the adversarial effectiveness to infer the weakness of the tested AV.
    
[^82]: 《面向通用模型的自由数据选择》

    Towards Free Data Selection with General-Purpose Models. (arXiv:2309.17342v1 [cs.CV])

    [http://arxiv.org/abs/2309.17342](http://arxiv.org/abs/2309.17342)

    本文提出了一种新的数据选择流程，利用通用模型在单次推理中选择来自不同数据集的数据，无需额外的训练或监督。通过定义和利用语义模式提取微妙的局部信息，我们实现了对所有数据样本的选择。

    

    一个理想的数据选择算法可以高效地选择最具信息量的样本，以最大化有限的注释预算的效用。然而，目前的方法（例如主动学习方法）通常遵循一个繁琐的流程，反复进行耗时的模型训练和批量数据选择。在本文中，我们挑战了这种现状，通过设计一个独特的数据选择流程，利用现有的通用模型，在单次推理中选择来自不同数据集的数据，而无需额外的训练或监督。我们提出了一种新的自由数据选择（FreeSel）方法来实现这个新的流程。具体地，我们定义了从通用模型的中间特征中提取的语义模式，以捕捉每个图像中微妙的局部信息。然后，我们通过基于距离的采样在细粒度的语义模式级别上实现了对所有数据样本的选择。FreeSel绕过了原来的耗时训练和批量数据选择的流程。

    A desirable data selection algorithm can efficiently choose the most informative samples to maximize the utility of limited annotation budgets. However, current approaches, represented by active learning methods, typically follow a cumbersome pipeline that iterates the time-consuming model training and batch data selection repeatedly. In this paper, we challenge this status quo by designing a distinct data selection pipeline that utilizes existing general-purpose models to select data from various datasets with a single-pass inference without the need for additional training or supervision. A novel free data selection (FreeSel) method is proposed following this new pipeline. Specifically, we define semantic patterns extracted from inter-mediate features of the general-purpose model to capture subtle local information in each image. We then enable the selection of all data samples in a single pass through distance-based sampling at the fine-grained semantic pattern level. FreeSel bypass
    
[^83]: 深度强化学习在控制系统中的可靠性量化

    Reliability Quantification of Deep Reinforcement Learning-based Control. (arXiv:2309.16977v1 [eess.SY])

    [http://arxiv.org/abs/2309.16977](http://arxiv.org/abs/2309.16977)

    本论文提出了一种用于量化深度强化学习控制系统可靠性的方法，通过使用两个神经网络来评估控制的可靠性差异。

    

    深度强化学习（DRL）在安全关键系统中的可靠性量化是人工智能在实际应用中的重要挑战。本研究提出了一种用于量化DRL控制可靠性的方法。首先，应用了一种现有方法——随机噪声提取，以明确需要解决的问题。其次，提出了一种新的可靠性量化方法来解决这些问题。该方法使用两个神经网络来量化可靠性：参考网络和评估网络。它们具有相同的结构和相同的初始参数。在训练之前，两个网络的输出相同。在训练过程中，评估网络的参数被更新，以最大化训练数据上的参考网络和评估网络之间的差异。因此，可以基于两个网络的输出差异评估特定状态下DRL控制的可靠性。

    Reliability quantification of deep reinforcement learning (DRL)-based control is a significant challenge for the practical application of artificial intelligence (AI) in safety-critical systems. This study proposes a method for quantifying the reliability of DRL-based control. First, an existing method, random noise distillation, was applied to the reliability evaluation to clarify the issues to be solved. Second, a novel method for reliability quantification was proposed to solve these issues. The reliability is quantified using two neural networks: reference and evaluator. They have the same structure with the same initial parameters. The outputs of the two networks were the same before training. During training, the evaluator network parameters were updated to maximize the difference between the reference and evaluator networks for trained data. Thus, the reliability of the DRL-based control for a state can be evaluated based on the difference in output between the two networks. The
    
[^84]: 频道视觉Transformer：一张图值C x 16 x 16个词

    Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v1 [cs.CV])

    [http://arxiv.org/abs/2309.16108](http://arxiv.org/abs/2309.16108)

    本文提出了ChannelViT模型，通过对ViT架构的修改和引入分层通道采样技术，增强了对多通道图像的推理能力和鲁棒性，适用于显微镜和卫星成像等领域。

    

    视觉Transformer在现代计算机视觉领域中已经成为一种强大的架构。然而，它在某些图像领域的应用，如显微镜和卫星成像，面临着独特的挑战。在这些领域中，图像通常包含多个通道，每个通道都携带着语义上不同和独立的信息。此外，模型必须对输入通道的稀疏性表现出鲁棒性，在训练或测试过程中可能没有密集可用的通道。在本文中，我们提出了对ViT架构的修改，增强了对输入通道之间的推理，并引入了分层通道采样(HCS)作为一种附加的正则化技术，以确保在测试过程中仅出现部分通道时的鲁棒性。我们提出的模型ChannelViT独立地构建补丁令牌并利用可学习的通道嵌入将其添加到补丁令牌中，类似于位置嵌入。我们进行了评估

    Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate
    
[^85]: 学习高效前沿

    Learning the Efficient Frontier. (arXiv:2309.15775v1 [cs.LG])

    [http://arxiv.org/abs/2309.15775](http://arxiv.org/abs/2309.15775)

    本文引入了NeuralEF，一个快速的神经逼近框架，能够鲁棒地预测高效前沿问题的解，同时处理异构线性约束和可变数量的优化输入。

    

    高效前沿（EF）是一个基本的资源配置问题，在给定风险水平下寻找最优投资组合以最大化收益。传统上，通过解一个凸优化问题来找到最优解。本文引入了NeuralEF：一个快速的神经逼近框架，可以鲁棒地预测相对异构线性约束和可变数量的优化输入的EF凸优化问题的结果。通过将优化问题重新定义为序列到序列问题，我们展示了NeuralEF是加速大规模模拟并处理不连续行为的可行解决方案。

    The efficient frontier (EF) is a fundamental resource allocation problem where one has to find an optimal portfolio maximizing a reward at a given level of risk. This optimal solution is traditionally found by solving a convex optimization problem. In this paper, we introduce NeuralEF: a fast neural approximation framework that robustly forecasts the result of the EF convex optimization problem with respect to heterogeneous linear constraints and variable number of optimization inputs. By reformulating an optimization problem as a sequence to sequence problem, we show that NeuralEF is a viable solution to accelerate large-scale simulation while handling discontinuous behavior.
    
[^86]: 基于潜在图的生物医学表格数据半监督学习

    Latent Graph Powered Semi-Supervised Learning on Biomedical Tabular Data. (arXiv:2309.15757v1 [cs.LG])

    [http://arxiv.org/abs/2309.15757](http://arxiv.org/abs/2309.15757)

    本文提出了一种基于潜在图的半监督学习方法，通过利用图的表示来捕捉数据之间的关系，并实现了全局和局部知识的有效融合。在生物医学数据集上的评估中，我们的方法表现出了最先进的结果。

    

    在半监督学习领域中，现有方法未充分利用（有）标记数据之间的实例间关系的潜力。本文通过提供一种推断捕捉内在数据关系的潜在图的方法来解决这个限制。通过利用基于图的表示，我们的方法促进了信息在整个图中的无缝传播，能够有效地融合全局和局部知识。通过在生物医学表格数据集上的评估，我们比较了我们的方法与其他当代方法的能力。我们的工作证明了发现实例间关系作为构建强化半监督学习技术的鲁棒潜在图的实际手段的重要性。我们的方法在三个生物医学数据集上取得了最先进的结果。

    In the domain of semi-supervised learning, the current approaches insufficiently exploit the potential of considering inter-instance relationships among (un)labeled data. In this work, we address this limitation by providing an approach for inferring latent graphs that capture the intrinsic data relationships. By leveraging graph-based representations, our approach facilitates the seamless propagation of information throughout the graph, enabling the effective incorporation of global and local knowledge. Through evaluations on biomedical tabular datasets, we compare the capabilities of our approach to other contemporary methods. Our work demonstrates the significance of inter-instance relationship discovery as practical means for constructing robust latent graphs to enhance semi-supervised learning techniques. Our method achieves state-of-the-art results on three biomedical datasets.
    
[^87]: HyPoradise：基于大语言模型的生成式语音识别的开放基准线

    HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models. (arXiv:2309.15701v1 [cs.CL])

    [http://arxiv.org/abs/2309.15701](http://arxiv.org/abs/2309.15701)

    本文引入了第一个开源基准测试，利用大型语言模型进行自动语音识别错误修正，实现了与人类水平相当的性能，具有重要的实际应用价值。

    

    深度神经网络的进展使得自动语音识别系统在几个公开的干净语音数据集上达到了人类水平。然而，即使是最先进的自动语音识别系统在面对逆境时也会出现性能下降，因为良好训练的声学模型对于语音领域的变异性很敏感，如背景噪声。受到这一观察的启发，我们引入了第一个开源基准测试，利用外部的大型语言模型（LLMs）来进行自动语音识别错误修正，其中N最佳解码假设为真实转录预测提供了有信息量的元素。这种方法与传统的语言模型重评分策略不同，后者只能选择一个候选假设作为最终预测。

    Advancements in deep neural networks have allowed automatic speech recognition (ASR) systems to attain human parity on several publicly available clean speech datasets. However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acoustic model is sensitive to variations in the speech domain, e.g., background noise. Intuitively, humans address this issue by relying on their linguistic knowledge: the meaning of ambiguous spoken terms is usually inferred from contextual cues thereby reducing the dependency on the auditory system. Inspired by this observation, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses provide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as the o
    
[^88]: IBM最大量子处理器的张量网络模拟的高效方法

    Efficient tensor network simulation of IBM's largest quantum processors. (arXiv:2309.15642v1 [quant-ph])

    [http://arxiv.org/abs/2309.15642](http://arxiv.org/abs/2309.15642)

    本文展示了如何使用量子启发的二维张量网络高效模拟IBM最大的量子处理器，通过简单的张量更新实现前所未有的准确度和极低的计算资源消耗，并为最新的IBM量子机器设立了基准。

    

    我们展示了如何使用量子启发的二维张量网络来高效准确地模拟IBM最大的量子处理器，即Eagle（127个量子比特），Osprey（433个量子比特）和Condor（1121个量子比特）。我们使用基于图的投影纠缠对态（gPEPS）模拟了一个复杂的量子多体系统的动力学，具体来说，这是IBM最近在Nature 618年第500-505页（2023年）上考虑的踢击易辛实验-我们在PRB 99, 195105（2019年）中提出了这个模型。我们的结果表明，对于该模型，简单的张量更新已经足以以极低的计算资源实现非常大的前所未有的精度。除了模拟127个量子比特的原始实验外，我们还将结果扩展到433个和1121个量子比特，从而为最新的IBM量子机器设定了一个基准。我们还报道了无限多个量子比特的准确模拟。我们的结果表明，gPEPS是高效模拟量子计算机的自然工具。

    We show how quantum-inspired 2d tensor networks can be used to efficiently and accurately simulate the largest quantum processors from IBM, namely Eagle (127 qubits), Osprey (433 qubits) and Condor (1121 qubits). We simulate the dynamics of a complex quantum many-body system -- specifically, the kicked Ising experiment considered recently by IBM in Nature 618, p. 500-505 (2023) -using graph-based Projected Entangled Pair States (gPEPS), which was proposed by some of us in PRB 99, 195105 (2019). Our results show that simple tensor updates are already sufficient to achieve very large unprecedented accuracy with remarkably low computational resources for this model. Apart from simulating the original experiment for 127 qubits, we also extend our results to 433 and 1121 qubits, thus setting a benchmark for the newest IBM quantum machines. We also report accurate simulations for infinitely-many qubits. Our results show that gPEPS are a natural tool to efficiently simulate quantum computer
    
[^89]: 连续治疗的双重稳健近端因果学习

    Doubly Robust Proximal Causal Learning for Continuous Treatments. (arXiv:2309.12819v1 [stat.ME])

    [http://arxiv.org/abs/2309.12819](http://arxiv.org/abs/2309.12819)

    本文提出了一种基于核函数的双重稳健近端因果学习方法，用于处理连续治疗，并提出了一种高效求解干扰函数的新方法。

    

    近端因果学习是在存在未测量混淆因素下识别因果效应的有希望的框架。在该框架中，补充稳健（DR）估计器被推导出来，并在估计中展示了其有效性，特别是在模型假设被违反时。然而，当前形式的DR估计器仅限于二进制治疗，而在许多现实世界的应用中，治疗可以是连续的。连续治疗的主要障碍在于在原始DR估计器中存在的delta函数，使得在因果效应估计中不可行，并在干扰函数估计中引入了沉重的计算负担。为了解决这些挑战，我们提出了一种基于核函数的连续治疗的DR估计器，可以很好地处理连续治疗。配备其平滑性，我们展示了其Oracle形式是影响函数的一致近似。此外，我们提出了一种新的方法来高效解决干扰函数。

    Proximal causal learning is a promising framework for identifying the causal effect under the existence of unmeasured confounders. Within this framework, the doubly robust (DR) estimator was derived and has shown its effectiveness in estimation, especially when the model assumption is violated. However, the current form of the DR estimator is restricted to binary treatments, while the treatment can be continuous in many real-world applications. The primary obstacle to continuous treatments resides in the delta function present in the original DR estimator, making it infeasible in causal effect estimation and introducing a heavy computational burden in nuisance function estimation. To address these challenges, we propose a kernel-based DR estimator that can well handle continuous treatments. Equipped with its smoothness, we show that its oracle form is a consistent approximation of the influence function. Further, we propose a new approach to efficiently solve the nuisance functions. We
    
[^90]: Google的Bard在对抗图像攻击方面有多强大？

    How Robust is Google's Bard to Adversarial Image Attacks?. (arXiv:2309.11751v1 [cs.CV])

    [http://arxiv.org/abs/2309.11751](http://arxiv.org/abs/2309.11751)

    本文研究了Google的Bard在对抗图像攻击方面的鲁棒性，并发现它可以被攻击以输出错误的图像描述。这一攻击还可以对其他多模态语言模型产生影响。研究还发现了Bard的两种防御机制。

    

    结合文本和其他模态（尤其是视觉）的多模态大型语言模型（MLLM）在各种多模态任务中取得了前所未有的性能。然而，由于视觉模型的未解决的对抗鲁棒性问题，引入视觉输入可能使MLLM面临更严重的安全风险和安全风险。在这项工作中，我们研究了Google的Bard的对抗鲁棒性，它是一个竞争性的聊天机器人，最近发布了其多模态能力，以更好地了解商业MLLM的漏洞。通过攻击白盒子代理视觉编码器或MLLM，生成的对抗性示例可以使Bard以22％的成功率仅基于可转移性输出错误的图像描述。我们还表明，对抗性示例还可以攻击其他MLLM，例如，对Bing Chat的成功攻击率为26％，对ERNIE bot的成功攻击率为86％。此外，我们确定了Bard的两种防御机制，包括面部检测。

    Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection
    
[^91]: Clustered FedStack：基于贝叶斯信息准则的中间全局模型。

    Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion. (arXiv:2309.11044v1 [cs.LG])

    [http://arxiv.org/abs/2309.11044](http://arxiv.org/abs/2309.11044)

    提出了一种基于贝叶斯信息准则的中间全局模型Clustered FedStack。通过将本地客户端的模型预测和输出层权重发送到服务器，构建一个强大的全局模型，并使用聚类机制对本地客户进行聚类。

    

    联邦学习（FL）是目前人工智能领域最受欢迎的技术之一，因其协作学习和保护客户隐私的能力而受到青睐。然而，它面临非独立和非独立分布（非IID）以及本地客户之间标签不平衡等挑战。为了解决这些限制，研究团队探索了各种方法，如使用本地模型参数、联邦生成对抗学习和联邦表示学习。在我们的研究中，我们提出了一种基于已发表的Stacked Federated Learning（FedStack）框架的新颖Clustered FedStack框架。本地客户端将其模型预测和输出层权重发送到服务器，然后构建一个强大的全局模型。这个全局模型使用聚类机制基于其输出层权重对本地客户进行聚类。我们采用了三种聚类机制，分别是K-Means、Agglomerative、DBSCAN。

    Federated Learning (FL) is currently one of the most popular technologies in the field of Artificial Intelligence (AI) due to its collaborative learning and ability to preserve client privacy. However, it faces challenges such as non-identically and non-independently distributed (non-IID) and data with imbalanced labels among local clients. To address these limitations, the research community has explored various approaches such as using local model parameters, federated generative adversarial learning, and federated representation learning. In our study, we propose a novel Clustered FedStack framework based on the previously published Stacked Federated Learning (FedStack) framework. The local clients send their model predictions and output layer weights to a server, which then builds a robust global model. This global model clusters the local clients based on their output layer weights using a clustering mechanism. We adopt three clustering mechanisms, namely K-Means, Agglomerative, a
    
[^92]: 复制：在流量分类中使用Flowpic输入表示的对比学习和数据增强

    Replication: Contrastive Learning and Data Augmentation in Traffic Classification Using a Flowpic Input Representation. (arXiv:2309.09733v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.09733](http://arxiv.org/abs/2309.09733)

    这项研究复制并重现了一项先前的流量分类研究，该研究使用了对比学习和数据增强的深度学习方法，证明了使用Flowpic输入表示能够在仅有100个样本的情况下实现高准确性。

    

    在过去的几年里，由于深度学习的兴起，我们见证了对流量分类（TC）的重新关注。然而，绝大部分 TC 文献缺乏代码工件、跨数据集的性能评估以及与机器学习（ML）方法的参考比较。其中，IMC22[16]的一项最近研究值得关注，因为它采用了最近的深度学习方法（即少样本学习、自监督对比学习和数据增强），这对于网络来说非常有吸引力，因为它们可以从少量样本中学习并在不同的数据集之间传输。[16]的主要结果是，在UCDAVIS19、ISCX-VPN和ISCX-Tor数据集上，使用这些深度学习方法，只需100个输入样本就可以实现非常高的准确性，使用的输入表示叫做“flowpic”（即在时间上演变的数据包大小的分指标的二维直方图）。本文的目标是(i)在相同的数据集上重现[16]的结果，以及(ii)复制其最显著的方面（即对比学习和数据增强）。

    Over the last years we witnessed a renewed interest toward Traffic Classification (TC) captivated by the rise of Deep Learning (DL). Yet, the vast majority of TC literature lacks code artifacts, performance assessments across datasets and reference comparisons against Machine Learning (ML) methods. Among those works, a recent study from IMC22 [16] is worth of attention since it adopts recent DL methodologies (namely, few-shot learning, self-supervision via contrastive learning and data augmentation) appealing for networking as they enable to learn from a few samples and transfer across datasets. The main result of [16] on the UCDAVIS19, ISCX-VPN and ISCX-Tor datasets is that, with such DL methodologies, 100 input samples are enough to achieve very high accuracy using an input representation called "flowpic" (i.e., a per-flow 2d histograms of the packets size evolution over time). In this paper (i) we reproduce [16] on the same datasets and (ii) we replicate its most salient aspect (the
    
[^93]: 具有有界更新的迭代学习算法的泛化误差界限

    Generalization error bounds for iterative learning algorithms with bounded updates. (arXiv:2309.05077v1 [cs.LG])

    [http://arxiv.org/abs/2309.05077](http://arxiv.org/abs/2309.05077)

    本文研究了具有有界更新的迭代学习算法在非凸损失函数上的泛化特性，提出了一种新颖的泛化误差界限，利用了信息论技术。研究表明，在模型维度和训练数据样本数量相等的情况下，界限得到了改善。

    

    本文探讨了具有有界更新的迭代学习算法在非凸损失函数上的泛化特性，采用了信息论技术。我们的主要贡献是针对具有有界更新的算法提出了一种新颖的泛化误差界限，超出了以前只关注随机梯度下降（SGD）的范围。我们的方法引入了两个主要的创新之处：1）我们将互信息重新定义为更新的不确定性，提供了一种新的视角；2）我们不使用互信息的链式法则，而是采用方差分解技术来将信息分解到迭代中，从而允许简化的代理过程。我们在各种设置下分析了我们的泛化界限，并在模型维度以与训练数据样本数量相同的速率增加时展示了改进的界限。为了弥合理论与实践之间的差距，我们还研究了先前观察到的情况。

    This paper explores the generalization characteristics of iterative learning algorithms with bounded updates for non-convex loss functions, employing information-theoretic techniques. Our key contribution is a novel bound for the generalization error of these algorithms with bounded updates, extending beyond the scope of previous works that only focused on Stochastic Gradient Descent (SGD). Our approach introduces two main novelties: 1) we reformulate the mutual information as the uncertainty of updates, providing a new perspective, and 2) instead of using the chaining rule of mutual information, we employ a variance decomposition technique to decompose information across iterations, allowing for a simpler surrogate process. We analyze our generalization bound under various settings and demonstrate improved bounds when the model dimension increases at the same rate as the number of training data samples. To bridge the gap between theory and practice, we also examine the previously obse
    
[^94]: 在教育数据挖掘中深度学习技术的综合调研

    A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining. (arXiv:2309.04761v1 [cs.LG])

    [http://arxiv.org/abs/2309.04761](http://arxiv.org/abs/2309.04761)

    本调研综合审查了在教育数据挖掘中深度学习技术的最新研究进展，包括对知识跟踪、学生不良行为检测、性能预测和个性化推荐等典型教育场景的应用。同时提供了公共数据集和处理工具的综合概述，并指出了未来的研究方向。

    

    教育数据挖掘(EDM)作为研究的重要领域，利用计算技术来分析教育数据。随着教育数据的复杂性和多样性增加，深度学习技术在解决分析和建模这些数据所面临的挑战方面表现出了显著的优势。本调研旨在系统地审查深度学习在EDM领域的最新研究进展。我们首先提供了关于EDM和深度学习的简要介绍，强调了它们在现代教育环境中的重要性。接下来，我们详细回顾了在四个典型教育场景中应用的深度学习技术，包括知识跟踪、学生不良行为检测、性能预测和个性化推荐。此外，我们还提供了EDM的公共数据集和处理工具的综合概述。最后，我们指出了该研究领域的新兴趋势和未来方向。

    Educational Data Mining (EDM) has emerged as a vital field of research, which harnesses the power of computational techniques to analyze educational data. With the increasing complexity and diversity of educational data, Deep Learning techniques have shown significant advantages in addressing the challenges associated with analyzing and modeling this data. This survey aims to systematically review the state-of-the-art in EDM with Deep Learning. We begin by providing a brief introduction to EDM and Deep Learning, highlighting their relevance in the context of modern education. Next, we present a detailed review of Deep Learning techniques applied in four typical educational scenarios, including knowledge tracing, undesirable student detecting, performance prediction, and personalized recommendation. Furthermore, a comprehensive overview of public datasets and processing tools for EDM is provided. Finally, we point out emerging trends and future directions in this research area.
    
[^95]: RLSynC: 离线-在线强化学习用于合成方法的合成物补全

    RLSynC: Offline-Online Reinforcement Learning for Synthon Completion. (arXiv:2309.02671v1 [cs.LG])

    [http://arxiv.org/abs/2309.02671](http://arxiv.org/abs/2309.02671)

    RLSynC是一种离线-在线强化学习方法，用于半模板化逆向合成中的合成物补全。它使用多个代理同时完成合成物的补全，并通过正向合成模型评估反应物的合成能力来指导行动搜索。

    

    逆向合成是确定能够反应形成所需产物的一组反应物分子的过程。半模板化逆向合成方法首先预测产物中的反应中心，然后将生成的合成物重新补全成反应物。这些方法能够提供必要的可解释性和高实用性，以指导合成规划。我们开发了一种新的离线-在线强化学习方法RLSynC，用于半模板化方法中的合成物补全。RLSynC为每个合成物分配一个代理，所有代理都通过同步进行逐步行动，完成合成物的补全。RLSynC通过同时进行离线训练和在线交互来学习策略，从而可以探索新的反应空间。RLSynC使用正向合成模型来评估预测的反应物在合成产物时的可能性，从而指导行动搜索。

    Retrosynthesis is the process of determining the set of reactant molecules that can react to form a desired product. Semi-template-based retrosynthesis methods, which imitate the reverse logic of synthesis reactions, first predict the reaction centers in the products, and then complete the resulting synthons back into reactants. These methods enable necessary interpretability and high practical utility to inform synthesis planning. We develop a new offline-online reinforcement learning method RLSynC for synthon completion in semi-template-based methods. RLSynC assigns one agent to each synthon, all of which complete the synthons by conducting actions step by step in a synchronized fashion. RLSynC learns the policy from both offline training episodes and online interactions which allow RLSynC to explore new reaction spaces. RLSynC uses a forward synthesis model to evaluate the likelihood of the predicted reactants in synthesizing a product, and thus guides the action search. We compare 
    
[^96]: 联邦正交训练：减轻连续联邦学习中的全局灾难性遗忘

    Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning. (arXiv:2309.01289v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01289](http://arxiv.org/abs/2309.01289)

    本研究提出了一种名为联邦正交训练（FOT）的方法，用于解决连续联邦学习中的全局灾难性遗忘问题，该方法克服了现有方法对过去数据的不切实际假设和隐私原则的违反。

    

    联邦学习（FL）因其能够实现分散数据上的隐私保护训练而受到了极大的关注。当前联邦学习领域的文献主要集中在单任务学习上。然而，随着时间的推移，客户端可能会出现新的任务，全局模型应该在不遗忘之前任务的情况下学习这些任务。这种真实场景被称为连续联邦学习（CFL）。CFL面临的主要挑战是全局灾难性遗忘，即当全局模型在新任务上训练时，其在旧任务上的性能下降。近期有一些关于CFL的研究提出了解决全局灾难性遗忘问题的方法。然而，这些方法要么对过去数据样本的可用性做出了不切实际的假设，要么违反了FL的隐私原则。我们提出了一种新方法，联邦正交训练（FOT），以克服这些缺点并解决全局灾难性遗忘问题。

    Federated Learning (FL) has gained significant attraction due to its ability to enable privacy-preserving training over decentralized data. Current literature in FL mostly focuses on single-task learning. However, over time, new tasks may appear in the clients and the global model should learn these tasks without forgetting previous tasks. This real-world scenario is known as Continual Federated Learning (CFL). The main challenge of CFL is Global Catastrophic Forgetting, which corresponds to the fact that when the global model is trained on new tasks, its performance on old tasks decreases. There have been a few recent works on CFL to propose methods that aim to address the global catastrophic forgetting problem. However, these works either have unrealistic assumptions on the availability of past data samples or violate the privacy principles of FL. We propose a novel method, Federated Orthogonal Training (FOT), to overcome these drawbacks and address the global catastrophic forgetting
    
[^97]: 串联熟练操作策略实现长程操作的顺序灵巧性

    Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation. (arXiv:2309.00987v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2309.00987](http://arxiv.org/abs/2309.00987)

    本研究提出了一种基于强化学习的通用系统，名为顺序灵巧性，用于解决由一系列互不相同的子任务组成的长程操作问题。该系统通过串联多个熟练策略来实现长程任务目标，并具备自主策略切换和绕过多余阶段的能力。实验结果表明，该系统在真实世界中表现出了良好的泛化性能。

    

    许多现实世界中的操作任务由一系列互不相同的子任务组成。这些长程、复杂的任务凸显了熟练手的潜力，它们具有适应性和多功能性，在不需要重新抓取或使用外部工具的情况下能够无缝地在不同功能模式之间过渡。然而，由于熟练手的高维行动空间和长程任务的复杂组合动力学，产生了挑战。我们提出了一种基于强化学习（RL）的通用系统——顺序灵巧性，通过串联多个熟练策略来实现长程任务目标。该系统的核心是一个渐进调优子策略的过渡可行性函数，用于增强串联成功率，并且还能够实现自主策略切换以应对失败和绕过多余的阶段。尽管只在仿真环境中训练了几个任务对象，该系统在真实世界中表现出了很好的泛化性能。

    Many real-world manipulation tasks consist of a series of subtasks that are significantly different from one another. Such long-horizon, complex tasks highlight the potential of dexterous hands, which possess adaptability and versatility, capable of seamlessly transitioning between different modes of functionality without the need for re-grasping or external tools. However, the challenges arise due to the high-dimensional action space of dexterous hand and complex compositional dynamics of the long-horizon tasks. We present Sequential Dexterity, a general system based on reinforcement learning (RL) that chains multiple dexterous policies for achieving long-horizon task goals. The core of the system is a transition feasibility function that progressively finetunes the sub-policies for enhancing chaining success rate, while also enables autonomous policy-switching for recovery from failures and bypassing redundant stages. Despite being trained only in simulation with a few task objects, 
    
[^98]: 关于多智能体相互作用的微分博弈、最优控制和基于能量的模型之间的联系

    On a Connection between Differential Games, Optimal Control, and Energy-based Models for Multi-Agent Interactions. (arXiv:2308.16539v1 [cs.RO])

    [http://arxiv.org/abs/2308.16539](http://arxiv.org/abs/2308.16539)

    本论文研究了微分博弈、最优控制和基于能量的模型之间的联系，并提出了基于能量的潜在博弈的新的端到端学习应用，通过神经网络和可微分的博弈论优化层的组合来提高预测性能。

    

    博弈论提供了一个可解释的数学框架，用于建模多智能体的相互作用。然而，在现实世界中的机器人应用中，博弈论的适用性受到多个挑战的阻碍，比如未知的智能体偏好和目标。为了解决这些挑战，我们展示了微分博弈、最优控制和基于能量的模型之间的联系，并展示了如何将现有方法统一到我们提出的基于能量的潜在博弈的形式化中。在这个形式化的基础上，本文引入了一种新的端到端学习应用，将神经网络用于博弈参数推断，并通过可微分的博弈论优化层作为归纳偏好。使用模拟的移动机器人行人相互作用和真实世界中的自动驾驶数据的实验证据表明，博弈论层改善了各种神经网络主干的预测性能。

    Game theory offers an interpretable mathematical framework for modeling multi-agent interactions. However, its applicability in real-world robotics applications is hindered by several challenges, such as unknown agents' preferences and goals. To address these challenges, we show a connection between differential games, optimal control, and energy-based models and demonstrate how existing approaches can be unified under our proposed Energy-based Potential Game formulation. Building upon this formulation, this work introduces a new end-to-end learning application that combines neural networks for game-parameter inference with a differentiable game-theoretic optimization layer, acting as an inductive bias. The experiments using simulated mobile robot pedestrian interactions and real-world automated driving data provide empirical evidence that the game-theoretic layer improves the predictive performance of various neural network backbones.
    
[^99]: DECODE: 用于检测极端质量比激发的扩张卷积神经网络

    DECODE: DilatEd COnvolutional neural network for Detecting Extreme-mass-ratio inspirals. (arXiv:2308.16422v1 [astro-ph.IM])

    [http://arxiv.org/abs/2308.16422](http://arxiv.org/abs/2308.16422)

    该论文介绍了一种名为DECODE的扩张卷积神经网络模型，用于检测极端质量比激发的信号。该模型通过在频域进行序列建模，并考虑时间延迟干涉仪以处理多通道TDI数据。

    

    由于其复杂的波形、持久的持续时间和低信噪比，极端质量比激发(EMRI)的检测是复杂的，这使得它们与紧凑的二进制融合相比更难被识别。虽然基于匹配滤波的技术以其计算要求而闻名，但现有的基于深度学习的方法主要处理时域数据，并且通常受到数据持续时间和信噪比的限制。此外，大多数现有工作忽略了时间延迟干涉仪(TDI)并在探测器响应计算中应用了长波近似，从而限制了其处理激光频率噪声的能力。在这项研究中，我们介绍了DECODE，这是一个以频域序列建模为重点的端到端模型，用于EMRI信号检测。DECODE围绕着一个以扩张因果卷积神经网络为中心，使用考虑到TDI-1.5探测器响应的合成数据进行训练，可以高效地处理一年的多通道TDI数据。

    The detection of Extreme Mass Ratio Inspirals (EMRIs) is intricate due to their complex waveforms, extended duration, and low signal-to-noise ratio (SNR), making them more challenging to be identified compared to compact binary coalescences. While matched filtering-based techniques are known for their computational demands, existing deep learning-based methods primarily handle time-domain data and are often constrained by data duration and SNR. In addition, most existing work ignores time-delay interferometry (TDI) and applies the long-wavelength approximation in detector response calculations, thus limiting their ability to handle laser frequency noise. In this study, we introduce DECODE, an end-to-end model focusing on EMRI signal detection by sequence modeling in the frequency domain. Centered around a dilated causal convolutional neural network, trained on synthetic data considering TDI-1.5 detector response, DECODE can efficiently process a year's worth of multichannel TDI data wi
    
[^100]: 基于相关性的模糊聚类有效性指标与次要选项检测器

    A correlation-based fuzzy cluster validity index with secondary options detector. (arXiv:2308.14785v1 [stat.ML])

    [http://arxiv.org/abs/2308.14785](http://arxiv.org/abs/2308.14785)

    本研究提出了一种基于相关性的模糊聚类有效性指标，该指标考虑了在聚类数量选择时可能存在的多个选项，并通过评估在多种数据集上的性能，与现有指标进行比较。

    

    应用聚类分析时，最佳聚类数量是主要关注点之一。已经引入了多个聚类有效性指标来解决这个问题。然而，在某些情况下，有多个选项可以作为最终的聚类数量。大多数现有工作在这个领域忽视了这一方面。在本研究中，我们引入了一种基于相关性的模糊聚类有效性指标，称为Wiroonsri-Preedasawakul（WP）指标。该指标根据一对数据点的实际距离与相应对的调整质心之间的距离之间的相关性来定义。我们评估并比较了我们的指标与Xie-Beni，Pakhira-Bandyopadhyay-Maulik，Tang，Wu-Li，广义C和Kwon2等几个现有指标的性能。我们在四种类型的数据集上进行了评估：人工数据集，现实世界数据集，带有等级的模拟数据集和图像数据集，使用模糊c-mea算法。

    The optimal number of clusters is one of the main concerns when applying cluster analysis. Several cluster validity indexes have been introduced to address this problem. However, in some situations, there is more than one option that can be chosen as the final number of clusters. This aspect has been overlooked by most of the existing works in this area. In this study, we introduce a correlation-based fuzzy cluster validity index known as the Wiroonsri-Preedasawakul (WP) index. This index is defined based on the correlation between the actual distance between a pair of data points and the distance between adjusted centroids with respect to that pair. We evaluate and compare the performance of our index with several existing indexes, including Xie-Beni, Pakhira-Bandyopadhyay-Maulik, Tang, Wu-Li, generalized C, and Kwon2. We conduct this evaluation on four types of datasets: artificial datasets, real-world datasets, simulated datasets with ranks, and image datasets, using the fuzzy c-mea
    
[^101]: 一个包含声道动态超声图像序列的小型词汇数据库

    A small vocabulary database of ultrasound image sequences of vocal tract dynamics. (arXiv:2308.13941v1 [cs.SD])

    [http://arxiv.org/abs/2308.13941](http://arxiv.org/abs/2308.13941)

    本文介绍了一个新的数据库，包含了连续的发声器官和声学语音数据，用超声图像和声音数据来研究言语生成过程中舌头上轮廓的可视化。这个数据库是由哥伦比亚圣坦德区的17名年轻被试完成的。

    

    本文介绍了一个包含连续的发声器官和声学语音数据的新数据库。发声器官数据对应于声道动态的超声视频，可以在言语生成过程中可视化舌头上轮廓。声学数据由定向心脏麦克风获取，包括30个短句子。该数据库包括来自哥伦比亚圣坦德区的17名年轻被试（男性8名，女性9名），他们报告没有任何言语病理问题。

    This paper presents a new database consisting of concurrent articulatory and acoustic speech data. The articulatory data correspond to ultrasound videos of the vocal tract dynamics, which allow the visualization of the tongue upper contour during the speech production process. Acoustic data is composed of 30 short sentences that were acquired by a directional cardioid microphone. This database includes data from 17 young subjects (8 male and 9 female) from the Santander region in Colombia, who reported not having any speech pathology.
    
[^102]: LMSanitator: 针对任务不可知后门的Prompt-Tuning防御机制

    LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors. (arXiv:2308.13904v1 [cs.CL])

    [http://arxiv.org/abs/2308.13904](http://arxiv.org/abs/2308.13904)

    LMSanitator是一种新颖的方法，用于检测和消除Transformer模型中的任务不可知后门。与传统方法不同，LMSanitator通过逆转预定义的攻击向量而不是触发器，实现更好的收敛性能和后门检测精确度。

    

    Prompt-Tuning已经成为一种引人注目的范式，用于部署大规模语言模型，因为它具有强大的下游任务性能和高效的多任务服务能力。尽管它被广泛采用，我们实证表明，Prompt-Tuning容易受到任务不可知后门的攻击，这些后门存在于预训练模型中，可以影响任意的下游任务。目前的后门检测方法无法防御任务不可知后门，因为它们很难在逆转后门触发器方面收敛。为了解决这个问题，我们提出了LMSanitator，一种在Transformer模型上检测和去除任务不可知后门的新方法。LMSanitator不直接逆转触发器，而是逆转预定义的攻击向量（预训练模型在输入嵌入触发器时的输出），从而实现更好的收敛性能和后门检测精确度。

    Prompt-tuning has emerged as an attractive paradigm for deploying large-scale language models due to its strong downstream task performance and efficient multitask serving ability. Despite its wide adoption, we empirically show that prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside in the pretrained models and can affect arbitrary downstream tasks. The state-of-the-art backdoor detection approaches cannot defend against task-agnostic backdoors since they hardly converge in reversing the backdoor triggers. To address this issue, we propose LMSanitator, a novel approach for detecting and removing task-agnostic backdoors on Transformer models. Instead of directly inversing the triggers, LMSanitator aims to inverse the predefined attack vectors (pretrained models' output when the input is embedded with triggers) of the task-agnostic backdoors, which achieves much better convergence performance and backdoor detection accuracy. LMSanitator further leverages prom
    
[^103]: 连续时间线性动态系统的系统识别

    System Identification for Continuous-time Linear Dynamical Systems. (arXiv:2308.11933v1 [cs.LG])

    [http://arxiv.org/abs/2308.11933](http://arxiv.org/abs/2308.11933)

    本文解决了在连续时间下观测不规则采样的情况下，Kalman滤波器的系统识别问题。通过引入连续时间Ito随机微分方程来推广Kalman滤波器的学习，并提供一个新颖的两滤波器的后验计算方法，通过贝叶斯派生获得的解析形式的后验计算方法可以高效地估计SDE的参数。

    

    Kalman滤波器的系统识别问题在学习动态系统的基础参数时，通常假设观测值在等间隔的时间点采样。然而，在许多应用中，这个假设是有限制和不切实际的。本文针对连续离散滤波器的系统识别问题，通过求解连续时间Ito随机微分方程（SDE）来推广Kalman滤波器的学习。我们引入了一个新颖的两滤波器，具有贝叶斯派生的解析形式的后验，这样可以得到不需要预先计算的正向传递的解析更新。利用这种解析的高效计算后验的方法，我们提供了一种EM过程，用于估计SDE的参数，自然地纳入了不规则采样的测量。

    The problem of system identification for the Kalman filter, relying on the expectation-maximization (EM) procedure to learn the underlying parameters of a dynamical system, has largely been studied assuming that observations are sampled at equally-spaced time points. However, in many applications this is a restrictive and unrealistic assumption. This paper addresses system identification for the continuous-discrete filter, with the aim of generalizing learning for the Kalman filter by relying on a solution to a continuous-time It\^o stochastic differential equation (SDE) for the latent state and covariance dynamics. We introduce a novel two-filter, analytical form for the posterior with a Bayesian derivation, which yields analytical updates which do not require the forward-pass to be pre-computed. Using this analytical and efficient computation of the posterior, we provide an EM procedure which estimates the parameters of the SDE, naturally incorporating irregularly sampled measurement
    
[^104]: 将大型语言模型集成到调试C编译器中以生成上下文错误解释

    Integrating Large Language Models into the Debugging C Compiler for generating contextual error explanations. (arXiv:2308.11873v1 [cs.SE])

    [http://arxiv.org/abs/2308.11873](http://arxiv.org/abs/2308.11873)

    本文介绍了一种利用大型语言模型将调试C编译器的错误解释改进的方法，并通过专家评估证明其在编译时和运行时错误解释的准确性方面的有效性。

    

    本文介绍了一种使用大型语言模型（LLM）在我们的调试C编译器（DCC）中生成增强型编译器错误解释的方法。众所周知，编译器错误消息对于初学者学习如何编程是一个障碍。虽然我们最初在入门编程（CS1）中使用DCC已经通过提供常见错误的保护机制和翻译通常含义隐晦的编译器错误消息，有助于教授C语言给初学者，但我们提出将LLM生成的解释纳入进来会进一步增强初学者的学习体验。通过专家评估，我们观察到LLM生成的编译器错误解释在90%的编译时错误和75%的运行时错误中在概念上是准确的。此外，新的DCC帮助工具已经越来越多被学生使用，每周平均有1047次独特运行。

    This paper introduces a method for Large Language Models (LLM) to produce enhanced compiler error explanations, in simple language, within our Debugging C Compiler (DCC). It is well documented that compiler error messages have been known to present a barrier for novices learning how to program. Although our initial use of DCC in introductory programming (CS1) has been instrumental in teaching C to novice programmers by providing safeguards to commonly occurring errors and translating the usually cryptic compiler error messages at both compile- and run-time, we proposed that incorporating LLM-generated explanations would further enhance the learning experience for novice programmers. Through an expert evaluation, we observed that LLM-generated explanations for compiler errors were conceptually accurate in 90% of compile-time errors, and 75% of run-time errors. Additionally, the new DCC-help tool has been increasingly adopted by students, with an average of 1047 unique runs per week, dem
    
[^105]: GPFL: 同时学习全局和个性化特征信息以实现个性化联邦学习

    GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning. (arXiv:2308.10279v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10279](http://arxiv.org/abs/2308.10279)

    这里是中文总结出的一句话要点：GPFL是一种新的个性化联邦学习方法，它能够同时学习全局和个性化的特征信息，在效果、可扩展性、公平性、稳定性和隐私性方面优于其他方法，并减轻了过拟合现象，提升了准确度。

    

    联邦学习（FL）因其保护隐私和协作学习的能力而受到欢迎。最近，个性化FL（pFL）因其能够解决统计异质性并在FL中实现个性化而受到关注。然而，从特征提取的角度来看，大多数现有的pFL方法只关注在本地训练过程中提取全局或个性化的特征信息，这无法满足pFL的协作学习和个性化目标。为了解决这个问题，我们提出了一种新的pFL方法，名为GPFL，用于在每个客户端上同时学习全局和个性化的特征信息。我们在三种统计异质的设置下对六个数据集进行了大量实验，并展示了GPFL在效果、可扩展性、公平性、稳定性和隐私性方面优于十种最先进方法的优越性。此外，GPFL减轻了过拟合现象，并在准确度上超过了基准线最高8.99%。

    Federated Learning (FL) is popular for its privacy-preserving and collaborative learning capabilities. Recently, personalized FL (pFL) has received attention for its ability to address statistical heterogeneity and achieve personalization in FL. However, from the perspective of feature extraction, most existing pFL methods only focus on extracting global or personalized feature information during local training, which fails to meet the collaborative learning and personalization goals of pFL. To address this, we propose a new pFL method, named GPFL, to simultaneously learn global and personalized feature information on each client. We conduct extensive experiments on six datasets in three statistically heterogeneous settings and show the superiority of GPFL over ten state-of-the-art methods regarding effectiveness, scalability, fairness, stability, and privacy. Besides, GPFL mitigates overfitting and outperforms the baselines by up to 8.99% in accuracy.
    
[^106]: 无监督异常检测的图编码解码网络

    A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection. (arXiv:2308.07774v1 [cs.LG])

    [http://arxiv.org/abs/2308.07774](http://arxiv.org/abs/2308.07774)

    本文提出了一种无监督图编码解码模型，用于检测图中的异常节点。在编码阶段，通过设计一种新的池化机制，该模型能够根据节点的异常程度对节点进行排序。模型的池化过程具有较低的计算复杂度和更高的可解释性。

    

    许多图神经网络中的关键组件是池化操作，它旨在减小图的大小同时保留重要的结构信息。然而，大多数现有的图池化策略依赖于通过使用图神经网络层获得的分配矩阵，该矩阵具有可训练的参数，往往导致显著的计算复杂性和池化过程的缺乏可解释性。本文提出了一种无监督图编码解码模型，通过学习一种异常评分函数对节点进行排序，从而检测出图中的异常节点。在编码阶段，我们设计了一种新的池化机制，命名为LCPool，它利用局部约束线性编码进行特征编码，通过求解带有局部正则化项的最小二乘优化问题来找到聚类分配矩阵。通过在编码过程中强制执行局部约束，LCPool被设计成免费

    A key component of many graph neural networks (GNNs) is the pooling operation, which seeks to reduce the size of a graph while preserving important structural information. However, most existing graph pooling strategies rely on an assignment matrix obtained by employing a GNN layer, which is characterized by trainable parameters, often leading to significant computational complexity and a lack of interpretability in the pooling process. In this paper, we propose an unsupervised graph encoder-decoder model to detect abnormal nodes from graphs by learning an anomaly scoring function to rank nodes based on their degree of abnormality. In the encoding stage, we design a novel pooling mechanism, named LCPool, which leverages locality-constrained linear coding for feature encoding to find a cluster assignment matrix by solving a least-squares optimization problem with a locality regularization term. By enforcing locality constraints during the coding process, LCPool is designed to be free fr
    
[^107]: SynJax: JAX的结构化概率分布

    SynJax: Structured Probability Distributions for JAX. (arXiv:2308.03291v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.03291](http://arxiv.org/abs/2308.03291)

    SynJax是一个针对JAX的结构化概率分布库，通过提供高效的向量化实现解决了对于结构化对象的难以实现的问题。

    

    深度学习软件库的发展使得该领域取得了显著的进展，它使用户能够专注于建模，同时让库来处理针对现代硬件加速器进行优化执行的繁琐和耗时的任务。然而，这仅对特定类型的深度学习模型有益，例如Transformer，其基本操作易于映射到向量化计算。而对于显式考虑结构化对象（如树和分割）的模型，并没有同样的受益，因为它们需要定制的难以以向量化形式实现的算法。SynJax通过提供用于结构化分布的推理算法的高效向量化实现来直接解决这个问题，包括对齐、标记、分割、组成树和生成树的处理。使用SynJax，我们可以构建大规模的可微分模型，显式地对数据的结构进行建模。代码可在https://g中获得。

    The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form.  SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available at https://g
    
[^108]: 在边缘端的高效模型适应用于持续学习

    Efficient Model Adaptation for Continual Learning at the Edge. (arXiv:2308.02084v1 [cs.LG])

    [http://arxiv.org/abs/2308.02084](http://arxiv.org/abs/2308.02084)

    这篇论文提出了一个名为Encoder-Adaptor-Reconfigurator（EAR）框架，用于在领域漂移下进行高效的持续学习。该框架使用了固定的深度神经网络（DNN）特征编码器，并在编码器之上训练浅层网络来处理新数据。通过结合DNN和超维计算（HDC），该框架能够检测新数据是否属于分布之外（OOD），并能够识别出... (摘要内容省略)

    

    大多数机器学习系统在训练和部署过程中假设数据分布是固定和匹配的，但这通常是错误的假设。当机器学习模型部署在真实设备上时，数据分布常常会随时间变化，原因是环境因素、传感器特性和感兴趣的任务发生了变化。本文提出了一种名为Encoder-Adaptor-Reconfigurator（EAR）框架的方法，用于处理领域漂移下的高效持续学习。EAR框架利用固定的深度神经网络（DNN）特征编码器，并在编码器之上训练浅层网络来处理新数据。EAR框架能够通过将DNN与超维计算（HDC）相结合，检测出新数据是否属于分布之外（OOD），并能够识别出

    Most machine learning (ML) systems assume stationary and matching data distributions during training and deployment. This is often a false assumption. When ML models are deployed on real devices, data distributions often shift over time due to changes in environmental factors, sensor characteristics, and task-of-interest. While it is possible to have a human-in-the-loop to monitor for distribution shifts and engineer new architectures in response to these shifts, such a setup is not cost-effective. Instead, non-stationary automated ML (AutoML) models are needed. This paper presents the Encoder-Adaptor-Reconfigurator (EAR) framework for efficient continual learning under domain shifts. The EAR framework uses a fixed deep neural network (DNN) feature encoder and trains shallow networks on top of the encoder to handle novel data. The EAR framework is capable of 1) detecting when new data is out-of-distribution (OOD) by combining DNNs with hyperdimensional computing (HDC), 2) identifying l
    
[^109]: 使用组合扩散模型实现训练数据保护

    Training Data Protection with Compositional Diffusion Models. (arXiv:2308.01937v1 [cs.LG])

    [http://arxiv.org/abs/2308.01937](http://arxiv.org/abs/2308.01937)

    使用分区扩散模型（CDM）训练不同的扩散模型，并在推断时任意组合它们，实现了训练数据保护和选择性遗忘，同时还可以根据用户访问权限提供定制模型。

    

    我们引入了分区扩散模型（CDM），一种在不同数据源上训练不同扩散模型（或提示）并在推断时任意组合它们的方法。这些单独的模型可以在孤立状态下、在不同时间、在不同分布和领域上进行训练，并可以后续组合以达到与同时训练所有数据的理想模型相当的性能。此外，每个模型只包含其在训练期间接触到的数据子集的信息，可以实现多种形式的训练数据保护。特别是，CDM是第一种可以实现大规模扩散模型的选择性遗忘和持续学习的方法，并且允许根据用户访问权限提供定制模型。CDM还可以确定生成特定样本的数据子集的重要性。

    We introduce Compartmentalized Diffusion Models (CDM), a method to train different diffusion models (or prompts) on distinct data sources and arbitrarily compose them at inference time. The individual models can be trained in isolation, at different times, and on different distributions and domains and can be later composed to achieve performance comparable to a paragon model trained on all data simultaneously. Furthermore, each model only contains information about the subset of the data it was exposed to during training, enabling several forms of training data protection. In particular, CDMs are the first method to enable both selective forgetting and continual learning for large-scale diffusion models, as well as allowing serving customized models based on the user's access rights. CDMs also allow determining the importance of a subset of the data in generating particular samples.
    
[^110]: 可解释的深度学习用于肿瘤动力建模和使用神经-ODE进行整体生存预测

    Explainable Deep Learning for Tumor Dynamic Modeling and Overall Survival Prediction using Neural-ODE. (arXiv:2308.01362v1 [q-bio.QM])

    [http://arxiv.org/abs/2308.01362](http://arxiv.org/abs/2308.01362)

    该论文介绍了一种可解释的深度学习方法，使用神经-ODE进行肿瘤动力建模和整体生存预测。该方法能够从截断数据中进行无偏预测，并提供了一种融合多模态数据的有原则的方式。

    

    虽然肿瘤动力建模已被广泛应用于支持肿瘤药物的开发，但仍然需要增加预测能力，实现个性化治疗并改善决策。我们提出使用肿瘤动力神经-ODE（TDNODE）作为一种药理学信息的神经网络，以从纵向肿瘤大小数据中实现模型发现。我们展示了TDNODE在克服现有模型的一个关键限制上的能力，即能够从截断数据中进行无偏预测。编码器-解码器架构设计用于表达具有时间的广义齐次性这一基本特性的基础动力学定律。因此，建模形式使得编码器输出可以被解释为动力学速率指标，其中倒数时间作为物理单位。我们展示了生成的指标可以高准确度地用于预测患者的整体生存。所提出的建模形式为融合多模态数据提供了一个有原则的方式。

    While tumor dynamic modeling has been widely applied to support the development of oncology drugs, there remains a need to increase predictivity, enable personalized therapy, and improve decision-making. We propose the use of Tumor Dynamic Neural-ODE (TDNODE) as a pharmacology-informed neural network to enable model discovery from longitudinal tumor size data. We show that TDNODE overcomes a key limitation of existing models in its ability to make unbiased predictions from truncated data. The encoder-decoder architecture is designed to express an underlying dynamical law which possesses the fundamental property of generalized homogeneity with respect to time. Thus, the modeling formalism enables the encoder output to be interpreted as kinetic rate metrics, with inverse time as the physical unit. We show that the generated metrics can be used to predict patients' overall survival (OS) with high accuracy. The proposed modeling formalism provides a principled way to integrate multimodal d
    
[^111]: 从零开始发现适应性符号算法

    Discovering Adaptable Symbolic Algorithms from Scratch. (arXiv:2307.16890v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2307.16890](http://arxiv.org/abs/2307.16890)

    提出了一种从零开始发现零-shot可适应策略的方法，通过演化模块化策略来构建具有线性寄存器机的控制算法，并根据环境变化即时调整模型参数和推理算法。在逼真的仿真四足机器人上演示了该方法的有效性，并在名为Cataclysmic Cartpole的非稳态控制任务上进行了详细分析。

    

    在现实世界中部署的自主机器人将需要能够快速适应环境变化的控制策略。为此，我们提出了AutoRobotics-Zero（ARZ），这是一种基于AutoML-Zero的方法，可以从零开始发现零-shot可适应策略。与只优化模型参数的神经网络适应策略不同，ARZ可以构建具有线性寄存器机的完全表达能力的控制算法。我们演化模块化策略，通过调整模型参数和即时修改推理算法来适应突发环境变化。我们在一个逼真的仿真四足机器人上演示了我们的方法，通过演化出避免在单个肢体突然断裂时摔倒的安全控制策略。这是一个具有挑战性的任务，两种流行的神经网络基准方法都失败了。最后，我们对一项名为Cataclysmic Cartpole的新颖而具有挑战性的非稳态控制任务进行了详细分析。结果证实了我们的发现。

    Autonomous robots deployed in the real world will need control policies that rapidly adapt to environmental changes. To this end, we propose AutoRobotics-Zero (ARZ), a method based on AutoML-Zero that discovers zero-shot adaptable policies from scratch. In contrast to neural network adaptation policies, where only model parameters are optimized, ARZ can build control algorithms with the full expressive power of a linear register machine. We evolve modular policies that tune their model parameters and alter their inference algorithm on-the-fly to adapt to sudden environmental changes. We demonstrate our method on a realistic simulated quadruped robot, for which we evolve safe control policies that avoid falling when individual limbs suddenly break. This is a challenging task in which two popular neural network baselines fail. Finally, we conduct a detailed analysis of our method on a novel and challenging non-stationary control task dubbed Cataclysmic Cartpole. Results confirm our findi
    
[^112]: Med-HALT:大规模语言模型中医疗领域幻觉测试

    Med-HALT: Medical Domain Hallucination Test for Large Language Models. (arXiv:2307.15343v1 [cs.CL])

    [http://arxiv.org/abs/2307.15343](http://arxiv.org/abs/2307.15343)

    Med-HALT是一个新的基准和数据集，用于评估和减少大规模语言模型中医疗领域的幻觉问题。这个数据集包括多种创新的测试模式，并评估了领先的LLMs在性能上的差异。

    

    本研究论文关注大规模语言模型（LLMs）中幻觉问题的挑战，特别是在医疗领域的背景下。幻觉指这些模型生成了合理但未经验证或错误的信息，这可能对医疗应用产生严重影响。我们提出了一个新的基准和数据集，Med-HALT（医疗领域幻觉测试），专门设计用于评估和减少幻觉。Med-HALT提供了一个多元化的跨国数据集，这些数据集来自不同国家的医疗检查，包括多种创新的测试模式。Med-HALT包括两类测试：推理和基于记忆的幻觉测试，旨在评估LLMs的问题解决和信息检索能力。我们的研究评估了文本Davinci，GPT-3.5，LlaMa-2，MPT和Falcon等领先的LLMs，揭示了它们在性能上的显著差异。这篇论文提供了有关数据集的详细见解，促进了进一步的研究和发展。

    This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs's problem-solving and information retrieval abilities.  Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting
    
[^113]: 神经Schr\"odinger桥接配对Sinkhorn损失：应用于胶体自组装的数据驱动最小工作控制

    Neural Schr\"odinger Bridge with Sinkhorn Losses: Application to Data-driven Minimum Effort Control of Colloidal Self-assembly. (arXiv:2307.14442v1 [math.OC] CROSS LISTED)

    [http://arxiv.org/abs/2307.14442](http://arxiv.org/abs/2307.14442)

    本文介绍了一种基于神经网络的Schr\"odinger桥接算法，应用于胶体自组装的最小工作控制问题中。与现有方法相比，该方法考虑了胶体自组装中的非线性控制系数，并提出了一种数据驱动的学习和控制框架。

    

    我们展示了胶体自组装的最小工作控制可以自然地在次序参数空间中以广义Schr\"odinger桥接问题的形式表达 - 这是一类在30年代末Erwin Schr\"odinger的作品中起源的固定时域随机最优控制问题。在近年来，这类问题在控制和机器学习社区中重新兴起了研究活动。与现有文献中关于这类问题的理论和计算有所不同，胶体自组装的控制漂移和扩散系数通常在控制方面是非线性的，且难以从基于物理建模中获得。我们推导了这类广义问题的最优性条件，并展示了导致的方程系统在结构上与现有结果完全不同，标准的计算方法不再适用。出于这个动机，我们提出了一个数据驱动的学习和控制框架。

    We show that the minimum effort control of colloidal self-assembly can be naturally formulated in the order-parameter space as a generalized Schr\"odinger bridge problem -- a class of fixed-horizon stochastic optimal control problems that originated in the works of Erwin Schr\"odinger in the early 1930s. In recent years, this class of problems has seen a resurgence of research activities in control and machine learning communities. Different from the existing literature on the theory and computation for such problems, the controlled drift and diffusion coefficients for colloidal self-assembly are typically non-affine in control, and are difficult to obtain from physics-based modeling. We deduce the conditions of optimality for such generalized problems, and show that the resulting system of equations is structurally very different from the existing results in a way that standard computational approaches no longer apply. Thus motivated, we propose a data-driven learning and control fram
    
[^114]: QAmplifyNet：使用可解释的混合量子-经典神经网络推动供应链缺货预测的边界

    QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum-Classical Neural Network. (arXiv:2307.12906v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12906](http://arxiv.org/abs/2307.12906)

    QAmplifyNet是一种使用量子启发技术的混合量子-经典神经网络，能够高效预测供应链缺货。它在处理短时间、不平衡数据集方面表现出色，并具有良好的可解释性。

    

    供应链管理依赖于准确的缺货预测，以优化库存控制，降低成本并提高客户满意度。然而，传统的机器学习模型在处理大规模数据集和复杂关系方面存在困难，限制了实际数据收集。本研究介绍了一种新的供应链缺货预测方法论框架，解决了处理大规模数据集的挑战。我们提出的模型QAmplifyNet在一个量子-经典混合神经网络中使用了量子启发式技术，能够在短时间和不平衡数据集上有效预测缺货。在基准数据集上的实验评估显示，QAmplifyNet在处理短时间、不平衡数据集方面优于传统模型、量子集成、量子神经网络和深度强化学习。其在处理短时间、不平衡数据集的能力使其成为供应链管理的理想解决方案。为了增强模型的可解释性，我们使用可解释的人工智能技术。

    Supply chain management relies on accurate backorder prediction for optimizing inventory control, reducing costs, and enhancing customer satisfaction. However, traditional machine-learning models struggle with large-scale datasets and complex relationships, hindering real-world data collection. This research introduces a novel methodological framework for supply chain backorder prediction, addressing the challenge of handling large datasets. Our proposed model, QAmplifyNet, employs quantum-inspired techniques within a quantum-classical neural network to predict backorders effectively on short and imbalanced datasets. Experimental evaluations on a benchmark dataset demonstrate QAmplifyNet's superiority over classical models, quantum ensembles, quantum neural networks, and deep reinforcement learning. Its proficiency in handling short, imbalanced datasets makes it an ideal solution for supply chain management. To enhance model interpretability, we use Explainable Artificial Intelligence 
    
[^115]: Temporal Graph Benchmark的实证评估

    An Empirical Evaluation of Temporal Graph Benchmark. (arXiv:2307.12510v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12510](http://arxiv.org/abs/2307.12510)

    本文对Temporal Graph Benchmark进行了实证评估，通过扩展动态图库(DyGLib)到TGB，并使用十一种动态图学习方法进行全面比较。实验发现不同模型在不同数据集上表现出不同的性能，一些基线方法的性能可以通过使用DyGLib显著提高。

    

    本文通过将我们的动态图库(DyGLib)扩展到Temporal Graph Benchmark (TGB)，对TGB进行了实证评估。与TGB相比，我们包括了十一种流行的动态图学习方法进行更全面的比较。通过实验，我们发现：（1）不同模型在不同数据集上表现出不同的性能，这与之前的观察一致；（2）使用DyGLib时，一些基线方法的性能可以显著提高。本工作旨在方便研究人员在TGB上评估各种动态图学习方法，并试图提供可直接参考的结果供后续研究使用。本项目中使用的所有资源均可在https://github.com/yule-BUAA/DyGLib_TGB上公开获取。本工作正在进行中，欢迎社区提供反馈以进行改进。

    In this paper, we conduct an empirical evaluation of Temporal Graph Benchmark (TGB) by extending our Dynamic Graph Library (DyGLib) to TGB. Compared with TGB, we include eleven popular dynamic graph learning methods for more exhaustive comparisons. Through the experiments, we find that (1) different models depict varying performance across various datasets, which is in line with previous observations; (2) the performance of some baselines can be significantly improved over the reported results in TGB when using DyGLib. This work aims to ease the researchers' efforts in evaluating various dynamic graph learning methods on TGB and attempts to offer results that can be directly referenced in the follow-up research. All the used resources in this project are publicly available at https://github.com/yule-BUAA/DyGLib_TGB. This work is in progress, and feedback from the community is welcomed for improvements.
    
[^116]: 集成学习用于混合格网卫星和测量降水数据

    Ensemble learning for blending gridded satellite and gauge-measured precipitation data. (arXiv:2307.06840v1 [cs.LG])

    [http://arxiv.org/abs/2307.06840](http://arxiv.org/abs/2307.06840)

    本研究填补论文领域中的空白，提出了11个新的集成学习算法并对其进行了广泛的比较，旨在改进卫星降水产品的准确性。

    

    在改善卫星降水产品准确性方面，常常使用回归算法。在这种情况下，基于地面的测量是因变量，卫星数据是预测变量，还有地形因素。与此同时，各个领域越来越认识到通过集成学习将多个算法结合起来可以显著提高预测性能。然而，目前文献中缺乏足够数量的集成学习算法以提高卫星降水产品的准确性，并对它们进行大规模比较。本文通过在整个美国和15年时间段内进行广泛比较，填补了这个特定的空白，提出了11个新的集成学习算法。我们使用PERSIANN（使用人工神经网络的遥感信息估计降水）和IMERG（多星联合反演估计降水）的月度数据。

    Regression algorithms are regularly used for improving the accuracy of satellite precipitation products. In this context, ground-based measurements are the dependent variable and the satellite data are the predictor variables, together with topography factors. Alongside this, it is increasingly recognised in many fields that combinations of algorithms through ensemble learning can lead to substantial predictive performance improvements. Still, a sufficient number of ensemble learners for improving the accuracy of satellite precipitation products and their large-scale comparison are currently missing from the literature. In this work, we fill this specific gap by proposing 11 new ensemble learners in the field and by extensively comparing them for the entire contiguous United States and for a 15-year period. We use monthly data from the PERSIANN (Precipitation Estimation from Remotely Sensed Information using Artificial Neural Networks) and IMERG (Integrated Multi-satellitE Retrievals f
    
[^117]: 差分私有的解耦图卷积用于多粒度拓扑保护

    Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection. (arXiv:2307.06422v1 [cs.LG])

    [http://arxiv.org/abs/2307.06422](http://arxiv.org/abs/2307.06422)

    本论文提出了一种差分私有的解耦图卷积方法，用于多粒度拓扑保护。引入了图差分隐私框架，可以确保模型参数和预测的私密性。

    

    图学习方法，如基于图卷积的图神经网络（GNNs），在解决涉及图结构数据的实际学习问题方面非常成功。然而，图学习方法不仅通过其模型参数，还通过其模型预测暴露了敏感的用户信息和交互。因此，仅提供模型权重隐私的标准差分隐私（DP）技术是不充分的。这尤其适用于通过图卷积直接利用相邻节点属性进行节点预测的情况，这会带来额外的隐私泄露风险。为了解决这个问题，我们引入了图差分隐私（GDP），这是一个新的适用于图学习环境的形式化差分隐私框架，可以确保模型参数和预测都是可证明的私有的。此外，由于节点属性和图结构可能存在不同的隐私要求，我们引入了一种新颖的放松的节点层次隐私概念。

    Graph learning methods, such as Graph Neural Networks (GNNs) based on graph convolutions, are highly successful in solving real-world learning problems involving graph-structured data. However, graph learning methods expose sensitive user information and interactions not only through their model parameters but also through their model predictions. Consequently, standard Differential Privacy (DP) techniques that merely offer model weight privacy are inadequate. This is especially the case for node predictions that leverage neighboring node attributes directly via graph convolutions that create additional risks of privacy leakage. To address this problem, we introduce Graph Differential Privacy (GDP), a new formal DP framework tailored to graph learning settings that ensures both provably private model parameters and predictions. Furthermore, since there may be different privacy requirements for the node attributes and graph structure, we introduce a novel notion of relaxed node-level da
    
[^118]: 具有编码数据结构的变分量子回归算法

    Variational quantum regression algorithm with encoded data structure. (arXiv:2307.03334v1 [quant-ph])

    [http://arxiv.org/abs/2307.03334](http://arxiv.org/abs/2307.03334)

    本文介绍了一个具有编码数据结构的变分量子回归算法，在量子机器学习中具有模型解释性，并能有效地处理互连度较高的量子比特。算法通过压缩编码和数字-模拟门操作，大大提高了在噪声中尺度量子计算机上的运行时间复杂度。

    

    变分量子算法(VQAs)被广泛应用于解决实际问题，如组合优化、量子化学模拟、量子机器学习和噪声量子计算机上的量子错误纠正。对于变分量子机器学习，尚未开发出将模型解释性内嵌到算法中的变分算法。本文构建了一个量子回归算法，并确定了变分参数与学习回归系数之间的直接关系，同时采用了将数据直接编码为反映经典数据表结构的量子幅度的电路。该算法特别适用于互连度较高的量子比特。通过压缩编码和数字-模拟门操作，运行时间复杂度在数据输入量编码的情况下对数级更有优势，显著提升了噪声中尺度量子计算机的性能。

    Variational quantum algorithms (VQAs) prevail to solve practical problems such as combinatorial optimization, quantum chemistry simulation, quantum machine learning, and quantum error correction on noisy quantum computers. For variational quantum machine learning, a variational algorithm with model interpretability built into the algorithm is yet to be exploited. In this paper, we construct a quantum regression algorithm and identify the direct relation of variational parameters to learned regression coefficients, while employing a circuit that directly encodes the data in quantum amplitudes reflecting the structure of the classical data table. The algorithm is particularly suitable for well-connected qubits. With compressed encoding and digital-analog gate operation, the run time complexity is logarithmically more advantageous than that for digital 2-local gate native hardware with the number of data entries encoded, a decent improvement in noisy intermediate-scale quantum computers a
    
[^119]: 严格约束应用中的AutoML

    AutoML in Heavily Constrained Applications. (arXiv:2306.16913v1 [cs.LG])

    [http://arxiv.org/abs/2306.16913](http://arxiv.org/abs/2306.16913)

    本文提出了Caml，一种在严格约束的应用中使用元学习的AutoML方法。Caml能够自动适应特定任务的AutoML参数，并考虑用户定义的约束，生成满足约束且具有高预测性能的流程。

    

    为了优化特定任务的机器学习流程，需要对各种超参数进行仔细配置，通常由AutoML系统支持，该系统优化给定训练数据集的超参数。然而，根据AutoML系统的二阶元配置，AutoML过程的性能可能会有很大差异。目前的AutoML系统无法自动适应特定用例的配置。此外，它们也无法编译用户定义的应用约束，以确保流程及其生成的有效性和效率。在本文中，我们提出了Caml，它使用元学习自动适应其自身的AutoML参数，比如搜索策略、验证策略和搜索空间，以适应特定的任务。Caml的动态AutoML策略考虑用户定义的约束，并获得具有高预测性能的满足约束的流程。

    Optimizing a machine learning pipeline for a task at hand requires careful configuration of various hyperparameters, typically supported by an AutoML system that optimizes the hyperparameters for the given training dataset. Yet, depending on the AutoML system's own second-order meta-configuration, the performance of the AutoML process can vary significantly. Current AutoML systems cannot automatically adapt their own configuration to a specific use case. Further, they cannot compile user-defined application constraints on the effectiveness and efficiency of the pipeline and its generation. In this paper, we propose Caml, which uses meta-learning to automatically adapt its own AutoML parameters, such as the search strategy, the validation strategy, and the search space, for a task at hand. The dynamic AutoML strategy of Caml takes user-defined constraints into account and obtains constraint-satisfying pipelines with high predictive performance.
    
[^120]: CLUE: 离线强化学习的校准潜在导向

    CLUE: Calibrated Latent Guidance for Offline Reinforcement Learning. (arXiv:2306.13412v1 [cs.LG])

    [http://arxiv.org/abs/2306.13412](http://arxiv.org/abs/2306.13412)

    CLUE使用条件可变自编码器实现专家数据的内在奖励，消除了离线强化学习中其它繁重的外在奖励工作。

    

    离线强化学习旨在从预先收集和标记的数据集中学习最优策略，消除了在线强化学习中耗时的数据收集过程。但是，离线强化学习仍然需要确定和制定每个数据转换的外在奖励，这仍然是一个繁重的工作。为了解决这个问题，我们提出了CLUE：通过使用一些专家数据为离线强化学习任务提供内在奖励来消除外在奖励的需求。为了实现这一点，我们引入了一种条件可变自编码器来学习一个潜在空间，使得内在奖励可以直接在潜在空间中进行评估。CLUE的关键思想是通过将专家数据的嵌入强制转换为校准的上下文表示，使内在奖励与专家意图保持一致。我们验证了专家驱动的内在奖励在多个环境中的有效性。

    Offline reinforcement learning (RL) aims to learn an optimal policy from pre-collected and labeled datasets, which eliminates the time-consuming data collection in online RL. However, offline RL still bears a large burden of specifying/handcrafting extrinsic rewards for each transition in the offline data. As a remedy for the labor-intensive labeling, we propose to endow offline RL tasks with a few expert data and utilize the limited expert data to drive intrinsic rewards, thus eliminating the need for extrinsic rewards. To achieve that, we introduce \textbf{C}alibrated \textbf{L}atent g\textbf{U}idanc\textbf{E} (CLUE), which utilizes a conditional variational auto-encoder to learn a latent space such that intrinsic rewards can be directly qualified over the latent space. CLUE's key idea is to align the intrinsic rewards consistent with the expert intention via enforcing the embeddings of expert data to a calibrated contextual representation. We instantiate the expert-driven intrinsic 
    
[^121]: 基于几何深度学习的结构药物设计系统综述

    A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design. (arXiv:2306.11768v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.11768](http://arxiv.org/abs/2306.11768)

    本文在系统回顾几何深度学习在结构药物设计中的最新进展，分别讨论了不同任务并按不同的几何深度学习方法进行组织。该领域的前景看好，但仍存在挑战。

    

    结构药物设计利用蛋白质的三维几何结构来识别潜在的药物候选物，在药物发现中变得越来越重要。然而，基于物理化学建模和专家领域知识的传统方法费时费力。近年来，几何深度学习的发展，可以处理和整合三维几何数据，加上类似AlphaFold的工具提供准确的蛋白质三维结构预测，极大地推动了结构药物设计的进展。在本文中，我们系统地回顾了几何深度学习在结构药物设计中的最新进展。我们从结构药物设计中的主流任务、常用的3D蛋白质表示和预测/生成模型入手，然后详细介绍每个任务的回顾（例如结合位点预测、结合构象生成、\emph{de novo} 分子设计等），并按不同的几何深度学习方法进行组织。最后，我们总结了该领域未来研究的挑战和前景。

    Structure-based drug design (SBDD), which utilizes the three-dimensional geometry of proteins to identify potential drug candidates, is becoming increasingly vital in drug discovery. However, traditional methods based on physiochemical modeling and experts' domain knowledge are time-consuming and laborious. The recent advancements in geometric deep learning, which integrates and processes 3D geometric data, coupled with the availability of accurate protein 3D structure predictions from tools like AlphaFold, have significantly propelled progress in structure-based drug design. In this paper, we systematically review the recent progress of geometric deep learning for structure-based drug design. We start with a brief discussion of the mainstream tasks in structure-based drug design, commonly used 3D protein representations and representative predictive/generative models. Then we delve into detailed reviews for each task (binding site prediction, binding pose generation, \emph{de novo} mo
    
[^122]: 正则化鲁棒的MDPs和风险敏感的MDPs：等价性、策略梯度和样本复杂度

    Regularized Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy Gradient, and Sample Complexity. (arXiv:2306.11626v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2306.11626](http://arxiv.org/abs/2306.11626)

    本论文研究了正则化鲁棒MDP问题和风险敏感MDP问题的相关性，并提出了有效的学习算法和样本复杂度分析。

    

    本论文关注于正则化鲁棒马尔可夫决策过程（MDP）问题的强化学习，它是鲁棒MDP框架的一个扩展。我们首先介绍了风险敏感MDP，并建立了风险敏感MDP和正则化鲁棒MDP之间的等价性。这种等价性为解决正则化RMDP提供了另一种视角，并且使得设计高效的学习算法成为可能。在这种等价性的基础上，我们进一步推导了正则化鲁棒MDP问题的策略梯度定理，并在具有直接参数化的表格设置下证明了精确策略梯度方法的全局收敛性。我们还提出了一种基于样本的离线学习算法，即鲁棒的FZI迭代，用于具有KL散度正则化项的特定正则化鲁棒MDP问题，并分析了算法的样本复杂度。我们的结果也得到了数值模拟的支持。

    This paper focuses on reinforcement learning for the regularized robust Markov decision process (MDP) problem, an extension of the robust MDP framework. We first introduce the risk-sensitive MDP and establish the equivalence between risk-sensitive MDP and regularized robust MDP. This equivalence offers an alternative perspective for addressing the regularized RMDP and enables the design of efficient learning algorithms. Given this equivalence, we further derive the policy gradient theorem for the regularized robust MDP problem and prove the global convergence of the exact policy gradient method under the tabular setting with direct parameterization. We also propose a sample-based offline learning algorithm, namely the robust fitted-Z iteration (RFZI), for a specific regularized robust MDP problem with a KL-divergence regularization term and analyze the sample complexity of the algorithm. Our results are also supported by numerical simulations.
    
[^123]: 超越正常：关于互信息估计的评估

    Beyond Normal: On the Evaluation of Mutual Information Estimators. (arXiv:2306.11078v1 [stat.ML])

    [http://arxiv.org/abs/2306.11078](http://arxiv.org/abs/2306.11078)

    本文提出了一种语言无关的互信息估计基准平台，并讨论了经典和神经估计器在处理高维数据、长尾分布和高互信息时的普适性和局限性。

    

    互信息是一种常用的统计相关度量，已在表示学习、因果性、域泛化和计算生物学等领域得到应用。然而，互信息估计通常只在简单的概率分布族类（即多元正态分布和具有一维随机变量的选择分布）上进行评估。在本文中，我们展示了如何构建具有已知基准互信息的各种分布族，并提出了一种语言无关的互信息估计基准平台。我们讨论了经典和神经网络估计器在涉及高维度、稀疏相互作用、长尾分布和高互信息的情境中的普适性和局限性。最后，我们为从业人员提供了选择适当的估计器以适应所考虑问题难度和应用估计互信息时需要考虑的问题的指南。

    Mutual information is a general statistical dependency measure which has found applications in representation learning, causality, domain generalization and computational biology. However, mutual information estimators are typically evaluated on simple families of probability distributions, namely multivariate normal distribution and selected distributions with one-dimensional random variables. In this paper, we show how to construct a diverse family of distributions with known ground-truth mutual information and propose a language-independent benchmarking platform for mutual information estimators. We discuss the general applicability and limitations of classical and neural estimators in settings involving high dimensions, sparse interactions, long-tailed distributions, and high mutual information. Finally, we provide guidelines for practitioners on how to select appropriate estimator adapted to the difficulty of problem considered and issues one needs to consider when applying an est
    
[^124]: NAR-Former V2：重新思考Transformer用于通用神经网络表示学习

    NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning. (arXiv:2306.10792v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10792](http://arxiv.org/abs/2306.10792)

    本文重新思考了Transformer和图神经网络在网络表示学习中的不同特性，并提出了一个修改后的基于Transformer的通用神经网络表示学习模型NAR-Former V2。

    

    随着越来越多的深度学习模型被应用于现实世界的应用，对于建模和学习神经网络本身的表示的需求也越来越大。高效的表示可以用于预测网络的目标属性，而无需实际训练和部署过程，从而促进高效的网络部署和设计。最近，受到Transformer的成功启发，一些基于Transformer的表示学习框架已被提出，并在处理基于细胞结构的模型方面取得了有希望的性能。然而，图神经网络（GNN）方法仍然主导了整个网络的学习表示领域。在本文中，我们重新审视Transformer，并与GNN进行比较，分析它们不同的架构特性。然后，我们提出了一个修改后的基于Transformer的通用神经网络表示学习模型NAR-Former V2。

    As more deep learning models are being applied in real-world applications, there is a growing need for modeling and learning the representations of neural networks themselves. An efficient representation can be used to predict target attributes of networks without the need for actual training and deployment procedures, facilitating efficient network deployment and design. Recently, inspired by the success of Transformer, some Transformer-based representation learning frameworks have been proposed and achieved promising performance in handling cell-structured models. However, graph neural network (GNN) based approaches still dominate the field of learning representation for the entire network. In this paper, we revisit Transformer and compare it with GNN to analyse their different architecture characteristics. We then propose a modified Transformer-based universal neural network representation learning model NAR-Former V2. It can learn efficient representations from both cell-structured
    
[^125]: DIFFender：基于扩散的对抗性防御方法用于抵御Patch攻击

    DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks. (arXiv:2306.09124v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.09124](http://arxiv.org/abs/2306.09124)

    DIFFender是一种基于扩散的对抗性防御方法，通过定位和恢复两个阶段的操作，利用文本引导的扩散模型来防御对抗性Patch，从而提高其整体防御性能。

    

    对抗性攻击，尤其是Patch攻击，对深度学习模型的鲁棒性和可靠性构成了重大威胁。开发可靠的防御方法以抵御Patch攻击对于实际应用至关重要，然而当前在这个领域的研究还不令人满意。在本文中，我们提出了DIFFender，一种新颖的防御方法，它利用文本引导的扩散模型来防御对抗性Patch。DIFFender包括两个主要阶段：Patch定位和Patch恢复。在定位阶段，我们发现并利用了扩散模型的一个有趣特性，以有效地识别对抗性Patch的位置。在恢复阶段，我们利用扩散模型重建图像中的对抗性区域同时保持视觉内容的完整性。重要的是，这两个阶段都受到统一的扩散模型的精心引导，因此我们可以利用它们之间的紧密相互作用来提高整个防御性能。

    Adversarial attacks, particularly patch attacks, pose significant threats to the robustness and reliability of deep learning models. Developing reliable defenses against patch attacks is crucial for real-world applications, yet current research in this area is not satisfactory. In this paper, we propose DIFFender, a novel defense method that leverages a text-guided diffusion model to defend against adversarial patches. DIFFender includes two main stages: patch localization and patch restoration. In the localization stage, we find and exploit an intriguing property of the diffusion model to effectively identify the locations of adversarial patches. In the restoration stage, we employ the diffusion model to reconstruct the adversarial regions in the images while preserving the integrity of the visual content. Importantly, these two stages are carefully guided by a unified diffusion model, thus we can utilize the close interaction between them to improve the whole defense performance. Mor
    
[^126]: 关于结构预测中的认证泛化

    On Certified Generalization in Structured Prediction. (arXiv:2306.09112v1 [stat.ML])

    [http://arxiv.org/abs/2306.09112](http://arxiv.org/abs/2306.09112)

    该论文提出了一种新的结构化预测PAC-Bayesian风险界限，它可以随着结构化示例的数量和大小的变化而进行泛化，为使用生成模型建立结构化预测的泛化界限迈出了第一步。

    

    在结构预测中，目标对象具有丰富的内部结构，这种结构无法分解为独立的组件，并违反了常见的独立同分布假设。这一挑战在应用程序中表现为指数级的输出空间，如图像分割或场景图生成。我们提出了一种新的结构化预测PAC-Bayesian风险界限，其中泛化速率不仅随着结构化示例的数量而且还随着它们的大小而变化。基本假设符合生成模型上的最新研究，即数据由分解参考度量的Knothe-Rosenblatt重新排列生成。这使得我们可以将随机输出变量之间的结构显式地提取到Wasserstein依赖矩阵中。我们的工作为利用强大的生成模型在结构预测这种具有挑战性的情况下建立判别式下游任务的泛化界限迈出了初步的一步。

    In structured prediction, target objects have rich internal structure which does not factorize into independent components and violates common i.i.d. assumptions. This challenge becomes apparent through the exponentially large output space in applications such as image segmentation or scene graph generation. We present a novel PAC-Bayesian risk bound for structured prediction wherein the rate of generalization scales not only with the number of structured examples but also with their size. The underlying assumption, conforming to ongoing research on generative models, is that data are generated by the Knothe-Rosenblatt rearrangement of a factorizing reference measure. This allows to explicitly distill the structure between random output variables into a Wasserstein dependency matrix. Our work makes a preliminary step towards leveraging powerful generative models to establish generalization bounds for discriminative downstream tasks in the challenging setting of structured prediction.
    
[^127]: 实现合成主动推理代理，第一部分：认识目标和图形说明语言

    Realising Synthetic Active Inference Agents, Part I: Epistemic Objectives and Graphical Specification Language. (arXiv:2306.08014v1 [cs.AI])

    [http://arxiv.org/abs/2306.08014](http://arxiv.org/abs/2306.08014)

    本文介绍了自由能原理和主动推理的理论框架，并推导了适用于任意图形模型的主动推理版本。同时引入了一种新的图形说明语言（GSL）来明确规定系统目标。

    

    自由能原理（FEP）是一种描述系统如何通过最小化自由能泛函而自组织成具有连贯性、稳定结构（智能）的理论框架。主动推理（AIF）是FEP的一个推论，它明确了能够为未来进行规划（代理）的系统是如何通过最小化包含信息寻求组件的特定自由能泛函来运作的。本文是一个系列中的第一篇，我们在自由形式因子图上推导了AIF的合成版本。本文重点推导了AIF所使用的自由能泛函的局部版本。这使我们能够构造一个适用于任意图形模型并与有关消息传递算法的先前工作接口的AIF版本。结果消息是在我们的伴侣论文中得出的。我们还发现因子图形式中存在一个缺口。虽然因子图表达了生成模型，但在指定系统目标方面缺乏一个图形化语言。我们引入了一个因子图描述法的新扩展，称为图形说明语言（GSL），它使系统目标得到明确规定。

    The Free Energy Principle (FEP) is a theoretical framework for describing how (intelligent) systems self-organise into coherent, stable structures by minimising a free energy functional. Active Inference (AIF) is a corollary of the FEP that specifically details how systems that are able to plan for the future (agents) function by minimising particular free energy functionals that incorporate information seeking components. This paper is the first in a series of two where we derive a synthetic version of AIF on free form factor graphs. The present paper focuses on deriving a local version of the free energy functionals used for AIF. This enables us to construct a version of AIF which applies to arbitrary graphical models and interfaces with prior work on message passing algorithms. The resulting messages are derived in our companion paper. We also identify a gap in the graphical notation used for factor graphs. While factor graphs are great at expressing a generative model, they have so
    
[^128]: 一通适用于参数高效微调的通用LoRA算法

    One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning. (arXiv:2306.07967v1 [cs.LG])

    [http://arxiv.org/abs/2306.07967](http://arxiv.org/abs/2306.07967)

    本论文提出了一种通用的参数高效微调算法——GLoRA，该算法通过广义提示模块、模块化的适配器层和可扩展的结构搜索具有了对不同任务和数据集的更高灵活性和适应性，并在各类基准测试中表现出了优异的精度。

    

    本文提出了一种先进的通用参数高效微调任务算法——广义LoRA（GLoRA）。GLoRA 使用广义提示模块来优化预训练模型权重和调整中间激活状态，从而提供了更多的灵活性和跨异构任务和数据集的能力。此外，GLoRA 通过使用可扩展的、模块化的、层次的结构搜索来帮助有效的参数调整，学习每个层的适配器，从一个统一的数学公式起源，GLoRA 具有强大的迁移学习、少样本学习和领域泛化能力，通过权重和激活状态上的附加维度来适应新任务。综合实验表明，在自然、专业和结构化基准测试中，GLoRA 的精度优于所有先前的方法，且在各种数据集上使用更少的参数和计算达到了优越的精度。此外，我们的结构重新设计可以大幅减少运算时间和模型大小。

    We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adjusts to new tasks through additional dimensions on weights and activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured benchmarks, achieving superior accuracy with fewer parameters and computations on various datasets. Furthermore, our structural re-par
    
[^129]: 如何从短短三分钟的数据中学习和推广：受限于物理学和不确定性感知的神经随机微分方程

    How to Learn and Generalize From Three Minutes of Data: Physics-Constrained and Uncertainty-Aware Neural Stochastic Differential Equations. (arXiv:2306.06335v1 [cs.LG])

    [http://arxiv.org/abs/2306.06335](http://arxiv.org/abs/2306.06335)

    本文提出了一种使用神经随机微分方程学习控制动力学模型的方法，其中利用了物理知识作为归纳偏置，并在设计中利用距离感知的估计不确定性，在小数据集上进行训练，同时可以在长时间范围内进行准确预测，可用于模型预测控制和模型基础增强学习。

    

    我们提出了一个框架和算法，使用神经随机微分方程（SDEs）来学习受控动力学模型。我们构建漂移项，利用先验的物理知识作为归纳偏置，并设计扩散项来表示学习模型预测中的不确定性的距离感知估计——当在接近训练数据集状态的状态下评估时，它匹配系统的基本随机性，并且当在训练范围之外的状态下评估时，它会预测高度随机的动态。提出的神经SDEs可以快速评估，以用于模型预测控制算法，也可以用作模型基础增强学习的模拟器。此外，即使在覆盖状态空间有限区域的小数据集上进行训练，它们也可以在长时间范围内进行准确预测。我们通过控制小型无人机和模拟机器腿的运动（考虑到模型不确定性和仿真到现实的差异）以及模拟建筑的热力学动态（考虑到缺失数据并在小数据集上进行训练），展示了这些功能。

    We present a framework and algorithms to learn controlled dynamics models using neural stochastic differential equations (SDEs) -- SDEs whose drift and diffusion terms are both parametrized by neural networks. We construct the drift term to leverage a priori physics knowledge as inductive bias, and we design the diffusion term to represent a distance-aware estimate of the uncertainty in the learned model's predictions -- it matches the system's underlying stochasticity when evaluated on states near those from the training dataset, and it predicts highly stochastic dynamics when evaluated on states beyond the training regime. The proposed neural SDEs can be evaluated quickly enough for use in model predictive control algorithms, or they can be used as simulators for model-based reinforcement learning. Furthermore, they make accurate predictions over long time horizons, even when trained on small datasets that cover limited regions of the state space. We demonstrate these capabilities th
    
[^130]: 基于结合多样化奖励微调权重插值的帕累托最优对齐的奖励汤

    Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. (arXiv:2306.04488v1 [cs.LG])

    [http://arxiv.org/abs/2306.04488](http://arxiv.org/abs/2306.04488)

    本文提出了 rewarded soup 方法，通过结合多种代理奖励，实现微调权重插值，从而在整个偏好空间中实现帕累托最优广义化。该方法在强化学习任务上具有有效性。

    

    基础模型首先在大量无参考数据集上进行预训练，然后在有标注的数据上进行微调。强化学习，特别是来自人类反馈的强化学习(RLHF)，可以进一步使网络与预期的使用相匹配。然而，代理奖励的缺陷可能会妨碍训练，导致次优结果；现实任务和人类意见的多样性加剧了这个问题。本文提出通过采用多策略方法来拥抱多样化奖励的异质性。我们的目标不是专注于单一的先验奖励，而是在整个偏好空间中实现帕累托最优广义化。为此，我们提出了 rewarded soup，首先独立地专门化多个网络(每个代理奖励一个)，然后在它们的权重之间进行线性插值。通过实验表明，这种方法是成功的，因为我们展示了当多样化奖励来自共享的预训练初始化时，权重仍然保持线性连接。我们展示了该方法在强化学习任务上的有效性。

    Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effective
    
[^131]: 在大脑模型中进行序列计算

    Computation with Sequences in a Model of the Brain. (arXiv:2306.03812v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2306.03812](http://arxiv.org/abs/2306.03812)

    这篇论文在大脑模型中展示了如何通过捕获时间序列来进行一系列关于神经元集群的计算。

    

    尽管机器学习在许多应用上超过了人类水平，但是大脑学习能力的普遍性、稳健性和快速性仍然无法匹敌。认知如何产生于神经活动是神经科学中一个核心的悬而未决的问题，与智能研究密不可分。在Papadimitriou [2020]中提出了一个简单的神经活动形式模型，并通过数学证明和模拟显示可以利用神经元集群的创建和操纵来实现某些简单的认知操作。然而，许多智能行为依赖于能够识别、存储和操作时间序列的刺激（例如计划、语言、导航）。在这里，我们展示了在同一模型中，通过突触权重和可塑性，时间可以自然地以优先顺序进行捕获，从而可以进行一系列关于神经元集群序列的计算。

    Even as machine learning exceeds human-level performance on many applications, the generality, robustness, and rapidity of the brain's learning capabilities remain unmatched. How cognition arises from neural activity is a central open question in neuroscience, inextricable from the study of intelligence itself. A simple formal model of neural activity was proposed in Papadimitriou [2020] and has been subsequently shown, through both mathematical proofs and simulations, to be capable of implementing certain simple cognitive operations via the creation and manipulation of assemblies of neurons. However, many intelligent behaviors rely on the ability to recognize, store, and manipulate temporal sequences of stimuli (planning, language, navigation, to list a few). Here we show that, in the same model, time can be captured naturally as precedence through synaptic weights and plasticity, and, as a result, a range of computations on sequences of assemblies can be carried out. In particular, r
    
[^132]: 揭示图神经网络中的结构差异性：一个尺码适用于所有吗？

    Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?. (arXiv:2306.01323v1 [cs.LG])

    [http://arxiv.org/abs/2306.01323](http://arxiv.org/abs/2306.01323)

    本研究证明GNN在同构图中的同构节点和异构图中的异构节点上表现良好，而在另一组节点上表现不佳。该研究提出了一种新框架，通过使用加权聚合提高GNN在不同结构模式节点上的表现，有效解决结构差异性问题。

    

    近期关于图神经网络（GNN）的研究提供了实证和理论证据，支持它们在捕捉同构和某些异构图上的结构模式方面的有效性。值得注意的是，大多数实际中的同构和异构图都由同构和异构结构模式的混合节点组成，表现出一定的结构差异性。然而，关于不同结构模式下的节点（例如在异构图中的同构节点）在GNN分类任务中的表现分析仍然很有限。本文通过理论和实证研究证明，GNN在同构图中的同构节点和异构图中的异构节点上的表现通常是出色的，而在另一组节点上表现不佳，表现出性能差异性。我们进一步识别了测试展示不同结构模式节点时GNN的效应，并提出了一种通过使用GNN的加权聚合以适应性结构差异性的新框架的解决方案。在各种数据集上的实验表明，所提出的方法在解决结构差异性和提高节点分类任务的性能方面是有效的。

    Recent studies on Graph Neural Networks(GNNs) provide both empirical and theoretical evidence supporting their effectiveness in capturing structural patterns on both homophilic and certain heterophilic graphs. Notably, most real-world homophilic and heterophilic graphs are comprised of a mixture of nodes in both homophilic and heterophilic structural patterns, exhibiting a structural disparity. However, the analysis of GNN performance with respect to nodes exhibiting different structural patterns, e.g., homophilic nodes in heterophilic graphs, remains rather limited. In the present study, we provide evidence that Graph Neural Networks(GNNs) on node classification typically perform admirably on homophilic nodes within homophilic graphs and heterophilic nodes within heterophilic graphs while struggling on the opposite node set, exhibiting a performance disparity. We theoretically and empirically identify effects of GNNs on testing nodes exhibiting distinct structural patterns. We then pr
    
[^133]: SnapFusion：移动设备上两秒内的文本到图像扩散模型

    SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds. (arXiv:2306.00980v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.00980](http://arxiv.org/abs/2306.00980)

    本文提出了一种通用方法，首次在移动设备上运行文本到图像扩散模型不到2秒，这是通过引入高效的网络架构和改进步骤蒸馏来实现的。

    

    文本到图像扩散模型可以从自然语言描述中创建出惊人的图像，不亚于专业艺术家和摄影师的作品。然而，这些模型较大，具有复杂的网络架构和数十个去噪迭代，使其计算昂贵且运行缓慢。因此，需要高端GPU和基于云的推理来按比例运行扩散模型。这是昂贵的，并且涉及隐私问题，尤其是当用户数据发送到第三方时。为了克服这些挑战，我们提出了一种通用方法，首次在不到2秒钟内解锁了在移动设备上运行文本到图像扩散模型，通过引入高效的网络架构和改进步骤蒸馏来实现此目标。

    Text-to-image diffusion models can create stunning images from natural language descriptions that rival the work of professional artists and photographers. However, these models are large, with complex network architectures and tens of denoising iterations, making them computationally expensive and slow to run. As a result, high-end GPUs and cloud-based inference are required to run diffusion models at scale. This is costly and has privacy implications, especially when user data is sent to a third party. To overcome these challenges, we present a generic approach that, for the first time, unlocks running text-to-image diffusion models on mobile devices in less than $2$ seconds. We achieve so by introducing efficient network architecture and improving step distillation. Specifically, we propose an efficient UNet by identifying the redundancy of the original model and reducing the computation of the image decoder via data distillation. Further, we enhance the step distillation by explori
    
[^134]: 基于谱嵌入的深度学习研究

    Going Deeper with Spectral Embeddings. (arXiv:2306.00742v1 [cs.LG])

    [http://arxiv.org/abs/2306.00742](http://arxiv.org/abs/2306.00742)

    本文提出两种新的谱嵌入方法，一种基于函数分析原理和核方法，另一种基于深度网络优化损失，提供理论保证和实际有效的算法，并提供新的采样算法。

    

    为了有效地处理海量的数据，从而更好地对其进行表征，科学家们采用表示学习。最近，这些方法与一些底层运算的谱分解之间展现出明显的联系。在历史上，是通过在数据的顶部构建图形来建立明确的谱嵌入，而我们提出了两种新的方法：一种基于函数分析原理和核方法构建的，这将导致具有理论保证的算法，另一种基于深度网络训练以优化基本变分损失的算法，它们产生了实际有效的算法。此外，我们提供了一种新的采样算法，利用学习到的表征来在一步中生成新的样本。

    To make sense of millions of raw data and represent them efficiently, practitioners rely on representation learning. Recently, deep connections have been shown between these approaches and the spectral decompositions of some underlying operators. Historically, explicit spectral embeddings were built from graphs constructed on top of the data. In contrast, we propose two new methods to build spectral embeddings: one based on functional analysis principles and kernel methods, which leads to algorithms with theoretical guarantees, and the other based on deep networks trained to optimize principled variational losses, which yield practically efficient algorithms. Furthermore, we provide a new sampling algorithm that leverages learned representations to generate new samples in a single step.
    
[^135]: 基于联邦学习的代码异味检测方法(FedCSD)

    FedCSD: A Federated Learning Based Approach for Code-Smell Detection. (arXiv:2306.00038v1 [cs.SE])

    [http://arxiv.org/abs/2306.00038](http://arxiv.org/abs/2306.00038)

    本文提出了一种名为FedCSD的基于联邦学习的代码异味检测方法，可以在保护数据隐私的同时，让组织协作训练联邦学习模型。该方法在三个实验中分别使用不同的数据集，实现了高精度的检测效果，并且比集中式和交叉验证方法具有更好的性能表现。

    

    本文提出了一种名为FedCSD的基于联邦学习的代码异味检测方法，可以在保护数据隐私的同时，让组织协作训练联邦学习模型。通过三个实验来支持这些断言，这些实验利用了三个手动验证的数据集，来检测和研究不同的代码异味场景。

    This paper proposes a Federated Learning Code Smell Detection (FedCSD) approach that allows organizations to collaboratively train federated ML models while preserving their data privacy. These assertions have been supported by three experiments that have significantly leveraged three manually validated datasets aimed at detecting and examining different code smell scenarios. In experiment 1, which was concerned with a centralized training experiment, dataset two achieved the lowest accuracy (92.30%) with fewer smells, while datasets one and three achieved the highest accuracy with a slight difference (98.90% and 99.5%, respectively). This was followed by experiment 2, which was concerned with cross-evaluation, where each ML model was trained using one dataset, which was then evaluated over the other two datasets. Results from this experiment show a significant drop in the model's accuracy (lowest accuracy: 63.80\%) where fewer smells exist in the training dataset, which has a noticeab
    
[^136]: 评估地理空间背景信息在出行方式检测中的应用

    Evaluating geospatial context information for travel mode detection. (arXiv:2305.19428v1 [physics.soc-ph])

    [http://arxiv.org/abs/2305.19428](http://arxiv.org/abs/2305.19428)

    本研究确定了与相关工作有关的背景表示法，并基于随机森林模型和SHAP方法评估了地理空间背景信息在出行方式检测中的贡献，实验结果表明描述与基础设施网络关系的特征对预测有显着贡献。

    

    利用全球定位卫星系统（GNSS）轨迹检测出行方式对了解个人出行行为至关重要，是实现可持续交通系统的先决条件。虽然研究已经认识到将地理空间背景信息纳入出行方式检测模型中的好处，但很少有文章总结了背景建模方法，并分析了这些背景特征的重要性，这妨碍了高效模型的开发。本文确定了与相关工作有关的背景表示法，并提出了一种分析管道，基于随机森林模型和SHapley Additive exPlanation（SHAP）方法评估地理空间背景信息在出行方式检测中的贡献。通过在大规模GNSS跟踪数据集上的实验，我们发现描述与基础设施网络关系的特征，例如到铁路或道路网络的距离，对模型的预测有显着贡献。

    Detecting travel modes from global navigation satellite system (GNSS) trajectories is essential for understanding individual travel behaviour and a prerequisite for achieving sustainable transport systems. While studies have acknowledged the benefits of incorporating geospatial context information into travel mode detection models, few have summarized context modelling approaches and analyzed the significance of these context features, hindering the development of an efficient model. Here, we identify context representations from related work and propose an analytical pipeline to assess the contribution of geospatial context information for travel mode detection based on a random forest model and the SHapley Additive exPlanation (SHAP) method. Through experiments on a large-scale GNSS tracking dataset, we report that features describing relationships with infrastructure networks, such as the distance to the railway or road network, significantly contribute to the model's prediction. Mo
    
[^137]: 广义因果敏感性分析的尖锐界限

    Sharp Bounds for Generalized Causal Sensitivity Analysis. (arXiv:2305.16988v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16988](http://arxiv.org/abs/2305.16988)

    本文提出了一个统一的框架用于广义因果敏感性分析，通过提出一个灵活的边际敏感性模型，推导出各种因果效应的尖锐界限。该框架适用于多种设置，包括离散、连续和时变的治疗。

    

    从观察数据中进行因果推断对于许多学科如医学和经济学至关重要。然而，在放松无混淆性假设（因果敏感性分析）下的因果效应的尖锐界限仍在研究中。到目前为止，具有尖锐界限的工作仅限于相对简单的设置（例如，单一二元治疗）。在本文中，我们提出了一个统一的框架，用于各种设置下的未观察混淆因素下的因果敏感性分析。为此，我们提出了边际敏感性模型（MSM）的灵活推广，然后推导出大类因果效应的尖锐界限。这包括（条件）平均处理效应，中介分析和路径分析的效应，以及分布效应。此外，我们的敏感性模型适用于离散、连续和时变的治疗。它使我们能够将未观察混淆导致的部分识别问题解释为分布偏移的问题。

    Causal inference from observational data is crucial for many disciplines such as medicine and economics. However, sharp bounds for causal effects under relaxations of the unconfoundedness assumption (causal sensitivity analysis) are subject to ongoing research. So far, works with sharp bounds are restricted to fairly simple settings (e.g., a single binary treatment). In this paper, we propose a unified framework for causal sensitivity analysis under unobserved confounding in various settings. For this, we propose a flexible generalization of the marginal sensitivity model (MSM) and then derive sharp bounds for a large class of causal effects. This includes (conditional) average treatment effects, effects for mediation analysis and path analysis, and distributional effects. Furthermore, our sensitivity model is applicable to discrete, continuous, and time-varying treatments. It allows us to interpret the partial identification problem under unobserved confounding as a distribution shift
    
[^138]: 扩散模型的并行采样

    Parallel Sampling of Diffusion Models. (arXiv:2305.16317v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16317](http://arxiv.org/abs/2305.16317)

    本文提出了一种新方法，ParaDiGMS，可以通过并行处理多个步骤来加速预训练扩散模型的采样。ParaDiGMS是第一个使计算速度和采样效率实现平衡的扩散采样方法，并与现有方法兼容。

    

    扩散模型是强大的生成模型，但采样速度缓慢，通常需要进行1000次顺序去噪步骤才能得到一个样本。因此，本文探索了一种交换计算机处理速度和采样效率的方法。通过猜测未来的去噪步骤的解决方案并逐步细化至收敛的Picard迭代，我们展示了惊人的发现：尽管去噪步骤有顺序性，但仍然可以并行采样。基于这一洞见，我们提出了ParaDiGMS，这是一种通过以并行方式去噪多个步骤加速预训练扩散模型采样的新方法。ParaDiGMS是第一个在计算处理速度和采样效率上实现平衡的扩散采样方法，甚至还兼容现有的方法。

    Diffusion models are powerful generative models but suffer from slow sampling, often taking 1000 sequential denoising steps for one sample. As a result, considerable efforts have been directed toward reducing the number of denoising steps, but these methods hurt sample quality. Instead of reducing the number of denoising steps (trading quality for speed), in this paper we explore an orthogonal approach: can we run the denoising steps in parallel (trading compute for speed)? In spite of the sequential nature of the denoising steps, we show that surprisingly it is possible to parallelize sampling via Picard iterations, by guessing the solution of future denoising steps and iteratively refining until convergence. With this insight, we present ParaDiGMS, a novel method to accelerate the sampling of pretrained diffusion models by denoising multiple steps in parallel. ParaDiGMS is the first diffusion sampling method that enables trading compute for speed and is even compatible with existing 
    
[^139]: 对大型语言模型的对抗性示范攻击

    Adversarial Demonstration Attacks on Large Language Models. (arXiv:2305.14950v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14950](http://arxiv.org/abs/2305.14950)

    本文研究了对大型语言模型进行对抗性示范攻击的安全问题，并提出了一种新的攻击方法advICL，通过改变示范而不改变输入来误导模型。实验结果表明，随着示范数量的增加，上下文学习的鲁棒性降低。

    

    随着更强大的大型语言模型（LLMs）的出现，如ChatGPT和GPT-4，上下文学习（ICL）通过利用数据-标签对作为预先条件提示，已经在利用这些模型进行特定任务方面获得显著的重要性。虽然引入示范可以大大提高LLMs在各种任务上的性能，但它也可能引入新的安全问题：攻击者可以仅仅操纵示范而不改变输入来进行攻击。在本文中，我们从对抗性的角度调查了ICL的安全问题，重点关注示范的影响。我们提出了一种名为advICL的新型攻击方法，旨在仅仅改变示范而不改变输入以误导模型。我们的结果表明，随着示范数量的增加，上下文学习的鲁棒性将会降低。此外，我们还发现示范的固有特性是可以被使用的。

    With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (
    
[^140]: 寻找多头注意力的支柱

    Finding the Pillars of Strength for Multi-Head Attention. (arXiv:2305.14380v1 [cs.LG])

    [http://arxiv.org/abs/2305.14380](http://arxiv.org/abs/2305.14380)

    本研究提出聚合头注意力(Grouped Head Attention)，使用自监督组约束进行训练，为注意力头进行分组，其中每个组专注于一个重要而独特的特征子集。此方法可以缓解MHA的冗余性和过度参数化问题，并导致更有效和高效的MHA，进而在基准测试中取得了性能提升。

    

    最近的研究揭示了多头注意力(Multi-Head Attention, MHA)的一些问题，例如冗余性和过度参数化。具体而言，MHA的头最初设计为从不同的表征子空间中关注信息，然而，先前的研究发现一些注意力头可能学习类似的特征，并且可以通过修剪来提高效率而不会损害性能。受最小冗余特征选择的启发，我们假设聚焦于最具代表性和独特性的特征，并最小化资源消耗，可以缓解上述问题，并导致更有效和高效的MHA。具体地，我们提出了聚合头注意力(Grouped Head Attention)，使用自监督组约束进行训练，为注意力头进行分组，其中每个组专注于一个重要而独特的特征子集。此外，我们还提出了一种投票保留程序(Voting-to-Stay)，以删除冗余头，从而实现具有更轻量级权重的转换器。此外，我们的方法在三个知名基准测试中取得了显著的性能提升，并且我们的消融研究提供了支持性的分析。

    Recent studies have revealed some issues of Multi-Head Attention (MHA), e.g., redundancy and over-parameterization. Specifically, the heads of MHA were originally designed to attend to information from different representation subspaces, whereas prior studies found that some attention heads likely learn similar features and can be pruned without harming performance. Inspired by the minimum-redundancy feature selection, we assume that focusing on the most representative and distinctive features with minimum resources can mitigate the above issues and lead to more effective and efficient MHAs. In particular, we propose Grouped Head Attention, trained with a self-supervised group constraint that group attention heads, where each group focuses on an essential but distinctive feature subset. We additionally propose a Voting-to-Stay procedure to remove redundant heads, thus achieving a transformer with lighter weights. Moreover, our method achieves significant performance gains on three well
    
[^141]: VIP5：面向推荐的多模态基础模型

    VIP5: Towards Multimodal Foundation Models for Recommendation. (arXiv:2305.14302v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2305.14302](http://arxiv.org/abs/2305.14302)

    VIP5是一个多模态基础模型，通过统一图像、文本和个性化模态，实现了多模态的共享架构，提高了推荐系统的效果。

    

    计算机视觉（CV）、自然语言处理（NLP）和推荐系统（RecSys）是三个重要的人工智能应用，它们传统上独立发展，导致了不同的建模和工程方法。这妨碍了这些领域直接从彼此的进展中受益。随着基础模型的最新发展，大型语言模型已经成为统一不同模态和问题表述的潜在通用接口。基于此，我们提出了开发一个多模态基础模型（MFM），考虑了图像、文本和个性化模态，在P5推荐范式下统一各种模态和推荐任务，因此命名为VIP5（Visual P5），以改进推荐功能。为了实现这一目标，我们引入多模态个性化提示来适应多个模态。

    Computer Vision (CV), Natural Language Processing (NLP), and Recommender Systems (RecSys) are three prominent AI applications that have traditionally developed independently, resulting in disparate modeling and engineering methodologies. This has impeded the ability for these fields to directly benefit from each other's advancements. With the recent development of foundation models, large language models have emerged as a potential general-purpose interface for unifying different modalities and problem formulations. In light of this, we propose the development of a multimodal foundation model (MFM) considering visual, textual, and personalization modalities under the P5 recommendation paradigm, thus named VIP5 (Visual P5), to unify various modalities and recommendation tasks. This will enable the processing of multiple modalities in a shared architecture for improved recommendations. To achieve this, we introduce multimodal personalized prompts to accommodate multiple modalities under 
    
[^142]: 具有覆盖保证的分摊变分推断

    Amortized Variational Inference with Coverage Guarantees. (arXiv:2305.14275v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2305.14275](http://arxiv.org/abs/2305.14275)

    提出了一种称为CANVI的方法，通过构建一致化预测器并使用预测效率进行比较，来提供具有保证的后验近似结果。该方法可以快速计算，易于实现，并且对于候选近似器的设计决策无需关注。此外，CANVI能够在无似然的情况下使用。

    

    分摊变分推断产生了一个后验近似，可以快速计算给定任何新观测。然而，对于这些近似后验的质量，很少有保证。我们提出了一种称为CANVI的一致化分摊神经变分推断的方法，该方法可扩展、易于实现，并提供了保证的边际覆盖。给定一系列候选的分摊后验近似器，CANVI基于每个候选构建一致化预测器，使用预测效率这个度量标准比较预测器，并返回最高效的预测器。CANVI确保所得到的预测器构建的区域以用户指定的概率水平包含真实值。CANVI对候选近似器的制定决策不关心，并且只需要访问前向模型的样本，可以在无似然的情况下使用。我们证明了预测效率的下界。

    Amortized variational inference produces a posterior approximation that can be rapidly computed given any new observation. Unfortunately, there are few guarantees about the quality of these approximate posteriors. We propose Conformalized Amortized Neural Variational Inference (CANVI), a procedure that is scalable, easily implemented, and provides guaranteed marginal coverage. Given a collection of candidate amortized posterior approximators, CANVI constructs conformalized predictors based on each candidate, compares the predictors using a metric known as predictive efficiency, and returns the most efficient predictor. CANVI ensures that the resulting predictor constructs regions that contain the truth with a user-specified level of probability. CANVI is agnostic to design decisions in formulating the candidate approximators and only requires access to samples from the forward model, permitting its use in likelihood-free settings. We prove lower bounds on the predictive efficiency of t
    
[^143]: 基于度量学习提高视觉语音识别中正常与无声语音之间的差距

    Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning. (arXiv:2305.14203v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2305.14203](http://arxiv.org/abs/2305.14203)

    本论文提出了一种基于度量学习的方法，以缩小视觉语音识别中正常和无声语音之间的差距。通过利用正常和无声语音之间的共享内容，我们的模型能够有效地学习和预测Viseme身份，从而提高了无声VSR的准确性。

    

    本文提出了一种新颖的度量学习方法，以解决视觉语音识别（VSR）中正常与无声语音之间性能差距的问题。现有的VSR模型在处理无声语音时，由于两者之间的嘴唇运动差异，导致准确性下降。为了解决这个问题并解决无声语音训练数据的稀缺性，我们提出了一种基于Viseme的度量学习方法，利用正常和无声语音之间的共享字面内容。具体而言，我们的目标是在潜在空间中将两种语音类型的输入彼此靠近，如果它们具有相似的Viseme表示。通过最小化预测的Viseme概率分布之间和两种语音类型内部的Kullback-Leibler散度，我们的模型能够有效地学习和预测Viseme身份。我们的评估结果表明，即使训练数据有限，我们的方法也能提高无声VSR的准确性。

    This paper presents a novel metric learning approach to address the performance gap between normal and silent speech in visual speech recognition (VSR). The difference in lip movements between the two poses a challenge for existing VSR models, which exhibit degraded accuracy when applied to silent speech. To solve this issue and tackle the scarcity of training data for silent speech, we propose to leverage the shared literal content between normal and silent speech and present a metric learning approach based on visemes. Specifically, we aim to map the input of two speech types close to each other in a latent space if they have similar viseme representations. By minimizing the Kullback-Leibler divergence of the predicted viseme probability distributions between and within the two speech types, our model effectively learns and predicts viseme identities. Our evaluation demonstrates that our method improves the accuracy of silent VSR, even when limited training data is available.
    
[^144]: ZeroSCROLLS：一个用于长文本理解的零Shot基准测试

    ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding. (arXiv:2305.14196v1 [cs.CL])

    [http://arxiv.org/abs/2305.14196](http://arxiv.org/abs/2305.14196)

    ZeroSCROLLS是一个用于长文本自然语言理解的零Shot基准测试，包括六个任务和四个数据集，能够评估大型语言模型的性能。当前，GPT-4的平均得分最高，但在聚合任务等多个挑战上，仍有改进的空间。

    

    我们介绍了 ZeroSCROLLS，这是一个用于长文本自然语言理解的零Shot基准测试，仅包含测试集而没有训练或开发数据。我们从SCROLLS基准测试中适应了六个任务，并添加了四个新数据集，包括两个新的信息融合任务，例如聚合正面评价的百分比。使用ZeroSCROLLS，我们对开源和闭源大型语言模型进行了全面评估，发现Claude优于ChatGPT，并且GPT-4获得了最高的平均分数。然而，在ZeroSCROLLS的多个开放挑战方面（例如，聚合任务），还有改进的空间，因为模型很难通过朴素的基准测试。由于最先进的技术还在不断更新，我们邀请研究人员在实时的ZeroSCROLLS排行榜上评估他们的想法。

    We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test sets, without training or development data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest average score. However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline. As the state of the art is a moving target, we invite researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard
    
[^145]: CoT Collection: 通过思维链条微调来提高语言模型的零样本和少样本学习能力

    The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning. (arXiv:2305.14045v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14045](http://arxiv.org/abs/2305.14045)

    本文通过思维链条微调（CoT fine-tuning）来提高小型语言模型在零样本和少样本学习中的能力，并引入了CoT Collection数据集来增强模型的逐步推理能力。实验结果表明，在未见任务和4个特定领域任务上，通过CoT fine-tuning可以显著提升模型的准确度和学习能力。

    

    已知对于小于100亿参数的语言模型，在解决未见过的任务时，其链式思维推理能力不如大型语言模型。本文旨在通过使用思维链条的置信度调整来赋予较小的语言模型逐步推理的能力。为了实现这一目标，我们首先引入了一个新的调整指令数据集——CoT Collection，该数据集通过增加184万个置信度注释到1060个任务的现有Flan Collection（包含9个思维链条任务）中。我们展示了在未见任务上使用CoT Collection对Flan-T5（3B和11B）进行思维链条微调，使得较小的语言模型能够在思维链条能力方面表现更好。在BIG-Bench-Hard（BBH）基准测试上，我们报告了零样本任务准确度方面的平均提升：+4.34%（Flan-T5 3B）和+2.60%（Flan-T5 11B）。此外，我们还展示了使用CoT Collection进行指令调整使语言模型在4个特定领域任务上具有更强的少样本学习能力。

    Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B & 11B) with CoT Collection enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of +4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with CoT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks, resulting 
    
[^146]: Group-Invariant GAN的统计保证

    Statistical Guarantees of Group-Invariant GANs. (arXiv:2305.13517v1 [stat.ML])

    [http://arxiv.org/abs/2305.13517](http://arxiv.org/abs/2305.13517)

    本研究提出了群不变GAN的统计保证，发现当学习群不变分布时，群不变GAN所需样本数会按群体大小的幂比例减少。

    

    Group-Invariant生成对抗网络(GAN)是一种GAN，其中生成器和判别器具有硬性集团对称性。实证研究表明，这些网络能够学习具有显着改进数据效率的集团不变分布。在本研究中，我们旨在通过分析群不变GAN的样本复杂度减少来严格量化这种改进。我们的研究发现，在学习群不变分布时，群不变GAN所需样本数按照群体大小的幂比例减少，这个幂取决于分布支持的本质维度。据我们所知，这项工作是首个为群不变生成模型，特别是GAN提供统计估计的工作，并可以为其他群不变生成模型的研究提供借鉴。

    Group-invariant generative adversarial networks (GANs) are a type of GANs in which the generators and discriminators are hardwired with group symmetries. Empirical studies have shown that these networks are capable of learning group-invariant distributions with significantly improved data efficiency. In this study, we aim to rigorously quantify this improvement by analyzing the reduction in sample complexity for group-invariant GANs. Our findings indicate that when learning group-invariant distributions, the number of samples required for group-invariant GANs decreases proportionally with a power of the group size, and this power depends on the intrinsic dimension of the distribution's support. To our knowledge, this work presents the first statistical estimation for group-invariant generative models, specifically for GANs, and it may shed light on the study of other group-invariant generative models.
    
[^147]: 基于一般回归误差假设来研究无噪声回归最小二乘估计值的均方误差

    The Mean Squared Error of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors. (arXiv:2305.12883v1 [math.ST])

    [http://arxiv.org/abs/2305.12883](http://arxiv.org/abs/2305.12883)

    该论文研究了基于一般回归误差假设的无噪声回归最小二乘估计值的均方误差，并发现包含大量不重要的参数可以有效地降低估计器的均方误差。

    

    近年来，最小$\ell_2$范数（无岭）插值最小二乘估计器的研究方兴未艾。然而，大多数分析都局限于简单的回归误差结构，假设误差是独立同分布的，具有零均值和相同的方差，与特征向量无关。此外，这些理论分析的主要重点是样本外预测风险。本文通过检查无岭插值最小二乘估计器的均方误差，允许更一般的回归误差假设，打破了现有文献的局限性。具体而言，我们研究过度参数化的潜在好处，通过描绘有限样本中的均方误差来表征均方误差。我们的研究结果表明，相对于样本量，包含大量不重要的参数可以有效地降低估计器的均方误差。

    In recent years, there has been a significant growth in research focusing on minimum $\ell_2$ norm (ridgeless) interpolation least squares estimators. However, the majority of these analyses have been limited to a simple regression error structure, assuming independent and identically distributed errors with zero mean and common variance, independent of the feature vectors. Additionally, the main focus of these theoretical analyses has been on the out-of-sample prediction risk. This paper breaks away from the existing literature by examining the mean squared error of the ridgeless interpolation least squares estimator, allowing for more general assumptions about the regression errors. Specifically, we investigate the potential benefits of overparameterization by characterizing the mean squared error in a finite sample. Our findings reveal that including a large number of unimportant parameters relative to the sample size can effectively reduce the mean squared error of the estimator. N
    
[^148]: 通过重新标记最小训练子集来翻转预测

    Relabel Minimal Training Subset to Flip a Prediction. (arXiv:2305.12809v1 [cs.LG])

    [http://arxiv.org/abs/2305.12809](http://arxiv.org/abs/2305.12809)

    本文利用扩展影响函数提出了一种有效的识别和重新标记最小训练子集的方法，并证明其始终能够成功翻转测试结果，同时还提供了挑战模型预测、评估模型鲁棒性和洞察训练集偏差等多重作用。

    

    Yang等人发现，仅删除1%的训练数据就可能导致预测结果翻转。鉴于机器学习模型中存在噪声数据的普遍性，本文提出了一个问题：在模型训练之前通过重新标记一个小的训练数据子集可否导致测试结果翻转？本文利用扩展影响函数提出了一种有效的识别和重新标记这种子集的方法，并证明了其始终能够产生成功的结果。这种机制有多重作用：（1）提供了一种补充方法，可以通过恢复可能错误标记的训练数据来挑战模型预测；（2）评估模型的鲁棒性，因为本文发现子集的大小与训练集中噪声数据的比例之间存在显著关系；（3）提供了洞察训练集偏差的见解。据我们所知，这项工作代表了对识别最小训练子集问题的第一次研究。

    Yang et al. (2023) discovered that removing a mere 1% of training points can often lead to the flipping of a prediction. Given the prevalence of noisy data in machine learning models, we pose the question: can we also result in the flipping of a test prediction by relabeling a small subset of the training data before the model is trained? In this paper, utilizing the extended influence function, we propose an efficient procedure for identifying and relabeling such a subset, demonstrating consistent success. This mechanism serves multiple purposes: (1) providing a complementary approach to challenge model predictions by recovering potentially mislabeled training points; (2) evaluating model resilience, as our research uncovers a significant relationship between the subset's size and the ratio of noisy data in the training set; and (3) offering insights into bias within the training set. To the best of our knowledge, this work represents the first investigation into the problem of identi
    
[^149]: 你的解释可靠吗？通过融合XAI和对抗攻击来探究LIME在解释文本分类器中的稳定性

    Are Your Explanations Reliable? Investigating the Stability of LIME in Explaining Text Classifiers by Marrying XAI and Adversarial Attack. (arXiv:2305.12351v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12351](http://arxiv.org/abs/2305.12351)

    本文研究了解释AI中常用的工具LIME在文本数据上的稳定性，并提出了一种新算法XAIFooler来扰动文本输入并操纵解释，以解决这个问题。

    

    LIME已经成为可解释AI（XAI）框架中最常被引用的工具之一，在关键的机器学习应用中集成其中，例如医疗保健和金融。然而，尤其是在文本数据的背景下，其稳定性仍然鲜为人知，这是由于文本空间的独特约束。为了解决这些挑战，本文首先评估了LIME在文本数据上固有的不稳定性，以建立基准，然后提出了一种新颖的算法XAIFooler，以扰动文本输入并操纵解释，将LIME的稳定性作为一个文本扰动优化问题进行研究。XAIFooler符合约束条件，保留了文本语义和原始预测，并引入了Rank-biased Overlap（RBO）作为XAIFooler优化的关键部分，以满足所有解释相似度测量的要求。在真实的文本数据集上进行了大量实验，证明了XAIFool的可行性。

    LIME has emerged as one of the most commonly referenced tools in explainable AI (XAI) frameworks that is integrated into critical machine learning applications--e.g., healthcare and finance. However, its stability remains little explored, especially in the context of text data, due to the unique text-space constraints. To address these challenges, in this paper, we first evaluate the inherent instability of LIME on text data to establish a baseline, and then propose a novel algorithm XAIFooler to perturb text inputs and manipulate explanations that casts investigation on the stability of LIME as a text perturbation optimization problem. XAIFooler conforms to the constraints to preserve text semantics and original prediction with small perturbations, and introduces Rank-biased Overlap (RBO) as a key part to guide the optimization of XAIFooler that satisfies all the requirements for explanation similarity measure. Extensive experiments on real-world text datasets demonstrate that XAIFool
    
[^150]: 稳定性边缘处的逻辑回归梯度下降的隐式偏差

    Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability. (arXiv:2305.11788v1 [cs.LG])

    [http://arxiv.org/abs/2305.11788](http://arxiv.org/abs/2305.11788)

    本文研究了逻辑回归常数步长梯度下降在稳定性边缘的收敛性和隐式偏差，证明了逻辑损失可以通过任何常数步长的梯度下降进行最小化，同时也发现了指数损失下的发散性问题，强调了稳定性边缘下梯度下降的不稳定性。

    

    最近的研究表明，在机器学习优化中，梯度下降 (GD) 经常在稳定性边缘 (EoS) [Cohen 等，2021] 运行，其中步长被设置为大，导致由 GD 迭代引起的非单调损失。本文研究在 EoS 区域内使用常数步长 GD 进行逻辑回归的收敛性和隐式偏差，对于线性可分的数据。尽管存在局部振荡，我们证明逻辑损失可以通过任何常数步长的 GD 在长时间尺度上进行最小化。此外，我们证明，在任何常数步长下，当投影到最大边际方向 (硬边 SVM 方向) 时，GD 迭代趋向于无穷大，并在投影到最大边缘的正交补空间时，收敛于最小化强凸势能的固定向量。相反，我们也表明，在 EoS 区域，GD 迭代可能在指数损失下发生灾难性发散，突显了 EoS 区域中 GD 的不稳定性。

    Recent research has observed that in machine learning optimization, gradient descent (GD) often operates at the edge of stability (EoS) [Cohen, et al., 2021], where the stepsizes are set to be large, resulting in non-monotonic losses induced by the GD iterates. This paper studies the convergence and implicit bias of constant-stepsize GD for logistic regression on linearly separable data in the EoS regime. Despite the presence of local oscillations, we prove that the logistic loss can be minimized by GD with any constant stepsize over a long time scale. Furthermore, we prove that with any constant stepsize, the GD iterates tend to infinity when projected to a max-margin direction (the hard-margin SVM direction) and converge to a fixed vector that minimizes a strongly convex potential when projected to the orthogonal complement of the max-margin direction. In contrast, we also show that in the EoS regime, GD iterates may diverge catastrophically under the exponential loss, highlighting t
    
[^151]: LLM自身可读取和生成CXR图像

    LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])

    [http://arxiv.org/abs/2305.11490](http://arxiv.org/abs/2305.11490)

    该论文提出了一种新方法，可以在不需要进行结构更改、额外训练、或训练专门网络的情况下，通过微调预先训练的LLM来读取和生成像文本一样的图像，并应用于胸部X线（CXR）图像的生成任务中。

    

    借助于近期大语言模型（LLMs）的显著发展，人们正积极尝试将LLMs的实用性扩展到多模态任务。已经有人尝试连接语言和视觉信息，并且也在不断尝试为LLMs添加视觉能力。然而，现有的尝试只使用LLMs作为图像解码器，没有尝试通过自然语言来生成图像。通过采用VQ-GAN框架，将图像的潜在表示视为一种文本标记，我们提出了一种新方法，可以微调预先训练的LLM，以像文本一样读取和生成图像，而无需进行结构更改、额外的训练目标或训练专门的网络，同时仍保留LLM的指令跟随能力。我们将此框架应用于胸部X线（CXR）图像的生成任务中，因为这是一个复杂信息在视觉和语言之间翻译的领域。

    Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
    
[^152]: 超越指数图：有限时间收敛的通信效率拓扑用于分散学习

    Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence. (arXiv:2305.11420v1 [cs.LG])

    [http://arxiv.org/abs/2305.11420](http://arxiv.org/abs/2305.11420)

    本文介绍了一种新型拓扑——基础$(k+1)$图，其中节点在有限的迭代次数后能达到确切的共识，具有快速共识率和小的最大度数，从而可以用于分散式SGD。

    

    近年来越来越多的研究关注于分散式学习在并行计算和隐私保护中的应用。许多最近的研究指出，具有更快共识率（即谱间隙）的底层网络拓扑可导致分散式学习的更好收敛速度和准确性。然而，具有快速共识率的拓扑，如指数图，通常具有较大的最大度数，这会导致重要的通信成本。因此，寻求既具有快速共识率又具有小的最大度数的拓扑是重要的。在本研究中，我们提出了一种结合快速共识率和小最大度的新型拓扑，称为基础$(k+1)$ 图。与现有的拓扑不同，基础$(k+1)$ 图使所有节点在有限的迭代次数后都能达到确切的共识，对于任何节点数和最大度k都适用。得益于这个有利的属性，基础$(k+1)$ 图赋予了分散式SGD

    Decentralized learning has recently been attracting increasing attention for its applications in parallel computation and privacy preservation. Many recent studies stated that the underlying network topology with a faster consensus rate (a.k.a. spectral gap) leads to a better convergence rate and accuracy for decentralized learning. However, a topology with a fast consensus rate, e.g., the exponential graph, generally has a large maximum degree, which incurs significant communication costs. Thus, seeking topologies with both a fast consensus rate and small maximum degree is important. In this study, we propose a novel topology combining both a fast consensus rate and small maximum degree called the Base-$(k + 1)$ Graph. Unlike the existing topologies, the Base-$(k + 1)$ Graph enables all nodes to reach the exact consensus after a finite number of iterations for any number of nodes and maximum degree k. Thanks to this favorable property, the Base-$(k + 1)$ Graph endows Decentralized SGD
    
[^153]: 关于一般函数逼近下的均场强化学习的统计效率

    On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])

    [http://arxiv.org/abs/2305.11283](http://arxiv.org/abs/2305.11283)

    本文研究了一般函数逼近下的均场控制(MFC)和均场博弈(MFG)中的强化学习的统计效率，提出了基于乐观最大似然估计的算法，并仅对转移动力学具有Lipschitz连续性的假设，最后建立了一个指数级的下界支持MFC设置。

    

    本文研究了一般函数逼近下的均场控制（MFC）和均场博弈（MFG）中强化学习的统计效率。引入了一种称为Mean-Field Model-Based Eluder Dimension (MBED)的新概念，包含了一系列丰富的均场强化学习问题。此外，我们提出了基于乐观最大似然估计的算法，可以返回一个$\epsilon$优的策略，适用于MFC或$\epsilon$纳什均衡策略适用于MFG，样本复杂度多项式与相关参数无关，与状态、动作和代理数量无关。值得注意的是，我们的结果仅对转移动力学具有Lipschitz连续性的假设，避免了以前的强结构假设。最后，在tabular设置下，假设有一个生成模型，我们建立了一个指数级的下界支持MFC设置，同时提供了一种新颖的样本高效的模型消除算法以逼近最优策略。

    In this paper, we study the statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MBED), which subsumes a rich family of Mean-Field RL problems. Additionally, we propose algorithms based on Optimistic Maximal Likelihood Estimation, which can return an $\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial w.r.t. relevant parameters and independent of the number of states, actions and the number of agents. Notably, our results only require a mild assumption of Lipschitz continuity on transition dynamics and avoid strong structural assumptions in previous work. Finally, in the tabular setting, given the access to a generative model, we establish an exponential lower bound for MFC setting, while providing a novel sample-efficient model elimination algorithm to approxim
    
[^154]: 在三周内为8,000个腹部CT扫描标注多器官分割

    Annotating 8,000 Abdominal CT Volumes for Multi-Organ Segmentation in Three Weeks. (arXiv:2305.09666v1 [eess.IV])

    [http://arxiv.org/abs/2305.09666](http://arxiv.org/abs/2305.09666)

    本文提出了一种高效方法在短时间内标记8000个腹部CT扫描中的8个器官，建立了迄今为止最大的多器官数据集。

    

    医学影像标注，特别是器官分割，是费时费力的。本文提出了一种系统高效的方法来加速器官分割的标注过程。我们标注了8,448个腹部CT扫描，标记了脾脏、肝脏、肾脏、胃、胆囊、胰腺、主动脉和下腔静脉。传统的标注方法需要一位经验丰富的标注员1600周，而我们的标注方法仅用了三周。

    Annotating medical images, particularly for organ segmentation, is laborious and time-consuming. For example, annotating an abdominal organ requires an estimated rate of 30-60 minutes per CT volume based on the expertise of an annotator and the size, visibility, and complexity of the organ. Therefore, publicly available datasets for multi-organ segmentation are often limited in data size and organ diversity. This paper proposes a systematic and efficient method to expedite the annotation process for organ segmentation. We have created the largest multi-organ dataset (by far) with the spleen, liver, kidneys, stomach, gallbladder, pancreas, aorta, and IVC annotated in 8,448 CT volumes, equating to 3.2 million slices. The conventional annotation methods would take an experienced annotator up to 1,600 weeks (or roughly 30.8 years) to complete this task. In contrast, our annotation method has accomplished this task in three weeks (based on an 8-hour workday, five days a week) while maintain
    
[^155]: DPMLBench：差分隐私机器学习的整体评估

    DPMLBench: Holistic Evaluation of Differentially Private Machine Learning. (arXiv:2305.05900v1 [cs.LG])

    [http://arxiv.org/abs/2305.05900](http://arxiv.org/abs/2305.05900)

    本文提出了DPMLBench框架，通过在图像分类任务上综合衡量加强DP-SGD的DPML算法的实用性和防御能力，填补了比较DPML算法改进表现的空白，提高了DPML算法的性能。

    

    差分隐私（DP）作为一种严格的数学定义，量化了隐私泄露，已成为隐私保护的一个广为接受的标准。结合强大的机器学习技术，差分隐私机器学习（DPML）变得越来越重要。然而，作为最经典的DPML算法之一，DP-SGD会造成显著的效用损失，这阻碍了DPML在实践中的部署。为了缓解这个问题，许多研究最近提出了基于DP-SGD的改进算法，但是这些研究是孤立的，无法全面衡量算法中提出的改进的表现。更重要的是，还缺乏全面研究来比较这些DPML算法的改进在效用、防御能力和泛化能力方面的表现。本文通过在图像分类任务上对改进的DPML算法进行综合测量，对实用性和防御能力进行评估，填补了这一空白。我们提出了一个具有建设性和全面性的框架DPMLBench来评估DPML算法，并将其应用于度量所提出的算法在不同的隐私预算、数据集和模型下的实用性和防御能力表现。实验结果表明我们提出的DPMLBench框架优于现有框架，同时还显示出现有最先进的DPML算法的显著改进。

    Differential privacy (DP), as a rigorous mathematical definition quantifying privacy leakage, has become a well-accepted standard for privacy protection. Combined with powerful machine learning techniques, differentially private machine learning (DPML) is increasingly important. As the most classic DPML algorithm, DP-SGD incurs a significant loss of utility, which hinders DPML's deployment in practice. Many studies have recently proposed improved algorithms based on DP-SGD to mitigate utility loss. However, these studies are isolated and cannot comprehensively measure the performance of improvements proposed in algorithms. More importantly, there is a lack of comprehensive research to compare improvements in these DPML algorithms across utility, defensive capabilities, and generalizability.  We fill this gap by performing a holistic measurement of improved DPML algorithms on utility and defense capability against membership inference attacks (MIAs) on image classification tasks. We fir
    
[^156]: 用显式和隐式知识推动AI药物发现

    Empowering AI drug discovery with explicit and implicit knowledge. (arXiv:2305.01523v1 [cs.LG])

    [http://arxiv.org/abs/2305.01523](http://arxiv.org/abs/2305.01523)

    DeepEIK是一个统一的深度学习框架，利用显式和隐式知识进行AI药物发现，通过特征融合和注意力机制来提高模型预测准确性，并在多个任务上显著优于现有方法。

    

    近年来，独立利用知识图谱中的显式知识或生物医学文献中的隐式知识进行AI药物发现的研究迅速增长。这些方法极大地提高了多个下游任务上AI模型的预测准确性。然而，独立地整合显式和隐式知识会阻碍对分子的理解。本研究提出了DeepEIK，这是一个统一的深度学习框架，结合了显式和隐式知识来进行AI药物发现。

    Motivation: Recently, research on independently utilizing either explicit knowledge from knowledge graphs or implicit knowledge from biomedical literature for AI drug discovery has been growing rapidly. These approaches have greatly improved the prediction accuracy of AI models on multiple downstream tasks. However, integrating explicit and implicit knowledge independently hinders their understanding of molecules. Results: We propose DeepEIK, a unified deep learning framework that incorporates both explicit and implicit knowledge for AI drug discovery. We adopt feature fusion to process the multi-modal inputs, and leverage the attention mechanism to denoise the text information. Experiments show that DeepEIK significantly outperforms state-of-the-art methods on crucial tasks in AI drug discovery including drug-target interaction prediction, drug property prediction and protein-protein interaction prediction. Further studies show that benefiting from explicit and implicit knowledge, our
    
[^157]: 能量为基础的切片Wasserstein距离

    Energy-Based Sliced Wasserstein Distance. (arXiv:2304.13586v1 [stat.ML])

    [http://arxiv.org/abs/2304.13586](http://arxiv.org/abs/2304.13586)

    本文提出了一种能量为基础的切片Wasserstein距离，并将其参数化，以克服传统方法中的固定先验分布缺乏信息和优化最佳分布昂贵不稳定的局限。

    

    切片Wasserstein（SW）距离被广泛认为是两个概率测度之间的一种统计有效且计算高效的度量。SW距离的一个关键部分是切片分布。目前有两种方法来选择这个分布。第一种方法是使用固定的先验分布。第二种是优化归属于参数分布族的最佳分布，并且可以最大化期望的距离。然而，这两种方法都有局限性。固定的先验分布在突出能够区分两个常规概率测度的投影方向方面缺乏信息。而优化最佳分布通常是昂贵和不稳定的。此外，设计候选分布的参数分布族可能会很容易被错误指定。为了解决这些问题，我们提出将切片分布设计为基于能量的分布，并将其参数化，从而使其更加通用而稳健。

    The sliced Wasserstein (SW) distance has been widely recognized as a statistically effective and computationally efficient metric between two probability measures. A key component of the SW distance is the slicing distribution. There are two existing approaches for choosing this distribution. The first approach is using a fixed prior distribution. The second approach is optimizing for the best distribution which belongs to a parametric family of distributions and can maximize the expected distance. However, both approaches have their limitations. A fixed prior distribution is non-informative in terms of highlighting projecting directions that can discriminate two general probability measures. Doing optimization for the best distribution is often expensive and unstable. Moreover, designing the parametric family of the candidate distribution could be easily misspecified. To address the issues, we propose to design the slicing distribution as an energy-based distribution that is parameter
    
[^158]: QuMoS: 保护量子机器学习模型安全的框架

    QuMoS: A Framework for Preserving Security of Quantum Machine Learning Model. (arXiv:2304.11511v1 [quant-ph])

    [http://arxiv.org/abs/2304.11511](http://arxiv.org/abs/2304.11511)

    QuMoS是一个保护 QML 模型安全的框架，通过经典加密、量子混淆和诱饵样本等多种技术来保护模型免受窃取攻击，并具有较高的分类准确性。

    

    安全性一直是机器学习应用中的重要问题。由于模型训练的高成本，如收集相关样本、标记数据和消耗计算资源等，模型窃取攻击是最基本但至关重要的问题之一。而在量子计算中，这样的量子机器学习（QML）模型窃取攻击也存在，甚至更加严重，因为传统的加密方法很难直接应用于量子计算。另一方面，由于有限的量子计算资源，近期培训 QML 模型的货币成本甚至可能比经典模型更高。因此，一家公司开发的经过良好调整的 QML 模型可以被委派给量子云提供商作为服务，供普通用户使用。在这种情况下，如果云提供商受到攻击，QML 模型将泄漏。为了解决这个问题，我们提出了一个新的框架，即 QuMoS，用于保护 QML 模型的安全性。QuMoS 包括一系列技术，包括经典加密、量子混淆和诱饵样本，以防止模型窃取攻击。我们还提供了使用 PennyLane 软件库和 Google Cirq 包的具体 QuMoS 实现。模拟结果表明，我们的框架可以有效地防止 QML 模型被盗，同时保持高分类准确性。

    Security has always been a critical issue in machine learning (ML) applications. Due to the high cost of model training -- such as collecting relevant samples, labeling data, and consuming computing power -model-stealing attack is one of the most fundamental but vitally important issues. When it comes to quantum computing, such a quantum machine learning (QML) model-stealing attack also exists and it is even more severe because the traditional encryption method can hardly be directly applied to quantum computation. On the other hand, due to the limited quantum computing resources, the monetary cost of training QML model can be even higher than classical ones in the near term. Therefore, a well-tuned QML model developed by a company can be delegated to a quantum cloud provider as a service to be used by ordinary users. In this case, the QML model will be leaked if the cloud provider is under attack. To address such a problem, we propose a novel framework, namely QuMoS, to preserve mod
    
[^159]: 风能中的数字孪生：新兴技术和行业未来方向。

    Digital Twins in Wind Energy: Emerging Technologies and Industry-Informed Future Directions. (arXiv:2304.11405v1 [cs.HC])

    [http://arxiv.org/abs/2304.11405](http://arxiv.org/abs/2304.11405)

    本文综合介绍了数字孪生技术及其在风能行业中的应用，并从产业角度识别了未来的研究需求和挑战，最终提供数字孪生和其在风能应用领域未来研究和发展的路线图。

    

    本文全面介绍了数字孪生技术及其能力水平，重点关注其在风能行业中的应用。它在0-5的标度上界定了数字孪生及其能力水平的定义；0-独立、1-描述、2-诊断、3-预测、4-规定、5-自治。从产业角度，它确定了风能行业的现状和研究需求。本文从研究机构的角度提出了应对这些挑战的方法，并提供了一套推荐措施，以促进技术的接受。本文的贡献在于它综合了当前知识状态并从产业角度识别了未来的研究需求和挑战，并提供了数字孪生和其在风能应用领域未来研究和发展的路线图。

    This article presents a comprehensive overview of the digital twin technology and its capability levels, with a specific focus on its applications in the wind energy industry. It consolidates the definitions of digital twin and its capability levels on a scale from 0-5; 0-standalone, 1-descriptive, 2-diagnostic, 3-predictive, 4-prescriptive, 5-autonomous. It then, from an industrial perspective, identifies the current state of the art and research needs in the wind energy sector. The article proposes approaches to the identified challenges from the perspective of research institutes and offers a set of recommendations for diverse stakeholders to facilitate the acceptance of the technology. The contribution of this article lies in its synthesis of the current state of knowledge and its identification of future research needs and challenges from an industry perspective, ultimately providing a roadmap for future research and development in the field of digital twin and its applications in
    
[^160]: 离散与反向传播的桥梁：直通法与其它方法

    Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])

    [http://arxiv.org/abs/2304.08612](http://arxiv.org/abs/2304.08612)

    本文提出了一种新方法来逼近生成离散潜变量的参数的梯度，其中包括了一些数值方法，实现了二阶精度，取得了实验上的持续改进。

    

    反向传播是深度学习中的基石，但其仅限于计算连续变量的梯度，限制了涉及离散潜变量的问题的研究。针对这个问题，我们提出了一种新的方法来近似生成离散潜变量的参数的梯度。我们首先考察了广泛使用的 Straight-Through（ST）启发式方法，并证明它作为梯度的一阶近似值。在此基础上，我们提出了一种新的方法，称为 ReinMax，它集成了 Heun's Method，一种解ODE的二阶数值方法，以近似梯度。我们的方法实现了二阶精度，而不需要 Hessian 或其他二阶导数。我们进行了结构化输出预测和无监督生成建模任务的实验。我们的结果显示，\ours 在现有技术中带来了持续的改进，包括 ST 和 Straight-Through Gum。

    Backpropagation, the cornerstone of deep learning, is limited to computing gradients solely for continuous variables. This limitation hinders various research on problems involving discrete latent variables. To address this issue, we propose a novel approach for approximating the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose a novel method called ReinMax, which integrates Heun's Method, a second-order numerical method for solving ODEs, to approximate the gradient. Our method achieves second-order accuracy without requiring Hessian or other second-order derivatives. We conduct experiments on structured output prediction and unsupervised generative modeling tasks. Our results show that \ours brings consistent improvements over the state of the art, including ST and Straight-Through Gum
    
[^161]: “Polytuplet Loss: 训练阅读理解和逻辑推理模型的反向方法”

    Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2304.01046](http://arxiv.org/abs/2304.01046)

    本文研究了一种训练阅读理解和逻辑推理模型的反向方法，利用相对准确性的策略来训练模型，通过Polytuplet Loss函数来确保优先学习答案选择的相对正确性，获得了不错的成果，提出了具有一般性的训练方法和模型架构。

    

    在整个学校教育过程中，学生们将受到阅读理解和逻辑推理的考验。学生们已经开发了各种策略来完成此类考试，其中有些被认为是通常表现优于其他策略的。这样一种策略涉及强调相对准确性而非绝对准确性，理论上可以在不完全掌握解题所需信息的情况下得出正确答案。本文研究了应用这种策略来训练迁移学习模型以解决阅读理解和逻辑推理问题的有效性。这些模型在具有挑战性的阅读理解和逻辑推理基准数据集ReClor上进行了评估。尽管以前的研究集中于逻辑推理技能，但我们专注于一种通用的训练方法和模型架构。我们提出了Polytuplet Loss函数，是三元组损失函数的扩展，以确保优先学习答案选择的相对正确性而非学习绝对正确性。

    Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
    
[^162]: TPMCF: 使用多源协同特征进行时间QoS预测

    TPMCF: Temporal QoS Prediction using Multi-Source Collaborative Features. (arXiv:2303.18201v1 [cs.SE])

    [http://arxiv.org/abs/2303.18201](http://arxiv.org/abs/2303.18201)

    本文提出了一种新的方法TPMCF，利用多源特征进行QoS预测。该方法利用带有注意力机制的编码器-解码器架构，并使用协作特征捕捉用户和服务之间的关系，有效地处理数据稀疏和异常值。

    

    最近，随着服务API的快速部署，个性化的服务推荐在电子商务行业的增长中发挥了至关重要的作用。决定服务性能的服务质量(QoS)参数经常被用于推荐，但随时间波动。因此，QoS的预测对于在等价服务中识别合适的服务至关重要。当代的时间QoS预测方法由于各种限制而很难达到期望的精度，例如无法处理数据稀疏和异常值以及捕获用户-服务交互之间的高阶时间关系。虽然最近一些基于循环神经网络的体系结构可以建模QoS数据之间的时间关系，但由于缺乏其他特征（例如协作特征）来理解用户-服务交互之间的关系，预测精度会降低。本文通过提出一种解决方案TPMCF，来解决上述挑战。TPMCF利用多源特征（包括时间、用户和服务特征）进行QoS预测。具体地，它使用一个带有注意机制的新颖编码器解码器架构来利用用户-服务交互之间的高阶时间关系。此外，它使用协作特征来捕捉用户和服务之间的关系，并处理数据稀疏和异常值。对实际数据集进行的大量实验证明了TPMCF的有效性和优越性。

    Recently, with the rapid deployment of service APIs, personalized service recommendations have played a paramount role in the growth of the e-commerce industry. Quality-of-Service (QoS) parameters determining the service performance, often used for recommendation, fluctuate over time. Thus, the QoS prediction is essential to identify a suitable service among functionally equivalent services over time. The contemporary temporal QoS prediction methods hardly achieved the desired accuracy due to various limitations, such as the inability to handle data sparsity and outliers and capture higher-order temporal relationships among user-service interactions. Even though some recent recurrent neural-network-based architectures can model temporal relationships among QoS data, prediction accuracy degrades due to the absence of other features (e.g., collaborative features) to comprehend the relationship among the user-service interactions. This paper addresses the above challenges and proposes a s
    
[^163]: PCA-Net：操作学习的复杂性上下界

    Operator learning with PCA-Net: upper and lower complexity bounds. (arXiv:2303.16317v1 [cs.LG])

    [http://arxiv.org/abs/2303.16317](http://arxiv.org/abs/2303.16317)

    本文发展了PCA-Net的近似理论，得出了通用逼近结果，并识别出了使用PCA-Net进行高效操作学习的潜在障碍：输出分布的复杂性和算子空间的内在复杂性。

    

    神经算子在计算科学和工程中备受关注。PCA-Net是一种最近提出的神经算子架构，它将主成分分析(PCA)与神经网络相结合，以逼近潜在的算子。本文对这种方法进行了近似理论的发展，改进并显着扩展了此方向的以前的工作。在定性界限方面，本文得出了新颖的通用逼近结果，在对潜在算子和数据生成分布的最小假设的前提下。在定量限制方面，本文识别了使用PCA-Net进行高效操作学习的两个潜在障碍，通过导出下界进行了严格证明，第一个障碍与输出分布的复杂性有关，由PCA特征值的缓慢衰减来衡量；另一个障碍涉及无限维输入和输出空间之间的算子空间的内在复杂性。

    Neural operators are gaining attention in computational science and engineering. PCA-Net is a recently proposed neural operator architecture which combines principal component analysis (PCA) with neural networks to approximate an underlying operator. The present work develops approximation theory for this approach, improving and significantly extending previous work in this direction. In terms of qualitative bounds, this paper derives a novel universal approximation result, under minimal assumptions on the underlying operator and the data-generating distribution. In terms of quantitative bounds, two potential obstacles to efficient operator learning with PCA-Net are identified, and made rigorous through the derivation of lower complexity bounds; the first relates to the complexity of the output distribution, measured by a slow decay of the PCA eigenvalues. The other obstacle relates the inherent complexity of the space of operators between infinite-dimensional input and output spaces, 
    
[^164]: 结构化动态定价：全局收缩模型下的最优遗憾

    Structured Dynamic Pricing: Optimal Regret in a Global Shrinkage Model. (arXiv:2303.15652v1 [cs.LG])

    [http://arxiv.org/abs/2303.15652](http://arxiv.org/abs/2303.15652)

    本文提出了一种在流式纵向数据设置中的动态定价策略，该策略基于全局收缩结构和PSGD方法，并明确地将遗憾作为时间、模型参数和数据集规模的函数。

    

    本文考虑在流式纵向数据设置中的动态定价策略，目的是最大化在大量客户细分中的累计利润。我们考虑一种动态概率模型，其中消费者的偏好和价格敏感度随时间变化。基于这一众所周知的发现，具有相似特征的消费者会表现出相似的行为，我们考虑一种全局收缩结构，该结构假定不同细分中的消费者偏好可以用空间自回归模型很好地近似。在这样的流式纵向设置中，我们通过遗憾来衡量动态定价策略的性能，遗憾是与预先知道模型参数序列的千里眼相比预期的收入损失。我们提出了一种基于惩罚随机梯度下降（PSGD）的定价策略，并明确地将其遗憾作为时间函数、模型参数的时间变化性和消费者数据集的规模。

    We consider dynamic pricing strategies in a streamed longitudinal data set-up where the objective is to maximize, over time, the cumulative profit across a large number of customer segments. We consider a dynamic probit model with the consumers' preferences as well as price sensitivity varying over time. Building on the well-known finding that consumers sharing similar characteristics act in similar ways, we consider a global shrinkage structure, which assumes that the consumers' preferences across the different segments can be well approximated by a spatial autoregressive (SAR) model. In such a streamed longitudinal set-up, we measure the performance of a dynamic pricing policy via regret, which is the expected revenue loss compared to a clairvoyant that knows the sequence of model parameters in advance. We propose a pricing policy based on penalized stochastic gradient descent (PSGD) and explicitly characterize its regret as functions of time, the temporal variability in the model pa
    
[^165]: MAE预前置训练对于亿级预训练的有效性

    The effectiveness of MAE pre-pretraining for billion-scale pretraining. (arXiv:2303.13496v1 [cs.CV])

    [http://arxiv.org/abs/2303.13496](http://arxiv.org/abs/2303.13496)

    本文在计算机视觉领域提出了一种自我监督的MAE技术预前置训练方法，该方法适用于亿级预训练规模，并可显著提高模型收敛性和下游转移性能。

    

    本文重新审视了计算机视觉中用于视觉识别任务的标准预训练-微调范式。通常情况下，最先进的基础模型使用数十亿张图像的大规模（弱）监督数据集进行预训练。我们引入了一个额外的预前置训练阶段，它使用了自我监督的MAE技术来初始化模型。虽然MAE技术仅被证明能够与模型大小相缩放，但我们发现它也可以随数据集大小缩放。因此，我们基于MAE的预前置训练可同时适用于训练基础模型的模型和数据规模。预前置训练在一系列模型规模（参数数百万到数十亿）和数据集大小（图像数百万到数十亿）上一致提高了模型收敛性和下游转移性能，且我们还测量了其在10个不同的视觉识别任务上的有效性，包括图像分类、视频识别和目标检测。

    This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video reco
    
[^166]: 基于上下文学习的医学时间约束抽取范围研究

    The Scope of In-Context Learning for the Extraction of Medical Temporal Constraints. (arXiv:2303.09366v1 [cs.CL])

    [http://arxiv.org/abs/2303.09366](http://arxiv.org/abs/2303.09366)

    本研究定义了一种MTC分类，开发了一种基于CFG模型的抽取方法，并通过ICL自动提取和标准化DUGs中的MTC，有望通过定义安全的患者活动模式来推进以患者为中心的医疗应用。

    

    药物治疗通常对患者的日常活动施加时间约束。违反医学时间约束（MTC）会导致缺乏治疗依从性，以及不良的健康结果和增加的医疗费用。这些MTC在患者教育材料和临床文本中的药物使用指南（DUGs）中被发现。通过在计算上表示DUGs中的MTC，将有助于通过帮助定义安全的患者活动模式来推进以患者为中心的医疗应用。我们定义了一种新颖的在DUGs中发现的MTC分类法，并开发了一种基于上下文无关文法（CFG）的模型来计算地表示MTC。此外，我们发布了三个新的数据集，共计N = 836个带标准化的MTC标记的DUGs。我们开发了一种上下文学习（ICL）解决方案，用于自动提取和标准化DUGs中发现的MTC，跨所有数据集实现了平均F1得分0.62。最后，我们对ICL模型进行了严格的研究。

    Medications often impose temporal constraints on everyday patient activity. Violations of such medical temporal constraints (MTCs) lead to a lack of treatment adherence, in addition to poor health outcomes and increased healthcare expenses. These MTCs are found in drug usage guidelines (DUGs) in both patient education materials and clinical texts. Computationally representing MTCs in DUGs will advance patient-centric healthcare applications by helping to define safe patient activity patterns. We define a novel taxonomy of MTCs found in DUGs and develop a novel context-free grammar (CFG) based model to computationally represent MTCs from unstructured DUGs. Additionally, we release three new datasets with a combined total of N = 836 DUGs labeled with normalized MTCs. We develop an in-context learning (ICL) solution for automatically extracting and normalizing MTCs found in DUGs, achieving an average F1 score of 0.62 across all datasets. Finally, we rigorously investigate ICL model perfor
    
[^167]: 转换器在预测掩码单词时是否解析？

    Do Transformers Parse while Predicting the Masked Word?. (arXiv:2303.08117v1 [cs.CL])

    [http://arxiv.org/abs/2303.08117](http://arxiv.org/abs/2303.08117)

    本文探讨了预训练语言模型是否实际上进行解析以及为什么能捕捉解析结构，证明了类似于BERT或RoBERTa这样的掩码语言模型可以近似执行英语PCFG的Inside-Outside算法。

    

    已经证明，预训练的语言模型在使用类似于掩码语言建模这样的无监督损失函数进行训练时，可以对语言结构进行编码，例如依赖关系和组成成分分析树。但是人们对于这些模型是否实际上进行解析或仅进行与解析弱相关的一些计算存在疑问。本文在生成建模的上下文中一步步回答了上述问题，探讨了(a)是否有可能明确描述具有现实嵌入维度，头数等的转换器，能够进行解析甚至近似解析；(b)预训练模型为什么能够捕捉解析结构？我们展示了类似于BERT或RoBERTa这样的中等大小的掩码语言模型可以近似执行英语PCFG（Marcus等，1993）的Inside-Outside算法。我们还展示了，在PCFG生成语言建模损失上，Inside-Outside算法是最优的。

    Pre-trained language models have been shown to encode linguistic structures, e.g. dependency and constituency parse trees, in their embeddings while being trained on unsupervised loss functions like masked language modeling. Some doubts have been raised whether the models actually are doing parsing or only some computation weakly correlated with it. We study questions: (a) Is it possible to explicitly describe transformers with realistic embedding dimension, number of heads, etc. that are capable of doing parsing -- or even approximate parsing? (b) Why do pre-trained models capture parsing structure? This paper takes a step toward answering these questions in the context of generative modeling with PCFGs. We show that masked language models like BERT or RoBERTa of moderate sizes can approximately execute the Inside-Outside algorithm for the English PCFG [Marcus et al, 1993]. We also show that the Inside-Outside algorithm is optimal for masked language modeling loss on the PCFG-generate
    
[^168]: 在给定具有循环的非循环摘要因果图的时间序列中，用于集体异常的根本原因的识别

    Root Cause Identification for Collective Anomalies in Time Series given an Acyclic Summary Causal Graph with Loops. (arXiv:2303.04038v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.04038](http://arxiv.org/abs/2303.04038)

    本文提出了一种方法来识别给定时间序列和非循环摘要因果图中集体异常的根本原因，并通过将问题划分为独立子问题和比较直接效应等方式来解决该问题

    

    本文提出了一种方法，用于识别给定观测时间序列和一个抽象表示正常状态下动态系统中因果关系的非循环摘要因果图的集体异常的根本原因。本文首先展示了如何通过使用d-分离将相关异常分组，将根本原因识别问题分为多个独立子问题。此外，本文还展示了在此设置下如何直接从图中和异常出现时间中找到一些根本原因。最后，它展示了如何通过比较正常和异常状态下的直接影响来找到其余的根本原因。为此，引入了一个用于识别直接影响的调整集。在模拟和真实数据集上进行的大量实验表明了所提方法的有效性。

    This paper presents an approach for identifying the root causes of collective anomalies given observational time series and an acyclic summary causal graph which depicts an abstraction of causal relations present in a dynamic system at its normal regime. The paper first shows how the problem of root cause identification can be divided into many independent subproblems by grouping related anomalies using d-separation. Further, it shows how, under this setting, some root causes can be found directly from the graph and from the time of appearance of anomalies. Finally, it shows, how the rest of the root causes can be found by comparing direct effects in the normal and in the anomalous regime. To this end, an adjustment set for identifying direct effects is introduced. Extensive experiments conducted on both simulated and real-world datasets demonstrate the effectiveness of the proposed method.
    
[^169]: 基于模型的离线强化学习的环境转换器和策略优化

    Environment Transformer and Policy Optimization for Model-Based Offline Reinforcement Learning. (arXiv:2303.03811v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03811](http://arxiv.org/abs/2303.03811)

    论文提出了一种称为环境转换器的不确定性感知序列建模架构，用于解决基于模型的离线强化学习中训练时间和计算资源需求增加的问题，并减少环境动力学模型累积误差的干扰。

    

    在机器人任务中，与实际环境交互以获取数据通常是昂贵且耗时的。基于模型的离线强化学习提供了一种可行的解决方案。一方面，它消除了与实际环境的交互要求。另一方面，它从离线数据集中学习转换动力学和奖励函数，并生成模拟的回合以加速训练。以前的基于模型的离线强化学习方法采用概率集合神经网络（NN）来建模aleatoric不确定性和epistemic不确定性。然而，这导致了训练时间和计算资源需求的指数增加。此外，这些方法在模拟长期回合时容易受到环境动力学模型的累积误差的干扰。为了解决上述问题，我们提出了一种称为环境转换器的不确定性感知序列建模架构。

    Interacting with the actual environment to acquire data is often costly and time-consuming in robotic tasks. Model-based offline reinforcement learning (RL) provides a feasible solution. On the one hand, it eliminates the requirements of interaction with the actual environment. On the other hand, it learns the transition dynamics and reward function from the offline datasets and generates simulated rollouts to accelerate training. Previous model-based offline RL methods adopt probabilistic ensemble neural networks (NN) to model aleatoric uncertainty and epistemic uncertainty. However, this results in an exponential increase in training time and computing resource requirements. Furthermore, these methods are easily disturbed by the accumulative errors of the environment dynamics models when simulating long-term rollouts. To solve the above problems, we propose an uncertainty-aware sequence modeling architecture called Environment Transformer. It models the probability distribution of th
    
[^170]: 辅助函数作为Koopman可观测量：基于数据的动力系统多项式优化方法

    Auxiliary Functions as Koopman Observables: Data-Driven Polynomial Optimization for Dynamical Systems. (arXiv:2303.01483v2 [math.DS] UPDATED)

    [http://arxiv.org/abs/2303.01483](http://arxiv.org/abs/2303.01483)

    提出了一种基于数据的动力系统分析方法，使用辅助函数作为Koopman可观测量，不需要明确的模型发现，可以适用于确定性和随机动力学，具有收敛性和性能优势。

    

    我们提出了一种灵活的基于数据的动力系统分析方法，不需要明确的模型发现。该方法源于从数据中逼近Koopman算子的技术，并且以一个可以通过数值求解的半定规划问题来实现。此外，该方法不关心数据是通过确定性还是随机过程生成的，因此用户无需进行任何调整即可应用于不同的情况。严格的收敛性结果证明了该方法的适用性，并将文献中类似的结果进行了扩展和统一。通过对确定性和随机动力学的吸引子上发现Lyapunov函数、执行遍历优化以及界定极值的示例，证明了该方法的性能。

    We present a flexible data-driven method for dynamical system analysis that does not require explicit model discovery. The method is rooted in well-established techniques for approximating the Koopman operator from data and is implemented as a semidefinite program that can be solved numerically. Furthermore, the method is agnostic of whether data is generated through a deterministic or stochastic process, so its implementation requires no prior adjustments by the user to accommodate these different scenarios. Rigorous convergence results justify the applicability of the method, while also extending and uniting similar results from across the literature. Examples on discovering Lyapunov functions, performing ergodic optimization, and bounding extrema over attractors for both deterministic and stochastic dynamics exemplify these convergence results and demonstrate the performance of the method.
    
[^171]: 通过多模态引导在长视频中定位时刻

    Localizing Moments in Long Video Via Multimodal Guidance. (arXiv:2302.13372v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.13372](http://arxiv.org/abs/2302.13372)

    本文提出了一种通过多模态引导方法，在长视频中提高自然语言 grounding 性能的方法，通过识别和修剪不可描述的窗口，实验证明这种方法优于其他方法。

    

    最近推出了大规模、长格式的MAD和Ego4D数据集，使研究人员能够研究当前最先进的视频 grounding 方法在长格式设置中的性能，有趣的发现是：当前的 grounding 方法单独无法处理这一具有挑战性的任务和设置，因为它们无法处理长视频序列。在本文中，我们提出了一种改进长视频自然语言 grounding 性能的方法，通过识别和修剪不可描述的窗口。我们设计了一个引导 grounding 框架，包括一个引导模型和一个基础 grounding 模型。引导模型强调可描述的窗口，而基础 grounding 模型分析短时窗口，确定哪些片段与给定的语言查询准确匹配。我们提供了两个引导模型的设计：Query-Agnostic 和 Query-Dependent，以平衡效率和准确性。实验证明，我们提出的方法优于其他方法。

    The recent introduction of the large-scale, long-form MAD and Ego4D datasets has enabled researchers to investigate the performance of current state-of-the-art methods for video grounding in the long-form setup, with interesting findings: current grounding methods alone fail at tackling this challenging task and setup due to their inability to process long video sequences. In this paper, we propose a method for improving the performance of natural language grounding in long videos by identifying and pruning out non-describable windows. We design a guided grounding framework consisting of a Guidance Model and a base grounding model. The Guidance Model emphasizes describable windows, while the base grounding model analyzes short temporal windows to determine which segments accurately match a given language query. We offer two designs for the Guidance Model: Query-Agnostic and Query-Dependent, which balance efficiency and accuracy. Experiments demonstrate that our proposed method outperfo
    
[^172]: 测试反事实场景：揭示在公平原则下的歧视差异 (arXiv:2302.11944v2 [stat.ML] UPDATED)

    Counterfactual Situation Testing: Uncovering Discrimination under Fairness given the Difference. (arXiv:2302.11944v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.11944](http://arxiv.org/abs/2302.11944)

    我们提出了一种反事实场景测试框架，通过比较数据集中类似的保护和非保护实例来检测分类器中的歧视，通过比较组间决策结果差异，来发现个人歧视。该框架可以更好地对「给定差异的公平原则」进行操作，以揭示在公平原则下的歧视差异。

    

    我们提出了一种被称为反事实场景测试(CST)的因果数据挖掘框架来检测分类器中的歧视情况。CST旨在以可操作且有意义的方式回答一种直观问题：“如果个人或投诉人所属的受保护身份不同，模型的结果将会是什么？”它通过反事实推理来对法律基础的情景测试进行扩展，以操作“给定差异的公平原则”的概念。对于任何投诉人，我们在分类器使用的数据集中找到并比较相似的受保护和非受保护实例，构造控制组和测试组，两组的决策结果差异意味着潜在的个人歧视。与情境测试不同，情境测试是围绕投诉人构建两组，我们根据因果知识在投诉人的反事实生成测试组。反事实旨在反映受保护属性对结果的影响。

    We present counterfactual situation testing (CST), a causal data mining framework for detecting discrimination in classifiers. CST aims to answer in an actionable and meaningful way the intuitive question "what would have been the model outcome had the individual, or complainant, been of a different protected status?" It extends the legally-grounded situation testing of Thanh et al. (2011) by operationalizing the notion of fairness given the difference using counterfactual reasoning. For any complainant, we find and compare similar protected and non-protected instances in the dataset used by the classifier to construct a control and test group, where a difference between the decision outcomes of the two groups implies potential individual discrimination. Unlike situation testing, which builds both groups around the complainant, we build the test group on the complainant's counterfactual generated using causal knowledge. The counterfactual is intended to reflect how the protected attrib
    
[^173]: 一站式解决方案：利用预训练 LM 进行强大的时间序列分析

    One Fits All:Power General Time Series Analysis by Pretrained LM. (arXiv:2302.11939v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11939](http://arxiv.org/abs/2302.11939)

    本论文提出了一种称为 Frozen Pretrained Transformer (FPT) 的预训练模型，利用从数十亿标记训练出来的语言或 CV 模型进行时间序列分析的所有主要类型任务的微调，进而使其在所有任务中都具备着最先进的性能和泛化能力。

    

    尽管预训练模型在自然语言处理 (NLP) 和计算机视觉 (CV) 领域取得了巨大成功，但在通用时间序列分析领域取得的进展有限。与 NLP 和 CV 不同的是，这些领域采用统一模型即可执行不同的任务，而在每个时间序列分析任务中，专门设计的方法仍然占据主导地位，如分类、异常检测、预测和少样本学习。阻碍预训练模型发展的主要挑战是缺乏大量用于训练的数据。在本文中，我们通过利用从数十亿标记训练出来的语言或 CV 模型，来解决这一挑战，用于时间序列分析。具体而言，我们避免改变预训练语言或图像模型中残差块中的自注意力和前向传递层。这种模型被称为冻结的预训练变压器 (FPT)，通过对涉及时间序列分析的所有主要类型的任务进行微调进行评估，包括分类、异常检测、预测和少样本学习等。实验结果证明，FPT 在所有任务中都具有最先进的性能和泛化能力。

    Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time s
    
[^174]: 神经注意力记忆

    Neural Attention Memory. (arXiv:2302.09422v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09422](http://arxiv.org/abs/2302.09422)

    神经注意力记忆（NAM）是一种记忆架构，通过可微分线性代数操作可被读写，可应用于记忆增强型神经网络、小样本学习和高效的长程关注。实验证明NAM在各方面均具有优越性能。

    

    我们提出了一种新颖的注意力机制视角，将其重新设计为神经网络的记忆架构，即神经注意力记忆（NAM）。NAM是一种可通过可微分线性代数运算进行读写的记忆结构。我们探索了NAM的三个用例：记忆增强型神经网络（MANN）、小样本学习和高效的长程关注。首先，我们设计了两种基于NAM的MANN，分别是长短期记忆（LSAM）和NAM图灵机（NAM-TM），在算法性零样本泛化任务中展现出比其他基线（如可微分神经计算机）更好的计算能力。接下来，我们将NAM应用于N-way K-shot学习任务，并展示其在减少误报的效果上比基线余弦分类器更有效。最后，我们实现了一种带有NAM的高效Transformer，并通过长距离竞技任务对其进行评估，表明NAM可以成为缩放点积的高效和有效的替代方案。

    We propose a novel perspective of the attention mechanism by reinventing it as a memory architecture for neural networks, namely Neural Attention Memory (NAM). NAM is a memory structure that is both readable and writable via differentiable linear algebra operations. We explore three use cases of NAM: memory-augmented neural network (MANN), few-shot learning, and efficient long-range attention. First, we design two NAM-based MANNs of Long Short-term Memory (LSAM) and NAM Turing Machine (NAM-TM) that show better computational powers in algorithmic zero-shot generalization tasks compared to other baselines such as differentiable neural computer (DNC). Next, we apply NAM to the N-way K-shot learning task and show that it is more effective at reducing false positives compared to the baseline cosine classifier. Finally, we implement an efficient Transformer with NAM and evaluate it with long-range arena tasks to show that NAM can be an efficient and effective alternative for scaled dot-produ
    
[^175]: 超越分布偏移：从训练动态视角解析虚假特征

    Beyond Distribution Shift: Spurious Features Through the Lens of Training Dynamics. (arXiv:2302.09344v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09344](http://arxiv.org/abs/2302.09344)

    本文通过观察训练过程中的学习动态，研究了深度神经网络中虚假特征的影响，发现并非所有虚假特征都是有害的。我们提出了一种模型和数据集相关的定义来区分虚假特征的有害程度，并利用示例困难度方法来量化模型的易学性和识别虚假特征。

    

    深度神经网络(DNNs)在训练过程中容易学习到与标签相关但与学习问题无关的虚假特征。这会损害模型的泛化能力，并在安全关键应用中引发问题。本文旨在通过观察训练过程中内部神经元的学习动态，更好地理解虚假特征的影响。我们做出以下观察：(1) 虽然先前的研究强调了虚假特征对DNNs泛化能力的有害影响，但我们强调并非所有虚假特征都是有害的。虚假特征的有害程度取决于相较于给定模型的核心特征而言，它们更难还是更容易学习。这个定义是模型和数据集相关的。 (2) 我们基于此前提，利用示例困难度方法(如Prediction Depth)来量化模型的"易学性"，并识别虚假特征。

    Deep Neural Networks (DNNs) are prone to learning spurious features that correlate with the label during training but are irrelevant to the learning problem. This hurts model generalization and poses problems when deploying them in safety-critical applications. This paper aims to better understand the effects of spurious features through the lens of the learning dynamics of the internal neurons during the training process. We make the following observations: (1) While previous works highlight the harmful effects of spurious features on the generalization ability of DNNs, we emphasize that not all spurious features are harmful. Spurious features can be "benign" or "harmful" depending on whether they are "harder" or "easier" to learn than the core features for a given model. This definition is model and dataset-dependent. (2) We build upon this premise and use instance difficulty methods (like Prediction Depth (Baldock et al., 2021)) to quantify "easiness" for a given model and to identi
    
[^176]: 超越统计相似性：重新思考机器学习在工程设计中的度量方法

    Beyond Statistical Similarity: Rethinking Metrics for Deep Generative Models in Engineering Design. (arXiv:2302.02913v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02913](http://arxiv.org/abs/2302.02913)

    本文提供了一篇深度学习在工程设计中度量方法的综述和指南。传统的基于似然性的统计度量方法在对工程应用的要求上可能无法充分捕捉，因此本文编辑了一组全面的新度量标准，旨在解决传统度量标准的缺点，并更好地与工程设计的需求相一致。通过案例研究，本文展示了这些度量标准如何应用于评估深度生成模型在工程设计中的性能，并发现这些度量标准在捕捉设计的重要细微差别方面表现优于传统的统计度量标准。

    

    深度生成模型，如变分自编码器（VAEs），生成对抗网络（GANs），扩散模型和Transformer等，在图像和语音合成、自然语言处理和药物开发等各种应用中显示出巨大的潜力。然而，在工程设计问题中应用这些模型时，评估这些模型的性能可能会很具有挑战性，因为传统的基于似然性的统计度量方法可能无法充分捕捉工程应用的要求。本文旨在提供一篇深度学习在工程设计中的度量指南和综述。首先，我们总结了深度生成模型的“经典”评估度量标准，这些标准基于机器学习理论和典型的计算机应用，然后使用案例研究，强调了这些度量标准为何很少能够转化为设计问题但又因缺乏确立的替代选择而经常使用。接下来，我们编辑了一组全面的新度量标准，旨在解决传统度量标准的缺点，并更好地与工程设计的需求相一致。我们演示了如何应用这些度量标准来评估深度生成模型在工程设计应用中的性能。我们的结果表明，提出的度量方法在捕捉设计的重要细微差别方面优于传统的统计度量标准，因此在工程设计情境中为深度生成模型提供了更准确的评估。

    Deep generative models, such as Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Diffusion Models, and Transformers, have shown great promise in a variety of applications, including image and speech synthesis, natural language processing, and drug discovery. However, when applied to engineering design problems, evaluating the performance of these models can be challenging, as traditional statistical metrics based on likelihood may not fully capture the requirements of engineering applications. This paper doubles as a review and a practical guide to evaluation metrics for deep generative models (DGMs) in engineering design. We first summarize well-accepted `classic' evaluation metrics for deep generative models grounded in machine learning theory and typical computer science applications. Using case studies, we then highlight why these metrics seldom translate well to design problems but see frequent use due to the lack of established alternatives. Next, we curat
    
[^177]: 基于频率变换的深度学习时间序列分析综述

    A Survey on Deep Learning based Time Series Analysis with Frequency Transformation. (arXiv:2302.02173v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02173](http://arxiv.org/abs/2302.02173)

    近期，频率变换（FT）在深度学习时间序列分析中得到广泛应用，显著提高了准确性和效率。本文系统回顾和总结了基于FT的深度学习时间序列模型的研究进展，并探讨了其优势、限制以及主要方法。

    

    最近，频率变换（FT）越来越多地被纳入深度学习模型中，可以显著提高时间序列分析的最新准确性和效率。频率变换的优势，如高效性和全局视角，在各种时间序列任务和应用中被迅速探索和利用，展示了频率变换作为一种新的深度学习范式在时间序列分析领域的潜力。尽管这个新兴领域受到了越来越多的关注和研究，但目前还缺乏对基于频率变换的深度学习时间序列模型的系统回顾和深入分析。目前还不清楚为什么频率变换可以提升时间序列分析的效果，以及它在该领域的限制是什么。为了填补这些空白，我们提供了一份全面的综述，系统调查和总结了基于频率变换的深度学习时间序列分析的最新研究进展。具体而言，我们探讨了主要的方法。

    Recently, frequency transformation (FT) has been increasingly incorporated into deep learning models to significantly enhance state-of-the-art accuracy and efficiency in time series analysis. The advantages of FT, such as high efficiency and a global view, have been rapidly explored and exploited in various time series tasks and applications, demonstrating the promising potential of FT as a new deep learning paradigm for time series analysis. Despite the growing attention and the proliferation of research in this emerging field, there is currently a lack of a systematic review and in-depth analysis of deep learning-based time series models with FT. It is also unclear why FT can enhance time series analysis and what its limitations in the field are. To address these gaps, we present a comprehensive review that systematically investigates and summarizes the recent research advancements in deep learning-based time series analysis with FT. Specifically, we explore the primary approaches us
    
[^178]: 防御性机器学习：用对抗性混淆来抵御架构侧信道攻击

    Defensive ML: Defending Architectural Side-channels with Adversarial Obfuscation. (arXiv:2302.01474v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.01474](http://arxiv.org/abs/2302.01474)

    本论文提出了一种防御性机器学习的方法，通过对抗性混淆来抵御计算机架构中的侧信道攻击。该方法可以设计、实现、训练和部署不同环境下的防御器，有效解决了使用机器学习进行信号分析的安全威胁。

    

    使用机器学习（ML）进行信号分析的侧信道攻击已经成为计算机安全的突出威胁，因为ML模型很容易找到信号中的模式。为了解决这个问题，本文探讨了在计算机架构层面上使用对抗性机器学习（AML）方法来对抗性侧信道的混淆。我们将这种方法称为防御性机器学习，将信号的混淆器称为防御器。防御性机器学习是一个工作流程，用于为不同环境设计、实现、训练和部署防御器。首先，我们根据侧信道的物理特性和硬件限制设计了一个防御器架构。接下来，我们使用我们的DefenderGAN结构来训练防御器。最后，我们应用防御性机器学习来阻止两种侧信道攻击：一种基于内存争用，另一种基于应用程序功耗。前者使用具有纳秒级响应时间的硬件防御器，以半正常性能影响获得较高的安全性。

    Side-channel attacks that use machine learning (ML) for signal analysis have become prominent threats to computer security, as ML models easily find patterns in signals. To address this problem, this paper explores using Adversarial Machine Learning (AML) methods as a defense at the computer architecture layer to obfuscate side channels. We call this approach Defensive ML, and the generator to obfuscate signals, defender. Defensive ML is a workflow to design, implement, train, and deploy defenders for different environments. First, we design a defender architecture given the physical characteristics and hardware constraints of the side-channel. Next, we use our DefenderGAN structure to train the defender. Finally, we apply defensive ML to thwart two side-channel attacks: one based on memory contention and the other on application power. The former uses a hardware defender with ns-level response time that attains a high level of security with half the performance impact of a traditional
    
[^179]: 通过合作协同进化搜索确定基于机器学习自主系统的危险边界

    Identifying the Hazard Boundary of ML-enabled Autonomous Systems Using Cooperative Co-Evolutionary Search. (arXiv:2301.13807v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2301.13807](http://arxiv.org/abs/2301.13807)

    本研究致力于通过合作协同进化搜索，确定机器学习自主系统中的ML组件的危险边界。这种边界可以用于构建安全监视器，并在达到危险边界时采取预定义的回退机制。

    

    在机器学习（ML）驱动的自主系统（MLAS）中，确定ML组件（MLCs）的危险边界对于分析中的MLAS至关重要。鉴于这种边界可以捕捉到导致危险的MLC行为和系统背景条件，例如，在达到危险边界时，可以构建一个安全监视器，可以在运行时采取任何预定义的回退机制。然而，确定ML组件的这种危险边界是具有挑战性的。这是由于问题空间将系统环境（即场景）和MLC行为（即输入和输出）组合在一起，过于庞大，无法通过全面的探索甚至是传统的元启发式算法，如遗传算法来处理。此外，确定任何MLAS安全违规所需的模拟的高计算成本使问题更加具有挑战性。此外，以确定性地考虑问题空间中的一个区域是不切实际的。

    In Machine Learning (ML)-enabled autonomous systems (MLASs), it is essential to identify the hazard boundary of ML Components (MLCs) in the MLAS under analysis. Given that such boundary captures the conditions in terms of MLC behavior and system context that can lead to hazards, it can then be used to, for example, build a safety monitor that can take any predefined fallback mechanisms at runtime when reaching the hazard boundary. However, determining such hazard boundary for an ML component is challenging. This is due to the problem space combining system contexts (i.e., scenarios) and MLC behaviors (i.e., inputs and outputs) being far too large for exhaustive exploration and even to handle using conventional metaheuristics, such as genetic algorithms. Additionally, the high computational cost of simulations required to determine any MLAS safety violations makes the problem even more challenging. Furthermore, it is unrealistic to consider a region in the problem space deterministicall
    
[^180]: 通过单光子量子行走实现可解释的量子机器学习

    Towards interpretable quantum machine learning via single-photon quantum walks. (arXiv:2301.13669v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2301.13669](http://arxiv.org/abs/2301.13669)

    通过单光子量子行走实现可解释的量子机器学习，使用变分量子算法量子化了项目模拟模型，实现了超越经典模型能力的量子干涉，为解释性量子机器学习的实现铺平了道路。

    

    变分量子算法是一种有前途的量子机器学习方法，其中经典神经网络被参数化的量子电路取代。然而，这两种方法都存在一个明显的限制，即缺乏可解释性。在这里，我们提出了一种用于量子项目模拟（PS）的变分量子方法，这是一种旨在实现可解释人工智能的强化学习模型。在PS中，决策被建模为描述代理记忆的图上的随机漫步。为了实现量子化的模型，我们考虑了通过变分算法训练的可调谐Mach-Zehnder干涉仪晶格中的单光子量子行走。通过使用迁移学习的例子，我们展示了量子化的PS模型可以利用量子干涉来获得超越其经典对应模型能力的能力。最后，我们讨论了量子干涉在训练和追踪决策过程中的作用，为实现解释性量子机器学习铺平了道路。

    Variational quantum algorithms represent a promising approach to quantum machine learning where classical neural networks are replaced by parametrized quantum circuits. However, both approaches suffer from a clear limitation, that is a lack of interpretability. Here, we present a variational method to quantize projective simulation (PS), a reinforcement learning model aimed at interpretable artificial intelligence. Decision making in PS is modeled as a random walk on a graph describing the agent's memory. To implement the quantized model, we consider quantum walks of single photons in a lattice of tunable Mach-Zehnder interferometers trained via variational algorithms. Using an example from transfer learning, we show that the quantized PS model can exploit quantum interference to acquire capabilities beyond those of its classical counterpart. Finally, we discuss the role of quantum interference for training and tracing the decision making process, paving the way for realizations of int
    
[^181]: DiffSTG: 带有去噪扩散模型的概率时空图预测

    DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising Diffusion Models. (arXiv:2301.13629v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13629](http://arxiv.org/abs/2301.13629)

    本论文提出了一种新的方法DiffSTG，该方法结合了STGNN的时空学习能力和扩散模型的不确定性测量，可以有效减小STG预测中的排名概率分数和均方根误差。

    

    时空图神经网络（STGNN）已成为时空图（STG）预测的主要模型。然而，它们无法对STG数据中的内在不确定性进行建模，这使得它们在决策任务中的实用性受到限制。为此，本文关注概率STG预测，由于建模不确定性和复杂的ST依赖关系的困难，这是一个具有挑战性的问题。在本研究中，我们首次尝试将流行的去噪扩散概率模型推广到STG，提出了一种称为DiffSTG的新的非自回归框架，并在该框架中引入了第一个STG去噪网络UGnet。我们的方法将STGNN的时空学习能力与扩散模型的不确定性测量相结合。大量实验证实DiffSTG将持续排名概率分数（CRPS）降低了4%-14%，均方根误差（RMSE）降低了2%-7%。

    Spatio-temporal graph neural networks (STGNN) have emerged as the dominant model for spatio-temporal graph (STG) forecasting. Despite their success, they fail to model intrinsic uncertainties within STG data, which cripples their practicality in downstream tasks for decision-making. To this end, this paper focuses on probabilistic STG forecasting, which is challenging due to the difficulty in modeling uncertainties and complex ST dependencies. In this study, we present the first attempt to generalize the popular denoising diffusion probabilistic models to STGs, leading to a novel non-autoregressive framework called DiffSTG, along with the first denoising network UGnet for STG in the framework. Our approach combines the spatio-temporal learning capabilities of STGNNs with the uncertainty measurements of diffusion models. Extensive experiments validate that DiffSTG reduces the Continuous Ranked Probability Score (CRPS) by 4%-14%, and Root Mean Squared Error (RMSE) by 2%-7% over existing 
    
[^182]: 神经网络学习放大决策边界附近的区域

    Neural networks learn to magnify areas near decision boundaries. (arXiv:2301.11375v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11375](http://arxiv.org/abs/2301.11375)

    神经网络训练能够放大决策边界附近的局部区域，改善整个系统的泛化能力。

    

    我们研究了训练如何塑造神经网络特征图诱导的黎曼几何。在宽度为无限的情况下，具有随机参数的神经网络在输入空间上引导高度对称的度量。训练分类任务的网络中的特征学习放大了沿决策边界的局部区域。这些变化与先前提出的用于手动调整核方法以改善泛化的几何方法一致。

    We study how training molds the Riemannian geometry induced by neural network feature maps. At infinite width, neural networks with random parameters induce highly symmetric metrics on input space. Feature learning in networks trained to perform classification tasks magnifies local areas along decision boundaries. These changes are consistent with previously proposed geometric approaches for hand-tuning of kernel methods to improve generalization.
    
[^183]: XLM-V: 克服多语言掩码语言模型中的词汇瓶颈

    XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models. (arXiv:2301.10472v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10472](http://arxiv.org/abs/2301.10472)

    XLM-V通过克服多语言掩码语言模型中的词汇瓶颈，引入了一种新方法。使用一个一百万标记词汇表，XLM-V在各项任务中均优于XLM-R。

    

    大型多语言语言模型通常依赖于一个在100多种语言间共享的单一词汇表。随着这些模型参数和深度的增加，词汇大小基本保持不变。这种"词汇瓶颈"限制了XLM-R等多语言模型的表达能力。在本文中，我们引入了一种新方法，通过减少在词汇上的跨语言共享，为每种语言分配足够的覆盖能力，从而扩展到非常大的多语言词汇表。使用我们的词汇进行分词通常比XLM-R更语义有意义且更短。利用这个改进的词汇，我们训练了一个具有100万个标记词汇表的多语言语言模型XLM-V。XLM-V在我们测试的每个任务上表现优于XLM-R，包括自然语言推理（XNLI）、问答（MLQA，XQuAD，TyDiQA）和命名实体识别（WikiAnn）。

    Large multilingual language models typically rely on a single vocabulary shared across 100+ languages. As these models have increased in parameter count and depth, vocabulary size has remained largely unchanged. This \textit{vocabulary bottleneck} limits the representational capabilities of multilingual models like XLM-R. In this paper, we introduce a new approach for scaling to very large multilingual vocabularies by de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity to achieve sufficient coverage for each individual language. Tokenizations using our vocabulary are typically more semantically meaningful and shorter compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V, a multilingual language model with a one million token vocabulary. XLM-V outperforms XLM-R on every task we tested on ranging from natural language inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), to named entity recognition (WikiAnn).
    
[^184]: 本文研究了基于因果关系的本地化和基于知识编辑的语言模型中的令人惊讶的差异，探讨了是否本地化能够提供编辑指导。

    Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models. (arXiv:2301.04213v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.04213](http://arxiv.org/abs/2301.04213)

    本文研究了语言模型中本地化与编辑之间的关系，发现将事实本地化到特定模型参数并不能提供编辑指导。因果追踪方法并不能指导编辑哪个模型层来覆盖存储的事实。

    

    语言模型在预训练阶段学习到大量的事实信息，最近的研究将这些信息定位到模型的特定权重，如中间层MLP权重。然而，我们发现通过编辑不同于现有方法所建议的存储事实位置的权重，可以改变模型中的事实存储方式。这一发现令人意外，因为我们原本期望将事实本地化到特定的模型参数可以告诉我们在模型中如何操纵知识，这一假设曾激发过模型编辑方法的研究。具体而言，我们发现表示去噪（也称为因果追踪）所得出的本地化结论并不能提供任何关于应该在哪个模型MLP层进行编辑以覆盖现有存储事实的新事实的见解。这一发现对过去的研究如何依赖因果追踪来选择需要编辑的模型层提出了质疑。接下来，我们考虑了几种编辑方法的变体。

    Language models learn a great quantity of factual information during pretraining, and recent work localizes this information to specific model weights like mid-layer MLP weights. In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific model parameters would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Specifically, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This finding raises questions about how past work relies on Causal Tracing to select which model layers to edit. Next, we consider several variants of the editing pr
    
[^185]: 马尔可夫切片Wasserstein距离：超越独立投影

    Markovian Sliced Wasserstein Distances: Beyond Independent Projections. (arXiv:2301.03749v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.03749](http://arxiv.org/abs/2301.03749)

    马尔可夫切片Wasserstein（MSW）距离是一种新的SW距离家族，通过在投影方向上施加一阶马尔可夫结构，解决了切片Wasserstein（SW）距离中独立投影导致的冗余投影的问题，并且具有较低的计算复杂度。（found in translation）

    

    切片Wasserstein（SW）距离由于独立的均匀随机投影方向而导致冗余投影。为了部分克服这个问题，最大K切片Wasserstein（Max-K-SW）距离（$K\geq1$）寻求最佳的区分正交投影方向。尽管能够减少投影数量，但Max-K-SW的度量性在实践中不能保证，原因是优化的非最优性。此外，正交约束也在计算上是昂贵的，可能不太有效。为了解决这个问题，我们引入了一种新的SW距离家族，称为马尔可夫切片Wasserstein（MSW）距离，它在投影方向上施加了一阶马尔可夫结构。我们通过指定马尔可夫结构，包括先验分布、转移分布以及燃烧和稀疏化技术，讨论了MSW的各种成员。此外，我们还研究了MSW的理论性质，包括拓扑性质（found in translation）

    Sliced Wasserstein (SW) distance suffers from redundant projections due to independent uniform random projecting directions. To partially overcome the issue, max K sliced Wasserstein (Max-K-SW) distance ($K\geq 1$), seeks the best discriminative orthogonal projecting directions. Despite being able to reduce the number of projections, the metricity of Max-K-SW cannot be guaranteed in practice due to the non-optimality of the optimization. Moreover, the orthogonality constraint is also computationally expensive and might not be effective. To address the problem, we introduce a new family of SW distances, named Markovian sliced Wasserstein (MSW) distance, which imposes a first-order Markov structure on projecting directions. We discuss various members of MSW by specifying the Markov structure including the prior distribution, the transition distribution, and the burning and thinning technique. Moreover, we investigate the theoretical properties of MSW including topological properties (met
    
[^186]: MixupE：从方向导数角度理解和改进Mixup技术

    MixupE: Understanding and Improving Mixup from Directional Derivative Perspective. (arXiv:2212.13381v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13381](http://arxiv.org/abs/2212.13381)

    本文从方向导数的角度分析了深度神经网络中常用的数据增强技术Mixup，发现它隐含地对所有阶数的无限多个方向导数进行了正则化。基于这一思路，作者提出了改进版的Mixup，理论上证明具有更好的泛化性能，并在各种领域的数据集上进行了验证，表现出比Mixup更好的效果。

    

    Mixup是一种深度神经网络中流行的数据增强技术，通过线性插值输入和它们的标签生成额外的样本。该技术已被证实在许多学习范式和应用中提高了泛化性能。本文首先对Mixup进行分析，发现它隐含地对所有阶数的无限多个方向导数进行了正则化。基于这一新的洞见，我们提出了一种改进版本的Mixup，理论上证明它可以比原始版本具有更好的泛化性能。为了证明这种方法的有效性，我们在各种领域进行了实验，例如图像、表格数据、语音和图形。我们的结果表明，所提出的方法改进了Mixup在多个数据集上的表现，在使用各种架构时都表现出比Mixup更好的性能，例如在ImageNet的top-1精度上比Mixup提高了0.8%。

    Mixup is a popular data augmentation technique for training deep neural networks where additional samples are generated by linearly interpolating pairs of inputs and their labels. This technique is known to improve the generalization performance in many learning paradigms and applications. In this work, we first analyze Mixup and show that it implicitly regularizes infinitely many directional derivatives of all orders. Based on this new insight, we propose an improved version of Mixup, theoretically justified to deliver better generalization performance than the vanilla Mixup. To demonstrate the effectiveness of the proposed method, we conduct experiments across various domains such as images, tabular data, speech, and graphs. Our results show that the proposed method improves Mixup across multiple datasets using a variety of architectures, for instance, exhibiting an improvement over Mixup by 0.8% in ImageNet top-1 accuracy.
    
[^187]: Refiner: 针对联邦学习中的梯度泄漏攻击的数据精炼方法

    Refiner: Data Refining against Gradient Leakage Attacks in Federated Learning. (arXiv:2212.02042v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02042](http://arxiv.org/abs/2212.02042)

    Refiner提出了一种创新的防御范式，通过构建与原始数据具有低语义相似性的健壮数据，有效地混淆梯度泄漏攻击者，从而提高联邦学习系统的隐私保护能力。

    

    最近的研究引起了对联邦学习系统易受梯度泄漏攻击的关注。这类攻击利用客户端上传的梯度来重构其敏感数据，从而破坏了联邦学习的隐私保护能力。为了应对这一威胁，已经提出了各种防御机制来减轻攻击的影响，这些机制通过操纵上传的梯度来防止攻击。然而，实证评估表明这些防御措施在面对复杂攻击时具有有限的弹性，这表明迫切需要更有效的防御方法。本文提出了一种新的防御范式，不同于传统的梯度扰动方法，而是专注于构建健壮数据。直观地说，如果健壮数据与客户端原始数据具有很低的语义相似性，与健壮数据相关的梯度可以有效地混淆攻击者。为此，我们设计了Refiner，它同时优化了两个指标，用于隐私保护和...

    Recent works have brought attention to the vulnerability of Federated Learning (FL) systems to gradient leakage attacks. Such attacks exploit clients' uploaded gradients to reconstruct their sensitive data, thereby compromising the privacy protection capability of FL. In response, various defense mechanisms have been proposed to mitigate this threat by manipulating the uploaded gradients. Unfortunately, empirical evaluations have demonstrated limited resilience of these defenses against sophisticated attacks, indicating an urgent need for more effective defenses. In this paper, we explore a novel defensive paradigm that departs from conventional gradient perturbation approaches and instead focuses on the construction of robust data. Intuitively, if robust data exhibits low semantic similarity with clients' raw data, the gradients associated with robust data can effectively obfuscate attackers. To this end, we design Refiner that jointly optimizes two metrics for privacy protection and 
    
[^188]: 数据驱动的多项式随机森林: 一种具有强一致性的新随机森林变体

    Data-driven multinomial random forest: A new random forest variant with strong consistency. (arXiv:2211.15154v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15154](http://arxiv.org/abs/2211.15154)

    本研究改进了先前弱一致性随机森林变体的证明方法，提出了一种数据驱动的多项式随机森林（DMRF），并表明DMRF在分类和回归问题中具有更好的性能，并且实现了概率1的强一致性。

    

    本文将一些先前弱一致性随机森林变体的证明方法修改为强一致性证明方法，并改进了这些变体的数据利用，以获得更好的理论性质和实验性能。此外，我们提出了一种数据驱动的多项式随机森林（DMRF），其与BreimanRF（由Breiman提出）具有相同的复杂度，同时以概率1满足强一致性。在分类和回归问题上，它比先前仅满足弱一致性的RF变体具有更好的性能，并且在大多数情况下甚至超过了BreimanRF在分类任务上的表现。据我们所知，DMRF是当前实现了概率1的强一致性的低复杂性和高性能随机森林的一种变体。

    In this paper, we modify the proof methods of some previously weakly consistent variants of random forests into strongly consistent proof methods, and improve the data utilization of these variants in order to obtain better theoretical properties and experimental performance. In addition, we propose a data-driven multinomial random forest (DMRF), which has the same complexity with BreimanRF (proposed by Breiman) while satisfying strong consistency with probability 1. It has better performance in classification and regression problems than previous RF variants that only satisfy weak consistency, and in most cases even surpasses BreimanRF in classification tasks. To the best of our knowledge, DMRF is currently a low-complexity and high-performing variation of random forests that achieves strong consistency with probability 1.
    
[^189]: Powderworld：通过多样化任务分布来理解泛化的平台

    Powderworld: A Platform for Understanding Generalization via Rich Task Distributions. (arXiv:2211.13051v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.13051](http://arxiv.org/abs/2211.13051)

    Powderworld是一个直接在GPU上运行的轻量级但表现力强的模拟环境，用于提供泛化性的研究平台，包括世界建模和强化学习。实验表明，增加环境的复杂性可以改善世界模型和某些强化学习代理的泛化性能。

    

    强化学习面临的重大挑战之一是能够泛化到新任务。然而，泛化代理需要一组丰富、多样化的任务进行训练。为这些任务设计一个理想的环境很困难——理想的环境应支持一系列新兴现象、丰富的任务空间和快速的运行时。为了解决这个瓶颈问题，本文提出了Powderworld，一个直接在GPU上运行的轻量级但表现力强的模拟环境。在Powderworld内，提出了两个激发挑战的分布，一个用于世界建模，一个用于强化学习。每个分布都包含手动设计的测试任务，以检查泛化性能。实验表明，增加环境的复杂性可以改善世界模型和某些强化学习代理的泛化性能，但可能会抑制高方差环境下的学习。Powderworld旨在通过提供一种支持泛化研究的环境来解决这个问题。

    One of the grand challenges of reinforcement learning is the ability to generalize to new tasks. However, general agents require a set of rich, diverse tasks to train on. Designing a `foundation environment' for such tasks is tricky -- the ideal environment would support a range of emergent phenomena, an expressive task space, and fast runtime. To take a step towards addressing this research bottleneck, this work presents Powderworld, a lightweight yet expressive simulation environment running directly on the GPU. Within Powderworld, two motivating challenges distributions are presented, one for world-modelling and one for reinforcement learning. Each contains hand-designed test tasks to examine generalization. Experiments indicate that increasing the environment's complexity improves generalization for world models and certain reinforcement learning agents, yet may inhibit learning in high-variance environments. Powderworld aims to support the study of generalization by providing a so
    
[^190]: 基于数据驱动的网络神经科学：关于数据收集与基准的研究

    Data-Driven Network Neuroscience: On Data Collection and Benchmark. (arXiv:2211.12421v3 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2211.12421](http://arxiv.org/abs/2211.12421)

    本文提供了一个全面且高质量的人脑功能网络数据集，用于神经科学、机器学习和图分析的交叉研究。它利用解剖学和功能性磁共振成像来理解人脑的功能连接，并具有识别潜在神经退行性疾病的重要性。通过利用机器学习和图分析研究大脑的网络形式，能够预测这些疾病的早期发生。脑网络以图形方式表示，保留了丰富的结构和位置信息，传统的检查方法无法捕捉到这些信息。然而，缺乏公开可访问的脑网络数据限制了数据驱动的研究。

    

    本文提供了一份全面且高质量的人脑功能网络数据集，用于神经科学、机器学习和图分析的交叉研究。解剖学和功能性磁共振成像被用于理解人脑的功能连接，并且在识别阿尔茨海默氏症、帕金森症和自闭症等潜在的神经退行性疾病方面尤为重要。最近，利用机器学习和图分析研究以脑网络的形式来研究大脑的方法变得越来越流行，特别是用于预测这些疾病的早期发生。作为一个图形表示的脑网络保留了丰富的结构和位置信息，传统的检查方法无法捕捉到这些信息。然而，缺乏公开可访问的脑网络数据限制了研究人员进行数据驱动的探索。其中主要的困难在于复杂的领域特定的预处理步骤。

    This paper presents a comprehensive and quality collection of functional human brain \emph{network} data for potential research in the intersection of neuroscience, machine learning, and graph analytics. Anatomical and functional MRI images have been used to understand the functional connectivity of the human brain and are particularly important in identifying underlying neurodegenerative conditions such as Alzheimer's, Parkinson's, and Autism. Recently, the study of the brain in the form of brain networks using machine learning and graph analytics has become increasingly popular, especially to predict the early onset of these conditions. A brain network, represented as a graph, retains rich structural and positional information that traditional examination methods are unable to capture. However, the lack of publicly accessible brain network data prevents researchers from data-driven explorations. One of the main difficulties lies in the complicated domain-specific preprocessing steps 
    
[^191]: 模块化源分离的潜在迭代优化

    Latent Iterative Refinement for Modular Source Separation. (arXiv:2211.11917v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.11917](http://arxiv.org/abs/2211.11917)

    通过将深度神经网络模型的训练和推理过程重新定义为潜在信号表示的迭代映射，能够显著提高模型的资源利用效率。

    

    传统的源分离方法通过最小化整个训练集上的经验风险，将深度神经网络模型端到端地训练。在推理阶段，用户通过获取静态计算图，并在特定的混合信号上运行完整的模型，来获取估计的源信号。此外，许多模型由多个基本处理块组成，这些处理块按顺序应用。我们认为，通过将模型的训练和推理过程重新定义为潜在信号表示的迭代映射，我们可以显著提高资源利用效率。首先，我们可以多次对输出应用相同的处理块，以改进输入信号并提高参数效率。在训练过程中，我们可以采用基块级的程序，这样可以减少内存需求。因此，可以训练一个非常复杂的模型...

    Traditional source separation approaches train deep neural network models end-to-end with all the data available at once by minimizing the empirical risk on the whole training set. On the inference side, after training the model, the user fetches a static computation graph and runs the full model on some specified observed mixture signal to get the estimated source signals. Additionally, many of those models consist of several basic processing blocks which are applied sequentially. We argue that we can significantly increase resource efficiency during both training and inference stages by reformulating a model's training and inference procedures as iterative mappings of latent signal representations. First, we can apply the same processing block more than once on its output to refine the input signal and consequently improve parameter efficiency. During training, we can follow a block-wise procedure which enables a reduction on memory requirements. Thus, one can train a very complicate
    
[^192]: 自适应合并下的纵向网络有效估计

    Efficient Estimation for Longitudinal Network via Adaptive Merging. (arXiv:2211.07866v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.07866](http://arxiv.org/abs/2211.07866)

    本文提出了一个有效的纵向网络估计框架，利用自适应合并、张量分解和点过程等方法来减少估计偏差和方差。

    

    纵向网络由多个节点之间的时间边序列组成，其中时间边在实时中被观察到。随着在线社交平台和电子商务的兴起，它已经变得普遍，但在文献中往往被忽略。本文提出了一个有效的纵向网络估计框架，利用自适应网络合并、张量分解和点过程的优势。它合并相邻的稀疏网络，以扩大观测边的数量并减少估计方差，同时通过利用本地时间结构进行自适应网络邻域控制引入的估计偏差。提出了一个投影梯度下降算法来促进估计，其中每次迭代的估计错误上界被建立。进行了彻底的分析，以量化所提出方法的渐近行为，结果表明它可以显着减少估计偏差。

    Longitudinal network consists of a sequence of temporal edges among multiple nodes, where the temporal edges are observed in real time. It has become ubiquitous with the rise of online social platform and e-commerce, but largely under-investigated in literature. In this paper, we propose an efficient estimation framework for longitudinal network, leveraging strengths of adaptive network merging, tensor decomposition and point process. It merges neighboring sparse networks so as to enlarge the number of observed edges and reduce estimation variance, whereas the estimation bias introduced by network merging is controlled by exploiting local temporal structures for adaptive network neighborhood. A projected gradient descent algorithm is proposed to facilitate estimation, where the upper bound of the estimation error in each iteration is established. A thorough analysis is conducted to quantify the asymptotic behavior of the proposed method, which shows that it can significantly reduce the
    
[^193]: 通过多智能体强化学习实现具有二阶动力学的车辆的高效域覆盖

    Efficient Domain Coverage for Vehicles with Second-Order Dynamics via Multi-Agent Reinforcement Learning. (arXiv:2211.05952v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.05952](http://arxiv.org/abs/2211.05952)

    本研究提出了一种通过多智能体强化学习解决具有二阶动力学的车辆高效域覆盖问题的方法，网络架构采用了LSTM和自注意力技术，训练好的策略在性能上明显超过了传统的经典控制策略。

    

    合作的自主多智能体系统覆盖指定区域具有许多潜在应用，如无人机搜索和救援、森林火灾应对和实时高分辨率监测。传统方法涉及基于传感器数据设计基于模型的控制策略。然而，设计基于模型的控制器是具有挑战性的，最先进的经典控制策略仍然存在较大程度的次优性。在本文中，我们提出了一种适用于涉及具有二阶动力学的多智能体高效域覆盖问题的强化学习（RL）方法。我们的方法基于多智能体近端策略优化算法（MAPPO）。我们提出的网络架构包括LSTM和自注意力的结合，使得训练好的策略能够适应可变数量的智能体。我们训练好的策略明显优于最先进的经典控制策略。我们展示了...

    Collaborative autonomous multi-agent systems covering a specified area have many potential applications, such as UAV search and rescue, forest fire fighting, and real-time high-resolution monitoring. Traditional approaches for such coverage problems involve designing a model-based control policy based on sensor data. However, designing model-based controllers is challenging, and the state-of-the-art classical control policy still exhibits a large degree of sub-optimality. In this paper, we present a reinforcement learning (RL) approach for the multi-agent efficient domain coverage problem involving agents with second-order dynamics. Our approach is based on the Multi-Agent Proximal Policy Optimization Algorithm (MAPPO). Our proposed network architecture includes the incorporation of LSTM and self-attention, which allows the trained policy to adapt to a variable number of agents. Our trained policy significantly outperforms the state-of-the-art classical control policy. We demonstrate o
    
[^194]: 生成肿瘤空间蛋白组的反事实解释，以发现增强免疫渗透的有效策略

    Generating counterfactual explanations of tumor spatial proteomes to discover effective strategies for enhancing immune infiltration. (arXiv:2211.04020v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2211.04020](http://arxiv.org/abs/2211.04020)

    通过使用空间组学技术和反事实优化策略，该论文提出了一种设计肿瘤扰动以增强T细胞浸润的方法，为提高实体肿瘤的免疫治疗效果提供了一种新思路。

    

    肿瘤微环境(TME)对癌症预后有重要影响，其中免疫组成是关键。虽然可以改变免疫组成的治疗方法，包括免疫疗法，在治疗血液肿瘤方面取得了令人兴奋的成果，但对于免疫冷却的实体肿瘤效果较差。空间组学技术以前所未有的分子细节捕获了TME的空间组织，揭示了免疫细胞定位和分子信号之间的关系。在这里，我们将T细胞浸润预测定义为一种自我监督机器学习问题，并开发了一种反事实优化策略，利用患者肿瘤的大规模空间组学数据设计预测增强T细胞浸润的肿瘤扰动。卷积神经网络根据成像质谱细胞分析提供的TME信号分子预测T细胞分布。然后，基于梯度的反事实生成计算扰动。

    The tumor microenvironment (TME) significantly impacts cancer prognosis due to its immune composition. While therapies for altering the immune composition, including immunotherapies, have shown exciting results for treating hematological cancers, they are less effective for immunologically-cold, solid tumors. Spatial omics technologies capture the spatial organization of the TME with unprecedented molecular detail, revealing the relationship between immune cell localization and molecular signals. Here, we formulate T-cell infiltration prediction as a self-supervised machine learning problem and develop a counterfactual optimization strategy that leverages large scale spatial omics profiles of patient tumors to design tumor perturbations predicted to boost T-cell infiltration. A convolutional neural network predicts T-cell distribution based on signaling molecules in the TME provided by imaging mass cytometry. Gradient-based counterfactual generation, then, computes perturbations predic
    
[^195]: 知识图谱嵌入：基于表示空间的综述

    Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces. (arXiv:2211.03536v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.03536](http://arxiv.org/abs/2211.03536)

    本文从表示空间的角度对知识图谱嵌入技术进行了综述，通过分类和讨论不同的数学角度和方法，介绍了KGE模型及其优势。

    

    知识图谱嵌入（KGE）是一种越来越受欢迎的技术，旨在将知识图谱中的实体和关系表示为低维语义空间，用于广泛的应用，如链接预测，知识推理和知识补全。本文从表示空间的角度对现有的KGE技术进行了系统综述。特别地，我们基于表示空间的三个数学角度（代数角度、几何角度和分析角度）构建了一个细粒度分类，介绍了基本数学空间的严格定义，然后深入研究了KGE模型及其数学特性。我们进一步讨论了三个类别中的不同KGE方法，并总结了空间优势在不同嵌入需求上的作用。通过整理来自下游任务的实验结果，我们还探讨了KGE的优势。

    Knowledge graph embedding (KGE) is an increasingly popular technique that aims to represent entities and relations of knowledge graphs into low-dimensional semantic spaces for a wide spectrum of applications such as link prediction, knowledge reasoning and knowledge completion. In this paper, we provide a systematic review of existing KGE techniques based on representation spaces. Particularly, we build a fine-grained classification to categorise the models based on three mathematical perspectives of the representation spaces: (1) Algebraic perspective, (2) Geometric perspective, and (3) Analytical perspective. We introduce the rigorous definitions of fundamental mathematical spaces before diving into KGE models and their mathematical properties. We further discuss different KGE methods over the three categories, as well as summarise how spatial advantages work over different embedding needs. By collating the experimental results from downstream tasks, we also explore the advantages of
    
[^196]: 用于留一法交叉验证的集中不等式

    Concentration inequalities for leave-one-out cross validation. (arXiv:2211.02478v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2211.02478](http://arxiv.org/abs/2211.02478)

    本文证明了估计器的稳定性足以说明留一法交叉验证是可靠的，并通过提供集中界限超出Lipschitz连续性假设的损失函数或估计器，为我们提供了一个相对丰富的分布类。

    

    在本文中，我们证明了估计器的稳定性足以说明留一法交叉验证是一个可靠的过程，通过在一个通用的框架中提供集中界限。特别是，我们在损失函数或估计器上提供了超过Lipschitz连续性假设的集中界限。我们通过依赖具有满足对数Sobolev不等式的分布的随机变量来获得我们的结果，这为我们提供了一个相对丰富的分布类。我们通过考虑几个有趣的例子来说明我们的方法，包括线性回归，核密度估计以及稳定/截断估计器，例如稳定的核回归。

    In this article we prove that estimator stability is enough to show that leave-one-out cross validation is a sound procedure, by providing concentration bounds in a general framework. In particular, we provide concentration bounds beyond Lipschitz continuity assumptions on the loss or on the estimator. We obtain our results by relying on random variables with distribution satisfying the logarithmic Sobolev inequality, providing us a relatively rich class of distributions. We illustrate our method by considering several interesting examples, including linear regression, kernel density estimation, and stabilized/truncated estimators such as stabilized kernel regression.
    
[^197]: 全球野火预测的深度学习

    Deep Learning for Global Wildfire Forecasting. (arXiv:2211.00534v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00534](http://arxiv.org/abs/2211.00534)

    本研究创建了一个全球火灾数据集，并使用分割深度学习模型预测全球烧毁面积，为半季度尺度提供了高效准确的预测能力。

    

    预计气候变化将通过加剧火灾天气情况而加剧野火活动。改善我们在全球范围内预测野火的能力对于减轻其负面影响至关重要。在这项工作中，我们创建了一个全球火灾数据集，并使用分割深度学习模型展示了一个用于预测全球烧毁面积的原型，实现了半季度尺度上的预测。尤其是，我们提供了一个开放获取的全球分析就绪数据立方体，其中包含与季节性和半季度性火灾驱动因素（气候、植被、海洋指数、与人相关的变量）以及2001-2021年的历史烧毁面积和野火排放相关的各种变量。我们训练了一个深度学习模型，将全球野火预测视为图像分割任务，并成功预测了烧毁区域出现的前8天、16天、32天和64天。我们的工作推动了深度学习在全球烧毁面积预测中的应用。

    Climate change is expected to aggravate wildfire activity through the exacerbation of fire weather. Improving our capabilities to anticipate wildfires on a global scale is of uttermost importance for mitigating their negative effects. In this work, we create a global fire dataset and demonstrate a prototype for predicting the presence of global burned areas on a sub-seasonal scale with the use of segmentation deep learning models. Particularly, we present an open-access global analysis-ready datacube, which contains a variety of variables related to the seasonal and sub-seasonal fire drivers (climate, vegetation, oceanic indices, human-related variables), as well as the historical burned areas and wildfire emissions for 2001-2021. We train a deep learning model, which treats global wildfire forecasting as an image segmentation task and skillfully predicts the presence of burned areas 8, 16, 32 and 64 days ahead of time. Our work motivates the use of deep learning for global burned area
    
[^198]: Occam学习

    Occam learning. (arXiv:2210.13179v2 [cond-mat.dis-nn] UPDATED)

    [http://arxiv.org/abs/2210.13179](http://arxiv.org/abs/2210.13179)

    本文讨论了一种具有固定隐藏层分布的概率神经网络模型，该模型选择简单、易解释，不需要过度参数化，同时训练有效。模型的隐藏单元为二元变量时具有以特征为基础的自然解释。作者认为隐藏变量的分布应该遵循最大关联度原则，并介绍了分层特征模型（HFM）作为满足这一原则并对特征空间进行中性先验组织的模型。

    

    我们讨论了一种无监督学习的概率神经网络模型，在这种模型中，隐藏层的分布是固定的。我们认为采用这种体系架构的机器学习具有许多令人满意的性质。例如，该模型可以选择为简单且易解释的模型，不需要过度参数化，而且在热力学意义下，训练更有效。当隐藏单元为二元变量时，这些模型具有以特征为基础的自然解释。我们表明，缺乏特征的状态对应于在特征方面最大程度的无知状态，并且，学习第一个特征取决于数据的非高斯统计属性。我们认为应该根据最大关联度原则选择隐藏变量的分布。我们介绍了分层特征模型（HFM）作为满足这一原则并对特征空间进行中性先验组织的模型。

    We discuss probabilistic neural network models for unsupervised learning where the distribution of the hidden layer is fixed. We argue that learning machines with this architecture enjoy a number of desirable properties. For example, the model can be chosen as a simple and interpretable one, it does not need to be over-parametrised and training is argued to be efficient in a thermodynamic sense. When hidden units are binary variables, these models have a natural interpretation in terms of features. We show that the featureless state corresponds to a state of maximal ignorance about the features and that learning the first feature depends on non-Gaussian statistical properties of the data. We suggest that the distribution of hidden variables should be chosen according to the principle of maximal relevance. We introduce the Hierarchical Feature Model (HFM) as an example of a model that satisfies this principle, and that encodes a neutral a priori organisation of the feature space. We pre
    
[^199]: 最优AdaBoost算法的收敛性

    Optimal AdaBoost Converges. (arXiv:2210.07808v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.07808](http://arxiv.org/abs/2210.07808)

    本研究通过形式证明，展示了最优AdaBoost算法的分类器和边缘的收敛性质，结果与几十年的研究相一致。

    

    本研究是关于AdaBoost机器学习算法分类器和边缘的收敛性性质的形式证明的预印本集合。针对这些收敛性性质的猜想和特殊情况已经编写了各种数学和计算机科学论文。此外，AdaBoost的边缘在围绕该算法的研究中占据重要地位。在本文的顶点，我们展示了AdaBoost的分类器和边缘如何收敛到与几十年的研究相一致的值。在此之后，我们展示了与组合分类器相关的各种数量是如何收敛的。

    The following work is a preprint collection of formal proofs regarding the convergence properties of the AdaBoost machine learning algorithm's classifier and margins. Various math and computer science papers have been written regarding conjectures and special cases of these convergence properties. Furthermore, the margins of AdaBoost feature prominently in the research surrounding the algorithm. At the zenith of this paper we present how AdaBoost's classifier and margins converge on a value that agrees with decades of research. After this, we show how various quantities associated with the combined classifier converge.
    
[^200]: 使用图神经网络预测流体-结构相互作用

    Predicting fluid-structure interaction with graph neural networks. (arXiv:2210.04193v2 [physics.flu-dyn] UPDATED)

    [http://arxiv.org/abs/2210.04193](http://arxiv.org/abs/2210.04193)

    本文提出了一种使用图神经网络预测流体-结构相互作用的降阶建模框架，通过准单体的方法将系统状态在时间上演变为两个子网络，并采用超图神经网络预测流体状态的演变，具有很好的效果。

    

    我们提出了一个旋转等变、准单体的图神经网络框架，用于流体-结构相互作用系统的降阶建模。通过任意拉格朗日-欧拉公式，系统状态在时间上演变有两个子网络。通过复数值的适当正交分解，将网格的移动简化为几个系数的演变，这些系数的时间预测由一个单层多层感知机处理。采用有限元启发式超图神经网络，根据整个系统的状态预测流体状态的演变。结构状态隐含地通过网格在固液界面上的运动进行建模，因此提出的框架是准单体的。我们在两个典型的流体-结构系统上评估了该框架的有效性，即弹性安装的圆柱体周围的流动和弹性达...

    We present a rotation equivariant, quasi-monolithic graph neural network framework for the reduced-order modeling of fluid-structure interaction systems. With the aid of an arbitrary Lagrangian-Eulerian formulation, the system states are evolved temporally with two sub-networks. The movement of the mesh is reduced to the evolution of several coefficients via complex-valued proper orthogonal decomposition, and the prediction of these coefficients over time is handled by a single multi-layer perceptron. A finite element-inspired hypergraph neural network is employed to predict the evolution of the fluid state based on the state of the whole system. The structural state is implicitly modeled by the movement of the mesh on the solid-fluid interface; hence it makes the proposed framework quasi-monolithic. The effectiveness of the proposed framework is assessed on two prototypical fluid-structure systems, namely the flow around an elastically-mounted cylinder, and the flow around a hyperelas
    
[^201]: 图神经网络的通用Prompt调整方法

    Universal Prompt Tuning for Graph Neural Networks. (arXiv:2209.15240v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15240](http://arxiv.org/abs/2209.15240)

    本文介绍了一种名为Graph Prompt Feature（GPF）的新方法，可通用地调整预先训练过的图神经网络模型，操作于输入的特征空间，能够对应任何形式的Prompt函数。

    

    近年来，Prompt调整在适应预训练模型方面引起了研究热潮。与语言领域采用的统一预训练策略不同，图形领域展示了多样化的预训练策略，设计适当的基于Prompt的图神经网络调整方法面临挑战。本文引入了一种名为Graph Prompt Feature (GPF) 的通用Prompt调整方法，可适用于任何预训练策略下的预训练图神经网络模型。GPF在输入图形的特征空间上操作，理论上可实现与任何形式的Prompt函数等效的效果。因此，我们不再需要明确说明每个预训练策略对应的Prompt函数。相反，我们采用GPF来实现调整。

    In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to o
    
[^202]: Pareto Actor-Critic用于多智能体强化学习中的均衡选择

    Pareto Actor-Critic for Equilibrium Selection in Multi-Agent Reinforcement Learning. (arXiv:2209.14344v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14344](http://arxiv.org/abs/2209.14344)

    本文提出了Pareto Actor-Critic（Pareto-AC）算法来解决多智能体强化学习中的均衡选择问题，该算法利用无冲突游戏的性质，即Pareto最优均衡最大化了所有智能体的回报。实验结果显示Pareto-AC相比其他七种最先进的算法更能收敛到更高的回合回报。

    

    本文关注于在无冲突多智能体博弈中的均衡选择问题，具体研究了在多个现有均衡中选择Pareto最优均衡的问题。已经表明，许多最先进的多智能体强化学习算法由于每个智能体在训练过程中对其他智能体政策的不确定性而容易收敛到Pareto支配的均衡状态。为了解决次优均衡选择问题，我们提出了Pareto Actor-Critic（Pareto-AC），这是一种用于无冲突游戏（合作游戏的超集）的演员-评论家算法，其利用了一个简单的性质：无冲突游戏中的Pareto最优均衡最大化了所有智能体的回报，因此对于所有智能体来说是首选结果。我们在各种多智能体博弈中评估了Pareto-AC，并显示它收敛到更高的回合回报，与七种最先进的多智能体强化学习算法相比，Pareto-AC成功地收敛到了一个

    This work focuses on equilibrium selection in no-conflict multi-agent games, where we specifically study the problem of selecting a Pareto-optimal equilibrium among several existing equilibria. It has been shown that many state-of-the-art multi-agent reinforcement learning (MARL) algorithms are prone to converging to Pareto-dominated equilibria due to the uncertainty each agent has about the policy of the other agents during training. To address sub-optimal equilibrium selection, we propose Pareto Actor-Critic (Pareto-AC), which is an actor-critic algorithm that utilises a simple property of no-conflict games (a superset of cooperative games): the Pareto-optimal equilibrium in a no-conflict game maximises the returns of all agents and therefore is the preferred outcome for all agents. We evaluate Pareto-AC in a diverse set of multi-agent games and show that it converges to higher episodic returns compared to seven state-of-the-art MARL algorithms and that it successfully converges to a
    
[^203]: 信号时间逻辑谓词的模型预测鲁棒性

    Model Predictive Robustness of Signal Temporal Logic Predicates. (arXiv:2209.07881v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.07881](http://arxiv.org/abs/2209.07881)

    本论文提出了一种新的模型预测鲁棒性的概念来更系统地评估信号时间逻辑中底层谓词的鲁棒性，并成功验证了该方法在自动驾驶中的应用。

    

    信号时间逻辑的鲁棒性不仅评估了一个信号是否符合规范，而且还提供了一个衡量公式被满足或违反的程度的指标。鲁棒性的计算基于对底层谓词的鲁棒性进行评估。然而，谓词的鲁棒性通常以一种无模型的方式定义，即不包括系统动态。而且，精确定义复杂谓词的鲁棒性通常是非平凡的。为了解决这些问题，我们提出了一种模型预测鲁棒性的概念，通过考虑基于模型的预测，提供了比之前方法更系统的评估鲁棒性的方法。特别地，我们使用高斯过程回归来学习基于预先计算的预测的鲁棒性，以便可以在线高效地计算鲁棒性值。我们评估了我们的方法，并使用在记录的数据上使用在形式化交通规则中使用的谓词的自动驾驶用例验证了我们的方法。

    The robustness of signal temporal logic not only assesses whether a signal adheres to a specification but also provides a measure of how much a formula is fulfilled or violated. The calculation of robustness is based on evaluating the robustness of underlying predicates. However, the robustness of predicates is usually defined in a model-free way, i.e., without including the system dynamics. Moreover, it is often nontrivial to define the robustness of complicated predicates precisely. To address these issues, we propose a notion of model predictive robustness, which provides a more systematic way of evaluating robustness compared to previous approaches by considering model-based predictions. In particular, we use Gaussian process regression to learn the robustness based on precomputed predictions so that robustness values can be efficiently computed online. We evaluate our approach for the use case of autonomous driving with predicates used in formalized traffic rules on a recorded dat
    
[^204]: 5q032e@SMM4H'22: 基于Transformer的COVID-19相关推文前提分类

    5q032e@SMM4H'22: Transformer-based classification of premise in tweets related to COVID-19. (arXiv:2209.03851v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.03851](http://arxiv.org/abs/2209.03851)

    本文提出了一个基于Transformer的预测模型，用于在Twitter文本中分类前提。实验结果表明，RoBERTa模型在前提预测任务中表现出竞争性的性能。

    

    社交网络数据评估的自动化是自然语言处理的经典挑战之一。在COVID-19大流行期间，从公共信息中挖掘人们的立场对于理解对健康命令的态度至关重要。在本文中，作者们提出了基于Transformer架构的预测模型，用于对推特文本中前提的分类。本工作是作为2022年Social Media Mining for Health (SMM4H)研讨会的一部分完成的。我们在构建高效捕捉推文语义的流程时，探索了现代基于Transformer的分类器。我们的实验在一个推特数据集上表明，在前提预测任务中，RoBERTa模型优于其他Transformer模型。该模型在ROC AUC值为0.807，F1分数为0.7648方面取得了竞争性的性能。

    Automation of social network data assessment is one of the classic challenges of natural language processing. During the COVID-19 pandemic, mining people's stances from public messages have become crucial regarding understanding attitudes towards health orders. In this paper, the authors propose the predictive model based on transformer architecture to classify the presence of premise in Twitter texts. This work is completed as part of the Social Media Mining for Health (SMM4H) Workshop 2022. We explored modern transformer-based classifiers in order to construct the pipeline efficiently capturing tweets semantics. Our experiments on a Twitter dataset showed that RoBERTa is superior to the other transformer models in the case of the premise prediction task. The model achieved competitive performance with respect to ROC AUC value 0.807, and 0.7648 for the F1 score.
    
[^205]: 攻击脉冲：关于脉冲神经网络对抗性样本的可转移性与安全性的研究

    Attacking the Spike: On the Transferability and Security of Spiking Neural Networks to Adversarial Examples. (arXiv:2209.03358v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2209.03358](http://arxiv.org/abs/2209.03358)

    这项研究主要关注于脉冲神经网络(SNNs)对抗性样本的鲁棒性和转移性。研究发现，成功的白盒对抗攻击SNNs在很大程度上依赖于替代梯度技术，并且非SNN架构创建的对抗样本往往不被SNNs误分类。

    

    脉冲神经网络(SNNs)因其高能效和最近在分类性能上的进展而受到广泛关注。然而，与传统的深度学习方法不同，对SNNs对抗性样本的鲁棒性的分析和研究仍然相对不完善。在这项工作中，我们关注于推进SNNs的对抗攻击方面，并做出了三个主要贡献。首先，我们展示了成功的白盒对抗攻击SNNs在很大程度上依赖于底层的替代梯度技术，即使在对抗性训练SNNs的情况下也一样。其次，利用最佳的替代梯度技术，我们分析了对抗攻击在SNNs和其他最先进的架构如Vision Transformers(ViTs)和Big Transfer Convolutional Neural Networks(CNNs)之间的可转移性。我们证明了非SNN架构创建的对抗样本往往不被SNNs误分类。第三，由于缺乏一个共性

    Spiking neural networks (SNNs) have attracted much attention for their high energy efficiency and for recent advances in their classification performance. However, unlike traditional deep learning approaches, the analysis and study of the robustness of SNNs to adversarial examples remain relatively underdeveloped. In this work, we focus on advancing the adversarial attack side of SNNs and make three major contributions. First, we show that successful white-box adversarial attacks on SNNs are highly dependent on the underlying surrogate gradient technique, even in the case of adversarially trained SNNs. Second, using the best surrogate gradient technique, we analyze the transferability of adversarial attacks on SNNs and other state-of-the-art architectures like Vision Transformers (ViTs) and Big Transfer Convolutional Neural Networks (CNNs). We demonstrate that the adversarial examples created by non-SNN architectures are not misclassified often by SNNs. Third, due to the lack of an ubi
    
[^206]: 深度强化学习中的白盒对抗策略研究

    White-Box Adversarial Policies in Deep Reinforcement Learning. (arXiv:2209.02167v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.02167](http://arxiv.org/abs/2209.02167)

    本文研究了在强化学习中训练对抗策略的方法，提出了基于白盒攻击的策略，能够访问目标代理的内部状态，从而识别其漏洞，攻击成功率更高。

    

    在强化学习中，对抗策略可以通过训练对抗代理来最小化目标代理的奖励来开发。之前的研究研究了黑盒版本的这些攻击，其中对手仅观察世界状态，并将目标代理视为环境的任何其他部分。然而，这并没有考虑问题中的附加结构。在这项工作中，我们从白盒攻击的文献中获得灵感，以训练更有效的对抗策略。我们研究了白盒对抗策略，并显示访问目标代理的内部状态可以用于识别其漏洞。我们做出了两个贡献。(1)我们介绍了白盒对抗策略，其中攻击者在每个时间步观察目标的内部状态和世界状态。我们制定了使用这些策略攻击2人游戏和生成文本语言模型中的代理的方法。(2)我们证明了与黑盒攻击相比，这些策略可以实现更高的攻击成功率，特别是当目标代理的内部状态比较复杂时。

    In reinforcement learning (RL), adversarial policies can be developed by training an adversarial agent to minimize a target agent's rewards. Prior work has studied black-box versions of these attacks where the adversary only observes the world state and treats the target agent as any other part of the environment. However, this does not take into account additional structure in the problem. In this work, we take inspiration from the literature on white-box attacks to train more effective adversarial policies. We study white-box adversarial policies and show that having access to a target agent's internal state can be useful for identifying its vulnerabilities. We make two contributions. (1) We introduce white-box adversarial policies where an attacker observes both a target's internal state and the world state at each timestep. We formulate ways of using these policies to attack agents in 2-player games and text-generating language models. (2) We demonstrate that these policies can ach
    
[^207]: 神经影像的管线不变表示学习

    Pipeline-Invariant Representation Learning for Neuroimaging. (arXiv:2208.12909v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.12909](http://arxiv.org/abs/2208.12909)

    本研究评估了预处理管线对神经影像的影响，并提出了两种管线不变表示学习方法，用于提高分类性能的鲁棒性和捕捉相似神经网络表示。实验证明，这些模型具有独特和共享优势。

    

    深度学习在神经影像领域得到了广泛应用，包括从磁共振成像（MRI）体积中预测脑-表型关系。MRI数据通常需要在建模之前进行广泛的预处理，但是不同的MRI预处理管线引入的变异性可能导致不同的科学发现，即使使用相同的数据。在数据为中心的视角下，我们首先评估预处理管线选择对监督学习模型的下游性能的影响。然后，我们提出了两种管线不变表示学习方法（MPSL和PXL），以提高分类性能的鲁棒性并捕捉相似的神经网络表示。通过使用来自英国生物库（UK Biobank）数据集的2000名人体对象，我们展示了所提出的模型的独特和共享优势，特别是MPSL可用于改善新管线的外部样本泛化能力，而PXL可用于改善同一样本在不同管线间的性能一致性。

    Deep learning has been widely applied in neuroimaging, including predicting brain-phenotype relationships from magnetic resonance imaging (MRI) volumes. MRI data usually requires extensive preprocessing prior to modeling, but variation introduced by different MRI preprocessing pipelines may lead to different scientific findings, even when using the identical data. Motivated by the data-centric perspective, we first evaluate how preprocessing pipeline selection can impact the downstream performance of a supervised learning model. We next propose two pipeline-invariant representation learning methodologies, MPSL and PXL, to improve robustness in classification performance and to capture similar neural network representations. Using 2000 human subjects from the UK Biobank dataset, we demonstrate that proposed models present unique and shared advantages, in particular that MPSL can be used to improve out-of-sample generalization to new pipelines, while PXL can be used to improve within-sam
    
[^208]: 超图SBM中的社区发现：给定相似性矩阵的最优恢复

    Community Detection in the Hypergraph SBM: Optimal Recovery Given the Similarity Matrix. (arXiv:2208.12227v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2208.12227](http://arxiv.org/abs/2208.12227)

    本文研究了超图随机块模型中的社区发现问题，通过分析相似性矩阵和设计谱算法，实现了在不同密度条件下的精确恢复和高效计算。

    

    社区发现是网络科学中的一个基础问题。本文考虑了超图随机块模型（HSBM）中的社区发现问题，重点是确切的社区恢复。我们研究了基于相似性矩阵W的多项式时间算法的性能，其中W_{ij}表示同时包含i和j的超边的数量。在这个信息模型下，Kim，Bandeira和Goemans确定了在对数度数区间内的精确恢复的信息论阈值，他们提出了一个半定规划松弛，认为这是最优的猜想。在本文中，我们证实了这个猜想。我们还设计了一个简单且高效的谱算法，几乎具有线性运行时间，并且证明它达到了信息论阈值。此外，这个谱算法还能在更密集的情况下成功，并且比之前的方法更高效，因此被认为是一种有效的方法。

    Community detection is a fundamental problem in network science. In this paper, we consider community detection in hypergraphs drawn from the $hypergraph$ $stochastic$ $block$ $model$ (HSBM), with a focus on exact community recovery. We study the performance of polynomial-time algorithms which operate on the $similarity$ $matrix$ $W$, where $W_{ij}$ reports the number of hyperedges containing both $i$ and $j$. Under this information model, Kim, Bandeira, and Goemans determined the information-theoretic threshold for exact recovery in the logarithmic degree regime, and proposed a semidefinite programming relaxation which they conjectured to be optimal. In this paper, we confirm this conjecture. We also design a simple and highly efficient spectral algorithm with nearly linear runtime and show that it achieves the information-theoretic threshold. Moreover, the spectral algorithm also succeeds in denser regimes and is considerably more efficient than previous approaches, establishing it a
    
[^209]: 带有同时检测社区和异常的签名网络嵌入及应用

    Signed Network Embedding with Application to Simultaneous Detection of Communities and Anomalies. (arXiv:2207.09324v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2207.09324](http://arxiv.org/abs/2207.09324)

    本文开发了一个统一的嵌入模型，用于解决签名网络中的平衡结构和异常效应，并在社区检测、异常检测和网络推断等任务中取得了良好表现。

    

    在现实生活中，我们经常观察到带有附加符号信息的网络，然而这些信息在现有的网络模型中被大部分忽视了。本文针对签名网络开发了一个统一的嵌入模型，以解开错综复杂的平衡结构和异常效应，从而可以极大地促进下游分析，包括社区检测、异常检测和网络推断。所提出的模型通过低秩加稀疏矩阵分解捕捉平衡结构和异常效应，并通过正则化方法联合估计二者。在网络嵌入、社区检测和异常检测方面，它的理论保证是建立在渐近一致性和有限样本概率边界的基础上。所提出的嵌入模型的优势还通过对合成网络和国际关系网络进行广泛的数值实验得到了证明。

    Signed networks are frequently observed in real life with additional sign information associated with each edge, yet such information has been largely ignored in existing network models. This paper develops a unified embedding model for signed networks to disentangle the intertwined balance structure and anomaly effect, which can greatly facilitate the downstream analysis, including community detection, anomaly detection, and network inference. The proposed model captures both balance structure and anomaly effect through a low rank plus sparse matrix decomposition, which are jointly estimated via a regularized formulation. Its theoretical guarantees are established in terms of asymptotic consistency and finite-sample probability bounds for network embedding, community detection and anomaly detection. The advantage of the proposed embedding model is also demonstrated through extensive numerical experiments on both synthetic networks and an international relation network.
    
[^210]: 用于外样本模型评估的诊断工具

    Diagnostic Tool for Out-of-Sample Model Evaluation. (arXiv:2206.10982v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.10982](http://arxiv.org/abs/2206.10982)

    本文提出了一种用于外样本模型评估的诊断工具，可以通过有限的校准数据集来表征模型在未来外样本上的损失，并提供了简单易用且易于解释的方法。该工具可以量化分布转变的影响，促进回归分析，帮助实现模型选择和超参数调优。

    

    模型适配性评估是机器学习的关键部分。标准范式是通过对训练数据上的选择损失函数进行平均，以实现在未来数据上获得小的损失。在本文中，我们考虑使用有限的校准数据集来表征模型在未来外样本上的损失。我们提出了一个简单的模型诊断工具，在弱假设下提供有限样本的保证。该工具简单易用且易于解释。通过展示几个数值实验，我们展示了提出的方法如何量化分布转变的影响，促进回归分析，以及实现模型选择和超参数调优。

    Assessment of model fitness is a key part of machine learning. The standard paradigm is to learn models by minimizing a chosen loss function averaged over training data, with the aim of achieving small losses on future data. In this paper, we consider the use of a finite calibration data set to characterize the future, out-of-sample losses of a model. We propose a simple model diagnostic tool that provides finite-sample guarantees under weak assumptions. The tool is simple to compute and to interpret. Several numerical experiments are presented to show how the proposed method quantifies the impact of distribution shifts, aids the analysis of regression, and enables model selection as well as hyper-parameter tuning.
    
[^211]: 一种用于土壤湿度获取的机器学习数据融合模型

    A Machine Learning Data Fusion Model for Soil Moisture Retrieval. (arXiv:2206.09649v3 [physics.ao-ph] UPDATED)

    [http://arxiv.org/abs/2206.09649](http://arxiv.org/abs/2206.09649)

    本研究开发了一种深度学习模型，利用多种传感器数据和地球物理变量，可以准确估算土壤顶部的体积湿度含量。该模型在全球范围内的1300个传感器数据上进行训练和评估，表现出较高的相关性和较小的误差，并可用于生成高分辨率的土壤湿度图。

    

    我们开发了一种基于深度学习的卷积回归模型，用于估算土壤顶部约5厘米处的体积土壤湿度含量。输入预测因子包括Sentinel-1（主动雷达）、Sentinel-2（光学图像）和SMAP（被动雷达），以及来自SoilGrids的地球物理变量和来自GLDAS的模拟土壤湿度场。该模型在全球范围内的约1300个原地传感器数据上进行了训练和评估，得到了平均每个传感器相关系数为0.727和ubRMSE为0.054的结果，并可用于生成320m分辨率的土壤湿度图。这些结果与其他13个地点的土壤湿度研究进行了基准测试，并使用消减研究来确定重要的预测因子。

    We develop a deep learning based convolutional-regression model that estimates the volumetric soil moisture content in the top ~5 cm of soil. Input predictors include Sentinel-1 (active radar), Sentinel-2 (optical imagery), and SMAP (passive radar) as well as geophysical variables from SoilGrids and modelled soil moisture fields from GLDAS. The model was trained and evaluated on data from ~1300 in-situ sensors globally over the period 2015 - 2021 and obtained an average per-sensor correlation of 0.727 and ubRMSE of 0.054, and can be used to produce a soil moisture map at a nominal 320m resolution. These results are benchmarked against 13 other soil moisture works at different locations, and an ablation study was used to identify important predictors.
    
[^212]: TFLEX: 时间特征逻辑嵌入框架用于时间知识图谱上的复杂推理

    TFLEX: Temporal Feature-Logic Embedding Framework for Complex Reasoning over Temporal Knowledge Graph. (arXiv:2205.14307v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14307](http://arxiv.org/abs/2205.14307)

    本文提出了第一个用于时间知识图谱的复杂查询嵌入方法TFLEX，能够自然地建模所有一阶逻辑（FOL）运算，同时扩展了向量逻辑以处理三个额外的时间运算符。

    

    在许多人工智能任务中，知识图谱上的多跳逻辑推理发挥着基本作用。最近的复杂查询嵌入（CQE）方法侧重于静态知识图谱，而时间知识图谱（TKG）尚未得到充分探索。TKG上的推理面临两个挑战：1.查询应该回答实体或时间戳；2.运算符应该同时考虑实体集上的集合逻辑和时间戳集上的时间逻辑。为了解决这个问题，我们定义了TKG上的多跳逻辑推理问题。通过三个生成的数据集，我们提出了第一个名为TFLEX的时间CQE，用于回答时间复杂查询。我们利用向量逻辑计算Temporal Feature-Logic嵌入的逻辑部分，从而自然地建模实体集上的所有一阶逻辑（FOL）运算。此外，我们的框架扩展时间戳集上的向量逻辑，以处理三个额外的时间运算符（After，Before和Between）。

    Multi-hop logical reasoning over knowledge graph (KG) plays a fundamental role in many artificial intelligence tasks. Recent complex query embedding (CQE) methods for reasoning focus on static KGs, while temporal knowledge graphs (TKGs) have not been fully explored. Reasoning over TKGs has two challenges: 1. The query should answer entities or timestamps; 2. The operators should consider both set logic on entity set and temporal logic on timestamp set. To bridge this gap, we define the multi-hop logical reasoning problem on TKGs. With generated three datasets, we propose the first temporal CQE named Temporal Feature-Logic Embedding framework (TFLEX) to answer the temporal complex queries. We utilize vector logic to compute the logic part of Temporal Feature-Logic embeddings, thus naturally modeling all First-Order Logic (FOL) operations on entity set. In addition, our framework extends vector logic on timestamp set to cope with three extra temporal operators (After, Before and Between)
    
[^213]: Speculative Decoding: 无损加速自回归翻译

    Speculative Decoding: Lossless Speedup of Autoregressive Translation. (arXiv:2203.16487v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.16487](http://arxiv.org/abs/2203.16487)

    Speculative Decoding是一种新型解码范式，结合了自回归翻译（AT）和非自回归翻译（NAT）的优势，提供了无损加速的翻译方法。在每个解码步骤中，它推测性地预测下一个标记，并使用验证模型确保翻译结果与AT完全相同。通过推测解码和验证的协作，实现了更快的解码速度，同时保持翻译质量不变。实验证明，原始的SpecDec与AT贪婪解码的结果完全相同。

    

    与之前一些牺牲翻译质量加速自回归翻译（AT）的工作不同，我们提出了Speculative Decoding（SpecDec）-一种受计算机体系结构中的推测执行启发的新型解码范式，它结合了AT和非自回归翻译（NAT）的各自优势，实现了在翻译过程中的无损加速。在每个解码步骤中，SpecDec首先使用NAT模型推测性地预测（即解码）下一个k个标记，然后使用AT模型验证这些标记，只有通过验证的预测标记才会被接受作为解码结果，以确保其翻译结果与AT完全相同。NAT的推测和AT的验证之间的协作使得解码速度大大提高，同时不损失翻译质量，这是由于推测解码所支持的并行计算。我们在4个标准WMT翻译基准上进行实验，并证实原始的SpecDec与AT贪婪解码的结果完全相同，速度提高了约 $k$倍。

    Different from some previous work accelerating autoregressive translation (AT) at the sacrifice of quality, we propose Speculative Decoding (SpecDec) -a novel decoding paradigm inspired by speculative execution in computer architecture, which combines respective advantages of AT and non-autoregressive translation (NAT) for lossless speedup of translation. At each decoding step, SpecDec first speculatively drafts (i.e. decodes) next $k$ tokens with an NAT model and then verifies them with an AT model, where only the drafted tokens passing the verification are accepted as decoded tokens for guaranteeing its translation result is exactly the same as AT. The collaboration of NAT drafting and AT verification leads to a much higher decoding speed without quality loss due to parallel computing enabled by speculative decoding.  We conduct experiments in 4 standard WMT translation benchmarks and confirm the vanilla SpecDec yields exactly the same results as AT greedy decoding with an around $
    
[^214]: 使用图神经网络处理稀疏奖励

    Dealing with Sparse Rewards Using Graph Neural Networks. (arXiv:2203.13424v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.13424](http://arxiv.org/abs/2203.13424)

    本研究提出了基于图神经网络的奖励塑造方法的两种修改，一种采用先进的聚合函数，另一种利用注意力机制。实验证实了这些解决方案在三维环境导航任务中的有效性。

    

    部分可观察环境中的深度强化学习本身就是一项困难的任务，当奖励信号稀疏时更加复杂。大多数涉及三维环境导航的任务只提供有限的信息给智能体。普遍情况下，智能体从环境中接收到一个视觉观察输入，并在一集结束时得到奖励。良好的奖励函数可以大大改善这类任务的强化学习算法的收敛性。增加奖励信号密度的经典方法是用补充奖励来改善奖励。这种技术被称为奖励塑造。在本研究中，我们提出了两种改进最近一种基于图卷积网络的奖励塑造方法的修改：一种涉及先进的聚合函数，另一种利用注意力机制。我们通过实验证实了我们解决方案在三维环境导航任务中的有效性。

    Deep reinforcement learning in partially observable environments is a difficult task in itself, and can be further complicated by a sparse reward signal. Most tasks involving navigation in three-dimensional environments provide the agent with extremely limited information. Typically, the agent receives a visual observation input from the environment and is rewarded once at the end of the episode. A good reward function could substantially improve the convergence of reinforcement learning algorithms for such tasks. The classic approach to increase the density of the reward signal is to augment it with supplementary rewards. This technique is called the reward shaping. In this study, we propose two modifications of one of the recent reward shaping methods based on graph convolutional networks: the first involving advanced aggregation functions, and the second utilizing the attention mechanism. We empirically validate the effectiveness of our solutions for the task of navigation in a 3D e
    
[^215]: 基于星形细胞对关键期的神经可塑性神经网络，通过现有和记忆性的大脑可塑性和突触形成实现突触竞争和强度平衡。（arXiv: 2203.11740v12 [cs.NE] UPDATED）

    Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation. (arXiv:2203.11740v12 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2203.11740](http://arxiv.org/abs/2203.11740)

    该论文提出基于星形细胞作用的神经网络，通过突触的竞争和强度平衡实现现有和记忆性的大脑可塑性和突触形成，并探讨了与关键期相关的神经紊乱和负面和正面记忆的持久性对突触激活的影响。

    

    除了突触共享连接权重之外，PNN还包括突触有效范围的权重[14-25]。PNN考虑突触强度平衡在突触吞噬的动态和长度常数之和的静态中[14]，并包含了鱼群行为的先导行为。突触形成在实验和模拟中会抑制树突生成[15]。类似于Spring Boot中的强制韧性，反向回路的记忆持久度梯度也存在。相对较好和较差的梯度信息存储在类似于脑褶的记忆痕迹细胞中，在反向回路的突触形成中。争议认为人类海马神经元的再生能力是否持续到老年，并可能在后期迭代中形成新的更长的回路[17,18]。关闭关键期会导致神经紊乱在实验和模拟中[19]。考虑到负面和正面记忆的持久性，有助于更好地激活突触。

    In addition to the weights of synaptic shared connections, PNN includes weights of synaptic effective ranges [14-25]. PNN considers synaptic strength balance in dynamic of phagocytosing of synapses and static of constant sum of synapses length [14], and includes the lead behavior of the school of fish. Synapse formation will inhibit dendrites generation in experiments and simulations [15]. The memory persistence gradient of retrograde circuit similar to the Enforcing Resilience in a Spring Boot. The relatively good and inferior gradient information stored in memory engram cells in synapse formation of retrograde circuit like the folds in brain [16]. The controversy was claimed if human hippocampal neurogenesis persists throughout aging, may have a new and longer circuit in late iteration [17,18]. Closing the critical period will cause neurological disorder in experiments and simulations [19]. Considering both negative and positive memories persistence help activate synapse better than 
    
[^216]: 基于密集卷积神经网络的胸部疾病多标签分类方法

    Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs. (arXiv:2202.03583v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2202.03583](http://arxiv.org/abs/2202.03583)

    本研究提出了一种基于密集卷积神经网络和GRADCAM的胸部X光疾病多标签诊断模型，获得了在Cardiomegaly条件下最高的AUC得分0.896，并使用热图提高了模型的可解释性。

    

    传统的X光图像病理识别方法依赖于熟练的人类解释，并且往往耗时。深度学习技术的出现使自动诊断系统的开发成为可能，但这类系统的表现取决于模型的质量和它提供的可解释性水平。本文提出了一种使用密集卷积神经网络（DenseNet）和GRADCAM进行模型可解释性的胸部X光疾病多标签诊断模型。我们使用前置X光训练了我们的模型，并使用各种定量指标（包括受试者操作特征曲线下面积（AUC））评估了模型的性能。我们的模型在Cardiomegaly条件下达到了最高的AUC得分0.896，并获得了0.826的准确度。而在Nodule条件下获得了最低的AUC得分0.655，准确度为0.66。为了提高模型可解释性，并在决策方面建立信任，我们使用GRADCAM生成了热图，突出显示了对诊断最重要的X光区域。

    Traditional methods of identifying pathologies in X-ray images rely heavily on skilled human interpretation and are often time-consuming. The advent of deep learning techniques has enabled the development of automated disease diagnosis systems, but the performance of such systems is dependent on the quality of the model and the level of interpretability it provides. In this paper, we propose a multi-label disease diagnosis model for chest X-rays using a dense convolutional neural network (DenseNet) and model interpretability using GRADCAM. We trained our model using frontal X-rays and evaluated its performance using various quantitative metrics, including the area under the receiver operating characteristic curve (AUC). Our proposed model achieved the highest AUC score of 0.896 for the condition Cardiomegaly with an accuracy of 0.826, while the lowest AUC score was obtained for Nodule, at 0.655 with an accuracy of 0.66. To promote model interpretability and build trust in decision maki
    
[^217]: 神经语言模型中的反事实记忆

    Counterfactual Memorization in Neural Language Models. (arXiv:2112.12938v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2112.12938](http://arxiv.org/abs/2112.12938)

    本研究提出了一种反事实记忆的概念，用于描述神经语言模型在训练中忽略了哪些特定文档的预测变化。通过研究标准文本数据集中的反事实记忆训练样本，我们可以估计每个记忆样本对验证集和生成文本的影响，并直接提供记忆来源的证据。

    

    在各种自然语言处理任务中广泛使用的现代神经语言模型存在从训练数据中记忆敏感信息的风险。理解这种记忆对于实际应用和学习理论的角度都很重要。在先前的语言模型记忆研究中存在一个开放问题，即如何过滤掉“常见”记忆。事实上，大多数记忆标准与在训练集中出现的次数强烈相关，捕捉到常见短语、公共知识、模板化文本或其他重复数据。我们提出了一种反事实记忆的概念，描述了模型的预测在省略特定文档进行训练时如何改变。我们在标准文本数据集中确定并研究了反事实记忆的训练样本。我们估计了每个记忆训练样本对验证集和生成文本的影响，展示了这如何直接提供记忆来源的证据。

    Modern neural language models that are widely used in various NLP tasks risk memorizing sensitive information from their training data. Understanding this memorization is important in real world applications and also from a learning-theoretical perspective. An open question in previous studies of language model memorization is how to filter out "common" memorization. In fact, most memorization criteria strongly correlate with the number of occurrences in the training set, capturing memorized familiar phrases, public knowledge, templated texts, or other repeated data. We formulate a notion of counterfactual memorization which characterizes how a model's predictions change if a particular document is omitted during training. We identify and study counterfactually-memorized training examples in standard text datasets. We estimate the influence of each memorized training example on the validation set and on generated texts, showing how this can provide direct evidence of the source of memo
    
[^218]: 利用注意机制的高阶张量池化在动作识别中的应用

    High-order Tensor Pooling with Attention for Action Recognition. (arXiv:2110.05216v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2110.05216](http://arxiv.org/abs/2110.05216)

    本论文提出了一种利用注意机制进行高阶张量池化的方法，通过引入特征值幂归一化（EPN）来防止爆发现象并提高动作识别的准确性。

    

    本论文提出了一种利用注意机制进行高阶张量池化的方法，旨在捕捉神经网络生成的特征向量的高阶统计信息，形成张量描述符。张量描述符要求具备鲁棒的相似性度量，以应对聚合向量数量较少和爆发现象，即某些特征出现的频率高于或低于统计预期的情况。我们将图拉普拉斯矩阵上的热扩散过程与协方差/自相关矩阵的特征值幂归一化（EPN）密切相关，其逆形成了一个环状图拉普拉斯矩阵。我们证明了热扩散过程与EPN具有相同的作用，即增强或减弱特征值谱的幅度，从而防止爆发现象。我们将高阶张量配备了EPN，它可以作为高阶出现的谱检测器，以防止爆发现象。我们还证明，对于一个由d维特征描述符构建的阶数为r的张量，这样的检测器可以给出至少存在一个高阶出现的可能性。

    We aim at capturing high-order statistics of feature vectors formed by a neural network, and propose end-to-end second- and higher-order pooling to form a tensor descriptor. Tensor descriptors require a robust similarity measure due to low numbers of aggregated vectors and the burstiness phenomenon, when a given feature appears more/less frequently than statistically expected. The Heat Diffusion Process (HDP) on a graph Laplacian is closely related to the Eigenvalue Power Normalization (EPN) of the covariance/auto-correlation matrix, whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPN play the same role, i.e., to boost or dampen the magnitude of the eigenspectrum thus preventing the burstiness. We equip higher-order tensors with EPN which acts as a spectral detector of higher-order occurrences to prevent burstiness. We also prove that for a tensor of order r built from d dimensional feature descriptors, such a detector gives the likelihood if at least one high
    
[^219]: 对抗训练中的标签噪声：研究鲁棒过度拟合的新视角

    Label Noise in Adversarial Training: A Novel Perspective to Study Robust Overfitting. (arXiv:2110.03135v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03135](http://arxiv.org/abs/2110.03135)

    该论文发现了对抗训练中存在的标签噪声，并解释了其对鲁棒过度拟合的普遍存在以及扰动半径和数据质量的依赖性。通过该论文提出的方法，可以自动校准标签以应对标签噪声和鲁棒过度拟合。

    

    我们展示了在对抗训练中存在标签噪声。这种标签噪声是由于对抗样本的真实标签分布与从干净样本继承的标签之间的不匹配造成的 - 真实标签分布被对抗扰动扭曲，但从干净样本继承标签的常见做法却忽略了这一点。认识到标签噪声有助于洞察对抗训练中鲁棒过度拟合的普遍存在，并解释了其对扰动半径和数据质量的奇特依赖性。此外，我们的标签噪声视角与我们对对抗训练中纪元双下降现象的观察相吻合。在我们的分析指导下，我们提出了一种方法来自动校准标签以应对标签噪声和鲁棒过度拟合。我们的方法在各种模型和数据集上实现了一致的性能提升，而不引入新的超参数或额外的调整。

    We show that label noise exists in adversarial training. Such label noise is due to the mismatch between the true label distribution of adversarial examples and the label inherited from clean examples - the true label distribution is distorted by the adversarial perturbation, but is neglected by the common practice that inherits labels from clean examples. Recognizing label noise sheds insights on the prevalence of robust overfitting in adversarial training, and explains its intriguing dependence on perturbation radius and data quality. Also, our label noise perspective aligns well with our observations of the epoch-wise double descent in adversarial training. Guided by our analyses, we proposed a method to automatically calibrate the label to address the label noise and robust overfitting. Our method achieves consistent performance improvements across various models and datasets without introducing new hyper-parameters or additional tuning.
    
[^220]: 基于正则简单形体的流形感知深度聚类: 最大化嵌入向量之间的角度

    Manifold-Aware Deep Clustering: Maximizing Angles between Embedding Vectors Based on Regular Simplex. (arXiv:2106.02331v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2106.02331](http://arxiv.org/abs/2106.02331)

    本文提出了一种名为流形感知深度聚类（M-DC）的新方法，它通过基于正则简单形体的唯一损失函数，最大化超空间中目标角度，从而在提高聚类性能时更有效地增强了超空间利用率。

    

    本文提出了一种新的深度聚类（DC）方法，称为流形感知深度聚类（M-DC），它能够比原始的DC更有效地增强超空间利用率。原始的DC存在一个限制，即由于其使用基于One-hot向量的损失函数，两个说话者的嵌入必须具有正交关系，而我们的方法基于正则简单形体的特性，推导出一个旨在最大化超空间中目标角度的唯一损失函数。我们提出的损失函数在将说话者错误分配时比原始DC施加了更高的惩罚。从DC到M-DC的转变只需更改DC中损失函数的一个术语，而不需要对网络结构或模型参数进行任何其他修改。因此，我们的方法具有很高的实用性，因为它不会影响原始的推理部分。实验证明，所提出的方法改善了原始DC及其扩展方法的性能。

    This paper presents a new deep clustering (DC) method called manifold-aware DC (M-DC) that can enhance hyperspace utilization more effectively than the original DC. The original DC has a limitation in that a pair of two speakers has to be embedded having an orthogonal relationship due to its use of the one-hot vector-based loss function, while our method derives a unique loss function aimed at maximizing the target angle in the hyperspace based on the nature of a regular simplex. Our proposed loss imposes a higher penalty than the original DC when the speaker is assigned incorrectly. The change from DC to M-DC can be easily achieved by rewriting just one term in the loss function of DC, without any other modifications to the network architecture or model parameters. As such, our method has high practicability because it does not affect the original inference part. The experimental results show that the proposed method improves the performances of the original DC and its expansion metho
    
[^221]: 神经网络的可塑性研究

    A study on the plasticity of neural networks. (arXiv:2106.00042v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.00042](http://arxiv.org/abs/2106.00042)

    本研究探讨了神经网络的可塑性问题，发现在利用先前获得知识进行微调时，预训练模型可能无法达到相同的泛化能力，这对于持续学习具有重要影响。

    

    多个环境中的一个共同目标，如持续学习或迁移学习，是利用先前获得的知识来更快地在当前任务上收敛。通常，这通过微调来实现，其中一个隐含的假设是网络保持其可塑性，即它在任何给定任务上能够达到的性能不受先前任务的负面影响。最近观察到，在与重新初始化的模型相比，一个在与微调的数据具有相同分布的预训练模型可能无法达到相同的泛化能力。我们构建并扩展了这个观察结果，并提供了其背后机制的假设。我们讨论了对于严重依赖于优化预训练模型的持续学习而言，失去可塑性的影响。

    One aim shared by multiple settings, such as continual learning or transfer learning, is to leverage previously acquired knowledge to converge faster on the current task. Usually this is done through fine-tuning, where an implicit assumption is that the network maintains its plasticity, meaning that the performance it can reach on any given task is not affected negatively by previously seen tasks. It has been observed recently that a pretrained model on data from the same distribution as the one it is fine-tuned on might not reach the same generalisation as a freshly initialised one. We build and extend this observation, providing a hypothesis for the mechanics behind it. We discuss the implication of losing plasticity for continual learning which heavily relies on optimising pretrained models.
    
[^222]: 带有神经网络项的PDE约束模型：优化与全局收敛

    PDE-constrained Models with Neural Network Terms: Optimization and Global Convergence. (arXiv:2105.08633v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.08633](http://arxiv.org/abs/2105.08633)

    本研究严格研究了带有神经网络项的线性椭圆PDE模型的优化问题，并使用梯度下降和伴随PDE方法证明了神经网络-PDE优化过程的全局收敛。在流体力学中的实际应用中，我们使用了这个方法来训练神经网络模型作为一个闭环。

    

    最近的研究使用深度学习在科学和工程中发展了偏微分方程（PDE）模型。PDE的功能形式由神经网络确定，并且神经网络参数根据可用数据进行校准。嵌入PDE中的神经网络的校准可以通过在PDE上进行优化来实现。受这些应用的启发，我们对带有神经网络项的一类线性椭圆PDE的优化进行了严格研究。使用梯度下降优化PDE中的神经网络参数，其中梯度是使用伴随PDE计算的。随着参数数量的增加，PDE和伴随PDE收敛到一个非局部的PDE系统。使用此极限PDE系统，我们能够证明在优化过程中神经网络-PDE收敛到全局最小值。最后，我们使用此伴随方法在流体力学中训练一个神经网络模型，其中神经网络作为一个闭环运作。

    Recent research has used deep learning to develop partial differential equation (PDE) models in science and engineering. The functional form of the PDE is determined by a neural network, and the neural network parameters are calibrated to available data. Calibration of the embedded neural network can be performed by optimizing over the PDE. Motivated by these applications, we rigorously study the optimization of a class of linear elliptic PDEs with neural network terms. The neural network parameters in the PDE are optimized using gradient descent, where the gradient is evaluated using an adjoint PDE. As the number of parameters become large, the PDE and adjoint PDE converge to a non-local PDE system. Using this limit PDE system, we are able to prove convergence of the neural network-PDE to a global minimum during the optimization. Finally, we use this adjoint method to train a neural network model for an application in fluid mechanics, in which the neural network functions as a closure
    
[^223]: BEAUTY动力的BEAST

    BEAUTY Powered BEAST. (arXiv:2103.00674v5 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2103.00674](http://arxiv.org/abs/2103.00674)

    本文研究了使用BEAUTY方法进行分布无关的拟合优度检验。该方法通过二进制展开逼近特征函数，并将许多重要的独立性检验统一起来。使用数据自适应权重的BEAST检验提供了稳健的功效，同时提出了一个可行功效的参考。

    

    我们研究了使用提出的二进制展开近似均匀性（BEAUTY）方法的无分布拟合优度检验。该方法推广了著名的欧拉公式，并通过期望的二进制交互的线性组合来逼近任何联合分布函数的特征函数。这个新颖的理论通过特定的二次对称统计的逼近，将许多重要的独立性检验统一起来，其中确定性权重矩阵表征每个检验的功效性质。为了获得稳健的功效，我们使用数据自适应权重来检验检验统计量，称为二进制展开自适应对称性检验（BEAST）。利用二进制展开过程的性质，我们证明了均匀性的Neyman-Pearson检验可以通过oracle加权和的对称性统计量来近似。具有这个oracle的BEAST提供了可行功效的参考。

    We study distribution-free goodness-of-fit tests with the proposed Binary Expansion Approximation of UniformiTY (BEAUTY) approach. This method generalizes the renowned Euler's formula, and approximates the characteristic function of any copula through a linear combination of expectations of binary interactions from marginal binary expansions. This novel theory enables a unification of many important tests of independence via approximations from specific quadratic forms of symmetry statistics, where the deterministic weight matrix characterizes the power properties of each test. To achieve a robust power, we examine test statistics with data-adaptive weights, referred to as the Binary Expansion Adaptive Symmetry Test (BEAST). Using properties of the binary expansion filtration, we demonstrate that the Neyman-Pearson test of uniformity can be approximated by an oracle weighted sum of symmetry statistics. The BEAST with this oracle provides a useful benchmark of feasible power. To approac
    
[^224]: 图形平滑卷积网络用于异常检测

    Graph Fairing Convolutional Networks for Anomaly Detection. (arXiv:2010.10274v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.10274](http://arxiv.org/abs/2010.10274)

    本文引入了一种图形平滑卷积网络用于半监督异常检测，通过使用跳跃连接和图结构信息来学习有区分性的节点表示。

    

    图卷积是许多基于图结构数据的深度神经网络的基本构建块。在本文中，我们引入了一种简单而非常有效的带有跳跃连接的图卷积网络，用于半监督异常检测。我们模型的逐层传播规则在理论上受到几何处理中隐式平滑概念的启发，包括用于聚合来自相邻节点的信息的图卷积模块和用于组合逐层邻居表示的跳跃连接模块。这个传播规则是通过雅可比方法从隐式平滑方程的迭代解导出的。除了通过网络层之间的跳跃连接捕获来自远程图节点的信息外，我们的方法还利用图结构和节点特征来学习有区分性的节点表示。这些跳跃连接是根据我们提出的网络架构经过设计整合的。

    Graph convolution is a fundamental building block for many deep neural networks on graph-structured data. In this paper, we introduce a simple, yet very effective graph convolutional network with skip connections for semi-supervised anomaly detection. The proposed layerwise propagation rule of our model is theoretically motivated by the concept of implicit fairing in geometry processing, and comprises a graph convolution module for aggregating information from immediate node neighbors and a skip connection module for combining layer-wise neighborhood representations. This propagation rule is derived from the iterative solution of the implicit fairing equation via the Jacobi method. In addition to capturing information from distant graph nodes through skip connections between the network's layers, our approach exploits both the graph structure and node features for learning discriminative node representations. These skip connections are integrated by design in our proposed network archi
    
[^225]: 快速适应性非单调子模最大化问题在背包约束下的研究

    Fast Adaptive Non-Monotone Submodular Maximization Subject to a Knapsack Constraint. (arXiv:2007.05014v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2007.05014](http://arxiv.org/abs/2007.05014)

    本研究提出了一种针对非单调子模最大化问题的快速适应性算法，能够在背包约束下以O(log n)的复杂度接近最优解，并可以通过修改将函数值询问的数量减少到O(n)而保持较低的适应性复杂度。

    

    子模最大化是一个经典的算法问题，在数据挖掘和机器学习中有多种应用。针对大规模实例的需求，设计能够在解的质量和适用性之间取得平衡的算法成为动力。本文针对非单调子模最大化在背包约束下的问题，得到了第一个具有常数逼近比和接近最优的O(log n)适应性复杂度的算法。此外，我们的算法在求解过程中询问的函数值数量可以通过修改变为O(n)而保持适应性复杂度为O(log^2n)。

    Submodular maximization is a classic algorithmic problem with multiple applications in data mining and machine learning; there, the growing need to deal with massive instances motivates the design of algorithms balancing the quality of the solution with applicability. For the latter, an important measure is the \emph{adaptive complexity}, which captures the number of sequential rounds of parallel computation needed by an algorithm to terminate. In this work, we obtain the first \emph{constant factor} approximation algorithm for non-monotone submodular maximization subject to a knapsack constraint with \emph{near-optimal} $O(\log n)$ adaptive complexity. Low adaptivity by itself, however, is not enough: a crucial feature to account for is represented by the total number of function evaluations (or value queries). Our algorithm asks $\tilde{O}(n^2)$ value queries but can be modified to run with only $\tilde{O}(n)$ instead while retaining a low adaptive complexity of $O(\log^2n)$. Besides
    
[^226]: Ansor: 生成用于深度学习的高性能张量程序

    Ansor: Generating High-Performance Tensor Programs for Deep Learning. (arXiv:2006.06762v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.06762](http://arxiv.org/abs/2006.06762)

    Ansor是一个针对深度学习应用的张量程序生成框架，通过采样程序和使用进化搜索和学习的成本模型进行微调，能够高效地找到高性能的张量程序。

    

    高性能的张量程序对于保证深度神经网络的高效执行至关重要。然而，为不同的运算符在各种硬件平台上获得高性能张量程序是非常具有挑战性的。目前，深度学习系统依赖于供应商提供的内核库或各种搜索策略来获取高性能的张量程序。这些方法要么需要大量的工程工作来开发特定于平台的优化代码，要么由于受限的搜索空间和无效的探索策略而无法找到高性能的程序。我们提出了一种名为Ansor的张量程序生成框架，用于深度学习应用。与现有的搜索策略相比，Ansor通过从搜索空间的分层表示中采样程序来探索更多的优化组合。然后，Ansor使用进化搜索和学习的成本模型来对采样出的程序进行微调，以找到最佳程序。

    High-performance tensor programs are crucial to guarantee efficient execution of deep neural networks. However, obtaining performant tensor programs for different operators on various hardware platforms is notoriously challenging. Currently, deep learning systems rely on vendor-provided kernel libraries or various search strategies to get performant tensor programs. These approaches either require significant engineering effort to develop platform-specific optimization code or fall short of finding high-performance programs due to restricted search space and ineffective exploration strategy.  We present Ansor, a tensor program generation framework for deep learning applications. Compared with existing search strategies, Ansor explores many more optimization combinations by sampling programs from a hierarchical representation of the search space. Ansor then fine-tunes the sampled programs with evolutionary search and a learned cost model to identify the best programs. Ansor can find hig
    
[^227]: 条件随机梯度下降的渐近分析

    Asymptotic Analysis of Conditioned Stochastic Gradient Descent. (arXiv:2006.02745v5 [math.ST] UPDATED)

    [http://arxiv.org/abs/2006.02745](http://arxiv.org/abs/2006.02745)

    本文研究了一类称为条件随机梯度下降算法的通用类别，在进行梯度迭代时对梯度方向进行预条件。使用离散时间方法和鞅工具，在较弱的假设下证明了迭代序列的重新缩放收敛性，适用于包括随机一阶和二阶方法在内的广泛条件矩阵类别。同时，还介绍了具有独立兴趣的几乎肯定收敛的结果。

    

    本文研究了一类称为条件随机梯度下降（SGD）算法的一般类别，该算法基于梯度方向的预条件。使用离散时间方法和鞅工具，我们在较弱的假设下证明了迭代序列的重新缩放收敛性，适用于包括随机一阶和二阶方法在内的广泛条件矩阵类别。同时还介绍了具有独立兴趣的几乎肯定收敛的结果。有趣的是，渐近正态性结果包括一个随机等连续性性质，因此当条件矩阵是逆Hessian的估计时，该算法是渐近最优的。

    In this paper, we investigate a general class of stochastic gradient descent (SGD) algorithms, called Conditioned SGD, based on a preconditioning of the gradient direction. Using a discrete-time approach with martingale tools, we establish under mild assumptions the weak convergence of the rescaled sequence of iterates for a broad class of conditioning matrices including stochastic first-order and second-order methods. Almost sure convergence results, which may be of independent interest, are also presented. Interestingly, the asymptotic normality result consists in a stochastic equicontinuity property so when the conditioning matrix is an estimate of the inverse Hessian, the algorithm is asymptotically optimal.
    
[^228]: 时间卷积注意力网络用于序列建模

    Temporal Convolutional Attention-based Network For Sequence Modeling. (arXiv:2002.12530v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2002.12530](http://arxiv.org/abs/2002.12530)

    我们提出了一种叫做时间卷积注意力网络（TCAN）的架构，它结合了时间卷积网络和注意机制，既能替代循环网络，又能吸收前馈模型的优势。在实验中，我们改进了最新的困惑度结果到30.28（基于单词的PTB），1.092（基于字符的PTB）。

    

    随着前馈模型的发展，用于序列建模的默认模型逐渐演变为取代循环网络。许多基于卷积网络和注意机制的强大前馈模型被提出，并显示出更多处理序列建模任务的潜力。我们想知道是否有一种架构既能实现对循环网络的近似替代，又能吸收前馈模型的优势。因此，我们提出了一种探索性架构，称为时间卷积注意力网络（TCAN），它结合了时间卷积网络和注意机制。TCAN包括两个部分，一个是时间注意力（TA），用于捕捉序列内的相关特征，另一个是增强残差（ER），用于提取浅层的重要信息并传递给深层。我们将bpc/困惑度的最新结果改进到30.28（基于单词的PTB），1.092（基于字符的PTB）。

    With the development of feed-forward models, the default model for sequence modeling has gradually evolved to replace recurrent networks. Many powerful feed-forward models based on convolutional networks and attention mechanism were proposed and show more potential to handle sequence modeling tasks. We wonder that is there an architecture that can not only achieve an approximate substitution of recurrent network, but also absorb the advantages of feed-forward models. So we propose an exploratory architecture referred to Temporal Convolutional Attention-based Network (TCAN) which combines temporal convolutional network and attention mechanism. TCAN includes two parts, one is Temporal Attention (TA) which captures relevant features inside the sequence, the other is Enhanced Residual (ER) which extracts shallow layer's important information and transfers to deep layers. We improve the state-of-the-art results of bpc/perplexity to 30.28 on word-level PTB, 1.092 on character-level PTB, and 
    
[^229]: 主动逆向奖励设计

    Active Inverse Reward Design. (arXiv:1809.03060v3 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/1809.03060](http://arxiv.org/abs/1809.03060)

    这篇论文提出了一种主动逆向奖励设计的方法，通过询问用户比较不同的奖励函数来选择具有最大信息量的查询，从而达到了在训练环境中保证良好行为的目的。与其他方法相比，该方法可以通过引起子优行为的偏好来收集额外信息，同时在测试环境中取得了明显的优势。

    

    AI代理的设计者经常通过试错的过程来迭代奖励函数，直到获得期望的行为，但这只能在训练环境中保证良好的行为。我们提出将这个过程结构化为一系列查询，询问用户在不同的奖励函数之间进行比较。因此我们可以主动选择具有最大信息量的查询来了解真实奖励。与要求设计者提供最佳行为的方法相比，这种方法可以通过引起子优行为之间的偏好来收集额外的信息。在每次查询之后，我们需要通过观察设计者选择的代理奖励函数来更新对真实奖励函数的后验。最近提出的逆向奖励设计 (IRD) 可以实现这一点。我们的方法在测试环境中明显优于IRD。特别是，它可以询问设计者有关可解释、线性奖励函数，并且仍然可以推断出非线性奖励函数。

    Designers of AI agents often iterate on the reward function in a trial-and-error process until they get the desired behavior, but this only guarantees good behavior in the training environment. We propose structuring this process as a series of queries asking the user to compare between different reward functions. Thus we can actively select queries for maximum informativeness about the true reward. In contrast to approaches asking the designer for optimal behavior, this allows us to gather additional information by eliciting preferences between suboptimal behaviors. After each query, we need to update the posterior over the true reward function from observing the proxy reward function chosen by the designer. The recently proposed Inverse Reward Design (IRD) enables this. Our approach substantially outperforms IRD in test environments. In particular, it can query the designer about interpretable, linear reward functions and still infer non-linear ones.
    

