# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ZipIt! Merging Models from Different Tasks without Training.](http://arxiv.org/abs/2305.03053) | 本文介绍了一种无需额外训练即可合并不同任务上训练的模型的方法“ZipIt！”。 |
| [^2] | [Tracking through Containers and Occluders in the Wild.](http://arxiv.org/abs/2305.03052) | 本文介绍了一个新的基准模型 $\textbf{TCOW}$，用于在重度遮挡和容器中进行视觉跟踪。我们创建了一组混合的合成和真实数据集，评估了两种最新的基于变压器的视频模型，并发现它们在某些情况下能够出人意料地跟踪目标，但仍存在相当大的性能差距，必须进一步研究。 |
| [^3] | [Controllable Visual-Tactile Synthesis.](http://arxiv.org/abs/2305.03051) | 本文利用深度生成模型实现了视觉-触觉的交互合成，用户可以通过触感表面触摸和看到合成物体，包括创建一个新的触感数据集和开发一个条件生成模型。 |
| [^4] | [Personalize Segment Anything Model with One Shot.](http://arxiv.org/abs/2305.03048) | 本文提出了一种无需训练的SAM个性化方法PerSAM，只需要一张带有参考掩模的单张图像即可定位和分割目标概念，还提出了高效的一次性微调变体PerSAM-F，旨在解决掩模不确定性问题。 |
| [^5] | [Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision.](http://arxiv.org/abs/2305.03047) | 这篇论文提出了SELF-ALIGN方法，使用基于原则的推理和LLMs的生成能力以最少的人类监督实现AI代理的自我对齐。 |
| [^6] | [Are VAEs Bad at Reconstructing Molecular Graphs?.](http://arxiv.org/abs/2305.03041) | 本文研究了最先进的分子生成模型在大规模、化学多样化数据集上的重构性能，发现重构准确性惊人地低。然而，改善重构并不一定会带来更好的采样或优化性能。 |
| [^7] | [SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks.](http://arxiv.org/abs/2305.03039) | 通过分析159个交互式可视化工具及其用户反馈，本论文提出了在计算笔记本中设计可视分析工具的独特机会和考虑因素。 |
| [^8] | [Learning Hand-Held Object Reconstruction from In-The-Wild Videos.](http://arxiv.org/abs/2305.03036) | 本研究提出了一种从野外视频中自动提取三维监督来扩展手持物体重建模型的学习方法。通过使用手部姿势作为物体姿势的代理和学习数据驱动的三维形状先验知识等方法，有效地解决了未知相机姿势和遮挡等问题，从而通过从单个RGB图像预测物体三维形状的占据网络得到了优秀的结果。 |
| [^9] | [FastAMI -- a Monte Carlo Approach to the Adjustment for Chance in Clustering Comparison Metrics.](http://arxiv.org/abs/2305.03022) | FastAMI是一种用于大型数据集的聚类比较的快速方法，通过蒙特卡罗模拟来实现偶然性调整，相比于传统的基于排列的方法更准确。 |
| [^10] | [Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study.](http://arxiv.org/abs/2305.03017) | 本研究使用BERT和Query-Aware LSH提高非正式文档中代码示例推荐的质量，重点关注于Stack Overflow上的Java编程语言。研究使用BERT将代码示例转换为数值向量。 |
| [^11] | [When Do Neural Nets Outperform Boosted Trees on Tabular Data?.](http://arxiv.org/abs/2305.02997) | 这项研究通过对176个数据集的比较分析发现，在许多数据集中，GBDT和NN之间的性能差异可以忽略不计，或者GBDT的轻微超参数调整比选择最佳算法更重要。此外，研究人员对965个元特征进行了分析，发现GBDT在高维稀疏数据上表现更好。 |
| [^12] | [Adaptive Selection of Anchor Items for CUR-based k-NN search with Cross-Encoders.](http://arxiv.org/abs/2305.02996) | 本文提出了一种自适应锚点选择方法，可以在保持较小的计算成本的同时，实现与随机抽样锚点相当或者更好的k-NN召回性能。 |
| [^13] | [On the nonlinear correlation of ML performance between data subpopulations.](http://arxiv.org/abs/2305.02995) | 在不同数据子群体间，机器学习模型的内部准确性和外部准确性之间的相关性是非线性的，呈现出“月亮形”的相关性。 |
| [^14] | [SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data.](http://arxiv.org/abs/2305.02993) | 本论文介绍SemEval 2023的任务七，旨在进行临床试验数据的多证据自然语言推理，该任务难度较大，证据选择任务相对于蕴含任务表现更佳。 |
| [^15] | [ExeKGLib: Knowledge Graphs-Empowered Machine Learning Analytics.](http://arxiv.org/abs/2305.02966) | ExeKGLib是一个基于知识图谱的Python机器学习库，可帮助不具备深入ML知识的用户构建可执行的ML工作流，并提高透明度和可重用性。 |
| [^16] | [Weighted Tallying Bandits: Overcoming Intractability via Repeated Exposure Optimality.](http://arxiv.org/abs/2305.02955) | 该论文提出了一种受权重影响的计数赌博机(WTB)设置，通过Repeated Exposure Optimality(REO)来研究它。他们提出了一个算法来满足REO，并提供了最优的遗憾边界。 |
| [^17] | [Rethinking Population-assisted Off-policy Reinforcement Learning.](http://arxiv.org/abs/2305.02949) | 本文旨在重新审视基于种群协助的离策略强化学习，通过实验证明了将来自种群优化迭代的多样化数据添加到离策略更新中会降低性能，并提出了一种简单而有效的解决方案，即仅使用种群优化的最新数据作为离策略更新的校正项，从而显著改善了样本利用率和最终性能。 |
| [^18] | [Leveraging gradient-derived metrics for data selection and valuation in differentially private training.](http://arxiv.org/abs/2305.02942) | 研究如何在隐私增强技术下，利用梯度信息识别训练中感兴趣的数据样本进行数据选择和估价。 |
| [^19] | [Piecewise Normalizing Flows.](http://arxiv.org/abs/2305.02930) | 介绍了一种分段归一化流方法，将目标分布分成集群，并通过训练模拟复杂的多模态目标。这种方法可以更好地匹配标准正态基础分布的拓扑结构。 |
| [^20] | [Recent Advances in the Foundations and Applications of Unbiased Learning to Rank.](http://arxiv.org/abs/2305.02914) | 本文介绍了无偏学习排序（ULTR）的基础概念和最新进展，以及几种实际应用的方法。教程分为四个部分：偏差的概述，ULTR的最新估计技术，ULTR在实际应用中的表现，以及ULTR与排名公平性的联系。 |
| [^21] | [FedCBO: Reaching Group Consensus in Clustered Federated Learning through Consensus-based Optimization.](http://arxiv.org/abs/2305.02894) | 本文基于一致性优化（CBO）的思想，提出了一种新的解决方案，通过互动粒子系统实现对于聚类联邦学习中各个群组的有效模型训练. |
| [^22] | [Input Layer Binarization with Bit-Plane Encoding.](http://arxiv.org/abs/2305.02885) | 该论文提出了一种使用比特平面编码实现的输入层二值化的方法，避免了传统方法中由于数据扩展导致的计算量增加问题，从而实现了完全二值化的模型，并在多个数据集上进行了评估。 |
| [^23] | [Simple Noisy Environment Augmentation for Reinforcement Learning.](http://arxiv.org/abs/2305.02882) | 本论文探讨了一系列通用封装，用于噪声增强RL环境，提供了两种新的增强技术，有助于代理人探索和改善训练数据多样性。同时，介绍了控制噪声注入频率的超参数噪声率。在实验中使用了三种流行的RL算法，Soft Actor-Critic（SAC），Twin Delayed DDPG（TD3）和Proximal Policy，以研究这些封装对回报的影响。 |
| [^24] | [Trainability barriers and opportunities in quantum generative modeling.](http://arxiv.org/abs/2305.02881) | 本文研究了量子生成模型的可训练性障碍，如荒芜高原和指数损失集中，使用隐式生成模型和明确损失会产生一种新的荒芜高原现象。最大均值差可以是低秩且可训练的或全局性且不可训练的。但是，可训练性所需的低秩损失通常不能区分高频和低频特征。 |
| [^25] | [Hierarchical Transformer for Scalable Graph Learning.](http://arxiv.org/abs/2305.02866) | 本文提出了分层可扩展图Transformer (HSGT)用于解决图表示学习中的规模问题和上下文信息捕获不足问题，通过构建多尺度图分层结构，HSGT实现了对大型图的快速和内存高效处理，并在基准数据集上展现了卓越的性能表现。 |
| [^26] | [Maximum Causal Entropy Inverse Constrained Reinforcement Learning.](http://arxiv.org/abs/2305.02857) | 该论文提出了一种新颖的最大因果熵逆约束强化学习方法，通过利用最大因果熵原理来学习代理遵守限制的约束条件和最优策略，使用遵守这些限制的代理的实例进行学习。此方法已被证明在各种任务和环境中优于最先进的方法。 |
| [^27] | [Impossibility of Depth Reduction in Explainable Clustering.](http://arxiv.org/abs/2305.02850) | 可解释聚类中，决策树深度是无法减少的固有复杂度度量之一，减少深度会显著降低聚类质量。 |
| [^28] | [Interpretable Sentence Representation with Variational Autoencoders and Attention.](http://arxiv.org/abs/2305.02810) | 本论文提出了使用VAEs和Transformers构建两种具有归纳偏差的模型，用于提高自然语言处理中表示学习技术的解释性和数据效率，能够将潜在表示中的信息分离为可理解的概念。实验结果表明这些模型提供了直观且可解释的表示形式，具有实用性。 |
| [^29] | [Maximizing Submodular Functions for Recommendation in the Presence of Biases.](http://arxiv.org/abs/2305.02806) | 该论文研究了如何在存在偏见的情况下，通过最大化子模函数来优化推荐系统。先前研究指出，基于公平性约束的干预可以确保比例代表性，并在存在偏见时获得接近最优的效用。而本文则探讨了一组能够捕捉这种目的的子模函数。 |
| [^30] | [Tensor PCA from basis in tensor space.](http://arxiv.org/abs/2305.02803) | 本文提出了一种张量PCA的数学框架，通过自伴张量算子导出张量空间中的基础以解决以往方法的局限性，实验结果表明了该方法的有效性。 |
| [^31] | [Class-Distribution-Aware Pseudo Labeling for Semi-Supervised Multi-Label Learning.](http://arxiv.org/abs/2305.02795) | 本论文提出了一种面向半监督多标签学习的类别分布感知伪标记方法，能够在控制伪标签数目的情况下，更准确地逼近真实分布，从而实现更好的多标签分类性能。 |
| [^32] | [BranchNorm: Robustly Scaling Extremely Deep Transformers.](http://arxiv.org/abs/2305.02790) | BranchNorm提出了一种新的方法，通过动态重新调整Transformer的非残差分支，理论上稳定了训练，并在随后的训练阶段中促进了更好的收敛。实验结果表明，BranchNorm在训练稳定性和收敛性能之间取得了更好的平衡。 |
| [^33] | [A Momentum-Incorporated Non-Negative Latent Factorization of Tensors Model for Dynamic Network Representation.](http://arxiv.org/abs/2305.02782) | 本文提出一种基于动量SGD的非线性LFT模型（MNNL）用于从高维不完整张量中提取非负潜在因子，以提高大规模动态网络的表征准确性和收敛速度。 |
| [^34] | [Interpretable Regional Descriptors: Hyperbox-Based Local Explanations.](http://arxiv.org/abs/2305.02780) | 本文介绍了一种可解释的区域描述符，它是一种模型无关的局部解释方法，通过描述超立方体来预测特征值可更改但不影响预测结果，并提供"即使是"参数，揭示决策的特征和偏差。 |
| [^35] | [Efficient Personalized Federated Learning via Sparse Model-Adaptation.](http://arxiv.org/abs/2305.02776) | 提出了一种名为pFedGate的新方法，通过自适应和高效的方式学习稀疏的本地模型，使得客户端可以发挥其模型容量的全部潜力，从而提高个性化联邦学习的效率。 |
| [^36] | [VendorLink: An NLP approach for Identifying & Linking Vendor Migrants & Potential Aliases on Darknet Markets.](http://arxiv.org/abs/2305.02763) | 本论文提出了一种名为VendorLink的NLP方法，能够有效地识别和链接暗网市场上的供应商被迁移和潜在别名，减少非法市场的匿名性，具有重要的实际意义。 |
| [^37] | [Multi-Domain Learning From Insufficient Annotations.](http://arxiv.org/abs/2305.02757) | 提出了一种名为多领域对比学习（MDCL）的新方法，在原有方法的基础上，利用来自标记和未标记数据的语义和结构信息，解决了不充分注释的问题，并在实验中取得了优异的成果。 |
| [^38] | [Explainable Reinforcement Learning via a Causal World Model.](http://arxiv.org/abs/2305.02749) | 本文提出了一种新的可解释强化学习框架，通过学习因果世界模型来解释行动的长期影响以及教学习者如何影响环境变量并最终导致奖励。 |
| [^39] | [Can Fair Federated Learning reduce the need for Personalisation?.](http://arxiv.org/abs/2305.02728) | 本文探索了联邦学习的公正性与个性化需求的关系，提出个性化公正联邦学习（PFL）算法，能够解决准确性差异以及在FL模型表现不佳时提供参与激励的问题。 |
| [^40] | [Using interpretable boosting algorithms for modeling environmental and agricultural data.](http://arxiv.org/abs/2305.02699) | 本文介绍了如何使用可解释的提升算法来分析高维环境和农业数据。通过考虑组结构和使用两步提升方法，我们预测了农民在面对气候灾害时的财务脆弱性。重要的预测变量包括自然资产、灌溉类型和附近农场的作物损坏。交互作用也提高了预测能力。 |
| [^41] | [In-situ Anomaly Detection in Additive Manufacturing with Graph Neural Networks.](http://arxiv.org/abs/2305.02695) | 本研究利用图神经网络训练模型进行金属增材制造现场异常检测，通过预测与新观察值之间差异计算异常得分，F1得分为0.821，为开发稳健的缺陷检测方法提供了重要工具。 |
| [^42] | [PGB: A PubMed Graph Benchmark for Heterogeneous Network Representation Learning.](http://arxiv.org/abs/2305.02691) | 介绍了一个基于PubMed数据库的PGB基准数据集，用于评估生物医学文献的异构图嵌入。该数据集包含丰富的元数据和来自不同数据集的21个系统性评价主题的评估任务。 |
| [^43] | [Statistical Optimality of Deep Wide Neural Networks.](http://arxiv.org/abs/2305.02657) | 本文研究了深度宽松弛ReLU神经网络的泛化能力，证明适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中，但过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。 |
| [^44] | [Variations on a Theme by Blahut and Arimoto.](http://arxiv.org/abs/2305.02650) | 本文提出了BA算法的一种新的修改，通过让乘数在每次迭代中通过一维求根来更新，这使得算法能够直接计算所需失真的RD函数，而无需像原始算法一样探索整个RD曲线。 |
| [^45] | [Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders.](http://arxiv.org/abs/2305.02640) | 本文提出了因果强度变分模型，解决了从不确定数据中恢复因果关系存在的低样本利用率和分布假设无能力的问题，同时考虑了潜在混淆因素。 |
| [^46] | [Conformal Nucleus Sampling.](http://arxiv.org/abs/2305.02633) | 本研究探讨了符合语言模型的核心抽样，并采用符合性预测进行校准。结果表明，OPT模型过于自信，并且校准显示出中度的逆比例缩放与模型大小。 |
| [^47] | [A framework for the emergence and analysis of language in social learning agents.](http://arxiv.org/abs/2305.02632) | 本研究提出了一个模拟语言特征的通信协议，用于分析个体和共享抽象的形成及其对任务表现的影响。通过优化信息内容以最大化学生奖励改善了信息编码，提高了学习表现。 |
| [^48] | [Integrating Psychometrics and Computing Perspectives on Bias and Fairness in Affective Computing: A Case Study of Automated Video Interviews.](http://arxiv.org/abs/2305.02629) | 本研究基于心理测量学提供了情感计算中偏见和公平性的一个例子。我们讨论了在美国法律背景下衡量公平性和偏见的各种方法和指标，并在自动视频面试中从多模态数据中测量了一些类型的偏见和公平性。我们鼓励情感计算研究人员和从业者将偏见和公平性纳入研究过程和产品中，并考虑他们在促进公平合理系统方面的作用、代理和责任。 |
| [^49] | [Critical heat flux diagnosis using conditional generative adversarial networks.](http://arxiv.org/abs/2305.02622) | 本研究基于cGANs提出了一种用于在临界热流时重构沸腾系统热数据的图像到图像翻译方法，有望成为可靠的非侵入式临界热流诊断方法。 |
| [^50] | [High-dimensional Bayesian Optimization via Semi-supervised Learning with Optimized Unlabeled Data Sampling.](http://arxiv.org/abs/2305.02614) | 本文提出基于半监督学习的高维贝叶斯优化方法，利用特定的未标记数据采样、参数化采样分布的优化及动态选择无标记数据等策略，解决了高维贝叶斯优化难以处理的问题。 |
| [^51] | [IMAP: Intrinsically Motivated Adversarial Policy.](http://arxiv.org/abs/2305.02605) | IMAP是一个内在驱动的对抗策略，无需受害者策略的任何知识，能够高效地进行黑盒规避攻击，并且在单一和多智能体环境中优于目前最先进的方法。 |
| [^52] | [On the Expressivity Role of LayerNorm in Transformers' Attention.](http://arxiv.org/abs/2305.02582) | 本文揭示了LayerNorm在Transformers的Attention层中有着至关重要的作用，通过对输入向量进行投影并对所有向量进行缩放，LayerNorm可以帮助注意力机制更好地处理输入。 |
| [^53] | [Joint Graph Learning and Model Fitting in Laplacian Regularized Stratified Models.](http://arxiv.org/abs/2305.02573) | 本文提出了一种联合图学习和模型拟合的通用方法，适用于广泛的LRSM问题，并在合成和真实世界数据集上实现了最先进的性能。 |
| [^54] | [Conditional and Residual Methods in Scalable Coding for Humans and Machines.](http://arxiv.org/abs/2305.02562) | 该论文提出了适用于人类和机器的可扩展编码中的条件编码和残差编码方法，并应用于图像重建，取得了类似的性能表现。 |
| [^55] | [Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era.](http://arxiv.org/abs/2305.02555) | ChatGPT和Bard等AI工具需要持续大量且高质量的数据来提高其性能，但现行的版权法则限制了它们对各种数据的获取。与数据提供者分享收益将有助于将AI工具与大多数版权数据拥有者之间的敌对关系转变为合作关系，使AI生态系统更健康。 |
| [^56] | [FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction.](http://arxiv.org/abs/2305.02549) | 该论文提出了一种用于表格文档信息提取的多模态图形对比学习策略（FormNetV2），该方法能够统一所有模态的自监督预训练到一个损失中，并在多个基准测试中取得了最佳表现。 |
| [^57] | [Nearly-Linear Time and Streaming Algorithms for Outlier-Robust PCA.](http://arxiv.org/abs/2305.02544) | 该研究开发出了一种近似线性时间的鲁棒PCA算法，具有接近最优的误差保证，并且还开发了一种单遍流式PCA算法，具有几乎线性的内存使用。 |
| [^58] | [Correcting for Interference in Experiments: A Case Study at Douyin.](http://arxiv.org/abs/2305.02542) | 该研究提出了一种新型的Monte-Carlo估计器，它能够修正在抖音双向内容市场平台上实验时的干扰问题，并在现场实验中将实验吞吐量提高了一倍。 |
| [^59] | [Cuttlefish: Low-rank Model Training without All The Tuning.](http://arxiv.org/abs/2305.02538) | Cuttlefish 是一种新的自动化低秩训练方法，可以有效地减少可训练参数的数量，而无需调整因式分解超参数即可实现加速，可生成比完全秩训练小多达 5.6 倍的模型。 |
| [^60] | [Reinforcement Learning with Delayed, Composite, and Partially Anonymous Reward.](http://arxiv.org/abs/2305.02527) | 本文提出了一种算法用于解决具有延迟、复合和部分匿名奖励反馈的无限时平均奖励马尔可夫决策过程(MDP)，并取得了较好的效果。 |
| [^61] | [BitGNN: Unleashing the Performance Potential of Binary Graph Neural Networks on GPUs.](http://arxiv.org/abs/2305.02522) | 本文提出了一种从效率角度重新设计的二进制GNN推理后端算法，用于充分发挥GPU上位操作的特性，实验结果表明提出的算法比最先进的二进制GNN实现提高了8-22倍的性能，保持相同准确性。 |
| [^62] | [Meta-Learning Enabled Score-Based Generative Model for 1.5T-Like Image Reconstruction from 0.5T MRI.](http://arxiv.org/abs/2305.02509) | 该论文提出了一种新颖的元学习方法，通过教师-学生机制实现从0.5T MRI重建1.5T-like图像。该方法能够解决从低场强到高场强MRI图像的映射问题。 |
| [^63] | [Stimulative Training++: Go Beyond The Performance Limits of Residual Networks.](http://arxiv.org/abs/2305.02507) | 本文从一种新的社会心理学角度重新审视残差网络的训练过程，发现了网络贡献不足问题并提出解决方案和改进策略，以提高残差网络的性能。 |
| [^64] | [String Diagrams with Factorized Densities.](http://arxiv.org/abs/2305.02506) | 本文描述了一个定义在随机变量集上的联合密度的范畴及其意义，以帮助概率编程和因果推断中的组合推理。 |
| [^65] | [Learning Missing Modal Electronic Health Records with Unified Multi-modal Data Embedding and Modality-Aware Attention.](http://arxiv.org/abs/2305.02504) | 该论文提出了一种能够统一处理多模态电子病历的学习方法，并通过引入跳过瓶颈和模态感知注意力解决了缺失模态的情况，取得了在死亡率、血管加压素需要和插管需要预测等方面的表现优于其他基线模型。 |
| [^66] | [AutoML-GPT: Automatic Machine Learning with GPT.](http://arxiv.org/abs/2305.02499) | AutoML-GPT 是一种基于 GPT 的自动机器学习方法，利用大型语言模型动态地利用各种人工智能模型，自动化训练管道，节约了选择模型架构、优化算法和调整超参数的人力和时间成本。 |
| [^67] | [Revisiting Graph Contrastive Learning for Anomaly Detection.](http://arxiv.org/abs/2305.02496) | 本文重新审视了用于异常检测的图对比学习方法。通过深入探讨现有方法的基本机制，提出了统一的多GNN和增强图对比框架MAG，并从中提取轻量级实例L-MAG和M-MAG。 |
| [^68] | [How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 1: A Paradigmatic Theory.](http://arxiv.org/abs/2305.02485) | 本文提出基于强化学习的方法，设计联合市场以应对电力行业脱碳，实现电力系统的安全和经济效益，并为环境做出贡献。该范型理论的框架将在两部分中详细介绍。 |
| [^69] | [Breast Cancer Diagnosis Using Machine Learning Techniques.](http://arxiv.org/abs/2305.02482) | 本文回顾和讨论了来自不同源的最新机器学习技术在乳腺癌诊断中的应用，其中包括热成像、红外热成像、电阻抗层析成像以及血液检测中发现的生物标志物，这些技术比传统方法更快、更可靠和更便宜，并且机器学习技术能够提高诊断的准确性。 |
| [^70] | [MLHOps: Machine Learning for Healthcare Operations.](http://arxiv.org/abs/2305.02474) | 本文综述了机器学习在医疗保健环境中应用的一般性流程，并为开发人员和临床医生提供了指南，以便他们能够部署和维护自己的模型；重点关注包括数据源、准备、工程、工具、长期监控和更新、以及道德考虑等方面，这为MLHOps全流程提供了指导。 |
| [^71] | [Semisupervised regression in latent structure networks on unknown manifolds.](http://arxiv.org/abs/2305.02473) | 本文提出了一种基于半监督回归的潜在结构网络模型，在未知机流形上使用流形学习和图嵌入技术进行响应预测，并为这些响应建立了收敛保证。 |
| [^72] | [Multiplicity Boost Of Transit Signal Classifiers: Validation of 69 New Exoplanets Using The Multiplicity Boost of ExoMiner.](http://arxiv.org/abs/2305.02470) | 该论文介绍了一种可提高探测行星信号分类器性能的框架，称为多重性增强分类器，基于现有的分类器并使用多重性信息来验证69个新的系外行星。 |
| [^73] | [The System Model and the User Model: Exploring AI Dashboard Design.](http://arxiv.org/abs/2305.02469) | 论文探讨了基于神经网络的AI系统应该有面板以提高其可用性和安全性，并且界面应该具有基于系统模型和用户模型状态的并行显示。 |
| [^74] | [Shap-E: Generating Conditional 3D Implicit Functions.](http://arxiv.org/abs/2305.02463) | Shap-E能够生成有条件的3D隐式函数，可以呈现为纹理网格和神经辐射场，收敛更快且生成的3D模型质量相当或更好。 |
| [^75] | [Tensorizing flows: a tool for variational inference.](http://arxiv.org/abs/2305.02460) | 张量流是一种扩展标准化流的工具，可以通过结合流与张量网络来改善在学习多峰分布的困难变分推断任务中的结果。 |
| [^76] | [Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge.](http://arxiv.org/abs/2305.02459) | 本文提出并探究了基于转移和主动学习的稀有类问题的解决方案，包括利用在密切相关任务上训练的模型和评估获取策略来解决共振检测的稀有类问题，并且发现了一种名为PRC的有效的策略来指导注释。 |
| [^77] | [Streaming PCA for Markovian Data.](http://arxiv.org/abs/2305.02456) | 本文提出了一种面向马尔可夫数据采样的流式PCA算法，并获得了该算法在整个数据集上的第一个尖锐率，提高了算法的效率。同时，本文提出的自适应方案在模拟和真实数据示例中表现良好。 |
| [^78] | [Bayesian Safety Validation for Black-Box Systems.](http://arxiv.org/abs/2305.02449) | 本文提出了一种名为贝叶斯安全验证的算法，将黑盒安全验证问题转化为贝叶斯优化问题。该算法通过概率代理模型拟合快速预测故障，利用重要性采样估计操作域内的故障概率，从而实现了对高维、危险、计算昂贵的系统的高效估计。 |
| [^79] | [Reward Teaching for Federated Multi-armed Bandits.](http://arxiv.org/abs/2305.02441) | 本论文提出了一种基于奖励教学思想的联邦多臂老虎机设计，通过隐式本地奖励调整来指导客户端朝着全局最优性，队服务端提出了老虎机学习和目标教学任务进行了优化。 |
| [^80] | [Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs.](http://arxiv.org/abs/2305.02440) | 本文提出了一个新的度量方法，用于比较Transformer API上模型推理效率的相关成本，以解决现有度量方法在软件和硬件优化和共享基础设施方面的缺陷。 |
| [^81] | [GAMIVAL: Video Quality Prediction on Mobile Cloud Gaming Content.](http://arxiv.org/abs/2305.02422) | GAMIVAL是一种新型的游戏专用无参考视频质量评估模型，结合了多种优点。在移动云游戏内容的主观质量评估数据库上进行测试，表现出更好的NR VQA性能。 |
| [^82] | [Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents.](http://arxiv.org/abs/2305.02412) | 本文介绍了Plan，Eliminate，和Track（PET）框架，该框架利用预先训练的大型语言模型（LLM）帮助智能体简化控制任务，从而解决了LLM直接作为智能体所面临的一些限制和问题。 |
| [^83] | [Normalizing flows for lattice gauge theory in arbitrary space-time dimension.](http://arxiv.org/abs/2305.02402) | 本文提出了新的正则流算法来推广到更高维度的规范场理论中，成功地应用于SU(3)规范场理论的原理证明实验，并显示出可行性。 |
| [^84] | [Synthetic DOmain-Targeted Augmentation (S-DOTA) Improves Model Generalization in Digital Pathology.](http://arxiv.org/abs/2305.02401) | 本研究比较了四种提高模型泛化性能的增强方法，包括两种基于S-DOTA的合成增强方法、一个基于ICC profile的颜色校准方法和一个传统的基线方法。实验结果表明使用S-DOTA增强方法能够显著提高数字病理图像模型的泛化性能。 |
| [^85] | [Widespread Increases in Future Wildfire Risk to Global Forest Carbon Offset Projects Revealed by Explainable AI.](http://arxiv.org/abs/2305.02397) | 研究利用可解释人工智能模型对全球森林野火风险进行了预测，发现2050年到2080年期间，全球森林碳抵消项目将面临更大的火灾风险，其中火灾的暴露程度预计将增加55%[37-76%]。 |
| [^86] | [Can Feature Engineering Help Quantum Machine Learning for Malware Detection?.](http://arxiv.org/abs/2305.02396) | 本文通过量子机器学习与特征选择策略相结合的混合框架，以降低恶意软件分类器培训时间，初步结果表明在模拟器上可以达到78.91％的测试准确性。 |
| [^87] | [Defending against Insertion-based Textual Backdoor Attacks via Attribution.](http://arxiv.org/abs/2305.02394) | 本文提出了一种基于归因的管道AttDef，用于防御两种插入式污染攻击BadNL和InSent，该管道可以成功缓解插入式文本后门攻击并在四个基准数据集上平均提高了56.59%至79.97%和15.25%至48.34%的准确率。 |
| [^88] | [Approximating CKY with Transformers.](http://arxiv.org/abs/2305.02386) | 本文研究了Transformer模型逼近CKY算法的能力，提出了一种用梯度预测解析的方法，在标准基准测试中表现竞争力更好，同时速度更快。在随机PCFG下解析时，性能下降，但加入额外的归纳偏差是有帮助的。 |
| [^89] | [Learning to Detect Novel and Fine-Grained Acoustic Sequences Using Pretrained Audio Representations.](http://arxiv.org/abs/2305.02382) | 本研究探讨了预训练音频表示在少样本声事件检测中的应用，提出了针对新颖声学序列的少样本检测任务，并通过实验验证了预训练表示的通用效用，证明其适用于该任务并启用了相应的少样本框架。 |
| [^90] | [MaskSearch: Querying Image Masks at Scale.](http://arxiv.org/abs/2305.02375) | MaskSearch是一个系统，通过使用新颖的索引技术和高效的过滤器验证查询执行框架，加速对图像掩模数据库的查询，可以将个体查询加速高达两个数量级，优于现有方法。 |
| [^91] | [Metric Tools for Sensitivity Analysis with Applications to Neural Networks.](http://arxiv.org/abs/2305.02368) | 本文提出了一种度量框架和新的聚合指标，用于敏感性分析，可以适用于任何可微分模型。其中新的聚合指标基于三种方法获取有价值的信息，并在神经网络模型上进行了验证。 |
| [^92] | [Using Language Models on Low-end Hardware.](http://arxiv.org/abs/2305.02350) | 本论文评估了在低端硬件上使用固定语言模型来训练文本分类网络的可行性，并发现在某些情况下，不对语言模型进行微调可以在更快的训练中产生竞争性的效果，仅需要原先内存的四分之一即可。 |
| [^93] | [Structures of Neural Network Effective Theories.](http://arxiv.org/abs/2305.02334) | 该论文提出了一种简化深度神经网络有效场论计算的图解方法，并指出单一条件决定了所有神经元预激活的关联函数的临界性，这可能有助于推动深度学习和场论模拟的进展。 |
| [^94] | [Correlation-Driven Multi-Level Multimodal Learning for Anomaly Detection on Multiple Energy Sources.](http://arxiv.org/abs/2305.02323) | 本文提出了一种基于相关性的多层多模态学习（CMDML）方法，以实现多能源异常检测。CMDML 能够综合不同能源源之间的相关性属性和复杂性，并在实验中得到了表现优于最先进方法的结果。 |
| [^95] | [Representation Learning via Manifold Flattening and Reconstruction.](http://arxiv.org/abs/2305.01777) | 本文提出了一种通过神经网络构建展平流形的算法FlatNet，具有可解释性和可扩展性，同时在测试数据上具有良好的泛化性能。 |
| [^96] | [BrainNPT: Pre-training of Transformer networks for brain network classification.](http://arxiv.org/abs/2305.01666) | 本文提出了一种名为BrainNPT的基于Transformer的神经网络，用于脑功能网络分类，并提出了两种预训练策略，利用未标记的脑网络数据来学习结构。 |
| [^97] | [Generalizing Dataset Distillation via Deep Generative Prior.](http://arxiv.org/abs/2305.01649) | 该方法提出一种基于预训练深度生成模型的学习先验的数据集蒸馏方法，通过在生成模型的潜在空间中将大量图像蒸馏为少量中间特征向量，显著提高了在所有设置中的跨体系结构的泛化能力。 |
| [^98] | [An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework.](http://arxiv.org/abs/2305.01322) | 该论文关注强化学习中的探索研究，提出了一个能够自主管理探索策略的多模式智能体非单体探索方法，并通过实验结果展示了该方法的优越性能。 |
| [^99] | [Expertise Trees Resolve Knowledge Limitations in Collective Decision-Making.](http://arxiv.org/abs/2305.01063) | 本研究通过引入专业知识树算法，解决了集体决策中专业知识水平不同的问题，并在多个问题上进行了验证。 |
| [^100] | [Gradient-based Maximally Interfered Retrieval for Domain Incremental 3D Object Detection.](http://arxiv.org/abs/2304.14460) | 对于域增量3D物体检测，GMIR提出了一种基于梯度的最大干扰恢复策略，可在微调时定期从以前的领域数据集中检索样本。 |
| [^101] | [DataComp: In search of the next generation of multimodal datasets.](http://arxiv.org/abs/2304.14108) | DataComp是一个基准测试，旨在通过提出新的训练集来解决数据集在机器学习生态系统中的缺陷。它提供了一个多规模设计的实验测试平台，使用12.8B个图像-文本对的新候选池，让研究人员可以通过设计新的过滤技术或策划新的数据源并评估它们的新数据集来进行创新。 |
| [^102] | [Learning Trajectories are Generalization Indicators.](http://arxiv.org/abs/2304.12579) | 本文研究了DNN的学习轨迹与其在优化后的泛化能力的联系，提出了一种基于学习轨迹复杂性和训练集偏置和多样性比率的新的泛化上界，并通过实验验证了其有效性。 |
| [^103] | [To Compress or Not to Compress -- Self-Supervised Learning and Information Theory: A Review.](http://arxiv.org/abs/2304.09355) | 本文从信息论的角度回顾了各种自监督学习方法，并提出了一个正式的“自监督信息理论学习问题”统一框架。此外，讨论了压缩性和压缩算法在自监督学习中的作用，并凸显了潜在的未来方向。 |
| [^104] | [Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method.](http://arxiv.org/abs/2304.07056) | 本文介绍了大规模压缩面部视频质量评估（CFVQA）数据库，用于系统地了解面部视频感知质量和多样化压缩失真。生成式编码方法被确定为具有合理的感知码率失真折衷的有前途的替代方法，利用面部视频的统计先验知识。 |
| [^105] | [Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding.](http://arxiv.org/abs/2304.04099) | 本研究提出了一种新颖的主题嵌入方法和一个可扩展的无监督在线故事发现框架USTORY，可以动态表示文章和故事，并考虑它们共享的时间主题和新颖性，以帮助人们消化大量的新闻流。 |
| [^106] | [A Unified Characterization of Private Learnability via Graph Theory.](http://arxiv.org/abs/2304.03996) | 本文提供了一个统一的框架，使用图论的语言刻画了差分隐私的两种情形下，纯粹和近似的学习性。我们通过定义矛盾图$G$来捕捉 $\mathcal{H}$ 的组合结构，发现分数团数和团数是描述差分隐私学习性的重要因素，并提出了几种算法对其进行估计。 |
| [^107] | [Generative AI for learning: Investigating the potential of synthetic learning videos.](http://arxiv.org/abs/2304.03784) | 本研究探讨了使用生成AI合成视频来创建在线教育内容的效用，使用混合方法和成年学习者进行实验。结果显示，合成学习视频对学习内容获取和学习体验有着积极的影响。 |
| [^108] | [DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion.](http://arxiv.org/abs/2303.12743) | 该论文提出了一种多样化和逼真的增强方法，可以创建整体对象并灵活地定位和旋转对象，并相应地应用自遮挡和外遮挡。通过迭代构建多个对象来提高整体对象构造的多样性，构造的对象可以在训练帧中随机放置和旋转。 |
| [^109] | [DR-VIDAL -- Doubly Robust Variational Information-theoretic Deep Adversarial Learning for Counterfactual Prediction and Treatment Effect Estimation on Real World Data.](http://arxiv.org/abs/2303.04201) | DR-VIDAL是一个新型的生成框架，可用于处理真实世界数据中的干预措施对结果的因果效应估计，并具有处理混淆偏差和模型不良的能力。 |
| [^110] | [Unsupervised Pathology Detection: A Deep Dive Into the State of the Art.](http://arxiv.org/abs/2303.00609) | 本文深入研究了无监督病理检测技术的最新发展，通过评估和基准测试多种尖端方法，证明了工业和医疗文献中新开发的特征建模方法在各种模态和数据集上创立了新的技术水平。 |
| [^111] | [Domain-Specific Pre-training Improves Confidence in Whole Slide Image Classification.](http://arxiv.org/abs/2302.09833) | 本文研究了面向领域的预训练对整张切片图像分类的作用，发现使用面向领域的预训练可以提高多实例学习模型在切片图像分类任务中的性能。 |
| [^112] | [Physics-based parameterized neural ordinary differential equations: prediction of laser ignition in a rocket combustor.](http://arxiv.org/abs/2302.08629) | 本论文提出了一种基于参数化神经常微分方程的物理数据驱动框架，用于快速、准确地对模型火箭燃烧室中的激光起爆进行预测。 |
| [^113] | [Learning How to Infer Partial MDPs for In-Context Adaptation and Exploration.](http://arxiv.org/abs/2302.04250) | 本论文介绍了一种通过学习部分马尔可夫决策过程来进行上下文适应和探索的新方法，其使用变压器进行推理过程学习，考虑了模型假设空间，假设表示为小的马尔可夫决策过程，可以在性价比高的情况下进行动态规划。该方法在Symbolic Alchemy基准测试中表现出与精确后验抽样相近的适应速度和探索利用平衡。 |
| [^114] | [Phase Transitions in the Detection of Correlated Databases.](http://arxiv.org/abs/2302.03380) | 研究了检测相关数据库中的相变问题，根据$n$和$d$的渐近规律确定了最优检测展现出相变的尖锐阈值，补充了矩阵感应模型中相位恢复的性能。 |
| [^115] | [Interpretations of Domain Adaptations via Layer Variational Analysis.](http://arxiv.org/abs/2302.01798) | 本研究通过层变分分析证明了转移学习的成功可以通过相应的数据条件得到保证，并提出了一种基于网络的转移学习的替代方法，该方法在领域适应方面显示出了效率和准确性的提高。 |
| [^116] | [A Survey on Efficient Training of Transformers.](http://arxiv.org/abs/2302.01107) | 本文是对高效训练Transformer领域的系统综述，分析和比较了在训练中为中间张量节省计算和内存成本的方法与硬件/算法共同设计的技术，并探讨了未来研究的挑战和前景。 |
| [^117] | [Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data.](http://arxiv.org/abs/2302.00674) | 本文提出了一种在少样本学习过程中假定有辅助数据的训练范式FLAD，并针对自动混合辅助和目标数据的方法局限，提出了两种计算复杂度独立于辅助数据集数量的算法，通过FLAD和这两种算法的比较，可以发现这两种算法的表现更好。 |
| [^118] | [Learning Topology-Preserving Data Representations.](http://arxiv.org/abs/2302.00136) | 本文提出了一种名为RTD-AE的方法用于学习保持数据拓扑结构的降维表示,在保留全局结构和拓扑性方面，其表现优于现有最先进竞争对手。 |
| [^119] | [Combinatorial Inference on the Optimal Assortment in Multinomial Logit Models.](http://arxiv.org/abs/2301.12254) | 本文提出了一种基于多项式logit模型的推断框架，可以测试最优产品组合是否具有特定性质。 |
| [^120] | [Incorporating Background Knowledge in Symbolic Regression using a Computer Algebra System.](http://arxiv.org/abs/2301.11919) | 通过增加软约束，将背景知识（符号数学约束）纳入符号回归（SR）可以提高其搜索效率和模型意义性，从而生成与理论相关且与数据一致的表达式。 |
| [^121] | [Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning.](http://arxiv.org/abs/2301.11916) | 本研究发现，大型语言模型可以被视为隐式的主题模型，并提出了一种算法，从注释数据中选择最佳示范，大大提高了上下文学习的能力。 |
| [^122] | [Mathematical analysis of singularities in the diffusion model under the submanifold assumption.](http://arxiv.org/abs/2301.07882) | 本文提供了扩散模型中漂移项的数学分析。通过次流形假设，提出一种新的目标函数和相关的损失函数，可处理低维流形上的奇异数据分布，解决了均值漂移函数和得分函数渐近发散的问题。 |
| [^123] | [Understanding the Spectral Bias of Coordinate Based MLPs Via Training Dynamics.](http://arxiv.org/abs/2301.05816) | 该论文研究了基于坐标的MLPs的谱偏置对高频组件收敛的阻碍，并提出使用高频正弦波编码输入来克服这一限制。 |
| [^124] | [Online Hyperparameter Optimization for Class-Incremental Learning.](http://arxiv.org/abs/2301.05032) | 本文提出了一种能够自适应优化稳定性和可塑性权衡的在线增量学习方法，并将超参数优化过程形式化为在线MDP问题。 |
| [^125] | [A Stochastic Proximal Polyak Step Size.](http://arxiv.org/abs/2301.04935) | 本文开发了一种正则化的随机梯度下降ProxSPS算法，相比随机Polyak步长（SPS）更稳定易调整，同时在图像分类任务中表现良好，可导致网络具有更小的权重参数。 |
| [^126] | [Optimizing Serially Concatenated Neural Codes with Classical Decoders.](http://arxiv.org/abs/2212.10355) | 本文介绍了一种使用经典解码器优化神经编码的方法，并且应用BCJR算法进行了最优解码，形成了迭代Turbo解码器，优化了学习编码并取得了较好的效果。 |
| [^127] | [Reasoning with Language Model Prompting: A Survey.](http://arxiv.org/abs/2212.09597) | 本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。 |
| [^128] | [xTrimoABFold: De novo Antibody Structure Prediction without MSA.](http://arxiv.org/abs/2212.00735) | xTrimoABFold是一种基于深度抗体语言模型的新型抗体结构预测方法，无需多序列比对，有望促进高通量药物设计的应用。 |
| [^129] | [Multiresolution kernel matrix algebra.](http://arxiv.org/abs/2211.11681) | 该论文提出了一种压缩核矩阵的稀疏代数，可用于高效分析散乱数据并计算更复杂的矩阵函数，如${\bm A}^\alpha$或$\exp({\bm A})$，并应用于空间统计学中的高斯过程学习算法。 |
| [^130] | [MedleyVox: An Evaluation Dataset for Multiple Singing Voices Separation.](http://arxiv.org/abs/2211.07302) | 本文提出了用于多声部歌曲分离的评估数据集 MedleyVox，为解决缺乏用于训练的现有多声部数据集的问题，提出了使用各种单声部数据集构建多声部混合物的策略，并提出了改进的超分辨率网络（iSRNet），实现了可比较的性能。 |
| [^131] | [Global Performance Guarantees for Neural Network Models of AC Power Flow.](http://arxiv.org/abs/2211.07125) | 本文首次开发了一种可行的神经网络验证程序，它结合了非线性AC电力流方程的ground truth，以确定最坏的神经网络性能。使用顺序添加有针对性的切割，我们迭代地收紧我们的公式，直到解决方案足够紧密或达到一个安全性阈值。 |
| [^132] | [Unbiased Supervised Contrastive Learning.](http://arxiv.org/abs/2211.05568) | 本文提出了一种新的监督对比损失形式（epsilon-SupInfoNCE）以及一种新的去偏正则化损失（FairKL），旨在解决从有偏数据中学习无偏模型的问题。 |
| [^133] | [Domain Adaptation under Missingness Shift.](http://arxiv.org/abs/2211.02093) | 本文解决了领域自适应问题中缺失数据转移的情况，提出了DAMS方法。针对缺失数据指标不可用的情况，提供了理论结果，包括协变量转移被违反、最优源预测器可能比总是预测均值表现更差、最优目标预测器可被识别等。 |
| [^134] | [Exploring the impact of weather on Metro demand forecasting using machine learning method.](http://arxiv.org/abs/2210.13965) | 本文通过对亚洲地铁系统的客流数据和气象记录进行机器学习预测，发现把天气变量加入预测模型中可以提高周末的预测准确度。 |
| [^135] | [Exploration Policies for On-the-Fly Controller Synthesis: A Reinforcement Learning Approach.](http://arxiv.org/abs/2210.05393) | 本文提出了一种基于强化学习的方法，用于产生启发式来指导有向控制器合成算法的增量探索过程。 |
| [^136] | [DALL-E-Bot: Introducing Web-Scale Diffusion Models to Robotics.](http://arxiv.org/abs/2210.02438) | DALL-E-Bot是第一个将Web规模扩散模型引入到机器人学中的工作，可以在没有任何额外示例或训练的情况下零-shot地推断并重新排列场景中的物体，具有可行性和可扩展性，这为机器人学习提供了一个有前途的方向。 |
| [^137] | [CrAM: A Compression-Aware Minimizer.](http://arxiv.org/abs/2207.14200) | 提出一种新的压缩感知的优化器CrAM，其产生的密集模型可以在训练后通过单次压缩而不会带来显著的精度损失，该优化器在一些标准基准测试中的实验结果显示出竞争力。 |
| [^138] | [Secure Embedding Aggregation for Federated Representation Learning.](http://arxiv.org/abs/2206.09097) | 本论文提出了一种安全嵌入聚合协议\scheme，该协议在联合表示学习框架下，能够在保护客户端隐私的前提下，实现多个客户端的嵌入表示聚合。 |
| [^139] | [Posterior Coreset Construction with Kernelized Stein Discrepancy for Model-Based Reinforcement Learning.](http://arxiv.org/abs/2206.01162) | 本文提出一种通过核化Stein距离构建后验Coreset的MBRL方法，在放松转移模型高斯或Lipschitz的限制下表现出优异的性能，并且可以应用于大规模训练。 |
| [^140] | [Federated Learning in Satellite Constellations.](http://arxiv.org/abs/2206.00307) | 本文介绍了卫星星座中的联邦学习，根据卫星的通信能力、星座设计和参数服务器的位置提出了卫星FL的分类。卫星FL面临着独特的挑战和机遇。 |
| [^141] | [Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks.](http://arxiv.org/abs/2205.15171) | 提出一种新颖的模块化偏差缓解方法，在推理时间按需集成到核心模型中的独立去偏置子网络，在性别、种族和年龄等受保护属性的分类任务中，该方法在缓解偏差方面是有效的，并且在精度和灵活性方面优于现有技术方法。 |
| [^142] | [MiniDisc: Minimal Distillation Schedule for Language Model Compression.](http://arxiv.org/abs/2205.14570) | 本研究提出了一个叫做MiniDisc的最小蒸馏计划，可以在最少一次尝试中调度最优的教师助手，用于实现语言模型压缩。 |
| [^143] | [Adapting and Evaluating Influence-Estimation Methods for Gradient-Boosted Decision Trees.](http://arxiv.org/abs/2205.00359) | 该研究将深度学习模型的影响估计方法改编到了梯度提升决策树上，命名为TREX和BoostIn，旨在帮助更好地理解GBDT的预测和改进性能。 |
| [^144] | [Interval Bound Interpolation for Few-shot Learning with Few Tasks.](http://arxiv.org/abs/2204.03511) | 在微小样本学习中引入了区间界概念，通过最小化任务及其相应边界之间的距离来保留训练任务周围的领域，并通过插值来人为形成新任务进行训练。 |
| [^145] | [ECOLA: Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations.](http://arxiv.org/abs/2203.09590) | 本文讨论如何将文本数据与时间知识嵌入相结合以加强时间知识嵌入表征的质量，提出了ECOLA方法，该方法考虑了时间因素，并将文本信息注入到时间知识嵌入中。 |
| [^146] | [A Novel Evolutionary Algorithm for Hierarchical Neural Architecture Search.](http://arxiv.org/abs/2107.08484) | 本文提出了一种新的进化算法用于神经结构搜索，采用层级模块组织拓扑结构，应用于全局搜索空间，结合策略系统来推广表现良好的子结构，通过Fashion-MNIST和NAS-Bench101的实验得出了较好的结果。 |
| [^147] | [Non-linear Functional Modeling using Neural Networks.](http://arxiv.org/abs/2104.09371) | 本文提出了一种基于神经网络的、适用于函数数据的新型非线性模型。我们提出了两种变体，旨在显式利用函数数据中固有的结构，并通过全面的模拟研究和实际数据示例证明了该方法的有效性。 |
| [^148] | [The Role of Cross-Silo Federated Learning in Facilitating Data Sharing in the Agri-Food Sector.](http://arxiv.org/abs/2104.07468) | 跨边界联邦学习技术在农食品行业数据共享方面提供了解决方案，可以共享分散数据、保护数据贡献者的隐私和安全，并使用数据构建更好的机器学习模型。 |
| [^149] | [Neural Generalization of Multiple Kernel Learning.](http://arxiv.org/abs/2102.13337) | 本文提出了一种神经通用化的多核学习方法（NGMKL），将传统的多核学习框架扩展到具有非线性激活函数的多层神经网络中。实验证明该方法提高了算法的复杂度，并导致更高的识别精度。 |
| [^150] | [QNLP in Practice: Running Compositional Models of Meaning on a Quantum Computer.](http://arxiv.org/abs/2102.12846) | 本文介绍了在嘈杂的中间规模量子计算机上进行的首个大于100个句子数据集的NLP实验结果，成功地训练了解决简单句子分类任务的NLP模型，证明了组合模型的含义与量子理论具有形式相似性。 |
| [^151] | [GTEA: Inductive Representation Learning on Temporal Interaction Graphs via Temporal Edge Aggregation.](http://arxiv.org/abs/2009.05266) | 本文提出了 GTEA框架，用于在时序交互图上进行归纳学习，结合了时间动态建模和图嵌入，通过聚合相邻节点和边嵌入的特征，共同学习了 TIG 的拓扑和时间依赖关系，而且引入了一种稀疏感知的自注意机制，在多个时间序列预测任务中表现出有效性。 |
| [^152] | [Vertex Nomination in Richly Attributed Networks.](http://arxiv.org/abs/2005.02151) | 本文探讨了富有属性网络中顶点提名的双重作用，并提出了一种新颖的基于内容感知的网络嵌入方法，证明该方法优于现有的不利用内容和上下文的顶点提名方法。 |
| [^153] | [Extrapolation-based Prediction-Correction Methods for Time-varying Convex Optimization.](http://arxiv.org/abs/2004.11709) | 本文提出了一种基于外推策略的在线优化算法，解决了大量信号处理和机器学习中的实时优化问题，并且通过使用算子理论工具，分析了原始问题和对偶问题的显式界限。 |

# 详细

[^1]: ZipIt！无需训练即可合并不同任务的模型

    ZipIt! Merging Models from Different Tasks without Training. (arXiv:2305.03053v1 [cs.CV])

    [http://arxiv.org/abs/2305.03053](http://arxiv.org/abs/2305.03053)

    本文介绍了一种无需额外训练即可合并不同任务上训练的模型的方法“ZipIt！”。

    

    典型的深度视觉识别模型能够执行它们经过训练的单一任务。本文解决将完全不同的、每个解决一个独立任务的模型合并成一个多任务模型的极其困难的问题，而且不需要任何额外的训练。以前的模型合并工作将一个模型置换到另一个模型的空间中，再将它们相加。虽然这对于在同一个任务上经过训练的模型起作用，但我们发现，这未能考虑到在不同任务上经过训练的模型之间的差异。因此，我们引入了“ZipIt！”，这是一种通用的方法，用于合并相同结构的两个任意模型，其中包括两种简单的策略。首先，为了考虑到在模型之间没有共享的特征，我们将模型合并问题扩展到还允许合并每个模型中的特征，定义一个通用的“zip”操作。其次，我们添加支持部分压缩模型的功能，直到特定层。

    Typical deep visual recognition models are capable of performing the one task they were trained on. In this paper, we tackle the extremely difficult problem of combining completely distinct models with different initializations, each solving a separate task, into one multi-task model without any additional training. Prior work in model merging permutes one model to the space of the other then adds them together. While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce "ZipIt!", a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren't shared between models, we expand the model merging problem to additionally allow for merging features within each model by defining a general "zip" operation. Second, we add support for partially zipping the models up until a specified layer
    
[^2]: 在复杂环境中跟踪含容器和遮挡物的目标

    Tracking through Containers and Occluders in the Wild. (arXiv:2305.03052v1 [cs.CV])

    [http://arxiv.org/abs/2305.03052](http://arxiv.org/abs/2305.03052)

    本文介绍了一个新的基准模型 $\textbf{TCOW}$，用于在重度遮挡和容器中进行视觉跟踪。我们创建了一组混合的合成和真实数据集，评估了两种最新的基于变压器的视频模型，并发现它们在某些情况下能够出人意料地跟踪目标，但仍存在相当大的性能差距，必须进一步研究。

    

    在杂乱且动态的环境中跟踪具有持久性的目标仍是计算机视觉系统面临的难题。本文介绍了一个新的基准模型 $\textbf{TCOW}$，用于在重度遮挡和容器中进行视觉跟踪。我们设定了一个任务，即在给定视频序列的情况下，分割出目标物体的投影范围以及周围的容器或遮挡物。为了研究这个任务，我们创建了一组混合的合成和真实数据集，以支持模型在各种任务变化形式下的监督学习和结构化评估。我们评估了两种最新的基于变压器的视频模型，并发现尽管它们在某些任务变化的设置下能够出人意料地跟踪目标，但在我们宣称一个跟踪模型已经获得了真正的对象恒常性概念之前，仍存在相当大的性能差距。

    Tracking objects with persistence in cluttered and dynamic environments remains a difficult challenge for computer vision systems. In this paper, we introduce $\textbf{TCOW}$, a new benchmark and model for visual tracking through heavy occlusion and containment. We set up a task where the goal is to, given a video sequence, segment both the projected extent of the target object, as well as the surrounding container or occluder whenever one exists. To study this task, we create a mixture of synthetic and annotated real datasets to support both supervised learning and structured evaluation of model performance under various forms of task variation, such as moving or nested containment. We evaluate two recent transformer-based video models and find that while they can be surprisingly capable of tracking targets under certain settings of task variation, there remains a considerable performance gap before we can claim a tracking model to have acquired a true notion of object permanence.
    
[^3]: 可控的视觉-触觉合成

    Controllable Visual-Tactile Synthesis. (arXiv:2305.03051v1 [cs.CV])

    [http://arxiv.org/abs/2305.03051](http://arxiv.org/abs/2305.03051)

    本文利用深度生成模型实现了视觉-触觉的交互合成，用户可以通过触感表面触摸和看到合成物体，包括创建一个新的触感数据集和开发一个条件生成模型。

    

    深度生成模型在图形设计、电子商务和虚拟试穿等内容创作应用方面具有各种可能性，然而目前的研究主要集中在合成逼真的视觉输出上，往往忽略其他感官模态，如触觉，这限制了与用户之间的物理交互。在这项工作中，我们利用深度生成模型创建了一种多感官体验，用户可以在触觉表面上滑动手指时触摸和看到合成对象。主要的挑战在于视觉和触觉感知之间的巨大尺度差异以及触觉传感数据到触觉渲染设备的显式映射的缺乏。为了弥合这一差距，我们使用GelSight传感器收集高分辨率的触感数据，并创建了一个新的视觉触觉服装数据集。然后，我们开发了一种有条件的生成模型，从单个草图中合成视觉和触觉输出。我们评估了我们的方法，包括图像质量和触觉渲染精度。

    Deep generative models have various content creation applications such as graphic design, e-commerce, and virtual Try-on. However, current works mainly focus on synthesizing realistic visual outputs, often ignoring other sensory modalities, such as touch, which limits physical interaction with users. In this work, we leverage deep generative models to create a multi-sensory experience where users can touch and see the synthesized object when sliding their fingers on a haptic surface. The main challenges lie in the significant scale discrepancy between vision and touch sensing and the lack of explicit mapping from touch sensing data to a haptic rendering device. To bridge this gap, we collect high-resolution tactile data with a GelSight sensor and create a new visuotactile clothing dataset. We then develop a conditional generative model that synthesizes both visual and tactile outputs from a single sketch. We evaluate our method regarding image quality and tactile rendering accuracy. Fi
    
[^4]: 个性化一次性分割模型

    Personalize Segment Anything Model with One Shot. (arXiv:2305.03048v1 [cs.CV])

    [http://arxiv.org/abs/2305.03048](http://arxiv.org/abs/2305.03048)

    本文提出了一种无需训练的SAM个性化方法PerSAM，只需要一张带有参考掩模的单张图像即可定位和分割目标概念，还提出了高效的一次性微调变体PerSAM-F，旨在解决掩模不确定性问题。

    

    在大数据预训练的推动下，分割任何物体模型（SAM）已被证明是一个强大且高效的框架，革新了分割模型领域。尽管SAM非常通用，但自动为特定视觉概念定制SAM而不需要手动提示，如在不同图像中自动分割你的宠物狗等， 还未深入研究。本文提出了一种无需训练的SAM个性化方法，称为PerSAM。只需要一张带有参考掩模的单张图像，PerSAM首先通过位置先验定位目标概念，并通过三种技术来在其他图像或视频中分割它：目标引导注意力，目标语义提示和级联后处理。这样，我们有效地适应了SAM的私人使用而无需任何训练。为了进一步缓解掩模的不确定性，我们提出了一个高效的一次性微调变体，即PerSAM-F。冻结整个SAM，我们引入了两个可学习权重用于多尺度掩模，仅训练2个参数即可。

    Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promptable framework, revolutionizing the segmentation models. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under explored, e.g., automatically segmenting your pet dog in different images. In this paper, we propose a training-free Personalization approach for SAM, termed as PerSAM. Given only a single image with a reference mask, PerSAM first localizes the target concept by a location prior, and segments it within other images or videos via three techniques: target-guided attention, target-semantic prompting, and cascaded post-refinement. In this way, we effectively adapt SAM for private use without any training. To further alleviate the mask ambiguity, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce two learnable weights for multi-scale masks, only training 2 parameters wit
    
[^5]: 原则驱动自我对齐的最小人力监督的语言模型从零开始构建

    Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. (arXiv:2305.03047v1 [cs.LG])

    [http://arxiv.org/abs/2305.03047](http://arxiv.org/abs/2305.03047)

    这篇论文提出了SELF-ALIGN方法，使用基于原则的推理和LLMs的生成能力以最少的人类监督实现AI代理的自我对齐。

    

    最近的AI助手代理，如ChatGPT，主要依赖于监督微调和人类反馈的强化学习来对齐大型语言模型的输出与人类意图，确保它们是有用的、道德的、可靠的。然而，这种依赖性可能会极大地限制AI助手代理的真正潜力，因为获得人类监督的成本很高，相关问题有质量、可靠性、多样性、自一致性和不良偏见。为了解决这些挑战，我们提出了一种新的方法 SELF-ALIGN，它结合了基于原则的推理和LLMs的生成能力，以最少的人类监督实现AI代理的自我对齐。方法包括四个阶段：第一，我们使用LLM生成合成提示，使用主题引导方法增加提示多样性；第二，我们使用一小组人工编写的AI模型原则，并指导AI模型遵循；

    Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and gu
    
[^6]: VAE在重构分子结构方面效果如何？

    Are VAEs Bad at Reconstructing Molecular Graphs?. (arXiv:2305.03041v1 [cs.LG])

    [http://arxiv.org/abs/2305.03041](http://arxiv.org/abs/2305.03041)

    本文研究了最先进的分子生成模型在大规模、化学多样化数据集上的重构性能，发现重构准确性惊人地低。然而，改善重构并不一定会带来更好的采样或优化性能。

    

    当前许多分子的生成模型为分子图的变分自编码器。然而，最先进的模型在大规模且化学多样化的数据集上的重构能力尚未得到充分比较。在本文中，我们展示了当几个最先进的生成模型在相同条件下进行评估时，它们的重构准确性惊人的低，甚至不如之前看似更难的数据集。然而，我们还表明，提高重构并不直接导致更好的采样或优化性能。

    Many contemporary generative models of molecules are variational auto-encoders of molecular graphs. One term in their training loss pertains to reconstructing the input, yet reconstruction capabilities of state-of-the-art models have not yet been thoroughly compared on a large and chemically diverse dataset. In this work, we show that when several state-of-the-art generative models are evaluated under the same conditions, their reconstruction accuracy is surprisingly low, worse than what was previously reported on seemingly harder datasets. However, we show that improving reconstruction does not directly lead to better sampling or optimization performance. Failed reconstructions from the MoLeR model are usually similar to the inputs, assembling the same motifs in a different way, and possess similar chemical properties such as solubility. Finally, we show that the input molecule and its failed reconstruction are usually mapped by the different encoders to statistically distinguishable 
    
[^7]: SuperNOVA: 计算笔记本中互动可视化的设计策略与机会

    SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks. (arXiv:2305.03039v1 [cs.HC])

    [http://arxiv.org/abs/2305.03039](http://arxiv.org/abs/2305.03039)

    通过分析159个交互式可视化工具及其用户反馈，本论文提出了在计算笔记本中设计可视分析工具的独特机会和考虑因素。

    

    计算笔记本，如 Jupyter Notebook，已成为数据科学家的事实编程环境。许多可视化研究者和实践者已开发出支持笔记本的交互式可视化工具。然而，关于在笔记本中设计 VA 工具的适当方法的了解甚少。为了填补这一关键的研究空白，我们通过分析 159 个笔记本 VA 工具及其用户反馈来研究这一领域的设计策略。我们的分析包括来自学术论文的 62 个系统和来自通过在 GitHub 上抓取 860 万个笔记本而获得的包含交互式可视化的 55k 个笔记本池中的 103 个系统。我们还研究了 15 个用户研究的发现和 379 个 GitHub 问题中的用户反馈。通过这项工作，我们确定了未来笔记本 VA 工具的独特设计机会和考虑因素，例如在笔记本中使用和操作多模态数据以及平衡可视化笔记本的程度。

    Computational notebooks such as Jupyter Notebook have become data scientists' de facto programming environments. Many visualization researchers and practitioners have developed interactive visualization tools that support notebooks. However, little is known about the appropriate design of visual analytics (VA) tools in notebooks. To bridge this critical research gap, we investigate the design strategies in this space by analyzing 159 notebook VA tools and their users' feedback. Our analysis encompasses 62 systems from academic papers and 103 systems sourced from a pool of 55k notebooks containing interactive visualizations that we obtain via scraping 8.6 million notebooks on GitHub. We also examine findings from 15 user studies and user feedback in 379 GitHub issues. Through this work, we identify unique design opportunities and considerations for future notebook VA tools, such as using and manipulating multimodal data in notebooks as well as balancing the degree of visualization-noteb
    
[^8]: 从野外视频中学习手持物体重建

    Learning Hand-Held Object Reconstruction from In-The-Wild Videos. (arXiv:2305.03036v1 [cs.CV])

    [http://arxiv.org/abs/2305.03036](http://arxiv.org/abs/2305.03036)

    本研究提出了一种从野外视频中自动提取三维监督来扩展手持物体重建模型的学习方法。通过使用手部姿势作为物体姿势的代理和学习数据驱动的三维形状先验知识等方法，有效地解决了未知相机姿势和遮挡等问题，从而通过从单个RGB图像预测物体三维形状的占据网络得到了优秀的结果。

    

    先前的单影像手持物体重建方法依赖于难以在真实世界中规模化收集的直接3D形状监督，因此这些方法在野外环境下面对新颖物体时难以推广。本文从生动的野外原始视频数据中自动提取三维监督，并通过多视角二维监督来扩展手持物体重建模型的学习。这需要应对两个关键挑战：未知的相机姿势和遮挡。对于前者，我们使用手部姿势作为物体姿势的代理。对于后者，我们使用ObMan数据集中合成的物体来学习数据驱动的三维形状先验知识。我们使用这些间接的三维线索来训练占据网络，从单个RGB图像预测物体的三维形状。

    Prior works for reconstructing hand-held objects from a single image rely on direct 3D shape supervision which is challenging to gather in real world at scale. Consequently, these approaches do not generalize well when presented with novel objects in in-the-wild settings. While 3D supervision is a major bottleneck, there is an abundance of in-the-wild raw video data showing hand-object interactions. In this paper, we automatically extract 3D supervision (via multiview 2D supervision) from such raw video data to scale up the learning of models for hand-held object reconstruction. This requires tackling two key challenges: unknown camera pose and occlusion. For the former, we use hand pose (predicted from existing techniques, e.g. FrankMocap) as a proxy for object pose. For the latter, we learn data-driven 3D shape priors using synthetic objects from the ObMan dataset. We use these indirect 3D cues to train occupancy networks that predict the 3D shape of objects from a single RGB image. 
    
[^9]: FastAMI -- 一种蒙特卡罗方法用于聚类比较度量中的偶然性调整

    FastAMI -- a Monte Carlo Approach to the Adjustment for Chance in Clustering Comparison Metrics. (arXiv:2305.03022v1 [cs.LG])

    [http://arxiv.org/abs/2305.03022](http://arxiv.org/abs/2305.03022)

    FastAMI是一种用于大型数据集的聚类比较的快速方法，通过蒙特卡罗模拟来实现偶然性调整，相比于传统的基于排列的方法更准确。

    

    聚类是机器学习的核心，随着数据可用性的增加，其应用日益增多。然而，随着数据集的增长，带有偶然性调整的聚类比较变得计算困难，导致没有偏见的真实比较和解决方案选择。我们提出了FastAMI，一种基于蒙特卡罗的方法，用于高效地估计经过调整的互信息（AMI）并将其扩展到标准化互信息（SMI）。与准确计算相比，我们的方法足够快，可以为大型数据集启用这些带有调整的信息论比较，同时保持比配对方法更准确的结果。

    Clustering is at the very core of machine learning, and its applications proliferate with the increasing availability of data. However, as datasets grow, comparing clusterings with an adjustment for chance becomes computationally difficult, preventing unbiased ground-truth comparisons and solution selection. We propose FastAMI, a Monte Carlo-based method to efficiently approximate the Adjusted Mutual Information (AMI) and extend it to the Standardized Mutual Information (SMI). The approach is compared with the exact calculation and a recently developed variant of the AMI based on pairwise permutations, using both synthetic and real data. In contrast to the exact calculation our method is fast enough to enable these adjusted information-theoretic comparisons for large datasets while maintaining considerably more accurate results than the pairwise approach.
    
[^10]: 使用BERT和Query-Aware LSH提高非正式文档中代码示例推荐：一项比较研究

    Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study. (arXiv:2305.03017v1 [cs.SE])

    [http://arxiv.org/abs/2305.03017](http://arxiv.org/abs/2305.03017)

    本研究使用BERT和Query-Aware LSH提高非正式文档中代码示例推荐的质量，重点关注于Stack Overflow上的Java编程语言。研究使用BERT将代码示例转换为数值向量。

    

    过去和最近一直在进行代码示例推荐的研究，以帮助开发人员完成软件开发任务。由于开发人员经常花费大量时间在互联网上寻找相关的代码示例，利用开源项目和非正式文档。为了找到有用的代码示例，非正式文档（如Stack Overflow讨论和论坛）可以非常宝贵。我们的研究重点是Stack Overflow，它是软件开发人员讨论不同主题的流行资源。为了提高推荐代码示例的质量，我们收集并推荐了Java编程语言中最佳的代码示例。我们采用了BERT来进行处理，它是一个大型语言模型（LLM），可以有效地从文本数据中提取语义信息。我们的第一步是使用BERT将代码示例转换为数值向量。

    The study of code example recommendation has been conducted extensively in the past and recently in order to assist developers in their software development tasks. This is because developers often spend significant time searching for relevant code examples on the internet, utilizing open-source projects and informal documentation. For finding useful code examples, informal documentation, such as Stack Overflow discussions and forums, can be invaluable. We have focused our research on Stack Overflow, which is a popular resource for discussing different topics among software developers. For increasing the quality of the recommended code examples, we have collected and recommended the best code examples in the Java programming language. We have utilized BERT in our approach, which is a Large Language Model (LLM) for text representation that can effectively extract semantic information from textual data. Our first step involved using BERT to convert code examples into numerical vectors. Su
    
[^11]: 神经网络何时在表格数据上胜过增强树？

    When Do Neural Nets Outperform Boosted Trees on Tabular Data?. (arXiv:2305.02997v1 [cs.LG])

    [http://arxiv.org/abs/2305.02997](http://arxiv.org/abs/2305.02997)

    这项研究通过对176个数据集的比较分析发现，在许多数据集中，GBDT和NN之间的性能差异可以忽略不计，或者GBDT的轻微超参数调整比选择最佳算法更重要。此外，研究人员对965个元特征进行了分析，发现GBDT在高维稀疏数据上表现更好。

    

    表格数据是机器学习中最常用的数据类型之一。尽管神经网络（NN）在表格数据上取得了最近的进展，但人们仍在积极讨论NN是否通常优于梯度提升决策树（GBDT）在表格数据上的表现，一些最近的工作要么认为GBDT在表格数据上一贯优于NN，要么认为NN优于GBDT。在这项工作中，我们退一步问：'这重要吗？'我们通过对176个数据集比较19种算法，进行了迄今为止最大的表格数据分析，并发现'NN vs. GBDT'争论被过分强调：令人惊讶的是，在相当多的数据集中，GBDT和NN之间的性能差异要么可以忽略不计，要么GBDT的轻微超参数调整比选择最佳算法更重要。接下来，我们分析了965个元特征，以确定数据集的哪些特性使NN或GBDT更适合表现良好。例如，我们发现GBDT要比NN在高维稀疏数据上表现更好。

    Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and ask, 'does it matter?' We conduct the largest tabular data analysis to date, by comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than selecting the best algorithm. Next, we analyze 965 metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better th
    
[^12]: 带有交叉编码器的CUR k-NN搜索的自适应锚定项选择

    Adaptive Selection of Anchor Items for CUR-based k-NN search with Cross-Encoders. (arXiv:2305.02996v1 [cs.IR])

    [http://arxiv.org/abs/2305.02996](http://arxiv.org/abs/2305.02996)

    本文提出了一种自适应锚点选择方法，可以在保持较小的计算成本的同时，实现与随机抽样锚点相当或者更好的k-NN召回性能。

    

    本文提出了一种自适应锚点选择方法，以改善ANNCUR模型中高前k项的逼近误差和召回率。该方法可以在保持较小的计算成本的同时，实现与随机抽样锚点相当或者更好的k-NN召回性能。

    Cross-encoder models, which jointly encode and score a query-item pair, are typically prohibitively expensive for k-nearest neighbor search. Consequently, k-NN search is performed not with a cross-encoder, but with a heuristic retrieve (e.g., using BM25 or dual-encoder) and re-rank approach. Recent work proposes ANNCUR (Yadav et al., 2022) which uses CUR matrix factorization to produce an embedding space for efficient vector-based search that directly approximates the cross-encoder without the need for dual-encoders. ANNCUR defines this shared query-item embedding space by scoring the test query against anchor items which are sampled uniformly at random. While this minimizes average approximation error over all items, unsuitably high approximation error on top-k items remains and leads to poor recall of top-k (and especially top-1) items. Increasing the number of anchor items is a straightforward way of improving the approximation error and hence k-NN recall of ANNCUR but at the cost o
    
[^13]: 关于数据子群体间机器学习模型性能的非线性相关性

    On the nonlinear correlation of ML performance between data subpopulations. (arXiv:2305.02995v1 [cs.LG])

    [http://arxiv.org/abs/2305.02995](http://arxiv.org/abs/2305.02995)

    在不同数据子群体间，机器学习模型的内部准确性和外部准确性之间的相关性是非线性的，呈现出“月亮形”的相关性。

    

    理解机器学习模型在不同数据分布下的性能对于可靠的应用至关重要。尽管最新的经验研究认为训练数据内部的准确性和新数据外部的准确性之间存在近乎完美的线性相关性，但我们在各种数据集、模型和训练时期进行了严格的实验和分析，发现在子群体转移下，内部准确性和外部准确性之间的相关性更为微妙，并且在上升阶段存在“月亮形”的相关性（抛物线上升曲线）。

    Understanding the performance of machine learning (ML) models across diverse data distributions is critically important for reliable applications. Despite recent empirical studies positing a near-perfect linear correlation between in-distribution (ID) and out-of-distribution (OOD) accuracies, we empirically demonstrate that this correlation is more nuanced under subpopulation shifts. Through rigorous experimentation and analysis across a variety of datasets, models, and training epochs, we demonstrate that OOD performance often has a nonlinear correlation with ID performance in subpopulation shifts. Our findings, which contrast previous studies that have posited a linear correlation in model performance during distribution shifts, reveal a "moon shape" correlation (parabolic uptrend curve) between the test performance on the majority subpopulation and the minority subpopulation. This non-trivial nonlinear correlation holds across model architectures, hyperparameters, training durations
    
[^14]: SemEval-2023任务7: 临床试验数据的多证据自然语言推理

    SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data. (arXiv:2305.02993v1 [cs.CL])

    [http://arxiv.org/abs/2305.02993](http://arxiv.org/abs/2305.02993)

    本论文介绍SemEval 2023的任务七，旨在进行临床试验数据的多证据自然语言推理，该任务难度较大，证据选择任务相对于蕴含任务表现更佳。

    

    本篇论文介绍SemEval 2023任务7的结果，该任务主要涉及临床试验数据中的多证据自然语言推理（NLI4CT），由两个子任务组成：一个是自然语言推理（NLI）任务，另一个是证据选择任务。这两个任务需要进行医学和数字推理，这对于开发能够进行大规模医疗证据解释和检索、提供个性化基于证据的保健具有重要意义。第1个子任务“蕴含任务”收到了来自40位参赛者的643份提交，第2个子任务“证据选择任务”收到了来自23位参赛者的364份提交。这两个任务具有挑战性，大部分提交的系统在蕴含任务上未能明显优于大多数类基线，而我们观察到证据选择任务的表现明显优于蕴含任务。增加模型参数会导致模型在测试集上表现更差。

    This paper describes the results of SemEval 2023 task 7 -- Multi-Evidence Natural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2 tasks, a Natural Language Inference (NLI) task, and an evidence selection task on clinical trial data. The proposed challenges require multi-hop biomedical and numerical reasoning, which are of significant importance to the development of systems capable of large-scale interpretation and retrieval of medical evidence, to provide personalized evidence-based care.  Task 1, the entailment task, received 643 submissions from 40 participants, and Task 2, the evidence selection task, received 364 submissions from 23 participants. The tasks are challenging, with the majority of submitted systems failing to significantly outperform the majority class baseline on the entailment task, and we observe significantly better performance on the evidence selection task than on the entailment task. Increasing the number of model parameters leads to a di
    
[^15]: ExeKGLib：基于知识图谱的机器学习分析库

    ExeKGLib: Knowledge Graphs-Empowered Machine Learning Analytics. (arXiv:2305.02966v1 [cs.LG])

    [http://arxiv.org/abs/2305.02966](http://arxiv.org/abs/2305.02966)

    ExeKGLib是一个基于知识图谱的Python机器学习库，可帮助不具备深入ML知识的用户构建可执行的ML工作流，并提高透明度和可重用性。

    

    许多机器学习（ML）库对ML从业者开放。典型的ML流程是复杂的，由一系列步骤组成，每个步骤都调用了几个ML库。在这篇论文中，我们介绍了ExeKGLib，这是一个Python库，允许具有编码技能和最小ML知识的用户构建ML流水线。ExeKGLib依赖于知识图谱，以改善构建的ML工作流的透明度和可重用性，并确保其可执行性。我们演示了ExeKGLib的用法，并将其与传统的ML代码进行比较，以展示其优势。

    Many machine learning (ML) libraries are accessible online for ML practitioners. Typical ML pipelines are complex and consist of a series of steps, each of them invoking several ML libraries. In this demo paper, we present ExeKGLib, a Python library that allows users with coding skills and minimal ML knowledge to build ML pipelines. ExeKGLib relies on knowledge graphs to improve the transparency and reusability of the built ML workflows, and to ensure that they are executable. We demonstrate the usage of ExeKGLib and compare it with conventional ML code to show its benefits.
    
[^16]: 受权重影响的计数赌博机: 通过重复暴露来克服不可解性

    Weighted Tallying Bandits: Overcoming Intractability via Repeated Exposure Optimality. (arXiv:2305.02955v1 [stat.ML])

    [http://arxiv.org/abs/2305.02955](http://arxiv.org/abs/2305.02955)

    该论文提出了一种受权重影响的计数赌博机(WTB)设置，通过Repeated Exposure Optimality(REO)来研究它。他们提出了一个算法来满足REO，并提供了最优的遗憾边界。

    

    在在线学习的推荐系统或众包应用中，人类的偏好或能力通常是算法最近行动的一个函数。 相关工作已经形式化了设置，在这些设置中，行动的损失是最近$m$个时间步中该行动的播放次数的函数，其中$m$对应于人类记忆能力的上限。 为了更忠实地反映人类记忆随时间的衰减，我们引入了受权重影响的计数赌博机(WTB)，它通过要求行动损失是最近$m$个时间步中该臂被玩的次数的加权总和的函数来概括这个设置。除非进一步假设，否则WTB设置是不可解的。因此，我们在Repeated Exposure Optimality(REO)下研究了它，该条件是受人体生理学文献的启发，它要求存在一种行动，当反复播放时，最终将产生比任何其他行动更小的损失。 我们提出了一种算法，满足WTB设置下的REO，并提供了最优地缩放$m$和行动集大小的遗憾边界。 我们的证明技术要求在一种解耦形式下进行新颖的浓度结果，这可能是独立感兴趣的。

    In recommender system or crowdsourcing applications of online learning, a human's preferences or abilities are often a function of the algorithm's recent actions. Motivated by this, a significant line of work has formalized settings where an action's loss is a function of the number of times that action was recently played in the prior $m$ timesteps, where $m$ corresponds to a bound on human memory capacity. To more faithfully capture decay of human memory with time, we introduce the Weighted Tallying Bandit (WTB), which generalizes this setting by requiring that an action's loss is a function of a \emph{weighted} summation of the number of times that arm was played in the last $m$ timesteps. This WTB setting is intractable without further assumption. So we study it under Repeated Exposure Optimality (REO), a condition motivated by the literature on human physiology, which requires the existence of an action that when repetitively played will eventually yield smaller loss than any othe
    
[^17]: 重新思考基于种群协助的离策略强化学习

    Rethinking Population-assisted Off-policy Reinforcement Learning. (arXiv:2305.02949v1 [cs.LG])

    [http://arxiv.org/abs/2305.02949](http://arxiv.org/abs/2305.02949)

    本文旨在重新审视基于种群协助的离策略强化学习，通过实验证明了将来自种群优化迭代的多样化数据添加到离策略更新中会降低性能，并提出了一种简单而有效的解决方案，即仅使用种群优化的最新数据作为离策略更新的校正项，从而显著改善了样本利用率和最终性能。

    

    离策略强化学习算法由于梯度更新和在回放缓冲区中的数据重用而具有高效的样本利用率，但由于有限的探索能力往往难以收敛到局部最优解。另一方面，基于种群的算法提供了一种自然的探索策略，但其启发式黑盒操作效率低下。近期的算法将这两种方法相结合，通过共享回放缓冲区连接它们。然而，种群优化迭代中使用多样化数据对离策略强化学习算法的影响尚未得到彻底的研究。本文首先分析了基于种群优化的离策略强化学习算法的使用，揭示了使用种群数据可能会引入被忽视的误差并影响性能。为了检验这一点，我们提出了一种统一和可扩展的训练设计，并在开放AI gym中的机器人运动任务中进行了实验。结果表明，在训练过程中添加种群数据确实会损害性能，而使用种群优化的最新数据作为离策略更新的校正项是一个简单而有效的解决方案。这个校正项可以在样本利用率和最终性能方面都显著改进。

    While off-policy reinforcement learning (RL) algorithms are sample efficient due to gradient-based updates and data reuse in the replay buffer, they struggle with convergence to local optima due to limited exploration. On the other hand, population-based algorithms offer a natural exploration strategy, but their heuristic black-box operators are inefficient. Recent algorithms have integrated these two methods, connecting them through a shared replay buffer. However, the effect of using diverse data from population optimization iterations on off-policy RL algorithms has not been thoroughly investigated. In this paper, we first analyze the use of off-policy RL algorithms in combination with population-based algorithms, showing that the use of population data could introduce an overlooked error and harm performance. To test this, we propose a uniform and scalable training design and conduct experiments on our tailored framework in robot locomotion tasks from the OpenAI gym. Our results su
    
[^18]: 利用梯度衡量数据选择和估价在差分隐私训练中的应用

    Leveraging gradient-derived metrics for data selection and valuation in differentially private training. (arXiv:2305.02942v1 [cs.LG])

    [http://arxiv.org/abs/2305.02942](http://arxiv.org/abs/2305.02942)

    研究如何在隐私增强技术下，利用梯度信息识别训练中感兴趣的数据样本进行数据选择和估价。

    

    由于监管担忧和参与度的不足，为机器学习模型进行协作训练获取高质量数据可能是一项具有挑战性的任务。隐私增强技术（PET）是解决监管问题的一种常用方法，差分隐私（DP）训练是其中最常用的一种方法。本文研究了如何使用梯度信息来识别隐私训练中感兴趣的训练样本。我们展示了在最严格的隐私设置中，存在着能够为客户提供有原则的数据选择工具的技术。

    Obtaining high-quality data for collaborative training of machine learning models can be a challenging task due to A) the regulatory concerns and B) lack of incentive to participate. The first issue can be addressed through the use of privacy enhancing technologies (PET), one of the most frequently used one being differentially private (DP) training. The second challenge can be addressed by identifying which data points can be beneficial for model training and rewarding data owners for sharing this data. However, DP in deep learning typically adversely affects atypical (often informative) data samples, making it difficult to assess the usefulness of individual contributions. In this work we investigate how to leverage gradient information to identify training samples of interest in private training settings. We show that there exist techniques which are able to provide the clients with the tools for principled data selection even in strictest privacy settings.
    
[^19]: 分段归一化流

    Piecewise Normalizing Flows. (arXiv:2305.02930v1 [stat.ML])

    [http://arxiv.org/abs/2305.02930](http://arxiv.org/abs/2305.02930)

    介绍了一种分段归一化流方法，将目标分布分成集群，并通过训练模拟复杂的多模态目标。这种方法可以更好地匹配标准正态基础分布的拓扑结构。

    

    归一化流是一种通过从基础分布进行可逆转换来对复杂概率密度进行建模的成熟方法。然而，目标分布能否精确地被归一化流所捕捉，强烈受到基础分布的拓扑结构的影响。目标和基础分布之间的拓扑不匹配可能导致性能差，如对于多模态问题。一些不同的工作试图通过使用高斯混合模型 [Izmailov et al., 2020、Ardizzone et al., 2020、Hagemann and Neumayer, 2021] 或学习接受/拒绝采样 [Stimper et al., 2022] 来修改基础分布的拓扑结构以更好地匹配目标分布。我们引入了分段归一化流，将目标分布分成集群，并训练一系列流来模拟复杂的多模态目标。

    Normalizing flows are an established approach for modelling complex probability densities through invertible transformations from a base distribution. However, the accuracy with which the target distribution can be captured by the normalizing flow is strongly influenced by the topology of the base distribution. A mismatch between the topology of the target and the base can result in a poor performance, as is the case for multi-modal problems. A number of different works have attempted to modify the topology of the base distribution to better match the target, either through the use of Gaussian Mixture Models [Izmailov et al., 2020, Ardizzone et al., 2020, Hagemann and Neumayer, 2021] or learned accept/reject sampling [Stimper et al., 2022]. We introduce piecewise normalizing flows which divide the target distribution into clusters, with topologies that better match the standard normal base distribution, and train a series of flows to model complex multi-modal targets. The piecewise nat
    
[^20]: 无偏学习排序的基础和应用的最新进展

    Recent Advances in the Foundations and Applications of Unbiased Learning to Rank. (arXiv:2305.02914v1 [cs.IR])

    [http://arxiv.org/abs/2305.02914](http://arxiv.org/abs/2305.02914)

    本文介绍了无偏学习排序（ULTR）的基础概念和最新进展，以及几种实际应用的方法。教程分为四个部分：偏差的概述，ULTR的最新估计技术，ULTR在实际应用中的表现，以及ULTR与排名公平性的联系。

    

    无偏学习排序（ULTR）领域自诞生以来一直处于非常活跃的状态，并在近年来取得了几项有影响力的进展。本教程既介绍了该领域的核心概念，又概述了其基础的最新进展以及其方法的几种应用。本教程分为四部分：首先，我们概述了可以用ULTR方法解决的不同形式的偏差。其次，我们全面讨论了ULTR领域的最新估计技术。第三，我们调查了ULTR在实际应用中的发布结果。第四，我们讨论了ULTR与排名公平性之间的联系。最后，我们简要反思了ULTR研究及其应用的未来。本教程旨在使对开发新的ULTR解决方案或在实际应用中利用它们的研究人员和工业实践者受益。

    Since its inception, the field of unbiased learning to rank (ULTR) has remained very active and has seen several impactful advancements in recent years. This tutorial provides both an introduction to the core concepts of the field and an overview of recent advancements in its foundations along with several applications of its methods. The tutorial is divided into four parts: Firstly, we give an overview of the different forms of bias that can be addressed with ULTR methods. Secondly, we present a comprehensive discussion of the latest estimation techniques in the ULTR field. Thirdly, we survey published results of ULTR in real-world applications. Fourthly, we discuss the connection between ULTR and fairness in ranking. We end by briefly reflecting on the future of ULTR research and its applications. This tutorial is intended to benefit both researchers and industry practitioners who are interested in developing new ULTR solutions or utilizing them in real-world applications.
    
[^21]: 利用一致性优化实现集群联邦学习的组共识

    FedCBO: Reaching Group Consensus in Clustered Federated Learning through Consensus-based Optimization. (arXiv:2305.02894v1 [cs.LG])

    [http://arxiv.org/abs/2305.02894](http://arxiv.org/abs/2305.02894)

    本文基于一致性优化（CBO）的思想，提出了一种新的解决方案，通过互动粒子系统实现对于聚类联邦学习中各个群组的有效模型训练.

    

    联邦学习是现代机器学习中的重要框架，旨在整合来自多个用户的学习模型的训练，每个用户都有自己的本地数据集，以满足数据隐私和通信丢失约束。在聚类联邦学习中，假设用户之间存在附加的未知群组结构，并且目标是训练对每个群组有用的模型，而不仅仅是为所有用户训练一个单一的全局模型。本文提出了一种新的解决方案，通过一致性优化（CBO）的思想，解决了聚类联邦学习的问题。我们的新型CBO类型方法基于一个互动粒子系统，忽略了群组成员资格。我们的模型得到了严格的数学推理支持，包括描述我们的粒子系统大量粒子极限的平均场分析，以及同时全局优化的收敛保证。

    Federated learning is an important framework in modern machine learning that seeks to integrate the training of learning models from multiple users, each user having their own local data set, in a way that is sensitive to data privacy and to communication loss constraints. In clustered federated learning, one assumes an additional unknown group structure among users, and the goal is to train models that are useful for each group, rather than simply training a single global model for all users. In this paper, we propose a novel solution to the problem of clustered federated learning that is inspired by ideas in consensus-based optimization (CBO). Our new CBO-type method is based on a system of interacting particles that is oblivious to group memberships. Our model is motivated by rigorous mathematical reasoning, including a mean field analysis describing the large number of particles limit of our particle system, as well as convergence guarantees for the simultaneous global optimization
    
[^22]: 比特平面编码实现的输入层二值化

    Input Layer Binarization with Bit-Plane Encoding. (arXiv:2305.02885v1 [cs.LG])

    [http://arxiv.org/abs/2305.02885](http://arxiv.org/abs/2305.02885)

    该论文提出了一种使用比特平面编码实现的输入层二值化的方法，避免了传统方法中由于数据扩展导致的计算量增加问题，从而实现了完全二值化的模型，并在多个数据集上进行了评估。

    

    二值神经网络（BNN）使用1位权重和激活来在边缘设备上高效执行深度卷积神经网络。然而，传统上将第一层二值化的效果不佳，导致精度损失很大。本文提出了一种新的方法，直接使用8位表示输入数据来进行第一层二值化；我们利用标准的比特平面编码按比特的方式提取特征(使用深度卷积); 经过重新加权和特征融合，最终的模型是完全二值化的，我们的第一层二值化方法是与模型无关的。该概念在三个分类数据集（CIFAR10、SVHN和CIFAR100）上进行了评估。

    Binary Neural Networks (BNNs) use 1-bit weights and activations to efficiently execute deep convolutional neural networks on edge devices. Nevertheless, the binarization of the first layer is conventionally excluded, as it leads to a large accuracy loss. The few works addressing the first layer binarization, typically increase the number of input channels to enhance data representation; such data expansion raises the amount of operations needed and it is feasible only on systems with enough computational resources. In this work, we present a new method to binarize the first layer using directly the 8-bit representation of input data; we exploit the standard bit-planes encoding to extract features bit-wise (using depth-wise convolutions); after a re-weighting stage, features are fused again. The resulting model is fully binarized and our first layer binarization approach is model independent. The concept is evaluated on three classification datasets (CIFAR10, SVHN and CIFAR100) for diff
    
[^23]: 简单的嘈杂环境增强用于强化学习

    Simple Noisy Environment Augmentation for Reinforcement Learning. (arXiv:2305.02882v1 [cs.LG])

    [http://arxiv.org/abs/2305.02882](http://arxiv.org/abs/2305.02882)

    本论文探讨了一系列通用封装，用于噪声增强RL环境，提供了两种新的增强技术，有助于代理人探索和改善训练数据多样性。同时，介绍了控制噪声注入频率的超参数噪声率。在实验中使用了三种流行的RL算法，Soft Actor-Critic（SAC），Twin Delayed DDPG（TD3）和Proximal Policy，以研究这些封装对回报的影响。

    

    数据增强是提升机器学习模型性能的一种广泛应用的技术，尤其是在计算机视觉和自然语言处理方面。最近，越来越多的人开始应用增强技术来解决强化学习（RL）问题，主要专注于基于图像的增强。本文旨在探讨一系列通用封装，设计用于噪声增强RL环境，并鼓励代理人探索和改善训练数据多样性，适用于广泛的RL算法和环境。具体而言，我们集中于涉及状态、奖励和转换动态的增强，并介绍了两种新的增强技术。此外，我们引入噪声率超参数，以控制噪声注入的频率。我们使用三种流行的RL算法，Soft Actor-Critic（SAC），Twin Delayed DDPG（TD3）和Proximal Policy进行实验，研究这些封装对回报的影响。

    Data augmentation is a widely used technique for improving model performance in machine learning, particularly in computer vision and natural language processing. Recently, there has been increasing interest in applying augmentation techniques to reinforcement learning (RL) problems, with a focus on image-based augmentation. In this paper, we explore a set of generic wrappers designed to augment RL environments with noise and encourage agent exploration and improve training data diversity which are applicable to a broad spectrum of RL algorithms and environments. Specifically, we concentrate on augmentations concerning states, rewards, and transition dynamics and introduce two novel augmentation techniques. In addition, we introduce a noise rate hyperparameter for control over the frequency of noise injection. We present experimental results on the impact of these wrappers on return using three popular RL algorithms, Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3), and Proximal Policy
    
[^24]: 量子生成建模中的可训练性障碍和机遇

    Trainability barriers and opportunities in quantum generative modeling. (arXiv:2305.02881v1 [quant-ph])

    [http://arxiv.org/abs/2305.02881](http://arxiv.org/abs/2305.02881)

    本文研究了量子生成模型的可训练性障碍，如荒芜高原和指数损失集中，使用隐式生成模型和明确损失会产生一种新的荒芜高原现象。最大均值差可以是低秩且可训练的或全局性且不可训练的。但是，可训练性所需的低秩损失通常不能区分高频和低频特征。

    

    量子生成模型提供了本质高效的采样策略，因此在量子硬件上实现近期优势有很大的潜力。然而，它们的可扩展性仍存在重要问题。本文研究量子生成模型的可训练性障碍，如荒芜高原和指数损失集中。我们探索了明确和隐含模型、损失之间的相互作用，并表明使用隐式生成模型（如基于量子电路的模型）和明确损失（如KL散度）会产生一种新的荒芜高原现象。相比之下，最大均值差（MMD），作为隐式损失的一个流行例子，可以看作是一个观测量的期望值，该观测量可能是低秩且可训练的，也可能是全局性且不可训练的，具体取决于核函数的选择。然而，我们同时强调，可训练性所需的低秩损失通常不能区分高频和低频特征。

    Quantum generative models, in providing inherently efficient sampling strategies, show promise for achieving a near-term advantage on quantum hardware. Nonetheless, important questions remain regarding their scalability. In this work, we investigate the barriers to the trainability of quantum generative models posed by barren plateaus and exponential loss concentration. We explore the interplay between explicit and implicit models and losses, and show that using implicit generative models (such as quantum circuit-based models) with explicit losses (such as the KL divergence) leads to a new flavour of barren plateau. In contrast, the Maximum Mean Discrepancy (MMD), which is a popular example of an implicit loss, can be viewed as the expectation value of an observable that is either low-bodied and trainable, or global and untrainable depending on the choice of kernel. However, in parallel, we highlight that the low-bodied losses required for trainability cannot in general distinguish hig
    
[^25]: 分层Transformer用于可扩展图学习

    Hierarchical Transformer for Scalable Graph Learning. (arXiv:2305.02866v1 [cs.LG])

    [http://arxiv.org/abs/2305.02866](http://arxiv.org/abs/2305.02866)

    本文提出了分层可扩展图Transformer (HSGT)用于解决图表示学习中的规模问题和上下文信息捕获不足问题，通过构建多尺度图分层结构，HSGT实现了对大型图的快速和内存高效处理，并在基准数据集上展现了卓越的性能表现。

    

    图Transformer在机器学习领域中越来越受到关注，并在图表示学习的基准测试中展现出了最先进的性能。然而，由于当前实现的图Transformer主要集中在学习小规模图的表示上，全局自注意机制的二次复杂度对于应用于较大规模图的全批量训练构成了挑战。此外，传统的基于采样的方法无法捕捉必要的高层次上下文信息，导致性能严重下降。在本文中，我们引入了分层可扩展图Transformer (HSGT)作为这些挑战的解决方案。HSGT成功地将Transformer架构扩展到大规模图上的节点表示学习任务中，同时保持高性能。通过利用通过粗化技术构建的图分层结构，HSGT有效地更新和存储多尺度信息，从而实现对大型图的快速和内存高效处理。我们在几个基准数据集上评估了HSGT，并展示了它相对于现有方法的卓越性能。

    Graph Transformer is gaining increasing attention in the field of machine learning and has demonstrated state-of-the-art performance on benchmarks for graph representation learning. However, as current implementations of Graph Transformer primarily focus on learning representations of small-scale graphs, the quadratic complexity of the global self-attention mechanism presents a challenge for full-batch training when applied to larger graphs. Additionally, conventional sampling-based methods fail to capture necessary high-level contextual information, resulting in a significant loss of performance. In this paper, we introduce the Hierarchical Scalable Graph Transformer (HSGT) as a solution to these challenges. HSGT successfully scales the Transformer architecture to node representation learning tasks on large-scale graphs, while maintaining high performance. By utilizing graph hierarchies constructed through coarsening techniques, HSGT efficiently updates and stores multi-scale informat
    
[^26]: 最大因果熵逆约束强化学习

    Maximum Causal Entropy Inverse Constrained Reinforcement Learning. (arXiv:2305.02857v1 [cs.LG])

    [http://arxiv.org/abs/2305.02857](http://arxiv.org/abs/2305.02857)

    该论文提出了一种新颖的最大因果熵逆约束强化学习方法，通过利用最大因果熵原理来学习代理遵守限制的约束条件和最优策略，使用遵守这些限制的代理的实例进行学习。此方法已被证明在各种任务和环境中优于最先进的方法。

    

    在实际环境中，当人们与人工智能代理进行交互时，代理的行为与该环境的价值观、社会规范或其他要求相一致至关重要。然而，许多环境都有难以规定和转移给学习代理的隐含限制。为了应对这一挑战，我们提出了一种新的方法，利用最大因果熵原理来学习代理遵守限制的约束条件和最优策略，并使用遵守这些限制的代理的实例进行学习。我们在表格设置中证明了收敛性，并提供了一种适用于复杂环境的近似方法。我们通过评估所获得的奖励和违反约束的次数来评估学习到的策略的有效性，并根据其对其他代理的可转移性来评估学习到的成本函数。我们的方法已被证明在各种任务和环境中优于最先进的方法。

    When deploying artificial agents in real-world environments where they interact with humans, it is crucial that their behavior is aligned with the values, social norms or other requirements of that environment. However, many environments have implicit constraints that are difficult to specify and transfer to a learning agent. To address this challenge, we propose a novel method that utilizes the principle of maximum causal entropy to learn constraints and an optimal policy that adheres to these constraints, using demonstrations of agents that abide by the constraints. We prove convergence in a tabular setting and provide an approximation which scales to complex environments. We evaluate the effectiveness of the learned policy by assessing the reward received and the number of constraint violations, and we evaluate the learned cost function based on its transferability to other agents. Our method has been shown to outperform state-of-the-art approaches across a variety of tasks and envi
    
[^27]: 可解释聚类中深度减少的不可能性证明

    Impossibility of Depth Reduction in Explainable Clustering. (arXiv:2305.02850v1 [cs.LG])

    [http://arxiv.org/abs/2305.02850](http://arxiv.org/abs/2305.02850)

    可解释聚类中，决策树深度是无法减少的固有复杂度度量之一，减少深度会显著降低聚类质量。

    

    近年来，可解释聚类引起了许多关注。本论文在Euclidean平面中证明，对于可解释的k-means和k-median聚类问题，决策树的深度是不可避免的复杂度度量之一，无法减少而不显著降低聚类质量。我们证明了对于任何在Euclidean平面上的数据X，深度为k-1的决策树的k-means/k-median聚类代价与X的优化聚类代价相同，但是对于深度小于k-1的决策树，其聚类代价相对于最优聚类代价而言是不可接受的。我们还将结果扩展到了k-center目标。

    Over the last few years Explainable Clustering has gathered a lot of attention. Dasgupta et al. [ICML'20] initiated the study of explainable k-means and k-median clustering problems where the explanation is captured by a threshold decision tree which partitions the space at each node using axis parallel hyperplanes. Recently, Laber et al. [Pattern Recognition'23] made a case to consider the depth of the decision tree as an additional complexity measure of interest.  In this work, we prove that even when the input points are in the Euclidean plane, then any depth reduction in the explanation incurs unbounded loss in the k-means and k-median cost. Formally, we show that there exists a data set X in the Euclidean plane, for which there is a decision tree of depth k-1 whose k-means/k-median cost matches the optimal clustering cost of X, but every decision tree of depth less than k-1 has unbounded cost w.r.t. the optimal cost of clustering. We extend our results to the k-center objective as
    
[^28]: 用变分自编码器和注意力机制实现可解释的句子表示

    Interpretable Sentence Representation with Variational Autoencoders and Attention. (arXiv:2305.02810v1 [cs.CL])

    [http://arxiv.org/abs/2305.02810](http://arxiv.org/abs/2305.02810)

    本论文提出了使用VAEs和Transformers构建两种具有归纳偏差的模型，用于提高自然语言处理中表示学习技术的解释性和数据效率，能够将潜在表示中的信息分离为可理解的概念。实验结果表明这些模型提供了直观且可解释的表示形式，具有实用性。

    

    本论文旨在增强自然语言处理中最近一些表示学习技术的解释性，同时考虑到缺乏注释数据的情况下进行研究的方法。我们选择利用变分自编码器（VAEs），因为它们在将观察结果与隐藏的生成因素联系起来方面很有效，并且在数据效率学习和可解释表示学习方面也很有效。我们首先删除半监督VAEs运行方案中的不必要组件，使得它们更快速、更小、更易于设计。其次，我们使用VAEs和Transformer构建了两个具有归纳偏差的模型，将潜在表示中的信息分离成可理解的概念，而不需要注释数据。我们的实验证明，这些模型提供了直观且可解释的表示形式，对自然语言处理中的下游任务非常有用。

    In this thesis, we develop methods to enhance the interpretability of recent representation learning techniques in natural language processing (NLP) while accounting for the unavailability of annotated data. We choose to leverage Variational Autoencoders (VAEs) due to their efficiency in relating observations to latent generative factors and their effectiveness in data-efficient learning and interpretable representation learning. As a first contribution, we identify and remove unnecessary components in the functioning scheme of semi-supervised VAEs making them faster, smaller and easier to design. Our second and main contribution is to use VAEs and Transformers to build two models with inductive bias to separate information in latent representations into understandable concepts without annotated data. The first model, Attention-Driven VAE (ADVAE), is able to separately represent and control information about syntactic roles in sentences. The second model, QKVAE, uses separate latent va
    
[^29]: 在存在偏见的情况下，最大化子模函数用于推荐系统

    Maximizing Submodular Functions for Recommendation in the Presence of Biases. (arXiv:2305.02806v1 [cs.LG])

    [http://arxiv.org/abs/2305.02806](http://arxiv.org/abs/2305.02806)

    该论文研究了如何在存在偏见的情况下，通过最大化子模函数来优化推荐系统。先前研究指出，基于公平性约束的干预可以确保比例代表性，并在存在偏见时获得接近最优的效用。而本文则探讨了一组能够捕捉这种目的的子模函数。

    

    子集选择任务在推荐系统和搜索引擎中经常出现，要求选择一些最大化用户价值的物品子集。子集的价值往往呈现出递减的回报，因此，使用子模函数来建模。然而，在许多应用中，发现输入具有社会偏见，会降低输出子集的效用，因此需要干预以提高其效用。本文研究了一组子模函数的最大化，这些函数涵盖了上述应用中出现的函数。

    Subset selection tasks, arise in recommendation systems and search engines and ask to select a subset of items that maximize the value for the user. The values of subsets often display diminishing returns, and hence, submodular functions have been used to model them. If the inputs defining the submodular function are known, then existing algorithms can be used. In many applications, however, inputs have been observed to have social biases that reduce the utility of the output subset. Hence, interventions to improve the utility are desired. Prior works focus on maximizing linear functions -- a special case of submodular functions -- and show that fairness constraint-based interventions can not only ensure proportional representation but also achieve near-optimal utility in the presence of biases. We study the maximization of a family of submodular functions that capture functions arising in the aforementioned applications. Our first result is that, unlike linear functions, constraint-ba
    
[^30]: 张量空间中的基础张量PCA

    Tensor PCA from basis in tensor space. (arXiv:2305.02803v1 [math.NA])

    [http://arxiv.org/abs/2305.02803](http://arxiv.org/abs/2305.02803)

    本文提出了一种张量PCA的数学框架，通过自伴张量算子导出张量空间中的基础以解决以往方法的局限性，实验结果表明了该方法的有效性。

    

    本文提出了一种张量PCA的数学框架，该方法能够克服以前通过迭代求解优化问题来提取低维子空间的方法的局限性。该方法的核心是从实自伴张量算子中导出张量空间中的基础，从而将基础的导出问题转化为特征值问题。本文研究了三种不同情况的导出：i）从自伴张量算子中导出基础；ii）导出秩为1的基础；iii）从子空间中导出基础。特别是，证明了实自伴张量算子的特征值方程与标准矩阵特征值方程的等价性。针对所考虑的三种情况，采用了子空间方法来导出张量PCA。基于图像数据集的实验验证了所提出的数学框架。

    The aim of this paper is to present a mathematical framework for tensor PCA. The proposed approach is able to overcome the limitations of previous methods that extract a low dimensional subspace by iteratively solving an optimization problem. The core of the proposed approach is the derivation of a basis in tensor space from a real self-adjoint tensor operator, thus reducing the problem of deriving a basis to an eigenvalue problem. Three different cases have been studied to derive: i) a basis from a self-adjoint tensor operator; ii) a rank-1 basis; iii) a basis in a subspace. In particular, the equivalence between eigenvalue equation for a real self-adjoint tensor operator and standard matrix eigenvalue equation has been proven. For all the three cases considered, a subspace approach has been adopted to derive a tensor PCA. Experiments on image datasets validate the proposed mathematical framework.
    
[^31]: 面向半监督多标签学习的类别分布感知伪标记方法

    Class-Distribution-Aware Pseudo Labeling for Semi-Supervised Multi-Label Learning. (arXiv:2305.02795v1 [cs.LG])

    [http://arxiv.org/abs/2305.02795](http://arxiv.org/abs/2305.02795)

    本论文提出了一种面向半监督多标签学习的类别分布感知伪标记方法，能够在控制伪标签数目的情况下，更准确地逼近真实分布，从而实现更好的多标签分类性能。

    

    伪标记是一种利用未标记数据信息的流行且有效方法。然而，传统的基于实例的伪标记方法通常根据其预测概率为每个未标记实例分配一个伪标签。由于真实标签数目未知，这些方法在半监督多标签学习（SSMLL）场景下难以很好地推广，因为它们会面临引入假正标签或忽略真正标签的风险。本文提出了一种面向SSMLL问题的类别分布感知伪标记（CAP）方法，鼓励伪标签的类别分布接近真实分布。具体而言，我们设计了一个包括类别感知阈值的正则化学习框架来控制每个类别的伪标签数目。鉴于标记和未标记的示例是根据同一分布采样的，我们通过利用标记示例的类别分布确定阈值并随着模型参数一起更新。在几个基准数据集上的实验结果表明，我们提出的方法在多标签分类性能方面显著优于现有的最先进方法。

    Pseudo labeling is a popular and effective method to leverage the information of unlabeled data. Conventional instance-aware pseudo labeling methods often assign each unlabeled instance with a pseudo label based on its predicted probabilities. However, due to the unknown number of true labels, these methods cannot generalize well to semi-supervised multi-label learning (SSMLL) scenarios, since they would suffer from the risk of either introducing false positive labels or neglecting true positive ones. In this paper, we propose to solve the SSMLL problems by performing Class-distribution-Aware Pseudo labeling (CAP), which encourages the class distribution of pseudo labels to approximate the true one. Specifically, we design a regularized learning framework consisting of the class-aware thresholds to control the number of pseudo labels for each class. Given that the labeled and unlabeled examples are sampled according to the same distribution, we determine the thresholds by exploiting th
    
[^32]: BranchNorm: 鲁棒地扩展极深的Transformer

    BranchNorm: Robustly Scaling Extremely Deep Transformers. (arXiv:2305.02790v1 [cs.LG])

    [http://arxiv.org/abs/2305.02790](http://arxiv.org/abs/2305.02790)

    BranchNorm提出了一种新的方法，通过动态重新调整Transformer的非残差分支，理论上稳定了训练，并在随后的训练阶段中促进了更好的收敛。实验结果表明，BranchNorm在训练稳定性和收敛性能之间取得了更好的平衡。

    

    最近，DeepNorm将Transformer扩展到极深（即1000层），展示了深度扩展的潜力。为了稳定深度模型的训练，DeepNorm试图将模型更新约束为一个恒定值。尽管应用这种约束可以使模型在早期训练阶段受益，但可能导致整个训练过程中模型训练不足。在本文中，我们提出了BranchNorm，它根据训练期间动态重新调整Transformer的非残差分支。BranchNorm不仅在早期阶段理论上稳定了训练，而且在随后的训练阶段中促进了更好的收敛。多个翻译任务的实验结果表明，BranchNorm在训练稳定性和收敛性能之间取得了更好的平衡。

    Recently, DeepNorm scales Transformers into extremely deep (i.e., 1000 layers) and reveals the promising potential of deep scaling. To stabilize the training of deep models, DeepNorm (Wang et al., 2022) attempts to constrain the model update to a constant value. Although applying such a constraint can benefit the early stage of model training, it may lead to undertrained models during the whole training procedure. In this paper, we propose BranchNorm, which dynamically rescales the non-residual branch of Transformer in accordance with the training period. BranchNorm not only theoretically stabilizes the training with smooth gradient norms at the early stage, but also encourages better convergence in the subsequent training stage. Experiment results on multiple translation tasks demonstrate that BranchNorm achieves a better trade-off between training stability and converge performance.
    
[^33]: 动态网络表征的动量非负张量分解模型

    A Momentum-Incorporated Non-Negative Latent Factorization of Tensors Model for Dynamic Network Representation. (arXiv:2305.02782v1 [cs.LG])

    [http://arxiv.org/abs/2305.02782](http://arxiv.org/abs/2305.02782)

    本文提出一种基于动量SGD的非线性LFT模型（MNNL）用于从高维不完整张量中提取非负潜在因子，以提高大规模动态网络的表征准确性和收敛速度。

    

    大规模动态网络（LDN）由于其实体数量和大规模动态交互而成为许多大数据相关应用程序的数据源。它们可以被建模为包含有时间模式知识的高维不完整（HDI）张量。张量的潜在因子分解（LFT）模型可以有效地提取这些时间模式，可以使用随机梯度下降（SGD）求解器来建立。然而，基于SGD的LFT模型通常受到训练方案的限制，且收敛速度较慢。为解决这个问题，本文提出了一种基于动量SGD的非线性LFT模型（MNNL），该模型从HDI张量中提取非负潜在因子，使得训练无约束并与一般的训练方案兼容，同时提高了收敛精度和速度。两个LDN数据集上的实证研究表明，与现有模型相比，MNNL模型具有更高的预测准确性和收敛速度。

    A large-scale dynamic network (LDN) is a source of data in many big data-related applications due to their large number of entities and large-scale dynamic interactions. They can be modeled as a high-dimensional incomplete (HDI) tensor that contains a wealth of knowledge about time patterns. A Latent factorization of tensors (LFT) model efficiently extracts this time pattern, which can be established using stochastic gradient descent (SGD) solvers. However, LFT models based on SGD are often limited by training schemes and have poor tail convergence. To solve this problem, this paper proposes a novel nonlinear LFT model (MNNL) based on momentum-incorporated SGD, which extracts non-negative latent factors from HDI tensors to make training unconstrained and compatible with general training schemes, while improving convergence accuracy and speed. Empirical studies on two LDN datasets show that compared to existing models, the MNNL model has higher prediction accuracy and convergence speed.
    
[^34]: 可解释的区域描述符：基于超立方体的局部解释

    Interpretable Regional Descriptors: Hyperbox-Based Local Explanations. (arXiv:2305.02780v1 [stat.ML])

    [http://arxiv.org/abs/2305.02780](http://arxiv.org/abs/2305.02780)

    本文介绍了一种可解释的区域描述符，它是一种模型无关的局部解释方法，通过描述超立方体来预测特征值可更改但不影响预测结果，并提供"即使是"参数，揭示决策的特征和偏差。

    

    本文介绍了一种用于模型无关的局部解释的可解释的区域描述符（IRDs），它们是描述观测值特征值可更改而不影响其预测的超立方体。通过提供一组“即使是”参数（半事实的解释），它们证明了一个预测，并指出哪些特征影响了预测以及是否存在点偏差或不可信。一个具体的用例展示了它对于机器学习模型的构建者和决策受影响人员都是有价值的。我们将IRDs的搜索形式化为一个优化问题，并引入了一个计算IRDs的统一框架，包括期望、初始化技术和后处理方法。我们展示了如何将现有的超立方体方法适应到这个统一框架中。一项基准研究比较了基于多个质量指标的方法，并确定了两种改进IRDs的策略。

    This work introduces interpretable regional descriptors, or IRDs, for local, model-agnostic interpretations. IRDs are hyperboxes that describe how an observation's feature values can be changed without affecting its prediction. They justify a prediction by providing a set of "even if" arguments (semi-factual explanations), and they indicate which features affect a prediction and whether pointwise biases or implausibilities exist. A concrete use case shows that this is valuable for both machine learning modelers and persons subject to a decision. We formalize the search for IRDs as an optimization problem and introduce a unifying framework for computing IRDs that covers desiderata, initialization techniques, and a post-processing method. We show how existing hyperbox methods can be adapted to fit into this unified framework. A benchmark study compares the methods based on several quality measures and identifies two strategies to improve IRDs.
    
[^35]: 通过稀疏模型自适应实现高效个性化联邦学习

    Efficient Personalized Federated Learning via Sparse Model-Adaptation. (arXiv:2305.02776v1 [cs.LG])

    [http://arxiv.org/abs/2305.02776](http://arxiv.org/abs/2305.02776)

    提出了一种名为pFedGate的新方法，通过自适应和高效的方式学习稀疏的本地模型，使得客户端可以发挥其模型容量的全部潜力，从而提高个性化联邦学习的效率。

    

    联邦学习（FL）旨在为多个客户端培训机器学习模型，而不共享其私有数据。由于客户端本地数据分布的异构性，最近的研究探索了个性化FL，通过辅助全局模型学习并部署不同的局部模型。但是，客户端可能在计算和通信资源方面存在异质性。个性化模型的容量和效率受到最低资源客户端的限制，导致个性化FL的性能不佳和实用性有限。为克服这些挑战，我们提出了一种名为pFedGate的新方法，通过自适应和高效地学习稀疏的本地模型来实现高效个性化FL。通过轻量级可训练的门控层，pFedGate能够产生不同的稀疏模型，从而发挥客户端在模型容量方面的全部潜力，考虑到异构数据和计算能力的影响。

    Federated Learning (FL) aims to train machine learning models for multiple clients without sharing their own private data. Due to the heterogeneity of clients' local data distribution, recent studies explore the personalized FL that learns and deploys distinct local models with the help of auxiliary global models. However, the clients can be heterogeneous in terms of not only local data distribution, but also their computation and communication resources. The capacity and efficiency of personalized models are restricted by the lowest-resource clients, leading to sub-optimal performance and limited practicality of personalized FL. To overcome these challenges, we propose a novel approach named pFedGate for efficient personalized FL by adaptively and efficiently learning sparse local models. With a lightweight trainable gating layer, pFedGate enables clients to reach their full potential in model capacity by generating different sparse models accounting for both the heterogeneous data di
    
[^36]: VendorLink：一种NLP方法用于识别和链接暗网市场上的供应商被迁移和潜在别名

    VendorLink: An NLP approach for Identifying & Linking Vendor Migrants & Potential Aliases on Darknet Markets. (arXiv:2305.02763v1 [cs.CY])

    [http://arxiv.org/abs/2305.02763](http://arxiv.org/abs/2305.02763)

    本论文提出了一种名为VendorLink的NLP方法，能够有效地识别和链接暗网市场上的供应商被迁移和潜在别名，减少非法市场的匿名性，具有重要的实际意义。

    

    暗网上的匿名性使得供应商可以使用多个供应商别名或频繁迁移市场而不被发现。因此，在暗网上发现非法市场及其联系人是具有挑战性的。为了识别非法市场和供应商之间的关系，我们提出了VendorLink，这是一种基于NLP的方法，通过检查写作模式来验证、识别和链接七个公共暗网市场上的唯一供应商帐户。与现有文献不同，VendorLink利用有监督的预训练的优势来执行封闭集供应商验证、开放集供应商识别和低资源市场适应任务。通过VendorLink，我们在Alphabay-Dreams-Silk数据集中揭示了15个移民和71个潜在别名，在Valhalla-Berlusconi数据集中揭示了17个移民和3个潜在别名，在Traderoute-Agora数据集中揭示了75个移民和10个潜在别名。

    The anonymity on the Darknet allows vendors to stay undetected by using multiple vendor aliases or frequently migrating between markets. Consequently, illegal markets and their connections are challenging to uncover on the Darknet. To identify relationships between illegal markets and their vendors, we propose VendorLink, an NLP-based approach that examines writing patterns to verify, identify, and link unique vendor accounts across text advertisements (ads) on seven public Darknet markets. In contrast to existing literature, VendorLink utilizes the strength of supervised pre-training to perform closed-set vendor verification, open-set vendor identification, and low-resource market adaption tasks. Through VendorLink, we uncover (i) 15 migrants and 71 potential aliases in the Alphabay-Dreams-Silk dataset, (ii) 17 migrants and 3 potential aliases in the Valhalla-Berlusconi dataset, and (iii) 75 migrants and 10 potential aliases in the Traderoute-Agora dataset. Altogether, our approach ca
    
[^37]: 不充分标注下的多领域学习

    Multi-Domain Learning From Insufficient Annotations. (arXiv:2305.02757v1 [cs.LG])

    [http://arxiv.org/abs/2305.02757](http://arxiv.org/abs/2305.02757)

    提出了一种名为多领域对比学习（MDCL）的新方法，在原有方法的基础上，利用来自标记和未标记数据的语义和结构信息，解决了不充分注释的问题，并在实验中取得了优异的成果。

    

    多领域学习(MDL)指同时在来自不同领域的数据集上构建一个模型或一组模型。传统方法强调域共享信息的提取和域私有信息的保留，遵循共享-私有架构(SP模型)，这比单领域学习具有明显优势。然而，在每个领域中有限的已注释数据的可用性，严重阻碍了传统监督MDL方法在实际应用中的有效性。本文介绍了一种称为多领域对比学习(MDCL)的新方法，通过捕获来自标记和未标记数据的语义和结构信息，缓解了不充分注释的影响。具体而言，MDCL包括两个模块：域间语义对齐和域内对比。前者旨在将不同领域中相同语义类别的已标注实例在共享的隐空间中对齐，而后者旨在在每个领域内最大化分离来自不同类别的实例。我们在三个多领域学习任务上进行实验，包括图像分类、情感分析和假新闻检测，结果表明我们提出的MDCL方法在各种注释方案下优于现有的最先进MDL方法。

    Multi-domain learning (MDL) refers to simultaneously constructing a model or a set of models on datasets collected from different domains. Conventional approaches emphasize domain-shared information extraction and domain-private information preservation, following the shared-private framework (SP models), which offers significant advantages over single-domain learning. However, the limited availability of annotated data in each domain considerably hinders the effectiveness of conventional supervised MDL approaches in real-world applications. In this paper, we introduce a novel method called multi-domain contrastive learning (MDCL) to alleviate the impact of insufficient annotations by capturing both semantic and structural information from both labeled and unlabeled data.Specifically, MDCL comprises two modules: inter-domain semantic alignment and intra-domain contrast. The former aims to align annotated instances of the same semantic category from distinct domains within a shared hidd
    
[^38]: 通过因果世界模型实现可解释强化学习

    Explainable Reinforcement Learning via a Causal World Model. (arXiv:2305.02749v1 [cs.LG])

    [http://arxiv.org/abs/2305.02749](http://arxiv.org/abs/2305.02749)

    本文提出了一种新的可解释强化学习框架，通过学习因果世界模型来解释行动的长期影响以及教学习者如何影响环境变量并最终导致奖励。

    

    给强化学习提供解释是一项挑战，因为行动可能对未来产生长期影响。本文提出了一种新的可解释强化学习框架：通过学习一个因果世界模型而不预先知道环境的因果结构。该模型捕捉到动作的影响，使我们能够通过因果链来解释行动的长期影响，从而揭示出行动是如何影响环境变量并最终导致奖励的。与大多数解释性模型的低准确性不同，我们的模型保持高准确性的同时提高了解释性，使其适用于基于模型的学习。因此，我们证明了我们的因果模型可以成为解释性和学习之间的桥梁。

    Generating explanations for reinforcement learning (RL) is challenging as actions may produce long-term effects on the future. In this paper, we develop a novel framework for explainable RL by learning a causal world model without prior knowledge of the causal structure of the environment. The model captures the influence of actions, allowing us to interpret the long-term effects of actions through causal chains, which present how actions influence environmental variables and finally lead to rewards. Different from most explanatory models which suffer from low accuracy, our model remains accurate while improving explainability, making it applicable in model-based learning. As a result, we demonstrate that our causal model can serve as the bridge between explainability and learning.
    
[^39]: 公正联邦学习可以减少个性化需求吗？

    Can Fair Federated Learning reduce the need for Personalisation?. (arXiv:2305.02728v1 [cs.LG])

    [http://arxiv.org/abs/2305.02728](http://arxiv.org/abs/2305.02728)

    本文探索了联邦学习的公正性与个性化需求的关系，提出个性化公正联邦学习（PFL）算法，能够解决准确性差异以及在FL模型表现不佳时提供参与激励的问题。

    

    联邦学习（FL）使得在不共享数据的情况下在边缘客户端上训练ML模型成为可能。然而，联邦模型在本地数据上的性能会有所差异，这会造成对于没有从FL中受益的客户端参与的不利影响。公正FL通过关注损失更高的客户端来减少准确性差异，而个性化调整则在本地微调模型。在FL模型相对于本地训练模型表现较差时，个性化提供了参与激励。本文评估了两种公正FL算法作为个性化的起点。结果表明，基于公正FL的方法对于语言任务并没有产生相对性能优势，并且在图像任务中可能会使表现不佳的客户端数量增加一倍。相反，我们提出了个性化公正联邦学习（PFL），并展示了在语言和图像任务中减少准确性差异的同时，在FL模型相对于本地模型表现较差时提供了参与激励。

    Federated Learning (FL) enables training ML models on edge clients without sharing data. However, the federated model's performance on local data varies, disincentivising the participation of clients who benefit little from FL. Fair FL reduces accuracy disparity by focusing on clients with higher losses while personalisation locally fine-tunes the model. Personalisation provides a participation incentive when an FL model underperforms relative to one trained locally. For situations where the federated model provides a lower accuracy than a model trained entirely locally by a client, personalisation improves the accuracy of the pre-trained federated weights to be similar to or exceed those of the local client model. This paper evaluates two Fair FL (FFL) algorithms as starting points for personalisation. Our results show that FFL provides no benefit to relative performance in a language task and may double the number of underperforming clients for an image task. Instead, we propose Pers
    
[^40]: 使用可解释的提升算法建模环境和农业数据

    Using interpretable boosting algorithms for modeling environmental and agricultural data. (arXiv:2305.02699v1 [stat.ML])

    [http://arxiv.org/abs/2305.02699](http://arxiv.org/abs/2305.02699)

    本文介绍了如何使用可解释的提升算法来分析高维环境和农业数据。通过考虑组结构和使用两步提升方法，我们预测了农民在面对气候灾害时的财务脆弱性。重要的预测变量包括自然资产、灌溉类型和附近农场的作物损坏。交互作用也提高了预测能力。

    

    我们阐述了如何使用基于岭正则化广义线性模型的解释性提升算法来分析高维环境数据。我们以智利和突尼斯的农民在面对气候灾害时的财务脆弱性为例，使用环境、社会、人类和生物物理数据进行预测。我们展示了如何考虑组结构以及如何在高维数据集中找到交互作用，使用一种新的两步提升方法。所提出方法的优点和功效都得到了实证和讨论。结果表明，在两步提升中引入交互作用可以提高预测能力。在预测所有类型的脆弱性方面，最重要的变量是自然资产。其他重要变量包括灌溉类型、经济资产和附近农场作物损坏的存在。

    We describe how interpretable boosting algorithms based on ridge-regularized generalized linear models can be used to analyze high-dimensional environmental data. We illustrate this by using environmental, social, human and biophysical data to predict the financial vulnerability of farmers in Chile and Tunisia against climate hazards. We show how group structures can be considered and how interactions can be found in high-dimensional datasets using a novel 2-step boosting approach. The advantages and efficacy of the proposed method are shown and discussed. Results indicate that the presence of interaction effects only improves predictive power when included in two-step boosting. The most important variable in predicting all types of vulnerabilities are natural assets. Other important variables are the type of irrigation, economic assets and the presence of crop damage of near farms.
    
[^41]: 基于图神经网络的金属增材制造现场异常检测

    In-situ Anomaly Detection in Additive Manufacturing with Graph Neural Networks. (arXiv:2305.02695v1 [cs.CV])

    [http://arxiv.org/abs/2305.02695](http://arxiv.org/abs/2305.02695)

    本研究利用图神经网络训练模型进行金属增材制造现场异常检测，通过预测与新观察值之间差异计算异常得分，F1得分为0.821，为开发稳健的缺陷检测方法提供了重要工具。

    

    在金属增材制造中，将设计转化为高质量产品面临着罕见事件导致缺陷形成的挑战。然而，现场检测这些事件可降低检查成本、实现矫正措施，也是实现量身定制材料性能的第一步。本研究使用激光输入信息训练模型预测标准的激光熔化条件。然后通过预测与新观察值之间的差异计算异常得分。该模型在一个已知缺陷的数据集上评估，F1得分为0.821。研究表明，异常检测方法是开发稳健的缺陷检测方法的重要工具。

    Transforming a design into a high-quality product is a challenge in metal additive manufacturing due to rare events which can cause defects to form. Detecting these events in-situ could, however, reduce inspection costs, enable corrective action, and is the first step towards a future of tailored material properties. In this study a model is trained on laser input information to predict nominal laser melting conditions. An anomaly score is then calculated by taking the difference between the predictions and new observations. The model is evaluated on a dataset with known defects achieving an F1 score of 0.821. This study shows that anomaly detection methods are an important tool in developing robust defect detection methods.
    
[^42]: PGB：用于异构网络表示学习的PubMed图数据集基准

    PGB: A PubMed Graph Benchmark for Heterogeneous Network Representation Learning. (arXiv:2305.02691v1 [cs.LG])

    [http://arxiv.org/abs/2305.02691](http://arxiv.org/abs/2305.02691)

    介绍了一个基于PubMed数据库的PGB基准数据集，用于评估生物医学文献的异构图嵌入。该数据集包含丰富的元数据和来自不同数据集的21个系统性评价主题的评估任务。

    

    生物医学文献的数量快速增长，但是捕捉这些文章的文献信息的异质性仍然相对较少研究。尽管通过异构图神经网络的图挖掘研究已经成为研究热点，但它们是否捕捉到了PubMed数据库的异质性仍不清楚，而这是一个包含超过3300万篇文章的庞大数字资料库。我们介绍了PubMed Graph Benchmark（PGB），这是一个用于评估生物医学文献异构图嵌入的新的基准数据集。PGB是迄今为止最大的异构网络之一，包含3000万篇英文文章。该基准数据集包含丰富的元数据，包括摘要、作者、引用、MeSH术语、MeSH层次结构和其他一些信息。基准数据集包含来自3个不同数据集的21个系统性评价主题的评估任务。在PGB中，我们将与PubMed生物医学文章相关的元数据聚合成一致的。

    There has been a rapid growth in biomedical literature, yet capturing the heterogeneity of the bibliographic information of these articles remains relatively understudied. Although graph mining research via heterogeneous graph neural networks has taken center stage, it remains unclear whether these approaches capture the heterogeneity of the PubMed database, a vast digital repository containing over 33 million articles. We introduce PubMed Graph Benchmark (PGB), a new benchmark dataset for evaluating heterogeneous graph embeddings for biomedical literature. PGB is one of the largest heterogeneous networks to date and consists of 30 million English articles. The benchmark contains rich metadata including abstract, authors, citations, MeSH terms, MeSH hierarchy, and some other information. The benchmark contains an evaluation task of 21 systematic reviews topics from 3 different datasets. In PGB, we aggregate the metadata associated with the biomedical articles from PubMed into a unified
    
[^43]: 深度宽松弛神经网络的统计优化性

    Statistical Optimality of Deep Wide Neural Networks. (arXiv:2305.02657v1 [stat.ML])

    [http://arxiv.org/abs/2305.02657](http://arxiv.org/abs/2305.02657)

    本文研究了深度宽松弛ReLU神经网络的泛化能力，证明适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中，但过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。

    

    本文研究了定义在有界域$\mathcal X \subset \mathbb R^{d}$上的深度宽松弛ReLU神经网络的泛化能力。首先证明了神经网络的泛化能力可以被相应的深度神经切向核回归所完全描绘。然后，我们研究了深度神经切向核的谱特性，并证明了深度神经切向核在$\mathcal{X}$上为正定，其特征值衰减率为$(d+1)/d$。由于核回归中已经建立的理论，我们得出结论，适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中。最后，我们证明过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。

    In this paper, we consider the generalization ability of deep wide feedforward ReLU neural networks defined on a bounded domain $\mathcal X \subset \mathbb R^{d}$. We first demonstrate that the generalization ability of the neural network can be fully characterized by that of the corresponding deep neural tangent kernel (NTK) regression. We then investigate on the spectral properties of the deep NTK and show that the deep NTK is positive definite on $\mathcal{X}$ and its eigenvalue decay rate is $(d+1)/d$. Thanks to the well established theories in kernel regression, we then conclude that multilayer wide neural networks trained by gradient descent with proper early stopping achieve the minimax rate, provided that the regression function lies in the reproducing kernel Hilbert space (RKHS) associated with the corresponding NTK. Finally, we illustrate that the overfitted multilayer wide neural networks can not generalize well on $\mathbb S^{d}$.
    
[^44]: Blahut和Arimoto的主题变体

    Variations on a Theme by Blahut and Arimoto. (arXiv:2305.02650v1 [cs.IT])

    [http://arxiv.org/abs/2305.02650](http://arxiv.org/abs/2305.02650)

    本文提出了BA算法的一种新的修改，通过让乘数在每次迭代中通过一维求根来更新，这使得算法能够直接计算所需失真的RD函数，而无需像原始算法一样探索整个RD曲线。

    

    Blahut-Arimoto（BA）算法在计算速率失真（RD）函数方面起着基础性作用，该算法通过交替最小化带有固定乘数的Lagrangian具有理想的单调收敛属性。在本文中，我们提出了BA算法的新颖修改，使乘数每次迭代通过相对于单调单变量函数的一维求根步骤更新，这可以通过牛顿法有效实现。这允许以灵活和高效的方式更新乘数，克服了原始BA算法的一个主要缺点，其中乘数在整个迭代过程中都是固定的。因此，修改后的算法能够直接计算所需失真的RD函数，而不像原始BA算法一样探索整个RD曲线。理论分析表明，修改后的算法仍会收敛到RD函数。

    The Blahut-Arimoto (BA) algorithm has played a fundamental role in the numerical computation of rate-distortion (RD) functions. This algorithm possesses a desirable monotonic convergence property by alternatively minimizing its Lagrangian with a fixed multiplier. In this paper, we propose a novel modification of the BA algorithm, letting the multiplier be updated in each iteration via a one-dimensional root-finding step with respect to a monotonic univariate function, which can be efficiently implemented by Newton's method. This allows the multiplier to be updated in a flexible and efficient manner, overcoming a major drawback of the original BA algorithm wherein the multiplier is fixed throughout iterations. Consequently, the modified algorithm is capable of directly computing the RD function for a given target distortion, without exploring the entire RD curve as in the original BA algorithm. A theoretical analysis shows that the modified algorithm still converges to the RD function a
    
[^45]: 学习在存在隐性混淆因素的情况下从不确定数据中恢复因果关系

    Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders. (arXiv:2305.02640v1 [cs.LG])

    [http://arxiv.org/abs/2305.02640](http://arxiv.org/abs/2305.02640)

    本文提出了因果强度变分模型，解决了从不确定数据中恢复因果关系存在的低样本利用率和分布假设无能力的问题，同时考虑了潜在混淆因素。

    

    在具有潜在变量的因果发现中，我们定义了两个数据范式：确定数据：具有观察节点单值的单个骨架结构，和不确定数据：具有观察节点多值的一组多骨架结构。多个骨架引入低样本利用率，多个值引入了分布假设的无能力，这两者导致从不确定数据中恢复因果关系至今仍然未被充分探索。我们设计了因果强度变分模型来解决这两个问题。具体地，我们利用因果强度而不是独立噪声作为潜变量来调节证据下界。通过这种设计思想，不同骨架的因果强度被看作是一个分布，并可以表示为单值因果图矩阵。此外，考虑到潜在混淆因素，我们将因果图G分解为两个相关子图O和C。O包含观察节点之间的纯关系，而C表示混淆因素。

    In Causal Discovery with latent variables, We define two data paradigms: definite data: a single-skeleton structure with observed nodes single-value, and indefinite data: a set of multi-skeleton structures with observed nodes multi-value. Multi,skeletons induce low sample utilization and multi values induce incapability of the distribution assumption, both leading that recovering causal relations from indefinite data is, as of yet, largely unexplored. We design the causal strength variational model to settle down these two problems. Specifically, we leverage the causal strength instead of independent noise as latent variable to mediate evidence lower bound. By this design ethos, The causal strength of different skeletons is regarded as a distribution and can be expressed as a single-valued causal graph matrix. Moreover, considering the latent confounders, we disentangle the causal graph G into two relatisubgraphs O and C. O contains pure relations between observed nodes, while C repres
    
[^46]: 符合语言模型的核心抽样

    Conformal Nucleus Sampling. (arXiv:2305.02633v1 [cs.CL])

    [http://arxiv.org/abs/2305.02633](http://arxiv.org/abs/2305.02633)

    本研究探讨了符合语言模型的核心抽样，并采用符合性预测进行校准。结果表明，OPT模型过于自信，并且校准显示出中度的逆比例缩放与模型大小。

    

    语言模型生成文本的过程是基于依次抽样下一个单词。基于核心（top-p）抽样的解码过程会从最小可能的单词集中选择，这些单词的累计概率超过概率p。在本研究中，我们评估了在各种语言环境下，top-p集是否真正与其概率含义对齐。我们采用了符合性预测，这是一种校准程序，根据所需的置信水平，专注于构建最小预测集，以校准参数p作为下一个单词分布熵的函数。我们发现OPT模型过于自信，校准显示出中度的逆比例缩放与模型大小。

    Language models generate text based on successively sampling the next word. A decoding procedure based on nucleus (top-$p$) sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability $p$. In this work, we assess whether a top-$p$ set is indeed aligned with its probabilistic meaning in various linguistic contexts. We employ conformal prediction, a calibration procedure that focuses on the construction of minimal prediction sets according to a desired confidence level, to calibrate the parameter $p$ as a function of the entropy of the next word distribution. We find that OPT models are overconfident, and that calibration shows a moderate inverse scaling with model size.
    
[^47]: 一种社交学习代理中语言出现与分析的框架

    A framework for the emergence and analysis of language in social learning agents. (arXiv:2305.02632v1 [cs.CL])

    [http://arxiv.org/abs/2305.02632](http://arxiv.org/abs/2305.02632)

    本研究提出了一个模拟语言特征的通信协议，用于分析个体和共享抽象的形成及其对任务表现的影响。通过优化信息内容以最大化学生奖励改善了信息编码，提高了学习表现。

    

    人工神经网络（ANNs）越来越被用作研究模型，但它们的普适性和表征不变性仍存在问题。生物神经网络在社交约束下演化形成可传达的表征，展示了泛化能力。本研究提出了一种合作代理之间的通信协议，用于分析个体和共享抽象的形成及其对任务表现的影响。该通信协议旨在通过低维表示编码高维信息，模拟语言特征。使用网格世界迷宫和强化学习，教师ANNs向学生ANN传递压缩消息，以便更好地完成任务。通过这种方式，学生实现了更高的目标发现率，并在不同的任务世界中推广了目标位置。进一步优化信息内容以最大化学生奖励改善了信息编码，表明精确的表征可以在通信协议中得到实现。

    Artificial neural networks (ANNs) are increasingly used as research models, but questions remain about their generalizability and representational invariance. Biological neural networks under social constraints evolved to enable communicable representations, demonstrating generalization capabilities. This study proposes a communication protocol between cooperative agents to analyze the formation of individual and shared abstractions and their impact on task performance. This communication protocol aims to mimic language features by encoding high-dimensional information through low-dimensional representation. Using grid-world mazes and reinforcement learning, teacher ANNs pass a compressed message to a student ANN for better task completion. Through this, the student achieves a higher goal-finding rate and generalizes the goal location across task worlds. Further optimizing message content to maximize student reward improves information encoding, suggesting that an accurate representati
    
[^48]: 将心理测量学和计算视角整合于情感计算中的偏见和公平性：自动视频面试的案例研究

    Integrating Psychometrics and Computing Perspectives on Bias and Fairness in Affective Computing: A Case Study of Automated Video Interviews. (arXiv:2305.02629v1 [cs.LG])

    [http://arxiv.org/abs/2305.02629](http://arxiv.org/abs/2305.02629)

    本研究基于心理测量学提供了情感计算中偏见和公平性的一个例子。我们讨论了在美国法律背景下衡量公平性和偏见的各种方法和指标，并在自动视频面试中从多模态数据中测量了一些类型的偏见和公平性。我们鼓励情感计算研究人员和从业者将偏见和公平性纳入研究过程和产品中，并考虑他们在促进公平合理系统方面的作用、代理和责任。

    

    本文提供了一个基于心理测量学的对偏见和公平性在情感计算的机器学习流程中的应用的论述。我们扩展了人际交流框架，阐明了如何确定在推断人类情感和其他心理结构时可能出现的偏见来源。讨论了衡量公平性和偏见的各种方法和指标，以及它们在美国法律背景下的相关影响。我们通过一个案例研究来说明如何在自动视频面试中从多模态数据中测量一些类型的偏见和公平性，以推断应聘者的人格和可雇佣性。我们鼓励情感计算研究人员和从业者将偏见和公平性纳入研究过程和产品中，并考虑他们在促进公平合理系统方面的作用、代理和责任。

    We provide a psychometric-grounded exposition of bias and fairness as applied to a typical machine learning pipeline for affective computing. We expand on an interpersonal communication framework to elucidate how to identify sources of bias that may arise in the process of inferring human emotions and other psychological constructs from observed behavior. Various methods and metrics for measuring fairness and bias are discussed along with pertinent implications within the United States legal context. We illustrate how to measure some types of bias and fairness in a case study involving automatic personality and hireability inference from multimodal data collected in video interviews for mock job applications. We encourage affective computing researchers and practitioners to encapsulate bias and fairness in their research processes and products and to consider their role, agency, and responsibility in promoting equitable and just systems.
    
[^49]: 基于条件生成对抗网络的临界热流诊断

    Critical heat flux diagnosis using conditional generative adversarial networks. (arXiv:2305.02622v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2305.02622](http://arxiv.org/abs/2305.02622)

    本研究基于cGANs提出了一种用于在临界热流时重构沸腾系统热数据的图像到图像翻译方法，有望成为可靠的非侵入式临界热流诊断方法。

    

    临界热流 (CHF) 是高热流量热力系统中沸腾换热过程的关键安全界限。鉴定CHF对于防止设备损坏和确保系统整体安全至关重要，但其具有复杂性，难以进行。为了深入了解这一复杂现象，各种方法学已被开发出来，但是高分辨率数据的获取受到了巨大的资源消耗限制。本研究提出了一种基于条件生成对抗网络 (cGANs) 的数据驱动图像到图像翻译方法，用于在CHF时重构沸腾系统的热数据。监督学习过程依赖于成对图像，其中包括从流沸腾实验中获得的全反射可视化和红外热测温测量。我们提出的方法不仅有可能提供将相界面动力学和热扰动联系起来的证据，还可能是一种可靠的非侵入式临界热流诊断方法。

    The critical heat flux (CHF) is an essential safety boundary in boiling heat transfer processes employed in high heat flux thermal-hydraulic systems. Identifying CHF is vital for preventing equipment damage and ensuring overall system safety, yet it is challenging due to the complexity of the phenomena. For an in-depth understanding of the complicated phenomena, various methodologies have been devised, but the acquisition of high-resolution data is limited by the substantial resource consumption required. This study presents a data-driven, image-to-image translation method for reconstructing thermal data of a boiling system at CHF using conditional generative adversarial networks (cGANs). The supervised learning process relies on paired images, which include total reflection visualizations and infrared thermometry measurements obtained from flow boiling experiments. Our proposed approach has the potential to not only provide evidence connecting phase interface dynamics with thermal dis
    
[^50]: 基于半监督学习的高维贝叶斯优化及优化无标签数据采样

    High-dimensional Bayesian Optimization via Semi-supervised Learning with Optimized Unlabeled Data Sampling. (arXiv:2305.02614v1 [cs.LG])

    [http://arxiv.org/abs/2305.02614](http://arxiv.org/abs/2305.02614)

    本文提出基于半监督学习的高维贝叶斯优化方法，利用特定的未标记数据采样、参数化采样分布的优化及动态选择无标记数据等策略，解决了高维贝叶斯优化难以处理的问题。

    

    贝叶斯优化（BO）是一种寻找黑箱函数全局最优解的强大工具。虽然黑箱函数的评估成本往往很高，但减少昂贵标记数据的使用是理想的。本文首次提出了一种教师-学生模型，利用半监督学习在BO环境下利用大量未标记的数据。其中，关键在于选择验证和未标记数据以提高BO的表现。为了优化无标签数据的采样，我们采用黑箱参数化采样分布，将其优化为所采用双层优化框架的一部分。更进一步，通过从动态适应的极值分布中选择未标签数据，我们证明了BO的性能可以进一步提高。我们的BO方法在学习后的低维潜在空间中运行，使其可扩展到高维问题。

    Bayesian optimization (BO) is a powerful tool for seeking the global optimum of black-box functions. While evaluations of the black-box functions can be highly costly, it is desirable to reduce the use of expensive labeled data. For the first time, we introduce a teacher-student model to exploit semi-supervised learning that can make use of large amounts of unlabelled data under the context of BO. Importantly, we show that the selection of the validation and unlabeled data is key to the performance of BO. To optimize the sampling of unlabeled data, we employ a black-box parameterized sampling distribution optimized as part of the employed bi-level optimization framework. Taking one step further, we demonstrate that the performance of BO can be further improved by selecting unlabeled data from a dynamically fitted extreme value distribution. Our BO method operates in a learned latent space with reduced dimensionality, making it scalable to high-dimensional problems. The proposed approac
    
[^51]: IMAP: 内在驱动的对抗策略

    IMAP: Intrinsically Motivated Adversarial Policy. (arXiv:2305.02605v1 [cs.LG])

    [http://arxiv.org/abs/2305.02605](http://arxiv.org/abs/2305.02605)

    IMAP是一个内在驱动的对抗策略，无需受害者策略的任何知识，能够高效地进行黑盒规避攻击，并且在单一和多智能体环境中优于目前最先进的方法。

    

    强化学习（RL）代理在部署过程中容易受到规避攻击的影响。在单智能体环境中，攻击者可以对策略或值网络的输入或输出注入无法察觉的扰动；在多智能体环境中，攻击者可以通过控制对手间接影响受害者的观察。 对抗性策略为解决此类攻击提供了一种有前途的解决方案。然而，目前的方法要么需要受害者政策的完美或部分知识，要么由于任务相关奖励的稀疏性而导致样本效率低下。为克服这些局限性，我们提出了内在驱动的对抗政策（IMAP），用于单智能体和多智能体环境中高效的黑盒规避攻击，而不需任何关于受害者策略的知识。 IMAP利用基于状态覆盖率，策略覆盖率，风险和政策分歧的四个内在目标，以鼓励探索并发现更强的攻击技能。我们还描述了一种处理多个具有不同实力的对手的可推广算法。我们的实验表明，IMAP在单智能体和多智能体环境中均优于最先进的方法，包括两个Atari游戏，一个机器人运动任务和一个多智能体游戏。

    Reinforcement learning (RL) agents are known to be vulnerable to evasion attacks during deployment. In single-agent environments, attackers can inject imperceptible perturbations on the policy or value network's inputs or outputs; in multi-agent environments, attackers can control an adversarial opponent to indirectly influence the victim's observation. Adversarial policies offer a promising solution to craft such attacks. Still, current approaches either require perfect or partial knowledge of the victim policy or suffer from sample inefficiency due to the sparsity of task-related rewards. To overcome these limitations, we propose the Intrinsically Motivated Adversarial Policy (IMAP) for efficient black-box evasion attacks in single- and multi-agent environments without any knowledge of the victim policy. IMAP uses four intrinsic objectives based on state coverage, policy coverage, risk, and policy divergence to encourage exploration and discover stronger attacking skills. We also des
    
[^52]: 关于LayerNorm在Transformers的Attention中表达能力的作用

    On the Expressivity Role of LayerNorm in Transformers' Attention. (arXiv:2305.02582v1 [cs.LG])

    [http://arxiv.org/abs/2305.02582](http://arxiv.org/abs/2305.02582)

    本文揭示了LayerNorm在Transformers的Attention层中有着至关重要的作用，通过对输入向量进行投影并对所有向量进行缩放，LayerNorm可以帮助注意力机制更好地处理输入。

    

    Layer Normalization（LayerNorm）是所有基于Transformer的模型中都具有的组件。在本文中，我们展示LayerNorm对随后的多头注意力层的表达能力至关重要，这与通常认为LayerNorm仅在前向传播期间归一化激活值和在反向传播期间归一化梯度的共识不同。我们考虑了LayerNorm的几何解释，并认为它由两个组成部分组成：（a）将输入向量投影到正交于$\left[1,1,...,1\right]$向量的$d-1$空间，并且（b）将所有向量缩放到相同的$\sqrt{d}$范数。我们展示了每个组件对随后的Transformers中的注意力层都很重要：（a）投影允许注意机制创建一个关注所有键等量的注意查询，从而减轻了注意机制学习此操作的需要；（b）缩放使每个键都有可能接收到最高的注意力分配权重。

    Layer Normalization (LayerNorm) is an inherent component in all Transformer-based models. In this paper, we show that LayerNorm is crucial to the expressivity of the multi-head attention layer that follows it. This is in contrast to the common belief that LayerNorm's only role is to normalize the activations during the forward pass, and their gradients during the backward pass. We consider a geometric interpretation of LayerNorm and show that it consists of two components: (a) projection of the input vectors to a $d-1$ space that is orthogonal to the $\left[1,1,...,1\right]$ vector, and (b) scaling of all vectors to the same norm of $\sqrt{d}$. We show that each of these components is important for the attention layer that follows it in Transformers: (a) projection allows the attention mechanism to create an attention query that attends to all keys equally, offloading the need to learn this operation by the attention; and (b) scaling allows each key to potentially receive the highest a
    
[^53]: 拉普拉斯正则化分层模型中的联合图学习和模型拟合

    Joint Graph Learning and Model Fitting in Laplacian Regularized Stratified Models. (arXiv:2305.02573v1 [stat.ML])

    [http://arxiv.org/abs/2305.02573](http://arxiv.org/abs/2305.02573)

    本文提出了一种联合图学习和模型拟合的通用方法，适用于广泛的LRSM问题，并在合成和真实世界数据集上实现了最先进的性能。

    

    拉普拉斯正则化分层模型（LRSM）是利用子问题的显式或隐式网络结构，由分类特征称为层（例如年龄、区域、时间、预测时间、等），并从相邻层中获取数据以增强每个子问题的参数学习。它们已广泛应用于机器学习和信号处理问题，包括但不限于时间序列预测，表示学习，图聚类，最大间隔分类和一般少量样本学习。然而，现有的LRSM研究要么假设已知图形，要么仅限于特定应用。在本文中，我们首先展示了LRSM中图权重的重要性和敏感性，并证明了当节点之间参数比例和样本量不平衡时，敏感性可能会任意增大。然后，我们提出了一种通用方法，在拟合模型的同时联合学习图，适用于广泛的LRSM问题。具体而言，我们制定了联合图学习和模型拟合问题，并通过交替学习图（通过稀疏逆协方差估计）和拟合模型（通过近端梯度下降）来解决它。我们的方法不仅在合成和真实世界数据集上实现了最先进的性能，还揭示了有关问题的有趣图案和结构。

    Laplacian regularized stratified models (LRSM) are models that utilize the explicit or implicit network structure of the sub-problems as defined by the categorical features called strata (e.g., age, region, time, forecast horizon, etc.), and draw upon data from neighboring strata to enhance the parameter learning of each sub-problem. They have been widely applied in machine learning and signal processing problems, including but not limited to time series forecasting, representation learning, graph clustering, max-margin classification, and general few-shot learning. Nevertheless, existing works on LRSM have either assumed a known graph or are restricted to specific applications. In this paper, we start by showing the importance and sensitivity of graph weights in LRSM, and provably show that the sensitivity can be arbitrarily large when the parameter scales and sample sizes are heavily imbalanced across nodes. We then propose a generic approach to jointly learn the graph while fitting 
    
[^54]: 可扩展编码中的条件编码和残差编码方法，适用于人类和机器

    Conditional and Residual Methods in Scalable Coding for Humans and Machines. (arXiv:2305.02562v1 [eess.IV])

    [http://arxiv.org/abs/2305.02562](http://arxiv.org/abs/2305.02562)

    该论文提出了适用于人类和机器的可扩展编码中的条件编码和残差编码方法，并应用于图像重建，取得了类似的性能表现。

    

    我们提出了适用于人类和机器的可扩展编码中的条件编码和残差编码方法。我们的重点是利用计算机视觉任务中可用的信息来优化重构任务的速率失真性能。我们包括了两种方法的信息分析，提供基准，并提出了一种熵模型，适用于条件编码，具备增加的建模能力和与之前的工作类似的可操作性。我们将这些方法应用于图像重建，其中一种实验使用在Cityscapes数据集上创建的语义分割表示，另一种使用在COCO数据集上创建的目标检测表示。在两个实验中，我们获得了条件和残差方法之间的类似性能，得到的速率失真曲线包含在我们的基线内。

    We present methods for conditional and residual coding in the context of scalable coding for humans and machines. Our focus is on optimizing the rate-distortion performance of the reconstruction task using the information available in the computer vision task. We include an information analysis of both approaches to provide baselines and also propose an entropy model suitable for conditional coding with increased modelling capacity and similar tractability as previous work. We apply these methods to image reconstruction, using, in one instance, representations created for semantic segmentation on the Cityscapes dataset, and in another instance, representations created for object detection on the COCO dataset. In both experiments, we obtain similar performance between the conditional and residual methods, with the resulting rate-distortion curves contained within our baselines.
    
[^55]: ChatGPT和Bard是否应该与其数据提供者分享收益？AI时代的新商业模式

    Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era. (arXiv:2305.02555v1 [cs.LG])

    [http://arxiv.org/abs/2305.02555](http://arxiv.org/abs/2305.02555)

    ChatGPT和Bard等AI工具需要持续大量且高质量的数据来提高其性能，但现行的版权法则限制了它们对各种数据的获取。与数据提供者分享收益将有助于将AI工具与大多数版权数据拥有者之间的敌对关系转变为合作关系，使AI生态系统更健康。

    

    随着ChatGPT等各种人工智能工具越来越受欢迎，我们正进入真正的AI时代。我们可以预见，卓越的AI工具很快将获得可观的利润。一个关键问题出现了：除了传统的利益相关者和股东，AI工具是否应该与它们的训练数据提供者分享收益？答案是肯定的。大型AI工具，例如大型语言模型，始终需要更多、更高质量的数据来不断改进，但当前的版权法限制了它们对各种类型数据的获取。在AI工具和数据提供者之间分享收益可以将当前敌对的零和游戏关系转变为一种合作和互利的关系，而这种关系对于促进AI工具、用户和数据提供者之间的良性循环发展、推动AI技术并建立健康的AI生态系统是必要的。然而，当前的收益分享商业模式……

    With various AI tools such as ChatGPT becoming increasingly popular, we are entering a true AI era. We can foresee that exceptional AI tools will soon reap considerable profits. A crucial question arise: should AI tools share revenue with their training data providers in additional to traditional stakeholders and shareholders? The answer is Yes. Large AI tools, such as large language models, always require more and better quality data to continuously improve, but current copyright laws limit their access to various types of data. Sharing revenue between AI tools and their data providers could transform the current hostile zero-sum game relationship between AI tools and a majority of copyrighted data owners into a collaborative and mutually beneficial one, which is necessary to facilitate the development of a virtuous cycle among AI tools, their users and data providers that drives forward AI technology and builds a healthy AI ecosystem. However, current revenue-sharing business models 
    
[^56]: FormNetV2：用于表格文档信息提取的多模态图形对比学习

    FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction. (arXiv:2305.02549v1 [cs.CL])

    [http://arxiv.org/abs/2305.02549](http://arxiv.org/abs/2305.02549)

    该论文提出了一种用于表格文档信息提取的多模态图形对比学习策略（FormNetV2），该方法能够统一所有模态的自监督预训练到一个损失中，并在多个基准测试中取得了最佳表现。

    

    自监督预训练技术的出现导致了多模态学习在表格文档理解中的激增。然而，现有的扩展掩码语言建模到其他模态的方法需要仔细的多任务调整、复杂的重构目标设计或额外的预训练数据。在FormNetV2中，我们引入了一种集中的多模态图对比学习策略，以统一所有模态的自监督预训练到一个损失中。图对比目标最大化多模态表示的一致性，为所有模态提供自然的相互作用，而不需要特殊的定制。此外，我们在连接图边缘的一对标记的边框内提取图像特征，捕捉更有针对性的视觉线索，而无需加载经过复杂和单独预训练的图像嵌入器。FormNetV2在FUNSD、CORD、SROIE和Payment基准测试中确立了最新的最佳表现水平。

    The recent advent of self-supervised pre-training techniques has led to a surge in the use of multimodal learning in form document understanding. However, existing approaches that extend the mask language modeling to other modalities require careful multi-task tuning, complex reconstruction target designs, or additional pre-training data. In FormNetV2, we introduce a centralized multimodal graph contrastive learning strategy to unify self-supervised pre-training for all modalities in one loss. The graph contrastive objective maximizes the agreement of multimodal representations, providing a natural interplay for all modalities without special customization. In addition, we extract image features within the bounding box that joins a pair of tokens connected by a graph edge, capturing more targeted visual cues without loading a sophisticated and separately pre-trained image embedder. FormNetV2 establishes new state-of-the-art performance on FUNSD, CORD, SROIE and Payment benchmarks with 
    
[^57]: 异常值鲁棒主成分分析的近似线性时间和流式算法

    Nearly-Linear Time and Streaming Algorithms for Outlier-Robust PCA. (arXiv:2305.02544v1 [cs.LG])

    [http://arxiv.org/abs/2305.02544](http://arxiv.org/abs/2305.02544)

    该研究开发出了一种近似线性时间的鲁棒PCA算法，具有接近最优的误差保证，并且还开发了一种单遍流式PCA算法，具有几乎线性的内存使用。

    

    我们研究主成分分析，其中给定来自分布的$\mathbb{R}^d$的数据集，任务是找到一个单位向量$v$，在沿$v$投影后，近似地最大化分布的方差。尽管是一个经典的任务，但如果数据包含即使是少量的异常值，标准估计器也会严重失败，这激发了鲁棒主成分分析的问题。最近的工作已经开发出计算效率较高的鲁棒PCA算法，但要么需要超线性时间，要么具有次优的误差保证。我们的主要贡献是开发出一种近似线性时间的鲁棒PCA算法，并具有接近最优的误差保证。我们还开发了一种单遍流式PCA算法，其内存使用几乎与维数成线性比。

    We study principal component analysis (PCA), where given a dataset in $\mathbb{R}^d$ from a distribution, the task is to find a unit vector $v$ that approximately maximizes the variance of the distribution after being projected along $v$. Despite being a classical task, standard estimators fail drastically if the data contains even a small fraction of outliers, motivating the problem of robust PCA. Recent work has developed computationally-efficient algorithms for robust PCA that either take super-linear time or have sub-optimal error guarantees. Our main contribution is to develop a nearly-linear time algorithm for robust PCA with near-optimal error guarantees. We also develop a single-pass streaming algorithm for robust PCA with memory usage nearly-linear in the dimension.
    
[^58]: 修正干扰问题：以抖音为例的案例研究

    Correcting for Interference in Experiments: A Case Study at Douyin. (arXiv:2305.02542v1 [stat.ME])

    [http://arxiv.org/abs/2305.02542](http://arxiv.org/abs/2305.02542)

    该研究提出了一种新型的Monte-Carlo估计器，它能够修正在抖音双向内容市场平台上实验时的干扰问题，并在现场实验中将实验吞吐量提高了一倍。

    

    干扰是在双向内容市场平台上进行实验时的普遍问题，例如中国版的TikTok——抖音。在许多情况下，创作者是实验的自然单位，但是创作者通过争夺观众有限的时间和注意力互相干扰。目前实践中使用的“朴素”估计器简单地忽略了干扰，但这样做会产生治疗效果的误差。我们将这样的实验推断问题形式化为政策评估中的问题。虽然偏差很小，但形式上行为策略不够实用。我们引入了一种基于“Q值差异”的(DQ)技术的新型Monte-Carlo估计器，它达到了治疗效果在二阶的偏差，同时保持了样本效率。在理论方面，我们的贡献是发展了一个通用的政策评估泰勒展开理论，将DQ理论扩展到所有主要的MDP公式。在实践方面，我们的估计器被部署在抖音上，并在现场实验中将实验吞吐量提高了一倍。

    Interference is a ubiquitous problem in experiments conducted on two-sided content marketplaces, such as Douyin (China's analog of TikTok). In many cases, creators are the natural unit of experimentation, but creators interfere with each other through competition for viewers' limited time and attention. "Naive" estimators currently used in practice simply ignore the interference, but in doing so incur bias on the order of the treatment effect. We formalize the problem of inference in such experiments as one of policy evaluation. Off-policy estimators, while unbiased, are impractically high variance. We introduce a novel Monte-Carlo estimator, based on "Differences-in-Qs" (DQ) techniques, which achieves bias that is second-order in the treatment effect, while remaining sample-efficient to estimate. On the theoretical side, our contribution is to develop a generalized theory of Taylor expansions for policy evaluation, which extends DQ theory to all major MDP formulations. On the practica
    
[^59]: Cuttlefish: 无需调整超参数的低秩模型训练

    Cuttlefish: Low-rank Model Training without All The Tuning. (arXiv:2305.02538v1 [cs.LG])

    [http://arxiv.org/abs/2305.02538](http://arxiv.org/abs/2305.02538)

    Cuttlefish 是一种新的自动化低秩训练方法，可以有效地减少可训练参数的数量，而无需调整因式分解超参数即可实现加速，可生成比完全秩训练小多达 5.6 倍的模型。

    

    最近的研究表明，训练低秩神经网络可以有效减少可训练参数的总数，而不会影响预测准确率，从而实现端到端的加速。但是，低秩模型的训练需要调整多个因式分解超参数。在本文中，我们通过引入 Cuttlefish，一种自动化的低秩训练方法，消除了调整因式分解超参数的需要。Cuttlefish 利用了一种观察结果，即在几个 epoch 的完全秩训练后，每个层的稳定秩稳定在一个常数值。一旦所有层的稳定秩都收敛，Cuttlefish 就从完全秩训练切换到低秩训练，将每个因式分解的维数设置为其相应的稳定秩。我们的实验结果表明，使用 Cuttlefish 生成的模型比完全秩训练小多达 5.6 倍。

    Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacrificing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing Cuttlefish, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. Cuttlefish leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. Cuttlefish switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that Cuttlefish generates models up to 5.6 times smaller than full-rank 
    
[^60]: 《延迟、复合和部分匿名奖励的强化学习》

    Reinforcement Learning with Delayed, Composite, and Partially Anonymous Reward. (arXiv:2305.02527v1 [cs.LG])

    [http://arxiv.org/abs/2305.02527](http://arxiv.org/abs/2305.02527)

    本文提出了一种算法用于解决具有延迟、复合和部分匿名奖励反馈的无限时平均奖励马尔可夫决策过程(MDP)，并取得了较好的效果。

    

    我们研究了具有延迟、复合和部分匿名奖励反馈的无限时平均奖励马尔可夫决策过程(MDP)。奖励的延迟和复杂性意味着在给定状态下采取行动生成的奖励被分解为不同的组成部分，并在延迟的时间实例中被顺序实现。部分匿名属性意味着对于每个状态，学习者只观察到在该状态下采取不同行动产生的过去奖励组成部分的总和，但是在观察实例中实现。我们提出了一种名为$\mathrm{DUCRL2}$的算法，用于获得此设置的近似最优策略，并表明它实现了$\tilde{\mathcal{O}}\left(DS\sqrt{AT} + d (SA)^3\right)$ 的遗憾界，其中$S$和$A$分别是状态和动作空间的大小，$D$是MDP的直径，$d$是一个由最大奖励延迟限制的参数，$T$表示时间的长度。

    We investigate an infinite-horizon average reward Markov Decision Process (MDP) with delayed, composite, and partially anonymous reward feedback. The delay and compositeness of rewards mean that rewards generated as a result of taking an action at a given state are fragmented into different components, and they are sequentially realized at delayed time instances. The partial anonymity attribute implies that a learner, for each state, only observes the aggregate of past reward components generated as a result of different actions taken at that state, but realized at the observation instance. We propose an algorithm named $\mathrm{DUCRL2}$ to obtain a near-optimal policy for this setting and show that it achieves a regret bound of $\tilde{\mathcal{O}}\left(DS\sqrt{AT} + d (SA)^3\right)$ where $S$ and $A$ are the sizes of the state and action spaces, respectively, $D$ is the diameter of the MDP, $d$ is a parameter upper bounded by the maximum reward delay, and $T$ denotes the time horizon
    
[^61]: BitGNN：释放二进制图神经网络在GPU上的性能潜力

    BitGNN: Unleashing the Performance Potential of Binary Graph Neural Networks on GPUs. (arXiv:2305.02522v1 [cs.DC])

    [http://arxiv.org/abs/2305.02522](http://arxiv.org/abs/2305.02522)

    本文提出了一种从效率角度重新设计的二进制GNN推理后端算法，用于充分发挥GPU上位操作的特性，实验结果表明提出的算法比最先进的二进制GNN实现提高了8-22倍的性能，保持相同准确性。

    

    最近的研究表明，通过对张量进行二值化，二进制图神经网络（GNN）可以节省GNN计算的计算量。然而，先前的工作主要集中在算法设计或训练技术上，没有完全实现将性能潜力显现到加速器硬件上的方法。本文从效率的角度重新设计了二进制GNN推理后端，并提出了一系列抽象和技术，以最佳地映射二进制GNN及其计算，以适应GPU上的位操作的特性。在使用GCNs、GraphSAGE和GraphSAINT的真实图上，实验结果表明，所提出的技术在保持相同准确性的同时，比最先进的二进制GNN实现提高了8-22倍。BitGNN代码已公开发布。

    Recent studies have shown that Binary Graph Neural Networks (GNNs) are promising for saving computations of GNNs through binarized tensors. Prior work, however, mainly focused on algorithm designs or training techniques, leaving it open to how to materialize the performance potential on accelerator hardware fully. This work redesigns the binary GNN inference backend from the efficiency perspective. It fills the gap by proposing a series of abstractions and techniques to map binary GNNs and their computations best to fit the nature of bit manipulations on GPUs. Results on real-world graphs with GCNs, GraphSAGE, and GraphSAINT show that the proposed techniques outperform state-of-the-art binary GNN implementations by 8-22X with the same accuracy maintained. BitGNN code is publicly available.
    
[^62]: 元学习增强的基于分数的生成模型用于0.5T MRI的1.5T-like图像重建

    Meta-Learning Enabled Score-Based Generative Model for 1.5T-Like Image Reconstruction from 0.5T MRI. (arXiv:2305.02509v1 [eess.IV])

    [http://arxiv.org/abs/2305.02509](http://arxiv.org/abs/2305.02509)

    该论文提出了一种新颖的元学习方法，通过教师-学生机制实现从0.5T MRI重建1.5T-like图像。该方法能够解决从低场强到高场强MRI图像的映射问题。

    

    磁共振成像（MRI）在低场强下具有降低的信噪比（SNR），在从高场强MRI图像生成低场强MRI图像时，信号会受到破坏，因此重建低场强像高场强的图像是一个复杂的问题。此外，获取成对的低场强和高场强MR图像通常是不现实的。我们理论上发现，这些挑战的组合使得直接学习从低场强MR图像到高场强MR图像的映射的常规深度学习方法不适用。为了克服这些挑战，我们引入了一种新颖的元学习方法，采用了教师 - 学生机制。首先，一个基于最优传输的教师从高场强到低场强MR图像中学习降级过程，并生成伪成对的高场强和低场强MRI图像。然后，一个基于分数的学生解决重建高场强图像的逆问题，最终生成1.5T-like的图像。

    Magnetic resonance imaging (MRI) is known to have reduced signal-to-noise ratios (SNR) at lower field strengths, leading to signal degradation when producing a low-field MRI image from a high-field one. Therefore, reconstructing a high-field-like image from a low-field MRI is a complex problem due to the ill-posed nature of the task. Additionally, obtaining paired low-field and high-field MR images is often not practical. We theoretically uncovered that the combination of these challenges renders conventional deep learning methods that directly learn the mapping from a low-field MR image to a high-field MR image unsuitable. To overcome these challenges, we introduce a novel meta-learning approach that employs a teacher-student mechanism. Firstly, an optimal-transport-driven teacher learns the degradation process from high-field to low-field MR images and generates pseudo-paired high-field and low-field MRI images. Then, a score-based student solves the inverse problem of reconstructing
    
[^63]: Stimulative Training++：超越残差网络性能极限

    Stimulative Training++: Go Beyond The Performance Limits of Residual Networks. (arXiv:2305.02507v1 [cs.LG])

    [http://arxiv.org/abs/2305.02507](http://arxiv.org/abs/2305.02507)

    本文从一种新的社会心理学角度重新审视残差网络的训练过程，发现了网络贡献不足问题并提出解决方案和改进策略，以提高残差网络的性能。

    

    残差网络在近期深度神经网络模型中表现出极大的成功，并变得不可或缺。本文从一种新的社会心理学角度重新考察了残差网络的训练过程，并进一步提出了一种新的训练方案以及三种改进策略，以提高残差网络的性能。我们发现一种类似社会贡献的被忽视的问题，即在残差网络内部，子网络在群体中工作时倾向于比独自工作时付出更少的努力，这被我们定义为“网络贡献不足”。与社会中表现出的个体生产力和整体绩效下降类似，网络贡献不足必然导致残差网络的性能下降。

    Residual networks have shown great success and become indispensable in recent deep neural network models. In this work, we aim to re-investigate the training process of residual networks from a novel social psychology perspective of loafing, and further propose a new training scheme as well as three improved strategies for boosting residual networks beyond their performance limits. Previous research has suggested that residual networks can be considered as ensembles of shallow networks, which implies that the final performance of a residual network is influenced by a group of subnetworks. We identify a previously overlooked problem that is analogous to social loafing, where subnetworks within a residual network are prone to exert less effort when working as part of a group compared to working alone. We define this problem as \textit{network loafing}. Similar to the decreased individual productivity and overall performance as demonstrated in society, network loafing inevitably causes su
    
[^64]: 带有分解密度的字符串图表

    String Diagrams with Factorized Densities. (arXiv:2305.02506v1 [cs.PL])

    [http://arxiv.org/abs/2305.02506](http://arxiv.org/abs/2305.02506)

    本文描述了一个定义在随机变量集上的联合密度的范畴及其意义，以帮助概率编程和因果推断中的组合推理。

    

    有关概率编程和因果模型的研究越来越多地强调了需要在扩展定向图模型的模型类之间进行组合推理的必要性。概率编程和因果模型都定义了一组随机变量上的联合概率密度，并且展示了可以用于推理因果关系和条件独立性的稀疏结构。本文基于最近有关概率映射的马尔可夫范畴的工作，定义了一个范畴，其态射将分别由每个样本空间分解的联合密度与从样本到返回值的确定性映射组合。这是迈向最近的范畴论概率测度描述和通常在概率编程和因果推断中使用的分解密度的操作定义之间的缩小差距的一步。

    A growing body of research on probabilistic programs and causal models has highlighted the need to reason compositionally about model classes that extend directed graphical models. Both probabilistic programs and causal models define a joint probability density over a set of random variables, and exhibit sparse structure that can be used to reason about causation and conditional independence. This work builds on recent work on Markov categories of probabilistic mappings to define a category whose morphisms combine a joint density, factorized over each sample space, with a deterministic mapping from samples to return values. This is a step towards closing the gap between recent category-theoretic descriptions of probability measures, and the operational definitions of factorized densities that are commonly employed in probabilistic programming and causal inference.
    
[^65]: 统一多模态数据嵌入和模态感知注意力学习缺失模态电子病历

    Learning Missing Modal Electronic Health Records with Unified Multi-modal Data Embedding and Modality-Aware Attention. (arXiv:2305.02504v1 [cs.LG])

    [http://arxiv.org/abs/2305.02504](http://arxiv.org/abs/2305.02504)

    该论文提出了一种能够统一处理多模态电子病历的学习方法，并通过引入跳过瓶颈和模态感知注意力解决了缺失模态的情况，取得了在死亡率、血管加压素需要和插管需要预测等方面的表现优于其他基线模型。

    

    电子病历(EHR)通过多种模态提供了丰富的信息。然而，学习多模态EHR目前面临两个主要挑战，即数据嵌入和缺失模态的情况。缺乏跨模态的共享嵌入函数会丢弃不同EHR模态之间的时间关系。另一方面，大多数EHR研究仅依赖EHR时序，并且因此，EHR中的缺失模态尚未得到很好的探索。因此，在本研究中，我们引入了统一多模态集嵌入(UMSE)和模态感知注意力(MAA)与跳过瓶颈(SB)。UMSE对待所有EHR模态而无需单独的插补模块或容易出错的向前传递，而MAA与SB学习缺失的模态EHR，具有有效的模态感知注意力。我们的模型在MIMIC-IV数据集中的死亡率、血管加压素需要和插管需要预测中优于其他基线模型。

    Electronic Health Record (EHR) provides abundant information through various modalities. However, learning multi-modal EHR is currently facing two major challenges, namely, 1) data embedding and 2) cases with missing modality. A lack of shared embedding function across modalities can discard the temporal relationship between different EHR modalities. On the other hand, most EHR studies are limited to relying only on EHR Times-series, and therefore, missing modality in EHR has not been well-explored. Therefore, in this study, we introduce a Unified Multi-modal Set Embedding (UMSE) and Modality-Aware Attention (MAA) with Skip Bottleneck (SB). UMSE treats all EHR modalities without a separate imputation module or error-prone carry-forward, whereas MAA with SB learns missing modal EHR with effective modality-aware attention. Our model outperforms other baseline models in mortality, vasopressor need, and intubation need prediction with the MIMIC-IV dataset.
    
[^66]: AutoML-GPT: 基于 GPT 的自动机器学习

    AutoML-GPT: Automatic Machine Learning with GPT. (arXiv:2305.02499v1 [cs.CL])

    [http://arxiv.org/abs/2305.02499](http://arxiv.org/abs/2305.02499)

    AutoML-GPT 是一种基于 GPT 的自动机器学习方法，利用大型语言模型动态地利用各种人工智能模型，自动化训练管道，节约了选择模型架构、优化算法和调整超参数的人力和时间成本。

    

    AI 任务涵盖了广泛的领域和领域。虽然为特定任务和应用程序设计了众多 AI 模型，但它们通常需要大量的人力投入来查找正确的模型架构、优化算法和超参数。最近，像 ChatGPT 这样的大型语言模型 (LLM) 在推理、理解和交互的各个方面展现出了卓越的能力。因此，我们提出了开发面向任务的提示并自动利用 LLM 自动化训练管道的想法。为了实现这个概念，我们推出了 AutoML-GPT，它采用 GPT 作为连接多种 AI 模型的桥梁，并动态地使用优化超参数训练模型。AutoML-GPT 从模型和数据卡中动态获取用户请求，并组成相应的提示段落。最终，通过这个提示段落，AutoML-GPT 将自动从数据处理到模型架构、超参数调整进行实验。

    AI tasks encompass a wide range of domains and fields. While numerous AI models have been designed for specific tasks and applications, they often require considerable human efforts in finding the right model architecture, optimization algorithm, and hyperparameters. Recent advances in large language models (LLMs) like ChatGPT show remarkable capabilities in various aspects of reasoning, comprehension, and interaction. Consequently, we propose developing task-oriented prompts and automatically utilizing LLMs to automate the training pipeline. To implement this concept, we present the AutoML-GPT, which employs GPT as the bridge to diverse AI models and dynamically trains models with optimized hyperparameters. AutoML-GPT dynamically takes user requests from the model and data cards and composes the corresponding prompt paragraph. Ultimately, with this prompt paragraph, AutoML-GPT will automatically conduct the experiments from data processing to model architecture, hyperparameter tuning,
    
[^67]: 重新审视用于异常检测的图对比学习

    Revisiting Graph Contrastive Learning for Anomaly Detection. (arXiv:2305.02496v1 [cs.LG])

    [http://arxiv.org/abs/2305.02496](http://arxiv.org/abs/2305.02496)

    本文重新审视了用于异常检测的图对比学习方法。通过深入探讨现有方法的基本机制，提出了统一的多GNN和增强图对比框架MAG，并从中提取轻量级实例L-MAG和M-MAG。

    

    最近，将图神经网络（GNN）与对比学习相结合进行异常检测引起了越来越多的关注。现有的图对比异常检测（GCAD）方法主要集中于通过图扩充和多尺度对比模块改善检测能力。然而，这些模块的基本机制尚未得到充分的探索。我们深入研究了多尺度和图扩充机制，并观察到多尺度对比模块并没有增强表达，而多GNN模块是隐藏的贡献者。先前的研究往往将多GNN带来的收益归因于多尺度模块。在本文中，我们深入探讨了这种误解，并提出了多GNN和增强图对比框架MAG，将现有的GCAD方法统一在对比自监督的视角下。我们从MAG框架中提取了两个变体，L-MAG和M-MAG。L-MAG是轻量级的实例。

    Combining Graph neural networks (GNNs) with contrastive learning for anomaly detection has drawn rising attention recently. Existing graph contrastive anomaly detection (GCAD) methods have primarily focused on improving detection capability through graph augmentation and multi-scale contrast modules. However, the underlying mechanisms of how these modules work have not been fully explored. We dive into the multi-scale and graph augmentation mechanism and observed that multi-scale contrast modules do not enhance the expression, while the multi-GNN modules are the hidden contributors. Previous studies have tended to attribute the benefits brought by multi-GNN to the multi-scale modules. In the paper, we delve into the misconception and propose Multi-GNN and Augmented Graph contrastive framework MAG, which unified the existing GCAD methods in the contrastive self-supervised perspective. We extracted two variants from the MAG framework, L-MAG and M-MAG. The L-MAG is the lightweight instanc
    
[^68]: 如何利用强化学习促进未来电力市场设计？第一部分：范型理论。（arXiv:2305.02485v1 [cs.AI]）

    How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 1: A Paradigmatic Theory. (arXiv:2305.02485v1 [cs.AI])

    [http://arxiv.org/abs/2305.02485](http://arxiv.org/abs/2305.02485)

    本文提出基于强化学习的方法，设计联合市场以应对电力行业脱碳，实现电力系统的安全和经济效益，并为环境做出贡献。该范型理论的框架将在两部分中详细介绍。

    

    面对电力行业脱碳的迫切需求，重新设计电力市场是一种宏观层面的方法，以适应可再生能源的高渗透率，并实现电力系统的操作安全、经济效率和环境友好性。然而，现有的市场设计方法学存在于能源现货市场（ESM）、辅助服务市场（ASM）和金融市场（FM）之间协调不足，即“联合市场”，以及缺乏可靠的基于模拟的验证。为了解决这些缺陷，本文将基于强化学习（RL）的模拟，开发联合市场设计的范型理论和详细方法。第一部分提出了这种新型市场设计哲学的理论和框架。首先，总结了在设计联合市场时存在的有争议的市场设计选项作为目标研究问题。其次，提出了马尔可夫博弈模型。

    In face of the pressing need of decarbonization in the power sector, the re-design of electricity market is necessary as a Marco-level approach to accommodate the high penetration of renewable generations, and to achieve power system operation security, economic efficiency, and environmental friendliness. However, existing market design methodologies suffer from the lack of coordination among energy spot market (ESM), ancillary service market (ASM) and financial market (FM), i.e., the "joint market", and the lack of reliable simulation-based verification. To tackle these deficiencies, this two-part paper develops a paradigmatic theory and detailed methods of the joint market design using reinforcement-learning (RL)-based simulation. In Part 1, the theory and framework of this novel market design philosophy are proposed. First, the controversial market design options while designing the joint market are summarized as the targeted research questions. Second, the Markov game model is deve
    
[^69]: 使用机器学习技术的乳腺癌诊断

    Breast Cancer Diagnosis Using Machine Learning Techniques. (arXiv:2305.02482v1 [cs.LG])

    [http://arxiv.org/abs/2305.02482](http://arxiv.org/abs/2305.02482)

    本文回顾和讨论了来自不同源的最新机器学习技术在乳腺癌诊断中的应用，其中包括热成像、红外热成像、电阻抗层析成像以及血液检测中发现的生物标志物，这些技术比传统方法更快、更可靠和更便宜，并且机器学习技术能够提高诊断的准确性。

    

    乳腺癌是女性生命中最具威胁的疾病之一，因此早期和准确的诊断在减少患者死亡风险方面起着关键作用。最近计算工具、红外相机以及生物阻抗定量设备的最新进展，为其他参考技术的出现提供了机会，如热成像、红外热成像、电阻抗层析成像以及血液检测中发现的生物标志物，因此比其他方法更快、更可靠和更便宜。在过去的20年中，上述技术一直被认为是乳腺癌诊断的并行和扩展方法，许多作者得出结论，假阳性和假阴性率显著降低。此外，当筛查方法与临床诊断一起使用时，可以提高诊断的准确性。本文回顾和讨论了来自不同源的最新机器学习技术在乳腺癌诊断中的应用，考虑它们的性能、可用性和重要性。

    Breast cancer is one of the most threatening diseases in women's life; thus, the early and accurate diagnosis plays a key role in reducing the risk of death in a patient's life. Mammography stands as the reference technique for breast cancer screening; nevertheless, many countries still lack access to mammograms due to economic, social, and cultural issues. Latest advances in computational tools, infrared cameras and devices for bio-impedance quantification, have given a chance to emerge other reference techniques like thermography, infrared thermography, electrical impedance tomography and biomarkers found in blood tests, therefore being faster, reliable and cheaper than other methods. In the last two decades, the techniques mentioned above have been considered as parallel and extended approaches for breast cancer diagnosis, as well many authors concluded that false positives and false negatives rates are significantly reduced. Moreover, when a screening method works together with a c
    
[^70]: MLHOps: 机器学习在医疗运营中的应用

    MLHOps: Machine Learning for Healthcare Operations. (arXiv:2305.02474v1 [cs.LG])

    [http://arxiv.org/abs/2305.02474](http://arxiv.org/abs/2305.02474)

    本文综述了机器学习在医疗保健环境中应用的一般性流程，并为开发人员和临床医生提供了指南，以便他们能够部署和维护自己的模型；重点关注包括数据源、准备、工程、工具、长期监控和更新、以及道德考虑等方面，这为MLHOps全流程提供了指导。

    

    机器学习健康运营（MLHOps）是在医疗保健环境中可靠、高效、可用和道德的部署和维护机器学习模型的组合过程。本文提供了该领域工作的综述和开发人员、临床医生在临床实践中部署和维护自己模型的指南。我们涵盖了操作通用机器学习的基本概念，描述了MLHOps流水线的初始设置（包括数据源、准备、工程和工具）。我们随后描述了长期监测和更新（包括数据分布变化和模型更新）和道德考虑（包括偏见、公平性、可解释性和隐私）。因此，本文提供了MLHOps全流程从概念到初始和持续部署的指导。

    Machine Learning Health Operations (MLHOps) is the combination of processes for reliable, efficient, usable, and ethical deployment and maintenance of machine learning models in healthcare settings. This paper provides both a survey of work in this area and guidelines for developers and clinicians to deploy and maintain their own models in clinical practice. We cover the foundational concepts of general machine learning operations, describe the initial setup of MLHOps pipelines (including data sources, preparation, engineering, and tools). We then describe long-term monitoring and updating (including data distribution shifts and model updating) and ethical considerations (including bias, fairness, interpretability, and privacy). This work therefore provides guidance across the full pipeline of MLHOps from conception to initial and ongoing deployment.
    
[^71]: 未知机流形上的潜在结构网络中的半监督回归。

    Semisupervised regression in latent structure networks on unknown manifolds. (arXiv:2305.02473v1 [stat.ML])

    [http://arxiv.org/abs/2305.02473](http://arxiv.org/abs/2305.02473)

    本文提出了一种基于半监督回归的潜在结构网络模型，在未知机流形上使用流形学习和图嵌入技术进行响应预测，并为这些响应建立了收敛保证。

    

    随机图在建模各种应用中的网络越来越受到关注。潜在位置随机图模型认为每个节点都与潜在位置向量相关联，并且这些向量在潜在空间中遵循某些几何结构。本文考虑随机点积图，其中在其各自的潜在位置的内积给定的概率下形成两个节点之间的边缘。我们假设潜在位置向量位于未知的一维曲线上，并通过回归模型与响应协变量耦合。利用潜在位置向量的底层几何结构，我们提出了一种流形学习和图嵌入技术，以预测样本外节点上的响应变量，并为这些响应建立了收敛保证。我们的理论结果得到模拟和对Drosophila大脑数据的应用的支持。

    Random graphs are increasingly becoming objects of interest for modeling networks in a wide range of applications. Latent position random graph models posit that each node is associated with a latent position vector, and that these vectors follow some geometric structure in the latent space. In this paper, we consider random dot product graphs, in which an edge is formed between two nodes with probability given by the inner product of their respective latent positions. We assume that the latent position vectors lie on an unknown one-dimensional curve and are coupled with a response covariate via a regression model. Using the geometry of the underlying latent position vectors, we propose a manifold learning and graph embedding technique to predict the response variable on out-of-sample nodes, and we establish convergence guarantees for these responses. Our theoretical results are supported by simulations and an application to Drosophila brain data.
    
[^72]: 探测行星信号的多重性增强分类器：使用ExoMiner的多重性增强验证69个新行星

    Multiplicity Boost Of Transit Signal Classifiers: Validation of 69 New Exoplanets Using The Multiplicity Boost of ExoMiner. (arXiv:2305.02470v1 [astro-ph.EP])

    [http://arxiv.org/abs/2305.02470](http://arxiv.org/abs/2305.02470)

    该论文介绍了一种可提高探测行星信号分类器性能的框架，称为多重性增强分类器，基于现有的分类器并使用多重性信息来验证69个新的系外行星。

    

    大多数已知的系外行星是通过验证技术而不是通过补充观测进行确认的。这些技术生成的分数通常代表了有关信号的某些信息（用x表示）给出的探测行星信号的概率（y（x）=行星）。这项工作引入了一种框架，即在给定的探测行星信号确认器的基础上，利用多重性信息改善其性能。我们将此框架应用于几个现有的分类器，包括vespa（Morton等人2016）、Robovetter（Coughlin等人2017）、AstroNet（Shallue和Vanderburg 2018）、ExoNet（Ansdel等人2018）、GPC和RFC（Armstrong等人2020）以及ExoMiner（Valizadegan等人2022），以支持我们的分类结果的有效性。

    Most existing exoplanets are discovered using validation techniques rather than being confirmed by complementary observations. These techniques generate a score that is typically the probability of the transit signal being an exoplanet (y(x)=exoplanet) given some information related to that signal (represented by x). Except for the validation technique in Rowe et al. (2014) that uses multiplicity information to generate these probability scores, the existing validation techniques ignore the multiplicity boost information. In this work, we introduce a framework with the following premise: given an existing transit signal vetter (classifier), improve its performance using multiplicity information. We apply this framework to several existing classifiers, which include vespa (Morton et al. 2016), Robovetter (Coughlin et al. 2017), AstroNet (Shallue & Vanderburg 2018), ExoNet (Ansdel et al. 2018), GPC and RFC (Armstrong et al. 2020), and ExoMiner (Valizadegan et al. 2022), to support our cl
    
[^73]: 系统模型和用户模型：探索人工智能面板设计

    The System Model and the User Model: Exploring AI Dashboard Design. (arXiv:2305.02469v1 [cs.HC])

    [http://arxiv.org/abs/2305.02469](http://arxiv.org/abs/2305.02469)

    论文探讨了基于神经网络的AI系统应该有面板以提高其可用性和安全性，并且界面应该具有基于系统模型和用户模型状态的并行显示。

    

    这是一篇关于界面设计和人工智能的推测性论文。最近，基于大语言模型的聊天机器人受到了广泛关注，包括被报道的不良交互。我们认为问题的部分原因是文本并不是所有你需要的东西：复杂的AI系统应该有面板，就像所有其他复杂的设备一样。假设基于神经网络的AI系统将包含可解释的周围世界方面的模型，我们讨论这些面板可能显示的数据。我们推测，对于许多系统来说，最重要的模型将是用户模型和系统模型。我们称之为“系统模型”和“用户模型”。我们认为，为了可用性和安全性，面向基于对话的AI系统的界面应该具有基于系统模型和用户模型状态的并行显示。找到识别、解释和显示这两个模型的方法应该是界面研究的核心部分。

    This is a speculative essay on interface design and artificial intelligence. Recently there has been a surge of attention to chatbots based on large language models, including widely reported unsavory interactions. We contend that part of the problem is that text is not all you need: sophisticated AI systems should have dashboards, just like all other complicated devices. Assuming the hypothesis that AI systems based on neural networks will contain interpretable models of aspects of the world around them, we discuss what data such dashboards might display. We conjecture that, for many systems, the two most important models will be of the user and of the system itself. We call these the System Model and User Model. We argue that, for usability and safety, interfaces to dialogue-based AI systems should have a parallel display based on the state of the System Model and the User Model. Finding ways to identify, interpret, and display these two models should be a core part of interface rese
    
[^74]: Shap-E: 生成有条件的3D隐式函数

    Shap-E: Generating Conditional 3D Implicit Functions. (arXiv:2305.02463v1 [cs.CV])

    [http://arxiv.org/abs/2305.02463](http://arxiv.org/abs/2305.02463)

    Shap-E能够生成有条件的3D隐式函数，可以呈现为纹理网格和神经辐射场，收敛更快且生成的3D模型质量相当或更好。

    

    本文介绍了Shap-E，一种用于生成3D模型的有条件生成模型。与最近的3D生成模型不同的是，Shap-E直接生成可以呈现为纹理网格和神经辐射场的隐式函数的参数，而不是单一的输出表示。我们分两个阶段训练Shap-E：首先，我们训练一个编码器，将3D模型确定性地映射到隐式函数的参数中；其次，我们训练一个条件扩散模型，用于输出编码器的结果。在大规模的3D文本数据匹配数据集上训练时，我们得到的模型能够在几秒钟内生成复杂和多样化的3D模型。与点云的显式生成模型Point-E相比，Shap-E收敛更快，并且在对高维度、多表示输出空间进行建模的情况下，达到了相当或更好的样本质量。我们在https://github.com/openai/shap-e上公开了模型权重、推理代码和样本。

    We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space. We release model weights, inference code, and samples at https://github.com/openai/shap-e.
    
[^75]: 张量流：一种变分推断工具。

    Tensorizing flows: a tool for variational inference. (arXiv:2305.02460v1 [cs.LG])

    [http://arxiv.org/abs/2305.02460](http://arxiv.org/abs/2305.02460)

    张量流是一种扩展标准化流的工具，可以通过结合流与张量网络来改善在学习多峰分布的困难变分推断任务中的结果。

    

    基于深度神经网络的表达能力，标准化流在生成建模中取得了巨大的成功。标准化流还成功地应用于变分推断，其中我们尝试基于分布的对数似然或能量函数学习采样器，而不是基于数据。在变分推断中，正态流中使用的参考高斯分布的单峰性可能导致学习多峰分布时出现困难。我们引入了一个标准流的扩展，在这个扩展中，高斯参考被一个通过张量网络（特别是矩阵积态或张量列车）构造的参考分布所取代。我们展示了通过在困难的变分推断任务中结合流和张量网络，我们可以改善使用任一工具所获得的结果。

    Fueled by the expressive power of deep neural networks, normalizing flows have achieved spectacular success in generative modeling, or learning to draw new samples from a distribution given a finite dataset of training samples. Normalizing flows have also been applied successfully to variational inference, wherein one attempts to learn a sampler based on an expression for the log-likelihood or energy function of the distribution, rather than on data. In variational inference, the unimodality of the reference Gaussian distribution used within the normalizing flow can cause difficulties in learning multimodal distributions. We introduce an extension of normalizing flows in which the Gaussian reference is replaced with a reference distribution that is constructed via a tensor network, specifically a matrix product state or tensor train. We show that by combining flows with tensor networks on difficult variational inference tasks, we can improve on the results obtained by using either tool
    
[^76]: 转移学习与主动学习用于共鸣检测：解决稀有类挑战

    Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge. (arXiv:2305.02459v1 [cs.CL])

    [http://arxiv.org/abs/2305.02459](http://arxiv.org/abs/2305.02459)

    本文提出并探究了基于转移和主动学习的稀有类问题的解决方案，包括利用在密切相关任务上训练的模型和评估获取策略来解决共振检测的稀有类问题，并且发现了一种名为PRC的有效的策略来指导注释。

    

    尽管基于变压器的系统使得使用更少的训练样例能够得到更高的准确性，但对于稀有类任务（即类别标签非常少见的情况，例如<5%的样本），数据采集障碍仍然存在。主动学习一般被提出用于缓解这种挑战，但选择策略，即选择稀有类示例的标准，尚未得到系统评估。此外，变压器可以实现迭代迁移学习方法。我们提出并研究了转移和主动学习解决了通过利用在密切相关任务上训练的模型和评估获取策略来解决共振检测的稀有类问题，其中包括一种提出的稀有类概率（PRC）方法。我们针对特定的稀有类问题（从社交媒体中收集认知共振的语言样本）进行了这些实验。我们发现PRC是指导注释的简单而有效的策略，最终可以提高性能，并且转移学习可以在稀缺数据情况下提供显著的改进。

    While transformer-based systems have enabled greater accuracies with fewer training examples, data acquisition obstacles still persist for rare-class tasks -- when the class label is very infrequent (e.g. < 5% of samples). Active learning has in general been proposed to alleviate such challenges, but choice of selection strategy, the criteria by which rare-class examples are chosen, has not been systematically evaluated. Further, transformers enable iterative transfer-learning approaches. We propose and investigate transfer- and active learning solutions to the rare class problem of dissonance detection through utilizing models trained on closely related tasks and the evaluation of acquisition strategies, including a proposed probability-of-rare-class (PRC) approach. We perform these experiments for a specific rare class problem: collecting language samples of cognitive dissonance from social media. We find that PRC is a simple and effective strategy to guide annotations and ultimately
    
[^77]: 面向马尔可夫数据的流式PCA算法

    Streaming PCA for Markovian Data. (arXiv:2305.02456v1 [math.ST])

    [http://arxiv.org/abs/2305.02456](http://arxiv.org/abs/2305.02456)

    本文提出了一种面向马尔可夫数据采样的流式PCA算法，并获得了该算法在整个数据集上的第一个尖锐率，提高了算法的效率。同时，本文提出的自适应方案在模拟和真实数据示例中表现良好。

    

    自从Oja在1982年的经典论文中首次提出以来，Oja算法已成为流式主成分分析(PCA)的一种常用方法。本文研究了流式PCA问题，其中数据点从一个不可约、无周期、可逆的马尔可夫链中采样。我们的目标是估计平稳分布的未知协方差矩阵的前一个特征向量。这种情况适用于只能从马尔可夫链蒙特卡罗(MCMC)类型的算法中采样数据，并且目标是对该链的平稳分布的参数进行推断的情况。现有文献中大多数Oja算法的收敛保证都假定数据点是IID采样的。对于具有马尔可夫依赖关系的数据流，人们通常对数据进行下采样以获得"几乎"独立的数据流。在本文中，我们获得了Oja算法在整个数据集上的第一个尖锐率，其中去掉了$n$的对数依赖性，结果是$\mathcal{O}(n^{-1})$的速率。我们还提出了一种自适应方案来调整算法的步长，它在模拟和真实数据示例中都表现更好。

    Since its inception in Erikki Oja's seminal paper in 1982, Oja's algorithm has become an established method for streaming principle component analysis (PCA). We study the problem of streaming PCA, where the data-points are sampled from an irreducible, aperiodic, and reversible Markov chain. Our goal is to estimate the top eigenvector of the unknown covariance matrix of the stationary distribution. This setting has implications in situations where data can only be sampled from a Markov Chain Monte Carlo (MCMC) type algorithm, and the goal is to do inference for parameters of the stationary distribution of this chain. Most convergence guarantees for Oja's algorithm in the literature assume that the data-points are sampled IID. For data streams with Markovian dependence, one typically downsamples the data to get a "nearly" independent data stream. In this paper, we obtain the first sharp rate for Oja's algorithm on the entire data, where we remove the logarithmic dependence on $n$ resulti
    
[^78]: 黑盒系统的贝叶斯安全验证

    Bayesian Safety Validation for Black-Box Systems. (arXiv:2305.02449v1 [cs.LG])

    [http://arxiv.org/abs/2305.02449](http://arxiv.org/abs/2305.02449)

    本文提出了一种名为贝叶斯安全验证的算法，将黑盒安全验证问题转化为贝叶斯优化问题。该算法通过概率代理模型拟合快速预测故障，利用重要性采样估计操作域内的故障概率，从而实现了对高维、危险、计算昂贵的系统的高效估计。

    

    对于安全关键系统准确估计故障概率对认证至关重要。由于高维输入空间、危险测试场景和计算昂贵的仿真器，估计通常具有挑战性，因此研究高效估计技术十分重要。本文将黑盒安全验证问题重新定义为贝叶斯优化问题，并引入一种算法——贝叶斯安全验证，该算法通过迭代拟合概率代理模型来高效预测故障。该算法旨在搜索故障、计算最可能的故障，并利用重要性采样估计操作域内的故障概率。我们引入了三种采集函数，重点是通过覆盖设计空间、优化解析派生的故障边界和采样预测的故障区域来减少不确定性。主要涉及只输出二进制指标的系统。

    Accurately estimating the probability of failure for safety-critical systems is important for certification. Estimation is often challenging due to high-dimensional input spaces, dangerous test scenarios, and computationally expensive simulators; thus, efficient estimation techniques are important to study. This work reframes the problem of black-box safety validation as a Bayesian optimization problem and introduces an algorithm, Bayesian safety validation, that iteratively fits a probabilistic surrogate model to efficiently predict failures. The algorithm is designed to search for failures, compute the most-likely failure, and estimate the failure probability over an operating domain using importance sampling. We introduce a set of three acquisition functions that focus on reducing uncertainty by covering the design space, optimizing the analytically derived failure boundaries, and sampling the predicted failure regions. Mainly concerned with systems that only output a binary indicat
    
[^79]: 基于奖励教学的联邦多臂老虎机设计

    Reward Teaching for Federated Multi-armed Bandits. (arXiv:2305.02441v1 [stat.ML])

    [http://arxiv.org/abs/2305.02441](http://arxiv.org/abs/2305.02441)

    本论文提出了一种基于奖励教学思想的联邦多臂老虎机设计，通过隐式本地奖励调整来指导客户端朝着全局最优性，队服务端提出了老虎机学习和目标教学任务进行了优化。

    

    目前大部分已有的联邦多臂老虎机（FMAB）设计都基于假设客户端会实现指定的设计来与服务器协作。但实际上，可能无法修改客户端现有的协议。为了应对这一挑战，该工作关注始终最大化其个体累积奖励的客户端，并引入了“奖励教学”的新思想，即通过隐式的本地奖励调整指导客户端朝着全局最优性。在这个框架下，服务器面临两个密切耦合的任务，即老虎机学习和目标教学，它们的结合非常复杂和具有挑战性。首先设计了一个名为 “Teaching-After-Learning（TAL）” 的分阶段方法，分别鼓励和限制客户端的探索。当客户端策略满足一定的温和要求时，建立了TAL的综合性能分析。通过开发新的技术方法来分析TAL的热启动和算法，我们展示了TAL可以比现有的FMAB设计带来显著的改进。

    Most of the existing federated multi-armed bandits (FMAB) designs are based on the presumption that clients will implement the specified design to collaborate with the server. In reality, however, it may not be possible to modify the client's existing protocols. To address this challenge, this work focuses on clients who always maximize their individual cumulative rewards, and introduces a novel idea of "reward teaching", where the server guides the clients towards global optimality through implicit local reward adjustments. Under this framework, the server faces two tightly coupled tasks of bandit learning and target teaching, whose combination is non-trivial and challenging. A phased approach, called Teaching-After-Learning (TAL), is first designed to encourage and discourage clients' explorations separately. General performance analyses of TAL are established when the clients' strategies satisfy certain mild requirements. With novel technical approaches developed to analyze the warm
    
[^80]: 廉价评估自回归Transformer API推断效率度量

    Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs. (arXiv:2305.02440v1 [cs.LG])

    [http://arxiv.org/abs/2305.02440](http://arxiv.org/abs/2305.02440)

    本文提出了一个新的度量方法，用于比较Transformer API上模型推理效率的相关成本，以解决现有度量方法在软件和硬件优化和共享基础设施方面的缺陷。

    

    大型语言模型(LMM)在自然语言处理的很多最先进系统中发挥着作用。然而，即使在推理时，这些模型也非常计算密集，这引发了一个自然的问题: 部署更大的模型的额外成本是否值得预期的能力提升?更好地理解这种权衡需要一个推理效率度量，它既可以在来自不同供应商的模型之间轻松比较，又可以代表在隔离性能环境中运行查询的真实成本。但是，今天访问LLMs在很大程度上仅限于黑匣子文本生成API，通过此接口测量的原始运行时间不能满足这些愿望:模型提供者可以应用与模型不相关的各种软件和硬件优化，而在共享基础设施上提供的模型容易受到性能争用的影响。为了解决这些问题，我们提出了一种新的度量方法，用于比较在用于自回归Transformer API中服务的模型上运行查询的相关成本。

    Large language models (LLMs) power many state-of-the-art systems in natural language processing. However, these models are extremely computationally expensive, even at inference time, raising the natural question: when is the extra cost of deploying a larger model worth the anticipated boost in capabilities? Better understanding this tradeoff fundamentally could benefit from an inference efficiency metric that is both (i) easily comparable across models from different providers, and (ii) representative of the true cost of running queries in an isolated performance environment. Unfortunately, access to LLMs today is largely restricted to black-box text generation APIs and raw runtimes measured through this interface do not satisfy these desiderata: model providers can apply various software and hardware optimizations orthogonal to the model, and models served on shared infrastructure are susceptible to performance contention. To circumvent these problems, we propose a new metric for com
    
[^81]: GAMIVAL：移动云游戏内容的视频质量预测

    GAMIVAL: Video Quality Prediction on Mobile Cloud Gaming Content. (arXiv:2305.02422v1 [eess.IV])

    [http://arxiv.org/abs/2305.02422](http://arxiv.org/abs/2305.02422)

    GAMIVAL是一种新型的游戏专用无参考视频质量评估模型，结合了多种优点。在移动云游戏内容的主观质量评估数据库上进行测试，表现出更好的NR VQA性能。

    

    近年来，移动云游戏行业迅速增长。当游戏视频从云服务器传输到客户端设备时，需要一种可以监测失真视频质量而无需参考视频的算法。然而，创建可以准确预测由计算机图形引擎渲染的流式游戏视频质量的无参考视频质量评估（NR VQA）模型是一项具有挑战性的问题，因为游戏内容通常在统计上与自然视频不同，缺乏细节，并包含许多平滑区域。我们创建了一种名为Gaming Video Quality Evaluator（GAMIVAL）的新型游戏专用NR VQA模型，结合和利用空间和时间游戏失真场景统计模型、神经噪声模型和客观质量模型的优点。GAMIVAL已经在一个大型的移动云游戏内容主观质量评估数据库上进行了训练和测试，并在游戏内容的NR VQA模型方面超越了最先进的模型。

    The mobile cloud gaming industry has been rapidly growing over the last decade. When streaming gaming videos are transmitted to customers' client devices from cloud servers, algorithms that can monitor distorted video quality without having any reference video available are desirable tools. However, creating No-Reference Video Quality Assessment (NR VQA) models that can accurately predict the quality of streaming gaming videos rendered by computer graphics engines is a challenging problem, since gaming content generally differs statistically from naturalistic videos, often lacks detail, and contains many smooth regions. Until recently, the problem has been further complicated by the lack of adequate subjective quality databases of mobile gaming content. We have created a new gaming-specific NR VQA model called the Gaming Video Quality Evaluator (GAMIVAL), which combines and leverages the advantages of spatial and temporal gaming distorted scene statistics models, a neural noise model, 
    
[^82]: 计划、消除和跟踪——语言模型是具备体验的智能体的良师益友。

    Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents. (arXiv:2305.02412v1 [cs.CL])

    [http://arxiv.org/abs/2305.02412](http://arxiv.org/abs/2305.02412)

    本文介绍了Plan，Eliminate，和Track（PET）框架，该框架利用预先训练的大型语言模型（LLM）帮助智能体简化控制任务，从而解决了LLM直接作为智能体所面临的一些限制和问题。

    

    预训练的大型语言模型(LLMs)可以捕捉到关于世界的程序化知识。最近的研究利用LLM产生的抽象计划来简化具有挑战性的控制任务，通过动作打分或动作建模（微调）来实现。然而，变压器架构继承了几个限制，使得LLM难以直接作为智能体：例如有限的输入长度，微调的效率，预训练的偏见以及与非文本环境的不兼容性。为了与低级别可训练的执行器保持兼容性，我们建议使用LLMs中的知识来简化控制问题，而不是解决问题。 我们提出了Plan，Eliminate和Track（PET）框架。计划模块将任务描述转化为高层次子任务的列表。消除模块从当前子任务的观察中屏蔽不相关的对象和容器。最后，跟踪模块确定智能体是否已经实现了当前子任务。

    Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accompli
    
[^83]: 正则流在任意空时维度规范场理论中的应用

    Normalizing flows for lattice gauge theory in arbitrary space-time dimension. (arXiv:2305.02402v1 [hep-lat])

    [http://arxiv.org/abs/2305.02402](http://arxiv.org/abs/2305.02402)

    本文提出了新的正则流算法来推广到更高维度的规范场理论中，成功地应用于SU(3)规范场理论的原理证明实验，并显示出可行性。

    

    正则流已经被成功地应用于二维规范场理论中的场态采样，但在更高维空时中的应用仍然面临挑战。本文提出了新的算法发展，使用规范等变流体系结构，推广到更高维的规则结构。特别地，我们讨论具有可追溯和无偏雅可比行列式的蒙皮自回归转换，这是可伸缩和近似精确的流体态采样算法的关键。为了确保可行性，我们在四维时空中进行了SU(3)规范场理论的原理证明实验，并展示了结果。

    Applications of normalizing flows to the sampling of field configurations in lattice gauge theory have so far been explored almost exclusively in two space-time dimensions. We report new algorithmic developments of gauge-equivariant flow architectures facilitating the generalization to higher-dimensional lattice geometries. Specifically, we discuss masked autoregressive transformations with tractable and unbiased Jacobian determinants, a key ingredient for scalable and asymptotically exact flow-based sampling algorithms. For concreteness, results from a proof-of-principle application to SU(3) lattice gauge theory in four space-time dimensions are reported.
    
[^84]: “合成 DOmain-Targeted Augmentation (S-DOTA) 提升数字病理学模型泛化性能”

    Synthetic DOmain-Targeted Augmentation (S-DOTA) Improves Model Generalization in Digital Pathology. (arXiv:2305.02401v1 [eess.IV])

    [http://arxiv.org/abs/2305.02401](http://arxiv.org/abs/2305.02401)

    本研究比较了四种提高模型泛化性能的增强方法，包括两种基于S-DOTA的合成增强方法、一个基于ICC profile的颜色校准方法和一个传统的基线方法。实验结果表明使用S-DOTA增强方法能够显著提高数字病理图像模型的泛化性能。

    

    机器学习算法有望改善数字病理学中的患者结果，但是这些工具的泛化性能目前受到组织样本制备、染色程序和扫描设备差异的敏感性限制，这些差异会导致数字切片中的域漂移。为了克服这个限制并提高模型的泛化性能，我们研究了两种合成 DOmain-Targeted Augmentation (S-DOTA) 方法——CycleGAN-enabled Scanner Transform (ST) 和 targeted Stain Vector Augmentation (SVA)的有效性，并将它们与国际色彩委员会 (ICC) 基于配置文件的颜色校准 (ICC Cal) 方法和使用传统亮度、颜色和噪声增强的基线方法进行了比较。我们评估了这些技术提高模型对各种任务和设置的泛化能力的能力：包括四个模型、两种模型类型(组织分割和细胞分类)、两种损失函数、六个实验室、六个扫描仪和三个指标(肝病、结肠癌和直肠癌)。

    Machine learning algorithms have the potential to improve patient outcomes in digital pathology. However, generalization of these tools is currently limited by sensitivity to variations in tissue preparation, staining procedures and scanning equipment that lead to domain shift in digitized slides. To overcome this limitation and improve model generalization, we studied the effectiveness of two Synthetic DOmain-Targeted Augmentation (S-DOTA) methods, namely CycleGAN-enabled Scanner Transform (ST) and targeted Stain Vector Augmentation (SVA), and compared them against the International Color Consortium (ICC) profile-based color calibration (ICC Cal) method and a baseline method using traditional brightness, color and noise augmentations. We evaluated the ability of these techniques to improve model generalization to various tasks and settings: four models, two model types (tissue segmentation and cell classification), two loss functions, six labs, six scanners, and three indications (hep
    
[^85]: 可解释人工智能揭示全球森林碳抵消项目未来火灾风险普遍上升

    Widespread Increases in Future Wildfire Risk to Global Forest Carbon Offset Projects Revealed by Explainable AI. (arXiv:2305.02397v1 [cs.LG])

    [http://arxiv.org/abs/2305.02397](http://arxiv.org/abs/2305.02397)

    研究利用可解释人工智能模型对全球森林野火风险进行了预测，发现2050年到2080年期间，全球森林碳抵消项目将面临更大的火灾风险，其中火灾的暴露程度预计将增加55%[37-76%]。

    

    碳抵消计划在应对气候变化方面至关重要。森林碳抵消项目长期稳定和可行性面临的一个新威胁是山火，可以释放大量碳并限制相关抵消积分的功效。然而，分析森林碳项目的火灾风险是具有挑战性的，因为现有的长期火灾风险预测模型在预测准确性方面受到限制。因此，我们提出了一个可解释的人工智能（XAI）模型，基于700万全球卫星野火观测数据进行训练。验证结果表明，具有高分辨率、增强准确性的全球野火风险预测具有巨大的潜力，且该模型优于美国国家大气研究中心的领先火灾模型。应用于190个全球森林碳项目的集合中，我们发现在中等情景下（SSP2-4.5），火灾暴露预计会在2080年上升55% [37-76%]。我们的结果表明，全球森林碳抵消项目面临更大的火灾风险。

    Carbon offset programs are critical in the fight against climate change. One emerging threat to the long-term stability and viability of forest carbon offset projects is wildfires, which can release large amounts of carbon and limit the efficacy of associated offsetting credits. However, analysis of wildfire risk to forest carbon projects is challenging because existing models for forecasting long-term fire risk are limited in predictive accuracy. Therefore, we propose an explainable artificial intelligence (XAI) model trained on 7 million global satellite wildfire observations. Validation results suggest substantial potential for high resolution, enhanced accuracy projections of global wildfire risk, and the model outperforms the U.S. National Center for Atmospheric Research's leading fire model. Applied to a collection of 190 global forest carbon projects, we find that fire exposure is projected to increase 55% [37-76%] by 2080 under a mid-range scenario (SSP2-4.5). Our results indic
    
[^86]: 特征工程能帮助量子机器学习进行恶意软件检测吗？

    Can Feature Engineering Help Quantum Machine Learning for Malware Detection?. (arXiv:2305.02396v1 [cs.LG])

    [http://arxiv.org/abs/2305.02396](http://arxiv.org/abs/2305.02396)

    本文通过量子机器学习与特征选择策略相结合的混合框架，以降低恶意软件分类器培训时间，初步结果表明在模拟器上可以达到78.91％的测试准确性。

    

    随着恶意软件攻击数量和复杂度的增加，基于机器学习（ML）的恶意软件检测系统变得越来越重要。同时，许多用于恶意软件分类的流行ML模型都是有监督学习。这些有监督分类器通常对新型恶意软件的推广效果不好。因此，需要经常重新训练它们以检测新的恶意软件样本，这可能非常耗时。本文通过理论量子ML与特征选择策略相结合的混合框架来解决这个问题，以降低数据大小和恶意软件分类器培训时间。初步结果表明，使用XGBoost选择的特征的VQC在模拟器上可以获得78.91％的测试准确性。使用XGBoost选择的特征训练的模型在IBM 5 qubits机器上的平均准确性为74％（+-11.35％）。

    With the increasing number and sophistication of malware attacks, malware detection systems based on machine learning (ML) grow in importance. At the same time, many popular ML models used in malware classification are supervised solutions. These supervised classifiers often do not generalize well to novel malware. Therefore, they need to be re-trained frequently to detect new malware specimens, which can be time-consuming. Our work addresses this problem in a hybrid framework of theoretical Quantum ML, combined with feature selection strategies to reduce the data size and malware classifier training time. The preliminary results show that VQC with XGBoost selected features can get a 78.91% test accuracy on the simulator. The average accuracy for the model trained using the features selected with XGBoost was 74% (+- 11.35%) on the IBM 5 qubits machines.
    
[^87]: 基于归因的防御插入式文本后门攻击

    Defending against Insertion-based Textual Backdoor Attacks via Attribution. (arXiv:2305.02394v1 [cs.CL])

    [http://arxiv.org/abs/2305.02394](http://arxiv.org/abs/2305.02394)

    本文提出了一种基于归因的管道AttDef，用于防御两种插入式污染攻击BadNL和InSent，该管道可以成功缓解插入式文本后门攻击并在四个基准数据集上平均提高了56.59%至79.97%和15.25%至48.34%的准确率。

    

    文本后门攻击是一种新型攻击模式，已被证明在训练期间向模型添加后门是有效的。防御此类后门攻击已变得紧迫和重要。本文提出了一种名为AttDef的高效归因管道，用于防御两种插入式污染攻击BadNL和InSent。具体而言，我们将具有较大归因分数的令牌视为潜在触发器，因为较大的归因词对于错误预测结果做出较大贡献，因此更有可能是污染触发器。此外，我们进一步利用外部预训练语言模型来区分输入是否被污染。我们展示了我们的方法可以在两种常见的攻击场景（污染训练数据和测试数据）中具有足够的泛化性，这一点持续改善了之前的方法。例如，AttDef在四个基准数据集上可以成功缓解两种攻击，平均准确率为79.97%（提高了56.59%）和48.34%（提高了15.25%），证明了它在防御插入式文本后门攻击方面的有效性。

    Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training. Defending against such backdoor attacks has become urgent and important. In this paper, we propose AttDef, an efficient attribution-based pipeline to defend against two insertion-based poisoning attacks, BadNL and InSent. Specifically, we regard the tokens with larger attribution scores as potential triggers since larger attribution words contribute more to the false prediction results and therefore are more likely to be poison triggers. Additionally, we further utilize an external pre-trained language model to distinguish whether input is poisoned or not. We show that our proposed method can generalize sufficiently well in two common attack scenarios (poisoning training data and testing data), which consistently improves previous methods. For instance, AttDef can successfully mitigate both attacks with an average accuracy of 79.97% (56.59% up) and 48.34% 
    
[^88]: 用Transformer逼近CKY算法

    Approximating CKY with Transformers. (arXiv:2305.02386v1 [cs.CL])

    [http://arxiv.org/abs/2305.02386](http://arxiv.org/abs/2305.02386)

    本文研究了Transformer模型逼近CKY算法的能力，提出了一种用梯度预测解析的方法，在标准基准测试中表现竞争力更好，同时速度更快。在随机PCFG下解析时，性能下降，但加入额外的归纳偏差是有帮助的。

    

    本文研究了Transformer模型逼近CKY算法的能力，直接预测句子的解析，避免了CKY算法对句子长度的三次依赖。在标准的组成句分析基准测试中，我们发现这种方法比使用CKY的可比分析器取得了竞争或更好的性能，同时速度更快。我们还评估了在随机PCFG下进行解析的可行性。在这里，我们发现在语法变得更加模糊的情况下，性能下降，这表明Transformer没有完全捕捉到CKY计算。然而，我们也发现，结合额外的归纳偏差是有帮助的，并提出了一种新方法，利用相对于图表表示的梯度来预测解析，类比于CKY算法与图表相关的一个分区函数变体的子梯度。

    We investigate the ability of transformer models to approximate the CKY algorithm, using them to directly predict a parse and thus avoid the CKY algorithm's cubic dependence on sentence length. We find that on standard constituency parsing benchmarks this approach achieves competitive or better performance than comparable parsers that make use of CKY, while being faster. We also evaluate the viability of this approach for parsing under random PCFGs. Here we find that performance declines as the grammar becomes more ambiguous, suggesting that the transformer is not fully capturing the CKY computation. However, we also find that incorporating additional inductive bias is helpful, and we propose a novel approach that makes use of gradients with respect to chart representations in predicting the parse, in analogy with the CKY algorithm being the subgradient of a partition function variant with respect to the chart.
    
[^89]: 使用预训练音频表示学习检测新颖和细粒度声学序列

    Learning to Detect Novel and Fine-Grained Acoustic Sequences Using Pretrained Audio Representations. (arXiv:2305.02382v1 [cs.SD])

    [http://arxiv.org/abs/2305.02382](http://arxiv.org/abs/2305.02382)

    本研究探讨了预训练音频表示在少样本声事件检测中的应用，提出了针对新颖声学序列的少样本检测任务，并通过实验验证了预训练表示的通用效用，证明其适用于该任务并启用了相应的少样本框架。

    

    本项工作探讨了预训练音频表示在少样本声事件检测中的应用。我们特别针对少样本检测新颖声学序列的任务进行研究，即在不假设非目标音频的情况下检测具有语义上有意义的时间结构的声事件。我们开发了预训练适当表示的过程，以及将它们转移到我们的少样本学习场景中的方法。我们的实验评估了我们预训练表示的通用效用，以及通过由真实世界的声学序列构成的任务来提出的少样本方法的效用。我们的预训练嵌入适用于所提出的任务，可以启用我们的少样本框架的多个方面。

    This work investigates pretrained audio representations for few shot Sound Event Detection. We specifically address the task of few shot detection of novel acoustic sequences, or sound events with semantically meaningful temporal structure, without assuming access to non-target audio. We develop procedures for pretraining suitable representations, and methods which transfer them to our few shot learning scenario. Our experiments evaluate the general purpose utility of our pretrained representations on AudioSet, and the utility of proposed few shot methods via tasks constructed from real-world acoustic sequences. Our pretrained embeddings are suitable to the proposed task, and enable multiple aspects of our few shot framework.
    
[^90]: MaskSearch：规模化查询图像掩模

    MaskSearch: Querying Image Masks at Scale. (arXiv:2305.02375v1 [cs.DB])

    [http://arxiv.org/abs/2305.02375](http://arxiv.org/abs/2305.02375)

    MaskSearch是一个系统，通过使用新颖的索引技术和高效的过滤器验证查询执行框架，加速对图像掩模数据库的查询，可以将个体查询加速高达两个数量级，优于现有方法。

    

    在图像数据库上的机器学习任务通常会生成注释图像内容的掩模（例如显着性地图，分割地图），并且可以实现各种应用程序（例如确定模型是否学习了虚假关联，或者图像是否被恶意修改以误导模型）。尽管基于掩模属性检索示例的查询对实践者很有价值，但现有系统无法高效地支持此类查询。在本文中，我们正式确定了这个问题，并提出了一个系统MaskSearch，着重于加速对图像掩模数据库的查询。MaskSearch利用了一种新颖的索引技术和一种高效的过滤器验证查询执行框架。我们的原型在现实世界数据集上的实验表明，使用索引大小约为数据5%的MaskSearch可以将个体查询加速高达两个数量级，并且在模拟数据集探索的各种多查询工作负载上始终表现优异，优于现有方法。

    Machine learning tasks over image databases often generate masks that annotate image content (e.g., saliency maps, segmentation maps) and enable a variety of applications (e.g., determine if a model is learning spurious correlations or if an image was maliciously modified to mislead a model). While queries that retrieve examples based on mask properties are valuable to practitioners, existing systems do not support such queries efficiently. In this paper, we formalize the problem and propose a system, MaskSearch, that focuses on accelerating queries over databases of image masks. MaskSearch leverages a novel indexing technique and an efficient filter-verification query execution framework. Experiments on real-world datasets with our prototype show that MaskSearch, using indexes approximately 5% the size of the data, accelerates individual queries by up to two orders of magnitude and consistently outperforms existing methods on various multi-query workloads that simulate dataset explora
    
[^91]: 应用于神经网络的敏感性分析度量工具

    Metric Tools for Sensitivity Analysis with Applications to Neural Networks. (arXiv:2305.02368v1 [cs.LG])

    [http://arxiv.org/abs/2305.02368](http://arxiv.org/abs/2305.02368)

    本文提出了一种度量框架和新的聚合指标，用于敏感性分析，可以适用于任何可微分模型。其中新的聚合指标基于三种方法获取有价值的信息，并在神经网络模型上进行了验证。

    

    随着机器学习模型被考虑用于拥有重大社会影响的自主决策，了解这些模型如何工作的需求迅速增长。可解释人工智能(XAI)旨在为机器学习模型所做的预测提供解释，以使该模型对用户更具可信度和透明度。本文提出了一个理论框架，使用度量技巧来研究敏感度分析。从这个度量框架开始，介绍了新的聚合指标，以根据三种方法从偏导数中获取有价值的信息：局部扰动分析、全局扰动分析和混合方法。这些指标适用于任何可微分模型，并且它们的性能在神经网络模型上进行了说明。

    As Machine Learning models are considered for autonomous decisions with significant social impact, the need for understanding how these models work rises rapidly. Explainable Artificial Intelligence (XAI) aims to provide interpretations for predictions made by Machine Learning models, in order to make the model trustworthy and more transparent for the user. For example, selecting relevant input variables for the problem directly impacts the model's ability to learn and make accurate predictions, so obtaining information about input importance play a crucial role when training the model. One of the main XAI techniques to obtain input variable importance is the sensitivity analysis based on partial derivatives. However, existing literature of this method provide no justification of the aggregation metrics used to retrieved information from the partial derivatives.  In this paper, a theoretical framework is proposed to study sensitivities of ML models using metric techniques. From this me
    
[^92]: 在低端硬件上使用语言模型

    Using Language Models on Low-end Hardware. (arXiv:2305.02350v1 [cs.CL])

    [http://arxiv.org/abs/2305.02350](http://arxiv.org/abs/2305.02350)

    本论文评估了在低端硬件上使用固定语言模型来训练文本分类网络的可行性，并发现在某些情况下，不对语言模型进行微调可以在更快的训练中产生竞争性的效果，仅需要原先内存的四分之一即可。

    

    本文评估了在低端硬件上使用固定语言模型来训练文本分类网络的可行性。我们将语言模型与CNN架构相结合，并组成了包括单标签和多标签分类的话题、情感和风格的8组数据集的综合基准。我们的观察总结成一个权衡列表，并得出结论，即在某些情况下，不对语言模型进行微调可以在更快的训练中产生竞争性的效果，仅需要原先内存的四分之一即可。

    This paper evaluates the viability of using fixed language models for training text classification networks on low-end hardware. We combine language models with a CNN architecture and put together a comprehensive benchmark with 8 datasets covering single-label and multi-label classification of topic, sentiment, and genre. Our observations are distilled into a list of trade-offs, concluding that there are scenarios, where not fine-tuning a language model yields competitive effectiveness at faster training, requiring only a quarter of the memory compared to fine-tuning.
    
[^93]: 神经网络有效理论的结构

    Structures of Neural Network Effective Theories. (arXiv:2305.02334v1 [hep-th])

    [http://arxiv.org/abs/2305.02334](http://arxiv.org/abs/2305.02334)

    该论文提出了一种简化深度神经网络有效场论计算的图解方法，并指出单一条件决定了所有神经元预激活的关联函数的临界性，这可能有助于推动深度学习和场论模拟的进展。

    

    我们提出了一种图解方法，用于研究深度神经网络初始状态下的有效场论（EFT），这种方法可以极大地简化计算有限宽度修正神经元统计量的过程。EFT计算的结构使得所有神经元预激活的关联函数的临界性都受到单一条件的控制。理解这样的EFT可能有助于进展深度学习和场论模拟。

    We develop a diagrammatic approach to effective field theories (EFTs) corresponding to deep neural networks at initialization, which dramatically simplifies computations of finite-width corrections to neuron statistics. The structures of EFT calculations make it transparent that a single condition governs criticality of all connected correlators of neuron preactivations. Understanding of such EFTs may facilitate progress in both deep learning and field theory simulations.
    
[^94]: 基于相关性的多层多模态学习在多能源异常检测中的应用

    Correlation-Driven Multi-Level Multimodal Learning for Anomaly Detection on Multiple Energy Sources. (arXiv:2305.02323v1 [cs.LG])

    [http://arxiv.org/abs/2305.02323](http://arxiv.org/abs/2305.02323)

    本文提出了一种基于相关性的多层多模态学习（CMDML）方法，以实现多能源异常检测。CMDML 能够综合不同能源源之间的相关性属性和复杂性，并在实验中得到了表现优于最先进方法的结果。

    

    先进的计量基础设施（AMI）已被广泛用作智能能源消费测量系统。电力是AMI可以收集的代表性能源，大多数现有的检测异常能源消耗的研究都集中在单个能源源上，即电力。最近，其他能源源如水、气和供暖也被积极收集。因此，有必要开发跨多个能源源的异常检测统一方法;然而，研究尚未解决这个问题。这个问题的固有困难在于异常通常没有注释。此外，现有的异常定义工作仅依赖于个体能源源。在本文中，我们首先提出了一种方法，考虑到不同能源源之间的相关性，以定义异常。然后，我们提出了一种新的基于相关性的多层多模态学习（CMDML）方法，用于跨多个能源源的异常检测。具体而言，CMDML集成了多个层次的特征和多模态融合技术，以捕捉不同能源源之间复杂的相关性。在实际数据集上的实验证明，CMDML优于现有的最先进方法。

    Advanced metering infrastructure (AMI) has been widely used as an intelligent energy consumption measurement system. Electric power was the representative energy source that can be collected by AMI; most existing studies to detect abnormal energy consumption have focused on a single energy source, i.e., power. Recently, other energy sources such as water, gas, and heating have also been actively collected. As a result, it is necessary to develop a unified methodology for anomaly detection across multiple energy sources; however, research efforts have rarely been made to tackle this issue. The inherent difficulty with this issue stems from the fact that anomalies are not usually annotated. Moreover, existing works of anomaly definition depend on only individual energy sources. In this paper, we first propose a method for defining anomalies considering not only individual energy sources but also correlations between them. Then, we propose a new Correlation-driven Multi-Level Multimodal L
    
[^95]: 通过流形展平和重构进行表示学习

    Representation Learning via Manifold Flattening and Reconstruction. (arXiv:2305.01777v1 [cs.LG])

    [http://arxiv.org/abs/2305.01777](http://arxiv.org/abs/2305.01777)

    本文提出了一种通过神经网络构建展平流形的算法FlatNet，具有可解释性和可扩展性，同时在测试数据上具有良好的泛化性能。

    

    本文提出了一种算法，可以从流形的有限样本中显式构建一对神经网络，用于线性化和重构嵌入子流形。我们所生成的神经网络称为展平网络（FlatNet），在理论上具有可解释性，在计算上可扩展性强，并且在测试数据上具有良好的泛化性能，这种平衡通常在基于流形的学习方法中难以实现。我们基于合成的高维流形数据和2D图像数据进行了实证实验，并与其他模型进行了比较。我们的代码是公开的。

    This work proposes an algorithm for explicitly constructing a pair of neural networks that linearize and reconstruct an embedded submanifold, from finite samples of this manifold. Our such-generated neural networks, called flattening networks (FlatNet), are theoretically interpretable, computationally feasible at scale, and generalize well to test data, a balance not typically found in manifold-based learning methods. We present empirical results and comparisons to other models on synthetic high-dimensional manifold data and 2D image data. Our code is publicly available.
    
[^96]: BrainNPT：用于脑网络分类的Transformer网络的预训练

    BrainNPT: Pre-training of Transformer networks for brain network classification. (arXiv:2305.01666v1 [q-bio.NC])

    [http://arxiv.org/abs/2305.01666](http://arxiv.org/abs/2305.01666)

    本文提出了一种名为BrainNPT的基于Transformer的神经网络，用于脑功能网络分类，并提出了两种预训练策略，利用未标记的脑网络数据来学习结构。

    

    近年来，深度学习方法在脑成像分析方面的进展迅速，但往往受到有限标记数据的限制。在未标记数据上预训练的模型已在许多领域中展示了有前景的特征学习改进，包括自然语言处理和计算机视觉。然而，在脑网络分析中，这种技术尚未得到充分探索。在本文中，我们以Transformer网络为基础的预训练方法为重点，利用现有的未标记数据进行脑功能网络分类。首先，我们提出了一种基于Transformer的神经网络，名为BrainNPT，用于脑功能网络分类。所提出的方法利用<cls>标记作为分类嵌入向量，以便于Transformer模型有效地捕获脑网络的表示。其次，我们提出了两种预训练策略的预训练架构，用于BrainNPT模型，以利用未标记的脑网络数据来学习结构。

    Deep learning methods have advanced quickly in brain imaging analysis over the past few years, but they are usually restricted by the limited labeled data. Pre-trained model on unlabeled data has presented promising improvement in feature learning in many domains, including natural language processing and computer vision. However, this technique is under-explored in brain network analysis. In this paper, we focused on pre-training methods with Transformer networks to leverage existing unlabeled data for brain functional network classification. First, we proposed a Transformer-based neural network, named as BrainNPT, for brain functional network classification. The proposed method leveraged <cls> token as a classification embedding vector for the Transformer model to effectively capture the representation of brain network. Second, We proposed a pre-training architecture with two pre-training strategies for BrainNPT model to leverage unlabeled brain network data to learn the structure in
    
[^97]: 基于深度生成先验的数据集蒸馏方法的泛化

    Generalizing Dataset Distillation via Deep Generative Prior. (arXiv:2305.01649v1 [cs.CV])

    [http://arxiv.org/abs/2305.01649](http://arxiv.org/abs/2305.01649)

    该方法提出一种基于预训练深度生成模型的学习先验的数据集蒸馏方法，通过在生成模型的潜在空间中将大量图像蒸馏为少量中间特征向量，显著提高了在所有设置中的跨体系结构的泛化能力。

    

    数据集蒸馏旨在将整个数据集的知识蒸馏到几个合成图像中。其思想是合成少量的合成数据点，并将其作为训练数据提供给学习算法，以得到一个逼近原始数据训练的模型。尽管该领域最近取得了进展，但现有的数据集蒸馏方法无法推广到新的体系结构并扩展到高分辨率数据集。为了克服上述问题，我们建议使用预训练深度生成模型的学习先验来合成蒸馏的数据。为实现这一目的，我们提出了一种新的优化算法，在生成模型的潜在空间中将大量图像蒸馏为少量中间特征向量。我们的方法增强了现有技术，显著提高了在所有设置中的跨体系结构的泛化能力。

    Dataset Distillation aims to distill an entire dataset's knowledge into a few synthetic images. The idea is to synthesize a small number of synthetic data points that, when given to a learning algorithm as training data, result in a model approximating one trained on the original data. Despite recent progress in the field, existing dataset distillation methods fail to generalize to new architectures and scale to high-resolution datasets. To overcome the above issues, we propose to use the learned prior from pre-trained deep generative models to synthesize the distilled data. To achieve this, we present a new optimization algorithm that distills a large number of images into a few intermediate feature vectors in the generative model's latent space. Our method augments existing techniques, significantly improving cross-architecture generalization in all settings.
    
[^98]: 基于Option框架的多模式探索自主非单体智能体

    An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework. (arXiv:2305.01322v1 [cs.AI])

    [http://arxiv.org/abs/2305.01322](http://arxiv.org/abs/2305.01322)

    该论文关注强化学习中的探索研究，提出了一个能够自主管理探索策略的多模式智能体非单体探索方法，并通过实验结果展示了该方法的优越性能。

    

    强化学习领域的探索研究主要关注“如何探索”的探索方式，而“何时探索”的探索研究一直没有成为重点。典型的探索行为通常将探索行为与智能体的开发利用行为绑定在一起。最近出现了非单体探索行为的研究，以研究人类和动物的模式切换行为。本研究的最终目的是使智能体能够自主决定何时探索或利用。我们在Option框架中描述了自主多模式探索的初始研究。通过比较实验结果，我们展示了我们的方法相对于现有的非单体探索方法的更高性能。

    Most exploration research on reinforcement learning (RL) has paid attention to `the way of exploration', which is `how to explore'. The other exploration research, `when to explore', has not been the main focus of RL exploration research. \textcolor{black}{The issue of `when' of a monolithic exploration in the usual RL exploration behaviour binds an exploratory action to an exploitational action of an agent. Recently, a non-monolithic exploration research has emerged to examine the mode-switching exploration behaviour of humans and animals.} The ultimate purpose of our research is to enable an agent to decide when to explore or exploit autonomously. We describe the initial research of an autonomous multi-mode exploration of non-monolithic behaviour in an options framework. The higher performance of our method is shown against the existing non-monolithic exploration method through comparative experimental results.
    
[^99]: 专业知识树在集体决策中解决知识局限性问题

    Expertise Trees Resolve Knowledge Limitations in Collective Decision-Making. (arXiv:2305.01063v1 [cs.AI])

    [http://arxiv.org/abs/2305.01063](http://arxiv.org/abs/2305.01063)

    本研究通过引入专业知识树算法，解决了集体决策中专业知识水平不同的问题，并在多个问题上进行了验证。

    

    向决策者提供建议的专家往往会显示出随问题实例变化而变化的专业知识水平。在实践中，这可能导致针对少数情况的次优或歧视性决策。在本文中，我们将这种知识深度和广度的变化建模为将问题空间划分为不同专业知识区域。我们提供了一些新算法，它们明确考虑并适应问题实例与专家知识之间的关系。我们首先提出并强调了一种基于最近邻查询的天真方法的缺点。为了解决这些问题，我们引入了一种新的算法——专业知识树，它构建决策树，使学习者能够选择适当的模型。我们提供了理论见解，并在一系列现有方法被证明不足以解决问题的问题上进行了经验证实了我们的新方法的改进性能。

    Experts advising decision-makers are likely to display expertise which varies as a function of the problem instance. In practice, this may lead to sub-optimal or discriminatory decisions against minority cases. In this work we model such changes in depth and breadth of knowledge as a partitioning of the problem space into regions of differing expertise. We provide here new algorithms that explicitly consider and adapt to the relationship between problem instances and experts' knowledge. We first propose and highlight the drawbacks of a naive approach based on nearest neighbor queries. To address these drawbacks we then introduce a novel algorithm - expertise trees - that constructs decision trees enabling the learner to select appropriate models. We provide theoretical insights and empirically validate the improved performance of our novel approach on a range of problems for which existing methods proved to be inadequate.
    
[^100]: 基于梯度的最大干扰恢复的域增量3D物体检测

    Gradient-based Maximally Interfered Retrieval for Domain Incremental 3D Object Detection. (arXiv:2304.14460v1 [cs.CV])

    [http://arxiv.org/abs/2304.14460](http://arxiv.org/abs/2304.14460)

    对于域增量3D物体检测，GMIR提出了一种基于梯度的最大干扰恢复策略，可在微调时定期从以前的领域数据集中检索样本。

    

    在所有天气条件下实现准确的3D物体检测仍然是实现自主车辆广泛部署的关键挑战，因为目前大部分工作是在清晰的天气数据上进行的。为了推广到逆境天气条件，监督方法在所有天气数据上从头开始训练表现得最好，而不是在清晰天气数据上微调预训练模型。但是，从所有数据开始训练最终会因数据集不断增长并包含了所有可能的天气条件而变得不可行和昂贵。而在不同天气领域的原始数据上进行简单的微调可能会导致对先前学习领域的灾难性遗忘。受回放式连续学习方法的成功启发，我们提出了基于梯度的最大干扰恢复（GMIR），这是一种对回放进行梯度采样的策略。在微调过程中，GMIR会定期从以前的领域数据集中检索样本。

    Accurate 3D object detection in all weather conditions remains a key challenge to enable the widespread deployment of autonomous vehicles, as most work to date has been performed on clear weather data. In order to generalize to adverse weather conditions, supervised methods perform best if trained from scratch on all weather data instead of finetuning a model pretrained on clear weather data. Training from scratch on all data will eventually become computationally infeasible and expensive as datasets continue to grow and encompass the full extent of possible weather conditions. On the other hand, naive finetuning on data from a different weather domain can result in catastrophic forgetting of the previously learned domain. Inspired by the success of replay-based continual learning methods, we propose Gradient-based Maximally Interfered Retrieval (GMIR), a gradient based sampling strategy for replay. During finetuning, GMIR periodically retrieves samples from the previous domain dataset
    
[^101]: DataComp：寻找下一代多模态数据集

    DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])

    [http://arxiv.org/abs/2304.14108](http://arxiv.org/abs/2304.14108)

    DataComp是一个基准测试，旨在通过提出新的训练集来解决数据集在机器学习生态系统中的缺陷。它提供了一个多规模设计的实验测试平台，使用12.8B个图像-文本对的新候选池，让研究人员可以通过设计新的过滤技术或策划新的数据源并评估它们的新数据集来进行创新。

    

    大型的多模态数据集在近期的突破中起到了关键作用，比如CLIP、Stable Diffusion和GPT-4等。与此同时，数据集很少得到与模型架构或训练算法同等的研究关注。为了解决这个在机器学习生态系统中的缺陷，我们介绍了DataComp，一个基准测试，其中训练代码是固定的，研究人员通过提出新的训练集来进行创新。我们提供了一个基于Common Crawl的新候选池，其中包含12.8B个图像-文本对的数据集实验测试平台。参加我们基准测试的研究人员可以设计新的过滤技术或策划新的数据源，并通过运行我们标准化的CLIP训练代码并在38个下游测试集上进行测试来评估他们的新数据集。我们的基准测试包含多个规模，四个候选池大小和相应的计算预算，在训练期间涵盖了从12.8M到12.8B个样本。这种多规模设计有助于研究规模趋势，并为研究人员提供了更多的选择余地。

    Large multimodal datasets have been instrumental in recent breakthroughs such as CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a benchmark where the training code is fixed and researchers innovate by proposing new training sets. We provide a testbed for dataset experiments centered around a new candidate pool of 12.8B image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing on 38 downstream test sets. Our benchmark consists of multiple scales, with four candidate pool sizes and associated compute budgets ranging from 12.8M to 12.8B samples seen during training. This multi-scale design facilitates the study of scaling trends and makes the
    
[^102]: 学习轨迹是泛化指标

    Learning Trajectories are Generalization Indicators. (arXiv:2304.12579v1 [cs.LG])

    [http://arxiv.org/abs/2304.12579](http://arxiv.org/abs/2304.12579)

    本文研究了DNN的学习轨迹与其在优化后的泛化能力的联系，提出了一种基于学习轨迹复杂性和训练集偏置和多样性比率的新的泛化上界，并通过实验验证了其有效性。

    

    本文旨在研究深度神经网络（DNN）的学习轨迹与其在广泛使用的梯度下降和随机梯度下降算法优化时对应的泛化能力之间的联系。本文构建了线性近似函数来模拟轨迹信息，并在此基础上提出了一种基于更丰富轨迹信息的新的泛化上界。我们提出的泛化上界依赖于学习轨迹的复杂性以及训练集的偏置和多样性比之间的比率。实验结果表明，该方法可以有效地捕捉不同训练步骤、学习率和标签噪声水平下的泛化趋势。

    The aim of this paper is to investigate the connection between learning trajectories of the Deep Neural Networks (DNNs) and their corresponding generalization capabilities when being optimized with broadly used gradient descent and stochastic gradient descent algorithms. In this paper, we construct Linear Approximation Function to model the trajectory information and we propose a new generalization bound with richer trajectory information based on it. Our proposed generalization bound relies on the complexity of learning trajectory and the ratio between the bias and diversity of training set. Experimental results indicate that the proposed method effectively captures the generalization trend across various training steps, learning rates, and label noise levels.
    
[^103]: 压缩与否——自监督学习与信息论:一篇综述

    To Compress or Not to Compress -- Self-Supervised Learning and Information Theory: A Review. (arXiv:2304.09355v1 [cs.LG])

    [http://arxiv.org/abs/2304.09355](http://arxiv.org/abs/2304.09355)

    本文从信息论的角度回顾了各种自监督学习方法，并提出了一个正式的“自监督信息理论学习问题”统一框架。此外，讨论了压缩性和压缩算法在自监督学习中的作用，并凸显了潜在的未来方向。

    

    深度神经网络在监督学习任务中表现出了卓越的性能，但需要大量的标注数据。自监督学习提供了一个替代范例，使得模型可以在没有明确标签的情况下学习。信息论在理解和优化深度神经网络方面起着关键作用。特别地，信息瓶颈原则被应用于在监督设置中优化压缩和相关信息保存之间的权衡。然而，自监督学习中的最佳信息目标仍然不清楚。在本文中，我们从信息论的角度回顾了各种自监督学习方法，并提出了一个正式的“自监督信息理论学习问题”统一框架。我们将现有研究融合成一个一致的框架，研究了最近的自监督方法，并确定了研究机会和挑战。此外，我们还讨论了压缩性和压缩算法在自监督学习中的作用，并凸显了潜在的未来方向。

    Deep neural networks have demonstrated remarkable performance in supervised learning tasks but require large amounts of labeled data. Self-supervised learning offers an alternative paradigm, enabling the model to learn from data without explicit labels. Information theory has been instrumental in understanding and optimizing deep neural networks. Specifically, the information bottleneck principle has been applied to optimize the trade-off between compression and relevant information preservation in supervised settings. However, the optimal information objective in self-supervised learning remains unclear. In this paper, we review various approaches to self-supervised learning from an information-theoretic standpoint and present a unified framework that formalizes the \textit{self-supervised information-theoretic learning problem}. We integrate existing research into a coherent framework, examine recent self-supervised methods, and identify research opportunities and challenges. Moreove
    
[^104]: 面部视频压缩的感知质量评估：基准和有效方法

    Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method. (arXiv:2304.07056v1 [eess.IV])

    [http://arxiv.org/abs/2304.07056](http://arxiv.org/abs/2304.07056)

    本文介绍了大规模压缩面部视频质量评估（CFVQA）数据库，用于系统地了解面部视频感知质量和多样化压缩失真。生成式编码方法被确定为具有合理的感知码率失真折衷的有前途的替代方法，利用面部视频的统计先验知识。

    

    近年来，对面部视频压缩的需求呈指数级增长，人工智能的成功使得超出了传统的混合视频编码范围。生成式编码方法被确定为具有合理的感知码率失真折衷的有前途的替代方法，利用面部视频的统计先验知识。然而，空间和时间域中扭曲类型的极大多样性，从传统的混合编码框架到生成模型，给压缩面部视频质量评估（VQA）带来了巨大挑战。在本文中，我们介绍了大规模压缩面部视频质量评估（CFVQA）数据库，这是系统地了解面部视频感知质量和多样化压缩失真的第一次尝试。该数据库包含 3,240 个压缩的面部视频片段，涵盖多个压缩级别，这些片段来自 135 个源视频，具有多样性。

    Recent years have witnessed an exponential increase in the demand for face video compression, and the success of artificial intelligence has expanded the boundaries beyond traditional hybrid video coding. Generative coding approaches have been identified as promising alternatives with reasonable perceptual rate-distortion trade-offs, leveraging the statistical priors of face videos. However, the great diversity of distortion types in spatial and temporal domains, ranging from the traditional hybrid coding frameworks to generative models, present grand challenges in compressed face video quality assessment (VQA). In this paper, we introduce the large-scale Compressed Face Video Quality Assessment (CFVQA) database, which is the first attempt to systematically understand the perceptual quality and diversified compression distortions in face videos. The database contains 3,240 compressed face video clips in multiple compression levels, which are derived from 135 source videos with diversif
    
[^105]: 通过可扩展的主题嵌入从连续新闻流中无监督地发现故事

    Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding. (arXiv:2304.04099v1 [cs.IR])

    [http://arxiv.org/abs/2304.04099](http://arxiv.org/abs/2304.04099)

    本研究提出了一种新颖的主题嵌入方法和一个可扩展的无监督在线故事发现框架USTORY，可以动态表示文章和故事，并考虑它们共享的时间主题和新颖性，以帮助人们消化大量的新闻流。

    

    无监督地发现实时相关新闻文章故事，有助于人们在不需要昂贵人工注释的情况下消化大量的新闻流。现有的无监督在线故事发现研究的普遍方法是用符号或基于图的嵌入来表示新闻文章，并将它们逐步聚类成故事。最近的大型语言模型有望进一步改善嵌入，但是通过无差别地编码文章中的所有信息来直接采用这些模型无法有效处理富含文本且不断发展的新闻流。在这项工作中，我们提出了一种新颖的主题嵌入方法，使用现成的预训练句子编码器来动态表示文章和故事，并考虑它们共享的时间主题。为了实现无监督在线故事发现的想法，引入了一个可扩展框架USTORY，包括两个主要技术，即主题和时间感知的动态嵌入和新颖性感知的自适应聚类。

    Unsupervised discovery of stories with correlated news articles in real-time helps people digest massive news streams without expensive human annotations. A common approach of the existing studies for unsupervised online story discovery is to represent news articles with symbolic- or graph-based embedding and incrementally cluster them into stories. Recent large language models are expected to improve the embedding further, but a straightforward adoption of the models by indiscriminately encoding all information in articles is ineffective to deal with text-rich and evolving news streams. In this work, we propose a novel thematic embedding with an off-the-shelf pretrained sentence encoder to dynamically represent articles and stories by considering their shared temporal themes. To realize the idea for unsupervised online story discovery, a scalable framework USTORY is introduced with two main techniques, theme- and time-aware dynamic embedding and novelty-aware adaptive clustering, fuel
    
[^106]: 用图论统一刻画差分隐私可学习性

    A Unified Characterization of Private Learnability via Graph Theory. (arXiv:2304.03996v1 [cs.LG])

    [http://arxiv.org/abs/2304.03996](http://arxiv.org/abs/2304.03996)

    本文提供了一个统一的框架，使用图论的语言刻画了差分隐私的两种情形下，纯粹和近似的学习性。我们通过定义矛盾图$G$来捕捉 $\mathcal{H}$ 的组合结构，发现分数团数和团数是描述差分隐私学习性的重要因素，并提出了几种算法对其进行估计。

    

    我们提供了一个统一的框架来刻画纯粹的和近似的差分隐私学习性。该框架使用了图论的语言:对于一个概念类 $\mathcal{H}$,我们定义了 $\mathcal{H}$ 的矛盾图 $G$。它的顶点是可实现的数据集，如果两个数据集 $S$，$S'$ 相互矛盾(即，在 $S$ 和 $S'$ 中有一个点 $x$ 具有不同的标记)，则它们之间有一条边连接。我们的主要发现是，$G$ 的组合结构与在差分隐私下学习 $\mathcal{H}$ 密切相关。在纯粹的差分隐私下学习 $\mathcal{H}$ 的捕获为 $G$ 的分数团数。在近似差分隐私下学习 $\mathcal{H}$ 的捕获为 $G$ 的团数。因此，我们确定了描述差分隐私可学习性的图论维度：团维和分数团维。同时，我们揭示了矛盾图的一些性质，这些性质可能是独立感兴趣的。我们还提出了几种算法来估计 $G$ 的这些度量，通过这些算法，我们实现了几种概念类的实验研究。

    We provide a unified framework for characterizing pure and approximate differentially private (DP) learnabiliity. The framework uses the language of graph theory: for a concept class $\mathcal{H}$, we define the contradiction graph $G$ of $\mathcal{H}$. It vertices are realizable datasets, and two datasets $S,S'$ are connected by an edge if they contradict each other (i.e., there is a point $x$ that is labeled differently in $S$ and $S'$). Our main finding is that the combinatorial structure of $G$ is deeply related to learning $\mathcal{H}$ under DP. Learning $\mathcal{H}$ under pure DP is captured by the fractional clique number of $G$. Learning $\mathcal{H}$ under approximate DP is captured by the clique number of $G$. Consequently, we identify graph-theoretic dimensions that characterize DP learnability: the clique dimension and fractional clique dimension. Along the way, we reveal properties of the contradiction graph which may be of independent interest. We also suggest several o
    
[^107]: 生成AI用于学习：研究合成学习视频的潜力。

    Generative AI for learning: Investigating the potential of synthetic learning videos. (arXiv:2304.03784v1 [cs.CV])

    [http://arxiv.org/abs/2304.03784](http://arxiv.org/abs/2304.03784)

    本研究探讨了使用生成AI合成视频来创建在线教育内容的效用，使用混合方法和成年学习者进行实验。结果显示，合成学习视频对学习内容获取和学习体验有着积极的影响。

    

    最近，生成人工智能（AI）的最新进展已经引起了全球的关注。像Dalle-2和ChatGPT这样的工具表明，以前被认为超出了AI能力的任务现在可以以各种新方式增加创意媒体的生产力，包括生成合成视频。本研究探讨了使用生成AI合成视频来创建在在线教育环境下可行的教育内容的效用。到目前为止，还没有研究调查AI生成的合成媒体在现实世界中的教育价值。为了填补这一空白，我们采用混合方法随机将成年学习者（n = 83）分配到两种微型学习条件之一，收集前后学习评估，并调查参与者的学习体验。控制组

    Recent advances in generative artificial intelligence (AI) have captured worldwide attention. Tools such as Dalle-2 and ChatGPT suggest that tasks previously thought to be beyond the capabilities of AI may now augment the productivity of creative media in various new ways, including through the generation of synthetic video. This research paper explores the utility of using AI-generated synthetic video to create viable educational content for online educational settings. To date, there is limited research investigating the real-world educational value of AI-generated synthetic media. To address this gap, we examined the impact of using AI-generated synthetic video in an online learning platform on both learners content acquisition and learning experience. We took a mixed-method approach, randomly assigning adult learners (n=83) into one of two micro-learning conditions, collecting pre- and post-learning assessments, and surveying participants on their learning experience. The control c
    
[^108]: DR.CPO：通过迭代构建、随机放置和 HPR 遮蔽实现的多样化和逼真的三维增强

    DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion. (arXiv:2303.12743v1 [cs.CV])

    [http://arxiv.org/abs/2303.12743](http://arxiv.org/abs/2303.12743)

    该论文提出了一种多样化和逼真的增强方法，可以创建整体对象并灵活地定位和旋转对象，并相应地应用自遮挡和外遮挡。通过迭代构建多个对象来提高整体对象构造的多样性，构造的对象可以在训练帧中随机放置和旋转。

    

    在自动驾驶中，数据增强常用于改进三维物体检测。最基本的方法包括插入复制对象和旋转和缩放整个训练帧。也已经开发了许多变体。然而，现有方法与现实世界的可能性相比相当有限。在这项工作中，我们开发了一种多样化和逼真增强方法，可以灵活地构造整体对象，自由地定位和旋转对象，并相应地应用自遮挡和外遮挡。为了提高整体对象构造的多样性，我们开发了一种迭代方法，将从现实世界观察到的多个对象随机组合成单个对象。与现有增强方法不同的是，构造的对象可以随机放置和旋转在训练帧中，因为适当的遮挡可以反映在最终整体对象中。最后，为了防止过度增强导致过拟合，我们介绍了一种分层遮挡概率设置，通过对象的位置和大小调整遮挡强度。

    In autonomous driving, data augmentation is commonly used for improving 3D object detection. The most basic methods include insertion of copied objects and rotation and scaling of the entire training frame. Numerous variants have been developed as well. The existing methods, however, are considerably limited when compared to the variety of the real world possibilities. In this work, we develop a diversified and realistic augmentation method that can flexibly construct a whole-body object, freely locate and rotate the object, and apply self-occlusion and external-occlusion accordingly. To improve the diversity of the whole-body object construction, we develop an iterative method that stochastically combines multiple objects observed from the real world into a single object. Unlike the existing augmentation methods, the constructed objects can be randomly located and rotated in the training frame because proper occlusions can be reflected to the whole-body objects in the final step. Fina
    
[^109]: DR-VIDAL--双重稳健变分信息论深度对抗学习用于真实世界数据的反事实预测和治疗效果估计

    DR-VIDAL -- Doubly Robust Variational Information-theoretic Deep Adversarial Learning for Counterfactual Prediction and Treatment Effect Estimation on Real World Data. (arXiv:2303.04201v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04201](http://arxiv.org/abs/2303.04201)

    DR-VIDAL是一个新型的生成框架，可用于处理真实世界数据中的干预措施对结果的因果效应估计，并具有处理混淆偏差和模型不良的能力。

    

    从真实世界的观察性（非随机化）数据中确定干预措施对结果的因果效应，例如使用电子健康记录的治疗重用，由于潜在偏差而具有挑战性。因果深度学习已经改进了传统技术，用于估计个性化治疗效果（ITE）。我们提出了双重稳健变分信息论深度对抗学习（DR-VIDAL），这是一个结合了治疗和结果两个联合模型的新型生成框架，确保无偏的ITE估计，即使其中一个模型设定不正确。DR-VIDAL整合了： （i）变分自编码器（VAE）根据因果假设将混淆变量分解为潜在变量; （ii）基于信息论的生成对抗网络（Info-GAN）用于生成反事实情况; （iii）一个双重稳健块，其中包括治疗倾向于预测结果。在合成和真实数据集（Infant Health和Development Program，Transforming Clinical Practice Initiative [TCPI]）中进行实验，我们证明了DR-VIDAL在估计ITE方面优于现有的最先进方法，因为它具有处理混淆偏差和模型不正确的能力。

    Determining causal effects of interventions onto outcomes from real-world, observational (non-randomized) data, e.g., treatment repurposing using electronic health records, is challenging due to underlying bias. Causal deep learning has improved over traditional techniques for estimating individualized treatment effects (ITE). We present the Doubly Robust Variational Information-theoretic Deep Adversarial Learning (DR-VIDAL), a novel generative framework that combines two joint models of treatment and outcome, ensuring an unbiased ITE estimation even when one of the two is misspecified. DR-VIDAL integrates: (i) a variational autoencoder (VAE) to factorize confounders into latent variables according to causal assumptions; (ii) an information-theoretic generative adversarial network (Info-GAN) to generate counterfactuals; (iii) a doubly robust block incorporating treatment propensities for outcome predictions. On synthetic and real-world datasets (Infant Health and Development Program, T
    
[^110]: 无监督病理检测：深入探究现有技术的最新进展

    Unsupervised Pathology Detection: A Deep Dive Into the State of the Art. (arXiv:2303.00609v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.00609](http://arxiv.org/abs/2303.00609)

    本文深入研究了无监督病理检测技术的最新发展，通过评估和基准测试多种尖端方法，证明了工业和医疗文献中新开发的特征建模方法在各种模态和数据集上创立了新的技术水平。

    

    深度无监督方法正在越来越多地被用于医学图像中的病理检测和分割应用，因为它们承诺可以减轻对大规模标注数据集的需求，并且在检测任何一种罕见病理方面比有监督方法更具普适性。随着无监督异常检测 (UAD) 文献的不断增长和新的范式的出现，重要的是要在一个公共框架中不断地评估和基准测试新的方法，以重新评估最新技术进展，并确定有前途的研究方向。为此，我们在多个医学数据集上评估了多种尖端 UAD 方法，并将其与已在脑 MRI 的 UAD 中确立的最新技术进展进行比较。我们的实验表明，工业和医疗文献中新开发的特征建模方法相对于之前的工作实现了更高的性能，并在各种模态和数据集上创立了新的技术水平。

    Deep unsupervised approaches are gathering increased attention for applications such as pathology detection and segmentation in medical images since they promise to alleviate the need for large labeled datasets and are more generalizable than their supervised counterparts in detecting any kind of rare pathology. As the Unsupervised Anomaly Detection (UAD) literature continuously grows and new paradigms emerge, it is vital to continuously evaluate and benchmark new methods in a common framework, in order to reassess the state-of-the-art (SOTA) and identify promising research directions. To this end, we evaluate a diverse selection of cutting-edge UAD methods on multiple medical datasets, comparing them against the established SOTA in UAD for brain MRI. Our experiments demonstrate that newly developed feature-modeling methods from the industrial and medical literature achieve increased performance compared to previous work and set the new SOTA in a variety of modalities and datasets. Add
    
[^111]: 面向领域的预训练提高了整张切片图像分类的信心

    Domain-Specific Pre-training Improves Confidence in Whole Slide Image Classification. (arXiv:2302.09833v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.09833](http://arxiv.org/abs/2302.09833)

    本文研究了面向领域的预训练对整张切片图像分类的作用，发现使用面向领域的预训练可以提高多实例学习模型在切片图像分类任务中的性能。

    

    整张切片图像或组织病理学图像在数字病理学中被广泛使用。由于其大小和缺乏像素级注释，整张切片图像对于临床诊断的深度学习模型构成了巨大挑战。最近在计算病理学领域，提出了基于多实例学习的新模型。多实例学习需要创建补丁，并使用这些补丁的编码进行诊断。这些模型使用通用的预训练模型（在ImageNet上预训练的ResNet-50）进行补丁编码。最近提出的KimiaNet是一种基于DenseNet121的面向领域的预训练模型，该模型是在TCGA切片上进行预训练的。本文展示了面向领域的预训练对整张切片图像分类的影响。为了调查面向领域的预训练的效果，我们考虑了当前最先进的多实例学习模型：1）基于注意力的CLAM模型和2）自注意的TransMIL模型，并评估了模型的性能。

    Whole Slide Images (WSIs) or histopathology images are used in digital pathology. WSIs pose great challenges to deep learning models for clinical diagnosis, owing to their size and lack of pixel-level annotations. With the recent advancements in computational pathology, newer multiple-instance learning-based models have been proposed. Multiple-instance learning for WSIs necessitates creating patches and uses the encoding of these patches for diagnosis. These models use generic pre-trained models (ResNet-50 pre-trained on ImageNet) for patch encoding. The recently proposed KimiaNet, a DenseNet121 model pre-trained on TCGA slides, is a domain-specific pre-trained model. This paper shows the effect of domain-specific pre-training on WSI classification. To investigate the effect of domain-specific pre-training, we considered the current state-of-the-art multiple-instance learning models, 1) CLAM, an attention-based model, and 2) TransMIL, a self-attention-based model, and evaluated the mod
    
[^112]: 基于物理参数化神经常微分方程的火箭燃烧室激光起爆预测

    Physics-based parameterized neural ordinary differential equations: prediction of laser ignition in a rocket combustor. (arXiv:2302.08629v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08629](http://arxiv.org/abs/2302.08629)

    本论文提出了一种基于参数化神经常微分方程的物理数据驱动框架，用于快速、准确地对模型火箭燃烧室中的激光起爆进行预测。

    

    本文提出了一种新颖的基于物理的数据驱动框架，用于基于参数化神经常微分方程(PNODE)对模型火箭燃烧室激光起爆进行降阶建模。深度神经网络作为激光起爆高维参数的函数嵌入其中，以预测包括热源函数、预指数因子和活化能等在内的0D流动模型中的各种术语。使用0D流动模型的控制方程，我们的PNODE只需要有限数量的训练样本，就可以预测温度、压力和物种的质量分数等各种数量的轨迹，同时满足物理约束条件。我们在高保真计算流体动力学(CFD)模拟激光诱导点火的原型火箭燃烧室的解决快照上验证了基于物理的PNODE的有效性。我们将基于物理的PNODE的性能与核岭回归和全连接神经网络进行了比较。

    In this work, we present a novel physics-based data-driven framework for reduced-order modeling of laser ignition in a model rocket combustor based on parameterized neural ordinary differential equations (PNODE). Deep neural networks are embedded as functions of high-dimensional parameters of laser ignition to predict various terms in a 0D flow model including the heat source function, pre-exponential factors, and activation energy. Using the governing equations of a 0D flow model, our PNODE needs only a limited number of training samples and predicts trajectories of various quantities such as temperature, pressure, and mass fractions of species while satisfying physical constraints. We validate our physics-based PNODE on solution snapshots of high-fidelity Computational Fluid Dynamics (CFD) simulations of laser-induced ignition in a prototype rocket combustor. We compare the performance of our physics-based PNODE with that of kernel ridge regression and fully connected neural networks
    
[^113]: 学习如何推断部分马尔可夫决策过程进行上下文适应和探索

    Learning How to Infer Partial MDPs for In-Context Adaptation and Exploration. (arXiv:2302.04250v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04250](http://arxiv.org/abs/2302.04250)

    本论文介绍了一种通过学习部分马尔可夫决策过程来进行上下文适应和探索的新方法，其使用变压器进行推理过程学习，考虑了模型假设空间，假设表示为小的马尔可夫决策过程，可以在性价比高的情况下进行动态规划。该方法在Symbolic Alchemy基准测试中表现出与精确后验抽样相近的适应速度和探索利用平衡。

    

    为了在任务间进行泛化，智能体应该从过去的任务中获取知识，以促进未来任务中的适应和探索。我们关注上下文适应和探索问题，其中一个智能体只依赖于上下文，即状态、动作和/或奖励的历史记录，而不是梯度更新。后验抽样是一种有前途的方法，但它需要贝叶斯推理和动态规划，通常涉及未知量（例如，先验）和昂贵的计算。为了解决这些困难，我们使用一个变压器来从训练任务中学习推理过程，并考虑一个假设空间的部分模型，表示为小的马尔可夫决策过程，这对于动态规划来说是廉价的。在我们版本的Symbolic Alchemy基准测试中，我们的方法的适应速度和探索利用平衡接近于精确的后验抽样神谕。我们还展示了即使部分模型排除了r

    To generalize across tasks, an agent should acquire knowledge from past tasks that facilitate adaptation and exploration in future tasks. We focus on the problem of in-context adaptation and exploration, where an agent only relies on context, i.e., history of states, actions and/or rewards, rather than gradient-based updates. Posterior sampling (extension of Thompson sampling) is a promising approach, but it requires Bayesian inference and dynamic programming, which often involve unknowns (e.g., a prior) and costly computations. To address these difficulties, we use a transformer to learn an inference process from training tasks and consider a hypothesis space of partial models, represented as small Markov decision processes that are cheap for dynamic programming. In our version of the Symbolic Alchemy benchmark, our method's adaptation speed and exploration-exploitation balance approach those of an exact posterior sampling oracle. We also show that even though partial models exclude r
    
[^114]: 检测相关数据库中的相变

    Phase Transitions in the Detection of Correlated Databases. (arXiv:2302.03380v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03380](http://arxiv.org/abs/2302.03380)

    研究了检测相关数据库中的相变问题，根据$n$和$d$的渐近规律确定了最优检测展现出相变的尖锐阈值，补充了矩阵感应模型中相位恢复的性能。

    

    我们研究了检测由$n$个用户组成的两个高斯数据库$\mathsf{X}\in\mathbb{R}^{n\times d}$和$\mathsf{Y}^{n\times d}$之间相关性的问题。我们将其作为一个假设检验问题：在零假设下，这两个数据库是统计独立的。在替代假设下，存在一个未知排列$\sigma$（或行排列），使得$\mathsf{X}$与$\mathsf{Y}^\sigma$相关。我们确定了在$n$和$d$的渐近区域下，最优检测显示出相变的尖锐阈值。具体而言，我们证明如果$\rho^2d\to 0$，当$d\to \infty$时，则无论$n$的值如何，弱检测（略优于随机猜测）是不可能的。这补充了矩阵感应模型中相位恢复的性能，该模型的目标是恢复低秩矩阵，而不是检测两个矩阵之间的相关性。

    We study the problem of detecting the correlation between two Gaussian databases $\mathsf{X}\in\mathbb{R}^{n\times d}$ and $\mathsf{Y}^{n\times d}$, each composed of $n$ users with $d$ features. This problem is relevant in the analysis of social media, computational biology, etc. We formulate this as a hypothesis testing problem: under the null hypothesis, these two databases are statistically independent. Under the alternative, however, there exists an unknown permutation $\sigma$ over the set of $n$ users (or, row permutation), such that $\mathsf{X}$ is $\rho$-correlated with $\mathsf{Y}^\sigma$, a permuted version of $\mathsf{Y}$. We determine sharp thresholds at which optimal testing exhibits a phase transition, depending on the asymptotic regime of $n$ and $d$. Specifically, we prove that if $\rho^2d\to0$, as $d\to\infty$, then weak detection (performing slightly better than random guessing) is statistically impossible, irrespectively of the value of $n$. This compliments the perf
    
[^115]: 通过层变分分析解释领域适应

    Interpretations of Domain Adaptations via Layer Variational Analysis. (arXiv:2302.01798v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01798](http://arxiv.org/abs/2302.01798)

    本研究通过层变分分析证明了转移学习的成功可以通过相应的数据条件得到保证，并提出了一种基于网络的转移学习的替代方法，该方法在领域适应方面显示出了效率和准确性的提高。

    This study establishes the theory of transfer learning in deep learning through formal derivations and heuristic analysis, proving that the success of transfer learning can be guaranteed with corresponding data conditions. An alternative method for network-based transfer learning is proposed, which shows an increase in efficiency and accuracy for domain adaptation.

    转移学习在许多应用中表现出高效的性能，但有限的文献报道了其背后的机制。本研究建立了正式的推导和启发式分析，以制定深度学习中转移学习的理论。我们的框架利用层变分分析证明了转移学习的成功可以通过相应的数据条件得到保证。此外，我们的理论计算产生了对知识转移过程的直观解释。随后，我们推导出了一种基于网络的转移学习的替代方法。该方法在领域适应方面显示出了效率和准确性的提高。当适应期间的新领域数据足够稀疏时，它特别有优势。对各种任务的数值实验验证了我们的理论，并验证了我们的分析表达式在领域适应方面比梯度下降方法表现更好。

    Transfer learning is known to perform efficiently in many applications empirically, yet limited literature reports the mechanism behind the scene. This study establishes both formal derivations and heuristic analysis to formulate the theory of transfer learning in deep learning. Our framework utilizing layer variational analysis proves that the success of transfer learning can be guaranteed with corresponding data conditions. Moreover, our theoretical calculation yields intuitive interpretations towards the knowledge transfer process. Subsequently, an alternative method for network-based transfer learning is derived. The method shows an increase in efficiency and accuracy for domain adaptation. It is particularly advantageous when new domain data is sufficiently sparse during adaptation. Numerical experiments over diverse tasks validated our theory and verified that our analytic expression achieved better performance in domain adaptation than the gradient descent method.
    
[^116]: Transformers训练的高效方法综述

    A Survey on Efficient Training of Transformers. (arXiv:2302.01107v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01107](http://arxiv.org/abs/2302.01107)

    本文是对高效训练Transformer领域的系统综述，分析和比较了在训练中为中间张量节省计算和内存成本的方法与硬件/算法共同设计的技术，并探讨了未来研究的挑战和前景。

    

    近年来，Transformers的发展为计算资源提出了巨大要求，强调了开发高效训练技术以通过有效利用计算和内存资源使Transformer的训练更快、更低成本且更高精度的重要性。本研究提供了高效训练Transformer的首个系统综述，覆盖了加速算术和硬件领域的最新进展，重点关注前者。我们分析和比较了在训练期间为中间张量节省计算和内存成本的方法，以及硬件/算法共同设计的技术。最后，我们讨论了未来研究的挑战和有前途的领域。

    Recent advances in Transformers have come with a huge requirement on computing resources, highlighting the importance of developing efficient training techniques to make Transformer training faster, at lower cost, and to higher accuracy by the efficient use of computation and memory resources. This survey provides the first systematic overview of the efficient training of Transformers, covering the recent progress in acceleration arithmetic and hardware, with a focus on the former. We analyze and compare methods that save computation and memory costs for intermediate tensors during training, together with techniques on hardware/algorithm co-design. We finally discuss challenges and promising areas for future research.
    
[^117]: 探索和利用辅助数据来改善小样本泛化问题

    Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data. (arXiv:2302.00674v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00674](http://arxiv.org/abs/2302.00674)

    本文提出了一种在少样本学习过程中假定有辅助数据的训练范式FLAD，并针对自动混合辅助和目标数据的方法局限，提出了两种计算复杂度独立于辅助数据集数量的算法，通过FLAD和这两种算法的比较，可以发现这两种算法的表现更好。

    

    小样本学习在许多实际应用中都有价值，但学习一个通用的模型且不过度拟合少数标记数据点是具有挑战性的。本文关注辅助数据的小样本学习（FLAD），一种在少样本学习过程中假定有辅助数据的训练范式，以期提高泛化性能。先前的工作已经提出了自动混合辅助和目标数据的方法，但这些方法通常随辅助数据集的数量呈线性（或更差）缩放，从而限制了它们的实用性。在本文中，我们将FLAD与在多臂老虎机设置中至关重要的探索与利用困境联系起来，并推导出算法，其计算复杂度独立于辅助数据集的数量，从而使我们能够扩展到比先前方法多100倍的辅助数据集。我们提出了两种算法——EXP3-FLAD和UCB1-FLAD，并将它们与先前只进行探索或利用的FLAD方法进行了比较，发现这些算法表现更好。

    Few-shot learning is valuable in many real-world applications, but learning a generalizable model without overfitting to the few labeled datapoints is challenging. In this work, we focus on Few-shot Learning with Auxiliary Data (FLAD), a training paradigm that assumes access to auxiliary data during few-shot learning in hopes of improving generalization. Previous works have proposed automated methods for mixing auxiliary and target data, but these methods typically scale linearly (or worse) with the number of auxiliary datasets, limiting their practicality. In this work we relate FLAD to the explore-exploit dilemma that is central to the multi-armed bandit setting and derive algorithms whose computational complexity is independent of the number of auxiliary datasets, allowing us to scale to 100x more auxiliary datasets than prior methods. We propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- and compare them with prior FLAD methods that either explore or exploit, finding that the com
    
[^118]: 学习保持拓扑结构的数据表示

    Learning Topology-Preserving Data Representations. (arXiv:2302.00136v2 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2302.00136](http://arxiv.org/abs/2302.00136)

    本文提出了一种名为RTD-AE的方法用于学习保持数据拓扑结构的降维表示,在保留全局结构和拓扑性方面，其表现优于现有最先进竞争对手。

    

    本文提出了一种用于学习拓扑保持数据表示（降维）的方法。该方法旨在通过强制拓扑特征（聚类、环、2D空洞等）及其本地化的相似性提供数据流形和其潜在表示之间的拓扑相似性。该方法的核心是在潜在空间中在原始高维数据和低维表示之间最小化表示拓扑散度（RTD）。RTD最小化提供了强有力的理论保证，拓扑特征的相似性。我们开发了一种RTD分化方案，并将其应用为自编码器损失项。 RTD-AE方法与现有最先进竞争对手相比，通过线性相关、三重距离排名准确性以及持久条形码之间的Wasserstein距离等测量，更好地保留了数据流形的全局结构和拓扑性。

    We propose a method for learning topology-preserving data representations (dimensionality reduction). The method aims to provide topological similarity between the data manifold and its latent representation via enforcing the similarity in topological features (clusters, loops, 2D voids, etc.) and their localization. The core of the method is the minimization of the Representation Topology Divergence (RTD) between original high-dimensional data and low-dimensional representation in latent space. RTD minimization provides closeness in topological features with strong theoretical guarantees. We develop a scheme for RTD differentiation and apply it as a loss term for the autoencoder. The proposed method "RTD-AE" better preserves the global structure and topology of the data manifold than state-of-the-art competitors as measured by linear correlation, triplet distance ranking accuracy, and Wasserstein distance between persistence barcodes.
    
[^119]: 多项式Logit模型中最优产品组合的组合推断

    Combinatorial Inference on the Optimal Assortment in Multinomial Logit Models. (arXiv:2301.12254v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.12254](http://arxiv.org/abs/2301.12254)

    本文提出了一种基于多项式logit模型的推断框架，可以测试最优产品组合是否具有特定性质。

    

    最优的产品组合优化已经成为实践中的重要问题。本文提出了一种新的推断框架，用于测试最优产品组合是否具有特定性质。我们考虑了广泛采用的多项式logit（MNL）模型，并将其用于研究最优组合问题。

    Assortment optimization has received active explorations in the past few decades due to its practical importance. Despite the extensive literature dealing with optimization algorithms and latent score estimation, uncertainty quantification for the optimal assortment still needs to be explored and is of great practical significance. Instead of estimating and recovering the complete optimal offer set, decision-makers may only be interested in testing whether a given property holds true for the optimal assortment, such as whether they should include several products of interest in the optimal set, or how many categories of products the optimal set should include. This paper proposes a novel inferential framework for testing such properties. We consider the widely adopted multinomial logit (MNL) model, where we assume that each customer will purchase an item within the offered products with a probability proportional to the underlying preference score associated with the product. We reduce
    
[^120]: 在计算代数系统中引入背景知识的符号回归

    Incorporating Background Knowledge in Symbolic Regression using a Computer Algebra System. (arXiv:2301.11919v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11919](http://arxiv.org/abs/2301.11919)

    通过增加软约束，将背景知识（符号数学约束）纳入符号回归（SR）可以提高其搜索效率和模型意义性，从而生成与理论相关且与数据一致的表达式。

    

    符号回归（SR）可以生成符合给定数据集的解释性精简表达式，比黑匣子方法更增加了人类对结构的理解。加入背景知识（以符号数学约束形式）允许生成与理论相关且与数据一致的表达式。我们特别研究了将约束添加到传统的基于遗传算法（GA）的SR（PySR）以及基于马尔可夫链蒙特卡洛（MCMC）的贝叶斯SR体系结构（贝叶斯机器科学家）中，并将这些应用于从实验、历史数据集中重新发现吸附方程。我们发现，虽然硬约束会妨碍GA和MCMC SR的搜索，但软约束可以提高搜索有效性和模型意义性，计算成本增加了一个数量级。如果约束与数据不相关，则可能会损害表现。

    Symbolic Regression (SR) can generate interpretable, concise expressions that fit a given dataset, allowing for more human understanding of the structure than black-box approaches. The addition of background knowledge (in the form of symbolic mathematical constraints) allows for the generation of expressions that are meaningful with respect to theory while also being consistent with data. We specifically examine the addition of constraints to traditional genetic algorithm (GA) based SR (PySR) as well as a Markov-chain Monte Carlo (MCMC) based Bayesian SR architecture (Bayesian Machine Scientist), and apply these to rediscovering adsorption equations from experimental, historical datasets. We find that, while hard constraints prevent GA and MCMC SR from searching, soft constraints can lead to improved performance both in terms of search effectiveness and model meaningfulness, with computational costs increasing by about an order-of-magnitude. If the constraints do not correlate well wit
    
[^121]: 大型语言模型可被视为隐含的主题模型：解释和寻找好的示范以实现上下文学习

    Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11916](http://arxiv.org/abs/2301.11916)

    本研究发现，大型语言模型可以被视为隐式的主题模型，并提出了一种算法，从注释数据中选择最佳示范，大大提高了上下文学习的能力。

    

    近年来，预训练的大型语言模型表现出了在推理时实现少量样本学习能力的显著效率，被称为上下文学习。 然而，现有文献强调这种能力对少量样本示范的选择很敏感。本研究旨在通过贝叶斯视角研究上下文学习现象，将大型语言模型视为从示范中隐含地推断出相关信息的主题模型。在此前提下，我们提出了一种算法，用于从一组注释数据中选择最佳示范，并证明相对于随机选择基线的平均值，在八个不同的真实文本分类数据集上平均每个 GPT2 和 GPT3 模型有显着的 12.5% 的提升。我们的实证发现支持我们的假设，即大型语言模型可被视为隐含的主题模型。

    In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that la
    
[^122]: 基于次流形假设下扩散模型奇异性的数学分析

    Mathematical analysis of singularities in the diffusion model under the submanifold assumption. (arXiv:2301.07882v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.07882](http://arxiv.org/abs/2301.07882)

    本文提供了扩散模型中漂移项的数学分析。通过次流形假设，提出一种新的目标函数和相关的损失函数，可处理低维流形上的奇异数据分布，解决了均值漂移函数和得分函数渐近发散的问题。

    

    本文提供了机器学习中扩散模型的数学分析。以条件期望表示反向采样流程的漂移项，其中涉及数据分布和前向扩散。训练过程旨在通过最小化与条件期望相关的均方残差来寻找此类漂移函数。使用前向扩散的Green函数的小时间近似，我们证明了DDPM中的解析均值漂移函数和SGM中的得分函数在采样过程的最后阶段，对于像那些集中在低维流形上的奇异数据分布而言，渐近地发散，因此难以通过网络进行逼近。为了克服这个困难，我们推导出了一个新的目标函数和相关的损失函数，即使在处理奇异数据分布时仍然保持有界。我们通过几个数值实验来说明理论发现。

    This paper provide several mathematical analyses of the diffusion model in machine learning. The drift term of the backwards sampling process is represented as a conditional expectation involving the data distribution and the forward diffusion. The training process aims to find such a drift function by minimizing the mean-squared residue related to the conditional expectation. Using small-time approximations of the Green's function of the forward diffusion, we show that the analytical mean drift function in DDPM and the score function in SGM asymptotically blow up in the final stages of the sampling process for singular data distributions such as those concentrated on lower-dimensional manifolds, and is therefore difficult to approximate by a network. To overcome this difficulty, we derive a new target function and associated loss, which remains bounded even for singular data distributions. We illustrate the theoretical findings with several numerical examples.
    
[^123]: 通过训练动态理解基于坐标的MLPs的谱偏置

    Understanding the Spectral Bias of Coordinate Based MLPs Via Training Dynamics. (arXiv:2301.05816v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05816](http://arxiv.org/abs/2301.05816)

    该论文研究了基于坐标的MLPs的谱偏置对高频组件收敛的阻碍，并提出使用高频正弦波编码输入来克服这一限制。

    

    谱偏置是神经网络训练的重要观察结果，它表示网络在收敛到更高频率组件前，会学习目标函数的低频表示。这一属性与超参数网络的良好泛化能力有关，但在应用于场景渲染时，采用具有ReLU激活的多层感知器(MLPs)利用密集的低维坐标输入会导致严重的谱偏差，完全阻碍了收敛到高频组件。为了克服这个限制，可以使用高频正弦波编码输入。以前的研究试图使用神经切向核(NTK)和傅里叶分析来解释坐标系中的谱偏差及其严重性。然而，这种方法存在各种限制，因为NTK不能捕捉到真正的网络动态，而傅里叶分析只能提供对频率组件的全局视角。

    Spectral bias is an important observation of neural network training, stating that the network will learn a low frequency representation of the target function before converging to higher frequency components. This property is interesting due to its link to good generalization in over-parameterized networks. However, in applications to scene rendering, where multi-layer perceptrons (MLPs) with ReLU activations utilize dense, low dimensional coordinate based inputs, a severe spectral bias occurs that obstructs convergence to high freqeuncy components entirely. In order to overcome this limitation, one can encode the inputs using high frequency sinusoids. Previous works attempted to explain both spectral bias and its severity in the coordinate based regime using Neural Tangent Kernel (NTK) and Fourier analysis. However, such methods come with various limitations, since NTK does not capture real network dynamics, and Fourier analysis only offers a global perspective on the frequency compo
    
[^124]: 面向增量学习的在线超参数优化

    Online Hyperparameter Optimization for Class-Incremental Learning. (arXiv:2301.05032v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05032](http://arxiv.org/abs/2301.05032)

    本文提出了一种能够自适应优化稳定性和可塑性权衡的在线增量学习方法，并将超参数优化过程形式化为在线MDP问题。

    

    逐步增加类别时，分类增量学习（CIL）旨在训练分类模型。CIL的固有挑战是稳定性和可塑性的权衡，即CIL模型应保持稳定以保留旧知识并保持可塑性以吸收新知识。但是，目前没有任何一种现有的CIL模型能够在不同的接收数据设置下实现最佳的权衡——通常情况下，从一半开始（TFH）的设置需要更多的稳定性，而从头开始（TFS）的设置需要更多的可塑性。为此，我们设计了一种在线学习方法，可以自适应地优化权衡，而不需要事先了解设置。具体而言，我们首先介绍了影响权衡的关键超参数，例如，知识蒸馏（KD）损失权重、学习率和分类器类型。然后，我们将超参数优化过程 形式化为在线马尔可夫决策过程（MDP）问题，并提出了一种特定的算法来解决它。

    Class-incremental learning (CIL) aims to train a classification model while the number of classes increases phase-by-phase. An inherent challenge of CIL is the stability-plasticity tradeoff, i.e., CIL models should keep stable to retain old knowledge and keep plastic to absorb new knowledge. However, none of the existing CIL models can achieve the optimal tradeoff in different data-receiving settings--where typically the training-from-half (TFH) setting needs more stability, but the training-from-scratch (TFS) needs more plasticity. To this end, we design an online learning method that can adaptively optimize the tradeoff without knowing the setting as a priori. Specifically, we first introduce the key hyperparameters that influence the trade-off, e.g., knowledge distillation (KD) loss weights, learning rates, and classifier types. Then, we formulate the hyperparameter optimization process as an online Markov Decision Process (MDP) problem and propose a specific algorithm to solve it. 
    
[^125]: 一种随机Proximal Polyak步长方法

    A Stochastic Proximal Polyak Step Size. (arXiv:2301.04935v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2301.04935](http://arxiv.org/abs/2301.04935)

    本文开发了一种正则化的随机梯度下降ProxSPS算法，相比随机Polyak步长（SPS）更稳定易调整，同时在图像分类任务中表现良好，可导致网络具有更小的权重参数。

    

    最近，随机Polyak步长（SPS）已成为随机梯度下降的竞争性自适应步长方案。在本文中，我们开发了ProxSPS，这是SPS的proximal变体，可以处理正则化项。开发SPS的proximal变体特别重要，因为SPS需要目标函数的下界才能发挥良好的作用。当目标函数是损失和正则化项的总和时，可用的总和下界估计可能不准确。相比之下，ProxSPS只需要对损失进行下界估计，而这通常很容易得到。因此，我们展示了ProxSPS更易于调整，在正则化的情况下更稳定。此外，对于图像分类任务，ProxSPS表现与AdamW一样好，几乎不需要调整，并且导致具有更小权重参数的网络。我们还为ProxSPS提供了广泛的收敛性分析，包括非光滑、光滑、弱凸和强凸设置。

    Recently, the stochastic Polyak step size (SPS) has emerged as a competitive adaptive step size scheme for stochastic gradient descent. Here we develop ProxSPS, a proximal variant of SPS that can handle regularization terms. Developing a proximal variant of SPS is particularly important, since SPS requires a lower bound of the objective function to work well. When the objective function is the sum of a loss and a regularizer, available estimates of a lower bound of the sum can be loose. In contrast, ProxSPS only requires a lower bound for the loss which is often readily available. As a consequence, we show that ProxSPS is easier to tune and more stable in the presence of regularization. Furthermore for image classification tasks, ProxSPS performs as well as AdamW with little to no tuning, and results in a network with smaller weight parameters. We also provide an extensive convergence analysis for ProxSPS that includes the non-smooth, smooth, weakly convex and strongly convex setting.
    
[^126]: 优化串联神经编码的经典解码器

    Optimizing Serially Concatenated Neural Codes with Classical Decoders. (arXiv:2212.10355v3 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2212.10355](http://arxiv.org/abs/2212.10355)

    本文介绍了一种使用经典解码器优化神经编码的方法，并且应用BCJR算法进行了最优解码，形成了迭代Turbo解码器，优化了学习编码并取得了较好的效果。

    

    本文提出了一种在短长度编码中改进的方法，即使用基于深度学习的编码序列生成器——实值神经编码器与经典解码器相结合的方式。具体而言，通过经典解码器，可以更好地了解这些神经编码的优点及缺陷。研究显示，基于卷积神经网络（CNN）的编码的局部感受野使得可以应用BCJR算法对其进行最优解码，并且保证计算复杂度可操作。我们利用这些极大后验（MAP）分解器来构建经典（迭代）Turbo解码器，对串联或并联CNN编码器进行近似的最大似然（ML）解码来优化学习编码。据我们所知，这是首次将经典解码算法应用于神经编码器并优化解码。

    For improving short-length codes, we demonstrate that classic decoders can also be used with real-valued, neural encoders, i.e., deep-learning based codeword sequence generators. Here, the classical decoder can be a valuable tool to gain insights into these neural codes and shed light on weaknesses. Specifically, the turbo-autoencoder is a recently developed channel coding scheme where both encoder and decoder are replaced by neural networks. We first show that the limited receptive field of convolutional neural network (CNN)-based codes enables the application of the BCJR algorithm to optimally decode them with feasible computational complexity. These maximum a posteriori (MAP) component decoders then are used to form classical (iterative) turbo decoders for parallel or serially concatenated CNN encoders, offering a close-to-maximum likelihood (ML) decoding of the learned codes. To the best of our knowledge, this is the first time that a classical decoding algorithm is applied to a no
    
[^127]: 使用语言模型提示进行推理：一项调查

    Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09597](http://arxiv.org/abs/2212.09597)

    本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。

    

    推理作为复杂问题解决的重要能力，可以为医疗诊断、谈判等各种实际应用提供后端支持。本文对使用语言模型提示进行推理的前沿研究进行了综合调查。我们介绍了研究成果的比较和总结，并提供了系统资源以帮助初学者。我们还讨论了新兴推理能力出现的潜在原因，并突出了未来的研究方向。资源可在 https://github.com/zjunlp/Prompt4ReasoningPapers 上获取（定期更新）。

    Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
    
[^128]: xTrimoABFold：无多序列比对的新型抗体结构预测方法

    xTrimoABFold: De novo Antibody Structure Prediction without MSA. (arXiv:2212.00735v2 [q-bio.QM] CROSS LISTED)

    [http://arxiv.org/abs/2212.00735](http://arxiv.org/abs/2212.00735)

    xTrimoABFold是一种基于深度抗体语言模型的新型抗体结构预测方法，无需多序列比对，有望促进高通量药物设计的应用。

    

    在抗体工程领域，设计一个新型抗体以正确地结合特定抗原的表位是一项重要的任务。了解抗体结构和其表位可以促进对其功能的机制理解。因此，从其序列预测抗体结构一直是一项高度有价值的任务，而AlphaFold2提供了一种基于蛋白质序列预测蛋白质结构的解决方案，但对于抗体，特别是对于抗体的互补决定区（CDRs），其预测效率和准确性有限制。

    In the field of antibody engineering, an essential task is to design a novel antibody whose paratopes bind to a specific antigen with correct epitopes. Understanding antibody structure and its paratope can facilitate a mechanistic understanding of its function. Therefore, antibody structure prediction from its sequence alone has always been a highly valuable problem for de novo antibody design. AlphaFold2, a breakthrough in the field of structural biology, provides a solution to predict protein structure based on protein sequences and computationally expensive coevolutionary multiple sequence alignments (MSAs). However, the computational efficiency and undesirable prediction accuracy of antibodies, especially on the complementarity-determining regions (CDRs) of antibodies limit their applications in the industrially high-throughput drug design. To learn an informative representation of antibodies, we employed a deep antibody language model (ALM) on curated sequences from the observed a
    
[^129]: 多分辨率核矩阵代数

    Multiresolution kernel matrix algebra. (arXiv:2211.11681v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2211.11681](http://arxiv.org/abs/2211.11681)

    该论文提出了一种压缩核矩阵的稀疏代数，可用于高效分析散乱数据并计算更复杂的矩阵函数，如${\bm A}^\alpha$或$\exp({\bm A})$，并应用于空间统计学中的高斯过程学习算法。

    

    我们提出了一种用于压缩核矩阵的稀疏代数，以实现对散乱数据的高效分析。我们展示了通过采样压缩核矩阵可以在某种S格式下产生最优稀疏矩阵。针对有限可微核的核矩阵，可以实现S格式矩阵的加法和乘法，并且它们的内存和计算代价随矩阵大小$N$线性增长。我们证明并利用了一个事实，即核矩阵（如果存在）的逆矩阵也可以在S格式下被压缩。选择逆运算可以直接计算相应稀疏模式中的条目。S格式矩阵操作使得能够高效近似计算更复杂的矩阵函数，如${\bm A}^\alpha$或$\exp({\bm A})$。该矩阵代数通过伪微分计算得到数学上的正当性。作为应用，我们考虑了用于空间统计学的高效高斯过程学习算法。

    We propose a sparse algebra for samplet compressed kernel matrices, to enable efficient scattered data analysis. We show the compression of kernel matrices by means of samplets produces optimally sparse matrices in a certain S-format. It can be performed in cost and memory that scale essentially linearly with the matrix size $N$, for kernels of finite differentiability, along with addition and multiplication of S-formatted matrices. We prove and exploit the fact that the inverse of a kernel matrix (if it exists) is compressible in the S-format as well. Selected inversion allows to directly compute the entries in the corresponding sparsity pattern. The S-formatted matrix operations enable the efficient, approximate computation of more complicated matrix functions such as ${\bm A}^\alpha$ or $\exp({\bm A})$. The matrix algebra is justified mathematically by pseudo differential calculus. As an application, efficient Gaussian process learning algorithms for spatial statistics is considered
    
[^130]: MedleyVox: 一份用于多声部歌曲分离的评估数据集

    MedleyVox: An Evaluation Dataset for Multiple Singing Voices Separation. (arXiv:2211.07302v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.07302](http://arxiv.org/abs/2211.07302)

    本文提出了用于多声部歌曲分离的评估数据集 MedleyVox，为解决缺乏用于训练的现有多声部数据集的问题，提出了使用各种单声部数据集构建多声部混合物的策略，并提出了改进的超分辨率网络（iSRNet），实现了可比较的性能。

    

    多声部歌曲分离是音乐源分离研究中鲜为人知的领域。缺乏基准数据集阻碍了其进展。本文提出了一份评估数据集，并为多声部歌曲分离提供了基准研究。首先，我们介绍了 MedleyVox，这是一份用于多声部歌曲分离的评估数据集。我们通过将其分类为 i) 合唱、ii) 二重唱、iii) 主声部和其余声部、和 iv) N 声部分离来指定该数据集中的问题定义。其次，为了解决缺乏用于训练的现有多声部数据集的问题，我们提出了一种使用各种单声部数据集构建多声部混合物的策略。第三，我们提出了改进的超分辨率网络（iSRNet），它极大地增强了分离网络的初始估计。与 Conv-TasNet 和多声部混合物构建策略一起进行联合训练，所提出的 iSRNet 实现了可比较的性能。

    Separation of multiple singing voices into each voice is a rarely studied area in music source separation research. The absence of a benchmark dataset has hindered its progress. In this paper, we present an evaluation dataset and provide baseline studies for multiple singing voices separation. First, we introduce MedleyVox, an evaluation dataset for multiple singing voices separation. We specify the problem definition in this dataset by categorizing it into i) unison, ii) duet, iii) main vs. rest, and iv) N-singing separation. Second, to overcome the absence of existing multi-singing datasets for a training purpose, we present a strategy for construction of multiple singing mixtures using various single-singing datasets. Third, we propose the improved super-resolution network (iSRNet), which greatly enhances initial estimates of separation networks. Jointly trained with the Conv-TasNet and the multi-singing mixture construction strategy, the proposed iSRNet achieved comparable performa
    
[^131]: AC电力流的神经网络建模的全局性能保证

    Global Performance Guarantees for Neural Network Models of AC Power Flow. (arXiv:2211.07125v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2211.07125](http://arxiv.org/abs/2211.07125)

    本文首次开发了一种可行的神经网络验证程序，它结合了非线性AC电力流方程的ground truth，以确定最坏的神经网络性能。使用顺序添加有针对性的切割，我们迭代地收紧我们的公式，直到解决方案足够紧密或达到一个安全性阈值。

    

    机器学习可以生成既快又准的黑盒子模型。但严格验证黑盒模型的准确性是计算上具有挑战性的。对于电力系统来说，学习AC电力流是任何希望显著加速计算的机器学习黑盒模型的基石，无论是为了优化、控制还是动力学。本文首次开发一种可行的神经网络验证程序，它结合了非线性AC电力流方程的ground truth，以确定最坏的神经网络性能。我们的方法称为Sequential Targeted Tightening (STT)，它利用松弛的凸规划重构了原始的验证问题，该问题是一个混合整数二次规划（MIQP）。通过顺序添加有针对性的切割，我们迭代地收紧我们的公式，直到解决方案足够紧密或达到一个安全性阈值。

    Machine learning can generate black-box surrogate models which are both extremely fast and highly accurate. Rigorously verifying the accuracy of these black-box models, however, is computationally challenging. When it comes to power systems, learning AC power flow is the cornerstone of any machine learning surrogate model wishing to drastically accelerate computations, whether it is for optimization, control, or dynamics. This paper develops for the first time, to our knowledge, a tractable neural network verification procedure which incorporates the ground truth of the non-linear AC power flow equations to determine worst-case neural network performance. Our approach, termed Sequential Targeted Tightening (STT), leverages a loosely convexified reformulation of the original verification problem, which is a mixed integer quadratic program (MIQP). Using the sequential addition of targeted cuts, we iteratively tighten our formulation until either the solution is sufficiently tight or a sa
    
[^132]: 无偏的监督对比学习

    Unbiased Supervised Contrastive Learning. (arXiv:2211.05568v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.05568](http://arxiv.org/abs/2211.05568)

    本文提出了一种新的监督对比损失形式（epsilon-SupInfoNCE）以及一种新的去偏正则化损失（FairKL），旨在解决从有偏数据中学习无偏模型的问题。

    

    许多数据集存在偏差，即它们包含仅在数据集中与目标类高度相关的易于学习的特征，但不在真实的数据分布中。因此，从有偏数据中学习无偏模型已成为近年来非常相关的研究课题。在这项工作中，我们解决了学习对偏差具有鲁棒性的表征的问题。我们首先提出了一种基于边缘的理论框架，可以帮助我们澄清为什么最近的对比损失（InfoNCE，SupCon等）在处理偏差数据时可能失败。基于此，我们推导出了一种新的监督对比损失形式（epsilon-SupInfoNCE），提供了更准确的对正负样本之间最小距离的控制。此外，由于我们的理论框架，我们还提出了FairKL，一种新的去偏正则化损失，即使在极度偏差的数据情况下也可以很好地工作。我们在标准的视觉数据集上验证了所提出的损失。

    Many datasets are biased, namely they contain easy-to-learn features that are highly correlated with the target class only in the dataset but not in the true underlying distribution of the data. For this reason, learning unbiased models from biased data has become a very relevant research topic in the last years. In this work, we tackle the problem of learning representations that are robust to biases. We first present a margin-based theoretical framework that allows us to clarify why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when dealing with biased data. Based on that, we derive a novel formulation of the supervised contrastive loss (epsilon-SupInfoNCE), providing more accurate control of the minimal distance between positive and negative samples. Furthermore, thanks to our theoretical framework, we also propose FairKL, a new debiasing regularization loss, that works well even with extremely biased data. We validate the proposed losses on standard vision datasets inc
    
[^133]: 缺失数据转移下的领域自适应

    Domain Adaptation under Missingness Shift. (arXiv:2211.02093v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02093](http://arxiv.org/abs/2211.02093)

    本文解决了领域自适应问题中缺失数据转移的情况，提出了DAMS方法。针对缺失数据指标不可用的情况，提供了理论结果，包括协变量转移被违反、最优源预测器可能比总是预测均值表现更差、最优目标预测器可被识别等。

    

    缺失数据的比率通常会因记录政策而变化，即使基础特征相对稳定，这种情况可能在不同的时间和地点发生变化。本文介绍了缺失数据转移下的领域自适应问题(DAMS)。在这里，(有标签的)源数据和(无标签的)目标数据可以互换，但存在不同的缺失数据机制。我们展示了，如果缺失数据指标是可用的，DAMS就会归结为协变量转移。针对这种指标不可用的情况，我们为基于完全随机下报告的讨论提供了以下理论结果：(i)协变量转移被违反了(需要适应)；(ii)与总是预测均值相比，最优的线性源预测器在目标域上的表现可能会变得更糟；(iii)即使缺失率本身无法确定，也能够识别出最优的目标预测器；(iv)对于线性模型，一个简单的分析调整可以得到最优参数的一致估计。

    Rates of missing data often depend on record-keeping policies and thus may change across times and locations, even when the underlying features are comparatively stable. In this paper, we introduce the problem of Domain Adaptation under Missingness Shift (DAMS). Here, (labeled) source data and (unlabeled) target data would be exchangeable but for different missing data mechanisms. We show that if missing data indicators are available, DAMS reduces to covariate shift. Addressing cases where such indicators are absent, we establish the following theoretical results for underreporting completely at random: (i) covariate shift is violated (adaptation is required); (ii) the optimal linear source predictor can perform arbitrarily worse on the target domain than always predicting the mean; (iii) the optimal target predictor can be identified, even when the missingness rates themselves are not; and (iv) for linear models, a simple analytic adjustment yields consistent estimates of the optimal 
    
[^134]: 利用机器学习方法探究天气对地铁客流预测的影响

    Exploring the impact of weather on Metro demand forecasting using machine learning method. (arXiv:2210.13965v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13965](http://arxiv.org/abs/2210.13965)

    本文通过对亚洲地铁系统的客流数据和气象记录进行机器学习预测，发现把天气变量加入预测模型中可以提高周末的预测准确度。

    

    城市轨道交通的发展为城市交通管理和缓解拥挤等问题提供了重要帮助。本文使用2018年4月至6月亚洲一个地铁系统的真实客流数据，进行短时段客流预测，将车站分为四类进行预测，并收集了同期气象记录。然后采用不同输入的机器学习方法和多元回归来评估每种天气元素对典型地铁站小时客流预测的改善效果。研究结果表明，输入天气变量可提高周末预测准确度，而在工作日预测性能仅略有改善，不同元素对预测的贡献程度不同。

    Urban rail transit provides significant comprehensive benefits such as large traffic volume and high speed, serving as one of the most important components of urban traffic construction management and congestion solution. Using real passenger flow data of an Asian subway system from April to June of 2018, this work analyzes the space-time distribution of the passenger flow using short-term traffic flow prediction. Stations are divided into four types for passenger flow forecasting, and meteorological records are collected for the same period. Then, machine learning methods with different inputs are applied and multivariate regression is performed to evaluate the improvement effect of each weather element on passenger flow forecasting of representative metro stations on hourly basis. Our results show that by inputting weather variables the precision of prediction on weekends enhanced while the performance on weekdays only improved marginally, while the contribution of different elements
    
[^135]: 基于强化学习的即兴控制器合成策略探索算法研究

    Exploration Policies for On-the-Fly Controller Synthesis: A Reinforcement Learning Approach. (arXiv:2210.05393v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05393](http://arxiv.org/abs/2210.05393)

    本文提出了一种基于强化学习的方法，用于产生启发式来指导有向控制器合成算法的增量探索过程。

    

    控制器合成是基于模型的规划的一种，本文提出一种基于强化学习方法获得启发式来进行有向控制器合成的算法研究。此算法的增量探索由与领域无关的人工设计引导。

    Controller synthesis is in essence a case of model-based planning for non-deterministic environments in which plans (actually ''strategies'') are meant to preserve system goals indefinitely. In the case of supervisory control environments are specified as the parallel composition of state machines and valid strategies are required to be ''non-blocking'' (i.e., always enabling the environment to reach certain marked states) in addition to safe (i.e., keep the system within a safe zone). Recently, On-the-fly Directed Controller Synthesis techniques were proposed to avoid the exploration of the entire -and exponentially large-environment space, at the cost of non-maximal permissiveness, to either find a strategy or conclude that there is none. The incremental exploration of the plant is currently guided by a domain-independent human-designed heuristic. In this work, we propose a new method for obtaining heuristics based on Reinforcement Learning (RL). The synthesis algorithm is thus frame
    
[^136]: DALL-E-Bot：将Web规模的扩散模型引入到机器人学中

    DALL-E-Bot: Introducing Web-Scale Diffusion Models to Robotics. (arXiv:2210.02438v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.02438](http://arxiv.org/abs/2210.02438)

    DALL-E-Bot是第一个将Web规模扩散模型引入到机器人学中的工作，可以在没有任何额外示例或训练的情况下零-shot地推断并重新排列场景中的物体，具有可行性和可扩展性，这为机器人学习提供了一个有前途的方向。

    

    我们首次探索了将Web规模的扩散模型应用于机器人学的工作。DALL-E-Bot使机器人能够重新排列场景中的物体，首先推断出这些物体的文本描述，然后生成代表这些物体的自然、类人的排列图像，并最终根据目标图像物理地排列这些物体。我们展示了这是使用DALL-E实现的零-shot方式，不需要任何进一步的示例排列、数据收集或训练。由于DALL-E的Web规模预训练，在不受限于预定义的物体或场景情况下，DALL-E-Bot是完全自主的。鼓励真实世界的结果，包括人类研究和客观指标，表明将Web规模扩散模型集成到机器人管道中是一个有前途的方向，可实现可扩展的、无监督的机器人学习。

    We introduce the first work to explore web-scale diffusion models for robotics. DALL-E-Bot enables a robot to rearrange objects in a scene, by first inferring a text description of those objects, then generating an image representing a natural, human-like arrangement of those objects, and finally physically arranging the objects according to that goal image. We show that this is possible zero-shot using DALL-E, without needing any further example arrangements, data collection, or training. DALL-E-Bot is fully autonomous and is not restricted to a pre-defined set of objects or scenes, thanks to DALL-E's web-scale pre-training. Encouraging real-world results, with both human studies and objective metrics, show that integrating web-scale diffusion models into robotics pipelines is a promising direction for scalable, unsupervised robot learning.
    
[^137]: CrAM:一种压缩感知的优化器

    CrAM: A Compression-Aware Minimizer. (arXiv:2207.14200v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.14200](http://arxiv.org/abs/2207.14200)

    提出一种新的压缩感知的优化器CrAM，其产生的密集模型可以在训练后通过单次压缩而不会带来显著的精度损失，该优化器在一些标准基准测试中的实验结果显示出竞争力。

    

    在实际应用中，深度神经网络通常需要通过剪枝和/或量化进行压缩。本文提出了一种新的压缩感知的优化器CrAM，通过一种原则性的方式改变优化步骤，产生出的模型在压缩操作（如剪枝）下具有稳定的局部损失行为。因此，通过CrAM训练的密集模型应该在训练后能够通过单次压缩而不会带来显著的精度损失。在一些标准基准测试中的实验结果，如ImageNet分类的残差网络和用于语言建模的BERT模型，都表明CrAM产生的密集模型可以比标准的SGD / Adam基线更加准确，但在权重剪枝操作下稳定，即可以在一次剪枝到70-80％稀疏度并且几乎没有精度损失，到90％时具有合理（约1％）的精度损失，这与渐进式压缩方法相当竞争力。

    Deep neural networks (DNNs) often have to be compressed, via pruning and/or quantization, before they can be deployed in practical settings. In this work we propose a new compression-aware minimizer dubbed CrAM that modifies the optimization step in a principled way, in order to produce models whose local loss behavior is stable under compression operations such as pruning. Thus, dense models trained via CrAM should be compressible post-training, in a single step, without significant accuracy loss. Experimental results on standard benchmarks, such as residual networks for ImageNet classification and BERT models for language modelling, show that CrAM produces dense models that can be more accurate than the standard SGD/Adam-based baselines, but which are stable under weight pruning: specifically, we can prune models in one-shot to 70-80% sparsity with almost no accuracy loss, and to 90% with reasonable ($\sim 1\%$) accuracy loss, which is competitive with gradual compression methods. Ad
    
[^138]: 安全嵌入聚合用于联合表示学习

    Secure Embedding Aggregation for Federated Representation Learning. (arXiv:2206.09097v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.09097](http://arxiv.org/abs/2206.09097)

    本论文提出了一种安全嵌入聚合协议\scheme，该协议在联合表示学习框架下，能够在保护客户端隐私的前提下，实现多个客户端的嵌入表示聚合。

    

    本论文考虑联合表示学习框架，在中央服务器的协助下，一组$N$个分布式客户端共同训练它们的私有数据，用于表示（或嵌入）一组实体（例如社交网络中的用户）。在这个框架下，为了聚合客户端私有训练的本地嵌入表示，我们开发了一种名为\scheme 的安全嵌入聚合协议，它利用所有客户端之间的潜在聚合机会，同时提供对每个客户端的本地实体和相应嵌入表示的隐私保护，防止好奇的服务器和多达 $T<N/2$的串通客户端的攻击。

    We consider a federated representation learning framework, where with the assistance of a central server, a group of $N$ distributed clients train collaboratively over their private data, for the representations (or embeddings) of a set of entities (e.g., users in a social network). Under this framework, for the key step of aggregating local embeddings trained privately at the clients, we develop a secure embedding aggregation protocol named \scheme, which leverages all potential aggregation opportunities among all the clients, while providing privacy guarantees for the set of local entities and corresponding embeddings \emph{simultaneously} at each client, against a curious server and up to $T < N/2$ colluding clients.
    
[^139]: 带有核化Stein距离的后验Coreset构建用于基于模型的强化学习

    Posterior Coreset Construction with Kernelized Stein Discrepancy for Model-Based Reinforcement Learning. (arXiv:2206.01162v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.01162](http://arxiv.org/abs/2206.01162)

    本文提出一种通过核化Stein距离构建后验Coreset的MBRL方法，在放松转移模型高斯或Lipschitz的限制下表现出优异的性能，并且可以应用于大规模训练。

    

    模型为基础的强化学习方法在实践中表现出优异的性能，但在大空间的理论保证大多数限于转移模型为高斯或Lipschitz的情况下，并且需要后验估计其表示复杂度随时间增长而无界。 在本文中，我们开发了一种新的MBRL方法，(i) 放松目标转移模型属于通用混合模型族的假设；(ii) 通过包含压缩步骤以仅由统计显着的过去状态 - 操作对的贝叶斯Coreset组成，适用于大规模训练；(iii) 表现出亚线性的贝叶斯遗憾。为了实现这些结果，我们采用一种基于Stein方法的方法，该方法在构造后验和目标上满足平滑性条件的情况下，允许以核Stein距离（KSD）的形式封闭地评估分布距离。前面提到的后验Coreset构建是通过最小化这个KSD来完成的，其中混合组件的位置取决于l-inf预算，以限制中心的数量。我们在一系列模拟机器人控制任务中展示了该方法的有效性。

    Model-based approaches to reinforcement learning (MBRL) exhibit favorable performance in practice, but their theoretical guarantees in large spaces are mostly restricted to the setting when transition model is Gaussian or Lipschitz, and demands a posterior estimate whose representational complexity grows unbounded with time. In this work, we develop a novel MBRL method (i) which relaxes the assumptions on the target transition model to belong to a generic family of mixture models; (ii) is applicable to large-scale training by incorporating a compression step such that the posterior estimate consists of a Bayesian coreset of only statistically significant past state-action pairs; and (iii) exhibits a sublinear Bayesian regret. To achieve these results, we adopt an approach based upon Stein's method, which, under a smoothness condition on the constructed posterior and target, allows distributional distance to be evaluated in closed form as the kernelized Stein discrepancy (KSD). The afor
    
[^140]: 卫星星座中的联邦学习

    Federated Learning in Satellite Constellations. (arXiv:2206.00307v3 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2206.00307](http://arxiv.org/abs/2206.00307)

    本文介绍了卫星星座中的联邦学习，根据卫星的通信能力、星座设计和参数服务器的位置提出了卫星FL的分类。卫星FL面临着独特的挑战和机遇。

    

    联邦学习（FL）是一种分布式机器学习范例，适用于连通性有限和间歇性的系统。本文介绍了卫星星座对FL带来的新环境，其中连接模式明显不同于常规的地面FL。重点是在低地球轨道中的大型卫星星座，每个卫星使用本地存储的数据集参与数据驱动的FL任务。这种情况源于连接小型卫星的兆级星座和卫星中集成人工智能的趋势。我们根据卫星的通信能力、星座设计和参数服务器的位置提出了卫星FL的分类。本文提供了当前领域最新技术的综合概述，并讨论了卫星FL的独特挑战与机遇。

    Federated learning (FL) has recently emerged as a distributed machine learning paradigm for systems with limited and intermittent connectivity. This paper presents the new context brought to FL by satellite constellations, where the connectivity patterns are significantly different from the ones observed in conventional terrestrial FL. The focus is on large constellations in low Earth orbit (LEO), where each satellites participates in a data-driven FL task using a locally stored dataset. This scenario is motivated by the trend towards mega constellations of interconnected small satellites in LEO and the integration of artificial intelligence in satellites. We propose a classification of satellite FL based on the communication capabilities of the satellites, the constellation design, and the location of the parameter server. A comprehensive overview of the current state-of-the-art in this field is provided and the unique challenges and opportunities of satellite FL are discussed. Finall
    
[^141]: 带有属性删除子网络的模块化和按需偏差缓解方法

    Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks. (arXiv:2205.15171v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15171](http://arxiv.org/abs/2205.15171)

    提出一种新颖的模块化偏差缓解方法，在推理时间按需集成到核心模型中的独立去偏置子网络，在性别、种族和年龄等受保护属性的分类任务中，该方法在缓解偏差方面是有效的，并且在精度和灵活性方面优于现有技术方法。

    

    社会偏见反映在大型预训练语言模型及其在下游任务中的微调版本中。常见的处理偏差的方法引入了额外的优化标准，并更新模型以达到新的去偏置状态。然而，在实践中，最终用户和从业人员可能更喜欢切换回原始模型，或仅对特定子集的保护属性应用去偏置。为了实现这一点，我们提出了一种新颖的模块化偏差缓解方法，包括独立高度稀疏的去偏置子网络，其中每个去偏置模块可以在推理时间按需集成到核心模型中。我们的方法借鉴了“diff”剪枝的概念，并提出了一种适合于各种表示分离优化的新型训练方式。我们在具有性别、种族和年龄等受保护属性的三个分类任务上进行了实验。结果表明，我们的模块化方法在缓解偏差方面是有效的，并且在精度和灵活性方面优于现有技术方法。

    Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The resul
    
[^142]: MiniDisc: 最小蒸馏计划用于语言模型压缩

    MiniDisc: Minimal Distillation Schedule for Language Model Compression. (arXiv:2205.14570v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.14570](http://arxiv.org/abs/2205.14570)

    本研究提出了一个叫做MiniDisc的最小蒸馏计划，可以在最少一次尝试中调度最优的教师助手，用于实现语言模型压缩。

    

    最近的研究发现，在教师模型和学生模型之间存在较大的容量差距时，语言模型蒸馏的效果不佳，引入了教师助手辅助蒸馏来弥补差距。然而，现有的基于教师助手的方法需要大量的尝试才能调度出最优的教师助手。为此，我们提出了一种最小蒸馏计划（MiniDisc），可以在最少一次尝试中调度最优的教师助手。MiniDisc是基于教师助手的规模-性能的权衡来度量教师助手的最优性，并可以在不对学生进行实验的情况下调度最优的教师助手。

    Recent studies have uncovered that language model distillation is less effective when facing a large capacity gap between the teacher and the student, and introduced teacher assistant-based distillation to bridge the gap. As a connection, the scale and the performance of the teacher assistant is of vital importance to bring the knowledge from the teacher to the student. However, existing teacher assistant-based methods require maximally many trials before scheduling an optimal teacher assistant. To this end, we propose a minimal distillation schedule (MiniDisc) for scheduling the optimal teacher assistant in minimally one trial. In particular, motivated by the finding that the performance of the student is positively correlated to the scale-performance tradeoff of the teacher assistant, MiniDisc is designed with a $\lambda$-tradeoff to measure the optimality of the teacher assistant without trial distillation to the student. MiniDisc then can schedule the optimal teacher assistant with
    
[^143]: 适应并评估用于梯度提升决策树的影响估计方法

    Adapting and Evaluating Influence-Estimation Methods for Gradient-Boosted Decision Trees. (arXiv:2205.00359v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.00359](http://arxiv.org/abs/2205.00359)

    该研究将深度学习模型的影响估计方法改编到了梯度提升决策树上，命名为TREX和BoostIn，旨在帮助更好地理解GBDT的预测和改进性能。

    

    影响估计分析了对训练数据的更改如何导致不同的模型预测；这种分析可以帮助我们更好地理解这些预测、做出这些预测的模型以及它们训练的数据集。然而，大多数影响估计技术都是为具有连续参数的深度学习模型设计的。梯度提升决策树（GBDT）是一种强大且广泛使用的模型类别；然而，这些模型是黑盒子，具有不透明的决策过程。为了更好地理解 GBDT 预测并普遍改进这些模型，我们将最近和流行的深度学习模型的影响估计方法适应到了 GBDT 上。具体而言，我们使用 representer-point 方法和 TracIn 方法改编了这些方法，分别命名为 TREX 和 BoostIn；源代码可在 https://github.com/jjbrophy47/tree_influence 上找到。我们使用五种不同的评估方法将这些方法与 LeafInfluence 和其他基准线进行了比较。

    Influence estimation analyzes how changes to the training data can lead to different model predictions; this analysis can help us better understand these predictions, the models making those predictions, and the data sets they're trained on. However, most influence-estimation techniques are designed for deep learning models with continuous parameters. Gradient-boosted decision trees (GBDTs) are a powerful and widely-used class of models; however, these models are black boxes with opaque decision-making processes. In the pursuit of better understanding GBDT predictions and generally improving these models, we adapt recent and popular influence-estimation methods designed for deep learning models to GBDTs. Specifically, we adapt representer-point methods and TracIn, denoting our new methods TREX and BoostIn, respectively; source code is available at https://github.com/jjbrophy47/tree_influence. We compare these methods to LeafInfluence and other baselines using 5 different evaluation mea
    
[^144]: 微小样本学习中的区间界插值算法

    Interval Bound Interpolation for Few-shot Learning with Few Tasks. (arXiv:2204.03511v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.03511](http://arxiv.org/abs/2204.03511)

    在微小样本学习中引入了区间界概念，通过最小化任务及其相应边界之间的距离来保留训练任务周围的领域，并通过插值来人为形成新任务进行训练。

    

    微小样本学习旨在将通过对多样化任务进行训练所获取的知识转移到具有有限标记数据的相同任务分布中的新任务。实现有效的少次学习泛化的基本前提是学习任务流形的好的表示方法。在仅有受限数量任务的情况下，这变得更加困难。在这种少任务少学习情况下，显式地保留任务流形中的本地邻域并利用其生成训练人工任务可以协助提高性能。为此，我们将完全强韧性训练文献中的区间界概念引入到了微小样本学习中。区间界用于描述训练任务周围的领域。这些邻域可以通过最小化与任务及其相应边界之间的距离来保留。然后利用一种新颖的策略通过插值来人为形成新任务进行训练。

    Few-shot learning aims to transfer the knowledge acquired from training on a diverse set of tasks to unseen tasks from the same task distribution with a limited amount of labeled data. The underlying requirement for effective few-shot generalization is to learn a good representation of the task manifold. This becomes more difficult when only a limited number of tasks are available for training. In such a few-task few-shot setting, it is beneficial to explicitly preserve the local neighborhoods from the task manifold and exploit this to generate artificial tasks for training. To this end, we introduce the notion of interval bounds from the provably robust training literature to few-shot learning. The interval bounds are used to characterize neighborhoods around the training tasks. These neighborhoods can then be preserved by minimizing the distance between a task and its respective bounds. We then use a novel strategy to artificially form new tasks for training by interpolating between 
    
[^145]: ECOLA: 使用上下文化的语言表示增强时间知识嵌入

    ECOLA: Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations. (arXiv:2203.09590v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.09590](http://arxiv.org/abs/2203.09590)

    本文讨论如何将文本数据与时间知识嵌入相结合以加强时间知识嵌入表征的质量，提出了ECOLA方法，该方法考虑了时间因素，并将文本信息注入到时间知识嵌入中。

    

    传统的知识嵌入模型不能充分利用丰富的文本信息，因此已经进行了大量的研究以利用文本来增强知识嵌入。然而，现有的增强方法无法应用于包含时间依赖事件知识和复杂时间动态的时间知识图（tKG）。 特别是，现有的增强方法通常假定知识嵌入是独立于时间的。相反，在tKG模型中，实体嵌入通常会不断演化，这就提出了将时间相关文本与实体对齐的挑战。因此，我们在本文中提出了研究如何将文本数据与时间知识嵌入相结合来增强时间知识嵌入。作为这项任务的一种方法，我们提出了使用上下文化的语言表示增强时间知识嵌入（ECOLA），它考虑了时间因素，并将文本信息注入到时间知识嵌入中。为了评估ECOLA，我们引入了三个数据集

    Since conventional knowledge embedding models cannot take full advantage of the abundant textual information, there have been extensive research efforts in enhancing knowledge embedding using texts. However, existing enhancement approaches cannot apply to temporal knowledge graphs (tKGs), which contain time-dependent event knowledge with complex temporal dynamics. Specifically, existing enhancement approaches often assume knowledge embedding is time-independent. In contrast, the entity embedding in tKG models usually evolves, which poses the challenge of aligning temporally relevant texts with entities. To this end, we propose to study enhancing temporal knowledge embedding with textual data in this paper. As an approach to this task, we propose Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations (ECOLA), which takes the temporal aspect into account and injects textual information into temporal knowledge embedding. To evaluate ECOLA, we introduce three n
    
[^146]: 一种新的用于层级神经结构搜索的进化算法

    A Novel Evolutionary Algorithm for Hierarchical Neural Architecture Search. (arXiv:2107.08484v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2107.08484](http://arxiv.org/abs/2107.08484)

    本文提出了一种新的进化算法用于神经结构搜索，采用层级模块组织拓扑结构，应用于全局搜索空间，结合策略系统来推广表现良好的子结构，通过Fashion-MNIST和NAS-Bench101的实验得出了较好的结果。

    

    本文提出了一种新的进化算法用于神经结构搜索，适用于全局搜索空间。该算法的架构表示将拓扑结构组织为多个层级模块，设计过程利用该表示来探索搜索空间。我们还采用了一种策略系统，推广表现良好的子结构到后续世代。我们将该方法应用于Fashion-MNIST和NAS-Bench101，相对较少的世代即可实现分别达到$93.2\%$和$94.8\%$的准确率。

    In this work, we propose a novel evolutionary algorithm for neural architecture search, applicable to global search spaces. The algorithm's architectural representation organizes the topology in multiple hierarchical modules, while the design process exploits this representation, in order to explore the search space. We also employ a curation system, which promotes the utilization of well performing sub-structures to subsequent generations. We apply our method to Fashion-MNIST and NAS-Bench101, achieving accuracies of $93.2\%$ and $94.8\%$ respectively in a relatively small number of generations.
    
[^147]: 基于神经网络的非线性函数建模

    Non-linear Functional Modeling using Neural Networks. (arXiv:2104.09371v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.09371](http://arxiv.org/abs/2104.09371)

    本文提出了一种基于神经网络的、适用于函数数据的新型非线性模型。我们提出了两种变体，旨在显式利用函数数据中固有的结构，并通过全面的模拟研究和实际数据示例证明了该方法的有效性。

    

    本文介绍了一种基于神经网络的新型非线性函数数据模型。深度学习在非线性建模方面非常成功，但在函数数据设置方面却很少有研究。我们提出了两种变体：一种是具有连续隐藏层的函数神经网络，称为函数直接神经网络（FDNN），另一种则利用基扩展和连续隐藏层，称为基函数神经网络（FBNN）。两种变体都是设计用来显式利用函数数据中固有的结构。为了拟合这些模型，我们导出了一种基于函数梯度的优化算法。我们通过全面的模拟研究和实际数据示例展示了所提出方法在处理复杂函数模型方面的有效性。

    We introduce a new class of non-linear models for functional data based on neural networks. Deep learning has been very successful in non-linear modeling, but there has been little work done in the functional data setting. We propose two variations of our framework: a functional neural network with continuous hidden layers, called the Functional Direct Neural Network (FDNN), and a second version that utilizes basis expansions and continuous hidden layers, called the Functional Basis Neural Network (FBNN). Both are designed explicitly to exploit the structure inherent in functional data. To fit these models we derive a functional gradient based optimization algorithm. The effectiveness of the proposed methods in handling complex functional models is demonstrated by comprehensive simulation studies and real data examples.
    
[^148]: 跨边界联邦学习在农食品行业数据共享中的作用

    The Role of Cross-Silo Federated Learning in Facilitating Data Sharing in the Agri-Food Sector. (arXiv:2104.07468v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.07468](http://arxiv.org/abs/2104.07468)

    跨边界联邦学习技术在农食品行业数据共享方面提供了解决方案，可以共享分散数据、保护数据贡献者的隐私和安全，并使用数据构建更好的机器学习模型。

    

    数据共享仍然是普遍采用新型人工智能技术的主要障碍，尤其是在农食品行业。在这种情况下，数据的保密性是很自然的，因为数据是数据所有者的宝贵资产，需恰当地使用才能为其提供有用的运营和流程洞察，从而获得竞争优势。不幸的是，许多新型人工智能技术需要大量的训练数据才能执行得好，而在许多情况下这是不现实的。然而，最近的机器学习进展，例如联邦学习和隐私保护技术，可以通过提供基础设施和支持技术，使用来自各种来源的数据来训练模型，而不必共享原始数据本身。在本文中，我们提出了一种基于联邦学习的技术解决方案，利用分散的数据（即未被交换或共享但保留给交易伙伴的数据），以构建更好的机器学习模型在农食品行业。我们展示了我们的方法如何提高模型的质量并减少所需的数据量。我们提出的跨边界联邦学习框架促进了不同组织之间的数据共享，同时确保了数据贡献者的隐私和安全。

    Data sharing remains a major hindering factor when it comes to adopting emerging AI technologies in general, but particularly in the agri-food sector. Protectiveness of data is natural in this setting; data is a precious commodity for data owners, which if used properly can provide them with useful insights on operations and processes leading to a competitive advantage. Unfortunately, novel AI technologies often require large amounts of training data in order to perform well, something that in many scenarios is unrealistic. However, recent machine learning advances, e.g. federated learning and privacy-preserving technologies, can offer a solution to this issue via providing the infrastructure and underpinning technologies needed to use data from various sources to train models without ever sharing the raw data themselves. In this paper, we propose a technical solution based on federated learning that uses decentralized data, (i.e. data that are not exchanged or shared but remain with t
    
[^149]: 多核学习的神经通用化

    Neural Generalization of Multiple Kernel Learning. (arXiv:2102.13337v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.13337](http://arxiv.org/abs/2102.13337)

    本文提出了一种神经通用化的多核学习方法（NGMKL），将传统的多核学习框架扩展到具有非线性激活函数的多层神经网络中。实验证明该方法提高了算法的复杂度，并导致更高的识别精度。

    

    多核学习是学习核函数的传统方法。MKL算法增强了核方法的性能。然而，与深度学习模型相比，这些方法的复杂度较低，在识别精度方面不如这些模型。在本文中，我们展示了一个典型的MKL算法可以被解释为具有线性激活函数的单层神经网络。通过这种解释，我们提出了多核学习的神经通用化（NGMKL），该方法将传统的多核学习框架扩展到具有非线性激活函数的多层神经网络中。我们在几个基准测试中的实验表明，所提出的方法提高了MKL算法的复杂度，导致更高的识别精度。

    Multiple Kernel Learning is a conventional way to learn the kernel function in kernel-based methods. MKL algorithms enhance the performance of kernel methods. However, these methods have a lower complexity compared to deep learning models and are inferior to these models in terms of recognition accuracy. Deep learning models can learn complex functions by applying nonlinear transformations to data through several layers. In this paper, we show that a typical MKL algorithm can be interpreted as a one-layer neural network with linear activation functions. By this interpretation, we propose a Neural Generalization of Multiple Kernel Learning (NGMKL), which extends the conventional multiple kernel learning framework to a multi-layer neural network with nonlinear activation functions. Our experiments on several benchmarks show that the proposed method improves the complexity of MKL algorithms and leads to higher recognition accuracy.
    
[^150]: 实践中的QNLP：在量子计算机上运行组合模型的含义。

    QNLP in Practice: Running Compositional Models of Meaning on a Quantum Computer. (arXiv:2102.12846v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2102.12846](http://arxiv.org/abs/2102.12846)

    本文介绍了在嘈杂的中间规模量子计算机上进行的首个大于100个句子数据集的NLP实验结果，成功地训练了解决简单句子分类任务的NLP模型，证明了组合模型的含义与量子理论具有形式相似性。

    

    量子自然语言处理（QNLP）涉及设计和实现旨在在量子硬件上运行的NLP模型。在本文中，我们介绍了在嘈杂的中间规模量子（NISQ）计算机上进行的首个大于100个句子数据集的NLP实验结果。利用由Coecke、Sadrzadeh和Clark（2010）提出的含义组合模型与量子理论的形式相似性，我们创建了具有自然映射到量子电路的句子表示。我们使用这些表示来实现并成功训练在量子硬件上解决简单句子分类任务的NLP模型。我们进行了量子模拟，比较了Coecke等人的语法敏感模型与使用较少或无语法的两个基线，具体而言，我们实现了“词袋”模型的量子模拟，其中根本不考虑语法，以及单词序列模型的量子模拟，仅尊重单词顺序。

    Quantum Natural Language Processing (QNLP) deals with the design and implementation of NLP models intended to be run on quantum hardware. In this paper, we present results on the first NLP experiments conducted on Noisy Intermediate-Scale Quantum (NISQ) computers for datasets of size greater than 100 sentences. Exploiting the formal similarity of the compositional model of meaning by Coecke, Sadrzadeh and Clark (2010) with quantum theory, we create representations for sentences that have a natural mapping to quantum circuits. We use these representations to implement and successfully train NLP models that solve simple sentence classification tasks on quantum hardware. We conduct quantum simulations that compare the syntax-sensitive model of Coecke et al. with two baselines that use less or no syntax; specifically, we implement the quantum analogues of a "bag-of-words" model, where syntax is not taken into account at all, and of a word-sequence model, where only word order is respected.
    
[^151]: GTEA: 通过时间边聚合在时序交互图上进行归纳表征学习

    GTEA: Inductive Representation Learning on Temporal Interaction Graphs via Temporal Edge Aggregation. (arXiv:2009.05266v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.05266](http://arxiv.org/abs/2009.05266)

    本文提出了 GTEA框架，用于在时序交互图上进行归纳学习，结合了时间动态建模和图嵌入，通过聚合相邻节点和边嵌入的特征，共同学习了 TIG 的拓扑和时间依赖关系，而且引入了一种稀疏感知的自注意机制，在多个时间序列预测任务中表现出有效性。

    

    本文提出了Graph Temporal Edge Aggregation (GTEA)框架，用于在时序交互图（TIGs）上进行归纳学习。不同于以往工作，GTEA将交互序列的时间动态建模在连续时间空间中，并同时利用图中丰富的节点和边/交互属性。具体来说，我们将序列模型与时间编码器相结合，学习相邻节点之间的成对交互动力学。这有助于捕捉节点对沿历史的复杂时间交互模式，生成边嵌入可以输入到GNN骨干。通过聚合相邻节点和相应的边嵌入的特征，GTEA共同学习TIG的拓扑和时间依赖关系。此外，还采用了一种稀疏感知自注意机制进行邻居聚合，突出更重要的邻居并抑制GTEA的琐碎噪声。通过联合优化整个框架，我们验证了其在多个时间序列预测任务中的有效性。

    In this paper, we propose the Graph Temporal Edge Aggregation (GTEA) framework for inductive learning on Temporal Interaction Graphs (TIGs). Different from previous works, GTEA models the temporal dynamics of interaction sequences in the continuous-time space and simultaneously takes advantage of both rich node and edge/ interaction attributes in the graph. Concretely, we integrate a sequence model with a time encoder to learn pairwise interactional dynamics between two adjacent nodes.This helps capture complex temporal interactional patterns of a node pair along the history, which generates edge embeddings that can be fed into a GNN backbone. By aggregating features of neighboring nodes and the corresponding edge embeddings, GTEA jointly learns both topological and temporal dependencies of a TIG. In addition, a sparsity-inducing self-attention scheme is incorporated for neighbor aggregation, which highlights more important neighbors and suppresses trivial noises for GTEA. By jointly o
    
[^152]: 富有属性网络中的顶点提名

    Vertex Nomination in Richly Attributed Networks. (arXiv:2005.02151v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2005.02151](http://arxiv.org/abs/2005.02151)

    本文探讨了富有属性网络中顶点提名的双重作用，并提出了一种新颖的基于内容感知的网络嵌入方法，证明该方法优于现有的不利用内容和上下文的顶点提名方法。

    

    顶点提名是一项轻度监督的网络信息检索任务，在这个任务中，感兴趣的一张图的顶点被用来查询第二张图以发现感兴趣的第二张图的顶点。与其他信息检索任务类似，顶点提名方案的输出是第二张图中顶点的排序列表，理想情况下，未知的感兴趣的顶点应该集中在列表的顶部。顶点提名方案为高效地挖掘复杂网络中的相关信息提供了有用的工具。在本文中，我们从理论和实践两方面探讨了内容（即边缘和顶点属性）和上下文（即网络拓扑结构）在顶点提名中的双重作用。我们提供了必要和充分的条件，证明了利用内容和上下文的顶点提名方案能够超越仅利用内容或上下文的方案。虽然内容和上下文的联合效用在其他网络分析任务中已经得到证实，但我们证明在顶点提名的背景下，这种联合效用也是成立的。此外，我们提出了一种新颖的基于内容感知的网络嵌入方法，用于顶点提名，可以有效地结合局部和全局网络属性信息。我们在真实的社交和引用网络上进行了实验，证明了我们提出的方法优于不利用内容和上下文的现有的顶点提名方法。

    Vertex nomination is a lightly-supervised network information retrieval task in which vertices of interest in one graph are used to query a second graph to discover vertices of interest in the second graph. Similar to other information retrieval tasks, the output of a vertex nomination scheme is a ranked list of the vertices in the second graph, with the heretofore unknown vertices of interest ideally concentrating at the top of the list. Vertex nomination schemes provide a useful suite of tools for efficiently mining complex networks for pertinent information. In this paper, we explore, both theoretically and practically, the dual roles of content (i.e., edge and vertex attributes) and context (i.e., network topology) in vertex nomination. We provide necessary and sufficient conditions under which vertex nomination schemes that leverage both content and context outperform schemes that leverage only content or context separately. While the joint utility of both content and context has 
    
[^153]: 基于外推的时变凸优化预测校正方法

    Extrapolation-based Prediction-Correction Methods for Time-varying Convex Optimization. (arXiv:2004.11709v4 [math.OC] UPDATED)

    [http://arxiv.org/abs/2004.11709](http://arxiv.org/abs/2004.11709)

    本文提出了一种基于外推策略的在线优化算法，解决了大量信号处理和机器学习中的实时优化问题，并且通过使用算子理论工具，分析了原始问题和对偶问题的显式界限。

    

    本文针对信号处理和机器学习中经常遇到、具有数据流的在线优化问题，讨论了基于预测校正范式的在线优化算法，包括原始问题和对偶问题。特别地，我们利用出现在许多信号处理问题中的典型正则化最小二乘结构，提出了一种新的定制预测策略，即基于外推的策略。然后，通过运用算子理论工具，我们分别分析了该方法作用于原始问题和对偶问题时的收敛性，推导出了追踪误差（即与时变最优解距离）的显式界限。我们进一步讨论了该算法应用于信号处理、机器学习和机器人问题时的实证表现。

    In this paper, we focus on the solution of online optimization problems that arise often in signal processing and machine learning, in which we have access to streaming sources of data. We discuss algorithms for online optimization based on the prediction-correction paradigm, both in the primal and dual space. In particular, we leverage the typical regularized least-squares structure appearing in many signal processing problems to propose a novel and tailored prediction strategy, which we call extrapolation-based. By using tools from operator theory, we then analyze the convergence of the proposed methods as applied both to primal and dual problems, deriving an explicit bound for the tracking error, that is, the distance from the time-varying optimal solution. We further discuss the empirical performance of the algorithm when applied to signal processing, machine learning, and robotics problems.
    

