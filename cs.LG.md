# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic Speech Recognition.](http://arxiv.org/abs/2401.02417) | 本研究引入了一种对话对比学习方法，用于对自动语音识别模型进行自监督微调，以改善低质量语音识别结果导致的自然语言理解应用的失败。实验证明，这种方法在任务导向的对话数据集上可以将ASR模型的性能提高达到19.2%。 |
| [^2] | [ODIN: A Single Model for 2D and 3D Perception.](http://arxiv.org/abs/2401.02416) | ODIN是一个模型，可以同时对2D RGB图像和3D点云进行分割和标记，使用变压器架构进行2D和3D视图间的信息融合。 |
| [^3] | [Simulation-Based Inference with Quantile Regression.](http://arxiv.org/abs/2401.02413) | 提出了一种基于模拟推断和分位数回归的新方法，通过学习个体化的分位数来估计后验样本，并使用局部累积密度函数定义贝叶斯可信区间，具有更快的评估速度。同时，还可以集成后处理扩展步骤以保证后验估计的无偏性，而计算成本几乎可以忽略不计。 |
| [^4] | [LLM Augmented LLMs: Expanding Capabilities through Composition.](http://arxiv.org/abs/2401.02412) | 本文提出了CALM方法，通过组合现有的基础模型和更具体的模型，使用交叉注意力来增强模型的表示并实现新的能力。CALM可以通过“重用”现有模型和一些额外的参数和数据来扩展新任务上的模型规模，并且保留现有模型的功能。 |
| [^5] | [What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs.](http://arxiv.org/abs/2401.02411) | 本文提出了将神经体积渲染扩展到本地2D图像更高分辨率的技术，以解决3D GANs无法解决2D图像中丰富的3D几何图形的问题。 |
| [^6] | [Real-Time 2D Temperature Field Prediction in Metal Additive Manufacturing Using Physics-Informed Neural Networks.](http://arxiv.org/abs/2401.02403) | 本论文介绍了一种基于物理信息的神经网络框架，用于实时预测金属增材制造中的二维温度场。该框架结合了基于物理信息的输入、损失函数和卷积长短期记忆架构，通过使用实时温度数据，可以预测未来时间戳下不同几何形状、沉积模式和工艺的温度场。 |
| [^7] | [Generating synthetic data for neural operators.](http://arxiv.org/abs/2401.02398) | 该论文提出了一种生成神经算子的合成数据的新方法，为训练网络提供不需要数值求解PDE的数据。 |
| [^8] | [Integration of physics-informed operator learning and finite element method for parametric learning of partial differential equations.](http://arxiv.org/abs/2401.02363) | 本文提出了一种使用物理信息深度学习技术参数化求解偏微分方程的方法，在异质固体中研究了稳态热方程。该方法可以独立于有限元方法等经典求解器，并使用新颖的损失函数定义，降低了导数阶数和自动微分需求。 |
| [^9] | [A Survey Analyzing Generalization in Deep Reinforcement Learning.](http://arxiv.org/abs/2401.02349) | 本文调查了深度强化学习中的泛化性能。深度强化学习策略存在过拟合问题，限制了它们的鲁棒性和泛化能力。研究形式化和统一了提高泛化性和克服过拟合的不同解决方案。 |
| [^10] | [Multi-Source Domain Adaptation with Transformer-based Feature Generation for Subject-Independent EEG-based Emotion Recognition.](http://arxiv.org/abs/2401.02344) | 本文提出了一种基于变压器特征生成器的多源域自适应方法（MSDA-TF），用于独立于学科的基于脑电信号的情绪识别。该方法通过利用多个来源的信息，保留浅层空间、时间和频谱的脑电数据表示，并提取其中的全局依赖关系。 |
| [^11] | [Evasive Hardware Trojan through Adversarial Power Trace.](http://arxiv.org/abs/2401.02342) | 本论文介绍了一种基于对抗功率追踪的逃逸硬件木马方法，通过对抗性噪声混淆侧信道分析，使得硬件木马能够绕过机器学习的检测方法。 |
| [^12] | [Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models.](http://arxiv.org/abs/2401.02333) | 本研究提出了一种创新的方法，通过上下文化表格数据来提高 RAG 系统中处理复杂表格查询的准确性，提高了摘要的效率。 |
| [^13] | [Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning.](http://arxiv.org/abs/2401.02329) | 本研究提出了一种异质联邦学习方法FedED，通过同时进行空类别蒸馏和逻辑抑制，解决了在联邦学习中尚未充分识别空类别的问题。 |
| [^14] | [A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning.](http://arxiv.org/abs/2401.02325) | 这篇论文提出了一种鲁棒的分位数Huber损失函数，在分布强化学习中通过捕捉噪声并调整参数来增强对异常值的鲁棒性。实证测试验证了该方法的有效性。 |
| [^15] | [Multi-Agent Context Learning Strategy for Interference-Aware Beam Allocation in mmWave Vehicular Communications.](http://arxiv.org/abs/2401.02323) | 该论文提出了一种名为MACOL的新策略，利用背景赌博机管理干扰，并分配mmWave波束以服务车辆。 |
| [^16] | [Robust Physics Informed Neural Networks.](http://arxiv.org/abs/2401.02300) | 引入了一种鲁棒的物理信息神经网络（RPINNs）来近似偏微分方程（PDE）的解，该网络在训练过程中考虑了PDE的控制物理法则，解决了传统PINNs中损失函数与真实误差不鲁棒的问题。 |
| [^17] | [Training Single-Layer Morphological Perceptron Using Convex-Concave Programming.](http://arxiv.org/abs/2401.02296) | 本文提出了一种使用凸凹规划训练单层形态感知器的方法，该方法通过K-DDCCP算法结合SLMP模型和WDCCP算法来解决二分类问题，并通过实验证实了其有效性。 |
| [^18] | [Path-based Explanation for Knowledge Graph Completion.](http://arxiv.org/abs/2401.02290) | 基于路径的KGC解释器Power-Link通过引入图加权技术，实现了可解释的知识图谱补全，推动了模型透明度和可靠性的提升。 |
| [^19] | [DEM: A Method for Certifying Deep Neural Network Classifier Outputs in Aerospace.](http://arxiv.org/abs/2401.02283) | 这项工作提出了一种新的、以输出为中心的方法，通过统计验证技术来认证深度神经网络(DNN)分类器的输出。该方法能够标记可能不可靠的特定输入，以便后续由人工专家检查。与现有技术相比，该方法主要关注单个输出而不是整个DNN的认证。 |
| [^20] | [Lightweight Fish Classification Model for Sustainable Marine Management: Indonesian Case.](http://arxiv.org/abs/2401.02278) | 这项研究提出了一种轻量级的鱼类分类模型，利用机器学习技术识别保护鱼类物种，并在有限硬件上运行。该模型在印度尼西亚水域的鱼类图像数据集上进行训练，可以对捕获的鱼类进行分类，并给出是否可食用的建议。 |
| [^21] | [Universal Approximation Theorem for Vector- and Hypercomplex-Valued Neural Networks.](http://arxiv.org/abs/2401.02277) | 该论文通过引入非退化代数的概念，扩展了通用逼近定理，使其适用于广泛的向量值神经网络，包括超复值模型。这对于神经网络在回归和分类任务等多种应用中的应用具有重要意义。 |
| [^22] | [Uncertainty-Aware Deep Attention Recurrent Neural Network for Heterogeneous Time Series Imputation.](http://arxiv.org/abs/2401.02258) | 该论文提出了鉴于复杂数据中出现的问题，能够同时估计异质多变量时间序列中缺失值及其相关不确定性的深度关注循环神经网络插补方法。 |
| [^23] | [Balancing Continual Learning and Fine-tuning for Human Activity Recognition.](http://arxiv.org/abs/2401.02255) | 本文研究了在基于可穿戴设备的人类活动识别中，采用连续自监督学习模型CaSSLe和半监督连续学习模型Kaizen来平衡表征学习和下游分类的问题。这些方案通过对比学习和自训练的结合，实现了知识保留和定制化功能。 |
| [^24] | [L3Cube-IndicNews: News-based Short Text and Long Document Classification Datasets in Indic Languages.](http://arxiv.org/abs/2401.02254) | L3Cube-IndicNews是一个面向印度语系的多语种文本分类数据集，包括短标题、长文档和长段落三个数据集。它提供了10种印度语言的新闻文章，每个数据集包含10个或更多类别的文章。这个数据集可以用于深入分析和评估。 |
| [^25] | [Policy-regularized Offline Multi-objective Reinforcement Learning.](http://arxiv.org/abs/2401.02244) | 本文将离线规范化方法扩展到多目标强化学习中，以利用离线轨迹数据训练多目标政策。在面对偏好不一致的演示问题时，提出了过滤方法和正则化技术。通过将偏好条件化标量化更新与政策规范化相结合，可以同时学习一组策略，从而降低计算成本。 |
| [^26] | [U-Mixer: An Unet-Mixer Architecture with Stationarity Correction for Time Series Forecasting.](http://arxiv.org/abs/2401.02236) | U-Mixer是一种结合Unet和Mixer的框架，通过稳定性校正方法解决时间序列预测中的非稳态挑战，捕捉局部时间依赖关系，并合并特征以获取全面的数据表示。 |
| [^27] | [Trajectory-Oriented Policy Optimization with Sparse Rewards.](http://arxiv.org/abs/2401.02225) | 该论文提出了一种基于轨迹导向的稀疏奖励策略优化方法，通过利用离线示范轨迹，在稀疏奖励环境中实现更快速、更高效的在线强化学习。 |
| [^28] | [Robust bilinear factor analysis based on the matrix-variate $t$ distribution.](http://arxiv.org/abs/2401.02203) | 本文提出了一种基于矩阵变量t分布的鲁棒双线性因子分析（tbfa）模型，可以从重尾或受污染的矩阵数据中同时提取行和列变量的公共因子。 |
| [^29] | [LADRI: LeArning-based Dynamic Risk Indicator in Automated Driving System.](http://arxiv.org/abs/2401.02199) | 本研究提出了一种基于学习的动态风险评估框架，利用人工神经网络分析和分类实时的车载传感器数据，以提升自动驾驶系统的安全水平和情境意识。 |
| [^30] | [Nodule detection and generation on chest X-rays: NODE21 Challenge.](http://arxiv.org/abs/2401.02192) | 这篇文章总结了NODE21挑战的结果，以及进一步实验证明了合成生成的结节训练图像对检测算法性能的影响。 |
| [^31] | [FairGridSearch: A Framework to Compare Fairness-Enhancing Models.](http://arxiv.org/abs/2401.02183) | 本文提出了一种比较公平性增强模型的框架FairGridSearch，通过实验不同模型参数组合并推荐最佳组合。研究结果表明，选择适当的准确度和公平性度量对模型评估非常重要。 |
| [^32] | [Disentangle Estimation of Causal Effects from Cross-Silo Data.](http://arxiv.org/abs/2401.02154) | 引入一种新颖的解耦架构来解决跨平台数据中因果效应估计的问题，并通过引入全局约束条件来提高准确性。 |
| [^33] | [Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy and Directions.](http://arxiv.org/abs/2401.02143) | 这项综述研究了使用图神经网络（GNN）进行表格数据学习（TDL）的领域。研究发现，深度学习方法在分类和回归任务方面表现出优越性能，但目前对数据实例和特征值之间潜在相关性的表达不足。GNN以其能力模拟复杂关系和相互作用，并在TDL领域得到了广泛应用。本综述对GNN4TDL方法进行了系统回顾，提供了对其演化领域的洞见，并提出了一个全面的分类。 |
| [^34] | [PosCUDA: Position based Convolution for Unlearnable Audio Datasets.](http://arxiv.org/abs/2401.02135) | PosCUDA是一种用于创建无法学习的音频数据集的位置卷积方法，通过在小音频块上使用按类别的卷积和私钥控制块的位置，实现了对数据集的无法学习，并保持音频数据质量。 |
| [^35] | [ACP-ESM: A novel framework for classification of anticancer peptides using protein-oriented transformer approach.](http://arxiv.org/abs/2401.02124) | ACP-ESM是一个使用以蛋白质为导向的Transformer方法的新框架，用于分类抗癌肽。这种方法可以优化抗癌肽的稳定性，改善选择性，并提高对癌细胞的传递性。 |
| [^36] | [Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation.](http://arxiv.org/abs/2401.02117) | 该论文介绍了一个移动ALOHA系统，用于学习双手移动操作。通过低成本的远程操作和整体身体控制，系统能够完成复杂的移动操作任务，并通过联合训练提高成功率达到90%。 |
| [^37] | [Cadmium Zinc Telluride (CZT) photon counting detector Characterisation for soft tissue imaging.](http://arxiv.org/abs/2401.02106) | 该研究表征了镉锌碲化物（CZT）光子计数探测器在识别各种组织方面的性能，推动了使用光子计数探测技术来克服传统CT探测器的限制。 |
| [^38] | [Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe.](http://arxiv.org/abs/2401.02088) | 本论文重新评估了流水线并行体的内存平衡问题，并提出了一种名为BPipe的技术来解决该问题。验证实验结果表明，虽然BPipe在GPT-3模型上有效，但在LLaMA训练中并未获得相似的好处。我们还分析了BPipe在不同模型上性能差异的原因，并引入了一种新的方法来估计BPipe的性能。 |
| [^39] | [View-based Explanations for Graph Neural Networks.](http://arxiv.org/abs/2401.02086) | 这篇论文提出了一种基于视图的解释方法来解释图神经网络(GNNs)的行为，通过生成解释视图和图模式来解释特定类别的结果。 |
| [^40] | [Energy based diffusion generator for efficient sampling of Boltzmann distributions.](http://arxiv.org/abs/2401.02080) | 介绍了一种称为基于能量的扩散生成器的新型采样器，用于从任意目标分布中生成样本，并通过扩散模型和广义哈密顿动力学提高采样性能。在各种复杂分布函数上的实证评估中表现出优越性。 |
| [^41] | [U-Trustworthy Models.Reliability, Competence, and Confidence in Decision-Making.](http://arxiv.org/abs/2401.02062) | 提出了一种基于哲学文献的新的信任框架$\mathcal{U}$-信任度，用于评估AI系统的信任性，该框架挑战了传统的概率框架，并提出了解决方案以克服偏见和误导性信任评估的风险。 |
| [^42] | [Neural Collapse for Cross-entropy Class-Imbalanced Learning with Unconstrained ReLU Feature Model.](http://arxiv.org/abs/2401.02058) | 本文研究了在交叉熵类别不平衡学习中的神经折叠现象，证明了在存在类别不平衡情况下，神经折叠仍然存在，但类均值的几何特性会发生偏移。 |
| [^43] | [Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN Ticket.](http://arxiv.org/abs/2401.02020) | Spikformer V2是一种基于SNNs和自注意机制的脉冲神经网络，通过提出脉冲自注意机制和脉冲Transformer来实现高准确度的图像识别。 |
| [^44] | [From Function to Distribution Modeling: A PAC-Generative Approach to Offline Optimization.](http://arxiv.org/abs/2401.02019) | 本文提出了一种PAC-Generative方法应用于离线优化问题，在这种方法中，将优化问题视为从生成模型中抽样的过程，而不是学习和优化未知的目标函数。通过学习离线数据示例，得到了一个有效的生成模型。 |
| [^45] | [SwitchTab: Switched Autoencoders Are Effective Tabular Learners.](http://arxiv.org/abs/2401.02013) | SwitchTab是一种专门设计用于捕捉表格数据中潜在依赖关系的自监督方法。它引入了一个非对称的编码器-解码器框架来解耦数据对中的互相关联和显著特征，从而生成更具代表性的嵌入。在大量实验中，SwitchTab在细调下端到端预测任务中展现出了优越的性能。 |
| [^46] | [Fast & Fair: Efficient Second-Order Robust Optimization for Fairness in Machine Learning.](http://arxiv.org/abs/2401.02012) | 该项目通过对抗训练技术开发了一种高效的二阶鲁棒优化方法，以提高机器学习中的公平性。该方法能够减小深度神经网络因与敏感属性相关的偏见而导致的不公平问题，并且比纯一阶方法更高效。 |
| [^47] | [Decentralized Multi-Task Online Convex Optimization Under Random Link Failures.](http://arxiv.org/abs/2401.02011) | 本文研究了分散多任务在线凸优化中的随机链路故障问题，提出了替代缺失决策的健壮分散鞍点算法，并证明了算法达到了O(sqrt(T))的regret和O(T^fra)的复杂度。 |
| [^48] | [Two-Stage Surrogate Modeling for Data-Driven Design Optimization with Application to Composite Microstructure Generation.](http://arxiv.org/abs/2401.02008) | 本文介绍了一种两阶段的机器学习代理建模框架，通过集成符合推理来解决科学工程领域中的反问题，提供了一种可应用广泛且高效的方法。 |
| [^49] | [Mean-Field Assisted Deep Boltzmann Learning with Probabilistic Computers.](http://arxiv.org/abs/2401.01996) | 本文展示了借助概率计算机的均场辅助深度玻尔兹曼学习，通过使用特定硬件和稀疏网络，提出了两类均场理论辅助学习算法，以训练深度且无限制的玻尔兹曼机，具有重要的可训练性贡献。 |
| [^50] | [GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised Learning.](http://arxiv.org/abs/2401.01990) | GPS-SSL是一种将先验知识注入到自监督学习中的通用方法，通过设计度量空间并利用最近邻采样生成正样本。它可以减少对强数据增强的依赖，因此在Cifar10上达到了更好的效果。 |
| [^51] | [Representation Learning of Multivariate Time Series using Attention and Adversarial Training.](http://arxiv.org/abs/2401.01987) | 本文提出了一种使用注意力和对抗训练的方法，用于表示学习多变量时间序列数据。实验结果表明，生成的信号与示例数据集的相似性较高。 |
| [^52] | [Beyond Regrets: Geometric Metrics for Bayesian Optimization.](http://arxiv.org/abs/2401.01981) | 本论文提出了四个新的几何度量，可以比较贝叶斯优化算法在考虑查询点和全局最优解的几何特性时的性能。 |
| [^53] | [Tailor: Size Recommendations for High-End Fashion Marketplaces.](http://arxiv.org/abs/2401.01978) | Tailor是一个针对高端时尚市场的尺寸建议的新方法，通过整合隐式和显式用户信号，采用序列分类方法来提供个性化的尺寸建议。该方法比其他方法提高了准确性，并通过使用加购物车的交互增加了用户覆盖范围。 |
| [^54] | [Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers.](http://arxiv.org/abs/2401.01974) | 本文提出了一种框架来实现零样本组合视觉推理，通过引入抽象的空间和时间例程以及利用少量的nun |
| [^55] | [Can We Generate Realistic Hands Only Using Convolution?.](http://arxiv.org/abs/2401.01951) | 本文展示了通过为卷积层提供具有相对$n$维笛卡尔坐标系的单一输入通道，可以缓解图像生成模型无法重现复杂几何特征的问题，显著提高了GAN和VAE生成的手部和面部图像质量。 |
| [^56] | [IoT in the Era of Generative AI: Vision and Challenges.](http://arxiv.org/abs/2401.01923) | 在生成式人工智能时代的物联网，Generative AI的进展带来了巨大的希望，同时也面临着高资源需求、及时工程、设备端推理、安全等关键挑战。 |
| [^57] | [Unsupervised Object-Centric Learning from Multiple Unspecified Viewpoints.](http://arxiv.org/abs/2401.01922) | 本文提出了一个无监督的深度生成模型，用于从多个未指定的视角学习组合性场景表示。该模型能够将潜在表示分为与视角无关的部分和与视角相关的部分。 |
| [^58] | [AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets.](http://arxiv.org/abs/2401.01916) | 通过有针对性和持续的预训练，我们在天文学问题回答中扩展了AstroLLaMA，通过使用紧凑的LLaMA-2模型和专门的天文学语料库，我们实现了在专门主题理解方面的显著改进。我们还通过对特定领域的对话数据集进行微调，发布了带有聊天功能的AstroLLaMA。 |
| [^59] | [Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network.](http://arxiv.org/abs/2401.01912) | 本研究提出了一种缩小时间步长的脉冲神经网络（SSNN），通过将SNN分成多个阶段，逐步缩小时间步长，实现了低延迟的神经形态对象识别，并通过添加早期分类器解决了性能下降的问题。 |
| [^60] | [Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP.](http://arxiv.org/abs/2401.01911) | 该研究探讨了医学图像-文本基础模型中非配对训练引发的标签差异问题，并以此作为后门攻击的案例研究，分析了其对医学FM供应链的影响。 |
| [^61] | [Machine-learning-based particle identification with missing data.](http://arxiv.org/abs/2401.01905) | 本文提出了一种处理带有缺失数据的粒子识别的方法，使用机器学习模型，首次解决了在ALICE实验中由于不同探测器使用不同技术而产生的数据缺失问题。 |
| [^62] | [Reputation-Based Federated Learning Defense to Mitigate Threats in EEG Signal Classification.](http://arxiv.org/abs/2401.01896) | 本文提出了一个基于声誉的联邦学习防御框架，用于在脑电信号分类中防御安全威胁。该框架通过联合训练模型和引入声誉机制来保护隐私和缓解数据污染攻击。 |
| [^63] | [A Robust Adversary Detection-Deactivation Method for Metaverse-oriented Collaborative Deep Learning.](http://arxiv.org/abs/2401.01895) | 本文提出了一种适用于元宇宙导向的协同深度学习的鲁棒性对抗检测和停用方法，用于解决CDL训练过程中的安全弱点，保护预训练的大型模型和本地敏感数据集。 |
| [^64] | [Approximating Numerical Flux by Fourier Neural Operators for the Hyperbolic Conservation Laws.](http://arxiv.org/abs/2401.01783) | 该研究通过在传统的数值方案中运用神经网络来替换数值通量，解决了神经网络方法缺乏鲁棒性和泛化能力的问题，并展示了该方法在超声古典守恒定律方面具有优势。 |
| [^65] | [Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences.](http://arxiv.org/abs/2401.01641) | 本文提出了一种基于生成预训练方法的金融交易上下文嵌入模型，该模型在公共数据集上的测试中表现优于最先进的自监督方法，并在卡片欺诈检测问题上取得了显著的性能提升。 |
| [^66] | [Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models.](http://arxiv.org/abs/2401.00625) | 本调研系统地解决了大型语言模型的资源效率挑战，介绍了各种优化方法和技术，包括计算、内存、能量、财务和网络资源的优化，在LLM的生命周期的各个阶段都具有应用价值，同时提出了细致的资源效率技术分类。 |
| [^67] | [Real-Time FJ/MAC PDE Solvers via Tensorized, Back-Propagation-Free Optical PINN Training.](http://arxiv.org/abs/2401.00413) | 本论文开发了一种基于光学计算的物理相关神经网络（PINN）训练框架，用于实时解决高维PDE问题。该方法具有低功耗和超低延迟，通过避免反向传播过程和使用张量压缩方法来提高收敛性和可扩展性。 |
| [^68] | [Knowledge Enhanced Conditional Imputation for Healthcare Time-series.](http://arxiv.org/abs/2312.16713) | 本研究提出了一种知识增强的条件插补方法，针对医疗时间序列数据中的缺失数据问题。通过整合先进的知识嵌入和非均匀掩蔽策略，该方法能够灵活适应不同模式的电子健康记录中的缺失数据分布不平衡问题。 |
| [^69] | [Preference as Reward, Maximum Preference Optimization with Importance Sampling.](http://arxiv.org/abs/2312.16430) | 本文提出了一种使用重要性抽样进行最大偏好优化的算法，该算法通过直接优化生成策略来消除对奖励模型的需求，提高了数据利用率和稳定性，并通过解决KL正则化问题来改善偏好学习效果。 |
| [^70] | [A Multi-Modal Contrastive Diffusion Model for Therapeutic Peptide Generation.](http://arxiv.org/abs/2312.15665) | 这项研究提出了一个多模式对比扩散模型，融合了序列和结构信息，用于生成治疗肽。模型通过对比学习策略捕捉两种模态之间的一致性，提高了生成效果。 |
| [^71] | [Adversarial Data Poisoning for Fake News Detection: How to Make a Model Misclassify a Target News without Modifying It.](http://arxiv.org/abs/2312.15228) | 本文分析了对抗性攻击在假新闻检测模型中的威胁，研究了攻击者如何在不修改原始目标新闻的情况下通过引入污染数据来操纵模型的行为。 |
| [^72] | [Probabilistic Modeling for Sequences of Sets in Continuous-Time.](http://arxiv.org/abs/2312.15045) | 本文提出了一个通用的连续时间序列集合的概率建模框架，适用于处理每个事件与一组项目相关联的情况。引入了适用于任何强度为基础的递归神经点过程模型的推理方法，可用于回答关于序列历史条件下的概率查询问题。 |
| [^73] | [Theory of Hallucinations based on Equivariance.](http://arxiv.org/abs/2312.14504) | 本研究提出了基于等变性的幻觉理论，探讨了大型语言模型中幻觉的成因，并开发了一种衡量语言模型幻觉程度的交叉熵误差函数。通过测试语言模型在获得等变性方面的能力，研究表明某些类型的等变语言模型对理解复杂社交关系表现出色。 |
| [^74] | [Underwater Acoustic Signal Recognition Based on Salient Feature.](http://arxiv.org/abs/2312.13143) | 这项研究提出了一种基于显著特征的水声信号识别方法，利用深度学习模型从频谱中提取特征，不断学习以分类水声信号，以应对复杂关系和提高识别准确率。 |
| [^75] | [SLP-Net:An efficient lightweight network for segmentation of skin lesions.](http://arxiv.org/abs/2312.12789) | 提出了一种名为SLP-Net的超轻量级皮肤病变分割网络，基于脉冲神经机制，具有较小的参数数量和高计算速度。与常见的编码器-解码器结构不同，使用特征适应模块实现多尺度信息解码。在实验中表现出在准确度和Dice系数方面均优于其他方法，具有良好的泛化能力。 |
| [^76] | [Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy.](http://arxiv.org/abs/2312.12728) | 本研究介绍了一种通用的推理加速框架，用于提高大型语言模型（LLMs）的推理速度，并在保持生成准确性的同时降低成本。该框架在支付宝的检索增强生成（RAG）系统中得到了应用。 |
| [^77] | [Continual Learning: Forget-free Winning Subnetworks for Video Representations.](http://arxiv.org/abs/2312.11973) | 本研究基于"彩票票据假设"，提出了一种连续学习方法，通过利用稀疏子网络和FSO进行任务增量学习、少样本类增量学习和视频增量学习，实现高效学习和有效的权重重用。 |
| [^78] | [Evaluating Language-Model Agents on Realistic Autonomous Tasks.](http://arxiv.org/abs/2312.11671) | 这篇论文评估了语言模型代理在现实自主任务中的表现，发现这些代理只能完成最简单的任务，对于更具挑战性的任务有一定进展。 |
| [^79] | [Uncertainty in GNN Learning Evaluations: A Comparison Between Measures for Quantifying Randomness in GNN Community Detection.](http://arxiv.org/abs/2312.09015) | 本研究比较了GNN社区检测中不同的度量方法，在考虑随机性的情况下，评估了算法排名的一致性，并发现在忽视超参数调查时性能明显下降。 |
| [^80] | [Entropy and the Kullback-Leibler Divergence for Bayesian Networks: Computational Complexity and Efficient Implementation.](http://arxiv.org/abs/2312.01520) | 本文提出了一种计算贝叶斯网络中Shannon熵和Kullback-Leibler散度的高效算法，并通过一系列数值示例进行了演示。此外，还展示了如何将高斯贝叶斯网络中KL的计算复杂度从立方降低到二次。 |
| [^81] | [LinFlo-Net: A two-stage deep learning method to generate simulation ready meshes of the heart.](http://arxiv.org/abs/2310.20065) | 本文提出了一种新的深度学习模型，通过两阶段形变和新型损失函数的应用，实现了自动生成心脏薄壁结构的计算机模型。该模型在准确性和网格质量方面表现出色，可以直接用于物理模拟，减少后处理的需求。 |
| [^82] | [CBD: A Certified Backdoor Detector Based on Local Dominant Probability.](http://arxiv.org/abs/2310.17498) | 本文提出了第一个可信后门检测器（CBD），它基于一种新颖的、可调节的符合预测方案，即局部主导概率统计。CBD能够提供对分类器的检测推断结果，并给出攻击保证可检测的条件和假阳性率的概率上界。实验证明，具有更高鲁棒性的触发器和更小扰动幅度的攻击更容易被检测出来。 |
| [^83] | [Better and Simpler Lower Bounds for Differentially Private Statistical Estimation.](http://arxiv.org/abs/2310.06289) | 本文提出了更好、更简单的差分隐私统计估计的下界，适用于估计高斯协方差的谱误差和有界$k$阶矩的重尾分布的均值估计。 |
| [^84] | [Smoothing Methods for Automatic Differentiation Across Conditional Branches.](http://arxiv.org/abs/2310.03585) | 本研究提出了一种通过结合平滑解释和自动微分的方法来处理具有条件分支的程序，并成功计算出平滑程序的梯度，从而支持分支程序参数合成和机器学习流程中的模型校准。 |
| [^85] | [Auto-grading C programming assignments with CodeBERT and Random Forest Regressor.](http://arxiv.org/abs/2309.15216) | 本研究提供了使用机器学习和深度学习方法对C编程作业进行自动评分的分析，并引入了基于代码的转换器词嵌入模型CodeBERT。测试结果表明该策略的有效性，并讨论了统计方法和深度学习技术之间的对比。 |
| [^86] | [WFTNet: Exploiting Global and Local Periodicity in Long-term Time Series Forecasting.](http://arxiv.org/abs/2309.11319) | 本文提出了一种利用全局和局部周期性的Wavelet-Fourier变换网络（WFTNet）用于长期时间序列预测。在利用傅里叶和小波变换提取时频信息的基础上，引入了周期重要性加权系数（PWC）来平衡全局和局部频率模式的重要性。大量实验证明，WFTNet在各种时间序列数据集上优于其他基线模型。 |
| [^87] | [Let There Be Sound: Reconstructing High Quality Speech from Silent Videos.](http://arxiv.org/abs/2308.15256) | 本文介绍了一个重建高质量语音的唇语转语音系统，通过解决一对多映射问题和细节精炼来显著改进生成质量。 |
| [^88] | [Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion.](http://arxiv.org/abs/2308.12517) | 本文提出了一种新的强化学习框架，为复杂机器人系统训练神经网络控制器。该框架引入了奖励和约束的概念，通过设计高效的策略优化算法来处理约束，以减少计算开销。通过应用于不同腿式机器人的运动控制器训练中，展示了该框架的有效性。 |
| [^89] | [GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text.](http://arxiv.org/abs/2308.06911) | GIT-Mol是一种多模态大型语言模型，可在分子科学中处理图像、图形和文本信息。通过新提出的GIT-Former架构，该模型能够将多种模态的数据对齐到一个统一的潜在空间中。与基线相比，GIT-Mol在性质预测和分子生成有效性方面取得了显著改进。此外，该模型还可用于化合物名称识别和化学反应预测等下游任务。 |
| [^90] | [A General Implicit Framework for Fast NeRF Composition and Rendering.](http://arxiv.org/abs/2308.04669) | 本研究提出了一个通用的隐式框架，可以快速合成和渲染NeRF对象。通过引入神经深度场的新的表面表示方法，可以实现动态阴影并允许多个NeRF对象以任意刚体变换无缝地放置和渲染在一起。 |
| [^91] | [Learning to Generate Training Datasets for Robust Semantic Segmentation.](http://arxiv.org/abs/2308.02535) | 本文提出了一种新的方法，通过生成真实和可信的扰动或异常图像来提高语义分割技术的鲁棒性。通过设计和训练Robusta，一种鲁棒的条件生成对抗网络，可以为训练可靠的分割模型提供可用的数据集，从而显著增强语义分割技术在面对现实世界的扰动和分布变化时的鲁棒性。 |
| [^92] | [Provably Powerful Graph Neural Networks for Directed Multigraphs.](http://arxiv.org/abs/2306.11586) | 本文分析了一组简单的改进方法，将标准的消息传递图神经网络（GNN）转化为可证明强大的有向多图神经网络，能够检测任何有向子图模式。实验结果展示了这些改进方法在合成子图检测任务和金融犯罪分析任务上的出色性能。 |
| [^93] | [Simplifying and Empowering Transformers for Large-Graph Representations.](http://arxiv.org/abs/2306.10759) | 本文通过实验证明，在大型图上使用一层注意力即可获得令人惊讶的竞争性能，挑战了在语言和视觉任务中复杂模型的应用。这促使我们重新思考在大型图上设计Transformer的理念，以提高可扩展性。 |
| [^94] | [Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal.](http://arxiv.org/abs/2306.04502) | 本文提出了一种名为AGRA的自适应梯度异常值去除方法，能够在模型训练过程中动态调整数据集从而有效提高模型学习效果。 |
| [^95] | [Quantifying Deep Learning Model Uncertainty in Conformal Prediction.](http://arxiv.org/abs/2306.00876) | 本文针对深度学习模型的不确定性进行了量化，采用了符合性预测框架来计算模型的置信水平，并与其他不确定性量化方法进行了比较。 |
| [^96] | [The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs).](http://arxiv.org/abs/2305.17033) | 这个论文介绍了CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023挑战，该挑战是首个专注于儿童脑肿瘤的BraTS挑战，旨在评估儿童脑胶质瘤的体积分割算法的发展。儿童中枢神经系统肿瘤是儿童癌症相关死亡的主要原因，并且对这些实体的诊断和治疗存在一些挑战。 |
| [^97] | [A Generalizable Physics-informed Learning Framework for Risk Probability Estimation.](http://arxiv.org/abs/2305.06432) | 本文提出了一种基于物理学的学习框架，通过将MC方法与基于物理学的神经网络相结合，有效评估长期风险概率及其梯度。数值结果表明，该方法具有更好的样本效率，能够适应系统变化。 |
| [^98] | [STAS: Spatial-Temporal Return Decomposition for Multi-agent Reinforcement Learning.](http://arxiv.org/abs/2304.07520) | 提出了一种名为STAS的新方法，用于多智能体强化学习中时空回报分解，可以对代理进行信用分配。该方法引入了Shapley值和空间-时间注意机制来解决先前方法中延迟全局回报的复杂关系问题。在各种基准环境下，该方法表现良好。 |
| [^99] | [Model sparsification can simplify machine unlearning.](http://arxiv.org/abs/2304.04934) | 本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。 |
| [^100] | [On Model Compression for Neural Networks: Framework, Algorithm, and Convergence Guarantee.](http://arxiv.org/abs/2303.06815) | 本文提出了一个框架和算法，从非凸优化的角度来进行神经网络模型压缩。算法解决了梯度消失/爆炸问题，并保证了收敛性。 |
| [^101] | [Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction.](http://arxiv.org/abs/2303.02468) | 本研究研究了在软硬标签预测中，不同激活函数对于深度神经网络模型输出的影响，并引入了一种新的正弦激活函数。 |
| [^102] | [Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays.](http://arxiv.org/abs/2302.13991) | 通过内容感知的风格不变模型，我们提出了一种解决深度学习医学图像分析中源领域不匹配挑战的方法。我们采用了风格随机化模块来提取既是风格不变又是内容偏好的领域不变特征，在胸部X射线疾病检测中取得了良好的性能。 |
| [^103] | [Improved uncertainty quantification for neural networks with Bayesian last layer.](http://arxiv.org/abs/2302.10975) | 本文提出了一种改进神经网络不确定性量化的方法，使用贝叶斯最后一层近似不可处理的贝叶斯神经网络，通过最大化边际来获得权重的点估计。 |
| [^104] | [Attacks in Adversarial Machine Learning: A Systematic Survey from the Life-cycle Perspective.](http://arxiv.org/abs/2302.09457) | 本研究通过对抗机器学习中攻击现象的生命周期角度进行系统调查，提供一种统一的视角和数学框架，总结整体进展。 |
| [^105] | [Stochastic Approximation Approaches to Group Distributionally Robust Optimization.](http://arxiv.org/abs/2302.09267) | 本文提出了一种随机逼近法，用于组分布式鲁棒优化，该算法利用在线学习技术，将每轮所需的样本数从$m$个降至$1$个，同时保持相同的样本复杂度。 |
| [^106] | [A Comprehensive Survey on Graph Summarization with Graph Neural Networks.](http://arxiv.org/abs/2302.06114) | 本文综述了基于图神经网络的深度学习概括技术，在保留图形关键特征的同时，对大规模、高维度和复杂的现代图形数据进行处理的方法，包括图神经网络、图形自编码器等。 |
| [^107] | [Learning Discretized Neural Networks under Ricci Flow.](http://arxiv.org/abs/2302.03390) | 本文使用信息几何构造了线性几乎欧几里得流形，通过引入偏微分方程Ricci流，解决了离散神经网络训练中梯度无穷或零的问题。 |
| [^108] | [Computational Discovery of Microstructured Composites with Optimal Stiffness-Toughness Trade-Offs.](http://arxiv.org/abs/2302.01078) | 该论文介绍了一种结合了物理实验、数值模拟和人工神经网络的可推广管道，用于系统地发现具有最佳刚度-韧度权衡的微结构复合材料。这种方法通过嵌套循环的建议-验证工作流，成功弥合了模拟和现实之间的差距，并以高样本效率发现了刚度和韧度兼具的材料。进一步分析帕累托最优设计，可以自动识别出现有的增加韧性机制。 |
| [^109] | [Approximating the Shapley Value without Marginal Contributions.](http://arxiv.org/abs/2302.00736) | 本文提出了两种无参数、领域无关的Shapley值近似算法SVARM和Stratified SVARM，它们基于一种与边际贡献概念脱钩的Shapley值表示。我们证明了它们在近似质量方面具有无与伦比的理论保证，并通过实证结果将其与合成游戏和常见可解释性用例进行了比较。 |
| [^110] | [Anatomy-aware and acquisition-agnostic joint registration with SynthMorph.](http://arxiv.org/abs/2301.11329) | SynthMorph是一个易于使用的DL工具，用于无需预处理即可直接从MRI扫描仪上对任何脑图像进行联合仿射-可变形配准，采用了从标签图生成具有极大差异图像的策略，实现了更准确和鲁棒的图像配准。 |
| [^111] | [DeepTaster: Adversarial Perturbation-Based Fingerprinting to Identify Proprietary Dataset Use in Deep Neural Networks.](http://arxiv.org/abs/2211.13535) | DeepTaster是一种新颖的DNN指纹识别技术，可以有效识别使用受害者数据非法构建嫌疑模型的攻击，即使嫌疑模型的架构与受害模型不同。 |
| [^112] | [Controlling Moments with Kernel Stein Discrepancies.](http://arxiv.org/abs/2211.05408) | 本研究分析了核斯坦离差（KSD）控制性质，发现标准KSD无法控制矩的收敛，提出了可控制矩和弱收敛的下游扩散KSD，并且发展了可以准确描述$q$-Wasserstein收敛的KSD。 |
| [^113] | [Generalized Quadratic Embeddings for Nonlinear Dynamics using Deep Learning.](http://arxiv.org/abs/2211.00357) | 本文提出了一种使用深度学习来处理非线性动力学的方法。该方法通过学习坐标转换，将非线性系统的动力学表示为二次系统，从而简化了非线性系统的建模过程。 |
| [^114] | [Towards Optimization and Model Selection for Domain Generalization: A Mixup-guided Solution.](http://arxiv.org/abs/2209.00652) | 本文提出了一种基于Mixup引导的优化与选择技术，用于领域泛化，通过生成目标领域指导数据和更接近目标分布的验证数据集，能够提高模型性能。 |
| [^115] | [ULF: Unsupervised Labeling Function Correction using Cross-Validation for Weak Supervision.](http://arxiv.org/abs/2204.06863) | ULF是一种用于弱监督学习的无监督标签函数校正方法，通过基于交叉验证原理的噪声降低技术，有效提高了弱监督学习的性能，无需手动标记。 |
| [^116] | [Federated Optimization of Smooth Loss Functions.](http://arxiv.org/abs/2201.01954) | 本文研究了在联邦学习框架下的平均分布优化平滑损失函数的问题，提出了联邦低秩梯度下降（FedLRGD）算法来利用数据的平滑性，从而实现更好的优化结果。 |
| [^117] | [Data Valuation for Vertical Federated Learning: A Model-free and Privacy-preserving Method.](http://arxiv.org/abs/2112.08364) | 提出了一种垂直联合学习中的数据价值评估方法FedValue，该方法是隐私保护、针对任务而无需模型的。它包括数据价值度量MShapley-CMI和联合计算方法，有助于解决垂直联合学习中评估数据方数据价值的问题。 |
| [^118] | [Polymorphic dynamic programming by algebraic shortcut fusion.](http://arxiv.org/abs/2107.01752) | 本文提出了基于代数快捷规范与DP算法的多态动态规划方法，可用于解决在半环上表示的所有组合问题，包括优化、逻辑推理、模糊集、可微softmax等问题。 |
| [^119] | [Covert Channel Attack to Federated Learning Systems.](http://arxiv.org/abs/2104.10561) | 这篇论文介绍了一种针对联邦学习系统的新型攻击模型，通过在联邦训练期间污染全局模型实现隐蔽通信，而不影响模型性能。 |
| [^120] | [Handling Noisy Labels via One-Step Abductive Multi-Target Learning and Its Application to Helicobacter Pylori Segmentation.](http://arxiv.org/abs/2011.14956) | 本文研究了处理噪声标签的新方法，特别针对医学组织病理学图像分析中的困难情况。通过一步式绳索多目标学习，该方法克服了标签中存在的复杂噪声和评估策略不明确的问题。 |

# 详细

[^1]: 任务导向对话作为自监督自动语音识别的催化剂

    Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic Speech Recognition. (arXiv:2401.02417v1 [eess.AS])

    [http://arxiv.org/abs/2401.02417](http://arxiv.org/abs/2401.02417)

    本研究引入了一种对话对比学习方法，用于对自动语音识别模型进行自监督微调，以改善低质量语音识别结果导致的自然语言理解应用的失败。实验证明，这种方法在任务导向的对话数据集上可以将ASR模型的性能提高达到19.2%。

    

    虽然自动语音识别（ASR）系统的词错误率不断下降，但基于ASR系统构建的自然语言理解（NLU）应用仍然归因于低质量的语音识别结果导致的大量失败。现有的助手系统收集了大量这些失败的交互，但是这些系统通常无法从这些交互中学习，即使是离线学习也是如此。在这项工作中，我们引入了CLC：对话对比学习，这是一系列自监督方法，用于对模型进行对比微调，利用失败对话中容易检测到的人工痕迹。我们证明了我们的CLC系列方法可以提高ASR模型在OD3上的性能，这是一个新的公开的大规模半合成元数据集，对话是以任务为导向的，提升幅度高达19.2%。这些增益也可以转移到现实世界的系统中，我们展示了CLC可以帮助提高性能。

    While word error rates of automatic speech recognition (ASR) systems have consistently fallen, natural language understanding (NLU) applications built on top of ASR systems still attribute significant numbers of failures to low-quality speech recognition results. Existing assistant systems collect large numbers of these unsuccessful interactions, but these systems usually fail to learn from these interactions, even in an offline fashion. In this work, we introduce CLC: Contrastive Learning for Conversations, a family of methods for contrastive fine-tuning of models in a self-supervised fashion, making use of easily detectable artifacts in unsuccessful conversations with assistants. We demonstrate that our CLC family of approaches can improve the performance of ASR models on OD3, a new public large-scale semi-synthetic meta-dataset of audio task-oriented dialogues, by up to 19.2%. These gains transfer to real-world systems as well, where we show that CLC can help to improve performance 
    
[^2]: ODIN: 一个用于2D和3D感知的单一模型

    ODIN: A Single Model for 2D and 3D Perception. (arXiv:2401.02416v1 [cs.CV])

    [http://arxiv.org/abs/2401.02416](http://arxiv.org/abs/2401.02416)

    ODIN是一个模型，可以同时对2D RGB图像和3D点云进行分割和标记，使用变压器架构进行2D和3D视图间的信息融合。

    

    目前的先进模型在像ScanNet这样的当代3D感知基准上使用并标记依赖于数据集提供的3D点云，该点云是通过对感知到的多视角RGB-D图像进行后处理获得的。它们通常在领域内进行训练，放弃了大规模的2D预训练，并且胜过将姿态RGB-D多视角图像进行特征化的替代方案。消耗姿态图像和后处理的3D点云之间的性能差距，加剧了2D和3D感知需要不同模型架构的观点。在本文中，我们挑战这个观点，并提出ODIN（Omni-Dimensional INstance segmentation），一种能够使用变压器架构对2D RGB图像和3D点云进行分割和标记的模型，该模型通过交替的2D视图内和3D视图间信息融合来区分2D和3D特征操作，利用涉及的令牌的位置编码来捕捉2D补丁令牌和3D坐标的像素坐标。

    State-of-the-art models on contemporary 3D perception benchmarks like ScanNet consume and label dataset-provided 3D point clouds, obtained through post processing of sensed multiview RGB-D images. They are typically trained in-domain, forego large-scale 2D pre-training and outperform alternatives that featurize the posed RGB-D multiview images instead. The gap in performance between methods that consume posed images versus post-processed 3D point clouds has fueled the belief that 2D and 3D perception require distinct model architectures. In this paper, we challenge this view and propose ODIN (Omni-Dimensional INstance segmentation), a model that can segment and label both 2D RGB images and 3D point clouds, using a transformer architecture that alternates between 2D within-view and 3D cross-view information fusion. Our model differentiates 2D and 3D feature operations through the positional encodings of the tokens involved, which capture pixel coordinates for 2D patch tokens and 3D coor
    
[^3]: 基于分位数回归的模拟推断方法

    Simulation-Based Inference with Quantile Regression. (arXiv:2401.02413v1 [stat.ML])

    [http://arxiv.org/abs/2401.02413](http://arxiv.org/abs/2401.02413)

    提出了一种基于模拟推断和分位数回归的新方法，通过学习个体化的分位数来估计后验样本，并使用局部累积密度函数定义贝叶斯可信区间，具有更快的评估速度。同时，还可以集成后处理扩展步骤以保证后验估计的无偏性，而计算成本几乎可以忽略不计。

    

    我们提出了一种基于条件分位数回归的新型模拟推断（Simulation-Based Inference，SBI）方法——神经分位数估计（Neural Quantile Estimation，NQE）。NQE通过自回归方式学习每个后验维度的单一维度分位数，以数据和之前的后验维度为条件。后验样本通过使用单调三次埃尔米特样条插值预测分位数进行获取，并对尾部行为和多模态分布进行了特殊处理。我们引入了一种使用局部累积密度函数（CDF）的贝叶斯可信区间的替代定义，其评估速度比传统的最高后验密度区域（HPDR）快得多。在模拟预算有限和/或已知模型错误规范的情况下，可以将后处理扩展步骤集成到NQE中，以确保后验估计的无偏性，且附加的计算成本可以忽略不计。我们证明了所提出的NQE方法达到了最新的研究水平。

    We present Neural Quantile Estimation (NQE), a novel Simulation-Based Inference (SBI) method based on conditional quantile regression. NQE autoregressively learns individual one dimensional quantiles for each posterior dimension, conditioned on the data and previous posterior dimensions. Posterior samples are obtained by interpolating the predicted quantiles using monotonic cubic Hermite spline, with specific treatment for the tail behavior and multi-modal distributions. We introduce an alternative definition for the Bayesian credible region using the local Cumulative Density Function (CDF), offering substantially faster evaluation than the traditional Highest Posterior Density Region (HPDR). In case of limited simulation budget and/or known model misspecification, a post-processing broadening step can be integrated into NQE to ensure the unbiasedness of the posterior estimation with negligible additional computational cost. We demonstrate that the proposed NQE method achieves state-of
    
[^4]: LLM增强的LLMs：通过组合扩展功能

    LLM Augmented LLMs: Expanding Capabilities through Composition. (arXiv:2401.02412v1 [cs.LG])

    [http://arxiv.org/abs/2401.02412](http://arxiv.org/abs/2401.02412)

    本文提出了CALM方法，通过组合现有的基础模型和更具体的模型，使用交叉注意力来增强模型的表示并实现新的能力。CALM可以通过“重用”现有模型和一些额外的参数和数据来扩展新任务上的模型规模，并且保留现有模型的功能。

    

    在大型数据集上训练的具有数十亿个参数的基础模型已经展现出在各个领域具有非平凡技能。然而，由于它们的整体结构，对它们进行增强或赋予新的技能是具有挑战性和成本高昂的。另一方面，由于其适应能力，正在训练多个新领域和任务的模型实例。在这项工作中，我们研究了利用现有的基础模型与更具体模型进行高效实用的组合，以实现新的功能。为此，我们提出了CALM -用于增强语言模型的组合模型-，它引入了模型之间的交叉注意力，以组合它们的表示并实现新的能力。CALM的显著特点包括：(i)通过“重用”现有LLMs和一些额外的参数和数据来扩展新任务上的LLMs的规模，(ii)保持现有模型权重不变，从而保留现有功能，(iii)应用新功能只需要对增加的模型进行微调。

    Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Appli
    
[^5]: 你所见即所GAN: 在3D GAN中为高保真度几何图形渲染每个像素

    What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs. (arXiv:2401.02411v1 [cs.CV])

    [http://arxiv.org/abs/2401.02411](http://arxiv.org/abs/2401.02411)

    本文提出了将神经体积渲染扩展到本地2D图像更高分辨率的技术，以解决3D GANs无法解决2D图像中丰富的3D几何图形的问题。

    

    3D感知生成对抗网络(GANs)在通过神经体积渲染从2D图像集合中学习生成多视角一致的图像和场景3D几何图形方面取得了显著进展。然而，体积渲染中密集采样导致的显著内存和计算开销迫使3D GANs采用基于块的训练或采用低分辨率渲染与后处理的2D超分辨率，这损害了多视角一致性和解决几何图形的质量。因此，3D GANs尚不能完全解决2D图像中丰富的3D几何图形。在本文中，我们提出了将神经体积渲染扩展到本地2D图像更高分辨率的技术，因此能以前所未有的细节解决细粒度的3D几何图形。我们的方法使用基于学习的采样器来加速3D GAN训练中的神经渲染，使用更少的深度采样次数高达5倍。这使我们能够明确地“渲染每个像素”。

    3D-aware Generative Adversarial Networks (GANs) have shown remarkable progress in learning to generate multi-view-consistent images and 3D geometries of scenes from collections of 2D images via neural volume rendering. Yet, the significant memory and computational costs of dense sampling in volume rendering have forced 3D GANs to adopt patch-based training or employ low-resolution rendering with post-processing 2D super resolution, which sacrifices multiview consistency and the quality of resolved geometry. Consequently, 3D GANs have not yet been able to fully resolve the rich 3D geometry present in 2D images. In this work, we propose techniques to scale neural volume rendering to the much higher resolution of native 2D images, thereby resolving fine-grained 3D geometry with unprecedented detail. Our approach employs learning-based samplers for accelerating neural rendering for 3D GAN training using up to 5 times fewer depth samples. This enables us to explicitly "render every pixel" o
    
[^6]: 金属增材制造中基于物理信息的神经网络实时二维温度场预测

    Real-Time 2D Temperature Field Prediction in Metal Additive Manufacturing Using Physics-Informed Neural Networks. (arXiv:2401.02403v1 [cs.LG])

    [http://arxiv.org/abs/2401.02403](http://arxiv.org/abs/2401.02403)

    本论文介绍了一种基于物理信息的神经网络框架，用于实时预测金属增材制造中的二维温度场。该框架结合了基于物理信息的输入、损失函数和卷积长短期记忆架构，通过使用实时温度数据，可以预测未来时间戳下不同几何形状、沉积模式和工艺的温度场。

    

    准确预测金属增材制造过程中的温度场对于防止过热、调整工艺参数和确保工艺稳定性至关重要。虽然基于物理的计算模型提供了精确性，但往往耗时且不适用于迭代设计场景中的实时预测和在线控制。相反，机器学习模型严重依赖高质量数据集，而在金属增材制造领域内获得这样的数据集往往成本高且具有挑战性。我们的工作通过引入一种专门针对金属增材制造中温度场预测的基于物理信息的神经网络框架来解决这个问题。该框架包括基于物理信息的输入、基于物理信息的损失函数和卷积长短期记忆(ConvLSTM)架构。利用工艺过程中的实时温度数据，我们的模型预测未来时间戳下各种几何形状、沉积模式和工艺的二维温度场。

    Accurately predicting the temperature field in metal additive manufacturing (AM) processes is critical to preventing overheating, adjusting process parameters, and ensuring process stability. While physics-based computational models offer precision, they are often time-consuming and unsuitable for real-time predictions and online control in iterative design scenarios. Conversely, machine learning models rely heavily on high-quality datasets, which can be costly and challenging to obtain within the metal AM domain. Our work addresses this by introducing a physics-informed neural network framework specifically designed for temperature field prediction in metal AM. This framework incorporates a physics-informed input, physics-informed loss function, and a Convolutional Long Short-Term Memory (ConvLSTM) architecture. Utilizing real-time temperature data from the process, our model predicts 2D temperature fields for future timestamps across diverse geometries, deposition patterns, and proce
    
[^7]: 生成神经算子的合成数据

    Generating synthetic data for neural operators. (arXiv:2401.02398v1 [cs.LG])

    [http://arxiv.org/abs/2401.02398](http://arxiv.org/abs/2401.02398)

    该论文提出了一种生成神经算子的合成数据的新方法，为训练网络提供不需要数值求解PDE的数据。

    

    近期文献中的许多发展展示了深度学习在获取偏微分方程（PDEs）的数值解方面的潜力，这超出了当前数值求解器的能力。然而，数据驱动的神经算子都存在同样的问题：训练网络所需的数据依赖于传统的数值求解器，如有限差分或有限元等。本文提出了一种新方法，用于生成合成的函数训练数据，而无需数值求解PDE。我们的方法很简单：我们从已知解位于的经典理论解空间（例如$H_0^1(\Omega)$）中抽取大量独立同分布的“随机函数”$u_j$，然后将每个随机解方案代入方程并获得相应的右侧函数$f_j$，将$(f_j, u_j)_{j=1}^N$作为监督训练数据。

    Numerous developments in the recent literature show the promising potential of deep learning in obtaining numerical solutions to partial differential equations (PDEs) beyond the reach of current numerical solvers. However, data-driven neural operators all suffer from the same problem: the data needed to train a network depends on classical numerical solvers such as finite difference or finite element, among others. In this paper, we propose a new approach to generating synthetic functional training data that does not require solving a PDE numerically. The way we do this is simple: we draw a large number $N$ of independent and identically distributed `random functions' $u_j$ from the underlying solution space (e.g., $H_0^1(\Omega)$) in which we know the solution lies according to classical theory. We then plug each such random candidate solution into the equation and get a corresponding right-hand side function $f_j$ for the equation, and consider $(f_j, u_j)_{j=1}^N$ as supervised trai
    
[^8]: 物理信息操作学习和有限元方法的融合，用于参数学习偏微分方程

    Integration of physics-informed operator learning and finite element method for parametric learning of partial differential equations. (arXiv:2401.02363v1 [cs.LG])

    [http://arxiv.org/abs/2401.02363](http://arxiv.org/abs/2401.02363)

    本文提出了一种使用物理信息深度学习技术参数化求解偏微分方程的方法，在异质固体中研究了稳态热方程。该方法可以独立于有限元方法等经典求解器，并使用新颖的损失函数定义，降低了导数阶数和自动微分需求。

    

    我们提出了一种使用物理信息深度学习技术来参数化求解偏微分方程的方法。重点是研究具有显著相位对比的异质固体中的稳态热方程。类似的方程在化学扩散、静电学和达西流等各种应用中都存在。神经网络的目标是建立复杂热导率分布与温度分布之间的关系，以及在固定边界条件下微结构中的热流分量。一个独特的方面是我们在数据上独立于有限元方法等经典求解器。一个值得注意的贡献在于我们对损失函数的新颖定义，基于离散化的弱形式控制方程。这不仅降低了所需导数的阶数，还消除了在构建损失项中的自动微分需求，接受了潜在的数值不稳定性。

    We present a method that employs physics-informed deep learning techniques for parametrically solving partial differential equations. The focus is on the steady-state heat equations within heterogeneous solids exhibiting significant phase contrast. Similar equations manifest in diverse applications like chemical diffusion, electrostatics, and Darcy flow. The neural network aims to establish the link between the complex thermal conductivity profiles and temperature distributions, as well as heat flux components within the microstructure, under fixed boundary conditions. A distinctive aspect is our independence from classical solvers like finite element methods for data. A noteworthy contribution lies in our novel approach to defining the loss function, based on the discretized weak form of the governing equation. This not only reduces the required order of derivatives but also eliminates the need for automatic differentiation in the construction of loss terms, accepting potential numeri
    
[^9]: 分析深度强化学习中泛化性能的调查

    A Survey Analyzing Generalization in Deep Reinforcement Learning. (arXiv:2401.02349v1 [cs.LG])

    [http://arxiv.org/abs/2401.02349](http://arxiv.org/abs/2401.02349)

    本文调查了深度强化学习中的泛化性能。深度强化学习策略存在过拟合问题，限制了它们的鲁棒性和泛化能力。研究形式化和统一了提高泛化性和克服过拟合的不同解决方案。

    

    利用深度神经网络解决高维状态或动作空间中的问题，强化学习研究在实践中取得了重要的成功和关注。尽管深度强化学习策略目前在许多领域中正在被应用，从医疗应用到自动驾驶车辆，但关于深度强化学习策略的泛化能力仍有许多待解答的问题。在本文中，我们将概述深度强化学习策略遇到过拟合问题的根本原因，限制了它们的鲁棒性和泛化能力。此外，我们将对提高泛化性和克服状态-动作值函数中的过拟合的不同解决方案进行形式化和统一。我们相信我们的研究可以为当前深度强化学习的进展提供一个简洁系统的统一分析，并有助于构建健壮的深度神经网络策略。

    Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to self driving vehicles, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will outline the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their robustness and generalization capabilities. Furthermore, we will formalize and unify the diverse solution approaches to increase generalization, and overcome overfitting in state-action value functions. We believe our study can provide a compact systematic unified analysis for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies 
    
[^10]: 基于变压器特征生成的多源域自适应用于独立于学科的基于脑电信号的情绪识别

    Multi-Source Domain Adaptation with Transformer-based Feature Generation for Subject-Independent EEG-based Emotion Recognition. (arXiv:2401.02344v1 [cs.LG])

    [http://arxiv.org/abs/2401.02344](http://arxiv.org/abs/2401.02344)

    本文提出了一种基于变压器特征生成器的多源域自适应方法（MSDA-TF），用于独立于学科的基于脑电信号的情绪识别。该方法通过利用多个来源的信息，保留浅层空间、时间和频谱的脑电数据表示，并提取其中的全局依赖关系。

    

    尽管基于深度学习的算法在通过脑电信号进行情绪识别方面展示了出色的性能，但个体之间的脑信号模式变化可能会降低模型在不同学科中的有效性。尽管迁移学习技术展示出了有希望的结果，但仍面临与不充分的特征表示相关的挑战，并可能忽视源主体本身可以具有不同特征的事实。在这项工作中，我们提出了一种基于变压器特征生成器的多源域自适应方法（MSDA-TF），旨在利用多个来源的信息。所提出的特征生成器保留卷积层以捕捉浅层空间、时间和频谱的脑电数据表示，而自注意机制提取这些特征中的全局依赖性。在自适应过程中，我们根据相关性将源主体分组。

    Although deep learning-based algorithms have demonstrated excellent performance in automated emotion recognition via electroencephalogram (EEG) signals, variations across brain signal patterns of individuals can diminish the model's effectiveness when applied across different subjects. While transfer learning techniques have exhibited promising outcomes, they still encounter challenges related to inadequate feature representations and may overlook the fact that source subjects themselves can possess distinct characteristics. In this work, we propose a multi-source domain adaptation approach with a transformer-based feature generator (MSDA-TF) designed to leverage information from multiple sources. The proposed feature generator retains convolutional layers to capture shallow spatial, temporal, and spectral EEG data representations, while self-attention mechanisms extract global dependencies within these features. During the adaptation process, we group the source subjects based on corr
    
[^11]: 基于对抗功率追踪的逃逸硬件木马

    Evasive Hardware Trojan through Adversarial Power Trace. (arXiv:2401.02342v1 [cs.CR])

    [http://arxiv.org/abs/2401.02342](http://arxiv.org/abs/2401.02342)

    本论文介绍了一种基于对抗功率追踪的逃逸硬件木马方法，通过对抗性噪声混淆侧信道分析，使得硬件木马能够绕过机器学习的检测方法。

    

    集成电路（IC）供应链的全球化使得IC易受到硬件木马（HT）的威胁。面对这种威胁，一种有希望的方法是使用基于机器学习（ML）的侧信道分析，这种方法具有非入侵性的优势，并能在黄金芯片无干扰的情况下高效地检测HT。本文质疑基于机器学习的侧信道分析来检测HT的可靠性。我们引入了一种HT混淆（HTO）方法，以允许HT绕过这种检测方法。我们的方法的一个关键方面是在电路的一部分和HT一起设计和实现对抗性噪声，而不是通过模拟对抗性迹线来理论上误导模型。我们详细介绍了ASIC和FPGA的HTO方法，并使用TrustHub基准进行评估。有趣的是，我们发现HTO可以在ASIC设计中只使用一个晶体管来实现。

    The globalization of the Integrated Circuit (IC) supply chain, driven by time-to-market and cost considerations, has made ICs vulnerable to hardware Trojans (HTs). Against this threat, a promising approach is to use Machine Learning (ML)-based side-channel analysis, which has the advantage of being a non-intrusive method, along with efficiently detecting HTs under golden chip-free settings. In this paper, we question the trustworthiness of ML-based HT detection via side-channel analysis. We introduce a HT obfuscation (HTO) approach to allow HTs to bypass this detection method. Rather than theoretically misleading the model by simulated adversarial traces, a key aspect of our approach is the design and implementation of adversarial noise as part of the circuitry, alongside the HT. We detail HTO methodologies for ASICs and FPGAs, and evaluate our approach using TrustHub benchmark. Interestingly, we found that HTO can be implemented with only a single transistor for ASIC designs to genera
    
[^12]: 超越提取：为语言模型提供上下文化的表格数据以实现高效摘要

    Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models. (arXiv:2401.02333v1 [cs.LG])

    [http://arxiv.org/abs/2401.02333](http://arxiv.org/abs/2401.02333)

    本研究提出了一种创新的方法，通过上下文化表格数据来提高 RAG 系统中处理复杂表格查询的准确性，提高了摘要的效率。

    

    传统的检索增强生成 (RAG) 架构在从各种文件中检索信息方面已被证明是有效的。然而，在处理包含复杂表格结构的 PDF 文档中的复杂表格查询时会遇到挑战。本研究引入了一种创新的方法来提高 RAG 系统中复杂表格查询的准确性。我们的方法涉及将 PDF 存储在检索数据库中，并单独提取表格内容。提取的表格经过上下文丰富的处理，将标题与相应的值连接起来。为了确保对丰富数据的全面理解，我们使用经过微调的 Llama-2-chat 语言模型在 RAG 架构中进行摘要。此外，我们通过一次性提示使用 ChatGPT 3.5 API 增强表格数据的上下文含义。然后，将这些丰富的数据与其他 PDF 文件一起输入检索数据库。

    The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective for retrieving information from diverse documents. However, challenges arise in handling complex table queries, especially within PDF documents containing intricate tabular structures.This research introduces an innovative approach to enhance the accuracy of complex table queries in RAG-based systems. Our methodology involves storing PDFs in the retrieval database and extracting tabular content separately. The extracted tables undergo a process of context enrichment, concatenating headers with corresponding values. To ensure a comprehensive understanding of the enriched data, we employ a fine-tuned version of the Llama-2-chat language model for summarisation within the RAG architecture. Furthermore, we augment the tabular data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt. This enriched data is then fed into the retrieval database alongside other PDFs. Our appr
    
[^13]: 不是所有的少数群体都是平等的: 空类别感知的异质联邦学习方法

    Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning. (arXiv:2401.02329v1 [cs.LG])

    [http://arxiv.org/abs/2401.02329](http://arxiv.org/abs/2401.02329)

    本研究提出了一种异质联邦学习方法FedED，通过同时进行空类别蒸馏和逻辑抑制，解决了在联邦学习中尚未充分识别空类别的问题。

    

    数据异质性是联邦学习中的一个重大挑战，表现为客户端之间本地数据分布的差异。现有方法常常在本地训练过程中采用类别平衡的技术来解决本地类别分布的异质性问题。然而，在少数类别中由于过拟合本地不平衡数据而导致准确性较差的问题仍然存在。本文提出了FedED，这是一种新颖的异质联邦学习方法，同时整合了空类别蒸馏和逻辑抑制。具体而言，空类别蒸馏利用知识蒸馏的方法在每个客户端的本地训练中保留了与空类别相关的重要信息。此外，逻辑抑制直接阻断了预测结果中对空类别的输出。

    Data heterogeneity, characterized by disparities in local data distribution across clients, poses a significant challenge in federated learning. Substantial efforts have been devoted to addressing the heterogeneity in local label distribution. As minority classes suffer from worse accuracy due to overfitting on local imbalanced data, prior methods often incorporate class-balanced learning techniques during local training. Despite the improved mean accuracy across all classes, we observe that empty classes-referring to categories absent from a client's data distribution-are still not well recognized. This paper introduces FedED, a novel approach in heterogeneous federated learning that integrates both empty-class distillation and logit suppression simultaneously. Specifically, empty-class distillation leverages knowledge distillation during local training on each client to retain essential information related to empty classes from the global model. Moreover, logit suppression directly p
    
[^14]: 分布强化学习中具有可解释参数调整的鲁棒分位数Huber损失

    A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning. (arXiv:2401.02325v1 [cs.LG])

    [http://arxiv.org/abs/2401.02325](http://arxiv.org/abs/2401.02325)

    这篇论文提出了一种鲁棒的分位数Huber损失函数，在分布强化学习中通过捕捉噪声并调整参数来增强对异常值的鲁棒性。实证测试验证了该方法的有效性。

    

    分布强化学习通过最小化分位数Huber损失函数来估计回报分布，该函数从高斯分布之间的Wasserstein距离计算中产生，捕捉到当前和目标分位数值中的噪声。与经典的分位数Huber损失相比，这种创新的损失函数增强了对异常值的鲁棒性，并且通过近似数据中噪声的数量来调整参数。实证测试在分布强化学习的常见应用Atari游戏和最近的对冲策略中验证了该方法的有效性。

    Distributional Reinforcement Learning (RL) estimates return distribution mainly by learning quantile values via minimizing the quantile Huber loss function, entailing a threshold parameter often selected heuristically or via hyperparameter search, which may not generalize well and can be suboptimal. This paper introduces a generalized quantile Huber loss function derived from Wasserstein distance (WD) calculation between Gaussian distributions, capturing noise in predicted (current) and target (Bellman-updated) quantile values. Compared to the classical quantile Huber loss, this innovative loss function enhances robustness against outliers. Notably, the classical Huber loss function can be seen as an approximation of our proposed loss, enabling parameter adjustment by approximating the amount of noise in the data during the learning process. Empirical tests on Atari games, a common application in distributional RL, and a recent hedging strategy using distributional RL, validate the eff
    
[^15]: 多智能体上下文学习策略在毫米波车联网通信中的干扰感知波束分配中的应用

    Multi-Agent Context Learning Strategy for Interference-Aware Beam Allocation in mmWave Vehicular Communications. (arXiv:2401.02323v1 [eess.SP])

    [http://arxiv.org/abs/2401.02323](http://arxiv.org/abs/2401.02323)

    该论文提出了一种名为MACOL的新策略，利用背景赌博机管理干扰，并分配mmWave波束以服务车辆。

    

    毫米波（mmWave）被认为是5G及其后续网络的关键技术之一，因为它可以增强信道带宽和网络容量。在各种应用中广泛讨论了将mmWave应用于车联网通信。然而，将mmWave应用于车联网通信面临着高移动节点和mmWave波束狭窄覆盖的挑战。由于密集网络中的高移动性，重叠的波束可能会引起强干扰，导致性能下降。为了解决干扰控制中的复杂性问题，我们开发了一种名为多智能体上下文学习（MACOL）的新策略，该策略利用背景赌博机来管理干扰，同时分配mmWave波束以服务车辆。

    Millimeter wave (mmWave) has been recognized as one of key technologies for 5G and beyond networks due to its potential to enhance channel bandwidth and network capacity. The use of mmWave for various applications including vehicular communications has been extensively discussed. However, applying mmWave to vehicular communications faces challenges of high mobility nodes and narrow coverage along the mmWave beams. Due to high mobility in dense networks, overlapping beams can cause strong interference which leads to performance degradation. As a remedy, beam switching capability in mmWave can be utilized. Then, frequent beam switching and cell change become inevitable to manage interference, which increase computational and signalling complexity. In order to deal with the complexity in interference control, we develop a new strategy called Multi-Agent Context Learning (MACOL), which utilizes Contextual Bandit to manage interference while allocating mmWave beams to serve vehicles in the 
    
[^16]: 鲁棒的物理信息神经网络

    Robust Physics Informed Neural Networks. (arXiv:2401.02300v1 [cs.LG])

    [http://arxiv.org/abs/2401.02300](http://arxiv.org/abs/2401.02300)

    引入了一种鲁棒的物理信息神经网络（RPINNs）来近似偏微分方程（PDE）的解，该网络在训练过程中考虑了PDE的控制物理法则，解决了传统PINNs中损失函数与真实误差不鲁棒的问题。

    

    我们引入了一种鲁棒版本的物理信息神经网络（RPINNs）来近似偏微分方程（PDE）的解。标准的物理信息神经网络（PINN）在学习过程中考虑了由PDE描述的控制物理法则。该网络在由物理域和边界随机选择的数据集上进行训练。PINNs已成功应用于解决由PDE和边界条件描述的各种问题。传统PINNs中的损失函数基于PDE的强残差。这种PINNs中的损失函数通常对真实误差不具有鲁棒性。PINNs中的损失函数与真实误差可能相差很大，这使得训练过程更加困难。特别是，如果我们不知道精确解，我们就不能估计训练过程是否已经以所需的精度收敛到解。这在我们不知道精确解时尤其正确，

    We introduce a Robust version of the Physics-Informed Neural Networks (RPINNs) to approximate the Partial Differential Equations (PDEs) solution. Standard Physics Informed Neural Networks (PINN) takes into account the governing physical laws described by PDE during the learning process. The network is trained on a data set that consists of randomly selected points in the physical domain and its boundary. PINNs have been successfully applied to solve various problems described by PDEs with boundary conditions. The loss function in traditional PINNs is based on the strong residuals of the PDEs. This loss function in PINNs is generally not robust with respect to the true error. The loss function in PINNs can be far from the true error, which makes the training process more difficult. In particular, we do not know if the training process has already converged to the solution with the required accuracy. This is especially true if we do not know the exact solution, so we cannot estimate the 
    
[^17]: 使用凸凹规划训练单层形态感知器

    Training Single-Layer Morphological Perceptron Using Convex-Concave Programming. (arXiv:2401.02296v1 [cs.LG])

    [http://arxiv.org/abs/2401.02296](http://arxiv.org/abs/2401.02296)

    本文提出了一种使用凸凹规划训练单层形态感知器的方法，该方法通过K-DDCCP算法结合SLMP模型和WDCCP算法来解决二分类问题，并通过实验证实了其有效性。

    

    本文讨论了使用凸凹规划（DCCP）训练单层形态感知器的方法。我们提出了一种称为K-DDCCP的算法，该算法将Ritter和Urcid提出的现有单层形态感知器（SLMP）模型与Charisopoulos和Maragos提出的加权凸凹规划（WDCCP）算法相结合。所提出的训练算法利用凸凹规划过程（DCCP）并将二分类问题建模为非凸优化问题。为了解决这个问题，约束被表示为凸函数之差，从而可以应用DCCP软件包。实验结果证实了K-DDCCP算法在解决二分类问题方面的有效性。总体而言，这项工作通过提出一种扩展SLMP模型能力的算法，为形态学神经网络领域做出了贡献。

    This paper concerns the training of a single-layer morphological perceptron using disciplined convex-concave programming (DCCP). We introduce an algorithm referred to as K-DDCCP, which combines the existing single-layer morphological perceptron (SLMP) model proposed by Ritter and Urcid with the weighted disciplined convex-concave programming (WDCCP) algorithm by Charisopoulos and Maragos. The proposed training algorithm leverages the disciplined convex-concave procedure (DCCP) and formulates a non-convex optimization problem for binary classification. To tackle this problem, the constraints are expressed as differences of convex functions, enabling the application of the DCCP package. The experimental results confirm the effectiveness of the K-DDCCP algorithm in solving binary classification problems. Overall, this work contributes to the field of morphological neural networks by proposing an algorithm that extends the capabilities of the SLMP model.
    
[^18]: 基于路径的知识图谱补全的解释方法

    Path-based Explanation for Knowledge Graph Completion. (arXiv:2401.02290v1 [cs.LG])

    [http://arxiv.org/abs/2401.02290](http://arxiv.org/abs/2401.02290)

    基于路径的KGC解释器Power-Link通过引入图加权技术，实现了可解释的知识图谱补全，推动了模型透明度和可靠性的提升。

    

    近年来，图神经网络（GNNs）通过建模实体和关系的交互在知识图谱补全（KGC）任务中取得了巨大成功。然而，对预测结果的解释却没有得到必要的关注。对基于GNN的KGC模型结果进行适当解释，可以增加模型的透明度，并帮助研究人员开发更可靠的模型。现有的KGC解释方法主要依赖于实例/子图的方法，而在某些场景下，路径可以提供更友好和可解释的解释。然而，还没有对生成基于路径的知识图谱解释方法进行充分探索。为了填补这一空白，我们提出了Power-Link，这是第一个探索基于路径的KGC解释器。我们设计了一种新颖的图加权技术，使得可以以完全可并行化和内存高效的训练方案生成基于路径的解释。我们还引入了三个新的度量指标，用于评估解释的质量和有效性。

    Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph Completion (KGC) by modelling how entities and relations interact in recent years. However, the explanation of the predicted facts has not caught the necessary attention. Proper explanations for the results of GNN-based KGC models increase model transparency and help researchers develop more reliable models. Existing practices for explaining KGC tasks rely on instance/subgraph-based approaches, while in some scenarios, paths can provide more user-friendly and interpretable explanations. Nonetheless, the methods for generating path-based explanations for KGs have not been well-explored. To address this gap, we propose Power-Link, the first path-based KGC explainer that explores GNN-based models. We design a novel simplified graph-powering technique, which enables the generation of path-based explanations with a fully parallelisable and memory-efficient training scheme. We further introduce three new metrics for 
    
[^19]: DEM: 航空航天中用于认证深度神经网络分类器输出的方法

    DEM: A Method for Certifying Deep Neural Network Classifier Outputs in Aerospace. (arXiv:2401.02283v1 [cs.SE])

    [http://arxiv.org/abs/2401.02283](http://arxiv.org/abs/2401.02283)

    这项工作提出了一种新的、以输出为中心的方法，通过统计验证技术来认证深度神经网络(DNN)分类器的输出。该方法能够标记可能不可靠的特定输入，以便后续由人工专家检查。与现有技术相比，该方法主要关注单个输出而不是整个DNN的认证。

    

    航空航天领域的软件开发要求遵循严格、高质量的标准。尽管在这个领域中存在着商用软件的监管指南（例如ARP-4754和DO-178），但这些指南并不适用于具有深度神经网络（DNN）组件的软件。因此，如何使航空航天系统受益于深度学习的革命尚不清楚。我们的研究旨在通过一种新颖的、以输出为中心的方法来解决这个挑战，用于DNN的认证。我们的方法采用统计验证技术，并具有能够标记DNN输出可能不可靠的特定输入的关键优势，以便后续由专家检查。为了实现这一点，我们的方法对DNN对其他附近输入的预测进行统计分析，以检测不一致性。这与现有技术相反，后者通常试图对整个DNN进行认证，而非单个输出。

    Software development in the aerospace domain requires adhering to strict, high-quality standards. While there exist regulatory guidelines for commercial software in this domain (e.g., ARP-4754 and DO-178), these do not apply to software with deep neural network (DNN) components. Consequently, it is unclear how to allow aerospace systems to benefit from the deep learning revolution. Our work here seeks to address this challenge with a novel, output-centric approach for DNN certification. Our method employs statistical verification techniques, and has the key advantage of being able to flag specific inputs for which the DNN's output may be unreliable - so that they may be later inspected by a human expert. To achieve this, our method conducts a statistical analysis of the DNN's predictions for other, nearby inputs, in order to detect inconsistencies. This is in contrast to existing techniques, which typically attempt to certify the entire DNN, as opposed to individual outputs. Our method
    
[^20]: 轻量级鱼类分类模型在可持续海洋管理中的应用: 印度尼西亚案例。

    Lightweight Fish Classification Model for Sustainable Marine Management: Indonesian Case. (arXiv:2401.02278v1 [cs.CV])

    [http://arxiv.org/abs/2401.02278](http://arxiv.org/abs/2401.02278)

    这项研究提出了一种轻量级的鱼类分类模型，利用机器学习技术识别保护鱼类物种，并在有限硬件上运行。该模型在印度尼西亚水域的鱼类图像数据集上进行训练，可以对捕获的鱼类进行分类，并给出是否可食用的建议。

    

    对海产品的巨大需求导致了对海洋资源的过度开发和一些物种濒临灭绝。在可持续海洋发展中，过度捕捞是其中一个主要问题。为了保护海洋资源和可持续捕捞，本研究提出了一种先进的鱼类分类技术，利用最先进的机器学习方法来识别受保护的鱼类物种。我们使用改进的MobileNet模型设计了一个轻量级分类器M-MobileNet，可以在有限硬件上运行。作为研究的一部分，我们编制了一个包含37,462张印度尼西亚群岛水域鱼类图片的标注数据集。所提出的模型对数据集进行训练，将捕获的鱼类的图像分类为不同的物种，并给出是否可食用的建议。我们修改的MobileNet模型仅使用了50%的顶层参数，约42%的GTX 860M利用率，并实现了。

    The enormous demand for seafood products has led to exploitation of marine resources and near-extinction of some species. In particular, overfishing is one the main issues in sustainable marine development. In alignment with the protection of marine resources and sustainable fishing, this study proposes to advance fish classification techniques that support identifying protected fish species using state-of-the-art machine learning. We use a custom modification of the MobileNet model to design a lightweight classifier called M-MobileNet that is capable of running on limited hardware. As part of the study, we compiled a labeled dataset of 37,462 images of fish found in the waters of the Indonesian archipelago. The proposed model is trained on the dataset to classify images of the captured fish into their species and give recommendations on whether they are consumable or not. Our modified MobileNet model uses only 50\% of the top layer parameters with about 42% GTX 860M utility and achiev
    
[^21]: 向量值和超复值神经网络的通用逼近定理

    Universal Approximation Theorem for Vector- and Hypercomplex-Valued Neural Networks. (arXiv:2401.02277v1 [cs.LG])

    [http://arxiv.org/abs/2401.02277](http://arxiv.org/abs/2401.02277)

    该论文通过引入非退化代数的概念，扩展了通用逼近定理，使其适用于广泛的向量值神经网络，包括超复值模型。这对于神经网络在回归和分类任务等多种应用中的应用具有重要意义。

    

    通用逼近定理表明，具有一层隐藏层的神经网络可以以任意所需的精度逼近紧集上的连续函数。该定理支持了神经网络在回归和分类任务等各种应用中的使用。此外，对于实值神经网络和一些超复值神经网络（例如复数、四元数、四元数矢量和Clifford值神经网络），该定理均有效。然而，超复值神经网络是一种在具有附加代数或几何性质的代数上定义的向量值神经网络。本文将通用逼近定理扩展到了广泛的向量值神经网络，包括超复值模型作为特殊实例。具体而言，我们引入了非退化代数的概念，并阐述了在这种代数上定义的神经网络的通用逼近定理。

    The universal approximation theorem states that a neural network with one hidden layer can approximate continuous functions on compact sets with any desired precision. This theorem supports using neural networks for various applications, including regression and classification tasks. Furthermore, it is valid for real-valued neural networks and some hypercomplex-valued neural networks such as complex-, quaternion-, tessarine-, and Clifford-valued neural networks. However, hypercomplex-valued neural networks are a type of vector-valued neural network defined on an algebra with additional algebraic or geometric properties. This paper extends the universal approximation theorem for a wide range of vector-valued neural networks, including hypercomplex-valued models as particular instances. Precisely, we introduce the concept of non-degenerate algebra and state the universal approximation theorem for neural networks defined on such algebras.
    
[^22]: 不确定性感知的深度关注循环神经网络用于异质时间序列插补

    Uncertainty-Aware Deep Attention Recurrent Neural Network for Heterogeneous Time Series Imputation. (arXiv:2401.02258v1 [cs.LG])

    [http://arxiv.org/abs/2401.02258](http://arxiv.org/abs/2401.02258)

    该论文提出了鉴于复杂数据中出现的问题，能够同时估计异质多变量时间序列中缺失值及其相关不确定性的深度关注循环神经网络插补方法。

    

    多变量时间序列中普遍存在缺失，给可靠的下游分析带来了障碍。尽管递归网络插补达到了最先进的水平，但现有模型不能扩展到可以缓解复杂数据中出现的问题的深度结构。此外，插补还存在估计地面真值偏差的风险。然而，对插补值的置信度始终是未被测量的或从模型输出后计算的。我们提出了DEep Attention Recurrent Imputation (DEARI)，它在异质多变量时间序列中同时估计缺失值及其相关的不确定性。通过联合表示特征相关性和时序动态，我们采用了自注意机制和有效的残差组件，实现了一个具有良好插补性能和稳定收敛性的深度循环神经网络。我们还利用自监督度量学习来通过优化样本相似性来提高性能。

    Missingness is ubiquitous in multivariate time series and poses an obstacle to reliable downstream analysis. Although recurrent network imputation achieved the SOTA, existing models do not scale to deep architectures that can potentially alleviate issues arising in complex data. Moreover, imputation carries the risk of biased estimations of the ground truth. Yet, confidence in the imputed values is always unmeasured or computed post hoc from model output. We propose DEep Attention Recurrent Imputation (DEARI), which jointly estimates missing values and their associated uncertainty in heterogeneous multivariate time series. By jointly representing feature-wise correlations and temporal dynamics, we adopt a self attention mechanism, along with an effective residual component, to achieve a deep recurrent neural network with good imputation performance and stable convergence. We also leverage self-supervised metric learning to boost performance by optimizing sample similarity. Finally, we 
    
[^23]: 平衡连续学习和微调在人类活动识别中的应用

    Balancing Continual Learning and Fine-tuning for Human Activity Recognition. (arXiv:2401.02255v1 [cs.LG])

    [http://arxiv.org/abs/2401.02255](http://arxiv.org/abs/2401.02255)

    本文研究了在基于可穿戴设备的人类活动识别中，采用连续自监督学习模型CaSSLe和半监督连续学习模型Kaizen来平衡表征学习和下游分类的问题。这些方案通过对比学习和自训练的结合，实现了知识保留和定制化功能。

    

    基于可穿戴设备的人类活动识别（Human Activity Recognition，HAR）是人机交互机器学习中的一个关键任务，因为它能够对人类行为进行基本的理解。由于人类行为的动态性，连续学习可以为HAR系统提供符合用户需求的定制化功能。然而，由于使用可穿戴传感器收集标记数据的困难性，现有的着重于有监督连续学习的方法应用范围有限，而无监督连续学习方法仅处理表征学习，并将分类器训练推迟到稍后的阶段。本研究探索了将连续自监督学习模型CaSSLe和半监督连续学习模型Kaizen应用于基于可穿戴设备的HAR任务，并对其进行自适应改进。这些方案重新利用对比学习来保留知识，并且Kaizen将其与自训练相结合，形成一个统一的方案，可以同时进行表征学习和下游分类。

    Wearable-based Human Activity Recognition (HAR) is a key task in human-centric machine learning due to its fundamental understanding of human behaviours. Due to the dynamic nature of human behaviours, continual learning promises HAR systems that are tailored to users' needs. However, because of the difficulty in collecting labelled data with wearable sensors, existing approaches that focus on supervised continual learning have limited applicability, while unsupervised continual learning methods only handle representation learning while delaying classifier training to a later stage. This work explores the adoption and adaptation of CaSSLe, a continual self-supervised learning model, and Kaizen, a semi-supervised continual learning model that balances representation learning and down-stream classification, for the task of wearable-based HAR. These schemes re-purpose contrastive learning for knowledge retention and, Kaizen combines that with self-training in a unified scheme that can leve
    
[^24]: L3Cube-IndicNews：印度语系新闻短文和长文分类数据集

    L3Cube-IndicNews: News-based Short Text and Long Document Classification Datasets in Indic Languages. (arXiv:2401.02254v1 [cs.CL])

    [http://arxiv.org/abs/2401.02254](http://arxiv.org/abs/2401.02254)

    L3Cube-IndicNews是一个面向印度语系的多语种文本分类数据集，包括短标题、长文档和长段落三个数据集。它提供了10种印度语言的新闻文章，每个数据集包含10个或更多类别的文章。这个数据集可以用于深入分析和评估。

    

    在这项工作中，我们介绍了L3Cube-IndicNews，这是一个多语种文本分类语料库，旨在为印度地区的各大方言语言提供高质量的数据集，特别关注新闻标题和文章。我们的工作主要集中在10种主要的印度语言上，包括印地语、孟加拉语、马拉地语、泰卢固语、泰米尔语、古吉拉特语、卡纳达语、奥里亚语、马拉雅拉姆语和旁遮普语。每个新闻数据集包含10个或更多类别的新闻文章。L3Cube-IndicNews提供了3个不同数据集，针对不同的文档长度进行分类：短标题分类（SHC）数据集包含新闻标题和新闻类别，长文档分类（LDC）数据集包含整个新闻文章和新闻类别，长段落分类（LPC）数据集包含新闻的子文章和新闻类别。我们在所有3个数据集中都保持了一致的标签，以进行深入的基于长度的分析。我们使用4个指标对每个印度语言数据集进行评估。

    In this work, we introduce L3Cube-IndicNews, a multilingual text classification corpus aimed at curating a high-quality dataset for Indian regional languages, with a specific focus on news headlines and articles. We have centered our work on 10 prominent Indic languages, including Hindi, Bengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and Punjabi. Each of these news datasets comprises 10 or more classes of news articles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle different document lengths that are classified as: Short Headlines Classification (SHC) dataset containing the news headline and news category, Long Document Classification (LDC) dataset containing the whole news article and the news category, and Long Paragraph Classification (LPC) containing sub-articles of the news and the news category. We maintain consistent labeling across all 3 datasets for in-depth length-based analysis. We evaluate each of these Indic language datasets using 4 
    
[^25]: 政策规范化的离线多目标强化学习

    Policy-regularized Offline Multi-objective Reinforcement Learning. (arXiv:2401.02244v1 [cs.LG])

    [http://arxiv.org/abs/2401.02244](http://arxiv.org/abs/2401.02244)

    本文将离线规范化方法扩展到多目标强化学习中，以利用离线轨迹数据训练多目标政策。在面对偏好不一致的演示问题时，提出了过滤方法和正则化技术。通过将偏好条件化标量化更新与政策规范化相结合，可以同时学习一组策略，从而降低计算成本。

    

    本文旨在利用仅使用离线轨迹数据来训练多目标强化学习的政策。我们将广泛采用的用于单目标离线强化学习问题的离线规范化方法扩展到多目标设置，以实现上述目标。然而，在离线多目标强化学习中，这样的方法面临新的挑战，即偏好不一致的演示问题。我们提出了两种解决这个问题的方法：1）通过近似行为偏好来过滤出偏好不一致的演示，和2）采用具有高策略表达能力的正则化技术。此外，我们将偏好条件化标量化更新方法融入到政策规范化的离线强化学习中，以使用单个策略网络同时学习一组策略，从而减少为各种偏好训练大量个体策略所产生的计算成本。最后，我们引入了正则化权重调整方法...

    In this paper, we aim to utilize only offline trajectory data to train a policy for multi-objective RL. We extend the offline policy-regularized method, a widely-adopted approach for single-objective offline RL problems, into the multi-objective setting in order to achieve the above goal. However, such methods face a new challenge in offline MORL settings, namely the preference-inconsistent demonstration problem. We propose two solutions to this problem: 1) filtering out preference-inconsistent demonstrations via approximating behavior preferences, and 2) adopting regularization techniques with high policy expressiveness. Moreover, we integrate the preference-conditioned scalarized update method into policy-regularized offline RL, in order to simultaneously learn a set of policies using a single policy network, thus reducing the computational cost induced by the training of a large number of individual policies for various preferences. Finally, we introduce Regularization Weight Adapta
    
[^26]: U-Mixer: 一种带有稳定性校正的Unet-Mixer时间序列预测架构

    U-Mixer: An Unet-Mixer Architecture with Stationarity Correction for Time Series Forecasting. (arXiv:2401.02236v1 [cs.LG])

    [http://arxiv.org/abs/2401.02236](http://arxiv.org/abs/2401.02236)

    U-Mixer是一种结合Unet和Mixer的框架，通过稳定性校正方法解决时间序列预测中的非稳态挑战，捕捉局部时间依赖关系，并合并特征以获取全面的数据表示。

    

    时间序列预测是各个领域中的关键任务。由于趋势、季节性或不规则波动等因素的影响，时间序列通常呈现非稳态。这阻碍了通过深层次的稳定特征传播，破坏了特征分布，并使学习数据分布变化变得复杂。因此，许多现有模型在捕捉潜在模式方面存在困难，导致预测性能下降。在本研究中，我们通过提出的名为U-Mixer的框架来解决时间序列预测中的非稳态挑战。通过结合Unet和Mixer，U-Mixer有效地捕捉不同块和通道之间的局部时间依赖关系，避免通道之间分布变化的影响，并合并低层和高层特征以获取全面的数据表示。其主要贡献是一种新颖的稳定性校正方法，通过约束差异来明确恢复数据分布。

    Time series forecasting is a crucial task in various domains. Caused by factors such as trends, seasonality, or irregular fluctuations, time series often exhibits non-stationary. It obstructs stable feature propagation through deep layers, disrupts feature distributions, and complicates learning data distribution changes. As a result, many existing models struggle to capture the underlying patterns, leading to degraded forecasting performance. In this study, we tackle the challenge of non-stationarity in time series forecasting with our proposed framework called U-Mixer. By combining Unet and Mixer, U-Mixer effectively captures local temporal dependencies between different patches and channels separately to avoid the influence of distribution variations among channels, and merge low- and high-levels features to obtain comprehensive data representations. The key contribution is a novel stationarity correction method, explicitly restoring data distribution by constraining the difference 
    
[^27]: 基于轨迹导向的稀疏奖励策略优化

    Trajectory-Oriented Policy Optimization with Sparse Rewards. (arXiv:2401.02225v1 [cs.LG])

    [http://arxiv.org/abs/2401.02225](http://arxiv.org/abs/2401.02225)

    该论文提出了一种基于轨迹导向的稀疏奖励策略优化方法，通过利用离线示范轨迹，在稀疏奖励环境中实现更快速、更高效的在线强化学习。

    

    深度强化学习(DRL)在稀疏奖励任务中仍然具有挑战性。这些稀疏奖励通常只表示任务是否部分或完全完成，这意味着在代理获得有用反馈之前必须执行许多探索动作。因此，大多数现有的DRL算法无法在合理的时间内学习可行的策略。为了解决这个问题，我们开发了一种利用离线示范轨迹的方法，在稀疏奖励环境中进行更快速和更高效的在线强化学习。我们的关键见解是，通过将离线示范轨迹视为指导而不是模仿它们，我们的方法学习了一种使状态-动作访问分布与离线示范相匹配的策略。具体来说，我们引入了一种基于最大均值差异(MMD)的轨迹距离，并将策略优化建模为一个受距离约束的优化问题。

    Deep reinforcement learning (DRL) remains challenging in tasks with sparse rewards. These sparse rewards often only indicate whether the task is partially or fully completed, meaning that many exploration actions must be performed before the agent obtains useful feedback. Hence, most existing DRL algorithms fail to learn feasible policies within a reasonable time frame. To overcome this problem, we develop an approach that exploits offline demonstration trajectories for faster and more efficient online RL in sparse reward settings. Our key insight is that by regarding offline demonstration trajectories as guidance, instead of imitating them, our method learns a policy whose state-action visitation marginal distribution matches that of offline demonstrations. Specifically, we introduce a novel trajectory distance based on maximum mean discrepancy (MMD) and formulate policy optimization as a distance-constrained optimization problem. Then, we show that this distance-constrained optimizat
    
[^28]: 基于矩阵变量t分布的鲁棒双线性因子分析

    Robust bilinear factor analysis based on the matrix-variate $t$ distribution. (arXiv:2401.02203v1 [stat.ML])

    [http://arxiv.org/abs/2401.02203](http://arxiv.org/abs/2401.02203)

    本文提出了一种基于矩阵变量t分布的鲁棒双线性因子分析（tbfa）模型，可以从重尾或受污染的矩阵数据中同时提取行和列变量的公共因子。

    

    基于多元t分布的因子分析（tfa）是一种在重尾或受污染数据上提取公共因子的有用的鲁棒工具。然而，tfa只适用于向量数据。当tfa应用于矩阵数据时，通常会先将矩阵观测向量化。这带来了两个挑战：（i）数据的固有矩阵结构被破坏，（ii）鲁棒性可能丢失，因为向量化的矩阵数据通常会导致较高的数据维度，这容易导致tfa的崩溃。为了解决这些问题，本文从矩阵数据的内在结构出发，提出了一种新的鲁棒因子分析模型，即基于矩阵变量t分布的双线性因子分析（tbfa）。其创新之处在于它能够同时对感兴趣的行和列变量从重尾或受污染的矩阵数据中提取公共因子。本文提出了两种最大似然估计的高效算法来求解模型参数。

    Factor Analysis based on multivariate $t$ distribution ($t$fa) is a useful robust tool for extracting common factors on heavy-tailed or contaminated data. However, $t$fa is only applicable to vector data. When $t$fa is applied to matrix data, it is common to first vectorize the matrix observations. This introduces two challenges for $t$fa: (i) the inherent matrix structure of the data is broken, and (ii) robustness may be lost, as vectorized matrix data typically results in a high data dimension, which could easily lead to the breakdown of $t$fa. To address these issues, starting from the intrinsic matrix structure of matrix data, a novel robust factor analysis model, namely bilinear factor analysis built on the matrix-variate $t$ distribution ($t$bfa), is proposed in this paper. The novelty is that it is capable to simultaneously extract common factors for both row and column variables of interest on heavy-tailed or contaminated matrix data. Two efficient algorithms for maximum likeli
    
[^29]: LADRI: 基于学习的自动驾驶系统中的动态风险指标

    LADRI: LeArning-based Dynamic Risk Indicator in Automated Driving System. (arXiv:2401.02199v1 [eess.SY])

    [http://arxiv.org/abs/2401.02199](http://arxiv.org/abs/2401.02199)

    本研究提出了一种基于学习的动态风险评估框架，利用人工神经网络分析和分类实时的车载传感器数据，以提升自动驾驶系统的安全水平和情境意识。

    

    随着自动驾驶系统（ADS）的演进，智能交通的视野不断扩大，确保极其安全变得比以往任何时候都更为迫切。传统的风险评估方法主要用于人工驾驶的车辆，无法充分适应ADS多方面、不断演变的环境。本文引入了一种基于人工神经网络（ANN）的实时动态风险评估（DRA）框架。我们提出的解决方案突破了这些限制，利用深度学习的核心部分——ANN，通过分析和分类实时的车载传感器（OBS）数据来细致地分析和分类风险维度。这种以学习为中心的方法不仅提升了ADS的情境意识，还丰富了其对即时运行环境的理解。通过分析OBS数据，系统能够准确定位其当前的风险配置文件，从而提高了乘客和更广泛旅途中的安全前景。

    As the horizon of intelligent transportation expands with the evolution of Automated Driving Systems (ADS), ensuring paramount safety becomes more imperative than ever. Traditional risk assessment methodologies, primarily crafted for human-driven vehicles, grapple to adequately adapt to the multifaceted, evolving environments of ADS. This paper introduces a framework for real-time Dynamic Risk Assessment (DRA) in ADS, harnessing the potency of Artificial Neural Networks (ANNs).  Our proposed solution transcends these limitations, drawing upon ANNs, a cornerstone of deep learning, to meticulously analyze and categorize risk dimensions using real-time On-board Sensor (OBS) data. This learning-centric approach not only elevates the ADS's situational awareness but also enriches its understanding of immediate operational contexts. By dissecting OBS data, the system is empowered to pinpoint its current risk profile, thereby enhancing safety prospects for onboard passengers and the broader tr
    
[^30]: 胸部X射线上结节的检测与生成: NODE21挑战

    Nodule detection and generation on chest X-rays: NODE21 Challenge. (arXiv:2401.02192v1 [eess.IV])

    [http://arxiv.org/abs/2401.02192](http://arxiv.org/abs/2401.02192)

    这篇文章总结了NODE21挑战的结果，以及进一步实验证明了合成生成的结节训练图像对检测算法性能的影响。

    

    肺结节可能是肺癌的早期表现，是男性和女性中导致癌症相关死亡的主要原因。许多研究已经证实，深度学习方法可以在胸部X射线中检测肺结节方面获得高性能水平。然而，缺乏黄金标准的公共数据集阻碍了研究的进展，并且阻止了对该任务方法的基准测试。为了解决这个问题，我们组织了一个公共研究挑战，NODE21，旨在检测与生成胸部X射线中的肺结节。检测部分评估了最先进的结节检测系统，生成部分确定了结节生成算法在增加训练数据并改善检测系统性能方面的效用。本文总结了NODE21挑战的结果，并进行了广泛的额外实验，以研究合成生成的结节训练图像对检测算法的影响。

    Pulmonary nodules may be an early manifestation of lung cancer, the leading cause of cancer-related deaths among both men and women. Numerous studies have established that deep learning methods can yield high-performance levels in the detection of lung nodules in chest X-rays. However, the lack of gold-standard public datasets slows down the progression of the research and prevents benchmarking of methods for this task. To address this, we organized a public research challenge, NODE21, aimed at the detection and generation of lung nodules in chest X-rays. While the detection track assesses state-of-the-art nodule detection systems, the generation track determines the utility of nodule generation algorithms to augment training data and hence improve the performance of the detection systems. This paper summarizes the results of the NODE21 challenge and performs extensive additional experiments to examine the impact of the synthetically generated nodule training images on the detection al
    
[^31]: 公平性增强模型比较的框架: FairGridSearch

    FairGridSearch: A Framework to Compare Fairness-Enhancing Models. (arXiv:2401.02183v1 [cs.LG])

    [http://arxiv.org/abs/2401.02183](http://arxiv.org/abs/2401.02183)

    本文提出了一种比较公平性增强模型的框架FairGridSearch，通过实验不同模型参数组合并推荐最佳组合。研究结果表明，选择适当的准确度和公平性度量对模型评估非常重要。

    

    机器学习模型在关键决策应用中的使用越来越多。然而，这些模型容易复制或甚至放大现实世界数据中的偏见。尽管文献中存在各种偏见缓解方法和基本估计器，但选择特定应用的最佳模型仍然具有挑战性。本文针对二元分类，提出了一种新颖的比较公平性增强模型的框架FairGridSearch。FairGridSearch通过实验不同模型参数组合，并推荐最佳组合。本研究将FairGridSearch应用于三个流行数据集(成年人、COMPAS和德国信用)，并分析度量选择、基本估计器选择和分类阈值对模型公平性的影响。结果突出了选择合适的准确度和公平性度量对模型评估的重要性。此外，不同的基本估计器和分类阈值也会对模型的公平性产生影响。

    Machine learning models are increasingly used in critical decision-making applications. However, these models are susceptible to replicating or even amplifying bias present in real-world data. While there are various bias mitigation methods and base estimators in the literature, selecting the optimal model for a specific application remains challenging.  This paper focuses on binary classification and proposes FairGridSearch, a novel framework for comparing fairness-enhancing models. FairGridSearch enables experimentation with different model parameter combinations and recommends the best one. The study applies FairGridSearch to three popular datasets (Adult, COMPAS, and German Credit) and analyzes the impacts of metric selection, base estimator choice, and classification threshold on model fairness.  The results highlight the significance of selecting appropriate accuracy and fairness metrics for model evaluation. Additionally, different base estimators and classification threshold va
    
[^32]: 跨平台数据中的因果效应估计方法

    Disentangle Estimation of Causal Effects from Cross-Silo Data. (arXiv:2401.02154v1 [cs.LG])

    [http://arxiv.org/abs/2401.02154](http://arxiv.org/abs/2401.02154)

    引入一种新颖的解耦架构来解决跨平台数据中因果效应估计的问题，并通过引入全局约束条件来提高准确性。

    

    在诸如药物研发等关键领域，估计不同事件之间的因果效应非常重要。然而，与事件相关的数据特征可能分布在不同的平台上，并且在各方之间保持私密，阻碍了它们之间的直接信息交流。这反过来可能导致局部因果效应的估计存在偏差，依赖于仅子集协变量的特征。为了解决这个挑战，我们引入了一种创新的解耦架构，旨在通过共享和私有分支的组合促进模型参数的无缝跨平台传递，丰富因果机制。此外，我们引入了全局约束条件来有效减轻各个缺失域内的偏差，从而提高了我们因果效应估计的准确性。在新的半合成数据集上进行的大量实验证明，我们的方法优于最先进的方法。

    Estimating causal effects among different events is of great importance to critical fields such as drug development. Nevertheless, the data features associated with events may be distributed across various silos and remain private within respective parties, impeding direct information exchange between them. This, in turn, can result in biased estimations of local causal effects, which rely on the characteristics of only a subset of the covariates. To tackle this challenge, we introduce an innovative disentangle architecture designed to facilitate the seamless cross-silo transmission of model parameters, enriched with causal mechanisms, through a combination of shared and private branches. Besides, we introduce global constraints into the equation to effectively mitigate bias within the various missing domains, thereby elevating the accuracy of our causal effect estimation. Extensive experiments conducted on new semi-synthetic datasets show that our method outperforms state-of-the-art b
    
[^33]: 图神经网络在表格数据学习中的应用：一项带有分类和方向的综述

    Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy and Directions. (arXiv:2401.02143v1 [cs.LG])

    [http://arxiv.org/abs/2401.02143](http://arxiv.org/abs/2401.02143)

    这项综述研究了使用图神经网络（GNN）进行表格数据学习（TDL）的领域。研究发现，深度学习方法在分类和回归任务方面表现出优越性能，但目前对数据实例和特征值之间潜在相关性的表达不足。GNN以其能力模拟复杂关系和相互作用，并在TDL领域得到了广泛应用。本综述对GNN4TDL方法进行了系统回顾，提供了对其演化领域的洞见，并提出了一个全面的分类。

    

    在这项综述中，我们深入研究了使用图神经网络（GNN）进行表格数据学习（TDL）的领域，与传统方法相比，基于深度学习的方法在分类和回归任务中显示出优越的性能。该综述突出了深度神经TDL方法中的一个关键差距：数据实例和特征值之间的潜在相关性的表述不足。GNN以其天然能力来模拟表格数据的复杂关系和相互作用，在各种TDL领域中引起了重要的兴趣和应用。我们的综述对设计和实现GNN用于TDL（GNN4TDL）的方法进行了系统回顾。它包括对基础问题的详细研究和基于GNN的TDL方法的概述，为其不断发展的领域提供了深入见解。我们提出了一个关注构建图结构和表示学习的全面分类。

    In this survey, we dive into Tabular Data Learning (TDL) using Graph Neural Networks (GNNs), a domain where deep learning-based approaches have increasingly shown superior performance in both classification and regression tasks compared to traditional methods. The survey highlights a critical gap in deep neural TDL methods: the underrepresentation of latent correlations among data instances and feature values. GNNs, with their innate capability to model intricate relationships and interactions between diverse elements of tabular data, have garnered significant interest and application across various TDL domains. Our survey provides a systematic review of the methods involved in designing and implementing GNNs for TDL (GNN4TDL). It encompasses a detailed investigation into the foundational aspects and an overview of GNN-based TDL methods, offering insights into their evolving landscape. We present a comprehensive taxonomy focused on constructing graph structures and representation learn
    
[^34]: PosCUDA: 用于无法学习的音频数据集的位置卷积

    PosCUDA: Position based Convolution for Unlearnable Audio Datasets. (arXiv:2401.02135v1 [cs.SD])

    [http://arxiv.org/abs/2401.02135](http://arxiv.org/abs/2401.02135)

    PosCUDA是一种用于创建无法学习的音频数据集的位置卷积方法，通过在小音频块上使用按类别的卷积和私钥控制块的位置，实现了对数据集的无法学习，并保持音频数据质量。

    

    深度学习模型需要大量干净的数据才能达到良好的性能。为了避免昂贵的数据获取成本，研究人员使用互联网上丰富的数据。这引发了关于在未经授权的情况下滥用个人数据用于模型训练的重大隐私问题。最近的研究如CUDA提出了解决这个问题的方法，即通过添加按类别模糊处理来使数据集无法学习，即模型永远无法使用已获取的数据集进行学习。然而，这些方法往往会降低数据的质量，使其对实际应用无用。我们引入了PosCUDA，一种用于创建无法学习的音频数据集的位置卷积。PosCUDA在小音频块上使用按类别的卷积。块的位置基于每个类别的私钥，因此模型可以学习位置模糊和标签之间的关系，同时无法泛化。我们通过实验证明了PosCUDA可以实现无法学习性能，同时维持音频数据的质量。

    Deep learning models require large amounts of clean data to acheive good performance. To avoid the cost of expensive data acquisition, researchers use the abundant data available on the internet. This raises significant privacy concerns on the potential misuse of personal data for model training without authorisation. Recent works such as CUDA propose solutions to this problem by adding class-wise blurs to make datasets unlearnable, i.e a model can never use the acquired dataset for learning. However these methods often reduce the quality of the data making it useless for practical applications. We introduce PosCUDA, a position based convolution for creating unlearnable audio datasets. PosCUDA uses class-wise convolutions on small patches of audio. The location of the patches are based on a private key for each class, hence the model learns the relations between positional blurs and labels, while failing to generalize. We empirically show that PosCUDA can achieve unlearnability while m
    
[^35]: ACP-ESM:一种使用以蛋白质为导向的Transformer方法进行抗癌肽分类的新框架

    ACP-ESM: A novel framework for classification of anticancer peptides using protein-oriented transformer approach. (arXiv:2401.02124v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.02124](http://arxiv.org/abs/2401.02124)

    ACP-ESM是一个使用以蛋白质为导向的Transformer方法的新框架，用于分类抗癌肽。这种方法可以优化抗癌肽的稳定性，改善选择性，并提高对癌细胞的传递性。

    

    抗癌肽(ACPs)是一类在癌症研究和治疗领域引起重视的分子。 ACPs是由氨基酸构成的短链，是蛋白质的构建基块，并具有选择性靶向和杀死癌细胞的能力。 ACPs之所以具有针对癌细胞的选择性，往往归因于癌细胞表面特性与正常细胞的差异。因此，ACP被认为是癌症治疗的潜在候选药物。ACPs可以单独使用或与化疗和放疗等治疗方式联合使用。虽然ACP作为一种新型癌症治疗方法具有潜力，但仍需克服一些挑战，包括优化其稳定性，改善选择性，提高对癌细胞的传递性，并且应对肽序列数量的不断增加。

    Anticancer peptides (ACPs) are a class of molecules that have gained significant attention in the field of cancer research and therapy. ACPs are short chains of amino acids, the building blocks of proteins, and they possess the ability to selectively target and kill cancer cells. One of the key advantages of ACPs is their ability to selectively target cancer cells while sparing healthy cells to a greater extent. This selectivity is often attributed to differences in the surface properties of cancer cells compared to normal cells. That is why ACPs are being investigated as potential candidates for cancer therapy. ACPs may be used alone or in combination with other treatment modalities like chemotherapy and radiation therapy. While ACPs hold promise as a novel approach to cancer treatment, there are challenges to overcome, including optimizing their stability, improving selectivity, and enhancing their delivery to cancer cells, continuous increasing in number of peptide sequences, develo
    
[^36]: 移动ALOHA：低成本全身远程操作学习双手移动操作

    Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation. (arXiv:2401.02117v1 [cs.RO])

    [http://arxiv.org/abs/2401.02117](http://arxiv.org/abs/2401.02117)

    该论文介绍了一个移动ALOHA系统，用于学习双手移动操作。通过低成本的远程操作和整体身体控制，系统能够完成复杂的移动操作任务，并通过联合训练提高成功率达到90%。

    

    仿真学习来自人类演示在机器人学中已经展现出令人印象深刻的性能。然而，大多数的结果集中在桌面操作上，缺乏对于通常有用任务所需要的移动性和灵活性。在这项工作中，我们开发了一个系统，用于模仿移动操作任务，该任务是双手操作且需要全身控制的。我们首先介绍了移动ALOHA，这是一个低成本和全身远程操作数据收集系统。它通过增加一个移动底盘和一个全身远程操作接口来增强ALOHA系统。然后，使用通过移动ALOHA收集的数据，我们进行监督式行为克隆，并发现与现有的静态ALOHA数据集进行联合训练可以提高移动操作任务的性能。对于每个任务进行50次演示，联合训练可以将成功率提高90%，使得移动ALOHA能够自主完成复杂的移动操作任务，如炒虾和上菜，并打开一个双门壁柜存放重型烹饪设备。

    Imitation learning from human demonstrations has shown impressive performance in robotics. However, most results focus on table-top manipulation, lacking the mobility and dexterity necessary for generally useful tasks. In this work, we develop a system for imitating mobile manipulation tasks that are bimanual and require whole-body control. We first present Mobile ALOHA, a low-cost and whole-body teleoperation system for data collection. It augments the ALOHA system with a mobile base, and a whole-body teleoperation interface. Using data collected with Mobile ALOHA, we then perform supervised behavior cloning and find that co-training with existing static ALOHA datasets boosts performance on mobile manipulation tasks. With 50 demonstrations for each task, co-training can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously complete complex mobile manipulation tasks such as sauteing and serving a piece of shrimp, opening a two-door wall cabinet to store heavy cooki
    
[^37]: 针对软组织成像的镉锌碲化物(CZT)光子计数探测器表征

    Cadmium Zinc Telluride (CZT) photon counting detector Characterisation for soft tissue imaging. (arXiv:2401.02106v1 [physics.ins-det])

    [http://arxiv.org/abs/2401.02106](http://arxiv.org/abs/2401.02106)

    该研究表征了镉锌碲化物（CZT）光子计数探测器在识别各种组织方面的性能，推动了使用光子计数探测技术来克服传统CT探测器的限制。

    

    近年来，光子计数检测技术的应用引起了重大的X射线成像研究兴趣。计算机断层扫描（CT）扫描仪可以从光子计数探测器中受益，这是一种新技术，有潜力克服传统CT探测器的关键限制。研究人员正在研究在光子计数探测器中应用半导体探测材料用于检测软组织对比度的有效性和灵敏度。本研究旨在表征镉锌碲化物光子计数探测器在识别各种组织方面的性能。通过将X射线管电压和电流分别设置为25 keV、35 keV和0.5 mA、1.0 mA，评估了CZT探测器的最佳帧速率（FPS），通过保持最佳FPS固定，将探测器能量阈值从15 keV到35 keV设置为小步，将X射线管的电流设置为0.1 mA到1.0 mA的范围，以找到电压和电流之间的关系。

    The use of photon counting detection technology has resulted in significant X-ray imaging research interest in recent years. Computed Tomography (CT) scanners can benefit from photon-counting detectors, which are new technology with the potential to overcome key limitations of conventional CT detectors. Researchers are still studying the effectiveness and sensitivity of semiconductor detector materials in photon counting detectors for detecting soft tissue contrasts. This study aimed to characterize the performance of the Cadmium Zinc Telluride photon counting detector in identifying various tissues. An optimal frame rate per second (FPS) of CZT detector was evaluated by setting the X-ray tube voltage and current at 25 keV, 35 keV and 0.5 mA, 1.0 mA respectively by keeping the optimum FPS fixed, the detector energy thresholds were set in small steps from 15 keV to 35 keV and the Currents were set for X-ray tubes in ranges of 0.1 mA to 1.0 mA to find the relationship between voltage and
    
[^38]: 重新评估内存平衡的流水线并行体：BPipe

    Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe. (arXiv:2401.02088v1 [cs.LG])

    [http://arxiv.org/abs/2401.02088](http://arxiv.org/abs/2401.02088)

    本论文重新评估了流水线并行体的内存平衡问题，并提出了一种名为BPipe的技术来解决该问题。验证实验结果表明，虽然BPipe在GPT-3模型上有效，但在LLaMA训练中并未获得相似的好处。我们还分析了BPipe在不同模型上性能差异的原因，并引入了一种新的方法来估计BPipe的性能。

    

    流水线并行体是训练大规模Transformer模型的一种重要技术。然而，它在内存消耗上存在不平衡的问题，导致内存利用不充分。BPipe技术被提出来解决这个问题，并在GPT-3模型上证明了有效性。然而，我们的实验在LLaMA训练中未获得类似的好处。此外，在应用flash attention时，BPipe在GPT-3训练中只带来微不足道的好处。我们分析了BPipe在GPT-3和LLaMA上性能差异的根本原因。此外，我们介绍了一种新的方法来估计BPipe的性能。

    Pipeline parallelism is an essential technique in the training of large-scale Transformer models. However, it suffers from imbalanced memory consumption, leading to insufficient memory utilization. The BPipe technique was proposed to address this issue and has proven effective in the GPT-3 model. Nevertheless, our experiments have not yielded similar benefits for LLaMA training. Additionally, BPipe only yields negligible benefits for GPT-3 training when applying flash attention. We analyze the underlying causes of the divergent performance of BPipe on GPT-3 and LLaMA. Furthermore, we introduce a novel method to estimate the performance of BPipe.
    
[^39]: 基于视图的图神经网络解释

    View-based Explanations for Graph Neural Networks. (arXiv:2401.02086v1 [cs.LG])

    [http://arxiv.org/abs/2401.02086](http://arxiv.org/abs/2401.02086)

    这篇论文提出了一种基于视图的解释方法来解释图神经网络(GNNs)的行为，通过生成解释视图和图模式来解释特定类别的结果。

    

    研究生成图神经网络(GNNs)的解释，以了解它们在图分类等分析任务中的行为。现有的方法旨在理解GNNs的整体结果，而不是针对特定类别的解释，并且可能返回难以访问或直接查询的解释结构。我们提出了一种新颖的范式GVEX，用于生成图解释的图视图。我们设计了一种两层的解释结构，称为解释视图。解释视图包括一组图模式和一组诱发的解释子图。给定一个包含多个图和由基于GNN的分类器M分配的特定类别标签l的数据库G，它简洁地描述了最好解释为什么l由M分配的G的分数。我们提出了质量度量方法，并制定了一个优化问题来计算GNN解释的最佳解释视图。我们证明了该问题是Σ^2_P难的。

    Generating explanations for graph neural networks (GNNs) has been studied to understand their behavior in analytical tasks such as graph classification. Existing approaches aim to understand the overall results of GNNs rather than providing explanations for specific class labels of interest, and may return explanation structures that are hard to access, nor directly queryable.  We propose GVEX, a novel paradigm that generates Graph Views for EXplanation. (1) We design a two-tier explanation structure called explanation views. An explanation view consists of a set of graph patterns and a set of induced explanation subgraphs. Given a database G of multiple graphs and a specific class label l assigned by a GNN-based classifier M, it concisely describes the fraction of G that best explains why l is assigned by M. (2) We propose quality measures and formulate an optimization problem to compute optimal explanation views for GNN explanation. We show that the problem is $\Sigma^2_P$-hard. (3) 
    
[^40]: 基于能量的扩散生成器用于高效采样Boltzmann分布

    Energy based diffusion generator for efficient sampling of Boltzmann distributions. (arXiv:2401.02080v1 [cs.LG])

    [http://arxiv.org/abs/2401.02080](http://arxiv.org/abs/2401.02080)

    介绍了一种称为基于能量的扩散生成器的新型采样器，用于从任意目标分布中生成样本，并通过扩散模型和广义哈密顿动力学提高采样性能。在各种复杂分布函数上的实证评估中表现出优越性。

    

    我们介绍了一种称为基于能量的扩散生成器的新型采样器，用于从任意目标分布中生成样本。采样模型采用类似变分自编码器的结构，利用解码器将来自简单分布的潜在变量转换为逼近目标分布的随机变量，并设计了基于扩散模型的编码器。利用扩散模型对复杂分布的强大建模能力，我们可以获得生成样本和目标分布之间的Kullback-Leibler散度的准确变分估计。此外，我们提出了基于广义哈密顿动力学的解码器，进一步提高采样性能。通过实证评估，我们展示了我们的方法在各种复杂分布函数上的有效性，展示了其相对于现有方法的优越性。

    We introduce a novel sampler called the energy based diffusion generator for generating samples from arbitrary target distributions. The sampling model employs a structure similar to a variational autoencoder, utilizing a decoder to transform latent variables from a simple distribution into random variables approximating the target distribution, and we design an encoder based on the diffusion model. Leveraging the powerful modeling capacity of the diffusion model for complex distributions, we can obtain an accurate variational estimate of the Kullback-Leibler divergence between the distributions of the generated samples and the target. Moreover, we propose a decoder based on generalized Hamiltonian dynamics to further enhance sampling performance. Through empirical evaluation, we demonstrate the effectiveness of our method across various complex distribution functions, showcasing its superiority compared to existing methods.
    
[^41]: U-信任模型 - 决策中的可靠性、能力和信心

    U-Trustworthy Models.Reliability, Competence, and Confidence in Decision-Making. (arXiv:2401.02062v1 [stat.ML])

    [http://arxiv.org/abs/2401.02062](http://arxiv.org/abs/2401.02062)

    提出了一种基于哲学文献的新的信任框架$\mathcal{U}$-信任度，用于评估AI系统的信任性，该框架挑战了传统的概率框架，并提出了解决方案以克服偏见和误导性信任评估的风险。

    

    随着对预测模型中偏见和歧视的担忧日益增长，人工智能社区越来越关注评估AI系统的信任性。传统上，信任的AI文献依赖于概率框架和校准作为信任的先决条件。在这项工作中，我们离开了这个观点，提出了一个受哲学文献中关于信任的启示的新的信任框架。我们提出了一个精确的数学定义，称为$\mathcal{U}$-信任度，专门为最大化效用函数的任务子集量身定制。我们认为一个模型的$\mathcal{U}$-信任度取决于它在这个任务子集中最大化贝叶斯效用的能力。我们的第一组结果挑战了概率框架，通过展示它可能偏向不太可信的模型，并引入误导性信任评估的风险。在$\mathcal{U}$-信任度的背景下，我们提供了一些解决方案，以克服这些挑战，并提高模型的信任度估计。

    With growing concerns regarding bias and discrimination in predictive models, the AI community has increasingly focused on assessing AI system trustworthiness. Conventionally, trustworthy AI literature relies on the probabilistic framework and calibration as prerequisites for trustworthiness. In this work, we depart from this viewpoint by proposing a novel trust framework inspired by the philosophy literature on trust. We present a precise mathematical definition of trustworthiness, termed $\mathcal{U}$-trustworthiness, specifically tailored for a subset of tasks aimed at maximizing a utility function. We argue that a model's $\mathcal{U}$-trustworthiness is contingent upon its ability to maximize Bayes utility within this task subset. Our first set of results challenges the probabilistic framework by demonstrating its potential to favor less trustworthy models and introduce the risk of misleading trustworthiness assessments. Within the context of $\mathcal{U}$-trustworthiness, we prov
    
[^42]: 使用无约束ReLU特征模型进行交叉熵类别不平衡学习的神经折叠

    Neural Collapse for Cross-entropy Class-Imbalanced Learning with Unconstrained ReLU Feature Model. (arXiv:2401.02058v1 [cs.LG])

    [http://arxiv.org/abs/2401.02058](http://arxiv.org/abs/2401.02058)

    本文研究了在交叉熵类别不平衡学习中的神经折叠现象，证明了在存在类别不平衡情况下，神经折叠仍然存在，但类均值的几何特性会发生偏移。

    

    目前训练深度神经网络进行分类任务的范式包括最小化经验风险，将训练损失值推向零，即使训练误差已经消失。在训练的最后阶段，观察到最后一层特征会折叠到它们的类均值，并且这些类均值会收敛到一个简单典型等角紧框（ETF）的顶点。这一现象被称为神经折叠（NC）。为了从理论上理解这一现象，最近的研究使用了一个简化的无约束特征模型来证明NC出现在训练问题的全局解中。然而，当训练数据集存在类别不平衡时，一些NC特性将不再成立。例如，当损失收敛时，类均值几何会偏离简单典型等角紧框。在本文中，我们将NC推广到不平衡情况下的交叉熵损失和无约束ReLU特征模型。我们证明，在训练问题的全局解中，当存在类别不平衡时，NC仍然存在，但对于类均值的几何特性会发生偏移。

    The current paradigm of training deep neural networks for classification tasks includes minimizing the empirical risk that pushes the training loss value towards zero, even after the training error has been vanished. In this terminal phase of training, it has been observed that the last-layer features collapse to their class-means and these class-means converge to the vertices of a simplex Equiangular Tight Frame (ETF). This phenomenon is termed as Neural Collapse (NC). To theoretically understand this phenomenon, recent works employ a simplified unconstrained feature model to prove that NC emerges at the global solutions of the training problem. However, when the training dataset is class-imbalanced, some NC properties will no longer be true. For example, the class-means geometry will skew away from the simplex ETF when the loss converges. In this paper, we generalize NC to imbalanced regime for cross-entropy loss under the unconstrained ReLU feature model. We prove that, while the wi
    
[^43]: Spikformer V2：通过SNN Ticket在ImageNet上实现高准确度

    Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN Ticket. (arXiv:2401.02020v1 [cs.NE])

    [http://arxiv.org/abs/2401.02020](http://arxiv.org/abs/2401.02020)

    Spikformer V2是一种基于SNNs和自注意机制的脉冲神经网络，通过提出脉冲自注意机制和脉冲Transformer来实现高准确度的图像识别。

    

    脉冲神经网络（SNNs）因其生物学合理的结构而闻名，但其性能受到限制。基于生物启发结构的高性能Transformer中的自注意机制在现有的SNNs中缺失。为此，我们探索了利用自注意能力和SNNs的生物特性的潜力，并提出了一种新颖的脉冲自注意（SSA）和脉冲Transformer（Spikformer）。SSA机制消除了对softmax的需求，并利用基于脉冲的查询、键和值捕获稀疏的视觉特征。这种无乘法的稀疏计算使得SSA高效且节能。此外，我们还开发了一种脉冲卷积干细胞（SCS）和补充卷积层来增强Spikformer的架构。加上SCS的Spikformer被称为Spikformer V2。为了训练更大更深的Spikformer V2，我们引入了一种开创性的探+

    Spiking Neural Networks (SNNs), known for their biologically plausible architecture, face the challenge of limited performance. The self-attention mechanism, which is the cornerstone of the high-performance Transformer and also a biologically inspired structure, is absent in existing SNNs. To this end, we explore the potential of leveraging both self-attention capability and biological properties of SNNs, and propose a novel Spiking Self-Attention (SSA) and Spiking Transformer (Spikformer). The SSA mechanism eliminates the need for softmax and captures the sparse visual feature employing spike-based Query, Key, and Value. This sparse computation without multiplication makes SSA efficient and energy-saving. Further, we develop a Spiking Convolutional Stem (SCS) with supplementary convolutional layers to enhance the architecture of Spikformer. The Spikformer enhanced with the SCS is referred to as Spikformer V2. To train larger and deeper Spikformer V2, we introduce a pioneering explorat
    
[^44]: 从函数到分布建模：一种PAC-Generative方法应用于离线优化

    From Function to Distribution Modeling: A PAC-Generative Approach to Offline Optimization. (arXiv:2401.02019v1 [cs.LG])

    [http://arxiv.org/abs/2401.02019](http://arxiv.org/abs/2401.02019)

    本文提出了一种PAC-Generative方法应用于离线优化问题，在这种方法中，将优化问题视为从生成模型中抽样的过程，而不是学习和优化未知的目标函数。通过学习离线数据示例，得到了一个有效的生成模型。

    

    本文考虑了离线优化问题，其中目标函数除了一组“离线”数据示例外是未知的。近年来，各种机器学习技术被广泛应用于离线优化问题，但其中大部分工作集中在学习未知目标函数的替代模型，然后应用现有的优化算法。虽然建模未知目标函数的思想直观且吸引人，但从学习的角度来看，根据优化目标调整学习者的目标也变得非常困难。本文不是学习然后优化未知目标函数，而是从一个更直接但不太直观的角度看待优化，即将优化视为从生成模型中抽样的过程。为了从离线数据示例中学习一个有效的生成模型，我们考虑了“重新加权”的标准技术。

    This paper considers the problem of offline optimization, where the objective function is unknown except for a collection of ``offline" data examples. While recent years have seen a flurry of work on applying various machine learning techniques to the offline optimization problem, the majority of these work focused on learning a surrogate of the unknown objective function and then applying existing optimization algorithms. While the idea of modeling the unknown objective function is intuitive and appealing, from the learning point of view it also makes it very difficult to tune the objective of the learner according to the objective of optimization. Instead of learning and then optimizing the unknown objective function, in this paper we take on a less intuitive but more direct view that optimization can be thought of as a process of sampling from a generative model. To learn an effective generative model from the offline data examples, we consider the standard technique of ``re-weighti
    
[^45]: SwitchTab: 切换式自编码器在表格学习中的有效性

    SwitchTab: Switched Autoencoders Are Effective Tabular Learners. (arXiv:2401.02013v1 [cs.LG])

    [http://arxiv.org/abs/2401.02013](http://arxiv.org/abs/2401.02013)

    SwitchTab是一种专门设计用于捕捉表格数据中潜在依赖关系的自监督方法。它引入了一个非对称的编码器-解码器框架来解耦数据对中的互相关联和显著特征，从而生成更具代表性的嵌入。在大量实验中，SwitchTab在细调下端到端预测任务中展现出了优越的性能。

    

    自监督表示学习方法在计算机视觉和自然语言处理中取得了显著的成功，其中数据样本呈现明确的空间或语义依赖关系。然而，将这些方法应用于表格数据是具有挑战性的，因为数据样本之间的依赖关系不太明显。本文通过引入SwitchTab，一种专门设计用于捕捉表格数据中的潜在依赖关系的新型自监督方法，来解决这一限制。SwitchTab利用一个非对称的编码器-解码器框架来解耦数据对中的互相关联和显著特征，从而产生更具代表性的嵌入。这些嵌入进而有助于更好的决策边界，并在下游任务中取得更好的结果。为了验证SwitchTab的有效性，我们在涉及表格数据的各个领域进行了大量实验。结果展示了在细调下端到端预测任务中表现出优越的性能。

    Self-supervised representation learning methods have achieved significant success in computer vision and natural language processing, where data samples exhibit explicit spatial or semantic dependencies. However, applying these methods to tabular data is challenging due to the less pronounced dependencies among data samples. In this paper, we address this limitation by introducing SwitchTab, a novel self-supervised method specifically designed to capture latent dependencies in tabular data. SwitchTab leverages an asymmetric encoder-decoder framework to decouple mutual and salient features among data pairs, resulting in more representative embeddings. These embeddings, in turn, contribute to better decision boundaries and lead to improved results in downstream tasks. To validate the effectiveness of SwitchTab, we conduct extensive experiments across various domains involving tabular data. The results showcase superior performance in end-to-end prediction tasks with fine-tuning. Moreover
    
[^46]: 快速而公平: 高效的二阶鲁棒优化用于机器学习中的公平性

    Fast & Fair: Efficient Second-Order Robust Optimization for Fairness in Machine Learning. (arXiv:2401.02012v1 [cs.LG])

    [http://arxiv.org/abs/2401.02012](http://arxiv.org/abs/2401.02012)

    该项目通过对抗训练技术开发了一种高效的二阶鲁棒优化方法，以提高机器学习中的公平性。该方法能够减小深度神经网络因与敏感属性相关的偏见而导致的不公平问题，并且比纯一阶方法更高效。

    

    该项目探讨了对抗训练技术，以开发更公平的深度神经网络（DNN），以减轻其已知存在的固有偏差。DNN对于与种族和性别等敏感属性相关的偏见很容易受到影响，这可能导致改变生活的结果（例如，用于逮捕嫌疑人的人脸识别软件中的人口统计偏见）。我们提出了一个鲁棒优化问题，并证明可以通过使用仿射线性模型，在多个合成和真实数据集中改善公平性。利用二阶信息，我们能够比纯一阶方法更高效地找到解决方案。

    This project explores adversarial training techniques to develop fairer Deep Neural Networks (DNNs) to mitigate the inherent bias they are known to exhibit. DNNs are susceptible to inheriting bias with respect to sensitive attributes such as race and gender, which can lead to life-altering outcomes (e.g., demographic bias in facial recognition software used to arrest a suspect). We propose a robust optimization problem, which we demonstrate can improve fairness in several datasets, both synthetic and real-world, using an affine linear model. Leveraging second order information, we are able to find a solution to our optimization problem more efficiently than a purely first order method.
    
[^47]: 分散多任务在线凸优化在随机链路失效下的应用

    Decentralized Multi-Task Online Convex Optimization Under Random Link Failures. (arXiv:2401.02011v1 [cs.LG])

    [http://arxiv.org/abs/2401.02011](http://arxiv.org/abs/2401.02011)

    本文研究了分散多任务在线凸优化中的随机链路故障问题，提出了替代缺失决策的健壮分散鞍点算法，并证明了算法达到了O(sqrt(T))的regret和O(T^fra)的复杂度。

    

    分散的优化方法通常需要邻居之间的信息交换。由于网络拥塞、硬件/软件问题、通信中断和其他因素，传输故障可能发生。在本文中，我们研究了在分散多任务在线凸优化中的随机链路故障问题，其中代理通过成对约束相互耦合。虽然在约束优化中广泛使用，但传统的鞍点算法在这里并不直接适用，因为存在随机丢包。为了解决这个问题，我们开发了一种健壮的分散鞍点算法，以对抗具有异质概率的随机链路失效，通过用邻居的最新接收到的值替代缺失决策。然后，通过谨慎地限制由这种替代产生的累积偏差，我们首先证明了我们的算法达到了O(sqrt(T))的regret和O(T^fra)的复杂度。

    Decentralized optimization methods often entail information exchange between neighbors. Transmission failures can happen due to network congestion, hardware/software issues, communication outage, and other factors. In this paper, we investigate the random link failure problem in decentralized multi-task online convex optimization, where agents have individual decisions that are coupled with each other via pairwise constraints. Although widely used in constrained optimization, conventional saddle-point algorithms are not directly applicable here because of random packet dropping. To address this issue, we develop a robust decentralized saddle-point algorithm against random link failures with heterogeneous probabilities by replacing the missing decisions of neighbors with their latest received values. Then, by judiciously bounding the accumulated deviation stemming from this replacement, we first establish that our algorithm achieves $\mathcal{O}(\sqrt{T})$ regret and $\mathcal{O}(T^\fra
    
[^48]: 两阶段代理建模用于数据驱动设计优化，并应用于复合材料微结构生成

    Two-Stage Surrogate Modeling for Data-Driven Design Optimization with Application to Composite Microstructure Generation. (arXiv:2401.02008v1 [cs.LG])

    [http://arxiv.org/abs/2401.02008](http://arxiv.org/abs/2401.02008)

    本文介绍了一种两阶段的机器学习代理建模框架，通过集成符合推理来解决科学工程领域中的反问题，提供了一种可应用广泛且高效的方法。

    

    本文介绍了一种新型的两阶段基于机器学习的代理建模框架，用于解决科学和工程领域中的反问题。在提出的框架的第一阶段，一种称为“学习者”的机器学习模型识别出一组在输入设计空间中，其预测输出与期望结果密切一致的候选项。随后，在第二阶段中，采用一个独立的代理模型作为“评估者”，来评估在第一阶段生成的缩减候选空间。这个评估过程通过用户定义的覆盖水平来消除不准确和不确定的解决方案。该框架的独特贡献在于集成了符合推理，提供了一种多功能和高效的方法，可以广泛应用。为了证明所提出的框架相对于传统的单阶段反问题的有效性，我们进行了几个基准测试，并研究了一个工程应用。

    This paper introduces a novel two-stage machine learning-based surrogate modeling framework to address inverse problems in scientific and engineering fields. In the first stage of the proposed framework, a machine learning model termed the "learner" identifies a limited set of candidates within the input design space whose predicted outputs closely align with desired outcomes. Subsequently, in the second stage, a separate surrogate model, functioning as an "evaluator," is employed to assess the reduced candidate space generated in the first stage. This evaluation process eliminates inaccurate and uncertain solutions, guided by a user-defined coverage level. The framework's distinctive contribution is the integration of conformal inference, providing a versatile and efficient approach that can be widely applicable. To demonstrate the effectiveness of the proposed framework compared to conventional single-stage inverse problems, we conduct several benchmark tests and investigate an engin
    
[^49]: 借助概率计算机的均场辅助深度玻尔兹曼学习

    Mean-Field Assisted Deep Boltzmann Learning with Probabilistic Computers. (arXiv:2401.01996v1 [cs.ET])

    [http://arxiv.org/abs/2401.01996](http://arxiv.org/abs/2401.01996)

    本文展示了借助概率计算机的均场辅助深度玻尔兹曼学习，通过使用特定硬件和稀疏网络，提出了两类均场理论辅助学习算法，以训练深度且无限制的玻尔兹曼机，具有重要的可训练性贡献。

    

    虽然作为启发于物理学的、基于能量的和生成性质的模型，普通玻尔兹曼机被认为难以训练。这种观点导致了对玻尔兹曼机的简化，如限制层内连接或逐层训练的深度玻尔兹曼机。而最近发展的面向特定领域的硬件（具体地说是具有概率位的概率计算机）可能改变对深度玻尔兹曼机可训练性的认知。本文展示了如何利用生成十亿亿马尔可夫链蒙特卡洛采样率的概率计算机在最初用于D-Wave的原有稀疏网络上训练深度且无限制的玻尔兹曼机。为了最大化概率计算机的学习效率，我们引入了两类均场理论辅助学习算法，即xMFTs（x = Naive和Hierarchical）。xMFTs用于估计对比散度正相位中的平均值和相关性。

    Despite their appeal as physics-inspired, energy-based and generative nature, general Boltzmann Machines (BM) are considered intractable to train. This belief led to simplified models of BMs with restricted intralayer connections or layer-by-layer training of deep BMs. Recent developments in domain-specific hardware -- specifically probabilistic computers (p-computer) with probabilistic bits (p-bit) -- may change established wisdom on the tractability of deep BMs. In this paper, we show that deep and unrestricted BMs can be trained using p-computers generating hundreds of billions of Markov Chain Monte Carlo (MCMC) samples per second, on sparse networks developed originally for use in D-Wave's annealers. To maximize the efficiency of learning the p-computer, we introduce two families of Mean-Field Theory assisted learning algorithms, or xMFTs (x = Naive and Hierarchical). The xMFTs are used to estimate the averages and correlations during the positive phase of the contrastive divergenc
    
[^50]: GPS-SSL: 引导正样本采样将先验知识注入到自监督学习中

    GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised Learning. (arXiv:2401.01990v1 [cs.CV])

    [http://arxiv.org/abs/2401.01990](http://arxiv.org/abs/2401.01990)

    GPS-SSL是一种将先验知识注入到自监督学习中的通用方法，通过设计度量空间并利用最近邻采样生成正样本。它可以减少对强数据增强的依赖，因此在Cifar10上达到了更好的效果。

    

    我们提出了引导正样本采样自监督学习（GPS-SSL），这是一种将先验知识注入到自监督学习（SSL）正样本选择的通用方法。当前的SSL方法利用数据增强（DA）生成正样本，并将先验知识结合进去，但是错误或者过弱的DA会严重降低所学到的表示的质量。GPS-SSL则提出设计一个度量空间，使得欧氏距离成为语义关系的有意义的替代。在这个空间中，可以通过最近邻采样生成正样本。任何先验知识都可以独立地嵌入到这个度量空间中，而不受所使用的DA影响。由于其简单性，GPS-SSL适用于任何SSL方法，如SimCLR或BYOL。GPS-SSL的一个关键好处是减少了定制强DA的压力。例如，GPS-SSL在Cifar10上使用弱DA达到了85.58％，而基准值只达到了37.51％。

    We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a general method to inject a priori knowledge into Self-Supervised Learning (SSL) positive samples selection. Current SSL methods leverage Data-Augmentations (DA) for generating positive samples and incorporate prior knowledge - an incorrect, or too weak DA will drastically reduce the quality of the learned representation. GPS-SSL proposes instead to design a metric space where Euclidean distances become a meaningful proxy for semantic relationship. In that space, it is now possible to generate positive samples from nearest neighbor sampling. Any prior knowledge can now be embedded into that metric space independently from the employed DA. From its simplicity, GPS-SSL is applicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is in reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches 85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We therefore move a 
    
[^51]: 使用注意力和对抗训练对多变量时间序列进行表示学习

    Representation Learning of Multivariate Time Series using Attention and Adversarial Training. (arXiv:2401.01987v1 [cs.LG])

    [http://arxiv.org/abs/2401.01987](http://arxiv.org/abs/2401.01987)

    本文提出了一种使用注意力和对抗训练的方法，用于表示学习多变量时间序列数据。实验结果表明，生成的信号与示例数据集的相似性较高。

    

    可信的机器学习的一个关键因素是开发出对训练数据具有鲁棒性的表示方法。只有在此保证下，方法才能合法地人工生成数据，例如，对抗不平衡的数据集或为黑盒决策系统提供反事实解释。近年来，生成对抗网络（GANs）在形成稳定的表示和生成逼真数据方面取得了相当大的成果。虽然许多应用集中于生成图像数据，但在生成时间序列数据，特别是多变量信号方面，付出的努力较少。本文提出了一种基于Transformer的自编码器，通过对抗训练方案进行正则化，以生成人工多变量时间序列信号。通过t-SNE可视化、动态时间规整（DTW）和熵得分对表示进行评估。我们的结果表明，生成的信号与示例数据集的相似性较高，而不是使用常规的自编码器。

    A critical factor in trustworthy machine learning is to develop robust representations of the training data. Only under this guarantee methods are legitimate to artificially generate data, for example, to counteract imbalanced datasets or provide counterfactual explanations for blackbox decision-making systems. In recent years, Generative Adversarial Networks (GANs) have shown considerable results in forming stable representations and generating realistic data. While many applications focus on generating image data, less effort has been made in generating time series data, especially multivariate signals. In this work, a Transformer-based autoencoder is proposed that is regularized using an adversarial training scheme to generate artificial multivariate time series signals. The representation is evaluated using t-SNE visualizations, Dynamic Time Warping (DTW) and Entropy scores. Our results indicate that the generated signals exhibit higher similarity to an exemplary dataset than using
    
[^52]: 超越遗憾：贝叶斯优化的几何度量

    Beyond Regrets: Geometric Metrics for Bayesian Optimization. (arXiv:2401.01981v1 [cs.LG])

    [http://arxiv.org/abs/2401.01981](http://arxiv.org/abs/2401.01981)

    本论文提出了四个新的几何度量，可以比较贝叶斯优化算法在考虑查询点和全局最优解的几何特性时的性能。

    

    贝叶斯优化是一种针对黑盒子目标函数的原则性优化策略。它在科学发现和实验设计等各种实际应用中的效果得到了证明。通常，贝叶斯优化的性能是通过基于遗憾的度量来评估的，如瞬时遗憾、简单遗憾和累积遗憾。这些度量仅依赖于函数评估，因此它们不考虑查询点和全局解之间的几何关系，也不考虑查询点本身。值得注意的是，它们不能区分是否成功找到了多个全局解。此外，它们也不能评估贝叶斯优化在给定搜索空间中利用和探索的能力。为了解决这些问题，我们提出了四个新的几何度量，即精确度、召回率、平均度和平均距离。这些度量使我们能够比较考虑查询点和全局最优解的几何特性的贝叶斯优化算法。

    Bayesian optimization is a principled optimization strategy for a black-box objective function. It shows its effectiveness in a wide variety of real-world applications such as scientific discovery and experimental design. In general, the performance of Bayesian optimization is assessed by regret-based metrics such as instantaneous, simple, and cumulative regrets. These metrics only rely on function evaluations, so that they do not consider geometric relationships between query points and global solutions, or query points themselves. Notably, they cannot discriminate if multiple global solutions are successfully found. Moreover, they do not evaluate Bayesian optimization's abilities to exploit and explore a search space given. To tackle these issues, we propose four new geometric metrics, i.e., precision, recall, average degree, and average distance. These metrics allow us to compare Bayesian optimization algorithms considering the geometry of both query points and global optima, or que
    
[^53]: Tailor: 高端时尚市场的尺寸建议

    Tailor: Size Recommendations for High-End Fashion Marketplaces. (arXiv:2401.01978v1 [cs.IR])

    [http://arxiv.org/abs/2401.01978](http://arxiv.org/abs/2401.01978)

    Tailor是一个针对高端时尚市场的尺寸建议的新方法，通过整合隐式和显式用户信号，采用序列分类方法来提供个性化的尺寸建议。该方法比其他方法提高了准确性，并通过使用加购物车的交互增加了用户覆盖范围。

    

    在不断变化和动态的高端时尚市场中，提供准确和个性化的尺寸建议已成为一个关键方面。满足顾客在这方面的期望不仅对确保他们的满意度至关重要，也在促进顾客保持，这是任何时尚零售商成功的关键指标。我们提出了一种新的序列分类方法来解决这个问题，将隐式（加购物车）和显式（退货原因）用户信号进行整合。我们的方法包括两个不同的模型：一个采用LSTM对用户信号进行编码，另一个利用注意机制。我们的最佳模型的准确性比SFNet提高了45.7%。通过使用加购物车的交互，与仅使用订单相比，我们将用户覆盖范围增加了24.5%。此外，我们通过进行实验以测量模型的延迟性能，评估模型在实时推荐场景中的可用性。

    In the ever-changing and dynamic realm of high-end fashion marketplaces, providing accurate and personalized size recommendations has become a critical aspect. Meeting customer expectations in this regard is not only crucial for ensuring their satisfaction but also plays a pivotal role in driving customer retention, which is a key metric for the success of any fashion retailer. We propose a novel sequence classification approach to address this problem, integrating implicit (Add2Bag) and explicit (ReturnReason) user signals. Our approach comprises two distinct models: one employs LSTMs to encode the user signals, while the other leverages an Attention mechanism. Our best model outperforms SFNet, improving accuracy by 45.7%. By using Add2Bag interactions we increase the user coverage by 24.5% when compared with only using Orders. Moreover, we evaluate the models' usability in real-time recommendation scenarios by conducting experiments to measure their latency performance.
    
[^54]: 实现真正的零样本组合视觉推理：以LLMs为程序员

    Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers. (arXiv:2401.01974v1 [cs.CV])

    [http://arxiv.org/abs/2401.01974](http://arxiv.org/abs/2401.01974)

    本文提出了一种框架来实现零样本组合视觉推理，通过引入抽象的空间和时间例程以及利用少量的nun

    

    视觉推理主要采用端到端神经网络，拥有数十亿个模型参数和训练样本。然而，即使是最大的模型在组合推理、泛化、细粒度空间和时间推理以及计数方面也存在困难。在理论上，使用大型语言模型(LLMs)作为控制器进行视觉推理可以解决这些限制，通过将任务分解为子任务，并通过调度一组(视觉)工具来解决子任务。最近，这些模型在组合视觉问答、视觉 grounding 和视频的时间推理等任务上取得了很好的性能。然而，现有模型在当前形式下严重依赖于在提示中针对具体数据集和任务进行人工设计的上下文示例，这需要高技能程序员投入大量的劳动力。在这项工作中，我们提出了一个框架来缓解这些问题，通过引入具有空间和时间抽象的例程，并利用少量的nu

    Visual reasoning is dominated by end-to-end neural networks scaled to billions of model parameters and training examples. However, even the largest models struggle with compositional reasoning, generalization, fine-grained spatial and temporal reasoning, and counting. Visual reasoning with large language models (LLMs) as controllers can, in principle, address these limitations by decomposing the task and solving subtasks by orchestrating a set of (visual) tools. Recently, these models achieved great performance on tasks such as compositional visual question answering, visual grounding, and video temporal reasoning. Nevertheless, in their current form, these models heavily rely on human engineering of in-context examples in the prompt, which are often dataset- and task-specific and require significant labor by highly skilled programmers. In this work, we present a framework that mitigates these issues by introducing spatially and temporally abstract routines and by leveraging a small nu
    
[^55]: 使用卷积能否仅生成逼真的手部图像？

    Can We Generate Realistic Hands Only Using Convolution?. (arXiv:2401.01951v1 [cs.CV])

    [http://arxiv.org/abs/2401.01951](http://arxiv.org/abs/2401.01951)

    本文展示了通过为卷积层提供具有相对$n$维笛卡尔坐标系的单一输入通道，可以缓解图像生成模型无法重现复杂几何特征的问题，显著提高了GAN和VAE生成的手部和面部图像质量。

    

    长达十年之久，图像生成模型一直无法重现复杂的几何特征，例如人手和手指中所存在的特征，这一问题在图像生成领域一直存在。虽然通过增加模型大小和多样化训练数据集已经取得了一定进展，但这个问题在各种模型中仍然普遍存在，从去噪扩散模型到生成对抗网络（GAN），这指向了底层结构的根本缺陷。在本文中，我们通过为卷积层提供一个单一输入通道，其中包含相对$n$维笛卡尔坐标系，来展示如何缓解这个问题。我们展示了这种方法极大地改善了GAN和变分自动编码器（VAE）生成的手部和面部图像的质量。

    The enduring inability of image generative models to recreate intricate geometric features, such as those present in human hands and fingers has been an ongoing problem in image generation for nearly a decade. While strides have been made by increasing model sizes and diversifying training datasets, this issue remains prevalent across all models, from denoising diffusion models to Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in the underlying architectures. In this paper, we demonstrate how this problem can be mitigated by augmenting convolution layers geometric capabilities through providing them with a single input channel incorporating the relative $n$-dimensional Cartesian coordinate system. We show that this drastically improves quality of hand and face images generated by GANs and Variational AutoEncoders (VAE).
    
[^56]: 在生成式人工智能时代的物联网: 视野与挑战

    IoT in the Era of Generative AI: Vision and Challenges. (arXiv:2401.01923v1 [cs.DC])

    [http://arxiv.org/abs/2401.01923](http://arxiv.org/abs/2401.01923)

    在生成式人工智能时代的物联网，Generative AI的进展带来了巨大的希望，同时也面临着高资源需求、及时工程、设备端推理、安全等关键挑战。

    

    带有感知、网络和计算能力的物联网设备，如智能手机、可穿戴设备、智能音箱和家庭机器人，已经无缝地融入到我们的日常生活中。最近生成式人工智能（Generative AI）的进展，如GPT、LLaMA、DALL-E和稳定扩散等，给物联网的发展带来了巨大的希望。本文分享了我们对Generative AI在物联网中带来的好处的看法和愿景，并讨论了Generative AI在物联网相关领域的一些重要应用。充分利用Generative AI在物联网中是一个复杂的挑战。我们确定了一些最关键的挑战，包括Generative AI模型的高资源需求、及时工程、设备端推理、卸载、设备端微调、联邦学习、安全以及开发工具和基准，并讨论了当前存在的差距以及使Generative AI在物联网中实现的有希望的机会。我们希望这篇文章能够激发新的研究和创新。

    Equipped with sensing, networking, and computing capabilities, Internet of Things (IoT) such as smartphones, wearables, smart speakers, and household robots have been seamlessly weaved into our daily lives. Recent advancements in Generative AI exemplified by GPT, LLaMA, DALL-E, and Stable Difussion hold immense promise to push IoT to the next level. In this article, we share our vision and views on the benefits that Generative AI brings to IoT, and discuss some of the most important applications of Generative AI in IoT-related domains. Fully harnessing Generative AI in IoT is a complex challenge. We identify some of the most critical challenges including high resource demands of the Generative AI models, prompt engineering, on-device inference, offloading, on-device fine-tuning, federated learning, security, as well as development tools and benchmarks, and discuss current gaps as well as promising opportunities on enabling Generative AI for IoT. We hope this article can inspire new res
    
[^57]: 从多个未指定视角进行无监督的物体中心学习

    Unsupervised Object-Centric Learning from Multiple Unspecified Viewpoints. (arXiv:2401.01922v1 [cs.CV])

    [http://arxiv.org/abs/2401.01922](http://arxiv.org/abs/2401.01922)

    本文提出了一个无监督的深度生成模型，用于从多个未指定的视角学习组合性场景表示。该模型能够将潜在表示分为与视角无关的部分和与视角相关的部分。

    

    视觉场景的多样性非常丰富，不仅因为对象和背景有无限可能的组合，而且因为同一场景的观察在视角变化时可能会有很大的差异。在观察多个视角的多物体视觉场景时，人类可以从每个视角对场景进行组合性感知，同时实现所谓的“对象恒常性”，尽管具体的视角是未知的。这种能力对于人类在移动过程中识别相同对象和高效学习视觉信息至关重要。设计具备类似能力的模型具有很大的吸引力。在本文中，我们考虑了一个新颖的问题，即在没有任何监督的情况下，从多个未指定的视角学习组合性场景表示，并提出了一个深度生成模型，将潜在表示分为与视角无关的部分和与视角相关的部分来解决这个问题。

    Visual scenes are extremely diverse, not only because there are infinite possible combinations of objects and backgrounds but also because the observations of the same scene may vary greatly with the change of viewpoints. When observing a multi-object visual scene from multiple viewpoints, humans can perceive the scene compositionally from each viewpoint while achieving the so-called ``object constancy'' across different viewpoints, even though the exact viewpoints are untold. This ability is essential for humans to identify the same object while moving and to learn from vision efficiently. It is intriguing to design models that have a similar ability. In this paper, we consider a novel problem of learning compositional scene representations from multiple unspecified (i.e., unknown and unrelated) viewpoints without using any supervision and propose a deep generative model which separates latent representations into a viewpoint-independent part and a viewpoint-dependent part to solve th
    
[^58]: AstroLLaMA-Chat: 使用对话和多样化数据集扩展AstroLLaMA

    AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets. (arXiv:2401.01916v1 [astro-ph.IM])

    [http://arxiv.org/abs/2401.01916](http://arxiv.org/abs/2401.01916)

    通过有针对性和持续的预训练，我们在天文学问题回答中扩展了AstroLLaMA，通过使用紧凑的LLaMA-2模型和专门的天文学语料库，我们实现了在专门主题理解方面的显著改进。我们还通过对特定领域的对话数据集进行微调，发布了带有聊天功能的AstroLLaMA。

    

    通过有针对性和持续的预训练，我们探索了在天文学问题回答中增强LLM性能的潜力。通过使用一个紧凑的7B参数的LLaMA-2模型，并且专注于一组经过筛选的天文学语料库，包括摘要、介绍和结论，我们在专门主题理解方面取得了显著的改进。虽然像GPT-4这样的通用LLMs在更广泛的问题回答场景中由于更强大的推理能力而表现出色，但我们的发现表明，有限资源的持续预训练仍然可以提高模型在专门主题上的性能。此外，我们提出了AstroLLaMA的扩展：在特定领域的对话数据集上对7B LLaMA模型进行微调，最终发布了适用于社区使用的具有聊天功能的AstroLLaMA。全面的定量基准测试正在进行中，并将在即将发布的完整论文中详细介绍。模型AstroLLaMA-Chat现已在...

    We explore the potential of enhancing LLM performance in astronomy-focused question-answering through targeted, continual pre-training. By employing a compact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of astronomy corpus -- comprising abstracts, introductions, and conclusions -- we achieve notable improvements in specialized topic comprehension. While general LLMs like GPT-4 outperform in broader question-answering scenarios due to superior reasoning capabilities, our findings suggest that continual pre-training with limited resources can still enhance model performance on specialized topics. Additionally, we present an extension of AstroLLaMA: the fine-tuning of the 7B LLaMA model on a domain-specific conversational dataset, culminating in the release of the chat-enabled AstroLLaMA for community use. Comprehensive quantitative benchmarking is currently in progress and will be detailed in an upcoming full paper. The model, AstroLLaMA-Chat, is now available at
    
[^59]: 缩小时间步长：实现具有脉冲神经网络的低延迟神经形态对象识别

    Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network. (arXiv:2401.01912v1 [cs.CV])

    [http://arxiv.org/abs/2401.01912](http://arxiv.org/abs/2401.01912)

    本研究提出了一种缩小时间步长的脉冲神经网络（SSNN），通过将SNN分成多个阶段，逐步缩小时间步长，实现了低延迟的神经形态对象识别，并通过添加早期分类器解决了性能下降的问题。

    

    脉冲神经网络（SNNs）的神经形态对象识别是低功耗神经形态计算的基石。然而，现有的SNNs在识别神经形态对象时存在显著的延迟问题，需要使用10到40个或更多的时间步长。在低延迟情况下，现有SNNs的性能严重降低。在本研究中，我们提出了缩小SNN（SSNN）来实现低延迟的神经形态对象识别，而不降低性能。具体而言，我们通过将SNNs分成多个阶段，逐步缩小时间步长来减少推断延迟。在时间步长缩小过程中，时间变换器平滑地转换时间尺度并最大程度地保留信息。此外，我们在训练过程中向SNN添加多个早期分类器，以减轻替代梯度和真实梯度之间的不匹配，以及梯度消失/爆炸问题，从而消除了性能下降的问题。

    Neuromorphic object recognition with spiking neural networks (SNNs) is the cornerstone of low-power neuromorphic computing. However, existing SNNs suffer from significant latency, utilizing 10 to 40 timesteps or more, to recognize neuromorphic objects. At low latencies, the performance of existing SNNs is drastically degraded. In this work, we propose the Shrinking SNN (SSNN) to achieve low-latency neuromorphic object recognition without reducing performance. Concretely, we alleviate the temporal redundancy in SNNs by dividing SNNs into multiple stages with progressively shrinking timesteps, which significantly reduces the inference latency. During timestep shrinkage, the temporal transformer smoothly transforms the temporal scale and preserves the information maximally. Moreover, we add multiple early classifiers to the SNN during training to mitigate the mismatch between the surrogate gradient and the true gradient, as well as the gradient vanishing/exploding, thus eliminating the pe
    
[^60]: 对非配对的医学图像-文本基础模型的后门攻击：MedCLIP的试验研究

    Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP. (arXiv:2401.01911v1 [cs.CV])

    [http://arxiv.org/abs/2401.01911](http://arxiv.org/abs/2401.01911)

    该研究探讨了医学图像-文本基础模型中非配对训练引发的标签差异问题，并以此作为后门攻击的案例研究，分析了其对医学FM供应链的影响。

    

    在最近几年中，基础模型（FMs）在深度学习领域中已经确立了其作为基石性进展的角色。通过从大量数据集中提取复杂模式，这些模型始终在各种下游任务中实现最先进的结果，而又不需要大量的计算资源。值得注意的是，基于对比学习的视觉-语言对照学习医学FM MedCLIP采用了非配对的图像-文本训练。虽然医学领域经常采用非配对训练来增强数据，但与该方法相关的潜在安全问题的探索并未跟上其实际使用的步伐。值得注意的是，非配对训练中固有的增强能力也表明微小的标签差异可能导致重要的模型偏差。在本研究中，我们将这种标签差异框架化为后门攻击问题，并进一步分析其对整个FM供应链的影响。我们的评估主要围绕对医学FMs的研究。

    In recent years, foundation models (FMs) have solidified their role as cornerstone advancements in the deep learning domain. By extracting intricate patterns from vast datasets, these models consistently achieve state-of-the-art results across a spectrum of downstream tasks, all without necessitating extensive computational resources. Notably, MedCLIP, a vision-language contrastive learning-based medical FM, has been designed using unpaired image-text training. While the medical domain has often adopted unpaired training to amplify data, the exploration of potential security concerns linked to this approach hasn't kept pace with its practical usage. Notably, the augmentation capabilities inherent in unpaired training also indicate that minor label discrepancies can result in significant model deviations. In this study, we frame this label discrepancy as a backdoor attack problem. We further analyze its impact on medical FMs throughout the FM supply chain. Our evaluation primarily revol
    
[^61]: 基于机器学习的带有缺失数据的粒子识别

    Machine-learning-based particle identification with missing data. (arXiv:2401.01905v1 [physics.ins-det])

    [http://arxiv.org/abs/2401.01905](http://arxiv.org/abs/2401.01905)

    本文提出了一种处理带有缺失数据的粒子识别的方法，使用机器学习模型，首次解决了在ALICE实验中由于不同探测器使用不同技术而产生的数据缺失问题。

    

    本研究介绍了一种新颖的方法，用于在CERN的Large Hadron Collider的ALICE实验范围内进行粒子鉴别（PID）。鉴定LHC提供的超相对论碰撞产物是ALICE的关键目标之一。通常使用的PID方法依赖于手工选择，将实验数据与理论模拟进行比较。为了提高基线方法的性能，新的方法使用机器学习模型，在分类任务中学习正确的分配。然而，由于不同子探测器使用不同的检测技术，以及有限的探测器效率和接受度，产生的粒子并不总是在ALICE的所有组件中产生信号。这导致数据中存在缺失值。机器学习技术无法训练这些示例，因此在训练过程中跳过了数据的重要部分。在本研究中，我们提出了一种第一种适用于PID的方法，该方法可以处理具有缺失数据的情况。

    In this work, we introduce a novel method for Particle Identification (PID) within the scope of the ALICE experiment at the Large Hadron Collider at CERN. Identifying products of ultrarelativisitc collisions delivered by the LHC is one of the crucial objectives of ALICE. Typically employed PID methods rely on hand-crafted selections, which compare experimental data to theoretical simulations. To improve the performance of the baseline methods, novel approaches use machine learning models that learn the proper assignment in a classification task. However, because of the various detection techniques used by different subdetectors, as well as the limited detector efficiency and acceptance, produced particles do not always yield signals in all of the ALICE components. This results in data with missing values. Machine learning techniques cannot be trained with such examples, so a significant part of the data is skipped during training. In this work, we propose the first method for PID that 
    
[^62]: 基于声誉的联邦学习防御来减轻脑电信号分类中的威胁

    Reputation-Based Federated Learning Defense to Mitigate Threats in EEG Signal Classification. (arXiv:2401.01896v1 [cs.CR])

    [http://arxiv.org/abs/2401.01896](http://arxiv.org/abs/2401.01896)

    本文提出了一个基于声誉的联邦学习防御框架，用于在脑电信号分类中防御安全威胁。该框架通过联合训练模型和引入声誉机制来保护隐私和缓解数据污染攻击。

    

    本文提出了一个基于声誉的威胁缓解框架，用于在联邦学习的模型聚合过程中防御电脑脑电图（EEG）信号分类的潜在安全威胁。虽然由于脑-计算机界面（BCI）技术的出现，脑电信号分析受到了关注，但由于分布式的EEG数据的性质以及相关的隐私和安全问题，是很难为EEG分析创建高效的学习模型的。为了解决这些挑战，所提出的防御框架利用了联邦学习范式，通过与来自分散源的本地化数据的协作模型训练来保护隐私，并引入基于声誉的机制来缓解数据污染攻击的影响和识别受损参与者。为了评估所提出的基于声誉的联邦学习防御框架的效率，基于数据训练的风险级别进行了数据污染攻击的实验。

    This paper presents a reputation-based threat mitigation framework that defends potential security threats in electroencephalogram (EEG) signal classification during model aggregation of Federated Learning. While EEG signal analysis has attracted attention because of the emergence of brain-computer interface (BCI) technology, it is difficult to create efficient learning models for EEG analysis because of the distributed nature of EEG data and related privacy and security concerns. To address these challenges, the proposed defending framework leverages the Federated Learning paradigm to preserve privacy by collaborative model training with localized data from dispersed sources and introduces a reputation-based mechanism to mitigate the influence of data poisoning attacks and identify compromised participants. To assess the efficiency of the proposed reputation-based federated learning defense framework, data poisoning attacks based on the risk level of training data derived by Explainab
    
[^63]: 适用于元宇宙导向的协同深度学习的鲁棒性对抗检测和停用方法

    A Robust Adversary Detection-Deactivation Method for Metaverse-oriented Collaborative Deep Learning. (arXiv:2401.01895v1 [cs.CR])

    [http://arxiv.org/abs/2401.01895](http://arxiv.org/abs/2401.01895)

    本文提出了一种适用于元宇宙导向的协同深度学习的鲁棒性对抗检测和停用方法，用于解决CDL训练过程中的安全弱点，保护预训练的大型模型和本地敏感数据集。

    

    元宇宙正趋向于创建一个可以将现实世界转移到大量实时互动支持的在线平台上的数字情境。预训练的人工智能（AI）模型正在展示它们在帮助元宇宙实现优秀响应时的不断增强能力，而现在，许多大型模型都是通过多个参与者协同训练的，这种方式被称为协同深度学习（CDL）。然而，CDL训练过程中存在几个安全弱点，可能对预训练的大型模型或个体实体拥有的本地敏感数据集造成致命攻击。在CDL中，恶意参与者可以隐藏在主要无辜者中，并悄无声息地上传欺骗性参数以降低模型性能，或者他们可以滥用下载的参数构建生成对抗网络（GAN）以非法获取他人的私人信息。

    Metaverse is trending to create a digital circumstance that can transfer the real world to an online platform supported by large quantities of real-time interactions. Pre-trained Artificial Intelligence (AI) models are demonstrating their increasing capability in aiding the metaverse to achieve an excellent response with negligible delay, and nowadays, many large models are collaboratively trained by various participants in a manner named collaborative deep learning (CDL). However, several security weaknesses can threaten the safety of the CDL training process, which might result in fatal attacks to either the pre-trained large model or the local sensitive data sets possessed by an individual entity. In CDL, malicious participants can hide within the major innocent and silently uploads deceptive parameters to degenerate the model performance, or they can abuse the downloaded parameters to construct a Generative Adversarial Network (GAN) to acquire the private information of others ille
    
[^64]: 使用傅里叶神经算子逼近数值通量的超波古典守恒定律

    Approximating Numerical Flux by Fourier Neural Operators for the Hyperbolic Conservation Laws. (arXiv:2401.01783v1 [math.NA])

    [http://arxiv.org/abs/2401.01783](http://arxiv.org/abs/2401.01783)

    该研究通过在传统的数值方案中运用神经网络来替换数值通量，解决了神经网络方法缺乏鲁棒性和泛化能力的问题，并展示了该方法在超声古典守恒定律方面具有优势。

    

    传统的数值方案用于数值解PDE，最近发展了基于神经网络的方法。然而，使用神经网络的方法，如PINN和神经算子，缺乏鲁棒性和泛化能力。为了弥补这些缺点，有很多种类型的研究将传统的数值方案和机器学习方法结合起来，通过用神经网络替代数值方案中的一小部分来实现。在本文中，我们专注于超声古典守恒定律，将数值方案中的数值通量替换为神经算子。为此，我们构造了受数值方案启发的损失函数，并通过FNO逼近数值通量。通过实验证明，我们的方法通过与原始方法的比较具有数值方案和FNO的优势。例如，我们演示了我们的方法具有鲁棒性，分辨率不变性和数据驱动方法的可行性。

    Classical numerical schemes exist for solving PDEs numerically, and recently, neural network-based methods have been developed. However, methodologies using neural networks, such as PINN and neural operators, lack robustness and generalization power. To compensate for such drawbacks, there are many types of research combining classical numerical schemes and machine learning methods by replacing a small portion of the numerical schemes with neural networks. In this work, we focus on hyperbolic conservation laws and replace numerical fluxes in the numerical schemes by neural operator. For this, we construct losses that are motivated by numerical schemes for conservation laws and approximate numerical flux by FNO. Through experiments, we show that our methodology has advantages of both numerical schemes and FNO by comparing with original methods. For instance, we demonstrate our method gains robustness, resolution invariance property, and feasibility of a data-driven method. Our method es
    
[^65]: 朝着基于交易序列的基础采购模型: 预训练的生成自回归

    Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences. (arXiv:2401.01641v1 [cs.LG])

    [http://arxiv.org/abs/2401.01641](http://arxiv.org/abs/2401.01641)

    本文提出了一种基于生成预训练方法的金融交易上下文嵌入模型，该模型在公共数据集上的测试中表现优于最先进的自监督方法，并在卡片欺诈检测问题上取得了显著的性能提升。

    

    机器学习模型广泛应用于现代金融系统中，用于欺诈检测和留存预测等用例。大多数模型基于有标签数据的监督学习和手动设计的特征。目前，大规模自监督生成模型在自然语言处理和计算机视觉方面取得了巨大成功，但尚未将其应用于金融交易的多变量时间序列。本文提出了一种生成预训练方法，可用于获取金融交易的上下文嵌入。公共数据集上的基准测试表明，它在一系列下游任务中优于最先进的自监督方法。我们还使用包含51亿笔交易的180个发卡银行的语料库进行大规模预训练嵌入模型，并将其应用于保留数据集上的卡片欺诈检测问题。嵌入模型显著提高了性能。

    Machine learning models underpin many modern financial systems for use cases such as fraud detection and churn prediction. Most are based on supervised learning with hand-engineered features, which relies heavily on the availability of labelled data. Large self-supervised generative models have shown tremendous success in natural language processing and computer vision, yet so far they haven't been adapted to multivariate time series of financial transactions. In this paper, we present a generative pretraining method that can be used to obtain contextualised embeddings of financial transactions. Benchmarks on public datasets demonstrate that it outperforms state-of-the-art self-supervised methods on a range of downstream tasks. We additionally perform large-scale pretraining of an embedding model using a corpus of data from 180 issuing banks containing 5.1 billion transactions and apply it to the card fraud detection problem on hold-out datasets. The embedding model significantly impro
    
[^66]: 超越效率：资源高效的大型语言模型的系统调研

    Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models. (arXiv:2401.00625v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.00625](http://arxiv.org/abs/2401.00625)

    本调研系统地解决了大型语言模型的资源效率挑战，介绍了各种优化方法和技术，包括计算、内存、能量、财务和网络资源的优化，在LLM的生命周期的各个阶段都具有应用价值，同时提出了细致的资源效率技术分类。

    

    大型语言模型（LLMs）领域的迅猛发展代表了人工智能领域的一个重要进步，这些模型，如OpenAI的ChatGPT，带来了计算、内存、能量和财务资源高消耗等重大挑战，尤其是在资源有限的环境中。本调研旨在系统地解决这些挑战，并审查了一系列旨在增强LLMs资源效率的技术。我们根据优化重点对方法进行分类：计算、内存、能量、财务和网络资源，以及它们在LLM生命周期的各个阶段（包括架构设计、预训练、微调和系统设计）中的适用性。此外，该调研通过特定资源类型引入了细致的资源效率技术分类，揭示了不同资源之间的复杂关系和映射。

    The burgeoning field of Large Language Models (LLMs), exemplified by sophisticated models like OpenAI's ChatGPT, represents a significant advancement in artificial intelligence. These models, however, bring forth substantial challenges in the high consumption of computational, memory, energy, and financial resources, especially in environments with limited resource capabilities. This survey aims to systematically address these challenges by reviewing a broad spectrum of techniques designed to enhance the resource efficiency of LLMs. We categorize methods based on their optimization focus: computational, memory, energy, financial, and network resources and their applicability across various stages of an LLM's lifecycle, including architecture design, pretraining, finetuning, and system design. Additionally, the survey introduces a nuanced categorization of resource efficiency techniques by their specific resource types, which uncovers the intricate relationships and mappings between var
    
[^67]: 通过张量化、无反向传播的光学PINN训练实现实时FJ/MAC PDE求解器

    Real-Time FJ/MAC PDE Solvers via Tensorized, Back-Propagation-Free Optical PINN Training. (arXiv:2401.00413v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.00413](http://arxiv.org/abs/2401.00413)

    本论文开发了一种基于光学计算的物理相关神经网络（PINN）训练框架，用于实时解决高维PDE问题。该方法具有低功耗和超低延迟，通过避免反向传播过程和使用张量压缩方法来提高收敛性和可扩展性。

    

    在实际应用中，数值解各种偏微分方程（PDE）通常需要大量的计算时间、能量消耗和硬件资源。这限制了它们在许多场景（如自主系统、超音速流动）中的应用，这些场景具有有限的能源预算和需要近实时响应。利用光学计算，本文开发了一个芯片上的物理相关神经网络（PINN）训练框架，旨在使用fJ/MAC光子功耗和超低延迟解决高维PDE。尽管光学神经网络具有超高速度，但在光学芯片上训练PINN是困难的，原因是（1）光子设备的尺寸较大，（2）缺乏可扩展的光学存储设备来存储反向传播过程的中间结果。为了实现实际的光学PINN训练，本文提出了一种可扩展的方法来避免反向传播过程。我们还采用张量压缩方法来提高收敛性和可扩展性。

    Solving partial differential equations (PDEs) numerically often requires huge computing time, energy cost, and hardware resources in practical applications. This has limited their applications in many scenarios (e.g., autonomous systems, supersonic flows) that have a limited energy budget and require near real-time response. Leveraging optical computing, this paper develops an on-chip training framework for physics-informed neural networks (PINNs), aiming to solve high-dimensional PDEs with fJ/MAC photonic power consumption and ultra-low latency. Despite the ultra-high speed of optical neural networks, training a PINN on an optical chip is hard due to (1) the large size of photonic devices, and (2) the lack of scalable optical memory devices to store the intermediate results of back-propagation (BP). To enable realistic optical PINN training, this paper presents a scalable method to avoid the BP process. We also employ a tensor-compressed approach to improve the convergence and scalabi
    
[^68]: 知识增强的医疗时间序列条件插补方法

    Knowledge Enhanced Conditional Imputation for Healthcare Time-series. (arXiv:2312.16713v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16713](http://arxiv.org/abs/2312.16713)

    本研究提出了一种知识增强的条件插补方法，针对医疗时间序列数据中的缺失数据问题。通过整合先进的知识嵌入和非均匀掩蔽策略，该方法能够灵活适应不同模式的电子健康记录中的缺失数据分布不平衡问题。

    

    本研究提出了一种新颖的方法来解决多变量时间序列中的缺失数据问题，特别关注医疗数据的复杂性。我们的条件自注意力插补（CSAI）模型以基于Transformer的框架为基础，引入了一种针对医疗时间序列数据细节的条件隐藏状态初始化方式。该方法与传统的插补技术不同，它特别针对医疗数据集中缺失数据分布的不平衡问题，这一关键问题常常被忽视。通过整合先进的知识嵌入和非均匀掩蔽策略，CSAI能够灵活适应电子健康记录（EHR）中缺失数据的不同模式。

    This study presents a novel approach to addressing the challenge of missing data in multivariate time series, with a particular focus on the complexities of healthcare data. Our Conditional Self-Attention Imputation (CSAI) model, grounded in a transformer-based framework, introduces a conditional hidden state initialization tailored to the intricacies of medical time series data. This methodology diverges from traditional imputation techniques by specifically targeting the imbalance in missing data distribution, a crucial aspect often overlooked in healthcare datasets. By integrating advanced knowledge embedding and a non-uniform masking strategy, CSAI adeptly adjusts to the distinct patterns of missing data in Electronic Health Records (EHRs).
    
[^69]: 偏好作为奖励，使用重要性抽样进行最大偏好优化

    Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16430](http://arxiv.org/abs/2312.16430)

    本文提出了一种使用重要性抽样进行最大偏好优化的算法，该算法通过直接优化生成策略来消除对奖励模型的需求，提高了数据利用率和稳定性，并通过解决KL正则化问题来改善偏好学习效果。

    

    偏好学习是将语言模型与人类价值观对齐的关键技术。从人类反馈强化学习（RLHF）是一种基于模型的算法，用于优化偏好学习，首先拟合偏好分数的奖励模型，然后使用基于策略梯度算法进行优化，以最大化奖励。RLHF的处理过程复杂、耗时且不稳定。直接偏好优化（DPO）算法使用离策略算法直接优化生成策略，消除了对奖励模型的需求，具有高效和稳定的数据利用率。DPO使用布拉德利-特里模型和对数损失，导致在偏好接近确定性时忽略了KL正则化项而过度拟合偏好数据。IPO使用一种基于根查找的成对均方误差损失来解决忽略KL正则化问题，并学习到最优策略。但是IPO的成对损失仍然无法使KL正则化生效。本文设计了一种新的算法，使用重要性抽样技术来解决偏好学习中的优化问题。

    Preference learning is a key technology for aligning language models with human values. Reinforcement Learning from Human Feedback (RLHF) is a model based algorithm to optimize preference learning, which first fitting a reward model for preference score, and then optimizing generating policy with on-policy PPO algorithm to maximize the reward. The processing of RLHF is complex, time-consuming and unstable. Direct Preference Optimization (DPO) algorithm using off-policy algorithm to direct optimize generating policy and eliminating the need for reward model, which is data efficient and stable. DPO use Bradley-Terry model and log-loss which leads to over-fitting to the preference data at the expense of ignoring KL-regularization term when preference near deterministic. IPO uses a root-finding pairwise MSE loss to solve the ignoring KL-regularization problem, and learning an optimal policy. But IPO's pairwise loss still can't s make the KL-regularization to work. In this paper, we design 
    
[^70]: 用于治疗肽生成的多模式对比扩散模型

    A Multi-Modal Contrastive Diffusion Model for Therapeutic Peptide Generation. (arXiv:2312.15665v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2312.15665](http://arxiv.org/abs/2312.15665)

    这项研究提出了一个多模式对比扩散模型，融合了序列和结构信息，用于生成治疗肽。模型通过对比学习策略捕捉两种模态之间的一致性，提高了生成效果。

    

    治疗肽代表一类对治疗人类疾病至关重要的药物。近年来，深度生成模型展现出在生成治疗肽方面的巨大潜力，但它们仅利用序列或结构信息，限制了生成效果。本研究提出了一个多模式对比扩散模型（MMCD），将序列和结构模态融合在一个扩散框架中，共同生成新的肽序列和结构。具体而言，MMCD分别构建了序列模态和结构模态的扩散模型，并设计了一种多模式对比学习策略，在每个扩散时间步中进行区间对比和内对比，旨在捕捉两种模态之间的一致性并提升模型性能。区间对比通过最大化嵌入的一致性来对齐肽的序列和结构，而内对比则通过区分嵌入来增强学习效果。

    Therapeutic peptides represent a unique class of pharmaceutical agents crucial for the treatment of human diseases. Recently, deep generative models have exhibited remarkable potential for generating therapeutic peptides, but they only utilize sequence or structure information alone, which hinders the performance in generation. In this study, we propose a Multi-Modal Contrastive Diffusion model (MMCD), fusing both sequence and structure modalities in a diffusion framework to co-generate novel peptide sequences and structures. Specifically, MMCD constructs the sequence-modal and structure-modal diffusion models, respectively, and devises a multi-modal contrastive learning strategy with intercontrastive and intra-contrastive in each diffusion timestep, aiming to capture the consistency between two modalities and boost model performance. The inter-contrastive aligns sequences and structures of peptides by maximizing the agreement of their embeddings, while the intra-contrastive differenti
    
[^71]: 对抗性数据污染用于假新闻检测：如何使模型在不修改目标新闻的情况下将其错误分类

    Adversarial Data Poisoning for Fake News Detection: How to Make a Model Misclassify a Target News without Modifying It. (arXiv:2312.15228v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.15228](http://arxiv.org/abs/2312.15228)

    本文分析了对抗性攻击在假新闻检测模型中的威胁，研究了攻击者如何在不修改原始目标新闻的情况下通过引入污染数据来操纵模型的行为。

    

    假新闻检测模型对于对抗性攻击具有脆弱性。在这篇文章中，我们分析了攻击者如何在不修改原始目标新闻的情况下破坏在线学习检测器对特定新闻内容的性能。在某些情况下，例如社交网络中，攻击者无法完全控制所有信息，这种情况确实可能发生。因此，我们展示了攻击者如何可能通过将污染数据引入训练数据来操纵在线学习方法的行为。我们的初步研究结果显示，基于复杂性和攻击类型，逻辑回归模型对此的易受攻击性各不相同。

    Fake news detection models are critical to countering disinformation but can be manipulated through adversarial attacks. In this position paper, we analyze how an attacker can compromise the performance of an online learning detector on specific news content without being able to manipulate the original target news. In some contexts, such as social networks, where the attacker cannot exert complete control over all the information, this scenario can indeed be quite plausible. Therefore, we show how an attacker could potentially introduce poisoning data into the training data to manipulate the behavior of an online learning method. Our initial findings reveal varying susceptibility of logistic regression models based on complexity and attack type.
    
[^72]: 连续时间序列集合的概率建模

    Probabilistic Modeling for Sequences of Sets in Continuous-Time. (arXiv:2312.15045v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.15045](http://arxiv.org/abs/2312.15045)

    本文提出了一个通用的连续时间序列集合的概率建模框架，适用于处理每个事件与一组项目相关联的情况。引入了适用于任何强度为基础的递归神经点过程模型的推理方法，可用于回答关于序列历史条件下的概率查询问题。

    

    在连续时间事件数据的统计参数模型工具箱中，神经标记时间点过程是一个有价值的补充。这些模型适用于每个事件与单个项目（单个事件类型或“标记”）相关联的序列，但不适用于每个事件与一组项目相关联的实际情况。本文中，我们开发了一个通用的连续时间集合数值数据建模框架，与任何基于强度的递归神经点过程模型兼容。此外，我们还开发了推理方法，可使用这些模型回答诸如“在考虑序列历史的条件下，项目A在项目B之前观察到的概率”等概率查询问题。由于问题设置的连续时间性质和每个事件的潜在结果空间的组合极大，对于神经模型来说，计算这些查询的精确答案通常是不可行的。

    Neural marked temporal point processes have been a valuable addition to the existing toolbox of statistical parametric models for continuous-time event data. These models are useful for sequences where each event is associated with a single item (a single type of event or a "mark") -- but such models are not suited for the practical situation where each event is associated with a set of items. In this work, we develop a general framework for modeling set-valued data in continuous-time, compatible with any intensity-based recurrent neural point process model. In addition, we develop inference methods that can use such models to answer probabilistic queries such as "the probability of item $A$ being observed before item $B$," conditioned on sequence history. Computing exact answers for such queries is generally intractable for neural models due to both the continuous-time nature of the problem setting and the combinatorially-large space of potential outcomes for each event. To address th
    
[^73]: 基于等变性的幻觉理论

    Theory of Hallucinations based on Equivariance. (arXiv:2312.14504v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.14504](http://arxiv.org/abs/2312.14504)

    本研究提出了基于等变性的幻觉理论，探讨了大型语言模型中幻觉的成因，并开发了一种衡量语言模型幻觉程度的交叉熵误差函数。通过测试语言模型在获得等变性方面的能力，研究表明某些类型的等变语言模型对理解复杂社交关系表现出色。

    

    本研究旨在获取创建对幻觉免疫的大型语言模型所需的知识。现代大型语言模型中的幻觉往往归因于对现实社交关系的误解。因此，我假设能够彻底掌握所有这些关系的大型语言模型将不会出现幻觉。此外，我提出了某些类型的等变语言模型在学习和理解这些关系方面表现出色。在此基础上，我开发了一种专门用于创建语言模型幻觉程度的交叉熵误差函数，该函数衡量了它们获得等变性的程度。利用这个指标，我测试了语言模型在获得字符级等变性方面的能力。特别地，我引入并采用了一种基于T5（文本到文本转换变压器）的新技术，可以高效地理解经过排列的输入文本而无需解释。

    This study aims to acquire knowledge for creating very large language models that are immune to hallucinations. Hallucinations in contemporary large language models are often attributed to a misunderstanding of real-world social relationships. Therefore, I hypothesize that very large language models capable of thoroughly grasping all these relationships will be free from hallucinations. Additionally, I propose that certain types of equivariant language models are adept at learning and understanding these relationships. Building on this, I have developed a specialized cross-entropy error function to create a hallucination scale for language models, which measures their extent of equivariance acquisition. Utilizing this scale, I tested language models for their ability to acquire character-level equivariance. In particular, I introduce and employ a novel technique based on T5 (Text To Text Transfer Transformer) that efficiently understands permuted input texts without the need for explic
    
[^74]: 基于显著特征的水声信号识别

    Underwater Acoustic Signal Recognition Based on Salient Feature. (arXiv:2312.13143v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2312.13143](http://arxiv.org/abs/2312.13143)

    这项研究提出了一种基于显著特征的水声信号识别方法，利用深度学习模型从频谱中提取特征，不断学习以分类水声信号，以应对复杂关系和提高识别准确率。

    

    随着技术的快速发展，复杂环境中水声信号的识别变得越来越重要。目前，主流的水声信号识别主要依赖于时频分析来提取频谱特征，在该领域广泛应用。然而，现有的识别方法过于依赖专家系统，面临着知识库受限和处理复杂关系的挑战。这些限制源于规则或推理引擎的复杂性和维护困难。鉴于深度学习在处理复杂关系方面的潜在优势，本文提出了一种利用神经网络进行水声信号识别的方法。该方法涉及从频谱中提取特征并不断学习以分类水声信号。深度学习模型能够自动学习抽象特征，提高识别准确率。

    With the rapid advancement of technology, the recognition of underwater acoustic signals in complex environments has become increasingly crucial. Currently, mainstream underwater acoustic signal recognition relies primarily on time-frequency analysis to extract spectral features, finding widespread applications in the field. However, existing recognition methods heavily depend on expert systems, facing limitations such as restricted knowledge bases and challenges in handling complex relationships. These limitations stem from the complexity and maintenance difficulties associated with rules or inference engines. Recognizing the potential advantages of deep learning in handling intricate relationships, this paper proposes a method utilizing neural networks for underwater acoustic signal recognition. The proposed approach involves continual learning of features extracted from spectra for the classification of underwater acoustic signals. Deep learning models can automatically learn abstra
    
[^75]: SLP-Net：用于皮肤病变分割的高效轻量级网络

    SLP-Net:An efficient lightweight network for segmentation of skin lesions. (arXiv:2312.12789v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2312.12789](http://arxiv.org/abs/2312.12789)

    提出了一种名为SLP-Net的超轻量级皮肤病变分割网络，基于脉冲神经机制，具有较小的参数数量和高计算速度。与常见的编码器-解码器结构不同，使用特征适应模块实现多尺度信息解码。在实验中表现出在准确度和Dice系数方面均优于其他方法，具有良好的泛化能力。

    

    对于黑素瘤的及时治疗至关重要。为了帮助医生快速准确地识别病变区域，我们提出了一种新的皮肤病变分割技术，名为SLP-Net，这是一种基于脉冲神经P(SNP)系统机制的超轻量级分割网络。大多数现有的卷积神经网络在高分割准确度的同时忽视了高硬件成本。相反，SLP-Net具有非常少的参数和高计算速度。我们设计了一个轻量级的多尺度特征提取器，没有常见的编码器-解码器结构。我们设计了一个特征适应模块来替代解码器，并实现多尺度信息解码。在ISIC2018挑战赛上的实验证明，所提出的模型在Acc和DSC上具有最高的性能，而在PH2数据集上的实验证明了良好的泛化能力。

    Prompt treatment for melanoma is crucial. To assist physicians in identifying lesion areas precisely in a quick manner, we propose a novel skin lesion segmentation technique namely SLP-Net, an ultra-lightweight segmentation network based on the spiking neural P(SNP) systems type mechanism. Most existing convolutional neural networks achieve high segmentation accuracy while neglecting the high hardware cost. SLP-Net, on the contrary, has a very small number of parameters and a high computation speed. We design a lightweight multi-scale feature extractor without the usual encoder-decoder structure. Rather than a decoder, a feature adaptation module is designed to replace it and implement multi-scale information decoding. Experiments at the ISIC2018 challenge demonstrate that the proposed model has the highest Acc and DSC among the state-of-the-art methods, while experiments on the PH2 dataset also demonstrate a favorable generalization ability. Finally, we compare the computational compl
    
[^76]: Lookahead:一种用于具有无损生成准确性的大型语言模型的推理加速框架

    Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy. (arXiv:2312.12728v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2312.12728](http://arxiv.org/abs/2312.12728)

    本研究介绍了一种通用的推理加速框架，用于提高大型语言模型（LLMs）的推理速度，并在保持生成准确性的同时降低成本。该框架在支付宝的检索增强生成（RAG）系统中得到了应用。

    

    随着大型语言模型（LLMs）在各种任务中取得了重大进展，如问答、翻译、文本摘要和对话系统，尤其是对于像支付宝这样为数十亿用户提供重要金融产品的需要准确信息的情况，信息的准确性变得至关重要。为了解决这个问题，支付宝开发了一种称为检索增强生成（RAG）系统的方法，该系统将LLMs与最准确和最新的信息相结合。然而，对于为数百万用户提供服务的真实产品来说，LLMs的推理速度成为一个关键因素，而不仅仅是一个实验性的模型。因此，本文提出了一种通用的推理加速框架，通过加速推理过程，实现了我们的RAG系统的速度大幅提升和成本降低，同时保持着无损的生成准确性。在传统的推理过程中，每个令牌都由LLMs按顺序生成，导致的时间消耗与生成的令牌数成正比。

    As Large Language Models (LLMs) have made significant advancements across various tasks, such as question answering, translation, text summarization, and dialogue systems, the need for accuracy in information becomes crucial, especially for serious financial products serving billions of users like Alipay. To address this, Alipay has developed a Retrieval-Augmented Generation (RAG) system that grounds LLMs on the most accurate and up-to-date information. However, for a real-world product serving millions of users, the inference speed of LLMs becomes a critical factor compared to a mere experimental model.  Hence, this paper presents a generic framework for accelerating the inference process, resulting in a substantial increase in speed and cost reduction for our RAG system, with lossless generation accuracy. In the traditional inference process, each token is generated sequentially by the LLM, leading to a time consumption proportional to the number of generated tokens. To enhance this 
    
[^77]: 连续学习: 面向视频表示的免遗忘优胜子网络

    Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.11973](http://arxiv.org/abs/2312.11973)

    本研究基于"彩票票据假设"，提出了一种连续学习方法，通过利用稀疏子网络和FSO进行任务增量学习、少样本类增量学习和视频增量学习，实现高效学习和有效的权重重用。

    

    受到"彩票票据假设"（LTH）的启发，该假设强调在较大的密集网络中存在高效子网络，研究了在适当的稀疏条件下表现优秀的优胜子网络（WSN）在各种连续学习任务中的应用。它利用来自密集网络的预先存在的权重，在任务增量学习（TIL）场景中实现高效学习。在少样本类增量学习（FSCIL）中，设计了一种称为软子网络（SoftNet）的WSN变体，以防止数据样本稀缺时的过拟合。此外，考虑了WSN权重的稀疏重用，用于视频增量学习（VIL）。考虑了在WSN中使用傅立叶子神经运算器（FSO），它能够对视频进行紧凑编码，并在不同带宽下识别可重用的子网络。我们将FSO集成到不同的连续学习架构中，包括VIL、TIL和FSCIL。

    Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the existence of efficient subnetworks within larger, dense networks, a high-performing Winning Subnetwork (WSN) in terms of task performance under appropriate sparsity conditions is considered for various continual learning tasks. It leverages pre-existing weights from dense networks to achieve efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is designed to prevent overfitting when the data samples are scarce. Furthermore, the sparse reuse of WSN weights is considered for Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It enables compact encoding of videos and identifies reusable subnetworks across varying bandwidths. We have integrated FSO into different architectural frameworks for continual learning, including VIL, TIL, and FSCIL. Our c
    
[^78]: 评估语言模型代理在现实自主任务中的表现

    Evaluating Language-Model Agents on Realistic Autonomous Tasks. (arXiv:2312.11671v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.11671](http://arxiv.org/abs/2312.11671)

    这篇论文评估了语言模型代理在现实自主任务中的表现，发现这些代理只能完成最简单的任务，对于更具挑战性的任务有一定进展。

    

    在这篇报告中，我们探索了语言模型代理在野外获取资源、复制自身和适应新挑战的能力。我们称这些能力为"自主复制和适应"或者ARA。我们认为具备ARA能力的系统可能具有广泛而难以预测的后果，并且对于衡量和预测ARA能力可能有助于制定相关的安全、监测和对齐措施。此外，一旦系统具备ARA能力，对系统能力的限制可能变得更加困难。我们构建了四个简单的示例代理，将语言模型与允许其在世界中采取行动的工具相结合。然后，我们对这些代理在与ARA相关的12个任务上进行评估。我们发现这些语言模型代理只能完成任务列表中最简单的任务，尽管对于更具挑战性的任务也有一定的进展。不幸的是，这些评估还没有完成。

    In this report, we explore the ability of language model agents to acquire resources, create copies of themselves, and adapt to novel challenges they encounter in the wild. We refer to this cluster of capabilities as "autonomous replication and adaptation" or ARA. We believe that systems capable of ARA could have wide-reaching and hard-to-anticipate consequences, and that measuring and forecasting ARA may be useful for informing measures around security, monitoring, and alignment. Additionally, once a system is capable of ARA, placing bounds on a system's capabilities may become significantly more difficult.  We construct four simple example agents that combine language models with tools that allow them to take actions in the world. We then evaluate these agents on 12 tasks relevant to ARA. We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks. Unfortunately, these evaluations are not 
    
[^79]: GNN学习评估中的不确定性：量化GNN社区检测中随机性的度量方法比较

    Uncertainty in GNN Learning Evaluations: A Comparison Between Measures for Quantifying Randomness in GNN Community Detection. (arXiv:2312.09015v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.09015](http://arxiv.org/abs/2312.09015)

    本研究比较了GNN社区检测中不同的度量方法，在考虑随机性的情况下，评估了算法排名的一致性，并发现在忽视超参数调查时性能明显下降。

    

    (1) 图神经网络(GNN)在无监督社区检测中的增强能力归因于它们能够编码图的连接和特征信息空间。潜在社区的识别在从社交网络到基因组学的各个领域都具有实际意义。由于影响GNN评估的决策众多，当前实际性能基准令人困惑。(2) 比较了三个指标来评估算法排名在存在随机性时的一致性。通过使用默认超参数进行超参数优化的结果的一致性和性能质量进行评估。(3) 结果比较了使用默认超参数进行超参数优化，发现在忽视超参数调查时性能明显下降。指标的比较表明，排名中的并列名次可能会大大改变量化结果。

    (1) The enhanced capability of Graph Neural Networks (GNNs) in unsupervised community detection of clustered nodes is attributed to their capacity to encode both the connectivity and feature information spaces of graphs. The identification of latent communities holds practical significance in various domains, from social networks to genomics. Current real-world performance benchmarks are perplexing due to the multitude of decisions influencing GNN evaluations for this task. (2) Three metrics are compared to assess the consistency of algorithm rankings in the presence of randomness. The consistency and quality of performance between the results under a hyperparameter optimisation with the default hyperparameters is evaluated. (3) The results compare hyperparameter optimisation with default hyperparameters, revealing a significant performance loss when neglecting hyperparameter investigation. A comparison of metrics indicates that ties in ranks can substantially alter the quantification 
    
[^80]: Bayesian网络的熵和Kullback-Leibler散度：计算复杂度和高效实现

    Entropy and the Kullback-Leibler Divergence for Bayesian Networks: Computational Complexity and Efficient Implementation. (arXiv:2312.01520v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.01520](http://arxiv.org/abs/2312.01520)

    本文提出了一种计算贝叶斯网络中Shannon熵和Kullback-Leibler散度的高效算法，并通过一系列数值示例进行了演示。此外，还展示了如何将高斯贝叶斯网络中KL的计算复杂度从立方降低到二次。

    

    贝叶斯网络（BNs）是机器学习和因果推断中的基础模型。它们的图结构可以处理高维问题，并将其分为稀疏的一系列较小问题，这是Judea Pearl的因果性的基础，也决定了它们的可解释性和可理解性。尽管它们很受欢迎，但在文献中几乎没有关于如何在最常见的分布假设下计算BNs的Shannon熵和Kullback-Leibler（KL）散度的资源。在本文中，我们利用BNs的图结构提供了计算效率高的算法，并用一整套数值示例说明了它们。在此过程中，我们展示了可以将高斯BNs的KL计算复杂度从立方降低到二次的可能性。

    Bayesian networks (BNs) are a foundational model in machine learning and causal inference. Their graphical structure can handle high-dimensional problems, divide them into a sparse collection of smaller ones, underlies Judea Pearl's causality, and determines their explainability and interpretability. Despite their popularity, there are almost no resources in the literature on how to compute Shannon's entropy and the Kullback-Leibler (KL) divergence for BNs under their most common distributional assumptions. In this paper, we provide computationally efficient algorithms for both by leveraging BNs' graphical structure, and we illustrate them with a complete set of numerical examples. In the process, we show it is possible to reduce the computational complexity of KL from cubic to quadratic for Gaussian BNs.
    
[^81]: LinFlo-Net: 一种生成心脏模拟网格的两阶段深度学习方法

    LinFlo-Net: A two-stage deep learning method to generate simulation ready meshes of the heart. (arXiv:2310.20065v1 [cs.CV])

    [http://arxiv.org/abs/2310.20065](http://arxiv.org/abs/2310.20065)

    本文提出了一种新的深度学习模型，通过两阶段形变和新型损失函数的应用，实现了自动生成心脏薄壁结构的计算机模型。该模型在准确性和网格质量方面表现出色，可以直接用于物理模拟，减少后处理的需求。

    

    本文提出了一种深度学习模型，可以从患者成像数据中自动生成人类心脏的计算机模型，并且特别强调其能够生成薄壁心脏结构。我们的方法通过将一个模板网格形变到给定图像的心脏结构上。与以前采用这种方法的深度学习方法相比，我们的框架旨在最小化网格自穿透，这通常发生在形变的表面网格之间的小距离。我们通过使用两阶段的形变过程和一种基于运动动力学导出的新型损失函数来实现这一点，该损失函数惩罚表面接触和相互渗透。我们的模型在与最先进方法相比的同时，还能产生不自相交的网格。产生的网格在基于物理的模拟中可直接使用，最大限度地减少了后处理和清理的需求。

    We present a deep learning model to automatically generate computer models of the human heart from patient imaging data with an emphasis on its capability to generate thin-walled cardiac structures. Our method works by deforming a template mesh to fit the cardiac structures to the given image. Compared with prior deep learning methods that adopted this approach, our framework is designed to minimize mesh self-penetration, which typically arises when deforming surface meshes separated by small distances. We achieve this by using a two-stage diffeomorphic deformation process along with a novel loss function derived from the kinematics of motion that penalizes surface contact and interpenetration. Our model demonstrates comparable accuracy with state-of-the-art methods while additionally producing meshes free of self-intersections. The resultant meshes are readily usable in physics based simulation, minimizing the need for post-processing and cleanup.
    
[^82]: CBD: 基于局部主导概率的可信后门检测器

    CBD: A Certified Backdoor Detector Based on Local Dominant Probability. (arXiv:2310.17498v1 [cs.LG])

    [http://arxiv.org/abs/2310.17498](http://arxiv.org/abs/2310.17498)

    本文提出了第一个可信后门检测器（CBD），它基于一种新颖的、可调节的符合预测方案，即局部主导概率统计。CBD能够提供对分类器的检测推断结果，并给出攻击保证可检测的条件和假阳性率的概率上界。实验证明，具有更高鲁棒性的触发器和更小扰动幅度的攻击更容易被检测出来。

    

    后门攻击是深度神经网络面临的常见威胁。在测试过程中，嵌入了后门触发器的样本将被后门模型误分类为对抗目标，而没有后门触发器的样本将被正确分类。本文提出了第一个可信后门检测器（CBD），它基于我们提出的局部主导概率统计的一种新颖的、可调节的符合预测方案。对于受检测的任何分类器，CBD提供了1）检测推断结果，2）攻击在同一分类域下保证可检测的条件，并且3）假阳性率的概率上界。我们的理论结果表明，具有更高鲁棒性的触发器、更小扰动幅度的攻击更有可能被有保证地检测出来。此外，我们在考虑各种后门类型的四个基准数据集上进行了大量实验证明。

    Backdoor attack is a common threat to deep neural networks. During testing, samples embedded with a backdoor trigger will be misclassified as an adversarial target by a backdoored model, while samples without the backdoor trigger will be correctly classified. In this paper, we present the first certified backdoor detector (CBD), which is based on a novel, adjustable conformal prediction scheme based on our proposed statistic local dominant probability. For any classifier under inspection, CBD provides 1) a detection inference, 2) the condition under which the attacks are guaranteed to be detectable for the same classification domain, and 3) a probabilistic upper bound for the false positive rate. Our theoretical results show that attacks with triggers that are more resilient to test-time noise and have smaller perturbation magnitudes are more likely to be detected with guarantees. Moreover, we conduct extensive experiments on four benchmark datasets considering various backdoor types, 
    
[^83]: 更好、更简单的差分隐私统计估计的下界

    Better and Simpler Lower Bounds for Differentially Private Statistical Estimation. (arXiv:2310.06289v1 [math.ST])

    [http://arxiv.org/abs/2310.06289](http://arxiv.org/abs/2310.06289)

    本文提出了更好、更简单的差分隐私统计估计的下界，适用于估计高斯协方差的谱误差和有界$k$阶矩的重尾分布的均值估计。

    

    我们为两个众所周知的高维私有估计任务提供了改进的下界。首先，我们证明，在近似差分隐私的情况下，对于用于估计高斯协方差的谱误差$\alpha$，任意$\alpha \le O(1)$，需要$\tilde{\Omega} \left(\frac{d^{3/2}}{\alpha \varepsilon} + \frac{d}{\alpha^2}\right)$个样本，这是紧凑的，仅差对数因子。这比先前的工作在$\alpha \le O\left(\frac{1}{\sqrt{d}}\right)$时建立的结果要好，并且比之前的工作更简单。接下来，我们证明，在近似差分隐私的情况下，对于有界$k$阶矩的重尾分布的均值估计，需要$\tilde{\Omega}\left(\frac{d}{\alpha^{k/(k-1)} \varepsilon} + \frac{d}{\alpha^2}\right)$个样本。这与已知的上界相吻合，并改进了此问题的已知最佳下界，该下界仅适用于纯粹的差分隐私或$k = 2$的情况。我们的技术遵循指纹方式的方法。

    We provide improved lower bounds for two well-known high-dimensional private estimation tasks. First, we prove that for estimating the covariance of a Gaussian up to spectral error $\alpha$ with approximate differential privacy, one needs $\tilde{\Omega}\left(\frac{d^{3/2}}{\alpha \varepsilon} + \frac{d}{\alpha^2}\right)$ samples for any $\alpha \le O(1)$, which is tight up to logarithmic factors. This improves over previous work which established this for $\alpha \le O\left(\frac{1}{\sqrt{d}}\right)$, and is also simpler than previous work. Next, we prove that for estimating the mean of a heavy-tailed distribution with bounded $k$th moments with approximate differential privacy, one needs $\tilde{\Omega}\left(\frac{d}{\alpha^{k/(k-1)} \varepsilon} + \frac{d}{\alpha^2}\right)$ samples. This matches known upper bounds and improves over the best known lower bound for this problem, which only hold for pure differential privacy, or when $k = 2$. Our techniques follow the method of fingerpr
    
[^84]: 跨条件分支的自动微分平滑方法

    Smoothing Methods for Automatic Differentiation Across Conditional Branches. (arXiv:2310.03585v1 [cs.LG])

    [http://arxiv.org/abs/2310.03585](http://arxiv.org/abs/2310.03585)

    本研究提出了一种通过结合平滑解释和自动微分的方法来处理具有条件分支的程序，并成功计算出平滑程序的梯度，从而支持分支程序参数合成和机器学习流程中的模型校准。

    

    具有条件分支引入的不连续性的程序对假定目标函数响应曲面具有一定平滑性的数学优化方法提出了挑战。平滑解释（SI）是一种抽象解释形式，它以高斯核近似程序输出的卷积，从而以原则性的方式平滑其输出。在这里，我们将SI与自动微分（AD）相结合，以高效地计算平滑程序的梯度。与在常规程序执行中进行的自动微分不同，这些梯度还捕捉了替代控制流路径的影响。SI与AD的组合使得支持基于梯度的分支程序参数合成成为可能，例如在机器学习流程中，对仿真模型进行校准或将其与神经网络模型结合。我们详细说明了SI中为可行性而进行的近似的影响，并提出了改进方法。

    Programs involving discontinuities introduced by control flow constructs such as conditional branches pose challenges to mathematical optimization methods that assume a degree of smoothness in the objective function's response surface. Smooth interpretation (SI) is a form of abstract interpretation that approximates the convolution of a program's output with a Gaussian kernel, thus smoothing its output in a principled manner. Here, we combine SI with automatic differentiation (AD) to efficiently compute gradients of smoothed programs. In contrast to AD across a regular program execution, these gradients also capture the effects of alternative control flow paths. The combination of SI with AD enables the direct gradient-based parameter synthesis for branching programs, allowing for instance the calibration of simulation models or their combination with neural network models in machine learning pipelines. We detail the effects of the approximations made for tractability in SI and propose
    
[^85]: 使用CodeBERT和Random Forest Regressor进行自动评分的C编程作业

    Auto-grading C programming assignments with CodeBERT and Random Forest Regressor. (arXiv:2309.15216v1 [cs.LG])

    [http://arxiv.org/abs/2309.15216](http://arxiv.org/abs/2309.15216)

    本研究提供了使用机器学习和深度学习方法对C编程作业进行自动评分的分析，并引入了基于代码的转换器词嵌入模型CodeBERT。测试结果表明该策略的有效性，并讨论了统计方法和深度学习技术之间的对比。

    

    手动评分编程作业因复杂性和主观性而具有挑战性。然而，使用深度学习进行自动评分简化了任务。它客观地评估代码质量，检测错误，并准确地分配分数，减轻了教师的负担，同时确保了高效和公平的评估。本研究使用回归、卷积神经网络（CNN）和长短期记忆（LSTM）等机器学习和深度学习方法对C编程作业进行自动评分的分析。使用一种基于代码的转换器词嵌入模型CodeBERT，将文本代码输入转换为向量，然后将向量输入到几个模型中。测试结果证明了建议策略的有效性，均方根误差（RMSE）为1.89。本研究还讨论了统计方法和深度学习技术之间的对比。

    Grading coding assignments manually is challenging due to complexity and subjectivity. However, auto-grading with deep learning simplifies the task. It objectively assesses code quality, detects errors, and assigns marks accurately, reducing the burden on instructors while ensuring efficient and fair assessment. This study provides an analysis of auto-grading of the C programming assignments using machine learning and deep learning approaches like regression, convolutional neural networks (CNN) and long short-term memory (LSTM). Using a code-based transformer word embedding model called CodeBERT, the textual code inputs were transformed into vectors, and the vectors were then fed into several models. The testing findings demonstrated the efficacy of the suggested strategy with a root mean squared error (RMSE) of 1.89. The contrast between statistical methods and deep learning techniques is discussed in the study.
    
[^86]: WFTNet：利用全局和局部周期性在长期时间序列预测中的应用

    WFTNet: Exploiting Global and Local Periodicity in Long-term Time Series Forecasting. (arXiv:2309.11319v1 [cs.LG])

    [http://arxiv.org/abs/2309.11319](http://arxiv.org/abs/2309.11319)

    本文提出了一种利用全局和局部周期性的Wavelet-Fourier变换网络（WFTNet）用于长期时间序列预测。在利用傅里叶和小波变换提取时频信息的基础上，引入了周期重要性加权系数（PWC）来平衡全局和局部频率模式的重要性。大量实验证明，WFTNet在各种时间序列数据集上优于其他基线模型。

    

    最近的卷积神经网络和Transformer模型尝试利用频率和周期性信息进行长期时间序列预测。然而，大多数现有的工作都基于傅里叶变换，无法捕捉到细粒度和局部频率结构。在本文中，我们提出了一种Wavelet-Fourier Transform Network（WFTNet）用于长期时间序列预测。WFTNet利用傅里叶变换和小波变换从信号中提取全面的时频信息，其中傅里叶变换捕捉全局周期模式，而小波变换捕捉局部周期模式。此外，我们引入了一种周期重要性加权系数（PWC），以自适应地平衡全局和局部频率模式的重要性。在各种时间序列数据集上进行的大量实验证明，WFTNet始终优于其他最先进的基线模型。

    Recent CNN and Transformer-based models tried to utilize frequency and periodicity information for long-term time series forecasting. However, most existing work is based on Fourier transform, which cannot capture fine-grained and local frequency structure. In this paper, we propose a Wavelet-Fourier Transform Network (WFTNet) for long-term time series forecasting. WFTNet utilizes both Fourier and wavelet transforms to extract comprehensive temporal-frequency information from the signal, where Fourier transform captures the global periodic patterns and wavelet transform captures the local ones. Furthermore, we introduce a Periodicity-Weighted Coefficient (PWC) to adaptively balance the importance of global and local frequency patterns. Extensive experiments on various time series datasets show that WFTNet consistently outperforms other state-of-the-art baseline.
    
[^87]: 让声音存在：从无声视频中重建高质量语音

    Let There Be Sound: Reconstructing High Quality Speech from Silent Videos. (arXiv:2308.15256v1 [eess.AS])

    [http://arxiv.org/abs/2308.15256](http://arxiv.org/abs/2308.15256)

    本文介绍了一个重建高质量语音的唇语转语音系统，通过解决一对多映射问题和细节精炼来显著改进生成质量。

    

    本研究的目标是仅通过唇运动重建高质量的语音，也被称为唇语转语音。唇语转语音系统的一个关键挑战是由于同形异音和多样化语音变化而造成的一对多映射，导致发音错误和过度平滑的语音。在本文中，我们提出了一种新颖的唇语转语音系统，通过从多个角度缓解一对多映射问题，显著改进了生成质量。具体来说，我们结合了（1）自我监督的语音表示来消除同形异音，和（2）声学变异信息来建模多样化的语音风格。此外，为了更好地解决上述问题，我们采用了基于流的后处理网络，捕捉和精炼所生成语音的细节。我们进行了大量实验，并证明我们的方法实现了接近真实人类语音的生成质量，超过了现有方法。

    The goal of this work is to reconstruct high quality speech from lip motions alone, a task also known as lip-to-speech. A key challenge of lip-to-speech systems is the one-to-many mapping caused by (1) the existence of homophenes and (2) multiple speech variations, resulting in a mispronounced and over-smoothed speech. In this paper, we propose a novel lip-to-speech system that significantly improves the generation quality by alleviating the one-to-many mapping problem from multiple perspectives. Specifically, we incorporate (1) self-supervised speech representations to disambiguate homophenes, and (2) acoustic variance information to model diverse speech styles. Additionally, to better solve the aforementioned problem, we employ a flow based post-net which captures and refines the details of the generated speech. We perform extensive experiments and demonstrate that our method achieves the generation quality close to that of real human utterance, outperforming existing methods in term
    
[^88]: 不仅仅奖励，还有约束：用于腿式机器人运动的应用

    Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion. (arXiv:2308.12517v1 [cs.RO])

    [http://arxiv.org/abs/2308.12517](http://arxiv.org/abs/2308.12517)

    本文提出了一种新的强化学习框架，为复杂机器人系统训练神经网络控制器。该框架引入了奖励和约束的概念，通过设计高效的策略优化算法来处理约束，以减少计算开销。通过应用于不同腿式机器人的运动控制器训练中，展示了该框架的有效性。

    

    早期的一些研究通过设计神经网络控制器并使用无模型强化学习来训练，展示了复杂机器人系统中令人印象深刻的控制性能。然而，这些具有自然动作风格和高任务性能的出色控制器是通过进行大量奖励工程而开发的，该过程非常费时费力，需要设计大量奖励项并确定合适的奖励系数。在这项工作中，我们提出了一种新的强化学习框架，用于训练同时包含奖励和约束的神经网络控制器。为了让工程师能够适当地反映他们对约束的意图并以最小的计算开销处理它们，我们提出了两种约束类型和一种高效的策略优化算法。该学习框架被应用于训练不同形态和物理属性的几个腿式机器人的运动控制器。

    Several earlier studies have shown impressive control performance in complex robotic systems by designing the controller using a neural network and training it with model-free reinforcement learning. However, these outstanding controllers with natural motion style and high task performance are developed through extensive reward engineering, which is a highly laborious and time-consuming process of designing numerous reward terms and determining suitable reward coefficients. In this work, we propose a novel reinforcement learning framework for training neural network controllers for complex robotic systems consisting of both rewards and constraints. To let the engineers appropriately reflect their intent to constraints and handle them with minimal computation overhead, two constraint types and an efficient policy optimization algorithm are suggested. The learning framework is applied to train locomotion controllers for several legged robots with different morphology and physical attribu
    
[^89]: GIT-Mol：一种多模态大型语言模型用于分子科学中的图像，图形和文本

    GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text. (arXiv:2308.06911v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.06911](http://arxiv.org/abs/2308.06911)

    GIT-Mol是一种多模态大型语言模型，可在分子科学中处理图像、图形和文本信息。通过新提出的GIT-Former架构，该模型能够将多种模态的数据对齐到一个统一的潜在空间中。与基线相比，GIT-Mol在性质预测和分子生成有效性方面取得了显著改进。此外，该模型还可用于化合物名称识别和化学反应预测等下游任务。

    

    大型语言模型在自然语言处理方面取得了重要进展，通过处理分子的文本表示，为分子科学中的创新应用提供了可能。然而，大多数现有的语言模型无法捕捉具有复杂分子结构或图像的丰富信息。在本文中，我们引入了GIT-Mol，一种集成了图形、图像和文本信息的多模态大型语言模型。为了促进多模态分子数据的集成，我们提出了GIT-Former，一种新颖的架构，能够将所有模态对齐到统一的潜在空间中。与基线相比，我们在性质预测方面实现了5%-10%的准确性提高，并在分子生成有效性方面提高了20.2%。通过任意到语言的分子翻译策略，我们的模型有潜力进行更多的下游任务，例如化合物名称识别和化学反应预测。

    Large language models have made significant strides in natural language processing, enabling innovative applications in molecular science by processing textual representations of molecules. However, most existing language models cannot capture the rich information with complex molecular structures or images. In this paper, we introduce GIT-Mol, a multi-modal large language model that integrates the Graph, Image, and Text information. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture that is capable of aligning all modalities into a unified latent space. We achieve a 5%-10% accuracy increase in properties prediction and a 20.2% boost in molecule generation validity compared to the baselines. With the any-to-language molecular translation strategy, our model has the potential to perform more downstream tasks, such as compound name recognition and chemical reaction prediction.
    
[^90]: 快速NeRF合成和渲染的通用隐式框架

    A General Implicit Framework for Fast NeRF Composition and Rendering. (arXiv:2308.04669v1 [cs.CV])

    [http://arxiv.org/abs/2308.04669](http://arxiv.org/abs/2308.04669)

    本研究提出了一个通用的隐式框架，可以快速合成和渲染NeRF对象。通过引入神经深度场的新的表面表示方法，可以实现动态阴影并允许多个NeRF对象以任意刚体变换无缝地放置和渲染在一起。

    

    最近，各种神经辐射场方法在高渲染速度方面取得了显著成功。然而，当前的加速方法专门化并且不适用于各种隐式方法，这阻碍了对不同类型的NeRF作品进行实时合成。由于NeRF依赖于沿光线采样，因此可以提供一般性的指导。我们提出了一个通用的隐式流水线来快速合成NeRF对象。这种新方法使得动态阴影可以使用解析光源在对象内部或对象之间进行投射，同时允许多个NeRF对象以任意刚体变换无缝地放置和渲染在一起。主要地，我们的工作引入了一种称为神经深度场（NeDF）的新的表面表示方法，它通过允许光线和隐式表面之间的直接相交计算来快速确定对象之间的空间关系。它利用一个交点神经网络来加速查询NeRF。

    Recently, a variety of Neural radiance fields methods have garnered remarkable success in high render speed. However, current accelerating methods is specialized and not compatible for various implicit method, which prevent a real-time composition over different kinds of NeRF works. Since NeRF relies on sampling along rays, it's possible to provide a guidance generally. We propose a general implicit pipeline to rapidly compose NeRF objects. This new method enables the casting of dynamic shadows within or between objects using analytical light sources while allowing multiple NeRF objects to be seamlessly placed and rendered together with any arbitrary rigid transformations. Mainly, our work introduces a new surface representation known as Neural Depth Fields (NeDF) that quickly determines the spatial relationship between objects by allowing direct intersection computation between rays and implicit surfaces. It leverages an intersection neural network to query NeRF for acceleration inste
    
[^91]: 学习生成用于鲁棒语义分割的训练数据集

    Learning to Generate Training Datasets for Robust Semantic Segmentation. (arXiv:2308.02535v1 [cs.CV])

    [http://arxiv.org/abs/2308.02535](http://arxiv.org/abs/2308.02535)

    本文提出了一种新的方法，通过生成真实和可信的扰动或异常图像来提高语义分割技术的鲁棒性。通过设计和训练Robusta，一种鲁棒的条件生成对抗网络，可以为训练可靠的分割模型提供可用的数据集，从而显著增强语义分割技术在面对现实世界的扰动和分布变化时的鲁棒性。

    

    近年来，语义分割技术取得了显著进展，但是它们对现实世界的扰动和训练过程中未见过的数据样本的鲁棒性仍然是一个挑战，尤其是在安全关键的应用中。本文提出了一种新的方法，通过利用标签到图像生成器和图像到标签分割模型之间的协同作用来提高语义分割技术的鲁棒性。具体来说，我们设计并训练了一个新的鲁棒的条件生成对抗网络Robusta，用于生成真实和可信的扰动或异常图像，这些图像可以用来训练可靠的分割模型。我们对所提出的生成模型进行了深入研究，评估了下游分割网络的性能和鲁棒性，并证明我们的方法可以显著提高语义分割技术在面对现实世界的扰动、分布变化和超出分布的情况下的鲁棒性。

    Semantic segmentation techniques have shown significant progress in recent years, but their robustness to real-world perturbations and data samples not seen during training remains a challenge, particularly in safety-critical applications. In this paper, we propose a novel approach to improve the robustness of semantic segmentation techniques by leveraging the synergy between label-to-image generators and image-to-label segmentation models. Specifically, we design and train Robusta, a novel robust conditional generative adversarial network to generate realistic and plausible perturbed or outlier images that can be used to train reliable segmentation models. We conduct in-depth studies of the proposed generative model, assess the performance and robustness of the downstream segmentation network, and demonstrate that our approach can significantly enhance the robustness of semantic segmentation techniques in the face of real-world perturbations, distribution shifts, and out-of-distributi
    
[^92]: 可证明强大的有向多图神经网络

    Provably Powerful Graph Neural Networks for Directed Multigraphs. (arXiv:2306.11586v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11586](http://arxiv.org/abs/2306.11586)

    本文分析了一组简单的改进方法，将标准的消息传递图神经网络（GNN）转化为可证明强大的有向多图神经网络，能够检测任何有向子图模式。实验结果展示了这些改进方法在合成子图检测任务和金融犯罪分析任务上的出色性能。

    

    本文分析了一组简单的改进方法，将标准的消息传递图神经网络（GNN）转化为可证明强大的有向多图神经网络。改进方法包括多图端口编号、个体ID和反向消息传递。我们证明这些方法的组合在理论上能够检测任何有向子图模式。为了验证我们提出的改进方法在实践中的有效性，我们在合成子图检测任务上进行了实验，结果表明其具有出色的性能，几乎可以得到完美的结果。此外，我们将提出的改进方法应用于两个金融犯罪分析任务。我们观察到在检测洗钱交易方面有显著的改善，将标准的消息传递GNN的少数类F1分数提高了高达30%，并且与基于树和GNN的基准相媲美或超越。在一个实际的网络钓鱼检测数据集上也观察到了类似令人印象深刻的结果，提升了三个标准方法的性能。

    This paper analyses a set of simple adaptations that transform standard message-passing Graph Neural Networks (GNN) into provably powerful directed multigraph neural networks. The adaptations include multigraph port numbering, ego IDs, and reverse message passing. We prove that the combination of these theoretically enables the detection of any directed subgraph pattern. To validate the effectiveness of our proposed adaptations in practice, we conduct experiments on synthetic subgraph detection tasks, which demonstrate outstanding performance with almost perfect results. Moreover, we apply our proposed adaptations to two financial crime analysis tasks. We observe dramatic improvements in detecting money laundering transactions, improving the minority-class F1 score of a standard message-passing GNN by up to 30%, and closely matching or outperforming tree-based and GNN baselines. Similarly impressive results are observed on a real-world phishing detection dataset, boosting three standar
    
[^93]: 为大型图表示简化和增强Transformer

    Simplifying and Empowering Transformers for Large-Graph Representations. (arXiv:2306.10759v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10759](http://arxiv.org/abs/2306.10759)

    本文通过实验证明，在大型图上使用一层注意力即可获得令人惊讶的竞争性能，挑战了在语言和视觉任务中复杂模型的应用。这促使我们重新思考在大型图上设计Transformer的理念，以提高可扩展性。

    

    在大型图上学习表示是一个长期存在的挑战，因为其中涉及了大量数据点之间的相互依赖关系。Transformer作为一种新兴的用于图结构数据的基本编码器类别，由于其全局注意力可以捕捉到邻节点之外的所有对影响，因此在小型图上表现出了有希望的性能。尽管如此，现有方法往往继承了Transformer在语言和视觉任务中的思想，并通过堆叠深层多头注意力来采用复杂的模型。本文通过关于节点属性预测基准的实验证明，即使只使用一层注意力也能在节点数量从千级到十亿级的范围内带来令人惊讶的竞争性能。这鼓励我们重新思考在大型图上设计Transformer的理念，其中全局注意力是一个阻碍可扩展性的计算开销。我们将提出的方案称为简化图Transformer。

    Learning representations on large-sized graphs is a long-standing challenge due to the inter-dependence nature involved in massive data points. Transformers, as an emerging class of foundation encoders for graph-structured data, have shown promising performance on small graphs due to its global attention capable of capturing all-pair influence beyond neighboring nodes. Even so, existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated models by stacking deep multi-head attentions. In this paper, we critically demonstrate that even using a one-layer attention can bring up surprisingly competitive performance across node property prediction benchmarks where node numbers range from thousand-level to billion-level. This encourages us to rethink the design philosophy for Transformers on large graphs, where the global attention is a computation overhead hindering the scalability. We frame the proposed scheme as Simplified Graph Trans
    
[^94]: 自适应基于梯度的异常值去除的嘈杂标签学习方法

    Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal. (arXiv:2306.04502v1 [cs.LG])

    [http://arxiv.org/abs/2306.04502](http://arxiv.org/abs/2306.04502)

    本文提出了一种名为AGRA的自适应梯度异常值去除方法，能够在模型训练过程中动态调整数据集从而有效提高模型学习效果。

    

    训练可靠和高性能模型需要准确和丰富的数据集，但即便是人工标注的数据集也会包含错误，更不用说自动标注的数据集了。现有的一些数据去噪方法主要集中于检测异常值并进行永久性去除，但这种方法很容易过度或者欠度过滤数据集。在本论文中，我们提出了一种新的自适应梯度异常值去除方法（AGRA），不同于在模型训练之前清洗数据集，我们的方法在训练过程中动态调整数据集。通过比较一组样本的累积梯度和单个样本的梯度，我们的方法可以决定是否在当前更新时保留对应的样本，以此来确定它是否有助于模型的学习效果。在多个数据集上进行的广泛评估表明，AGRA方法的有效性，并且全面的结果分析证实了我们方法的理论和实践收益。

    An accurate and substantial dataset is necessary to train a reliable and well-performing model. However, even manually labeled datasets contain errors, not to mention automatically labeled ones. The problem of data denoising was addressed in different existing research, most of which focuses on the detection of outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates the AGRA effectiveness, while comprehensive results analysis sup
    
[^95]: 在符合性预测中量化深度学习模型的不确定性

    Quantifying Deep Learning Model Uncertainty in Conformal Prediction. (arXiv:2306.00876v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.00876](http://arxiv.org/abs/2306.00876)

    本文针对深度学习模型的不确定性进行了量化，采用了符合性预测框架来计算模型的置信水平，并与其他不确定性量化方法进行了比较。

    

    在机器学习和统计建模中，精确估计深度神经网络的预测不确定性对于可靠的决策至关重要，尤其在医疗人工智能的背景下。符合性预测（CP）已经成为一个有希望的框架，通过提供良好校准的置信水平来表示模型的不确定性以进行单个预测。然而，对于在符合性预测中量化模型不确定性的研究仍然是一个活跃的领域，还没有完全解决。在本文中，我们探讨了最先进的符合性预测方法及其理论基础。我们提出了一个概率方法来量化符合性预测中产生的预测集的模型不确定性，并为计算得到的不确定性提供了认证边界。通过这样做，我们可以将通过CP测量的模型不确定性与其他不确定性量化方法（如贝叶斯方法、MC-Dropout和DeepEnsemble）进行比较.

    Precise estimation of predictive uncertainty in deep neural networks is a critical requirement for reliable decision-making in machine learning and statistical modeling, particularly in the context of medical AI. Conformal Prediction (CP) has emerged as a promising framework for representing the model uncertainty by providing well-calibrated confidence levels for individual predictions. However, the quantification of model uncertainty in conformal prediction remains an active research area, yet to be fully addressed. In this paper, we explore state-of-the-art CP methodologies and their theoretical foundations. We propose a probabilistic approach in quantifying the model uncertainty derived from the produced prediction sets in conformal prediction and provide certified boundaries for the computed uncertainty. By doing so, we allow model uncertainty measured by CP to be compared by other uncertainty quantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and Evidentia
    
[^96]: 《大脑肿瘤分割（BraTS）挑战2023：关注儿科（CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs）》

    The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs). (arXiv:2305.17033v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.17033](http://arxiv.org/abs/2305.17033)

    这个论文介绍了CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023挑战，该挑战是首个专注于儿童脑肿瘤的BraTS挑战，旨在评估儿童脑胶质瘤的体积分割算法的发展。儿童中枢神经系统肿瘤是儿童癌症相关死亡的主要原因，并且对这些实体的诊断和治疗存在一些挑战。

    

    儿童中枢神经系统肿瘤是儿童癌症相关死亡的最常见原因。儿童高级别胶质瘤的五年生存率不到20％。由于罕见，对这些实体的诊断通常会延迟，其治疗主要基于历史治疗理念，并且临床试验需要多机构合作。MICCAI大脑肿瘤分割（BraTS）挑战是一个里程碑式的社区基准事件，已经成功创建资源12年，用于成人胶质瘤的分割和分析。在这里，我们介绍了CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023挑战，这是首个专注于儿童脑肿瘤的BraTS挑战，其中包括多个国际合作组织专注于儿科神经肿瘤和临床试验的数据。BraTS-PEDs 2023挑战侧重于评估用于儿童脑胶质瘤的体积分割算法的发展。

    Pediatric tumors of the central nervous system are the most common cause of cancer-related death in children. The five-year survival rate for high-grade gliomas in children is less than 20\%. Due to their rarity, the diagnosis of these entities is often delayed, their treatment is mainly based on historic treatment concepts, and clinical trials require multi-institutional collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a landmark community benchmark event with a successful history of 12 years of resource creation for the segmentation and analysis of adult glioma. Here we present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which represents the first BraTS challenge focused on pediatric brain tumors with data acquired across multiple international consortia dedicated to pediatric neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on benchmarking the development of volumentric segmentation algorithms for pediatric brain gli
    
[^97]: 一种适用于风险概率估计的可推广、物理学基础学习框架

    A Generalizable Physics-informed Learning Framework for Risk Probability Estimation. (arXiv:2305.06432v1 [eess.SY])

    [http://arxiv.org/abs/2305.06432](http://arxiv.org/abs/2305.06432)

    本文提出了一种基于物理学的学习框架，通过将MC方法与基于物理学的神经网络相结合，有效评估长期风险概率及其梯度。数值结果表明，该方法具有更好的样本效率，能够适应系统变化。

    

    准确评估长期风险概率及其梯度对于许多随机安全控制方法至关重要。然而，在实时和未知或变化的环境中计算这些风险概率是具有挑战性的。在本文中，我们开发了一种有效的方法来评估长期风险概率及其梯度。所提出的方法利用了长期风险概率满足某些偏微分方程(PDEs)的事实，该方程表征了概率之间的邻近关系，以将MC方法和基于物理学的神经网络相结合。我们提供了在特定训练配置下给出估计误差的理论保证。数值结果表明，所提出的方法具有更好的样本效率，能够很好地推广到未知区域，并能够适应系统变化，相比MC方法和现有的数据驱动方法，它表现出更好的性能。

    Accurate estimates of long-term risk probabilities and their gradients are critical for many stochastic safe control methods. However, computing such risk probabilities in real-time and in unseen or changing environments is challenging. Monte Carlo (MC) methods cannot accurately evaluate the probabilities and their gradients as an infinitesimal devisor can amplify the sampling noise. In this paper, we develop an efficient method to evaluate the probabilities of long-term risk and their gradients. The proposed method exploits the fact that long-term risk probability satisfies certain partial differential equations (PDEs), which characterize the neighboring relations between the probabilities, to integrate MC methods and physics-informed neural networks. We provide theoretical guarantees of the estimation error given certain choices of training configurations. Numerical results show the proposed method has better sample efficiency, generalizes well to unseen regions, and can adapt to sys
    
[^98]: STAS: 多智能体强化学习的时空回报分解

    STAS: Spatial-Temporal Return Decomposition for Multi-agent Reinforcement Learning. (arXiv:2304.07520v1 [cs.AI])

    [http://arxiv.org/abs/2304.07520](http://arxiv.org/abs/2304.07520)

    提出了一种名为STAS的新方法，用于多智能体强化学习中时空回报分解，可以对代理进行信用分配。该方法引入了Shapley值和空间-时间注意机制来解决先前方法中延迟全局回报的复杂关系问题。在各种基准环境下，该方法表现良好。

    

    集中式训练和分散式执行（CTDE）已被证明是合作多智能体强化学习（MARL）中有效的范例。其中一个主要的挑战是赋信用值，即通过代理的贡献来给代理赋信用值。先前的研究集中于隐式地分解联合价值函数或显式地计算所有代理的支付分配。然而，在只有在周期性强化学习设置中，全局奖励只能在周期结束时显示。现有的方法通常不起作用。它们缺乏对延迟全局奖励在时间维度中复杂关系的建模功能，并且受偏差和方差的影响较大。我们提出了一种名为空间时间关注与 Shapley（STAS）的新方法，用于回报分解；STAS 在时间和空间维度上学习信用分配。它首先将全局回报分解回到每个时间步，然后使用Shapley值来评估协作MARL中每个代理的贡献。 STAS 还引入了一种空间 - 时间关注机制，以捕获延迟全局奖励的复杂关系。我们的实验表明，在各种基准环境中，STAS 能够胜过最先进的方法。

    Centralized Training with Decentralized Execution (CTDE) has been proven to be an effective paradigm in cooperative multi-agent reinforcement learning (MARL). One of the major challenges is yet credit assignment, which aims to credit agents by their contributions. Prior studies focus on either implicitly decomposing the joint value function or explicitly computing the payoff distribution of all agents. However, in episodic reinforcement learning settings where global rewards can only be revealed at the end of the episode, existing methods usually fail to work. They lack the functionality of modeling complicated relations of the delayed global reward in the temporal dimension and suffer from large variance and bias. We propose a novel method named Spatial-Temporal Attention with Shapley (STAS) for return decomposition; STAS learns credit assignment in both the temporal and the spatial dimension. It first decomposes the global return back to each time step, then utilizes Shapley Value to
    
[^99]: 模型稀疏化可以简化机器反学习

    Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])

    [http://arxiv.org/abs/2304.04934](http://arxiv.org/abs/2304.04934)

    本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。

    

    最近的数据管制要求机器反学习（MU）：从模型中移除指定样例的影响。虽然可以通过使用剩余数据从头开始进行模型重新训练来进行精确反学习，但是其计算成本导致了近似但高效的反学习方案的开发。除了数据中心的MU解决方案，我们通过一种新颖的基于模型的视角推进MU：通过权值修剪进行稀疏化。我们的理论和实践结果表明，模型稀疏性可以提高近似反学习器的多标准反学习性能，缩小近似间隙，同时保持高效。有了这个认识，我们制定了两个新的稀疏感知反学习元方案，称为“先修剪，然后反学习”和“稀疏感知反学习”。广泛的实验表明，我们的发现和提议在各种场景下始终有益于MU，包括按类数据擦除、随机数据擦除和后门数据伪造等。

    Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
    
[^100]: 关于神经网络模型压缩的框架、算法和收敛保证

    On Model Compression for Neural Networks: Framework, Algorithm, and Convergence Guarantee. (arXiv:2303.06815v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06815](http://arxiv.org/abs/2303.06815)

    本文提出了一个框架和算法，从非凸优化的角度来进行神经网络模型压缩。算法解决了梯度消失/爆炸问题，并保证了收敛性。

    

    模型压缩对于部署神经网络（NN）至关重要，特别是在许多应用程序中计算设备的内存和存储有限的情况下。本文关注两种神经网络模型压缩技术：低秩逼近和权重裁剪，这些技术目前非常流行。然而，使用低秩逼近和权重裁剪训练神经网络总是会遭受显著的准确性损失和收敛问题。本文提出了一个全面的框架，从非凸优化的新视角设计了适当的目标函数来进行模型压缩。然后，我们引入了一种块坐标下降（BCD）算法NN-BCD来解决非凸优化问题。我们算法的一个优点是可以获得具有闭式形式的高效迭代方案，从而避免了梯度消失/爆炸的问题。此外，我们的算法利用了Kurdyka-{\L}ojasiewicz (K{\L})性质，保证了算法的收敛性。

    Model compression is a crucial part of deploying neural networks (NNs), especially when the memory and storage of computing devices are limited in many applications. This paper focuses on two model compression techniques: low-rank approximation and weight pruning in neural networks, which are very popular nowadays. However, training NN with low-rank approximation and weight pruning always suffers significant accuracy loss and convergence issues. In this paper, a holistic framework is proposed for model compression from a novel perspective of nonconvex optimization by designing an appropriate objective function. Then, we introduce NN-BCD, a block coordinate descent (BCD) algorithm to solve the nonconvex optimization. One advantage of our algorithm is that an efficient iteration scheme can be derived with closed-form, which is gradient-free. Therefore, our algorithm will not suffer from vanishing/exploding gradient problems. Furthermore, with the Kurdyka-{\L}ojasiewicz (K{\L}) property o
    
[^101]: SemEval-2023任务11的Lon-ea：软硬标签预测中激活函数的比较。

    Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction. (arXiv:2303.02468v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.02468](http://arxiv.org/abs/2303.02468)

    本研究研究了在软硬标签预测中，不同激活函数对于深度神经网络模型输出的影响，并引入了一种新的正弦激活函数。

    

    我们研究在学习不同意任务的软硬标签预测中，深度神经网络模型输出层中不同激活函数的影响。在该任务中，目标是通过预测软标签来量化不同意量。为了预测软标签，我们使用基于BERT的预处理器和编码器，并改变输出层中使用的激活函数，同时保持其他参数不变。然后将软标签用于硬标签预测。考虑的激活函数包括sigmoid函数以及添加到模型中的阶跃函数和本文中首次介绍的正弦激活函数。

    We study the influence of different activation functions in the output layer of deep neural network models for soft and hard label prediction in the learning with disagreement task. In this task, the goal is to quantify the amount of disagreement via predicting soft labels. To predict the soft labels, we use BERT-based preprocessors and encoders and vary the activation function used in the output layer, while keeping other parameters constant. The soft labels are then used for the hard label prediction. The activation functions considered are sigmoid as well as a step-function that is added to the model post-training and a sinusoidal activation function, which is introduced for the first time in this paper.
    
[^102]: 通过内容感知的风格不变模型学习对未知领域进行泛化：用于胸部X射线疾病检测的翻译摘要

    Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays. (arXiv:2302.13991v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.13991](http://arxiv.org/abs/2302.13991)

    通过内容感知的风格不变模型，我们提出了一种解决深度学习医学图像分析中源领域不匹配挑战的方法。我们采用了风格随机化模块来提取既是风格不变又是内容偏好的领域不变特征，在胸部X射线疾病检测中取得了良好的性能。

    

    在基于深度学习的医学图像分析中，由于源领域不匹配而导致性能降低一直是一个长期存在的挑战，特别是在胸部X射线（CXR）领域。为了解决这种领域转移问题，已经提出了一些方法（如对抗训练，多领域混合），用于提取领域不变的高级特征。然而，这些方法并没有明确规范提取的领域不变特征的内容和风格特征。最近的研究表明，CNN模型对风格（例如，无信息的纹理）有很强的偏好，而不是对内容（例如，形状）的偏好，这与人类视觉系统形成鲜明对比。放射科医师倾向于从CXR图像中学习视觉线索，并因此在多个领域中表现良好。因此，在从CXR图像进行病理诊断的医学成像中，模型应该提取既是风格不变又是内容偏好的领域不变特征。受此启发，我们在实验中使用了新颖的风格随机化模块（SRMs）。

    Performance degradation due to source domain mismatch is a longstanding challenge in deep learning-based medical image analysis, particularly for chest X-rays (CXRs). Several methods (e.g., adversarial training, multi-domain mixups) have been proposed to extract domain-invariant high-level features to address this domain shift. However, these methods do not explicitly regularize the content and style characteristics of the extracted domain-invariant features. Recent studies have demonstrated that CNN models exhibit a strong bias toward styles (e.g., uninformative textures) rather than content (e.g., shape), in stark contrast to the human-vision system. Radiologists tend to learn visual cues from CXRs and thus perform well across multiple domains. Therefore, in medical imaging for pathology diagnosis from CXR images, models should extract domain-invariant features that are style-invariant and content-biased. Motivated by this, we employ the novel style randomization modules (SRMs) at bo
    
[^103]: 使用贝叶斯最后一层改进神经网络的不确定性量化

    Improved uncertainty quantification for neural networks with Bayesian last layer. (arXiv:2302.10975v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10975](http://arxiv.org/abs/2302.10975)

    本文提出了一种改进神经网络不确定性量化的方法，使用贝叶斯最后一层近似不可处理的贝叶斯神经网络，通过最大化边际来获得权重的点估计。

    

    不确定性量化是机器学习中的重要任务，神经网络在这方面通常表现不佳。这对于安全关键应用来说是一个限制，因此常常倾向于使用能够感知不确定性的方法，如高斯过程或贝叶斯线性回归。贝叶斯神经网络是一种解决这个问题的方法，它假设所有参数都服从概率分布，并产生分布预测。然而，训练和推断通常是不可处理的，需要使用近似方法。一种有前景的近似方法是具有贝叶斯最后一层的神经网络。他们仅在最后一个线性层中假设分布权重，并产生一个正态分布的预测。具有贝叶斯最后一层的神经网络可以看作是具有学习非线性特征的贝叶斯线性回归模型。为了近似不可处理的贝叶斯神经网络，应通过最大化边际来获得除最后一层以外的所有分布权重的点估计。

    Uncertainty quantification is an essential task in machine learning - a task in which neural networks (NNs) have traditionally not excelled. This can be a limitation for safety-critical applications, where uncertainty-aware methods like Gaussian processes or Bayesian linear regression are often preferred. Bayesian neural networks are an approach to address this limitation. They assume probability distributions for all parameters and yield distributed predictions. However, training and inference are typically intractable and approximations must be employed. A promising approximation is NNs with Bayesian last layer (BLL). They assume distributed weights only in the last linear layer and yield a normally distributed prediction. NNs with BLL can be seen as a Bayesian linear regression model with learned nonlinear features. To approximate the intractable Bayesian neural network, point estimates of the distributed weights in all but the last layer should be obtained by maximizing the margina
    
[^104]: 对抗机器学习中的攻击：从生命周期角度的系统性调查

    Attacks in Adversarial Machine Learning: A Systematic Survey from the Life-cycle Perspective. (arXiv:2302.09457v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09457](http://arxiv.org/abs/2302.09457)

    本研究通过对抗机器学习中攻击现象的生命周期角度进行系统调查，提供一种统一的视角和数学框架，总结整体进展。

    

    对抗性机器学习（AML）研究机器学习中的对抗现象，这些现象可能与人类的预测不一致或出乎意料。最近已经开发了一些范式来探索发生在机器学习系统不同阶段的对抗现象，例如在预训练、训练和推断阶段发生的后门攻击；在后训练、部署和推断阶段发生的权重攻击；在推断阶段发生的对抗攻击。然而，尽管这些对抗范式有着共同的目标，但它们的发展几乎是独立的，对于AML领域仍然没有完整的整体认知。本文旨在为AML社区提供一个统一的视角，系统地回顾该领域的整体进展。我们首先提供了关于AML的通用定义，然后提出了一个统一的数学框架，以涵盖现有的攻击范式。

    Adversarial machine learning (AML) studies the adversarial phenomenon of machine learning, which may make inconsistent or unexpected predictions with humans. Some paradigms have been recently developed to explore this adversarial phenomenon occurring at different stages of a machine learning system, such as backdoor attack occurring at the pre-training, in-training and inference stage; weight attack occurring at the post-training, deployment and inference stage; adversarial attack occurring at the inference stage. However, although these adversarial paradigms share a common goal, their developments are almost independent, and there is still no big picture of AML. In this work, we aim to provide a unified perspective to the AML community to systematically review the overall progress of this field. We firstly provide a general definition about AML, and then propose a unified mathematical framework to covering existing attack paradigms. According to the proposed unified framework, we buil
    
[^105]: 随机逼近法用于组分布式鲁棒优化

    Stochastic Approximation Approaches to Group Distributionally Robust Optimization. (arXiv:2302.09267v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09267](http://arxiv.org/abs/2302.09267)

    本文提出了一种随机逼近法，用于组分布式鲁棒优化，该算法利用在线学习技术，将每轮所需的样本数从$m$个降至$1$个，同时保持相同的样本复杂度。

    

    本文研究组分布式鲁棒优化（GDRO），目的是学习一个能在$m$个不同分布上表现良好的模型。首先，我们将GDRO建模为随机凸凹鞍点问题，并证明使用$m$个样本的随机镜像下降法(SMD)，能够实现$O(m(\log m)/\epsilon ^2)$个样本的复杂度，以找到一个$\epsilon$-最优解，这与$\Omega(m/\epsilon ^2)$的下界想匹配，除了一个对数因子。接下来，我们利用在线学习技术，将每轮所需的样本数从$m$个降至$1$个，同时保持相同的样本复杂度。具体而言，我们将GDRO构造为一个双人博弈，其中一个玩家简单地执行SMD，另一个执行一种用于非明显多臂老虎机的在线算法。接下来，我们考虑一个更实际的情况，即可以从每个分布中绘制的样本数量不同，并提出一种新的公式。

    This paper investigates group distributionally robust optimization (GDRO), with the purpose to learn a model that performs well over $m$ different distributions. First, we formulate GDRO as a stochastic convex-concave saddle-point problem, and demonstrate that stochastic mirror descent (SMD), using $m$ samples in each iteration, achieves an $O(m (\log m)/\epsilon^2)$ sample complexity for finding an $\epsilon$-optimal solution, which matches the $\Omega(m/\epsilon^2)$ lower bound up to a logarithmic factor. Then, we make use of techniques from online learning to reduce the number of samples required in each round from $m$ to $1$, keeping the same sample complexity. Specifically, we cast GDRO as a two-players game where one player simply performs SMD and the other executes an online algorithm for non-oblivious multi-armed bandits. Next, we consider a more practical scenario where the number of samples that can be drawn from each distribution is different, and propose a novel formulation
    
[^106]: 基于图神经网络的图形概括综述

    A Comprehensive Survey on Graph Summarization with Graph Neural Networks. (arXiv:2302.06114v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06114](http://arxiv.org/abs/2302.06114)

    本文综述了基于图神经网络的深度学习概括技术，在保留图形关键特征的同时，对大规模、高维度和复杂的现代图形数据进行处理的方法，包括图神经网络、图形自编码器等。

    

    随着大规模图形的普及，越来越多的计算挑战暴露出来，需要提取、处理和解释大型图形数据。因此，寻找一种能够总结这些广阔图形的方法，同时保留其关键特征是自然而然的。过去，大多数图形概括技术旨在从统计学角度捕捉图形的最重要部分。然而，今天，现代图形数据的高维度和复杂性使得深度学习技术更加流行。因此，本文介绍了基于图神经网络(GNNs)的深度学习概括技术进展的综述。我们的调查包括对当前最先进方法的回顾，包括循环GNNs、卷积GNNs、图形自编码器和图形注意力网络。同时还讨论了一条新兴的研究方向，即使用图形强化学习来评估和改进图形质量。

    As large-scale graphs become more widespread, more and more computational challenges with extracting, processing, and interpreting large graph data are being exposed. It is therefore natural to search for ways to summarize these expansive graphs while preserving their key characteristics. In the past, most graph summarization techniques sought to capture the most important part of a graph statistically. However, today, the high dimensionality and complexity of modern graph data are making deep learning techniques more popular. Hence, this paper presents a comprehensive survey of progress in deep learning summarization techniques that rely on graph neural networks (GNNs). Our investigation includes a review of the current state-of-the-art approaches, including recurrent GNNs, convolutional GNNs, graph autoencoders, and graph attention networks. A new burgeoning line of research is also discussed where graph reinforcement learning is being used to evaluate and improve the quality of grap
    
[^107]: 在Ricci流下学习离散神经网络

    Learning Discretized Neural Networks under Ricci Flow. (arXiv:2302.03390v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03390](http://arxiv.org/abs/2302.03390)

    本文使用信息几何构造了线性几乎欧几里得流形，通过引入偏微分方程Ricci流，解决了离散神经网络训练中梯度无穷或零的问题。

    

    本文考虑了由低精度权重和激活函数构成的离散神经网络（DNNs）在训练过程中由于非可微分离散函数而遭受无穷或零梯度的问题。本文针对此问题，提出了把 STE近似梯度看作整体偏差的度量扰动，通过对偶理论将其看作黎曼流形上的度量扰动，并在信息几何的基础上为 DNN构造了线性几乎欧几里得（LNE）流形以处理扰动。

    In this paper, we consider Discretized Neural Networks (DNNs) consisting of low-precision weights and activations, which suffer from either infinite or zero gradients due to the non-differentiable discrete function in the training process. In this case, most training-based DNNs employ the standard Straight-Through Estimator (STE) to approximate the gradient w.r.t. discrete values. However, the STE gives rise to the problem of gradient mismatch, due to the perturbations of the approximated gradient. To address this problem, this paper reveals that this mismatch can be viewed as a metric perturbation in a Riemannian manifold through the lens of duality theory. Further, on the basis of the information geometry, we construct the Linearly Nearly Euclidean (LNE) manifold for DNNs as a background to deal with perturbations. By introducing a partial differential equation on metrics, i.e., the Ricci flow, we prove the dynamical stability and convergence of the LNE metric with the $L^2$-norm per
    
[^108]: 计算发现具有最佳刚度-韧度权衡的微结构复合材料

    Computational Discovery of Microstructured Composites with Optimal Stiffness-Toughness Trade-Offs. (arXiv:2302.01078v2 [cond-mat.mtrl-sci] UPDATED)

    [http://arxiv.org/abs/2302.01078](http://arxiv.org/abs/2302.01078)

    该论文介绍了一种结合了物理实验、数值模拟和人工神经网络的可推广管道，用于系统地发现具有最佳刚度-韧度权衡的微结构复合材料。这种方法通过嵌套循环的建议-验证工作流，成功弥合了模拟和现实之间的差距，并以高样本效率发现了刚度和韧度兼具的材料。进一步分析帕累托最优设计，可以自动识别出现有的增加韧性机制。

    

    刚度和韧度之间的冲突是工程材料设计中的一个基本问题。然而，尚未演示使用整个帕累托前沿的数据高效探索来系统地发现具有最佳刚度-韧度权衡的微结构复合材料，这主要是由于模拟与现实之间的差异以及对整个帕累托前沿的数据高效探索的缺乏。我们引入了一种可推广的管道，将物理实验、数值模拟和人工神经网络相结合，以应对这两个挑战。在没有任何预设材料设计专家知识的情况下，我们的方法实施了一个嵌套循环的建议-验证工作流，以弥合模拟和现实之间的差距，并以高样本效率发现了既刚又韧的微结构复合材料。对帕累托最优设计的进一步分析使我们能够自动识别出现有的增加韧性机制，而这些机制以前是通过试验和生物模仿来发现的。

    The conflict between stiffness and toughness is a fundamental problem in engineering materials design. However, the systematic discovery of microstructured composites with optimal stiffness-toughness trade-offs has never been demonstrated, hindered by the discrepancies between simulation and reality and the lack of data-efficient exploration of the entire Pareto front. We introduce a generalizable pipeline that integrates physical experiments, numerical simulations, and artificial neural networks to address both challenges. Without any prescribed expert knowledge of material design, our approach implements a nested-loop proposal-validation workflow to bridge the simulation-to-reality gap and discover microstructured composites that are stiff and tough with high sample efficiency. Further analysis of Pareto-optimal designs allows us to automatically identify existing toughness enhancement mechanisms, which were previously discovered through trial-and-error or biomimicry. On a broader sc
    
[^109]: 不使用边际贡献近似计算Shapley值

    Approximating the Shapley Value without Marginal Contributions. (arXiv:2302.00736v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00736](http://arxiv.org/abs/2302.00736)

    本文提出了两种无参数、领域无关的Shapley值近似算法SVARM和Stratified SVARM，它们基于一种与边际贡献概念脱钩的Shapley值表示。我们证明了它们在近似质量方面具有无与伦比的理论保证，并通过实证结果将其与合成游戏和常见可解释性用例进行了比较。

    

    Shapley值是为合作博弈中的玩家分配有意义的贡献值的最流行方法，最近在可解释的人工智能中得到了广泛应用。Shapley值的有意义性源于仅有Shapley值满足的公理属性，然而，确切计算的代价是随着玩家数量指数级增长。因此，许多研究致力于高效近似Shapley值，其中大部分围绕着玩家的边际贡献的概念。在本文中，我们提出了两种基于与边际贡献概念脱钩的Shapley值表示的无参数、领域无关的近似算法SVARM和Stratified SVARM。我们证明了它们在近似质量方面的无与伦比的理论保证，并提供了包括合成游戏和常用可解释性用例的实证结果进行比较。

    The Shapley value is arguably the most popular approach for assigning a meaningful contribution value to players in a cooperative game, which has recently been used intensively in explainable artificial intelligence. The meaningfulness is due to axiomatic properties that only the Shapley value satisfies, which, however, comes at the expense of an exact computation growing exponentially with the number of agents. Accordingly, a number of works are devoted to the efficient approximation of the Shapley values, most of them revolve around the notion of an agent's marginal contribution. In this paper, we propose with SVARM and Stratified SVARM two parameter-free and domain-independent approximation algorithms based on a representation of the Shapley value detached from the notion of marginal contributions. We prove unmatched theoretical guarantees regarding their approximation quality and provide empirical results including synthetic games as well as common explainability use cases comparin
    
[^110]: SynthMorph实现的考虑解剖结构和无关采集方法的联合配准

    Anatomy-aware and acquisition-agnostic joint registration with SynthMorph. (arXiv:2301.11329v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.11329](http://arxiv.org/abs/2301.11329)

    SynthMorph是一个易于使用的DL工具，用于无需预处理即可直接从MRI扫描仪上对任何脑图像进行联合仿射-可变形配准，采用了从标签图生成具有极大差异图像的策略，实现了更准确和鲁棒的图像配准。

    

    仿射图像配准是医学图像分析的基石。虽然传统算法可以实现优秀的准确性，但它们需要为每一对图像进行耗时的优化。深度学习方法通过学习一个将图像对映射到输出变换的函数来解决这个问题。评估这个函数是快速的，但捕捉大的变换可能是具有挑战性的，而且如果测试图像的特征从训练领域变化，如分辨率，网络往往会出现困难。大多数仿射方法是对解剖结构无知的，意味着如果算法考虑图像中的所有结构，配准会不准确。我们通过SynthMorph解决了这些缺点，它是一个易于使用的DL工具，用于对任何脑图像进行联合仿射-可变形配准，无需预处理即可直接从MRI扫描仪进行操作。首先，我们利用从标签图生成的具有极大差异的图像来训练网络的策略，从而实现对训练过程中未见的多样化采集规范的鲁棒性能。其次，我们优化网络的损失函数，使其能够考虑不同的解剖特征和学习抵制采集特定限制的变换。通过这些创新，我们实现了更准确和鲁棒的图像配准。

    Affine image registration is a cornerstone of medical-image analysis. While classical algorithms can achieve excellent accuracy, they solve a time-consuming optimization for every image pair. Deep-learning (DL) methods learn a function that maps an image pair to an output transform. Evaluating the function is fast, but capturing large transforms can be challenging, and networks tend to struggle if a test-image characteristic shifts from the training domain, such as resolution. Most affine methods are agnostic to anatomy, meaning the registration will be inaccurate if algorithms consider all structures in the image.  We address these shortcomings with SynthMorph, an easy-to-use DL tool for joint affine-deformable registration of any brain image without preprocessing, right off the MRI scanner. First, we leverage a strategy to train networks with wildly varying images synthesized from label maps, yielding robust performance across acquisition specifics unseen at training. Second, we opti
    
[^111]: DeepTaster: 基于对抗扰动的指纹识别技术，用于在深度神经网络中识别专有数据集的使用。

    DeepTaster: Adversarial Perturbation-Based Fingerprinting to Identify Proprietary Dataset Use in Deep Neural Networks. (arXiv:2211.13535v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2211.13535](http://arxiv.org/abs/2211.13535)

    DeepTaster是一种新颖的DNN指纹识别技术，可以有效识别使用受害者数据非法构建嫌疑模型的攻击，即使嫌疑模型的架构与受害模型不同。

    

    训练深度神经网络（DNN）需要大量的数据集和强大的计算资源，这导致一些所有者限制未经许可的重新分发。将保密数据嵌入DNN的水印技术被用于保护所有权，但这些技术可能会降低模型性能，并且易受水印去除攻击的影响。最近，DeepJudge被引入为一种替代方法，用于衡量嫌疑模型与受害模型之间的相似性。虽然DeepJudge在解决水印技术的不足方面表现出了潜力，但它主要应用于嫌疑模型抄袭受害模型架构的情况。在本研究中，我们引入了DeepTaster，一种新颖的DNN指纹识别技术，用于解决使用受害者数据非法构建嫌疑模型的情况。DeepTaster能够有效地识别这种DNN模型盗窃攻击，即使嫌疑模型的架构与受害模型不同。为了实现这一点，DeepTaster生成一个对受害模型具有鲁棒性且可以一致性地识别的独特指纹。

    Training deep neural networks (DNNs) requires large datasets and powerful computing resources, which has led some owners to restrict redistribution without permission. Watermarking techniques that embed confidential data into DNNs have been used to protect ownership, but these can degrade model performance and are vulnerable to watermark removal attacks. Recently, DeepJudge was introduced as an alternative approach to measuring the similarity between a suspect and a victim model. While DeepJudge shows promise in addressing the shortcomings of watermarking, it primarily addresses situations where the suspect model copies the victim's architecture. In this study, we introduce DeepTaster, a novel DNN fingerprinting technique, to address scenarios where a victim's data is unlawfully used to build a suspect model. DeepTaster can effectively identify such DNN model theft attacks, even when the suspect model's architecture deviates from the victim's. To accomplish this, DeepTaster generates a
    
[^112]: 用核斯坦离差控制矩

    Controlling Moments with Kernel Stein Discrepancies. (arXiv:2211.05408v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.05408](http://arxiv.org/abs/2211.05408)

    本研究分析了核斯坦离差（KSD）控制性质，发现标准KSD无法控制矩的收敛，提出了可控制矩和弱收敛的下游扩散KSD，并且发展了可以准确描述$q$-Wasserstein收敛的KSD。

    

    核斯坦离差（KSD）用于衡量分布逼近的质量，并且可以在目标密度具有不可计算的归一化常数时计算。显著的应用包括诊断近似MCMC采样器和非归一化统计模型的适配度检验。本文分析了KSD的收敛控制性质。我们首先证明了用于弱收敛控制的标准KSD无法控制矩的收敛。为了解决这个限制，我们提供了一组充分条件，下游扩散KSD可以同时控制矩和弱收敛。作为一个直接的结果，我们发展了对于每个$q>0$，第一组已知可以准确描述$q$-Wasserstein收敛的KSD。

    Kernel Stein discrepancies (KSDs) measure the quality of a distributional approximation and can be computed even when the target density has an intractable normalizing constant. Notable applications include the diagnosis of approximate MCMC samplers and goodness-of-fit tests for unnormalized statistical models. The present work analyzes the convergence control properties of KSDs. We first show that standard KSDs used for weak convergence control fail to control moment convergence. To address this limitation, we next provide sufficient conditions under which alternative diffusion KSDs control both moment and weak convergence. As an immediate consequence we develop, for each $q > 0$, the first KSDs known to exactly characterize $q$-Wasserstein convergence.
    
[^113]: 使用深度学习的广义二次嵌入来处理非线性动力学

    Generalized Quadratic Embeddings for Nonlinear Dynamics using Deep Learning. (arXiv:2211.00357v2 [math.DS] UPDATED)

    [http://arxiv.org/abs/2211.00357](http://arxiv.org/abs/2211.00357)

    本文提出了一种使用深度学习来处理非线性动力学的方法。该方法通过学习坐标转换，将非线性系统的动力学表示为二次系统，从而简化了非线性系统的建模过程。

    

    工程设计过程通常依赖于能够描述底层动态行为的数学建模。在这项工作中，我们提出了一种数据驱动的方法来对非线性系统的动力学建模。为了简化这个任务，我们的目标是找到一个坐标转换，使我们能够使用一个常见的简单模型结构来表示非线性系统的动力学。常见简单模型的优点在于可以将为其开发的定制设计工具应用于研究各种非线性系统。最简单的常见模型可以认为是线性的，但线性系统往往无法准确捕捉非线性系统的复杂动力学。在这项工作中，我们提出使用二次系统作为常见结构，受到升维原理的启发。根据这一原理，光滑的非线性系统可以在适当的坐标中表示为二次系统，而不会产生近似误差。然而，仅仅通过数值方法找到这些坐标是困难的，所以我们提出使用深度学习来学习这些坐标。

    The engineering design process often relies on mathematical modeling that can describe the underlying dynamic behavior. In this work, we present a data-driven methodology for modeling the dynamics of nonlinear systems. To simplify this task, we aim to identify a coordinate transformation that allows us to represent the dynamics of nonlinear systems using a common, simple model structure. The advantage of a common simple model is that customized design tools developed for it can be applied to study a large variety of nonlinear systems. The simplest common model -- one can think of -- is linear, but linear systems often fall short in accurately capturing the complex dynamics of nonlinear systems. In this work, we propose using quadratic systems as the common structure, inspired by the lifting principle. According to this principle, smooth nonlinear systems can be expressed as quadratic systems in suitable coordinates without approximation errors. However, finding these coordinates solely
    
[^114]: 近似优化和模型选择在领域泛化中的应用：一种基于Mixup引导的解决方案

    Towards Optimization and Model Selection for Domain Generalization: A Mixup-guided Solution. (arXiv:2209.00652v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.00652](http://arxiv.org/abs/2209.00652)

    本文提出了一种基于Mixup引导的优化与选择技术，用于领域泛化，通过生成目标领域指导数据和更接近目标分布的验证数据集，能够提高模型性能。

    

    训练和测试数据之间的分布偏移通常会削弱模型的性能。近年来，许多工作关注于领域泛化 (DG)，其中存在分布偏移且目标数据是未见过的。尽管在算法设计方面取得了进展，但两个基本因素长期以来一直被忽视：1) 面向正则化目标的优化，以及2) 针对DG的模型选择，因为无法利用有关目标领域的知识。在本文中，我们提出了一种基于Mixup引导的优化与选择技术，用于DG。对于优化，我们利用调整后的Mixup生成一个分布与目标领域有指导作用的超出分布的数据集，并使用Pareto优化进行优化。对于模型选择，我们生成一个更接近目标分布的验证数据集，从而可以更好地代表目标数据。我们还提供了一些关于我们提议的理论见解。全面的实验表明我们的方法的有效性。

    The distribution shifts between training and test data typically undermine the performance of models. In recent years, lots of work pays attention to domain generalization (DG) where distribution shifts exist, and target data are unseen. Despite the progress in algorithm design, two foundational factors have long been ignored: 1) the optimization for regularization-based objectives, and 2) the model selection for DG since no knowledge about the target domain can be utilized. In this paper, we propose Mixup guided optimization and selection techniques for DG. For optimization, we utilize an adapted Mixup to generate an out-of-distribution dataset that can guide the preference direction and optimize with Pareto optimization. For model selection, we generate a validation dataset with a closer distance to the target distribution, and thereby it can better represent the target data. We also present some theoretical insights behind our proposals. Comprehensive experiments demonstrate that ou
    
[^115]: ULF: 无监督标签函数校正方法——基于交叉验证的弱监督学习

    ULF: Unsupervised Labeling Function Correction using Cross-Validation for Weak Supervision. (arXiv:2204.06863v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.06863](http://arxiv.org/abs/2204.06863)

    ULF是一种用于弱监督学习的无监督标签函数校正方法，通过基于交叉验证原理的噪声降低技术，有效提高了弱监督学习的性能，无需手动标记。

    

    弱监督学习是一种代替手动标记数据的经济有效方法，该方法通过使用预定义的一组标签函数（LFs）对数据样本进行自动标注。标签函数是基于规则的机制，用于为相关类别生成人工标签。在本研究中，我们探索了基于k折交叉验证原理的弱监督学习噪声降低技术。我们引入了一种名为ULF的新算法，用于无监督标签函数校正，通过利用在所有标签函数之外训练的模型来识别和纠正特定于保留标签函数的偏差，从而对弱监督数据进行去噪。具体来说，ULF通过重新估计高可靠性交叉验证样本上的标签函数分配，来改进标签函数对类别的分配。在多个数据集上的评估证实了ULF在增强弱监督学习方面的有效性，而无需手动标记。

    A cost-effective alternative to manual data labeling is weak supervision (WS), where data samples are automatically annotated using a predefined set of labeling functions (LFs), rule-based mechanisms that generate artificial labels for the associated classes. In this work, we investigate noise reduction techniques for WS based on the principle of k-fold cross-validation. We introduce a new algorithm ULF for Unsupervised Labeling Function correction, which denoises WS data by leveraging models trained on all but some LFs to identify and correct biases specific to the held-out LFs. Specifically, ULF refines the allocation of LFs to classes by re-estimating this assignment on highly reliable cross-validated samples. Evaluation on multiple datasets confirms ULF's effectiveness in enhancing WS learning without the need for manual labeling
    
[^116]: 平均分布优化平滑损失函数

    Federated Optimization of Smooth Loss Functions. (arXiv:2201.01954v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.01954](http://arxiv.org/abs/2201.01954)

    本文研究了在联邦学习框架下的平均分布优化平滑损失函数的问题，提出了联邦低秩梯度下降（FedLRGD）算法来利用数据的平滑性，从而实现更好的优化结果。

    

    在这项工作中，我们研究了在联邦学习框架下的经验风险最小化（ERM），其中一个中央服务器使用存储在$m$个客户端上的训练数据来最小化ERM目标函数。在这种情况下，联邦平均（FedAve）算法是确定ERM问题的$\epsilon$近似解的主要方法。类似于标准优化算法，FedAve的收敛分析仅依赖于优化参数中损失函数的平滑性。然而，损失函数在训练数据中通常也非常平滑。为了利用这种额外的平滑性，我们提出了联邦低秩梯度下降（FedLRGD）算法。由于数据的平滑性在损失函数上引入了近似的低秩结构，我们的方法首先在服务器和客户端之间进行了几轮通信，以学习服务器可以用来近似客户端梯度的权重。然后，我们的方法在服务器上解决ERM问题。

    In this work, we study empirical risk minimization (ERM) within a federated learning framework, where a central server minimizes an ERM objective function using training data that is stored across $m$ clients. In this setting, the Federated Averaging (FedAve) algorithm is the staple for determining $\epsilon$-approximate solutions to the ERM problem. Similar to standard optimization algorithms, the convergence analysis of FedAve only relies on smoothness of the loss function in the optimization parameter. However, loss functions are often very smooth in the training data too. To exploit this additional smoothness, we propose the Federated Low Rank Gradient Descent (FedLRGD) algorithm. Since smoothness in data induces an approximate low rank structure on the loss function, our method first performs a few rounds of communication between the server and clients to learn weights that the server can use to approximate clients' gradients. Then, our method solves the ERM problem at the server 
    
[^117]: 垂直联合学习的数据价值评估：一种无模型且保护隐私的方法

    Data Valuation for Vertical Federated Learning: A Model-free and Privacy-preserving Method. (arXiv:2112.08364v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.08364](http://arxiv.org/abs/2112.08364)

    提出了一种垂直联合学习中的数据价值评估方法FedValue，该方法是隐私保护、针对任务而无需模型的。它包括数据价值度量MShapley-CMI和联合计算方法，有助于解决垂直联合学习中评估数据方数据价值的问题。

    

    垂直联合学习（VFL）是一种有前景的预测分析范式，通过与多个数据提供方（即数据方）的合作，在分散和保护隐私的方式下，赋予一个组织（即任务方）提高其预测模型的能力。尽管对VFL的兴趣不断增长，但缺乏有效且安全的工具来评估数据方拥有的数据价值，制约了VFL在商业环境中的应用。为解决这个问题，我们提出了FedValue，一种针对VFL的保护隐私且针对任务的无模型数据价值评估方法，它由数据价值度量和联合计算方法组成。具体来说，我们首先引入一种新颖的数据价值度量，即MShapley-CMI。该度量评估数据方对预测分析任务的贡献，而无需执行机器学习模型，使其非常适合于VFL的实际应用。接下来，我们开发了一种创新的联合计算方法。

    Vertical Federated learning (VFL) is a promising paradigm for predictive analytics, empowering an organization (i.e., task party) to enhance its predictive models through collaborations with multiple data suppliers (i.e., data parties) in a decentralized and privacy-preserving way. Despite the fast-growing interest in VFL, the lack of effective and secure tools for assessing the value of data owned by data parties hinders the application of VFL in business contexts. In response, we propose FedValue, a privacy-preserving, task-specific but model-free data valuation method for VFL, which consists of a data valuation metric and a federated computation method. Specifically, we first introduce a novel data valuation metric, namely MShapley-CMI. The metric evaluates a data party's contribution to a predictive analytics task without the need of executing a machine learning model, making it well-suited for real-world applications of VFL. Next, we develop an innovative federated computation met
    
[^118]: 代数快捷融合的多态动态规划

    Polymorphic dynamic programming by algebraic shortcut fusion. (arXiv:2107.01752v4 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2107.01752](http://arxiv.org/abs/2107.01752)

    本文提出了基于代数快捷规范与DP算法的多态动态规划方法，可用于解决在半环上表示的所有组合问题，包括优化、逻辑推理、模糊集、可微softmax等问题。

    

    动态规划(DP)是一种广泛适用的算法设计范式，用于高效而确切地解决不可避免的组合问题。然而，这种算法的设计通常以不正式的方式呈现，并且往往难以正确应用。本文提出了一种严格的代数形式主义，可用于从现有的DP算法或简单的功能递归中系统地推导出新的DP算法。这些推导产生的算法是可证明正确和多态的，在任何半环上都能应用于可用半环表示的组合问题的全部范围。这包括：优化、最优概率和维特比译码、概率边缘化、逻辑推理、模糊集、可微softmax以及关系和溯源查询。该方法建立在许多现有文献中关于建设性算法的想法基础之上，基于代数快捷规范与DP算法的融合。我们通过许多实例展示了我们方法的力量和广泛性。

    Dynamic programming (DP) is a broadly applicable algorithmic design paradigm for the efficient, exact solution of otherwise intractable, combinatorial problems. However, the design of such algorithms is often presented informally in an ad-hoc manner, and as a result is often difficult to apply correctly. In this paper, we present a rigorous algebraic formalism for systematically deriving novel DP algorithms, either from existing DP algorithms or from simple functional recurrences. These derivations lead to algorithms which are provably correct and polymorphic over any semiring, which means that they can be applied to the full scope of combinatorial problems expressible in terms of semirings. This includes, for example: optimization, optimal probability and Viterbi decoding, probabilistic marginalization, logical inference, fuzzy sets, differentiable softmax, and relational and provenance queries. The approach, building on many ideas from the existing literature on constructive algorith
    
[^119]: 对联邦学习系统的隐蔽信道攻击

    Covert Channel Attack to Federated Learning Systems. (arXiv:2104.10561v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2104.10561](http://arxiv.org/abs/2104.10561)

    这篇论文介绍了一种针对联邦学习系统的新型攻击模型，通过在联邦训练期间污染全局模型实现隐蔽通信，而不影响模型性能。

    

    联邦学习通过在众多边缘客户端之间分布模型训练，超越了传统的集中式机器学习。这些客户端合作训练一个全局模型，而不会泄露他们的本地私有训练数据。然后，全局模型在所有参与者之间共享，用于本地预测。本文提出了一个新颖的攻击模型，旨在将联邦学习系统转化为隐蔽信道，实现隐秘的通信基础设施。主要思想是，在联邦训练期间，恶意发送者可以通过提交专门构造的样本来污染全局模型。虽然模型污染对其他参与者影响几乎可以忽略不计，也不会改变整体模型性能，但它可以被恶意接收者观察到，并用于传输一个比特位。

    Federated learning (FL) goes beyond traditional, centralized machine learning by distributing model training among a large collection of edge clients. These clients cooperatively train a global, e.g., cloud-hosted, model without disclosing their local, private training data. The global model is then shared among all the participants which use it for local predictions. In this paper, we put forward a novel attacker model aiming at turning FL systems into covert channels to implement a stealth communication infrastructure. The main intuition is that, during federated training, a malicious sender can poison the global model by submitting purposely crafted examples. Although the effect of the model poisoning is negligible to other participants, and does not alter the overall model performance, it can be observed by a malicious receiver and used to transmit a single bit.
    
[^120]: 通过一步式绳索多目标学习处理噪声标签及其在幽门螺杆菌分割中的应用

    Handling Noisy Labels via One-Step Abductive Multi-Target Learning and Its Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.14956](http://arxiv.org/abs/2011.14956)

    本文研究了处理噪声标签的新方法，特别针对医学组织病理学图像分析中的困难情况。通过一步式绳索多目标学习，该方法克服了标签中存在的复杂噪声和评估策略不明确的问题。

    

    由于在许多现实场景中缺乏准确的地面实况标签，因此从噪声标签中学习是一个重要问题。在实践中，针对这个问题的不同方法首先对可能有噪声标签的实例进行一些纠正，然后用纠正信息更新预测模型。然而，在医学组织病理学全切片图像分析（MHWSIA）等特定领域中，专家往往难以或甚至无法手动实现无噪声的地面实况标签，导致标签存在复杂噪声。这种情况引发了两个更加困难的问题：1）由于标签中存在复杂噪声，先前方法纠正可能有噪声标签的实例的方法学存在局限性；2）由于收集无噪声地面实况标签非常困难，验证/测试的适当评估策略不明确。本文重点研究了缓解以上问题的方法。

    Learning from noisy labels is an important concern because of the lack of accurate ground-truth labels in plenty of real-world scenarios. In practice, various approaches for this concern first make some corrections corresponding to potentially noisy-labeled instances, and then update predictive model with information of the made corrections. However, in specific areas, such as medical histopathology whole slide image analysis (MHWSIA), it is often difficult or even impossible for experts to manually achieve the noisy-free ground-truth labels which leads to labels with complex noise. This situation raises two more difficult problems: 1) the methodology of approaches making corrections corresponding to potentially noisy-labeled instances has limitations due to the complex noise existing in labels; and 2) the appropriate evaluation strategy for validation/testing is unclear because of the great difficulty in collecting the noisy-free ground-truth labels. In this paper, we focus on allevia
    

