# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents.](http://arxiv.org/abs/2401.12963) | AutoRT是一个利用现有的基础模型来扩展机器人在未知场景中的部署的系统，通过利用视觉-语言模型和大型语言模型，提出多样化和新颖的指令，并有效地推理自主权和安全性的权衡。 |
| [^2] | [Chatterbox: Robust Transport for LLM Token Streaming under Unstable Network.](http://arxiv.org/abs/2401.12961) | 聊天宝盒（Chatterbox）是针对LLM Chatbots中的令牌流问题提出的一种新颖的传输层方案，通过将新生成的令牌和当前未确认的令牌放入下一个发送的数据包中，实现了稳定的传输和渲染，避免了在不稳定网络环境下的停顿现象。 |
| [^3] | [Bayesian Semi-structured Subspace Inference.](http://arxiv.org/abs/2401.12950) | 本文提出了一种贝叶斯近似方法，通过使用子空间推理来解决半结构回归模型中解释性不确定性的问题，并展示了其在推断输入-输出关系和捕捉多重要素方面的有效性。 |
| [^4] | [Reward-Relevance-Filtered Linear Offline Reinforcement Learning.](http://arxiv.org/abs/2401.12934) | 本文研究了在线决策理论环境中线性函数逼近的离线强化学习，提出了一种基于奖励相关性过滤的方法，将状态-动作值函数的估计限制在稀疏组件上，具有理论保证，并且样本复杂度仅取决于稀疏组件的大小。 |
| [^5] | [pyAKI - An Open Source Solution to Automated KDIGO classification.](http://arxiv.org/abs/2401.12930) | pyAKI是一种自动化的开源解决方案，用于实施KDIGO标准对急性肾损伤进行分类。与专家标注相比，pyAKI展现出了更优质的性能。 |
| [^6] | [DsDm: Model-Aware Dataset Selection with Datamodels.](http://arxiv.org/abs/2401.12926) | 该论文提出了一种模型感知的数据集选择方法，通过将数据集选择视为一个优化问题来解决，并明确地建模了学习过程如何使用训练数据点来预测目标任务。该方法在提高语言模型性能方面表现出色。 |
| [^7] | [Performance Analysis of Support Vector Machine (SVM) on Challenging Datasets for Forest Fire Detection.](http://arxiv.org/abs/2401.12924) | 本文对于使用图像数据集进行森林火灾检测的支持向量机（SVM）进行了性能分析，并研究了关键因素如数据预处理、特征提取和模型训练。这项研究有助于开发高效的森林火灾检测系统。 |
| [^8] | [Deep multitask neural networks for solving some stochastic optimal control problems.](http://arxiv.org/abs/2401.12923) | 本文针对某些难以模拟底层状态变量的随机最优控制问题，引入了使用多任务神经网络的有效解决方案，并通过实验证明了该方法的优越性。 |
| [^9] | [Model-Free $\delta$-Policy Iteration Based on Damped Newton Method for Nonlinear Continuous-Time H$\infty$ Tracking Control.](http://arxiv.org/abs/2401.12882) | 这篇论文提出了一种基于阻尼牛顿方法的无模型$\delta$策略迭代算法，用于解决未知连续时间非线性系统的H∞跟踪控制问题。该算法通过迭代求解广义跟踪Bellman方程，可以寻找出跟踪Hamilton-Jacobi-Isaac (HJI)方程的最优解。提供了基于策略学习和离策略学习的算法，其中离策略学习方法是一个无模型的算法。 |
| [^10] | [Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported Coordination of Mobile Crowdsourcing.](http://arxiv.org/abs/2401.12866) | 移动众包系统中任务分配和质量问题引起了研究者的关注。本文提出了一种基于数据流学习的方法来预测任务结果，并通过任务转移来解决任务分配中的问题。这些机制有助于提高移动众包系统的服务质量。 |
| [^11] | [Classification of grapevine varieties using UAV hyperspectral imaging.](http://arxiv.org/abs/2401.12851) | 该研究提出了一种使用无人机高光谱成像对17种红白葡萄品种进行分类的方法，相比于传统的破坏性方法，这种方法更高效和 less prohibitive，能够纠正和降低大量数据的采样率，并且能够处理葡萄品种高度相似的高光谱特征。 |
| [^12] | [Learning safety critics via a non-contractive binary bellman operator.](http://arxiv.org/abs/2401.12849) | 本文提出了一种通过利用安全属性的二进制安全评论家算子学习避免达到不安全区域的方法，并研究了其特性和固定点。 |
| [^13] | [An embedding-based distance for temporal graphs.](http://arxiv.org/abs/2401.12843) | 本研究提出了一种基于图嵌入的时间图距离计算方法，能够有效区分具有不同结构和时间属性的图，适用于大规模时间图。 |
| [^14] | [Iterated Relevance Matrix Analysis (IRMA) for the identification of class-discriminative subspaces.](http://arxiv.org/abs/2401.12842) | 迭代相关矩阵分析（IRMA）通过迭代识别类别特定信息的线性子空间并用于降维和训练鲁棒分类器。 |
| [^15] | [Enhancing Next Destination Prediction: A Novel LSTM Approach Using Real-World Airline Data.](http://arxiv.org/abs/2401.12830) | 提出了一种新颖的基于LSTM的模型架构，通过准确捕捉旅行数据中的序列模式和依赖关系，实现了对个人旅行者未来目的地的准确预测。实验结果表明该模型在不同数据规模和性能指标上表现出色，为提升目的地预测方法做出了贡献，并使公司能够提供个性化推荐和优化客户体验。 |
| [^16] | [MAPPING: Debiasing Graph Neural Networks for Fair Node Classification with Limited Sensitive Information Leakage.](http://arxiv.org/abs/2401.12824) | 本文提出了一种使用有限敏感信息泄露的去偏置图神经网络进行公平节点分类的方法，该方法克服了非独立同分布图结构中的拓扑依赖问题，并构建了一个模型无关的去偏置框架，以防止下游误用并提高训练的可靠性。 |
| [^17] | [Deep Learning Based Simulators for the Phosphorus Removal Process Control in Wastewater Treatment via Deep Reinforcement Learning Algorithms.](http://arxiv.org/abs/2401.12822) | 本研究使用深度强化学习算法创建了磷去除过程控制的模拟器，通过试错来学习控制策略。然而，模拟器的性能在更长时间范围内受到模型误差累积的限制。 |
| [^18] | [DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained Self-supervised Vision Transformer.](http://arxiv.org/abs/2401.12820) | 本文提出了一种新颖的数据驱动无监督语义分割方法（DatUS^2），通过自监督训练生成语义一致且密集的伪标注分割掩模，用于评估视觉特征质量。 |
| [^19] | [Dynamic Layer Tying for Parameter-Efficient Transformers.](http://arxiv.org/abs/2401.12819) | 本论文提出了一种动态层绑定的方法，通过使用强化学习动态选择层并将它们绑定在一起，来减少深度Transformer网络中的可训练参数数量并提高性能。 |
| [^20] | [Binary structured physics-informed neural networks for solving equations with rapidly changing solutions.](http://arxiv.org/abs/2401.12806) | 本论文提出了一种二进制结构的物理信息神经网络框架，通过利用二进制结构来捕捉局部特征，并解决了传统物理信息神经网络在处理具有快速变化解的方程时的困难。 |
| [^21] | [Enhancements for 5G NR PRACH Reception: An AI/ML Approach.](http://arxiv.org/abs/2401.12803) | 这项研究提出了一种基于AI/ML的方法来增强5G NR PRACH接收，通过使用两个神经网络分别估计RAPID和TA，与传统相关方法相比，实验证明了这种方法在性能上的改进。 |
| [^22] | [Deep Learning-based Target-To-User Association in Integrated Sensing and Communication Systems.](http://arxiv.org/abs/2401.12801) | 本文提出了一种深度学习方法，用于在集成感知和通信系统中将雷达目标与通信用户设备进行关联。该方法通过对雷达数据进行处理，实现了联合多目标检测和波束推理。这一方法在主动切换和波束预测等通信任务中具有潜在应用价值。 |
| [^23] | [Deep Learning in Physical Layer: Review on Data Driven End-to-End Communication Systems and their Enabling Semantic Applications.](http://arxiv.org/abs/2401.12800) | 这篇论文综述了物理层的深度学习在数据驱动的端到端通信系统中的应用，以及它们所支持的语义应用。通过深度学习的表示学习，这些系统表现出了增强的适应性和性能，能够理解和适应数据传输的上下文和意图。 |
| [^24] | [MORPH: Towards Automated Concept Drift Adaptation for Malware Detection.](http://arxiv.org/abs/2401.12790) | MORPH是一种专为神经网络设计的有效的基于伪标签的概念漂移适应方法，通过重新训练模型以适应数据分布变化，它在恶意软件检测中能够有效缓解概念漂移，并且能够降低注释工作量，相比现有方法在准确性和鲁棒性方面有显著的改进。 |
| [^25] | [A Review of Deep Learning Methods for Photoplethysmography Data.](http://arxiv.org/abs/2401.12783) | 本综述系统地回顾了自2017年至2023年期间应用深度学习模型处理光电容积法数据的论文。研究发现，深度学习在个人健康管理和其他应用中具有显著成果。根据任务的不同，这些论文被分为医学相关和非医学相关两大类别，医学相关又细分为七个子组，包括血压分析... |
| [^26] | [DeepRicci: Self-supervised Graph Structure-Feature Co-Refinement for Alleviating Over-squashing.](http://arxiv.org/abs/2401.12780) | DeepRicci是一种自监督图结构-特征协同精化技术，旨在缓解典型GNNs中的过度挤压问题。它通过考虑Ricci曲率来改进GNNs的性能，并提出了一个自监督黎曼模型DeepRicci来解决这些挑战。 |
| [^27] | [Fast Nonlinear Two-Time-Scale Stochastic Approximation: Achieving $\mathcal{O}(1/k)$ Finite-Sample Complexity.](http://arxiv.org/abs/2401.12764) | 本文提出了一种新型的两时间尺度随机逼近方法，用于寻找耦合非线性算子的根，并且在强单调条件下证明了该方法的优化收敛速率为$\mathcal{O}(1/k)$。 |
| [^28] | [On the Utility of Probing Trajectories for Algorithm-Selection.](http://arxiv.org/abs/2401.12745) | 本论文提出了一种新颖的算法选择方法，通过使用对实例进行短期求解得到的探测轨迹来描述实例，并用于训练算法选择模型。该方法具有前景和新颖性。 |
| [^29] | [TNANet: A Temporal-Noise-Aware Neural Network for Suicidal Ideation Prediction with Noisy Physiological Data.](http://arxiv.org/abs/2401.12733) | TNANet是一种专为分析带噪声生理时间序列数据而设计的神经网络模型，通过合并先进的编码技术和置信度学习来提高自杀倾向预测的准确性。 |
| [^30] | [The Distributional Uncertainty of the SHAP score in Explainable Machine Learning.](http://arxiv.org/abs/2401.12731) | 本研究提出了一个原则性框架，用于处理在未知实体群体分布下的SHAP评分问题。通过考虑一个不确定性区域，我们可以确定所有特征的SHAP评分的紧束范围。 |
| [^31] | [Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios.](http://arxiv.org/abs/2401.12729) | 通过生成合成数据和比例类平衡技术，提高了针对小目标的物体检测性能。这项研究解决了工业场景中收集和注释小目标数据的难题，并讨论了比例类平衡技术的效果。 |
| [^32] | [Falcon: Fair Active Learning using Multi-armed Bandits.](http://arxiv.org/abs/2401.12722) | Falcon是一个使用多臂赌博机的公平主动学习框架，通过策略性样本选择来改善机器学习模型的公平性。它通过识别对于提高公平性最具信息量的“目标群体”样本，并采用一种试错方法来解决样本选择中没有ground truth标签的挑战。 |
| [^33] | [Gas trap prediction from 3D seismic and well test data using machine learning.](http://arxiv.org/abs/2401.12717) | 本文通过选择具有已建立的气体饱和度和过滤特性的体积，使用数据处理方法和机器学习算法预测气体圈闭，并取得了高效的结果。 |
| [^34] | [When Redundancy Matters: Machine Teaching of Representations.](http://arxiv.org/abs/2401.12711) | 传统的机器教学中，概念可以有许多等效的表示方式，这种冗余性对搜索空间有强烈影响。本文探索了教授表示的方法，并提出了几种教学模式，分析了它们在不同表示语言下的教学有效性提升。实验结果表明，Greedy模式能更好地处理冗余，但仍有改进空间。 |
| [^35] | [Deep Neural Network Benchmarks for Selective Classification.](http://arxiv.org/abs/2401.12708) | 本论文研究了用于选择性分类的深度神经网络，目的是设计一种选择机制来平衡被拒绝的预测比例和所选预测的预测性能改进。 |
| [^36] | [Energy-based Automated Model Evaluation.](http://arxiv.org/abs/2401.12689) | 提出了一种基于能量的自动化模型评估方法，通过建立关于个体样本相关信息的元分布统计量，能够更高效和有效地评估机器学习模型的性能，解决了AutoEval框架中的过度自信、存储和计算成本高等问题。 |
| [^37] | [DVL Calibration using Data-driven Methods.](http://arxiv.org/abs/2401.12687) | 本文提出了一种使用数据驱动方法进行DVL校准的深度学习框架，通过模拟数据的实验结果表明，该方法在准确性和校准时间上分别比基于模型的方法提高了35%和80%。 |
| [^38] | [Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach.](http://arxiv.org/abs/2401.12686) | 这篇论文提出了一种在稀疏图上学习平均场对局博弈的新方法，通过引入图形扩展的概念，解决了现有方法对于稀疏网络拓扑结构的限制。 |
| [^39] | [LLpowershap: Logistic Loss-based Automated Shapley Values Feature Selection Method.](http://arxiv.org/abs/2401.12683) | LLpowershap是一种基于逻辑损失的自动Shapley值特征选择方法，可以在选择的特征集合中识别出具有最小噪声的信息特征。 |
| [^40] | [Non-Neighbors Also Matter to Kriging: A New Contrastive-Prototypical Learning.](http://arxiv.org/abs/2401.12681) | 这篇论文提出了一种新的对比性典型学习方法，用于改进Kriging过程中邻居和非邻居的信息利用，从而提高属性估计的准确性。 |
| [^41] | [Feature Selection via Robust Weighted Score for High Dimensional Binary Class-Imbalanced Gene Expression Data.](http://arxiv.org/abs/2401.12667) | 本文提出了一种用于高维基因表达二分类问题的稳健加权分数（ROWSU）方法，解决了基因表达数据中高度倾斜的类别分布对分类算法性能的不利影响问题。 |
| [^42] | [Integrating Human Expertise in Continuous Spaces: A Novel Interactive Bayesian Optimization Framework with Preference Expected Improvement.](http://arxiv.org/abs/2401.12662) | 提出了一种将人类专业知识与机器学习相结合的新型交互式贝叶斯优化框架，通过捕捉用户偏好和引入新的收益函数提高机器学习系统的效率。 |
| [^43] | [Consistency Enhancement-Based Deep Multiview Clustering via Contrastive Learning.](http://arxiv.org/abs/2401.12648) | 本文提出了一种基于一致性增强的深度多视图聚类方法通过对比学习（CCEC）。该方法通过引入语义连接块并入特征表示中，以保持多个视图间的一致信息，并通过谱聚类改善聚类的表示过程。实验结果显示，该方法在多个数据集上的表现优于其他现有方法。 |
| [^44] | [On the Robustness of Deep Learning-aided Symbol Detectors to Varying Conditions and Imperfect Channel Knowledge.](http://arxiv.org/abs/2401.12645) | 本文研究了深度学习辅助符号检测器在对变动条件和不完全信道知识的情况下的鲁棒性。研究结果显示，该算法在学习噪声信道数据和不完全信道衰减特性方面表现出显著优势，但其持续性和适用性有待进一步研究。 |
| [^45] | [Binary Feature Mask Optimization for Feature Selection.](http://arxiv.org/abs/2401.12644) | 这个论文提出了一种新颖的特征选择框架，通过使用特征屏蔽方法来消除特征，而不是从数据集中移除它们。这种方法不需要重新训练机器学习模型，可以综合考虑特征子集的重要性，为通用机器学习模型的特征选择问题提供了一种新的解决方案。 |
| [^46] | [A Reply to Makelov et al. (2023)'s "Interpretability Illusion" Arguments.](http://arxiv.org/abs/2401.12631) | 本文回应了Makelov等人(2023)的论文，该论文评述了子空间交换干预方法的"解释性错觉"问题。我们指出，所谓的"解释性错觉"可以包括直观和可取的解释，而Makelov等人(2023)发现的"错觉"是他们训练和评估范例的产物。尽管我们不同意他们的核心表述，但他们的例子和讨论推动了可解释性领域的发展。 |
| [^47] | [Full-Stack Optimization for CAM-Only DNN Inference.](http://arxiv.org/abs/2401.12630) | 本文研究了CAM只有深度学习神经网络推理的全栈优化，通过算法优化和关联处理器的设计，以及使用赛车磁记忆来实现，成功降低了能量消耗和延迟，并提高了神经网络的精度和可靠性。 |
| [^48] | [Blind Channel Estimation and Joint Symbol Detection with Data-Driven Factor Graphs.](http://arxiv.org/abs/2401.12627) | 本论文研究了在时变线性干扰信道上基于因子图的盲信道估计和联合符号检测方法。通过使用置信传播算法和期望最大化算法相互交织的迭代，可以降低复杂度并提高性能。通过引入数据驱动的方法，算法在离线训练样本数量较少的情况下也能取得显著的性能提升。 |
| [^49] | [Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control.](http://arxiv.org/abs/2401.12624) | 这项工作通过将语言导向的语义通信与新兴通信相结合，通过知识蒸馏的方式，提出了一种面向多智能体远程控制的新框架，实现了更快的行程时间和更高的训练收敛速度。 |
| [^50] | [The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting -- An Analytical Model.](http://arxiv.org/abs/2401.12617) | 本文研究了任务相似性和过参数化如何联合影响连续学习中的灾难性遗忘，并发现在过参数化模型中，中等任务相似性导致最多的遗忘，而在插值阈值附近，遗忘随期望任务相似性单调减少。 |
| [^51] | [Prompt Smells: An Omen for Undesirable Generative AI Outputs.](http://arxiv.org/abs/2401.12611) | 本文提出了两个新概念，分别是对GenAI输出可取性的定义和"提示气味"的概念，旨在解决GenAI模型应用中的限制和挑战。 |
| [^52] | [The twin peaks of learning neural networks.](http://arxiv.org/abs/2401.12610) | 该论文研究了神经网络的双峰现象，发现高度过参数化的模型可以避免过拟合并实现良好的测试性能，与传统的偏差-方差折衷法则不同。研究分析了布尔均值维度（BMD）与网络复杂性和敏感性之间的关系，得到了在高维度范围内BMD的可解释表达式，发现BMD在网络过参数化程度增加时达到极值点。 |
| [^53] | [Fast Semi-supervised Unmixing using Non-convex Optimization.](http://arxiv.org/abs/2401.12609) | 本文介绍了一种用于解混合的快速半监督非凸优化模型，该模型考虑了库不匹配和丰度约束，并提出了两种基于先验的半监督解混合方法。实验证明，实施凸性约束优于稀疏先验对于端元库的表现。 |
| [^54] | [Interpreting Equivariant Representations.](http://arxiv.org/abs/2401.12588) | 本文研究了潜在表示的等变性以及在使用中考虑等变模型的归纳偏差的重要性，提出了选择不变投影的原则，并展示了两个实例的影响。 |
| [^55] | [LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools.](http://arxiv.org/abs/2401.12576) | LLMCheckup是一个可解释性工具，通过连接大型语言模型与可解释的AI工具，使用户能够与模型进行对话，生成自我解释并提供建议。 |
| [^56] | [Graph Contrastive Invariant Learning from the Causal Perspective.](http://arxiv.org/abs/2401.12564) | 本文从因果关系的角度研究了图对比不变学习，并提出了一种新的GCL方法，通过引入谱图扩增和设计不变性目标和独立性目标来更好地学习不变表示。 |
| [^57] | [UR4NNV: Neural Network Verification, Under-approximation Reachability Works!.](http://arxiv.org/abs/2401.12550) | 本文提出了UR4NNV验证框架，利用欠估计可达性分析进行DNN验证。该框架对具有ReLU激活的DNN进行欠估计，并通过试错方法有效地证伪DNN属性。 |
| [^58] | [On Building Myopic MPC Policies using Supervised Learning.](http://arxiv.org/abs/2401.12546) | 本论文提出了一种使用监督学习构建近视MPC策略的方法，通过离线学习最优值函数，可以显著减少在线计算负担，而不影响控制器的性能。 |
| [^59] | [Efficient Constrained $k$-Center Clustering with Background Knowledge.](http://arxiv.org/abs/2401.12533) | 本论文提出了一种在k中心聚类上利用背景知识的约束聚类算法，通过采用一系列技术，得到了效率高且具有最佳近似比例2的算法。 |
| [^60] | [DAFA: Distance-Aware Fair Adversarial Training.](http://arxiv.org/abs/2401.12532) | DAFA通过考虑类别之间的相似性，引入了不同的损失权重和对抗边界，并调整它们以提高在对抗训练中的鲁棒公平性。 |
| [^61] | [BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models.](http://arxiv.org/abs/2401.12522) | BiTA是一种用于大语言模型的创新方法，通过双向调整实现了无损加速。它采用简化的半自回归生成和草稿验证，通过高效的基于树的解码同时进行候选生成和验证，提高了推理效率。这种方法不需要额外的辅助模型或显著的额外内存开销。 |
| [^62] | [Key Information Retrieval to Classify the Unstructured Data Content of Preferential Trade Agreements.](http://arxiv.org/abs/2401.12520) | 本论文介绍了一种用于长文本分类和预测的新方法，通过嵌入技术对长文本进行压缩，然后采用双向编码器表示来自Transformers的嵌入方法进行文本分类训练，实验结果显示在优惠贸易协定的长文本分类方面取得了显著的性能提升。 |
| [^63] | [DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing High-Quality Implicit Neural Representations.](http://arxiv.org/abs/2401.12517) | DDMI是一种面向领域无关的隐式神经表示的高质量合成的潜在扩散模型，通过生成自适应位置嵌入而不是网络权重，解决了现有方法中生成质量较低的问题。 |
| [^64] | [Digital cloning of online social networks for language-sensitive agent-based modeling of misinformation spread.](http://arxiv.org/abs/2401.12509) | 本研究开发了一个基于代理建模和自然语言处理技术的仿真框架，用于研究在线社交网络中的误信息传播。通过数字克隆已知的误信息共享网络，我们提高了模拟的真实性和普适性，并且考虑到了讨论主题、用户偏好和在线社区动态等因素。 |
| [^65] | [On the Stochastic (Variance-Reduced) Proximal Gradient Method for Regularized Expected Reward Optimization.](http://arxiv.org/abs/2401.12508) | 本文介绍了一种用于正则化预期奖励优化问题的随机近端梯度法，该方法通过引入方差减小的技术以提高收敛速度。实验结果表明，在满足一定条件的情况下，该方法的样本复杂度可以达到$O(\epsilon^{-3})$。 |
| [^66] | [Building Minimal and Reusable Causal State Abstractions for Reinforcement Learning.](http://arxiv.org/abs/2401.12497) | 本文介绍了一种称为Causal Bisimulation Modeling (CBM)的方法，该方法通过学习动态和奖励函数中的因果关系来构建最小和可重用的任务特定抽象。实证验证表明，CBM学习到的隐式动态模型比显式模型更准确地识别出底层的因果关系和状态抽象。 |
| [^67] | [DexTouch: Learning to Seek and Manipulate Objects with Tactile Dexterity.](http://arxiv.org/abs/2401.12496) | 本论文介绍了一种利用触觉灵巧性寻找和操作物体的多指机器人系统。通过使用触觉传感器进行物体搜索和操作，我们证明了即使在没有依赖于视觉信息的情况下，机器人也能够具备类似人类的触觉能力。 |
| [^68] | [Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?.](http://arxiv.org/abs/2401.12492) | 本研究比较了以群体属性、个体用户和组合方法来模拟人的上下文。合并群体和个体特征显著提高了用户级回归任务的性能，而模拟个体用户则显著提高了单个文档级分类任务的性能。 |
| [^69] | [Unsupervised Learning Method for the Wave Equation Based on Finite Difference Residual Constraints Loss.](http://arxiv.org/abs/2401.12489) | 本文提出了一种新型的无监督学习方法，通过基于有限差分残差约束的波动方程，解决了现有深度学习方法中存在的问题。该方法具有高效、低成本和强泛化能力，并通过实验证明其优于传统的物理感知神经网络。 |
| [^70] | [Adiabatic Quantum Support Vector Machines.](http://arxiv.org/abs/2401.12485) | 本文描述了一种用于训练支持向量机的绝热量子方法，与经典方法相比，我们的方法在时间复杂度上取得了一个数量级的改进，并且在五个基准数据集上取得了与经典方法相当的测试准确率。我们还展示了我们的方法具有良好的可扩展性。 |
| [^71] | [Mini-batch Submodular Maximization.](http://arxiv.org/abs/2401.12478) | 我们提出了第一个小批量算法，用于在约束条件下最大化非负单调可分解的子模函数F，该算法在实践中比基于稀疏化方法的做法更好。 |
| [^72] | [Bayesian identification of nonseparable Hamiltonians with multiplicative noise using deep learning and reduced-order modeling.](http://arxiv.org/abs/2401.12476) | 本文提出了一种用于学习非分离哈密顿系统的结构保持的贝叶斯方法，可以处理统计相关的加性和乘性噪声，并且通过将结构保持方法纳入框架中，提供了对高维系统的高效识别。 |
| [^73] | [Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment.](http://arxiv.org/abs/2401.12474) | 本文介绍了一种名为Ditto的角色扮演自我对齐方法，通过对角色知识的利用，使大型语言模型能够模拟角色扮演对话，从而增强其角色扮演能力。实验证明，Ditto在角色扮演基准和MT-Bench的评估中取得了出色的结果。 |
| [^74] | [Reinforcement Learning for Graph Coloring: Understanding the Power and Limits of Non-Label Invariant Representations.](http://arxiv.org/abs/2401.12470) | 本论文将寄存器分配问题转化为图着色问题，并展示了Proximal Policy Optimization模型通过学习解决图着色问题。同时，我们还发现图的标记对模型性能至关重要，并提出了机器学习模型需要具有标签重新排序不变性的图表示。 |
| [^75] | [Multi-agent deep reinforcement learning with centralized training and decentralized execution for transportation infrastructure management.](http://arxiv.org/abs/2401.12455) | 这项研究提出了一种基于集中训练和分散执行的多Agent深度强化学习框架，用于管理交通基础设施系统的整个生命周期，在处理高维度空间中的不确定性和约束条件时能够降低长期风险和成本。 |
| [^76] | [Post-Training Embedding Alignment for Decoupling Enrollment and Runtime Speaker Recognition Models.](http://arxiv.org/abs/2401.12440) | 该论文提出了一种后训练嵌入对齐的方法，用于解决注册和运行时说话人识别模型耦合的问题。实验结果表明，这种方法在共享说话人嵌入空间中明显优于传统的余弦相似度计算方法。 |
| [^77] | [Secure Federated Learning Approaches to Diagnosing COVID-19.](http://arxiv.org/abs/2401.12438) | 本文介绍了一种使用安全联邦学习方法诊断COVID-19的模型，通过在多个设备上进行本地数据样本的算法训练，无需数据共享。该模型在胸部X光诊断方面取得了进展，并解决了HIPAA合规限制的挑战。 |
| [^78] | [Wasserstein Differential Privacy.](http://arxiv.org/abs/2401.12436) | Wasserstein Differential Privacy是一种用于测量隐私泄漏风险的替代DP框架，满足对称性和三角不等式性质，并具有13个优秀性质。Wasserstein accountant是一种通用的隐私计算方法，可以稳定地获得隐私预算。 |
| [^79] | [Quantitative Analysis of Molecular Transport in the Extracellular Space Using Physics-Informed Neural Network.](http://arxiv.org/abs/2401.12435) | 本文提出了一种使用物理信息神经网络对细胞外间隙中分子传输进行定量分析的新方法，解决了对分子传输形式不清楚的挑战，并实现了自动计算扩散系数和分子速度的优化功能。 |
| [^80] | [The Neglected Tails of Vision-Language Models.](http://arxiv.org/abs/2401.12425) | 本文通过分析预训练文本来衡量视觉语言模型中概念的频率，并发现流行的VLM数据集展示了长尾概念分布，这与按类别的准确率强烈相关。 |
| [^81] | [DALex: Lexicase-like Selection via Diverse Aggregation.](http://arxiv.org/abs/2401.12424) | 本文提出了一种新的选择算法DALex，它通过加权训练案例误差的和来选择最佳个体，相比标准的词法选择更快速。 |
| [^82] | [Towards Improved Variational Inference for Deep Bayesian Models.](http://arxiv.org/abs/2401.12418) | 本论文探讨了改进深度贝叶斯模型的变分推断方法，旨在解决深度模型训练过程中的过度自信和不准确预测问题。通过使用变分推断提供的后验近似和边缘似然下界，可以优化超参数并实现模型选择。 |
| [^83] | [Enhancing Reliability of Neural Networks at the Edge: Inverted Normalization with Stochastic Affine Transformations.](http://arxiv.org/abs/2401.12416) | 本论文介绍了一种通过倒置归一化和随机仿射变换来提高内存计算架构中贝叶斯神经网络鲁棒性和推理精度的方法 |
| [^84] | [How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data.](http://arxiv.org/abs/2401.12413) | 本文研究了如何通过仅有的少量微小多语言平行数据来增强以英语为中心的模型的零样本翻译能力，实现大幅度的非英文整体改进，并保持英文方向上的性能能力。研究表明，即使在随机抽取的少量方向集上进行微调，也可以获得可比较的整体改进。 |
| [^85] | [Enhancing In-context Learning via Linear Probe Calibration.](http://arxiv.org/abs/2401.12406) | 本研究提出了一种名为线性探测校准（LinC）的技术，通过校准模型的输出概率，显著提高了上下文学习（ICL）在生成预训练变压器（GPT）模型上的测试性能。 |
| [^86] | [Longitudinal Sentiment Classification of Reddit Posts.](http://arxiv.org/abs/2401.12382) | 本研究对四所加拿大主要大学的学生在Reddit上撰写的帖子进行纵向情感分类。通过调整情感阈值，我们成功构建了分类器，能够将帖子情感分类为积极和消极类别，并且结果在不同大学数据集中一致。 |
| [^87] | [SubgroupTE: Advancing Treatment Effect Estimation with Subgroup Identification.](http://arxiv.org/abs/2401.12369) | SubgroupTE是一种新的治疗效果估计模型，通过子群识别提高了估计的精度，考虑了不同子群体的治疗反应，从而更准确地估计治疗效果。 |
| [^88] | [VC dimension of Graph Neural Networks with Pfaffian activation functions.](http://arxiv.org/abs/2401.12362) | 本文分析了图神经网络（GNN）中使用不同常用激活函数（如sigmoid和双曲正切）时的VC维度，采用了Pfaffian函数理论框架，通过架构参数和合作数量提供了界限。 |
| [^89] | [Efficient Collaborations through Weight-Driven Coalition Dynamics in Federated Learning Systems.](http://arxiv.org/abs/2401.12356) | 本文介绍了一种利用设备模型权重之间的距离评估相似性和差异性的联邦学习模型，通过指导设备之间的联合形成，以及利用质心的概念对来自多个设备的更新进行聚合，提供了一种在物联网机器学习中具有潜力的结构化、优越性能和高效通信的模型。 |
| [^90] | [Scaling Up Quantization-Aware Neural Architecture Search for Efficient Deep Learning on the Edge.](http://arxiv.org/abs/2401.12350) | 本文提出了一种在较大规模任务上实现量化感知神经架构搜索（QA-NAS）的方法，通过利用块状形式实现了在边缘计算中进行高效的深度学习。实验结果在语义分割任务上展示了相对于DeepLabV3模型更小的FB-MP模型和更快的INT8模型，而不会损害任务性能。 |
| [^91] | [OCT-SelfNet: A Self-Supervised Framework with Multi-Modal Datasets for Generalized and Robust Retinal Disease Detection.](http://arxiv.org/abs/2401.12344) | OCT-SelfNet是一种用于眼科疾病检测的自监督机器学习框架，通过结合多模态数据集和两阶段训练方法，实现了广义和鲁棒的检测结果。 |
| [^92] | [Contrastive Learning and Cycle Consistency-based Transductive Transfer Learning for Target Annotation.](http://arxiv.org/abs/2401.12340) | 本论文提出了一种基于对比学习和循环一致性的混合非配对域转换网络（H-CUT）来解决自动目标识别（ATR）中标记数据不足的问题。该方法在跨领域转导迁移学习中取得了显著低的FID分数，并通过注意力和熵来强调领域特定区域，以生成高质量的合成图像。 |
| [^93] | [A Precise Characterization of SGD Stability Using Loss Surface Geometry.](http://arxiv.org/abs/2401.12332) | 本文精确刻画了利用损失面几何分析 SGD 稳定性的关键条件，为理解其实际有效性提供了新的方法。 |
| [^94] | [Multi-Agent Dynamic Relational Reasoning for Social Robot Navigation.](http://arxiv.org/abs/2401.12275) | 本文提出了一种多Agent动态关系推理方法，通过明确推断关系结构的演化，来实现在社交机器人导航中的有效性。方法包括推断超边缘以实现群体推理和轨迹预测器生成未来状态。 |
| [^95] | [Transfer Learning for Nonparametric Regression: Non-asymptotic Minimax Analysis and Adaptive Procedure.](http://arxiv.org/abs/2401.12272) | 本文研究了非参数回归的迁移学习问题，提出了一种新的置信阈值估计器来实现渐近最小风险，并发现了迁移学习中的两个独特现象：自动平滑和超加速。此外，我们还提出了一种数据驱动算法，可以适应广泛的参数空间，并在仿真研究和真实世界的例子中证明了该方法的优势。 |
| [^96] | [Machine learning-based network intrusion detection for big and imbalanced data using oversampling, stacking feature embedding and feature extraction.](http://arxiv.org/abs/2401.12262) | 该论文提出了一种基于机器学习的网络入侵检测模型，使用过采样方法解决数据不平衡问题，结合堆叠特征嵌入和特征提取方法，并通过评估多个基准数据集证明了该模型的性能。 |
| [^97] | [Emergent Dominance Hierarchies in Reinforcement Learning Agents.](http://arxiv.org/abs/2401.12258) | 本研究在强化学习中探讨了一种新的支配等级现象，并证明了在没有明确编程和内在奖励的情况下，强化学习代理能够自主发明、学习、实施和传递支配等级给新的群体。 |
| [^98] | [Transfer learning-assisted inverse modeling in nanophotonics based on mixture density networks.](http://arxiv.org/abs/2401.12254) | 本文提出了一种基于迁移学习增强的混合密度网络模型的纳米光子结构逆向建模方法，可以高效地预测多个可能的解决方案。 |
| [^99] | [Accelerating Sinkhorn Algorithm with Sparse Newton Iterations.](http://arxiv.org/abs/2401.12253) | 该论文提出了一种扩展的Sinkhorn算法，通过引入提前停止和牛顿迭代子程序，实现了可能的超指数收敛。他们利用了Sinkhorn算法最大化凹性李雅普诺夫势的特性，发现了势函数的Hessian矩阵近似稀疏，从而将每次迭代的复杂性降低到了$O(n^2)$。 |
| [^100] | [Diffusion Representation for Asymmetric Kernels.](http://arxiv.org/abs/2401.12251) | 本文扩展了扩散映射形式，用于处理由非对称核引导的数据集。通过使用先验坐标系和基于Fourier基的坐标系，可以减少数据集的维数，并提高计算效率。 |
| [^101] | [Orion-14B: Open-source Multilingual Large Language Models.](http://arxiv.org/abs/2401.12246) | Orion-14B是一个具有140亿参数的开源多语言大型语言模型。在该研究中，我们采用数据调度方法对一个基础模型进行训练，使用了来自多种语言的2.5万亿个标记的多样化语料库。我们还对对话应用和其他特定用例进行了微调。评估结果显示，Orion-14B在广泛的任务中取得了领先的性能。我们将Orion-14B模型系列及其相关代码公开，以鼓励未来在这一领域的研究和实际应用。 |
| [^102] | [Large-scale Reinforcement Learning for Diffusion Models.](http://arxiv.org/abs/2401.12244) | 本文介绍了一种大规模强化学习算法，用于改进文本到图像的扩散模型，能够提高模型与人类偏好的一致性，并生成更受人类喜欢的样本。 |
| [^103] | [Constraint-Generation Policy Optimization (CGPO): Nonlinear Programming for Policy Optimization in Mixed Discrete-Continuous MDPs.](http://arxiv.org/abs/2401.12243) | Constraint-Generation Policy Optimization (CGPO)是一种针对混合离散连续MDPs的策略优化方法，能够提供有界的策略误差保证，推导出最优策略，并生成最坏情况的状态轨迹来诊断策略缺陷。 |
| [^104] | [BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models.](http://arxiv.org/abs/2401.12242) | BadChain是对大型语言模型(LLM)采用从思维链提示(COT)的一种新的后门攻击方法。它不需要访问训练数据集或模型参数，并且具有较低的计算开销。 |
| [^105] | [Quantised Neural Network Accelerators for Low-Power IDS in Automotive Networks.](http://arxiv.org/abs/2401.12240) | 本文研究了低功耗汽车网络中使用量化神经网络加速器作为入侵检测系统的应用，实现了较低的延迟和推理能耗，并且达到了与现有方法相似的分类性能。 |
| [^106] | [Spatial Scaper: A Library to Simulate and Augment Soundscapes for Sound Event Localization and Detection in Realistic Rooms.](http://arxiv.org/abs/2401.12238) | Spatial Scaper是一个用于模拟和增强声场的库，可用于声音事件定位和检测。它通过模拟虚拟房间和应用数据增强技术，提供了更具多样性的训练数据，从而改进了声学模型的性能。 |
| [^107] | [A distribution-guided Mapper algorithm.](http://arxiv.org/abs/2401.12237) | 这项工作引入了一种名为D-Mapper的分布引导Mapper算法，使用概率模型和数据固有特征生成密度引导的覆盖，并提供增强的拓扑特征。 |
| [^108] | [The Surprising Harmfulness of Benign Overfitting for Adversarial Robustness.](http://arxiv.org/abs/2401.12236) | 这项研究证明了即使机器学习模型在训练过程中对噪声数据拟合得很好，对敌对示例具有鲁棒性，但当面临敌对操纵的数据时，过度拟合的模型可能会给系统带来意外的危害。 |
| [^109] | [Stochastic Dynamic Power Dispatch with High Generalization and Few-Shot Adaption via Contextual Meta Graph Reinforcement Learning.](http://arxiv.org/abs/2401.12235) | 本文提出了一种通过上下文元图强化学习方法，实现具有高泛化性和少样本适应性的随机动态电力分配。通过引入更一般化的上下文MDP和可扩展的图表示，该方法能够处理多变量不确定性的实时多阶段随机电力分配问题，填补了现有研究的泛化性和实用性低的缺点。 |
| [^110] | [A Lightweight FPGA-based IDS-ECU Architecture for Automotive CAN.](http://arxiv.org/abs/2401.12234) | 本文提出了一种基于轻量级FPGA的IDS-ECU架构，在传统ECU功能的基础上，用于汽车CAN总线入侵检测系统(IDS)。通过使用现成的深度学习处理单元(DPU)IP块进行加速，该架构能够检测多种攻击向量，并具有几乎零的ECU功能开销。 |
| [^111] | [Memorization in Self-Supervised Learning Improves Downstream Generalization.](http://arxiv.org/abs/2401.12233) | 自监督学习中的记忆化问题一直是一个挑战，本文提出了SSLMem框架，用于定义自监督学习中的记忆化，并通过实证分析证明了在大规模数据集和强数据增强的情况下，记忆化仍然存在。 |
| [^112] | [Machine Learning Modeling Of SiRNA Structure-Potency Relationship With Applications Against Sars-Cov-2 Spike Gene.](http://arxiv.org/abs/2401.12232) | 机器学习模型揭示了SiRNA结构与效力关系，为抗击Sars-Cov-2 Spike基因提供了应用前景。 |
| [^113] | [Disentangled Condensation for Large-scale Graphs.](http://arxiv.org/abs/2401.12231) | 本文提出了用于大规模图的解缠结凝聚方法DisCo，通过节点和边的凝聚模块实现了对大规模图的高效缩凝，提高了可扩展性和压缩图的保真度。 |
| [^114] | [Computing in the Era of Large Generative Models: From Cloud-Native to AI-Native.](http://arxiv.org/abs/2401.12230) | 本文探讨了大型生成AI模型和云原生计算架构的交集，提出了一种利用云原生技术和高级机器学习运行时的AI原生计算范式，旨在优化成本并提高资源可访问性，未来的研究和发展具有潜力。 |
| [^115] | [Multimodal Data Curation via Object Detection and Filter Ensembles.](http://arxiv.org/abs/2401.12225) | 本论文提出了一种通过组合目标检测和弱监督集成的方法，用于整理多模态数据，并在DataComp竞赛的过滤器任务中取得了4%至4.2%的性能改进。 |
| [^116] | [Learning Dynamics from Multicellular Graphs with Deep Neural Networks.](http://arxiv.org/abs/2401.12196) | 本研究提出了使用基于图的深度神经网络来预测多细胞集合体的运动能力。实验结果表明，这种方法能够准确地识别多细胞生物系统中的复杂图特征，并超越传统机械模型的能力。同时，研究者建议通过合作努力来构建一个多细胞数据库，以进一步推动多细胞动力学研究的发展。 |
| [^117] | [Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations.](http://arxiv.org/abs/2401.11792) | 本文介绍了一种安全且广义的端到端自主驾驶系统 (SGADS)，使用强化学习和示范相结合的方法解决了现有方法的低安全性、泛化能力差和采样效率低的问题，同时引入了变分推理和归一化流以准确预测驾驶轨迹，并提出了鲁棒性安全约束的制定方法。 |
| [^118] | [GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient Inversion Attacks?.](http://arxiv.org/abs/2401.11748) | 本文提出了一种新颖的梯度反转攻击方法GI-PIP，不需要依赖不切实际的辅助数据集，通过利用异常检测模型从较少的数据中捕获底层分布，并能在图像恢复方面表现出优异的能力，同时在分布泛化方面也更强大。 |
| [^119] | [TIM: An Efficient Temporal Interaction Module for Spiking Transformer.](http://arxiv.org/abs/2401.11687) | TIM 是一种高效的时间交互模块，用于增强脉冲神经网络 (SNNs) 的时间数据处理能力。 |
| [^120] | [HARDCORE: H-field and power loss estimation for arbitrary waveforms with residual, dilated convolutional neural networks in ferrite cores.](http://arxiv.org/abs/2401.11488) | 本论文提出了HARDCORE方法，使用残差卷积神经网络和物理信息扩展来高效估计铁芯中任意波形的H场和功率损耗。关键解决方案是通过重建bh曲线并根据曲线的面积估计功率损耗，并采用专家特征工程和信息丰富的输入来实现简明的模型架构。 |
| [^121] | [Sequential Model for Predicting Patient Adherence in Subcutaneous Immunotherapy for Allergic Rhinitis.](http://arxiv.org/abs/2401.11447) | 本研究利用新颖的机器学习模型，准确预测患者的非依从风险和相关的系统症状评分，为长期过敏性鼻炎亚卡激素皮下免疫治疗的管理提供了一种新的方法。 |
| [^122] | [PartIR: Composing SPMD Partitioning Strategies for Machine Learning.](http://arxiv.org/abs/2401.11202) | PartIR是一种用于机器学习的分区系统，具备表达力强和可预测性强的特点。它通过高级程序员发出的分区策略驱动，并采用增量重写方法，能够组合不同的分片策略，评估结果表明其可预测性、表达能力和达到峰值性能能力强。 |
| [^123] | [Data Augmentation for Traffic Classification.](http://arxiv.org/abs/2401.10754) | 这项工作通过对交通分类任务中的数据增强进行分析和实验，发现时间序列顺序和掩码的增强在交通分类中更适用，同时提出了简单的潜在空间分析可以解释增强效果的思路。 |
| [^124] | [ChatQA: Building GPT-4 Level Conversational QA Models.](http://arxiv.org/abs/2401.10225) | ChatQA是一系列对话问答模型，可以达到GPT-4级别的准确性。通过两阶段的指令调整方法，可以显著提高大型语言模型在零-shot对话问答中的结果。使用密集检索器进行问答数据集的微调可以实现与最先进的查询重写模型相当的结果，同时降低部署成本。ChatQA-70B在10个对话问答数据集上的平均得分超过了GPT-4，且不依赖于任何来自OpenAI GPT模型的合成数据。 |
| [^125] | [Spatial-Temporal Large Language Model for Traffic Prediction.](http://arxiv.org/abs/2401.10134) | 本文提出了一种空间-时间大语言模型（ST-LLM）用于交通预测，通过参数扩展和预训练来提高预测准确性，并利用空间-时间嵌入模块学习标记的空间位置和全局时间表示。 |
| [^126] | [Infinite-Horizon Graph Filters: Leveraging Power Series to Enhance Sparse Information Aggregation.](http://arxiv.org/abs/2401.09943) | 提出了一种新颖的图幂滤波器神经网络 (GPFN)，通过使用幂级数图滤波器来增强节点分类。GPFN设计了一种基于收敛幂级数的具有无限接收域的图滤波器构建方法，并能集成任何幂级数并捕捉长程依赖关系。 |
| [^127] | [Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep Learning.](http://arxiv.org/abs/2401.09479) | 本文提出了一种使用多模态深度学习进行硬件特洛伊检测的方法，通过生成对抗网络扩充数据，并采用早融合和晚融合策略进行评估。通过估计不确定性量化指标，实现风险感知的决策制定。 |
| [^128] | [Partial Diacritization: A Context-Contrastive Inference Approach.](http://arxiv.org/abs/2401.08919) | 部分音标化是选择标记部分字符来提高阅读可读性和准确性的新方法。上下文对比的部分音标化（CCPD）集成了现有的阿拉伯音标化系统，并通过衡量部分音标化的新指标来判断需要标记哪些字符。 |
| [^129] | [SpecSTG: A Fast Spectral Diffusion Framework for Probabilistic Spatio-Temporal Traffic Forecasting.](http://arxiv.org/abs/2401.08119) | 我们提出了一种快速谱扩散框架SpecSTG，用于概率时空交通预测。该方法在谱域中生成未来时间序列的傅里叶表示，利用空间信息来更好地利用交通数据中的空间依赖性和系统模式。 |
| [^130] | [Joint Unsupervised and Supervised Training for Automatic Speech Recognition via Bilevel Optimization.](http://arxiv.org/abs/2401.06980) | 本文提出了一种双层优化的训练方法，用于自动语音识别，通过联合无监督和监督训练来提高性能。 |
| [^131] | [RudolfV: A Foundation Model by Pathologists for Pathologists.](http://arxiv.org/abs/2401.04079) | 本文提出了一种由病理学家为病理学家构建的基础模型，通过数据整理和融入病理学领域知识，扩展了数字病理学全玻片图像基础模型，解决了人工智能在处理罕见疾病方面的挑战。 |
| [^132] | [S$^{2}$-DMs:Skip-Step Diffusion Models.](http://arxiv.org/abs/2401.01520) | 本论文提出了一种新的训练方法S$^{2}$-DMs，通过创新的$L_{skip}$重新整合选择性采样阶段中省略的信息，显著提高了样本质量，并且实现简单，对代码修改要求少，与各种采样算法兼容。 |
| [^133] | [Explainability-Driven Leaf Disease Classification using Adversarial Training and Knowledge Distillation.](http://arxiv.org/abs/2401.00334) | 本研究通过对抗训练来提高植物叶片疾病分类模型对对抗攻击的鲁棒性，并通过可解释性技术获得模型的决策过程，同时通过模型压缩技术提高计算效率。实验证明，鲁棒性可能以分类准确性为代价，而学生模型可以以较低的性能损失蒸馏复杂模型的知识，从而提高计算效率。 |
| [^134] | [Self-supervised Pretraining for Robust Personalized Voice Activity Detection in Adverse Conditions.](http://arxiv.org/abs/2312.16613) | 本文提出了使用自我监督预训练方法来改善恶劣条件下个性化语音活动检测模型性能，并且实验证明该方法不仅可以提高在干净条件下的性能，而且能够得到更鲁棒的模型。 |
| [^135] | [Causal Forecasting for Pricing.](http://arxiv.org/abs/2312.15282) | 本文提出了一种在定价环境下进行需求预测的新方法，通过将因果推断的双重机器学习方法和最先进的基于变压器的预测模型结合在一起，我们的方法在完全控制的情况下更好地估计因果效应，并在离线政策设置中优于其他预测方法。 |
| [^136] | [Robust Loss Functions for Training Decision Trees with Noisy Labels.](http://arxiv.org/abs/2312.12937) | 本文研究了在带有噪声标签的数据上训练决策树的鲁棒损失函数。我们的研究主要有三个贡献：提供了对现有损失函数鲁棒性的新洞察，引入了分布损失函数的框架，并介绍了一种高效的贪婪减少不纯度的学习算法。 |
| [^137] | [Tracking Any Object Amodally.](http://arxiv.org/abs/2312.12433) | 本论文介绍了一种追踪任何物体的非现态方法，利用数据增强和微调现态跟踪器，可以提高追踪的效果。 |
| [^138] | [Preference and Concurrence Aware Bayesian Graph Neural Networks for Recommender Systems.](http://arxiv.org/abs/2312.11486) | 本文提出了一个偏好和并发感知的贝叶斯图神经网络框架用于解决推荐系统中的问题，通过生成模型和关注图结构信息来捕捉用户与物品的高阶信息，实验证明了其有效性。 |
| [^139] | [Towards Trustworthy AI Software Development Assistance.](http://arxiv.org/abs/2312.09126) | 本研究致力于构建可信的AI软件开发助手。通过综合架构、训练基础LLM和使用图形代码表示，以及整合知识图等技术手段，提供高质量的代码生成和相关解释。 |
| [^140] | [On the Nystrom Approximation for Preconditioning in Kernel Machines.](http://arxiv.org/abs/2312.03311) | 本文分析了核机器预处理中使用Nystrom逼近的权衡。研究表明，使用对数大小的样本能够让Nystrom逼近的预处理器几乎与梯度下降同样有效地加速。 |
| [^141] | [Conditional Variational Diffusion Models.](http://arxiv.org/abs/2312.02246) | 该论文提出了一种新的条件变分扩散模型，通过学习调度作为训练过程的一部分，解决了扩散模型的敏感性问题，并且能够适应不同的应用场景，提供高质量的解决方案。 |
| [^142] | [A ripple in time: a discontinuity in American history.](http://arxiv.org/abs/2312.01185) | 该论文通过使用向量嵌入和非线性降维方法，发现GPT-2与UMAP的结合可以提供更好的分离和聚类效果。同时，经过微调的DistilBERT模型可用于识别总统和演讲的年份。 |
| [^143] | [Personalized Predictions of Glioblastoma Infiltration: Mathematical Models, Physics-Informed Neural Networks and Multimodal Scans.](http://arxiv.org/abs/2311.16536) | 本研究利用物理信息神经网络（PINNs）从单个三维结构MRI快照中估计胶质母细胞瘤（GBM）生长模型的患者特异性参数，并通过整合理论和数据进行个性化预测，为胶质母细胞瘤的治疗设计提供了关键创新。 |
| [^144] | [Imagination-Augmented Hierarchical Reinforcement Learning for Safe and Interactive Autonomous Driving in Urban Environments.](http://arxiv.org/abs/2311.10309) | 本文提出了一种名为想象力增强分层强化学习 (IAHRL)的方法，通过有效地整合想象力到分层强化学习中，使智能体能够在现实世界的导航任务中学习安全和交互的行为。 |
| [^145] | [Reservoir-Computing Model for Mapping and Forecasting Neuronal Interactions from Electrophysiological Data.](http://arxiv.org/abs/2311.03131) | 该论文介绍了一种基于水库计算网络的计算模型，可以从电生理测量数据中解码神经元网络的时空信息，并在宏观领域内重建网络结构。实验证明该模型比其他常用方法更准确地预测了网络的连接图，并且能够预测网络对特定输入的响应能力。 |
| [^146] | [From Generative AI to Generative Internet of Things: Fundamentals, Framework, and Outlooks.](http://arxiv.org/abs/2310.18382) | 本文介绍了生成物联网（GIoT）的概念和潜在前景。通过将生成人工智能（GAI）集成到现代物联网中，可以实现更高效和智能的物联网应用。文章提出了一个基于GAI的安全激励机制框架，采用生成扩散模型（GDM）和区块链技术来解决GIoT面临的挑战。另外，对现代车辆交通监控进行了案例研究。 |
| [^147] | [A Stability Principle for Learning under Non-Stationarity.](http://arxiv.org/abs/2310.18304) | 本研究提出了一个适用于非稳态环境的统计学习框架，通过应用稳定性原则选择回溯窗口来最大化历史数据利用，并保持累积偏差在可接受范围内。该方法展示了对未知非稳态的适应性，遗憾界在强凸或满足Lipschitz条件下是极小化的最优解。该研究的创新点是函数相似度度量和非稳态数据序列划分技术。 |
| [^148] | [HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks.](http://arxiv.org/abs/2310.15318) | HetGPT是一种预训练异构图神经网络的方法，通过利用提示调整来解决预训练与下游任务之间的不匹配问题。 |
| [^149] | [Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction.](http://arxiv.org/abs/2310.05207) | 本文提出了一种新的面部动作单位（AU）检测框架，通过共享参数和引入多任务学习，在面部标志检测和AU域分离与重建之间实现了更好的性能。实验证明我们方法在野外AU检测方面优于现有方法。 |
| [^150] | [Retrieval meets Long Context Large Language Models.](http://arxiv.org/abs/2310.03025) | "本论文研究了将检索增强和长上下文窗口的大语言模型相结合的解决方案，发现在长上下文任务中，通过检索增强的LLM使用4K上下文窗口可以取得与通过长上下文窗口微调的LLM使用16K上下文窗口相当的性能，同时计算量要少得多。此外，无论上下文窗口大小如何，检索都可以显著提高LLM的性能。" |
| [^151] | [A Geometric Framework for Neural Feature Learning.](http://arxiv.org/abs/2309.10140) | 本论文提出了一个基于神经特征学习的几何框架，在特征空间中利用几何结构解决学习问题。通过引入特征几何，将统计依赖和特征统一到同一空间中，并使用嵌套技术设计学习算法，展示了其在多变量学习问题中的应用。 |
| [^152] | [Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction.](http://arxiv.org/abs/2309.10016) | 本研究评估了GPT-3在抗癌药物敏感性预测任务中的潜力，并发现药物的SMILES表示和细胞系的基因组突变特征对药物反应具有预测能力。这些结果有助于在精准肿瘤学中设计更有效的治疗方案。 |
| [^153] | [Score-Based Generative Models for PET Image Reconstruction.](http://arxiv.org/abs/2308.14190) | 本研究提出了适用于PET图像重建的基于分数的生成模型，通过应对PET图像重建中的挑战，包括高方差的泊松噪声和广泛的动态范围，展示了改进PET重建的显著潜力。 |
| [^154] | [Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks.](http://arxiv.org/abs/2308.11624) | 本论文提出了一种利用人工智能和图表示技术对TCAD器件模拟中的半导体器件进行编码的创新方法，通过引入图注意力网络和通用编码方案，实现了全面的数据驱动建模，为研究人员提供了在设备级上应用基于人工智能的电子设计自动化解决方案的可能性。 |
| [^155] | [Revisiting column-generation-based matheuristic for learning classification trees.](http://arxiv.org/abs/2308.11477) | 该论文改进了基于列生成的启发式方法，以提高学习分类树的效果。通过减少子问题数量、使用数据依赖约束作为割平面以及生成违反约束的数据点，该方法提高了可伸缩性并适用于大型数据集。 |
| [^156] | [Deciphering Raw Data in Neuro-Symbolic Learning with Provable Guarantees.](http://arxiv.org/abs/2308.10487) | 本文介绍了一种使用神经符号混合系统进行机器学习和符号推理的方法，并通过检查知识库来确定它们在促进成功学习方面的效力。研究发现许多知识库满足判据，但也存在一些无法满足的知识库。 |
| [^157] | [Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings.](http://arxiv.org/abs/2307.03212) | 提出了一种区域关注的多视角表示学习（ROMER）的算法，用于捕捉多视角之间的依赖关系，学习城市区域的表达能力，并在多源城市数据中优于现有方法。 |
| [^158] | [When Does Confidence-Based Cascade Deferral Suffice?.](http://arxiv.org/abs/2307.02764) | 本研究旨在探讨何时基于置信度的级联延迟可能失败，以及何时备选的延迟策略可能表现更好。通过理论分析和实验证明事后延迟机制能够显著提高性能。 |
| [^159] | [CasTGAN: Cascaded Generative Adversarial Network for Realistic Tabular Data Synthesis.](http://arxiv.org/abs/2307.00384) | 本文提出了一个级联生成对抗网络（CasTGAN）框架，用于生成逼真的表格数据，并特别关注输出的有效性。通过采用级联架构，其中专门的生成器对每个特征进行采样，使得合成输出更能代表真实数据。实验结果表明，CasTGAN能够产生更真实有效的合成表格数据。 |
| [^160] | [Physics-informed invertible neural network for the Koopman operator learning.](http://arxiv.org/abs/2306.17396) | 本论文提出了一种基于物理信息的可逆神经网络，用于学习Koopman算子。 FlowDMD算法利用耦合流可逆神经网络的特性，学习Koopman算子的不变子空间，并准确重构状态变量。实验证明了该算法的优越性能。 |
| [^161] | [Insights From Insurance for Fair Machine Learning.](http://arxiv.org/abs/2306.14624) | 本文通过将保险业相关概念与机器学习的公平性问题联系起来，提供了一个新的视角，并突出了被忽视的责任和集合与个体之间的紧张关系。 |
| [^162] | [QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules.](http://arxiv.org/abs/2306.09549) | 该论文提出了一种新的量子哈密顿数据集QH9，用于为各种分子提供精确的哈密顿矩阵。通过设计基准任务，展示了当前机器学习模型有能力预测任意分子的哈密顿矩阵。 |
| [^163] | [DeepSeaNet: Improving Underwater Object Detection using EfficientDet.](http://arxiv.org/abs/2306.06075) | 本文研究了在水下环境下，使用EfficientDet等模型提高水下物体检测的效率，并通过对比多个模型的精度和推理时间，发现效率更高的模型可以更好地应对水下物体检测的挑战。 |
| [^164] | [Leaping through tree space: continuous phylogenetic inference for rooted and unrooted trees.](http://arxiv.org/abs/2306.05739) | 本研究首次在连续空间中进行树形系统探索和推断，用于有根和无根树，优于当前最佳方法并在实验中证明了其效果，可用于加速生命科学的新进化发现。 |
| [^165] | [A Lightweight Method for Tackling Unknown Participation Probabilities in Federated Averaging.](http://arxiv.org/abs/2306.03401) | 本文提出了一种轻量级方法来调整联邦平均中的聚合权重，通过根据每个客户的参与历史来处理具有不同参与率的客户，解决了在联邦学习中未知参与概率的问题。 |
| [^166] | [Data-Driven Regret Balancing for Online Model Selection in Bandits.](http://arxiv.org/abs/2306.02869) | 论文讨论在具有赌博反馈的随机环境中进行选择，提出了两种基于数据的模型选择算法，并证明了其保证。通过利用实际遗憾，这些算法在实际中取得了好效果。 |
| [^167] | [Policy Gradient Algorithms for Robust MDPs with Non-Rectangular Uncertainty Sets.](http://arxiv.org/abs/2305.19004) | 本文提出了针对具有非矩形不确定性集的强健MDP的策略梯度算法，并开发了投射Langevin动力学算法和确定性策略梯度方法。数值实验展示了这些算法的有效性。 |
| [^168] | [Determinantal Point Process Attention Over Grid Codes Supports Out of Distribution Generalization.](http://arxiv.org/abs/2305.18417) | 本文描述了一个基于点过程注意力和网格编码的算法，在分布外测试集上实现了泛化能力，为理解大脑强泛化能力提供了见解。 |
| [^169] | [Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery.](http://arxiv.org/abs/2305.14259) | 本文介绍了一种基于文献的发现方法，通过上下文化的学习生成新的科学方向，克服了标准方法在预测关联、忽略上下文等方面的局限性。模型使用了引文和知识图关系的网络，并使用大型语言模型进行评估，发现GPT4在生成创新思想方面表现出色。 |
| [^170] | [SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification.](http://arxiv.org/abs/2305.09781) | SpecInfer是一种LLM服务系统，通过利用推测推断和令牌树验证来加速生成式大语言模型的推断过程，显著减少了为它们提供服务所需的端到端延迟和计算要求，同时确保模型质量。 |
| [^171] | [ZipIt! Merging Models from Different Tasks without Training.](http://arxiv.org/abs/2305.03053) | 本文介绍了一种无需额外训练即可合并不同任务上训练的模型的方法“ZipIt！”。 |
| [^172] | [Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement.](http://arxiv.org/abs/2304.14391) | 本文提出一种基于能量模型的零样本场景重新排列规划器，通过语言指导的空间概念来实现长指令以及在训练时从未见过的空间概念组合。本文的模型在指令导向操作基准测试以及组合指令基准测试中表现良好，优于基于语言表达的最先进方法，并且可以成功地解决之前从未见过的复杂指令和场景。 |
| [^173] | [Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian Optimization with Spatiotemporal feature fusion.](http://arxiv.org/abs/2304.09994) | 本研究提出了一种基于CNN-RNN混合特征融合建模的贝叶斯优化城市洪水预测模型，实现了静态和动态的预测，并通过结合多个CNN和RNN模型，在精度上取得了显著提高。 |
| [^174] | [Sample-efficient Adversarial Imitation Learning.](http://arxiv.org/abs/2303.07846) | 本研究提出了一种利用自监督表示来增强样本效率的对抗性模仿学习方法，从而学习不受扭曲影响的状态和动作表示以建立非图像控制任务的预测表征。 |
| [^175] | [Calibrating Transformers via Sparse Gaussian Processes.](http://arxiv.org/abs/2303.02444) | 提出了一种通过Sparse Gaussian Process attention (SGPA)来校准Transformer模型不确定性的方法。在文本、图像和图形的预测任务中，SGPA-based Transformers在预测准确性上表现出竞争力，并显著改善了内分布校准和外分布的鲁棒性和检测能力。 |
| [^176] | [Loss-Controlling Calibration for Predictive Models.](http://arxiv.org/abs/2301.04378) | 本文提出了一种学习框架，用于对可交换数据进行损失控制预测的校准预测模型。通过引入保持交换性质的变换，我们证明了在获得测试标签时有有限样本控制保证，并发展了一种近似方法来构建预测器。这种方法是符合损失控制预测的自然扩展。 |
| [^177] | [Conformal Loss-Controlling Prediction.](http://arxiv.org/abs/2301.02424) | 本文提出了一种名为操纵损失控制的预测的学习框架，扩展了一致预测的范围，使其适用于需要控制损失函数值的情况。通过实验证明了该框架在有限样本情况下的控制保证。 |
| [^178] | [Homophily modulates double descent generalization in graph convolution networks.](http://arxiv.org/abs/2212.13069) | 本文通过使用统计物理和随机矩阵理论的分析工具，精确地表征了简单图卷积网络在背景随机块模型上的泛化，提出了同质性在图卷积网络的泛化中的调制作用。 |
| [^179] | [Refined Edge Usage of Graph Neural Networks for Edge Prediction.](http://arxiv.org/abs/2212.12970) | 这项研究提出了一种新的边缘预测范式（EMPIRE），通过细化边缘使用方法解决了节点分类任务和边缘预测任务之间的区别。该方法引入了边缘拆分技术和新的消息传递机制，以更好地利用边缘的拓扑结构和监督信号。 |
| [^180] | [The Normalized Cross Density Functional: A Framework to Quantify Statistical Dependence for Random Processes.](http://arxiv.org/abs/2212.04631) | 本文提出了一种用于量化随机过程统计依赖关系的框架，通过最大化交替协方差估计和规范化交叉密度来衡量多变量统计依赖性，并应用于机器学习架构中。 |
| [^181] | [Copula Conformal Prediction for Multi-step Time Series Forecasting.](http://arxiv.org/abs/2212.03281) | 本文提出了一种 Copula 联合预测算法 CopulaCPTS，用于多元、多步时间序列预测，经过实验验证，其置信区间比现有技术更精准和更锐利。 |
| [^182] | [Optimal Algorithms for Stochastic Complementary Composite Minimization.](http://arxiv.org/abs/2211.01758) | 本文研究了随机情况下的互补复合最小化问题，并提出了新的期望超额风险界限和高概率超额风险界限的算法，这是一类具有重要理论和实际意义的问题。 |
| [^183] | [Homotopy-based training of NeuralODEs for accurate dynamics discovery.](http://arxiv.org/abs/2210.01407) | 本论文提出了一种新的神经常微分方程训练方法，基于同步和同伦优化，可以用于从时间序列数据中提取动力学规律，而无需对模型架构进行修改。 |
| [^184] | [A Comprehensive Benchmark for COVID-19 Predictive Modeling Using Electronic Health Records in Intensive Care.](http://arxiv.org/abs/2209.07805) | 本文提出了两个针对COVID-19患者的临床预测任务：Outcome-specific length-of-stay prediction 和 Early mortality prediction，旨在填补临床应用和传统预测任务之间的差距，并提供一个基准测试框架，以实现公平比较各种模型。 |
| [^185] | [Neural-Rendezvous: Provably Robust Guidance and Control to Encounter Interstellar Objects.](http://arxiv.org/abs/2208.04883) | 本文提出了神经会合，一种深度学习导航和控制框架，用于可靠、准确和自主地遭遇快速移动的星际物体。它通过点最小范数追踪控制和谱归一化深度神经网络引导策略来提供高概率指数上界的飞行器交付误差。 |
| [^186] | [Online Bilevel Optimization: Regret Analysis of Online Alternating Gradient Methods.](http://arxiv.org/abs/2207.02829) | 本文介绍了一种在线双层优化设置，提供了新的双层遗憾定义，开发了一种能够利用光滑性的在线交替时间平均梯度方法，并给出了相关的遗憾界限。 |
| [^187] | [Gradual Domain Adaptation via Normalizing Flows.](http://arxiv.org/abs/2206.11492) | 该论文提出使用标准化流来解决逐渐领域适应中中间域有限且距离较大的问题，并通过从源域到高斯混合分布学习目标域的分布变换。 |
| [^188] | [Empowering GNNs via Edge-Aware Weisfeiler-Lehman Algorithm.](http://arxiv.org/abs/2206.02059) | 本文提出了一种基于边缘感知的Weisfeiler-Lehman算法，以增强图神经网络的表达能力，同时保持消息传递方案的可扩展性。实验表明，我们NC-GNN框架在各种基准测试中表现出有效性和高效性。 |
| [^189] | [Adaptive Local Neighborhood-based Neural Networks for MR Image Reconstruction from Undersampled Data.](http://arxiv.org/abs/2206.00775) | 本文提出了一种基于自适应本地邻域的神经网络技术，用于从稀疏采样数据中高效重建MR图像，该技术具有较强的适应性和鲁棒性。 |
| [^190] | [Personalized Algorithmic Recourse with Preference Elicitation.](http://arxiv.org/abs/2205.13743) | 研究提出了PEAR方法，这是一个首个能够针对最终用户需求提供个性化算法补救成本的人机交互方法。该方法利用贝叶斯偏好引导的见解，通过最大化原则性信息增益度量来计算目标用户选择的预期效用，然后将偏好引导整合到强化学习框架中。该方法显著提高了算法干预的经济实用性和用户友好性。 |
| [^191] | [Choice of training label matters: how to best use deep learning for quantitative MRI parameter estimation.](http://arxiv.org/abs/2205.05587) | 本研究表明监督学习方法在选择标签上的天真选择导致了低偏差参数估计的限制。通过使用有意不是真实标签的训练标签，我们发现自监督方法可以提供比监督方法更低的偏差参数估计。 |
| [^192] | [Self-Supervised Anomaly Detection: A Survey and Outlook.](http://arxiv.org/abs/2205.05173) | 自监督学习的出现引发了新型异常检测算法的发展，其表现优于现有的最先进方法。本文全面综述了当前自监督异常检测方法的技术细节，并讨论了它们的优势和缺点，同时比较了这些模型与其他自监督异常检测模型以及最先进的异常检测模型的性能。 |
| [^193] | [Robust stabilization of polytopic systems via fast and reliable neural network-based approximations.](http://arxiv.org/abs/2204.13209) | 本文通过快速可靠的神经网络近似方法，实现了对多顶点系统进行稳定化控制。离线的混合整数优化方法确保了系统在可调整大小和收敛速度的集合内保持有界。 |
| [^194] | [Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities.](http://arxiv.org/abs/2203.13883) | 这项研究总结了多模式虚假信息检测的方法、挑战和机遇。由于社交媒体平台的转变，虚假信息的性质也发生了变化。研究人员已经开发出自动检测跨模态不协调的技术，但仍面临挑战和不足之处，进一步的研究机会也在等待着挖掘。 |
| [^195] | [Regenerative Particle Thompson Sampling.](http://arxiv.org/abs/2203.08082) | 本文提出了再生粒子汤普森抽样（RPTS），通过重新生成适应的粒子来解决汤普森抽样中粒子权重收敛于零的问题。RPTS在代表性赌博问题中展现出了灵活性和效果的提升，包括对5G网络切片的应用。 |
| [^196] | [SkipNode: On Alleviating Performance Degradation for Deep Graph Convolutional Networks.](http://arxiv.org/abs/2112.11628) | SkipNode提出了一个插拔式模块来缓解深度图卷积网络性能下降问题，通过跳过部分卷积操作效果显著，有效抑制过度平滑和梯度消失。 |
| [^197] | [Generalized Out-of-Distribution Detection: A Survey.](http://arxiv.org/abs/2110.11334) | 广义的离群检测的调查研究探讨了离群检测的重要性及其与异常检测、新颖性检测和开放集识别等问题的联系，对于提高机器学习系统的可靠性和安全性具有关键意义。 |
| [^198] | [DPGNN: Dual-Perception Graph Neural Network for Representation Learning.](http://arxiv.org/abs/2110.07869) | DPGNN是一种新颖的双感知图神经网络，通过引入多步消息源、节点特定的消息输出和多空间消息交互来增强图神经网络的表达能力。 |
| [^199] | [MNL-Bandit with Knapsacks: a near-optimal algorithm.](http://arxiv.org/abs/2106.01135) | 这篇论文介绍了一种解决动态商品选择问题的算法，通过使用近似最优策略，可在未知需求情况下最大化总体预期收入。在大库存环境下，该算法能够接近最优解。 |
| [^200] | [Dual Online Stein Variational Inference for Control and Dynamics.](http://arxiv.org/abs/2103.12890) | 本文提出了一种双在线斯坦变分推理算法，能够实时估计模型参数和控制输入的分布，在复杂环境中适应不确定性。 |

# 详细

[^1]: AutoRT：大规模编排机器人代理的具身基础模型

    AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents. (arXiv:2401.12963v1 [cs.RO])

    [http://arxiv.org/abs/2401.12963](http://arxiv.org/abs/2401.12963)

    AutoRT是一个利用现有的基础模型来扩展机器人在未知场景中的部署的系统，通过利用视觉-语言模型和大型语言模型，提出多样化和新颖的指令，并有效地推理自主权和安全性的权衡。

    

    拥有语言、视觉和行动等功能的具身基础模型已经彻底改变了利用互联网规模的数据来推理有用任务的能力。然而，训练具身基础模型的一个关键挑战是缺乏基于物理世界的数据。在本文中，我们提出了AutoRT，一个利用现有的基础模型来扩展完全未知场景中操作机器人的部署的系统，只需要最少的人工监督。AutoRT利用视觉-语言模型(VLMs)实现场景理解和基础绑定，并进一步利用大型语言模型(LLMs)提出多样化和新颖的指令，供一组机器人执行。通过利用基础模型的知识来指导数据收集，AutoRT能够有效地推理自主权和安全性的权衡，同时显著扩大机器人学习的数据收集。我们演示了AutoRT向20多个机器人提议指令。

    Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multi
    
[^2]: 聊天宝盒：不稳定网络下LLM Token Streaming的稳健传输

    Chatterbox: Robust Transport for LLM Token Streaming under Unstable Network. (arXiv:2401.12961v1 [cs.NI])

    [http://arxiv.org/abs/2401.12961](http://arxiv.org/abs/2401.12961)

    聊天宝盒（Chatterbox）是针对LLM Chatbots中的令牌流问题提出的一种新颖的传输层方案，通过将新生成的令牌和当前未确认的令牌放入下一个发送的数据包中，实现了稳定的传输和渲染，避免了在不稳定网络环境下的停顿现象。

    

    为了实时渲染生成的令牌，LLM服务器逐个生成响应令牌，并通过网络将每个生成的令牌（或少量令牌组）流式传输到用户，我们称之为LLM令牌流。然而，在不稳定的网络条件下，LLM令牌传输体验可能会受到极大的停顿影响，因为一次数据包丢失可能会阻塞后续数据包中包含的令牌的渲染，即使它们按时到达。通过真实世界的测量研究，我们发现当前的应用程序（包括ChatGPT，Claude和Bard）在不稳定网络条件下都会遭受停顿问题的增加。针对LLM Chatbots中出现的这个新兴的令牌流问题，我们提出了一种新颖的传输层方案，称为聊天宝盒（Chatterbox），它将新生成的令牌和当前未确认的令牌放入下一个发送的数据包中。这样，每个数据包都包含一些新的令牌，并且在接收到时可以独立进行渲染，从而避免了停顿问题。

    To render each generated token in real time, the LLM server generates response tokens one by one and streams each generated token (or group of a few tokens) through the network to the user right after it is generated, which we refer to as LLM token streaming. However, under unstable network conditions, the LLM token streaming experience could suffer greatly from stalls since one packet loss could block the rendering of tokens contained in subsequent packets even if they arrive on time. With a real-world measurement study, we show that current applications including ChatGPT, Claude, and Bard all suffer from increased stall under unstable network.  For this emerging token streaming problem in LLM Chatbots, we propose a novel transport layer scheme, called Chatterbox, which puts new generated tokens as well as currently unacknowledged tokens in the next outgoing packet. This ensures that each packet contains some new tokens and can be independently rendered when received, thus avoiding af
    
[^3]: 贝叶斯半结构子空间推理

    Bayesian Semi-structured Subspace Inference. (arXiv:2401.12950v1 [cs.LG])

    [http://arxiv.org/abs/2401.12950](http://arxiv.org/abs/2401.12950)

    本文提出了一种贝叶斯近似方法，通过使用子空间推理来解决半结构回归模型中解释性不确定性的问题，并展示了其在推断输入-输出关系和捕捉多重要素方面的有效性。

    

    半结构回归模型能够联合建模可解释的结构化特征效应和复杂的非结构化特征效应。结构化模型部分受统计模型的启发，可用于推断重要特征的输入-输出关系。非结构化部分定义了一个任意深度的神经网络，从而提供了足够的灵活性，以实现竞争性的预测性能。虽然这些模型也可以解释随机不确定性，但在解释性不确定性方面仍然缺乏工作。在本文中，我们通过使用子空间推理，针对半结构回归模型提出了一种贝叶斯近似方法，解决了这个问题。为此，我们扩展了对结构化效应的完整参数空间和非结构化效应的子空间的联合后验采样的子空间推理。除了这种混合采样方案，我们的方法还允许子空间的可调复杂性，并能捕捉多重要素。

    Semi-structured regression models enable the joint modeling of interpretable structured and complex unstructured feature effects. The structured model part is inspired by statistical models and can be used to infer the input-output relationship for features of particular importance. The complex unstructured part defines an arbitrary deep neural network and thereby provides enough flexibility to achieve competitive prediction performance. While these models can also account for aleatoric uncertainty, there is still a lack of work on accounting for epistemic uncertainty. In this paper, we address this problem by presenting a Bayesian approximation for semi-structured regression models using subspace inference. To this end, we extend subspace inference for joint posterior sampling from a full parameter space for structured effects and a subspace for unstructured effects. Apart from this hybrid sampling scheme, our method allows for tunable complexity of the subspace and can capture multip
    
[^4]: 基于奖励相关性过滤的线性离线强化学习

    Reward-Relevance-Filtered Linear Offline Reinforcement Learning. (arXiv:2401.12934v1 [stat.ML])

    [http://arxiv.org/abs/2401.12934](http://arxiv.org/abs/2401.12934)

    本文研究了在线决策理论环境中线性函数逼近的离线强化学习，提出了一种基于奖励相关性过滤的方法，将状态-动作值函数的估计限制在稀疏组件上，具有理论保证，并且样本复杂度仅取决于稀疏组件的大小。

    

    本文研究了在线决策理论环境中线性函数逼近的离线强化学习，其中假设数据生成过程具有决策理论稀疏性而不是估计稀疏性。数据生成过程的结构性限制预设了转移可以分解为一个影响奖励的稀疏组件，并且可能影响不影响奖励的其他外生动力学。虽然用于估计全状态过渡属性的最小可调整集合取决于整个状态，但最优策略，因此状态-动作值函数只依赖于稀疏组件：我们将其称为因果/决策论稀疏性。我们通过修改阈值岭回归在最小二乘策略评估中的应用提出了一种过滤奖励的方法，将状态-动作值函数的估计限制在稀疏组件上。我们为奖励过滤的线性拟合Q-迭代提供了理论保证，样本复杂度仅取决于稀疏组件的大小。

    This paper studies offline reinforcement learning with linear function approximation in a setting with decision-theoretic, but not estimation sparsity. The structural restrictions of the data-generating process presume that the transitions factor into a sparse component that affects the reward and could affect additional exogenous dynamics that do not affect the reward. Although the minimally sufficient adjustment set for estimation of full-state transition properties depends on the whole state, the optimal policy and therefore state-action value function depends only on the sparse component: we call this causal/decision-theoretic sparsity. We develop a method for reward-filtering the estimation of the state-action value function to the sparse component by a modification of thresholded lasso in least-squares policy evaluation. We provide theoretical guarantees for our reward-filtered linear fitted-Q-iteration, with sample complexity depending only on the size of the sparse component.
    
[^5]: pyAKI - 一种自动化KDIGO分类的开源解决方案

    pyAKI - An Open Source Solution to Automated KDIGO classification. (arXiv:2401.12930v1 [cs.LG])

    [http://arxiv.org/abs/2401.12930](http://arxiv.org/abs/2401.12930)

    pyAKI是一种自动化的开源解决方案，用于实施KDIGO标准对急性肾损伤进行分类。与专家标注相比，pyAKI展现出了更优质的性能。

    

    急性肾损伤（AKI）是危重病患者中常见的并发症，影响着高达50%的重症监护病房患者。缺乏标准化和开源工具，以应用肾脏疾病改善全球结果（KDIGO）标准对时间序列数据进行分类，对工作量和研究质量产生了负面影响。本项目介绍了pyAKI，一个开源流程，通过提供全面的KDIGO标准一致性实施方案，填补了这一空白。利用常用的重症监护病房研究数据库Medical Information Mart for Intensive Care (MIMIC)-IV的子集，我们定义了一个标准化的数据模型，以确保可重复性。与专家注释的验证表明，pyAKI在实施KDIGO标准方面表现出了稳健的性能。比较分析表明，它能够超越人工标签的质量。本工作将pyAKI作为一种开源解决方案引入。

    Acute Kidney Injury (AKI) is a frequent complication in critically ill patients, affecting up to 50% of patients in the intensive care units. The lack of standardized and open-source tools for applying the Kidney Disease Improving Global Outcomes (KDIGO) criteria to time series data has a negative impact on workload and study quality. This project introduces pyAKI, an open-source pipeline addressing this gap by providing a comprehensive solution for consistent KDIGO criteria implementation.  The pyAKI pipeline was developed and validated using a subset of the Medical Information Mart for Intensive Care (MIMIC)-IV database, a commonly used database in critical care research. We defined a standardized data model in order to ensure reproducibility. Validation against expert annotations demonstrated pyAKI's robust performance in implementing KDIGO criteria. Comparative analysis revealed its ability to surpass the quality of human labels.  This work introduces pyAKI as an open-source soluti
    
[^6]: DsDm：具有数据模型的模型感知数据集选择

    DsDm: Model-Aware Dataset Selection with Datamodels. (arXiv:2401.12926v1 [cs.LG])

    [http://arxiv.org/abs/2401.12926](http://arxiv.org/abs/2401.12926)

    该论文提出了一种模型感知的数据集选择方法，通过将数据集选择视为一个优化问题来解决，并明确地建模了学习过程如何使用训练数据点来预测目标任务。该方法在提高语言模型性能方面表现出色。

    

    在选择用于训练大规模模型的数据时，标准做法是根据人类对数据质量的认知进行筛选。这种筛选可以得到直观上能提高模型行为的数据点。然而，在实践中通常相反的情况可能发生：我们发现根据与“高质量”数据源的相似性进行选择可能不会增加（甚至可能削弱）与随机选择数据相比的性能。为了开发更好的数据选择方法，我们首先将数据集选择作为一个优化问题来解决：给定目标任务、学习算法和候选数据，选择最大化模型性能的子集。这个框架避免了手动选择数据质量的概念，并明确地建模了学习过程如何使用训练数据点来预测目标任务。我们的方法显著提高了语言模型（LM）在预先指定的任务和以前不包括的任务上的性能。

    When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with "high quality" data sources may not increase (and can even hurt) performance compared to randomly selecting data.  To develop better methods for selecting data, we start by framing dataset selection as an optimization problem that we can directly solve for: given target tasks, a learning algorithm, and candidate data, select the subset that maximizes model performance. This framework thus avoids handpicked notions of data quality, and instead models explicitly how the learning process uses train datapoints to predict on the target tasks. Our resulting method greatly improves language model (LM) performance on both pre-specified tasks and previously
    
[^7]: 对于森林火灾检测中具有挑战性数据集的支持向量机（SVM）的性能分析

    Performance Analysis of Support Vector Machine (SVM) on Challenging Datasets for Forest Fire Detection. (arXiv:2401.12924v1 [stat.ML])

    [http://arxiv.org/abs/2401.12924](http://arxiv.org/abs/2401.12924)

    本文对于使用图像数据集进行森林火灾检测的支持向量机（SVM）进行了性能分析，并研究了关键因素如数据预处理、特征提取和模型训练。这项研究有助于开发高效的森林火灾检测系统。

    

    本文深入分析了使用图像数据集进行森林火灾检测的支持向量机（SVM）的性能和利用情况。随着森林火灾对生态系统和人类定居点的威胁日益增加，迅速准确的检测系统的需求至关重要。SVM以其强大的分类能力而闻名，在图像中识别与火灾相关的模式方面表现出熟练度。通过在标记数据上进行训练，SVM获得了识别与火灾相关的独特属性的能力，如火焰、烟雾或森林区域视觉特征的变化。本文全面研究了使用SVM的各个要素，包括数据预处理、特征提取和模型训练。严格评估了准确性、效率和实际适用性等参数。从这项研究中获得的知识有助于开发高效的森林火灾检测系统。

    This article delves into the analysis of performance and utilization of Support Vector Machines (SVMs) for the critical task of forest fire detection using image datasets. With the increasing threat of forest fires to ecosystems and human settlements, the need for rapid and accurate detection systems is of utmost importance. SVMs, renowned for their strong classification capabilities, exhibit proficiency in recognizing patterns associated with fire within images. By training on labeled data, SVMs acquire the ability to identify distinctive attributes associated with fire, such as flames, smoke, or alterations in the visual characteristics of the forest area. The document thoroughly examines the use of SVMs, covering crucial elements like data preprocessing, feature extraction, and model training. It rigorously evaluates parameters such as accuracy, efficiency, and practical applicability. The knowledge gained from this study aids in the development of efficient forest fire detection sy
    
[^8]: 用于解决一些随机最优控制问题的深度多任务神经网络

    Deep multitask neural networks for solving some stochastic optimal control problems. (arXiv:2401.12923v1 [stat.ML])

    [http://arxiv.org/abs/2401.12923](http://arxiv.org/abs/2401.12923)

    本文针对某些难以模拟底层状态变量的随机最优控制问题，引入了使用多任务神经网络的有效解决方案，并通过实验证明了该方法的优越性。

    

    大多数现有的基于神经网络的方法用于使用相关的反向动态规划原理解决随机最优控制问题，这些方法依赖于模拟底层状态变量的能力。然而，在某些问题中，这种模拟是不可行的，导致状态变量空间的离散化和需要为每个数据点训练一个神经网络。当处理大的状态变量空间时，这种方法在计算上变得低效。在本文中，我们考虑了一类这种类型的随机最优控制问题，并引入了一种使用多任务神经网络的有效解决方案。为了训练我们的多任务神经网络，我们引入了一种新的方案，在任务之间动态平衡学习。通过对真实世界的衍生品定价问题进行数值实验，我们证明了我们的方法优于最先进的方法。

    Most existing neural network-based approaches for solving stochastic optimal control problems using the associated backward dynamic programming principle rely on the ability to simulate the underlying state variables. However, in some problems, this simulation is infeasible, leading to the discretization of state variable space and the need to train one neural network for each data point. This approach becomes computationally inefficient when dealing with large state variable spaces. In this paper, we consider a class of this type of stochastic optimal control problems and introduce an effective solution employing multitask neural networks. To train our multitask neural network, we introduce a novel scheme that dynamically balances the learning across tasks. Through numerical experiments on real-world derivatives pricing problems, we prove that our method outperforms state-of-the-art approaches.
    
[^9]: 基于阻尼牛顿法的无模型$\delta$策略迭代用于非线性连续时间H∞跟踪控制

    Model-Free $\delta$-Policy Iteration Based on Damped Newton Method for Nonlinear Continuous-Time H$\infty$ Tracking Control. (arXiv:2401.12882v1 [cs.LG])

    [http://arxiv.org/abs/2401.12882](http://arxiv.org/abs/2401.12882)

    这篇论文提出了一种基于阻尼牛顿方法的无模型$\delta$策略迭代算法，用于解决未知连续时间非线性系统的H∞跟踪控制问题。该算法通过迭代求解广义跟踪Bellman方程，可以寻找出跟踪Hamilton-Jacobi-Isaac (HJI)方程的最优解。提供了基于策略学习和离策略学习的算法，其中离策略学习方法是一个无模型的算法。

    

    本论文提出了一种基于阻尼牛顿法的{\delta}-PI算法，用于未知连续时间非线性系统的H∞跟踪控制问题。通过使用折现性能函数和增广系统来得到跟踪Hamilton-Jacobi-Isaac (HJI)方程。跟踪HJI方程是一个非线性偏微分方程，传统的强化学习方法用于解决跟踪HJI方程主要基于牛顿法，通常只能满足局部收敛并且需要一个良好的初始猜测。在阻尼牛顿迭代算子方程的基础上，首先推导出了广义的跟踪Bellman方程。{\delta}-PI算法通过迭代求解广义跟踪Bellman方程来寻找跟踪HJI方程的最优解。分别提供了基于策略学习和离策略学习的{\delta}-PI强化学习方法。基于离策略学习的{\delta}-PI算法是一个无模型的算法。

    This paper presents a {\delta}-PI algorithm which is based on damped Newton method for the H{\infty} tracking control problem of unknown continuous-time nonlinear system. A discounted performance function and an augmented system are used to get the tracking Hamilton-Jacobi-Isaac (HJI) equation. Tracking HJI equation is a nonlinear partial differential equation, traditional reinforcement learning methods for solving the tracking HJI equation are mostly based on the Newton method, which usually only satisfies local convergence and needs a good initial guess. Based upon the damped Newton iteration operator equation, a generalized tracking Bellman equation is derived firstly. The {\delta}-PI algorithm can seek the optimal solution of the tracking HJI equation by iteratively solving the generalized tracking Bellman equation. On-policy learning and off-policy learning {\delta}-PI reinforcement learning methods are provided, respectively. Off-policy version {\delta}-PI algorithm is a model-fr
    
[^10]: 在数据流支持的移动众包协调中评估协作和自治代理

    Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported Coordination of Mobile Crowdsourcing. (arXiv:2401.12866v1 [cs.AI])

    [http://arxiv.org/abs/2401.12866](http://arxiv.org/abs/2401.12866)

    移动众包系统中任务分配和质量问题引起了研究者的关注。本文提出了一种基于数据流学习的方法来预测任务结果，并通过任务转移来解决任务分配中的问题。这些机制有助于提高移动众包系统的服务质量。

    

    移动众包是指完成任务需要众包工作者在按需劳动力中进行物理移动的系统。Evidence suggests that in such systems, tasks often get assigned to crowdworkers who struggle to complete those tasks successfully, resulting in high failure rates and low service quality. 提高服务质量的一个有希望的解决方案是不断适应任务分配，并在出现导致失败的事件时将任务转移给更合适的工作者，他们使用不同的路线或车辆。然而，在移动众包中实现任务转移是困难的，因为工作者是自治的，可能拒绝转移请求。此外，任务结果是不确定的，需要进行预测。在本文中，我们提出了不同的机制来实现移动众包中的结果预测和任务协调。首先，我们分析了不同的数据流学习方法来预测任务结果。其次，基于此我们p

    Mobile crowdsourcing refers to systems where the completion of tasks necessarily requires physical movement of crowdworkers in an on-demand workforce. Evidence suggests that in such systems, tasks often get assigned to crowdworkers who struggle to complete those tasks successfully, resulting in high failure rates and low service quality. A promising solution to ensure higher quality of service is to continuously adapt the assignment and respond to failure-causing events by transferring tasks to better-suited workers who use different routes or vehicles. However, implementing task transfers in mobile crowdsourcing is difficult because workers are autonomous and may reject transfer requests. Moreover, task outcomes are uncertain and need to be predicted. In this paper, we propose different mechanisms to achieve outcome prediction and task coordination in mobile crowdsourcing. First, we analyze different data stream learning approaches for the prediction of task outcomes. Second, based on
    
[^11]: 使用无人机高光谱成像对葡萄品种进行分类

    Classification of grapevine varieties using UAV hyperspectral imaging. (arXiv:2401.12851v1 [cs.CV])

    [http://arxiv.org/abs/2401.12851](http://arxiv.org/abs/2401.12851)

    该研究提出了一种使用无人机高光谱成像对17种红白葡萄品种进行分类的方法，相比于传统的破坏性方法，这种方法更高效和 less prohibitive，能够纠正和降低大量数据的采样率，并且能够处理葡萄品种高度相似的高光谱特征。

    

    不同葡萄品种的分类是精准葡萄栽培中一个相关的表型分析任务，因为它可以估计不同品种的葡萄园行的生长情况，并涉及到葡萄酒行业的其他应用。这项任务可以通过破坏性的方法来完成，需要耗时的数据收集和实验室分析等任务。然而，无人机提供了一种更高效、 less prohibitive的方法来收集高光谱数据，尽管可能会获得更嘈杂的数据。因此，第一个任务是处理这些数据以纠正和降低大量数据的采样率。此外，葡萄品种的高光谱特征非常相似。在这项工作中，提出了一种卷积神经网络（CNN）用于对17种红白葡萄品种进行分类。与分类单个样本不同，这些样本与其周围环境一起进行处理。因此，需要进行空间特征的提取。

    The classification of different grapevine varieties is a relevant phenotyping task in Precision Viticulture since it enables estimating the growth of vineyard rows dedicated to different varieties, among other applications concerning the wine industry. This task can be performed with destructive methods that require time-consuming tasks, including data collection and analysis in the laboratory. However, Unmanned Aerial Vehicles (UAV) provide a more efficient and less prohibitive approach to collecting hyperspectral data, despite acquiring noisier data. Therefore, the first task is the processing of these data to correct and downsample large amounts of data. In addition, the hyperspectral signatures of grape varieties are very similar. In this work, a Convolutional Neural Network (CNN) is proposed for classifying seventeen varieties of red and white grape variants. Rather than classifying single samples, these are processed together with their neighbourhood. Hence, the extraction of spa
    
[^12]: 通过非压缩二进制Bellman算子学习安全评论家

    Learning safety critics via a non-contractive binary bellman operator. (arXiv:2401.12849v1 [cs.LG])

    [http://arxiv.org/abs/2401.12849](http://arxiv.org/abs/2401.12849)

    本文提出了一种通过利用安全属性的二进制安全评论家算子学习避免达到不安全区域的方法，并研究了其特性和固定点。

    

    在强化学习中，自然地强制执行有限失败的安全性是一个阻碍其在实际应用中使用的核心挑战。一个具有广泛实际意义的安全性概念是避免（不安全的）状态空间的区域。尽管这样的安全目标可以通过类似动作值函数的安全评论家来捕捉，但相关的算子缺乏古典Bellman算子所具有的收缩和唯一性特性。在这项工作中，我们通过利用安全是二进制属性来克服安全评论家算子的非收缩性。为此，我们研究了与寻求避免达到不安全区域的确定性动态系统相关的二进制安全评论家的特性。我们制定了相应的安全性二进制贝尔曼方程（B2E）并研究了其特性。虽然结果算子仍然是非压缩的，但我们完全表征了其表示的固定点，除了...

    The inability to naturally enforce safety in Reinforcement Learning (RL), with limited failures, is a core challenge impeding its use in real-world applications. One notion of safety of vast practical relevance is the ability to avoid (unsafe) regions of the state space. Though such a safety goal can be captured by an action-value-like function, a.k.a. safety critics, the associated operator lacks the desired contraction and uniqueness properties that the classical Bellman operator enjoys. In this work, we overcome the non-contractiveness of safety critic operators by leveraging that safety is a binary property. To that end, we study the properties of the binary safety critic associated with a deterministic dynamical system that seeks to avoid reaching an unsafe region. We formulate the corresponding binary Bellman equation (B2E) for safety and study its properties. While the resulting operator is still non-contractive, we fully characterize its fixed points representing--except for a 
    
[^13]: 基于嵌入距离计算的时间图

    An embedding-based distance for temporal graphs. (arXiv:2401.12843v1 [cs.SI])

    [http://arxiv.org/abs/2401.12843](http://arxiv.org/abs/2401.12843)

    本研究提出了一种基于图嵌入的时间图距离计算方法，能够有效区分具有不同结构和时间属性的图，适用于大规模时间图。

    

    我们基于使用时间尊重的随机游走构建的图嵌入来定义了一种时间图之间的距离。我们研究了匹配图和不匹配图的情况，当存在已知的节点关系时，以及当不存在该关系并且图可能具有不同的大小时的情况。通过使用真实和合成的时间网络数据，我们展示了我们所提出的距离定义的优势，表明它能够区分具有不同结构和时间属性的图。利用最先进的机器学习技术，我们提出了一种适用于大规模时间图的距离计算的高效实现。

    We define a distance between temporal graphs based on graph embeddings built using time-respecting random walks. We study both the case of matched graphs, when there exists a known relation between the nodes, and the unmatched case, when such a relation is unavailable and the graphs may be of different sizes. We illustrate the interest of our distance definition, using both real and synthetic temporal network data, by showing its ability to discriminate between graphs with different structural and temporal properties. Leveraging state-of-the-art machine learning techniques, we propose an efficient implementation of distance computation that is viable for large-scale temporal graphs.
    
[^14]: 迭代相关矩阵分析（IRMA）用于识别具有类别区分能力的子空间

    Iterated Relevance Matrix Analysis (IRMA) for the identification of class-discriminative subspaces. (arXiv:2401.12842v1 [cs.LG])

    [http://arxiv.org/abs/2401.12842](http://arxiv.org/abs/2401.12842)

    迭代相关矩阵分析（IRMA）通过迭代识别类别特定信息的线性子空间并用于降维和训练鲁棒分类器。

    

    我们引入并研究了广义矩阵学习矢量量化的迭代应用，用于分析分类问题中的特征相关性，以及构建具有类别区分能力的子空间。建议的迭代相关矩阵分析（IRMA）使用广义矩阵学习矢量量化（GMLVQ）来识别表示所考虑数据集的分类特定信息的线性子空间。通过迭代确定一个新的具有区分能力的子空间，同时投影出所有先前识别出的子空间，可以找到一个包含所有类别特定信息的组合子空间。这有助于对特征相关性进行详细分析，并实现标记数据集的改进低维表示和可视化。此外，基于IRMA的具有类别区分能力的子空间可用于降维和训练具有潜在改进性能的鲁棒分类器。

    We introduce and investigate the iterated application of Generalized Matrix Learning Vector Quantizaton for the analysis of feature relevances in classification problems, as well as for the construction of class-discriminative subspaces. The suggested Iterated Relevance Matrix Analysis (IRMA) identifies a linear subspace representing the classification specific information of the considered data sets using Generalized Matrix Learning Vector Quantization (GMLVQ). By iteratively determining a new discriminative subspace while projecting out all previously identified ones, a combined subspace carrying all class-specific information can be found. This facilitates a detailed analysis of feature relevances, and enables improved low-dimensional representations and visualizations of labeled data sets. Additionally, the IRMA-based class-discriminative subspace can be used for dimensionality reduction and the training of robust classifiers with potentially improved performance.
    
[^15]: 提升下一个目的地预测：一种基于真实航空数据的新颖LSTM方法

    Enhancing Next Destination Prediction: A Novel LSTM Approach Using Real-World Airline Data. (arXiv:2401.12830v1 [cs.LG])

    [http://arxiv.org/abs/2401.12830](http://arxiv.org/abs/2401.12830)

    提出了一种新颖的基于LSTM的模型架构，通过准确捕捉旅行数据中的序列模式和依赖关系，实现了对个人旅行者未来目的地的准确预测。实验结果表明该模型在不同数据规模和性能指标上表现出色，为提升目的地预测方法做出了贡献，并使公司能够提供个性化推荐和优化客户体验。

    

    在现代交通行业中，准确预测旅行者的下一个目的地为公司带来很多好处，例如提高客户满意度和定向营销。本研究旨在开发一种准确捕捉旅行数据中的序列模式和依赖关系的模型，实现对个人旅行者未来目的地的准确预测。为了实现这一目标，提出了一种基于长短期记忆（LSTM）的滑动窗口方法的新颖模型架构，用于交通业中的目的地预测。实验结果表明，该模型在不同数据规模和性能指标上取得了令人满意的表现和高分数。本研究在推进目的地预测方法方面做出了贡献，使公司能够提供个性化推荐并优化动态旅行环境中的客户体验。

    In the modern transportation industry, accurate prediction of travelers' next destinations brings multiple benefits to companies, such as customer satisfaction and targeted marketing. This study focuses on developing a precise model that captures the sequential patterns and dependencies in travel data, enabling accurate predictions of individual travelers' future destinations. To achieve this, a novel model architecture with a sliding window approach based on Long Short-Term Memory (LSTM) is proposed for destination prediction in the transportation industry. The experimental results highlight satisfactory performance and high scores achieved by the proposed model across different data sizes and performance metrics. This research contributes to advancing destination prediction methods, empowering companies to deliver personalized recommendations and optimize customer experiences in the dynamic travel landscape.
    
[^16]: MAPPING: 使用有限敏感信息泄露的去偏置图神经网络进行公平节点分类

    MAPPING: Debiasing Graph Neural Networks for Fair Node Classification with Limited Sensitive Information Leakage. (arXiv:2401.12824v1 [cs.LG])

    [http://arxiv.org/abs/2401.12824](http://arxiv.org/abs/2401.12824)

    本文提出了一种使用有限敏感信息泄露的去偏置图神经网络进行公平节点分类的方法，该方法克服了非独立同分布图结构中的拓扑依赖问题，并构建了一个模型无关的去偏置框架，以防止下游误用并提高训练的可靠性。

    

    尽管在各种基于网络的应用中取得了显著的成功，但图神经网络（GNN）继承并进一步加剧了历史上的偏见和社会刻板印象，这严重阻碍了它们在在线临床诊断、金融信贷等高风险领域的部署。然而，当前的公平性研究主要集中在独立同分布数据上，并不能简单地复制到具有拓扑依赖的非独立同分布图结构中。现有的公平图学习通常偏好于使用成对约束来实现公平性，但无法克服维度限制并将其推广到多个敏感属性；此外，大多数研究集中在处理技术上来强制并调整公平性，在预处理阶段构建一个模型无关的去偏置GNN框架，以防止下游误用并提高训练的可靠性在先前的工作中，GNN往往倾向于增强公平性或增加预测性能，因此在二者之间进行全面权衡仍然是一个挑战。

    Despite remarkable success in diverse web-based applications, Graph Neural Networks(GNNs) inherit and further exacerbate historical discrimination and social stereotypes, which critically hinder their deployments in high-stake domains such as online clinical diagnosis, financial crediting, etc. However, current fairness research that primarily craft on i.i.d data, cannot be trivially replicated to non-i.i.d. graph structures with topological dependence among samples. Existing fair graph learning typically favors pairwise constraints to achieve fairness but fails to cast off dimensional limitations and generalize them into multiple sensitive attributes; besides, most studies focus on in-processing techniques to enforce and calibrate fairness, constructing a model-agnostic debiasing GNN framework at the pre-processing stage to prevent downstream misuses and improve training reliability is still largely under-explored. Furthermore, previous work on GNNs tend to enhance either fairness or 
    
[^17]: 基于深度强化学习算法的深度学习模拟器用于废水处理中磷去除过程控制

    Deep Learning Based Simulators for the Phosphorus Removal Process Control in Wastewater Treatment via Deep Reinforcement Learning Algorithms. (arXiv:2401.12822v1 [eess.SY])

    [http://arxiv.org/abs/2401.12822](http://arxiv.org/abs/2401.12822)

    本研究使用深度强化学习算法创建了磷去除过程控制的模拟器，通过试错来学习控制策略。然而，模拟器的性能在更长时间范围内受到模型误差累积的限制。

    

    磷去除对于废水处理至关重要，以减少对有限资源的依赖。深度强化学习是一种可以通过试错来学习控制策略的机器学习技术，可以优化复杂和非线性的系统，包括废水处理厂的处理过程。然而，将深度强化学习应用于化学和生物过程是具有挑战性的，因为需要准确的模拟器。本研究训练了六个模型来识别磷去除过程，并使用它们创建了一个用于深度强化学习环境的模拟器。虽然模型的准确性很高（>97%），但不确定性和错误预测行为限制了它们作为模拟器在更长时间范围内的性能。模型预测误差的累积被确定为这个问题的原因之一。这种改进过程控制的方法涉及为深度强化学习算法创建模拟环境，使用监控与数据采集系统（SCADA）数据。

    Phosphorus removal is vital in wastewater treatment to reduce reliance on limited resources. Deep reinforcement learning (DRL) is a machine learning technique that can optimize complex and nonlinear systems, including the processes in wastewater treatment plants, by learning control policies through trial and error. However, applying DRL to chemical and biological processes is challenging due to the need for accurate simulators. This study trained six models to identify the phosphorus removal process and used them to create a simulator for the DRL environment. Although the models achieved high accuracy (>97%), uncertainty and incorrect prediction behavior limited their performance as simulators over longer horizons. Compounding errors in the models' predictions were identified as one of the causes of this problem. This approach for improving process control involves creating simulation environments for DRL algorithms, using data from supervisory control and data acquisition (SCADA) sys
    
[^18]: DatUS^2: 使用预训练的自监督视觉Transformer进行数据驱动的无监督语义分割

    DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained Self-supervised Vision Transformer. (arXiv:2401.12820v1 [cs.CV])

    [http://arxiv.org/abs/2401.12820](http://arxiv.org/abs/2401.12820)

    本文提出了一种新颖的数据驱动无监督语义分割方法（DatUS^2），通过自监督训练生成语义一致且密集的伪标注分割掩模，用于评估视觉特征质量。

    

    连续提出了几种自监督训练方案的建议，使得开发通用基础模型更近了一步。在这个过程中，无监督下游任务被认为是验证通过自监督训练方案学习到的视觉特征质量的方法之一。然而，尚未探索无监督的密集语义分割作为一种下游任务，它可以利用和评估自监督训练过程中引入的语义信息在补丁级特征表示中的质量。因此，本文提出了一种新颖的基于数据驱动的无监督语义分割方法（DatUS^2）作为一种下游任务。DatUS^2为未标记的图像数据集生成了语义一致且密集的伪标注分割掩模，而不使用任何视觉先验或同步数据。我们将这些伪标注分割掩模与真实掩模进行了比较以进行评估。

    Successive proposals of several self-supervised training schemes continue to emerge, taking one step closer to developing a universal foundation model. In this process, the unsupervised downstream tasks are recognized as one of the evaluation methods to validate the quality of visual features learned with a self-supervised training scheme. However, unsupervised dense semantic segmentation has not been explored as a downstream task, which can utilize and evaluate the quality of semantic information introduced in patch-level feature representations during self-supervised training of a vision transformer. Therefore, this paper proposes a novel data-driven approach for unsupervised semantic segmentation (DatUS^2) as a downstream task. DatUS^2 generates semantically consistent and dense pseudo annotate segmentation masks for the unlabeled image dataset without using any visual-prior or synchronized data. We compare these pseudo-annotated segmentation masks with ground truth masks for evalua
    
[^19]: 动态层绑定用于参数高效的Transformer

    Dynamic Layer Tying for Parameter-Efficient Transformers. (arXiv:2401.12819v1 [cs.LG])

    [http://arxiv.org/abs/2401.12819](http://arxiv.org/abs/2401.12819)

    本论文提出了一种动态层绑定的方法，通过使用强化学习动态选择层并将它们绑定在一起，来减少深度Transformer网络中的可训练参数数量并提高性能。

    

    在减少深度Transformer网络中可训练参数的过程中，我们使用强化学习在训练期间动态选择层并将它们绑定在一起。每隔一段时间，RL agent会被询问是否独立训练每个层$i$，还是复制前一个层$j<i$的权重。这样做有助于共享权重，减少可训练参数的数量，同时也作为一种有效的正则化技术。实验评估验证了我们的模型在困惑度方面略优于基准Transformer模型，并且显著减少了可训练参数的数量。特别是，在训练期间的内存消耗比常规训练方法少一个数量级。

    In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer $i$ independently or to copy the weights of a previous layer $j<i$. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.
    
[^20]: 二进制结构的物理信息神经网络用于解决具有快速变化解的方程

    Binary structured physics-informed neural networks for solving equations with rapidly changing solutions. (arXiv:2401.12806v1 [cs.LG])

    [http://arxiv.org/abs/2401.12806](http://arxiv.org/abs/2401.12806)

    本论文提出了一种二进制结构的物理信息神经网络框架，通过利用二进制结构来捕捉局部特征，并解决了传统物理信息神经网络在处理具有快速变化解的方程时的困难。

    

    物理信息神经网络(PINNs)，基于深度学习，已成为解决偏微分方程(PDEs)的一种有前途的方法。通过将PDEs描述的物理信息嵌入前馈神经网络中，PINNs被训练为替代模型，以近似解决方案而无需标签数据。然而，尽管PINNs表现出了卓越的性能，但它们在处理具有快速变化解的方程时可能会遇到困难。这些困难包括收敛速度慢、易陷入局部最小值和解决精度降低。为了解决这些问题，我们提出了一种二进制结构的物理信息神经网络(BsPINN)框架，该框架使用二进制结构的神经网络(BsNN)作为神经网络组件。通过利用二进制结构，BsPINNs在捕捉局部特征方面表现出色

    Physics-informed neural networks (PINNs), rooted in deep learning, have emerged as a promising approach for solving partial differential equations (PDEs). By embedding the physical information described by PDEs into feedforward neural networks, PINNs are trained as surrogate models to approximate solutions without the need for label data. Nevertheless, even though PINNs have shown remarkable performance, they can face difficulties, especially when dealing with equations featuring rapidly changing solutions. These difficulties encompass slow convergence, susceptibility to becoming trapped in local minima, and reduced solution accuracy. To address these issues, we propose a binary structured physics-informed neural network (BsPINN) framework, which employs binary structured neural network (BsNN) as the neural network component. By leveraging a binary structure that reduces inter-neuron connections compared to fully connected neural networks, BsPINNs excel in capturing the local features 
    
[^21]: 5G NR PRACH接收的增强：一种基于AI/ML的方法

    Enhancements for 5G NR PRACH Reception: An AI/ML Approach. (arXiv:2401.12803v1 [cs.IT])

    [http://arxiv.org/abs/2401.12803](http://arxiv.org/abs/2401.12803)

    这项研究提出了一种基于AI/ML的方法来增强5G NR PRACH接收，通过使用两个神经网络分别估计RAPID和TA，与传统相关方法相比，实验证明了这种方法在性能上的改进。

    

    随机接入是使用户设备（UE）与基站（gNB）进行最初连接的重要步骤。UE通过嵌入在已知基序列的相位旋转中的前导索引（RAPID）来进行自我识别，并将其发送到物理随机接入通道（PRACH）。PRACH上的信号还能够估计由UE位置引起的传播时延（常称为时间提前，TA）。传统接收器使用基于相关技术的方法来估计RAPID和TA。本文介绍了一种使用AI/ML模型的替代接收器方法，其中提出了两个神经网络，一个用于RAPID，一个用于TA。与其他研究不同，这两个模型可以并行运行，而不是顺序运行。通过使用模拟数据和实际硬件捕获的实验，我们证明了基于AI/ML的技术相对于传统的相关方法具有更好的性能。

    Random Access is an important step in enabling the initial attachment of a User Equipment (UE) to a Base Station (gNB). The UE identifies itself by embedding a Preamble Index (RAPID) in the phase rotation of a known base sequence, which it transmits on the Physical Random Access Channel (PRACH). The signal on the PRACH also enables the estimation of propagation delay, often known as Timing Advance (TA), which is induced by virtue of the UE's position. Traditional receivers estimate the RAPID and TA using correlation-based techniques. This paper presents an alternative receiver approach that uses AI/ML models, wherein two neural networks are proposed, one for the RAPID and one for the TA. Different from other works, these two models can run in parallel as opposed to sequentially. Experiments with both simulated data and over-the-air hardware captures highlight the improved performance of the proposed AI/ML-based techniques compared to conventional correlation methods.
    
[^22]: 深度学习在集成感知与通信系统中基于目标到用户关联的应用

    Deep Learning-based Target-To-User Association in Integrated Sensing and Communication Systems. (arXiv:2401.12801v1 [cs.NI])

    [http://arxiv.org/abs/2401.12801](http://arxiv.org/abs/2401.12801)

    本文提出了一种深度学习方法，用于在集成感知和通信系统中将雷达目标与通信用户设备进行关联。该方法通过对雷达数据进行处理，实现了联合多目标检测和波束推理。这一方法在主动切换和波束预测等通信任务中具有潜在应用价值。

    

    在集成感知与通信（ISAC）系统中，将雷达目标与通信用户设备（UEs）进行匹配对于几种通信任务是有意义的，如主动切换和波束预测。本文考虑了一个雷达辅助通信系统，一个基站（BS）配备有多输入多输出（MIMO）雷达，雷达具有双重目标：（i）将车辆雷达目标与通信波束空间中的车辆设备（VEs）关联起来，（ii）根据雷达数据预测每个VE的波束形成矢量。提出的目标到用户（T2U）关联分为两个阶段。首先，通过距离-角度图像检测车辆雷达目标，并为每个目标估计一个波束形成矢量。然后，将推断得到的每个目标的波束形成矢量与BS用于通信的波束形成矢量进行匹配，以执行目标到用户（T2U）关联。通过修改你只看脸部网络（YOLO）算法实现了联合多目标检测和波束推理。

    In Integrated Sensing and Communication (ISAC) systems, matching the radar targets with communication user equipments (UEs) is functional to several communication tasks, such as proactive handover and beam prediction. In this paper, we consider a radar-assisted communication system where a base station (BS) is equipped with a multiple-input-multiple-output (MIMO) radar that has a double aim: (i) associate vehicular radar targets to vehicular equipments (VEs) in the communication beamspace and (ii) predict the beamforming vector for each VE from radar data. The proposed target-to-user (T2U) association consists of two stages. First, vehicular radar targets are detected from range-angle images, and, for each, a beamforming vector is estimated. Then, the inferred per-target beamforming vectors are matched with the ones utilized at the BS for communication to perform target-to-user (T2U) association. Joint multi-target detection and beam inference is obtained by modifying the you only look
    
[^23]: 物理层的深度学习：数据驱动的端到端通信系统及其支持的语义应用综述

    Deep Learning in Physical Layer: Review on Data Driven End-to-End Communication Systems and their Enabling Semantic Applications. (arXiv:2401.12800v1 [cs.NI])

    [http://arxiv.org/abs/2401.12800](http://arxiv.org/abs/2401.12800)

    这篇论文综述了物理层的深度学习在数据驱动的端到端通信系统中的应用，以及它们所支持的语义应用。通过深度学习的表示学习，这些系统表现出了增强的适应性和性能，能够理解和适应数据传输的上下文和意图。

    

    深度学习已经通过数据驱动的端到端学习和优化物理层实现无线通信系统的范式转变。通过利用深度学习的表示学习，端到端系统在复杂的无线环境中表现出了增强的适应性和性能，满足了5G及其以上网络系统和应用的需求。数据驱动技术在物理层的发展使得在文本、图像、音频、视频和多模态传输等各种模态下实现了高级的语义应用。这些应用从传统的比特级通信转变为语义级智能通信系统，能够理解和适应数据传输的上下文和意图。虽然物理层作为数据驱动的端到端通信的深度学习架构是实现语义通信系统的关键因素，近年来有各种研究分别对它们进行了调查，

    Deep Learning (DL) has enabled a paradigm shift in wireless communication system with data driven end-to-end (E2E) learning and optimization of the Physical Layer (PHY). By leveraging the representation learning of DL, E2E systems exhibit enhanced adaptability and performance in complex wireless environments, fulfilling the demands of 5G and beyond network systems and applications. The evolution of data-driven techniques in the PHY has enabled advanced semantic applications across various modalities including text, image, audio, video, and multi-modal transmissions. These applications transcend from traditional bit-level communication to semantic-level intelligent communication systems, which are capable of understanding and adapting to the context and intent of the data transmission. Although PHY as a DL architecture for data-driven E2E communication is a key factor in enabling semantic communication systems (SemCom), and various studies in recent years have surveyed them separately, 
    
[^24]: MORPH：为恶意软件检测提供自动概念漂移适应的方法

    MORPH: Towards Automated Concept Drift Adaptation for Malware Detection. (arXiv:2401.12790v1 [cs.LG])

    [http://arxiv.org/abs/2401.12790](http://arxiv.org/abs/2401.12790)

    MORPH是一种专为神经网络设计的有效的基于伪标签的概念漂移适应方法，通过重新训练模型以适应数据分布变化，它在恶意软件检测中能够有效缓解概念漂移，并且能够降低注释工作量，相比现有方法在准确性和鲁棒性方面有显著的改进。

    

    概念漂移对于恶意软件检测来说是一个重要的挑战，因为训练好的机器学习模型的性能会随着时间的推移而下降，使其变得不实用。尽管之前的恶意软件概念漂移适应研究主要集中在主动学习上，即通过选择代表性样本来更新模型，但自训练已经成为缓解概念漂移的一种有希望的方法。自训练是指使用伪标签重新训练模型以适应数据分布的变化。在这项研究中，我们提出了一种名为MORPH的有效基于伪标签的概念漂移适应方法，专门为神经网络设计。通过对Android和Windows恶意软件数据集的大量实验分析，我们证明了我们的方法在缓解概念漂移方面的有效性。我们的方法在与主动学习相结合时，降低了注释工作量。此外，我们的方法在准确性和鲁棒性方面显著优于现有的方法。

    Concept drift is a significant challenge for malware detection, as the performance of trained machine learning models degrades over time, rendering them impractical. While prior research in malware concept drift adaptation has primarily focused on active learning, which involves selecting representative samples to update the model, self-training has emerged as a promising approach to mitigate concept drift. Self-training involves retraining the model using pseudo labels to adapt to shifting data distributions. In this research, we propose MORPH -- an effective pseudo-label-based concept drift adaptation method specifically designed for neural networks. Through extensive experimental analysis of Android and Windows malware datasets, we demonstrate the efficacy of our approach in mitigating the impact of concept drift. Our method offers the advantage of reducing annotation efforts when combined with active learning. Furthermore, our method significantly improves over existing works in au
    
[^25]: 深度学习方法在光电容积法数据中的应用综述

    A Review of Deep Learning Methods for Photoplethysmography Data. (arXiv:2401.12783v1 [cs.AI])

    [http://arxiv.org/abs/2401.12783](http://arxiv.org/abs/2401.12783)

    本综述系统地回顾了自2017年至2023年期间应用深度学习模型处理光电容积法数据的论文。研究发现，深度学习在个人健康管理和其他应用中具有显著成果。根据任务的不同，这些论文被分为医学相关和非医学相关两大类别，医学相关又细分为七个子组，包括血压分析...

    

    光电容积法（PPG）是一种非常有前景的设备，因为它具有便携性、用户友好操作和非侵入性测量多种生理信息的能力。深度学习的最新进展，通过利用PPG信号，展示了在个人健康管理和其他多方面应用任务上取得了显著的成果。本综述系统地回顾了自2017年1月1日至2023年7月31日期间在Google学术、PubMed和Dimensions发表的应用深度学习模型处理PPG数据的论文。每篇论文从任务、模型和数据三个关键角度进行分析。最终提取了193篇论文，其中使用了不同的深度学习框架来处理PPG信号。根据这些论文所涉及的任务，我们将它们分为两大类别：医学相关和非医学相关。医学相关任务进一步分为七个子组，包括血压分析...

    Photoplethysmography (PPG) is a highly promising device due to its advantages in portability, user-friendly operation, and non-invasive capabilities to measure a wide range of physiological information. Recent advancements in deep learning have demonstrated remarkable outcomes by leveraging PPG signals for tasks related to personal health management and other multifaceted applications. In this review, we systematically reviewed papers that applied deep learning models to process PPG data between January 1st of 2017 and July 31st of 2023 from Google Scholar, PubMed and Dimensions. Each paper is analyzed from three key perspectives: tasks, models, and data. We finally extracted 193 papers where different deep learning frameworks were used to process PPG signals. Based on the tasks addressed in these papers, we categorized them into two major groups: medical-related, and non-medical-related. The medical-related tasks were further divided into seven subgroups, including blood pressure anal
    
[^26]: DeepRicci:用于缓解过度挤压的自监督图结构-特征协同精化技术

    DeepRicci: Self-supervised Graph Structure-Feature Co-Refinement for Alleviating Over-squashing. (arXiv:2401.12780v1 [cs.LG])

    [http://arxiv.org/abs/2401.12780](http://arxiv.org/abs/2401.12780)

    DeepRicci是一种自监督图结构-特征协同精化技术，旨在缓解典型GNNs中的过度挤压问题。它通过考虑Ricci曲率来改进GNNs的性能，并提出了一个自监督黎曼模型DeepRicci来解决这些挑战。

    

    图神经网络（GNNs）在图形学习和挖掘方面展示出强大的能力，而图结构学习（GSL）在提升GNNs的性能方面起着重要作用。在文献中，大多数GSL解决方案要么主要关注具有任务特定监督（如节点分类）的结构精化问题，要么忽视GNNs自身的固有缺陷（如过度挤压），导致尽管采用了精致设计，但性能亚优。鉴于这些限制，我们提出了研究用于有效缓解典型GNNs中的过度挤压问题的自监督图结构-特征协同精化技术。在本文中，我们从黎曼几何中基本不同的角度来考虑黎曼曲率，其中我们遇到了建模、利用和计算黎曼曲率的挑战。为了解决这些挑战，我们提出了一个自监督黎曼模型，DeepRicci。

    Graph Neural Networks (GNNs) have shown great power for learning and mining on graphs, and Graph Structure Learning (GSL) plays an important role in boosting GNNs with a refined graph. In the literature, most GSL solutions either primarily focus on structure refinement with task-specific supervision (i.e., node classification), or overlook the inherent weakness of GNNs themselves (e.g., over-squashing), resulting in suboptimal performance despite sophisticated designs. In light of these limitations, we propose to study self-supervised graph structure-feature co-refinement for effectively alleviating the issue of over-squashing in typical GNNs. In this paper, we take a fundamentally different perspective of the Ricci curvature in Riemannian geometry, in which we encounter the challenges of modeling, utilizing and computing Ricci curvature. To tackle these challenges, we present a self-supervised Riemannian model, DeepRicci. Specifically, we introduce a latent Riemannian space of heterog
    
[^27]: 快速非线性的两时间尺度随机逼近：实现$\mathcal{O}(1/k)$有限样本复杂度

    Fast Nonlinear Two-Time-Scale Stochastic Approximation: Achieving $\mathcal{O}(1/k)$ Finite-Sample Complexity. (arXiv:2401.12764v1 [math.OC])

    [http://arxiv.org/abs/2401.12764](http://arxiv.org/abs/2401.12764)

    本文提出了一种新型的两时间尺度随机逼近方法，用于寻找耦合非线性算子的根，并且在强单调条件下证明了该方法的优化收敛速率为$\mathcal{O}(1/k)$。

    

    本文提出了一种新型的两时间尺度随机逼近方法，用于寻找耦合非线性算子的根，仅假设可以观测到这些算子的噪声样本。我们的关键思想是利用经典的Ruppert-Polyak平均技术通过样本动态估计算子的值。然后，这些平均步骤的估计值将用于两时间尺度随机逼近更新以找到所需的解。我们的主要理论结果是在底层非线性算子的强单调条件下，所提出方法产生的迭代的均方误差以优化的速率$\mathcal{O}(1/k)$收敛于零，其中$k$为迭代次数。我们的结果显著改进了现有的两时间尺度随机逼近结果，最佳已知有限时间收敛速率为$\mathcal{O}(1/k^{2/3})$。

    This paper proposes to develop a new variant of the two-time-scale stochastic approximation to find the roots of two coupled nonlinear operators, assuming only noisy samples of these operators can be observed. Our key idea is to leverage the classic Ruppert-Polyak averaging technique to dynamically estimate the operators through their samples. The estimated values of these averaging steps will then be used in the two-time-scale stochastic approximation updates to find the desired solution. Our main theoretical result is to show that under the strongly monotone condition of the underlying nonlinear operators the mean-squared errors of the iterates generated by the proposed method converge to zero at an optimal rate $\mathcal{O}(1/k)$, where $k$ is the number of iterations. Our result significantly improves the existing result of two-time-scale stochastic approximation, where the best known finite-time convergence rate is $\mathcal{O}(1/k^{2/3})$.
    
[^28]: 关于探测轨迹在算法选择中的实用性

    On the Utility of Probing Trajectories for Algorithm-Selection. (arXiv:2401.12745v1 [cs.LG])

    [http://arxiv.org/abs/2401.12745](http://arxiv.org/abs/2401.12745)

    本论文提出了一种新颖的算法选择方法，通过使用对实例进行短期求解得到的探测轨迹来描述实例，并用于训练算法选择模型。该方法具有前景和新颖性。

    

    机器学习方法在算法选择中通常将描述实例的数据作为输入。输入数据可以是从实例描述或适应度景观中提取的特征，也可以是实例本身的直接表示，即图像或文本描述。无论选择的输入是什么，都存在这样一种隐含假设：相似的实例会引起算法相似的性能，并且模型能够学习到这种关系。我们认为，仅从实例角度来看算法选择可能会误导，因为它未能考虑算法对实例之间相似性的`看法'。我们提出了一种新颖的"算法中心"方法来描述实例，可以用于训练算法选择模型：具体而言，我们使用将一个求解器应用于实例一小段时间后计算得到的短探测轨迹。这种方法被证明是有前途的，并提供了一种新的算法选择方法。

    Machine-learning approaches to algorithm-selection typically take data describing an instance as input. Input data can take the form of features derived from the instance description or fitness landscape, or can be a direct representation of the instance itself, i.e. an image or textual description. Regardless of the choice of input, there is an implicit assumption that instances that are similar will elicit similar performance from algorithm, and that a model is capable of learning this relationship. We argue that viewing algorithm-selection purely from an instance perspective can be misleading as it fails to account for how an algorithm `views' similarity between instances. We propose a novel `algorithm-centric' method for describing instances that can be used to train models for algorithm-selection: specifically, we use short probing trajectories calculated by applying a solver to an instance for a very short period of time. The approach is demonstrated to be promising, providing co
    
[^29]: TNANet: 一种针对带有噪声生理数据的自杀倾向预测的时间噪声感知神经网络

    TNANet: A Temporal-Noise-Aware Neural Network for Suicidal Ideation Prediction with Noisy Physiological Data. (arXiv:2401.12733v1 [cs.CY])

    [http://arxiv.org/abs/2401.12733](http://arxiv.org/abs/2401.12733)

    TNANet是一种专为分析带噪声生理时间序列数据而设计的神经网络模型，通过合并先进的编码技术和置信度学习来提高自杀倾向预测的准确性。

    

    在存在固有噪声的情况下，深度学习模型的强健泛化仍然是一个重大挑战，尤其当标签是主观的并且噪声在自然环境中很难辨别时。这个问题在许多实际应用中特别明显。在本文中，我们解决了监测自杀倾向的特殊且重要的情景，其中时间序列数据，如光电容积图（PPG），易受此类噪声影响。当前的方法主要集中在图像和文本数据或处理人为引入的噪声上，忽视了时间序列分析中的自然噪声的复杂性。为了解决这个问题，我们引入了一种专门用于分析带有噪声的生理时间序列数据的新型神经网络模型，称为TNANet，它将先进的编码技术与置信度学习相结合，提高了预测准确性。我们的另一个贡献是收集了一个专门从现实环境中获取的PPG信号的数据集，用于自杀倾向的预测。

    The robust generalization of deep learning models in the presence of inherent noise remains a significant challenge, especially when labels are subjective and noise is indiscernible in natural settings. This problem is particularly pronounced in many practical applications. In this paper, we address a special and important scenario of monitoring suicidal ideation, where time-series data, such as photoplethysmography (PPG), is susceptible to such noise. Current methods predominantly focus on image and text data or address artificially introduced noise, neglecting the complexities of natural noise in time-series analysis. To tackle this, we introduce a novel neural network model tailored for analyzing noisy physiological time-series data, named TNANet, which merges advanced encoding techniques with confidence learning, enhancing prediction accuracy. Another contribution of our work is the collection of a specialized dataset of PPG signals derived from real-world environments for suicidal
    
[^30]: SHAP评分在可解释机器学习中的分布不确定性

    The Distributional Uncertainty of the SHAP score in Explainable Machine Learning. (arXiv:2401.12731v1 [cs.AI])

    [http://arxiv.org/abs/2401.12731](http://arxiv.org/abs/2401.12731)

    本研究提出了一个原则性框架，用于处理在未知实体群体分布下的SHAP评分问题。通过考虑一个不确定性区域，我们可以确定所有特征的SHAP评分的紧束范围。

    

    归属分数反映了输入实体中的特征值对机器学习模型输出的重要性。其中最受欢迎的评分之一是SHAP评分，它是合作博弈理论中Shapley值的具体实例。该评分的定义依赖于实体群体的概率分布。由于通常不知道精确的分布，因此需要主观地进行分配或从数据中进行估计，这可能会导致误导性的特征评分。在本文中，我们提出了一个基于不知道实体群体分布的SHAP评分推理的原则性框架。在我们的框架中，我们考虑一个包含潜在分布的不确定性区域，而特征的SHAP评分成为在该区域上定义的一个函数。我们研究了找到该函数的最大值和最小值的基本问题，这使我们能够确定所有特征的SHAP评分的紧束范围。

    Attribution scores reflect how important the feature values in an input entity are for the output of a machine learning model. One of the most popular attribution scores is the SHAP score, which is an instantiation of the general Shapley value used in coalition game theory. The definition of this score relies on a probability distribution on the entity population. Since the exact distribution is generally unknown, it needs to be assigned subjectively or be estimated from data, which may lead to misleading feature scores. In this paper, we propose a principled framework for reasoning on SHAP scores under unknown entity population distributions. In our framework, we consider an uncertainty region that contains the potential distributions, and the SHAP score of a feature becomes a function defined over this region. We study the basic problems of finding maxima and minima of this function, which allows us to determine tight ranges for the SHAP scores of all features. In particular, we pinp
    
[^31]: 通过生成合成数据和比例类平衡技术提高小目标的物体检测性能：在工业场景中的比较研究

    Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios. (arXiv:2401.12729v1 [cs.CV])

    [http://arxiv.org/abs/2401.12729](http://arxiv.org/abs/2401.12729)

    通过生成合成数据和比例类平衡技术，提高了针对小目标的物体检测性能。这项研究解决了工业场景中收集和注释小目标数据的难题，并讨论了比例类平衡技术的效果。

    

    目标检测在提取局部类别信息方面被证明是一种重要的计算机视觉方法，并在工业中有多种应用。尽管许多最先进的目标检测模型在中等和大型目标上表现良好，但它们在小目标上表现不足。在大多数工业应用场景中，收集和注释小目标数据是困难的，因为这需要耗费时间且容易出现人为错误。此外，这些数据集往往不平衡，经常导致模型收敛效果不佳。为了解决这个挑战，本研究提出了一种新颖的方法，通过注入额外的数据点来改善目标检测模型的性能。使用合成数据生成技术，可以最小化收集和注释小目标数据点的困难，并创建一个具有平衡分布的数据集。本文讨论了一种简单的比例类平衡技术的效果，以实现模型的有效收敛。

    Object Detection (OD) has proven to be a significant computer vision method in extracting localized class information and has multiple applications in the industry. Although many of the state-of-the-art (SOTA) OD models perform well on medium and large sized objects, they seem to under perform on small objects. In most of the industrial use cases, it is difficult to collect and annotate data for small objects, as it is time-consuming and prone to human errors. Additionally, those datasets are likely to be unbalanced and often result in an inefficient model convergence. To tackle this challenge, this study presents a novel approach that injects additional data points to improve the performance of the OD models. Using synthetic data generation, the difficulties in data collection and annotations for small object data points can be minimized and to create a dataset with balanced distribution. This paper discusses the effects of a simple proportional class-balancing technique, to enable be
    
[^32]: Falcon: 使用多臂赌博机进行公平主动学习

    Falcon: Fair Active Learning using Multi-armed Bandits. (arXiv:2401.12722v1 [cs.LG])

    [http://arxiv.org/abs/2401.12722](http://arxiv.org/abs/2401.12722)

    Falcon是一个使用多臂赌博机的公平主动学习框架，通过策略性样本选择来改善机器学习模型的公平性。它通过识别对于提高公平性最具信息量的“目标群体”样本，并采用一种试错方法来解决样本选择中没有ground truth标签的挑战。

    

    偏倚数据可能导致不公平的机器学习模型，强调在数据分析的开始阶段嵌入公平性的重要性，特别是在数据集的筛选和标定过程中。为此，我们提出了一种可扩展的公平主动学习框架Falcon。Falcon采用了一种以数据为中心的方法，通过策略性样本选择来改善机器学习模型的公平性。给定用户指定的群体公平度量，Falcon确定了对提高公平性最有信息量的“目标群体”样本（例如（属性=女性，标签=正面））。然而，由于在样本选择过程中不可用ground truth标签来定义这些目标群体，出现了挑战。为了解决这个问题，我们提出了一种新颖的试错方法，在预测标签与期望标签不同时并落在目标群体之外时，我们推迟使用该样本。我们还观察到这样做会产生权衡，选择更有信息量的样本会增加样本进入目标群体之外的可能性。

    Biased data can lead to unfair machine learning models, highlighting the importance of embedding fairness at the beginning of data analysis, particularly during dataset curation and labeling. In response, we propose Falcon, a scalable fair active learning framework. Falcon adopts a data-centric approach that improves machine learning model fairness via strategic sample selection. Given a user-specified group fairness measure, Falcon identifies samples from "target groups" (e.g., (attribute=female, label=positive)) that are the most informative for improving fairness. However, a challenge arises since these target groups are defined using ground truth labels that are not available during sample selection. To handle this, we propose a novel trial-and-error method, where we postpone using a sample if the predicted label is different from the expected one and falls outside the target group. We also observe the trade-off that selecting more informative samples results in higher likelihood o
    
[^33]: 使用机器学习从三维地震和井测试数据中预测气体圈闭

    Gas trap prediction from 3D seismic and well test data using machine learning. (arXiv:2401.12717v1 [physics.geo-ph])

    [http://arxiv.org/abs/2401.12717](http://arxiv.org/abs/2401.12717)

    本文通过选择具有已建立的气体饱和度和过滤特性的体积，使用数据处理方法和机器学习算法预测气体圈闭，并取得了高效的结果。

    

    本文旨在创建和应用一种方法论的方法，通过选择具有已建立的气体饱和度和过滤特性的体积来预测气体圈闭。该论文将创建的训练数据集用于数据处理方法和集成机器学习算法的顺序应用过程堆栈中。结果得到了一个校准的可能属于气田的研究空间的概率立方体。该方法的高效性在三口盲井的延迟测试样本上得到了展示。气体储层预测质量度量f1得分的最终值为0.893846.

    The aim of this work is to create and apply a methodological approach for predicting gas traps from 3D seismic data and gas well testing. The paper formalizes the approach to creating a training dataset by selecting volumes with established gas saturation and filtration properties within the seismic wavefield. The training dataset thus created is used in a process stack of sequential application of data processing methods and ensemble machine learning algorithms. As a result, a cube of calibrated probabilities of belonging of the study space to gas reservoirs was obtained. The high efficiency of this approach is shown on a delayed test sample of three wells (blind wells). The final value of the gas reservoir prediction quality metric f1 score was 0.893846.
    
[^34]: 当冗余性很重要：表示的机器教学

    When Redundancy Matters: Machine Teaching of Representations. (arXiv:2401.12711v1 [cs.LG])

    [http://arxiv.org/abs/2401.12711](http://arxiv.org/abs/2401.12711)

    传统的机器教学中，概念可以有许多等效的表示方式，这种冗余性对搜索空间有强烈影响。本文探索了教授表示的方法，并提出了几种教学模式，分析了它们在不同表示语言下的教学有效性提升。实验结果表明，Greedy模式能更好地处理冗余，但仍有改进空间。

    

    在传统的机器教学中，教师想要通过一个有限的示例集合来向学习者教授一个概念，即证明集。但是概念可以有许多等效的表示方式。这种冗余性强烈影响了搜索空间，以至于教师和学习者可能无法轻易确定每个表示的等价类。在这种常见情况下，我们探索了教授表示的想法。我们使用了几种利用表示和证明集大小的教学模式（Eager、Greedy和Optimal），并分析了对某些表示语言（DNF表达式和图灵完备的P3程序）的教学有效性的提升。我们的理论和实验结果表明，存在各种类型的冗余，通过这里引入的Greedy模式比Eager模式更好地处理，尽管两者都可能远离最优。对于P3程序，我们发现证明集通常是不同的。

    In traditional machine teaching, a teacher wants to teach a concept to a learner, by means of a finite set of examples, the witness set. But concepts can have many equivalent representations. This redundancy strongly affects the search space, to the extent that teacher and learner may not be able to easily determine the equivalence class of each representation. In this common situation, instead of teaching concepts, we explore the idea of teaching representations. We work with several teaching schemas that exploit representation and witness size (Eager, Greedy and Optimal) and analyze the gains in teaching effectiveness for some representational languages (DNF expressions and Turing-complete P3 programs). Our theoretical and experimental results indicate that there are various types of redundancy, handled better by the Greedy schema introduced here than by the Eager schema, although both can be arbitrarily far away from the Optimal. For P3 programs we found that witness sets are usuall
    
[^35]: 用于选择性分类的深度神经网络基准

    Deep Neural Network Benchmarks for Selective Classification. (arXiv:2401.12708v1 [cs.LG])

    [http://arxiv.org/abs/2401.12708](http://arxiv.org/abs/2401.12708)

    本论文研究了用于选择性分类的深度神经网络，目的是设计一种选择机制来平衡被拒绝的预测比例和所选预测的预测性能改进。

    

    随着机器学习模型在许多具有社会敏感性的任务中的部署增加，对可靠和可信预测的需求也日益增长。实现这些要求的一种方法是允许模型在存在高错误风险时放弃进行预测。这需要为模型添加选择机制，该机制选择模型将提供预测的例子。选择性分类框架旨在设计一个平衡被拒绝预测比例（即模型不进行预测的例子比例）与在所选预测上的预测性能改进之间的机制。存在多个选择性分类框架，其中大多数依赖于深度神经网络架构。然而，现有方法的实证评估仍局限于部分方法和设置之间的比较，给实践者提供了很少的见解。

    With the increasing deployment of machine learning models in many socially-sensitive tasks, there is a growing demand for reliable and trustworthy predictions. One way to accomplish these requirements is to allow a model to abstain from making a prediction when there is a high risk of making an error. This requires adding a selection mechanism to the model, which selects those examples for which the model will provide a prediction. The selective classification framework aims to design a mechanism that balances the fraction of rejected predictions (i.e., the proportion of examples for which the model does not make a prediction) versus the improvement in predictive performance on the selected predictions. Multiple selective classification frameworks exist, most of which rely on deep neural network architectures. However, the empirical evaluation of the existing approaches is still limited to partial comparisons among methods and settings, providing practitioners with little insight into 
    
[^36]: 基于能量的自动化模型评估

    Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])

    [http://arxiv.org/abs/2401.12689](http://arxiv.org/abs/2401.12689)

    提出了一种基于能量的自动化模型评估方法，通过建立关于个体样本相关信息的元分布统计量，能够更高效和有效地评估机器学习模型的性能，解决了AutoEval框架中的过度自信、存储和计算成本高等问题。

    

    传统的机器学习模型评估协议依赖于标记的、假设独立同分布的测试数据集，而这在实际应用中往往并不常见。自动模型评估（AutoEval）提出了一种替代传统工作流程的方法，通过形成一个接近预测性能的测试管线，而无需真实标签的存在。尽管AutoEval框架近年来取得了一些成功，但仍存在过度自信、存储和计算成本高的问题。因此，我们提出了一种新颖的度量方式——元分布能量（MDE），它可以使AutoEval框架更加高效和有效。MDE的核心是建立一个关于个体样本相关信息（能量）的元分布统计量，然后通过基于能量的学习提供更平滑的表示能力。我们通过将MDE与分类损失相连接，进一步提供了理论洞见。我们还提供了大量实验证据来验证我们的方法。

    The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive
    
[^37]: 使用数据驱动方法进行DVL校准

    DVL Calibration using Data-driven Methods. (arXiv:2401.12687v1 [cs.RO])

    [http://arxiv.org/abs/2401.12687](http://arxiv.org/abs/2401.12687)

    本文提出了一种使用数据驱动方法进行DVL校准的深度学习框架，通过模拟数据的实验结果表明，该方法在准确性和校准时间上分别比基于模型的方法提高了35%和80%。

    

    自主水下载具(AUVs)在各种水下应用中被广泛使用，从海底地图绘制到工业操作。在水下，AUV的导航解决方案通常依赖惯性传感器和多普勒速度日志(DVL)的融合。为了获得准确的DVL测量结果，需要在任务开始前进行校准过程。基于模型的校准方法包括利用全球导航卫星系统信号的滤波方法。在本文中，我们提出了一个端到端的深度学习框架用于校准过程。使用模拟数据，我们展示了我们提出的方法在准确性和所需校准时间上比基于模型的方法提高了35%和80%。

    Autonomous underwater vehicles (AUVs) are used in a wide range of underwater applications, ranging from seafloor mapping to industrial operations. While underwater, the AUV navigation solution commonly relies on the fusion between inertial sensors and Doppler velocity logs (DVL). To achieve accurate DVL measurements a calibration procedure should be conducted before the mission begins. Model-based calibration approaches include filtering approaches utilizing global navigation satellite system signals. In this paper, we propose an end-to-end deep-learning framework for the calibration procedure. Using stimulative data, we show that our proposed approach outperforms model-based approaches by 35% in accuracy and 80% in the required calibration time.
    
[^38]: 在稀疏图上学习平均场对局博弈：一种混合图形扩展方法

    Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach. (arXiv:2401.12686v1 [cs.MA])

    [http://arxiv.org/abs/2401.12686](http://arxiv.org/abs/2401.12686)

    这篇论文提出了一种在稀疏图上学习平均场对局博弈的新方法，通过引入图形扩展的概念，解决了现有方法对于稀疏网络拓扑结构的限制。

    

    学习大规模代理群体的行为是许多研究领域中的重要任务。虽然多代理强化学习（MARL）领域在解决这些系统方面取得了重要进展，但对于许多代理的解决方案通常在计算上是不可行的，且缺乏理论保证。平均场对局博弈（MFGs）解决了这两个问题，并且可以扩展到包括代理之间的网络结构的图形平均场对局博弈（GMFGs）。尽管具有诸多优点，但GMFGs的现实世界应用受到图形只能捕捉密集图的限制。由于大多数实验证明的网络显示出一定程度的稀疏性，例如幂律图，因此GMFG框架无法捕捉这些网络拓扑结构。因此，我们提出了一种新颖的图形对局博弈（GXMFGs）的概念，它建立在图论概念图形扩展（graphexes）基础上。图形扩展是稀疏图序列的极限对象，还具有其他一些理想特性，如sma

    Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the sma
    
[^39]: LLpowershap: 基于逻辑损失的自动Shapley值特征选择方法

    LLpowershap: Logistic Loss-based Automated Shapley Values Feature Selection Method. (arXiv:2401.12683v1 [cs.LG])

    [http://arxiv.org/abs/2401.12683](http://arxiv.org/abs/2401.12683)

    LLpowershap是一种基于逻辑损失的自动Shapley值特征选择方法，可以在选择的特征集合中识别出具有最小噪声的信息特征。

    

    Shapley值在机器学习中被广泛应用，不仅可以解释黑盒机器学习模型，还可以用于模型调试、敏感性和公平性分析，以及选择重要特征进行稳健建模和进一步分析。最近，已经提出了许多利用Shapley值的特征选择方法。本文介绍了一种新的特征选择方法LLpowershap，该方法利用基于损失的Shapley值在选择的特征集合中识别出具有最小噪声的信息特征。我们的模拟结果表明，与其他方法相比，LLpowershap不仅识别出更多的信息特征，而且输出的噪声特征较少。

    Shapley values have been used extensively in machine learning, not only to explain black box machine learning models, but among other tasks, also to conduct model debugging, sensitivity and fairness analyses and to select important features for robust modelling and for further follow-up analyses. Shapley values satisfy certain axioms that promote fairness in distributing contributions of features toward prediction or reducing error, after accounting for non-linear relationships and interactions when complex machine learning models are employed. Recently, a number of feature selection methods utilising Shapley values have been introduced. Here, we present a novel feature selection method, LLpowershap, which makes use of loss-based Shapley values to identify informative features with minimal noise among the selected sets of features. Our simulation results show that LLpowershap not only identifies higher number of informative features but outputs fewer noise features compared to other st
    
[^40]: Kriging 的扩展:一种新的对比性典型学习方法

    Non-Neighbors Also Matter to Kriging: A New Contrastive-Prototypical Learning. (arXiv:2401.12681v1 [cs.LG])

    [http://arxiv.org/abs/2401.12681](http://arxiv.org/abs/2401.12681)

    这篇论文提出了一种新的对比性典型学习方法，用于改进Kriging过程中邻居和非邻居的信息利用，从而提高属性估计的准确性。

    

    Kriging的目标是通过空间邻近或物理连接中的观测值来估计未采样地理位置的属性，从而有助于减轻由于部署不足的传感器引起的监测偏差。现有的工作假设邻居的信息为估计未观察到的目标的属性提供基础，而忽略了非邻居。然而，非邻居也可以提供有益的信息，而邻居也可能是误导性的。为此，我们提出了一种称为“对比性典型”自监督学习的扩展方法，用于改进邻居的有价值信息并回收非邻居的信息。作为预训练范式，我们从表示的新视角进行Kriging任务：我们首先学习强大而通用的表示，然后从表示中恢复属性。设计了一个邻居对比模块，通过缩小目标与邻居的表示距离，粗略地学习表示。

    Kriging aims at estimating the attributes of unsampled geo-locations from observations in the spatial vicinity or physical connections, which helps mitigate skewed monitoring caused by under-deployed sensors. Existing works assume that neighbors' information offers the basis for estimating the attributes of the unobserved target while ignoring non-neighbors. However, non-neighbors could also offer constructive information, and neighbors could also be misleading. To this end, we propose ``Contrastive-Prototypical'' self-supervised learning for Kriging (KCP) to refine valuable information from neighbors and recycle the one from non-neighbors. As a pre-trained paradigm, we conduct the Kriging task from a new perspective of representation: we aim to first learn robust and general representations and then recover attributes from representations. A neighboring contrastive module is designed that coarsely learns the representations by narrowing the representation distance between the target a
    
[^41]: 使用稳健加权分数进行高维二分类不平衡基因表达数据的特征选择

    Feature Selection via Robust Weighted Score for High Dimensional Binary Class-Imbalanced Gene Expression Data. (arXiv:2401.12667v1 [stat.ML])

    [http://arxiv.org/abs/2401.12667](http://arxiv.org/abs/2401.12667)

    本文提出了一种用于高维基因表达二分类问题的稳健加权分数（ROWSU）方法，解决了基因表达数据中高度倾斜的类别分布对分类算法性能的不利影响问题。

    

    本文提出了一种用于高维基因表达二分类问题的稳健加权分数（ROWSU）方法，用于选择最具有区分性的特征。该方法解决了基因表达数据中高度倾斜的类别分布对分类算法性能的不利影响问题。首先，通过从少数类别观测数据中合成生成数据点来平衡训练数据集。其次，采用贪婪搜索方法选择最小的基因子集。然后，引入一种新颖的加权稳健分数，其中权重由支持向量计算，以获得一组精炼的基因。基于这种方法得到的最高分数基因与贪婪搜索方法选择的最小基因子集相结合，形成最终的基因集合。这种新颖的方法确保在存在偏斜类别分布的情况下选择最具有区分性的基因。

    In this paper, a robust weighted score for unbalanced data (ROWSU) is proposed for selecting the most discriminative feature for high dimensional gene expression binary classification with class-imbalance problem. The method addresses one of the most challenging problems of highly skewed class distributions in gene expression datasets that adversely affect the performance of classification algorithms. First, the training dataset is balanced by synthetically generating data points from minority class observations. Second, a minimum subset of genes is selected using a greedy search approach. Third, a novel weighted robust score, where the weights are computed by support vectors, is introduced to obtain a refined set of genes. The highest-scoring genes based on this approach are combined with the minimum subset of genes selected by the greedy search approach to form the final set of genes. The novel method ensures the selection of the most discriminative genes, even in the presence of ske
    
[^42]: 将人类专业知识融入连续空间: 一种新颖的具有偏好预期改善的交互式贝叶斯优化框架

    Integrating Human Expertise in Continuous Spaces: A Novel Interactive Bayesian Optimization Framework with Preference Expected Improvement. (arXiv:2401.12662v1 [cs.RO])

    [http://arxiv.org/abs/2401.12662](http://arxiv.org/abs/2401.12662)

    提出了一种将人类专业知识与机器学习相结合的新型交互式贝叶斯优化框架，通过捕捉用户偏好和引入新的收益函数提高机器学习系统的效率。

    

    交互式机器学习（IML）旨在将人类专业知识与机器学习过程相结合。然而，大多数现有算法不能应用于实际场景，因为它们的状态空间和/或行为空间仅限于离散值。此外，所有现有方法之间的交互受到在多个建议之间做出决策的限制。因此，我们提出了一种基于贝叶斯优化（BO）的新型框架。交互式贝叶斯优化（IBO）实现了机器学习算法和人类之间的合作。该框架捕捉用户偏好，并提供界面给用户手动调整策略。此外，我们还加入了一种新的收益函数，偏好预期改善（PEI），通过用户偏好的概率模型来提高系统的效率。我们的方法旨在确保机器能从人类专业知识中受益，以实现更加协调和有效的学习过程。

    Interactive Machine Learning (IML) seeks to integrate human expertise into machine learning processes. However, most existing algorithms cannot be applied to Realworld Scenarios because their state spaces and/or action spaces are limited to discrete values. Furthermore, the interaction of all existing methods is restricted to deciding between multiple proposals. We therefore propose a novel framework based on Bayesian Optimization (BO). Interactive Bayesian Optimization (IBO) enables collaboration between machine learning algorithms and humans. This framework captures user preferences and provides an interface for users to shape the strategy by hand. Additionally, we've incorporated a new acquisition function, Preference Expected Improvement (PEI), to refine the system's efficiency using a probabilistic model of the user preferences. Our approach is geared towards ensuring that machines can benefit from human expertise, aiming for a more aligned and effective learning process. In the c
    
[^43]: 基于一致性增强的深度多视图聚类方法通过对比学习

    Consistency Enhancement-Based Deep Multiview Clustering via Contrastive Learning. (arXiv:2401.12648v1 [cs.LG])

    [http://arxiv.org/abs/2401.12648](http://arxiv.org/abs/2401.12648)

    本文提出了一种基于一致性增强的深度多视图聚类方法通过对比学习（CCEC）。该方法通过引入语义连接块并入特征表示中，以保持多个视图间的一致信息，并通过谱聚类改善聚类的表示过程。实验结果显示，该方法在多个数据集上的表现优于其他现有方法。

    

    多视图聚类（MVC）通过综合多个视图的信息，将数据样本分为有意义的聚类。而基于深度学习的方法在MVC场景中展现了强大的特征学习能力。然而，有效地泛化特征表示并保持一致性仍然是一个棘手的问题。此外，大多数基于对比学习的现有深度聚类方法在聚类过程中忽略了聚类表示的一致性。本文展示了如何解决上述问题，并提出了一种通过对比学习的一致增强型深度MVC方法（CCEC）。具体而言，将语义连接块并入特征表示中，以保持多个视图间的一致信息。此外，通过谱聚类改善聚类的表示过程，并提高了多个视图间的一致性。实验结果显示，我们的方法在多个数据集上的表现优于其他现有方法。

    Multiview clustering (MVC) segregates data samples into meaningful clusters by synthesizing information across multiple views. Moreover, deep learning-based methods have demonstrated their strong feature learning capabilities in MVC scenarios. However, effectively generalizing feature representations while maintaining consistency is still an intractable problem. In addition, most existing deep clustering methods based on contrastive learning overlook the consistency of the clustering representations during the clustering process. In this paper, we show how the above problems can be overcome and propose a consistent enhancement-based deep MVC method via contrastive learning (CCEC). Specifically, semantic connection blocks are incorporated into a feature representation to preserve the consistent information among multiple views. Furthermore, the representation process for clustering is enhanced through spectral clustering, and the consistency across multiple views is improved. Experiment
    
[^44]: 对于变动条件和不完全信道知识的深度学习辅助符号检测器的鲁棒性分析

    On the Robustness of Deep Learning-aided Symbol Detectors to Varying Conditions and Imperfect Channel Knowledge. (arXiv:2401.12645v1 [cs.IT])

    [http://arxiv.org/abs/2401.12645](http://arxiv.org/abs/2401.12645)

    本文研究了深度学习辅助符号检测器在对变动条件和不完全信道知识的情况下的鲁棒性。研究结果显示，该算法在学习噪声信道数据和不完全信道衰减特性方面表现出显著优势，但其持续性和适用性有待进一步研究。

    

    最近，引入了一种专门用于具有符号间干扰的信道的数据驱动型Bahl-Cocke-Jelinek-Raviv（BCJR）算法。这种被称为BCJRNet算法利用神经网络计算信道的似然概率。BCJRNet在应用于具有理想指数衰减特性的定态信道时表现出对信道衰减估计不准确的鲁棒性。然而，对于实际中常见的时变信道以及接收机只能获取到错误的信道参数的情况，BCJRNet的泛化能力尚未得到充分探索。本文的主要贡献是对现有文献中的结果进行扩展，涵盖了实际传输中出现的各种不完全信道知识情况。我们的研究结果表明，通过学习噪声信道数据和不完全信道衰减特性，在定态传输场景下，BCJRNet显著优于传统的BCJR算法。然而，这种优势是否可持续以及在其他情况下的适用性需要进一步研究。

    Recently, a data-driven Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm tailored to channels with intersymbol interference has been introduced. This so-called BCJRNet algorithm utilizes neural networks to calculate channel likelihoods. BCJRNet has demonstrated resilience against inaccurate channel tap estimations when applied to a time-invariant channel with ideal exponential decay profiles. However, its generalization capabilities for practically-relevant time-varying channels, where the receiver can only access incorrect channel parameters, remain largely unexplored. The primary contribution of this paper is to expand upon the results from existing literature to encompass a variety of imperfect channel knowledge cases that appear in real-world transmissions. Our findings demonstrate that BCJRNet significantly outperforms the conventional BCJR algorithm for stationary transmission scenarios when learning from noisy channel data and with imperfect channel decay profiles. However, this advant
    
[^45]: 二进制特征屏蔽优化用于特征选择

    Binary Feature Mask Optimization for Feature Selection. (arXiv:2401.12644v1 [cs.LG])

    [http://arxiv.org/abs/2401.12644](http://arxiv.org/abs/2401.12644)

    这个论文提出了一种新颖的特征选择框架，通过使用特征屏蔽方法来消除特征，而不是从数据集中移除它们。这种方法不需要重新训练机器学习模型，可以综合考虑特征子集的重要性，为通用机器学习模型的特征选择问题提供了一种新的解决方案。

    

    我们研究了通用机器学习模型的特征选择问题。我们引入了一种新颖的框架，该框架考虑了模型的预测结果来选择特征。我们的框架通过使用一种新颖的特征屏蔽方法，在特征选择过程中消除特征，而不是从数据集中完全移除它们。这使我们能够在特征选择过程中使用相同的机器学习模型，而不像其他特征选择方法那样需要在每次迭代中重新训练机器学习模型，因为数据集的维度不同。我们使用机器学习模型的预测结果来获取屏蔽操作符，这为模型的预测性能提供了对特征子集的全面观察。特征选择文献中存在各种方法。然而，没有研究引入一个针对通用机器学习模型的无需训练的框架，以整体考虑特征子集的重要性，而不是只关注单个特征的重要性。

    We investigate feature selection problem for generic machine learning (ML) models. We introduce a novel framework that selects features considering the predictions of the model. Our framework innovates by using a novel feature masking approach to eliminate the features during the selection process, instead of completely removing them from the dataset. This allows us to use the same ML model during feature selection, unlike other feature selection methods where we need to train the ML model again as the dataset has different dimensions on each iteration. We obtain the mask operator using the predictions of the ML model, which offers a comprehensive view on the subsets of the features essential for the predictive performance of the model. A variety of approaches exist in the feature selection literature. However, no study has introduced a training-free framework for a generic ML model to select features while considering the importance of the feature subsets as a whole, instead of focusi
    
[^46]: 对Makelov等人(2023)的《可解释性错觉》论点的回应

    A Reply to Makelov et al. (2023)'s "Interpretability Illusion" Arguments. (arXiv:2401.12631v1 [cs.LG])

    [http://arxiv.org/abs/2401.12631](http://arxiv.org/abs/2401.12631)

    本文回应了Makelov等人(2023)的论文，该论文评述了子空间交换干预方法的"解释性错觉"问题。我们指出，所谓的"解释性错觉"可以包括直观和可取的解释，而Makelov等人(2023)发现的"错觉"是他们训练和评估范例的产物。尽管我们不同意他们的核心表述，但他们的例子和讨论推动了可解释性领域的发展。

    

    我们回应了Makelov等人(2023)的最新论文，该论文评述了诸如分布式对齐搜索(DAS; Geiger等人，2023)这样的子空间交换干预方法，并声称这些方法可能引起"解释性错觉"。我们首先回顾了Makelov等人(2023)对"解释性错觉"的技术概念，然后展示了即使直观和可取的解释在这个意义上也可能成为错觉。因此，他们发现"错觉"的方法可能会拒绝他们认为"非错觉"的解释。接着，我们认为Makelov等人(2023)在实践中看到的"错觉"是他们训练和评估范例的产物。最后，我们强调，尽管我们不同意他们的核心表述，但Makelov等人(2023)的例子和讨论无疑推动了可解释性领域的发展。

    We respond to the recent paper by Makelov et al. (2023), which reviews subspace interchange intervention methods like distributed alignment search (DAS; Geiger et al. 2023) and claims that these methods potentially cause "interpretability illusions". We first review Makelov et al. (2023)'s technical notion of what an "interpretability illusion" is, and then we show that even intuitive and desirable explanations can qualify as illusions in this sense. As a result, their method of discovering "illusions" can reject explanations they consider "non-illusory". We then argue that the illusions Makelov et al. (2023) see in practice are artifacts of their training and evaluation paradigms. We close by emphasizing that, though we disagree with their core characterization, Makelov et al. (2023)'s examples and discussion have undoubtedly pushed the field of interpretability forward.
    
[^47]: CAM只有深度学习神经网络推理的全栈优化

    Full-Stack Optimization for CAM-Only DNN Inference. (arXiv:2401.12630v1 [cs.AR])

    [http://arxiv.org/abs/2401.12630](http://arxiv.org/abs/2401.12630)

    本文研究了CAM只有深度学习神经网络推理的全栈优化，通过算法优化和关联处理器的设计，以及使用赛车磁记忆来实现，成功降低了能量消耗和延迟，并提高了神经网络的精度和可靠性。

    

    在过去的几年里，神经网络的精度在各个领域都有了显著提高。然而，它们日益增长的复杂性导致冯·诺伊曼系统在能源需求和延迟方面变得难以承受。最近有几个计算内存（CIM）系统被提出来克服这个问题，但在准确性、硬件可靠性和大型模型的可扩展性之间仍然存在着权衡问题。此外，对于一些CIM设计，激活的移动仍然需要相当大的时间和能量。本文研究了三元权重神经网络和关联处理器（AP）的算法优化相结合的方法，这些关联处理器使用了赛车磁记忆（RTM）来实现。我们提出了一种新的编译流程，通过减少AP中的算术强度来优化卷积。通过利用基于RTM的AP的优势，这种方法在处理准确性、能源效率和可靠性问题的同时，大大降低了内存中的数据传输。

    The accuracy of neural networks has greatly improved across various domains over the past years. Their ever-increasing complexity, however, leads to prohibitively high energy demands and latency in von Neumann systems. Several computing-in-memory (CIM) systems have recently been proposed to overcome this, but trade-offs involving accuracy, hardware reliability, and scalability for large models remain a challenge. Additionally, for some CIM designs, the activation movement still requires considerable time and energy. This paper explores the combination of algorithmic optimizations for ternary weight neural networks and associative processors (APs) implemented using racetrack memory (RTM). We propose a novel compilation flow to optimize convolutions on APs by reducing their arithmetic intensity. By leveraging the benefits of RTM-based APs, this approach substantially reduces data transfers within the memory while addressing accuracy, energy efficiency, and reliability concerns. Concretel
    
[^48]: 基于数据驱动因子图的盲信道估计和联合符号检测

    Blind Channel Estimation and Joint Symbol Detection with Data-Driven Factor Graphs. (arXiv:2401.12627v1 [cs.IT])

    [http://arxiv.org/abs/2401.12627](http://arxiv.org/abs/2401.12627)

    本论文研究了在时变线性干扰信道上基于因子图的盲信道估计和联合符号检测方法。通过使用置信传播算法和期望最大化算法相互交织的迭代，可以降低复杂度并提高性能。通过引入数据驱动的方法，算法在离线训练样本数量较少的情况下也能取得显著的性能提升。

    

    我们研究了在时变线性干扰信道上盲联合信道估计和符号检测的因子图框架的应用。具体来说，我们考虑了最大似然估计的期望最大化（EM）算法，该算法通常由于需要在每次迭代中计算逐符号后验分布而导致计算复杂度高。我们通过在适当的因子图上使用置信传播（BP）算法来有效地逼近后验分布，从而解决了这个问题。通过交织BP和EM的迭代，检测复杂度进一步减少到每个EM步骤只需要一次BP迭代。此外，我们提出了我们算法的数据驱动版本，它引入了BP更新的动量，并学习了适当的EM参数更新计划，从而在仅有少量离线训练样本的情况下显著改善了性能-复杂度权衡。我们的数值实验证明了其出色的性能。

    We investigate the application of the factor graph framework for blind joint channel estimation and symbol detection on time-variant linear inter-symbol interference channels. In particular, we consider the expectation maximization (EM) algorithm for maximum likelihood estimation, which typically suffers from high complexity as it requires the computation of the symbol-wise posterior distributions in every iteration. We address this issue by efficiently approximating the posteriors using the belief propagation (BP) algorithm on a suitable factor graph. By interweaving the iterations of BP and EM, the detection complexity can be further reduced to a single BP iteration per EM step. In addition, we propose a data-driven version of our algorithm that introduces momentum in the BP updates and learns a suitable EM parameter update schedule, thereby significantly improving the performance-complexity tradeoff with a few offline training samples. Our numerical experiments demonstrate the excel
    
[^49]: 面向多智能体远程控制的基于语言到新兴通信的知识蒸馏

    Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control. (arXiv:2401.12624v1 [cs.AI])

    [http://arxiv.org/abs/2401.12624](http://arxiv.org/abs/2401.12624)

    这项工作通过将语言导向的语义通信与新兴通信相结合，通过知识蒸馏的方式，提出了一种面向多智能体远程控制的新框架，实现了更快的行程时间和更高的训练收敛速度。

    

    在这项工作中，我们比较了基于多智能体深度强化学习（MADRL）的新兴通信（EC）和由预训练的大型语言模型（LLM）使用人类语言的面向语言的语义通信（LSC）。在一个多智能体远程导航任务中，使用包含位置和通道地图的多模态输入数据，结果表明，EC在使用多模态数据时会产生高的训练成本和困难，而LSC由于LLM尺寸较大，会导致高的推理计算成本。为了解决它们各自的瓶颈，我们提出了一种通过知识蒸馏（KD）引导EC训练使用LSC的新颖框架：语言引导的EC（LEC）。模拟验证了LEC实现了更快的行程时间，避免了信道质量差的区域，并且在与EC相比能够加速MADRL训练收敛达到61.8%。

    In this work, we compare emergent communication (EC) built upon multi-agent deep reinforcement learning (MADRL) and language-oriented semantic communication (LSC) empowered by a pre-trained large language model (LLM) using human language. In a multi-agent remote navigation task, with multimodal input data comprising location and channel maps, it is shown that EC incurs high training cost and struggles when using multimodal data, whereas LSC yields high inference computing cost due to the LLM's large size. To address their respective bottlenecks, we propose a novel framework of language-guided EC (LEC) by guiding the EC training using LSC via knowledge distillation (KD). Simulations corroborate that LEC achieves faster travel time while avoiding areas with poor channel conditions, as well as speeding up the MADRL training convergence by up to 61.8% compared to EC.
    
[^50]: 任务相似性和过参数化对灾难性遗忘的联合影响 - 一种分析模型

    The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting -- An Analytical Model. (arXiv:2401.12617v1 [cs.LG])

    [http://arxiv.org/abs/2401.12617](http://arxiv.org/abs/2401.12617)

    本文研究了任务相似性和过参数化如何联合影响连续学习中的灾难性遗忘，并发现在过参数化模型中，中等任务相似性导致最多的遗忘，而在插值阈值附近，遗忘随期望任务相似性单调减少。

    

    在连续学习中，灾难性遗忘受到多个任务方面的影响。以往的研究分别分析了遗忘受任务相似性或过参数化的影响。相反，我们的论文在可分析的模型中研究了任务相似性和过参数化如何共同影响遗忘。具体而言，我们关注双任务连续线性回归，其中第二个任务是任意第一个任务的随机正交变换（随机排列任务的抽象）。我们推导了期望遗忘的精确解析表达式，并揭示了一个微妙的模式。在过参数化模型中，中等任务相似性导致最多的遗忘。然而，在插值阈值附近，遗忘随期望任务相似性单调减少。我们用合成数据上的线性回归和已建立的排列任务基准上的神经网络验证了我们的发现。

    In continual learning, catastrophic forgetting is affected by multiple aspects of the tasks. Previous works have analyzed separately how forgetting is affected by either task similarity or overparameterization. In contrast, our paper examines how task similarity and overparameterization jointly affect forgetting in an analyzable model. Specifically, we focus on two-task continual linear regression, where the second task is a random orthogonal transformation of an arbitrary first task (an abstraction of random permutation tasks). We derive an exact analytical expression for the expected forgetting - and uncover a nuanced pattern. In highly overparameterized models, intermediate task similarity causes the most forgetting. However, near the interpolation threshold, forgetting decreases monotonically with the expected task similarity. We validate our findings with linear regression on synthetic data, and with neural networks on established permutation task benchmarks.
    
[^51]: Prompt Smells: AI 生成结果的不受欢迎的征兆

    Prompt Smells: An Omen for Undesirable Generative AI Outputs. (arXiv:2401.12611v1 [cs.LG])

    [http://arxiv.org/abs/2401.12611](http://arxiv.org/abs/2401.12611)

    本文提出了两个新概念，分别是对GenAI输出可取性的定义和"提示气味"的概念，旨在解决GenAI模型应用中的限制和挑战。

    

    最近的生成人工智能（GenAI）趋势关注各种应用，包括创作故事、插图、诗歌、文章、计算机代码、音乐作品和视频。外在幻觉是这种GenAI的一个关键限制，可能导致在实现和维护GenAI的可信度方面面临重大挑战。在本文中，我们提出了两个新概念，我们认为这将有助于研究社区解决与GenAI模型应用相关的限制。首先，我们提出了对GenAI输出"可取性"的定义，以及观察到影响其的三个因素。其次，我们借鉴马丁·福勒（Martin Fowler）的"代码气味"，提出了"提示气味"的概念，以及它们对GenAI输出的可取性所产生的不良影响。我们期望我们的工作将为关于GenAI输出可取性的持续讨论做出贡献，并在有意义的方式上推动该领域的发展。

    Recent Generative Artificial Intelligence (GenAI) trends focus on various applications, including creating stories, illustrations, poems, articles, computer code, music compositions, and videos. Extrinsic hallucinations are a critical limitation of such GenAI, which can lead to significant challenges in achieving and maintaining the trustworthiness of GenAI. In this paper, we propose two new concepts that we believe will aid the research community in addressing limitations associated with the application of GenAI models. First, we propose a definition for the "desirability" of GenAI outputs and three factors which are observed to influence it. Second, drawing inspiration from Martin Fowler's code smells, we propose the concept of "prompt smells" and the adverse effects they are observed to have on the desirability of GenAI outputs. We expect our work will contribute to the ongoing conversation about the desirability of GenAI outputs and help advance the field in a meaningful way.
    
[^52]: 学习神经网络的双峰现象

    The twin peaks of learning neural networks. (arXiv:2401.12610v1 [cs.LG])

    [http://arxiv.org/abs/2401.12610](http://arxiv.org/abs/2401.12610)

    该论文研究了神经网络的双峰现象，发现高度过参数化的模型可以避免过拟合并实现良好的测试性能，与传统的偏差-方差折衷法则不同。研究分析了布尔均值维度（BMD）与网络复杂性和敏感性之间的关系，得到了在高维度范围内BMD的可解释表达式，发现BMD在网络过参数化程度增加时达到极值点。

    

    最近的研究表明，在神经网络的泛化误差方面存在双峰现象，即高度过参数化的模型可以避免过拟合并实现良好的测试性能，与统计学习理论描述的标准偏差-方差折衷法则不符。在本研究中，我们探讨了这一现象与神经网络所表示的函数的复杂性和敏感性增加之间的联系。具体而言，我们研究了布尔均值维度（BMD），这是在布尔函数分析背景下发展起来的一种度量。针对随机特征模型的简单教师-学生设置，我们基于副本方法进行理论分析，得到了一个可解释的BMD表达式，其中数据点的数量、特征的数量和输入大小在高维度范围内不断增长。我们发现，随着网络过参数化程度的增加，BMD达到一个极值点。

    Recent works demonstrated the existence of a double-descent phenomenon for the generalization error of neural networks, where highly overparameterized models escape overfitting and achieve good test performance, at odds with the standard bias-variance trade-off described by statistical learning theory. In the present work, we explore a link between this phenomenon and the increase of complexity and sensitivity of the function represented by neural networks. In particular, we study the Boolean mean dimension (BMD), a metric developed in the context of Boolean function analysis. Focusing on a simple teacher-student setting for the random feature model, we derive a theoretical analysis based on the replica method that yields an interpretable expression for the BMD, in the high dimensional regime where the number of data points, the number of features, and the input size grow to infinity. We find that, as the degree of overparameterization of the network is increased, the BMD reaches an ev
    
[^53]: 快速半监督非凸优化解混合

    Fast Semi-supervised Unmixing using Non-convex Optimization. (arXiv:2401.12609v1 [cs.CV])

    [http://arxiv.org/abs/2401.12609](http://arxiv.org/abs/2401.12609)

    本文介绍了一种用于解混合的快速半监督非凸优化模型，该模型考虑了库不匹配和丰度约束，并提出了两种基于先验的半监督解混合方法。实验证明，实施凸性约束优于稀疏先验对于端元库的表现。

    

    本文介绍了一种针对半监督/基于库的解混合设计的新型线性模型。我们的模型考虑了库不匹配，并能够实施丰度和等于一的约束。与传统的稀疏解混合方法不同，该模型涉及到非凸优化，具有重要的计算挑战。我们展示了交替乘法器方法（ADMM）在循环求解这些复杂问题中的效果。我们提出了两种半监督解混合方法，每种方法都依赖于应用于新模型的不同先验以及丰度和等于一的约束：稀疏先验和凸性约束。我们的实验证明，实施凸性约束优于稀疏先验对于端元库的表现。这些结果在三个模拟数据集（考虑了光谱变化和不同像素纯度水平）和Cuprite数据集上得到了证实。此外，我们与传统方法进行了比较。

    In this paper, we introduce a novel linear model tailored for semisupervised/library-based unmixing. Our model incorporates considerations for library mismatch while enabling the enforcement of the abundance sum-to-one constraint (ASC). Unlike conventional sparse unmixing methods, this model involves nonconvex optimization, presenting significant computational challenges. We demonstrate the efficacy of Alternating Methods of Multipliers (ADMM) in cyclically solving these intricate problems. We propose two semisupervised unmixing approaches, each relying on distinct priors applied to the new model in addition to the ASC: sparsity prior and convexity constraint. Our experimental results validate that enforcing the convexity constraint outperforms the sparsity prior for the endmember library. These results are corroborated across three simulated datasets (accounting for spectral variability and varying pixel purity levels) and the Cuprite dataset. Additionally, our comparison with convent
    
[^54]: 解读等变表示

    Interpreting Equivariant Representations. (arXiv:2401.12588v1 [cs.LG])

    [http://arxiv.org/abs/2401.12588](http://arxiv.org/abs/2401.12588)

    本文研究了潜在表示的等变性以及在使用中考虑等变模型的归纳偏差的重要性，提出了选择不变投影的原则，并展示了两个实例的影响。

    

    对于深度学习模型的可视化、插值或特征提取等下游任务，潜在表示被广泛使用。不变和等变神经网络是用于强制执行归纳偏差的强大且已建立的模型。本文表明，在使用潜在表示时，必须同时考虑等变模型施加的归纳偏差。我们展示了不考虑归纳偏差会导致下游任务性能下降，相反，通过使用潜在表示的不变投影可以有效地考虑归纳偏差。我们提出了选择这样一个投影的原则，并展示了在两个常见例子中使用这些原则的影响：首先，我们研究了一种用于分子图生成的置换等变变分自动编码器；在这里，我们展示了可以设计出不产生信息损失的不变投影。

    Latent representations are used extensively for downstream tasks, such as visualization, interpolation or feature extraction of deep learning models. Invariant and equivariant neural networks are powerful and well-established models for enforcing inductive biases. In this paper, we demonstrate that the inductive bias imposed on the by an equivariant model must also be taken into account when using latent representations. We show how not accounting for the inductive biases leads to decreased performance on downstream tasks, and vice versa, how accounting for inductive biases can be done effectively by using an invariant projection of the latent representations. We propose principles for how to choose such a projection, and show the impact of using these principles in two common examples: First, we study a permutation equivariant variational auto-encoder trained for molecule graph generation; here we show that invariant projections can be designed that incur no loss of information in the
    
[^55]: LLMCheckup：通过可解释性工具对大型语言模型进行对话式检查

    LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools. (arXiv:2401.12576v1 [cs.CL])

    [http://arxiv.org/abs/2401.12576](http://arxiv.org/abs/2401.12576)

    LLMCheckup是一个可解释性工具，通过连接大型语言模型与可解释的AI工具，使用户能够与模型进行对话，生成自我解释并提供建议。

    

    提供以对话形式进行解释的可解释性工具已经证明在增强用户理解方面具有效果，因为一次性解释有时无法提供足够的信息给用户。然而，当前基于对话的解释方案需要许多依赖项，并且不容易转移到它们未设计的任务上。通过LLMCheckup，我们提供了一个易于访问的工具，允许用户与任何最新的大型语言模型（LLM）进行对话以了解其行为。我们使LLMs能够自行生成所有解释，并通过与一系列可解释性AI（XAI）工具（例如特征归因、基于嵌入的相似性以及反事实和基于理由生成的提示策略）连接，以完成意图识别而无需微调。LLM（自我）解释以交互对话的形式呈现，支持后续问题和生成建议。

    Interpretability tools that offer explanations in the form of a dialogue have demonstrated their efficacy in enhancing users' understanding, as one-off explanations may occasionally fall short in providing sufficient information to the user. Current solutions for dialogue-based explanations, however, require many dependencies and are not easily transferable to tasks they were not designed for. With LLMCheckup, we present an easily accessible tool that allows users to chat with any state-of-the-art large language model (LLM) about its behavior. We enable LLMs to generate all explanations by themselves and take care of intent recognition without fine-tuning, by connecting them with a broad spectrum of Explainable AI (XAI) tools, e.g. feature attributions, embedding-based similarity, and prompting strategies for counterfactual and rationale generation. LLM (self-)explanations are presented as an interactive dialogue that supports follow-up questions and generates suggestions. LLMCheckup p
    
[^56]: 从因果关系的角度看图对比不变学习

    Graph Contrastive Invariant Learning from the Causal Perspective. (arXiv:2401.12564v1 [cs.LG])

    [http://arxiv.org/abs/2401.12564](http://arxiv.org/abs/2401.12564)

    本文从因果关系的角度研究了图对比不变学习，并提出了一种新的GCL方法，通过引入谱图扩增和设计不变性目标和独立性目标来更好地学习不变表示。

    

    图对比学习（GCL）是一种通过对比两个扩增图来自我监督学习节点表示的方法，已引起了相当大的关注。GCL通常被认为是学习不变表示的方法。然而，这种理解在实践中总是正确的吗？本文首先从因果关系的角度研究了GCL。通过使用结构因果模型（SCM）分析GCL，我们发现传统的GCL由于图中包含的非因果信息，可能无法很好地学习不变表示。那么，我们如何修复并促使当前的GCL学习更好的不变表示呢？SCM提供了两个要求并激励我们提出了一种新的GCL方法。特别地，我们引入了谱图扩增来模拟对非因果因素的干预。然后，我们设计了不变性目标和独立性目标，以更好地捕捉因果因素。具体来说，（i）不变性目标鼓励e

    Graph contrastive learning (GCL), learning the node representation by contrasting two augmented graphs in a self-supervised way, has attracted considerable attention. GCL is usually believed to learn the invariant representation. However, does this understanding always hold in practice? In this paper, we first study GCL from the perspective of causality. By analyzing GCL with the structural causal model (SCM), we discover that traditional GCL may not well learn the invariant representations due to the non-causal information contained in the graph. How can we fix it and encourage the current GCL to learn better invariant representations? The SCM offers two requirements and motives us to propose a novel GCL method. Particularly, we introduce the spectral graph augmentation to simulate the intervention upon non-causal factors. Then we design the invariance objective and independence objective to better capture the causal factors. Specifically, (i) the invariance objective encourages the e
    
[^57]: UR4NNV: 神经网络验证，基于欠估计可达性的方法！

    UR4NNV: Neural Network Verification, Under-approximation Reachability Works!. (arXiv:2401.12550v1 [cs.AI])

    [http://arxiv.org/abs/2401.12550](http://arxiv.org/abs/2401.12550)

    本文提出了UR4NNV验证框架，利用欠估计可达性分析进行DNN验证。该框架对具有ReLU激活的DNN进行欠估计，并通过试错方法有效地证伪DNN属性。

    

    最近，深度神经网络（DNNs）的形式验证引起了广泛关注，基于过度估计的方法因其有效性和高效性而变得流行。然而，这些策略在解决涉及确切输出区域或引入的近似误差是否违反所讨论属性的“未知困境”方面面临挑战。为了解决这个问题，本文首次引入了UR4NNV验证框架，该框架利用欠估计可达性分析进行DNN验证。UR4NNV专注于具有修正线性单元（ReLU）激活的DNN，并采用基于二叉树分支的欠估计算法。在每个周期中，UR4NNV对可达集合的子多面体进行欠估计，并针对给定的属性验证该多面体。通过试错方法，UR4NNV在达到验证周期边界和失败时提供了有效地证伪DNN属性的信心水平。

    Recently, formal verification of deep neural networks (DNNs) has garnered considerable attention, and over-approximation based methods have become popular due to their effectiveness and efficiency. However, these strategies face challenges in addressing the "unknown dilemma" concerning whether the exact output region or the introduced approximation error violates the property in question. To address this, this paper introduces the UR4NNV verification framework, which utilizes under-approximation reachability analysis for DNN verification for the first time. UR4NNV focuses on DNNs with Rectified Linear Unit (ReLU) activations and employs a binary tree branch-based under-approximation algorithm. In each epoch, UR4NNV under-approximates a sub-polytope of the reachable set and verifies this polytope against the given property. Through a trial-and-error approach, UR4NNV effectively falsifies DNN properties while providing confidence levels when reaching verification epoch bounds and failing
    
[^58]: 使用监督学习构建近视MPC策略

    On Building Myopic MPC Policies using Supervised Learning. (arXiv:2401.12546v1 [cs.LG])

    [http://arxiv.org/abs/2401.12546](http://arxiv.org/abs/2401.12546)

    本论文提出了一种使用监督学习构建近视MPC策略的方法，通过离线学习最优值函数，可以显著减少在线计算负担，而不影响控制器的性能。

    

    近期，在模型预测控制（MPC）中，结合监督学习技术应用引起了广泛关注，尤其是在近似显式MPC领域，其中使用函数逼近器（如深度神经网络）通过离线生成的最佳状态-动作对来学习MPC策略。虽然近似显式MPC的目标是尽可能准确地复制MPC策略，用训练好的神经网络替代在线优化，但通常会失去解决在线优化问题所带来的性能保证。本文提出了一种替代策略，即使用监督学习离线学习最优值函数而不是学习最优策略。然后，在一个非常短的预测时间范围内，将其作为近视MPC的成本函数，从而显著减少在线计算负担，不影响控制器的性能。该方法与现有方法存在差异。

    The application of supervised learning techniques in combination with model predictive control (MPC) has recently generated significant interest, particularly in the area of approximate explicit MPC, where function approximators like deep neural networks are used to learn the MPC policy via optimal state-action pairs generated offline. While the aim of approximate explicit MPC is to closely replicate the MPC policy, substituting online optimization with a trained neural network, the performance guarantees that come with solving the online optimization problem are typically lost. This paper considers an alternative strategy, where supervised learning is used to learn the optimal value function offline instead of learning the optimal policy. This can then be used as the cost-to-go function in a myopic MPC with a very short prediction horizon, such that the online computation burden reduces significantly without affecting the controller performance. This approach differs from existing wor
    
[^59]: 有效利用背景知识的约束k中心聚类

    Efficient Constrained $k$-Center Clustering with Background Knowledge. (arXiv:2401.12533v1 [cs.LG])

    [http://arxiv.org/abs/2401.12533](http://arxiv.org/abs/2401.12533)

    本论文提出了一种在k中心聚类上利用背景知识的约束聚类算法，通过采用一系列技术，得到了效率高且具有最佳近似比例2的算法。

    

    中心为基础的聚类在理论和实践中都引起了重要的研究兴趣。在许多实际应用中，输入数据通常包含可以用于改进聚类结果的背景知识。在这项工作中，我们基于广泛采用的k中心聚类，并将其输入的背景知识建模为必连（ML）和不连（CL）约束集。然而，大多数包括k中心在内的聚类问题本质上都是NP困难的，而更复杂的受约束变体被认为受到更严重的近似和计算障碍的限制，极大地限制了它们的适用性。通过采用一系列技术，包括反支配集，线性规划（LP）整数平面和LP对偶性，我们得到了第一个具有最佳近似比例2的约束k中心的高效近似算法。我们还构建了竞争基准算法，并对我们的近似算法进行了实证评估。

    Center-based clustering has attracted significant research interest from both theory and practice. In many practical applications, input data often contain background knowledge that can be used to improve clustering results. In this work, we build on widely adopted $k$-center clustering and model its input background knowledge as must-link (ML) and cannot-link (CL) constraint sets. However, most clustering problems including $k$-center are inherently $\mathcal{NP}$-hard, while the more complex constrained variants are known to suffer severer approximation and computation barriers that significantly limit their applicability. By employing a suite of techniques including reverse dominating sets, linear programming (LP) integral polyhedron, and LP duality, we arrive at the first efficient approximation algorithm for constrained $k$-center with the best possible ratio of 2. We also construct competitive baseline algorithms and empirically evaluate our approximation algorithm against them o
    
[^60]: DAFA：距离感知公平对抗训练

    DAFA: Distance-Aware Fair Adversarial Training. (arXiv:2401.12532v1 [cs.LG])

    [http://arxiv.org/abs/2401.12532](http://arxiv.org/abs/2401.12532)

    DAFA通过考虑类别之间的相似性，引入了不同的损失权重和对抗边界，并调整它们以提高在对抗训练中的鲁棒公平性。

    

    标准训练中类别间的准确性差异在对抗训练中被放大，这被称为鲁棒公平问题。现有的方法为增强鲁棒公平性而牺牲模型对易类别的性能，以改善对难类别的性能。然而，我们观察到在对抗攻击下，模型对最差类别样本的预测大多偏向于与最差类别相似的类别，而不是易类别。通过理论和实证分析，我们证明了随着类别之间距离的减小，鲁棒公平性会恶化。受到这些观察的启发，我们引入了距离感知公平对抗训练（DAFA）方法，通过考虑类别之间的相似性来解决鲁棒公平性问题。具体而言，我们的方法为每个类别分配不同的损失权重和对抗边界，并调整它们以促进一种权衡关系。

    The disparity in accuracy between classes in standard training is amplified during adversarial training, a phenomenon termed the robust fairness problem. Existing methodologies aimed to enhance robust fairness by sacrificing the model's performance on easier classes in order to improve its performance on harder ones. However, we observe that under adversarial attacks, the majority of the model's predictions for samples from the worst class are biased towards classes similar to the worst class, rather than towards the easy classes. Through theoretical and empirical analysis, we demonstrate that robust fairness deteriorates as the distance between classes decreases. Motivated by these insights, we introduce the Distance-Aware Fair Adversarial training (DAFA) methodology, which addresses robust fairness by taking into account the similarities between classes. Specifically, our method assigns distinct loss weights and adversarial margins to each class and adjusts them to encourage a trade-
    
[^61]: BiTA: 大语言模型中无损加速的双向调整

    BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models. (arXiv:2401.12522v1 [cs.CL])

    [http://arxiv.org/abs/2401.12522](http://arxiv.org/abs/2401.12522)

    BiTA是一种用于大语言模型的创新方法，通过双向调整实现了无损加速。它采用简化的半自回归生成和草稿验证，通过高效的基于树的解码同时进行候选生成和验证，提高了推理效率。这种方法不需要额外的辅助模型或显著的额外内存开销。

    

    大型语言模型（LLMs）通常在推理过程中使用自回归生成，导致高内存带宽需求和延迟延长。为了减轻这种效率低下的问题，我们提出了一种创新方法——双向调整以实现无损加速（BiTA），通过简化的半自回归生成和草稿验证来加速LLMs。受启发于提示调整的概念，我们使用一种参数高效的设计，称为双向调整，来增强LLMs在半自回归生成方面的能力。采用高效的基于树的解码，模型可以同时进行草稿候选生成和验证，确保输出结果与它们的自回归对应物在贪婪抽样下完全相同。BiTA作为一个轻量级的插件模块，可以无缝增强现有LLMs的推理效率，而无需额外的辅助模型或承担显著的额外内存开销。通过应用提出的BiTA，LLaMA-2-70B-Chat实现了

    Large language models (LLMs) commonly employ autoregressive generation during inference, leading to high memory bandwidth demand and consequently extended latency. To mitigate this inefficiency, we present Bi-directional Tuning for lossless Acceleration (BiTA), an innovative method expediting LLMs via streamlined semi-autoregressive generation and draft verification. Inspired by the concept of prompt tuning, we enhance LLMs with a parameter-efficient design called bi-directional tuning for the capability in semi-autoregressive generation. Employing efficient tree-based decoding, the models perform draft candidate generation and verification in parallel, ensuring outputs identical to their autoregressive counterparts under greedy sampling. BiTA serves as a lightweight plug-in module, seamlessly boosting the inference efficiency of existing LLMs without requiring additional assistance models or incurring significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat achieve
    
[^62]: 对优惠贸易协定的非结构化数据内容进行关键信息检索的研究

    Key Information Retrieval to Classify the Unstructured Data Content of Preferential Trade Agreements. (arXiv:2401.12520v1 [cs.CL])

    [http://arxiv.org/abs/2401.12520](http://arxiv.org/abs/2401.12520)

    本论文介绍了一种用于长文本分类和预测的新方法，通过嵌入技术对长文本进行压缩，然后采用双向编码器表示来自Transformers的嵌入方法进行文本分类训练，实验结果显示在优惠贸易协定的长文本分类方面取得了显著的性能提升。

    

    随着文本数据的迅速增长，预测长文本已经成为自然语言处理领域的重要挑战。传统的文本预测方法在处理长文本时遇到困难，主要是由于冗余和无关信息的存在，这影响了模型从文本中捕捉重要见解的能力。为了解决这个问题，我们提出了一种新的长文本分类和预测方法。首先，我们采用嵌入技术来对长文本进行压缩，以减少其中的冗余信息。随后，我们使用双向编码器表示来自Transformers（BERT）的嵌入方法进行文本分类训练。实验结果表明，我们的方法在优惠贸易协定的长文本分类方面实现了显著的性能提升。此外，通过嵌入方法对文本进行压缩不仅增强了预测表现，而且有助于提取关键信息。

    With the rapid proliferation of textual data, predicting long texts has emerged as a significant challenge in the domain of natural language processing. Traditional text prediction methods encounter substantial difficulties when grappling with long texts, primarily due to the presence of redundant and irrelevant information, which impedes the model's capacity to capture pivotal insights from the text. To address this issue, we introduce a novel approach to long-text classification and prediction. Initially, we employ embedding techniques to condense the long texts, aiming to diminish the redundancy therein. Subsequently,the Bidirectional Encoder Representations from Transformers (BERT) embedding method is utilized for text classification training. Experimental outcomes indicate that our method realizes considerable performance enhancements in classifying long texts of Preferential Trade Agreements. Furthermore, the condensation of text through embedding methods not only augments predic
    
[^63]: DDMI: 面向领域无关的隐式神经表示的高质量合成的潜在扩散模型

    DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing High-Quality Implicit Neural Representations. (arXiv:2401.12517v1 [cs.LG])

    [http://arxiv.org/abs/2401.12517](http://arxiv.org/abs/2401.12517)

    DDMI是一种面向领域无关的隐式神经表示的高质量合成的潜在扩散模型，通过生成自适应位置嵌入而不是网络权重，解决了现有方法中生成质量较低的问题。

    

    最近的研究引入了一类用于合成各个领域中任意连续信号的隐式神经表示生成模型，为领域无关的生成模型打开了大门，但往往无法实现高质量的生成。我们观察到现有方法通过生成神经网络的权重来参数化隐式神经表示，并使用固定的位置嵌入来评估网络。可以说，这种架构限制了生成模型的表达能力，导致隐式神经表示生成的质量较低。为了解决这个限制，我们提出了一种面向领域无关的隐式神经表示的潜在扩散模型 (DDMI)，其生成自适应位置嵌入而不是网络权重。具体而言，我们开发了一个离散到连续空间的变分自编码器 (D2C-VAE)，它在共享的潜在空间中无缝连接离散数据和连续信号函数。此外，我们引入了一种新颖的...

    Recent studies have introduced a new class of generative models for synthesizing implicit neural representations (INRs) that capture arbitrary continuous signals in various domains. These models opened the door for domain-agnostic generative models, but they often fail to achieve high-quality generation. We observed that the existing methods generate the weights of neural networks to parameterize INRs and evaluate the network with fixed positional embeddings (PEs). Arguably, this architecture limits the expressive power of generative models and results in low-quality INR generation. To address this limitation, we propose Domain-agnostic Latent Diffusion Model for INRs (DDMI) that generates adaptive positional embeddings instead of neural networks' weights. Specifically, we develop a Discrete-to-continuous space Variational AutoEncoder (D2C-VAE), which seamlessly connects discrete data and the continuous signal functions in the shared latent space. Additionally, we introduce a novel con
    
[^64]: 在线社交网络数字克隆的语言敏感代理建模研究误信息传播

    Digital cloning of online social networks for language-sensitive agent-based modeling of misinformation spread. (arXiv:2401.12509v1 [cs.SI])

    [http://arxiv.org/abs/2401.12509](http://arxiv.org/abs/2401.12509)

    本研究开发了一个基于代理建模和自然语言处理技术的仿真框架，用于研究在线社交网络中的误信息传播。通过数字克隆已知的误信息共享网络，我们提高了模拟的真实性和普适性，并且考虑到了讨论主题、用户偏好和在线社区动态等因素。

    

    我们开发了一个仿真框架，将代理建模和自然语言处理技术相结合，研究在线社交网络中的误信息传播。虽然在这个领域中存在许多其他的代理建模仿真，但它们在提供可操作的见解方面受限于其对现有网络的真实性和普适性的不足。为了部分解决这些问题，我们通过下载超过一万名用户的社交媒体历史记录，创建了一个已知误信息共享网络的“数字克隆”。我们解析这些历史记录，提取出网络的结构，并对成员之间信息分享和传播的微妙方式进行建模。与这个领域中许多其他的代理建模方法不同，我们框架中用户之间的信息分享对讨论主题、用户偏好和在线社区动态都非常敏感。为了评估我们方法的真实性，我们用一组记录在 t 中的帖子对克隆网络进行了种子设置。

    We develop a simulation framework for studying misinformation spread within online social networks that blends agent-based modeling and natural language processing techniques. While many other agent-based simulations exist in this space, their ability to provide actionable insights in in part limited by their lack of fidelity and generalizability to existing networks. To partially address these concerns, we create a 'digital clone' of a known misinformation sharing network by downloading social media histories for over ten thousand of its users. We parse these histories to both extract the structure of the network and model the nuanced ways in which information is shared and spread among its members. Unlike many other agent-based methods in this space, information sharing between users in our framework is sensitive to topic of discussion, user preferences, and online community dynamics. To evaluate the fidelity of our method, we seed our cloned network with a set of posts recorded in t
    
[^65]: 关于正则化预期奖励优化的随机（方差减小）近端梯度法

    On the Stochastic (Variance-Reduced) Proximal Gradient Method for Regularized Expected Reward Optimization. (arXiv:2401.12508v1 [cs.LG])

    [http://arxiv.org/abs/2401.12508](http://arxiv.org/abs/2401.12508)

    本文介绍了一种用于正则化预期奖励优化问题的随机近端梯度法，该方法通过引入方差减小的技术以提高收敛速度。实验结果表明，在满足一定条件的情况下，该方法的样本复杂度可以达到$O(\epsilon^{-3})$。

    

    我们考虑在非明显设置中的正则化预期奖励优化问题，该问题涵盖了强化学习中的许多现有问题。为了解决这样的优化问题，我们应用和分析了经典的随机近端梯度法。具体而言，该方法已经证明在标准条件下，可以达到$O(\epsilon^{-4})$的样本复杂度来寻找$\epsilon$-稳定点。由于经典随机梯度估计器的方差通常较大，导致收敛速度较慢，因此我们还应用了一种高效的随机方差减小近端梯度法，其中采用了基于重要性抽样的概率梯度估计器 (PAGE)。据我们所知，这种方法的应用代表了在解决一般的正则化奖励优化问题上的一种新方法。我们的分析表明，在额外条件下，样本复杂度可以从$O(\epsilon^{-4})$提高到$O(\epsilon^{-3})$。

    We consider a regularized expected reward optimization problem in the non-oblivious setting that covers many existing problems in reinforcement learning (RL). In order to solve such an optimization problem, we apply and analyze the classical stochastic proximal gradient method. In particular, the method has shown to admit an $O(\epsilon^{-4})$ sample complexity to an $\epsilon$-stationary point, under standard conditions. Since the variance of the classical stochastic gradient estimator is typically large which slows down the convergence, we also apply an efficient stochastic variance-reduce proximal gradient method with an importance sampling based ProbAbilistic Gradient Estimator (PAGE). To the best of our knowledge, the application of this method represents a novel approach in addressing the general regularized reward optimization problem. Our analysis shows that the sample complexity can be improved from $O(\epsilon^{-4})$ to $O(\epsilon^{-3})$ under additional conditions. Our resu
    
[^66]: 构建最小和可重用的因果状态抽象用于强化学习

    Building Minimal and Reusable Causal State Abstractions for Reinforcement Learning. (arXiv:2401.12497v1 [cs.AI])

    [http://arxiv.org/abs/2401.12497](http://arxiv.org/abs/2401.12497)

    本文介绍了一种称为Causal Bisimulation Modeling (CBM)的方法，该方法通过学习动态和奖励函数中的因果关系来构建最小和可重用的任务特定抽象。实证验证表明，CBM学习到的隐式动态模型比显式模型更准确地识别出底层的因果关系和状态抽象。

    

    强化学习算法的两个期望是能够从相对较少的经验中学习，并学习适用于一系列问题规范的策略。在因素化状态空间中，实现这两个目标的一种方法是学习状态抽象，只保留学习任务所需的变量。本文介绍了一种称为Causal Bisimulation Modeling (CBM)的方法，该方法学习每个任务的动态和奖励函数中的因果关系，以获得一个最小、任务特定的抽象。CBM利用和改进了隐式建模技术，训练了一个高保真度的因果动态模型，可以在同一环境中为所有任务重复使用。在操作环境和Deepmind Control Suite上的实证验证表明，CBM学习到的隐式动态模型比显式模型更准确地识别出底层的因果关系和状态抽象。

    Two desiderata of reinforcement learning (RL) algorithms are the ability to learn from relatively little experience and the ability to learn policies that generalize to a range of problem specifications. In factored state spaces, one approach towards achieving both goals is to learn state abstractions, which only keep the necessary variables for learning the tasks at hand. This paper introduces Causal Bisimulation Modeling (CBM), a method that learns the causal relationships in the dynamics and reward functions for each task to derive a minimal, task-specific abstraction. CBM leverages and improves implicit modeling to train a high-fidelity causal dynamics model that can be reused for all tasks in the same environment. Empirical validation on manipulation environments and Deepmind Control Suite reveals that CBM's learned implicit dynamics models identify the underlying causal relationships and state abstractions more accurately than explicit ones. Furthermore, the derived state abstrac
    
[^67]: DexTouch：学习使用触觉灵巧性寻找和操作物体

    DexTouch: Learning to Seek and Manipulate Objects with Tactile Dexterity. (arXiv:2401.12496v1 [cs.RO])

    [http://arxiv.org/abs/2401.12496](http://arxiv.org/abs/2401.12496)

    本论文介绍了一种利用触觉灵巧性寻找和操作物体的多指机器人系统。通过使用触觉传感器进行物体搜索和操作，我们证明了即使在没有依赖于视觉信息的情况下，机器人也能够具备类似人类的触觉能力。

    

    触觉能力对于熟练执行各种任务是至关重要的，它能够在没有依赖于视觉信息的情况下搜索和操作物体。随着时间的推移，已经进行了大量研究将人类的触觉能力应用于机器人。在本文中，我们介绍了一个多指机器人系统，旨在利用触觉感受器搜索和操作物体，而不依赖于视觉信息。使用触觉传感器来搜索随机放置的目标物体，并进行模拟日常任务的物体操作。本研究的目标是赋予机器人类似人类的触觉能力。为了实现这一目标，我们在机器人手的一侧实现了二值触觉传感器，以尽量减少模拟与真实环境之间的差距。通过在仿真中通过强化学习训练策略，并将训练好的策略转移到真实环境中，我们证明了使用触觉传感器进行物体搜索和操作是可行的，即使在没有依赖于视觉信息的情况下。

    The sense of touch is an essential ability for skillfully performing a variety of tasks, providing the capacity to search and manipulate objects without relying on visual information. Extensive research has been conducted over time to apply these human tactile abilities to robots. In this paper, we introduce a multi-finger robot system designed to search for and manipulate objects using the sense of touch without relying on visual information. Randomly located target objects are searched using tactile sensors, and the objects are manipulated for tasks that mimic daily-life. The objective of the study is to endow robots with human-like tactile capabilities. To achieve this, binary tactile sensors are implemented on one side of the robot hand to minimize the Sim2Real gap. Training the policy through reinforcement learning in simulation and transferring the trained policy to the real environment, we demonstrate that object search and manipulation using tactile sensors is possible even in 
    
[^68]: 比较以人为中心的语言建模：模拟群体、个体特点还是两者兼顾？

    Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?. (arXiv:2401.12492v1 [cs.CL])

    [http://arxiv.org/abs/2401.12492](http://arxiv.org/abs/2401.12492)

    本研究比较了以群体属性、个体用户和组合方法来模拟人的上下文。合并群体和个体特征显著提高了用户级回归任务的性能，而模拟个体用户则显著提高了单个文档级分类任务的性能。

    

    自然语言处理在将人的上下文纳入其模型中取得了进展，但使用群体属性（如45岁以上的人群）还是模拟个体人物更有效的问题尚未确定。群体属性在技术上更容易实现，但是过于粗糙：并非所有45岁以上的人都以相同的方式书写。相反，模拟个体人物能够捕捉每个人身份的复杂性，允许更个性化的表示，但我们可能需要模拟无限数量的用户并且需要可能无法获取的数据。我们比较了通过群体属性、个体用户和组合方法来模拟人的上下文。将群体和个体特征结合起来，显著提高了基于用户文档的用户级回归任务（如年龄估计或个性评估）的性能。模拟个体用户显著提高了单个文档级分类任务（如立场和主题检测）的性能。

    Natural language processing has made progress in incorporating human context into its models, but whether it is more effective to use group-wise attributes (e.g., over-45-year-olds) or model individuals remains open. Group attributes are technically easier but coarse: not all 45-year-olds write the same way. In contrast, modeling individuals captures the complexity of each person's identity. It allows for a more personalized representation, but we may have to model an infinite number of users and require data that may be impossible to get. We compare modeling human context via group attributes, individual users, and combined approaches. Combining group and individual features significantly benefits user-level regression tasks like age estimation or personality assessment from a user's documents. Modeling individual users significantly improves the performance of single document-level classification tasks like stance and topic detection. We also find that individual-user modeling does w
    
[^69]: 基于有限差分残差约束损失的波动方程无监督学习方法

    Unsupervised Learning Method for the Wave Equation Based on Finite Difference Residual Constraints Loss. (arXiv:2401.12489v1 [cs.LG])

    [http://arxiv.org/abs/2401.12489](http://arxiv.org/abs/2401.12489)

    本文提出了一种新型的无监督学习方法，通过基于有限差分残差约束的波动方程，解决了现有深度学习方法中存在的问题。该方法具有高效、低成本和强泛化能力，并通过实验证明其优于传统的物理感知神经网络。

    

    波动方程是一种重要的物理偏微分方程，在近年来，深度学习已经展示出在加速或替代传统数值方法中解决波动方程方面的潜力。然而，现有的深度学习方法存在着高数据获取成本、低训练效率以及对边界条件的不充分泛化能力。为了解决这些问题，本文提出了一种基于有限差分残差约束的波动方程无监督学习方法。我们构建了一种基于结构化网格和有限差分方法的新型有限差分残差约束，以及一种无监督训练策略，使卷积神经网络可以在无数据的情况下进行训练并预测波的前向传播过程。实验结果表明，有限差分残差约束相对于基于物理信息约束的物理感知神经网络（PINNs）具有更好的适应性和更低的拟合成本。

    The wave equation is an important physical partial differential equation, and in recent years, deep learning has shown promise in accelerating or replacing traditional numerical methods for solving it. However, existing deep learning methods suffer from high data acquisition costs, low training efficiency, and insufficient generalization capability for boundary conditions. To address these issues, this paper proposes an unsupervised learning method for the wave equation based on finite difference residual constraints. We construct a novel finite difference residual constraint based on structured grids and finite difference methods, as well as an unsupervised training strategy, enabling convolutional neural networks to train without data and predict the forward propagation process of waves. Experimental results show that finite difference residual constraints have advantages over physics-informed neural networks (PINNs) type physical information constraints, such as easier fitting, lowe
    
[^70]: 绝热量子支持向量机

    Adiabatic Quantum Support Vector Machines. (arXiv:2401.12485v1 [cs.LG])

    [http://arxiv.org/abs/2401.12485](http://arxiv.org/abs/2401.12485)

    本文描述了一种用于训练支持向量机的绝热量子方法，与经典方法相比，我们的方法在时间复杂度上取得了一个数量级的改进，并且在五个基准数据集上取得了与经典方法相当的测试准确率。我们还展示了我们的方法具有良好的可扩展性。

    

    绝热量子计算机可以解决困难的优化问题（例如二次无约束二进制优化问题），并且它们似乎非常适合用于训练机器学习模型。在本文中，我们描述了一种用于训练支持向量机的绝热量子方法。我们展示了我们的量子方法的时间复杂度比经典方法好一个数量级。接下来，我们在五个基准数据集（Iris，Wisconsin乳腺癌（WBC），Wine，Digits和Lambeq）上将我们的量子方法的测试准确率与使用Python中的Scikit-learn库的经典方法进行了比较。我们展示了我们的量子方法获得了与经典方法相当的准确度。最后，我们进行了一项可扩展性研究，其中我们计算了量子方法和经典方法在训练数据集中特征数量和数据点数量增加时的总训练时间。我们的可扩展性结果显示，量子方法具有良好的可扩展性。

    Adiabatic quantum computers can solve difficult optimization problems (e.g., the quadratic unconstrained binary optimization problem), and they seem well suited to train machine learning models. In this paper, we describe an adiabatic quantum approach for training support vector machines. We show that the time complexity of our quantum approach is an order of magnitude better than the classical approach. Next, we compare the test accuracy of our quantum approach against a classical approach that uses the Scikit-learn library in Python across five benchmark datasets (Iris, Wisconsin Breast Cancer (WBC), Wine, Digits, and Lambeq). We show that our quantum approach obtains accuracies on par with the classical approach. Finally, we perform a scalability study in which we compute the total training times of the quantum approach and the classical approach with increasing number of features and number of data points in the training dataset. Our scalability results show that the quantum approa
    
[^71]: 小批量子模最大化算法

    Mini-batch Submodular Maximization. (arXiv:2401.12478v1 [cs.LG])

    [http://arxiv.org/abs/2401.12478](http://arxiv.org/abs/2401.12478)

    我们提出了第一个小批量算法，用于在约束条件下最大化非负单调可分解的子模函数F，该算法在实践中比基于稀疏化方法的做法更好。

    

    我们提出了第一个用于在一组约束条件下最大化一个非负单调可分解的子模函数F的小批量算法，其中F等于$f^i$的和。我们在理论和实践上都超越了基于稀疏化方法的做法。实验证明，我们的算法生成的解比基于稀疏化方法生成的解要好得多。

    We present the first mini-batch algorithm for maximizing a non-negative monotone decomposable submodular function, $F=\sum_{i=1}^N f^i$, under a set of constraints. We improve over the sparsifier based approach both in theory and in practice. We experimentally observe that our algorithm generates solutions that are far superior to those generated by the sparsifier based approach.
    
[^72]: 用深度学习和降阶建模进行贝叶斯非分离哈密顿系统的识别和多项式噪声 (arXiv:2401.12476v1 [stat.ML])

    Bayesian identification of nonseparable Hamiltonians with multiplicative noise using deep learning and reduced-order modeling. (arXiv:2401.12476v1 [stat.ML])

    [http://arxiv.org/abs/2401.12476](http://arxiv.org/abs/2401.12476)

    本文提出了一种用于学习非分离哈密顿系统的结构保持的贝叶斯方法，可以处理统计相关的加性和乘性噪声，并且通过将结构保持方法纳入框架中，提供了对高维系统的高效识别。

    

    本文提出了一种结构保持的贝叶斯方法，用于学习使用随机动力模型的非分离哈密顿系统，该系统允许统计相关的，矢量值的加性和乘性测量噪声。该方法由三个主要方面组成。首先，我们推导了一个用于评估贝叶斯后验中的似然函数所需的统计相关的，矢量值的加性和乘性噪声模型的高斯滤波器。其次，我们开发了一种新算法，用于对高维系统进行高效的贝叶斯系统识别。第三，我们演示了如何将结构保持方法纳入所提议的框架中，使用非分离哈密顿系统作为一个举例的系统类别。我们将贝叶斯方法与一种最先进的机器学习方法在一个典型的非分离哈密顿模型和带有小型噪声训练数据集的混沌双摆模型上进行了比较，实验结果表明

    This paper presents a structure-preserving Bayesian approach for learning nonseparable Hamiltonian systems using stochastic dynamic models allowing for statistically-dependent, vector-valued additive and multiplicative measurement noise. The approach is comprised of three main facets. First, we derive a Gaussian filter for a statistically-dependent, vector-valued, additive and multiplicative noise model that is needed to evaluate the likelihood within the Bayesian posterior. Second, we develop a novel algorithm for cost-effective application of Bayesian system identification to high-dimensional systems. Third, we demonstrate how structure-preserving methods can be incorporated into the proposed framework, using nonseparable Hamiltonians as an illustrative system class. We compare the Bayesian method to a state-of-the-art machine learning method on a canonical nonseparable Hamiltonian model and a chaotic double pendulum model with small, noisy training datasets. The results show that us
    
[^73]: 大型语言模型是所有字符的叠加：通过自我对齐实现任意角色扮演

    Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment. (arXiv:2401.12474v1 [cs.CL])

    [http://arxiv.org/abs/2401.12474](http://arxiv.org/abs/2401.12474)

    本文介绍了一种名为Ditto的角色扮演自我对齐方法，通过对角色知识的利用，使大型语言模型能够模拟角色扮演对话，从而增强其角色扮演能力。实验证明，Ditto在角色扮演基准和MT-Bench的评估中取得了出色的结果。

    

    大量的工作已经投入到通过模拟专有对手来增强开源大型语言模型（LLMs）的角色扮演能力。然而，我们认为LLMs本质上具有角色扮演能力，因为它们在广泛的训练语料库中蕴含了对角色和潜在对话的广泛了解。因此，在这项研究中，我们引入了Ditto，一种用于角色扮演的自我对齐方法。Ditto利用角色知识，鼓励LLM按照指令模拟角色扮演对话，作为阅读理解的一种变体。该方法创建了一个包含4000个字符的角色扮演训练集，相对于目前可用数据集的角色数量增加了十倍。随后，我们使用这个自动生成的数据集对LLM进行微调，以增强其角色扮演能力。通过评估我们精心构建且可复制的角色扮演基准和MT-Bench的角色扮演子集，Ditto在各项指标上表现出色。

    Considerable efforts have been invested in augmenting the role-playing proficiency of open-source large language models (LLMs) by emulating proprietary counterparts. Nevertheless, we posit that LLMs inherently harbor role-play capabilities, owing to the extensive knowledge of characters and potential dialogues ingrained in their vast training corpora. Thus, in this study, we introduce Ditto, a self-alignment method for role-play. Ditto capitalizes on character knowledge, encouraging an instruction-following LLM to simulate role-play dialogues as a variant of reading comprehension. This method creates a role-play training set comprising 4,000 characters, surpassing the scale of currently available datasets by tenfold regarding the number of roles. Subsequently, we fine-tune the LLM using this self-generated dataset to augment its role-playing capabilities. Upon evaluating our meticulously constructed and reproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in var
    
[^74]: 基于图着色的强化学习：理解非标签不变表示的能力和局限性

    Reinforcement Learning for Graph Coloring: Understanding the Power and Limits of Non-Label Invariant Representations. (arXiv:2401.12470v1 [cs.LG])

    [http://arxiv.org/abs/2401.12470](http://arxiv.org/abs/2401.12470)

    本论文将寄存器分配问题转化为图着色问题，并展示了Proximal Policy Optimization模型通过学习解决图着色问题。同时，我们还发现图的标记对模型性能至关重要，并提出了机器学习模型需要具有标签重新排序不变性的图表示。

    

    寄存器分配是现代编译器中最重要的问题之一。在拥有几乎无限数量的用户变量和少量CPU寄存器的情况下，将变量分配给寄存器以避免冲突是一个复杂的任务。本研究将寄存器分配问题转化为图着色问题，并使用PyTorch和OpenAI Gymnasium Environments等技术展示了Proximal Policy Optimization模型能够学习解决图着色问题。同时，我们还展示了图的标记对模型性能的重要性，通过获取图的矩阵表示并对其进行排列来进行测试。然后测试模型在每个排列上的效果，并展示当给出同一图的重新标记时其效果不佳。我们的主要贡献在于表明了机器学习模型需要具有标签重新排序不变性的图表示以实现一致的性能。

    Register allocation is one of the most important problems for modern compilers. With a practically unlimited number of user variables and a small number of CPU registers, assigning variables to registers without conflicts is a complex task. This work demonstrates the use of casting the register allocation problem as a graph coloring problem. Using technologies such as PyTorch and OpenAI Gymnasium Environments we will show that a Proximal Policy Optimization model can learn to solve the graph coloring problem. We will also show that the labeling of a graph is critical to the performance of the model by taking the matrix representation of a graph and permuting it. We then test the model's effectiveness on each of these permutations and show that it is not effective when given a relabeling of the same graph. Our main contribution lies in showing the need for label reordering invariant representations of graphs for machine learning models to achieve consistent performance.
    
[^75]: 基于集中训练和分散执行的多Agent深度强化学习在交通基础设施管理中的应用

    Multi-agent deep reinforcement learning with centralized training and decentralized execution for transportation infrastructure management. (arXiv:2401.12455v1 [cs.MA])

    [http://arxiv.org/abs/2401.12455](http://arxiv.org/abs/2401.12455)

    这项研究提出了一种基于集中训练和分散执行的多Agent深度强化学习框架，用于管理交通基础设施系统的整个生命周期，在处理高维度空间中的不确定性和约束条件时能够降低长期风险和成本。

    

    我们提出了一种多Agent深度强化学习框架，用于在交通基础设施的整个生命周期内进行管理。这种工程系统的生命周期管理是一个需要大量计算的任务，需要适当的顺序检查和维护决策，能够在处理不同的不确定性和约束条件时降低长期风险和成本，这些不确定性和约束条件存在于高维空间中。到目前为止，静态的基于年龄或条件的维护方法和基于风险或定期检查计划主要解决了这类优化问题。然而，在这些方法下，优化性、可扩展性和不确定性限制经常显现出来。本工作中的优化问题以约束的部分可观察马尔可夫决策过程(POMDPs)框架为基础，为具有观察不确定性、风险考虑和随机顺序决策的问题提供了综合的数学基础。

    We present a multi-agent Deep Reinforcement Learning (DRL) framework for managing large transportation infrastructure systems over their life-cycle. Life-cycle management of such engineering systems is a computationally intensive task, requiring appropriate sequential inspection and maintenance decisions able to reduce long-term risks and costs, while dealing with different uncertainties and constraints that lie in high-dimensional spaces. To date, static age- or condition-based maintenance methods and risk-based or periodic inspection plans have mostly addressed this class of optimization problems. However, optimality, scalability, and uncertainty limitations are often manifested under such approaches. The optimization problem in this work is cast in the framework of constrained Partially Observable Markov Decision Processes (POMDPs), which provides a comprehensive mathematical basis for stochastic sequential decision settings with observation uncertainties, risk considerations, and l
    
[^76]: 用于解耦注册和运行时说话人识别模型的后训练嵌入对齐

    Post-Training Embedding Alignment for Decoupling Enrollment and Runtime Speaker Recognition Models. (arXiv:2401.12440v1 [eess.AS])

    [http://arxiv.org/abs/2401.12440](http://arxiv.org/abs/2401.12440)

    该论文提出了一种后训练嵌入对齐的方法，用于解决注册和运行时说话人识别模型耦合的问题。实验结果表明，这种方法在共享说话人嵌入空间中明显优于传统的余弦相似度计算方法。

    

    自动说话人识别是个人化广泛语音服务的关键步骤。传统的说话人识别系统使用对称的注册-验证框架，在一个模型中从注册语音中离线提取嵌入，同时从运行时语音中在线提取嵌入。由于注册和运行时的不同情况，如计算和延迟约束的差异，一些应用程序会从采用不对称的注册-验证框架中获益，这种框架使用不同模型进行注册和运行时的嵌入生成。为了支持这种不对称的说话人识别，其中的两个模型可以独立更新，我们提出使用轻量级神经网络将两个独立模型的嵌入映射到共享的说话人嵌入空间。我们的结果表明，这种方法在共享说话人逻辑空间中显著优于余弦相似度计算，这些计算是使用反向训练的模型进行。

    Automated speaker identification (SID) is a crucial step for the personalization of a wide range of speech-enabled services. Typical SID systems use a symmetric enrollment-verification framework with a single model to derive embeddings both offline for voice profiles extracted from enrollment utterances, and online from runtime utterances. Due to the distinct circumstances of enrollment and runtime, such as different computation and latency constraints, several applications would benefit from an asymmetric enrollment-verification framework that uses different models for enrollment and runtime embedding generation. To support this asymmetric SID where each of the two models can be updated independently, we propose using a lightweight neural network to map the embeddings from the two independent models to a shared speaker embedding space. Our results show that this approach significantly outperforms cosine scoring in a shared speaker logit space for models that were trained with a contra
    
[^77]: 使用安全的联邦学习方法诊断COVID-19

    Secure Federated Learning Approaches to Diagnosing COVID-19. (arXiv:2401.12438v1 [eess.IV])

    [http://arxiv.org/abs/2401.12438](http://arxiv.org/abs/2401.12438)

    本文介绍了一种使用安全联邦学习方法诊断COVID-19的模型，通过在多个设备上进行本地数据样本的算法训练，无需数据共享。该模型在胸部X光诊断方面取得了进展，并解决了HIPAA合规限制的挑战。

    

    最近的流行病凸显了在医院环境中准确诊断COVID-19的重要性。在这方面的一个主要挑战是基于胸部X光片将COVID-19与其他呼吸道疾病区分开来，这还受HIPAA合规限制的限制，这些限制限制了对患者X光片的比较。本文介绍了一种符合HIPAA合规的模型，用于协助诊断COVID-19，利用了联邦学习。联邦学习是一种分布式机器学习方法，允许在多个分散设备上使用本地数据样本进行算法训练，无需数据共享。我们的模型在胸部X光诊断模型方面取得了进展。我们调查了该领域的知名竞赛中的领先模型，并开发了适用于特定医院数据的自己的模型。考虑到模型在联邦学习环境中的运作，我们探讨了偏倚数据更新对模型性能的潜在影响。

    The recent pandemic has underscored the importance of accurately diagnosing COVID-19 in hospital settings. A major challenge in this regard is differentiating COVID-19 from other respiratory illnesses based on chest X-rays, compounded by the restrictions of HIPAA compliance which limit the comparison of patient X-rays. This paper introduces a HIPAA-compliant model to aid in the diagnosis of COVID-19, utilizing federated learning. Federated learning is a distributed machine learning approach that allows for algorithm training across multiple decentralized devices using local data samples, without the need for data sharing. Our model advances previous efforts in chest X-ray diagnostic models. We examined leading models from established competitions in this domain and developed our own models tailored to be effective with specific hospital data. Considering the model's operation in a federated learning context, we explored the potential impact of biased data updates on the model's perform
    
[^78]: Wasserstein差分隐私

    Wasserstein Differential Privacy. (arXiv:2401.12436v1 [cs.LG])

    [http://arxiv.org/abs/2401.12436](http://arxiv.org/abs/2401.12436)

    Wasserstein Differential Privacy是一种用于测量隐私泄漏风险的替代DP框架，满足对称性和三角不等式性质，并具有13个优秀性质。Wasserstein accountant是一种通用的隐私计算方法，可以稳定地获得隐私预算。

    

    差分隐私（DP）在隐私保护机器学习领域取得了显著的成果。然而，现有的DP框架并不满足成为度量的所有条件，这导致它们无法推导出更好的基本私有性质，并导致过高的隐私预算值。我们提出了Wasserstein差分隐私（WDP），这是一种用于测量隐私泄漏风险的替代DP框架，满足对称性和三角不等式性质。我们展示并证明WDP具有13个优秀性质，这些性质可以为WDP比其他DP框架表现更好提供理论支持。此外，我们推导出一种称为Wasserstein机制的通用隐私计算方法，使得WDP可以应用于包含子采样的随机梯度下降（SGD）场景。基本机制、组合和深度学习的实验证明，由Wasserstein机制得到的隐私预算相对稳定。

    Differential privacy (DP) has achieved remarkable results in the field of privacy-preserving machine learning. However, existing DP frameworks do not satisfy all the conditions for becoming metrics, which prevents them from deriving better basic private properties and leads to exaggerated values on privacy budgets. We propose Wasserstein differential privacy (WDP), an alternative DP framework to measure the risk of privacy leakage, which satisfies the properties of symmetry and triangle inequality. We show and prove that WDP has 13 excellent properties, which can be theoretical supports for the better performance of WDP than other DP frameworks. In addition, we derive a general privacy accounting method called Wasserstein accountant, which enables WDP to be applied in stochastic gradient descent (SGD) scenarios containing sub-sampling. Experiments on basic mechanisms, compositions and deep learning show that the privacy budgets obtained by Wasserstein accountant are relatively stable a
    
[^79]: 使用物理信息神经网络对细胞外间隙中的分子传输进行定量分析

    Quantitative Analysis of Molecular Transport in the Extracellular Space Using Physics-Informed Neural Network. (arXiv:2401.12435v1 [cs.AI])

    [http://arxiv.org/abs/2401.12435](http://arxiv.org/abs/2401.12435)

    本文提出了一种使用物理信息神经网络对细胞外间隙中分子传输进行定量分析的新方法，解决了对分子传输形式不清楚的挑战，并实现了自动计算扩散系数和分子速度的优化功能。

    

    大脑的细胞外间隙 (ECS)是位于细胞之间或细胞与血管之间的不规则、极其迂回的纳米级空间，对神经细胞的生存至关重要。它在记忆、情绪和感觉等高级脑功能中起着关键作用。然而，ECS内分子传输的具体形式仍然不清楚。为了解决这个问题，本文提出了一种新的方法，通过使用物理信息神经网络 (PINN) 解决从对流-扩散方程 (ADE) 导出的一个逆问题，定量分析ECS内的分子传输。PINN为ADE提供了一个简化的解决方案，而无需复杂的数学公式或网格设置。此外，PINN的优化功能可自动计算决定长期分子传输的扩散系数和由对流驱动的分子速度。因此，所提出的方法允许进行定量分析。

    The brain extracellular space (ECS), an irregular, extremely tortuous nanoscale space located between cells or between cells and blood vessels, is crucial for nerve cell survival. It plays a pivotal role in high-level brain functions such as memory, emotion, and sensation. However, the specific form of molecular transport within the ECS remain elusive. To address this challenge, this paper proposes a novel approach to quantitatively analyze the molecular transport within the ECS by solving an inverse problem derived from the advection-diffusion equation (ADE) using a physics-informed neural network (PINN). PINN provides a streamlined solution to the ADE without the need for intricate mathematical formulations or grid settings. Additionally, the optimization of PINN facilitates the automatic computation of the diffusion coefficient governing long-term molecule transport and the velocity of molecules driven by advection. Consequently, the proposed method allows for the quantitative analy
    
[^80]: 视觉语言模型中被忽视的尾部

    The Neglected Tails of Vision-Language Models. (arXiv:2401.12425v1 [cs.CV])

    [http://arxiv.org/abs/2401.12425](http://arxiv.org/abs/2401.12425)

    本文通过分析预训练文本来衡量视觉语言模型中概念的频率，并发现流行的VLM数据集展示了长尾概念分布，这与按类别的准确率强烈相关。

    

    视觉语言模型（VLM）在零样本识别方面表现出色，但在视觉概念上的表现极不均衡。例如，尽管CLIP在ImageNet上具有令人印象深刻的平均零样本准确率（72.7％），但在十个概念（如gyromitra和night snake）上的准确率不到10％，这可能是因为这些概念在VLM的非均衡预训练数据中的表示不足。然而，评估这种不平衡是具有挑战性的，因为在VLM的大规模预训练数据中计算特定概念的频率是非常复杂的。我们的工作首次尝试使用分析预训练文本来测量概念频率。我们利用现成的语言模型来帮助计算包含给定概念的同义词的相关文本，并解决语言歧义。我们确认像LAION这样的流行的VLM数据集确实展示了长尾概念分布，并且这与按类别的准确率强烈相关。此外，当代的多模式系统，如视觉聊天机器人和文本-视觉推理模型，在这种长尾分布下经常难以达到高性能。

    Vision-language models (VLMs) excel in zero-shot recognition but exhibit drastically imbalanced performance across visual concepts. For example, CLIP, despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields $<$10% on ten concepts (e.g., gyromitra and night snake), presumably, because these concepts are under-represented in VLMs' imbalanced pretraining data. Yet, assessing this imbalance is challenging as it is non-trivial to calculate the frequency of specific concepts within VLMs' large-scale pretraining data. Our work makes the first attempt to measure the concept frequency by analyzing pretraining texts. We use off-the-shelf language models to help count relevant texts that contain synonyms of the given concepts and resolve linguistic ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit long-tailed concept distributions, which strongly correlate with per-class accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and text-
    
[^81]: DALex: 通过多样聚合实现类似词法选择的选择算法

    DALex: Lexicase-like Selection via Diverse Aggregation. (arXiv:2401.12424v1 [cs.NE])

    [http://arxiv.org/abs/2401.12424](http://arxiv.org/abs/2401.12424)

    本文提出了一种新的选择算法DALex，它通过加权训练案例误差的和来选择最佳个体，相比标准的词法选择更快速。

    

    在进化计算和机器学习的多个领域中，词法选择被证明相比其他选择算法具有优势。词法选择在其标准形式下，根据随机顺序的训练案例进行逐一考虑，并基于此过程对种群或其他集合进行筛选。然而，这个逐步筛选的过程可能会耗时，尤其是在具有大量训练案例的情况下。本文提出了一种新的方法DALex（即多样聚合词法选择），该方法在选择个体方面与词法选择几乎等效，但速度更快。DALex方法根据加权训练案例误差的和选择最佳个体，其中权重是随机采样的。这使得我们可以将选择所需的核心计算形式化为矩阵乘法，而不是递归循环比较，从而可以利用优化和并行化的计算。

    Lexicase selection has been shown to provide advantages over other selection algorithms in several areas of evolutionary computation and machine learning. In its standard form, lexicase selection filters a population or other collection based on randomly ordered training cases that are considered one at a time. This iterated filtering process can be time-consuming, particularly in settings with large numbers of training cases. In this paper, we propose a new method that is nearly equivalent to lexicase selection in terms of the individuals that it selects, but which does so significantly more quickly. The new method, called DALex (for Diversely Aggregated Lexicase), selects the best individual with respect to a weighted sum of training case errors, where the weights are randomly sampled. This allows us to formulate the core computation required for selection as matrix multiplication instead of recursive loops of comparisons, which in turn allows us to take advantage of optimized and pa
    
[^82]: 改进深度贝叶斯模型的变分推断方法

    Towards Improved Variational Inference for Deep Bayesian Models. (arXiv:2401.12418v1 [cs.LG])

    [http://arxiv.org/abs/2401.12418](http://arxiv.org/abs/2401.12418)

    本论文探讨了改进深度贝叶斯模型的变分推断方法，旨在解决深度模型训练过程中的过度自信和不准确预测问题。通过使用变分推断提供的后验近似和边缘似然下界，可以优化超参数并实现模型选择。

    

    在过去的十年中，深度学习在计算机视觉、自然语言处理和强化学习等多个领域取得了重大突破。然而，众所周知，通过最大似然估计训练的深度模型往往过于自信，并且给出的预测不准确。贝叶斯深度学习试图通过给模型参数设置先验来解决这个问题，然后将先验与似然函数结合进行后验推断。然而，对于深度模型来说，真实的后验是无法计算的，因此需要使用近似方法。在本论文中，我们探讨了使用变分推断作为近似的方法，因为它既可以近似后验分布又可以提供边缘似然的下界。如果下界足够紧致，这个下界可以用来优化超参数和进行模型选择。然而，这种能力很少受到重视。

    Deep learning has revolutionized the last decade, being at the forefront of extraordinary advances in a wide range of tasks including computer vision, natural language processing, and reinforcement learning, to name but a few. However, it is well-known that deep models trained via maximum likelihood estimation tend to be overconfident and give poorly-calibrated predictions. Bayesian deep learning attempts to address this by placing priors on the model parameters, which are then combined with a likelihood to perform posterior inference. Unfortunately, for deep models, the true posterior is intractable, forcing the user to resort to approximations. In this thesis, we explore the use of variational inference (VI) as an approximation, as it is unique in simultaneously approximating the posterior and providing a lower bound to the marginal likelihood. If tight enough, this lower bound can be used to optimize hyperparameters and to facilitate model selection. However, this capacity has rarel
    
[^83]: 在边缘上增强神经网络的可靠性：倒置归一化与随机仿射变换

    Enhancing Reliability of Neural Networks at the Edge: Inverted Normalization with Stochastic Affine Transformations. (arXiv:2401.12416v1 [cs.LG])

    [http://arxiv.org/abs/2401.12416](http://arxiv.org/abs/2401.12416)

    本论文介绍了一种通过倒置归一化和随机仿射变换来提高内存计算架构中贝叶斯神经网络鲁棒性和推理精度的方法

    

    贝叶斯神经网络（BayNNs）在预测中自然地提供了不确定性，使其成为安全关键应用的合适选择。此外，利用基于忆阻器的内存计算（IMC）架构实现贝叶斯神经网络使其适用于资源有限的边缘应用。除了预测不确定性外，对计算中的噪音具有固有的鲁棒性也是确保功能安全的关键。特别是，基于忆阻器的IMC对制造和运行时的变化、漂移和故障等各种非理想因素敏感，这可能会显著降低推理精度。在本文中，我们提出了一种方法，以在部署在IMC架构中的BayNNs上增强鲁棒性和推理精度。为了实现这一目标，我们引入了一种新颖的归一化层与随机仿射变换相结合。在各种基准数据集上的实证结果显示出推理精度的逐渐降低

    Bayesian Neural Networks (BayNNs) naturally provide uncertainty in their predictions, making them a suitable choice in safety-critical applications. Additionally, their realization using memristor-based in-memory computing (IMC) architectures enables them for resource-constrained edge applications. In addition to predictive uncertainty, however, the ability to be inherently robust to noise in computation is also essential to ensure functional safety. In particular, memristor-based IMCs are susceptible to various sources of non-idealities such as manufacturing and runtime variations, drift, and failure, which can significantly reduce inference accuracy. In this paper, we propose a method to inherently enhance the robustness and inference accuracy of BayNNs deployed in IMC architectures. To achieve this, we introduce a novel normalization layer combined with stochastic affine transformations. Empirical results in various benchmark datasets show a graceful degradation in inference accurac
    
[^84]: 100个样本可以走多远？通过微小的多语言平行数据解锁全面的零样本跨语言翻译

    How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data. (arXiv:2401.12413v1 [cs.CL])

    [http://arxiv.org/abs/2401.12413](http://arxiv.org/abs/2401.12413)

    本文研究了如何通过仅有的少量微小多语言平行数据来增强以英语为中心的模型的零样本翻译能力，实现大幅度的非英文整体改进，并保持英文方向上的性能能力。研究表明，即使在随机抽取的少量方向集上进行微调，也可以获得可比较的整体改进。

    

    零样本翻译是一个开放问题，旨在在多语言机器翻译（MMT）中翻译训练过程中未见过的语言对。一种常见但资源消耗较大的解决方案是尽可能挖掘更多的翻译方向并添加到平行语料库中。本文展示了通过使用仅有的少量微小多语言平行数据来优化以英语为中心的模型的零样本能力。例如，在EC30数据集上，我们展示了仅使用100个多语言平行样本就能够实现+21.7 ChrF非英文整体改进（870个方向），同时保持在以英语为中心的方向上的能力。我们进一步研究了微调数据的规模效应和其转移能力。令人惊讶的是，我们的实证分析表明，即使是在一个小的、随机抽取的方向集（10%）上进行微调，也可以获得可比较的整体改进。此外，所得到的非英文性能与英文性能非常接近。

    Zero-shot translation is an open problem, aiming to translate between language pairs unseen during training in Multilingual Machine Translation (MMT). A common, albeit resource-consuming, solution is to mine as many translation directions as possible to add to the parallel corpus. In this paper, we show that the zero-shot capability of an English-centric model can be easily enhanced by fine-tuning with a very small amount of multi-parallel data. For example, on the EC30 dataset, we show that up to +21.7 ChrF non-English overall improvements (870 directions) can be achieved by using only 100 multi-parallel samples, meanwhile preserving capability in English-centric directions. We further study the size effect of fine-tuning data and its transfer capabilities. Surprisingly, our empirical analysis shows that comparable overall improvements can be achieved even through fine-tuning in a small, randomly sampled direction set (10\%). Also, the resulting non-English performance is quite close 
    
[^85]: 通过线性探测校准提高上下文学习

    Enhancing In-context Learning via Linear Probe Calibration. (arXiv:2401.12406v1 [cs.CL])

    [http://arxiv.org/abs/2401.12406](http://arxiv.org/abs/2401.12406)

    本研究提出了一种名为线性探测校准（LinC）的技术，通过校准模型的输出概率，显著提高了上下文学习（ICL）在生成预训练变压器（GPT）模型上的测试性能。

    

    上下文学习（ICL）是一种新的自然语言处理范式，利用生成预训练变压器（GPT）等模型。这种方法使用包含上下文演示的提示来为新的查询输入生成相应的输出。然而，在实际情况下应用ICL无法随着样本数量的增加而扩展，并且对不同的提示模板和演示排列缺乏鲁棒性。本文首先展示了使用ICL的GPT模型基于基于香农熵的新度量而导致不可靠的预测。然后，我们提出了一种称为线性探测校准（LinC）的新技术，它可以校准模型的输出概率，从而得到可靠的预测和改进的性能，且仅需要极少量的额外样本（仅需五个已标记的数据样本）。LinC显著提高了GPT模型在各种基准数据集上的ICL测试性能，平均改善效果很大。

    In-context learning (ICL) is a new paradigm for natural language processing that utilizes Generative Pre-trained Transformer (GPT)-like models. This approach uses prompts that include in-context demonstrations to generate the corresponding output for a new query input. However, applying ICL in real cases does not scale with the number of samples, and lacks robustness to different prompt templates and demonstration permutations. In this paper, we first show that GPT-like models using ICL result in unreliable predictions based on a new metric based on Shannon entropy. Then, to solve this problem, we propose a new technique called the Linear Probe Calibration (LinC), a method that calibrates the model's output probabilities, resulting in reliable predictions and improved performance, while requiring only minimal additional samples (as few as five labeled data samples). LinC significantly enhances the ICL test performance of GPT models on various benchmark datasets, with an average improve
    
[^86]: Reddit帖子的纵向情感分类

    Longitudinal Sentiment Classification of Reddit Posts. (arXiv:2401.12382v1 [cs.CL])

    [http://arxiv.org/abs/2401.12382](http://arxiv.org/abs/2401.12382)

    本研究对四所加拿大主要大学的学生在Reddit上撰写的帖子进行纵向情感分类。通过调整情感阈值，我们成功构建了分类器，能够将帖子情感分类为积极和消极类别，并且结果在不同大学数据集中一致。

    

    我们报告了对四所加拿大主要大学的学生撰写的Reddit帖子进行纵向情感分类的结果。我们使用帖子的文本，重点关注2020年至2023年之间的时间。通过精细调整情感阈值在[-0.075, 0.075]范围内，我们成功构建了能够将帖子情感分类为积极和消极类别的分类器。值得注意的是，我们的情感分类结果在四个大学数据集中是一致的。

    We report results of a longitudinal sentiment classification of Reddit posts written by students of four major Canadian universities. We work with the texts of the posts, concentrating on the years 2020-2023. By finely tuning a sentiment threshold to a range of [-0.075,0.075], we successfully built classifiers proficient in categorizing post sentiments into positive and negative categories. Noticeably, our sentiment classification results are consistent across the four university data sets.
    
[^87]: SubgroupTE: 借助子群识别推进治疗效果估计

    SubgroupTE: Advancing Treatment Effect Estimation with Subgroup Identification. (arXiv:2401.12369v1 [cs.LG])

    [http://arxiv.org/abs/2401.12369](http://arxiv.org/abs/2401.12369)

    SubgroupTE是一种新的治疗效果估计模型，通过子群识别提高了估计的精度，考虑了不同子群体的治疗反应，从而更准确地估计治疗效果。

    

    精确估计治疗效果对于评估干预措施的有效性至关重要。虽然深度学习模型在学习治疗效果估计中的可对立表示方面展现出有希望的性能，但这些模型的一个主要局限是将整个人群视为一个同质群体，忽视具有不同治疗效果的不同子群体的多样性。这个局限限制了准确估计治疗效果和提供子群体特定治疗建议的能力。本文提出了一种名为SubgroupTE的新颖治疗效果估计模型，该模型在治疗效果估计中结合了子群识别。SubgroupTE通过考虑子群特定的因果效应，识别具有不同治疗反应的异质子群体，从而更准确地估计治疗效果。此外，SubgroupTE通过迭代优化子群分组和治疗效果估计，提高了估计的精度。

    Precise estimation of treatment effects is crucial for evaluating intervention effectiveness. While deep learning models have exhibited promising performance in learning counterfactual representations for treatment effect estimation (TEE), a major limitation in most of these models is that they treat the entire population as a homogeneous group, overlooking the diversity of treatment effects across potential subgroups that have varying treatment effects. This limitation restricts the ability to precisely estimate treatment effects and provide subgroup-specific treatment recommendations. In this paper, we propose a novel treatment effect estimation model, named SubgroupTE, which incorporates subgroup identification in TEE. SubgroupTE identifies heterogeneous subgroups with different treatment responses and more precisely estimates treatment effects by considering subgroup-specific causal effects. In addition, SubgroupTE iteratively optimizes subgrouping and treatment effect estimation n
    
[^88]: 图神经网络中带有Pfaffian激活函数的VC维度

    VC dimension of Graph Neural Networks with Pfaffian activation functions. (arXiv:2401.12362v1 [stat.ML])

    [http://arxiv.org/abs/2401.12362](http://arxiv.org/abs/2401.12362)

    本文分析了图神经网络（GNN）中使用不同常用激活函数（如sigmoid和双曲正切）时的VC维度，采用了Pfaffian函数理论框架，通过架构参数和合作数量提供了界限。

    

    图神经网络（GNN）近年来作为一种强大的工具出现，以数据驱动的方式学习各种图领域的任务；基于消息传递机制，GNN由于其与Weisfeiler-Lehman（WL）图同构测试密切相关的直观表达而越来越受欢迎，它们已被证明等价。从理论角度看，GNN被证明是通用逼近器，并且最近对具有分段多项式激活函数的GNN的泛化能力（即，对Vapnik Cherovenikis（VC）维度的界限）进行了研究。我们的工作目标是将对GNN的VC维度的分析扩展到其他常用激活函数，如sigmoid和双曲正切，使用Pfaffian函数理论框架。提供了与架构参数（深度，神经元数量，输入尺寸）以及与合作数量有关的界限。

    Graph Neural Networks (GNNs) have emerged in recent years as a powerful tool to learn tasks across a wide range of graph domains in a data-driven fashion; based on a message passing mechanism, GNNs have gained increasing popularity due to their intuitive formulation, closely linked with the Weisfeiler-Lehman (WL) test for graph isomorphism, to which they have proven equivalent. From a theoretical point of view, GNNs have been shown to be universal approximators, and their generalization capability (namely, bounds on the Vapnik Chervonekis (VC) dimension) has recently been investigated for GNNs with piecewise polynomial activation functions. The aim of our work is to extend this analysis on the VC dimension of GNNs to other commonly used activation functions, such as sigmoid and hyperbolic tangent, using the framework of Pfaffian function theory. Bounds are provided with respect to architecture parameters (depth, number of neurons, input size) as well as with respect to the number of co
    
[^89]: 在联邦学习系统中通过基于权重的联合动力学实现高效协作

    Efficient Collaborations through Weight-Driven Coalition Dynamics in Federated Learning Systems. (arXiv:2401.12356v1 [cs.LG])

    [http://arxiv.org/abs/2401.12356](http://arxiv.org/abs/2401.12356)

    本文介绍了一种利用设备模型权重之间的距离评估相似性和差异性的联邦学习模型，通过指导设备之间的联合形成，以及利用质心的概念对来自多个设备的更新进行聚合，提供了一种在物联网机器学习中具有潜力的结构化、优越性能和高效通信的模型。

    

    在物联网时代，分散式机器学习范式的重要性日益突出。本文介绍了一种联邦学习模型，该模型利用设备模型权重之间的欧氏距离来评估它们的相似性和差异性。这是我们系统的基础，用于根据模型权重的接近程度来指导设备之间的联合形成。此外，通过引入质心的概念，表示模型权重的平均值，有助于对来自多个设备的更新进行聚合。我们使用同质和异质数据分布来评估我们的方法，并将其与传统的联邦学习平均算法进行了比较。数值结果表明，我们的方法在提供结构化、性能优越且通信高效的物联网机器学习模型方面具有潜力。

    In the era of the Internet of Things (IoT), decentralized paradigms for machine learning are gaining prominence. In this paper, we introduce a federated learning model that capitalizes on the Euclidean distance between device model weights to assess their similarity and disparity. This is foundational for our system, directing the formation of coalitions among devices based on the closeness of their model weights. Furthermore, the concept of a barycenter, representing the average of model weights, helps in the aggregation of updates from multiple devices. We evaluate our approach using homogeneous and heterogeneous data distribution, comparing it against traditional federated learning averaging algorithm. Numerical results demonstrate its potential in offering structured, outperformed and communication-efficient model for IoT-based machine learning.
    
[^90]: 在边缘计算中扩展量化感知神经架构搜索以进行高效的深度学习

    Scaling Up Quantization-Aware Neural Architecture Search for Efficient Deep Learning on the Edge. (arXiv:2401.12350v1 [cs.CV])

    [http://arxiv.org/abs/2401.12350](http://arxiv.org/abs/2401.12350)

    本文提出了一种在较大规模任务上实现量化感知神经架构搜索（QA-NAS）的方法，通过利用块状形式实现了在边缘计算中进行高效的深度学习。实验结果在语义分割任务上展示了相对于DeepLabV3模型更小的FB-MP模型和更快的INT8模型，而不会损害任务性能。

    

    神经架构搜索（NAS）已成为边缘设备设计准确高效网络的事实标准方法。由于边缘部署通常需要对模型进行量化，因此最近的研究探索了量化感知神经架构搜索（QA-NAS）以搜索高精度高效的量化模型。但是，现有的QA-NAS方法，特别是少位混合精度（FB-MP）方法，在较大任务上无法扩展。因此，QA-NAS大多受限于低规模任务和小型网络。在这项工作中，我们提出了一种方法，通过利用block-wise NAS引入的块状形式，实现了在大规模任务上启用QA-NAS（INT8和FB-MP）。我们在Cityscapes数据集的语义分割任务中展示了强大的结果，找到了比DeepLabV3（INT8）尺寸小33%的FB-MP模型和速度快17.6%的INT8模型，而不会牺牲任务性能。

    Neural Architecture Search (NAS) has become the de-facto approach for designing accurate and efficient networks for edge devices. Since models are typically quantized for edge deployment, recent work has investigated quantization-aware NAS (QA-NAS) to search for highly accurate and efficient quantized models. However, existing QA-NAS approaches, particularly few-bit mixed-precision (FB-MP) methods, do not scale to larger tasks. Consequently, QA-NAS has mostly been limited to low-scale tasks and tiny networks. In this work, we present an approach to enable QA-NAS (INT8 and FB-MP) on large-scale tasks by leveraging the block-wise formulation introduced by block-wise NAS. We demonstrate strong results for the semantic segmentation task on the Cityscapes dataset, finding FB-MP models 33% smaller and INT8 models 17.6% faster than DeepLabV3 (INT8) without compromising task performance.
    
[^91]: OCT-SelfNet:一种基于多模态数据的自监督框架，用于广义和鲁棒的视网膜疾病检测

    OCT-SelfNet: A Self-Supervised Framework with Multi-Modal Datasets for Generalized and Robust Retinal Disease Detection. (arXiv:2401.12344v1 [cs.CV])

    [http://arxiv.org/abs/2401.12344](http://arxiv.org/abs/2401.12344)

    OCT-SelfNet是一种用于眼科疾病检测的自监督机器学习框架，通过结合多模态数据集和两阶段训练方法，实现了广义和鲁棒的检测结果。

    

    尽管AI的革命性影响和本地训练算法的发展，但在医学AI中从多模态数据实现广义学习仍然是一个重大挑战。这个差距限制了可扩展医学AI解决方案的实际部署。为了解决这个挑战，我们的研究提出了一种自监督的鲁棒机器学习框架OCT-SelfNet，用于使用光学相干断层扫描（OCT）图像检测眼科疾病。在这项工作中，综合了来自不同机构的各种数据集，实现了更全面的表征范围。我们的方法通过使用基于SwinV2骨架的自监督预训练和监督微调的双阶段训练方法，提供了一个适用于临床实际部署的解决方案。在三个数据集上进行了大量实验证明了不同编码器骨架、低数据设置、未见数据设置和数据增强的影响。

    Despite the revolutionary impact of AI and the development of locally trained algorithms, achieving widespread generalized learning from multi-modal data in medical AI remains a significant challenge. This gap hinders the practical deployment of scalable medical AI solutions. Addressing this challenge, our research contributes a self-supervised robust machine learning framework, OCT-SelfNet, for detecting eye diseases using optical coherence tomography (OCT) images. In this work, various data sets from various institutions are combined enabling a more comprehensive range of representation. Our method addresses the issue using a two-phase training approach that combines self-supervised pretraining and supervised fine-tuning with a mask autoencoder based on the SwinV2 backbone by providing a solution for real-world clinical deployment. Extensive experiments on three datasets with different encoder backbones, low data settings, unseen data settings, and the effect of augmentation show tha
    
[^92]: 基于对比学习和循环一致性的跨领域转导迁移学习用于目标标注

    Contrastive Learning and Cycle Consistency-based Transductive Transfer Learning for Target Annotation. (arXiv:2401.12340v1 [cs.CV])

    [http://arxiv.org/abs/2401.12340](http://arxiv.org/abs/2401.12340)

    本论文提出了一种基于对比学习和循环一致性的混合非配对域转换网络（H-CUT）来解决自动目标识别（ATR）中标记数据不足的问题。该方法在跨领域转导迁移学习中取得了显著低的FID分数，并通过注意力和熵来强调领域特定区域，以生成高质量的合成图像。

    

    自动目标识别（ATR）的注释是一项极具挑战性的任务，主要由于目标域中标记数据的缺乏。因此，通过利用源域图像的标记信息来构建最佳目标域分类器是至关重要的。先前在文献中提出了一种包含基于CycleGAN的非配对域转换网络的跨领域转导迁移学习（TTL）方法，用于有效的ATR标注。尽管该方法显示出了ATR的巨大潜力，但它严重受到注释性能较低、更高的Fr\'echet Inception Distance（FID）分数以及合成图像中存在的视觉伪影的困扰。为了解决这些问题，我们提出了一种基于对比学习和循环一致性的混合非配对域转换（H-CUT）网络，它实现了显著较低的FID分数。它结合了注意力和熵来强调领域特定的区域，噪声特征混合模块用于生成高质量的合成图像。

    Annotating automatic target recognition (ATR) is a highly challenging task, primarily due to the unavailability of labeled data in the target domain. Hence, it is essential to construct an optimal target domain classifier by utilizing the labeled information of the source domain images. The transductive transfer learning (TTL) method that incorporates a CycleGAN-based unpaired domain translation network has been previously proposed in the literature for effective ATR annotation. Although this method demonstrates great potential for ATR, it severely suffers from lower annotation performance, higher Fr\'echet Inception Distance (FID) score, and the presence of visual artifacts in the synthetic images. To address these issues, we propose a hybrid contrastive learning base unpaired domain translation (H-CUT) network that achieves a significantly lower FID score. It incorporates both attention and entropy to emphasize the domain-specific region, a noisy feature mixup module to generate high
    
[^93]: 利用损失面几何进行 SGD 稳定性的精确刻画

    A Precise Characterization of SGD Stability Using Loss Surface Geometry. (arXiv:2401.12332v1 [cs.LG])

    [http://arxiv.org/abs/2401.12332](http://arxiv.org/abs/2401.12332)

    本文精确刻画了利用损失面几何分析 SGD 稳定性的关键条件，为理解其实际有效性提供了新的方法。

    

    随机梯度下降（SGD）是一种基于经验实证成功的优化算法，但其理论理解相对有限。最近的研究揭示了SGD实际有效性的一个关键因素：它引发的隐式正则化。一些研究探索了在稳定点附近的SGD的线性稳定性属性，作为过参数化神经网络中尖锐度和泛化误差的预测代理（Wu等人，2022；Jastrzebski等人，2019；Cohen等人，2021）。在本文中，我们深入研究了线性稳定性与尖锐度之间的关系。具体而言，我们详细划定了线性稳定性的必要和充分条件，这取决于SGD的超参数和最优解处的尖锐度。为此，我们引入了损失Hessian的新型一致性度量，它包含了损失函数的相关几何属性。

    Stochastic Gradient Descent (SGD) stands as a cornerstone optimization algorithm with proven real-world empirical successes but relatively limited theoretical understanding. Recent research has illuminated a key factor contributing to its practical efficacy: the implicit regularization it instigates. Several studies have investigated the linear stability property of SGD in the vicinity of a stationary point as a predictive proxy for sharpness and generalization error in overparameterized neural networks (Wu et al., 2022; Jastrzebski et al., 2019; Cohen et al., 2021). In this paper, we delve deeper into the relationship between linear stability and sharpness. More specifically, we meticulously delineate the necessary and sufficient conditions for linear stability, contingent on hyperparameters of SGD and the sharpness at the optimum. Towards this end, we introduce a novel coherence measure of the loss Hessian that encapsulates pertinent geometric properties of the loss function that are
    
[^94]: 多Agent动态关系推理用于社交机器人导航

    Multi-Agent Dynamic Relational Reasoning for Social Robot Navigation. (arXiv:2401.12275v1 [cs.RO])

    [http://arxiv.org/abs/2401.12275](http://arxiv.org/abs/2401.12275)

    本文提出了一种多Agent动态关系推理方法，通过明确推断关系结构的演化，来实现在社交机器人导航中的有效性。方法包括推断超边缘以实现群体推理和轨迹预测器生成未来状态。

    

    社交机器人导航在日常生活的各种情景下可以提供帮助，但需要安全的人机交互和高效的轨迹规划。在多Agent交互系统中，建模成对的关系已经被广泛研究，但是捕捉更大规模的群体活动的能力有限。在本文中，我们提出了一种系统的关系推理方法，通过明确推断正在演变的关系结构，展示了其在多Agent轨迹预测和社交机器人导航中的有效性。除了节点对之间的边缘（即Agent），我们还提出了推断超边缘的方法，以自适应地连接多个节点，以便进行群体推理。我们的方法推断动态演化的关系图和超图，以捕捉关系的演化，轨迹预测器利用这些图来生成未来状态。同时，我们提出了对锐度和逻辑稀疏性进行正则化的方法。

    Social robot navigation can be helpful in various contexts of daily life but requires safe human-robot interactions and efficient trajectory planning. While modeling pairwise relations has been widely studied in multi-agent interacting systems, the ability to capture larger-scale group-wise activities is limited. In this paper, we propose a systematic relational reasoning approach with explicit inference of the underlying dynamically evolving relational structures, and we demonstrate its effectiveness for multi-agent trajectory prediction and social robot navigation. In addition to the edges between pairs of nodes (i.e., agents), we propose to infer hyperedges that adaptively connect multiple nodes to enable group-wise reasoning in an unsupervised manner. Our approach infers dynamically evolving relation graphs and hypergraphs to capture the evolution of relations, which the trajectory predictor employs to generate future states. Meanwhile, we propose to regularize the sharpness and sp
    
[^95]: 针对非参数回归的迁移学习：非渐近极小化分析和自适应过程

    Transfer Learning for Nonparametric Regression: Non-asymptotic Minimax Analysis and Adaptive Procedure. (arXiv:2401.12272v1 [stat.ML])

    [http://arxiv.org/abs/2401.12272](http://arxiv.org/abs/2401.12272)

    本文研究了非参数回归的迁移学习问题，提出了一种新的置信阈值估计器来实现渐近最小风险，并发现了迁移学习中的两个独特现象：自动平滑和超加速。此外，我们还提出了一种数据驱动算法，可以适应广泛的参数空间，并在仿真研究和真实世界的例子中证明了该方法的优势。

    

    本文研究了非参数回归的迁移学习问题。首先，我们研究了该问题的非渐近极小风险，并开发了一种新的估计器，称为置信阈值估计器，证明该估计器在一个对数因子的范围内实现了渐近极小的风险。我们的结果展示了迁移学习中的两个独特现象：自动平滑和超加速，这使其与传统设置中的非参数回归有所区别。然后，我们提出了一种数据驱动算法，通过自适应地在广泛的参数空间中实现了对数因子的渐近最小风险。通过仿真研究评估了自适应迁移学习算法的数值性能，并提供了一个真实世界的例子来展示该方法的好处。

    Transfer learning for nonparametric regression is considered. We first study the non-asymptotic minimax risk for this problem and develop a novel estimator called the confidence thresholding estimator, which is shown to achieve the minimax optimal risk up to a logarithmic factor. Our results demonstrate two unique phenomena in transfer learning: auto-smoothing and super-acceleration, which differentiate it from nonparametric regression in a traditional setting. We then propose a data-driven algorithm that adaptively achieves the minimax risk up to a logarithmic factor across a wide range of parameter spaces. Simulation studies are conducted to evaluate the numerical performance of the adaptive transfer learning algorithm, and a real-world example is provided to demonstrate the benefits of the proposed method.
    
[^96]: 基于机器学习的网络入侵检测针对大规模和不平衡数据的应用，使用过采样、堆叠特征嵌入和特征提取方法

    Machine learning-based network intrusion detection for big and imbalanced data using oversampling, stacking feature embedding and feature extraction. (arXiv:2401.12262v1 [cs.CR])

    [http://arxiv.org/abs/2401.12262](http://arxiv.org/abs/2401.12262)

    该论文提出了一种基于机器学习的网络入侵检测模型，使用过采样方法解决数据不平衡问题，结合堆叠特征嵌入和特征提取方法，并通过评估多个基准数据集证明了该模型的性能。

    

    网络安全已成为全球关注的重要问题。入侵检测系统（IDS）通过检测恶意行为和活动在保护互联网络方面发挥关键作用。基于机器学习（ML）的行为分析在IDS中具有发现动态网络威胁、识别异常和恶意行为的潜力。然而，随着数据量的增加，当训练ML模型时，降维变得越来越困难。针对这一问题，我们的论文引入了一种新颖的基于机器学习的网络入侵检测模型，该模型使用随机过采样（RO）来解决数据不平衡问题，并结合聚类结果的堆叠特征嵌入和主成分分析（PCA）进行降维，专门针对大规模和不平衡数据集。我们使用三个前沿的基准数据集（UNSW-NB15、CIC-IDS-2017和CIC-IDS）仔细评估了该模型的性能。

    Cybersecurity has emerged as a critical global concern. Intrusion Detection Systems (IDS) play a critical role in protecting interconnected networks by detecting malicious actors and activities. Machine Learning (ML)-based behavior analysis within the IDS has considerable potential for detecting dynamic cyber threats, identifying abnormalities, and identifying malicious conduct within the network. However, as the number of data grows, dimension reduction becomes an increasingly difficult task when training ML models. Addressing this, our paper introduces a novel ML-based network intrusion detection model that uses Random Oversampling (RO) to address data imbalance and Stacking Feature Embedding based on clustering results, as well as Principal Component Analysis (PCA) for dimension reduction and is specifically designed for large and imbalanced datasets. This model's performance is carefully evaluated using three cutting-edge benchmark datasets: UNSW-NB15, CIC-IDS-2017, and CIC-IDS-201
    
[^97]: 强化学习代理中的新兴支配等级

    Emergent Dominance Hierarchies in Reinforcement Learning Agents. (arXiv:2401.12258v1 [cs.MA])

    [http://arxiv.org/abs/2401.12258](http://arxiv.org/abs/2401.12258)

    本研究在强化学习中探讨了一种新的支配等级现象，并证明了在没有明确编程和内在奖励的情况下，强化学习代理能够自主发明、学习、实施和传递支配等级给新的群体。

    

    现代强化学习算法在各种任务中能够胜过人类。多智能体强化学习(MARL)设置提出了额外的挑战，成功的混合动机代理协作取决于个体和群体目标之间的微妙平衡。社会习惯和规范，往往受到人类机构的启发，被用作实现这种平衡的工具。在本文中，我们研究了一种基本且经过深入研究的社会习惯，即支配等级，它在动物和人类社会中都存在。我们将支配等级的行为理论应用于人工智能代理，并尽可能少地修改现有的术语和定义。我们证明，在没有明确编程或内在奖励的情况下，强化学习代理的群体能够发明、学习、实施和传递支配等级给新的群体。所产生的支配等级有一个

    Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges, and successful cooperation in mixed-motive groups of agents depends on a delicate balancing act between individual and group objectives. Social conventions and norms, often inspired by human institutions, are used as tools for striking this balance.  In this paper, we examine a fundamental, well-studied social convention that underlies cooperation in both animal and human societies: Dominance hierarchies.  We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing the established terminology and definitions with as few amendments as possible. We demonstrate that populations of RL agents, operating without explicit programming or intrinsic rewards, can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge have a 
    
[^98]: 基于混合密度网络的纳米光子学逆向建模中的迁移学习辅助

    Transfer learning-assisted inverse modeling in nanophotonics based on mixture density networks. (arXiv:2401.12254v1 [cs.LG])

    [http://arxiv.org/abs/2401.12254](http://arxiv.org/abs/2401.12254)

    本文提出了一种基于迁移学习增强的混合密度网络模型的纳米光子结构逆向建模方法，可以高效地预测多个可能的解决方案。

    

    纳米光子结构的模拟依赖于电磁求解器，在理解其行为方面起着关键作用。然而，这些求解器通常具有显著的计算成本，使得它们在优化等设计任务中的应用变得不切实际。为了解决这个挑战，已经探索了机器学习技术，用于精确和高效地建模和设计光子器件。深度神经网络在这个领域特别受到关注。它们可以用于创建前向模型和逆向模型。逆向建模方法避免了将前向模型与优化器耦合的需求，并直接执行最佳设计参数值的预测。

    The simulation of nanophotonic structures relies on electromagnetic solvers, which play a crucial role in understanding their behavior. However, these solvers often come with a significant computational cost, making their application in design tasks, such as optimization, impractical. To address this challenge, machine learning techniques have been explored for accurate and efficient modeling and design of photonic devices. Deep neural networks, in particular, have gained considerable attention in this field. They can be used to create both forward and inverse models. An inverse modeling approach avoids the need for coupling a forward model with an optimizer and directly performs the prediction of the optimal design parameters values.  In this paper, we propose an inverse modeling method for nanophotonic structures, based on a mixture density network model enhanced by transfer learning. Mixture density networks can predict multiple possible solutions at a time including their respectiv
    
[^99]: 使用稀疏牛顿迭代加速Sinkhorn算法

    Accelerating Sinkhorn Algorithm with Sparse Newton Iterations. (arXiv:2401.12253v1 [math.OC])

    [http://arxiv.org/abs/2401.12253](http://arxiv.org/abs/2401.12253)

    该论文提出了一种扩展的Sinkhorn算法，通过引入提前停止和牛顿迭代子程序，实现了可能的超指数收敛。他们利用了Sinkhorn算法最大化凹性李雅普诺夫势的特性，发现了势函数的Hessian矩阵近似稀疏，从而将每次迭代的复杂性降低到了$O(n^2)$。

    

    在机器学习中，计算统计分布之间的最优传输距离是一项基本任务。最近的一项突破性进展是熵正则化和Sinkhorn算法，它只使用矩阵缩放并保证近似解的线性运行时间。尽管Sinkhorn算法取得了成功，但由于可能需要大量迭代来达到收敛，它的运行时间仍可能较慢。为了实现可能的超指数收敛，我们提出了Sinkhorn-Newton-Sparse（SNS），这是Sinkhorn算法的一个扩展，通过引入矩阵缩放步骤的提前停止和一个特征牛顿子程序的第二阶段来实现。采用Sinkhorn算法最大化凹性李雅普诺夫势的变分视角，我们得出结论，势函数的Hessian矩阵近似稀疏。稀疏化Hessian矩阵导致每次迭代的复杂性为快速的$O(n^2)$，与传统Sinkhorn算法相同。

    Computing the optimal transport distance between statistical distributions is a fundamental task in machine learning. One remarkable recent advancement is entropic regularization and the Sinkhorn algorithm, which utilizes only matrix scaling and guarantees an approximated solution with near-linear runtime. Despite the success of the Sinkhorn algorithm, its runtime may still be slow due to the potentially large number of iterations needed for convergence. To achieve possibly super-exponential convergence, we present Sinkhorn-Newton-Sparse (SNS), an extension to the Sinkhorn algorithm, by introducing early stopping for the matrix scaling steps and a second stage featuring a Newton-type subroutine. Adopting the variational viewpoint that the Sinkhorn algorithm maximizes a concave Lyapunov potential, we offer the insight that the Hessian matrix of the potential function is approximately sparse. Sparsification of the Hessian results in a fast $O(n^2)$ per-iteration complexity, the same as t
    
[^100]: 非对称核的扩散表示

    Diffusion Representation for Asymmetric Kernels. (arXiv:2401.12251v1 [cs.LG])

    [http://arxiv.org/abs/2401.12251](http://arxiv.org/abs/2401.12251)

    本文扩展了扩散映射形式，用于处理由非对称核引导的数据集。通过使用先验坐标系和基于Fourier基的坐标系，可以减少数据集的维数，并提高计算效率。

    

    我们将扩散映射形式扩展到由非对称核引导的数据集。证明了结果展开的分析收敛性，并提出了一种执行维数减少的算法。本文研究的数据集的几何结构是由非对称核引导的。我们使用先验坐标系来表示这个几何结构，从而能够提高数据集减少维数的计算复杂度。采用与Fourier基的张量积相关联的坐标系来表示扩散映射得到的底层几何结构，从而减少数据集的维数，并利用二维快速Fourier变换算法（2-D FFT）提供的加速。我们将我们的结果与其他特征值展开得到的结果进行比较，并通过合成数据和包括聚类等实际应用的真实数据验证了算法的效率。

    We extend the diffusion-map formalism to data sets that are induced by asymmetric kernels. Analytical convergence results of the resulting expansion are proved, and an algorithm is proposed to perform the dimensional reduction. In this work we study data sets in which its geometry structure is induced by an asymmetric kernel. We use a priori coordinate system to represent this geometry and, thus, be able to improve the computational complexity of reducing the dimensionality of data sets. A coordinate system connected to the tensor product of Fourier basis is used to represent the underlying geometric structure obtained by the diffusion-map, thus reducing the dimensionality of the data set and making use of the speedup provided by the two-dimensional Fast Fourier Transform algorithm (2-D FFT). We compare our results with those obtained by other eigenvalue expansions, and verify the efficiency of the algorithms with synthetic data, as well as with real data from applications including cl
    
[^101]: Orion-14B: 开源多语言大型语言模型

    Orion-14B: Open-source Multilingual Large Language Models. (arXiv:2401.12246v1 [cs.CL])

    [http://arxiv.org/abs/2401.12246](http://arxiv.org/abs/2401.12246)

    Orion-14B是一个具有140亿参数的开源多语言大型语言模型。在该研究中，我们采用数据调度方法对一个基础模型进行训练，使用了来自多种语言的2.5万亿个标记的多样化语料库。我们还对对话应用和其他特定用例进行了微调。评估结果显示，Orion-14B在广泛的任务中取得了领先的性能。我们将Orion-14B模型系列及其相关代码公开，以鼓励未来在这一领域的研究和实际应用。

    

    在这项研究中，我们介绍了Orion-14B，一个具有140亿参数的多语言大型语言模型集合。我们采用数据调度方法，在包括英语、中文、日语、韩语和其他语言的文本中，对一个基础模型进行了训练，使用了来自2.5万亿个标记的多样化语料库。此外，我们还针对对话应用和其他特定用例进行了一系列模型的微调。我们的评估结果表明，Orion-14B在广泛的任务领域中实现了领先的性能。我们将Orion-14B模型系列及其相关代码公开可访问，旨在激发未来在该领域的研究和实际应用。

    In this study, we introduce Orion-14B, a collection of multilingual large language models with 14 billion parameters. We utilize a data scheduling approach to train a foundational model on a diverse corpus of 2.5 trillion tokens, sourced from texts in English, Chinese, Japanese, Korean, and other languages. Additionally, we fine-tuned a series of models tailored for conversational applications and other specific use cases. Our evaluation results demonstrate that Orion-14B achieves state-of-the-art performance across a broad spectrum of tasks. We make the Orion-14B model family and its associated code publicly accessible https://github.com/OrionStarAI/Orion, aiming to inspire future research and practical applications in the field.
    
[^102]: 大规模强化学习用于扩散模型

    Large-scale Reinforcement Learning for Diffusion Models. (arXiv:2401.12244v1 [cs.CV])

    [http://arxiv.org/abs/2401.12244](http://arxiv.org/abs/2401.12244)

    本文介绍了一种大规模强化学习算法，用于改进文本到图像的扩散模型，能够提高模型与人类偏好的一致性，并生成更受人类喜欢的样本。

    

    文本到图像的扩散模型是一类深度生成模型，已经展示出了令人印象深刻的高质量图像生成能力。然而，这些模型容易受到网页规模的文本-图像训练对的隐式偏见的影响，可能无法准确地建模我们关心的图像方面。这可能导致次优的样本、模型偏见和与人类道德和喜好不符的图像。在本文中，我们提出了一种有效的可扩展算法，使用强化学习（RL）来改进扩散模型，涵盖了数百万个图像的人类偏好、组合性和公平性等多样的回报函数。我们展示了我们的方法如何大大优于现有的方法，使扩散模型与人类偏好相一致。我们进一步说明了这如何大大改进了预训练的稳定扩散（SD）模型，所生成的样本在80.3%的时间内优于基本SD模型的样本。

    Text-to-image diffusion models are a class of deep generative models that have demonstrated an impressive capacity for high-quality image generation. However, these models are susceptible to implicit biases that arise from web-scale text-image training pairs and may inaccurately model aspects of images we care about. This can result in suboptimal samples, model bias, and images that do not align with human ethics and preferences. In this paper, we present an effective scalable algorithm to improve diffusion models using Reinforcement Learning (RL) across a diverse set of reward functions, such as human preference, compositionality, and fairness over millions of images. We illustrate how our approach substantially outperforms existing methods for aligning diffusion models with human preferences. We further illustrate how this substantially improves pretrained Stable Diffusion (SD) models, generating samples that are preferred by humans 80.3% of the time over those from the base SD model
    
[^103]: Constraint-Generation Policy Optimization (CGPO): 针对混合离散连续MDPs中的策略优化的非线性规划

    Constraint-Generation Policy Optimization (CGPO): Nonlinear Programming for Policy Optimization in Mixed Discrete-Continuous MDPs. (arXiv:2401.12243v1 [math.OC])

    [http://arxiv.org/abs/2401.12243](http://arxiv.org/abs/2401.12243)

    Constraint-Generation Policy Optimization (CGPO)是一种针对混合离散连续MDPs的策略优化方法，能够提供有界的策略误差保证，推导出最优策略，并生成最坏情况的状态轨迹来诊断策略缺陷。

    

    我们提出了Constraint-Generation Policy Optimization (CGPO)方法，用于在混合离散连续Markov Decision Processes (DC-MDPs)中优化策略参数。CGPO不仅能够提供有界的策略误差保证，覆盖具有表达能力的非线性动力学的无数初始状态范围的DC-MDPs，而且在结束时可以明确地推导出最优策略。此外，CGPO还能够生成最坏情况的状态轨迹来诊断策略缺陷，并提供最优行动的反事实解释。为了实现这些结果，CGPO提出了一个双层的混合整数非线性优化框架，用于在定义的表达能力类别（即分段(非)线性）内优化策略，并将其转化为一个最优的约束生成方法，通过对抗性生成最坏情况的状态轨迹。此外，借助现代非线性优化器，CGPO可以获得解决方案。

    We propose Constraint-Generation Policy Optimization (CGPO) for optimizing policy parameters within compact and interpretable policy classes for mixed discrete-continuous Markov Decision Processes (DC-MDPs). CGPO is not only able to provide bounded policy error guarantees over an infinite range of initial states for many DC-MDPs with expressive nonlinear dynamics, but it can also provably derive optimal policies in cases where it terminates with zero error. Furthermore, CGPO can generate worst-case state trajectories to diagnose policy deficiencies and provide counterfactual explanations of optimal actions. To achieve such results, CGPO proposes a bi-level mixed-integer nonlinear optimization framework for optimizing policies within defined expressivity classes (i.e. piecewise (non)-linear) and reduces it to an optimal constraint generation methodology that adversarially generates worst-case state trajectories. Furthermore, leveraging modern nonlinear optimizers, CGPO can obtain soluti
    
[^104]: BadChain: 大型语言模型中的后门思维链提示

    BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models. (arXiv:2401.12242v1 [cs.CR])

    [http://arxiv.org/abs/2401.12242](http://arxiv.org/abs/2401.12242)

    BadChain是对大型语言模型(LLM)采用从思维链提示(COT)的一种新的后门攻击方法。它不需要访问训练数据集或模型参数，并且具有较低的计算开销。

    

    大型语言模型(LLM)在处理需要系统推理过程的任务时，表现出从思维链提示(COT)中受益。然而，COT提示也在推理中出现新的后门攻击形式，即在特定的后门触发条件下，模型将输出意外的恶意内容。传统的发动后门攻击的方法包括污染训练数据集或在部署过程中直接操纵模型参数。然而，这些方法对于通常通过API访问的商业LLM来说并不实用。本文提出了BadChain，它是针对采用COT提示的LLM的第一个后门攻击，不需要访问训练数据集或模型参数，并且计算开销较低。BadChain利用LLM的内在推理能力，在推理过程中插入一个后门推理步骤。

    Large language models (LLMs) are shown to benefit from chain-of-thought (COT) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, COT prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output unintended malicious content under specific backdoor-triggered conditions during inference. Traditional methods for launching backdoor attacks involve either contaminating the training dataset with backdoored instances or directly manipulating the model parameters during deployment. However, these approaches are not practical for commercial LLMs that typically operate via API access. In this paper, we propose BadChain, the first backdoor attack against LLMs employing COT prompting, which does not require access to the training dataset or model parameters and imposes low computational overhead. BadChain leverages the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning step int
    
[^105]: 低功耗汽车网络中量化神经网络加速器的应用

    Quantised Neural Network Accelerators for Low-Power IDS in Automotive Networks. (arXiv:2401.12240v1 [cs.CR])

    [http://arxiv.org/abs/2401.12240](http://arxiv.org/abs/2401.12240)

    本文研究了低功耗汽车网络中使用量化神经网络加速器作为入侵检测系统的应用，实现了较低的延迟和推理能耗，并且达到了与现有方法相似的分类性能。

    

    本文探讨了作为汽车控制区域网络（CAN）入侵检测系统（IDS）使用低功耗定制量化多层感知器（MLPs）。我们利用AMD / Xilinx的FINN框架对我们的MLP进行量化，训练和生成硬件IP，以检测CAN网络上的拒绝服务（DoS）和模糊攻击，使用ZCU104（XCZU7EV）FPGA作为我们的目标ECU架构，集成IDS能力。我们的方法在延迟（每条消息处理延迟0.12毫秒）和推理能耗（每次推理0.25毫焦）方面取得了显著的改进，同时在文献中达到了类似的分类性能。

    In this paper, we explore low-power custom quantised Multi-Layer Perceptrons (MLPs) as an Intrusion Detection System (IDS) for automotive controller area network (CAN). We utilise the FINN framework from AMD/Xilinx to quantise, train and generate hardware IP of our MLP to detect denial of service (DoS) and fuzzying attacks on CAN network, using ZCU104 (XCZU7EV) FPGA as our target ECU architecture with integrated IDS capabilities. Our approach achieves significant improvements in latency (0.12 ms per-message processing latency) and inference energy consumption (0.25 mJ per inference) while achieving similar classification performance as state-of-the-art approaches in the literature.
    
[^106]: Spatial Scaper：一个用于在真实环境中模拟和增强声场以进行声音事件定位和检测的库

    Spatial Scaper: A Library to Simulate and Augment Soundscapes for Sound Event Localization and Detection in Realistic Rooms. (arXiv:2401.12238v1 [eess.AS])

    [http://arxiv.org/abs/2401.12238](http://arxiv.org/abs/2401.12238)

    Spatial Scaper是一个用于模拟和增强声场的库，可用于声音事件定位和检测。它通过模拟虚拟房间和应用数据增强技术，提供了更具多样性的训练数据，从而改进了声学模型的性能。

    

    声音事件定位和检测(SELD)是机器听觉中的一个重要任务。主要的进展依赖于在特定房间中具有声音事件和强时空标签的模拟数据。SELD数据通过将空间定位的房间冲激响应（RIRs）与声波形进行卷积来模拟，以将声音事件放置在声景中。然而，RIRs需要在特定房间中手动收集。我们提出了SpatialScaper，一个用于SELD数据模拟和增强的库。与现有工具相比，SpatialScaper通过参数（如大小和墙面吸收）模拟虚拟房间。这允许对前景和背景声源进行参数化放置（包括移动）。SpatialScaper还包括可应用于现有SELD数据的数据增强流水线。作为一个案例研究，我们使用SpatialScaper向DCASE SELD数据中添加了房间。使用我们的数据训练模型导致了随着声学多样性的直接提高的渐进性能改进。

    Sound event localization and detection (SELD) is an important task in machine listening. Major advancements rely on simulated data with sound events in specific rooms and strong spatio-temporal labels. SELD data is simulated by convolving spatialy-localized room impulse responses (RIRs) with sound waveforms to place sound events in a soundscape. However, RIRs require manual collection in specific rooms. We present SpatialScaper, a library for SELD data simulation and augmentation. Compared to existing tools, SpatialScaper emulates virtual rooms via parameters such as size and wall absorption. This allows for parameterized placement (including movement) of foreground and background sound sources. SpatialScaper also includes data augmentation pipelines that can be applied to existing SELD data. As a case study, we use SpatialScaper to add rooms to the DCASE SELD data. Training a model with our data led to progressive performance improves as a direct function of acoustic diversity. These 
    
[^107]: 一种分布引导的Mapper算法

    A distribution-guided Mapper algorithm. (arXiv:2401.12237v1 [math.AT])

    [http://arxiv.org/abs/2401.12237](http://arxiv.org/abs/2401.12237)

    这项工作引入了一种名为D-Mapper的分布引导Mapper算法，使用概率模型和数据固有特征生成密度引导的覆盖，并提供增强的拓扑特征。

    

    动机：Mapper算法是拓扑数据分析中探索数据形状的重要工具。使用数据集作为输入，Mapper算法输出代表整个数据集拓扑特征的图形。这个图形通常被认为是数据的一个Reeb图的近似。经典的Mapper算法使用固定的区间长度和重叠比率，这可能无法揭示数据的微妙特征，尤其是当底层结构复杂时。结果：在这项工作中，我们引入了一种名为D-Mapper的分布引导Mapper算法，利用概率模型的属性和数据固有特征生成密度引导的覆盖，并提供增强的拓扑特征。我们提出的算法是一种基于概率模型的方法，可以作为非概率性方法的替代。此外，我们引入了一个度量来考虑重叠聚类的质量和扩展持续同调。

    Motivation: The Mapper algorithm is an essential tool to explore shape of data in topology data analysis. With a dataset as an input, the Mapper algorithm outputs a graph representing the topological features of the whole dataset. This graph is often regarded as an approximation of a reeb graph of data. The classic Mapper algorithm uses fixed interval lengths and overlapping ratios, which might fail to reveal subtle features of data, especially when the underlying structure is complex.  Results: In this work, we introduce a distribution guided Mapper algorithm named D-Mapper, that utilizes the property of the probability model and data intrinsic characteristics to generate density guided covers and provides enhanced topological features. Our proposed algorithm is a probabilistic model-based approach, which could serve as an alternative to non-prababilistic ones. Moreover, we introduce a metric accounting for both the quality of overlap clustering and extended persistence homology to me
    
[^108]: 无害过度拟合对敌对鲁棒性的意外危害

    The Surprising Harmfulness of Benign Overfitting for Adversarial Robustness. (arXiv:2401.12236v1 [cs.LG])

    [http://arxiv.org/abs/2401.12236](http://arxiv.org/abs/2401.12236)

    这项研究证明了即使机器学习模型在训练过程中对噪声数据拟合得很好，对敌对示例具有鲁棒性，但当面临敌对操纵的数据时，过度拟合的模型可能会给系统带来意外的危害。

    

    最近的实证和理论研究已经证明了大规模机器学习模型对训练噪声数据的泛化能力。在本文中，我们证明了一个令人惊讶的结果：即使真正的数据本身对敌对示例具有鲁棒性，而且过度拟合的模型在“标准”的样本外风险目标上是无害的，但在样本外数据受到敌对操纵时，这种无害的过度拟合过程可能是有害的。具体而言，我们的主要结果包含两个部分：（i）在过度参数化线性模型中，最小范数估计总是在“无害过度拟合”设置中导致敌对易受攻击；（ii）我们验证了每个岭回归估计器的标准风险和“敌对”风险之间的渐进权衡结果，这意味着在适当的条件下，这两个项目不能同时通过任何单个岭正则化参数的选择来保持很小。

    Recent empirical and theoretical studies have established the generalization capabilities of large machine learning models that are trained to (approximately or exactly) fit noisy data. In this work, we prove a surprising result that even if the ground truth itself is robust to adversarial examples, and the benignly overfitted model is benign in terms of the ``standard'' out-of-sample risk objective, this benign overfitting process can be harmful when out-of-sample data are subject to adversarial manipulation. More specifically, our main results contain two parts: (i) the min-norm estimator in overparameterized linear model always leads to adversarial vulnerability in the ``benign overfitting'' setting; (ii) we verify an asymptotic trade-off result between the standard risk and the ``adversarial'' risk of every ridge regression estimator, implying that under suitable conditions these two items cannot both be small at the same time by any single choice of the ridge regularization parame
    
[^109]: 通过上下文元图强化学习，实现具有高泛化性和少样本适应性的随机动态电力分配

    Stochastic Dynamic Power Dispatch with High Generalization and Few-Shot Adaption via Contextual Meta Graph Reinforcement Learning. (arXiv:2401.12235v1 [cs.LG])

    [http://arxiv.org/abs/2401.12235](http://arxiv.org/abs/2401.12235)

    本文提出了一种通过上下文元图强化学习方法，实现具有高泛化性和少样本适应性的随机动态电力分配。通过引入更一般化的上下文MDP和可扩展的图表示，该方法能够处理多变量不确定性的实时多阶段随机电力分配问题，填补了现有研究的泛化性和实用性低的缺点。

    

    强化学习是一种用于多阶段顺序决策问题的新兴方法。本文研究了考虑多变量不确定性的实时多阶段随机电力分配问题。目前的研究存在泛化性和实用性不高的问题，即学习到的分配策略只能处理特定情景下的分配问题，如果实际样本和训练样本不一致，性能会显著下降。为填补这些空白，提出了一种新颖的上下文元图强化学习（Meta-GRL）方法，用于实现高度泛化的多阶段最优分配策略。具体而言，引入了更一般化的上下文马尔可夫决策过程（MDP）和可扩展的图表示，以实现更一般化的多阶段随机电力分配建模。提出了一个上层元学习器，用于对不同的分配情景进行编码，并学习如何实现分配任务的识别，而下层策略学习器则学习具体的电力分配策略。

    Reinforcement learning is an emerging approaches to facilitate multi-stage sequential decision-making problems. This paper studies a real-time multi-stage stochastic power dispatch considering multivariate uncertainties. Current researches suffer from low generalization and practicality, that is, the learned dispatch policy can only handle a specific dispatch scenario, its performance degrades significantly if actual samples and training samples are inconsistent. To fill these gaps, a novel contextual meta graph reinforcement learning (Meta-GRL) for a highly generalized multi-stage optimal dispatch policy is proposed. Specifically, a more general contextual Markov decision process (MDP) and scalable graph representation are introduced to achieve a more generalized multi-stage stochastic power dispatch modeling. An upper meta-learner is proposed to encode context for different dispatch scenarios and learn how to achieve dispatch task identification while the lower policy learner learns 
    
[^110]: 一种用于汽车CAN总线的基于轻量级FPGA的IDS-ECU架构

    A Lightweight FPGA-based IDS-ECU Architecture for Automotive CAN. (arXiv:2401.12234v1 [cs.AR])

    [http://arxiv.org/abs/2401.12234](http://arxiv.org/abs/2401.12234)

    本文提出了一种基于轻量级FPGA的IDS-ECU架构，在传统ECU功能的基础上，用于汽车CAN总线入侵检测系统(IDS)。通过使用现成的深度学习处理单元(DPU)IP块进行加速，该架构能够检测多种攻击向量，并具有几乎零的ECU功能开销。

    

    近年来，车辆中复杂的软件驱动功能呈指数增长，导致电子控制单元(ECU)、网络能力和接口数量不断增加。扩展的功能也带来了新的漏洞层面，使入侵检测和管理成为关键能力；然而，这往往会导致更多的ECU和网络元素，因为计算开销很高。在本文中，我们提出了一种整合的ECU架构，包括基于现成混合FPGA设备的汽车控制器区域网络(CAN)入侵检测系统(IDS)，以及传统ECU功能，对于ECU功能几乎没有开销。我们提出了两种量化的多层感知机(QMLP)作为独立的IDS，用于检测一系列攻击向量，包括拒绝服务、模糊测试和欺骗攻击，并使用来自Xilinx的现成深度学习处理单元(DPU)IP块进行加速。

    Recent years have seen an exponential rise in complex software-driven functionality in vehicles, leading to a rising number of electronic control units (ECUs), network capabilities, and interfaces. These expanded capabilities also bring-in new planes of vulnerabilities making intrusion detection and management a critical capability; however, this can often result in more ECUs and network elements due to the high computational overheads. In this paper, we present a consolidated ECU architecture incorporating an Intrusion Detection System (IDS) for Automotive Controller Area Network (CAN) along with traditional ECU functionality on an off-the-shelf hybrid FPGA device, with near-zero overhead for the ECU functionality. We propose two quantised multi-layer perceptrons (QMLP's) as isolated IDSs for detecting a range of attack vectors including Denial-of-Service, Fuzzing and Spoofing, which are accelerated using off-the-shelf deep-learning processing unit (DPU) IP block from Xilinx, operatin
    
[^111]: 自监督学习中的记忆化提高了下游概括能力

    Memorization in Self-Supervised Learning Improves Downstream Generalization. (arXiv:2401.12233v1 [cs.LG])

    [http://arxiv.org/abs/2401.12233](http://arxiv.org/abs/2401.12233)

    自监督学习中的记忆化问题一直是一个挑战，本文提出了SSLMem框架，用于定义自监督学习中的记忆化，并通过实证分析证明了在大规模数据集和强数据增强的情况下，记忆化仍然存在。

    

    自监督学习（SSL）最近因其在无标签数据上训练高性能编码器的能力而受到重视，这些数据通常来源于互联网的抓取。然而，经验证据表明，SSL编码器会记忆其训练数据的私人信息，并在推理时泄露这些信息。现有的监督学习记忆化的理论定义依赖于标签，因此无法适用于SSL。为了填补这一空白，我们提出了SSLMem，一个在SSL内定义记忆化的框架。我们的定义通过比较训练在这些数据点上的编码器和未被训练在这些数据点上的编码器返回的数据点和他们的增强视图的表示的对齐差异。通过对不同编码器架构和数据集的综合实证分析，我们强调了即使SSL依赖于大型数据集和强大的数据增强，这都是监督学习中作为正则化手段的已知技术，记忆化仍然存在。

    Self-supervised learning (SSL) has recently received significant attention due to its ability to train high-performance encoders purely on unlabeled data-often scraped from the internet. This data can still be sensitive and empirical evidence suggests that SSL encoders memorize private information of their training data and can disclose them at inference time. Since existing theoretical definitions of memorization from supervised learning rely on labels, they do not transfer to SSL. To address this gap, we propose SSLMem, a framework for defining memorization within SSL. Our definition compares the difference in alignment of representations for data points and their augmented views returned by both encoders that were trained on these data points and encoders that were not. Through comprehensive empirical analysis on diverse encoder architectures and datasets we highlight that even though SSL relies on large datasets and strong augmentations-both known in supervised learning as regulari
    
[^112]: 机器学习建模SiRNA结构与效力关系及其在Sars-Cov-2 Spike基因上的应用

    Machine Learning Modeling Of SiRNA Structure-Potency Relationship With Applications Against Sars-Cov-2 Spike Gene. (arXiv:2401.12232v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.12232](http://arxiv.org/abs/2401.12232)

    机器学习模型揭示了SiRNA结构与效力关系，为抗击Sars-Cov-2 Spike基因提供了应用前景。

    

    制药研发过程漫长且昂贵，将近十年时间才能将一种新药带到市场上。然而，生物技术、计算方法和机器学习算法的进步具有改革药物发现的潜力，可以加快进程并改善患者的结果。COVID-19疫情进一步加速和深化了对这些技术的潜力的认识，特别是在药物重用和疗效预测方面。与此同时，非小分子治疗模式如细胞疗法、单克隆抗体和RNA干扰技术因其能够针对特定疾病途径和/或患者人群而变得重要。在RNA干扰领域，已进行了许多实验以设计和选择高效的siRNA。然而，针对高效siRNA的已建立模式有时矛盾并且无法一致地确定。

    The pharmaceutical Research and development (R&D) process is lengthy and costly, taking nearly a decade to bring a new drug to the market. However, advancements in biotechnology, computational methods, and machine learning algorithms have the potential to revolutionize drug discovery, speeding up the process and improving patient outcomes. The COVID-19 pandemic has further accelerated and deepened the recognition of the potential of these techniques, especially in the areas of drug repurposing and efficacy predictions. Meanwhile, non-small molecule therapeutic modalities such as cell therapies, monoclonal antibodies, and RNA interference (RNAi) technology have gained importance due to their ability to target specific disease pathways and/or patient populations. In the field of RNAi, many experiments have been carried out to design and select highly efficient siRNAs. However, the established patterns for efficient siRNAs are sometimes contradictory and unable to consistently determine t
    
[^113]: 大规模图的解缠结凝聚

    Disentangled Condensation for Large-scale Graphs. (arXiv:2401.12231v1 [cs.SI])

    [http://arxiv.org/abs/2401.12231](http://arxiv.org/abs/2401.12231)

    本文提出了用于大规模图的解缠结凝聚方法DisCo，通过节点和边的凝聚模块实现了对大规模图的高效缩凝，提高了可扩展性和压缩图的保真度。

    

    图解缠结已经成为一种有趣的技术，为大规模图提供了一种更紧凑但信息丰富的小图，以节省大规模图学习的昂贵成本。尽管取得了有前途的结果，但先前的图解缠结方法常常采用纠缠的缩凝策略，同时涉及节点和边的缩凝，导致大量的GPU内存需求。这种纠缠的策略极大地阻碍了图解缠结的可扩展性，削弱了它对极大规模图的缩凝和高保真度压缩图的能力。因此，本文提出了用于大规模图的解缠结凝聚，简称为DisCo，以提供可扩展的图解缠结，适用于不同规模的图。DisCo的核心是两个互补的组件，即节点和边的凝聚模块，在解缠的方式下实现节点和边的凝聚。

    Graph condensation has emerged as an intriguing technique to provide Graph Neural Networks for large-scale graphs with a more compact yet informative small graph to save the expensive costs of large-scale graph learning. Despite the promising results achieved, previous graph condensation methods often employ an entangled condensation strategy that involves condensing nodes and edges simultaneously, leading to substantial GPU memory demands. This entangled strategy has considerably impeded the scalability of graph condensation, impairing its capability to condense extremely large-scale graphs and produce condensed graphs with high fidelity. Therefore, this paper presents Disentangled Condensation for large-scale graphs, abbreviated as DisCo, to provide scalable graph condensation for graphs of varying sizes. At the heart of DisCo are two complementary components, namely node and edge condensation modules, that realize the condensation of nodes and edges in a disentangled manner. In the 
    
[^114]: 在大型生成模型时代的计算：从云原生到AI原生

    Computing in the Era of Large Generative Models: From Cloud-Native to AI-Native. (arXiv:2401.12230v1 [cs.DC])

    [http://arxiv.org/abs/2401.12230](http://arxiv.org/abs/2401.12230)

    本文探讨了大型生成AI模型和云原生计算架构的交集，提出了一种利用云原生技术和高级机器学习运行时的AI原生计算范式，旨在优化成本并提高资源可访问性，未来的研究和发展具有潜力。

    

    本文研究了大型生成AI模型和云原生计算架构的交集。最近的大型模型（如ChatGPT）在功能上具有革命性，但面临着成本上涨和对高端GPU需求增加的挑战。通过将大型模型作为服务（LMaaS）和云数据库作为服务（DBaaS）进行类比，我们描述了一种AI原生计算范 Paradigm, 其利用了云原生技术（如多租户和无服务器计算）和高级机器学习运行时（如批量LoRA推理）的能力。这些共同的努力旨在优化成本和提高资源可访问性。将这两个领域融合的旅程才刚刚开始，我们希望能够激发未来在这一领域的研究和发展。

    In this paper, we investigate the intersection of large generative AI models and cloud-native computing architectures. Recent large models such as ChatGPT, while revolutionary in their capabilities, face challenges like escalating costs and demand for high-end GPUs. Drawing analogies between large-model-as-a-service (LMaaS) and cloud database-as-a-service (DBaaS), we describe an AI-native computing paradigm that harnesses the power of both cloud-native technologies (e.g., multi-tenancy and serverless computing) and advanced machine learning runtime (e.g., batched LoRA inference). These joint efforts aim to optimize costs-of-goods-sold (COGS) and improve resource accessibility. The journey of merging these two domains is just at the beginning and we hope to stimulate future research and development in this area.
    
[^115]: 通过目标检测和过滤器集成进行多模态数据整理

    Multimodal Data Curation via Object Detection and Filter Ensembles. (arXiv:2401.12225v1 [cs.CV])

    [http://arxiv.org/abs/2401.12225](http://arxiv.org/abs/2401.12225)

    本论文提出了一种通过组合目标检测和弱监督集成的方法，用于整理多模态数据，并在DataComp竞赛的过滤器任务中取得了4%至4.2%的性能改进。

    

    我们提出了一种用于整理多模态数据的方法，该方法用于2023年DataComp竞赛的过滤器任务。我们的技术结合了目标检测和基于弱监督的集成学习。在我们的方法的第一步中，我们使用一个开箱即用的零样本目标检测模型提取细粒度信息并生成各种过滤器设计。在第二步中，我们使用弱监督来集成过滤规则。这种方法相对于最佳基准的性能提高了4％，在撰写本文时在小规模轨道上排名第一。此外，在中等规模轨道上，我们通过简单地使用现有基线和弱监督进行集成，实现了相对于基线的显着4.2％的改进。

    We propose an approach for curating multimodal data that we used for our entry in the 2023 DataComp competition filtering track. Our technique combines object detection and weak supervision-based ensembling. In the first of two steps in our approach, we employ an out-of-the-box zero-shot object detection model to extract granular information and produce a variety of filter designs. In the second step, we employ weak supervision to ensemble filtering rules. This approach results in a 4% performance improvement when compared to the best-performing baseline, producing the top-ranking position in the small scale track at the time of writing. Furthermore, in the medium scale track, we achieve a noteworthy 4.2% improvement over the baseline by simply ensembling existing baselines with weak supervision.
    
[^116]: 使用深度神经网络从多细胞图中学习动力学

    Learning Dynamics from Multicellular Graphs with Deep Neural Networks. (arXiv:2401.12196v1 [physics.bio-ph] CROSS LISTED)

    [http://arxiv.org/abs/2401.12196](http://arxiv.org/abs/2401.12196)

    本研究提出了使用基于图的深度神经网络来预测多细胞集合体的运动能力。实验结果表明，这种方法能够准确地识别多细胞生物系统中的复杂图特征，并超越传统机械模型的能力。同时，研究者建议通过合作努力来构建一个多细胞数据库，以进一步推动多细胞动力学研究的发展。

    

    多细胞自组装的推断是理解形态发生的核心问题，包括胚胎、器官结构、肿瘤等。然而，很难找到能够指示多细胞动力学的结构特征。在这里，我们提出利用基于图的深度神经网络（GNN）的预测能力来发现可以预测动力学的重要图特征。为了证明，我们应用了一个物理学启发的 GNN（piGNN）来预测多细胞集合体的运动能力，从它们在实验和模拟中的位置快照中。我们证明了 piGNN 能够在多细胞生物系统的复杂图特征中导航，这是经典机械模型无法实现的。随着越来越多的多细胞数据的积累，我们提出可以进行合作努力，创建一个多细胞数据库（MDB），从中可以构建一个大型的多细胞图模型。

    The inference of multicellular self-assembly is the central quest of understanding morphogenesis, including embryos, organoids, tumors, and many others. However, it has been tremendously difficult to identify structural features that can indicate multicellular dynamics. Here we propose to harness the predictive power of graph-based deep neural networks (GNN) to discover important graph features that can predict dynamics. To demonstrate, we apply a physically informed GNN (piGNN) to predict the motility of multicellular collectives from a snapshot of their positions both in experiments and simulations. We demonstrate that piGNN is capable of navigating through complex graph features of multicellular living systems, which otherwise can not be achieved by classical mechanistic models. With increasing amounts of multicellular data, we propose that collaborative efforts can be made to create a multicellular data bank (MDB) from which it is possible to construct a large multicellular graph m
    
[^117]: 安全且广义的端到端自主驾驶系统：基于强化学习和示范的研究

    Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2401.11792](http://arxiv.org/abs/2401.11792)

    本文介绍了一种安全且广义的端到端自主驾驶系统 (SGADS)，使用强化学习和示范相结合的方法解决了现有方法的低安全性、泛化能力差和采样效率低的问题，同时引入了变分推理和归一化流以准确预测驾驶轨迹，并提出了鲁棒性安全约束的制定方法。

    

    一个智能驾驶系统应该能够根据当前环境和车辆状态动态制定适当的驾驶策略，同时确保系统的安全性和可靠性。然而，基于强化学习和模仿学习的现有方法存在安全性低、泛化能力差和采样效率低的问题。此外，它们无法准确预测未来的驾驶轨迹，而准确预测未来的驾驶轨迹是做出最优决策的前提。为了解决这些问题，本文引入了一种复杂而多样场景下的安全且广义的端到端自主驾驶系统 (SGADS)。我们的SGADS与变分推理和归一化流结合，使智能车辆能够准确预测未来的驾驶轨迹。此外，我们提出了鲁棒性安全约束的制定。此外，我们将强化学习与示范相结合进行增强学习。

    An intelligent driving system should be capable of dynamically formulating appropriate driving strategies based on the current environment and vehicle status, while ensuring the security and reliability of the system. However, existing methods based on reinforcement learning and imitation learning suffer from low safety, poor generalization, and inefficient sampling. Additionally, they cannot accurately predict future driving trajectories, and the accurate prediction of future driving trajectories is a precondition for making optimal decisions. To solve these problems, in this paper, we introduce a Safe and Generalized end-to-end Autonomous Driving System (SGADS) for complex and various scenarios. Our SGADS incorporates variational inference with normalizing flows, enabling the intelligent vehicle to accurately predict future driving trajectories. Moreover, we propose the formulation of robust safety constraints. Furthermore, we combine reinforcement learning with demonstrations to aug
    
[^118]: GI-PIP：梯度反转攻击是否需要不切实际的辅助数据集？

    GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient Inversion Attacks?. (arXiv:2401.11748v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2401.11748](http://arxiv.org/abs/2401.11748)

    本文提出了一种新颖的梯度反转攻击方法GI-PIP，不需要依赖不切实际的辅助数据集，通过利用异常检测模型从较少的数据中捕获底层分布，并能在图像恢复方面表现出优异的能力，同时在分布泛化方面也更强大。

    

    深度梯度反转攻击通过准确地恢复共享梯度中的隐私数据，对联邦学习构成了严重威胁。然而，现有技术在访问过多的辅助数据方面依赖于不切实际的假设，这违反了联邦学习的基本数据分区原则。本文提出了一种新颖的方法，即使用实用图像先验的梯度反转攻击（GI-PIP），在经过修订的威胁模型下。GI-PIP利用异常检测模型从更少的数据中捕获底层分布，而基于GAN的方法需要消耗更多的数据来合成图像。然后，利用提取出的分布来调节攻击过程作为异常得分损失。实验结果表明，GI-PIP只使用了ImageNet数据的3.8%即可实现16.12 dB的PSNR恢复，而基于GAN的方法则需要超过70%的数据。此外，与基于GAN的方法相比，GI-PIP在分布泛化方面表现出更强大的能力。

    Deep gradient inversion attacks expose a serious threat to Federated Learning (FL) by accurately recovering private data from shared gradients. However, the state-of-the-art heavily relies on impractical assumptions to access excessive auxiliary data, which violates the basic data partitioning principle of FL. In this paper, a novel method, Gradient Inversion Attack using Practical Image Prior (GI-PIP), is proposed under a revised threat model. GI-PIP exploits anomaly detection models to capture the underlying distribution from fewer data, while GAN-based methods consume significant more data to synthesize images. The extracted distribution is then leveraged to regulate the attack process as Anomaly Score loss. Experimental results show that GI-PIP achieves a 16.12 dB PSNR recovery using only 3.8% data of ImageNet, while GAN-based methods necessitate over 70%. Moreover, GI-PIP exhibits superior capability on distribution generalization compared to GAN-based methods. Our approach signif
    
[^119]: TIM: 一种高效的时间交互模块用于脉冲变压器

    TIM: An Efficient Temporal Interaction Module for Spiking Transformer. (arXiv:2401.11687v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2401.11687](http://arxiv.org/abs/2401.11687)

    TIM 是一种高效的时间交互模块，用于增强脉冲神经网络 (SNNs) 的时间数据处理能力。

    

    脉冲神经网络 (SNNs) 作为第三代神经网络，因其生物合理性和计算效率而备受关注，尤其是在处理多样化数据集方面。受神经网络架构进展的启发，注意力机制的整合导致了脉冲变压器的发展。这些变压器在增强 SNNs 的能力方面显示了潜力，尤其是在静态数据集和神经形态数据集的领域。尽管取得了进展，但这些系统中存在着明显的差距，特别是脉冲自注意 (SSA) 机制在利用 SNNs 的时间处理潜力方面的有效性。为了解决这个问题，我们引入了一种名为 Temporal Interaction Module (TIM) 的新型卷积增强技术，旨在增强 SNN 架构中的时间数据处理能力。TIM 的整合与现有的 SNN 框架无缝衔接，高效，只需要额外的最少参数。

    Spiking Neural Networks (SNNs), as the third generation of neural networks, have gained prominence for their biological plausibility and computational efficiency, especially in processing diverse datasets. The integration of attention mechanisms, inspired by advancements in neural network architectures, has led to the development of Spiking Transformers. These have shown promise in enhancing SNNs' capabilities, particularly in the realms of both static and neuromorphic datasets. Despite their progress, a discernible gap exists in these systems, specifically in the Spiking Self Attention (SSA) mechanism's effectiveness in leveraging the temporal processing potential of SNNs. To address this, we introduce the Temporal Interaction Module (TIM), a novel, convolution-based enhancement designed to augment the temporal data processing abilities within SNN architectures. TIM's integration into existing SNN frameworks is seamless and efficient, requiring minimal additional parameters while sign
    
[^120]: HARDCORE：基于残差、扩张卷积神经网络在铁芯中进行任意波形的H场和功率损耗估计

    HARDCORE: H-field and power loss estimation for arbitrary waveforms with residual, dilated convolutional neural networks in ferrite cores. (arXiv:2401.11488v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2401.11488](http://arxiv.org/abs/2401.11488)

    本论文提出了HARDCORE方法，使用残差卷积神经网络和物理信息扩展来高效估计铁芯中任意波形的H场和功率损耗。关键解决方案是通过重建bh曲线并根据曲线的面积估计功率损耗，并采用专家特征工程和信息丰富的输入来实现简明的模型架构。

    

    MagNet Challenge 2023呼吁参赛者开发数据驱动模型，用于对环形铁芯中的材料特定、波形无关的稳态功率损耗进行估计。以下介绍的HARDCORE方法表明，预先在观测数据上训练的具有物理信息扩展的残差卷积神经网络可以高效完成这项任务。一个关键的解决方案元素是一个中间模型层，首先重建bh曲线，然后根据曲线的面积估计功率损耗，从而使所提出的拓扑结构在物理上可解释。此外，着重于基于专家的特征工程和信息丰富的输入，以实现简明的模型架构。为每种材料从头开始训练一个模型，而拓扑结构保持不变。模型之间进行了帕累托式的性能权衡。

    The MagNet Challenge 2023 calls upon competitors to develop data-driven models for the material-specific, waveform-agnostic estimation of steady-state power losses in toroidal ferrite cores. The following HARDCORE (H-field and power loss estimation for Arbitrary waveforms with Residual, Dilated convolutional neural networks in ferrite COREs) approach shows that a residual convolutional neural network with physics-informed extensions can serve this task efficiently when trained on observational data beforehand. One key solution element is an intermediate model layer which first reconstructs the bh curve and then estimates the power losses based on the curve's area rendering the proposed topology physically interpretable. In addition, emphasis was placed on expert-based feature engineering and information-rich inputs in order to enable a lean model architecture. A model is trained from scratch for each material, while the topology remains the same. A Pareto-style trade-off between model 
    
[^121]: 预测过敏性鼻炎亚卡激素皮下免疫治疗中患者依从性的序列模型

    Sequential Model for Predicting Patient Adherence in Subcutaneous Immunotherapy for Allergic Rhinitis. (arXiv:2401.11447v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.11447](http://arxiv.org/abs/2401.11447)

    本研究利用新颖的机器学习模型，准确预测患者的非依从风险和相关的系统症状评分，为长期过敏性鼻炎亚卡激素皮下免疫治疗的管理提供了一种新的方法。

    

    目标：皮下免疫治疗(SCIT)是过敏性鼻炎的长效因果治疗。如何提高患者对变应原免疫治疗(AIT)的依从性以最大化治疗效果，在AIT管理中起着至关重要的作用。本研究旨在利用新颖的机器学习模型，准确预测患者的非依从风险和相关的系统症状评分，为长期AIT的管理提供一种新的方法。方法：本研究开发和分析了两种模型，序列潜在行为者-评论家模型(SLAC)和长短期记忆模型(LSTM)，并基于评分和依从性预测能力进行评估。结果：在排除第一时间步的偏倚样本后，SLAC模型的预测依从准确率为60%-72%，而LSTM模型的准确率为66%-84%，根据时间步长的不同而变化。SLAC模型的均方根误差(RMSE)范围在0.93到2.22之间，而LSTM模型的RMSE范围在...

    Objective: Subcutaneous Immunotherapy (SCIT) is the long-lasting causal treatment of allergic rhinitis. How to enhance the adherence of patients to maximize the benefit of allergen immunotherapy (AIT) plays a crucial role in the management of AIT. This study aims to leverage novel machine learning models to precisely predict the risk of non-adherence of patients and related systematic symptom scores, to provide a novel approach in the management of long-term AIT.  Methods: The research develops and analyzes two models, Sequential Latent Actor-Critic (SLAC) and Long Short-Term Memory (LSTM), evaluating them based on scoring and adherence prediction capabilities.  Results: Excluding the biased samples at the first time step, the predictive adherence accuracy of the SLAC models is from $60\,\%$ to $72\%$, and for LSTM models, it is $66\,\%$ to $84\,\%$, varying according to the time steps. The range of Root Mean Square Error (RMSE) for SLAC models is between $0.93$ and $2.22$, while for L
    
[^122]: PartIR: 为机器学习组合SPMD分区策略

    PartIR: Composing SPMD Partitioning Strategies for Machine Learning. (arXiv:2401.11202v1 [cs.LG])

    [http://arxiv.org/abs/2401.11202](http://arxiv.org/abs/2401.11202)

    PartIR是一种用于机器学习的分区系统，具备表达力强和可预测性强的特点。它通过高级程序员发出的分区策略驱动，并采用增量重写方法，能够组合不同的分片策略，评估结果表明其可预测性、表达能力和达到峰值性能能力强。

    

    现代大规模神经网络（NN）的训练需要结合数据、模型或优化器分片的并行化策略。当策略变得复杂时，分区工具需要具备以下特点：1）表达力强，允许组合简单策略；2）可预测性强，可以通过分析估算性能。我们提出了PartIR，一种用于NN分区的设计。PartIR采用增量重写方法，与硬件和运行时无关。我们提供了一个简单而强大的API用于组合分片策略，并提供了一个模拟器进行验证。整个过程由高级程序员发出的分区策略驱动，既可以手动也可以自动。重要的是，这些策略与模型代码分开指定，易于更改。我们通过对几种不同模型的评估来展示PartIR的可预测性、表达能力和达到峰值性能的能力。

    Training of modern large neural networks (NN) requires a combination of parallelization strategies encompassing data, model, or optimizer sharding. When strategies increase in complexity, it becomes necessary for partitioning tools to be 1) expressive, allowing the composition of simpler strategies, and 2) predictable to estimate performance analytically. We present PartIR, our design for a NN partitioning system. PartIR is focused on an incremental approach to rewriting and is hardware-and-runtime agnostic. We present a simple but powerful API for composing sharding strategies and a simulator to validate them. The process is driven by high-level programmer-issued partitioning tactics, which can be both manual and automatic. Importantly, the tactics are specified separately from the model code, making them easy to change. We evaluate PartIR on several different models to demonstrate its predictability, expressibility, and ability to reach peak performance..
    
[^123]: 交通分类的数据增强

    Data Augmentation for Traffic Classification. (arXiv:2401.10754v1 [cs.LG])

    [http://arxiv.org/abs/2401.10754](http://arxiv.org/abs/2401.10754)

    这项工作通过对交通分类任务中的数据增强进行分析和实验，发现时间序列顺序和掩码的增强在交通分类中更适用，同时提出了简单的潜在空间分析可以解释增强效果的思路。

    

    数据增强（DA）是一种广泛应用于计算机视觉（CV）和自然语言处理（NLP）任务中以提高模型性能的技术，通过添加合成样本来丰富训练数据。然而，在网络环境中，特别是在交通分类（TC）任务中，DA很难获得广泛应用。在本研究中，我们通过将18种增强函数应用于3个TC数据集的数据包时间序列作为输入表示，并考虑各种训练条件，来填补这个空白。我们的结果表明，（i）DA可以获得以前未被探索的好处，（ii）作用于时间序列的顺序和掩码的增强在TC中更合适，以及（iii）简单的潜在空间分析可以提供关于为什么增强会产生积极或消极影响的一些线索。

    Data Augmentation (DA) -- enriching training data by adding synthetic samples -- is a technique widely adopted in Computer Vision (CV) and Natural Language Processing (NLP) tasks to improve models performance. Yet, DA has struggled to gain traction in networking contexts, particularly in Traffic Classification (TC) tasks. In this work, we fulfill this gap by benchmarking 18 augmentation functions applied to 3 TC datasets using packet time series as input representation and considering a variety of training conditions. Our results show that (i) DA can reap benefits previously unexplored with (ii) augmentations acting on time series sequence order and masking being a better suit for TC and (iii) simple latent space analysis can provide hints about why augmentations have positive or negative effects.
    
[^124]: ChatQA: 构建GPT-4级对话问答模型

    ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])

    [http://arxiv.org/abs/2401.10225](http://arxiv.org/abs/2401.10225)

    ChatQA是一系列对话问答模型，可以达到GPT-4级别的准确性。通过两阶段的指令调整方法，可以显著提高大型语言模型在零-shot对话问答中的结果。使用密集检索器进行问答数据集的微调可以实现与最先进的查询重写模型相当的结果，同时降低部署成本。ChatQA-70B在10个对话问答数据集上的平均得分超过了GPT-4，且不依赖于任何来自OpenAI GPT模型的合成数据。

    

    在这项工作中，我们介绍了ChatQA，一系列具有GPT-4级别准确性的对话问答模型。具体地，我们提出了一个两阶段的指令调整方法，可以显著提高大型语言模型（LLM）在零-shot对话问答中的结果。为了处理对话问答中的检索问题，我们在多轮问答数据集上进行了密集检索器的微调，这样可以提供与使用最先进的查询重写模型相当的结果，同时大大降低部署成本。值得注意的是，我们的ChatQA-70B可以在10个对话问答数据集的平均分上超过GPT-4（54.14 vs. 53.90），而不依赖于OpenAI GPT模型的任何合成数据。

    In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
    
[^125]: 空间-时间大语言模型用于交通预测

    Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v1 [cs.LG])

    [http://arxiv.org/abs/2401.10134](http://arxiv.org/abs/2401.10134)

    本文提出了一种空间-时间大语言模型（ST-LLM）用于交通预测，通过参数扩展和预训练来提高预测准确性，并利用空间-时间嵌入模块学习标记的空间位置和全局时间表示。

    

    交通预测是智能交通系统的关键组成部分，它通过使用历史数据来预测特定位置的未来交通情况。尽管现有的交通预测模型通常强调开发复杂的神经网络结构，但它们的准确性并未相应提高。最近，大型语言模型（LLMs）在时间序列分析方面显示出了出色的能力。与现有模型不同，LLMs主要通过参数扩展和广泛的预训练来进步，同时保持其基本结构。本文提出了一种空间-时间大语言模型（ST-LLM）用于交通预测。具体而言，ST-LLM将每个位置的时间步长定义为标记，并结合空间-时间嵌入模块来学习标记的空间位置和全局时间表示。然后，这些表示被融合以为每个标记提供统一的空间和时间信息。

    Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we
    
[^126]: 无限时域图滤波器：利用幂级数增强稀疏信息聚合

    Infinite-Horizon Graph Filters: Leveraging Power Series to Enhance Sparse Information Aggregation. (arXiv:2401.09943v1 [cs.LG])

    [http://arxiv.org/abs/2401.09943](http://arxiv.org/abs/2401.09943)

    提出了一种新颖的图幂滤波器神经网络 (GPFN)，通过使用幂级数图滤波器来增强节点分类。GPFN设计了一种基于收敛幂级数的具有无限接收域的图滤波器构建方法，并能集成任何幂级数并捕捉长程依赖关系。

    

    近年来，图神经网络在各种图学习任务中表现出了相当的有效性，特别是基于消息传递方法。然而，它们的性能常常受到有限接收域的限制，在稀疏图存在的情况下，挑战变得更加严峻。鉴于具有无限扩展能力的幂级数，我们提出了一种新颖的图幂滤波器神经网络 (GPFN)，通过使用幂级数图滤波器来增强节点分类。具体而言，我们的GPFN设计了一种基于收敛幂级数的具有无限接收域的图滤波器构建方法，可以在频谱和空间域中进行分析。此外，我们还从理论上证明了我们的GPFN是一个通用框架，可以集成任何幂级数并捕捉长程依赖关系。最后，我们在三个数据集上进行了实验，并展示了GPFN在稀疏图上的优越性能。

    Graph Neural Networks (GNNs) have shown considerable effectiveness in a variety of graph learning tasks, particularly those based on the message-passing approach in recent years. However, their performance is often constrained by a limited receptive field, a challenge that becomes more acute in the presence of sparse graphs. In light of the power series, which possesses infinite expansion capabilities, we propose a novel \underline{G}raph \underline{P}ower \underline{F}ilter \underline{N}eural Network (GPFN) that enhances node classification by employing a power series graph filter to augment the receptive field. Concretely, our GPFN designs a new way to build a graph filter with an infinite receptive field based on the convergence power series, which can be analyzed in the spectral and spatial domains. Besides, we theoretically prove that our GPFN is a general framework that can integrate any power series and capture long-range dependencies. Finally, experimental results on three data
    
[^127]: 使用多模态深度学习的不确定性感知硬件特洛伊检测

    Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep Learning. (arXiv:2401.09479v1 [cs.CR])

    [http://arxiv.org/abs/2401.09479](http://arxiv.org/abs/2401.09479)

    本文提出了一种使用多模态深度学习进行硬件特洛伊检测的方法，通过生成对抗网络扩充数据，并采用早融合和晚融合策略进行评估。通过估计不确定性量化指标，实现风险感知的决策制定。

    

    在零信任的无厂无印造制造时代，硬件特洛伊在芯片生产的各个阶段被插入的风险增加了。为了应对这一问题，已经开发了各种机器学习解决方案用于检测硬件特洛伊。尽管大部分关注点都集中在统计学或深度学习方法上，但受到特洛伊感染基准样本数量有限的影响，检测准确性受限，无法检测到零日特洛伊。为了填补这一差距，我们首先采用生成对抗网络来扩充数据，以两种替代表示模态，图形和表格，确保数据集以代表性的方式分布。此外，我们提出了一种多模态深度学习方法来检测硬件特洛伊，并评估了早融合和晚融合策略的结果。我们还估计了每个预测的不确定性量化指标，用于风险感知的决策制定。结果不仅确认了我们方法的有效性，而且表明了不确定性估计对硬件特洛伊检测的重要性。

    The risk of hardware Trojans being inserted at various stages of chip production has increased in a zero-trust fabless era. To counter this, various machine learning solutions have been developed for the detection of hardware Trojans. While most of the focus has been on either a statistical or deep learning approach, the limited number of Trojan-infected benchmarks affects the detection accuracy and restricts the possibility of detecting zero-day Trojans. To close the gap, we first employ generative adversarial networks to amplify our data in two alternative representation modalities, a graph and a tabular, ensuring that the dataset is distributed in a representative manner. Further, we propose a multimodal deep learning approach to detect hardware Trojans and evaluate the results from both early fusion and late fusion strategies. We also estimate the uncertainty quantification metrics of each prediction for risk-aware decision-making. The outcomes not only confirms the efficacy of our
    
[^128]: 部分音标化：一种上下文对比推理方法

    Partial Diacritization: A Context-Contrastive Inference Approach. (arXiv:2401.08919v1 [cs.CL])

    [http://arxiv.org/abs/2401.08919](http://arxiv.org/abs/2401.08919)

    部分音标化是选择标记部分字符来提高阅读可读性和准确性的新方法。上下文对比的部分音标化（CCPD）集成了现有的阿拉伯音标化系统，并通过衡量部分音标化的新指标来判断需要标记哪些字符。

    

    音标化在提高阿拉伯文本可读性和消除歧义方面起着关键作用。目前的努力主要集中在标记每个符合条件的字符（全音标化）。相比之下，部分音标化（PD）是选择标记子集以在必要时提供帮助。研究表明，过多的音标符号会妨碍熟练读者，降低阅读速度和准确性。我们进行了一项行为实验，并显示出部分标记的文本通常比完全标记的文本更容易阅读，有时甚至比纯文本更容易。在这种情况下，我们介绍了上下文对比的部分音标化（CCPD）-一种与现有阿拉伯音标化系统无缝集成的新方法。CCPD对每个单词进行两次处理，一次有上下文，一次没有，并且只对两次推理之间存在差异的字符进行音标化。此外，我们还引入了衡量部分音标化的新指标。

    Diacritization plays a pivotal role in improving readability and disambiguating the meaning of Arabic texts. Efforts have so far focused on marking every eligible character (Full Diacritization). Comparatively overlooked, Partial Diacritzation (PD) is the selection of a subset of characters to be marked to aid comprehension where needed. Research has indicated that excessive diacritic marks can hinder skilled readers--reducing reading speed and accuracy. We conduct a behavioral experiment and show that partially marked text is often easier to read than fully marked text, and sometimes easier than plain text. In this light, we introduce Context-Contrastive Partial Diacritization (CCPD)--a novel approach to PD which integrates seamlessly with existing Arabic diacritization systems. CCPD processes each word twice, once with context and once without, and diacritizes only the characters with disparities between the two inferences. Further, we introduce novel indicators for measuring partial
    
[^129]: SpecSTG:一种用于概率时空交通预测的快速谱扩散框架

    SpecSTG: A Fast Spectral Diffusion Framework for Probabilistic Spatio-Temporal Traffic Forecasting. (arXiv:2401.08119v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.08119](http://arxiv.org/abs/2401.08119)

    我们提出了一种快速谱扩散框架SpecSTG，用于概率时空交通预测。该方法在谱域中生成未来时间序列的傅里叶表示，利用空间信息来更好地利用交通数据中的空间依赖性和系统模式。

    

    交通预测是时空图学习的一个重要应用，传统上依赖确定性模型进行准确的点估计。然而，这些模型无法识别未来观测中意外波动的潜在风险。为了解决这个问题，概率方法，特别是扩散模型的变种，已成为具有不确定性感知的解决方案。然而，现有的扩散方法通常只关注为交通网络中的每个传感器生成单独的未来时间序列，导致空间网络特征在概率学习过程中参与不足。为了更好地利用交通数据中固有的空间依赖性和系统模式，我们提出了一种新颖的谱扩散框架——SpecSTG。我们的方法生成未来时间序列的傅里叶表示，将学习过程转化为充满空间信息的谱域。此外，我们的方法还结合了空间网络特征的影响。

    Traffic forecasting, a crucial application of spatio-temporal graph (STG) learning, has traditionally relied on deterministic models for accurate point estimations. Yet, these models fall short of identifying latent risks of unexpected volatility in future observations. To address this gap, probabilistic methods, especially variants of diffusion models, have emerged as uncertainty-aware solutions. However, existing diffusion methods typically focus on generating separate future time series for individual sensors in the traffic network, resulting in insufficient involvement of spatial network characteristics in the probabilistic learning process. To better leverage spatial dependencies and systematic patterns inherent in traffic data, we propose SpecSTG, a novel spectral diffusion framework. Our method generates the Fourier representation of future time series, transforming the learning process into the spectral domain enriched with spatial information. Additionally, our approach incorp
    
[^130]: 通过双层优化进行联合无监督和监督训练的自动语音识别方法

    Joint Unsupervised and Supervised Training for Automatic Speech Recognition via Bilevel Optimization. (arXiv:2401.06980v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2401.06980](http://arxiv.org/abs/2401.06980)

    本文提出了一种双层优化的训练方法，用于自动语音识别，通过联合无监督和监督训练来提高性能。

    

    本文提出了一种新颖的基于双层优化的训练方法，用于自动语音识别（ASR）任务中的声学模型训练，我们称之为“双层联合无监督和监督训练（BL-JUST）”。BL-JUST采用下层和上层优化，分别使用无监督损失和监督损失，利用最近在惩罚型双层优化方面取得的进展来解决这一具有可承受复杂度和严格收敛性保证的挑战性ASR问题。

    In this paper, we present a novel bilevel optimization-based training approach to training acoustic models for automatic speech recognition (ASR) tasks that we term {bi-level joint unsupervised and supervised training (BL-JUST)}. {BL-JUST employs a lower and upper level optimization with an unsupervised loss and a supervised loss respectively, leveraging recent advances in penalty-based bilevel optimization to solve this challenging ASR problem with affordable complexity and rigorous convergence guarantees.} To evaluate BL-JUST, extensive experiments on the LibriSpeech and TED-LIUM v2 datasets have been conducted. BL-JUST achieves superior performance over the commonly used pre-training followed by fine-tuning strategy.
    
[^131]: RudolfV：一种由病理学家为病理学家构建的基础模型

    RudolfV: A Foundation Model by Pathologists for Pathologists. (arXiv:2401.04079v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2401.04079](http://arxiv.org/abs/2401.04079)

    本文提出了一种由病理学家为病理学家构建的基础模型，通过数据整理和融入病理学领域知识，扩展了数字病理学全玻片图像基础模型，解决了人工智能在处理罕见疾病方面的挑战。

    

    组织病理学在临床医学和生物医学研究中起着核心作用。虽然人工智能在许多病理学任务上显示出有希望的结果，但在泛化和处理训练数据稀缺的罕见疾病方面仍然是一个挑战。在学习来自有限标记数据之前，从无标记数据中提取知识到基础模型可以解决这些挑战。在这项工作中，我们通过半自动数据整理和融入病理学领域知识，扩展了数字病理学全玻片图像基础模型的最新技术。具体而言，我们结合计算和病理学领域知识(1)整理了一个多样化的数据集，包括10.3万个玻片图像对应的7.5亿个图像块，涵盖了来自欧美不同修复、染色和扫描协议以及不同指示和实验室的数据，(2)用于对语义上相似的玻片和组织块进行分组。

    Histopathology plays a central role in clinical medicine and biomedical research. While artificial intelligence shows promising results on many pathological tasks, generalization and dealing with rare diseases, where training data is scarce, remains a challenge. Distilling knowledge from unlabeled data into a foundation model before learning from, potentially limited, labeled data provides a viable path to address these challenges. In this work, we extend the state of the art of foundation models for digital pathology whole slide images by semi-automated data curation and incorporating pathologist domain knowledge. Specifically, we combine computational and pathologist domain knowledge (1) to curate a diverse dataset of 103k slides corresponding to 750 million image patches covering data from different fixation, staining, and scanning protocols as well as data from different indications and labs across the EU and US, (2) for grouping semantically similar slides and tissue patches, and 
    
[^132]: S$^{2}$-DMs：跳过步骤的扩散模型

    S$^{2}$-DMs:Skip-Step Diffusion Models. (arXiv:2401.01520v1 [cs.CV])

    [http://arxiv.org/abs/2401.01520](http://arxiv.org/abs/2401.01520)

    本论文提出了一种新的训练方法S$^{2}$-DMs，通过创新的$L_{skip}$重新整合选择性采样阶段中省略的信息，显著提高了样本质量，并且实现简单，对代码修改要求少，与各种采样算法兼容。

    

    扩散模型已经成为强大的生成工具，样本质量与生成对抗网络（GANs）相当，并且反映了自回归模型的似然分数。其中一部分模型，如DDIMs，展示了固有的不对称性：它们在训练过程中使用$T$个步骤，但在生成过程中只从其中的子集进行采样。这种选择性采样方法虽然优化了速度，但无意中错过了未采样步骤中的重要信息，导致样本质量可能出现问题。为了解决这个问题，我们提出了S$^{2}$-DMs，一种新的训练方法，使用创新的$L_{skip}$，精心设计以重新整合在选择性采样阶段中省略的信息。这种方法的好处很多：它显著提高了样本质量，实现起来非常简单，需要最少的代码修改，并且足够灵活，可以与各种采样算法兼容。在CIFAR10数据集上，使用我们的模型训练的结果...

    Diffusion models have emerged as powerful generative tools, rivaling GANs in sample quality and mirroring the likelihood scores of autoregressive models. A subset of these models, exemplified by DDIMs, exhibit an inherent asymmetry: they are trained over $T$ steps but only sample from a subset of $T$ during generation. This selective sampling approach, though optimized for speed, inadvertently misses out on vital information from the unsampled steps, leading to potential compromises in sample quality. To address this issue, we present the S$^{2}$-DMs, which is a new training method by using an innovative $L_{skip}$, meticulously designed to reintegrate the information omitted during the selective sampling phase. The benefits of this approach are manifold: it notably enhances sample quality, is exceptionally simple to implement, requires minimal code modifications, and is flexible enough to be compatible with various sampling algorithms. On the CIFAR10 dataset, models trained using our 
    
[^133]: 使用对抗训练和知识蒸馏的可解释性导向叶片疾病分类

    Explainability-Driven Leaf Disease Classification using Adversarial Training and Knowledge Distillation. (arXiv:2401.00334v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.00334](http://arxiv.org/abs/2401.00334)

    本研究通过对抗训练来提高植物叶片疾病分类模型对对抗攻击的鲁棒性，并通过可解释性技术获得模型的决策过程，同时通过模型压缩技术提高计算效率。实验证明，鲁棒性可能以分类准确性为代价，而学生模型可以以较低的性能损失蒸馏复杂模型的知识，从而提高计算效率。

    

    本研究关注植物叶片疾病分类，并探索了对抗训练、模型可解释性和模型压缩三个关键方面。通过对抗训练增强模型对对抗攻击的鲁棒性，在威胁存在的情况下确保准确分类。借助可解释性技术，我们深入了解模型的决策过程，提高了信任和透明度。此外，我们还探索了模型压缩技术，以优化计算效率同时保持分类性能。通过实验证明，在基准数据集上，鲁棒性可能以分类准确性为代价，对常规测试的性能损失为3%-20%，对对抗攻击测试的性能提高为50%-70%。我们还证明，一个学生模型在性能稍有降低的情况下可以比复杂模型高15-25倍的计算效率，蒸馏了更复杂模型的知识。

    This work focuses on plant leaf disease classification and explores three crucial aspects: adversarial training, model explainability, and model compression. The models' robustness against adversarial attacks is enhanced through adversarial training, ensuring accurate classification even in the presence of threats. Leveraging explainability techniques, we gain insights into the model's decision-making process, improving trust and transparency. Additionally, we explore model compression techniques to optimize computational efficiency while maintaining classification performance. Through our experiments, we determine that on a benchmark dataset, the robustness can be the price of the classification accuracy with performance reductions of 3%-20% for regular tests and gains of 50%-70% for adversarial attack tests. We also demonstrate that a student model can be 15-25 times more computationally efficient for a slight performance reduction, distilling the knowledge of more complex models.
    
[^134]: 自我监督预训练用于恶劣条件下的个性化语音活动检测的鲁棒性提升

    Self-supervised Pretraining for Robust Personalized Voice Activity Detection in Adverse Conditions. (arXiv:2312.16613v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2312.16613](http://arxiv.org/abs/2312.16613)

    本文提出了使用自我监督预训练方法来改善恶劣条件下个性化语音活动检测模型性能，并且实验证明该方法不仅可以提高在干净条件下的性能，而且能够得到更鲁棒的模型。

    

    本文提出了在大规模未标记数据集上使用自我监督预训练的方法，以改善恶劣条件下个性化语音活动检测模型的性能。我们使用自回归预测编码框架预训练了一个长短期记忆(LSTM)-编码器，并对个性化语音活动检测进行了微调。此外，我们还提出了一种降噪变体的自回归预测编码方法，旨在提高个性化语音活动检测的鲁棒性。我们对训练好的模型进行了系统评估，包括在干净语音和不同信噪比水平下受各种噪声污染的语音上的评估，并与纯监督模型进行了比较。实验证明，自我监督预训练不仅能在干净条件下提高性能，而且相比于纯监督学习，还能产生更具鲁棒性的模型。

    In this paper, we propose the use of self-supervised pretraining on a large unlabelled data set to improve the performance of a personalized voice activity detection (VAD) model in adverse conditions. We pretrain a long short-term memory (LSTM)-encoder using the autoregressive predictive coding (APC) framework and fine-tune it for personalized VAD. We also propose a denoising variant of APC, with the goal of improving the robustness of personalized VAD. The trained models are systematically evaluated on both clean speech and speech contaminated by various types of noise at different SNR-levels and compared to a purely supervised model. Our experiments show that self-supervised pretraining not only improves performance in clean conditions, but also yields models which are more robust to adverse conditions compared to purely supervised learning.
    
[^135]: 定价的因果预测方法

    Causal Forecasting for Pricing. (arXiv:2312.15282v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2312.15282](http://arxiv.org/abs/2312.15282)

    本文提出了一种在定价环境下进行需求预测的新方法，通过将因果推断的双重机器学习方法和最先进的基于变压器的预测模型结合在一起，我们的方法在完全控制的情况下更好地估计因果效应，并在离线政策设置中优于其他预测方法。

    

    本文提出了一种在定价环境下进行需求预测的新方法。在这种情况下，建模价格作为需求的输入变量之间的因果关系至关重要，因为零售商的目标是以（利润）最佳方式设定价格，以解决下游决策问题。我们的方法将因果推断的双重机器学习方法和最先进的基于变压器的预测模型结合在一起。通过大量的实证实验，我们一方面展示了我们的方法在完全控制的情况下对合成的、但现实的数据更好地估计因果效应。另一方面，我们还展示了在实际数据中，我们的方法在离线政策设置（即定价政策发生变化时）中优于其他预测方法，而在在线政策设置中略有落后。

    This paper proposes a novel method for demand forecasting in a pricing context. Here, modeling the causal relationship between price as an input variable to demand is crucial because retailers aim to set prices in a (profit) optimal manner in a downstream decision making problem. Our methods bring together the Double Machine Learning methodology for causal inference and state-of-the-art transformer-based forecasting models. In extensive empirical experiments, we show on the one hand that our method estimates the causal effect better in a fully controlled setting via synthetic, yet realistic data. On the other hand, we demonstrate on real-world data that our method outperforms forecasting methods in off-policy settings (i.e., when there's a change in the pricing policy) while only slightly trailing in the on-policy setting.
    
[^136]: 训练带有噪声标签的决策树的鲁棒损失函数

    Robust Loss Functions for Training Decision Trees with Noisy Labels. (arXiv:2312.12937v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.12937](http://arxiv.org/abs/2312.12937)

    本文研究了在带有噪声标签的数据上训练决策树的鲁棒损失函数。我们的研究主要有三个贡献：提供了对现有损失函数鲁棒性的新洞察，引入了分布损失函数的框架，并介绍了一种高效的贪婪减少不纯度的学习算法。

    

    我们考虑使用有噪声标签的数据训练决策树，重点研究可以导致鲁棒学习算法的损失函数。我们的贡献有三个。首先，我们对决策树学习背景下许多现有损失函数的鲁棒性提供了新颖的理论洞察力。我们展示了一些损失属于我们所称的保守损失类别，并且保守损失在训练过程中会出现提前停止行为，而在测试过程中具有容忍噪声的预测能力。其次，我们引入了一个构建鲁棒损失函数的框架，称为分布损失。这些损失基于假设的边缘分布应用基于百分位的惩罚，它们通过鲁棒性参数自然地允许适应不同的噪声率。特别地，我们引入了一种称为负指数损失的新损失，它可以导致高效的贪婪减少不纯度的学习算法。最后，我们在多个数据集和噪声条件下进行了实验证明了我们的方法的有效性。

    We consider training decision trees using noisily labeled data, focusing on loss functions that can lead to robust learning algorithms. Our contributions are threefold. First, we offer novel theoretical insights on the robustness of many existing loss functions in the context of decision tree learning. We show that some of the losses belong to a class of what we call conservative losses, and the conservative losses lead to an early stopping behavior during training and noise-tolerant predictions during testing. Second, we introduce a framework for constructing robust loss functions, called distribution losses. These losses apply percentile-based penalties based on an assumed margin distribution, and they naturally allow adapting to different noise rates via a robustness parameter. In particular, we introduce a new loss called the negative exponential loss, which leads to an efficient greedy impurity-reduction learning algorithm. Lastly, our experiments on multiple datasets and noise se
    
[^137]: 追踪任何物体的非现态方法

    Tracking Any Object Amodally. (arXiv:2312.12433v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.12433](http://arxiv.org/abs/2312.12433)

    本论文介绍了一种追踪任何物体的非现态方法，利用数据增强和微调现态跟踪器，可以提高追踪的效果。

    

    非现态感知是一种从部分可见性中理解完整物体结构的基本技能，它对于婴儿甚至是成人都非常重要。它的重要性延伸到了自动驾驶等应用领域，对于理解重叠物体至关重要。然而，现代的检测和跟踪算法通常忽视了这一关键能力，可能是因为大多数数据集中普遍使用的是现态标注。为了解决非现态数据的匮乏问题，我们引入了TAO-Amodal基准，其中包含数千个视频序列中的880个多样化的物体类别。我们的数据集包括可见和遮挡对象的非现态和现态边界框，包括部分超出画面范围的物体。为了增强非现态追踪的目标永久性，我们利用了一个轻量级的插件模块，即非现态扩展器，通过对几百个视频序列进行数据增强的微调，将标准的现态跟踪器转化为非现态跟踪器。我们取得了3.3％和1.6％的改进效果。

    Amodal perception, the ability to comprehend complete object structures from partial visibility, is a fundamental skill, even for infants. Its significance extends to applications like autonomous driving, where a clear understanding of heavily occluded objects is essential. However, modern detection and tracking algorithms often overlook this critical capability, perhaps due to the prevalence of modal annotations in most datasets. To address the scarcity of amodal data, we introduce the TAO-Amodal benchmark, featuring 880 diverse categories in thousands of video sequences. Our dataset includes amodal and modal bounding boxes for visible and occluded objects, including objects that are partially out-of-frame. To enhance amodal tracking with object permanence, we leverage a lightweight plug-in module, the amodal expander, to transform standard, modal trackers into amodal ones through fine-tuning on a few hundred video sequences with data augmentation. We achieve a 3.3\% and 1.6\% improve
    
[^138]: 偏好和并发感知的贝叶斯图神经网络用于推荐系统

    Preference and Concurrence Aware Bayesian Graph Neural Networks for Recommender Systems. (arXiv:2312.11486v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2312.11486](http://arxiv.org/abs/2312.11486)

    本文提出了一个偏好和并发感知的贝叶斯图神经网络框架用于解决推荐系统中的问题，通过生成模型和关注图结构信息来捕捉用户与物品的高阶信息，实验证明了其有效性。

    

    基于图的协同过滤方法对于推荐系统具有突出的性能，因为它们可以捕捉用户和物品之间的高阶信息，其中图是由观察到的用户-物品交互构建的，这些交互可能会丢失链接或包含虚假的正交互。贝叶斯图神经网络框架通过生成模型解决了这个问题。关键问题是设计一族适合推荐系统的图生成模型。我们提出了一种高效的生成模型，它同时考虑了用户的偏好、物品的并发以及一些重要的图结构信息。在四个流行的基准数据集上的实验证明了我们提出的图生成方法在推荐系统中的有效性。

    Graph-based collaborative filtering methods have prevailing performance for recommender systems since they can capture high-order information between users and items, in which the graphs are constructed from the observed user-item interactions that might miss links or contain spurious positive interactions in industrial scenarios. The Bayesian Graph Neural Network framework approaches this issue with generative models for the interaction graphs. The critical problem is to devise a proper family of graph generative models tailored to recommender systems. We propose an efficient generative model that jointly considers the preferences of users, the concurrence of items and some important graph structure information. Experiments on four popular benchmark datasets demonstrate the effectiveness of our proposed graph generative methods for recommender systems.
    
[^139]: 构建可信的AI软件开发助手的研究

    Towards Trustworthy AI Software Development Assistance. (arXiv:2312.09126v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2312.09126](http://arxiv.org/abs/2312.09126)

    本研究致力于构建可信的AI软件开发助手。通过综合架构、训练基础LLM和使用图形代码表示，以及整合知识图等技术手段，提供高质量的代码生成和相关解释。

    

    预计在不久的将来，AI软件开发助手将在软件行业中发挥重要作用。然而，目前的软件开发助手往往不可靠，经常生成错误、不安全或低质量的代码。我们通过引入一个综合架构来解决这些问题，用于构建、训练和使用可信的AI软件开发助手。架构的核心是一个在代表真实世界编码场景和复杂软件架构的数据集上训练的基础LLM，并在代码质量标准上进行了精调。LLM将利用基于图的代码表示进行高级语义理解。我们设想将知识图集成到系统中，提供最新的背景知识，并使助手能够提供适当的解释。最后，一个用于受限解码的模块化框架将确保某些保证（例如，对正确性的保证）。

    It is expected that in the near future, AI software development assistants will play an important role in the software industry. However, current software development assistants tend to be unreliable, often producing incorrect, unsafe, or low-quality code. We seek to resolve these issues by introducing a holistic architecture for constructing, training, and using trustworthy AI software development assistants. In the center of the architecture, there is a foundational LLM trained on datasets representative of real-world coding scenarios and complex software architectures, and fine-tuned on code quality criteria beyond correctness. The LLM will make use of graph-based code representations for advanced semantic comprehension. We envision a knowledge graph integrated into the system to provide up-to-date background knowledge and to enable the assistant to provide appropriate explanations. Finally, a modular framework for constrained decoding will ensure that certain guarantees (e.g., for 
    
[^140]: 对于核机器在预处理中的Nystrom逼近

    On the Nystrom Approximation for Preconditioning in Kernel Machines. (arXiv:2312.03311v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2312.03311](http://arxiv.org/abs/2312.03311)

    本文分析了核机器预处理中使用Nystrom逼近的权衡。研究表明，使用对数大小的样本能够让Nystrom逼近的预处理器几乎与梯度下降同样有效地加速。

    

    核方法是机器学习中一类流行的非线性预测模型。学习核模型的可扩展算法需要具有迭代性质，但由于糟糕的条件，收敛可能很慢。谱预处理是加快训练核模型迭代算法收敛速度的重要工具。然而，计算和存储谱预处理器可能代价高昂，会导致大量的计算和存储开销，限制了核方法在大型数据集问题上的应用。Nystrom逼近的谱预处理器通常更便宜和更容易计算和存储，并在实际应用中取得了成功。本文分析了使用这种逼近预处理器的权衡。具体来说，我们表明与数据集大小相关的对数样本数量能够让基于Nystrom逼近的预处理器几乎与梯度下降同样有效地加速。

    Kernel methods are a popular class of nonlinear predictive models in machine learning. Scalable algorithms for learning kernel models need to be iterative in nature, but convergence can be slow due to poor conditioning. Spectral preconditioning is an important tool to speed-up the convergence of such iterative algorithms for training kernel models. However computing and storing a spectral preconditioner can be expensive which can lead to large computational and storage overheads, precluding the application of kernel methods to problems with large datasets. A Nystrom approximation of the spectral preconditioner is often cheaper to compute and store, and has demonstrated success in practical applications. In this paper we analyze the trade-offs of using such an approximated preconditioner. Specifically, we show that a sample of logarithmic size (as a function of the size of the dataset) enables the Nystrom-based approximated preconditioner to accelerate gradient descent nearly as well as
    
[^141]: 条件变分扩散模型

    Conditional Variational Diffusion Models. (arXiv:2312.02246v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.02246](http://arxiv.org/abs/2312.02246)

    该论文提出了一种新的条件变分扩散模型，通过学习调度作为训练过程的一部分，解决了扩散模型的敏感性问题，并且能够适应不同的应用场景，提供高质量的解决方案。

    

    逆问题旨在从观测中确定参数，这是工程和科学中的一个关键任务。最近，生成模型，特别是扩散模型，因其能够产生逼真的解决方案和良好的数学特性而在这一领域中越来越受欢迎。尽管取得了成功，但扩散模型的一个重要缺点是对方差调度的选择敏感，该调度控制着扩散过程的动态。为特定应用程序微调这个调度是至关重要的，但时间成本高昂，并且不能保证最优结果。我们提出了一种新颖的方法，将学习调度作为训练过程的一部分。我们的方法支持对数据的概率条件，提供高质量的解决方案，并且具有灵活性，能够在最小的开销下适应不同的应用。这种方法在两个不相关的逆问题中进行了测试：超分辨率显微镜和定量相位成像，结果表明比较或更好。

    Inverse problems aim to determine parameters from observations, a crucial task in engineering and science. Lately, generative models, especially diffusion models, have gained popularity in this area for their ability to produce realistic solutions and their good mathematical properties. Despite their success, an important drawback of diffusion models is their sensitivity to the choice of variance schedule, which controls the dynamics of the diffusion process. Fine-tuning this schedule for specific applications is crucial but time-costly and does not guarantee an optimal result. We propose a novel approach for learning the schedule as part of the training process. Our method supports probabilistic conditioning on data, provides high-quality solutions, and is flexible, proving able to adapt to different applications with minimum overhead. This approach is tested in two unrelated inverse problems: super-resolution microscopy and quantitative phase imaging, yielding comparable or superior 
    
[^142]: 时间中的涟漪：美国历史中的不连续性

    A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.01185](http://arxiv.org/abs/2312.01185)

    该论文通过使用向量嵌入和非线性降维方法，发现GPT-2与UMAP的结合可以提供更好的分离和聚类效果。同时，经过微调的DistilBERT模型可用于识别总统和演讲的年份。

    

    在这篇论文中，我们使用来自Kaggle的国情咨文数据集对美国历史的总体时间线及咨文本身的特点和性质进行了一些令人惊讶（也有些不那么令人惊讶）的观察。我们的主要方法是使用向量嵌入，如BERT（DistilBERT）和GPT-2。虽然广泛认为BERT（及其变体）最适合NLP分类任务，但我们发现GPT-2结合UMAP等非线性降维方法可以提供更好的分离和更强的聚类效果。这使得GPT-2 + UMAP成为一个有趣的替代方案。在我们的情况下，不需要对模型进行微调，预训练的GPT-2模型就足够好用。我们还使用了经过微调的DistilBERT模型来检测哪位总统发表了哪篇演讲，并取得了非常好的结果（准确率为93\% - 95\%，具体取决于运行情况）。为了确定写作年份，我们还执行了一个类似的任务。

    In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
    
[^143]: 个性化预测胶质母细胞瘤浸润:数学模型、物理信息神经网络和多模态扫描

    Personalized Predictions of Glioblastoma Infiltration: Mathematical Models, Physics-Informed Neural Networks and Multimodal Scans. (arXiv:2311.16536v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.16536](http://arxiv.org/abs/2311.16536)

    本研究利用物理信息神经网络（PINNs）从单个三维结构MRI快照中估计胶质母细胞瘤（GBM）生长模型的患者特异性参数，并通过整合理论和数据进行个性化预测，为胶质母细胞瘤的治疗设计提供了关键创新。

    

    预测胶质母细胞瘤（GBM）从医学MRI扫描中的浸润对于理解肿瘤生长动力学和设计个体化放射治疗计划至关重要。GBM生长的数学模型可以在预测肿瘤细胞的空间分布中补充数据。然而，这需要从临床数据中估计模型的患者特异性参数，由于时间数据有限且成像和诊断之间的时间有限，这是一个具有挑战性的反问题。本研究提出了一种使用物理信息神经网络（PINNs）从单个三维结构MRI快照中估计GBM生长反应扩散偏微分方程模型的患者特异性参数的方法。PINNs将数据和PDE嵌入到损失函数中，从而整合了理论和数据。关键创新包括特征无量纲参数的识别和估计，利用无量纲参数的预训练步骤以及微调步骤。

    Predicting the infiltration of Glioblastoma (GBM) from medical MRI scans is crucial for understanding tumor growth dynamics and designing personalized radiotherapy treatment plans.Mathematical models of GBM growth can complement the data in the prediction of spatial distributions of tumor cells. However, this requires estimating patient-specific parameters of the model from clinical data, which is a challenging inverse problem due to limited temporal data and the limited time between imaging and diagnosis. This work proposes a method that uses Physics-Informed Neural Networks (PINNs) to estimate patient-specific parameters of a reaction-diffusion PDE model of GBM growth from a single 3D structural MRI snapshot. PINNs embed both the data and the PDE into a loss function, thus integrating theory and data. Key innovations include the identification and estimation of characteristic non-dimensional parameters, a pre-training step that utilizes the non-dimensional parameters and a fine-tunin
    
[^144]: 基于想象力增强的分层强化学习用于城市环境中安全交互自动驾驶

    Imagination-Augmented Hierarchical Reinforcement Learning for Safe and Interactive Autonomous Driving in Urban Environments. (arXiv:2311.10309v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.10309](http://arxiv.org/abs/2311.10309)

    本文提出了一种名为想象力增强分层强化学习 (IAHRL)的方法，通过有效地整合想象力到分层强化学习中，使智能体能够在现实世界的导航任务中学习安全和交互的行为。

    

    分层强化学习通过显式地利用分层结构将时间抽象纳入强化学习中。现代的分层强化学习通常设计一个由高层策略和低层策略组成的分层代理。高层策略选择较低频率激活的低层策略，而激活的低层策略在每个时间步选择一个动作。最近的分层强化学习算法在合成导航任务中相对于标准的强化学习算法取得了性能提升。但是，我们无法将这些分层强化学习算法应用于现实世界的导航任务。主要挑战之一是现实世界的导航任务需要智能体在动态环境中执行安全和交互行为。在本文中，我们提出了一种想象力增强的分层强化学习算法(IAHRL)，它有效地将想象力整合到分层强化学习中，使智能体能够在现实世界的导航任务中学习安全和交互的行为。

    Hierarchical reinforcement learning (HRL) incorporates temporal abstraction into reinforcement learning (RL) by explicitly taking advantage of hierarchical structure. Modern HRL typically designs a hierarchical agent composed of a high-level policy and low-level policies. The high-level policy selects which low-level policy to activate at a lower frequency and the activated low-level policy selects an action at each time step. Recent HRL algorithms have achieved performance gains over standard RL algorithms in synthetic navigation tasks. However, we cannot apply these HRL algorithms to real-world navigation tasks. One of the main challenges is that real-world navigation tasks require an agent to perform safe and interactive behaviors in dynamic environments. In this paper, we propose imagination-augmented HRL (IAHRL) that efficiently integrates imagination into HRL to enable an agent to learn safe and interactive behaviors in real-world navigation tasks. Imagination is to predict the c
    
[^145]: 从电生理数据中映射和预测神经元相互作用的水库计算模型

    Reservoir-Computing Model for Mapping and Forecasting Neuronal Interactions from Electrophysiological Data. (arXiv:2311.03131v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2311.03131](http://arxiv.org/abs/2311.03131)

    该论文介绍了一种基于水库计算网络的计算模型，可以从电生理测量数据中解码神经元网络的时空信息，并在宏观领域内重建网络结构。实验证明该模型比其他常用方法更准确地预测了网络的连接图，并且能够预测网络对特定输入的响应能力。

    

    神经元网络的电生理特性能够在非常短的时间尺度内揭示不同细胞单元之间的各种相互作用。在分析这些信号的过程中，一个挑战就是找出给定网络的形态和功能。在这项工作中，我们开发了一个基于水库计算网络（RCN）架构的计算模型，它可以解码神经元培养物的电生理测量数据的时空信息，并在宏观领域内重建表示神经元单元之间连接性的网络结构。我们证明该模型可以比交叉相关和传递熵等常用方法更准确地预测网络的连接图。此外，我们还通过实验展示了该模型预测网络对特定输入（如局部刺激）的响应能力。

    Electrophysiological nature of neuronal networks allows to reveal various interactions between different cell units at a very short time-scales. One of the many challenges in analyzing these signals is to retrieve the morphology and functionality of a given network. In this work we developed a computational model, based on Reservoir Computing Network (RCN) architecture, which decodes the spatio-temporal data from electro-physiological measurements of neuronal cultures and reconstructs the network structure on a macroscopic domain, representing the connectivity between neuronal units. We demonstrate that the model can predict the connectivity map of the network with higher accuracy than the common methods such as Cross-Correlation and Transfer-Entropy. In addition, we experimentally demonstrate the ability of the model to predict a network response to a specific input, such as localized stimulus.
    
[^146]: 从生成AI到生成物联网：基础知识、框架和展望

    From Generative AI to Generative Internet of Things: Fundamentals, Framework, and Outlooks. (arXiv:2310.18382v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.18382](http://arxiv.org/abs/2310.18382)

    本文介绍了生成物联网（GIoT）的概念和潜在前景。通过将生成人工智能（GAI）集成到现代物联网中，可以实现更高效和智能的物联网应用。文章提出了一个基于GAI的安全激励机制框架，采用生成扩散模型（GDM）和区块链技术来解决GIoT面临的挑战。另外，对现代车辆交通监控进行了案例研究。

    

    生成人工智能（GAI）具有生成逼真数据和促进高级决策的能力。通过将GAI集成到现代物联网（IoT）中，生成物联网（GIoT）正在兴起，具有革命化社会各个方面的巨大潜力，可实现更高效和智能的物联网应用，如智能监控和语音助手。在本文中，我们介绍了GIoT的概念，并探索了其潜在前景。具体而言，我们首先概述了四种GAI技术，并研究了有前景的GIoT应用。然后，我们详细阐述了实现GIoT面临的主要挑战，并提出了一个基于GAI的安全激励机制框架来解决这些挑战，在该框架中，我们采用了生成扩散模型（GDM）进行激励机制设计，并应用区块链技术进行安全的GIoT管理。此外，我们对现代车辆交通监控进行了案例研究。

    Generative Artificial Intelligence (GAI) possesses the capabilities of generating realistic data and facilitating advanced decision-making. By integrating GAI into modern Internet of Things (IoT), Generative Internet of Things (GIoT) is emerging and holds immense potential to revolutionize various aspects of society, enabling more efficient and intelligent IoT applications, such as smart surveillance and voice assistants. In this article, we present the concept of GIoT and conduct an exploration of its potential prospects. Specifically, we first overview four GAI techniques and investigate promising GIoT applications. Then, we elaborate on the main challenges in enabling GIoT and propose a general GAI-based secure incentive mechanism framework to address them, in which we adopt Generative Diffusion Models (GDMs) for incentive mechanism designs and apply blockchain technologies for secure GIoT management. Moreover, we conduct a case study on modern Internet of Vehicle traffic monitoring
    
[^147]: 学习非稳态条件下的稳定性原则

    A Stability Principle for Learning under Non-Stationarity. (arXiv:2310.18304v1 [cs.LG])

    [http://arxiv.org/abs/2310.18304](http://arxiv.org/abs/2310.18304)

    本研究提出了一个适用于非稳态环境的统计学习框架，通过应用稳定性原则选择回溯窗口来最大化历史数据利用，并保持累积偏差在可接受范围内。该方法展示了对未知非稳态的适应性，遗憾界在强凸或满足Lipschitz条件下是极小化的最优解。该研究的创新点是函数相似度度量和非稳态数据序列划分技术。

    

    我们在非稳定环境中开发了一个灵活的统计学习框架。在每个时间段，我们的方法应用稳定性原则来选择一个回溯窗口，最大限度地利用历史数据，同时将累积偏差保持在与随机误差相对可接受的范围内。我们的理论展示了该方法对未知非稳定性的适应性。当人口损失函数强凸或仅满足Lipschitz条件时，遗憾界是极小化的最优解，仅受对数因子的影响。我们的分析核心是两个新颖的组成部分：函数之间的相似度度量和将非稳态数据序列划分为准稳态片段的分割技术。

    We develop a versatile framework for statistical learning in non-stationary environments. In each time period, our approach applies a stability principle to select a look-back window that maximizes the utilization of historical data while keeping the cumulative bias within an acceptable range relative to the stochastic error. Our theory showcases the adaptability of this approach to unknown non-stationarity. The regret bound is minimax optimal up to logarithmic factors when the population losses are strongly convex, or Lipschitz only. At the heart of our analysis lie two novel components: a measure of similarity between functions and a segmentation technique for dividing the non-stationary data sequence into quasi-stationary pieces.
    
[^148]: HetGPT: 利用预训练异构图神经网络中的提示调整的能力

    HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks. (arXiv:2310.15318v1 [cs.LG])

    [http://arxiv.org/abs/2310.15318](http://arxiv.org/abs/2310.15318)

    HetGPT是一种预训练异构图神经网络的方法，通过利用提示调整来解决预训练与下游任务之间的不匹配问题。

    

    图表现为表示和分析Web中的复杂模式和丰富信息的自然选择，使得在线页面分类和社交推荐等应用成为可能。然而，当前的“预训练，微调”范式在图机器学习任务中广泛应用，特别是在有限标记节点的情况下，往往存在预训练目标任务与下游任务之间的不匹配问题。这种差距可能导致“负转移”问题，即预训练所获得的知识对下游任务的性能产生不利影响。自然语言处理领域中基于提示的学习的兴起表明了将“预训练，提示”范式应用于图形的潜力，作为一种替代方案。然而，现有的图形提示技术针对的是同质图，忽视了Web图的内在异构性。为了填补这一差距，我们提出了HetGPT，

    Graphs have emerged as a natural choice to represent and analyze the intricate patterns and rich information of the Web, enabling applications such as online page classification and social recommendation. The prevailing "pre-train, fine-tune" paradigm has been widely adopted in graph machine learning tasks, particularly in scenarios with limited labeled nodes. However, this approach often exhibits a misalignment between the training objectives of pretext tasks and those of downstream tasks. This gap can result in the "negative transfer" problem, wherein the knowledge gained from pre-training adversely affects performance in the downstream tasks. The surge in prompt-based learning within Natural Language Processing (NLP) suggests the potential of adapting a "pre-train, prompt" paradigm to graphs as an alternative. However, existing graph prompting techniques are tailored to homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To bridge this gap, we propose HetGPT, a 
    
[^149]: 通过同时学习面部标志检测、域分离和重建来提高面部动作单位检测的精度

    Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction. (arXiv:2310.05207v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.05207](http://arxiv.org/abs/2310.05207)

    本文提出了一种新的面部动作单位（AU）检测框架，通过共享参数和引入多任务学习，在面部标志检测和AU域分离与重建之间实现了更好的性能。实验证明我们方法在野外AU检测方面优于现有方法。

    

    最近，如何将大量的在野非标记面部图像引入监督式面部动作单位（AU）检测框架中成为一个具有挑战性的问题。本文提出了一种新的AU检测框架，通过共享同构面部提取模块的参数，引入多任务学习，同时学习AU域分离和重建以及面部标志检测。另外，我们提出了一种基于对比学习的新特征对齐方案，通过简单的投影器和改进的对比损失添加了四个额外的中间监督器来促进特征重建的过程。在两个基准测试上的实验结果表明，我们在野外AU检测方面优于现有的方法。

    Recently how to introduce large amounts of unlabeled facial images in the wild into supervised Facial Action Unit (AU) detection frameworks has become a challenging problem. In this paper, we propose a new AU detection framework where multi-task learning is introduced to jointly learn AU domain separation and reconstruction and facial landmark detection by sharing the parameters of homostructural facial extraction modules. In addition, we propose a new feature alignment scheme based on contrastive learning by simple projectors and an improved contrastive loss, which adds four additional intermediate supervisors to promote the feature reconstruction process. Experimental results on two benchmarks demonstrate our superiority against the state-of-the-art methods for AU detection in the wild.
    
[^150]: "检索遇上长篇大语言模型"

    Retrieval meets Long Context Large Language Models. (arXiv:2310.03025v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.03025](http://arxiv.org/abs/2310.03025)

    "本论文研究了将检索增强和长上下文窗口的大语言模型相结合的解决方案，发现在长上下文任务中，通过检索增强的LLM使用4K上下文窗口可以取得与通过长上下文窗口微调的LLM使用16K上下文窗口相当的性能，同时计算量要少得多。此外，无论上下文窗口大小如何，检索都可以显著提高LLM的性能。"

    

    "最近，扩展大语言模型（LLM）的上下文窗口越来越流行，而将检索与LLM相结合的解决方案已存在多年。自然而然的问题是：检索增强与长上下文窗口，哪个对下游任务更好？这两种方法可以结合起来以兼顾利弊吗？在这项工作中，我们使用两个最先进的预训练LLM（即一个私有的43B GPT和Llama2-70B）来研究这两种解决方案。也许令人惊讶的是，我们发现在长上下文任务中，LLM使用4K上下文窗口并通过简单的检索增强在生成时可以达到与通过长上下文窗口进行位置插值的微调LLM使用16K上下文窗口相当的性能，同时计算量要少得多。更重要的是，我们证明了不论其扩展的上下文窗口大小如何，检索都可以显著提高LLM的性能。我们最好的模型是具有32K上下文窗口的检索增强Llama2-70B。"

    Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context wi
    
[^151]: 一个基于神经特征学习的几何框架

    A Geometric Framework for Neural Feature Learning. (arXiv:2309.10140v1 [cs.LG])

    [http://arxiv.org/abs/2309.10140](http://arxiv.org/abs/2309.10140)

    本论文提出了一个基于神经特征学习的几何框架，在特征空间中利用几何结构解决学习问题。通过引入特征几何，将统计依赖和特征统一到同一空间中，并使用嵌套技术设计学习算法，展示了其在多变量学习问题中的应用。

    

    我们提出了一个基于神经特征提取器的学习系统设计的新框架，通过利用特征空间中的几何结构。首先，我们引入了特征几何，它将统计依赖和特征统一到同一个具有几何结构的函数空间中。通过应用特征几何，我们将每个学习问题形式化为解决由学习设置指定的依赖组件的最佳特征近似解。我们提出了一种嵌套技术来设计学习算法，从数据样本中学习最佳特征，这可以应用于现有的网络架构和优化器。为了展示嵌套技术的应用，我们进一步讨论了多变量学习问题，包括条件推理和多模态学习，在这些问题中，我们提出了最佳特征并揭示了它们与经典方法的联系。

    We present a novel framework for learning system design based on neural feature extractors by exploiting geometric structures in feature spaces. First, we introduce the feature geometry, which unifies statistical dependence and features in the same functional space with geometric structures. By applying the feature geometry, we formulate each learning problem as solving the optimal feature approximation of the dependence component specified by the learning setting. We propose a nesting technique for designing learning algorithms to learn the optimal features from data samples, which can be applied to off-the-shelf network architectures and optimizers. To demonstrate the application of the nesting technique, we further discuss multivariate learning problems, including conditioned inference and multimodal learning, where we present the optimal features and reveal their connections to classical approaches.
    
[^152]: GPT-3用于抗癌药物敏感性预测的评估

    Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction. (arXiv:2309.10016v1 [cs.LG])

    [http://arxiv.org/abs/2309.10016](http://arxiv.org/abs/2309.10016)

    本研究评估了GPT-3在抗癌药物敏感性预测任务中的潜力，并发现药物的SMILES表示和细胞系的基因组突变特征对药物反应具有预测能力。这些结果有助于在精准肿瘤学中设计更有效的治疗方案。

    

    本研究使用结构化的药物基因组数据，在五种组织类型中探究了GPT-3在抗癌药物敏感性预测任务中的潜力，并分别采用零样本提示和微调范式对其性能进行了评估。药物的SMILES表示和细胞系的基因组突变特征对药物反应具有预测能力。本研究的结果有望为精准肿瘤学中设计更有效的治疗方案铺平道路。

    In this study, we investigated the potential of GPT-3 for the anti-cancer drug sensitivity prediction task using structured pharmacogenomics data across five tissue types and evaluated its performance with zero-shot prompting and fine-tuning paradigms. The drug's smile representation and cell line's genomic mutation features were predictive of the drug response. The results from this study have the potential to pave the way for designing more efficient treatment protocols in precision oncology.
    
[^153]: 基于分数的生成模型用于PET图像重建

    Score-Based Generative Models for PET Image Reconstruction. (arXiv:2308.14190v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2308.14190](http://arxiv.org/abs/2308.14190)

    本研究提出了适用于PET图像重建的基于分数的生成模型，通过应对PET图像重建中的挑战，包括高方差的泊松噪声和广泛的动态范围，展示了改进PET重建的显著潜力。

    

    基于分数的生成模型已经在磁共振成像或计算机断层扫描等医学图像重建任务中展现出高度有前景的效果。然而，他们在正电子发射断层扫描（PET）中的应用仍然相对未知。PET图像重建涉及多种挑战，包括高方差的泊松噪声和广泛的动态范围。为了解决这些挑战，我们提出了几种适用于PET的基于分数的生成模型的特定适应方法。我们的方法适用于2D和3D PET，并且提供了使用磁共振图像进行引导重建的扩展。我们通过在没有病变的患者真实数据上进行了广泛的2D和3D的$\textit{in-silico}$实验中验证了这种方法，并通过没有病变的数据以及带有病变的数据进行了评估。这证明了该方法的鲁棒性和显著的PET重建改进潜力。

    Score-based generative models have demonstrated highly promising results for medical image reconstruction tasks in magnetic resonance imaging or computed tomography. However, their application to Positron Emission Tomography (PET) is still largely unexplored. PET image reconstruction involves a variety of challenges, including Poisson noise with high variance and a wide dynamic range. To address these challenges, we propose several PET-specific adaptations of score-based generative models. The proposed framework is developed for both 2D and 3D PET. In addition, we provide an extension to guided reconstruction using magnetic resonance images. We validate the approach through extensive 2D and 3D $\textit{in-silico}$ experiments with a model trained on patient-realistic data without lesions, and evaluate on data without lesions as well as out-of-distribution data with lesions. This demonstrates the proposed method's robustness and significant potential for improved PET reconstruction.
    
[^154]: 用通用设备编码和图注意力网络革新TCAD模拟

    Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks. (arXiv:2308.11624v1 [cs.LG])

    [http://arxiv.org/abs/2308.11624](http://arxiv.org/abs/2308.11624)

    本论文提出了一种利用人工智能和图表示技术对TCAD器件模拟中的半导体器件进行编码的创新方法，通过引入图注意力网络和通用编码方案，实现了全面的数据驱动建模，为研究人员提供了在设备级上应用基于人工智能的电子设计自动化解决方案的可能性。

    

    本论文提出了一种创新方法，利用人工智能和图表示技术来对TCAD器件模拟中的半导体器件进行编码。提出了一种基于图的通用编码方案，不仅考虑了材料级和器件级嵌入，还引入了一种新颖的基于空间关系的嵌入，受有限元网格中常用的插值操作启发而来。利用器件模拟的通用物理定律进行全面的数据驱动建模，包括基于泊松仿真的替代和基于漂移扩散模型的电流-电压（IV）预测。这两者都是使用一种新颖的图注意力网络（称为RelGAT）实现的。论文还提供了基于Sentaurus TCAD器件模拟器的详细技术细节，使研究人员可以在设备级上采用提出的基于人工智能的电子设计自动化解决方案。

    An innovative methodology that leverages artificial intelligence (AI) and graph representation for semiconductor device encoding in TCAD device simulation is proposed. A graph-based universal encoding scheme is presented that not only considers material-level and device-level embeddings, but also introduces a novel spatial relationship embedding inspired by interpolation operations typically used in finite element meshing. Universal physical laws from device simulations are leveraged for comprehensive data-driven modeling, which encompasses surrogate Poisson emulation and current-voltage (IV) prediction based on drift-diffusion model. Both are achieved using a novel graph attention network, referred to as RelGAT. Comprehensive technical details based on the device simulator Sentaurus TCAD are presented, empowering researchers to adopt the proposed AI-driven Electronic Design Automation (EDA) solution at the device level.
    
[^155]: 重新审视基于列生成的启发式方法用于学习分类树

    Revisiting column-generation-based matheuristic for learning classification trees. (arXiv:2308.11477v1 [cs.LG])

    [http://arxiv.org/abs/2308.11477](http://arxiv.org/abs/2308.11477)

    该论文改进了基于列生成的启发式方法，以提高学习分类树的效果。通过减少子问题数量、使用数据依赖约束作为割平面以及生成违反约束的数据点，该方法提高了可伸缩性并适用于大型数据集。

    

    决策树是机器学习中解决分类问题的高度可解释性模型。传统的机器学习算法训练决策树快速但生成的树在准确性上不够优化。文献中其他离散优化模型解决了最优性问题但只在较小的数据集上表现良好。firat2020column提出了一种基于列生成的启发式方法来学习决策树。该方法提高了可伸缩性，并可以处理大型数据集。在这篇论文中，我们描述了对该列生成方法的改进。首先，我们修改了子问题模型以显著减少多类分类实例中的子问题数量。接下来，我们证明了主问题中的数据依赖约束是蕴含的，并将其用作割平面。此外，我们描述了一个分离模型来生成线性规划松弛解违反其对应的数据点。

    Decision trees are highly interpretable models for solving classification problems in machine learning (ML). The standard ML algorithms for training decision trees are fast but generate suboptimal trees in terms of accuracy. Other discrete optimization models in the literature address the optimality problem but only work well on relatively small datasets. \cite{firat2020column} proposed a column-generation-based heuristic approach for learning decision trees. This approach improves scalability and can work with large datasets. In this paper, we describe improvements to this column generation approach. First, we modify the subproblem model to significantly reduce the number of subproblems in multiclass classification instances. Next, we show that the data-dependent constraints in the master problem are implied, and use them as cutting planes. Furthermore, we describe a separation model to generate data points for which the linear programming relaxation solution violates their correspond
    
[^156]: 用可证明的保证解密神经符号学习中的原始数据

    Deciphering Raw Data in Neuro-Symbolic Learning with Provable Guarantees. (arXiv:2308.10487v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.10487](http://arxiv.org/abs/2308.10487)

    本文介绍了一种使用神经符号混合系统进行机器学习和符号推理的方法，并通过检查知识库来确定它们在促进成功学习方面的效力。研究发现许多知识库满足判据，但也存在一些无法满足的知识库。

    

    神经符号混合系统在整合机器学习和符号推理方面很有前景，其中感知模型通过逻辑推理从符号知识库中推断出信息。尽管有经验证据表明混合系统能够学习准确的感知模型，但对于可学习性的理论理解仍然不足。因此，对于为什么混合系统能够成功完成特定任务以及在不同的知识库下可能失败仍不清楚。本文介绍了一种新的方式，用于描述来自知识库的监督信号，并建立了一个判据来确定知识在促进成功学习方面的效力。这是首次允许我们通过检查正在研究的知识库来回答上述两个问题。我们的分析表明，许多知识库满足判据，从而实现有效的学习，而有些则无法满足，表明存在问题。

    Neuro-symbolic hybrid systems are promising for integrating machine learning and symbolic reasoning, where perception models are facilitated with information inferred from a symbolic knowledge base through logical reasoning. Despite empirical evidence showing the ability of hybrid systems to learn accurate perception models, the theoretical understanding of learnability is still lacking. Hence, it remains unclear why a hybrid system succeeds for a specific task and when it may fail given a different knowledge base. In this paper, we introduce a novel way of characterising supervision signals from a knowledge base, and establish a criterion for determining the knowledge's efficacy in facilitating successful learning. This, for the first time, allows us to address the two questions above by inspecting the knowledge base under investigation. Our analysis suggests that many knowledge bases satisfy the criterion, thus enabling effective learning, while some fail to satisfy it, indicating po
    
[^157]: 基于区域关注的多视角表示学习用于城市区域嵌入

    Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings. (arXiv:2307.03212v1 [cs.CV])

    [http://arxiv.org/abs/2307.03212](http://arxiv.org/abs/2307.03212)

    提出了一种区域关注的多视角表示学习（ROMER）的算法，用于捕捉多视角之间的依赖关系，学习城市区域的表达能力，并在多源城市数据中优于现有方法。

    

    城市区域嵌入是一个重要且具有高度挑战性的问题，由于城市数据的复杂性和不断变化的性质。为了解决这些挑战，我们提出了一种区域关注的多视角表示学习（ROMER），以捕捉多视角之间的依赖关系，并学习城市区域的表达能力，而不受刚性邻域条件的限制。我们的模型专注于从多源城市数据中学习城市区域表示。首先，我们从移动流模式、POI语义和签到动态中捕捉多视角的相关性。然后，我们采用全局图注意网络来学习图中任意两个顶点的相似性。为了全面考虑和共享多个视角的特征，我们进一步提出了一个两阶段的融合模块，利用外部注意力学习权重来融合多视角嵌入。在真实世界数据集上进行的两个下游任务的大量实验证明，我们的模型优于现有的方法。

    Urban region embedding is an important and yet highly challenging issue due to the complexity and constantly changing nature of urban data. To address the challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER) to capture multi-view dependencies and learn expressive representations of urban regions without the constraints of rigid neighbourhood region conditions. Our model focus on learn urban region representation from multi-source urban data. First, we capture the multi-view correlations from mobility flow patterns, POI semantics and check-in dynamics. Then, we adopt global graph attention networks to learn similarity of any two vertices in graphs. To comprehensively consider and share features of multiple views, a two-stage fusion module is further proposed to learn weights with external attention to fuse multi-view embeddings. Extensive experiments for two downstream tasks on real-world datasets demonstrate that our model outperforms state-of-the-art methods
    
[^158]: 何时使用基于置信度的级联延迟足够？

    When Does Confidence-Based Cascade Deferral Suffice?. (arXiv:2307.02764v1 [cs.LG])

    [http://arxiv.org/abs/2307.02764](http://arxiv.org/abs/2307.02764)

    本研究旨在探讨何时基于置信度的级联延迟可能失败，以及何时备选的延迟策略可能表现更好。通过理论分析和实验证明事后延迟机制能够显著提高性能。

    

    级联是一种经典的策略，可以实现适应性地在样本之间变化的推理成本，其中按顺序调用一系列分类器。延迟规则确定是否调用序列中的下一个分类器，或者终止预测。一种简单的延迟规则利用当前分类器的置信度，例如基于最大预测的softmax概率。尽管对级联结构不敏感——例如不建模下游模型的错误——但这种基于置信度的延迟经常在实践中表现出色。在本文中，我们旨在更好地理解基于置信度的延迟可能失败的条件，以及何时备选的延迟策略可能更好。我们首先对最优延迟规则进行了理论表征，精确地描述了基于置信度的延迟可能受到影响的设置。然后我们研究了事后延迟机制，并验证它们可以显著提高性能。

    Cascades are a classical strategy to enable inference cost to vary adaptively across samples, wherein a sequence of classifiers are invoked in turn. A deferral rule determines whether to invoke the next classifier in the sequence, or to terminate prediction. One simple deferral rule employs the confidence of the current classifier, e.g., based on the maximum predicted softmax probability. Despite being oblivious to the structure of the cascade -- e.g., not modelling the errors of downstream models -- such confidence-based deferral often works remarkably well in practice. In this paper, we seek to better understand the conditions under which confidence-based deferral may fail, and when alternate deferral strategies can perform better. We first present a theoretical characterisation of the optimal deferral rule, which precisely characterises settings under which confidence-based deferral may suffer. We then study post-hoc deferral mechanisms, and demonstrate they can significantly improv
    
[^159]: CasTGAN: 用于逼真表格数据合成的级联生成对抗网络

    CasTGAN: Cascaded Generative Adversarial Network for Realistic Tabular Data Synthesis. (arXiv:2307.00384v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00384](http://arxiv.org/abs/2307.00384)

    本文提出了一个级联生成对抗网络（CasTGAN）框架，用于生成逼真的表格数据，并特别关注输出的有效性。通过采用级联架构，其中专门的生成器对每个特征进行采样，使得合成输出更能代表真实数据。实验结果表明，CasTGAN能够产生更真实有效的合成表格数据。

    

    近年来，生成对抗网络（GANs）因其在生成可用于多种目的的合成数据方面的能力而引起了广泛关注。虽然GAN已经成功地生成了复制原始数据集动态的合成数据样本，但合成数据的有效性和潜在的隐私问题仍然是不容忽视的挑战。在本研究中，我们设计了一个级联表格GAN框架（CasTGAN），用于生成逼真的表格数据，并特别关注输出的有效性。在这个上下文中，有效性是指在真实数据中存在的特征之间的依赖关系，通常被传统的生成模型所误解。我们的关键思想是采用级联架构，其中专门的生成器对每个特征进行采样，使得合成输出更能代表真实数据。我们的实验结果显示，CasTGAN能够产生更真实有效的合成表格数据。

    Generative adversarial networks (GANs) have drawn considerable attention in recent years for their proven capability in generating synthetic data which can be utilised for multiple purposes. While GANs have demonstrated tremendous successes in producing synthetic data samples that replicate the dynamics of the original datasets, the validity of the synthetic data and the underlying privacy concerns represent major challenges which are not sufficiently addressed. In this work, we design a cascaded tabular GAN framework (CasTGAN) for generating realistic tabular data with a specific focus on the validity of the output. In this context, validity refers to the the dependency between features that can be found in the real data, but is typically misrepresented by traditional generative models. Our key idea entails that employing a cascaded architecture in which a dedicated generator samples each feature, the synthetic output becomes more representative of the real data. Our experimental resu
    
[^160]: 用于Koopman算子学习的物理信息反转神经网络

    Physics-informed invertible neural network for the Koopman operator learning. (arXiv:2306.17396v1 [math.NA])

    [http://arxiv.org/abs/2306.17396](http://arxiv.org/abs/2306.17396)

    本论文提出了一种基于物理信息的可逆神经网络，用于学习Koopman算子。 FlowDMD算法利用耦合流可逆神经网络的特性，学习Koopman算子的不变子空间，并准确重构状态变量。实验证明了该算法的优越性能。

    

    在Koopman算子理论中，通过一组可观测函数，将一个有限维的非线性系统转化为一个无穷但线性的系统。然而，基于先前知识手动选择能够覆盖Koopman算子不变子空间的可观测函数是低效和具有挑战性的，特别是在对底层系统几乎没有信息或没有任何信息的情况下。此外，目前的方法往往忽视可观测函数可逆性的重要性，导致结果不准确。为了应对这些挑战，我们提出了所谓的FlowDMD，即基于流的动态模态分解，利用耦合流可逆神经网络（CF-INN）框架。FlowDMD利用CF-INN的内在可逆特性，学习Koopman算子的不变子空间，并准确重构状态变量。数值实验证明了我们的算法相比当前方法的卓越性能。

    In Koopman operator theory, a finite-dimensional nonlinear system is transformed into an infinite but linear system using a set of observable functions. However, manually selecting observable functions that span the invariant subspace of the Koopman operator based on prior knowledge is inefficient and challenging, particularly when little or no information is available about the underlying systems. Furthermore, current methodologies tend to disregard the importance of the invertibility of observable functions, which leads to inaccurate results. To address these challenges, we propose the so-called FlowDMD, a Flow-based Dynamic Mode Decomposition that utilizes the Coupling Flow Invertible Neural Network (CF-INN) framework. FlowDMD leverages the intrinsically invertible characteristics of the CF-INN to learn the invariant subspaces of the Koopman operator and accurately reconstruct state variables. Numerical experiments demonstrate the superior performance of our algorithm compared to st
    
[^161]: 从保险业的经验对公平机器学习的启示

    Insights From Insurance for Fair Machine Learning. (arXiv:2306.14624v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14624](http://arxiv.org/abs/2306.14624)

    本文通过将保险业相关概念与机器学习的公平性问题联系起来，提供了一个新的视角，并突出了被忽视的责任和集合与个体之间的紧张关系。

    

    我们认为保险业可以作为机器学习系统社会环境的类比，从而使机器学习学者能够从丰富而跨学科的保险文献中获得启示。通过追踪保险中不确定性、公平性和责任的相互作用，为机器学习中的公平性问题提供了新的视角。我们将保险公平观念与机器学习的相关观念联系起来，并利用这个桥梁来问题化校准公平性。在这个过程中，我们突出了机器学习文献中经常被忽视的两个主题：责任和集合与个体之间的紧张关系。

    We argue that insurance can act as an analogon for the social situatedness of machine learning systems, hence allowing machine learning scholars to take insights from the rich and interdisciplinary insurance literature. Tracing the interaction of uncertainty, fairness and responsibility in insurance provides a fresh perspective on fairness in machine learning. We link insurance fairness conceptions to their machine learning relatives, and use this bridge to problematize fairness as calibration. In this process, we bring to the forefront two themes that have been largely overlooked in the machine learning literature: responsibility and aggregate-individual tensions.
    
[^162]: QH9：QM9分子的量子哈密顿预测基准测试

    QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules. (arXiv:2306.09549v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.09549](http://arxiv.org/abs/2306.09549)

    该论文提出了一种新的量子哈密顿数据集QH9，用于为各种分子提供精确的哈密顿矩阵。通过设计基准任务，展示了当前机器学习模型有能力预测任意分子的哈密顿矩阵。

    

    监督式机器学习方法越来越被用于加速电子结构预测，作为第一性原理计算方法（如密度泛函理论（DFT））的替代品。虽然许多量子化学数据集侧重于化学性质和原子力，但准确且高效地预测哈密顿矩阵的能力是非常重要和基本的物理量，它确定了物理系统和化学性质的量子状态。在这项工作中，我们生成了一个新的量子哈密顿数据集，命名为QH9，基于QM9数据集为2,399个分子动力学轨迹和130,831个稳定分子几何形态提供精确的哈密顿矩阵。通过设计各种分子的基准任务，我们展示了当前机器学习模型有能力预测任意分子的哈密顿矩阵。QH9数据集和基准模型都提供。

    Supervised machine learning approaches have been increasingly used in accelerating electronic structure prediction as surrogates of first-principle computational methods, such as density functional theory (DFT). While numerous quantum chemistry datasets focus on chemical properties and atomic forces, the ability to achieve accurate and efficient prediction of the Hamiltonian matrix is highly desired, as it is the most important and fundamental physical quantity that determines the quantum states of physical systems and chemical properties. In this work, we generate a new Quantum Hamiltonian dataset, named as QH9, to provide precise Hamiltonian matrices for 2,399 molecular dynamics trajectories and 130,831 stable molecular geometries, based on the QM9 dataset. By designing benchmark tasks with various molecules, we show that current machine learning models have the capacity to predict Hamiltonian matrices for arbitrary molecules. Both the QH9 dataset and the baseline models are provided
    
[^163]: DeepSeaNet: 使用EfficientDet提高水下目标检测的效率

    DeepSeaNet: Improving Underwater Object Detection using EfficientDet. (arXiv:2306.06075v1 [cs.CV])

    [http://arxiv.org/abs/2306.06075](http://arxiv.org/abs/2306.06075)

    本文研究了在水下环境下，使用EfficientDet等模型提高水下物体检测的效率，并通过对比多个模型的精度和推理时间，发现效率更高的模型可以更好地应对水下物体检测的挑战。

    

    海洋动物和深海物体在保护水生生物的安全时很难识别和监测。当水中含有盐和颗粒状杂质时，挑战变得更加严峻。在这种天然对抗性环境中，传统的CNN方法开始失败并且计算成本很高。本项目包括在现有的带注释的水下数据集（称为“Brackish-Dataset”）上实施和评估各种目标检测模型，包括EfficientDet、YOLOv5、YOLOv8和Detectron2。该数据集包括在视野有限的下利姆弗约尔登水中捕捉到的鱼类、蟹、海星和其他水生动物的标注图像序列。本研究项目的目的是研究较新模型在同样数据集上的效率，并通过精度和推理时间将它们与先前结果进行对比。首先，我比较了YOLOv3（31.10%平均精度均值（mAP））、YOLOv4（83.72% mAP）、YOLOv5（97.6%）、YOLOv8（98.20%）的结果。

    Marine animals and deep underwater objects are difficult to recognize and monitor for safety of aquatic life. There is an increasing challenge when the water is saline with granular particles and impurities. In such natural adversarial environment, traditional approaches like CNN start to fail and are expensive to compute. This project involves implementing and evaluating various object detection models, including EfficientDet, YOLOv5, YOLOv8, and Detectron2, on an existing annotated underwater dataset, called the Brackish-Dataset. The dataset comprises annotated image sequences of fish, crabs, starfish, and other aquatic animals captured in Limfjorden water with limited visibility. The aim of this research project is to study the efficiency of newer models on the same dataset and contrast them with the previous results based on accuracy and inference time. Firstly, I compare the results of YOLOv3 (31.10% mean Average Precision (mAP)), YOLOv4 (83.72% mAP), YOLOv5 (97.6%), YOLOv8 (98.20
    
[^164]: 跃迁于树空间：连续的树形系统推断方法用于有根和无根树

    Leaping through tree space: continuous phylogenetic inference for rooted and unrooted trees. (arXiv:2306.05739v1 [q-bio.PE])

    [http://arxiv.org/abs/2306.05739](http://arxiv.org/abs/2306.05739)

    本研究首次在连续空间中进行树形系统探索和推断，用于有根和无根树，优于当前最佳方法并在实验中证明了其效果，可用于加速生命科学的新进化发现。

    

    生物进化系统学现在是生命科学中的一个基础，可以阐明生命早期支系和传染病的起源和传播。然而，从可能的树的广阔空间中找到合适的系统树仍然具有挑战性。为了解决这个问题，我们首次在连续空间中进行了树形系统探索和推断，使梯度计算成为可能。这种连续的放松方式允许在有根和无根树中跨越树空间，且不易收敛到局部最小值。我们的方法优于当前最佳的无根树推断方法，并且在模拟中准确地推断出树和树根。该方法在实际数据中也很有效，我们在颌口动物的系统发育中证明了这一点。事实上，仅具有超指数信号的少数基因通常足以分辨脊椎动物的主要谱系。通过我们的方法，我们希望加速发现生命科学中的新进化发现。

    Phylogenetics is now fundamental in life sciences, providing insights into the earliest branches of life and the origins and spread of epidemics. However, finding suitable phylogenies from the vast space of possible trees remains challenging. To address this problem, for the first time, we perform both tree exploration and inference in a continuous space where the computation of gradients is possible. This continuous relaxation allows for major leaps across tree space in both rooted and unrooted trees, and is less susceptible to convergence to local minima. Our approach outperforms the current best methods for inference on unrooted trees and, in simulation, accurately infers the tree and root in ultrametric cases. The approach is effective in cases of empirical data with negligible amounts of data, which we demonstrate on the phylogeny of jawed vertebrates. Indeed, only a few genes with an ultrametric signal were generally sufficient for resolving the major lineages of vertebrate. With
    
[^165]: 处理联邦平均中未知参与概率的轻量级方法

    A Lightweight Method for Tackling Unknown Participation Probabilities in Federated Averaging. (arXiv:2306.03401v1 [cs.LG])

    [http://arxiv.org/abs/2306.03401](http://arxiv.org/abs/2306.03401)

    本文提出了一种轻量级方法来调整联邦平均中的聚合权重，通过根据每个客户的参与历史来处理具有不同参与率的客户，解决了在联邦学习中未知参与概率的问题。

    

    在联邦学习中，客户端通常具有先验未知的不同参与率，如果不适当处理，则可能会对联邦学习的性能造成重大影响。现有的解决方法通常基于全局方差缩减，这需要大量额外的内存，其乘法因子等于客户总数。一个重要的未解决问题是找到一种轻量级方法来处理具备不同参与率客户的联邦学习。在这篇论文中，我们通过根据每个客户的参与历史来调整联邦平均（FedAvg）中的聚合权重来解决此问题。我们首先展示了在具有异构参与概率的情况下，非最优聚合权重的FedAvg可能会从原始FL目标的最优解偏离，这表明需要找到最优聚合权重。然而，当参与概率不可知时计算最优权重非常困难。

    In federated learning (FL), clients usually have diverse participation probabilities that are unknown a priori, which can significantly harm the performance of FL if not handled properly. Existing works aiming at addressing this problem are usually based on global variance reduction, which requires a substantial amount of additional memory in a multiplicative factor equal to the total number of clients. An important open problem is to find a lightweight method for FL in the presence of clients with unknown participation rates. In this paper, we address this problem by adapting the aggregation weights in federated averaging (FedAvg) based on the participation history of each client. We first show that, with heterogeneous participation probabilities, FedAvg with non-optimal aggregation weights can diverge from the optimal solution of the original FL objective, indicating the need of finding optimal aggregation weights. However, it is difficult to compute the optimal weights when the part
    
[^166]: 基于数据驱动的遗憾平衡在线模型选择的研究

    Data-Driven Regret Balancing for Online Model Selection in Bandits. (arXiv:2306.02869v1 [cs.LG])

    [http://arxiv.org/abs/2306.02869](http://arxiv.org/abs/2306.02869)

    论文讨论在具有赌博反馈的随机环境中进行选择，提出了两种基于数据的模型选择算法，并证明了其保证。通过利用实际遗憾，这些算法在实际中取得了好效果。

    

    我们考虑在具有赌博反馈的随机环境中进行顺序决策模型选择，其中元学习器可以使用一组基本学习器，并根据每个基本学习器推荐的策略动态决策。我们通过遗憾平衡来执行模型选择，但与此相关的最近文献不同的是，我们没有假设任何关于基本学习器的先验知识，如候选遗憾保证；相反，我们以数据驱动的方式揭示这些数量。因此，元学习器能够利用每个基本学习器在给定的学习环境中产生的实际遗憾（而不是期望遗憾），并挑选出最佳的遗憾。我们设计了两个模型选择算法，操作更为雄心勃勃的遗憾概念，并且除了通过遗憾平衡证明模型选择保证外，我们还在实验中展示了处理实际遗憾的令人信服的实际优势。

    We consider model selection for sequential decision making in stochastic environments with bandit feedback, where a meta-learner has at its disposal a pool of base learners, and decides on the fly which action to take based on the policies recommended by each base learner. Model selection is performed by regret balancing but, unlike the recent literature on this subject, we do not assume any prior knowledge about the base learners like candidate regret guarantees; instead, we uncover these quantities in a data-driven manner. The meta-learner is therefore able to leverage the realized regret incurred by each base learner for the learning environment at hand (as opposed to the expected regret), and single out the best such regret. We design two model selection algorithms operating with this more ambitious notion of regret and, besides proving model selection guarantees via regret balancing, we experimentally demonstrate the compelling practical benefits of dealing with actual regrets ins
    
[^167]: 非矩形不确定性集的强健MDP的策略梯度算法

    Policy Gradient Algorithms for Robust MDPs with Non-Rectangular Uncertainty Sets. (arXiv:2305.19004v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2305.19004](http://arxiv.org/abs/2305.19004)

    本文提出了针对具有非矩形不确定性集的强健MDP的策略梯度算法，并开发了投射Langevin动力学算法和确定性策略梯度方法。数值实验展示了这些算法的有效性。

    

    我们为具有非矩形不确定性集的强健无限时域马尔可夫决策过程（MDP）提出了一个策略梯度算法，从而解决了强健MDP文献中的一个开放性挑战。确实，显示统计最优性质并充分利用有限数据的不确定性集往往不是矩形的。不幸的是，对应的强健MDPs不能用动态规划技术解决，并且实际上是可证明的不可解决的。这促使我们开发一个针对强健策略评估问题量身定制的投射Langevin动力学算法，该算法提供全局最优性保证。我们还提出了一种确定性策略梯度方法，该方法近似解决了强健策略评估问题，并证明了近似误差与不确定性集的非矩形度量成比例。数值实验展示了我们的投影Langevin动力学算法可以避免局部最优，而算法是量身定制的。

    We propose a policy gradient algorithm for robust infinite-horizon Markov Decision Processes (MDPs) with non-rectangular uncertainty sets, thereby addressing an open challenge in the robust MDP literature. Indeed, uncertainty sets that display statistical optimality properties and make optimal use of limited data often fail to be rectangular. Unfortunately, the corresponding robust MDPs cannot be solved with dynamic programming techniques and are in fact provably intractable. This prompts us to develop a projected Langevin dynamics algorithm tailored to the robust policy evaluation problem, which offers global optimality guarantees. We also propose a deterministic policy gradient method that solves the robust policy evaluation problem approximately, and we prove that the approximation error scales with a new measure of non-rectangularity of the uncertainty set. Numerical experiments showcase that our projected Langevin dynamics algorithm can escape local optima, while algorithms tailor
    
[^168]: 点过程注意力支持网格编码以实现分布外泛化

    Determinantal Point Process Attention Over Grid Codes Supports Out of Distribution Generalization. (arXiv:2305.18417v1 [cs.LG])

    [http://arxiv.org/abs/2305.18417](http://arxiv.org/abs/2305.18417)

    本文描述了一个基于点过程注意力和网格编码的算法，在分布外测试集上实现了泛化能力，为理解大脑强泛化能力提供了见解。

    

    深度神经网络在模仿类人智能方面取得了巨大进展，并且越来越多地被用来理解大脑如何解决复杂的计算问题。然而，它们仍然不能提供关于大脑如何支持人类能够实现的强形式泛化的见解。其中一个例子是分布外（OOD）泛化——在训练集分布之外的测试样例上成功执行。在这里，我们识别出大脑处理的特征，这些特征可能有助于实现这种能力。我们描述了一个两部分算法，利用神经计算的特定特征实现OOD泛化，并通过评估两个具有挑战性的认知任务的表现来提供概念验证。首先，我们利用哺乳动物大脑使用类似网格的表示（例如在内嗅皮层中）来表示度量空间的事实。

    Deep neural networks have made tremendous gains in emulating human-like intelligence, and have been used increasingly as ways of understanding how the brain may solve the complex computational problems on which this relies. However, these still fall short of, and therefore fail to provide insight into how the brain supports strong forms of generalization of which humans are capable. One such case is out-of-distribution (OOD) generalization -successful performance on test examples that lie outside the distribution of the training set. Here, we identify properties of processing in the brain that may contribute to this ability. We describe a two-part algorithm that draws on specific features of neural computation to achieve OOD generalization, and provide a proof of concept by evaluating performance on two challenging cognitive tasks. First we draw on the fact that the mammalian brain represents metric spaces using grid-like representations (e.g., in entorhinal cortex): abstract represe
    
[^169]: 使用基于文献的语境化学习生成新的科学方向

    Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14259](http://arxiv.org/abs/2305.14259)

    本文介绍了一种基于文献的发现方法，通过上下文化的学习生成新的科学方向，克服了标准方法在预测关联、忽略上下文等方面的局限性。模型使用了引文和知识图关系的网络，并使用大型语言模型进行评估，发现GPT4在生成创新思想方面表现出色。

    

    基于文献的发现（LBD）旨在通过挖掘论文并生成假设来发现新的科学知识。标准的LBD仅限于预测离散概念之间的两两关系（例如，药物和疾病的关联）。LBD还忽略了关键的上下文，例如实验设置（例如，药物评估的特定患者群体）和人类科学家考虑的背景知识和动机（例如，找到没有特定副作用的药物候选）。我们通过一种新颖的上下文化LBD（C-LBD）表述来解决这些局限性：以自然语言生成科学假设，同时将它们联系到控制假设搜索空间的上下文中。我们提出了一个建模框架，使用获得的引文和知识图关系的异构网络中的“灵感”，并创建了一个从论文中派生的新数据集。我们使用强大的大型语言模型（LLM）进行评估，发现GPT4倾向于生成具有创新性的思想。

    Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
    
[^170]: SpecInfer：利用推测推断和令牌树验证加速生成式大语言模型的服务

    SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])

    [http://arxiv.org/abs/2305.09781](http://arxiv.org/abs/2305.09781)

    SpecInfer是一种LLM服务系统，通过利用推测推断和令牌树验证来加速生成式大语言模型的推断过程，显著减少了为它们提供服务所需的端到端延迟和计算要求，同时确保模型质量。

    

    由于生成式大语言模型（LLMs）需要高计算和内存需求，因此快速和廉价地为它们提供服务是具有挑战性的。本文介绍SpecInfer，一个LLM服务系统，它利用推测推断和令牌树验证加速生成式LLM推断。SpecInfer背后的关键是将各种小型语言模型进行集体提升调整，共同预测LLM的输出； 预测结果组织成一个令牌树，其中每个节点都表示候选令牌序列。通过一种新颖的基于树的并行解码机制，以LMM作为令牌树验证器来验证令牌树所代表的所有候选令牌序列的正确性。SpecInfer使用LLM作为令牌树验证器，而不是增量解码器，从而显著减少了为生成式LLM提供服务所需的端到端延迟和计算要求，同时可确保模型质量。

    The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree verification. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.
    
[^171]: ZipIt！无需训练即可合并不同任务的模型

    ZipIt! Merging Models from Different Tasks without Training. (arXiv:2305.03053v1 [cs.CV])

    [http://arxiv.org/abs/2305.03053](http://arxiv.org/abs/2305.03053)

    本文介绍了一种无需额外训练即可合并不同任务上训练的模型的方法“ZipIt！”。

    

    典型的深度视觉识别模型能够执行它们经过训练的单一任务。本文解决将完全不同的、每个解决一个独立任务的模型合并成一个多任务模型的极其困难的问题，而且不需要任何额外的训练。以前的模型合并工作将一个模型置换到另一个模型的空间中，再将它们相加。虽然这对于在同一个任务上经过训练的模型起作用，但我们发现，这未能考虑到在不同任务上经过训练的模型之间的差异。因此，我们引入了“ZipIt！”，这是一种通用的方法，用于合并相同结构的两个任意模型，其中包括两种简单的策略。首先，为了考虑到在模型之间没有共享的特征，我们将模型合并问题扩展到还允许合并每个模型中的特征，定义一个通用的“zip”操作。其次，我们添加支持部分压缩模型的功能，直到特定层。

    Typical deep visual recognition models are capable of performing the one task they were trained on. In this paper, we tackle the extremely difficult problem of combining completely distinct models with different initializations, each solving a separate task, into one multi-task model without any additional training. Prior work in model merging permutes one model to the space of the other then adds them together. While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce "ZipIt!", a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren't shared between models, we expand the model merging problem to additionally allow for merging features within each model by defining a general "zip" operation. Second, we add support for partially zipping the models up until a specified layer
    
[^172]: 基于能量模型的零样本场景重新排列规划器

    Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])

    [http://arxiv.org/abs/2304.14391](http://arxiv.org/abs/2304.14391)

    本文提出一种基于能量模型的零样本场景重新排列规划器，通过语言指导的空间概念来实现长指令以及在训练时从未见过的空间概念组合。本文的模型在指令导向操作基准测试以及组合指令基准测试中表现良好，优于基于语言表达的最先进方法，并且可以成功地解决之前从未见过的复杂指令和场景。

    

    本文致力于开发一个场景重排框架，可以解释长指令以及在训练时从未见过的空间概念组合。我们提出使用相对对象排列的能量函数来表示语言指导的空间概念。语言解析器将指令映射到相应的能量函数，而开放式视觉语言模型将它们的参数基于场景中的相关对象进行修正。通过梯度下降求解能量函数的总和，并利用基于本地计算机视觉的策略将对象重新定位到推断的目标位置，即可生成目标场景配置。我们在已建立的指令导向操作基准测试以及我们提出的组合指令基准测试中测试了模型，结果表明，我们的模型的绩效优于基于语言表达的最先进方法，并且可以成功地解决之前从未见过的复杂指令和场景。

    Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
    
[^173]: 基于LSTM-DeepLabv3+和时空特征融合的贝叶斯优化城市洪水预测模型改进

    Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian Optimization with Spatiotemporal feature fusion. (arXiv:2304.09994v1 [cs.LG])

    [http://arxiv.org/abs/2304.09994](http://arxiv.org/abs/2304.09994)

    本研究提出了一种基于CNN-RNN混合特征融合建模的贝叶斯优化城市洪水预测模型，实现了静态和动态的预测，并通过结合多个CNN和RNN模型，在精度上取得了显著提高。

    

    深度学习模型因其相对传统方法更高的准确性和效率，逐渐成为流行的洪水预测方法。但是，当前的机器学习方法通常依赖于单独的空间或时间特征分析，并对输入数据的类型、数量和维度存在限制。本研究提出了一个基于CNN-RNN的混合特征融合建模方法，用于城市洪水预测，将CNN在处理空间特征方面的优势和RNN在分析不同维度的时间序列方面的优势整合起来。这种方法允许进行静态和动态的洪水预测。应用贝叶斯优化来确定七个最具影响力的洪水驱动因素，并确定最佳组合策略。通过结合四个CNN（FCN，UNet，SegNet，DeepLabv3+）和三个RNN（LSTM，BiLSTM，GRU），最优混合模型被确定为LSTM-DeepLabv3+。该模型实现了最高的预测准确性（MAE、RMSE、NSE和KGE率）。

    Deep learning models have become increasingly popular for flood prediction due to their superior accuracy and efficiency compared to traditional methods. However, current machine learning methods often rely on separate spatial or temporal feature analysis and have limitations on the types, number, and dimensions of input data. This study presented a CNN-RNN hybrid feature fusion modelling approach for urban flood prediction, which integrated the strengths of CNNs in processing spatial features and RNNs in analyzing different dimensions of time sequences. This approach allowed for both static and dynamic flood predictions. Bayesian optimization was applied to identify the seven most influential flood-driven factors and determine the best combination strategy. By combining four CNNs (FCN, UNet, SegNet, DeepLabv3+) and three RNNs (LSTM, BiLSTM, GRU), the optimal hybrid model was identified as LSTM-DeepLabv3+. This model achieved the highest prediction accuracy (MAE, RMSE, NSE, and KGE wer
    
[^174]: 高效率对抗性模仿学习

    Sample-efficient Adversarial Imitation Learning. (arXiv:2303.07846v1 [cs.LG])

    [http://arxiv.org/abs/2303.07846](http://arxiv.org/abs/2303.07846)

    本研究提出了一种利用自监督表示来增强样本效率的对抗性模仿学习方法，从而学习不受扭曲影响的状态和动作表示以建立非图像控制任务的预测表征。

    

    模仿学习即通过演示进行学习，已经被研究并应用于序贯决策任务中，在这类任务中，奖励函数并不是预定义的。然而，模仿学习方法仍需要大量的专家演示样本才能成功模仿专家的行为。为提高样本效率，我们利用自监督表示学习，该方法可以从给定的数据生成大量的训练信号。在本研究中，我们提出了一种基于自监督表示的对抗性模仿学习方法，以学习不受各种扭曲影响的状态和动作表示，并建立非图像控制任务的预测表征。特别是，与现有的表格数据自监督学习方法相比，我们提出了一种针对状态和动作表示的不同损坏方法，以使其能够抵抗各种扭曲。理论和实证观察表明，使一个信息量大的特征流形与一个简单的生成器与一个复杂的分类器协同工作能够提高状态表征的质量。

    Imitation learning, in which learning is performed by demonstration, has been studied and advanced for sequential decision-making tasks in which a reward function is not predefined. However, imitation learning methods still require numerous expert demonstration samples to successfully imitate an expert's behavior. To improve sample efficiency, we utilize self-supervised representation learning, which can generate vast training signals from the given data. In this study, we propose a self-supervised representation-based adversarial imitation learning method to learn state and action representations that are robust to diverse distortions and temporally predictive, on non-image control tasks. In particular, in comparison with existing self-supervised learning methods for tabular data, we propose a different corruption method for state and action representations that is robust to diverse distortions. We theoretically and empirically observe that making an informative feature manifold with 
    
[^175]: 通过稀疏高斯过程校准Transformer

    Calibrating Transformers via Sparse Gaussian Processes. (arXiv:2303.02444v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02444](http://arxiv.org/abs/2303.02444)

    提出了一种通过Sparse Gaussian Process attention (SGPA)来校准Transformer模型不确定性的方法。在文本、图像和图形的预测任务中，SGPA-based Transformers在预测准确性上表现出竞争力，并显著改善了内分布校准和外分布的鲁棒性和检测能力。

    

    Transformer模型在自然语言处理、语音识别和计算机视觉等广泛应用中取得了巨大成功。将Transformer的成功扩展到安全关键领域需要准确估计的不确定性，这方面的研究较少。为了解决这个问题，我们提出了稀疏高斯过程注意力（SGPA），它直接在Transformer的多头自注意力块（MHA）的输出空间中进行贝叶斯推断，以校准其不确定性。它用一个有效的对称核替代了缩放点积操作，并使用稀疏高斯过程（SGP）技术来近似MHA输出的后验过程。经验上，在文本、图像和图形的一系列预测任务中，基于SGPA的Transformer模型实现了有竞争力的预测准确性，同时显著改善了内分布校准和外分布的鲁棒性和检测能力。

    Transformer models have achieved profound success in prediction tasks in a wide range of applications in natural language processing, speech recognition and computer vision. Extending Transformer's success to safety-critical domains requires calibrated uncertainty estimation which remains under-explored. To address this, we propose Sparse Gaussian Process attention (SGPA), which performs Bayesian inference directly in the output space of multi-head attention blocks (MHAs) in transformer to calibrate its uncertainty. It replaces the scaled dot-product operation with a valid symmetric kernel and uses sparse Gaussian processes (SGP) techniques to approximate the posterior processes of MHA outputs. Empirically, on a suite of prediction tasks on text, images and graphs, SGPA-based Transformers achieve competitive predictive accuracy, while noticeably improving both in-distribution calibration and out-of-distribution robustness and detection.
    
[^176]: 预测模型的损失控制校准

    Loss-Controlling Calibration for Predictive Models. (arXiv:2301.04378v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.04378](http://arxiv.org/abs/2301.04378)

    本文提出了一种学习框架，用于对可交换数据进行损失控制预测的校准预测模型。通过引入保持交换性质的变换，我们证明了在获得测试标签时有有限样本控制保证，并发展了一种近似方法来构建预测器。这种方法是符合损失控制预测的自然扩展。

    

    我们提出了一个学习框架，用于对可交换数据进行损失控制预测的校准预测模型，扩展了我们最近提出的适用于更一般情况的符合损失控制预测。与之相比，通过提出的损失控制方法构建的预测器不限于集合预测器，并且损失函数可以是任何可测函数，而不需要单调性假设。为了以有效的方式控制损失值，我们引入保持交换性质的变换，以证明在获得测试标签时有有限样本控制保证，并然后发展出一种构建预测器的近似方法。这些变换可以建立在任何预定义的函数上，包括使用优化算法进行参数搜索。这种方法是符合损失控制预测的自然扩展，因为当集合预测器具有嵌套属性且损失函数为单调函数时，它可以化简为后者。

    We propose a learning framework for calibrating predictive models to make loss-controlling prediction for exchangeable data, which extends our recently proposed conformal loss-controlling prediction for more general cases. By comparison, the predictors built by the proposed loss-controlling approach are not limited to set predictors, and the loss function can be any measurable function without the monotone assumption. To control the loss values in an efficient way, we introduce transformations preserving exchangeability to prove finite-sample controlling guarantee when the test label is obtained, and then develop an approximation approach to construct predictors. The transformations can be built on any predefined function, which include using optimization algorithms for parameter searching. This approach is a natural extension of conformal loss-controlling prediction, since it can be reduced to the latter when the set predictors have the nesting property and the loss functions are mono
    
[^177]: 操纵损失控制的一致预测

    Conformal Loss-Controlling Prediction. (arXiv:2301.02424v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.02424](http://arxiv.org/abs/2301.02424)

    本文提出了一种名为操纵损失控制的预测的学习框架，扩展了一致预测的范围，使其适用于需要控制损失函数值的情况。通过实验证明了该框架在有限样本情况下的控制保证。

    

    一致预测是一个控制预测集的预测覆盖率的学习框架，它可以建立在任何用于点预测的学习算法上。本文提出了一个名为操纵损失控制的预测的学习框架，它将一致预测扩展到需要控制损失函数值的情况。与现有的关于控制预测集风险和一致风险控制的工作不同，该论文中提出的方法侧重于对任何测试对象的损失，这是从错误覆盖损失到一些通用损失的一致预测的扩展。在有限样本情况下，通过假设数据的可互换性证明了控制保证，并且通过分类具有变化损失的实验和数值天气预报应用的统计后处理进行了实证测试。

    Conformal prediction is a learning framework controlling prediction coverage of prediction sets, which can be built on any learning algorithm for point prediction. This work proposes a learning framework named conformal loss-controlling prediction, which extends conformal prediction to the situation where the value of a loss function needs to be controlled. Different from existing works about risk-controlling prediction sets and conformal risk control with the purpose of controlling the expected values of loss functions, the proposed approach in this paper focuses on the loss for any test object, which is an extension of conformal prediction from miscoverage loss to some general loss. The controlling guarantee is proved under the assumption of exchangeability of data in finite-sample cases and the framework is tested empirically for classification with a class-varying loss and statistical postprocessing of numerical weather forecasting applications, which are introduced as point-wise c
    
[^178]: 同质性在图卷积网络的双下降泛化中的调制作用

    Homophily modulates double descent generalization in graph convolution networks. (arXiv:2212.13069v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13069](http://arxiv.org/abs/2212.13069)

    本文通过使用统计物理和随机矩阵理论的分析工具，精确地表征了简单图卷积网络在背景随机块模型上的泛化，提出了同质性在图卷积网络的泛化中的调制作用。

    

    图神经网络是用于关系数据集（如代谢、交通和社交网络）的最成功的机器学习模型之一。然而，它们对数据中编码的各种交互的强大泛化的决定因素并不为人所知。来自统计学习理论的方法无法解释出现的现象，如双下降或风险取决于交互性质的问题。我们使用统计物理和随机矩阵理论的分析工具来精确地表征简单图卷积网络在背景随机块模型上的泛化。导出的曲线现象学上十分丰富：它们解释了同质性和异质性学习之间的区别，并预测了最近作品所质疑的GNN中双下降现象的存在。我们展示了风险如何取决于图中的噪声、特征中的噪声和用于训练的节点比例之间的相互作用。我们的分析为理解同质性如何调制图神经网络的泛化提供了第一步。

    Graph neural networks are among the most successful machine learning models for relational datasets like metabolic, transportation, and social networks. Yet the determinants of their strong generalization for diverse interactions encoded in the data are not well understood. Methods from statistical learning theory do not explain emergent phenomena such as double descent or the dependence of risk on the nature of interactions. We use analytical tools from statistical physics and random matrix theory to precisely characterize generalization in simple graph convolution networks on the contextual stochastic block model. The derived curves are phenomenologically rich: they explain the distinction between learning on homophilic and heterophilic and they predict double descent whose existence in GNNs has been questioned by recent work. We show how risk depends on the interplay between the noise in the graph, noise in the features, and the proportion of nodes used for training. Our analysis pr
    
[^179]: 基于图神经网络的边缘预测中细化边缘使用方法

    Refined Edge Usage of Graph Neural Networks for Edge Prediction. (arXiv:2212.12970v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12970](http://arxiv.org/abs/2212.12970)

    这项研究提出了一种新的边缘预测范式（EMPIRE），通过细化边缘使用方法解决了节点分类任务和边缘预测任务之间的区别。该方法引入了边缘拆分技术和新的消息传递机制，以更好地利用边缘的拓扑结构和监督信号。

    

    图神经网络（GNNs）最初用于节点分类，也激发了许多关于边缘预测（即链路预测）的最新研究。然而，现有方法在关于这两个任务的区别方面缺乏精细的设计，这一点常常被忽视：（i）对于节点分类任务而言，边仅构成拓扑结构，但在边缘预测任务中既可以作为拓扑结构，也可以作为监督信号（即标签）；（ii）节点分类是对每个节点进行预测，而边缘预测则由每对节点决定。为此，我们提出了一种称为边缘感知消息传递神经网络（EMPIRE）的新型边缘预测范式。具体而言，我们首先引入一种边缘拆分技术来指定每个边的使用方式，其中每个边仅用作拓扑结构或监督信号（分别称为拓扑边或监督边）。然后，我们开发了一种新的消息传递机制，生成消息的传递的过程。

    Graph Neural Networks (GNNs), originally proposed for node classification, have also motivated many recent works on edge prediction (a.k.a., link prediction). However, existing methods lack elaborate design regarding the distinctions between two tasks that have been frequently overlooked: (i) edges only constitute the topology in the node classification task but can be used as both the topology and the supervisions (i.e., labels) in the edge prediction task; (ii) the node classification makes prediction over each individual node, while the edge prediction is determinated by each pair of nodes. To this end, we propose a novel edge prediction paradigm named Edge-aware Message PassIng neuRal nEtworks (EMPIRE). Concretely, we first introduce an edge splitting technique to specify use of each edge where each edge is solely used as either the topology or the supervision (named as topology edge or supervision edge). We then develop a new message passing mechanism that generates the messages t
    
[^180]: 规范化交叉密度函数：一种用于量化随机过程统计依赖关系的框架

    The Normalized Cross Density Functional: A Framework to Quantify Statistical Dependence for Random Processes. (arXiv:2212.04631v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04631](http://arxiv.org/abs/2212.04631)

    本文提出了一种用于量化随机过程统计依赖关系的框架，通过最大化交替协方差估计和规范化交叉密度来衡量多变量统计依赖性，并应用于机器学习架构中。

    

    本文提出了一种基于阿尔弗雷德·雷尼（Alfr\'ed R\'enyi）的功能方法论，通过对两个连续随机过程（r.p.）之间的统计依赖关系进行新颖的多变量定义。将随机过程样本对的互信息的对数论证命名为规范化交叉密度（NCD），定义了一种对称和自伴的正定函数。我们证明，最大化交替协方差估计（ACE）递归应用于输入样本对的联合概率密度，符合雷尼的最大相关性的所有性质。我们提出了NCD的特征谱作为一种新颖的多变量度量，用于衡量输入和输出r.p.之间的统计依赖关系。利用r.p.的实现，也可以直接估计多变量统计依赖性。提出的功能最大相关算法（FMCA）应用于由两个神经网络构建的机器学习架构上，通过逼近联合训练来同时学习。

    This paper proposes a novel multivariate definition of statistical dependence between two continuous random processes (r.p.) using a functional methodology inspired by Alfr\'ed R\'enyi. The argument of the logarithm of mutual information between pairs of samples of a r.p., named here the normalized cross density (NCD), defines a symmetric and self-adjoint positive definite function. We show that maximizing the alternating covariance estimation (ACE) recursion, applied to each of the joint probability density of input sample pairs, obeys all the properties of Renyi's maximal correlation. We propose the NCD's eigenspectrum as a novel multivariate measure of the statistical dependence between the input and output r.p.  The multivariate statistical dependence can also be estimated directly from r.p. realizations. The proposed functional maximum correlation algorithm (FMCA) is applied to a machine learning architecture built from two neural networks that learn concurrently by approximating 
    
[^181]: Copula联合预测用于多步时间序列预测

    Copula Conformal Prediction for Multi-step Time Series Forecasting. (arXiv:2212.03281v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03281](http://arxiv.org/abs/2212.03281)

    本文提出了一种 Copula 联合预测算法 CopulaCPTS，用于多元、多步时间序列预测，经过实验验证，其置信区间比现有技术更精准和更锐利。

    

    精确的不确定性度量是构建强大可靠的机器学习系统的关键步骤。拟合预测是一种流行的无分布不确定性量化算法，因其易于实现、统计覆盖保证和对底层预测算法的多样性而受到欢迎。然而，现有的时间序列置信预测算法仅限于单步预测，未考虑时序依赖。本文提出一种 Copula 联合预测算法，用于多元、多步时间序列预测 CopulaCPTS。我们证明了 CopulaCPTS 具有有限的样本*有效性保证。在多个合成和真实世界的多元时间序列数据集上，我们展示了 CopulaCPTS 的多步预测可产生比现有技术更精准和更锐利的置信区间。

    Accurate uncertainty measurement is a key step to building robust and reliable machine learning systems. Conformal prediction is a distribution-free uncertainty quantification algorithm popular for its ease of implementation, statistical coverage guarantees, and versatility for underlying forecasters. However, existing conformal prediction algorithms for time series are limited to single-step prediction without considering the temporal dependency. In this paper we propose a Copula Conformal Prediction algorithm for multivariate, multi-step Time Series forecasting, CopulaCPTS. We prove that CopulaCPTS has finite sample validity guarantee. On several synthetic and real-world multivariate time series datasets, we show that CopulaCPTS produces more calibrated and sharp confidence intervals for multi-step prediction tasks than existing techniques.
    
[^182]: 随机互补复合最小化的最优算法

    Optimal Algorithms for Stochastic Complementary Composite Minimization. (arXiv:2211.01758v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01758](http://arxiv.org/abs/2211.01758)

    本文研究了随机情况下的互补复合最小化问题，并提出了新的期望超额风险界限和高概率超额风险界限的算法，这是一类具有重要理论和实际意义的问题。

    

    受统计学和机器学习中的正则化技术的启发，我们研究了随机情况下的互补复合最小化问题。这个问题对应于最小化一个（弱）平滑函数与一个具有随机一阶预言机的结构化均匀凸（可能是非平滑和非Lipschitz）正则化项之和。尽管已经对相关的设置进行了大量的工作，但在我们工作之前，对于这个问题的复杂性界限是未知的。我们通过提供新的期望超额风险界限和高概率超额风险界限来填补了这个空白。我们的算法几乎是最优的，这一点我们通过对这类问题的复杂性较低界限的新证明来证明。我们最后通过提供数值结果来比较我们的方法与现有技术的性能。

    Inspired by regularization techniques in statistics and machine learning, we study complementary composite minimization in the stochastic setting. This problem corresponds to the minimization of the sum of a (weakly) smooth function endowed with a stochastic first-order oracle, and a structured uniformly convex (possibly nonsmooth and non-Lipschitz) regularization term. Despite intensive work on closely related settings, prior to our work no complexity bounds for this problem were known. We close this gap by providing novel excess risk bounds, both in expectation and with high probability. Our algorithms are nearly optimal, which we prove via novel lower complexity bounds for this class of problems. We conclude by providing numerical results comparing our methods to the state of the art.
    
[^183]: 基于同步和同伦优化的神经常微分方程训练方法用于精确动力学发现

    Homotopy-based training of NeuralODEs for accurate dynamics discovery. (arXiv:2210.01407v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01407](http://arxiv.org/abs/2210.01407)

    本论文提出了一种新的神经常微分方程训练方法，基于同步和同伦优化，可以用于从时间序列数据中提取动力学规律，而无需对模型架构进行修改。

    

    神经常微分方程（NeuralODEs）作为神经网络和物理科学基于微分方程的建模范式之间的桥梁，是从时间序列数据中提取动力学规律的一种有吸引力的方式。然而，这些模型通常表现出长时间的训练和次优的结果，特别是对于更长时间段的数据。本文提出了一种基于同步和同伦优化的神经常微分方程训练方法，不需要对模型架构进行改变。我们展示了将模型动力学和训练数据同步可以驯服原本不规则的损失和的景象，同伦优化可以利用这一点来增强训练。

    Neural Ordinary Differential Equations (NeuralODEs) present an attractive way to extract dynamical laws from time series data, as they bridge neural networks with the differential equation-based modeling paradigm of the physical sciences. However, these models often display long training times and suboptimal results, especially for longer duration data. While a common strategy in the literature imposes strong constraints to the NeuralODE architecture to inherently promote stable model dynamics, such methods are ill-suited for dynamics discovery as the unknown governing equation is not guaranteed to satisfy the assumed constraints. In this paper, we develop a new training method for NeuralODEs, based on synchronization and homotopy optimization, that does not require changes to the model architecture. We show that synchronizing the model dynamics and the training data tames the originally irregular loss landscape, which homotopy optimization can then leverage to enhance training. Throug
    
[^184]: 一项使用电子病历在重症监护室中进行 COVID-19 预测建模的全面基准测试

    A Comprehensive Benchmark for COVID-19 Predictive Modeling Using Electronic Health Records in Intensive Care. (arXiv:2209.07805v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07805](http://arxiv.org/abs/2209.07805)

    本文提出了两个针对COVID-19患者的临床预测任务：Outcome-specific length-of-stay prediction 和 Early mortality prediction，旨在填补临床应用和传统预测任务之间的差距，并提供一个基准测试框架，以实现公平比较各种模型。

    

    COVID-19 疫情给全球医疗保健系统带来了沉重的负担，造成巨大的社会破坏和经济损失。许多深度学习模型已经被提出来使用电子病历数据进行临床预测任务，例如 COVID-19 患者在重症监护室中的死亡风险预测。尽管在某些临床应用中取得了初步的成功，但目前缺乏可公平比较各种模型的基准测试结果，以便为实际临床使用选择最佳模型。此外，传统预测任务的制定与重症监护室的实际临床实践存在差异。为填补这些差距，我们提出了两个临床预测任务：针对 COVID-19 重症监护室患者的特定结果预测和早期死亡预测。这两个任务是根据天真的住院时间和死亡率预测任务进行调整的，以适应 COVID-19 的临床实践。

    The COVID-19 pandemic has posed a heavy burden to the healthcare system worldwide and caused huge social disruption and economic loss. Many deep learning models have been proposed to conduct clinical predictive tasks such as mortality prediction for COVID-19 patients in intensive care units using Electronic Health Record (EHR) data. Despite their initial success in certain clinical applications, there is currently a lack of benchmarking results to achieve a fair comparison so that we can select the optimal model for clinical use. Furthermore, there is a discrepancy between the formulation of traditional prediction tasks and real-world clinical practice in intensive care. To fill these gaps, we propose two clinical prediction tasks, Outcome-specific length-of-stay prediction and Early mortality prediction for COVID-19 patients in intensive care units. The two tasks are adapted from the naive length-of-stay and mortality prediction tasks to accommodate the clinical practice for COVID-19 
    
[^185]: 神经会合：面向星际物体的可靠导航和控制的证明

    Neural-Rendezvous: Provably Robust Guidance and Control to Encounter Interstellar Objects. (arXiv:2208.04883v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2208.04883](http://arxiv.org/abs/2208.04883)

    本文提出了神经会合，一种深度学习导航和控制框架，用于可靠、准确和自主地遭遇快速移动的星际物体。它通过点最小范数追踪控制和谱归一化深度神经网络引导策略来提供高概率指数上界的飞行器交付误差。

    

    星际物体（ISOs）很可能是不可替代的原始材料，在理解系外行星星系方面具有重要价值。然而，由于其运行轨道难以约束，通常具有较高的倾角和相对速度，使用传统的人在环路方法探索ISOs具有相当大的挑战性。本文提出了一种名为神经会合的深度学习导航和控制框架，用于在实时中以可靠、准确和自主的方式遭遇快速移动的物体，包括ISOs。它在基于谱归一化的深度神经网络的引导策略之上使用点最小范数追踪控制，其中参数通过直接惩罚MPC状态轨迹跟踪误差的损失函数进行调优。我们展示了神经会合在预期的飞行器交付误差上提供了高概率指数上界，其证明利用了随机递增稳定性分析。

    Interstellar objects (ISOs) are likely representatives of primitive materials invaluable in understanding exoplanetary star systems. Due to their poorly constrained orbits with generally high inclinations and relative velocities, however, exploring ISOs with conventional human-in-the-loop approaches is significantly challenging. This paper presents Neural-Rendezvous, a deep learning-based guidance and control framework for encountering fast-moving objects, including ISOs, robustly, accurately, and autonomously in real time. It uses pointwise minimum norm tracking control on top of a guidance policy modeled by a spectrally-normalized deep neural network, where its hyperparameters are tuned with a loss function directly penalizing the MPC state trajectory tracking error. We show that Neural-Rendezvous provides a high probability exponential bound on the expected spacecraft delivery error, the proof of which leverages stochastic incremental stability analysis. In particular, it is used to
    
[^186]: 在线双层优化：在线交替梯度方法的遗憾分析

    Online Bilevel Optimization: Regret Analysis of Online Alternating Gradient Methods. (arXiv:2207.02829v5 [math.OC] UPDATED)

    [http://arxiv.org/abs/2207.02829](http://arxiv.org/abs/2207.02829)

    本文介绍了一种在线双层优化设置，提供了新的双层遗憾定义，开发了一种能够利用光滑性的在线交替时间平均梯度方法，并给出了相关的遗憾界限。

    

    本文介绍了一种在线双层优化设置，其中依次透露一系列的时变双层问题。我们扩展了已知的单层在线算法的遗憾界限到双层设置。具体而言，我们提供了新的"双层遗憾"定义，开发了一种能够利用光滑性的在线交替时间平均梯度方法，并给出了关于内部和外部最小化序列的路径长度的遗憾界限。

    This paper introduces an \textit{online bilevel optimization} setting in which a sequence of time-varying bilevel problems are revealed one after the other. We extend the known regret bounds for single-level online algorithms to the bilevel setting. Specifically, we provide new notions of \textit{bilevel regret}, develop an online alternating time-averaged gradient method that is capable of leveraging smoothness, and give regret bounds in terms of the path-length of the inner and outer minimizer sequences.
    
[^187]: 通过标准化流进行逐渐领域适应

    Gradual Domain Adaptation via Normalizing Flows. (arXiv:2206.11492v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.11492](http://arxiv.org/abs/2206.11492)

    该论文提出使用标准化流来解决逐渐领域适应中中间域有限且距离较大的问题，并通过从源域到高斯混合分布学习目标域的分布变换。

    

    当源域和目标域之间存在较大差距时，传统的领域适应方法效果不佳。逐渐领域适应是解决该问题的一种方法，它涉及利用逐渐从源域转移到目标域的中间域。在先前的工作中，假设中间域的数量较大且相邻域之间的距离较小，因此，涉及使用无标签数据集进行自我训练的逐渐领域适应算法是可行的。然而，在实践中，逐渐自我训练将失败，因为中间域的数量有限且相邻域之间的距离较大。我们提出使用标准化流来解决这个问题，同时保持无监督领域适应的框架。所提出的方法通过从源域到高斯混合分布学习目标域的分布变换。

    Standard domain adaptation methods do not work well when a large gap exists between the source and target domains. Gradual domain adaptation is one of the approaches used to address the problem. It involves leveraging the intermediate domain, which gradually shifts from the source domain to the target domain. In previous work, it is assumed that the number of intermediate domains is large and the distance between adjacent domains is small; hence, the gradual domain adaptation algorithm, involving self-training with unlabeled datasets, is applicable. In practice, however, gradual self-training will fail because the number of intermediate domains is limited and the distance between adjacent domains is large. We propose the use of normalizing flows to deal with this problem while maintaining the framework of unsupervised domain adaptation. The proposed method learns a transformation from the distribution of the target domain to the Gaussian mixture distribution via the source domain. We e
    
[^188]: 通过基于边的Weisfeiler-Lehman算法增强GNN

    Empowering GNNs via Edge-Aware Weisfeiler-Lehman Algorithm. (arXiv:2206.02059v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02059](http://arxiv.org/abs/2206.02059)

    本文提出了一种基于边缘感知的Weisfeiler-Lehman算法，以增强图神经网络的表达能力，同时保持消息传递方案的可扩展性。实验表明，我们NC-GNN框架在各种基准测试中表现出有效性和高效性。

    

    消息传递图神经网络(GNN)的表达能力被已知的一维Weisfeiler-Lehman (1-WL)算法上界所限制。为了实现更强大的GNN，现有的尝试要么需要特定的特征，要么涉及高时间和空间复杂度的操作。在本文中，我们提出了一个通用且可证明具有强大表达力的GNN框架，保持了消息传递方案的可扩展性。具体而言，我们首先通过考虑邻居之间的边缘来授权1-WL进行图同构测试，从而产生NC-1-WL。 NC-1-WL的表达能力在理论上被显示为严格高于1-WL且低于3-WL。进一步，我们提出了NC-GNN框架作为NC-1-WL的可区分神经版本。我们的简单NC-GNN实现可证明与NC-1-WL一样强大。实验表明，我们的NC-GNN在各种基准测试中表现出有效性和高效性。

    Message passing graph neural networks (GNNs) are known to have their expressiveness upper-bounded by 1-dimensional Weisfeiler-Lehman (1-WL) algorithm. To achieve more powerful GNNs, existing attempts either require ad hoc features, or involve operations that incur high time and space complexities. In this work, we propose a general and provably powerful GNN framework that preserves the scalability of the message passing scheme. In particular, we first propose to empower 1-WL for graph isomorphism test by considering edges among neighbors, giving rise to NC-1-WL. The expressiveness of NC-1-WL is shown to be strictly above 1-WL and below 3-WL theoretically. Further, we propose the NC-GNN framework as a differentiable neural version of NC-1-WL. Our simple implementation of NC-GNN is provably as powerful as NC-1-WL. Experiments demonstrate that our NC-GNN performs effectively and efficiently on various benchmarks.
    
[^189]: 基于自适应本地邻域的神经网络用于从稀疏采样数据中重建MR图像

    Adaptive Local Neighborhood-based Neural Networks for MR Image Reconstruction from Undersampled Data. (arXiv:2206.00775v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2206.00775](http://arxiv.org/abs/2206.00775)

    本文提出了一种基于自适应本地邻域的神经网络技术，用于从稀疏采样数据中高效重建MR图像，该技术具有较强的适应性和鲁棒性。

    

    最近的医学图像重建技术致力于以尽可能低的成本和对患者产生最小不良影响的方式生成适用于临床使用的高质量医学图像。最近的研究表明，使用深度学习可以从稀疏采样的k空间数据中重建MR图像具有显著的潜力。在本文中，我们提出了一种在重建时通过对训练集的小区域进行适应性估计来快速估计深度神经网络的技术。简而言之，我们的算法在搜索与测试重建相似的数据集邻居和训练这些邻居上的局部网络，然后更新测试重建之间进行交替。由于我们的重建模型是在某种程度上与正在重建的图像相似的数据集上学习而不是在大规模多样的训练集上拟合的，因此它对新的扫描更具适应性。它还可以处理训练集中的变化。

    Recent medical image reconstruction techniques focus on generating high-quality medical images suitable for clinical use at the lowest possible cost and with the fewest possible adverse effects on patients. Recent works have shown significant promise for reconstructing MR images from sparsely sampled k-space data using deep learning. In this work, we propose a technique that rapidly estimates deep neural networks directly at reconstruction time by fitting them on small adaptively estimated neighborhoods of a training set. In brief, our algorithm alternates between searching for neighbors in a data set that are similar to the test reconstruction, and training a local network on these neighbors followed by updating the test reconstruction. Because our reconstruction model is learned on a dataset that is in some sense similar to the image being reconstructed rather than being fit on a large, diverse training set, it is more adaptive to new scans. It can also handle changes in training set
    
[^190]: 带有偏好引导的个性化算法干预研究

    Personalized Algorithmic Recourse with Preference Elicitation. (arXiv:2205.13743v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13743](http://arxiv.org/abs/2205.13743)

    研究提出了PEAR方法，这是一个首个能够针对最终用户需求提供个性化算法补救成本的人机交互方法。该方法利用贝叶斯偏好引导的见解，通过最大化原则性信息增益度量来计算目标用户选择的预期效用，然后将偏好引导整合到强化学习框架中。该方法显著提高了算法干预的经济实用性和用户友好性。

    

    算法干预（AR）的问题是计算用户执行一系列操作以颠覆不良机器决策的过程。该过程的操作序列不应该对用户的实施提出过高的要求。然而，大多数AR方法都假设所有用户的操作成本相同，因此可能会向某些用户推荐昂贵的补救计划。为了解决这个问题，我们提出了PEAR，这是一种首个可提供个性化算法补救成本的人机交互方法，以满足任何最终用户的需求。PEAR利用贝叶斯偏好引导的见解，通过向目标用户发出选择集查询来迭代地改善对操作成本的估计值。这些查询的计算是通过最大化选择的预期效用来计算的，这是一种能够考虑成本估计和用户响应不确定性的原则性信息增益度量。PEAR将偏好引导整合到强化学习框架中，同时考虑用户实现AR任务所需达成目标的偏好，以及执行每个操作所涉及的成本。我们通过引入更具挑战性的AR任务来评估PEAR，并显示其比现有的方法找到了更为经济实用且用户友好的补救计划。

    Algorithmic Recourse (AR) is the problem of computing a sequence of actions that -- once performed by a user -- overturns an undesirable machine decision. It is paramount that the sequence of actions does not require too much effort for users to implement. Yet, most approaches to AR assume that actions cost the same for all users, and thus may recommend unfairly expensive recourse plans to certain users. Prompted by this observation, we introduce PEAR, the first human-in-the-loop approach capable of providing personalized algorithmic recourse tailored to the needs of any end-user. PEAR builds on insights from Bayesian Preference Elicitation to iteratively refine an estimate of the costs of actions by asking choice set queries to the target user. The queries themselves are computed by maximizing the Expected Utility of Selection, a principled measure of information gain accounting for uncertainty on both the cost estimate and the user's responses. PEAR integrates elicitation into a Rein
    
[^191]: 训练标签的选择很重要：如何最好地使用深度学习进行定量MRI参数估计

    Choice of training label matters: how to best use deep learning for quantitative MRI parameter estimation. (arXiv:2205.05587v3 [physics.med-ph] UPDATED)

    [http://arxiv.org/abs/2205.05587](http://arxiv.org/abs/2205.05587)

    本研究表明监督学习方法在选择标签上的天真选择导致了低偏差参数估计的限制。通过使用有意不是真实标签的训练标签，我们发现自监督方法可以提供比监督方法更低的偏差参数估计。

    

    深度学习作为一种定量MRI参数估计方法正在变得越来越流行。已经提出了一系列竞争性的实现方法，其中包括监督学习和自监督学习。自监督方法有时被称为无监督方法，它们通常基于自动编码器。而迄今为止的监督方法则是基于已知标签进行训练。这两种学习范式已被证明具有不同的优势。值得注意的是，自监督方法提供了比监督方法更低偏差的参数估计。这个结果与直觉相反 - 在理论上，将先前的知识与监督标签结合应该可以提高准确性。在这项工作中，我们展示了监督方法的这种明显限制源于对训练标签的天真选择。通过训练不是真实标签的标签，我们展示了之前与自监督相关联的低偏差参数估计。

    Deep learning (DL) is gaining popularity as a parameter estimation method for quantitative MRI. A range of competing implementations have been proposed, relying on either supervised or self-supervised learning. Self-supervised approaches, sometimes referred to as unsupervised, have been loosely based on auto-encoders, whereas supervised methods have, to date, been trained on groundtruth labels. These two learning paradigms have been shown to have distinct strengths. Notably, self-supervised approaches have offered lower-bias parameter estimates than their supervised alternatives. This result is counterintuitive - incorporating prior knowledge with supervised labels should, in theory, lead to improved accuracy. In this work, we show that this apparent limitation of supervised approaches stems from the naive choice of groundtruth training labels. By training on labels which are deliberately not groundtruth, we show that the low-bias parameter estimation previously associated with self-su
    
[^192]: 自监督异常检测：综述与展望

    Self-Supervised Anomaly Detection: A Survey and Outlook. (arXiv:2205.05173v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.05173](http://arxiv.org/abs/2205.05173)

    自监督学习的出现引发了新型异常检测算法的发展，其表现优于现有的最先进方法。本文全面综述了当前自监督异常检测方法的技术细节，并讨论了它们的优势和缺点，同时比较了这些模型与其他自监督异常检测模型以及最先进的异常检测模型的性能。

    

    异常检测在网络安全、金融和医疗等各个领域中起到了至关重要的作用，通过识别偏离正常行为的模式或事件。近年来，深度学习模型的显著增长使得在异常检测领域取得了重大进展。值得注意的是，自监督学习的出现引发了新型异常检测算法的发展，其表现优于现有的最先进方法。本文旨在全面综述当前自监督异常检测方法的技术细节，并讨论它们的优势和缺点。我们还比较了这些模型与其他自监督异常检测模型以及最先进的异常检测模型的性能。最后，本文对自监督异常检测的未来方向进行了讨论，包括开发更加有效和高效的算法等等。

    Anomaly detection (AD) plays a crucial role in various domains, including cybersecurity, finance, and healthcare, by identifying patterns or events that deviate from normal behaviour. In recent years, significant progress has been made in this field due to the remarkable growth of deep learning models. Notably, the advent of self-supervised learning has sparked the development of novel AD algorithms that outperform the existing state-of-the-art approaches by a considerable margin. This paper aims to provide a comprehensive review of the current methodologies in self-supervised anomaly detection. We present technical details of the standard methods and discuss their strengths and drawbacks. We also compare the performance of these models against each other and other state-of-the-art anomaly detection models. Finally, the paper concludes with a discussion of future directions for self-supervised anomaly detection, including the development of more effective and efficient algorithms and t
    
[^193]: 通过快速可靠的神经网络近似对多顶点系统进行坚固稳定化

    Robust stabilization of polytopic systems via fast and reliable neural network-based approximations. (arXiv:2204.13209v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2204.13209](http://arxiv.org/abs/2204.13209)

    本文通过快速可靠的神经网络近似方法，实现了对多顶点系统进行稳定化控制。离线的混合整数优化方法确保了系统在可调整大小和收敛速度的集合内保持有界。

    

    本文考虑使用快速可靠的神经网络（NN）近似传统稳定控制器的设计，用于具有多顶点不确定性的线性系统，包括具有可变结构和基于选择策略的控制律。在最近的可靠控制替代设计方法基础上，我们开发了一种系统的过程，用于在训练得到的修正线性单元（ReLU）近似替代传统控制器时，证明线性不确定系统的闭环稳定性和性能。首先，提供了一种充分条件，其中包括ReLU基于传统控制器的状态到输入映射的最坏情况近似误差，确保系统在可调整大小和收敛速度的集合内最终有界。然后，我们开发了一种离线的混合整数优化方法，可以精确计算该数量。

    We consider the design of fast and reliable neural network (NN)-based approximations of traditional stabilizing controllers for linear systems with polytopic uncertainty, including control laws with variable structure and those based on a (minimal) selection policy. Building upon recent approaches for the design of reliable control surrogates with guaranteed structural properties, we develop a systematic procedure to certify the closed-loop stability and performance of a linear uncertain system when a trained rectified linear unit (ReLU)-based approximation replaces such traditional controllers. First, we provide a sufficient condition, which involves the worst-case approximation error between ReLU-based and traditional controller-based state-to-input mappings, ensuring that the system is ultimately bounded within a set with adjustable size and convergence rate. Then, we develop an offline, mixed-integer optimization-based method that allows us to compute that quantity exactly.
    
[^194]: 多模式的虚假信息检测：方法、挑战与机遇

    Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.13883](http://arxiv.org/abs/2203.13883)

    这项研究总结了多模式虚假信息检测的方法、挑战和机遇。由于社交媒体平台的转变，虚假信息的性质也发生了变化。研究人员已经开发出自动检测跨模态不协调的技术，但仍面临挑战和不足之处，进一步的研究机会也在等待着挖掘。

    

    随着社交媒体平台从文本为主的论坛转向多模式环境，社交媒体中的虚假信息的性质也相应发生了变化。利用图像和视频等视觉模态更受用户青睐和吸引力的事实，以及文本内容有时被粗略浏览的情况，虚假信息传播者最近开始针对模态之间的上下文连接，例如文本和图像之间的关系。因此，许多研究人员已经开发出自动检测网页内容中可能存在的跨模态不协调的技术。我们分析、分类和识别现有的方法，以及它们面临的挑战和不足之处，以揭示多模式虚假信息检测领域的新研究机会。

    As social media platforms are evolving from text-based forums into multi-modal environments, the nature of misinformation in social media is also transforming accordingly. Taking advantage of the fact that visual modalities such as images and videos are more favorable and attractive to the users and textual contents are sometimes skimmed carelessly, misinformation spreaders have recently targeted contextual connections between the modalities e.g., text and image. Hence many researchers have developed automatic techniques for detecting possible cross-modal discordance in web-based content. We analyze, categorize and identify existing approaches in addition to challenges and shortcomings they face in order to unearth new research opportunities in the field of multi-modal misinformation detection.
    
[^195]: 再生粒子汤普森抽样

    Regenerative Particle Thompson Sampling. (arXiv:2203.08082v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.08082](http://arxiv.org/abs/2203.08082)

    本文提出了再生粒子汤普森抽样（RPTS），通过重新生成适应的粒子来解决汤普森抽样中粒子权重收敛于零的问题。RPTS在代表性赌博问题中展现出了灵活性和效果的提升，包括对5G网络切片的应用。

    

    本文提出了再生粒子汤普森抽样（RPTS），这是汤普森抽样的一种灵活变体。汤普森抽样本身是一种贝叶斯启发式算法，用于解决随机赌博机问题，但由于维护连续的后验分布的复杂性，它很难在实践中实现。粒子汤普森抽样（PTS）是汤普森抽样的一种近似方式，通过用离散分布替换在一组加权静态粒子上支持的连续分布来获取。我们观察到在PTS中，除了少数适应的粒子之外，所有其他粒子的权重都收敛于零。RPTS基于启发式方法：删除衰减的不适应粒子，并在适应的幸存粒子附近再生新粒子。实证结果表明，从PTS到RPTS的普遍改进和RPTS在一组代表性赌博问题中的灵活性和功效，包括对5G网络切片的应用。

    This paper proposes regenerative particle Thompson sampling (RPTS), a flexible variation of Thompson sampling. Thompson sampling itself is a Bayesian heuristic for solving stochastic bandit problems, but it is hard to implement in practice due to the intractability of maintaining a continuous posterior distribution. Particle Thompson sampling (PTS) is an approximation of Thompson sampling obtained by simply replacing the continuous distribution by a discrete distribution supported at a set of weighted static particles. We observe that in PTS, the weights of all but a few fit particles converge to zero. RPTS is based on the heuristic: delete the decaying unfit particles and regenerate new particles in the vicinity of fit surviving particles. Empirical evidence shows uniform improvement from PTS to RPTS and flexibility and efficacy of RPTS across a set of representative bandit problems, including an application to 5G network slicing.
    
[^196]: SkipNode: 缓解深度图卷积网络性能下降问题

    SkipNode: On Alleviating Performance Degradation for Deep Graph Convolutional Networks. (arXiv:2112.11628v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.11628](http://arxiv.org/abs/2112.11628)

    SkipNode提出了一个插拔式模块来缓解深度图卷积网络性能下降问题，通过跳过部分卷积操作效果显著，有效抑制过度平滑和梯度消失。

    

    图卷积网络（GCNs）在模型加深时容易出现性能下降。然而，之前的研究仅将性能退化归因为过度平滑。本文通过理论和实验分析，探讨了深度GCNs性能下降的根本原因：过度平滑和梯度消失有着相互加强的作用，在深度GCNs中导致性能迅速恶化。另一方面，现有的反过度平滑方法都在模型深度上执行完整卷积操作。由于模型深度的增加，它们无法有效抵抗过度平滑的指数收敛。在本文中，我们提出了一个简单而有效的插拔式模块Skipnode，来克服深度GCNs的性能下降问题。它通过在每个卷积层中对图节点进行采样，跳过卷积操作。通过这种方式，可以有效抑制过度平滑和梯度消失，因为（1）不对所有节点执行卷积操作。

    Graph Convolutional Networks (GCNs) suffer from performance degradation when models go deeper. However, earlier works only attributed the performance degeneration to over-smoothing. In this paper, we conduct theoretical and experimental analysis to explore the fundamental causes of performance degradation in deep GCNs: over-smoothing and gradient vanishing have a mutually reinforcing effect that causes the performance to deteriorate more quickly in deep GCNs. On the other hand, existing anti-over-smoothing methods all perform full convolutions up to the model depth. They could not well resist the exponential convergence of over-smoothing due to model depth increasing. In this work, we propose a simple yet effective plug-and-play module, Skipnode, to overcome the performance degradation of deep GCNs. It samples graph nodes in each convolutional layer to skip the convolution operation. In this way, both over-smoothing and gradient vanishing can be effectively suppressed since (1) not all
    
[^197]: 广义的离群检测：一项调查研究

    Generalized Out-of-Distribution Detection: A Survey. (arXiv:2110.11334v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2110.11334](http://arxiv.org/abs/2110.11334)

    广义的离群检测的调查研究探讨了离群检测的重要性及其与异常检测、新颖性检测和开放集识别等问题的联系，对于提高机器学习系统的可靠性和安全性具有关键意义。

    

    离群检测对于确保机器学习系统的可靠性和安全性至关重要。例如，在自动驾驶中，当系统检测到在训练时从未见过且无法作出安全决策的异常场景或对象时，我们希望驾驶系统能发出警报并将控制交给人类。离群检测一词首次出现在2017年，自那以后引起了研究界的越来越多的关注，从基于分类的方法到基于密度和距离的方法，研究方法五花八门。同时，离群检测与异常检测、新颖性检测、开放集识别和异常检测等其他问题密切相关。尽管目标相同，但这些主题在定义和问题设置上有微妙的差异，经常使读者和从业者感到困惑。

    Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen during training time and cannot make a safe decision. The term, OOD detection, first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD), are closely related to OOD detection in terms of motivation and methodology. Despite common goals, these topics develop in isolation, and their subtle differences in definition and problem setting often confuse readers and practitioners. In this su
    
[^198]: DPGNN: 双感知图神经网络用于表示学习

    DPGNN: Dual-Perception Graph Neural Network for Representation Learning. (arXiv:2110.07869v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.07869](http://arxiv.org/abs/2110.07869)

    DPGNN是一种新颖的双感知图神经网络，通过引入多步消息源、节点特定的消息输出和多空间消息交互来增强图神经网络的表达能力。

    

    近年来，图神经网络在许多基于图的任务中引起了越来越多的关注，并在其中取得了显著的性能，尤其是在图上半监督学习方面。然而，大多数现有的图神经网络都基于消息传递范式，在单一拓扑空间中迭代地聚合邻居信息。尽管它们取得了成功，但是图神经网络的表达能力受到一些缺点的限制，例如消息源扩展的不灵活性、忽视节点级消息输出的差异以及单一消息空间的限制。为了解决这些缺点，我们提出了一种新的消息传递范式，基于多步消息源的属性、节点特定的消息输出和多空间消息交互。为了验证其有效性，我们将这种新的消息传递范式实例化为一个名为DPGNN的双感知图神经网络，它应用了一种节点对步骤的注意机制来自适应地聚合节点特定的多步邻域信息。

    Graph neural networks (GNNs) have drawn increasing attention in recent years and achieved remarkable performance in many graph-based tasks, especially in semi-supervised learning on graphs. However, most existing GNNs are based on the message-passing paradigm to iteratively aggregate neighborhood information in a single topology space. Despite their success, the expressive power of GNNs is limited by some drawbacks, such as inflexibility of message source expansion, negligence of node-level message output discrepancy, and restriction of single message space. To address these drawbacks, we present a novel message-passing paradigm, based on the properties of multi-step message source, node-specific message output, and multi-space message interaction. To verify its validity, we instantiate the new message-passing paradigm as a Dual-Perception Graph Neural Network (DPGNN), which applies a node-to-step attention mechanism to aggregate node-specific multi-step neighborhood information adapti
    
[^199]: MNL-带有背包的剧集挑选问题：一种近似最优算法

    MNL-Bandit with Knapsacks: a near-optimal algorithm. (arXiv:2106.01135v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.01135](http://arxiv.org/abs/2106.01135)

    这篇论文介绍了一种解决动态商品选择问题的算法，通过使用近似最优策略，可在未知需求情况下最大化总体预期收入。在大库存环境下，该算法能够接近最优解。

    

    我们考虑一种动态的商品选择问题，其中卖方拥有固定库存的N种可替代产品，并面临在T个时期内顺序到达的未知需求。在每个时期，卖方需要决定要向客户提供的产品组合（基数最多为K）。顾客的反应遵循具有参数v的未知多项式对数模型（MNL）。卖方的目标是在给定固定初始库存的情况下最大化总体预期收入。我们给出了一种策略，达到了$\tilde O\Big(K \sqrt{KN T}\Big(\sqrt{v_{\text{max}}} + \frac{1}{q_{\text{min}}}\text{OPT}\Big)\Big)$的遗憾值，在模型参数的一种温和假设下。特别地，我们的策略在高库存环境下达到了接近最优的$\tilde O(\sqrt{T})$遗憾值。我们的策略基于基于UCB的方法。

    We consider a dynamic assortment selection problem where a seller has a fixed inventory of $N$ substitutable products and faces an unknown demand that arrives sequentially over $T$ periods. In each period, the seller needs to decide on the assortment of products (of cardinality at most $K$) to offer to the customers. The customer's response follows an unknown multinomial logit model (MNL) with parameters $v$. The goal of the seller is to maximize the total expected revenue given the fixed initial inventory of $N$ products. We give a policy that achieves a regret of $\tilde O\Big(K \sqrt{KN T}\Big(\sqrt{v_{\text{max}}} + \frac{1}{q_{\text{min}}}\text{OPT}\Big)\Big)$, where $v_{\text{max}}\leq 1$ is the maximum utility for any product and $q_{\text{min}}$ the minimum inventory level, under a mild assumption on the model parameters. In particular, our policy achieves a near-optimal $\tilde O(\sqrt{T})$ regret in a large-inventory setting.  Our policy builds upon the UCB-based approach for
    
[^200]: 控制与动力学问题的双在线斯坦变分推理

    Dual Online Stein Variational Inference for Control and Dynamics. (arXiv:2103.12890v1 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2103.12890](http://arxiv.org/abs/2103.12890)

    本文提出了一种双在线斯坦变分推理算法，能够实时估计模型参数和控制输入的分布，在复杂环境中适应不确定性。

    

    模型预测控制（MPC）方案在许多具有挑战性的控制任务中表现出积极而稳健的性能，能够应对非线性系统动力学、约束和观测噪声。尽管这些方法取得了成功，但它们通常依赖于简单的控制分布，这可能限制了它们在高度不确定和复杂的环境中的性能。MPC框架必须能够根据最新的测量结果适应系统参数的变化分布。在本文中，我们设计了一种隐式变分推理算法，能够实时估计模型参数和控制输入的分布。该方法利用斯坦变分梯度下降来逼近目标分布，通过贝叶斯形式进行更新。这使得可以近似复杂的多模态后验分布，通常出现在具有挑战性和现实的机器人导航任务中。

    Model predictive control (MPC) schemes have a proven track record for delivering aggressive and robust performance in many challenging control tasks, coping with nonlinear system dynamics, constraints, and observational noise. Despite their success, these methods often rely on simple control distributions, which can limit their performance in highly uncertain and complex environments. MPC frameworks must be able to accommodate changing distributions over system parameters, based on the most recent measurements. In this paper, we devise an implicit variational inference algorithm able to estimate distributions over model parameters and control inputs on-the-fly. The method incorporates Stein Variational gradient descent to approximate the target distributions as a collection of particles, and performs updates based on a Bayesian formulation. This enables the approximation of complex multi-modal posterior distributions, typically occurring in challenging and realistic robot navigation ta
    

