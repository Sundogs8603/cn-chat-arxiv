# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ExtremeCast: Boosting Extreme Value Prediction for Global Weather Forecast](https://rss.arxiv.org/abs/2402.01295) | ExtremeCast提出了一种新的损失函数Exloss，实现了针对极值的准确预测，同时引入了无需训练的极值增强策略ExEnsemble，提高了预报的稳健性 |
| [^2] | [Calib3D: Calibrating Model Preferences for Reliable 3D Scene Understanding](https://arxiv.org/abs/2403.17010) | Calib3D是一个从不确定性估计的角度出发，对多个3D场景理解模型进行了全面评估，发现现有模型虽然准确但不可靠，从而阐明了安全关键的背景下的重要性。 |
| [^3] | [Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows](https://arxiv.org/abs/2403.16995) | 语言校正流是一种基于标准概率流模型的新方法，通过学习常微分方程模型在源分布和目标分布之间传输，提供了统一和有效的生成模型和领域转移解决方案。 |
| [^4] | [Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation](https://arxiv.org/abs/2403.16990) | 该论文研究了多主体文本到图像生成中的有界注意力方法，解决了扩散模型注意力层混合不同主体视觉特征导致的语义泄漏问题。 |
| [^5] | [Dynamic Relative Representations for Goal-Oriented Semantic Communications](https://arxiv.org/abs/2403.16986) | 本文提出了一个新颖的面向目标的语义通信框架，利用相对表示通过潜在空间对齐来缓解语义不匹配，并实现了能效高、延迟低的目标导向语义通信。 |
| [^6] | [Self-STORM: Deep Unrolled Self-Supervised Learning for Super-Resolution Microscopy](https://arxiv.org/abs/2403.16974) | 介绍了深度展开自监督学习用于超分辨率显微镜，通过训练模型自编码器从给定测量中学习，减少对大量训练数据的需求 |
| [^7] | [VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild](https://arxiv.org/abs/2403.16973) | VoiceCraft是一个基于标记填充的神经编解码器语言模型，在语音编辑和零-shot文本到语音任务上表现出色，实现了在多样性数据集上的最新性能。 |
| [^8] | [Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability](https://arxiv.org/abs/2403.16970) | 该论文引入了一种新的深度学习框架，用于联合疾病诊断和胸部X光扫描对应视觉显著性图的预测，通过设计新颖的双编码器多任务UNet并利用多尺度特征融合分类器来提高计算辅助诊断的可解释性和质量。 |
| [^9] | [Visual Whole-Body Control for Legged Loco-Manipulation](https://arxiv.org/abs/2403.16967) | 这项研究提出了一种利用视觉全身控制的框架，使腿式机器人能够同时控制腿部和手臂，以扩展操作能力，并通过仿真训练和Sim2Real转移实现了在捡起不同物体方面取得显著改进。 |
| [^10] | [Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance](https://arxiv.org/abs/2403.16952) | 该研究发现了数据混合规律，可以量化地预测模型性能与数据混合比例之间的关系，并提出了一种方法来通过拟合函数形式来引导理想的数据混合选择，从而优化大型语言模型的训练混合。 |
| [^11] | [Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators](https://arxiv.org/abs/2403.16950) | 在大型语言模型评估中，通过引入成对偏好搜索方法PAIRS，成功解决了LLMs与人类判断不一致的问题，并取得了优于直接打分的最先进性能。 |
| [^12] | [Backpropagation through space, time, and the brain](https://arxiv.org/abs/2403.16933) | 提出了 Generalized Latent Equilibrium (GLE)，它是一种针对神经元网络的物理动态局部时空信用分配的计算框架。 |
| [^13] | [FLIGAN: Enhancing Federated Learning with Incomplete Data using GAN](https://arxiv.org/abs/2403.16930) | FLIGAN利用生成对抗网络（GAN）来生成合成数据，解决联邦学习中不完整数据的问题，提升模型的鲁棒性和完整性。 |
| [^14] | [SCOD: From Heuristics to Theory](https://arxiv.org/abs/2403.16916) | 本文提出了针对SCOD问题的最优策略，包括基于贝叶斯分类器和随机线性分类器的选择器，以解决不确定或离群样本预测可靠性问题。 |
| [^15] | [Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models](https://arxiv.org/abs/2403.16915) | 本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调，在专题文档检索中显著改善了效果。 |
| [^16] | [State Space Models as Foundation Models: A Control Theoretic Overview](https://arxiv.org/abs/2403.16899) | 将状态空间模型整合到深度神经网络架构中，为控制理论家和研究人员提供了一种有效建模动态系统的新途径。 |
| [^17] | [Discrete Latent Graph Generative Modeling with Diffusion Bridges](https://arxiv.org/abs/2403.16883) | GLAD是一个在离散潜在空间上操作的图生成模型，通过适应扩散桥结构学习其离散潜在空间的先验，避免了依赖于原始数据空间的分解，在图生成任务中表现出优越性。 |
| [^18] | [Proprioception Is All You Need: Terrain Classification for Boreal Forests](https://arxiv.org/abs/2403.16877) | 通过引入 BorealTC 数据集，结合现有数据集，我们评估了基于卷积神经网络（CNN）和新颖的状态空间模型（SSM）-Mamba体系结构在北方森林地形分类上的表现。 |
| [^19] | [Conformal Off-Policy Prediction for Multi-Agent Systems](https://arxiv.org/abs/2403.16871) | 这项工作介绍了MA-COPP，这是第一个解决涉及多智能体系统的离策略预测问题的一致预测方法。 |
| [^20] | [INPC: Implicit Neural Point Clouds for Radiance Field Rendering](https://arxiv.org/abs/2403.16862) | 提出了一种新颖的隐式点云表示方法，结合了连续八叉树概率场和多分辨率哈希网格，实现了快速渲染和保留细致几何细节的优势，并且在几个常见基准数据集上实现了最先进的图像质量。 |
| [^21] | [DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts](https://arxiv.org/abs/2403.16861) | DISL数据集包含了514,506个部署在以太坊主网上的独特Solidity文件，成为了开发智能合约设计工具和基准测试的重要资源。 |
| [^22] | [Semantic-Aware Remote Estimation of Multiple Markov Sources Under Constraints](https://arxiv.org/abs/2403.16855) | 研究了具有语义意识的远程多马尔可夫源估计的最优调度策略以及开发了一种新颖的策略搜索算法 Insec-RVI。 |
| [^23] | [Can ChatGPT predict article retraction based on Twitter mentions?](https://arxiv.org/abs/2403.16851) | 本研究探讨了ChatGPT是否能够基于Twitter提及来预测文章的撤回，研究发现在预测未来被撤回的有问题文章方面是具有一定潜力的。 |
| [^24] | [GreeDy and CoDy: Counterfactual Explainers for Dynamic Graphs](https://arxiv.org/abs/2403.16846) | 该论文介绍了两种针对动态图的新颖反事实解释方法：GreeDy和CoDy。实验证明，CoDy在寻找重要反事实输入方面表现优异，成功率高达59%。 |
| [^25] | [Do LLM Agents Have Regret? A Case Study in Online Learning and Games](https://arxiv.org/abs/2403.16843) | 通过研究在线学习和博弈论中的基准决策设置，评估LLM代理的交互行为和性能，以了解它们在多代理环境中的潜力和限制。 |
| [^26] | [Convergence of a model-free entropy-regularized inverse reinforcement learning algorithm](https://arxiv.org/abs/2403.16829) | 提出一个无模型的算法来解决熵正则化的逆强化学习问题，该算法能够使用有限样本恢复出专家表现最佳的奖励，并且最终得到的最优策略与专家策略非常接近。 |
| [^27] | [Weak Convergence Analysis of Online Neural Actor-Critic Algorithms](https://arxiv.org/abs/2403.16825) | 在线神经演员-评论算法中，我们证明当隐藏单元和训练步数的数量$\rightarrow \infty$时，单层神经网络将收敛于随机ODE，通过建立数据样本的几何遍历性和使用泊松方程证明模型更新波动消失，演员神经网络和评论神经网络收敛到具有随机初始条件的ODE系统的解。 |
| [^28] | [Resource and Mobility Management in Hybrid LiFi and WiFi Networks: A User-Centric Learning Approach](https://arxiv.org/abs/2403.16823) | 研究了在混合LiFi和WiFi网络中基于用户中心的负载平衡方法，以解决现有网络中心化方法在处理移动性方面的问题。 |
| [^29] | [Multiple-Source Localization from a Single-Snapshot Observation Using Graph Bayesian Optimization](https://arxiv.org/abs/2403.16818) | 提出了一种基于贝叶斯优化的BOSouL模拟方法，用于从单快照观测中实现多源定位，以解决现有方法在信息有限、源交互作用和扩散模型依赖性方面的限制。 |
| [^30] | [An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems](https://arxiv.org/abs/2403.16809) | 本文提出了一种利用大型语言模型(LLMs)来优化人在回路系统的方法，并通过案例研究展示了在购物中心中模拟各种人群热量偏好的应用。 |
| [^31] | [Cluster-Based Normalization Layer for Neural Networks](https://arxiv.org/abs/2403.16798) | 该论文提出了一种基于聚类的神经网络规范化方法CB-Norm，通过引入高斯混合模型，解决了梯度稳定性和学习加速方面的挑战。 |
| [^32] | [Iso-Diffusion: Improving Diffusion Probabilistic Models Using the Isotropy of the Additive Gaussian Noise](https://arxiv.org/abs/2403.16790) | 利用加性噪声的各向同性作为约束，改进了扩散概率模型的保真度，验证实验表明这种约束的整合显著提高了模型的性能。 |
| [^33] | [The Anatomy of Adversarial Attacks: Concept-based XAI Dissection](https://arxiv.org/abs/2403.16782) | 对抗性攻击对卷积神经网络学到的概念产生了实质性的影响，引入新概念或修改现有概念，并且这种影响可以通过线性分解对扰动进行解释。 |
| [^34] | [Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases](https://arxiv.org/abs/2403.16776) | 使用扩散模型生成形变场，将一般人口图谱转变为特定子人口的图谱，确保结构合理性，避免幻觉。 |
| [^35] | [Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation](https://arxiv.org/abs/2403.16771) | 本文提出了用于鲁棒性混合代码翻译的合成数据生成和联合学习方法，包括开发了Hinglish到英语的平行语料库以及提出的能够处理噪声的联合训练模型RCMT。 |
| [^36] | [DeepKnowledge: Generalisation-Driven Deep Learning Testing](https://arxiv.org/abs/2403.16768) | DeepKnowledge提出了一种基于知识泛化理论的深度学习系统测试方法，旨在提高DNN的稳健性并减少黑匣子模型的风险。 |
| [^37] | [One-Shot Domain Incremental Learning](https://arxiv.org/abs/2403.16707) | 提出了一种处理单次领域增量学习中批归一化层统计数据困难的技术，并展示了其有效性。 |
| [^38] | [Assessing the Performance of Deep Learning for Automated Gleason Grading in Prostate Cancer](https://arxiv.org/abs/2403.16695) | 该研究评估了11种深度神经网络架构在前列腺癌自动格里森分级中的性能，发现ConvNeXt表现最佳，新架构实现了优越性能但在区分密切相关的格里森分级方面存在挑战，为提高格里森分级系统奠定了基础。 |
| [^39] | [Synapse: Learning Preferential Concepts from Visual Demonstrations](https://arxiv.org/abs/2403.16689) | Synapse是一种神经符号化方法，旨在从有限演示中高效学习偏好概念，通过将偏好表示为神经符号程序并利用视觉解析、大型语言模型和程序合成相结合的方式来学习个人偏好。 |
| [^40] | [A note on generalization bounds for losses with finite moments](https://arxiv.org/abs/2403.16681) | 本文研究了损失函数具有有限矩的泛化界，并推导了高概率的PAC-Bayes界，进一步揭示了对损失函数有界方差的情况下界的改进。此外，该研究将结果扩展到期望和单次抽样PAC-Bayes中，并获得了针对有界损失函数的快速速率界。 |
| [^41] | [Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics](https://arxiv.org/abs/2403.16680) | 对称基础卷积提出了使用可分离基础函数的连续卷积的通用公式作为现有方法的超集，并评估了大量基础函数在不同流体模拟中的性能，结果表明偶数和奇数对称性是稳定性和准确性的关键因素。 |
| [^42] | [DeepGleason: a System for Automated Gleason Grading of Prostate Cancer using Deep Neural Networks](https://arxiv.org/abs/2403.16678) | DeepGleason是一个使用深度神经网络自动对前列腺癌进行格里森分级的系统，在tile-wise分类方法和ConvNeXt架构的基础上，提供了一个标准化的工具，并比较了各种最先进的架构。 |
| [^43] | [FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression](https://arxiv.org/abs/2403.16677) | FOOL是一种OEC本地和任务不可知的特征压缩方法，通过最大化吞吐量、嵌入上下文和利用瓷砖间的依赖关系，降低传输成本，同时保持预测性能。 |
| [^44] | [Understanding the Functional Roles of Modelling Components in Spiking Neural Networks](https://arxiv.org/abs/2403.16674) | 系统研究揭示了尖峰神经网络中滤泄、重置和循环等建模组件在平衡记忆保留、时间处理和动态建模方面的功能角色。 |
| [^45] | [Graph Augmentation for Recommendation](https://arxiv.org/abs/2403.16656) | 提出了一个名为GraphAug的框架，通过引入强大的数据增强程序生成去噪的自监督信号，解决了推荐系统中对比学习中存在的数据噪声和GNN架构平滑问题。 |
| [^46] | [A Novel Loss Function-based Support Vector Machine for Binary Classification](https://arxiv.org/abs/2403.16654) | 通过引入新型Slide损失函数（$\ell_s$），我们提出了一种$\ell_s$-SVM分类器，有效地解决了传统SVM在边缘正确分类样本惩罚程度方面的不足，提升了泛化能力。 |
| [^47] | [Bridging the Sim-to-Real Gap with Bayesian Inference](https://arxiv.org/abs/2403.16644) | 提出了SIM-FSVGD方法，通过利用低保真度的物理先验，成功缩小模拟到现实的差距，能够在低数据量的情况下学习准确的动力学，并在实验中展示了在高性能赛车系统上的有效性。 |
| [^48] | [Multi-Scale Texture Loss for CT denoising with GANs](https://arxiv.org/abs/2403.16640) | 该研究提出了一种利用灰度共生矩阵的多尺度纹理损失函数，以帮助生成对抗网络更好地捕捉复杂的图像关系。 |
| [^49] | [A comparative analysis of embedding models for patent similarity](https://arxiv.org/abs/2403.16630) | 本文比较了不同类型的专利嵌入模型在专利相似性计算任务上的表现，并具体探讨了Sentence Transformers (SBERT) 架构在专利相似性任务中的性能。 |
| [^50] | [Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting](https://arxiv.org/abs/2403.16612) | 通过校准贝叶斯UNet++模型，我们可以获得更可靠和更清晰的次季节预测，这对于安全关键的机器学习应用如天气预测员来说尤为重要。 |
| [^51] | [Distributed collaborative anomalous sound detection by embedding sharing](https://arxiv.org/abs/2403.16610) | 提出了一种基于嵌入分享的分布式协作异常声音检测方法，每个客户端使用共同的预训练模型计算嵌入，并在服务器上对其进行聚合，通过异常暴露来执行异常声音检测，平均提高了6.8%的AUC。 |
| [^52] | [Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction and Defect-Focus](https://arxiv.org/abs/2403.16607) | Style Filter是一种在工业领域中量身定制的方法，通过在知识转移之前选择性地过滤源域数据，在减少数据量的同时，保持或提升了迁移学习策略的性能。 |
| [^53] | [EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for Medical Image Segmentation](https://arxiv.org/abs/2403.16594) | 提出了一种专家分歧引导的不确定性估计（EDUE）方法，用于医学图像分割，在训练过程中通过地面实况注释的变异性来引导模型，并采用随机抽样策略以提高校准信心，实现了平均55%和23%的相关性改进。 |
| [^54] | [Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy](https://arxiv.org/abs/2403.16591) | 论文探讨了本地差分隐私、贝叶斯隐私及其之间的相互关系，揭示了关于效用-隐私权衡的新见解，并提出了一个框架来突出攻击和防御策略的相互作用和效果。 |
| [^55] | [In the Search for Optimal Multi-view Learning Models for Crop Classification with Global Remote Sensing Data](https://arxiv.org/abs/2403.16582) | 研究调查了在全球范围内同时选择融合策略和编码器架构对作物分类具有的影响。 |
| [^56] | [Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization](https://arxiv.org/abs/2403.16576) | 通过直接基于能量偏好优化的方法，解决了抗原特异性抗体设计中的蛋白质序列-结构共设计问题，以生成具有理性结构和良好结合亲和力的抗体设计。 |
| [^57] | [NSINA: A News Corpus for Sinhala](https://arxiv.org/abs/2403.16571) | NSINA是为解决僧伽罗语中LLMs适应性挑战而引入的最大新闻语料库，为改进该语言的自然语言处理提供了宝贵资源和基准。 |
| [^58] | [Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors](https://arxiv.org/abs/2403.16569) | 该研究揭示了神经网络在参数学习和对抗中的漏洞，引入了一种针对解释敏感的后门攻击的防御方法，通过统计分析来限制对神经网络模型的攻击。 |
| [^59] | [FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning](https://arxiv.org/abs/2403.16561) | 提出了FedFixer来解决联邦学习中异构标签噪声的问题，引入个性化模型与全局模型合作来有效选择干净的客户端特定样本，通过置信度正则化器和基于样本共享的双模型更新策略来减轻过拟合问题 |
| [^60] | [Accelerating Federated Learning by Selecting Beneficial Herd of Local Gradients](https://arxiv.org/abs/2403.16557) | 提出了BHerd策略，通过选择有益的局部梯度加速联邦学习模型的收敛效率 |
| [^61] | [Differentially Private Online Federated Learning with Correlated Noise](https://arxiv.org/abs/2403.16542) | 提出一种利用相关噪声提高效用并确保隐私的差分隐私在线联邦学习算法，解决了DP噪声和本地更新带来的挑战，并在动态环境中建立了动态遗憾界。 |
| [^62] | [Causal Discovery from Poisson Branching Structural Causal Model Using High-Order Cumulant with Path Analysis](https://arxiv.org/abs/2403.16523) | 从计数数据中发现因果结构的关键挑战在于非可辨识性问题，本研究发现在泊松分支结构因果模型中，如果根顶点$X$是已知的，则可以确定从$X$到其子节点$Y$的因果顺序。 |
| [^63] | [Human Understanding AI Paper Challenge 2024 -- Dataset Design](https://arxiv.org/abs/2403.16509) | 2024年人类理解人工智能论文挑战将提供数据集，涉及人工智能技术以理解人类日常生活的研究和开发，重点考虑数据处理和学习模型的问题 |
| [^64] | [PathoTune: Adapting Visual Foundation Model to Pathological Specialists](https://arxiv.org/abs/2403.16497) | PathoTune提出了一个框架，能够通过多模态提示微调，将病理甚至视觉基础模型高效地调整到病理特定任务，从而缓解基础-任务差距和任务-实例差距。 |
| [^65] | [LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural Network for Traffic Flow Forecasting](https://arxiv.org/abs/2403.16495) | LSTTN框架综合考虑历史交通流量中的长期和短期特征，通过掩码子序列Transformer解决了现有STGNNs模型只能利用短程交通流量数据的限制，实现了对交通流量复杂趋势和周期特征的充分学习 |
| [^66] | [Determined Multi-Label Learning via Similarity-Based Prompt](https://arxiv.org/abs/2403.16482) | 提出了一种名为“Determined Multi-Label Learning”（DMLL）的新的标注设置，旨在有效减少多标签任务中固有的标注成本。 |
| [^67] | [Learning from Reduced Labels for Long-Tailed Data](https://arxiv.org/abs/2403.16469) | 提出了一种名为Reduced Label的新型弱监督标签设置，能够高效地学习长尾数据，避免了尾部样本监督信息的下降，降低了标签成本 |
| [^68] | [Training Generative Adversarial Network-Based Vocoder with Limited Data Using Augmentation-Conditional Discriminator](https://arxiv.org/abs/2403.16464) | 本文提出使用增强条件鉴别器在有限数据下训练生成对抗网络声码器，以避免数据采集成本高的问题，解决了标准鉴别器对增强数据的不敏感性，提高了模型的泛化能力。 |
| [^69] | [FedAC: A Adaptive Clustered Federated Learning Framework for Heterogeneous Data](https://arxiv.org/abs/2403.16460) | FedAC框架通过解耦神经网络，使用不同聚合方法为每个子模块提供全局知识，引入经济高效的在线模型相似度度量，及集群数量微调模块，显著提高了性能。 |
| [^70] | [On the rates of convergence for learning with convolutional neural networks](https://arxiv.org/abs/2403.16459) | 该研究提出了对具有一定权重约束的CNNs的新逼近上界，以及对前馈神经网络的覆盖数做了新的分析，为基于CNNs的学习问题推导了收敛速率，并在学习平滑函数和二元分类方面取得了极小最优的结果。 |
| [^71] | [DeepMachining: Online Prediction of Machining Errors of Lathe Machines](https://arxiv.org/abs/2403.16451) | DeepMachining是一种基于深度学习的AI系统，可以在线预测车床机床加工操作的误差，通过预训练和微调模型，实现了高准确性预测，是首批使用预训练深度学习模型预测车床机床加工误差的工厂实验之一。 |
| [^72] | [If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions](https://arxiv.org/abs/2403.16442) | 通过新颖的Extract and Explore（EX2）方法，研究发现在视觉-语言模型（VLM）中，重要的特征描述包括非视觉属性，虚假描述影响VLM表示，不同的VLM优先考虑不同的内容。 |
| [^73] | [Producing and Leveraging Online Map Uncertainty in Trajectory Prediction](https://arxiv.org/abs/2403.16439) | 通过扩展现有的在线地图估计方法以额外估计不确定性，能够更好地将在线制图与轨迹预测集成，从而在训练和预测性能上取得显著改善。 |
| [^74] | [An incremental MaxSAT-based model to learn balanced rules](https://arxiv.org/abs/2403.16418) | 提出了一种基于递增式MaxSAT的学习平衡规则模型IMLIB，结合了SAT和MaxSAT方法，限制规则大小以实现平衡，并提高模型性能。 |
| [^75] | [Ensemble Adversarial Defense via Integration of Multiple Dispersed Low Curvature Models](https://arxiv.org/abs/2403.16405) | 通过降低攻击传递性来增强集成多样性，二阶梯度作为对抗性鲁棒性的关键因素。 |
| [^76] | [Rethinking the Representation in Federated Unsupervised Learning with Non-IID Data](https://arxiv.org/abs/2403.16398) | 提出了FedU2方法，通过灵活的统一正则化器（FUR）和高效的统一聚合器（EUA），增强了在具有非IID数据的FUSL中生成统一和统一表示。 |
| [^77] | [Concurrent Linguistic Error Detection (CLED) for Large Language Models](https://arxiv.org/abs/2403.16393) | 提出了一种针对大型语言模型的并发语言错误检测方案，通过提取文本的语言特征并使用分类器进行错误检测。 |
| [^78] | [Physics-informed RL for Maximal Safety Probability Estimation](https://arxiv.org/abs/2403.16391) | 利用物理信息强化学习算法可以估计最大安全行动的长期安全概率，避免风险过度逼近导致的保守行为 |
| [^79] | [Real-time Adaptation for Condition Monitoring Signal Prediction using Label-aware Neural Processes](https://arxiv.org/abs/2403.16377) | 提出了一种神经过程方法，可以在实时条件监测信号预测中实现表示能力和敏捷性的权衡 |
| [^80] | [ProIn: Learning to Predict Trajectory Based on Progressive Interactions for Autonomous Driving](https://arxiv.org/abs/2403.16374) | 通过渐进交互网络，代理在学习中逐渐聚焦于相关地图，以更好地捕获相关地图约束的特征表示。 |
| [^81] | [SignSGD with Federated Voting](https://arxiv.org/abs/2403.16372) | signSGD-FV是一种具有联邦投票的优化器，通过在线学习边缘设备的权重分配以克服SignSGD-MV在小批量大小不同的工作者间收敛困难的问题。 |
| [^82] | [Learning Action-based Representations Using Invariance](https://arxiv.org/abs/2403.16369) | 提出了一种新的方法，动作双模拟编码，通过递归不变性约束扩展了单步控制性，学习了一个可以平滑折扣远期元素的多步控制度量 |
| [^83] | [Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion](https://arxiv.org/abs/2403.16365) | 使用引导扩散生成的基础样本能产生更强大的毒药和后门，优于先前最先进的攻击。 |
| [^84] | [ChatDBG: An AI-Powered Debugging Assistant](https://arxiv.org/abs/2403.16354) | ChatDBG是第一个AI-Powered调试助手，通过将大型语言模型集成到传统调试器中，实现了程序员与调试器之间的协作对话，能够处理复杂问题、执行根本原因分析，并探索开放性查询。 |
| [^85] | [ChatGPT Incorrectness Detection in Software Reviews](https://arxiv.org/abs/2403.16347) | 开发了一种名为CID的工具，通过迭代提示ChatGPT并要求提出情境相似但文本有分歧的问题，来自动测试和检测ChatGPT响应中的不正确性。 |
| [^86] | [Predictive Inference in Multi-environment Scenarios](https://arxiv.org/abs/2403.16336) | 本研究提出了在多环境预测问题中构建有效置信区间和置信集的方法，并展示了一种新的调整方法以适应问题难度，从而减少预测集大小，这在神经感应和物种分类数据集中的实际表现中得到验证。 |
| [^87] | [MEDDAP: Medical Dataset Enhancement via Diversified Augmentation Pipeline](https://arxiv.org/abs/2403.16335) | MEDDAP通过稳定扩散（SD）模型自动生成新的信息标记样本，提升医疗数据集的效用 |
| [^88] | [Graphs Generalization under Distribution Shifts](https://arxiv.org/abs/2403.16334) | 本文介绍了一种名为GLIDER的新框架，旨在解决图结构数据中分布转移带来的挑战，并实现泛化性能优越。 |
| [^89] | [Modeling Analog Dynamic Range Compressors using Deep Learning and State-space Models](https://arxiv.org/abs/2403.16331) | 通过分析模拟原型，使用深度学习和状态空间模型开发了模拟动态范围压缩器的逼真数字模型，具有更高的效率和更少的参数。 |
| [^90] | [Artificial Neural Microcircuits as Building Blocks: Concept and Challenges](https://arxiv.org/abs/2403.16327) | 本文探索了一种新的方法，在生物学中神经微电路的启发下，使用人工神经微电路作为组装大型神经网络的基本构建模块，避免了结构上的齐质化所带来的复杂训练和学习工具的问题。 |
| [^91] | [Optimization on a Finer Scale: Bounded Local Subgradient Variation Perspective](https://arxiv.org/abs/2403.16317) | 该研究在有界局部次梯度变化的条件下研究非光滑优化问题，提出的目标函数类能帮助更好理解传统优化问题的复杂性，并在一般情况下降低oracle复杂度。 |
| [^92] | [Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling](https://arxiv.org/abs/2403.16293) | 该研究提出了一个名为IRL的框架，通过将DNN解释为决策树，解决了DRL调度中深度神经网络缺乏可解释性的问题。 |
| [^93] | [The Evolution of Football Betting- A Machine Learning Approach to Match Outcome Forecasting and Bookmaker Odds Estimation](https://arxiv.org/abs/2403.16282) | 本研究利用机器学习算法预测英超联赛的比赛结果，通过分析历史数据和研究各种特征的意义，确定最有效的预测因素。 |
| [^94] | [Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble](https://arxiv.org/abs/2403.16260) | 通过引入新颖的定性和定量模型集成评估方法，作者揭示了现有集成方法的关键缺陷，提出了提高传统模型集成维度的方法，以克服特征表示中的多样性限制。 |
| [^95] | [Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis](https://arxiv.org/abs/2403.16258) | 采用非各向同性扩散模型和新颖的熵模型在神经图像压缩中，以区分频率内容并加速熵解码步骤，提高生成高质量图像的能力。 |
| [^96] | [Improving Sequence-to-Sequence Models for Abstractive Text Summarization Using Meta Heuristic Approaches](https://arxiv.org/abs/2403.16247) | 改进使用元启发方法的序列到序列模型，以提高抽象文本摘要的准确性和有效性 |
| [^97] | [Partially Blinded Unlearning: Class Unlearning for Deep Networks a Bayesian Perspective](https://arxiv.org/abs/2403.16246) | 该研究提出了一种针对预训练分类网络中特定类别数据的目的性消除方法，以降低模型对未学习数据类别的性能影响，同时最小化其他类别性能的不利影响。 |
| [^98] | [On the Equivalency, Substitutability, and Flexibility of Synthetic Data](https://arxiv.org/abs/2403.16244) | 本研究从经验的角度探讨了合成数据在解决实际问题中的有效性，着重研究了合成数据与真实数据的等效性、可替代性以及生成器的灵活性。 |
| [^99] | [An early warning indicator trained on stochastic disease-spreading models with different noises](https://arxiv.org/abs/2403.16233) | 使用深度学习算法训练在受噪声影响的疾病传播模型上的早期预警指标，以提供传染病爆发的有效预警信号。 |
| [^100] | [CoverUp: Coverage-Guided LLM-Based Test Generation](https://arxiv.org/abs/2403.16218) | CoverUp通过覆盖率分析和大型语言模型相结合的方式，驱动生成高覆盖率的Python回归测试，并在改进覆盖率方面取得显著成就。 |
| [^101] | [Systematic construction of continuous-time neural networks for linear dynamical systems](https://arxiv.org/abs/2403.16215) | 提出了一种系统化构建神经网络架构的方法，用于模拟线性定常系统，并提出了一种无梯度算法，直接从给定的系统中计算稀疏的网络架构和参数，具有水平隐藏层的新型神经网络架构范式 |
| [^102] | [Leveraging Deep Learning and Xception Architecture for High-Accuracy MRI Classification in Alzheimer Diagnosis](https://arxiv.org/abs/2403.16212) | 这项研究旨在利用深度学习模型对MRI图像进行分类，通过一系列创新的数据处理和模型构建步骤，识别不同阶段的阿尔茨海默病。 |
| [^103] | [Convergence analysis of OT-Flow for sample generation](https://arxiv.org/abs/2403.16208) | 本文旨在为OT-Flow这一深度生成模型建立一些收敛结果，包括在正则化项参数趋于无穷大时，OT-Flow与最优输运问题的收敛关系，以及随着样本数趋于无穷大时，离散损失函数与连续损失函数之间的收敛关系。 |
| [^104] | [From Discrete to Continuous: Deep Fair Clustering With Transferable Representations](https://arxiv.org/abs/2403.16201) | 提出一种能够同时处理离散和连续敏感属性的深度公平聚类方法，通过信息瓶颈风格的目标函数学习公平且有利于聚类的表示 |
| [^105] | [Logic-based Explanations for Linear Support Vector Classifiers with Reject Option](https://arxiv.org/abs/2403.16190) | 提出了一种基于逻辑的方法，针对具有拒绝选项的线性SVC，提供了正确性和极小性保证的解释，相比启发式算法Anchors，我们的方法给出了更短的解释 |
| [^106] | [Subspace Defense: Discarding Adversarial Perturbations by Learning a Subspace for Clean Signals](https://arxiv.org/abs/2403.16176) | 通过学习一个仅存在干净信号特征的子空间并丢弃扰动特征，使得深度神经网络能够更好地区分对抗性示例。 |
| [^107] | [An Analytic Solution to Covariance Propagation in Neural Networks](https://arxiv.org/abs/2403.16163) | 该论文提出了一种无需样本的矩传播技术，能够准确表征神经网络的输入输出分布，其关键创新在于提供了通过非线性激活函数传递的随机变量协方差的解析解。 |
| [^108] | [One Masked Model is All You Need for Sensor Fault Detection, Isolation and Accommodation](https://arxiv.org/abs/2403.16153) | 提出了一种使用掩盖模型和自监督学习进行传感器故障检测、隔离和容错的新框架，通过训练过程中创建随机掩码来统一找到并纠正故障传感器，有效性在公共数据集和实际风力涡轮数据集上得到验证。 |
| [^109] | [A Survey on Consumer IoT Traffic: Security and Privacy](https://arxiv.org/abs/2403.16149) | 本调查针对消费者物联网（CIoT）流量分析从安全和隐私的角度出发，总结了CIoT流量分析的新特征、最新进展和挑战，认为通过流量分析可以揭示CIoT领域中的安全和隐私问题。 |
| [^110] | [Predicting Energy Budgets in Droplet Dynamics: A Recurrent Neural Network Approach](https://arxiv.org/abs/2403.16144) | 该研究应用循环神经网络方法，特别是LSTM模型，来预测受表面张力影响的流体流动中的能量预算。 |
| [^111] | [CFAT: Unleashing TriangularWindows for Image Super-resolution](https://arxiv.org/abs/2403.16143) | 提出了一种 CFAT 模型，引入了非重叠三角窗口技术，与矩形窗口技术结合，用于图像超分辨率，以减少边界失真并增加独特移位模式。 |
| [^112] | [A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A Knowledge-Based Perspective](https://arxiv.org/abs/2403.16137) | 该论文从基于知识的角度全面调查和分析了图基础模型的自监督预训练任务，涉及微观和宏观知识，包括9个知识类别、25个预训练任务以及各种下游任务适应策略。 |
| [^113] | [Complementary Recommendation in E-commerce: Definition, Approaches, and Future Directions](https://arxiv.org/abs/2403.16135) | 本文全面总结和比较了电子商务领域中34项代表性互补推荐研究，包括建模产品之间的互补关系，不同研究问题下的模型分类与比较，以及在同一数据集上进行的实验结果分析。 |
| [^114] | [SSHPool: The Separated Subgraph-based Hierarchical Pooling](https://arxiv.org/abs/2403.16133) | 提出了基于分隔子图的分层池化（SSHPool）方法，通过将节点分配到不同的簇并利用局部图卷积单元进行压缩，解决了现有图神经网络中的过度平滑问题，有效提取了原始图结构的分层全局特征。 |
| [^115] | [Runtime Monitoring and Fault Detection for Neural Network-Controlled Systems](https://arxiv.org/abs/2403.16132) | 本文设计了稳定的区间观测器，用于增强受神经网络控制的非线性系统的运行时安全性，并能监测系统的安全性并检测故障。 |
| [^116] | [AKBR: Learning Adaptive Kernel-based Representations for Graph Classification](https://arxiv.org/abs/2403.16130) | 提出了一种用于图分类的自适应核表示学习模型，通过特征通道注意机制捕捉不同子结构之间的相互依赖关系，有效识别结构重要性，并计算与重要子结构相关的图对之间的R-卷积核。 |
| [^117] | [A Codesign of Scheduling and Parallelization for Large Model Training in Heterogeneous Clusters](https://arxiv.org/abs/2403.16125) | 在异构集群中，本文提出了Crius，一个用于有效调度多个大型模型的训练系统，引入了称为"Cell"的新调度粒度，以解决集群调度中低开销和准确性能数据获取之间的矛盾。 |
| [^118] | [Self-Supervised Multi-Frame Neural Scene Flow](https://arxiv.org/abs/2403.16116) | 通过研究表明，Neural Scene Flow Prior (NSFP)的性能与输入点云的数量呈反比关系，因此我们提出了一种简单而有效的多帧点云场景流估计方法。 |
| [^119] | [Opportunities and challenges in the application of large artificial intelligence models in radiology](https://arxiv.org/abs/2403.16112) | 本文介绍了大型人工智能模型在放射学中的应用，总结了其在放射学教育、报告生成和影像应用方面的最新研究进展，并指出了大型AI模型在放射学中面临的挑战，旨在推动该领域的快速革命。 |
| [^120] | [A Transformer approach for Electricity Price Forecasting](https://arxiv.org/abs/2403.16108) | 这种独特的Transformer模型在电力价格预测中取得了更好的表现，为可靠和可持续的电力系统运行提供了有前景的解决方案。 |
| [^121] | [A Multi-Label Dataset of French Fake News: Human and Machine Insights](https://arxiv.org/abs/2403.16099) | 通过建立一份包括 100 篇文档的多标签数据集 OBSINFOX，研究了人类与机器在认定假新闻特征上的差异，并发现了语料库中讽刺文本的普遍存在。 |
| [^122] | [LLMs as Compiler for Arabic Programming Language](https://arxiv.org/abs/2403.16087) | 本文介绍了APL（阿拉伯编程语言），它使用LLM作为半编译器，将阿拉伯文本代码转换为Python代码并运行，构建了完整的流水线。 |
| [^123] | [IBCB: Efficient Inverse Batched Contextual Bandit for Behavioral Evolution History](https://arxiv.org/abs/2403.16075) | 提出了一种逆批处理上下文强化学习（IBCB）框架，可以高效地根据专家的行为演变历史对环境奖励参数和学习策略进行估计。 |
| [^124] | [Manifold Regularization Classification Model Based On Improved Diffusion Map](https://arxiv.org/abs/2403.16059) | 本文提出了基于改进扩散映射的流形正则化分类模型，通过改进标签传播模型，有效克服了原始流形正则化模型在局部区域性能上的限制。 |
| [^125] | [Enhancing Demand Prediction in Open Systems by Cartogram-aided Deep Learning](https://arxiv.org/abs/2403.16049) | 提出了一种利用卡托图方法的深度学习框架，可预测未安装数据的站点需求并实现长期预测。 |
| [^126] | [Node Classification via Semantic-Structural Attention-Enhanced Graph Convolutional Networks](https://arxiv.org/abs/2403.16033) | 该论文提出了一种名为语义-结构注意增强图卷积网络（SSA-GCN），能够同时模拟图结构并从知识图谱和复杂网络的角度提取无监督特征，以提升节点分类性能。 |
| [^127] | [Learning Directed Acyclic Graphs from Partial Orderings](https://arxiv.org/abs/2403.16031) | 本文针对当可用部分因果顺序变量时学习DAGs的中间问题，提出了一个通用估计框架，并展示了有效的估计算法。 |
| [^128] | [VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections](https://arxiv.org/abs/2403.16030) | 通过为每个节点分配由个性化PageRank（PPR）采样的令牌列表，然后仅在此列表上应用标准多头自注意力来计算其节点表示，来解决图变换器小批量训练中的样本限制问题。 |
| [^129] | [Exploring the Impact of Dataset Bias on Dataset Distillation](https://arxiv.org/abs/2403.16028) | 针对数据集精简中的数据集偏差问题，我们进行了首次系统调查与探索，构建并评估了两个偏见数据集，为未来研究提供了基础。 |
| [^130] | [A Unified Module for Accelerating STABLE-DIFFUSION: LCM-LORA](https://arxiv.org/abs/2403.16024) | 论文研究了用于加速稳定扩散过程的统一模块，重点关注lcm-lora模块，提供了无条件稳定扩散合成加速方法的理论基础和数值结果。 |
| [^131] | [A Federated Parameter Aggregation Method for Node Classification Tasks with Different Graph Network Structures](https://arxiv.org/abs/2403.16004) | 提出了一种适用于具有不同图网络结构的节点分类任务的联邦参数聚合方法FLGNN，并验证了其有效性。同时，设计了隐私安全的成员推理攻击实验和差分隐私防御实验。 |
| [^132] | [Near-Optimal differentially private low-rank trace regression with guaranteed private initialization](https://arxiv.org/abs/2403.15999) | 该论文研究了在痕迹回归模型下以高斯测量矩阵进行差分隐私估计，提出了具有保证私有初始化的近最优算法，引入了一个高效的DP初始化算法和基于Riemannian优化的差分隐私算法，同时讨论了估计结果的非平凡差距。 |
| [^133] | [Knowledge-guided Machine Learning: Current Trends and Future Prospects](https://arxiv.org/abs/2403.15989) | 知识引导的机器学习（KGML）结合科学知识和数据在机器学习框架中，以实现更好的泛化能力、科学一致性和结果可解释性。 |
| [^134] | [CBGT-Net: A Neuromimetic Architecture for Robust Classification of Streaming Data](https://arxiv.org/abs/2403.15974) | CBGT-Net是一种受CBGT回路启发的神经网络模型，通过积累足够的证据后才对流数据产生分类决策，在图像分类任务中表现出更高的准确性和稳健性。 |
| [^135] | [Detection of Problem Gambling with Less Features Using Machine Learning Methods](https://arxiv.org/abs/2403.15962) | 提出了一种使用更少的特征进行问题赌博检测的深度神经网络模型，并发现在不影响性能的情况下可以将特征从102个减少到5个。 |
| [^136] | [Understanding The Effectiveness of Lossy Compression in Machine Learning Training Sets](https://arxiv.org/abs/2403.15953) | 深入探究损失压缩对模型质量的影响，发现现代损失压缩方法可以在保证质量损失在1%以下的情况下实现50-100倍的压缩比提升。 |
| [^137] | [Explore until Confident: Efficient Exploration for Embodied Question Answering](https://arxiv.org/abs/2403.15941) | 通过利用大型视觉-语言模型的语义推理能力，结合深度信息和视觉提示，提出了一种方法来解决具身问答中的有效探索和回答问题的挑战 |
| [^138] | [LlamBERT: Large-scale low-cost data annotation in NLP](https://arxiv.org/abs/2403.15938) | LlamBERT是一种利用大规模语言模型注释未标记数据库并用于微调变压器编码器的混合方法，在降低成本的同时略微牺牲准确性。 |
| [^139] | [Sample and Communication Efficient Fully Decentralized MARL Policy Evaluation via a New Approach: Local TD update](https://arxiv.org/abs/2403.15935) | 通过引入本地TD更新方法，该研究提出了一种新的方法来降低完全分散式MARL策略评估中的通讯和样本复杂性。 |
| [^140] | [Understanding Domain-Size Generalization in Markov Logic Networks](https://arxiv.org/abs/2403.15933) | 本文量化了马尔科夫逻辑网络在不同大小领域间内部一致性缺失的问题，并提出最大化数据对数似然同时最小化参数方差的方式来优化领域大小泛化。 |
| [^141] | [Safe Reinforcement Learning for Constrained Markov Decision Processes with Stochastic Stopping Time](https://arxiv.org/abs/2403.15928) | 提出了一种在线强化学习算法，用于带有安全性约束的马尔可夫决策过程，能够学习到安全的最优策略，同时提出了计算安全基线策略的方法，证明了算法的有效性，并展示了通过定义代理集实现有效探索。 |
| [^142] | [Deep Gaussian Covariance Network with Trajectory Sampling for Data-Efficient Policy Search](https://arxiv.org/abs/2403.15908) | 结合轨迹抽样和深度高斯协方差网络，以提高基于模型的强化学习问题的数据高效性。 |
| [^143] | [Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices](https://arxiv.org/abs/2403.15905) | 提出了面向资源受限设备的低能耗自适应个性化框架目标块微调，根据数据漂移类型微调不同模块以实现最佳性能和降低能源消耗。 |
| [^144] | [Leveraging Zero-Shot Prompting for Efficient Language Model Distillation](https://arxiv.org/abs/2403.15886) | 通过利用零-shot提示来引出教师模型的理由，减少手工制作的少-shot示例的必要性，并降低所需的总记号数，这直接转化为成本节约。 |
| [^145] | [Fast and Unified Path Gradient Estimators for Normalizing Flows](https://arxiv.org/abs/2403.15881) | 提出了一种快速路径梯度估计器，显著提高了计算效率，并适用于所有实用的归一化流架构，具有正则化效果并减小了方差。 |
| [^146] | [Initialisation and Topology Effects in Decentralised Federated Learning](https://arxiv.org/abs/2403.15855) | 分散式联邦学习的有效性受到连接设备网络拓扑结构的显著影响，我们提出了基于底层网络节点特征向量中心性分布的改进神经网络初始化策略，大大提高了训练效率。 |
| [^147] | [TablePuppet: A Generic Framework for Relational Federated Learning](https://arxiv.org/abs/2403.15839) | TablePuppet提出了关系数据联邦学习（RFL）的通用框架，使用连接中的学习（LoJ）和联合中的学习（LoU）来处理分布式关系表中的训练数据 |
| [^148] | [Centered Masking for Language-Image Pre-Training](https://arxiv.org/abs/2403.15837) | 使用中心掩蔽的GLIP技术在语言-图像预训练中取代了随机掩蔽，利用高斯分布提高了性能，并且易于获得且适用于不具有明显中心焦点的数据集。 |
| [^149] | [Scaling Learning based Policy Optimization for Temporal Tasks via Dropout](https://arxiv.org/abs/2403.15826) | 本文介绍了一种基于模型的方法用于训练在高度非线性环境中运行的自主智能体的反馈控制器，通过对任务进行形式化表述，实现对特定任务目标的定量满足语义，并利用前馈神经网络学习反馈控制器。 |
| [^150] | [Carbon Intensity-Aware Adaptive Inference of DNNs](https://arxiv.org/abs/2403.15824) | 通过基于碳排放强度的自适应模型选择，本研究提出的方法能够在低碳强度时段使用更高准确性的模型，在高碳强度时段使用更低准确性的模型，有效改善视觉识别服务的准确性，最多提高碳排放效率达80%。 |
| [^151] | [Efficient Data Access Paths for Mixed Vector-Relational Search](https://arxiv.org/abs/2403.15807) | 提出了针对高效混合向量-关系搜索的替代数据访问路径设计和优化方法。 |
| [^152] | [Understanding Emergent Abilities of Language Models from the Loss Perspective](https://arxiv.org/abs/2403.15796) | 本文从损失角度重新定义了语言模型的突现能力，发现具有相同预训练损失的模型在不同任务上表现相似，而当预训练损失低于特定阈值时，模型将展现出突现能力。 |
| [^153] | [Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled Autoencoder for Mixed Tabular Datasets](https://arxiv.org/abs/2403.15790) | 这里是中文总结出的一句话要点: 本文针对表格数据领域中自监督学习中的数据不平衡挑战，提出了一种用于混合表格数据集的缩放自动编码器，填补了研究中的缺口。 |
| [^154] | [A Fairness-Oriented Reinforcement Learning Approach for the Operation and Control of Shared Micromobility Services](https://arxiv.org/abs/2403.15780) | 本研究介绍了一种在共享微移动服务运营与控制中实现性能优化和算法公平性平衡的前沿调查，利用Q-Learning算法确保方法稳健，能够实现各种站点类别之间的公平结果。 |
| [^155] | [Supervised Learning via Ensembles of Diverse Functional Representations: the Functional Voting Classifier](https://arxiv.org/abs/2403.15778) | 这份论文着重探讨了功能数据的集成学习，并展示了如何利用不同的功能数据表示训练集成成员，以及如何通过多数投票组合基模型预测。 |
| [^156] | [FusionINN: Invertible Image Fusion for Brain Tumor Monitoring](https://arxiv.org/abs/2403.15769) | FusionINN引入了一种新颖的可逆图像融合框架，可以高效生成融合图像，并解开融合过程的逆向分解，保证无损的像素映射。 |
| [^157] | [BEND: Bagging Deep Learning Training Based on Efficient Neural Network Diffusion](https://arxiv.org/abs/2403.15766) | 本文提出了一种基于神经网络扩散模型的Bagging深度学习训练算法（BEND），有效构建了多个基本分类器，简单而有效。 |
| [^158] | [User-Side Realization](https://arxiv.org/abs/2403.15757) | 用户端实现为用户提供了积极的解决方案，通过在用户端运行通用算法来解决常见问题，无需服务提供商改变服务本身。 |
| [^159] | [Horoballs and the subgradient method](https://arxiv.org/abs/2403.15749) | 该论文提出了一种在Hadamard空间中进行凸优化的迭代方法，与传统假设不同，其复杂性不依赖于空间曲率的下界。 |
| [^160] | [On the Fragility of Active Learners](https://arxiv.org/abs/2403.15744) | 本研究发现主动学习技术只在特定情境下有效，对文本分类从业者的建议是选择适当的文本表示和分类器同样重要。 |
| [^161] | [Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large Language Models](https://arxiv.org/abs/2403.15740) | 通过在文档中插入个人密码并识别生成内容中的“幽灵句子”，普通用户可以确认大型语言模型是否滥用其数据，从而实现数据版权保护。 |
| [^162] | [Space Group Informed Transformer for Crystalline Materials Generation](https://arxiv.org/abs/2403.15734) | CrystalFormer是一种基于变压器的自回归模型，专门设计用于受空间群控制的晶体材料生成，它通过预测单位胞中对称不等价原子的种类和位置来生成晶体，并在有效性、新颖性和稳定性等方面达到了与最先进性能相匹配的水平。 |
| [^163] | [Convection-Diffusion Equation: A Theoretically Certified Framework for Neural Networks](https://arxiv.org/abs/2403.15726) | 本文探讨了神经网络的偏微分方程模型，并提出了基于对流扩散方程的神经网络框架及结构设计，在理论上认证了神经网络的数学基础，实验证实了提出模型的性能。 |
| [^164] | [Ev-Edge: Efficient Execution of Event-based Vision Algorithms on Commodity Edge Platforms](https://arxiv.org/abs/2403.15717) | 提出了 Ev-Edge 框架，通过三个关键优化措施提高在商品边缘平台上执行基于事件的视觉算法的性能 |
| [^165] | [Identifiable Latent Neural Causal Models](https://arxiv.org/abs/2403.15711) | 该研究确定了在潜在附加噪声模型背景下导致可识别性的分布变化类型的充分且必要条件，同时提出了当只有部分分布变化满足条件时的部分可识别性结果。 |
| [^166] | [Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs](https://arxiv.org/abs/2403.15707) | 介绍了新的Dynamic Signal Distribution (DSD)分类任务，模拟图像由$k$个维度为$d$的补丁组成，以解决CNNs相对于LCNs和FCNs的统计优势问题 |
| [^167] | [G-ACIL: Analytic Learning for Exemplar-Free Generalized Class Incremental Learning](https://arxiv.org/abs/2403.15706) | 在这项研究中，我们提出了一种面向非范例化的广义分析类增量学习，通过采用分析学习并提供了对GCIL情景的分析解决方案，有效地解决了模型快速遗忘和数据隐私侵犯问题。 |
| [^168] | [Group Benefits Instances Selection for Data Purification](https://arxiv.org/abs/2403.15694) | 提出了一种名为GRIP的方法，通过群体正则化策略估计类别软标签以提高噪声鲁棒性，减少对嘈杂标签的过拟合并学习类间相似性以改善分类结果。 |
| [^169] | [EAGLE: A Domain Generalization Framework for AI-generated Text Detection](https://arxiv.org/abs/2403.15690) | EAGLE提出了一个领域泛化框架，能够利用从旧语言模型中获得的标记数据，学习特征的不变性，从而检测出未知目标生成器生成的文本。 |
| [^170] | [Differentiable Information Bottleneck for Deterministic Multi-view Clustering](https://arxiv.org/abs/2403.15681) | 提出了一种新的可微分信息瓶颈（DIB）方法，用于确定性多视角聚类，可以通过拟合相互信息来提供分析性解决方案，避免了变分逼近的需要。 |
| [^171] | [The Effectiveness of Local Updates for Decentralized Learning under Data Heterogeneity](https://arxiv.org/abs/2403.15654) | 通过在分散式学习中引入多个本地更新步骤，可以降低通信复杂度，从而在数据异质性低且网络高度连通时有效降低通信成本。 |
| [^172] | [Parametric Encoding with Attention and Convolution Mitigate Spectral Bias of Neural Partial Differential Equation Solvers](https://arxiv.org/abs/2403.15652) | 引入了Parametric Grid Convolutional Attention Networks（PGCANs），通过使用参数化编码器和注意力来解决偏微分方程系统，提高解决器的泛化性能和信息传播速率 |
| [^173] | [Differentially Private Next-Token Prediction of Large Language Models](https://arxiv.org/abs/2403.15638) | 提出了Private Mixing of Ensemble Distributions (PMixED)：通过将模型的输出分布投影到公共LLM的输出分布周围的集合上，并采样平均来实现实际的下一个标记预测，以更轻量化的方式实现对隐私敏感的大型语言模型的预测。 |
| [^174] | [Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization](https://arxiv.org/abs/2403.15605) | 引入了一种新颖的FedDG架构方法gPerXAN，通过规范化方案和引导正则化器配合工作，实现了个性化显式组装规范化，有助于客户端模型对领域特征进行有选择性过滤。 |
| [^175] | [An ensemble of data-driven weather prediction models for operational sub-seasonal forecasting](https://arxiv.org/abs/2403.15598) | 通过使用数据驱动天气预测模型的多模型集成方法，可以实现接近最新技术水平的 sub-seasonal 至 seasonal 天气预测。 |
| [^176] | [Analyzing Male Domestic Violence through Exploratory Data Analysis and Explainable Machine Learning Insights](https://arxiv.org/abs/2403.15594) | 该研究是关于在孟加拉国背景下对男性家庭暴力进行开创性探索，揭示了男性受害者的存在、模式和潜在因素，填补了现有文献对男性受害者研究空白的重要性。 |
| [^177] | [FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in RKHSs](https://arxiv.org/abs/2403.15593) | FairerCLIP提出了一种在RKHSs中使用函数去除CLIP的零样本预测偏见的通用方法，使得预测更公平且更能抵抗虚假相关性。 |
| [^178] | [Data-centric Prediction Explanation via Kernelized Stein Discrepancy](https://arxiv.org/abs/2403.15576) | 该论文提出了一种基于内核化斯坦不相容性的数据中心预测解释方法，通过利用内核函数识别提供最佳预测支持给测试点的训练样本，取得了优异性能。 |
| [^179] | [Do not trust what you trust: Miscalibration in Semi-supervised Learning](https://arxiv.org/abs/2403.15567) | SSL方法基于伪标签存在显著的误校准问题，通过最小化最小熵和引入一个惩罚项，可以缓解这一问题。 |
| [^180] | [A2DMN: Anatomy-Aware Dilated Multiscale Network for Breast Ultrasound Semantic Segmentation](https://arxiv.org/abs/2403.15560) | 提出了一种新颖的乳腺解剖感知网络，用于捕获细致图像细节和编码乳腺解剖的新平滑度项，有效改善了肌肉、乳房和肿瘤类别的分割准确性。 |
| [^181] | [Conformal online model aggregation](https://arxiv.org/abs/2403.15527) | 该论文提出了一种基于投票的在线依从模型聚合方法，可以根据过去表现调整模型权重。 |
| [^182] | [Latent Neural Cellular Automata for Resource-Efficient Image Restoration](https://arxiv.org/abs/2403.15525) | 这项工作介绍了一种名为Latent Neural Cellular Automata (LNCA)的新型架构，旨在解决神经元细胞自动机的资源限制，通过将计算从传统输入空间转移到特别设计的潜在空间，以此实现对图像恢复任务的改进。 |
| [^183] | [PPA-Game: Characterizing and Learning Competitive Dynamics Among Online Content Creators](https://arxiv.org/abs/2403.15524) | 引入了PPA-Game模型来表征类似YouTube和TikTok平台上的内容创作者之间竞争动态，分析显示纯纳什均衡在大多数情况下是常见的，提出了一种在线算法用于最大化每个代理者的累积收益。 |
| [^184] | [Towards auditory attention decoding with noise-tagging: A pilot study](https://arxiv.org/abs/2403.15523) | 这项试点研究首次尝试使用噪声标记刺激协议进行听觉注意力解码，取得了较高的性能表现。 |
| [^185] | [Learning to walk on new ground: Calibration-free decoding for c-VEP BCI](https://arxiv.org/abs/2403.15521) | 该研究介绍了一种基于事件相关电位的新型方法（UMM），与目前先进的c-VEP零训练方法（CCA）进行比较，证明了它们在无需校准的BCI系统中的有效性。 |
| [^186] | [GTC: GNN-Transformer Co-contrastive Learning for Self-supervised Heterogeneous Graph Representation](https://arxiv.org/abs/2403.15520) | 该论文提出了一个用于GNN和Transformer的合作学习方案，并构建了GTC架构，通过整合GNN的本地信息聚合和Transformer的全局信息建模能力，解决了过度平滑问题。 |
| [^187] | [Improving Forward Compatibility in Class Incremental Learning by Increasing Representation Rank and Feature Richness](https://arxiv.org/abs/2403.15517) | 该研究引入了一种基于有效秩的特征丰富性增强（RFR）方法，通过增加表示的有效秩，实现了提高前向兼容性的目标。同时，在后向兼容性和前向兼容性方面取得了双重目标。 |
| [^188] | [Enhancing Effectiveness and Robustness in a Low-Resource Regime via Decision-Boundary-aware Data Augmentation](https://arxiv.org/abs/2403.15512) | 本文提出了一种决策边界感知的数据增强策略，通过移动潜在特征、重构生成模糊版本以及采用中K采样来增强生成句子的多样性，从而比较于其他方法提高了在低资源环境中的有效性和稳健性。 |
| [^189] | [Multiple-Input Auto-Encoder Guided Feature Selection for IoT Intrusion Detection Systems](https://arxiv.org/abs/2403.15511) | 该论文提出了一种名为多输入自动编码器(MIAE)的新型神经网络架构，通过训练MIAE模型，在无监督学习模式下将异构输入转换为较低维表示，有助于分类器区分正常行为和不同类型的攻击。 |
| [^190] | [Privacy-Preserving End-to-End Spoken Language Understanding](https://arxiv.org/abs/2403.15510) | 本文提出了一种新颖的SLU多任务隐私保护模型，防止语音识别和身份识别攻击。 |
| [^191] | [Twin Auto-Encoder Model for Learning Separable Representation in Cyberattack Detection](https://arxiv.org/abs/2403.15509) | 提出一种新型双自动编码器模型(TAE)，通过将潜在表示转换为可分离表示来解决网络攻击检测中混合表示的问题 |
| [^192] | [Sequential Decision-Making for Inline Text Autocomplete](https://arxiv.org/abs/2403.15502) | 通过强化学习和顺序决策制定改进了文本输入系统中的在线自动完成建议，将认知负荷纳入模型训练目标，基于文本输入速度的奖励函数。 |
| [^193] | [Gene Regulatory Network Inference in the Presence of Dropouts: a Causal View](https://arxiv.org/abs/2403.15500) | 引入了因果丢失模型来解决基因调控网络推断中因丢失值引起的偏倚问题。 |
| [^194] | [A Causal Analysis of CO2 Reduction Strategies in Electricity Markets Through Machine Learning-Driven Metalearners](https://arxiv.org/abs/2403.15499) | 通过Causal Machine Learning方法分析电力市场中定价政策对CO2水平的影响，挑战传统智慧，发现可能增加CO2强度，并结合机器学习元算法增强研究深度，并提供宝贵见解。 |
| [^195] | [Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models](https://arxiv.org/abs/2403.15498) | 棋类语言模型在没有先验知识的情况下，通过下一个字符预测训练，仍能学习出内部表示的棋盘状态 |
| [^196] | [On the Detection of Anomalous or Out-Of-Distribution Data in Vision Models Using Statistical Techniques](https://arxiv.org/abs/2403.15497) | 本文评估了Benford's law作为一种检测视觉模型中异常或超出分布数据的方法，探讨了其在过滤异常数据点和标记超出分布数据方面的潜在应用。 |
| [^197] | [Multiple and Gyro-Free Inertial Datasets](https://arxiv.org/abs/2403.15494) | 设计并记录了无陀螺惯导系统（GFINS）和多惯性测量单元（MIMU）数据集，以填补该领域的研究空白。 |
| [^198] | [EEG decoding with conditional identification information](https://arxiv.org/abs/2403.15489) | 通过将个体的有条件识别信息整合到神经网络中，这项研究提出了一种新的方法来增强模型表示，从而改善脑电图信号的解码准确性。 |
| [^199] | [MOGAM: A Multimodal Object-oriented Graph Attention Model for Depression Detection](https://arxiv.org/abs/2403.15485) | MOGAM模型是为了克服现有抑郁症检测方法在不同社交媒体数据类型上的限制而提出，通过多模态数据的综合应用，实现了更具可扩展性和多功能性的解决方案。 |
| [^200] | [RakutenAI-7B: Extending Large Language Models for Japanese](https://arxiv.org/abs/2403.15484) | RakutenAI-7B是一套日本导向的大型语言模型，在日本LM Harness基准测试中表现最好，分别发布了指导和聊天调整的模型。 |
| [^201] | [Rolling bearing fault diagnosis method based on generative adversarial enhanced multi-scale convolutional neural network model](https://arxiv.org/abs/2403.15483) | 提出了一种基于生成对抗增强的多尺度卷积神经网络模型的滚动轴承故障诊断方法，使用Gram角场编码技术和梯度惩罚Wasserstein距离生成对抗网络来扩展原始训练集。 |
| [^202] | [Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors](https://arxiv.org/abs/2403.15482) | 利用大型语言模型为初学者同行辅导员提供多级详细反馈，赋能规模化的支持心理健康问题患者。 |
| [^203] | [SpikeGraphormer: A High-Performance Graph Transformer with Spiking Graph Attention](https://arxiv.org/abs/2403.15480) | 本研究将脉冲神经网络与图变换器集成，设计了脉冲图注意力模块，通过稀疏加法和掩码操作取代矩阵乘法，实现了线性复杂度，从而在大规模图上实现了所有节点之间的交互。 |
| [^204] | [Learning to Infer Generative Template Programs for Visual Concepts](https://arxiv.org/abs/2403.15476) | 探索了一种学习如何推断捕捉视觉概念的通用模板程序的神经符号系统，引入了模板程序概念，支持多种概念相关任务，提出了一种学习范式来训练网络直接推断模板程序，实验证明该方法优于任务特定替代方法，并与特定领域方法竞争性地执行。 |
| [^205] | [EC-IoU: Orienting Safety for Object Detectors via Ego-Centric Intersection-over-Union](https://arxiv.org/abs/2403.15474) | 通过EC-IoU度量，本文引入了一种定向安全性物体检测方法，可以在安全关键领域中提高物体检测器的性能，并在KITTI数据集上取得了比IoU更好的结果。 |
| [^206] | [Isometric Neural Machine Translation using Phoneme Count Ratio Reward-based Reinforcement Learning](https://arxiv.org/abs/2403.15469) | 本论文提出了使用强化学习（RL）开发的等距NMT系统，重点在于优化源语言和目标语言句对中音素计数的对齐。 |
| [^207] | [Using Super-Resolution Imaging for Recognition of Low-Resolution Blurred License Plates: A Comparative Study of Real-ESRGAN, A-ESRGAN, and StarSRGAN](https://arxiv.org/abs/2403.15466) | 本研究利用超分辨率技术对模糊的车牌进行处理，并比较了Real-ESRGAN、A-ESRGAN和StarSRGAN三种模型的效果。 |
| [^208] | [Most Likely Sequence Generation for $n$-Grams, Transformers, HMMs, and Markov Chains, by Using Rollout Algorithms](https://arxiv.org/abs/2403.15465) | 本文介绍了一种使用展开算法为$n$-grams，Transformers，HMMs和马尔可夫链生成最有可能的序列的方法 |
| [^209] | [LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction](https://arxiv.org/abs/2403.15464) | 该论文提出了一种新方法，结合预测性代理推理和批判性代理指导，利用LLMs对结构化患者就诊数据进行预测，取得了较好的效果。 |
| [^210] | [Unveiling the Anomalies in an Ever-Changing World: A Benchmark for Pixel-Level Anomaly Detection in Continual Learning](https://arxiv.org/abs/2403.15463) | 该研究针对连续学习环境中的像素级异常检测问题，实现了针对此问题的最先进技术，并提供了一个真实世界数据集作为可靠基准，为该领域的进一步发展奠定了基础。 |
| [^211] | [FUELVISION: A Multimodal Data Fusion and Multimodel Ensemble Algorithm for Wildfire Fuels Mapping](https://arxiv.org/abs/2403.15462) | 通过多模态数据融合和多模型集成方法，采用生成式人工智能技术开发伪标记和完全合成数据集，以提高野火燃料映射的准确性。 |
| [^212] | [Fine-Tuning Pre-trained Language Models to Detect In-Game Trash Talks](https://arxiv.org/abs/2403.15458) | 本研究调查了预训练语言模型在检测游戏内垃圾话和毒性信息方面的能力，使用BERT和GPT模型在DOTA 2游戏对战的聊天数据上进行评估。 |
| [^213] | [Improving Sampling Methods for Fine-tuning SentenceBERT in Text Streams](https://arxiv.org/abs/2403.15455) | 本研究旨在解决概念漂移问题，通过探索七种文本采样方法的有效性，精细调整语言模型，从而减轻性能下降。 |
| [^214] | [What is Wrong with End-to-End Learning for Phase Retrieval?](https://arxiv.org/abs/2403.15448) | 对称破坏技术在解决深度学习方法中前向模型对称性造成的困难方面发挥了重要作用。 |
| [^215] | [Decoding Multilingual Topic Dynamics and Trend Identification through ARIMA Time Series Analysis on Social Networks: A Novel Data Translation Framework Enhanced by LDA/HDP Models](https://arxiv.org/abs/2403.15445) | 该研究提出了一种新方法，通过ARIMA时间序列分析和LDA/HDP模型提取多语言社交网络中的主题动态，特别关注在危机期间的交流趋势，这一方法在语言一致性任务中表现出色。 |
| [^216] | [A Survey of IMU Based Cross-Modal Transfer Learning in Human Activity Recognition](https://arxiv.org/abs/2403.15444) | 本篇论文调查了如何在人类活动/动作识别中实现跨模态迁移学习，探讨了IMU数据在此领域中的潜在应用，对HAR问题进行了重要性探讨，并比较了不同类型的多模态HAR数据集。 |
| [^217] | [Introducing an ensemble method for the early detection of Alzheimer's disease through the analysis of PET scan images](https://arxiv.org/abs/2403.15443) | 通过分析PET扫描图像，引入了一种集成方法早期检测阿尔茨海默病，并且在分类阿尔茨海默病时使用了多种深度学习和传统机器学习模型。 |
| [^218] | [Unified Generative Modeling of 3D Molecules via Bayesian Flow Networks](https://arxiv.org/abs/2403.15441) | 本文引入了Geometric Bayesian Flow Networks (GeoBFN)，通过在分布的可微分参数空间中对不同模态进行建模，实现了对多模态性和噪声敏感性的分子几何形状的自然拟合。GeoBFN通过优化的训练和采样技术，在多个3D分子生成基准上取得了最先进的性能。 |
| [^219] | [Federated Learning based on Pruning and Recovery](https://arxiv.org/abs/2403.15439) | 提出了一种基于修剪与恢复的联邦学习训练框架，通过整合异步学习算法和修剪技术，有效解决了传统算法在异构设备场景中的低效问题。 |
| [^220] | [Unsupervised Adaptive Deep Learning Method For BCI Motor Imagery Decoding](https://arxiv.org/abs/2403.15438) | 提出了一种无监督自适应深度学习方法，用于BCI运动想象解码，在不需要监督的情况下达到离线性能水平，同时无需重新训练模型，在实时观察基础上实现数据的持续重新对齐，展示了在跨主体场景下的有效性，并提供实验代码。 |
| [^221] | [HyPer-EP: Meta-Learning Hybrid Personalized Models for Cardiac Electrophysiology](https://arxiv.org/abs/2403.15433) | 提出了一个新颖的混合建模框架，结合了基于物理已知表达式和神经网络建模的元学习方法，用于描述个性化心脏数字孪生模型，实现了基于物理和神经网络组件的分离识别。 |
| [^222] | [BRIEDGE: EEG-Adaptive Edge AI for Multi-Brain to Multi-Robot Interaction](https://arxiv.org/abs/2403.15432) | BRIEDGE提出了一个用于多脑到多机器人交互的端到端系统，通过 EEG 自适应神经网络和编解码通信框架实现，引入了基于Informer的ProbSparse自注意机制以提高分类准确性。 |
| [^223] | [Transferring BCI models from calibration to control: Observing shifts in EEG features](https://arxiv.org/abs/2403.15431) | 通过标准校准会话和基于EMG的新BCI控制会话，观察到了感觉运动节律的相似之处，并发现了控制范例引入的额外准备效果。 |
| [^224] | [A Three-Phases SFT Hybrid Model Integrated Strong Prior Module and Data Overlap Estimation in the Eduation Context](https://arxiv.org/abs/2403.15426) | 提出了一种在教育领域中应用的三阶段监督微调模型，通过先验和数据重叠估计实现了教育知识的结构拆卸和增量引导输出。 |
| [^225] | [Cross-user activity recognition using deep domain adaptation with temporal relation information](https://arxiv.org/abs/2403.15424) | 该论文提出了Deep Temporal State Domain Adaptation（DTSDA）模型，用于处理跨用户活动识别中的行为变异挑战。 |
| [^226] | [Cross-user activity recognition via temporal relation optimal transport](https://arxiv.org/abs/2403.15423) | 本文提出了一种基于时间关系最优输运的跨用户活动识别方法，旨在解决现有基于i.i.d.假设的域自适应方法在时间序列数据中的局限性 |
| [^227] | [Machine Learning Techniques for Sensor-based Human Activity Recognition with Data Heterogeneity -- A Review](https://arxiv.org/abs/2403.15422) | 通过研究机器学习如何处理传感器数据异质性，能够改善人体活动识别系统的性能，降低计算成本，更快地开发出个性化的自适应模型。 |
| [^228] | [Agile gesture recognition for low-power applications: customisation for generalisation](https://arxiv.org/abs/2403.15421) | 通过自适应和敏捷误差校正，提升了传统手势识别模型在低功耗设备上的性能 |
| [^229] | [Attention is all you need for boosting graph convolutional neural network](https://arxiv.org/abs/2403.15419) | GKEDM插件模块通过多头注意力机制提取和聚合图信息，增强节点表示并提高GCN的性能，同时可以作为知识蒸馏的辅助转移器。 |
| [^230] | [Enhancing Automatic Modulation Recognition for IoT Applications Using Transformers](https://arxiv.org/abs/2403.15417) | 使用Transformer网络提出了一种高效的自动调制识别方法，在物联网环境中具有最佳的识别准确率。 |
| [^231] | [Fuzzy hyperparameters update in a second order optimization](https://arxiv.org/abs/2403.15416) | 介绍了一种在二阶优化中加速收敛的混合方法，利用在线有限差分逼近对角Hessian矩阵，并应用模糊推理于多个超参数。 |
| [^232] | [Physics-informed and Unsupervised Riemannian Domain Adaptation for Machine Learning on Heterogeneous EEG Datasets](https://arxiv.org/abs/2403.15415) | 提出一种利用物理信息和无监督方法的黎曼域自适应技术，能够有效整合不同EEG数据集以进行机器学习，特别在脑机接口任务和生物标志物应用中表现出鲁棒性。 |
| [^233] | [Coupled generator decomposition for fusion of electro- and magnetoencephalography data](https://arxiv.org/abs/2403.15409) | 介绍了耦合发生器分解的概念，推广了稀疏主成分分析（SPCA），并通过脑电图和脑磁图数据的融合实验展示了其在识别共同特征的有效性。 |
| [^234] | [Multi-modal Heart Failure Risk Estimation based on Short ECG and Sampled Long-Term HRV](https://arxiv.org/abs/2403.15408) | 提出了结合30秒ECG记录和长期HRV数据的多模态方法用于估算心力衰竭住院风险，并引入了两种生存模型：XGBoost模型和ResNet模型。 |
| [^235] | [ChatGPT in Linear Algebra: Strides Forward, Steps to Go](https://arxiv.org/abs/2403.15399) | ChatGPT在线性代数中取得巨大改进，但目前仍然无法完全取代人类教师，软件理解问题的能力引发了人们对其潜力的思考。 |
| [^236] | ["Model Cards for Model Reporting" in 2024: Reclassifying Category of Ethical Considerations in Terms of Trustworthiness and Risk Management](https://arxiv.org/abs/2403.15394) | 将模型报告中的道德考虑类别重新分类为可信度和风险管理类别，针对可信度AI领域的最新发展提出了新的观点。 |
| [^237] | [Detection of Opioid Users from Reddit Posts via an Attention-based Bidirectional Recurrent Neural Network](https://arxiv.org/abs/2403.15393) | 通过机器学习方法分析Reddit用户帖子，检测阿片类药物使用者，帮助改善对阿片类药物危机的监测和理解。 |
| [^238] | [A Transfer Attack to Image Watermarks](https://arxiv.org/abs/2403.15365) | 水印领域的研究表明，即使在攻击者无法访问水印模型或检测API的情况下，水印基础的AI生成图像检测器也无法抵抗对抗攻击。 |
| [^239] | [Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning](https://arxiv.org/abs/2403.15048) | 该研究提出了一种用于检测由TTI模型生成的卡通角色图像中视觉幻觉的系统，通过结合姿势感知上下文视觉学习和视觉语言模型，利用RGB图像和姿势信息，实现了更准确的决策，显著提高了视觉幻觉的识别能力，推动了TTI模型在非照片真实领域的发展。 |
| [^240] | [Gravitational Duals from Equations of State](https://arxiv.org/abs/2403.14763) | 引力双对偶理论提出了一种基于物理信息神经网络的新方法，可以从预设的状态方程推导出对应的引力理论。 |
| [^241] | [Developing and Deploying Industry Standards for Artificial Intelligence in Education (AIED): Challenges, Strategies, and Future Directions](https://arxiv.org/abs/2403.14689) | 教育领域人工智能的发展需要制定和实施产业标准，以解决互操作性、可扩展性和道德治理等挑战。 |
| [^242] | [ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training](https://arxiv.org/abs/2403.14589) | 提出了A$^3$T框架，通过ActRe提示代理实现了ReAct风格代理对代理轨迹的自主标注，同时增强了新的轨迹合成能力。 |
| [^243] | [An Analysis of Linear Time Series Forecasting Models](https://arxiv.org/abs/2403.14587) | 分析了线性时间序列预测模型，证明了几种流行的线性模型变体等效于标准的线性回归，提供了实验证据支持这一结论。 |
| [^244] | [A survey on Concept-based Approaches For Model Improvement](https://arxiv.org/abs/2403.14566) | 基于概念方法对深度神经网络进行解释性改进，提供了人类可理解的决策解释，使得检测伪关联和固有偏见成为可能。 |
| [^245] | [Improving Image Classification Accuracy through Complementary Intra-Class and Inter-Class Mixup](https://arxiv.org/abs/2403.14137) | 通过提出一种新的mixup方法，该方法针对类内混合进行了优化，提高了图像分类准确性。 |
| [^246] | [C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion](https://arxiv.org/abs/2403.14119) | 本文研究了在测试时提示调整过程中通过利用CLIP的固有属性来探讨校准的方法，发现提示选择显著影响了CLIP中的校准，其中导致更高文本特征离散性的提示会产生更好校准的预测。 |
| [^247] | [Carbon Footprint Reduction for Sustainable Data Centers in Real-Time](https://arxiv.org/abs/2403.14092) | 我们提出了一种Data Center Carbon Footprint Reduction (DC-CFR) 多代理强化学习（MARL）框架，旨在实时优化数据中心以减少碳足迹。 |
| [^248] | [Extracting Emotion Phrases from Tweets using BART](https://arxiv.org/abs/2403.14050) | 本文提出了一种基于BART的情感分析方法，利用问答框架从文本中提取特定情绪短语，并通过分类器预测答案跨度位置，实现对情绪短语的精确提取。 |
| [^249] | [Integrating Wearable Sensor Data and Self-reported Diaries for Personalized Affect Forecasting](https://arxiv.org/abs/2403.13841) | 本论文提出了一种整合了可穿戴传感器数据和自我报告日记的多模态深度学习模型，用于情感状态的预测。 |
| [^250] | [Decentralized Federated Learning: Model Update Tracking Under Imperfect Information Sharing](https://arxiv.org/abs/2403.13247) | 提出了适用于存在嘈杂通信通道的分散式联邦学习算法FedNMUT，通过梯度跟踪减小数据异质性影响并最小化通信开销，在噪声参数中实现客户端共识，优于现有方法和传统参数混合方法。 |
| [^251] | [A tutorial on learning from preferences and choices with Gaussian Processes](https://arxiv.org/abs/2403.11782) | 提供了一个使用高斯过程进行偏好学习的框架，能够将理性原则融入学习过程，涵盖了多种偏好学习模型。 |
| [^252] | [Time Series Compression using Quaternion Valued Neural Networks and Quaternion Backpropagation](https://arxiv.org/abs/2403.11722) | 提出了一种使用四元值神经网络和四元反向传播进行时间序列压缩的方法，在保留特征之间关系的同时在故障分类中展现出潜力。 |
| [^253] | [A learning-based solution approach to the application placement problem in mobile edge computing under uncertainty](https://arxiv.org/abs/2403.11259) | 通过机器学习模型将用户请求分配给服务器，以解决在移动边缘计算中应用部署问题的二阶段随机规划。 |
| [^254] | [Reinforcement Learning with Options](https://arxiv.org/abs/2403.10855) | 本论文提出了使用选项的分层强化学习方法，通过构建Hierarchical Policy learning来解决高维复杂环境中学习的问题。 |
| [^255] | [LightIt: Illumination Modeling and Control for Diffusion Models](https://arxiv.org/abs/2403.10615) | LightIt提出了一种用于图像生成的显式照明控制方法，通过条件生成来实现对图像生成的照明控制，同时训练了一个身份保持的重照模型。 |
| [^256] | [A Conceptual Framework For White Box Neural Networks](https://arxiv.org/abs/2403.09863) | 引入语义特征作为通用概念框架，实现了完全可解释的神经网络层，为白盒神经网络的范式转变开辟了新的可能性。 |
| [^257] | [Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition](https://arxiv.org/abs/2403.09506) | 本研究提出了一种名为运动一致增强（MCA）的数据增强方法，通过引入外观变化来鼓励模型优先考虑视频中的运动信息，而不是静态外观。 |
| [^258] | [Deep Limit Order Book Forecasting](https://arxiv.org/abs/2403.09267) | 该研究利用深度学习方法预测纳斯达克交易所股票的限价订单簿中间价格变动，提出了一个创新的操作框架来评估预测的实用性。 |
| [^259] | [Knowledge Graph Large Language Model (KG-LLM) for Link Prediction](https://arxiv.org/abs/2403.07311) | 该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。 |
| [^260] | [Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning](https://arxiv.org/abs/2403.06725) | 本文提出了名为LoReKT的低资源知识追踪框架，通过监督预训练和微调重要性机制，旨在从丰富资源的KT数据集中学习可转移的参数和表示来改进低资源知识追踪任务。 |
| [^261] | [Distributionally Generative Augmentation for Fair Facial Attribute Classification](https://arxiv.org/abs/2403.06606) | 该论文提出了一种新颖的分布生成增强方法，用于在偏见数据上训练公平的面部属性分类模型，无需额外标注虚假属性，通过生成模型识别潜在的虚假属性，并在图像空间中显示，以提高模型可解释性。 |
| [^262] | [Decoupled Data Consistency with Diffusion Purification for Image Restoration](https://arxiv.org/abs/2403.06054) | 通过分离反向过程和数据一致性步骤，提出了一种新颖的基于扩散的图像恢复求解器。 |
| [^263] | [Defending Against Unforeseen Failure Modes with Latent Adversarial Training](https://arxiv.org/abs/2403.05030) | 本研究利用潜在对抗训练（LAT）来防御AI系统中未预见的故障模式，通过利用网络实际用于预测的压缩、抽象和结构化概念的潜在表示，有效清除了恶意软件和对抗性攻击。 |
| [^264] | [Restricted Bayesian Neural Network](https://arxiv.org/abs/2403.04810) | 本研究提出了限制贝叶斯神经网络的新架构，显著减少了网络存储空间复杂性，并引入了一种能够有效处理不确定性的算法，确保在目标函数缺乏完美凸性时稳健地收敛至全局最优解。 |
| [^265] | [Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability](https://arxiv.org/abs/2403.03967) | 通过环境-固有维度差异的概念，论文证明了维度差异使干净训练的模型更容易受到数据空间脱离流形方向的对抗扰动攻击。 |
| [^266] | [Latent Dataset Distillation with Diffusion Models](https://arxiv.org/abs/2403.03881) | 这项研究提出了使用扩散模型进行潜在数据集蒸馏（LD3M），结合潜在空间中的扩散和数据集蒸馏的方法，以解决不同模型架构导致准确性下降和生成高分辨率图像的挑战。 |
| [^267] | [A Second Look on BASS -- Boosting Abstractive Summarization with Unified Semantic Graphs -- A Replication Study](https://arxiv.org/abs/2403.02930) | 通过复制研究BASS框架，发现了与原始工作相比性能上的差异，并强调了撰写可复制论文的关键实践。 |
| [^268] | [Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels](https://arxiv.org/abs/2403.02746) | 提出了一个名为Paraformer的弱监督框架，通过低分辨率历史土地覆盖数据指导大规模高分辨率土地覆盖映射，设计了CNN-Transformer特征提取器来综合捕获局部和全局背景信息 |
| [^269] | [Concurrent Learning of Policy and Unknown Safety Constraints in Reinforcement Learning](https://arxiv.org/abs/2402.15893) | 提出了一种同时学习安全RL控制策略和识别未知安全约束参数的新方法。 |
| [^270] | [A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning](https://arxiv.org/abs/2402.11922) | 提出了一种生成式预训练框架 GPDiff，通过在源城市数据优化的模型参数上进行预训练，将STG迁移学习转化为预训练生成式超网络，实现了对不同数据分布和特定城市特征的适应性。 |
| [^271] | [Stochastic Hessian Fitting on Lie Group](https://arxiv.org/abs/2402.11858) | 本文研究了在随机Hessian-向量乘积上拟合Hessian或其逆，揭示了不同Hessian拟合方法的收敛速率，并证明了在特定李群上的Hessian拟合问题在轻微条件下是强凸的。 |
| [^272] | [Exploring the Adversarial Capabilities of Large Language Models](https://arxiv.org/abs/2402.09132) | 本研究探索了大型语言模型的对抗能力，并发现其能够成功地制造对抗性示例以愚弄安全措施，特别是在仇恨言论检测方面具有重大影响。 |
| [^273] | [BioNeRF: Biologically Plausible Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2402.07310) | BioNeRF是一种生物合理的架构，通过辐射场对场景进行3D表示并合成新视图。它实现了一种认知启发的机制，提高了存储能力和提取信息的能力，并在真实世界图像和合成数据的两个数据集上超过了以人的感知为基础的质量度量的最新结果。 |
| [^274] | [Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment](https://arxiv.org/abs/2402.04599) | JEANIE是一种通过时间-视角对齐的方法，用于测量三维骨架序列的相似度。它能够解决视频序列中速度、时间位置和姿势的干扰变化问题。在评估了骨架Few-shot动作识别任务后，JEANIE在支持-查询序列对的时间块匹配方面表现出了良好的效果。 |
| [^275] | [Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback](https://arxiv.org/abs/2402.02423) | Uni-RLHF是一个通用的强化学习平台和基准套件，致力于处理多样化的人类反馈，解决了在RLHF中量化进展的挑战，并提供了用户友好的注释界面和离线基准实现。 |
| [^276] | [Self-Supervised Contrastive Forecasting](https://arxiv.org/abs/2402.02023) | 该论文介绍了一种通过采用对比学习和增强的分解架构，并结合全局自相关性的自监督方法来解决长期预测中的挑战。实验证明，该方法在九个长期基准上的多个实验中胜过了14个基线模型。 |
| [^277] | [LOCOST: State-Space Models for Long Document Abstractive Summarization](https://arxiv.org/abs/2401.17919) | LOCOST是一种基于状态空间模型的编码器-解码器架构，用于处理长文档的抽象摘要生成。与基于稀疏注意模式的最先进模型相比，LOCOST具有更低的计算复杂度，并且能够在训练和推断期间节省大量内存。在评估中，LOCOST在长文档摘要化任务上达到了93-96%的性能水平，并且能够处理超过600K个标记的输入文本。 |
| [^278] | [Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators](https://arxiv.org/abs/2401.17548) | 本文提出了一种新方法，LIFT，通过利用通道相关性和领先指标，为多元时间序列预测提供准确的预测。LIFT方法可以无缝与任意时间序列预测方法协作，大量实验证明了其有效性。 |
| [^279] | [A Large-Scale Empirical Study on Improving the Fairness of Image Classification Models](https://arxiv.org/abs/2401.03695) | 本研究进行了大规模实证研究，比较了现有最先进的公平性改进技术在图像分类模型上的表现，揭示了它们之间的显著差异 |
| [^280] | [Discovering modular solutions that generalize compositionally](https://arxiv.org/abs/2312.15001) | 通过在教师-学生设置中研究模块化系统，我们探讨了模块化系统在发现隐藏组合结构方面的作用，并证明了其在识别潜在模块方面的重要性。 |
| [^281] | [Multi-agent reinforcement learning using echo-state network and its application to pedestrian dynamics](https://arxiv.org/abs/2312.11834) | 通过使用回声状态网络将行人实现为MARL代理，研究了他们学习避让其他代理向前移动的能力，在密度不太高的情况下取得成功。 |
| [^282] | [Spectral methods for Neural Integral Equations](https://arxiv.org/abs/2312.05654) | 本文引入了一个基于谱方法的神经积分方程框架，通过在谱域中学习算子，降低了计算成本，并保证了高插值精度。 |
| [^283] | [I-PHYRE: Interactive Physical Reasoning](https://arxiv.org/abs/2312.03009) | I-PHYRE是一个挑战代理同时展示直观的物理推理、多步规划和就地干预能力的框架。 |
| [^284] | [Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model](https://arxiv.org/abs/2311.13231) | 该论文提出了一种名为D3PO的方法，通过使用人类反馈直接微调扩散模型，无需奖励模型，从而在消除了奖励模型的前提下改进了现有方法，并解决了DPO方法直接应用的内存需求问题。 |
| [^285] | [EnduRL: Enhancing Safety, Stability, and Efficiency of Mixed Traffic Under Real-World Perturbations Via Reinforcement Learning](https://arxiv.org/abs/2311.12261) | 通过分析真实世界驾驶轨迹并将其应用于机器人车辆训练，可以提高混合交通在各种环境中的安全性、效率和稳定性 |
| [^286] | [Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models](https://arxiv.org/abs/2311.11202) | 本研究专注于真实世界数据集的可信度，提出了一个系统化框架，用于评估数据集的可信度，识别标签错误，并评估嘈杂标签对不安全评论和对话分类的影响，以提高训练无害语言模型的质量。 |
| [^287] | [Tactics2D: A Reinforcement Learning Environment Library with Generative Scenarios for Driving Decision-making](https://arxiv.org/abs/2311.11058) | Tactics2D是一个具有自动生成交通场景功能的强化学习环境库，旨在为研究人员提供探索学习驱动的驾驶决策模型的工具。 |
| [^288] | [A Poincar\'e Inequality and Consistency Results for Signal Sampling on Large Graphs](https://arxiv.org/abs/2311.10610) | 该论文介绍了一种针对图极限中图上信号的信号采样理论，证明了Poincar\'e不等式并展示了一致性结果。 |
| [^289] | [Evaluating Neighbor Explainability for Graph Neural Networks](https://arxiv.org/abs/2311.08118) | 评价图神经网络中邻居的可解释性，提出新的度量标准并发现基于梯度的方法在GNN领域的解释没有太大差异，同时发现很多技术在没有自环的GNNs下无法准确识别重要邻居。 |
| [^290] | [Causal Question Answering with Reinforcement Learning](https://arxiv.org/abs/2311.02760) | 本研究通过强化学习在因果图上进行因果问答，引入了一种基于演员评论的 agent，有效解决了当前因果问答方法无法提供解释或证据的问题 |
| [^291] | [Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions](https://arxiv.org/abs/2311.02695) | 本文提出了一种新的方法，可以在一个环境中允许定位多个变量的干预，并在因果表示学习中首次得出了可识别性结果。 |
| [^292] | [Bird's Eye View Based Pretrained World model for Visual Navigation](https://arxiv.org/abs/2310.18847) | 通过基于鸟瞰视角图像的预训练世界模型，实现了从模拟器到真实世界的零迁移 |
| [^293] | [A general learning scheme for classical and quantum Ising machines](https://arxiv.org/abs/2310.18411) | 提出了一种基于伊辛结构的新机器学习模型，可以通过梯度下降进行高效训练，并利用伊辛机自己估计的偏导数优化损失函数，同时在量子领域展示了多种学习任务的新可能性。 |
| [^294] | [BatteryML:An Open-source platform for Machine Learning on Battery Degradation](https://arxiv.org/abs/2310.14714) | BatteryML是一个开源平台，通过一站式、全面的方法统一了电池衰减建模的数据预处理、特征提取和模型实现，提高了研究应用的实用性和效率。 |
| [^295] | [RTSUM: Relation Triple-based Interpretable Summarization with Multi-level Salience Visualization](https://arxiv.org/abs/2310.13895) | 提出了RTSUM框架，利用关系三元组进行摘要生成，并开发了一个可解释的摘要工具，支持多级显著性可视化。 |
| [^296] | [Towards Robust Multi-Modal Reasoning via Model Selection](https://arxiv.org/abs/2310.08446) | 多模态代理在处理复杂挑战时需要考虑模型选择的重要性，以避免执行的脆弱性。 |
| [^297] | [A Meta-Learning Perspective on Transformers for Causal Language Modeling](https://arxiv.org/abs/2310.05884) | 本文从元学习视角探讨了Transformer用于因果语言建模时的内部优化过程，发现并分析了Transformer-based因果语言模型中学习到的token表示范数的特殊特征。 |
| [^298] | [HyMNet: a Multimodal Deep Learning System for Hypertension Classification using Fundus Photographs and Cardiometabolic Risk Factors](https://arxiv.org/abs/2310.01099) | 介绍了HyMNet，一种结合眼底图像和心脏代谢风险因素的多模态深度学习系统，用于改善高血压检测能力。 |
| [^299] | [Subtractive Mixture Models via Squaring: Representation and Learning](https://arxiv.org/abs/2310.00724) | 通过平方操作实现的消减混合模型在表达能力上优于传统加法混合模型，并在真实世界分布估计任务中得到了实验证明。 |
| [^300] | [Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic](https://arxiv.org/abs/2309.13339) | 提出了LoT（Logical Thoughts）提示，一个自我改进框架，利用根植于符号逻辑的原则，特别是归谬法，逐步验证和纠正大型语言模型的零射链推理过程。 |
| [^301] | [Latent assimilation with implicit neural representations for unknown dynamics](https://arxiv.org/abs/2309.09574) | LAINR通过引入球面隐式神经表示（SINR）和数据驱动的不确定性估计器，提高了同化过程的效率，并在准确性和效率方面优于现有基于自动编码器的方法。 |
| [^302] | [Kinematics-aware Trajectory Generation and Prediction with Latent Stochastic Differential Modeling](https://arxiv.org/abs/2309.09317) | 本研究通过潜在随机微分建模，提出了一种运动学感知的方法，同时结合数据驱动和模型驱动的优势，解决了自动驾驶中轨迹生成和预测中的物理现实性挑战。 |
| [^303] | [Improving the forecast accuracy of wind power by leveraging multiple hierarchical structure](https://arxiv.org/abs/2308.03472) | 通过整合风电场中风力发电机的横截面和时间层次结构，构建跨时层次结构，从而提高风电场的预测准确性。 |
| [^304] | [Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization](https://arxiv.org/abs/2307.12851) | 神经元在两层ReLU网络中早期阶段会尝试与输入数据对齐，对齐时间上界为$\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$，在对齐阶段过后损失收敛速度为$\mathcal{O}(\frac{1}{t})$ |
| [^305] | [On the resilience of Collaborative Learning-based Recommender Systems Against Community Detection Attack](https://arxiv.org/abs/2306.08929) | 本文研究了协作学习推荐系统针对一种新型隐私攻击——社区检测攻击（CDA）的韧性。 |
| [^306] | [Identification of Energy Management Configuration Concepts from a Set of Pareto-optimal Solutions](https://arxiv.org/abs/2306.08318) | 通过概念识别技术从庞大的帕累托最优解数据集中找到相关和可行的能源管理配置。 |
| [^307] | [Rethinking the Evaluation Protocol of Domain Generalization](https://arxiv.org/abs/2305.15253) | 重新评估领域泛化的评估协议，提出采用自监督预训练或从头开始训练，使用多个测试领域，以更准确评估OOD泛化能力。 |
| [^308] | [Can Copyright be Reduced to Privacy?](https://arxiv.org/abs/2305.14822) | 生成式人工智能模型可能违反版权法，研究人员正在探索算法稳定性技术以确保对生成模型的负责任使用。 |
| [^309] | [LLM Paternity Test: Generated Text Detection with LLM Genetic Inheritance](https://arxiv.org/abs/2305.12519) | LLM-Pat提出了一种基于模型的生成文本检测方法，通过重建并比较候选文本与其对应的“兄弟”文本的相似性，从而判断候选文本是否由机器生成。 |
| [^310] | [EHRDiff: Exploring Realistic EHR Synthesis with Diffusion Models](https://arxiv.org/abs/2303.05656) | 该研究探索了使用扩散模型在电子健康记录（EHR）数据合成方面的潜力，挑战传统基于生成对抗网络（GAN）的方法。 |
| [^311] | [The autoregressive neural network architecture of the Boltzmann distribution of pairwise interacting spins systems](https://arxiv.org/abs/2302.08347) | 该论文提出了将二进制成对相互作用系统的伯尔曼分布精确映射为自回归形式的方法，得到的ARNN架构具有明确定义的物理含义，并且可以通过统计物理技术推导出特定系统的新ARNN。 |
| [^312] | [LMC: Fast Training of GNNs via Subgraph Sampling with Provable Convergence](https://arxiv.org/abs/2302.00924) | LMC是第一个带有收敛性保证的子图抽样方法，旨在解决邻居爆炸问题，提高训练收敛速度。 |
| [^313] | [Sequential Estimation of Gaussian Process-based Deep State-Space Models](https://arxiv.org/abs/2301.12528) | 该论文提出了一种基于高斯过程和深度高斯过程的方法，用于顺序估计包括潜在过程在内的状态空间和深度状态空间模型的未知量。 |
| [^314] | [FreshGNN: Reducing Memory Access via Stable Historical Embeddings for Graph Neural Network Training](https://arxiv.org/abs/2301.07482) | FreshGNN是一种通用的GNN小批量训练框架，通过稳定的历史缓存存储和重复使用GNN节点嵌入，在训练大型GNN模型时减少内存访问，解决了当前框架存在的准确性降低问题。 |
| [^315] | [Masked Vector Quantization](https://arxiv.org/abs/2301.06626) | 提出了一种Masked Vector Quantization（MVQ）框架，通过学习掩码配置并使用Multiple Hypothese Dropout（MH-Dropout）训练制度，增加了每个代码向量的表示能力，在图像数据集上取得了显著的性能提升。 |
| [^316] | [Distributional Robustness Bounds Generalization Errors](https://arxiv.org/abs/2212.09962) | 分布鲁棒性界定了泛化错误，Bayesian方法在可能近似正确意义上是分布鲁棒的，同时正则化的经验风险最小化方法也被证明是等价于Bayesian方法的。 |
| [^317] | [Uncertainty Quantification of MLE for Entity Ranking with Covariates](https://arxiv.org/abs/2212.09961) | 提出了一种新颖的 Covariate-Assisted Ranking Estimation (CARE) 模型，扩展了 Bradley-Terry-Luce (BTL) 模型，通过将协变量信息结合进排名估计中，解决了实体排名问题中的不确定性。 |
| [^318] | [A survey on knowledge-enhanced multimodal learning](https://arxiv.org/abs/2211.12328) | 知识图和其他知识来源填补了视觉语言学习模型在日常知识理解方面的差距，提升了模型的性能和可解释性。 |
| [^319] | [Utilizing Synthetic Data in Supervised Learning for Robust 5-DoF Magnetic Marker Localization](https://arxiv.org/abs/2211.07556) | 使用神经网络绕过传统迭代优化方法和磁偶极模型的限制，直接推断磁标记的位置和方向，提高定位的准确性和效率 |
| [^320] | [Detection of diabetic retinopathy using longitudinal self-supervised learning](https://arxiv.org/abs/2209.00915) | 本研究探讨了利用纵向自监督学习对糖尿病视网膜病变进行诊断的益处，通过比较不同方法模拟疾病进展并在纵向眼底照片中检测早期DR严重程度变化，取得了较高的AUC。 |
| [^321] | [On the Privacy Effect of Data Enhancement via the Lens of Memorization](https://arxiv.org/abs/2208.08270) | 该论文通过记忆的视角研究了数据增强对机器学习模型隐私泄露的影响，发现先前的成员推理攻击产生误导性结果，提出了一种新的攻击方法来评估个体样本的记忆程度。 |
| [^322] | [Dynamical softassign and adaptive parameter tuning for graph matching](https://arxiv.org/abs/2208.08233) | 本文提出了一种图匹配算法，结合了自适应步长参数和动态softassign策略，能够提高收敛性和效率，特别适用于完全连接的图匹配问题。 |
| [^323] | [An adjoint-free algorithm for conditional nonlinear optimal perturbations (CNOPs) via sampling](https://arxiv.org/abs/2208.00956) | 通过抽样算法获取条件非线性最优扰动，避免传统优化方法中昂贵的梯度计算成本和无法使用的伴随技术。 |
| [^324] | [Generalizing to Unseen Domains with Wasserstein Distributional Robustness under Limited Source Knowledge](https://arxiv.org/abs/2207.04913) | 提出了一种WDRDG框架，通过优化类别特定的Wasserstein不确定性集合中的条件分布，实现在未知领域中基于有限源知识的泛化。 |
| [^325] | [Sample compression schemes for balls in graphs](https://arxiv.org/abs/2206.13254) | 设计了适用于树、环、区间图、环树和自由立方体中位图的球的样本压缩方案，具有不同大小，为未解决的VC维度$d$的问题提供了新的见解。 |
| [^326] | [A cGAN Ensemble-based Uncertainty-aware Surrogate Model for Offline Model-based Optimization in Industrial Control Problems](https://arxiv.org/abs/2205.07250) | 引入了一个基于cGAN集成的关注不确定性的替代模型，用于可靠处理工业控制问题中的离线模型优化，实验结果表明其优于竞争基线模型。 |
| [^327] | [On The Effectiveness of One-Class Support Vector Machine in Different Defect Prediction Scenarios](https://arxiv.org/abs/2202.12074) | 本文研究了一类支持向量机在不同缺陷预测场景中的有效性，并发现其在跨版本和跨项目的缺陷预测模型中表现出色。 |
| [^328] | [A Theoretical Understanding of Gradient Bias in Meta-Reinforcement Learning](https://arxiv.org/abs/2112.15400) | 本文对梯度为基础的元强化学习中的梯度偏差进行了深入理论理解，提出了统一框架描述GMRL算法的变化，并指出现有的随机元梯度估计器实际上是有偏的。 |
| [^329] | [Online Action Learning in High Dimensions: A Conservative Perspective](https://arxiv.org/abs/2009.13961) | 该论文将$\epsilon_t$-贪心启发式方法扩展到高维度情境中，采用保守导向策略，实现在实用应用中对新奇性的重视，同时限制了采纳不寻常动作，有效控制了累积遗憾。 |
| [^330] | [Learning Concepts Definable in First-Order Logic with Counting](https://arxiv.org/abs/1909.03820) | 该研究将一阶逻辑与计数符号相结合，证明了可以在多对数度结构下以次线性时间一致学习可定义的分类器，为包含数值方面的机器学习扩展学习框架迈出了第一步。 |
| [^331] | [Multi-view Deep Subspace Clustering Networks](https://arxiv.org/abs/1908.01978) | 提出了一种名为多视角深度子空间聚类网络（MvDSCN）的方法，通过端到端学习多视角自表示矩阵，解决了现有方法中多视角关系未嵌入特征学习，以及深度学习端到端学习不适用于多视角聚类的问题。 |
| [^332] | [Deep Learning Based Sphere Decoding](https://arxiv.org/abs/1807.03162) | 提出了一种基于深度学习的球解码算法，通过智能学习解码超球体的半径，实现了性能接近最优解码的效果，并显著降低了计算复杂度。 |
| [^333] | [Thompson Sampling for Stochastic Bandits with Noisy Contexts: An Information-Theoretic Regret Analysis.](http://arxiv.org/abs/2401.11565) | 本文研究了具有噪音上下文的随机赌臂问题，并提出了一种Thompson采样算法，通过贝叶斯框架进行分析，证明了算法的贝叶斯后悔，并扩展了问题到延迟观察真实上下文的情况，并实证了算法的性能。 |
| [^334] | [Do Vision and Language Encoders Represent the World Similarly?.](http://arxiv.org/abs/2401.05224) | 通过分析视觉和语言模型的潜在空间结构，发现未对齐和对齐的编码器的表示空间在语义上是相似的。我们提出了两种方法来匹配未对齐编码器，无需训练即可实现匹配。 |
| [^335] | [Preference as Reward, Maximum Preference Optimization with Importance Sampling.](http://arxiv.org/abs/2312.16430) | 本文提出了一种使用重要性抽样进行最大偏好优化的算法，该算法通过直接优化生成策略来消除对奖励模型的需求，提高了数据利用率和稳定性，并通过解决KL正则化问题来改善偏好学习效果。 |
| [^336] | [VQPy: An Object-Oriented Approach to Modern Video Analytics.](http://arxiv.org/abs/2311.01623) | VQPy是一种面向对象的视频分析方法，它使用Python变体作为前端，并具有可扩展的后端，可以自动构建和优化基于视频对象的处理流程。 |
| [^337] | [Best of Both Worlds: Stochastic and Adversarial Convex Function Chasing.](http://arxiv.org/abs/2311.00181) | 该论文研究了随机和对抗性环境下的凸函数追踪问题，并给出了同时在两种情境下达到性能保证的算法。这是首个使用随机框架研究该问题的工作，提出了一种融合两种情境的最佳算法。 |
| [^338] | [Ghost on the Shell: An Expressive Representation of General 3D Shapes.](http://arxiv.org/abs/2310.15168) | 这里是中文总结出的一句话要点 |
| [^339] | [A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis.](http://arxiv.org/abs/2310.11959) | 我们提出了一种名为MSD-Mixer的多尺度分解MLP-Mixer模型，它能够将时间序列分解成不同的组成部分，并在不同的层次中表示这些组成部分。我们还提出了一种新颖的时间拼接方法，以处理多尺度的时间模式和通道间的依赖关系。我们的模型能够比现有方法更好地进行时间序列分析。 |
| [^340] | [SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents.](http://arxiv.org/abs/2310.11667) | SOTOPIA是一个用于评估语言智能中的社交智能的交互式环境。通过模拟复杂的社交互动，并使用全面的评估框架，我们发现不同模型之间的社交智能存在显著差异，特别是在SOTOPIA-hard情景下。GPT-4在这个子集上的目标完成率较低。 |
| [^341] | [Exploring the Design Space of Diffusion Autoencoders for Face Morphing.](http://arxiv.org/abs/2310.09484) | 这项研究探索了面向人脸变形的扩散自编码器的设计空间，研究了采样算法、逆向DDIM求解器和部分采样的方法。 |
| [^342] | [PhyloGFN: Phylogenetic inference with generative flow networks.](http://arxiv.org/abs/2310.08774) | PhyloGFN是一种基于生成流网络的系统发育推断方法，通过采样复杂的组合结构，能够产生多样且高质量的进化假设，并在边缘似然估计方面具有竞争力。 |
| [^343] | [Kernel-Elastic Autoencoder for Molecular Design.](http://arxiv.org/abs/2310.08685) | 核-弹性自编码器（KAE）是一种在分子设计领域具有增强性能的自监督生成模型。KAE实现了有效生成和准确重构的挑战，具有显著的多样性和最先进的性能。此外，KAE还可以根据特定条件生成分子，并在分子对接应用中表现出优秀的性能。 |
| [^344] | [Data driven modeling of self-similar dynamics.](http://arxiv.org/abs/2310.08282) | 本文介绍了一个多尺度神经网络框架，利用自相似性作为先验知识，对自相似动力系统进行建模。对于确定性动力学，框架可以判断动力学是否自相似；对于不确定性动力学，它可以确定哪个参数集更接近自相似性。方法可以提取与尺度无关的核进行任意尺度的建模，并识别自相似系统中的幂律指数。初步测试表明，方法对Ising模型的临界指数具有理论一致性。 |
| [^345] | [CacheGen: Fast Context Loading for Language Model Applications.](http://arxiv.org/abs/2310.07240) | CacheGen是一种用于语言模型应用的技术，通过对上下文进行压缩来减少LLM的网络获取和处理延迟。 |
| [^346] | [Lemur: Integrating Large Language Models in Automated Program Verification.](http://arxiv.org/abs/2310.04870) | 本论文提出了一种将LLMs和自动推理器结合起来进行自动程序验证的通用方法，并证明了其完备性。这个方法在一些合成和竞争基准上取得了实际的改进。 |
| [^347] | [Local Search GFlowNets.](http://arxiv.org/abs/2310.02710) | 本文提出使用局部搜索训练GFlowNets，通过破坏和重构的方式探索局部邻域，分别由反向和正向策略引导，使得样本偏向高奖励解决方案。 |
| [^348] | [SEA: Sparse Linear Attention with Estimated Attention Mask.](http://arxiv.org/abs/2310.01777) | 提出了SEA方法，可以通过估计注意力掩码实现线性复杂度的稀疏注意力，解决了transformer处理长序列时注意力操作复杂度高的问题，并保持了可解释性。 |
| [^349] | [TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series.](http://arxiv.org/abs/2310.01327) | TACTiS-2是一种改进的多变量时间序列关注联合分布模型，采用了简化的目标函数和线性参数数量，具有更好的训练动态和最先进的性能。 |
| [^350] | [Mathematical structure of perfect predictive reservoir computing for autoregressive type of time series data.](http://arxiv.org/abs/2310.00290) | 这篇论文研究了储备计算在自回归时间序列数据中的数学结构，并揭示了其隐藏的权重矩阵结构，以实现对AR类型时间序列数据的完美预测。 |
| [^351] | [Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models.](http://arxiv.org/abs/2309.15531) | 该论文提出了一种重新思考频道维度的方法，以隔离大型语言模型低位权重量化中的异常值。通过将权重按输入通道内进行量化分组，可以解决激活异常值的问题，并成功地使得低于4位的量化成为可能。 |
| [^352] | [Density Estimation via Measure Transport: Outlook for Applications in the Biological Sciences.](http://arxiv.org/abs/2309.15366) | 通过测度传递方法进行密度估计在生物科学中具有广阔的应用前景，尤其是在处理稀疏数据的情况下，使用稀疏传递映射可以揭示数据中隐藏的信息。 |
| [^353] | [Tactile Estimation of Extrinsic Contact Patch for Stable Placement.](http://arxiv.org/abs/2309.14552) | 本文介绍了一种利用触觉读数推测物体放置稳定性的方法，通过对接触区域的估计可以有效设计机器人的反馈技能，提高机器人的精细操控能力。 |
| [^354] | [K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling.](http://arxiv.org/abs/2309.11093) | 研究者介绍了一种新颖的K-pop歌词翻译数据集，该数据集揭示了K-pop歌词翻译的独特特征，并构建了一个神经歌词翻译模型，强调了专用数据集的重要性。 |
| [^355] | [User Training with Error Augmentation for Electromyogram-based Gesture Classification.](http://arxiv.org/abs/2309.07289) | 该论文研究了一种基于错误增强的肌电信号手势分类的用户训练系统，实验结果表明，相对于基线，使用修改反馈的条件下能够显著提高手势分类准确性和类别区分度。 |
| [^356] | [Using wearable device-based machine learning models to autonomously identify older adults with poor cognition.](http://arxiv.org/abs/2309.07133) | 通过使用可穿戴设备的机器学习模型，可以在老年人正常生活条件下连续监测其认知水平，并能够预测认知能力较差的情况，为早期干预提供了替代方法。 |
| [^357] | [InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation.](http://arxiv.org/abs/2309.06380) | 本研究提出了InstaFlow，一种基于扩散的文本到图像生成方法，将稳定扩散模型转化为一步模型，通过修正的流动方法提高了噪声与图像之间的对应关系，实现了高质量、高速度的图像生成。 |
| [^358] | [Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood.](http://arxiv.org/abs/2309.05153) | 本文通过协同扩散恢复似然（CDRL）提出了一种方法，用于学习和采样一系列基于能量的模型（EBMs），通过在不断嘈杂化的数据集版本上定义不同噪声水平的EBMs，并与初始化模型配对协同训练。这种方法旨在关闭EBMs和其他生成框架之间的样本质量差距。 |
| [^359] | [ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation Following the Metaphor Identification Procedure.](http://arxiv.org/abs/2309.03103) | ContrastWSD是一种使用了词义消岐的隐喻检测模型，通过将隐喻识别过程和词义消岐结合起来，提取并对比单词的上下文含义和基本含义，以提高隐喻检测的效果，超过其他仅依赖上下文嵌入或集成基本定义和外部知识的方法。 |
| [^360] | [BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial Network.](http://arxiv.org/abs/2309.02836) | 本文研究了利用切片对抗网络（SAN）来增强基于GAN的神经声码器，并通过实验证明了通过修改最小二乘GAN的损失函数，SAN可以改善声码器的性能。 |
| [^361] | [Separable Hamiltonian Neural Networks.](http://arxiv.org/abs/2309.01069) | 这篇论文介绍了可分离哈密顿神经网络的应用，它通过嵌入可加性分离性来解决高维哈密顿系统中的复杂性问题。 |
| [^362] | [A Huber Loss Minimization Approach to Byzantine Robust Federated Learning.](http://arxiv.org/abs/2308.12581) | 本文介绍了一种基于Huber损失最小化的新型聚合器用于拜占庭鲁棒的联邦学习。在独立同分布假设下，该方法具有与现有方法相比的优势，并对非i.i.d数据进行了扩展分析。 |
| [^363] | [A Survey for Federated Learning Evaluations: Goals and Measures.](http://arxiv.org/abs/2308.11841) | 本研究调查了联邦学习的评估目标和指标，介绍了一个开源平台FedEval，提供了标准化的评估框架，为联邦学习算法的效用、效率和安全性方面进行评估，并讨论了该领域面临的挑战和未来研究方向。 |
| [^364] | [ALI-DPFL: Differentially Private Federated Learning with Adaptive Local Iterations.](http://arxiv.org/abs/2308.10457) | ALI-DPFL是一种进行差分隐私联邦学习的算法，通过自适应本地迭代来优化性能，并在实验中展示了显著的改进。 |
| [^365] | [Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis.](http://arxiv.org/abs/2308.07942) | 基于图神经网络和规则的归纳知识图谱补全研究了基于规则的方法在实践中的表现不佳的原因，发现不合理的实体没有排名和只考虑最具信息量的路径是影响因素。提出了一些解决这些问题的规则方法的变体，发现其性能接近于基于图神经网络的方法NBFNet。这些变体仅使用了NBFNet所依赖的证据的一小部分。 |
| [^366] | [Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity.](http://arxiv.org/abs/2308.01562) | 本研究提出了一种剪枝增强的分层联邦学习（PHFL）方法，用于解决无线网络中的带宽稀缺和系统异构性问题。通过模型剪枝和无线通信的优化，实现了在严格的延迟和能耗约束下收敛速度的最优化。 |
| [^367] | [A Call to Reflect on Evaluation Practices for Age Estimation: Comparative Analysis of the State-of-the-Art and a Unified Benchmark.](http://arxiv.org/abs/2307.04570) | 该论文提出反思评估年龄估计实践的呼吁，对现有技术进行了比较分析，并提出了统一基准。研究发现当前评估协议存在问题，并描述了如何解决它们。通过比较分析，发现方法之间的性能差异微不足道，而其他因素的影响更大。研究利用得到的见解提出使用FaRL方法来解决这些问题。 |
| [^368] | [Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging.](http://arxiv.org/abs/2306.16788) | 本研究通过将多个经过迭代幅度剪枝的模型进行平均，解决了同时利用稀疏性和参数平均的问题，并显著提升了泛化性能。 |
| [^369] | [Samplet basis pursuit.](http://arxiv.org/abs/2306.10180) | 本文提出了基于Samplet坐标下核学习的方法，其中引入l1正则化项可以增加系数的稀疏性。相比于单尺度基，Samplet基可以更好地表示更多类型的信号。作者提出了使用软阈值和半光滑牛顿法解决该问题的方法，并通过实验证明了其优越性。 |
| [^370] | [Differentially Private Conditional Independence Testing.](http://arxiv.org/abs/2306.06721) | 本文介绍了两个差分隐私条件独立性检验方法，可适用于Z为连续值的一般情况。 |
| [^371] | [Migrate Demographic Group For Fair GNNs.](http://arxiv.org/abs/2306.04212) | 该论文提出了一个名为FairMigration的新框架，可以动态迁移族群，而不是用原始的敏感属性来固定族群，以训练公平的GNN。 |
| [^372] | [Bootstrapped Training of Score-Conditioned Generator for Offline Design of Biological Sequences.](http://arxiv.org/abs/2306.03111) | 本文提出了一种BootGen算法，使用代理得分函数增强生物序列生成器的训练数据集，并产生多样化的设计，将其应用于优化生物序列，取得了比竞争对手更好的结果。 |
| [^373] | [DiffusionNAG: Predictor-guided Neural Architecture Generation with Diffusion Models.](http://arxiv.org/abs/2305.16943) | DiffusionNAG是一种基于扩散模型的神经结构生成方法，通过考虑神经结构的有向图特性，并结合参数化的预测器的指导，可以更高效地生成具有期望特性的任务最优结构。 |
| [^374] | [Counterfactual Learning on Graphs: A Survey.](http://arxiv.org/abs/2304.01391) | 本文综述了图上反事实学习的研究进展，包括反事实公平性、可解释性、链路预测等不同应用问题，并提出了未来的研究方向。 |
| [^375] | [Investigating and Mitigating the Side Effects of Noisy Views in Multi-view Clustering in Practical Scenarios.](http://arxiv.org/abs/2303.17245) | 本文提出了一种理论上基础的深度MvC方法（MvCAN），旨在解决实际场景中嘈杂视图的问题，通过实现多视图一致性、互补性和噪声鲁棒性来减少嘈杂视图的副作用，并在实验证明该方法优于现有的MvC方法。 |
| [^376] | [Sparse joint shift in multinomial classification.](http://arxiv.org/abs/2303.16971) | 该论文提出了一种稀疏联合偏移模型，用于解决整体数据集偏移问题，提供了传递SJS、修正类后验概率、SJS的可辨认性、SJS与协变量转移关系等新结果。 |
| [^377] | [FrankenSplit: Saliency Guided Neural Feature Compression with Shallow Variational Bottleneck Injection.](http://arxiv.org/abs/2302.10681) | 本文提出了一种基于显著性指导的神经特征压缩与浅层变分瓶颈注入的新的资源意识压缩模型的框架，实现了比最先进的SC方法低60％的比特率，并且比现有的编解码标准的下放快16倍。 |
| [^378] | [Federated Auto-weighted Domain Adaptation.](http://arxiv.org/abs/2302.05049) | 这篇论文提出了一种用于联邦领域自适应的新聚合规则-联邦梯度投影。在此基础上，开发了一个自动加权方案，用于最优地结合源和目标梯度，以解决在数据稀缺和领域转移时常见的技术失败的问题。 |
| [^379] | [Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++.](http://arxiv.org/abs/2301.11118) | Box$^2$EL方法通过将概念和角色表示为盒子，克服了传统方法中角色表示受限的问题，并在实验中取得了领先的结果。 |
| [^380] | [Graph Neural Networks can Recover the Hidden Features Solely from the Graph Structure.](http://arxiv.org/abs/2301.10956) | 本文研究了图神经网络是否可以利用图结构从而实现对潜在特征的恢复，并表明GNN可以完全利用图结构。 |
| [^381] | [Development and Evaluation of a Learning-based Model for Real-time Haptic Texture Rendering.](http://arxiv.org/abs/2212.13332) | 该论文开发了一个基于学习的模型，用于实时触觉质地渲染，在多个用户的研究中评估了其感知性能。这个模型可以推广到各种质地和用户交互的变化，并使用视觉触觉传感器的数据进行实时渲染。 |
| [^382] | [Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation.](http://arxiv.org/abs/2212.06370) | 本文提出了一种双重精度质量驱动的神经网络，可以自动地学习基于回归的神经网络的预测区间，而非只提供传统的目标估计。该方法通过设计一种新颖的损失函数，最小化平均预测区间宽度以及提高覆盖概率来提高PI的质量和精度，且比最先进的方法更加具有计算效率。 |
| [^383] | [Exponential Concentration of Stochastic Approximation with Non-vanishing Gradient.](http://arxiv.org/abs/2208.07243) | 本研究分析了非零梯度的随机逼近算法的行为，并证明了指数级的集中性界限，这对于投影随机梯度下降算法的收敛速度有重要意义。 |
| [^384] | [EVOTER: Evolution of Transparent Explainable Rule-sets.](http://arxiv.org/abs/2204.10438) | EVOTER使用简单的逻辑表达式演化出透明可解释的规则集，与黑盒模型性能相似，可以揭示数据中的偏见并为未来构建可靠的AI系统提供基础。 |
| [^385] | [Tight Convergence Rate Bounds for Optimization Under Power Law Spectral Conditions.](http://arxiv.org/abs/2202.00992) | 本文提出了一种新的谱条件，用于提供具有幂律优化轨迹的问题的更紧密上界，演示了如何统一获得最优加速方法及其计划和收敛上界。 |
| [^386] | [Decentralized Multi-Armed Bandits Can Outperform Centralized Upper Confidence Bound Algorithms.](http://arxiv.org/abs/2111.10933) | 本文研究了一个多智能体网络中的分布式多臂赌博机问题，提出了两种完全分布式的算法，基于经典的UCB算法和KL-UCB算法，实验证明这些算法能达到更好的对数渐近后悔，智能体之间的邻居关系越多，后悔值越好。 |
| [^387] | [CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity.](http://arxiv.org/abs/1902.05605) | CrossQ是一种轻量级算法，通过巧妙运用批归一化和删除目标网络的方式，提高了深度强化学习的样本效率，减少了计算成本，并且实施简单。 |

# 详细

[^1]: ExtremeCast: 提升全球天气预报的极值预测能力

    ExtremeCast: Boosting Extreme Value Prediction for Global Weather Forecast

    [https://rss.arxiv.org/abs/2402.01295](https://rss.arxiv.org/abs/2402.01295)

    ExtremeCast提出了一种新的损失函数Exloss，实现了针对极值的准确预测，同时引入了无需训练的极值增强策略ExEnsemble，提高了预报的稳健性

    

    基于机器学习的数据驱动天气预报在全球中期预报中已经得到了快速发展，并且相较于传统的基于物理的动力学模型表现出更好的性能。然而，大多数这些机器学习模型在准确预测极端天气方面存在困难，而极端值预测与此密切相关。通过数学分析，我们证明使用对称损失，如均方误差（MSE），会导致预测有偏差并低估极值。为了解决这个问题，我们引入了Exloss，一种新的损失函数，通过非对称优化突出极值，以获得准确的极端天气预报。此外，我们还引入了一种无需训练的极值增强策略ExEnsemble，它增加了像素值的方差，并提高了预报的稳健性。结合先进的全球天气预报模型，广泛的实验证明了我们的方法

    Data-driven weather forecast based on machine learning (ML) has experienced rapid development and demonstrated superior performance in the global medium-range forecast compared to traditional physics-based dynamical models. However, most of these ML models struggle with accurately predicting extreme weather, which is closely related to the extreme value prediction. Through mathematical analysis, we prove that the use of symmetric losses, such as the Mean Squared Error (MSE), leads to biased predictions and underestimation of extreme values. To address this issue, we introduce Exloss, a novel loss function that performs asymmetric optimization and highlights extreme values to obtain accurate extreme weather forecast. Furthermore, we introduce a training-free extreme value enhancement strategy named ExEnsemble, which increases the variance of pixel values and improves the forecast robustness. Combined with an advanced global weather forecast model, extensive experiments show that our sol
    
[^2]: Calib3D：校准模型偏好以实现可靠的3D场景理解

    Calib3D: Calibrating Model Preferences for Reliable 3D Scene Understanding

    [https://arxiv.org/abs/2403.17010](https://arxiv.org/abs/2403.17010)

    Calib3D是一个从不确定性估计的角度出发，对多个3D场景理解模型进行了全面评估，发现现有模型虽然准确但不可靠，从而阐明了安全关键的背景下的重要性。

    

    安全关键的3D场景理解任务需要的不仅仅是准确的预测，还需要来自3D感知模型的自信预测。本研究推出了Calib3D，这是一项开创性的工作，旨在从不确定性估计的角度基准和审查3D场景理解模型的可靠性。我们全面评估了28个最先进的模型在10个不同的3D数据集上，揭示了能够处理3D场景理解中的误差不确定性和认知不确定性的有见地的现象。我们发现，尽管现有模型取得了令人印象深刻的准确度水平，但它们经常无法提供可靠的不确定性估计 -- 这个关键的缺陷严重损害了它们在安全敏感环境中的适用性。通过对关键因素（如网络容量、LiDAR表示、光栅分辨率和3D数据增强技术）进行了广泛分析，我们直接将这些方面与模型校准相关联。

    arXiv:2403.17010v1 Announce Type: cross  Abstract: Safety-critical 3D scene understanding tasks necessitate not only accurate but also confident predictions from 3D perception models. This study introduces Calib3D, a pioneering effort to benchmark and scrutinize the reliability of 3D scene understanding models from an uncertainty estimation viewpoint. We comprehensively evaluate 28 state-of-the-art models across 10 diverse 3D datasets, uncovering insightful phenomena that cope with both the aleatoric and epistemic uncertainties in 3D scene understanding. We discover that despite achieving impressive levels of accuracy, existing models frequently fail to provide reliable uncertainty estimates -- a pitfall that critically undermines their applicability in safety-sensitive contexts. Through extensive analysis of key factors such as network capacity, LiDAR representations, rasterization resolutions, and 3D data augmentation techniques, we correlate these aspects directly with the model cal
    
[^3]: 语言校正流：通过概率流推动扩散语言生成

    Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows

    [https://arxiv.org/abs/2403.16995](https://arxiv.org/abs/2403.16995)

    语言校正流是一种基于标准概率流模型的新方法，通过学习常微分方程模型在源分布和目标分布之间传输，提供了统一和有效的生成模型和领域转移解决方案。

    

    最近的研究表明，在扩散语言模型基础上控制句子属性（例如情感）和结构（例如句法结构）取得了成功。一个推动高质量样本生成的关键组成部分是迭代去噪数千步。尽管有益，但从噪声开始的复杂性和学习步骤限制了其在许多NLP实际应用中的实现。本文提出了Language Rectified Flow方法。我们的方法基于标准概率流模型的重构。语言校正流学习（神经）常微分方程模型在源分布和目标分布之间传输，为生成建模和域转移提供了统一和有效的解决方案。从源分布开始，我们的语言校正流产生快速仿真和有效。

    arXiv:2403.16995v1 Announce Type: cross  Abstract: Recent works have demonstrated success in controlling sentence attributes ($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the diffusion language model. A key component that drives theimpressive performance for generating high-quality samples from noise is iteratively denoise for thousands of steps. While beneficial, the complexity of starting from the noise and the learning steps has limited its implementation to many NLP real-world applications. This paper proposes Language Rectified Flow ({\ours}). Our method is based on the reformulation of the standard probabilistic flow models. Language rectified flow learns (neural) ordinary differential equation models to transport between the source distribution and the target distribution, hence providing a unified and effective solution to generative modeling and domain transfer. From the source distribution, our language rectified flow yields fast simulation and effe
    
[^4]: 做自己：多主体文本到图像生成的有界注意力

    Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation

    [https://arxiv.org/abs/2403.16990](https://arxiv.org/abs/2403.16990)

    该论文研究了多主体文本到图像生成中的有界注意力方法，解决了扩散模型注意力层混合不同主体视觉特征导致的语义泄漏问题。

    

    arXiv:2403.16990v1 公告类型：跨主体文本到图像扩散模型具有生成多样且高质量图像的前所未有能力。然而，它们常常难以忠实地捕捉包含多个主题的复杂输入提示的预期语义。最近，许多布局到图像的扩展已被引入以提高用户控制，旨在定位由特定令牌表示的主题。然而，这些方法通常会产生语义不准确的图像，特别是在处理多个在语义上或视觉上相似的主题时。在这项工作中，我们研究并分析了这些限制的原因。我们的探索揭示了这个主要问题起源于去噪过程中主题之间的无意义语义泄漏。这种泄漏归因于扩散模型的注意力层，这些层倾向于混合不同主体的视觉特征。为了解决这些问题，我们引入了有界注意力，一种无需训练的方法

    arXiv:2403.16990v1 Announce Type: cross  Abstract: Text-to-image diffusion models have an unprecedented ability to generate diverse and high-quality images. However, they often struggle to faithfully capture the intended semantics of complex input prompts that include multiple subjects. Recently, numerous layout-to-image extensions have been introduced to improve user control, aiming to localize subjects represented by specific tokens. Yet, these methods often produce semantically inaccurate images, especially when dealing with multiple semantically or visually similar subjects. In this work, we study and analyze the causes of these limitations. Our exploration reveals that the primary issue stems from inadvertent semantic leakage between subjects in the denoising process. This leakage is attributed to the diffusion model's attention layers, which tend to blend the visual features of different subjects. To address these issues, we introduce Bounded Attention, a training-free method for
    
[^5]: 面向目标导向语义通信的动态相对表示

    Dynamic Relative Representations for Goal-Oriented Semantic Communications

    [https://arxiv.org/abs/2403.16986](https://arxiv.org/abs/2403.16986)

    本文提出了一个新颖的面向目标的语义通信框架，利用相对表示通过潜在空间对齐来缓解语义不匹配，并实现了能效高、延迟低的目标导向语义通信。

    

    在未来的6G无线网络中，通信的语义和有效性方面将发挥基础作用，将含义和相关性纳入传输。然而，当设备使用不同的语言、逻辑或内部表示时，会出现障碍，导致语义不匹配，可能危及理解。在潜在空间通信中，这一挑战表现为深度神经网络对数据进行编码时高维表示不匹配。本文提出了一个新颖的面向目标的语义通信框架，利用相对表示来通过潜在空间对齐缓解语义不匹配。我们提出了一种动态优化策略，以调整相对表示、通信参数和计算资源，实现能效高、延迟低的目标导向语义通信。数值结果证明了我们的方法在缓解中起作用的有效性。

    arXiv:2403.16986v1 Announce Type: cross  Abstract: In future 6G wireless networks, semantic and effectiveness aspects of communications will play a fundamental role, incorporating meaning and relevance into transmissions. However, obstacles arise when devices employ diverse languages, logic, or internal representations, leading to semantic mismatches that might jeopardize understanding. In latent space communication, this challenge manifests as misalignment within high-dimensional representations where deep neural networks encode data. This paper presents a novel framework for goal-oriented semantic communication, leveraging relative representations to mitigate semantic mismatches via latent space alignment. We propose a dynamic optimization strategy that adapts relative representations, communication parameters, and computation resources for energy-efficient, low-latency, goal-oriented semantic communications. Numerical results demonstrate our methodology's effectiveness in mitigating
    
[^6]: 自我STORM：用于超分辨率显微镜的深度展开自监督学习

    Self-STORM: Deep Unrolled Self-Supervised Learning for Super-Resolution Microscopy

    [https://arxiv.org/abs/2403.16974](https://arxiv.org/abs/2403.16974)

    介绍了深度展开自监督学习用于超分辨率显微镜，通过训练模型自编码器从给定测量中学习，减少对大量训练数据的需求

    

    利用荧光分子创建一长序列低密度、衍射限制图像，能实现高精度分子定位。然而，这种方法需要漫长的成像时间，限制了在短时间尺度上观察活细胞的动态相互作用。许多技术被开发用来减少定位所需帧数，从经典迭代优化到深度神经网络。特别地，深度算法展开结合了迭代稀疏恢复算法的结构和监督深度学习的性能提升。然而，这种方法的稳健性高度依赖于有足够的训练数据。本文介绍了深度展开自监督学习，通过训练一个序列特定、基于模型的自编码器，它仅从给定的测量中学习，从而缓解了这种数据需求。我们提出的方法超越了性能

    arXiv:2403.16974v1 Announce Type: cross  Abstract: The use of fluorescent molecules to create long sequences of low-density, diffraction-limited images enables highly-precise molecule localization. However, this methodology requires lengthy imaging times, which limits the ability to view dynamic interactions of live cells on short time scales. Many techniques have been developed to reduce the number of frames needed for localization, from classic iterative optimization to deep neural networks. Particularly, deep algorithm unrolling utilizes both the structure of iterative sparse recovery algorithms and the performance gains of supervised deep learning. However, the robustness of this approach is highly dependant on having sufficient training data. In this paper we introduce deep unrolled self-supervised learning, which alleviates the need for such data by training a sequence-specific, model-based autoencoder that learns only from given measurements. Our proposed method exceeds the perf
    
[^7]: VoiceCraft：野外零-shot语音编辑和文本到语音

    VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild

    [https://arxiv.org/abs/2403.16973](https://arxiv.org/abs/2403.16973)

    VoiceCraft是一个基于标记填充的神经编解码器语言模型，在语音编辑和零-shot文本到语音任务上表现出色，实现了在多样性数据集上的最新性能。

    

    我们介绍了VoiceCraft，一个基于标记填充的神经编解码器语言模型，实现了在有声书、互联网视频和播客上语音编辑和零-shot文本到语音（TTS）方面的最新性能。VoiceCraft采用Transformer解码器架构，并引入了一种标记重排过程，结合了因果掩码和延迟堆叠，以实现在现有序列内的生成。在语音编辑任务上，VoiceCraft生成的编辑语音在自然度方面几乎与未编辑的录音难以区分，经人类评估；对于零-shot TTS，我们的模型优于先前的最先进模型，包括VALLE和流行的商业模型XTTS-v2。关键的是，这些模型在具有多样口音、语音风格、录制条件、背景噪音和音乐的具有挑战性和真实性的数据集上进行了评估，我们的模型与其他模型相比表现始终良好。

    arXiv:2403.16973v1 Announce Type: cross  Abstract: We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models a
    
[^8]: 联合胸部X光诊断和临床视觉注意力预测的多阶段协作学习：增强可解释性

    Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability

    [https://arxiv.org/abs/2403.16970](https://arxiv.org/abs/2403.16970)

    该论文引入了一种新的深度学习框架，用于联合疾病诊断和胸部X光扫描对应视觉显著性图的预测，通过设计新颖的双编码器多任务UNet并利用多尺度特征融合分类器来提高计算辅助诊断的可解释性和质量。

    

    随着深度学习成为计算辅助诊断的最新技术，自动决策的可解释性对临床部署至关重要。尽管在这一领域提出了各种方法，但在放射学筛查过程中临床医生的视觉注意力图为提供重要洞察提供了独特的资产，并有可能提高计算辅助诊断的质量。通过这篇论文，我们引入了一种新颖的深度学习框架，用于联合疾病诊断和胸部X光扫描对应视觉显著性图的预测。具体来说，我们设计了一种新颖的双编码器多任务UNet，利用了DenseNet201主干和基于残差和膨胀激励块的编码器来提取用于显著性图预测的多样特征，并使用多尺度特征融合分类器进行疾病分类。

    arXiv:2403.16970v1 Announce Type: cross  Abstract: As deep learning has become the state-of-the-art for computer-assisted diagnosis, interpretability of the automatic decisions is crucial for clinical deployment. While various methods were proposed in this domain, visual attention maps of clinicians during radiological screening offer a unique asset to provide important insights and can potentially enhance the quality of computer-assisted diagnosis. With this paper, we introduce a novel deep-learning framework for joint disease diagnosis and prediction of corresponding visual saliency maps for chest X-ray scans. Specifically, we designed a novel dual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based encoder to extract diverse features for saliency map prediction, and a multi-scale feature-fusion classifier to perform disease classification. To tackle the issue of asynchronous training schedules of individual tasks
    
[^9]: 用于腿式定点机器人运动操作的视觉全身控制

    Visual Whole-Body Control for Legged Loco-Manipulation

    [https://arxiv.org/abs/2403.16967](https://arxiv.org/abs/2403.16967)

    这项研究提出了一种利用视觉全身控制的框架，使腿式机器人能够同时控制腿部和手臂，以扩展操作能力，并通过仿真训练和Sim2Real转移实现了在捡起不同物体方面取得显著改进。

    

    我们研究了使用配备手臂的腿式机器人进行移动操作的问题，即腿式定点操作。尽管机器人的腿通常用于移动，但通过进行全身控制，可以扩大其操作能力。也就是说，机器人可以同时控制腿部和手臂，以扩展其工作空间。我们提出了一个能够使用视觉观测自主进行全身控制的框架。我们的方法称为\ourFull~(\our)，由一个低级策略和一个高级策略组成。低级策略使用所有自由度来跟踪末端执行器的位置，高级策略根据视觉输入提出末端执行器位置。我们在仿真中训练了两个级别的策略，并进行了从Sim到实物的转移以进行实际机器人部署。我们进行了大量实验证明，在不同配置下（高度、）捡起不同物体方面，相对基线方法取得了显著改进。

    arXiv:2403.16967v1 Announce Type: cross  Abstract: We study the problem of mobile manipulation using legged robots equipped with an arm, namely legged loco-manipulation. The robot legs, while usually utilized for mobility, offer an opportunity to amplify the manipulation capabilities by conducting whole-body control. That is, the robot can control the legs and the arm at the same time to extend its workspace. We propose a framework that can conduct the whole-body control autonomously with visual observations. Our approach, namely \ourFull~(\our), is composed of a low-level policy using all degrees of freedom to track the end-effector manipulator position and a high-level policy proposing the end-effector position based on visual inputs. We train both levels of policies in simulation and perform Sim2Real transfer for real robot deployment. We perform extensive experiments and show significant improvements over baselines in picking up diverse objects in different configurations (heights,
    
[^10]: 数据混合规律：通过预测语言建模性能来优化数据混合

    Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance

    [https://arxiv.org/abs/2403.16952](https://arxiv.org/abs/2403.16952)

    该研究发现了数据混合规律，可以量化地预测模型性能与数据混合比例之间的关系，并提出了一种方法来通过拟合函数形式来引导理想的数据混合选择，从而优化大型语言模型的训练混合。

    

    大型语言模型的预训练数据包括多个领域（例如网络文本、学术论文、代码），其混合比例对结果模型的能力至关重要。现有的工作通常依赖于启发式方法或定性策略来调整比例，我们发现了模型性能与混合比例之间的函数形式的定量可预测性，我们称之为数据混合规律。在样本混合上拟合这种函数揭示了未见混合的模型性能，从而引导选择理想的数据混合。此外，我们提出了训练步骤、模型大小和我们的数据混合规律的缩放规律的嵌套使用，以使得仅通过小规模训练就能够预测在各种混合数据下训练的大模型的性能。此外，实验结果验证了我们的方法有效地优化了训练混合。

    arXiv:2403.16952v1 Announce Type: cross  Abstract: Pretraining data of large language models composes multiple domains (e.g., web texts, academic papers, codes), whose mixture proportions crucially impact the competence of outcome models. While existing endeavors rely on heuristics or qualitative strategies to tune the proportions, we discover the quantitative predictability of model performance regarding the mixture proportions in function forms, which we refer to as the data mixing laws. Fitting such functions on sample mixtures unveils model performance on unseen mixtures before actual runs, thus guiding the selection of an ideal data mixture. Furthermore, we propose nested use of the scaling laws of training steps, model sizes, and our data mixing law to enable predicting the performance of large models trained on massive data under various mixtures with only small-scale training. Moreover, experimental results verify that our method effectively optimizes the training mixture of a 
    
[^11]: 与人类判断相一致：大型语言模型评估中成对偏好的作用

    Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators

    [https://arxiv.org/abs/2403.16950](https://arxiv.org/abs/2403.16950)

    在大型语言模型评估中，通过引入成对偏好搜索方法PAIRS，成功解决了LLMs与人类判断不一致的问题，并取得了优于直接打分的最先进性能。

    

    大型语言模型（LLMs）作为自动评估器在评估生成的自然语言质量方面表现出有希望的能力。然而，LLMs在评估中仍存在偏见，常常难以生成与人类评估一致的连贯评估。在这项工作中，我们首先对LLM评估器与人类判断之间的不一致进行系统研究，揭示现有旨在减轻偏见的校准方法不足以有效将LLM评估器对齐。受到RLHF中对偏好数据的使用的启发，我们将评估形式化为一个排序问题，并引入Pairwise-preference Search（PAIRS），这是一种以LLMs进行成对比较并有效对候选文本进行排序的基于不确定性引导的搜索方法。PAIRS在代表性评估任务上实现了最先进的性能，并且显示出比直接打分有显著改进。

    arXiv:2403.16950v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PAIRS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthe
    
[^12]: 通过空间、时间和大脑进行反向传播

    Backpropagation through space, time, and the brain

    [https://arxiv.org/abs/2403.16933](https://arxiv.org/abs/2403.16933)

    提出了 Generalized Latent Equilibrium (GLE)，它是一种针对神经元网络的物理动态局部时空信用分配的计算框架。

    

    有效的神经网络学习需要根据它们对解决任务的相对贡献来调整单个突触。然而，无论是生物还是人工的物理神经系统都受到时空局限。这样的网络如何执行高效的信用分配，在很大程度上仍是一个悬而未决的问题。在机器学习中，错误的反向传播算法几乎普遍被空间（BP）和时间（BPTT）两种方式给出答案。然而，BP(TT)被广泛认为依赖于不具生物学意义的假设，特别是关于时空局限性，而正向传播模型，如实时递归学习（RTRL），则受到内存约束的限制。我们引入了广义潜在平衡（GLE），这是一个针对神经元物理动态网络完全局部时空信用分配的计算框架。我们从

    arXiv:2403.16933v1 Announce Type: cross  Abstract: Effective learning in neuronal networks requires the adaptation of individual synapses given their relative contribution to solving a task. However, physical neuronal systems -- whether biological or artificial -- are constrained by spatio-temporal locality. How such networks can perform efficient credit assignment, remains, to a large extent, an open question. In Machine Learning, the answer is almost universally given by the error backpropagation algorithm, through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely on biologically implausible assumptions, in particular with respect to spatiotemporal (non-)locality, while forward-propagation models such as real-time recurrent learning (RTRL) suffer from prohibitive memory constraints. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by 
    
[^13]: FLIGAN: 使用 GAN 改进不完整数据的联邦学习

    FLIGAN: Enhancing Federated Learning with Incomplete Data using GAN

    [https://arxiv.org/abs/2403.16930](https://arxiv.org/abs/2403.16930)

    FLIGAN利用生成对抗网络（GAN）来生成合成数据，解决联邦学习中不完整数据的问题，提升模型的鲁棒性和完整性。

    

    联邦学习提供了一种隐私保护机制，用于在网络化设备（如移动设备、物联网边缘节点）上对机器学习模型进行分布式训练。它通过在网络上不共享实际数据来实现边缘的人工智能。现有研究通常侧重于非独立同分布数据的通用方面和客户系统特征的异质性，但他们常常忽视模型开发中不足的数据问题，这可能来自边缘节点上不均匀的类别标签分布，以及高度可变的数据容量。在这项工作中，我们提出了一种名为FLIGAN的创新方法来解决联邦学习中数据不完整性的问题。首先，我们利用生成对抗网络（GAN）来敏锐捕捉复杂数据分布，并生成与真实数据紧密相似的合成数据。然后，我们使用合成数据来增强模型的鲁棒性和完整性。

    arXiv:2403.16930v1 Announce Type: new  Abstract: Federated Learning (FL) provides a privacy-preserving mechanism for distributed training of machine learning models on networked devices (e.g., mobile devices, IoT edge nodes). It enables Artificial Intelligence (AI) at the edge by creating models without sharing the actual data across the network. Existing research works typically focus on generic aspects of non-IID data and heterogeneity in client's system characteristics, but they often neglect the issue of insufficient data for model development, which can arise from uneven class label distribution and highly variable data volumes across edge nodes. In this work, we propose FLIGAN, a novel approach to address the issue of data incompleteness in FL. First, we leverage Generative Adversarial Networks (GANs) to adeptly capture complex data distributions and generate synthetic data that closely resemble the real-world data. Then, we use synthetic data to enhance the robustness and comple
    
[^14]: 从启发式到理论：SCOD

    SCOD: From Heuristics to Theory

    [https://arxiv.org/abs/2403.16916](https://arxiv.org/abs/2403.16916)

    本文提出了针对SCOD问题的最优策略，包括基于贝叶斯分类器和随机线性分类器的选择器，以解决不确定或离群样本预测可靠性问题。

    

    本文解决了设计可靠的预测模型，当面对不确定或离群样本时避免预测的问题 - 一种最近提出的被称为Selective Classification in the presence of Out-of-Distribution data (SCOD)的问题。我们对SCOD做出了三个关键贡献。首先，我们证明了最优的SCOD策略涉及基于贝叶斯分类器进行内部分布（ID）数据和在2D空间中表示为随机线性分类器的选择器，使用ID分类器的条件风险和ID与离群分布（OOD）数据的似然比作为输入。这与当前OOD检测方法和专为SCOD开发的Softmax Information Retaining Combination (SIRC)的次优策略形成对比。其次，我们建立了在一个无分布设置中，当仅依赖于条件分布和IID样本的逼近可能性时，SCOD问题不可能被正确学习。

    arXiv:2403.16916v1 Announce Type: new  Abstract: This paper addresses the problem of designing reliable prediction models that abstain from predictions when faced with uncertain or out-of-distribution samples - a recently proposed problem known as Selective Classification in the presence of Out-of-Distribution data (SCOD). We make three key contributions to SCOD. Firstly, we demonstrate that the optimal SCOD strategy involves a Bayes classifier for in-distribution (ID) data and a selector represented as a stochastic linear classifier in a 2D space, using i) the conditional risk of the ID classifier, and ii) the likelihood ratio of ID and out-of-distribution (OOD) data as input. This contrasts with suboptimal strategies from current OOD detection methods and the Softmax Information Retaining Combination (SIRC), specifically developed for SCOD. Secondly, we establish that in a distribution-free setting, the SCOD problem is not Probably Approximately Correct learnable when relying solely 
    
[^15]: 利用预训练语言模型进行粗调优的专题文档检索

    Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models

    [https://arxiv.org/abs/2403.16915](https://arxiv.org/abs/2403.16915)

    本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调，在专题文档检索中显著改善了效果。

    

    在信息检索系统中，利用预训练语言模型（PLM-based IR）进行微调需要学习查询表示和查询-文档关系，除了下游任务特定的学习。本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调。通过在粗调优学习查询表示和查询-文档关系，我们旨在减少微调的负担，提高下游IR任务的学习效果。我们提出了用于粗调优的查询-文档对预测（QDPP），其预测查询-文档对的适当性。评估实验显示，所提出的方法显著改善了四个专题文档检索数据集中的MRR和/或nDCG@5。此外，查询预测任务的结果表明，粗调优促进了查询表示和查询-文档关系的学习。

    arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
    
[^16]: 基于状态空间模型的基础模型：一个控制理论概述

    State Space Models as Foundation Models: A Control Theoretic Overview

    [https://arxiv.org/abs/2403.16899](https://arxiv.org/abs/2403.16899)

    将状态空间模型整合到深度神经网络架构中，为控制理论家和研究人员提供了一种有效建模动态系统的新途径。

    

    近年来，将线性状态空间模型（SSM）整合到基础模型的深度神经网络架构中引起了越来越多的关注。最近Mamba的成功展示了比现有最先进的Transformer架构在语言任务中表现更好。基础模型，如GPT-4，旨在将序列数据编码为潜在空间，以学习数据的压缩表示。控制理论家使用SSM来有效地建模动态系统追求相同的目标。因此，SSM可以自然地与深度序列建模相连接，提供了在相应研究领域之间创造协同作用的机会。本文旨在向控制理论家简要介绍基于SSM的架构，并总结最新的研究进展。它系统回顾了最成功的SSM提议，并突出了它们的m

    arXiv:2403.16899v1 Announce Type: cross  Abstract: In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their m
    
[^17]: 带扩散桥的离散潜在图生成建模

    Discrete Latent Graph Generative Modeling with Diffusion Bridges

    [https://arxiv.org/abs/2403.16883](https://arxiv.org/abs/2403.16883)

    GLAD是一个在离散潜在空间上操作的图生成模型，通过适应扩散桥结构学习其离散潜在空间的先验，避免了依赖于原始数据空间的分解，在图生成任务中表现出优越性。

    

    学习潜在空间中的图生成模型相比于在原始数据空间上操作的模型受到较少关注，迄今表现出的性能乏善可陈。我们提出了GLAD，一个潜在空间图生成模型。与大多数先前的潜在空间图生成模型不同，GLAD在保留图结构的离散性质方面运行，无需进行诸如潜在空间连续性等不自然的假设。我们通过将扩散桥调整到其结构，来学习我们离散潜在空间的先验。通过在适当构建的潜在空间上操作，我们避免依赖于常用于在原始数据空间操作的模型中的分解。我们在一系列图基准数据集上进行实验，明显展示了离散潜在空间的优越性，并取得了最先进的图生成性能，使GLA

    arXiv:2403.16883v1 Announce Type: new  Abstract: Learning graph generative models over latent spaces has received less attention compared to models that operate on the original data space and has so far demonstrated lacklustre performance. We present GLAD a latent space graph generative model. Unlike most previous latent space graph generative models, GLAD operates on a discrete latent space that preserves to a significant extent the discrete nature of the graph structures making no unnatural assumptions such as latent space continuity. We learn the prior of our discrete latent space by adapting diffusion bridges to its structure. By operating over an appropriately constructed latent space we avoid relying on decompositions that are often used in models that operate in the original data space. We present experiments on a series of graph benchmark datasets which clearly show the superiority of the discrete latent space and obtain state of the art graph generative performance, making GLA
    
[^18]: 感知力就是你所需要的：北方森林的地形分类

    Proprioception Is All You Need: Terrain Classification for Boreal Forests

    [https://arxiv.org/abs/2403.16877](https://arxiv.org/abs/2403.16877)

    通过引入 BorealTC 数据集，结合现有数据集，我们评估了基于卷积神经网络（CNN）和新颖的状态空间模型（SSM）-Mamba体系结构在北方森林地形分类上的表现。

    

    最近的领域机器人学研究强调了抵御不同类型地形的重要性。北方森林特别受到许多限制机动性的地形的影响，这些地形应该在越野自主导航中加以考虑。此外，作为地球上最大的陆地生物群落之一，北方森林是预计自主车辆将日益普及的地区。在本文中，我们通过引入BorealTC来解决这个问题，这是一个用于基于感知力的地形分类（TC）的公开可用数据集。我们的数据集记录了Husky A200的116分钟的惯性测量单元（IMU）、电机电流和轮胎里程数据，重点关注典型的北方森林地形，特别是雪、冰和淤泥壤。结合我们的数据集与另一个来自最新技术的数据集，我们在TC t

    arXiv:2403.16877v1 Announce Type: cross  Abstract: Recent works in field robotics highlighted the importance of resiliency against different types of terrains. Boreal forests, in particular, are home to many mobility-impeding terrains that should be considered for off-road autonomous navigation. Also, being one of the largest land biomes on Earth, boreal forests are an area where autonomous vehicles are expected to become increasingly common. In this paper, we address this issue by introducing BorealTC, a publicly available dataset for proprioceptive-based terrain classification (TC). Recorded with a Husky A200, our dataset contains 116 min of Inertial Measurement Unit (IMU), motor current, and wheel odometry data, focusing on typical boreal forest terrains, notably snow, ice, and silty loam. Combining our dataset with another dataset from the state-of-the-art, we evaluate both a Convolutional Neural Network (CNN) and the novel state space model (SSM)-based Mamba architecture on a TC t
    
[^19]: 多智能体系统的一致离策略预测

    Conformal Off-Policy Prediction for Multi-Agent Systems

    [https://arxiv.org/abs/2403.16871](https://arxiv.org/abs/2403.16871)

    这项工作介绍了MA-COPP，这是第一个解决涉及多智能体系统的离策略预测问题的一致预测方法。

    

    离策略预测（OPP），即仅使用在一个正常（行为）策略下收集的数据来预测目标策略的结果，在数据驱动的安全关键系统分析中是一个重要问题，在这种系统中，部署新策略可能是不安全的。为了实现可信的离策略预测，最近关于一致离策略预测（COPP）的工作利用一致预测框架来在目标过程下推导带有概率保证的预测区域。现有的COPP方法可以考虑由策略切换引起的分布偏移，但仅限于单智能体系统和标量结果（例如，奖励）。在这项工作中，我们介绍了MA-COPP，这是第一个解决涉及多智能体系统的OPP问题的一致预测方法，在一个或多个“自我”智能体改变策略时为所有智能体轨迹推导联合预测区域。与单智能体场景不同，这种情况下

    arXiv:2403.16871v1 Announce Type: cross  Abstract: Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy using only data collected under a nominal (behavioural) policy, is a paramount problem in data-driven analysis of safety-critical systems where the deployment of a new policy may be unsafe. To achieve dependable off-policy predictions, recent work on Conformal Off-Policy Prediction (COPP) leverage the conformal prediction framework to derive prediction regions with probabilistic guarantees under the target process. Existing COPP methods can account for the distribution shifts induced by policy switching, but are limited to single-agent systems and scalar outcomes (e.g., rewards). In this work, we introduce MA-COPP, the first conformal prediction method to solve OPP problems involving multi-agent systems, deriving joint prediction regions for all agents' trajectories when one or more "ego" agents change their policies. Unlike the single-agent scenario, this se
    
[^20]: INPC：用于辐射场渲染的隐式神经点云

    INPC: Implicit Neural Point Clouds for Radiance Field Rendering

    [https://arxiv.org/abs/2403.16862](https://arxiv.org/abs/2403.16862)

    提出了一种新颖的隐式点云表示方法，结合了连续八叉树概率场和多分辨率哈希网格，实现了快速渲染和保留细致几何细节的优势，并且在几个常见基准数据集上实现了最先进的图像质量。

    

    我们引入了一种新的方法，用于重建和合成无边界的现实世界场景。与以往使用体积场、基于网格的模型或离散点云代理的方法相比，我们提出了一种混合场景表示，它在连续八叉树概率场和多分辨率哈希网格中隐含地编码点云。通过这样做，我们结合了两个世界的优势，保留了在优化过程中有利的行为：我们的新颖隐式点云表示和可微的双线性光栅化器实现了快速渲染，同时保留了细微的几何细节，而无需依赖于像结构运动点云这样的初始先验。我们的方法在几个常见基准数据集上实现了最先进的图像质量。此外，我们实现了快速推理，可交互帧速率，并且可以提取显式点云以进一步提高性能。

    arXiv:2403.16862v1 Announce Type: cross  Abstract: We introduce a new approach for reconstruction and novel-view synthesis of unbounded real-world scenes. In contrast to previous methods using either volumetric fields, grid-based models, or discrete point cloud proxies, we propose a hybrid scene representation, which implicitly encodes a point cloud in a continuous octree-based probability field and a multi-resolution hash grid. In doing so, we combine the benefits of both worlds by retaining favorable behavior during optimization: Our novel implicit point cloud representation and differentiable bilinear rasterizer enable fast rendering while preserving fine geometric detail without depending on initial priors like structure-from-motion point clouds. Our method achieves state-of-the-art image quality on several common benchmark datasets. Furthermore, we achieve fast inference at interactive frame rates, and can extract explicit point clouds to further enhance performance.
    
[^21]: 使用大规模Solidity智能合约数据集推动研究

    DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts

    [https://arxiv.org/abs/2403.16861](https://arxiv.org/abs/2403.16861)

    DISL数据集包含了514,506个部署在以太坊主网上的独特Solidity文件，成为了开发智能合约设计工具和基准测试的重要资源。

    

    DISL数据集包含了514,506个部署在以太坊主网上的独特Solidity文件，满足了大规模、多样化的真实世界智能合约数据集的需求。DISL成为了开发机器学习系统和为智能合约设计的软件工程工具进行基准测试的资源。通过收集截至2024年1月15日在Etherscan上验证的每个智能合约，DISL在规模和时效性上超越了现有数据集。

    arXiv:2403.16861v1 Announce Type: cross  Abstract: The DISL dataset features a collection of $514,506$ unique Solidity files that have been deployed to Ethereum mainnet. It caters to the need for a large and diverse dataset of real-world smart contracts. DISL serves as a resource for developing machine learning systems and for benchmarking software engineering tools designed for smart contracts. By aggregating every verified smart contract from Etherscan up to January 15, 2024, DISL surpasses existing datasets in size and recency.
    
[^22]: 具有语义意识的远程多马尔可夫源估计在约束条件下的研究

    Semantic-Aware Remote Estimation of Multiple Markov Sources Under Constraints

    [https://arxiv.org/abs/2403.16855](https://arxiv.org/abs/2403.16855)

    研究了具有语义意识的远程多马尔可夫源估计的最优调度策略以及开发了一种新颖的策略搜索算法 Insec-RVI。

    

    本文研究了在丢失和受速率限制的通道上对多个马尔可夫源进行语义感知通信的远程估计。与大多数现有研究将所有源状态视为相等不同，我们利用信息的语义并考虑远程执行器对不同状态的估计误差具有不同的容忍度。我们旨在找到一个最优调度策略，以在传输频率约束下最小化估计误差的长期状态相关成本。我们从理论上通过利用平均成本约束马尔可夫决策过程（CMDP）理论和Lagrange动态规划展示了最优策略的结构。通过利用最优结构结果，我们开发了一种新颖的策略搜索算法，称为交叉搜索加相对值迭代（Insec-RVI），可以仅通过少量迭代找到最优策略。为了避免“维度诅咒”...

    arXiv:2403.16855v1 Announce Type: cross  Abstract: This paper studies semantic-aware communication for remote estimation of multiple Markov sources over a lossy and rate-constrained channel. Unlike most existing studies that treat all source states equally, we exploit the semantics of information and consider that the remote actuator has different tolerances for the estimation errors of different states. We aim to find an optimal scheduling policy that minimizes the long-term state-dependent costs of estimation errors under a transmission frequency constraint. We theoretically show the structure of the optimal policy by leveraging the average-cost Constrained Markov Decision Process (CMDP) theory and the Lagrangian dynamic programming. By exploiting the optimal structural results, we develop a novel policy search algorithm, termed intersection search plus relative value iteration (Insec-RVI), that can find the optimal policy using only a few iterations. To avoid the ``curse of dimensio
    
[^23]: ChatGPT是否能够基于Twitter提及来预测文章的撤回？

    Can ChatGPT predict article retraction based on Twitter mentions?

    [https://arxiv.org/abs/2403.16851](https://arxiv.org/abs/2403.16851)

    本研究探讨了ChatGPT是否能够基于Twitter提及来预测文章的撤回，研究发现在预测未来被撤回的有问题文章方面是具有一定潜力的。

    

    检测有问题的研究文章具有重要意义，本研究探讨了根据被撤回文章在Twitter上的提及是否能够在文章被撤回前发出信号，从而在预测未来被撤回的有问题文章方面发挥作用。分析了包括3,505篇已撤回文章及其相关Twitter提及在内的数据集，以及使用粗糙精确匹配方法获取的具有类似特征的3,505篇未撤回文章。通过四种预测方法评估了Twitter提及在预测文章撤回方面的有效性，包括手动标注、关键词识别、机器学习模型和ChatGPT。手动标注的结果表明，的确有被撤回的文章，其Twitter提及包含在撤回前发出信号的可识别证据，尽管它们只占所有被撤回文章的一小部分。

    arXiv:2403.16851v1 Announce Type: cross  Abstract: Detecting problematic research articles timely is a vital task. This study explores whether Twitter mentions of retracted articles can signal potential problems with the articles prior to retraction, thereby playing a role in predicting future retraction of problematic articles. A dataset comprising 3,505 retracted articles and their associated Twitter mentions is analyzed, alongside 3,505 non-retracted articles with similar characteristics obtained using the Coarsened Exact Matching method. The effectiveness of Twitter mentions in predicting article retraction is evaluated by four prediction methods, including manual labelling, keyword identification, machine learning models, and ChatGPT. Manual labelling results indicate that there are indeed retracted articles with their Twitter mentions containing recognizable evidence signaling problems before retraction, although they represent only a limited share of all retracted articles with 
    
[^24]: GreeDy和CoDy：动态图的反事实解释器

    GreeDy and CoDy: Counterfactual Explainers for Dynamic Graphs

    [https://arxiv.org/abs/2403.16846](https://arxiv.org/abs/2403.16846)

    该论文介绍了两种针对动态图的新颖反事实解释方法：GreeDy和CoDy。实验证明，CoDy在寻找重要反事实输入方面表现优异，成功率高达59%。

    

    时间图神经网络（TGNNs）对于建模具有时间变化交互的动态图至关重要，但由于其复杂的模型结构，在可解释性方面面临重大挑战。反事实解释对于理解模型决策至关重要，它研究输入图的变化如何影响结果。本文介绍了两种新颖的 TGNNs 反事实解释方法：GreeDy（动态图的贪心解释器）和 CoDy（动态图的反事实解释器）。它们将解释视为一个搜索问题，寻找改变模型预测的输入图修改。GreeDy 使用简单的贪心方法，而 CoDy 使用复杂的蒙特卡洛树搜索算法。实验证明，两种方法都能有效生成清晰的解释。值得注意的是，CoDy 的性能优于 GreeDy 和现有的事实方法，寻找到重要的反事实输入的成功率提高了高达 59\%。这突出了 CoDy 的优势。

    arXiv:2403.16846v1 Announce Type: cross  Abstract: Temporal Graph Neural Networks (TGNNs), crucial for modeling dynamic graphs with time-varying interactions, face a significant challenge in explainability due to their complex model structure. Counterfactual explanations, crucial for understanding model decisions, examine how input graph changes affect outcomes. This paper introduces two novel counterfactual explanation methods for TGNNs: GreeDy (Greedy Explainer for Dynamic Graphs) and CoDy (Counterfactual Explainer for Dynamic Graphs). They treat explanations as a search problem, seeking input graph alterations that alter model predictions. GreeDy uses a simple, greedy approach, while CoDy employs a sophisticated Monte Carlo Tree Search algorithm. Experiments show both methods effectively generate clear explanations. Notably, CoDy outperforms GreeDy and existing factual methods, with up to 59\% higher success rate in finding significant counterfactual inputs. This highlights CoDy's p
    
[^25]: LLM代理是否会感到后悔？在线学习和游戏案例研究

    Do LLM Agents Have Regret? A Case Study in Online Learning and Games

    [https://arxiv.org/abs/2403.16843](https://arxiv.org/abs/2403.16843)

    通过研究在线学习和博弈论中的基准决策设置，评估LLM代理的交互行为和性能，以了解它们在多代理环境中的潜力和限制。

    

    大型语言模型(LLMs)越来越多地被用于(交互式)决策制定，通过开发基于LLM的自主代理。尽管它们取得了不断的成功，但LLM代理在决策制定中的表现尚未通过定量指标进行充分调查，特别是在它们相互作用时的多代理设置中，这是实际应用中的典型场景。为了更好地理解LLM代理在这些交互环境中的限制，我们建议研究它们在在线学习和博弈论的基准决策设置中的相互作用，并通过\emph{后悔}性能指标进行评估。我们首先在经典(非平稳)在线学习问题中经验性地研究LLMs的无后悔行为，以及当LLM代理通过进行重复游戏进行交互时均衡的出现。然后我们对无后悔行为提供一些理论洞见。

    arXiv:2403.16843v1 Announce Type: cross  Abstract: Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behavior
    
[^26]: 一个无模型的熵正则化逆强化学习算法的收敛性

    Convergence of a model-free entropy-regularized inverse reinforcement learning algorithm

    [https://arxiv.org/abs/2403.16829](https://arxiv.org/abs/2403.16829)

    提出一个无模型的算法来解决熵正则化的逆强化学习问题，该算法能够使用有限样本恢复出专家表现最佳的奖励，并且最终得到的最优策略与专家策略非常接近。

    

    在给定一组专家演示数据集的情况下，逆强化学习旨在恢复一个专家表现最佳的奖励。本文提出了一个无模型的算法来解决熵正则化逆强化学习问题。具体而言，我们采用随机梯度下降更新奖励，采用随机软策略迭代更新策略。假设可以访问一个生成模型，我们证明了我们的算法能够保证使用$\mathcal{O}(1/\varepsilon^{2})$个马尔可夫决策过程（MDP）样本恢复出一个使专家表现最佳的奖励。此外，通过$\mathcal{O}(1/\varepsilon^{4})$个样本，我们证明了与恢复奖励对应的最优策略在总变差距离上与专家策略$\varepsilon$-接近。

    arXiv:2403.16829v1 Announce Type: cross  Abstract: Given a dataset of expert demonstrations, inverse reinforcement learning (IRL) aims to recover a reward for which the expert is optimal. This work proposes a model-free algorithm to solve entropy-regularized IRL problem. In particular, we employ a stochastic gradient descent update for the reward and a stochastic soft policy iteration update for the policy. Assuming access to a generative model, we prove that our algorithm is guaranteed to recover a reward for which the expert is $\varepsilon$-optimal using $\mathcal{O}(1/\varepsilon^{2})$ samples of the Markov decision process (MDP). Furthermore, with $\mathcal{O}(1/\varepsilon^{4})$ samples we prove that the optimal policy corresponding to the recovered reward is $\varepsilon$-close to the expert policy in total variation distance.
    
[^27]: 在线神经演员-评论算法的弱收敛分析

    Weak Convergence Analysis of Online Neural Actor-Critic Algorithms

    [https://arxiv.org/abs/2403.16825](https://arxiv.org/abs/2403.16825)

    在线神经演员-评论算法中，我们证明当隐藏单元和训练步数的数量$\rightarrow \infty$时，单层神经网络将收敛于随机ODE，通过建立数据样本的几何遍历性和使用泊松方程证明模型更新波动消失，演员神经网络和评论神经网络收敛到具有随机初始条件的ODE系统的解。

    

    我们证明，使用在线演员评论算法训练的单层神经网络在隐藏单元和训练步数的数量$\rightarrow \infty$时，收敛于一个随机常微分方程（ODE）。在线演员评论算法中，随着模型的更新，数据样本的分布会动态变化，这对于任何收敛分析来说都是一个关键挑战。我们在固定演员策略下建立了数据样本的几何遍历性。然后，使用泊松方程，我们证明由于随机到达的数据样本带来的模型更新波动会随着参数更新次数的增加$\rightarrow \infty$而消失。利用泊松方程和弱收敛技术，我们证明演员神经网络和评论神经网络收敛到具有随机初始条件的ODE系统的解。

    arXiv:2403.16825v1 Announce Type: new  Abstract: We prove that a single-layer neural network trained with the online actor critic algorithm converges in distribution to a random ordinary differential equation (ODE) as the number of hidden units and the number of training steps $\rightarrow \infty$. In the online actor-critic algorithm, the distribution of the data samples dynamically changes as the model is updated, which is a key challenge for any convergence analysis. We establish the geometric ergodicity of the data samples under a fixed actor policy. Then, using a Poisson equation, we prove that the fluctuations of the model updates around the limit distribution due to the randomly-arriving data samples vanish as the number of parameter updates $\rightarrow \infty$. Using the Poisson equation and weak convergence techniques, we prove that the actor neural network and critic neural network converge to the solutions of a system of ODEs with random initial conditions. Analysis of the 
    
[^28]: 混合LiFi和WiFi网络中的资源和移动管理：基于用户中心的学习方法

    Resource and Mobility Management in Hybrid LiFi and WiFi Networks: A User-Centric Learning Approach

    [https://arxiv.org/abs/2403.16823](https://arxiv.org/abs/2403.16823)

    研究了在混合LiFi和WiFi网络中基于用户中心的负载平衡方法，以解决现有网络中心化方法在处理移动性方面的问题。

    

    混合光通信（LiFi）和无线局域网（WiFi）网络（HLWNets）是一种新兴的室内无线通信范式，结合了LiFi的宽展光谱优势和WiFi的无处不在的覆盖优势。然而，负载平衡（LB）成为这种混合网络资源管理的关键挑战。现有的负载平衡方法大多是网络中心化的，依赖中央单元一次性为所有用户制定解决方案。因此，不论用户的移动状态如何，解决方案都需要同步更新所有用户。这会影响网络性能的两个方面：i）当更新频率较低时，会影响快速移动用户的连接性；ii）当更新频率较高时，会导致对于慢速移动用户不必要的切换以及巨大的反馈成本。受此启发，我们研究了允许用户更新其

    arXiv:2403.16823v1 Announce Type: cross  Abstract: Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks (HLWNets) are an emerging indoor wireless communication paradigm, which combines the advantages of the capacious optical spectra of LiFi and ubiquitous coverage of WiFi. Meanwhile, load balancing (LB) becomes a key challenge in resource management for such hybrid networks. The existing LB methods are mostly network-centric, relying on a central unit to make a solution for the users all at once. Consequently, the solution needs to be updated for all users at the same pace, regardless of their moving status. This would affect the network performance in two aspects: i) when the update frequency is low, it would compromise the connectivity of fast-moving users; ii) when the update frequency is high, it would cause unnecessary handovers as well as hefty feedback costs for slow-moving users. Motivated by this, we investigate user-centric LB which allows users to update their 
    
[^29]: 使用图贝叶斯优化从单快照观测实现多源定位

    Multiple-Source Localization from a Single-Snapshot Observation Using Graph Bayesian Optimization

    [https://arxiv.org/abs/2403.16818](https://arxiv.org/abs/2403.16818)

    提出了一种基于贝叶斯优化的BOSouL模拟方法，用于从单快照观测中实现多源定位，以解决现有方法在信息有限、源交互作用和扩散模型依赖性方面的限制。

    

    由于其在各种应用中的重要性，源定位已经引起了相当大的关注，成为应对扩散危害的最重要手段之一。从单个快照观测中实现多源定位尤其重要，但这一问题的内在复杂性，如信息有限、源之间的相互作用以及对扩散模型的依赖，给解决带来了挑战。目前的方法通常利用启发式和贪婪选择，通常与一个扩散模型绑定。因此，它们的有效性受到约束。为了解决这些限制，我们提出了一种基于模拟的方法，称为BOSouL。采用贝叶斯优化（BO）来近似结果，其样本效率高。一个代理函数对来自有限信息的不确定性进行建模。它以节点集合而非单个节点作为输入。BOSouL可以实现

    arXiv:2403.16818v1 Announce Type: new  Abstract: Due to the significance of its various applications, source localization has garnered considerable attention as one of the most important means to confront diffusion hazards. Multi-source localization from a single-snapshot observation is especially relevant due to its prevalence. However, the inherent complexities of this problem, such as limited information, interactions among sources, and dependence on diffusion models, pose challenges to resolution. Current methods typically utilize heuristics and greedy selection, and they are usually bonded with one diffusion model. Consequently, their effectiveness is constrained. To address these limitations, we propose a simulation-based method termed BOSouL. Bayesian optimization (BO) is adopted to approximate the results for its sample efficiency. A surrogate function models uncertainty from the limited information. It takes sets of nodes as the input instead of individual nodes. BOSouL can in
    
[^30]: 基于LLM的数字孪生体用于优化人在回路系统

    An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems

    [https://arxiv.org/abs/2403.16809](https://arxiv.org/abs/2403.16809)

    本文提出了一种利用大型语言模型(LLMs)来优化人在回路系统的方法，并通过案例研究展示了在购物中心中模拟各种人群热量偏好的应用。

    

    随着物联网和物理系统(CPS-IoT)应用和基础模型的日益普及，新的应用正在兴起，利用环境的实时控制。例如，对于供暖、通风和空调(HVAC)系统的实时控制可以在不需要为人员舒适而运行时减少其使用，从而降低能源消耗。然而，在实践中，对这种人在回路(HITL)系统中人类偏好的实时反馈收集是困难的。我们提出使用大型语言模型(LLMs)来应对CPS优化中动态环境和难以获取数据的挑战。在本文中，我们提出了一个案例研究，利用LLM代理模仿购物中心中各种人群（如年轻家庭、老年人）的行为和热量偏好。聚合的热量偏好被整合到一个代理体中。

    arXiv:2403.16809v1 Announce Type: cross  Abstract: The increasing prevalence of Cyber-Physical Systems and the Internet of Things (CPS-IoT) applications and Foundation Models are enabling new applications that leverage real-time control of the environment. For example, real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems can reduce its usage when not needed for the comfort of human occupants, hence reducing energy consumption. Collecting real-time feedback on human preferences in such human-in-the-loop (HITL) systems, however, is difficult in practice. We propose the use of large language models (LLMs) to deal with the challenges of dynamic environments and difficult-to-obtain data in CPS optimization. In this paper, we present a case study that employs LLM agents to mimic the behaviors and thermal preferences of various population groups (e.g. young families, the elderly) in a shopping mall. The aggregated thermal preferences are integrated into an agent-in-th
    
[^31]: 基于聚类的神经网络规范化层

    Cluster-Based Normalization Layer for Neural Networks

    [https://arxiv.org/abs/2403.16798](https://arxiv.org/abs/2403.16798)

    该论文提出了一种基于聚类的神经网络规范化方法CB-Norm，通过引入高斯混合模型，解决了梯度稳定性和学习加速方面的挑战。

    

    深度学习在神经网络训练过程中面临重要挑战，包括内部协变量漂移、标签漂移、梯度消失/爆炸、过拟合和计算复杂性。传统的规范化方法，如批标准化，旨在解决其中一些问题，但通常依赖于限制其适应性的假设。混合规范化在处理多个高斯分布时面临计算障碍。本文介绍了基于聚类的规范化（CB-Norm）的两个变体——监督式基于聚类的规范化（SCB-Norm）和无监督式基于聚类的规范化（UCB-Norm），提出了一种开创性的一步规范化方法。CB-Norm利用高斯混合模型来专门解决与梯度稳定性和学习加速有关的挑战。

    arXiv:2403.16798v1 Announce Type: cross  Abstract: Deep learning faces significant challenges during the training of neural networks, including internal covariate shift, label shift, vanishing/exploding gradients, overfitting, and computational complexity. While conventional normalization methods, such as Batch Normalization, aim to tackle some of these issues, they often depend on assumptions that constrain their adaptability. Mixture Normalization faces computational hurdles in its pursuit of handling multiple Gaussian distributions.   This paper introduces Cluster-Based Normalization (CB-Norm) in two variants - Supervised Cluster-Based Normalization (SCB-Norm) and Unsupervised Cluster-Based Normalization (UCB-Norm) - proposing a groundbreaking one-step normalization approach. CB-Norm leverages a Gaussian mixture model to specifically address challenges related to gradient stability and learning acceleration.   For SCB-Norm, a supervised variant, the novel mechanism involves introduc
    
[^32]: Iso-Diffusion: 使用加性高斯噪声的各向同性改进扩散概率模型

    Iso-Diffusion: Improving Diffusion Probabilistic Models Using the Isotropy of the Additive Gaussian Noise

    [https://arxiv.org/abs/2403.16790](https://arxiv.org/abs/2403.16790)

    利用加性噪声的各向同性作为约束，改进了扩散概率模型的保真度，验证实验表明这种约束的整合显著提高了模型的性能。

    

    去噪扩散概率模型（DDPMs）在生成人工智能领域取得了很大成就。尽管它们性能很高，但还有改进的空间，特别是通过利用强加结构完整性的统计属性来提高样本保真度，如各向同性。仅减小加性和预测噪声之间的均方误差并不能强加对预测噪声为各向同性的约束。因此，我们受到动力，利用加性噪声的各向同性作为目标函数的约束来增强DDPMs的保真度。我们的方法简单，并可应用于任何DDPM变体。我们通过在四个合成2D数据集上进行的实验以及无条件图像生成来验证我们的方法。正如结果所示，这种约束的整合改善了2D数据集的保真度指标Precision和Density以及无条件图像生成的指标。

    arXiv:2403.16790v1 Announce Type: new  Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have accomplished much in the realm of generative AI. Despite their high performance, there is room for improvement, especially in terms of sample fidelity by utilizing statistical properties that impose structural integrity, such as isotropy. Minimizing the mean squared error between the additive and predicted noise alone does not impose constraints on the predicted noise to be isotropic. Thus, we were motivated to utilize the isotropy of the additive noise as a constraint on the objective function to enhance the fidelity of DDPMs. Our approach is simple and can be applied to any DDPM variant. We validate our approach by presenting experiments conducted on four synthetic 2D datasets as well as on unconditional image generation. As demonstrated by the results, the incorporation of this constraint improves the fidelity metrics, Precision and Density for the 2D datasets as well as for the un
    
[^33]: 对抗性攻击的解剖学：基于概念的XAI解剖

    The Anatomy of Adversarial Attacks: Concept-based XAI Dissection

    [https://arxiv.org/abs/2403.16782](https://arxiv.org/abs/2403.16782)

    对抗性攻击对卷积神经网络学到的概念产生了实质性的影响，引入新概念或修改现有概念，并且这种影响可以通过线性分解对扰动进行解释。

    

    对抗性攻击(AAs)对深度神经网络的可靠性和鲁棒性构成重大威胁。虽然这些攻击对模型预测的影响已得到广泛研究，但它们对这些模型中学习到的表示和概念的影响仍然大多未被探索。在这项工作中，我们使用可解释的人工智能(XAI)技术，对卷积神经网络(CNNs)中对抗性攻击对学习到的概念的影响进行了深入分析。通过对各种网络架构和有针对性的AA技术进行了大量实验，我们揭示了几个关键发现。首先，AAs在特征空间中造成概念组成的实质性变化，引入新概念或修改现有概念。其次，对抗性扰动本身可以被线性分解为一组潜在矢量分量，其中部分分量负责攻击。

    arXiv:2403.16782v1 Announce Type: cross  Abstract: Adversarial attacks (AAs) pose a significant threat to the reliability and robustness of deep neural networks. While the impact of these attacks on model predictions has been extensively studied, their effect on the learned representations and concepts within these models remains largely unexplored. In this work, we perform an in-depth analysis of the influence of AAs on the concepts learned by convolutional neural networks (CNNs) using eXplainable artificial intelligence (XAI) techniques. Through an extensive set of experiments across various network architectures and targeted AA techniques, we unveil several key findings. First, AAs induce substantial alterations in the concept composition within the feature space, introducing new concepts or modifying existing ones. Second, the adversarial perturbation itself can be linearly decomposed into a set of latent vector components, with a subset of these being responsible for the attack's 
    
[^34]: Diff-Def: 通过扩散生成的形变场进行有条件的图谱制作

    Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases

    [https://arxiv.org/abs/2403.16776](https://arxiv.org/abs/2403.16776)

    使用扩散模型生成形变场，将一般人口图谱转变为特定子人口的图谱，确保结构合理性，避免幻觉。

    

    解剖图谱广泛应用于人口分析。有条件的图谱针对通过特定条件（如人口统计学或病理学）定义的特定子人口，并允许研究与年龄相关的形态学差异等细粒度解剖学差异。现有方法使用基于配准的方法或生成模型，前者无法处理大的解剖学变异，后者可能在训练过程中出现不稳定和幻觉。为了克服这些限制，我们使用潜在扩散模型生成形变场，将一个常规人口图谱转变为代表特定子人口的图谱。通过生成形变场，并将有条件的图谱注册到一组图像附近，我们确保结构的合理性，避免直接图像合成时可能出现的幻觉。我们将我们的方法与几种最先进的方法进行了比较。

    arXiv:2403.16776v1 Announce Type: cross  Abstract: Anatomical atlases are widely used for population analysis. Conditional atlases target a particular sub-population defined via certain conditions (e.g. demographics or pathologies) and allow for the investigation of fine-grained anatomical differences - such as morphological changes correlated with age. Existing approaches use either registration-based methods that are unable to handle large anatomical variations or generative models, which can suffer from training instabilities and hallucinations. To overcome these limitations, we use latent diffusion models to generate deformation fields, which transform a general population atlas into one representing a specific sub-population. By generating a deformation field and registering the conditional atlas to a neighbourhood of images, we ensure structural plausibility and avoid hallucinations, which can occur during direct image synthesis. We compare our method to several state-of-the-art 
    
[^35]: 用于鲁棒性混合代码翻译的合成数据生成和联合学习

    Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation

    [https://arxiv.org/abs/2403.16771](https://arxiv.org/abs/2403.16771)

    本文提出了用于鲁棒性混合代码翻译的合成数据生成和联合学习方法，包括开发了Hinglish到英语的平行语料库以及提出的能够处理噪声的联合训练模型RCMT。

    

    现代多语言世界中的广泛网络交流为在单个话语中混合多种语言（又称混合代码语言）提供了机会。由于标注数据的稀缺和噪音的存在，这给计算模型带来了严峻挑战。在资源匮乏的环境中缓解数据稀缺问题的潜在解决方案是通过翻译利用资源丰富语言中的现有数据。本文针对混合代码（印地语和孟加拉语）到英语的机器翻译问题。首先，我们合成开发了HINMIX一个印地语到英语的平行语料库，包含约420万个句对。随后，我们提出了RCMT，一种基于强健扰动的联合训练模型，通过在干净和带噪声单词之间共享参数，学习处理现实世界混合代码文本中的噪声。此外，我们展示了RCMT在零-shot设置中对孟加拉语的适应能力。

    arXiv:2403.16771v1 Announce Type: new  Abstract: The widespread online communication in a modern multilingual world has provided opportunities to blend more than one language (aka code-mixed language) in a single utterance. This has resulted a formidable challenge for the computational models due to the scarcity of annotated data and presence of noise. A potential solution to mitigate the data scarcity problem in low-resource setup is to leverage existing data in resource-rich language through translation. In this paper, we tackle the problem of code-mixed (Hinglish and Bengalish) to English machine translation. First, we synthetically develop HINMIX, a parallel corpus of Hinglish to English, with ~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation based joint-training model that learns to handle noise in the real-world code-mixed text by parameter sharing across clean and noisy words. Further, we show the adaptability of RCMT in a zero-shot setup for Bengalish t
    
[^36]: DeepKnowledge: 基于泛化驱动的深度学习测试

    DeepKnowledge: Generalisation-Driven Deep Learning Testing

    [https://arxiv.org/abs/2403.16768](https://arxiv.org/abs/2403.16768)

    DeepKnowledge提出了一种基于知识泛化理论的深度学习系统测试方法，旨在提高DNN的稳健性并减少黑匣子模型的风险。

    

    尽管深度神经网络取得了前所未有的成功，但它们对数据分布的微小变化极为脆弱，这要求有效的测试技术来评估它们的可靠性。尽管近年来在深度神经网络测试方面取得了进展，但缺乏系统化的测试方法来评估深度神经网络在训练分布之外的数据上泛化和运行的能力。我们通过DeepKnowledge来解决这一问题，这是一种基于知识泛化理论的深度神经网络系统测试方法，旨在增强深度神经网络的稳健性，并减少“黑匣子”模型的剩余风险。根据这一理论，DeepKnowledge认为核心计算DNN单元，称为转移知识神经元，在域变化下可以泛化。DeepKnowledge提供了一种客观的信心度量，用于评估给定数据分布变化的DNN测试活动，并利用这些信息来推动泛化。

    arXiv:2403.16768v1 Announce Type: cross  Abstract: Despite their unprecedented success, DNNs are notoriously fragile to small shifts in data distribution, demanding effective testing techniques that can assess their dependability. Despite recent advances in DNN testing, there is a lack of systematic testing approaches that assess the DNN's capability to generalise and operate comparably beyond data in their training distribution. We address this gap with DeepKnowledge, a systematic testing methodology for DNN-based systems founded on the theory of knowledge generalisation, which aims to enhance DNN robustness and reduce the residual risk of 'black box' models. Conforming to this theory, DeepKnowledge posits that core computational DNN units, termed Transfer Knowledge neurons, can generalise under domain shift. DeepKnowledge provides an objective confidence measurement on testing activities of DNN given data distribution shifts and uses this information to instrument a generalisation-in
    
[^37]: 单次领域增量学习

    One-Shot Domain Incremental Learning

    [https://arxiv.org/abs/2403.16707](https://arxiv.org/abs/2403.16707)

    提出了一种处理单次领域增量学习中批归一化层统计数据困难的技术，并展示了其有效性。

    

    在以前关于用于分类的深度神经网络模型的研究中已经讨论了领域增量学习（DIL）。在DIL中，我们假设随着时间的推移观察新领域上的样本。模型必须对所有领域上的输入进行分类。然而，在实践中，我们可能会遇到这样一种情况，即我们需要在新领域的样本仅间歇性地被观察的约束下执行DIL。因此，在本研究中，我们考虑了一个极端情况，即我们只有一份来自新领域的样本，我们称之为单次DIL。我们首先经验性地表明现有的DIL方法在单次DIL中表现不佳。通过各种调查，我们分析了这种失败的原因。根据我们的分析，我们明确了单次DIL的困难是由批归一化层中的统计数据引起的。因此，我们提出了一种关于这些统计数据的技术，并展示了我们技术的有效性。

    arXiv:2403.16707v1 Announce Type: cross  Abstract: Domain incremental learning (DIL) has been discussed in previous studies on deep neural network models for classification. In DIL, we assume that samples on new domains are observed over time. The models must classify inputs on all domains. In practice, however, we may encounter a situation where we need to perform DIL under the constraint that the samples on the new domain are observed only infrequently. Therefore, in this study, we consider the extreme case where we have only one sample from the new domain, which we call one-shot DIL. We first empirically show that existing DIL methods do not work well in one-shot DIL. We have analyzed the reason for this failure through various investigations. According to our analysis, we clarify that the difficulty of one-shot DIL is caused by the statistics in the batch normalization layers. Therefore, we propose a technique regarding these statistics and demonstrate the effectiveness of our tech
    
[^38]: 评估深度学习在前列腺癌自动格里森分级中的性能

    Assessing the Performance of Deep Learning for Automated Gleason Grading in Prostate Cancer

    [https://arxiv.org/abs/2403.16695](https://arxiv.org/abs/2403.16695)

    该研究评估了11种深度神经网络架构在前列腺癌自动格里森分级中的性能，发现ConvNeXt表现最佳，新架构实现了优越性能但在区分密切相关的格里森分级方面存在挑战，为提高格里森分级系统奠定了基础。

    

    前列腺癌是一个主要的健康问题，需要先进的诊断工具。利用数字病理学和人工智能，本研究探讨了11种深度神经网络架构在自动化前列腺癌格里森分级中的潜力，重点是比较传统和最近的架构。基于AUCMEDI框架的标准化图像分类管道，利用一个包含34,264个注释的组织切片的内部数据集，便于进行强大的评估。结果表明，不同架构之间的敏感性不同，ConvNeXt表现出最佳性能。值得注意的是，较新的架构在实现卓越性能的同时，面临着区分关系密切的格里森分级的挑战。ConvNeXt模型能够学习复杂性和泛化能力之间的平衡。总体而言，本研究为提高格里森分级系统奠定了基础，可能会对临床实践产生重要影响。

    arXiv:2403.16695v1 Announce Type: cross  Abstract: Prostate cancer is a dominant health concern calling for advanced diagnostic tools. Utilizing digital pathology and artificial intelligence, this study explores the potential of 11 deep neural network architectures for automated Gleason grading in prostate carcinoma focusing on comparing traditional and recent architectures. A standardized image classification pipeline, based on the AUCMEDI framework, facilitated robust evaluation using an in-house dataset consisting of 34,264 annotated tissue tiles. The results indicated varying sensitivity across architectures, with ConvNeXt demonstrating the strongest performance. Notably, newer architectures achieved superior performance, even though with challenges in differentiating closely related Gleason grades. The ConvNeXt model was capable of learning a balance between complexity and generalizability. Overall, this study lays the groundwork for enhanced Gleason grading systems, potentially i
    
[^39]: Synapse: 从视觉演示中学习优先概念

    Synapse: Learning Preferential Concepts from Visual Demonstrations

    [https://arxiv.org/abs/2403.16689](https://arxiv.org/abs/2403.16689)

    Synapse是一种神经符号化方法，旨在从有限演示中高效学习偏好概念，通过将偏好表示为神经符号程序并利用视觉解析、大型语言模型和程序合成相结合的方式来学习个人偏好。

    

    本文解决了偏好学习问题，旨在从视觉输入中学习用户特定偏好（例如，“好停车位”，“方便的下车位置”）。尽管与学习事实概念（例如，“红色立方体”）相似，但偏好学习是一个基本更加困难的问题，因为它涉及主观性质和个人特定训练数据的缺乏。我们使用一种名为Synapse的新框架来解决这个问题，这是一种神经符号化方法，旨在有效地从有限演示中学习偏好概念。Synapse将偏好表示为在图像上运作的领域特定语言（DSL）中的神经符号程序，并利用视觉解析、大型语言模型和程序合成的新组合来学习代表个人偏好的程序。我们通过广泛的实验评估了Synapse，包括一个关注与移动相关的用户案例研究。

    arXiv:2403.16689v1 Announce Type: cross  Abstract: This paper addresses the problem of preference learning, which aims to learn user-specific preferences (e.g., "good parking spot", "convenient drop-off location") from visual input. Despite its similarity to learning factual concepts (e.g., "red cube"), preference learning is a fundamentally harder problem due to its subjective nature and the paucity of person-specific training data. We address this problem using a new framework called Synapse, which is a neuro-symbolic approach designed to efficiently learn preferential concepts from limited demonstrations. Synapse represents preferences as neuro-symbolic programs in a domain-specific language (DSL) that operates over images, and leverages a novel combination of visual parsing, large language models, and program synthesis to learn programs representing individual preferences. We evaluate Synapse through extensive experimentation including a user case study focusing on mobility-related
    
[^40]: 一篇关于具有有限矩的损失函数泛化界的注记

    A note on generalization bounds for losses with finite moments

    [https://arxiv.org/abs/2403.16681](https://arxiv.org/abs/2403.16681)

    本文研究了损失函数具有有限矩的泛化界，并推导了高概率的PAC-Bayes界，进一步揭示了对损失函数有界方差的情况下界的改进。此外，该研究将结果扩展到期望和单次抽样PAC-Bayes中，并获得了针对有界损失函数的快速速率界。

    

    本文研究了Alquier [1]提出的截断方法，用于推导具有重尾特性的无界损失函数的高概率PAC-Bayes界。假设$p$-阶矩有界，得到的界在$p=2$时插值为缓慢的速率$1 / \sqrt{n}$，在$p \to \infty$且损失函数基本有界时插值为快速的速率$1 / n$。此外，本文导出了具有有界方差的损失函数的高概率PAC-Bayes界。该界对置信参数和依赖度量的依赖关系相比文献中先前的界具有指数级的改进。最后，本文将所有结果推广到期望保证和单次抽样PAC-Bayes中。为此，在这些设置中，它获得了[2]中针对有界损失函数的PAC-Bayes快速速率界的类似物。

    arXiv:2403.16681v1 Announce Type: cross  Abstract: This paper studies the truncation method from Alquier [1] to derive high-probability PAC-Bayes bounds for unbounded losses with heavy tails. Assuming that the $p$-th moment is bounded, the resulting bounds interpolate between a slow rate $1 / \sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p \to \infty$ and the loss is essentially bounded. Moreover, the paper derives a high-probability PAC-Bayes bound for losses with a bounded variance. This bound has an exponentially better dependence on the confidence parameter and the dependency measure than previous bounds in the literature. Finally, the paper extends all results to guarantees in expectation and single-draw PAC-Bayes. In order to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded losses from [2] in these settings.
    
[^41]: 对学习拉格朗日流体力学的对称基础卷积

    Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics

    [https://arxiv.org/abs/2403.16680](https://arxiv.org/abs/2403.16680)

    对称基础卷积提出了使用可分离基础函数的连续卷积的通用公式作为现有方法的超集，并评估了大量基础函数在不同流体模拟中的性能，结果表明偶数和奇数对称性是稳定性和准确性的关键因素。

    

    学习物理模拟已经成为机器学习领域许多最新研究工作的一个基本和核心方面，特别是针对纳维-斯托克斯基础的流体力学。经典数值求解器传统上在逆问题中计算昂贵且具有挑战性，而神经求解器旨在通过机器学习解决这两个问题。我们提出了使用可分离基础函数的连续卷积的通用公式作为现有方法的超集，并评估了大量基础函数在（a）可压缩1D SPH模拟，（b）弱可压缩2D SPH模拟和（c）不可压缩2D SPH模拟中的情况。我们展示了在基础函数中包括的偶对称性和奇对称性是稳定性和准确性的关键方面。我们广泛的评估显示，基于傅里叶的连续卷积在准确性和泛化性方面优于所有其他架构。

    arXiv:2403.16680v1 Announce Type: new  Abstract: Learning physical simulations has been an essential and central aspect of many recent research efforts in machine learning, particularly for Navier-Stokes-based fluid mechanics. Classic numerical solvers have traditionally been computationally expensive and challenging to use in inverse problems, whereas Neural solvers aim to address both concerns through machine learning. We propose a general formulation for continuous convolutions using separable basis functions as a superset of existing methods and evaluate a large set of basis functions in the context of (a) a compressible 1D SPH simulation, (b) a weakly compressible 2D SPH simulation, and (c) an incompressible 2D SPH Simulation. We demonstrate that even and odd symmetries included in the basis functions are key aspects of stability and accuracy. Our broad evaluation shows that Fourier-based continuous convolutions outperform all other architectures regarding accuracy and generalizat
    
[^42]: DeepGleason: 使用深度神经网络自动评估前列腺癌格里森分级的系统

    DeepGleason: a System for Automated Gleason Grading of Prostate Cancer using Deep Neural Networks

    [https://arxiv.org/abs/2403.16678](https://arxiv.org/abs/2403.16678)

    DeepGleason是一个使用深度神经网络自动对前列腺癌进行格里森分级的系统，在tile-wise分类方法和ConvNeXt架构的基础上，提供了一个标准化的工具，并比较了各种最先进的架构。

    

    在数字病理学和人工智能的进展为临床决策支持和增强诊断工作流提供了有希望的机会。先前的研究已经证明了人工智能在自动格里森分级方面的潜力，但缺乏最先进的方法论和模型可重用性。为了解决这个问题，我们提出了DeepGleason：一个基于开源深度神经网络的图像分类系统，用于利用前列腺组织切片的全切片组织病理学图像进行自动格里森分级。我们的工具采用标准化的AUCMEDI框架，采用以瓷砖为单位的分类方法，结合精调的图像预处理技术和ConvNeXt架构，该架构与各种最先进的架构进行了比较。神经网络模型在一组共369张前列腺癌切片的34,264个带注释瓷砖的内部数据集上进行了训练和验证。我们证明了DeepGle

    arXiv:2403.16678v1 Announce Type: cross  Abstract: Advances in digital pathology and artificial intelligence (AI) offer promising opportunities for clinical decision support and enhancing diagnostic workflows. Previous studies already demonstrated AI's potential for automated Gleason grading, but lack state-of-the-art methodology and model reusability. To address this issue, we propose DeepGleason: an open-source deep neural network based image classification system for automated Gleason grading using whole-slide histopathology images from prostate tissue sections. Implemented with the standardized AUCMEDI framework, our tool employs a tile-wise classification approach utilizing fine-tuned image preprocessing techniques in combination with a ConvNeXt architecture which was compared to various state-of-the-art architectures. The neural network model was trained and validated on an in-house dataset of 34,264 annotated tiles from 369 prostate carcinoma slides. We demonstrated that DeepGle
    
[^43]: FOOL: 用神经特征压缩解决卫星计算中的下行瓶颈问题

    FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression

    [https://arxiv.org/abs/2403.16677](https://arxiv.org/abs/2403.16677)

    FOOL是一种OEC本地和任务不可知的特征压缩方法，通过最大化吞吐量、嵌入上下文和利用瓷砖间的依赖关系，降低传输成本，同时保持预测性能。

    

    具有传感器的纳卫星星座捕获大范围地理区域，为地球观测提供了前所未有的机会。随着星座规模的增加，网络争用形成了下行瓶颈。轨道边缘计算（OEC）利用有限的机载计算资源通过在源头处理原始捕获来减少传输成本。然而，由于依赖粗糙的过滤方法或过分优先考虑特定下游任务，目前的解决方案具有有限的实用性。本文提出了FOOL，一种OEC本地和任务不可知的特征压缩方法，可保留预测性能。FOOL将高分辨率卫星图像进行分区，以最大化吞吐量。此外，它嵌入上下文并利用瓷砖间的依赖关系，以较低的开销降低传输成本。虽然FOOL是一种特征压缩器，但它可以在低

    arXiv:2403.16677v1 Announce Type: new  Abstract: Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation. As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks.   This work presents FOOL, an OEC-native and task-agnostic feature compression method that preserves prediction performance. FOOL partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While FOOL is a feature compressor, it can recover images with competitive scores on perceptual quality measures at low
    
[^44]: 理解尖峰神经网络中建模组件的功能角色

    Understanding the Functional Roles of Modelling Components in Spiking Neural Networks

    [https://arxiv.org/abs/2403.16674](https://arxiv.org/abs/2403.16674)

    系统研究揭示了尖峰神经网络中滤泄、重置和循环等建模组件在平衡记忆保留、时间处理和动态建模方面的功能角色。

    

    受大脑神经回路启发，尖峰神经网络（SNNs）在实现高计算效率和生物保真度方面很有前景。然而，优化SNNs相当困难，因为其建模组件的功能角色仍不清楚。通过设计和评估经典模型的几个变体，我们系统研究了滤泄、重置和循环这些关键建模组件在基于漏积分放电（LIF）的SNNs中的功能角色。通过大量实验，我们演示了这些组件如何影响SNNs的准确性、泛化性和稳健性。具体来说，我们发现滤泄在平衡记忆保留和稳健性方面起着至关重要的作用，重置机制对于不间断的时间处理和计算效率至关重要，而循环则丰富了模型复杂动态的能力，但会损害稳健性。

    arXiv:2403.16674v1 Announce Type: cross  Abstract: Spiking neural networks (SNNs), inspired by the neural circuits of the brain, are promising in achieving high computational efficiency with biological fidelity. Nevertheless, it is quite difficult to optimize SNNs because the functional roles of their modelling components remain unclear. By designing and evaluating several variants of the classic model, we systematically investigate the functional roles of key modelling components, leakage, reset, and recurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive experiments, we demonstrate how these components influence the accuracy, generalization, and robustness of SNNs. Specifically, we find that the leakage plays a crucial role in balancing memory retention and robustness, the reset mechanism is essential for uninterrupted temporal processing and computational efficiency, and the recurrence enriches the capability to model complex dynamics at a cost of robustness degr
    
[^45]: 推荐系统的图增强

    Graph Augmentation for Recommendation

    [https://arxiv.org/abs/2403.16656](https://arxiv.org/abs/2403.16656)

    提出了一个名为GraphAug的框架，通过引入强大的数据增强程序生成去噪的自监督信号，解决了推荐系统中对比学习中存在的数据噪声和GNN架构平滑问题。

    

    Graph augmentation与对比学习在推荐系统领域引起了广泛关注，因为它能够在标记数据有限的情况下学习出富有表现力的用户表示。然而，直接将现有的GCL模型应用于现实推荐环境中存在挑战。主要问题有两个。首先，对比学习中忽略数据噪声可能导致嘈杂的自监督信号，从而降低性能。其次，许多现有的GCL方法依赖于图神经网络（GNN）架构，这可能由于非自适应信息传递而导致过度平滑问题。为解决这些挑战，我们提出了一个被称为GraphAug的原则性框架。该框架引入了一个强大的数据增强程序，生成去噪的自监督信号，提升推荐系统的性能。GraphAug框架还融入了图信息

    arXiv:2403.16656v1 Announce Type: new  Abstract: Graph augmentation with contrastive learning has gained significant attention in the field of recommendation systems due to its ability to learn expressive user representations, even when labeled data is limited. However, directly applying existing GCL models to real-world recommendation environments poses challenges. There are two primary issues to address. Firstly, the lack of consideration for data noise in contrastive learning can result in noisy self-supervised signals, leading to degraded performance. Secondly, many existing GCL approaches rely on graph neural network (GNN) architectures, which can suffer from over-smoothing problems due to non-adaptive message passing. To address these challenges, we propose a principled framework called GraphAug. This framework introduces a robust data augmentor that generates denoised self-supervised signals, enhancing recommender systems. The GraphAug framework incorporates a graph information 
    
[^46]: 一种基于新型损失函数的二分类支持向量机

    A Novel Loss Function-based Support Vector Machine for Binary Classification

    [https://arxiv.org/abs/2403.16654](https://arxiv.org/abs/2403.16654)

    通过引入新型Slide损失函数（$\ell_s$），我们提出了一种$\ell_s$-SVM分类器，有效地解决了传统SVM在边缘正确分类样本惩罚程度方面的不足，提升了泛化能力。

    

    先前的支持向量机（SVM）包括0/1损失SVM、铰链损失SVM、坡度损失SVM、截断针垫损失SVM等，在正确分类的样本在边际内的惩罚程度上存在一定的疏忽。这种疏忽在一定程度上影响了SVM分类器的泛化能力。为了解决这一限制，我们从置信边际的角度提出了一种新型Slide损失函数（$\ell_s$）来构建支持向量机分类器（$\ell_s$-SVM）。通过引入近端稳定点的概念，并利用Lipschitz连续性的属性，推导出了$\ell_s$-SVM的一阶最优性条件。基于此，我们定义了$\ell_s$支持向量和$\ell_s$-SVM的工作集。为了高效处理$\ell_s$-SVM，我们设计了一个具有工作集的快速交替方向乘法器方法（$\ell_s$-ADMM），并提供了收敛性分析。

    arXiv:2403.16654v1 Announce Type: new  Abstract: The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss SVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the degree of penalty for the correctly classified samples within the margin. This oversight affects the generalization ability of the SVM classifier to some extent. To address this limitation, from the perspective of confidence margin, we propose a novel Slide loss function ($\ell_s$) to construct the support vector machine classifier($\ell_s$-SVM). By introducing the concept of proximal stationary point, and utilizing the property of Lipschitz continuity, we derive the first-order optimality conditions for $\ell_s$-SVM. Based on this, we define the $\ell_s$ support vectors and working set of $\ell_s$-SVM. To efficiently handle $\ell_s$-SVM, we devise a fast alternating direction method of multipliers with the working set ($\ell_s$-ADMM), and provide the convergence analysis. The numerical 
    
[^47]: 用贝叶斯推断缩小模拟到现实的差距

    Bridging the Sim-to-Real Gap with Bayesian Inference

    [https://arxiv.org/abs/2403.16644](https://arxiv.org/abs/2403.16644)

    提出了SIM-FSVGD方法，通过利用低保真度的物理先验，成功缩小模拟到现实的差距，能够在低数据量的情况下学习准确的动力学，并在实验中展示了在高性能赛车系统上的有效性。

    

    我们提出了SIM-FSVGD来从数据中学习机器人动力学。与传统方法相比，SIM-FSVGD利用低保真度的物理先验，如模拟器的形式，来规范神经网络模型的训练。在低数据情况下已经学习准确的动力学，SIM-FSVGD在更多数据可用时也能够扩展和表现出色。我们通过实验证明，学习隐式物理先验导致准确的平均模型估计以及精确的不确定性量化。我们展示了SIM-FSVGD在高性能RC赛车系统上缩小模拟到现实差距的有效性。使用基于模型的RL，我们展示了一个高度动态的停车转向动作，使用的数据量仅为现有技术的一半。

    arXiv:2403.16644v1 Announce Type: cross  Abstract: We present SIM-FSVGD for learning robot dynamics from data. As opposed to traditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in the form of simulators, to regularize the training of neural network models. While learning accurate dynamics already in the low data regime, SIM-FSVGD scales and excels also when more data is available. We empirically show that learning with implicit physical priors results in accurate mean model estimation as well as precise uncertainty quantification. We demonstrate the effectiveness of SIM-FSVGD in bridging the sim-to-real gap on a high-performance RC racecar system. Using model-based RL, we demonstrate a highly dynamic parking maneuver with drifting, using less than half the data compared to the state of the art.
    
[^48]: 使用GAN进行CT去噪的多尺度纹理损失

    Multi-Scale Texture Loss for CT denoising with GANs

    [https://arxiv.org/abs/2403.16640](https://arxiv.org/abs/2403.16640)

    该研究提出了一种利用灰度共生矩阵的多尺度纹理损失函数，以帮助生成对抗网络更好地捕捉复杂的图像关系。

    

    生成对抗网络（GANs）已被证明在医学影像中的去噪应用中是一个强大的框架。然而，基于GAN的去噪算法仍然存在捕捉图像内复杂关系的局限性。为了在训练过程中掌握高度复杂和非线性的纹理关系，本文提出了一种利用灰度共生矩阵（GLCM）固有的多尺度性质的损失函数。尽管深度学习的最新进展在分类和检测任务中表现出优越性能，我们假设将其信息内容整合到GANs的训练中会是有价值的。因此，我们提出了适用于基于梯度优化的GLCM的可微分实现。

    arXiv:2403.16640v1 Announce Type: cross  Abstract: Generative Adversarial Networks (GANs) have proved as a powerful framework for denoising applications in medical imaging. However, GAN-based denoising algorithms still suffer from limitations in capturing complex relationships within the images. In this regard, the loss function plays a crucial role in guiding the image generation process, encompassing how much a synthetic image differs from a real image. To grasp highly complex and non-linear textural relationships in the training process, this work presents a loss function that leverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence Matrix (GLCM). Although the recent advances in deep learning have demonstrated superior performance in classification and detection tasks, we hypothesize that its information content can be valuable when integrated into GANs' training. To this end, we propose a differentiable implementation of the GLCM suited for gradient-based optimiza
    
[^49]: 专利相似性嵌入模型的比较分析

    A comparative analysis of embedding models for patent similarity

    [https://arxiv.org/abs/2403.16630](https://arxiv.org/abs/2403.16630)

    本文比较了不同类型的专利嵌入模型在专利相似性计算任务上的表现，并具体探讨了Sentence Transformers (SBERT) 架构在专利相似性任务中的性能。

    

    本文对基于文本的专利相似性领域做出了两个贡献。首先，它比较了不同类型的专利特定预训练嵌入模型（如word2vec和doc2vec模型）和上下文词嵌入模型（如基于transformers的模型）在专利相似性计算任务上的表现。其次，它具体比较了具有不同训练阶段的Sentence Transformers（SBERT）架构在专利相似性任务上的性能。为评估模型的性能，我们使用关于专利干涉的信息，即两个或多个专利申请中的专利要求被专利审查员证明存在重叠的现象。因此，我们将这些干涉案例视为两个专利之间的最大相似性的代理，并用它们作为基准来评估不同嵌入模型的性能。

    arXiv:2403.16630v1 Announce Type: new  Abstract: This paper makes two contributions to the field of text-based patent similarity. First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static word embeddings (such as word2vec and doc2vec models) and contextual word embeddings (such as transformers based models), on the task of patent similarity calculation. Second, it compares specifically the performance of Sentence Transformers (SBERT) architectures with different training phases on the patent similarity task. To assess the models' performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners. Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models. Our results p
    
[^50]: 为次季节预测校准贝叶斯UNet++

    Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting

    [https://arxiv.org/abs/2403.16612](https://arxiv.org/abs/2403.16612)

    通过校准贝叶斯UNet++模型，我们可以获得更可靠和更清晰的次季节预测，这对于安全关键的机器学习应用如天气预测员来说尤为重要。

    

    季节性预测在检测由气候变化引起的极端热和寒冷时是一项关键任务。对预测的信心应当是可靠的，因为一年中温度的微小增加对世界有着巨大影响。对神经网络进行校准提供了一种确保我们对预测的信心的方式。然而，校准回归模型是一个研究不足的话题，特别是在预测者中。我们校准了基于UNet++结构的模型，该模型被证明在温度异常方面优于基于物理的模型。我们展示出，在预测误差和校准误差之间略微权衡的情况下，可以获得更可靠和更清晰的预测。我们认为，校准应当成为诸如天气预报员等安全关键的机器学习应用的重要组成部分。

    arXiv:2403.16612v1 Announce Type: new  Abstract: Seasonal forecasting is a crucial task when it comes to detecting the extreme heat and colds that occur due to climate change. Confidence in the predictions should be reliable since a small increase in the temperatures in a year has a big impact on the world. Calibration of the neural networks provides a way to ensure our confidence in the predictions. However, calibrating regression models is an under-researched topic, especially in forecasters. We calibrate a UNet++ based architecture, which was shown to outperform physics-based models in temperature anomalies. We show that with a slight trade-off between prediction error and calibration error, it is possible to get more reliable and sharper forecasts. We believe that calibration should be an important part of safety-critical machine learning applications such as weather forecasters.
    
[^51]: 基于嵌入分享的分布式协作异常声音检测

    Distributed collaborative anomalous sound detection by embedding sharing

    [https://arxiv.org/abs/2403.16610](https://arxiv.org/abs/2403.16610)

    提出了一种基于嵌入分享的分布式协作异常声音检测方法，每个客户端使用共同的预训练模型计算嵌入，并在服务器上对其进行聚合，通过异常暴露来执行异常声音检测，平均提高了6.8%的AUC。

    

    为了开发一种机器声音监测系统，提出了一种检测异常声音的方法。本文探讨了一种多个客户端合作学习异常声音检测模型的方法，同时保持彼此的原始数据私密。在工业机器异常声音检测的背景下，每个客户端拥有来自不同机器或不同运行状态的数据，这使得通过联邦学习或分布式学习学习变得具有挑战性。在我们提出的方法中，每个客户端使用为声音数据分类而开发的通用预训练模型计算嵌入，然后在服务器上对这些计算得到的嵌入进行聚合，通过异常暴露来执行异常声音检测。实验表明，我们提出的方法将异常声音检测的AUC平均提高了6.8%。

    arXiv:2403.16610v1 Announce Type: cross  Abstract: To develop a machine sound monitoring system, a method for detecting anomalous sound is proposed. In this paper, we explore a method for multiple clients to collaboratively learn an anomalous sound detection model while keeping their raw data private from each other. In the context of industrial machine anomalous sound detection, each client possesses data from different machines or different operational states, making it challenging to learn through federated learning or split learning. In our proposed method, each client calculates embeddings using a common pre-trained model developed for sound data classification, and these calculated embeddings are aggregated on the server to perform anomalous sound detection through outlier exposure. Experiments showed that our proposed method improves the AUC of anomalous sound detection by an average of 6.8%.
    
[^52]: 通过风格滤波增强工业迁移学习：成本降低和缺陷聚焦

    Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction and Defect-Focus

    [https://arxiv.org/abs/2403.16607](https://arxiv.org/abs/2403.16607)

    Style Filter是一种在工业领域中量身定制的方法，通过在知识转移之前选择性地过滤源域数据，在减少数据量的同时，保持或提升了迁移学习策略的性能。

    

    在工业领域数据稀缺的挑战下，迁移学习作为一个关键范式应运而生。本研究引入了Style Filter，一种为工业环境量身定制的方法论。通过在知识转移之前有选择性地过滤源域数据，Style Filter减少了数据量，同时保持甚至提升了迁移学习策略的性能。Style Filter提供无需标签操作，最小化先前知识依赖，独立于特定模型，并可重复利用，本文在真实工业数据集上进行了评估，突显了其在深度学习领域中在常规迁移策略之前使用时的有效性。结果强调了Style Filter在真实工业应用中的有效性。

    arXiv:2403.16607v1 Announce Type: new  Abstract: Addressing the challenge of data scarcity in industrial domains, transfer learning emerges as a pivotal paradigm. This work introduces Style Filter, a tailored methodology for industrial contexts. By selectively filtering source domain data before knowledge transfer, Style Filter reduces the quantity of data while maintaining or even enhancing the performance of transfer learning strategy. Offering label-free operation, minimal reliance on prior knowledge, independence from specific models, and re-utilization, Style Filter is evaluated on authentic industrial datasets, highlighting its effectiveness when employed before conventional transfer strategies in the deep learning domain. The results underscore the effectiveness of Style Filter in real-world industrial applications.
    
[^53]: EDUE:专家分歧引导的一次过不确定性估计用于医学图像分割

    EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for Medical Image Segmentation

    [https://arxiv.org/abs/2403.16594](https://arxiv.org/abs/2403.16594)

    提出了一种专家分歧引导的不确定性估计（EDUE）方法，用于医学图像分割，在训练过程中通过地面实况注释的变异性来引导模型，并采用随机抽样策略以提高校准信心，实现了平均55%和23%的相关性改进。

    

    在医学应用中部署深度学习模型依赖于预测性能和其他重要因素，如传达可信的预测不确定性。不确定性估计（UE）方法为评估预测可靠性和提高模型置信度校准提供可能的解决方案。尽管对UE的兴趣日益增加，但仍存在挑战，如需要明确捕获适当不确定性的方法，并将不确定性估计与领域专家之间的现实分歧进行对齐。本文提出了一种专家分歧引导的不确定性估计（EDUE）方法，用于医学图像分割。通过利用来自多个评分者的地面实况注释中的变异性，我们在训练过程中引导模型，并结合基于随机抽样的策略，以增强校准信心。我们的方法在与图像专家分歧平均相关性方面实现了55%和23%的改进。

    arXiv:2403.16594v1 Announce Type: cross  Abstract: Deploying deep learning (DL) models in medical applications relies on predictive performance and other critical factors, such as conveying trustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide potential solutions for evaluating prediction reliability and improving the model confidence calibration. Despite increasing interest in UE, challenges persist, such as the need for explicit methods to capture aleatoric uncertainty and align uncertainty estimates with real-life disagreements among domain experts. This paper proposes an Expert Disagreement-Guided Uncertainty Estimation (EDUE) for medical image segmentation. By leveraging variability in ground-truth annotations from multiple raters, we guide the model during training and incorporate random sampling-based strategies to enhance calibration confidence. Our method achieves 55% and 23% improvement in correlation on average with expert disagreements at the image
    
[^54]: 揭示本地差分隐私、平均贝叶斯隐私和最大贝叶斯隐私之间的相互作用

    Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy

    [https://arxiv.org/abs/2403.16591](https://arxiv.org/abs/2403.16591)

    论文探讨了本地差分隐私、贝叶斯隐私及其之间的相互关系，揭示了关于效用-隐私权衡的新见解，并提出了一个框架来突出攻击和防御策略的相互作用和效果。

    

    机器学习的迅速发展导致了隐私定义的多样化，由于对隐私构成的威胁，包括本地差分隐私（LDP）的概念。虽然被广泛接受并在许多领域中被利用，但这种传统的隐私测量方法仍然存在一定限制，从无法防止推断披露到缺乏对对手背景知识的考虑。在这项全面研究中，我们引入贝叶斯隐私并深入探讨本地差分隐私和其贝叶斯对应物之间错综复杂的关系，揭示了关于效用-隐私权衡的新见解。我们引入了一个框架，概括了攻击和防御策略，突出它们之间的相互作用和效果。我们的理论贡献基于平均贝叶斯隐私（ABP）和最大贝叶斯隐私之间的严格定义和关系。

    arXiv:2403.16591v1 Announce Type: cross  Abstract: The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Max
    
[^55]: 在利用全球遥感数据进行作物分类的多视图学习模型的最佳选择研究

    In the Search for Optimal Multi-view Learning Models for Crop Classification with Global Remote Sensing Data

    [https://arxiv.org/abs/2403.16582](https://arxiv.org/abs/2403.16582)

    研究调查了在全球范围内同时选择融合策略和编码器架构对作物分类具有的影响。

    

    作物分类在研究作物模式变化、资源管理和碳固存中具有至关重要的作用。采用数据驱动技术进行预测时，利用各种时间数据源是必要的。深度学习模型已被证明对将时间序列数据映射到高级表示以进行预测任务非常有效。然而，当处理多个输入模式时，它们面临着重大挑战。文献对多视图学习（MVL）场景提供了有限的指导，主要集中在探索具有特定编码器的融合策略，并在局部地区对其进行验证。相反，我们研究了在全球范围内对农田土地和作物类型进行分类时同时选择融合策略和编码器架构的影响。

    arXiv:2403.16582v1 Announce Type: cross  Abstract: Crop classification is of critical importance due to its role in studying crop pattern changes, resource management, and carbon sequestration. When employing data-driven techniques for its prediction, utilizing various temporal data sources is necessary. Deep learning models have proven to be effective for this task by mapping time series data to high-level representation for prediction. However, they face substantial challenges when dealing with multiple input patterns. The literature offers limited guidance for Multi-View Learning (MVL) scenarios, as it has primarily focused on exploring fusion strategies with specific encoders and validating them in local regions. In contrast, we investigate the impact of simultaneous selection of the fusion strategy and the encoder architecture evaluated on a global-scale cropland and crop-type classifications. We use a range of five fusion strategies (Input, Feature, Decision, Ensemble, Hybrid) an
    
[^56]: 通过直接基于能量偏好优化的抗原特异性抗体设计

    Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization

    [https://arxiv.org/abs/2403.16576](https://arxiv.org/abs/2403.16576)

    通过直接基于能量偏好优化的方法，解决了抗原特异性抗体设计中的蛋白质序列-结构共设计问题，以生成具有理性结构和良好结合亲和力的抗体设计。

    

    抗体设计是一个至关重要的任务，对各种领域都有重要影响，如治疗和生物学，由于其错综复杂的性质，面临着相当大的挑战。在本文中，我们将抗原特异性抗体设计作为一个蛋白质序列-结构共设计问题，考虑了理性和功能性。利用一个预先训练的条件扩散模型，该模型联合建模抗体中互补决定区（CDR）的序列和结构，并结合了等变神经网络，我们提出了直接基于能量偏好优化的方法，以引导生成既具有合理结构又具有明显结合亲和力的抗体。我们的方法涉及使用残基级分解能量偏好对预先训练的扩散模型进行微调。此外，我们采用梯度手术来解决各种类型能量之间的冲突，例如吸引和斥

    arXiv:2403.16576v1 Announce Type: cross  Abstract: Antibody design, a crucial task with significant implications across various disciplines such as therapeutics and biology, presents considerable challenges due to its intricate nature. In this paper, we tackle antigen-specific antibody design as a protein sequence-structure co-design problem, considering both rationality and functionality. Leveraging a pre-trained conditional diffusion model that jointly models sequences and structures of complementarity-determining regions (CDR) in antibodies with equivariant neural networks, we propose direct energy-based preference optimization to guide the generation of antibodies with both rational structures and considerable binding affinities to given antigens. Our method involves fine-tuning the pre-trained diffusion model using a residue-level decomposed energy preference. Additionally, we employ gradient surgery to address conflicts between various types of energy, such as attraction and repu
    
[^57]: NSINA：用于僧伽罗语的新闻语料库

    NSINA: A News Corpus for Sinhala

    [https://arxiv.org/abs/2403.16571](https://arxiv.org/abs/2403.16571)

    NSINA是为解决僧伽罗语中LLMs适应性挑战而引入的最大新闻语料库，为改进该语言的自然语言处理提供了宝贵资源和基准。

    

    大型语言模型（LLMs）的引入推动了自然语言处理（NLP）的发展，但它们的有效性在很大程度上取决于预训练资源。尤其是在低资源语言（如僧伽罗语）中，这一点尤为明显，因为它们面临着两个主要挑战：缺乏充足的训练数据和有限的基准数据集。为应对这一问题，本研究介绍了NSINA，这是一个包括来自热门僧伽罗语新闻网站的50万多篇文章的全面新闻语料库，以及三项NLP任务：新闻媒体识别、新闻类别预测和新闻标题生成。NSINA的发布旨在为适应僧伽罗语的LLMs带来解决方案，提供有价值的资源和用于改进僧伽罗语NLP的基准。NSINA是迄今为止最大的僧伽罗语新闻语料库。

    arXiv:2403.16571v1 Announce Type: cross  Abstract: The introduction of large language models (LLMs) has advanced natural language processing (NLP), but their effectiveness is largely dependent on pre-training resources. This is especially evident in low-resource languages, such as Sinhala, which face two primary challenges: the lack of substantial training data and limited benchmarking datasets. In response, this study introduces NSINA, a comprehensive news corpus of over 500,000 articles from popular Sinhala news websites, along with three NLP tasks: news media identification, news category prediction, and news headline generation. The release of NSINA aims to provide a solution to challenges in adapting LLMs to Sinhala, offering valuable resources and benchmarks for improving NLP in the Sinhala language. NSINA is the largest news corpus for Sinhala, available up to date.
    
[^58]: 揭示神经网络在参数学习和对抗中的漏洞

    Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors

    [https://arxiv.org/abs/2403.16569](https://arxiv.org/abs/2403.16569)

    该研究揭示了神经网络在参数学习和对抗中的漏洞，引入了一种针对解释敏感的后门攻击的防御方法，通过统计分析来限制对神经网络模型的攻击。

    

    解释性人工智能（XAI）策略在增加对神经网络的理解和可信度方面起着至关重要的作用。然而，这些技术可能会产生误导性的解释。遮蔽攻击可以严重改变机器学习算法的预测和解释，在输入中添加不可见的视觉工件，同时保持模型的准确性，从而提供误导性信息。这给确保XAI方法的可信度带来了严峻挑战。为了保证XAI方法的可靠性，我们利用统计分析来突出遮蔽攻击后CNN内部CNN权重的变化。我们引入了一种专门设计的方法，旨在限制此类攻击在评估阶段的有效性，避免额外的训练需求。我们提出的方法可以抵御大多数现代的对解释敏感的对抗攻击。

    arXiv:2403.16569v1 Announce Type: new  Abstract: Explainable Artificial Intelligence (XAI) strategies play a crucial part in increasing the understanding and trustworthiness of neural networks. Nonetheless, these techniques could potentially generate misleading explanations. Blinding attacks can drastically alter a machine learning algorithm's prediction and explanation, providing misleading information by adding visually unnoticeable artifacts into the input, while maintaining the model's accuracy. It poses a serious challenge in ensuring the reliability of XAI methods. To ensure the reliability of XAI methods poses a real challenge, we leverage statistical analysis to highlight the changes in CNN weights within a CNN following blinding attacks. We introduce a method specifically designed to limit the effectiveness of such attacks during the evaluation phase, avoiding the need for extra training. The method we suggest defences against most modern explanation-aware adversarial attacks,
    
[^59]: FedFixer：在联邦学习中缓解异构标签噪声

    FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning

    [https://arxiv.org/abs/2403.16561](https://arxiv.org/abs/2403.16561)

    提出了FedFixer来解决联邦学习中异构标签噪声的问题，引入个性化模型与全局模型合作来有效选择干净的客户端特定样本，通过置信度正则化器和基于样本共享的双模型更新策略来减轻过拟合问题

    

    联邦学习(FL)在性能上严重依赖标签质量。然而，个体客户端之间的标签分布通常同时存在噪声和异构性。在异构标签噪声中由客户端特定样本引起的高损失对区分客户端特定和嘈杂标签样本构成了挑战，影响了现有标签噪声学习方法的有效性。为了解决这个问题，我们提出了FedFixer，其中引入了个性化模型与全局模型合作，以有效选择干净的客户端特定样本。在双模型中，仅在本地级别更新个性化模型可能导致由于样本有限而对噪声数据过度拟合，进而影响局部和全局模型的性能。为减轻过拟合，我们从两个角度解决了这个问题。

    arXiv:2403.16561v1 Announce Type: cross  Abstract: Federated Learning (FL) heavily depends on label quality for its performance. However, the label distribution among individual clients is always both noisy and heterogeneous. The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches. To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples. In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models' performance. To mitigate overfitting, we address this concern from two perspectives. Firstly, we employ a confidence regularizer to alleviate the impact of unconfident 
    
[^60]: 通过选择有益的局部梯度加速联邦学习

    Accelerating Federated Learning by Selecting Beneficial Herd of Local Gradients

    [https://arxiv.org/abs/2403.16557](https://arxiv.org/abs/2403.16557)

    提出了BHerd策略，通过选择有益的局部梯度加速联邦学习模型的收敛效率

    

    联邦学习（FL）是一种在通信网络系统中进行分布式机器学习的框架。然而，系统中的非独立同分布（Non-IID）数据对全局模型的收敛效率产生负面影响，因为只有这些数据样本的一个子集对模型收敛是有益的。为了寻找这个子集，一种可靠的方法是确定一个度量的有效性来对数据集中的样本进行排序。在本文中，我们提出了BHerd策略，该策略选择有益的局部梯度来加速FL模型的收敛。具体地，我们将局部数据集的分布映射到局部梯度上，并使用探羊策略获取梯度集合的排列，排列中较为先进的梯度靠近梯度集合的平均值。这些排在前面的梯度将被选中发送到服务器进行全局更新。

    arXiv:2403.16557v1 Announce Type: new  Abstract: Federated Learning (FL) is a distributed machine learning framework in communication network systems. However, the systems' Non-Independent and Identically Distributed (Non-IID) data negatively affect the convergence efficiency of the global model, since only a subset of these data samples are beneficial for model convergence. In pursuit of this subset, a reliable approach involves determining a measure of validity to rank the samples within the dataset. In this paper, We propose the BHerd strategy which selects a beneficial herd of local gradients to accelerate the convergence of the FL model. Specifically, we map the distribution of the local dataset to the local gradients and use the Herding strategy to obtain a permutation of the set of gradients, where the more advanced gradients in the permutation are closer to the average of the set of gradients. These top portion of the gradients will be selected and sent to the server for global
    
[^61]: 具有相关噪声的差分隐私在线联邦学习

    Differentially Private Online Federated Learning with Correlated Noise

    [https://arxiv.org/abs/2403.16542](https://arxiv.org/abs/2403.16542)

    提出一种利用相关噪声提高效用并确保隐私的差分隐私在线联邦学习算法，解决了DP噪声和本地更新带来的挑战，并在动态环境中建立了动态遗憾界。

    

    我们提出了一种新颖的差分隐私算法，用于在线联邦学习，利用时间相关的噪声来提高效用同时确保连续发布的模型的隐私性。为了解决源自DP噪声和本地更新带来的流式非独立同分布数据的挑战，我们开发了扰动迭代分析来控制DP噪声对效用的影响。此外，我们展示了在准强凸条件下如何有效管理来自本地更新的漂移误差。在$(\epsilon, \delta)$-DP预算范围内，我们建立了整个时间段上的动态遗憾界，量化了关键参数的影响以及动态环境变化的强度。数值实验证实了所提算法的有效性。

    arXiv:2403.16542v1 Announce Type: new  Abstract: We propose a novel differentially private algorithm for online federated learning that employs temporally correlated noise to improve the utility while ensuring the privacy of the continuously released models. To address challenges stemming from DP noise and local updates with streaming noniid data, we develop a perturbed iterate analysis to control the impact of the DP noise on the utility. Moreover, we demonstrate how the drift errors from local updates can be effectively managed under a quasi-strong convexity condition. Subject to an $(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the entire time horizon that quantifies the impact of key parameters and the intensity of changes in dynamic environments. Numerical experiments validate the efficacy of the proposed algorithm.
    
[^62]: 利用高阶累积量和路径分析从泊松分支结构因果模型中发现因果关系

    Causal Discovery from Poisson Branching Structural Causal Model Using High-Order Cumulant with Path Analysis

    [https://arxiv.org/abs/2403.16523](https://arxiv.org/abs/2403.16523)

    从计数数据中发现因果结构的关键挑战在于非可辨识性问题，本研究发现在泊松分支结构因果模型中，如果根顶点$X$是已知的，则可以确定从$X$到其子节点$Y$的因果顺序。

    

    计数数据在金融、神经科学和流行病学等领域中自然产生，在各种科学和工业场景中发现计数数据之间的因果结构是一项关键任务。计数数据的一个最常见特征是由二项式稀疏运算符和独立的泊松分布描述的固有分支结构，该结构捕捉了分支和噪声。例如，在人口计数情景中，死亡和移民对计数有贡献，其中生存遵循伯努利分布，移民遵循泊松分布。然而，由于不可辨识性问题，从这些数据中发现因果关系具有挑战性：单一因果对是马尔可夫等价的，即$X\rightarrow Y$和$Y\rightarrow X$在分布上是等价的。幸运的是，在这项工作中，我们发现如果$X$是一个根顶点，那么从$X$到其子节点$Y$的因果顺序是可识别的。

    arXiv:2403.16523v1 Announce Type: cross  Abstract: Count data naturally arise in many fields, such as finance, neuroscience, and epidemiology, and discovering causal structure among count data is a crucial task in various scientific and industrial scenarios. One of the most common characteristics of count data is the inherent branching structure described by a binomial thinning operator and an independent Poisson distribution that captures both branching and noise. For instance, in a population count scenario, mortality and immigration contribute to the count, where survival follows a Bernoulli distribution, and immigration follows a Poisson distribution. However, causal discovery from such data is challenging due to the non-identifiability issue: a single causal pair is Markov equivalent, i.e., $X\rightarrow Y$ and $Y\rightarrow X$ are distributed equivalent. Fortunately, in this work, we found that the causal order from $X$ to its child $Y$ is identifiable if $X$ is a root vertex and
    
[^63]: 2024年人类理解人工智能论文挑战——数据集设计

    Human Understanding AI Paper Challenge 2024 -- Dataset Design

    [https://arxiv.org/abs/2403.16509](https://arxiv.org/abs/2403.16509)

    2024年人类理解人工智能论文挑战将提供数据集，涉及人工智能技术以理解人类日常生活的研究和开发，重点考虑数据处理和学习模型的问题

    

    arXiv:2403.16509v1 发布类型：新的 摘要：在2024年，我们将举办一场研究论文竞赛（第三届人类理解人工智能论文挑战赛），旨在研究和开发人工智能技术以理解人类日常生活。本文介绍了将提供给参赛者的数据集，并总结了在数据处理和学习模型开发中需要考虑的问题。

    arXiv:2403.16509v1 Announce Type: new  Abstract: In 2024, we will hold a research paper competition (the third Human Understanding AI Paper Challenge) for the research and development of artificial intelligence technologies to understand human daily life. This document introduces the datasets that will be provided to participants in the competition, and summarizes the issues to consider in data processing and learning model development.
    
[^64]: PathoTune: 将视觉基础模型调整至病理专家

    PathoTune: Adapting Visual Foundation Model to Pathological Specialists

    [https://arxiv.org/abs/2403.16497](https://arxiv.org/abs/2403.16497)

    PathoTune提出了一个框架，能够通过多模态提示微调，将病理甚至视觉基础模型高效地调整到病理特定任务，从而缓解基础-任务差距和任务-实例差距。

    

    在自然图像理解走向预训练微调的时代的同时，病理影像的研究也在不断发展。尽管主要关注预训练病理基础模型，但如何将基础模型调整到下游任务中却鲜有研究。为了下游调整，我们提出存在两个域差距，即基础-任务差距和任务-实例差距。为了缓解这些差距，我们引入了 PathoTune，这是一个旨在通过多模态提示微调，高效地将病理甚至视觉基础模型调整到病理特定任务的框架。所提出的框架利用任务特定的视觉提示和任务特定的文本提示来识别任务相关特征，以及实例特定的视觉提示来编码单个病理图像特征。在多个数据集上以补丁级别和WSI级别的结果表明，其性能优于单模态。

    arXiv:2403.16497v1 Announce Type: cross  Abstract: As natural image understanding moves towards the pretrain-finetune era, research in pathology imaging is concurrently evolving. Despite the predominant focus on pretraining pathological foundation models, how to adapt foundation models to downstream tasks is little explored. For downstream adaptation, we propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the Task-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework designed to efficiently adapt pathological or even visual foundation models to pathology-specific tasks via multi-modal prompt tuning. The proposed framework leverages Task-specific Visual Prompts and Task-specific Textual Prompts to identify task-relevant features, along with Instance-specific Visual Prompts for encoding single pathological image features. Results across multiple datasets at both patch-level and WSI-level demonstrate its superior performance over single-modality
    
[^65]: LSTTN：基于长短期Transformer的时空神经网络用于交通流量预测

    LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural Network for Traffic Flow Forecasting

    [https://arxiv.org/abs/2403.16495](https://arxiv.org/abs/2403.16495)

    LSTTN框架综合考虑历史交通流量中的长期和短期特征，通过掩码子序列Transformer解决了现有STGNNs模型只能利用短程交通流量数据的限制，实现了对交通流量复杂趋势和周期特征的充分学习

    

    准确的交通预测是智能交通系统中一个基本问题，通过时空图神经网络（STGNNs）学习带关键信息的长程交通表示是当前交通流量预测模型的基本假设。然而，由于结构限制，现有的STGNNs只能利用短程交通流量数据；因此，这些模型无法充分学习交通流量中的复杂趋势和周期特征。此外，从长期历史交通系列中提取关键时间信息并获得紧凑表示也具有挑战性。为解决上述问题，我们提出了一种新颖的LSTTN（Long-Short Term Transformer-based Network）框架，全面考虑历史交通流量中的长期和短期特征。首先，我们利用一个掩码子序列Transformer来推断来自少量未被掩码的子序列的内容

    arXiv:2403.16495v1 Announce Type: cross  Abstract: Accurate traffic forecasting is a fundamental problem in intelligent transportation systems and learning long-range traffic representations with key information through spatiotemporal graph neural networks (STGNNs) is a basic assumption of current traffic flow prediction models. However, due to structural limitations, existing STGNNs can only utilize short-range traffic flow data; therefore, the models cannot adequately learn the complex trends and periodic features in traffic flow. Besides, it is challenging to extract the key temporal information from the long historical traffic series and obtain a compact representation. To solve the above problems, we propose a novel LSTTN (Long-Short Term Transformer-based Network) framework comprehensively considering the long- and short-term features in historical traffic flow. First, we employ a masked subseries Transformer to infer the content of masked subseries from a small portion of unmask
    
[^66]: 基于相似性提示的确定性多标签学习

    Determined Multi-Label Learning via Similarity-Based Prompt

    [https://arxiv.org/abs/2403.16482](https://arxiv.org/abs/2403.16482)

    提出了一种名为“Determined Multi-Label Learning”（DMLL）的新的标注设置，旨在有效减少多标签任务中固有的标注成本。

    

    在多标签分类中，每个训练实例同时与多个类标签相关联。然而，为每个训练实例收集完全精确的类标签对于现实世界的应用来说是耗时且耗力的。为了缓解这一问题，提出了一种名为“Determined Multi-Label Learning”（DMLL）的新的标注设置，旨在有效减少多标签任务中固有的标注成本。在这种新的标注设置中，每个训练实例与一个“确定性标签”（“是”或“否”）相关联，表示该训练实例是否包含所提供的类标签。所提供的类标签从整个候选标签集中随机均匀选择。此外，每个训练实例只需确定一次，这显著降低了多标签数据集标注任务的成本。

    arXiv:2403.16482v1 Announce Type: new  Abstract: In multi-label classification, each training instance is associated with multiple class labels simultaneously. Unfortunately, collecting the fully precise class labels for each training instance is time- and labor-consuming for real-world applications. To alleviate this problem, a novel labeling setting termed \textit{Determined Multi-Label Learning} (DMLL) is proposed, aiming to effectively alleviate the labeling cost inherent in multi-label tasks. In this novel labeling setting, each training instance is associated with a \textit{determined label} (either "Yes" or "No"), which indicates whether the training instance contains the provided class label. The provided class label is randomly and uniformly selected from the whole candidate labels set. Besides, each training instance only need to be determined once, which significantly reduce the annotation cost of the labeling task for multi-label datasets. In this paper, we theoretically de
    
[^67]: 学习从减少标签的长尾数据中

    Learning from Reduced Labels for Long-Tailed Data

    [https://arxiv.org/abs/2403.16469](https://arxiv.org/abs/2403.16469)

    提出了一种名为Reduced Label的新型弱监督标签设置，能够高效地学习长尾数据，避免了尾部样本监督信息的下降，降低了标签成本

    

    长尾数据在现实世界的分类任务中普遍存在，并且严重依赖监督信息，这使得注释过程异常耗时且费力。然而，尽管减少标注成本是缓解标签成本的常见方法，但现有的弱监督学习方法很难充分保留尾部样本的监督信息，导致尾部类别的准确率下降。为了缓解这一问题，我们提出了一种名为Reduced Label的新型弱监督标签设置。所提出的标签设置不仅避免了尾部样本的监督信息下降，还减少了与长尾数据相关的标签成本。此外，我们提出了一个简单直观且高效的无偏框架，具有强大的理论保证，可以从这些Reduced Labels中学习。在包括Imag在内的基准数据集上进行了广泛的实验

    arXiv:2403.16469v1 Announce Type: new  Abstract: Long-tailed data is prevalent in real-world classification tasks and heavily relies on supervised information, which makes the annotation process exceptionally labor-intensive and time-consuming. Unfortunately, despite being a common approach to mitigate labeling costs, existing weakly supervised learning methods struggle to adequately preserve supervised information for tail samples, resulting in a decline in accuracy for the tail classes. To alleviate this problem, we introduce a novel weakly supervised labeling setting called Reduced Label. The proposed labeling setting not only avoids the decline of supervised information for the tail samples, but also decreases the labeling costs associated with long-tailed data. Additionally, we propose an straightforward and highly efficient unbiased framework with strong theoretical guarantees to learn from these Reduced Labels. Extensive experiments conducted on benchmark datasets including Imag
    
[^68]: 使用增强条件鉴别器在有限数据下训练生成对抗网络声码器

    Training Generative Adversarial Network-Based Vocoder with Limited Data Using Augmentation-Conditional Discriminator

    [https://arxiv.org/abs/2403.16464](https://arxiv.org/abs/2403.16464)

    本文提出使用增强条件鉴别器在有限数据下训练生成对抗网络声码器，以避免数据采集成本高的问题，解决了标准鉴别器对增强数据的不敏感性，提高了模型的泛化能力。

    

    生成对抗网络（GAN）声码器在语音合成中常用，因为它具有快速、轻量和高质量的特点。然而，这种数据驱动模型需要大量训练数据，导致高昂的数据采集成本。这促使我们在有限数据下训练GAN声码器。一种有前途的解决方案是增强训练数据以避免过拟合。然而，标准鉴别器是无条件的，对于由数据增强引起的分布变化不敏感。因此，增强后的语音（可能非常出色）可能被认为是真实语音。为了解决这个问题，我们提出了一种增强条件鉴别器（AugCondD），除了语音外，它还接收增强状态作为输入，从而根据增强状态评估输入语音，而不抑制原始的非增强学习。

    arXiv:2403.16464v1 Announce Type: cross  Abstract: A generative adversarial network (GAN)-based vocoder trained with an adversarial discriminator is commonly used for speech synthesis because of its fast, lightweight, and high-quality characteristics. However, this data-driven model requires a large amount of training data incurring high data-collection costs. This fact motivates us to train a GAN-based vocoder on limited data. A promising solution is to augment the training data to avoid overfitting. However, a standard discriminator is unconditional and insensitive to distributional changes caused by data augmentation. Thus, augmented speech (which can be extraordinary) may be considered real speech. To address this issue, we propose an augmentation-conditional discriminator (AugCondD) that receives the augmentation state as input in addition to speech, thereby assessing the input speech according to the augmentation state, without inhibiting the learning of the original non-augmente
    
[^69]: FedAC：一种用于异构数据的自适应分簇联邦学习框架

    FedAC: A Adaptive Clustered Federated Learning Framework for Heterogeneous Data

    [https://arxiv.org/abs/2403.16460](https://arxiv.org/abs/2403.16460)

    FedAC框架通过解耦神经网络，使用不同聚合方法为每个子模块提供全局知识，引入经济高效的在线模型相似度度量，及集群数量微调模块，显著提高了性能。

    

    本文提出了一种自适应的分簇联邦学习框架FedAC，该框架通过解耦神经网络并利用不同的聚合方法为每个子模块有效地将全局知识整合到簇内学习中，显著提高了性能；引入了一种基于降维的经济高效的在线模型相似度度量；并且结合了一个用于改进复杂性的集群数量微调模块，从而提高了适应性和可伸缩性。

    arXiv:2403.16460v1 Announce Type: cross  Abstract: Clustered federated learning (CFL) is proposed to mitigate the performance deterioration stemming from data heterogeneity in federated learning (FL) by grouping similar clients for cluster-wise model training. However, current CFL methods struggle due to inadequate integration of global and intra-cluster knowledge and the absence of an efficient online model similarity metric, while treating the cluster count as a fixed hyperparameter limits flexibility and robustness. In this paper, we propose an adaptive CFL framework, named FedAC, which (1) efficiently integrates global knowledge into intra-cluster learning by decoupling neural networks and utilizing distinct aggregation methods for each submodule, significantly enhancing performance; (2) includes a costeffective online model similarity metric based on dimensionality reduction; (3) incorporates a cluster number fine-tuning module for improved adaptability and scalability in complex,
    
[^70]: 关于使用卷积神经网络进行学习收敛速率的研究

    On the rates of convergence for learning with convolutional neural networks

    [https://arxiv.org/abs/2403.16459](https://arxiv.org/abs/2403.16459)

    该研究提出了对具有一定权重约束的CNNs的新逼近上界，以及对前馈神经网络的覆盖数做了新的分析，为基于CNNs的学习问题推导了收敛速率，并在学习平滑函数和二元分类方面取得了极小最优的结果。

    

    我们研究了卷积神经网络（CNNs）的逼近和学习能力。第一个结果证明了在权重上有一定约束条件下CNNs的新逼近上界。第二个结果给出了对前馈神经网络的覆盖数的新分析，其中CNNs是其特例。该分析详细考虑了权重的大小，在某些情况下给出了比现有文献更好的上界。利用这两个结果，我们能够推导基于CNNs的估计器在许多学习问题中的收敛速率。特别地，我们在非参数回归设置中为基于CNNs的最小二乘学习平滑函数建立了极小最优的收敛速率。对于二元分类，我们推导了具有铰链损失和逻辑损失的CNN分类器的收敛速度。同时还表明所得到的速率在几种情况下是极小最优的。

    arXiv:2403.16459v1 Announce Type: new  Abstract: We study the approximation and learning capacities of convolutional neural networks (CNNs). Our first result proves a new approximation bound for CNNs with certain constraint on the weights. Our second result gives a new analysis on the covering number of feed-forward neural networks, which include CNNs as special cases. The analysis carefully takes into account the size of the weights and hence gives better bounds than existing literature in some situations. Using these two results, we are able to derive rates of convergence for estimators based on CNNs in many learning problems. In particular, we establish minimax optimal convergence rates of the least squares based on CNNs for learning smooth functions in the nonparametric regression setting. For binary classification, we derive convergence rates for CNN classifiers with hinge loss and logistic loss. It is also shown that the obtained rates are minimax optimal in several settings.
    
[^71]: DeepMachining: 铣床机床加工误差在线预测

    DeepMachining: Online Prediction of Machining Errors of Lathe Machines

    [https://arxiv.org/abs/2403.16451](https://arxiv.org/abs/2403.16451)

    DeepMachining是一种基于深度学习的AI系统，可以在线预测车床机床加工操作的误差，通过预训练和微调模型，实现了高准确性预测，是首批使用预训练深度学习模型预测车床机床加工误差的工厂实验之一。

    

    我们描述了DeepMachining，这是一种基于深度学习的人工智能系统，用于在线预测车床加工操作的加工误差。我们基于工厂的制造数据构建并评估了DeepMachining。具体来说，我们首先对特定车床机床操作预训练深度学习模型，以学习加工状态的显著特征。然后，我们微调预训练模型以适应特定加工任务。我们展示了DeepMachining在涉及不同工件和刀具的多个任务中实现了高预测准确性。据我们所知，这项工作是使用预训练深度学习模型预测车床机床加工误差的首批工厂实验之一。

    arXiv:2403.16451v1 Announce Type: cross  Abstract: We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states. Then, we fine-tune the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.
    
[^72]: 如果CLIP能说话: 通过它们的首选概念描述理解视觉-语言模型的表示

    If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions

    [https://arxiv.org/abs/2403.16442](https://arxiv.org/abs/2403.16442)

    通过新颖的Extract and Explore（EX2）方法，研究发现在视觉-语言模型（VLM）中，重要的特征描述包括非视觉属性，虚假描述影响VLM表示，不同的VLM优先考虑不同的内容。

    

    最近的研究常常假设视觉-语言模型（VLM）的表示是基于形状等视觉属性。然而，目前尚不清楚VLM在表示概念时在多大程度上将这些信息作为优先考虑对象。我们提出了一种新颖的方法，称为Extract and Explore（EX2），用于刻画VLM的重要文本特征。EX2使用强化学习将一个大型语言模型与VLM首选项对齐，并生成包含VLM重要特征的描述。然后，我们检查这些描述以确定对VLM表示有贡献的特征。我们发现，虽然提供了没有帮助信息的虚假描述（例如，单击放大概念的照片），但在VLM表示中起着重要作用。更重要的是，在信息丰富的描述中，VLM在表示视觉概念时显著依赖非视觉属性（如栖息地）。此外，我们的分析揭示了不同的VLM优先考虑不同的内容。

    arXiv:2403.16442v1 Announce Type: new  Abstract: Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore (EX2), a novel approach to characterize important textual features for VLMs. EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate the important features for the VLM. Then, we inspect the descriptions to identify the features that contribute to VLM representations. We find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat to represent visual concepts. Also, our analysis reveals that different VLMs prioritize differen
    
[^73]: 在轨迹预测中生成和利用在线地图不确定性

    Producing and Leveraging Online Map Uncertainty in Trajectory Prediction

    [https://arxiv.org/abs/2403.16439](https://arxiv.org/abs/2403.16439)

    通过扩展现有的在线地图估计方法以额外估计不确定性，能够更好地将在线制图与轨迹预测集成，从而在训练和预测性能上取得显著改善。

    

    高清晰（HD）地图在现代自动驾驶车辆（AV）系统的发展中发挥着至关重要的作用，尽管伴随着高昂的标注和维护成本。因此，许多最近的研究提出了从传感器数据在线估计HD地图的方法，使AV能够在先前未映射的区域操作。然而，当前的在线地图估计方法是独立开发的，这使得它们难以集成在AV系统中。具体来说，它们不生成不确定性或置信度估计。在这项工作中，我们扩展了多种最先进的在线地图估计方法，以额外估计不确定性，并展示了这如何能够更紧密地将在线地图制图与轨迹预测集成在一起。通过这样做，我们发现将不确定性整合进去可以使训练收敛速度提高高达50％，在真实世界的nuScenes驾驶数据上，预测性能提高高达15％。

    arXiv:2403.16439v1 Announce Type: cross  Abstract: High-definition (HD) maps have played an integral role in the development of modern autonomous vehicle (AV) stacks, albeit with high associated labeling and maintenance costs. As a result, many recent works have proposed methods for estimating HD maps online from sensor data, enabling AVs to operate outside of previously-mapped regions. However, current online map estimation approaches are developed in isolation of their downstream tasks, complicating their integration in AV stacks. In particular, they do not produce uncertainty or confidence estimates. In this work, we extend multiple state-of-the-art online map estimation methods to additionally estimate uncertainty and show how this enables more tightly integrating online mapping with trajectory forecasting. In doing so, we find that incorporating uncertainty yields up to 50% faster training convergence and up to 15% better prediction performance on the real-world nuScenes driving d
    
[^74]: 一种基于递增式MaxSAT的学习平衡规则模型

    An incremental MaxSAT-based model to learn balanced rules

    [https://arxiv.org/abs/2403.16418](https://arxiv.org/abs/2403.16418)

    提出了一种基于递增式MaxSAT的学习平衡规则模型IMLIB，结合了SAT和MaxSAT方法，限制规则大小以实现平衡，并提高模型性能。

    

    机器学习领域的不断发展导致了众多应用程序的开发，能够有效地解决各种问题并进行准确预测。然而，在某些情况下，仅准确性可能不足够。许多实际问题还需要解释和可解释性。本文旨在提出一种基于MaxSAT的增量模型用于学习可解释且平衡的规则，称为IMLIB。这个新模型基于另外两种方法，一种是基于SAT的，另一种是基于MaxSAT的。基于SAT的方法限制了每个生成规则的大小，使得可以平衡它们。我们认为这样一组规则比一个混合了大规则和小规则更容易理解。基于MaxSAT的方法，称为IMLI，提出了一种提高性能的技术。

    arXiv:2403.16418v1 Announce Type: cross  Abstract: The increasing advancements in the field of machine learning have led to the development of numerous applications that effectively address a wide range of problems with accurate predictions. However, in certain cases, accuracy alone may not be sufficient. Many real-world problems also demand explanations and interpretability behind the predictions. One of the most popular interpretable models that are classification rules. This work aims to propose an incremental model for learning interpretable and balanced rules based on MaxSAT, called IMLIB. This new model was based on two other approaches, one based on SAT and the other on MaxSAT. The one based on SAT limits the size of each generated rule, making it possible to balance them. We suggest that such a set of rules seem more natural to be understood compared to a mixture of large and small rules. The approach based on MaxSAT, called IMLI, presents a technique to increase performance th
    
[^75]: 通过整合多个分散的低曲率模型来进行集成对抗性防御

    Ensemble Adversarial Defense via Integration of Multiple Dispersed Low Curvature Models

    [https://arxiv.org/abs/2403.16405](https://arxiv.org/abs/2403.16405)

    通过降低攻击传递性来增强集成多样性，二阶梯度作为对抗性鲁棒性的关键因素。

    

    深度学习模型集成已被广泛探索，以增强对抗性攻击的防御能力。子模型之间的多样性增加了欺骗大多数集成所需的攻击成本，从而提高了对抗性鲁棒性。本文旨在通过降低攻击可转移性来增强集成多样性。我们确定了描述损失曲率的二阶梯度作为对抗性鲁棒性的关键因素。

    arXiv:2403.16405v1 Announce Type: new  Abstract: The integration of an ensemble of deep learning models has been extensively explored to enhance defense against adversarial attacks. The diversity among sub-models increases the attack cost required to deceive the majority of the ensemble, thereby improving the adversarial robustness. While existing approaches mainly center on increasing diversity in feature representations or dispersion of first-order gradients with respect to input, the limited correlation between these diversity metrics and adversarial robustness constrains the performance of ensemble adversarial defense. In this work, we aim to enhance ensemble diversity by reducing attack transferability. We identify second-order gradients, which depict the loss curvature, as a key factor in adversarial robustness. Computing the Hessian matrix involved in second-order gradients is computationally expensive. To address this, we approximate the Hessian-vector product using differentia
    
[^76]: 重新思考联邦非IID数据的无监督学习中的表示

    Rethinking the Representation in Federated Unsupervised Learning with Non-IID Data

    [https://arxiv.org/abs/2403.16398](https://arxiv.org/abs/2403.16398)

    提出了FedU2方法，通过灵活的统一正则化器（FUR）和高效的统一聚合器（EUA），增强了在具有非IID数据的FUSL中生成统一和统一表示。

    

    arXiv:2403.16398v1 公告类型: 跨领域 摘要: 联邦学习在建模分布式数据方面表现出有效性。在实践中，客户端数据标签不完善，这使得联邦非IID数据的无监督学习（FUSL）具有潜力。然而，现有FUSL方法的性能受到表示不足的影响，即（1）局部和全局模型之间的表示崩溃纠缠，以及（2）局部模型之间表示空间的不一致。前者表示局部模型中的表示崩溃将随后影响全局模型和其他局部模型。后者意味着由于缺乏监督信号，客户端模型数据表示具有不一致的参数。在这项工作中，我们提出了FedU2，该方法增强了在具有非IID数据的FUSL中生成统一和统一表示。具体而言，FedU2由灵活的统一正则化器（FUR）和高效的统一聚合器（EUA）组成。每个FUR

    arXiv:2403.16398v1 Announce Type: cross  Abstract: Federated learning achieves effective performance in modeling decentralized data. In practice, client data are not well-labeled, which makes it potential for federated unsupervised learning (FUSL) with non-IID data. However, the performance of existing FUSL methods suffers from insufficient representations, i.e., (1) representation collapse entanglement among local and global models, and (2) inconsistent representation spaces among local models. The former indicates that representation collapse in local model will subsequently impact the global model and other local models. The latter means that clients model data representation with inconsistent parameters due to the deficiency of supervision signals. In this work, we propose FedU2 which enhances generating uniform and unified representation in FUSL with non-IID data. Specifically, FedU2 consists of flexible uniform regularizer (FUR) and efficient unified aggregator (EUA). FUR in each
    
[^77]: 大型语言模型的并发语言错误检测（CLED）

    Concurrent Linguistic Error Detection (CLED) for Large Language Models

    [https://arxiv.org/abs/2403.16393](https://arxiv.org/abs/2403.16393)

    提出了一种针对大型语言模型的并发语言错误检测方案，通过提取文本的语言特征并使用分类器进行错误检测。

    

    大型语言模型（LLMs）的广泛采用使得它们的可靠性成为一个紧迫问题。错误的检测是减轻其对系统影响的第一步，因此，LLMs的高效错误检测是一个重要问题。基于对LLMs输出进行的观察，我们提出进行并发语言错误检测（CLED）；该方案提取LLMs生成文本的一些语言特征，并将它们输入到一个并发分类器中进行错误检测。

    arXiv:2403.16393v1 Announce Type: new  Abstract: The wide adoption of Large language models (LLMs) makes their dependability a pressing concern. Detection of errors is the first step to mitigating their impact on a system and thus, efficient error detection for LLMs is an important issue. In many settings, the LLM is considered as a black box with no access to the internal nodes; this prevents the use of many error detection schemes that need access to the model's internal nodes. An interesting observation is that the output of LLMs in error-free operation should be valid and normal text. Therefore, when the text is not valid or differs significantly from normal text, it is likely that there is an error. Based on this observation we propose to perform Concurrent Linguistic Error Detection (CLED); this scheme extracts some linguistic features of the text generated by the LLM and feeds them to a concurrent classifier that detects errors. Since the proposed error detection mechanism only 
    
[^78]: 物理信息强化强化学习用于最大安全概率估计

    Physics-informed RL for Maximal Safety Probability Estimation

    [https://arxiv.org/abs/2403.16391](https://arxiv.org/abs/2403.16391)

    利用物理信息强化学习算法可以估计最大安全行动的长期安全概率，避免风险过度逼近导致的保守行为

    

    精确的风险量化和可达性分析对于安全控制和学习至关重要，但从稀有事件、风险状态或长期轨迹中采样可能成本过高。我们研究了如何在没有足够覆盖来自风险状态和长期轨迹的样本的情况下估计最大安全行动的长期安全概率。在控制和学习中使用最大安全概率预计可以避免由于风险过度逼近而导致的保守行为。我们首先展示了长期安全概率可以转化为加法成本并且可以使用标准强化学习方法来解决。然后我们将这个概率推导为偏微分方程（PDE）的解，并提出了物理信息强化学习（PIRL）算法。所提出的方法可以使用稀疏奖励进行学习，因为物理约束。

    arXiv:2403.16391v1 Announce Type: cross  Abstract: Accurate risk quantification and reachability analysis are crucial for safe control and learning, but sampling from rare events, risky states, or long-term trajectories can be prohibitively costly. Motivated by this, we study how to estimate the long-term safety probability of maximally safe actions without sufficient coverage of samples from risky states and long-term trajectories. The use of maximal safety probability in control and learning is expected to avoid conservative behaviors due to over-approximation of risk. Here, we first show that long-term safety probability, which is multiplicative in time, can be converted into additive costs and be solved using standard reinforcement learning methods. We then derive this probability as solutions of partial differential equations (PDEs) and propose Physics-Informed Reinforcement Learning (PIRL) algorithm. The proposed method can learn using sparse rewards because the physics constrain
    
[^79]: 使用具有标签感知的神经过程进行实时适应条件监测信号预测

    Real-time Adaptation for Condition Monitoring Signal Prediction using Label-aware Neural Processes

    [https://arxiv.org/abs/2403.16377](https://arxiv.org/abs/2403.16377)

    提出了一种神经过程方法，可以在实时条件监测信号预测中实现表示能力和敏捷性的权衡

    

    建立一个快速适应实时条件监测（CM）信号的预测模型对于工程系统/单元至关重要。然而，许多当前方法在表示能力和在线环境中的敏捷性之间存在权衡问题。本文提出了一种基于神经过程的方法，解决了这一权衡问题。它将CM信号中的可用观测编码到表示空间中，然后重建信号的历史和演变以进行预测。

    arXiv:2403.16377v1 Announce Type: new  Abstract: Building a predictive model that rapidly adapts to real-time condition monitoring (CM) signals is critical for engineering systems/units. Unfortunately, many current methods suffer from a trade-off between representation power and agility in online settings. For instance, parametric methods that assume an underlying functional form for CM signals facilitate efficient online prediction updates. However, this simplification leads to vulnerability to model specifications and an inability to capture complex signals. On the other hand, approaches based on over-parameterized or non-parametric models can excel at explaining complex nonlinear signals, but real-time updates for such models pose a challenging task. In this paper, we propose a neural process-based approach that addresses this trade-off. It encodes available observations within a CM signal into a representation space and then reconstructs the signal's history and evolution for predi
    
[^80]: 基于渐进交互学习的自动驾驶轨迹预测

    ProIn: Learning to Predict Trajectory Based on Progressive Interactions for Autonomous Driving

    [https://arxiv.org/abs/2403.16374](https://arxiv.org/abs/2403.16374)

    通过渐进交互网络，代理在学习中逐渐聚焦于相关地图，以更好地捕获相关地图约束的特征表示。

    

    对于自动驾驶来说，准确预测行人、骑车者和其他周围车辆（统称为代理）的运动轨迹非常重要。本文提出了一种渐进交互网络，使代理的特征能够逐渐聚焦于相关地图，以更好地学习捕获相关地图约束的代理特征表示。

    arXiv:2403.16374v1 Announce Type: new  Abstract: Accurate motion prediction of pedestrians, cyclists, and other surrounding vehicles (all called agents) is very important for autonomous driving. Most existing works capture map information through an one-stage interaction with map by vector-based attention, to provide map constraints for social interaction and multi-modal differentiation. However, these methods have to encode all required map rules into the focal agent's feature, so as to retain all possible intentions' paths while at the meantime to adapt to potential social interaction. In this work, a progressive interaction network is proposed to enable the agent's feature to progressively focus on relevant maps, in order to better learn agents' feature representation capturing the relevant map constraints. The network progressively encode the complex influence of map constraints into the agent's feature through graph convolutions at the following three stages: after historical traj
    
[^81]: 具有联邦投票的SignSGD

    SignSGD with Federated Voting

    [https://arxiv.org/abs/2403.16372](https://arxiv.org/abs/2403.16372)

    signSGD-FV是一种具有联邦投票的优化器，通过在线学习边缘设备的权重分配以克服SignSGD-MV在小批量大小不同的工作者间收敛困难的问题。

    

    分布式学习通常用于通过利用多个边缘设备的计算能力来加速模型训练。然而，在实际应用中，由于工作者和中央参数服务器之间需要大量信息交换，通信延迟成为瓶颈。SignSGD与多数投票（signSGD-MV）是一种有效的分布式学习算法，通过一位量化可以显著减少通信成本。然而，由于异构的计算能力，当工作者之间的小批量大小不同时，它无法收敛。为了克服这一问题，我们提出了一种新颖的具有联邦投票的signSGD优化器（signSGD-FV）。联邦投票的理念在于利用可学习权重执行加权多数投票。服务器根据边缘设备的计算能力在线学习分配给这些设备的权重。

    arXiv:2403.16372v1 Announce Type: new  Abstract: Distributed learning is commonly used for accelerating model training by harnessing the computational capabilities of multiple-edge devices. However, in practical applications, the communication delay emerges as a bottleneck due to the substantial information exchange required between workers and a central parameter server. SignSGD with majority voting (signSGD-MV) is an effective distributed learning algorithm that can significantly reduce communication costs by one-bit quantization. However, due to heterogeneous computational capabilities, it fails to converge when the mini-batch sizes differ among workers. To overcome this, we propose a novel signSGD optimizer with \textit{federated voting} (signSGD-FV). The idea of federated voting is to exploit learnable weights to perform weighted majority voting. The server learns the weights assigned to the edge devices in an online fashion based on their computational capabilities. Subsequently,
    
[^82]: 使用不变性学习基于动作的表示

    Learning Action-based Representations Using Invariance

    [https://arxiv.org/abs/2403.16369](https://arxiv.org/abs/2403.16369)

    提出了一种新的方法，动作双模拟编码，通过递归不变性约束扩展了单步控制性，学习了一个可以平滑折扣远期元素的多步控制度量

    

    强化学习代理使用高维度观测必须能够在许多外源性干扰中识别相关状态特征。一个能够捕捉可控性的表示通过确定影响代理控制的因素来识别这些状态元素。虽然诸如逆动力学和互信息等方法可以捕捉有限数量的时间步的可控性，但捕获长时间元素仍然是一个具有挑战性的问题。短视的可控性可以捕捉代理即将撞向墙壁的瞬间，但不能在代理还有一定距离之时捕捉墙壁的控制相关性。为解决这个问题，我们提出了动作双模拟编码，这是一种受到双模拟不变量假度量启发的方法，它通过递归不变性约束扩展了单步控制性。通过这种方式，动作双模拟学习了一个平滑折扣远期元素的多步控制度量。

    arXiv:2403.16369v1 Announce Type: cross  Abstract: Robust reinforcement learning agents using high-dimensional observations must be able to identify relevant state features amidst many exogeneous distractors. A representation that captures controllability identifies these state elements by determining what affects agent control. While methods such as inverse dynamics and mutual information capture controllability for a limited number of timesteps, capturing long-horizon elements remains a challenging problem. Myopic controllability can capture the moment right before an agent crashes into a wall, but not the control-relevance of the wall while the agent is still some distance away. To address this we introduce action-bisimulation encoding, a method inspired by the bisimulation invariance pseudometric, that extends single-step controllability with a recursive invariance constraint. By doing this, action-bisimulation learns a multi-step controllability metric that smoothly discounts dist
    
[^83]: 从头开始利用引导扩散生成强大的毒药和后门

    Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion

    [https://arxiv.org/abs/2403.16365](https://arxiv.org/abs/2403.16365)

    使用引导扩散生成的基础样本能产生更强大的毒药和后门，优于先前最先进的攻击。

    

    现代神经网络经常在大规模数据集上进行训练，这些数据集经常不受人工检查而是通过网络抓取得到。由于这种不安全的筛选过程，攻击者可以通过向互联网上传恶意数据并等待受害者抓取和训练它，来污染或植入后门到生成的模型中。现有的创建毒药和后门的方法通常从随机抽取的干净数据（称为基础样本）开始，然后修改这些样本来生成毒药。但是，一些基础样本可能比其他更容易受到毒害。因此，我们可以通过仔细选择基础样本来精心制作更强大的毒药。在这项工作中，我们使用引导扩散从头开始合成基础样本，这些基础样本比先前的最先进攻击产生的更强大的毒药和后门。我们的Guided Diffusion Poisoning (GDP)基础样本可以与任何下游毒害或后门结合使用。

    arXiv:2403.16365v1 Announce Type: new  Abstract: Modern neural networks are often trained on massive datasets that are web scraped with minimal human inspection. As a result of this insecure curation pipeline, an adversary can poison or backdoor the resulting model by uploading malicious data to the internet and waiting for a victim to scrape and train on it. Existing approaches for creating poisons and backdoors start with randomly sampled clean data, called base samples, and then modify those samples to craft poisons. However, some base samples may be significantly more amenable to poisoning than others. As a result, we may be able to craft more potent poisons by carefully choosing the base samples. In this work, we use guided diffusion to synthesize base samples from scratch that lead to significantly more potent poisons and backdoors than previous state-of-the-art attacks. Our Guided Diffusion Poisoning (GDP) base samples can be combined with any downstream poisoning or backdoor at
    
[^84]: ChatDBG: 一种基于人工智能的调试助手

    ChatDBG: An AI-Powered Debugging Assistant

    [https://arxiv.org/abs/2403.16354](https://arxiv.org/abs/2403.16354)

    ChatDBG是第一个AI-Powered调试助手，通过将大型语言模型集成到传统调试器中，实现了程序员与调试器之间的协作对话，能够处理复杂问题、执行根本原因分析，并探索开放性查询。

    

    本文介绍了ChatDBG，这是第一个基于人工智能的调试助手。ChatDBG集成了大型语言模型(LLMs)，显著增强了传统调试器的功能和用户友好性。ChatDBG允许程序员与调试器进行协作对话，使他们能够提出关于程序状态的复杂问题，对崩溃或断言失败进行根本原因分析，并探索诸如“为什么x为空？”之类的开放性查询。为了处理这些查询，ChatDBG授予LLM自主权，通过发出命令来浏览堆栈和检查程序状态进行调试；然后报告其发现并将控制权交还给程序员。我们的ChatDBG原型与标准调试器集成，包括LLDB、GDB和WinDBG用于本地代码以及用于Python的Pdb。我们在各种代码集合上进行了评估，包括具有已知错误的C/C++代码和一套Python代码。

    arXiv:2403.16354v1 Announce Type: cross  Abstract: This paper presents ChatDBG, the first AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like "why is x null?". To handle these queries, ChatDBG grants the LLM autonomy to take the wheel and drive debugging by issuing commands to navigate through stacks and inspect program state; it then reports its findings and yields back control to the programmer. Our ChatDBG prototype integrates with standard debuggers including LLDB, GDB, and WinDBG for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code includi
    
[^85]: ChatGPT在软件评论中的不正确性检测

    ChatGPT Incorrectness Detection in Software Reviews

    [https://arxiv.org/abs/2403.16347](https://arxiv.org/abs/2403.16347)

    开发了一种名为CID的工具，通过迭代提示ChatGPT并要求提出情境相似但文本有分歧的问题，来自动测试和检测ChatGPT响应中的不正确性。

    

    我们对135名软件工程（SE）从业者进行了调查，以了解他们如何使用生成式AI聊天机器人（如ChatGPT）进行SE任务。我们发现他们希望将ChatGPT用于软件库选择等SE任务，但经常担心ChatGPT响应的真实性。我们开发了一套技术和一种名为CID（ChatGPT不正确性检测器）的工具，用于自动测试和检测ChatGPT响应中的不正确性。CID基于通过请求对ChatGPT进行迭代提示来提问具有情境相似但文本有分歧的问题（使用一种利用文本中的变态关系的方法）。CID的基本原则是，对于给定的问题，与其他响应不同（跨问题的多个化身）的响应可能是不正确的响应。 在一个关于库选择的基准研究中，我们展示CID能够检测ChatGPT的不正确响应。

    arXiv:2403.16347v1 Announce Type: cross  Abstract: We conducted a survey of 135 software engineering (SE) practitioners to understand how they use Generative AI-based chatbots like ChatGPT for SE tasks. We find that they want to use ChatGPT for SE tasks like software library selection but often worry about the truthfulness of ChatGPT responses. We developed a suite of techniques and a tool called CID (ChatGPT Incorrectness Detector) to automatically test and detect the incorrectness in ChatGPT responses. CID is based on the iterative prompting to ChatGPT by asking it contextually similar but textually divergent questions (using an approach that utilizes metamorphic relationships in texts). The underlying principle in CID is that for a given question, a response that is different from other responses (across multiple incarnations of the question) is likely an incorrect response. In a benchmark study of library selection, we show that CID can detect incorrect responses from ChatGPT with 
    
[^86]: 多环境场景中的预测推断

    Predictive Inference in Multi-environment Scenarios

    [https://arxiv.org/abs/2403.16336](https://arxiv.org/abs/2403.16336)

    本研究提出了在多环境预测问题中构建有效置信区间和置信集的方法，并展示了一种新的调整方法以适应问题难度，从而减少预测集大小，这在神经感应和物种分类数据集中的实际表现中得到验证。

    

    我们解决了在跨多个环境的预测问题中构建有效置信区间和置信集的挑战。我们研究了适用于这些问题的两种覆盖类型，扩展了Jackknife和分裂一致方法，展示了如何在这种非传统的层次数据生成场景中获得无分布覆盖。我们的贡献还包括对非实值响应设置的扩展，以及这些一般问题中预测推断的一致性理论。我们展示了一种新的调整方法，以适应问题难度，这适用于具有层次数据的预测推断的现有方法以及我们开发的方法；这通过神经化学感应和物种分类数据集评估了这些方法的实际性能。

    arXiv:2403.16336v1 Announce Type: cross  Abstract: We address the challenge of constructing valid confidence intervals and sets in problems of prediction across multiple environments. We investigate two types of coverage suitable for these problems, extending the jackknife and split-conformal methods to show how to obtain distribution-free coverage in such non-traditional, hierarchical data-generating scenarios. Our contributions also include extensions for settings with non-real-valued responses and a theory of consistency for predictive inference in these general problems. We demonstrate a novel resizing method to adapt to problem difficulty, which applies both to existing approaches for predictive inference with hierarchical data and the methods we develop; this reduces prediction set sizes using limited information from the test environment, a key to the methods' practical performance, which we evaluate through neurochemical sensing and species classification datasets.
    
[^87]: 通过多样化增强管道提升医疗数据集的效用：MEDDAP

    MEDDAP: Medical Dataset Enhancement via Diversified Augmentation Pipeline

    [https://arxiv.org/abs/2403.16335](https://arxiv.org/abs/2403.16335)

    MEDDAP通过稳定扩散（SD）模型自动生成新的信息标记样本，提升医疗数据集的效用

    

    深度神经网络（DNNs）的有效性严重依赖于丰富和准确的训练数据。然而，在医疗案例中，收集和注释大规模数据通常昂贵且耗时，尤其是当从业者已被工作占据。为了解决这一挑战，我们引入了一种名为MEDDAP的新型管道，利用稳定扩散（SD）模型通过自动生成新的信息标记样本来增强现有的小数据集。

    arXiv:2403.16335v1 Announce Type: cross  Abstract: The effectiveness of Deep Neural Networks (DNNs) heavily relies on the abundance and accuracy of available training data. However, collecting and annotating data on a large scale is often both costly and time-intensive, particularly in medical cases where practitioners are already occupied with their duties. Moreover, ensuring that the model remains robust across various scenarios of image capture is crucial in medical domains, especially when dealing with ultrasound images that vary based on the settings of different devices and the manual operation of the transducer. To address this challenge, we introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion (SD) models to augment existing small datasets by automatically generating new informative labeled samples. Pretrained checkpoints for SD are typically based on natural images, and training them for medical images requires significant GPU resources due to their heavy 
    
[^88]: 图在分布转移下的泛化

    Graphs Generalization under Distribution Shifts

    [https://arxiv.org/abs/2403.16334](https://arxiv.org/abs/2403.16334)

    本文介绍了一种名为GLIDER的新框架，旨在解决图结构数据中分布转移带来的挑战，并实现泛化性能优越。

    

    传统的机器学习方法严重依赖于独立同分布的假设，当测试分布与训练分布有所偏离时会受到限制。为了解决这一关键问题，针对已知分布转移而达到令人满意的泛化性能。然而，目前针对图结构数据的分布外泛化方法存在缺乏明晰性且尚未得到充分探索，主要由于两个主要挑战。首先，图中的分布转移通常同时发生在节点属性和图拓扑上。其次，在各种分布转移中捕获不变信息被证明是一个极具挑战性的问题。为了克服这些障碍，本文提出了一种全新的框架，即图学习不变域生成（GLIDER）。其目标是(1)多样化变化

    arXiv:2403.16334v1 Announce Type: cross  Abstract: Traditional machine learning methods heavily rely on the independent and identically distribution assumption, which imposes limitations when the test distribution deviates from the training distribution. To address this crucial issue, out-of-distribution (OOD) generalization, which aims to achieve satisfactory generalization performance when faced with unknown distribution shifts, has made a significant process. However, the OOD method for graph-structured data currently lacks clarity and remains relatively unexplored due to two primary challenges. Firstly, distribution shifts on graphs often occur simultaneously on node attributes and graph topology. Secondly, capturing invariant information amidst diverse distribution shifts proves to be a formidable challenge. To overcome these obstacles, in this paper, we introduce a novel framework, namely Graph Learning Invariant Domain genERation (GLIDER). The goal is to (1) diversify variations
    
[^89]: 使用深度学习和状态空间模型对模拟模拟动态范围压缩器进行建模

    Modeling Analog Dynamic Range Compressors using Deep Learning and State-space Models

    [https://arxiv.org/abs/2403.16331](https://arxiv.org/abs/2403.16331)

    通过分析模拟原型，使用深度学习和状态空间模型开发了模拟动态范围压缩器的逼真数字模型，具有更高的效率和更少的参数。

    

    我们描述了一种开发数字音频生产中动态范围压缩器逼真数字模型的新方法，该方法通过分析其模拟原型进行。尽管逼真的数字动态压缩器在许多应用中都有潜在用途，但由于这些压缩器在长时间尺度上非线性运行，因此设计过程具有挑战性。我们的方法基于结构化状态空间序列模型（S4），由于状态空间模型（SSM）的实施已被证明在学习长程依赖性方面高效，并且有望用于建模动态范围压缩器。本文中我们提出了一个具有S4层的深度学习模型，用于建模Teletronix LA-2A模拟动态范围压缩器。该模型是因果的，在实时执行时效率高，并且在参数更少的情况下实现了与先前深度学习模型大致相同的质量。

    arXiv:2403.16331v1 Announce Type: cross  Abstract: We describe a novel approach for developing realistic digital models of dynamic range compressors for digital audio production by analyzing their analog prototypes. While realistic digital dynamic compressors are potentially useful for many applications, the design process is challenging because the compressors operate nonlinearly over long time scales. Our approach is based on the structured state space sequence model (S4), as implementing the state-space model (SSM) has proven to be efficient at learning long-range dependencies and is promising for modeling dynamic range compressors. We present in this paper a deep learning model with S4 layers to model the Teletronix LA-2A analog dynamic range compressor. The model is causal, executes efficiently in real time, and achieves roughly the same quality as previous deep-learning models but with fewer parameters.
    
[^90]: 人工神经微电路作为基本构建模块：概念与挑战

    Artificial Neural Microcircuits as Building Blocks: Concept and Challenges

    [https://arxiv.org/abs/2403.16327](https://arxiv.org/abs/2403.16327)

    本文探索了一种新的方法，在生物学中神经微电路的启发下，使用人工神经微电路作为组装大型神经网络的基本构建模块，避免了结构上的齐质化所带来的复杂训练和学习工具的问题。

    

    人工神经网络(ANNs)是最广泛应用的生物启发式计算形式之一。然而，目前的趋势是将ANNs结构上保持齐质。此外，这种结构上的齐质性需要应用复杂的训练和学习工具，以产生特定应用的ANNs，容易遇到过拟合等问题。本文探讨了一种新的方法，灵感来自于神经微电路在生物学中所扮演的角色，即有机神经系统的“基本处理元件”。如何使用人工神经微电路(ANMs)组装大型神经网络，特别是脉冲神经网络(SNNs)，旨在作为现成零件，藉由利用新颖性搜索来产生这种微电路目录的初步工作结果；接着是扩展这项初步工作的努力，包括讨论相应的挑战。

    arXiv:2403.16327v1 Announce Type: cross  Abstract: Artificial Neural Networks (ANNs) are one of the most widely employed forms of bio-inspired computation. However the current trend is for ANNs to be structurally homogeneous. Furthermore, this structural homogeneity requires the application of complex training and learning tools that produce application specific ANNs, susceptible to pitfalls such as overfitting. In this paper, an new approach is explored, inspired by the role played in biology by Neural Microcircuits, the so called ``fundamental processing elements'' of organic nervous systems. How large neural networks, particularly Spiking Neural Networks (SNNs) can be assembled using Artificial Neural Microcircuits (ANMs), intended as off-the-shelf components, is articulated; the results of initial work to produce a catalogue of such Microcircuits though the use of Novelty Search is shown; followed by efforts to expand upon this initial work, including a discussion of challenges unc
    
[^91]: 在更细粒度上的优化：有界局部次梯度变化的视角

    Optimization on a Finer Scale: Bounded Local Subgradient Variation Perspective

    [https://arxiv.org/abs/2403.16317](https://arxiv.org/abs/2403.16317)

    该研究在有界局部次梯度变化的条件下研究非光滑优化问题，提出的目标函数类能帮助更好理解传统优化问题的复杂性，并在一般情况下降低oracle复杂度。

    

    我们在有界局部次梯度变化的条件下开始研究非光滑优化问题，它假设在点附近的小区域内，(次)梯度之间存在有限的差异，可以用平均或最大方式求值。由此产生的目标函数类包括传统优化中传统研究的目标函数类，这些类根据目标函数的Lipschitz连续性或其梯度的H\"{o}lder/Lipschitz连续性定义。此外，该定义类包含那些既不是Lipschitz连续的也没有H\"{o}lder连续梯度的函数。当限制在传统优化问题类时，定义研究类的参数导致更加精细的复杂性界限，在最坏情况下恢复传统的oracle复杂度界限，但一般情况下会导致那些不是“最坏情况”的函数具有较低的oracle 复杂性。

    arXiv:2403.16317v1 Announce Type: cross  Abstract: We initiate the study of nonsmooth optimization problems under bounded local subgradient variation, which postulates bounded difference between (sub)gradients in small local regions around points, in either average or maximum sense. The resulting class of objective functions encapsulates the classes of objective functions traditionally studied in optimization, which are defined based on either Lipschitz continuity of the objective or H\"{o}lder/Lipschitz continuity of its gradient. Further, the defined class contains functions that are neither Lipschitz continuous nor have a H\"{o}lder continuous gradient. When restricted to the traditional classes of optimization problems, the parameters defining the studied classes lead to more fine-grained complexity bounds, recovering traditional oracle complexity bounds in the worst case but generally leading to lower oracle complexity for functions that are not ``worst case.'' Some highlights of 
    
[^92]: 深度强化学习驱动调度可解释建模

    Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling

    [https://arxiv.org/abs/2403.16293](https://arxiv.org/abs/2403.16293)

    该研究提出了一个名为IRL的框架，通过将DNN解释为决策树，解决了DRL调度中深度神经网络缺乏可解释性的问题。

    

    在高性能计算（HPC）领域，最近探索了使用深度强化学习进行集群调度（DRL调度），取得了良好的成果。然而，深度神经网络（DNN）缺乏可解释性，使其成为系统管理员看不懂的黑匣模型，这导致了DRL调度的实际部署困难。本文提出了一个名为IRL（可解释强化学习）的框架，以解决DRL调度的可解释性问题。其核心思想是通过利用模仿学习将DNN（即DRL策略）解释为决策树。与DNN不同，决策树模型是非参数化的，易于人类理解。为了提取一个有效且高效的决策树，IRL结合了数据集聚合（DAgger）算法，并引入了评论概念。

    arXiv:2403.16293v1 Announce Type: new  Abstract: In the field of high-performance computing (HPC), there has been recent exploration into the use of deep reinforcement learning for cluster scheduling (DRL scheduling), which has demonstrated promising outcomes. However, a significant challenge arises from the lack of interpretability in deep neural networks (DNN), rendering them as black-box models to system managers. This lack of model interpretability hinders the practical deployment of DRL scheduling. In this work, we present a framework called IRL (Interpretable Reinforcement Learning) to address the issue of interpretability of DRL scheduling. The core idea is to interpret DNN (i.e., the DRL policy) as a decision tree by utilizing imitation learning. Unlike DNN, decision tree models are non-parametric and easily comprehensible to humans. To extract an effective and efficient decision tree, IRL incorporates the Dataset Aggregation (DAgger) algorithm and introduces the notion of crit
    
[^93]: 足球博彩的演变-机器学习方法预测比赛结果和书maker赔率估计

    The Evolution of Football Betting- A Machine Learning Approach to Match Outcome Forecasting and Bookmaker Odds Estimation

    [https://arxiv.org/abs/2403.16282](https://arxiv.org/abs/2403.16282)

    本研究利用机器学习算法预测英超联赛的比赛结果，通过分析历史数据和研究各种特征的意义，确定最有效的预测因素。

    

    本文探讨了职业足球和博彩行业的重要历史，追溯了其从秘密开始发展为一个利润丰厚的百万英镑企业的演变过程。从1960年合法化赌博开始，再到由Thorold Charles Reep开创的足球数据收集方法的进步，这两个领域之间的共生关系推动了快速增长和创新。在过去的六十年里，这两个行业都经历了根本性的变革，数据收集方法从基础的记笔记发展到了高清摄像头和人工智能驱动分析这样的先进技术。因此，本研究的主要目的是利用机器学习算法来预测英超联赛的比赛结果。通过分析历史数据和研究各种特征的意义，本研究旨在确定最有效的预测因素。

    arXiv:2403.16282v1 Announce Type: new  Abstract: This paper explores the significant history of professional football and the betting industry, tracing its evolution from clandestine beginnings to a lucrative multi-million-pound enterprise. Initiated by the legalization of gambling in 1960 and complemented by advancements in football data gathering pioneered by Thorold Charles Reep, the symbiotic relationship between these sectors has propelled rapid growth and innovation. Over the past six decades, both industries have undergone radical transformations, with data collection methods evolving from rudimentary notetaking to sophisticated technologies such as high-definition cameras and Artificial Intelligence (AI)-driven analytics. Therefore, the primary aim of this study is to utilize Machine Learning (ML) algorithms to forecast premier league football match outcomes. By analyzing historical data and investigating the significance of various features, the study seeks to identify the mos
    
[^94]: 通过深度多理解集成实现越界检测

    Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble

    [https://arxiv.org/abs/2403.16260](https://arxiv.org/abs/2403.16260)

    通过引入新颖的定性和定量模型集成评估方法，作者揭示了现有集成方法的关键缺陷，提出了提高传统模型集成维度的方法，以克服特征表示中的多样性限制。

    

    最近的研究强调了越界（OOD）特征表示领域规模对模型在OOD检测中效果的重要作用。因此，采用模型集成作为增强这一特征表示领域的突出策略已经成为一种突出的策略，利用预期的模型多样性。然而，我们引入了新颖的定性和定量模型集成评估方法，特别是损失盆/障碍可视化和自耦合指数，揭示了现有集成方法的一个关键缺陷。我们发现这些方法包含可进行仿射变换的权重，表现出有限的可变性，从而未能实现特征表示中所需的多样性。

    arXiv:2403.16260v1 Announce Type: cross  Abstract: Recent research underscores the pivotal role of the Out-of-Distribution (OOD) feature representation field scale in determining the efficacy of models in OOD detection. Consequently, the adoption of model ensembles has emerged as a prominent strategy to augment this feature representation field, capitalizing on anticipated model diversity.   However, our introduction of novel qualitative and quantitative model ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and the Self-Coupling Index, reveals a critical drawback in existing ensemble methods. We find that these methods incorporate weights that are affine-transformable, exhibiting limited variability and thus failing to achieve the desired diversity in feature representation.   To address this limitation, we elevate the dimensions of traditional model ensembles, incorporating various factors such as different weight initializations, data holdout, etc., into di
    
[^95]: 在模糊扩散合成中的拉普拉斯引导熵模型神经编解码器中

    Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis

    [https://arxiv.org/abs/2403.16258](https://arxiv.org/abs/2403.16258)

    采用非各向同性扩散模型和新颖的熵模型在神经图像压缩中，以区分频率内容并加速熵解码步骤，提高生成高质量图像的能力。

    

    将高斯解码器替换为条件扩散模型可以增强神经图像压缩中重建图像的感知质量，但它们对图像数据缺乏归纳偏差，限制了它们实现最先进感知水平的能力。为解决这一限制，我们在解码器端采用了非各向同性扩散模型。该模型施加了一种旨在区分频率内容的归纳偏差，从而促进高质量图像的生成。此外，我们的框架配备了一种新颖的熵模型，通过利用潜在空间中的空间通道相关性准确建模潜在表征的概率分布，同时加速熵解码步骤。这种通道级熵模型利用每个通道块内的本地和全局空间上下文。全局空间上下文是建立在专门设计用于……

    arXiv:2403.16258v1 Announce Type: cross  Abstract: While replacing Gaussian decoders with a conditional diffusion model enhances the perceptual quality of reconstructions in neural image compression, their lack of inductive bias for image data restricts their ability to achieve state-of-the-art perceptual levels. To address this limitation, we adopt a non-isotropic diffusion model at the decoder side. This model imposes an inductive bias aimed at distinguishing between frequency contents, thereby facilitating the generation of high-quality images. Moreover, our framework is equipped with a novel entropy model that accurately models the probability distribution of latent representation by exploiting spatio-channel correlations in latent space, while accelerating the entropy decoding step. This channel-wise entropy model leverages both local and global spatial contexts within each channel chunk. The global spatial context is built upon the Transformer, which is specifically designed for 
    
[^96]: 使用元启发方法改进序列到序列模型，用于抽象文本摘要

    Improving Sequence-to-Sequence Models for Abstractive Text Summarization Using Meta Heuristic Approaches

    [https://arxiv.org/abs/2403.16247](https://arxiv.org/abs/2403.16247)

    改进使用元启发方法的序列到序列模型，以提高抽象文本摘要的准确性和有效性

    

    随着人类社会过渡到信息时代，我们注意力的减少是一个必然趋势，花时间阅读冗长新闻文章的人群正在迅速减少，而对简洁信息的需求比以往任何时候都更高。因此，通过简洁地总结顶级新闻文章和最直观的标题，提供重要新闻的快速概述是至关重要的。人类在尝试进行摘要时，会从来源中提取基本信息，并从原始提取中添加有用短语和语法注释。人类有创建抽象的独特能力。然而，自动摘要是一个复杂的问题。对于神经抽象文本摘要，使用序列到序列（seq2seq）模型的应用程度不断增加。已经提出了许多创新策略来进一步发展当前的seq2seq模型，使其能够处理差异

    arXiv:2403.16247v1 Announce Type: new  Abstract: As human society transitions into the information age, reduction in our attention span is a contingency, and people who spend time reading lengthy news articles are decreasing rapidly and the need for succinct information is higher than ever before. Therefore, it is essential to provide a quick overview of important news by concisely summarizing the top news article and the most intuitive headline. When humans try to make summaries, they extract the essential information from the source and add useful phrases and grammatical annotations from the original extract. Humans have a unique ability to create abstractions. However, automatic summarization is a complicated problem to solve. The use of sequence-to-sequence (seq2seq) models for neural abstractive text summarization has been ascending as far as prevalence. Numerous innovative strategies have been proposed to develop the current seq2seq models further, permitting them to handle diffe
    
[^97]: 部分遮蔽的遗忘: 一种贝叶斯视角下的深度网络类别遗忘

    Partially Blinded Unlearning: Class Unlearning for Deep Networks a Bayesian Perspective

    [https://arxiv.org/abs/2403.16246](https://arxiv.org/abs/2403.16246)

    该研究提出了一种针对预训练分类网络中特定类别数据的目的性消除方法，以降低模型对未学习数据类别的性能影响，同时最小化其他类别性能的不利影响。

    

    为了遵守监管个人数据隐私和安全的标准，机器学习模型必须系统地消除从用户训练数据的特定子集导出的无法再利用的信息。机器遗忘这一新兴学科已成为一个重要的研究领域，促进了有选择性地丢弃预训练模型中指定给特定数据集或类别的信息的过程，从而消除了必须从头开始进行广泛重新训练的必要性。本研究的主要目的是制定一种针对从预训练分类网络中特定类别的数据中目的性消除信息的方法论。这种有意的去除旨在降低模型对未学习数据类别的性能，同时最小化对模型在其他类别中性能的任何不利影响。

    arXiv:2403.16246v1 Announce Type: new  Abstract: In order to adhere to regulatory standards governing individual data privacy and safety, machine learning models must systematically eliminate information derived from specific subsets of a user's training data that can no longer be utilized. The emerging discipline of Machine Unlearning has arisen as a pivotal area of research, facilitating the process of selectively discarding information designated to specific sets or classes of data from a pre-trained model, thereby eliminating the necessity for extensive retraining from scratch. The principal aim of this study is to formulate a methodology tailored for the purposeful elimination of information linked to a specific class of data from a pre-trained classification network. This intentional removal is crafted to degrade the model's performance specifically concerning the unlearned data class while concurrently minimizing any detrimental impacts on the model's performance in other classe
    
[^98]: 关于合成数据的等效性、可替代性和灵活性

    On the Equivalency, Substitutability, and Flexibility of Synthetic Data

    [https://arxiv.org/abs/2403.16244](https://arxiv.org/abs/2403.16244)

    本研究从经验的角度探讨了合成数据在解决实际问题中的有效性，着重研究了合成数据与真实数据的等效性、可替代性以及生成器的灵活性。

    

    我们从经验的角度研究了合成数据在现实世界场景中的有效性。利用合成数据训练感知模型已经成为社区采纳的关键策略，因为它具有高效性、扩展性、完美的注释和低成本。尽管已经证明有优势，但很少有研究强调如何有效生成合成数据集来解决实际问题，以及合成数据在多大程度上可以减少实际数据收集的工作量。为了回答这些问题，我们系统地研究了合成数据的几个有趣属性 -- 合成数据与真实数据的等效性，合成数据对实际数据的替代性，以及合成数据生成器的灵活性来填补领域差距。利用M3Act合成数据生成器，我们在DanceTrack和MOT17上进行了实验。我们的结果表明，合成数据不仅增强了模型的性能

    arXiv:2403.16244v1 Announce Type: new  Abstract: We study, from an empirical standpoint, the efficacy of synthetic data in real-world scenarios. Leveraging synthetic data for training perception models has become a key strategy embraced by the community due to its efficiency, scalability, perfect annotations, and low costs. Despite proven advantages, few studies put their stress on how to efficiently generate synthetic datasets to solve real-world problems and to what extent synthetic data can reduce the effort for real-world data collection. To answer the questions, we systematically investigate several interesting properties of synthetic data -- the equivalency of synthetic data to real-world data, the substitutability of synthetic data for real data, and the flexibility of synthetic data generators to close up domain gaps. Leveraging the M3Act synthetic data generator, we conduct experiments on DanceTrack and MOT17. Our results suggest that synthetic data not only enhances model per
    
[^99]: 训练在具有不同噪声的随机疾病传播模型上的预警指标

    An early warning indicator trained on stochastic disease-spreading models with different noises

    [https://arxiv.org/abs/2403.16233](https://arxiv.org/abs/2403.16233)

    使用深度学习算法训练在受噪声影响的疾病传播模型上的早期预警指标，以提供传染病爆发的有效预警信号。

    

    及时通过可靠的早期预警信号（EWSs）检测疾病爆发对于有效的公共卫生缓解战略至关重要。然而，真实世界疾病传播的复杂动态，常常受到多种噪声源和早期爆发阶段有限数据的影响，对于开发可靠的EWSs构成了重大挑战，因为现有指标的性能因外部和内在噪声的变化而不同。在这里，我们解决了当测量被加性白噪声、乘性环境噪声和人口噪声污染时建模疾病的挑战，将其纳入标准传染病数学模型。为了应对这些噪声源引入的复杂性，我们采用深度学习算法，通过在感染病爆发中受噪声影响的模型上进行训练来提供EWS。通过应用，指标的有效性得到了证明。

    arXiv:2403.16233v1 Announce Type: new  Abstract: The timely detection of disease outbreaks through reliable early warning signals (EWSs) is indispensable for effective public health mitigation strategies. Nevertheless, the intricate dynamics of real-world disease spread, often influenced by diverse sources of noise and limited data in the early stages of outbreaks, pose a significant challenge in developing reliable EWSs, as the performance of existing indicators varies with extrinsic and intrinsic noises. Here, we address the challenge of modeling disease when the measurements are corrupted by additive white noise, multiplicative environmental noise, and demographic noise into a standard epidemic mathematical model. To navigate the complexities introduced by these noise sources, we employ a deep learning algorithm that provides EWS in infectious disease outbreak by training on noise-induced disease-spreading models. The indicator's effectiveness is demonstrated through its application
    
[^100]: CoverUp：基于覆盖率引导的LLM测试生成系统

    CoverUp: Coverage-Guided LLM-Based Test Generation

    [https://arxiv.org/abs/2403.16218](https://arxiv.org/abs/2403.16218)

    CoverUp通过覆盖率分析和大型语言模型相结合的方式，驱动生成高覆盖率的Python回归测试，并在改进覆盖率方面取得显著成就。

    

    本文介绍了CoverUp，这是一个新型系统，通过覆盖率分析和大型语言模型（LLM）的结合驱动生成高覆盖率的Python回归测试。CoverUp通过迭代改善覆盖率，将覆盖率分析与LLM对话交替进行，以便将注意力集中在尚未涵盖的代码行和分支上。最终的测试套件相比当前技术水平显著提高了覆盖率：与CodaMosa相比，一种混合LLM / 基于搜索的软件测试系统，CoverUp在各方面都大幅提高了覆盖率。以模块为基础，CoverUp实现了81%的中位线覆盖率（对比62%）、53%的分支覆盖率（对比35%）和78%的线+分支覆盖率（对比55%）。我们展示了CoverUp的迭代、覆盖率引导方法对其有效性至关重要，为其成功的近一半作出了贡献。

    arXiv:2403.16218v1 Announce Type: cross  Abstract: This paper presents CoverUp, a novel system that drives the generation of high-coverage Python regression tests via a combination of coverage analysis and large-language models (LLMs). CoverUp iteratively improves coverage, interleaving coverage analysis with dialogs with the LLM to focus its attention on as yet uncovered lines and branches. The resulting test suites significantly improve coverage over the current state of the art: compared to CodaMosa, a hybrid LLM / search-based software testing system, CoverUp substantially improves coverage across the board. On a per-module basis, CoverUp achieves median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and line+branch coverage of 78% (vs. 55%). We show that CoverUp's iterative, coverage-guided approach is crucial to its effectiveness, contributing to nearly half of its successes.
    
[^101]: 连续时间神经网络系统化构建用于线性动力系统

    Systematic construction of continuous-time neural networks for linear dynamical systems

    [https://arxiv.org/abs/2403.16215](https://arxiv.org/abs/2403.16215)

    提出了一种系统化构建神经网络架构的方法，用于模拟线性定常系统，并提出了一种无梯度算法，直接从给定的系统中计算稀疏的网络架构和参数，具有水平隐藏层的新型神经网络架构范式

    

    arXiv:2403.16215v1 公告类型：新摘要：发现适合模拟复杂动力系统的神经网络架构往往是一个艰巨的挑战，通常涉及大量的试错和在高维超参数空间中导航。在本文中，我们讨论了一种系统化构建神经网络架构的方法，用于模拟一类动力系统，即线性定常(LTI)系统。我们使用连续时间神经网络的变体，其中每个神经元的输出连续地演化为一阶或二阶常微分方程的解。我们提出了一个无梯度算法，直接从给定的LTI系统中计算稀疏的网络架构和网络参数，利用其特性，而不是从数据中推导网络架构和参数。我们提出了一个具有水平隐藏层的新型神经网络架构范式，并阐明了为什么使用con

    arXiv:2403.16215v1 Announce Type: new  Abstract: Discovering a suitable neural network architecture for modeling complex dynamical systems poses a formidable challenge, often involving extensive trial and error and navigation through a high-dimensional hyper-parameter space. In this paper, we discuss a systematic approach to constructing neural architectures for modeling a subclass of dynamical systems, namely, Linear Time-Invariant (LTI) systems. We use a variant of continuous-time neural networks in which the output of each neuron evolves continuously as a solution of a first-order or second-order Ordinary Differential Equation (ODE). Instead of deriving the network architecture and parameters from data, we propose a gradient-free algorithm to compute sparse architecture and network parameters directly from the given LTI system, leveraging its properties. We bring forth a novel neural architecture paradigm featuring horizontal hidden layers and provide insights into why employing con
    
[^102]: 利用深度学习和Xception架构提高阿尔茨海默病MRI分类的准确性

    Leveraging Deep Learning and Xception Architecture for High-Accuracy MRI Classification in Alzheimer Diagnosis

    [https://arxiv.org/abs/2403.16212](https://arxiv.org/abs/2403.16212)

    这项研究旨在利用深度学习模型对MRI图像进行分类，通过一系列创新的数据处理和模型构建步骤，识别不同阶段的阿尔茨海默病。

    

    探索深度学习技术在医学诊断领域的应用，磁共振成像（MRI）为观察和诊断复杂神经退行性疾病如阿尔茨海默病提供了独特视角。随着深度学习尤其是卷积神经网络（CNNs）和Xception网络架构的进步，我们现在能够以前所未有的准确性分析和分类大量MRI数据。这项技术的进展不仅提高了我们对脑结构变化的理解，还为通过非侵入性手段监测疾病进展打开了新的途径，同时也有望在疾病早期阶段实现精确诊断。

    arXiv:2403.16212v1 Announce Type: cross  Abstract: Exploring the application of deep learning technologies in the field of medical diagnostics, Magnetic Resonance Imaging (MRI) provides a unique perspective for observing and diagnosing complex neurodegenerative diseases such as Alzheimer Disease (AD). With advancements in deep learning, particularly in Convolutional Neural Networks (CNNs) and the Xception network architecture, we are now able to analyze and classify vast amounts of MRI data with unprecedented accuracy. The progress of this technology not only enhances our understanding of brain structural changes but also opens up new avenues for monitoring disease progression through non-invasive means and potentially allows for precise diagnosis in the early stages of the disease.   This study aims to classify MRI images using deep learning models to identify different stages of Alzheimer Disease through a series of innovative data processing and model construction steps. Our experim
    
[^103]: OT-Flow样本生成的收敛性分析

    Convergence analysis of OT-Flow for sample generation

    [https://arxiv.org/abs/2403.16208](https://arxiv.org/abs/2403.16208)

    本文旨在为OT-Flow这一深度生成模型建立一些收敛结果，包括在正则化项参数趋于无穷大时，OT-Flow与最优输运问题的收敛关系，以及随着样本数趋于无穷大时，离散损失函数与连续损失函数之间的收敛关系。

    

    深度生成模型的目标是学习数据的潜在分布并生成新的数据。尽管生成模型的多样性和实践中的高质量生成性能，大多数模型缺乏严格的理论收敛证明。在这项工作中，我们旨在建立OT-Flow这一深度生成模型的一些收敛结果。首先，通过重新构建OT-Flow模型的框架，我们证明了在正则化项参数$\alpha$趋于无穷大时，OT-Flow模型的表述与对应的最优输运（OT）问题之间的$\Gamma$-收敛。其次，由于损失函数在训练中将通过蒙特卡洛方法逼近，当样本数$N$趋于无穷大时，我们也证明了离散损失函数与连续损失函数之间的收敛性。与此同时，神经网络的逼近能力为离散损失函数提供了一个上界。

    arXiv:2403.16208v1 Announce Type: cross  Abstract: Deep generative models aim to learn the underlying distribution of data and generate new ones. Despite the diversity of generative models and their high-quality generation performance in practice, most of them lack rigorous theoretical convergence proofs. In this work, we aim to establish some convergence results for OT-Flow, one of the deep generative models. First, by reformulating the framework of OT-Flow model, we establish the $\Gamma$-convergence of the formulation of OT-flow to the corresponding optimal transport (OT) problem as the regularization term parameter $\alpha$ goes to infinity. Second, since the loss function will be approximated by Monte Carlo method in training, we established the convergence between the discrete loss function and the continuous one when the sample number $N$ goes to infinity as well. Meanwhile, the approximation capability of the neural network provides an upper bound for the discrete loss function
    
[^104]: 从离散到连续: 具有可迁移表示的深度公平聚类

    From Discrete to Continuous: Deep Fair Clustering With Transferable Representations

    [https://arxiv.org/abs/2403.16201](https://arxiv.org/abs/2403.16201)

    提出一种能够同时处理离散和连续敏感属性的深度公平聚类方法，通过信息瓶颈风格的目标函数学习公平且有利于聚类的表示

    

    我们考虑了深度公平聚类的问题，通过深度神经网络提取的表示将数据分成簇，同时隐藏敏感数据属性。现有方法为了实现公平性，提出了基于群体公平性标准的各种公平性相关目标函数。然而，这些工作通常假定敏感属性是离散的，对于连续敏感变量（如区域中女性人口比例）不起作用。此外，现有工作忽略了从聚类任务中学习的表示来提高其他任务性能的潜力。鉴于这些局限性，我们提出了一种灵活的深度公平聚类方法，可以同时处理离散和连续敏感属性。具体而言，我们设计了一个信息瓶颈风格的目标函数来学习公平且有利于聚类的表示。

    arXiv:2403.16201v1 Announce Type: new  Abstract: We consider the problem of deep fair clustering, which partitions data into clusters via the representations extracted by deep neural networks while hiding sensitive data attributes. To achieve fairness, existing methods present a variety of fairness-related objective functions based on the group fairness criterion. However, these works typically assume that the sensitive attributes are discrete and do not work for continuous sensitive variables, such as the proportion of the female population in an area. Besides, the potential of the representations learned from clustering tasks to improve performance on other tasks is ignored by existing works. In light of these limitations, we propose a flexible deep fair clustering method that can handle discrete and continuous sensitive attributes simultaneously. Specifically, we design an information bottleneck style objective function to learn fair and clustering-friendly representations. Furtherm
    
[^105]: 基于逻辑的支持向量机线性分类器拒绝选项的解释

    Logic-based Explanations for Linear Support Vector Classifiers with Reject Option

    [https://arxiv.org/abs/2403.16190](https://arxiv.org/abs/2403.16190)

    提出了一种基于逻辑的方法，针对具有拒绝选项的线性SVC，提供了正确性和极小性保证的解释，相比启发式算法Anchors，我们的方法给出了更短的解释

    

    支持向量分类器（SVC）是一种用于线性分类问题的众所周知的机器学习（ML）模型。它可以与拒绝选项策略结合使用，拒绝难以正确分类的实例，并将它们委托给专家。这进一步增加了模型的信心。因此，获得拒绝原因的解释对于不盲目信任所获结果是重要的。尽管大多数相关工作已经开发出了针对机器学习模型提供这种解释的手段，但就我们所知，尚未有人为存在拒绝选项时提供这种解释。我们提出了一种基于逻辑的方法，对具有拒绝选项的线性SVC的解释进行正确性和极小性的形式保证。我们通过将其与生成解释的启发式算法Anchors进行比较来评估我们的方法。所获结果显示，我们提出的方法给出了更短的解释。

    arXiv:2403.16190v1 Announce Type: new  Abstract: Support Vector Classifier (SVC) is a well-known Machine Learning (ML) model for linear classification problems. It can be used in conjunction with a reject option strategy to reject instances that are hard to correctly classify and delegate them to a specialist. This further increases the confidence of the model. Given this, obtaining an explanation of the cause of rejection is important to not blindly trust the obtained results. While most of the related work has developed means to give such explanations for machine learning models, to the best of our knowledge none have done so for when reject option is present. We propose a logic-based approach with formal guarantees on the correctness and minimality of explanations for linear SVCs with reject option. We evaluate our approach by comparing it to Anchors, which is a heuristic algorithm for generating explanations. Obtained results show that our proposed method gives shorter explanations
    
[^106]: 子空间防御：通过学习干净信号的子空间来丢弃对抗性扰动

    Subspace Defense: Discarding Adversarial Perturbations by Learning a Subspace for Clean Signals

    [https://arxiv.org/abs/2403.16176](https://arxiv.org/abs/2403.16176)

    通过学习一个仅存在干净信号特征的子空间并丢弃扰动特征，使得深度神经网络能够更好地区分对抗性示例。

    

    深度神经网络(DNNs)极易受到对抗性攻击的影响，即在正常示例上放置精心制作的扰动以欺骗DNNs。为了更好地理解这类攻击，需要对对抗性示例携带的特征进行刻画。本文通过对样本特征子空间进行谱分析来解决这一挑战。我们首先经验性地展示，无论是干净信号还是对抗性扰动的特征都是冗余的，并分别在低维线性子空间中展开，互相之间重叠很小，而经典的低维子空间投影可以将扰动特征排除在干净信号子空间之外。这使得DNNs能够学习一个仅存在干净信号特征的子空间，而丢弃扰动特征，这有助于区分对抗性示例。为了防止残余的扰动，提出了一种二次最优方向残差网络(QPRN)。

    arXiv:2403.16176v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) are notoriously vulnerable to adversarial attacks that place carefully crafted perturbations on normal examples to fool DNNs. To better understand such attacks, a characterization of the features carried by adversarial examples is needed. In this paper, we tackle this challenge by inspecting the subspaces of sample features through spectral analysis. We first empirically show that the features of either clean signals or adversarial perturbations are redundant and span in low-dimensional linear subspaces respectively with minimal overlap, and the classical low-dimensional subspace projection can suppress perturbation features out of the subspace of clean signals. This makes it possible for DNNs to learn a subspace where only features of clean signals exist while those of perturbations are discarded, which can facilitate the distinction of adversarial examples. To prevent the residual perturbations that is ine
    
[^107]: 神经网络中协方差传播的解析解

    An Analytic Solution to Covariance Propagation in Neural Networks

    [https://arxiv.org/abs/2403.16163](https://arxiv.org/abs/2403.16163)

    该论文提出了一种无需样本的矩传播技术，能够准确表征神经网络的输入输出分布，其关键创新在于提供了通过非线性激活函数传递的随机变量协方差的解析解。

    

    神经网络的不确定性量化对于衡量深度学习系统的可靠性和鲁棒性至关重要。然而，这通常涉及昂贵或不准确的采样方法和近似。本文提出了一种无需样本的矩传播技术，通过网络传播均值向量和协方差矩阵，准确表征神经网络的输入输出分布。我们的技术的一个关键优势是为通过非线性激活函数（如Heaviside、ReLU和GELU）传递的随机变量的协方差提供了解析解。通过分析经过训练的神经网络的输入输出分布以及训练贝叶斯神经网络的实验，展示了所提出技术的广泛适用性和优点。

    arXiv:2403.16163v1 Announce Type: cross  Abstract: Uncertainty quantification of neural networks is critical to measuring the reliability and robustness of deep learning systems. However, this often involves costly or inaccurate sampling methods and approximations. This paper presents a sample-free moment propagation technique that propagates mean vectors and covariance matrices across a network to accurately characterize the input-output distributions of neural networks. A key enabler of our technique is an analytic solution for the covariance of random variables passed through nonlinear activation functions, such as Heaviside, ReLU, and GELU. The wide applicability and merits of the proposed technique are shown in experiments analyzing the input-output distributions of trained neural networks and training Bayesian neural networks.
    
[^108]: 一个掩盖模型就足够实现传感器故障检测、隔离和容错

    One Masked Model is All You Need for Sensor Fault Detection, Isolation and Accommodation

    [https://arxiv.org/abs/2403.16153](https://arxiv.org/abs/2403.16153)

    提出了一种使用掩盖模型和自监督学习进行传感器故障检测、隔离和容错的新框架，通过训练过程中创建随机掩码来统一找到并纠正故障传感器，有效性在公共数据集和实际风力涡轮数据集上得到验证。

    

    精确可靠的传感器测量对于确保风力涡轮等复杂工程系统的安全性和长期性至关重要。在本文中，我们提出了一种使用掩盖模型和自监督学习进行传感器故障检测、隔离和容错（FDIA）的新框架。我们提出的方法是一种通用的时间序列建模方法，可以应用于任何能够进行序列建模的神经网络（NN）模型，并捕捉不同传感器之间复杂的时空关系。在训练过程中，提出的掩盖方法创建随机掩码，它就像一个故障，针对一个或多个传感器，使训练和推断任务统一：找到有故障的传感器并进行纠正。我们在一个公共数据集和GE近海风力涡轮的真实数据集上验证了我们提出的技术，并展示了它在检测、诊断和纠正传感器故障方面的有效性。

    arXiv:2403.16153v1 Announce Type: cross  Abstract: Accurate and reliable sensor measurements are critical for ensuring the safety and longevity of complex engineering systems such as wind turbines. In this paper, we propose a novel framework for sensor fault detection, isolation, and accommodation (FDIA) using masked models and self-supervised learning. Our proposed approach is a general time series modeling approach that can be applied to any neural network (NN) model capable of sequence modeling, and captures the complex spatio-temporal relationships among different sensors. During training, the proposed masked approach creates a random mask, which acts like a fault, for one or more sensors, making the training and inference task unified: finding the faulty sensors and correcting them. We validate our proposed technique on both a public dataset and a real-world dataset from GE offshore wind turbines, and demonstrate its effectiveness in detecting, diagnosing and correcting sensor fau
    
[^109]: 消费者物联网流量的调查：安全与隐私

    A Survey on Consumer IoT Traffic: Security and Privacy

    [https://arxiv.org/abs/2403.16149](https://arxiv.org/abs/2403.16149)

    本调查针对消费者物联网（CIoT）流量分析从安全和隐私的角度出发，总结了CIoT流量分析的新特征、最新进展和挑战，认为通过流量分析可以揭示CIoT领域中的安全和隐私问题。

    

    在过去几年里，消费者物联网（CIoT）已经进入了公众生活。尽管CIoT提高了人们日常生活的便利性，但也带来了新的安全和隐私问题。我们尝试通过流量分析这一安全领域中的流行方法，找出研究人员可以从流量分析中了解CIoT安全和隐私方面的内容。本调查从安全和隐私角度探讨了CIoT流量分析中的新特征、CIoT流量分析的最新进展以及尚未解决的挑战。我们从2018年1月至2023年12月收集了310篇与CIoT流量分析有关的安全和隐私角度的论文，总结了识别了CIoT新特征的CIoT流量分析过程。然后，我们根据五个应用目标详细介绍了现有的研究工作：设备指纹识别、用户活动推断、恶意行为检测、隐私泄露以及通信模式识别。

    arXiv:2403.16149v1 Announce Type: cross  Abstract: For the past few years, the Consumer Internet of Things (CIoT) has entered public lives. While CIoT has improved the convenience of people's daily lives, it has also brought new security and privacy concerns. In this survey, we try to figure out what researchers can learn about the security and privacy of CIoT by traffic analysis, a popular method in the security community. From the security and privacy perspective, this survey seeks out the new characteristics in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic analysis, and the challenges yet to be solved. We collected 310 papers from January 2018 to December 2023 related to CIoT traffic analysis from the security and privacy perspective and summarized the process of CIoT traffic analysis in which the new characteristics of CIoT are identified. Then, we detail existing works based on five application goals: device fingerprinting, user activity inference, malicious
    
[^110]: 预测液滴动力学中的能量预算：一种循环神经网络方法

    Predicting Energy Budgets in Droplet Dynamics: A Recurrent Neural Network Approach

    [https://arxiv.org/abs/2403.16144](https://arxiv.org/abs/2403.16144)

    该研究应用循环神经网络方法，特别是LSTM模型，来预测受表面张力影响的流体流动中的能量预算。

    

    arXiv:2403.16144v1 公告类型: 交叉摘要: 流体力学中的神经网络为探索复杂流动（包括多相和自由表面流动）提供了一种高效方法。 循环神经网络，特别是长短期记忆（LSTM）模型，证明了对学习从瞬态输入到动态输出的映射具有吸引力。 本研究应用LSTM来预测受表面张力影响的流体流动的瞬态和静态输出。 具体而言，我们探索了两种不同的液滴动力学场景：具有不同初始形状的液滴与固体表面碰撞，以及两个液滴碰撞后凝聚。 仅使用无量纲数和来自数值模拟的几何时间序列数据，LSTM可以预测能量预算。 采用标记格点法前向跟踪方法结合标记格点法有限差分策略来模拟液滴动力学。 使用馈送循环神经网络（RNN）架构

    arXiv:2403.16144v1 Announce Type: cross  Abstract: Neural networks in fluid mechanics offer an efficient approach for exploring complex flows, including multiphase and free surface flows. The recurrent neural network, particularly the Long Short-Term Memory (LSTM) model, proves attractive for learning mappings from transient inputs to dynamic outputs. This study applies LSTM to predict transient and static outputs for fluid flows under surface tension effects. Specifically, we explore two distinct droplet dynamic scenarios: droplets with diverse initial shapes impacting with solid surfaces, as well as the coalescence of two droplets following collision. Using only dimensionless numbers and geometric time series data from numerical simulations, LSTM predicts the energy budget. The marker-and-cell front-tracking methodology combined with a marker-and-cell finite-difference strategy is adopted for simulating the droplet dynamics. Using a recurrent neural network (RNN) architecture fed wit
    
[^111]: CFAT：释放三角窗口进行图像超分辨率

    CFAT: Unleashing TriangularWindows for Image Super-resolution

    [https://arxiv.org/abs/2403.16143](https://arxiv.org/abs/2403.16143)

    提出了一种 CFAT 模型，引入了非重叠三角窗口技术，与矩形窗口技术结合，用于图像超分辨率，以减少边界失真并增加独特移位模式。

    

    基于Transformer的模型通过利用其捕捉复杂上下文特征的固有能力，彻底改变了图像超分辨率（SR）领域。目前在Transformer架构中使用的重叠矩形偏移窗口技术是超分辨率模型中常见的做法，可以改善图像放大的质量和鲁棒性。然而，它在边界处存在失真，并且具有有限的独特移位模式。为了克服这些弱点，我们提出了一种非重叠三角窗口技术，它与矩形窗口同步工作，以减轻边界级别的失真，并允许模型访问更多独特的移位模式。本文提出了一种将三角-矩形窗口本地注意力与基于通道的全局注意力技术相结合的复合融合注意力Transformer（CFAT），用于图像超分辨率。因此，CFAT实现了注意力机制

    arXiv:2403.16143v1 Announce Type: cross  Abstract: Transformer-based models have revolutionized the field of image super-resolution (SR) by harnessing their inherent ability to capture complex contextual features. The overlapping rectangular shifted window technique used in transformer architecture nowadays is a common practice in super-resolution models to improve the quality and robustness of image upscaling. However, it suffers from distortion at the boundaries and has limited unique shifting modes. To overcome these weaknesses, we propose a non-overlapping triangular window technique that synchronously works with the rectangular one to mitigate boundary-level distortion and allows the model to access more unique sifting modes. In this paper, we propose a Composite Fusion Attention Transformer (CFAT) that incorporates triangular-rectangular window-based local attention with a channel-based global attention technique in image super-resolution. As a result, CFAT enables attention mech
    
[^112]: 自监督预训练图基础模型的调查：基于知识的视角

    A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A Knowledge-Based Perspective

    [https://arxiv.org/abs/2403.16137](https://arxiv.org/abs/2403.16137)

    该论文从基于知识的角度全面调查和分析了图基础模型的自监督预训练任务，涉及微观和宏观知识，包括9个知识类别、25个预训练任务以及各种下游任务适应策略。

    

    图自监督学习现在是预训练图基础模型的首选方法，包括图神经网络、图变换器，以及更近期的基于大型语言模型（LLM）的图模型。文章全面调查和分析了基于知识的视角下的图基础模型的预训练任务，包括微观（节点、链接等）和宏观知识（簇、全局结构等）。涵盖了共计9个知识类别和25个预训练任务，以及各种下游任务适应策略。

    arXiv:2403.16137v1 Announce Type: new  Abstract: Graph self-supervised learning is now a go-to method for pre-training graph foundation models, including graph neural networks, graph transformers, and more recent large language model (LLM)-based graph models. There is a wide variety of knowledge patterns embedded in the structure and properties of graphs which may be used for pre-training, but we lack a systematic overview of self-supervised pre-training tasks from the perspective of graph knowledge. In this paper, we comprehensively survey and analyze the pre-training tasks of graph foundation models from a knowledge-based perspective, consisting of microscopic (nodes, links, etc) and macroscopic knowledge (clusters, global structure, etc). It covers a total of 9 knowledge categories and 25 pre-training tasks, as well as various downstream task adaptation strategies. Furthermore, an extensive list of the related papers with detailed metadata is provided at https://github.com/Newiz430/
    
[^113]: 电子商务中的互补推荐：定义、方法与未来方向

    Complementary Recommendation in E-commerce: Definition, Approaches, and Future Directions

    [https://arxiv.org/abs/2403.16135](https://arxiv.org/abs/2403.16135)

    本文全面总结和比较了电子商务领域中34项代表性互补推荐研究，包括建模产品之间的互补关系，不同研究问题下的模型分类与比较，以及在同一数据集上进行的实验结果分析。

    

    近年来，互补推荐在电子商务领域受到了广泛关注。本文对2009年至2024年间进行的34项代表性研究进行了全面总结和比较。首先，我们比较了用于建模产品之间互补关系的数据和方法，包括简单的互补性以及更复杂的情景，例如非对称互补性、产品之间替代和互补关系共存，以及不同产品对之间的互补程度不同。接下来，我们根据互补推荐的研究问题对模型进行分类并进行比较，如多样性、个性化和冷启动等。此外，我们对不同研究在同一数据集上进行的实验结果进行比较分析，有助于确定研究的优势和劣势。

    arXiv:2403.16135v1 Announce Type: cross  Abstract: In recent years, complementary recommendation has received extensive attention in the e-commerce domain. In this paper, we comprehensively summarize and compare 34 representative studies conducted between 2009 and 2024. Firstly, we compare the data and methods used for modeling complementary relationships between products, including simple complementarity and more complex scenarios such as asymmetric complementarity, the coexistence of substitution and complementarity relationships between products, and varying degrees of complementarity between different pairs of products. Next, we classify and compare the models based on the research problems of complementary recommendation, such as diversity, personalization, and cold-start. Furthermore, we provide a comparative analysis of experimental results from different studies conducted on the same dataset, which helps identify the strengths and weaknesses of the research. Compared to previou
    
[^114]: 基于分隔子图的分层池化SSHPool

    SSHPool: The Separated Subgraph-based Hierarchical Pooling

    [https://arxiv.org/abs/2403.16133](https://arxiv.org/abs/2403.16133)

    提出了基于分隔子图的分层池化（SSHPool）方法，通过将节点分配到不同的簇并利用局部图卷积单元进行压缩，解决了现有图神经网络中的过度平滑问题，有效提取了原始图结构的分层全局特征。

    

    在本文中，我们提出了一种新颖的本地图池化方法，称为基于分隔子图的分层池化（SSHPool），用于图分类。通过将一个样本图的节点分配到不同的簇中，从而产生一系列分隔的子图。我们分别使用本地图卷积单元作为局部结构，进一步将每个子图压缩成一个粗糙节点，将原始图转化为粗糙图。由于这些子图由不同的簇分隔开，结构信息无法在它们之间传播，局部卷积操作可以显著避免大多数现有图神经网络（GNNs）中出现的过度平滑问题。通过在结果粗糙图上层次地执行所提议的程序，SSHPool可以有效地提取原始图结构的分层全局特征。

    arXiv:2403.16133v1 Announce Type: new  Abstract: In this paper, we develop a novel local graph pooling method, namely the Separated Subgraph-based Hierarchical Pooling (SSHPool), for graph classification. To this end, we commence by assigning the nodes of a sample graph into different clusters, resulting in a family of separated subgraphs. We individually employ a local graph convolution units as the local structure to further compress each subgraph into a coarsened node, transforming the original graph into a coarsened graph. Since these subgraphs are separated by different clusters and the structural information cannot be propagated between them, the local convolution operation can significantly avoid the over-smoothing problem arising in most existing Graph Neural Networks (GNNs). By hierarchically performing the proposed procedures on the resulting coarsened graph, the proposed SSHPool can effectively extract the hierarchical global feature of the original graph structure, encapsul
    
[^115]: 神经网络控制系统的运行时监控与故障检测

    Runtime Monitoring and Fault Detection for Neural Network-Controlled Systems

    [https://arxiv.org/abs/2403.16132](https://arxiv.org/abs/2403.16132)

    本文设计了稳定的区间观测器，用于增强受神经网络控制的非线性系统的运行时安全性，并能监测系统的安全性并检测故障。

    

    将深度学习方法应用于控制复杂非线性系统的趋势正在兴起。本文考虑在存在干扰和测量噪声的情况下，提升神经网络控制的非线性系统的运行时安全性。设计了一个稳定区间观测器，用于为神经网络、非线性函数和系统状态生成准确的下界和上界。所获得的区间被用于监控实时系统安全性，并检测系统输出或执行器中的故障。通过对自适应巡航控制车辆系统进行模拟，展示了所提出设计的有效性。

    arXiv:2403.16132v1 Announce Type: cross  Abstract: There is an emerging trend in applying deep learning methods to control complex nonlinear systems. This paper considers enhancing the runtime safety of nonlinear systems controlled by neural networks in the presence of disturbance and measurement noise. A robustly stable interval observer is designed to generate sound and precise lower and upper bounds for the neural network, nonlinear function, and system state. The obtained interval is utilised to monitor the real-time system safety and detect faults in the system outputs or actuators. An adaptive cruise control vehicular system is simulated to demonstrate effectiveness of the proposed design.
    
[^116]: AKBR: 学习自适应基于核的图分类表示

    AKBR: Learning Adaptive Kernel-based Representations for Graph Classification

    [https://arxiv.org/abs/2403.16130](https://arxiv.org/abs/2403.16130)

    提出了一种用于图分类的自适应核表示学习模型，通过特征通道注意机制捕捉不同子结构之间的相互依赖关系，有效识别结构重要性，并计算与重要子结构相关的图对之间的R-卷积核。

    

    在本文中，我们提出了一种新模型，用于学习自适应基于核的图分类表示（AKBR）。与仅通过计算图之间同构子结构对的数量来定义的最先进的 R-卷积图核不同，无法为分类器提供端到端学习机制，所提出的AKBR方法旨在定义一个端到端表示学习模型，为图构建自适应核矩阵。为此，我们首先利用一种新颖的特征通道注意机制来捕捉原始图中不同子结构不变性之间的相互依赖关系。所提出的AKBR模型因此可以有效地确定不同子结构的结构重要性，并计算与由其结构注意力指定的更重要子结构相关的成对图之间的R-卷积核。由于结果核矩阵的每一行...（此处被截断）

    arXiv:2403.16130v1 Announce Type: cross  Abstract: In this paper, we propose a new model to learn Adaptive Kernel-based Representations (AKBR) for graph classification. Unlike state-of-the-art R-convolution graph kernels that are defined by merely counting any pair of isomorphic substructures between graphs and cannot provide an end-to-end learning mechanism for the classifier, the proposed AKBR approach aims to define an end-to-end representation learning model to construct an adaptive kernel matrix for graphs. To this end, we commence by leveraging a novel feature-channel attention mechanism to capture the interdependencies between different substructure invariants of original graphs. The proposed AKBR model can thus effectively identify the structural importance of different substructures, and compute the R-convolution kernel between pairwise graphs associated with the more significant substructures specified by their structural attentions. Since each row of the resulting kernel mat
    
[^117]: 一个用于异构集群中大型模型训练的调度和并行化代码设计

    A Codesign of Scheduling and Parallelization for Large Model Training in Heterogeneous Clusters

    [https://arxiv.org/abs/2403.16125](https://arxiv.org/abs/2403.16125)

    在异构集群中，本文提出了Crius，一个用于有效调度多个大型模型的训练系统，引入了称为"Cell"的新调度粒度，以解决集群调度中低开销和准确性能数据获取之间的矛盾。

    

    考虑调度和自适应并行性结合，为提高异构GPU集群上大型模型训练效率提供了巨大机遇。然而，将自适应并行性整合到集群调度器中扩展了集群调度空间。新空间是原始调度空间和自适应并行性的并行探索空间的乘积（还包括流水线，并行、数据并行和张量并行）。指数级扩大的调度空间和自适应并行性中不断变化的最佳并行计划共同导致了低开销和准确性能数据获取之间的矛盾，以实现高效的集群调度。

    arXiv:2403.16125v1 Announce Type: cross  Abstract: Joint consideration of scheduling and adaptive parallelism offers great opportunities for improving the training efficiency of large models on heterogeneous GPU clusters. However, integrating adaptive parallelism into a cluster scheduler expands the cluster scheduling space. The new space is the product of the original scheduling space and the parallelism exploration space of adaptive parallelism (also a product of pipeline, data, and tensor parallelism). The exponentially enlarged scheduling space and ever-changing optimal parallelism plan from adaptive parallelism together result in the contradiction between low-overhead and accurate performance data acquisition for efficient cluster scheduling. This paper presents Crius, a training system for efficiently scheduling multiple large models with adaptive parallelism in a heterogeneous cluster. Crius proposes a novel scheduling granularity called Cell. It represents a job with determinis
    
[^118]: 自监督多帧神经场景流

    Self-Supervised Multi-Frame Neural Scene Flow

    [https://arxiv.org/abs/2403.16116](https://arxiv.org/abs/2403.16116)

    通过研究表明，Neural Scene Flow Prior (NSFP)的性能与输入点云的数量呈反比关系，因此我们提出了一种简单而有效的多帧点云场景流估计方法。

    

    Neural Scene Flow Prior (NSFP)和Fast Neural Scene Flow (FNSF)在大规模场景中具有出色的自适应性，但是它们的泛化能力的基础原因尚不清楚。我们的研究通过统一稳定性的视角来审视NSFP的泛化能力，揭示其性能与输入点云数量呈反比关系。这一发现揭示了NSFP在处理大规模点云场景流估计任务中的有效性。受这些理论洞察的启发，我们进一步通过利用多帧历史点云来改进场景流估计，从而增加了点云的数量。因此，我们提出了一种简单而有效的多帧点云场景流估计方法。

    arXiv:2403.16116v1 Announce Type: cross  Abstract: Neural Scene Flow Prior (NSFP) and Fast Neural Scene Flow (FNSF) have shown remarkable adaptability in the context of large out-of-distribution autonomous driving. Despite their success, the underlying reasons for their astonishing generalization capabilities remain unclear. Our research addresses this gap by examining the generalization capabilities of NSFP through the lens of uniform stability, revealing that its performance is inversely proportional to the number of input point clouds. This finding sheds light on NSFP's effectiveness in handling large-scale point cloud scene flow estimation tasks. Motivated by such theoretical insights, we further explore the improvement of scene flow estimation by leveraging historical point clouds across multiple frames, which inherently increases the number of point clouds. Consequently, we propose a simple and effective method for multi-frame point cloud scene flow estimation, along with a theor
    
[^119]: 在放射学中应用大型人工智能模型的机遇与挑战

    Opportunities and challenges in the application of large artificial intelligence models in radiology

    [https://arxiv.org/abs/2403.16112](https://arxiv.org/abs/2403.16112)

    本文介绍了大型人工智能模型在放射学中的应用，总结了其在放射学教育、报告生成和影像应用方面的最新研究进展，并指出了大型AI模型在放射学中面临的挑战，旨在推动该领域的快速革命。

    

    受ChatGPT的影响，人工智能（AI）大型模型在全球范围内的研究和开发迎来了高潮。随着人们享受着这种AI大型模型带来的便利，越来越多的细分领域中提出了大型模型，尤其是放射学成像领域中的大型模型。本文首先介绍了大型模型的发展历史、技术细节、工作流程、多模式大型模型工作原理以及视频生成大型模型的工作原理。其次，我们总结了AI大型模型在放射学教育、放射学报告生成、单模式和多模式放射学应用方面的最新研究进展。最后，本文还总结了放射学中大型AI模型面临的一些挑战，旨在更好地推动放射学领域的快速革命。

    arXiv:2403.16112v1 Announce Type: cross  Abstract: Influenced by ChatGPT, artificial intelligence (AI) large models have witnessed a global upsurge in large model research and development. As people enjoy the convenience by this AI large model, more and more large models in subdivided fields are gradually being proposed, especially large models in radiology imaging field. This article first introduces the development history of large models, technical details, workflow, working principles of multimodal large models and working principles of video generation large models. Secondly, we summarize the latest research progress of AI large models in radiology education, radiology report generation, applications of unimodal and multimodal radiology. Finally, this paper also summarizes some of the challenges of large AI models in radiology, with the aim of better promoting the rapid revolution in the field of radiography.
    
[^120]: 一种用于电力价格预测的Transformer方法

    A Transformer approach for Electricity Price Forecasting

    [https://arxiv.org/abs/2403.16108](https://arxiv.org/abs/2403.16108)

    这种独特的Transformer模型在电力价格预测中取得了更好的表现，为可靠和可持续的电力系统运行提供了有前景的解决方案。

    

    本文提出了一种使用纯Transformer模型进行电力价格预测（EPF）的新方法。与其他方法不同，没有使用其他递归网络结合注意力机制。因此，表明注意力层足以捕捉时间模式。该论文还通过使用开源EPF工具进行了对模型的公平比较，并提供了代码以增强EPF研究的可再现性和透明度。结果表明，Transformer模型优于传统方法，为可靠和可持续的电力系统运行提供了一种有希望的解决方案。

    arXiv:2403.16108v1 Announce Type: cross  Abstract: This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model. As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism. Hence, showing that the attention layer is enough for capturing the temporal patterns. The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research. The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation.
    
[^121]: 一份法国假新闻的多标签数据集：人类与机器视角

    A Multi-Label Dataset of French Fake News: Human and Machine Insights

    [https://arxiv.org/abs/2403.16099](https://arxiv.org/abs/2403.16099)

    通过建立一份包括 100 篇文档的多标签数据集 OBSINFOX，研究了人类与机器在认定假新闻特征上的差异，并发现了语料库中讽刺文本的普遍存在。

    

    我们提出了一个由 8 位注释者使用 11 个标签注释的来自 17 个法国被专家机构认为不可靠的新闻来源选取的 100 篇文档的语料库 OBSINFOX。通过收集比通常更多的标签和注释者，我们可以识别人类认为具有代表性的假新闻的特征，并将其与自动分类器的预测进行比较。我们使用 Gate Cloud 进行主题和体裁分析，这表明语料库中类似讽刺的文本普遍存在。然后我们使用 VAGO 主观性分析器及其神经版本，以澄清标签“主观”与标签“假新闻”之间的联系。该带有注释的数据集可通过以下网址在线获取：https://github.com/obs-info/obsinfox

    arXiv:2403.16099v1 Announce Type: new  Abstract: We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of French press considered unreliable by expert agencies, annotated using 11 labels by 8 annotators. By collecting more labels than usual, by more annotators than is typically done, we can identify features that humans consider as characteristic of fake news, and compare them to the predictions of automated classifiers. We present a topic and genre analysis using Gate Cloud, indicative of the prevalence of satire-like text in the corpus. We then use the subjectivity analyzer VAGO, and a neural version of it, to clarify the link between ascriptions of the label Subjective and ascriptions of the label Fake News. The annotated dataset is available online at the following url: https://github.com/obs-info/obsinfox   Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion, Exaggeration, French Press
    
[^122]: LLM作为阿拉伯编程语言的编译器

    LLMs as Compiler for Arabic Programming Language

    [https://arxiv.org/abs/2403.16087](https://arxiv.org/abs/2403.16087)

    本文介绍了APL（阿拉伯编程语言），它使用LLM作为半编译器，将阿拉伯文本代码转换为Python代码并运行，构建了完整的流水线。

    

    在这篇论文中，我们介绍了APL (Arabic Programming Language)，它使用大型语言模型(LLM)作为半编译器，将阿拉伯文本代码转换为Python代码，然后运行该代码。设计了从APL文本结构到提示（使用提示工程）再到使用PyRunner运行生成的Python代码的完整流水线。该项目包括三部分：Python库，具有简单界面的游乐场和这篇研究论文。

    arXiv:2403.16087v1 Announce Type: cross  Abstract: In this paper we introduce APL (Arabic Programming Language) that uses Large language models (LLM) as semi-compiler to covert Arabic text code to python code then run the code. Designing a full pipeline from the structure of the APL text then a prompt (using prompt engineering) then running the prodcued python code using PyRunner. This project has a three parts first python library, a playground with simple interface and this research paper.
    
[^123]: IBCB: 高效的逆批处理上下文强化学习用于行为演变历史

    IBCB: Efficient Inverse Batched Contextual Bandit for Behavioral Evolution History

    [https://arxiv.org/abs/2403.16075](https://arxiv.org/abs/2403.16075)

    提出了一种逆批处理上下文强化学习（IBCB）框架，可以高效地根据专家的行为演变历史对环境奖励参数和学习策略进行估计。

    

    传统的模仿学习关注专家的行为机制建模，需要大量由某个固定专家生成的交互历史。然而，在许多流式应用中，如流式推荐系统，在线决策者通常在决策过程中进行在线学习，这意味着在线决策者生成的交互历史包括他们从新手专家到有经验专家的行为演变。这给现有的只能利用有经验专家数据的模仿学习方法带来了新挑战。为了解决这个问题，本文提出了一种逆批处理上下文强化学习（IBCB）框架，能够高效地进行基于专家行为演变历史的环境奖励参数和学习策略的估计。具体来说，IBCB将逆问题形式化为简单的二次规划。

    arXiv:2403.16075v1 Announce Type: new  Abstract: Traditional imitation learning focuses on modeling the behavioral mechanisms of experts, which requires a large amount of interaction history generated by some fixed expert. However, in many streaming applications, such as streaming recommender systems, online decision-makers typically engage in online learning during the decision-making process, meaning that the interaction history generated by online decision-makers includes their behavioral evolution from novice expert to experienced expert. This poses a new challenge for existing imitation learning approaches that can only utilize data from experienced experts. To address this issue, this paper proposes an inverse batched contextual bandit (IBCB) framework that can efficiently perform estimations of environment reward parameters and learned policy based on the expert's behavioral evolution history. Specifically, IBCB formulates the inverse problem into a simple quadratic programming 
    
[^124]: 基于改进的扩散映射的流形正则化分类模型

    Manifold Regularization Classification Model Based On Improved Diffusion Map

    [https://arxiv.org/abs/2403.16059](https://arxiv.org/abs/2403.16059)

    本文提出了基于改进扩散映射的流形正则化分类模型，通过改进标签传播模型，有效克服了原始流形正则化模型在局部区域性能上的限制。

    

    流形正则化模型是一种半监督学习模型，利用数据集的几何结构，包括少量有标签样本和大量无标签样本，生成分类器。然而，原始的流形范数限制了模型性能只局限于局部区域。为了克服这一局限，本文提出了一种改进流形正则化的方法，基于标签传播模型。我们首先增强扩散映射算法的概率转移矩阵，可用于估计Neumann热核，使其能够准确描述流形上的标签传播过程。利用该矩阵，在数据集上建立一个描述不同时间步骤下标签分布的标签传播函数。随后，我们将标签传播函数扩展到整个数据流形。我们证明了扩展的标签传播函数c

    arXiv:2403.16059v1 Announce Type: cross  Abstract: Manifold regularization model is a semi-supervised learning model that leverages the geometric structure of a dataset, comprising a small number of labeled samples and a large number of unlabeled samples, to generate classifiers. However, the original manifold norm limits the performance of models to local regions. To address this limitation, this paper proposes an approach to improve manifold regularization based on a label propagation model. We initially enhance the probability transition matrix of the diffusion map algorithm, which can be used to estimate the Neumann heat kernel, enabling it to accurately depict the label propagation process on the manifold. Using this matrix, we establish a label propagation function on the dataset to describe the distribution of labels at different time steps. Subsequently, we extend the label propagation function to the entire data manifold. We prove that the extended label propagation function c
    
[^125]: 通过基于卡托图辅助的深度学习增强开放系统的需求预测

    Enhancing Demand Prediction in Open Systems by Cartogram-aided Deep Learning

    [https://arxiv.org/abs/2403.16049](https://arxiv.org/abs/2403.16049)

    提出了一种利用卡托图方法的深度学习框架，可预测未安装数据的站点需求并实现长期预测。

    

    在各个领域预测时间模式面临着重大挑战，因为它们的轨迹细微且经常是非线性的。为了解决这一挑战，预测框架不断被完善，采用数据驱动的统计方法、数学模型和机器学习方法。最近，作为一个具有挑战性的系统之一，共享交通系统如共享单车由于城市约束和环境问题而备受关注。由于系统的开放性和各站点之间的使用模式不平衡，预测单车站点的租借和归还模式仍然是一项艰巨的任务。在本研究中，我们提出了一个深度学习框架，通过利用卡托图方法来预测租借和归还模式。卡托图方法有助于对新安装站点的需求进行预测，这些新站点没有训练数据，同时实现了长期预测，这是以前尚未实现的。

    arXiv:2403.16049v1 Announce Type: new  Abstract: Predicting temporal patterns across various domains poses significant challenges due to their nuanced and often nonlinear trajectories. To address this challenge, prediction frameworks have been continuously refined, employing data-driven statistical methods, mathematical models, and machine learning. Recently, as one of the challenging systems, shared transport systems such as public bicycles have gained prominence due to urban constraints and environmental concerns. Predicting rental and return patterns at bicycle stations remains a formidable task due to the system's openness and imbalanced usage patterns across stations. In this study, we propose a deep learning framework to predict rental and return patterns by leveraging cartogram approaches. The cartogram approach facilitates the prediction of demand for newly installed stations with no training data as well as long-period prediction, which has not been achieved before. We apply t
    
[^126]: 通过语义-结构注意增强图卷积网络进行节点分类

    Node Classification via Semantic-Structural Attention-Enhanced Graph Convolutional Networks

    [https://arxiv.org/abs/2403.16033](https://arxiv.org/abs/2403.16033)

    该论文提出了一种名为语义-结构注意增强图卷积网络（SSA-GCN），能够同时模拟图结构并从知识图谱和复杂网络的角度提取无监督特征，以提升节点分类性能。

    

    图数据，也称为复杂网络数据，在各个领域和应用中无处不在。之前的图神经网络模型主要专注于通过监督学习目标提取特定任务的结构特征，但在捕捉整个图的固有语义和结构特征方面表现不佳。本文介绍了一种语义-结构注意增强图卷积网络（SSA-GCN），不仅模拟了图结构，还从总体上提取了无监督特征以增强顶点分类性能。SSA-GCN的关键贡献在三个方面表现：首先，它通过从知识图谱的角度进行无监督特征提取来获得语义信息；其次，它通过从复杂网络的角度进行无监督特征提取来获得结构信息；最后，它通过交叉注意力机制将这些特征融合在一起。

    arXiv:2403.16033v1 Announce Type: cross  Abstract: Graph data, also known as complex network data, is omnipresent across various domains and applications. Prior graph neural network models primarily focused on extracting task-specific structural features through supervised learning objectives, but they fell short in capturing the inherent semantic and structural features of the entire graph. In this paper, we introduce the semantic-structural attention-enhanced graph convolutional network (SSA-GCN), which not only models the graph structure but also extracts generalized unsupervised features to enhance vertex classification performance. The SSA-GCN's key contributions lie in three aspects: firstly, it derives semantic information through unsupervised feature extraction from a knowledge graph perspective; secondly, it obtains structural information through unsupervised feature extraction from a complex network perspective; and finally, it integrates these features through a cross-attent
    
[^127]: 从偏序关系中学习有向无环图

    Learning Directed Acyclic Graphs from Partial Orderings

    [https://arxiv.org/abs/2403.16031](https://arxiv.org/abs/2403.16031)

    本文针对当可用部分因果顺序变量时学习DAGs的中间问题，提出了一个通用估计框架，并展示了有效的估计算法。

    

    有向无环图（DAGs）通常用于模拟随机变量之间的因果关系。通常来说，学习DAG结构在计算和统计方面都具有挑战性。此外，在没有额外信息的情况下，边的方向可能无法从观测数据中估计出来。本文考虑了在可用部分因果顺序变量的情况下学习DAGs的中间问题。我们提出了一个利用部分顺序的通用估计框架，并提出了低维和高维问题的有效估计算法。所提出框架的优势通过数值研究进行了说明。

    arXiv:2403.16031v1 Announce Type: cross  Abstract: Directed acyclic graphs (DAGs) are commonly used to model causal relationships among random variables. In general, learning the DAG structure is both computationally and statistically challenging. Moreover, without additional information, the direction of edges may not be estimable from observational data. In contrast, given a complete causal ordering of the variables, the problem can be solved efficiently, even in high dimensions. In this paper, we consider the intermediate problem of learning DAGs when a partial causal ordering of variables is available. We propose a general estimation framework for leveraging the partial ordering and present efficient estimation algorithms for low- and high-dimensional problems. The advantages of the proposed framework are illustrated via numerical studies.
    
[^128]: VCR-Graphormer：通过虚拟连接实现的小批量图变换器

    VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections

    [https://arxiv.org/abs/2403.16030](https://arxiv.org/abs/2403.16030)

    通过为每个节点分配由个性化PageRank（PPR）采样的令牌列表，然后仅在此列表上应用标准多头自注意力来计算其节点表示，来解决图变换器小批量训练中的样本限制问题。

    

    图变换器通过采用能够捕获复杂拓扑和特征信息的表达性表示的注意力机制，被证明是一种有效的图学习方法。图变换器传统上对每对节点执行密集注意力（或全局注意力）来学习节点表示向量，导致二次计算成本对于大规模图数据是无法承受的。因此，图变换器的小批量训练是一个有前途的方向，但每个小批量中的有限样本无法支持有效的密集注意力以编码信息丰富的表示。

    arXiv:2403.16030v1 Announce Type: new  Abstract: Graph transformer has been proven as an effective graph learning method for its adoption of attention mechanism that is capable of capturing expressive representations from complex topological and feature information of graphs. Graph transformer conventionally performs dense attention (or global attention) for every pair of nodes to learn node representation vectors, resulting in quadratic computational costs that are unaffordable for large-scale graph data. Therefore, mini-batch training for graph transformers is a promising direction, but limited samples in each mini-batch can not support effective dense attention to encode informative representations. Facing this bottleneck, (1) we start by assigning each node a token list that is sampled by personalized PageRank (PPR) and then apply standard multi-head self-attention only on this list to compute its node representations. This PPR tokenization method decouples model training from comp
    
[^129]: 探究数据集偏差对数据集精简的影响

    Exploring the Impact of Dataset Bias on Dataset Distillation

    [https://arxiv.org/abs/2403.16028](https://arxiv.org/abs/2403.16028)

    针对数据集精简中的数据集偏差问题，我们进行了首次系统调查与探索，构建并评估了两个偏见数据集，为未来研究提供了基础。

    

    数据集精简（DD）是一种有前途的技术，用于合成一个保留原始数据集中基本信息的较小数据集。这个合成数据集可以作为原始大规模数据集的替代品，并有助于减轻训练负担。然而，当前的DD方法通常在假设数据集不具有偏见的情况下运作，忽视了数据集本身可能存在的潜在偏见问题。为了填补这一空白，我们系统地调查了数据集偏差对DD的影响。据我们所知，这是DD领域的首次探索。鉴于目前没有适用于DD的偏见数据集，我们首先构建了两个偏见数据集，CMNIST-DD和CCIFAR10-DD，为随后的分析奠定基础。然后我们利用现有的DD方法在CMNIST-DD和CCIFAR10-DD上生成合成数据集，并按照标准流程评估它们的性能。

    arXiv:2403.16028v1 Announce Type: cross  Abstract: Dataset Distillation (DD) is a promising technique to synthesize a smaller dataset that preserves essential information from the original dataset. This synthetic dataset can serve as a substitute for the original large-scale one, and help alleviate the training workload. However, current DD methods typically operate under the assumption that the dataset is unbiased, overlooking potential bias issues within the dataset itself. To fill in this blank, we systematically investigate the influence of dataset bias on DD. To the best of our knowledge, this is the first exploration in the DD domain. Given that there are no suitable biased datasets for DD, we first construct two biased datasets, CMNIST-DD and CCIFAR10-DD, to establish a foundation for subsequent analysis. Then we utilize existing DD methods to generate synthetic datasets on CMNIST-DD and CCIFAR10-DD, and evaluate their performance following the standard process. Experiments demo
    
[^130]: 用于加速STABLE-DIFFUSION的统一模块：LCM-LORA

    A Unified Module for Accelerating STABLE-DIFFUSION: LCM-LORA

    [https://arxiv.org/abs/2403.16024](https://arxiv.org/abs/2403.16024)

    论文研究了用于加速稳定扩散过程的统一模块，重点关注lcm-lora模块，提供了无条件稳定扩散合成加速方法的理论基础和数值结果。

    

    这篇论文对用于加速稳定扩散过程的统一模块进行了全面研究，特别关注lcm-lora模块。稳定扩散过程在各种科学和工程领域中起着至关重要的作用，加速这些过程对于提高计算性能至关重要。为了解决固定源离散矩问题的标准迭代过程通常在光学厚情况下表现出较慢的收敛性的挑战，已经开发了无条件稳定扩散加速方法，旨在提高传输方程和离散矩问题的计算效率。这项研究探讨了无条件稳定扩散合成加速方法的理论基础和数值结果，为模型离散矩问题的稳定性和性能提供了见解。

    arXiv:2403.16024v1 Announce Type: new  Abstract: This paper presents a comprehensive study on the unified module for accelerating stable-diffusion processes, specifically focusing on the lcm-lora module. Stable-diffusion processes play a crucial role in various scientific and engineering domains, and their acceleration is of paramount importance for efficient computational performance. The standard iterative procedures for solving fixed-source discrete ordinates problems often exhibit slow convergence, particularly in optically thick scenarios. To address this challenge, unconditionally stable diffusion-acceleration methods have been developed, aiming to enhance the computational efficiency of transport equations and discrete ordinates problems. This study delves into the theoretical foundations and numerical results of unconditionally stable diffusion synthetic acceleration methods, providing insights into their stability and performance for model discrete ordinates problems. Furtherm
    
[^131]: 一种适用于具有不同图网络结构的节点分类任务的联邦参数聚合方法

    A Federated Parameter Aggregation Method for Node Classification Tasks with Different Graph Network Structures

    [https://arxiv.org/abs/2403.16004](https://arxiv.org/abs/2403.16004)

    提出了一种适用于具有不同图网络结构的节点分类任务的联邦参数聚合方法FLGNN，并验证了其有效性。同时，设计了隐私安全的成员推理攻击实验和差分隐私防御实验。

    

    在过去几年里，由于其协作训练多个来源数据而不会泄露隐私的能力，联邦学习已经广泛应用于各种经典机器学习领域。然而，在图神经网络领域，由客户端持有的图的节点和网络结构在许多实际应用中是不同的，直接共享模型梯度的聚合方法不能直接应用于这种情况。因此，本文提出了一种适用于各种图联邦场景的联邦聚合方法FLGNN，并探讨了图神经网络模型每一层参数共享的聚合效果。通过在真实数据集上的实验验证了联邦聚合方法FLGNN的有效性。另外，为了保护FLGNN的隐私安全，本文设计了成员推理攻击实验和差分隐私防御实验。

    arXiv:2403.16004v1 Announce Type: cross  Abstract: Over the past few years, federated learning has become widely used in various classical machine learning fields because of its collaborative ability to train data from multiple sources without compromising privacy. However, in the area of graph neural networks, the nodes and network structures of graphs held by clients are different in many practical applications, and the aggregation method that directly shares model gradients cannot be directly applied to this scenario. Therefore, this work proposes a federated aggregation method FLGNN applied to various graph federation scenarios and investigates the aggregation effect of parameter sharing at each layer of the graph neural network model. The effectiveness of the federated aggregation method FLGNN is verified by experiments on real datasets. Additionally, for the privacy security of FLGNN, this paper designs membership inference attack experiments and differential privacy defense expe
    
[^132]: 具有保证私有初始化的近最优差分隐私低秩痕迹回归

    Near-Optimal differentially private low-rank trace regression with guaranteed private initialization

    [https://arxiv.org/abs/2403.15999](https://arxiv.org/abs/2403.15999)

    该论文研究了在痕迹回归模型下以高斯测量矩阵进行差分隐私估计，提出了具有保证私有初始化的近最优算法，引入了一个高效的DP初始化算法和基于Riemannian优化的差分隐私算法，同时讨论了估计结果的非平凡差距。

    

    我们研究了在具有高斯测量矩阵的痕迹回归模型下，对秩为$r$的矩阵$M \in \RR^{d_1\times d_2}$进行差分隐私（DP）估计。在理论上，精确表征了非私有谱初始化的敏感性，并建立了在Schatten-$q$范数下估计$M$的差分隐私约束极小概率下界。在方法论上，本文引入了一个计算效率高的DP初始化算法，其样本大小为$n \geq \wt O (r^2 (d_1\vee d_2))$。在一定的正则条件下，DP初始化落入围绕$M$的局部球内。我们还提出了一种基于黎曼优化的用于估计$M$的差分隐私算法（DP-RGrad），通过DP初始化和样本大小$n \geq \wt O(r (d_1 + d_2))$实现了近最优收敛速度。最后，本文讨论了在差分隐私初始化和样本大小条件下的所估计的$M$之间的非平凡差距。

    arXiv:2403.15999v1 Announce Type: cross  Abstract: We study differentially private (DP) estimation of a rank-$r$ matrix $M \in \RR^{d_1\times d_2}$ under the trace regression model with Gaussian measurement matrices. Theoretically, the sensitivity of non-private spectral initialization is precisely characterized, and the differential-privacy-constrained minimax lower bound for estimating $M$ under the Schatten-$q$ norm is established. Methodologically, the paper introduces a computationally efficient algorithm for DP-initialization with a sample size of $n \geq \wt O (r^2 (d_1\vee d_2))$. Under certain regularity conditions, the DP-initialization falls within a local ball surrounding $M$. We also propose a differentially private algorithm for estimating $M$ based on Riemannian optimization (DP-RGrad), which achieves a near-optimal convergence rate with the DP-initialization and sample size of $n \geq \wt O(r (d_1 + d_2))$. Finally, the paper discusses the non-trivial gap between the mi
    
[^133]: 知识引导的机器学习：当前趋势与未来展望

    Knowledge-guided Machine Learning: Current Trends and Future Prospects

    [https://arxiv.org/abs/2403.15989](https://arxiv.org/abs/2403.15989)

    知识引导的机器学习（KGML）结合科学知识和数据在机器学习框架中，以实现更好的泛化能力、科学一致性和结果可解释性。

    

    本文概述了科学建模，并讨论了与基于过程的模型相比，机器学习方法在科学建模中的互补优势和劣势。文章还介绍了新兴领域科学知识引导的机器学习（KGML）研究的当前状态，旨在利用科学知识和数据在机器学习框架中实现更好的泛化能力、科学一致性和结果可解释性。我们从使用的科学知识类型、探讨的知识-ML集成形式以及在机器学习中整合科学知识的方法等方面讨论了KGML研究的不同方面。我们还讨论了在环境科学中发展的KGML方法的一些常见用例类别，以每个类别中的实例为例。

    arXiv:2403.15989v1 Announce Type: cross  Abstract: This paper presents an overview of scientific modeling and discusses the complementary strengths and weaknesses of ML methods for scientific modeling in comparison to process-based models. It also provides an introduction to the current state of research in the emerging field of scientific knowledge-guided machine learning (KGML) that aims to use both scientific knowledge and data in ML frameworks to achieve better generalizability, scientific consistency, and explainability of results. We discuss different facets of KGML research in terms of the type of scientific knowledge used, the form of knowledge-ML integration explored, and the method for incorporating scientific knowledge in ML. We also discuss some of the common categories of use cases in environmental sciences where KGML methods are being developed, using illustrative examples in each category.
    
[^134]: CBGT-Net: 一种用于对流数据进行稳健分类的类脑模仿架构

    CBGT-Net: A Neuromimetic Architecture for Robust Classification of Streaming Data

    [https://arxiv.org/abs/2403.15974](https://arxiv.org/abs/2403.15974)

    CBGT-Net是一种受CBGT回路启发的神经网络模型，通过积累足够的证据后才对流数据产生分类决策，在图像分类任务中表现出更高的准确性和稳健性。

    

    这篇论文描述了CBGT-Net，这是一种受哺乳动物大脑中皮质-基底节-丘脑（CBGT）回路启发的神经网络模型。与传统的神经网络模型不同，CBGT-Net学习在观察到的数据流中达到足够证据标准后产生输出。对于每次观察，CBGT-Net生成一个矢量，明确表示观察为每个潜在决定提供的证据量，随时间累积证据，并在累积证据超过预定义阈值时生成决策。我们在两个图像分类任务上评估了所提出的模型，在这些任务中模型需要根据从图像中提取的小补丁流来预测图像类别。我们展示了CBGT-Net相比传统方法在准确性和稳健性方面提供了改进。

    arXiv:2403.15974v1 Announce Type: cross  Abstract: This paper describes CBGT-Net, a neural network model inspired by the cortico-basal ganglia-thalamic (CBGT) circuits found in mammalian brains. Unlike traditional neural network models, which either generate an output for each provided input, or an output after a fixed sequence of inputs, the CBGT-Net learns to produce an output after a sufficient criteria for evidence is achieved from a stream of observed data. For each observation, the CBGT-Net generates a vector that explicitly represents the amount of evidence the observation provides for each potential decision, accumulates the evidence over time, and generates a decision when the accumulated evidence exceeds a pre-defined threshold. We evaluate the proposed model on two image classification tasks, where models need to predict image categories based on a stream of small patches extracted from the image. We show that the CBGT-Net provides improved accuracy and robustness compared t
    
[^135]: 使用机器学习方法检测问题赌博，使用更少的特征

    Detection of Problem Gambling with Less Features Using Machine Learning Methods

    [https://arxiv.org/abs/2403.15962](https://arxiv.org/abs/2403.15962)

    提出了一种使用更少的特征进行问题赌博检测的深度神经网络模型，并发现在不影响性能的情况下可以将特征从102个减少到5个。

    

    在赌博研究中，基于对用户每日行为数据的监控而执行分析特征。在执行问题赌博检测时，现有数据集为建立基于机器学习的模型提供了相对丰富的分析特征。然而，考虑到在实际应用中收集分析特征的复杂性和成本，使用更少的特征进行精确检测将极大减少数据收集成本。在这项研究中，我们提出了一个深度神经网络 PGN4，在使用有限的分析特征时表现良好。通过对两个数据集进行实验，我们发现当将102个特征减少到5个特征时，PGN4仅遭遇了轻微的性能下降。此外，我们发现了两个数据集中排名前5的特征的共同点。

    arXiv:2403.15962v1 Announce Type: cross  Abstract: Analytic features in gambling study are performed based on the amount of data monitoring on user daily actions. While performing the detection of problem gambling, existing datasets provide relatively rich analytic features for building machine learning based model. However, considering the complexity and cost of collecting the analytic features in real applications, conducting precise detection with less features will tremendously reduce the cost of data collection. In this study, we propose a deep neural networks PGN4 that performs well when using limited analytic features. Through the experiment on two datasets, we discover that PGN4 only experiences a mere performance drop when cutting 102 features to 5 features. Besides, we find the commonality within the top 5 features from two datasets.
    
[^136]: 了解损失压缩在机器学习训练集中的有效性

    Understanding The Effectiveness of Lossy Compression in Machine Learning Training Sets

    [https://arxiv.org/abs/2403.15953](https://arxiv.org/abs/2403.15953)

    深入探究损失压缩对模型质量的影响，发现现代损失压缩方法可以在保证质量损失在1%以下的情况下实现50-100倍的压缩比提升。

    

    学习和人工智能（ML/AI）技术在高性能计算（HPC）中变得日益普及。然而，这些方法依赖于大量的浮点数据用于训练和验证，需要方法在广域网络（WAN）上共享数据或将其从边缘设备传输到数据中心。数据压缩可以解决这些问题，但需要深入了解损失压缩如何影响模型质量。我们设计了评估ML/AI数据减少技术的系统方法，并使用它对17种数据减少方法在7个ML/AI应用程序上进行了非常全面的评估，表明现代的损失压缩方法可以实现50-100倍的压缩比改善，质量损失在1%以下。我们提出了关键见解，指导未来的使用和de

    arXiv:2403.15953v1 Announce Type: cross  Abstract: Learning and Artificial Intelligence (ML/AI) techniques have become increasingly prevalent in high performance computing (HPC). However, these methods depend on vast volumes of floating point data for training and validation which need methods to share the data on a wide area network (WAN) or to transfer it from edge devices to data centers. Data compression can be a solution to these problems, but an in-depth understanding of how lossy compression affects model quality is needed. Prior work largely considers a single application or compression method. We designed a systematic methodology for evaluating data reduction techniques for ML/AI, and we use it to perform a very comprehensive evaluation with 17 data reduction methods on 7 ML/AI applications to show modern lossy compression methods can achieve a 50-100x compression ratio improvement for a 1% or less loss in quality. We identify critical insights that guide the future use and de
    
[^137]: 探索直到自信: 面向具身问答的高效探索

    Explore until Confident: Efficient Exploration for Embodied Question Answering

    [https://arxiv.org/abs/2403.15941](https://arxiv.org/abs/2403.15941)

    通过利用大型视觉-语言模型的语义推理能力，结合深度信息和视觉提示，提出了一种方法来解决具身问答中的有效探索和回答问题的挑战

    

    我们考虑了具身问答（EQA）的问题，这指的是在需要主动探索环境以收集信息直到对问题的答案有自信的具身代理，例如机器人。在这项工作中，我们利用大规模视觉-语言模型（VLMs）的强大语义推理能力来高效探索和回答这些问题。然而，在EQA中使用VLMs时存在两个主要挑战：它们没有内部记忆将场景映射以便规划如何随时间探索，并且它们的置信度可能被错误校准并可能导致机器人过早停止探索或过度探索。我们提出了一种方法，首先基于深度信息和通过视觉提示VLM来构建场景的语义地图-利用其对场景相关区域的广泛知识来进行探索。接下来，我们使用符合预测来校准VLM的置信度。

    arXiv:2403.15941v1 Announce Type: cross  Abstract: We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's 
    
[^138]: LlamBERT：在自然语言处理中大规模、低成本的数据注释

    LlamBERT: Large-scale low-cost data annotation in NLP

    [https://arxiv.org/abs/2403.15938](https://arxiv.org/abs/2403.15938)

    LlamBERT是一种利用大规模语言模型注释未标记数据库并用于微调变压器编码器的混合方法，在降低成本的同时略微牺牲准确性。

    

    大型语言模型(LLMs)，如GPT-4和Llama 2，在各种自然语言处理(NLP)任务中表现出卓越的能力。尽管它们非常有效，但与它们的使用相关的高成本带来了挑战。我们提出了LlamBERT，这是一种混合方法，利用LLMs对大量未标记数据库的小子集进行注释，并将结果用于微调类似BERT和RoBERTa的变压器编码器。这一策略在两个不同的数据集上进行了评估：IMDb影评数据集和UMLS Meta-Thesaurus。我们的结果表明，LlamBERT方法在稍微牺牲准确性的同时，提供了更高的成本效益。

    arXiv:2403.15938v1 Announce Type: cross  Abstract: Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable proficiency in a wide range of natural language processing (NLP) tasks. Despite their effectiveness, the high costs associated with their use pose a challenge. We present LlamBERT, a hybrid approach that leverages LLMs to annotate a small subset of large, unlabeled databases and uses the results for fine-tuning transformer encoders like BERT and RoBERTa. This strategy is evaluated on two diverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus. Our results indicate that the LlamBERT approach slightly compromises on accuracy while offering much greater cost-effectiveness.
    
[^139]: 一种新方法：通过本地TD更新实现样本和通讯高效的完全分散式MARL策略评估

    Sample and Communication Efficient Fully Decentralized MARL Policy Evaluation via a New Approach: Local TD update

    [https://arxiv.org/abs/2403.15935](https://arxiv.org/abs/2403.15935)

    通过引入本地TD更新方法，该研究提出了一种新的方法来降低完全分散式MARL策略评估中的通讯和样本复杂性。

    

    在完全分散式多智能体强化学习（MARL）的Actor-Critic框架中，MARL策略评估（PE）问题是其中的一个关键组成部分，其中一组$N$个智能体通过与邻居通信合作评估给定策略的全局状态值函数。对于MARL-PE，一个关键挑战是如何降低采样和通讯复杂性，这些复杂性定义为收敛到一些$\epsilon$-稳定点所需的训练样本数和通讯轮次。为了降低MARL-PE中的通讯复杂性，一个“自然”的想法是在每次连续通讯的轮之间执行多个本地TD更新步骤，以减少通讯频率。然而，由于普遍存在的不同代理之间奖励异质性可能导致的“代理漂移”现象，本地TD更新方法的有效性仍不清楚。这引发了一个有趣的研究问题。

    arXiv:2403.15935v1 Announce Type: new  Abstract: In actor-critic framework for fully decentralized multi-agent reinforcement learning (MARL), one of the key components is the MARL policy evaluation (PE) problem, where a set of $N$ agents work cooperatively to evaluate the value function of the global states for a given policy through communicating with their neighbors. In MARL-PE, a critical challenge is how to lower the sample and communication complexities, which are defined as the number of training samples and communication rounds needed to converge to some $\epsilon$-stationary point. To lower communication complexity in MARL-PE, a "natural'' idea is to perform multiple local TD-update steps between each consecutive rounds of communication to reduce the communication frequency. However, the validity of the local TD-update approach remains unclear due to the potential "agent-drift'' phenomenon resulting from heterogeneous rewards across agents in general. This leads to an interesti
    
[^140]: 理解马尔科夫逻辑网络中的域大小泛化

    Understanding Domain-Size Generalization in Markov Logic Networks

    [https://arxiv.org/abs/2403.15933](https://arxiv.org/abs/2403.15933)

    本文量化了马尔科夫逻辑网络在不同大小领域间内部一致性缺失的问题，并提出最大化数据对数似然同时最小化参数方差的方式来优化领域大小泛化。

    

    我们研究了马尔科夫逻辑网络（MLNs）在不同大小的关系结构之间的泛化行为。多个研究注意到，在给定域上学习的MLNs在不同大小的域上泛化很差。这种行为源于MLN在不同域大小上使用时的内部一致性缺失。在本文中，我们量化了这种不一致性，并将其限制在MLN参数的方差范围内。参数方差还限制了从不同域大小中取出的MLN边缘分布之间的KL散度。我们利用这些界限展示，最大化数据对数似然同时最小化参数方差，对应于域大小泛化的两个自然概念。我们的理论结果适用于指数随机图和其他基于马尔科夫网络的关系模型。最后，我们观察到已知的解决方案会减少方差

    arXiv:2403.15933v1 Announce Type: new  Abstract: We study the generalization behavior of Markov Logic Networks (MLNs) across relational structures of different sizes. Multiple works have noticed that MLNs learned on a given domain generalize poorly across domains of different sizes. This behavior emerges from a lack of internal consistency within an MLN when used across different domain sizes. In this paper, we quantify this inconsistency and bound it in terms of the variance of the MLN parameters. The parameter variance also bounds the KL divergence between an MLN's marginal distributions taken from different domain sizes. We use these bounds to show that maximizing the data log-likelihood while simultaneously minimizing the parameter variance corresponds to two natural notions of generalization across domain sizes. Our theoretical results apply to Exponential Random Graphs and other Markov network based relational models. Finally, we observe that solutions known to decrease the varia
    
[^141]: 带有随机停止时间的约束马尔可夫决策过程的安全强化学习

    Safe Reinforcement Learning for Constrained Markov Decision Processes with Stochastic Stopping Time

    [https://arxiv.org/abs/2403.15928](https://arxiv.org/abs/2403.15928)

    提出了一种在线强化学习算法，用于带有安全性约束的马尔可夫决策过程，能够学习到安全的最优策略，同时提出了计算安全基线策略的方法，证明了算法的有效性，并展示了通过定义代理集实现有效探索。

    

    在本文中，我们提出了一种用于带有安全性约束的马尔可夫决策过程的在线强化学习算法。尽管科学界已经引起必要的关注，但考虑到随机停止时间，学习在学习阶段不违反安全性约束的最优策略的问题尚未得到解决。为此，我们提出了一种基于线性规划的算法，不需要一个过程模型。我们展示了学到的策略具有较高的安全性保证。我们还提出了一种计算安全基线策略的方法，这对于开发不违反安全性约束的算法至关重要。最后，我们提供了模拟结果来展示所提出算法的有效性。此外，我们证明了通过定义一个称为代理集的状态空间子集可以实现有效的探索。

    arXiv:2403.15928v1 Announce Type: new  Abstract: In this paper, we present an online reinforcement learning algorithm for constrained Markov decision processes with a safety constraint. Despite the necessary attention of the scientific community, considering stochastic stopping time, the problem of learning optimal policy without violating safety constraints during the learning phase is yet to be addressed. To this end, we propose an algorithm based on linear programming that does not require a process model. We show that the learned policy is safe with high confidence. We also propose a method to compute a safe baseline policy, which is central in developing algorithms that do not violate the safety constraints. Finally, we provide simulation results to show the efficacy of the proposed algorithm. Further, we demonstrate that efficient exploration can be achieved by defining a subset of the state-space called proxy set.
    
[^142]: 使用轨迹抽样的深度高斯协方差网络进行数据高效策略搜索

    Deep Gaussian Covariance Network with Trajectory Sampling for Data-Efficient Policy Search

    [https://arxiv.org/abs/2403.15908](https://arxiv.org/abs/2403.15908)

    结合轨迹抽样和深度高斯协方差网络，以提高基于模型的强化学习问题的数据高效性。

    

    概率世界模型通过利用其认识不确定性指导策略，提高了基于模型的强化学习（MBRL）的数据效率，改善了探索性能并获得了新样本。此外，概率方法中的不确定性感知学习流程导致的稳健策略比不考虑不确定性的解决方案对噪声观测更不敏感。我们提出将轨迹抽样和深度高斯协方差网络（DGCN）相结合，以在最优控制环境中实现MBRL问题的数据高效解决方案。我们使用高斯过程、贝叶斯神经网络和DGCN三种不同的概率世界模型，比较了轨迹抽样和基于密度的近似法在不确定性传播方面的效果。我们通过四个不同的知名测试环境提供了经验证据，证明我们的方法提高了其他不确定性传播方法和概率世界模型组合的样本效率。

    arXiv:2403.15908v1 Announce Type: new  Abstract: Probabilistic world models increase data efficiency of model-based reinforcement learning (MBRL) by guiding the policy with their epistemic uncertainty to improve exploration and acquire new samples. Moreover, the uncertainty-aware learning procedures in probabilistic approaches lead to robust policies that are less sensitive to noisy observations compared to uncertainty unaware solutions. We propose to combine trajectory sampling and deep Gaussian covariance network (DGCN) for a data-efficient solution to MBRL problems in an optimal control setting. We compare trajectory sampling with density-based approximation for uncertainty propagation using three different probabilistic world models; Gaussian processes, Bayesian neural networks, and DGCNs. We provide empirical evidence using four different well-known test environments, that our method improves the sample-efficiency over other combinations of uncertainty propagation methods and prob
    
[^143]: 面向资源受限设备的低能量自适应个性化研究

    Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices

    [https://arxiv.org/abs/2403.15905](https://arxiv.org/abs/2403.15905)

    提出了面向资源受限设备的低能耗自适应个性化框架目标块微调，根据数据漂移类型微调不同模块以实现最佳性能和降低能源消耗。

    

    机器学习（ML）模型个性化以解决数据漂移问题在物联网（IoT）应用中是一个重要挑战。目前，大多数方法侧重于微调完整基础模型或其最后几层以适应新数据，但往往忽视能源成本。我们提出了一种面向资源受限设备的低能耗自适应个性化框架——目标块微调（TBFT）。我们将数据漂移和个性化分为三种类型：输入级别、特征级别和输出级别。针对每种类型，我们微调不同模型块以实现在降低能源成本的情况下达到最佳性能。具体而言，输入级、特征级和输出级对应于微调模型的前端、中段和后端。

    arXiv:2403.15905v1 Announce Type: new  Abstract: The personalization of machine learning (ML) models to address data drift is a significant challenge in the context of Internet of Things (IoT) applications. Presently, most approaches focus on fine-tuning either the full base model or its last few layers to adapt to new data, while often neglecting energy costs. However, various types of data drift exist, and fine-tuning the full base model or the last few layers may not result in optimal performance in certain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy adaptive personalization framework designed for resource-constrained devices. We categorize data drift and personalization into three types: input-level, feature-level, and output-level. For each type, we fine-tune different blocks of the model to achieve optimal performance with reduced energy costs. Specifically, input-, feature-, and output-level correspond to fine-tuning the front, middle, and rear blocks of 
    
[^144]: 利用零-shot提示实现高效语言模型蒸馏

    Leveraging Zero-Shot Prompting for Efficient Language Model Distillation

    [https://arxiv.org/abs/2403.15886](https://arxiv.org/abs/2403.15886)

    通过利用零-shot提示来引出教师模型的理由，减少手工制作的少-shot示例的必要性，并降低所需的总记号数，这直接转化为成本节约。

    

    本文介绍了一种新颖的方法，用于将LLMs高效地蒸馏为更小、特定于应用的模型，显著降低运营成本和人工劳动。该技术利用LLMs的推理能力为未标记数据生成标签和自然语言理由，以解决将计算密集型LLMs部署到特定应用或边缘设备的挑战。我们的方法通过采用多任务训练框架，其中学生模型模仿这些理由以及教师模型的预测，来增强微调和蒸馏。关键贡献包括利用零-shot提示来引出教师模型的理由，减少手工制作的少-shot示例的必要性，并降低所需的总记号数，这直接转化为成本节约，考虑到主要技术公司LLM APIs的按记号计费模型。此外，本文还调查了影响

    arXiv:2403.15886v1 Announce Type: cross  Abstract: This paper introduces a novel approach for efficiently distilling LLMs into smaller, application-specific models, significantly reducing operational costs and manual labor. Addressing the challenge of deploying computationally intensive LLMs in specific applications or edge devices, this technique utilizes LLMs' reasoning capabilities to generate labels and natural language rationales for unlabeled data. Our approach enhances both finetuning and distillation by employing a multi-task training framework where student models mimic these rationales alongside teacher predictions. Key contributions include the employment of zero-shot prompting to elicit teacher model rationales, reducing the necessity for handcrafted few-shot examples and lowering the overall token count required, which directly translates to cost savings given the pay-per-token billing model of major tech companies' LLM APIs. Additionally, the paper investigates the impact
    
[^145]: 快速统一的路径梯度估计器用于归一化流

    Fast and Unified Path Gradient Estimators for Normalizing Flows

    [https://arxiv.org/abs/2403.15881](https://arxiv.org/abs/2403.15881)

    提出了一种快速路径梯度估计器，显著提高了计算效率，并适用于所有实用的归一化流架构，具有正则化效果并减小了方差。

    

    最近的研究表明，归一化流的路径梯度估计器与变分推断的标准估计器相比具有更低的方差，从而改善了训练。然而，从计算角度看，它们往往昂贵且无法以可扩展的方式应用于最大似然训练，严重阻碍了它们的广泛采用。在这项工作中，我们克服了这些关键限制。具体来说，我们提出了一种快速路径梯度估计器，显著提高了计算效率，并适用于所有实用的归一化流架构。然后，我们证明该估计器也可以应用于最大似然训练，对此具有正则化效果，因为它可以考虑给定目标能量函数的形式。我们凭经验证明其在多个自然科学应用中表现卓越，并减小了方差。

    arXiv:2403.15881v1 Announce Type: new  Abstract: Recent work shows that path gradient estimators for normalizing flows have lower variance compared to standard estimators for variational inference, resulting in improved training. However, they are often prohibitively more expensive from a computational point of view and cannot be applied to maximum likelihood training in a scalable manner, which severely hinders their widespread adoption. In this work, we overcome these crucial limitations. Specifically, we propose a fast path gradient estimator which improves computational efficiency significantly and works for all normalizing flow architectures of practical relevance. We then show that this estimator can also be applied to maximum likelihood training for which it has a regularizing effect as it can take the form of a given target energy function into account. We empirically establish its superior performance and reduced variance for several natural sciences applications.
    
[^146]: 初始值和拓扑结构在分散式联邦学习中的影响

    Initialisation and Topology Effects in Decentralised Federated Learning

    [https://arxiv.org/abs/2403.15855](https://arxiv.org/abs/2403.15855)

    分散式联邦学习的有效性受到连接设备网络拓扑结构的显著影响，我们提出了基于底层网络节点特征向量中心性分布的改进神经网络初始化策略，大大提高了训练效率。

    

    具有完全分散式特征的联邦学习使得在网络上分布式设备上对个体机器学习模型进行协作训练，同时保持训练数据本地化。这种方法增强了数据隐私性，消除了单点故障和中央协调的必要性。我们的研究强调了分散式联邦学习的有效性受到连接设备的网络拓扑结构的显著影响。一个简化的数值模型用于研究这些系统的早期行为，使我们得出了一个利用底层网络节点的特征向量中心性分布的改进人工神经网络初始值策略，从而大大提高了训练效率。此外，我们的研究探讨了在我们提出的初始化策略下的比例行为和环境参数的选择。这项工作为更多研究打开了道路。

    arXiv:2403.15855v1 Announce Type: cross  Abstract: Fully decentralised federated learning enables collaborative training of individual machine learning models on distributed devices on a network while keeping the training data localised. This approach enhances data privacy and eliminates both the single point of failure and the necessity for central coordination. Our research highlights that the effectiveness of decentralised federated learning is significantly influenced by the network topology of connected devices. A simplified numerical model for studying the early behaviour of these systems leads us to an improved artificial neural network initialisation strategy, which leverages the distribution of eigenvector centralities of the nodes of the underlying network, leading to a radically improved training efficiency. Additionally, our study explores the scaling behaviour and choice of environmental parameters under our proposed initialisation strategy. This work paves the way for mor
    
[^147]: TablePuppet：关系数据联邦学习的通用框架

    TablePuppet: A Generic Framework for Relational Federated Learning

    [https://arxiv.org/abs/2403.15839](https://arxiv.org/abs/2403.15839)

    TablePuppet提出了关系数据联邦学习（RFL）的通用框架，使用连接中的学习（LoJ）和联合中的学习（LoU）来处理分布式关系表中的训练数据

    

    当前的联邦学习（FL）方法将分散的训练数据视为单个表，分为水平方式（按行）或垂直方式（按列）分配给参与者。然而，这些方法无法处理分布在数据库中的关系表。这种情况需要进行复杂的SQL操作，如连接和合并操作，以获得训练数据，这些操作要么成本高昂，要么受到隐私问题的限制。这引出了一个问题：我们能否直接在分布式关系表上运行FL呢？在本文中，我们将这一问题形式化为关系数据联邦学习（RFL）。我们提出了TablePuppet，一个用于RFL的通用框架，将学习过程分解为两个步骤：（1）连接中的学习（LoJ），然后是（2）联合中的学习（LoU）。简而言之，LoJ将学习推向加入的垂直表，LoU进一步将学习推向每个垂直表的水平分区

    arXiv:2403.15839v1 Announce Type: new  Abstract: Current federated learning (FL) approaches view decentralized training data as a single table, divided among participants either horizontally (by rows) or vertically (by columns). However, these approaches are inadequate for handling distributed relational tables across databases. This scenario requires intricate SQL operations like joins and unions to obtain the training data, which is either costly or restricted by privacy concerns. This raises the question: can we directly run FL on distributed relational tables?   In this paper, we formalize this problem as relational federated learning (RFL). We propose TablePuppet, a generic framework for RFL that decomposes the learning process into two steps: (1) learning over join (LoJ) followed by (2) learning over union (LoU). In a nutshell, LoJ pushes learning down onto the vertical tables being joined, and LoU further pushes learning down onto the horizontal partitions of each vertical table
    
[^148]: 语言-图像预训练的中心掩蔽技术

    Centered Masking for Language-Image Pre-Training

    [https://arxiv.org/abs/2403.15837](https://arxiv.org/abs/2403.15837)

    使用中心掩蔽的GLIP技术在语言-图像预训练中取代了随机掩蔽，利用高斯分布提高了性能，并且易于获得且适用于不具有明显中心焦点的数据集。

    

    我们引入了用于语言-图像预训练（GLIP）的高斯掩蔽，这是一种新颖、直接和有效的技术，用于在视觉-语言模型的预训练过程中对图像补丁进行掩蔽。GLIP基于快速语言-图像预训练（FLIP），该方法在训练CLIP模型时随机屏蔽图像补丁。GLIP将随机屏蔽替换为中心掩蔽，使用高斯分布，并受到图像中心重要性的启发。在一系列下游数据集和任务中，GLIP保留了与FLIP相同的计算节省能力，同时改善了性能，这是由我们的实验结果所证实的。我们展示了GLIP的好处很容易获得，无需精细调整高斯，也适用于包含无明显中心焦点图片的数据集。

    arXiv:2403.15837v1 Announce Type: cross  Abstract: We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel, straightforward, and effective technique for masking image patches during pre-training of a vision-language model. GLIP builds on Fast Language-Image Pre-Training (FLIP), which randomly masks image patches while training a CLIP model. GLIP replaces random masking with centered masking, that uses a Gaussian distribution and is inspired by the importance of image patches at the center of the image. GLIP retains the same computational savings as FLIP, while improving performance across a range of downstream datasets and tasks, as demonstrated by our experimental results. We show the benefits of GLIP to be easy to obtain, requiring no delicate tuning of the Gaussian, and also applicable to data sets containing images without an obvious center focus.
    
[^149]: 通过Dropout对时间任务进行比例学习的策略优化扩展

    Scaling Learning based Policy Optimization for Temporal Tasks via Dropout

    [https://arxiv.org/abs/2403.15826](https://arxiv.org/abs/2403.15826)

    本文介绍了一种基于模型的方法用于训练在高度非线性环境中运行的自主智能体的反馈控制器，通过对任务进行形式化表述，实现对特定任务目标的定量满足语义，并利用前馈神经网络学习反馈控制器。

    

    本文介绍了一种基于模型的方法，用于训练在高度非线性环境中运行的自主智能体的反馈控制器。我们希望经过训练的策略能够确保该智能体满足特定的任务目标，这些目标以离散时间信号时间逻辑（DT-STL）表示。通过将任务重新表述为形式化框架（如DT-STL），一个优势是允许定量满足语义。换句话说，给定一个轨迹和一个DT-STL公式，我们可以计算鲁棒性，这可以解释为轨迹与满足该公式的轨迹集之间的近似有符号距离。我们利用反馈控制器，并假设使用前馈神经网络来学习这些反馈控制器。我们展示了这个学习问题与训练递归神经网络（RNNs）类似的地方，其中递归单元的数量与智能体的时间视野成比例。

    arXiv:2403.15826v1 Announce Type: cross  Abstract: This paper introduces a model-based approach for training feedback controllers for an autonomous agent operating in a highly nonlinear environment. We desire the trained policy to ensure that the agent satisfies specific task objectives, expressed in discrete-time Signal Temporal Logic (DT-STL). One advantage for reformulation of a task via formal frameworks, like DT-STL, is that it permits quantitative satisfaction semantics. In other words, given a trajectory and a DT-STL formula, we can compute the robustness, which can be interpreted as an approximate signed distance between the trajectory and the set of trajectories satisfying the formula. We utilize feedback controllers, and we assume a feed forward neural network for learning these feedback controllers. We show how this learning problem is similar to training recurrent neural networks (RNNs), where the number of recurrent units is proportional to the temporal horizon of the agen
    
[^150]: 基于碳排放强度的深度神经网络自适应推断

    Carbon Intensity-Aware Adaptive Inference of DNNs

    [https://arxiv.org/abs/2403.15824](https://arxiv.org/abs/2403.15824)

    通过基于碳排放强度的自适应模型选择，本研究提出的方法能够在低碳强度时段使用更高准确性的模型，在高碳强度时段使用更低准确性的模型，有效改善视觉识别服务的准确性，最多提高碳排放效率达80%。

    

    DNN推断以其巨大的能耗及由此产生的高碳足迹而闻名，通过根据一天中碳排放强度的变化调整模型大小和准确性，可以使其更加可持续。我们的启发式算法在低强度时段使用更大、更高准确性的模型，在高强度时段使用更小、更低准确性的模型。我们还引入了一个指标，即碳排放效率，以量化自适应模型选择在碳足迹方面的有效性。评估结果表明，所提出的方法能够将视觉识别服务的准确性提升高达80%以上。

    arXiv:2403.15824v1 Announce Type: cross  Abstract: DNN inference, known for its significant energy consumption and the resulting high carbon footprint, can be made more sustainable by adapting model size and accuracy to the varying carbon intensity throughout the day. Our heuristic algorithm uses larger, high-accuracy models during low-intensity periods and smaller, lower-accuracy ones during high-intensity periods. We also introduce a metric, carbon-emission efficiency, which quantitatively measures the efficacy of adaptive model selection in terms of carbon footprint. The evaluation showed that the proposed approach could improve the carbon emission efficiency in improving the accuracy of vision recognition services by up to 80%.
    
[^151]: 高效的混合向量-关系搜索数据访问路径

    Efficient Data Access Paths for Mixed Vector-Relational Search

    [https://arxiv.org/abs/2403.15807](https://arxiv.org/abs/2403.15807)

    提出了针对高效混合向量-关系搜索的替代数据访问路径设计和优化方法。

    

    机器学习能力的快速增长以及使用向量嵌入进行数据处理方法的采用引发了对创建向量数据管理系统的极大兴趣。本文重新评估精确但穷尽的扫描式搜索，并提出硬件优化和替代张量式公式化和批处理以抵消成本。我们概述复杂的访问路径设计空间，主要由关系选择性驱动，以及考虑的决策。

    arXiv:2403.15807v1 Announce Type: cross  Abstract: The rapid growth of machine learning capabilities and the adoption of data processing methods using vector embeddings sparked a great interest in creating systems for vector data management. While the predominant approach of vector data management is to use specialized index structures for fast search over the entirety of the vector embeddings, once combined with other (meta)data, the search queries can also become selective on relational attributes - typical for analytical queries. As using vector indexes differs from traditional relational data access, we revisit and analyze alternative access paths for efficient mixed vector-relational search.   We first evaluate the accurate but exhaustive scan-based search and propose hardware optimizations and alternative tensor-based formulation and batching to offset the cost. We outline the complex access-path design space, primarily driven by relational selectivity, and the decisions to consi
    
[^152]: 从损失角度理解语言模型的突现能力

    Understanding Emergent Abilities of Language Models from the Loss Perspective

    [https://arxiv.org/abs/2403.15796](https://arxiv.org/abs/2403.15796)

    本文从损失角度重新定义了语言模型的突现能力，发现具有相同预训练损失的模型在不同任务上表现相似，而当预训练损失低于特定阈值时，模型将展现出突现能力。

    

    近期研究质疑了传统认为语言模型的突现能力仅存在于大模型中的观点。这种怀疑源自两点观察：1）较小的模型也能展现出对突现能力的高性能；2）质疑用于测量这些能力的不连续性指标。本文提议从预训练损失的角度研究突现能力，而非模型大小或训练计算。我们展示了具有相同预训练损失但不同模型和数据大小的模型，在各种下游任务上表现相同。我们还发现，当某一模型的预训练损失低于特定阈值时，在某些任务上表现出突现能力，而不论指标的连续性如何；而在达到该阈值之前，其性能仍保持在随机猜测水平。这启发我们重新定义突现能力为那些......

    arXiv:2403.15796v1 Announce Type: cross  Abstract: Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that
    
[^153]: ISS登机：不平衡的自监督：发现用于混合表格数据集的缩放自动编码器

    Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled Autoencoder for Mixed Tabular Datasets

    [https://arxiv.org/abs/2403.15790](https://arxiv.org/abs/2403.15790)

    这里是中文总结出的一句话要点: 本文针对表格数据领域中自监督学习中的数据不平衡挑战，提出了一种用于混合表格数据集的缩放自动编码器，填补了研究中的缺口。

    

    arXiv:2403.15790v1公告类型：新摘要：自监督学习领域，特别是在表格数据领域，不太受到广泛关注。现有研究主要集中在图像数据集上。本文旨在填补这一空白，通过探讨自监督学习中数据不平衡在表格数据领域中所带来的具体挑战，重点放在自动编码器上。自动编码器广泛用于学习和构建数据集的新表示，特别是用于降维。它们也经常用于生成模型学习，如变分自动编码器中所见。在处理混合表格数据时，定性变量通常使用独热编码器与标准损失函数（均方误差或交叉熵）进行编码。在本文中，我们分析了这种方法的缺点，特别是当分类变量不平衡时。我们提出了一个新的度量以平衡学习：一个多监督的 Ba

    arXiv:2403.15790v1 Announce Type: new  Abstract: The field of imbalanced self-supervised learning, especially in the context of tabular data, has not been extensively studied. Existing research has predominantly focused on image datasets. This paper aims to fill this gap by examining the specific challenges posed by data imbalance in self-supervised learning in the domain of tabular data, with a primary focus on autoencoders. Autoencoders are widely employed for learning and constructing a new representation of a dataset, particularly for dimensionality reduction. They are also often used for generative model learning, as seen in variational autoencoders. When dealing with mixed tabular data, qualitative variables are often encoded using a one-hot encoder with a standard loss function (MSE or Cross Entropy). In this paper, we analyze the drawbacks of this approach, especially when categorical variables are imbalanced. We propose a novel metric to balance learning: a Multi-Supervised Ba
    
[^154]: 面向公平性的共享微移动服务运营与控制的强化学习方法

    A Fairness-Oriented Reinforcement Learning Approach for the Operation and Control of Shared Micromobility Services

    [https://arxiv.org/abs/2403.15780](https://arxiv.org/abs/2403.15780)

    本研究介绍了一种在共享微移动服务运营与控制中实现性能优化和算法公平性平衡的前沿调查，利用Q-Learning算法确保方法稳健，能够实现各种站点类别之间的公平结果。

    

    随着机器学习系统在各种应用领域变得日益普遍，包括那些直接涉及人类的领域，平等和算法公平性的必要性在人工智能界愈发突出。另一方面，在共享微移动系统的背景下，公平性导向方法的探索仍然有限。为填补这一空白，我们引入了一项探讨性研究，探讨了共享微移动服务运营与控制中性能优化与算法公平性之间的平衡。我们的研究运用强化学习中的Q-Learning算法，利用其收敛保证来确保我们提出的方法的稳健性。值得注意的是，我们的方法在不同站点类别（中心、边缘和远程）之间能够实现公平的结果，这是通过基尼系数来衡量的。

    arXiv:2403.15780v1 Announce Type: cross  Abstract: As Machine Learning systems become increasingly popular across diverse application domains, including those with direct human implications, the imperative of equity and algorithmic fairness has risen to prominence in the Artificial Intelligence community. On the other hand, in the context of Shared Micromobility Systems, the exploration of fairness-oriented approaches remains limited. Addressing this gap, we introduce a pioneering investigation into the balance between performance optimization and algorithmic fairness in the operation and control of Shared Micromobility Services. Our study leverages the Q-Learning algorithm in Reinforcement Learning, benefiting from its convergence guarantees to ensure the robustness of our proposed approach. Notably, our methodology stands out for its ability to achieve equitable outcomes, as measured by the Gini index, across different station categories--central, peripheral, and remote. Through stra
    
[^155]: 通过多样化功能表示的集成学习：功能投票分类器

    Supervised Learning via Ensembles of Diverse Functional Representations: the Functional Voting Classifier

    [https://arxiv.org/abs/2403.15778](https://arxiv.org/abs/2403.15778)

    这份论文着重探讨了功能数据的集成学习，并展示了如何利用不同的功能数据表示训练集成成员，以及如何通过多数投票组合基模型预测。

    

    许多传统的统计和机器学习方法在直接应用于高维时序观测时面临挑战。在最近几十年中，功能数据分析(FDA)作为一种模拟和分析天然为时间域内的函数的数据的框架已经广泛流行起来。虽然在FDA文献中对监督分类进行了广泛探讨，但功能分类器的集成学习却最近才成为一个备受关注的话题。因此，后者从各种统计角度呈现出未经探索的方面和挑战。本文的焦点在于功能数据的集成学习，并旨在展示如何利用不同的功能数据表示来训练集成成员以及如何通过多数投票来组合基模型预测。所谓的功能投票Cla

    arXiv:2403.15778v1 Announce Type: cross  Abstract: Many conventional statistical and machine learning methods face challenges when applied directly to high dimensional temporal observations. In recent decades, Functional Data Analysis (FDA) has gained widespread popularity as a framework for modeling and analyzing data that are, by their nature, functions in the domain of time. Although supervised classification has been extensively explored in recent decades within the FDA literature, ensemble learning of functional classifiers has only recently emerged as a topic of significant interest. Thus, the latter subject presents unexplored facets and challenges from various statistical perspectives. The focal point of this paper lies in the realm of ensemble learning for functional data and aims to show how different functional data representations can be used to train ensemble members and how base model predictions can be combined through majority voting. The so-called Functional Voting Cla
    
[^156]: FusionINN：可逆图像融合用于脑肿瘤监测

    FusionINN: Invertible Image Fusion for Brain Tumor Monitoring

    [https://arxiv.org/abs/2403.15769](https://arxiv.org/abs/2403.15769)

    FusionINN引入了一种新颖的可逆图像融合框架，可以高效生成融合图像，并解开融合过程的逆向分解，保证无损的像素映射。

    

    图像融合通常使用不可逆神经网络将多个源图像合并为单个融合图像。然而，对于临床专家，仅依赖融合图像可能不足以做出诊断决策，因为融合机制混合了来自源图像的特征，从而难以解释潜在的肿瘤病理。我们引入了FusionINN，一种新颖的可逆图像融合框架，能够高效生成融合图像，并通过求解融合过程的逆过程将其分解回源图像。FusionINN通过整合一个正态分布的潜在图像与融合图像一起，以促进分解过程的生成建模，从而保证无损的一对一像素映射。据我们所知，我们是首次研究融合图像的可分解性，这对于生命敏感应用程序尤为关键。

    arXiv:2403.15769v1 Announce Type: cross  Abstract: Image fusion typically employs non-invertible neural networks to merge multiple source images into a single fused image. However, for clinical experts, solely relying on fused images may be insufficient for making diagnostic decisions, as the fusion mechanism blends features from source images, thereby making it difficult to interpret the underlying tumor pathology. We introduce FusionINN, a novel invertible image fusion framework, capable of efficiently generating fused images and also decomposing them back to the source images by solving the inverse of the fusion process. FusionINN guarantees lossless one-to-one pixel mapping by integrating a normally distributed latent image alongside the fused image to facilitate the generative modeling of the decomposition process. To the best of our knowledge, we are the first to investigate the decomposability of fused images, which is particularly crucial for life-sensitive applications such as
    
[^157]: 基于高效神经网络扩散的Bagging深度学习训练（BEND）

    BEND: Bagging Deep Learning Training Based on Efficient Neural Network Diffusion

    [https://arxiv.org/abs/2403.15766](https://arxiv.org/abs/2403.15766)

    本文提出了一种基于神经网络扩散模型的Bagging深度学习训练算法（BEND），有效构建了多个基本分类器，简单而有效。

    

    Bagging通过整合多个基本分类器构建一个强大的单一分类器来降低模型方差，在机器学习领域取得了巨大成功。本文提出了一种基于高效神经网络扩散的Bagging深度学习训练算法（BEND），利用神经网络扩散模型高效构建基本分类器。这种方法简单而有效。

    arXiv:2403.15766v1 Announce Type: cross  Abstract: Bagging has achieved great success in the field of machine learning by integrating multiple base classifiers to build a single strong classifier to reduce model variance. The performance improvement of bagging mainly relies on the number and diversity of base classifiers. However, traditional deep learning model training methods are expensive to train individually and difficult to train multiple models with low similarity in a restricted dataset. Recently, diffusion models, which have been tremendously successful in the fields of imaging and vision, have been found to be effective in generating neural network model weights and biases with diversity. We creatively propose a Bagging deep learning training algorithm based on Efficient Neural network Diffusion (BEND). The originality of BEND comes from the first use of a neural network diffusion model to efficiently build base classifiers for bagging. Our approach is simple but effective, 
    
[^158]: 用户端实现

    User-Side Realization

    [https://arxiv.org/abs/2403.15757](https://arxiv.org/abs/2403.15757)

    用户端实现为用户提供了积极的解决方案，通过在用户端运行通用算法来解决常见问题，无需服务提供商改变服务本身。

    

    用户对服务感到不满意。由于服务并非量身定制给用户，因此不满意是自然而然的。问题在于，即使用户感到不满意，他们通常也没有解决不满的手段。用户无法修改服务的源代码，也无法强迫服务提供商进行更改。用户别无选择，只能保持不满意或退出服务。用户端实现通过提供通用算法来处理用户端的常见问题，为解决这一问题提供了积极的解决方案。这些算法在用户端运行，并在不需要服务提供商改变服务本身的情况下解决问题。

    arXiv:2403.15757v1 Announce Type: cross  Abstract: Users are dissatisfied with services. Since the service is not tailor-made for a user, it is natural for dissatisfaction to arise. The problem is, that even if users are dissatisfied, they often do not have the means to resolve their dissatisfaction. The user cannot alter the source code of the service, nor can they force the service provider to change. The user has no choice but to remain dissatisfied or quit the service. User-side realization offers proactive solutions to this problem by providing general algorithms to deal with common problems on the user's side. These algorithms run on the user's side and solve the problems without having the service provider change the service itself.
    
[^159]: Horoballs和次梯度方法

    Horoballs and the subgradient method

    [https://arxiv.org/abs/2403.15749](https://arxiv.org/abs/2403.15749)

    该论文提出了一种在Hadamard空间中进行凸优化的迭代方法，与传统假设不同，其复杂性不依赖于空间曲率的下界。

    

    为了探索Hadamard空间上的凸优化问题，我们考虑了一种类似于次梯度算法的迭代。传统上，这类方法假设底层空间是流形，并且目标是测地凸的：这些方法是使用切空间和指数映射来描述的。相比之下，我们的迭代适用于一般的Hadamard空间，是在底层空间本身中构建的，并且依赖于客观水平集的horospherical凸性。对于这个受限制的客观类，我们证明了与通常形式的复杂性结果。值得注意的是，复杂性不依赖于空间曲率的下界。

    arXiv:2403.15749v1 Announce Type: cross  Abstract: To explore convex optimization on Hadamard spaces, we consider an iteration in the style of a subgradient algorithm. Traditionally, such methods assume that the underlying spaces are manifolds and that the objectives are geodesically convex: the methods are described using tangent spaces and exponential maps. By contrast, our iteration applies in a general Hadamard space, is framed in the underlying space itself, and relies instead on horospherical convexity of the objective level sets. For this restricted class of objectives, we prove a complexity result of the usual form. Notably, the complexity does not depend on a lower bound on the space curvature.
    
[^160]: 论主动学习者的脆弱性

    On the Fragility of Active Learners

    [https://arxiv.org/abs/2403.15744](https://arxiv.org/abs/2403.15744)

    本研究发现主动学习技术只在特定情境下有效，对文本分类从业者的建议是选择适当的文本表示和分类器同样重要。

    

    主动学习（AL）技术旨在通过迭代选择最有可能提高预测准确性的实例，最大程度地利用标注预算。然而，与随机抽样相比，在不同设置下（例如不同数据集，分类器），它们的益处并不一致。在这项实证研究中，我们研究了不同因素的组合如何可能掩盖主动学习技术的任何收益。专注于文本分类，我们在大约1000个实验中严格评估了进行分类，我们在大约1000个实验中严格评估了AL技术，这些实验在数据集、批大小、文本表示和分类器方面变化。我们表明，AL只在一组有限的情境中有效。我们还解决了使用与现实世界期望更好对齐的度量的问题。这项研究的影响在于对从业者的洞察：(a) 文本表示和分类器的选择与AL技术的选择一样重要，(b) 选择的

    arXiv:2403.15744v1 Announce Type: cross  Abstract: Active learning (AL) techniques aim to maximally utilize a labeling budget by iteratively selecting instances that are most likely to improve prediction accuracy. However, their benefit compared to random sampling has not been consistent across various setups, e.g., different datasets, classifiers. In this empirical study, we examine how a combination of different factors might obscure any gains from an AL technique.   Focusing on text classification, we rigorously evaluate AL techniques over around 1000 experiments that vary wrt the dataset, batch size, text representation and the classifier. We show that AL is only effective in a narrow set of circumstances. We also address the problem of using metrics that are better aligned with real world expectations.   The impact of this study is in its insights for a practitioner: (a) the choice of text representation and classifier is as important as that of an AL technique, (b) choice of the 
    
[^161]: Ghost Sentence：一种供普通用户使用的工具，用于对大型语言模型中的数据进行版权保护

    Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large Language Models

    [https://arxiv.org/abs/2403.15740](https://arxiv.org/abs/2403.15740)

    通过在文档中插入个人密码并识别生成内容中的“幽灵句子”，普通用户可以确认大型语言模型是否滥用其数据，从而实现数据版权保护。

    

    Web用户数据在预训练大型语言模型（LLMs）及其微调变种的生态系统中起着核心作用。本文提出了一种方法，建议用户在其文档中反复插入个人密码，使LLMs能够记忆这些密码。这些用户文档中隐藏的密码，被称为“幽灵句子”，一旦它们出现在LLMs生成的内容中，用户就可以确信他们的数据被用于训练。为了探索这种版权工具的有效性和用法，我们利用幽灵句子定义了“用户训练数据识别”任务。我们创建了来自不同来源、不同规模的多个数据集，并使用不同规模的LLMs进行测试。为了评估，我们引入了一个最后$k$个单词验证的方式。

    arXiv:2403.15740v1 Announce Type: new  Abstract: Web user data plays a central role in the ecosystem of pre-trained large language models (LLMs) and their fine-tuned variants. Billions of data are crawled from the web and fed to LLMs. How can \textit{\textbf{everyday web users}} confirm if LLMs misuse their data without permission? In this work, we suggest that users repeatedly insert personal passphrases into their documents, enabling LLMs to memorize them. These concealed passphrases in user documents, referred to as \textit{ghost sentences}, once they are identified in the generated content of LLMs, users can be sure that their data is used for training. To explore the effectiveness and usage of this copyrighting tool, we define the \textit{user training data identification} task with ghost sentences. Multiple datasets from various sources at different scales are created and tested with LLMs of different sizes. For evaluation, we introduce a last $k$ words verification manner along 
    
[^162]: 基于空间群信息的晶体材料生成变压器

    Space Group Informed Transformer for Crystalline Materials Generation

    [https://arxiv.org/abs/2403.15734](https://arxiv.org/abs/2403.15734)

    CrystalFormer是一种基于变压器的自回归模型，专门设计用于受空间群控制的晶体材料生成，它通过预测单位胞中对称不等价原子的种类和位置来生成晶体，并在有效性、新颖性和稳定性等方面达到了与最先进性能相匹配的水平。

    

    我们引入了CrystalFormer，这是一种基于变压器的自回归模型，专门设计用于受空间群控制的晶体材料生成。空间群对称性显著简化了晶体空间，这对于数据和计算有效的晶体材料生成建模至关重要。通过利用Wyckoff位置的显著离散和顺序特性，CrystalFormer学会了通过直接预测单位胞中对称不等价原子的种类和位置来生成晶体。我们的结果表明，CrystalFormer在生成的晶体材料的有效性、新颖性和稳定性方面与标准基准上的最新性能相匹配。我们的分析还表明，CrystalFormer从数据中吸收了合理的固体化学信息用于生成建模。CrystalFormer统一了基于对称性的结构搜索和生成性预训练。

    arXiv:2403.15734v1 Announce Type: cross  Abstract: We introduce CrystalFormer, a transformer-based autoregressive model specifically designed for space group-controlled generation of crystalline materials. The space group symmetry significantly simplifies the crystal space, which is crucial for data and compute efficient generative modeling of crystalline materials. Leveraging the prominent discrete and sequential nature of the Wyckoff positions, CrystalFormer learns to generate crystals by directly predicting the species and locations of symmetry-inequivalent atoms in the unit cell. Our results demonstrate that CrystalFormer matches state-of-the-art performance on standard benchmarks for both validity, novelty, and stability of the generated crystalline materials. Our analysis also shows that CrystalFormer ingests sensible solid-state chemistry information from data for generative modeling. The CrystalFormer unifies symmetry-based structure search and generative pre-training in the re
    
[^163]: 对流扩散方程：神经网络的理论认证框架

    Convection-Diffusion Equation: A Theoretically Certified Framework for Neural Networks

    [https://arxiv.org/abs/2403.15726](https://arxiv.org/abs/2403.15726)

    本文探讨了神经网络的偏微分方程模型，并提出了基于对流扩散方程的神经网络框架及结构设计，在理论上认证了神经网络的数学基础，实验证实了提出模型的性能。

    

    在这篇论文中，我们研究了神经网络的偏微分方程模型。神经网络可以被看作是从一个简单的基础模型映射到一个复杂函数。通过扎实的分析，我们展示了这个映射可以被形式化为一个对流扩散方程。这个在理论上认证的框架为神经网络提供了数学基础和更多理解。此外，基于对流扩散方程模型，我们设计了一种新颖的网络结构，将扩散机制融入网络架构中。在基准数据集和真实应用中进行了大量实验，验证了所提出模型的性能。

    arXiv:2403.15726v1 Announce Type: new  Abstract: In this paper, we study the partial differential equation models of neural networks. Neural network can be viewed as a map from a simple base model to a complicate function. Based on solid analysis, we show that this map can be formulated by a convection-diffusion equation. This theoretically certified framework gives mathematical foundation and more understanding of neural networks. Moreover, based on the convection-diffusion equation model, we design a novel network structure, which incorporates diffusion mechanism into network architecture. Extensive experiments on both benchmark datasets and real-world applications validate the performance of the proposed model.
    
[^164]: Ev-Edge: 在商品边缘平台上对基于事件的视觉算法进行高效执行

    Ev-Edge: Efficient Execution of Event-based Vision Algorithms on Commodity Edge Platforms

    [https://arxiv.org/abs/2403.15717](https://arxiv.org/abs/2403.15717)

    提出了 Ev-Edge 框架，通过三个关键优化措施提高在商品边缘平台上执行基于事件的视觉算法的性能

    

    事件相机已经成为自主导航系统的一种有前途的感知模式，因为它们具有高时间分辨率、高动态范围和可以忽略的运动模糊。为了处理这些传感器的异步时间事件流，最近的研究表明，需要一种由人工神经网络（ANNs）、脉冲神经网络（SNNs）以及混合SNN-ANN算法组成的组合，以在一系列感知任务上实现高准确度。然而，我们发现在商品边缘平台上执行此类工作负载（该平台具有诸如CPU、GPU和神经加速器之类的异构处理单元）会导致性能较差。这是由于事件流的不规则性质、算法的多样性特征与硬件平台之间的不匹配。我们提出了Ev-Edge，这是一个包含三个关键优化的框架，以提高性能

    arXiv:2403.15717v1 Announce Type: new  Abstract: Event cameras have emerged as a promising sensing modality for autonomous navigation systems, owing to their high temporal resolution, high dynamic range and negligible motion blur. To process the asynchronous temporal event streams from such sensors, recent research has shown that a mix of Artificial Neural Networks (ANNs), Spiking Neural Networks (SNNs) as well as hybrid SNN-ANN algorithms are necessary to achieve high accuracies across a range of perception tasks. However, we observe that executing such workloads on commodity edge platforms which feature heterogeneous processing elements such as CPUs, GPUs and neural accelerators results in inferior performance. This is due to the mismatch between the irregular nature of event streams and diverse characteristics of algorithms on the one hand and the underlying hardware platform on the other. We propose Ev-Edge, a framework that contains three key optimizations to boost the performance
    
[^165]: 可识别的潜在神经因果模型

    Identifiable Latent Neural Causal Models

    [https://arxiv.org/abs/2403.15711](https://arxiv.org/abs/2403.15711)

    该研究确定了在潜在附加噪声模型背景下导致可识别性的分布变化类型的充分且必要条件，同时提出了当只有部分分布变化满足条件时的部分可识别性结果。

    

    因果表征学习旨在从低级观测数据中揭示潜在的高级因果表征。它特别擅长预测在未见分布变化下，因为这些变化通常可以解释为干预的后果。因此，利用{已见}分布变化成为帮助识别因果表征的自然策略，进而有助于预测以前{未见}分布的情况。确定这些分布变化的类型（或条件）对于因果表征的可识别性至关重要。该工作建立了在潜在附加噪声模型背景下，表征导致可识别性的分布变化类型的充分且必要条件。此外，我们提出了当只有部分分布变化满足条件时的部分可识别性结果。

    arXiv:2403.15711v1 Announce Type: new  Abstract: Causal representation learning seeks to uncover latent, high-level causal representations from low-level observed data. It is particularly good at predictions under unseen distribution shifts, because these shifts can generally be interpreted as consequences of interventions. Hence leveraging {seen} distribution shifts becomes a natural strategy to help identifying causal representations, which in turn benefits predictions where distributions are previously {unseen}. Determining the types (or conditions) of such distribution shifts that do contribute to the identifiability of causal representations is critical. This work establishes a {sufficient} and {necessary} condition characterizing the types of distribution shifts for identifiability in the context of latent additive noise models. Furthermore, we present partial identifiability results when only a portion of distribution shifts meets the condition. In addition, we extend our findin
    
[^166]: 地域性和权重共享在基于图像的任务中的作用：CNN、LCN和FCN之间的样本复杂性分离

    Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs

    [https://arxiv.org/abs/2403.15707](https://arxiv.org/abs/2403.15707)

    介绍了新的Dynamic Signal Distribution (DSD)分类任务，模拟图像由$k$个维度为$d$的补丁组成，以解决CNNs相对于LCNs和FCNs的统计优势问题

    

    视觉任务的特点是地域性和平移不变性。卷积神经网络（CNNs）在这些任务上表现出色，这在很大程度上归因于其架构中固有的地域性和权重共享的归纳偏差。现有的试图量化这些偏差在CNNs上相对于局部连接的卷积神经网络（LCNs）和全连接神经网络（FCNs）的统计优势的尝试可以归为以下几类：要么它们忽视优化器，仅提供具有统一收敛上界但没有分隔下界的统计收敛性，要么考虑到不真实地反映现实世界视觉任务中的地域性和平移不变性的简单任务。为了解决这些不足，我们介绍了动态信号分布（DSD）分类任务，它将图像建模为包含$k$个尺寸为$d$的补丁，标签是de

    arXiv:2403.15707v1 Announce Type: cross  Abstract: Vision tasks are characterized by the properties of locality and translation invariance. The superior performance of convolutional neural networks (CNNs) on these tasks is widely attributed to the inductive bias of locality and weight sharing baked into their architecture. Existing attempts to quantify the statistical benefits of these biases in CNNs over locally connected convolutional neural networks (LCNs) and fully connected neural networks (FCNs) fall into one of the following categories: either they disregard the optimizer and only provide uniform convergence upper bounds with no separating lower bounds, or they consider simplistic tasks that do not truly mirror the locality and translation invariance as found in real-world vision tasks. To address these deficiencies, we introduce the Dynamic Signal Distribution (DSD) classification task that models an image as consisting of $k$ patches, each of dimension $d$, and the label is de
    
[^167]: G-ACIL：面向非范例化的广义类增量学习的分析学习

    G-ACIL: Analytic Learning for Exemplar-Free Generalized Class Incremental Learning

    [https://arxiv.org/abs/2403.15706](https://arxiv.org/abs/2403.15706)

    在这项研究中，我们提出了一种面向非范例化的广义分析类增量学习，通过采用分析学习并提供了对GCIL情景的分析解决方案，有效地解决了模型快速遗忘和数据隐私侵犯问题。

    

    分类增量学习(CIL)在顺序任务上训练网络，每个任务有不同的类别，但存在灾难性遗忘问题，当学习新任务时快速遗忘先前学到的知识。广义CIL(GCIL)旨在解决更接近现实情景下的CIL问题，即新数据具有混合数据类别和未知样本分布大小，导致遗忘加剧。现有的针对GCIL的尝试要么性能不佳，要么通过保存历史范例侵犯数据隐私。为了解决这个问题，本文提出了一种面向非范例化的广义分析类增量学习(G-ACIL)。G-ACIL采用分析学习(一种无梯度训练技术)，并为GCIL情景提供分析解(即闭合形式)。该解决方案通过将传入数据分解为暴露类和未暴露类，实现了增长类之间的等效性。

    arXiv:2403.15706v1 Announce Type: new  Abstract: Class incremental learning (CIL) trains a network on sequential tasks with separated categories but suffers from catastrophic forgetting, where models quickly lose previously learned knowledge when acquiring new tasks. The generalized CIL (GCIL) aims to address the CIL problem in a more real-world scenario, where incoming data have mixed data categories and unknown sample size distribution, leading to intensified forgetting. Existing attempts for the GCIL either have poor performance, or invade data privacy by saving historical exemplars. To address this, in this paper, we propose an exemplar-free generalized analytic class incremental learning (G-ACIL). The G-ACIL adopts analytic learning (a gradient-free training technique), and delivers an analytical solution (i.e., closed-form) to the GCIL scenario. This solution is derived via decomposing the incoming data into exposed and unexposed classes, allowing an equivalence between the incre
    
[^168]: 数据净化中的群体福利实例选择

    Group Benefits Instances Selection for Data Purification

    [https://arxiv.org/abs/2403.15694](https://arxiv.org/abs/2403.15694)

    提出了一种名为GRIP的方法，通过群体正则化策略估计类别软标签以提高噪声鲁棒性，减少对嘈杂标签的过拟合并学习类间相似性以改善分类结果。

    

    人工为训练深度模型注释数据集是非常费时费力的。为了克服这种劣势，直接利用网络图像来进行训练数据成为一种自然选择。然而，网络数据中的标签噪声通常会降低模型性能。现有的对抗标签噪声的方法通常是在合成嘈杂数据集上设计和测试的。然而，它们往往无法在真实世界嘈杂数据集上取得令人满意的结果。为此，我们提出了一种名为GRIP的方法，用于缓解合成和真实世界数据集的标签噪声问题。具体来说，GRIP利用一种群体正则化策略，估计类别软标签以提高噪声鲁棒性。软标签监督降低了对嘈杂标签的过拟合，并学习了有利于分类的类间相似性。此外，一个全局实例净化操作通过m

    arXiv:2403.15694v1 Announce Type: new  Abstract: Manually annotating datasets for training deep models is very labor-intensive and time-consuming. To overcome such inferiority, directly leveraging web images to conduct training data becomes a natural choice. Nevertheless, the presence of label noise in web data usually degrades the model performance. Existing methods for combating label noise are typically designed and tested on synthetic noisy datasets. However, they tend to fail to achieve satisfying results on real-world noisy datasets. To this end, we propose a method named GRIP to alleviate the noisy label problem for both synthetic and real-world datasets. Specifically, GRIP utilizes a group regularization strategy that estimates class soft labels to improve noise robustness. Soft label supervision reduces overfitting on noisy labels and learns inter-class similarities to benefit classification. Furthermore, an instance purification operation globally identifies noisy labels by m
    
[^169]: EAGLE：面向人工智能生成文本检测的领域泛化框架

    EAGLE: A Domain Generalization Framework for AI-generated Text Detection

    [https://arxiv.org/abs/2403.15690](https://arxiv.org/abs/2403.15690)

    EAGLE提出了一个领域泛化框架，能够利用从旧语言模型中获得的标记数据，学习特征的不变性，从而检测出未知目标生成器生成的文本。

    

    随着大型语言模型（LLMs）能力的提升，负责任和安全使用这些LLMs的一个重要步骤是能够检测这些模型生成的文本。尽管监督式AI生成的文本检测器在旧LLMs生成的文本上表现良好，但随着新LLMs的频繁发布，构建用于识别这些新模型文本的监督检测器将需要新的标记训练数据，在实践中是不可行的。在这项工作中，我们解决这个问题，提出了一个用于检测来自未知目标生成器的AI生成文本的领域泛化框架。我们提出的框架EAGLE利用迄今为止从旧语言模型获得的标记数据，并学习跨这些生成器不变的特征，以便检测由未知目标生成器生成的文本。EAGLE通过结合自监督的表征能力来学习这种领域不变特征。

    arXiv:2403.15690v1 Announce Type: cross  Abstract: With the advancement in capabilities of Large Language Models (LLMs), one major step in the responsible and safe use of such LLMs is to be able to detect text generated by these models. While supervised AI-generated text detectors perform well on text generated by older LLMs, with the frequent release of new LLMs, building supervised detectors for identifying text from such new models would require new labeled training data, which is infeasible in practice. In this work, we tackle this problem and propose a domain generalization framework for the detection of AI-generated text from unseen target generators. Our proposed framework, EAGLE, leverages the labeled data that is available so far from older language models and learns features invariant across these generators, in order to detect text generated by an unknown target generator. EAGLE learns such domain-invariant features by combining the representational power of self-supervised 
    
[^170]: 可微分信息瓶颈在确定性多视角聚类中的应用

    Differentiable Information Bottleneck for Deterministic Multi-view Clustering

    [https://arxiv.org/abs/2403.15681](https://arxiv.org/abs/2403.15681)

    提出了一种新的可微分信息瓶颈（DIB）方法，用于确定性多视角聚类，可以通过拟合相互信息来提供分析性解决方案，避免了变分逼近的需要。

    

    最近几年，信息瓶颈（IB）原理为深度多视角聚类（MVC）提供了信息理论框架，通过压缩多视角观察数据来保留多视角相关信息。本文提出了一种新的可微分信息瓶颈（DIB）方法，通过拟合相互信息而不需要变分逼近，提供了一种确定性和分析性的MVC解决方案。具体地，我们首先提出通过利用归一化核格拉姆矩阵直接拟合高维空间的相互信息，而无需任何辅助神经估计。

    arXiv:2403.15681v1 Announce Type: cross  Abstract: In recent several years, the information bottleneck (IB) principle provides an information-theoretic framework for deep multi-view clustering (MVC) by compressing multi-view observations while preserving the relevant information of multiple views. Although existing IB-based deep MVC methods have achieved huge success, they rely on variational approximation and distribution assumption to estimate the lower bound of mutual information, which is a notoriously hard and impractical problem in high-dimensional multi-view spaces. In this work, we propose a new differentiable information bottleneck (DIB) method, which provides a deterministic and analytical MVC solution by fitting the mutual information without the necessity of variational approximation. Specifically, we first propose to directly fit the mutual information of high-dimensional spaces by leveraging normalized kernel Gram matrix, which does not require any auxiliary neural estima
    
[^171]: 基于数据异质性的本地更新对分散式学习的有效性研究

    The Effectiveness of Local Updates for Decentralized Learning under Data Heterogeneity

    [https://arxiv.org/abs/2403.15654](https://arxiv.org/abs/2403.15654)

    通过在分散式学习中引入多个本地更新步骤，可以降低通信复杂度，从而在数据异质性低且网络高度连通时有效降低通信成本。

    

    我们重新审视了两种基本的分散式优化方法，即Decentralized Gradient Tracking (DGT) 和 Decentralized Gradient Descent (DGD)，并引入了多个本地更新步骤。我们考虑了两种情境，并且证明了加入 $K > 1$ 个本地更新步骤能够降低通信复杂度。具体而言，对于 $\mu$-强凸和 $L$-光滑损失函数，我们证明了本地 DGT 方法实现了通信复杂度为 $\tilde{\mathcal{O}} \Big(\frac{L}{\mu K} + \frac{\delta}{\mu (1 - \rho)} + \frac{\rho }{(1 - \rho)^2} \cdot \frac{L+ \delta}{\mu}\Big)$，其中 $\rho$ 衡量网络连通性，$\delta$ 表示本地损失的二阶异质性。我们的结果揭示了通信和计算之间的权衡，并表明在数据异质性低且网络高度连通时，增加 $K$ 能有效降低通信成本。

    arXiv:2403.15654v1 Announce Type: new  Abstract: We revisit two fundamental decentralized optimization methods, Decentralized Gradient Tracking (DGT) and Decentralized Gradient Descent (DGD), with multiple local updates. We consider two settings and demonstrate that incorporating $K > 1$ local update steps can reduce communication complexity. Specifically, for $\mu$-strongly convex and $L$-smooth loss functions, we proved that local DGT achieves communication complexity $\tilde{\mathcal{O}} \Big(\frac{L}{\mu K} + \frac{\delta}{\mu (1 - \rho)} + \frac{\rho }{(1 - \rho)^2} \cdot \frac{L+ \delta}{\mu}\Big)$, where $\rho$ measures the network connectivity and $\delta$ measures the second-order heterogeneity of the local loss. Our result reveals the tradeoff between communication and computation and shows increasing $K$ can effectively reduce communication costs when the data heterogeneity is low and the network is well-connected. We then consider the over-parameterization regime where the 
    
[^172]: 使用注意力和卷积的参数化编码缓解神经偏微分方程求解器的谱偏差

    Parametric Encoding with Attention and Convolution Mitigate Spectral Bias of Neural Partial Differential Equation Solvers

    [https://arxiv.org/abs/2403.15652](https://arxiv.org/abs/2403.15652)

    引入了Parametric Grid Convolutional Attention Networks（PGCANs），通过使用参数化编码器和注意力来解决偏微分方程系统，提高解决器的泛化性能和信息传播速率

    

    深度神经网络(DNNs)越来越多地用于解决在建模各种系统和物理现象时自然产生的偏微分方程(PDEs)。然而，随着PDE复杂性的增加，这些DNN的准确性会降低，它们也会受到谱偏差的影响，因为它们倾向于学习低频解的特征。为了解决这些问题，我们引入了Parametric Grid Convolutional Attention Networks (PGCANs)，可以在领域不依赖任何标记数据的情况下解决PDE系统。PGCAN的主要思想是用基于网格的编码器对输入空间进行参数化，其参数通过一个DNN解码器与输出相连接，后者利用注意力来优先选择特征进行训练。我们的编码器提供了本地化学习能力，并使用卷积层来避免过拟合，改善从边界到域内部的信息传播速率。

    arXiv:2403.15652v1 Announce Type: new  Abstract: Deep neural networks (DNNs) are increasingly used to solve partial differential equations (PDEs) that naturally arise while modeling a wide range of systems and physical phenomena. However, the accuracy of such DNNs decreases as the PDE complexity increases and they also suffer from spectral bias as they tend to learn the low-frequency solution characteristics. To address these issues, we introduce Parametric Grid Convolutional Attention Networks (PGCANs) that can solve PDE systems without leveraging any labeled data in the domain. The main idea of PGCAN is to parameterize the input space with a grid-based encoder whose parameters are connected to the output via a DNN decoder that leverages attention to prioritize feature training. Our encoder provides a localized learning ability and uses convolution layers to avoid overfitting and improve information propagation rate from the boundaries to the interior of the domain. We test the perfor
    
[^173]: 大型语言模型的差分私有下一个标记预测

    Differentially Private Next-Token Prediction of Large Language Models

    [https://arxiv.org/abs/2403.15638](https://arxiv.org/abs/2403.15638)

    提出了Private Mixing of Ensemble Distributions (PMixED)：通过将模型的输出分布投影到公共LLM的输出分布周围的集合上，并采样平均来实现实际的下一个标记预测，以更轻量化的方式实现对隐私敏感的大型语言模型的预测。

    

    确保大型语言模型（LLMs）的隐私日益重要。DP-SGD是实现这一目标的最广泛采用的技术，它以一种保证差分隐私的方式训练模型。然而，DP-SGD需要比SGD更长的训练时间和更大的内存需求，同时过高估计对手具有白盒访问模型的能力。更现实的场景假设只有对隐私敏感的LLM进行黑盒访问。在这些观察的基础上，我们提出了私有混合集合分布（PMixED）：一种通过将模型的每个输出分布从一个经过精细调整的LLM集合投影到公共LLM输出分布周围的集合上，然后对投影分布进行平均并从中抽样来实现实际的下一个标记预测的私有预测协议。我们的方法比DP-SGD更轻量化，因为它与模型无关。

    arXiv:2403.15638v1 Announce Type: cross  Abstract: Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly important. The most widely adopted technique to accomplish this is DP-SGD, which trains a model in such a way that guarantees Differential Privacy (DP). However, DP-SGD requires longer training times and larger memory requirements than SGD, while overestimating an adversary's capabilities in having white box access to the model. A more realistic scenario assumes only black-box access to a privacy-sensitive LLM. Motivated by these observations, we present Private Mixing of Ensemble Distributions (PMixED): a private prediction protocol that achieves practical next-token prediction by projecting each of the model's output distribution from an ensemble of fine-tuned LLMs onto a set around a public LLM's output distribution, then averaging the projected distributions and sampling from it. Our approach is more lightweight than DP-SGD in that it is model agnostic, i
    
[^174]: 为联邦领域泛化高效组合规范化层与正则化方法

    Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization

    [https://arxiv.org/abs/2403.15605](https://arxiv.org/abs/2403.15605)

    引入了一种新颖的FedDG架构方法gPerXAN，通过规范化方案和引导正则化器配合工作，实现了个性化显式组装规范化，有助于客户端模型对领域特征进行有选择性过滤。

    

    领域转移是机器学习中一个严峻的问题，会导致模型在未知领域测试时性能下降。联邦领域泛化（FedDG）旨在以隐私保护的方式使用协作客户端训练全局模型，能够很好地泛化到可能存在领域转移的未知客户端。然而，大多数现有的FedDG方法可能会导致额外的数据泄露隐私风险，或者在客户端通信和计算成本方面产生显著开销，这在联邦学习范式中是主要关注的问题。为了解决这些挑战，我们引入了一种新颖的FedDG架构方法，即gPerXAN，它依赖于一个规范化方案与引导正则化器配合工作。具体来说，我们精心设计了个性化显式组装规范化，以强制客户端模型有选择地过滤对本地数据有偏向的特定领域特征。

    arXiv:2403.15605v1 Announce Type: cross  Abstract: Domain shift is a formidable issue in Machine Learning that causes a model to suffer from performance degradation when tested on unseen domains. Federated Domain Generalization (FedDG) attempts to train a global model using collaborative clients in a privacy-preserving manner that can generalize well to unseen clients possibly with domain shift. However, most existing FedDG methods either cause additional privacy risks of data leakage or induce significant costs in client communication and computation, which are major concerns in the Federated Learning paradigm. To circumvent these challenges, here we introduce a novel architectural method for FedDG, namely gPerXAN, which relies on a normalization scheme working with a guiding regularizer. In particular, we carefully design Personalized eXplicitly Assembled Normalization to enforce client models selectively filtering domain-specific features that are biased towards local data while ret
    
[^175]: 一种用于运营亚季节天气预测的数据驱动天气预测模型集成

    An ensemble of data-driven weather prediction models for operational sub-seasonal forecasting

    [https://arxiv.org/abs/2403.15598](https://arxiv.org/abs/2403.15598)

    通过使用数据驱动天气预测模型的多模型集成方法，可以实现接近最新技术水平的 sub-seasonal 至 seasonal 天气预测。

    

    我们提出了一个运维即推广的多模型集成天气预测系统，该系统使用混合数据驱动的天气预测模型与欧洲中期天气预报中心（ECMWF）海洋模型耦合，在1度分辨率下预测全球天气的4周前瞻。对于2米温度的预测，我们的集成平均优于原始的ECMWF扩展范围集成，优势为4-17%，具体取决于前瞻时长。然而，在应用统计偏差校正后，ECMWF集成在4周时较集成检验约好3%。对于其他地面参数，我们的集成也与ECMWF的集成相差不到几个百分点。我们展示了利用多模型集成方法和数据驱动的天气预测模型可以实现接近最新技术水平的次季节至季节预测。

    arXiv:2403.15598v1 Announce Type: cross  Abstract: We present an operations-ready multi-model ensemble weather forecasting system which uses hybrid data-driven weather prediction models coupled with the European Centre for Medium-range Weather Forecasts (ECMWF) ocean model to predict global weather at 1-degree resolution for 4 weeks of lead time. For predictions of 2-meter temperature, our ensemble on average outperforms the raw ECMWF extended-range ensemble by 4-17%, depending on the lead time. However, after applying statistical bias corrections, the ECMWF ensemble is about 3% better at 4 weeks. For other surface parameters, our ensemble is also within a few percentage points of ECMWF's ensemble. We demonstrate that it is possible to achieve near-state-of-the-art subseasonal-to-seasonal forecasts using a multi-model ensembling approach with data-driven weather prediction models.
    
[^176]: 通过探索性数据分析和可解释的机器学习洞见分析男性家庭暴力

    Analyzing Male Domestic Violence through Exploratory Data Analysis and Explainable Machine Learning Insights

    [https://arxiv.org/abs/2403.15594](https://arxiv.org/abs/2403.15594)

    该研究是关于在孟加拉国背景下对男性家庭暴力进行开创性探索，揭示了男性受害者的存在、模式和潜在因素，填补了现有文献对男性受害者研究空白的重要性。

    

    家庭暴力通常被视为一个关于女性受害者的性别问题，在近年来越来越受到关注。尽管有这种关注，孟加拉国特别是男性受害者仍然主要被忽视。我们的研究代表了在孟加拉国背景下对男性家庭暴力（MDV）这一未被充分探讨领域的开创性探索，揭示了其普遍性、模式和潜在因素。现有文献主要强调家庭暴力情境中女性的受害，导致对男性受害者的研究空白。我们从孟加拉国主要城市收集了数据，并进行了探索性数据分析以了解潜在动态。我们使用了11种传统机器学习模型（包括默认和优化的超参数）、2种深度学习和4种集成模型。尽管采用了各种方法，CatBoost由于其...

    arXiv:2403.15594v1 Announce Type: cross  Abstract: Domestic violence, which is often perceived as a gendered issue among female victims, has gained increasing attention in recent years. Despite this focus, male victims of domestic abuse remain primarily overlooked, particularly in Bangladesh. Our study represents a pioneering exploration of the underexplored realm of male domestic violence (MDV) within the Bangladeshi context, shedding light on its prevalence, patterns, and underlying factors. Existing literature predominantly emphasizes female victimization in domestic violence scenarios, leading to an absence of research on male victims. We collected data from the major cities of Bangladesh and conducted exploratory data analysis to understand the underlying dynamics. We implemented 11 traditional machine learning models with default and optimized hyperparameters, 2 deep learning, and 4 ensemble models. Despite various approaches, CatBoost has emerged as the top performer due to its 
    
[^177]: FairerCLIP: 在RKHSs中使用函数去除CLIP的零样本预测偏见

    FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in RKHSs

    [https://arxiv.org/abs/2403.15593](https://arxiv.org/abs/2403.15593)

    FairerCLIP提出了一种在RKHSs中使用函数去除CLIP的零样本预测偏见的通用方法，使得预测更公平且更能抵抗虚假相关性。

    

    大型预训练的视觉-语言模型（如CLIP）提供了文本和图像的紧凑通用表示，已被证明在多个下游零样本预测任务中有效。然而，由于它们训练过程的性质，这些模型可能存在以下问题：1）传播或放大社会偏见和2）学习依赖虚假特征。本文提出FairerCLIP，一种通用方法，使CLIP的零样本预测更加公平且更能抵抗虚假相关性。我们在再生核希尔伯特空间（RKHSs）中联合去偏CLIP的图像和文本表示问题，这带来了多个好处：1）灵活性：与现有方法不同，现有方法要么专门用于学习有地面真相标签的情况，要么专门用于学习没有地面真相标签的情况，FairerCLIP能够适应两种学习情况。2）优化便利性：FairerCLIP对于迭代优化非常合适。

    arXiv:2403.15593v1 Announce Type: cross  Abstract: Large pre-trained vision-language models such as CLIP provide compact and general-purpose representations of text and images that are demonstrably effective across multiple downstream zero-shot prediction tasks. However, owing to the nature of their training process, these models have the potential to 1) propagate or amplify societal biases in the training data and 2) learn to rely on spurious features. This paper proposes FairerCLIP, a general approach for making zero-shot predictions of CLIP more fair and robust to spurious correlations. We formulate the problem of jointly debiasing CLIP's image and text representations in reproducing kernel Hilbert spaces (RKHSs), which affords multiple benefits: 1) Flexibility: Unlike existing approaches, which are specialized to either learn with or without ground-truth labels, FairerCLIP is adaptable to learning in both scenarios. 2) Ease of Optimization: FairerCLIP lends itself to an iterative o
    
[^178]: 基于内核化斯坦不相容性的数据中心预测解释

    Data-centric Prediction Explanation via Kernelized Stein Discrepancy

    [https://arxiv.org/abs/2403.15576](https://arxiv.org/abs/2403.15576)

    该论文提出了一种基于内核化斯坦不相容性的数据中心预测解释方法，通过利用内核函数识别提供最佳预测支持给测试点的训练样本，取得了优异性能。

    

    现有的基于示例的预测解释方法通常通过模型的参数或潜在表示来连接测试和训练数据点。尽管这些方法提供了有关模型预测原因的线索，但它们经常表现出固有的缺陷，比如产生显着的计算开销或生成粗粒度的解释。本文提出了一种高精度和数据中心的解释（HD-Explain），这是一种利用内核化斯坦不相容性（KSD）属性的简单预测解释方法。具体来说，KSD唯一地为经过训练的模型定义了一个参数化的内核函数，用于编码与模型相关的数据相关性。通过利用内核函数，可以有效地识别提供最佳预测支持给测试点的训练样本。我们在多个分类领域进行了彻底的分析和实验，结果表明HD-Explain取得了优异的性能。

    arXiv:2403.15576v1 Announce Type: new  Abstract: Existing example-based prediction explanation methods often bridge test and training data points through the model's parameters or latent representations. While these methods offer clues to the causes of model predictions, they often exhibit innate shortcomings, such as incurring significant computational overhead or producing coarse-grained explanations. This paper presents a Highly-precise and Data-centric Explanation (HD-Explain), a straightforward prediction explanation method exploiting properties of Kernelized Stein Discrepancy (KSD). Specifically, the KSD uniquely defines a parameterized kernel function for a trained model that encodes model-dependent data correlation. By leveraging the kernel function, one can identify training samples that provide the best predictive support to a test point efficiently. We conducted thorough analyses and experiments across multiple classification domains, where we show that HD-Explain outperform
    
[^179]: 不要相信你所信任的：半监督学习中的误校准问题

    Do not trust what you trust: Miscalibration in Semi-supervised Learning

    [https://arxiv.org/abs/2403.15567](https://arxiv.org/abs/2403.15567)

    SSL方法基于伪标签存在显著的误校准问题，通过最小化最小熵和引入一个惩罚项，可以缓解这一问题。

    

    最先进的半监督学习（SSL）方法依赖于高度自信的预测，作为伪标签，引导对未标记样本的训练。这种策略的固有缺点来自于不确定性估计的质量，因为伪标签仅基于它们的不确定性程度进行过滤，而不考虑其预测的正确性。因此，在伪标记过程中评估和增强网络预测的不确定性非常重要。在这项工作中，我们在实证上证明了基于伪标签的SSL方法存在显著的误校准问题，并形式化地证明了最小化最小熵（Shannon熵的下界）可能是误校准的一个潜在原因。为了缓解这个问题，我们集成了一个简单的惩罚项，强制未标记样本的预测的logit距离保持较低，从而防止网络预测

    arXiv:2403.15567v1 Announce Type: new  Abstract: State-of-the-art semi-supervised learning (SSL) approaches rely on highly confident predictions to serve as pseudo-labels that guide the training on unlabeled samples. An inherent drawback of this strategy stems from the quality of the uncertainty estimates, as pseudo-labels are filtered only based on their degree of uncertainty, regardless of the correctness of their predictions. Thus, assessing and enhancing the uncertainty of network predictions is of paramount importance in the pseudo-labeling process. In this work, we empirically demonstrate that SSL methods based on pseudo-labels are significantly miscalibrated, and formally demonstrate the minimization of the min-entropy, a lower bound of the Shannon entropy, as a potential cause for miscalibration. To alleviate this issue, we integrate a simple penalty term, which enforces the logit distances of the predictions on unlabeled samples to remain low, preventing the network prediction
    
[^180]: A2DMN：面向乳腺超声语义分割的解剖感知扩张多尺度网络

    A2DMN: Anatomy-Aware Dilated Multiscale Network for Breast Ultrasound Semantic Segmentation

    [https://arxiv.org/abs/2403.15560](https://arxiv.org/abs/2403.15560)

    提出了一种新颖的乳腺解剖感知网络，用于捕获细致图像细节和编码乳腺解剖的新平滑度项，有效改善了肌肉、乳房和肿瘤类别的分割准确性。

    

    最近，用于乳腺超声图像语义分割的卷积神经网络已经取得了巨大成功; 然而，仍然存在两个主要挑战。首先，大多数当前方法固有地缺乏利用组织解剖的能力，导致图像区域被错误分类。其次，它们由于不断下采样操作而难以产生准确的边界。为解决这些问题，我们提出了一种新颖的乳腺解剖感知网络，用于捕获精细图像细节，并提出了一个编码乳腺解剖的新平滑度项。它跨多个空间尺度合并上下文信息，以生成更准确的语义边界。我们进行了大量实验证明了所提方法与八种最先进方法在325幅乳腺超声图像数据集上的比较。结果表明，所提方法显著改善了肌肉、乳房和肿瘤类别的分割，并产生

    arXiv:2403.15560v1 Announce Type: cross  Abstract: In recent years, convolutional neural networks for semantic segmentation of breast ultrasound (BUS) images have shown great success; however, two major challenges still exist. 1) Most current approaches inherently lack the ability to utilize tissue anatomy, resulting in misclassified image regions. 2) They struggle to produce accurate boundaries due to the repeated down-sampling operations. To address these issues, we propose a novel breast anatomy-aware network for capturing fine image details and a new smoothness term that encodes breast anatomy. It incorporates context information across multiple spatial scales to generate more accurate semantic boundaries. Extensive experiments are conducted to compare the proposed method and eight state-of-the-art approaches using a BUS dataset with 325 images. The results demonstrate the proposed method significantly improves the segmentation of the muscle, mammary, and tumor classes and produces
    
[^181]: 依从在线模型聚合

    Conformal online model aggregation

    [https://arxiv.org/abs/2403.15527](https://arxiv.org/abs/2403.15527)

    该论文提出了一种基于投票的在线依从模型聚合方法，可以根据过去表现调整模型权重。

    

    依从预测为机器学习模型提供了一种合理的不确定性量化概念，而不需要做出强烈的分布假设。它适用于任何黑盒预测模型，并将点预测转换成具有预定义边际覆盖保证的集预测。然而，依从预测只在事先确定底层机器学习模型的情况下起作用。依从预测中相对较少涉及的问题是模型选择和/或聚合：对于给定的问题，应该如何依从化众多预测方法（随机森林、神经网络、正则化线性模型等）？本文提出了一种新的依从模型聚合方法，用于在线设置，该方法基于将来自多个算法的预测集进行投票，其中根据过去表现调整模型上的权重。

    arXiv:2403.15527v1 Announce Type: cross  Abstract: Conformal prediction equips machine learning models with a reasonable notion of uncertainty quantification without making strong distributional assumptions. It wraps around any black-box prediction model and converts point predictions into set predictions that have a predefined marginal coverage guarantee. However, conformal prediction only works if we fix the underlying machine learning model in advance. A relatively unaddressed issue in conformal prediction is that of model selection and/or aggregation: for a given problem, which of the plethora of prediction methods (random forests, neural nets, regularized linear models, etc.) should we conformalize? This paper proposes a new approach towards conformal model aggregation in online settings that is based on combining the prediction sets from several algorithms by voting, where weights on the models are adapted over time based on past performance.
    
[^182]: 资源高效图像恢复的潜在神经元细胞自动机

    Latent Neural Cellular Automata for Resource-Efficient Image Restoration

    [https://arxiv.org/abs/2403.15525](https://arxiv.org/abs/2403.15525)

    这项工作介绍了一种名为Latent Neural Cellular Automata (LNCA)的新型架构，旨在解决神经元细胞自动机的资源限制，通过将计算从传统输入空间转移到特别设计的潜在空间，以此实现对图像恢复任务的改进。

    

    神经元细胞自动机代表了传统细胞自动机模型的演变，通过整合基于深度学习的转换函数而得以增强。这种从手动到数据驱动的转变显著增加了这些模型的适应性，使其能够应用于包括内容生成和人工生命在内的各种领域。然而，它们广泛应用受到了显著的计算要求的阻碍。在本工作中，我们引入了潜在神经元细胞自动机（LNCA）模型，这是一种旨在解决神经元细胞自动机资源限制的新型架构。我们的方法将计算从传统输入空间转移到一个经过专门设计的潜在空间，依赖于一个预训练的自动编码器。我们将我们的模型应用于图像恢复的背景下，旨在从其退化版本重建高质量图像。这种修改不仅改善...

    arXiv:2403.15525v1 Announce Type: cross  Abstract: Neural cellular automata represent an evolution of the traditional cellular automata model, enhanced by the integration of a deep learning-based transition function. This shift from a manual to a data-driven approach significantly increases the adaptability of these models, enabling their application in diverse domains, including content generation and artificial life. However, their widespread application has been hampered by significant computational requirements. In this work, we introduce the Latent Neural Cellular Automata (LNCA) model, a novel architecture designed to address the resource limitations of neural cellular automata. Our approach shifts the computation from the conventional input space to a specially designed latent space, relying on a pre-trained autoencoder. We apply our model in the context of image restoration, which aims to reconstruct high-quality images from their degraded versions. This modification not only r
    
[^183]: PPA-Game：表征和学习在线内容创作者之间的竞争动态

    PPA-Game: Characterizing and Learning Competitive Dynamics Among Online Content Creators

    [https://arxiv.org/abs/2403.15524](https://arxiv.org/abs/2403.15524)

    引入了PPA-Game模型来表征类似YouTube和TikTok平台上的内容创作者之间竞争动态，分析显示纯纳什均衡在大多数情况下是常见的，提出了一种在线算法用于最大化每个代理者的累积收益。

    

    我们引入了比例性收益分配游戏（PPA-Game）来模拟代理者如何竞争可分配资源和消费者的注意力，类似于YouTube和TikTok等平台上的内容创作者。根据异质权重为代理者分配收益，反映了创作者之间内容质量的多样性。我们的分析表明，纯纳什均衡（PNE）并不在每种情况下都有保证，但在我们的模拟中，通常会观察到，其缺乏情况是罕见的。除了分析静态收益外，我们进一步讨论了代理者关于资源收益的在线学习，将多玩家多臂老虎机框架整合在一起。我们提出了一种在线算法，在$T$轮中促进每个代理者累积收益的最大化。从理论上讲，我们建立了任何代理者的遗憾在任何$\eta > 0$下都受到$O(\log^{1 + \eta} T)$的限制。经验结果进一步验证了我们的算法的有效性。

    arXiv:2403.15524v1 Announce Type: cross  Abstract: We introduce the Proportional Payoff Allocation Game (PPA-Game) to model how agents, akin to content creators on platforms like YouTube and TikTok, compete for divisible resources and consumers' attention. Payoffs are allocated to agents based on heterogeneous weights, reflecting the diversity in content quality among creators. Our analysis reveals that although a pure Nash equilibrium (PNE) is not guaranteed in every scenario, it is commonly observed, with its absence being rare in our simulations. Beyond analyzing static payoffs, we further discuss the agents' online learning about resource payoffs by integrating a multi-player multi-armed bandit framework. We propose an online algorithm facilitating each agent's maximization of cumulative payoffs over $T$ rounds. Theoretically, we establish that the regret of any agent is bounded by $O(\log^{1 + \eta} T)$ for any $\eta > 0$. Empirical results further validate the effectiveness of ou
    
[^184]: 采用噪声标记的听觉注意力解码研究：一项试点研究

    Towards auditory attention decoding with noise-tagging: A pilot study

    [https://arxiv.org/abs/2403.15523](https://arxiv.org/abs/2403.15523)

    这项试点研究首次尝试使用噪声标记刺激协议进行听觉注意力解码，取得了较高的性能表现。

    

    听觉注意力解码(AAD)旨在从大脑活动中提取被关注的说话者，提供了神经导向听觉设备和脑机接口等领域的应用前景。本试点研究首次尝试使用噪声标记刺激协议进行AAD，该协议引发了可靠的编码调制诱发电位，但在听觉模式下的探索还很有限。研究参与者依次呈现两个荷兰语言语音刺激，这些刺激被幅度调制为具有唯一二进制伪随机噪声码，有效地为其标记了附加可解码信息。我们比较了未调制音频与使用不同调制深度调制的音频的解码，以及传统AAD方法与标准解码噪声码方法的对比。我们的试点研究发现，与未调制音频相比，70至100%的调制深度的传统方法表现出更高的性能。

    arXiv:2403.15523v1 Announce Type: cross  Abstract: Auditory attention decoding (AAD) aims to extract from brain activity the attended speaker amidst candidate speakers, offering promising applications for neuro-steered hearing devices and brain-computer interfacing. This pilot study makes a first step towards AAD using the noise-tagging stimulus protocol, which evokes reliable code-modulated evoked potentials, but is minimally explored in the auditory modality. Participants were sequentially presented with two Dutch speech stimuli that were amplitude modulated with a unique binary pseudo-random noise-code, effectively tagging these with additional decodable information. We compared the decoding of unmodulated audio against audio modulated with various modulation depths, and a conventional AAD method against a standard method to decode noise-codes. Our pilot study revealed higher performances for the conventional method with 70 to 100 percent modulation depths compared to unmodulated au
    
[^185]: 学习在新领域行走：面向c-VEP BCI的无校准解码方法

    Learning to walk on new ground: Calibration-free decoding for c-VEP BCI

    [https://arxiv.org/abs/2403.15521](https://arxiv.org/abs/2403.15521)

    该研究介绍了一种基于事件相关电位的新型方法（UMM），与目前先进的c-VEP零训练方法（CCA）进行比较，证明了它们在无需校准的BCI系统中的有效性。

    

    本研究探讨了两种零训练方法，旨在增强脑机接口（BCI）的可用性，消除了校准会话的需求。我们介绍了一种根植于事件相关电位（ERP）领域的新颖方法，即无监督均值最大化（UMM），用于快速编码调制视觉诱发电位（c-VEP）刺激协议。我们将UMM与使用典型相关分析（CCA）的c-VEP零训练方法进行比较。比较包括对CCA和UMM的即时分类以及对先前分类试验进行累积学习的分类。我们的研究显示了两种方法在处理c-VEP数据集的复杂性方面的有效性，突出了它们之间的差异和独特优势。这项研究不仅为无校准BCI方法的实际实施提供了见解，还为进一步的探索和改进铺平了道路。

    arXiv:2403.15521v1 Announce Type: cross  Abstract: This study explores two zero-training methods aimed at enhancing the usability of brain-computer interfaces (BCIs) by eliminating the need for a calibration session. We introduce a novel method rooted in the event-related potential (ERP) domain, unsupervised mean maximization (UMM), to the fast code-modulated visual evoked potential (c-VEP) stimulus protocol. We compare UMM to the state-of-the-art c-VEP zero-training method that uses canonical correlation analysis (CCA). The comparison includes instantaneous classification and classification with cumulative learning from previously classified trials for both CCA and UMM. Our study shows the effectiveness of both methods in navigating the complexities of a c-VEP dataset, highlighting their differences and distinct strengths. This research not only provides insights into the practical implementation of calibration-free BCI methods but also paves the way for further exploration and refine
    
[^186]: GTC：GNN-Transformer自监督异构图表示的共轭对比学习

    GTC: GNN-Transformer Co-contrastive Learning for Self-supervised Heterogeneous Graph Representation

    [https://arxiv.org/abs/2403.15520](https://arxiv.org/abs/2403.15520)

    该论文提出了一个用于GNN和Transformer的合作学习方案，并构建了GTC架构，通过整合GNN的本地信息聚合和Transformer的全局信息建模能力，解决了过度平滑问题。

    

    图神经网络（GNN）由于具有传递信息的机制，能够很好地聚合本地信息，在各种图任务中已经成为最强大的工具。然而，过度平滑一直阻碍着GNN进一步深入和捕获多跳邻居。与GNN不同，Transformer可以通过多头自注意力和适当的Transformer结构来建模全局信息和多跳交互，并且能够更好地解决过度平滑问题。因此，我们是否可以提出一个新颖的框架，将GNN和Transformer结合起来，整合GNN的本地信息聚合和Transformer的全局信息建模能力，以消除过度平滑问题？为了实现这一点，本文提出了一个用于GNN-Transformer的协同学习方案，并构建了GTC架构。GTC利用GNN和Transformer分支分别对来自不同视图的节点信息进行编码，并建立了对比

    arXiv:2403.15520v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have emerged as the most powerful weapon for various graph tasks due to the message-passing mechanism's great local information aggregation ability. However, over-smoothing has always hindered GNNs from going deeper and capturing multi-hop neighbors. Unlike GNNs, Transformers can model global information and multi-hop interactions via multi-head self-attention and a proper Transformer structure can show more immunity to the over-smoothing problem. So, can we propose a novel framework to combine GNN and Transformer, integrating both GNN's local information aggregation and Transformer's global information modeling ability to eliminate the over-smoothing problem? To realize this, this paper proposes a collaborative learning scheme for GNN-Transformer and constructs GTC architecture. GTC leverages the GNN and Transformer branch to encode node information from different views respectively, and establishes contrast
    
[^187]: 通过增加表示秩和特征丰富性来改善类增量学习中的前向兼容性

    Improving Forward Compatibility in Class Incremental Learning by Increasing Representation Rank and Feature Richness

    [https://arxiv.org/abs/2403.15517](https://arxiv.org/abs/2403.15517)

    该研究引入了一种基于有效秩的特征丰富性增强（RFR）方法，通过增加表示的有效秩，实现了提高前向兼容性的目标。同时，在后向兼容性和前向兼容性方面取得了双重目标。

    

    类增量学习（CIL）是连续学习中的一个重要子领域，旨在使模型能够在保留先前任务知识的同时逐渐学习新的分类任务。本研究引入了一种基于有效秩的特征丰富性增强（RFR）方法，旨在提高前向兼容性。具体而言，该方法通过增加基础阶段表示的有效秩，从而有助于合并更多与未见新任务相关的信息特征。因此，RFR在后向兼容性和前向兼容性方面实现了双重目标：最大限度地减小特征提取器的影响。

    arXiv:2403.15517v1 Announce Type: new  Abstract: Class Incremental Learning (CIL) constitutes a pivotal subfield within continual learning, aimed at enabling models to progressively learn new classification tasks while retaining knowledge obtained from prior tasks. Although previous studies have predominantly focused on backward compatible approaches to mitigate catastrophic forgetting, recent investigations have introduced forward compatible methods to enhance performance on novel tasks and complement existing backward compatible methods. In this study, we introduce an effective-Rank based Feature Richness enhancement (RFR) method, designed for improving forward compatibility. Specifically, this method increases the effective rank of representations during the base session, thereby facilitating the incorporation of more informative features pertinent to unseen novel tasks. Consequently, RFR achieves dual objectives in backward and forward compatibility: minimizing feature extractor mo
    
[^188]: 通过决策边界感知数据增强在低资源环境中提高有效性和稳健性

    Enhancing Effectiveness and Robustness in a Low-Resource Regime via Decision-Boundary-aware Data Augmentation

    [https://arxiv.org/abs/2403.15512](https://arxiv.org/abs/2403.15512)

    本文提出了一种决策边界感知的数据增强策略，通过移动潜在特征、重构生成模糊版本以及采用中K采样来增强生成句子的多样性，从而比较于其他方法提高了在低资源环境中的有效性和稳健性。

    

    在低资源环境中利用深度学习模型需要进行数据增强，但直接方法（如mixup和cutout）在文本数据上的应用受限于其离散特性。本文受到决策边界的最新研究启发，提出了一种决策边界感知的数据增强策略，利用预训练的语言模型来增强稳健性。该技术首先专注于将潜在特征移近决策边界，然后进行重构以生成一个带有软标签的模糊版本。此外，建议使用中K采样来增强生成句子的多样性。

    arXiv:2403.15512v1 Announce Type: cross  Abstract: Efforts to leverage deep learning models in low-resource regimes have led to numerous augmentation studies. However, the direct application of methods such as mixup and cutout to text data, is limited due to their discrete characteristics. While methods using pretrained language models have exhibited efficiency, they require additional considerations for robustness. Inspired by recent studies on decision boundaries, this paper proposes a decision-boundary-aware data augmentation strategy to enhance robustness using pretrained language models. The proposed technique first focuses on shifting the latent features closer to the decision boundary, followed by reconstruction to generate an ambiguous version with a soft label. Additionally, mid-K sampling is suggested to enhance the diversity of the generated sentences. This paper demonstrates the performance of the proposed augmentation strategy compared to other methods through extensive ex
    
[^189]: IoT入侵检测系统中的多输入自动编码器引导特征选择

    Multiple-Input Auto-Encoder Guided Feature Selection for IoT Intrusion Detection Systems

    [https://arxiv.org/abs/2403.15511](https://arxiv.org/abs/2403.15511)

    该论文提出了一种名为多输入自动编码器(MIAE)的新型神经网络架构，通过训练MIAE模型，在无监督学习模式下将异构输入转换为较低维表示，有助于分类器区分正常行为和不同类型的攻击。

    

    入侵检测系统(IDSs)受益于IoT数据特征的多样性和泛化，数据的多样性使得在IoT IDSs中训练有效的机器学习模型变得困难。本文首先介绍了一种名为多输入自动编码器(MIAE)的新型神经网络架构。MIAE由多个子编码器组成，可以处理具有不同特征的不同来源的输入。 MIAE模型以无监督学习模式进行训练，将异构输入转换为较低维表示，有助于分类器区分正常行为和不同类型的攻击。

    arXiv:2403.15511v1 Announce Type: cross  Abstract: While intrusion detection systems (IDSs) benefit from the diversity and generalization of IoT data features, the data diversity (e.g., the heterogeneity and high dimensions of data) also makes it difficult to train effective machine learning models in IoT IDSs. This also leads to potentially redundant/noisy features that may decrease the accuracy of the detection engine in IDSs. This paper first introduces a novel neural network architecture called Multiple-Input Auto-Encoder (MIAE). MIAE consists of multiple sub-encoders that can process inputs from different sources with different characteristics. The MIAE model is trained in an unsupervised learning mode to transform the heterogeneous inputs into lower-dimensional representation, which helps classifiers distinguish between normal behaviour and different types of attacks. To distil and retain more relevant features but remove less important/redundant ones during the training process,
    
[^190]: 隐私保护的端到端口语理解

    Privacy-Preserving End-to-End Spoken Language Understanding

    [https://arxiv.org/abs/2403.15510](https://arxiv.org/abs/2403.15510)

    本文提出了一种新颖的SLU多任务隐私保护模型，防止语音识别和身份识别攻击。

    

    语音语言理解（SLU）是物联网设备中人机交互的关键技术之一，提供了一个易于使用的用户界面。人类的言语可能包含大量用户敏感信息，如性别、身份和敏感内容。因此出现了新型安全和隐私侵犯。用户不希望将个人敏感信息暴露给不可信任的第三方的恶意攻击。因此，SLU系统需要确保潜在的恶意攻击者无法推断出用户的敏感属性，同时又应避免大大损害SLU的准确性。为了解决上述挑战，本文提出了一种全新的SLU多任务隐私保护模型，以防止语音识别（ASR）和身份识别（IR）攻击。该模型使用隐藏层分离技术，使SLU信息仅分布在隐藏层的特定部分。

    arXiv:2403.15510v1 Announce Type: cross  Abstract: Spoken language understanding (SLU), one of the key enabling technologies for human-computer interaction in IoT devices, provides an easy-to-use user interface. Human speech can contain a lot of user-sensitive information, such as gender, identity, and sensitive content. New types of security and privacy breaches have thus emerged. Users do not want to expose their personal sensitive information to malicious attacks by untrusted third parties. Thus, the SLU system needs to ensure that a potential malicious attacker cannot deduce the sensitive attributes of the users, while it should avoid greatly compromising the SLU accuracy. To address the above challenge, this paper proposes a novel SLU multi-task privacy-preserving model to prevent both the speech recognition (ASR) and identity recognition (IR) attacks. The model uses the hidden layer separation technique so that SLU information is distributed only in a specific portion of the hidd
    
[^191]: 双自动编码器模型用于学习网络攻击检测中的可分离表示

    Twin Auto-Encoder Model for Learning Separable Representation in Cyberattack Detection

    [https://arxiv.org/abs/2403.15509](https://arxiv.org/abs/2403.15509)

    提出一种新型双自动编码器模型(TAE)，通过将潜在表示转换为可分离表示来解决网络攻击检测中混合表示的问题

    

    表征学习在网络攻击检测等许多问题的成功中起着关键作用。大多数网络攻击检测的表征学习方法基于自动编码器（AE）模型的潜在向量。为了解决AEs表示中混合的问题，我们提出了一种称为双自动编码器（TAE）的新型模型。TAE将潜在表示确定地转换为更易区分的表示，即\textit{可分离表示}，并在输出端重建可分离表示。

    arXiv:2403.15509v1 Announce Type: cross  Abstract: Representation Learning (RL) plays a pivotal role in the success of many problems including cyberattack detection. Most of the RL methods for cyberattack detection are based on the latent vector of Auto-Encoder (AE) models. An AE transforms raw data into a new latent representation that better exposes the underlying characteristics of the input data. Thus, it is very useful for identifying cyberattacks. However, due to the heterogeneity and sophistication of cyberattacks, the representation of AEs is often entangled/mixed resulting in the difficulty for downstream attack detection models. To tackle this problem, we propose a novel mod called Twin Auto-Encoder (TAE). TAE deterministically transforms the latent representation into a more distinguishable representation namely the \textit{separable representation} and the reconstructsuct the separable representation at the output. The output of TAE called the \textit{reconstruction represe
    
[^192]: 在线文字自动完成的顺序决策

    Sequential Decision-Making for Inline Text Autocomplete

    [https://arxiv.org/abs/2403.15502](https://arxiv.org/abs/2403.15502)

    通过强化学习和顺序决策制定改进了文本输入系统中的在线自动完成建议，将认知负荷纳入模型训练目标，基于文本输入速度的奖励函数。

    

    自动完成建议是现代文本输入系统中的基本功能，应用于诸如消息传递和电子邮件撰写等领域。通常，自动完成建议是从具有置信度阈值的语言模型生成的。然而，这个阈值并没有直接考虑用户因显示建议而施加的认知负荷，例如从输入切换到阅读建议的上下文以及决定是否接受建议的时间。在本文中，我们研究了通过顺序决策制定来改进文本输入系统中的在线自动完成建议问题，并使用强化学习通过随时间与目标用户的重复交互来学习建议策略。这种制定允许我们将认知负荷因素纳入训练自动完成模型的目标中，通过基于文本输入速度的奖励函数。我们获得了理论方面的...

    arXiv:2403.15502v1 Announce Type: new  Abstract: Autocomplete suggestions are fundamental to modern text entry systems, with applications in domains such as messaging and email composition. Typically, autocomplete suggestions are generated from a language model with a confidence threshold. However, this threshold does not directly take into account the cognitive load imposed on the user by surfacing suggestions, such as the effort to switch contexts from typing to reading the suggestion, and the time to decide whether to accept the suggestion. In this paper, we study the problem of improving inline autocomplete suggestions in text entry systems via a sequential decision-making formulation, and use reinforcement learning to learn suggestion policies through repeated interactions with a target user over time. This formulation allows us to factor cognitive load into the objective of training an autocomplete model, through a reward function based on text entry speed. We acquired theoretica
    
[^193]: 在缺失值存在的情况下进行基因调控网络推断：一个因果视图

    Gene Regulatory Network Inference in the Presence of Dropouts: a Causal View

    [https://arxiv.org/abs/2403.15500](https://arxiv.org/abs/2403.15500)

    引入了因果丢失模型来解决基因调控网络推断中因丢失值引起的偏倚问题。

    

    基因调控网络推断（GRNI）是一个具有挑战性的问题，尤其是由于单细胞RNA测序数据中存在零值：一些是代表没有基因表达的生物零值，而其他一些是由于测序过程（也称为丢失值）导致的技术性零值，这可能通过扭曲测定基因表达的联合分布而偏倚GRNI。现有方法通常通过插补处理丢失值，这可能会引入伪关系，因为真实的联合分布通常无法识别。为了解决这个问题，我们引入了一个因果图模型来表征丢失机制，即因果丢失模型。

    arXiv:2403.15500v1 Announce Type: cross  Abstract: Gene regulatory network inference (GRNI) is a challenging problem, particularly owing to the presence of zeros in single-cell RNA sequencing data: some are biological zeros representing no gene expression, while some others are technical zeros arising from the sequencing procedure (aka dropouts), which may bias GRNI by distorting the joint distribution of the measured gene expressions. Existing approaches typically handle dropout error via imputation, which may introduce spurious relations as the true joint distribution is generally unidentifiable. To tackle this issue, we introduce a causal graphical model to characterize the dropout mechanism, namely, Causal Dropout Model. We provide a simple yet effective theoretical result: interestingly, the conditional independence (CI) relations in the data with dropouts, after deleting the samples with zero values (regardless if technical or not) for the conditioned variables, are asymptoticall
    
[^194]: 通过机器学习驱动的元学习器对电力市场中CO2减排策略进行因果分析

    A Causal Analysis of CO2 Reduction Strategies in Electricity Markets Through Machine Learning-Driven Metalearners

    [https://arxiv.org/abs/2403.15499](https://arxiv.org/abs/2403.15499)

    通过Causal Machine Learning方法分析电力市场中定价政策对CO2水平的影响，挑战传统智慧，发现可能增加CO2强度，并结合机器学习元算法增强研究深度，并提供宝贵见解。

    

    本研究采用因果机器学习（CausalML）统计方法，分析电力定价政策对家庭部门二氧化碳（CO2）水平的影响。研究调查潜在结果与处理效果之间的因果关系，其中定价政策的变化是处理效果，我们的分析挑战了围绕基于激励的电力定价的传统智慧。研究结果表明，采用这些政策可能会无意中增加CO2强度。此外，我们整合了基于机器学习的元算法，反映了当代统计方法，以增强我们的因果分析深度。该研究对学习者X、T、S和R进行了比较分析，以确定基于规定问题的具体目标和背景细微之处的最佳方法。这项研究为可持续发展问题上的持续对话提供了宝贵的见解。

    arXiv:2403.15499v1 Announce Type: cross  Abstract: This study employs the Causal Machine Learning (CausalML) statistical method to analyze the influence of electricity pricing policies on carbon dioxide (CO2) levels in the household sector. Investigating the causality between potential outcomes and treatment effects, where changes in pricing policies are the treatment, our analysis challenges the conventional wisdom surrounding incentive-based electricity pricing. The study's findings suggest that adopting such policies may inadvertently increase CO2 intensity. Additionally, we integrate a machine learning-based meta-algorithm, reflecting a contemporary statistical approach, to enhance the depth of our causal analysis. The study conducts a comparative analysis of learners X, T, S, and R to ascertain the optimal methods based on the defined question's specified goals and contextual nuances. This research contributes valuable insights to the ongoing dialogue on sustainable development pr
    
[^195]: 棋类语言模型中的新颖世界模型和潜变量估计

    Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models

    [https://arxiv.org/abs/2403.15498](https://arxiv.org/abs/2403.15498)

    棋类语言模型在没有先验知识的情况下，通过下一个字符预测训练，仍能学习出内部表示的棋盘状态

    

    语言模型展现了前所未有的能力，引发了关于其性能来源的讨论。是仅仅学习句法模式和表面统计结果，还是从文本中提取语义和世界模型？我们在象棋这个更复杂的领域扩展了之前的工作，通过在真实游戏中训练模型，使用线性探测和对比激活来研究模型的内部表示。尽管模型没有先验的游戏知识，仅仅通过下一个字符预测进行训练，我们发现了关于棋盘状态的内部表示的证据。

    arXiv:2403.15498v1 Announce Type: cross  Abstract: Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model's internal representations using linear probes and contrastive activations. The model is given no a priori knowledge of the game and is solely trained on next character prediction, yet we find evidence of internal representations of board state. We validate these internal representations by using them to make interventions on the model's activations and edit its internal board state. Un
    
[^196]: 通过统计技术在视觉模型中检测异常或超出分布的数据

    On the Detection of Anomalous or Out-Of-Distribution Data in Vision Models Using Statistical Techniques

    [https://arxiv.org/abs/2403.15497](https://arxiv.org/abs/2403.15497)

    本文评估了Benford's law作为一种检测视觉模型中异常或超出分布数据的方法，探讨了其在过滤异常数据点和标记超出分布数据方面的潜在应用。

    

    超出分布的数据和异常输入是当今机器学习系统的漏洞，经常导致系统进行错误预测。这些模型使用的数据范围广泛，因此检测非典型输入是一项困难且重要的任务。我们评估了本福特定律作为一种用于量化真实和受损输入之间差异的方法。我们认为，在许多情境下，它可以作为异常数据点的过滤器，标志着超出分布的数据。我们希望就这些应用和这种技术尚未被充分探索的领域进行讨论。

    arXiv:2403.15497v1 Announce Type: cross  Abstract: Out-of-distribution data and anomalous inputs are vulnerabilities of machine learning systems today, often causing systems to make incorrect predictions. The diverse range of data on which these models are used makes detecting atypical inputs a difficult and important task. We assess a tool, Benford's law, as a method used to quantify the difference between real and corrupted inputs. We believe that in many settings, it could function as a filter for anomalous data points and for signalling out-of-distribution data. We hope to open a discussion on these applications and further areas where this technique is underexplored.
    
[^197]: 多传感器和无陀螺惯导数据集

    Multiple and Gyro-Free Inertial Datasets

    [https://arxiv.org/abs/2403.15494](https://arxiv.org/abs/2403.15494)

    设计并记录了无陀螺惯导系统（GFINS）和多惯性测量单元（MIMU）数据集，以填补该领域的研究空白。

    

    一个惯性导航系统（INS）利用三个正交加速度计和陀螺仪来确定平台的位置、速度和方向。INS有无数的应用，包括机器人技术、自主平台和物联网。最近的研究探讨了数据驱动方法与INS的整合，突出了重大的创新，提高了准确性和效率。尽管这一领域的兴趣不断增长，INS数据集也不断增加，但对于无陀螺惯导系统（GFINS）和多惯性测量单元（MIMU）架构却没有可用的数据集。为了填补这一空白并激发该领域的进一步研究，我们使用了54个惯性传感器分组成九个惯性测量单元，设计并记录了GFINS和MIMU数据集。这些传感器可用于定义和评估不同类型的MIMU和GFINS架构。

    arXiv:2403.15494v1 Announce Type: cross  Abstract: An inertial navigation system (INS) utilizes three orthogonal accelerometers and gyroscopes to determine platform position, velocity, and orientation. There are countless applications for INS, including robotics, autonomous platforms, and the internet of things. Recent research explores the integration of data-driven methods with INS, highlighting significant innovations, improving accuracy and efficiency. Despite the growing interest in this field and the availability of INS datasets, no datasets are available for gyro-free INS (GFINS) and multiple inertial measurement unit (MIMU) architectures. To fill this gap and to stimulate further research in this field, we designed and recorded GFINS and MIMU datasets using 54 inertial sensors grouped in nine inertial measurement units. These sensors can be used to define and evaluate different types of MIMU and GFINS architectures. The inertial sensors were arranged in three different sensor c
    
[^198]: 使用有条件识别信息的脑电图解码

    EEG decoding with conditional identification information

    [https://arxiv.org/abs/2403.15489](https://arxiv.org/abs/2403.15489)

    通过将个体的有条件识别信息整合到神经网络中，这项研究提出了一种新的方法来增强模型表示，从而改善脑电图信号的解码准确性。

    

    脑电图信号的解码对于揭示人类大脑并推动脑机接口至关重要。传统的机器学习算法受到脑电图信号中高噪音水平和个体间固有变化的阻碍。最近深度神经网络（DNNs）的进展显示出潜力，归功于其先进的非线性建模能力。然而，DNN在解码未见个体的脑电图样本方面仍面临挑战。为了解决这一问题，本文引入了一种新颖的方法，通过将每个个体的有条件识别信息纳入神经网络，从而通过脑电图和个人特征的协同作用增强模型表示。我们在WithMe数据集上测试了我们的模型，并证明包含这些标识符显著提高了训练集中的主体和未见主体的准确性。这种增强显示了改进脑电图解码的潜力。

    arXiv:2403.15489v1 Announce Type: cross  Abstract: Decoding EEG signals is crucial for unraveling human brain and advancing brain-computer interfaces. Traditional machine learning algorithms have been hindered by the high noise levels and inherent inter-person variations in EEG signals. Recent advances in deep neural networks (DNNs) have shown promise, owing to their advanced nonlinear modeling capabilities. However, DNN still faces challenge in decoding EEG samples of unseen individuals. To address this, this paper introduces a novel approach by incorporating the conditional identification information of each individual into the neural network, thereby enhancing model representation through the synergistic interaction of EEG and personal traits. We test our model on the WithMe dataset and demonstrated that the inclusion of these identifiers substantially boosts accuracy for both subjects in the training set and unseen subjects. This enhancement suggests promising potential for improvi
    
[^199]: MOGAM：一种用于抑郁症检测的多模态面向对象图注意模型

    MOGAM: A Multimodal Object-oriented Graph Attention Model for Depression Detection

    [https://arxiv.org/abs/2403.15485](https://arxiv.org/abs/2403.15485)

    MOGAM模型是为了克服现有抑郁症检测方法在不同社交媒体数据类型上的限制而提出，通过多模态数据的综合应用，实现了更具可扩展性和多功能性的解决方案。

    

    早期检测在抑郁症治疗中起着至关重要的作用。因此，许多研究关注社交媒体平台，个体在该平台表达情绪，旨在实现抑郁症的早期检测。然而，现有方法主要依赖特定特征，导致在不同类型的社交媒体数据集（如文本、图像或视频）上的可扩展性有限。为克服这一限制，我们引入了一种多模态面向对象图注意模型（MOGAM），可应用于各种数据类型，提供更具可伸缩性和多功能性的解决方案。此外，为确保我们的模型能够捕捉抑郁症的真实症状，我们仅收集了具有临床诊断的用户的视频日志。为了利用视频日志的多样特征，我们采用多模态方法，并收集额外的元数据，如视频日志的标题、描述和持续时间。为了有效地聚合

    arXiv:2403.15485v1 Announce Type: cross  Abstract: Early detection plays a crucial role in the treatment of depression. Therefore, numerous studies have focused on social media platforms, where individuals express their emotions, aiming to achieve early detection of depression. However, the majority of existing approaches often rely on specific features, leading to limited scalability across different types of social media datasets, such as text, images, or videos. To overcome this limitation, we introduce a Multimodal Object-Oriented Graph Attention Model (MOGAM), which can be applied to diverse types of data, offering a more scalable and versatile solution. Furthermore, to ensure that our model can capture authentic symptoms of depression, we only include vlogs from users with a clinical diagnosis. To leverage the diverse features of vlogs, we adopt a multimodal approach and collect additional metadata such as the title, description, and duration of the vlogs. To effectively aggregat
    
[^200]: RakutenAI-7B：为日本语言扩展大型语言模型

    RakutenAI-7B: Extending Large Language Models for Japanese

    [https://arxiv.org/abs/2403.15484](https://arxiv.org/abs/2403.15484)

    RakutenAI-7B是一套日本导向的大型语言模型，在日本LM Harness基准测试中表现最好，分别发布了指导和聊天调整的模型。

    

    我们介绍了RakutenAI-7B，这是一套以日本为导向的大型语言模型，在日本LM Harness基准测试中取得了最佳性能，优于开放的7B模型。除了基础模型外，我们还发布了根据指导和聊天进行调整的模型，分别是RakutenAI-7B-instruct和RakutenAI-7B-chat，使用Apache 2.0许可发布。

    arXiv:2403.15484v1 Announce Type: new  Abstract: We introduce RakutenAI-7B, a suite of Japanese-oriented large language models that achieve the best performance on the Japanese LM Harness benchmarks among the open 7B models. Along with the foundation model, we release instruction- and chat-tuned models, RakutenAI-7B-instruct and RakutenAI-7B-chat respectively, under the Apache 2.0 license.
    
[^201]: 基于生成对抗增强的多尺度卷积神经网络模型的滚动轴承故障诊断方法

    Rolling bearing fault diagnosis method based on generative adversarial enhanced multi-scale convolutional neural network model

    [https://arxiv.org/abs/2403.15483](https://arxiv.org/abs/2403.15483)

    提出了一种基于生成对抗增强的多尺度卷积神经网络模型的滚动轴承故障诊断方法，使用Gram角场编码技术和梯度惩罚Wasserstein距离生成对抗网络来扩展原始训练集。

    

    为了解决当前卷积神经网络无法有效捕捉滚动轴承时域信号之间相关特征的问题，以及模型精度受样本数量和质量限制的问题，提出了一种基于生成对抗增强的多尺度卷积神经网络模型的滚动轴承故障诊断方法。首先，采用Gram角场编码技术对滚动轴承的时域信号进行编码，并生成特征图以保留振动信号的完整信息。然后将生成的数据分为训练集、验证集和测试集。其中，训练集输入梯度惩罚Wasserstein距离生成对抗网络进行训练，得到具有类似特征的新样本，从而扩展原始训练集。

    arXiv:2403.15483v1 Announce Type: cross  Abstract: In order to solve the problem that current convolutional neural networks can not capture the correlation features between the time domain signals of rolling bearings effectively, and the model accuracy is limited by the number and quality of samples, a rolling bearing fault diagnosis method based on generative adversarial enhanced multi-scale convolutional neural network model is proposed. Firstly, Gram angular field coding technique is used to encode the time domain signal of the rolling bearing and generate the feature map to retain the complete information of the vibration signal. Then, the re-sulting data is divided into a training set, a validation set, and a test set. Among them, the training set is input into the gradient penalty Wasserstein distance generation adversarial network to complete the training, and a new sample with similar features to the training sample is obtained, and then the original training set is expanded. N
    
[^202]: 利用大型语言模型生成多级反馈，为初学者同行辅导员赋能

    Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors

    [https://arxiv.org/abs/2403.15482](https://arxiv.org/abs/2403.15482)

    利用大型语言模型为初学者同行辅导员提供多级详细反馈，赋能规模化的支持心理健康问题患者。

    

    实践和个性化反馈是培养具有临床技能的同行辅导员的关键过程。然而，现有的反馈机制主要依赖于人工监督。同行辅导员经常缺乏机制来从经验丰富的导师那里获得详细的反馈，这使得他们难以支持使用同行辅导的大量心理健康问题患者。我们的工作旨在利用大型语言模型提供情境化和多级反馈，以赋能规模化的初学者同行辅导员。为实现这一目标，我们与一组高级心理治疗督导共同设计了一个多级反馈分类法，然后构建了一个具有完整反馈注释的400次情绪支持对话的公开可用数据集。我们进一步设计了一种基于大型语言模型的自我改进方法，以增强反馈的自动生成。

    arXiv:2403.15482v1 Announce Type: new  Abstract: Realistic practice and tailored feedback are key processes for training peer counselors with clinical skills. However, existing mechanisms of providing feedback largely rely on human supervision. Peer counselors often lack mechanisms to receive detailed feedback from experienced mentors, making it difficult for them to support the large number of people with mental health issues who use peer counseling. Our work aims to leverage large language models to provide contextualized and multi-level feedback to empower peer counselors, especially novices, at scale. To achieve this, we co-design with a group of senior psychotherapy supervisors to develop a multi-level feedback taxonomy, and then construct a publicly available dataset with comprehensive feedback annotations of 400 emotional support conversations. We further design a self-improvement method on top of large language models to enhance the automatic generation of feedback. Via qualita
    
[^203]: SpikeGraphormer：一种具有脉冲图注意力的高性能图变换器

    SpikeGraphormer: A High-Performance Graph Transformer with Spiking Graph Attention

    [https://arxiv.org/abs/2403.15480](https://arxiv.org/abs/2403.15480)

    本研究将脉冲神经网络与图变换器集成，设计了脉冲图注意力模块，通过稀疏加法和掩码操作取代矩阵乘法，实现了线性复杂度，从而在大规模图上实现了所有节点之间的交互。

    

    最近，图变换器已经成为一种有希望的解决方案，能够缓解图神经网络（GNN）的固有限制并增强图表示性能。不幸的是，由于自我注意力在大规模图上的二次复杂性，特别是对于节点任务，图变换器在计算上是昂贵的。相比之下，脉冲神经网络（SNN）具有事件驱动和二进制脉冲属性，可以进行高效的能量计算。在这项工作中，我们对将SNN与图变换器进行集成提出了一种新的见解，并设计了一个脉冲图注意力（SGA）模块。矩阵乘法被稀疏加法和掩码操作所取代。线性复杂性使得可以在具有有限GPU内存的大规模图上进行所有节点间的交互。据我们所知，我们的工作是第一次尝试将SNN引入图变换器。此外，我们设计了SpikeGraph。

    arXiv:2403.15480v1 Announce Type: cross  Abstract: Recently, Graph Transformers have emerged as a promising solution to alleviate the inherent limitations of Graph Neural Networks (GNNs) and enhance graph representation performance. Unfortunately, Graph Transformers are computationally expensive due to the quadratic complexity inherent in self-attention when applied over large-scale graphs, especially for node tasks. In contrast, spiking neural networks (SNNs), with event-driven and binary spikes properties, can perform energy-efficient computation. In this work, we propose a novel insight into integrating SNNs with Graph Transformers and design a Spiking Graph Attention (SGA) module. The matrix multiplication is replaced by sparse addition and mask operations. The linear complexity enables all-pair node interactions on large-scale graphs with limited GPU memory. To our knowledge, our work is the first attempt to introduce SNNs into Graph Transformers. Furthermore, we design SpikeGraph
    
[^204]: 学习推断生成视觉概念的模板程序

    Learning to Infer Generative Template Programs for Visual Concepts

    [https://arxiv.org/abs/2403.15476](https://arxiv.org/abs/2403.15476)

    探索了一种学习如何推断捕捉视觉概念的通用模板程序的神经符号系统，引入了模板程序概念，支持多种概念相关任务，提出了一种学习范式来训练网络直接推断模板程序，实验证明该方法优于任务特定替代方法，并与特定领域方法竞争性地执行。

    

    人们可以从少量示例中灵活掌握视觉概念。我们探索了一种神经符号系统，学习如何以一种通用方式推断捕捉视觉概念的程序。我们引入了模板程序：来自特定领域语言的程序表达式，指定了输入概念中常见的结构和参数模式。我们的框架支持多个与概念相关的任务，包括通过解析进行少样本生成和共分割。我们开发了一种学习范式，允许我们训练网络直接从包含概念分组的视觉数据集中推断模板程序。我们在多个视觉领域进行实验：2D布局、Omniglot字符和3D形状。我们发现我们的方法胜过了任务特定的替代方法，并在有限领域竞争性地执行了针对特定领域的方法。

    arXiv:2403.15476v1 Announce Type: cross  Abstract: People grasp flexible visual concepts from a few examples. We explore a neurosymbolic system that learns how to infer programs that capture visual concepts in a domain-general fashion. We introduce Template Programs: programmatic expressions from a domain-specific language that specify structural and parametric patterns common to an input concept. Our framework supports multiple concept-related tasks, including few-shot generation and co-segmentation through parsing. We develop a learning paradigm that allows us to train networks that infer Template Programs directly from visual datasets that contain concept groupings. We run experiments across multiple visual domains: 2D layouts, Omniglot characters, and 3D shapes. We find that our method outperforms task-specific alternatives, and performs competitively against domain-specific approaches for the limited domains where they exist.
    
[^205]: EC-IoU: 通过自我中心交并联调整物体检测器的安全性

    EC-IoU: Orienting Safety for Object Detectors via Ego-Centric Intersection-over-Union

    [https://arxiv.org/abs/2403.15474](https://arxiv.org/abs/2403.15474)

    通过EC-IoU度量，本文引入了一种定向安全性物体检测方法，可以在安全关键领域中提高物体检测器的性能，并在KITTI数据集上取得了比IoU更好的结果。

    

    本文介绍了通过一种新颖的自我中心交并联（EC-IoU）度量来定向安全性物体检测，解决了在自动驾驶等安全关键领域应用最先进的基于学习的感知模型时面临的实际问题。具体来说，我们提出了一种加权机制来优化广泛使用的IoU度量，使其能够根据自我代理人的视角覆盖更近的地面真实对象点的预测分配更高的分数。所提出的EC-IoU度量可以用于典型的评估过程，选择有更高安全性表现的物体检测器用于下游任务。它还可以集成到常见损失函数中进行模型微调。尽管面向安全性，但我们在KITTI数据集上的实验表明，使用EC-IoU训练的模型在均值平均精度方面的性能可能会优于使用IoU训练的变体。

    arXiv:2403.15474v1 Announce Type: cross  Abstract: This paper presents safety-oriented object detection via a novel Ego-Centric Intersection-over-Union (EC-IoU) measure, addressing practical concerns when applying state-of-the-art learning-based perception models in safety-critical domains such as autonomous driving. Concretely, we propose a weighting mechanism to refine the widely used IoU measure, allowing it to assign a higher score to a prediction that covers closer points of a ground-truth object from the ego agent's perspective. The proposed EC-IoU measure can be used in typical evaluation processes to select object detectors with higher safety-related performance for downstream tasks. It can also be integrated into common loss functions for model fine-tuning. While geared towards safety, our experiment with the KITTI dataset demonstrates the performance of a model trained on EC-IoU can be better than that of a variant trained on IoU in terms of mean Average Precision as well.
    
[^206]: 使用音素计数比奖励驱动的强化学习实现等距神经机器翻译

    Isometric Neural Machine Translation using Phoneme Count Ratio Reward-based Reinforcement Learning

    [https://arxiv.org/abs/2403.15469](https://arxiv.org/abs/2403.15469)

    本论文提出了使用强化学习（RL）开发的等距NMT系统，重点在于优化源语言和目标语言句对中音素计数的对齐。

    

    传统的自动视频配音（AVD）流水线由三个关键模块组成，即自动语音识别（ASR）、神经机器翻译（NMT）和文本到语音（TTS）。AVD管道中使用等距-NMT算法来调节合成输出文本的长度，以确保在配音过程之后，视频和音频的对齐同步。先前的方法集中于调整机器翻译模型源语言和目标语言文本中字符和单词的数量。然而，我们的方法旨在调整音素的数量，因为它们与语音持续时间密切相关。本文提出使用强化学习（RL）开发等距NMT系统，重点优化源语言和目标语言句对中音素计数的对齐。为了评估我们的模型，我们提出了

    arXiv:2403.15469v1 Announce Type: new  Abstract: Traditional Automatic Video Dubbing (AVD) pipeline consists of three key modules, namely, Automatic Speech Recognition (ASR), Neural Machine Translation (NMT), and Text-to-Speech (TTS). Within AVD pipelines, isometric-NMT algorithms are employed to regulate the length of the synthesized output text. This is done to guarantee synchronization with respect to the alignment of video and audio subsequent to the dubbing process. Previous approaches have focused on aligning the number of characters and words in the source and target language texts of Machine Translation models. However, our approach aims to align the number of phonemes instead, as they are closely associated with speech duration. In this paper, we present the development of an isometric NMT system using Reinforcement Learning (RL), with a focus on optimizing the alignment of phoneme counts in the source and target language sentence pairs. To evaluate our models, we propose the 
    
[^207]: 利用超分辨率成像技术识别低分辨率模糊车牌：Real-ESRGAN、A-ESRGAN和StarSRGAN的比较研究

    Using Super-Resolution Imaging for Recognition of Low-Resolution Blurred License Plates: A Comparative Study of Real-ESRGAN, A-ESRGAN, and StarSRGAN

    [https://arxiv.org/abs/2403.15466](https://arxiv.org/abs/2403.15466)

    本研究利用超分辨率技术对模糊的车牌进行处理，并比较了Real-ESRGAN、A-ESRGAN和StarSRGAN三种模型的效果。

    

    随着技术的强劲发展，车牌识别技术现在可以在各种场景中得到适当应用，如道路监控、被盗车辆追踪、停车场出入口检测等。然而，这些应用正常运行的前提是车牌必须足够“清晰”，才能被系统识别出正确的车牌号码。如果由于外部因素使车牌变模糊，那么识别的准确性将大大降低。尽管台湾有许多道路监控摄像头，但大多数摄像头的质量不佳，经常导致由于照片分辨率低而无法识别车牌号码。因此，这项研究着重于使用超分辨率技术处理模糊的车牌。本研究主要将对三个超分辨率模型进行微调：Real-ESRGAN、A-ESRGAN和StarSRGAN。

    arXiv:2403.15466v1 Announce Type: cross  Abstract: With the robust development of technology, license plate recognition technology can now be properly applied in various scenarios, such as road monitoring, tracking of stolen vehicles, detection at parking lot entrances and exits, and so on. However, the precondition for these applications to function normally is that the license plate must be 'clear' enough to be recognized by the system with the correct license plate number. If the license plate becomes blurred due to some external factors, then the accuracy of recognition will be greatly reduced. Although there are many road surveillance cameras in Taiwan, the quality of most cameras is not good, often leading to the inability to recognize license plate numbers due to low photo resolution. Therefore, this study focuses on using super-resolution technology to process blurred license plates. This study will mainly fine-tune three super-resolution models: Real-ESRGAN, A-ESRGAN, and Star
    
[^208]: 使用展开算法为$n$-grams，Transformers，HMMs和马尔可夫链生成最有可能的序列

    Most Likely Sequence Generation for $n$-Grams, Transformers, HMMs, and Markov Chains, by Using Rollout Algorithms

    [https://arxiv.org/abs/2403.15465](https://arxiv.org/abs/2403.15465)

    本文介绍了一种使用展开算法为$n$-grams，Transformers，HMMs和马尔可夫链生成最有可能的序列的方法

    

    在本文中，我们考虑了一个具有$n$-gram结构的transformer，例如底层的ChatGPT。Transformer提供了下一个单词的概率，可以用来生成单词序列。我们考虑了基于这些概率计算高可能性单词序列的方法。计算从给定初始状态开始的最优（即最有可能）单词序列是一个棘手的问题，因此我们提出了在时间复杂度为$N$和$n$-gram词汇量的低阶多项式的方法来计算$N$个单词的高可能性序列。这些方法基于近似动态规划中的展开方法，一种单策略迭代，可以改善任何给定启发式策略的性能。在我们的情况下，我们使用一种贪婪启发式，生成具有最高概率的下一个单词。我们通过分析、示例和计算实验表明了我们的m

    arXiv:2403.15465v1 Announce Type: cross  Abstract: In this paper we consider a transformer with an $n$-gram structure, such as the one underlying ChatGPT. The transformer provides next word probabilities, which can be used to generate word sequences. We consider methods for computing word sequences that are highly likely, based on these probabilities. Computing the optimal (i.e., most likely) word sequence starting with a given initial state is an intractable problem, so we propose methods to compute highly likely sequences of $N$ words in time that is a low order polynomial in $N$ and in the vocabulary size of the $n$-gram. These methods are based on the rollout approach from approximate dynamic programming, a form of single policy iteration, which can improve the performance of any given heuristic policy. In our case we use a greedy heuristic that generates as next word one that has the highest probability. We show with analysis, examples, and computational experimentation that our m
    
[^209]: 基于LLMs的少样本疾病预测：结合预测性代理推理和批判性代理指导的新方法

    LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction

    [https://arxiv.org/abs/2403.15464](https://arxiv.org/abs/2403.15464)

    该论文提出了一种新方法，结合预测性代理推理和批判性代理指导，利用LLMs对结构化患者就诊数据进行预测，取得了较好的效果。

    

    arXiv:2403.15464v1 类型：跨学科 摘要：电子健康记录(EHRs)包含对健康相关预测任务，如疾病预测，有价值的患者数据。传统方法依赖于需要大量标记数据集的监督学习方法，这可能是昂贵和具有挑战性的。在本研究中，我们调查了将大型语言模型(LLMs)应用于将结构化患者就诊数据(例如诊断、实验室、处方)转换为自然语言叙述的可行性。我们使用各种面向EHR预测的提示策略评估了LLMs的零样本和少样本性能。此外，我们提出了一种新方法，利用具有不同角色的LLM代理：一个进行预测并生成推理过程的预测代理，以及分析不正确预测并为改善预测代理推理提供指导的批评代理。我们的结果表明，通过提出的方法，LLMs能够...

    arXiv:2403.15464v1 Announce Type: cross  Abstract: Electronic health records (EHRs) contain valuable patient data for health-related prediction tasks, such as disease prediction. Traditional approaches rely on supervised learning methods that require large labeled datasets, which can be expensive and challenging to obtain. In this study, we investigate the feasibility of applying Large Language Models (LLMs) to convert structured patient visit data (e.g., diagnoses, labs, prescriptions) into natural language narratives. We evaluate the zero-shot and few-shot performance of LLMs using various EHR-prediction-oriented prompting strategies. Furthermore, we propose a novel approach that utilizes LLM agents with different roles: a predictor agent that makes predictions and generates reasoning processes and a critic agent that analyzes incorrect predictions and provides guidance for improving the reasoning of the predictor agent. Our results demonstrate that with the proposed approach, LLMs c
    
[^210]: 揭示在不断变化的世界中的异常：连续学习中像素级异常检测的基准

    Unveiling the Anomalies in an Ever-Changing World: A Benchmark for Pixel-Level Anomaly Detection in Continual Learning

    [https://arxiv.org/abs/2403.15463](https://arxiv.org/abs/2403.15463)

    该研究针对连续学习环境中的像素级异常检测问题，实现了针对此问题的最先进技术，并提供了一个真实世界数据集作为可靠基准，为该领域的进一步发展奠定了基础。

    

    异常检测是许多现实世界应用中一个相关的问题，尤其是在处理图像时。然而，人们很少关注输入数据分布随时间变化所带来的问题，这可能会导致性能显著下降。在这项研究中，我们调查了连续学习设置下的像素级异常检测问题，其中新数据随时间到来，目标是在新旧数据上表现良好。我们实现了几种最先进的技术来解决经典设置下的异常检测问题，并对其进行改进以在连续学习设置中运行。为了验证这些方法，我们使用了一个真实世界的图像数据集，其中包含基于像素的异常，以提供一个可靠的基准，并作为该领域进一步发展的基础。我们提供了一项全面的分析，讨论了哪些异常检测方法和哪些方法族看起来表现最好。

    arXiv:2403.15463v1 Announce Type: cross  Abstract: Anomaly Detection is a relevant problem in numerous real-world applications, especially when dealing with images. However, little attention has been paid to the issue of changes over time in the input data distribution, which may cause a significant decrease in performance. In this study, we investigate the problem of Pixel-Level Anomaly Detection in the Continual Learning setting, where new data arrives over time and the goal is to perform well on new and old data. We implement several state-of-the-art techniques to solve the Anomaly Detection problem in the classic setting and adapt them to work in the Continual Learning setting. To validate the approaches, we use a real-world dataset of images with pixel-based anomalies to provide a reliable benchmark and serve as a foundation for further advancements in the field. We provide a comprehensive analysis, discussing which Anomaly Detection methods and which families of approaches seem m
    
[^211]: FUELVISION: 一种用于野火燃料映射的多模态数据融合和多模型集成算法

    FUELVISION: A Multimodal Data Fusion and Multimodel Ensemble Algorithm for Wildfire Fuels Mapping

    [https://arxiv.org/abs/2403.15462](https://arxiv.org/abs/2403.15462)

    通过多模态数据融合和多模型集成方法，采用生成式人工智能技术开发伪标记和完全合成数据集，以提高野火燃料映射的准确性。

    

    燃料条件的准确评估是预测火灾点燃和行为以及风险管理的前提条件。本文提出的方法利用包括Landsat-8光学影像、两种SAR（合成孔径雷达）影像Sentinel-1（C波段）和PALSAR（L波段）以及地形特征在内的各种数据源，捕获燃料类型和分布的综合信息。通过训练集成模型来预测类似“Scott and Burgan 40”这样的景观尺度燃料，使用美国农业部林务局获得的森林清查与分析（FIA）田间调查地块数据。然而，由于训练数据量不足，这种基本方法产生了相对较差的结果。针对地面真实数据可用性的限制，使用生成式人工智能方法开发了伪标记和完全合成数据集。这些合成数据集被用于增强来自加利福尼亚

    arXiv:2403.15462v1 Announce Type: cross  Abstract: Accurate assessment of fuel conditions is a prerequisite for fire ignition and behavior prediction, and risk management. The method proposed herein leverages diverse data sources including Landsat-8 optical imagery, Sentinel-1 (C-band) Synthetic Aperture Radar (SAR) imagery, PALSAR (L-band) SAR imagery, and terrain features to capture comprehensive information about fuel types and distributions. An ensemble model was trained to predict landscape-scale fuels such as the 'Scott and Burgan 40' using the as-received Forest Inventory and Analysis (FIA) field survey plot data obtained from the USDA Forest Service. However, this basic approach yielded relatively poor results due to the inadequate amount of training data. Pseudo-labeled and fully synthetic datasets were developed using generative AI approaches to address the limitations of ground truth data availability. These synthetic datasets were used for augmenting the FIA data from Calif
    
[^212]: 调整预训练语言模型以检测游戏内垃圾话

    Fine-Tuning Pre-trained Language Models to Detect In-Game Trash Talks

    [https://arxiv.org/abs/2403.15458](https://arxiv.org/abs/2403.15458)

    本研究调查了预训练语言模型在检测游戏内垃圾话和毒性信息方面的能力，使用BERT和GPT模型在DOTA 2游戏对战的聊天数据上进行评估。

    

    玩在线手机和电脑游戏时常见的问题与玩家之间的有毒行为和滥用沟通有关。基于不同的报告和研究，本研究还讨论了在线仇恨言论和毒性对玩家游戏表现和整体幸福感的影响。本研究调查了预训练语言模型对分类或检测游戏内垃圾话或有毒信息的能力。研究采用并评估了预训练的BERT和GPT语言模型在检测游戏内聊天中的毒性方面的表现。利用公开可用的API，收集了来自DOTA 2游戏对战的游戏内聊天数据，经过处理、审查和标记为非毒性、轻微（毒性）和有毒。该研究能够收集约两千个游戏内聊天以训练和测试BERT（Base-uncased）、BERT（Large-uncased）和GPT-3模型。基于这三种模型的最先进表现，本研究得出结论

    arXiv:2403.15458v1 Announce Type: new  Abstract: Common problems in playing online mobile and computer games were related to toxic behavior and abusive communication among players. Based on different reports and studies, the study also discusses the impact of online hate speech and toxicity on players' in-game performance and overall well-being. This study investigates the capability of pre-trained language models to classify or detect trash talk or toxic in-game messages The study employs and evaluates the performance of pre-trained BERT and GPT language models in detecting toxicity within in-game chats. Using publicly available APIs, in-game chat data from DOTA 2 game matches were collected, processed, reviewed, and labeled as non-toxic, mild (toxicity), and toxic. The study was able to collect around two thousand in-game chats to train and test BERT (Base-uncased), BERT (Large-uncased), and GPT-3 models. Based on the three models' state-of-the-art performance, this study concludes p
    
[^213]: 改进文本流中用于微调SentenceBERT的采样方法

    Improving Sampling Methods for Fine-tuning SentenceBERT in Text Streams

    [https://arxiv.org/abs/2403.15455](https://arxiv.org/abs/2403.15455)

    本研究旨在解决概念漂移问题，通过探索七种文本采样方法的有效性，精细调整语言模型，从而减轻性能下降。

    

    互联网上文本数据的激增为机构和公司提供了一个独特的机会，可以监测公众对其服务和产品的意见。考虑到这些数据的快速生成，处理依次到达、潜在无限的文本流的文本流挖掘设置通常比传统的批量学习更合适。虽然预训练语言模型通常因其在流式内容中高质量的文本向量化能力而被广泛采用，但它们在适应概念漂移（数据分布随时间发生变化，从而对模型性能产生负面影响的现象）方面面临挑战。本研究解决了概念漂移问题，探讨了七种文本采样方法对精心微调语言模型的效果，从而减轻性能下降。我们准确评估了这些方法对使用四种不同方式进行微调的SBERT模型的影响。

    arXiv:2403.15455v1 Announce Type: new  Abstract: The proliferation of textual data on the Internet presents a unique opportunity for institutions and companies to monitor public opinion about their services and products. Given the rapid generation of such data, the text stream mining setting, which handles sequentially arriving, potentially infinite text streams, is often more suitable than traditional batch learning. While pre-trained language models are commonly employed for their high-quality text vectorization capabilities in streaming contexts, they face challenges adapting to concept drift - the phenomenon where the data distribution changes over time, adversely affecting model performance. Addressing the issue of concept drift, this study explores the efficacy of seven text sampling methods designed to selectively fine-tune language models, thereby mitigating performance degradation. We precisely assess the impact of these methods on fine-tuning the SBERT model using four differ
    
[^214]: 端到端学习在位相恢复中存在什么问题？

    What is Wrong with End-to-End Learning for Phase Retrieval?

    [https://arxiv.org/abs/2403.15448](https://arxiv.org/abs/2403.15448)

    对称破坏技术在解决深度学习方法中前向模型对称性造成的困难方面发挥了重要作用。

    

    针对成像科学中普遍存在的非线性反问题，前向模型中的对称性很常见。当采用数据驱动的深度学习方法来解决这些问题时，这些内在对称性可能导致很大的学习困难。本文解释了这些困难是如何产生的，更重要的是如何通过在任何学习之前对训练集进行预处理，即对称破坏，来克服这些困难。我们以远场相位恢复（FFPR）为例，这对许多科学成像领域至关重要，并展示对称破坏可以显著改善数据驱动学习。我们还制定了对称破坏的数学原理。

    arXiv:2403.15448v1 Announce Type: cross  Abstract: For nonlinear inverse problems that are prevalent in imaging science, symmetries in the forward model are common. When data-driven deep learning approaches are used to solve such problems, these intrinsic symmetries can cause substantial learning difficulties. In this paper, we explain how such difficulties arise and, more importantly, how to overcome them by preprocessing the training set before any learning, i.e., symmetry breaking. We take far-field phase retrieval (FFPR), which is central to many areas of scientific imaging, as an example and show that symmetric breaking can substantially improve data-driven learning. We also formulate the mathematical principle of symmetry breaking.
    
[^215]: 通过ARIMA时间序列分析揭示社交网络上的多语言主题动态和趋势识别：LDA/HDP模型增强的新型数据翻译框架

    Decoding Multilingual Topic Dynamics and Trend Identification through ARIMA Time Series Analysis on Social Networks: A Novel Data Translation Framework Enhanced by LDA/HDP Models

    [https://arxiv.org/abs/2403.15445](https://arxiv.org/abs/2403.15445)

    该研究提出了一种新方法，通过ARIMA时间序列分析和LDA/HDP模型提取多语言社交网络中的主题动态，特别关注在危机期间的交流趋势，这一方法在语言一致性任务中表现出色。

    

    在这项研究中，作者提出了一种新的方法，能够破译多语言主题动态，并识别危机期间的交流趋势。我们关注突尼斯社交网络中在冠状病毒大流行期间以及其他显著主题（如体育和政治）中的对话。我们首先对与这些主题相关的各种多语言评论进行聚合。然后在数据预处理过程中对数据集进行了严格的精炼。我们引入我们的无英语到英语的机器翻译方法来处理语言差异。对这种方法的实证测试显示了很高的准确性和F1值，突显了它适用于语言一致任务的特点。深入研究，采用了先进的建模技术，特别是LDA和HDP模型，从翻译内容中提取相关主题。这导致应用ARIMA时间序列分析来解码不断变化的主题趋势。

    arXiv:2403.15445v1 Announce Type: cross  Abstract: In this study, the authors present a novel methodology adept at decoding multilingual topic dynamics and identifying communication trends during crises. We focus on dialogues within Tunisian social networks during the Coronavirus Pandemic and other notable themes like sports and politics. We start by aggregating a varied multilingual corpus of comments relevant to these subjects. This dataset undergoes rigorous refinement during data preprocessing. We then introduce our No-English-to-English Machine Translation approach to handle linguistic differences. Empirical tests of this method showed high accuracy and F1 scores, highlighting its suitability for linguistically coherent tasks. Delving deeper, advanced modeling techniques, specifically LDA and HDP models are employed to extract pertinent topics from the translated content. This leads to applying ARIMA time series analysis to decode evolving topic trends. Applying our method to a mu
    
[^216]: 基于IMU的跨模态迁移学习在人类活动识别中的调查

    A Survey of IMU Based Cross-Modal Transfer Learning in Human Activity Recognition

    [https://arxiv.org/abs/2403.15444](https://arxiv.org/abs/2403.15444)

    本篇论文调查了如何在人类活动/动作识别中实现跨模态迁移学习，探讨了IMU数据在此领域中的潜在应用，对HAR问题进行了重要性探讨，并比较了不同类型的多模态HAR数据集。

    

    尽管生活在一个多感知世界中，大多数人工智能模型仍然局限于对人体运动和行为的文本和视觉理解。事实上，对人类运动的完整情境意识最好是通过传感器的组合来理解。在本调查中，我们研究了如何在人类活动/动作识别（HAR）中跨模态迁移学习中传递和利用知识。我们阐述了IMU数据及其在跨模态学习中的适用性的重要性和潜力，以及研究HAR问题的重要性。我们通过时间和抽象性将HAR相关任务进行了分类，然后比较了各种类型的多模态HAR数据集。我们还区分和详细阐述了文献中许多相关但不一致使用的术语，如迁移学习、域自适应、表示学习、传感器融合和多模态学习，并描述了跨模态学习如何适应。

    arXiv:2403.15444v1 Announce Type: cross  Abstract: Despite living in a multi-sensory world, most AI models are limited to textual and visual understanding of human motion and behavior. In fact, full situational awareness of human motion could best be understood through a combination of sensors. In this survey we investigate how knowledge can be transferred and utilized amongst modalities for Human Activity/Action Recognition (HAR), i.e. cross-modality transfer learning. We motivate the importance and potential of IMU data and its applicability in cross-modality learning as well as the importance of studying the HAR problem. We categorize HAR related tasks by time and abstractness and then compare various types of multimodal HAR datasets. We also distinguish and expound on many related but inconsistently used terms in the literature, such as transfer learning, domain adaptation, representation learning, sensor fusion, and multimodal learning, and describe how cross-modal learning fits w
    
[^217]: 引入一种集成方法，通过分析PET扫描图像早期检测阿尔茨海默病

    Introducing an ensemble method for the early detection of Alzheimer's disease through the analysis of PET scan images

    [https://arxiv.org/abs/2403.15443](https://arxiv.org/abs/2403.15443)

    通过分析PET扫描图像，引入了一种集成方法早期检测阿尔茨海默病，并且在分类阿尔茨海默病时使用了多种深度学习和传统机器学习模型。

    

    阿尔茨海默病是一种逐渐恶化的神经退行性疾病，主要影响记忆、思维和行为等认知功能。本病存在一个关键阶段，即轻度认知障碍，非常重要尽早诊断，因为一些逐渐发展为病症的MCI患者会发展为这种疾病。本研究探讨了将阿尔茨海默病分类为四个不同组：控制正常（CN）、逐渐发展的轻度认知障碍（pMCI）、稳定的轻度认知障碍（sMCI）和阿尔茨海默病（AD）的具有挑战性的任务。这种分类是基于对从ADNI数据集获得的PET扫描图像的彻底检查，这提供了对疾病进展的彻底理解。已经使用了几种深度学习和传统机器学习模型来检测阿尔茨海默病。在本文中，使用了三种深度学习模型，即VGG16、AlexNet和自定义的卷积神经网络。

    arXiv:2403.15443v1 Announce Type: cross  Abstract: Alzheimer's disease is a progressive neurodegenerative disorder that primarily affects cognitive functions such as memory, thinking, and behavior. In this disease, there is a critical phase, mild cognitive impairment, that is really important to be diagnosed early since some patients with progressive MCI will develop the disease. This study delves into the challenging task of classifying Alzheimer's disease into four distinct groups: control normal (CN), progressive mild cognitive impairment (pMCI), stable mild cognitive impairment (sMCI), and Alzheimer's disease (AD). This classification is based on a thorough examination of PET scan images obtained from the ADNI dataset, which provides a thorough understanding of the disease's progression. Several deep-learning and traditional machine-learning models have been used to detect Alzheimer's disease. In this paper, three deep-learning models, namely VGG16 and AlexNet, and a custom Convolu
    
[^218]: 统一生成建模：基于贝叶斯流网络的3D分子

    Unified Generative Modeling of 3D Molecules via Bayesian Flow Networks

    [https://arxiv.org/abs/2403.15441](https://arxiv.org/abs/2403.15441)

    本文引入了Geometric Bayesian Flow Networks (GeoBFN)，通过在分布的可微分参数空间中对不同模态进行建模，实现了对多模态性和噪声敏感性的分子几何形状的自然拟合。GeoBFN通过优化的训练和采样技术，在多个3D分子生成基准上取得了最先进的性能。

    

    先进的生成模型（例如扩散模型）源自对数据分布的简化连续性假设，尽管显示出了良好的进展，但由于分子几何的多模态性和对噪声敏感的特性，很难直接应用于几何生成应用。本文引入了几何贝叶斯流网络（GeoBFN），通过在可微分参数空间中建模不同模态，自然地适配分子几何形状。GeoBFN通过在分布参数上合成移变相互依赖建模，保持了SE-(3)不变密度建模属性，并统一了不同模态的概率建模。通过优化的训练和采样技术，我们展示了GeoBFN在多个3D分子生成基准中取得了最先进的性能，最大限度地提高了生成质量（在QM9中稳定生成90.87%的分子，在原子方面稳定生成85.6%）

    arXiv:2403.15441v1 Announce Type: cross  Abstract: Advanced generative model (e.g., diffusion model) derived from simplified continuity assumptions of data distribution, though showing promising progress, has been difficult to apply directly to geometry generation applications due to the multi-modality and noise-sensitive nature of molecule geometry. This work introduces Geometric Bayesian Flow Networks (GeoBFN), which naturally fits molecule geometry by modeling diverse modalities in the differentiable parameter space of distributions. GeoBFN maintains the SE-(3) invariant density modeling property by incorporating equivariant inter-dependency modeling on parameters of distributions and unifying the probabilistic modeling of different modalities. Through optimized training and sampling techniques, we demonstrate that GeoBFN achieves state-of-the-art performance on multiple 3D molecule generation benchmarks in terms of generation quality (90.87% molecule stability in QM9 and 85.6% atom
    
[^219]: 基于修剪与恢复的联邦学习

    Federated Learning based on Pruning and Recovery

    [https://arxiv.org/abs/2403.15439](https://arxiv.org/abs/2403.15439)

    提出了一种基于修剪与恢复的联邦学习训练框架，通过整合异步学习算法和修剪技术，有效解决了传统算法在异构设备场景中的低效问题。

    

    提出了一种新颖的面向异构环境的联邦学习训练框架，考虑了现实环境中客户端的网络速度多样性。该框架整合了异步学习算法和修剪技术，有效解决了传统联邦学习算法在涉及异构设备的场景中的低效问题，同时解决了异步算法中某些客户端训练不足的滞后问题。通过训练过程中模型大小的增量恢复，该框架加快了模型训练速度同时保持模型准确性。此外，引入了对联邦学习聚合过程的优化，包括缓冲机制，使得异步联邦学习能够类似于同步学习进行操作。另外，服务器向客户端传输全局模型的过程中也进行了优化。

    arXiv:2403.15439v1 Announce Type: new  Abstract: A novel federated learning training framework for heterogeneous environments is presented, taking into account the diverse network speeds of clients in realistic settings. This framework integrates asynchronous learning algorithms and pruning techniques, effectively addressing the inefficiencies of traditional federated learning algorithms in scenarios involving heterogeneous devices, as well as tackling the staleness issue and inadequate training of certain clients in asynchronous algorithms. Through the incremental restoration of model size during training, the framework expedites model training while preserving model accuracy. Furthermore, enhancements to the federated learning aggregation process are introduced, incorporating a buffering mechanism to enable asynchronous federated learning to operate akin to synchronous learning. Additionally, optimizations in the process of the server transmitting the global model to clients reduce c
    
[^220]: BCI运动想象解码的无监督自适应深度学习方法

    Unsupervised Adaptive Deep Learning Method For BCI Motor Imagery Decoding

    [https://arxiv.org/abs/2403.15438](https://arxiv.org/abs/2403.15438)

    提出了一种无监督自适应深度学习方法，用于BCI运动想象解码，在不需要监督的情况下达到离线性能水平，同时无需重新训练模型，在实时观察基础上实现数据的持续重新对齐，展示了在跨主体场景下的有效性，并提供实验代码。

    

    在脑-计算机界面的背景下，我们提出了一种自适应方法，能够在线使用而不需要监督，同时达到离线性能水平。有趣的是，我们的方法不需要重新训练模型，而是使用一个冻结的高效深度学习骨干，同时基于实时观察持续地对数据进行重新对齐，无论是在输入空间还是潜在空间。我们展示了该方法在考虑具有挑战性的跨主体场景下，从脑电图数据中对运动想象进行解码的有效性。为了可重现性，我们分享了实验代码。

    arXiv:2403.15438v1 Announce Type: cross  Abstract: In the context of Brain-Computer Interfaces, we propose an adaptive method that reaches offline performance level while being usable online without requiring supervision. Interestingly, our method does not require retraining the model, as it consists in using a frozen efficient deep learning backbone while continuously realigning data, both at input and latent spaces, based on streaming observations. We demonstrate its efficiency for Motor Imagery brain decoding from electroencephalography data, considering challenging cross-subject scenarios. For reproducibility, we share the code of our experiments.
    
[^221]: HyPer-EP: 为心脏电生理而元学习的混合个性化模型

    HyPer-EP: Meta-Learning Hybrid Personalized Models for Cardiac Electrophysiology

    [https://arxiv.org/abs/2403.15433](https://arxiv.org/abs/2403.15433)

    提出了一个新颖的混合建模框架，结合了基于物理已知表达式和神经网络建模的元学习方法，用于描述个性化心脏数字孪生模型，实现了基于物理和神经网络组件的分离识别。

    

    个性化虚拟心脏模型在临床上展示出越来越大的潜力，尽管在给定患者特定数据的情况下估计其参数仍然是一个挑战。传统的基于物理的建模方法在计算上成本高昂，往往忽视了这些模型中的结构性错误，这是由于模型简化和假设所造成的。另一方面，现代深度学习方法依赖于数据监督，并且缺乏可解释性。在本文中，我们提出了一个新颖的混合建模框架，将个性化心脏数字孪生描述为基于物理已知表达式和神经网络建模未知与现实之间差距的组合。然后，我们提出了一个新颖的元学习框架，以实现混合模型中基于物理和神经网络组件的分离识别。我们展示了这种混合建模框架的可行性和普适性。

    arXiv:2403.15433v1 Announce Type: cross  Abstract: Personalized virtual heart models have demonstrated increasing potential for clinical use, although the estimation of their parameters given patient-specific data remain a challenge. Traditional physics-based modeling approaches are computationally costly and often neglect the inherent structural errors in these models due to model simplifications and assumptions. Modern deep learning approaches, on the other hand, rely heavily on data supervision and lacks interpretability. In this paper, we present a novel hybrid modeling framework to describe a personalized cardiac digital twin as a combination of a physics-based known expression augmented by neural network modeling of its unknown gap to reality. We then present a novel meta-learning framework to enable the separate identification of both the physics-based and neural components in the hybrid model. We demonstrate the feasibility and generality of this hybrid modeling framework with 
    
[^222]: BRIEDGE: EEG自适应边缘人工智能用于多脑到多机器人交互

    BRIEDGE: EEG-Adaptive Edge AI for Multi-Brain to Multi-Robot Interaction

    [https://arxiv.org/abs/2403.15432](https://arxiv.org/abs/2403.15432)

    BRIEDGE提出了一个用于多脑到多机器人交互的端到端系统，通过 EEG 自适应神经网络和编解码通信框架实现，引入了基于Informer的ProbSparse自注意机制以提高分类准确性。

    

    最近 EEG 基于脑机接口技术的进展揭示了通过整合传感、计算、通信和控制实现脑到机器人协作的潜力。在本文中，我们介绍了 BRIEDGE 作为一个端到端系统，用于通过 EEG 自适应神经网络和编解码通信框架实现多脑到多机器人交互，如图1所示。正如所描绘的那样，边缘移动服务器或边缘便携服务器将从用户收集 EEG 数据，并利用 EEG 自适应神经网络识别用户的意图。编解码通信框架然后对 EEG 基础的语义信息进行编码，并在数据传输过程中解码成命令。为了更好地提取异质 EEG 数据的联合特征以及增强分类准确性，BRIEDGE 引入了一种基于 Informer 的 ProbSparse 自注意机制。同时，平行和安全的进行转移。

    arXiv:2403.15432v1 Announce Type: cross  Abstract: Recent advances in EEG-based BCI technologies have revealed the potential of brain-to-robot collaboration through the integration of sensing, computing, communication, and control. In this paper, we present BRIEDGE as an end-to-end system for multi-brain to multi-robot interaction through an EEG-adaptive neural network and an encoding-decoding communication framework, as illustrated in Fig.1. As depicted, the edge mobile server or edge portable server will collect EEG data from the users and utilize the EEG-adaptive neural network to identify the users' intentions. The encoding-decoding communication framework then encodes the EEG-based semantic information and decodes it into commands in the process of data transmission. To better extract the joint features of heterogeneous EEG data as well as enhance classification accuracy, BRIEDGE introduces an informer-based ProbSparse self-attention mechanism. Meanwhile, parallel and secure trans
    
[^223]: 将BCI模型从校准转移到控制：观察EEG特征中的变化

    Transferring BCI models from calibration to control: Observing shifts in EEG features

    [https://arxiv.org/abs/2403.15431](https://arxiv.org/abs/2403.15431)

    通过标准校准会话和基于EMG的新BCI控制会话，观察到了感觉运动节律的相似之处，并发现了控制范例引入的额外准备效果。

    

    公共基于运动想象的脑-计算机界面（BCI）数据集被用来开发越来越好的分类器。然而，它们通常遵循离散范例，在这些范例中，参与者定期执行运动想象。当用户尝试使用这样的BCI执行控制任务时，EEG模式可能会发生什么变化通常不明确。这可能导致泛化错误。我们展示了一种包含标准校准会话和基于EMG的新BCI控制会话的新范例。这使我们能够观察到感觉运动节律的相似之处，并观察到控制范例引入的额外准备效果。在运动相关皮层电位中，我们发现了校准与控制会话之间的巨大差异。我们展示了一个基于CSP的机器学习模型，它在校准数据上训练，可以对BCI控制驾驶数据进行出人意料地良好预测。

    arXiv:2403.15431v1 Announce Type: cross  Abstract: Public Motor Imagery-based brain-computer interface (BCI) datasets are being used to develop increasingly good classifiers. However, they usually follow discrete paradigms where participants perform Motor Imagery at regularly timed intervals. It is often unclear what changes may happen in the EEG patterns when users attempt to perform a control task with such a BCI. This may lead to generalisation errors. We demonstrate a new paradigm containing a standard calibration session and a novel BCI control session based on EMG. This allows us to observe similarities in sensorimotor rhythms, and observe the additional preparation effects introduced by the control paradigm. In the Movement Related Cortical Potentials we found large differences between the calibration and control sessions. We demonstrate a CSP-based Machine Learning model trained on the calibration data that can make surprisingly good predictions on the BCI-controlled driving da
    
[^224]: 教育环境下集成强先验模块和数据重叠估计的三阶段SFT混合模型

    A Three-Phases SFT Hybrid Model Integrated Strong Prior Module and Data Overlap Estimation in the Eduation Context

    [https://arxiv.org/abs/2403.15426](https://arxiv.org/abs/2403.15426)

    提出了一种在教育领域中应用的三阶段监督微调模型，通过先验和数据重叠估计实现了教育知识的结构拆卸和增量引导输出。

    

    在本文中，我们提出了一种端到端基于先验的三阶段监督微调模型，证明比传统微调方法更有竞争力。具体而言，我们的模型实现了教育知识的结构拆卸和增量引导输出。为此，我们通过采样器和重叠估计神经网络对三种类型的数据进行了健壮的分类，将预处理数据集分三批注入预训练模型进行LORA微调。然后，我们设计了一个先验模块，将系统提示、向量数据库和抽象语法树任务分割相结合。最后，对基于先验的微调模型应用了压缩方法和正则化约束，随后在输出端进行文本过滤以获得增量引导结果。我们的模型代表了真正以丰富的教育知识、分步指导的特点体现导师角色的第一项研究努力。

    arXiv:2403.15426v1 Announce Type: cross  Abstract: In this paper, we propose an end-to-end prior-based three-phases supervised fine-tuned model, which is proved more competitive than traditional fine-tuning method. More specifically, our model realizes the structural disassembly and incremental guided output of educational knowledge. To this end, we robustify data classification of three types via a sampler and overlap estimation neural network, and inject the preprocessing datasets into pre-trained model in three batches for LORA fine-tuning. Then, we design a prior module couples system prompt, vector databases, and abstract syntax tree task segmentation. Finally, the compression method and regularization constraint are applied to the prior-based fine-tuned model, followed by text filter at the output end to obtain incremental guided results. Our model represents the first research effort to truly embody the tutor role with the features of abundant educational knowledge, step-by-step
    
[^225]: 使用具有时间关系信息的深度领域自适应进行跨用户活动识别

    Cross-user activity recognition using deep domain adaptation with temporal relation information

    [https://arxiv.org/abs/2403.15424](https://arxiv.org/abs/2403.15424)

    该论文提出了Deep Temporal State Domain Adaptation（DTSDA）模型，用于处理跨用户活动识别中的行为变异挑战。

    

    人类活动识别（HAR）是普适计算的基石，具有在健康监测和环境辅助生活等多个领域中有前景的应用。尽管取得了显著进展，基于传感器的HAR方法通常在训练和测试数据具有相同分布的假设下运作。然而，在许多现实场景中，特别是在基于传感器的HAR中，这一假设因分布之外的挑战而无效，包括来自异构传感器的差异、随时间变化以及个体行为变异。本文聚焦于后者，探索跨用户HAR问题，其中个体之间的行为变异导致不同数据分布。为了解决这一挑战，我们提出了Deep Temporal State Domain Adaptation（DTSDA）模型，这是一种为跨用户HAR中的时间序列领域自适应量身定制的创新方法。

    arXiv:2403.15424v1 Announce Type: cross  Abstract: Human Activity Recognition (HAR) is a cornerstone of ubiquitous computing, with promising applications in diverse fields such as health monitoring and ambient assisted living. Despite significant advancements, sensor-based HAR methods often operate under the assumption that training and testing data have identical distributions. However, in many real-world scenarios, particularly in sensor-based HAR, this assumption is invalidated by out-of-distribution ($\displaystyle o.o.d.$) challenges, including differences from heterogeneous sensors, change over time, and individual behavioural variability. This paper centres on the latter, exploring the cross-user HAR problem where behavioural variability across individuals results in differing data distributions. To address this challenge, we introduce the Deep Temporal State Domain Adaptation (DTSDA) model, an innovative approach tailored for time series domain adaptation in cross-user HAR. Con
    
[^226]: 基于时间关系最优输运的跨用户活动识别

    Cross-user activity recognition via temporal relation optimal transport

    [https://arxiv.org/abs/2403.15423](https://arxiv.org/abs/2403.15423)

    本文提出了一种基于时间关系最优输运的跨用户活动识别方法，旨在解决现有基于i.i.d.假设的域自适应方法在时间序列数据中的局限性

    

    当前关于人类活动识别（HAR）的研究主要假设训练和测试数据来自相同分布，以实现泛化模型，这意味着所有数据被认为是独立和同分布（i.i.d.）。在许多实际应用中，这一假设不成立，收集的训练和目标测试数据集具有不均匀分布，如跨用户HAR的情况。域自适应是跨用户HAR任务的一种有前途的方法。现有的基于域自适应的工作基于每个域中的样本是i.i.d.的假设，并且不考虑时间序列数据中隐藏的时间关系知识以对齐数据分布。这一关于i.i.d.的强假设对于基于时间序列的域自适应方法可能不太适用，因为时间序列细分和特征形成的样本

    arXiv:2403.15423v1 Announce Type: cross  Abstract: Current research on human activity recognition (HAR) mainly assumes that training and testing data are drawn from the same distribution to achieve a generalised model, which means all the data are considered to be independent and identically distributed $\displaystyle (i.i.d.) $. In many real-world applications, this assumption does not hold, and collected training and target testing datasets have non-uniform distribution, such as in the case of cross-user HAR. Domain adaptation is a promising approach for cross-user HAR tasks. Existing domain adaptation works based on the assumption that samples in each domain are $\displaystyle i.i.d. $ and do not consider the knowledge of temporal relation hidden in time series data for aligning data distribution. This strong assumption of $\displaystyle i.i.d. $ may not be suitable for time series-related domain adaptation methods because the samples formed by time series segmentation and feature e
    
[^227]: 基于传感器的人体活动识别中的数据异质性机器学习技术--综述

    Machine Learning Techniques for Sensor-based Human Activity Recognition with Data Heterogeneity -- A Review

    [https://arxiv.org/abs/2403.15422](https://arxiv.org/abs/2403.15422)

    通过研究机器学习如何处理传感器数据异质性，能够改善人体活动识别系统的性能，降低计算成本，更快地开发出个性化的自适应模型。

    

    arXiv:2403.15422v1 公告类型: 跨领域 摘要: 基于传感器的人体活动识别（HAR）在普适计算中至关重要，通过多维观察分析行为。 尽管研究取得了进展，HAR面临挑战，特别是在数据分布假设方面。 大多数研究通常假设各个数据集之间具有均匀的数据分布，与实际传感器数据在人类活动中的多样性相矛盾。 解决数据异质性问题可以提高性能，降低计算成本，并有助于开发个性化、自适应模型，减少标注数据。 本综述研究了机器学习如何处理HAR中的数据异质性，通过对数据异质性类型进行分类、应用相应的适当机器学习方法、总结可用数据集，并讨论未来挑战。

    arXiv:2403.15422v1 Announce Type: cross  Abstract: Sensor-based Human Activity Recognition (HAR) is crucial in ubiquitous computing, analysing behaviours through multi-dimensional observations. Despite research progress, HAR confronts challenges, particularly in data distribution assumptions. Most studies often assume uniform data distributions across datasets, contrasting with the varied nature of practical sensor data in human activities. Addressing data heterogeneity issues can improve performance, reduce computational costs, and aid in developing personalized, adaptive models with less annotated data. This review investigates how machine learning addresses data heterogeneity in HAR, by categorizing data heterogeneity types, applying corresponding suitable machine learning methods, summarizing available datasets, and discussing future challenges.
    
[^228]: 面向低功耗应用的敏捷手势识别：通用性定制化

    Agile gesture recognition for low-power applications: customisation for generalisation

    [https://arxiv.org/abs/2403.15421](https://arxiv.org/abs/2403.15421)

    通过自适应和敏捷误差校正，提升了传统手势识别模型在低功耗设备上的性能

    

    自动手势识别长期以来一直是人工智能社区的焦点。传统上，这一领域的研究主要集中在对手部图像连续流的访问情境上。然而，对于在低功耗传感器设备上运行的手势识别技术的需求日益增长。本研究揭示了一种新颖的模式识别系统方法，利用自适应和敏捷误差校正，旨在提高传统手势识别模型在带有li的设备上的性能。

    arXiv:2403.15421v1 Announce Type: cross  Abstract: Automated hand gesture recognition has long been a focal point in the AI community. Traditionally, research in this field has predominantly focused on scenarios with access to a continuous flow of hand's images. This focus has been driven by the widespread use of cameras and the abundant availability of image data. However, there is an increasing demand for gesture recognition technologies that operate on low-power sensor devices. This is due to the rising concerns for data leakage and end-user privacy, as well as the limited battery capacity and the computing power in low-cost devices. Moreover, the challenge in data collection for individually designed hardware also hinders the generalisation of a gesture recognition model.   In this study, we unveil a novel methodology for pattern recognition systems using adaptive and agile error correction, designed to enhance the performance of legacy gesture recognition models on devices with li
    
[^229]: 仅需注意力即可提升图卷积神经网络

    Attention is all you need for boosting graph convolutional neural network

    [https://arxiv.org/abs/2403.15419](https://arxiv.org/abs/2403.15419)

    GKEDM插件模块通过多头注意力机制提取和聚合图信息，增强节点表示并提高GCN的性能，同时可以作为知识蒸馏的辅助转移器。

    

    图卷积神经网络（GCNs）在处理非网格域的图数据方面具有强大的能力。它们能够捕捉图中的拓扑逻辑结构和节点特征，并将它们融合到节点的最终表示中。GCNs在推荐系统、社交网络和蛋白质分子结构等各个领域得到了广泛研究。本文提出了一种名为图知识增强和蒸馏模块（GKEDM）的插件模块。GKEDM可以通过多头注意力机制提取和聚合图信息，增强节点表示并提高GCN的性能。此外，GKEDM还可以作为知识蒸馏的辅助转移器。通过一种特殊设计的注意力蒸馏方法，GKEDM可以...

    arXiv:2403.15419v1 Announce Type: new  Abstract: Graph Convolutional Neural Networks (GCNs) possess strong capabilities for processing graph data in non-grid domains. They can capture the topological logical structure and node features in graphs and integrate them into nodes' final representations. GCNs have been extensively studied in various fields, such as recommendation systems, social networks, and protein molecular structures. With the increasing application of graph neural networks, research has focused on improving their performance while compressing their size. In this work, a plug-in module named Graph Knowledge Enhancement and Distillation Module (GKEDM) is proposed. GKEDM can enhance node representations and improve the performance of GCNs by extracting and aggregating graph information via multi-head attention mechanism. Furthermore, GKEDM can serve as an auxiliary transferor for knowledge distillation. With a specially designed attention distillation method, GKEDM can dis
    
[^230]: 利用Transformer技术增强物联网应用的自动调制识别

    Enhancing Automatic Modulation Recognition for IoT Applications Using Transformers

    [https://arxiv.org/abs/2403.15417](https://arxiv.org/abs/2403.15417)

    使用Transformer网络提出了一种高效的自动调制识别方法，在物联网环境中具有最佳的识别准确率。

    

    自动调制识别(AMR)对于确定传入信号的调制类型至关重要。结合先进的深度学习方法能够实现快速处理和最小资源使用，这对物联网应用至关重要。我们引入了一种使用Transformer网络的新颖方法，专门设计用于解决物联网环境中普遍存在的模型大小限制。我们进行了大量实验，结果显示我们提出的方法优于先进的深度学习技术，实现了最高的识别准确率。

    arXiv:2403.15417v1 Announce Type: cross  Abstract: Automatic modulation recognition (AMR) is critical for determining the modulation type of incoming signals. Integrating advanced deep learning approaches enables rapid processing and minimal resource usage, essential for IoT applications. We have introduced a novel method using Transformer networks for efficient AMR, designed specifically to address the constraints on model size prevalent in IoT environments. Our extensive experiments reveal that our proposed method outperformed advanced deep learning techniques, achieving the highest recognition accuracy.
    
[^231]: 在二阶优化中模糊超参数更新的研究

    Fuzzy hyperparameters update in a second order optimization

    [https://arxiv.org/abs/2403.15416](https://arxiv.org/abs/2403.15416)

    介绍了一种在二阶优化中加速收敛的混合方法，利用在线有限差分逼近对角Hessian矩阵，并应用模糊推理于多个超参数。

    

    本研究将提出一种混合方法，以加速二阶优化中的收敛速度。将介绍对角Hessian矩阵的在线有限差分逼近，以及对几个超参数进行模糊推理。已取得竞争性成果。

    arXiv:2403.15416v1 Announce Type: cross  Abstract: This research will present a hybrid approach to accelerate convergence in a second order optimization. An online finite difference approximation of the diagonal Hessian matrix will be introduced, along with fuzzy inferencing of several hyperparameters. Competitive results have been achieved
    
[^232]: 基于物理信息和无监督的黎曼域自适应用于异构脑电数据的机器学习

    Physics-informed and Unsupervised Riemannian Domain Adaptation for Machine Learning on Heterogeneous EEG Datasets

    [https://arxiv.org/abs/2403.15415](https://arxiv.org/abs/2403.15415)

    提出一种利用物理信息和无监督方法的黎曼域自适应技术，能够有效整合不同EEG数据集以进行机器学习，特别在脑机接口任务和生物标志物应用中表现出鲁棒性。

    

    由于会话、受试者和设备的变异性，将脑电图 (EEG) 数据集用于监督机器学习 (ML) 具有挑战性。ML算法通常需要在训练和测试时具有相同的特征，由于不同数据集之间传感器数量和位置的变化而复杂化了分析。简单的通道选择会丢失有价值的数据，尤其是在共享少量通道的数据集中表现更差。为了解决这个问题，我们提出了一个利用EEG信号物理信息的无监督方法。我们使用场插值将EEG通道映射到固定位置，促进无源领域自适应。利用黎曼几何分类流程和迁移学习步骤，我们的方法在脑机接口 (BCI) 任务和潜在生物标志物应用中展示出强大的性能。与一种称为超越维度的基于统计的方法进行比较，一种基于信号的插值

    arXiv:2403.15415v1 Announce Type: cross  Abstract: Combining electroencephalogram (EEG) datasets for supervised machine learning (ML) is challenging due to session, subject, and device variability. ML algorithms typically require identical features at train and test time, complicating analysis due to varying sensor numbers and positions across datasets. Simple channel selection discards valuable data, leading to poorer performance, especially with datasets sharing few channels. To address this, we propose an unsupervised approach leveraging EEG signal physics. We map EEG channels to fixed positions using field interpolation, facilitating source-free domain adaptation. Leveraging Riemannian geometry classification pipelines and transfer learning steps, our method demonstrates robust performance in brain-computer interface (BCI) tasks and potential biomarker applications. Comparative analysis against a statistical-based approach known as Dimensionality Transcending, a signal-based imputa
    
[^233]: 耦合发生器分解用于融合脑电图和脑磁图数据

    Coupled generator decomposition for fusion of electro- and magnetoencephalography data

    [https://arxiv.org/abs/2403.15409](https://arxiv.org/abs/2403.15409)

    介绍了耦合发生器分解的概念，推广了稀疏主成分分析（SPCA），并通过脑电图和脑磁图数据的融合实验展示了其在识别共同特征的有效性。

    

    数据融合建模可以识别跨不同数据源的共同特征，同时考虑源特异性变异。在这里，我们介绍了\textit{耦合发生器分解}的概念，展示了它如何推广了数据融合的稀疏主成分分析（SPCA）。通过利用多个受试者、多模态（脑电图和脑磁图（EEG和MEG））神经影像实验的数据，我们展示了该框架在识别对面部知觉刺激的共同特征方面的效果，同时考虑了模态和受试者特异性变异。通过脑电图/脑磁图试验的拆分交叉验证，我们研究了不同复杂性模型的最佳模型顺序和正则化强度，将其与假设对刺激有共享脑反应的群体水平模型进行比较。我们的发现显示，与混乱的面孔相比，约$\sim170ms$颞下回面区激活发生了改变。

    arXiv:2403.15409v1 Announce Type: cross  Abstract: Data fusion modeling can identify common features across diverse data sources while accounting for source-specific variability. Here we introduce the concept of a \textit{coupled generator decomposition} and demonstrate how it generalizes sparse principal component analysis (SPCA) for data fusion. Leveraging data from a multisubject, multimodal (electro- and magnetoencephalography (EEG and MEG)) neuroimaging experiment, we demonstrate the efficacy of the framework in identifying common features in response to face perception stimuli, while accommodating modality- and subject-specific variability. Through split-half cross-validation of EEG/MEG trials, we investigate the optimal model order and regularization strengths for models of varying complexity, comparing these to a group-level model assuming shared brain responses to stimuli. Our findings reveal altered $\sim170ms$ fusiform face area activation for scrambled faces, as opposed to 
    
[^234]: 基于短时ECG和采样长期HRV的多模态心力衰竭风险评估

    Multi-modal Heart Failure Risk Estimation based on Short ECG and Sampled Long-Term HRV

    [https://arxiv.org/abs/2403.15408](https://arxiv.org/abs/2403.15408)

    提出了结合30秒ECG记录和长期HRV数据的多模态方法用于估算心力衰竭住院风险，并引入了两种生存模型：XGBoost模型和ResNet模型。

    

    心血管疾病，包括心力衰竭（HF），仍然是全球主要的死亡原因，常常难以早期检测。在此背景下，可访问和有效的风险评估是不可或缺的。传统方法依赖于资源密集型的诊断测试，通常在症状发作后进行。心电图（ECG）技术的广泛可用性和机器学习的力量正成为智能医疗领域的可行替代方案。在本文中，我们提出了几种多模态方法，结合30秒ECG记录和近似的长期心率变异性（HRV）数据，来估算HF住院风险。我们引入了两种生存模型：一个XGBoost模型，用于加速失败时间（AFT），结合了全面的ECG特征；以及一个从原始ECG中学习的ResNet模型。我们通过我们从超级长期HRV中提取的新颖长期HRVs来扩展这些模型。

    arXiv:2403.15408v1 Announce Type: cross  Abstract: Cardiovascular diseases, including Heart Failure (HF), remain a leading global cause of mortality, often evading early detection. In this context, accessible and effective risk assessment is indispensable. Traditional approaches rely on resource-intensive diagnostic tests, typically administered after the onset of symptoms. The widespread availability of electrocardiogram (ECG) technology and the power of Machine Learning are emerging as viable alternatives within smart healthcare. In this paper, we propose several multi-modal approaches that combine 30-second ECG recordings and approximate long-term Heart Rate Variability (HRV) data to estimate the risk of HF hospitalization. We introduce two survival models: an XGBoost model with Accelerated Failure Time (AFT) incorporating comprehensive ECG features and a ResNet model that learns from the raw ECG. We extend these with our novel long-term HRVs extracted from the combination of ultra-
    
[^235]: ChatGPT在线性代数中的运用：取得进展，留待进一步探索

    ChatGPT in Linear Algebra: Strides Forward, Steps to Go

    [https://arxiv.org/abs/2403.15399](https://arxiv.org/abs/2403.15399)

    ChatGPT在线性代数中取得巨大改进，但目前仍然无法完全取代人类教师，软件理解问题的能力引发了人们对其潜力的思考。

    

    一旦新技术出现，教育界就会探索其实用性以及在教育中的应用可能性。本文分析了关于基础线性代数主题的ChatGPT会话。我们反思了过去一年内ChatGPT在我们感兴趣的领域所进行的过程，强调了在应对线性代数问题上所取得的巨大改进。尤其是，本文讨论了这个软件是否可以作为教学助手，甚至在某种程度上取代人类教师的问题。截至本文撰写时，答案通常是否定的。对于可以为之积极的方面，给出了一些关于原始工程的反思。与该软件的交流给人一种在与人类交谈的印象，有时会产生这样一个问题：软件是否理解这个问题。因此，读者的注意力被引向了f

    arXiv:2403.15399v1 Announce Type: cross  Abstract: As soon as a new technology emerges, the education community explores its affordances and the possibilities to apply it in education. In this paper, we analyze sessions with ChatGPT around topics in basic Linear Algebra. We reflect the process undertaken by the ChatGPT along the recent year in our area of interest, emphasising the vast improvement that has been done in grappling with Linear Algebra problems. In particular, the question whether this software can be a teaching assistant or even somehow replace the human teacher, is addressed. As of the time this paper is written, the answer is generally negative. For the small part where the answer can be positive, some reflections about an original instrumental genesis are given.   Communication with the software gives the impression to talk to a human, and sometimes the question is whether the software understands the question or not. Therefore, the reader's attention is drawn to the f
    
[^236]: 2024年《模型报告的模型卡》：以可信度和风险管理重新分类道德考虑的类别

    "Model Cards for Model Reporting" in 2024: Reclassifying Category of Ethical Considerations in Terms of Trustworthiness and Risk Management

    [https://arxiv.org/abs/2403.15394](https://arxiv.org/abs/2403.15394)

    将模型报告中的道德考虑类别重新分类为可信度和风险管理类别，针对可信度AI领域的最新发展提出了新的观点。

    

    2019年，题为《模型报告的模型卡》的论文介绍了一种新的工具，用于记录模型性能，并鼓励透明报告练习，针对一系列明确定义的类别。该论文中详细介绍的一个类别是道德考虑，其中包括数据、人类生命、缓解措施、风险和危害以及用例等子类别。我们提议重新分类原始模型卡中的这一类别，这是因为最近成熟的领域称为可信度人工智能，该术语分析算法属性是否表明AI系统值得受到利益相关者的信任。在我们对可信度人工智能的研究中，我们强调了三个备受尊重的组织——欧盟委员会AI高级专家组、经济合作与发展组织和美国国家标准与技术研究所——他们就可信度人工智能的各个方面撰写了准则。这些最近的出版物汇聚了许多共同点。

    arXiv:2403.15394v1 Announce Type: cross  Abstract: In 2019, the paper entitled "Model Cards for Model Reporting" introduced a new tool for documenting model performance and encouraged the practice of transparent reporting for a defined list of categories. One of the categories detailed in that paper is ethical considerations, which includes the subcategories of data, human life, mitigations, risks and harms, and use cases. We propose to reclassify this category in the original model card due to the recent maturing of the field known as trustworthy AI, a term which analyzes whether the algorithmic properties of the model indicate that the AI system is deserving of trust from its stakeholders. In our examination of trustworthy AI, we highlight three respected organizations - the European Commission's High-Level Expert Group on AI, the OECD, and the U.S.-based NIST - that have written guidelines on various aspects of trustworthy AI. These recent publications converge on numerous character
    
[^237]: 通过基于注意力的双向递归神经网络从Reddit帖子中检测阿片类药物使用者

    Detection of Opioid Users from Reddit Posts via an Attention-based Bidirectional Recurrent Neural Network

    [https://arxiv.org/abs/2403.15393](https://arxiv.org/abs/2403.15393)

    通过机器学习方法分析Reddit用户帖子，检测阿片类药物使用者，帮助改善对阿片类药物危机的监测和理解。

    

    阿片类药物危机指的是因阿片类药物过量使用和成瘾而导致的日益增长的住院和死亡案例，已经成为美国严重的健康问题。为应对此危机，联邦和地方政府以及卫生社区已经制定了许多策略。其中，通过更好的健康监测来提高我们对危机的了解是当务之急之一。除了直接测试，机器学习方法也可能通过分析社交媒体数据来检测阿片类药物使用者，因为许多阿片类药物使用者可能选择不做测试，但可能会匿名在社交媒体上分享他们的经历。在这篇论文中，我们利用机器学习的最新进展，收集并分析了来自流行社交网络Reddit的用户帖子，以确定阿片类药物使用者。

    arXiv:2403.15393v1 Announce Type: new  Abstract: The opioid epidemic, referring to the growing hospitalizations and deaths because of overdose of opioid usage and addiction, has become a severe health problem in the United States. Many strategies have been developed by the federal and local governments and health communities to combat this crisis. Among them, improving our understanding of the epidemic through better health surveillance is one of the top priorities. In addition to direct testing, machine learning approaches may also allow us to detect opioid users by analyzing data from social media because many opioid users may choose not to do the tests but may share their experiences on social media anonymously. In this paper, we take advantage of recent advances in machine learning, collect and analyze user posts from a popular social network Reddit with the goal to identify opioid users. Posts from more than 1,000 users who have posted on three sub-reddits over a period of one mon
    
[^238]: 一种针对图像水印的转移攻击

    A Transfer Attack to Image Watermarks

    [https://arxiv.org/abs/2403.15365](https://arxiv.org/abs/2403.15365)

    水印领域的研究表明，即使在攻击者无法访问水印模型或检测API的情况下，水印基础的AI生成图像检测器也无法抵抗对抗攻击。

    

    水印已被广泛应用于工业领域，用于检测由人工智能生成的图像。文献中对这种基于水印的检测器在白盒和黑盒环境下对抗攻击的稳健性有很好的理解。然而，在无盒环境下的稳健性却知之甚少。具体来说，多项研究声称图像水印在这种环境下是稳健的。在这项工作中，我们提出了一种新的转移对抗攻击来针对无盒环境下的图像水印。我们的转移攻击向带水印的图像添加微扰，以躲避被攻击者训练的多个替代水印模型，并且经过扰动的带水印图像也能躲避目标水印模型。我们的主要贡献是理论上和经验上展示了，基于水印的人工智能生成图像检测器即使攻击者没有访问水印模型或检测API，也不具有对抗攻击的稳健性。

    arXiv:2403.15365v1 Announce Type: cross  Abstract: Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and black-box settings is well understood in the literature. However, the robustness in the no-box setting is much less understood. In particular, multiple studies claimed that image watermark is robust in such setting. In this work, we propose a new transfer evasion attack to image watermark in the no-box setting. Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model. Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API.
    
[^239]: 卡通幻觉检测: 姿势感知上下文视觉学习

    Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning

    [https://arxiv.org/abs/2403.15048](https://arxiv.org/abs/2403.15048)

    该研究提出了一种用于检测由TTI模型生成的卡通角色图像中视觉幻觉的系统，通过结合姿势感知上下文视觉学习和视觉语言模型，利用RGB图像和姿势信息，实现了更准确的决策，显著提高了视觉幻觉的识别能力，推动了TTI模型在非照片真实领域的发展。

    

    大规模文本到图像（TTI）模型已经成为各种生成领域中生成训练数据的常见方法。然而，视觉幻觉，尤其是在非照片真实风格如卡通人物中包含了感知上关键的缺陷，依然是一个令人担忧的问题。我们提出了一种新颖的用于检测TTI模型生成的卡通角色图像的视觉幻觉检测系统。我们的方法利用了姿势感知上下文视觉学习（PA-ICVL）与视觉语言模型（VLMs），同时利用RGB图像和姿势信息。通过从一个经过微调的姿势估计器中获得姿势指导，我们使VLM能够做出更准确的决策。实验结果表明，在识别视觉幻觉方面，与仅依赖于RGB图像的基线方法相比，取得了显著的改进。这项研究通过减轻视觉幻觉，推动了TTI模型在非照片真实领域的潜力。

    arXiv:2403.15048v1 Announce Type: cross  Abstract: Large-scale Text-to-Image (TTI) models have become a common approach for generating training data in various generative fields. However, visual hallucinations, which contain perceptually critical defects, remain a concern, especially in non-photorealistic styles like cartoon characters. We propose a novel visual hallucination detection system for cartoon character images generated by TTI models. Our approach leverages pose-aware in-context visual learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB images and pose information. By incorporating pose guidance from a fine-tuned pose estimator, we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual hallucinations compared to baseline methods relying solely on RGB images. This research advances TTI models by mitigating visual hallucinations, expanding their potential in non-photorealistic domains.
    
[^240]: 从状态方程到引力对偶

    Gravitational Duals from Equations of State

    [https://arxiv.org/abs/2403.14763](https://arxiv.org/abs/2403.14763)

    引力双对偶理论提出了一种基于物理信息神经网络的新方法，可以从预设的状态方程推导出对应的引力理论。

    

    引力双对偶理论将五维引力理论与四维量子场论在平直空间中相关联。在这种映射下，场论的状态方程被编码在引力理论的黑洞解中。解五维爱因斯坦方程以确定状态方程是一个算法性的、直接的问题。确定引力理论从而产生特定状态方程是一个更具挑战性的、反向问题。我们提出了一种基于物理信息神经网络的新方法来解决这个问题。由此产生的算法不仅受数据驱动，还受爱因斯坦方程的物理知识驱动。我们成功地将其应用于具有交叉点、一级和二级相变的理论中。

    arXiv:2403.14763v1 Announce Type: cross  Abstract: Holography relates gravitational theories in five dimensions to four-dimensional quantum field theories in flat space. Under this map, the equation of state of the field theory is encoded in the black hole solutions of the gravitational theory. Solving the five-dimensional Einstein's equations to determine the equation of state is an algorithmic, direct problem. Determining the gravitational theory that gives rise to a prescribed equation of state is a much more challenging, inverse problem. We present a novel approach to solve this problem based on physics-informed neural networks. The resulting algorithm is not only data-driven but also informed by the physics of the Einstein's equations. We successfully apply it to theories with crossovers, first- and second-order phase transitions.
    
[^241]: 发展和部署教育领域人工智能产业标准：挑战、策略和未来方向

    Developing and Deploying Industry Standards for Artificial Intelligence in Education (AIED): Challenges, Strategies, and Future Directions

    [https://arxiv.org/abs/2403.14689](https://arxiv.org/abs/2403.14689)

    教育领域人工智能的发展需要制定和实施产业标准，以解决互操作性、可扩展性和道德治理等挑战。

    

    人工智能在教育领域的应用承诺通过提供个性化学习体验、自动化行政和教学任务以及降低内容创建成本来革新教育实践。然而，在开发和部署教育领域人工智能解决方案方面缺乏标准化实践导致生态系统分散，给互操作性、可扩展性和道德治理带来挑战。本文旨在解决在教育领域人工智能发展和实施产业标准的紧迫需求，提供对当前局势、挑战和克服这些障碍的策略方法的全面分析。我们开始通过研究AIED在不同教育环境中的各种应用，并确定缺乏标准化的关键领域，包括系统互操作性、本体映射、数据集成、评估和道德治理。

    arXiv:2403.14689v1 Announce Type: cross  Abstract: The adoption of Artificial Intelligence in Education (AIED) holds the promise of revolutionizing educational practices by offering personalized learning experiences, automating administrative and pedagogical tasks, and reducing the cost of content creation. However, the lack of standardized practices in the development and deployment of AIED solutions has led to fragmented ecosystems, which presents challenges in interoperability, scalability, and ethical governance. This article aims to address the critical need to develop and implement industry standards in AIED, offering a comprehensive analysis of the current landscape, challenges, and strategic approaches to overcome these obstacles. We begin by examining the various applications of AIED in various educational settings and identify key areas lacking in standardization, including system interoperability, ontology mapping, data integration, evaluation, and ethical governance. Then, 
    
[^242]: ReAct遇上ActRe：对比性自训练中的代理轨迹自动标注

    ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training

    [https://arxiv.org/abs/2403.14589](https://arxiv.org/abs/2403.14589)

    提出了A$^3$T框架，通过ActRe提示代理实现了ReAct风格代理对代理轨迹的自主标注，同时增强了新的轨迹合成能力。

    

    arXiv:2403.14589v1 公告类型：新 文摘：语言代理通过与基础模型推理展示了自主决策能力。最近，人们致力于通过多步推理和行动轨迹作为训练数据来训练语言代理以提高性能。然而，收集这样的轨迹仍需要相当大的人力，无论是通过人工标注还是实施多样化提示框架。在这项工作中，我们提出了A$^3$T，一个允许以ReAct风格自主注释代理轨迹的框架。其中心是一个ActRe提示代理，它解释任意动作的原因。当随机抽取外部动作时，ReAct风格代理可以查询ActRe代理以获取其文本理由。新颖的轨迹然后通过将ActRe的后验推理前置到抽样动作中进行综合合成。通过这种方式，ReAct风格代理可执行

    arXiv:2403.14589v1 Announce Type: new  Abstract: Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent exe
    
[^243]: 线性时间序列预测模型分析

    An Analysis of Linear Time Series Forecasting Models

    [https://arxiv.org/abs/2403.14587](https://arxiv.org/abs/2403.14587)

    分析了线性时间序列预测模型，证明了几种流行的线性模型变体等效于标准的线性回归，提供了实验证据支持这一结论。

    

    尽管简单，线性模型在时间序列预测中表现良好，即使与更深层次和更昂贵的模型进行对比也是如此。已经提出了许多线性模型的变体，通常包括某种形式的特征规范化，可以改善模型的泛化能力。在本文中，我们分析了使用这些线性模型架构可以表达的函数集合。通过这样做，我们展示了几种流行的线性时间序列预测模型变体等效，并在功能上无法区分标准的非约束线性回归。我们为每种线性变体描述了模型类别。我们证明了每个模型都可以重新解释为在适当扩充的特征集上的非约束线性回归，因此在使用均方损失函数时可以得到封闭形式的解决方案。我们提供实验证据表明，待检查的模型学习到了几乎相同的解决方案。

    arXiv:2403.14587v1 Announce Type: new  Abstract: Despite their simplicity, linear models perform well at time series forecasting, even when pitted against deeper and more expensive models. A number of variations to the linear model have been proposed, often including some form of feature normalisation that improves model generalisation. In this paper we analyse the sets of functions expressible using these linear model architectures. In so doing we show that several popular variants of linear models for time series forecasting are equivalent and functionally indistinguishable from standard, unconstrained linear regression. We characterise the model classes for each linear variant. We demonstrate that each model can be reinterpreted as unconstrained linear regression over a suitably augmented feature set, and therefore admit closed-form solutions when using a mean-squared loss function. We provide experimental evidence that the models under inspection learn nearly identical solutions, a
    
[^244]: 一项关于基于概念方法改进模型的调查

    A survey on Concept-based Approaches For Model Improvement

    [https://arxiv.org/abs/2403.14566](https://arxiv.org/abs/2403.14566)

    基于概念方法对深度神经网络进行解释性改进，提供了人类可理解的决策解释，使得检测伪关联和固有偏见成为可能。

    

    最近研究的重点已经从仅仅提高深度神经网络（DNN）在各种任务中的性能转变为使DNN更易解释给人类。可解释人工智能（XAI）领域已经观察到各种技术，包括基于显著性和基于概念的方法。基于概念的方法用所谓的概念在简单的人类可理解术语中解释模型的决策。概念是数据的人类可解释单元，是人类思维的基石。用概念的解释能够检测到伪关联、固有偏见或聪明汉。随着基于概念的解释的出现，出现了各种概念表示方法和自动概念发现算法。一些最近的方法使用概念进行事后模型解缠评估，而其他人使用它们进行事前训练。基于概念的方法是新的，有许多表示方法。

    arXiv:2403.14566v1 Announce Type: new  Abstract: The focus of recent research has shifted from merely increasing the Deep Neural Networks (DNNs) performance in various tasks to DNNs, which are more interpretable to humans. The field of eXplainable Artificial Intelligence (XAI) has observed various techniques, including saliency-based and concept-based approaches. Concept-based approaches explain the model's decisions in simple human understandable terms called Concepts. Concepts are human interpretable units of data and are the thinking ground of humans. Explanations in terms of concepts enable detecting spurious correlations, inherent biases, or clever-hans. With the advent of concept-based explanations, there have been various concept representation methods and automatic concept discovery algorithms. Some recent methods use concepts for post-hoc model disentanglement evaluation, while others use them for ante-hoc training. The concept-based approaches are new, with many representatio
    
[^245]: 通过互补的类内和类间Mixup提高图像分类准确性

    Improving Image Classification Accuracy through Complementary Intra-Class and Inter-Class Mixup

    [https://arxiv.org/abs/2403.14137](https://arxiv.org/abs/2403.14137)

    通过提出一种新的mixup方法，该方法针对类内混合进行了优化，提高了图像分类准确性。

    

    MixUp及其变体，在图像分类任务中存在两个关键限制。首先，它们通常忽略了同一类别内的混合（类内Mixup），导致同一类别样本之间的关系被低估。其次，尽管这些方法通过不同类别之间的混合（类间Mixup）有效增强了类间可分离性，但在通过其混合操作改进类内凝聚力方面表现不足，限制了它们的分类性能。为了解决这些问题，我们提出了一种新颖的mixup方法和一个全面的综合解决方案。我们的mixup方法专门针对常常被忽视的类内Mixup，以加强类内凝聚性-这是目前的mixup技术没有提供的特性。对于每个小批量，我们的方法利用小批量中每个类别的未增强原始图像的特征表示来生成a

    arXiv:2403.14137v1 Announce Type: cross  Abstract: MixUp and its variants, such as Manifold MixUp, have two key limitations in image classification tasks. First, they often neglect mixing within the same class (intra-class mixup), leading to an underutilization of the relationships among samples within the same class. Second, although these methods effectively enhance inter-class separability by mixing between different classes (inter-class mixup), they fall short in improving intra-class cohesion through their mixing operations, limiting their classification performance. To tackle these issues, we propose a novel mixup method and a comprehensive integrated solution.Our mixup approach specifically targets intra-class mixup, an aspect commonly overlooked, to strengthen intra-class cohesion-a feature not provided by current mixup techniques.For each mini-batch, our method utilizes feature representations of unaugmented original images from each class within the mini-batch to generate a s
    
[^246]: C-TPT：通过文本特征离散性的校准测试时提示调整视觉-语言模型

    C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion

    [https://arxiv.org/abs/2403.14119](https://arxiv.org/abs/2403.14119)

    本文研究了在测试时提示调整过程中通过利用CLIP的固有属性来探讨校准的方法，发现提示选择显著影响了CLIP中的校准，其中导致更高文本特征离散性的提示会产生更好校准的预测。

    

    在深度学习中，测试时适应已经引起了人们的关注，作为一种在不需要标记数据的情况下对模型进行微调的方法。一个主要的例证是最近提出的用于大规模视觉-语言模型（如CLIP）的测试时提示调整。然而，这些提示主要是为了提高准确性而开发的，忽视了校准的重要性——量化预测不确定性的关键方面。然而，传统的校准方法依赖大量标记数据，这使得它们在测试时场景下不切实际。为此，本文通过利用CLIP的固有属性，在测试时提示调整过程中探讨校准。通过一系列观察，我们发现提示选择显著影响了CLIP中的校准，其中导致更高文本特征离散性的提示会产生更好校准的预测。

    arXiv:2403.14119v1 Announce Type: cross  Abstract: In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration-a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion
    
[^247]: 可持续数据中心实时减少碳足迹

    Carbon Footprint Reduction for Sustainable Data Centers in Real-Time

    [https://arxiv.org/abs/2403.14092](https://arxiv.org/abs/2403.14092)

    我们提出了一种Data Center Carbon Footprint Reduction (DC-CFR) 多代理强化学习（MARL）框架，旨在实时优化数据中心以减少碳足迹。

    

    随着机器学习工作负载显著增加能源消耗，碳排放低的可持续数据中心正成为全球政府和企业关注的重点。为了实现这一目标，需要在冷却和IT负载中进行功耗优化的范式转变，基于可再生能源在电网中的可用性来调整灵活负载，利用数据中心不间断电源中的电池存储，使用协作代理。这些优化策略之间的复杂关系以及它们对变化的外部因素（如天气和电网碳排放强度）的依赖使得这是一个困难的问题。目前缺乏一个能够在动态实际环境中同时优化所有这些目标的实时控制器。我们提出了一种数据中心碳足迹减少（DC-CFR）多代理强化学习（MARL）框架，能够优化多个角度的数据中心。

    arXiv:2403.14092v1 Announce Type: cross  Abstract: As machine learning workloads significantly increase energy consumption, sustainable data centers with low carbon emissions are becoming a top priority for governments and corporations worldwide. This requires a paradigm shift in optimizing power consumption in cooling and IT loads, shifting flexible loads based on the availability of renewable energy in the power grid, and leveraging battery storage from the uninterrupted power supply in data centers, using collaborative agents. The complex association between these optimization strategies and their dependencies on variable external factors like weather and the power grid carbon intensity makes this a hard problem. Currently, a real-time controller to optimize all these goals simultaneously in a dynamic real-world setting is lacking. We propose a Data Center Carbon Footprint Reduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that optimizes data centers for the mult
    
[^248]: 使用BART从推文中提取情绪短语

    Extracting Emotion Phrases from Tweets using BART

    [https://arxiv.org/abs/2403.14050](https://arxiv.org/abs/2403.14050)

    本文提出了一种基于BART的情感分析方法，利用问答框架从文本中提取特定情绪短语，并通过分类器预测答案跨度位置，实现对情绪短语的精确提取。

    

    情感分析是一项旨在识别和提取文本中情绪方面的自然语言处理任务。然而，许多现有的情感分析方法主要是对文本的整体极性进行分类，忽略了传达情绪的具体短语。在本文中，我们应用了一种基于问答框架的情感分析方法。我们利用双向自回归变换器（BART），一个预训练的序列到序列模型，从给定文本中提取放大给定情感极性的短语。我们创建一个自然语言问题，确定要提取的特定情绪，然后引导BART专注于文本中相关的情感线索。我们在BART中使用一个分类器来预测文本中答案跨度的开始和结束位置，从而帮助确定提取的情绪短语的精确边界。

    arXiv:2403.14050v2 Announce Type: replace  Abstract: Sentiment analysis is a natural language processing task that aims to identify and extract the emotional aspects of a text. However, many existing sentiment analysis methods primarily classify the overall polarity of a text, overlooking the specific phrases that convey sentiment. In this paper, we applied an approach to sentiment analysis based on a question-answering framework. Our approach leverages the power of Bidirectional Autoregressive Transformer (BART), a pre-trained sequence-to-sequence model, to extract a phrase from a given text that amplifies a given sentiment polarity. We create a natural language question that identifies the specific emotion to extract and then guide BART to pay attention to the relevant emotional cues in the text. We use a classifier within BART to predict the start and end positions of the answer span within the text, which helps to identify the precise boundaries of the extracted emotion phrase. Our
    
[^249]: 整合可穿戴传感器数据和自我报告日记用于个性化情感预测

    Integrating Wearable Sensor Data and Self-reported Diaries for Personalized Affect Forecasting

    [https://arxiv.org/abs/2403.13841](https://arxiv.org/abs/2403.13841)

    本论文提出了一种整合了可穿戴传感器数据和自我报告日记的多模态深度学习模型，用于情感状态的预测。

    

    情绪状态作为情感的指标对整体健康至关重要，因此在其发作前准确预测是至关重要的。目前的研究主要集中在使用来自可穿戴和移动设备的数据进行与短期情感检测。这些研究通常专注于客观的感官测量，往往忽略其他形式的自我报告信息，如日记和笔记。在本文中，我们提出了一种用于情感状态预测的多模态深度学习模型。该模型结合了一个transformer编码器和一个预训练语言模型，实现了客观指标和自我报告日记的综合分析。为了验证我们的模型，我们进行了一项纵向研究，招募了大学生并在一年内对其进行监测，收集了包括生理、环境、睡眠、代谢和身体活动参数在内的广泛数据集，同时参与者提供了开放式文本日记。

    arXiv:2403.13841v1 Announce Type: cross  Abstract: Emotional states, as indicators of affect, are pivotal to overall health, making their accurate prediction before onset crucial. Current studies are primarily centered on immediate short-term affect detection using data from wearable and mobile devices. These studies typically focus on objective sensory measures, often neglecting other forms of self-reported information like diaries and notes. In this paper, we propose a multimodal deep learning model for affect status forecasting. This model combines a transformer encoder with a pre-trained language model, facilitating the integrated analysis of objective metrics and self-reported diaries. To validate our model, we conduct a longitudinal study, enrolling college students and monitoring them over a year, to collect an extensive dataset including physiological, environmental, sleep, metabolic, and physical activity parameters, alongside open-ended textual diaries provided by the partici
    
[^250]: 分散式联邦学习：在信息分享不完全情况下的模型更新跟踪

    Decentralized Federated Learning: Model Update Tracking Under Imperfect Information Sharing

    [https://arxiv.org/abs/2403.13247](https://arxiv.org/abs/2403.13247)

    提出了适用于存在嘈杂通信通道的分散式联邦学习算法FedNMUT，通过梯度跟踪减小数据异质性影响并最小化通信开销，在噪声参数中实现客户端共识，优于现有方法和传统参数混合方法。

    

    提出了一种新颖的分散式嘈杂模型更新跟踪联邦学习算法（FedNMUT），该算法旨在在反映信息交换不完整的嘈杂通信通道存在的情况下有效运行。该算法使用梯度跟踪来最小化数据异质性的影响，同时最小化通信开销。所提出的算法将噪声纳入其参数中，以模拟嘈杂通信渠道的条件，从而通过通信图拓扑在这种具有挑战性的环境中实现客户端之间的共识。FedNMUT将参数共享和噪声纳入作为优先考虑，以增强分散式学习系统对嘈杂通信的抵抗力。通过理论和实证验证表明，在性能上，FedNMUT相对于现有的最先进方法和传统的参数混合方法表现更优越。

    arXiv:2403.13247v1 Announce Type: new  Abstract: A novel Decentralized Noisy Model Update Tracking Federated Learning algorithm (FedNMUT) is proposed, which is tailored to function efficiently in the presence of noisy communication channels that reflect imperfect information exchange. This algorithm uses gradient tracking to minimize the impact of data heterogeneity while minimizing communication overhead. The proposed algorithm incorporates noise into its parameters to mimic the conditions of noisy communication channels, thereby enabling consensus among clients through a communication graph topology in such challenging environments. FedNMUT prioritizes parameter sharing and noise incorporation to increase the resilience of decentralized learning systems against noisy communications. Through theoretical and empirical validation, it is demonstrated that the performance of FedNMUT is superior compared to the existing state-of-the-art methods and conventional parameter-mixing approaches 
    
[^251]: 使用高斯过程从偏好和选择中学习的教程

    A tutorial on learning from preferences and choices with Gaussian Processes

    [https://arxiv.org/abs/2403.11782](https://arxiv.org/abs/2403.11782)

    提供了一个使用高斯过程进行偏好学习的框架，能够将理性原则融入学习过程，涵盖了多种偏好学习模型。

    

    偏好建模位于经济学、决策理论、机器学习和统计学的交叉点。通过理解个体的偏好及其选择方式，我们可以构建更接近他们期望的产品，为跨领域的更高效、个性化应用铺平道路。此教程的目标是提供一个连贯、全面的偏好学习框架，使用高斯过程演示如何将理性原则（来自经济学和决策理论）无缝地纳入学习过程中。通过合适地定制似然函数，这一框架使得能够构建涵盖随机效用模型、辨识限制和对象和标签偏好的多重冲突效用情景的偏好学习模型。

    arXiv:2403.11782v1 Announce Type: new  Abstract: Preference modelling lies at the intersection of economics, decision theory, machine learning and statistics. By understanding individuals' preferences and how they make choices, we can build products that closely match their expectations, paving the way for more efficient and personalised applications across a wide range of domains. The objective of this tutorial is to present a cohesive and comprehensive framework for preference learning with Gaussian Processes (GPs), demonstrating how to seamlessly incorporate rationality principles (from economics and decision theory) into the learning process. By suitably tailoring the likelihood function, this framework enables the construction of preference learning models that encompass random utility models, limits of discernment, and scenarios with multiple conflicting utilities for both object- and label-preference. This tutorial builds upon established research while simultaneously introducin
    
[^252]: 使用四元值神经网络和四元反向传播进行时间序列压缩

    Time Series Compression using Quaternion Valued Neural Networks and Quaternion Backpropagation

    [https://arxiv.org/abs/2403.11722](https://arxiv.org/abs/2403.11722)

    提出了一种使用四元值神经网络和四元反向传播进行时间序列压缩的方法，在保留特征之间关系的同时在故障分类中展现出潜力。

    

    我们提出了一种新颖的四元数时间序列压缩方法，将长时间序列划分为数据段，提取这些块的最小值、最大值、均值和标准差作为代表性特征，并将它们封装在四元数中，得到一个四元数值时间序列。这个时间序列使用四元数值神经网络层进行处理，我们旨在通过使用哈密顿积来保留这些特征之间的关系。为了训练这个四元数神经网络，我们推导出使用GHR微积分的四元数反向传播，这对于四元数空间中的有效乘积和链规则是必需的。此外，我们研究了推导更新规则与自动微分之间的联系。我们将我们提出的压缩方法应用于Tennessee Eastman数据集，在两个设置中使用压缩数据进行故障分类：一个完全监督的设置和另一个半监督的设置。

    arXiv:2403.11722v1 Announce Type: new  Abstract: We propose a novel quaternionic time-series compression methodology where we divide a long time-series into segments of data, extract the min, max, mean and standard deviation of these chunks as representative features and encapsulate them in a quaternion, yielding a quaternion valued time-series. This time-series is processed using quaternion valued neural network layers, where we aim to preserve the relation between these features through the usage of the Hamilton product. To train this quaternion neural network, we derive quaternion backpropagation employing the GHR calculus, which is required for a valid product and chain rule in quaternion space. Furthermore, we investigate the connection between the derived update rules and automatic differentiation. We apply our proposed compression method on the Tennessee Eastman Dataset, where we perform fault classification using the compressed data in two settings: a fully supervised one and i
    
[^253]: 移动边缘计算中应用部署问题的基于学习的解决方案

    A learning-based solution approach to the application placement problem in mobile edge computing under uncertainty

    [https://arxiv.org/abs/2403.11259](https://arxiv.org/abs/2403.11259)

    通过机器学习模型将用户请求分配给服务器，以解决在移动边缘计算中应用部署问题的二阶段随机规划。

    

    在移动边缘计算服务器中放置应用程序是一个复杂的挑战，涉及许多服务器、用户及其请求。现有算法需要很长时间解决具有重大不确定性情景的高维问题。因此，需要一种有效的方法来最大化服务质量，同时考虑所有技术约束。其中一种方法是机器学习，它模拟了在边缘服务器中部署应用程序的最佳解决方案。机器学习模型预计将学习如何根据用户和服务器的空间位置将用户请求分配给服务器。本研究将问题构建为二阶段随机规划。通过变化参数如用户位置、请求速率和解决优化模型生成足够数量的训练记录。然后，基于每个用户距离可用服务器的距离特征，

    arXiv:2403.11259v1 Announce Type: cross  Abstract: Placing applications in mobile edge computing servers presents a complex challenge involving many servers, users, and their requests. Existing algorithms take a long time to solve high-dimensional problems with significant uncertainty scenarios. Therefore, an efficient approach is required to maximize the quality of service while considering all technical constraints. One of these approaches is machine learning, which emulates optimal solutions for application placement in edge servers. Machine learning models are expected to learn how to allocate user requests to servers based on the spatial positions of users and servers. In this study, the problem is formulated as a two-stage stochastic programming. A sufficient amount of training records is generated by varying parameters such as user locations, their request rates, and solving the optimization model. Then, based on the distance features of each user from the available servers and 
    
[^254]: 使用选项的强化学习

    Reinforcement Learning with Options

    [https://arxiv.org/abs/2403.10855](https://arxiv.org/abs/2403.10855)

    本论文提出了使用选项的分层强化学习方法，通过构建Hierarchical Policy learning来解决高维复杂环境中学习的问题。

    

    这篇论文旨在探索强化学习领域，并建立在现有方法的基础上，提出改进的方法来解决在高维复杂环境中学习的问题。它通过以分层方式分解学习任务，即分层强化学习，来实现这些目标。第一章我们熟悉马尔可夫决策过程框架，并展示一些最近的技术。之后，我们构建了分层策略学习，以应对单个基本策略的局限性。这个层次结构由顶层的管理代理和底层的员工代理组成。在最后一章，也是本论文的核心部分，我们尝试独立于管理级别学习层次结构的低层元素，即所谓的“Eigenoption”。基于环境的图结构。

    arXiv:2403.10855v1 Announce Type: new  Abstract: The current thesis aims to explore the reinforcement learning field and build on existing methods to produce improved ones to tackle the problem of learning in high-dimensional and complex environments. It addresses such goals by decomposing learning tasks in a hierarchical fashion known as Hierarchical Reinforcement Learning.   We start in the first chapter by getting familiar with the Markov Decision Process framework and presenting some of its recent techniques that the following chapters use. We then proceed to build our Hierarchical Policy learning as an answer to the limitations of a single primitive policy. The hierarchy is composed of a manager agent at the top and employee agents at the lower level.   In the last chapter, which is the core of this thesis, we attempt to learn lower-level elements of the hierarchy independently of the manager level in what is known as the "Eigenoption". Based on the graph structure of the environm
    
[^255]: LightIt：扩散模型的照明建模与控制

    LightIt: Illumination Modeling and Control for Diffusion Models

    [https://arxiv.org/abs/2403.10615](https://arxiv.org/abs/2403.10615)

    LightIt提出了一种用于图像生成的显式照明控制方法，通过条件生成来实现对图像生成的照明控制，同时训练了一个身份保持的重照模型。

    

    我们介绍了LightIt，这是一种用于图像生成的显式照明控制方法。最近的生成方法缺乏照明控制，而这对于图像生成的许多艺术方面至关重要，比如设置整体情绪或电影外观。为了克服这些限制，我们提出在生成过程中以遮蔽和法线图为条件。我们使用单次反射遮蔽来建模照明，包括投射阴影。我们首先训练一个遮蔽估计模块来生成真实世界图像和遮蔽对的数据集。然后，我们使用估计的遮蔽和法线作为输入训练控制网络。我们的方法展示了在许多场景中高质量的图像生成和照明控制。此外，我们使用我们生成的数据集来训练一个保持身份的重照模型，以图像和目标遮蔽为条件。我们的方法是第一个能够生成具有可控、一致性的图像的方法。

    arXiv:2403.10615v1 Announce Type: cross  Abstract: We introduce LightIt, a method for explicit illumination control for image generation. Recent generative methods lack lighting control, which is crucial to numerous artistic aspects of image generation such as setting the overall mood or cinematic appearance. To overcome these limitations, we propose to condition the generation on shading and normal maps. We model the lighting with single bounce shading, which includes cast shadows. We first train a shading estimation module to generate a dataset of real-world images and shading pairs. Then, we train a control network using the estimated shading and normals as input. Our method demonstrates high-quality image generation and lighting control in numerous scenes. Additionally, we use our generated dataset to train an identity-preserving relighting model, conditioned on an image and a target shading. Our method is the first that enables the generation of images with controllable, consisten
    
[^256]: 一个白盒神经网络的概念框架

    A Conceptual Framework For White Box Neural Networks

    [https://arxiv.org/abs/2403.09863](https://arxiv.org/abs/2403.09863)

    引入语义特征作为通用概念框架，实现了完全可解释的神经网络层，为白盒神经网络的范式转变开辟了新的可能性。

    

    本文引入语义特征作为完全可解释神经网络层的通用概念框架。一个充分动机的MNIST相关子问题的概念验证模型包括4个这样的层，总共4800个可学习参数。该模型易于解释，无需任何形式的对抗训练即可实现人类水平的对抗测试准确率，需要较少的超参数调节，并且可以在单个CPU上快速训练。该技术的通用性承诺为彻底民主化和真正通用的白盒神经网络带来了希望。代码可在https://github.com/314-Foundation/white-box-nn找到。

    arXiv:2403.09863v1 Announce Type: cross  Abstract: This paper introduces semantic features as a general conceptual framework for fully explainable neural network layers. A well-motivated proof of concept model for relevant subproblem of MNIST consists of 4 such layers with the total of 4.8K learnable parameters. The model is easily interpretable, achieves human-level adversarial test accuracy with no form of adversarial training, requires little hyperparameter tuning and can be quickly trained on a single CPU. The general nature of the technique bears promise for a paradigm shift towards radically democratised and truly generalizable white box neural networks. The code is available at https://github.com/314-Foundation/white-box-nn
    
[^257]: 不要以外表判断: 用于视频识别的运动一致增强

    Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition

    [https://arxiv.org/abs/2403.09506](https://arxiv.org/abs/2403.09506)

    本研究提出了一种名为运动一致增强（MCA）的数据增强方法，通过引入外观变化来鼓励模型优先考虑视频中的运动信息，而不是静态外观。

    

    当前目标识别中的训练流程在数据增强时忽略了色调抖动，因为它不仅会带来对分类有害的外观变化，而且在实践中实现也是低效的。本研究探讨了色调变化在视频识别中的影响，并发现这种变化是有益的，因为对于包含运动信息的视频来说，静态外观不是那么重要。基于这一观察结果，我们提出了一种用于视频识别的数据增强方法，名为运动一致增强（MCA），它在视频中引入外观变化，隐式鼓励模型优先考虑运动模式，而不是静态外观。具体来说，我们提出了一种名为SwapMix的操作，用于高效修改视频样本的外观，并引入了变异对齐（VA）来解决SwapMix引起的分布偏移，迫使模型去学习

    arXiv:2403.09506v1 Announce Type: cross  Abstract: Current training pipelines in object recognition neglect Hue Jittering when doing data augmentation as it not only brings appearance changes that are detrimental to classification, but also the implementation is inefficient in practice. In this study, we investigate the effect of hue variance in the context of video recognition and find this variance to be beneficial since static appearances are less important in videos that contain motion information. Based on this observation, we propose a data augmentation method for video recognition, named Motion Coherent Augmentation (MCA), that introduces appearance variation in videos and implicitly encourages the model to prioritize motion patterns, rather than static appearances. Concretely, we propose an operation SwapMix to efficiently modify the appearance of video samples, and introduce Variation Alignment (VA) to resolve the distribution shift caused by SwapMix, enforcing the model to le
    
[^258]: 深度限价订单簿预测

    Deep Limit Order Book Forecasting

    [https://arxiv.org/abs/2403.09267](https://arxiv.org/abs/2403.09267)

    该研究利用深度学习方法预测纳斯达克交易所股票的限价订单簿中间价格变动，提出了一个创新的操作框架来评估预测的实用性。

    

    我们利用尖端的深度学习方法探索了在纳斯达克交易所上交易的一组异质股票的高频限价订单簿中间价格变动的可预测性。在此过程中，我们发布了“LOBFrame”，一个开源代码库，可以高效处理大规模限价订单簿数据，并定量评估最先进的深度学习模型的预测能力。我们的结果是双重的。我们证明股票的微观结构特征影响深度学习方法的有效性，并且它们的高预测能力不一定对应可操作的交易信号。我们认为传统的机器学习指标未能充分评估限价订单簿环境中预测的质量。作为替代，我们提出了一个创新的操作框架，通过专注于准确预测的概率来评估预测的实用性。

    arXiv:2403.09267v1 Announce Type: cross  Abstract: We exploit cutting-edge deep learning methodologies to explore the predictability of high-frequency Limit Order Book mid-price changes for a heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we release `LOBFrame', an open-source code base, to efficiently process large-scale Limit Order Book data and quantitatively assess state-of-the-art deep learning models' forecasting capabilities. Our results are twofold. We demonstrate that the stocks' microstructural characteristics influence the efficacy of deep learning methods and that their high forecasting power does not necessarily correspond to actionable trading signals. We argue that traditional machine learning metrics fail to adequately assess the quality of forecasts in the Limit Order Book context. As an alternative, we propose an innovative operational framework that assesses predictions' practicality by focusing on the probability of accurately forecasting com
    
[^259]: 知识图谱大型语言模型（KG-LLM）用于链接预测

    Knowledge Graph Large Language Model (KG-LLM) for Link Prediction

    [https://arxiv.org/abs/2403.07311](https://arxiv.org/abs/2403.07311)

    该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。

    

    在知识图谱分析领域，预测知识图谱（KGs）内多个链接的任务是一个挑战，由于自然语言处理（NLP）和知识图嵌入技术的进步，这一挑战变得越来越可解决。本文介绍了一种新的方法，即知识图谱大型语言模型框架（KG-LLM），该框架利用关键的NLP范例，包括思维链提示（CoT）和上下文学习（ICL），以增强知识图谱中的多跳链接预测。通过将KG转换为CoT提示，我们的框架旨在识别并学习实体及其相互关系的潜在表示。为了展示KG-LLM框架的有效性，我们在该框架内微调了三种主要的大型语言模型（LLMs），同时采用了非ICL和ICL任务进行全面评估。此外，我们探讨了该框架为LLMs提供零次尝试能力的潜力。

    arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
    
[^260]: 通过监督预训练和重要性机制微调改进低资源知识追踪任务

    Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning

    [https://arxiv.org/abs/2403.06725](https://arxiv.org/abs/2403.06725)

    本文提出了名为LoReKT的低资源知识追踪框架，通过监督预训练和微调重要性机制，旨在从丰富资源的KT数据集中学习可转移的参数和表示来改进低资源知识追踪任务。

    

    知识追踪（KT）旨在基于学生的历史互动来估计他们的知识掌握程度。最近，基于深度学习的KT（DLKT）方法在KT任务中取得了令人印象深刻的表现。然而，由于各种原因，如预算限制和隐私问题，许多实际场景中观察到的互动非常有限，即低资源KT数据集。直接在低资源KT数据集上训练DLKT模型可能会导致过拟合，并且很难选择适当的深度神经架构。因此，在本文中，我们提出了一个名为LoReKT的低资源KT框架来应对上述挑战。受盛行的“预训练和微调”范式的启发，我们旨在在预训练阶段从丰富资源的KT数据集中学习可转移的参数和表示。

    arXiv:2403.06725v1 Announce Type: cross  Abstract: Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets. Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges. Inspired by the prevalent "pre-training and fine-tuning" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subseque
    
[^261]: 分布生成增强公平面部属性分类

    Distributionally Generative Augmentation for Fair Facial Attribute Classification

    [https://arxiv.org/abs/2403.06606](https://arxiv.org/abs/2403.06606)

    该论文提出了一种新颖的分布生成增强方法，用于在偏见数据上训练公平的面部属性分类模型，无需额外标注虚假属性，通过生成模型识别潜在的虚假属性，并在图像空间中显示，以提高模型可解释性。

    

    面部属性分类（FAC）在广泛的应用中具有重要的潜力。然而，传统方法训练的FAC模型可能存在不公平性，因为在不同的数据子群体中展示准确性不一致。这种不公平性主要归因于数据中的偏见，其中一些虚假属性（例如，男性）在统计上与目标属性（例如，微笑）相关。大多数现有的注重公平性的方法依赖于虚假属性的标签，但这在实践中可能无法获得。本文提出了一种新颖的基于生成的两阶段框架，用于在带有偏见的数据上训练公平的FAC模型，而无需额外注释。首先，我们基于生成模型识别潜在的虚假属性。值得注意的是，通过明确展示图像空间中的虚假属性，它增强了可解释性。随后，对于每个图像，我们首先编辑虚假属性，随机抽样一定程度，

    arXiv:2403.06606v1 Announce Type: cross  Abstract: Facial Attribute Classification (FAC) holds substantial promise in widespread applications. However, FAC models trained by traditional methodologies can be unfair by exhibiting accuracy inconsistencies across varied data subpopulations. This unfairness is largely attributed to bias in data, where some spurious attributes (e.g., Male) statistically correlate with the target attribute (e.g., Smiling). Most of existing fairness-aware methods rely on the labels of spurious attributes, which may be unavailable in practice. This work proposes a novel, generation-based two-stage framework to train a fair FAC model on biased data without additional annotation. Initially, we identify the potential spurious attributes based on generative models. Notably, it enhances interpretability by explicitly showing the spurious attributes in image space. Following this, for each image, we first edit the spurious attributes with a random degree sampled from
    
[^262]: 具有扩散净化的分离数据一致性的图像恢复

    Decoupled Data Consistency with Diffusion Purification for Image Restoration

    [https://arxiv.org/abs/2403.06054](https://arxiv.org/abs/2403.06054)

    通过分离反向过程和数据一致性步骤，提出了一种新颖的基于扩散的图像恢复求解器。

    

    最近，扩散模型作为一种强大的深度生成先验类别已经引起了人们的关注，由于其出色地建模数据分布的能力，在各种图像恢复任务中表现出色。为了解决图像恢复问题，许多现有技术通过将额外的似然梯度步骤纳入到扩散模型的反向采样过程中来实现数据一致性。然而，这些额外的梯度步骤对于实际应用中存在挑战，因为它们造成了巨大的计算开销，从而增加了推理时间。当使用加速的扩散模型采样器时，这些额外的步骤还会导致额外的困难，因为数据一致性步骤的数量受限于反向采样步骤的数量。在这项工作中，我们提出了一种新颖的基于扩散的图像恢复求解器，通过将反向过程与数据一致性步骤分离来解决这些问题。我们的方法涉及

    arXiv:2403.06054v1 Announce Type: cross  Abstract: Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involv
    
[^263]: 利用潜在对抗训练防御未预见的故障模式

    Defending Against Unforeseen Failure Modes with Latent Adversarial Training

    [https://arxiv.org/abs/2403.05030](https://arxiv.org/abs/2403.05030)

    本研究利用潜在对抗训练（LAT）来防御AI系统中未预见的故障模式，通过利用网络实际用于预测的压缩、抽象和结构化概念的潜在表示，有效清除了恶意软件和对抗性攻击。

    

    人工智能系统有时在部署后会展示出有害的意外行为。尽管开发人员进行了大量诊断和调试，这种情况经常发生。由于攻击面非常广泛，从模型中减少风险具有挑战性。耗尽地搜索可能导致模型失败的输入是不可行的。红队和对抗训练（AT）通常用于使人工智能系统更加健壮。然而，它们并不足以避免许多与对抗训练不同的真实世界故障模式。在这项工作中，我们利用潜在对抗训练（LAT）来防御漏洞，而无需生成引发这些漏洞的输入。LAT利用网络实际用于预测的压缩、抽象和结构化概念的潜在表示。我们使用LAT来清除恶意软件并防御针对保留类别的对抗性攻击。我们展示在图像分类、文本分类

    arXiv:2403.05030v1 Announce Type: cross  Abstract: AI systems sometimes exhibit harmful unintended behaviors post-deployment. This is often despite extensive diagnostics and debugging by developers. Minimizing risks from models is challenging because the attack surface is so large. It is not tractable to exhaustively search for inputs that may cause a model to fail. Red-teaming and adversarial training (AT) are commonly used to make AI systems more robust. However, they have not been sufficient to avoid many real-world failure modes that differ from the ones adversarially trained on. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use LAT to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classifi
    
[^264]: 限制贝叶斯神经网络

    Restricted Bayesian Neural Network

    [https://arxiv.org/abs/2403.04810](https://arxiv.org/abs/2403.04810)

    本研究提出了限制贝叶斯神经网络的新架构，显著减少了网络存储空间复杂性，并引入了一种能够有效处理不确定性的算法，确保在目标函数缺乏完美凸性时稳健地收敛至全局最优解。

    

    现代深度学习工具在解决复杂问题方面非常有效。然而，它们作为黑盒模型的运行方式增加了预测的不确定性。此外，它们面临着各种挑战，包括在大型网络中需要大量存储空间、过拟合、欠拟合、梯度消失等问题。本研究探讨了贝叶斯神经网络的概念，提出了一种能够显著减少网络存储空间复杂性的新型架构。此外，我们介绍了一种能够有效处理不确定性的算法，确保稳健的收敛值，避免陷入局部最优解，尤其是当目标函数缺乏完美的凸性时。

    arXiv:2403.04810v1 Announce Type: cross  Abstract: Modern deep learning tools are remarkably effective in addressing intricate problems. However, their operation as black-box models introduces increased uncertainty in predictions. Additionally, they contend with various challenges, including the need for substantial storage space in large networks, issues of overfitting, underfitting, vanishing gradients, and more. This study explores the concept of Bayesian Neural Networks, presenting a novel architecture designed to significantly alleviate the storage space complexity of a network. Furthermore, we introduce an algorithm adept at efficiently handling uncertainties, ensuring robust convergence values without becoming trapped in local optima, particularly when the objective function lacks perfect convexity.
    
[^265]: 环境-固有维度差异对对抗脆弱性的影响

    Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability

    [https://arxiv.org/abs/2403.03967](https://arxiv.org/abs/2403.03967)

    通过环境-固有维度差异的概念，论文证明了维度差异使干净训练的模型更容易受到数据空间脱离流形方向的对抗扰动攻击。

    

    论文介绍了对机器学习模型的对抗攻击存在且对人类来说几乎无法察觉这一事实，在理论上仍然相当神秘。文章引入了两种对抗攻击的概念：自然或在流形上的攻击，这些攻击是可以被人类/神谕感知到的；非自然或脱离流形的攻击，这些攻击则无法被感知到。文章认为脱离流形的攻击存在是数据固有维度与环境维度之间的差异的必然结果。对于2层ReLU网络，我们证明了即使维度差异不影响从观测数据空间中抽取样本的泛化性能，它仍会使干净训练的模型更容易受到数据空间脱离流形方向的对抗扰动攻击。我们的主要结果提供了在/脱离流形攻击的$\ell_2,\ell_{\infty}$攻击强度与维度差异之间明确的关系。

    arXiv:2403.03967v1 Announce Type: new  Abstract: The existence of adversarial attacks on machine learning models imperceptible to a human is still quite a mystery from a theoretical perspective. In this work, we introduce two notions of adversarial attacks: natural or on-manifold attacks, which are perceptible by a human/oracle, and unnatural or off-manifold attacks, which are not. We argue that the existence of the off-manifold attacks is a natural consequence of the dimension gap between the intrinsic and ambient dimensions of the data. For 2-layer ReLU networks, we prove that even though the dimension gap does not affect generalization performance on samples drawn from the observed data space, it makes the clean-trained model more vulnerable to adversarial perturbations in the off-manifold direction of the data space. Our main results provide an explicit relationship between the $\ell_2,\ell_{\infty}$ attack strength of the on/off-manifold attack and the dimension gap.
    
[^266]: 使用扩散模型进行潜在数据集蒸馏

    Latent Dataset Distillation with Diffusion Models

    [https://arxiv.org/abs/2403.03881](https://arxiv.org/abs/2403.03881)

    这项研究提出了使用扩散模型进行潜在数据集蒸馏（LD3M），结合潜在空间中的扩散和数据集蒸馏的方法，以解决不同模型架构导致准确性下降和生成高分辨率图像的挑战。

    

    机器学习的有效性传统上依赖于越来越大的数据集的可用性。然而，大型数据集带来存储挑战，并且包含一些非影响力样本，在训练过程中可以被忽略而不影响模型最终的准确性。为了应对这些限制，出现了将数据集信息蒸馏成一组压缩样本（合成样本），即蒸馏数据集的概念。其中一个关键方面是选择用于连接原始和合成数据集的架构（通常是ConvNet）。然而，如果所使用的模型架构与蒸馏过程中使用的模型不同，则最终准确性会降低。另一个挑战是生成高分辨率图像，例如128x128及更高。

    arXiv:2403.03881v1 Announce Type: cross  Abstract: The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both chal
    
[^267]: BASS的再审视--利用统一语义图提升抽象摘要--一项复制研究

    A Second Look on BASS -- Boosting Abstractive Summarization with Unified Semantic Graphs -- A Replication Study

    [https://arxiv.org/abs/2403.02930](https://arxiv.org/abs/2403.02930)

    通过复制研究BASS框架，发现了与原始工作相比性能上的差异，并强调了撰写可复制论文的关键实践。

    

    我们展示了对BASS框架的详细复制研究，这是一个基于统一语义图概念的抽象摘要系统。我们的调查包括复制关键组件时遇到的挑战，以及一个消融研究来系统地隔离在复制新颖组件时根源于错误来源。我们的发现揭示了与原始工作相比性能上的差异。我们强调了即使是被合理省略的细节对于复制像BASS这样的先进框架的重要性，并强调了撰写可复制论文的关键实践。

    arXiv:2403.02930v1 Announce Type: new  Abstract: We present a detailed replication study of the BASS framework, an abstractive summarization system based on the notion of Unified Semantic Graphs. Our investigation includes challenges in replicating key components and an ablation study to systematically isolate error sources rooted in replicating novel components. Our findings reveal discrepancies in performance compared to the original work. We highlight the significance of paying careful attention even to reasonably omitted details for replicating advanced frameworks like BASS, and emphasize key practices for writing replicable papers.
    
[^268]: 不需要精确指导的学习：从低分辨率历史标签更新大规模高分辨率土地覆盖图

    Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels

    [https://arxiv.org/abs/2403.02746](https://arxiv.org/abs/2403.02746)

    提出了一个名为Paraformer的弱监督框架，通过低分辨率历史土地覆盖数据指导大规模高分辨率土地覆盖映射，设计了CNN-Transformer特征提取器来综合捕获局部和全局背景信息

    

    大规模高分辨率（HR）土地覆盖映射是调查地球表面和解决人类面临的许多挑战的重要任务。然而，由于复杂的地面细节、各种地貌和广泛地理区域内准确训练标签的稀缺性，这仍然是一项非平凡的任务。在本文中，我们提出了一个高效的弱监督框架（Paraformer），即低到高网络（L2HNet）V2，用于在低分辨率（LR）的易获历史土地覆盖数据指导大规模高分辨率土地覆盖映射。具体来说，现有的土地覆盖映射方法显示了CNN在保留局部地面细节方面的优势，但仍然存在在各种地貌中全局建模不足的问题。因此，我们设计了Paraformer中的并行CNN-Transformer特征提取器，包括一个无降采样CNN分支和一个Transformer分支，来共同捕获局部和全局背景信息

    arXiv:2403.02746v1 Announce Type: cross  Abstract: Large-scale high-resolution (HR) land-cover mapping is a vital task to survey the Earth's surface and resolve many challenges facing humanity. However, it is still a non-trivial task hindered by complex ground details, various landforms, and the scarcity of accurate training labels over a wide-span geographic area. In this paper, we propose an efficient, weakly supervised framework (Paraformer), a.k.a. Low-to-High Network (L2HNet) V2, to guide large-scale HR land-cover mapping with easy-access historical land-cover data of low resolution (LR). Specifically, existing land-cover mapping approaches reveal the dominance of CNNs in preserving local ground details but still suffer from insufficient global modeling in various landforms. Therefore, we design a parallel CNN-Transformer feature extractor in Paraformer, consisting of a downsampling-free CNN branch and a Transformer branch, to jointly capture local and global contextual informatio
    
[^269]: 强化学习中并行学习策略和未知安全约束

    Concurrent Learning of Policy and Unknown Safety Constraints in Reinforcement Learning

    [https://arxiv.org/abs/2402.15893](https://arxiv.org/abs/2402.15893)

    提出了一种同时学习安全RL控制策略和识别未知安全约束参数的新方法。

    

    强化学习（RL）在过去几十年中已经彻底改变了跨多个领域的决策制定。然而，在实际场景中部署RL策略面临着确保安全的关键挑战。传统的安全RL方法主要集中于将预定义的安全约束纳入到策略学习过程中。然而，在动态和不可预测的实际环境中，这种对预定义安全约束的依赖在安全控制RL任务中具有限制，因为这些约束可能无法得到或不够适应。为弥补这一差距，我们提出了一种新颖的方法，同时学习安全的RL控制策略并确定给定环境的未知安全约束参数。通过使用一个参数化的信号时间逻辑（pSTL）安全规范和一个小的初始标记数据集进行初始化，我们将问题构建为一个双层优化任务，巧妙地将受限策略op

    arXiv:2402.15893v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has revolutionized decision-making across a wide range of domains over the past few decades. Yet, deploying RL policies in real-world scenarios presents the crucial challenge of ensuring safety. Traditional safe RL approaches have predominantly focused on incorporating predefined safety constraints into the policy learning process. However, this reliance on predefined safety constraints poses limitations in dynamic and unpredictable real-world settings where such constraints may not be available or sufficiently adaptable. Bridging this gap, we propose a novel approach that concurrently learns a safe RL control policy and identifies the unknown safety constraint parameters of a given environment. Initializing with a parametric signal temporal logic (pSTL) safety specification and a small initial labeled dataset, we frame the problem as a bilevel optimization task, intricately integrating constrained policy op
    
[^270]: 一种用于时空图迁移学习的生成式预训练框架

    A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning

    [https://arxiv.org/abs/2402.11922](https://arxiv.org/abs/2402.11922)

    提出了一种生成式预训练框架 GPDiff，通过在源城市数据优化的模型参数上进行预训练，将STG迁移学习转化为预训练生成式超网络，实现了对不同数据分布和特定城市特征的适应性。

    

    时空图（STG）学习对于智慧城市应用至关重要，然而在许多城市和地区往往存在数据稀缺问题。为了弥补这一差距，我们提出了一种新颖的生成式预训练框架 GPDiff，用于STG迁移学习。与传统方法不同，我们的解决方案采用了一种新颖的方法，通过在经过源城市数据优化的一系列模型参数上进行生成式预训练来执行STG迁移学习。

    arXiv:2402.11922v1 Announce Type: new  Abstract: Spatio-temporal graph (STG) learning is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPDiff, for STG transfer learning. Unlike conventional approaches that heavily rely on common feature extraction or intricate transfer learning designs, our solution takes a novel approach by performing generative pre-training on a collection of model parameters optimized with data from source cities. We recast STG transfer learning as pre-training a generative hypernetwork, which generates tailored model parameters guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPDiff employs a diffusion model with a transformer-based denoising network, which is model-agnostic to integrate with powerful STG models. By addressing challenges arising from data gaps and the
    
[^271]: 在李群上的随机Hessian拟合

    Stochastic Hessian Fitting on Lie Group

    [https://arxiv.org/abs/2402.11858](https://arxiv.org/abs/2402.11858)

    本文研究了在随机Hessian-向量乘积上拟合Hessian或其逆，揭示了不同Hessian拟合方法的收敛速率，并证明了在特定李群上的Hessian拟合问题在轻微条件下是强凸的。

    

    本文研究了在随机Hessian-向量乘积上拟合Hessian或其逆。使用了一个Hessian拟合准则，可用于推导大部分常用方法，如BFGS、高斯牛顿、AdaGrad等。我们的研究揭示了不同Hessian拟合方法的不同收敛速率，例如，在欧几里德空间中的梯度下降的次线性速率和对称正定（SPL）矩阵和某些李群上的梯度下降的线性速率。在特定且足够一般的李群上的Hessian拟合问题在轻微条件下被证明是强凸的。为了确认我们的分析，这些方法在不同设置下进行了测试，如有噪声的Hessian-向量乘积、时变的Hessians和低精度算术。这些发现对依赖于随机二阶优化的方法是有用的。

    arXiv:2402.11858v1 Announce Type: cross  Abstract: This paper studies the fitting of Hessian or its inverse with stochastic Hessian-vector products. A Hessian fitting criterion, which can be used to derive most of the commonly used methods, e.g., BFGS, Gaussian-Newton, AdaGrad, etc., is used for the analysis. Our studies reveal different convergence rates for different Hessian fitting methods, e.g., sublinear rates for gradient descent in the Euclidean space and a commonly used closed-form solution, linear rates for gradient descent on the manifold of symmetric positive definite (SPL) matrices and certain Lie groups. The Hessian fitting problem is further shown to be strongly convex under mild conditions on a specific yet general enough Lie group. To confirm our analysis, these methods are tested under different settings like noisy Hessian-vector products, time varying Hessians, and low precision arithmetic. These findings are useful for stochastic second order optimizations that rely 
    
[^272]: 探索大型语言模型的对抗能力

    Exploring the Adversarial Capabilities of Large Language Models

    [https://arxiv.org/abs/2402.09132](https://arxiv.org/abs/2402.09132)

    本研究探索了大型语言模型的对抗能力，并发现其能够成功地制造对抗性示例以愚弄安全措施，特别是在仇恨言论检测方面具有重大影响。

    

    大型语言模型（LLMs）的普及引发了广泛和普遍的兴趣，因为它们具有强大的语言生成能力，为行业和研究提供了巨大的潜力。尽管以前的研究探讨了LLMs的安全性和隐私问题，但这些模型能否表现出对抗行为的程度仍然尚未完全探索。为了填补这一空白，我们研究常见的公开可用LLMs是否具有能力扰乱文本样本以愚弄安全措施，即所谓的对抗示例或攻击。更具体地说，我们调查LLMs是否本质上能够从良性样本中制造对抗性示例以愚弄现有的安全防线。我们的实验重点关注仇恨言论检测，发现LLMs成功地找到了对抗性扰动，有效地破坏了对仇恨言论检测系统的防御。我们的发现对（半）自动化安全评估和防御具有重要影响。

    arXiv:2402.09132v1 Announce Type: new Abstract: The proliferation of large language models (LLMs) has sparked widespread and general interest due to their strong language generation capabilities, offering great potential for both industry and research. While previous research delved into the security and privacy issues of LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored. Addressing this gap, we investigate whether common publicly available LLMs have inherent capabilities to perturb text samples to fool safety measures, so-called adversarial examples resp.~attacks. More specifically, we investigate whether LLMs are inherently able to craft adversarial examples out of benign samples to fool existing safe rails. Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively undermining hate speech detection systems. Our findings carry significant implications for (semi-)aut
    
[^273]: BioNeRF: 用于视图合成的生物合理神经辐射场

    BioNeRF: Biologically Plausible Neural Radiance Fields for View Synthesis

    [https://arxiv.org/abs/2402.07310](https://arxiv.org/abs/2402.07310)

    BioNeRF是一种生物合理的架构，通过辐射场对场景进行3D表示并合成新视图。它实现了一种认知启发的机制，提高了存储能力和提取信息的能力，并在真实世界图像和合成数据的两个数据集上超过了以人的感知为基础的质量度量的最新结果。

    

    本文介绍了BioNeRF，一种生物合理的架构，它通过辐射场对场景进行3D表示并合成新视图。由于NeRF依赖于网络权重来存储场景的三维表示，BioNeRF实现了一种受认知启发的机制，将来自多个来源的输入融合成内存类似的结构，提高存储能力并提取更多内在和相关信息。BioNeRF还模仿了金字塔细胞中关于上下文信息的一种行为，其中内存作为上下文提供，并与两个后续神经模型的输入相结合，一个负责生成容积密度，另一个负责渲染场景的颜色。实验结果表明，BioNeRF在两个数据集（真实世界图像和合成数据）上超过了以人的感知为基础的质量度量的最新结果。

    This paper presents BioNeRF, a biologically plausible architecture that models scenes in a 3D representation and synthesizes new views through radiance fields. Since NeRF relies on the network weights to store the scene's 3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism that fuses inputs from multiple sources into a memory-like structure, improving the storing capacity and extracting more intrinsic and correlated information. BioNeRF also mimics a behavior observed in pyramidal cells concerning contextual information, in which the memory is provided as the context and combined with the inputs of two subsequent neural models, one responsible for producing the volumetric densities and the other the colors used to render the scene. Experimental results show that BioNeRF outperforms state-of-the-art results concerning a quality measure that encodes human perception in two datasets: real-world images and synthetic data.
    
[^274]: 见 JEANIE：通过时间-视角对齐的三维骨架序列相似度测量

    Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment

    [https://arxiv.org/abs/2402.04599](https://arxiv.org/abs/2402.04599)

    JEANIE是一种通过时间-视角对齐的方法，用于测量三维骨架序列的相似度。它能够解决视频序列中速度、时间位置和姿势的干扰变化问题。在评估了骨架Few-shot动作识别任务后，JEANIE在支持-查询序列对的时间块匹配方面表现出了良好的效果。

    

    视频序列表现出显著的干扰性变化，包括动作速度、时间位置和主体姿势，导致在比较两组帧或评估两个序列的相似度时产生时间-视角不匹配的问题。因此，我们提出了一种用于序列对比的联合时间和摄像机视角对齐方法（JEANIE）。我们特别关注能够在三维中轻松操作摄像机和主体姿势的三维骨架序列。我们在骨架Few-shot动作识别（FSAR）上评估了JEANIE，其中由于新类别样本有限，通过匹配好支持-查询序列对的时间块（组成序列的时间块）来排除干扰变化是至关重要的。针对查询序列，我们通过模拟多个摄像机位置创建多个视角。对于支持序列，我们将其与模拟出的查询序列进行匹配，类似于流行的动态时间规整（DTW）。具体而言，每个支持时间块可以与视角模拟的查询序列匹配，如DTW。

    Video sequences exhibit significant nuisance variations (undesired effects) of speed of actions, temporal locations, and subjects' poses, leading to temporal-viewpoint misalignment when comparing two sets of frames or evaluating the similarity of two sequences. Thus, we propose Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D skeleton sequences whose camera and subjects' poses can be easily manipulated in 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where matching well temporal blocks (temporal chunks that make up a sequence) of support-query sequence pairs (by factoring out nuisance variations) is essential due to limited samples of novel classes. Given a query sequence, we create its several views by simulating several camera locations. For a support sequence, we match it with view-simulated query sequences, as in the popular Dynamic Time Warping (DTW). Specifically, each support temporal block can be m
    
[^275]: Uni-RLHF: 用于多样化人类反馈的强化学习通用平台和基准套件

    Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback

    [https://arxiv.org/abs/2402.02423](https://arxiv.org/abs/2402.02423)

    Uni-RLHF是一个通用的强化学习平台和基准套件，致力于处理多样化的人类反馈，解决了在RLHF中量化进展的挑战，并提供了用户友好的注释界面和离线基准实现。

    

    强化学习与人类反馈（RLHF）通过与人类偏好对齐，避免了昂贵的手动奖励设计，已经受到了广泛关注。考虑到不同环境中不同学习方法和多样化的人类反馈类型对RLHF的进步进行量化是具有挑战性的，因为缺乏标准化的注释平台和广泛使用的统一基准。为了填补这个空白，我们引入了Uni-RLHF，这是一个为RLHF量身定制的综合系统实现。它旨在提供一个完整的从真实人类反馈到实际问题发展的工作流。Uni-RLHF包含三个部分：1）通用的多反馈注释平台，2）大规模的众包反馈数据集，3）模块化的离线RLHF基准实现。Uni-RLHF开发了一个用户友好的注释界面，适用于各种反馈类型，并与主要的强化学习框架兼容。

    Reinforcement Learning with Human Feedback (RLHF) has received significant attention for performing tasks without the need for costly manual reward design by aligning human preferences. It is crucial to consider diverse human feedback types and various learning methods in different environments. However, quantifying progress in RLHF with diverse feedback is challenging due to the lack of standardized annotation platforms and widely used unified benchmarks. To bridge this gap, we introduce Uni-RLHF, a comprehensive system implementation tailored for RLHF. It aims to provide a complete workflow from real human feedback, fostering progress in the development of practical problems. Uni-RLHF contains three packages: 1) a universal multi-feedback annotation platform, 2) large-scale crowdsourced feedback datasets, and 3) modular offline RLHF baseline implementations. Uni-RLHF develops a user-friendly annotation interface tailored to various feedback types, compatible with a wide range of main
    
[^276]: 自监督对比预测

    Self-Supervised Contrastive Forecasting

    [https://arxiv.org/abs/2402.02023](https://arxiv.org/abs/2402.02023)

    该论文介绍了一种通过采用对比学习和增强的分解架构，并结合全局自相关性的自监督方法来解决长期预测中的挑战。实验证明，该方法在九个长期基准上的多个实验中胜过了14个基线模型。

    

    长期预测由于处理长序列的时间和内存复杂性而面临独特挑战。现有方法依赖于滑动窗口来处理长序列，难以有效捕捉部分在短窗口内被捕捉到的长期变化（即外窗口变化）。本文介绍了一种新颖的方法，通过采用对比学习和增强的分解架构，专门设计用于聚焦长期变化，从而克服了这个限制。为此，我们的对比损失将整个时间序列中的全局自相关性纳入考虑，以自监督方式构建正负对。当与我们的分解网络结合使用时，我们的对比学习显著提高了长期预测性能。广泛的实验表明，我们的方法在九个长期基准上的多个实验中胜过了14个基线模型。

    Long-term forecasting presents unique challenges due to the time and memory complexity of handling long sequences. Existing methods, which rely on sliding windows to process long sequences, struggle to effectively capture long-term variations that are partially caught within the short window (i.e., outer-window variations). In this paper, we introduce a novel approach that overcomes this limitation by employing contrastive learning and enhanced decomposition architecture, specifically designed to focus on long-term variations. To this end, our contrastive loss incorporates global autocorrelation held in the whole time series, which facilitates the construction of positive and negative pairs in a self-supervised manner. When combined with our decomposition networks, our contrastive learning significantly improves long-term forecasting performance. Extensive experiments demonstrate that our approach outperforms 14 baseline models in multiple experiments over nine long-term benchmarks, es
    
[^277]: LOCOST: 长文档抽象摘要化的状态空间模型

    LOCOST: State-Space Models for Long Document Abstractive Summarization

    [https://arxiv.org/abs/2401.17919](https://arxiv.org/abs/2401.17919)

    LOCOST是一种基于状态空间模型的编码器-解码器架构，用于处理长文档的抽象摘要生成。与基于稀疏注意模式的最先进模型相比，LOCOST具有更低的计算复杂度，并且能够在训练和推断期间节省大量内存。在评估中，LOCOST在长文档摘要化任务上达到了93-96%的性能水平，并且能够处理超过600K个标记的输入文本。

    

    状态空间模型是编码长序列和捕捉长期依赖的低复杂度替代方案，我们提出了LOCOST：一种基于状态空间模型的编码器-解码器架构，用于具有长上下文输入的条件文本生成。这种架构的计算复杂度为O（L log L），可以处理比基于稀疏注意模式的最先进模型更长的序列。我们在一系列长文档抽象摘要化任务上评估了我们的模型。该模型在性能水平上达到了与相同大小的最优稀疏变压器相当的93-96%，同时在训练期间节省了高达50%的内存，在推断期间节省了高达87%的内存。此外，LOCOST有效地处理超过600K个标记的输入文本，为完整书摘要化设定了新的最新结果，并为长输入处理开辟了新的视角。

    State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of $O(L \log L)$, this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.
    
[^278]: 重新思考多元时间序列预测的通道相关性：从领先指标中学习

    Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators

    [https://arxiv.org/abs/2401.17548](https://arxiv.org/abs/2401.17548)

    本文提出了一种新方法，LIFT，通过利用通道相关性和领先指标，为多元时间序列预测提供准确的预测。LIFT方法可以无缝与任意时间序列预测方法协作，大量实验证明了其有效性。

    

    最近，独立于通道的方法在多元时间序列（MTS）预测中取得了最先进的性能。尽管这些方法减少了过拟合的风险，但它们错过了利用通道相关性进行准确预测的潜在机会。我们认为，在变量之间存在局部平稳的领先-滞后关系，即一些滞后变量在短时间内可能遵循领先指标。利用这种通道相关性是有益的，因为领先指标提供了先进信息，可以用来减少滞后变量的预测难度。在本文中，我们提出了一种名为LIFT的新方法，该方法首先在每个时间步骤高效地估计领先指标及其领先步骤，然后巧妙地允许滞后变量利用来自领先指标的先进信息。LIFT作为一个插件，可以与任意时间序列预测方法无缝协作。进行了大量实验证明了LIFT方法的有效性。

    Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments o
    
[^279]: 一项关于改善图像分类模型公平性的大规模实证研究

    A Large-Scale Empirical Study on Improving the Fairness of Image Classification Models

    [https://arxiv.org/abs/2401.03695](https://arxiv.org/abs/2401.03695)

    本研究进行了大规模实证研究，比较了现有最先进的公平性改进技术在图像分类模型上的表现，揭示了它们之间的显著差异

    

    公平性一直是影响深度学习模型在实际应用中被采纳的关键问题。为了提高模型的公平性，已经提出了许多现有方法，并证实在各自的情境中是有效的。然而，它们之间仍然没有进行系统的评估以便在相同情境下进行全面比较，这使得理解它们之间的性能差异变得困难，阻碍了研究进展和实际应用。为了填补这一空白，本文致力于进行首次大规模实证研究，全面比较现有最先进的公平性改进技术的性能。具体而言，我们针对广泛使用的图像分类应用场景，利用三个不同数据集和五种常用性能指标，总共评估了来自不同类别的 13 种方法。我们的研究结果揭示了不同方法之间的显著差异。

    arXiv:2401.03695v2 Announce Type: replace-cross  Abstract: Fairness has been a critical issue that affects the adoption of deep learning models in real practice. To improve model fairness, many existing methods have been proposed and evaluated to be effective in their own contexts. However, there is still no systematic evaluation among them for a comprehensive comparison under the same context, which makes it hard to understand the performance distinction among them, hindering the research progress and practical adoption of them. To fill this gap, this paper endeavours to conduct the first large-scale empirical study to comprehensively compare the performance of existing state-of-the-art fairness improving techniques. Specifically, we target the widely-used application scenario of image classification, and utilized three different datasets and five commonly-used performance metrics to assess in total 13 methods from diverse categories. Our findings reveal substantial variations in the 
    
[^280]: 发现能够通用组合的模块化解决方案

    Discovering modular solutions that generalize compositionally

    [https://arxiv.org/abs/2312.15001](https://arxiv.org/abs/2312.15001)

    通过在教师-学生设置中研究模块化系统，我们探讨了模块化系统在发现隐藏组合结构方面的作用，并证明了其在识别潜在模块方面的重要性。

    

    许多复杂任务可以分解为更简单、独立的部分。发现这种潜在的组合结构有可能实现组合泛化。尽管取得了进展，但我们最强大的系统仍然难以灵活组合。因此，使模型更模块化似乎是自然的，以帮助捕捉许多任务的组合性质。然而，模块化系统在何种情况下能够发现隐藏的组合结构尚不清楚。为了解决这个问题，我们研究了一个教师-学生设置，其中教师是模块化的，我们完全控制着地面真实模块的组合。这使我们能够将组合泛化的问题与发现潜在模块的问题联系起来。具体而言，我们在代表一个通用类别的乘法相互作用的超网络中研究了模块化。我们在理论上证明了纯粹的线性变换识别

    arXiv:2312.15001v2 Announce Type: replace  Abstract: Many complex tasks can be decomposed into simpler, independent parts. Discovering such underlying compositional structure has the potential to enable compositional generalization. Despite progress, our most powerful systems struggle to compose flexibly. It therefore seems natural to make models more modular to help capture the compositional nature of many tasks. However, it is unclear under which circumstances modular systems can discover hidden compositional structure. To shed light on this question, we study a teacher-student setting with a modular teacher where we have full control over the composition of ground truth modules. This allows us to relate the problem of compositional generalization to that of identification of the underlying modules. In particular we study modularity in hypernetworks representing a general class of multiplicative interactions. We show theoretically that identification up to linear transformation purel
    
[^281]: 使用回声状态网络的多智能体强化学习及其在行人动态中的应用

    Multi-agent reinforcement learning using echo-state network and its application to pedestrian dynamics

    [https://arxiv.org/abs/2312.11834](https://arxiv.org/abs/2312.11834)

    通过使用回声状态网络将行人实现为MARL代理，研究了他们学习避让其他代理向前移动的能力，在密度不太高的情况下取得成功。

    

    近年来，研究使用多智能体强化学习（MARL）模拟行人。本研究考虑了网格世界环境中的道路，并将行人实现为使用回声状态网络和最小二乘策略迭代方法的MARL代理。在这个环境下，研究了这些代理学习避开其他代理向前移动的能力。具体而言，我们考虑了两种任务：窄直接路径和宽绕道之间的选择，以及走廊中的双向行人流。模拟结果表明，当代理密度不太高时，学习是成功的。

    arXiv:2312.11834v2 Announce Type: replace-cross  Abstract: In recent years, simulations of pedestrians using the multi-agent reinforcement learning (MARL) have been studied. This study considered the roads on a grid-world environment, and implemented pedestrians as MARL agents using an echo-state network and the least squares policy iteration method. Under this environment, the ability of these agents to learn to move forward by avoiding other agents was investigated. Specifically, we considered two types of tasks: the choice between a narrow direct route and a broad detour, and the bidirectional pedestrian flow in a corridor. The simulations results indicated that the learning was successful when the density of the agents was not that high.
    
[^282]: 神经积分方程的谱方法

    Spectral methods for Neural Integral Equations

    [https://arxiv.org/abs/2312.05654](https://arxiv.org/abs/2312.05654)

    本文引入了一个基于谱方法的神经积分方程框架，通过在谱域中学习算子，降低了计算成本，并保证了高插值精度。

    

    arXiv:2312.05654v3 公告类型：替换-跨交摘要：神经积分方程是基于积分方程理论的深度学习模型，其中模型由积分算子和通过优化过程学习的相应方程（第二种）组成。这种方法允许利用机器学习中积分算子的非局部特性，但计算成本很高。在本文中，我们介绍了基于谱方法的神经积分方程框架，该方法使我们能够在谱域中学习一个算子，从而降低计算成本，同时保证高插值精度。我们研究了我们方法的性质，并展示了关于模型近似能力和收敛到数值方法解的各种理论保证。我们提供了数值实验来展示所得模型的实际有效性。

    arXiv:2312.05654v3 Announce Type: replace-cross  Abstract: Neural integral equations are deep learning models based on the theory of integral equations, where the model consists of an integral operator and the corresponding equation (of the second kind) which is learned through an optimization procedure. This approach allows to leverage the nonlocal properties of integral operators in machine learning, but it is computationally expensive. In this article, we introduce a framework for neural integral equations based on spectral methods that allows us to learn an operator in the spectral domain, resulting in a cheaper computational cost, as well as in high interpolation accuracy. We study the properties of our methods and show various theoretical guarantees regarding the approximation capabilities of the model, and convergence to solutions of the numerical methods. We provide numerical experiments to demonstrate the practical effectiveness of the resulting model.
    
[^283]: I-PHYRE: 交互式物理推理

    I-PHYRE: Interactive Physical Reasoning

    [https://arxiv.org/abs/2312.03009](https://arxiv.org/abs/2312.03009)

    I-PHYRE是一个挑战代理同时展示直观的物理推理、多步规划和就地干预能力的框架。

    

    当前的评估协议主要评估静态场景中的物理推理，存在评估代理与动态事件进行交互的能力的缺口。尽管当代方法允许代理修改初始场景配置并观察结果，但它们缺乏实时与事件交互的能力。为了解决这一问题，我们引入了I-PHYRE，一个框架挑战代理同时展示直观的物理推理、多步规划和就地干预。这里，直观的物理推理指的是快速、近似理解物理学以解决复杂问题；多步表示在I-PHYRE中需要进行广泛的序列规划，考虑到每次干预都可能显著改变后续选择；而就地意味着在场景内及时进行物体操作的必要性，在这里，微小的时间偏差可能导致任务失败。我们制定了四个游戏分支。

    arXiv:2312.03009v2 Announce Type: replace  Abstract: Current evaluation protocols predominantly assess physical reasoning in stationary scenes, creating a gap in evaluating agents' abilities to interact with dynamic events. While contemporary methods allow agents to modify initial scene configurations and observe consequences, they lack the capability to interact with events in real time. To address this, we introduce I-PHYRE, a framework that challenges agents to simultaneously exhibit intuitive physical reasoning, multi-step planning, and in-situ intervention. Here, intuitive physical reasoning refers to a quick, approximate understanding of physics to address complex problems; multi-step denotes the need for extensive sequence planning in I-PHYRE, considering each intervention can significantly alter subsequent choices; and in-situ implies the necessity for timely object manipulation within a scene, where minor timing deviations can result in task failure. We formulate four game spl
    
[^284]: 使用人类反馈来微调扩散模型而无需任何奖励模型

    Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model

    [https://arxiv.org/abs/2311.13231](https://arxiv.org/abs/2311.13231)

    该论文提出了一种名为D3PO的方法，通过使用人类反馈直接微调扩散模型，无需奖励模型，从而在消除了奖励模型的前提下改进了现有方法，并解决了DPO方法直接应用的内存需求问题。

    

    使用带有人类反馈的强化学习（RLHF）在微调扩散模型方面显示出了显著的潜力。以往的方法首先是通过训练与人类偏好相一致的奖励模型，然后利用强化学习技术来微调基础模型。然而，设计高效的奖励模型需要大量数据集、最佳架构和手动超参数调整，使得这一过程既耗时又成本高昂。直接偏好优化（DPO）方法，在微调大型语言模型方面表现出色，消除了对奖励模型的需求。然而，扩散模型去噪过程的大量GPU内存需求阻碍了DPO方法的直接应用。为解决这一问题，我们引入了直接偏好去噪扩散策略优化（D3PO）方法来直接微调扩散模型。理论分析表明，尽管D3PO提供了改进，但在具有明显优势的同时仍需要更多研究。

    arXiv:2311.13231v3 Announce Type: replace-cross  Abstract: Using reinforcement learning with human feedback (RLHF) has shown significant promise in fine-tuning diffusion models. Previous methods start by training a reward model that aligns with human preferences, then leverage RL techniques to fine-tune the underlying models. However, crafting an efficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter tuning, making the process both time and cost-intensive. The direct preference optimization (DPO) method, effective in fine-tuning large language models, eliminates the necessity for a reward model. However, the extensive GPU memory requirement of the diffusion model's denoising process hinders the direct application of the DPO method. To address this issue, we introduce the Direct Preference for Denoising Diffusion Policy Optimization (D3PO) method to directly fine-tune diffusion models. The theoretical analysis demonstrates that although D3PO o
    
[^285]: EnduRL: 通过强化学习提高混合交通在真实世界扰动下的安全性、稳定性和效率

    EnduRL: Enhancing Safety, Stability, and Efficiency of Mixed Traffic Under Real-World Perturbations Via Reinforcement Learning

    [https://arxiv.org/abs/2311.12261](https://arxiv.org/abs/2311.12261)

    通过分析真实世界驾驶轨迹并将其应用于机器人车辆训练，可以提高混合交通在各种环境中的安全性、效率和稳定性

    

    arXiv:2311.12261v2 公告类型: replace-cross 摘要: 人驾驶车辆（HVs）放大了交通中自然发生的扰动，导致拥堵-这是增加燃油消耗、增加碰撞风险以及减少道路容量利用的主要原因。虽然先前的研究表明，机器人车辆（RVs）可以被利用来缓解这些问题，但大多数此类研究依赖于使用简化的人车跟驰行为模型的模拟。在这项工作中，我们分析了真实世界的行车轨迹并提取了各种加速度曲线。然后将这些曲线结合到模拟中，用于训练RVs以缓解拥堵。我们通过在两种混合交通环境（环形和瓶颈）中进行的综合实验评估了混合交通的安全性、效率和稳定性，包括不同的交通密度、配置和RV渗透率。结果表明，在真实世界的扰动下，先前的RV控制器遇到

    arXiv:2311.12261v2 Announce Type: replace-cross  Abstract: Human-driven vehicles (HVs) amplify naturally occurring perturbations in traffic, leading to congestion--a major contributor to increased fuel consumption, higher collision risks, and reduced road capacity utilization. While previous research demonstrates that Robot Vehicles (RVs) can be leveraged to mitigate these issues, most such studies rely on simulations with simplistic models of human car-following behaviors. In this work, we analyze real-world driving trajectories and extract a wide range of acceleration profiles. We then incorporates these profiles into simulations for training RVs to mitigate congestion. We evaluate the safety, efficiency, and stability of mixed traffic via comprehensive experiments conducted in two mixed traffic environments (Ring and Bottleneck) at various traffic densities, configurations, and RV penetration rates. The results show that under real-world perturbations, prior RV controllers experienc
    
[^286]: 揭示和提高数据可信度：训练无害语言模型的数据集研究

    Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models

    [https://arxiv.org/abs/2311.11202](https://arxiv.org/abs/2311.11202)

    本研究专注于真实世界数据集的可信度，提出了一个系统化框架，用于评估数据集的可信度，识别标签错误，并评估嘈杂标签对不安全评论和对话分类的影响，以提高训练无害语言模型的质量。

    

    arXiv:2311.11202v2宣布类型：替换-跨文档摘要：语言模型在各种任务中显示出潜力，但在训练、微调或对齐过程中可能受到不希望的数据的影响。因此，注解的正确性，即数据集的可信度，变得非常重要。本研究聚焦于真实世界数据集的可信度，其中包括可用于训练无害语言模型的流行基准数据集，如Jigsaw Civil Comments、Anthropic Harmless和Red Team、PKU BeaverTails和SafeRLHF。考虑到人们清洗这些数据集的成本和难度，我们引入了一个系统化框架，用于评估数据集的可信度，识别标签错误，并评估策划语言数据中嘈杂标签的影响，特别关注不安全评论和对话分类。

    arXiv:2311.11202v2 Announce Type: replace-cross  Abstract: Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we 
    
[^287]: Tactics2D：一种具有生成场景的强化学习环境库，用于驾驶决策制定

    Tactics2D: A Reinforcement Learning Environment Library with Generative Scenarios for Driving Decision-making

    [https://arxiv.org/abs/2311.11058](https://arxiv.org/abs/2311.11058)

    Tactics2D是一个具有自动生成交通场景功能的强化学习环境库，旨在为研究人员提供探索学习驱动的驾驶决策模型的工具。

    

    Tactics2D是一个开源的强化学习环境库，具有自动生成多样化和具有挑战性的交通场景的功能。其主要目标是为研究人员提供一个开箱即用的工具包，用于探索基于学习的驾驶决策模型。该库实现了基于规则和数据驱动方法来生成互动交通场景。Tactics2D的显著特点包括与现实世界日志和数据格式的广泛兼容性，可定制的交通场景组件以及丰富的内置功能模板。Tactics2D考虑到用户友好性而开发，提供了详细的文档和交互式在线教程。该软件保持了稳固的可靠性，超过90%的代码通过了单元测试。要访问源代码并参与讨论，请访问Tactics2D的官方GitHub页面https://github.com/WoodOxen/Tactics2D。

    arXiv:2311.11058v2 Announce Type: replace  Abstract: Tactics2D is an open-source Reinforcement Learning environment library featured with auto-generation of diverse and challenging traffic scenarios. Its primary goal is to provide an out-of-the-box toolkit for researchers to explore learning-based driving decision-making models. This library implements both rule-based and data-driven approaches to generate interactive traffic scenarios. Noteworthy features of Tactics2D include expansive compatibility with real-world log and data formats, customizable traffic scenario components, and rich built-in functional templates. Developed with user-friendliness in mind, Tactics2D offers detailed documentation and an interactive online tutorial. The software maintains robust reliability, with over 90% code passing unit testing. For access to the source code and participation in discussions, visit the official GitHub page for Tactcis2D at https://github.com/WoodOxen/Tactics2D.
    
[^288]: 信号在大图上的采样的Poincar\'e不等式和一致性结果

    A Poincar\'e Inequality and Consistency Results for Signal Sampling on Large Graphs

    [https://arxiv.org/abs/2311.10610](https://arxiv.org/abs/2311.10610)

    该论文介绍了一种针对图极限中图上信号的信号采样理论，证明了Poincar\'e不等式并展示了一致性结果。

    

    大规模图机器学习具有挑战性，因为学习模型的复杂性随着图的大小而增加。对图进行子采样是一种可行的替代方案，但在图上进行采样是非平凡的，因为图是非欧几里得的。现有的图采样技术不仅需要计算大矩阵的谱，而且在图发生变化（例如增长）时需要重复这些计算。本文介绍了一种用于一种图极限--图上的信号采样理论。我们证明了图上信号的Poincar\'e不等式，并展示了满足这一不等式的节点子集的补集是图上信号Paley-Wiener空间的唯一采样集。通过与谱聚类和高斯消元的联系，我们证明了这样的采样集是一致的，即收敛的图序列上的唯一采样集收敛到图极限上的唯一采样集。

    arXiv:2311.10610v2 Announce Type: replace  Abstract: Large-scale graph machine learning is challenging as the complexity of learning models scales with the graph size. Subsampling the graph is a viable alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean. Existing graph sampling techniques require not only computing the spectra of large matrices but also repeating these computations when the graph changes, e.g., grows. In this paper, we introduce a signal sampling theory for a type of graph limit -- the graphon. We prove a Poincar\'e inequality for graphon signals and show that complements of node subsets satisfying this inequality are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting connections with spectral clustering and Gaussian elimination, we prove that such sampling sets are consistent in the sense that unique sampling sets on a convergent graph sequence converge to unique sampling sets on the graphon. We then propose a related
    
[^289]: 评价图神经网络的邻居可解释性

    Evaluating Neighbor Explainability for Graph Neural Networks

    [https://arxiv.org/abs/2311.08118](https://arxiv.org/abs/2311.08118)

    评价图神经网络中邻居的可解释性，提出新的度量标准并发现基于梯度的方法在GNN领域的解释没有太大差异，同时发现很多技术在没有自环的GNNs下无法准确识别重要邻居。

    

    图神经网络（GNNs）中的可解释性是近年来新兴领域。在这篇文章中，我们解决了一个问题，即在对节点进行分类时，确定每个邻居对于 GNN 的重要性以及如何衡量这一特定任务的表现。为此，各种已知的可解释性方法被重新构造以获取邻居重要性，并提出了四种新的度量标准。我们的结果表明，在 GNN 领域，基于梯度的技术提供的解释几乎没有差异。此外，许多可解释性技术在使用没有自环的 GNNs 时未能识别重要的邻居。

    arXiv:2311.08118v2 Announce Type: replace-cross  Abstract: Explainability in Graph Neural Networks (GNNs) is a new field growing in the last few years. In this publication we address the problem of determining how important is each neighbor for the GNN when classifying a node and how to measure the performance for this specific task. To do this, various known explainability methods are reformulated to get the neighbor importance and four new metrics are presented. Our results show that there is almost no difference between the explanations provided by gradient-based techniques in the GNN domain. In addition, many explainability techniques failed to identify important neighbors when GNNs without self-loops are used.
    
[^290]: 用强化学习进行因果问答

    Causal Question Answering with Reinforcement Learning

    [https://arxiv.org/abs/2311.02760](https://arxiv.org/abs/2311.02760)

    本研究通过强化学习在因果图上进行因果问答，引入了一种基于演员评论的 agent，有效解决了当前因果问答方法无法提供解释或证据的问题

    

    因果问题探究不同事件或现象之间的因果关系。它们对各种用例都很重要，包括虚拟助手和搜索引擎。然而，许多当前的因果问答方法不能为其答案提供解释或证据。因此，在这篇论文中，我们旨在用因果图回答因果问题，这是一个关于名词短语之间因果关系的大规模数据集，同时提供关系的来源数据。受最近将强化学习成功应用于知识图任务的启发，例如链接预测和事实检查，我们探讨了将强化学习应用于因果图进行因果问答的方法。我们引入了一种基于演员评论的 agent，它学习通过图搜索来回答因果问题。我们通过监督学习程序引导 agent 处理大规模操作空间和sp

    arXiv:2311.02760v2 Announce Type: replace  Abstract: Causal questions inquire about causal relationships between different events or phenomena. They are important for a variety of use cases, including virtual assistants and search engines. However, many current approaches to causal question answering cannot provide explanations or evidence for their answers. Hence, in this paper, we aim to answer causal questions with a causality graph, a large-scale dataset of causal relations between noun phrases along with the relations' provenance data. Inspired by recent, successful applications of reinforcement learning to knowledge graph tasks, such as link prediction and fact-checking, we explore the application of reinforcement learning on a causality graph for causal question answering. We introduce an Actor-Critic-based agent which learns to search through the graph to answer causal questions. We bootstrap the agent with a supervised learning procedure to deal with large action spaces and sp
    
[^291]: 从多节点干预中识别线性混合因果表示

    Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions

    [https://arxiv.org/abs/2311.02695](https://arxiv.org/abs/2311.02695)

    本文提出了一种新的方法，可以在一个环境中允许定位多个变量的干预，并在因果表示学习中首次得出了可识别性结果。

    

    推断从低级观察中得出高级因果变量的任务，通常称为因果表示学习，本质上是欠约束的。因此，为了解决这个问题的最近工作集中在导致潜在潜在因果变量可识别性的各种假设上。大量之前的方法考虑在因果模型上不同干预下收集的多环境数据。几乎所有这些工作共同点是对每个环境中只干预一个变量的限制性假设。在这项工作中，我们放宽了这一假设，并为因果表示学习提供了第一个允许在一个环境中通过干预定位多个变量的可识别性结果。我们的方法取决于关于各个环境中干预的覆盖范围和多样性的一般假设，其中也包含

    arXiv:2311.02695v2 Announce Type: replace-cross  Abstract: The task of inferring high-level causal variables from low-level observations, commonly referred to as causal representation learning, is fundamentally underconstrained. As such, recent works to address this problem focus on various assumptions that lead to identifiability of the underlying latent causal variables. A large corpus of these preceding approaches consider multi-environment data collected under different interventions on the causal model. What is common to virtually all of these works is the restrictive assumption that in each environment, only a single variable is intervened on. In this work, we relax this assumption and provide the first identifiability result for causal representation learning that allows for multiple variables to be targeted by an intervention within one environment. Our approach hinges on a general assumption on the coverage and diversity of interventions across environments, which also include
    
[^292]: 基于鸟瞰视角预训练世界模型用于视觉导航

    Bird's Eye View Based Pretrained World model for Visual Navigation

    [https://arxiv.org/abs/2310.18847](https://arxiv.org/abs/2310.18847)

    通过基于鸟瞰视角图像的预训练世界模型，实现了从模拟器到真实世界的零迁移

    

    Sim2Real转移已经变得流行，因为它有助于从廉价的模拟器转移到现实世界。本文提出了一个新颖的系统，在传统世界模型中融合组件，完全在模拟器中训练，可以零迁移到真实世界。为了促进转移，我们使用基于鸟瞰视角图像的中间表示。因此，我们的机器人通过先学习将复杂的基于第一人称视角的RGB图像转换为BEV表示，然后学习使用这些表示来在模拟中导航。当在真实世界中进行测试时，机器人使用将FPV基础RGB图像转换为FPV到BEV转换器学习的嵌入的感知模型，并且可以被下游策略使用。利用状态检查模块，使用锚定图像和混合

    arXiv:2310.18847v2 Announce Type: replace-cross  Abstract: Sim2Real transfer has gained popularity because it helps transfer from inexpensive simulators to real world. This paper presents a novel system that fuses components in a traditional World Model into a robust system, trained entirely within a simulator, that Zero-Shot transfers to the real world. To facilitate transfer, we use an intermediary representation that is based on \textit{Bird's Eye View (BEV)} images. Thus, our robot learns to navigate in a simulator by first learning to translate from complex \textit{First-Person View (FPV)} based RGB images to BEV representations, then learning to navigate using those representations. Later, when tested in the real world, the robot uses the perception model that translates FPV-based RGB images to embeddings that were learned by the FPV to BEV translator and that can be used by the downstream policy. The incorporation of state-checking modules using \textit{Anchor images} and Mixtur
    
[^293]: 一种适用于经典和量子伊辛机的通用学习方案

    A general learning scheme for classical and quantum Ising machines

    [https://arxiv.org/abs/2310.18411](https://arxiv.org/abs/2310.18411)

    提出了一种基于伊辛结构的新机器学习模型，可以通过梯度下降进行高效训练，并利用伊辛机自己估计的偏导数优化损失函数，同时在量子领域展示了多种学习任务的新可能性。

    

    伊辛机是专门设计用于找到伊辛模型基态的任何硬件。相关示例包括相干伊辛机和量子退火器。在本文中，我们提出了一种基于伊辛结构的新机器学习模型，可以通过梯度下降进行高效训练。我们对训练过程进行了数学表征，该过程是基于优化损失函数，其偏导数不是显式计算的，而是由伊辛机自己估计的。此外，我们提供了关于所提出学习模型的训练和执行的一些实验结果。这些结果指出了伊辛机为不同学习任务提供的新可能性。特别是，在量子领域，量子资源被用于模型的执行和训练，为量子机器学习提供了一个有希望的前景。

    arXiv:2310.18411v2 Announce Type: replace  Abstract: An Ising machine is any hardware specifically designed for finding the ground state of the Ising model. Relevant examples are coherent Ising machines and quantum annealers. In this paper, we propose a new machine learning model that is based on the Ising structure and can be efficiently trained using gradient descent. We provide a mathematical characterization of the training process, which is based upon optimizing a loss function whose partial derivatives are not explicitly calculated but estimated by the Ising machine itself. Moreover, we present some experimental results on the training and execution of the proposed learning model. These results point out new possibilities offered by Ising machines for different learning tasks. In particular, in the quantum realm, the quantum resources are used for both the execution and the training of the model, providing a promising perspective in quantum machine learning.
    
[^294]: BatteryML：一个用于电池衰减机器学习的开源平台

    BatteryML:An Open-source platform for Machine Learning on Battery Degradation

    [https://arxiv.org/abs/2310.14714](https://arxiv.org/abs/2310.14714)

    BatteryML是一个开源平台，通过一站式、全面的方法统一了电池衰减建模的数据预处理、特征提取和模型实现，提高了研究应用的实用性和效率。

    

    电池衰减仍然是能源存储领域的一个关键问题，而机器学习作为推动洞察和解决方案的有效工具正在崛起。然而，电化学科学和机器学习的交叉领域带来了复杂的挑战。机器学习专家经常在处理电池科学的复杂性上苦苦挣扎，而电池研究人员则面临着将复杂模型调整到特定数据集的障碍。此外，缺乏涵盖数据格式和评估基准的电池衰减建模的统一标准。鉴于这些障碍，我们提出了BatteryML - 一个一站式、全面且开源的平台，旨在统一数据预处理、特征提取以及传统和最先进模型的实现。这种简化的方法有望提高研究应用的实用性和效率。

    arXiv:2310.14714v4 Announce Type: replace-cross  Abstract: Battery degradation remains a pivotal concern in the energy storage domain, with machine learning emerging as a potent tool to drive forward insights and solutions. However, this intersection of electrochemical science and machine learning poses complex challenges. Machine learning experts often grapple with the intricacies of battery science, while battery researchers face hurdles in adapting intricate models tailored to specific datasets. Beyond this, a cohesive standard for battery degradation modeling, inclusive of data formats and evaluative benchmarks, is conspicuously absent. Recognizing these impediments, we present BatteryML - a one-step, all-encompass, and open-source platform designed to unify data preprocessing, feature extraction, and the implementation of both traditional and state-of-the-art models. This streamlined approach promises to enhance the practicality and efficiency of research applications. BatteryML s
    
[^295]: RTSUM：基于关系三元组的可解释摘要与多级显著性可视化

    RTSUM: Relation Triple-based Interpretable Summarization with Multi-level Salience Visualization

    [https://arxiv.org/abs/2310.13895](https://arxiv.org/abs/2310.13895)

    提出了RTSUM框架，利用关系三元组进行摘要生成，并开发了一个可解释的摘要工具，支持多级显著性可视化。

    

    在本文中，我们提出了RTSUM，这是一种利用关系三元组作为摘要基本单元的无监督摘要框架。给定输入文档，RTSUM首先通过多级显著性评分选择显著的关系三元组，然后利用文本到文本语言模型从选定的关系三元组生成简洁摘要。在RTSUM的基础上，我们还开发了一个用于解释性摘要工具的网络演示，提供了对输出摘要的细粒度解释。通过支持自定义选项，我们的工具在三个不同级别上可视化文本单元的显著性：句子、关系三元组和短语。代码已公开可用。

    arXiv:2310.13895v2 Announce Type: replace  Abstract: In this paper, we present RTSUM, an unsupervised summarization framework that utilizes relation triples as the basic unit for summarization. Given an input document, RTSUM first selects salient relation triples via multi-level salience scoring and then generates a concise summary from the selected relation triples by using a text-to-text language model. On the basis of RTSUM, we also develop a web demo for an interpretable summarizing tool, providing fine-grained interpretations with the output summary. With support for customization options, our tool visualizes the salience for textual units at three distinct levels: sentences, relation triples, and phrases. The codes,are publicly available.
    
[^296]: 通过模型选择实现健壮的多模态推理

    Towards Robust Multi-Modal Reasoning via Model Selection

    [https://arxiv.org/abs/2310.08446](https://arxiv.org/abs/2310.08446)

    多模态代理在处理复杂挑战时需要考虑模型选择的重要性，以避免执行的脆弱性。

    

    最近的研究普遍承认了大型语言模型（LLM）的推理能力，在工具学习和自主代理研究中鼓舞了研究。LLM充当代理的“大脑”，为协作多步任务求解集成多个工具。多模态代理通过整合各种人工智能模型处理复杂挑战，在处理直观任务时不像调用计算器或天气API那样。然而，当前的多模态代理忽视了模型选择的重要性：它们主要专注于计划和执行阶段，只会为每个子任务调用预定义的任务特定模型，使执行变得脆弱。与此同时，其他传统的模型选择方法要么与多模态代理场景不兼容或不理想，因为它们忽视了多步推理产生的子任务之间的依赖关系。因此，我们确定了主要挑战。

    arXiv:2310.08446v2 Announce Type: replace-cross  Abstract: The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the "brain" of the agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning. To this end, we identify the key challenges
    
[^297]: 从元学习视角看Transformer用于因果语言建模

    A Meta-Learning Perspective on Transformers for Causal Language Modeling

    [https://arxiv.org/abs/2310.05884](https://arxiv.org/abs/2310.05884)

    本文从元学习视角探讨了Transformer用于因果语言建模时的内部优化过程，发现并分析了Transformer-based因果语言模型中学习到的token表示范数的特殊特征。

    

    Transformer架构在开发大型因果语言模型方面变得显著。然而，解释其能力的机制尚不为人所了解。本文侧重于训练过程，建立了一个元学习视角来探究Transformer架构在因果语言建模任务训练时的内部优化过程，详细说明了Transformer内部的一个优化过程。此外，在这个内部优化过程中，我们发现并理论分析了Transformer-based因果语言模型中学习到的token表示的范数的特殊特征。我们的分析得到了各种设置中的实验证实支持。

    arXiv:2310.05884v2 Announce Type: replace-cross  Abstract: The Transformer architecture has become prominent in developing large causal language models. However, mechanisms to explain its capabilities are not well understood. Focused on the training process, here we establish a meta-learning view of the Transformer architecture when trained for the causal language modeling task, by explicating an inner optimization process within the Transformer. Further, within the inner optimization, we discover and theoretically analyze a special characteristic of the norms of learned token representations within Transformer-based causal language models. Our analysis is supported by experiments in various settings.
    
[^298]: HyMNet：一种利用眼底照片和心脏代谢风险因子进行高血压分类的多模态深度学习系统

    HyMNet: a Multimodal Deep Learning System for Hypertension Classification using Fundus Photographs and Cardiometabolic Risk Factors

    [https://arxiv.org/abs/2310.01099](https://arxiv.org/abs/2310.01099)

    介绍了HyMNet，一种结合眼底图像和心脏代谢风险因素的多模态深度学习系统，用于改善高血压检测能力。

    

    近年来，深度学习在预测高血压（HTN）方面表现出了潜力。然而，大多数先前的研究主要集中在分析单一类型的数据上，这可能无法捕捉到HTN风险的全部复杂性。为了解决这一限制，本研究引入了一种称为HyMNet的多模态深度学习（MMDL）系统，该系统结合了眼底图像和心脏代谢风险因素，特别是年龄和性别，以改善高血压检测能力。我们的MMDL系统使用了在160万张视网膜图像上预先训练的基础模型RETFound来进行眼底路径，并使用全连接神经网络来进行年龄和性别路径。这两个路径通过连接每个路径的特征向量进行联合训练，然后将其送入融合网络。该系统在沙特国民警卫卫生事务部收集的1,243名个体的5,016张眼底图像上进行了训练。

    arXiv:2310.01099v2 Announce Type: replace-cross  Abstract: In recent years, deep learning has shown promise in predicting hypertension (HTN) from fundus images. However, most prior research has primarily focused on analyzing a single type of data, which may not capture the full complexity of HTN risk. To address this limitation, this study introduces a multimodal deep learning (MMDL) system, dubbed HyMNet, which combines fundus images and cardiometabolic risk factors, specifically age and gender, to improve hypertension detection capabilities. Our MMDL system uses RETFound, a foundation model pre-trained on 1.6 million retinal images, for the fundus path and a fully connected neural network for the age and gender path. The two paths are jointly trained by concatenating the feature vectors from each path that are then fed into a fusion network. The system was trained on 5,016 retinal images from 1,243 individuals collected from the Saudi Ministry of National Guard Health Affairs. The re
    
[^299]: 通过平方的消减混合模型:表示和学习

    Subtractive Mixture Models via Squaring: Representation and Learning

    [https://arxiv.org/abs/2310.00724](https://arxiv.org/abs/2310.00724)

    通过平方操作实现的消减混合模型在表达能力上优于传统加法混合模型，并在真实世界分布估计任务中得到了实验证明。

    

    混合模型传统上是通过将几个分布作为组件相加来表示和学习的。允许混合减去概率质量或密度可以大大减少建模复杂分布所需的组件数量。然而，学习这种减法混合模型并确保它们仍然编码非负函数是具有挑战性的。我们探讨了如何通过平方来学习和执行深度减法混合模型。我们在概率电路框架中进行这些研究，这使我们能够表示张量化的混合模型并泛化其他减法模型。我们在理论上证明了允许减法的平方电路类可以比传统的加法混合模型具有指数级更具表达力；我们在一系列真实世界分布估计任务上实证展示了这种增加的表达力。

    arXiv:2310.00724v2 Announce Type: replace-cross  Abstract: Mixture models are traditionally represented and learned by adding several distributions as components. Allowing mixtures to subtract probability mass or density can drastically reduce the number of components needed to model complex distributions. However, learning such subtractive mixtures while ensuring they still encode a non-negative function is challenging. We investigate how to learn and perform inference on deep subtractive mixtures by squaring them. We do this in the framework of probabilistic circuits, which enable us to represent tensorized mixtures and generalize several other subtractive models. We theoretically prove that the class of squared circuits allowing subtractions can be exponentially more expressive than traditional additive mixtures; and, we empirically show this increased expressiveness on a series of real-world distribution estimation tasks.
    
[^300]: 通过逻辑增强大型语言模型中的零射链推理能力

    Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic

    [https://arxiv.org/abs/2309.13339](https://arxiv.org/abs/2309.13339)

    提出了LoT（Logical Thoughts）提示，一个自我改进框架，利用根植于符号逻辑的原则，特别是归谬法，逐步验证和纠正大型语言模型的零射链推理过程。

    

    大型语言模型的最新进展展示了它们在各个领域的 remarkable generalizability。然而，它们的推理能力仍有很大的提升空间，特别是在需要多步推理的情况下。尽管大型语言模型具有广泛的知识，但它们的推理经常未能有效利用这些知识来建立连贯的思维范式。这些模型有时会出现幻觉，因为它们的推理过程未受逻辑原则的限制。为了改进大型语言模型的零射链推理能力，我们提出了 LoT（Logical Thoughts）提示，这是一个自我改进的框架，利用根植于符号逻辑的原则，特别是归谬法，逐步系统地验证和纠正推理过程。在语言任务上进行的实验评估

    arXiv:2309.13339v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts) prompting, a self-improvement framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in
    
[^301]: 采用隐式神经表示的潜变同化未知动态

    Latent assimilation with implicit neural representations for unknown dynamics

    [https://arxiv.org/abs/2309.09574](https://arxiv.org/abs/2309.09574)

    LAINR通过引入球面隐式神经表示（SINR）和数据驱动的不确定性估计器，提高了同化过程的效率，并在准确性和效率方面优于现有基于自动编码器的方法。

    

    数据同化在广泛的应用中至关重要，但常常面临挑战，如由于数据维度和对底层机制的不完全理解而导致的高计算成本。为解决这些挑战，本研究提出了一种新颖的同化框架，称为具有隐式神经表示的潜变同化（LAINR）。通过引入球面隐式神经表示（SINR）以及对训练的神经网络进行数据驱动的不确定性估计器，LAINR提高了同化过程的效率。实验结果表明，与基于自动编码器的现有方法相比，LAINR在准确性和效率方面具有一定优势。

    arXiv:2309.09574v2 Announce Type: replace  Abstract: Data assimilation is crucial in a wide range of applications, but it often faces challenges such as high computational costs due to data dimensionality and incomplete understanding of underlying mechanisms. To address these challenges, this study presents a novel assimilation framework, termed Latent Assimilation with Implicit Neural Representations (LAINR). By introducing Spherical Implicit Neural Representations (SINR) along with a data-driven uncertainty estimator of the trained neural networks, LAINR enhances efficiency in assimilation process. Experimental results indicate that LAINR holds certain advantage over existing methods based on AutoEncoders, both in terms of accuracy and efficiency.
    
[^302]: 具有潜在随机微分建模的运动学感知轨迹生成与预测

    Kinematics-aware Trajectory Generation and Prediction with Latent Stochastic Differential Modeling

    [https://arxiv.org/abs/2309.09317](https://arxiv.org/abs/2309.09317)

    本研究通过潜在随机微分建模，提出了一种运动学感知的方法，同时结合数据驱动和模型驱动的优势，解决了自动驾驶中轨迹生成和预测中的物理现实性挑战。

    

    轨迹生成和轨迹预测是自动驾驶中的两项关键任务，前者在开发过程中生成各种测试轨迹，后者预测周围车辆的轨迹。近年来，新兴的数据驱动深度学习方法在学习各种交通场景和提高性能方面表现出色，但如何确保生成/预测的轨迹在物理上合理仍然是一个挑战。这个挑战的根源在于学习方法通常作为不透明黑匣子运行，并不遵守物理法则，而现有基于模型的方法提供物理可行的结果，但受到预定义模型结构的约束。

    arXiv:2309.09317v2 Announce Type: replace  Abstract: Trajectory generation and trajectory prediction are two critical tasks in autonomous driving, which generate various trajectories for testing during development and predict the trajectories of surrounding vehicles during operation, respectively. In recent years, emerging data-driven deep learning-based methods have shown great promise for these two tasks in learning various traffic scenarios and improving average performance without assuming physical models. However, it remains a challenging problem for these methods to ensure that the generated/predicted trajectories are physically realistic. This challenge arises because learning-based approaches often function as opaque black boxes and do not adhere to physical laws. Conversely, existing model-based methods provide physically feasible results but are constrained by predefined model structures, limiting their capabilities to address complex scenarios. To address the limitations of 
    
[^303]: 通过利用多层次结构提高风力发电的预测准确性

    Improving the forecast accuracy of wind power by leveraging multiple hierarchical structure

    [https://arxiv.org/abs/2308.03472](https://arxiv.org/abs/2308.03472)

    通过整合风电场中风力发电机的横截面和时间层次结构，构建跨时层次结构，从而提高风电场的预测准确性。

    

    可再生能源发电对全球减碳至关重要。预测可再生能源，特别是风能，具有挑战性，因为风能发电受气候条件的不确定性影响。最近通过协调实现的层次预测在短期内显著提高了风能预测的质量。我们利用风电场中风力发电机的横截面和时间层次结构，构建横时层次结构，进一步研究跨横截面和时间维度的整合如何增加风电场的预测准确性。我们发现，跨时间协调在多个时间汇总中优于单独跨横截面协调。此外，基于机器学习的跨时协调预测表现出对较粗时间聚合的高准确性。

    arXiv:2308.03472v2 Announce Type: replace  Abstract: Renewable energy generation is of utmost importance for global decarbonization. Forecasting renewable energies, particularly wind energy, is challenging due to the inherent uncertainty in wind energy generation, which depends on weather conditions. Recent advances in hierarchical forecasting through reconciliation have demonstrated a significant increase in the quality of wind energy forecasts for short-term periods. We leverage the cross-sectional and temporal hierarchical structure of turbines in wind farms and build cross-temporal hierarchies to further investigate how integrated cross-sectional and temporal dimensions can add value to forecast accuracy in wind farms. We found that cross-temporal reconciliation was superior to individual cross-sectional reconciliation at multiple temporal aggregations. Additionally, machine learning based forecasts that were cross-temporally reconciled demonstrated high accuracy at coarser tempora
    
[^304]: 具有小初始化的两层ReLU网络中的早期神经元对齐

    Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization

    [https://arxiv.org/abs/2307.12851](https://arxiv.org/abs/2307.12851)

    神经元在两层ReLU网络中早期阶段会尝试与输入数据对齐，对齐时间上界为$\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$，在对齐阶段过后损失收敛速度为$\mathcal{O}(\frac{1}{t})$

    

    本文研究了使用梯度流和小初始化对双层ReLU网络进行二元分类训练的问题。我们考虑一个带有良好分离的输入向量的训练数据集：任何具有相同标签的输入数据对正相关，具有不同标签的输入数据对负相关。我们的分析显示，在训练的早期阶段，第一层中的神经元试图与第二层上的权重对应的正数据或负数据对齐。对神经元方向动态的仔细分析使我们能够提供一个$\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$的时间上界，这是为了让所有神经元与输入数据良好对齐所需的时间，其中$n$是数据点的个数，$\mu$衡量数据分离的程度。在早期对齐阶段之后，损失以$\mathcal{O}(\frac{1}{t})$的速度收敛到零。

    arXiv:2307.12851v2 Announce Type: replace  Abstract: This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t
    
[^305]: 关于协作学习推荐系统针对社区检测攻击的韧性研究

    On the resilience of Collaborative Learning-based Recommender Systems Against Community Detection Attack

    [https://arxiv.org/abs/2306.08929](https://arxiv.org/abs/2306.08929)

    本文研究了协作学习推荐系统针对一种新型隐私攻击——社区检测攻击（CDA）的韧性。

    

    协作学习推荐系统源于协作学习技术（如联邦学习和八卦学习）的成功。在这些系统中，用户参与推荐系统的训练同时在其设备上保留已消费项目的历史记录。虽然这些解决方案乍一看似乎有利于保护参与者的隐私，但最近的研究表明，协作学习可能容易受到各种隐私攻击的威胁。本文研究了协作学习推荐系统针对一种称为社区检测攻击（CDA）的新型隐私攻击的韧性。这种攻击使得对手能够基于一个选择的项目集（如识别对特定兴趣点感兴趣的用户）来识别社区成员。通过在三个真实推荐数据集上进行实验，使用两种最先进的推荐

    arXiv:2306.08929v2 Announce Type: replace-cross  Abstract: Collaborative-learning-based recommender systems emerged following the success of collaborative learning techniques such as Federated Learning (FL) and Gossip Learning (GL). In these systems, users participate in the training of a recommender system while maintaining their history of consumed items on their devices. While these solutions seemed appealing for preserving the privacy of the participants at first glance, recent studies have revealed that collaborative learning can be vulnerable to various privacy attacks. In this paper, we study the resilience of collaborative learning-based recommender systems against a novel privacy attack called Community Detection Attack (CDA). This attack enables an adversary to identify community members based on a chosen set of items (eg., identifying users interested in specific points-of-interest). Through experiments on three real recommendation datasets using two state-of-the-art recomme
    
[^306]: 从一组帕累托最优解中识别能源管理配置概念

    Identification of Energy Management Configuration Concepts from a Set of Pareto-optimal Solutions

    [https://arxiv.org/abs/2306.08318](https://arxiv.org/abs/2306.08318)

    通过概念识别技术从庞大的帕累托最优解数据集中找到相关和可行的能源管理配置。

    

    在设施和建筑物中实施资源高效的能源管理系统在向可持续社会转型中变得越来越重要。然而，基于多个通常存在冲突目标（如成本、对电网运行不确定性的稳健性或可再生能源利用）选择合适配置是一个困难的多目标决策问题。最近开发的概念识别技术可以通过将配置选项分为语义上有意义的组（概念）来帮助决策者。在这个过程中，将目标和设计参数分成不同集合（称为描述空间）是一个非常重要的步骤。本研究侧重利用概念识别技术从一个非常庞大的帕累托最优解数据集中找到相关和可行的能源管理配置。

    arXiv:2306.08318v2 Announce Type: replace  Abstract: Implementing resource efficient energy management systems in facilities and buildings becomes increasingly important in the transformation to a sustainable society. However, selecting a suitable configuration based on multiple, typically conflicting objectives, such as cost, robustness with respect to uncertainty of grid operation, or renewable energy utilization, is a difficult multi-criteria decision making problem. The recently developed concept identification technique can facilitate a decision maker by sorting configuration options into semantically meaningful groups (concepts). In this process, the partitioning of the objectives and design parameters into different sets (called description spaces) is a very important step. In this study we focus on utilizing the concept identification technique for finding relevant and viable energy management configurations from a very large data set of Pareto-optimal solutions. The data set c
    
[^307]: 重新思考领域泛化的评估协议

    Rethinking the Evaluation Protocol of Domain Generalization

    [https://arxiv.org/abs/2305.15253](https://arxiv.org/abs/2305.15253)

    重新评估领域泛化的评估协议，提出采用自监督预训练或从头开始训练，使用多个测试领域，以更准确评估OOD泛化能力。

    

    领域泛化目的是通过利用从多个训练领域学到的共同知识，解决面向未见测试领域的超出分布（OOD）泛化挑战。为了准确评估OOD泛化能力，需要测试数据信息不可用。然而，当前的领域泛化协议仍可能存在潜在的测试数据信息泄漏。本文从当前评估协议的两个方面：在ImageNet上进行监督预训练和oracle模型选择，探讨测试数据信息泄漏的风险。我们提出修改当前协议的建议，即应该采用自监督预训练或从头开始训练，而不是采用当前的监督预训练，并且应该使用多个测试领域。这将导致对OOD泛化能力更精确的评估。我们还重新运行了带有修改后协议的算法。

    arXiv:2305.15253v2 Announce Type: replace-cross  Abstract: Domain generalization aims to solve the challenge of Out-of-Distribution (OOD) generalization by leveraging common knowledge learned from multiple training domains to generalize to unseen test domains. To accurately evaluate the OOD generalization ability, it is required that test data information is unavailable. However, the current domain generalization protocol may still have potential test data information leakage. This paper examines the risks of test data information leakage from two aspects of the current evaluation protocol: supervised pretraining on ImageNet and oracle model selection. We propose modifications to the current protocol that we should employ self-supervised pretraining or train from scratch instead of employing the current supervised pretraining, and we should use multiple test domains. These would result in a more precise evaluation of OOD generalization ability. We also rerun the algorithms with the mod
    
[^308]: 版权是否可以简化为隐私？

    Can Copyright be Reduced to Privacy?

    [https://arxiv.org/abs/2305.14822](https://arxiv.org/abs/2305.14822)

    生成式人工智能模型可能违反版权法，研究人员正在探索算法稳定性技术以确保对生成模型的负责任使用。

    

    有人越来越担心生成式人工智能模型将产生与它们训练所依据的受版权保护的材料非常相似的产出。随着生成模型的质量和复杂性大幅提高，并且包含受版权保护材料的广泛数据集的可用性不断扩大，人们对这种担忧加剧。研究人员正在积极探索缓解生成侵权样本风险的策略，最近的一些工作建议采用差分隐私等技术和其他形式的算法稳定性来提供关于缺乏侵权复制的保证。在这项工作中，我们研究了这些算法稳定性技术是否适合确保对生成式模型的负责任使用，而不会无意中违反版权法。我们认为，尽管这些技术旨在验证数据集中可识别信息的存在，因此是面向隐私的，但它们可能存在一些仍需要解决的和版权相关的问题。

    arXiv:2305.14822v2 Announce Type: replace  Abstract: There is a growing concern that generative AI models will generate outputs closely resembling the copyrighted materials for which they are trained. This worry has intensified as the quality and complexity of generative models have immensely improved, and the availability of extensive datasets containing copyrighted material has expanded. Researchers are actively exploring strategies to mitigate the risk of generating infringing samples, with a recent line of work suggesting to employ techniques such as differential privacy and other forms of algorithmic stability to provide guarantees on the lack of infringing copying. In this work, we examine whether such algorithmic stability techniques are suitable to ensure the responsible use of generative models without inadvertently violating copyright laws. We argue that while these techniques aim to verify the presence of identifiable information in datasets, thus being privacy-oriented, cop
    
[^309]: LLM亲子鉴定：LLM遗传继承中的生成文本检测

    LLM Paternity Test: Generated Text Detection with LLM Genetic Inheritance

    [https://arxiv.org/abs/2305.12519](https://arxiv.org/abs/2305.12519)

    LLM-Pat提出了一种基于模型的生成文本检测方法，通过重建并比较候选文本与其对应的“兄弟”文本的相似性，从而判断候选文本是否由机器生成。

    

    大语言模型（LLMs）可以生成携带各种滥用风险的文本，包括抄袭、在电子商务平台上发布虚假评论，或者制作引人注目的虚假推文。因此，检测文本是否由机器生成变得越来越重要。虽然现有的检测方法表现出色，但由于严重依赖训练数据，它们往往缺乏泛化能力。为缓解这一问题，我们提出了一种与模型相关的生成文本检测方法，即LLM亲子鉴定（LLM-Pat）。具体而言，给定任何候选文本（"子类"），LLM-Pat使用一个中间LLM（"父类"）重建与给定文本对应的"兄弟"文本，然后衡量候选文本与其"兄弟"文本之间的相似性。高相似性表明候选文本是由机器生成，类似于基因特征。我们已构建了数据集...

    arXiv:2305.12519v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) can generate texts that carry the risk of various misuses, including plagiarism, planting fake reviews on e-commerce platforms, or creating inflammatory false tweets. Detecting whether a text is machine-generated has thus become increasingly important. While existing detection methods exhibit superior performance, they often lack generalizability due to their heavy dependence on training data. To alleviate this problem, we propose a model-related generated text detection method, the LLM Paternity Test (LLM-Pat). Specifically, given any candidate text (\textit{child}), LLM-Pat employs an intermediary LLM (\textit{parent}) to reconstruct a \textit{sibling} text corresponding to the given text and then measures the similarity between candidate texts and their sibling texts. High similarity indicates that the candidate text is machine-generated, akin to genetic traits. We have constructed datasets encom
    
[^310]: EHRDiff: 使用扩散模型探索真实的电子健康记录综合

    EHRDiff: Exploring Realistic EHR Synthesis with Diffusion Models

    [https://arxiv.org/abs/2303.05656](https://arxiv.org/abs/2303.05656)

    该研究探索了使用扩散模型在电子健康记录（EHR）数据合成方面的潜力，挑战传统基于生成对抗网络（GAN）的方法。

    

    电子健康记录（EHR）包含丰富的生物医学信息，是发展精准医学系统的宝贵资源。然而，隐私担忧导致研究人员获取高质量和大规模EHR数据的限制，阻碍了方法研发的进展。最近的研究深入探讨了通过生成建模技术合成真实的EHR数据，其中大多数提出的方法依赖生成对抗网络（GAN）及其变体用于EHR综合。尽管基于GAN的方法在生成EHR数据方面取得了最先进的性能，但这些方法难以训练且容易出现模式坍塌。扩散模型是生成建模中最新引入的，已经在图像生成方面取得了尖端性能，但它们在EHR数据综合方面的效力仍未得到充分探索。本研究中，我们探讨了d

    arXiv:2303.05656v2 Announce Type: replace  Abstract: Electronic health records (EHR) contain a wealth of biomedical information, serving as valuable resources for the development of precision medicine systems. However, privacy concerns have resulted in limited access to high-quality and large-scale EHR data for researchers, impeding progress in methodological development. Recent research has delved into synthesizing realistic EHR data through generative modeling techniques, where a majority of proposed methods relied on generative adversarial networks (GAN) and their variants for EHR synthesis. Despite GAN-based methods attaining state-of-the-art performance in generating EHR data, these approaches are difficult to train and prone to mode collapse. Recently introduced in generative modeling, diffusion models have established cutting-edge performance in image generation, but their efficacy in EHR data synthesis remains largely unexplored. In this study, we investigate the potential of d
    
[^311]: 自伯尔曼分布的成对相互作用自旋系统的自回归神经网络架构

    The autoregressive neural network architecture of the Boltzmann distribution of pairwise interacting spins systems

    [https://arxiv.org/abs/2302.08347](https://arxiv.org/abs/2302.08347)

    该论文提出了将二进制成对相互作用系统的伯尔曼分布精确映射为自回归形式的方法，得到的ARNN架构具有明确定义的物理含义，并且可以通过统计物理技术推导出特定系统的新ARNN。

    

    arXiv:2302.08347v3 公告类型: 替换-跨 Abstract: 生成式自回归神经网络（ARNNs）最近在图像和语言生成任务中展现出杰出的结果，促成了生成模型在科学和商业应用中日益流行。本工作将二进制成对相互作用系统的伯尔曼分布精确映射为自回归形式。结果的ARNN架构具有与哈密顿耦合和外场对应的第一层的权重和偏置，具有诸如残差连接和具有明确定义物理含义的递归架构等广泛使用的结构。此外，其架构的明确表述使得可以利用统计物理技术推导特定系统的新ARNN。作为示例，从两个知名的平均场系统，居里-魏斯和Sherrington-Kirkpatrick模型，导出了新的有效ARNN架构，展示了

    arXiv:2302.08347v3 Announce Type: replace-cross  Abstract: Generative Autoregressive Neural Networks (ARNNs) have recently demonstrated exceptional results in image and language generation tasks, contributing to the growing popularity of generative models in both scientific and commercial applications. This work presents an exact mapping of the Boltzmann distribution of binary pairwise interacting systems into autoregressive form. The resulting ARNN architecture has weights and biases of its first layer corresponding to the Hamiltonian's couplings and external fields, featuring widely used structures such as the residual connections and a recurrent architecture with clear physical meanings. Moreover, its architecture's explicit formulation enables the use of statistical physics techniques to derive new ARNNs for specific systems. As examples, new effective ARNN architectures are derived from two well-known mean-field systems, the Curie-Weiss and Sherrington-Kirkpatrick models, showing 
    
[^312]: LMC: 基于子图抽样的GNN快速训练与收敛性保证

    LMC: Fast Training of GNNs via Subgraph Sampling with Provable Convergence

    [https://arxiv.org/abs/2302.00924](https://arxiv.org/abs/2302.00924)

    LMC是第一个带有收敛性保证的子图抽样方法，旨在解决邻居爆炸问题，提高训练收敛速度。

    

    基于消息传递的图神经网络（GNNs）在许多实际应用中取得了巨大成功。然而，在大规模图上训练GNNs存在着众所周知的邻居爆炸问题，即节点与消息传递层数的增加呈指数级增加的依赖关系。子图抽样方法是一类有前途的小批量训练技术，通过在反向传播中丢弃小批量之外的消息来避免邻居爆炸问题，但这会牺牲梯度估计的准确性。为解决这一挑战，我们提出了一种新颖的带有收敛性保证的子图抽样方法，即局部消息补偿（LMC）。

    arXiv:2302.00924v3 Announce Type: replace  Abstract: The message passing-based graph neural networks (GNNs) have achieved great success in many real-world applications. However, training GNNs on large-scale graphs suffers from the well-known neighbor explosion problem, i.e., the exponentially increasing dependencies of nodes with the number of message passing layers. Subgraph-wise sampling methods -- a promising class of mini-batch training techniques -- discard messages outside the mini-batches in backward passes to avoid the neighbor explosion problem at the expense of gradient estimation accuracy. This poses significant challenges to their convergence analysis and convergence speeds, which seriously limits their reliable real-world applications. To address this challenge, we propose a novel subgraph-wise sampling method with a convergence guarantee, namely Local Message Compensation (LMC). To the best of our knowledge, LMC is the {\it first} subgraph-wise sampling method with provab
    
[^313]: 基于高斯过程的深度状态空间模型的顺序估计

    Sequential Estimation of Gaussian Process-based Deep State-Space Models

    [https://arxiv.org/abs/2301.12528](https://arxiv.org/abs/2301.12528)

    该论文提出了一种基于高斯过程和深度高斯过程的方法，用于顺序估计包括潜在过程在内的状态空间和深度状态空间模型的未知量。

    

    我们考虑了包括函数估计和模型的潜在过程在内的状态空间和深度状态空间模型的未知量的顺序估计问题。所提出的方法依赖于通过随机特征的高斯过程实现的高斯和深度高斯过程。在这些模型中，我们有两组未知量，高度非线性未知量（潜在过程的值）和条件线性未知量（基于随机特征的高斯过程的常数参数）。我们提出了一种基于粒子滤波的方法，其中基于随机特征的高斯过程的参数在获得状态的预测密度时被整合在外，不需要粒子。我们还提出了该方法的集合版本，其中集合的每个成员都有自己的特征集。通过多个实验，我们展示了该方法可以跟踪潜在过程直到...

    arXiv:2301.12528v2 Announce Type: replace  Abstract: We consider the problem of sequential estimation of the unknowns of state-space and deep state-space models that include estimation of functions and latent processes of the models. The proposed approach relies on Gaussian and deep Gaussian processes that are implemented via random feature-based Gaussian processes. In these models, we have two sets of unknowns, highly nonlinear unknowns (the values of the latent processes) and conditionally linear unknowns (the constant parameters of the random feature-based Gaussian processes). We present a method based on particle filtering where the parameters of the random feature-based Gaussian processes are integrated out in obtaining the predictive density of the states and do not need particles. We also propose an ensemble version of the method, with each member of the ensemble having its own set of features. With several experiments, we show that the method can track the latent processes up t
    
[^314]: FreshGNN: 通过稳定的历史嵌入减少图神经网络训练中的内存访问

    FreshGNN: Reducing Memory Access via Stable Historical Embeddings for Graph Neural Network Training

    [https://arxiv.org/abs/2301.07482](https://arxiv.org/abs/2301.07482)

    FreshGNN是一种通用的GNN小批量训练框架，通过稳定的历史缓存存储和重复使用GNN节点嵌入，在训练大型GNN模型时减少内存访问，解决了当前框架存在的准确性降低问题。

    

    训练大规模真实世界图上的图神经网络（GNN）模型时的一个关键性能瓶颈是将节点特征加载到GPU上。由于GPU内存有限，需要进行昂贵的数据移动，以便在速度较慢的备用设备上存储这些特征（例如CPU内存）。此外，图结构的不规则性导致数据局部性差，进一步加剧了问题。因此，目前能够高效训练大型GNN模型的现有框架通常由于涉及当前可用的快捷方式而导致显著的准确性降低。 为解决这些限制，我们提出了FreshGNN，这是一个通用的GNN小批量训练框架，它利用一个历史缓存来存储和重复使用GNN节点嵌入，而不是通过在每次迭代中提取原始特征来重新计算它们。 其成功的关键在于相应的缓存策略。

    arXiv:2301.07482v3 Announce Type: replace  Abstract: A key performance bottleneck when training graph neural network (GNN) models on large, real-world graphs is loading node features onto a GPU. Due to limited GPU memory, expensive data movement is necessary to facilitate the storage of these features on alternative devices with slower access (e.g. CPU memory). Moreover, the irregularity of graph structures contributes to poor data locality which further exacerbates the problem. Consequently, existing frameworks capable of efficiently training large GNN models usually incur a significant accuracy degradation because of the currently-available shortcuts involved. To address these limitations, we instead propose FreshGNN, a general-purpose GNN mini-batch training framework that leverages a historical cache for storing and reusing GNN node embeddings instead of re-computing them through fetching raw features at every iteration. Critical to its success, the corresponding cache policy is de
    
[^315]: 掩码向量量化

    Masked Vector Quantization

    [https://arxiv.org/abs/2301.06626](https://arxiv.org/abs/2301.06626)

    提出了一种Masked Vector Quantization（MVQ）框架，通过学习掩码配置并使用Multiple Hypothese Dropout（MH-Dropout）训练制度，增加了每个代码向量的表示能力，在图像数据集上取得了显著的性能提升。

    

    具有离散潜在表示的生成模型最近展示了学习复杂高维数据分布的令人印象深刻的能力。然而，它们的性能依赖于每个实例的长序列标记和大量码书条目，导致长采样时间和相当大的计算来拟合分类后验。为了解决这些问题，我们提出了掩码向量量化（MVQ）框架，通过学习掩码配置，通过称为多假设丢失（MH-Dropout）的随机胜者通吃训练制度，增加每个代码向量的表示能力。在ImageNet 64×64上，MVQ将现有向量量化架构中的FID降低了高达68%（每实例2个标记）和57%（每实例5个标记）。这些改进随着减少码书条目的代码条目而扩大，并且在推断过程中可以实现7-45倍的标记采样加速。

    arXiv:2301.06626v2 Announce Type: replace  Abstract: Generative models with discrete latent representations have recently demonstrated an impressive ability to learn complex high-dimensional data distributions. However, their performance relies on a long sequence of tokens per instance and a large number of codebook entries, resulting in long sampling times and considerable computation to fit the categorical posterior. To address these issues, we propose the Masked Vector Quantization (MVQ) framework which increases the representational capacity of each code vector by learning mask configurations via a stochastic winner-takes-all training regime called Multiple Hypothese Dropout (MH-Dropout). On ImageNet 64$\times$64, MVQ reduces FID in existing vector quantization architectures by up to $68\%$ at 2 tokens per instance and $57\%$ at 5 tokens. These improvements widen as codebook entries is reduced and allows for $7\textit{--}45\times$ speed-up in token sampling during inference. As an 
    
[^316]: 分布鲁棒性界定了泛化错误

    Distributional Robustness Bounds Generalization Errors

    [https://arxiv.org/abs/2212.09962](https://arxiv.org/abs/2212.09962)

    分布鲁棒性界定了泛化错误，Bayesian方法在可能近似正确意义上是分布鲁棒的，同时正则化的经验风险最小化方法也被证明是等价于Bayesian方法的。

    

    Bayesian methods, distributionally robust optimization methods, and regularization methods是值得信赖的机器学习的基石，用于抵抗分布不确定性，比如经验分布与真实基础分布之间的不确定性。本文研究了这三种框架之间的联系，特别地探讨了为何这些框架倾向于具有更小的泛化错误。具体地，首先，我们提出了“分布鲁棒性”的定量定义，提出了“鲁棒性度量”的概念，并形式化了分布鲁棒性优化中的几个哲学概念。其次，我们表明Bayesian方法在可能近似正确意义上是分布鲁棒的；此外，通过构造类似Dirichlet过程的先验于贝叶斯非参数模型中，可以证明任何正则化的经验风险最小化方法等价于

    arXiv:2212.09962v3 Announce Type: replace  Abstract: Bayesian methods, distributionally robust optimization methods, and regularization methods are three pillars of trustworthy machine learning combating distributional uncertainty, e.g., the uncertainty of an empirical distribution compared to the true underlying distribution. This paper investigates the connections among the three frameworks and, in particular, explores why these frameworks tend to have smaller generalization errors. Specifically, first, we suggest a quantitative definition for "distributional robustness", propose the concept of "robustness measure", and formalize several philosophical concepts in distributionally robust optimization. Second, we show that Bayesian methods are distributionally robust in the probably approximately correct (PAC) sense; in addition, by constructing a Dirichlet-process-like prior in Bayesian nonparametrics, it can be proven that any regularized empirical risk minimization method is equival
    
[^317]: 实体排名的最大似然估计不确定性量化与外生变量

    Uncertainty Quantification of MLE for Entity Ranking with Covariates

    [https://arxiv.org/abs/2212.09961](https://arxiv.org/abs/2212.09961)

    提出了一种新颖的 Covariate-Assisted Ranking Estimation (CARE) 模型，扩展了 Bradley-Terry-Luce (BTL) 模型，通过将协变量信息结合进排名估计中，解决了实体排名问题中的不确定性。

    

    本文关注基于成对比较和额外协变量信息（如所比较项目的属性）的排名问题的统计估计和推断。尽管进行了大量研究，但先前的文献中很少有人在协变量信息存在的更现实情境下研究了这个问题。为了解决这个问题，我们提出了一个新颖的模型，即 Covariate-Assisted Ranking Estimation (CARE) 模型，它通过引入协变量信息扩展了著名的 Bradley-Terry-Luce (BTL) 模型。具体而言，我们假设每个比较项目的潜在分数不是固定的 $\{\theta_i^*\}_{i=1}^n$，而是由 $\{\alpha_i^*+{x}_i^\top\beta^*\}_{i=1}^n$ 给出，其中 $\alpha_i^*$ 和 ${x}_i^\top\beta^*$ 分别代表第 $i$ 个项目的潜在基准分数和协变量分数。我们加入了自然的可识别性条件，并推导了 $\ell_{\infty}$-

    arXiv:2212.09961v2 Announce Type: replace-cross  Abstract: This paper concerns with statistical estimation and inference for the ranking problems based on pairwise comparisons with additional covariate information such as the attributes of the compared items. Despite extensive studies, few prior literatures investigate this problem under the more realistic setting where covariate information exists. To tackle this issue, we propose a novel model, Covariate-Assisted Ranking Estimation (CARE) model, that extends the well-known Bradley-Terry-Luce (BTL) model, by incorporating the covariate information. Specifically, instead of assuming every compared item has a fixed latent score $\{\theta_i^*\}_{i=1}^n$, we assume the underlying scores are given by $\{\alpha_i^*+{x}_i^\top\beta^*\}_{i=1}^n$, where $\alpha_i^*$ and ${x}_i^\top\beta^*$ represent latent baseline and covariate score of the $i$-th item, respectively. We impose natural identifiability conditions and derive the $\ell_{\infty}$-
    
[^318]: 知识增强多模态学习综述

    A survey on knowledge-enhanced multimodal learning

    [https://arxiv.org/abs/2211.12328](https://arxiv.org/abs/2211.12328)

    知识图和其他知识来源填补了视觉语言学习模型在日常知识理解方面的差距，提升了模型的性能和可解释性。

    

    多模态学习是一个越来越受关注的领域，旨在将各种模态结合成一个联合表示。特别是在视觉语言学习领域，已经开发出多种模型和技术，针对涉及图像和文本的各种任务。VL模型通过扩展Transformer的思想，使得两种模态可以相互学习，已经达到了前所未有的性能。大规模的预训练程序使得VL模型能够获得一定水平的现实世界理解，尽管仍然存在许多差距：对常识、事实、时间和其他日常知识方面的限制理解，对VL任务的可扩展性提出了质疑。知识图和其他知识来源可以通过明确提供缺失信息来填补这些差距，解锁VL模型的新能力。同时，知识图增强了可解释性、公平性。

    arXiv:2211.12328v3 Announce Type: replace-cross  Abstract: Multimodal learning has been a field of increasing interest, aiming to combine various modalities in a single joint representation. Especially in the area of visiolinguistic (VL) learning multiple models and techniques have been developed, targeting a variety of tasks that involve images and text. VL models have reached unprecedented performances by extending the idea of Transformers, so that both modalities can learn from each other. Massive pre-training procedures enable VL models to acquire a certain level of real-world understanding, although many gaps can be identified: the limited comprehension of commonsense, factual, temporal and other everyday knowledge aspects questions the extendability of VL tasks. Knowledge graphs and other knowledge sources can fill those gaps by explicitly providing missing information, unlocking novel capabilities of VL models. In the same time, knowledge graphs enhance explainability, fairness 
    
[^319]: 利用合成数据进行监督学习以实现稳健的5自由度磁标记定位

    Utilizing Synthetic Data in Supervised Learning for Robust 5-DoF Magnetic Marker Localization

    [https://arxiv.org/abs/2211.07556](https://arxiv.org/abs/2211.07556)

    使用神经网络绕过传统迭代优化方法和磁偶极模型的限制，直接推断磁标记的位置和方向，提高定位的准确性和效率

    

    追踪被动磁标记在推动医疗保健和机器人技术方面起着至关重要的作用，有望显著提高系统的精度和效率。传统上，由于需要进行迭代优化过程，磁标记的跟踪在计算上是昂贵的。此外，这些方法依赖于磁偶极模型来进行优化，由于该模型在处理非球形磁铁和传感器之间的短距离时存在显著的不准确性，这可能导致结果不准确。我们的论文引入了一种新颖的方法，利用神经网络来规避这些限制，直接推断标记的位置和方向，以实现更准确的结果。

    arXiv:2211.07556v2 Announce Type: replace  Abstract: Tracking passive magnetic markers plays a vital role in advancing healthcare and robotics, offering the potential to significantly improve the precision and efficiency of systems. This technology is key to developing smarter, more responsive tools and devices, such as enhanced surgical instruments, precise diagnostic tools, and robots with improved environmental interaction capabilities. However, traditionally, the tracking of magnetic markers is computationally expensive due to the requirement for iterative optimization procedures. Moreover, these methods depend on the magnetic dipole model for their optimization function, which can yield imprecise outcomes due to the model's significant inaccuracies when dealing with short distances between non-spherical magnet and sensor.Our paper introduces a novel approach that leverages neural networks to bypass these limitations, directly inferring the marker's position and orientation to accu
    
[^320]: 使用纵向自监督学习进行糖尿病视网膜病变检测

    Detection of diabetic retinopathy using longitudinal self-supervised learning

    [https://arxiv.org/abs/2209.00915](https://arxiv.org/abs/2209.00915)

    本研究探讨了利用纵向自监督学习对糖尿病视网膜病变进行诊断的益处，通过比较不同方法模拟疾病进展并在纵向眼底照片中检测早期DR严重程度变化，取得了较高的AUC。

    

    纵向成像能够捕捉静态解剖结构和疾病进展中的动态变化，实现对疾病的更早和更好的个体化病理管理。然而，传统的检测糖尿病视网膜病变（DR）的方法很少利用纵向信息来改善DR分析。在这项工作中，我们研究了利用具有纵向性质的自监督学习来进行DR诊断的好处。我们比较了不同的纵向自监督学习（LSSL）方法，用来模拟从纵向视网膜彩色眼底照片（CFP）中检测早期DR严重程度变化，使用一对连续检查。实验是在一个纵向DR筛查数据集上进行的，有或没有那些训练好的编码器（LSSL）作为纵向假设任务。结果在基线（从头开始训练的模型）上实现了0.875的AUC。

    arXiv:2209.00915v3 Announce Type: replace-cross  Abstract: Longitudinal imaging is able to capture both static anatomical structures and dynamic changes in disease progression towards earlier and better patient-specific pathology management. However, conventional approaches for detecting diabetic retinopathy (DR) rarely take advantage of longitudinal information to improve DR analysis. In this work, we investigate the benefit of exploiting self-supervised learning with a longitudinal nature for DR diagnosis purposes. We compare different longitudinal self-supervised learning (LSSL) methods to model the disease progression from longitudinal retinal color fundus photographs (CFP) to detect early DR severity changes using a pair of consecutive exams. The experiments were conducted on a longitudinal DR screening dataset with or without those trained encoders (LSSL) acting as a longitudinal pretext task. Results achieve an AUC of 0.875 for the baseline (model trained from scratch) and an AU
    
[^321]: 关于数据增强对隐私影响的研究

    On the Privacy Effect of Data Enhancement via the Lens of Memorization

    [https://arxiv.org/abs/2208.08270](https://arxiv.org/abs/2208.08270)

    该论文通过记忆的视角研究了数据增强对机器学习模型隐私泄露的影响，发现先前的成员推理攻击产生误导性结果，提出了一种新的攻击方法来评估个体样本的记忆程度。

    

    机器学习带来了严重的隐私问题，因为已经显示学习的模型可能会揭示有关其训练数据的敏感信息。 许多研究探讨了广泛采用的数据增强和对抗训练技术（在论文中称为数据增强）对机器学习模型隐私泄露的影响。 这种隐私效应通常通过成员推理攻击（MIAs）来衡量，其目的是确定特定示例是否属于训练集。 我们提出从称为记忆的新视角来研究隐私。 通过记忆的视角，我们发现先前部署的MIAs产生误导性结果，因为它们不太可能识别高隐私风险样本是否作为成员，相比之下，识别低隐私风险样本更容易。 为解决这一问题，我们部署了一种最近的攻击，可以捕获个体样本的记忆程度进行评估。

    arXiv:2208.08270v3 Announce Type: replace  Abstract: Machine learning poses severe privacy concerns as it has been shown that the learned models can reveal sensitive information about their training data. Many works have investigated the effect of widely adopted data augmentation and adversarial training techniques, termed data enhancement in the paper, on the privacy leakage of machine learning models. Such privacy effects are often measured by membership inference attacks (MIAs), which aim to identify whether a particular example belongs to the training set or not. We propose to investigate privacy from a new perspective called memorization. Through the lens of memorization, we find that previously deployed MIAs produce misleading results as they are less likely to identify samples with higher privacy risks as members compared to samples with low privacy risks. To solve this problem, we deploy a recent attack that can capture individual samples' memorization degrees for evaluation. T
    
[^322]: 图匹配的动态softassign和自适应参数调整

    Dynamical softassign and adaptive parameter tuning for graph matching

    [https://arxiv.org/abs/2208.08233](https://arxiv.org/abs/2208.08233)

    本文提出了一种图匹配算法，结合了自适应步长参数和动态softassign策略，能够提高收敛性和效率，特别适用于完全连接的图匹配问题。

    

    本文研究了一种称为约束梯度方法的图匹配问题的统一框架。在该框架内的流行算法包括递归分配（GA）、整数投影固定点法（IPFP）和双随机投影固定点法（DSPFP）。 这些算法在步长参数和约束算子上有所不同。我们提出的自适应步长参数可以保证基础算法的收敛性，增强它们的效率和准确性。初步分析表明，在完全连接的图匹配中，最优步长参数有很高的概率为1。其次，我们提出了一种动态策略来处理softassign这一流行约束算子在节点基数和溢出风险方面的敏感性。 结合自适应步长参数和动态softassign，我们提出了一种新颖的图匹配算法：softas

    arXiv:2208.08233v3 Announce Type: replace-cross  Abstract: This paper studies a unified framework for graph matching problems called the constrained gradient method. Popular algorithms within this framework include graduated assignment (GA), integer projected fixed-point method (IPFP), and doubly stochastic projected fixed-point method (DSPFP). These algorithms differ from the step size parameter and constrained operator. Our contributed adaptive step size parameter can guarantee the underlying algorithms' convergence and enhance their efficiency and accuracy. A preliminary analysis suggests that the optimal step size parameter has a high probability of being 1 in fully connected graph matching. Secondly, we propose a dynamic strategy for softassign, a popular constrained operator, to address its sensitivity concerning nodes' cardinality and risk of overflow. Combining the adaptive step size parameter and the dynamical softassign, we propose a novel graph matching algorithm: the softas
    
[^323]: 一种基于抽样的无伴随算法用于条件非线性最优扰动（CNOPs）

    An adjoint-free algorithm for conditional nonlinear optimal perturbations (CNOPs) via sampling

    [https://arxiv.org/abs/2208.00956](https://arxiv.org/abs/2208.00956)

    通过抽样算法获取条件非线性最优扰动，避免传统优化方法中昂贵的梯度计算成本和无法使用的伴随技术。

    

    在本文中，我们提出了一种基于最先进的统计机器学习技术的抽样算法，用于获得条件非线性最优扰动（CNOPs），与传统（确定性）优化方法不同。具体来说，传统方法在实践中无法使用，需要数值计算梯度（一阶信息），因此计算成本昂贵，需要大量运行数值模型的次数。然而，抽样方法直接将梯度降低到目标函数值（零阶信息），同时避免使用无法用于许多大气和海洋模型的伴随技术，并且需要大量存储空间。我们从大数定律出发对抽样算法进行了直观分析，并进一步提出了一个Chernoff类型的集中不等式，以严格刻画其

    arXiv:2208.00956v5 Announce Type: replace-cross  Abstract: In this paper, we propose a sampling algorithm based on state-of-the-art statistical machine learning techniques to obtain conditional nonlinear optimal perturbations (CNOPs), which is different from traditional (deterministic) optimization methods.1 Specifically, the traditional approach is unavailable in practice, which requires numerically computing the gradient (first-order information) such that the computation cost is expensive, since it needs a large number of times to run numerical models. However, the sampling approach directly reduces the gradient to the objective function value (zeroth-order information), which also avoids using the adjoint technique that is unusable for many atmosphere and ocean models and requires large amounts of storage. We show an intuitive analysis for the sampling algorithm from the law of large numbers and further present a Chernoff-type concentration inequality to rigorously characterize the
    
[^324]: 具有有限源知识下的Wasserstein分布鲁棒性对未知领域的泛化

    Generalizing to Unseen Domains with Wasserstein Distributional Robustness under Limited Source Knowledge

    [https://arxiv.org/abs/2207.04913](https://arxiv.org/abs/2207.04913)

    提出了一种WDRDG框架，通过优化类别特定的Wasserstein不确定性集合中的条件分布，实现在未知领域中基于有限源知识的泛化。

    

    域泛化旨在学习一个通用模型，可以在未知目标领域上表现良好，并结合来自多个源域的知识。本研究考虑了在不同类别的条件分布之间发生不同领域转变的情形。当源域中标记样本有限时，现有方法不够健壮。为了解决这一问题，我们提出了一种新颖的领域泛化框架，称为{Wasserstein分布鲁棒领域泛化} (WDRDG)，灵感来自分布鲁棒优化的概念。我们鼓励在类别特定Wasserstein不确定性集合中的条件分布上的鲁棒性，并优化分类器在这些不确定性集合上的最坏情况性能。我们进一步开发了一个利用最优输运的测试时间适应模块，以量化未知目标之间关系。

    arXiv:2207.04913v2 Announce Type: replace  Abstract: Domain generalization aims at learning a universal model that performs well on unseen target domains, incorporating knowledge from multiple source domains. In this research, we consider the scenario where different domain shifts occur among conditional distributions of different classes across domains. When labeled samples in the source domains are limited, existing approaches are not sufficiently robust. To address this problem, we propose a novel domain generalization framework called {Wasserstein Distributionally Robust Domain Generalization} (WDRDG), inspired by the concept of distributionally robust optimization. We encourage robustness over conditional distributions within class-specific Wasserstein uncertainty sets and optimize the worst-case performance of a classifier over these uncertainty sets. We further develop a test-time adaptation module leveraging optimal transport to quantify the relationship between the unseen targ
    
[^325]: 图中球的样本压缩方案

    Sample compression schemes for balls in graphs

    [https://arxiv.org/abs/2206.13254](https://arxiv.org/abs/2206.13254)

    设计了适用于树、环、区间图、环树和自由立方体中位图的球的样本压缩方案，具有不同大小，为未解决的VC维度$d$的问题提供了新的见解。

    

    机器学习中的一个未解问题是任何VC维度$d$的集合族是否接受大小为$O(d)$的样本压缩方案。本文研究了图中球的这个问题。对于图$G=(V,E)$中的一个球$B=B_r(x)$，球$B$的一个可实现样本是$V$的一个有符号子集$X=(X^+,X^-)$，使得$B$包含$X^+$且与$X^-$不相交。大小为$k$的适当样本压缩方案包括一个压缩器和一个重构器。压缩器将任何可实现样本$X$映射为大小最多为$k$的子样本$X'$。重构器将这样的子样本$X'$映射为$G$的球$B'$，使得$B'$包括$X^+$并且与$X^-$不相交。对于任意半径$r$的球，我们设计了树的大小为$2$的适当标记样本压缩方案，环的大小为$3$，区间图的大小为$4$，环树的大小为$6$，以及自由立方体中位图的大小为$22$。

    arXiv:2206.13254v2 Announce Type: replace-cross  Abstract: One of the open problems in machine learning is whether any set-family of VC-dimension $d$ admits a sample compression scheme of size $O(d)$. In this paper, we study this problem for balls in graphs. For a ball $B=B_r(x)$ of a graph $G=(V,E)$, a realizable sample for $B$ is a signed subset $X=(X^+,X^-)$ of $V$ such that $B$ contains $X^+$ and is disjoint from $X^-$. A proper sample compression scheme of size $k$ consists of a compressor and a reconstructor. The compressor maps any realizable sample $X$ to a subsample $X'$ of size at most $k$. The reconstructor maps each such subsample $X'$ to a ball $B'$ of $G$ such that $B'$ includes $X^+$ and is disjoint from $X^-$.   For balls of arbitrary radius $r$, we design proper labeled sample compression schemes of size $2$ for trees, of size $3$ for cycles, of size $4$ for interval graphs, of size $6$ for trees of cycles, and of size $22$ for cube-free median graphs. For balls of a g
    
[^326]: 一个基于cGAN集成的关注不确定性的离线模型优化工业控制问题的替代模型

    A cGAN Ensemble-based Uncertainty-aware Surrogate Model for Offline Model-based Optimization in Industrial Control Problems

    [https://arxiv.org/abs/2205.07250](https://arxiv.org/abs/2205.07250)

    引入了一个基于cGAN集成的关注不确定性的替代模型，用于可靠处理工业控制问题中的离线模型优化，实验结果表明其优于竞争基线模型。

    

    本研究关注将离线模型优化应用于真实世界工业控制问题时遇到的两个重要问题。第一个问题是如何创建一个可靠的概率模型，准确捕捉嘈杂工业数据中存在的动态特性。第二个问题是如何在不主动收集工业系统反馈的情况下可靠地优化控制参数。具体来说，我们引入了一个新颖的基于cGAN集成的关注不确定性的替代模型，用于可靠地处理工业控制问题中的离线模型优化。通过在两个代表性案例上进行的大量实验来证明所提方法的有效性，即离散控制案例和连续控制案例。这些实验结果表明，我们的方法在工业控制的离线模型优化领域中胜过了几个竞争基线模型。

    arXiv:2205.07250v2 Announce Type: replace-cross  Abstract: This study focuses on two important problems related to applying offline model-based optimization to real-world industrial control problems. The first problem is how to create a reliable probabilistic model that accurately captures the dynamics present in noisy industrial data. The second problem is how to reliably optimize control parameters without actively collecting feedback from industrial systems. Specifically, we introduce a novel cGAN ensemble-based uncertainty-aware surrogate model for reliable offline model-based optimization in industrial control problems. The effectiveness of the proposed method is demonstrated through extensive experiments conducted on two representative cases, namely a discrete control case and a continuous control case. The results of these experiments show that our method outperforms several competitive baselines in the field of offline model-based optimization for industrial control.
    
[^327]: 对于一类支持向量机在不同缺陷预测场景中的有效性研究

    On The Effectiveness of One-Class Support Vector Machine in Different Defect Prediction Scenarios

    [https://arxiv.org/abs/2202.12074](https://arxiv.org/abs/2202.12074)

    本文研究了一类支持向量机在不同缺陷预测场景中的有效性，并发现其在跨版本和跨项目的缺陷预测模型中表现出色。

    

    缺陷预测旨在识别在软件提供给最终用户之前可能引起故障的软件组件。迄今为止，这一任务被建模为一个双类别分类问题，然而其本质也允许将其构建为一个一类分类任务。先前的研究表明，一类支持向量机（OCSVM）在项目内缺陷预测方面可以胜过双类别分类器，然而当应用于更细粒度的情况（即基于提交级别的缺陷预测）时并不有效。在本文中，我们进一步研究仅从一类进行学习是否足以生成在两种其他不同场景（即粒度）中产生有效缺陷预测模型，即跨版本和跨项目缺陷预测模型，并为了完整性复制在项目内粒度的先前工作。我们的实证结果证实了OCSVM的表现

    arXiv:2202.12074v2 Announce Type: replace-cross  Abstract: Defect prediction aims at identifying software components that are likely to cause faults before a software is made available to the end-user. To date, this task has been modeled as a two-class classification problem, however its nature also allows it to be formulated as a one-class classification task. Previous studies show that One-Class Support Vector Machine (OCSVM) can outperform two-class classifiers for within-project defect prediction, however it is not effective when employed at a finer granularity (i.e., commit-level defect prediction). In this paper, we further investigate whether learning from one class only is sufficient to produce effective defect prediction model in two other different scenarios (i.e., granularity), namely cross-version and cross-project defect prediction models, as well as replicate the previous work at within-project granularity for completeness. Our empirical results confirm that OCSVM perform
    
[^328]: Meta-Reinforcement Learning中梯度偏差的理论理解

    A Theoretical Understanding of Gradient Bias in Meta-Reinforcement Learning

    [https://arxiv.org/abs/2112.15400](https://arxiv.org/abs/2112.15400)

    本文对梯度为基础的元强化学习中的梯度偏差进行了深入理论理解，提出了统一框架描述GMRL算法的变化，并指出现有的随机元梯度估计器实际上是有偏的。

    

    梯度为基础的元强化学习（GMRL）是指保持两级优化程序的方法，其中外层元学习者指导内层梯度为基础的强化学习者实现快速适应。在本文中，我们开发了一个统一框架，描述了GMRL算法的变化，并指出GMRL采用的现有随机元梯度估计器实际上是有偏的。这种元梯度偏差来自两个方面：1）由两级问题结构引起的合成偏差，对内部更新步骤$K$、学习率$\alpha$、估计方差$\hat{\sigma}^{2}_{\text{In}}$和样本大小$|\tau|$有一个上限为$\mathcal{O}(K\alpha^{K}\hat{\sigma}_{\text{In}}|\tau|^{-0.5}$；2）由于使用自动微分而导致的多步Hessian估计偏差$\hat{\Delta}_{H}$，其具有多项式影响$\mathcal{O}((K-1)(\hat{\Delta}_...

    arXiv:2112.15400v4 Announce Type: replace-cross  Abstract: Gradient-based Meta-RL (GMRL) refers to methods that maintain two-level optimisation procedures wherein the outer-loop meta-learner guides the inner-loop gradient-based reinforcement learner to achieve fast adaptations. In this paper, we develop a unified framework that describes variations of GMRL algorithms and points out that existing stochastic meta-gradient estimators adopted by GMRL are actually \textbf{biased}. Such meta-gradient bias comes from two sources: 1) the compositional bias incurred by the two-level problem structure, which has an upper bound of $\mathcal{O}\big(K\alpha^{K}\hat{\sigma}_{\text{In}}|\tau|^{-0.5}\big)$ \emph{w.r.t.} inner-loop update step $K$, learning rate $\alpha$, estimate variance $\hat{\sigma}^{2}_{\text{In}}$ and sample size $|\tau|$, and 2) the multi-step Hessian estimation bias $\hat{\Delta}_{H}$ due to the use of autodiff, which has a polynomial impact $\mathcal{O}\big((K-1)(\hat{\Delta}_
    
[^329]: 高维度的在线动作学习：一个保守的观点

    Online Action Learning in High Dimensions: A Conservative Perspective

    [https://arxiv.org/abs/2009.13961](https://arxiv.org/abs/2009.13961)

    该论文将$\epsilon_t$-贪心启发式方法扩展到高维度情境中，采用保守导向策略，实现在实用应用中对新奇性的重视，同时限制了采纳不寻常动作，有效控制了累积遗憾。

    

    顺序学习问题在多个研究领域和实际应用中很常见。在本文中，我们将最流行的学习解决方案之一，$\epsilon_t$-贪心启发式，扩展到考虑保守导向的高维情境中。我们通过将原始规则用于采纳全新动作的时间的一部分，分配给在一组有前途的动作中进行更加专注的搜索来实现这一点。所得规则可能对仍然重视惊喜但限制采纳不寻常动作的实际应用有用。我们发现了对于保守高维度衰减$\epsilon_t$-贪心规则的累积遗憾提供了合理边界的概率很高。

    arXiv:2009.13961v4 Announce Type: replace-cross  Abstract: Sequential learning problems are common in several fields of research and practical applications. Examples include dynamic pricing and assortment, design of auctions and incentives and permeate a large number of sequential treatment experiments. In this paper, we extend one of the most popular learning solutions, the $\epsilon_t$-greedy heuristics, to high-dimensional contexts considering a conservative directive. We do this by allocating part of the time the original rule uses to adopt completely new actions to a more focused search in a restrictive set of promising actions. The resulting rule might be useful for practical applications that still values surprises, although at a decreasing rate, while also has restrictions on the adoption of unusual actions. With high probability, we find reasonable bounds for the cumulative regret of a conservative high-dimensional decaying $\epsilon_t$-greedy rule. Also, we provide a lower bo
    
[^330]: 用计数符号的一阶逻辑定义的概念的学习

    Learning Concepts Definable in First-Order Logic with Counting

    [https://arxiv.org/abs/1909.03820](https://arxiv.org/abs/1909.03820)

    该研究将一阶逻辑与计数符号相结合，证明了可以在多对数度结构下以次线性时间一致学习可定义的分类器，为包含数值方面的机器学习扩展学习框架迈出了第一步。

    

    我们研究了在Grohe和Tur\'an引入的逻辑框架下的关系背景结构上的布尔分类问题。众所周知(Grohe和Ritzert, LICS 2017)，在多对数度结构上的一阶逻辑可定义的分类器可以在次线性时间内学习，其中结构的度和运行时间是以结构的大小为单位来衡量的。我们将结果推广到了由Kuske和Schweikardt(LICS 2017)引入的带计数的一阶逻辑FOCN，它作为一个广泛推广各种计数逻辑的表现逻辑。具体来说，我们证明了可以在多对数度结构类上定义的FOCN中的分类器可以在次线性时间内一致地学习。这可以看作是将学习框架扩展以包含机器学习的数值方面的第一步。我们将这一结果扩展到了无视的概率

    arXiv:1909.03820v2 Announce Type: replace-cross  Abstract: We study Boolean classification problems over relational background structures in the logical framework introduced by Grohe and Tur\'an (TOCS 2004). It is known (Grohe and Ritzert, LICS 2017) that classifiers definable in first-order logic over structures of polylogarithmic degree can be learned in sublinear time, where the degree of the structure and the running time are measured in terms of the size of the structure. We generalise the results to the first-order logic with counting FOCN, which was introduced by Kuske and Schweikardt (LICS 2017) as an expressive logic generalising various other counting logics. Specifically, we prove that classifiers definable in FOCN over classes of structures of polylogarithmic degree can be consistently learned in sublinear time. This can be seen as a first step towards extending the learning framework to include numerical aspects of machine learning. We extend the result to agnostic probabl
    
[^331]: 多视角深度子空间聚类网络

    Multi-view Deep Subspace Clustering Networks

    [https://arxiv.org/abs/1908.01978](https://arxiv.org/abs/1908.01978)

    提出了一种名为多视角深度子空间聚类网络（MvDSCN）的方法，通过端到端学习多视角自表示矩阵，解决了现有方法中多视角关系未嵌入特征学习，以及深度学习端到端学习不适用于多视角聚类的问题。

    

    多视角子空间聚类旨在通过融合多个互补信息视角来发现数据的内在结构。现有大多数方法首先提取多种手工设计特征，然后学习用于聚类的联合相似矩阵。该方法的缺点在于两个方面：1）多视角关系未嵌入特征学习中，2）深度学习的端到端学习方式不适用于多视角聚类。即使提取了深度特征，也很难在不同数据集上选择适当的骨干进行聚类。为了解决这些问题，我们提出了多视角深度子空间聚类网络（MvDSCN），它以端到端的方式学习多视角自表示矩阵。MvDSCN由两个子网络组成，即多样性网络（Dnet）和普适性网络（Unet）。使用深度卷积构建潜在空间。

    arXiv:1908.01978v2 Announce Type: replace-cross  Abstract: Multi-view subspace clustering aims to discover the inherent structure of data by fusing multiple views of complementary information. Most existing methods first extract multiple types of handcrafted features and then learn a joint affinity matrix for clustering. The disadvantage of this approach lies in two aspects: 1) multi-view relations are not embedded into feature learning, and 2) the end-to-end learning manner of deep learning is not suitable for multi-view clustering. Even when deep features have been extracted, it is a nontrivial problem to choose a proper backbone for clustering on different datasets. To address these issues, we propose the Multi-view Deep Subspace Clustering Networks (MvDSCN), which learns a multi-view self-representation matrix in an end-to-end manner. The MvDSCN consists of two sub-networks, \ie, a diversity network (Dnet) and a universality network (Unet). A latent space is built using deep convol
    
[^332]: 基于深度学习的球解码算法

    Deep Learning Based Sphere Decoding

    [https://arxiv.org/abs/1807.03162](https://arxiv.org/abs/1807.03162)

    提出了一种基于深度学习的球解码算法，通过智能学习解码超球体的半径，实现了性能接近最优解码的效果，并显著降低了计算复杂度。

    

    本文提出了一种基于深度学习（DL）的球解码算法，其中解码超球体的半径由深度神经网络（DNN）学习。所提出的算法在广泛范围的信噪比（SNR）下实现的性能非常接近于最优最大似然解码（MLD），与现有的球解码变体相比，计算复杂度显著降低。这种改进归因于DNN智能学习解码中使用的超球体的半径。所提出的基于DL的算法的预期复杂度被通过分析推导，并与现有方法进行了比较。结果表明，在DL算法中，解码超球体内的晶格点的数量在平均情况和最坏情况下均大大减少。通过高维模拟展示了所提出算法的有效性。

    arXiv:1807.03162v2 Announce Type: replace-cross  Abstract: In this paper, a deep learning (DL)-based sphere decoding algorithm is proposed, where the radius of the decoding hypersphere is learned by a deep neural network (DNN). The performance achieved by the proposed algorithm is very close to the optimal maximum likelihood decoding (MLD) over a wide range of signal-to-noise ratios (SNRs), while the computational complexity, compared to existing sphere decoding variants, is significantly reduced. This improvement is attributed to DNN's ability of intelligently learning the radius of the hypersphere used in decoding. The expected complexity of the proposed DL-based algorithm is analytically derived and compared with existing ones. It is shown that the number of lattice points inside the decoding hypersphere drastically reduces in the DL-based algorithm in both the average and worst-case senses. The effectiveness of the proposed algorithm is shown through simulation for high-dimensional
    
[^333]: Thompson采样用于具有噪音上下文的随机赌臂问题的信息论性后悔分析

    Thompson Sampling for Stochastic Bandits with Noisy Contexts: An Information-Theoretic Regret Analysis. (arXiv:2401.11565v1 [cs.LG])

    [http://arxiv.org/abs/2401.11565](http://arxiv.org/abs/2401.11565)

    本文研究了具有噪音上下文的随机赌臂问题，并提出了一种Thompson采样算法，通过贝叶斯框架进行分析，证明了算法的贝叶斯后悔，并扩展了问题到延迟观察真实上下文的情况，并实证了算法的性能。

    

    我们研究了一种随机上下文线性赌臂问题，其中代理通过一个未知噪声参数的噪声信道观察到真实上下文的噪声，我们的目标是设计一个动作策略，可以近似于具有奖励模型、噪声参数和从观察到的噪声上下文中真实上下文的预测分布的oracle的动作策略。在贝叶斯框架下，我们引入了一种针对具有高斯上下文噪声的高斯赌臂的Thompson采样算法。采用信息论分析，我们证明了我们的算法相对于oracle的动作策略的贝叶斯后悔。我们还将这个问题扩展到了代理在接收到奖励后延迟观察到真实上下文的情况，并展示了延迟真实上下文导致更低的贝叶斯后悔。最后，我们通过与基线算法的比较实证地展示了所提出算法的性能。

    We explore a stochastic contextual linear bandit problem where the agent observes a noisy, corrupted version of the true context through a noise channel with an unknown noise parameter. Our objective is to design an action policy that can approximate" that of an oracle, which has access to the reward model, the channel parameter, and the predictive distribution of the true context from the observed noisy context. In a Bayesian framework, we introduce a Thompson sampling algorithm for Gaussian bandits with Gaussian context noise. Adopting an information-theoretic analysis, we demonstrate the Bayesian regret of our algorithm concerning the oracle's action policy. We also extend this problem to a scenario where the agent observes the true context with some delay after receiving the reward and show that delayed true contexts lead to lower Bayesian regret. Finally, we empirically demonstrate the performance of the proposed algorithms against baselines.
    
[^334]: 视觉和语言编码器是否以相似方式表示世界？

    Do Vision and Language Encoders Represent the World Similarly?. (arXiv:2401.05224v1 [cs.CV])

    [http://arxiv.org/abs/2401.05224](http://arxiv.org/abs/2401.05224)

    通过分析视觉和语言模型的潜在空间结构，发现未对齐和对齐的编码器的表示空间在语义上是相似的。我们提出了两种方法来匹配未对齐编码器，无需训练即可实现匹配。

    

    已经成为视觉语言任务中事实上的模型的对齐的文本-图像编码器（如CLIP）已经取得了令人印象深刻的表现。此外，模态特定的编码器在各自领域中也取得了令人印象深刻的表现。这引出了一个核心问题：由于它们基本上表示同一个物理世界，单模态的视觉和语言编码器之间是否存在对齐？通过使用中心核对齐（CKA）分析图像-标题基准上视觉和语言模型的潜在空间结构，我们发现未对齐和对齐的编码器的表示空间在语义上是相似的。在像CLIP这样的对齐编码器中缺乏统计相似性的情况下，我们显示了可能存在无需任何训练的未对齐编码器的匹配。我们将这视为利用图之间的语义相似性的有种子图匹配问题，并提出了两种方法 - 快速二次分配问题优化和一种基于新颖的局部CKA度量的匹配/检索方法。

    Aligned text-image encoders such as CLIP have become the de facto model for vision-language tasks. Furthermore, modality-specific encoders achieve impressive performances in their respective domains. This raises a central question: does an alignment exist between uni-modal vision and language encoders since they fundamentally represent the same physical world? Analyzing the latent spaces structure of vision and language models on image-caption benchmarks using the Centered Kernel Alignment (CKA), we find that the representation spaces of unaligned and aligned encoders are semantically similar. In the absence of statistical similarity in aligned encoders like CLIP, we show that a possible matching of unaligned encoders exists without any training. We frame this as a seeded graph-matching problem exploiting the semantic similarity between graphs and propose two methods - a Fast Quadratic Assignment Problem optimization, and a novel localized CKA metric-based matching/retrieval. We demons
    
[^335]: 偏好作为奖励，使用重要性抽样进行最大偏好优化

    Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16430](http://arxiv.org/abs/2312.16430)

    本文提出了一种使用重要性抽样进行最大偏好优化的算法，该算法通过直接优化生成策略来消除对奖励模型的需求，提高了数据利用率和稳定性，并通过解决KL正则化问题来改善偏好学习效果。

    

    偏好学习是将语言模型与人类价值观对齐的关键技术。从人类反馈强化学习（RLHF）是一种基于模型的算法，用于优化偏好学习，首先拟合偏好分数的奖励模型，然后使用基于策略梯度算法进行优化，以最大化奖励。RLHF的处理过程复杂、耗时且不稳定。直接偏好优化（DPO）算法使用离策略算法直接优化生成策略，消除了对奖励模型的需求，具有高效和稳定的数据利用率。DPO使用布拉德利-特里模型和对数损失，导致在偏好接近确定性时忽略了KL正则化项而过度拟合偏好数据。IPO使用一种基于根查找的成对均方误差损失来解决忽略KL正则化问题，并学习到最优策略。但是IPO的成对损失仍然无法使KL正则化生效。本文设计了一种新的算法，使用重要性抽样技术来解决偏好学习中的优化问题。

    Preference learning is a key technology for aligning language models with human values. Reinforcement Learning from Human Feedback (RLHF) is a model based algorithm to optimize preference learning, which first fitting a reward model for preference score, and then optimizing generating policy with on-policy PPO algorithm to maximize the reward. The processing of RLHF is complex, time-consuming and unstable. Direct Preference Optimization (DPO) algorithm using off-policy algorithm to direct optimize generating policy and eliminating the need for reward model, which is data efficient and stable. DPO use Bradley-Terry model and log-loss which leads to over-fitting to the preference data at the expense of ignoring KL-regularization term when preference near deterministic. IPO uses a root-finding pairwise MSE loss to solve the ignoring KL-regularization problem, and learning an optimal policy. But IPO's pairwise loss still can't s make the KL-regularization to work. In this paper, we design 
    
[^336]: VQPy：一种面向现代视频分析的面向对象方法。

    VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])

    [http://arxiv.org/abs/2311.01623](http://arxiv.org/abs/2311.01623)

    VQPy是一种面向对象的视频分析方法，它使用Python变体作为前端，并具有可扩展的后端，可以自动构建和优化基于视频对象的处理流程。

    

    视频分析广泛应用于当今系统和服务中。在视频分析的前沿是用户开发的视频查询，以找到特定感兴趣的对象。基于视频对象（例如人，动物，汽车等）与传统面向对象语言建模的对象相似的洞察力，我们提出了一种面向视频分析的面向对象方法。这种方法名为VQPy，包括一个前端（一种Python变体，其中包含用户可以表达视频对象及其交互的结构）和一个可扩展的后端，可以基于视频对象自动生成和优化管道。我们已经实施和开源了VQPy，它已经作为Cisco DeepVision框架的一部分产品化。

    Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
    
[^337]: 最佳结合: 随机和对抗性凸函数追踪

    Best of Both Worlds: Stochastic and Adversarial Convex Function Chasing. (arXiv:2311.00181v1 [math.OC])

    [http://arxiv.org/abs/2311.00181](http://arxiv.org/abs/2311.00181)

    该论文研究了随机和对抗性环境下的凸函数追踪问题，并给出了同时在两种情境下达到性能保证的算法。这是首个使用随机框架研究该问题的工作，提出了一种融合两种情境的最佳算法。

    

    凸函数追踪(CFC)是一个在线优化问题，每一轮$t$，玩家根据损失函数$f_t(x_t)$和切换动作的额外成本$c(x_t,x_{t-1})$选择动作$x_t$。我们研究了随机和对抗性环境下的CFC问题，并给出了在两种情境下同时获得性能保证的算法。具体而言，我们考虑了平方$\ell_2$范数的切换成本和一类广泛的二次损失函数，对于这类损失函数，极小化序列要么形成鞅，要么由对手选择。这是首个使用随机框架研究CFC问题的工作。我们给出了最佳随机在线算法的特征，并通过对随机和对抗性情景的比较，证明了在随机情境下，对抗性最优算法表现不佳。受此启发，我们提出了一种融合了两种情境的最佳算法。

    Convex function chasing (CFC) is an online optimization problem in which during each round $t$, a player plays an action $x_t$ in response to a hitting cost $f_t(x_t)$ and an additional cost of $c(x_t,x_{t-1})$ for switching actions. We study the CFC problem in stochastic and adversarial environments, giving algorithms that achieve performance guarantees simultaneously in both settings. Specifically, we consider the squared $\ell_2$-norm switching costs and a broad class of quadratic hitting costs for which the sequence of minimizers either forms a martingale or is chosen adversarially. This is the first work that studies the CFC problem using a stochastic framework. We provide a characterization of the optimal stochastic online algorithm and, drawing a comparison between the stochastic and adversarial scenarios, we demonstrate that the adversarial-optimal algorithm exhibits suboptimal performance in the stochastic context. Motivated by this, we provide a best-of-both-worlds algorithm 
    
[^338]: 这里是翻译过的论文标题

    Ghost on the Shell: An Expressive Representation of General 3D Shapes. (arXiv:2310.15168v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.15168](http://arxiv.org/abs/2310.15168)

    这里是中文总结出的一句话要点

    

    这里是翻译过的论文摘要

    The creation of photorealistic virtual worlds requires the accurate modeling of 3D surface geometry for a wide range of objects. For this, meshes are appealing since they 1) enable fast physics-based rendering with realistic material and lighting, 2) support physical simulation, and 3) are memory-efficient for modern graphics pipelines. Recent work on reconstructing and statistically modeling 3D shape, however, has critiqued meshes as being topologically inflexible. To capture a wide range of object shapes, any 3D representation must be able to model solid, watertight, shapes as well as thin, open, surfaces. Recent work has focused on the former, and methods for reconstructing open surfaces do not support fast reconstruction with material and lighting or unconditional generative modelling. Inspired by the observation that open surfaces can be seen as islands floating on watertight surfaces, we parameterize open surfaces by defining a manifold signed distance field on watertight templat
    
[^339]: 一种用于时间序列分析的多尺度分解MLP-Mixer

    A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis. (arXiv:2310.11959v1 [cs.LG])

    [http://arxiv.org/abs/2310.11959](http://arxiv.org/abs/2310.11959)

    我们提出了一种名为MSD-Mixer的多尺度分解MLP-Mixer模型，它能够将时间序列分解成不同的组成部分，并在不同的层次中表示这些组成部分。我们还提出了一种新颖的时间拼接方法，以处理多尺度的时间模式和通道间的依赖关系。我们的模型能够比现有方法更好地进行时间序列分析。

    

    时间序列数据通常具有独特的组成和复杂的多尺度时间变化，需要在其分析中特别考虑分解和多尺度建模。现有的深度学习方法只适用于单变量时间序列，并且对子序列级别的建模和分解不够充分。为了解决这个问题，我们提出了MSD-Mixer，一种多尺度分解的MLP-Mixer，它学会了将输入的时间序列明确地分解成不同的组成部分，并在不同的层次中表示这些组成部分。为了处理多尺度的时间模式和通道间的依赖关系，我们提出了一种新颖的时间拼接方法，将时间序列建模为多尺度子序列，即patches，并使用MLPs来组合patches内部和patches间的变化以及通道间的相关性。此外，我们提出了一个损失函数来约束分解残差的幅度和自相关性，以实现完整的分解。

    Time series data, often characterized by unique composition and complex multi-scale temporal variations, requires special consideration of decomposition and multi-scale modeling in its analysis. Existing deep learning methods on this best fit to only univariate time series, and have not sufficiently accounted for sub-series level modeling and decomposition completeness. To address this, we propose MSD-Mixer, a Multi-Scale Decomposition MLP-Mixer which learns to explicitly decompose the input time series into different components, and represents the components in different layers. To handle multi-scale temporal patterns and inter-channel dependencies, we propose a novel temporal patching approach to model the time series as multi-scale sub-series, i.e., patches, and employ MLPs to mix intra- and inter-patch variations and channel-wise correlations. In addition, we propose a loss function to constrain both the magnitude and autocorrelation of the decomposition residual for decomposition 
    
[^340]: SOTOPIA: 交互式评估语言智能中的社交智能

    SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents. (arXiv:2310.11667v1 [cs.AI])

    [http://arxiv.org/abs/2310.11667](http://arxiv.org/abs/2310.11667)

    SOTOPIA是一个用于评估语言智能中的社交智能的交互式环境。通过模拟复杂的社交互动，并使用全面的评估框架，我们发现不同模型之间的社交智能存在显著差异，特别是在SOTOPIA-hard情景下。GPT-4在这个子集上的目标完成率较低。

    

    人类是社交的存在；我们在日常互动中追求社交目标，这是社交智能的关键方面。然而，人工智能系统在这个领域的能力仍然难以捉摸。我们提出了SOTOPIA，一个开放式环境，用于模拟人工智能代理之间的复杂社交互动并评估它们的社交智能。在我们的环境中，代理人扮演角色，在各种场景下相互协作、合作、交流和竞争，以实现复杂的社交目标。我们模拟了LLM-based代理人与人类之间在这个任务空间内的角色扮演互动，并使用一个名为SOTOPIA-Eval的整体评估框架对它们的表现进行评估。通过SOTOPIA，我们发现这些模型在社交智能方面存在显著差异，并确定了SOTOPIA的一个子集，即SOTOPIA-hard，对所有模型来说都具有挑战性。我们发现在这个子集上，GPT-4的目标完成率显著较低。

    Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completio
    
[^341]: 探索面向人脸变形的扩散自编码器的设计空间

    Exploring the Design Space of Diffusion Autoencoders for Face Morphing. (arXiv:2310.09484v1 [cs.CV])

    [http://arxiv.org/abs/2310.09484](http://arxiv.org/abs/2310.09484)

    这项研究探索了面向人脸变形的扩散自编码器的设计空间，研究了采样算法、逆向DDIM求解器和部分采样的方法。

    

    通过扩散自编码器创建的人脸变形是一种最近的创新，而这种方法的设计空间尚未得到充分探索。我们探索了设计空间的三个方面，即1）采样算法，2）逆向DDIM求解器，以及3）通过添加少量噪声进行部分采样。

    Face morphs created by Diffusion Autoencoders are a recent innovation and the design space of such an approach has not been well explored. We explore three axes of the design space, i.e., 1) sampling algorithms, 2) the reverse DDIM solver, and 3) partial sampling through small amounts of added noise.
    
[^342]: PhyloGFN: 基于生成流网络的系统发育推断

    PhyloGFN: Phylogenetic inference with generative flow networks. (arXiv:2310.08774v1 [q-bio.PE])

    [http://arxiv.org/abs/2310.08774](http://arxiv.org/abs/2310.08774)

    PhyloGFN是一种基于生成流网络的系统发育推断方法，通过采样复杂的组合结构，能够产生多样且高质量的进化假设，并在边缘似然估计方面具有竞争力。

    

    系统发育学是计算生物学的一个分支，研究生物实体之间的进化关系。尽管有着悠久的历史和众多应用，但从序列数据推断系统发育树仍然具有挑战性：树空间的高复杂性对当前的组合和概率技术构成了重要障碍。在本文中，我们采用生成流网络（GFlowNets）的框架来解决系统发育学中的两个核心问题：基于最简原则的和贝叶斯的系统发育推断。由于GFlowNets适用于采样复杂的组合结构，它们是探索和采样树拓扑和进化距离的多模态后验分布的自然选择。我们证明了我们的摊还后验采样器PhyloGFN在真实基准数据集上产生多样且高质量的进化假设。PhyloGFN在边缘似然估计方面与之前的工作相比具有竞争力。

    Phylogenetics is a branch of computational biology that studies the evolutionary relationships among biological entities. Its long history and numerous applications notwithstanding, inference of phylogenetic trees from sequence data remains challenging: the high complexity of tree space poses a significant obstacle for the current combinatorial and probabilistic techniques. In this paper, we adopt the framework of generative flow networks (GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and Bayesian phylogenetic inference. Because GFlowNets are well-suited for sampling complex combinatorial structures, they are a natural choice for exploring and sampling from the multimodal posterior distribution over tree topologies and evolutionary distances. We demonstrate that our amortized posterior sampler, PhyloGFN, produces diverse and high-quality evolutionary hypotheses on real benchmark datasets. PhyloGFN is competitive with prior works in marginal likelihood estimat
    
[^343]: 分子设计的核-弹性自编码器

    Kernel-Elastic Autoencoder for Molecular Design. (arXiv:2310.08685v1 [cs.LG])

    [http://arxiv.org/abs/2310.08685](http://arxiv.org/abs/2310.08685)

    核-弹性自编码器（KAE）是一种在分子设计领域具有增强性能的自监督生成模型。KAE实现了有效生成和准确重构的挑战，具有显著的多样性和最先进的性能。此外，KAE还可以根据特定条件生成分子，并在分子对接应用中表现出优秀的性能。

    

    我们引入了核-弹性自编码器（KAE），这是一种基于变压器架构的自监督生成模型，具有增强的分子设计性能。KAE基于两种新的损失函数进行建模：修改的最大均匀位移和加权重构。KAE解决了同时实现有效生成和准确重构的长期挑战。KAE在分子生成中实现了显著的多样性，并在独立测试数据集上保持了近乎完美的重构，超越了先前的分子生成模型。KAE实现了条件生成，并允许基于波束搜索进行解码，从而在受限优化中实现了最先进的性能。此外，KAE可以根据分子对接应用中的有利结合亲和力进行条件生成，并通过AutoDock Vina和Glide得分进行确认，胜过训练数据集中的所有现有候选分子。除了分子设计，我们预期KAE可以应用于其他领域。

    We introduce the Kernel-Elastic Autoencoder (KAE), a self-supervised generative model based on the transformer architecture with enhanced performance for molecular design. KAE is formulated based on two novel loss functions: modified maximum mean discrepancy and weighted reconstruction. KAE addresses the long-standing challenge of achieving valid generation and accurate reconstruction at the same time. KAE achieves remarkable diversity in molecule generation while maintaining near-perfect reconstructions on the independent testing dataset, surpassing previous molecule-generating models. KAE enables conditional generation and allows for decoding based on beam search resulting in state-of-the-art performance in constrained optimizations. Furthermore, KAE can generate molecules conditional to favorable binding affinities in docking applications as confirmed by AutoDock Vina and Glide scores, outperforming all existing candidates from the training dataset. Beyond molecular design, we antic
    
[^344]: 数据驱动建模自相似动力学

    Data driven modeling of self-similar dynamics. (arXiv:2310.08282v1 [cs.LG])

    [http://arxiv.org/abs/2310.08282](http://arxiv.org/abs/2310.08282)

    本文介绍了一个多尺度神经网络框架，利用自相似性作为先验知识，对自相似动力系统进行建模。对于确定性动力学，框架可以判断动力学是否自相似；对于不确定性动力学，它可以确定哪个参数集更接近自相似性。方法可以提取与尺度无关的核进行任意尺度的建模，并识别自相似系统中的幂律指数。初步测试表明，方法对Ising模型的临界指数具有理论一致性。

    

    多尺度建模复杂系统对于理解其复杂性至关重要。数据驱动多尺度建模已成为解决复杂系统挑战的一种有希望的方法。另一方面，自相似性在复杂系统中普遍存在，这表明大规模复杂系统可以以较低成本进行建模。本文引入了一个多尺度神经网络框架，将自相似性作为先验知识，并实现了对自相似动力系统的建模。对于确定性动力学，我们的框架可以判断动力学是否自相似。对于不确定性动力学，它可以比较和确定哪个参数集更接近自相似性。该框架允许我们从动力学中提取与尺度无关的核进行任意尺度的建模。此外，我们的方法可以识别自相似系统中的幂律指数。对Ising模型的初步测试产生了与理论一致的临界指数。

    Multiscale modeling of complex systems is crucial for understanding their intricacies. Data-driven multiscale modeling has emerged as a promising approach to tackle challenges associated with complex systems. On the other hand, self-similarity is prevalent in complex systems, hinting that large-scale complex systems can be modeled at a reduced cost. In this paper, we introduce a multiscale neural network framework that incorporates self-similarity as prior knowledge, facilitating the modeling of self-similar dynamical systems. For deterministic dynamics, our framework can discern whether the dynamics are self-similar. For uncertain dynamics, it can compare and determine which parameter set is closer to self-similarity. The framework allows us to extract scale-invariant kernels from the dynamics for modeling at any scale. Moreover, our method can identify the power law exponents in self-similar systems. Preliminary tests on the Ising model yielded critical exponents consistent with theo
    
[^345]: CacheGen：用于语言模型应用的快速上下文加载

    CacheGen: Fast Context Loading for Language Model Applications. (arXiv:2310.07240v1 [cs.NI])

    [http://arxiv.org/abs/2310.07240](http://arxiv.org/abs/2310.07240)

    CacheGen是一种用于语言模型应用的技术，通过对上下文进行压缩来减少LLM的网络获取和处理延迟。

    

    随着大型语言模型（LLM）承担越来越复杂的任务，其输入将整合更长的上下文，以应对需要领域知识或用户特定的对话历史的问题。然而，使用长上下文对于响应式的LLM系统来说是一个挑战，因为在所有上下文被获取和LLM处理之前，无法生成任何内容。现有系统仅通过优化上下文处理的计算延迟（例如，通过缓存文本上下文的中间键值特征）来解决问题，但往往会导致上下文获取的网络延迟更长（例如，键值特征消耗的带宽比文本上下文大几个数量级）。本文介绍了CacheGen，以最小化LLM上下文获取和处理的延迟。CacheGen通过将长上下文的键值（KV）特征压缩为更紧凑的比特流表示，减少了传输所需的带宽。编码器结合了自适应量化和......

    As large language models (LLMs) take on more complex tasks, their inputs incorporate longer contexts to respond to questions that require domain knowledge or user-specific conversational histories. Yet, using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until all the contexts are fetched to and processed by the LLM. Existing systems optimize only the computation delay in context processing (e.g., by caching intermediate key-value features of the text context) but often cause longer network delays in context fetching (e.g., key-value features consume orders of magnitude larger bandwidth than the text context).  This paper presents CacheGen to minimize the delays in fetching and processing contexts for LLMs. CacheGen reduces the bandwidth needed for transmitting long contexts' key-value (KV) features through a novel encoder that compresses KV features into more compact bitstream representations. The encoder combines adaptive quantization with a 
    
[^346]: Lemur：在自动程序验证中集成大型语言模型

    Lemur: Integrating Large Language Models in Automated Program Verification. (arXiv:2310.04870v2 [cs.FL] UPDATED)

    [http://arxiv.org/abs/2310.04870](http://arxiv.org/abs/2310.04870)

    本论文提出了一种将LLMs和自动推理器结合起来进行自动程序验证的通用方法，并证明了其完备性。这个方法在一些合成和竞争基准上取得了实际的改进。

    

    LLMs在代码理解能力上的展示引发了一个问题：它们是否可以用于自动程序验证，这是一个通常需要高级抽象推理的任务，对于验证工具来说是具有挑战性的。我们提出了一种将LLMs的能力和自动推理器结合起来进行自动程序验证的通用方法。我们正式描述了这种方法论，将其作为推导规则的集合进行论证其完备性。我们将计算机推理形成为一个完备的自动验证过程，这在一组合成和竞争基准上带来了实际的改进。

    The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that often demands high-level abstract reasoning about program properties, which is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.
    
[^347]: 本地搜索GFlowNets

    Local Search GFlowNets. (arXiv:2310.02710v1 [cs.LG])

    [http://arxiv.org/abs/2310.02710](http://arxiv.org/abs/2310.02710)

    本文提出使用局部搜索训练GFlowNets，通过破坏和重构的方式探索局部邻域，分别由反向和正向策略引导，使得样本偏向高奖励解决方案。

    

    生成流网络(GFlowNets)是一种学习与奖励成比例的离散对象分布的摊还采样方法。GFlowNets具有生成多样样本的显著能力，但由于广泛样本空间上的过度探索，有时难以一致地生成高奖励的样本。本文提出使用局部搜索训练GFlowNets，通过破坏和重构的方式探索局部邻域，分别由反向和正向策略引导。这使得样本偏向高奖励解决方案，而传统的GFlowNet解决方案生成方案则使用正向策略从头生成解决方案。大量实验证明在几个生化任务中取得了显著的性能改进。

    Generative Flow Networks (GFlowNets) are amortized sampling methods that learn a distribution over discrete objects proportional to their rewards. GFlowNets exhibit a remarkable ability to generate diverse samples, yet occasionally struggle to consistently produce samples with high rewards due to over-exploration on wide sample space. This paper proposes to train GFlowNets with local search which focuses on exploiting high rewarded sample space to resolve this issue. Our main idea is to explore the local neighborhood via destruction and reconstruction guided by backward and forward policies, respectively. This allows biasing the samples toward high-reward solutions, which is not possible for a typical GFlowNet solution generation scheme which uses the forward policy to generate the solution from scratch. Extensive experiments demonstrate a remarkable performance improvement in several biochemical tasks. Source code is available: \url{https://github.com/dbsxodud-11/ls_gfn}.
    
[^348]: 采用估计的注意力掩码的稀疏线性注意力（SEA）

    SEA: Sparse Linear Attention with Estimated Attention Mask. (arXiv:2310.01777v1 [cs.CL])

    [http://arxiv.org/abs/2310.01777](http://arxiv.org/abs/2310.01777)

    提出了SEA方法，可以通过估计注意力掩码实现线性复杂度的稀疏注意力，解决了transformer处理长序列时注意力操作复杂度高的问题，并保持了可解释性。

    

    近年来，transformer架构在需要对序列元素之间的成对关系建模的任务上取得了重大突破，如自然语言理解任务。然而，由于注意力操作的二次复杂度，transformer在处理长序列时存在困难，因此先前的研究旨在通过稀疏化或线性逼近注意力矩阵来降低复杂度。然而，这些方法无法直接从教师的注意力矩阵中提取知识，并且通常需要完全重新训练。此外，先前的稀疏和线性方法如果不能产生完全二次的注意力矩阵，还可能失去可解释性。为了解决这些挑战，我们提出了SEA：采用估计注意力掩码的稀疏线性注意力方法。SEA通过基于核的线性注意力方法估计注意力矩阵，并创建一个对完整注意力矩阵进行稀疏逼近的方法。

    The transformer architecture has made breakthroughs in recent years on tasks which require modeling pairwise relationships between sequential elements, as is the case in natural language understanding. However, transformers struggle with long sequences due to the quadratic complexity of the attention operation, and previous research has aimed to lower the complexity by sparsifying or linearly approximating the attention matrix. Yet, these approaches cannot straightforwardly distill knowledge from a teacher's attention matrix, and often require complete retraining from scratch. Furthermore, previous sparse and linear approaches may also lose interpretability if they do not produce full quadratic attention matrices. To address these challenges, we propose SEA: Sparse linear attention with an Estimated Attention mask. SEA estimates the attention matrix with linear complexity via kernel-based linear attention, then creates a sparse approximation to the full attention matrix with a top-k se
    
[^349]: TACTiS-2：更好、更快、更简单的多变量时间序列关注联合分布模型

    TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series. (arXiv:2310.01327v1 [cs.LG])

    [http://arxiv.org/abs/2310.01327](http://arxiv.org/abs/2310.01327)

    TACTiS-2是一种改进的多变量时间序列关注联合分布模型，采用了简化的目标函数和线性参数数量，具有更好的训练动态和最先进的性能。

    

    我们引入了一种新的模型用于多变量概率时间序列预测，旨在灵活地处理包括预测、插值和它们的组合等一系列任务。基于联合分布理论，我们提出了一种简化的目标函数，用于最近引入的基于Transformer的关注联合分布模型（TACTiS）。新的目标函数的分布参数数量与变量数量呈线性而非阶乘关系。新的目标函数需要引入一种训练课程，并且需要对原始架构进行必要的改动。我们展示了得到的模型具有显著改善的训练动态，并在多样的真实世界预测任务中实现了最先进的性能，同时保持了先前工作的灵活性，如无缝处理不对齐和采样不均匀的时间序列。

    We introduce a new model for multivariate probabilistic time series prediction, designed to flexibly address a range of tasks including forecasting, interpolation, and their combinations. Building on copula theory, we propose a simplified objective for the recently-introduced transformer-based attentional copulas (TACTiS), wherein the number of distributional parameters now scales linearly with the number of variables instead of factorially. The new objective requires the introduction of a training curriculum, which goes hand-in-hand with necessary changes to the original architecture. We show that the resulting model has significantly better training dynamics and achieves state-of-the-art performance across diverse real-world forecasting tasks, while maintaining the flexibility of prior work, such as seamless handling of unaligned and unevenly-sampled time series.
    
[^350]: 完美预测储备计算在自回归时间序列数据中的数学结构

    Mathematical structure of perfect predictive reservoir computing for autoregressive type of time series data. (arXiv:2310.00290v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00290](http://arxiv.org/abs/2310.00290)

    这篇论文研究了储备计算在自回归时间序列数据中的数学结构，并揭示了其隐藏的权重矩阵结构，以实现对AR类型时间序列数据的完美预测。

    

    储备计算（RC）是一种递归神经网络（RNN），毫无疑问，RC将越来越广泛地用于构建时间序列数据的未来预测模型，具有低训练成本、高速度和高计算能力。然而，对于RC神经网络的数学结构的研究直到最近才开始。Bollt（2021）阐明了自回归（AR）模型对于理解RC神经网络的数学结构的必要性，并指出Wold分解定理是理解这些结构的里程碑。在铭记这一著名结果的基础上，本文阐明了RC神经网络中输入和循环权重矩阵的隐藏结构，并展示了这些结构对于AR类型的时间序列数据实现了完美预测。

    Reservoir Computing (RC) is a type of recursive neural network (RNN), and there can be no doubt that the RC will be more and more widely used for building future prediction models for time-series data, with low training cost, high speed and high computational power. However, research into the mathematical structure of RC neural networks has only recently begun. Bollt (2021) clarified the necessity of the autoregressive (AR) model for gaining the insight into the mathematical structure of RC neural networks, and indicated that the Wold decomposition theorem is the milestone for understanding of these. Keeping this celebrated result in mind, in this paper, we clarify hidden structures of input and recurrent weight matrices in RC neural networks, and show that such structures attain perfect prediction for the AR type of time series data.
    
[^351]: 重新思考频道维度以隔离大型语言模型的低位权重量化中的异常值

    Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models. (arXiv:2309.15531v1 [cs.LG])

    [http://arxiv.org/abs/2309.15531](http://arxiv.org/abs/2309.15531)

    该论文提出了一种重新思考频道维度的方法，以隔离大型语言模型低位权重量化中的异常值。通过将权重按输入通道内进行量化分组，可以解决激活异常值的问题，并成功地使得低于4位的量化成为可能。

    

    大型语言模型（LLMs）在各种任务中近期展示了显著成功。然而，在有效地为LLMs提供服务方面一直是个挑战，这主要是由于其大内存瓶颈，特别是在小批量推理设置（如移动设备）中。仅对权重进行量化可能是一种有希望的方法，但是由于存在大幅度激活异常值，低于4位的量化仍然是一个挑战。为了减轻不可取的异常效果，我们首先提出了每个输入通道（IC）内进行量化分组的per-IC量化方法，这是一种简单但有效的方法，而不是传统的每个输出通道（OC）内进行量化分组。我们的方法的动机是观察到激活异常值影响权重矩阵的输入维度，因此在IC方向上对权重进行类似分组可以将异常值隔离到一个分组内。我们还发现激活的异常值并不决定量化的难度，其固有的权重敏感性也存在。通过per-IC量化作为方法，我们的方法成功地解决了大型语言模型低位权重量化中的异常值问题。

    Large Language Models (LLMs) have recently demonstrated a remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to its large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output channel (OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can isolate outliers to be within a group. We also find that activation outliers do not dictate quantization difficulty, and inherent weight sensitivities also exist. With per-IC quantization as
    
[^352]: 通过测度传递进行密度估计：生物科学中的应用展望

    Density Estimation via Measure Transport: Outlook for Applications in the Biological Sciences. (arXiv:2309.15366v1 [q-bio.QM])

    [http://arxiv.org/abs/2309.15366](http://arxiv.org/abs/2309.15366)

    通过测度传递方法进行密度估计在生物科学中具有广阔的应用前景，尤其是在处理稀疏数据的情况下，使用稀疏传递映射可以揭示数据中隐藏的信息。

    

    测度传递方法的一个优势是其允许对根据广泛概率测度分布的数据进行统一的处理和分析。在这个框架下，我们通过计算研究的结果来评估测度传递技术的潜力，特别是三角传递映射的使用，作为支持生物科学研究的工作流的一部分。稀疏数据场景在辐射生物学等领域很常见。我们发现，在数据稀缺时，稀疏传递映射是有优势的。具体而言，通过计算一系列（稀疏的）自适应传递映射的统计信息，这些映射是在随机选择的一系列可用数据样本子集上进行训练的，可以揭示数据中隐藏的信息。因此，在本文考虑的辐射生物学应用中，此方法为生成假设提供了一个工具。

    One among several advantages of measure transport methods is that they allow for a unified framework for processing and analysis of data distributed according to a wide class of probability measures. Within this context, we present results from computational studies aimed at assessing the potential of measure transport techniques, specifically, the use of triangular transport maps, as part of a workflow intended to support research in the biological sciences. Scarce data scenarios, which are common in domains such as radiation biology, are of particular interest. We find that when data is scarce, sparse transport maps are advantageous. In particular, statistics gathered from computing series of (sparse) adaptive transport maps, trained on a series of randomly chosen subsets of the set of available data samples, leads to uncovering information hidden in the data. As a result, in the radiation biology application considered here, this approach provides a tool for generating hypotheses ab
    
[^353]: 稳定放置的外部接触块的触觉估计

    Tactile Estimation of Extrinsic Contact Patch for Stable Placement. (arXiv:2309.14552v1 [cs.RO])

    [http://arxiv.org/abs/2309.14552](http://arxiv.org/abs/2309.14552)

    本文介绍了一种利用触觉读数推测物体放置稳定性的方法，通过对接触区域的估计可以有效设计机器人的反馈技能，提高机器人的精细操控能力。

    

    对于机器人的精细操作技能来说，准确感知接触交互至关重要。本文提出了一种为机器人设计反馈技能的方法，该机器人必须学习将复杂形状的物体堆叠在一起。为了设计这样一个系统，机器人应该能够根据非常轻微的接触交互来推理放置的稳定性。我们的实验结果表明，可以根据接触形成过程中的触觉读数来推测物体放置的稳定性。具体而言，我们使用力和触觉观测来估计抓取物体和其环境之间的接触区域，从而估计接触形成过程中物体的稳定性。这种接触区域可以用来估计释放抓取后物体的稳定性。所提出的方法在一款非常流行的棋盘游戏中使用了多种物体对进行了验证。

    Precise perception of contact interactions is essential for the fine-grained manipulation skills for robots. In this paper, we present the design of feedback skills for robots that must learn to stack complex-shaped objects on top of each other. To design such a system, a robot should be able to reason about the stability of placement from very gentle contact interactions. Our results demonstrate that it is possible to infer the stability of object placement based on tactile readings during contact formation between the object and its environment. In particular, we estimate the contact patch between a grasped object and its environment using force and tactile observations to estimate the stability of the object during a contact formation. The contact patch could be used to estimate the stability of the object upon the release of the grasp. The proposed method is demonstrated on various pairs of objects that are used in a very popular board game.
    
[^354]: K-pop歌词翻译：数据集、分析与神经建模

    K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling. (arXiv:2309.11093v1 [cs.CL])

    [http://arxiv.org/abs/2309.11093](http://arxiv.org/abs/2309.11093)

    研究者介绍了一种新颖的K-pop歌词翻译数据集，该数据集揭示了K-pop歌词翻译的独特特征，并构建了一个神经歌词翻译模型，强调了专用数据集的重要性。

    

    歌词翻译作为一个研究了一个世纪的领域，如今吸引着计算语言学研究者的注意。我们在以往研究中发现了两个限制。首先，在歌词翻译研究中，尽管K-pop非常受欢迎，但主要关注的是西方流派和语言，没有研究集中在K-pop上。其次，歌词翻译领域缺乏可公开获得的数据集；据我们所知，目前尚无此类数据集。为了拓宽歌词翻译研究的流派和语言范围，我们引入了一种新颖的可唱歌词翻译数据集，其中约89%为K-pop歌词。该数据集通过逐行和逐节对齐了韩语和英语歌词。我们利用该数据集揭示了K-pop歌词翻译的独特特征，与其他广泛研究的流派区分开，并构建了一个神经歌词翻译模型，从而强调了专用数据集的重要性。

    Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89\% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for
    
[^355]: 基于错误增强的肌电信号手势分类的用户训练

    User Training with Error Augmentation for Electromyogram-based Gesture Classification. (arXiv:2309.07289v1 [cs.HC])

    [http://arxiv.org/abs/2309.07289](http://arxiv.org/abs/2309.07289)

    该论文研究了一种基于错误增强的肌电信号手势分类的用户训练系统，实验结果表明，相对于基线，使用修改反馈的条件下能够显著提高手势分类准确性和类别区分度。

    

    我们设计并测试了一个实时控制用户界面的系统，通过从腕带配置的八个电极中提取表面肌电活动（sEMG）。sEMG数据被实时流入一个机器学习算法，用于手势的实时分类。在初始模型校准后，参与者在人类学习阶段中被提供了三种类型的反馈：真实反馈，在其中预测的手势分类算法的概率被显示而不进行任何改动；修改反馈，在其中我们对这些概率进行了错误的隐藏增强处理；和无反馈。然后通过一系列的迷你游戏评估了用户的表现，要求被试使用八个手势来操作游戏角色完成任务。实验结果表明，与基线相比，修改反馈条件下的准确性和手势类别区分度显著提高。

    We designed and tested a system for real-time control of a user interface by extracting surface electromyographic (sEMG) activity from eight electrodes in a wrist-band configuration. sEMG data were streamed into a machine-learning algorithm that classified hand gestures in real-time. After an initial model calibration, participants were presented with one of three types of feedback during a human-learning stage: veridical feedback, in which predicted probabilities from the gesture classification algorithm were displayed without alteration, modified feedback, in which we applied a hidden augmentation of error to these probabilities, and no feedback. User performance was then evaluated in a series of minigames, in which subjects were required to use eight gestures to manipulate their game avatar to complete a task. Experimental results indicated that, relative to baseline, the modified feedback condition led to significantly improved accuracy and improved gesture class separation. These 
    
[^356]: 使用可穿戴设备的机器学习模型自动识别认知能力较差的老年人

    Using wearable device-based machine learning models to autonomously identify older adults with poor cognition. (arXiv:2309.07133v1 [eess.SP])

    [http://arxiv.org/abs/2309.07133](http://arxiv.org/abs/2309.07133)

    通过使用可穿戴设备的机器学习模型，可以在老年人正常生活条件下连续监测其认知水平，并能够预测认知能力较差的情况，为早期干预提供了替代方法。

    

    进行认知测试对患者和临床医生来说非常耗时。基于可穿戴设备的预测模型可以在正常生活条件下进行持续的健康监测，并可以作为早期干预认知障碍老年人的一种替代方法。在本研究中，我们首先提取了与生物钟节律、环境光照暴露、身体活动水平、睡眠和信号处理相关的新颖可穿戴特征。然后，我们评估了基于可穿戴设备的机器学习模型根据数字符号替代测试（DSST）、建立阿尔茨海默病登记簿学习子测验（CERAD-WL）和动物流利性测试（AFT）的结果来预测认知能力较差的能力。我们发现，与包含年龄、性别、教育水平、婚姻状况、家庭收入、糖尿病状态和抑郁状况等基准模型相比，基于可穿戴设备的模型在预测三个认知结果时具有显着更高的AUC。

    Conducting cognitive tests is time-consuming for patients and clinicians. Wearable device-based prediction models allow for continuous health monitoring under normal living conditions and could offer an alternative to identifying older adults with cognitive impairments for early interventions. In this study, we first derived novel wearable-based features related to circadian rhythms, ambient light exposure, physical activity levels, sleep, and signal processing. Then, we quantified the ability of wearable-based machine-learning models to predict poor cognition based on outcomes from the Digit Symbol Substitution Test (DSST), the Consortium to Establish a Registry for Alzheimers Disease Word-Learning subtest (CERAD-WL), and the Animal Fluency Test (AFT). We found that the wearable-based models had significantly higher AUCs when predicting all three cognitive outcomes compared to benchmark models containing age, sex, education, marital status, household income, diabetic status, depressio
    
[^357]: InstaFlow: 一步即可实现高质量基于扩散的文本到图像生成

    InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation. (arXiv:2309.06380v1 [cs.LG])

    [http://arxiv.org/abs/2309.06380](http://arxiv.org/abs/2309.06380)

    本研究提出了InstaFlow，一种基于扩散的文本到图像生成方法，将稳定扩散模型转化为一步模型，通过修正的流动方法提高了噪声与图像之间的对应关系，实现了高质量、高速度的图像生成。

    

    扩散模型以其出色的质量和创造力彻底改变了文本到图像生成领域。然而，其多步采样过程被认为很慢，通常需要十几步推断才能获得令人满意的结果。以往试图通过蒸馏来提高采样速度和减少计算成本的尝试都未能实现功能齐全的一步模型。本文中，我们探索了一种最近的方法，即修正的流动方法，这种方法到目前为止只应用于小数据集。修正的流动方法的核心在于其重新流动的过程，它将概率流的轨迹变得直线，改进了噪声与图像之间的耦合关系，并通过学生模型便于蒸馏过程。我们提出了一种新的文本条件的流程，将稳定扩散模型（SD）转化为超快速的一步模型，在其中我们发现重新流动在改善噪声与图像之间的对应关系方面起着关键作用。凭借我们的新流程，我们能够以较快的速度直接生成高质量的图像。

    Diffusion models have revolutionized text-to-image generation with its exceptional quality and creativity. However, its multi-step sampling process is known to be slow, often requiring tens of inference steps to obtain satisfactory results. Previous attempts to improve its sampling speed and reduce computational costs through distillation have been unsuccessful in achieving a functional one-step model. In this paper, we explore a recent method called Rectified Flow, which, thus far, has only been applied to small datasets. The core of Rectified Flow lies in its \emph{reflow} procedure, which straightens the trajectories of probability flows, refines the coupling between noises and images, and facilitates the distillation process with student models. We propose a novel text-conditioned pipeline to turn Stable Diffusion (SD) into an ultra-fast one-step model, in which we find reflow plays a critical role in improving the assignment between noise and images. Leveraging our new pipeline, w
    
[^358]: 通过协同扩散恢复似然学习基于能量的模型

    Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood. (arXiv:2309.05153v1 [stat.ML])

    [http://arxiv.org/abs/2309.05153](http://arxiv.org/abs/2309.05153)

    本文通过协同扩散恢复似然（CDRL）提出了一种方法，用于学习和采样一系列基于能量的模型（EBMs），通过在不断嘈杂化的数据集版本上定义不同噪声水平的EBMs，并与初始化模型配对协同训练。这种方法旨在关闭EBMs和其他生成框架之间的样本质量差距。

    

    在高维数据上使用最大似然估计训练能量基准模型（EBMs）可能具有挑战性且耗时较长。因此，EBMs和其他生成框架（如GANs和扩散模型）之间存在明显的样本质量差距。为了弥补这一差距，受最近通过最大化扩散恢复似然（DRL）来学习EBMs的努力的启发，我们提出了协同扩散恢复似然（CDRL），一种有效的方法来可行地学习和从一系列EBMs中进行采样，这些EBMs定义在越来越嘈杂的数据集版本上，并与每个EBM的初始化模型配对。在每个噪声水平上，初始化模型学习在EBM的采样过程中分摊，而两个模型在协同训练框架内共同估计。初始化模型生成的样本作为起始点，经过EBM的几个采样步骤进行改进。通过改进后的样本，通过最大化恢复似然来优化EBM。

    Training energy-based models (EBMs) with maximum likelihood estimation on high-dimensional data can be both challenging and time-consuming. As a result, there a noticeable gap in sample quality between EBMs and other generative frameworks like GANs and diffusion models. To close this gap, inspired by the recent efforts of learning EBMs by maximimizing diffusion recovery likelihood (DRL), we propose cooperative diffusion recovery likelihood (CDRL), an effective approach to tractably learn and sample from a series of EBMs defined on increasingly noisy versons of a dataset, paired with an initializer model for each EBM. At each noise level, the initializer model learns to amortize the sampling process of the EBM, and the two models are jointly estimated within a cooperative training framework. Samples from the initializer serve as starting points that are refined by a few sampling steps from the EBM. With the refined samples, the EBM is optimized by maximizing recovery likelihood, while t
    
[^359]: ContrastWSD: 使用词义消岐加强隐喻检测

    ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation Following the Metaphor Identification Procedure. (arXiv:2309.03103v1 [cs.CL])

    [http://arxiv.org/abs/2309.03103](http://arxiv.org/abs/2309.03103)

    ContrastWSD是一种使用了词义消岐的隐喻检测模型，通过将隐喻识别过程和词义消岐结合起来，提取并对比单词的上下文含义和基本含义，以提高隐喻检测的效果，超过其他仅依赖上下文嵌入或集成基本定义和外部知识的方法。

    

    本文提出了ContrastWSD，一种基于RoBERTa的隐喻检测模型，它集成了隐喻识别过程(MIP)和词义消岐(WSD)来提取并对比单词的上下文含义和基本含义，以确定它在句子中是否以隐喻的方式使用。通过利用WSD模型得出的单词词义，我们的模型增强了隐喻检测过程，并超过了仅依赖上下文嵌入或仅集成基本定义和其他外部知识的其他方法。我们在多个基准数据集上评估了我们的方法，并与强基线进行比较，结果表明它在推进隐喻检测方面的有效性。

    This paper presents ContrastWSD, a RoBERTa-based metaphor detection model that integrates the Metaphor Identification Procedure (MIP) and Word Sense Disambiguation (WSD) to extract and contrast the contextual meaning with the basic meaning of a word to determine whether it is used metaphorically in a sentence. By utilizing the word senses derived from a WSD model, our model enhances the metaphor detection process and outperforms other methods that rely solely on contextual embeddings or integrate only the basic definitions and other external knowledge. We evaluate our approach on various benchmark datasets and compare it with strong baselines, indicating the effectiveness in advancing metaphor detection.
    
[^360]: BigVSAN: 利用切片对抗网络增强基于GAN的神经声码器

    BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial Network. (arXiv:2309.02836v1 [cs.SD])

    [http://arxiv.org/abs/2309.02836](http://arxiv.org/abs/2309.02836)

    本文研究了利用切片对抗网络（SAN）来增强基于GAN的神经声码器，并通过实验证明了通过修改最小二乘GAN的损失函数，SAN可以改善声码器的性能。

    

    基于生成对抗网络（GAN）的声码器已经得到广泛研究，因为它们可以比实时更快地合成高保真音频波形。然而，已有研究报告发现大多数GAN在特征空间中无法获得区分真假数据的最佳投影。在文献中已经证明，切片对抗网络（SAN）作为一种改进的GAN训练框架，在图像生成任务中是有效的。本文中，我们研究了SAN在声码任务中的有效性。为此，我们提出了一种方案，修改了大多数基于GAN的声码器所采用的最小二乘GAN，使其损失函数满足SAN的要求。通过实验，我们证明了SAN可以通过小的修改改善包括BigVGAN在内的基于GAN的声码器的性能。我们的代码可在https://github.com/sony/bigvsan上获得。

    Generative adversarial network (GAN)-based vocoders have been intensively studied because they can synthesize high-fidelity audio waveforms faster than real-time. However, it has been reported that most GANs fail to obtain the optimal projection for discriminating between real and fake data in the feature space. In the literature, it has been demonstrated that slicing adversarial network (SAN), an improved GAN training framework that can find the optimal projection, is effective in the image generation task. In this paper, we investigate the effectiveness of SAN in the vocoding task. For this purpose, we propose a scheme to modify least-squares GAN, which most GAN-based vocoders adopt, so that their loss functions satisfy the requirements of SAN. Through our experiments, we demonstrate that SAN can improve the performance of GAN-based vocoders, including BigVGAN, with small modifications. Our code is available at https://github.com/sony/bigvsan.
    
[^361]: 可分离哈密顿神经网络

    Separable Hamiltonian Neural Networks. (arXiv:2309.01069v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01069](http://arxiv.org/abs/2309.01069)

    这篇论文介绍了可分离哈密顿神经网络的应用，它通过嵌入可加性分离性来解决高维哈密顿系统中的复杂性问题。

    

    利用离散观测数据建模动力系统是现代科学和工程数据系统面临的挑战之一。 哈密顿系统是一类基本且广泛存在的动力系统。 哈密顿神经网络是最先进的模型，可以在汉密尔顿方程的学习偏差下，从离散观测的向量场中无监督地回归动力系统的哈密顿量。然而，哈密顿动力学通常很复杂，特别是在高维情况下，其中哈密顿系统的状态空间相对于样本数量是很大的。 最近发现的一种缓解状态变量之间复杂性的方法是利用哈密顿系统的可加性分离性，并将该可加性分离性嵌入哈密顿神经网络中。根据物理学驱动的机器学习的术语，我们提出了三种可分离的哈密顿神经网络。这些模型嵌入了可加性分离性。

    The modelling of dynamical systems from discrete observations is a challenge faced by modern scientific and engineering data systems. Hamiltonian systems are one such fundamental and ubiquitous class of dynamical systems. Hamiltonian neural networks are state-of-the-art models that unsupervised-ly regress the Hamiltonian of a dynamical system from discrete observations of its vector field under the learning bias of Hamilton's equations. Yet Hamiltonian dynamics are often complicated, especially in higher dimensions where the state space of the Hamiltonian system is large relative to the number of samples. A recently discovered remedy to alleviate the complexity between state variables in the state space is to leverage the additive separability of the Hamiltonian system and embed that additive separability into the Hamiltonian neural network. Following the nomenclature of physics-informed machine learning, we propose three separable Hamiltonian neural networks. These models embed additi
    
[^362]: 一种Huber损失最小化方法用于拜占庭鲁棒的联邦学习

    A Huber Loss Minimization Approach to Byzantine Robust Federated Learning. (arXiv:2308.12581v1 [cs.LG])

    [http://arxiv.org/abs/2308.12581](http://arxiv.org/abs/2308.12581)

    本文介绍了一种基于Huber损失最小化的新型聚合器用于拜占庭鲁棒的联邦学习。在独立同分布假设下，该方法具有与现有方法相比的优势，并对非i.i.d数据进行了扩展分析。

    

    联邦学习系统容易受到对抗攻击。为了应对这个问题，我们引入了一种基于Huber损失最小化的新型聚合器，并提供了全面的理论分析。在独立同分布（i.i.d）假设下，与现有方法相比，我们的方法具有几个优势。首先，它对于被攻击客户端比率$\epsilon$具有最优的依赖关系。其次，我们的方法不需要对$\epsilon$有精确的知识。第三，它允许不同的客户端具有不均等的数据大小。然后，我们将分析扩展到包括非i.i.d数据，这意味着客户端具有略有不同的分布。

    Federated learning systems are susceptible to adversarial attacks. To combat this, we introduce a novel aggregator based on Huber loss minimization, and provide a comprehensive theoretical analysis. Under independent and identically distributed (i.i.d) assumption, our approach has several advantages compared to existing methods. Firstly, it has optimal dependence on $\epsilon$, which stands for the ratio of attacked clients. Secondly, our approach does not need precise knowledge of $\epsilon$. Thirdly, it allows different clients to have unequal data sizes. We then broaden our analysis to include non-i.i.d data, such that clients have slightly different distributions.
    
[^363]: 《联邦学习评估：目标和指标的调查》

    A Survey for Federated Learning Evaluations: Goals and Measures. (arXiv:2308.11841v1 [cs.LG])

    [http://arxiv.org/abs/2308.11841](http://arxiv.org/abs/2308.11841)

    本研究调查了联邦学习的评估目标和指标，介绍了一个开源平台FedEval，提供了标准化的评估框架，为联邦学习算法的效用、效率和安全性方面进行评估，并讨论了该领域面临的挑战和未来研究方向。

    

    评估是一种系统评估一个系统如何实现其预期目标的方法。联邦学习（FL）是一种保护隐私的机器学习新范式，允许多个参与方在不共享敏感数据的情况下共同训练模型。然而，评估FL具有跨学科性和多样化的目标（如效用、效率和安全性），因此具有挑战性。在本调查中，我们首先回顾了现有研究中采用的主要评估目标，然后探讨了每个目标使用的评估指标。我们还介绍了FedEval，一个开源平台，它提供了一个标准化和全面的联邦学习算法评估框架，涵盖了其效用、效率和安全性。最后，我们讨论了联邦学习评估面临的几个挑战和未来研究方向。

    Evaluation is a systematic approach to assessing how well a system achieves its intended purpose. Federated learning (FL) is a novel paradigm for privacy-preserving machine learning that allows multiple parties to collaboratively train models without sharing sensitive data. However, evaluating FL is challenging due to its interdisciplinary nature and diverse goals, such as utility, efficiency, and security. In this survey, we first review the major evaluation goals adopted in the existing studies and then explore the evaluation metrics used for each goal. We also introduce FedEval, an open-source platform that provides a standardized and comprehensive evaluation framework for FL algorithms in terms of their utility, efficiency, and security. Finally, we discuss several challenges and future research directions for FL evaluation.
    
[^364]: ALI-DPFL: 具有自适应本地迭代的差分隐私联邦学习

    ALI-DPFL: Differentially Private Federated Learning with Adaptive Local Iterations. (arXiv:2308.10457v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10457](http://arxiv.org/abs/2308.10457)

    ALI-DPFL是一种进行差分隐私联邦学习的算法，通过自适应本地迭代来优化性能，并在实验中展示了显著的改进。

    

    联邦学习是一种分布式机器学习技术，通过共享训练参数而不是原始数据，允许多个设备或组织之间进行模型训练。然而，攻击者仍然可以通过对这些训练参数的推理攻击（例如差分攻击）来推断个体信息。因此，差分隐私被广泛应用于联邦学习中以防止此类攻击。我们在资源受限的场景中考虑差分隐私联邦学习，其中既有隐私预算受限，又有通信轮次受限。通过理论分析收敛性，我们可以找到在任意两个顺序全局更新之间的客户机之间的最佳差分隐私本地迭代次数。基于此，我们设计了一种具有自适应本地迭代的差分隐私联邦学习算法（ALI-DPFL）。我们在FashionMNIST和CIFAR10数据集上对我们的算法进行实验，并展示了显著更好的性能。

    Federated Learning (FL) is a distributed machine learning technique that allows model training among multiple devices or organizations by sharing training parameters instead of raw data. However, adversaries can still infer individual information through inference attacks (e.g. differential attacks) on these training parameters. As a result, Differential Privacy (DP) has been widely used in FL to prevent such attacks. We consider differentially private federated learning in a resource-constrained scenario, where both privacy budget and communication round are constrained. By theoretically analyzing the convergence, we can find the optimal number of differentially private local iterations for clients between any two sequential global updates. Based on this, we design an algorithm of differentially private federated learning with adaptive local iterations (ALI-DPFL). We experiment our algorithm on the FashionMNIST and CIFAR10 datasets, and demonstrate significantly better performances th
    
[^365]: 基于图神经网络和规则的归纳知识图谱补全的分析

    Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis. (arXiv:2308.07942v1 [cs.AI])

    [http://arxiv.org/abs/2308.07942](http://arxiv.org/abs/2308.07942)

    基于图神经网络和规则的归纳知识图谱补全研究了基于规则的方法在实践中的表现不佳的原因，发现不合理的实体没有排名和只考虑最具信息量的路径是影响因素。提出了一些解决这些问题的规则方法的变体，发现其性能接近于基于图神经网络的方法NBFNet。这些变体仅使用了NBFNet所依赖的证据的一小部分。

    

    归纳知识图谱补全的任务要求模型从训练图谱中学习推理模式，然后可以用来在分离的测试图谱上进行预测。虽然基于规则的方法似乎很适合这个任务，但在实践中，它们的表现明显不如基于图神经网络（GNNs）的最先进方法，如NBFNet。我们假设基于规则的方法表现不佳是由于两个因素：（i）不合理的实体根本没有排名，（ii）在确定给定链接预测答案的置信度时，只考虑了最具信息量的路径。为了分析这些因素的影响，我们研究了一些针对上述问题的规则方法的变体。我们发现，所得到的模型的性能接近NBFNet。至关重要的是，考虑到的变体只使用了NBFNet所依赖的证据的一小部分。

    The task of inductive knowledge graph completion requires models to learn inference patterns from a training graph, which can then be used to make predictions on a disjoint test graph. Rule-based methods seem like a natural fit for this task, but in practice they significantly underperform state-of-the-art methods based on Graph Neural Networks (GNNs), such as NBFNet. We hypothesise that the underperformance of rule-based methods is due to two factors: (i) implausible entities are not ranked at all and (ii) only the most informative path is taken into account when determining the confidence in a given link prediction answer. To analyse the impact of these factors, we study a number of variants of a rule-based approach, which are specifically aimed at addressing the aforementioned issues. We find that the resulting models can achieve a performance which is close to that of NBFNet. Crucially, the considered variants only use a small fraction of the evidence that NBFNet relies on, which m
    
[^366]: 在无线网络中的分层联邦学习：剪枝解决带宽稀缺和系统异构性问题

    Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity. (arXiv:2308.01562v1 [eess.SY])

    [http://arxiv.org/abs/2308.01562](http://arxiv.org/abs/2308.01562)

    本研究提出了一种剪枝增强的分层联邦学习（PHFL）方法，用于解决无线网络中的带宽稀缺和系统异构性问题。通过模型剪枝和无线通信的优化，实现了在严格的延迟和能耗约束下收敛速度的最优化。

    

    在实际无线网络中，由于终端用户与中央服务器之间存在多个层级，用户设备的计算和电池能力有限，而服务基站具有固定的带宽。鉴于这些实际约束和系统模型，本文利用模型剪枝，并提出一种适用于异构网络的剪枝增强分层联邦学习（PHFL）方法。我们首先推导出收敛速度的上界，清晰地展示了模型剪枝和客户端与关联基站之间的无线通信的影响。然后，我们联合优化模型剪枝比率、客户端的中央处理器（CPU）频率和传输功率，以在严格的延迟和能耗约束下最小化收敛界的可控项。然而，由于原始问题不是凸问题，我们采用逐步凸逼近（SCA）方法，联合优化参数。

    While a practical wireless network has many tiers where end users do not directly communicate with the central server, the users' devices have limited computation and battery powers, and the serving base station (BS) has a fixed bandwidth. Owing to these practical constraints and system models, this paper leverages model pruning and proposes a pruning-enabled hierarchical federated learning (PHFL) in heterogeneous networks (HetNets). We first derive an upper bound of the convergence rate that clearly demonstrates the impact of the model pruning and wireless communications between the clients and the associated BS. Then we jointly optimize the model pruning ratio, central processing unit (CPU) frequency and transmission power of the clients in order to minimize the controllable terms of the convergence bound under strict delay and energy constraints. However, since the original problem is not convex, we perform successive convex approximation (SCA) and jointly optimize the parameters fo
    
[^367]: 评估年龄估计实践的反思呼吁：现有技术的比较分析和统一基准

    A Call to Reflect on Evaluation Practices for Age Estimation: Comparative Analysis of the State-of-the-Art and a Unified Benchmark. (arXiv:2307.04570v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.04570](http://arxiv.org/abs/2307.04570)

    该论文提出反思评估年龄估计实践的呼吁，对现有技术进行了比较分析，并提出了统一基准。研究发现当前评估协议存在问题，并描述了如何解决它们。通过比较分析，发现方法之间的性能差异微不足道，而其他因素的影响更大。研究利用得到的见解提出使用FaRL方法来解决这些问题。

    

    由于基准流程的不一致性导致的发布结果的不可靠性，比较不同的年龄估计方法面临着挑战。先前的研究报告指出，使用专门的方法在过去十年中持续改善了性能，然而我们的发现对这些声明提出质疑。本文识别出当前使用的评估协议中存在的两个琐碎但持续存在的问题，并描述了如何解决它们。我们详细描述了我们的评估协议，并提供了使用该协议的具体示例。我们利用该协议对最先进的面部年龄估计方法进行了广泛的比较分析。令人惊讶的是，我们发现方法之间的性能差异与其他因素（如面部对齐、面部覆盖、图像分辨率、模型架构或用于预训练的数据量）相比微不足道。我们利用这些见解提出使用FaRL方法来解决这些问题。

    Comparing different age estimation methods poses a challenge due to the unreliability of published results stemming from inconsistencies in the benchmarking process. Previous studies have reported continuous performance improvements over the past decade using specialized methods; however, our findings challenge these claims. This paper identifies two trivial, yet persistent issues with the currently used evaluation protocol and describes how to resolve them. We describe our evaluation protocol in detail and provide specific examples of how the protocol should be used. We utilize the protocol to offer an extensive comparative analysis for state-of-the-art facial age estimation methods. Surprisingly, we find that the performance differences between the methods are negligible compared to the effect of other factors, such as facial alignment, facial coverage, image resolution, model architecture, or the amount of data used for pretraining. We use the gained insights to propose using FaRL a
    
[^368]: 稀疏模型汤：通过模型平均改进修剪的方法

    Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging. (arXiv:2306.16788v1 [cs.LG])

    [http://arxiv.org/abs/2306.16788](http://arxiv.org/abs/2306.16788)

    本研究通过将多个经过迭代幅度剪枝的模型进行平均，解决了同时利用稀疏性和参数平均的问题，并显著提升了泛化性能。

    

    神经网络可以通过剪枝显著压缩，从而得到稀疏模型，这些模型需要更少的存储和浮点运算，同时保持预测性能。模型汤（Wortsman等人，2022年）通过将多个模型的参数平均成一个单一模型来改善泛化和超出分布性能，而不增加推理时间。然而，识别处于相同损失区域的模型以同时利用稀疏性和参数平均是具有挑战性的，因为对任意稀疏模型进行平均会降低整体稀疏度，原因是不同的稀疏连接性。在这项工作中，我们通过展示在迭代幅度剪枝（IMP）的单次重新训练阶段中探索不同的超参数配置（例如批次排序或权重衰减）产生的模型适合进行平均，并且通过设计共享相同的稀疏连接性来解决这些挑战。平均这些模型显著提升了泛化性能。

    Neural networks can be significantly compressed by pruning, leading to sparse models requiring considerably less storage and floating-point operations while maintaining predictive performance. Model soups (Wortsman et al., 2022) improve generalization and out-of-distribution performance by averaging the parameters of multiple models into a single one without increased inference time. However, identifying models in the same loss basin to leverage both sparsity and parameter averaging is challenging, as averaging arbitrary sparse models reduces the overall sparsity due to differing sparse connectivities. In this work, we address these challenges by demonstrating that exploring a single retraining phase of Iterative Magnitude Pruning (IMP) with varying hyperparameter configurations, such as batch ordering or weight decay, produces models that are suitable for averaging and share the same sparse connectivity by design. Averaging these models significantly enhances generalization performanc
    
[^369]: 基于Samplet基 Pursuit 的核学习方法

    Samplet basis pursuit. (arXiv:2306.10180v1 [stat.ML])

    [http://arxiv.org/abs/2306.10180](http://arxiv.org/abs/2306.10180)

    本文提出了基于Samplet坐标下核学习的方法，其中引入l1正则化项可以增加系数的稀疏性。相比于单尺度基，Samplet基可以更好地表示更多类型的信号。作者提出了使用软阈值和半光滑牛顿法解决该问题的方法，并通过实验证明了其优越性。

    

    本文考虑了基于l1正则化的Samplet坐标下的核学习问题。在Samplet基的系数上，应用l1正则化项可以强制增加稀疏性。因此，我们称这种方法为Samplet基 Pursuit。Samplet基是波形类型的有符号测度，专门用于散乱数据。它们具有与小波相似的本地化、多分辨率分析和数据压缩性质。可以在Samplet基上稀疏地表示的信号类比单尺度基上能够表示稀疏的信号类别要大得多。特别地，仅用基函数映射的几个特征叠加即可表示的所有信号也可以在Samplet坐标下实现稀疏表示。我们提出了一种高效解决该问题的方法，将软阈值和半光滑牛顿法相结合，并将该方法与快速迭代收缩阈值算法进行了比较。实验结果表明了该方法在稀疏性和预测精度方面的优势。

    We consider kernel-based learning in samplet coordinates with l1-regularization. The application of an l1-regularization term enforces sparsity of the coefficients with respect to the samplet basis. Therefore, we call this approach samplet basis pursuit. Samplets are wavelet-type signed measures, which are tailored to scattered data. They provide similar properties as wavelets in terms of localization, multiresolution analysis, and data compression. The class of signals that can sparsely be represented in a samplet basis is considerably larger than the class of signals which exhibit a sparse representation in the single-scale basis. In particular, every signal that can be represented by the superposition of only a few features of the canonical feature map is also sparse in samplet coordinates. We propose the efficient solution of the problem under consideration by combining soft-shrinkage with the semi-smooth Newton method and compare the approach to the fast iterative shrinkage thresh
    
[^370]: 差分隐私条件独立性检验

    Differentially Private Conditional Independence Testing. (arXiv:2306.06721v1 [stat.ML])

    [http://arxiv.org/abs/2306.06721](http://arxiv.org/abs/2306.06721)

    本文介绍了两个差分隐私条件独立性检验方法，可适用于Z为连续值的一般情况。

    

    条件独立性（CI）检验在统计数据分析中被广泛使用，例如，它们是许多因果图发现算法的构建块。CI测试旨在接受或拒绝$X \perp \!\!\! \perp Y \mid Z$的零假设，其中$X \in \mathbb{R}，Y \in \mathbb{R}，Z \in \mathbb{R}^d$。本文研究了在差分隐私约束下的条件独立性检验。我们设计了基于Shah和Peters（2020）的一般化协方差测量和基于Cand\`es等人的条件随机化检验的两种私人CI测试过程（在模型-X假设下）。我们提供了关于我们测试性能的理论保证，并在实证上验证它们。这些是第一个适用于Z为连续的一般情况的私人CI测试。

    Conditional independence (CI) tests are widely used in statistical data analysis, e.g., they are the building block of many algorithms for causal graph discovery. The goal of a CI test is to accept or reject the null hypothesis that $X \perp \!\!\! \perp Y \mid Z$, where $X \in \mathbb{R}, Y \in \mathbb{R}, Z \in \mathbb{R}^d$. In this work, we investigate conditional independence testing under the constraint of differential privacy. We design two private CI testing procedures: one based on the generalized covariance measure of Shah and Peters (2020) and another based on the conditional randomization test of Cand\`es et al. (2016) (under the model-X assumption). We provide theoretical guarantees on the performance of our tests and validate them empirically. These are the first private CI tests that work for the general case when $Z$ is continuous.
    
[^371]: 迁移族群以实现公平GNN

    Migrate Demographic Group For Fair GNNs. (arXiv:2306.04212v1 [cs.LG])

    [http://arxiv.org/abs/2306.04212](http://arxiv.org/abs/2306.04212)

    该论文提出了一个名为FairMigration的新框架，可以动态迁移族群，而不是用原始的敏感属性来固定族群，以训练公平的GNN。

    

    由于图形学习的卓越性能，图神经网络（GNN）已被应用于许多场景。然而，在设计GNN时常常忽略公平性。因此，训练数据中的有偏信息很容易影响普通的GNN，导致对特定人口群体（根据敏感属性，如种族和年龄划分）的偏见结果。已经有一些努力来解决公平性问题。然而，现有的公平技术通常通过原始敏感属性将族群进行划分，并假定它们是固定的。与原始敏感属性相关的有偏信息将通过训练过程，无论实施公平技术与否。迫切需要解决这个问题，以训练公平的GNN。为了解决这个问题，我们提出了一个全新的框架，FairMigration，它可以动态迁移族群，而不是用原始的敏感属性固定它们。FairMigration由两个训练阶段组成。

    Graph Neural networks (GNNs) have been applied in many scenarios due to the superior performance of graph learning. However, fairness is always ignored when designing GNNs. As a consequence, biased information in training data can easily affect vanilla GNNs, causing biased results toward particular demographic groups (divided by sensitive attributes, such as race and age). There have been efforts to address the fairness issue. However, existing fair techniques generally divide the demographic groups by raw sensitive attributes and assume that are fixed. The biased information correlated with raw sensitive attributes will run through the training process regardless of the implemented fair techniques. It is urgent to resolve this problem for training fair GNNs. To tackle this problem, we propose a brand new framework, FairMigration, which can dynamically migrate the demographic groups instead of keeping that fixed with raw sensitive attributes. FairMigration is composed of two training s
    
[^372]: 针对离线设计生物序列的得分条件生成器的自助增强训练

    Bootstrapped Training of Score-Conditioned Generator for Offline Design of Biological Sequences. (arXiv:2306.03111v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.03111](http://arxiv.org/abs/2306.03111)

    本文提出了一种BootGen算法，使用代理得分函数增强生物序列生成器的训练数据集，并产生多样化的设计，将其应用于优化生物序列，取得了比竞争对手更好的结果。

    

    本文研究了优化生物序列（如蛋白质、DNA和RNA）以最大化仅在离线数据集中评估的黑匣子得分函数的问题。我们提出了一种新颖的解决方案——得分条件生成器的自助增强训练（BootGen）算法。我们的算法重复了一个两阶段过程。在第一阶段，我们的算法使用排名加权法训练生物序列生成器，以提高基于高分数的序列生成的准确性。接下来的阶段涉及到自助增强，通过自动生成的数据并标记代理得分函数，来增强训练数据集。我们的关键思想是将基于得分的生成与代理得分函数对齐，将代理得分函数的知识传递给生成器。训练后，我们聚合来自多个自助增强生成器和代理的样本，产生多样化的设计。大量实验表明，我们的方法在生物序列优化方面胜过竞争对手。

    We study the problem of optimizing biological sequences, e.g., proteins, DNA, and RNA, to maximize a black-box score function that is only evaluated in an offline dataset. We propose a novel solution, bootstrapped training of score-conditioned generator (BootGen) algorithm. Our algorithm repeats a two-stage process. In the first stage, our algorithm trains the biological sequence generator with rank-based weights to enhance the accuracy of sequence generation based on high scores. The subsequent stage involves bootstrapping, which augments the training dataset with self-generated data labeled by a proxy score function. Our key idea is to align the score-based generation with a proxy score function, which distills the knowledge of the proxy score function to the generator. After training, we aggregate samples from multiple bootstrapped generators and proxies to produce a diverse design. Extensive experiments show that our method outperforms competitive baselines on biological sequential
    
[^373]: DiffusionNAG: 基于扩散模型的预测引导神经结构生成

    DiffusionNAG: Predictor-guided Neural Architecture Generation with Diffusion Models. (arXiv:2305.16943v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16943](http://arxiv.org/abs/2305.16943)

    DiffusionNAG是一种基于扩散模型的神经结构生成方法，通过考虑神经结构的有向图特性，并结合参数化的预测器的指导，可以更高效地生成具有期望特性的任务最优结构。

    

    现有的神经架构搜索(NAS)方法存在着对许多与任务无关的架构进行重复采样和训练所需的过长时间的问题。为了解决这些问题，我们提出了一种从NAS转向基于扩散模型的新型条件神经结构生成(NAG)框架，命名为DiffusionNAG。具体地，我们将神经结构视为有向图，并提出了一种用于生成神经结构的图扩散模型。此外，在参数化的预测器的指导下，DiffusionNAG可以通过从更有可能满足所需特性的区域中进行采样，灵活生成具有期望特性的任务最优结构。与使用属性预测器对架构进行采样和过滤的先前NAS方案相比，这种条件NAG方案显著更高效。我们通过在两个基于预测器的NAS场景下进行大量实验验证了DiffusionNAG的有效性。

    Existing NAS methods suffer from either an excessive amount of time for repetitive sampling and training of many task-irrelevant architectures. To tackle such limitations of existing NAS methods, we propose a paradigm shift from NAS to a novel conditional Neural Architecture Generation (NAG) framework based on diffusion models, dubbed DiffusionNAG. Specifically, we consider the neural architectures as directed graphs and propose a graph diffusion model for generating them. Moreover, with the guidance of parameterized predictors, DiffusionNAG can flexibly generate task-optimal architectures with the desired properties for diverse tasks, by sampling from a region that is more likely to satisfy the properties. This conditional NAG scheme is significantly more efficient than previous NAS schemes which sample the architectures and filter them using the property predictors. We validate the effectiveness of DiffusionNAG through extensive experiments in two predictor-based NAS scenarios: Trans
    
[^374]: 图上反事实学习：综述

    Counterfactual Learning on Graphs: A Survey. (arXiv:2304.01391v1 [cs.LG])

    [http://arxiv.org/abs/2304.01391](http://arxiv.org/abs/2304.01391)

    本文综述了图上反事实学习的研究进展，包括反事实公平性、可解释性、链路预测等不同应用问题，并提出了未来的研究方向。

    

    图结构数据在现实世界中非常普遍，如社交网络、分子图和交易网络。图神经网络（GNNs）在图表示学习方面取得了巨大的成功，促进了各种下游任务。然而，GNN具有一些缺点，如缺乏可解释性、容易继承训练数据的偏见，不能建模因果关系。最近，图上反事实学习已经显示出在缓解这些缺点方面具有很有前途的结果。为了促进这个有前途的方向的发展，本综述将分类和全面地评估反事实图学习论文，为每个类别提供背景和激励性例子、一般框架和代表性方法的讨论。最后，我们总结了图上反事实学习的挑战和机遇，并指出未来的研究方向。

    Graph-structured data are pervasive in the real-world such as social networks, molecular graphs and transaction networks. Graph neural networks (GNNs) have achieved great success in representation learning on graphs, facilitating various downstream tasks. However, GNNs have several drawbacks such as lacking interpretability, can easily inherit the bias of the training data and cannot model the casual relations. Recently, counterfactual learning on graphs has shown promising results in alleviating these drawbacks. Various graph counterfactual learning approaches have been proposed for counterfactual fairness, explainability, link prediction and other applications on graphs. To facilitate the development of this promising direction, in this survey, we categorize and comprehensively review papers on graph counterfactual learning. We divide existing methods into four categories based on research problems studied. For each category, we provide background and motivating examples, a general f
    
[^375]: 研究和减轻多视角聚类中实际场景中嘈杂视图的副作用

    Investigating and Mitigating the Side Effects of Noisy Views in Multi-view Clustering in Practical Scenarios. (arXiv:2303.17245v1 [cs.LG])

    [http://arxiv.org/abs/2303.17245](http://arxiv.org/abs/2303.17245)

    本文提出了一种理论上基础的深度MvC方法（MvCAN），旨在解决实际场景中嘈杂视图的问题，通过实现多视图一致性、互补性和噪声鲁棒性来减少嘈杂视图的副作用，并在实验证明该方法优于现有的MvC方法。

    

    多视角聚类（MvC）旨在探索多视图数据的类别结构，而无需标签监督。多视图比单视图提供更多信息，因此现有的MvC方法可以实现令人满意的性能。然而，在实际场景中，如果视图嘈杂，它们的性能可能会严重退化。在本文中，我们首先正式研究了嘈杂视图的缺点，随后提出了一个理论上基础的深度MvC方法（称为MvCAN）来解决这个问题。具体来说，我们提出了一个新颖的MvC目标，使得不共享参数和不一致的聚类预测可以跨越多个视图，以减少嘈杂视图的副作用。此外，设计了一种非参数迭代过程，以生成一个稳健的学习目标，以挖掘多个视图的有用信息。理论分析表明，MvCAN的工作是通过实现多视图一致性，互补性和噪声鲁棒性来实现的。最后，对公开基准数据集和新收集的实际数据集进行的实验证明，MvCAN在处理实际场景中的嘈杂视图方面优于现有的MvC方法。

    Multi-view clustering (MvC) aims at exploring the category structure among multi-view data without label supervision. Multiple views provide more information than single views and thus existing MvC methods can achieve satisfactory performance. However, their performance might seriously degenerate when the views are noisy in practical scenarios. In this paper, we first formally investigate the drawback of noisy views and then propose a theoretically grounded deep MvC method (namely MvCAN) to address this issue. Specifically, we propose a novel MvC objective that enables un-shared parameters and inconsistent clustering predictions across multiple views to reduce the side effects of noisy views. Furthermore, a non-parametric iterative process is designed to generate a robust learning target for mining multiple views' useful information. Theoretical analysis reveals that MvCAN works by achieving the multi-view consistency, complementarity, and noise robustness. Finally, experiments on publ
    
[^376]: 多项式分类中的稀疏联合偏移

    Sparse joint shift in multinomial classification. (arXiv:2303.16971v1 [stat.ML])

    [http://arxiv.org/abs/2303.16971](http://arxiv.org/abs/2303.16971)

    该论文提出了一种稀疏联合偏移模型，用于解决整体数据集偏移问题，提供了传递SJS、修正类后验概率、SJS的可辨认性、SJS与协变量转移关系等新结果。

    

    稀疏联合偏移（SJS）是一种针对数据集整体偏移的可处理模型，可能会导致特征和标签的边际分布以及后验概率和类条件特征分布的变化。在没有标签观测的情况下，为目标数据集拟合SJS可能会产生标签的有效预测和类先验概率的估计。我们在特征集之间传递SJS方面提供了新的结果，提出了一个基于目标分布的类后验概率的条件修正公式，确定性SJS的可辨认性以及SJS和协变量转移之间的关系。此外，我们指出了用于估计SJS特征的算法中的不一致性，因为它们可能会妨碍寻找最优解。

    Sparse joint shift (SJS) was recently proposed as a tractable model for general dataset shift which may cause changes to the marginal distributions of features and labels as well as the posterior probabilities and the class-conditional feature distributions. Fitting SJS for a target dataset without label observations may produce valid predictions of labels and estimates of class prior probabilities. We present new results on the transmission of SJS from sets of features to larger sets of features, a conditional correction formula for the class posterior probabilities under the target distribution, identifiability of SJS, and the relationship between SJS and covariate shift. In addition, we point out inconsistencies in the algorithms which were proposed for estimating the characteristics of SJS, as they could hamper the search for optimal solutions.
    
[^377]: FrankenSplit:基于显著性指导的神经特征压缩与浅层变分瓶颈注入

    FrankenSplit: Saliency Guided Neural Feature Compression with Shallow Variational Bottleneck Injection. (arXiv:2302.10681v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.10681](http://arxiv.org/abs/2302.10681)

    本文提出了一种基于显著性指导的神经特征压缩与浅层变分瓶颈注入的新的资源意识压缩模型的框架，实现了比最先进的SC方法低60％的比特率，并且比现有的编解码标准的下放快16倍。

    

    移动AI加速器的崛起使得对延迟敏感的应用可以在客户端上执行轻量级深度神经网络（DNN）。然而，需要强大模型的关键应用程序需要将请求下放，而高维数据将争夺有限的带宽。本文提出了一种新的资源意识压缩模型的框架并在反映边缘设备和服务器之间不对称资源分配的环境中进行了广泛评估。我们的方法在不降低准确性的情况下实现了比最先进的SC方法低60％的比特率，并且比现有的编解码标准的下放快16倍。

    The rise of mobile AI accelerators allows latency-sensitive applications to execute lightweight Deep Neural Networks (DNNs) on the client side. However, critical applications require powerful models that edge devices cannot host and must therefore offload requests, where the high-dimensional data will compete for limited bandwidth. This work proposes shifting away from focusing on executing shallow layers of partitioned DNNs. Instead, it advocates concentrating the local resources on variational compression optimized for machine interpretability. We introduce a novel framework for resource-conscious compression models and extensively evaluate our method in an environment reflecting the asymmetric resource distribution between edge devices and servers. Our method achieves 60\% lower bitrate than a state-of-the-art SC method without decreasing accuracy and is up to 16x faster than offloading with existing codec standards.
    
[^378]: 联邦自加权领域自适应

    Federated Auto-weighted Domain Adaptation. (arXiv:2302.05049v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05049](http://arxiv.org/abs/2302.05049)

    这篇论文提出了一种用于联邦领域自适应的新聚合规则-联邦梯度投影。在此基础上，开发了一个自动加权方案，用于最优地结合源和目标梯度，以解决在数据稀缺和领域转移时常见的技术失败的问题。

    

    联邦领域自适应（FDA）是描述多个源客户端协作改善目标客户端性能的联邦学习设置，其中目标领域数据有限。源领域和目标领域之间的领域转移，加上目标领域的稀疏数据，使得FDA成为一个具有挑战性的问题，例如，常见的技术（如FedAvg和微调）在存在显著领域转移和数据稀缺性时通常会失败。为了全面了解这个问题，我们介绍了表征FDA设置的度量标准，并提出了用于分析聚合规则性能的理论框架。我们还提出了用于FDA的一种新的聚合规则，称为联邦梯度投影（$\texttt{FedGP}$），用于在训练期间聚合源梯度和目标梯度。重要的是，我们的框架使得开发一个自动加权方案成为可能，这个方案能够最优地结合源和目标梯度。

    Federated Domain Adaptation (FDA) describes the federated learning setting where a set of source clients work collaboratively to improve the performance of a target client where limited data is available. The domain shift between the source and target domains, coupled with sparse data in the target domain, makes FDA a challenging problem, e.g., common techniques such as FedAvg and fine-tuning, often fail with the presence of significant domain shift and data scarcity. To comprehensively understand the problem, we introduce metrics that characterize the FDA setting and put forth a theoretical framework for analyzing the performance of aggregation rules. We also propose a novel aggregation rule for FDA, Federated Gradient Projection ($\texttt{FedGP}$), used to aggregate the source gradients and target gradient during training. Importantly, our framework enables the development of an $\textit{auto-weighting scheme}$ that optimally combines the source and target gradients. This scheme impr
    
[^379]: Box$^2$EL: EL++描述逻辑中的概念和角色盒子嵌入的概念和角色盒子嵌入方法及其作用

    Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++. (arXiv:2301.11118v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.11118](http://arxiv.org/abs/2301.11118)

    Box$^2$EL方法通过将概念和角色表示为盒子，克服了传统方法中角色表示受限的问题，并在实验中取得了领先的结果。

    

    描述逻辑本体论扩展了知识图谱与概念信息和逻辑背景知识。近年来，人们对这种本体论的归纳推理技术越来越感兴趣，这些技术有望补充传统的演绎推理算法。类似于知识图谱的完善，现有的一些方法通过在潜在空间中学习本体论嵌入，同时确保这些嵌入能够准确地捕捉到底层描述逻辑的逻辑语义。然而，它们存在一些问题，主要是由于受限的角色表示。我们提出了Box$^2$EL方法，将概念和角色都表示为盒子（即轴对齐超矩形），并展示了它如何克服之前方法的局限性。我们在理论上证明了我们模型的正确性，并进行了大量的实验评估，在各种数据集上取得了领先的结果。作为我们评估的一部分，我们引入了一个新的基准。

    Description logic (DL) ontologies extend knowledge graphs (KGs) with conceptual information and logical background knowledge. In recent years, there has been growing interest in inductive reasoning techniques for such ontologies, which promise to complement classical deductive reasoning algorithms. Similar to KG completion, several existing approaches learn ontology embeddings in a latent space, while additionally ensuring that they faithfully capture the logical semantics of the underlying DL. However, they suffer from several shortcomings, mainly due to a limiting role representation. We propose Box$^2$EL, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles) and demonstrate how it overcomes the limitations of previous methods. We theoretically prove the soundness of our model and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets. As part of our evaluation, we introduce a novel benchmark for 
    
[^380]: 图神经网络可以仅仅从图结构中恢复出隐藏的特征。

    Graph Neural Networks can Recover the Hidden Features Solely from the Graph Structure. (arXiv:2301.10956v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10956](http://arxiv.org/abs/2301.10956)

    本文研究了图神经网络是否可以利用图结构从而实现对潜在特征的恢复，并表明GNN可以完全利用图结构。

    

    图神经网络(GNN)是处理图学习问题的流行模型，它在许多实际任务中表现出强大的实证性能。但是，其理论性质尚未完全阐明。在本文中，我们从GNN的表达能力角度研究了GNN是否可以利用图结构。在我们的分析中，我们考虑受隐藏（或潜在）节点特征控制的图生成过程，这些特征包含有关图结构的所有信息。这种框架的典型示例是从隐藏特征构建的kNN图。在我们的主要结果中，我们表明GNN可以仅从输入图中恢复出隐藏节点特征，即使所有节点特征，包括隐藏特征本身和任何间接提示都不可用。GNN可以进一步利用恢复的节点特征进行下游任务。这些结果表明，GNN可以完全利用图结构，从而可以使用图结构自身实现。

    Graph Neural Networks (GNNs) are popular models for graph learning problems. GNNs show strong empirical performance in many practical tasks. However, the theoretical properties have not been completely elucidated. In this paper, we investigate whether GNNs can exploit the graph structure from the perspective of the expressive power of GNNs. In our analysis, we consider graph generation processes that are controlled by hidden (or latent) node features, which contain all information about the graph structure. A typical example of this framework is kNN graphs constructed from the hidden features. In our main results, we show that GNNs can recover the hidden node features from the input graph alone, even when all node features, including the hidden features themselves and any indirect hints, are unavailable. GNNs can further use the recovered node features for downstream tasks. These results show that GNNs can fully exploit the graph structure by themselves, and in effect, GNNs can use bot
    
[^381]: 实时触觉质地渲染的基于学习的模型的开发和评估

    Development and Evaluation of a Learning-based Model for Real-time Haptic Texture Rendering. (arXiv:2212.13332v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2212.13332](http://arxiv.org/abs/2212.13332)

    该论文开发了一个基于学习的模型，用于实时触觉质地渲染，在多个用户的研究中评估了其感知性能。这个模型可以推广到各种质地和用户交互的变化，并使用视觉触觉传感器的数据进行实时渲染。

    

    当前的虚拟现实（VR）环境缺乏人类在现实生活中的交互中经历到的丰富触觉信号，例如在表面上的横向移动中感受到的质地感。在VR环境中添加逼真的触觉质地需要一个能够推广到用户交互的变化和世界上各种各样的质地的模型。目前存在用于触觉质地渲染的方法，但通常需要为每种质地开发一个模型，导致可扩展性较低。我们提出了一个基于深度学习的动作条件模型，用于触觉质地渲染，并通过多个用户的感知性能评估来呈现逼真的质地振动。该模型统一适用于所有材料，使用来自基于视觉的触觉传感器（GelSight）的数据，在实时条件下呈现适当的表面。在质地渲染方面，我们使用一个高带宽的振触觉传感器连接到一个3D系统。

    Current Virtual Reality (VR) environments lack the rich haptic signals that humans experience during real-life interactions, such as the sensation of texture during lateral movement on a surface. Adding realistic haptic textures to VR environments requires a model that generalizes to variations of a user's interaction and to the wide variety of existing textures in the world. Current methodologies for haptic texture rendering exist, but they usually develop one model per texture, resulting in low scalability. We present a deep learning-based action-conditional model for haptic texture rendering and evaluate its perceptual performance in rendering realistic texture vibrations through a multi part human user study. This model is unified over all materials and uses data from a vision-based tactile sensor (GelSight) to render the appropriate surface conditioned on the user's action in real time. For rendering texture, we use a high-bandwidth vibrotactile transducer attached to a 3D Systems
    
[^382]: 双重精度质量驱动的神经网络用于生成预测区间

    Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation. (arXiv:2212.06370v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06370](http://arxiv.org/abs/2212.06370)

    本文提出了一种双重精度质量驱动的神经网络，可以自动地学习基于回归的神经网络的预测区间，而非只提供传统的目标估计。该方法通过设计一种新颖的损失函数，最小化平均预测区间宽度以及提高覆盖概率来提高PI的质量和精度，且比最先进的方法更加具有计算效率。

    

    在深度学习模型在实际应用中，准确的不确定性量化对于提高其可靠性至关重要。对于回归任务，应该在深度学习模型的确定性预测之外提供预测区间(PIs)。只要Pis足够窄而且捕获了大部分的概率密度，这些Pis就是有用的或"高质量"的。本文提出了一种方法，可以自动地为回归神经网络学习预测区间，除了传统的目标预测之外。具体而言，我们训练两个伴侣神经网络：一个使用一个输出，目标估计，另一个使用两个输出，相应PI的上限和下限的值。我们的主要贡献是为生成PI的网络设计了一种新颖的损失函数，该函数考虑了目标估计网络的输出，并且具有两个优化目标：减小平均预测区间宽度和提高Pis的质量(通过其覆盖概率进行测量)。我们在几个回归数据集上评估了我们的方法，并证明了我们的方法可以产生比最先进的方法更准确且质量更高的预测区间，同时又具有计算效率。

    Accurate uncertainty quantification is necessary to enhance the reliability of deep learning models in real-world applications. In the case of regression tasks, prediction intervals (PIs) should be provided along with the deterministic predictions of deep learning models. Such PIs are useful or "high-quality" as long as they are sufficiently narrow and capture most of the probability density. In this paper, we present a method to learn prediction intervals for regression-based neural networks automatically in addition to the conventional target predictions. In particular, we train two companion neural networks: one that uses one output, the target estimate, and another that uses two outputs, the upper and lower bounds of the corresponding PI. Our main contribution is the design of a novel loss function for the PI-generation network that takes into account the output of the target-estimation network and has two optimization objectives: minimizing the mean prediction interval width and e
    
[^383]: 非零梯度的随机逼近的指数集中性分析

    Exponential Concentration of Stochastic Approximation with Non-vanishing Gradient. (arXiv:2208.07243v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.07243](http://arxiv.org/abs/2208.07243)

    本研究分析了非零梯度的随机逼近算法的行为，并证明了指数级的集中性界限，这对于投影随机梯度下降算法的收敛速度有重要意义。

    

    我们分析了随机逼近算法的行为，其中每一步迭代，期望中向目标取得进展。当进展与算法的步长成比例时，我们证明了指数级的集中性界限。这些尾部界限与更常见的随机逼近的渐近正态结果形成对比。我们开发的方法依赖于几何阻尼性证明。这扩展了Hajek（1982）对马尔可夫链的结果到随机逼近算法的领域。对于具有非零梯度的投影随机梯度下降算法，我们的结果可以用来证明$O(1/t)$和线性收敛速度。

    We analyze the behavior of stochastic approximation algorithms where iterates, in expectation, make progress towards an objective at each step. When progress is proportional to the step size of the algorithm, we prove exponential concentration bounds. These tail-bounds contrast asymptotic normality results which are more frequently associated with stochastic approximation. The methods that we develop rely on a geometric ergodicity proof. This extends a result on Markov chains due to Hajek (1982) to the area of stochastic approximation algorithms. For Projected Stochastic Gradient Descent with a non-vanishing gradient, our results can be used to prove $O(1/t)$ and linear convergence rates.
    
[^384]: EVOTER：透明可解释规则集的进化

    EVOTER: Evolution of Transparent Explainable Rule-sets. (arXiv:2204.10438v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2204.10438](http://arxiv.org/abs/2204.10438)

    EVOTER使用简单的逻辑表达式演化出透明可解释的规则集，与黑盒模型性能相似，可以揭示数据中的偏见并为未来构建可靠的AI系统提供基础。

    

    大多数AI系统是黑盒子，为给定的输入生成合理的输出。然而，某些领域具有解释能力和信任度要求，这些要求不能直接满足这些方法。因此，该论文提出了一种替代方法，即开始时模型就是透明的和可解释的。该方法使用简单的逻辑表达式演化出规则集，称为EVOTER。EVOTER在多个预测/分类和处方/政策搜索领域进行了评估，有和没有代理。结果显示，它能够发现和黑盒模型相似的有意义的规则集。这些规则可以提供领域的见解，并使数据中隐藏的偏见显性化。也可以直接对它们进行编辑，以消除偏见并添加约束。因此，EVOTER为未来构建值得信赖的AI系统的可靠基础。

    Most AI systems are black boxes generating reasonable outputs for given inputs. Some domains, however, have explainability and trustworthiness requirements that cannot be directly met by these approaches. Various methods have therefore been developed to interpret black-box models after training. This paper advocates an alternative approach where the models are transparent and explainable to begin with. This approach, EVOTER, evolves rule-sets based on simple logical expressions. The approach is evaluated in several prediction/classification and prescription/policy search domains with and without a surrogate. It is shown to discover meaningful rule sets that perform similarly to black-box models. The rules can provide insight into the domain, and make biases hidden in the data explicit. It may also be possible to edit them directly to remove biases and add constraints. EVOTER thus forms a promising foundation for building trustworthy AI systems for real-world applications in the future.
    
[^385]: 基于幂律谱条件下的优化收敛率紧密上界

    Tight Convergence Rate Bounds for Optimization Under Power Law Spectral Conditions. (arXiv:2202.00992v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2202.00992](http://arxiv.org/abs/2202.00992)

    本文提出了一种新的谱条件，用于提供具有幂律优化轨迹的问题的更紧密上界，演示了如何统一获得最优加速方法及其计划和收敛上界。

    

    对于二次问题的优化性能，取决于谱的低能部分。对于大型（有效无限维）问题，这部分谱通常可以通过幂律分布自然表示或近似，导致梯度算法的迭代解表现出幂律收敛率。本文提出了一种新的谱条件，用于提供具有幂律优化轨迹的问题的更紧密上界。我们利用这个条件来建立一张广泛优化算法的上下界完整图像——梯度下降、最陡下降、重球、共轭梯度——并强调了学习率和动量的基本计划。特别的，我们演示了如何统一获得最优加速方法及其计划和收敛上界，对于给定谱形状。此外，我们还提供了对于首个证明。

    Performance of optimization on quadratic problems sensitively depends on the low-lying part of the spectrum. For large (effectively infinite-dimensional) problems, this part of the spectrum can often be naturally represented or approximated by power law distributions, resulting in power law convergence rates for iterative solutions of these problems by gradient-based algorithms. In this paper, we propose a new spectral condition providing tighter upper bounds for problems with power law optimization trajectories. We use this condition to build a complete picture of upper and lower bounds for a wide range of optimization algorithms -- Gradient Descent, Steepest Descent, Heavy Ball, and Conjugate Gradients -- with an emphasis on the underlying schedules of learning rate and momentum. In particular, we demonstrate how an optimally accelerated method, its schedule, and convergence upper bound can be obtained in a unified manner for a given shape of the spectrum. Also, we provide first proo
    
[^386]: 分布式多臂赌博机可以超越集中式上置信界限算法

    Decentralized Multi-Armed Bandits Can Outperform Centralized Upper Confidence Bound Algorithms. (arXiv:2111.10933v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.10933](http://arxiv.org/abs/2111.10933)

    本文研究了一个多智能体网络中的分布式多臂赌博机问题，提出了两种完全分布式的算法，基于经典的UCB算法和KL-UCB算法，实验证明这些算法能达到更好的对数渐近后悔，智能体之间的邻居关系越多，后悔值越好。

    

    本文研究了一个多智能体网络中的分布式多臂赌博机问题。假设N个智能体同时解决了这个问题，他们面对着一组共同的M个臂并共享相同的臂奖励分布。每个智能体只能从邻居处接收信息，智能体之间的邻居关系由一个无向图描述。本文提出了两种完全分布式的多臂赌博机算法，分别基于经典的上置信界限（UCB）算法和最先进的KL-UCB算法。所提出的分布式算法使网络中的每个智能体能够实现比其单一智能体相应算法更好的对数渐近后悔，前提是智能体至少有一个邻居，而且智能体有越多的邻居，后悔值会越好，这意味着整体的和大于其组成部分。

    This paper studies a decentralized multi-armed bandit problem in a multi-agent network. The problem is simultaneously solved by N agents assuming they face a common set of M arms and share the same arms' reward distributions. Each agent can receive information only from its neighbors, where the neighbor relationships among the agents are described by an undirected graph. Two fully decentralized multi-armed bandit algorithms are proposed, respectively based on the classic upper confidence bound (UCB) algorithm and the state-of-the-art KL-UCB algorithm. The proposed decentralized algorithms permit each agent in the network to achieve a better logarithmic asymptotic regret than their single-agent counterparts, provided that the agent has at least one neighbor, and the more neighbors an agent has, the better regret it will have, meaning that the sum is more than its component parts.
    
[^387]: CrossQ: 用于提高深度强化学习样本效率和简洁性的批归一化方法

    CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity. (arXiv:1902.05605v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1902.05605](http://arxiv.org/abs/1902.05605)

    CrossQ是一种轻量级算法，通过巧妙运用批归一化和删除目标网络的方式，提高了深度强化学习的样本效率，减少了计算成本，并且实施简单。

    

    在深度强化学习中，样本效率是一个关键问题。最近的算法，如REDQ和DroQ，通过将批次标准化的更新数据（UTD）比率增加到每个环境样本上的20个梯度更新步骤，改善了样本效率。然而，这样做会带来大幅增加的计算成本。为了减少这种计算负担，我们引入了CrossQ：一种轻量级算法，它巧妙地运用批归一化，并去除了目标网络，以在保持低UTD比率为1的同时超越目前的最新样本效率。值得注意的是，CrossQ不依赖于当前方法中使用的高级偏差缩减方案。CrossQ的贡献有三个方面：（1）最先进的样本效率，（2）与REDQ和DroQ相比大幅减少计算成本，（3）实施简单，仅需要在SAC之上添加几行代码。

    Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce Cross$Q$: a lightweight algorithm that makes careful use of Batch Normalization and removes target networks to surpass the state-of-the-art in sample efficiency while maintaining a low UTD ratio of $1$. Notably, Cross$Q$ does not rely on advanced bias-reduction schemes used in current methods. Cross$Q$'s contributions are thus threefold: (1) state-of-the-art sample efficiency, (2) substantial reduction in computational cost compared to REDQ and DroQ, and (3) ease of implementation, requiring just a few lines of code on top of SAC.
    

