# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models.](http://arxiv.org/abs/2401.01335) | 本文提出了一种名为自我对弱语言模型进行细调（SPIN）的方法，通过模型自我对弈生成训练数据，并从中优化模型策略，从而将弱语言模型转化为强语言模型，无需额外的人类标注数据。 |
| [^2] | [An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction.](http://arxiv.org/abs/2401.01326) | 这篇论文提出了一种新颖的方法，通过将联合实体和关系抽取问题作为条件序列生成问题来解决。该方法使用了基于跨度的图生成方式，并通过指向机制将生成的输出与原始文本对齐。评估结果证明了该方法的有效性，并获得了竞争性的结果。 |
| [^3] | [LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning.](http://arxiv.org/abs/2401.01325) | 本研究提出了一种名为Self-Extend的方法，通过自身扩展现有LLMs的上下文窗口，无需调整，充分利用LLMs处理长上下文的固有能力。 |
| [^4] | [Learning solutions to some toy constrained optimization problems in infinite dimensional Hilbert spaces.](http://arxiv.org/abs/2401.01306) | 本文提出了在无限维希尔伯特空间中运用深度学习实现罚函数法和增广拉格朗日法的约束优化算法，并在玩具问题上进行了测试，证明这两种方法都能够产生不错的近似解。在约束函数本身是函数的情况下，通过拉格朗日乘子更新规则的计算优势，实现了显著的加速。 |
| [^5] | [Integrating Edges into U-Net Models with Explainable Activation Maps for Brain Tumor Segmentation using MR Images.](http://arxiv.org/abs/2401.01303) | 本文提出了一种将边缘整合到U-Net模型中的方法，并使用可解释的激活图对脑肿瘤进行分割。作者通过提取边缘并进行重建，得到了边缘的真实情况，并与脑肿瘤一起作为目标进行研究。 |
| [^6] | [Efficient Sparse Least Absolute Deviation Regression with Differential Privacy.](http://arxiv.org/abs/2401.01294) | 本论文研究了稀疏最小绝对偏差回归问题中的隐私保护学习解决方案，通过开发一种快速的算法，将非光滑损失问题转化为惩罚最小二乘估计问题，并采用三阶段噪声注入来保证差分隐私保护。 |
| [^7] | [A Comprehensive Study of Knowledge Editing for Large Language Models.](http://arxiv.org/abs/2401.01286) | 本研究全面研究了大型语言模型的知识编辑，旨在有效修改模型的行为，同时保持整体性能。 |
| [^8] | [GEqO: ML-Accelerated Semantic Equivalence Detection.](http://arxiv.org/abs/2401.01280) | 这篇论文提出了一种机器学习加速的语义等价检测方法，解决了在大规模分析引擎上检测等价关系的问题。 |
| [^9] | [Learning-based agricultural management in partially observable environments subject to climate variability.](http://arxiv.org/abs/2401.01273) | 本研究引入了一种将深度强化学习与循环神经网络相结合的创新框架，利用Gym-DSSAT模拟器训练智能agent来掌握最佳氮肥管理策略。研究强调了利用序列观测开发更高效氮肥输入策略的优势，并探讨了气候变异对农业管理的影响。 |
| [^10] | [Optimal Rates of Kernel Ridge Regression under Source Condition in Large Dimensions.](http://arxiv.org/abs/2401.01270) | 本研究研究了在大维度下的核脊回归的行为以及其最优选择的正则化参数。结果发现在满足源条件$s>0$时，KRR是最优的选择；而当$s>1$时，KRR不是最优的选择。此外，研究还展示了速率曲线在不同$\gamma$和$s$下的周期性台阶行为和多次下降行为。 |
| [^11] | [$f$-Divergence Based Classification: Beyond the Use of Cross-Entropy.](http://arxiv.org/abs/2401.01268) | 这项研究提出了一种基于$f$散度的分类方法，超越了传统的使用交叉熵的方法。通过提取基于变分表示的$f$散度目标函数，该方法采用贝叶斯视角将分类任务视为最大后验概率问题，并提出了五个采用不同$f$散度的后验概率估计器。此外，通过一种自底向上的方法，还提出了一种新的基于平移对数的$f$散度的目标函数。这项研究在理论上证明了收敛性的属性。 |
| [^12] | [Fairness Certification for Natural Language Processing and Large Language Models.](http://arxiv.org/abs/2401.01262) | 这项研究旨在为自然语言处理领域开发公平性认证方法。通过综述大量文献和专家访谈，我们提出了六个公平性标准，为操作化和测试过程提供了基础。 |
| [^13] | [Do Concept Bottleneck Models Obey Locality?.](http://arxiv.org/abs/2401.01259) | 本文研究了概念瓶颈模型（CBMs）是否能够正确捕捉到概念之间的条件独立程度，通过分析对于概念局部性之外特征的变化如何影响概念的预测。 |
| [^14] | [Towards Model-Free LQR Control over Rate-Limited Channels.](http://arxiv.org/abs/2401.01258) | 这篇论文研究了在速率限制通道上实现模型无关的LQR控制的问题。通过引入自适应量化梯度下降（AQGD）算法，作者证明了在噪声电路中可以实现控制问题的解决。 |
| [^15] | [Contrastive Sequential Interaction Network Learning on Co-Evolving Riemannian Spaces.](http://arxiv.org/abs/2401.01243) | 这篇论文提出了一个基于共同进化黎曼空间的对比顺序交互网络学习模型，用于解决推荐系统中用户和项目节点的区分问题，以及动态表示空间的演化问题。 |
| [^16] | [Encoding Binary Events from Continuous Time Series in Rooted Trees using Contrastive Learning.](http://arxiv.org/abs/2401.01242) | 本研究提出了一种对连续时间序列数据进行二进制事件编码的对比学习方法，具有推断本地网络拓扑结构的潜力。 |
| [^17] | [Graph Elimination Networks.](http://arxiv.org/abs/2401.01233) | 本文提出了图消除网络（GENs），其通过消除邻域传播过程中的冗余来解决图神经网络（GNN）在深层次上性能下降的问题。GENs可以增强节点对远距离邻域的感知，并扩展网络传播的深度。 |
| [^18] | [Motif-aware Riemannian Graph Neural Network with Generative-Contrastive Learning.](http://arxiv.org/abs/2401.01232) | 本论文提出了一种基于主题的黎曼图神经网络模型，通过生成-对比学习在黎曼流形中进行自监督学习，以捕捉多样曲率流形中的主题规律。 |
| [^19] | [Zero-Shot Position Debiasing for Large Language Models.](http://arxiv.org/abs/2401.01218) | 本文提出了一种零样本位置去偏方法（ZOE）来降低大语言模型（LLMs）的位置偏差问题，该方法利用预训练的LLMs的无监督响应进行去偏。实验证实ZOE在多个数据集和任务中均表现出优异的性能。 |
| [^20] | [Whole-examination AI estimation of fetal biometrics from 20-week ultrasound scans.](http://arxiv.org/abs/2401.01201) | 本文提出了一种全新的方法来估计胎儿生物测量，通过从每个超声扫描帧中提取生物测量，并使用贝叶斯方法进行准确估计和异常值拒绝，该方法能够实现与人类相当的性能。 |
| [^21] | [JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial Example.](http://arxiv.org/abs/2401.01199) | JMA是一种通用算法，用于生成几乎最优的定向对抗样本。该算法通过最小化Jacobian引起的马氏距离，考虑了将输入样本的潜在空间表示在给定方向上移动所需的投入。该算法在解决对抗样本问题方面提供了最优解。 |
| [^22] | [Deep-ELA: Deep Exploratory Landscape Analysis with Self-Supervised Pretrained Transformers for Single- and Multi-Objective Continuous Optimization Problems.](http://arxiv.org/abs/2401.01192) | 本文提出了Deep-ELA方法，通过使用自监督预训练的变换器，对单目标和多目标连续优化问题进行深度探索性景观分析，解决了传统ELA特征存在的强相关性和在多目标优化问题中的局限性问题。 |
| [^23] | [Freeze the backbones: A Parameter-Efficient Contrastive Approach to Robust Medical Vision-Language Pre-training.](http://arxiv.org/abs/2401.01179) | 这项研究提出了一种冻结主干的适配器框架，可以实现参数高效的抗干扰医学视觉语言预训练。实验证明，该框架在保留信息的同时大大减少了可训练参数，并在医学图像分类和分割任务上取得了竞争性的性能。 |
| [^24] | [Fundamental Limitation of Semantic Communications: Neural Estimation for Rate-Distortion.](http://arxiv.org/abs/2401.01176) | 本文研究了语义通信在离散记忆通道上的基本限制。通过采用语义速率失真函数（SRDF），我们研究了最小压缩率、观测失真、语义失真和通道容量之间的关系。对于未知语义源分布，我们提出了一种基于神经网络的方法来学习分布。对于已知语义源分布的特殊情况，我们设计了一个级联神经网络来估计SRDF。 |
| [^25] | [Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing Bearing Faults.](http://arxiv.org/abs/2401.01172) | 本文提出了一种融合时间频率分析和深度学习技术的方法，用于在实际条件下诊断带有时间变化速度和不同噪声水平的轴承故障。这种方法有效地解析与不同轴承故障相关的独特动态模式。 |
| [^26] | [FedQV: Leveraging Quadratic Voting in Federated Learning.](http://arxiv.org/abs/2401.01168) | 本文提出了FedQV，这是一个在联邦学习中利用二次投票机制的新型聚合算法，旨在解决现有方法中容易受到污染攻击的问题。理论和实证分析表明，FedQV是一个真实机制，并且具有与现有方法相匹配的收敛速率。 |
| [^27] | [Reinforcement Learning for SAR View Angle Inversion with Differentiable SAR Renderer.](http://arxiv.org/abs/2401.01165) | 本研究提出了一个交互式的深度强化学习框架，利用可微分的SAR渲染器进行SAR视角反演。该框架能够模拟人类的角度预测过程，并在实时生成任意视角的SAR图像的同时，有效抑制了复杂的背景干扰，增强了对时间变化的敏感性。 |
| [^28] | [Train-Free Segmentation in MRI with Cubical Persistent Homology.](http://arxiv.org/abs/2401.01160) | 这是一种使用拓扑数据分析进行MRI图像分割的新方法，相比传统机器学习方法具有优势，无需大量注释数据集，提供更可解释和稳定的分割框架。 |
| [^29] | [Deep Learning-Based Detection for Marker Codes over Insertion and Deletion Channels.](http://arxiv.org/abs/2401.01155) | 本文提出了两种基于深度学习的CSI无关检测算法，用于在插入和删除通道上检测标记码。这些算法可以在没有完美CSI知识的情况下工作，并具有潜在的应用于未来的存储系统。 |
| [^30] | [PAC-Bayes-Chernoff bounds for unbounded losses.](http://arxiv.org/abs/2401.01148) | 这篇论文提出了一种用于无界损失的高概率PAC-Bayes参考界限，并通过优化自由参数解决了一些开放问题，并通过灵活的假设产生了新的广义界限。 |
| [^31] | [HAAQI-Net: A non-intrusive neural music quality assessment model for hearing aids.](http://arxiv.org/abs/2401.01145) | HAAQI-Net是一种适用于助听器用户的非侵入性神经音质评估模型，通过使用BLSTM和注意力机制，以及预训练的BEATs进行声学特征提取，能够快速且准确地预测音乐的HAAQI得分，相比传统方法具有更高的性能和更低的推理时间。 |
| [^32] | [Explainable Adaptive Tree-based Model Selection for Time Series Forecasting.](http://arxiv.org/abs/2401.01124) | 提出了一种用于时间序列预测的在线选择基于树的模型的新方法，采用了TreeSHAP解释性方法进行模型的专门化，以应对基于树的模型在实际决策中的过拟合问题。 |
| [^33] | [Utilizing Autoregressive Networks for Full Lifecycle Data Generation of Rolling Bearings for RUL Prediction.](http://arxiv.org/abs/2401.01119) | 本文介绍了一种利用CVGAN模型生成滚动轴承振动信号的方法，该模型能够根据历史振动数据和剩余寿命条件生成一维振动信号，同时提出了一种自回归生成方法来指导信号的生成。实验证明，CVGAN模型在MMD和FID指标方面优于其他高级方法。 |
| [^34] | [Scalable manifold learning by uniform landmark sampling and constrained locally linear embedding.](http://arxiv.org/abs/2401.01100) | 通过均匀地标抽样和约束局部线性嵌入，提出了一种可伸缩的流形学习方法，可以有效处理大规模和高维数据，并解决全局结构失真和可伸缩性问题。 |
| [^35] | [Efficient Parallel Audio Generation using Group Masked Language Modeling.](http://arxiv.org/abs/2401.01099) | 我们提出了一种有效的并行语音生成方法，通过使用组掩码语言模型和组迭代并行解码，能够快速生成高质量的音频，并成功捕捉提示语音的说话人风格，提高了计算效率。 |
| [^36] | [Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control.](http://arxiv.org/abs/2401.01085) | Imperio是一个使用语言引导的后门攻击工具，可以通过语言指令实现任意模型的控制，扩展了NLP模型的后门攻击能力。 |
| [^37] | [Global Convergence of Natural Policy Gradient with Hessian-aided Momentum Variance Reduction.](http://arxiv.org/abs/2401.01084) | 本文开发了一种新的自然策略梯度变体NPG-HM，采用Hessian辅助动量技术进行方差减小，通过随机梯度下降解决子问题。实验证明NPG-HM在通用Fisher非退化策略参数化下可以实现全局最后迭代的$\epsilon$-最优性，并且在Mujoco环境中表现出卓越的性能。 |
| [^38] | [Aircraft Landing Time Prediction with Deep Learning on Trajectory Images.](http://arxiv.org/abs/2401.01083) | 本研究提出了一种基于轨迹图像的深度学习方法，用于预测飞机的降落时间。通过使用轨迹图像和额外的输入信息，我们能够利用深度卷积神经网络进行准确的预测。 |
| [^39] | [Constrained Online Two-stage Stochastic Optimization: Algorithm with (and without) Predictions.](http://arxiv.org/abs/2401.01077) | 这项研究考虑了在线两阶段随机优化问题，采用对抗性学习算法开发了在线算法。当模型参数来自未知的非平稳分布且给定了机器学习的预测时，提出的算法具有较低的遗憾边界。 |
| [^40] | [Enhancing Automatic Modulation Recognition through Robust Global Feature Extraction.](http://arxiv.org/abs/2401.01056) | 该论文提出了一种名为TLDNN的混合深度框架，将Transformer和LSTM的结构结合，通过全局特征提取和捕捉时域依赖性的增强，为自动调制识别带来了改进。 |
| [^41] | [Elastic Multi-Gradient Descent for Parallel Continual Learning.](http://arxiv.org/abs/2401.01054) | 这是一篇关于并行连续学习的论文，介绍了在动态多任务场景下的挑战和解决方法。通过使用任务特定的弹性因子，可以解决梯度差异和负迁移的问题。 |
| [^42] | [PAC-Bayesian Domain Adaptation Bounds for Multi-view learning.](http://arxiv.org/abs/2401.01048) | 本文提出了一种PAC-Bayesian多视图领域自适应界限的方法，将多视图应用于领域自适应，通过引入一种新型距离，并给出了相应的界限。 |
| [^43] | [Sharp Analysis of Power Iteration for Tensor PCA.](http://arxiv.org/abs/2401.01047) | 本文中，我们对Tensor PCA模型中的功率迭代算法进行了详细分析，超越了之前的限制，并建立了关于收敛次数的尖锐界限和算法阈值。我们还提出了一种有效的停止准则来获得高度相关的解决方案。 |
| [^44] | [CautionSuicide: A Deep Learning Based Approach for Detecting Suicidal Ideation in Real Time Chatbot Conversation.](http://arxiv.org/abs/2401.01023) | 本文提出了一种基于深度学习的模型，用于实时检测聊天机器人对话中的自杀意向，并提供了将此模型集成到聊天机器人支持系统的框架。 |
| [^45] | [Class Relevance Learning For Out-of-distribution Detection.](http://arxiv.org/abs/2401.01021) | 本研究提出了一种针对超出分布检测的创新类别相关学习方法，通过建立一个全面的类别相关学习框架， strategically harnessing interclass relationships within the OOD pipeline，从而显著增强了OOD检测能力。 |
| [^46] | [Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact Detection with Self-Supervised Learning.](http://arxiv.org/abs/2401.01013) | 本研究通过自监督学习提高了Transformer在PPG信号伪迹检测中的鲁棒性和效力，并发现对比学习是最稳定且表现最优的SSL技术。进一步优化对比损失函数对于对比SSL至关重要。 |
| [^47] | [Unsupervised Continual Anomaly Detection with Contrastively-learned Prompt.](http://arxiv.org/abs/2401.01010) | 本论文提出了一种名为UCAD的无监督连续异常检测框架，通过对比学习提示为UAD增加了连续学习能力。UCAD通过利用关键提示知识库和结构对比学习来指导异常检测，解决了遗忘和计算负担问题。 |
| [^48] | [Predicting the activity of chemical compounds based on machine learning approaches.](http://arxiv.org/abs/2401.01004) | 本研究使用机器学习方法探索了预测化合物活性的问题，并在100种不同的技术组合上进行了实验。最终根据一组评估标准选择了最佳的解决方案，并在大数据集上进行了验证。 |
| [^49] | [Machine Learning Classification of Alzheimer's Disease Stages Using Cerebrospinal Fluid Biomarkers Alone.](http://arxiv.org/abs/2401.00981) | 本论文利用脑脊液生物标志物水平，通过机器学习模型对阿尔茨海默病的不同阶段进行分类。研究发现，淀粉样蛋白β1-42、T-τ和P-τ等生物标志物对早期诊断阿尔茨海默病具有潜力。 |
| [^50] | [Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models.](http://arxiv.org/abs/2401.00974) | 本论文研究了欺诈检测模型训练中针对下游任务的生成模型选择问题，并调查了在不同的可解释性和性能约束条件下的最佳实践。研究结果表明，在合成训练欺诈检测模型时，贝叶斯网络（BN）的生成模型优于神经网络（NN）的生成模型。 |
| [^51] | [Facebook Report on Privacy of fNIRS data.](http://arxiv.org/abs/2401.00973) | 本项目旨在开发保护隐私的fNIRS数据的机器学习模型训练技术，并探索集中式和联邦学习方法，以保障多客户之间共享模型的同时保护私有数据。 |
| [^52] | [Robust Meta-Model for Predicting the Need for Blood Transfusion in Non-traumatic ICU Patients.](http://arxiv.org/abs/2401.00972) | 该研究开发了一种鲁棒的元模型，用于预测非创伤ICU患者未来24小时内需要输血的概率。 |
| [^53] | [Improve Fidelity and Utility of Synthetic Credit Card Transaction Time Series from Data-centric Perspective.](http://arxiv.org/abs/2401.00965) | 本文从数据中心的角度提出了五种预处理方案，以增强条件概率自回归模型（CPAR）的训练，从而改善合成信用卡交易数据的保真度和实用性。进一步通过定制欺诈检测模型的训练验证了合成数据的实用性，并为金融领域的合成数据训练提供了宝贵的见解和实践指导。 |
| [^54] | [Data Augmentation Techniques for Cross-Domain WiFi CSI-based Human Activity Recognition.](http://arxiv.org/abs/2401.00964) | 本研究应用基于图像学习的数据增强技术于WiFi CSI，旨在解决人体活动识别中模型泛化能力差的问题。通过跨场景和跨系统的实验，研究了线性视线（LOS）和非线性视线（NLOS）穿墙场景之间以及不同天线系统之间的泛化效果。通过构建基于EfficientNetV2架构的活动识别模型并进行消融研究，评估了不同数据增强技术的效果。 |
| [^55] | [Automated Model Selection for Tabular Data.](http://arxiv.org/abs/2401.00961) | 本文介绍了一种自动化模型选择算法，用于表格数据的预测。该算法考虑了特征之间的交互，并包含了基于优先级的随机网格搜索和贪婪搜索两种不同的特征选择方法。 |
| [^56] | [Learning Long Sequences in Spiking Neural Networks.](http://arxiv.org/abs/2401.00955) | 这项研究首次系统地探索了最新的状态空间模型（SSM）与脉冲神经网络（SNN）在长序列建模方面的结合。结果表明，在经典的长序列建模任务中，基于SSM的SNN能够超越Transformers并且在顺序图像分类中以更少的参数超越当前最先进的SNN。 |
| [^57] | [Families of costs with zero and nonnegative MTW tensor in optimal transport.](http://arxiv.org/abs/2401.00953) | 这篇论文介绍了在最优传送中使用形式为$\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$的费用函数时的零和非负MTW张量的计算方法，并提供了MTW张量在零向量上为零的条件以及相应的线性ODE的简化方法。此外，还给出了逆函数的解析表达式以及一些具体的应用情况。 |
| [^58] | [Unsupervised Graph-based Learning Method for Sub-band Allocation in 6G Subnetworks.](http://arxiv.org/abs/2401.00950) | 本文提出了一种无监督的基于图的学习方法，用于在6G子网络中进行子频带分配。该方法通过优化使用图神经网络的子频带分配，实现了与集中式贪婪着色子频带分配方法相近的性能，并且具有更低的计算时间复杂度和较小的信令开销。 |
| [^59] | [Data Assimilation in Chaotic Systems Using Deep Reinforcement Learning.](http://arxiv.org/abs/2401.00916) | 本文介绍了一种使用强化学习在混沌系统中进行数据同化的新策略。该方法通过使用完整或部分观测的状态变量进行状态校正，旨在最小化观测和预测状态之间的误差。 |
| [^60] | [WoodScape Motion Segmentation for Autonomous Driving -- CVPR 2023 OmniCV Workshop Challenge.](http://arxiv.org/abs/2401.00910) | 运动分割是自动驾驶中重要的任务，传统的卷积神经网络方法在处理摄像机自我运动、鱼眼镜头径向畸变和时间一致性方面效果较差。本论文提出了WoodScape鱼眼运动分割挑战，使用合成数据集和真实数据集来提高机器学习模型性能，这是首个专注于鱼眼运动分割的比赛之一，旨在探索和评估其潜力和影响。 |
| [^61] | [Taming Mode Collapse in Score Distillation for Text-to-3D Generation.](http://arxiv.org/abs/2401.00909) | 本文揭示了现有的基于Score Distillation的文本到3D生成框架存在模式崩溃问题，通过在变分目标中加入熵项来改进生成的3D模型的多样性，解决了Janus伪像问题。 |
| [^62] | [LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models.](http://arxiv.org/abs/2401.00907) | LaFFi是一种用于微调语言模型的替代方法，通过要求模型预测标注者将会给出的反馈，显著提高了在问答任务中的准确性，为应用自然语言反馈提供了一个有前途的方向。 |
| [^63] | [Evaluating the Fairness of the MIMIC-IV Dataset and a Baseline Algorithm: Application to the ICU Length of Stay Prediction.](http://arxiv.org/abs/2401.00902) | 本文使用MIMIC-IV数据集来评估一个XGBoost二分类模型在预测重症监护病房住院时间时的公平性和偏见，结果发现了数据集在人口统计属性上存在类别不平衡，并提出了减少偏见的机器学习技术建议。 |
| [^64] | [Detecting the presence of sperm whales echolocation clicks in noisy environments.](http://arxiv.org/abs/2401.00900) | 该研究使用了稳定的多脉冲结构（MPS）作为折射鲸回声定位点击的检测指标，能够在嘈杂环境中识别和分类点击存在。这种方法能够处理高噪声和低信噪比的情况。 |
| [^65] | [Balanced Multi-modal Federated Learning via Cross-Modal Infiltration.](http://arxiv.org/abs/2401.00894) | 该论文提出了一种通过从全局主导模态进行知识传递的跨模态渗透联邦学习框架，有效解决了分布式环境中的模态不平衡和知识异质性问题。 |
| [^66] | [Attractor reconstruction with reservoir computers: The effect of the reservoir's conditional Lyapunov exponents on faithful attractor reconstruction.](http://arxiv.org/abs/2401.00885) | 该论文研究了储层计算在吸引子重建中的表现，发现驱动式储层的最大条件Lyapunov指数需要比真实系统的最小Lyapunov指数更小，储层的谱半径对吸引子重建起到重要作用。 |
| [^67] | [Automating Leukemia Diagnosis with Autoencoders: A Comparative Study.](http://arxiv.org/abs/2401.00883) | 本研究使用自动编码器开发出有价值的特征，提高了白血病诊断的准确性，并相较于传统的机器学习模型在精确度和F1-score指标上表现更好。 |
| [^68] | [Balanced Graph Structure Information for Brain Disease Detection.](http://arxiv.org/abs/2401.00876) | 这项工作提出了一种名为Bargrain的平衡脑图结构方法，通过同时模拟经过滤波的相关矩阵和最优样本图来改进脑疾病检测性能，并解决了仅依赖单一类型结构的限制。 |
| [^69] | [A Bayesian Unification of Self-Supervised Clustering and Energy-Based Models.](http://arxiv.org/abs/2401.00873) | 该论文研究了用贝叶斯方法统一自监督聚类和能量模型，提出了一种标准化的推导方法，并设计了一个新的可靠地惩罚失败模式的下界。这个下界使得能够训练一个标准的骨架架构，而无需使用非对称元素。 |
| [^70] | [Tensor Networks for Explainable Machine Learning in Cybersecurity.](http://arxiv.org/abs/2401.00867) | 张量网络可以帮助发展可解释的机器学习算法，并提供丰富的模型可解释性。在网络安全中，我们的无监督聚类算法基于矩阵乘积状态，在性能上与传统的深度学习模型相媲美。我们的方法还能提取特征概率、熵和互信息，提供了分类异常的引人入胜的叙述，并实现了前所未有的透明度和可解释性水平。 |
| [^71] | [Federated Multi-View Synthesizing for Metaverse.](http://arxiv.org/abs/2401.00859) | 本论文提出了一种用于元宇宙的联合多视角合成框架，通过三维感知的生成模型和联合学习方法，实现了高效的无线内容传递，满足元宇宙中严格的服务质量要求。 |
| [^72] | [Emissions Reporting Maturity Model: supporting cities to leverage emissions-related processes through performance indicators and artificial intelligence.](http://arxiv.org/abs/2401.00857) | 该论文提出了一个排放报告成熟度模型(ERMM)，通过使用性能指标和人工智能技术，支持城市解决气候变化和全球变暖所带来的挑战。 |
| [^73] | [Harmonizing Covariance and Expressiveness for Deep Hamiltonian Regression in Crystalline Material Research: a Hybrid Cascaded Regression Framework.](http://arxiv.org/abs/2401.00744) | 在深度哈密顿回归中，实现协方差和网络表达能力之间的平衡一直是一个挑战。本文提出了一个混合级联回归框架，在第一阶段通过协变神经网络建模并产生协变特征和基线预测，辅助第二阶段学习协方差。同时，第二阶段使用非线性图形Transformer网络进行结构建模，提高了哈密顿预测的表达能力。 |
| [^74] | [A Survey on Graph Neural Networks in Intelligent Transportation Systems.](http://arxiv.org/abs/2401.00713) | 本研究调查了智能交通系统中图神经网络的应用。图神经网络通过其强大的图建模能力，在交通领域表现出优秀性能。然而，目前研究主要集中在交通预测方面，其他领域仍需更多关注。 |
| [^75] | [A Reliable Knowledge Processing Framework for Combustion Science using Foundation Models.](http://arxiv.org/abs/2401.00544) | 本研究介绍了一种可靠的知识处理框架，将大型语言模型整合到燃烧科学中。该框架通过使用基础模型和RAG框架，处理多样化的燃烧研究数据，最大限度地减少计算和经济开销，同时优化数据隐私和准确性。 |
| [^76] | [Towards Auto-Modeling of Formal Verification for NextG Protocols: A Multimodal cross- and self-attention Large Language Model Approach.](http://arxiv.org/abs/2312.17353) | AVRE是一种用于形式验证NextG通信协议的自动建模系统，利用大型语言模型转化协议描述为依赖图和形式模型，并通过交叉和自注意力机制建立可量化的依赖关系，进而提高了复杂通信协议的验证准确性和相关性。 |
| [^77] | [Improving Intrusion Detection with Domain-Invariant Representation Learning in Latent Space.](http://arxiv.org/abs/2312.17300) | 本研究提出了一种使用多任务学习的两阶段表示学习技术，通过培养潜在空间中的特征，包括本地和跨领域特征，以增强对未知分布领域的泛化效果。此外，通过最小化先验和潜在空间之间的互信息来分离潜在空间，并且在多个网络安全数据集上评估了模型的效能。 |
| [^78] | [EyePreserve: Identity-Preserving Iris Synthesis.](http://arxiv.org/abs/2312.12028) | 本论文提出了一种完全数据驱动的、保持身份的、瞳孔尺寸变化的虹膜图像合成方法，能够合成不同瞳孔尺寸的虹膜图像，代表不存在的身份，并能够在保持身份的同时进行非线性纹理变形。 |
| [^79] | [Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated Robot Response.](http://arxiv.org/abs/2312.11460) | 本论文提出了一种混合内模方法，通过模拟机器人的响应来估计外部状态，这对于健壮的运动控制非常重要。使用对比学习优化嵌入表示，使其接近机器人的后继状态。这种方法只需要机器人的固有感知。 |
| [^80] | [Data-Efficient Multimodal Fusion on a Single GPU.](http://arxiv.org/abs/2312.10144) | 本论文提出了一种在单一GPU上进行数据高效多模态融合的方法，通过使用预训练的单模态编码器的潜在空间，我们在多模态对齐中取得了有竞争力的性能，且计算和数据量减少了数个数量级。 |
| [^81] | [Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming Controllers Inspired by Model Predictive Control.](http://arxiv.org/abs/2312.05332) | 本文提出了一种新的参数化控制器类，利用深度强化学习训练其控制器的参数，从而消除了常见控制器中的可验证性和性能保证的限制。该控制器类似于模型预测控制问题的二次规划求解器，具有可验证的属性，并且在控制性能和鲁棒性方面与其他控制器相媲美。同时，该控制器的计算效率显著优于传统的模型预测控制。 |
| [^82] | [On the Learnability of Watermarks for Language Models.](http://arxiv.org/abs/2312.04469) | 该论文研究了语言模型水印的可学习性，提出了水印蒸馏方法，通过训练学生模型使其模仿使用解码水印的教师模型的行为。结果表明，语言模型具有直接学习生成水印的能力，这对于水印的实际应用具有重要影响。 |
| [^83] | [A Study on the Calibration of In-context Learning.](http://arxiv.org/abs/2312.04021) | 本研究关注上下文学习（ICL），通过定制提示来调整静态语言模型（LMs），研究了在各种自然语言理解和推理任务中性能和校准之间的平衡。研究发现随着ICL示例数量的增加，模型的校准会先增加而后得到改善，而校准误差主要出现在低样本场景下。此外，微调和CoT提示等方法可能导致校准误差和不可靠的自然语言解释，提示需要针对可靠性场景开发新的方法。 |
| [^84] | [OpenVoice: Versatile Instant Voice Cloning.](http://arxiv.org/abs/2312.01479) | OpenVoice是一种多功能的语音克隆方法，可以复制参考讲话者的声音并在多种语言中生成语音。它具有灵活的语音风格控制和零样本跨语言语音克隆的能力。 |
| [^85] | [SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer.](http://arxiv.org/abs/2312.01187) | SASSL提出了一种基于神经风格迁移的增强技术，通过解耦语义和风格属性，在自监督学习中生成多样化的增强样本，从而提升了图像分类性能。 |
| [^86] | [DeepTreeGANv2: Iterative Pooling of Point Clouds.](http://arxiv.org/abs/2312.00042) | 这项工作介绍了DeepTreeGANv2，它是DeepTreeGAN的显著扩展，能够以树状方式迭代地聚合点云，模拟粒子与探测器的相互作用，实现在短时间内生成大型点云的目标。 |
| [^87] | [Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and Qualitative Analysis.](http://arxiv.org/abs/2311.15218) | 本研究提供了一个综合定量和定性分析的实时在线股票预测方法，通过提供一个包含了来自各种来源的数据集，将数字股票数据和定性文本数据结合起来进行分析。数据集包含了多个公司和道琼斯工业平均指数的数据，为训练提供了有效的数据基础。 |
| [^88] | [Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents.](http://arxiv.org/abs/2310.19923) | Jina Embeddings 2是一个能够处理长篇文档的文本嵌入模型，突破了传统512个标记限制，提供了高达8192个标记的容量。 |
| [^89] | [Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis.](http://arxiv.org/abs/2310.10477) | 该论文介绍了一种基于错误分析的对齐策略，通过暴露大型语言模型的错误输出并进行评估，以理解内部原因。通过这种方法，有毒回应可以转化为模型对齐的指导调谐语料，从而提高模型的安全性并训练其进行自我批评。 |
| [^90] | [A Deep Neural Network -- Mechanistic Hybrid Model to Predict Pharmacokinetics in Rat.](http://arxiv.org/abs/2310.09167) | 该论文提出了一种深度神经网络和机制混合模型，用于预测大鼠的药代动力学。通过训练更大的数据集、改进网络架构和参数化机制模型，成功减小了口服和静脉给药的误差，并将这种方法扩展到预测更多终点和处理不同的协变量。 |
| [^91] | [Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes.](http://arxiv.org/abs/2309.17207) | 本研究提出了记忆健身房，一种用于测试利用记忆为基础的深度强化学习智能体能力的基准。它包括部分可观察的二维环境和离散控制，并通过无尽任务对记忆能力、噪声抗性和泛化能力进行评估。研究还提供了一个使用Transformer-XL和Proximal Policy Optimization驱动的实现。 |
| [^92] | [Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections.](http://arxiv.org/abs/2309.16741) | 本文提出了一种通过深度编码器在低维潜空间中存储金融时间序列的多模态数据的框架，以捕捉数据的重要特征。 |
| [^93] | [Collaborative Watermarking for Adversarial Speech Synthesis.](http://arxiv.org/abs/2309.15224) | 本文提出了一种对抗性语音合成的协同水印技术，通过与现有对策模型合作进行训练，实现了对生成语音的有效检测和水印识别。 |
| [^94] | [Era Splitting.](http://arxiv.org/abs/2309.14496) | 本研究提出了两种新的分裂准则，使得决策树模型能够利用时代信息进行优化，从而将超分布泛化研究中的思想应用于决策树模型。 |
| [^95] | [Double Normalizing Flows: Flexible Bayesian Gaussian Process ODEs Learning.](http://arxiv.org/abs/2309.09222) | 这项研究将标准化流引入高斯过程常微分方程(ODE)模型，使其具备更灵活和表达性强的先验分布和非高斯的后验推断，从而提高了贝叶斯高斯过程ODE的准确性和不确定性估计。 |
| [^96] | [Identifying Constitutive Parameters for Complex Hyperelastic Solids using Physics-Informed Neural Networks.](http://arxiv.org/abs/2308.15640) | 本文介绍了一种基于物理信息神经网络（PINNs）的新框架，用于识别软材料在大变形平面应力条件下具有复杂组分行为的材料的组分参数。通过使用多模态的时间相关实验数据训练，我们的模型能够稳健地准确识别不可压缩Arruda-Boyce模型的组分参数。 |
| [^97] | [SyMOT-Flow: Learning optimal transport flow for two arbitrary distributions with maximum mean discrepancy.](http://arxiv.org/abs/2308.13815) | 本文介绍了一个名为SyMOT-Flow的新模型，它通过最小化两个未知分布样本之间的对称最大平均差异来训练可逆转换，并结合最优输运成本作为正则化，将未知分布转换为标准正态分布。实验证明这种转换可以实现更稳定准确的样本生成。 |
| [^98] | [SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling.](http://arxiv.org/abs/2308.04365) | SLEM是一种路径建模技术，通过集成机器学习超级学习者，实现了一致且无偏的因果效应估计，并在处理非线性关系时超过了传统的结构方程模型。 |
| [^99] | [Adaptive learning of density ratios in RKHS.](http://arxiv.org/abs/2307.16164) | 该论文研究在再生核希尔伯特空间中的一类密度比率估计方法，提出了一种自适应学习的参数选择原则，并在有限样本情况下推导出新的误差界。其方法在二次损失的情况下实现了极小化最优误差率。 |
| [^100] | [Risk-optimized Outlier Removal for Robust Point Cloud Classification.](http://arxiv.org/abs/2307.10875) | 提出了一种面向稳健点云分类的风险优化异常值去除方法，利用普通训练的模型消除额外的异常值并恢复数据。方法通过归因分析确定每个点对模型输出的影响，使用条件风险价值优化高风险点的过滤过程。该方法在不需要额外训练的情况下能够产生出色的结果。 |
| [^101] | [Language Models are Bounded Pragmatic Speakers.](http://arxiv.org/abs/2305.17760) | 本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。经过人类反馈的强化学习微调的大型语言模型具有概念上类似于 快与慢思考模型的思维模型，而这种思维模型被归因于人类。此研究凸显了采用认知概率建模方法对语言模型的理解、评估和推进的价值。 |
| [^102] | [Pseudo-Hamiltonian system identification.](http://arxiv.org/abs/2305.06920) | 该论文提出了一种能够在受未知干扰和阻尼影响时学习到内部动态解析项的伪哈密顿系统辨识模型，通过混合模型，即使难以找到扰动解析项，也能够准确地识别出动态，对于其他系统辨识模型无法处理的情况具有重要的应用价值。此外，该论文提出了一种使用四阶对称积分方案的方法，能够提高在噪声数据上的性能表现。 |
| [^103] | [Tensor PCA from basis in tensor space.](http://arxiv.org/abs/2305.02803) | 本文提出了一种张量PCA的数学框架，通过自伴张量算子导出张量空间中的基础以解决以往方法的局限性，实验结果表明了该方法的有效性。 |
| [^104] | [Pseudo-Hamiltonian neural networks for learning partial differential equations.](http://arxiv.org/abs/2304.14374) | 本文介绍了一种新方法伪哈密顿神经网络(PHNN)，可以用于学习偏微分方程。相比基线模型，PHNN表现更为优越，模型可应用于去除或改变外力情况并可分别得到三个不同物理解释的部分。 |
| [^105] | [When Do Graph Neural Networks Help with Node Classification: Investigating the Homophily Principle on Node Distinguishability.](http://arxiv.org/abs/2304.14274) | 同源性原则不一定是影响图神经网络优越性的唯一原因；本文提出Contextual Stochastic Block Model for Homophily (CSBM-H)以深入研究同源性对节点可区分性的影响。 |
| [^106] | [The contextual lasso: Sparse linear models via deep neural networks.](http://arxiv.org/abs/2302.00878) | 本论文提出了一种新的统计估计器——上下文套索，可以通过深度神经网络的方法解决解释性和拟合能力的矛盾问题，实现对可解释特征的稀疏拟合，并且稀疏模式和系数会随着上下文特征的变化而发生变化。 |
| [^107] | [Accelerated First-Order Optimization under Nonlinear Constraints.](http://arxiv.org/abs/2302.00316) | 设计了一种新的加速非线性约束下的一阶优化算法，其优点是避免了在整个可行集上进行优化，而且利用速度来表达约束，使得算法在决策变量数量和约束数量上的复杂度增长适度，适用于机器学习应用。 |
| [^108] | [Scaffold-Based Multi-Objective Drug Candidate Optimization.](http://arxiv.org/abs/2301.07175) | 基于支架的多目标药物候选优化的创新点在于引入了一种以支架为重点的基于图的马尔可夫链蒙特卡洛框架（ScaMARS），通过自我训练和处理更广泛范围的性质来生成具有最优性质的分子。 |
| [^109] | [Autonomous Assessment of Demonstration Sufficiency via Bayesian Inverse Reinforcement Learning.](http://arxiv.org/abs/2211.15542) | 本论文提出了一种基于贝叶斯逆强化学习和风险价值的自主评估方法，使得机器人可以通过计算高置信度边界来确定是否有足够数量的演示。作者定义了两种充分性指标，并在模拟环境中评估了该方法的可行性。 |
| [^110] | [tf.data service: A Case for Disaggregating ML Input Data Processing.](http://arxiv.org/abs/2210.14826) | 本文介绍了一个拆分机器学习输入数据处理的案例。通过构建tf.data服务，可以实现数据预处理的拆分，以提高加速器和主机资源的利用率。 |
| [^111] | [Approximation analysis of CNNs from a feature extraction view.](http://arxiv.org/abs/2210.09041) | 本文通过深度卷积神经网络对线性特征提取进行了近似分析，并展示了深度学习相对于传统线性变换的强大能力。通过创造性的构造，我们有效地实现了线性特征提取，并且探究了深层网络的函数逼近速度。线性特征的多分辨卷积分解在我们的工作中起着核心作用。 |
| [^112] | [Lossy Image Compression with Conditional Diffusion Models.](http://arxiv.org/abs/2209.06950) | 本文提出了一种利用条件扩散模型进行有损图像压缩的优化框架。通过引入额外的内容潜变量以及合成纹理变量，该方法在图像质量评估指标上表现出更强的性能。 |
| [^113] | [Ranking In Generalized Linear Bandits.](http://arxiv.org/abs/2207.00109) | 本文研究了广义线性Bandits中的排名问题，设计了UCB和Thompson Sampling类型算法来解决该问题，并对位置和物品之间的依赖关系进行了建模。研究结果在位置依赖性和排名问题与图论的连接等方面进行了推广。 |
| [^114] | [Estimating and Mitigating the Congestion Effect of Curbside Pick-ups and Drop-offs: A Causal Inference Approach.](http://arxiv.org/abs/2206.02164) | 该论文开发了一种严格的因果推断方法，评估了取货和送货对路边交通的拥堵影响，并提出了一种双重和分离的机器学习方法来量化这种影响。 |
| [^115] | [Efficiently Disentangle Causal Representations.](http://arxiv.org/abs/2201.01942) | 本文提出了一种高效的方法来学习具有因果机制的分离表示，通过估计原始和新分布之间的条件概率差异，并利用模型的泛化能力进行逼近。与现有方法相比，该方法只需要评估模型的泛化能力，而不需要依赖学习者对新分布的适应速度。实验证明该方法在各种任务上更加样本高效且速度更快。 |
| [^116] | [Joint Learning of Linear Time-Invariant Dynamical Systems.](http://arxiv.org/abs/2112.10955) | 本研究探讨了共同估计多个线性时不变系统的转移矩阵的方法，并展示了通过数据汇集可以显著提高估计准确性的重要收益。 |
| [^117] | [Sample-Efficient Safety Assurances using Conformal Prediction.](http://arxiv.org/abs/2109.14082) | 本研究提出了一种使用合规预测技术的框架，结合机器人/环境动力学模拟器来调整预警系统，能够以尽可能少的数据点证明达到可接受的误报率，并在驾驶员预警系统和机器人抓取应用中实证了该框架的有效性。 |

# 详细

[^1]: 自我对弱语言模型进行细调可以将其转化为强语言模型

    Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models. (arXiv:2401.01335v1 [cs.LG])

    [http://arxiv.org/abs/2401.01335](http://arxiv.org/abs/2401.01335)

    本文提出了一种名为自我对弱语言模型进行细调（SPIN）的方法，通过模型自我对弈生成训练数据，并从中优化模型策略，从而将弱语言模型转化为强语言模型，无需额外的人类标注数据。

    

    通过监督细调（SFT）利用人类标注数据的力量对于推进大型语言模型（LLMs）至关重要。本文探讨了在不需要获取额外人类标注数据的情况下，将弱语言模型发展成为强语言模型的可能性。我们提出了一种名为自我对弱语言模型进行细调（SPIN）的新的细调方法，该方法从一个经过监督细调的模型开始。SPIN的核心是自我对弱语言模型的机制，其中弱语言模型通过与自身的实例对弈来提升自己的能力。具体而言，弱语言模型通过生成自己的训练数据来优化自身策略，通过区分自我生成的回应与来自人类标注数据的回应来改进。我们的方法逐步将弱语言模型提升为强大的模型，充分发掘人类标注示范数据在SFT中的潜力。在理论上，我们证明了该方法的训练目标函数的全局最优解是可以达到的。

    Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achiev
    
[^2]: 一种用于联合实体和关系抽取的自回归文本到图框架

    An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction. (arXiv:2401.01326v1 [cs.CL])

    [http://arxiv.org/abs/2401.01326](http://arxiv.org/abs/2401.01326)

    这篇论文提出了一种新颖的方法，通过将联合实体和关系抽取问题作为条件序列生成问题来解决。该方法使用了基于跨度的图生成方式，并通过指向机制将生成的输出与原始文本对齐。评估结果证明了该方法的有效性，并获得了竞争性的结果。

    

    本文提出了一种新颖的方法，将非结构化文本中的联合实体和关系抽取问题作为条件序列生成问题来解决。与传统的生成式信息抽取模型不同，我们的方法是基于跨度的，它生成一个线性化的图，其中节点表示文本跨度，边表示关系三元组。我们的方法采用了一个具有指向机制的转换器编码器-解码器架构，使用一个动态词汇表来表示跨度和关系类型。我们的模型能够通过跨度表示捕捉实体和关系的结构特征和边界，同时通过指向机制将生成的输出与原始文本进行对齐。在基准数据集上的评估验证了我们方法的有效性，展示了竞争性的结果。代码可在https://github.com/urchade/ATG找到。

    In this paper, we propose a novel method for joint entity and relation extraction from unstructured text by framing it as a conditional sequence generation problem. In contrast to conventional generative information extraction models that are left-to-right token-level generators, our approach is \textit{span-based}. It generates a linearized graph where nodes represent text spans and edges represent relation triplets. Our method employs a transformer encoder-decoder architecture with pointing mechanism on a dynamic vocabulary of spans and relation types. Our model can capture the structural characteristics and boundaries of entities and relations through span representations while simultaneously grounding the generated output in the original text thanks to the pointing mechanism. Evaluation on benchmark datasets validates the effectiveness of our approach, demonstrating competitive results. Code is available at https://github.com/urchade/ATG.
    
[^3]: 自扩展LLM:无需调整的LLM上下文窗口。

    LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. (arXiv:2401.01325v1 [cs.CL])

    [http://arxiv.org/abs/2401.01325](http://arxiv.org/abs/2401.01325)

    本研究提出了一种名为Self-Extend的方法，通过自身扩展现有LLMs的上下文窗口，无需调整，充分利用LLMs处理长上下文的固有能力。

    

    本研究揭示了LLM在处理长上下文时的固有能力，而无需进行精调。在训练过程中，训练序列的有限长度可能限制了大型语言模型（LLMs）在推理过程中对长输入序列的应用。在本研究中，我们认为现有的LLMs本身具有处理长上下文的固有能力。基于这一观点，我们建议通过自身扩展LLMs的上下文窗口，以充分利用其固有能力。我们提出了Self-Extend方法来激发LLMs的长上下文处理潜力。基本思想是构建双层注意信息：群组级和邻居级。这两个级别通过原始模型的自注意力计算，这意味着所提方法不需要任何训练。只需修改四行代码，所提方法就可以轻松扩展现有LLMs的上下文窗口，而无需进行任何精调。我们进行了全面的实验证明，结果表明所提方法可以+摘要减掉文章最后一句話

    This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can 
    
[^4]: 在无限维希尔伯特空间中学习一些玩具约束优化问题的解决方案

    Learning solutions to some toy constrained optimization problems in infinite dimensional Hilbert spaces. (arXiv:2401.01306v1 [math.OC])

    [http://arxiv.org/abs/2401.01306](http://arxiv.org/abs/2401.01306)

    本文提出了在无限维希尔伯特空间中运用深度学习实现罚函数法和增广拉格朗日法的约束优化算法，并在玩具问题上进行了测试，证明这两种方法都能够产生不错的近似解。在约束函数本身是函数的情况下，通过拉格朗日乘子更新规则的计算优势，实现了显著的加速。

    

    本文介绍了在无限维希尔伯特空间中两种流行的理论约束优化算法——罚函数法和增广拉格朗日法的深度学习实现。我们在一些源自变分法或物理学的玩具问题上测试了这些算法。我们证明这两种方法都能够产生对测试问题的不错近似，并且在不同误差方面是可比较的。通过利用拉格朗日乘子更新规则在计算上比求解罚函数法中的子问题更简单的普遍情况，我们在输出约束函数本身是一个函数的情况下实现了显著加速。

    In this work we present deep learning implementations of two popular theoretical constrained optimization algorithms in infinite dimensional Hilbert spaces, namely, the penalty and the augmented Lagrangian methods. We test these algorithms on some toy problems originating in either calculus of variations or physics. We demonstrate that both methods are able to produce decent approximations for the test problems and are comparable in terms of different errors. Leveraging the common occurrence of the Lagrange multiplier update rule being computationally less expensive than solving subproblems in the penalty method, we achieve significant speedups in cases when the output of the constraint function is itself a function.
    
[^5]: 将边缘整合到U-Net模型中，并使用可解释的激活图对脑肿瘤分割进行MRI图像操作

    Integrating Edges into U-Net Models with Explainable Activation Maps for Brain Tumor Segmentation using MR Images. (arXiv:2401.01303v1 [eess.IV])

    [http://arxiv.org/abs/2401.01303](http://arxiv.org/abs/2401.01303)

    本文提出了一种将边缘整合到U-Net模型中的方法，并使用可解释的激活图对脑肿瘤进行分割。作者通过提取边缘并进行重建，得到了边缘的真实情况，并与脑肿瘤一起作为目标进行研究。

    

    从磁共振（MR）图像中手动划定肿瘤区域耗时、需要专家，并容易出现人为错误。近年来，深度学习模型已成为脑肿瘤分割的首选方法。U-Net及其变种在医学图像语义分割领域取得了良好结果。然而，U-Net及其变种倾向于过分分割肿瘤区域，可能无法准确划分肿瘤边缘。对于准确诊断、手术精度和治疗规划，肿瘤边缘与肿瘤区域同样重要。在本文中，作者旨在利用类似导数的滤波器从实际情况中提取边缘，然后进行边缘重建，以获得边缘的真实情况，除了脑肿瘤的真实情况。利用这两个真实情况，作者研究了几种U-Net及其变种结构，有些以及没有肿瘤边缘真实情况作为目标与肿瘤真实情况一起研究。

    Manual delineation of tumor regions from magnetic resonance (MR) images is time-consuming, requires an expert, and is prone to human error. In recent years, deep learning models have been the go-to approach for the segmentation of brain tumors. U-Net and its' variants for semantic segmentation of medical images have achieved good results in the literature. However, U-Net and its' variants tend to over-segment tumor regions and may not accurately segment the tumor edges. The edges of the tumor are as important as the tumor regions for accurate diagnosis, surgical precision, and treatment planning. In the proposed work, the authors aim to extract edges from the ground truth using a derivative-like filter followed by edge reconstruction to obtain an edge ground truth in addition to the brain tumor ground truth. Utilizing both ground truths, the author studies several U-Net and its' variant architectures with and without tumor edges ground truth as a target along with the tumor ground trut
    
[^6]: 高效的稀疏最小绝对偏差回归与差分隐私

    Efficient Sparse Least Absolute Deviation Regression with Differential Privacy. (arXiv:2401.01294v1 [stat.ML])

    [http://arxiv.org/abs/2401.01294](http://arxiv.org/abs/2401.01294)

    本论文研究了稀疏最小绝对偏差回归问题中的隐私保护学习解决方案，通过开发一种快速的算法，将非光滑损失问题转化为惩罚最小二乘估计问题，并采用三阶段噪声注入来保证差分隐私保护。

    

    最近几年来，隐私保护的机器学习算法因其在许多科学领域的重要应用而受到越来越多的关注。然而，在文献中，大多数隐私保护算法要求学习目标是强凸且Lipschitz平滑的，这不能涵盖广泛的鲁棒损失函数（例如，分位数/最小绝对损失）。在这项工作中，我们旨在为稀疏鲁棒回归问题开发一种快速的隐私保护学习解决方案。我们的学习损失包括一个鲁棒的最小绝对损失和一个l1稀疏惩罚项。为了在给定的隐私预算下快速解决非光滑损失，我们开发了一种快速鲁棒和隐私保护估计（FRAPPE）算法来进行最小绝对偏差回归。我们的算法通过将稀疏LAD问题重新表述为惩罚最小二乘估计问题，并采用三阶段噪声注入来保证（ε、δ）差分隐私保护。

    In recent years, privacy-preserving machine learning algorithms have attracted increasing attention because of their important applications in many scientific fields. However, in the literature, most privacy-preserving algorithms demand learning objectives to be strongly convex and Lipschitz smooth, which thus cannot cover a wide class of robust loss functions (e.g., quantile/least absolute loss). In this work, we aim to develop a fast privacy-preserving learning solution for a sparse robust regression problem. Our learning loss consists of a robust least absolute loss and an $\ell_1$ sparse penalty term. To fast solve the non-smooth loss under a given privacy budget, we develop a Fast Robust And Privacy-Preserving Estimation (FRAPPE) algorithm for least absolute deviation regression. Our algorithm achieves a fast estimation by reformulating the sparse LAD problem as a penalized least square estimation problem and adopts a three-stage noise injection to guarantee the $(\epsilon,\delta)
    
[^7]: 大型语言模型的知识编辑全面研究

    A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])

    [http://arxiv.org/abs/2401.01286](http://arxiv.org/abs/2401.01286)

    本研究全面研究了大型语言模型的知识编辑，旨在有效修改模型的行为，同时保持整体性能。

    

    大型语言模型(LLM)在理解和生成与人类交流紧密相似的文本方面展现出了非凡的能力。然而，其主要限制在于训练过程中的显著计算需求，这是由于其广泛的参数化造成的。这一挑战在于世界的动态性，需要频繁更新LLM以修正过时的信息或集成新知识，从而确保其持续的相关性。许多应用需要在训练后进行持续的模型调整，以解决缺陷或不良行为。近年来，对于LLM的知识编辑技术的兴趣越来越高，在特定领域内有效地修改LLM的行为，同时保持整体性能在各种输入中的表现。本文首先定义了知识编辑的目标和挑战，然后综述了现有的知识编辑方法和技术，并讨论了其应用和未来发展的方向。

    Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
    
[^8]: GEqO: 机器学习加速的语义等价检测

    GEqO: ML-Accelerated Semantic Equivalence Detection. (arXiv:2401.01280v1 [cs.DB])

    [http://arxiv.org/abs/2401.01280](http://arxiv.org/abs/2401.01280)

    这篇论文提出了一种机器学习加速的语义等价检测方法，解决了在大规模分析引擎上检测等价关系的问题。

    

    大规模的分析引擎已经成为现代数据驱动企业推导业务洞察和推动行动的核心依赖。这些引擎支持大量的分析作业，每天处理大量的数据，工作负载经常被多个作业的重叠计算所淹没。重复使用常见计算对于有效利用集群资源和减少作业执行时间至关重要。检测常见计算是减少这种计算冗余的第一步。然而，在大规模分析引擎上检测等价关系需要高效且可扩展的完全自动化解决方案。此外，为了最大程度地重复使用计算，需要在语义层面上检测等价关系，而不仅仅是在句法层面上（即能够检测外观不同的查询的语义等价性）。不幸的是，现有解决方案无法满足这些要求。

    Large scale analytics engines have become a core dependency for modern data-driven enterprises to derive business insights and drive actions. These engines support a large number of analytic jobs processing huge volumes of data on a daily basis, and workloads are often inundated with overlapping computations across multiple jobs. Reusing common computation is crucial for efficient cluster resource utilization and reducing job execution time. Detecting common computation is the first and key step for reducing this computational redundancy. However, detecting equivalence on large-scale analytics engines requires efficient and scalable solutions that are fully automated. In addition, to maximize computation reuse, equivalence needs to be detected at the semantic level instead of just the syntactic level (i.e., the ability to detect semantic equivalence of seemingly different-looking queries). Unfortunately, existing solutions fall short of satisfying these requirements.  In this paper, we
    
[^9]: 学习基于环境气候变异的部分可观测农业管理

    Learning-based agricultural management in partially observable environments subject to climate variability. (arXiv:2401.01273v1 [cs.LG])

    [http://arxiv.org/abs/2401.01273](http://arxiv.org/abs/2401.01273)

    本研究引入了一种将深度强化学习与循环神经网络相结合的创新框架，利用Gym-DSSAT模拟器训练智能agent来掌握最佳氮肥管理策略。研究强调了利用序列观测开发更高效氮肥输入策略的优势，并探讨了气候变异对农业管理的影响。

    

    农业管理在塑造作物产量、经济可盈利性和环境可持续性方面扮演着重要角色，特别关注施肥策略。然而，当面对极端天气条件（如热浪和干旱）时，传统指导方针的有效性减弱。本研究引入了一种创新框架，将深度强化学习（DRL）与循环神经网络（RNNs）相结合。利用Gym-DSSAT模拟器，我们训练了一个智能agent来掌握最佳氮肥管理。通过在爱荷华州玉米农作物上进行一系列模拟实验，我们比较了部分可观测马尔科夫决策过程（POMDP）模型和马尔科夫决策过程（MDP）模型。我们的研究强调了利用序列观测来开发更高效的氮肥输入策略的优势。此外，我们还探讨了气候的变异性对农业管理的影响。

    Agricultural management, with a particular focus on fertilization strategies, holds a central role in shaping crop yield, economic profitability, and environmental sustainability. While conventional guidelines offer valuable insights, their efficacy diminishes when confronted with extreme weather conditions, such as heatwaves and droughts. In this study, we introduce an innovative framework that integrates Deep Reinforcement Learning (DRL) with Recurrent Neural Networks (RNNs). Leveraging the Gym-DSSAT simulator, we train an intelligent agent to master optimal nitrogen fertilization management. Through a series of simulation experiments conducted on corn crops in Iowa, we compare Partially Observable Markov Decision Process (POMDP) models with Markov Decision Process (MDP) models. Our research underscores the advantages of utilizing sequential observations in developing more efficient nitrogen input policies. Additionally, we explore the impact of climate variability, particularly duri
    
[^10]: 在大维度下的源条件下的核脊回归的最优率

    Optimal Rates of Kernel Ridge Regression under Source Condition in Large Dimensions. (arXiv:2401.01270v1 [cs.LG])

    [http://arxiv.org/abs/2401.01270](http://arxiv.org/abs/2401.01270)

    本研究研究了在大维度下的核脊回归的行为以及其最优选择的正则化参数。结果发现在满足源条件$s>0$时，KRR是最优的选择；而当$s>1$时，KRR不是最优的选择。此外，研究还展示了速率曲线在不同$\gamma$和$s$下的周期性台阶行为和多次下降行为。

    

    在神经网络研究（如神经切向核理论）的启发下，我们对核脊回归（KRR）在大维度下的行为进行了研究，其中样本量$n \asymp d^{\gamma}$，其中$\gamma > 0$。给定与球$\mathbb{S}^{d}$上定义的内积核相关的RKHS $\mathcal{H}$，我们假设真实函数$f_{\rho}^{*} \in [\mathcal{H}]^{s}$，即$\mathcal{H}$的插值空间，其源条件$s>0$。我们首先确定了核脊回归在最优选择的正则化参数$\lambda$下的泛化误差的精确阶数（上界和下界）。然后我们进一步展示了当$0<s\le1$时，KRR是最小化风险的最优选择；当$s>1$时，KRR不是最小化风险的最优选择（也称饱和效应）。我们的结果说明了在$\gamma$的变化下，速率曲线呈现周期性的台阶行为和多次下降行为，并展示了这些曲线如何随$s>0$而变化。

    Motivated by the studies of neural networks (e.g.,the neural tangent kernel theory), we perform a study on the large-dimensional behavior of kernel ridge regression (KRR) where the sample size $n \asymp d^{\gamma}$ for some $\gamma > 0$. Given an RKHS $\mathcal{H}$ associated with an inner product kernel defined on the sphere $\mathbb{S}^{d}$, we suppose that the true function $f_{\rho}^{*} \in [\mathcal{H}]^{s}$, the interpolation space of $\mathcal{H}$ with source condition $s>0$. We first determined the exact order (both upper and lower bound) of the generalization error of kernel ridge regression for the optimally chosen regularization parameter $\lambda$. We then further showed that when $0<s\le1$, KRR is minimax optimal; and when $s>1$, KRR is not minimax optimal (a.k.a. he saturation effect). Our results illustrate that the curves of rate varying along $\gamma$ exhibit the periodic plateau behavior and the multiple descent behavior and show how the curves evolve with $s>0$. Inte
    
[^11]: 基于$f$散度的分类：超越交叉熵的应用

    $f$-Divergence Based Classification: Beyond the Use of Cross-Entropy. (arXiv:2401.01268v1 [cs.LG])

    [http://arxiv.org/abs/2401.01268](http://arxiv.org/abs/2401.01268)

    这项研究提出了一种基于$f$散度的分类方法，超越了传统的使用交叉熵的方法。通过提取基于变分表示的$f$散度目标函数，该方法采用贝叶斯视角将分类任务视为最大后验概率问题，并提出了五个采用不同$f$散度的后验概率估计器。此外，通过一种自底向上的方法，还提出了一种新的基于平移对数的$f$散度的目标函数。这项研究在理论上证明了收敛性的属性。

    

    在深度学习中，分类任务被形式化为通过最小化交叉熵来解决的优化问题。然而，目前在目标函数设计方面的最新进展使得$f$散度度量可以推广分类问题的优化问题。为了实现这个目标，我们采用贝叶斯视角，并将分类任务形式化为一个最大后验概率问题。我们提出了一类基于$f$散度变分表示的目标函数，从中提取了一系列五个后验概率估计器，利用了众所周知的$f$散度。此外，受到改进最先进方法的挑战的驱动力，我们提出了一种自底向上的方法，使我们能够提出一个对应于一种新的被称为平移对数 (SL) 的$f$散度的新目标函数（和后验概率估计器）的公式。首先，我们在理论上证明了收敛性的属性。

    In deep learning, classification tasks are formalized as optimization problems solved via the minimization of the cross-entropy. However, recent advancements in the design of objective functions allow the $f$-divergence measure to generalize the formulation of the optimization problem for classification. With this goal in mind, we adopt a Bayesian perspective and formulate the classification task as a maximum a posteriori probability problem. We propose a class of objective functions based on the variational representation of the $f$-divergence, from which we extract a list of five posterior probability estimators leveraging well-known $f$-divergences. In addition, driven by the challenge of improving the state-of-the-art approach, we propose a bottom-up method that leads us to the formulation of a new objective function (and posterior probability estimator) corresponding to a novel $f$-divergence referred to as shifted log (SL). First, we theoretically prove the convergence property o
    
[^12]: 自然语言处理和大型语言模型公平性认证

    Fairness Certification for Natural Language Processing and Large Language Models. (arXiv:2401.01262v1 [cs.CL])

    [http://arxiv.org/abs/2401.01262](http://arxiv.org/abs/2401.01262)

    这项研究旨在为自然语言处理领域开发公平性认证方法。通过综述大量文献和专家访谈，我们提出了六个公平性标准，为操作化和测试过程提供了基础。

    

    自然语言处理（NLP）在我们的日常生活中扮演着重要角色，特别是由于大型语言模型（LLM）的巨大进展。然而，NLP在招聘等公平关键应用场景中存在许多问题，例如作为专家系统或基于LLM的教育导师。由于NLP基于人类语言，可能会导致潜在的有害偏见渗入NLP系统，产生不公平的结果，歧视少数群体或引发法律问题。因此，开展NLP方法的公平性认证非常重要。我们采用定性研究方法，对算法公平性的大量文献进行了综述，并与该领域的多位专家进行了半结构化的专家访谈。我们系统地提出了NLP的六个公平性标准，并进一步细化为18个子类别。我们的标准为实施和测试过程提供了基础。

    Natural Language Processing (NLP) plays an important role in our daily lives, particularly due to the enormous progress of Large Language Models (LLM). However, NLP has many fairness-critical use cases, e.g., as an expert system in recruitment or as an LLM-based tutor in education. Since NLP is based on human language, potentially harmful biases can diffuse into NLP systems and produce unfair results, discriminate against minorities or generate legal issues. Hence, it is important to develop a fairness certification for NLP approaches. We follow a qualitative research approach towards a fairness certification for NLP. In particular, we have reviewed a large body of literature on algorithmic fairness, and we have conducted semi-structured expert interviews with a wide range of experts from that area. We have systematically devised six fairness criteria for NLP, which can be further refined into 18 sub-categories. Our criteria offer a foundation for operationalizing and testing processes
    
[^13]: 概念瓶颈模型是否遵循局部性？

    Do Concept Bottleneck Models Obey Locality?. (arXiv:2401.01259v1 [cs.LG])

    [http://arxiv.org/abs/2401.01259](http://arxiv.org/abs/2401.01259)

    本文研究了概念瓶颈模型（CBMs）是否能够正确捕捉到概念之间的条件独立程度，通过分析对于概念局部性之外特征的变化如何影响概念的预测。

    

    概念基础学习通过解释其预测结果使用人可理解的概念，改善了深度学习模型的可解释性。在这种范式下训练的深度学习模型严重依赖于神经网络能够学习独立于其他概念的给定概念的存在或不存在。然而，最近的研究强烈暗示这种假设可能在概念瓶颈模型（CBMs）这一典型的基于概念的可解释架构中不能成立。本文中，我们研究了当这些概念既在空间上（通过它们的值完全由固定子集的特征定义）又在语义上（通过它们的值仅与预定义的固定子集的概念相关联）定位时，CBMs是否正确捕捉到概念之间的条件独立程度。为了理解局部性，我们分析了概念之外的特征变化对概念预测的影响。

    Concept-based learning improves a deep learning model's interpretability by explaining its predictions via human-understandable concepts. Deep learning models trained under this paradigm heavily rely on the assumption that neural networks can learn to predict the presence or absence of a given concept independently of other concepts. Recent work, however, strongly suggests that this assumption may fail to hold in Concept Bottleneck Models (CBMs), a quintessential family of concept-based interpretable architectures. In this paper, we investigate whether CBMs correctly capture the degree of conditional independence across concepts when such concepts are localised both spatially, by having their values entirely defined by a fixed subset of features, and semantically, by having their values correlated with only a fixed subset of predefined concepts. To understand locality, we analyse how changes to features outside of a concept's spatial or semantic locality impact concept predictions. Our
    
[^14]: 实现模型无关的通过速率限制通道的LQR控制

    Towards Model-Free LQR Control over Rate-Limited Channels. (arXiv:2401.01258v1 [math.OC])

    [http://arxiv.org/abs/2401.01258](http://arxiv.org/abs/2401.01258)

    这篇论文研究了在速率限制通道上实现模型无关的LQR控制的问题。通过引入自适应量化梯度下降（AQGD）算法，作者证明了在噪声电路中可以实现控制问题的解决。

    

    鉴于模型无关方法在许多问题设置中的控制设计方面取得的成功，自然而然地会问，如果利用实际的通信通道来传输梯度或策略，情况会如何改变。尽管由此产生的问题与网络控制系统中研究的公式有类似之处，但那个领域的丰富文献通常假定系统的模型是已知的。为了在模型无关控制设计和网络控制系统领域之间建立联系，我们提出了一个问题：\textit{是否可以通过速率限制的通道以模型无关的方式解决基本的控制问题-例如线性二次调节器（LQR）问题？}为了回答这个问题，我们研究了一个设置，其中一个工作代理通过一个无噪声信道以有限的位速率传输量化策略梯度（LQR成本）到一个服务器。我们提出了一种名为自适应量化梯度下降（AQGD）的新算法，并证明了

    Given the success of model-free methods for control design in many problem settings, it is natural to ask how things will change if realistic communication channels are utilized for the transmission of gradients or policies. While the resulting problem has analogies with the formulations studied under the rubric of networked control systems, the rich literature in that area has typically assumed that the model of the system is known. As a step towards bridging the fields of model-free control design and networked control systems, we ask: \textit{Is it possible to solve basic control problems - such as the linear quadratic regulator (LQR) problem - in a model-free manner over a rate-limited channel?} Toward answering this question, we study a setting where a worker agent transmits quantized policy gradients (of the LQR cost) to a server over a noiseless channel with a finite bit-rate. We propose a new algorithm titled Adaptively Quantized Gradient Descent (\texttt{AQGD}), and prove that
    
[^15]: 基于共同进化流形空间的对比顺序交互网络学习

    Contrastive Sequential Interaction Network Learning on Co-Evolving Riemannian Spaces. (arXiv:2401.01243v1 [cs.LG])

    [http://arxiv.org/abs/2401.01243](http://arxiv.org/abs/2401.01243)

    这篇论文提出了一个基于共同进化黎曼空间的对比顺序交互网络学习模型，用于解决推荐系统中用户和项目节点的区分问题，以及动态表示空间的演化问题。

    

    顺序交互网络通常用于各种应用，如推荐系统。推测未来的交互行为是非常重要的，之前的研究主要集中在经典的零曲率欧几里得空间中的动态。尽管之前的方法取得了有希望的结果，但仍存在一系列重要的问题：在双分图性质中，将用户和项目节点放置在一个相同的空间中是否适当，无视它们之间的内在差异？在网络动态方面，与固定曲率空间不同，当新的交互不断到来时，表示空间是否会发生演化？在学习范式方面，我们是否可以摆脱昂贵的标签信息获取？为了解决上述问题，我们提出了一个新颖的基于共同进化黎曼空间的对比顺序交互网络学习模型CSINCERE。据我们所知，我们是第一个引入了一对共同进化的流形空间的模型。

    The sequential interaction network usually find itself in a variety of applications, e.g., recommender system. Herein, inferring future interaction is of fundamental importance, and previous efforts are mainly focused on the dynamics in the classic zero-curvature Euclidean space. Despite the promising results achieved by previous methods, a range of significant issues still largely remains open: On the bipartite nature, is it appropriate to place user and item nodes in one identical space regardless of their inherent difference? On the network dynamics, instead of a fixed curvature space, will the representation spaces evolve when new interactions arrive continuously? On the learning paradigm, can we get rid of the label information costly to acquire? To address the aforementioned issues, we propose a novel Contrastive model for Sequential Interaction Network learning on Co-Evolving RiEmannian spaces, CSINCERE. To the best of our knowledge, we are the first to introduce a couple of co-
    
[^16]: 使用对比学习在根树中对连续时间序列的二进制事件进行编码

    Encoding Binary Events from Continuous Time Series in Rooted Trees using Contrastive Learning. (arXiv:2401.01242v1 [cs.LG])

    [http://arxiv.org/abs/2401.01242](http://arxiv.org/abs/2401.01242)

    本研究提出了一种对连续时间序列数据进行二进制事件编码的对比学习方法，具有推断本地网络拓扑结构的潜力。

    

    广域基础设施所有者通常不知道他们的客户在本地网络中是如何连接的，这些网络以根树结构组织。最近的一项研究使用来自树叶（客户）的离散时间序列数据推断本地网络的拓扑结构。在本研究中，我们提出了一种对连续时间序列数据进行二进制事件编码的对比学习方法。作为初步结果，我们展示了我们的方法在学习有价值的编码器方面具有一定潜力。

    Broadband infrastructure owners do not always know how their customers are connected in the local networks, which are structured as rooted trees. A recent study is able to infer the topology of a local network using discrete time series data from the leaves of the tree (customers). In this study we propose a contrastive approach for learning a binary event encoder from continuous time series data. As a preliminary result, we show that our approach has some potential in learning a valuable encoder.
    
[^17]: 图消除网络

    Graph Elimination Networks. (arXiv:2401.01233v1 [cs.LG])

    [http://arxiv.org/abs/2401.01233](http://arxiv.org/abs/2401.01233)

    本文提出了图消除网络（GENs），其通过消除邻域传播过程中的冗余来解决图神经网络（GNN）在深层次上性能下降的问题。GENs可以增强节点对远距离邻域的感知，并扩展网络传播的深度。

    

    图神经网络（GNN）广泛应用于各个领域，但在深层次上表现不佳。现有研究通常将这个问题归因于节点过度平滑，即在多轮传播之后，节点表示变得无法区分。在本文中，我们深入研究了GNN的邻域传播机制，并发现GNN在深层次上性能下降的真正根本原因在于邻域特征传播的无效性。这种传播在每一步传播中导致节点当前表示的指数增长，使得捕捉长距离节点之间的有价值依赖关系变得极具挑战性。为了解决这个问题，我们引入了图消除网络（GENs），它使用一种特定的算法在邻域传播过程中消除冗余。我们证明了GENs可以增强节点对远距离邻域的感知，并扩展网络传播的深度。

    Graph Neural Networks (GNNs) are widely applied across various domains, yet they perform poorly in deep layers. Existing research typically attributes this problem to node over-smoothing, where node representations become indistinguishable after multiple rounds of propagation. In this paper, we delve into the neighborhood propagation mechanism of GNNs and discover that the real root cause of GNNs' performance degradation in deep layers lies in ineffective neighborhood feature propagation. This propagation leads to an exponential growth of a node's current representation at every propagation step, making it extremely challenging to capture valuable dependencies between long-distance nodes. To address this issue, we introduce Graph Elimination Networks (GENs), which employ a specific algorithm to eliminate redundancies during neighborhood propagation. We demonstrate that GENs can enhance nodes' perception of distant neighborhoods and extend the depth of network propagation. Extensive exp
    
[^18]: 具有生成-对比学习的基于主题的黎曼图神经网络

    Motif-aware Riemannian Graph Neural Network with Generative-Contrastive Learning. (arXiv:2401.01232v1 [cs.LG])

    [http://arxiv.org/abs/2401.01232](http://arxiv.org/abs/2401.01232)

    本论文提出了一种基于主题的黎曼图神经网络模型，通过生成-对比学习在黎曼流形中进行自监督学习，以捕捉多样曲率流形中的主题规律。

    

    图是典型的复杂结构的非欧几里得数据。近年来，黎曼图表示学习作为欧几里得方法的一种激动人心的替代方案出现了。然而，黎曼方法仍处于早期阶段：大多数方法只提供了一个曲率（半径），无视结构复杂性，由于指数/对数映射而导致数值不稳定，并且缺乏捕捉主题规律的能力。鉴于上述问题，我们提出了“基于主题的黎曼图表示学习”的问题，寻求一种数值稳定的编码器，在没有标签的情况下捕捉多样曲率流形中的主题规律。为此，我们提出了一种新颖的具有生成-对比学习的MotifRGC模型，以自监督方式在黎曼流形中进行minmax博弈。首先，我们提出了一种新类型的黎曼GCN（D-GCN），通过使用产品层来构建多样曲率流形

    Graphs are typical non-Euclidean data of complex structures. In recent years, Riemannian graph representation learning has emerged as an exciting alternative to Euclidean ones. However, Riemannian methods are still in an early stage: most of them present a single curvature (radius) regardless of structural complexity, suffer from numerical instability due to the exponential/logarithmic map, and lack the ability to capture motif regularity. In light of the issues above, we propose the problem of \emph{Motif-aware Riemannian Graph Representation Learning}, seeking a numerically stable encoder to capture motif regularity in a diverse-curvature manifold without labels. To this end, we present a novel Motif-aware Riemannian model with Generative-Contrastive learning (MotifRGC), which conducts a minmax game in Riemannian manifold in a self-supervised manner. First, we propose a new type of Riemannian GCN (D-GCN), in which we construct a diverse-curvature manifold by a product layer with the 
    
[^19]: 大语言模型的零样本位置去偏方法

    Zero-Shot Position Debiasing for Large Language Models. (arXiv:2401.01218v1 [cs.CL])

    [http://arxiv.org/abs/2401.01218](http://arxiv.org/abs/2401.01218)

    本文提出了一种零样本位置去偏方法（ZOE）来降低大语言模型（LLMs）的位置偏差问题，该方法利用预训练的LLMs的无监督响应进行去偏。实验证实ZOE在多个数据集和任务中均表现出优异的性能。

    

    微调已被证明是改善大语言模型（LLMs）领域性能的有效方法。然而，LLMs可能适应数据集偏见和预测的捷径，导致生成性能差。实验结果显示，LLMs容易表现出位置偏差，即利用位于开头或末尾或输入中特定位置线索的信息。现有的减轻位置偏差的工作需要外部偏差知识或带注释的非偏倚样本，在实际中不太实用。在这项工作中，我们提出了一种零样本位置去偏（ZOE）框架对LLMs进行位置去偏。ZOE利用预训练的LLMs的无监督响应进行去偏，因此不需要任何外部知识或数据集。为了提高无监督响应的质量，我们提出了一种主从对齐（MSA）模块来修剪这些响应。对八个数据集和五个任务的实验表明，ZOE始终优于其他方法。

    Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Experimental result shows that LLMs are prone to exhibit position bias, i.e., leveraging information positioned at the beginning or end, or specific positional cues within the input. Existing works on mitigating position bias require external bias knowledge or annotated non-biased samples, which is unpractical in reality. In this work, we propose a zero-shot position debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages unsupervised responses from pre-trained LLMs for debiasing, thus without any external knowledge or datasets. To improve the quality of unsupervised responses, we propose a master-slave alignment (MSA) module to prune these responses. Experiments on eight datasets and five tasks show that ZOE consistently outperform
    
[^20]: 从20周超声扫描中全面估计胎儿生物测量的AI方法

    Whole-examination AI estimation of fetal biometrics from 20-week ultrasound scans. (arXiv:2401.01201v1 [cs.CV])

    [http://arxiv.org/abs/2401.01201](http://arxiv.org/abs/2401.01201)

    本文提出了一种全新的方法来估计胎儿生物测量，通过从每个超声扫描帧中提取生物测量，并使用贝叶斯方法进行准确估计和异常值拒绝，该方法能够实现与人类相当的性能。

    

    当前的胎儿畸形筛查方法是基于从单个选择的超声图像中获得的生物测量。在本文中，我们引入了一种范式转变，通过聚合整个扫描中自动提取的生物测量，实现了与人类水平相当的性能，且无需操作员干预。我们使用卷积神经网络对超声视频录像的每个帧进行分类。然后，在显示适当解剖结构的每个帧中测量胎儿生物测量。我们使用贝叶斯方法从大量测量值中估计每个生物测量的真实值，并以概率方式拒绝异常值。我们对1457个20周超声扫描的录像进行了回顾性实验，在这些扫描中估计了胎儿生物测量，并将我们的估计与超声医生在扫描过程中的测量进行了比较。我们的方法在估计胎儿生物测量方面达到了人类水平的性能。

    The current approach to fetal anomaly screening is based on biometric measurements derived from individually selected ultrasound images. In this paper, we introduce a paradigm shift that attains human-level performance in biometric measurement by aggregating automatically extracted biometrics from every frame across an entire scan, with no need for operator intervention. We use a convolutional neural network to classify each frame of an ultrasound video recording. We then measure fetal biometrics in every frame where appropriate anatomy is visible. We use a Bayesian method to estimate the true value of each biometric from a large number of measurements and probabilistically reject outliers. We performed a retrospective experiment on 1457 recordings (comprising 48 million frames) of 20-week ultrasound scans, estimated fetal biometrics in those scans and compared our estimates to the measurements sonographers took during the scan. Our method achieves human-level performance in estimating
    
[^21]: JMA:一种快速生成几乎最优定向对抗样本的通用算法

    JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial Example. (arXiv:2401.01199v1 [cs.LG])

    [http://arxiv.org/abs/2401.01199](http://arxiv.org/abs/2401.01199)

    JMA是一种通用算法，用于生成几乎最优的定向对抗样本。该算法通过最小化Jacobian引起的马氏距离，考虑了将输入样本的潜在空间表示在给定方向上移动所需的投入。该算法在解决对抗样本问题方面提供了最优解。

    

    目前为止，大多数用于生成针对深度学习分类器的定向对抗样本的方法都是高度次优的，通常依赖于增加目标类别的可能性，因此隐含地专注于一热编码设置。在本文中，我们提出了一种更加通用的、理论上可靠的定向攻击方法，该方法利用最小化雅可比引起的马氏距离（JMA）项，考虑将输入样本的潜在空间表示在给定方向上移动所需的投入（在输入空间中）。通过利用沃尔夫二重性定理求解最小化问题，将问题简化为解非负最小二乘（NNLS）问题。所提出的算法为Szegedy等人最初引入的对抗样本问题的线性化版本提供了最优解。我们进行的实验证实了所提出的攻击的广泛性。

    Most of the approaches proposed so far to craft targeted adversarial examples against Deep Learning classifiers are highly suboptimal and typically rely on increasing the likelihood of the target class, thus implicitly focusing on one-hot encoding settings. In this paper, we propose a more general, theoretically sound, targeted attack that resorts to the minimization of a Jacobian-induced MAhalanobis distance (JMA) term, taking into account the effort (in the input space) required to move the latent space representation of the input sample in a given direction. The minimization is solved by exploiting the Wolfe duality theorem, reducing the problem to the solution of a Non-Negative Least Square (NNLS) problem. The proposed algorithm provides an optimal solution to a linearized version of the adversarial example problem originally introduced by Szegedy et al. \cite{szegedy2013intriguing}. The experiments we carried out confirm the generality of the proposed attack which is proven to be 
    
[^22]: Deep-ELA: 使用自监督预训练的变换器进行单目标和多目标连续优化问题的深度探索性景观分析

    Deep-ELA: Deep Exploratory Landscape Analysis with Self-Supervised Pretrained Transformers for Single- and Multi-Objective Continuous Optimization Problems. (arXiv:2401.01192v1 [cs.LG])

    [http://arxiv.org/abs/2401.01192](http://arxiv.org/abs/2401.01192)

    本文提出了Deep-ELA方法，通过使用自监督预训练的变换器，对单目标和多目标连续优化问题进行深度探索性景观分析，解决了传统ELA特征存在的强相关性和在多目标优化问题中的局限性问题。

    

    在许多最近的研究中，探索性景观分析（ELA）特征的潜力已经被证明可以对特定的单目标连续优化问题进行数值表征。这些数值特征为连续优化问题的各种机器学习任务提供输入，包括高级属性预测、自动算法选择和自动算法配置等。没有ELA特征，分析和理解单目标连续优化问题的特性将是不可能的。然而，尽管它们的实用性无可争议，ELA特征仍然存在一些缺点。其中包括（1.）多个特征之间的强相关性，以及（2.）其在多目标连续优化问题中的适用性非常有限。为解决这个问题，最近的研究提出了以深度学习为基础的方法作为ELA的替代方案。在这些研究中，例如，点云变换器被提出作为改进方法。

    In many recent works, the potential of Exploratory Landscape Analysis (ELA) features to numerically characterize, in particular, single-objective continuous optimization problems has been demonstrated. These numerical features provide the input for all kinds of machine learning tasks on continuous optimization problems, ranging, i.a., from High-level Property Prediction to Automated Algorithm Selection and Automated Algorithm Configuration. Without ELA features, analyzing and understanding the characteristics of single-objective continuous optimization problems would be impossible.  Yet, despite their undisputed usefulness, ELA features suffer from several drawbacks. These include, in particular, (1.) a strong correlation between multiple features, as well as (2.) its very limited applicability to multi-objective continuous optimization problems. As a remedy, recent works proposed deep learning-based approaches as alternatives to ELA. In these works, e.g., point-cloud transformers were
    
[^23]: 冻结主干：一种参数高效的抗干扰医学视觉语言预训练对比方法

    Freeze the backbones: A Parameter-Efficient Contrastive Approach to Robust Medical Vision-Language Pre-training. (arXiv:2401.01179v1 [cs.CV])

    [http://arxiv.org/abs/2401.01179](http://arxiv.org/abs/2401.01179)

    这项研究提出了一种冻结主干的适配器框架，可以实现参数高效的抗干扰医学视觉语言预训练。实验证明，该框架在保留信息的同时大大减少了可训练参数，并在医学图像分类和分割任务上取得了竞争性的性能。

    

    现代医疗常常在诊断中同时使用放射图像和文字报告，鼓励使用大型预训练模型进行视觉-语言自监督学习(VL-SSL)以学习多功能的医学视觉表征。然而，现有的大多数VL-SSL框架是端到端训练的，这是计算密集型的，并且可能丢失嵌入在预训练编码器中的重要先验信息。为了解决这两个问题，我们引入了无主干适配器框架，通过保持预训练图像和文本编码器的冻结状态，保留医学知识，并使用轻量级适配器模块进行跨模态学习。在三个数据集上进行的医学图像分类和分割任务的实验证明，与当前的预训练方法相比，我们的框架在保留信息的同时，可将可训练参数减少超过90%。值得注意的是，当只使用1%的数据进行微调时，适配器的性能优于几种基于Transformer的方法。

    Modern healthcare often utilises radiographic images alongside textual reports for diagnostics, encouraging the use of Vision-Language Self-Supervised Learning (VL-SSL) with large pre-trained models to learn versatile medical vision representations. However, most existing VL-SSL frameworks are trained end-to-end, which is computation-heavy and can lose vital prior information embedded in pre-trained encoders. To address both issues, we introduce the backbone-agnostic Adaptor framework, which preserves medical knowledge in pre-trained image and text encoders by keeping them frozen, and employs a lightweight Adaptor module for cross-modal learning. Experiments on medical image classification and segmentation tasks across three datasets reveal that our framework delivers competitive performance while cutting trainable parameters by over 90% compared to current pre-training approaches. Notably, when fine-tuned with just 1% of data, Adaptor outperforms several Transformer-based methods trai
    
[^24]: 语义通信的基本限制: 用于速率失真的神经估计

    Fundamental Limitation of Semantic Communications: Neural Estimation for Rate-Distortion. (arXiv:2401.01176v1 [cs.IT])

    [http://arxiv.org/abs/2401.01176](http://arxiv.org/abs/2401.01176)

    本文研究了语义通信在离散记忆通道上的基本限制。通过采用语义速率失真函数（SRDF），我们研究了最小压缩率、观测失真、语义失真和通道容量之间的关系。对于未知语义源分布，我们提出了一种基于神经网络的方法来学习分布。对于已知语义源分布的特殊情况，我们设计了一个级联神经网络来估计SRDF。

    

    本文研究了在离散记忆通道上语义通信的基本限制。我们考虑发送一个包含观测状态和对应的语义状态的语义源的场景，接收端能够恢复这两种状态。为了推导性能限制，我们采用了语义速率失真函数（SRDF）来研究最小压缩率、观测失真、语义失真和通道容量之间的关系。对于未知语义源分布的情况，仅有一组源样本可用时，我们提出了一种基于神经网络的方法，利用生成网络学习语义源分布。此外，对于语义状态是观测的确定函数的特殊情况，我们设计了级联神经网络来估计SRDF。对于完全已知的语义源分布情况，我们提出了一种通用的Blahut-Arimoto算法。

    This paper studies the fundamental limit of semantic communications over the discrete memoryless channel. We consider the scenario to send a semantic source consisting of an observation state and its corresponding semantic state, both of which are recovered at the receiver. To derive the performance limitation, we adopt the semantic rate-distortion function (SRDF) to study the relationship among the minimum compression rate, observation distortion, semantic distortion, and channel capacity. For the case with unknown semantic source distribution, while only a set of the source samples is available, we propose a neural-network-based method by leveraging the generative networks to learn the semantic source distribution. Furthermore, for a special case where the semantic state is a deterministic function of the observation, we design a cascade neural network to estimate the SRDF. For the case with perfectly known semantic source distribution, we propose a general Blahut-Arimoto algorithm t
    
[^25]: 振动信号的二次时间频率分析用于诊断轴承故障

    Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing Bearing Faults. (arXiv:2401.01172v1 [cs.LG])

    [http://arxiv.org/abs/2401.01172](http://arxiv.org/abs/2401.01172)

    本文提出了一种融合时间频率分析和深度学习技术的方法，用于在实际条件下诊断带有时间变化速度和不同噪声水平的轴承故障。这种方法有效地解析与不同轴承故障相关的独特动态模式。

    

    轴承故障的诊断对于降低维修成本和设备停机至关重要。轴承故障是机器振动的主要原因，分析其信号形态可以揭示其健康状况。然而，现有的方法主要针对控制环境进行优化，忽略了实际条件下的时间变化的转速和振动的非平稳性。本文提出了一种时间频率分析和深度学习技术的融合方法，用于在时间变化速度和不同噪声水平下诊断轴承故障。首先，我们制定了轴承故障引起的振动，并讨论了它们的非平稳性与轴承固有和操作参数之间的联系。我们还阐述了二次时间频率分布，并验证了它们解析与不同轴承故障相关的独特动态模式的有效性。基于此，我们设计了一个时间频率卷积神经网络。

    Diagnosis of bearing faults is paramount to reducing maintenance costs and operational breakdowns. Bearing faults are primary contributors to machine vibrations, and analyzing their signal morphology offers insights into their health status. Unfortunately, existing approaches are optimized for controlled environments, neglecting realistic conditions such as time-varying rotational speeds and the vibration's non-stationary nature. This paper presents a fusion of time-frequency analysis and deep learning techniques to diagnose bearing faults under time-varying speeds and varying noise levels. First, we formulate the bearing fault-induced vibrations and discuss the link between their non-stationarity and the bearing's inherent and operational parameters. We also elucidate quadratic time-frequency distributions and validate their effectiveness in resolving distinctive dynamic patterns associated with different bearing faults. Based on this, we design a time-frequency convolutional neural n
    
[^26]: FedQV: 在联邦学习中利用二次投票机制

    FedQV: Leveraging Quadratic Voting in Federated Learning. (arXiv:2401.01168v1 [cs.CR])

    [http://arxiv.org/abs/2401.01168](http://arxiv.org/abs/2401.01168)

    本文提出了FedQV，这是一个在联邦学习中利用二次投票机制的新型聚合算法，旨在解决现有方法中容易受到污染攻击的问题。理论和实证分析表明，FedQV是一个真实机制，并且具有与现有方法相匹配的收敛速率。

    

    联邦学习（FL）允许不同方共同训练一个全局模型，而不需要透露各自的本地标签。FL的关键步骤之一是将本地模型聚合成全局模型，这与公共决策，尤其是选举，有很多相似之处。在这种情况下，FL的一个主要弱点是其易受污染攻击的脆弱性，可以解释为现代聚合规则中“一人一票”原则（即1p1v）的后果。本文提出了FedQV，这是一个基于二次投票机制的新型聚合算法，该机制最近被提出作为1p1v选举的更好替代方案。我们的理论分析表明，FedQV是一个真实机制，根据自己的真实估值进行投标是一个占主导地位的策略，其收敛速率与现有方法相匹配。此外，我们使用多个真实数据集进行的实证分析...

    Federated Learning (FL) permits different parties to collaboratively train a global model without disclosing their respective local labels. A crucial step of FL, that of aggregating local models to produce the global one, shares many similarities with public decision-making, and elections in particular. In that context, a major weakness of FL, namely its vulnerability to poisoning attacks, can be interpreted as a consequence of the one person one vote (henceforth 1p1v) principle underpinning most contemporary aggregation rules. In this paper, we propose FedQV, a novel aggregation algorithm built upon the quadratic voting scheme, recently proposed as a better alternative to 1p1v-based elections. Our theoretical analysis establishes that FedQV is a truthful mechanism in which bidding according to one's true valuation is a dominant strategy that achieves a convergence rate that matches those of state-of-the-art methods. Furthermore, our empirical analysis using multiple real-world dataset
    
[^27]: 用可微分的SAR渲染器进行SAR视角反演的强化学习

    Reinforcement Learning for SAR View Angle Inversion with Differentiable SAR Renderer. (arXiv:2401.01165v1 [cs.LG])

    [http://arxiv.org/abs/2401.01165](http://arxiv.org/abs/2401.01165)

    本研究提出了一个交互式的深度强化学习框架，利用可微分的SAR渲染器进行SAR视角反演。该框架能够模拟人类的角度预测过程，并在实时生成任意视角的SAR图像的同时，有效抑制了复杂的背景干扰，增强了对时间变化的敏感性。

    

    电磁反问题一直是一个研究热点。本研究旨在给定一个目标模型，反转合成孔径雷达（SAR）图像中的雷达视角。然而，SAR数据的稀缺性，以及复杂的背景干扰和成像机制，限制了现有基于学习的方法的应用。为了解决这些挑战，我们提出了一个交互式的深度强化学习（DRL）框架，其中嵌入了一个名为可微分SAR渲染器（DSR）的电磁模拟器，以促进代理与环境之间的交互，模拟人类的角度预测过程。具体而言，DSR能够实时生成任意视角的SAR图像。并且，利用视角对应图像之间的顺序和语义方面的差异构造了DRL中的状态空间，有效抑制了复杂的背景干扰，增强了对时间变化的敏感性。

    The electromagnetic inverse problem has long been a research hotspot. This study aims to reverse radar view angles in synthetic aperture radar (SAR) images given a target model. Nonetheless, the scarcity of SAR data, combined with the intricate background interference and imaging mechanisms, limit the applications of existing learning-based approaches. To address these challenges, we propose an interactive deep reinforcement learning (DRL) framework, where an electromagnetic simulator named differentiable SAR render (DSR) is embedded to facilitate the interaction between the agent and the environment, simulating a human-like process of angle prediction. Specifically, DSR generates SAR images at arbitrary view angles in real-time. And the differences in sequential and semantic aspects between the view angle-corresponding images are leveraged to construct the state space in DRL, which effectively suppress the complex background interference, enhance the sensitivity to temporal variations
    
[^28]: 无需训练的MRI立方持续同调分割方法

    Train-Free Segmentation in MRI with Cubical Persistent Homology. (arXiv:2401.01160v1 [eess.IV])

    [http://arxiv.org/abs/2401.01160](http://arxiv.org/abs/2401.01160)

    这是一种使用拓扑数据分析进行MRI图像分割的新方法，相比传统机器学习方法具有优势，无需大量注释数据集，提供更可解释和稳定的分割框架。

    

    我们描述了一种新的MRI扫描分割方法，使用拓扑数据分析（TDA），相比传统的机器学习方法具有几个优点。它分为三个步骤，首先通过自动阈值确定要分割的整个对象，然后检测一个已知拓扑结构的独特子集，最后推导出分割的各个组成部分。虽然调用了TDA的经典思想，但这样的算法从未与深度学习方法分离提出。为了实现这一点，我们的方法除了考虑图像的同调性外，还考虑了代表性周期的定位，这是在这种情况下似乎从未被利用过的信息。特别是，它提供了无需大量注释数据集进行分割的能力。TDA还通过将拓扑特征明确映射到分割组件来提供更可解释和稳定的分割框架。

    We describe a new general method for segmentation in MRI scans using Topological Data Analysis (TDA), offering several advantages over traditional machine learning approaches. It works in three steps, first identifying the whole object to segment via automatic thresholding, then detecting a distinctive subset whose topology is known in advance, and finally deducing the various components of the segmentation. Although convoking classical ideas of TDA, such an algorithm has never been proposed separately from deep learning methods. To achieve this, our approach takes into account, in addition to the homology of the image, the localization of representative cycles, a piece of information that seems never to have been exploited in this context. In particular, it offers the ability to perform segmentation without the need for large annotated data sets. TDA also provides a more interpretable and stable framework for segmentation by explicitly mapping topological features to segmentation comp
    
[^29]: 基于深度学习的插入删除通道上标记码的检测方法

    Deep Learning-Based Detection for Marker Codes over Insertion and Deletion Channels. (arXiv:2401.01155v1 [cs.IT])

    [http://arxiv.org/abs/2401.01155](http://arxiv.org/abs/2401.01155)

    本文提出了两种基于深度学习的CSI无关检测算法，用于在插入和删除通道上检测标记码。这些算法可以在没有完美CSI知识的情况下工作，并具有潜在的应用于未来的存储系统。

    

    标记码是一种有效的编码方案，可以保护数据免受插入和删除操作的影响。它在未来的存储系统，如DNA存储和赛道记忆中具有潜在的应用。在解码标记码时，需要完美的信道状态信息（CSI），即插入和删除错误的概率，来检测插入和删除错误。然而，有时候很难获得完美的CSI或准确的信道模型。因此，有必要开发一种不需要完美CSI知识的标记码检测算法。在本文中，我们提出了两种基于深度学习的CSI无关检测算法，用于标记码检测。第一种是一种模型驱动的深度学习方法，它深度展开了原始的迭代检测算法。在这种方法中，CSI成为神经网络中的权重，这些权重可以从训练数据中学习得到。第二种是一种数据驱动的方法，它是一个端对端系统，基于深度双向神经网络。

    Marker code is an effective coding scheme to protect data from insertions and deletions. It has potential applications in future storage systems, such as DNA storage and racetrack memory. When decoding marker codes, perfect channel state information (CSI), i.e., insertion and deletion probabilities, are required to detect insertion and deletion errors. Sometimes, the perfect CSI is not easy to obtain or the accurate channel model is unknown. Therefore, it is deserved to develop detecting algorithms for marker code without the knowledge of perfect CSI. In this paper, we propose two CSI-agnostic detecting algorithms for marker code based on deep learning. The first one is a model-driven deep learning method, which deep unfolds the original iterative detecting algorithm of marker code. In this method, CSI become weights in neural networks and these weights can be learned from training data. The second one is a data-driven method which is an end-to-end system based on the deep bidirectiona
    
[^30]: 无界损失的PAC-Bayes-Chernoff界限

    PAC-Bayes-Chernoff bounds for unbounded losses. (arXiv:2401.01148v1 [stat.ML])

    [http://arxiv.org/abs/2401.01148](http://arxiv.org/abs/2401.01148)

    这篇论文提出了一种用于无界损失的高概率PAC-Bayes参考界限，并通过优化自由参数解决了一些开放问题，并通过灵活的假设产生了新的广义界限。

    

    我们提出了一种新的用于无界损失的高概率PAC-Bayes参考界限。这个结果可以理解为Chernoff界限的PAC-Bayes版本。证明技巧依赖于通过Cramér变换对损失进行统一边界的尾部随机变量。我们强调了我们主要结果的两个应用。首先，我们证明了我们的界限解决了许多PAC-Bayes界限上的自由参数优化的开放问题。最后，我们证明了我们的方法允许在损失函数上进行灵活的假设，从而产生了广义了之前的界限，并且可以通过最小化来获得类似Gibbs的后验概率。

    We present a new high-probability PAC-Bayes oracle bound for unbounded losses. This result can be understood as a PAC-Bayes version of the Chernoff bound. The proof technique relies on uniformly bounding the tail of certain random variable based on the Cram\'er transform of the loss. We highlight two applications of our main result. First, we show that our bound solves the open problem of optimizing the free parameter on many PAC-Bayes bounds. Finally, we show that our approach allows working with flexible assumptions on the loss function, resulting in novel bounds that generalize previous ones and can be minimized to obtain Gibbs-like posteriors.
    
[^31]: HAAQI-Net: 一种适用于助听器的非侵入性神经音质评估模型

    HAAQI-Net: A non-intrusive neural music quality assessment model for hearing aids. (arXiv:2401.01145v1 [eess.AS])

    [http://arxiv.org/abs/2401.01145](http://arxiv.org/abs/2401.01145)

    HAAQI-Net是一种适用于助听器用户的非侵入性神经音质评估模型，通过使用BLSTM和注意力机制，以及预训练的BEATs进行声学特征提取，能够快速且准确地预测音乐的HAAQI得分，相比传统方法具有更高的性能和更低的推理时间。

    

    本文介绍了HAAQI-Net，一种针对助听器用户定制的非侵入性深度学习音质评估模型。与传统方法如Hearing Aid Audio Quality Index (HAAQI) 不同，HAAQI-Net采用了带有注意力机制的双向长短期记忆网络(BLSTM)。该模型以评估的音乐样本和听力损失模式作为输入，生成预测的HAAQI得分。模型采用了预训练的来自音频变换器(BEATs)的双向编码器表示进行声学特征提取。通过将预测分数与真实分数进行比较，HAAQI-Net达到了0.9257的长期一致性相关(LCC)，0.9394的斯皮尔曼等级相关系数(SRCC)，和0.0080的均方误差(MSE)。值得注意的是，这种高性能伴随着推理时间的大幅减少：从62.52秒(HAAQI)减少到2.71秒(HAAQI-Net)，为助听器用户提供了高效的音质评估模型。

    This paper introduces HAAQI-Net, a non-intrusive deep learning model for music quality assessment tailored to hearing aid users. In contrast to traditional methods like the Hearing Aid Audio Quality Index (HAAQI), HAAQI-Net utilizes a Bidirectional Long Short-Term Memory (BLSTM) with attention. It takes an assessed music sample and a hearing loss pattern as input, generating a predicted HAAQI score. The model employs the pre-trained Bidirectional Encoder representation from Audio Transformers (BEATs) for acoustic feature extraction. Comparing predicted scores with ground truth, HAAQI-Net achieves a Longitudinal Concordance Correlation (LCC) of 0.9257, Spearman's Rank Correlation Coefficient (SRCC) of 0.9394, and Mean Squared Error (MSE) of 0.0080. Notably, this high performance comes with a substantial reduction in inference time: from 62.52 seconds (by HAAQI) to 2.71 seconds (by HAAQI-Net), serving as an efficient music quality assessment model for hearing aid users.
    
[^32]: 可解释的自适应基于树的模型选择用于时间序列预测

    Explainable Adaptive Tree-based Model Selection for Time Series Forecasting. (arXiv:2401.01124v1 [cs.LG])

    [http://arxiv.org/abs/2401.01124](http://arxiv.org/abs/2401.01124)

    提出了一种用于时间序列预测的在线选择基于树的模型的新方法，采用了TreeSHAP解释性方法进行模型的专门化，以应对基于树的模型在实际决策中的过拟合问题。

    

    基于树的模型已成功应用于各种任务，包括时间序列预测。由于其相对高的可解释性，它们在需求和广泛认可方面越来越受欢迎。然而，许多模型都存在过拟合问题，这限制了它们在实际决策中的应用。在在线预测设置中，这个问题变得更加严重，因为时间序列观测不断增加，而它们所绘制的分布可能随时间而变化。在这个背景下，我们提出了一种新颖的方法，使用TreeSHAP解释性方法在线选择基于树的模型来进行时间序列预测。我们从任意一组不同的基于树的模型开始。然后，我们概述了一个基于性能的排名，并具有一致的设计，使得TreeSHAP能够根据输入时间序列的不同区域来专门化基于树的预测器。在这个框架中，适当的模式...

    Tree-based models have been successfully applied to a wide variety of tasks, including time series forecasting. They are increasingly in demand and widely accepted because of their comparatively high level of interpretability. However, many of them suffer from the overfitting problem, which limits their application in real-world decision-making. This problem becomes even more severe in online-forecasting settings where time series observations are incrementally acquired, and the distributions from which they are drawn may keep changing over time. In this context, we propose a novel method for the online selection of tree-based models using the TreeSHAP explainability method in the task of time series forecasting. We start with an arbitrary set of different tree-based models. Then, we outline a performance-based ranking with a coherent design to make TreeSHAP able to specialize the tree-based forecasters across different regions in the input time series. In this framework, adequate mode
    
[^33]: 利用自回归网络进行滚动轴承全生命周期数据生成用于剩余寿命预测

    Utilizing Autoregressive Networks for Full Lifecycle Data Generation of Rolling Bearings for RUL Prediction. (arXiv:2401.01119v1 [cs.LG])

    [http://arxiv.org/abs/2401.01119](http://arxiv.org/abs/2401.01119)

    本文介绍了一种利用CVGAN模型生成滚动轴承振动信号的方法，该模型能够根据历史振动数据和剩余寿命条件生成一维振动信号，同时提出了一种自回归生成方法来指导信号的生成。实验证明，CVGAN模型在MMD和FID指标方面优于其他高级方法。

    

    在工业生产中，滚动轴承寿命的预测非常重要。然而，高质量的滚动轴承全生命周期数据的稀缺性一直是精确预测的主要制约因素。为了解决这一挑战，本文引入了CVGAN模型，这是一个能够生成水平和垂直方向上的一维振动信号的新框架，其受历史振动数据和剩余寿命的条件约束。此外，我们提出了一种自回归生成方法，可以迭代地利用之前生成的振动信息来指导当前信号的生成。通过在PHM 2012数据集上进行的实验证明了CVGAN模型的有效性。我们的研究结果表明，CVGAN模型在自回归和非自回归生成模式下，在MMD和FID指标方面表现优于许多高级方法。值得注意的是，训练使用了由CVGAN模型生成的全生命周期数据。

    The prediction of rolling bearing lifespan is of significant importance in industrial production. However, the scarcity of high-quality, full lifecycle data has been a major constraint in achieving precise predictions. To address this challenge, this paper introduces the CVGAN model, a novel framework capable of generating one-dimensional vibration signals in both horizontal and vertical directions, conditioned on historical vibration data and remaining useful life. In addition, we propose an autoregressive generation method that can iteratively utilize previously generated vibration information to guide the generation of current signals. The effectiveness of the CVGAN model is validated through experiments conducted on the PHM 2012 dataset. Our findings demonstrate that the CVGAN model, in terms of both MMD and FID metrics, outperforms many advanced methods in both autoregressive and non-autoregressive generation modes. Notably, training using the full lifecycle data generated by the 
    
[^34]: 通过均匀地标抽样和约束局部线性嵌入实现可伸缩的流形学习

    Scalable manifold learning by uniform landmark sampling and constrained locally linear embedding. (arXiv:2401.01100v1 [cs.LG])

    [http://arxiv.org/abs/2401.01100](http://arxiv.org/abs/2401.01100)

    通过均匀地标抽样和约束局部线性嵌入，提出了一种可伸缩的流形学习方法，可以有效处理大规模和高维数据，并解决全局结构失真和可伸缩性问题。

    

    流形学习是机器学习和数据科学中的关键方法，旨在揭示高维空间中复杂非线性流形内在的低维结构。通过利用流形假设，已经开发了各种非线性降维技术来促进可视化、分类、聚类和获得关键洞察。虽然现有的流形学习方法取得了显著的成功，但仍然存在全局结构中的大量失真问题，这阻碍了对底层模式的理解。可伸缩性问题也限制了它们处理大规模数据的适用性。在这里，我们提出了一种可伸缩的流形学习(scML)方法，可以以有效的方式处理大规模和高维数据。它通过寻找一组地标来构建整个数据的低维骨架，然后将非地标引入地标空间中

    As a pivotal approach in machine learning and data science, manifold learning aims to uncover the intrinsic low-dimensional structure within complex nonlinear manifolds in high-dimensional space. By exploiting the manifold hypothesis, various techniques for nonlinear dimension reduction have been developed to facilitate visualization, classification, clustering, and gaining key insights. Although existing manifold learning methods have achieved remarkable successes, they still suffer from extensive distortions incurred in the global structure, which hinders the understanding of underlying patterns. Scalability issues also limit their applicability for handling large-scale data. Here, we propose a scalable manifold learning (scML) method that can manipulate large-scale and high-dimensional data in an efficient manner. It starts by seeking a set of landmarks to construct the low-dimensional skeleton of the entire data and then incorporates the non-landmarks into the landmark space based 
    
[^35]: 有效的并行语音生成方法：使用组掩码语言模型

    Efficient Parallel Audio Generation using Group Masked Language Modeling. (arXiv:2401.01099v1 [eess.AS])

    [http://arxiv.org/abs/2401.01099](http://arxiv.org/abs/2401.01099)

    我们提出了一种有效的并行语音生成方法，通过使用组掩码语言模型和组迭代并行解码，能够快速生成高质量的音频，并成功捕捉提示语音的说话人风格，提高了计算效率。

    

    我们提出了一种快速且高质量的编解码语言模型，用于并行语音生成。虽然SoundStorm是一种先进的并行语音生成模型，相比自回归模型加速了推理速度，但由于迭代采样，它仍然受到推理速度慢的影响。为了解决这个问题，我们提出了组掩码语言模型（G-MLM）和组迭代并行解码（G-IPD），用于有效的并行语音生成。训练和采样方案都能够通过有效建模组间条件依赖来在少数迭代次数内合成高质量的音频。此外，我们的模型采用了基于交叉注意力的架构，以捕捉提示语音的说话人风格并提高计算效率。实验结果表明，我们提出的模型在基于提示的语音生成方面优于基线模型。

    We present a fast and high-quality codec language model for parallel audio generation. While SoundStorm, a state-of-the-art parallel audio generation model, accelerates inference speed compared to autoregressive models, it still suffers from slow inference due to iterative sampling. To resolve this problem, we propose Group-Masked Language Modeling~(G-MLM) and Group Iterative Parallel Decoding~(G-IPD) for efficient parallel audio generation. Both the training and sampling schemes enable the model to synthesize high-quality audio with a small number of iterations by effectively modeling the group-wise conditional dependencies. In addition, our model employs a cross-attention-based architecture to capture the speaker style of the prompt voice and improves computational efficiency. Experimental results demonstrate that our proposed model outperforms the baselines in prompt-based audio generation.
    
[^36]: Imperio: 使用语言引导的后门攻击实现任意模型控制

    Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control. (arXiv:2401.01085v1 [cs.CR])

    [http://arxiv.org/abs/2401.01085](http://arxiv.org/abs/2401.01085)

    Imperio是一个使用语言引导的后门攻击工具，可以通过语言指令实现任意模型的控制，扩展了NLP模型的后门攻击能力。

    

    在transformer架构的革命下，自然语言处理（NLP）受到了前所未有的关注。虽然NLP模型的进展已经引起了对其后门漏洞的广泛研究，但这些进展可能引入新的后门威胁还未被探索。本文提出了Imperio，它利用NLP模型的语言理解能力来丰富后门攻击。Imperio提供了一种新的模型控制体验，使对手通过语言引导的指令可以任意控制受害模型的输出。为此，我们使用语言模型来驱动条件触发生成器，并对其进行了优化，以扩展其对后门指令解释和执行的语言理解能力。我们在三个数据集、五种攻击和九种防御的实验中验证了Imperio的有效性。它可以从文本描述中产生上下文适应的触发器，并控制被攻击模型的输出。

    Revolutionized by the transformer architecture, natural language processing (NLP) has received unprecedented attention. While advancements in NLP models have led to extensive research into their backdoor vulnerabilities, the potential for these advancements to introduce new backdoor threats remains unexplored. This paper proposes Imperio, which harnesses the language understanding capabilities of NLP models to enrich backdoor attacks. Imperio provides a new model control experience. It empowers the adversary to control the victim model with arbitrary output through language-guided instructions. This is achieved using a language model to fuel a conditional trigger generator, with optimizations designed to extend its language understanding capabilities to backdoor instruction interpretation and execution. Our experiments across three datasets, five attacks, and nine defenses confirm Imperio's effectiveness. It can produce contextually adaptive triggers from text descriptions and control 
    
[^37]: 具有Hessian辅助动量方差减小的自然策略梯度全局收敛

    Global Convergence of Natural Policy Gradient with Hessian-aided Momentum Variance Reduction. (arXiv:2401.01084v1 [cs.LG])

    [http://arxiv.org/abs/2401.01084](http://arxiv.org/abs/2401.01084)

    本文开发了一种新的自然策略梯度变体NPG-HM，采用Hessian辅助动量技术进行方差减小，通过随机梯度下降解决子问题。实验证明NPG-HM在通用Fisher非退化策略参数化下可以实现全局最后迭代的$\epsilon$-最优性，并且在Mujoco环境中表现出卓越的性能。

    

    自然策略梯度（NPG）及其变体是强化学习中广泛使用的策略搜索方法。本文在之前的工作的基础上，开发了一种新的NPG变体，命名为NPG-HM，该方法利用了Hessian辅助动量技术进行方差减小，并通过随机梯度下降方法解决子问题。结果表明，NPG-HM可以在样本复杂度为$\mathcal{O}(\epsilon^{-2})$的情况下实现全局最后迭代的$\epsilon$-最优性，这是在通用的Fisher非退化策略参数化下自然策略梯度方法中已知的最佳结果。收敛分析建立在针对NPG的松弛弱梯度优势性质以及处理子问题时的错误分解的兼容函数逼近框架下。此外，基于Mujoco环境的数值实验表明NPG-HM相对于其他最先进的策略方法展现出卓越的性能。

    Natural policy gradient (NPG) and its variants are widely-used policy search methods in reinforcement learning. Inspired by prior work, a new NPG variant coined NPG-HM is developed in this paper, which utilizes the Hessian-aided momentum technique for variance reduction, while the sub-problem is solved via the stochastic gradient descent method. It is shown that NPG-HM can achieve the global last iterate $\epsilon$-optimality with a sample complexity of $\mathcal{O}(\epsilon^{-2})$, which is the best known result for natural policy gradient type methods under the generic Fisher non-degenerate policy parameterizations. The convergence analysis is built upon a relaxed weak gradient dominance property tailored for NPG under the compatible function approximation framework, as well as a neat way to decompose the error when handling the sub-problem. Moreover, numerical experiments on Mujoco-based environments demonstrate the superior performance of NPG-HM over other state-of-the-art policy g
    
[^38]: 基于轨迹图像的深度学习方法用于飞机降落时间预测

    Aircraft Landing Time Prediction with Deep Learning on Trajectory Images. (arXiv:2401.01083v1 [cs.LG])

    [http://arxiv.org/abs/2401.01083](http://arxiv.org/abs/2401.01083)

    本研究提出了一种基于轨迹图像的深度学习方法，用于预测飞机的降落时间。通过使用轨迹图像和额外的输入信息，我们能够利用深度卷积神经网络进行准确的预测。

    

    飞机降落时间（ALT）的预测对于空中交通管理至关重要，特别是对于跑道上的到达飞机排序。本研究提出了一种基于轨迹图像的深度学习方法，用于预测进入研究领域的飞机的ALT。具体而言，临时捕获窗口内的所有空中到达飞机的轨迹被用来生成一张图像，目标飞机的轨迹被标记为红色，背景飞机的轨迹被标记为蓝色。轨迹图像包括飞机的位置、速度、航向、相对距离和到达交通流量等各种信息。这使得我们能够使用先进的深度卷积神经网络对ALT进行建模。我们还使用实时跑道使用情况、轨迹数据以及飞机类型和天气条件等外部信息作为额外的输入。

    Aircraft landing time (ALT) prediction is crucial for air traffic management, especially for arrival aircraft sequencing on the runway. In this study, a trajectory image-based deep learning method is proposed to predict ALTs for the aircraft entering the research airspace that covers the Terminal Maneuvering Area (TMA). Specifically, the trajectories of all airborne arrival aircraft within the temporal capture window are used to generate an image with the target aircraft trajectory labeled as red and all background aircraft trajectory labeled as blue. The trajectory images contain various information, including the aircraft position, speed, heading, relative distances, and arrival traffic flows. It enables us to use state-of-the-art deep convolution neural networks for ALT modeling. We also use real-time runway usage obtained from the trajectory data and the external information such as aircraft types and weather conditions as additional inputs. Moreover, a convolution neural network (
    
[^39]: 受约束的在线两阶段随机优化：具有（和不具有）预测的算法

    Constrained Online Two-stage Stochastic Optimization: Algorithm with (and without) Predictions. (arXiv:2401.01077v1 [cs.LG])

    [http://arxiv.org/abs/2401.01077](http://arxiv.org/abs/2401.01077)

    这项研究考虑了在线两阶段随机优化问题，采用对抗性学习算法开发了在线算法。当模型参数来自未知的非平稳分布且给定了机器学习的预测时，提出的算法具有较低的遗憾边界。

    

    我们考虑了在有限时间段的T个期间内具有长期约束的在线两阶段随机优化问题。在每个期间，我们首先采取第一阶段的行动，然后观察模型参数的实现，并在可行集上采取第二阶段的行动，可行集同时依赖于第一阶段决策和模型参数。我们的目标是最小化累积目标值，同时保证长期平均的第二阶段决策属于一个集合。我们从对抗性学习算法中开发了在线算法来解决在线两阶段问题。此外，我们算法的遗憾边界可以转化为嵌入对抗性学习算法的遗憾边界。基于此框架，我们在不同的设置下获得了新的结果。当模型参数来自未知的非平稳分布且给定了对分布进行机器学习的预测时，我们从我们的框架中开发了一种新的算法，其遗憾边界为O（WT+√T），其中WT为...

    We consider an online two-stage stochastic optimization with long-term constraints over a finite horizon of $T$ periods. At each period, we take the first-stage action, observe a model parameter realization and then take the second-stage action from a feasible set that depends both on the first-stage decision and the model parameter. We aim to minimize the cumulative objective value while guaranteeing that the long-term average second-stage decision belongs to a set. We develop online algorithms for the online two-stage problem from adversarial learning algorithms. Also, the regret bound of our algorithm can be reduced to the regret bound of embedded adversarial learning algorithms. Based on this framework, we obtain new results under various settings. When the model parameters are drawn from unknown non-stationary distributions and we are given machine-learned predictions of the distributions, we develop a new algorithm from our framework with a regret $O(W_T+\sqrt{T})$, where $W_T$ m
    
[^40]: 通过鲁棒的全局特征提取增强自动调制识别

    Enhancing Automatic Modulation Recognition through Robust Global Feature Extraction. (arXiv:2401.01056v1 [eess.SP])

    [http://arxiv.org/abs/2401.01056](http://arxiv.org/abs/2401.01056)

    该论文提出了一种名为TLDNN的混合深度框架，将Transformer和LSTM的结构结合，通过全局特征提取和捕捉时域依赖性的增强，为自动调制识别带来了改进。

    

    自动调制识别在无线通信系统中起着至关重要的作用。近年来，深度学习自动调制识别策略取得了巨大的成功。调制信号展示了长期的时域依赖性，提取全局特征对于识别调制方案至关重要。传统上，人工专家分析星座图中的模式来分类调制方案。由于有限的感受野，传统的卷积网络擅长提取局部特征，但难以捕捉全局关系。为了解决这个问题，我们引入了一种名为TLDNN的新型混合深度框架，它将Transformer和长短期记忆（LSTM）的结构结合在一起。我们利用Transformer的自注意机制模拟信号序列中的全局关联，同时利用LSTM增强时域依赖性的捕捉。为了减轻射频指纹特征和信道特性等因素的影响。

    Automatic Modulation Recognition (AMR) plays a crucial role in wireless communication systems. Deep learning AMR strategies have achieved tremendous success in recent years. Modulated signals exhibit long temporal dependencies, and extracting global features is crucial in identifying modulation schemes. Traditionally, human experts analyze patterns in constellation diagrams to classify modulation schemes. Classical convolutional-based networks, due to their limited receptive fields, excel at extracting local features but struggle to capture global relationships. To address this limitation, we introduce a novel hybrid deep framework named TLDNN, which incorporates the architectures of the transformer and long short-term memory (LSTM). We utilize the self-attention mechanism of the transformer to model the global correlations in signal sequences while employing LSTM to enhance the capture of temporal dependencies. To mitigate the impact like RF fingerprint features and channel characteri
    
[^41]: 弹性多梯度下降用于并行连续学习

    Elastic Multi-Gradient Descent for Parallel Continual Learning. (arXiv:2401.01054v1 [cs.LG])

    [http://arxiv.org/abs/2401.01054](http://arxiv.org/abs/2401.01054)

    这是一篇关于并行连续学习的论文，介绍了在动态多任务场景下的挑战和解决方法。通过使用任务特定的弹性因子，可以解决梯度差异和负迁移的问题。

    

    连续学习（CL）的目标是从新的数据流中持续学习并完成相应的任务。过去研究的CL假设数据按任务的顺序给出，因此属于串行连续学习（SCL）。本文研究了动态多任务场景下的新兴范式——并行连续学习（PCL），其中在不同时间点遇到了多样的任务。PCL面临的挑战是训练数量不确定且学习进度不同的任务，导致很难保证所有遇到的任务都能得到有效的模型更新。在我们之前的会议论文中，我们主要研究了在多目标优化问题中测量和减小梯度之间的差异，然而，每次模型更新仍然可能存在负迁移。为了解决这个问题，在动态多目标优化问题中，我们引入了任务特定的弹性因子。

    The goal of Continual Learning (CL) is to continuously learn from new data streams and accomplish the corresponding tasks. Previously studied CL assumes that data are given in sequence nose-to-tail for different tasks, thus indeed belonging to Serial Continual Learning (SCL). This paper studies the novel paradigm of Parallel Continual Learning (PCL) in dynamic multi-task scenarios, where a diverse set of tasks is encountered at different time points. PCL presents challenges due to the training of an unspecified number of tasks with varying learning progress, leading to the difficulty of guaranteeing effective model updates for all encountered tasks. In our previous conference work, we focused on measuring and reducing the discrepancy among gradients in a multi-objective optimization problem, which, however, may still contain negative transfers in every model update. To address this issue, in the dynamic multi-objective optimization problem, we introduce task-specific elastic factors to
    
[^42]: PAC-Bayesian多视图学习领域自适应界限

    PAC-Bayesian Domain Adaptation Bounds for Multi-view learning. (arXiv:2401.01048v1 [cs.LG])

    [http://arxiv.org/abs/2401.01048](http://arxiv.org/abs/2401.01048)

    本文提出了一种PAC-Bayesian多视图领域自适应界限的方法，将多视图应用于领域自适应，通过引入一种新型距离，并给出了相应的界限。

    

    本文在多视图学习设置中提出了一系列关于领域自适应的新结果。在以往的研究中，很少关注将多个视图纳入领域自适应的方法。因此，我们提出了一种基于Pac-Bayesian理论的泛化界限分析，以整合目前分开处理的两种范式。首先，我们根据Germain等人的以往研究，将其提出的分布之间的距离用于多视图学习的领域自适应，从而引入了一种适用于多视图领域自适应设置的新型距离。然后，我们给出了用于估计引入的差异的Pac-Bayesian界限。最后，我们将不同的新界限与以前的研究进行了比较。

    This paper presents a series of new results for domain adaptation in the multi-view learning setting. The incorporation of multiple views in the domain adaptation was paid little attention in the previous studies. In this way, we propose an analysis of generalization bounds with Pac-Bayesian theory to consolidate the two paradigms, which are currently treated separately. Firstly, building on previous work by Germain et al., we adapt the distance between distribution proposed by Germain et al. for domain adaptation with the concept of multi-view learning. Thus, we introduce a novel distance that is tailored for the multi-view domain adaptation setting. Then, we give Pac-Bayesian bounds for estimating the introduced divergence. Finally, we compare the different new bounds with the previous studies.
    
[^43]: Tensor PCA的功率迭代的尖锐分析

    Sharp Analysis of Power Iteration for Tensor PCA. (arXiv:2401.01047v1 [cs.LG])

    [http://arxiv.org/abs/2401.01047](http://arxiv.org/abs/2401.01047)

    本文中，我们对Tensor PCA模型中的功率迭代算法进行了详细分析，超越了之前的限制，并建立了关于收敛次数的尖锐界限和算法阈值。我们还提出了一种有效的停止准则来获得高度相关的解决方案。

    

    我们调查了Richard和Montanari（2014）引入的Tensor PCA模型的功率迭代算法。之前研究Tensor功率迭代算法的工作要么仅限于固定次数的迭代，要么需要一个非平凡的与数据无关的初始化。在本文中，我们超越了这些限制，并对随机初始化的Tensor功率迭代的动态进行了多项式数量级的分析。我们的贡献有三个方面：首先，我们建立了对于广泛的信噪比范围下，功率迭代收敛到种植信号所需迭代次数的尖锐界限。其次，我们的分析揭示了实际的算法阈值比文献中猜测的要小一个polylog(n)的因子，其中n是环境维度。最后，我们提出了一种简单而有效的功率迭代停止准则，可以保证输出与真实信号高度相关的解决方案。

    We investigate the power iteration algorithm for the tensor PCA model introduced in Richard and Montanari (2014). Previous work studying the properties of tensor power iteration is either limited to a constant number of iterations, or requires a non-trivial data-independent initialization. In this paper, we move beyond these limitations and analyze the dynamics of randomly initialized tensor power iteration up to polynomially many steps. Our contributions are threefold: First, we establish sharp bounds on the number of iterations required for power method to converge to the planted signal, for a broad range of the signal-to-noise ratios. Second, our analysis reveals that the actual algorithmic threshold for power iteration is smaller than the one conjectured in literature by a polylog(n) factor, where n is the ambient dimension. Finally, we propose a simple and effective stopping criterion for power iteration, which provably outputs a solution that is highly correlated with the true si
    
[^44]: CautionSuicide: 一种基于深度学习的方法用于实时检测聊天机器人对话中的自杀意向

    CautionSuicide: A Deep Learning Based Approach for Detecting Suicidal Ideation in Real Time Chatbot Conversation. (arXiv:2401.01023v1 [cs.HC])

    [http://arxiv.org/abs/2401.01023](http://arxiv.org/abs/2401.01023)

    本文提出了一种基于深度学习的模型，用于实时检测聊天机器人对话中的自杀意向，并提供了将此模型集成到聊天机器人支持系统的框架。

    

    自杀被认为是现代社会最严重的问题之一。自杀造成了对国家、社区和家庭的悲剧性影响。有许多因素导致自杀意向。早期发现自杀意向可以通过为受害者提供所需的专业支持来防止自杀发生，尤其是当受害者没有意识到自杀意向的危险时。随着科技的使用增加，人们通过社交媒体、聊天机器人和其他数字平台在数字内容中分享和表达自己的意向。在本文中，我们提出了一种新颖而简单的基于深度学习的模型，用于检测数字内容中的自杀意向，主要关注聊天机器人作为主要数据源。此外，我们提供了一个框架，将所提出的自杀检测集成到基于聊天机器人的支持系统中。

    Suicide is recognized as one of the most serious concerns in the modern society. Suicide causes tragedy that affects countries, communities, and families. There are many factors that lead to suicidal ideations. Early detection of suicidal ideations can help to prevent suicide occurrence by providing the victim with the required professional support, especially when the victim does not recognize the danger of having suicidal ideations. As technology usage has increased, people share and express their ideations digitally via social media, chatbots, and other digital platforms. In this paper, we proposed a novel, simple deep learning-based model to detect suicidal ideations in digital content, mainly focusing on chatbots as the primary data source. In addition, we provide a framework that employs the proposed suicide detection integration with a chatbot-based support system.
    
[^45]: 超出分布检测的类别相关学习

    Class Relevance Learning For Out-of-distribution Detection. (arXiv:2401.01021v1 [cs.CV])

    [http://arxiv.org/abs/2401.01021](http://arxiv.org/abs/2401.01021)

    本研究提出了一种针对超出分布检测的创新类别相关学习方法，通过建立一个全面的类别相关学习框架， strategically harnessing interclass relationships within the OOD pipeline，从而显著增强了OOD检测能力。

    

    图像分类在各种应用中起着关键作用，然而，在实际场景中部署模型时仍面临挑战。特别是，这些模型在检测未在分类器训练中纳入的陌生类别时会出现问题，这是安全和有效的现实世界模型部署中的一大障碍，通常被称为超出分布（OOD）检测。虽然现有的技术，如max logits，旨在利用logits进行OOD识别，但它们通常忽视了有效检测的复杂类别间关系。本文提出了一种针对OOD检测量身定制的创新类别相关学习方法。我们的方法建立了一个全面的类别相关学习框架，在OOD管道内战略地利用类间关系。这个框架显著增强了OOD检测能力。在包括通用图像分类数据集（Near OOD和Far OOD）的各种数据集上进行了大量实验。

    Image classification plays a pivotal role across diverse applications, yet challenges persist when models are deployed in real-world scenarios. Notably, these models falter in detecting unfamiliar classes that were not incorporated during classifier training, a formidable hurdle for safe and effective real-world model deployment, commonly known as out-of-distribution (OOD) detection. While existing techniques, like max logits, aim to leverage logits for OOD identification, they often disregard the intricate interclass relationships that underlie effective detection. This paper presents an innovative class relevance learning method tailored for OOD detection. Our method establishes a comprehensive class relevance learning framework, strategically harnessing interclass relationships within the OOD pipeline. This framework significantly augments OOD detection capabilities. Extensive experimentation on diverse datasets, encompassing generic image classification datasets (Near OOD and Far O
    
[^46]: 基于自监督学习的PPG信号伪迹检测中提高Transformer的鲁棒性和效力

    Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact Detection with Self-Supervised Learning. (arXiv:2401.01013v1 [cs.LG])

    [http://arxiv.org/abs/2401.01013](http://arxiv.org/abs/2401.01013)

    本研究通过自监督学习提高了Transformer在PPG信号伪迹检测中的鲁棒性和效力，并发现对比学习是最稳定且表现最优的SSL技术。进一步优化对比损失函数对于对比SSL至关重要。

    

    最近在圣杰斯坦医学院的儿科重症监护室的研究表明，传统的机器学习方法，在有限数据情况下，如半监督标签传播和K最近邻，在从PPG信号中检测到伪迹方面比基于Transformer的模型表现更好。本研究通过采用自监督学习（SSL）从大量无标签数据中提取潜在特征，然后对有标签数据进行微调，以解决对丰富无标签数据的低效利用问题。我们的实验证明，SSL能够显著增强Transformer模型学习表示的能力，提高其在伪迹分类任务中的鲁棒性。在各种SSL技术中，包括掩码、对比学习和无标签自蒸馏（DINO）-对比学习在小型PPG数据集上表现最稳定、性能最优。此外，我们还探讨了优化对比损失函数的方法，对于对比SSL来说至关重要。受到...的启发

    Recent research at CHU Sainte Justine's Pediatric Critical Care Unit (PICU) has revealed that traditional machine learning methods, such as semi-supervised label propagation and K-nearest neighbors, outperform Transformer-based models in artifact detection from PPG signals, mainly when data is limited. This study addresses the underutilization of abundant unlabeled data by employing self-supervised learning (SSL) to extract latent features from these data, followed by fine-tuning on labeled data. Our experiments demonstrate that SSL significantly enhances the Transformer model's ability to learn representations, improving its robustness in artifact classification tasks. Among various SSL techniques, including masking, contrastive learning, and DINO (self-distillation with no labels)-contrastive learning exhibited the most stable and superior performance in small PPG datasets. Further, we delve into optimizing contrastive loss functions, which are crucial for contrastive SSL. Inspired b
    
[^47]: 无监督连续异常检测与对比学习提示

    Unsupervised Continual Anomaly Detection with Contrastively-learned Prompt. (arXiv:2401.01010v1 [cs.CV])

    [http://arxiv.org/abs/2401.01010](http://arxiv.org/abs/2401.01010)

    本论文提出了一种名为UCAD的无监督连续异常检测框架，通过对比学习提示为UAD增加了连续学习能力。UCAD通过利用关键提示知识库和结构对比学习来指导异常检测，解决了遗忘和计算负担问题。

    

    在工业制造中，无监督增量训练的异常检测至关重要，因为不可预测的缺陷使得获取足够的标记数据变得不可行。然而，当前的连续学习方法主要依赖于有监督的注释，而在无监督异常检测中的应用受限于缺乏监督。目前的无监督异常检测方法是针对不同类别依次训练独立的模型，导致灾难性遗忘和计算负担沉重。为了解决这个问题，我们引入了一种新颖的无监督连续异常检测框架UCAD，通过对比学习提示为UAD提供了连续学习能力。在提出的UCAD中，我们通过利用简洁的关键提示知识库设计了连续提示模块(CPM)，通过使用任务特定的“正常”知识指导任务无关的“异常”模型预测。此外，我们还设计了基于结构的对比学习(SCL)，并采用Segment Anything Model (SAM)。

    Unsupervised Anomaly Detection (UAD) with incremental training is crucial in industrial manufacturing, as unpredictable defects make obtaining sufficient labeled data infeasible. However, continual learning methods primarily rely on supervised annotations, while the application in UAD is limited due to the absence of supervision. Current UAD methods train separate models for different classes sequentially, leading to catastrophic forgetting and a heavy computational burden. To address this issue, we introduce a novel Unsupervised Continual Anomaly Detection framework called UCAD, which equips the UAD with continual learning capability through contrastively-learned prompts. In the proposed UCAD, we design a Continual Prompting Module (CPM) by utilizing a concise key-prompt-knowledge memory bank to guide task-invariant `anomaly' model predictions using task-specific `normal' knowledge. Moreover, Structure-based Contrastive Learning (SCL) is designed with the Segment Anything Model (SAM) 
    
[^48]: 基于机器学习方法预测化合物活性

    Predicting the activity of chemical compounds based on machine learning approaches. (arXiv:2401.01004v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.01004](http://arxiv.org/abs/2401.01004)

    本研究使用机器学习方法探索了预测化合物活性的问题，并在100种不同的技术组合上进行了实验。最终根据一组评估标准选择了最佳的解决方案，并在大数据集上进行了验证。

    

    探索机器学习在不同领域中解决特定挑战的方法和技术是必不可少的。在本研究中，我们解决了化学信息学领域的问题，即提供适当的解决方案以尽可能准确地预测化合物的活性。为了解决这个问题，本研究在100种不同的现有技术组合上进行了实验。这些解决方案是根据一组标准选择的，包括G-means、F1-score和AUC度量。结果在一个包含约10,000个化合物的PubChem数据集上进行了测试，这些化合物根据其活性进行了分类。

    Exploring methods and techniques of machine learning (ML) to address specific challenges in various fields is essential. In this work, we tackle a problem in the domain of Cheminformatics; that is, providing a suitable solution to aid in predicting the activity of a chemical compound to the best extent possible. To address the problem at hand, this study conducts experiments on 100 different combinations of existing techniques. These solutions are then selected based on a set of criteria that includes the G-means, F1-score, and AUC metrics. The results have been tested on a dataset of about 10,000 chemical compounds from PubChem that have been classified according to their activity
    
[^49]: 仅使用脑脊液生物标志物的机器学习分类阿尔茨海默病阶段

    Machine Learning Classification of Alzheimer's Disease Stages Using Cerebrospinal Fluid Biomarkers Alone. (arXiv:2401.00981v1 [cs.LG])

    [http://arxiv.org/abs/2401.00981](http://arxiv.org/abs/2401.00981)

    本论文利用脑脊液生物标志物水平，通过机器学习模型对阿尔茨海默病的不同阶段进行分类。研究发现，淀粉样蛋白β1-42、T-τ和P-τ等生物标志物对早期诊断阿尔茨海默病具有潜力。

    

    由于现有的方法无法在临床症状发生之前的潜伏期识别患者，早期诊断阿尔茨海默病是一个挑战。几项研究表明脑脊液生物标志物，淀粉样蛋白β1-42、T-τ和P-τ，对于早期诊断阿尔茨海默病阶段具有潜力。在这项工作中，我们利用机器学习模型根据脑脊液生物标志物水平单独分类阿尔茨海默病的不同阶段。分析了国家阿尔茨海默病协调中心数据库中的患者的电子健康记录，并根据简明精神状态评分和临床痴呆评定将患者细分。进行了统计和相关性分析以确定阿尔茨海默病阶段之间的显著差异。随后，利用K最近邻、集成提升树、集成袋装树、支持向量机等机器学习分类器进行分类。

    Early diagnosis of Alzheimer's disease is a challenge because the existing methodologies do not identify the patients in their preclinical stage, which can last up to a decade prior to the onset of clinical symptoms. Several research studies demonstrate the potential of cerebrospinal fluid biomarkers, amyloid beta 1-42, T-tau, and P-tau, in early diagnosis of Alzheimer's disease stages. In this work, we used machine learning models to classify different stages of Alzheimer's disease based on the cerebrospinal fluid biomarker levels alone. An electronic health record of patients from the National Alzheimer's Coordinating Centre database was analyzed and the patients were subdivided based on mini-mental state scores and clinical dementia ratings. Statistical and correlation analyses were performed to identify significant differences between the Alzheimer's stages. Afterward, machine learning classifiers including K-Nearest Neighbors, Ensemble Boosted Tree, Ensemble Bagged Tree, Support V
    
[^50]: 在合成数据训练中，针对下游任务的生成模型选择的欺诈检测模型研究

    Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models. (arXiv:2401.00974v1 [cs.LG])

    [http://arxiv.org/abs/2401.00974](http://arxiv.org/abs/2401.00974)

    本论文研究了欺诈检测模型训练中针对下游任务的生成模型选择问题，并调查了在不同的可解释性和性能约束条件下的最佳实践。研究结果表明，在合成训练欺诈检测模型时，贝叶斯网络（BN）的生成模型优于神经网络（NN）的生成模型。

    

    设计用于下游任务的生成模型选择流程是一个尚未解决但具有重要实际意义的问题。现有研究主要关注单一类型生成模型的实用性，对于合成训练任务中如何选择最佳生成模型家族，给定特定的机器学习模型类别和性能度量，提供的见解有限。本文针对训练欺诈检测模型的下游任务导向生成模型选择问题进行研究，并调查在模型可解释性和模型性能约束不同时的最佳实践。我们的研究支持以下观点：在模型可解释性约束下，神经网络（NN）和贝叶斯网络（BN）的生成模型均能很好完成合成训练任务，但在合成训练欺诈检测模型时，BN-based生成模型比NN-based更好。

    Devising procedures for downstream task-oriented generative model selections is an unresolved problem of practical importance. Existing studies focused on the utility of a single family of generative models. They provided limited insights on how synthetic data practitioners select the best family generative models for synthetic training tasks given a specific combination of machine learning model class and performance metric. In this paper, we approach the downstream task-oriented generative model selections problem in the case of training fraud detection models and investigate the best practice given different combinations of model interpretability and model performance constraints. Our investigation supports that, while both Neural Network(NN)-based and Bayesian Network(BN)-based generative models are both good to complete synthetic training task under loose model interpretability constrain, the BN-based generative models is better than NN-based when synthetic training fraud detectio
    
[^51]: Facebook隐私保护fNIRS数据的报告

    Facebook Report on Privacy of fNIRS data. (arXiv:2401.00973v1 [cs.LG])

    [http://arxiv.org/abs/2401.00973](http://arxiv.org/abs/2401.00973)

    本项目旨在开发保护隐私的fNIRS数据的机器学习模型训练技术，并探索集中式和联邦学习方法，以保障多客户之间共享模型的同时保护私有数据。

    

    本项目的主要目标是为fNIRS数据开发保护隐私的机器学习模型训练技术。该项目将在集中式环境中构建一个本地模型，同时采用差分隐私（DP）和认证鲁棒性。它还将探索协同联邦学习，以在多个客户之间训练共享模型，而不共享本地的fNIRS数据集。为了防止此类客户私有数据的无意泄漏，我们还将在联邦学习环境中实现差分隐私。

    The primary goal of this project is to develop privacy-preserving machine learning model training techniques for fNIRS data. This project will build a local model in a centralized setting with both differential privacy (DP) and certified robustness. It will also explore collaborative federated learning to train a shared model between multiple clients without sharing local fNIRS datasets. To prevent unintentional private information leakage of such clients' private datasets, we will also implement DP in the federated learning setting.
    
[^52]: 非创伤ICU患者血液输注需求预测的鲁棒元模型

    Robust Meta-Model for Predicting the Need for Blood Transfusion in Non-traumatic ICU Patients. (arXiv:2401.00972v1 [cs.LG])

    [http://arxiv.org/abs/2401.00972](http://arxiv.org/abs/2401.00972)

    该研究开发了一种鲁棒的元模型，用于预测非创伤ICU患者未来24小时内需要输血的概率。

    

    目标：在ICU环境中，血液输注是处理贫血和凝血功能障碍的重要手段，需要准确的预测以进行有效的资源分配和患者风险评估。然而，现有的临床决策支持系统主要针对特定患者人群的特殊医疗情况，并且专注于单一类型的血液输注。本研究旨在开发一种先进的基于机器学习的模型，以预测多样化的非创伤ICU患者在未来24小时内输注血液的可能性。

    Objective: Blood transfusions, crucial in managing anemia and coagulopathy in ICU settings, require accurate prediction for effective resource allocation and patient risk assessment. However, existing clinical decision support systems have primarily targeted a particular patient demographic with unique medical conditions and focused on a single type of blood transfusion. This study aims to develop an advanced machine learning-based model to predict the probability of transfusion necessity over the next 24 hours for a diverse range of non-traumatic ICU patients.  Methods: We conducted a retrospective cohort study on 72,072 adult non-traumatic ICU patients admitted to a high-volume US metropolitan academic hospital between 2016 and 2020. We developed a meta-learner and various machine learning models to serve as predictors, training them annually with four-year data and evaluating on the fifth, unseen year, iteratively over five years.  Results: The experimental results revealed that the
    
[^53]: 从数据中心的角度提高合成信用卡交易时间序列的保真度和实用性

    Improve Fidelity and Utility of Synthetic Credit Card Transaction Time Series from Data-centric Perspective. (arXiv:2401.00965v1 [cs.LG])

    [http://arxiv.org/abs/2401.00965](http://arxiv.org/abs/2401.00965)

    本文从数据中心的角度提出了五种预处理方案，以增强条件概率自回归模型（CPAR）的训练，从而改善合成信用卡交易数据的保真度和实用性。进一步通过定制欺诈检测模型的训练验证了合成数据的实用性，并为金融领域的合成数据训练提供了宝贵的见解和实践指导。

    

    在探索合成表格数据的生成模型训练中，特别是在顺序上下文中，如信用卡交易数据，存在着重大挑战。本文解决了这些挑战，重点是实现对实际数据的高保真度和对机器学习任务的最佳实用性。我们介绍了五种预处理方案来增强条件概率自回归模型（CPAR）的训练，展示了合成数据保真度和实用性的渐进性改进。在达到满意的保真度水平后，我们转向针对时间序列数据定制的欺诈检测模型的训练，评估合成数据的实用性。我们的研究结果为金融领域的合成数据从实际数据过渡到合成数据集的训练提供了宝贵的见解和实践指导，并阐明了合成信用卡交易时间序列的更广泛的方法论。

    Exploring generative model training for synthetic tabular data, specifically in sequential contexts such as credit card transaction data, presents significant challenges. This paper addresses these challenges, focusing on attaining both high fidelity to actual data and optimal utility for machine learning tasks. We introduce five pre-processing schemas to enhance the training of the Conditional Probabilistic Auto-Regressive Model (CPAR), demonstrating incremental improvements in the synthetic data's fidelity and utility. Upon achieving satisfactory fidelity levels, our attention shifts to training fraud detection models tailored for time-series data, evaluating the utility of the synthetic data. Our findings offer valuable insights and practical guidelines for synthetic data practitioners in the finance sector, transitioning from real to synthetic datasets for training purposes, and illuminating broader methodologies for synthesizing credit card transaction time series.
    
[^54]: 跨领域WiFi CSI基于人体活动识别的数据增强技术

    Data Augmentation Techniques for Cross-Domain WiFi CSI-based Human Activity Recognition. (arXiv:2401.00964v1 [cs.CV])

    [http://arxiv.org/abs/2401.00964](http://arxiv.org/abs/2401.00964)

    本研究应用基于图像学习的数据增强技术于WiFi CSI，旨在解决人体活动识别中模型泛化能力差的问题。通过跨场景和跨系统的实验，研究了线性视线（LOS）和非线性视线（NLOS）穿墙场景之间以及不同天线系统之间的泛化效果。通过构建基于EfficientNetV2架构的活动识别模型并进行消融研究，评估了不同数据增强技术的效果。

    

    基于WiFi信道状态信息（CSI）的人体活动识别能够在室内环境中实现无接触和视觉保护隐私的感知。然而，由于环境条件和感知硬件的差异，模型泛化能力差是这个领域中一个众所周知的问题。为了解决这个问题，本文将常用于基于图像学习的数据增强技术应用于WiFi CSI，并研究它们对模型泛化性能在跨场景和跨系统设置中的影响。特别是，我们专注于线性视线（LOS）和非线性视线（NLOS）穿墙场景之间的泛化，以及不同天线系统之间的泛化，这仍然是一个未经充分探索的领域。我们收集并公开了一个包含人体活动CSI幅度谱图的数据集。利用这个数据集，进行了一个消融研究，基于EfficientNetV2架构构建了活动识别模型，并评估了不同数据增强技术的效果。

    The recognition of human activities based on WiFi Channel State Information (CSI) enables contactless and visual privacy-preserving sensing in indoor environments. However, poor model generalization, due to varying environmental conditions and sensing hardware, is a well-known problem in this space. To address this issue, in this work, data augmentation techniques commonly used in image-based learning are applied to WiFi CSI to investigate their effects on model generalization performance in cross-scenario and cross-system settings. In particular, we focus on the generalization between line-of-sight (LOS) and non-line-of-sight (NLOS) through-wall scenarios, as well as on the generalization between different antenna systems, which remains under-explored. We collect and make publicly available a dataset of CSI amplitude spectrograms of human activities. Utilizing this data, an ablation study is conducted in which activity recognition models based on the EfficientNetV2 architecture are tr
    
[^55]: 自动化模型选择算法用于表格数据

    Automated Model Selection for Tabular Data. (arXiv:2401.00961v1 [cs.LG])

    [http://arxiv.org/abs/2401.00961](http://arxiv.org/abs/2401.00961)

    本文介绍了一种自动化模型选择算法，用于表格数据的预测。该算法考虑了特征之间的交互，并包含了基于优先级的随机网格搜索和贪婪搜索两种不同的特征选择方法。

    

    表格数据中的结构化数据包含独特且离散的特征，并且这些特征对目标的重要性各不相同。单个特征的组合可能比简单的单个特征贡献更具预测性和意义。R的混合效应线性模型库允许用户在模型设计中提供这种交互式特征组合。然而，鉴于有许多特征和可能的交互选择，模型选择变得非常困难。我们的目标是通过保持计算成本较小，自动化表格数据预测中的模型选择过程，并同时考虑特征之间的交互。该框架包括两种不同的特征选择方法：基于优先级的随机网格搜索和贪婪搜索方法。基于优先级的方法利用先验概率来引导搜索，高效地探索特征组合。贪婪方法通过迭代地添加特征构建解决方案。

    Structured data in the form of tabular datasets contain features that are distinct and discrete, with varying individual and relative importances to the target. Combinations of one or more features may be more predictive and meaningful than simple individual feature contributions. R's mixed effect linear models library allows users to provide such interactive feature combinations in the model design. However, given many features and possible interactions to select from, model selection becomes an exponentially difficult task. We aim to automate the model selection process for predictions on tabular datasets incorporating feature interactions while keeping computational costs small. The framework includes two distinct approaches for feature selection: a Priority-based Random Grid Search and a Greedy Search method. The Priority-based approach efficiently explores feature combinations using prior probabilities to guide the search. The Greedy method builds the solution iteratively by addin
    
[^56]: 在脉冲神经网络中学习长序列

    Learning Long Sequences in Spiking Neural Networks. (arXiv:2401.00955v1 [cs.NE])

    [http://arxiv.org/abs/2401.00955](http://arxiv.org/abs/2401.00955)

    这项研究首次系统地探索了最新的状态空间模型（SSM）与脉冲神经网络（SNN）在长序列建模方面的结合。结果表明，在经典的长序列建模任务中，基于SSM的SNN能够超越Transformers并且在顺序图像分类中以更少的参数超越当前最先进的SNN。

    

    脉冲神经网络（SNN）受到大脑的启发，能够实现高效的计算。然而，由于遗留的循环神经网络（RNN）的限制以及训练时使用非可微的二值脉冲激活函数，SNN在现代序列任务中与人工神经网络的竞争一直存在困难。最近，对于Transformers的高效替代方案的兴趣的复苏，使得一种名为状态空间模型（SSM）的最新递归架构得以出现。本研究首次系统地研究了最新的SSM与SNN相结合在长序列建模方面的交叉点。结果表明，基于SSM的SNN在一个经典的长序列建模基准任务中能够超越Transformers的性能。同时，还展示了基于SSM的SNN在顺序图像分类上可以以更少的参数超越当前最先进的SNN。最后，提出了一种新的特征

    Spiking neural networks (SNNs) take inspiration from the brain to enable energy-efficient computations. Since the advent of Transformers, SNNs have struggled to compete with artificial networks on modern sequential tasks, as they inherit limitations from recurrent neural networks (RNNs), with the added challenge of training with non-differentiable binary spiking activations. However, a recent renewed interest in efficient alternatives to Transformers has given rise to state-of-the-art recurrent architectures named state space models (SSMs). This work systematically investigates, for the first time, the intersection of state-of-the-art SSMs with SNNs for long-range sequence modelling. Results suggest that SSM-based SNNs can outperform the Transformer on all tasks of a well-established long-range sequence modelling benchmark. It is also shown that SSM-based SNNs can outperform current state-of-the-art SNNs with fewer parameters on sequential image classification. Finally, a novel feature
    
[^57]: 拥有零和非负MTW张量的费用族在最优传送中的应用

    Families of costs with zero and nonnegative MTW tensor in optimal transport. (arXiv:2401.00953v1 [math.AP])

    [http://arxiv.org/abs/2401.00953](http://arxiv.org/abs/2401.00953)

    这篇论文介绍了在最优传送中使用形式为$\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$的费用函数时的零和非负MTW张量的计算方法，并提供了MTW张量在零向量上为零的条件以及相应的线性ODE的简化方法。此外，还给出了逆函数的解析表达式以及一些具体的应用情况。

    

    我们计算了在$\mathbb{R}^n$上具有形式$\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$的费用函数的最优传送问题的MTW张量（或交叉曲率）。其中，$\mathsf{u}$是一个具有逆函数$\mathsf{s}$的标量函数，$x^{\ft}y$是属于$\mathbb{R}^n$开子集的向量$x，y$的非退化双线性配对。MTW张量在Kim-McCann度量下对于零向量的条件是一个四阶非线性ODE，可以被简化为具有常数系数$P$和$S$的形式为$\mathsf{s}^{(2)} - S\mathsf{s}^{(1)} + P\mathsf{s} = 0$的线性ODE。最终得到的逆函数包括Lambert和广义反双曲/三角函数。平方欧氏度量和$\log$型费用是这些解的实例。这个家族的最优映射也是显式的。

    We compute explicitly the MTW tensor (or cross curvature) for the optimal transport problem on $\mathbb{R}^n$ with a cost function of form $\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$, where $\mathsf{u}$ is a scalar function with inverse $\mathsf{s}$, $x^{\ft}y$ is a nondegenerate bilinear pairing of vectors $x, y$ belonging to an open subset of $\mathbb{R}^n$. The condition that the MTW-tensor vanishes on null vectors under the Kim-McCann metric is a fourth-order nonlinear ODE, which could be reduced to a linear ODE of the form $\mathsf{s}^{(2)} - S\mathsf{s}^{(1)} + P\mathsf{s} = 0$ with constant coefficients $P$ and $S$. The resulting inverse functions include {\it Lambert} and {\it generalized inverse hyperbolic\slash trigonometric} functions. The square Euclidean metric and $\log$-type costs are equivalent to instances of these solutions. The optimal map for the family is also explicit. For cost functions of a similar form on a hyperboloid model of the hyperbolic space and u
    
[^58]: 无监督的基于图的学习方法用于6G子网络的子频带分配

    Unsupervised Graph-based Learning Method for Sub-band Allocation in 6G Subnetworks. (arXiv:2401.00950v1 [cs.NI])

    [http://arxiv.org/abs/2401.00950](http://arxiv.org/abs/2401.00950)

    本文提出了一种无监督的基于图的学习方法，用于在6G子网络中进行子频带分配。该方法通过优化使用图神经网络的子频带分配，实现了与集中式贪婪着色子频带分配方法相近的性能，并且具有更低的计算时间复杂度和较小的信令开销。

    

    在本文中，我们提出了一种无监督的基于图的学习方法，用于在无线网络中进行频率子带分配。我们考虑在工厂环境中密集部署的子网络，这些子网络只有有限数量的子频带，必须被优化地分配以协调子网络间的干扰。我们将子网络部署建模为一个冲突图，并提出了一种受到图着色启发和Potts模型的无监督学习方法，利用图神经网络来优化子频带分配。数值评估表明，所提出的方法在较低的计算时间复杂度下，实现了与集中式贪婪着色子频带分配启发式方法接近的性能。此外，与需要所有互相干扰的信道信息的迭代优化启发式相比，它产生更少的信令开销。我们进一步证明该方法对不同的网络设置具有健壮性。

    In this paper, we present an unsupervised approach for frequency sub-band allocation in wireless networks using graph-based learning. We consider a dense deployment of subnetworks in the factory environment with a limited number of sub-bands which must be optimally allocated to coordinate inter-subnetwork interference. We model the subnetwork deployment as a conflict graph and propose an unsupervised learning approach inspired by the graph colouring heuristic and the Potts model to optimize the sub-band allocation using graph neural networks. The numerical evaluation shows that the proposed method achieves close performance to the centralized greedy colouring sub-band allocation heuristic with lower computational time complexity. In addition, it incurs reduced signalling overhead compared to iterative optimization heuristics that require all the mutual interfering channel information. We further demonstrate that the method is robust to different network settings.
    
[^59]: 使用深度强化学习在混沌系统中进行数据同化

    Data Assimilation in Chaotic Systems Using Deep Reinforcement Learning. (arXiv:2401.00916v1 [math.DS])

    [http://arxiv.org/abs/2401.00916](http://arxiv.org/abs/2401.00916)

    本文介绍了一种使用强化学习在混沌系统中进行数据同化的新策略。该方法通过使用完整或部分观测的状态变量进行状态校正，旨在最小化观测和预测状态之间的误差。

    

    数据同化在各种应用中起着关键作用，从气候预测和天气预报到自主车辆的轨迹规划。一个典型的例子是广泛使用的集合卡尔曼滤波器（EnKF），它依赖于线性更新来最小化预测状态集合的方差。最近的进展在这个领域中看到了深度学习方法的出现，主要是在有监督学习框架内。然而，这些模型在未经训练的情况下的适应性仍然是一个挑战。在本研究中，我们引入了一种新的数据同化策略，利用强化学习（RL）来使用完整或部分观测的状态变量进行状态校正。我们的研究重点是在混沌的Lorenz '63系统上展示这种方法，其中代理的目标是将观测和相应的预测状态之间的均方根误差最小化。因此，代理开发出了一种校正策略。

    Data assimilation (DA) plays a pivotal role in diverse applications, ranging from climate predictions and weather forecasts to trajectory planning for autonomous vehicles. A prime example is the widely used ensemble Kalman filter (EnKF), which relies on linear updates to minimize variance among the ensemble of forecast states. Recent advancements have seen the emergence of deep learning approaches in this domain, primarily within a supervised learning framework. However, the adaptability of such models to untrained scenarios remains a challenge. In this study, we introduce a novel DA strategy that utilizes reinforcement learning (RL) to apply state corrections using full or partial observations of the state variables. Our investigation focuses on demonstrating this approach to the chaotic Lorenz '63 system, where the agent's objective is to minimize the root-mean-squared error between the observations and corresponding forecast states. Consequently, the agent develops a correction stra
    
[^60]: 用于自动驾驶的WoodScape运动分割--CVPR 2023 OmniCV研讨会挑战

    WoodScape Motion Segmentation for Autonomous Driving -- CVPR 2023 OmniCV Workshop Challenge. (arXiv:2401.00910v1 [cs.CV])

    [http://arxiv.org/abs/2401.00910](http://arxiv.org/abs/2401.00910)

    运动分割是自动驾驶中重要的任务，传统的卷积神经网络方法在处理摄像机自我运动、鱼眼镜头径向畸变和时间一致性方面效果较差。本论文提出了WoodScape鱼眼运动分割挑战，使用合成数据集和真实数据集来提高机器学习模型性能，这是首个专注于鱼眼运动分割的比赛之一，旨在探索和评估其潜力和影响。

    

    运动分割是自动驾驶中复杂但不可或缺的任务。摄像机的自我运动、鱼眼镜头的径向畸变以及对时间一致性的需求引入了挑战，使得传统和标准的卷积神经网络（CNN）方法效果较差。相应的繁琐的数据标注、多样化和罕见情况的表征以及广泛的数据采集需求强调了用于改善机器学习模型性能的合成数据的必要性。为此，我们使用Parallel Domain开发的PD-WoodScape合成数据集，以及WoodScape鱼眼数据集。因此，我们提出了WoodScape鱼眼运动分割挑战，作为CVPR 2023全景计算机视觉（OmniCV）研讨会的一部分举行。作为首个专注于鱼眼运动分割的比赛之一，我们旨在探索和评估其潜力和影响。

    Motion segmentation is a complex yet indispensable task in autonomous driving. The challenges introduced by the ego-motion of the cameras, radial distortion in fisheye lenses, and the need for temporal consistency make the task more complicated, rendering traditional and standard Convolutional Neural Network (CNN) approaches less effective. The consequent laborious data labeling, representation of diverse and uncommon scenarios, and extensive data capture requirements underscore the imperative of synthetic data for improving machine learning model performance. To this end, we employ the PD-WoodScape synthetic dataset developed by Parallel Domain, alongside the WoodScape fisheye dataset. Thus, we present the WoodScape fisheye motion segmentation challenge for autonomous driving, held as part of the CVPR 2023 Workshop on Omnidirectional Computer Vision (OmniCV). As one of the first competitions focused on fisheye motion segmentation, we aim to explore and evaluate the potential and impac
    
[^61]: Score Distillation在文本到3D生成中解决模式崩溃的方法

    Taming Mode Collapse in Score Distillation for Text-to-3D Generation. (arXiv:2401.00909v1 [cs.CV])

    [http://arxiv.org/abs/2401.00909](http://arxiv.org/abs/2401.00909)

    本文揭示了现有的基于Score Distillation的文本到3D生成框架存在模式崩溃问题，通过在变分目标中加入熵项来改进生成的3D模型的多样性，解决了Janus伪像问题。

    

    尽管Score Distillation在文本到3D生成中表现出色，但这些技术普遍存在视图一致性问题，也被称为“Janus”伪像，即生成的对象伪造了多个前视图。虽然已经有一些经验有效的方法通过去偏置或者引导工程来解决这个问题，但对于这个问题的更严格的解释和解决方法仍然很难找到。在本文中，我们揭示了现有的基于Score Distillation的文本到3D生成框架陷入了在每个视图上独立最大似然求解的最大似然模式崩溃问题，实践中表现为Janus伪像。为了避免模式崩溃，我们通过在相应变分目标中重新引入熵项，对渲染图像的分布进行改进。最大化熵鼓励生成的3D模型在不同视图之间具有多样性，从而缓解了模式崩溃问题。

    Despite the remarkable performance of score distillation in text-to-3D generation, such techniques notoriously suffer from view inconsistency issues, also known as "Janus" artifact, where the generated objects fake each view with multiple front faces. Although empirically effective methods have approached this problem via score debiasing or prompt engineering, a more rigorous perspective to explain and tackle this problem remains elusive. In this paper, we reveal that the existing score distillation-based text-to-3D generation frameworks degenerate to maximal likelihood seeking on each view independently and thus suffer from the mode collapse problem, manifesting as the Janus artifact in practice. To tame mode collapse, we improve score distillation by re-establishing in entropy term in the corresponding variational objective, which is applied to the distribution of rendered images. Maximizing the entropy encourages diversity among different views in generated 3D assets, thereby mitiga
    
[^62]: LaFFi: 利用混合自然语言反馈来优化语言模型的微调

    LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models. (arXiv:2401.00907v1 [cs.LG])

    [http://arxiv.org/abs/2401.00907](http://arxiv.org/abs/2401.00907)

    LaFFi是一种用于微调语言模型的替代方法，通过要求模型预测标注者将会给出的反馈，显著提高了在问答任务中的准确性，为应用自然语言反馈提供了一个有前途的方向。

    

    大型语言模型（LLM）的微调可以将训练好的模型适应特定的下游任务，并显著提高任务特定性能。监督微调（SFT）是一种常见的方法，其中LLM被训练成产生期望的答案。然而，使用SFT训练的LLM在推理任务（如问答）中有时会出现简单错误和幻觉。在没有外部反馈的情况下，SFT很难学习到问题和期望答案之间的良好映射，特别是在数据集较小的情况下。本文介绍了一种名为自然语言反馈微调LLM（LaFFi）的替代方法。LaFFi要求LLM直接预测标注者将会给出的反馈。我们发现，这样的反思要求可以显著提高在领域内问答任务中的准确性，为在SFT LLM领域中应用自然语言反馈提供了一个有前途的方向。额外的消融研究表明这种方法的一部分可以被替代。

    Fine-tuning Large Language Models (LLMs) adapts a trained model to specific downstream tasks, significantly improving task-specific performance. Supervised Fine-Tuning (SFT) is a common approach, where an LLM is trained to produce desired answers. However, LLMs trained with SFT sometimes make simple mistakes and result in hallucinations on reasoning tasks such as question-answering. Without external feedback, it is difficult for SFT to learn a good mapping between the question and the desired answer, especially with a small dataset. This paper introduces an alternative to SFT called Natural Language Feedback for Finetuning LLMs (LaFFi). LaFFi has LLMs directly predict the feedback they will receive from an annotator. We find that requiring such reflection can significantly improve the accuracy in in-domain question-answering tasks, providing a promising direction for the application of natural language feedback in the realm of SFT LLMs. Additional ablation studies show that the portion
    
[^63]: 对MIMIC-IV数据集和基准算法的公平性评估：应用于重症监护病房住院时间预测

    Evaluating the Fairness of the MIMIC-IV Dataset and a Baseline Algorithm: Application to the ICU Length of Stay Prediction. (arXiv:2401.00902v1 [cs.LG])

    [http://arxiv.org/abs/2401.00902](http://arxiv.org/abs/2401.00902)

    本文使用MIMIC-IV数据集来评估一个XGBoost二分类模型在预测重症监护病房住院时间时的公平性和偏见，结果发现了数据集在人口统计属性上存在类别不平衡，并提出了减少偏见的机器学习技术建议。

    

    本文利用MIMIC-IV数据集，检查了一个XGBoost二分类模型在预测重症监护病房住院时间时的公平性和偏见。强调了重症监护病房在处理重症患者中的关键作用，研究解决了重症监护病房容量紧张的问题。它强调了住院时间预测对资源分配的重要性。研究揭示了数据集在人口统计属性上的类别不平衡，并采用数据预处理和特征提取。尽管XGBoost模型整体表现良好，但种族和保险属性方面的不平等反映出需要定制评估和持续监测。本文最后提出了关于减少偏见的注重公平性的机器学习技术建议，并强调了医疗专业人员和数据科学家之间的合作努力的必要性。

    This paper uses the MIMIC-IV dataset to examine the fairness and bias in an XGBoost binary classification model predicting the Intensive Care Unit (ICU) length of stay (LOS). Highlighting the critical role of the ICU in managing critically ill patients, the study addresses the growing strain on ICU capacity. It emphasizes the significance of LOS prediction for resource allocation. The research reveals class imbalances in the dataset across demographic attributes and employs data preprocessing and feature extraction. While the XGBoost model performs well overall, disparities across race and insurance attributes reflect the need for tailored assessments and continuous monitoring. The paper concludes with recommendations for fairness-aware machine learning techniques for mitigating biases and the need for collaborative efforts among healthcare professionals and data scientists.
    
[^64]: 在嘈杂环境中检测折射鲸回声定位点击的存在

    Detecting the presence of sperm whales echolocation clicks in noisy environments. (arXiv:2401.00900v1 [eess.AS])

    [http://arxiv.org/abs/2401.00900](http://arxiv.org/abs/2401.00900)

    该研究使用了稳定的多脉冲结构（MPS）作为折射鲸回声定位点击的检测指标，能够在嘈杂环境中识别和分类点击存在。这种方法能够处理高噪声和低信噪比的情况。

    

    折射鲸（Physeter macrocephalus）通过一系列的冲击性、类似点击的声音（称为回声定位点击）在水下导航。这些点击具有多脉冲结构（MPS），可作为一个独特的模式。在这项工作中，我们使用MPS的稳定性作为检测指标，识别和分类嘈杂环境中的点击存在。为了区分噪声瞬变和处理多只折射鲸同时发出的情况，我们的方法通过聚类MPS测量的时间序列来移除不符合间击间隔、持续时间和频谱限制的潜在点击。因此，我们的方法能够处理高噪声瞬变和低信噪比。我们使用三个数据集来检验我们的检测方法的性能：包含经过手工验证的环境噪声的地中海七个月的录音；来自多米尼加岛的经过手工标记的数据，其中包含了几天的记录。

    Sperm whales (Physeter macrocephalus) navigate underwater with a series of impulsive, click-like sounds known as echolocation clicks. These clicks are characterized by a multipulse structure (MPS) that serves as a distinctive pattern. In this work, we use the stability of the MPS as a detection metric for recognizing and classifying the presence of clicks in noisy environments. To distinguish between noise transients and to handle simultaneous emissions from multiple sperm whales, our approach clusters a time series of MPS measures while removing potential clicks that do not fulfil the limits of inter-click interval, duration and spectrum. As a result, our approach can handle high noise transients and low signal-to-noise ratio. The performance of our detection approach is examined using three datasets: seven months of recordings from the Mediterranean Sea containing manually verified ambient noise; several days of manually labelled data collected from the Dominica Island containing app
    
[^65]: 通过跨模态渗透实现平衡多模态联邦学习

    Balanced Multi-modal Federated Learning via Cross-Modal Infiltration. (arXiv:2401.00894v1 [cs.LG])

    [http://arxiv.org/abs/2401.00894](http://arxiv.org/abs/2401.00894)

    该论文提出了一种通过从全局主导模态进行知识传递的跨模态渗透联邦学习框架，有效解决了分布式环境中的模态不平衡和知识异质性问题。

    

    联邦学习是隐私保护分布式计算的基础，通过协同训练神经网络而不暴露客户端的原始数据。当前的联邦学习主要关注单一模态数据，而对于分布式多模态数据的知识利用仍然未被充分探索。现有的多模态联邦学习解决方案主要针对输入端的统计或模态异质性，然而在分布式环境中尚未解决"模态不平衡"的根本问题，这可能导致对不同模态的信息利用不足和异质知识聚合。本文提出了一种新颖的跨模态渗透联邦学习（FedCMI）框架，通过从全局主导模态进行知识传递，有效缓解模态不平衡和知识异质性问题。为了避免仅仅模仿主导模态行为导致弱模态信息的丢失，我们提出了一种注意力机制来选择性地传递知识。

    Federated learning (FL) underpins advancements in privacy-preserving distributed computing by collaboratively training neural networks without exposing clients' raw data. Current FL paradigms primarily focus on uni-modal data, while exploiting the knowledge from distributed multimodal data remains largely unexplored. Existing multimodal FL (MFL) solutions are mainly designed for statistical or modality heterogeneity from the input side, however, have yet to solve the fundamental issue,"modality imbalance", in distributed conditions, which can lead to inadequate information exploitation and heterogeneous knowledge aggregation on different modalities.In this paper, we propose a novel Cross-Modal Infiltration Federated Learning (FedCMI) framework that effectively alleviates modality imbalance and knowledge heterogeneity via knowledge transfer from the global dominant modality. To avoid the loss of information in the weak modality due to merely imitating the behavior of dominant modality, 
    
[^66]: 使用储层计算进行吸引子重建：储层的条件Lyapunov指数对忠实吸引子重建的影响

    Attractor reconstruction with reservoir computers: The effect of the reservoir's conditional Lyapunov exponents on faithful attractor reconstruction. (arXiv:2401.00885v1 [cs.LG])

    [http://arxiv.org/abs/2401.00885](http://arxiv.org/abs/2401.00885)

    该论文研究了储层计算在吸引子重建中的表现，发现驱动式储层的最大条件Lyapunov指数需要比真实系统的最小Lyapunov指数更小，储层的谱半径对吸引子重建起到重要作用。

    

    储层计算是一种机器学习技术，已被证明能够复制动力系统的混沌吸引子，包括分形维度和整个Lyapunov谱。我们定量地将驱动式储层计算的广义同步动力学与自主式储层计算在吸引子重建任务上的性能相关联。我们发现，为了成功进行吸引子重建和Lyapunov指数估计，驱动式储层的最大条件Lyapunov指数必须显著小于真实系统的最小（最负）Lyapunov指数。我们发现，储层的最大条件Lyapunov指数强烈依赖于储层邻接矩阵的谱半径，因此，对于吸引子重建和Lyapunov指数估计，谱半径较小的储层计算表现更好。

    Reservoir computing is a machine learning technique which has been shown to be able to replicate the chaotic attractor, including the fractal dimension and the entire Lyapunov spectrum, of the dynamical system on which it is trained. We quantitatively relate the generalized synchronization dynamics of a driven reservoir computer during the training stage to the performance of the autonomous reservoir computer at the attractor reconstruction task. We show that, for successful attractor reconstruction and Lyapunov exponent estimation, the largest conditional Lyapunov exponent of the driven reservoir must be significantly smaller (more negative) than the smallest (most negative) Lyapunov exponent of the true system. We find that the maximal conditional Lyapunov exponent of the reservoir depends strongly on the spectral radius of the reservoir adjacency matrix, and therefore, for attractor reconstruction and Lyapunov exponent estimation, small spectral radius reservoir computers perform be
    
[^67]: 使用自动编码器自动化白血病诊断：一项比较研究

    Automating Leukemia Diagnosis with Autoencoders: A Comparative Study. (arXiv:2401.00883v1 [cs.LG])

    [http://arxiv.org/abs/2401.00883](http://arxiv.org/abs/2401.00883)

    本研究使用自动编码器开发出有价值的特征，提高了白血病诊断的准确性，并相较于传统的机器学习模型在精确度和F1-score指标上表现更好。

    

    白血病是威胁人类生命的最常见和致命的癌症之一。来自患者关键参数的医疗数据中隐藏着宝贵的信息。在这个课题上，深度学习可以用来提取这些信息。本文使用自动编码器开发了有价值的特征，以帮助提高白血病诊断的准确性。我们尝试找到最佳的激活函数和优化器来在自动编码器中使用，并设计了最佳的神经网络结构。我们提出的架构与该领域的经典机器学习模型进行了比较。我们的方法在精确度和F1-score指标上比其他机器学习模型提升了超过11%。

    Leukemia is one of the most common and death-threatening types of cancer that threaten human life. Medical data from some of the patient's critical parameters contain valuable information hidden among these data. On this subject, deep learning can be used to extract this information. In this paper, AutoEncoders have been used to develop valuable features to help the precision of leukemia diagnosis. It has been attempted to get the best activation function and optimizer to use in AutoEncoder and designed the best architecture for this neural network. The proposed architecture is compared with this area's classical machine learning models. Our proposed method performs better than other machine learning in precision and f1-score metrics by more than 11%.
    
[^68]: 平衡的图结构信息用于脑疾病检测

    Balanced Graph Structure Information for Brain Disease Detection. (arXiv:2401.00876v1 [cs.LG])

    [http://arxiv.org/abs/2401.00876](http://arxiv.org/abs/2401.00876)

    这项工作提出了一种名为Bargrain的平衡脑图结构方法，通过同时模拟经过滤波的相关矩阵和最优样本图来改进脑疾病检测性能，并解决了仅依赖单一类型结构的限制。

    

    分析脑区间的连接对于检测自闭症或精神分裂等神经系统疾病至关重要。最近的研究采用图神经网络(GNNs)来利用脑中的图结构，提高检测性能。目前的方法使用ROI的血氧水平依赖性(BOLD)信号之间的相关性来生成图结构。其他方法通过端到端学习使用训练样本来学习最优的图结构。然而，独立实施这些方法会导致相关性图中的噪音数据问题以及最优图的过拟合问题。在这项工作中，我们提出了Bargrain(平衡的脑图结构)，它模拟了两种图结构：经过滤波的相关矩阵和使用图卷积网络(GCNs)生成的最优样本图。这种方法旨在充分利用两种图的优点，并解决仅依赖单一类型结构的局限性。

    Analyzing connections between brain regions of interest (ROI) is vital to detect neurological disorders such as autism or schizophrenia. Recent advancements employ graph neural networks (GNNs) to utilize graph structures in brains, improving detection performances. Current methods use correlation measures between ROI's blood-oxygen-level-dependent (BOLD) signals to generate the graph structure. Other methods use the training samples to learn the optimal graph structure through end-to-end learning. However, implementing those methods independently leads to some issues with noisy data for the correlation graphs and overfitting problems for the optimal graph. In this work, we proposed Bargrain (balanced graph structure for brains), which models two graph structures: filtered correlation matrix and optimal sample graph using graph convolution networks (GCNs). This approach aims to get advantages from both graphs and address the limitations of only relying on a single type of structure. Bas
    
[^69]: 用贝叶斯方法统一自监督聚类和能量模型

    A Bayesian Unification of Self-Supervised Clustering and Energy-Based Models. (arXiv:2401.00873v1 [cs.LG])

    [http://arxiv.org/abs/2401.00873](http://arxiv.org/abs/2401.00873)

    该论文研究了用贝叶斯方法统一自监督聚类和能量模型，提出了一种标准化的推导方法，并设计了一个新的可靠地惩罚失败模式的下界。这个下界使得能够训练一个标准的骨架架构，而无需使用非对称元素。

    

    自监督学习是一种利用大量无标签数据的流行且强大的方法，文献中提出了各种训练目标。本研究对最先进的自监督学习目标进行贝叶斯分析，阐明了每个类别中潜在的概率图模型，并提出了一种从基本原理出发推导这些模型的标准方法。分析还表明了将自监督学习与基于似然的生成模型自然整合的方法。我们在基于聚类的自监督学习和能量模型领域中实现了这个概念，引入了一个新的下界，经证明能可靠地惩罚最重要的失败模式。此外，这个新提出的下界使得能够训练一个标准的骨干架构，而无需使用诸如停止梯度、动量编码器或专门的聚类等非对称元素。

    Self-supervised learning is a popular and powerful method for utilizing large amounts of unlabeled data, for which a wide variety of training objectives have been proposed in the literature. In this study, we perform a Bayesian analysis of state-of-the-art self-supervised learning objectives, elucidating the underlying probabilistic graphical models in each class and presenting a standardized methodology for their derivation from first principles. The analysis also indicates a natural means of integrating self-supervised learning with likelihood-based generative models. We instantiate this concept within the realm of cluster-based self-supervised learning and energy models, introducing a novel lower bound which is proven to reliably penalize the most important failure modes. Furthermore, this newly proposed lower bound enables the training of a standard backbone architecture without the necessity for asymmetric elements such as stop gradients, momentum encoders, or specialized clusteri
    
[^70]: 张量网络在可解释的机器学习中在网络安全中的应用

    Tensor Networks for Explainable Machine Learning in Cybersecurity. (arXiv:2401.00867v1 [cs.LG])

    [http://arxiv.org/abs/2401.00867](http://arxiv.org/abs/2401.00867)

    张量网络可以帮助发展可解释的机器学习算法，并提供丰富的模型可解释性。在网络安全中，我们的无监督聚类算法基于矩阵乘积状态，在性能上与传统的深度学习模型相媲美。我们的方法还能提取特征概率、熵和互信息，提供了分类异常的引人入胜的叙述，并实现了前所未有的透明度和可解释性水平。

    

    本文展示了张量网络如何帮助发展可解释的机器学习算法。具体而言，我们基于矩阵乘积状态（MPS）开发了一种无监督聚类算法，并将其应用于实际使用案例中的对手生成的威胁情报。我们的研究证明，MPS在性能方面可以与传统的深度学习模型如自编码器和生成对抗网络相媲美，同时提供更丰富的模型可解释性。我们的方法自然地促进了特征概率、冯·诺伊曼熵和互信息的提取，为异常分类提供了引人入胜的叙述，并促进了前所未有的透明度和可解释性水平，这对于理解人工智能决策的基本原理至关重要。

    In this paper we show how tensor networks help in developing explainability of machine learning algorithms. Specifically, we develop an unsupervised clustering algorithm based on Matrix Product States (MPS) and apply it in the context of a real use-case of adversary-generated threat intelligence. Our investigation proves that MPS rival traditional deep learning models such as autoencoders and GANs in terms of performance, while providing much richer model interpretability. Our approach naturally facilitates the extraction of feature-wise probabilities, Von Neumann Entropy, and mutual information, offering a compelling narrative for classification of anomalies and fostering an unprecedented level of transparency and interpretability, something fundamental to understand the rationale behind artificial intelligence decisions.
    
[^71]: 为元宇宙进行联合多视角合成

    Federated Multi-View Synthesizing for Metaverse. (arXiv:2401.00859v1 [eess.IV])

    [http://arxiv.org/abs/2401.00859](http://arxiv.org/abs/2401.00859)

    本论文提出了一种用于元宇宙的联合多视角合成框架，通过三维感知的生成模型和联合学习方法，实现了高效的无线内容传递，满足元宇宙中严格的服务质量要求。

    

    元宇宙预计能提供沉浸式的娱乐、教育和商业应用。然而，虚拟现实（VR）在无线网络上的传输需要大量的数据和计算资源，因此需要引入满足严格的服务质量要求的新解决方案。通过边缘智能和深度学习的最新进展，我们开发了一种新的多视角合成框架，可以为元宇宙中的无线内容传递提供高效的计算、存储和通信资源。我们提出了一个三维感知的生成模型，利用单视角图像集合。这些单视角图像被传输给一组具有重叠视野的用户，相比传输瓦片或整个三维模型，可以避免大量的内容传输。然后，我们提出了一种联合学习方法，以保证高效的学习过程。通过表征垂直方向上的特征，可以提高训练性能。

    The metaverse is expected to provide immersive entertainment, education, and business applications. However, virtual reality (VR) transmission over wireless networks is data- and computation-intensive, making it critical to introduce novel solutions that meet stringent quality-of-service requirements. With recent advances in edge intelligence and deep learning, we have developed a novel multi-view synthesizing framework that can efficiently provide computation, storage, and communication resources for wireless content delivery in the metaverse. We propose a three-dimensional (3D)-aware generative model that uses collections of single-view images. These single-view images are transmitted to a group of users with overlapping fields of view, which avoids massive content transmission compared to transmitting tiles or whole 3D models. We then present a federated learning approach to guarantee an efficient learning process. The training performance can be improved by characterizing the verti
    
[^72]: 排放报告成熟度模型：通过性能指标和人工智能支持城市利用排放相关流程

    Emissions Reporting Maturity Model: supporting cities to leverage emissions-related processes through performance indicators and artificial intelligence. (arXiv:2401.00857v1 [cs.CY])

    [http://arxiv.org/abs/2401.00857](http://arxiv.org/abs/2401.00857)

    该论文提出了一个排放报告成熟度模型(ERMM)，通过使用性能指标和人工智能技术，支持城市解决气候变化和全球变暖所带来的挑战。

    

    自从“环境与发展会议”(Eco-92)以来，气候变化和全球变暖一直是全球热门话题。然而，在减少温室气体方面进展甚微。排放问题和挑战复杂，需要集中而全面的努力来解决。排放报告是温室气体减排政策的关键组成部分，因此是本研究的重点。本研究的主要目标是两方面：(一)提出一个排放报告评估模型，以提高排放报告的整体质量；(二)利用人工智能支持改善排放报告的倡议。因此，本研究提出了一个排放报告成熟度模型(ERMM)，用于检查、聚类和分析排放报告倡议的数据，并帮助城市应对气候变化和全球变暖挑战。

    Climate change and global warming have been trending topics worldwide since the Eco-92 conference. However, little progress has been made in reducing greenhouse gases (GHGs). The problems and challenges related to emissions are complex and require a concerted and comprehensive effort to address them. Emissions reporting is a critical component of GHG reduction policy and is therefore the focus of this work. The main goal of this work is two-fold: (i) to propose an emission reporting evaluation model to leverage emissions reporting overall quality and (ii) to use artificial intelligence (AI) to support the initiatives that improve emissions reporting. Thus, this work presents an Emissions Reporting Maturity Model (ERMM) for examining, clustering, and analysing data from emissions reporting initiatives to help the cities to deal with climate change and global warming challenges. The Performance Indicator Development Process (PIDP) proposed in this work provides ways to leverage the quali
    
[^73]: 在晶体材料研究中，将协方差和表达能力融合为深度哈密顿回归：一种混合级联回归框架

    Harmonizing Covariance and Expressiveness for Deep Hamiltonian Regression in Crystalline Material Research: a Hybrid Cascaded Regression Framework. (arXiv:2401.00744v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2401.00744](http://arxiv.org/abs/2401.00744)

    在深度哈密顿回归中，实现协方差和网络表达能力之间的平衡一直是一个挑战。本文提出了一个混合级联回归框架，在第一阶段通过协变神经网络建模并产生协变特征和基线预测，辅助第二阶段学习协方差。同时，第二阶段使用非线性图形Transformer网络进行结构建模，提高了哈密顿预测的表达能力。

    

    在材料研究中，深度学习用于哈密顿回归量子系统需要满足协方差定律，其中实现SO(3)等变性而不损失网络的表达能力是一个难以解决的挑战，因为非线性映射的理论等变性保证受限。为了解决协方差-表达能力困境，我们提出了一种混合框架，分为两个级联回归阶段。第一阶段使用一个理论上保证的协变神经网络来建模三维原子系统的对称性，产生理论上的协变特征和基线哈密顿预测，帮助第二阶段学习协变性。同时，第二阶段使用我们提出的非线性三维图形Transformer网络来进行三维原子系统的结构建模，将第一阶段的输出精细化为具有更好表达能力的哈密顿预测。通过理论上的协变性和更好的表达能力，实现了对哈密顿回归的改进。

    Deep learning for Hamiltonian regression of quantum systems in material research necessitates satisfying the covariance laws, among which achieving SO(3)-equivariance without sacrificing the expressiveness of networks remains an elusive challenge due to the restriction to non-linear mappings on guaranteeing theoretical equivariance. To alleviate the covariance-expressiveness dilemma, we propose a hybrid framework with two cascaded regression stages. The first stage, with a theoretically-guaranteed covariant neural network modeling symmetry properties of 3D atom systems, yields theoretically covariant features and baseline Hamiltonian predictions, assisting the second stage in learning covariance. Meanwhile, the second stage, powered by a non-linear 3D graph Transformer network we propose for structural modeling of 3D atomic systems, refines the first stage's output as a fine-grained prediction of Hamiltonians with better expressiveness capability. The combination of a theoretically cov
    
[^74]: 智能交通系统中图神经网络的调查

    A Survey on Graph Neural Networks in Intelligent Transportation Systems. (arXiv:2401.00713v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.00713](http://arxiv.org/abs/2401.00713)

    本研究调查了智能交通系统中图神经网络的应用。图神经网络通过其强大的图建模能力，在交通领域表现出优秀性能。然而，目前研究主要集中在交通预测方面，其他领域仍需更多关注。

    

    智能交通系统（ITS）对于改善交通拥堵、减少交通事故、优化城市规划等方面至关重要。然而，由于交通网络的复杂性，传统的机器学习和统计方法变得不受重视。随着人工智能时代的到来，许多深度学习框架在各个领域取得了显著进展，并且被认为是许多领域中有效的方法。作为一种深度学习方法，图神经网络（GNNs）由于其在建模与图相关的问题方面的强大能力，自2019年以来已经在ITS领域中崭露头角。因此，越来越多的学者关注GNN在交通领域的应用，表现出优秀的性能。然而，目前该领域的大部分研究仍集中在交通预测方面，而其他ITS领域，如自动驾驶车辆和城市规划，仍需要更多关注。

    Intelligent Transportation System (ITS) is vital in improving traffic congestion, reducing traffic accidents, optimizing urban planning, etc. However, due to the complexity of the traffic network, traditional machine learning and statistical methods are relegated to the background. With the advent of the artificial intelligence era, many deep learning frameworks have made remarkable progress in various fields and are now considered effective methods in many areas. As a deep learning method, Graph Neural Networks (GNNs) have emerged as a highly competitive method in the ITS field since 2019 due to their strong ability to model graph-related problems. As a result, more and more scholars pay attention to the applications of GNNs in transportation domains, which have shown excellent performance. However, most of the research in this area is still concentrated on traffic forecasting, while other ITS domains, such as autonomous vehicles and urban planning, still require more attention. This 
    
[^75]: 火燃科学中使用基础模型的可靠知识处理框架

    A Reliable Knowledge Processing Framework for Combustion Science using Foundation Models. (arXiv:2401.00544v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2401.00544](http://arxiv.org/abs/2401.00544)

    本研究介绍了一种可靠的知识处理框架，将大型语言模型整合到燃烧科学中。该框架通过使用基础模型和RAG框架，处理多样化的燃烧研究数据，最大限度地减少计算和经济开销，同时优化数据隐私和准确性。

    

    本研究探讨将大型语言模型（LLMs）整合到科学数据融合中，以燃烧科学为案例研究。通过整合基础模型与检索增强生成（RAG）框架，本研究介绍了一种处理多样化燃烧研究数据的方法，涵盖实验研究、模拟和文献等方面。燃烧研究的多方面性强调了知识处理在从丰富的、多样化的信息来源中导航和提取有价值信息中的关键作用。所开发的方法在优化数据隐私和准确性的同时，最大限度地减少了计算和经济开销。它包括提示工程和离线开源LLMs，为用户选择基础模型提供了自主性。本研究对文本分割策略进行了全面的研究，进行了LLMs之间的比较研究，并探索了各种优化的提示方式，以证明其有效性。

    This research explores the integration of large language models (LLMs) into scientific data assimilation, focusing on combustion science as a case study. Leveraging foundational models integrated with Retrieval-Augmented Generation (RAG) framework, the study introduces an approach to process diverse combustion research data, spanning experimental studies, simulations, and literature. The multifaceted nature of combustion research emphasizes the critical role of knowledge processing in navigating and extracting valuable information from a vast and diverse pool of sources. The developed approach minimizes computational and economic expenses while optimizing data privacy and accuracy. It incorporates prompt engineering and offline open-source LLMs, offering user autonomy in selecting base models. The study provides a thorough examination of text segmentation strategies, conducts comparative studies between LLMs, and explores various optimized prompts to demonstrate the effectiveness of th
    
[^76]: 迈向NextG协议形式验证的自动建模：一种多模态交叉和自注意力的大语言模型方法

    Towards Auto-Modeling of Formal Verification for NextG Protocols: A Multimodal cross- and self-attention Large Language Model Approach. (arXiv:2312.17353v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2312.17353](http://arxiv.org/abs/2312.17353)

    AVRE是一种用于形式验证NextG通信协议的自动建模系统，利用大型语言模型转化协议描述为依赖图和形式模型，并通过交叉和自注意力机制建立可量化的依赖关系，进而提高了复杂通信协议的验证准确性和相关性。

    

    本文介绍了Auto-modeling of Formal Verification with Real-world Prompting for 5G and NextG protocols（AVRE），这是一种新颖的系统，设计用于形式验证Next Generation（NextG）通信协议，解决了网络协议设计和验证中日益复杂和可扩展性挑战。利用大型语言模型（LLM），AVRE将协议描述转化为依赖图和形式模型，高效地解决了歧义问题并捕捉设计意图。系统通过交叉和自注意力机制，将变压器模型与LLM集成，自主建立可量化的依赖关系。经过HyFuzz实验平台的迭代反馈，AVRE显著提高了复杂通信协议正式验证的准确性和相关性，为验证复杂的通信系统提供了一种突破性的方法。我们将CAL的性能与最先进的LL进行了比较。

    This paper introduces Auto-modeling of Formal Verification with Real-world Prompting for 5G and NextG protocols (AVRE), a novel system designed for the formal verification of Next Generation (NextG) communication protocols, addressing the increasing complexity and scalability challenges in network protocol design and verification. Utilizing Large Language Models (LLMs), AVRE transforms protocol descriptions into dependency graphs and formal models, efficiently resolving ambiguities and capturing design intent. The system integrates a transformer model with LLMs to autonomously establish quantifiable dependency relationships through cross- and self-attention mechanisms. Enhanced by iterative feedback from the HyFuzz experimental platform, AVRE significantly advances the accuracy and relevance of formal verification in complex communication protocols, offering a groundbreaking approach to validating sophisticated communication systems. We compare CAL's performance with state-of-the-art L
    
[^77]: 在潜在空间中通过领域不变表示学习改善入侵检测

    Improving Intrusion Detection with Domain-Invariant Representation Learning in Latent Space. (arXiv:2312.17300v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2312.17300](http://arxiv.org/abs/2312.17300)

    本研究提出了一种使用多任务学习的两阶段表示学习技术，通过培养潜在空间中的特征，包括本地和跨领域特征，以增强对未知分布领域的泛化效果。此外，通过最小化先验和潜在空间之间的互信息来分离潜在空间，并且在多个网络安全数据集上评估了模型的效能。

    

    领域泛化聚焦于利用来自具有丰富训练数据和标签的多个相关领域的知识，增强对未知分布（IN）和超出分布（OOD）领域的推理。在我们的研究中，我们引入了一种两阶段表示学习技术，使用多任务学习。这种方法旨在从跨越多个领域的特征中培养一个潜在空间，包括本地和跨领域，以增强对IN和OOD领域的泛化。此外，我们尝试通过最小化先验与潜在空间之间的互信息来分离潜在空间，有效消除虚假特征相关性。综合而言，联合优化将促进领域不变特征学习。我们使用标准分类指标评估模型在多个网络安全数据集上的效能，对比了现代领域泛化方法的结果。

    Domain generalization focuses on leveraging knowledge from multiple related domains with ample training data and labels to enhance inference on unseen in-distribution (IN) and out-of-distribution (OOD) domains. In our study, we introduce a two-phase representation learning technique using multi-task learning. This approach aims to cultivate a latent space from features spanning multiple domains, encompassing both native and cross-domains, to amplify generalization to IN and OOD territories. Additionally, we attempt to disentangle the latent space by minimizing the mutual information between the prior and latent space, effectively de-correlating spurious feature correlations. Collectively, the joint optimization will facilitate domain-invariant feature learning. We assess the model's efficacy across multiple cybersecurity datasets, using standard classification metrics on both unseen IN and OOD sets, and juxtapose the results with contemporary domain generalization methods.
    
[^78]: EyePreserve: 保持身份的虹膜合成

    EyePreserve: Identity-Preserving Iris Synthesis. (arXiv:2312.12028v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.12028](http://arxiv.org/abs/2312.12028)

    本论文提出了一种完全数据驱动的、保持身份的、瞳孔尺寸变化的虹膜图像合成方法，能够合成不同瞳孔尺寸的虹膜图像，代表不存在的身份，并能够在保持身份的同时进行非线性纹理变形。

    

    在广泛的瞳孔尺寸范围内保持身份的同身份生物特征虹膜图像的合成是复杂的，因为它涉及到虹膜肌肉收缩机制，需要将虹膜非线性纹理变形模型嵌入到合成流程中。本论文提出了一种完全数据驱动的、保持身份的、瞳孔尺寸变化的虹膜图像合成方法。这种方法能够合成具有不同瞳孔尺寸的虹膜图像，代表不存在的身份，并能够在给定目标虹膜图像的分割掩膜下非线性地变形现有主体的虹膜图像纹理。虹膜识别实验表明，所提出的变形模型不仅在改变瞳孔尺寸时保持身份，而且在瞳孔尺寸有显著差异的同身份虹膜样本之间提供更好的相似度，与最先进的线性方法相比。

    Synthesis of same-identity biometric iris images, both for existing and non-existing identities while preserving the identity across a wide range of pupil sizes, is complex due to intricate iris muscle constriction mechanism, requiring a precise model of iris non-linear texture deformations to be embedded into the synthesis pipeline. This paper presents the first method of fully data-driven, identity-preserving, pupil size-varying s ynthesis of iris images. This approach is capable of synthesizing images of irises with different pupil sizes representing non-existing identities as well as non-linearly deforming the texture of iris images of existing subjects given the segmentation mask of the target iris image. Iris recognition experiments suggest that the proposed deformation model not only preserves the identity when changing the pupil size but offers better similarity between same-identity iris samples with significant differences in pupil size, compared to state-of-the-art linear an
    
[^79]: 混合内模：通过模拟机器人响应学习敏捷腿部运动

    Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated Robot Response. (arXiv:2312.11460v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2312.11460](http://arxiv.org/abs/2312.11460)

    本论文提出了一种混合内模方法，通过模拟机器人的响应来估计外部状态，这对于健壮的运动控制非常重要。使用对比学习优化嵌入表示，使其接近机器人的后继状态。这种方法只需要机器人的固有感知。

    

    健壮的运动控制依赖于准确的状态估计。然而，大多数四足机器人的传感器只能提供部分和嘈杂的观测，使得估计特别具有挑战性，特别是对于外部状态，如地形摩擦和高程图。受经典的内模控制原理的启发，我们将这些外部状态视为干扰，并引入混合内模（HIM）来根据机器人的响应来估计它们。我们将响应称为混合内嵌表示，它包含了机器人的显式速度和隐式稳定性表示，分别对应于运动任务的两个主要目标：显式追踪速度和隐式维持稳定性。我们使用对比学习来优化嵌入式表示，使其接近机器人的后继状态，其中自然嵌入了响应。HIM具有几个吸引人的好处：它只需要机器人的固有感知，即关节感知。

    Robust locomotion control depends on accurate state estimations. However, the sensors of most legged robots can only provide partial and noisy observations, making the estimation particularly challenging, especially for external states like terrain frictions and elevation maps. Inspired by the classical Internal Model Control principle, we consider these external states as disturbances and introduce Hybrid Internal Model (HIM) to estimate them according to the response of the robot. The response, which we refer to as the hybrid internal embedding, contains the robot's explicit velocity and implicit stability representation, corresponding to two primary goals for locomotion tasks: explicitly tracking velocity and implicitly maintaining stability. We use contrastive learning to optimize the embedding to be close to the robot's successor state, in which the response is naturally embedded. HIM has several appealing benefits: It only needs the robot's proprioceptions, i.e., those from joint
    
[^80]: 单一GPU上的数据高效多模态融合

    Data-Efficient Multimodal Fusion on a Single GPU. (arXiv:2312.10144v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.10144](http://arxiv.org/abs/2312.10144)

    本论文提出了一种在单一GPU上进行数据高效多模态融合的方法，通过使用预训练的单模态编码器的潜在空间，我们在多模态对齐中取得了有竞争力的性能，且计算和数据量减少了数个数量级。

    

    多模态对齐的目标是学习共享多模态输入之间的单一潜在空间。在这个领域中，最强大的模型通常是使用大规模数据集和大规模计算资源进行训练的，因此在许多实际场景中训练这些模型的成本非常高昂。我们推测，现有的在大量单模态数据上预训练的单模态编码器应该能够以更低的成本从单模态模型中创建多模态模型。因此，我们提出了FuseMix，一种多模态增强方案，该方案在任意预训练的单模态编码器的潜在空间中操作。通过使用FuseMix进行多模态对齐，我们在图像-文本和音频-文本检索任务中取得了有竞争力的性能，并在某些情况下超越了最先进的方法，而计算和数据量减少了数个数量级：例如，我们在Flickr30K的文本-图像检索任务中比CLIP的性能提高了约600倍，而计算和数据量减少了数个数量级。

    The goal of multimodal alignment is to learn a single latent space that is shared between multimodal inputs. The most powerful models in this space have been trained using massive datasets of paired inputs and large-scale computational resources, making them prohibitively expensive to train in many practical scenarios. We surmise that existing unimodal encoders pre-trained on large amounts of unimodal data should provide an effective bootstrap to create multimodal models from unimodal ones at much lower costs. We therefore propose FuseMix, a multimodal augmentation scheme that operates on the latent spaces of arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal alignment, we achieve competitive performance -- and in certain cases outperform state-of-the art methods -- in both image-text and audio-text retrieval, with orders of magnitude less compute and data: for example, we outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \! 600\times$ fewer GP
    
[^81]: 消除差距：基于模型预测控制的可验证模型无关二次规划控制器的学习

    Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming Controllers Inspired by Model Predictive Control. (arXiv:2312.05332v3 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2312.05332](http://arxiv.org/abs/2312.05332)

    本文提出了一种新的参数化控制器类，利用深度强化学习训练其控制器的参数，从而消除了常见控制器中的可验证性和性能保证的限制。该控制器类似于模型预测控制问题的二次规划求解器，具有可验证的属性，并且在控制性能和鲁棒性方面与其他控制器相媲美。同时，该控制器的计算效率显著优于传统的模型预测控制。

    

    本文介绍了一种新的参数化控制器类，受到模型预测控制（MPC）的启发。该控制器类似于线性MPC问题的二次规划（QP）求解器，但控制器的参数是通过深度强化学习（DRL）进行训练，而不是从系统模型中推导出来的。这种方法解决了常见控制器中使用MLP或其他通用神经网络架构的DRL的可验证性和性能保证的局限性，并且所学习的控制器具有与MPC类似的持续可行性和渐近稳定性等可验证属性。另一方面，数值实验表明，所提出的控制器在控制性能上与MPC和MLP控制器相匹配，并且对建模不确定性和噪声具有更优的鲁棒性。此外，所提出的控制器在计算效率上明显优于MPC。

    In this paper, we introduce a new class of parameterized controllers, drawing inspiration from Model Predictive Control (MPC). The controller resembles a Quadratic Programming (QP) solver of a linear MPC problem, with the parameters of the controller being trained via Deep Reinforcement Learning (DRL) rather than derived from system models. This approach addresses the limitations of common controllers with Multi-Layer Perceptron (MLP) or other general neural network architecture used in DRL, in terms of verifiability and performance guarantees, and the learned controllers possess verifiable properties like persistent feasibility and asymptotic stability akin to MPC. On the other hand, numerical examples illustrate that the proposed controller empirically matches MPC and MLP controllers in terms of control performance and has superior robustness against modeling uncertainty and noises. Furthermore, the proposed controller is significantly more computationally efficient compared to MPC a
    
[^82]: 论语言模型的水印可学习性研究

    On the Learnability of Watermarks for Language Models. (arXiv:2312.04469v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.04469](http://arxiv.org/abs/2312.04469)

    该论文研究了语言模型水印的可学习性，提出了水印蒸馏方法，通过训练学生模型使其模仿使用解码水印的教师模型的行为。结果表明，语言模型具有直接学习生成水印的能力，这对于水印的实际应用具有重要影响。

    

    语言模型输出的水印可以实现对模型生成文本的统计检测，广泛应用于语言模型的负责任部署中。现有的水印策略通过改变现有语言模型的解码器来操作，而语言模型直接学习生成水印的能力将对水印在实际应用中产生重要影响。首先，学习得到的水印可以用于构建能自然生成带水印文本的开放模型，使开放模型也能从水印中受益。其次，如果水印用于确定生成文本的来源，攻击者可以通过伪造水印并生成有害的带水印文本来损害受害模型的声誉。为了研究水印的可学习性，我们提出了水印蒸馏方法，该方法通过训练学生模型使其行为类似于使用基于解码的水印的教师模型。我们在三个实验数据集上测试了我们的方法。

    Watermarking of language model outputs enables statistical detection of model-generated text, which has many applications in the responsible deployment of language models. Existing watermarking strategies operate by altering the decoder of an existing language model, and the ability for a language model to directly learn to generate the watermark would have significant implications for the real-world deployment of watermarks. First, learned watermarks could be used to build open models that naturally generate watermarked text, allowing for open models to benefit from watermarking. Second, if watermarking is used to determine the provenance of generated text, an adversary can hurt the reputation of a victim model by spoofing its watermark and generating damaging watermarked text. To investigate the learnability of watermarks, we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking. We test our approach on three
    
[^83]: 关于上下文学习的校准研究

    A Study on the Calibration of In-context Learning. (arXiv:2312.04021v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.04021](http://arxiv.org/abs/2312.04021)

    本研究关注上下文学习（ICL），通过定制提示来调整静态语言模型（LMs），研究了在各种自然语言理解和推理任务中性能和校准之间的平衡。研究发现随着ICL示例数量的增加，模型的校准会先增加而后得到改善，而校准误差主要出现在低样本场景下。此外，微调和CoT提示等方法可能导致校准误差和不可靠的自然语言解释，提示需要针对可靠性场景开发新的方法。

    

    准确的不确定性量化对于语言模型（LMs）的安全部署至关重要，以前的研究已经证明了现代LMs校准性的改进。我们的研究重点是上下文学习（ICL），一种通过定制提示来调整静态LMs的常见方法，并研究在广泛的自然语言理解和推理任务中性能和校准之间的平衡。通过全面的实验，我们观察到，随着ICL示例数量的增加，模型最初会出现增加的校准误差，然后才能实现更好的校准，而校准误差往往在低样本场景下出现。此外，我们发现以提高可用性为目标的方法，如微调和CoT提示，可能导致校准误差和不可靠的自然语言解释，这表明在期望模型可靠性的场景中可能需要新的方法。

    Accurate uncertainty quantification is crucial for the safe deployment of language models (LMs), and prior research has demonstrated improvements in the calibration of modern LMs. Our study focuses on in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examines the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations, suggesting that new methods may be required for scenarios where models are expected to be reliable.
    
[^84]: OpenVoice: 多功能即时语音克隆

    OpenVoice: Versatile Instant Voice Cloning. (arXiv:2312.01479v5 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2312.01479](http://arxiv.org/abs/2312.01479)

    OpenVoice是一种多功能的语音克隆方法，可以复制参考讲话者的声音并在多种语言中生成语音。它具有灵活的语音风格控制和零样本跨语言语音克隆的能力。

    

    本文介绍了OpenVoice，一种多功能语音克隆方法，只需一小段参考讲话者的音频片段即可复制他们的声音并生成多种语言的语音。OpenVoice在以下领域中解决了一些开放性挑战：1）灵活的语音风格控制。OpenVoice可以精确控制语音风格，包括情感、口音、节奏、停顿和语调，同时复制参考讲话者的音色。之前的方法无法在克隆后灵活操控语音风格。2）零样本跨语言语音克隆。OpenVoice可以在未包含在大规模训练集中的语言上实现零样本的跨语言语音克隆。与之前的方法不同，之前的方法通常需要包含所有语言的大规模多讲话者多语言(MSML)数据集。

    We introduce OpenVoice, a versatile voice cloning approach that requires only a short audio clip from the reference speaker to replicate their voice and generate speech in multiple languages. OpenVoice represents a significant advancement in addressing the following open challenges in the field: 1) Flexible Voice Style Control. OpenVoice enables granular control over voice styles, including emotion, accent, rhythm, pauses, and intonation, in addition to replicating the tone color of the reference speaker. The voice styles are not directly copied from and constrained by the style of the reference speaker. Previous approaches lacked the ability to flexibly manipulate voice styles after cloning. 2) Zero-Shot Cross-Lingual Voice Cloning. OpenVoice achieves zero-shot cross-lingual voice cloning for languages not included in the massive-speaker training set. Unlike previous approaches, which typically require extensive massive-speaker multi-lingual (MSML) dataset for all languages, OpenVoice
    
[^85]: SASSL:通过神经风格迁移增强自监督学习

    SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer. (arXiv:2312.01187v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.01187](http://arxiv.org/abs/2312.01187)

    SASSL提出了一种基于神经风格迁移的增强技术，通过解耦语义和风格属性，在自监督学习中生成多样化的增强样本，从而提升了图像分类性能。

    

    自监督学习依赖于数据增强来从无标签图像中提取有意义的表征。现有的最先进的增强流水线包括了各种原始的转换，但通常忽略了自然图像的结构。因此，增强样本可能显示出退化的语义信息和低风格多样性，从而影响到自监督表征的下游性能。为了克服这个问题，我们提出了一种名为SASSL的新型增强技术，它基于神经风格迁移。该方法将图像中的语义和风格属性解耦，并仅对风格应用转换，保持内容，生成多样化的增强样本，更好地保留它们的语义属性。实验结果显示，与广为接受的MoCo v2相比，我们的技术在ImageNet上的top-1分类性能提升超过2%。

    Self-supervised learning relies heavily on data augmentation to extract meaningful representations from unlabeled images. While existing state-of-the-art augmentation pipelines incorporate a wide range of primitive transformations, these often disregard natural image structure. Thus, augmented samples can exhibit degraded semantic information and low stylistic diversity, affecting downstream performance of self-supervised representations. To overcome this, we propose SASSL: Style Augmentations for Self Supervised Learning, a novel augmentation technique based on Neural Style Transfer. The method decouples semantic and stylistic attributes in images and applies transformations exclusively to the style while preserving content, generating diverse augmented samples that better retain their semantic properties. Experimental results show our technique achieves a top-1 classification performance improvement of more than 2% on ImageNet compared to the well-established MoCo v2. We also measure
    
[^86]: DeepTreeGANv2：迭代池化的点云生成网络

    DeepTreeGANv2: Iterative Pooling of Point Clouds. (arXiv:2312.00042v2 [physics.data-an] UPDATED)

    [http://arxiv.org/abs/2312.00042](http://arxiv.org/abs/2312.00042)

    这项工作介绍了DeepTreeGANv2，它是DeepTreeGAN的显著扩展，能够以树状方式迭代地聚合点云，模拟粒子与探测器的相互作用，实现在短时间内生成大型点云的目标。

    

    在高能物理学中，为了模拟粒子与探测器的相互作用，需要进行详细而耗时的模拟。为了通过生成模型绕过这些模拟，需要在短时间内生成大型点云，同时正确建模粒子之间的复杂依赖关系。粒子阵列固有地基于树状过程，因为每个粒子是上一代粒子的衰变或探测器相互作用产生的。在这项工作中，我们提出了DeepTreeGANv2，这是DeepTreeGAN的显著扩展，具有一个评论者，能够以树状方式迭代地聚合这样的点云。我们展示了该模型能够重现复杂分布，并在公共的JetNet 150数据集上评估了其性能。

    In High Energy Physics, detailed and time-consuming simulations are used for particle interactions with detectors. To bypass these simulations with a generative model, the generation of large point clouds in a short time is required, while the complex dependencies between the particles must be correctly modelled. Particle showers are inherently tree-based processes, as each particle is produced by the decay or detector interaction of a particle of the previous generation. In this work, we present a significant extension to DeepTreeGAN, featuring a critic, that is able to aggregate such point clouds iteratively in a tree-based manner. We show that this model can reproduce complex distributions, and we evaluate its performance on the public JetNet 150 dataset.
    
[^87]: 实时在线股票预测利用综合定量和定性分析

    Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and Qualitative Analysis. (arXiv:2311.15218v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.15218](http://arxiv.org/abs/2311.15218)

    本研究提供了一个综合定量和定性分析的实时在线股票预测方法，通过提供一个包含了来自各种来源的数据集，将数字股票数据和定性文本数据结合起来进行分析。数据集包含了多个公司和道琼斯工业平均指数的数据，为训练提供了有效的数据基础。

    

    机器学习在金融领域的应用已经变得非常常见，尤其在股票市场预测中更是如此。股票市场高度波动，全球每分钟都会产生大量数据。从这些数据中提取有效的智能信息十分重要。然而，将数字股票数据与定性文本数据结合起来可能是一项具有挑战性的任务。在这项工作中，我们通过提供一个史无前例的公开可用数据集，从新闻档案、电视新闻字幕、广播文本、推文、每日财经报纸等处收集到了包括技术和基本数据以及情感数据。用于情感提取的文本数据总共超过140万条。该数据集包含从2018年1月到2022年12月为期一年的八家代表不同产业部门的公司的每日数据，以及道琼斯工业平均指数（DJIA）整体的数据。综合的基本数据和技术数据可直接用于训练。

    The application of Machine learning to finance has become a familiar approach, even more so in stock market forecasting. The stock market is highly volatile, and huge amounts of data are generated every minute globally. The extraction of effective intelligence from this data is of critical importance. However, a collaboration of numerical stock data with qualitative text data can be a challenging task. In this work, we accomplish this by providing an unprecedented, publicly available dataset with technical and fundamental data and sentiment that we gathered from news archives, TV news captions, radio transcripts, tweets, daily financial newspapers, etc. The text data entries used for sentiment extraction total more than 1.4 Million. The dataset consists of daily entries from January 2018 to December 2022 for eight companies representing diverse industrial sectors and the Dow Jones Industrial Average (DJIA) as a whole. Holistic Fundamental and Technical data is provided training ready f
    
[^88]: Jina Embeddings 2: 面向长篇文档的8192-Token通用文本嵌入模型

    Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])

    [http://arxiv.org/abs/2310.19923](http://arxiv.org/abs/2310.19923)

    Jina Embeddings 2是一个能够处理长篇文档的文本嵌入模型，突破了传统512个标记限制，提供了高达8192个标记的容量。

    

    文本嵌入模型已经成为将句子转化为固定大小特征向量的强大工具，这些向量包含了语义信息。尽管这些模型对于信息检索、语义聚类和文本重排序等任务至关重要，但大多数现有的开源模型，尤其是基于BERT等架构构建的模型，难以表示长篇文档，并且常常会进行截断。为了缓解这个挑战，一种常见的方法是将文档分割成更小的段落进行嵌入。然而，这种策略会导致更大的向量集合，进而增加内存消耗，并且在向量搜索时会出现计算密集和延迟升高的问题。为了解决这些挑战，我们介绍了Jina Embeddings 2，这是一个开源的文本嵌入模型，可以容纳高达8192个标记。该模型旨在突破传统的512个标记限制，能够灵活处理长篇文档。

    Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
    
[^89]: 从挫折中获得智慧：通过错误分析对齐大型语言模型

    Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10477](http://arxiv.org/abs/2310.10477)

    该论文介绍了一种基于错误分析的对齐策略，通过暴露大型语言模型的错误输出并进行评估，以理解内部原因。通过这种方法，有毒回应可以转化为模型对齐的指导调谐语料，从而提高模型的安全性并训练其进行自我批评。

    

    大型语言模型（LLMs）的快速发展既带来了机遇，也带来了挑战，特别是在意外生成有害和有毒回应方面。传统的对齐方法致力于引导LLMs朝着期望的性能发展并保护它们免受恶意内容的侵害，而本研究提出了一种基于错误分析的全新对齐策略，通过有意暴露LLMs的缺陷输出并进行深入评估，以完全理解内部原因，通过自然语言分析。因此，有毒回应可以转化为模型对齐的指导调谐语料，LLMs不仅可以避免生成有缺陷的回应，还可以训练其进行自我批评，发挥其辨别有毒内容的内在能力。实验结果表明，所提出的方法在安全指令遵循方面优于传统的对齐技术，同时还保持了卓越的效率。

    The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
    
[^90]: 一种深度神经网络和机制混合模型用于预测大鼠的药代动力学

    A Deep Neural Network -- Mechanistic Hybrid Model to Predict Pharmacokinetics in Rat. (arXiv:2310.09167v1 [q-bio.QM])

    [http://arxiv.org/abs/2310.09167](http://arxiv.org/abs/2310.09167)

    该论文提出了一种深度神经网络和机制混合模型，用于预测大鼠的药代动力学。通过训练更大的数据集、改进网络架构和参数化机制模型，成功减小了口服和静脉给药的误差，并将这种方法扩展到预测更多终点和处理不同的协变量。

    

    在小分子药物或农药的研发中，重要的一个方面就是它们在静脉和口服给药后的全身可用性。从候选化合物的化学结构预测全身可用性非常有价值，因为它可以让药物或农药的研发集中在具有良好动力学特性的化合物上。然而，这样的预测具有挑战性，因为可用性是分子性质、生物学和生理学之间复杂相互作用的结果，而训练数据非常稀缺。在这项工作中，我们改进了先前开发的混合模型[34]。我们将总的口服暴露的中值折叠误差从2.85降低到2.35，将静脉给药的误差从1.95降低到1.62。这是通过在更大的数据集上进行训练，改进神经网络架构以及机制模型的参数化实现的。此外，我们扩展了我们的方法来预测其他终点和处理不同的协变量。

    An important aspect in the development of small molecules as drugs or agro-chemicals is their systemic availability after intravenous and oral administration.The prediction of the systemic availability from the chemical structure of a poten-tial candidate is highly desirable, as it allows to focus the drug or agrochemicaldevelopment on compounds with a favorable kinetic profile. However, such pre-dictions are challenging as the availability is the result of the complex interplaybetween molecular properties, biology and physiology and training data is rare.In this work we improve the hybrid model developed earlier [34]. We reducethe median fold change error for the total oral exposure from 2.85 to 2.35 andfor intravenous administration from 1.95 to 1.62. This is achieved by trainingon a larger data set, improving the neural network architecture as well as theparametrization of mechanistic model. Further, we extend our approach to predictadditional endpoints and to handle different covar
    
[^91]: 记忆健身房：对内存为基础的智能体在无尽任务中的部分可观察挑战

    Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes. (arXiv:2309.17207v1 [cs.LG])

    [http://arxiv.org/abs/2309.17207](http://arxiv.org/abs/2309.17207)

    本研究提出了记忆健身房，一种用于测试利用记忆为基础的深度强化学习智能体能力的基准。它包括部分可观察的二维环境和离散控制，并通过无尽任务对记忆能力、噪声抗性和泛化能力进行评估。研究还提供了一个使用Transformer-XL和Proximal Policy Optimization驱动的实现。

    

    记忆健身房介绍了一个独特的基准测试，旨在测试深度强化学习智能体，特别是将门循环单元(GRU)与Transformer-XL(TrXL)相比，它们对于记忆长序列的能力、抗噪声和泛化能力。它采用了部分可观察的二维环境和离散控制，即Mortar Mayhem、Mystery Path和Searing Spotlights。这些最初是有限的环境被推广为新颖的无尽任务，作为一种自动课程，从车游戏"I packed my bag"中汲取灵感。这些无尽任务不仅有助于评估效率，而且有趣地评估了记忆为基础的方法的有效性。鉴于现有公开可用的记忆基准的稀缺性，我们提供了一个由TrXL和Proximal Policy Optimization驱动的实现。本实现利用TrXL作为以滑动窗口方法使用的情节性记忆。在有限环境的实验中，我们发现...

    Memory Gym introduces a unique benchmark designed to test Deep Reinforcement Learning agents, specifically comparing Gated Recurrent Unit (GRU) against Transformer-XL (TrXL), on their ability to memorize long sequences, withstand noise, and generalize. It features partially observable 2D environments with discrete controls, namely Mortar Mayhem, Mystery Path, and Searing Spotlights. These originally finite environments are extrapolated to novel endless tasks that act as an automatic curriculum, drawing inspiration from the car game ``I packed my bag". These endless tasks are not only beneficial for evaluating efficiency but also intriguingly valuable for assessing the effectiveness of approaches in memory-based agents. Given the scarcity of publicly available memory baselines, we contribute an implementation driven by TrXL and Proximal Policy Optimization. This implementation leverages TrXL as episodic memory using a sliding window approach. In our experiments on the finite environment
    
[^92]: 多模态金融时间序列通过潜空间投影的检索

    Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections. (arXiv:2309.16741v1 [cs.LG])

    [http://arxiv.org/abs/2309.16741](http://arxiv.org/abs/2309.16741)

    本文提出了一种通过深度编码器在低维潜空间中存储金融时间序列的多模态数据的框架，以捕捉数据的重要特征。

    

    金融公司通常处理和存储产生连续且高频的数十亿条时间序列数据。为了支持高效的数据存储和检索，出现了专门的时间序列数据库和系统。这些数据库支持通过类似于约束化结构化查询语言（SQL）的格式对时间序列进行索引和查询，以实现像“月度价格回报大于5%的股票”这样的查询，并以严格的格式表达。然而，这样的查询不能捕捉到高维时间序列数据的内在复杂性，它们往往可以通过图像或语言（例如“处于低波动性状态的股票”）更好地描述。而且，在时间序列空间中进行搜索所需的存储、计算时间和检索复杂度往往是非平凡的。在本文中，我们提出并演示了一种利用深度编码器在低维潜空间中存储金融时间序列的多模态数据的框架，使得潜空间投影可以捕捉到数据的重要特征。

    Financial firms commonly process and store billions of time-series data, generated continuously and at a high frequency. To support efficient data storage and retrieval, specialized time-series databases and systems have emerged. These databases support indexing and querying of time-series by a constrained Structured Query Language(SQL)-like format to enable queries like "Stocks with monthly price returns greater than 5%", and expressed in rigid formats. However, such queries do not capture the intrinsic complexity of high dimensional time-series data, which can often be better described by images or language (e.g., "A stock in low volatility regime"). Moreover, the required storage, computational time, and retrieval complexity to search in the time-series space are often non-trivial. In this paper, we propose and demonstrate a framework to store multi-modal data for financial time-series in a lower-dimensional latent space using deep encoders, such that the latent space projections ca
    
[^93]: 对抗性语音合成的协同水印技术

    Collaborative Watermarking for Adversarial Speech Synthesis. (arXiv:2309.15224v1 [eess.AS])

    [http://arxiv.org/abs/2309.15224](http://arxiv.org/abs/2309.15224)

    本文提出了一种对抗性语音合成的协同水印技术，通过与现有对策模型合作进行训练，实现了对生成语音的有效检测和水印识别。

    

    神经语音合成的进展使得技术不仅接近人类的自然度，而且能够以少量数据进行即时语音克隆，并且借助预训练模型具有高度可访问性。当然，生成内容的潜在泛滥引起了对合成语音检测和水印技术的需求。最近，合成语音检测的研究工作主要集中在自动说话人验证和欺骗对策挑战（ASVspoof）上，该挑战专注于被动对策。本文从另一角度出发，针对生成语音的检测，提出了一种协同训练方案，以在不干扰人类听众的情况下，能够通过协同机器检测到生成语音的水印。我们提出了一种与ASVspoof 2021基线对策模型合作的HiFi-GAN神经声码器的合作训练方案，并展示了其有效性。

    Advances in neural speech synthesis have brought us technology that is not only close to human naturalness, but is also capable of instant voice cloning with little data, and is highly accessible with pre-trained models available. Naturally, the potential flood of generated content raises the need for synthetic speech detection and watermarking. Recently, considerable research effort in synthetic speech detection has been related to the Automatic Speaker Verification and Spoofing Countermeasure Challenge (ASVspoof), which focuses on passive countermeasures. This paper takes a complementary view to generated speech detection: a synthesis system should make an active effort to watermark the generated speech in a way that aids detection by another machine, but remains transparent to a human listener. We propose a collaborative training scheme for synthetic speech watermarking and show that a HiFi-GAN neural vocoder collaborating with the ASVspoof 2021 baseline countermeasure models consis
    
[^94]: Era Splitting.（arXiv:2309.14496v1 [cs.LG]）

    Era Splitting. (arXiv:2309.14496v1 [cs.LG])

    [http://arxiv.org/abs/2309.14496](http://arxiv.org/abs/2309.14496)

    本研究提出了两种新的分裂准则，使得决策树模型能够利用时代信息进行优化，从而将超分布泛化研究中的思想应用于决策树模型。

    

    现实生活中的机器学习问题在时间和空间上会呈现出数据的分布变化。这种行为超出了传统的经验风险最小化范式的范围，该范式假设数据在时间和地点上是独立同分布的。新兴的超分布泛化领域通过将环境或时代信息融入算法中，来应对这个现实。迄今为止，大部分研究都集中在线性模型和/或神经网络上。在本研究中，我们针对决策树模型，包括随机森林和梯度提升决策树，开发了两种新的分裂准则，使得树模型能够利用与每个数据点相关的时代信息，来找到在数据的所有不相交时代中都是最优的切分点，从而将超分布泛化研究中的思想应用于决策树模型。

    Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optim
    
[^95]: 双重标准化流：灵活的贝叶斯高斯过程ODE学习

    Double Normalizing Flows: Flexible Bayesian Gaussian Process ODEs Learning. (arXiv:2309.09222v1 [cs.LG])

    [http://arxiv.org/abs/2309.09222](http://arxiv.org/abs/2309.09222)

    这项研究将标准化流引入高斯过程常微分方程(ODE)模型，使其具备更灵活和表达性强的先验分布和非高斯的后验推断，从而提高了贝叶斯高斯过程ODE的准确性和不确定性估计。

    

    最近，高斯过程被用来建模连续动力系统的向量场。对于这样的模型，贝叶斯推断已经得到了广泛研究，并应用于时间序列预测等任务，提供不确定性估计。然而，先前的高斯过程常微分方程(ODE)模型在具有非高斯过程先验的数据集上可能表现不佳，因为它们的约束先验和均值场后验可能缺乏灵活性。为了解决这个限制，我们引入了标准化流来重新参数化ODE的向量场，从而得到一个更灵活、更表达性的先验分布。此外，由于标准化流的解析可计算的概率密度函数，我们将它们应用于GP ODE的后验推断，生成一个非高斯的后验。通过这些标准化流的双重应用，我们的模型在贝叶斯高斯过程ODE中提高了准确性和不确定性估计。

    Recently, Gaussian processes have been utilized to model the vector field of continuous dynamical systems. Bayesian inference for such models \cite{hegde2022variational} has been extensively studied and has been applied in tasks such as time series prediction, providing uncertain estimates. However, previous Gaussian Process Ordinary Differential Equation (ODE) models may underperform on datasets with non-Gaussian process priors, as their constrained priors and mean-field posteriors may lack flexibility. To address this limitation, we incorporate normalizing flows to reparameterize the vector field of ODEs, resulting in a more flexible and expressive prior distribution. Additionally, due to the analytically tractable probability density functions of normalizing flows, we apply them to the posterior inference of GP ODEs, generating a non-Gaussian posterior. Through these dual applications of normalizing flows, our model improves accuracy and uncertainty estimates for Bayesian Gaussian P
    
[^96]: 使用物理信息神经网络识别复杂超弹性固体的组分参数

    Identifying Constitutive Parameters for Complex Hyperelastic Solids using Physics-Informed Neural Networks. (arXiv:2308.15640v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2308.15640](http://arxiv.org/abs/2308.15640)

    本文介绍了一种基于物理信息神经网络（PINNs）的新框架，用于识别软材料在大变形平面应力条件下具有复杂组分行为的材料的组分参数。通过使用多模态的时间相关实验数据训练，我们的模型能够稳健地准确识别不可压缩Arruda-Boyce模型的组分参数。

    

    在工程和生物材料中，特别是那些具有复杂几何和机械行为的材料中，识别组分参数仍然是一个长期存在的挑战。最近出现的物理信息神经网络（PINNs）为此提供了有希望的解决方案，但当前的框架通常仅限于基本的组分定律，并在与实验数据相结合时遇到实际约束。本文引入了一种新的基于PINN的框架，旨在识别软材料的材料参数，特别是那些在平面应力条件下呈现复杂组分行为的材料。该模型强调用多模态的时间相关实验数据训练PINN，其中包括全场变形和加载历史，以确保算法在嘈杂数据中仍然稳健。我们的结果表明，我们的框架可以准确识别不可压缩Arruda-Boyce模型的组分参数。

    Identifying constitutive parameters in engineering and biological materials, particularly those with intricate geometries and mechanical behaviors, remains a longstanding challenge. The recent advent of Physics-Informed Neural Networks (PINNs) offers promising solutions, but current frameworks are often limited to basic constitutive laws and encounter practical constraints when combined with experimental data. In this paper, we introduce a new PINN-based framework designed to identify material parameters for soft materials, specifically those exhibiting complex constitutive behaviors, under large deformation in plane stress conditions. Distinctively, our model emphasizes training PINNs with multi-modal time-dependent experimental datasets consisting of full-field deformation and loading history, ensuring algorithm robustness even amidst noisy data. Our results reveal that our framework can accurately identify constitutive parameters of the incompressible Arruda-Boyce model for samples 
    
[^97]: SyMOT-Flow: 学习两个任意分布之间的最优输运流动及最大平均差异

    SyMOT-Flow: Learning optimal transport flow for two arbitrary distributions with maximum mean discrepancy. (arXiv:2308.13815v1 [cs.LG])

    [http://arxiv.org/abs/2308.13815](http://arxiv.org/abs/2308.13815)

    本文介绍了一个名为SyMOT-Flow的新模型，它通过最小化两个未知分布样本之间的对称最大平均差异来训练可逆转换，并结合最优输运成本作为正则化，将未知分布转换为标准正态分布。实验证明这种转换可以实现更稳定准确的样本生成。

    

    找到两个未知概率分布之间的转换对于建模复杂数据分布和执行密度估计、样本生成和统计推断等任务至关重要。本文引入了一个名为SyMOT-Flow的新模型，通过最小化两个未知分布样本之间的对称最大平均差异来训练一个可逆转换，并结合最优输运成本作为正则化以获得一个短距离和可解释的转换。得到的转换可以实现更稳定准确的样本生成。我们为所提出的模型建立了一些理论结果，并通过低维示例和高维生成样本展示了其有效性。

    Finding a transformation between two unknown probability distributions from samples is crucial for modeling complex data distributions and perform tasks such as density estimation, sample generation, and statistical inference. One powerful framework for such transformations is normalizing flow, which transforms an unknown distribution into a standard normal distribution using an invertible network. In this paper, we introduce a novel model called SyMOT-Flow that trains an invertible transformation by minimizing the symmetric maximum mean discrepancy between samples from two unknown distributions, and we incorporate an optimal transport cost as regularization to obtain a short-distance and interpretable transformation. The resulted transformation leads to more stable and accurate sample generation. We establish several theoretical results for the proposed model and demonstrate its effectiveness with low-dimensional illustrative examples as well as high-dimensional generative samples obt
    
[^98]: SLEM：机器学习用于路径建模和因果推断的超级学习者方程模型

    SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling. (arXiv:2308.04365v1 [stat.ML])

    [http://arxiv.org/abs/2308.04365](http://arxiv.org/abs/2308.04365)

    SLEM是一种路径建模技术，通过集成机器学习超级学习者，实现了一致且无偏的因果效应估计，并在处理非线性关系时超过了传统的结构方程模型。

    

    因果推断是科学的关键目标，使研究人员能够通过观察数据得出关于对假定干预的预测的有意义的结论。路径模型、结构方程模型(SEMs)以及更一般的有向无环图(DAGs)能够明确地指定关于现象背后的因果结构的假设。与DAGs不同，SEMs假设线性关系，这可能导致函数错误规范，从而阻碍研究人员进行可靠的效果大小估计。相反，我们提出了超级学习者方程模型（SLEM），一种集成了机器学习超级学习者集成的路径建模技术。我们通过实证研究，证明了SLEM能够提供一致且无偏的因果效应估计，在与SEMs进行线性模型比较时表现出竞争力，并且在处理非线性关系时优于SEMs。

    Causal inference is a crucial goal of science, enabling researchers to arrive at meaningful conclusions regarding the predictions of hypothetical interventions using observational data. Path models, Structural Equation Models (SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to unambiguously specify assumptions regarding the causal structure underlying a phenomenon. Unlike DAGs, which make very few assumptions about the functional and parametric form, SEM assumes linearity. This can result in functional misspecification which prevents researchers from undertaking reliable effect size estimation. In contrast, we propose Super Learner Equation Modeling, a path modeling technique integrating machine learning Super Learner ensembles. We empirically demonstrate its ability to provide consistent and unbiased estimates of causal effects, its competitive performance for linear models when compared with SEM, and highlight its superiority over SEM when dealing with non
    
[^99]: 在RKHS中自适应学习密度比率

    Adaptive learning of density ratios in RKHS. (arXiv:2307.16164v1 [cs.LG])

    [http://arxiv.org/abs/2307.16164](http://arxiv.org/abs/2307.16164)

    该论文研究在再生核希尔伯特空间中的一类密度比率估计方法，提出了一种自适应学习的参数选择原则，并在有限样本情况下推导出新的误差界。其方法在二次损失的情况下实现了极小化最优误差率。

    

    从有限数量的密度观测中估计两个概率密度的比率是机器学习和统计学中的一个核心问题，应用包括双样本检验、分歧估计、生成建模、协变量转移适应、条件密度估计和新颖性检测。本研究分析了一大类密度比率估计方法，它们通过在再生核希尔伯特空间（RKHS）中最小化真实密度比率与模型之间的正则Bregman距离。我们推导出新的有限样本误差界，并提出了一种Lepskii类型的参数选择原则，在不知道密度比率的正则性的情况下最小化误差界。在二次损失的特殊情况下，我们的方法自适应地实现了极小化最优误差率。提供了一个数值示例。

    Estimating the ratio of two probability densities from finitely many observations of the densities is a central problem in machine learning and statistics with applications in two-sample testing, divergence estimation, generative modeling, covariate shift adaptation, conditional density estimation, and novelty detection. In this work, we analyze a large class of density ratio estimation methods that minimize a regularized Bregman divergence between the true density ratio and a model in a reproducing kernel Hilbert space (RKHS). We derive new finite-sample error bounds, and we propose a Lepskii type parameter choice principle that minimizes the bounds without knowledge of the regularity of the density ratio. In the special case of quadratic loss, our method adaptively achieves a minimax optimal error rate. A numerical illustration is provided.
    
[^100]: 面向稳健点云分类的风险优化异常值去除方法

    Risk-optimized Outlier Removal for Robust Point Cloud Classification. (arXiv:2307.10875v1 [cs.CV])

    [http://arxiv.org/abs/2307.10875](http://arxiv.org/abs/2307.10875)

    提出了一种面向稳健点云分类的风险优化异常值去除方法，利用普通训练的模型消除额外的异常值并恢复数据。方法通过归因分析确定每个点对模型输出的影响，使用条件风险价值优化高风险点的过滤过程。该方法在不需要额外训练的情况下能够产生出色的结果。

    

    点云深度模型在安全关键任务中的使用越来越普遍，但是点云噪声可能会影响这些模型的可靠性和安全性。为了解决这个问题，我们提出了一种新颖的点云异常值去除方法(PointCVaR)，它可以使标准训练的模型消除额外的异常值并恢复数据。我们的方法首先进行归因分析，以确定每个点对模型输出的影响，我们将其称为点的风险。然后，我们使用条件风险价值 (CVaR) 作为目标，优化高风险点的过滤过程。这种方法的基本原理是观察到点云噪声点往往聚集在风险分布的尾部，频率低但风险水平高，从而对分类结果产生重大干扰。尽管不需要额外的训练，我们的方法却能产生出色的结果。

    The popularity of point cloud deep models for safety-critical purposes has increased, but the reliability and security of these models can be compromised by intentional or naturally occurring point cloud noise. To combat this issue, we present a novel point cloud outlier removal method called PointCVaR, which empowers standard-trained models to eliminate additional outliers and restore the data. Our approach begins by conducting attribution analysis to determine the influence of each point on the model output, which we refer to as point risk. We then optimize the process of filtering high-risk points using Conditional Value at Risk (CVaR) as the objective. The rationale for this approach is based on the observation that noise points in point clouds tend to cluster in the tail of the risk distribution, with a low frequency but a high level of risk, resulting in significant interference with classification results. Despite requiring no additional training effort, our method produces exce
    
[^101]: 语言模型是有限实用说话者

    Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17760](http://arxiv.org/abs/2305.17760)

    本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。经过人类反馈的强化学习微调的大型语言模型具有概念上类似于 快与慢思考模型的思维模型，而这种思维模型被归因于人类。此研究凸显了采用认知概率建模方法对语言模型的理解、评估和推进的价值。

    

    本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。特别地，我们展示了经过人类反馈的强化学习微调的大型语言模型（Ouyang等人，2022）具有概念上类似于 快与慢思考模型（Kahneman，2011）的思维模型，而这种思维模型被心理学家们归因于人类。我们讨论了从人类反馈中的强化学习作为快与慢思考模型的局限性，并提出了扩展这个框架的途径。本研究实质上凸显了采用认知概率建模方法来获得对语言模型的理解、评估和推进方面的深刻见解的价值。

    How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
    
[^102]: 伪哈密顿系统辨识

    Pseudo-Hamiltonian system identification. (arXiv:2305.06920v1 [eess.SY])

    [http://arxiv.org/abs/2305.06920](http://arxiv.org/abs/2305.06920)

    该论文提出了一种能够在受未知干扰和阻尼影响时学习到内部动态解析项的伪哈密顿系统辨识模型，通过混合模型，即使难以找到扰动解析项，也能够准确地识别出动态，对于其他系统辨识模型无法处理的情况具有重要的应用价值。此外，该论文提出了一种使用四阶对称积分方案的方法，能够提高在噪声数据上的性能表现。

    

    当只有观测数据时，确定物理系统的基本动态可能具有挑战性。本文考虑可以建模为一阶常微分方程的系统。通过假设一定的伪哈密顿形式，即使模型在系统受到未知阻尼和外扰的数据上进行训练，我们也能够学习到内部动态的解析项。在难以找到扰动解析项的情况下，使用神经网络学习这些项的混合模型仍能够准确地识别出系统的动态，就像在理想情况下一样。这使得该模型适用于其他系统辨识模型无法处理的情况。此外，我们提出在损失函数中使用四阶对称积分方案，避免训练中的实际积分，并展示了在噪声数据上如何提高性能的各种示例。

    Identifying the underlying dynamics of physical systems can be challenging when only provided with observational data. In this work, we consider systems that can be modelled as first-order ordinary differential equations. By assuming a certain pseudo-Hamiltonian formulation, we are able to learn the analytic terms of internal dynamics even if the model is trained on data where the system is affected by unknown damping and external disturbances. In cases where it is difficult to find analytic terms for the disturbances, a hybrid model that uses a neural network to learn these can still accurately identify the dynamics of the system as if under ideal conditions. This makes the models applicable in situations where other system identification models fail. Furthermore, we propose to use a fourth-order symmetric integration scheme in the loss function and avoid actual integration in the training, and demonstrate on varied examples how this leads to increased performance on noisy data.
    
[^103]: 张量空间中的基础张量PCA

    Tensor PCA from basis in tensor space. (arXiv:2305.02803v1 [math.NA])

    [http://arxiv.org/abs/2305.02803](http://arxiv.org/abs/2305.02803)

    本文提出了一种张量PCA的数学框架，通过自伴张量算子导出张量空间中的基础以解决以往方法的局限性，实验结果表明了该方法的有效性。

    

    本文提出了一种张量PCA的数学框架，该方法能够克服以前通过迭代求解优化问题来提取低维子空间的方法的局限性。该方法的核心是从实自伴张量算子中导出张量空间中的基础，从而将基础的导出问题转化为特征值问题。本文研究了三种不同情况的导出：i）从自伴张量算子中导出基础；ii）导出秩为1的基础；iii）从子空间中导出基础。特别是，证明了实自伴张量算子的特征值方程与标准矩阵特征值方程的等价性。针对所考虑的三种情况，采用了子空间方法来导出张量PCA。基于图像数据集的实验验证了所提出的数学框架。

    The aim of this paper is to present a mathematical framework for tensor PCA. The proposed approach is able to overcome the limitations of previous methods that extract a low dimensional subspace by iteratively solving an optimization problem. The core of the proposed approach is the derivation of a basis in tensor space from a real self-adjoint tensor operator, thus reducing the problem of deriving a basis to an eigenvalue problem. Three different cases have been studied to derive: i) a basis from a self-adjoint tensor operator; ii) a rank-1 basis; iii) a basis in a subspace. In particular, the equivalence between eigenvalue equation for a real self-adjoint tensor operator and standard matrix eigenvalue equation has been proven. For all the three cases considered, a subspace approach has been adopted to derive a tensor PCA. Experiments on image datasets validate the proposed mathematical framework.
    
[^104]: 伪哈密顿神经网络用于学习偏微分方程

    Pseudo-Hamiltonian neural networks for learning partial differential equations. (arXiv:2304.14374v1 [cs.LG])

    [http://arxiv.org/abs/2304.14374](http://arxiv.org/abs/2304.14374)

    本文介绍了一种新方法伪哈密顿神经网络(PHNN)，可以用于学习偏微分方程。相比基线模型，PHNN表现更为优越，模型可应用于去除或改变外力情况并可分别得到三个不同物理解释的部分。

    

    最近提出了伪哈密顿神经网络(PHNN)来学习可以用普通微分方程建模的动力系统。本文将该方法扩展到了偏微分方程。所得模型由高达三个神经网络，模拟代表守恒、耗散和外力的项以及可以学习或为先前知识的离散卷积算子构成。我们通过数值结果表明PHNN相比单个神经网络建模的基线模型具有更优越的性能。此外，由于PHNN模型由三个具有不同物理解释的部分组成，可以分别研究这些部分以获得对系统的洞察，并且即使去除或改变外力，所学得的模型仍然适用。

    Pseudo-Hamiltonian neural networks (PHNN) were recently introduced for learning dynamical systems that can be modelled by ordinary differential equations. In this paper, we extend the method to partial differential equations. The resulting model is comprised of up to three neural networks, modelling terms representing conservation, dissipation and external forces, and discrete convolution operators that can either be learned or be prior knowledge. We demonstrate numerically the superior performance of PHNN compared to a baseline model that models the full dynamics by a single neural network. Moreover, since the PHNN model consists of three parts with different physical interpretations, these can be studied separately to gain insight into the system, and the learned model is applicable also if external forces are removed or changed.
    
[^105]: 图神经网络何时对节点分类有帮助：研究同源性原则对节点可区分性的影响

    When Do Graph Neural Networks Help with Node Classification: Investigating the Homophily Principle on Node Distinguishability. (arXiv:2304.14274v1 [cs.SI])

    [http://arxiv.org/abs/2304.14274](http://arxiv.org/abs/2304.14274)

    同源性原则不一定是影响图神经网络优越性的唯一原因；本文提出Contextual Stochastic Block Model for Homophily (CSBM-H)以深入研究同源性对节点可区分性的影响。

    

    同源性原则指相同类别的节点更有可能连接在一起，一直被认为是图神经网络（GNN）在节点分类（NC）任务上性能优越的主要原因。最近，人们提出理论结果认为，即使同源性原则被打破，只要来自同一类别的节点分享相似的邻居模式，GNN的优势仍然存在，这对同源性的有效性提出了质疑。然而，这个论点仅考虑了同类节点的可区分性，忽略了跨类别的可区分性，这是研究同源性效应的不足之处。在本文中，我们首先通过例子证明了上述不足，并认为可区分性的理想情况是同类节点的可区分性小于跨类别节点的可区分性。为了形式化这个想法，更好地理解同源性，我们提出了Contextual Stochastic Block Model for Homophily (CSBM-H)，并进行了全面的实验分析。

    Homophily principle, i.e. nodes with the same labels are more likely to be connected, was believed to be the main reason for the performance superiority of Graph Neural Networks (GNNs) over Neural Networks (NNs) on Node Classification (NC) tasks. Recently, people have developed theoretical results arguing that, even though the homophily principle is broken, the advantage of GNNs can still hold as long as nodes from the same class share similar neighborhood patterns, which questions the validity of homophily. However, this argument only considers intra-class Node Distinguishability (ND) and ignores inter-class ND, which is insufficient to study the effect of homophily. In this paper, we first demonstrate the aforementioned insufficiency with examples and argue that an ideal situation for ND is to have smaller intra-class ND than inter-class ND. To formulate this idea and have a better understanding of homophily, we propose Contextual Stochastic Block Model for Homophily (CSBM-H) and def
    
[^106]: 上下文套索：通过深度神经网络的方法实现稀疏线性模型

    The contextual lasso: Sparse linear models via deep neural networks. (arXiv:2302.00878v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.00878](http://arxiv.org/abs/2302.00878)

    本论文提出了一种新的统计估计器——上下文套索，可以通过深度神经网络的方法解决解释性和拟合能力的矛盾问题，实现对可解释特征的稀疏拟合，并且稀疏模式和系数会随着上下文特征的变化而发生变化。

    

    稀疏线性模型是可解释机器学习的黄金标准工具，本论文通过使用深度神经网络对稀疏线性模型进行改进，实现了可解释性和强大的拟合能力。上下文套索是一种新的统计估计器，它将输入特征分成可解释特征和上下文特征两组，并对可解释特征进行稀疏拟合，同时其稀疏模式和系数会随着上下文特征的变化而发生变化，这个过程通过深度神经网络无需参数地进行学习。

    Sparse linear models are a gold standard tool for interpretable machine learning, a field of emerging importance as predictive models permeate decision-making in many domains. Unfortunately, sparse linear models are far less flexible as functions of their input features than black-box models like deep neural networks. With this capability gap in mind, we study a not-uncommon situation where the input features dichotomize into two groups: explanatory features, which are candidates for inclusion as variables in an interpretable model, and contextual features, which select from the candidate variables and determine their effects. This dichotomy leads us to the contextual lasso, a new statistical estimator that fits a sparse linear model to the explanatory features such that the sparsity pattern and coefficients vary as a function of the contextual features. The fitting process learns this function nonparametrically via a deep neural network. To attain sparse coefficients, we train the net
    
[^107]: 加速非线性约束下的一阶优化

    Accelerated First-Order Optimization under Nonlinear Constraints. (arXiv:2302.00316v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.00316](http://arxiv.org/abs/2302.00316)

    设计了一种新的加速非线性约束下的一阶优化算法，其优点是避免了在整个可行集上进行优化，而且利用速度来表达约束，使得算法在决策变量数量和约束数量上的复杂度增长适度，适用于机器学习应用。

    

    我们利用约束优化和非光滑动力系统之间的类比，设计了一种新的加速非线性约束下的一阶优化算法。与Frank-Wolfe或投影梯度不同，这些算法避免了每次迭代在整个可行集上进行优化。我们证明了在非凸设置中的收敛性，并推导了在连续时间和离散时间中的凸设置的加速率。这些算法的一个重要特性是使用速度而不是位置来表达约束，这自然地导致可行集的稀疏、局部和凸近似（即使可行集是非凸的）。因此，复杂度在决策变量数量和约束数量上适度增长，使得该算法适用于机器学习应用。我们将算法应用于压缩感知和稀疏···

    We exploit analogies between first-order algorithms for constrained optimization and non-smooth dynamical systems to design a new class of accelerated first-order algorithms for constrained optimization. Unlike Frank-Wolfe or projected gradients, these algorithms avoid optimization over the entire feasible set at each iteration. We prove convergence to stationary points even in a nonconvex setting and we derive accelerated rates for the convex setting both in continuous time, as well as in discrete time. An important property of these algorithms is that constraints are expressed in terms of velocities instead of positions, which naturally leads to sparse, local and convex approximations of the feasible set (even if the feasible set is nonconvex). Thus, the complexity tends to grow mildly in the number of decision variables and in the number of constraints, which makes the algorithms suitable for machine learning applications. We apply our algorithms to a compressed sensing and a sparse
    
[^108]: 基于支架的多目标药物候选优化

    Scaffold-Based Multi-Objective Drug Candidate Optimization. (arXiv:2301.07175v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2301.07175](http://arxiv.org/abs/2301.07175)

    基于支架的多目标药物候选优化的创新点在于引入了一种以支架为重点的基于图的马尔可夫链蒙特卡洛框架（ScaMARS），通过自我训练和处理更广泛范围的性质来生成具有最优性质的分子。

    

    在治疗设计中，平衡各种理化性质对于分子开发至关重要，类似于多参数优化（MPO）评估多个变量以实现主要目标的方式。虽然现在可以使用“\textit{in silico}”方法预测许多分子特征，从而帮助早期药物开发，但高通量虚拟筛选产生的大量数据挑战了传统MPO方法的实用性。为了解决这个问题，我们引入了一种以支架为重点的基于图的马尔可夫链蒙特卡洛框架（ScaMARS），用于生成具有最优性质的分子。这个创新的框架能够自我训练，并处理更广泛范围的性质，根据起始支架采样不同的化学空间。对多个性质的基准分析显示，ScaMARS具有84.6％的多样性分数，并且与条件模型相比，成功率高达99.5％。将新特性集成到MPO中显著提高了效果。

    In therapeutic design, balancing various physiochemical properties is crucial for molecule development, similar to how Multiparameter Optimization (MPO) evaluates multiple variables to meet a primary goal. While many molecular features can now be predicted using \textit{in silico} methods, aiding early drug development, the vast data generated from high throughput virtual screening challenges the practicality of traditional MPO approaches. Addressing this, we introduce a scaffold focused graph-based Markov chain Monte Carlo framework (ScaMARS) built to generate molecules with optimal properties. This innovative framework is capable of self-training and handling a wider array of properties, sampling different chemical spaces according to the starting scaffold. The benchmark analysis on several properties shows that ScaMARS has a diversity score of 84.6\% and has a much higher success rate of 99.5\% compared to conditional models. The integration of new features into MPO significantly en
    
[^109]: 自主评估通过贝叶斯逆强化学习的演示充分性

    Autonomous Assessment of Demonstration Sufficiency via Bayesian Inverse Reinforcement Learning. (arXiv:2211.15542v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15542](http://arxiv.org/abs/2211.15542)

    本论文提出了一种基于贝叶斯逆强化学习和风险价值的自主评估方法，使得机器人可以通过计算高置信度边界来确定是否有足够数量的演示。作者定义了两种充分性指标，并在模拟环境中评估了该方法的可行性。

    

    我们研究了确定演示充分性的问题：机器人如何自我评估它是否已经从专家那里获得足够的演示以确保所需的性能水平？为了解决这个问题，我们提出了一种基于贝叶斯逆强化学习和风险价值的新型自我评估方法，使得学习从演示中的机器人能够计算其性能的高置信度边界，并使用这些边界来确定它们是否有足够数量的演示。我们提出并评估了两种充分性的定义：（1）标准化期望值差异，用于衡量相对于人类未观察到的奖励函数的遗憾，以及（2）相对于基准策略的改进百分比。我们展示了如何对这两个指标制定高置信度边界。我们在离散和连续状态空间领域的模拟中评估了我们的方法，并且说明了开发一个能够实现自主评估演示充分性的机器人系统的可行性。

    We examine the problem of determining demonstration sufficiency: how can a robot self-assess whether it has received enough demonstrations from an expert to ensure a desired level of performance? To address this problem, we propose a novel self-assessment approach based on Bayesian inverse reinforcement learning and value-at-risk, enabling learning-from-demonstration ("LfD") robots to compute high-confidence bounds on their performance and use these bounds to determine when they have a sufficient number of demonstrations. We propose and evaluate two definitions of sufficiency: (1) normalized expected value difference, which measures regret with respect to the human's unobserved reward function, and (2) percent improvement over a baseline policy. We demonstrate how to formulate high-confidence bounds on both of these metrics. We evaluate our approach in simulation for both discrete and continuous state-space domains and illustrate the feasibility of developing a robotic system that can 
    
[^110]: tf.data服务：拆分机器学习输入数据处理的案例

    tf.data service: A Case for Disaggregating ML Input Data Processing. (arXiv:2210.14826v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14826](http://arxiv.org/abs/2210.14826)

    本文介绍了一个拆分机器学习输入数据处理的案例。通过构建tf.data服务，可以实现数据预处理的拆分，以提高加速器和主机资源的利用率。

    

    机器学习中的计算通常在昂贵的专用硬件上执行，如GPU和TPU，它们提供高FLOP和每瓦性能。为了成本效益，必须保持这些加速器的高利用率。这需要以加速器可以接收和执行数据的速率预处理输入数据。为了避免数据停顿，用于机器学习计算的加速器核心所需的主机CPU和RAM在不同的作业中是可变的。因此，传统的在具有固定硬件比例的ML加速器主机上处理输入数据的方法会导致加速器或主机CPU和RAM的低利用率。在本文中，我们通过构建一个拆分的ML数据处理系统来解决这些问题。我们介绍了tf.data service，这是一个建立在TensorFlow的tf.data之上的开源拆分输入数据处理服务。我们展示了将数据预处理拆分的三个关键优势。

    Machine learning (ML) computations commonly execute on expensive specialized hardware, such as GPUs and TPUs, which provide high FLOPs and performance-per-watt. For cost efficiency, it is essential to keep these accelerators highly utilized. This requires preprocessing input data at the rate at which the accelerators can ingest and perform ML computations on the data. To avoid data stalls, the host CPU and RAM required for input data processing per accelerator core used for ML computations varies across jobs. Hence, the traditional approach of processing input data on ML accelerator hosts with a fixed hardware ratio leads to either under-utilizing the accelerators or the host CPU and RAM. In this paper, we address these concerns by building a disaggregated ML data processing system.  We present tf.data service, an open-source disaggregated input data processing service built on top of tf.data in TensorFlow. We show that disaggregating data preprocessing has three key advantages for lar
    
[^111]: 从特征提取角度的CNN近似分析

    Approximation analysis of CNNs from a feature extraction view. (arXiv:2210.09041v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09041](http://arxiv.org/abs/2210.09041)

    本文通过深度卷积神经网络对线性特征提取进行了近似分析，并展示了深度学习相对于传统线性变换的强大能力。通过创造性的构造，我们有效地实现了线性特征提取，并且探究了深层网络的函数逼近速度。线性特征的多分辨卷积分解在我们的工作中起着核心作用。

    

    基于深度神经网络的深度学习在许多实际应用中取得了非常成功的结果，但由于网络架构和结构的限制，它缺乏足够的理论理解。在本文中，我们通过深度多通道卷积神经网络(CNNs)建立了一些线性特征提取的分析，这证明了深度学习在传统线性变换（如傅里叶变换、小波变换、冗余字典编码方法）上的强大能力。此外，我们给出了一个精确的构造，展示了如何利用多通道CNNs高效地进行线性特征提取。它可以用于降低逼近高维函数所需的基本维度。我们还研究了使用通道实现的深层网络和全连接层的函数逼近速率。将线性特征因子分解为多分辨卷积在我们的工作中起着重要作用。

    Deep learning based on deep neural networks has been very successful in many practical applications, but it lacks enough theoretical understanding due to the network architectures and structures. In this paper we establish some analysis for linear feature extraction by a deep multi-channel convolutional neural networks (CNNs), which demonstrates the power of deep learning over traditional linear transformations, like Fourier, wavelets, redundant dictionary coding methods. Moreover, we give an exact construction presenting how linear features extraction can be conducted efficiently with multi-channel CNNs. It can be applied to lower the essential dimension for approximating a high dimensional function. Rates of function approximation by such deep networks implemented with channels and followed by fully-connected layers are investigated as well. Harmonic analysis for factorizing linear features into multi-resolution convolutions plays an essential role in our work. Nevertheless, a dedica
    
[^112]: 基于条件扩散模型的有损图像压缩

    Lossy Image Compression with Conditional Diffusion Models. (arXiv:2209.06950v5 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2209.06950](http://arxiv.org/abs/2209.06950)

    本文提出了一种利用条件扩散模型进行有损图像压缩的优化框架。通过引入额外的内容潜变量以及合成纹理变量，该方法在图像质量评估指标上表现出更强的性能。

    

    本文提出了一种利用扩散生成模型的端到端优化的有损图像压缩框架。该方法基于变换编码范式，将图像映射到潜在空间进行信息熵编码，然后再映射回数据空间进行重构。与基于变分自编码器(VAE)的神经压缩方法不同，我们的解码器是一个条件扩散模型。因此，我们的方法引入了一个额外的“内容”潜变量，反向扩散过程会对其进行条件化，并利用该变量存储图像信息。决定扩散过程的剩余“纹理”变量会在解码时合成。通过实验，我们展示了模型的性能可以根据感知度量进行调整。我们广泛的实验涉及了多个数据集和图像质量评估指标，结果表明我们的方法相较于基于生成对抗网络的方法能够得到更好的FID分数。

    This paper outlines an end-to-end optimized lossy image compression framework using diffusion generative models. The approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and, from there, mapped back to the data space for reconstruction. In contrast to VAE-based neural compression, where the (mean) decoder is a deterministic neural network, our decoder is a conditional diffusion model. Our approach thus introduces an additional "content" latent variable on which the reverse diffusion process is conditioned and uses this variable to store information about the image. The remaining "texture" variables characterizing the diffusion process are synthesized at decoding time. We show that the model's performance can be tuned toward perceptual metrics of interest. Our extensive experiments involving multiple datasets and image quality assessment metrics show that our approach yields stronger reported FID scores than the GAN-based mode
    
[^113]: 广义线性Bandits中的排名问题研究

    Ranking In Generalized Linear Bandits. (arXiv:2207.00109v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.00109](http://arxiv.org/abs/2207.00109)

    本文研究了广义线性Bandits中的排名问题，设计了UCB和Thompson Sampling类型算法来解决该问题，并对位置和物品之间的依赖关系进行了建模。研究结果在位置依赖性和排名问题与图论的连接等方面进行了推广。

    

    我们研究了广义线性Bandits中的排名问题。在每个时刻，学习代理选择一个有序的物品列表，并观察随机结果。在推荐系统中，显示一个有序的最具吸引力的物品列表并不总是最优的，因为位置和物品之间存在复杂的奖励函数。一个非常简单的例子是当所有最具吸引力的物品都来自同一类别时缺乏多样性。我们对有序列表中的位置和物品之间的依赖关系进行建模，并设计了用于解决这个问题的UCB和Thompson Sampling类型的算法。我们的工作在几个方向上推广了现有的研究，包括位置依赖性，其中位置折扣是一个特例，并将排名问题与图论相联系。

    We study the ranking problem in generalized linear bandits. At each time, the learning agent selects an ordered list of items and observes stochastic outcomes. In recommendation systems, displaying an ordered list of the most attractive items is not always optimal as both position and item dependencies result in a complex reward function. A very naive example is the lack of diversity when all the most attractive items are from the same category. We model the position and item dependencies in the ordered list and design UCB and Thompson Sampling type algorithms for this problem. Our work generalizes existing studies in several directions, including position dependencies where position discount is a particular case, and connecting the ranking problem to graph theory.
    
[^114]: 评估和减轻路边取货和送货的拥堵影响：一种因果推断方法

    Estimating and Mitigating the Congestion Effect of Curbside Pick-ups and Drop-offs: A Causal Inference Approach. (arXiv:2206.02164v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02164](http://arxiv.org/abs/2206.02164)

    该论文开发了一种严格的因果推断方法，评估了取货和送货对路边交通的拥堵影响，并提出了一种双重和分离的机器学习方法来量化这种影响。

    

    路边空间是城市道路网络中最繁忙的区域之一。特别是近年来，网约车和商业配送的快速增长导致了大量的取货和送货（PUDOs），这些占据了几十年前设计建造的有限路边空间。这些PUDOs可能导致路边利用率的拥堵和干扰主线交通流，明显带来显著的负面社会外部性。然而，目前缺乏一个严格量化和减轻PUDOs拥堵影响的分析框架，尤其是缺乏数据支持和混淆效应的参与。为了填补这一研究空白，本文采用严格的因果推断方法，评估PUDOs对一般区域网络的拥堵影响。建立了一个因果图来表示PUDOs和交通速度之间的时空关系，并提出了一种双重和分离的机器学习（DSML）方法来量化PUDOs对交通速度的影响。

    Curb space is one of the busiest areas in urban road networks. Especially in recent years, the rapid increase of ride-hailing trips and commercial deliveries has induced massive pick-ups/drop-offs (PUDOs), which occupy the limited curb space that was designed and built decades ago. These PUDOs could jam curbside utilization and disturb the mainline traffic flow, evidently leading to significant negative societal externalities. However, there is a lack of an analytical framework that rigorously quantifies and mitigates the congestion effect of PUDOs in the system view, particularly with little data support and involvement of confounding effects. To bridge this research gap, this paper develops a rigorous causal inference approach to estimate the congestion effect of PUDOs on general regional networks. A causal graph is set to represent the spatio-temporal relationship between PUDOs and traffic speed, and a double and separated machine learning (DSML) method is proposed to quantify how P
    
[^115]: 高效地解开因果表示交织问题

    Efficiently Disentangle Causal Representations. (arXiv:2201.01942v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.01942](http://arxiv.org/abs/2201.01942)

    本文提出了一种高效的方法来学习具有因果机制的分离表示，通过估计原始和新分布之间的条件概率差异，并利用模型的泛化能力进行逼近。与现有方法相比，该方法只需要评估模型的泛化能力，而不需要依赖学习者对新分布的适应速度。实验证明该方法在各种任务上更加样本高效且速度更快。

    

    本文提出了一种基于原始分布和新分布的条件概率之差的因果机制学习分离表示的高效方法。我们利用模型的泛化能力来逼近这种差异，使其适应标准的机器学习框架并能够高效地计算。与现有方法相比，该方法只需要评估模型的泛化能力，而不依赖于学习者对新分布的适应速度。我们为所提出方法的优势提供了理论解释，实验结果表明，所提出的技术在各种任务上比先前方法更节约样本，速度更快，分别提升了1.9-11.0倍和9.4-32.4倍。源代码可在 \url{https://github.com/yuanpeng16/EDCR} 找到。

    This paper proposes an efficient approach to learning disentangled representations with causal mechanisms based on the difference of conditional probabilities in original and new distributions. We approximate the difference with models' generalization abilities so that it fits in the standard machine learning framework and can be efficiently computed. In contrast to the state-of-the-art approach, which relies on the learner's adaptation speed to new distribution, the proposed approach only requires evaluating the model's generalization ability. We provide a theoretical explanation for the advantage of the proposed method, and our experiments show that the proposed technique is 1.9--11.0$\times$ more sample efficient and 9.4--32.4 times quicker than the previous method on various tasks. The source code is available at \url{https://github.com/yuanpeng16/EDCR}.
    
[^116]: 共同学习线性时不变动力系统

    Joint Learning of Linear Time-Invariant Dynamical Systems. (arXiv:2112.10955v6 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.10955](http://arxiv.org/abs/2112.10955)

    本研究探讨了共同估计多个线性时不变系统的转移矩阵的方法，并展示了通过数据汇集可以显著提高估计准确性的重要收益。

    

    线性时不变系统是系统论和应用中非常流行的模型。系统辨识中一个未受到充分关注的基本问题是如何利用相关线性系统之间的共性来更准确地估计它们的转移矩阵。为了解决这个问题，本文研究了联合估计多个系统的转移矩阵的方法。假设转移矩阵是一些未知共享基础矩阵的未知线性函数。我们建立了完全反映考虑的轨迹长度、维度和系统数量的有限时间估计误差率。所呈现的结果相当普遍，并显示了与单独学习每个系统相比，通过系统之间的数据汇集可以获得的显著收益。此外，结果还显示了对模型错误设定具有鲁棒性。为了得到这些结果，我们开发了新的技术方法。

    Linear time-invariant systems are very popular models in system theory and applications. A fundamental problem in system identification that remains rather unaddressed in extant literature is to leverage commonalities amongst related linear systems to estimate their transition matrices more accurately. To address this problem, the current paper investigates methods for jointly estimating the transition matrices of multiple systems. It is assumed that the transition matrices are unknown linear functions of some unknown shared basis matrices. We establish finite-time estimation error rates that fully reflect the roles of trajectory lengths, dimension, and number of systems under consideration. The presented results are fairly general and show the significant gains that can be achieved by pooling data across systems in comparison to learning each system individually. Further, they are shown to be robust against model misspecifications. To obtain the results, we develop novel techniques th
    
[^117]: 使用合规预测提供样本效率高的安全保证

    Sample-Efficient Safety Assurances using Conformal Prediction. (arXiv:2109.14082v5 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2109.14082](http://arxiv.org/abs/2109.14082)

    本研究提出了一种使用合规预测技术的框架，结合机器人/环境动力学模拟器来调整预警系统，能够以尽可能少的数据点证明达到可接受的误报率，并在驾驶员预警系统和机器人抓取应用中实证了该框架的有效性。

    

    在高风险的机器人应用中部署机器学习模型时，能够检测不安全情况的能力是至关重要的。预警系统可以在不采取纠正措施的情况下提前提供警报，以预示可能出现的不安全情况。为了可靠地提高安全性，这些预警系统应该具有可证明的误报率，即在不安全的情况下，会出现警报的概率小于ϵ。在这项工作中，我们提出了一个框架，将统计推断技术（称为合规预测）与机器人/环境动力学模拟器结合起来，以调整预警系统，以尽可能少的数据点证明达到ϵ的误报率。我们将我们的框架应用于驾驶员预警系统和机器人抓取应用，并在实证中证明了保证的误报率，同时观察到较低的误报率（阳性）。

    When deploying machine learning models in high-stakes robotics applications, the ability to detect unsafe situations is crucial. Early warning systems can provide alerts when an unsafe situation is imminent (in the absence of corrective action). To reliably improve safety, these warning systems should have a provable false negative rate; i.e. of the situations that are unsafe, fewer than $\epsilon$ will occur without an alert. In this work, we present a framework that combines a statistical inference technique known as conformal prediction with a simulator of robot/environment dynamics, in order to tune warning systems to provably achieve an $\epsilon$ false negative rate using as few as $1/\epsilon$ data points. We apply our framework to a driver warning system and a robotic grasping application, and empirically demonstrate guaranteed false negative rate while also observing low false detection (positive) rate.
    

