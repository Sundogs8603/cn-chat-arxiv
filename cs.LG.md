# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Multi-level protein pre-training with Vabs-Net](https://rss.arxiv.org/abs/2402.01481) | 这篇论文介绍了一种使用Vabs-Net进行多级蛋白质预训练的方法。当前大多数基于结构的预训练模型仅关注残基水平，但忽略了侧链原子的重要性。为了解决这个问题，论文提出了一种新的预训练策略，引入了跨度掩码，以在三维蛋白质预训练中同时建模残基和原子水平的信息，并改善了残基表示的表达能力。 |
| [^2] | [Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization](https://rss.arxiv.org/abs/2402.01401) | 通过Lipschitz正则化实现零样本机器遗忘，可以及时忘记私人或受版权保护的信息，同时保持模型性能。 |
| [^3] | [LoTR: Low Tensor Rank Weight Adaptation](https://rss.arxiv.org/abs/2402.01376) | LoTR是一种新颖的语言模型参数调优方法，通过引入低秩张量表示和张量分解，使得针对深层模型的参数效率更高，并且能够实现廉价且快速的下游调优。 |
| [^4] | [Supervised Algorithmic Fairness in Distribution Shifts: A Survey](https://rss.arxiv.org/abs/2402.01327) | 这篇综述研究了分布变化下的监督公平机器学习领域，调查了各种类型的分布变化和现有的解决方法，并列举了公开数据集和评估指标。研究发现六种常用的方法，并探讨了与相关研究领域的交互关系。 |
| [^5] | [Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum](https://rss.arxiv.org/abs/2402.01297) | 我们通过推导核矩阵的特征数界限，增强了核岭回归的测试误差界限。对于多项式谱衰减的核，我们恢复了先前的结果；对于指数谱衰减，我们提出了新的非平凡的界限。我们的研究表明，特征谱衰减多项式的核回归器具有良好的泛化能力，而特征谱指数衰减的核回归器则具有灾难性的过拟合。 |
| [^6] | [Cascaded Scaling Classifier: class incremental learning with probability scaling](https://rss.arxiv.org/abs/2402.01262) | 提出了级联缩放分类器，结合边际抑制和知识蒸馏方法，用于实现神经网络中的连续学习，并降低过去任务的遗忘。 |
| [^7] | [Efficient Causal Graph Discovery Using Large Language Models](https://rss.arxiv.org/abs/2402.01207) | 提出了一个新的框架，利用大型语言模型进行高效的因果图发现，采用了广度优先搜索方法，只需要线性数量的查询，同时能轻松结合观察数据以提高性能，具有高效性和数据效率，并在真实因果图上取得了最先进的结果，展示了其在不同领域的广泛适用性潜力。 |
| [^8] | [A Survey on Self-Supervised Learning for Non-Sequential Tabular Data](https://rss.arxiv.org/abs/2402.01204) | 本调研总结了自监督学习在非连续表格数据中的最新进展和挑战，将其方法分为预测性学习、对比学习和混合学习，讨论了应用问题，包括自动化数据工程、跨表格查询和隐私保护等。 |
| [^9] | [Test-Time Adaptation for Depth Completion](https://arxiv.org/abs/2402.03312) | 该论文提出了一种在线测试时间自适应方法，用于深度补全任务，通过在单次通过中缩小源数据和目标数据间的领域差距，提高模型性能。 |
| [^10] | [HASSOD: Hierarchical Adaptive Self-Supervised Object Detection](https://arxiv.org/abs/2402.03311) | HASSOD是一种分层自适应无监督目标检测系统，通过自监督学习，提高了检测性能和可解释性。 |
| [^11] | [AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion](https://arxiv.org/abs/2402.03309) | AONeuS是一种基于物理的多模态声光神经表面重建框架，通过融合高分辨率RGB测量和低分辨率深度成像声纳测量，能够在受限基线下实现准确的高分辨率三维表面重建。 |
| [^12] | [Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?](https://arxiv.org/abs/2402.03305) | 本研究通过实验探究了条件DDPMs学习生成2D球形高斯凸起的过程，在学习的过程中发现了潜在表示的关键，产生了与不同阶段对应的 qualitatively 不同的生成行为。 |
| [^13] | [Nevermind: Instruction Override and Moderation in Large Language Models](https://arxiv.org/abs/2402.03303) | 大语言模型具有覆盖和调节指令的能力，较大的模型在覆盖内部和上下文指令方面表现最佳，并且在绳索扩展时需要保持缓冲区来保持指令遵循能力。 |
| [^14] | [Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining](https://arxiv.org/abs/2402.03302) | Swin-UMamba是一种以Mamba为基础的新型UNet模型，通过结合局部特征和全局依赖的多尺度信息来实现准确的医学图像分割。与现有方法相比，Swin-UMamba具有更高的准确性、较低的内存消耗和更少的计算负担，并充分利用了预训练的威力。 |
| [^15] | [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300) | DeepSeekMath是一种开放语言模型，通过预训练和数据选择，提升了数学推理能力，实现了接近于竞赛级别水平的性能。 |
| [^16] | [GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models](https://arxiv.org/abs/2402.03299) | 本论文提出了一个通过角色扮演的系统，可以生成自然语言越狱，用于测试大型语言模型的指南遵循情况。系统通过收集现有越狱并将其组织成知识图来生成新的越狱，证明了其高效性和有效性。 |
| [^17] | [Ginger: An Efficient Curvature Approximation with Linear Complexity for General Neural Networks](https://arxiv.org/abs/2402.03295) | Ginger是一种用于通用神经网络的高效曲率近似方法，具有线性复杂度。它通过特征分解来逆向计算广义高斯牛顿矩阵，避免了传统方法中的高内存和高时间复杂度问题。 |
| [^18] | [Flora: Low-Rank Adapters Are Secretly Gradient Compressors](https://arxiv.org/abs/2402.03293) | 本文研究了低秩适配器的动力学，并提出了一种基于随机投影的方法Flora，通过重新采样投影矩阵实现高秩更新，同时减少优化状态的空间复杂度。 |
| [^19] | [Zero-shot Object-Level OOD Detection with Context-Aware Inpainting](https://arxiv.org/abs/2402.03292) | 本论文提出了一种用上下文感知修复的零样本物体级OOD检测方法RONIN。通过将检测到的对象进行修复替换，并使用预测的ID标签来条件化修复过程，使得重构的对象在OOD情况下与原始对象相差较远，从而有效区分ID和OOD样本。实验证明RONIN在多个数据集上取得了具有竞争力的结果。 |
| [^20] | [InstanceDiffusion: Instance-level Control for Image Generation](https://arxiv.org/abs/2402.03290) | InstanceDiffusion通过添加实例级控制，使文本到图像的扩散模型能够产生高质量图像，并在不同的位置条件下超过了专业先进模型。 |
| [^21] | [Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS](https://arxiv.org/abs/2402.03289) | 本文介绍了一种使用蒙特卡罗树搜索进行前瞻的自动变换器解码算法，可解决现有大型语言模型在RTL代码生成中存在的编译失败和PPA不敏感的问题，并在性能上取得了显著改进。 |
| [^22] | [A Lennard-Jones Layer for Distribution Normalization](https://arxiv.org/abs/2402.03287) | 我们引入了Lennard-Jones层（LJL）来均衡2D和3D点云的密度，通过系统重新排列点并模拟点之间的相互作用，我们可以达到近似均匀采样的效果。这种方法可以应用于点云生成和改善点分布问题。 |
| [^23] | [Training-Free Consistent Text-to-Image Generation](https://arxiv.org/abs/2402.03286) | 本文提出了一种无需训练的方法ConsiStory，通过共享预训练模型的内部激活，实现了一致的文本到图像生成。引入了主题驱动的共享注意力块和基于对应的特征注入，促进了图像之间的主题一致性，并采用了策略来保持布局多样性。 |
| [^24] | [Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models](https://arxiv.org/abs/2402.03284) | 本论文研究了如何用语言模型来表示对话中的不确定性，并提出了改进模型校准的微调策略。实验证明，这些策略可以使较小的模型具备与大型预训练模型相当的性能。 |
| [^25] | [A Framework for Partially Observed Reward-States in RLHF](https://arxiv.org/abs/2402.03282) | 这篇论文提出了一个针对RLHF的框架，在其中考虑了部分观察到的奖励状态，并通过将基数反馈和决斗反馈缩减为PORRL形式进行了建模和算法开发。 |
| [^26] | [Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models](https://arxiv.org/abs/2402.03271) | 通过引入不确定性感知规划（UoT）算法，我们实现了增强大型语言模型的主动寻求信息的能力，通过模拟未来场景、基于不确定性的奖励机制和奖励传播方案，优化问题提问方式。 |
| [^27] | [Multiclass Classification Procedure for Detecting Attacks on MQTT-IoT Protocol](https://arxiv.org/abs/2402.03270) | 本研究致力于创建适用于MQTT协议的物联网系统的入侵检测系统，通过使用集成方法和深度学习模型来分类攻击，在实验中取得了非常满意的结果。 |
| [^28] | [ISPA: Inter-Species Phonetic Alphabet for Transcribing Animal Sounds](https://arxiv.org/abs/2402.03269) | 本文介绍了ISPA（跨物种语音音标），这是一种精确、简洁、可解释的系统，用于将动物声音转录为文本。通过将动物声音表示为文本，我们有效地将其视为一种“外语”，并展示了已建立的人类语言机器学习范例和模型（如语言模型）能够成功应用于提高性能。 |
| [^29] | [Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation](https://arxiv.org/abs/2402.03268) | 本文研究了预训练语言模型的推理能力，并提出了从聚合间接推理路径的角度理解语言模型如何产生推理能力。通过对知识图谱和数学问题数据集进行实验和分析，发现增加无标签的随机游走推理路径可以提高实际应用中的多步推理能力。 |
| [^30] | [MobilityGPT: Enhanced Human Mobility Modeling with a GPT model](https://arxiv.org/abs/2402.03264) | MobilityGPT是一种基于GPT模型的增强型人类移动建模方法，通过将人类移动建模转换为自回归生成任务，并引入地理感知生成模型以及基于重力的采样方法和道路连接矩阵约束，实现了对生成的地理空间移动数据的语义真实性和各个特征的保持。 |
| [^31] | [Learning Best-in-Class Policies for the Predict-then-Optimize Framework](https://arxiv.org/abs/2402.03256) | 我们提出了一种新颖的决策感知替代损失函数家族，用于predict-then-optimize框架，并且通过数值证据证实了其在误设置下的优越性。 |
| [^32] | [Minimum Description Length and Generalization Guarantees for Representation Learning](https://arxiv.org/abs/2402.03254) | 本文提出了一个可压缩性框架，通过计算表示学习算法的泛化误差的上界，改进了现有启发式方法，并提供了关于理论泛化保证的新见解。 |
| [^33] | [Fair Active Ranking from Pairwise Preferences](https://arxiv.org/abs/2402.03252) | 本文研究了通过自适应地引发配对比较来公平主动排序的问题，提出了一种新颖的公平目标函数，并通过最小化群组误差的范数来实现对不同公平概念的探索。 |
| [^34] | [CLIP Can Understand Depth](https://arxiv.org/abs/2402.03251) | 本文研究了将CLIP用于单目深度估计的问题，通过联合训练反卷积解码器和可学习嵌入矩阵，使得CLIP能够理解深度，该方法在深度估计任务上取得了令人印象深刻的性能，并优于之前的方法。 |
| [^35] | [Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills](https://arxiv.org/abs/2402.03244) | 本论文提出了一种技能集优化（SSO）方法，通过构建和完善可转移的技能集来提高大型语言模型（LLM）的性能。该方法通过提取高奖励的共同子轨迹，生成子目标和说明，并在上下文中提供给LLM演员，以强化行为。实验结果显示，SSO在不同环境中能够优化技能集，并实现上下文策略改进。 |
| [^36] | [PINN-BO: A Black-box Optimization Algorithm using Physics-Informed Neural Networks](https://arxiv.org/abs/2402.03243) | PINN-BO是一种利用物理启发式神经网络和偏微分方程知识的黑箱优化算法，提高了优化的样本效率。 |
| [^37] | [FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition](https://arxiv.org/abs/2402.03241) | FROSTER是一种用于开放式词汇动作识别的框架，通过使用冻结的CLIP模型作为教师，在保持CLIP泛化能力的同时有效适应动作识别任务。 |
| [^38] | [ActiveAnno3D - An Active Learning Framework for Multi-Modal 3D Object Detection](https://arxiv.org/abs/2402.03235) | 这项工作提出了一种用于多模态3D物体检测的主动学习框架ActiveAnno3D。通过选择最具信息量的训练数据样本进行标注，我们能够在使用一半的训练数据时实现与传统方法相近的检测性能。 |
| [^39] | [Smart Flow Matching: On The Theory of Flow Matching Algorithms with Applications](https://arxiv.org/abs/2402.03232) | 本文提出了一种智能流匹配算法，通过精确的向量场公式最小化标准流的损失，并在训练向量场模型时展示了更小的方差和更好的学习结果。 |
| [^40] | [Improved prediction of future user activity in online A/B testing](https://arxiv.org/abs/2402.03231) | 本文提出了一种在在线A/B测试中改进对未来用户活动预测的方法，该方法利用贝叶斯非参数方法预测个体被介入的速率，并提供双重预测能力，预测未来时间窗口中新客户数量和被观察次数。 |
| [^41] | [CT-based Anatomical Segmentation for Thoracic Surgical Planning: A Benchmark Study for 3D U-shaped Deep Learning Models](https://arxiv.org/abs/2402.03230) | 本研究为基于CT的胸部手术规划中的解剖分割提供了针对3D U-shaped深度学习模型的基准研究，为临床应用和未来模型设计提供了宝贵的见解。 |
| [^42] | [IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images](https://arxiv.org/abs/2402.03227) | IGUANe是一种三维通用CycleGAN模型，通过集成多个域的训练实现了脑MR图像的多中心协调，使其成为通用生成器。 |
| [^43] | [FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion](https://arxiv.org/abs/2402.03226) | 本论文提出了一种名为FuseMoE的专家混合Transformer框架，通过创新的门控函数实现灵活融合多模态数据，能够有效地处理缺失模态和不规则采样数据，同时改善模型的预测性能，在临床风险预测任务中具有实际应用价值。 |
| [^44] | [The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents](https://arxiv.org/abs/2402.03220) | 该论文研究了在两层神经网络中学习多指数目标函数时，重复使用批次的梯度下降（GD）的训练动态。研究发现，与单次GD相比，多次GD能够克服目标函数的限制，仅需两个时间步骤就能实现网络与目标子空间的重叠，展示了在有限时间内有效学习的广泛函数类。这些结果基于动力平均场理论（DMFT）的分析。 |
| [^45] | [BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation](https://arxiv.org/abs/2402.03216) | BGE M3-嵌入是一种新的多语言、多功能和多粒度的文本嵌入模型，支持超过100种工作语言，并在多语言和跨语言检索任务上取得了最先进的性能。它能够同时执行密集检索、多向量检索和稀疏检索，并能处理不同粒度的输入。其有效训练包括了一种自知识蒸馏方法和优化的批处理策略。 |
| [^46] | [Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?](https://arxiv.org/abs/2402.03214) | 这项研究探讨了如何区分人类艺术和AI生成的图像，并提供了几种不同的方法，包括通过监督学习训练的分类器、扩散模型的研究工具以及专业艺术家的知识。这对防止欺诈、遵守政策以及避免模型崩溃都具有重要意义。 |
| [^47] | [Light and Optimal Schr\"odinger Bridge Matching](https://arxiv.org/abs/2402.03207) | 该论文提出了一种新的学习薛定谔桥匹配的方法，克服了现有方法中的限制，通过优化薛定谔桥的参数化来恢复输运计划。 |
| [^48] | [Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell Massive MIMO Systems](https://arxiv.org/abs/2402.03204) | 该论文提出了一种多智能体强化学习算法，通过优化多个大规模MIMO基站的睡眠模式和天线切换，实现在多小区网络中能量的节省，同时保持服务质量。仿真结果表明，这种算法相比基线策略具有更好的性能。 |
| [^49] | [Guidance with Spherical Gaussian Constraint for Conditional Diffusion](https://arxiv.org/abs/2402.03201) | 本文提出了一种用球面高斯约束的扩散算法（DSG），解决了在条件生成任务中采样过程中的流形偏离问题。这种算法通过优化将步骤限制在中间数据流形内，并能够使用较大的引导步长。 |
| [^50] | [Isotropy, Clusters, and Classifiers](https://arxiv.org/abs/2402.03191) | 同性质的嵌入空间对聚类和线性分类目标具有负面影响，这一事实得到了本文的实证支持，并对文献中的先前结果有所启示。 |
| [^51] | [Unified Hallucination Detection for Multimodal Large Language Models](https://arxiv.org/abs/2402.03190) | 该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。 |
| [^52] | [How Good is a Single Basin?](https://arxiv.org/abs/2402.03187) | 本文通过构建同一个盆地内的“相连”集成，并通过蒸馏将其他盆地的知识隐含纳入到同一个盆地内，弥补了连接增加对性能的负面影响，重新发现了（多盆地）深度集成的性能。因此推测，在没有从其他盆地学习的情况下，很难有效利用其他盆地的知识。 |
| [^53] | [Predicting Configuration Performance in Multiple Environments with Sequential Meta-learning](https://arxiv.org/abs/2402.03183) | 本论文提出了一个顺序元学习框架SeMPL，可以在多个环境下学习和预测给定软件配置的性能。与现有方法不同的是，SeMPL通过依次顺序训练元环境，实现了更准确的对新环境的性能预测。 |
| [^54] | [Empowering Time Series Analysis with Large Language Models: A Survey](https://arxiv.org/abs/2402.03182) | 该调查提供了对利用大型语言模型（LLMs）进行时间序列分析的现有方法的系统概述。最近的进展表明，预训练的LLMs可以捕捉复杂的时间序列数据依赖关系，并促进各种应用。 |
| [^55] | [Cool-chic video: Learned video coding with 800 parameters](https://arxiv.org/abs/2402.03179) | 我们提出了一个轻量级的学习视频编码器，使用800个参数和900次乘法来实现低解码复杂度。该编码器在压缩视频时能够利用时间冗余，并在接近AVC的速率失真条件下表现优于其他过拟合编解码器。 |
| [^56] | [CIDAR: Culturally Relevant Instruction Dataset For Arabic](https://arxiv.org/abs/2402.03177) | CIDAR是第一个由人工评审对齐文化的阿拉伯指令调优数据集，目的是解决现有指令数据集对西方文化的固有偏见所带来的影响，对于丰富将语言模型与阿拉伯文化对齐的研究工作具有重要意义。 |
| [^57] | [Comparison of Topic Modelling Approaches in the Banking Context](https://arxiv.org/abs/2402.03176) | 本研究在银行业背景下比较了主题建模方法。通过使用KernelPCA和K-means Clustering在BERTopic架构中，我们得到了连贯的主题，连贯性得分为0.8463。 |
| [^58] | [The Matrix: A Bayesian learning model for LLMs](https://arxiv.org/abs/2402.03175) | 本文介绍了一个贝叶斯学习模型，用于理解大型语言模型（LLMs）的行为。研究探索了LLMs的优化指标，并开发了一个新的基于预测下一个标记的模型。实验结果表明，LLMs的行为与贝叶斯学习一致，为上下文学习提供了新的见解。 |
| [^59] | [Decentralized Event-Triggered Online Learning for Safe Consensus of Multi-Agent Systems with Gaussian Process Regression](https://arxiv.org/abs/2402.03174) | 本文提出了一种分布式事件触发在线学习的方法，通过辅助动力学和高斯过程回归实现多智能体系统的共识控制，并通过Lyapunov理论提供了概率保证的预测误差界限。 |
| [^60] | [Homograph Attacks on Maghreb Sentiment Analyzers](https://arxiv.org/abs/2402.03171) | 同形异义词攻击对马格里布情感分析器造成了严重影响，将其性能从F1得分0.95降低到0.33。本研究主要旨在强调LLMs的弱点，并优先考虑机器学习的道德和责任。 |
| [^61] | [Is Mamba Capable of In-Context Learning?](https://arxiv.org/abs/2402.03170) | 本研究证明，新提出的选择性结构化状态空间模型Mamba具有与transformers类似的上下文学习（ICL）能力。对于涉及较长输入序列的ICL任务，Mamba可以成为transformers的高效替代品。 |
| [^62] | [A Random Matrix Approach to Low-Multilinear-Rank Tensor Approximation](https://arxiv.org/abs/2402.03169) | 该研究采用随机矩阵方法，在低多线性秩张量逼近中展示了对种植的低秩信号的估计，并根据大维谱行为和信噪比准确预测了重建性能，并给出了HOOI收敛的充分条件。 |
| [^63] | [Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity](https://arxiv.org/abs/2402.03167) | 本文提出了一种单循环的去中心化双级优化算法（D-SOBA），首次阐明了网络拓扑和数据异构性对去中心化双级算法的共同影响。D-SOBA在渐近速率、渐近梯度/海森复杂性和瞬态梯度/海森复杂性方面达到了最先进水平。 |
| [^64] | [Optimal and Near-Optimal Adaptive Vector Quantization](https://arxiv.org/abs/2402.03158) | 该论文提出了最优和近似最优的自适应向量量化算法，能够优化量化过程，在时间和空间复杂度上具有改进，可扩展应用于各种机器学习任务。 |
| [^65] | [DogSurf: Quadruped Robot Capable of GRU-based Surface Recognition for Blind Person Navigation](https://arxiv.org/abs/2402.03156) | DogSurf是一种四足机器人的新方法，可以帮助视力受损的人在现实世界中导航，在表面识别和分类任务中具有99.925%的准确率。 |
| [^66] | [Learning solutions of parametric Navier-Stokes with physics-informed neural networks](https://arxiv.org/abs/2402.03153) | 本论文利用物理合理的神经网络学习参数化的Navier-Stokes方程的解决方案函数，并通过生成的数值解进行训练，实现了对一系列参数下的速度和压力函数的插值。 |
| [^67] | [A Comparative Analysis of Microrings Based Incoherent Photonic GEMM Accelerators](https://arxiv.org/abs/2402.03149) | 本文详细分析了基于微环的相干光学GEMM加速器的组织，其通过分裂、聚合、调制、加权和求和等方式操作光信号以加速深度神经网络中的矩阵-矩阵乘法，提高吞吐量和能量效率。 |
| [^68] | [A Multi-step Loss Function for Robust Learning of the Dynamics in Model-based Reinforcement Learning](https://arxiv.org/abs/2402.03146) | 本文提出了一种用于稳健学习模型预测的多步损失函数，通过加权平均均方误差损失在不同未来时间点上来训练单步模型。这种损失函数在存在噪声的情况下尤为有效，并在各种任务中取得了显著的预测改善。 |
| [^69] | [SafEDMD: A certified learning architecture tailored to data-driven control of nonlinear dynamical systems](https://arxiv.org/abs/2402.03145) | SafEDMD是一种基于EDMD的学习架构，通过稳定性和认证导向，生成可靠的数据驱动替代模型，并基于半定规划进行认证控制器设计。它在多个基准示例上展示了优于现有方法的优势。 |
| [^70] | [Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models](https://arxiv.org/abs/2402.03142) | 本文提出了一种简单、通用、非参数的剪枝算法KEN，它能在保持模型性能的同时大幅节省内存，通过选择性地保留最重要的参数实现了对transformer模型的优化。与其他方法相比，KEN在最少参数减少25%的情况下实现了与原始模型相等或更好的性能。 |
| [^71] | [Boosting Long-Delayed Reinforcement Learning with Auxiliary Short-Delayed Task](https://arxiv.org/abs/2402.03141) | 这篇论文提出了一种名为Auxiliary-Delayed Reinforcement Learning (AD-RL)的方法，通过利用辅助的短时延任务来加速长时延任务的学习过程，同时在随机环境中保持性能。该方法能显著降低样本复杂度，并在确定性和随机基准测试中表现出优异的样本效率和性能。 |
| [^72] | [Enhancing Neural Subset Selection: Integrating Background Information into Set Representations](https://arxiv.org/abs/2402.03139) | 这项研究提出了一种能够将背景信息融入神经子集选择任务中的方法，通过将超集的不变量统计量纳入所关注的子集，实现了对特定超级子集的识别。 |
| [^73] | [Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations](https://arxiv.org/abs/2402.03138) | 本文提出了一种基于聚类和预训练表示的方法，用于在高维空间中进行探索。通过对随机和预训练表示进行聚类，可以有效计算状态数，特别是在多维环境中预训练表示更加有效。 |
| [^74] | [Sociolinguistically Informed Interpretability: A Case Study on Hinglish Emotion Classification](https://arxiv.org/abs/2402.03137) | 通过研究Hinglish情感分类，我们发现预训练语言模型能够学习到语言选择与情感表达之间的关联，尤其是在混合语言数据存在时。这对于情感分类的解释性具有重要意义。 |
| [^75] | [Constrained Decoding for Cross-lingual Label Projection](https://arxiv.org/abs/2402.03131) | 本文提出了一种解决零样本跨语言迁移学习中翻译质量下降问题的方法。 |
| [^76] | [How Free is Parameter-Free Stochastic Optimization?](https://arxiv.org/abs/2402.03126) | 这个论文研究了无参随机优化的问题，提出了一种完全无参的方法，通过简单的超参数搜索技术在非凸和凸设置下都能取得优于先进算法的性能。同时，论文还建立了一个下界，指出完全无参的方法在某些情况下无法实现。 |
| [^77] | [Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks](https://arxiv.org/abs/2402.03124) | 本研究旨在研究梯度反转攻击中消除硬标签约束，考虑到标签平滑和mixup技术的实际情况。我们提出了一种算法，可以同时恢复增强标签和输入特征，并为标签恢复方法提供了必要条件。 |
| [^78] | [Good Teachers Explain: Explanation-Enhanced Knowledge Distillation](https://arxiv.org/abs/2402.03119) | 通过优化解释增强的知识蒸馏（e$^2$KD）算法，可以让学生模型在准确性和学生-教师一致性方面都得到大幅度提升，确保学生模型从教师那里正确学到原因。 |
| [^79] | [Feature-Action Design Patterns for Storytelling Visualizations with Time Series Data](https://arxiv.org/abs/2402.03116) | 该论文提出了一种基于时间序列数据创建故事化可视化的方法，通过引入特征行为模式，使得故事板能够适应动态到达或选择的数据中可能出现的潜在特征。 |
| [^80] | [Discovering interpretable models of scientific image data with deep learning](https://arxiv.org/abs/2402.03115) | 本文提出了使用解耦表示学习、稀疏深度神经网络训练和符号回归的方法，在复杂图像数据中形成高度简约且准确性可媲美黑盒模型的可解释模型，进一步探讨了这些模型在解释生物现象方面的应用。 |
| [^81] | [Infrared Spectra Prediction for Diazo Groups Utilizing a Machine Learning Approach with Structural Attention Mechanism](https://arxiv.org/abs/2402.03112) | 本文提出了一种利用结构关注机制的机器学习方法，可以预测和解释偶氮化合物的红外光谱。该方法通过集中关注与功能团邻近的化学信息，显著提升了光谱预测的准确性、鲁棒性和可解释性，为揭示红外光谱特征与分子结构之间的相关性提供了一种可扩展和高效的途径。 |
| [^82] | [Non-Stationary Latent Auto-Regressive Bandits](https://arxiv.org/abs/2402.03110) | 本文提出了非平稳潜在自回归赌博机问题，并提出了一个算法，在这种环境下可以达到较低的遗憾率。 |
| [^83] | [High-dimensional Bayesian Optimization via Covariance Matrix Adaptation Strategy](https://arxiv.org/abs/2402.03104) | 本文提出了一种高维贝叶斯优化方法，通过协方差矩阵适应策略定义局部搜索区域，能够解决将贝叶斯优化应用于高维优化问题的挑战。 |
| [^84] | [Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases](https://arxiv.org/abs/2402.03099) | 该论文介绍了一种基于意图的提示校准方法，通过迭代优化和生成合成边界情况数据来改进提示工程，以提高大型语言模型的性能。 |
| [^85] | [Transcending Adversarial Perturbations: Manifold-Aided Adversarial Examples with Legitimate Semantics](https://arxiv.org/abs/2402.03095) | 本文提出一种基于流形辅助的生成模型，能够生成具有真实和合法语义的对抗样本。实验结果表明，这些对抗样本不仅具有更好的视觉质量，而且能够实现更高的攻击可迁移性。 |
| [^86] | [Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector](https://arxiv.org/abs/2402.03094) | 本文提出了一种跨领域少样本目标检测器，通过增强的开集目标检测方法来解决跨领域数据差异带来的性能下降问题。 |
| [^87] | [Dual Lagrangian Learning for Conic Optimization](https://arxiv.org/abs/2402.03086) | 本文介绍了对偶拉格朗日学习（DLL）方法，通过结合锥对偶理论和机器学习模型表示能力，在参数化线性和非线性锥优化问题上提供了有效的拉格朗日对偶界限，证明了其性能在优化问题上接近最优解的0.5%。 |
| [^88] | [Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual Text Processing](https://arxiv.org/abs/2402.03082) | 这份综述论文对近期视觉文本处理领域的最新进展进行了全面的、多角度的分析，包括文本图像增强、恢复、操作等方面。通过深入讨论特定的文本特征，如结构、笔划、语义、风格和空间内容等，揭示了有效利用这些独特文本特征在视觉文本处理中的重要性。 |
| [^89] | [Preference-Conditioned Language-Guided Abstraction](https://arxiv.org/abs/2402.03081) | 本研究提出了一种基于用户偏好的语言引导的抽象化方法，通过观察人类行为的变化并使用语言模型查询用户的偏好，构建适用于不同用户的状态抽象。 |
| [^90] | [Markov Persuasion Processes: Learning to Persuade from Scratch](https://arxiv.org/abs/2402.03077) | 这篇论文提出了马尔可夫说服过程模型，用于捕捉发送者和接收者顺序交互的情景。论文解决了现有模型中的问题，提供了针对发送者没有环境知识的解决方案。通过设计学习算法，证明了算法的性能。这个方法的总结要点是提出了马尔可夫说服过程模型，并提出了针对没有环境知识的发送者的学习算法。 |
| [^91] | [Learning to Abstract Visuomotor Mappings using Meta-Reinforcement Learning](https://arxiv.org/abs/2402.03072) | 本研究通过使用元强化学习来探索人类获取多个全新视觉运动映射的能力，并发现情境线索在学习中起到了重要的作用，提供了计算上的优势。 |
| [^92] | [Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes Uncertainty](https://arxiv.org/abs/2402.03055) | 概率演员-评论家算法（PAC）通过在评论家中建模和推断不确定性，以改进强化学习中的连续控制性能，并实现自适应的探索策略。 |
| [^93] | [Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations](https://arxiv.org/abs/2402.03053) | 本文提出了一种基于大型语言模型的多语言马来西亚嵌入方法，通过微调Llama2和Mistral模型，在语义相似性和RAG任务中取得了显著的性能提升。 |
| [^94] | [EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models](https://arxiv.org/abs/2402.03049) | EasyInstruct是一个易于使用的用于大型语言模型的指令处理框架，通过模块化指令生成、选择和提示，并考虑它们的组合和交互，使指令处理更加方便和高效。 |
| [^95] | [Cooperative Learning with Gaussian Processes for Euler-Lagrange Systems Tracking Control under Switching Topologies](https://arxiv.org/abs/2402.03048) | 本文提出了一种新颖的基于学习的方法来解决在切换通信拓扑下部分未知动力学的Euler-Lagrange多智能体系统的跟踪控制问题，通过高效的协作算法框架和高斯过程回归，捕捉智能体之间的相关性进行不确定性预测，并通过稳定性分析确保了有界的跟踪误差概率。 |
| [^96] | [PFDM: Parser-Free Virtual Try-on via Diffusion Model](https://arxiv.org/abs/2402.03047) | PFDM是一种基于扩散模型的免解析器虚拟试穿方法，可以无缝地在目标人物身上“穿”上服装，无需准确的分割掩码，并通过服装融合注意力机制达到高保真度的试穿效果。 |
| [^97] | [Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning](https://arxiv.org/abs/2402.03046) | Open RL Benchmark是一组跟踪强化学习实验的数据，并涵盖了广泛的RL库和参考实现。它提供了完整的原始数据，可以用来衡量RL算法的有效性，也可供社区进行下载、使用和贡献。 |
| [^98] | [SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach](https://arxiv.org/abs/2402.03043) | 本论文探究了可解释的人工智能（XAI）方法在文本领域的适用性，并将相似度差异和独特性（SIDU）方法扩展到文本数据，提供了对模型预测关键的有上下文意义的文本元素的解释。本研究采用了一个综合的三层评估框架来评估XAI方法。 |
| [^99] | [InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions](https://arxiv.org/abs/2402.03040) | “InteractiveVideo”是一个以用户为中心的视频生成框架，通过交互式的多模态指令，用户可以在整个生成过程中精确、有效地指导生成模型，并对视频的关键方面进行灵活调整。 |
| [^100] | [Automatic Combination of Sample Selection Strategies for Few-Shot Learning](https://arxiv.org/abs/2402.03038) | 本文研究了20种样本选择策略对少样本学习性能的影响，并提出了一种自动组合样本选择策略的方法（ACSESS），在多个数据集上证明了其优越性能。 |
| [^101] | [Functional SDE approximation inspired by a deep operator network architecture](https://arxiv.org/abs/2402.03028) | 本文提出了一种受深度算子网络结构启发的函数SDE近似方法，通过深度神经网络和多项式混沌展开实现对随机微分方程解的近似，并通过学习减轻指数级复杂度的问题。 |
| [^102] | [Understanding and Guiding Weakly Supervised Entity Alignment with Potential Isomorphism Propagation](https://arxiv.org/abs/2402.03025) | 本文通过传播视角分析了弱监督的实体对齐任务，并提出一种潜在同构传播操作符来增强知识图谱之间的邻域信息传播。通过验证，发现基于聚合的实体对齐模型中的潜在对齐实体具有同构子图。 |
| [^103] | [Data-induced multiscale losses and efficient multirate gradient descent schemes](https://arxiv.org/abs/2402.03021) | 本文研究了多尺度数据对机器学习算法的影响，并提出了一种基于数据的新的梯度下降方法，旨在提高训练效率。 |
| [^104] | [Taylor Videos for Action Recognition](https://arxiv.org/abs/2402.03019) | Taylor视频是一种新的视频格式，用于动作识别中的动作提取问题。它通过使用Taylor展开近似计算隐含的动作提取函数，从而解决了动作提取中的挑战性问题。 |
| [^105] | [Toward Green and Human-Like Artificial Intelligence: A Complete Survey on Contemporary Few-Shot Learning Approaches](https://arxiv.org/abs/2402.03017) | 本文全面调查了少样本学习领域的最新进展，探讨了该方法在解决深度学习在实际应用中的限制方面的潜力和挑战。 |
| [^106] | [Whom to Trust? Elective Learning for Distributed Gaussian Process Regression](https://arxiv.org/abs/2402.03014) | 本文介绍了一种使用高斯过程回归增强分布式协作学习的创新方法，并开发了一种选举学习算法，使智能体能够根据邻居的可信度有选择性地请求预测。该方法提高了个体预测的准确性，同时消除了计算密集型方差计算的需求，并确保了预测的可靠性。 |
| [^107] | [On the Impact of Output Perturbation on Fairness in Binary Linear Classification](https://arxiv.org/abs/2402.03011) | 输出扰动对二元线性分类中公平性的影响在个体公平性方面是有界的但与模型维度成正比，在群体公平性方面则由角边距的分布决定。 |
| [^108] | [Diffusive Gibbs Sampling](https://arxiv.org/abs/2402.03008) | 扩散吉布斯采样是一种创新的采样方法，通过集成扩散模型并应用吉布斯采样，有效地从具有远程和断开模态特征的分布中采样，表现出比其他方法更好的混合性能，并在多种任务中取得显著改进的结果。 |
| [^109] | [On the development of a practical Bayesian optimisation algorithm for expensive experiments and simulations with changing environmental conditions](https://arxiv.org/abs/2402.03006) | 本文在受环境条件变化影响的昂贵实验和模拟中，将贝叶斯优化方法推广到包含可控和不可控参数的系统优化中，通过在所有变量上拟合全局代理模型，但只在对不可控变量的测量条件下优化可控参数。 |
| [^110] | [Careful with that Scalpel: Improving Gradient Surgery with an EMA](https://arxiv.org/abs/2402.02998) | 通过将训练损失梯度和辅助梯度在训练梯度方向上的正交投影结合起来，使用EMA（指数移动平均）可以改进梯度手术，提高深度学习估计管道的性能。 |
| [^111] | [Text-Guided Image Clustering](https://arxiv.org/abs/2402.02996) | 这篇论文提出了一种文本引导的图像聚类方法，使用图像字幕和视觉问答模型生成文本，然后对生成的文本进行聚类，并通过注入任务或领域知识来改进聚类结果。实验证明，获得的文本表示通常优于图像特征，而基于关键词的解释可以更好地描述聚类。 |
| [^112] | [Decoding-time Realignment of Language Models](https://arxiv.org/abs/2402.02992) | 本研究提出了解码时间对齐（DeRa）方法，可以在不重新训练模型的情况下探索和评估不同的规则化强度，从而对齐语言模型和人类偏好。 |
| [^113] | [DexDiffuser: Generating Dexterous Grasps with Diffusion Models](https://arxiv.org/abs/2402.02989) | DexDiffuser是一种使用扩散模型生成灵巧抓取姿势的新方法，通过对物体点云的生成、评估和优化，实现了较高的抓取成功率。 |
| [^114] | [A Safety-Adapted Loss for Pedestrian Detection in Automated Driving](https://arxiv.org/abs/2402.02986) | 本文提出了一种安全意识的损失函数变体，利用训练过程中估计的每个行人的关键性评分，以增强对关键行人的检测性能。 |
| [^115] | [Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing](https://arxiv.org/abs/2402.02985) | 本论文提出了一种无监督的路况解析框架，利用视觉语言模型和基本计算机视觉模型来解决无人机高分辨率影像下的路况场景解析问题。该框架首先利用视觉语言模型快速检测路况感兴趣区域，然后利用视觉基础模型生成路况区域掩模，然后采用自监督表示学习网络提取特征表示，最后通过无监督聚类算法对特征进行聚类和标记。 |
| [^116] | [Review on Fault Diagnosis and Fault-Tolerant Control Scheme for Robotic Manipulators: Recent Advances in AI, Machine Learning, and Digital Twin](https://arxiv.org/abs/2402.02980) | 这篇综述文章介绍了针对机器人操纵器的故障诊断和容错控制方案的最新进展，重点关注人工智能、机器学习和数字孪生等尖端技术对机器人控制和容错能力的变革影响。 |
| [^117] | [Variational Flow Models: Flowing in Your Style](https://arxiv.org/abs/2402.02977) | 我们引入了一种变分流模型的方法，并提出了一种系统的无需训练的转换方法，使得快速采样成为可能，同时保持了采样的准确性和效率。 |
| [^118] | [Boosting, Voting Classifiers and Randomized Sample Compression Schemes](https://arxiv.org/abs/2402.02976) | 本研究提出了一种随机提升算法来解决传统提升算法的性能问题，并通过构建一个通用框架将样本压缩方法扩展到支持随机学习算法，实现了在样本大小上具有单对数依赖的泛化错误。 |
| [^119] | [Retrieval-Augmented Score Distillation for Text-to-3D Generation](https://arxiv.org/abs/2402.02972) | RetDream是一种针对文本到3D生成的检索增强的得分蒸馏方法，通过直接使用语义相关的资源，可以充分利用2D扩散模型的表现力和3D资源的几何一致性。 |
| [^120] | [Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features](https://arxiv.org/abs/2402.02969) | 通过研究随机特征，我们发现注意力层具有较高的词敏感性，这对于理解transformers的成功以及自然语言处理任务中的上下文含义非常重要。 |
| [^121] | [Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives](https://arxiv.org/abs/2402.02968) | 本调查对多模态多任务视觉理解基础模型在道路场景中的应用进行了系统分析，展示了其多模态和多任务学习能力，在处理各种驾驶相关任务方面具有强大的适应性，为实现对周围场景的更全面理解做出了重要贡献。 |
| [^122] | [Mixed Noise and Posterior Estimation with Conditional DeepGEM](https://arxiv.org/abs/2402.02964) | 本论文提出了一种用于联合估计贝叶斯逆问题中后验概率和噪声参数的新算法，该算法通过期望最大化（EM）算法解决问题，并使用条件标准化流来近似后验概率。该模型能够整合来自多个测量的信息。 |
| [^123] | [One-class anomaly detection through color-to-thermal AI for building envelope inspection](https://arxiv.org/abs/2402.02963) | 通过彩色图像到热成像的AI驱动预测，我们提出了一种无标签的方法用于检测建筑外观热成像检测中的异常情况，可以应用于辅助日常建筑检查或自动化检查大面积。 |
| [^124] | [Multi-Agent Reinforcement Learning for Offloading Cellular Communications with Cooperating UAVs](https://arxiv.org/abs/2402.02957) | 本文提出了一种新颖的方法，通过联合优化无人机轨迹和用户关联指标，最大化用户与无人机的关联，以有效地最大化多个无人机在卸载地面基站的数据流量方面的利用率。 |
| [^125] | [AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image](https://arxiv.org/abs/2402.02956) | AdaTreeFormer是一种从源领域学习并适应只有有限数量标注树木的目标领域的框架，利用一个共享的编码器和分层特征提取方案，实现了树木计数的少样本领域自适应。 |
| [^126] | [Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form Game Approach](https://arxiv.org/abs/2402.02954) | 本文通过应用最优性原理研究了分层信息共享的分布式部分可观察马尔可夫决策过程的解决方法。通过将问题分解成单阶段子游戏，并通过进一步分解子游戏，我们成功地解开了决策变量的纠缠，同时显著减少了时间复杂度。 |
| [^127] | [Unraveling the Key of Machine Learning Solutions for Android Malware Detection](https://arxiv.org/abs/2402.02953) | 本文通过调查和分析，提出了一个全面研究基于机器学习的Android恶意软件检测的方案，并重新实现了12个代表性方法的评估。 |
| [^128] | [On Least Squares Estimation in Softmax Gating Mixture of Experts](https://arxiv.org/abs/2402.02952) | 本研究探讨了在确定性MoE模型下使用最小二乘估计器的性能，并建立了强可识别性条件来描述不同类型专家函数的收敛行为。 |
| [^129] | [Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine Workers](https://arxiv.org/abs/2402.02951) | $\textsf{DynaBRO}$是一种动态拜占庭-强鲁棒学习的方法，能够适应切换拜占庭工作机制，并且在渐近收敛速率上与静态情况相匹配。通过多级蒙特卡洛渐变估计技术、强鲁棒工作机制更新的聚合和故障安全过滤器的引入，我们的方法能够经受住$\mathcal{O}(\sqrt{T})$轮拜占庭身份的改变。另外，通过使用自适应学习率，我们的方法消除了对百分比的需求。 |
| [^130] | [Kernel PCA for Out-of-Distribution Detection](https://arxiv.org/abs/2402.02949) | 本论文提出了使用核PCA进行外分布检测的方法，通过在主成分子空间中引入非线性映射，实现了对内分布和外分布数据的有效区分。 |
| [^131] | [HoughToRadon Transform: New Neural Network Layer for Features Improvement in Projection Space](https://arxiv.org/abs/2402.02946) | 本文引入了HoughToRadon变换层，该层通过在Hough变换层后进行改良，使得神经网络的速度得到提升，并且在语义图像分割问题上取得了97.7％的准确率。 |
| [^132] | [Exploring the Synergies of Hybrid CNNs and ViTs Architectures for Computer Vision: A survey](https://arxiv.org/abs/2402.02941) | 本文调查了卷积神经网络和视觉变换器混合架构在计算机视觉中的协同作用，提供了对最新混合CNN-ViT架构的综述。翻译 |
| [^133] | [Design and Implementation of an Automated Disaster-recovery System for a Kubernetes Cluster Using LSTM](https://arxiv.org/abs/2402.02938) | 本研究设计和实现了一个使用LSTM的自动化灾难恢复系统，该系统能够快速检测灾难并在15秒内自动从另一个Kubernetes集群恢复应用程序，提高了云环境中的数据管理和恢复效率。 |
| [^134] | [Panoramic Image Inpainting With Gated Convolution And Contextual Reconstruction Loss](https://arxiv.org/abs/2402.02936) | 本论文提出了一种全景图像修复框架，采用门控卷积来区分有效像素和无效像素，并利用上下文重建损失来引导生成器找到最适合修复缺失区域的参考补丁。 |
| [^135] | [InterpretCC: Conditional Computation for Inherently Interpretable Neural Networks](https://arxiv.org/abs/2402.02933) | InterpretCC是一种新的解释性神经网络模型，通过条件计算和稀疏激活特征，在保持性能的同时实现了人类中心的解释能力。该模型适用于需要可信解释、可操作解释和准确预测的人类面向领域。 |
| [^136] | [Domain Adaptation of Multilingual Semantic Search - Literature Review](https://arxiv.org/abs/2402.02932) | 这篇文献综述对当前在低资源环境下进行领域自适应和多语义搜索的方法进行了概述，提出了一种新的方法来聚类和有效地组合这些方法，并探讨了在低资源环境下将多语义搜索与领域自适应方法进行组合的可能性。 |
| [^137] | [Embedding Hardware Approximations in Discrete Genetic-based Training for Printed MLPs](https://arxiv.org/abs/2402.02930) | 本文将硬件近似嵌入到印刷多层感知器的训练过程中，通过离散遗传算法实现了最大化硬件近似的效益，在5%的精度损失下，相比基线，实现了超过5倍的面积和功耗的减少，并且超过了最先进的近似方法。 |
| [^138] | [Instance Segmentation XXL-CT Challenge of a Historic Airplane](https://arxiv.org/abs/2402.02928) | 该论文介绍了一个历史飞机的XXL-CT实例分割挑战，该挑战旨在评估机器学习图像分割领域的最新进展，并探索了自动或交互式实例分割方法在有效勾勒飞机组件的能力和局限性。 |
| [^139] | [Automated Cognate Detection as a Supervised Link Prediction Task with Cognate Transformer](https://arxiv.org/abs/2402.02926) | 本文提出了一种基于Transformer的方法，用于自动同源检测，实验证明该方法在一定程度的监督下表现优于现有方法，并能随着进一步增加监督而稳定改进。同时，我们还证明了接受多个序列对齐作为输入，并具有端到端架构的重要性。 |
| [^140] | [Mining a Minimal Set of Behavioral Patterns using Incremental Evaluation](https://arxiv.org/abs/2402.02921) | 本论文介绍了一种使用增量评估挖掘最小的行为模式集合的方法，以解决现有方法的可扩展性和实际应用中冗余模式的问题。 |
| [^141] | [DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network](https://arxiv.org/abs/2402.02910) | 本研究提出了一种使用双尺度多阶段时间卷积网络的Otago体操识别方法，通过单个腰部佩戴的IMU在老年人的日常生活中实现准确且稳定的识别，为康复举措提供支持。 |
| [^142] | [Digital Twin for Grey Box modeling of Multistory residential building thermal dynamics](https://arxiv.org/abs/2402.02909) | 本研究提出了一种架构，通过将实时物联网数据与建筑物的3D表示集成，来促进灰盒建模来研究建筑的热动力学。 |
| [^143] | [ViewFusion: Learning Composable Diffusion Models for Novel View Synthesis](https://arxiv.org/abs/2402.02906) | ViewFusion 是一种用于新视角合成的最新端到端生成方法，具有无与伦比的灵活性，通过同时应用扩散去噪和像素加权掩模的方法解决了先前方法的局限性。 |
| [^144] | [Replication of Impedance Identification Experiments on a Reinforcement-Learning-Controlled Digital Twin of Human Elbows](https://arxiv.org/abs/2402.02904) | 本研究利用强化学习控制的数字孪生模型成功复制了人体肘部阻抗识别实验，并发现强化学习智能体在稳定肘关节运动方面表现出比人体更高的阻抗，为虚拟环境模拟在神经机械研究中的潜力提供了初步证据。 |
| [^145] | [Black-Box Approximation and Optimization with Hierarchical Tucker Decomposition](https://arxiv.org/abs/2402.02890) | 该论文介绍了一种基于分层Tucker分解的黑盒逼近和优化方法，通过低秩分解和指数选择提高了准确性，并且在高维度问题上表现出鲁棒性。 |
| [^146] | [Exploring Federated Self-Supervised Learning for General Purpose Audio Understanding](https://arxiv.org/abs/2402.02889) | 本研究探索了联邦自监督学习在通用音频理解中的应用，提出了一种新的Federated SSL框架FASSL，并发现在音频检索任务中，音频F-SSL方法与集中式音频-SSL方法的性能不相上下。 |
| [^147] | [Time-, Memory- and Parameter-Efficient Visual Adaptation](https://arxiv.org/abs/2402.02887) | 这项研究提出了一种时间、内存和参数高效的视觉适应方法，通过设计一个轻量级的并行网络，在不反向传播梯度的情况下对预训练主干网络的特征进行操作，实现了在训练时间和内存使用上的高效。该方法在VTAB基准测试中取得了最先进的准确性和参数权衡，并超越了先前的工作在训练时间和内存使用方面的优势。 |
| [^148] | [Approximate Attributions for Off-the-Shelf Siamese Transformers](https://arxiv.org/abs/2402.02883) | 中文总结出的一句话要点: 本文介绍了一种适用于现有孪生变压器的近似归因方法，该方法在保留原模型性能的同时实现了准确归因能力。我们通过比较近似和准确归因，分析了模型对不同语言方面的关注，并发现孪生变压器主要忽略否定，同时深入研究了它们对句法角色的关注程度，以及如何判断语义上的差异。 |
| [^149] | [Unleashing the Expressive Power of Pulse-Based Quantum Neural Networks](https://arxiv.org/abs/2402.02880) | 本文研究了基于脉冲的量子神经网络的表达能力，证明了在特定条件下，这种模型可以逼近任意非线性函数，并可能在复杂学习任务上具有更强大的表达能力。 |
| [^150] | [How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning](https://arxiv.org/abs/2402.02872) | 本论文探索了大型语言模型如何进行上下文学习的机制，提出了一个使用定位和投影方法的假设。通过查询和键矩阵来计算输入文本与每个演示之间的注意力权重，以学习它们之间的相似度度量。实验证明了我们的分析。 |
| [^151] | [Statistics without Interpretation: A Sober Look at Explainable Machine Learning](https://arxiv.org/abs/2402.02870) | 解释算法往往数学上复杂且难以解释，这导致解释错误。为了向前推进，解释算法需要明确其输出的解释方式，并澄清可以和不能回答的问题。这一论点基于统计学和解释之间的区别，以及可解释机器学习和应用统计学之间的相似性。 |
| [^152] | [Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem](https://arxiv.org/abs/2402.02868) | 细调强化学习模型中的遗忘问题会导致转移效果差，研究发现常见且具有灾难性后果。通过使用标准的知识保留技术可以缓解这个问题并最大程度地利用细调的优势。 |
| [^153] | [Quantum Normalizing Flows for Anomaly Detection](https://arxiv.org/abs/2402.02866) | 本文介绍了一种将任意分布计算为预定义分布的量子标准化流方法，该方法在异常检测方面具有与传统方法相竞争的性能，并且可以在量子计算机上执行。 |
| [^154] | [On combining acoustic and modulation spectrograms in an attention LSTM-based system for speech intelligibility level classification](https://arxiv.org/abs/2402.02865) | 本研究探讨了使用声学和调制谱图相结合的注意力LSTM系统对语音可懂性级别进行分类的方法，并提出了使用逐帧调制谱图作为输入特征以及两种不同的融合策略。模型在包含不同严重程度的口吃语音的UA-Speech数据库上进行了评估。 |
| [^155] | [Graph Neural Machine: A New Model for Learning with Tabular Data](https://arxiv.org/abs/2402.02862) | 本论文提出了一种新的机器学习模型，图神经机器（GNM），用于处理表格数据。GNM使用同步消息传递方案，并用几乎完全图代替了多层感知机（MLP）的有向无环图。实验结果表明，在多个数据集上，GNM模型的性能优于MLP架构。 |
| [^156] | [Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning](https://arxiv.org/abs/2402.02858) | 本文对比了在基于模型的离线强化学习中，使用深度自回归密度网络和神经集合的方法。通过在D4RL基准测试上展示，我们质疑了使用神经集合的普遍观点，并发现单个良好校准的自回归模型可以获得更好的性能。同时，我们还分析了模型学习的静态指标，并得出了关于代理最终性能的重要模型特性。 |
| [^157] | [Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation](https://arxiv.org/abs/2402.02857) | 本文对于具有偏态梯度和自适应步长的SGD进行了全面的非渐进分析，证明了Adagrad和RMSProp算法在收敛速度上与无偏情况相似，并通过实验结果验证了收敛结果，展示了如何降低偏差的影响。 |
| [^158] | [Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation](https://arxiv.org/abs/2402.02855) | 本文提出了一种针对推荐模型的新型学习范式，称为动态稀疏学习（DSL），通过从头训练一个轻量级稀疏模型，解决了模型大小和学习效率的问题。 |
| [^159] | [Enhancing Compositional Generalization via Compositional Feature Alignment](https://arxiv.org/abs/2402.02851) | 通过组合特征对齐，增强了模型的组合通用性，使其能够推广到未见过的领域-类别组合。 |
| [^160] | [An Attention Long Short-Term Memory based system for automatic classification of speech intelligibility](https://arxiv.org/abs/2402.02850) | 本研究的主要贡献是使用长短期记忆网络和对数梅尔频谱图来预测语音可懂性水平，同时引入了注意力机制以提高性能。 |
| [^161] | [Machine Learning Resistant Amorphous Silicon Physically Unclonable Functions (PUFs)](https://arxiv.org/abs/2402.02846) | 这项研究调查了利用非晶硅材料制作物理不可克隆函数 (PUF) 的应用，通过使用机器学习算法分析攻击这些集成电路PUF的效果，发现深度神经网络 (DNNs) 是最有效的算法，但仍无法完全破解a-Si PUF的安全性，此外，研究发现非晶硅PUFs的机器学习抗性与其非线性响应的强度相关。 |
| [^162] | [Trinity: Syncretizing Multi-/Long-tail/Long-term Interests All in One](https://arxiv.org/abs/2402.02842) | 本文提出了一种统一的兴趣建模框架“Trinity”，通过利用长期线索来解决兴趣遗忘问题，并改善多兴趣建模任务。通过构建实时聚类系统和计算统计兴趣直方图，Trinity能够识别用户的兴趣并进行个性化推荐。 |
| [^163] | [Shortened LLaMA: A Simple Depth Pruning for Large Language Models](https://arxiv.org/abs/2402.02834) | 使用简单的深度修剪方法可以提高大规模语言模型的推理速度，在内存受限的条件下表现良好，对部署在本地和边缘设备上的LLMs有帮助。 |
| [^164] | [PowerGraph: A power grid benchmark dataset for graph neural networks](https://arxiv.org/abs/2402.02827) | PowerGraph是一个用于图神经网络的电网基准数据集，旨在通过机器学习模型实现电力网格断电的在线检测。 |
| [^165] | [SynthVision - Harnessing Minimal Input for Maximal Output in Computer Vision Models using Synthetic Image data](https://arxiv.org/abs/2402.02826) | 本研究提出了一种新方法，通过仅使用合成数据来构建一个全面的计算机视觉模型，用于检测人类乳头瘤病毒生殖器疣。使用扩散模型快速生成高质量的训练数据，并评估其对视觉模型的影响。 |
| [^166] | [Evading Data Contamination Detection for Language Models is (too) Easy](https://arxiv.org/abs/2402.02823) | 本研究指出语言模型数据污染的检测方法在面对恶意模型提供者的有意污染时存在漏洞，并提出了一种简单而有效的污染技术（EAL）来显著提高基准测试性能且逃避当前的检测方法。 |
| [^167] | [Revisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective](https://arxiv.org/abs/2402.02820) | 这篇论文提出了一种名为FCVAE的无监督时间序列异常检测方法，通过同时集成全局和局部频率特征，显著提高了对正常数据的重构精度。 |
| [^168] | [Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing](https://arxiv.org/abs/2402.02817) | 本文提出了一种基于贝叶斯最优的公平分类方法，通过先处理、中处理和后处理来最小化分类错误，并在给定群体公平性约束的情况下进行优化。该方法引入了线性和双线性差异度量的概念，并找到了贝叶斯最优公平分类器的形式。本方法能够处理多个公平性约束和常见情况。 |
| [^169] | [Intersectional Two-sided Fairness in Recommendation](https://arxiv.org/abs/2402.02816) | 本文针对推荐系统中的交叉双边公平性问题，提出了一种名为交叉双边公平推荐（ITFR）的新方法，通过利用锐度感知损失感知劣势群体，使用协作损失平衡开发不同交叉群体的一致区分能力，并利用预测得分归一化来公平对待不同交叉群体中的正例。实验证明该方法在提高交叉双边公平性方面取得了显著效果。 |
| [^170] | [State estimation of urban air pollution with statistical, physical, and super-learning graph models](https://arxiv.org/abs/2402.02812) | 本文介绍了基于城市图的不同重建方法，包括完全数据驱动、物理驱动和混合型，以及结合超级学习模型。这些方法在法国巴黎市中心的实时重建城市空气污染地图问题中进行了测试。 |
| [^171] | [Graph-enhanced Large Language Models in Asynchronous Plan Reasoning](https://arxiv.org/abs/2402.02805) | 本研究是关于使用大型语言模型(LLMs)进行异步计划推理的首次大规模研究。我们发现在没有提供任务解决过程插图的情况下，现有的LLMs表现不佳。为此，我们提出了一种称为Plan Like a Graph (PLaG)的新技术，通过将图与自然语言提示相结合，实现了最先进的结果。然而，当任务复杂性增加时，LLMs仍然存在严重降级的问题，突显了LLMs在模拟数字设备方面的局限性。这项研究为将LLMs作为高效自主代理迈出了重要一步。 |
| [^172] | [Large Language Model Distilling Medication Recommendation Model](https://arxiv.org/abs/2402.02803) | 本研究旨在利用大型语言模型改进药物推荐方法，以解决传统模型在医学数据语义理解和新患者处方推荐方面的挑战。 |
| [^173] | [KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models](https://arxiv.org/abs/2402.02801) | KS-Lottery是一种寻找多语言语言模型中有效参数的方法，通过使用Kolmogorov-Smirnov检验来分析参数分布偏移，并证明了在嵌入层中可以找到认证的中奖票。这种方法可以在微调中获得与全面微调相当的性能，同时减少了所需的参数数量。 |
| [^174] | [Joint Attention-Guided Feature Fusion Network for Saliency Detection of Surface Defects](https://arxiv.org/abs/2402.02797) | 提出了一种联合注意引导特征融合网络（JAFFNet），用于表面缺陷明显性检测。JAFFNet通过融合低层和高层特征，并引入稠密感受野模块，成功解决了缺陷尺度变化、复杂背景、低对比度等挑战。 |
| [^175] | [A Learning-Based Caching Mechanism for Edge Content Delivery](https://arxiv.org/abs/2402.02795) | 基于学习的边缘缓存框架HR-Cache能够优化边缘缓存，提高字节命中率，降低网络负载，并加速内容传递给用户。 |
| [^176] | [Rethinking Optimization and Architecture for Tiny Language Models](https://arxiv.org/abs/2402.02791) | 本研究重新思考了微型语言模型的优化和架构，通过经验研究发现了在微型语言模型中特别有效的设计公式，并在多语种数据集上训练了高性能的微型语言模型。 |
| [^177] | [Stable and Robust Deep Learning By Hyperbolic Tangent Exponential Linear Unit (TeLU)](https://arxiv.org/abs/2402.02790) | 本文提出了一种新的神经网络激活函数——双曲正切指数线性单元（TeLU），通过解决传统激活函数的局限性，如梯度消失和爆炸问题，实现了稳定和稳健的深度学习。与流行的激活函数相比，TeLU在稳定性和性能上具有更优异的表现，并在大规模测试中验证了其优越性能。 |
| [^178] | [Dual Knowledge Distillation for Efficient Sound Event Detection](https://arxiv.org/abs/2402.02781) | 这项研究提出了一种称为双重知识蒸馏的框架，用于开发高效的声音事件检测系统。这个框架通过时序平均知识蒸馏和增强嵌入特征蒸馏来稳定地蒸馏知识并提升上下文学习能力，在实验中取得了良好的性能。 |
| [^179] | [Accelerating Matroid Optimization through Fast Imprecise Oracles](https://arxiv.org/abs/2402.02774) | 本论文研究了如何通过使用快速但不准确的预测模型来加速拟阵优化问题，并提出了实际算法，这些算法在维持对不同质量的预测模型的鲁棒性的同时，只使用了很少的查询 |
| [^180] | [Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning](https://arxiv.org/abs/2402.02772) | 在强化学习中应用扩散模型进行规划常受限于基础分布和样本多样性。本文提出的CDiffuser方法通过对比学习来提高到达高回报状态的概率。 |
| [^181] | [Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate](https://arxiv.org/abs/2402.02769) | 通过从教学中学习（LoT）技术，我们提出了一种深度神经网络正则化技术，能够通过引入辅助学生模型来提升主模型的泛化性能。实验证明，LoT能有效识别具有泛化和可教授关系的信息。 |
| [^182] | [Intent Profiling and Translation Through Emergent Communication](https://arxiv.org/abs/2402.02768) | 本研究提出了一个基于人工智能的意图概要和翻译框架，通过新兴通信实现应用程序的意图概要，解决了应用程序与网络之间复杂的通信问题。 |
| [^183] | [Focal Modulation Networks for Interpretable Sound Classification](https://arxiv.org/abs/2402.02754) | 本文通过引入焦点调制网络（FocalNets）来解决音频领域中可解释性的问题，首次在环境声音分类任务中应用，并在可解释性和准确性方面优于类似大小的视觉变换器和专门用于音频领域的PIQ方法。 |
| [^184] | [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache](https://arxiv.org/abs/2402.02750) | 该论文提出了一种无需调整的非对称2位量化KV缓存技术，以解决存储注意力键和值的内存需求增加和推断速度受限问题。 |
| [^185] | [Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization](https://arxiv.org/abs/2402.02746) | 标准 Gaussian 过程在高维贝叶斯优化中表现优秀，经验证据显示其在函数估计和协方差建模中克服了高维输入困难，比专门为高维优化设计的方法表现更好。 |
| [^186] | [Glocal Hypergradient Estimation with Koopman Operator](https://arxiv.org/abs/2402.02741) | 本文提出了一种具有Koopman算子的全局超梯度估计方法，通过使用局部超梯度的轨迹来高效地近似全局超梯度，实现了超参数的贪婪优化，兼具可靠性和效率。 |
| [^187] | [DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models](https://arxiv.org/abs/2402.02739) | 本文首次系统地探究了后门扩散模型中毒噪声输入的可检测性，发现分布差异在木马检测中起着重要作用，并提出了一种低成本的触发检测机制。 |
| [^188] | [Improving Robustness of LiDAR-Camera Fusion Model against Weather Corruption from Fusion Strategy Perspective](https://arxiv.org/abs/2402.02738) | 本文从融合策略的视角评估了LiDAR-Camera融合模型对抗常见天气干扰的鲁棒性，并提出了一种灵活权重融合策略以增强模型的鲁棒性。 |
| [^189] | [Using Motion Cues to Supervise Single-Frame Body Pose and Shape Estimation in Low Data Regimes](https://arxiv.org/abs/2402.02736) | 本研究提出在数据不足的情况下，利用易获得的未注释视频来监督单帧人体姿势和形状估计。通过计算连续帧的姿势和光流，并通过强制保持图像光流与姿势变化光流的一致性，可以更有效地优化网络权重，并与使用更多带注释数据训练的方法表现相当。 |
| [^190] | [ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer](https://arxiv.org/abs/2402.02733) | 本研究提出了一种新颖的一阶段方法，结合肖像风格转换实现人脸逆龄化，解决了NPR图像上编辑年龄的问题，并在单个生成步骤中执行。该方法利用了现有的人脸逆龄化和风格转换网络，并且独特地融合了不同的潜在向量，从而保留了面部属性。 |
| [^191] | [A Generative Approach to Surrogate-based Black-box Attacks](https://arxiv.org/abs/2402.02732) | 提出了一种基于生成模型的黑盒替代攻击方法，通过学习样本分布，生成对抗性样本，用于攻击深度神经网络。 |
| [^192] | [Fast and Accurate Cooperative Radio Map Estimation Enabled by GAN](https://arxiv.org/abs/2402.02729) | 本文提出了一种基于GAN的合作式无线电地图估计方法（GAN-CRME），能够快速准确地估计无线电地图，无需发射机信息。该方法利用移动用户处的接收信号强度测量和地理地图之间的交互作用，通过深度神经网络推断无线电地图，并使用基于GAN的学习算法提高性能。 |
| [^193] | [FDNet: Frequency Domain Denoising Network For Cell Segmentation in Astrocytes Derived From Induced Pluripotent Stem Cells](https://arxiv.org/abs/2402.02724) | 本论文提出了一种名为FDNet的频域去噪神经网络用于识别由诱导多能干细胞分化而来的星形胶质细胞。由于背景和干扰信息的存在，观察星形胶质细胞变得困难，而现有的深度学习方法无法解决这个问题。本文还引入了一个新的数据集IAI704来解决这个问题。 |
| [^194] | [Discounted Adaptive Online Prediction](https://arxiv.org/abs/2402.02720) | 本论文提出了一种折扣自适应在线预测算法，该算法适应于复杂的损失序列和比较器，并改进了非自适应算法。算法具有无需结构性假设的理论保证，并且在超参数调整方面具有鲁棒性。通过在线符合预测任务的实验证明了算法的好处。 |
| [^195] | [Understanding the planning of LLM agents: A survey](https://arxiv.org/abs/2402.02716) | 这项调查研究系统地介绍了基于LLM的代理规划的最新进展和挑战。 |
| [^196] | [Position Paper: What Can Large Language Models Tell Us about Time Series Analysis](https://arxiv.org/abs/2402.02713) | 大语言模型有潜力颠覆时间序列分析，提升决策效率，推动时间序列分析智能的普及化。这种进展可以带来模态切换和时间序列问答等多种可能性。 |
| [^197] | [Architectural Strategies for the optimization of Physics-Informed Neural Networks](https://arxiv.org/abs/2402.02711) | 本研究从神经架构的角度研究了物理信息神经网络（PINNs）的优化。通过利用神经切向核（NTK），我们发现高斯激活函数在有效训练PINNs时超过其他替代激活函数。此外，我们引入了一种经过预处理的神经架构，进一步增强了优化过程。这些发现在多个PDEs的验证中得到了证实。 |
| [^198] | [Representation Surgery for Multi-Task Model Merging](https://arxiv.org/abs/2402.02705) | 该论文提出了一种名为“Surgery”的表征手术解决方案，用于减少多任务模型合并中的表示偏差。该方法通过一个轻量级的任务专用模块，针对合并模型的表示进行修正，以提高合并模型的性能。 |
| [^199] | [Understanding What Affects Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence](https://arxiv.org/abs/2402.02701) | 本文通过理论和实证研究，揭示了在测试环境具有干扰因素时影响视觉强化学习中泛化差距的关键因素。结果表明，最小化训练和测试环境之间的表示距离是减少泛化差距最关键的因素。 |
| [^200] | [Sample Complexity Characterization for Linear Contextual MDPs](https://arxiv.org/abs/2402.02700) | 本文研究了线性上下文马尔可夫决策过程（CMDPs）的样本复杂性表征，并提出了两种模型的新颖算法，证明它们具有所需的多项式样本复杂性。其中，对于第一个模型，通过去除可达性假设，改进了现有结果。 |
| [^201] | [Adversarial Data Augmentation for Robust Speaker Verification](https://arxiv.org/abs/2402.02699) | 本论文提出了一种称为对抗性数据增强的方法，它通过结合数据增强和对抗性学习的方式来提高深度说话人模型的鲁棒性。通过使用一个额外的增强分类器来分类不同类型的增强，网络能够生成对该分类器具有欺骗性的说话人嵌入，从而提高了在面对不相关声学变化时的泛化性。 |
| [^202] | [Beyond Expectations: Learning with Stochastic Dominance Made Practical](https://arxiv.org/abs/2402.02698) | 这项工作首次尝试建立了一个随机优势学习的通用框架，并推广了随机优势的概念以使其能够在任意两个随机变量之间进行比较。同时，我们还开发了一种有效的计算方法来处理连续性评估的问题。 |
| [^203] | [Deep Equilibrium Models are Almost Equivalent to Not-so-deep Explicit Models for High-dimensional Gaussian Mixtures](https://arxiv.org/abs/2402.02697) | 本文通过对深度均衡模型和显式神经网络模型进行理论分析和实验证明，在高维高斯混合数据下，可以通过设计浅显式网络来实现与给定深度均衡模型相同的特征光谱行为。 |
| [^204] | [Causal Feature Selection for Responsible Machine Learning](https://arxiv.org/abs/2402.02696) | 本调查论文研究了负责任机器学习中的因果特征选择，强调了其对解释性、公平性、对抗鲁棒性和域泛化的作用。 |
| [^205] | [Exploiting Class Probabilities for Black-box Sentence-level Attacks](https://arxiv.org/abs/2402.02695) | 该论文研究了在黑盒子句级攻击中利用类别概率的有效性，并开发了一种新的算法进行攻击。通过与基线方法进行对比，进行了广泛的评估。 |
| [^206] | [Description on IEEE ICME 2024 Grand Challenge: Semi-supervised Acoustic Scene Classification under Domain Shift](https://arxiv.org/abs/2402.02694) | 这项研究介绍了ICME 2024 Grand Challenge中的半监督领域转移下的声场分类任务，该任务探索了不同区域之间领域转移的挑战，同时研究了如何利用未标记数据来提升声场分类模型的性能。 |
| [^207] | [Statistical Guarantees for Link Prediction using Graph Neural Networks](https://arxiv.org/abs/2402.02692) | 本文提出了一种线性图神经网络（LG-GNN）架构，通过计算边缘概率来预测图中的链接，并推导了其在链接预测任务中的性能统计保证。这种架构对于稀疏和稠密图都适用，并在真实和合成数据集上验证了其优势。 |
| [^208] | [Poisson Process for Bayesian Optimization](https://arxiv.org/abs/2402.02687) | 提出了一种基于泊松过程的新型排名替代模型，引入了称为泊松过程贝叶斯优化（PoPBO）的高效BO框架，并从经典的LCB和EI模型中得出了两个定制的收集函数以适应它。 |
| [^209] | [Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions](https://arxiv.org/abs/2402.02686) | 本研究提出了一种名为多区域马尔可夫高斯过程的方法，将高斯过程和线性动态系统相结合，有效地发现了多个脑区之间的方向性通讯。通过建立LDS与多输出GP之间的联系，该模型实现了线性推断并提供了可解释的低维表示。 |
| [^210] | [Equivariant Symmetry Breaking Sets](https://arxiv.org/abs/2402.02681) | 这里是中文总结出的一句话要点: 该论文提出了一种全等变的对称破缺框架，通过引入对称破缺集来破坏等变神经网络中的对称性。这种方法通用且适用于任何群的等变性。 |
| [^211] | [Large Language Models are Geographically Biased](https://arxiv.org/abs/2402.02680) | 本文研究了大型语言模型的地理偏见，并展示了其对地理空间预测的系统错误，通过零射击地理空间预测来评估其对世界的认知。 |
| [^212] | [Counterfactual Explanations of Black-box Machine Learning Models using Causal Discovery with Applications to Credit Rating](https://arxiv.org/abs/2402.02678) | 本研究提出了一种新的可解释人工智能框架，通过使用反事实概率和额外的因果结构先验信息，克服了因果图未知的问题，可以解释黑盒机器学习模型并应用于信贷评级等领域。 |
| [^213] | [Verifiable evaluations of machine learning models using zkSNARKs](https://arxiv.org/abs/2402.02675) | 本论文提出了一种使用zkSNARKs进行机器学习模型评估的方法。通过模型推理和零知识计算证明，可以提供可验证的评估证明，验证模型在公开输入上的性能和公平性指标。这有助于解决闭源商业机器学习模型评估可信性的问题。 |
| [^214] | [Estimation of conditional average treatment effects on distributed data: A privacy-preserving approach](https://arxiv.org/abs/2402.02672) | 本论文提出了一种数据协作双机器学习（DC-DML）方法，该方法可以在保护分布式数据隐私的情况下估计条件平均治疗效果（CATE）模型。通过数值实验验证了该方法的有效性。该方法的三个主要贡献是：实现了对分布式数据上的非迭代通信的半参数CATE模型的估计和测试，提高了模型的鲁棒性。 |
| [^215] | [Utility-Based Reinforcement Learning: Unifying Single-objective and Multi-objective Reinforcement Learning](https://arxiv.org/abs/2402.02665) | 基于效用的强化学习范式将多目标强化学习引入到单目标强化学习中，具有多个潜在益处，并探讨了算法意义。 |
| [^216] | [Counterfactual Fairness Is Not Demographic Parity, and Other Observations](https://arxiv.org/abs/2402.02663) | 这里是中文总结出的一句话要点：文章探讨了因果概念与纯粹概率概念之间的等价性，并发现计算上的公正并不等同于人口统计数据的平等。同时还纠正了一些有关计算上的公正的误解。 |
| [^217] | [Image-Caption Encoding for Improving Zero-Shot Generalization](https://arxiv.org/abs/2402.02662) | 本研究提出了一种名为图像-字幕编码（ICE）的方法，通过在评估时对图像和字幕条件下的预测进行一致性约束，来改善图像分类模型的分布外泛化能力。 |
| [^218] | [Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision](https://arxiv.org/abs/2402.02658) | 本文介绍了一种名为模型引导的过程监督（MiPS）的新方法，该方法通过对推理模型的解决方案进行抽样完成来自动进行数据整理，从而避免了昂贵的人工注释。实证结果表明，与之前的工作相反，我们应优先选择验证器预测得分高的验证。这种方法显著改进了PaLM 2在数学和编码任务上的性能。 |
| [^219] | [Learning with Mixture of Prototypes for Out-of-Distribution Detection](https://arxiv.org/abs/2402.02653) | 本文提出了一种使用多个原型进行模型学习的方法，用于外部分布的检测任务。通过这种方法，可以更好地捕捉数据中的多样性，并提高模型的性能。 |
| [^220] | [Vision-Language Models Provide Promptable Representations for Reinforcement Learning](https://arxiv.org/abs/2402.02651) | 本论文提出一种利用预训练的视觉-语言模型作为可提示的表示，为强化学习提供世界知识，使得代理能够更快地学习新的行为。在实验中，我们发现使用这种表示训练的策略在复杂环境下表现更好，优于通用图像表示和遵循指示的方法。 |
| [^221] | [Variational DAG Estimation via State Augmentation With Stochastic Permutations](https://arxiv.org/abs/2402.02644) | 使用状态扩展和随机排列进行变分DAG估计的方法可以超越竞争的贝叶斯和非贝叶斯基准方法，从而在估计贝叶斯网络结构方面取得更好的性能。 |
| [^222] | [LLM-Enhanced Data Management](https://arxiv.org/abs/2402.02643) | LLMDB是一种LLM增强的数据管理范式，通过LLM的微调和提示工程嵌入领域特定知识，解决了虚构、高成本和低准确性的挑战，并实现了泛化能力和高推理能力。 |
| [^223] | [$C^*$-Algebraic Machine Learning: Moving in a New Direction](https://arxiv.org/abs/2402.02637) | $C^*$-代数机器学习是将$C^*$-代数与机器学习结合的新研究方向，它通过统一现有的学习策略，并构建更多元化和信息丰富的数据模型的新框架，为机器学习提供了一种新的方法。 |
| [^224] | [Can Large Language Models Learn Independent Causal Mechanisms?](https://arxiv.org/abs/2402.02636) | 本论文研究在大型语言模型中学习独立因果机制的方法，以增强模型在分布变化下的鲁棒性和泛化能力。 |
| [^225] | [Key-Graph Transformer for Image Restoration](https://arxiv.org/abs/2402.02634) | 我们提出了一种关键图变换器（KGT）用于高效地进行图像修复，通过选择性地连接关键节点，减少了计算负担，并且在6个图像修复任务上展示了最先进的性能。 |
| [^226] | [Predicting Machine Translation Performance on Low-Resource Languages: The Role of Domain Similarity](https://arxiv.org/abs/2402.02633) | 该论文研究了预测低资源语言机器翻译性能的关键因素，发现领域相似性对于预测模型性能具有最重要的影响。 |
| [^227] | [Learning to Understand: Identifying Interactions via the Mobius Transform](https://arxiv.org/abs/2402.02631) | 本文研究了通过Mobius变换识别相互作用的问题，提出了一种算法可以在非零系数较少的情况下精确恢复Mobius变换，并揭示了群体理论和信息论之间的联系。 |
| [^228] | [Stability Analysis of Various Symbolic Rule Extraction Methods from Recurrent Neural Network](https://arxiv.org/abs/2402.02627) | 本文分析了基于循环神经网络的两种规则提取方法：量化和等价查询。实验结果表明，O2RNN和基于量化的规则提取方法在稳定性和性能方面优于其他方法。 |
| [^229] | [Position bias in features](https://arxiv.org/abs/2402.02626) | 本文提出了一种用于搜索引擎的动态排名系统的特征，该特征可以准确估计文档相关性，但是在存在位置偏差的情况下会有高方差，并强调了准确估计位置偏差的必要性，建议同时使用有偏和无偏的位置偏差特征。 |
| [^230] | [Enhancing Transformer RNNs with Multiple Temporal Perspectives](https://arxiv.org/abs/2402.02625) | 引入了多个时间视角的概念，用于增强Transformer RNNs对顺序数据的理解能力，在参数数量最小增加的情况下取得了显著的改进。 |
| [^231] | [A Safe Reinforcement Learning driven Weights-varying Model Predictive Control for Autonomous Vehicle Motion Control](https://arxiv.org/abs/2402.02624) | 提出了一种安全的强化学习驱动的权重变化模型预测控制方法，用于自动驾驶车辆运动控制。该方法通过在运行时动态调整代价函数权重，可以提供上下文相关的最优闭环控制性能。 |
| [^232] | [DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging](https://arxiv.org/abs/2402.02622) | DenseFormer是对Transformer的简单修改，通过在每个transformer块之后进行深度加权平均，提高了模型的困惑度。学到的加权平均权重揭示了信息流的连贯模式，使得DenseFormer具有更高的数据效率，并且在相同困惑度下胜过传统的Transformer模型。 |
| [^233] | [Increasing Trust in Language Models through the Reuse of Verified Circuits](https://arxiv.org/abs/2402.02619) | 本文介绍了一种通过重复使用经过验证的电路来增加语言模型的可信度的方法。研究者通过构建数学和逻辑规范的框架，并对一个n位整数加法模型进行完全验证。他们插入训练好的加法模型到一个未经训练的模型中，通过训练组合模型执行加法和减法。他们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了减法模型的验证。 |
| [^234] | [The Virtues of Pessimism in Inverse Reinforcement Learning](https://arxiv.org/abs/2402.02616) | 本论文提出了一种使用离线强化学习算法来加速逆推强化学习中强化学习子程序的替代方法，通过保持与专家数据分布接近的悲观主义策略，提高了逆推强化学习的样本效率。 |
| [^235] | [PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?](https://arxiv.org/abs/2402.02611) | 本研究通过PuzzleBench数据集探索了LLMs解决困难的一阶组合推理问题的能力，并提出了Puzzle-LM方法，该方法将LLMs与符号求解器和程序解释器相结合，使其能够有效地推理这类问题。 |
| [^236] | [Accelerating Inverse Reinforcement Learning with Expert Bootstrapping](https://arxiv.org/abs/2402.02608) | 本文提出两个简单的方法来加速逆强化学习，通过更好地使用专家示范，减少对内部强化学习循环中的硬探索的需求。这些方法在MaxEntIRL的基础上取得了显著的增益。 |
| [^237] | [Evading Deep Learning-Based Malware Detectors via Obfuscation: A Deep Reinforcement Learning Approach](https://arxiv.org/abs/2402.02600) | 该论文提出了一种基于深度强化学习的方法，通过使用开源加密工具和强化学习框架成功地混淆恶意软件，规避了最先进的恶意软件检测器，并且在规避率方面超越了使用高级修改方法的技术。 |
| [^238] | [Dual Interior-Point Optimization Learning](https://arxiv.org/abs/2402.02596) | 本文介绍了双内点优化学习和双超梯度学习两种方法，用于学习带有有界变量的参数线性规划的对偶可行解。这些方法通过预测约束对应的对偶变量，确保对偶可行性，并且能够提供高保真度的对偶可行解和有效的对偶界限。 |
| [^239] | [Leveraging Continuously Differentiable Activation Functions for Learning in Quantized Noisy Environments](https://arxiv.org/abs/2402.02593) | 在量化噪声环境中，利用连续可微激活函数进行学习可以减轻模拟量化误差，为计算机视觉、信号处理等多个机器学习领域的硬件实现提供了指导。 |
| [^240] | [Unified Training of Universal Time Series Forecasting Transformers](https://arxiv.org/abs/2402.02592) | 本研究提出了一种改进的时间序列Transformer架构，名为Moirai，以解决时间序列预测中的跨频率学习、适应多变量时间序列以及解决大规模数据固有的不同分布特性等挑战，从而实现了统一训练通用时间序列预测Transformer。 |
| [^241] | [Controller Synthesis from Noisy-Input Noisy-Output Data](https://arxiv.org/abs/2402.02588) | 本论文研究了从受噪声影响的输入输出数据中合成动态输出反馈控制器的问题，并提出了一种能稳定所有与数据一致的可能系统的控制器设计方法，同时还解决了将结果扩展到通用多输入多输出系统的问题。 |
| [^242] | [ClipFormer: Key-Value Clipping of Transformers on Memristive Crossbars for Write Noise Mitigation](https://arxiv.org/abs/2402.02586) | ClipFormer算法用于减轻变压器在交叉栏上的写入噪声对注意力层中键值矩阵的影响，提高推理准确性。 |
| [^243] | [DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image Editing](https://arxiv.org/abs/2402.02583) | DiffEditor提出了一种解决现有扩散式图像编辑的准确性和灵活性问题的方法，通过引入图像提示和细节操作，以及结合随机微分方程和梯度引导，有效地提高了编辑效果。 |
| [^244] | [Impact of PSF misestimation and galaxy population bias on precision shear measurement using a CNN](https://arxiv.org/abs/2402.02578) | 本文研究了使用卷积神经网络进行精确剪切测量，并探究了点扩散函数错误估计和星系种群偏差对测量精度的影响。 |
| [^245] | [Gazebo Plants: Simulating Plant-Robot Interaction with Cosserat Rods](https://arxiv.org/abs/2402.02570) | 本研究针对机器人与植物的交互问题，提出了一个基于Cosserat杆的Gazebo仿真平台插件，有效解决了物理引擎对非刚性物体的限制，有助于农业机器人训练和优化。 |
| [^246] | [On the Complexity of Finite-Sum Smooth Optimization under the Polyak-{\L}ojasiewicz Condition](https://arxiv.org/abs/2402.02569) | 本文研究了满足Polyak-Lojasiewicz条件的有限和平滑优化问题的复杂性。在单机情况下，为了找到ε次优解，任何梯度方法都需要至少Ω(n+κ√nlog(1/ε))个IFO调用。在分布式情况下，最小化PL函数的问题的下界为Ω(κ/√γlog(1/ε))，Ω((κ+τκ/√γ)log(1/ε))和Ω(n+κ√... |
| [^247] | [DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models](https://arxiv.org/abs/2402.02563) | DefInt提出了一种默认干预框架，通过默认使用较小规模的语言模型生成推理思路，然后通过反思推理干预解决复杂推理问题，从而提高混合大型语言模型的效率和准确性。 |
| [^248] | [Foundation Model Makes Clustering a Better Initialization for Active Learning](https://arxiv.org/abs/2402.02561) | 本研究提出了一种基于基础模型和聚类方法的主动学习初始化方案，用于选择最具信息量的样本。基础模型是通过自监督训练在大规模数据集上训练得到的，并能生成适用于各种下游任务的紧凑嵌入表示。 |
| [^249] | [Enhancing Robustness in Biomedical NLI Models: A Probing Approach for Clinical Trials](https://arxiv.org/abs/2402.02558) | 本研究旨在提升生物医学自然语言推断（NLI）模型的鲁棒性，通过一种基于探测的方法研究了临床试验和模型对自然逻辑的理解。 |
| [^250] | [DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms in Vision Transformers](https://arxiv.org/abs/2402.02554) | 本文提出了一种对抗攻击方法DeSparsify，针对使用Token稀疏化机制的视觉Transformer，通过精心制作的对抗样本欺骗稀疏化机制，导致最坏情况的性能，以此耗尽操作系统的资源并保持隐蔽性。 |
| [^251] | [Neur2BiLO: Neural Bilevel Optimization](https://arxiv.org/abs/2402.02552) | Neur2BiLO是一个针对双层优化问题的框架，通过将神经网络近似引入到混合整数规划中，可以快速生成高质量的解决方案。 |
| [^252] | [Obstacle Avoidance Deep Reinforcement Learning-Based Trajectory Planner with Robust Low-Level Control for Robotic Manipulators](https://arxiv.org/abs/2402.02551) | 这篇论文提出了一种基于深度强化学习和鲁棒低级控制的机器人操作器的障碍物避障轨迹规划方法，该方法通过与环境的交互积极参与学习，绕过了计算复杂性，同时解决了非重复和随机的避障任务。 |
| [^253] | [Are Large Language Models Table-based Fact-Checkers?](https://arxiv.org/abs/2402.02549) | 本研究初步探讨了大型语言模型在基于表格的事实检查方面的潜力。实验结果表明，通过提示工程，大型语言模型在零样本和少样本的情况下可以实现可接受的表现。 |
| [^254] | [Classification of Tennis Actions Using Deep Learning](https://arxiv.org/abs/2402.02545) | 本研究使用深度学习探索了对网球动作进行分类的潜力和挑战，并通过使用学术网球数据集THETIS进行了训练和评估。最佳模型达到了74%的泛化准确率，为网球动作分类提供了良好的性能。然后，对最佳模型进行了错误分析，并指出了改进网球数据集的方向。同时，讨论了数据集和当前公开可用的网球数据集的限制，并提出了未来的研究方向。 |
| [^255] | [LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model](https://arxiv.org/abs/2402.02544) | LHRS-Bot 是一个利用自愿地理信息(VGI)增强的大型多模态语言模型，旨在解决近期MLLM在遥感领域中未对多样的地理景观和物体进行充分考虑的问题。通过引入多层次视觉-语言对齐策略和课程学习方法，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。 |
| [^256] | [CompeteSMoE - Effective Training of Sparse Mixture of Experts via Competition](https://arxiv.org/abs/2402.02526) | 本文提出了CompeteSMoE方法，通过竞争机制解决了稀疏专家混合模型的表示崩溃问题，实现了有效的训练，在大型语言模型上取得了强大性能提升。 |
| [^257] | [Absolute convergence and error thresholds in non-active adaptive sampling](https://arxiv.org/abs/2402.02522) | 提出了一种计算非主动自适应采样中绝对收敛和误差阈值的方法，可以确定模型何时不再增加质量，并提供了一个接近条件来估算模型实现目标的接近度，从而支持模型选择中学习参数的微调。 |
| [^258] | [SIMPL: A Simple and Efficient Multi-agent Motion Prediction Baseline for Autonomous Driving](https://arxiv.org/abs/2402.02519) | 本文提出了一种简单高效的自动驾驶多智能体运动预测基线(SIMPL)，通过引入紧凑高效的全局特征融合模块以及使用Bernstein基多项式对连续轨迹参数化的方法，实现了实时准确的运动预测，为下游规划任务提供了有价值的数据。 |
| [^259] | [Latent Graph Diffusion: A Unified Framework for Generation and Prediction on Graphs](https://arxiv.org/abs/2402.02518) | 本文提出了一种统一框架，能够使用一个模型解决各级别和各类型的图学习任务，通过潜在图扩散模型生成和预测节点、边和图级别的特征，具有可证明的保证，并在实验证明了其有效性。 |
| [^260] | [Adaptive scheduling for adaptive sampling in POS taggers construction](https://arxiv.org/abs/2402.02516) | 本论文提出了一种自适应调度的自适应采样方法，用于在构建词性标注器中加快训练速度，同时提高采样的鲁棒性。该方法分析学习曲线的形状，在任何时候增加或减少实例，以确保获得学习能力的净增益。 |
| [^261] | [Modeling of learning curves with applications to pos tagging](https://arxiv.org/abs/2402.02515) | 该论文提出了一种估计学习曲线演化的算法，可以通过部分训练数据结果来预测学习过程中达到期望准确度所需的时间。 |
| [^262] | [Deep Supervision by Gaussian Pseudo-label-based Morphological Attention for Abdominal Aorta Segmentation in Non-Contrast CTs](https://arxiv.org/abs/2402.02514) | 本文提出了一种基于高斯伪标签的形态注意力深度监督方法，应用于非对比度CT腹主动脉分割任务中。通过该方法，可以在训练过程中保留主动脉的形态特征并减轻模糊边界的影响，降低过拟合的风险。 |
| [^263] | [Early stopping by correlating online indicators in neural networks](https://arxiv.org/abs/2402.02513) | 本文提出了一种在神经网络中最小化泛化误差的新技术，通过利用一系列在线指标的时间相关性，找到过拟合现象，并提供了可靠的提前停止条件，从而提高了预测能力。 |
| [^264] | [PoCo: Policy Composition from and for Heterogeneous Robot Learning](https://arxiv.org/abs/2402.02511) | PoCo是一种策略组合方法，通过组合不同模态和领域的数据分布，实现了从异构数据中训练通用机器人策略的目标。该方法可以实现场景级和任务级的广义操作技能学习，并在推理时通过任务级组合和分析成本函数进行策略行为的自适应调整。 |
| [^265] | [Device Scheduling and Assignment in Hierarchical Federated Learning for Internet of Things](https://arxiv.org/abs/2402.02506) | 在物联网中，分层联合学习（HFL）通过将模型聚合分配给多个边缘服务器来解决联合学习中的网络拥塞问题。本文提出了一种改进的K-Center算法用于设备调度，并引入了基于深度强化学习的方法来优化设备分配。 |
| [^266] | [Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning](https://arxiv.org/abs/2402.02500) | 通过广泛实验发现基于点云的方法在机器人学习中表现出更好的性能，特别是在各种预训练和泛化任务中。结果表明，点云观测模态对于复杂机器人任务是有价值的。 |
| [^267] | [Robot Trajectron: Trajectory Prediction-based Shared Control for Robot Manipulation](https://arxiv.org/abs/2402.02499) | 本论文提出了一种新的轨迹预测和共享控制方法，通过预测机器人的运动轨迹和辅助操作者的意图，可以减轻操作者的认知负荷和提高操作效率。 |
| [^268] | [Weisfeiler Leman for Euclidean Equivariant Machine Learning](https://arxiv.org/abs/2402.02484) | 本文扩展了2-WL测试的适用范围，包括点云中的位置和速度，并提出了一种简单修改的PPGN架构，以获得一个可近似所有连续等变函数的通用等变架构。 |
| [^269] | [BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback](https://arxiv.org/abs/2402.02479) | BRAIn是一种基于贝叶斯奖励条件化缩减推断的自然语言生成方法，通过反馈来改进RLHF，在LLM对齐中表现出较好的可扩展性和性能。 |
| [^270] | [Why are hyperbolic neural networks effective? A study on hierarchical representation capability](https://arxiv.org/abs/2402.02478) | 本文通过大规模实验对为什么双曲神经网络有效进行了全面分析，并提出了几种预训练策略来增强层级表示能力，从而改进下游任务的性能。 |
| [^271] | [TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling](https://arxiv.org/abs/2402.02475) | TimeSiam是一种用于时间序列建模的预训练框架，通过使用Siamese网络和简单的数据增强方法，能够捕捉时间序列数据的内在时间相关性，并学习内部时序依赖的表示。 |
| [^272] | [Fast Peer Adaptation with Context-aware Exploration](https://arxiv.org/abs/2402.02468) | 本文提出了一种基于上下文感知的探索方法，用于快速适应具有不同策略的未知同伴。通过奖励智能体在历史上下文中有效识别同伴行为模式，该方法能够促进智能体积极探索和快速适应，从而在不确定同伴策略时收集信息反馈，并在有信心时利用上下文执行最佳反应。 |
| [^273] | [A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer](https://arxiv.org/abs/2402.02464) | 这篇论文介绍了GraphsGPT，它使用纯Transformer将非欧几里德图形转换为在欧几里德空间中可学习的图形单词，并通过解码器将图形单词重新构建为原始图形，保证了信息的等价性。预训练的GraphsGPT在图形表示学习和图形生成方面取得了突出成果。 |
| [^274] | [A Fast Method for Lasso and Logistic Lasso](https://arxiv.org/abs/2402.02463) | 本论文提出了一种快速解决Lasso和Logistic Lasso问题的方法，通过采用主动集方法和适当的求解器，成功实现了加速。在压缩感知、Lasso回归和Logistic Lasso回归实验中，与传统方法相比，我们的方法平均能提高约30倍的速度。 |
| [^275] | [Review of multimodal machine learning approaches in healthcare](https://arxiv.org/abs/2402.02460) | 这篇综述主要介绍医疗保健领域中多模态机器学习方法的最新研究进展。通过综合分析最近的文献，探讨了临床诊断中各种数据模态的应用以及融合技术的评估。重点关注影像数据的应用，并介绍了现有的多模态数据集和训练方法。 |
| [^276] | [On Minimum Trace Factor Analysis - An Old Song Sung to a New Tune](https://arxiv.org/abs/2402.02459) | 本文提出了最小化迹因子分析（MTFA）的放松版本，该方法能够有效降低因异方差噪声造成的过拟合问题，并解决了在因子分析和谱方法中常见的异常情况和病态诅咒问题。 |
| [^277] | [Discovering More Effective Tensor Network Structure Search Algorithms via Large Language Models (LLMs)](https://arxiv.org/abs/2402.02456) | 通过大型语言模型（LLMs），我们开发了GPTN-SS算法，用于自动设计更有效的张量网络结构搜索算法。实验证明，这些算法在探索和开发之间取得了更好的平衡，并在搜索高质量的TN结构方面展现出卓越的性能。 |
| [^278] | [On the Role of Initialization on the Implicit Bias in Deep Linear Networks](https://arxiv.org/abs/2402.02454) | 本研究探索了深度学习中隐式偏差的起源，重点研究了由权重初始化引起的隐式偏差。研究结果揭示了初始化对优化和泛化的悖论的作用，为深度学习的性能提供了更全面的理解。 |
| [^279] | [Surfing the modeling of PoS taggers in low-resource scenarios](https://arxiv.org/abs/2402.02449) | 在低资源实验场景中，我们评估了早期学习曲线估计作为选择最合适模型的实用方法，并研究了在资源贫乏环境中的可靠性。 |
| [^280] | [Breaking MLPerf Training: A Case Study on Optimizing BERT](https://arxiv.org/abs/2402.02447) | 通过改进负载平衡、通信和优化器等各个组件，我们提出了用于快速大规模训练BERT模型的新方法，实现了新水平的BERT训练性能。 |
| [^281] | [LQER: Low-Rank Quantization Error Reconstruction for LLMs](https://arxiv.org/abs/2402.02446) | LQER使用低秩逼近和激活引起的尺度矩阵，实现了对LLMs的近乎无损量化，无需知识蒸馏或梯度优化，并大幅减少硬件资源的使用。 |
| [^282] | [BECLR: Batch Enhanced Contrastive Few-Shot Learning](https://arxiv.org/abs/2402.02444) | 本论文提出了一种增强的批量对比式少样本学习方法（BECLR），通过引入动态聚类内存（DyCE）模块和迭代最优传输的分布对齐策略（OpTA），在预训练和推理阶段分别解决了正样本采样和样本偏差问题，提高了无监督少样本学习性能。 |
| [^283] | [A Momentum Accelerated Algorithm for ReLU-based Nonlinear Matrix Decomposition](https://arxiv.org/abs/2402.02442) | 提出了一种针对基于ReLU的非线性矩阵分解问题的动量加速算法，解决了现有模型中过拟合的问题，并通过实验验证了算法的有效性。 |
| [^284] | [TopoX: A Suite of Python Packages for Machine Learning on Topological Domains](https://arxiv.org/abs/2402.02441) | TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。 |
| [^285] | [DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching](https://arxiv.org/abs/2402.02439) | DiffStitch是一种使用基于扩散的轨迹拼接提升离线强化学习的方法。它通过有效地连接低奖励轨迹和高奖励轨迹，形成全局最优轨迹，以提高离线强化学习算法的性能。 |
| [^286] | [Fast and interpretable Support Vector Classification based on the truncated ANOVA decomposition](https://arxiv.org/abs/2402.02438) | 基于截断ANOVA分解的快速可解释支持向量分类法能够通过使用特征映射和少量维度的多变量基函数来快速且准确地进行高维散乱数据的分类。 |
| [^287] | [Uncertainty-Aware Perceiver](https://arxiv.org/abs/2402.02433) | 不确定性感知者是对感知者模型的五种变体，它们能够获得不确定性估计并在多个指标上取得显著的性能提升。 |
| [^288] | [Learning Mutual Excitation for Hand-to-Hand and Human-to-Human Interaction Recognition](https://arxiv.org/abs/2402.02431) | 本文介绍了一种学习相互激励的图卷积网络（me-GCN），用于手对手和人对人交互识别。通过堆叠相互激励图卷积层（me-GC），该网络能够自适应地建模成对实体之间的互相约束，并提取和合并深度特征。 |
| [^289] | [Exploiting Low-level Representations for Ultra-Fast Road Segmentation](https://arxiv.org/abs/2402.02430) | 本研究利用低层次特征表示进行道路分割，在主流网络模型的主要阶段实现了大部分道路像素的准确表示，提出了以低层次特征为主导的道路分割网络（LFD-RoadSeg）。 |
| [^290] | [Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning](https://arxiv.org/abs/2402.02429) | 本研究提出了一个统一的信息理论框架，将离线元强化学习中的不同方法整合起来，并提出了一种新算法UNICORN，展现了在广泛的任务上显著的泛化能力。 |
| [^291] | [Hybrid-Prediction Integrated Planning for Autonomous Driving](https://arxiv.org/abs/2402.02426) | 本论文介绍了一个混合预测综合规划（HPP）系统，通过引入边缘条件下的占用预测和博弈论运动预测器，解决了自动驾驶系统中预测和规划模块整合所面临的挑战。 |
| [^292] | [EuLagNet: Eulerian Fluid Prediction with Lagrangian Dynamics](https://arxiv.org/abs/2402.02425) | EuLagNet提出了一种新的拉格朗日引导范式，通过跟踪多尺度关键粒子的运动来捕捉多尺度流体动力学。这种方法克服了由于欧拉观察而导致的流体动力学困难，为准确预测未来的流体提供了一种有效的方法。 |
| [^293] | [Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback](https://arxiv.org/abs/2402.02423) | Uni-RLHF是一个通用的强化学习平台和基准套件，致力于处理多样化的人类反馈，解决了在RLHF中量化进展的挑战，并提供了用户友好的注释界面和离线基准实现。 |
| [^294] | [eXplainable Bayesian Multi-Perspective Generative Retrieval](https://arxiv.org/abs/2402.02418) | 本文将不确定性校准和可解释性整合到一个检索流程中，通过引入贝叶斯方法和多透视检索来校准不确定性。使用解释方法分析黑盒模型行为，并将重要性分数作为补充相关性分数，提升基础模型性能。实验证明我们的方法在问答和事实检验任务上显著提高了性能。 |
| [^295] | [Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction](https://arxiv.org/abs/2402.02416) | Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。 |
| [^296] | [GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language Model](https://arxiv.org/abs/2402.02408) | GLaPE提出了一种无依赖于金标签的提示评估方法，通过自一致性作为初始评估分数，进一步改进了产生相同答案的提示的得分的互相一致性，提供了与准确性相一致的可靠评估，即使在没有金标签的情况下。 |
| [^297] | [Defining Neural Network Architecture through Polytope Structures of Dataset](https://arxiv.org/abs/2402.02407) | 本文通过定义上下界确定神经网络宽度，与数据集的几何复杂性相关。同时开发了一种算法，可以从训练好的神经网络中推断数据集的多面体结构。 |
| [^298] | [FreDF: Learning to Forecast in Frequency Domain](https://arxiv.org/abs/2402.02399) | FreDF是一种在频域中学习预测的方法，解决了时间序列建模中标签序列的自相关问题，相比现有方法有更好的性能表现，并且与各种预测模型兼容。 |
| [^299] | [DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models](https://arxiv.org/abs/2402.02392) | DeLLMa是一个旨在提高不确定环境下决策精度的框架，通过多步骤的脚手架程序，借鉴决策理论和效用理论的原则，可以显著提高大型语言模型的决策性能。 |
| [^300] | [Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning](https://arxiv.org/abs/2402.02388) | 本文提出了SAGE，一个通用的面向解决方案的ABM生成框架，利用辅助验证和迭代上下文学习，自动生成针对特定问题的模型和解决方案。 |
| [^301] | [Revisiting the Power of Prompt for Visual Tuning](https://arxiv.org/abs/2402.02382) | 本研究提出了一种改进的视觉调整方法，通过采用下游令牌原型对提示进行初始化，进一步优化令牌构造，有效解决了视觉提示调整中的挑战，并取得了比其他方法更好的性能。 |
| [^302] | [NOAH: Learning Pairwise Object Category Attentions for Image Classification](https://arxiv.org/abs/2402.02377) | NOAH是一种新型的深度神经网络头部结构，通过使用成对对象类别注意力(POCA)来增强分类性能。它能够替代现有的DNN头部，提高分类准确率，并保持模型效率。 |
| [^303] | [AutoTimes: Autoregressive Time Series Forecasters via Large Language Models](https://arxiv.org/abs/2402.02370) | AutoTimes是一种基于大型语言模型的自回归时间序列预测器，利用语言模型的转换能力来处理时间序列数据，实现了与先前模型相当的性能。 |
| [^304] | [Timer: Transformers for Time Series Analysis at Scale](https://arxiv.org/abs/2402.02368) | 本文旨在早期开发大规模时间序列模型（LTSM），通过预训练和GPT风格架构，克服深度模型在小样本场景中的性能瓶颈，并实现在时间序列分析中的大样本泛化能力、可扩展性和任务普适性。 |
| [^305] | [Transolver: A Fast Transformer Solver for PDEs on General Geometries](https://arxiv.org/abs/2402.02366) | Transolver是一种快速的Transformer求解器，通过使用物理注意力和灵活形状的片段来学习离散化几何形状背后隐藏的内在物理状态。它能够有效地捕捉复杂几何形状下的复杂物理相关性，从而提供了具备内生几何生成能力的求解器。 |
| [^306] | [The Developmental Landscape of In-Context Learning](https://arxiv.org/abs/2402.02364) | 在transformers模型中，我们展示了在上下文学习中的离散发展阶段，并引入了两种方法来检测这些阶段的关键里程碑。我们使用行为和结构度量验证了这些方法的有效性。 |
| [^307] | [Unification of Symmetries Inside Neural Networks: Transformer, Feedforward and Neural ODE](https://arxiv.org/abs/2402.02362) | 本研究通过应用物理学中的规范对称性原理，将其应用于神经网络架构，发现各种机器学习模型的参数冗余可以解释为规范对称性。并证明了前馈神经网络中的参数冗余在神经ODE中升级为微分同胚，并找到了Transformer模型与神经ODE及其规范对称性的自然对应关系。 |
| [^308] | [Pruner: An Efficient Cross-Platform Tensor Compiler with Dual Awareness](https://arxiv.org/abs/2402.02361) | Pruner是一种高效跨平台张量编译器，通过参数化静态分析器（PSA）和模式感知成本模型（PaCM）实现张量程序优化，并使用动量转移学习（MTL）策略实现了跨平台适应性。 |
| [^309] | [Incremental Quasi-Newton Methods with Faster Superlinear Convergence Rates](https://arxiv.org/abs/2402.02359) | 本文提出了一种更高效的增量拟牛顿方法，通过引入对称秩一更新和块更新的方式，实现了无条件数的局部超线性收敛速率，并且在数值实验中取得了显著优于基线方法的结果。 |
| [^310] | [Multi-modal Causal Structure Learning and Root Cause Analysis](https://arxiv.org/abs/2402.02357) | 这种方法融合了多种模态的数据，利用对比学习提取模态间的关系，提高了根因定位的效果。 |
| [^311] | [Decentralized Sum-of-Nonconvex Optimization](https://arxiv.org/abs/2402.02356) | 本论文研究了分散场景下的非凸求和优化问题，并提出了一种具有加速效果的随机分散一阶算法。数值实验证实了该算法的有效性。 |
| [^312] | [Symbol: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning](https://arxiv.org/abs/2402.02355) | 本研究提出了一种名为\textsc{Symbol}的新框架，通过符号方程学习来自动发现黑盒优化器。实验结果表明，\textsc{Symbol}生成的优化器在超越现有基准线的同时，还展现出出色的零样本泛化能力。 |
| [^313] | [A Paradigm for Potential Model Performance Improvement in Classification and Regression Problems. A Proof of Concept](https://arxiv.org/abs/2402.02354) | 本文提出了一种方法来增强模型预测性能，通过生成多个辅助模型捕捉属性之间的关系，并生成额外的信息列来潜在地增强目标预测。 |
| [^314] | [Interference-Aware Emergent Random Access Protocol for Downlink LEO Satellite Networks](https://arxiv.org/abs/2402.02350) | 本文提出了一个多智能体深度强化学习（MADRL）框架，用于训练下行低地球轨道（LEO）卫星网络的多址接入协议。我们的方法集中压缩了新型随机接入信令（Ce2RACH），通过联合学习的额外信令消息交换来减轻卫星间干扰，并实现了高达36.65％的网络吞吐量提升。 |
| [^315] | [Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models](https://arxiv.org/abs/2402.02347) | 本研究通过引入Riemannian预条件器，增强了LoRA微调过程中的优化步骤。实验结果表明，使用该预条件器可以显著提升SGD和AdamW的收敛性和可靠性，并使训练过程更加稳健。此外，理论分析证明了在凸参数化下使用该预条件器微调ReLU网络的收敛速度与数据矩阵的条件数无关。这个新的Riemannian预条件器在经典的低秩矩阵恢复中已经有过研究。 |
| [^316] | [Closed-Loop Unsupervised Representation Disentanglement with $\beta$-VAE Distillation and Diffusion Probabilistic Feedback](https://arxiv.org/abs/2402.02346) | 本文提出了闭环无监督表示解缠方法CL-Dis，使用扩散自动编码器（Diff-AE）和β-VAE共同提取语义解缠表示，以解决表示解缠面临的问题。 |
| [^317] | [Stereographic Spherical Sliced Wasserstein Distances](https://arxiv.org/abs/2402.02345) | 本文提出了一种快速且高度并行的用于比较球形测度的距离，使用了立体投影和广义Radon变换，称之为立体投影球面切片瓦瑟斯坦（S3W）距离。通过仔细处理立体投影引起的距离畸变，并进行了理论分析，证明了该方法在速度和效果上的优势。 |
| [^318] | [MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters](https://arxiv.org/abs/2402.02342) | MetaOptimize是一个框架，通过动态调整学习率来优化机器学习算法中的元参数，以提高训练效率和模型性能。 |
| [^319] | [Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning](https://arxiv.org/abs/2402.02340) | 本论文提出了一种基于学习视觉提示的参数高效微调方法，能够在深度度量学习任务中使预训练模型适应本地数据域并保留先前获得的知识。 |
| [^320] | [Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation](https://arxiv.org/abs/2402.02339) | 本文提出了一种不确定性感知的测试时间优化（UAO）框架，通过量化关节点的不确定性来缓解过拟合问题，提高3D人体姿势估计的性能。 |
| [^321] | [Large Language Model Adaptation for Networking](https://arxiv.org/abs/2402.02338) | 本文首次研究了使用大型语言模型（LLM）适应网络问题的方法，通过利用LLM的预训练知识和强大推理能力，实现了“一模型适用于所有”的目标，并取得了更好的性能和更强的泛化能力。 |
| [^322] | [Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning](https://arxiv.org/abs/2402.02334) | 本文研究了深度表格学习中算术特征交互的必要性，通过引入AMFormer模型，实现了在细粒度表格数据建模、训练数据效率和泛化方面的优越性能。 |
| [^323] | [Copyright Protection in Generative AI: A Technical Perspective](https://arxiv.org/abs/2402.02333) | 本文从技术角度全面概述了在生成型人工智能中的版权保护问题，包括数据版权和模型版权两个方面，并提出了一些创新的方法和技术。 |
| [^324] | [Minusformer: Improving Time Series Forecasting by Progressively Learning Residuals](https://arxiv.org/abs/2402.02332) | Minusformer通过逐步学习残差来改进时间序列预测。它将Transformer模型中信息聚合机制从加法改为减法，并通过辅助输出分支逐层学习监督信号的残差，从而提高模型的灵活性和对抗过拟合的能力。 |
| [^325] | [Data-driven algorithm design using neural networks with applications to branch-and-cut](https://arxiv.org/abs/2402.02328) | 本文介绍了一种基于神经网络的数据驱动算法设计方法，并将其应用于分支定界框架中，用于解决混合整数优化问题。 |
| [^326] | [Role of Momentum in Smoothing Objective Function in Implicit Graduated Optimization](https://arxiv.org/abs/2402.02325) | 这篇论文揭示了具有动量的随机梯度下降算法平滑了目标函数，影响程度由多个超参数决定，同时提供了对动量改善泛化能力的理论解释和新见解。 |
| [^327] | [Dynamic Incremental Optimization for Best Subset Selection](https://arxiv.org/abs/2402.02322) | 本文研究了一类$\ell_0$正则化问题的对偶形式，并提出了一种高效的原对偶算法，通过充分利用对偶范围估计和增量策略，提高了最佳子集选择问题的解决方案的效率和统计性质。 |
| [^328] | [Active Learning for Graphs with Noisy Structures](https://arxiv.org/abs/2402.02321) | 本论文提出了一个针对有噪声图的主动学习框架GALClean，通过采用迭代方法实现数据选择和图的清理，解决了选标记数据和获取干净图结构这两个相互依赖的任务。 |
| [^329] | [Spin: An Efficient Secure Computation Framework with GPU Acceleration](https://arxiv.org/abs/2402.02320) | Spin是一个GPU加速的多方计算(MPC)框架，支持多个计算方和不诚实多数对抗设置。该框架提出了针对机器学习关键的非线性函数的优化协议，并进行了针对Transformer模型的注意力的新颖优化，以实现高效且安全的计算。 |
| [^330] | [Diversity Measurement and Subset Selection for Instruction Tuning Datasets](https://arxiv.org/abs/2402.02318) | 本文提出了一种用于指令调整数据集的多样性度量和子集选择的方法，通过使用确定性点过程来捕捉数据集的多样性和质量，并通过对数行列式距离来衡量数据集的多样性。实验证明，在归一化的权重梯度空间中，提出的多样性度量与指令遵循性能相关，并可以用于指导数据选择和分析数据集构建策略。 |
| [^331] | [INViT: A Generalizable Routing Problem Solver with Invariant Nested View Transformer](https://arxiv.org/abs/2402.02317) | INViT是一种具有不变嵌套视图转换器的解决路由问题的方法，通过强制嵌套设计和不变的视图，在编码器内部提高学习求解器的泛化能力，从而实现了在具有不同分布和不同问题规模的TSP和CVRP问题上卓越的泛化性能。 |
| [^332] | [Your Diffusion Model is Secretly a Certifiably Robust Classifier](https://arxiv.org/abs/2402.02316) | 这项研究提出了一种新的扩散分类器家族，称为噪声扩散分类器（NDCs），其具有最新的可证明的鲁棒性。通过将扩散分类器推广到分类高斯受损数据，并将其与随机平滑技术相结合，构建了具有非常量Lipschitzness的平滑分类器。这些NDCs显示出卓越的认证鲁棒性。 |
| [^333] | [Selecting Large Language Model to Fine-tune via Rectified Scaling Law](https://arxiv.org/abs/2402.02314) | 该论文研究了在资源受限的情况下如何选择合适的预训练语言模型进行微调的问题。通过引入修正的缩放定律和预学习数据大小的概念，作者提出了一种新颖的模型选择算法，可以选择接近最优的模型。 |
| [^334] | [Jailbreaking Attack against Multimodal Large Language Model](https://arxiv.org/abs/2402.02309) | 本文提出了一种针对多模态大型语言模型的越狱攻击方法，通过找到“图像越狱提示”，实现在多个未知提示和图像上对语言模型的越狱，并展示了与单模态语言模型的越狱方法之间的联系。同时，通过构建的方法，将该越狱方法应用于单模态语言模型，较现有方法更高效。 |
| [^335] | [Efficient Numerical Wave Propagation Enhanced by an End-to-End Deep Learning Model](https://arxiv.org/abs/2402.02304) | 本文提出了一个由端到端深度学习模型加强的高效数值波传播方法，通过结合数值求解器和深度学习组件，优化算法架构、数据生成和并行时间算法，实现了在保持速度的同时显著提高性能。 |
| [^336] | [A Review and Comparison of AI Enhanced Side Channel Analysis](https://arxiv.org/abs/2402.02299) | 本文回顾和比较了AI增强的侧信道分析技术，重点研究了基于深度学习的模型攻击，并研究了一些新的由深度学习技术增强的方法。 |
| [^337] | [Denoising Diffusion-Based Control of Nonlinear Systems](https://arxiv.org/abs/2402.02297) | 我们提出了一种基于去噪扩散概率模型的新方法，用于控制非线性动力系统，通过学习反向控制系统使得终止状态属于目标集合，我们证明了对于控制仿射系统且满足可控性条件时，控制系统能够完全追踪正向过程的轨迹，并通过数值实验验证了我们的理论结果。 |
| [^338] | [Goodness-of-Fit and Clustering of Spherical Data: the QuadratiK package in R and Python](https://arxiv.org/abs/2402.02290) | QuadratiK软件包是一个在R和Python中实现的数据分析工具，它提供了一套全面的拟合度测试和基于核方法的聚类技术，特别适用于处理球形数据。 |
| [^339] | [Future Directions in Foundations of Graph Machine Learning](https://arxiv.org/abs/2402.02287) | 图机器学习领域的未来方向应该是发展一个更加均衡的理论，从更完整的角度探究图神经网络的表达能力、泛化和优化之间的相互关系。 |
| [^340] | [Multi-Level Feature Aggregation and Recursive Alignment Network for Real-Time Semantic Segmentation](https://arxiv.org/abs/2402.02286) | 该论文提出了一种在实时推理速度下实现高分割准确性的多级特征聚合和递归对齐网络。使用ResNet-18作为骨干，通过多级特征聚合模块和递归对齐模块来提高模型性能。 |
| [^341] | [SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking](https://arxiv.org/abs/2402.02285) | SynthDST是一个针对对话状态跟踪设计的数据生成框架，利用合成数据来实现少样本提示，通过使用少量手工对话模板和对话模式，它能够生成自然、连贯和流畅的带有DST注释的对话，并使Join连通率提升4-5％. |
| [^342] | [Causal Bayesian Optimization via Exogenous Distribution Learning](https://arxiv.org/abs/2402.02277) | 本文引入了一种新的方法，通过学习外源变量的分布，提高了结构化因果模型的近似精度，并将因果贝叶斯优化扩展到更一般的因果方案。 |
| [^343] | [SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing Applications using a Generative Approach](https://arxiv.org/abs/2402.02275) | SudokuSens是一种生成框架，用于通过自动生成训练数据来提高IoT感知应用的深度学习模型鲁棒性。该框架通过模仿实际数据采集过程中未遇到的实验配置，增加了数据的多样性。 |
| [^344] | [InceptionCapsule: Inception-Resnet and CapsuleNet with self-attention for medical image Classification](https://arxiv.org/abs/2402.02274) | InceptionCapsule方法采用了自注意力机制、迁移学习和Inception-ResNet模型，在医学图像分类中解决了初始权重选择和特征提取的问题，取得了较高的准确率。 |
| [^345] | [Federated Learning with New Knowledge: Fundamentals, Advances, and Futures](https://arxiv.org/abs/2402.02268) | 本文系统地探讨了具有新知识的联邦学习，包括如何有效地融入新特征、任务、模型和算法，并分析了新知识到达的形式和时间对纳入过程的影响。同时讨论了联邦学习具有新知识时的潜在未来发展方向。 |
| [^346] | [MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers](https://arxiv.org/abs/2402.02263) | MixedNUTS是一种无需训练的方法，通过非线性混合分类器的转换和概率混合来实现准确性和鲁棒性的平衡。 |
| [^347] | [XTSFormer: Cross-Temporal-Scale Transformer for Irregular Time Event Prediction](https://arxiv.org/abs/2402.02258) | XTSFormer是一个用于不规则时间事件预测的跨时空尺度的Transformer模型，通过新颖的循环感知时间位置编码和分层的多尺度时间注意机制来解决不规则时间间隔、循环、周期性和多尺度事件交互等挑战。 |
| [^348] | [Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times](https://arxiv.org/abs/2402.02255) | 本研究发现，当基于Transformer的语言模型变得越来越大并且在大量数据上进行训练时，模型的惊讶估计与自然人阅读时间的适应性下降。而词频是解释这种适应性下降的关键因素，较大模型变体过度准确地预测了人群中最不频繁的词汇，而较大模型的训练过程中更准确地学习了罕见的词汇，这解释了训练数据量和模型尺寸对适应阅读时间的不利影响。 |
| [^349] | [Teacher-Student Learning based Low Complexity Relay Selection in Wireless Powered Communications](https://arxiv.org/abs/2402.02254) | 本文研究了在非线性能量收集条件下多源多中继射频能量收集网络中的联合中继选择、调度和功率控制问题。提出了两种基于卷积神经网络的架构，分别采用传统的2D卷积块和Inception块，以提高网络性能。 |
| [^350] | [Don't Label Twice: Quantity Beats Quality when Comparing Binary Classifiers on a Budget](https://arxiv.org/abs/2402.02249) | 在比较两个二元分类器的准确性时，通过收集更多样本的单个标签而不是汇总多个噪声标签能更好地利用预算。 |
| [^351] | [ExTTNet: A Deep Learning Algorithm for Extracting Table Texts from Invoice Images](https://arxiv.org/abs/2402.02246) | ExTTNet是一种深度学习算法，能够自动从发票图像中提取表格文字。使用光学字符识别（OCR）技术获取文本，并通过特征提取方法提高准确度。通过多层神经网络模型进行训练，最终获得了0.92的F1分数。 |
| [^352] | [Revisiting Generative Adversarial Networks for Binary Semantic Segmentation on Imbalanced Datasets](https://arxiv.org/abs/2402.02245) | 这项工作提出了一种基于生成对抗网络的深度学习框架，用于像素级别的路面异常区域检测。通过两个训练阶段和多尺度特征表示，该框架增强了生成器从异构输入中估计概率特征图的能力，并通过引入几种注意机制，解决了在严重不平衡数据集上训练模型时性能恶化的问题。 |
| [^353] | [Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models](https://arxiv.org/abs/2402.02244) | 这篇论文综述了近期为扩展大型语言模型中上下文长度而设计的技术和方法，并回顾了包括架构修改在内的多种技术，使得语言模型可以更有效地理解长上下文。 |
| [^354] | [Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey](https://arxiv.org/abs/2402.02242) | 本综述调研了面向预训练视觉模型的参数高效微调方法，通过最小参数修改超越全面微调的性能，提供了全面的概述和未来方向，并提供了丰富的资源收藏。 |
| [^355] | [Distributional Reduction: Unifying Dimensionality Reduction and Clustering with Gromov-Wasserstein Projection](https://arxiv.org/abs/2402.02239) | 本文提出了一种新的分布约简方法，利用格罗莫夫-瓦瑟斯坦投影统一了降维和聚类，通过优化问题同时解决降维和聚类，实验证明了该方法在多个领域表现出卓越性能。 |
| [^356] | [Federated Learning with Differential Privacy](https://arxiv.org/abs/2402.02230) | 本论文研究了带有差分隐私的联邦学习对模型性能的影响，发现在分布式和差分隐私设置下，非独立同分布和小型数据集的性能下降最为严重。 |
| [^357] | [Vanilla Bayesian Optimization Performs Great in High Dimension](https://arxiv.org/abs/2402.02229) | 本文研究了高维情况下贝叶斯优化算法的问题，并提出了一种改进方法，通过对先验假设进行简单的缩放，使普通贝叶斯优化在高维任务中表现出色。 |
| [^358] | [Rethinking the Starting Point: Enhancing Performance and Fairness of Federated Learning via Collaborative Pre-Training](https://arxiv.org/abs/2402.02225) | 本文提出了一种名为CoPreFL的协作预训练方法，该方法通过设计一个可适应任何联邦学习任务的预训练模型来提高性能和公平性。大量实验证实了该方法在提供可靠的初始化方面的有效性。 |
| [^359] | [Graph Foundation Models](https://arxiv.org/abs/2402.02216) | 图基础模型是图领域中一个新兴的研究课题，旨在开发一个能够跨不同图和任务进行泛化的图模型。我们提出了一种新的 GFM 发展视角，通过倡导“图词汇表”来编码图的不变性，有助于推进未来的GFM设计。 |
| [^360] | [Query-decision Regression between Shortest Path and Minimum Steiner Tree](https://arxiv.org/abs/2402.02211) | 该论文探讨了最短路径和最小斯坦纳树之间的查询决策回归问题，提出了两种有原则的学习框架，并通过实验研究证明了问题可得到较好的解决。 |
| [^361] | [Implicit Neural Representation of Tileable Material Textures](https://arxiv.org/abs/2402.02208) | 本文研究了一种使用正弦神经网络表示周期性可平铺纹理的隐式神经表示方法。该方法通过使用傅立叶级数初始化神经网络的第一层，实现了在任意空间坐标上直接评估纹理，避免了插值。通过添加正则化项和泊松方程，确保生成可平铺的纹理，并能高效重建高分辨率纹理。应用于反锯齿表面能够提供高视觉保真度和锐度的效果。 |
| [^362] | [Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models](https://arxiv.org/abs/2402.02207) | 该论文研究了大规模语言模型在安全微调过程中存在的问题，并提出了一种可行的解决方案。他们首先策划了一个包含各种有害类别的视觉语言安全指令遵循数据集VLGuard，并通过实验证明将该数据集集成到视觉语言微调中或进行事后微调，可以有效地对齐VLLMs的安全性。这对模型的有益性几乎没有影响甚至有所提升。这项研究的贡献是提供了一个有价值的资源，用于对现有VLLMs进行安全测试、训练新模型或保护预训练VLLMs。 |
| [^363] | [Multimodal Co-orchestration for Exploring Structure-Property Relationships in Combinatorial Libraries via Multi-Task Bayesian Optimization](https://arxiv.org/abs/2402.02198) | 本研究提出并实现了一种多模态协同编排的方法，通过多任务贝叶斯优化来探索组合库中的结构-性质关系。该方法利用变分自动编码器的降维和表示学习来控制复杂可观测量的测量。 |
| [^364] | [Sample-Efficient Clustering and Conquer Procedures for Parallel Large-Scale Ranking and Selection](https://arxiv.org/abs/2402.02196) | 我们提出了一种新颖的并行大规模排序和选择问题的聚类及征服方法，通过利用相关信息进行聚类以提高样本效率，在大规模AI应用中表现优异。 |
| [^365] | [Continuous Tensor Relaxation for Finding Diverse Solutions in Combinatorial Optimization Problems](https://arxiv.org/abs/2402.02190) | 本研究提出了连续张量放松方法(CTRA)，用于在组合优化问题中寻找多样化的解决方案。CTRA通过对离散决策变量进行连续放松，解决了寻找多样化解决方案的挑战。 |
| [^366] | [Evolution Guided Generative Flow Networks](https://arxiv.org/abs/2402.02186) | 本文提出了进化引导的生成流网络（EGFN），用于处理长时间跨度和稀疏奖励的挑战。通过使用进化算法训练一组代理参数，并将结果轨迹存储在优先回放缓冲区中，我们的方法在训练GFlowNets代理时展现出了很高的效果。 |
| [^367] | [Diffusion Cross-domain Recommendation](https://arxiv.org/abs/2402.02182) | 该论文研究了跨域推荐中的扩散模型，通过从辅助领域中添加数据来解决冷启动用户数据稀疏问题。其中，映射模块起着关键的作用，确定了跨域推荐模型的性能。 |
| [^368] | [Off-Policy Evaluation of Slate Bandit Policies via Optimizing Abstraction](https://arxiv.org/abs/2402.02171) | 我们提出了一种名为潜在IPS（LIPS）的新的Slate Bandit OPE估计器，通过在低维度的Slate抽象空间中定义重要性权重，并通过数据驱动的方式优化Slate抽象来减小偏差和方差。 |
| [^369] | [One Graph Model for Cross-domain Dynamic Link Prediction](https://arxiv.org/abs/2402.02168) | DyExpert是一种用于跨域链接预测的动态图模型，通过明确建模历史演化过程并结合链接预测，它可以学习特定下游图的演化模式，并在各个领域上取得了最先进的性能。 |
| [^370] | [Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting Generative AI-based Visualizations](https://arxiv.org/abs/2402.02167) | 这篇论文介绍了一个名为EvaLLM的概念模型栈，用于评估和解释基于生成AI的可视化。它解决了使用大型语言模型生成可视化时遇到的问题，并提出了一种理论评估的方法。 |
| [^371] | [Towards Optimal Adversarial Robust Q-learning with Bellman Infinity-error](https://arxiv.org/abs/2402.02165) | 本文研究了对抗鲁棒Q学习的最优化，并证明了一种确定性且稳态的最优鲁棒策略存在，该策略与贝尔曼最优策略一致。同时，阐明了在最小化贝尔曼误差以获得最优鲁棒策略时使用$L^{\infty}$-范数的必要性。 |
| [^372] | [A Bayesian cluster validity index](https://arxiv.org/abs/2402.02162) | 该论文提出了一个基于贝叶斯方法的聚类有效性指数，该指数根据现有的基础指数定义，并用于检测次优聚类数，通过与其他指数进行比较，验证了其有效性。 |
| [^373] | [Evaluating the Robustness of Off-Road Autonomous Driving Segmentation against Adversarial Attacks: A Dataset-Centric analysis](https://arxiv.org/abs/2402.02154) | 本研究评估了越野自动驾驶分割模型对抗攻击的鲁棒性，并提出了一个稳健数据集来研究非稳健特征对对抗攻击的影响。 |
| [^374] | [Position Paper: Why the Shooting in the Dark Method Dominates Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking](https://arxiv.org/abs/2402.02152) | 这篇论文质疑了推荐系统实践中目前常用的“摸着石头过河”方法，呼吁摒弃反乌托邦思维。论文提出了使用深度学习堆栈的非标准用法，以解锁奖励优化的推荐系统的潜力。 |
| [^375] | [Improving Diffusion Models for Inverse Problems Using Optimal Posterior Covariance](https://arxiv.org/abs/2402.02149) | 本文提出了一种改进无需重训练的传播模型的方法，通过优化后验协方差，提供了一种零样本解决方案，用于嘈杂的线性逆问题。根据最近的方法等价于对给定扩散噪声图像的干净图像的不可计算后验分布进行各向同性高斯近似的发现，我们提出了一种通用即插即用的后验协方差优化方法。为了实现无需重新训练的最优后验协方差，我们提供了基于两种方法的通用解决方案，这两种方法专门设计用于利用具有和不具有反向协方差的预训练模型。 |
| [^376] | [Using Deep Ensemble Forest for High Resolution Mapping of PM2.5 from MODIS MAIAC AOD in Tehran, Iran](https://arxiv.org/abs/2402.02139) | 本文评估了深度集成森林方法在从AOD数据估计PM2.5浓度方面的潜力，并发现其相比深度学习方法和随机森林等数据驱动方法具有更高的准确度。 |
| [^377] | [Grammar-based evolutionary approach for automated workflow composition with domain-specific operators and ensemble diversity](https://arxiv.org/abs/2402.02124) | 本文介绍了一种名为EvoFlow的基于语法的进化方法，用于自动化工作流组合。EvoFlow通过增强工作流结构的灵活性，并引入多样性集成策略，使从业者能够选择最适合其特定需求的算法。实验证明，EvoFlow在多个AutoML基准测试数据集上取得显著的性能提升，并在不同领域的实际应用中表现出优势。 |
| [^378] | [Enhancing crop classification accuracy by synthetic SAR-Optical data generation using deep learning](https://arxiv.org/abs/2402.02121) | 通过使用深度学习生成合成的SAR-光学数据，可以增强农作物分类的准确性。然而，传统数据生成方法面临训练数据不平衡的挑战。 |
| [^379] | [Handling Delayed Feedback in Distributed Online Optimization : A Projection-Free Approach](https://arxiv.org/abs/2402.02114) | 本研究提出了两种投影无关算法，用于处理分布式在线优化中的延迟反馈问题。这些算法在中心化和分布式设置中都能达到O(\sqrt{B})的遗憾界，对于延迟设置中的OCO问题是最优的。 |
| [^380] | [Accelerating Look-ahead in Bayesian Optimization: Multilevel Monte Carlo is All you Need](https://arxiv.org/abs/2402.02111) | 本文利用多层蒙特卡洛方法加速贝叶斯优化中的前瞻过程，并证明在涉及嵌套期望和最大化的问题中具有优势。 |
| [^381] | [Composite Active Learning: Towards Multi-Domain Active Learning with Theoretical Guarantees](https://arxiv.org/abs/2402.02110) | 复合主动学习为多领域主动学习提供了具备理论保证的方法，并且考虑了不同领域之间的相似性和数据分布变化。 |
| [^382] | [Learning Structure-Aware Representations of Dependent Types](https://arxiv.org/abs/2402.02104) | 本文扩展了Agda生态系统到机器学习领域，并发布了一种新颖的依赖类型编程语言的证明数据集。通过提出一种基于结构而非命名原则的新颖神经架构，我们能够准确地表示依赖类型程序，并在前提选择任务中取得了强大的初步结果。 |
| [^383] | [D\'ej\`a Vu Memorization in Vision-Language Models](https://arxiv.org/abs/2402.02103) | 这项研究提出了一种新方法来衡量视觉语言模型中的记忆现象，并发现对于使用图像-标题对进行训练的VLMs，模型确实会保留关于训练图像中的个别对象的信息，文本随机化可以在很大程度上减轻记忆现象而对模型的下游任务性能影响较小。 |
| [^384] | [Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models](https://arxiv.org/abs/2402.02099) | 分析了多语言语言模型中跨语言知识转移的评估方法和设置，发现高性能主要归因于非语言知识的因素，如任务和表层知识，并且跨语言传输的主要是数据工件和偏见，尤其是对于低资源语言。 |
| [^385] | [Self-attention Networks Localize When QK-eigenspectrum Concentrates](https://arxiv.org/abs/2402.02098) | 本文研究了自注意网络中的注意力定位问题，通过QK特征值谱的集中定位现象来解决不同观点之间的矛盾。 |
| [^386] | [Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing](https://arxiv.org/abs/2402.02097) | 提出了一种名为MACE的简单而有效的多智能体协同探索方法，通过共享局部新颖性来近似全局新颖性，并引入加权互信息来衡量智能体行动对其他智能体的影响，从而促进多智能体之间的协同探索。实验证明，MACE在稀疏奖励的多智能体环境中取得了出色的性能。 |
| [^387] | [Seeing is not always believing: The Space of Harmless Perturbations](https://arxiv.org/abs/2402.02095) | 在深度神经网络中，我们发现了一种无害扰动空间的存在，这种扰动不会影响网络对原始图像的输出。我们证明了在输入维度超过输出维度的情况下，存在一个连续的无害扰动子空间。我们还解决了一族通用扰动，这些扰动一致地影响网络输出。我们的工作揭示了深度神经网络与人类感知之间的差异，即深度神经网络对人类认为重要的扰动可能不会影响其识别能力。 |
| [^388] | [Risk-Sensitive Diffusion: Learning the Underlying Distribution from Noisy Samples](https://arxiv.org/abs/2402.02081) | 该论文提出了一种风险敏感的扩散模型，通过调整噪声样本的分布，减少误导并从中获取信息，有效地应对噪声样本的存在，并在合成和真实世界数据集上取得了良好的效果。 |
| [^389] | [Training Implicit Networks for Image Deblurring using Jacobian-Free Backpropagation](https://arxiv.org/abs/2402.02065) | 本文介绍了使用无雅各布逆向传播方法来训练隐式网络进行图像去模糊的研究。实验结果表明，该方法在降低计算成本的同时，具有与最先进的前馈网络和现有的隐式网络相媲美的效果。 |
| [^390] | [Break the Sequential Dependency of LLM Inference Using Lookahead Decoding](https://arxiv.org/abs/2402.02057) | 本文介绍了一种称为前瞻解码的精确、并行解码算法，通过交换每步操作数以减少总解码步骤的数量，加速了大型语言模型（LLM）的解码过程。它不需要辅助模型或数据存储，并且与并发内存高效的注意力机制兼容。实验证明，在代码补全任务中，前瞻解码可将自回归解码加速1.8倍，并且在多个GPU上实现强扩展性。 |
| [^391] | [Variance Alignment Score: A Simple But Tough-to-Beat Data Selection Method for Multimodal Contrastive Learning](https://arxiv.org/abs/2402.02055) | 提出了一种简单但有理论原则支持的度量方法——方差对齐分数(Variance Alignment Score，VAS)，用于解决数据选择问题。该方法通过最大化总的VAS来选择最具信息量的样本。 |
| [^392] | [Neural Scaling Laws on Graphs](https://arxiv.org/abs/2402.02054) | 本论文在图上深入研究了神经缩放定律，从模型和数据两个角度进行了探索。对于模型缩放，发现了缩放定律崩溃和过拟合之间的关系，以及深度图模型的模型深度对缩放行为的影响。对于数据缩放，提出了图数量不适合作为衡量缩放定律中图数据量的指标。 |
| [^393] | [Feature Selection using the concept of Peafowl Mating in IDS](https://arxiv.org/abs/2402.02052) | 本研究在入侵检测系统中提出了一种结合孔雀交配概念的特征选择算法，以实现高效的云环境安全保证。 |
| [^394] | [Nonlinear subspace clustering by functional link neural networks](https://arxiv.org/abs/2402.02051) | 本研究提出了一种基于功能链接神经网络的非线性子空间聚类方法，通过将数据样本转化为非线性域并利用学习机制构建自表示矩阵，同时引入局部相似性正则化以增强分组效应，改善聚类结果质量，并且在保证高计算效率的前提下获得良好的聚类准确性。 |
| [^395] | [Quality and Trust in LLM-generated Code](https://arxiv.org/abs/2402.02047) | 本论文研究了机器学习生成代码的质量和信任问题，提出了校准的重要性，并探讨了如何确定模型生成代码的正确性。 |
| [^396] | [Locally-Adaptive Quantization for Streaming Vector Search](https://arxiv.org/abs/2402.02044) | 本文研究了局部自适应量化在流式相似度搜索中的应用，引入了Turbo LVQ和multi-means LVQ两种改进方法，分别提升了搜索性能28%和27%。实验证明，LVQ及其新变体使得向量搜索变得极快。 |
| [^397] | [A Plug-in Tiny AI Module for Intelligent and Selective Sensor Data Transmission](https://arxiv.org/abs/2402.02043) | 提出了一种插件式微型AI模块，可以将智能和选择性传感器数据传输集成到物联网应用中。通过放置高效的机器学习模型靠近传感器，实现对数据传输的智能控制，筛选有价值的数据，并通过调节传输频率丢弃无关信息。通过量化和优化近传感器模型，实现实时传感器控制。定制训练过程和利用时间信息的“懒惰”传感器停用策略提升框架性能。该方法独立于其他物联网框架，可与之共存。 |
| [^398] | [Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm](https://arxiv.org/abs/2402.02042) | 该论文研究了无限时域平均回报受限MDPs的参数化通用策略，并提出了一种基于原始-对偶策略梯度算法，可在保证低遗憾的情况下管理约束条件，达到全局最优策略。算法的分析表明，其目标遗憾和约束违反均为 $\tilde{\mathcal{O}}({T}^{3/4})$。 |
| [^399] | [$\alpha$-Divergence Loss Function for Neural Density Ratio Estimation](https://arxiv.org/abs/2402.02041) | 本文提出了一种应用于神经密度比估计的$\alpha$-散度损失函数($\alpha$-Div)，通过简洁实现和稳定优化解决了现有方法中存在的优化问题。实验证明了这种损失函数的稳定性，并提出了对DRE任务的估计准确性的研究，同时给出了样本要求的解决方案。 |
| [^400] | [Interpreting Graph Neural Networks with In-Distributed Proxies](https://arxiv.org/abs/2402.02036) | 该论文提出了一种翻译图神经网络中可解释子图的代理图的新方法，解决了训练数据分布与可解释子图集之间的分布偏移问题。 |
| [^401] | [Universal Post-Training Reverse-Engineering Defense Against Backdoors in Deep Neural Networks](https://arxiv.org/abs/2402.02034) | 本文提出了一种针对深度神经网络中后门攻击的通用后训练反向工程防御方法，通过依赖内部特征图来检测和反向工程后门，并识别其目标类别，具有广泛适用性和低计算开销。 |
| [^402] | [RobustTSF: Towards Theory and Design of Robust Time Series Forecasting with Anomalies](https://arxiv.org/abs/2402.02032) | 本文研究了具有异常值的时间序列预测的理论与设计，提出了一种简单高效的算法来学习鲁棒的预测模型，并通过广泛的实验验证了方法的高鲁棒性和优越性能。 |
| [^403] | [Multi-fidelity physics constrained neural networks for dynamical systems](https://arxiv.org/abs/2402.02031) | 本文提出了一种多级逼真度的物理约束神经网络，通过定制的多逼真度自编码器将具有不同逼真度水平的数据融合到一个统一的潜在空间中。 |
| [^404] | [ScribFormer: Transformer Makes CNN Work Better for Scribble-based Medical Image Segmentation](https://arxiv.org/abs/2402.02029) | ScribFormer是一种新的CNN-Transformer混合解决方案，用于涂鸦监督的医学图像分割。它通过融合CNN学习的局部特征和Transformer获得的全局表示，有效克服了现有方法在涂鸦注释中学习全局形状信息的局限性。 |
| [^405] | [Unlearnable Examples For Time Series](https://arxiv.org/abs/2402.02028) | 本研究提出了一种新的UE生成方法，用于保护时间序列数据免受深度学习模型的未授权训练。通过选择性应用最小化误差噪声，我们使特定时间序列段对DNN模型不可学习，同时对人类观察者不可察觉。 |
| [^406] | [A Survey of Constraint Formulations in Safe Reinforcement Learning](https://arxiv.org/abs/2402.02025) | 本文综述了安全强化学习中的约束形式，包括对每种形式特别设计的算法。同时，揭示了常见问题形式之间的数学相互关系。 |
| [^407] | [Self-Supervised Contrastive Forecasting](https://arxiv.org/abs/2402.02023) | 该论文介绍了一种通过采用对比学习和增强的分解架构，并结合全局自相关性的自监督方法来解决长期预测中的挑战。实验证明，该方法在九个长期基准上的多个实验中胜过了14个基线模型。 |
| [^408] | [Transfer Learning in ECG Diagnosis: Is It Effective?](https://arxiv.org/abs/2402.02021) | 本研究首次对心电图诊断中的迁移学习进行了广泛的经验研究，发现微调对于小型数据集是较好的选择，当数据集足够大时，从头开始训练可以达到可比性能，但需要更长的训练时间。同时，迁移学习与卷积神经网络具有更好的兼容性。 |
| [^409] | [Position Paper: The Landscape and Challenges of HPC Research and LLMs](https://arxiv.org/abs/2402.02018) | 运用语言模型技术于高性能计算任务中具有巨大潜力 |
| [^410] | [Value-Aided Conditional Supervised Learning for Offline RL](https://arxiv.org/abs/2402.02017) | 该论文提出了一种称为价值增强的条件监督学习方法，通过将RCSL的稳定性与基于价值的方法的连接能力相结合，动态地根据轨迹回报将价值帮助注入损失函数中。实验证明，该方法不仅优于现有方法，而且在各种离线强化学习任务中实现了最高的轨迹回报，推动了离线强化学习的发展。 |
| [^411] | [GenFormer: A Deep-Learning-Based Approach for Generating Multivariate Stochastic Processes](https://arxiv.org/abs/2402.02010) | GenFormer是一种基于深度学习的方法，用于生成多元随机过程。它能保留目标统计特性，包括边际分布，并能在具有挑战性的应用中近似捕捉到其他期望的统计特性。应用于风速数据模拟的实验中，GenFormer模型用于计算风险管理的超越概率。 |
| [^412] | [Robust Multi-Task Learning with Excess Risks](https://arxiv.org/abs/2402.02009) | 提出了一种具有过多风险的多任务学习（ExcessMTL）方法，根据任务到收敛的距离来更新任务权重，以克服存在标签噪声时现有方法的限制。 |
| [^413] | [Understanding Time Series Anomaly State Detection through One-Class Classification](https://arxiv.org/abs/2402.02007) | 本文重新定义了时间序列异常检测问题为时间序列异常状态检测问题，并使用单类分类方法进行解决。 |
| [^414] | [PresAIse, An Enterprises Prescriptive AI Solution](https://arxiv.org/abs/2402.02006) | PresAIse是一种企业预测性人工智能解决方案，通过提供因果推断方法、可解释的决策制定方法和大型语言模型（LLMs）的集成，旨在解决企业预测性人工智能面临的数据限制、建议可解释性和数据科学家与业务用户之间的合作障碍等挑战。 |
| [^415] | [Topology-Informed Graph Transformer](https://arxiv.org/abs/2402.02005) | TIGT是一种基于拓扑信息的新型图形变换器，通过增强区分图同构性的能力和提高图形变换器性能，实现了对图同构性的检测和整体性能的增强。 |
| [^416] | [A Survey on Graph Condensation](https://arxiv.org/abs/2402.02000) | 本综述调查了图结构压缩（GC）的最新研究，提出了GC的形式化定义，并将现有方法按照目标和公式分类。同时，综述还包括了对数据集和评估指标的全面分析。 |
| [^417] | [A Novel Hyperdimensional Computing Framework for Online Time Series Forecasting on the Edge](https://arxiv.org/abs/2402.01999) | 本文提出了一种新颖的超维度计算框架，用于解决在线时间序列预测问题。通过将非线性低维数据映射到高维空间进行线性超维度预测，实现了快速、高效和轻量级的预测，同时适应了时间序列数据的变化。 |
| [^418] | [Online Uniform Risk Times Sampling: First Approximation Algorithms, Learning Augmentation with Full Confidence Interval Integration](https://arxiv.org/abs/2402.01995) | 本文首次引入在线均匀风险时间抽样问题，并提出了两种在线近似算法，一种带有学习增强，一种没有学习增强。通过竞争比分析，我们提供了严格的理论性能保证。我们通过合成实验和实际案例研究评估了算法的性能。 |
| [^419] | [Online Transfer Learning for RSV Case Detection](https://arxiv.org/abs/2402.01987) | 这项研究介绍了一种名为PVAW的在线多源转移学习方法，通过动态加权机制实现了对序列流行病学数据的自适应调整，并在分析RSV数据的应用中取得了显著的模型性能改进。 |
| [^420] | [Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes](https://arxiv.org/abs/2402.01981) | 本研究利用大型语言模型的零-shot能力提出了一种自我去偏差的技术，通过解释和重启两种方法，成功减少了九个不同社会群体的刻板印象程度，该技术不需要对训练数据、模型参数或解码策略进行修改，希望能启发其他零-shot偏见缓解技术的研究。 |
| [^421] | [Structure-Aware E(3)-Invariant Molecular Conformer Aggregation Networks](https://arxiv.org/abs/2402.01975) | 本论文提出了一种结构感知的E(3) 不变性分子构型聚合网络，将分子的2D表示与其多个构型的表示整合在一起，通过使用一个新型的2D-3D聚合机制，融合Gromov-Wasserstein变量问题，并使用高效的在线构型生成方法。 |
| [^422] | [Combining T-learning and DR-learning: a framework for oracle-efficient estimation of causal contrasts](https://arxiv.org/abs/2402.01972) | 这篇论文介绍了高效插件学习的框架，能够有效估计异质因果对比，并解决了其他学习策略的一些缺点。该框架构建了人口风险函数的高效插件估计器，具有稳定性和鲁棒性。 |
| [^423] | [Simulation-Enhanced Data Augmentation for Machine Learning Pathloss Prediction](https://arxiv.org/abs/2402.01969) | 本文提出了一种模拟增强数据增强方法，用于改善机器学习路径损耗预测中数据有限的挑战。该方法通过结合来自蜂窝覆盖模拟器生成的合成数据和独立收集的真实世界数据集，提供了关键的真实值用于模型训练。通过使用特征工程和高效稳健的梯度提升机器学习算法CatBoost，该方法显著提高了路径损耗预测的性能。 |
| [^424] | [A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions](https://arxiv.org/abs/2402.01968) | 这篇论文调查了上下文感知多代理系统的技术、挑战和未来发展方向，并提供了一个综合的概述。它介绍了上下文感知系统和多代理系统的特性，以及集成这些系统的通用过程。 |
| [^425] | [Analyzing Neural Network-Based Generative Diffusion Models through Convex Optimization](https://arxiv.org/abs/2402.01965) | 本研究通过凸优化方法分析了基于神经网络的生成扩散模型，揭示了这些模型在非渐近设置下的精确预测分数函数和收敛结果。 |
| [^426] | [No Need to Look Back: An Efficient and Scalable Approach for Temporal Network Representation Learning](https://arxiv.org/abs/2402.01964) | 本论文提出了一种高效可扩展的时态网络表示学习方法，该方法通过前向最近采样策略和GPU可执行的大小受限哈希表实现了对查询的快速响应和最小化推理延迟。 |
| [^427] | [Improving Large-Scale k-Nearest Neighbor Text Categorization with Label Autoencoders](https://arxiv.org/abs/2402.01963) | 本文提出了一种使用标签自编码器改进大规模k最近邻文本分类的方法，通过将大的标签空间映射到一个缩小的潜在空间，并从该潜在空间中重建出预测的标签，以解决在大型文档集合中处理自动语义索引的问题。 |
| [^428] | [Calibrated Uncertainty Quantification for Operator Learning via Conformal Prediction](https://arxiv.org/abs/2402.01960) | 通过符合预测方法，提出了一种校准不确定性量化的分位数神经运算器，能够在函数定义域上同时量化不确定性，无需分布假设，实验结果表明其在2D Darcy流动和3D车辆表面压力预测任务上优于基线方法。 |
| [^429] | [OPSurv: Orthogonal Polynomials Quadrature Algorithm for Survival Analysis](https://arxiv.org/abs/2402.01955) | 本文介绍了一种用于生存分析的新方法OPSurv，通过正交多项式积分算法提供连续时间功能性输出。该方法利用累积发生函数的初始零条件和正交多项式的分解，实现了对每个风险事件的功能近似系数的学习，并通过高斯-勒让德积分法构建累积发生函数的估计值，有效地解决了竞争风险场景中的过度拟合问题。 |
| [^430] | [Precedence-Constrained Winter Value for Effective Graph Data Valuation](https://arxiv.org/abs/2402.01943) | 提出了一种名为PC-Winter的优先约束冬季价值方法，用于有效地评估复杂图形数据的价值。通过解决复杂的图形结构以及计算挑战，PC-Winter方法在各种数据集和任务中展示了其有效性。 |
| [^431] | [Digits micro-model for accurate and secure transactions](https://arxiv.org/abs/2402.01931) | 本研究展示了数字微模型在准确和安全交易中的潜力，这些轻量级模型在数字识别特定任务上表现良好，并使用较少的训练时间和内存资源。 |
| [^432] | [Reducing Optimism Bias in Incomplete Cooperative Games](https://arxiv.org/abs/2402.01930) | 本文提出了一个框架，旨在通过优化揭示联盟价值的顺序来减少不完全合作博弈中的乐观偏误。 |
| [^433] | [Sample, estimate, aggregate: A recipe for causal discovery foundation models](https://arxiv.org/abs/2402.01929) | 本文提出一种因果发现框架，通过深度学习模型预训练与经典发现算法的结合，实现了快速、准确地推断因果结构，并在实验中展示了与现有方法相比更好的表现和推理速度。 |
| [^434] | [Robust Counterfactual Explanations in Machine Learning: A Survey](https://arxiv.org/abs/2402.01928) | 这项调查回顾了机器学习中强健反事实解释的研究，并分析了其对强健性的考虑。该调查讨论了现有解决方案及其局限性，为未来的发展提供了基础。 |
| [^435] | [A General Framework for Learning from Weak Supervision](https://arxiv.org/abs/2402.01922) | 本文介绍了一个通用框架，利用新算法从各种弱监督中学习，通过使用非确定性有限自动机和前向-后向算法来简化计算要求，并将时间复杂度降低到线性尺度。 |
| [^436] | [Preference Poisoning Attacks on Reward Model Learning](https://arxiv.org/abs/2402.01920) | 对于从偏好比较中学习奖励模型的方法存在偏好污染攻击的漏洞，攻击者可以通过翻转少量偏好比较来对目标结果进行操纵。我们提出了两类算法方法，并证明了这些攻击在实施恶意行为方面的有效性。 |
| [^437] | [CoLe and LYS at BioASQ MESINESP8 Task: similarity based descriptor assignment in Spanish](https://arxiv.org/abs/2402.01916) | 这篇论文介绍了基于相似度的描述符分配方法在BioASQ MESINESP8任务中的应用，通过使用传统的信息检索工具，并提出了适用于西班牙语等语言的方法。 |
| [^438] | [From PEFT to DEFT: Parameter Efficient Finetuning for Reducing Activation Density in Transformers](https://arxiv.org/abs/2402.01911) | 本论文提出了一种用于减少变压器模型中激活密度的参数高效微调方法 DEFT。研究发现预训练模型中存在激活稀疏性，并通过引入新的密度损失来促进更高的激活稀疏性。通过应用主流的PEFT技术，包括QLoRA、LoRA、Adapter、Prompt/Prefix Tuning，实验证明了该方法在不同下游任务中的有效性。 |
| [^439] | [On Catastrophic Inheritance of Large Foundation Models](https://arxiv.org/abs/2402.01909) | 这篇论文讨论了大型基础模型（LFMs）中的灾难性继承问题，指出了从有偏见的大规模预训练数据到LFMs在下游任务中的行为的弱点和限制。我们提出了UIM框架，旨在理解LFMs的灾难性继承问题，并解释其中的含义。 |
| [^440] | [EBV: Electronic Bee-Veterinarian for Principled Mining and Forecasting of Honeybee Time Series](https://arxiv.org/abs/2402.01902) | EBV是一种基于原理、有效、可解释和可扩展的方法，用于挖掘和预测蜜蜂的温度时间序列数据。 |
| [^441] | [Distributional Off-policy Evaluation with Bellman Residual Minimization](https://arxiv.org/abs/2402.01900) | 这篇论文研究了使用Bellman残差最小化的方法来解决分布式离线策略评估问题，并提出了一种称为能量Bellman残差最小化（EBRM）的方法来估计返回分布。在可实现性假设下，建立了EBRM估计器的有限样本误差界。 |
| [^442] | [The Role of Foundation Models in Neuro-Symbolic Learning and Reasoning](https://arxiv.org/abs/2402.01889) | 基于基础模型的神经符号学习和推理在NeSy任务中表现出更好的性能，并且减少了标注和手动工程的工作量。 |
| [^443] | [On f-Divergence Principled Domain Adaptation: An Improved Framework](https://arxiv.org/abs/2402.01887) | 本文改进了基于f-散度的无监督领域自适应（UDA）框架，引入了f-领域差异度量指标，并通过去除绝对值函数和引入缩放参数，提出了新的目标误差和样本复杂度界限，从而使得我们能够恢复以前的KL结果，将算法和理论之间的差距缩小，并通过定位技术开发了快速率的泛化界限。实验结果证明了基于f-DD的领域学习算法在流行的UDA基准测试中表现出了卓越的性能。 |
| [^444] | [Inverse Reinforcement Learning by Estimating Expertise of Demonstrators](https://arxiv.org/abs/2402.01886) | 本文介绍了一个新颖的框架，IRLEED，它通过估计演示者的专业知识来解决模仿学习中的次优和异质演示的问题。IRLEED通过结合演示者次优性的普适模型和最大熵IRL框架，有效地从多样的次优演示中得出最佳策略。 |
| [^445] | [Large Language Model Agent for Hyper-Parameter Optimization](https://arxiv.org/abs/2402.01881) | 基于大规模语言模型的AgentHPO技术通过自动化超参数优化，在机器学习任务中大大减少了试验次数，简化了设置过程，提升了解释性和用户信任。 |
| [^446] | [$\sigma$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial Examples](https://arxiv.org/abs/2402.01879) | 该论文提出了一种新的基于梯度的$\ell_0$范数攻击方法$\sigma$-zero，其利用了$\ell_0$范数的可微近似和自适应投影运算符，能够在非凸和非可微的约束下优化，从而评估深度网络对稀疏$\ell_0$范数攻击的鲁棒性。 |
| [^447] | [LiPO: Listwise Preference Optimization through Learning-to-Rank](https://arxiv.org/abs/2402.01878) | 本研究提出了一种名为LiPO的框架，用于将语言模型对齐问题定义为一个列表型排序问题。通过从排名列表中学习，该框架可以使策略更有效地学习到可行的响应。 |
| [^448] | [Mobile Fitting Room: On-device Virtual Try-on via Diffusion Models](https://arxiv.org/abs/2402.01877) | 移动试衣间是第一个基于设备内扩散模型的虚拟试穿系统，解决了高质量服装放置和模型压缩等技术挑战，实现了隐私保护和用户定制。它为时尚电子商务企业提供了有价值的服务和用户友好的虚拟试穿体验。 |
| [^449] | [Sets are all you need: Ultrafast jet classification on FPGAs for HL-LHC](https://arxiv.org/abs/2402.01876) | 该论文研究了在FPGA上进行高性能喷注分类的机器学习算法，通过量化感知的训练和高效的硬件实现，实现了低延迟和低资源消耗，为高亮度阶段标记的模型提供了初始设计。 |
| [^450] | [The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models](https://arxiv.org/abs/2402.01874) | 这项工作回顾了将强化学习和大语言模型结合起来的研究，并提出了一个新的分类方法，以审视这两个领域之间的协同关系。 |
| [^451] | [APIServe: Efficient API Support for Large-Language Model Inferencing](https://arxiv.org/abs/2402.01869) | APIServe是针对API增强的大型语言模型推理的一个高效工具，它最大限度地减少了由 API 调用引起的 GPU 资源浪费，并提高了整体服务吞吐量。 |
| [^452] | [Challenges in Training PINNs: A Loss Landscape Perspective](https://arxiv.org/abs/2402.01868) | 本文探讨了训练PINNs的挑战，强调了损失函数空间在训练过程中的作用，引入了新颖的二阶优化器NNCG并优化了PINN性能，为训练PINNs提供了有价值的洞见和更强大的优化策略。 |
| [^453] | [Leveraging Large Language Models for Structure Learning in Prompted Weak Supervision](https://arxiv.org/abs/2402.01867) | 这项工作利用大型语言模型在提示式弱监督中学习标注函数之间的统计依赖结构，通过结构细化模块提高了提示式弱监督流程的效果。 |
| [^454] | [What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement](https://arxiv.org/abs/2402.01865) | 本文研究了语言模型更新中的遗忘现象，提出了一种预测上游实例遗忘的方法，以改进重播过程的可控性和解释性。根据预训练实例的预-softmax对数几率分数变化与在线学习实例的相似性，提出了一种部分可解释的预测模型，在BART模型上表现良好但在T5模型上失败。此外，还展示了基于内积的黑盒分类器。 |
| [^455] | [DFML: Decentralized Federated Mutual Learning](https://arxiv.org/abs/2402.01863) | DFML是一个无服务器的分散式联邦互联学习框架，能够有效地处理模型和数据的异质性，并通过相互学习在客户端之间传授知识，以获得更快的收敛速度和更高的全局准确性。 |
| [^456] | [Parametric Feature Transfer: One-shot Federated Learning with Foundation Models](https://arxiv.org/abs/2402.01862) | FedPFT 使用参数特征迁移提高了一次性联邦学习的准确性和通信效率，并在不同数据异质性设置中显示出改进效果。 |
| [^457] | [Explaining latent representations of generative models with large multimodal models](https://arxiv.org/abs/2402.01858) | 该研究提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。对于不同生成模型的解释，我们量化了解释的不确定性，通过多模态模型进行了性能评估，且定性地展示了潜在因素的变化效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。 |
| [^458] | [Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models](https://arxiv.org/abs/2402.01857) | 本文评估了基于Foundation模型集成联邦学习中鲁棒性、隐私和公平性的挑战和问题，并提出了应对策略和研究方向。 |
| [^459] | [SPDE priors for uncertainty quantification of end-to-end neural data assimilation schemes](https://arxiv.org/abs/2402.01855) | SPDE先验在最优插值中的应用及其与神经网络的联合学习问题，为大规模地球物理数据集的时空插值提供了一种新的方法。 |
| [^460] | [Capturing waste collection planning expert knowledge in a fitness function through preference learning](https://arxiv.org/abs/2402.01849) | 本文提出了一种通过偏好学习将废物收集规划专家知识融入适应性函数的方法，通过制定关键绩效指标和偏好判断，实现了全局优化的废物收集流程。 |
| [^461] | [Multi-Armed Bandits with Interference](https://arxiv.org/abs/2402.01845) | 这篇论文研究了在在线平台中与干扰进行的实验。在多臂赌博机问题中，学习者分配不同的臂给每个实验单元，根据单元之间的空间距离和对手选择的匹配函数来决定每个单元在每轮的回报。研究发现，转换政策能够实现最佳的预期遗憾，但任何转换政策都会遭受一定的遗憾现象。 |
| [^462] | [SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?](https://arxiv.org/abs/2402.01832) | SynthCLIP是一种新的框架，用于训练完全合成的CLIP模型，通过生成大规模的合成图片和标题数据集，在性能上可以与在真实数据上训练的CLIP模型相媲美。 |
| [^463] | [Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities](https://arxiv.org/abs/2402.01831) | Audio Flamingo是一种新型音频语言模型，具备强大的音频理解能力、通过上下文学习和检索快速适应未见过的任务的能力以及强大的多轮对话能力，并且通过广泛的评估达到了最优成绩。 |
| [^464] | [Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment](https://arxiv.org/abs/2402.01830) | 本文提出了一种新的无监督评估方法，利用同行评审机制在开放环境中衡量LLMs。通过为每个LLM分配可学习的能力参数，以最大化各个LLM的能力和得分的一致性。结果表明，高层次的LLM能够更准确地评估其他模型的答案，并能够获得更高的响应得分。 |
| [^465] | [Predicting ATP binding sites in protein sequences using Deep Learning and Natural Language Processing](https://arxiv.org/abs/2402.01829) | 本研究使用深度学习和自然语言处理方法，开发了一种预测蛋白质序列中ATP结合位点的方法，实验证明了与最先进的基准模型相比的改进效果。 |
| [^466] | [Identification of Cognitive Decline from Spoken Language through Feature Selection and the Bag of Acoustic Words Model](https://arxiv.org/abs/2402.01824) | 通过特征选择和声学词袋模型，该研究旨在开发一种自动机器学习技术，通过分析自然口语语言的非词汇声学属性，实现快速准确诊断记忆障碍症状，从而提前识别认知能力下降的早期迹象。 |
| [^467] | [Ecologically rational meta-learned inference explains human category learning](https://arxiv.org/abs/2402.01821) | 本研究提出了一种叫做生态合理的元学习推断（ERMI）的模型，通过使用大型语言模型生成与现实世界任务统计一致的认知任务，并通过元学习框架推导适应这些任务的理性主体。实验证明，ERMI模型在定性和定量上都更好地解释了人类的数据。 |
| [^468] | [LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks](https://arxiv.org/abs/2402.01817) | LLMs无法独自进行规划或自我验证，但在规划/推理任务中可以作为通用近似知识源发挥更大作用。 |
| [^469] | [Distilling LLMs' Decomposition Abilities into Compact Language Models](https://arxiv.org/abs/2402.01812) | 本研究将LLMs的分解能力通过离线强化学习融入到紧凑模型中，通过开发AI生成的数据集和建立基准，突出了紧凑模型在复制复杂问题解决能力方面的潜力。 |
| [^470] | [A Distributionally Robust Optimisation Approach to Fair Credit Scoring](https://arxiv.org/abs/2402.01811) | 本文研究了如何在信用评分中应用分布鲁棒优化方法，并对现有技术的鲁棒性效果进行了实证评估。 |
| [^471] | [Misspecification uncertainties in near-deterministic regression](https://arxiv.org/abs/2402.01810) | 该论文研究了近确定性回归中错误规范化的不确定性问题，并提出了一种组合模型，以准确预测和控制参数不确定性。 |
| [^472] | [PhenoLinker: Phenotype-Gene Link Prediction and Explanation using Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2402.01809) | PhenoLinker通过使用异构图神经网络进行表型-基因关联预测和解释，为发现新的关联和理解人类遗传变异的后果提供了一种新的方法。 |
| [^473] | [An Auction-based Marketplace for Model Trading in Federated Learning](https://arxiv.org/abs/2402.01802) | 本论文提出了一个基于拍卖的联邦学习模型交易市场，允许模型的买卖并通过适当定价和激励机制来提高模型性能和实现最大交易量。 |
| [^474] | [Large Language Models for Time Series: A Survey](https://arxiv.org/abs/2402.01801) | 本调研论文深入探讨了大规模语言模型（LLM）在时间序列分析中的应用方法。通过解决LLM与数值型时间序列数据之间的差异挑战，揭示了LLM在时间序列领域的潜力，并提出了直接提示、量化、对齐、利用视觉方式和结合工具等方法。此外，还提供了对应用领域、评估方法和未来研究方向的讨论。 |
| [^475] | [Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward](https://arxiv.org/abs/2402.01799) | 本调查文章概述了在提高LLM推理效果方面的最新方法和进展，通过实验评估不同压缩技术的有效性，并提出改进LLM推理效率的潜在未来方向。 |
| [^476] | [Improved Quantization Strategies for Managing Heavy-tailed Gradients in Distributed Learning](https://arxiv.org/abs/2402.01798) | 本论文介绍了一种针对重尾梯度的新型压缩方案，在分布式学习中解决了梯度分布不均匀的挑战，并且将梯度截断和量化相结合，通过优化关键参数的值来减小量化引起的误差。 |
| [^477] | [Robust support vector machines via conic optimization](https://arxiv.org/abs/2402.01797) | 本文通过混合整数优化技术提出了一种新的损失函数，用于学习鲁棒支持向量机。与现有的方法相比，该方法更好地逼近了0-1损失，同时保持了学习问题的凸性。在实验中表现与标准SVMs相竞争，在存在异常值的情况下表现更优秀。 |
| [^478] | [Exploring transfer learning for pathological speech feature prediction: Impact of layer selection](https://arxiv.org/abs/2402.01796) | 本研究探索了用于病理语音特征预测的迁移学习，发现选择适当的层能显著提高性能并且学得的加权和具有更好的泛化能力。 |
| [^479] | [Few-Shot Scenario Testing for Autonomous Vehicles Based on Neighborhood Coverage and Similarity](https://arxiv.org/abs/2402.01795) | 本文提出了一个基于邻域覆盖和相似性的少样本场景测试框架，用于解决大规模部署之前自动驾驶车辆测试和评估中的不确定性和挑战。 |
| [^480] | [Variational Quantum Circuits Enhanced Generative Adversarial Network](https://arxiv.org/abs/2402.01791) | 这项工作提出了一种基于变分量子电路的混合量子-经典架构（QC-GAN），通过在手写图像生成任务上表现出更好的性能来改进传统GAN。这个架构在收敛时具有更少的训练参数和迭代次数，并且利用了量子电路的纠缠和表达能力。 |
| [^481] | [An introduction to graphical tensor notation for mechanistic interpretability](https://arxiv.org/abs/2402.01790) | 图形张量符号化是一种简单的表示张量操作的方法，对于理解深度学习系统和理解神经网络行为具有重要意义。本论文介绍了图形张量符号化的方法，并将其应用于不同的分解和解释语言模型的基础方法中。 |
| [^482] | [Harm Amplification in Text-to-Image Models](https://arxiv.org/abs/2402.01787) | 我们的研究提出了危害放大现象并发展了量化危害放大的方法，考虑模型输出的危害与用户输入的情境。我们还实证地研究了不同的方法在真实场景中的应用，并量化了由危害放大引起的性别之间的影响差异。 |
| [^483] | [COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations](https://arxiv.org/abs/2402.01786) | COA-GPT是一种利用大型语言模型快速高效生成有效行动方案的算法，它融合了军事学说和领域专业知识，并在军事游戏中的实验中展示了其快速生成战略合理COAs的优势。 |
| [^484] | [DoubleMLDeep: Estimation of Causal Effects with Multimodal Data](https://arxiv.org/abs/2402.01785) | 本文提出了一个利用文本和图像在因果推断和治疗效应估计中的双机器学习框架，并提出了一种生成半合成数据集的方法用于评估因果效应估计的性能。这些方法和架构在半合成数据集上进行了评估，并与标准方法进行了比较，显示了直接使用文本和图像进行因果研究的潜在好处。 |
| [^485] | [Hierarchical Multi-Label Classification of Online Vaccine Concerns](https://arxiv.org/abs/2402.01783) | 本文研究了在线疫苗关注的分层多标签分类任务，使用大型语言模型在零样本设置下检测疫苗关注，同时探索了不同提示策略的成本和准确性权衡，并提供了指导当前应用程序系统设计的具体经验教训。 |
| [^486] | [Benchmarking Spiking Neural Network Learning Methods with Varying Locality](https://arxiv.org/abs/2402.01782) | 本研究使用不同局部性对脉冲神经网络学习方法进行基准测试，并发现这些方法在性能和生物学合理性之间存在权衡。此外，研究还探讨了SNN的隐式循环特性。 |
| [^487] | [When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards](https://arxiv.org/abs/2402.01781) | 依赖基准排行榜的大型语言模型评估存在较高敏感性，微小的扰动会导致排名的显著变化。研究结果提供了几个最佳实践建议，包括选择混合评分方法来提高答案选择的性能。 |
| [^488] | [Plug-and-Play image restoration with Stochastic deNOising REgularization](https://arxiv.org/abs/2402.01779) | 本论文提出了一种新的即插即用图像恢复框架，称为随机去噪正则化（SNORE）。该框架在恰当噪声水平的图像上应用去噪器，并基于随机正则化提供了解决病态逆问题的随机梯度下降算法。实验结果表明，SNORE在去模糊和修复任务中与最先进的方法具有竞争力。 |
| [^489] | [Introduction to speech recognition](https://arxiv.org/abs/2402.01778) | 该论文介绍了使用Matlab实现的语音识别系统，通过语音建模、强大的计算机算法和机器学习方法，成功实现了对三个单词进行正确分类的任务。 |
| [^490] | [Disentangling the Roles of Target-Side Transfer and Regularization in Multilingual Machine Translation](https://arxiv.org/abs/2402.01772) | 本文通过大规模研究展示了多语言机器翻译中目标端转移的动态影响。我们发现，语言相似的辅助目标语言具有强大的正向知识转移能力，并且随着相似目标语言规模的增加，转移效果进一步增强。同时，远离的辅助目标语言也可以意外地对主要语言对产生正向转移效果。 |
| [^491] | [BlackMamba: Mixture of Experts for State-Space Models](https://arxiv.org/abs/2402.01771) | BlackMamba是一种结合了Mamba SSM和MoE的新型架构，它具有竞争力的性能和较低的推断和训练成本，在大规模语言建模领域具有潜在应用价值。 |
| [^492] | [Enriched Physics-informed Neural Networks for Dynamic Poisson-Nernst-Planck Systems](https://arxiv.org/abs/2402.01768) | 本文提出了一种增强的物理信息神经网络算法（EPINNs），用于解决具有强耦合和非线性特征的动态泊松-纳宁斯特-普朗克系统，该方法通过添加自适应损失权重和采用重采样策略来提高解算效率和精度。 |
| [^493] | [HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA](https://arxiv.org/abs/2402.01767) | HiQA是一个先进的多文档问答框架，使用分层的上下文增强和多路径检索机制，解决了大规模文档问答中的检索准确性问题，并在多文档环境中展示了最先进的性能。 |
| [^494] | [LLM Voting: Human Choices and AI Collective Decision Making](https://arxiv.org/abs/2402.01766) | 本文研究了大型语言模型（LLMs），特别是OpenAI的GPT4和LLaMA2的投票行为，并揭示了LLMs与人类在决策和偏见方面的差异。研究发现，在投票辅助中使用LLMs可能会导致更同质化的集体结果，强调了谨慎将LLMs整合到民主过程中的必要性。 |
| [^495] | [When Large Language Models Meet Vector Databases: A Survey](https://arxiv.org/abs/2402.01763) | 本综述论文深入分析了大型语言模型和向量数据库之间的交叉点，大型语言模型的突破带来了新的挑战，而向量数据库提供了潜在的解决方案，可以显著增强人工智能系统管理和利用多样数据的能力。 |
| [^496] | [Rethinking Interpretability in the Era of Large Language Models](https://arxiv.org/abs/2402.01761) | 大语言模型具有以自然语言解释的能力，能够重新定义解释性，并且在多个应用中展示出巨大潜力。 |
| [^497] | [Systematic Literature Review: Computational Approaches for Humour Style Classification](https://arxiv.org/abs/2402.01759) | 这项研究通过系统性文献综述展示了计算方法在幽默风格分类中的应用情况，并指出了当前的研究空白和有希望的方向。 |
| [^498] | [SpecDiff-GAN: A Spectrally-Shaped Noise Diffusion GAN for Speech and Music Synthesis](https://arxiv.org/abs/2402.01753) | SpecDiff-GAN 是一种基于 HiFi-GAN 的神经声码器，通过前向扩散过程来提高训练稳定性，并通过使用谱形状噪声分布使鉴别器任务更具挑战性。实验证实该模型在语音和音乐合成方面具有优越的音频质量和效率。 |
| [^499] | [Identifying False Content and Hate Speech in Sinhala YouTube Videos by Analyzing the Audio](https://arxiv.org/abs/2402.01752) | 该研究提出了一种通过分析音频来辨别僞作和仇恨言论在僧伽罗语YouTube视频中的解决方案。这些方法包括评估视频是否包含虚假信息以及检测其中是否存在仇恨言论。 |
| [^500] | [Towards Urban General Intelligence: A Review and Outlook of Urban Foundation Models](https://arxiv.org/abs/2402.01749) | 本文综述了城市基础模型在智能城市发展中的重要性和潜力，并提出了一个以数据为中心的分类方法。这个新兴领域面临着一些挑战，如缺乏清晰的定义和系统性的综述，需要进一步的研究和解决方案。 |
| [^501] | [Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems](https://arxiv.org/abs/2402.01748) | 本文提出了大型多模型(LMMs)作为AI原生无线系统的通用基础模型的设计框架，通过处理多模态感知数据、通过因果推理和检索增强生成(RAG)将物理符号表示与无线系统联系起来，并通过无线环境反馈实现可教导性，从而促进动态网络配置。 |
| [^502] | [3DG: A Framework for Using Generative AI for Handling Sparse Learner Performance Data From Intelligent Tutoring Systems](https://arxiv.org/abs/2402.01746) | 3DG框架是一种通过将数据表示为三维张量并结合张量分解和生成性人工智能模型，来处理智能辅导系统中学习者性能稀疏数据的创新方法。 |
| [^503] | [Unveiling Molecular Moieties through Hierarchical Graph Explainability](https://arxiv.org/abs/2402.01744) | 本论文提出了一种使用图神经网络和分层可解释人工智能技术的方法，能够准确预测生物活性并找到与之相关的最重要的成分。 |
| [^504] | [Towards Optimizing the Costs of LLM Usage](https://arxiv.org/abs/2402.01742) | 本论文提出了一种优化LLM使用成本的方法，通过估计输出质量并解决优化问题，实现在质量和延迟方面保持成本在预算范围内或最小化成本。 |
| [^505] | [OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2402.01739) | OpenMoE是一种开源的混合专家语言模型，通过训练和发布一系列具有可复现性的解码器模型，我们确认了MoE模型相比密集模型具有更有利的成本效益平衡，并且进行了对路由机制的深入分析，得出了三个重要发现。 |
| [^506] | [CFTM: Continuous time fractional topic model](https://arxiv.org/abs/2402.01734) | CFTM是一种新的动态主题建模方法，通过使用分数布朗运动来识别随时间变化的主题和词分布的正负相关性，揭示长期依赖性或粗糙度。实证研究结果表明，该模型能够有效地识别和跟踪主题的长期依赖性或粗糙度。 |
| [^507] | [CERM: Context-aware Literature-based Discovery via Sentiment Analysis](https://arxiv.org/abs/2402.01724) | CERM是一个通过情感分析进行基于文献的上下文感知发现的系统，旨在理解食品与健康之间的关系。通常情况下，基于食材营养成分或基于标记数据的计算模型已被用于食谱推荐和分析系统。然而，本研究提出了一种增强模型，通过捕捉食材与生物医学概念之间的固有关系，利用标记和未标记的数据来更好地支持食品相关研究。 |
| [^508] | [Deep Learning Based Amharic Chatbot for FAQs in Universities](https://arxiv.org/abs/2402.01720) | 本文提出了一个基于深度学习的阿姆哈拉语常见问题解答聊天机器人模型，可以帮助大学生解答常见问题，通过使用自然语言处理和深度学习技术，采用多种机器学习模型算法进行分析和分类，取得了最好的成绩。 |
| [^509] | [Measuring Moral Inconsistencies in Large Language Models](https://arxiv.org/abs/2402.01719) | 本研究提出了一种新的信息论度量方法，称为语义图熵（SGE），用于测量道德情景中大型语言模型（LLM）的一致性。与现有的一致性度量方法相比，SGE在五个LLMs上与人类判断更好地相关，为研究LLM不一致性的根本原因提供了新的思路。 |
| [^510] | [Bloom-epistemic and sentiment analysis hierarchical classification in course discussion forums](https://arxiv.org/abs/2402.01716) | 本研究提出了一种称为布鲁姆-认知和情感分析（BE-Sent）的层次化方法，用于评估教育讨论论坛中的情绪和布鲁姆的认知分类。方法包括数据收集、文本预处理、情感分析和认知分类。研究结果有助于了解学生在学习过程中的进展情况和知识水平。 |
| [^511] | [TrICy: Trigger-guided Data-to-text Generation with Intent aware Attention-Copy](https://arxiv.org/abs/2402.01714) | TrICy是一种轻量级框架，利用意图和触发器引导数据到文本生成任务，并通过关注-复制机制有效地处理了词汇表之外的词汇。实验结果表明其在不同数据集上取得了良好的性能。 |
| [^512] | [Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data](https://arxiv.org/abs/2402.01713) | 本研究探索了将大型语言模型（LLMs）应用于结构化纵向电子健康记录（EHR）数据的可行性，并着重研究了其零样本能力。通过考虑EHR特征和临床上下文，我们的方法在MIMIC-IV和TJH数据集上取得了良好的实验结果。 |
| [^513] | [Socially Aware Synthetic Data Generation for Suicidal Ideation Detection Using Large Language Models](https://arxiv.org/abs/2402.01712) | 通过利用生成式AI模型，我们提出了一种基于社会感知的合成数据生成方法，用于自杀意念检测。与传统模型相比，我们的方法在实际数据集上表现出有竞争力的性能。 |
| [^514] | [Exploring Educational Equity: A Machine Learning Approach to Unravel Achievement Disparities in Georgia](https://arxiv.org/abs/2402.01710) | COVID-19疫情对乔治亚州K-12教育系统造成了严重的不平等现象，尤其是在种族和民族成就差距方面。研究利用机器学习方法分析了不同人口统计学特征、地区和学科的学生成绩，发现了英语和数学熟练程度的显著下降，并且进一步探讨了社会经济地位和地理因素对学业成就的影响。 |
| [^515] | [MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds](https://arxiv.org/abs/2402.01706) | 本文通过构建多种语境，使用领域特定语言描述可能世界，并利用编译器，发现了大型语言模型在不同语境下的对齐问题。这种方法成本较低，能够更全面地研究LLM对齐问题。 |
| [^516] | [Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation](https://arxiv.org/abs/2402.01705) | 本研究超越行为主义的定义范围，提出了一种度量和减轻表征性伤害的框架，强调了大型语言模型在实施这些伤害时的脆弱性，并提出了减轻措施的建议。 |
| [^517] | [A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles](https://arxiv.org/abs/2402.01703) | 该研究提出了一种多角度的机器学习方法，用于分析洛杉矶警察与司机的互动。该方法利用多模态的数据包括音频、视频和文字信息，旨在提供对复杂和有争议的警民互动的分析工具。 |
| [^518] | [HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text Classification](https://arxiv.org/abs/2402.01696) | HiGen提出了一个基于文本生成的框架，利用语言模型来编码动态文本表示，在层次文本分类中考虑了文档各个部分的相关性，并引入了一个层级引导的损失函数。此外，还提供了一个新颖的用于HTC的数据集ENZYME。 |
| [^519] | [Language-Guided World Models: A Model-Based Approach to AI Control](https://arxiv.org/abs/2402.01695) | 语言引导的世界模型（LWMs）是一种基于模型的人工智能控制方法，它通过阅读语言描述来捕捉环境动态，提高了代理的沟通效率，并允许人类通过简洁的语言反馈同时改变他们在多个任务上的行为。 |
| [^520] | [ARGS: Alignment as Reward-Guided Search](https://arxiv.org/abs/2402.01694) | ARGS是一个对齐作为奖励导向的搜索框架，通过在解码过程中将模型的概率预测调整为奖励信号，实现生成具有语义多样性且与人类偏好对齐的文本。与基线相比，在不同任务和模型维度下，ARGS具有持续的奖励增益，表现出很好的性能。 |
| [^521] | [Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by Self-Supervised Representation Mixing and Embedding Initialization](https://arxiv.org/abs/2402.01692) | 本文提出了一种有效的转移学习框架，用于通过最少的标记和未标记数据实现语言自适应。该方法使用自监督特征进行预训练，在细调期间替换伪标签噪声部分，并结合嵌入初始化技巧，有效利用未标记数据的信息。实验证明，即使在仅有很少的数据情况下，该框架也能合成可理解的未知语言语音，并超越传统技术。这一研究结果展示了该高效语言自适应框架的潜力。 |
| [^522] | [Linguistic-Based Mild Cognitive Impairment Detection Using Informative Loss](https://arxiv.org/abs/2402.01690) | 本论文提出了一种基于自然语言处理技术的深度学习方法，用于在老年人中区分轻度认知障碍和正常认知条件。该方法使用了句子嵌入和句子交叉注意力模块，通过分析视频访谈的转录文本，提取时序特征进行分类。同时，引入了一种新的损失函数来建立稳健的模型。 |
| [^523] | ["Which LLM should I use?": Evaluating LLMs for tasks performed by Undergraduate Computer Science Students in India](https://arxiv.org/abs/2402.01687) | 本研究评估了各种LLMs在本科计算机科学学生常见任务中的效果，并指导学生选择适合他们的LLM. |
| [^524] | [A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm](https://arxiv.org/abs/2402.01684) | 提出了一个使用CGC-LORA算法在LLMs中实现1 + N多任务微调模式的统一框架，旨在解决高计算成本和摇摆问题。 |
| [^525] | [Prerequisite Structure Discovery in Intelligent Tutoring Systems](https://arxiv.org/abs/2402.01672) | 本研究提出了一种在智能辅导系统中整合知识结构和知识追踪的方法，通过学习者轨迹发现潜在的先决条件结构，并应用于内容推荐和评估推荐算法。 |
| [^526] | [Improved Performances and Motivation in Intelligent Tutoring Systems: Combining Machine Learning and Learner Choice](https://arxiv.org/abs/2402.01669) | 本研究通过结合机器学习和学生选择，改进了智能辅导系统的性能和动机。使用ZPDES算法，该系统能够最大化学习进展，并在实地研究中提高了不同学生群体的学习成绩。研究还探讨了学生选择对学习效率和动机的影响。 |
| [^527] | [Knowledge-Driven Deep Learning Paradigms for Wireless Network Optimization in 6G](https://arxiv.org/abs/2402.01665) | 基于知识驱动的深度学习范式在6G无线网络优化中的应用通过整合领域知识和神经网络构建来解决复杂的网络优化问题。 |
| [^528] | [Killer Apps: Low-Speed, Large-Scale AI Weapons](https://arxiv.org/abs/2402.01663) | 本文研究了AI武器的概念、部署、检测和潜在对策，强调了在信息领域内基于AI的心理操纵的潜力，以及其对全球个人、组织和社会的威胁。 |
| [^529] | [Untersuchung der Wirkung von Data Storytelling auf das Datenverstaendnis von Dashboard-Nutzern](https://arxiv.org/abs/2402.01658) | 本研究通过实证数据分析验证了数据叙事对组织绩效的正面影响，这部分是由于所传达的决策质量。这为进一步研究数据叙事的前因和后果提供了理论基础。 |
| [^530] | [A Deep Learning Approach Towards Student Performance Prediction in Online Courses: Challenges Based on a Global Perspective](https://arxiv.org/abs/2402.01655) | 该论文研究了基于深度学习的方法在全球范围内预测在线课程学生表现的挑战，并证明了深度学习模型在此方面具有有希望的性能。 |
| [^531] | [User-Centric AI Analytics for Chronic Health Conditions Management](https://arxiv.org/abs/2402.01652) | 本论文介绍了AI分析在管理慢性健康状况中的应用，特别是在无药物治疗方法中的挑战。研究呈现了用户中心的方法以及相关的研究问题和下一步工作。 |
| [^532] | [Forecasting Imports in OECD Member Countries and Iran by Using Neural Network Algorithms of LSTM](https://arxiv.org/abs/2402.01648) | 这项研究利用LSTM神经网络算法预测了选定的OECD成员国和伊朗在2021年到2025年的进口情况，结果表明其准确性达到了99%。 |
| [^533] | [Build Your Own Robot Friend: An Open-Source Learning Module for Accessible and Engaging AI Education](https://arxiv.org/abs/2402.01647) | 本文开发了一个面向大学和高中学生的开源学习模块，允许学生从零开始构建自己的机器人伙伴，并提供实践经验和入门知识，包括机器人技术、机器学习、软件工程和机械工程等。同时，该模块着重强调以人为本的人工智能，使学生能够更好地理解和开发。 |
| [^534] | [L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs](https://arxiv.org/abs/2402.01643) | 本文介绍了L-Tuning，一种在自然语言推理（NLI）框架内的高效微调方法，通过对预训练的LLM中的标签标记进行微调，提高了训练准确性和效率，并增强了模型的训练细微差别。 |
| [^535] | [Detection of Machine-Generated Text: Literature Survey](https://arxiv.org/abs/2402.01642) | 这项论文对机器生成文本的检测进行了文献综述，指出了语言模型生成的虚假文本大量存在于公共领域，因此需要采取预防措施来应对其可能带来的危险影响。 |
| [^536] | [Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/abs/2402.00798) | 本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。 |
| [^537] | [Developing A Multi-Agent and Self-Adaptive Framework with Deep Reinforcement Learning for Dynamic Portfolio Risk Management](https://arxiv.org/abs/2402.00515) | 本研究开发了一种基于深度强化学习的多智能体自适应框架，用于动态投资组合风险管理。通过两个协同反应的智能体，平衡整体投资组合回报和潜在风险，解决了在复杂金融市场环境下的投资策略问题。 |
| [^538] | [Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding](https://arxiv.org/abs/2402.00024) | LLaMA和ChatGPT比较分析了它们在SMILES字符串嵌入中的性能，在分子性质预测和药物-药物相互作用预测中，LLaMA相对于ChatGPT表现更好并且与现有方法相当。 |
| [^539] | [EEG-GPT: Exploring Capabilities of Large Language Models for EEG Classification and Interpretation](https://arxiv.org/abs/2401.18006) | EEG-GPT是一种利用大型语言模型来分类和解读EEG的方法，它能够实现多尺度电生理理解和分类，且在few-shot学习范式中表现出色。 |
| [^540] | [Efficient Subseasonal Weather Forecast using Teleconnection-informed Transformers](https://arxiv.org/abs/2401.17870) | 提出了一种使用远程连接通知的Transformer模型来实现高效的次季节天气预报，该方法通过利用预训练模型和集成远程连接通知的时间模块来改进预测能力。 |
| [^541] | [IGCN: Integrative Graph Convolutional Networks for Multi-modal Data](https://arxiv.org/abs/2401.17612) | IGCN是用于多模态数据的综合神经网络，通过综合分析多模态数据来获得更好的学习表示，并提高模型的可解释性。 |
| [^542] | [Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2401.17263) | 该论文提出了一种鲁棒的提示优化算法（RPO）用于对抗语言模型的破解攻击，通过梯度优化来确保输出的无害性，并成功降低了攻击成功率。 |
| [^543] | [The Bigger the Better? Rethinking the Effective Model Scale in Long-term Time Series Forecasting](https://arxiv.org/abs/2401.11929) | 本研究通过调查条件相关性和自相关性，揭示了输入数据中的冗余性，并提出了HDformer，这是一种轻量级的Transformer变种，利用蒸馏技术和快速网络连接层来降低模型复杂性。 |
| [^544] | [BanglaNet: Bangla Handwritten Character Recognition using Ensembling of Convolutional Neural Network](https://arxiv.org/abs/2401.08035) | 本文提出了BanglaNet，使用卷积神经网络集合的方法对孟加拉手写字符进行识别。实验结果表明，在三个基准数据集上，与最新的CNN研究相比，取得了显著的识别准确率提升。 |
| [^545] | [Towards Engineering Fair and Equitable Software Systems for Managing Low-Altitude Airspace Authorizations](https://arxiv.org/abs/2401.07353) | 本文旨在为管理低空领域授权而构建公平和公正的软件系统进行工程方法研究。研究结果表明，飞行特征和环境条件被认为是最重要的因素，但还应考虑飞行员和无人机的能力。此外，许多受访者对使用机器学习算法批准或拒绝飞行请求表示反对。 |
| [^546] | [Malware Detection in IOT Systems Using Machine Learning Techniques](https://arxiv.org/abs/2312.17683) | 本研究提出了一种基于CNN-LSTM混合模型的物联网恶意软件识别方法，通过与已有方法的比较和评估，展示了该方法的高准确率和突出性能，为增强物联网安全性提供了潜力。 |
| [^547] | [Bespoke Approximation of Multiplication-Accumulation and Activation Targeting Printed Multilayer Perceptrons](https://arxiv.org/abs/2312.17612) | 本研究提出了一种针对印刷电子技术中的限制的自动化框架，用于设计超低功耗的多层感知机（MLP）分类器。 |
| [^548] | [AdaNAS: Adaptively Post-processing with Self-supervised Neural Architecture Search for Ensemble Rainfall Forecasts](https://arxiv.org/abs/2312.16046) | AdaNAS是一个自适应后处理方法，利用自我监督神经架构搜索对集合降雨预报进行处理，能够提高降雨预测的准确性。它采用面向降雨的搜索空间和降雨层级规则化函数，有效消除噪声数据的影响。 |
| [^549] | [An extended asymmetric sigmoid with Perceptron (SIGTRON) for imbalanced linear classification](https://arxiv.org/abs/2312.16043) | 本文提出了一个新的多项式参数化sigmoid函数(SIGTRON)，并且介绍了其伴随的SIC模型。相比传统的成本敏感学习模型，在给定的训练数据集接近良好平衡的条件下，所提出的SIC模型对于数据集的变化更加适应，并通过创建倾斜的超平面方程来实现。 |
| [^550] | [BiSwift: Bandwidth Orchestrator for Multi-Stream Video Analytics on Edge](https://arxiv.org/abs/2312.15740) | BiSwift是一个用于边缘多流视频分析的带宽编排器，它通过一个创新的自适应混合编解码器和全局带宽控制器，实现了高并发实时视频分析以及多个流之间的准确性公平性。 |
| [^551] | [Faster Rates for Switchback Experiments](https://arxiv.org/abs/2312.15574) | 本研究提出了一种更快速的Switchback实验方法，通过使用整个时间块，以 $\sqrt{\log T/T}$ 的速率估计全局平均处理效应。 |
| [^552] | [Structured Probabilistic Coding](https://arxiv.org/abs/2312.13933) | 结构化概率编码（SPC）是一种新的监督式表示学习框架，通过编码和预测任务的信息来学习紧凑且信息丰富的表示，提高语言模型的泛化能力和语言理解能力，并通过结构化正则化实现更好的覆盖率。 |
| [^553] | [Multimodal Federated Learning with Missing Modality via Prototype Mask and Contrast](https://arxiv.org/abs/2312.13508) | 本文介绍了一种利用原型遮罩和对比处理来解决多模态联邦学习中缺失模态问题的方法，该方法可以有效缓解由于模态缺失引起的全局模型性能下降。 |
| [^554] | [On the Trade-off between the Number of Nodes and the Number of Trees in a Random Forest](https://arxiv.org/abs/2312.11540) | 本文研究了在随机森林中使用较小的决策树集合来代表一个决策树集合的问题，并给出了多数函数和分类错误的表示方法，同时讨论了k-选-n函数的相关结果。 |
| [^555] | [Spectral State Space Models](https://arxiv.org/abs/2312.06837) | 本文提出了一种称为光谱状态空间模型的序列预测架构，通过学习具有光谱滤波算法的线性动态系统实现。这些模型具有可证明的鲁棒性和固定卷积滤波器，适用于需要非常长程记忆的预测任务。 |
| [^556] | [Efficient Parallel Reinforcement Learning Framework using the Reactor Model](https://arxiv.org/abs/2312.04704) | 本论文提出了一种使用反应器模型的高效并行强化学习框架，可以有效解决现有框架在同步和输入/输出方面的低效问题。 |
| [^557] | [GraphMETRO: Mitigating Complex Graph Distribution Shifts via Mixture of Aligned Experts](https://arxiv.org/abs/2312.04693) | GraphMETRO是一种通过混合对齐专家来减轻复杂图分布变化的图神经网络架构，通过门控模型和多个专家模型对不同分布变化进行建模，并设计了新颖的目标函数进行平滑优化，在真实世界数据集上获得了显著的性能提升。 |
| [^558] | [Adapting Newton's Method to Neural Networks through a Summary of Higher-Order Derivatives](https://arxiv.org/abs/2312.03885) | 本论文通过计算高阶导数，将牛顿法应用于神经网络中，提出了一个适用于各种架构的深度神经网络的二阶优化方法。 |
| [^559] | [Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift](https://arxiv.org/abs/2312.00050) | Elijah是一种用于扩散模型的后门检测和消除框架，能够准确检测后门并将其效果减少至接近零，而不会显著牺牲模型的实用性。 |
| [^560] | [Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web](https://arxiv.org/abs/2311.18751) | 本文介绍了语言模型代理 (LMA) 在多步决策任务上的有希望的范例，在基本任务上具有出色的性能，但在组合任务上表现不佳。通过平衡数据分布，我们训练了一个新模型 HTML-T5++，在现实应用中取得了超越人类的性能，并在新基准测试中实现了最佳零-shot性能。 |
| [^561] | [Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization](https://arxiv.org/abs/2311.18703) | 该论文提出了一种名为PA-RL的方法，通过最小化熵率来引导强化学习智能体展现可预测的行为。研究展示了如何利用平均替代奖励实现确定性策略，并在动态模型的基础上近似计算值函数。 |
| [^562] | [Compelling ReLU Network Initialization and Training to Leverage Exponential Scaling with Depth](https://arxiv.org/abs/2311.18022) | 该论文提出了一种新的训练策略，通过重新参数化网络权重，使得神经网络的指数数量的激活模式得以展现，从而得到远远超过随机初始化的结果。 |
| [^563] | [Analyzing Sharpness-aware Minimization under Overparameterization](https://arxiv.org/abs/2311.17539) | 本文分析了在过参数化条件下的锐度感知最小化方法。通过实证和理论结果，发现过参数化对锐度感知最小化具有重要影响，并且在过参数化增加的情况下，锐度感知最小化仍然受益。 |
| [^564] | [ChatTraffic: Text-to-Traffic Generation via Diffusion Model](https://arxiv.org/abs/2311.16203) | 本研究提出了一种名为ChatTraffic的扩散模型，用于实现文本到交通生成。通过将生成模型与描述交通系统的文本结合，该方法能够解决传统交通预测方法中的两个挑战，并得到与真实交通数据一致的合成数据。 |
| [^565] | [Univariate Radial Basis Function Layers: Brain-inspired Deep Neural Layers for Low-Dimensional Inputs](https://arxiv.org/abs/2311.16148) | 该研究提出了一种新颖的深度神经网络层称为单变量径向基函数层，模拟了大脑中感觉神经元的处理方式，通过对每个输入维度进行专门处理，提供了一种适用于低维输入的替代方法，并在低维函数回归和强化学习任务中证明了其有效性。 |
| [^566] | [Metric Space Magnitude for Evaluating the Diversity of Latent Representations](https://arxiv.org/abs/2311.16054) | 基于度量空间大小的潜在表示多样性度量，可稳定计算，能够进行多尺度比较，在多个领域和任务中展现出优越性能。 |
| [^567] | [Data Diversity Matters for Robust Instruction Tuning](https://arxiv.org/abs/2311.14736) | 数据多样性对鲁棒指令调整非常重要，我们提出了一种新算法(QDIT)，通过同时控制数据集的多样性和质量，我们深入研究了多样性和质量对指令调整性能的影响，并得出了两个关键观点。 |
| [^568] | [One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space](https://arxiv.org/abs/2311.14652) | 本文研究了在超长上下文下内存效率的问题，提出一种用于超长Token注意力近似的单次流算法，通过构建矩阵$U_1, U_2$加速注意力计算，解决了部署大型语言模型时的计算资源问题。 |
| [^569] | [Assumption-lean and Data-adaptive Post-Prediction Inference](https://arxiv.org/abs/2311.14220) | 这项工作介绍了一种假设简化和数据自适应的后预测推断（POP-Inf）过程，可以有效且有力地基于机器学习预测结果进行统计推断。 |
| [^570] | [Sample as You Infer: Predictive Coding With Langevin Dynamics](https://arxiv.org/abs/2311.13664) | 本文提出了以采样为导向的Langevin动力学的预测编码算法，通过对PC推理过程注入高斯噪声实现过阻尼的Langevin采样，并改进了结果编码器自由训练方法，通过编码器网络提供摊销的热启动。此外，还验证了一种轻量级且易于计算的预处理形式，使得算法具有更好的性能和鲁棒性。 |
| [^571] | [Multi-Objective Reinforcement Learning Based on Decomposition: A Taxonomy and Framework](https://arxiv.org/abs/2311.12495) | 基于分解的多目标强化学习（MORL/D）是一种新的方法，连接了强化学习和多目标优化领域。该论文提出了一个全面的MORL/D分类法，为现有和潜在的MORL工作提供了结构化的基础。 |
| [^572] | [Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity](https://arxiv.org/abs/2311.12267) | 该论文研究了从一般环境中学习因果表示的问题，提供了基于这种环境生成的数据的可辨识性结果，并指出了受到围绕节点歧义的限制。同时提出了一个算法可以恢复出地面真实模型 |
| [^573] | [Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers](https://arxiv.org/abs/2311.10642) | 本研究探索使用浅层前馈神经网络替代注意力机制，通过知识蒸馏方法训练，实验证明了这种"无注意力的Transformers"可以与原始架构的性能媲美，并揭示了其简化复杂架构的潜力。 |
| [^574] | [Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning](https://arxiv.org/abs/2311.10246) | 本论文提出了一种基于惊喜性驱动的稳健可解释的k-NN算法，通过使用信息论的角度对传统算法进行新的阐释，实现了在非参数学习中的分类、回归、密度估计和异常检测等任务。 |
| [^575] | [Divergences between Language Models and Human Brains](https://arxiv.org/abs/2311.09308) | 该论文系统地探索了语言模型（LMs）和人类大脑在语言处理方面的差异，发现在社交/情感智能和物理常识领域，LMs无法很好地捕捉到人类的表现，但在这些领域对LMs进行微调可以提高其性能。 |
| [^576] | [Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models](https://arxiv.org/abs/2311.06233) | 这个工具使用数据污染问题（DCQ）的方法来检测和估计大型语言模型中的数据污染。在DCQ中，我们创建了每个数据集实例的扰动版本，并让语言模型从中选择原始实例，通过词级扰动来区分选项。这种方法利用了语言模型在预训练阶段暴露于原始实例时的固有特性。 |
| [^577] | [PowerFlowNet: Power Flow Approximation Using Message Passing Graph Neural Networks](https://arxiv.org/abs/2311.03415) | PowerFlowNet 是一种使用消息传递图神经网络进行功率流近似的新型架构，与传统的牛顿-拉夫逊方法相比，在简单的系统中速度提高了4倍，在实际的法国高电压网络中提高了145倍，同时在性能和执行时间方面明显优于其他传统方法。 |
| [^578] | [Multitask Kernel-based Learning with First-Order Logic Constraints](https://arxiv.org/abs/2311.03340) | 本文提出了一个通用框架，将有监督和无监督示例与一阶逻辑背景知识整合到核机器学习中，实现多任务学习。通过将一阶逻辑约束转化为连续实现的形式，有效处理基于核的谓词的输出。 |
| [^579] | [Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch](https://arxiv.org/abs/2311.03099) | 本文揭示了语言模型可以通过吸收同源模型的参数来获得新的能力，而无需重新训练或使用GPU。作者提出了DARE技术来稀疏化参数并将多个同源模型合并为一个模型。实验证明，DARE可以轻松删除大部分参数并实现多任务融合。 |
| [^580] | [Individualized Policy Evaluation and Learning under Clustered Network Interference](https://arxiv.org/abs/2311.02467) | 本文研究了集群网络干扰下个体化策略评估与学习的问题，提出了一种只假设半参数结构模型的方法，能够更准确地评估和学习最优的个体化处理规则。 |
| [^581] | [Adversarial Examples Are Not Real Features](https://arxiv.org/abs/2310.18936) | 该论文从多个学习范式的角度重新审视对抗样本的理论，发现非鲁棒特征在多种无监督学习范式中的效用较差，揭示了这些特征并不像鲁棒特征或自然特征那样真正有用。 |
| [^582] | [Rethinking Semi-Supervised Imbalanced Node Classification from Bias-Variance Decomposition](https://arxiv.org/abs/2310.18765) | 本文提出了一种新的方法来解决图神经网络中的类别不平衡问题。该方法将不平衡节点分类和偏差-方差分解相结合，利用图扩充技术估计方差，并通过正则项减轻不平衡的影响。在多个基准数据集上进行的测试结果表明，该方法在各种不平衡场景中优于现有最先进的方法，并为解决GNN中的不平衡节点分类问题提供了一种新颖的理论视角。 |
| [^583] | [MicroNAS: Memory and Latency Constrained Hardware-Aware Neural Architecture Search for Time Series Classification on Microcontrollers](https://arxiv.org/abs/2310.18384) | MicroNAS是针对微控制器上时间序列分类的硬件感知神经架构搜索系统，能够生成满足用户定义的执行延迟和内存消耗限制的优化架构。 |
| [^584] | [HelmFluid: Learning Helmholtz Dynamics for Interpretable Fluid Prediction](https://arxiv.org/abs/2310.10565) | 本文提出了HelmFluid，一个精确且可解释的流体预测器。通过学习Helmholtz动力学，将流体动力学分解为更可解的无旋和无散部分，并结合多尺度多头积分架构进行集成，HelmFluid能够更准确地预测流体的未来行为。 |
| [^585] | [Zero-Level-Set Encoder for Neural Distance Fields](https://arxiv.org/abs/2310.06644) | 本文提出了一种用于嵌入3D形状的神经网络架构，通过多尺度混合系统和连续可微的解码器，不仅能够生成有效的有符号距离场，还能够在训练和推断中仅使用零水平集的知识。同时，还提出了针对曲面法线不存在情况的损失函数修改。 |
| [^586] | [Guiding Language Model Math Reasoning with Planning Tokens](https://arxiv.org/abs/2310.05707) | 本论文介绍了一种通过引入规划标记来引导语言模型进行数学推理的方法。这种方法在保持推理过程中的一致性方面具有显著的准确性提升，而增加的训练参数很少。 |
| [^587] | [Local and Global Trend Bayesian Exponential Smoothing Models](https://arxiv.org/abs/2309.13950) | 本文描述了一组季节性和非季节性时间序列模型，可以用于建模增长速度介于线性和指数之间的时间序列。模型包括全局趋势从加法到乘法的平滑变化，与线性局部趋势相结合，并采用乘法季节性和异方差的加法误差。通过贝叶斯拟合技术，该模型在M3竞赛数据集上表现优于其他模型。 |
| [^588] | [DiffusionWorldViewer: Exposing and Broadening the Worldview Reflected by Generative Text-to-Image Models](https://arxiv.org/abs/2309.09944) | 本文章提出了DiffusionWorldViewer，一个交互界面，旨在揭示和拓宽生成式文本到图像模型的世界观。通过在输出的不同人口统计数据之间揭示世界观，并提供编辑工具，帮助用户在生成的图像中代表他们多样化的观点，并挑战当前模型中反映的有限世界观。 |
| [^589] | [Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes](https://arxiv.org/abs/2309.01922) | 本文提出了一种基于策略梯度的算法用于无限时域平均奖励马尔可夫决策过程，并证明了其全局收敛性和近似O(T^3/4)的遗憾界。 |
| [^590] | [Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed Bandit Approach](https://arxiv.org/abs/2308.10699) | 本论文提供了一种基于组合多臂赌博机的成本有效的在线决策框架，并利用后验抽样或BayesUCB进行探索。实验结果证明了该框架在实际问题中的适用性。 |
| [^591] | [UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming](https://arxiv.org/abs/2307.16375) | UniAP是一种新型的自动并行化方法，通过混合整数二次规划统一跨层和内层的自动并行化。与现有方法相比，UniAP在吞吐量方面表现更好，并且减少了策略优化时间。 |
| [^592] | [Quantum Neural Estimation of Entropies](https://arxiv.org/abs/2307.01171) | 本文提出了一种量子神经网络算法，可以准确估计量子系统中的熵度量，并表明其在下游任务中有潜在应用价值。 |
| [^593] | [Attention-Refined Unrolling for Sparse Sequential micro-Doppler Reconstruction](https://arxiv.org/abs/2306.14233) | 提出了STAR，一种基于神经网络的方法，能够从高度不完整的信道测量中重建人体运动的微多普勒序列。 |
| [^594] | [A Survey of Contextual Optimization Methods for Decision Making under Uncertainty](https://arxiv.org/abs/2306.10374) | 本综述文章对上下文优化领域进行了综述，介绍了将预测算法和优化技术结合解决不确定决策问题的研究进展。文章通过研究单阶段和两阶段随机规划问题，确定了三种主要的从数据中学习策略的框架，并讨论了它们的优点和缺点。 |
| [^595] | [Feature Importance Disparities for Data Bias Investigations](https://arxiv.org/abs/2303.01704) | 本文介绍了一种新的方法，通过给定一个数据集，利用特征重要性差异（FID）来调查数据偏差，并展示了实验证据支持该方法的有效性。 |
| [^596] | [Distributional GFlowNets with Quantile Flows](https://arxiv.org/abs/2302.05793) | 本文提出了一种带分布式量化流的GFlowNets模型，通过将流函数转化为分布，在训练过程中提供更多信息的学习信号。通过量化函数参数化每个边流，我们提出的算法可以学习风险敏感的策略，实现对风险不确定性场景的处理，并在现有基准上取得了显著改进。 |
| [^597] | [A Reparameterized Discrete Diffusion Model for Text Generation](https://arxiv.org/abs/2302.05737) | 本文提出了一种重新参数化离散扩散模型，该模型在文本生成方面表现出更好的灵活性、训练技术和生成效果，实验证明其较现有的扩散模型有显著的改进。 |
| [^598] | [Regularization and Optimization in Model-Based Clustering](https://arxiv.org/abs/2302.02450) | 本论文针对基于模型的聚类中的正则化和优化提出了解决的方法，通过设计更有效的通用GMM优化算法以及结合正则化策略，解决了局部最小值数量较多以及解决方案过度拟合数据的问题。 |
| [^599] | [The Fair Value of Data Under Heterogeneous Privacy Constraints in Federated Learning](https://arxiv.org/abs/2301.13336) | 本文提出了一种在给定隐私级别下基于公正定义的公平补偿用户数据的方法，并且考虑了隐私约束，是第一个明确考虑隐私约束的数据公平概念。同时，本文还研究了具有不同隐私级别、数据量和异质程度的公平分配下用户获得的补偿金额，并且讨论了平台被迫设计公平激励措施时的情况。 |
| [^600] | [You Can Have Better Graph Neural Networks by Not Training Weights at All: Finding Untrained GNNs Tickets](https://arxiv.org/abs/2211.15335) | 通过发现未经训练的稀疏子网络，我们可以在初始化时实现与完全训练的GNNs相媲美的性能，同时有效缓解GNNs过度平滑问题，为实现更深层GNNs提供了强大的工具。 |
| [^601] | [Almost Tight Error Bounds on Differentially Private Continual Counting](https://arxiv.org/abs/2211.05006) | 这项研究提出了一种新颖的机制，能够在差分隐私持续计数中减小误差，并证明其均方误差比二进制机制小一个因子10。研究还给出了几乎严格的常数界限，以及对私有学习算法超出风险的上界。 |
| [^602] | [Multimodal Speech Enhancement Using Burst Propagation](https://arxiv.org/abs/2209.03275) | 本论文提出了一种采用爆发传播的多模态语音增强解决方案，通过学习噪声信号和视觉刺激之间的相关性，放大相关信息并抑制噪声，从而赋予语音含义。 |
| [^603] | [ANAct: Adaptive Normalization for Activation Functions](https://arxiv.org/abs/2208.13315) | 本文研究了激活函数对神经网络前向和反向传播的负面影响，并提出了一种自适应归一化的激活函数方法ANAct来保持一致的梯度差异，通过实验证明了其有效性。 |
| [^604] | [Accelerated Algorithms for Constrained Nonconvex-Nonconcave Min-Max Optimization and Comonotone Inclusion](https://arxiv.org/abs/2206.05248) | 本论文提出了针对约束共单调极小-极大优化和共单调包含问题的加速算法，扩展了现有算法并实现了较优的收敛速率，同时证明了算法的收敛性。 |
| [^605] | [Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning](https://arxiv.org/abs/2204.04510) | 提出了一种将子图转化为节点的方法来学习子图的表示，该方法不仅显著降低了内存和计算成本，还捕捉了子图的局部和全局结构，并在多个基准测试上表现出色。 |
| [^606] | [Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence](https://arxiv.org/abs/2202.03482) | 本文重新审视了概念激活向量（CAVs）在建模人类可理解的概念中的应用，并引入了基于模式的CAVs来提供更准确的概念方向。 |
| [^607] | [Realizable Learning is All You Need](https://arxiv.org/abs/2111.04746) | 可实现学习与无偏学习的等价性是学习理论中的基本现象，我们提出了第一个独立于模型的框架来解释这个等价性，它可以适用于各种设置，并拓展了我们对各种学习情况的理解。 |
| [^608] | [Multiply Robust Causal Mediation Analysis with Continuous Treatments](https://arxiv.org/abs/2105.09254) | 本文提出了一种适用于连续治疗环境的多重稳健因果中介分析估计器，采用了核平滑方法，并具有多重稳健性和渐近正态性。 |
| [^609] | [A rigorous introduction to linear models](https://arxiv.org/abs/2105.04240) | 本书旨在向读者提供对线性模型及其理论的严格介绍，并总结了线性模型在回归问题中的重要性和应用。 |
| [^610] | [Context-self contrastive pretraining for crop type semantic segmentation](https://arxiv.org/abs/2104.04310) | 这项研究提出了一种针对农作物类型语义分割任务的上下文自对比预训练方法，通过学习嵌入空间的相似度度量，突出语义边界。实验结果表明，这种方法改善了该任务的最新技术表现，并提供了一个大规模的密集注释数据集以及数据生成流程。 |
| [^611] | [Optimal Clustering from Noisy Binary Feedback](https://arxiv.org/abs/1910.06002) | 本论文研究了通过二进制用户反馈进行聚类的问题，并提出了一种算法来最小化聚类恢复错误率。 |
| [^612] | [Agnostic Sample Compression Schemes for Regression](https://arxiv.org/abs/1810.01864) | 本文在绝对值损失函数为 $\ell_p$ 的不确定回归设置中构建了一种通用的逼近样本压缩方案，对于线性回归可以实现线性维度大小的压缩，对于 $\ell_1$ 和 $\ell_\infty$ 损失函数可以实现线性维度大小的有效完全样本压缩方案；同时，证明了其他 $\ell_p$ 损失函数不存在有限尺寸的完全不可知压缩方案的结果，并提出了开放问题。 |
| [^613] | [Encoding Temporal Statistical-space Priors via Augmented Representation.](http://arxiv.org/abs/2401.16808) | 通过增加表示的方式编码时间统计空间先验，以应对时间序列数据建模中的挑战。我们的方法在两个数据集上的实证推广性能明显优于五个最新的基准方法。具有高度模块化性质的方法适用于各种场景。 |
| [^614] | [Engineering A Large Language Model From Scratch.](http://arxiv.org/abs/2401.16736) | Atinuke是一种基于Transformer的神经网络，通过在处理时序数据的层与注意机制交织在一起，模拟人类语言，从而优化各种语言任务的性能。 |
| [^615] | [Context-Former: Stitching via Latent Conditioned Sequence Modeling.](http://arxiv.org/abs/2401.16452) | Context-Former是一种集成了基于情境信息的模仿学习和序列建模的方法，通过拼接次优轨迹片段来改善决策，并提高了Decision Transformer的性能。 |
| [^616] | [Ransomware threat mitigation through network traffic analysis and machine learning techniques.](http://arxiv.org/abs/2401.15285) | 本文介绍了一种通过分析网络流量和使用机器学习算法来识别和检测勒索软件的方法，该方法在实践中表现出高水平的精度和准确性。 |
| [^617] | [Towards Stable Preferences for Stakeholder-aligned Machine Learning.](http://arxiv.org/abs/2401.15268) | 本研究旨在通过数据驱动的方法，整合个人和团体的偏好，解决肾脏分配中的紧迫挑战，并评估偏好的稳定性。 |
| [^618] | [Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection.](http://arxiv.org/abs/2401.15222) | 本研究提出了一种使用多任务学习和迁移学习的方法，在临床文本中预测实体修饰语。实验结果表明，该方法在公开数据集和新数据集上均取得了最新技术的结果。 |
| [^619] | [EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty.](http://arxiv.org/abs/2401.15077) | EAGLE是一个无损加速语言模型推理的框架，通过在次顶层特征层面上自回归推理，并解决采样不确定性问题，实现了比传统方法更快3倍的速度。 |
| [^620] | [Machine learning-based analysis of glioma tissue sections: a review.](http://arxiv.org/abs/2401.15022) | 机器学习技术在胶质瘤组织切片分析中具有诊断和预测的潜力，当前研究聚焦于成人型弥漫性胶质瘤的苏木精和伊红染色组织切片，以及对该疾病的分类、分级、分子标记预测和生存预测等临床任务。 |
| [^621] | [Ricci flow-guided autoencoders in learning time-dependent dynamics.](http://arxiv.org/abs/2401.14591) | 利用Ricci流引导的自编码器方法能够学习非线性动力学，尤其是偏微分方程。该方法通过在训练中学习流形，并使用Ricci流使流形潜空间逐步适应动力学的变化，从而获得更好的表示能力。在实验中，我们展示了该方法在具有周期性和随机性的PDE上的应用，并评估了在分布内和外推场景中的误差。 |
| [^622] | [Design Your Own Universe: A Physics-Informed Agnostic Method for Enhancing Graph Neural Networks.](http://arxiv.org/abs/2401.14580) | 本文提出了一种物理信息引导的无偏方法来增强图神经网络，通过引入附加节点和使用正负权重重连连接来丰富图结构，以解决过度平滑和过度压缩的问题。 |
| [^623] | [Federated learning with distributed fixed design quantum chips and quantum channels.](http://arxiv.org/abs/2401.13421) | 本论文提出了一种具有分布式固定设计量子芯片和量子信道的量子联邦学习模型，通过量子态的传递和聚合梯度来更新参数，提供更高的隐私保护和指数级的效率。 |
| [^624] | [Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent.](http://arxiv.org/abs/2401.11940) | 本文提出了一种通过分解梯度下降方法解决低胞状秩张量恢复问题的高效方法，该方法通过将大张量分解为两个较小的因子张量，在减少计算成本和存储需求的同时，确保了收敛性。 |
| [^625] | [LoMA: Lossless Compressed Memory Attention.](http://arxiv.org/abs/2401.09486) | LoMA是一种无损压缩的内存注意力方法，可以有效地处理长文本并减少资源消耗。 |
| [^626] | [DCRMTA: Unbiased Causal Representation for Multi-touch Attribution.](http://arxiv.org/abs/2401.08875) | DCRMTA提出了一种无偏的多触点归因方法，通过建立转化预测模型和构建对照触点序列来减轻偏差的影响。 |
| [^627] | [Do We Really Even Need Data?.](http://arxiv.org/abs/2401.08702) | 本文探讨了使用预训练算法的预测作为因变量的统计挑战，并着重阐述了三个可能的错误来源。 |
| [^628] | [Differentially Private Estimation of CATE in Adaptive Experiment.](http://arxiv.org/abs/2401.08224) | 本研究提出在自适应实验中使用差分隐私估计条件平均处理效果(CATE)，平衡社会福利损失和统计功率，并通过匹配上下界以及帕累托最优性概念来获得最优解。 |
| [^629] | [An adaptive network-based approach for advanced forecasting of cryptocurrency values.](http://arxiv.org/abs/2401.05441) | 本文提出了一种使用自适应网络的方法，通过历史数据对比特币、以太坊和比特币支配度、以太坊支配度进行预测，具有较高的预测准确性。 |
| [^630] | [CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation in Classification Tasks.](http://arxiv.org/abs/2401.05043) | CreINNs是一种用于分类任务的Credal-Set Interval Neural Networks，通过保留传统的区间神经网络结构，捕捉权重不确定性，并使用概率区间的数学框架预测可信区间。实验结果表明，CreINNs在不确定性估计方面优于变分贝叶斯神经网络和深度集成，并且具有较低的计算复杂度和模型大小。 |
| [^631] | [LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning.](http://arxiv.org/abs/2401.01325) | 本研究提出了一种名为Self-Extend的方法，通过自身扩展现有LLMs的上下文窗口，无需调整，充分利用LLMs处理长上下文的固有能力。 |
| [^632] | [Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming Controllers Inspired by Model Predictive Control.](http://arxiv.org/abs/2312.05332) | 本文提出了一种新的参数化控制器类，利用深度强化学习训练其控制器的参数，从而消除了常见控制器中的可验证性和性能保证的限制。该控制器类似于模型预测控制问题的二次规划求解器，具有可验证的属性，并且在控制性能和鲁棒性方面与其他控制器相媲美。同时，该控制器的计算效率显著优于传统的模型预测控制。 |
| [^633] | [SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer.](http://arxiv.org/abs/2312.01187) | SASSL提出了一种基于神经风格迁移的增强技术，通过解耦语义和风格属性，在自监督学习中生成多样化的增强样本，从而提升了图像分类性能。 |
| [^634] | [Piecewise polynomial regression of tame functions via integer programming.](http://arxiv.org/abs/2311.13544) | 本论文提出了使用整数规划对温顺函数进行分段多项式回归的方法，这可以用于估计包含在许多应用中的温顺函数，并且展示了令人期待的计算结果。 |
| [^635] | [Anytime-Competitive Reinforcement Learning with Policy Prior.](http://arxiv.org/abs/2311.01568) | 本文研究了具有策略先验的任意时刻竞争性强化学习问题（A-CMDP）。我们提出了一种新的算法ACRL，该算法可以保证任意时刻的成本约束，并在实验中验证了其回报性能和成本约束保证。 |
| [^636] | [Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models.](http://arxiv.org/abs/2311.01441) | 本文提出了一个概念简单且轻量级的框架，通过结合知识蒸馏和数据增强的方法来提高视觉模型的鲁棒性，并从预训练的基础模型中获得鲁棒教师模型的知识。借助离散对抗蒸馏方法，我们生成更有信息量的对抗样本，取得了在对抗性样本上鲁棒性显著提升的结果。此外，我们提供了理论框架来支持在知识蒸馏和数据增强设置中使用鲁棒教师模型，并展示了在不同学生模型上的显著性能提升。我们的方法在计算负载方面的开销较小，并可以与其他数据增强方法轻松结合。 |
| [^637] | [Vision-Language Foundation Models as Effective Robot Imitators.](http://arxiv.org/abs/2311.01378) | 该论文介绍了一种利用视觉语言基础模型进行机器人操作的方法，通过在语言条件的操作数据集上进行微调，实现了在低性能平台上的有效模仿控制。 |
| [^638] | [An energy-based comparative analysis of common approaches to text classification in the Legal domain.](http://arxiv.org/abs/2311.01256) | 本研究通过在法律领域的文本分类任务上进行比较分析，综合考虑性能和能源消耗等指标，探讨了大型语言模型与传统方法的优劣，并强调了在性能相近的情况下应重视生产成本、能源消耗和碳足迹等方面的考量。 |
| [^639] | [Diffusion Models for Reinforcement Learning: A Survey.](http://arxiv.org/abs/2311.01223) | 强化学习中的扩散模型已经成为一种突出的生成模型，通过在样本质量和训练稳定性方面的优势改进了强化学习解决方案。该综述提供了这一新兴领域发展的概述，并探讨了扩散模型在强化学习中的分类法和应用。 |
| [^640] | [Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents.](http://arxiv.org/abs/2310.19923) | Jina Embeddings 2是一个能够处理长篇文档的文本嵌入模型，突破了传统512个标记限制，提供了高达8192个标记的容量。 |
| [^641] | [Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach.](http://arxiv.org/abs/2310.17496) | 该论文提出了一种名为加权训练的方法，用于解决A/B测试中由数据训练循环引起的干扰。该方法通过训练模型预测每个数据点在实验组或控制组中出现的概率，并在模型训练过程中应用加权损失，从而实现了最小方差的估计结果，并且不会引起训练分布的变化。 |
| [^642] | [Graph Neural Networks with a Distribution of Parametrized Graphs.](http://arxiv.org/abs/2310.16401) | 这篇论文提出了一种使用参数化和生成多个图来解决图神经网络仅使用单个观察图的挑战的方法。在节点分类和图回归任务上，通过最大似然估计网络参数，在多个图上使用马尔可夫链蒙特卡洛方法，结合PAC-Bayesian理论的原则，取得了性能改进。 |
| [^643] | [On the Inherent Privacy Properties of Discrete Denoising Diffusion Models.](http://arxiv.org/abs/2310.15524) | 本研究探索了离散扩散模型在隐私保护方面的潜力，提供了关于训练数据集中每个数据点的隐私泄露的洞察，以及通过数据预处理减少合成数据集生成中隐私风险的方法。 |
| [^644] | [DoGE: Domain Reweighting with Generalization Estimation.](http://arxiv.org/abs/2310.15393) | DoGE提出了一种基于泛化估计的领域重新加权方法。通过使用梯度估计函数评估每个领域对泛化目标的贡献，重新调整了预训练数据中不同领域的采样概率。实验结果表明，该方法在提高大型语言模型的泛化能力方面取得了显著效果。 |
| [^645] | [Unsupervised Federated Learning: A Federated Gradient EM Algorithm for Heterogeneous Mixture Models with Robustness against Adversarial Attacks.](http://arxiv.org/abs/2310.15330) | 本文介绍了一种针对带有异构混合比例的混合模型的无监督学习的新型联邦梯度EM算法，在适用于普通混合模型的全面有限样本理论基础上，对高斯混合模型（GMM）和混合回归（MoRs）进行了具体的估计误差分析。该算法具有适应未知任务相似性、抵抗对少部分数据源的对抗攻击、保护本地数据隐私以及计算和通信效率等关键优势。 |
| [^646] | [RealFM: A Realistic Mechanism to Incentivize Data Contribution and Device Participation.](http://arxiv.org/abs/2310.13681) | RealFM 是一个真实的联邦机制，旨在解决现实环境中联邦学习中的驻泊者问题，通过模拟设备效用、激励数据贡献和设备参与，并提供了非线性关系的模型准确性和效用，从而改善了服务器和设备的效用和数据贡献。 |
| [^647] | [Equivariant Deep Weight Space Alignment.](http://arxiv.org/abs/2310.13397) | 本论文提出了一个名为Deep-Align的新框架，用于学习解决权重对齐问题，以加速对齐过程并提高其质量。 |
| [^648] | [Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes.](http://arxiv.org/abs/2310.11677) | 本论文提出了一种加速自然策略梯度算法（ANPG），用于解决无限时间无折扣奖励马尔可夫决策过程。ANPG实现了样本和迭代复杂度的显著改进，克服了现有算法的局限性，并达到了最新的技术成果。 |
| [^649] | [A decoder-only foundation model for time-series forecasting.](http://arxiv.org/abs/2310.10688) | 本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。 |
| [^650] | [Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis.](http://arxiv.org/abs/2310.10477) | 该论文介绍了一种基于错误分析的对齐策略，通过暴露大型语言模型的错误输出并进行评估，以理解内部原因。通过这种方法，有毒回应可以转化为模型对齐的指导调谐语料，从而提高模型的安全性并训练其进行自我批评。 |
| [^651] | [Mirage: Model-Agnostic Graph Distillation for Graph Classification.](http://arxiv.org/abs/2310.09486) | Mirage是一种模型无关的图蒸馏算法，旨在构建一个更小的合成训练集，以在资源有限的环境中实现图分类，并克服了现有算法对完整数据集的依赖性和对建模流程变化的敏感性。 |
| [^652] | [Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders.](http://arxiv.org/abs/2310.08164) | 通过使用稀疏自编码器，我们提出了一种解释RLHF调整的语言模型中学习的奖励函数的新方法。这个方法能够通过比较自编码器隐藏空间来识别学习奖励模型准确性的独特特征，并提供了奖励完整性的抽象近似值。 |
| [^653] | [A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks.](http://arxiv.org/abs/2310.07891) | 这篇论文提出了一种关于两层神经网络中非线性特征学习的理论。通过一步梯度下降训练的过程中引入不同的多项式特征，该方法能够学习到目标函数的非线性组件，而更新的神经网络的性能则由这些特征所决定。 |
| [^654] | [Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach.](http://arxiv.org/abs/2310.06112) | 本文理论分析了宽深度神经网络的鲁棒过拟合现象，并提出了一种名为Adv-NTK的AT算法来增强神经网络的鲁棒性。 |
| [^655] | [Entropy-MCMC: Sampling from Flat Basins with Ease.](http://arxiv.org/abs/2310.05401) | 本文提出了一种Entropy-MCMC的方法，通过引入一个辅助的引导变量来在平坦盆地中进行采样，以解决深度神经网络后验分布的多模态问题，并证明了该方法的收敛性。 |
| [^656] | [Uncovering hidden geometry in Transformers via disentangling position and context.](http://arxiv.org/abs/2310.04861) | 本文通过分解transformer的隐藏状态，揭示了其在语义理解中的隐含几何结构。 |
| [^657] | [Otago Exercises Monitoring for Older Adults by a Single IMU and Hierarchical Machine Learning Models.](http://arxiv.org/abs/2310.03512) | 本研究提出了一种使用单一IMU和分层机器学习模型监测年长者奥塔哥锻炼的准确系统。利用深度学习模型判断患者是在进行OEP还是日常生活活动，可以监测OEP的参与情况。 |
| [^658] | [Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing.](http://arxiv.org/abs/2310.03052) | Memoria 是一个通用记忆网络，应用海比安理论来增强神经网络中的长期依赖。通过存储和检索信息，并使用根据海布规则变化的连接权重，Memoria 在诸如 BERT 和 GPT 之类的流行 Transformer 模型上显著改进了考虑长期依赖的能力。 |
| [^659] | [Learning to Scale Logits for Temperature-Conditional GFlowNets.](http://arxiv.org/abs/2310.02823) | 这项研究提出了一种名为LSL-GFN的新型架构设计，可以大大加速温度条件下GFlowNets的训练，从而提高GFlowNets的探索和利用能力。 |
| [^660] | [Tackling Hybrid Heterogeneity on Federated Optimization via Gradient Diversity Maximization.](http://arxiv.org/abs/2310.02702) | 本文探讨了混合异构性如何影响联邦优化，并提出了一种通过最大化梯度多样性来减轻混合异构性负面影响的方法。 |
| [^661] | [Exploring Federated Optimization by Reducing Variance of Adaptive Unbiased Client Sampling.](http://arxiv.org/abs/2310.02698) | 本文通过减少自适应无偏客户采样方差，探索了联邦优化中的一系列自适应客户采样技术，并提出了一种名为K-Vib的新型采样器，显著提高了联邦学习性能。 |
| [^662] | [Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems.](http://arxiv.org/abs/2310.02299) | 本文介绍了一种用于建模3D物理系统的松弛八面体群卷积技术，它可以在保持数据一致的最高等变性水平的同时，发现物理系统中微妙的对称性破缺因素。 |
| [^663] | [DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training.](http://arxiv.org/abs/2310.02025) | DeepZero是一个扩展零阶优化到深度神经网络训练的深度学习框架，通过创新的坐标梯度估计和稀疏诱导的零阶训练协议，实现了高准确性和计算效率的优化。 |
| [^664] | [Effective and Parameter-Efficient Reusing Fine-Tuned Models.](http://arxiv.org/abs/2310.01886) | 本文提出了一种有效且参数高效的方法，可以重复使用微调模型来处理下游任务，减轻存储和服务负担，并提出了PERU-FFT方法用于重复使用全面微调模型。 |
| [^665] | [Combining Spatial and Temporal Abstraction in Planning for Better Generalization.](http://arxiv.org/abs/2310.00229) | Skipper是一个基于模型的强化学习代理，利用时空抽象来在新情境中推广学到的技能。它自动将任务分解为子任务，实现稀疏决策和对环境相关部分的专注计算。实验结果表明，Skipper在零样本泛化方面具有显著优势。 |
| [^666] | [ResBit: Residual Bit Vector for Categorical Values.](http://arxiv.org/abs/2309.17196) | 本论文提出了一种名为ResBit的残差位向量方法，用于解决在深度学习中表示离散数据维度增加和无法恢复原始类别值的问题。 |
| [^667] | [Controlling Continuous Relaxation for Combinatorial Optimization.](http://arxiv.org/abs/2309.16965) | 本文研究了在相对密集的图上组合优化问题中物理启发的图神经网络（PI-GNN）求解器的表现。通过数值实验，我们发现PI-GNN求解器在学习早期可能陷入所有变量为零的局部解。为了解决这个问题，我们通过控制连续性和离散性提出了一种改进方法。 |
| [^668] | [AtomSurf : Surface Representation for Learning on Protein Structures.](http://arxiv.org/abs/2309.16519) | 本文研究了将蛋白质作为3D网格的表面表示，并提出了一种结合图表面的协同方法，既有竞争优势，又有实际应用潜力。 |
| [^669] | [Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs.](http://arxiv.org/abs/2309.15395) | 本文提出了一种无模型的算法，名为PRI，用于在线CMDPs中的最佳策略识别问题。该算法基于CMDPs的有限随机性属性，能够以低遗憾并以高概率识别出最优策略。 |
| [^670] | [Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text Auxiliary tasks.](http://arxiv.org/abs/2309.07794) | 本研究通过引入图像-文本辅助任务，有效地提高了社交媒体帖子的多模态分类，通过两个辅助损失函数对图像-文本表示进行调整，捕捉底层依赖关系和语义对应关系，实现了一致的改进。 |
| [^671] | [Unsupervised Contrast-Consistent Ranking with Language Models.](http://arxiv.org/abs/2309.06991) | 无监督的对比一致排序与语言模型，通过训练一个受逻辑约束引导的探测模型，实现在多个语句中始终映射到对比的真-假极点的排序任务。 |
| [^672] | [Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models.](http://arxiv.org/abs/2309.06642) | 本文提出了一种自适应扩散方法，通过潜在扩散模型实现样本自适应重建。该方法解决了现有求解器在适应重建任务困难程度、推理时间和资源分配方面的不足。 |
| [^673] | [Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models.](http://arxiv.org/abs/2309.06256) | 本研究实证了基础模型微调中的灾难性遗忘现象，微调过程中追求专业性会导致模型的广泛性损失。 |
| [^674] | [Data efficiency, dimensionality reduction, and the generalized symmetric information bottleneck.](http://arxiv.org/abs/2309.05649) | 广义对称信息瓶颈是一种同时压缩两个随机变量以保留信息的维度约简技术，相较于逐个压缩变量，它需要更少的数据来达到相同的误差。 |
| [^675] | [Everything, Everywhere All in One Evaluation: Using Multiverse Analysis to Evaluate the Influence of Model Design Decisions on Algorithmic Fairness.](http://arxiv.org/abs/2308.16681) | 通过多元宇宙分析评估模型设计决策对算法公平性的影响，可以揭示算法决策系统中设计决策的关键作用。 |
| [^676] | [Matbench Discovery -- An evaluation framework for machine learning crystal stability prediction.](http://arxiv.org/abs/2308.14920) | Matbench Discovery是一个评估机器学习晶体稳定性预测的框架，在热力学稳定性预测方面的测试中，CHGNet表现最佳。 |
| [^677] | [Fine-tuning can cripple your foundation model; preserving features may be the solution.](http://arxiv.org/abs/2308.13320) | 在微调过程中，基础模型可能会遗忘概念，我们提出了一种名为LDIFS的方法，用于解决这个问题，该方法在实验证明效果显著。 |
| [^678] | [Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature.](http://arxiv.org/abs/2308.12420) | 本研究通过NLP分析了ESG主导的DLT研究的演化，通过构建引用网络和命名实体识别任务，对DLT在ESG背景下的发展进行了文献综述。 |
| [^679] | [How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy.](http://arxiv.org/abs/2308.12252) | 本文提出了一种基于生成世界模型的学习流水线族，通过克服学习安全知情表示和分布漂移下缺失安全标签的挑战，实现了在线安全预测。这些流水线具有统计校准保证的安全机会预测能力。 |
| [^680] | [Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances.](http://arxiv.org/abs/2308.11129) | 本论文提出了一种层次距离结构编码（HDSE）方法，用于捕捉多层次图结构。经过在12个真实世界数据集上的实验，证明了该方法在10个基准数据集上实验效果达到了最先进水平。 |
| [^681] | [Natural Language is All a Graph Needs.](http://arxiv.org/abs/2308.07134) | 本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。 |
| [^682] | [Bayesian Flow Networks.](http://arxiv.org/abs/2308.07037) | 本文介绍了贝叶斯流网络（BFNs），一种新的生成模型，它通过贝叶斯推断修改了一组独立分布的参数，并将其作为输入传递给神经网络来生成另一个相互依赖的分布。该方法不需要前向过程，适用于连续和离散数据，并具有优化数据压缩的功能。 |
| [^683] | [Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale.](http://arxiv.org/abs/2308.02068) | 本研究利用日常抓取的1,404个不可靠新闻网站以及大型语言模型和聚类算法，提出了一个自动分析网络生态系统中传播的叙述的系统。通过识别55,301个叙述，描述了2022年传播最广泛的叙述，并确定了最具影响力的起源和放大叙述的网站。该系统可用于检测来自不可靠新闻网站的新叙述，并帮助事实核查组织更快地应对错误信息。 |
| [^684] | [Calibration in Deep Learning: A Survey of the State-of-the-Art.](http://arxiv.org/abs/2308.01222) | 本文回顾了深度学习中的校准方法的最新发展，并提供了对其原理的理解。研究表明，现代深度神经网络在预测能力上表现出色，但校准性较差，导致模型预测不可靠。因此，需要一些新的方法来改善模型的校准性。 |
| [^685] | [Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting.](http://arxiv.org/abs/2307.15299) | 本研究使用差分进化算法选择Transformer神经网络模型的优化超参数，以提高负荷预测的准确性。 |
| [^686] | [Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework.](http://arxiv.org/abs/2307.13147) | 该论文研究了将路径相关的NJ-ODE方法扩展到具有噪声观测和相关观测框架的问题。研究提出了两种扩展方法，并提供了理论保证和实证示例。 |
| [^687] | [Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques.](http://arxiv.org/abs/2307.12971) | 本文介绍了一种新的大数据-供应链管理框架，通过数据预处理和机器学习技术实现供应链预测，优化操作管理、透明度，并讨论了幻影库存对预测的不利影响。 |
| [^688] | [Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture.](http://arxiv.org/abs/2307.10843) | 本文提出了一种基于U-Net和卷积LSTM的深度学习架构，能够准确预测全球降水的即时情况，尤其在极端降水方面的预测准确率更高。 |
| [^689] | [HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning.](http://arxiv.org/abs/2307.09653) | HAT-CL是一个基于任务的硬注意力PyTorch库，以提供对连续学习中的灾难性遗忘现象的解决方案。它通过改善HAT的可用性和兼容性问题，并提供对现有网络复用的支持，实现了对PyTorch模块的自动化梯度操作和转换。此外，HAT-CL还引入了新颖的掩码操作技术。 |
| [^690] | [Optimal Compression of Unit Norm Vectors in the High Distortion Regime.](http://arxiv.org/abs/2307.07941) | 本研究探讨了在高失真情况下，将单位范数向量压缩到最少比特数的最优方法。研究发现，简单的压缩方案在这种情况下几乎是最优的。 |
| [^691] | [Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes.](http://arxiv.org/abs/2307.07604) | 本论文提出了一种通过填充和置换指纹编码的方法来产生困难实例，从而在各种情景下提供平滑下界。这方法适用于差分隐私平均问题和近似k. |
| [^692] | [DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding.](http://arxiv.org/abs/2307.06924) | DRAGON是一种基于对话的导航机器人，能够理解用户的指令并通过语言与用户沟通，为视力受损者提供导航和环境描述的帮助。 |
| [^693] | [Provably Faster Gradient Descent via Long Steps.](http://arxiv.org/abs/2307.06324) | 本研究通过计算机辅助分析技术，证明了非常数步长策略下的梯度下降方法经过长距离步骤可以实现更快的收敛速度。 |
| [^694] | [SITTA: A Semantic Image-Text Alignment for Image Captioning.](http://arxiv.org/abs/2307.05591) | SITTA是一种用于图像描述的语义图像文本对齐方法，通过构建线性映射成功地将多模态模型和语言模型的嵌入空间对齐，实现了丰富的语言能力和良好的图像-语言映射。 |
| [^695] | [Fast Empirical Scenarios.](http://arxiv.org/abs/2307.03927) | 该论文提出了两种快速的经验场景提取算法，一种识别之前未观察到的场景并提供场景的协方差矩阵表示，另一种从已实现的世界状态中选择重要的数据点，并与高阶样本矩一致，这些算法计算效率高且适用于一致的基于场景的建模和高维数值积分。 |
| [^696] | [How to Detect Unauthorized Data Usages in Text-to-image Diffusion Models.](http://arxiv.org/abs/2307.03108) | 本文提出了一种方法，通过在训练的文本到图像扩散模型中植入注入的记忆化内容，来检测未授权数据使用。该方法修改了受保护的图像数据集，添加了对人眼不可察觉但模型可以捕捉和记忆的内容，通过分析模型对注入内容的记忆来判断模型是否存在生成类似图像的能力。 |
| [^697] | [Optimizing protein fitness using Gibbs sampling with Graph-based Smoothing.](http://arxiv.org/abs/2307.00494) | 使用基于图形平滑的Gibbs采样方法（GGS）优化蛋白质适应性，消除了突变距离的限制，同时提高了搜索效率。该方法在发现高适应性蛋白质方面达到了最先进水平。 |
| [^698] | [Enhancing Adversarial Training via Reweighting Optimization Trajectory.](http://arxiv.org/abs/2306.14275) | 本文提出了一种名为“加权优化轨迹（WOT）”的新方法，通过优化历史轨迹，解决了对抗训练中的鲁棒泛化问题。 |
| [^699] | [TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support.](http://arxiv.org/abs/2306.13339) | TrustGuard是一种基于GNN的信任评估模型，支持信任动态性，抗击鲁棒并提供解释能力，它的实验结果在准确性、鲁棒性和可解释性方面都优于其他方法。 |
| [^700] | [Towards Quantum Federated Learning.](http://arxiv.org/abs/2306.09912) | 量子联邦学习通过将量子计算和联邦学习原理相结合，旨在利用量子技术增强学习过程中的隐私、安全和效率，并通过独特的分类法分类总结了这一快速发展领域的技术特点和未来研究方向。 |
| [^701] | [Differentially Private Domain Adaptation with Theoretical Guarantees.](http://arxiv.org/abs/2306.08838) | 该论文提出了两种具有差分隐私保障的自适应算法，用于在受隐私约束且有限标记数据条件下，从公开源领域到目标领域进行监督域自适应。该算法能够解决一般的优化问题，并具有有利的理论学习保证。 |
| [^702] | [Analysis and Approximate Inference of Large and Dense Random Kronecker Graphs.](http://arxiv.org/abs/2306.08489) | 本文对大规模密集随机Kronecker图进行了分析和近似推断，提出了“去噪声和求解”元算法，用于近似推断图参数，并具有较低的计算复杂度和性能保证。 |
| [^703] | [SqueezeLLM: Dense-and-Sparse Quantization.](http://arxiv.org/abs/2306.07629) | 本文提出了一种基于训练后的量化框架——SqueezeLLM，它不仅可以实现高达3位的无损压缩，而且在相同的内存约束下实现更高的量化性能。 |
| [^704] | [Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural Surface Rendering.](http://arxiv.org/abs/2306.07392) | 通过神经表面渲染，NeuGraspNet能够在混乱场景中有效地从任意视角预测6DoF抓取质量，并能够在遮挡的场景中采样抓取候选项。 |
| [^705] | [Does Long-Term Series Forecasting Need Complex Attention and Extra Long Inputs?.](http://arxiv.org/abs/2306.05035) | 本论文介绍了一种新的轻量级周期-注意机制，名为Periodformer，解决了长期序列预测中的两个主要问题，并证明了Transformer-based方法不需要额外长的输入序列来保证性能。 |
| [^706] | [Layer-level activation mechanism.](http://arxiv.org/abs/2306.04940) | 去噪声更好，表现更好的分层级别激活机制 |
| [^707] | [Exploiting Observation Bias to Improve Matrix Completion.](http://arxiv.org/abs/2306.04775) | 本研究利用观测偏差来改进矩阵补全问题，提出一个简单的两阶段算法，实现了与对未观测协变量的监督学习性能相当的结果。 |
| [^708] | [Optimal Fair Multi-Agent Bandits.](http://arxiv.org/abs/2306.04498) | 本文针对多智能体之间公平多臂赌博机学习问题提出了一种算法，通过分布式拍卖算法学习样本最优匹配，使用一种新的利用阶段和一种基于顺序统计的遗憾分析实现，相较于先前的结果遗憾阶数从$O(\log T \log\log T)$到了$O\left(N^3 \log N \log T \right)$，能够更好地处理多个智能体之间的依赖关系。 |
| [^709] | [How Can We Train Deep Learning Models Across Clouds and Continents? An Experimental Study.](http://arxiv.org/abs/2306.03163) | 本文通过实验研究，探究了在不同大陆、云供应商和数据中心范围内，使用分布式数据并行点深度学习训练是否是更具成本效益的选择，并比较了其与集中式训练的可扩展性潜力。 |
| [^710] | [Solving NP-hard Min-max Routing Problems as Sequential Generation with Equity Context.](http://arxiv.org/abs/2306.02689) | 本文提出了一个新的深度学习框架Equity-Transformer来解决大规模的最小最大路径问题。该模型利用可扩展的深度学习模型进行顺序决策，并生成考虑公平工作负载的顺序动作。研究显示，Equity-Transformer在两个代表性最小最大路径问题中具有卓越的性能。 |
| [^711] | [On Size-Independent Sample Complexity of ReLU Networks.](http://arxiv.org/abs/2306.01992) | 本文研究了ReLU神经网络的样本复杂度，给出了一个现有方法精细化的结果，实现了无深度依赖性的上界。 |
| [^712] | [Symmetric Exploration in Combinatorial Optimization is Free!.](http://arxiv.org/abs/2306.01276) | 该论文提出了一种免费的技术，通过利用对称性提高了基于DRL的组合优化求解器的性能，无需额外的目标函数评估，适用于广泛的组合优化任务，并在多种任务上进行实证评估证实了其有效性。 |
| [^713] | [Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training.](http://arxiv.org/abs/2306.01271) | 对抗训练是训练深度神经网络抗击对抗扰动的标准方法, 其学习机制导致干净泛化和强健过拟合现象同时发生。 |
| [^714] | [STEVE-1: A Generative Model for Text-to-Behavior in Minecraft.](http://arxiv.org/abs/2306.00937) | STEVE-1 是一种新的生成模型，能够在Minecraft中跟随各种短期开放型文本和视觉指令。STEVE-1利用预先训练的模型和最佳实践，通过自监督的行为克隆和回顾重新标记来微调，避免了昂贵的人工注释。 |
| [^715] | [Test-Time Training on Nearest Neighbors for Large Language Models.](http://arxiv.org/abs/2305.18466) | 该论文提出了一种基于最近邻的测试时间训练方法，通过检索和微调少量邻居的文本数据，该方法在大语言模型上显著提高了性能。 |
| [^716] | [Cross-Entropy Estimators for Sequential Experiment Design with Reinforcement Learning.](http://arxiv.org/abs/2305.18435) | 这篇论文提出了一个基于交叉熵估计器的备选下界估计方法，这个方法不需要对比样本，可以更精确地估计高信息增益，允许学习更优秀的设计策略，并且与隐式概率模型兼容。 |
| [^717] | [Kernel-SSL: Kernel KL Divergence for Self-supervised Learning.](http://arxiv.org/abs/2305.17326) | 本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。 |
| [^718] | [Laplace-Approximated Neural Additive Models: Improving Interpretability with Bayesian Inference.](http://arxiv.org/abs/2305.16905) | 本文提出了拉普拉斯逼近神经加性模型，该模型从贝叶斯角度考虑加性结构，在恢复的特征交互中提供可信区间，提供可处理的边缘似然估计，可用于执行隐式特征选择并对特征对进行排名。 |
| [^719] | [Neural incomplete factorization: learning preconditioners for the conjugate gradient method.](http://arxiv.org/abs/2305.16368) | 本文提出了一种名为神经不完全分解的新方法，利用自监督训练的图神经网络生成适用于特定问题域的有效预处理器。其通过替换传统手工预处理器显着提高了收敛和计算效率，在合成和真实问题上进行的实验均表现出竞争力。 |
| [^720] | [A Rational Model of Dimension-reduced Human Categorization.](http://arxiv.org/abs/2305.14383) | 提出了一个基于层次化混合模型的分类模型，以及基于生成过程的低维潜在空间的分类解释，该模型学习类别表示和特征集合，适用于高维刺激下，支持零-shot学习，并验证了该模型。 |
| [^721] | [Relabel Minimal Training Subset to Flip a Prediction.](http://arxiv.org/abs/2305.12809) | 本文利用扩展影响函数提出了一种有效的识别和重新标记最小训练子集的方法，并证明其始终能够成功翻转测试结果，同时还提供了挑战模型预测、评估模型鲁棒性和洞察训练集偏差等多重作用。 |
| [^722] | [Q-malizing flow and infinitesimal density ratio estimation.](http://arxiv.org/abs/2305.11857) | 研究提出了一种可以从一个数据分布P传输到任意访问通过有限样本的Q的流模型。这个模型通过神经ODE模型进行，可以进行无穷小DRE。 |
| [^723] | [State Representation Learning Using an Unbalanced Atlas.](http://arxiv.org/abs/2305.10267) | 本文介绍了一种使用不平衡图册（UA）方法的状态表示学习，该方法可以超越最先进的自监督学习方法。 |
| [^724] | [Smaller Language Models are Better Black-box Machine-Generated Text Detectors.](http://arxiv.org/abs/2305.09859) | 本文研究发现，小型语言模型更适用于作为通用文本检测器，可以更加精确地检测出机器生成的文本，而检测器和生成模型是否具有相同的架构或语料库并不会对检测性能产生显著影响。 |
| [^725] | [Heterogeneous Directed Hypergraph Neural Network over abstract syntax tree (AST) for Code Classification.](http://arxiv.org/abs/2305.04228) | 本研究提出了使用异构有向超图表示AST，并使用异构有向超图神经网络处理图形进行代码分类，超过了现有方法。 |
| [^726] | [High-dimensional Bayesian Optimization via Semi-supervised Learning with Optimized Unlabeled Data Sampling.](http://arxiv.org/abs/2305.02614) | 本文提出基于半监督学习的高维贝叶斯优化方法，利用特定的未标记数据采样、参数化采样分布的优化及动态选择无标记数据等策略，解决了高维贝叶斯优化难以处理的问题。 |
| [^727] | [Controlling Posterior Collapse by an Inverse Lipschitz Constraint on the Decoder Network.](http://arxiv.org/abs/2304.12770) | 本文提出了一种基于反Lipschitz约束的解码器网络，可以简单明了地控制广泛的VAE模型的后验坍塌程度，并带有具体的理论保证。 |
| [^728] | [Pointwise convergence theorem of gradient descent in sparse deep neural network.](http://arxiv.org/abs/2304.08172) | 本文研究了稀疏深度神经网络中梯度下降的点对点收敛定理，针对非光滑指示函数构造了一种特殊形状的DNN，实现了梯度下降过程的点对点收敛。 |
| [^729] | [SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM.](http://arxiv.org/abs/2304.05622) | 介绍了Segment Any Medical Model (SAMM)，它是用于3D Slicer的SAM的扩展。SAMM在医学图像分割上表现良好，在实时性和通用性方面都有很好的性能，可以推断出掩模。 |
| [^730] | [GLADE: Gradient Loss Augmented Degradation Enhancement for Unpaired Super-Resolution of Anisotropic MRI.](http://arxiv.org/abs/2303.11831) | 本文提出了一种可用于加速全腹MRI扫描的新方法GLADE，它通过使用梯度映射损失来合成高分辨率等向性3D腹部MR图像。 |
| [^731] | [Operating critical machine learning models in resource constrained regimes.](http://arxiv.org/abs/2303.10181) | 本文分享了在关键场景下使用机器学习模型时资源消耗和性能之间的权衡方法。考虑到模型在全球诊所中的部署，机器学习界正在为改进模型效率而努力。 |
| [^732] | [Mixed Traffic Control and Coordination from Pixels.](http://arxiv.org/abs/2302.09167) | 本研究考虑利用图像观察作为替代方法来进行混合交通控制。 |
| [^733] | [Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data.](http://arxiv.org/abs/2302.06279) | 本文研究使用神经形态数据和多样化的刺激在脉冲神经网络中的后门攻击问题。 |
| [^734] | [Graph Generation with Destination-Predicting Diffusion Mixture.](http://arxiv.org/abs/2302.03596) | 本文介绍了一种名为目标预测扩散混合的方法，用于解决传统扩散模型不能很好地建模图拓扑结构的问题，并在图生成任务上取得了最先进的性能表现。 |
| [^735] | [FedEBA+: Towards Fair and Effective Federated Learning via Entropy-Based Model.](http://arxiv.org/abs/2301.12407) | FedEBA+是一种新的联邦学习算法，它采用公平聚合方案和对齐更新方法，在同时提高全局模型性能的同时提高公平性。实验证明FedEBA+优于其他公平性联邦学习方法。 |
| [^736] | [Efficient Video Representation Learning via Motion-Aware Token Selection.](http://arxiv.org/abs/2211.10636) | 该论文提出了一种新的运动感知标记选择方法，针对视频中不同补丁的信息密度，选择包含丰富动态特性的标记，放弃无效的标记，从而大大降低计算和存储需求，实现了在单台机器上进行预训练和微调而不影响性能。 |
| [^737] | [Dive into Machine Learning Algorithms for Influenza Virus Host Prediction with Hemagglutinin Sequences.](http://arxiv.org/abs/2207.13842) | 本研究使用机器学习算法，以血凝素序列为基础，利用位置特异性评分矩阵和词嵌入的方法，通过多种评估指标在不同分类水平上评估了机器学习算法，并发现5-grams-transformer神经网络是最有效的算法，可以准确预测流感病毒序列的起源。 |

# 详细

[^1]: 用Vabs-Net进行多级蛋白质预训练

    Multi-level protein pre-training with Vabs-Net

    [https://rss.arxiv.org/abs/2402.01481](https://rss.arxiv.org/abs/2402.01481)

    这篇论文介绍了一种使用Vabs-Net进行多级蛋白质预训练的方法。当前大多数基于结构的预训练模型仅关注残基水平，但忽略了侧链原子的重要性。为了解决这个问题，论文提出了一种新的预训练策略，引入了跨度掩码，以在三维蛋白质预训练中同时建模残基和原子水平的信息，并改善了残基表示的表达能力。

    

    最近几年，三维结构预训练蛋白质模型的发展迅猛，相较于预训练蛋白质语言模型，在各种下游任务中取得了重大进展。然而，大多数现有的基于结构的预训练模型主要关注残基水平，即α碳原子，而忽略了其他原子，如侧链原子。我们认为，在残基和原子水平上对蛋白质进行建模很重要，因为侧链原子对于许多下游任务（如分子对接）也是至关重要的。然而，我们发现在预训练中天真地组合残基和原子信息通常会失败。我们发现，信息泄漏是包含原子结构的输入导致残基级预训练任务变得琐碎并导致残基表示不够充分的一个关键原因。为了解决这个问题，我们引入了一种基于跨度掩码预训练策略的三维蛋白质预训练方法。

    In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein
    
[^2]: 通过Lipschitz正则化在规模上实现零样本机器遗忘

    Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization

    [https://rss.arxiv.org/abs/2402.01401](https://rss.arxiv.org/abs/2402.01401)

    通过Lipschitz正则化实现零样本机器遗忘，可以及时忘记私人或受版权保护的信息，同时保持模型性能。

    

    为了遵守人工智能和数据规定，从训练得到的机器学习模型中遗忘私人或受版权保护的信息的需求变得越来越重要。遗忘的关键挑战是及时忘记必要的数据，同时保持模型性能。在这项工作中，我们解决了零样本遗忘的场景，即只有一个经过训练的模型和要遗忘的数据，遗忘算法必须能够移除数据。根据这样定义，现有的最先进的方法是不够的。基于Lipschitz连续性的概念，我们提出了一种方法，通过对样本扰动的输出进行平滑处理来诱导遗忘。我们展示了这种平滑性成功地实现了遗忘，同时保持了总体模型性能。我们对我们的方法进行了广泛的经验评估，包括一系列当代基准测试，验证了我们的方法在严格的零样本约束下达到了最先进的性能。

    To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. Under such a definition, existing state-of-the-art methods are insufficient. Building on the concepts of Lipschitz continuity, we present a method that induces smoothing of the forget sample's output, with respect to perturbations of that sample. We show this smoothing successfully results in forgetting while preserving general model performance. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method achieves state-of-the-art performance under the strict constraints of ze
    
[^3]: LoTR: 低张量秩权重自适应

    LoTR: Low Tensor Rank Weight Adaptation

    [https://rss.arxiv.org/abs/2402.01376](https://rss.arxiv.org/abs/2402.01376)

    LoTR是一种新颖的语言模型参数调优方法，通过引入低秩张量表示和张量分解，使得针对深层模型的参数效率更高，并且能够实现廉价且快速的下游调优。

    

    在本文中，我们将大型语言模型（LLM）上的低秩适应（LoRA）思想推广和扩展，这些模型基于Transformer架构。广泛使用的LoRA类方法是基于梯度更新的矩阵分解。我们引入了LoTR，一种新颖的LLM参数高效调优方法，它以张量分解的形式表示参数的梯度更新。每个层的低秩适配器都由三个矩阵的乘积构成，而张量结构是由这个乘积的左右乘子在层之间共享引起的。通过对低秩张量表示的一系列层同时压缩，LoTR能够比LoRA在特别是对于深层模型具有更好的参数效率。此外，核心张量不依赖于原始权重维度，可以任意缩小，从而实现非常廉价和快速的下游调优。

    In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture. Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update. We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition. Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers. Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models. Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning.
    
[^4]: 在分布变化中的监督算法公平性：一项综述

    Supervised Algorithmic Fairness in Distribution Shifts: A Survey

    [https://rss.arxiv.org/abs/2402.01327](https://rss.arxiv.org/abs/2402.01327)

    这篇综述研究了分布变化下的监督公平机器学习领域，调查了各种类型的分布变化和现有的解决方法，并列举了公开数据集和评估指标。研究发现六种常用的方法，并探讨了与相关研究领域的交互关系。

    

    在分布变化下的监督公平机器学习是一个新兴领域，解决了面对从源领域到目标领域的数据分布变化时，如何保持公平和无偏预测的挑战。在现实世界的应用中，机器学习模型通常是在特定数据集上进行训练，但在部署时，数据分布可能因各种因素而随时间发生变化。这种变化可能导致不公平的预测，对特定通过敏感属性（如种族和性别）来表征的群体产生不均衡的影响。在这项调查中，我们对各种类型的分布变化进行了总结，并全面调查了基于这些变化的现有方法，在文献中突出了六种常用的方法。此外，这份调查列出了用于实证研究的公开可用数据集和评估指标。我们进一步探讨了与相关研究领域的交互关系，并讨论了其中的重要创新和贡献。

    Supervised fairness-aware machine learning under distribution shifts is an emerging field that addresses the challenge of maintaining equitable and unbiased predictions when faced with changes in data distributions from source to target domains. In real-world applications, machine learning models are often trained on a specific dataset but deployed in environments where the data distribution may shift over time due to various factors. This shift can lead to unfair predictions, disproportionately affecting certain groups characterized by sensitive attributes, such as race and gender. In this survey, we provide a summary of various types of distribution shifts and comprehensively investigate existing methods based on these shifts, highlighting six commonly used approaches in the literature. Additionally, this survey lists publicly available datasets and evaluation metrics for empirical studies. We further explore the interconnection with related research fields, discuss the significant c
    
[^5]: 通过特征谱表征核岭回归的过拟合

    Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum

    [https://rss.arxiv.org/abs/2402.01297](https://rss.arxiv.org/abs/2402.01297)

    我们通过推导核矩阵的特征数界限，增强了核岭回归的测试误差界限。对于多项式谱衰减的核，我们恢复了先前的结果；对于指数谱衰减，我们提出了新的非平凡的界限。我们的研究表明，特征谱衰减多项式的核回归器具有良好的泛化能力，而特征谱指数衰减的核回归器则具有灾难性的过拟合。

    

    我们推导了核矩阵的条件数的新界限，然后利用这些界限增强了在固定输入维度的过参数化区域中核岭回归的现有非渐近测试误差界限。对于具有多项式谱衰减的核，我们恢复了先前工作的界限；对于指数衰减，我们的界限是非平凡和新颖的。我们对过拟合的结论是双重的：(i) 谱衰减多项式的核回归器必须在存在噪声标记的训练数据的情况下得到很好的泛化；这些模型表现出所谓的温和过拟合；(ii) 如果任何核岭回归器的特征谱指数衰减，则其泛化差，即表现出灾难性过拟合。这增加了核岭回归器表现出良性过拟合的可用特征谱衰减次多项式的极端情况的表征。我们的分析结合了新的随机矩阵理论(RMT)。

    We derive new bounds for the condition number of kernel matrices, which we then use to enhance existing non-asymptotic test error bounds for kernel ridgeless regression in the over-parameterized regime for a fixed input dimension. For kernels with polynomial spectral decay, we recover the bound from previous work; for exponential decay, our bound is non-trivial and novel.   Our conclusion on overfitting is two-fold: (i) kernel regressors whose eigenspectrum decays polynomially must generalize well, even in the presence of noisy labeled training data; these models exhibit so-called tempered overfitting; (ii) if the eigenspectrum of any kernel ridge regressor decays exponentially, then it generalizes poorly, i.e., it exhibits catastrophic overfitting. This adds to the available characterization of kernel ridge regressors exhibiting benign overfitting as the extremal case where the eigenspectrum of the kernel decays sub-polynomially. Our analysis combines new random matrix theory (RMT) te
    
[^6]: 级联缩放分类器：通过概率缩放进行类别增量学习

    Cascaded Scaling Classifier: class incremental learning with probability scaling

    [https://rss.arxiv.org/abs/2402.01262](https://rss.arxiv.org/abs/2402.01262)

    提出了级联缩放分类器，结合边际抑制和知识蒸馏方法，用于实现神经网络中的连续学习，并降低过去任务的遗忘。

    

    人类有能力获取新知识并将学习到的知识转移到不同的领域，仅有轻微的遗忘。同样的能力，在神经网络中实现连续学习是具有挑战性的，因为在学习新任务时会影响到过去学习的任务。这种遗忘可以通过回放存储的过去任务样本来缓解，但是对于长序列任务可能需要较大的存储空间；此外，这可能导致对保存样本的过拟合。在本文中，我们提出了一种新颖的正则化方法和一种新颖的增量分类器，分别称为边际抑制和级联缩放分类器。前者结合了软约束和知识蒸馏方法，以保留过去学习的知识同时有效地学习新的模式。后者是一种带有门控的增量分类器，帮助模型修改过去的预测而不直接干扰它们。这是通过...

    Humans are capable of acquiring new knowledge and transferring learned knowledge into different domains, incurring a small forgetting. The same ability, called Continual Learning, is challenging to achieve when operating with neural networks due to the forgetting affecting past learned tasks when learning new ones. This forgetting can be mitigated by replaying stored samples from past tasks, but a large memory size may be needed for long sequences of tasks; moreover, this could lead to overfitting on saved samples. In this paper, we propose a novel regularisation approach and a novel incremental classifier called, respectively, Margin Dampening and Cascaded Scaling Classifier. The first combines a soft constraint and a knowledge distillation approach to preserve past learned knowledge while allowing the model to learn new patterns effectively. The latter is a gated incremental classifier, helping the model modify past predictions without directly interfering with them. This is achieved
    
[^7]: 使用大型语言模型的高效因果图发现

    Efficient Causal Graph Discovery Using Large Language Models

    [https://rss.arxiv.org/abs/2402.01207](https://rss.arxiv.org/abs/2402.01207)

    提出了一个新的框架，利用大型语言模型进行高效的因果图发现，采用了广度优先搜索方法，只需要线性数量的查询，同时能轻松结合观察数据以提高性能，具有高效性和数据效率，并在真实因果图上取得了最先进的结果，展示了其在不同领域的广泛适用性潜力。

    

    我们提出了一个新的框架，利用LLMs进行完整的因果图发现。之前基于LLM的方法采用了成对查询的方法，但这需要二次查询的数量，对于较大的因果图来说很快变得不可行。相反，提出的框架采用了广度优先搜索（BFS）的方法，只需要线性数量的查询。我们还展示了当有所观察数据可用时，提出的方法可以轻松地进行结合以提高性能。除了更具时间和数据效率外，提出的框架在不同大小的真实因果图上取得了最先进的结果。结果证明了提出方法在发现因果关系方面的有效性和效率，展示了其在不同领域的因果图发现任务中的广泛适用性潜力。

    We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.
    
[^8]: 自监督学习在非连续表格数据中的应用调研

    A Survey on Self-Supervised Learning for Non-Sequential Tabular Data

    [https://rss.arxiv.org/abs/2402.01204](https://rss.arxiv.org/abs/2402.01204)

    本调研总结了自监督学习在非连续表格数据中的最新进展和挑战，将其方法分为预测性学习、对比学习和混合学习，讨论了应用问题，包括自动化数据工程、跨表格查询和隐私保护等。

    

    自监督学习（SSL）已经被应用于各个领域的许多最先进的模型中，其中SSL通过定义基于无标签数据集的预训练任务来学习上下文化和鲁棒的表示。最近，SSL已成为探索表格数据领域中表示学习能力的新趋势，这是一项更具挑战性的任务，因为它没有明确的关系来学习描述性的表示。本调研旨在系统地回顾和总结自监督学习在非连续表格数据（SSL4NS-TD）中的最新进展和挑战。首先，我们给出了NS-TD的正式定义，并阐明了它与相关研究的关联。然后，这些方法被分为三组——预测性学习、对比学习和混合学习，并介绍了每个方向的代表性方法的动机和优点。在此基础上，还介绍了SSL4NS-TD的应用问题，包括自动化数据工程、跨表格查询和隐私保护等。

    Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has been a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups -- predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods within each direction. On top of this, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table
    
[^9]: 深度补全的测试时间自适应

    Test-Time Adaptation for Depth Completion

    [https://arxiv.org/abs/2402.03312](https://arxiv.org/abs/2402.03312)

    该论文提出了一种在线测试时间自适应方法，用于深度补全任务，通过在单次通过中缩小源数据和目标数据间的领域差距，提高模型性能。

    

    当将在一些（源）数据集上训练的模型转移到目标测试数据时，常常会观察到性能下降，这是由于它们之间存在领域差距。现有的用于弥合这一差距的方法，如领域适应（DA），可能需要模型训练时使用的源数据（通常不可用），而其他方法，如无源DA，则需要多次通过测试数据。我们提出了一种在线测试时间自适应方法，用于深度补全，即从单个图像和相关的稀疏深度图推断出密集深度图的任务，以在一次通过中缩小性能差距。首先，我们对每种数据模态中的领域转移如何影响模型性能进行了研究。根据我们的观察，稀疏深度模态展现出比图像更小的协变量转移，因此我们设计了一个在源领域中训练的嵌入模块，它保留了从仅编码稀疏深度特征到编码图像和稀疏深度的特征的映射。在测试时间，我们使用这个嵌入模块实现自适应。

    It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them. Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass. We first present a study on how the domain shift in each data modality affects model performance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During t
    
[^10]: HASSOD：分层自适应无监督目标检测

    HASSOD: Hierarchical Adaptive Self-Supervised Object Detection

    [https://arxiv.org/abs/2402.03311](https://arxiv.org/abs/2402.03311)

    HASSOD是一种分层自适应无监督目标检测系统，通过自监督学习，提高了检测性能和可解释性。

    

    人类视觉感知系统展示了在学习时没有明确监督的情况下以及理解物体的部分整体组成方面的卓越能力。受到这两个能力的启发，我们提出了一种新颖的方法——分层自适应无监督目标检测(HASSOD)，该方法学习在没有人类监督的情况下检测物体并理解其组成。HASSOD采用分层自适应聚类策略，根据自监督的视觉表示将区域分组为对象掩码，自适应确定每个图像中的对象数量。此外，HASSOD通过分析掩码之间的覆盖关系和构建树结构来识别对象的层次级别。这种额外的自监督学习任务可以提高检测性能并增强可解释性。最后，我们放弃了之前方法中使用的低效的多轮自我训练过程，而采取了一种自我训练的方法，该方法在时间和计算资源上更高效。

    The human visual perception system demonstrates exceptional capabilities in learning without explicit supervision and understanding the part-to-whole composition of objects. Drawing inspiration from these two abilities, we propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a novel approach that learns to detect objects and understand their compositions without human supervision. HASSOD employs a hierarchical adaptive clustering strategy to group regions into object masks based on self-supervised visual representations, adaptively determining the number of objects per image. Furthermore, HASSOD identifies the hierarchical levels of objects in terms of composition, by analyzing coverage relations between masks and constructing tree structures. This additional self-supervised learning task leads to improved detection performance and enhanced interpretability. Lastly, we abandon the inefficient multi-round self-training process utilized in prior methods and instead ad
    
[^11]: AONeuS: 一种用于声光传感器融合的神经渲染框架

    AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion

    [https://arxiv.org/abs/2402.03309](https://arxiv.org/abs/2402.03309)

    AONeuS是一种基于物理的多模态声光神经表面重建框架，通过融合高分辨率RGB测量和低分辨率深度成像声纳测量，能够在受限基线下实现准确的高分辨率三维表面重建。

    

    水下感知和三维表面重建是具有广泛应用的挑战性问题，涉及建筑、安全、海洋考古和环境监测等领域。恶劣的操作条件、脆弱的环境和有限的导航控制通常导致水下航行器限制其运动范围和测量基线。在三维场景重建的背景下，我们知道较小的基线会增加重建难度。本文开发了一种基于物理的多模态声光神经表面重建框架（AONeuS），能够有效地将高分辨率RGB测量与低分辨率深度成像声纳测量进行融合。通过融合这些互补的模态，我们的框架可以从在受限基线上捕获的测量中重建出准确的高分辨率三维表面。通过大量的模拟和实验室实验，我们证明了...

    Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring. Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements. In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging. Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements. By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines. Through extensive simulations and in-lab experiments, we demonstrate tha
    
[^12]: 扩散模型是否学习语义有意义和高效的表示？

    Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?

    [https://arxiv.org/abs/2402.03305](https://arxiv.org/abs/2402.03305)

    本研究通过实验探究了条件DDPMs学习生成2D球形高斯凸起的过程，在学习的过程中发现了潜在表示的关键，产生了与不同阶段对应的 qualitatively 不同的生成行为。

    

    扩散模型能够以不寻常的方式生成图像，例如宇航员骑在月球上的马，并且有正确的阴影。这些输出表明了模型具有组合泛化的能力，但是模型是如何做到这一点的呢？我们在条件DDPMs上进行了控制实验，学习生成以指定的$x$和$y$位置为中心的2D球形高斯凸起。我们的结果表明，产生语义有意义的潜在表示对于实现高性能至关重要。在学习过程中，模型经历了三个不同的潜在表示阶段：(A阶段)没有潜在结构，(B阶段)一个混乱状态的2D流形，以及(C阶段)一个有序的2D流形。对应于这些阶段，我们发现了 qualitatively 不同的生成行为：1）生成多个凸起，2）生成一个凸起，但$x$和$y$位置不准确，3）生成一个凸起且位置准确。

    Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows. These outputs indicate the ability to perform compositional generalization, but how do the models do so? We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions. Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance. En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold. Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is genera
    
[^13]: 没关系：大语言模型中的指令覆盖和调节

    Nevermind: Instruction Override and Moderation in Large Language Models

    [https://arxiv.org/abs/2402.03303](https://arxiv.org/abs/2402.03303)

    大语言模型具有覆盖和调节指令的能力，较大的模型在覆盖内部和上下文指令方面表现最佳，并且在绳索扩展时需要保持缓冲区来保持指令遵循能力。

    

    鉴于近期大语言模型（LLMs）的令人印象深刻的能力，我们对最流行的专有模型和不同大小的开源模型进行了调查和基准测试，以解决在冲突情况下的明确指令遵循任务，例如覆盖。这些包括模型在其权重中覆盖知识的能力，覆盖（或调节）提示中提取的知识的能力，以及进行完全越狱的能力。实验表明，可以改进指令遵循的几个关键发现 - 较大的模型在遵循覆盖内部和上下文指令方面表现最佳，并且非常服从，甚至有些过度。当通过绳索扩展来扩展到更长的上下文时，需要保持与困惑边缘的显著缓冲区，以保持指令遵循能力。最后，我们观察到指令遵循的改善，以及随之而来的指令覆盖/越狱。

    Given the impressive capabilities of recent Large Language Models (LLMs), we investigate and benchmark the most popular proprietary and different sized open source models on the task of explicit instruction following in conflicting situations, e.g. overrides. These include the ability of the model to override the knowledge within the weights of the model, the ability to override (or moderate) extracted knowledge in the prompt, and lastly the ability to perform a full jailbreak. Experimentation performed suggest several key findings to improve instruction following - larger models perform the best in following instructions that override internal and contextual instructions, and are obedient, even to a fault. When scaling to longer contexts via rope scaling, a significant buffer needs to be maintained from the edge of the perplexity cliff in order to maintain instruction following capabilities. Finally, we observe improving instruction following, and subsequently instruction overrides/ja
    
[^14]: Swin-UMamba：以Mamba为基础的具有ImageNet预训练的UNet模型

    Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining

    [https://arxiv.org/abs/2402.03302](https://arxiv.org/abs/2402.03302)

    Swin-UMamba是一种以Mamba为基础的新型UNet模型，通过结合局部特征和全局依赖的多尺度信息来实现准确的医学图像分割。与现有方法相比，Swin-UMamba具有更高的准确性、较低的内存消耗和更少的计算负担，并充分利用了预训练的威力。

    

    准确的医学图像分割需要整合从局部特征到全局依赖的多尺度信息。然而，现有方法很难建模长距离的全局信息，卷积神经网络受到其局部感受野的限制，而视觉变换器的注意力机制受到高二次复杂性的影响。最近，基于Mamba的模型因其在长序列建模方面的出色能力而受到广泛关注。几项研究表明，这些模型在各种任务中能够胜过流行的视觉模型，提供更高的准确性、更低的内存消耗和更少的计算负担。然而，现有的基于Mamba的模型大多是从头开始训练，没有充分利用预训练的威力，而预训练已被证明对于高效的医学图像分析非常有效。本文介绍了一种新颖的基于Mamba的模型Swin-UMamba，专为医学图像分割任务而设计。

    Accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies. However, it is challenging for existing methods to model long-range global information, where convolutional neural networks (CNNs) are constrained by their local receptive fields, and vision transformers (ViTs) suffer from high quadratic complexity of their attention mechanism. Recently, Mamba-based models have gained great attention for their impressive ability in long sequence modeling. Several studies have demonstrated that these models can outperform popular vision models in various tasks, offering higher accuracy, lower memory consumption, and less computational burden. However, existing Mamba-based models are mostly trained from scratch and do not explore the power of pretraining, which has been proven to be quite effective for data-efficient medical image analysis. This paper introduces a novel Mamba-based model, Swin-UMamba, designed speci
    
[^15]: DeepSeekMath: 将开放语言模型中的数学推理能力推向极限

    DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

    [https://arxiv.org/abs/2402.03300](https://arxiv.org/abs/2402.03300)

    DeepSeekMath是一种开放语言模型，通过预训练和数据选择，提升了数学推理能力，实现了接近于竞赛级别水平的性能。

    

    数学推理由于其复杂和结构化的特性，对语言模型提出了重大挑战。本文介绍了DeepSeekMath 7B，它在Common Crawl中获取了120B个与数学相关的标记，并结合了自然语言和代码数据来继续预训练DeepSeek-Coder-Base-v1.5 7B。DeepSeekMath 7B在竞赛级别的数学基准测试中取得了令人印象深刻的51.7%的分数，无需依赖外部工具包和投票技术，接近了Gemini-Ultra和GPT-4的性能水平。DeepSeekMath 7B的自一致性在MATH上的64个样本中达到了60.9%的分数。DeepSeekMath的数学推理能力归因于两个关键因素：首先，我们通过精心设计的数据选择管道充分利用了公开可用的网络数据的巨大潜力。其次，我们引入了群体相对策略优化（GRPO），这是近端策略优化（PPO）的一个变体，可以增强数学推理能力。

    Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities w
    
[^16]: GUARD: 通过角色扮演生成自然语言越狱来测试大型语言模型遵循指南的合规性

    GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models

    [https://arxiv.org/abs/2402.03299](https://arxiv.org/abs/2402.03299)

    本论文提出了一个通过角色扮演的系统，可以生成自然语言越狱，用于测试大型语言模型的指南遵循情况。系统通过收集现有越狱并将其组织成知识图来生成新的越狱，证明了其高效性和有效性。

    

    发现绕过大型语言模型（LLM）的安全过滤和有害回应的"越狱"已经鼓励社区采取安全措施。其中一个主要的安全措施是在发布之前用越狱主动测试LLM。因此，这样的测试将需要一种能够大规模且高效地生成越狱的方法。本文在追随一种新颖而直观的策略下，以人类生成的方式来生成越狱。我们提出了一个角色扮演系统，将四种不同角色分配给用户LLM，以便协作生成新的越狱。此外，我们收集现有的越狱，并通过句子逐句进行聚类频率和语义模式的划分，将它们分成不同的独立特征。我们将这些特征组织成一个知识图，使其更易于访问和检索。我们的角色系统将利用这个知识图来生成新的越狱，证明了其有效性。

    The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
    
[^17]: Ginger: 一种用于通用神经网络的线性复杂度高效曲率近似方法

    Ginger: An Efficient Curvature Approximation with Linear Complexity for General Neural Networks

    [https://arxiv.org/abs/2402.03295](https://arxiv.org/abs/2402.03295)

    Ginger是一种用于通用神经网络的高效曲率近似方法，具有线性复杂度。它通过特征分解来逆向计算广义高斯牛顿矩阵，避免了传统方法中的高内存和高时间复杂度问题。

    

    二阶优化方法，如广义高斯牛顿法，由于利用了目标函数的曲率信息和预处理矩阵，被认为更加强大。尽管在理论上具有诱人的优势，但它们不易应用于现代深度学习。主要原因是计算矩阵的逆所需的二次内存和三次时间复杂度是不可行的，即使使用先进的硬件也不行。在这项工作中，我们提出了Ginger，一种用于广义高斯牛顿矩阵逆的特征分解方法。我们的方法在每次迭代中具有高效的线性内存和时间复杂度。我们直接维护条件矩阵的逆，以使近似更加准确，而不是近似条件矩阵。我们提供了Ginger在非凸目标上的收敛结果。我们在不同任务和不同模型架构上的实验证实了我们方法的有效性。我们的代码...

    Second-order optimization approaches like the generalized Gauss-Newton method are considered more powerful as they utilize the curvature information of the objective function with preconditioning matrices. Albeit offering tempting theoretical benefits, they are not easily applicable to modern deep learning. The major reason is due to the quadratic memory and cubic time complexity to compute the inverse of the matrix. These requirements are infeasible even with state-of-the-art hardware. In this work, we propose Ginger, an eigendecomposition for the inverse of the generalized Gauss-Newton matrix. Our method enjoys efficient linear memory and time complexity for each iteration. Instead of approximating the conditioning matrix, we directly maintain its inverse to make the approximation more accurate. We provide the convergence result of Ginger for non-convex objectives. Our experiments on different tasks with different model architectures verify the effectiveness of our method. Our code i
    
[^18]: Flora: 低秩适配器是悄悄的梯度压缩器

    Flora: Low-Rank Adapters Are Secretly Gradient Compressors

    [https://arxiv.org/abs/2402.03293](https://arxiv.org/abs/2402.03293)

    本文研究了低秩适配器的动力学，并提出了一种基于随机投影的方法Flora，通过重新采样投影矩阵实现高秩更新，同时减少优化状态的空间复杂度。

    

    尽管大型神经网络展示了完成不同任务的显着能力，但它们需要过多的内存使用来存储训练的优化状态。为了缓解这个问题，提出低秩适配（LoRA）来通过训练更少的参数来减少优化状态。然而，LoRA将整体权重更新矩阵限制为低秩，限制了模型的性能。在这项工作中，我们研究了LoRA的动力学，并确定它可以近似为随机投影。基于这一观察，我们提出了Flora，它能够通过重新采样投影矩阵实现高秩更新，同时享受优化状态的次线性空间复杂度。我们在不同任务和模型架构上进行实验证实了我们方法的有效性。

    Despite large neural networks demonstrating remarkable abilities to complete different tasks, they require excessive memory usage to store the optimization states for training. To alleviate this, the low-rank adaptation (LoRA) is proposed to reduce the optimization states by training fewer parameters. However, LoRA restricts overall weight update matrices to be low-rank, limiting the model performance. In this work, we investigate the dynamics of LoRA and identify that it can be approximated by a random projection. Based on this observation, we propose Flora, which is able to achieve high-rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states. We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach.
    
[^19]: 用上下文感知修复的零样本物体级OOD检测

    Zero-shot Object-Level OOD Detection with Context-Aware Inpainting

    [https://arxiv.org/abs/2402.03292](https://arxiv.org/abs/2402.03292)

    本论文提出了一种用上下文感知修复的零样本物体级OOD检测方法RONIN。通过将检测到的对象进行修复替换，并使用预测的ID标签来条件化修复过程，使得重构的对象在OOD情况下与原始对象相差较远，从而有效区分ID和OOD样本。实验证明RONIN在多个数据集上取得了具有竞争力的结果。

    

    机器学习算法越来越多地作为黑盒云服务或预训练模型提供，无法访问它们的训练数据。这就引发了零样本离群数据（OOD）检测的问题。具体而言，我们的目标是检测不属于分类器标签集但被错误地归类为入域（ID）对象的OOD对象。我们的方法RONIN使用现成的扩散模型来用修复替换掉检测到的对象。RONIN使用预测的ID标签来条件化修复过程，使输入对象接近入域域。结果是，重构的对象在ID情况下非常接近原始对象，在OOD情况下则相差较远，使得RONIN能够有效区分ID和OOD样本。通过大量实验证明，RONIN在零样本和非零样本设置下，相对于先前方法，在多个数据集上取得了具有竞争力的结果。

    Machine learning algorithms are increasingly provided as black-box cloud services or pre-trained models, without access to their training data. This motivates the problem of zero-shot out-of-distribution (OOD) detection. Concretely, we aim to detect OOD objects that do not belong to the classifier's label set but are erroneously classified as in-distribution (ID) objects. Our approach, RONIN, uses an off-the-shelf diffusion model to replace detected objects with inpainting. RONIN conditions the inpainting process with the predicted ID label, drawing the input object closer to the in-distribution domain. As a result, the reconstructed object is very close to the original in the ID cases and far in the OOD cases, allowing RONIN to effectively distinguish ID and OOD samples. Throughout extensive experiments, we demonstrate that RONIN achieves competitive results compared to previous approaches across several datasets, both in zero-shot and non-zero-shot settings.
    
[^20]: InstanceDiffusion：图像生成的实例级控制

    InstanceDiffusion: Instance-level Control for Image Generation

    [https://arxiv.org/abs/2402.03290](https://arxiv.org/abs/2402.03290)

    InstanceDiffusion通过添加实例级控制，使文本到图像的扩散模型能够产生高质量图像，并在不同的位置条件下超过了专业先进模型。

    

    文本到图像的扩散模型可以生成高质量图像，但不能对图像中的单个实例进行精确控制。我们引入InstanceDiffusion，将精确的实例级控制添加到文本到图像的扩散模型中。InstanceDiffusion支持每个实例的自由形式语言条件，并允许以简单的单个点、涂鸦、边界框或复杂的实例分割掩码及其组合方式指定实例位置。我们提出了三个重要的改进文本到图像模型的方法，以实现精确的实例级控制。我们的UniFusion块实现了文本到图像模型的实例级条件，ScaleU块提高了图像的保真度，Multi-instance采样器提高了多实例的生成效果。InstanceDiffusion在每个位置条件下明显超过了专业先进模型。值得注意的是，对于COCO数据集，我们在box输入方面超过了以前的最先进技术20.4%AP50 box。

    Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$ for box inputs, 
    
[^21]: 让每一步都有价值：使用MCTS的LLM基础高质量RTL代码生成

    Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS

    [https://arxiv.org/abs/2402.03289](https://arxiv.org/abs/2402.03289)

    本文介绍了一种使用蒙特卡罗树搜索进行前瞻的自动变换器解码算法，可解决现有大型语言模型在RTL代码生成中存在的编译失败和PPA不敏感的问题，并在性能上取得了显著改进。

    

    现有的用于寄存器传输级代码生成的大型语言模型(LLM)面临编译失败和亚最优功耗、性能和面积(PPA)效率等挑战。这是由于传统变换器解码算法缺乏对PPA的意识所致。为此，我们提出了一种自动变换器解码算法，它通过蒙特卡罗树搜索来进行前瞻，引导变换器生成可编译的、功能正确的、PPA优化的代码。在RTL代码集上使用经过微调的语言模型进行的实证评估表明，与仅使用提示的方法相比，我们提出的技术一致地生成功能正确的代码，并有效解决了朴素大型语言模型对PPA不敏感的缺点。对于最先进的LLM生成的最大设计（16位加法器），我们的技术可以在面积延迟乘积上实现31.8%的改进。

    Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency. This is due to the lack of PPA awareness in conventional transformer decoding algorithms. In response, we present an automated transformer decoding algorithm that integrates Monte Carlo tree-search for lookahead, guiding the transformer to produce compilable, functionally correct, and PPA-optimized code. Empirical evaluation with a fine-tuned language model on RTL codesets shows that our proposed technique consistently generates functionally correct code compared to prompting-only methods and effectively addresses the PPA-unawareness drawback of naive large language models. For the largest design generated by the state-of-the-art LLM (16-bit adder), our technique can achieve a 31.8% improvement in the area-delay product.
    
[^22]: 用于分布归一化的Lennard-Jones层

    A Lennard-Jones Layer for Distribution Normalization

    [https://arxiv.org/abs/2402.03287](https://arxiv.org/abs/2402.03287)

    我们引入了Lennard-Jones层（LJL）来均衡2D和3D点云的密度，通过系统重新排列点并模拟点之间的相互作用，我们可以达到近似均匀采样的效果。这种方法可以应用于点云生成和改善点分布问题。

    

    我们引入了Lennard-Jones层（LJL）来通过系统重新排列点而不破坏它们的整体结构（分布归一化）来均衡2D和3D点云的密度。LJL模拟了每个点在给定时间内考虑其最近邻的排斥和弱吸引相互作用的耗散过程。通过这个过程，将点推到一个势能谷中，达到一个明确定义的稳定配置，近似于均匀采样。我们将LJL应用于将随机生成的点云重新分布为随机均匀分布。此外，LJL还嵌入到点云网络的生成过程中，通过在推理过程的后期添加它们。我们定性和定量地评估了利用LJL改善3D点云生成的效果。最后，我们将LJL应用于改善基于得分的3D点分布。

    We introduce the Lennard-Jones layer (LJL) for the equalization of the density of 2D and 3D point clouds through systematically rearranging points without destroying their overall structure (distribution normalization). LJL simulates a dissipative process of repulsive and weakly attractive interactions between individual points by considering the nearest neighbor of each point at a given moment in time. This pushes the particles into a potential valley, reaching a well-defined stable configuration that approximates an equidistant sampling after the stabilization process. We apply LJLs to redistribute randomly generated point clouds into a randomized uniform distribution. Moreover, LJLs are embedded in the generation process of point cloud networks by adding them at later stages of the inference process. The improvements in 3D point cloud generation utilizing LJLs are evaluated qualitatively and quantitatively. Finally, we apply LJLs to improve the point distribution of a score-based 3D
    
[^23]: 无需训练的一致性文本到图像生成

    Training-Free Consistent Text-to-Image Generation

    [https://arxiv.org/abs/2402.03286](https://arxiv.org/abs/2402.03286)

    本文提出了一种无需训练的方法ConsiStory，通过共享预训练模型的内部激活，实现了一致的文本到图像生成。引入了主题驱动的共享注意力块和基于对应的特征注入，促进了图像之间的主题一致性，并采用了策略来保持布局多样性。

    

    文本到图像模型通过自然语言引导图像生成过程，提供了一种新的创造性灵活性。然而，使用这些模型在多样化的提示下一致地描绘相同的主题仍然具有挑战性。现有方法通过优化模型来教授它描述特定用户提供主题的新词汇或者为模型添加图像条件。这些方法要求针对每个主题进行漫长的优化或进行大规模预训练。此外，它们在将生成的图像与文本提示对齐和描绘多个主题方面遇到困难。在这里，我们介绍了一种无训练方法ConsiStory，通过共享预训练模型的内部激活来实现一致的主题生成。我们引入了一个主题驱动共享注意力块和基于对应的特征注入，以促进图像之间的主题一致性。此外，我们开发了策略以鼓励布局多样性，同时保持主题一致性。

    Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining su
    
[^24]: 交易，还是不交易（或者谁知道）？使用大型语言模型预测对话中的不确定性

    Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models

    [https://arxiv.org/abs/2402.03284](https://arxiv.org/abs/2402.03284)

    本论文研究了如何用语言模型来表示对话中的不确定性，并提出了改进模型校准的微调策略。实验证明，这些策略可以使较小的模型具备与大型预训练模型相当的性能。

    

    有效的对话者考虑他人的不确定目标、信念和情绪。但即使是最佳的人类对话者也无法完美地预测对话的轨迹。语言模型能够多好地表示对话中固有的不确定性？我们提出了FortUne Dial，这是“对话预测”任务的扩展：评估不仅仅以准确度为标准，还采用了对不确定性敏感的度量方法，有效地使个别实例可以放弃。我们研究了语言模型可能表示结果不确定性的两种方式（内部使用分数和直接使用令牌），并提出了改进两种表示的校准的微调策略。对八个困难的谈判语料库进行的实验表明，我们提出的微调策略（一种传统的监督策略和一种离线策略强化学习策略）可以使较小的开源模型校准得上与其尺寸相当的预训练模型。

    Effective interlocutors account for the uncertain goals, beliefs, and emotions of others. But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue. How well can language models represent inherent uncertainty in conversations? We propose FortUne Dial, an expansion of the long-standing "conversation forecasting" task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances. We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose fine-tuning strategies to improve calibration of both representations. Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies (a traditional supervision strategy and an off-policy reinforcement learning strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their si
    
[^25]: 一个部分观察到的奖励状态在RLHF中的框架

    A Framework for Partially Observed Reward-States in RLHF

    [https://arxiv.org/abs/2402.03282](https://arxiv.org/abs/2402.03282)

    这篇论文提出了一个针对RLHF的框架，在其中考虑了部分观察到的奖励状态，并通过将基数反馈和决斗反馈缩减为PORRL形式进行了建模和算法开发。

    

    最近几年来，强化学习从人类反馈（RLHF）的研究因其在LLMs的发展中起到的作用而变得重要。神经科学研究表明，人类对刺激的反应已知依赖于部分观察到的“内部状态”。不幸的是，当前的RLHF模型没有考虑到这一点。此外，大多数RLHF模型没有考虑到中间反馈，在实证研究中变得越来越重要，可以帮助提高样本复杂性和对齐性。为了解决这些局限性，我们将RLHF建模为部分观察到的奖励状态的强化学习（PORRL）。我们展示了从RLHF中两种主要形式的人类反馈 - 基数反馈和决斗反馈到PORRL的缩减。对于基数反馈，我们开发了通用的统计高效算法，并将它们实例化为POR-UCRL和POR-UCBVI。对于决斗反馈，我们表明，简单的基数反馈缩减不能达到亚线性的决斗回归。

    The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs. Neuroscience research shows that human responses to stimuli are known to depend on partially-observed "internal states." Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL). We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regr
    
[^26]: 想法的不确定性：不确定性感知规划增强大型语言模型的信息搜索能力

    Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models

    [https://arxiv.org/abs/2402.03271](https://arxiv.org/abs/2402.03271)

    通过引入不确定性感知规划（UoT）算法，我们实现了增强大型语言模型的主动寻求信息的能力，通过模拟未来场景、基于不确定性的奖励机制和奖励传播方案，优化问题提问方式。

    

    在面对不确定性时，寻求信息的能力至关重要。在许多实际应用中，比如医学诊断和故障排除，解决任务所需的信息不是初始给定的，而需要通过询问后续问题来主动寻求（例如，医生向患者询问症状的更多细节）。在这项工作中，我们引入了思想的不确定性（UoT），一种算法将大型语言模型的能力与主动提问信息的能力相结合。UoT结合了1）不确定性感知仿真方法，使模型能够模拟可能的未来场景，并估计其发生的可能性；2）基于不确定性的奖励机制，激励模型寻求信息；3）奖励传播方案，以最大化预期奖励的方式选择最佳的问题提问方式。在医学诊断、故障排除和'20的实验中。

    In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 
    
[^27]: 用于检测MQTT-IoT协议攻击的多类分类程序

    Multiclass Classification Procedure for Detecting Attacks on MQTT-IoT Protocol

    [https://arxiv.org/abs/2402.03270](https://arxiv.org/abs/2402.03270)

    本研究致力于创建适用于MQTT协议的物联网系统的入侵检测系统，通过使用集成方法和深度学习模型来分类攻击，在实验中取得了非常满意的结果。

    

    由于物联网系统中传感器和执行器的大量存在，系统必须使用各种技术和协议。这意味着物联网网络比传统网络更加异构。这给网络安全带来了新的挑战，以保护这些系统和设备，这些设备需要持续连接到互联网。入侵检测系统（IDS）用于保护物联网系统免受网络层面的各种异常和攻击。通过机器学习技术可以改进入侵检测系统（IDS）。我们的工作重点是创建分类模型，可以使用包含MQTT协议的物联网系统受攻击帧的数据集来为IDS提供输入。我们探讨了两种类型的攻击分类方法，集成方法和深度学习模型，具体来说是具有非常满意结果的循环网络。

    The large number of sensors and actuators that make up the Internet of Things obliges these systems to use diverse technologies and protocols. This means that IoT networks are more heterogeneous than traditional networks. This gives rise to new challenges in cybersecurity to protect these systems and devices which are characterized by being connected continuously to the Internet. Intrusion detection systems (IDS) are used to protect IoT systems from the various anomalies and attacks at the network level. Intrusion Detection Systems (IDS) can be improved through machine learning techniques. Our work focuses on creating classification models that can feed an IDS using a dataset containing frames under attacks of an IoT system that uses the MQTT protocol. We have addressed two types of method for classifying the attacks, ensemble methods and deep learning models, more specifically recurrent networks with very satisfactory results.
    
[^28]: ISPA: 用于转录动物声音的跨物种语音音标

    ISPA: Inter-Species Phonetic Alphabet for Transcribing Animal Sounds

    [https://arxiv.org/abs/2402.03269](https://arxiv.org/abs/2402.03269)

    本文介绍了ISPA（跨物种语音音标），这是一种精确、简洁、可解释的系统，用于将动物声音转录为文本。通过将动物声音表示为文本，我们有效地将其视为一种“外语”，并展示了已建立的人类语言机器学习范例和模型（如语言模型）能够成功应用于提高性能。

    

    传统上，生物声学依赖谱图和连续的每帧音频表示来分析动物声音，并作为机器学习模型的输入。与此同时，国际音标（IPA）系统提供了一种可解释的、与语言无关的方法来转录人类语音声音。本文介绍了ISPA（跨物种语音音标），这是一种精确、简洁、可解释的系统，用于将动物声音转录为文本。我们比较了基于声学和基于特征的方法来转录和分类动物声音，并证明了它们与使用连续、稠密音频表示的基准方法具有可比性的性能。通过将动物声音表示为文本，我们有效地将其视为一种“外语”，并展示了已建立的人类语言机器学习范例和模型（如语言模型）能够成功应用于提高性能。

    Traditionally, bioacoustics has relied on spectrograms and continuous, per-frame audio representations for the analysis of animal sounds, also serving as input to machine learning models. Meanwhile, the International Phonetic Alphabet (IPA) system has provided an interpretable, language-independent method for transcribing human speech sounds. In this paper, we introduce ISPA (Inter-Species Phonetic Alphabet), a precise, concise, and interpretable system designed for transcribing animal sounds into text. We compare acoustics-based and feature-based methods for transcribing and classifying animal sounds, demonstrating their comparable performance with baseline methods utilizing continuous, dense audio representations. By representing animal sounds with text, we effectively treat them as a "foreign language," and we show that established human language ML paradigms and models, such as language models, can be successfully applied to improve performance.
    
[^29]: 从推理路径聚合的角度理解语言模型的推理能力

    Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation

    [https://arxiv.org/abs/2402.03268](https://arxiv.org/abs/2402.03268)

    本文研究了预训练语言模型的推理能力，并提出了从聚合间接推理路径的角度理解语言模型如何产生推理能力。通过对知识图谱和数学问题数据集进行实验和分析，发现增加无标签的随机游走推理路径可以提高实际应用中的多步推理能力。

    

    预训练的语言模型能够在没有明确微调的情况下执行复杂的推理。为了理解预训练与下一个标记预测目标的关系如何促使推理能力的出现，我们提出可以将语言模型视为在预训练时通过聚合间接的推理路径来得出新结论。我们发现，这个视角在逻辑推理和数学推理等关键情况下非常有效。具体而言，我们将推理路径形式化为在知识/推理图上的随机游走路径。对学习的语言模型分布的分析表明，相关随机游走路径概率的加权和是解释语言模型推理的合理方式。对多个知识图谱和数学问题数据集进行的实验和分析揭示了训练对随机游走路径的影响，并表明增加无标签的随机游走推理路径可以提高现实世界的多步推理能力。

    Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step r
    
[^30]: MobilityGPT: 基于GPT模型的增强型人类移动建模

    MobilityGPT: Enhanced Human Mobility Modeling with a GPT model

    [https://arxiv.org/abs/2402.03264](https://arxiv.org/abs/2402.03264)

    MobilityGPT是一种基于GPT模型的增强型人类移动建模方法，通过将人类移动建模转换为自回归生成任务，并引入地理感知生成模型以及基于重力的采样方法和道路连接矩阵约束，实现了对生成的地理空间移动数据的语义真实性和各个特征的保持。

    

    生成模型在捕捉人类移动特征和生成合成轨迹方面取得了很好的效果。然而，确保生成的地理空间移动数据在语义上是真实的，包括一致的位置序列以及反映现实世界的特征，如对地理限制的约束，仍然具有挑战性。为了解决这些问题，我们将人类移动建模重新构建为一个自回归生成任务，利用了Generative Pre-trained Transformer (GPT)。为了确保其可控的生成来缓解上述挑战，我们提出了一种地理感知的生成模型，即MobilityGPT。我们通过引入基于重力的采样方法训练了一个用于语义序列相似性的transformer。然后，通过使用一个道路连接矩阵对训练过程进行约束，该矩阵给出了轨迹生成中序列之间的连接性，从而保持生成的轨迹在地理范围内。最后，我们构建了一个强化学习引导的改进目标函数来增强模型的生成性能。

    Generative models have shown promising results in capturing human mobility characteristics and generating synthetic trajectories. However, it remains challenging to ensure that the generated geospatial mobility data is semantically realistic, including consistent location sequences, and reflects real-world characteristics, such as constraining on geospatial limits. To address these issues, we reformat human mobility modeling as an autoregressive generation task, leveraging Generative Pre-trained Transformer (GPT). To ensure its controllable generation to alleviate the above challenges, we propose a geospatially-aware generative model, MobilityGPT. We propose a gravity-based sampling method to train a transformer for semantic sequence similarity. Then, we constrained the training process via a road connectivity matrix that provides the connectivity of sequences in trajectory generation, thereby keeping generated trajectories in geospatial limits. Lastly, we constructed a Reinforcement L
    
[^31]: 学习Predict-then-Optimize框架中的最优策略

    Learning Best-in-Class Policies for the Predict-then-Optimize Framework

    [https://arxiv.org/abs/2402.03256](https://arxiv.org/abs/2402.03256)

    我们提出了一种新颖的决策感知替代损失函数家族，用于predict-then-optimize框架，并且通过数值证据证实了其在误设置下的优越性。

    

    我们提出了一种新颖的决策感知替代损失函数家族，称为Perturbation Gradient（PG）损失，用于predict-then-optimize框架。这些损失直接近似了下游决策损失，并可以使用现成的基于梯度的方法进行优化。重要的是，与现有的替代损失不同，我们的PG损失的近似误差随着样本数量的增加而消失。这意味着优化我们的替代损失可以在渐近意义下得到最佳策略，即使在误设置下也是如此。这是第一个在误设置下的这样的结果，我们提供了数值证据证实了当基础模型误设置且噪声不是中心对称时，我们的PG损失在实践中显著优于现有的提案。鉴于在实践中误设置很常见--特别是当我们可能更喜欢一个更简单、更可解释的模型时--PG损失提供了一种新颖的、理论上有依据的、可计算的决策感知方法。

    We propose a novel family of decision-aware surrogate losses, called Perturbation Gradient (PG) losses, for the predict-then-optimize framework. These losses directly approximate the downstream decision loss and can be optimized using off-the-shelf gradient-based methods. Importantly, unlike existing surrogate losses, the approximation error of our PG losses vanishes as the number of samples grows. This implies that optimizing our surrogate loss yields a best-in-class policy asymptotically, even in misspecified settings. This is the first such result in misspecified settings and we provide numerical evidence confirming our PG losses substantively outperform existing proposals when the underlying model is misspecified and the noise is not centrally symmetric. Insofar as misspecification is commonplace in practice -- especially when we might prefer a simpler, more interpretable model -- PG losses offer a novel, theoretically justified, method for computationally tractable decision-aware 
    
[^32]: 表示学习的最小描述长度和泛化保证

    Minimum Description Length and Generalization Guarantees for Representation Learning

    [https://arxiv.org/abs/2402.03254](https://arxiv.org/abs/2402.03254)

    本文提出了一个可压缩性框架，通过计算表示学习算法的泛化误差的上界，改进了现有启发式方法，并提供了关于理论泛化保证的新见解。

    

    设计高效的统计有监督学习算法的一个主要挑战是找到不仅在可用训练样本上表现良好而且在未见数据上也表现良好的表示形式。尽管表示学习的研究引发了许多兴趣，但大多数现有方法都是启发式的；对于理论上的泛化保证几乎没有什么了解。在本文中，我们建立了一个可压缩性框架，使我们能够通过标签或潜在变量（表示形式）的"最小描述长度"（MDL）来推导表示学习算法的泛化误差的上界。与通常被认为反映算法泛化能力的编码器输入和表示之间的互信息相比，我们的新界限涉及表示（或标签）分布之间的"多字母"相对熵，在相关文献中对算法的泛化能力的反映还不足。

    A major challenge in designing efficient statistical supervised learning algorithms is finding representations that perform well not only on available training samples but also on unseen data. While the study of representation learning has spurred much interest, most existing such approaches are heuristic; and very little is known about theoretical generalization guarantees.   In this paper, we establish a compressibility framework that allows us to derive upper bounds on the generalization error of a representation learning algorithm in terms of the "Minimum Description Length" (MDL) of the labels or the latent variables (representations). Rather than the mutual information between the encoder's input and the representation, which is often believed to reflect the algorithm's generalization capability in the related literature but in fact, falls short of doing so, our new bounds involve the "multi-letter" relative entropy between the distribution of the representations (or labels) of t
    
[^33]: 来自配对偏好的公平主动排序

    Fair Active Ranking from Pairwise Preferences

    [https://arxiv.org/abs/2402.03252](https://arxiv.org/abs/2402.03252)

    本文研究了通过自适应地引发配对比较来公平主动排序的问题，提出了一种新颖的公平目标函数，并通过最小化群组误差的范数来实现对不同公平概念的探索。

    

    我们研究了通过自适应地引发配对比较来可能近似正确且公平（PACF）对项目进行排序的问题。给定一个属于不相交群组的$n$个项目集，我们的目标是根据我们提出的公平目标函数找到一个$(\epsilon, \delta)$-PACF-Ranking。我们假设可以访问一个Oracle，对于每个查询，学习者可以选择一对项目，并从Oracle接收到随机的获胜者反馈。我们提出的目标函数要求最小化群组误差的$\ell_q$范数，其中群组误差是该群组中所有项目误差的$\ell_p$范数，对于$p, q \geq 1$。这扩展了Saha＆Gopalan（2019）提出的$\epsilon$-Best-Ranking目标函数。通过采用我们的目标函数，我们获得了探索平等或比例误差等基本公平概念的灵活性。调整参数$p$和$q$可以按需定制。

    We investigate the problem of probably approximately correct and fair (PACF) ranking of items by adaptively evoking pairwise comparisons. Given a set of $n$ items that belong to disjoint groups, our goal is to find an $(\epsilon, \delta)$-PACF-Ranking according to a fair objective function that we propose. We assume access to an oracle, wherein, for each query, the learner can choose a pair of items and receive stochastic winner feedback from the oracle. Our proposed objective function asks to minimize the $\ell_q$ norm of the error of the groups, where the error of a group is the $\ell_p$ norm of the error of all the items within that group, for $p, q \geq 1$. This generalizes the objective function of $\epsilon$-Best-Ranking, proposed by Saha & Gopalan (2019).   By adopting our objective function, we gain the flexibility to explore fundamental fairness concepts like equal or proportionate errors within a unified framework. Adjusting parameters $p$ and $q$ allows tailoring to specific
    
[^34]: CLIP可以理解深度

    CLIP Can Understand Depth

    [https://arxiv.org/abs/2402.03251](https://arxiv.org/abs/2402.03251)

    本文研究了将CLIP用于单目深度估计的问题，通过联合训练反卷积解码器和可学习嵌入矩阵，使得CLIP能够理解深度，该方法在深度估计任务上取得了令人印象深刻的性能，并优于之前的方法。

    

    最近关于将CLIP推广到单目深度估计的研究表明，在网络爬取的数据上预训练的CLIP在图像块和与深度相关的提示之间得到适当相似性是低效的。在本文中，我们适应CLIP用于有意义的密集预测单目深度估计，而无需微调其原始的视觉-语言对齐。通过联合训练一个紧凑的反卷积解码器和一个名为mirror的小型可学习嵌入矩阵作为其文本编码器的静态提示，CLIP能够理解深度。通过这种方法，我们的模型在NYU Depth v2和KITTI数据集上展现出了令人印象深刻的性能，与几个先前的仅视觉模型相匹配，而且胜过了每个基于CLIP的深度估计模型。关于时间深度一致性和空间连续性的实验证明，我们提出的框架能够有效地优化CLIP的先验知识。此外，对于时滞研究进行了消融实验。

    Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts. In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment. By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth. With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin. Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework. Furthermore, an ablation study on 
    
[^35]: 技能集优化：通过可转移技能增强语言模型行为

    Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills

    [https://arxiv.org/abs/2402.03244](https://arxiv.org/abs/2402.03244)

    本论文提出了一种技能集优化（SSO）方法，通过构建和完善可转移的技能集来提高大型语言模型（LLM）的性能。该方法通过提取高奖励的共同子轨迹，生成子目标和说明，并在上下文中提供给LLM演员，以强化行为。实验结果显示，SSO在不同环境中能够优化技能集，并实现上下文策略改进。

    

    近期，大型语言模型（LLMs）已被用于交互环境中的顺序决策。然而，利用环境奖励信号来不断改进LLM演员的表现并不简单。我们提出了技能集优化（SSO）来通过构建和完善可转移技能集来提高LLM演员的性能。SSO通过提取具有高奖励的共同子轨迹并生成子目标和说明来构建技能。这些技能在上下文中提供给LLM演员，以强化具有高奖励的行为。然后，SSO通过修剪不再产生高奖励的技能来进一步完善技能集。我们在经典视频游戏NetHack和文本环境ScienceWorld中评估了我们的方法，以展示SSO优化技能集并进行上下文策略改进的能力。在我们的自定义NetHack任务中，SSO的性能超过基准方法40%，并超过了先前的最新状态。

    Large language models (LLMs) have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual LLM actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards. Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-
    
[^36]: PINN-BO:一种使用物理启发式神经网络的黑箱优化算法

    PINN-BO: A Black-box Optimization Algorithm using Physics-Informed Neural Networks

    [https://arxiv.org/abs/2402.03243](https://arxiv.org/abs/2402.03243)

    PINN-BO是一种利用物理启发式神经网络和偏微分方程知识的黑箱优化算法，提高了优化的样本效率。

    

    黑箱优化是一种在嘈杂且昂贵的黑箱函数中发现全局最优解的强大方法，这在现实世界的场景中经常遇到。最近，利用领域知识增强机器学习方法的效力引起了越来越多的兴趣。偏微分方程常常是阐明控制黑箱函数的基本原理的有效手段。在本文中，我们提出了PINN-BO，一种采用物理启发式神经网络的黑箱优化算法，它将偏微分方程的知识与优化的样本效率相结合。我们利用NTK理论的进展分析了我们算法的理论行为，证明了在黑箱函数评估中使用PDE和PINN-BO可以得到更紧的后悔界。我们在多种优化任务上进行了多个实验，并展示了其性能。

    Black-box optimization is a powerful approach for discovering global optima in noisy and expensive black-box functions, a problem widely encountered in real-world scenarios. Recently, there has been a growing interest in leveraging domain knowledge to enhance the efficacy of machine learning methods. Partial Differential Equations (PDEs) often provide an effective means for elucidating the fundamental principles governing the black-box functions. In this paper, we propose PINN-BO, a black-box optimization algorithm employing Physics-Informed Neural Networks that integrates the knowledge from Partial Differential Equations (PDEs) to improve the sample efficiency of the optimization. We analyze the theoretical behavior of our algorithm in terms of regret bound using advances in NTK theory and prove that the use of the PDE alongside the black-box function evaluations, PINN-BO leads to a tighter regret bound. We perform several experiments on a variety of optimization tasks and show that o
    
[^37]: FROSTER: 冻结的CLIP是用于开放式词汇动作识别的有力教师

    FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition

    [https://arxiv.org/abs/2402.03241](https://arxiv.org/abs/2402.03241)

    FROSTER是一种用于开放式词汇动作识别的框架，通过使用冻结的CLIP模型作为教师，在保持CLIP泛化能力的同时有效适应动作识别任务。

    

    本文介绍了FROSTER，一种用于开放式词汇动作识别的有效框架。CLIP模型在各种基于图像的任务中取得了显著的成功，得益于其在大规模图像-文本对上的预训练所展现出的强大泛化能力。然而，将CLIP直接应用于开放式词汇动作识别任务是具有挑战性的，因为CLIP的预训练中缺少时间信息。此外，对动作识别数据集进行CLIP的微调可能会导致过拟合，阻碍其泛化能力，导致处理未知动作时结果不尽如人意。

    In this paper, we introduce FROSTER, an effective framework for open-vocabulary action recognition. The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability stemming from pretaining on massive image-text pairs. However, applying CLIP directly to the open-vocabulary action recognition task is challenging due to the absence of temporal information in CLIP's pretraining. Further, fine-tuning CLIP on action recognition datasets may lead to overfitting and hinder its generalizability, resulting in unsatisfactory results when dealing with unseen actions.   To address these issues, FROSTER employs a residual feature distillation approach to ensure that CLIP retains its generalization capability while effectively adapting to the action recognition task. Specifically, the residual feature distillation treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises
    
[^38]: ActiveAnno3D - 一种用于多模态3D物体检测的主动学习框架

    ActiveAnno3D - An Active Learning Framework for Multi-Modal 3D Object Detection

    [https://arxiv.org/abs/2402.03235](https://arxiv.org/abs/2402.03235)

    这项工作提出了一种用于多模态3D物体检测的主动学习框架ActiveAnno3D。通过选择最具信息量的训练数据样本进行标注，我们能够在使用一半的训练数据时实现与传统方法相近的检测性能。

    

    大规模数据集的筛选仍然需要大量的时间和资源，数据通常需要人工标注，创建高质量数据集的难题依然存在。在这项工作中，我们使用主动学习的方法来解决多模态3D物体检测中的研究空白。我们提出了ActiveAnno3D，一个用于选择最具信息量的训练数据样本进行标注的主动学习框架。我们探索了各种连续训练方法，并集成了在计算需求和检测性能方面最高效的方法。此外，我们对nuScenes和TUM Traffic Intersection数据集进行了大量实验和消融研究，使用BEVFusion和PV-RCNN进行了测试。我们展示了当仅使用TUM Traffic Intersection数据集的一半训练数据（77.25 mAP相比于83.50 mAP）时，使用PV-RCNN和基于熵的查询策略几乎可以达到相同的性能，而BEVFusion则在使用一半的训练数据时获得了64.31的mAP。

    The curation of large-scale datasets is still costly and requires much time and resources. Data is often manually labeled, and the challenge of creating high-quality datasets remains. In this work, we fill the research gap using active learning for multi-modal 3D object detection. We propose ActiveAnno3D, an active learning framework to select data samples for labeling that are of maximum informativeness for training. We explore various continuous training methods and integrate the most efficient method regarding computational demand and detection performance. Furthermore, we perform extensive experiments and ablation studies with BEVFusion and PV-RCNN on the nuScenes and TUM Traffic Intersection dataset. We show that we can achieve almost the same performance with PV-RCNN and the entropy-based query strategy when using only half of the training data (77.25 mAP compared to 83.50 mAP) of the TUM Traffic Intersection dataset. BEVFusion achieved an mAP of 64.31 when using half of the trai
    
[^39]: 智能流匹配：关于流匹配算法的理论与应用

    Smart Flow Matching: On The Theory of Flow Matching Algorithms with Applications

    [https://arxiv.org/abs/2402.03232](https://arxiv.org/abs/2402.03232)

    本文提出了一种智能流匹配算法，通过精确的向量场公式最小化标准流的损失，并在训练向量场模型时展示了更小的方差和更好的学习结果。

    

    本文提出了一种精确的公式，用于最小化标准流的损失的向量场。该公式在给定的分布ρ₀和未知的分布ρ₁之间进行分析性依赖。基于这个公式，提供了一种新的损失和算法，用于以条件流匹配的方式训练向量场模型。与标准的条件流匹配方法相比，我们的损失在通过蒙特卡罗采样方法评估时表现出更小的方差。通过对合成模型和大维度表格数据模型进行数值实验，证明了使用所提出的算法能够获得更好的学习结果。

    The paper presents the exact formula for the vector field that minimizes the loss for the standard flow. This formula depends analytically on a given distribution \rho_0 and an unknown one \rho_1. Based on the presented formula, a new loss and algorithm for training a vector field model in the style of Conditional Flow Matching are provided. Our loss, in comparison to the standard Conditional Flow Matching approach, exhibits smaller variance when evaluated through Monte Carlo sampling methods. Numerical experiments on synthetic models and models on tabular data of large dimensions demonstrate better learning results with the use of the presented algorithm.
    
[^40]: 在在线A/B测试中改进对未来用户活动的预测

    Improved prediction of future user activity in online A/B testing

    [https://arxiv.org/abs/2402.03231](https://arxiv.org/abs/2402.03231)

    本文提出了一种在在线A/B测试中改进对未来用户活动预测的方法，该方法利用贝叶斯非参数方法预测个体被介入的速率，并提供双重预测能力，预测未来时间窗口中新客户数量和被观察次数。

    

    在在线随机实验或A/B测试中，准确预测参与率十分重要。这些预测不仅指导实验者优化实验的持续时间，还提高了治疗效果估计的精度。本文提出了一种新颖、简单和可扩展的贝叶斯非参数方法，用于在在线A/B测试中预测个体被介入的速率。我们的方法有双重预测能力：预测未来时间窗口中的新客户数量，以及与现有的可用方法不同，他们将被观察的次数。我们推导出用于形成对未来用户活动预测的所需数量的后验分布的闭合表达式，从而避免了使用数值算法如马尔可夫链蒙特卡洛。在全面阐述我们的模型之后，我们进行了实证研究，对我们的模型的性能进行了评估。

    In online randomized experiments or A/B tests, accurate predictions of participant inclusion rates are of paramount importance. These predictions not only guide experimenters in optimizing the experiment's duration but also enhance the precision of treatment effect estimates. In this paper we present a novel, straightforward, and scalable Bayesian nonparametric approach for predicting the rate at which individuals will be exposed to interventions within the realm of online A/B testing. Our approach stands out by offering dual prediction capabilities: it forecasts both the quantity of new customers expected in future time windows and, unlike available alternative methods, the number of times they will be observed. We derive closed-form expressions for the posterior distributions of the quantities needed to form predictions about future user activity, thereby bypassing the need for numerical algorithms such as Markov chain Monte Carlo. After a comprehensive exposition of our model, we te
    
[^41]: 基于CT的胸部手术规划的解剖分割：针对3D U-shaped深度学习模型的基准研究

    CT-based Anatomical Segmentation for Thoracic Surgical Planning: A Benchmark Study for 3D U-shaped Deep Learning Models

    [https://arxiv.org/abs/2402.03230](https://arxiv.org/abs/2402.03230)

    本研究为基于CT的胸部手术规划中的解剖分割提供了针对3D U-shaped深度学习模型的基准研究，为临床应用和未来模型设计提供了宝贵的见解。

    

    最近对患者特定胸部手术规划和仿真的兴趣日益增长，需要从自动医学图像分割算法中高效、稳健地创建数字解剖模型。深度学习(DL)现在是各种放射学任务的最先进技术，而U-shaped DL模型在医学图像分割方面表现出色，自2D UNet以来就一直如此。迄今为止，通过整合不同的注意力机制和网络配置，已经提出了许多U-shaped模型的变体。借助最近大型多标签数据库的发展，对这些模型的系统基准研究可以为临床部署和未来模型设计提供宝贵的见解，但此类研究仍然很少。我们进行了针对3D U-shaped模型(3DUNet、STUNet、AttentionUNet、SwinUNETR、FocalSegNet和一种新的具有四个变体的3D SwinUnet)的第一项基准研究，重点是基于CT的胸部解剖分割。

    Recent rising interests in patient-specific thoracic surgical planning and simulation require efficient and robust creation of digital anatomical models from automatic medical image segmentation algorithms. Deep learning (DL) is now state-of-the-art in various radiological tasks, and U-shaped DL models have particularly excelled in medical image segmentation since the inception of the 2D UNet. To date, many variants of U-shaped models have been proposed by the integration of different attention mechanisms and network configurations. Leveraging the recent development of large multi-label databases, systematic benchmark studies for these models can provide valuable insights for clinical deployment and future model designs, but such studies are still rare. We conduct the first benchmark study for variants of 3D U-shaped models (3DUNet, STUNet, AttentionUNet, SwinUNETR, FocalSegNet, and a novel 3D SwinUnet with four variants) with a focus on CT-based anatomical segmentation for thoracic su
    
[^42]: IGUANe: 一种适用于脑MR图像多中心协调的三维通用CycleGAN模型

    IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images

    [https://arxiv.org/abs/2402.03227](https://arxiv.org/abs/2402.03227)

    IGUANe是一种三维通用CycleGAN模型，通过集成多个域的训练实现了脑MR图像的多中心协调，使其成为通用生成器。

    

    在MRI研究中，来自多个采集点的图像数据的聚合可以增加样本大小，但可能引入阻碍后续分析一致性的与采集点相关的变异。图像翻译的深度学习方法已经成为协调MR图像跨站点的解决方案。在本研究中，我们引入了IGUANe（具有统一对抗网络的图像生成），这是一种原始的三维模型，它结合了域转换的优势和直接应用样式转移方法来实现多中心脑MR图像协调。IGUANe通过多对一策略，集成了任意数量的域进行训练，扩展了CycleGAN架构。在推断过程中，该模型可以应用于任何图像，甚至来自未知采集点，使其成为协调的通用生成器。在由11台不同扫描仪的T1加权图像组成的数据集上进行训练，IGUANe在未见站点的数据上进行了评估。

    In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the
    
[^43]: FuseMoE：用于灵活多模态融合的专家混合Transformer

    FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion

    [https://arxiv.org/abs/2402.03226](https://arxiv.org/abs/2402.03226)

    本论文提出了一种名为FuseMoE的专家混合Transformer框架，通过创新的门控函数实现灵活融合多模态数据，能够有效地处理缺失模态和不规则采样数据，同时改善模型的预测性能，在临床风险预测任务中具有实际应用价值。

    

    随着机器学习模型在关键领域越来越多地处理多模态数据，它们面临处理多种模态的双重挑战，这些模态经常因缺失元素而不完整，以及收集样本的时间不规则性和稀疏性。成功利用这种复杂数据，同时克服高质量训练样本的稀缺性，是提高这些模型预测性能的关键。我们引入了``FuseMoE''，这是一个集成创新门控函数的专家混合框架。FuseMoE旨在整合多种模态，并且在处理缺失模态和不规则采样数据轨迹的情况下非常有效。在理论上，我们独特的门控函数有助于提高收敛速度，在多个下游任务中表现更好。FuseMoE的实际实用性通过一系列具有挑战性的临床风险预测任务得到验证。

    As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in real world is validated by a challenging set of clinical risk prediction tasks.
    
[^44]: 重复使用批次在两层网络的梯度下降中的好处：打破信息和跳跃指数的诅咒

    The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents

    [https://arxiv.org/abs/2402.03220](https://arxiv.org/abs/2402.03220)

    该论文研究了在两层神经网络中学习多指数目标函数时，重复使用批次的梯度下降（GD）的训练动态。研究发现，与单次GD相比，多次GD能够克服目标函数的限制，仅需两个时间步骤就能实现网络与目标子空间的重叠，展示了在有限时间内有效学习的广泛函数类。这些结果基于动力平均场理论（DMFT）的分析。

    

    本研究探讨了学习多指数目标函数时，两层神经网络的训练动态。我们关注重复多次使用批次的多次梯度下降（GD），并展示它与单次梯度下降相比，显著改变了对于哪些函数是可学习的的结论。具体而言，我们发现具有有限步长的多次GD能够克服目标函数的信息指数（Ben Arous等人，2021）和跳跃指数（Abbe等人，2023）所给出的梯度流和单次GD的限制。我们发现，通过重复使用批次，网络仅需两个时间步骤就能与目标子空间达成重叠，即使函数不满足阶梯性质（Abbe等人，2021）。我们对能够在有限时间内有效学习的（广泛的）函数类进行了表征。我们的结果证明基于动力平均场理论（DMFT）的分析。我们进一步提供了动态的闭式描述。

    We investigate the training dynamics of two-layer neural networks when learning multi-index target functions. We focus on multi-pass gradient descent (GD) that reuses the batches multiple times and show that it significantly changes the conclusion about which functions are learnable compared to single-pass gradient descent. In particular, multi-pass GD with finite stepsize is found to overcome the limitations of gradient flow and single-pass GD given by the information exponent (Ben Arous et al., 2021) and leap exponent (Abbe et al., 2023) of the target function. We show that upon re-using batches, the network achieves in just two time steps an overlap with the target subspace even for functions not satisfying the staircase property (Abbe et al., 2021). We characterize the (broad) class of functions efficiently learned in finite time. The proof of our results is based on the analysis of the Dynamical Mean-Field Theory (DMFT). We further provide a closed-form description of the dynamica
    
[^45]: BGE M3-嵌入：通过自知识蒸馏实现多语言、多功能和多粒度的文本嵌入

    BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation

    [https://arxiv.org/abs/2402.03216](https://arxiv.org/abs/2402.03216)

    BGE M3-嵌入是一种新的多语言、多功能和多粒度的文本嵌入模型，支持超过100种工作语言，并在多语言和跨语言检索任务上取得了最先进的性能。它能够同时执行密集检索、多向量检索和稀疏检索，并能处理不同粒度的输入。其有效训练包括了一种自知识蒸馏方法和优化的批处理策略。

    

    在本文中，我们提出了一种新的嵌入模型，称为M3-嵌入，以其在多语言、多功能和多粒度方面的多样性而著称。它可以支持超过100种工作语言，在多语言和跨语言检索任务上取得了新的最先进性能。它可以同时执行嵌入模型的三种常见检索功能：密集检索、多向量检索和稀疏检索，为现实世界的IR应用提供了统一的模型基础。它能够处理不同粒度的输入，从短句到长达8192个标记的文档。M3-嵌入的有效训练包括以下技术贡献。我们提出了一种新颖的自知识蒸馏方法，可以将来自不同检索功能的相关性分数整合为教师信号，以提高训练质量。我们还优化了批处理策略。

    In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
    
[^46]: 有机或扩散：我们能区分人类艺术和AI生成的图像吗？

    Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?

    [https://arxiv.org/abs/2402.03214](https://arxiv.org/abs/2402.03214)

    这项研究探讨了如何区分人类艺术和AI生成的图像，并提供了几种不同的方法，包括通过监督学习训练的分类器、扩散模型的研究工具以及专业艺术家的知识。这对防止欺诈、遵守政策以及避免模型崩溃都具有重要意义。

    

    生成AI图像的出现完全颠覆了艺术界。从人类艺术中识别AI生成的图像是一个具有挑战性的问题，其影响随着时间的推移而不断增加。未能解决这个问题会导致不良行为者欺诈那些支付高价购买人类艺术品的个人和禁止使用AI图像的公司。这对于需要过滤训练数据以避免潜在模型崩溃的AI模型训练者来说也至关重要。区分人类艺术和AI图像的方法有多种，包括通过监督学习训练的分类器，针对扩散模型的研究工具，以及通过专业艺术家利用他们对艺术技巧的知识进行识别。在本文中，我们试图了解这些方法在现代生成模型的良性和对抗性环境中的表现如何。我们策划了7种风格的真实人类艺术，从5个生成模型生成了与之匹配的图像，并应用了8个检测器。

    The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors 
    
[^47]: 光学与优化的薛定谔桥匹配

    Light and Optimal Schr\"odinger Bridge Matching

    [https://arxiv.org/abs/2402.03207](https://arxiv.org/abs/2402.03207)

    该论文提出了一种新的学习薛定谔桥匹配的方法，克服了现有方法中的限制，通过优化薛定谔桥的参数化来恢复输运计划。

    

    薛定谔桥匹配(SB)近年来引起了机器学习界的关注，作为经典扩散模型的一个有希望的扩展，也与熵最优输运(EOT)相互关联。最近的SB求解器利用了普遍的桥匹配程序。这些程序旨在在只有两个分布之间的输运计划的情况下，恢复一种随机过程来输运质量。特别地，给定EOT计划，这些程序可以被适应用于解决SB。这个事实被最近的研究工作广泛利用，形成了基于匹配的SB求解器。关键就是恢复EOT计划：最近的工作要么使用启发式的近似方法(如小批量输运)或者建立迭代匹配程序，这样设计上在训练过程中累积了误差。我们解决了这些限制，并提出了一种新的学习SB的方法，我们称之为\textbf{优化的薛定谔桥匹配}。它利用了薛定谔桥的最优参数化。

    Schr\"odinger Bridges (SB) have recently gained the attention of the ML community as a promising extension of classic diffusion models which is also interconnected to the Entropic Optimal Transport (EOT). Recent solvers for SB exploit the pervasive bridge matching procedures. Such procedures aim to recover a stochastic process transporting the mass between distributions given only a transport plan between them. In particular, given the EOT plan, these procedures can be adapted to solve SB. This fact is heavily exploited by recent works giving rives to matching-based SB solvers. The cornerstone here is recovering the EOT plan: recent works either use heuristical approximations (e.g., the minibatch OT) or establish iterative matching procedures which by the design accumulate the error during the training. We address these limitations and propose a novel procedure to learn SB which we call the \textbf{optimal Schr\"odinger bridge matching}. It exploits the optimal parameterization of the 
    
[^48]: 多智能体强化学习在多小区大规模MIMO系统中的能量节省

    Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell Massive MIMO Systems

    [https://arxiv.org/abs/2402.03204](https://arxiv.org/abs/2402.03204)

    该论文提出了一种多智能体强化学习算法，通过优化多个大规模MIMO基站的睡眠模式和天线切换，实现在多小区网络中能量的节省，同时保持服务质量。仿真结果表明，这种算法相比基线策略具有更好的性能。

    

    我们开发了一种多智能体强化学习算法，通过对多级高级睡眠模式（ASM）和基站的天线切换进行决策，来最小化多个大规模MIMO（多输入多输出）基站在多小区网络中的总能量消耗，同时保持整体的服务质量（QoS）。将问题建模为分布式部分可观察的马尔可夫决策过程（DEC-POMDP），以实现个体基站之间的协作，这对于处理小区间干扰是必要的。设计了一种多智能体近端策略优化（MAPPO）算法来学习协同基站控制策略。为了提高其可伸缩性，进一步提出了一种称为MAPPO-neighbor策略的修改版本。仿真结果表明，训练的MAPPO智能体相比基线策略取得了更好的性能。特别是，与自动睡眠模式1（符号级休眠）算法相比，MAPPO-neighbor算法实现了更好的性能。

    We develop a multi-agent reinforcement learning (MARL) algorithm to minimize the total energy consumption of multiple massive MIMO (multiple-input multiple-output) base stations (BSs) in a multi-cell network while preserving the overall quality-of-service (QoS) by making decisions on the multi-level advanced sleep modes (ASMs) and antenna switching of these BSs. The problem is modeled as a decentralized partially observable Markov decision process (DEC-POMDP) to enable collaboration between individual BSs, which is necessary to tackle inter-cell interference. A multi-agent proximal policy optimization (MAPPO) algorithm is designed to learn a collaborative BS control policy. To enhance its scalability, a modified version called MAPPO-neighbor policy is further proposed. Simulation results demonstrate that the trained MAPPO agent achieves better performance compared to baseline policies. Specifically, compared to the auto sleep mode 1 (symbol-level sleeping) algorithm, the MAPPO-neighbor
    
[^49]: 用球面高斯约束进行条件扩散引导

    Guidance with Spherical Gaussian Constraint for Conditional Diffusion

    [https://arxiv.org/abs/2402.03201](https://arxiv.org/abs/2402.03201)

    本文提出了一种用球面高斯约束的扩散算法（DSG），解决了在条件生成任务中采样过程中的流形偏离问题。这种算法通过优化将步骤限制在中间数据流形内，并能够使用较大的引导步长。

    

    最近扩散模型的进展尝试通过利用可微的损失函数进行指导来处理条件生成任务，而无需额外的训练。虽然这些方法在一定程度上取得了成功，但它们往往在样本质量上做出妥协，并需要较小的引导步长，导致采样过程变长。本文揭示了在引导损失的采样过程中流形偏离的根本问题所在。我们通过建立损失引导的估计误差的特定下界从理论上证明了流形偏离的存在。为了减轻这个问题，我们提出了带有球面高斯约束（DSG）的扩散，从高维高斯分布的集中现象中汲取灵感。DSG通过优化有效地将引导步骤约束在中间数据流形内，并能够使用较大的引导步长。此外，我们提出了一个闭式公式。

    Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training. While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes. This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed. We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance. To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions. DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps. Furthermore, we present a closed-form 
    
[^50]: 同性质，聚类和分类器

    Isotropy, Clusters, and Classifiers

    [https://arxiv.org/abs/2402.03191](https://arxiv.org/abs/2402.03191)

    同性质的嵌入空间对聚类和线性分类目标具有负面影响，这一事实得到了本文的实证支持，并对文献中的先前结果有所启示。

    

    最近，关于嵌入空间是否均匀利用所有维度（即是否具有同性质）的问题引起了讨论。有证据支持和反对在嵌入空间中实施同性质。在本文中，我们强调同性质对嵌入空间的要求与聚类的存在不兼容，这也对线性分类目标产生了负面影响。我们通过实验证明了这个事实，并用它来阐明文献中的先前结果。

    Whether embedding spaces use all their dimensions equally, i.e., whether they are isotropic, has been a recent subject of discussion. Evidence has been accrued both for and against enforcing isotropy in embedding spaces. In the present paper, we stress that isotropy imposes requirements on the embedding space that are not compatible with the presence of clusters -- which also negatively impacts linear classification objectives. We demonstrate this fact empirically and use it to shed light on previous results from the literature.
    
[^51]: 统一的多模态大型语言模型的幻觉检测

    Unified Hallucination Detection for Multimodal Large Language Models

    [https://arxiv.org/abs/2402.03190](https://arxiv.org/abs/2402.03190)

    该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。

    

    尽管在多模态任务方面取得了重大进展，多模态大型语言模型(MLLMs)仍然存在幻觉的严重问题。因此，可靠地检测MLLMs中的幻觉已成为模型评估和实际应用部署保障的重要方面。之前在这个领域的研究受到了狭窄的任务焦点、不足的幻觉类别涵盖范围以及缺乏详细的细粒度的限制。针对这些挑战，我们的工作扩展了幻觉检测的研究范围。我们提出了一个新颖的元评估基准方法，MHaluBench，精心设计以促进幻觉检测方法的进展评估。此外，我们揭示了一个新颖的统一多模态幻觉检测框架，UNIHD，它利用一套辅助工具来稳健地验证幻觉的发生。我们通过实验证明了UNIHD的有效性。

    Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
    
[^52]: 单一盆地的好处有多大？

    How Good is a Single Basin?

    [https://arxiv.org/abs/2402.03187](https://arxiv.org/abs/2402.03187)

    本文通过构建同一个盆地内的“相连”集成，并通过蒸馏将其他盆地的知识隐含纳入到同一个盆地内，弥补了连接增加对性能的负面影响，重新发现了（多盆地）深度集成的性能。因此推测，在没有从其他盆地学习的情况下，很难有效利用其他盆地的知识。

    

    神经损失景观的多模态性常被认为是深度集成在实践中成功的主要驱动因素。在这项工作中，我们通过构建各种“相连”的集成来探究这种信念，这些集成被限制在同一个盆地内。通过我们的实验，我们证明了增加连接确实会对性能产生负面影响。然而，当通过蒸馏的方式隐含地将其他盆地的知识纳入到同一个盆地中时，我们发现性能差距可以通过再发现（多盆地）深度集成来减轻。因此，我们推测，虽然任何给定盆地中至少部分存在着额外盆地的知识，但在没有从其他盆地中学习它的情况下，很难有效利用它。

    The multi-modal nature of neural loss landscapes is often considered to be the main driver behind the empirical success of deep ensembles. In this work, we probe this belief by constructing various "connected" ensembles which are restricted to lie in the same basin. Through our experiments, we demonstrate that increased connectivity indeed negatively impacts performance. However, when incorporating the knowledge from other basins implicitly through distillation, we show that the gap in performance can be mitigated by re-discovering (multi-basin) deep ensembles within a single basin. Thus, we conjecture that while the extra-basin knowledge is at least partially present in any given basin, it cannot be easily harnessed without learning it from other basins.
    
[^53]: 用顺序元学习预测多个环境下的配置性能

    Predicting Configuration Performance in Multiple Environments with Sequential Meta-learning

    [https://arxiv.org/abs/2402.03183](https://arxiv.org/abs/2402.03183)

    本论文提出了一个顺序元学习框架SeMPL，可以在多个环境下学习和预测给定软件配置的性能。与现有方法不同的是，SeMPL通过依次顺序训练元环境，实现了更准确的对新环境的性能预测。

    

    学习和预测给定软件配置的性能对许多软件工程活动非常重要。当前的工作通常在单个环境下构建性能模型或未能正确处理来自不同环境的数据，从而限制了对新环境的准确性。本文针对多个环境下的配置性能学习，设计了SeMPL - 一种元学习框架，它从不同(meta)环境中测量的配置中学习共同的理解，并将它们推广到未知的目标环境中。其独特之处在于，与常见的元学习框架（如MAML和MetaSGD）并行训练元环境不同，我们依次顺序训练它们。训练顺序自然地允许区分

    Learning and predicting the performance of given software configurations are of high importance to many software engineering activities. While configurable software systems will almost certainly face diverse running environments (e.g., version, hardware, and workload), current work often either builds performance models under a single environment or fails to properly handle data from diverse settings, hence restricting their accuracy for new environments. In this paper, we target configuration performance learning under multiple environments. We do so by designing SeMPL - a meta-learning framework that learns the common understanding from configurations measured in distinct (meta) environments and generalizes them to the unforeseen, target environment. What makes it unique is that unlike common meta-learning frameworks (e.g., MAML and MetaSGD) that train the meta environments in parallel, we train them sequentially, one at a time. The order of training naturally allows discriminating t
    
[^54]: 通过大型语言模型增强时间序列分析：一项调查

    Empowering Time Series Analysis with Large Language Models: A Survey

    [https://arxiv.org/abs/2402.03182](https://arxiv.org/abs/2402.03182)

    该调查提供了对利用大型语言模型（LLMs）进行时间序列分析的现有方法的系统概述。最近的进展表明，预训练的LLMs可以捕捉复杂的时间序列数据依赖关系，并促进各种应用。

    

    最近，大型语言模型（LLMs）取得了显著进展，在各种自然语言任务中展示出了前所未有的能力。然而，对于时间序列分析来说，从头开始完全训练一个大型通用模型具有挑战性，原因是时间序列数据的规模和种类庞大，以及导致概念漂移的非稳态，阻碍了连续模型适应和重新训练。最近的进展表明，预训练的LLMs可以利用复杂的时间序列数据依赖关系，并促进各种应用。在这项调查中，我们系统概述了利用LLMs进行时间序列分析的现有方法。具体而言，我们首先阐述了在时间序列上应用语言模型的挑战和动机，以及LLMs的简要基础知识。接下来，我们总结了基于LLMs的时间序列分析的一般流程，并将现有方法分类。

    Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks. However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training. Recent advances have shown that pre-trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications. In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis. Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of LLMs. Next, we summarize the general pipeline for LLM-based time series analysis, categorize existing methods into different
    
[^55]: Cool-chic视频：通过800个参数学习视频编码

    Cool-chic video: Learned video coding with 800 parameters

    [https://arxiv.org/abs/2402.03179](https://arxiv.org/abs/2402.03179)

    我们提出了一个轻量级的学习视频编码器，使用800个参数和900次乘法来实现低解码复杂度。该编码器在压缩视频时能够利用时间冗余，并在接近AVC的速率失真条件下表现优于其他过拟合编解码器。

    

    我们提出了一种轻量级的通过学习的视频编解码器，每个解码像素有900次乘法，总共有800个参数。据我们所知，这是一个解码复杂度最低的神经视频编解码器之一。它基于过拟合的图片编解码器Cool-chic，并通过增加一个时域编码模块来强化视频的时间冗余。所提出的模型能够压缩视频以实现低延迟和随机访问配置，并在接近AVC的速率失真条件下优于其他过拟合编解码器，如FFNeRV。该系统是开源的：orange-opensource.github.io/Cool-Chic。

    We propose a lightweight learned video codec with 900 multiplications per decoded pixel and 800 parameters overall. To the best of our knowledge, this is one of the neural video codecs with the lowest decoding complexity. It is built upon the overfitted image codec Cool-chic and supplements it with an inter coding module to leverage the video's temporal redundancies. The proposed model is able to compress videos using both low-delay and random access configurations and achieves rate-distortion close to AVC while out-performing other overfitted codecs such as FFNeRV. The system is made open-source: orange-opensource.github.io/Cool-Chic.
    
[^56]: CIDAR: 阿拉伯文的文化相关指令数据集

    CIDAR: Culturally Relevant Instruction Dataset For Arabic

    [https://arxiv.org/abs/2402.03177](https://arxiv.org/abs/2402.03177)

    CIDAR是第一个由人工评审对齐文化的阿拉伯指令调优数据集，目的是解决现有指令数据集对西方文化的固有偏见所带来的影响，对于丰富将语言模型与阿拉伯文化对齐的研究工作具有重要意义。

    

    指令调优已成为教授大型语言模型遵循指令的一种重要方法。然而，现有的指令数据集主要面向英语或者来源于以英语为主导的语言模型，导致对西方文化的固有偏见。这种偏见对阿拉伯文等非英语语言的语言结构产生了重大影响，阿拉伯文反映了阿拉伯地区多样文化的独特语法。本文通过引入CIDAR，即第一个由人工评审对齐文化的阿拉伯指令调优数据集（https://hf.co/datasets/arbml/CIDAR），来解决这一限制。CIDAR包含了代表阿拉伯地区的10000个指令与输出对。我们通过对比分析其他模型在其他数据集上的调优结果，讨论了CIDAR的文化相关性。实验证明CIDAR可以帮助丰富将语言模型与阿拉伯文化对齐的研究工作。所有代码都可在...上找到。

    Instruction tuning has emerged as a prominent methodology for teaching Large Language Models (LLMs) to follow instructions. However, current instruction datasets predominantly cater to English or are derived from English-dominated LLMs, resulting in inherent biases toward Western culture. This bias significantly impacts the linguistic structures of non-English languages such as Arabic, which has a distinct grammar reflective of the diverse cultures across the Arab region. This paper addresses this limitation by introducing CIDAR: https://hf.co/datasets/arbml/CIDAR, the first open Arabic instruction-tuning dataset culturally-aligned by human reviewers. CIDAR contains 10,000 instruction and output pairs that represent the Arab region. We discuss the cultural relevance of CIDAR via the analysis and comparison to other models fine-tuned on other datasets. Our experiments show that CIDAR can help enrich research efforts in aligning LLMs with the Arabic culture. All the code is available at 
    
[^57]: 银行业背景下主题建模方法的比较

    Comparison of Topic Modelling Approaches in the Banking Context

    [https://arxiv.org/abs/2402.03176](https://arxiv.org/abs/2402.03176)

    本研究在银行业背景下比较了主题建模方法。通过使用KernelPCA和K-means Clustering在BERTopic架构中，我们得到了连贯的主题，连贯性得分为0.8463。

    

    主题建模是许多应用程序中自动提取主题的重要任务，例如情感分析和推荐系统。该方法对于服务行业来说是至关重要的，可以用于监控客户讨论。传统方法如潜在狄利克雷分配（Latent Dirichlet Allocation，LDA）在主题发现方面表现出色，但由于数据稀疏性和无法对文档中的单词顺序建模，它们的结果并不一致。因此，本研究提出了在BERTopic架构中使用核主成分分析（Kernel Principal Component Analysis，KernelPCA）和K-means聚类的方法。我们使用尼日利亚银行客户的推文准备了一个新数据集，然后使用该数据集来比较主题建模方法。我们的研究结果显示，在BERTopic架构中使用KernelPCA和K-means可以生成连贯的主题，其中连贯性得分为0.8463。

    Topic modelling is a prominent task for automatic topic extraction in many applications such as sentiment analysis and recommendation systems. The approach is vital for service industries to monitor their customer discussions. The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for topic discovery has shown great performances, however, they are not consistent in their results as these approaches suffer from data sparseness and inability to model the word order in a document. Thus, this study presents the use of Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the BERTopic architecture. We have prepared a new dataset using tweets from customers of Nigerian banks and we use this to compare the topic modelling approaches. Our findings showed KernelPCA and K-means in the BERTopic architecture-produced coherent topics with a coherence score of 0.8463.
    
[^58]: The Matrix: 一个用于LLMs的贝叶斯学习模型

    The Matrix: A Bayesian learning model for LLMs

    [https://arxiv.org/abs/2402.03175](https://arxiv.org/abs/2402.03175)

    本文介绍了一个贝叶斯学习模型，用于理解大型语言模型（LLMs）的行为。研究探索了LLMs的优化指标，并开发了一个新的基于预测下一个标记的模型。实验结果表明，LLMs的行为与贝叶斯学习一致，为上下文学习提供了新的见解。

    

    本文介绍了一个用于理解大型语言模型（LLMs）行为的贝叶斯学习模型。我们探索了基于预测下一个标记的LLM的优化指标，并开发了一个以此原则为基础的新型模型。我们的方法涉及构建一个由先验和多项式转移概率矩阵表示的理想生成文本模型，并研究LLMs如何逼近该矩阵。我们讨论了嵌入和多项式分布之间的映射的连续性，并提出了Dirichlet逼近定理来逼近任何先验。此外，我们演示了LLMs的文本生成如何与贝叶斯学习原理一致，并深入探讨了其在上下文学习中的影响，具体解释了为什么在更大的模型中出现了上下文学习，其中提示被视为需要更新的样本。我们的发现表明，LLMs的行为与贝叶斯学习一致，提供了新的见解。

    In this paper, we introduce a Bayesian learning model to understand the behavior of Large Language Models (LLMs). We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle. Our approach involves constructing an ideal generative text model represented by a multinomial transition probability matrix with a prior, and we examine how LLMs approximate this matrix. We discuss the continuity of the mapping between embeddings and multinomial distributions, and present the Dirichlet approximation theorem to approximate any prior. Additionally, we demonstrate how text generation by LLMs aligns with Bayesian learning principles and delve into the implications for in-context learning, specifically explaining why in-context learning emerges in larger models where prompts are considered as samples to be updated. Our findings indicate that the behavior of LLMs is consistent with Bayesian Learning, offering new insights
    
[^59]: 基于分布式事件触发在线学习的高斯过程回归多智能体系统安全共识

    Decentralized Event-Triggered Online Learning for Safe Consensus of Multi-Agent Systems with Gaussian Process Regression

    [https://arxiv.org/abs/2402.03174](https://arxiv.org/abs/2402.03174)

    本文提出了一种分布式事件触发在线学习的方法，通过辅助动力学和高斯过程回归实现多智能体系统的共识控制，并通过Lyapunov理论提供了概率保证的预测误差界限。

    

    多智能体系统中的共识控制已经引起了广泛的关注和实际应用。然而，在未知动力学下管理共识控制仍然是控制设计的重大挑战，原因是系统不确定性和环境扰动。本文提出了一种新颖的基于学习的分布式控制法则，通过辅助动力学来增强。利用高斯过程来补偿多智能体系统中的未知成分。为了持续提高高斯过程模型的预测性能，提出了一种数据高效的在线学习策略和分布式事件触发机制。此外，基于概率保证预测误差边界的Lyapunov理论确保了所提方法的控制性能。为了证明所提出的基于学习的控制器的有效性，进行了比较分析，并与传统的分布式控制器进行了对比。

    Consensus control in multi-agent systems has received significant attention and practical implementation across various domains. However, managing consensus control under unknown dynamics remains a significant challenge for control design due to system uncertainties and environmental disturbances. This paper presents a novel learning-based distributed control law, augmented by an auxiliary dynamics. Gaussian processes are harnessed to compensate for the unknown components of the multi-agent system. For continuous enhancement in predictive performance of Gaussian process model, a data-efficient online learning strategy with a decentralized event-triggered mechanism is proposed. Furthermore, the control performance of the proposed approach is ensured via the Lyapunov theory, based on a probabilistic guarantee for prediction error bounds. To demonstrate the efficacy of the proposed learning-based controller, a comparative analysis is conducted, contrasting it with both conventional distri
    
[^60]: 马格里布情感分析器上的同形异义词攻击研究

    Homograph Attacks on Maghreb Sentiment Analyzers

    [https://arxiv.org/abs/2402.03171](https://arxiv.org/abs/2402.03171)

    同形异义词攻击对马格里布情感分析器造成了严重影响，将其性能从F1得分0.95降低到0.33。本研究主要旨在强调LLMs的弱点，并优先考虑机器学习的道德和责任。

    

    我们研究了同形异义词攻击对马格里布北非国家不同阿拉伯方言情感分析（SA）任务的影响。当数据以“阿拉伯字母拼音”书写时，同形异义词攻击导致变压器分类性能从F1得分0.95下降到0.33，减少了65.3%。本研究的目标是凸显LLMs的弱点，并优先考虑机器学习的道德和责任。

    We examine the impact of homograph attacks on the Sentiment Analysis (SA) task of different Arabic dialects from the Maghreb North-African countries. Homograph attacks result in a 65.3% decrease in transformer classification from an F1-score of 0.95 to 0.33 when data is written in "Arabizi". The goal of this study is to highlight LLMs weaknesses' and to prioritize ethical and responsible Machine Learning.
    
[^61]: Mamba能否进行上下文学习？

    Is Mamba Capable of In-Context Learning?

    [https://arxiv.org/abs/2402.03170](https://arxiv.org/abs/2402.03170)

    本研究证明，新提出的选择性结构化状态空间模型Mamba具有与transformers类似的上下文学习（ICL）能力。对于涉及较长输入序列的ICL任务，Mamba可以成为transformers的高效替代品。

    

    本研究提供了经验证据，证明了新提出的选择性结构化状态空间模型Mamba具有与transformers类似的上下文学习（ICL）能力。我们在涉及简单函数逼近以及更复杂的自然语言处理问题的任务上评估了Mamba。我们的结果表明，在这两类任务中，Mamba在ICL方面的性能与transformer模型相匹配。进一步的分析揭示，类似transformers，Mamba似乎通过逐步优化其内部表示来解决ICL问题。总体而言，我们的研究表明，对于涉及较长输入序列的ICL任务，Mamba可以成为transformers的高效替代品。

    This work provides empirical evidence that Mamba, a newly proposed selective structured state space model, has similar in-context learning (ICL) capabilities as transformers. We evaluated Mamba on tasks involving simple function approximation as well as more complex natural language processing problems. Our results demonstrate that across both categories of tasks, Mamba matches the performance of transformer models for ICL. Further analysis reveals that like transformers, Mamba appears to solve ICL problems by incrementally optimizing its internal representations. Overall, our work suggests that Mamba can be an efficient alternative to transformers for ICL tasks involving longer input sequences.
    
[^62]: 低多线性秩张量逼近的随机矩阵方法

    A Random Matrix Approach to Low-Multilinear-Rank Tensor Approximation

    [https://arxiv.org/abs/2402.03169](https://arxiv.org/abs/2402.03169)

    该研究采用随机矩阵方法，在低多线性秩张量逼近中展示了对种植的低秩信号的估计，并根据大维谱行为和信噪比准确预测了重建性能，并给出了HOOI收敛的充分条件。

    

    本研究从计算阈值附近的一般尖峰张量模型，对种植的低秩信号估计进行了全面的认识。依靠大型随机矩阵理论的标准工具，我们表征了数据张量的展开的大维谱行为，并展示了决定主要信号方向可检测性的相关信噪比。这些结果可以准确地预测在非平凡区域的截断多线性奇异值分解(MLSVD)的重建性能。这一点尤其重要，因为它作为更高阶正交迭代(HOOI)方案的初始化，其收敛到最佳低多线性秩逼近完全取决于其初始化。我们给出了HOOI收敛的充分条件，并证明在大维极限下收敛前的迭代次数趋于1。

    This work presents a comprehensive understanding of the estimation of a planted low-rank signal from a general spiked tensor model near the computational threshold. Relying on standard tools from the theory of large random matrices, we characterize the large-dimensional spectral behavior of the unfoldings of the data tensor and exhibit relevant signal-to-noise ratios governing the detectability of the principal directions of the signal. These results allow to accurately predict the reconstruction performance of truncated multilinear SVD (MLSVD) in the non-trivial regime. This is particularly important since it serves as an initialization of the higher-order orthogonal iteration (HOOI) scheme, whose convergence to the best low-multilinear-rank approximation depends entirely on its initialization. We give a sufficient condition for the convergence of HOOI and show that the number of iterations before convergence tends to $1$ in the large-dimensional limit.
    
[^63]: 图上的去中心化双级优化: 无环算法更新和瞬态迭代复杂性

    Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity

    [https://arxiv.org/abs/2402.03167](https://arxiv.org/abs/2402.03167)

    本文提出了一种单循环的去中心化双级优化算法（D-SOBA），首次阐明了网络拓扑和数据异构性对去中心化双级算法的共同影响。D-SOBA在渐近速率、渐近梯度/海森复杂性和瞬态梯度/海森复杂性方面达到了最先进水平。

    

    随机双级优化（SBO）在处理嵌套结构方面的多样性使其在机器学习中变得越来越重要。为了解决大规模SBO，去中心化方法作为有效的范例出现，其中节点与直接相邻节点进行通信，无需中央服务器，从而提高通信效率和增强算法的稳健性。然而，当前的去中心化SBO算法面临挑战，包括昂贵的内部循环更新和对网络拓扑、数据异构性和嵌套双级算法结构的影响不明确。在本文中，我们引入了一种单循环的去中心化SBO（D-SOBA）算法，并建立了其瞬态迭代复杂性，首次澄清了网络拓扑和数据异构性对去中心化双级算法的共同影响。D-SOBA实现了最先进的渐近速率、渐近梯度/海森复杂性和瞬态梯度/海森复杂性。

    Stochastic bilevel optimization (SBO) is becoming increasingly essential in machine learning due to its versatility in handling nested structures. To address large-scale SBO, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness. However, current decentralized SBO algorithms face challenges, including expensive inner-loop updates and unclear understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures. In this paper, we introduce a single-loop decentralized SBO (D-SOBA) algorithm and establish its transient iteration complexity, which, for the first time, clarifies the joint influence of network topology and data heterogeneity on decentralized bilevel algorithms. D-SOBA achieves the state-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, and transien
    
[^64]: 最优和近似最优的自适应向量量化

    Optimal and Near-Optimal Adaptive Vector Quantization

    [https://arxiv.org/abs/2402.03158](https://arxiv.org/abs/2402.03158)

    该论文提出了最优和近似最优的自适应向量量化算法，能够优化量化过程，在时间和空间复杂度上具有改进，可扩展应用于各种机器学习任务。

    

    量化是许多机器学习用例中的基本优化，包括压缩梯度、模型权重和激活以及数据集。最准确的量化形式是“自适应”，其中通过最小化相对于给定输入的误差来优化，而不是针对最坏情况进行优化。然而，最优的自适应量化方法在运行时间和内存需求方面被认为是不可行的。我们重新审视了自适应向量量化（AVQ）问题，并提出了一种能够在渐近改进的时间和空间复杂度下找到最优解的算法。我们还提出了一种更快速的近似最优算法，以处理大输入。我们的实验表明，我们的算法可能会在各种机器学习应用中更广泛地使用AVQ。

    Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.   We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.
    
[^65]: DogSurf: 四足机器人可识别表面的GRU神经网络用于盲人导航

    DogSurf: Quadruped Robot Capable of GRU-based Surface Recognition for Blind Person Navigation

    [https://arxiv.org/abs/2402.03156](https://arxiv.org/abs/2402.03156)

    DogSurf是一种四足机器人的新方法，可以帮助视力受损的人在现实世界中导航，在表面识别和分类任务中具有99.925%的准确率。

    

    本文介绍了一种新的方法，即使用四足机器人来帮助视力受损的人在现实世界中进行导航。所提出的方法可以让四足机器人检测到滑溜的表面，并使用声音和触觉反馈告知用户何时停下。针对四足机器人的多类表面分类任务，提出了一个准确率为99.925%的最先进的GRU神经网络架构。在Unitree Go1 Edu机器人上收集了一个数据集，该数据集和代码已在公共领域发布。

    This paper introduces DogSurf - a newapproach of using quadruped robots to help visually impaired people navigate in real world. The presented method allows the quadruped robot to detect slippery surfaces, and to use audio and haptic feedback to inform the user when to stop. A state-of-the-art GRU-based neural network architecture with mean accuracy of 99.925% was proposed for the task of multiclass surface classification for quadruped robots. A dataset was collected on a Unitree Go1 Edu robot. The dataset and code have been posted to the public domain.
    
[^66]: 使用物理合理的神经网络学习参数化的Navier-Stokes方程的解决方案

    Learning solutions of parametric Navier-Stokes with physics-informed neural networks

    [https://arxiv.org/abs/2402.03153](https://arxiv.org/abs/2402.03153)

    本论文利用物理合理的神经网络学习参数化的Navier-Stokes方程的解决方案函数，并通过生成的数值解进行训练，实现了对一系列参数下的速度和压力函数的插值。

    

    我们利用物理合理的神经网络（PINNs）来学习参数化的Navier-Stokes方程（NSE）的解决方案函数。我们提出的方法在可行的优化问题设置中绕过了PINNs在收敛到高度非线性参数化PDEs（如NSE）的解决方案时的局限性。我们将感兴趣的参数作为PINNs的输入，与时空坐标一起训练PINNs，使用参数的实例生成的数值解的训练数据对PINNs进行训练。我们在经典的二维圆柱流问题上进行实验，旨在学习在一定范围的雷诺数作为感兴趣参数时的速度和压力函数。使用来自生成的数值模拟的训练数据，可以对一系列参数进行解决方案函数的插值。因此，我们将PINNs与无约束的传统神经网络（NN）在这个问题设置上进行比较，以研究考虑PDE的正则化的效果。

    We leverage Physics-Informed Neural Networks (PINNs) to learn solution functions of parametric Navier-Stokes Equations (NSE). Our proposed approach results in a feasible optimization problem setup that bypasses PINNs' limitations in converging to solutions of highly nonlinear parametric-PDEs like NSE. We consider the parameter(s) of interest as inputs of PINNs along with spatio-temporal coordinates, and train PINNs on generated numerical solutions of parametric-PDES for instances of the parameters. We perform experiments on the classical 2D flow past cylinder problem aiming to learn velocities and pressure functions over a range of Reynolds numbers as parameter of interest. Provision of training data from generated numerical simulations allows for interpolation of the solution functions for a range of parameters. Therefore, we compare PINNs with unconstrained conventional Neural Networks (NN) on this problem setup to investigate the effectiveness of considering the PDEs regularization 
    
[^67]: 基于微环的相干光学GEMM加速器的比较分析

    A Comparative Analysis of Microrings Based Incoherent Photonic GEMM Accelerators

    [https://arxiv.org/abs/2402.03149](https://arxiv.org/abs/2402.03149)

    本文详细分析了基于微环的相干光学GEMM加速器的组织，其通过分裂、聚合、调制、加权和求和等方式操作光信号以加速深度神经网络中的矩阵-矩阵乘法，提高吞吐量和能量效率。

    

    提出了几种基于微环谐振器（MRR）的模拟光学架构，以在深度神经网络中加速通用的矩阵-矩阵乘法（GEMM），具有出色的吞吐量和能量效率。为了实现GEMM功能，这些基于MRR的架构一般通过五种不同的方式操作光信号：（i）将多个光信号分裂（复制）以达到某种多分支，（ii）将多个光信号聚合（复用）以达到某种多输入，（iii）调制光信号以将输入值印置于模拟信号幅度上，（iv）对调制的光信号进行加权，以实现模拟输入权重相乘，（v）对光信号进行求和。MRR基于的GEMM加速器以任意顺序执行前四种信号操作，忽略了这些操作顺序对其性能的可能影响。本文对加速器组织进行了详细分析。

    Several microring resonator (MRR) based analog photonic architectures have been proposed to accelerate general matrix-matrix multiplications (GEMMs) in deep neural networks with exceptional throughput and energy efficiency. To implement GEMM functions, these MRR-based architectures, in general, manipulate optical signals in five different ways: (i) Splitting (copying) of multiple optical signals to achieve a certain fan-out, (ii) Aggregation (multiplexing) of multiple optical signals to achieve a certain fan-in, (iii) Modulation of optical signals to imprint input values onto analog signal amplitude, (iv) Weighting of modulated optical signals to achieve analog input-weight multiplication, (v) Summation of optical signals. The MRR-based GEMM accelerators undertake the first four ways of signal manipulation in an arbitrary order ignoring the possible impact of the order of these manipulations on their performance. In this paper, we conduct a detailed analysis of accelerator organization
    
[^68]: 用于稳健学习模型预测的多步损失函数

    A Multi-step Loss Function for Robust Learning of the Dynamics in Model-based Reinforcement Learning

    [https://arxiv.org/abs/2402.03146](https://arxiv.org/abs/2402.03146)

    本文提出了一种用于稳健学习模型预测的多步损失函数，通过加权平均均方误差损失在不同未来时间点上来训练单步模型。这种损失函数在存在噪声的情况下尤为有效，并在各种任务中取得了显著的预测改善。

    

    在基于模型的强化学习中，大多数算法依赖于从数据学到的单步动力学模型模拟轨迹。这种方法的一个关键挑战是随着轨迹长度的增加，单步预测误差的累积问题。本文通过使用多步目标来训练单步模型来解决这个问题。我们的目标是在不同的未来时间点上加权平均的均方误差(MSE)损失函数。我们发现，这种新的损失函数在数据存在噪声的情况下特别有用（观测上的加性高斯噪声），而这种情况在现实环境中经常出现。为了支持多步损失函数，首先我们在两种可处理的情况下研究了它的性质：i）一维线性系统，和ii）两参数非线性系统。其次，我们在各种任务（环境或数据集）中展示了使用这种损失函数训练的模型在未来预测中的平均R2得分方面取得了显著的改善。

    In model-based reinforcement learning, most algorithms rely on simulating trajectories from one-step models of the dynamics learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as the length of the trajectory grows. In this paper we tackle this issue by using a multi-step objective to train one-step models. Our objective is a weighted sum of the mean squared error (MSE) loss at various future horizons. We find that this new loss is particularly useful when the data is noisy (additive Gaussian noise in the observations), which is often the case in real-life environments. To support the multi-step loss, first we study its properties in two tractable cases: i) uni-dimensional linear system, and ii) two-parameter non-linear system. Second, we show in a variety of tasks (environments or datasets) that the models learned with this loss achieve a significant improvement in terms of the averaged R2-score on future prediction horizons. Finally,
    
[^69]: SafEDMD：一种专为非线性动态系统数据驱动控制而设计的认证学习架构

    SafEDMD: A certified learning architecture tailored to data-driven control of nonlinear dynamical systems

    [https://arxiv.org/abs/2402.03145](https://arxiv.org/abs/2402.03145)

    SafEDMD是一种基于EDMD的学习架构，通过稳定性和认证导向，生成可靠的数据驱动替代模型，并基于半定规划进行认证控制器设计。它在多个基准示例上展示了优于现有方法的优势。

    

    Koopman算子作为机器学习动态控制系统的理论基础，其中算子通过扩展动态模态分解（EDMD）启发式近似。在本文中，我们提出了稳定性和认证导向的EDMD（SafEDMD）：一种新颖的基于EDMD的学习架构，它提供了严格的证书，从而以数据驱动的方式生成可靠的替代模型。为了确保SafEDMD的可靠性，我们推导出比例误差界限，这些界限在原点处消失，并且适用于控制任务，从而基于半定规划进行认证控制器设计。我们通过几个基准示例说明了所开发的机制，并强调其相对于现有方法的优势。

    The Koopman operator serves as the theoretical backbone for machine learning of dynamical control systems, where the operator is heuristically approximated by extended dynamic mode decomposition (EDMD). In this paper, we propose Stability- and certificate-oriented EDMD (SafEDMD): a novel EDMD-based learning architecture which comes along with rigorous certificates, resulting in a reliable surrogate model generated in a data-driven fashion. To ensure trustworthiness of SafEDMD, we derive proportional error bounds, which vanish at the origin and are tailored for control tasks, leading to certified controller design based on semi-definite programming. We illustrate the developed machinery by means of several benchmark examples and highlight the advantages over state-of-the-art methods.
    
[^70]: "少即是多：一种针对大型语言模型的通用简单非参数剪枝算法"

    Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models

    [https://arxiv.org/abs/2402.03142](https://arxiv.org/abs/2402.03142)

    本文提出了一种简单、通用、非参数的剪枝算法KEN，它能在保持模型性能的同时大幅节省内存，通过选择性地保留最重要的参数实现了对transformer模型的优化。与其他方法相比，KEN在最少参数减少25%的情况下实现了与原始模型相等或更好的性能。

    

    神经网络剪枝由于神经网络模型的复杂性以及在各个领域的广泛应用而变得越来越重要。现有的剪枝算法通常存在架构特异性、过度复杂和依赖复杂计算等限制，使它们在实际应用中变得不可行。本文提出了基于核密度估计（KDE）的简单、通用、非结构化剪枝算法KEN。KEN的目标是通过有选择性地保留最重要的参数，同时将其他参数恢复到预训练状态，从而构建优化后的transformer模型。这种方法在保持模型性能的同时，只存储优化后的子网络，实现了显著的内存节省。对七个transformer模型进行了广泛的评估，结果表明KEN在最少参数减少25%的情况下实现了与原始模型相等或更好的性能。与其他方法进行了深入对比。

    Neural network pruning has become increasingly crucial due to the complexity of neural network models and their widespread use in various fields. Existing pruning algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on complex calculations, rendering them impractical for real-world applications. In this paper, we propose KEN: a straightforward, universal and unstructured pruning algorithm based on Kernel Density Estimation (KDE). KEN aims to construct optimized transformer models by selectively preserving the most significant parameters while restoring others to their pre-training state. This approach maintains model performance while allowing storage of only the optimized subnetwork, leading to significant memory savings. Extensive evaluations on seven transformer models demonstrate that KEN achieves equal or better performance than the original models with a minimum parameter reduction of 25%. In-depth comparisons against other 
    
[^71]: 使用辅助短时延任务提升长时延强化学习

    Boosting Long-Delayed Reinforcement Learning with Auxiliary Short-Delayed Task

    [https://arxiv.org/abs/2402.03141](https://arxiv.org/abs/2402.03141)

    这篇论文提出了一种名为Auxiliary-Delayed Reinforcement Learning (AD-RL)的方法，通过利用辅助的短时延任务来加速长时延任务的学习过程，同时在随机环境中保持性能。该方法能显著降低样本复杂度，并在确定性和随机基准测试中表现出优异的样本效率和性能。

    

    延迟情景下的强化学习是具有挑战性的，延迟情景是指观察和交互存在延迟的常见实际情况。现有技术中，状态增强技术在延迟步骤中可能会出现状态空间扩大或在随机环境中性能下降的问题。为了解决这些挑战，我们提出了一种新颖的Auxiliary-Delayed Reinforcement Learning（AD-RL），利用一个辅助的短时延任务来加速长时延任务的学习，同时不损害在随机环境中的性能。具体来说，AD-RL在短时延任务中学习值函数，然后将其与长时延任务中的自举和策略改进技术结合起来。我们理论上证明，与直接在原始长时延任务上学习相比，这样做可以大大减小样本复杂度。在确定性和随机基准测试中，我们的方法在样本效率和性能方面明显优于现有技术。

    Reinforcement learning is challenging in delayed scenarios, a common real-world situation where observations and interactions occur with delays. State-of-the-art (SOTA) state-augmentation techniques either suffer from the state-space explosion along with the delayed steps, or performance degeneration in stochastic environments. To address these challenges, our novel Auxiliary-Delayed Reinforcement Learning (AD-RL) leverages an auxiliary short-delayed task to accelerate the learning on a long-delayed task without compromising the performance in stochastic environments. Specifically, AD-RL learns the value function in the short-delayed task and then employs it with the bootstrapping and policy improvement techniques in the long-delayed task. We theoretically show that this can greatly reduce the sample complexity compared to directly learning on the original long-delayed task. On deterministic and stochastic benchmarks, our method remarkably outperforms the SOTAs in both sample efficienc
    
[^72]: 提升神经子集选择：将背景信息融入到集合表示中

    Enhancing Neural Subset Selection: Integrating Background Information into Set Representations

    [https://arxiv.org/abs/2402.03139](https://arxiv.org/abs/2402.03139)

    这项研究提出了一种能够将背景信息融入神经子集选择任务中的方法，通过将超集的不变量统计量纳入所关注的子集，实现了对特定超级子集的识别。

    

    学习神经子集选择任务，如AI辅助药物发现中的化合物选择，已经在各种应用中变得越来越重要。该领域中现有的方法主要集中于构建模型，捕捉效用函数值与其相应超集中子集之间的关系。然而，这些方法在利用神经网络建模集合函数时往往忽视了超集中包含的有价值信息。在这项工作中，我们采用概率论的观点来解决这个问题。我们的理论发现表明，当目标值在输入集合和子集的条件下时，将超集的不变量统计量纳入所关注的子集是有效学习的关键。这确保输出值对于子集及其相应的超集的排列是不变的，从而能够识别特定的超级子集。

    Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an \textit{invariant sufficient statistic} of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific super
    
[^73]: Just Cluster It: 一种使用聚类和预训练表示进行高维空间探索的方法

    Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations

    [https://arxiv.org/abs/2402.03138](https://arxiv.org/abs/2402.03138)

    本文提出了一种基于聚类和预训练表示的方法，用于在高维空间中进行探索。通过对随机和预训练表示进行聚类，可以有效计算状态数，特别是在多维环境中预训练表示更加有效。

    

    本文从表征为中心的角度探讨了强化学习中探索问题，将探索视为密度估计问题。我们研究了在三维环境中使用聚类来进行探索的有效性，基于一个观察：在三维环境中，与二维环境相比，状态转换中的像素变化的重要性不那么明显。我们提出了一种方法，通过对随机表示和预训练DINO表示进行周期性和全局聚类来计算状态数，即估计伪计数。令人惊讶的是，即使是随机特征也可以在三维环境中有效地进行聚类以计算状态数，然而当这些特征变得更加复杂时，预训练DINO表示由于其预训练的归纳偏差在表示中更加有效。总体而言，这为集成预训练表示提供了一条路径。

    In this paper we adopt a representation-centric perspective on exploration in reinforcement learning, viewing exploration fundamentally as a density estimation problem. We investigate the effectiveness of clustering representations for exploration in 3-D environments, based on the observation that the importance of pixel changes between transitions is less pronounced in 3-D environments compared to 2-D environments, where pixel changes between transitions are typically distinct and significant. We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts. Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations. Overall, this presents a pathway for integrating pre-t
    
[^74]: 社会语言学信息的解释性: 以Hinglish情感分类为例的案例研究

    Sociolinguistically Informed Interpretability: A Case Study on Hinglish Emotion Classification

    [https://arxiv.org/abs/2402.03137](https://arxiv.org/abs/2402.03137)

    通过研究Hinglish情感分类，我们发现预训练语言模型能够学习到语言选择与情感表达之间的关联，尤其是在混合语言数据存在时。这对于情感分类的解释性具有重要意义。

    

    情感分类是自然语言处理中一项具有挑战性的任务，因为语言表达具有固有的特殊性和主观性，尤其是在混合语言数据中。预训练语言模型（PLMs）已经在许多任务和语言中取得了很高的性能，但是否这些模型能够学习和适应不同语言间情感表达的差异仍然有待考验。社会语言学研究表明，Hinglish说话者在表达消极情绪时转用印地语，在表达积极情绪时转用英语。为了了解语言模型是否能学习这些关联，我们研究了3个PLMs在一个Hinglish情感分类数据集上语言对情感预测的影响。通过使用LIME和基于词元的语言ID，我们发现模型确实学习到了语言选择和情感表达之间的关联。此外，当任务特定数据稀缺时，预训练模型中存在混合语言数据可以增强这种学习。我们还得出结论，使用社会语言学信息可以提高情感分类的解释性。

    Emotion classification is a challenging task in NLP due to the inherent idiosyncratic and subjective nature of linguistic expression, especially with code-mixed data. Pre-trained language models (PLMs) have achieved high performance for many tasks and languages, but it remains to be seen whether these models learn and are robust to the differences in emotional expression across languages. Sociolinguistic studies have shown that Hinglish speakers switch to Hindi when expressing negative emotions and to English when expressing positive emotions. To understand if language models can learn these associations, we study the effect of language on emotion prediction across 3 PLMs on a Hinglish emotion classification dataset. Using LIME and token level language ID, we find that models do learn these associations between language choice and emotional expression. Moreover, having code-mixed data present in the pre-training can augment that learning when task-specific data is scarce. We also concl
    
[^75]: 用于跨语言标签投影的约束解码

    Constrained Decoding for Cross-lingual Label Projection

    [https://arxiv.org/abs/2402.03131](https://arxiv.org/abs/2402.03131)

    本文提出了一种解决零样本跨语言迁移学习中翻译质量下降问题的方法。

    

    在没有标记训练数据的情况下，利用多语言LLM进行零样本跨语言迁移已成为一种流行的学习范式，用于低资源语言。然而，在涉及对单词和短语进行细粒度预测的NLP任务中，零样本跨语言迁移学习的性能远远落后于监督微调方法。因此，通常利用翻译和标签投影来进一步提高性能，具体来说(1)将可用的以及带有黄金标签的训练数据从高资源语言(例如英语)翻译到低资源语言，和/或(2)将低资源语言中的测试数据翻译成高资源语言进行推理，然后将预测的跨度级别标签投射回原始测试数据。然而，最先进的基于标记的标签投影方法由于在输入到翻译模型的过程中注入了额外标记，导致翻译质量下降。本文中，我们提出了一种解决这个问题的方法。

    Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a popular learning paradigm for low-resource languages with no labeled training data. However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods. Therefore, it is common to exploit translation and label projection to further improve the performance by (1) translating training data that is available in a high-resource language (e.g., English) together with the gold labels into low-resource languages, and/or (2) translating test data in low-resource languages to a high-source language to run inference on, then projecting the predicted span-level labels back onto the original test data. However, state-of-the-art marker-based label projection methods suffer from translation quality degradation due to the extra label markers injected in the input to the translation model. In this work, we e
    
[^76]: 无参随机优化的自由度有多高？

    How Free is Parameter-Free Stochastic Optimization?

    [https://arxiv.org/abs/2402.03126](https://arxiv.org/abs/2402.03126)

    这个论文研究了无参随机优化的问题，提出了一种完全无参的方法，通过简单的超参数搜索技术在非凸和凸设置下都能取得优于先进算法的性能。同时，论文还建立了一个下界，指出完全无参的方法在某些情况下无法实现。

    

    我们研究了无参随机优化的问题，探讨了在什么条件下可以存在完全无参的方法：这些方法可以达到与最优调参方法相竞争的收敛速度，而不需要对真实问题参数有很多知识。现有的无参方法只能被视为“部分”无参，因为它们需要对真实问题参数有一些非平凡的知识，比如随机梯度范数的上界、到最小值的距离的上界等。在非凸设置中，我们证明了一个简单的超参数搜索技术可以得到一个完全无参的方法，在性能上超过了更复杂的先进算法。在具有噪声函数值的凸设置下，在较小的噪声假设下，我们也提供了类似的结果。最后，假设只能访问随机梯度，我们建立了一个下界，使得完全无参的方法无法实现。

    We study the problem of parameter-free stochastic optimization, inquiring whether, and under what conditions, do fully parameter-free methods exist: these are methods that achieve convergence rates competitive with optimally tuned methods, without requiring significant knowledge of the true problem parameters. Existing parameter-free methods can only be considered ``partially'' parameter-free, as they require some non-trivial knowledge of the true problem parameters, such as a bound on the stochastic gradient norms, a bound on the distance to a minimizer, etc. In the non-convex setting, we demonstrate that a simple hyperparameter search technique results in a fully parameter-free method that outperforms more sophisticated state-of-the-art algorithms. We also provide a similar result in the convex setting with access to noisy function values under mild noise assumptions. Finally, assuming only access to stochastic gradients, we establish a lower bound that renders fully parameter-free s
    
[^77]: 在梯度反转攻击中消除硬标签约束的研究

    Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks

    [https://arxiv.org/abs/2402.03124](https://arxiv.org/abs/2402.03124)

    本研究旨在研究梯度反转攻击中消除硬标签约束，考虑到标签平滑和mixup技术的实际情况。我们提出了一种算法，可以同时恢复增强标签和输入特征，并为标签恢复方法提供了必要条件。

    

    梯度反转攻击旨在从联邦学习框架中暴露的中间梯度中重构本地训练数据。尽管攻击成功，但以往的所有方法，从重构单个数据点，然后放宽到批处理级别的单图像限制，都只在硬标签约束下进行测试。即使对于单图像重建，我们仍然缺乏一种基于分析的算法来恢复增强的软标签。在这项工作中，我们将重点从扩大批量大小转向研究硬标签约束，考虑到在训练过程中使用标签平滑和mixup技术的更现实的情况。特别地，我们首次提出了一种新算法，同时从单输入梯度中恢复真实的增强标签和最后一个全连接层的输入特征，并为任何基于分析的标签恢复方法提供了必要条件。大量实验证实了...

    Gradient inversion attacks aim to reconstruct local training data from intermediate gradients exposed in the federated learning framework. Despite successful attacks, all previous methods, starting from reconstructing a single data point and then relaxing the single-image limit to batch level, are only tested under hard label constraints. Even for single-image reconstruction, we still lack an analysis-based algorithm to recover augmented soft labels. In this work, we change the focus from enlarging batchsize to investigating the hard label constraints, considering a more realistic circumstance where label smoothing and mixup techniques are used in the training process. In particular, we are the first to initiate a novel algorithm to simultaneously recover the ground-truth augmented label and the input feature of the last fully-connected layer from single-input gradients, and provide a necessary condition for any analytical-based label recovery methods. Extensive experiments testify to 
    
[^78]: 好的教师解释: 解释增强的知识蒸馏

    Good Teachers Explain: Explanation-Enhanced Knowledge Distillation

    [https://arxiv.org/abs/2402.03119](https://arxiv.org/abs/2402.03119)

    通过优化解释增强的知识蒸馏（e$^2$KD）算法，可以让学生模型在准确性和学生-教师一致性方面都得到大幅度提升，确保学生模型从教师那里正确学到原因。

    

    知识蒸馏已被证明可以将大型教师模型压缩成较小的学生模型。虽然已经知道学生模型可以达到与教师相似的准确性，但也已经发现学生模型通常不会学到相同的函数。然而，学生模型和教师模型之间共享相似属性，如基于相同的输入特征进行预测，通常是非常有价值的，因为这确保学生从教师那里学到了“正确的特征”。在这项工作中，我们探索了是否可以通过优化经典的知识蒸馏损失以及教师和学生所生成的解释的相似性来实现这一点。尽管这个想法简单且直观，但我们发现我们提出的“解释增强的知识蒸馏”（e$^2$KD）（1）在准确性和学生-教师一致性方面始终提供了大幅度的增益，（2）确保学生从教师那里学到了正确的原因。

    Knowledge Distillation (KD) has proven effective for compressing large teacher models into smaller student models. While it is well known that student models can achieve similar accuracies as the teachers, it has also been shown that they nonetheless often do not learn the same function. It is, however, often highly desirable that the student's and teacher's functions share similar properties such as basing the prediction on the same input features, as this ensures that students learn the 'right features' from the teachers. In this work, we explore whether this can be achieved by not only optimizing the classic KD loss but also the similarity of the explanations generated by the teacher and the student. Despite the idea being simple and intuitive, we find that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides large gains in terms of accuracy and student-teacher agreement, (2) ensures that the student learns from the teacher to be right for the right reasons and
    
[^79]: 基于时间序列数据的故事化可视化特征行为设计模式

    Feature-Action Design Patterns for Storytelling Visualizations with Time Series Data

    [https://arxiv.org/abs/2402.03116](https://arxiv.org/abs/2402.03116)

    该论文提出了一种基于时间序列数据创建故事化可视化的方法，通过引入特征行为模式，使得故事板能够适应动态到达或选择的数据中可能出现的潜在特征。

    

    我们提出了一种使用时间序列数据创建故事化可视化的方法。现如今，许多个人决策都依赖于定期访问动态数据，正如我们在COVID-19大流行期间所见。因此，为选择某个特定情境下个人选定的动态数据构建故事化可视化是可取的。由于需要讲述依赖于数据的故事，基于已知数据的预定义故事板不能轻松地适应动态数据，也无法扩展到许多不同的个人和情境。最初受COVID-19大流行期间传达时间序列数据的需求的激发，我们开发了一种新颖的计算机辅助方法，用于元创作故事，它可以设计包括特征行为模式的故事板，以预测动态到达或选择的数据中可能出现的潜在特征。除了涉及COVID-19数据的元故事板外，我们还提供了关于进展的故事板。

    We present a method to create storytelling visualization with time series data. Many personal decisions nowadays rely on access to dynamic data regularly, as we have seen during the COVID-19 pandemic. It is thus desirable to construct storytelling visualization for dynamic data that is selected by an individual for a specific context. Because of the need to tell data-dependent stories, predefined storyboards based on known data cannot accommodate dynamic data easily nor scale up to many different individuals and contexts. Motivated initially by the need to communicate time series data during the COVID-19 pandemic, we developed a novel computer-assisted method for meta-authoring of stories, which enables the design of storyboards that include feature-action patterns in anticipation of potential features that may appear in dynamically arrived or selected data. In addition to meta-storyboards involving COVID-19 data, we also present storyboards for telling stories about progress in a mach
    
[^80]: 用深度学习发现可解释的科学图像数据模型

    Discovering interpretable models of scientific image data with deep learning

    [https://arxiv.org/abs/2402.03115](https://arxiv.org/abs/2402.03115)

    本文提出了使用解耦表示学习、稀疏深度神经网络训练和符号回归的方法，在复杂图像数据中形成高度简约且准确性可媲美黑盒模型的可解释模型，进一步探讨了这些模型在解释生物现象方面的应用。

    

    如何在给定一些复杂的原始数据，如图像，情况下找到可解释的、与领域相关的自然现象模型？我们能否利用这些模型从数据中得出科学见解？本文提出了一些方法来实现这一目标。特别地，我们实现了解耦表示学习、稀疏深度神经网络训练和符号回归，并评估了它们在形成可解释的复杂图像数据模型方面的实用性。我们使用一个已经研究过的显微镜观察细胞状态分类的测试问题来证明它们与生物成像领域的相关性。我们发现这些方法可以产生高度简约的模型，其准确率达到黑盒基准模型的约98%，但复杂度仅为其一小部分。我们探讨了这些可解释模型在解释潜在生物现象方面的实用性。

    How can we find interpretable, domain-appropriate models of natural phenomena given some complex, raw data such as images? Can we use such models to derive scientific insight from the data? In this paper, we propose some methods for achieving this. In particular, we implement disentangled representation learning, sparse deep neural network training and symbolic regression, and assess their usefulness in forming interpretable models of complex image data. We demonstrate their relevance to the field of bioimaging using a well-studied test problem of classifying cell states in microscopy data. We find that such methods can produce highly parsimonious models that achieve $\sim98\%$ of the accuracy of black-box benchmark models, with a tiny fraction of the complexity. We explore the utility of such interpretable models in producing scientific explanations of the underlying biological phenomenon.
    
[^81]: 利用具有结构关注机制的机器学习方法预测偶氮基团的红外光谱

    Infrared Spectra Prediction for Diazo Groups Utilizing a Machine Learning Approach with Structural Attention Mechanism

    [https://arxiv.org/abs/2402.03112](https://arxiv.org/abs/2402.03112)

    本文提出了一种利用结构关注机制的机器学习方法，可以预测和解释偶氮化合物的红外光谱。该方法通过集中关注与功能团邻近的化学信息，显著提升了光谱预测的准确性、鲁棒性和可解释性，为揭示红外光谱特征与分子结构之间的相关性提供了一种可扩展和高效的途径。

    

    红外（IR）光谱是化学研究中的重要技术，通过振动和转动跃迁阐明分子结构和动力学。然而，独特的振动和转动模式所表征的复杂分子特征给分析带来了挑战。本文提出了一种采用结构关注机制的机器学习方法，针对偶氮化合物，以提升红外光谱预测和解释能力。我们的模型通过集中关注邻近功能团附近的化学信息，显著提升了光谱预测的准确性、鲁棒性和可解释性。这种方法不仅揭示了红外光谱特征与分子结构之间的相关性，还提供了一种可扩展和高效的途径来解析复杂的分子相互作用。

    Infrared (IR) spectroscopy is a pivotal technique in chemical research for elucidating molecular structures and dynamics through vibrational and rotational transitions. However, the intricate molecular fingerprints characterized by unique vibrational and rotational patterns present substantial analytical challenges. Here, we present a machine learning approach employing a Structural Attention Mechanism tailored to enhance the prediction and interpretation of infrared spectra, particularly for diazo compounds. Our model distinguishes itself by honing in on chemical information proximal to functional groups, thereby significantly bolstering the accuracy, robustness, and interpretability of spectral predictions. This method not only demystifies the correlations between infrared spectral features and molecular structures but also offers a scalable and efficient paradigm for dissecting complex molecular interactions.
    
[^82]: 非平稳潜在自回归赌博机

    Non-Stationary Latent Auto-Regressive Bandits

    [https://arxiv.org/abs/2402.03110](https://arxiv.org/abs/2402.03110)

    本文提出了非平稳潜在自回归赌博机问题，并提出了一个算法，在这种环境下可以达到较低的遗憾率。

    

    本文考虑具有非平稳奖励的随机多臂赌博机问题。我们提出了一个新颖的非平稳环境的公式，其中臂的平均奖励随时间变化是由一些未知的潜在自回归(AR)状态的顺序k决定的。我们将这个新的环境称为潜在AR赌博机。潜在AR赌博机的不同形式在许多现实世界的场景中都出现，特别是在行为健康或教育等新兴科学领域中，这里缺乏对环境的机制建模。如果AR顺序k已知，我们提出了一个算法，在这种情况下，算法表现出O(k√T)的遗憾率。实证结果显示，即使k被错误地估计，我们的算法在多个非平稳环境中也胜过标准的UCB算法。

    We consider the stochastic multi-armed bandit problem with non-stationary rewards. We present a novel formulation of non-stationarity in the environment where changes in the mean reward of the arms over time are due to some unknown, latent, auto-regressive (AR) state of order $k$. We call this new environment the latent AR bandit. Different forms of the latent AR bandit appear in many real-world settings, especially in emerging scientific fields such as behavioral health or education where there are few mechanistic models of the environment. If the AR order $k$ is known, we propose an algorithm that achieves $\tilde{O}(k\sqrt{T})$ regret in this setting. Empirically, our algorithm outperforms standard UCB across multiple non-stationary environments, even if $k$ is mis-specified.
    
[^83]: 高维贝叶斯优化通过协方差矩阵适应策略

    High-dimensional Bayesian Optimization via Covariance Matrix Adaptation Strategy

    [https://arxiv.org/abs/2402.03104](https://arxiv.org/abs/2402.03104)

    本文提出了一种高维贝叶斯优化方法，通过协方差矩阵适应策略定义局部搜索区域，能够解决将贝叶斯优化应用于高维优化问题的挑战。

    

    贝叶斯优化（BO）是一种寻找昂贵黑盒函数全局最优解的有效方法。然而，众所周知，将BO应用于高维优化问题具有挑战性。为了解决这个问题，一个有希望的解决方案是使用局部搜索策略将搜索域划分成包含全局最优解可能性较高的局部区域，然后在这些区域内使用BO优化目标函数。在本文中，我们提出了一种使用协方差矩阵适应（CMA）策略定义局部区域的新技术。具体来说，我们使用CMA来学习一个能够估计数据点作为目标函数全局最优解概率的搜索分布。基于这个搜索分布，我们定义由高概率数据点组成的局部区域。我们的方法作为一个元算法，可以整合现有的黑盒BO优化方法。

    Bayesian Optimization (BO) is an effective method for finding the global optimum of expensive black-box functions. However, it is well known that applying BO to high-dimensional optimization problems is challenging. To address this issue, a promising solution is to use a local search strategy that partitions the search domain into local regions with high likelihood of containing the global optimum, and then use BO to optimize the objective function within these regions. In this paper, we propose a novel technique for defining the local regions using the Covariance Matrix Adaptation (CMA) strategy. Specifically, we use CMA to learn a search distribution that can estimate the probabilities of data points being the global optimum of the objective function. Based on this search distribution, we then define the local regions consisting of data points with high probabilities of being the global optimum. Our approach serves as a meta-algorithm as it can incorporate existing black-box BO optim
    
[^84]: 基于意图的提示校准：用合成边界情况增强提示优化

    Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases

    [https://arxiv.org/abs/2402.03099](https://arxiv.org/abs/2402.03099)

    该论文介绍了一种基于意图的提示校准方法，通过迭代优化和生成合成边界情况数据来改进提示工程，以提高大型语言模型的性能。

    

    由于大型语言模型（LLMs）对给定提示的高度敏感性和文本任务指令的固有歧义，提示工程是一项具有挑战性和重要性的任务。通过采用一个包含上次试验结果的元提示并提出改进的提示，最近的研究表明LLMs自动进行提示工程的能力。然而，这需要一个高质量的基准来比较不同的提示，在许多实际应用场景中获取高质量的基准是困难且昂贵的。在这项工作中，我们引入了一种新的自动提示工程方法，使用校准过程来迭代地优化与用户意图相符的提示。在优化过程中，系统联合生成边界用例的合成数据，并根据生成的数据集进行提示优化。我们证明了我们方法的有效性。

    Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our
    
[^85]: 超越对抗性扰动：通过合法语义的流形辅助对抗性样本

    Transcending Adversarial Perturbations: Manifold-Aided Adversarial Examples with Legitimate Semantics

    [https://arxiv.org/abs/2402.03095](https://arxiv.org/abs/2402.03095)

    本文提出一种基于流形辅助的生成模型，能够生成具有真实和合法语义的对抗样本。实验结果表明，这些对抗样本不仅具有更好的视觉质量，而且能够实现更高的攻击可迁移性。

    

    深度神经网络对恶意微小扰动操纵的对抗性样本相当脆弱。尽管大多数传统对抗性攻击通过最小化对抗性样本和原始图像之间的几何距离来确保其视觉上的不可察觉性，但是这种几何距离的约束导致了有限的攻击可迁移性、较差的视觉质量和人类不可察觉的可解释性。本文提出了一个监督语义转换生成模型，用于生成具有真实和合法语义的对抗性样本，其中首次构建了一个不受限制的对抗性流形，其中包含连续的语义变化，实现了从非对抗性样本到对抗性样本的合法过渡。在MNIST和工业缺陷数据集上的综合实验表明，我们的对抗性样本不仅具有更好的视觉质量，而且具有更高的攻击可迁移性。

    Deep neural networks were significantly vulnerable to adversarial examples manipulated by malicious tiny perturbations. Although most conventional adversarial attacks ensured the visual imperceptibility between adversarial examples and corresponding raw images by minimizing their geometric distance, these constraints on geometric distance led to limited attack transferability, inferior visual quality, and human-imperceptible interpretability. In this paper, we proposed a supervised semantic-transformation generative model to generate adversarial examples with real and legitimate semantics, wherein an unrestricted adversarial manifold containing continuous semantic variations was constructed for the first time to realize a legitimate transition from non-adversarial examples to adversarial ones. Comprehensive experiments on MNIST and industrial defect datasets showed that our adversarial examples not only exhibited better visual quality but also achieved superior attack transferability a
    
[^86]: 跨领域少样本目标检测通过增强的开集目标检测器

    Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector

    [https://arxiv.org/abs/2402.03094](https://arxiv.org/abs/2402.03094)

    本文提出了一种跨领域少样本目标检测器，通过增强的开集目标检测方法来解决跨领域数据差异带来的性能下降问题。

    

    本文解决了跨领域少样本目标检测（CD-FSOD）的挑战，旨在开发一个准确的目标检测器，用最少的标记样本来检测新领域的目标。虽然基于转换器的开集检测器（例如DE-ViT）在开放词汇目标检测和传统的少样本目标检测方面表现出色，能够检测到训练过程中没有见过的类别，我们自然会提出两个关键问题：1）这种开集检测方法能否容易地推广到CD-FSOD？2）如果不能，如何在面对显著的领域差异时增强开集方法的结果？为了回答第一个问题，我们引入了几个衡量领域差异的指标，并建立了一个具有多样领域度量值的新的CD-FSOD基准。在这个基准上评估了一些最先进的开集目标检测方法，在域外数据集中观察到明显的性能下降。这表明采用这些方法在CD-FSOD上失败了。

    This paper addresses the challenge of cross-domain few-shot object detection (CD-FSOD), aiming to develop an accurate object detector for novel domains with minimal labeled examples. While transformer-based open-set detectors e.g., DE-ViT~\cite{zhang2023detect} have excelled in both open-vocabulary object detection and traditional few-shot object detection, detecting categories beyond those seen during training, we thus naturally raise two key questions: 1) can such open-set detection methods easily generalize to CD-FSOD? 2) If no, how to enhance the results of open-set methods when faced with significant domain gaps? To address the first question, we introduce several metrics to quantify domain variances and establish a new CD-FSOD benchmark with diverse domain metric values. Some State-Of-The-Art (SOTA) open-set object detection methods are evaluated on this benchmark, with evident performance degradation observed across out-of-domain datasets. This indicates the failure of adopting 
    
[^87]: 对偶拉格朗日学习用于锥优化问题

    Dual Lagrangian Learning for Conic Optimization

    [https://arxiv.org/abs/2402.03086](https://arxiv.org/abs/2402.03086)

    本文介绍了对偶拉格朗日学习（DLL）方法，通过结合锥对偶理论和机器学习模型表示能力，在参数化线性和非线性锥优化问题上提供了有效的拉格朗日对偶界限，证明了其性能在优化问题上接近最优解的0.5%。

    

    本文介绍了对偶拉格朗日学习（DLL），一种结合锥对偶理论和机器学习模型表示能力的原理性学习方法。DLL利用锥对偶提供对偶可行解，并因此对参数化线性和非线性锥优化问题提供有效的拉格朗日对偶界限。本文引入了可微分锥投影层，一个系统的对偶完成过程以及一个自监督学习框架。DLL的有效性在线性和非线性参数化优化问题上得到了证明，DLL可以在优化性能的0.5%之内提供有效的对偶界限。

    This paper presents Dual Lagrangian Learning (DLL), a principled learning methodology that combines conic duality theory with the represen- tation power of ML models. DLL leverages conic duality to provide dual-feasible solutions, and therefore valid Lagrangian dual bounds, for para- metric linear and nonlinear conic optimization problems. The paper introduces differentiable conic projection layers, a systematic dual com- pletion procedure, and a self-supervised learning framework. The effectiveness of DLL is demon- strated on linear and nonlinear parametric opti- mization problems for which DLL provides valid dual bounds within 0.5% of optimality.
    
[^88]: 视觉文本遇见低层次视觉：视觉文本处理的全面调查

    Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual Text Processing

    [https://arxiv.org/abs/2402.03082](https://arxiv.org/abs/2402.03082)

    这份综述论文对近期视觉文本处理领域的最新进展进行了全面的、多角度的分析，包括文本图像增强、恢复、操作等方面。通过深入讨论特定的文本特征，如结构、笔划、语义、风格和空间内容等，揭示了有效利用这些独特文本特征在视觉文本处理中的重要性。

    

    视觉文本是文档和场景图像中的关键元素，在计算机视觉领域引起了广泛关注。除了视觉文本的检测和识别，视觉文本处理领域也经历了研究的激增，这得益于基础生成模型的出现。然而，由于文本与普通对象具有不同的独特属性和特征，仍然存在挑战。在我们的研究中观察到，在视觉文本处理中有效地利用这些独特的文本特征是至关重要的。在本调查中，我们提出了一个全面的、多方面的对最近进展进行分析。首先，我们引入了一个层次分类体系，涵盖从文本图像增强和恢复到文本图像操作的各个领域，然后是不同的学习范式。随后，我们对特定的文本特征进行了深入讨论，如结构、笔划、语义、风格和空间内容。

    Visual text, a pivotal element in both document and scene images, speaks volumes and attracts significant attention in the computer vision domain. Beyond visual text detection and recognition, the field of visual text processing has experienced a surge in research, driven by the advent of fundamental generative models. However, challenges persist due to the unique properties and features that distinguish text from general objects. Effectively leveraging these unique textual characteristics is crucial in visual text processing, as observed in our study. In this survey, we present a comprehensive, multi-perspective analysis of recent advancements in this field. Initially, we introduce a hierarchical taxonomy encompassing areas ranging from text image enhancement and restoration to text image manipulation, followed by different learning paradigms. Subsequently, we conduct an in-depth discussion of how specific textual features such as structure, stroke, semantics, style, and spatial conte
    
[^89]: 基于用户偏好的语言引导的抽象化

    Preference-Conditioned Language-Guided Abstraction

    [https://arxiv.org/abs/2402.03081](https://arxiv.org/abs/2402.03081)

    本研究提出了一种基于用户偏好的语言引导的抽象化方法，通过观察人类行为的变化并使用语言模型查询用户的偏好，构建适用于不同用户的状态抽象。

    

    从示范中学习是人们教导机器人的常用方式，但它容易出现误导性特征相关性。最近的研究使用语言构建状态抽象，即包含任务相关特征的视觉表示，以进行更通用的学习。然而，这些抽象也取决于用户在任务中关注的偏好，这可能很难用语言描述或仅仅通过语言详尽说明。我们如何构建抽象来捕捉这些潜在偏好呢？我们观察到人类行为反映了他们的世界观。我们的关键观点是，人类行为的变化告诉我们人类对世界的偏好存在差异，即他们的状态抽象也不同。在这项工作中，我们提出使用语言模型（LM）直接查询这些偏好，给定已发生行为变化的知识。在我们的框架中，我们使用LM有两种方式：首先，根据文本描述查询用户的偏好。

    Learning from demonstrations is a common way for users to teach robots, but it is prone to spurious feature correlations. Recent work constructs state abstractions, i.e. visual representations containing task-relevant features, from language as a way to perform more generalizable learning. However, these abstractions also depend on a user's preference for what matters in a task, which may be hard to describe or infeasible to exhaustively specify using language alone. How do we construct abstractions to capture these latent preferences? We observe that how humans behave reveals how they see the world. Our key insight is that changes in human behavior inform us that there are differences in preferences for how humans see the world, i.e. their state abstractions. In this work, we propose using language models (LMs) to query for those preferences directly given knowledge that a change in behavior has occurred. In our framework, we use the LM in two ways: first, given a text description of 
    
[^90]: 马尔可夫说服过程：从零开始学会说服

    Markov Persuasion Processes: Learning to Persuade from Scratch

    [https://arxiv.org/abs/2402.03077](https://arxiv.org/abs/2402.03077)

    这篇论文提出了马尔可夫说服过程模型，用于捕捉发送者和接收者顺序交互的情景。论文解决了现有模型中的问题，提供了针对发送者没有环境知识的解决方案。通过设计学习算法，证明了算法的性能。这个方法的总结要点是提出了马尔可夫说服过程模型，并提出了针对没有环境知识的发送者的学习算法。

    

    在贝叶斯说服中，一个消息灵通的发送者可以策略性地向接收者透露信息，以说服他们采取期望的行动。最近，越来越多的关注点集中在发送者和接收者顺序交互的情境中。最近，引入了马尔可夫说服过程（MPPs）来捕捉在马尔可夫环境中，发送者面对一系列短视接收者的顺序情景。迄今为止，在文献中研究的MPPs存在一些问题，这些问题阻碍了它们在实践中的充分运作，例如，它们假设发送者知道接收者的奖励。我们通过处理发送者对环境没有任何了解的MPPs，解决了这些问题。我们设计了一个学习算法，用于发送者的部分反馈。我们证明了它的悔恨与最佳信息披露策略之间的差异以次线性增长，就像学习过程中累计的说服力损失一样。

    In Bayesian persuasion, an informed sender strategically discloses information to a receiver so as to persuade them to undertake desirable actions. Recently, a growing attention has been devoted to settings in which sender and receivers interact sequentially. Recently, Markov persuasion processes (MPPs) have been introduced to capture sequential scenarios where a sender faces a stream of myopic receivers in a Markovian environment. The MPPs studied so far in the literature suffer from issues that prevent them from being fully operational in practice, e.g., they assume that the sender knows receivers' rewards. We fix such issues by addressing MPPs where the sender has no knowledge about the environment. We design a learning algorithm for the sender, working with partial feedback. We prove that its regret with respect to an optimal information-disclosure policy grows sublinearly in the number of episodes, as it is the case for the loss in persuasiveness cumulated while learning. Moreover
    
[^91]: 学习使用元强化学习进行抽象视觉运动映射

    Learning to Abstract Visuomotor Mappings using Meta-Reinforcement Learning

    [https://arxiv.org/abs/2402.03072](https://arxiv.org/abs/2402.03072)

    本研究通过使用元强化学习来探索人类获取多个全新视觉运动映射的能力，并发现情境线索在学习中起到了重要的作用，提供了计算上的优势。

    

    我们研究了人类获得全新技能的多个视觉运动映射的能力。使用格子导航范式，我们测试了不同"格子世界"实施的情境线索是否能够使参与者更有效地学习两个不同的关键映射。我们的结果表明，当提供情境信息时，任务表现显著更好。对于元强化学习智能体来说也是如此，它们在执行任务时是否接收情境信息有所不同。我们评估了它们在预测任务中人类表现的准确性，并分析了它们的内部表示。结果表明，情境线索允许在使用不同的视觉运动映射时形成独立的空间和时间表示，而它们的缺失则更倾向于共享一个表示。虽然这两种策略都可以学习多个视觉运动映射，但我们证明了情境线索提供了计算上的优势。

    We investigated the human capacity to acquire multiple visuomotor mappings for de novo skills. Using a grid navigation paradigm, we tested whether contextual cues implemented as different "grid worlds", allow participants to learn two distinct key-mappings more efficiently. Our results indicate that when contextual information is provided, task performance is significantly better. The same held true for meta-reinforcement learning agents that differed in whether or not they receive contextual information when performing the task. We evaluated their accuracy in predicting human performance in the task and analyzed their internal representations. The results indicate that contextual cues allow the formation of separate representations in space and time when using different visuomotor mappings, whereas the absence of them favors sharing one representation. While both strategies can allow learning of multiple visuomotor mappings, we showed contextual cues provide a computational advantage 
    
[^92]: 概率演员-评论家：学习以PAC-Bayes不确定性进行探索

    Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes Uncertainty

    [https://arxiv.org/abs/2402.03055](https://arxiv.org/abs/2402.03055)

    概率演员-评论家算法（PAC）通过在评论家中建模和推断不确定性，以改进强化学习中的连续控制性能，并实现自适应的探索策略。

    

    我们引入了概率演员-评论家（PAC），这是一种新颖的强化学习算法，通过缓解探索与利用的平衡问题，改进了连续控制性能。PAC通过将随机策略和评论家无缝融合，创建了评论家不确定性估计和演员训练之间的动态协同作用。我们的PAC算法的关键贡献在于通过Probably Approximately Correct-Bayesian（PAC-Bayes）分析，明确建模和推断评论家的认知不确定性。这种对评论家不确定性的融入使PAC能够在学习过程中自适应调整其探索策略，指导演员的决策过程。与现有技术中的固定或预定的探索方案相比，PAC表现出更好的效果。通过PAC-Bayes分析引导的随机策略和评论家之间的协同作用，是向深度强化学习中更具自适应性和有效性的探索策略迈出的关键一步。

    We introduce Probabilistic Actor-Critic (PAC), a novel reinforcement learning algorithm with improved continuous control performance thanks to its ability to mitigate the exploration-exploitation trade-off. PAC achieves this by seamlessly integrating stochastic policies and critics, creating a dynamic synergy between the estimation of critic uncertainty and actor training. The key contribution of our PAC algorithm is that it explicitly models and infers epistemic uncertainty in the critic through Probably Approximately Correct-Bayesian (PAC-Bayes) analysis. This incorporation of critic uncertainty enables PAC to adapt its exploration strategy as it learns, guiding the actor's decision-making process. PAC compares favorably against fixed or pre-scheduled exploration schemes of the prior art. The synergy between stochastic policies and critics, guided by PAC-Bayes analysis, represents a fundamental step towards a more adaptive and effective exploration strategy in deep reinforcement lear
    
[^93]: 多语言马来西亚嵌入：利用大型语言模型进行语义表示

    Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations

    [https://arxiv.org/abs/2402.03053](https://arxiv.org/abs/2402.03053)

    本文提出了一种基于大型语言模型的多语言马来西亚嵌入方法，通过微调Llama2和Mistral模型，在语义相似性和RAG任务中取得了显著的性能提升。

    

    在这项工作中，我们对Llama2和Mistral两种马来西亚语言模型进行了全面探索，并在涉及负正组对的嵌入任务中进行了微调。我们发布了两个专门针对语义相似性和检索辅助生成（RAG）的模型。在语义相似性方面，我们的6亿参数Llama2模型在b.cari.com.my、c.cari.com.my、马来西亚新闻和马来西亚Twitter测试集的所有recall@k指标上都优于OpenAI的text-embedding-ada-002。在RAG模型领域，我们的方法在马来西亚环境中与OpenAI的text-embedding-ada-002相竞争。值得注意的是，我们的20亿参数Llama2模型在“Melayu”关键词研究论文数据集的Recall@5、Recall@10上取得了卓越表现，并在lom.agc.gov.my数据集的Recall@3、Recall@5和Recall@10上表现出色。这些发现强调了我们的微调策略的有效性，并突显了在语义相似性和RAG任务中的性能提升。

    In this work, we present a comprehensive exploration of finetuning Malaysian language models, specifically Llama2 and Mistral, on embedding tasks involving negative and positive pairs. We release two distinct models tailored for Semantic Similarity and Retrieval-Augmented Generation (RAG).   For Semantic Similarity, our 600 million parameter Llama2 model outperforms OpenAI text-embedding-ada-002 across all recall@k metrics for b.cari.com.my, c.cari.com.my, Malay news, and Malaysian Twitter test sets.   In the realm of RAG models, our approach proves competitive with OpenAI text-embedding-ada-002 in the Malaysian context. Notably, our 2 billion parameter Llama2 model achieves superior Recall@5, Recall@10 for the "Melayu" keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10 for the lom.agc.gov.my dataset.   These findings underscore the effectiveness of our finetuning strategy and highlight the performance gains in both Semantic Similarity and RAG tasks.   All 
    
[^94]: EasyInstruct：一个易于使用的用于大型语言模型的指令处理框架

    EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models

    [https://arxiv.org/abs/2402.03049](https://arxiv.org/abs/2402.03049)

    EasyInstruct是一个易于使用的用于大型语言模型的指令处理框架，通过模块化指令生成、选择和提示，并考虑它们的组合和交互，使指令处理更加方便和高效。

    

    近年来，指令调整已经引起了越来越多的关注，并成为增强大型语言模型（LLMs）能力的一种关键技术。为了构建高质量的指令数据集，已经提出了许多指令处理方法，旨在在数据数量和数据质量之间达到精巧的平衡。然而，由于各种指令处理方法之间仍然存在不一致，目前没有标准的开源指令处理实现框架可供社区使用，这使得从业者无法进一步开发和推进。为了促进指令处理的研究和开发，我们提出了EasyInstruct，一个易于使用的用于LLMs的指令处理框架，它将指令生成、选择和提示模块化，并考虑它们的组合和交互。EasyInstruct已经在https://github.com/zjunlp/EasyInstruct上公开发布，并得到了积极维护。

    In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
    
[^95]: 使用高斯过程的协作学习来处理切换拓扑下的Euler-Lagrange系统跟踪控制问题

    Cooperative Learning with Gaussian Processes for Euler-Lagrange Systems Tracking Control under Switching Topologies

    [https://arxiv.org/abs/2402.03048](https://arxiv.org/abs/2402.03048)

    本文提出了一种新颖的基于学习的方法来解决在切换通信拓扑下部分未知动力学的Euler-Lagrange多智能体系统的跟踪控制问题，通过高效的协作算法框架和高斯过程回归，捕捉智能体之间的相关性进行不确定性预测，并通过稳定性分析确保了有界的跟踪误差概率。

    

    本文提出了一种创新的基于学习的方法来解决在切换通信拓扑下部分未知动力学的Euler-Lagrange多智能体系统的跟踪控制问题。该方法利用基于高斯过程回归的相关性感知协作算法框架，能够灵活捕捉智能体之间的相关性并进行不确定性预测。其显著特点是通过规避计算密集的后验方差计算，实现了出色的聚合权重计算效率。通过李亚普诺夫稳定性分析，分布式控制算法确保了有界的跟踪误差概率。仿真实验验证了该协议在有效处理复杂场景中的功效，使其成为解决具有不确定动力学和动态通信结构特征的多智能体系统的鲁棒跟踪控制的有前景的解决方案。

    This work presents an innovative learning-based approach to tackle the tracking control problem of Euler-Lagrange multi-agent systems with partially unknown dynamics operating under switching communication topologies. The approach leverages a correlation-aware cooperative algorithm framework built upon Gaussian process regression, which adeptly captures inter-agent correlations for uncertainty predictions. A standout feature is its exceptional efficiency in deriving the aggregation weights achieved by circumventing the computationally intensive posterior variance calculations. Through Lyapunov stability analysis, the distributed control law ensures bounded tracking errors with high probability. Simulation experiments validate the protocol's efficacy in effectively managing complex scenarios, establishing it as a promising solution for robust tracking control in multi-agent systems characterized by uncertain dynamics and dynamic communication structures.
    
[^96]: PFDM: 透过扩散模型的免解析器虚拟试穿

    PFDM: Parser-Free Virtual Try-on via Diffusion Model

    [https://arxiv.org/abs/2402.03047](https://arxiv.org/abs/2402.03047)

    PFDM是一种基于扩散模型的免解析器虚拟试穿方法，可以无缝地在目标人物身上“穿”上服装，无需准确的分割掩码，并通过服装融合注意力机制达到高保真度的试穿效果。

    

    虚拟试穿可以显著提高在线和实体购物中的服装购物体验，引起计算机视觉方面的广泛关注。然而，为了实现高保真度的试穿效果，大多数最先进的方法仍依赖于准确的分割掩码，这些掩码通常是由准确无误的解析器或手动标注生成的。为了克服这个瓶颈，我们提出了一种基于扩散模型（PFDM）的免解析器虚拟试穿方法。给定两个图像，PFDM可以通过隐式变形无缝地将服装“穿”在目标人物身上，而无需其他信息。为了有效学习该模型，我们合成了许多伪图像，并通过在人物上穿上各种服装构建样品对。在大规模扩展数据集的监督下，我们使用提出的服装融合注意力（GFA）机制融合人物和服装特征。实验表明，我们提出的PFDM能成功处理复杂情况、合成高保真度的图像，并且超过了现有方法。

    Virtual try-on can significantly improve the garment shopping experiences in both online and in-store scenarios, attracting broad interest in computer vision. However, to achieve high-fidelity try-on performance, most state-of-the-art methods still rely on accurate segmentation masks, which are often produced by near-perfect parsers or manual labeling. To overcome the bottleneck, we propose a parser-free virtual try-on method based on the diffusion model (PFDM). Given two images, PFDM can "wear" garments on the target person seamlessly by implicitly warping without any other information. To learn the model effectively, we synthesize many pseudo-images and construct sample pairs by wearing various garments on persons. Supervised by the large-scale expanded dataset, we fuse the person and garment features using a proposed Garment Fusion Attention (GFA) mechanism. Experiments demonstrate that our proposed PFDM can successfully handle complex cases, synthesize high-fidelity images, and out
    
[^97]: 开放强化学习基准测试：全面跟踪的实验数据

    Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning

    [https://arxiv.org/abs/2402.03046](https://arxiv.org/abs/2402.03046)

    Open RL Benchmark是一组跟踪强化学习实验的数据，并涵盖了广泛的RL库和参考实现。它提供了完整的原始数据，可以用来衡量RL算法的有效性，也可供社区进行下载、使用和贡献。

    

    在许多强化学习（RL）论文中，学习曲线是衡量RL算法有效性的有用指标。然而，学习曲线的完整原始数据很少公开。因此，通常需要从头开始重现实验，这可能耗时且容易出错。我们提出了开放式强化学习基准测试，这是一组完全跟踪的强化学习实验，包括通常的数据如一次性回报，以及所有特定算法和系统指标。开放式强化学习基准测试是社区驱动的：任何人都可以下载、使用和贡献数据。到目前为止，已经跟踪了超过25000次运行，累计时间超过8年。开放式强化学习基准测试涵盖了各种强化学习库和参考实现。我们特别注重确保每个实验的精确可再现性，不仅提供完整的参数，还提供用于生成实验的依赖版本。

    In many Reinforcement Learning (RL) papers, learning curves are useful indicators to measure the effectiveness of RL algorithms. However, the complete raw data of the learning curves are rarely available. As a result, it is usually necessary to reproduce the experiments from scratch, which can be time-consuming and error-prone. We present Open RL Benchmark, a set of fully tracked RL experiments, including not only the usual data such as episodic return, but also all algorithm-specific and system metrics. Open RL Benchmark is community-driven: anyone can download, use, and contribute to the data. At the time of writing, more than 25,000 runs have been tracked, for a cumulative duration of more than 8 years. Open RL Benchmark covers a wide range of RL libraries and reference implementations. Special care is taken to ensure that each experiment is precisely reproducible by providing not only the full parameters, but also the versions of the dependencies used to generate it. In addition, O
    
[^98]: SIDU-TXT: 一种具有整体评估方法的NLP可解释的人工智能算法

    SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach

    [https://arxiv.org/abs/2402.03043](https://arxiv.org/abs/2402.03043)

    本论文探究了可解释的人工智能（XAI）方法在文本领域的适用性，并将相似度差异和独特性（SIDU）方法扩展到文本数据，提供了对模型预测关键的有上下文意义的文本元素的解释。本研究采用了一个综合的三层评估框架来评估XAI方法。

    

    可解释的人工智能（XAI）有助于解读“黑盒”模型。虽然已经提出并评估了多种主要用于图像领域的方法，但解释性在文本领域中的探索仍然是一个不断增长的研究领域。本文探讨了XAI方法在文本领域中的适用性。在这个背景下，我们扩展了一种被认为在基于图像的分类中定位整个显著区域能力优越的XAI方法——相似度差异和独特性（SIDU）的方法，将其应用于文本数据。扩展方法SIDU-TXT利用来自“黑盒”模型的特征激活图生成热力图，以词为单位提供解释，突出显示对模型预测至关重要的有上下文意义的文本元素。鉴于还没有统一的XAI评估标准，本研究应用了一个综合的三层评估框架：功能基础、人类基础和应用基础评估。

    Explainable AI (XAI) aids in deciphering 'black-box' models. While several methods have been proposed and evaluated primarily in the image domain, the exploration of explainability in the text domain remains a growing research area. In this paper, we delve into the applicability of XAI methods for the text domain. In this context, the 'Similarity Difference and Uniqueness' (SIDU) XAI method, recognized for its superior capability in localizing entire salient regions in image-based classification is extended to textual data. The extended method, SIDU-TXT, utilizes feature activation maps from 'black-box' models to generate heatmaps at a granular, word-based level, thereby providing explanations that highlight contextually significant textual elements crucial for model predictions. Given the absence of a unified standard for assessing XAI methods, this study applies a holistic three-tiered comprehensive evaluation framework: Functionally-Grounded, Human-Grounded and Application-Grounded,
    
[^99]: 交互式视频：以用户为中心的可控视频生成与多模态指令协同

    InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions

    [https://arxiv.org/abs/2402.03040](https://arxiv.org/abs/2402.03040)

    “InteractiveVideo”是一个以用户为中心的视频生成框架，通过交互式的多模态指令，用户可以在整个生成过程中精确、有效地指导生成模型，并对视频的关键方面进行灵活调整。

    

    我们引入了一种名为“交互式视频”的用户中心视频生成框架。与传统的基于用户提供图像或文本的生成方法不同，我们的框架设计用于动态交互，允许用户通过各种直观的机制在整个生成过程中指导生成模型，例如文本和图像提示、绘画、拖放等。我们提出了一种协同多模态指令机制，旨在将用户的多模态指令无缝集成到生成模型中，从而实现用户输入和生成过程之间的合作和响应式交互。这种方法通过精确而有效的用户指令，实现了对生成结果的迭代和精细化调整。通过“交互式视频”，用户可以灵活地精心调整视频的关键方面，包括绘画参考图像、编辑语义和调整视频动作等。

    We introduce $\textit{InteractiveVideo}$, a user-centric framework for video generation. Different from traditional generative approaches that operate based on user-provided images or text, our framework is designed for dynamic interaction, allowing users to instruct the generative model through various intuitive mechanisms during the whole generation process, e.g. text and image prompts, painting, drag-and-drop, etc. We propose a Synergistic Multimodal Instruction mechanism, designed to seamlessly integrate users' multimodal instructions into generative models, thus facilitating a cooperative and responsive interaction between user inputs and the generative process. This approach enables iterative and fine-grained refinement of the generation result through precise and effective user instructions. With $\textit{InteractiveVideo}$, users are given the flexibility to meticulously tailor key aspects of a video. They can paint the reference image, edit semantics, and adjust video motions 
    
[^100]: 自动组合样本选择策略用于少样本学习

    Automatic Combination of Sample Selection Strategies for Few-Shot Learning

    [https://arxiv.org/abs/2402.03038](https://arxiv.org/abs/2402.03038)

    本文研究了20种样本选择策略对少样本学习性能的影响，并提出了一种自动组合样本选择策略的方法（ACSESS），在多个数据集上证明了其优越性能。

    

    在少样本学习中，如元学习、少样本微调或上下文学习中，用于训练模型的有限样本数量对整体成功具有显著影响。尽管存在大量的样本选择策略，但它们对少样本学习性能的影响尚不十分明确，因为大部分只被在典型的监督设置中进行了评估。本文通过对8个图像和6个文本数据集上的5种少样本学习方法，彻底研究了20种样本选择策略对性能的影响。此外，我们提出了一种新的自动组合样本选择策略的方法（ACSESS），它充分利用了个体策略的优势和互补信息。实验结果表明，我们的方法始终优于个体选择策略，以及最近提出的上下文学习支持样本选择方法。

    In few-shot learning, such as meta-learning, few-shot fine-tuning or in-context learning, the limited number of samples used to train a model have a significant impact on the overall success. Although a large number of sample selection strategies exist, their impact on the performance of few-shot learning is not extensively known, as most of them have been so far evaluated in typical supervised settings only. In this paper, we thoroughly investigate the impact of 20 sample selection strategies on the performance of 5 few-shot learning approaches over 8 image and 6 text datasets. In addition, we propose a new method for automatic combination of sample selection strategies (ACSESS) that leverages the strengths and complementary information of the individual strategies. The experimental results show that our method consistently outperforms the individual selection strategies, as well as the recently proposed method for selecting support examples for in-context learning. We also show a str
    
[^101]: 受深度算子网络结构启发的函数SDE近似方法

    Functional SDE approximation inspired by a deep operator network architecture

    [https://arxiv.org/abs/2402.03028](https://arxiv.org/abs/2402.03028)

    本文提出了一种受深度算子网络结构启发的函数SDE近似方法，通过深度神经网络和多项式混沌展开实现对随机微分方程解的近似，并通过学习减轻指数级复杂度的问题。

    

    本文提出并分析了一种通过深度神经网络近似随机微分方程（SDE）解的新方法。该结构灵感来自于深度算子网络（DeepONets）的概念，它基于函数空间中的算子学习，以及在网络中表示的降维基础。在我们的设置中，我们利用了随机过程的多项式混沌展开（PCE），并将相应的架构称为SDEONet。在参数化偏微分方程的不确定性量化（UQ）领域中，PCE被广泛使用。然而，在SDE中并非如此，传统的采样方法占主导地位，而功能性方法很少见。截断的PCE存在一个主要挑战，即随着最大多项式阶数和基函数数量的增加，分量的数量呈指数级增长。所提出的SDEONet结构旨在通过学习来减轻指数级复杂度的问题。

    A novel approach to approximate solutions of Stochastic Differential Equations (SDEs) by Deep Neural Networks is derived and analysed. The architecture is inspired by the notion of Deep Operator Networks (DeepONets), which is based on operator learning in function spaces in terms of a reduced basis also represented in the network. In our setting, we make use of a polynomial chaos expansion (PCE) of stochastic processes and call the corresponding architecture SDEONet. The PCE has been used extensively in the area of uncertainty quantification (UQ) with parametric partial differential equations. This however is not the case with SDE, where classical sampling methods dominate and functional approaches are seen rarely. A main challenge with truncated PCEs occurs due to the drastic growth of the number of components with respect to the maximum polynomial degree and the number of basis elements. The proposed SDEONet architecture aims to alleviate the issue of exponential complexity by learni
    
[^102]: 通过潜在同构传播理解和引导弱监督的实体对齐

    Understanding and Guiding Weakly Supervised Entity Alignment with Potential Isomorphism Propagation

    [https://arxiv.org/abs/2402.03025](https://arxiv.org/abs/2402.03025)

    本文通过传播视角分析了弱监督的实体对齐任务，并提出一种潜在同构传播操作符来增强知识图谱之间的邻域信息传播。通过验证，发现基于聚合的实体对齐模型中的潜在对齐实体具有同构子图。

    

    弱监督的实体对齐是使用有限数量的种子对齐，在不同知识图谱之间识别等价实体的任务。尽管在基于聚合的弱监督实体对齐方面取得了重大进展，但在这种设置下的基本机制仍未被探索。在本文中，我们提出了一种传播视角来分析弱监督实体对齐，并解释了现有的基于聚合的实体对齐模型。我们的理论分析揭示了这些模型实质上是寻找用于对实体相似度进行传播的操作符。我们进一步证明，尽管不同知识图谱之间存在结构异质性，基于聚合的实体对齐模型中的潜在对齐实体具有同构子图，这是实体对齐的核心前提，但尚未被研究。利用这一洞见，我们引入了潜在同构传播操作符来增强跨知识图谱的邻域信息传播。我们开发了一个通用的实体对齐框架PipEA，实现了效果显著的实验结果。

    Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent entities across diverse knowledge graphs (KGs) using only a limited number of seed alignments. Despite substantial advances in aggregation-based weakly supervised EA, the underlying mechanisms in this setting remain unexplored. In this paper, we present a propagation perspective to analyze weakly supervised EA and explain the existing aggregation-based EA models. Our theoretical analysis reveals that these models essentially seek propagation operators for pairwise entity similarities. We further prove that, despite the structural heterogeneity of different KGs, the potentially aligned entities within aggregation-based EA models have isomorphic subgraphs, which is the core premise of EA but has not been investigated. Leveraging this insight, we introduce a potential isomorphism propagation operator to enhance the propagation of neighborhood information across KGs. We develop a general EA framework, PipEA, inco
    
[^103]: 数据诱导的多尺度损失和高效多速率梯度下降方案

    Data-induced multiscale losses and efficient multirate gradient descent schemes

    [https://arxiv.org/abs/2402.03021](https://arxiv.org/abs/2402.03021)

    本文研究了多尺度数据对机器学习算法的影响，并提出了一种基于数据的新的梯度下降方法，旨在提高训练效率。

    

    本文研究多尺度数据对机器学习算法的影响，特别是在深度学习的背景下。如果一个数据集的分布在不同方向上具有尺度的显著变化，则其被称为多尺度数据。本文揭示了损失景观中的多尺度结构，包括其梯度和来自数据的海森矩阵。相应地，本文引入了一种新的梯度下降方法，受科学计算中使用的多尺度算法的启发。这种方法试图超越经验性学习率选择，提供一种更系统、数据驱动的策略来提高训练效率，特别是在后期阶段。

    This paper investigates the impact of multiscale data on machine learning algorithms, particularly in the context of deep learning. A dataset is multiscale if its distribution shows large variations in scale across different directions. This paper reveals multiscale structures in the loss landscape, including its gradients and Hessians inherited from the data. Correspondingly, it introduces a novel gradient descent approach, drawing inspiration from multiscale algorithms used in scientific computing. This approach seeks to transcend empirical learning rate selection, offering a more systematic, data-informed strategy to enhance training efficiency, especially in the later stages.
    
[^104]: 用于动作识别的Taylor视频

    Taylor Videos for Action Recognition

    [https://arxiv.org/abs/2402.03019](https://arxiv.org/abs/2402.03019)

    Taylor视频是一种新的视频格式，用于动作识别中的动作提取问题。它通过使用Taylor展开近似计算隐含的动作提取函数，从而解决了动作提取中的挑战性问题。

    

    从视频中有效地提取动作是动作识别中一个重要且长期存在的问题。这个问题非常具有挑战性，因为动作(i)没有明确的形式，(ii)拥有诸如位移、速度和加速度等各种概念，(iii)通常会受到不稳定像素引起的噪声的干扰。为了解决这些挑战，我们提出了Taylor视频，一种新的视频格式，它突出显示了每个帧中的主要动作（例如挥手）被称为Taylor帧。Taylor视频的命名来源于Taylor级数，它使用重要的项来近似给定点上的函数。在视频的情境中，我们定义了一个隐含的动作提取函数，旨在从视频时间块中提取动作。在这个块中，我们使用帧、差分帧和高阶差分帧进行Taylor展开，以近似计算起始帧上的这个函数。我们展示了Taylor级数中高阶项的求和给我们提供了...

    Effectively extracting motions from video is a critical and long-standing problem for action recognition. This problem is very challenging because motions (i) do not have an explicit form, (ii) have various concepts such as displacement, velocity, and acceleration, and (iii) often contain noise caused by unstable pixels. Addressing these challenges, we propose the Taylor video, a new video format that highlights the dominate motions (e.g., a waving hand) in each of its frames named the Taylor frame. Taylor video is named after Taylor series, which approximates a function at a given point using important terms. In the scenario of videos, we define an implicit motion-extraction function which aims to extract motions from video temporal block. In this block, using the frames, the difference frames, and higher-order difference frames, we perform Taylor expansion to approximate this function at the starting frame. We show the summation of the higher-order terms in the Taylor series gives us
    
[^105]: 向绿色且类人的人工智能迈进：当代少样本学习方法的全面调查

    Toward Green and Human-Like Artificial Intelligence: A Complete Survey on Contemporary Few-Shot Learning Approaches

    [https://arxiv.org/abs/2402.03017](https://arxiv.org/abs/2402.03017)

    本文全面调查了少样本学习领域的最新进展，探讨了该方法在解决深度学习在实际应用中的限制方面的潜力和挑战。

    

    尽管深度学习取得了广泛的成功，但其对数据的需求和计算的昂贵性使其在许多数据受限的真实应用中不实用。少样本学习（FSL）旨在通过实现对新学习任务的快速适应来解决这些限制，并在近年来取得了显著发展。本调查提供了该领域最新进展的全面概述。首先，正式定义了FSL，并介绍了它与不同学习领域的关系。引入了一种新的分类法，扩展了以前提出的方法，并对经典和新领域中的实际应用进行了描述。最后，讨论了塑造该领域的最新趋势、突出挑战和有前途的未来研究方向。

    Despite deep learning's widespread success, its data-hungry and computationally expensive nature makes it impractical for many data-constrained real-world applications. Few-Shot Learning (FSL) aims to address these limitations by enabling rapid adaptation to novel learning tasks, seeing significant growth in recent years. This survey provides a comprehensive overview of the field's latest advancements. Initially, FSL is formally defined, and its relationship with different learning fields is presented. A novel taxonomy is introduced, extending previously proposed ones, and real-world applications in classic and novel fields are described. Finally, recent trends shaping the field, outstanding challenges, and promising future research directions are discussed.
    
[^106]: 信任谁？分布式高斯过程回归的选举学习

    Whom to Trust? Elective Learning for Distributed Gaussian Process Regression

    [https://arxiv.org/abs/2402.03014](https://arxiv.org/abs/2402.03014)

    本文介绍了一种使用高斯过程回归增强分布式协作学习的创新方法，并开发了一种选举学习算法，使智能体能够根据邻居的可信度有选择性地请求预测。该方法提高了个体预测的准确性，同时消除了计算密集型方差计算的需求，并确保了预测的可靠性。

    

    本文介绍了一种创新的方法，通过在多智能体系统中使用高斯过程（GP）回归来增强分布式协作学习。这项工作的主要贡献是开发了一种选举学习算法，即基于先验的选举式分布式GP（Pri-GP），它赋予了智能体基于信任度有选择地向邻居智能体请求预测的能力。所提出的Pri-GP有效提高了个体预测的准确性，特别是在智能体先验知识错误的情况下。此外，它消除了在分布式GP中确定聚合权重的计算密集型方差计算的需求。此外，我们在Pri-GP框架中建立了一个预测误差边界，确保了预测的可靠性，这被认为是在安全关键的多智能体系统应用中的一个重要性质。

    This paper introduces an innovative approach to enhance distributed cooperative learning using Gaussian process (GP) regression in multi-agent systems (MASs). The key contribution of this work is the development of an elective learning algorithm, namely prior-aware elective distributed GP (Pri-GP), which empowers agents with the capability to selectively request predictions from neighboring agents based on their trustworthiness. The proposed Pri-GP effectively improves individual prediction accuracy, especially in cases where the prior knowledge of an agent is incorrect. Moreover, it eliminates the need for computationally intensive variance calculations for determining aggregation weights in distributed GP. Furthermore, we establish a prediction error bound within the Pri-GP framework, ensuring the reliability of predictions, which is regarded as a crucial property in safety-critical MAS applications.
    
[^107]: 关于输出扰动对二元线性分类中公平性的影响

    On the Impact of Output Perturbation on Fairness in Binary Linear Classification

    [https://arxiv.org/abs/2402.03011](https://arxiv.org/abs/2402.03011)

    输出扰动对二元线性分类中公平性的影响在个体公平性方面是有界的但与模型维度成正比，在群体公平性方面则由角边距的分布决定。

    

    我们从理论上研究了差分隐私如何与二元线性分类中的个体和群体公平性相互作用。更具体地说，我们关注输出扰动机制，这是一种在保护隐私的机器学习中经典的方法。我们推导了扰动模型相对于原始模型能够达到的个体和群体公平性水平的高概率边界。因此，对于个体公平性，我们证明了输出扰动对公平性水平的影响是有界的，但随着模型的维度增长而增大。对于群体公平性，我们展示了这种影响由所谓的角边距（即非私有模型的有符号边距乘以每个示例的范数）的分布来决定。

    We theoretically study how differential privacy interacts with both individual and group fairness in binary linear classification. More precisely, we focus on the output perturbation mechanism, a classic approach in privacy-preserving machine learning. We derive high-probability bounds on the level of individual and group fairness that the perturbed models can achieve compared to the original model. Hence, for individual fairness, we prove that the impact of output perturbation on the level of fairness is bounded but grows with the dimension of the model. For group fairness, we show that this impact is determined by the distribution of so-called angular margins, that is signed margins of the non-private model re-scaled by the norm of each example.
    
[^108]: 扩散吉布斯采样

    Diffusive Gibbs Sampling

    [https://arxiv.org/abs/2402.03008](https://arxiv.org/abs/2402.03008)

    扩散吉布斯采样是一种创新的采样方法，通过集成扩散模型并应用吉布斯采样，有效地从具有远程和断开模态特征的分布中采样，表现出比其他方法更好的混合性能，并在多种任务中取得显著改进的结果。

    

    传统马尔可夫链蒙特卡洛（MCMC）方法在多模态分布的混合不足方面存在着挑战，特别是在贝叶斯推断和分子动力学等实际应用中。针对这个问题，我们提出了一种创新的采样方法——扩散吉布斯采样（DiGS），用于有效采样具有远程和断开模态特征的分布。DiGS集成了扩散模型的最新发展，利用高斯卷积创建一个辅助噪声分布，以在原始空间中连接孤立的模态，并应用吉布斯采样从两个空间中交替抽取样本。我们的方法在采样多模态分布方面表现出比并行温度法等最先进方法更好的混合性能。我们证明我们的采样器在各种任务中取得了显著改进的结果，包括高斯混合模型、贝叶斯神经网络和分子动力学。

    The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. Our approach exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering. We demonstrate that our sampler attains substantially improved results across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.
    
[^109]: 开发一种适用于受环境条件变化影响的昂贵实验和模拟的实用贝叶斯优化算法

    On the development of a practical Bayesian optimisation algorithm for expensive experiments and simulations with changing environmental conditions

    [https://arxiv.org/abs/2402.03006](https://arxiv.org/abs/2402.03006)

    本文在受环境条件变化影响的昂贵实验和模拟中，将贝叶斯优化方法推广到包含可控和不可控参数的系统优化中，通过在所有变量上拟合全局代理模型，但只在对不可控变量的测量条件下优化可控参数。

    

    工程实验通常在受控环境中进行，可以将参数设置为任何所需值。然而，在真实环境中，通常假设相同条件不成立，因为许多实验受不可控制的环境条件（如温度、湿度和风速）的影响。在优化这些实验时，应该重点关注在给定不可控变量条件下找到最优值。本文将贝叶斯优化方法推广到在包含可控和不可控参数的变化环境中进行系统优化。该推广通过在所有可控和环境变量上拟合全局代理模型，但只在对不可控变量的测量条件下优化可控参数。该方法在两个合成测试函数上进行了验证，研究了噪声水平、环境参数数量和参数波动的影响。

    Experiments in engineering are typically conducted in controlled environments where parameters can be set to any desired value. This assumes that the same applies in a real-world setting -- an assumption that is often incorrect as many experiments are influenced by uncontrollable environmental conditions such as temperature, humidity and wind speed. When optimising such experiments, the focus should lie on finding optimal values conditionally on these uncontrollable variables. This article extends Bayesian optimisation to the optimisation of systems in changing environments that include controllable and uncontrollable parameters. The extension fits a global surrogate model over all controllable and environmental variables but optimises only the controllable parameters conditional on measurements of the uncontrollable variables. The method is validated on two synthetic test functions and the effects of the noise level, the number of the environmental parameters, the parameter fluctuatio
    
[^110]: 小心使用手术刀：使用EMA改进梯度手术

    Careful with that Scalpel: Improving Gradient Surgery with an EMA

    [https://arxiv.org/abs/2402.02998](https://arxiv.org/abs/2402.02998)

    通过将训练损失梯度和辅助梯度在训练梯度方向上的正交投影结合起来，使用EMA（指数移动平均）可以改进梯度手术，提高深度学习估计管道的性能。

    

    在深度学习估计管道中，除了最小化单一的训练损失外，还依赖于辅助目标来量化和鼓励模型的可取属性（例如在另一个数据集上的表现，鲁棒性，与先前的一致性）。虽然将辅助损失与训练损失相加作为正则化的最简单方法，但最近的研究表明，通过混合梯度而不仅仅是简单相加，可以提高性能；这被称为梯度手术。我们将这个问题看作是一个约束最小化问题，其中辅助目标在训练损失的最小化集合中被最小化。为了解决这个双层问题，我们采用了一个参数更新方向，它将训练损失梯度和辅助梯度在训练梯度方向上的正交投影结合起来。在梯度来自小批次的情况下，我们解释了如何使用训练损失梯度的移动平均来维护。

    Beyond minimizing a single training loss, many deep learning estimation pipelines rely on an auxiliary objective to quantify and encourage desirable properties of the model (e.g. performance on another dataset, robustness, agreement with a prior). Although the simplest approach to incorporating an auxiliary loss is to sum it with the training loss as a regularizer, recent works have shown that one can improve performance by blending the gradients beyond a simple sum; this is known as gradient surgery. We cast the problem as a constrained minimization problem where the auxiliary objective is minimized among the set of minimizers of the training loss. To solve this bilevel problem, we follow a parameter update direction that combines the training loss gradient and the orthogonal projection of the auxiliary gradient to the training gradient. In a setting where gradients come from mini-batches, we explain how, using a moving average of the training loss gradients, we can carefully maintain
    
[^111]: 文本引导的图像聚类

    Text-Guided Image Clustering

    [https://arxiv.org/abs/2402.02996](https://arxiv.org/abs/2402.02996)

    这篇论文提出了一种文本引导的图像聚类方法，使用图像字幕和视觉问答模型生成文本，然后对生成的文本进行聚类，并通过注入任务或领域知识来改进聚类结果。实验证明，获得的文本表示通常优于图像特征，而基于关键词的解释可以更好地描述聚类。

    

    图像聚类将一组图像分成有意义的组，通常通过人工给出的注释进行解释。这些注释通常以文本形式存在，引发了使用文本作为图像聚类的抽象的问题。然而，当前的图像聚类方法忽视了生成的文本描述的使用。因此，我们提出了一种文本引导的图像聚类方法，即使用图像字幕和视觉问答（VQA）模型生成文本，然后对生成的文本进行聚类。此外，我们还介绍了一种通过提示VQA模型来注入任务或领域知识用于聚类的新方法。在八个不同的图像聚类数据集上，我们的结果表明，获得的文本表示通常优于图像特征。此外，我们提出了一种基于计数的聚类可解释性方法。我们的评估结果表明，基于关键词的解释比相应的聚类准确性更好地描述了聚类。总的来说，

    Image clustering divides a collection of images into meaningful groups, typically interpreted post-hoc via human-given annotations. Those are usually in the form of text, begging the question of using text as an abstraction for image clustering. Current image clustering methods, however, neglect the use of generated textual descriptions. We, therefore, propose Text-Guided Image Clustering, i.e., generating text using image captioning and visual question-answering (VQA) models and subsequently clustering the generated text. Further, we introduce a novel approach to inject task- or domain knowledge for clustering by prompting VQA models. Across eight diverse image clustering datasets, our results show that the obtained text representations often outperform image features. Additionally, we propose a counting-based cluster explainability method. Our evaluations show that the derived keyword-based explanations describe clusters better than the respective cluster accuracy suggests. Overall, 
    
[^112]: 论文标题：解码时间对齐的语言模型

    Decoding-time Realignment of Language Models

    [https://arxiv.org/abs/2402.02992](https://arxiv.org/abs/2402.02992)

    本研究提出了解码时间对齐（DeRa）方法，可以在不重新训练模型的情况下探索和评估不同的规则化强度，从而对齐语言模型和人类偏好。

    

    将语言模型与人类偏好对齐对于减少模型中的错误和偏差非常重要。对齐技术，如从人类反馈中进行的强化学习（RLHF），通常被视为在人类偏好奖励和鼓励保持与未对齐模型接近的接近性规则项之间进行优化的权衡。选择适当的规则化水平至关重要：规则化不足可能导致由于奖励欺骗而降低模型能力，而过度规则化则阻碍对齐。传统方法找到最佳规则化水平需要使用不同规则化强度重新训练多个模型。然而，这个过程耗费资源，特别是对于大型模型来说。为了解决这个挑战，我们提出了解码时间对齐（DeRa），一种简单的方法，在无需重新训练的情况下探索和评估不同的规则化强度。DeRa可以对对齐模型的程度进行控制。

    Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of al
    
[^113]: DexDiffuser: 使用扩散模型生成灵巧抓取姿势

    DexDiffuser: Generating Dexterous Grasps with Diffusion Models

    [https://arxiv.org/abs/2402.02989](https://arxiv.org/abs/2402.02989)

    DexDiffuser是一种使用扩散模型生成灵巧抓取姿势的新方法，通过对物体点云的生成、评估和优化，实现了较高的抓取成功率。

    

    我们引入了DexDiffuser，一种新颖的灵巧抓取方法，能够在部分物体点云上生成、评估和优化抓取姿势。DexDiffuser包括条件扩散型抓取采样器DexSampler和灵巧抓取评估器DexEvaluator。DexSampler通过对随机抓取进行迭代去噪，生成与物体点云条件相关的高质量抓取姿势。我们还引入了两种抓取优化策略：基于评估器的扩散(Evaluator-Guided Diffusion，EGD)和基于评估器的采样优化(Evaluator-based Sampling Refinement，ESR)。我们在虚拟环境和真实世界的实验中，使用Allegro Hand进行测试，结果表明DexDiffuser相比最先进的多指抓取生成方法FFHNet，平均抓取成功率提高了21.71-22.20%。

    We introduce DexDiffuser, a novel dexterous grasping method that generates, evaluates, and refines grasps on partial object point clouds. DexDiffuser includes the conditional diffusion-based grasp sampler DexSampler and the dexterous grasp evaluator DexEvaluator. DexSampler generates high-quality grasps conditioned on object point clouds by iterative denoising of randomly sampled grasps. We also introduce two grasp refinement strategies: Evaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR). Our simulation and real-world experiments on the Allegro Hand consistently demonstrate that DexDiffuser outperforms the state-of-the-art multi-finger grasp generation method FFHNet with an, on average, 21.71--22.20\% higher grasp success rate.
    
[^114]: 自动驾驶中用于行人检测的安全适应损失函数

    A Safety-Adapted Loss for Pedestrian Detection in Automated Driving

    [https://arxiv.org/abs/2402.02986](https://arxiv.org/abs/2402.02986)

    本文提出了一种安全意识的损失函数变体，利用训练过程中估计的每个行人的关键性评分，以增强对关键行人的检测性能。

    

    在安全关键领域，比如自动驾驶（AD）中，物体检测器的错误可能会危及行人和其他易受伤害的道路用户（VRU）。由于常见的评估指标不是一个合适的安全指标，最近的研究采用了一些方法来识别安全关键的VRU，并将风险反馈给物体检测器。然而，这些方法在深度神经网络（DNN）训练过程中并没有考虑安全因素。因此，最先进的DNN会对所有误检进行相同的惩罚，而不考虑它们的重要性。为了减少关键故障案例（即假阴性）的发生，可能需要一种安全意识的训练策略来提高对关键行人的检测性能。在本文中，我们提出了一种新的安全意识损失变分，在训练过程中利用了每个行人的关键性评分。我们利用了基于可达性集的碰撞时间（TTC-RSB）度量和距离...

    In safety-critical domains like automated driving (AD), errors by the object detector may endanger pedestrians and other vulnerable road users (VRU). As common evaluation metrics are not an adequate safety indicator, recent works employ approaches to identify safety-critical VRU and back-annotate the risk to the object detector. However, those approaches do not consider the safety factor in the deep neural network (DNN) training process. Thus, state-of-the-art DNN penalizes all misdetections equally irrespective of their criticality. Subsequently, to mitigate the occurrence of critical failure cases, i.e., false negatives, a safety-aware training strategy might be required to enhance the detection performance for critical pedestrians. In this paper, we propose a novel safety-aware loss variation that leverages the estimated per-pedestrian criticality scores during training. We exploit the reachability set-based time-to-collision (TTC-RSB) metric from the motion domain along with distan
    
[^115]: 无人机高分辨率影像的无监督语义分割用于路况场景解析

    Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing

    [https://arxiv.org/abs/2402.02985](https://arxiv.org/abs/2402.02985)

    本论文提出了一种无监督的路况解析框架，利用视觉语言模型和基本计算机视觉模型来解决无人机高分辨率影像下的路况场景解析问题。该框架首先利用视觉语言模型快速检测路况感兴趣区域，然后利用视觉基础模型生成路况区域掩模，然后采用自监督表示学习网络提取特征表示，最后通过无监督聚类算法对特征进行聚类和标记。

    

    在无人机图像中解析路况场景存在两个挑战。首先，无人机图像的高分辨率使得处理困难。其次，监督深度学习方法需要大量手动标注来训练强大而准确的模型。本文引入了一种无监督的路况解析框架，利用了近期在视觉语言模型和基本计算机视觉模型方面的进展。首先，采用视觉语言模型高效处理超大分辨率无人机图像，快速检测图像中的路况感兴趣区域。接下来，利用视觉基础模型SAM为没有类别信息的路况区域生成掩模。随后，采用自监督表示学习网络从所有掩模区域中提取特征表示。最后，应用无监督的聚类算法对这些特征表示进行聚类并为每个簇分配ID。然后，将掩模区域合并。

    Two challenges are presented when parsing road scenes in UAV images. First, the high resolution of UAV images makes processing difficult. Second, supervised deep learning methods require a large amount of manual annotations to train robust and accurate models. In this paper, an unsupervised road parsing framework that leverages recent advances in vision language models and fundamental computer vision model is introduced.Initially, a vision language model is employed to efficiently process ultra-large resolution UAV images to quickly detect road regions of interest in the images. Subsequently, the vision foundation model SAM is utilized to generate masks for the road regions without category information. Following that, a self-supervised representation learning network extracts feature representations from all masked regions. Finally, an unsupervised clustering algorithm is applied to cluster these feature representations and assign IDs to each cluster. The masked regions are combined w
    
[^116]: 机器人操纵器的故障诊断和容错控制方案综述：人工智能、机器学习和数字孪生的最新进展

    Review on Fault Diagnosis and Fault-Tolerant Control Scheme for Robotic Manipulators: Recent Advances in AI, Machine Learning, and Digital Twin

    [https://arxiv.org/abs/2402.02980](https://arxiv.org/abs/2402.02980)

    这篇综述文章介绍了针对机器人操纵器的故障诊断和容错控制方案的最新进展，重点关注人工智能、机器学习和数字孪生等尖端技术对机器人控制和容错能力的变革影响。

    

    这篇全面的综述文章深入探讨了针对机器人操纵器量身定制的容错控制（FTC）方案的复杂领域。我们的探索涵盖了FTC的历史演变，追溯其随时间的发展，并详细考察了人工智能（AI）、机器学习（ML）和数字孪生技术（DTT）等尖端技术相互融合所驱动的最新突破。本文特别强调了这些当代趋势在机器人操纵器控制和容错能力领域所产生的变革影响。通过深入研究历史背景，我们旨在提供对FTC方案演变的全面理解。这一过程包括了从基于模型和基于信号的方案过渡到传感器的作用，为后续探索AI、ML和DTT所带来的当今范式转变打下基础。我们在解剖这个复杂的氛围的同时，涵盖了各种近期研究所采用的AI、ML和DTT在机器人操纵器控制和容错能力方面的创新方法。

    This comprehensive review article delves into the intricate realm of fault-tolerant control (FTC) schemes tailored for robotic manipulators. Our exploration spans the historical evolution of FTC, tracing its development over time, and meticulously examines the recent breakthroughs fueled by the synergistic integration of cutting-edge technologies such as artificial intelligence (AI), machine learning (ML), and digital twin technologies (DTT). The article places a particular emphasis on the transformative influence these contemporary trends exert on the landscape of robotic manipulator control and fault tolerance.   By delving into the historical context, our aim is to provide a comprehensive understanding of the evolution of FTC schemes. This journey encompasses the transition from model-based and signal-based schemes to the role of sensors, setting the stage for an exploration of the present-day paradigm shift enabled by AI, ML, and DTT. The narrative unfolds as we dissect the intrica
    
[^117]: 变分流模型：以你的风格流动

    Variational Flow Models: Flowing in Your Style

    [https://arxiv.org/abs/2402.02977](https://arxiv.org/abs/2402.02977)

    我们引入了一种变分流模型的方法，并提出了一种系统的无需训练的转换方法，使得快速采样成为可能，同时保持了采样的准确性和效率。

    

    我们引入了一种对"后验流"模型进行变分推理解释的方法——用以将"概率流"推广到更广泛的随机过程类别，不必局限于扩散过程。我们将这种结果称为"变分流模型"。此外，我们提出了一种无需训练的系统方法，将由方程Xt = at * X0 + st * X1所描述的"线性"随机过程的后验流转化为直线恒速(SC)流，类似于矫正流。这种转化使得可以快速沿着原始的后验流进行采样，而无需训练一个新的SC流模型。我们的方法的灵活性使我们能够将转换扩展到两个不同"线性"随机过程的后验流之间进行互相转化。此外，我们还可以将高阶数值解法轻松集成到转换后的SC流中，进一步提高采样的准确性和效率。我们进行了严格的理论分析和大量实验结果的验证。

    We introduce a variational inference interpretation for models of "posterior flows" - generalizations of "probability flows" to a broader class of stochastic processes not necessarily diffusion processes. We coin the resulting models as "Variational Flow Models". Additionally, we propose a systematic training-free method to transform the posterior flow of a "linear" stochastic process characterized by the equation Xt = at * X0 + st * X1 into a straight constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation facilitates fast sampling along the original posterior flow without training a new model of the SC flow. The flexibility of our approach allows us to extend our transformation to inter-convert two posterior flows from distinct "linear" stochastic processes. Moreover, we can easily integrate high-order numerical solvers into the transformed SC flow, further enhancing sampling accuracy and efficiency. Rigorous theoretical analysis and extensive experimental result
    
[^118]: 提升，投票分类器和随机采样压缩方案

    Boosting, Voting Classifiers and Randomized Sample Compression Schemes

    [https://arxiv.org/abs/2402.02976](https://arxiv.org/abs/2402.02976)

    本研究提出了一种随机提升算法来解决传统提升算法的性能问题，并通过构建一个通用框架将样本压缩方法扩展到支持随机学习算法，实现了在样本大小上具有单对数依赖的泛化错误。

    

    在提升中，我们旨在利用多个弱学习器来产生一个强学习器。这个范式的核心是将强学习器建模为一个投票分类器，它输出弱学习器的加权多数投票。尽管许多成功的提升算法，如标志性的AdaBoost，产生投票分类器，但它们的理论性能长期以来一直不够优化：迄今为止，已知的使投票分类器达到给定准确性所需的训练样本数的最佳界限总是至少包含至多两个对数因子，而这已经超过了一般的弱到强学习器所能实现的范围。在这项工作中，我们通过提出一种随机提升算法打破这一障碍，该算法输出的投票分类器在样本大小上包含单对数依赖的泛化错误。我们通过构建一个通用框架将样本压缩方法扩展到支持随机学习算法来获得这个结果。

    In boosting, we aim to leverage multiple weak learners to produce a strong learner. At the center of this paradigm lies the concept of building the strong learner as a voting classifier, which outputs a weighted majority vote of the weak learners. While many successful boosting algorithms, such as the iconic AdaBoost, produce voting classifiers, their theoretical performance has long remained sub-optimal: the best known bounds on the number of training examples necessary for a voting classifier to obtain a given accuracy has so far always contained at least two logarithmic factors above what is known to be achievable by general weak-to-strong learners. In this work, we break this barrier by proposing a randomized boosting algorithm that outputs voting classifiers whose generalization error contains a single logarithmic dependency on the sample size. We obtain this result by building a general framework that extends sample compression methods to support randomized learning algorithms ba
    
[^119]: 检索增强的得分蒸馏用于文本到3D生成

    Retrieval-Augmented Score Distillation for Text-to-3D Generation

    [https://arxiv.org/abs/2402.02972](https://arxiv.org/abs/2402.02972)

    RetDream是一种针对文本到3D生成的检索增强的得分蒸馏方法，通过直接使用语义相关的资源，可以充分利用2D扩散模型的表现力和3D资源的几何一致性。

    

    文本到3D生成通过引入强大的2D扩散模型取得了显著的成功，但不足的3D先验知识也导致了3D几何的不一致性。最近，由于发布了大规模的多视角数据集，将扩散模型在多视角数据集上进行微调成为解决3D一致性问题的主流方法。然而，与2D数据相比，3D数据的质量和多样性有限，这导致了困难。为了回避这些平衡问题，我们探索了一种针对得分蒸馏的检索增强方法，名为RetDream。我们假设通过在优化过程中直接使用语义相关的资源，可以充分利用2D扩散模型的表现力和3D资源的几何一致性。为此，我们引入了一种新的基于检索的质量增强框架，用于文本到3D生成。我们利用检索到的资源来融入其

    Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry. Recently, since large-scale multi-view datasets have been released, fine-tuning the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem. However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data. To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score distillation, dubbed RetDream. We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process. To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation. We leverage the retrieved asset to incorporate its
    
[^120]: 关于注意力层的词敏感性的理解：通过随机特征的研究

    Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features

    [https://arxiv.org/abs/2402.02969](https://arxiv.org/abs/2402.02969)

    通过研究随机特征，我们发现注意力层具有较高的词敏感性，这对于理解transformers的成功以及自然语言处理任务中的上下文含义非常重要。

    

    揭示transformers异常成功背后原因需要更好地理解为什么注意力层适用于自然语言处理任务。特别是，这些任务要求预测模型捕捉上下文含义，即使句子很长，这往往取决于一个或几个词。我们的工作在随机特征的典型设置中研究了这一关键属性，称为词敏感性（WS）。我们展示了注意力层具有较高的WS，即在嵌入空间中存在一个向量，能够大幅扰动随机注意力特征映射。这个论点关键地利用了注意力层中softmax的作用，突显了它相对于其他激活函数（如ReLU）的优势。相反，标准随机特征的WS是$1/\sqrt{n}$阶的，$n$是文本样本中的单词数，因此它随上下文的长度而衰减。然后，我们将这些关于词敏感性的结果转化为泛化界：由于...

    Unveiling the reasons behind the exceptional success of transformers requires a better understanding of why attention layers are suitable for NLP tasks. In particular, such tasks require predictive models to capture contextual meaning which often depends on one or few words, even if the sentence is long. Our work studies this key property, dubbed word sensitivity (WS), in the prototypical setting of random features. We show that attention layers enjoy high WS, namely, there exists a vector in the space of embeddings that largely perturbs the random attention features map. The argument critically exploits the role of the softmax in the attention layer, highlighting its benefit compared to other activations (e.g., ReLU). In contrast, the WS of standard random features is of order $1/\sqrt{n}$, $n$ being the number of words in the textual sample, and thus it decays with the length of the context. We then translate these results on the word sensitivity into generalization bounds: due to th
    
[^121]: 探究多模态多任务基础模型用于道路场景理解：从学习视角的观点

    Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives

    [https://arxiv.org/abs/2402.02968](https://arxiv.org/abs/2402.02968)

    本调查对多模态多任务视觉理解基础模型在道路场景中的应用进行了系统分析，展示了其多模态和多任务学习能力，在处理各种驾驶相关任务方面具有强大的适应性，为实现对周围场景的更全面理解做出了重要贡献。

    

    基础模型的确对各个领域产生了深远的影响，成为显著塑造智能系统能力的关键组件。在智能车辆的背景下，利用基础模型的力量已被证明具有变革性，显著推进了视觉理解的进展。多模态多任务视觉理解基础模型(MM-VUFMs)具备多模态和多任务学习能力，能够有效地处理和融合来自不同模态的数据，并同时处理各种与驾驶相关的任务，具备强大的适应性，为对周围场景的更全面理解做出贡献。在本调查中，我们对专门设计用于道路场景的MM-VUFMs进行了系统分析。我们的目标不仅是提供对常见实践的综合概述，涉及任务特定模型、统一的多模态模型、统一的多任务模型和基础模型促进技术。

    Foundation models have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems. In the context of intelligent vehicles, leveraging the power of foundation models has proven to be transformative, offering notable advancements in visual understanding. Equipped with multi-modal and multi-task learning capabilities, multi-modal multi-task visual understanding foundation models (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene. In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes. Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techni
    
[^122]: 混合噪声与条件深度生成模型下的后验估计

    Mixed Noise and Posterior Estimation with Conditional DeepGEM

    [https://arxiv.org/abs/2402.02964](https://arxiv.org/abs/2402.02964)

    本论文提出了一种用于联合估计贝叶斯逆问题中后验概率和噪声参数的新算法，该算法通过期望最大化（EM）算法解决问题，并使用条件标准化流来近似后验概率。该模型能够整合来自多个测量的信息。

    

    受混合噪声模型的间接测量和纳米计量应用的启发，我们开发了一种新的算法，用于联合估计贝叶斯逆问题中的后验概率和噪声参数。我们提出通过期望最大化（EM）算法来解决这个问题。基于当前的噪声参数，我们在E步中学习了一个条件标准化流，以近似后验概率。在M步中，我们提出再次通过EM算法找到噪声参数的更新，其具有解析公式。我们将条件标准化流的训练与前向和反向KL进行比较，并展示我们的模型能够整合来自许多测量的信息，而不像之前的方法。

    Motivated by indirect measurements and applications from nanometrology with a mixed noise model, we develop a novel algorithm for jointly estimating the posterior and the noise parameters in Bayesian inverse problems. We propose to solve the problem by an expectation maximization (EM) algorithm. Based on the current noise parameters, we learn in the E-step a conditional normalizing flow that approximates the posterior. In the M-step, we propose to find the noise parameter updates again by an EM algorithm, which has analytical formulas. We compare the training of the conditional normalizing flow with the forward and reverse KL, and show that our model is able to incorporate information from many measurements, unlike previous approaches.
    
[^123]: 利用彩色到热成像AI进行建筑外观检测的单类异常检测

    One-class anomaly detection through color-to-thermal AI for building envelope inspection

    [https://arxiv.org/abs/2402.02963](https://arxiv.org/abs/2402.02963)

    通过彩色图像到热成像的AI驱动预测，我们提出了一种无标签的方法用于检测建筑外观热成像检测中的异常情况，可以应用于辅助日常建筑检查或自动化检查大面积。

    

    我们提出了一种无标签的方法来检测建筑外观热成像检测中的异常情况。该方法基于由AI驱动的从彩色图像预测热分布。该方法实际上是一个一类分类器，用于识别预测和实际热分布之间有较大差异的热图区域。该算法可以通过选择用于训练的目标样本来学习将某些特征标识为正常或异常。我们通过使用在不同室外温度下收集的数据来演示了这个原理，从而检测到了热桥。该方法可以应用于辅助人类专业人员进行日常建筑检查，或与移动平台结合以自动化对大面积进行检查。

    We present a label-free method for detecting anomalies during thermographic inspection of building envelopes. It is based on the AI-driven prediction of thermal distributions from color images. Effectively the method performs as a one-class classifier of the thermal image regions with high mismatch between the predicted and actual thermal distributions. The algorithm can learn to identify certain features as normal or anomalous by selecting the target sample used for training. We demonstrated this principle by training the algorithm with data collected at different outdoors temperature, which lead to the detection of thermal bridges. The method can be implemented to assist human professionals during routine building inspections or combined with mobile platforms for automating examination of large areas.
    
[^124]: 多智能体强化学习用于协助无人机卸载蜂窝通信

    Multi-Agent Reinforcement Learning for Offloading Cellular Communications with Cooperating UAVs

    [https://arxiv.org/abs/2402.02957](https://arxiv.org/abs/2402.02957)

    本文提出了一种新颖的方法，通过联合优化无人机轨迹和用户关联指标，最大化用户与无人机的关联，以有效地最大化多个无人机在卸载地面基站的数据流量方面的利用率。

    

    在物联网应用的背景下，有效的解决地面蜂窝网络中的智能数据收集方案至关重要。地面基站的有限频谱和覆盖范围给网络用户的数据率需求带来了挑战。无人机以其高敏捷性、移动性和灵活性而闻名，为卸载地面基站的数据流量提供了另一种手段，成为额外的接入点。本文介绍了一种新颖的方法，以有效地最大化多个无人机在卸载地面基站的数据流量方面的利用率。具体而言，重点是在质量保证约束下，通过联合优化无人机轨迹和用户关联指标，最大化用户与无人机的关联。由于所制定的无人机控制问题是非凸和组合的，本研究利用多智能体强化学习框架。在该框架中，每个无人机以非合作方式寻求从环境中学习，并通过学习来优化自己的策略.

    Effective solutions for intelligent data collection in terrestrial cellular networks are crucial, especially in the context of Internet of Things applications. The limited spectrum and coverage area of terrestrial base stations pose challenges in meeting the escalating data rate demands of network users. Unmanned aerial vehicles, known for their high agility, mobility, and flexibility, present an alternative means to offload data traffic from terrestrial BSs, serving as additional access points. This paper introduces a novel approach to efficiently maximize the utilization of multiple UAVs for data traffic offloading from terrestrial BSs. Specifically, the focus is on maximizing user association with UAVs by jointly optimizing UAV trajectories and users association indicators under quality of service constraints. Since, the formulated UAVs control problem is nonconvex and combinatorial, this study leverages the multi agent reinforcement learning framework. In this framework, each UAV a
    
[^125]: AdaTreeFormer: 从一张高分辨率图像中进行树木计数的少样本领域自适应

    AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image

    [https://arxiv.org/abs/2402.02956](https://arxiv.org/abs/2402.02956)

    AdaTreeFormer是一种从源领域学习并适应只有有限数量标注树木的目标领域的框架，利用一个共享的编码器和分层特征提取方案，实现了树木计数的少样本领域自适应。

    

    仅使用一张航空或卫星图像来估计和计数树木密度是摄影测量和遥感领域中一项困难的任务。然而，它在森林管理中起着至关重要的作用。不同地形上各种各样的树木种类严重阻碍了树木计数模型的良好表现。本文旨在提出一个从具有足够标注树木的源领域学习并适应只有有限数量标注树木的目标领域的框架。我们的方法称为AdaTreeFormer，包含一个共享的编码器和一个分层特征提取方案，用于从源领域和目标领域中提取稳健的特征。它还包括三个子网络：两个用于分别从源领域和目标领域提取自注意力图，并一个用于提取跨领域注意力图。对于后者，引入了一种注意力适应机制，用于从不同领域中提取相关信息。

    The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing. However, it plays a crucial role in the management of forests. The huge variety of trees in varied topography severely hinders tree counting models to perform well. The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains. It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps. For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different doma
    
[^126]: 解决分层信息共享的分布式部分可观察马尔可夫决策过程：一种广义博弈方法

    Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form Game Approach

    [https://arxiv.org/abs/2402.02954](https://arxiv.org/abs/2402.02954)

    本文通过应用最优性原理研究了分层信息共享的分布式部分可观察马尔可夫决策过程的解决方法。通过将问题分解成单阶段子游戏，并通过进一步分解子游戏，我们成功地解开了决策变量的纠缠，同时显著减少了时间复杂度。

    

    最近的理论表明，多人分散的部分可观察马尔可夫决策过程可以转化为等效的单人游戏，使得可以应用贝尔曼的最优性原理通过将其分解为单阶段子游戏来解决单人游戏。然而，这种方法在每个单阶段子游戏中纠缠了所有玩家的决策变量，导致指数复杂度的备份。本文展示了如何在保持分层信息共享的前提下解开这些决策变量的纠缠，这是我们社会中一种突出的管理风格。为了实现这个目标，我们应用最优性原理通过进一步将任何单阶段子游戏分解为更小的子游戏来解决它，使我们能够逐次进行单人决策。我们的方法揭示了存在于单阶段子游戏中的广义博弈解决方案，极大地减少了时间复杂度。我们的实验结果验证了我们方法的有效性，证明它可以在解决分层信息共享的分布式部分可观察马尔可夫决策过程中发挥重要作用。

    A recent theory shows that a multi-player decentralized partially observable Markov decision process can be transformed into an equivalent single-player game, enabling the application of \citeauthor{bellman}'s principle of optimality to solve the single-player game by breaking it down into single-stage subgames. However, this approach entangles the decision variables of all players at each single-stage subgame, resulting in backups with a double-exponential complexity. This paper demonstrates how to disentangle these decision variables while maintaining optimality under hierarchical information sharing, a prominent management style in our society. To achieve this, we apply the principle of optimality to solve any single-stage subgame by breaking it down further into smaller subgames, enabling us to make single-player decisions at a time. Our approach reveals that extensive-form games always exist with solutions to a single-stage subgame, significantly reducing time complexity. Our expe
    
[^127]: 揭示解决Android恶意软件检测的机器学习方案的关键

    Unraveling the Key of Machine Learning Solutions for Android Malware Detection

    [https://arxiv.org/abs/2402.02953](https://arxiv.org/abs/2402.02953)

    本文通过调查和分析，提出了一个全面研究基于机器学习的Android恶意软件检测的方案，并重新实现了12个代表性方法的评估。

    

    Android恶意软件检测作为对恶意应用程序的第一道防线。随着机器学习（ML）的快速发展，基于ML的Android恶意软件检测因其能够自动捕获Android APK中的恶意模式而受到越来越多的关注。这些基于学习的方法在检测恶意软件方面取得了有希望的结果。然而，缺乏对当前研究进展的深入分析，使得很难对这一领域的最新发展有一个全面的了解。本文对基于ML的Android恶意软件检测进行了全面的实证和定量分析。我们首先对文献进行了调查，并根据Android特征工程和ML建模过程将贡献分类。然后，我们设计了一个通用的ML-based Android恶意软件检测框架，重新实现了来自不同研究社区的12个代表性方法，并从三个方面对其进行了评估。

    Android malware detection serves as the front line against malicious apps. With the rapid advancement of machine learning (ML), ML-based Android malware detection has attracted increasing attention due to its capability of automatically capturing malicious patterns from Android APKs. These learning-driven methods have reported promising results in detecting malware. However, the absence of an in-depth analysis of current research progress makes it difficult to gain a holistic picture of the state of the art in this area.   This paper presents a comprehensive investigation to date into ML-based Android malware detection with empirical and quantitative analysis. We first survey the literature, categorizing contributions into a taxonomy based on the Android feature engineering and ML modeling pipeline. Then, we design a general-propose framework for ML-based Android malware detection, re-implement 12 representative approaches from different research communities, and evaluate them from thr
    
[^128]: 关于Softmax Gating混合专家模型中最小二乘估计的研究

    On Least Squares Estimation in Softmax Gating Mixture of Experts

    [https://arxiv.org/abs/2402.02952](https://arxiv.org/abs/2402.02952)

    本研究探讨了在确定性MoE模型下使用最小二乘估计器的性能，并建立了强可识别性条件来描述不同类型专家函数的收敛行为。

    

    专家模型是一种统计机器学习设计，使用Softmax Gating函数聚合多个专家网络，以形成一个更复杂和表达力更强的模型。尽管由于可扩展性而在多个应用领域中广泛使用，但MoE模型的数学和统计性质复杂且难以分析。因此，以前的理论工作主要集中在概率MoE模型上，这些模型假设数据是由高斯MoE模型生成的，这在实践中是不切实际的。在这项工作中，我们研究了在确定性MoE模型下最小二乘估计器（LSE）的性能，在该模型中，数据根据回归模型进行采样，这是一个尚未被充分探索的设置。我们建立了一个称为强可识别性的条件，以表征不同类型专家函数的收敛行为。我们证明了对于强可识别专家的估计速度，即

    Mixture of experts (MoE) model is a statistical machine learning design that aggregates multiple expert networks using a softmax gating function in order to form a more intricate and expressive model. Despite being commonly used in several applications owing to their scalability, the mathematical and statistical properties of MoE models are complex and difficult to analyze. As a result, previous theoretical works have primarily focused on probabilistic MoE models by imposing the impractical assumption that the data are generated from a Gaussian MoE model. In this work, we investigate the performance of the least squares estimators (LSE) under a deterministic MoE model where the data are sampled according to a regression model, a setting that has remained largely unexplored. We establish a condition called strong identifiability to characterize the convergence behavior of various types of expert functions. We demonstrate that the rates for estimating strongly identifiable experts, namel
    
[^129]: 动态拜占庭-强鲁棒学习：适应切换拜占庭工作机制

    Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine Workers

    [https://arxiv.org/abs/2402.02951](https://arxiv.org/abs/2402.02951)

    $\textsf{DynaBRO}$是一种动态拜占庭-强鲁棒学习的方法，能够适应切换拜占庭工作机制，并且在渐近收敛速率上与静态情况相匹配。通过多级蒙特卡洛渐变估计技术、强鲁棒工作机制更新的聚合和故障安全过滤器的引入，我们的方法能够经受住$\mathcal{O}(\sqrt{T})$轮拜占庭身份的改变。另外，通过使用自适应学习率，我们的方法消除了对百分比的需求。

    

    拜占庭-强鲁棒学习作为一种突出的容错分布式机器学习框架已经出现。然而，大多数技术考虑的是静态情况，其中在学习过程中拜占庭机器的身份保持不变。这种假设不能捕捉到现实世界中的动态拜占庭行为，可能包括短暂故障或有针对性的时间攻击。为了解决这个限制，我们提出了一种新的方法$\textsf{DynaBRO}$，它能够经受住$\mathcal{O}(\sqrt{T})$轮拜占庭身份的改变（其中$T$是总训练轮数），同时与静态情况下的渐近收敛速率相匹配。我们的方法将多级蒙特卡洛（MLMC）渐变估计技术与工作机制更新的强鲁棒聚合相结合，并引入了一个故障安全过滤器来限制动态拜占庭策略的偏差。此外，通过利用自适应学习率，我们的方法消除了对百分比的需求。

    Byzantine-robust learning has emerged as a prominent fault-tolerant distributed machine learning framework. However, most techniques consider the static setting, wherein the identity of Byzantine machines remains fixed during the learning process. This assumption does not capture real-world dynamic Byzantine behaviors, which may include transient malfunctions or targeted temporal attacks. Addressing this limitation, we propose $\textsf{DynaBRO}$ -- a new method capable of withstanding $\mathcal{O}(\sqrt{T})$ rounds of Byzantine identity alterations (where $T$ is the total number of training rounds), while matching the asymptotic convergence rate of the static setting. Our method combines a multi-level Monte Carlo (MLMC) gradient estimation technique with robust aggregation of worker updates and incorporates a fail-safe filter to limit bias from dynamic Byzantine strategies. Additionally, by leveraging an adaptive learning rate, our approach eliminates the need for knowing the percentag
    
[^130]: 外分布检测的核PCA

    Kernel PCA for Out-of-Distribution Detection

    [https://arxiv.org/abs/2402.02949](https://arxiv.org/abs/2402.02949)

    本论文提出了使用核PCA进行外分布检测的方法，通过在主成分子空间中引入非线性映射，实现了对内分布和外分布数据的有效区分。

    

    外分布（OoD）检测对于深度神经网络（DNN）的可靠性至关重要。现有的研究表明，直接应用于DNN特征的主成分分析（PCA）在检测来自内分布（InD）数据的OoD数据方面不足够。PCA的失败表明，仅通过在线性子空间中进行简单处理无法很好地将OoD和InD中的网络特征分离开来，而可以通过适当的非线性映射来解决。在这项工作中，我们利用核PCA（KPCA）框架进行OoD检测，寻找OoD和InD特征以显著不同的模式分配的子空间。我们设计了两种特征映射，在KPCA中引入非线性内核，以促进在主成分张成的子空间中InD和OoD数据之间的可分性。然后，通过在这种子空间中的重构误差，可以有效地得到$\mathcal{O}(1)$时间复杂度的检测结果。

    Out-of-Distribution (OoD) detection is vital for the reliability of Deep Neural Networks (DNNs). Existing works have shown the insufficiency of Principal Component Analysis (PCA) straightforwardly applied on the features of DNNs in detecting OoD data from In-Distribution (InD) data. The failure of PCA suggests that the network features residing in OoD and InD are not well separated by simply proceeding in a linear subspace, which instead can be resolved through proper nonlinear mappings. In this work, we leverage the framework of Kernel PCA (KPCA) for OoD detection, seeking subspaces where OoD and InD features are allocated with significantly different patterns. We devise two feature mappings that induce non-linear kernels in KPCA to advocate the separability between InD and OoD data in the subspace spanned by the principal components. Given any test sample, the reconstruction error in such subspace is then used to efficiently obtain the detection result with $\mathcal{O}(1)$ time comp
    
[^131]: HoughToRadon变换：投影空间特征改进的新型神经网络层

    HoughToRadon Transform: New Neural Network Layer for Features Improvement in Projection Space

    [https://arxiv.org/abs/2402.02946](https://arxiv.org/abs/2402.02946)

    本文引入了HoughToRadon变换层，该层通过在Hough变换层后进行改良，使得神经网络的速度得到提升，并且在语义图像分割问题上取得了97.7％的准确率。

    

    在本文中，我们介绍了HoughToRadon变换层，这是一种新颖的层，旨在改进与Hough变换相结合的神经网络的速度，以解决语义图像分割问题。通过在Hough变换层之后放置该层，“内部”卷积可以接收到具有新的有益属性的修改特征图，如较小的图像处理区域和参数空间与角度和偏移的线性性。这些属性在单独使用Hough变换时是不具备的。此外，HoughToRadon变换层允许我们使用两个新参数调整中间特征图的大小，从而使我们能够平衡结果神经网络的速度和质量。我们在开放的MIDV-500数据集上的实验证明，这种新方法在文档分割任务中节省了时间，并实现了97.7％的最新准确率，优于计算复杂度更大的HoughEncoder。

    In this paper, we introduce HoughToRadon Transform layer, a novel layer designed to improve the speed of neural networks incorporated with Hough Transform to solve semantic image segmentation problems. By placing it after a Hough Transform layer, "inner" convolutions receive modified feature maps with new beneficial properties, such as a smaller area of processed images and parameter space linearity by angle and shift. These properties were not presented in Hough Transform alone. Furthermore, HoughToRadon Transform layer allows us to adjust the size of intermediate feature maps using two new parameters, thus allowing us to balance the speed and quality of the resulting neural network. Our experiments on the open MIDV-500 dataset show that this new approach leads to time savings in document segmentation tasks and achieves state-of-the-art 97.7% accuracy, outperforming HoughEncoder with larger computational complexity.
    
[^132]: 探索卷积神经网络（CNN）和视觉变换器（ViT）混合架构在计算机视觉中的协同作用：一项调查

    Exploring the Synergies of Hybrid CNNs and ViTs Architectures for Computer Vision: A survey

    [https://arxiv.org/abs/2402.02941](https://arxiv.org/abs/2402.02941)

    本文调查了卷积神经网络和视觉变换器混合架构在计算机视觉中的协同作用，提供了对最新混合CNN-ViT架构的综述。翻译

    

    卷积神经网络（CNN）和视觉变换器（ViT）混合架构的协同作用已经成为一种突破性的方法，推动了计算机视觉的发展。这篇综述对最新的混合CNN-ViT架构的文献进行了全面的研究，探讨了这两种方法之间的协同作用。该调查的主要内容包括：(1)对普通CNN和ViT的背景介绍，(2)系统性地研究不同分类混合设计，探索融合CNN和ViT模型所实现的协同作用，(3)比较不同混合架构之间的应用任务特定的协同作用和分析，(4)混合模型面临的挑战和未来方向，(5)最后，调查总结了关键发现和建议。通过这种对混合计算机视觉架构的探索，该调查旨在作为一个指南，促进对CNN和ViT之间复杂动态的深入理解。

    The hybrid of Convolutional Neural Network (CNN) and Vision Transformers (ViT) architectures has emerged as a groundbreaking approach, pushing the boundaries of computer vision (CV). This comprehensive review provides a thorough examination of the literature on state-of-the-art hybrid CNN-ViT architectures, exploring the synergies between these two approaches. The main content of this survey includes: (1) a background on the vanilla CNN and ViT, (2) systematic review of various taxonomic hybrid designs to explore the synergy achieved through merging CNNs and ViTs models, (3) comparative analysis and application task-specific synergy between different hybrid architectures, (4) challenges and future directions for hybrid models, (5) lastly, the survey concludes with a summary of key findings and recommendations. Through this exploration of hybrid CV architectures, the survey aims to serve as a guiding resource, fostering a deeper understanding of the intricate dynamics between CNNs and V
    
[^133]: 使用LSTM设计与实现Kubernetes集群的自动化灾难恢复系统

    Design and Implementation of an Automated Disaster-recovery System for a Kubernetes Cluster Using LSTM

    [https://arxiv.org/abs/2402.02938](https://arxiv.org/abs/2402.02938)

    本研究设计和实现了一个使用LSTM的自动化灾难恢复系统，该系统能够快速检测灾难并在15秒内自动从另一个Kubernetes集群恢复应用程序，提高了云环境中的数据管理和恢复效率。

    

    随着现代商业环境中数据的日益重要性，有效的数据管理和保护策略正受到越来越多的研究关注。在云环境中，数据保护对于维护信息资产和保持可持续服务至关重要。本研究引入了一个系统结构，将Kubernetes管理平台与备份和恢复工具集成在一起。该系统旨在立即检测灾难，并自动从另一个Kubernetes集群恢复应用程序。实验结果表明，该系统在15秒内执行恢复过程，无需人工干预，实现了快速恢复。这从根本上减少了与手动恢复过程相比的潜在延迟和错误，从而提高了云环境中的数据管理和恢复效率。此外，我们的研究模型使用长短期记忆（LSTM）预测了集群的CPU利用率。

    With the increasing importance of data in the modern business environment, effective data man-agement and protection strategies are gaining increasing research attention. Data protection in a cloud environment is crucial for safeguarding information assets and maintaining sustainable services. This study introduces a system structure that integrates Kubernetes management plat-forms with backup and restoration tools. This system is designed to immediately detect disasters and automatically recover applications from another kubernetes cluster. The experimental results show that this system executes the restoration process within 15 s without human intervention, enabling rapid recovery. This, in turn, significantly reduces the potential for delays and errors compared with manual recovery processes, thereby enhancing data management and recovery ef-ficiency in cloud environments. Moreover, our research model predicts the CPU utilization of the cluster using Long Short-Term Memory (LSTM). T
    
[^134]: 具有门控卷积和上下文重建损失的全景图像修复

    Panoramic Image Inpainting With Gated Convolution And Contextual Reconstruction Loss

    [https://arxiv.org/abs/2402.02936](https://arxiv.org/abs/2402.02936)

    本论文提出了一种全景图像修复框架，采用门控卷积来区分有效像素和无效像素，并利用上下文重建损失来引导生成器找到最适合修复缺失区域的参考补丁。

    

    基于深度学习的方法在处理全景图像修补任务中取得了令人鼓舞的结果。然而，现有的方法很难区分有效像素和无效像素，并找到合适的参考区域来修补受损区域，从而导致修复结果中出现伪影。为了应对这些挑战，我们提出了一个全景图像修复框架，包括一个Face生成器，一个Cube生成器，一个侧分支和两个判别器。我们使用立方体映射（CMP）格式作为网络输入。生成器采用门控卷积来区分有效像素和无效像素，同时设计了一个侧分支，利用上下文重建（CR）损失来指导生成器找到最适合修复缺失区域的参考补丁。在PSNR和SSIM指标下，将所提出的方法与最先进的方法在SUN360街景数据集上进行了比较。实验结果和消融研究表明，

    Deep learning-based methods have demonstrated encouraging results in tackling the task of panoramic image inpainting. However, it is challenging for existing methods to distinguish valid pixels from invalid pixels and find suitable references for corrupted areas, thus leading to artifacts in the inpainted results. In response to these challenges, we propose a panoramic image inpainting framework that consists of a Face Generator, a Cube Generator, a side branch, and two discriminators. We use the Cubemap Projection (CMP) format as network input. The generator employs gated convolutions to distinguish valid pixels from invalid ones, while a side branch is designed utilizing contextual reconstruction (CR) loss to guide the generators to find the most suitable reference patch for inpainting the missing region. The proposed method is compared with state-of-the-art (SOTA) methods on SUN360 Street View dataset in terms of PSNR and SSIM. Experimental results and ablation study demonstrate tha
    
[^135]: InterpretCC: 适于解释的神经网络的条件计算

    InterpretCC: Conditional Computation for Inherently Interpretable Neural Networks

    [https://arxiv.org/abs/2402.02933](https://arxiv.org/abs/2402.02933)

    InterpretCC是一种新的解释性神经网络模型，通过条件计算和稀疏激活特征，在保持性能的同时实现了人类中心的解释能力。该模型适用于需要可信解释、可操作解释和准确预测的人类面向领域。

    

    神经网络的真实世界解释性在三个方面之间存在权衡：1）需要人类信任解释的近似（例如事后方法）；2）削弱了解释的可理解性（例如自动识别的特征掩码）；3）削弱了模型性能（例如决策树）。这些缺点对于面向人类的领域（如教育、医疗保健或自然语言）是不可接受的，这些领域需要可信的解释、可操作的解释和准确的预测。在这项工作中，我们提出了InterpretCC（可解释的条件计算），这是一种可解释性的设计神经网络系列，通过在预测之前自适应和稀疏地激活特征，确保人类中心的可解释性，同时保持与最先进模型相当的性能。我们将这个思想扩展为可解释的专家混合模型，允许人们离散地指定兴趣话题。

    Real-world interpretability for neural networks is a tradeoff between three concerns: 1) it requires humans to trust the explanation approximation (e.g. post-hoc approaches), 2) it compromises the understandability of the explanation (e.g. automatically identified feature masks), and 3) it compromises the model performance (e.g. decision trees). These shortcomings are unacceptable for human-facing domains, like education, healthcare, or natural language, which require trustworthy explanations, actionable interpretations, and accurate predictions. In this work, we present InterpretCC (interpretable conditional computation), a family of interpretable-by-design neural networks that guarantee human-centric interpretability while maintaining comparable performance to state-of-the-art models by adaptively and sparsely activating features before prediction. We extend this idea into an interpretable mixture-of-experts model, that allows humans to specify topics of interest, discretely separate
    
[^136]: 多语义搜索的领域自适应- 文献综述

    Domain Adaptation of Multilingual Semantic Search - Literature Review

    [https://arxiv.org/abs/2402.02932](https://arxiv.org/abs/2402.02932)

    这篇文献综述对当前在低资源环境下进行领域自适应和多语义搜索的方法进行了概述，提出了一种新的方法来聚类和有效地组合这些方法，并探讨了在低资源环境下将多语义搜索与领域自适应方法进行组合的可能性。

    

    这篇文献综述概述了在低资源环境下进行领域自适应和进行多语义搜索的当前方法。我们开发了一个新的分类方法来对领域自适应方法进行聚类，基于密集文本信息检索系统的部分，并注重如何有效地将它们进行组合。我们还探讨了将多语义搜索与低资源环境下的密集检索器的领域自适应方法进行组合的可能性。

    This literature review gives an overview of current approaches to perform domain adaptation in a low-resource and approaches to perform multilingual semantic search in a low-resource setting. We developed a new typology to cluster domain adaptation approaches based on the part of dense textual information retrieval systems, which they adapt, focusing on how to combine them efficiently. We also explore the possibilities of combining multilingual semantic search with domain adaptation approaches for dense retrievers in a low-resource setting.
    
[^137]: 将硬件近似嵌入离散基因训练中以用于印刷多层感知器

    Embedding Hardware Approximations in Discrete Genetic-based Training for Printed MLPs

    [https://arxiv.org/abs/2402.02930](https://arxiv.org/abs/2402.02930)

    本文将硬件近似嵌入到印刷多层感知器的训练过程中，通过离散遗传算法实现了最大化硬件近似的效益，在5%的精度损失下，相比基线，实现了超过5倍的面积和功耗的减少，并且超过了最先进的近似方法。

    

    印刷电子是一种有着低成本和灵活制造等独特特点的有望广泛应用于计算领域的技术。与传统的硅基技术不同，印刷电子可以实现可伸缩、可适应、非毒性的硬件。然而，由于印刷电子的特性尺寸较大，要实现复杂的电路如机器学习分类器是具有挑战性的。近似计算被证明可以降低机器学习电路（如多层感知器）的硬件成本。在本文中，我们通过将硬件近似嵌入到多层感知器的训练过程中来最大化近似计算的益处。由于硬件近似的离散性，我们提出并实现了一种基于遗传算法的硬件感知训练方法，专门为印刷多层感知器设计。在5%的精度损失下，相比基线，我们的多层感知器在面积和功耗上实现了超过5倍的减少，并且超过了最先进的近似方法。

    Printed Electronics (PE) stands out as a promisingtechnology for widespread computing due to its distinct attributes, such as low costs and flexible manufacturing. Unlike traditional silicon-based technologies, PE enables stretchable, conformal,and non-toxic hardware. However, PE are constrained by larger feature sizes, making it challenging to implement complex circuits such as machine learning (ML) classifiers. Approximate computing has been proven to reduce the hardware cost of ML circuits such as Multilayer Perceptrons (MLPs). In this paper, we maximize the benefits of approximate computing by integrating hardware approximation into the MLP training process. Due to the discrete nature of hardware approximation, we propose and implement a genetic-based, approximate, hardware-aware training approach specifically designed for printed MLPs. For a 5% accuracy loss, our MLPs achieve over 5x area and power reduction compared to the baseline while outperforming state of-the-art approximate
    
[^138]: 历史飞机的XXL-CT实例分割挑战

    Instance Segmentation XXL-CT Challenge of a Historic Airplane

    [https://arxiv.org/abs/2402.02928](https://arxiv.org/abs/2402.02928)

    该论文介绍了一个历史飞机的XXL-CT实例分割挑战，该挑战旨在评估机器学习图像分割领域的最新进展，并探索了自动或交互式实例分割方法在有效勾勒飞机组件的能力和局限性。

    

    在无损检测中，复合对象的XXL-CT成像实例分割面临独特的挑战。这种复杂性来源于缺乏已知的参考分割标签、适用的分割工具有限以及部分退化的图像质量。为了评估机器学习图像分割领域的最新进展，进行了“历史飞机的XXL-CT实例分割挑战”。该挑战旨在探索自动或交互式实例分割方法，以有效勾勒出飞机的不同组件，例如螺钉、铆钉、金属板或压力管。我们报告了这个挑战的组织和结果，并描述了提交的分割方法的能力和局限性。

    Instance segmentation of compound objects in XXL-CT imagery poses a unique challenge in non-destructive testing. This complexity arises from the lack of known reference segmentation labels, limited applicable segmentation tools, as well as partially degraded image quality. To asses recent advancements in the field of machine learning-based image segmentation, the "Instance Segmentation XXL-CT Challenge of a Historic Airplane" was conducted. The challenge aimed to explore automatic or interactive instance segmentation methods for an efficient delineation of the different aircraft components, such as screws, rivets, metal sheets or pressure tubes. We report the organization and outcome of this challenge and describe the capabilities and limitations of the submitted segmentation methods.
    
[^139]: 用具有同源转换器的监督链接预测任务进行自动同源检测

    Automated Cognate Detection as a Supervised Link Prediction Task with Cognate Transformer

    [https://arxiv.org/abs/2402.02926](https://arxiv.org/abs/2402.02926)

    本文提出了一种基于Transformer的方法，用于自动同源检测，实验证明该方法在一定程度的监督下表现优于现有方法，并能随着进一步增加监督而稳定改进。同时，我们还证明了接受多个序列对齐作为输入，并具有端到端架构的重要性。

    

    在历史语言学中，识别相关语言中的同源词是一个主要问题之一。自动同源识别对于识别音位对应关系、原始语言重建、语系分类等多个下游任务都有帮助。以往同源识别的最先进方法大多基于跨多语言词表计算的音素分布，对定义同源簇之间链接的同源标签的使用较少。本文提出了一种以计算生物学为灵感的基于Transformer的架构，用于自动同源检测。在一定程度的监督下，该方法的性能优于现有方法，并且在进一步增加监督的情况下表现出稳定的改进，从而证明了利用标记信息的有效性。我们还证明了接受多个序列对齐作为输入，并具有端到端架构的重要性。

    Identification of cognates across related languages is one of the primary problems in historical linguistics. Automated cognate identification is helpful for several downstream tasks including identifying sound correspondences, proto-language reconstruction, phylogenetic classification, etc. Previous state-of-the-art methods for cognate identification are mostly based on distributions of phonemes computed across multilingual wordlists and make little use of the cognacy labels that define links among cognate clusters. In this paper, we present a transformer-based architecture inspired by computational biology for the task of automated cognate detection. Beyond a certain amount of supervision, this method performs better than the existing methods, and shows steady improvement with further increase in supervision, thereby proving the efficacy of utilizing the labeled information. We also demonstrate that accepting multiple sequence alignments as input and having an end-to-end architecture
    
[^140]: 使用增量评估挖掘最小的行为模式集合的方法

    Mining a Minimal Set of Behavioral Patterns using Incremental Evaluation

    [https://arxiv.org/abs/2402.02921](https://arxiv.org/abs/2402.02921)

    本论文介绍了一种使用增量评估挖掘最小的行为模式集合的方法，以解决现有方法的可扩展性和实际应用中冗余模式的问题。

    

    过程挖掘提供了分析信息系统在执行过程中生成的事件日志的方法。它支持从医疗保健、制造业到电子商务等领域的过程设计、验证和执行。为了探索具有大量行为可变性的灵活流程的规律性，建议挖掘共同描述底层流程的重复行为模式。然而，现有的行为模式挖掘方法存在两个限制。首先，它们在生成模式候选人时只采用了增量计算，但在评估其质量时却没有使用增量计算，因此可扩展性有限。其次，基于挖掘模式的过程分析在实际应用场景中获得了大量冗余模式，导致效果有限。在本文中，我们解决了这些限制，以便更好地分析复杂过程。

    Process mining provides methods to analyse event logs generated by information systems during the execution of processes. It thereby supports the design, validation, and execution of processes in domains ranging from healthcare, through manufacturing, to e-commerce. To explore the regularities of flexible processes that show a large behavioral variability, it was suggested to mine recurrent behavioral patterns that jointly describe the underlying process. Existing approaches to behavioral pattern mining, however, suffer from two limitations. First, they show limited scalability as incremental computation is incorporated only in the generation of pattern candidates, but not in the evaluation of their quality. Second, process analysis based on mined patterns shows limited effectiveness due to an overwhelmingly large number of patterns obtained in practical application scenarios, many of which are redundant. In this paper, we address these limitations to facilitate the analysis of complex
    
[^141]: DS-MS-TCN: 使用双尺度多阶段时间卷积网络的Otago体操识别

    DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network

    [https://arxiv.org/abs/2402.02910](https://arxiv.org/abs/2402.02910)

    本研究提出了一种使用双尺度多阶段时间卷积网络的Otago体操识别方法，通过单个腰部佩戴的IMU在老年人的日常生活中实现准确且稳定的识别，为康复举措提供支持。

    

    Otago运动计划是针对老年人的重要康复举措，旨在增强平衡和力量。本研究利用单个腰部佩戴的惯性测量单元(IMU)，在老年人的日常生活中识别Otago体操动作，以解决现有研究在准确性和稳定性方面的限制。研究在实验室设置中招募了36名老年人，并对额外招募的7名老年人进行了家庭评估。研究提出了一种双尺度多阶段时间卷积网络(DS-MS-TCN)，用于两级序列到序列分类，将其纳入一个损失函数。在第一阶段，模型专注于识别每个体操动作的重复次数(微标签)。随后的阶段扩展了识别范围，包括完整的运动序列。

    The Otago Exercise Program (OEP) represents a crucial rehabilitation initiative tailored for older adults, aimed at enhancing balance and strength. Despite previous efforts utilizing wearable sensors for OEP recognition, existing studies have exhibited limitations in terms of accuracy and robustness. This study addresses these limitations by employing a single waist-mounted Inertial Measurement Unit (IMU) to recognize OEP exercises among community-dwelling older adults in their daily lives. A cohort of 36 older adults participated in laboratory settings, supplemented by an additional 7 older adults recruited for at-home assessments. The study proposes a Dual-Scale Multi-Stage Temporal Convolutional Network (DS-MS-TCN) designed for two-level sequence-to-sequence classification, incorporating them in one loss function. In the first stage, the model focuses on recognizing each repetition of the exercises (micro labels). Subsequent stages extend the recognition to encompass the complete ra
    
[^142]: 多层住宅建筑热动力学的灰盒模型的数字孪生

    Digital Twin for Grey Box modeling of Multistory residential building thermal dynamics

    [https://arxiv.org/abs/2402.02909](https://arxiv.org/abs/2402.02909)

    本研究提出了一种架构，通过将实时物联网数据与建筑物的3D表示集成，来促进灰盒建模来研究建筑的热动力学。

    

    建筑能效是一个广泛研究的课题，由于对环境问题和能源独立性的关注，它正在迅速受到关注。在北欧，仅供热能就占到总建筑能耗的70％。工业4.0技术，如物联网、大数据、云计算和机器学习，以及预测性和主动性数字孪生的创建，可以帮助减少这个数字。然而，建筑的热动力学是一个非常复杂的过程，依赖于许多变量。因此，常用的基于物理的白盒模型需要耗费大量时间，并需要广泛的专业知识。相反，主要依赖建筑能耗数据的黑盒预测模型缺乏基本的洞察力，并且阻碍了再利用。在这项研究中，我们提出了一种架构，以在集成实时物联网数据与建筑物的3D表示的同时促进建筑热动力学的灰盒建模。

    Buildings energy efficiency is a widely researched topic, which is rapidly gaining popularity due to rising environmental concerns and the need for energy independence. In Northern Europe heating energy alone accounts for up to 70 percent of the total building energy consumption. Industry 4.0 technologies such as IoT, big data, cloud computing and machine learning, along with the creation of predictive and proactive digital twins, can help to reduce this number. However, buildings thermal dynamics is a very complex process that depends on many variables. As a result, commonly used physics-based white box models are time-consuming and require vast expertise. On the contrary, black box forecasting models, which rely primarily on building energy consumption data, lack fundamental insights and hinder re-use. In this study we propose an architecture to facilitate grey box modelling of building thermal dynamics while integrating real time IoT data with 3D representation of buildings. The arc
    
[^143]: ViewFusion: 学习可组合的扩散模型用于新视角合成

    ViewFusion: Learning Composable Diffusion Models for Novel View Synthesis

    [https://arxiv.org/abs/2402.02906](https://arxiv.org/abs/2402.02906)

    ViewFusion 是一种用于新视角合成的最新端到端生成方法，具有无与伦比的灵活性，通过同时应用扩散去噪和像素加权掩模的方法解决了先前方法的局限性。

    

    深度学习为新视角合成这个老问题提供了丰富的新方法，从基于神经辐射场（NeRF）的方法到端到端的风格架构。每种方法都具有特定的优势，但也具有特定的适用性限制。这项工作引入了ViewFusion，这是一种具有无与伦比的灵活性的最新端到端生成方法，用于新视角合成。ViewFusion同时对场景的任意数量的输入视角应用扩散去噪步骤，然后将每个视角得到的噪声梯度与（推断得到的）像素加权掩模相结合，确保对于目标场景的每个区域，只考虑最具信息量的输入视角。我们的方法通过以下方式解决了先前方法的几个局限性：（1）可训练且能够泛化到多个场景和物体类别，（2）在训练和测试时自适应地采用可变数量的无姿态视图，（3）生成高质量的合成图像。

    Deep learning is providing a wealth of new approaches to the old problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures. Each approach offers specific strengths but also comes with specific limitations in their applicability. This work introduces ViewFusion, a state-of-the-art end-to-end generative approach to novel view synthesis with unparalleled flexibility. ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target scene only the most informative input views are taken into account. Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) gene
    
[^144]: 在基于强化学习控制的人肘数字孪生中复制阻抗识别实验

    Replication of Impedance Identification Experiments on a Reinforcement-Learning-Controlled Digital Twin of Human Elbows

    [https://arxiv.org/abs/2402.02904](https://arxiv.org/abs/2402.02904)

    本研究利用强化学习控制的数字孪生模型成功复制了人体肘部阻抗识别实验，并发现强化学习智能体在稳定肘关节运动方面表现出比人体更高的阻抗，为虚拟环境模拟在神经机械研究中的潜力提供了初步证据。

    

    本研究通过使用一种经过强化学习增强的全新人体运动仿真平台MyoSuite，在虚拟环境中复制了人体神经机械实验的先驱性工作。通过在一个肌肉骨骼模型上复制多种类型的人体肘部阻抗识别实验，将由强化学习智能体控制的肘关节运动与实际人体肘关节在扭矩干扰实验中识别到的阻抗进行比较。研究发现，强化学习智能体在稳定目标肘关节运动方面表现出比人体更高的肘关节阻抗，可能是由于其更短的反应时间和更好的感知能力。本研究首次探索了虚拟环境模拟在神经机械研究中的潜力，为传统实验方法提供了一种初步但有前景的替代方案。

    This study presents a pioneering effort to replicate human neuromechanical experiments within a virtual environment utilising a digital human model. By employing MyoSuite, a state-of-the-art human motion simulation platform enhanced by Reinforcement Learning (RL), multiple types of impedance identification experiments of human elbow were replicated on a musculoskeletal model. We compared the elbow movement controlled by an RL agent with the motion of an actual human elbow in terms of the impedance identified in torque-perturbation experiments. The findings reveal that the RL agent exhibits higher elbow impedance to stabilise the target elbow motion under perturbation than a human does, likely due to its shorter reaction time and superior sensory capabilities. This study serves as a preliminary exploration into the potential of virtual environment simulations for neuromechanical research, offering an initial yet promising alternative to conventional experimental approaches. An RL-contro
    
[^145]: 黑盒逼近与分层Tucker分解的优化

    Black-Box Approximation and Optimization with Hierarchical Tucker Decomposition

    [https://arxiv.org/abs/2402.02890](https://arxiv.org/abs/2402.02890)

    该论文介绍了一种基于分层Tucker分解的黑盒逼近和优化方法，通过低秩分解和指数选择提高了准确性，并且在高维度问题上表现出鲁棒性。

    

    我们开发了一种基于低秩分层Tucker分解和MaxVol指数选择方法的新方法HTBB，用于多维黑盒逼近和无梯度优化。14个复杂模型问题的数值实验表明，该方法在维度高达1000时具有稳健性，而且比传统的无梯度优化方法和基于流行的张量网络的逼近和优化方法结果更准确。

    We develop a new method HTBB for the multidimensional black-box approximation and gradient-free optimization, which is based on the low-rank hierarchical Tucker decomposition with the use of the MaxVol indices selection procedure. Numerical experiments for 14 complex model problems demonstrate the robustness of the proposed method for dimensions up to 1000, while it shows significantly more accurate results than classical gradient-free optimization methods, as well as approximation and optimization methods based on the popular tensor train decomposition, which represents a simpler case of a tensor network.
    
[^146]: 探索用于通用音频理解的联邦自监督学习

    Exploring Federated Self-Supervised Learning for General Purpose Audio Understanding

    [https://arxiv.org/abs/2402.02889](https://arxiv.org/abs/2402.02889)

    本研究探索了联邦自监督学习在通用音频理解中的应用，提出了一种新的Federated SSL框架FASSL，并发现在音频检索任务中，音频F-SSL方法与集中式音频-SSL方法的性能不相上下。

    

    联邦学习（FL）和自监督学习（SSL）的整合为利用音频数据进行通用音频理解提供了独特而协同的组合，而不会损害用户数据隐私。然而，目前很少有研究探讨在大规模异构音频源生成的非独立同分布（non-iid）数据情况下，SSL模型在FL模式下用于通用音频理解的性能。本文在仿真非独立同分布数据的大规模FL设置中评估了特征匹配和预测音频-SSL技术的性能。我们提出了一种新颖的Federated SSL（F-SSL）框架，名为FASSL，可以从大规模分散的异构客户端学习中间特征表示，存储未标记的音频数据。我们的研究发现，在音频检索任务中，音频F-SSL方法与集中式音频-SSL方法的性能不相上下。

    The integration of Federated Learning (FL) and Self-supervised Learning (SSL) offers a unique and synergetic combination to exploit the audio data for general-purpose audio understanding, without compromising user data privacy. However, rare efforts have been made to investigate the SSL models in the FL regime for general-purpose audio understanding, especially when the training data is generated by large-scale heterogeneous audio sources. In this paper, we evaluate the performance of feature-matching and predictive audio-SSL techniques when integrated into large-scale FL settings simulated with non-independently identically distributed (non-iid) data. We propose a novel Federated SSL (F-SSL) framework, dubbed FASSL, that enables learning intermediate feature representations from large-scale decentralized heterogeneous clients, holding unlabelled audio data. Our study has found that audio F-SSL approaches perform on par with the centralized audio-SSL approaches on the audio-retrieval t
    
[^147]: 时间、内存和参数高效的视觉适应

    Time-, Memory- and Parameter-Efficient Visual Adaptation

    [https://arxiv.org/abs/2402.02887](https://arxiv.org/abs/2402.02887)

    这项研究提出了一种时间、内存和参数高效的视觉适应方法，通过设计一个轻量级的并行网络，在不反向传播梯度的情况下对预训练主干网络的特征进行操作，实现了在训练时间和内存使用上的高效。该方法在VTAB基准测试中取得了最先进的准确性和参数权衡，并超越了先前的工作在训练时间和内存使用方面的优势。

    

    随着基础模型越来越受欢迎，迫切需要高效地对其进行下游任务的微调。虽然已经提出了许多适应方法，但它们仅在训练的参数数量上高效。然而，它们通常仍然需要在整个模型中反向传播梯度，这意味着它们的训练时间和内存成本并没有明显降低。我们提出了一种不通过主干网络反向传播梯度的适应方法。我们通过设计一个轻量级的并行网络，对从冻结的预训练主干网络中提取的特征进行操作来实现这一点。因此，我们的方法不仅在参数方面高效，而且在训练时间和内存使用方面也是高效的。我们的方法在常用的VTAB基准测试上取得了最先进的准确性和参数权衡，并进一步展示了我们在训练时间和内存使用方面超越了先前的工作。

    As foundation models become more popular, there is a growing need to efficiently finetune them for downstream tasks. Although numerous adaptation methods have been proposed, they are designed to be efficient only in terms of how many parameters are trained. They, however, typically still require backpropagating gradients throughout the model, meaning that their training-time and -memory cost does not reduce as significantly.   We propose an adaptation method which does not backpropagate gradients through the backbone. We achieve this by designing a lightweight network in parallel that operates on features from the frozen, pretrained backbone. As a result, our method is efficient not only in terms of parameters, but also in training-time and memory usage. Our approach achieves state-of-the-art accuracy-parameter trade-offs on the popular VTAB benchmark, and we further show how we outperform prior works with respect to training-time and -memory usage too. We further demonstrate the train
    
[^148]: 翻译过的论文标题: 适用于现有孪生变压器的近似归因方法

    Approximate Attributions for Off-the-Shelf Siamese Transformers

    [https://arxiv.org/abs/2402.02883](https://arxiv.org/abs/2402.02883)

    中文总结出的一句话要点: 本文介绍了一种适用于现有孪生变压器的近似归因方法，该方法在保留原模型性能的同时实现了准确归因能力。我们通过比较近似和准确归因，分析了模型对不同语言方面的关注，并发现孪生变压器主要忽略否定，同时深入研究了它们对句法角色的关注程度，以及如何判断语义上的差异。

    

    翻译过的论文摘要: 孪生编码器如句子变换器是目前最不理解的深度模型之一。现有的归因方法无法处理这种模型类别，因为它们比较两个输入而不是处理单个输入。为了弥补这一空白，我们最近提出了一种专门针对孪生编码器的归因方法(Moller等，2023)。然而，它需要对模型进行调整和微调，因此无法直接应用于现有模型。在这项工作中，我们重新评估了这些限制，并提出了(i)一种具有准确归因能力且保留原模型预测性能的模型，以及(ii)一种计算现有模型近似归因的方法。我们广泛比较了近似和准确归因，并使用它们来分析模型对不同语言方面的关注。我们深入了解了孪生变压器对句法角色的关注程度，确认它们主要忽略否定，并探索了它们如何判断语义上的差异。

    Siamese encoders such as sentence transformers are among the least understood deep models. Established attribution methods cannot tackle this model class since it compares two inputs rather than processing a single one. To address this gap, we have recently proposed an attribution method specifically for Siamese encoders (M\"oller et al., 2023). However, it requires models to be adjusted and fine-tuned and therefore cannot be directly applied to off-the-shelf models. In this work, we reassess these restrictions and propose (i) a model with exact attribution ability that retains the original model's predictive performance and (ii) a way to compute approximate attributions for off-the-shelf models. We extensively compare approximate and exact attributions and use them to analyze the models' attendance to different linguistic aspects. We gain insights into which syntactic roles Siamese transformers attend to, confirm that they mostly ignore negation, explore how they judge semantically op
    
[^149]: 释放脉冲量子神经网络的表达能力

    Unleashing the Expressive Power of Pulse-Based Quantum Neural Networks

    [https://arxiv.org/abs/2402.02880](https://arxiv.org/abs/2402.02880)

    本文研究了基于脉冲的量子神经网络的表达能力，证明了在特定条件下，这种模型可以逼近任意非线性函数，并可能在复杂学习任务上具有更强大的表达能力。

    

    基于噪声中等规模量子（NISQ）设备的量子机器学习（QML）需要最佳利用有限的量子资源。常用的基于门的 QML 模型对软件工程师很方便，但其表达能力受限于有限的相干时间内允许的电路深度。相比之下，基于脉冲的模型可以在相同的相干时间内构建“无限”深度的量子神经网络，可能为复杂学习任务释放更强大的表达能力。本文从量子控制理论的视角研究了这个潜力。我们首先指出，脉冲模型的非线性来自编码过程，可以看作是基于门模型中数据重新上传的连续极限。随后，我们证明了在基础物理系统具有集合可控性的条件下，脉冲模型可以逼近任意非线性函数。在这种条件下，数值模拟表明脉冲模型可以超过基于门的模型在特定学习任务上的性能。

    Quantum machine learning (QML) based on Noisy Intermediate-Scale Quantum (NISQ) devices requires the optimal utilization of limited quantum resources. The commonly used gate-based QML models are convenient for software engineers, but their expressivity is restricted by the permissible circuit depth within a finite coherence time. In contrast, pulse-based models enable the construction of "infinitely" deep quantum neural networks within the same coherence time, which may unleash greater expressive power for complex learning tasks. In this paper, we investigate this potential from the perspective of quantum control theory. We first indicate that the nonlinearity of pulse-based models comes from the encoding process that can be viewed as the continuous limit of data-reuploading in gate-based models. Subsequently, we prove that the pulse-based model can approximate arbitrary nonlinear functions when the underlying physical system is ensemble controllable. Under this condition, numerical si
    
[^150]: 大型语言模型如何进行上下文学习？查询和键矩阵是上下文头部进行度量学习的两个关键组成部分

    How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning

    [https://arxiv.org/abs/2402.02872](https://arxiv.org/abs/2402.02872)

    本论文探索了大型语言模型如何进行上下文学习的机制，提出了一个使用定位和投影方法的假设。通过查询和键矩阵来计算输入文本与每个演示之间的注意力权重，以学习它们之间的相似度度量。实验证明了我们的分析。

    

    我们探索了上下文学习的机制，并提出了使用定位和投影方法的假设。在浅层中，演示的特征被合并到相应的标签中，输入文本的特征被聚合到最后一个标记中。在深层中，上下文头部发挥了重要作用。在每个上下文头部中，值-输出矩阵提取了标签的特征。查询和键矩阵计算了输入文本与每个演示之间的注意力权重。注意力权重越大，越多的标签信息被传输到最后一个标记中，用于预测下一个单词。查询和键矩阵可以被视为学习输入文本与每个演示之间相似度度量的两个关键组成部分。基于这个假设，我们解释了为什么不平衡的标签和演示顺序会影响预测。我们在GPT2大型、Llama 7B、13B和30B上进行了实验。结果支持我们的分析。总体而言，我们的研究提供了一个关于大型语言模型如何进行上下文学习的理论解释和验证。

    We explore the mechanism of in-context learning and propose a hypothesis using locate-and-project method. In shallow layers, the features of demonstrations are merged into their corresponding labels, and the features of the input text are aggregated into the last token. In deep layers, in-context heads make great contributions. In each in-context head, the value-output matrix extracts the labels' features. Query and key matrices compute the attention weights between the input text and each demonstration. The larger the attention weight is, the more label information is transferred into the last token for predicting the next word. Query and key matrices can be regarded as two towers for learning the similarity metric between the input text and each demonstration. Based on this hypothesis, we explain why imbalanced labels and demonstration order affect predictions. We conduct experiments on GPT2 large, Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study provid
    
[^151]: 没有解释的统计学：对可解释机器学习的冷静观察

    Statistics without Interpretation: A Sober Look at Explainable Machine Learning

    [https://arxiv.org/abs/2402.02870](https://arxiv.org/abs/2402.02870)

    解释算法往往数学上复杂且难以解释，这导致解释错误。为了向前推进，解释算法需要明确其输出的解释方式，并澄清可以和不能回答的问题。这一论点基于统计学和解释之间的区别，以及可解释机器学习和应用统计学之间的相似性。

    

    在关于解释算法的快速发展的文献中，这些算法往往不清楚所用于何处及其使用方式。我们认为这是因为解释算法往往在数学上复杂且难以解释。然而，没有清晰解释的复杂统计方法很可能导致解释的错误，这一事实在文献中越来越明显。为了向前推进，关于解释算法的论文应明确解释算法的输出如何解释。他们还应澄清在给出解释的情况下可以回答哪些关于函数的问题，以及哪些问题无法回答。我们的论点基于统计学和它们的解释之间的区别。它还依赖于可解释机器学习和应用统计学之间的相似之处。

    In the rapidly growing literature on explanation algorithms, it often remains unclear what precisely these algorithms are for and how they should be used. We argue that this is because explanation algorithms are often mathematically complex but don't admit a clear interpretation. Unfortunately, complex statistical methods that don't have a clear interpretation are bound to lead to errors in interpretation, a fact that has become increasingly apparent in the literature. In order to move forward, papers on explanation algorithms should make clear how precisely the output of the algorithms should be interpreted. They should also clarify what questions about the function can and cannot be answered given the explanations. Our argument is based on the distinction between statistics and their interpretation. It also relies on parallels between explainable machine learning and applied statistics.
    
[^152]: 细调强化学习模型暗地里是一种遗忘缓解问题

    Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem

    [https://arxiv.org/abs/2402.02868](https://arxiv.org/abs/2402.02868)

    细调强化学习模型中的遗忘问题会导致转移效果差，研究发现常见且具有灾难性后果。通过使用标准的知识保留技术可以缓解这个问题并最大程度地利用细调的优势。

    

    细调是一种广泛应用的技术，允许从预训练模型中转移能力，最近基础模型的成功应用就证明了这一点。然而，细调强化学习（RL）模型仍然是一个挑战。本研究从动作和观察之间的相互作用的角度，将细调阶段未访问到的下游任务状态子空间中的预训练能力遗忘问题作为导致转移效果差的一个具体原因进行了概念化。模型在这个未访问到的状态子空间中的表现良好，但由于预训练使其失去了期望的转移优势。我们确定了该问题发生的条件，表明它是普遍存在的，并且在许多情况下是灾难性的。通过对具有挑战性的NetHack和Montezuma's Revenge环境进行详细的经验分析，我们展示了标准的知识保留技术如何缓解这个问题，从而使我们能充分利用细调的优势。

    Fine-tuning is a widespread technique that allows practitioners to transfer pre-trained capabilities, as recently showcased by the successful applications of foundation models. However, fine-tuning reinforcement learning (RL) models remains a challenge. This work conceptualizes one specific cause of poor transfer, accentuated in the RL setting by the interplay between actions and observations: forgetting of pre-trained capabilities. Namely, a model deteriorates on the state subspace of the downstream task not visited in the initial phase of fine-tuning, on which the model behaved well due to pre-training. This way, we lose the anticipated transfer benefits. We identify conditions when this problem occurs, showing that it is common and, in many cases, catastrophic. Through a detailed empirical analysis of the challenging NetHack and Montezuma's Revenge environments, we show that standard knowledge retention techniques mitigate the problem and thus allow us to take full advantage of the 
    
[^153]: 量子标准化流在异常检测中的应用

    Quantum Normalizing Flows for Anomaly Detection

    [https://arxiv.org/abs/2402.02866](https://arxiv.org/abs/2402.02866)

    本文介绍了一种将任意分布计算为预定义分布的量子标准化流方法，该方法在异常检测方面具有与传统方法相竞争的性能，并且可以在量子计算机上执行。

    

    标准化流将任意分布计算为预定义（例如正态）分布的双射映射。一旦学习到这样的映射，它可以用于解决不同的任务，例如异常检测。在本研究中，我们引入了用于量子架构的标准化流，描述了如何建模和优化这样的流，并在示例数据集上评估了我们的方法。我们提出的模型在异常检测方面表现出与传统方法（如孤立森林、局部离群因子（LOF）或单类支持向量机）相竞争的性能，同时可以在量子计算机上完全执行。

    A Normalizing Flow computes a bijective mapping from an arbitrary distribution to a predefined (e.g. normal) distribution. Such a flow can be used to address different tasks, e.g. anomaly detection, once such a mapping has been learned. In this work we introduce Normalizing Flows for Quantum architectures, describe how to model and optimize such a flow and evaluate our method on example datasets. Our proposed models show competitive performance for anomaly detection compared to classical methods, e.g. based on isolation forests, the local outlier factor (LOF) or single-class SVMs, while being fully executable on a quantum computer.
    
[^154]: 使用声学和调制谱图结合的注意力LSTM系统对语音可懂性级别进行分类的研究

    On combining acoustic and modulation spectrograms in an attention LSTM-based system for speech intelligibility level classification

    [https://arxiv.org/abs/2402.02865](https://arxiv.org/abs/2402.02865)

    本研究探讨了使用声学和调制谱图相结合的注意力LSTM系统对语音可懂性级别进行分类的方法，并提出了使用逐帧调制谱图作为输入特征以及两种不同的融合策略。模型在包含不同严重程度的口吃语音的UA-Speech数据库上进行了评估。

    

    语音可懂性受到多种因素的影响，如嘈杂环境、信道失真或生理问题。本研究针对后一种情况，解决了自动预测语音可懂性级别的问题。在我们以前的工作的基础上，提出了两个主要贡献。第一，提出了使用逐帧调制谱图作为输入特征，而不是从中获取丢弃重要时序信息的紧凑表示。第二，探讨了将逐帧声学对数梅尔和调制谱图在LSTM框架中进行决策级融合或后期融合以及话语级加权池化（WP）融合的两种不同策略。提出的模型在包含不同严重程度的口吃语音的UA-Speech数据库上进行评估。

    Speech intelligibility can be affected by multiple factors, such as noisy environments, channel distortions or physiological issues. In this work, we deal with the problem of automatic prediction of the speech intelligibility level in this latter case. Starting from our previous work, a non-intrusive system based on LSTM networks with attention mechanism designed for this task, we present two main contributions. In the first one, it is proposed the use of per-frame modulation spectrograms as input features, instead of compact representations derived from them that discard important temporal information. In the second one, two different strategies for the combination of per-frame acoustic log-mel and modulation spectrograms into the LSTM framework are explored: at decision level or late fusion and at utterance level or Weighted-Pooling (WP) fusion. The proposed models are evaluated with the UA-Speech database that contains dysarthric speech with different degrees of severity. On the one
    
[^155]: 图神经机器：一种处理表格数据的新模型

    Graph Neural Machine: A New Model for Learning with Tabular Data

    [https://arxiv.org/abs/2402.02862](https://arxiv.org/abs/2402.02862)

    本论文提出了一种新的机器学习模型，图神经机器（GNM），用于处理表格数据。GNM使用同步消息传递方案，并用几乎完全图代替了多层感知机（MLP）的有向无环图。实验结果表明，在多个数据集上，GNM模型的性能优于MLP架构。

    

    近年来，人们对将不同领域的数据映射到图结构的方法越来越感兴趣。神经网络模型如多层感知机（MLP）可以被建模为图。事实上，MLP可以表示为有向无环图。图神经网络（GNN）最近已成为在图上执行机器学习任务的标准工具。在这项工作中，我们展示了MLP等价于一个基于异步消息传递的GNN模型，该模型在MLP的图表示上操作。然后，我们提出了一种新的处理表格数据的机器学习模型，称为图神经机器（GNM），它用一个几乎完全图取代了MLP的有向无环图，并采用同步消息传递方案。我们表明单个GNM模型可以模拟多个MLP模型。我们在多个分类和回归数据集上评估了所提出的模型。在大多数情况下，GNM模型优于MLP架构。

    In recent years, there has been a growing interest in mapping data from different domains to graph structures. Among others, neural network models such as the multi-layer perceptron (MLP) can be modeled as graphs. In fact, MLPs can be represented as directed acyclic graphs. Graph neural networks (GNNs) have recently become the standard tool for performing machine learning tasks on graphs. In this work, we show that an MLP is equivalent to an asynchronous message passing GNN model which operates on the MLP's graph representation. We then propose a new machine learning model for tabular data, the so-called Graph Neural Machine (GNM), which replaces the MLP's directed acyclic graph with a nearly complete graph and which employs a synchronous message passing scheme. We show that a single GNM model can simulate multiple MLP models. We evaluate the proposed model in several classification and regression datasets. In most cases, the GNM model outperforms the MLP architecture.
    
[^156]: 深度自回归密度网络与神经集合在基于模型的离线强化学习中的对比

    Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning

    [https://arxiv.org/abs/2402.02858](https://arxiv.org/abs/2402.02858)

    本文对比了在基于模型的离线强化学习中，使用深度自回归密度网络和神经集合的方法。通过在D4RL基准测试上展示，我们质疑了使用神经集合的普遍观点，并发现单个良好校准的自回归模型可以获得更好的性能。同时，我们还分析了模型学习的静态指标，并得出了关于代理最终性能的重要模型特性。

    

    我们考虑仅有系统转换集合可用于策略优化的离线强化学习问题。在最近的研究进展中，我们考虑了一种基于模型的强化学习算法，该算法从可用数据中推断系统动态，并在模型推演上进行策略优化。这种方法容易受到模型误差的影响，可能会导致在真实系统上的灾难性失败。标准解决方案是依靠集合进行不确定性启发式，并避免在模型不确定性太大时利用模型。通过展示在D4RL基准测试上使用单个良好校准的自回归模型可以获得更好的性能，我们质疑了必须使用集合的普遍观点。我们还分析了与模型学习有关的静态指标，并得出了关于代理的最终性能的重要模型特性的结论。

    We consider the problem of offline reinforcement learning where only a set of system transitions is made available for policy optimization. Following recent advances in the field, we consider a model-based reinforcement learning algorithm that infers the system dynamics from the available data and performs policy optimization on imaginary model rollouts. This approach is vulnerable to exploiting model errors which can lead to catastrophic failures on the real system. The standard solution is to rely on ensembles for uncertainty heuristics and to avoid exploiting the model where it is too uncertain. We challenge the popular belief that we must resort to ensembles by showing that better performance can be obtained with a single well-calibrated autoregressive model on the D4RL benchmark. We also analyze static metrics of model-learning and conclude on the important model properties for the final performance of the agent.
    
[^157]: 偏态自适应随机逼近的非渐进分析

    Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation

    [https://arxiv.org/abs/2402.02857](https://arxiv.org/abs/2402.02857)

    本文对于具有偏态梯度和自适应步长的SGD进行了全面的非渐进分析，证明了Adagrad和RMSProp算法在收敛速度上与无偏情况相似，并通过实验结果验证了收敛结果，展示了如何降低偏差的影响。

    

    自适应步长随机梯度下降（SGD）现在广泛用于训练深度神经网络。大多数理论结果假设可以获得无偏的梯度估计器，然而在一些最近的深度学习和强化学习应用中，使用了蒙特卡洛方法，却无法满足这一假设。本文对具有偏态梯度和自适应步长的SGD进行了全面的非渐进性分析，针对凸和非凸平滑函数。我们的研究包括时变偏差，并强调控制偏差和均方误差（MSE）梯度估计的重要性。特别地，我们证明了使用偏态梯度的Adagrad和RMSProp算法对于非凸平滑函数的收敛速度与文献中无偏情况下的结果相似。最后，我们提供了使用变分自动编码器（VAE）的实验结果，证明了我们的收敛结果，并展示了如何通过适当的方法降低偏差的影响。

    Stochastic Gradient Descent (SGD) with adaptive steps is now widely used for training deep neural networks. Most theoretical results assume access to unbiased gradient estimators, which is not the case in several recent deep learning and reinforcement learning applications that use Monte Carlo methods. This paper provides a comprehensive non-asymptotic analysis of SGD with biased gradients and adaptive steps for convex and non-convex smooth functions. Our study incorporates time-dependent bias and emphasizes the importance of controlling the bias and Mean Squared Error (MSE) of the gradient estimator. In particular, we establish that Adagrad and RMSProp with biased gradients converge to critical points for smooth non-convex functions at a rate similar to existing results in the literature for the unbiased case. Finally, we provide experimental results using Variational Autoenconders (VAE) that illustrate our convergence results and show how the effect of bias can be reduced by appropri
    
[^158]: 动态稀疏学习：高效推荐的一种新范式

    Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation

    [https://arxiv.org/abs/2402.02855](https://arxiv.org/abs/2402.02855)

    本文提出了一种针对推荐模型的新型学习范式，称为动态稀疏学习（DSL），通过从头训练一个轻量级稀疏模型，解决了模型大小和学习效率的问题。

    

    在基于深度学习的推荐系统领域中，日益增长的用户和物品数量所带来的计算需求增加，给实际部署带来了显著挑战。这个挑战主要有两个方面：在降低模型大小的同时，有效学习用户和物品表示以实现高效推荐。尽管模型压缩和架构搜索方面有了相当大的进展，但现有方法面临着明显的限制。其中包括模型压缩中预训练/重新训练的额外计算开销以及架构设计中广泛的搜索空间。此外，在具有严格时间或空间限制的情况下，管理复杂性和遵守内存限制是有问题的。为了解决这些问题，本文引入了一种新的学习范式，称为动态稀疏学习（DSL），专门用于推荐模型。DSL创新性地从头开始训练一个轻量级稀疏模型，周期的重构。

    In the realm of deep learning-based recommendation systems, the increasing computational demands, driven by the growing number of users and items, pose a significant challenge to practical deployment. This challenge is primarily twofold: reducing the model size while effectively learning user and item representations for efficient recommendations. Despite considerable advancements in model compression and architecture search, prevalent approaches face notable constraints. These include substantial additional computational costs from pre-training/re-training in model compression and an extensive search space in architecture design. Additionally, managing complexity and adhering to memory constraints is problematic, especially in scenarios with strict time or space limitations. Addressing these issues, this paper introduces a novel learning paradigm, Dynamic Sparse Learning (DSL), tailored for recommendation models. DSL innovatively trains a lightweight sparse model from scratch, periodi
    
[^159]: 通过组合特征对齐增强组合通用性

    Enhancing Compositional Generalization via Compositional Feature Alignment

    [https://arxiv.org/abs/2402.02851](https://arxiv.org/abs/2402.02851)

    通过组合特征对齐，增强了模型的组合通用性，使其能够推广到未见过的领域-类别组合。

    

    机器学习模型在现实世界的应用中经常面临数据分布偏移的问题，即训练数据和测试数据分布之间存在差异。在常见的多领域多类别设置中，随着类别和领域数量的增加，很难为每个领域-类别组合收集训练数据。这个挑战自然地引发了对具备组合通用性（CG）能力的模型的探索，即模型可以推广到未见过的领域-类别组合。为了深入研究CG挑战，我们开发了CG-Bench，这是一套从现有实际图像数据集派生的CG基准测试，并观察到目前在基础模型（如CLIP和DINOv2）上流行的预训练-微调范式在这个挑战中存在困难。为了解决这个挑战，我们提出了组合特征对齐（CFA），这是一种简单的两阶段微调技术，它通过在预训练的编码器上学习两个正交线性头部来对齐类别和领域的标签。

    Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain lab
    
[^160]: 基于注意力的长短期记忆系统用于自动分类语音可懂性

    An Attention Long Short-Term Memory based system for automatic classification of speech intelligibility

    [https://arxiv.org/abs/2402.02850](https://arxiv.org/abs/2402.02850)

    本研究的主要贡献是使用长短期记忆网络和对数梅尔频谱图来预测语音可懂性水平，同时引入了注意力机制以提高性能。

    

    语音可懂性可能因多种因素而降低，如嘈杂的环境、技术困难或生物条件。本研究侧重于开发一种自动无侵入性系统，用于预测后一种情况下的语音可懂性水平。我们在这个问题上的主要贡献是使用长短期记忆（LSTM）网络和对数梅尔频谱图作为输入特征来实现这个目的。此外，这个基于LSTM的系统通过引入简单的注意力机制来进一步增强，以确定更相关的帧用于这个任务。所提出的模型在UA-Speech数据库上进行评估，该数据库包含不同严重程度的口吃语音。结果显示，注意力LSTM架构优于基于手工特征的参考支持向量机（SVM）和基于平均池化的LSTM系统。

    Speech intelligibility can be degraded due to multiple factors, such as noisy environments, technical difficulties or biological conditions. This work is focused on the development of an automatic non-intrusive system for predicting the speech intelligibility level in this latter case. The main contribution of our research on this topic is the use of Long Short-Term Memory (LSTM) networks with log-mel spectrograms as input features for this purpose. In addition, this LSTM-based system is further enhanced by the incorporation of a simple attention mechanism that is able to determine the more relevant frames to this task. The proposed models are evaluated with the UA-Speech database that contains dysarthric speech with different degrees of severity. Results show that the attention LSTM architecture outperforms both, a reference Support Vector Machine (SVM)-based system with hand-crafted features and a LSTM-based system with Mean-Pooling.
    
[^161]: 机器学习抗性非晶硅物理不可克隆函数 (PUFs) 的研究

    Machine Learning Resistant Amorphous Silicon Physically Unclonable Functions (PUFs)

    [https://arxiv.org/abs/2402.02846](https://arxiv.org/abs/2402.02846)

    这项研究调查了利用非晶硅材料制作物理不可克隆函数 (PUF) 的应用，通过使用机器学习算法分析攻击这些集成电路PUF的效果，发现深度神经网络 (DNNs) 是最有效的算法，但仍无法完全破解a-Si PUF的安全性，此外，研究发现非晶硅PUFs的机器学习抗性与其非线性响应的强度相关。

    

    我们研究了非线性波动混沌非晶硅 (a-Si) 腔体作为物理不可克隆函数 (PUFs) 的应用。已经证明，对集成电子PUFs的机器学习攻击可以非常有效地对PUF行为进行建模。通过应用包括线性回归、k-最近邻、决策树集成 (随机森林和梯度提升树) 和深度神经网络 (DNNs) 在内的算法，我们对集成的非晶硅光学PUFs进行了攻击研究。我们发现，在所有研究的算法中，DNNs表现最好，但仍无法完全破解a-Si PUF的安全性，我们通过一个私密信息度量将其量化。此外，我们发现非晶硅PUFs的机器学习抗性与其非线性响应的强度直接相关。

    We investigate usage of nonlinear wave chaotic amorphous silicon (a-Si) cavities as physically unclonable functions (PUF). Machine learning attacks on integrated electronic PUFs have been demonstrated to be very effective at modeling PUF behavior. Such attacks on integrated a-Si photonic PUFs are investigated through application of algorithms including linear regression, k-nearest neighbor, decision tree ensembles (random forests and gradient boosted trees), and deep neural networks (DNNs). We found that DNNs performed the best among all the algorithms studied but still failed to completely break the a-Si PUF security which we quantify through a private information metric. Furthermore, machine learning resistance of a-Si PUFs were found to be directly related to the strength of their nonlinear response.
    
[^162]: Trinity：将多/小众/长期兴趣整合为一个

    Trinity: Syncretizing Multi-/Long-tail/Long-term Interests All in One

    [https://arxiv.org/abs/2402.02842](https://arxiv.org/abs/2402.02842)

    本文提出了一种统一的兴趣建模框架“Trinity”，通过利用长期线索来解决兴趣遗忘问题，并改善多兴趣建模任务。通过构建实时聚类系统和计算统计兴趣直方图，Trinity能够识别用户的兴趣并进行个性化推荐。

    

    在推荐系统中的兴趣建模一直是改善用户体验的一个重要领域，许多现有工作已经研究了典型的兴趣建模任务（例如多兴趣、小众兴趣和长期兴趣）。然而，大多数研究只考虑了其中一个兴趣，并忽略了它们之间的相互关系。在本文中，我们认为这些任务面临共同的“兴趣遗忘”问题，而且存在一种解决方法可以同时减轻这个问题。我们认为长期线索可以成为基石，因为它们揭示了多种兴趣并澄清了小众兴趣。受到这个观察的启发，我们在检索阶段提出了一种新颖的统一框架“Trinity”，来解决兴趣遗忘问题并改善多兴趣建模任务。我们搭建了一个实时聚类系统，可以将物品投影到可枚举的簇中，并在这些簇上计算统计兴趣直方图。基于这些直方图，Trinity可以识别用户的兴趣，并进行个性化的推荐。

    Interest modeling in recommender system has been a constant topic for improving user experience, and typical interest modeling tasks (e.g. multi-interest, long-tail interest and long-term interest) have been investigated in many existing works. However, most of them only consider one interest in isolation, while neglecting their interrelationships. In this paper, we argue that these tasks suffer from a common "interest amnesia" problem, and a solution exists to mitigate it simultaneously. We figure that long-term cues can be the cornerstone since they reveal multi-interest and clarify long-tail interest. Inspired by the observation, we propose a novel and unified framework in the retrieval stage, "Trinity", to solve interest amnesia problem and improve multiple interest modeling tasks. We construct a real-time clustering system that enables us to project items into enumerable clusters, and calculate statistical interest histograms over these clusters. Based on these histograms, Trinity
    
[^163]: 简化的LLaMA: 大规模语言模型的简单深度修剪

    Shortened LLaMA: A Simple Depth Pruning for Large Language Models

    [https://arxiv.org/abs/2402.02834](https://arxiv.org/abs/2402.02834)

    使用简单的深度修剪方法可以提高大规模语言模型的推理速度，在内存受限的条件下表现良好，对部署在本地和边缘设备上的LLMs有帮助。

    

    现代大规模语言模型 (LLMs) 的结构化修剪已成为降低其高计算需求的一种方法。宽度修剪减小投影权重矩阵的大小 (例如通过删除注意力头)，同时保持层数不变。与此相反，深度修剪则删除整个层或块，同时保持剩余权重的大小不变。目前的大多数研究集中在宽度修剪或宽度和深度修剪的混合上，很少对两者 (宽度与深度) 在对LLM推理效率的影响方面进行比较分析。在这项工作中，我们展示了一种简单的深度修剪方法可以与最新的宽度修剪方法在零-shot任务性能方面竞争。我们的修剪方法提高了推理速度，特别是在内存受限的情况下，需要对运行LLMs进行有限批次大小的条件，此时宽度修剪无效。我们希望这项工作能够帮助将LLMs部署在本地和边缘设备上。

    Structured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs. Width pruning reduces the size of projection weight matrices (e.g., by removing attention heads) while maintaining the number of layers. Depth pruning, in contrast, removes entire layers or blocks, while keeping the size of the remaining weights unchanged. Most current research focuses on either width-only or a blend of width and depth pruning, with little comparative analysis between the two units (width vs. depth) concerning their impact on LLM inference efficiency. In this work, we show that a simple depth pruning approach can compete with recent width pruning methods in terms of zero-shot task performance. Our pruning method boosts inference speeds, especially under memory-constrained conditions that require limited batch sizes for running LLMs, where width pruning is ineffective. We hope this work can help deploy LLMs on local and edge devices.
    
[^164]: PowerGraph: 用于图神经网络的电网基准数据集

    PowerGraph: A power grid benchmark dataset for graph neural networks

    [https://arxiv.org/abs/2402.02827](https://arxiv.org/abs/2402.02827)

    PowerGraph是一个用于图神经网络的电网基准数据集，旨在通过机器学习模型实现电力网格断电的在线检测。

    

    公共图神经网络（GNN）基准数据集有助于使用GNN，并增强GNN在各个领域中的适用性。目前，社区中缺乏用于GNN应用的电力网格公共数据集。事实上，与其他机器学习技术相比，GNN可以潜在地捕捉到复杂的电力网格现象。电力网格是复杂的工程网络，天然适合于图表示。因此，GNN有潜力捕捉到电力网格的行为，而不用其他机器学习技术。为了实现这个目标，我们开发了一个用于级联故障事件的图数据集，这是导致电力网格断电的主要原因。历史断电数据集稀缺且不完整。通常通过计算昂贵的离线级联故障模拟来评估脆弱性和识别关键组件。相反，我们建议使用机器学习模型进行在线检测。

    Public Graph Neural Networks (GNN) benchmark datasets facilitate the use of GNN and enhance GNN applicability to diverse disciplines. The community currently lacks public datasets of electrical power grids for GNN applications. Indeed, GNNs can potentially capture complex power grid phenomena over alternative machine learning techniques. Power grids are complex engineered networks that are naturally amenable to graph representations. Therefore, GNN have the potential for capturing the behavior of power grids over alternative machine learning techniques. To this aim, we develop a graph dataset for cascading failure events, which are the major cause of blackouts in electric power grids. Historical blackout datasets are scarce and incomplete. The assessment of vulnerability and the identification of critical components are usually conducted via computationally expensive offline simulations of cascading failures. Instead, we propose using machine learning models for the online detection of
    
[^165]: SynthVision - 使用合成图像数据在计算机视觉模型中最大化产出的最小输入

    SynthVision - Harnessing Minimal Input for Maximal Output in Computer Vision Models using Synthetic Image data

    [https://arxiv.org/abs/2402.02826](https://arxiv.org/abs/2402.02826)

    本研究提出了一种新方法，通过仅使用合成数据来构建一个全面的计算机视觉模型，用于检测人类乳头瘤病毒生殖器疣。使用扩散模型快速生成高质量的训练数据，并评估其对视觉模型的影响。

    

    在应对紧急医疗危机（如流行病或生物恐怖主义事件）时，快速开发疾病检测的计算机视觉模型非常重要。然而，传统的数据收集方法对于这些场景来说太慢了，需要创新方法从很少的数据中快速生成可靠的模型。我们通过仅使用合成数据构建了一个全面的计算机视觉模型来检测人类乳头瘤病毒生殖器疣，展示了我们的新方法。在我们的研究中，我们采用了一个两阶段的实验设计，使用扩散模型。在第一阶段，扩散模型被用于从10张HPV引导图像生成大量多样的合成图像，专注于准确描绘生殖器疣。第二阶段涉及使用该合成数据集对视觉模型进行训练和测试。这种方法旨在评估扩散模型在快速生成高质量训练数据方面的有效性以及对视觉模型的影响。

    Rapid development of disease detection computer vision models is vital in response to urgent medical crises like epidemics or events of bioterrorism. However, traditional data gathering methods are too slow for these scenarios necessitating innovative approaches to generate reliable models quickly from minimal data. We demonstrate our new approach by building a comprehensive computer vision model for detecting Human Papilloma Virus Genital warts using only synthetic data. In our study, we employed a two phase experimental design using diffusion models. In the first phase diffusion models were utilized to generate a large number of diverse synthetic images from 10 HPV guide images explicitly focusing on accurately depicting genital warts. The second phase involved the training and testing vision model using this synthetic dataset. This method aimed to assess the effectiveness of diffusion models in rapidly generating high quality training data and the subsequent impact on the vision mod
    
[^166]: 逃避语言模型数据污染检测（太）容易

    Evading Data Contamination Detection for Language Models is (too) Easy

    [https://arxiv.org/abs/2402.02823](https://arxiv.org/abs/2402.02823)

    本研究指出语言模型数据污染的检测方法在面对恶意模型提供者的有意污染时存在漏洞，并提出了一种简单而有效的污染技术（EAL）来显著提高基准测试性能且逃避当前的检测方法。

    

    大型语言模型广泛使用，它们在基准测试上的性能经常指导用户对一个模型与另一个模型的偏好。然而，这些模型所训练的大量数据可能会意外地与公共基准测试数据发生污染，从而损害性能评估。尽管最近开发了一些污染检测方法来解决这个问题，但它们忽视了恶意模型提供者有意进行污染以避免被检测的可能性。我们认为这种情况非常重要，因为它对公共基准测试的可信度产生了怀疑。为了更严格地研究这个问题，我们提出了模型提供者和污染检测方法的分类，这揭示了现有方法中的漏洞，我们通过使用EAL这种简单而有效的污染技术，明显提高了基准测试的性能，并完全逃避了当前的检测方法。

    Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another. However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements. While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks. To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.
    
[^167]: 重访VAE：一种基于频率视角的无监督时间序列异常检测方法

    Revisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective

    [https://arxiv.org/abs/2402.02820](https://arxiv.org/abs/2402.02820)

    这篇论文提出了一种名为FCVAE的无监督时间序列异常检测方法，通过同时集成全局和局部频率特征，显著提高了对正常数据的重构精度。

    

    时间序列异常检测在Web系统中起着至关重要的作用。各种Web系统依赖时间序列数据来实时监测和识别异常，并启动诊断和修复程序。变分自编码器（VAEs）由于其卓越的降噪能力，在最近几十年中变得越来越受欢迎，这对于异常检测非常有用。然而，我们的研究发现，基于VAE的方法在同时捕捉长周期异质模式和详细短周期趋势方面面临着挑战。为了应对这些挑战，我们提出了一种新颖的单变量时间序列无监督异常检测方法：频率增强的条件变分自编码器（FCVAE）。为了确保准确的异常检测，FCVAE采用了一种创新的方法，将全局和局部频率特征同时集成到条件变分自编码器（CVAE）的条件中，从而显著提高了对正常数据的重构精度。

    Time series Anomaly Detection (AD) plays a crucial role for web systems. Various web systems rely on time series data to monitor and identify anomalies in real time, as well as to initiate diagnosis and remediation procedures. Variational Autoencoders (VAEs) have gained popularity in recent decades due to their superior de-noising capabilities, which are useful for anomaly detection. However, our study reveals that VAE-based methods face challenges in capturing long-periodic heterogeneous patterns and detailed short-periodic trends simultaneously. To address these challenges, we propose Frequency-enhanced Conditional Variational Autoencoder (FCVAE), a novel unsupervised AD method for univariate time series. To ensure an accurate AD, FCVAE exploits an innovative approach to concurrently integrate both the global and local frequency features into the condition of Conditional Variational Autoencoder (CVAE) to significantly increase the accuracy of reconstructing the normal data. Together 
    
[^168]: 基于先处理、中处理和后处理的线性差异约束下的贝叶斯最优公平分类

    Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing

    [https://arxiv.org/abs/2402.02817](https://arxiv.org/abs/2402.02817)

    本文提出了一种基于贝叶斯最优的公平分类方法，通过先处理、中处理和后处理来最小化分类错误，并在给定群体公平性约束的情况下进行优化。该方法引入了线性和双线性差异度量的概念，并找到了贝叶斯最优公平分类器的形式。本方法能够处理多个公平性约束和常见情况。

    

    机器学习算法可能对受保护的群体产生不公平的影响。为解决这个问题，我们开发了基于贝叶斯最优的公平分类方法，旨在在给定群体公平性约束的情况下最小化分类错误。我们引入了线性差异度量的概念，它们是概率分类器的线性函数；以及双线性差异度量，它们在群体回归函数方面也是线性的。我们证明了几种常见的差异度量（如人口平等、机会平等和预测平等）都是双线性的。我们通过揭示与Neyman-Pearson引理的连接，找到了在单一线性差异度量下的贝叶斯最优公平分类器的形式。对于双线性差异度量，贝叶斯最优公平分类器变成了群体阈值规则。我们的方法还可以处理多个公平性约束（如平等的几率）和受保护属性常见的情况。

    Machine learning algorithms may have disparate impacts on protected groups. To address this, we develop methods for Bayes-optimal fair classification, aiming to minimize classification error subject to given group fairness constraints. We introduce the notion of \emph{linear disparity measures}, which are linear functions of a probabilistic classifier; and \emph{bilinear disparity measures}, which are also linear in the group-wise regression functions. We show that several popular disparity measures -- the deviations from demographic parity, equality of opportunity, and predictive equality -- are bilinear.   We find the form of Bayes-optimal fair classifiers under a single linear disparity measure, by uncovering a connection with the Neyman-Pearson lemma. For bilinear disparity measures, Bayes-optimal fair classifiers become group-wise thresholding rules. Our approach can also handle multiple fairness constraints (such as equalized odds), and the common scenario when the protected attr
    
[^169]: 推荐系统中的交叉双边公平性

    Intersectional Two-sided Fairness in Recommendation

    [https://arxiv.org/abs/2402.02816](https://arxiv.org/abs/2402.02816)

    本文针对推荐系统中的交叉双边公平性问题，提出了一种名为交叉双边公平推荐（ITFR）的新方法，通过利用锐度感知损失感知劣势群体，使用协作损失平衡开发不同交叉群体的一致区分能力，并利用预测得分归一化来公平对待不同交叉群体中的正例。实验证明该方法在提高交叉双边公平性方面取得了显著效果。

    

    近年来，推荐系统的公平性引起了越来越多的关注。根据涉及的利益相关者，推荐系统的公平性可分为用户公平性、物品公平性和同时考虑用户和物品公平性的双边公平性。然而，我们认为即使推荐系统是双边公平的，交叉双边不公平仍然可能存在，这在本文使用真实世界数据的实证研究中得到了观察和展示，并且以前尚未得到很好的研究。为了缓解这个问题，我们提出了一种新方法，称为交叉双边公平推荐（ITFR）。我们的方法利用一个锐度感知损失来感知劣势群体，然后使用协作损失平衡来开发不同交叉群体的一致区分能力。此外，我们利用预测得分归一化来调整正面预测得分，以公平地对待不同交叉群体中的正例。广泛的实验结果表明，我们的方法在提高交叉双边公平性方面取得了显著的效果。

    Fairness of recommender systems (RS) has attracted increasing attention recently. Based on the involved stakeholders, the fairness of RS can be divided into user fairness, item fairness, and two-sided fairness which considers both user and item fairness simultaneously. However, we argue that the intersectional two-sided unfairness may still exist even if the RS is two-sided fair, which is observed and shown by empirical studies on real-world data in this paper, and has not been well-studied previously. To mitigate this problem, we propose a novel approach called Intersectional Two-sided Fairness Recommendation (ITFR). Our method utilizes a sharpness-aware loss to perceive disadvantaged groups, and then uses collaborative loss balance to develop consistent distinguishing abilities for different intersectional groups. Additionally, predicted score normalization is leveraged to align positive predicted scores to fairly treat positives in different intersectional groups. Extensive experime
    
[^170]: 基于统计、物理和超级学习图模型的城市空气污染状态估计

    State estimation of urban air pollution with statistical, physical, and super-learning graph models

    [https://arxiv.org/abs/2402.02812](https://arxiv.org/abs/2402.02812)

    本文介绍了基于城市图的不同重建方法，包括完全数据驱动、物理驱动和混合型，以及结合超级学习模型。这些方法在法国巴黎市中心的实时重建城市空气污染地图问题中进行了测试。

    

    本文考虑实时重建城市空气污染地图的问题。由于可用数据的异质性源、直接测量的稀缺性、噪声的存在以及需要考虑的大面积，这个任务是具有挑战性的。在这项工作中，我们引入了基于城市图的不同重建方法。我们的策略可以分为完全数据驱动、物理驱动或混合型，并结合了超级学习模型。这些方法在法国巴黎市中心的情况下进行了测试。

    We consider the problem of real-time reconstruction of urban air pollution maps. The task is challenging due to the heterogeneous sources of available data, the scarcity of direct measurements, the presence of noise, and the large surfaces that need to be considered. In this work, we introduce different reconstruction methods based on posing the problem on city graphs. Our strategies can be classified as fully data-driven, physics-driven, or hybrid, and we combine them with super-learning models. The performance of the methods is tested in the case of the inner city of Paris, France.
    
[^171]: 异步计划推理中的图增强大型语言模型的研究

    Graph-enhanced Large Language Models in Asynchronous Plan Reasoning

    [https://arxiv.org/abs/2402.02805](https://arxiv.org/abs/2402.02805)

    本研究是关于使用大型语言模型(LLMs)进行异步计划推理的首次大规模研究。我们发现在没有提供任务解决过程插图的情况下，现有的LLMs表现不佳。为此，我们提出了一种称为Plan Like a Graph (PLaG)的新技术，通过将图与自然语言提示相结合，实现了最先进的结果。然而，当任务复杂性增加时，LLMs仍然存在严重降级的问题，突显了LLMs在模拟数字设备方面的局限性。这项研究为将LLMs作为高效自主代理迈出了重要一步。

    

    异步计划推理具有挑战性，因为它需要顺序和并行规划以优化时间成本。大型语言模型(LLMs)能在这个任务中成功吗？在这里，我们进行了第一次大规模研究来探讨这个问题。我们发现一组代表性的闭源和开源LLMs，包括GPT-4和LLaMA-2，在我们的基准测试AsyncHow中，在没有提供任务解决过程的插图的情况下表现不佳。我们提出了一种称为Plan Like a Graph (PLaG)的新技术，它将图与自然语言提示相结合，实现了最先进的结果。我们展示了虽然PLaG能提升模型性能，但在任务复杂性增加时，LLMs仍然遭受严重降级，突出了利用LLMs模拟数字设备的局限性。我们将我们的研究视为将LLMs用作高效自主代理的一个令人兴奋的步骤。

    Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents.
    
[^172]: 大型语言模型蒸馏药物推荐模型

    Large Language Model Distilling Medication Recommendation Model

    [https://arxiv.org/abs/2402.02803](https://arxiv.org/abs/2402.02803)

    本研究旨在利用大型语言模型改进药物推荐方法，以解决传统模型在医学数据语义理解和新患者处方推荐方面的挑战。

    

    药物推荐是智能医疗系统中的重要方面，它根据患者特定的健康需求来推荐最合适的药物。然而，目前使用的许多复杂模型往往忽视医学数据的细微语义，而只是过度依赖标识。此外，这些模型在处理首次访问医院的患者的情况时面临重大挑战，因为它们缺乏之前的处方历史可以参考。为了解决这些问题，我们利用大型语言模型（LLMs）强大的语义理解和输入不可知的特性。我们的研究旨在利用LLMs改进现有的药物推荐方法。在本文中，我们介绍了一种新颖的方法，称为大型语言模型蒸馏药物推荐（LEADER）。我们首先创建合适的提示模板，使LLMs能够有效地推荐药物。

    The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs. Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities. Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon. To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs). Our research aims to transform existing medication recommendation methodologies using LLMs. In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER). We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively. However, the
    
[^173]: KS-Lottery: 寻找多语言语言模型中的认证彩票

    KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models

    [https://arxiv.org/abs/2402.02801](https://arxiv.org/abs/2402.02801)

    KS-Lottery是一种寻找多语言语言模型中有效参数的方法，通过使用Kolmogorov-Smirnov检验来分析参数分布偏移，并证明了在嵌入层中可以找到认证的中奖票。这种方法可以在微调中获得与全面微调相当的性能，同时减少了所需的参数数量。

    

    彩票票证假说认为在随机初始化的神经网络中存在“中奖票”。在微调场景中，语言模型中是否存在中奖票？我们如何找到这样的中奖票？在本文中，我们提出了KS-Lottery，一种用于识别在多语言微调中高度有效的LLM参数的方法。我们的核心思想是使用Kolmogorov-Smirnov检验来分析微调前后参数的分布偏移。我们进一步理论证明了KS-Lottery可以在嵌入层中找到认证的中奖票，微调这些参数可以保证与全面微调相同的性能。通过在翻译任务上将KS-Lottery与其他参数高效调优算法进行比较，实验结果表明，KS-Lottery找到了一个更小的参数集来进行微调，同时达到了与全面微调LLM相当的性能。令人惊讶的是，我们发现微调18个标记的嵌入层

    The lottery ticket hypothesis posits the existence of ``winning tickets'' within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning. Comparing KS-Lottery with other parameter-efficient tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for fine-tuning while achieving the comparable performance as full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens' embeddin
    
[^174]: 表面缺陷明显性检测的联合注意引导特征融合网络

    Joint Attention-Guided Feature Fusion Network for Saliency Detection of Surface Defects

    [https://arxiv.org/abs/2402.02797](https://arxiv.org/abs/2402.02797)

    提出了一种联合注意引导特征融合网络（JAFFNet），用于表面缺陷明显性检测。JAFFNet通过融合低层和高层特征，并引入稠密感受野模块，成功解决了缺陷尺度变化、复杂背景、低对比度等挑战。

    

    表面缺陷检测在工业制造和生产过程中起着重要作用。尽管基于卷积神经网络（CNN）的缺陷检测方法已经取得了很大的进步，但仍然面临着许多挑战，如缺陷尺度变化、复杂背景、低对比度等。为了解决这些问题，我们提出了一种基于编码器-解码器网络的联合注意引导特征融合网络（JAFFNet）用于表面缺陷明显性检测。JAFFNet主要在解码阶段融入了联合注意引导特征融合（JAFF）模块，以自适应地融合低层和高层特征。JAFF模块学习强调缺陷特征并抑制背景噪声，在检测低对比度缺陷方面有益。此外，JAFFNet还引入了一个稠密感受野（DRF）模块，在编码器后面捕获具有丰富上下文信息的特征，有助于检测不同尺度的缺陷。

    Surface defect inspection plays an important role in the process of industrial manufacture and production. Though Convolutional Neural Network (CNN) based defect inspection methods have made huge leaps, they still confront a lot of challenges such as defect scale variation, complex background, low contrast, and so on. To address these issues, we propose a joint attention-guided feature fusion network (JAFFNet) for saliency detection of surface defects based on the encoder-decoder network. JAFFNet mainly incorporates a joint attention-guided feature fusion (JAFF) module into decoding stages to adaptively fuse low-level and high-level features. The JAFF module learns to emphasize defect features and suppress background noise during feature fusion, which is beneficial for detecting low-contrast defects. In addition, JAFFNet introduces a dense receptive field (DRF) module following the encoder to capture features with rich context information, which helps detect defects of different scales
    
[^175]: 基于学习的边缘内容传递缓存机制

    A Learning-Based Caching Mechanism for Edge Content Delivery

    [https://arxiv.org/abs/2402.02795](https://arxiv.org/abs/2402.02795)

    基于学习的边缘缓存框架HR-Cache能够优化边缘缓存，提高字节命中率，降低网络负载，并加速内容传递给用户。

    

    随着5G网络的兴起和物联网(IoT)的发展，内容传递网络(CDNs)越来越多地扩展到网络边缘。这种转变带来了独特的挑战，特别是由于边缘的有限缓存存储和多样化的请求模式。边缘环境可以托管具有不同对象大小分布和对象访问模式的流量类别。这种复杂性使得传统的缓存策略很难发挥作用，传统策略通常依赖于请求频率或时间间隔等指标。尽管存在这些复杂性，优化边缘缓存至关重要。在边缘实现更高的字节命中率不仅可以减轻网络骨干的负载，还可以最小化运营成本并加快内容传递给最终用户。在本文中，我们介绍了一种名为HR-Cache的综合学习缓存框架，它基于危险率(Hazard Rate)排序原则，这是一种最初用来计算待命时间的规则。

    With the advent of 5G networks and the rise of the Internet of Things (IoT), Content Delivery Networks (CDNs) are increasingly extending into the network edge. This shift introduces unique challenges, particularly due to the limited cache storage and the diverse request patterns at the edge. These edge environments can host traffic classes characterized by varied object-size distributions and object-access patterns. Such complexity makes it difficult for traditional caching strategies, which often rely on metrics like request frequency or time intervals, to be effective. Despite these complexities, the optimization of edge caching is crucial. Improved byte hit rates at the edge not only alleviate the load on the network backbone but also minimize operational costs and expedite content delivery to end-users.   In this paper, we introduce HR-Cache, a comprehensive learning-based caching framework grounded in the principles of Hazard Rate (HR) ordering, a rule originally formulated to com
    
[^176]: 重新思考微型语言模型的优化和架构

    Rethinking Optimization and Architecture for Tiny Language Models

    [https://arxiv.org/abs/2402.02791](https://arxiv.org/abs/2402.02791)

    本研究重新思考了微型语言模型的优化和架构，通过经验研究发现了在微型语言模型中特别有效的设计公式，并在多语种数据集上训练了高性能的微型语言模型。

    

    大型语言模型（LLMs）的威力通过大量的数据和计算资源得到了证明。然而，在移动设备上应用语言模型面临着计算和内存成本的巨大挑战，迫切需要高性能的微型语言模型。受复杂训练过程的限制，优化语言模型的许多细节很少得到仔细研究。在本研究中，基于一个具有10亿参数的微型语言模型，我们仔细设计了一系列经验研究来分析每个组件的影响。主要讨论了三个方面，即神经架构、参数初始化和优化策略。多个设计公式在微型语言模型中经验性地被证明特别有效，包括分词器压缩、架构调整、参数继承和多轮训练。然后，我们在1.6T多语种数据集上训练了PanGu-$\pi$-1B Pro和PanGu-$\pi$-1.5B Pro。

    The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
    
[^177]: 通过双曲正切指数线性单元（TeLU）实现稳定和稳健的深度学习

    Stable and Robust Deep Learning By Hyperbolic Tangent Exponential Linear Unit (TeLU)

    [https://arxiv.org/abs/2402.02790](https://arxiv.org/abs/2402.02790)

    本文提出了一种新的神经网络激活函数——双曲正切指数线性单元（TeLU），通过解决传统激活函数的局限性，如梯度消失和爆炸问题，实现了稳定和稳健的深度学习。与流行的激活函数相比，TeLU在稳定性和性能上具有更优异的表现，并在大规模测试中验证了其优越性能。

    

    本文引入了一种新颖的神经网络激活函数——双曲正切指数线性单元（TeLU），表示为$f(x) = x{\cdot}tanh(e^x)$。TeLU旨在克服传统激活函数（如ReLU、GELU和Mish）的局限性，解决梯度消失和爆炸问题。我们的理论分析和实证评估表明，TeLU在稳定性和鲁棒性方面优于现有的激活函数，有效地将激活输出的均值调整为零，增强了训练的稳定性和收敛性。对包括Resnet-50在内的先进架构进行了广泛评估，与流行的激活函数（ReLU、GELU、SiLU、Mish、Logish、Smish）进行了对比，结果显示TeLU具有较低的方差和优秀的性能，即使在针对其他函数进行优化的超参数条件下也是如此。在包括CIFAR-10、CIFAR-100和TinyImageNet在内的具有挑战性的大规模测试中，涵盖了860个场景。

    In this paper, we introduce the Hyperbolic Tangent Exponential Linear Unit (TeLU), a novel neural network activation function, represented as $f(x) = x{\cdot}tanh(e^x)$. TeLU is designed to overcome the limitations of conventional activation functions like ReLU, GELU, and Mish by addressing the vanishing and, to an extent, the exploding gradient problems. Our theoretical analysis and empirical assessments reveal that TeLU outperforms existing activation functions in stability and robustness, effectively adjusting activation outputs' mean towards zero for enhanced training stability and convergence. Extensive evaluations against popular activation functions (ReLU, GELU, SiLU, Mish, Logish, Smish) across advanced architectures, including Resnet-50, demonstrate TeLU's lower variance and superior performance, even under hyperparameter conditions optimized for other functions. In large-scale tests with challenging datasets like CIFAR-10, CIFAR-100, and TinyImageNet, encompassing 860 scenari
    
[^178]: 双重知识蒸馏用于高效声音事件检测

    Dual Knowledge Distillation for Efficient Sound Event Detection

    [https://arxiv.org/abs/2402.02781](https://arxiv.org/abs/2402.02781)

    这项研究提出了一种称为双重知识蒸馏的框架，用于开发高效的声音事件检测系统。这个框架通过时序平均知识蒸馏和增强嵌入特征蒸馏来稳定地蒸馏知识并提升上下文学习能力，在实验中取得了良好的性能。

    

    声音事件检测（SED）对于识别特定声音及其在声学信号中的时间位置至关重要。特别是在设备上的应用中，由于计算资源有限，这变得很具挑战性。为了解决这个问题，我们在本研究中引入了一种新颖的框架，称之为双重知识蒸馏，用于开发高效的SED系统。我们提出的双重知识蒸馏以时序平均知识蒸馏（TAKD）为开端，利用从学生模型参数的时序平均得到的平均学生模型。这使得学生模型能够间接地从预训练的教师模型中学习，确保稳定的知识蒸馏。随后，我们引入了增强嵌入特征蒸馏（EEFD），其中包含在学生模型中引入了嵌入蒸馏层来增强上下文学习。在DCASE 2023任务4A公共评估数据集上，我们的双重知识蒸馏SED系统表现出很好的性能。

    Sound event detection (SED) is essential for recognizing specific sounds and their temporal locations within acoustic signals. This becomes challenging particularly for on-device applications, where computational resources are limited. To address this issue, we introduce a novel framework referred to as dual knowledge distillation for developing efficient SED systems in this work. Our proposed dual knowledge distillation commences with temporal-averaging knowledge distillation (TAKD), utilizing a mean student model derived from the temporal averaging of the student model's parameters. This allows the student model to indirectly learn from a pre-trained teacher model, ensuring a stable knowledge distillation. Subsequently, we introduce embedding-enhanced feature distillation (EEFD), which involves incorporating an embedding distillation layer within the student model to bolster contextual learning. On DCASE 2023 Task 4A public evaluation dataset, our proposed SED system with dual knowle
    
[^179]: 通过快速不准确的预测优化加速拟阵问题

    Accelerating Matroid Optimization through Fast Imprecise Oracles

    [https://arxiv.org/abs/2402.02774](https://arxiv.org/abs/2402.02774)

    本论文研究了如何通过使用快速但不准确的预测模型来加速拟阵优化问题，并提出了实际算法，这些算法在维持对不同质量的预测模型的鲁棒性的同时，只使用了很少的查询

    

    查询复杂模型以获得准确信息（例如流量模型、数据库系统、大型机器学习模型）通常需要耗费大量计算资源和较长的响应时间。因此，如果可以用较少的查询强模型解决不准确结果的问题，那么使用能够快速给出不准确结果的较弱模型是有优势的。在计算一个拟阵的最大权重基础的基础问题中，这个问题是许多组合优化问题的一个已知泛化。算法可以使用一个干净的查询拟阵信息的预测模型。我们额外提供了一个快速但脏的预测模型来模拟一个未知的、可能不同的拟阵。我们设计和分析了实际算法，这些算法只使用很少数量的干净查询相对于脏预测模型的质量，同时保持对任意质量差的脏拟阵的强健性，并接近给定问题的经典算法的性能。值得注意的是，我们证明了在许多方面我们的算法是最佳的

    Querying complex models for precise information (e.g. traffic models, database systems, large ML models) often entails intense computations and results in long response times. Thus, weaker models which give imprecise results quickly can be advantageous, provided inaccuracies can be resolved using few queries to a stronger model. In the fundamental problem of computing a maximum-weight basis of a matroid, a well-known generalization of many combinatorial optimization problems, algorithms have access to a clean oracle to query matroid information. We additionally equip algorithms with a fast but dirty oracle modelling an unknown, potentially different matroid. We design and analyze practical algorithms which only use few clean queries w.r.t. the quality of the dirty oracle, while maintaining robustness against arbitrarily poor dirty matroids, approaching the performance of classic algorithms for the given problem. Notably, we prove that our algorithms are, in many respects, best-possible
    
[^180]: 对比扩散器：通过对比学习规划高回报状态

    Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning

    [https://arxiv.org/abs/2402.02772](https://arxiv.org/abs/2402.02772)

    在强化学习中应用扩散模型进行规划常受限于基础分布和样本多样性。本文提出的CDiffuser方法通过对比学习来提高到达高回报状态的概率。

    

    最近在强化学习中应用扩散模型进行长期规划引起了广泛关注。几种基于扩散的方法成功地利用了扩散的建模能力进行任意分布的规划。这些方法为规划生成了后续轨迹，并取得了显著改进。然而，这些方法受到基础分布的限制，并忽视了样本的多样性，在这些方法中，不同状态具有不同的回报。它们仅仅利用扩散模型来学习离线数据集的分布，并生成与离线数据集具有相同分布的轨迹。因此，这些模型到达高回报状态的概率在很大程度上依赖于数据集的分布。即使配备了引导模型，性能仍然受到压制。针对这些限制，本文提出了一种名为CDiffuser的新方法，设计了一个返回函数

    Applying diffusion models in reinforcement learning for long-term planning has gained much attention recently. Several diffusion-based methods have successfully leveraged the modeling capabilities of diffusion for arbitrary distributions. These methods generate subsequent trajectories for planning and have demonstrated significant improvement. However, these methods are limited by their plain base distributions and their overlooking of the diversity of samples, in which different states have different returns. They simply leverage diffusion to learn the distribution of offline dataset, generate the trajectories whose states share the same distribution with the offline dataset. As a result, the probability of these models reaching the high-return states is largely dependent on the dataset distribution. Even equipped with the guidance model, the performance is still suppressed. To address these limitations, in this paper, we propose a novel method called CDiffuser, which devises a return
    
[^181]: 从教学中学习正则化: 易于模仿的可推广关系

    Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate

    [https://arxiv.org/abs/2402.02769](https://arxiv.org/abs/2402.02769)

    通过从教学中学习（LoT）技术，我们提出了一种深度神经网络正则化技术，能够通过引入辅助学生模型来提升主模型的泛化性能。实验证明，LoT能有效识别具有泛化和可教授关系的信息。

    

    泛化仍然是机器学习中的一个核心挑战。在这项工作中，我们提出了一种新颖的深度神经网络正则化技术，即从教学中学习（Learning from Teaching，简称LoT），以增强泛化性能。受到人类捕捉简明抽象模式的能力的启发，我们假设可推广的关系更容易教授。LoT通过辅助学生模型来实现这个概念，通过提供反馈来训练主模型和改进主模型，以捕捉更多具有泛化和可教授关系的信息。我们在计算机视觉、自然语言处理和强化学习等多个领域进行的实验结果表明，引入LoT相比仅在原始训练数据上训练模型带来了显著的好处。这表明了LoT在识别可推广信息方面的有效性。

    Generalization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to teach. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and improve the main model to capture more generalizable and teachable correlations by providing feedback. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to merely training models on the original training data. It suggests the effectiveness of LoT in identifying generalizable information without falling into th
    
[^182]: 意图概要和通过新兴通信进行翻译的研究

    Intent Profiling and Translation Through Emergent Communication

    [https://arxiv.org/abs/2402.02768](https://arxiv.org/abs/2402.02768)

    本研究提出了一个基于人工智能的意图概要和翻译框架，通过新兴通信实现应用程序的意图概要，解决了应用程序与网络之间复杂的通信问题。

    

    为了有效地表达和满足网络应用的需求，基于意图的网络管理作为一种有前景的解决方案出现了。在基于意图的方法中，用户和应用程序使用高级抽象语言来表达他们的意图。尽管这种抽象简化了网络操作，但它对于有效地表达应用程序的意图并将其映射到不同的网络能力提出了许多挑战。因此，在这项工作中，我们提出了一种基于人工智能的意图概要和翻译框架。我们考虑到一个场景，在这个场景中，与网络交互的应用程序使用他们的领域语言来表达他们对网络服务的需求。机器与机器之间的通信（即应用程序与网络之间的通信）是复杂的，因为它要求网络学习如何理解每个应用程序的领域语言，这既不实际也不可伸缩。相反，我们提出了一个基于新兴通信的意图概要框架，用于应用程序的意图概要。

    To effectively express and satisfy network application requirements, intent-based network management has emerged as a promising solution. In intent-based methods, users and applications express their intent in a high-level abstract language to the network. Although this abstraction simplifies network operation, it induces many challenges to efficiently express applications' intents and map them to different network capabilities. Therefore, in this work, we propose an AI-based framework for intent profiling and translation. We consider a scenario where applications interacting with the network express their needs for network services in their domain language. The machine-to-machine communication (i.e., between applications and the network) is complex since it requires networks to learn how to understand the domain languages of each application, which is neither practical nor scalable. Instead, a framework based on emergent communication is proposed for intent profiling, in which applica
    
[^183]: 可解释的声音分类的焦点调制网络

    Focal Modulation Networks for Interpretable Sound Classification

    [https://arxiv.org/abs/2402.02754](https://arxiv.org/abs/2402.02754)

    本文通过引入焦点调制网络（FocalNets）来解决音频领域中可解释性的问题，首次在环境声音分类任务中应用，并在可解释性和准确性方面优于类似大小的视觉变换器和专门用于音频领域的PIQ方法。

    

    深度神经网络的不断成功引发了与其黑盒本质相关的可解释性和信任的问题。虽然在视觉和语言领域对解释技术进行了广泛探索，但在音频领域的可解释性却受到了有限的关注，主要集中在事后解释上。本文通过利用最近提出的无注意力焦点调制网络（FocalNets），在音频领域中首次应用FocalNets来解决可解释性的问题。我们将FocalNets应用于环境声音分类任务，并在流行的ESC-50数据集上评估其可解释性。我们的方法在准确性和可解释性方面均优于类似大小的视觉变换器。此外，与专门用于音频领域事后解释的PIQ方法相比，我们的方法也具有竞争力。

    The increasing success of deep neural networks has raised concerns about their inherent black-box nature, posing challenges related to interpretability and trust. While there has been extensive exploration of interpretation techniques in vision and language, interpretability in the audio domain has received limited attention, primarily focusing on post-hoc explanations. This paper addresses the problem of interpretability by-design in the audio domain by utilizing the recently proposed attention-free focal modulation networks (FocalNets). We apply FocalNets to the task of environmental sound classification for the first time and evaluate their interpretability properties on the popular ESC-50 dataset. Our method outperforms a similarly sized vision transformer both in terms of accuracy and interpretability. Furthermore, it is competitive against PIQ, a method specifically designed for post-hoc interpretation in the audio domain.
    
[^184]: KIVI：一种无需调整的非对称2位量化KV缓存技术

    KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache

    [https://arxiv.org/abs/2402.02750](https://arxiv.org/abs/2402.02750)

    该论文提出了一种无需调整的非对称2位量化KV缓存技术，以解决存储注意力键和值的内存需求增加和推断速度受限问题。

    

    高效地为大型语言模型（LLMs）提供服务需要将许多请求批量处理以减少每个请求的成本。然而，存储注意力键和值以避免重新计算的键值（KV）缓存显著增加了内存需求，并成为速度和内存使用的新瓶颈。这种内存需求随着批处理大小和上下文长度的增加而增加。此外，推断速度受到KV缓存大小的限制，因为GPU的SRAM必须从主GPU内存中加载整个KV缓存以生成每个标记，导致计算核心在此过程中处于空闲状态。减小KV缓存大小的一个直接而有效的解决方案是量化，通过减少KV缓存所需的总字节数来实现。然而，目前缺乏对KV缓存元素分布进行深入研究以了解KV缓存量化的难度和限制。为了弥补这一空白，我们开展了一项全面的元素分布研究。。。

    Efficiently serving large language models (LLMs) requires batching many requests together to reduce the cost per request. Yet, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. This memory demand increases with larger batch sizes and longer context lengths. Additionally, the inference speed is limited by the size of KV cache, as the GPU's SRAM must load the entire KV cache from the main GPU memory for each token generated, causing the computational core to be idle during this process. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribut
    
[^185]: 标准 Gaussian 过程在高维贝叶斯优化中足以应对

    Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization

    [https://arxiv.org/abs/2402.02746](https://arxiv.org/abs/2402.02746)

    标准 Gaussian 过程在高维贝叶斯优化中表现优秀，经验证据显示其在函数估计和协方差建模中克服了高维输入困难，比专门为高维优化设计的方法表现更好。

    

    长期以来，人们普遍认为使用标准 Gaussian 过程（GP）进行贝叶斯优化（BO），即标准 BO，在高维优化问题中效果不佳。这种观念可以部分归因于 Gaussian 过程在协方差建模和函数估计中对高维输入的困难。虽然这些担忧看起来合理，但缺乏支持这种观点的经验证据。本文系统地研究了在各种合成和真实世界基准问题上，使用标准 GP 回归进行高维优化的贝叶斯优化。令人惊讶的是，标准 GP 的表现始终位于最佳范围内，往往比专门为高维优化设计的现有 BO 方法表现更好。与刻板印象相反，我们发现标准 GP 可以作为学习高维目标函数的能力强大的代理。在没有强结构假设的情况下，使用标准 GP 进行 BO 可以获得非常好的性能。

    There has been a long-standing and widespread belief that Bayesian Optimization (BO) with standard Gaussian process (GP), referred to as standard BO, is ineffective in high-dimensional optimization problems. This perception may partly stem from the intuition that GPs struggle with high-dimensional inputs for covariance modeling and function estimation. While these concerns seem reasonable, empirical evidence supporting this belief is lacking. In this paper, we systematically investigated BO with standard GP regression across a variety of synthetic and real-world benchmark problems for high-dimensional optimization. Surprisingly, the performance with standard GP consistently ranks among the best, often outperforming existing BO methods specifically designed for high-dimensional optimization by a large margin. Contrary to the stereotype, we found that standard GP can serve as a capable surrogate for learning high-dimensional target functions. Without strong structural assumptions, BO wit
    
[^186]: 具有Koopman算子的全局超梯度估计

    Glocal Hypergradient Estimation with Koopman Operator

    [https://arxiv.org/abs/2402.02741](https://arxiv.org/abs/2402.02741)

    本文提出了一种具有Koopman算子的全局超梯度估计方法，通过使用局部超梯度的轨迹来高效地近似全局超梯度，实现了超参数的贪婪优化，兼具可靠性和效率。

    

    基于梯度的超参数优化方法使用超梯度来更新超参数，即元标准的梯度与超参数的关系。先前的研究使用两种不同的更新策略：一种是使用模型训练完成后得到的全局超梯度来优化超参数，另一种是使用每个模型更新之后得到的局部超梯度。虽然全局超梯度具有可靠性，但计算成本显著；相反，局部超梯度速度快但常常不是最优的。在本文中，我们提出了glocal超梯度估计，将“全局”的质量与“局部”的效率结合起来。为此，我们使用Koopman算子理论来线性化超梯度的动态，以便可以仅通过使用局部超梯度的轨迹来高效地近似全局超梯度。因此，我们可以使用估计的全局超梯度贪婪地优化超参数，同时实现可靠性和效率。

    Gradient-based hyperparameter optimization methods update hyperparameters using hypergradients, gradients of a meta criterion with respect to hyperparameters. Previous research used two distinct update strategies: optimizing hyperparameters using global hypergradients obtained after completing model training or local hypergradients derived after every few model updates. While global hypergradients offer reliability, their computational cost is significant; conversely, local hypergradients provide speed but are often suboptimal. In this paper, we propose glocal hypergradient estimation, blending "global" quality with "local" efficiency. To this end, we use the Koopman operator theory to linearize the dynamics of hypergradients so that the global hypergradients can be efficiently approximated only by using a trajectory of local hypergradients. Consequently, we can optimize hyperparameters greedily using estimated global hypergradients, achieving both reliability and efficiency simultaneo
    
[^187]: DisDet: 探索扩散模型中后门攻击的可检测性

    DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models

    [https://arxiv.org/abs/2402.02739](https://arxiv.org/abs/2402.02739)

    本文首次系统地探究了后门扩散模型中毒噪声输入的可检测性，发现分布差异在木马检测中起着重要作用，并提出了一种低成本的触发检测机制。

    

    在令人兴奋的生成型AI时代中，扩散模型已成为一种非常强大且被广泛采用的内容生成和编辑工具，适用于各种数据模态，因此研究其潜在安全风险非常必要且关键。最近的一些开创性研究表明，扩散模型对于后门攻击的脆弱性，呼吁对这种流行且基础的AI技术的安全挑战进行深入分析和调查。在本文中，我们首次系统地探究了后门扩散模型中毒噪声输入的可检测性，这是一个重要的性能度量指标，但在现有研究中很少被探索。从防御者的角度出发，我们首先分析了现有扩散后门攻击中触发模式的属性，发现分布差异在木马检测中起着重要作用。基于这一发现，我们提出了一种低成本的触发检测机制。

    In the exciting generative AI era, the diffusion model has emerged as a very powerful and widely adopted content generation and editing tool for various data modalities, making the study of their potential security risks very necessary and critical. Very recently, some pioneering works have shown the vulnerability of the diffusion model against backdoor attacks, calling for in-depth analysis and investigation of the security challenges of this popular and fundamental AI technique.   In this paper, for the first time, we systematically explore the detectability of the poisoned noise input for the backdoored diffusion models, an important performance metric yet little explored in the existing works. Starting from the perspective of a defender, we first analyze the properties of the trigger pattern in the existing diffusion backdoor attacks, discovering the important role of distribution discrepancy in Trojan detection. Based on this finding, we propose a low-cost trigger detection mechan
    
[^188]: 基于融合策略视角的LiDAR-Camera融合模型对抗天气干扰的鲁棒性改进

    Improving Robustness of LiDAR-Camera Fusion Model against Weather Corruption from Fusion Strategy Perspective

    [https://arxiv.org/abs/2402.02738](https://arxiv.org/abs/2402.02738)

    本文从融合策略的视角评估了LiDAR-Camera融合模型对抗常见天气干扰的鲁棒性，并提出了一种灵活权重融合策略以增强模型的鲁棒性。

    

    近年来，LiDAR-Camera融合模型在自动驾驶中的3D物体检测任务中取得了显著进展。然而，在复杂的物理世界中，它们对常见的天气干扰（如雾、雨、雪和阳光）的鲁棒性仍未被充分探索。本文从融合策略的视角评估了融合模型在受损数据集上的鲁棒性。在评估的基础上，我们进一步提出了一种简洁而实用的融合策略，通过灵活权重融合来自LiDAR和相机源的特征以适应不同的天气场景，以增强融合模型的鲁棒性。在四种融合模型上进行的实验证实了该方法的广泛适用性和有效性。

    In recent years, LiDAR-camera fusion models have markedly advanced 3D object detection tasks in autonomous driving. However, their robustness against common weather corruption such as fog, rain, snow, and sunlight in the intricate physical world remains underexplored. In this paper, we evaluate the robustness of fusion models from the perspective of fusion strategies on the corrupted dataset. Based on the evaluation, we further propose a concise yet practical fusion strategy to enhance the robustness of the fusion models, namely flexibly weighted fusing features from LiDAR and camera sources to adapt to varying weather scenarios. Experiments conducted on four types of fusion models, each with two distinct lightweight implementations, confirm the broad applicability and effectiveness of the approach.
    
[^189]: 在数据不足的环境中，使用运动线索监督单帧人体姿势和形状估计

    Using Motion Cues to Supervise Single-Frame Body Pose and Shape Estimation in Low Data Regimes

    [https://arxiv.org/abs/2402.02736](https://arxiv.org/abs/2402.02736)

    本研究提出在数据不足的情况下，利用易获得的未注释视频来监督单帧人体姿势和形状估计。通过计算连续帧的姿势和光流，并通过强制保持图像光流与姿势变化光流的一致性，可以更有效地优化网络权重，并与使用更多带注释数据训练的方法表现相当。

    

    当有足够的带注释训练数据时，监督深度学习算法通过使用单个摄像机来估计人体姿势和形状。当这些数据不足时，可以通过使用其他信息源（如身体形状数据库）来学习先验知识来改进估计效果。然而，这种信息源也并不总是可用的。我们展示了在这种情况下，可以使用易获得的未注释视频来提供所需的监督信号。给定使用不足的带注释数据的训练模型，我们计算连续帧中的姿势以及它们之间的光流。然后，我们在图像光流和从一个帧到下一个的姿势变化中推断出的光流之间强制保持一致性。这提供了足够的附加监督来有效地优化网络权重，并与使用更多带注释数据训练的方法表现相当。

    When enough annotated training data is available, supervised deep-learning algorithms excel at estimating human body pose and shape using a single camera. The effects of too little such data being available can be mitigated by using other information sources, such as databases of body shapes, to learn priors. Unfortunately, such sources are not always available either. We show that, in such cases, easy-to-obtain unannotated videos can be used instead to provide the required supervisory signals. Given a trained model using too little annotated data, we compute poses in consecutive frames along with the optical flow between them. We then enforce consistency between the image optical flow and the one that can be inferred from the change in pose from one frame to the next. This provides enough additional supervision to effectively refine the network weights and to perform on par with methods trained using far more annotated data.
    
[^190]: ToonAging: 艺术肖像风格转换下的人脸逆龄化

    ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer

    [https://arxiv.org/abs/2402.02733](https://arxiv.org/abs/2402.02733)

    本研究提出了一种新颖的一阶段方法，结合肖像风格转换实现人脸逆龄化，解决了NPR图像上编辑年龄的问题，并在单个生成步骤中执行。该方法利用了现有的人脸逆龄化和风格转换网络，并且独特地融合了不同的潜在向量，从而保留了面部属性。

    

    人脸逆龄化是计算机视觉和图形学中的一个重要领域，在电影、广告和直播等逼真领域中具有重要应用。最近，将人脸逆龄化应用于非逼真图像，如漫画、插图和动画，在各种娱乐行业中成为一个新的需求。然而，缺乏一个能够无缝编辑NPR图像上显现年龄的网络意味着这些任务一直局限于一个简单的顺序方法，这往往会导致不愉快的伪影和由于域差异而丢失面部属性。在本文中，我们引入了一种新颖的单阶段人脸逆龄化方法，结合了肖像风格转换，在一个生成步骤中完成。我们利用现有的人脸逆龄化和风格转换网络，两者都在相同的PR领域进行训练。我们的方法独特地融合了不同的潜在向量，每个向量负责管理与衰老相关的属性。

    Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attribu
    
[^191]: 基于生成模型的黑盒替代攻击方法

    A Generative Approach to Surrogate-based Black-box Attacks

    [https://arxiv.org/abs/2402.02732](https://arxiv.org/abs/2402.02732)

    提出了一种基于生成模型的黑盒替代攻击方法，通过学习样本分布，生成对抗性样本，用于攻击深度神经网络。

    

    基于生成模型的黑盒替代攻击揭示了深度神经网络的易受攻击性。这些攻击旨在为给定的一组样本，通过黑盒目标反馈来生成对抗性样本。目前最先进的替代攻击方法是通过训练一个鉴别性替代模型来模拟目标模型的输出，以学习目标模型的决策边界。然后，白盒攻击针对替代模型生成与原始样本相似但属于其他类别的对抗性样本。然而，由于样本数量有限，鉴别性替代模型无法准确学习目标模型的决策边界，因此这些替代攻击方法的成功率较低。与鉴别性方法不同，我们提出了一种生成模型的替代方法，该模型学习目标模型决策边界附近或相邻样本的分布。通过生成模型学习的分布可以用于生成对抗性样本。

    Surrogate-based black-box attacks have exposed the heightened vulnerability of DNNs. These attacks are designed to craft adversarial examples for any samples with black-box target feedback for only a given set of samples. State-of-the-art surrogate-based attacks involve training a discriminative surrogate that mimics the target's outputs. The goal is to learn the decision boundaries of the target. The surrogate is then attacked by white-box attacks to craft adversarial examples similar to the original samples but belong to other classes. With limited samples, the discriminative surrogate fails to accurately learn the target's decision boundaries, and these surrogate-based attacks suffer from low success rates. Different from the discriminative approach, we propose a generative surrogate that learns the distribution of samples residing on or close to the target's decision boundaries. The distribution learned by the generative surrogate can be used to craft adversarial examples that have
    
[^192]: 基于GAN的快速准确合作式无线电地图估计

    Fast and Accurate Cooperative Radio Map Estimation Enabled by GAN

    [https://arxiv.org/abs/2402.02729](https://arxiv.org/abs/2402.02729)

    本文提出了一种基于GAN的合作式无线电地图估计方法（GAN-CRME），能够快速准确地估计无线电地图，无需发射机信息。该方法利用移动用户处的接收信号强度测量和地理地图之间的交互作用，通过深度神经网络推断无线电地图，并使用基于GAN的学习算法提高性能。

    

    在6G时代，支持多样化无线应用的实时无线资源监测和管理成为迫切需求。这要求快速准确地估计无线资源的分布，通常以地理环境中的空间信号功率强度表示，即无线电地图。本文提出了一种基于生成对抗网络（GAN）的合作式无线电地图估计（CRME）方法，称为GAN-CRME，它具备快速准确的无线电地图估计能力，无需发射机信息。通过利用移动用户处分布式接收信号强度（RSS）测量与地理地图之间的交互作用，利用深度神经网络估计器推断无线电地图，从而降低数据获取成本和计算复杂性。此外，提出了一种基于GAN的学习算法，通过利用深度神经网络估计器的推断能力来提高性能。

    In the 6G era, real-time radio resource monitoring and management are urged to support diverse wireless-empowered applications. This calls for fast and accurate estimation on the distribution of the radio resources, which is usually represented by the spatial signal power strength over the geographical environment, known as a radio map. In this paper, we present a cooperative radio map estimation (CRME) approach enabled by the generative adversarial network (GAN), called as GAN-CRME, which features fast and accurate radio map estimation without the transmitters' information. The radio map is inferred by exploiting the interaction between distributed received signal strength (RSS) measurements at mobile users and the geographical map using a deep neural network estimator, resulting in low data-acquisition cost and computational complexity. Moreover, a GAN-based learning algorithm is proposed to boost the inference capability of the deep neural network estimator by exploiting the power o
    
[^193]: FDNet: 频域去噪神经网络用于识别由诱导多能干细胞分化而来的星形胶质细胞

    FDNet: Frequency Domain Denoising Network For Cell Segmentation in Astrocytes Derived From Induced Pluripotent Stem Cells

    [https://arxiv.org/abs/2402.02724](https://arxiv.org/abs/2402.02724)

    本论文提出了一种名为FDNet的频域去噪神经网络用于识别由诱导多能干细胞分化而来的星形胶质细胞。由于背景和干扰信息的存在，观察星形胶质细胞变得困难，而现有的深度学习方法无法解决这个问题。本文还引入了一个新的数据集IAI704来解决这个问题。

    

    从体细胞人工生成的诱导多能干细胞（iPSCs）在疾病建模和神经退行性疾病的药物筛选中起着重要作用。从iPSCs分化出的星形胶质细胞是研究神经代谢的重要目标。通过在不同分化阶段观察显微镜图像中观察到的形态变化来监测星形胶质细胞的分化进程，然后通过分子生物学技术在成熟时确定。然而，星形胶质细胞通常会“完美地”融入背景中，而且其中一些细胞会被干扰信息（例如死细胞、培养基沉积物和细胞残骸）掩盖，这使得观察星形胶质细胞变得困难。由于缺乏标注数据集，现有的最先进的深度学习方法无法解决这个问题。本文提出了一种新的任务，即星形胶质细胞分割，并引入了一个名为IAI704的新数据集，其中包含704个图像及其co

    Artificially generated induced pluripotent stem cells (iPSCs) from somatic cells play an important role for disease modeling and drug screening of neurodegenerative diseases. Astrocytes differentiated from iPSCs are important targets to investigate neuronal metabolism. The astrocyte differentiation progress can be monitored through the variations of morphology observed from microscopy images at different differentiation stages, then determined by molecular biology techniques upon maturation. However, the astrocytes usually ``perfectly'' blend into the background and some of them are covered by interference information (i.e., dead cells, media sediments, and cell debris), which makes astrocytes difficult to observe. Due to the lack of annotated datasets, the existing state-of-the-art deep learning approaches cannot be used to address this issue. In this paper, we introduce a new task named astrocyte segmentation with a novel dataset, called IAI704, which contains 704 images and their co
    
[^194]: 折扣自适应在线预测

    Discounted Adaptive Online Prediction

    [https://arxiv.org/abs/2402.02720](https://arxiv.org/abs/2402.02720)

    本论文提出了一种折扣自适应在线预测算法，该算法适应于复杂的损失序列和比较器，并改进了非自适应算法。算法具有无需结构性假设的理论保证，并且在超参数调整方面具有鲁棒性。通过在线符合预测任务的实验证明了算法的好处。

    

    在线学习并不总是要记住一切。由于未来在统计上可能与过去有很大的不同，一个关键的挑战是在新数据到来时优雅地忘记历史。为了形式化这种直觉，我们运用最近发展的自适应在线学习技术重新思考了经典的折扣遗憾概念。我们的主要结果是一个新的算法，它适应于损失序列和比较器的复杂性，改进了广泛使用的非自适应算法-梯度下降算法，且具有恒定的学习率。特别地，我们的理论保证不需要任何结构性假设，只要求凸性，并且该算法经过证明对次优的超参数调整具有鲁棒性。我们进一步通过在线符合预测来展示这些好处，而在线符合预测是一个带有集合成员决策的下游在线学习任务。

    Online learning is not always about memorizing everything. Since the future can be statistically very different from the past, a critical challenge is to gracefully forget the history while new data comes in. To formalize this intuition, we revisit the classical notion of discounted regret using recently developed techniques in adaptive online learning. Our main result is a new algorithm that adapts to the complexity of both the loss sequence and the comparator, improving the widespread non-adaptive algorithm - gradient descent with a constant learning rate. In particular, our theoretical guarantee does not require any structural assumption beyond convexity, and the algorithm is provably robust to suboptimal hyperparameter tuning. We further demonstrate such benefits through online conformal prediction, a downstream online learning task with set-membership decisions.
    
[^195]: 理解LLM代理的规划：一项调查研究

    Understanding the planning of LLM agents: A survey

    [https://arxiv.org/abs/2402.02716](https://arxiv.org/abs/2402.02716)

    这项调查研究系统地介绍了基于LLM的代理规划的最新进展和挑战。

    

    由于大型语言模型（LLM）展示了显著的智能，将LLM用作自主代理的规划模块的进展引起了更多关注。本调查提供了对基于LLM的代理规划的首个系统视角，涵盖了旨在提高规划能力的最新研究。我们对LLM-代理规划的现有研究进行了分类，包括任务分解、计划选择、外部模块、反思和记忆。对于每个方向进行了全面分析，并进一步讨论了该领域的挑战。

    As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.
    
[^196]: 一篇位置论文: 大语言模型对时间序列分析有什么启示

    Position Paper: What Can Large Language Models Tell Us about Time Series Analysis

    [https://arxiv.org/abs/2402.02713](https://arxiv.org/abs/2402.02713)

    大语言模型有潜力颠覆时间序列分析，提升决策效率，推动时间序列分析智能的普及化。这种进展可以带来模态切换和时间序列问答等多种可能性。

    

    时间序列分析对于理解各种现实世界系统和应用中的复杂性至关重要。尽管大语言模型（LLM）最近取得了显著进展，但具备时间序列分析能力的人工通用智能（AGI）的发展仍处于初级阶段。目前大部分时间序列模型主要依赖领域知识和大量模型调整，主要集中在预测任务上。本文提出，目前的LLM具有颠覆时间序列分析的潜力，从而促进高效的决策和推进向更普适形式的时间序列分析智能发展。这种进步可以打开各种可能性，包括模态切换和时间序列问答。我们鼓励研究人员和实践者认识到LLM在推进时间序列分析方面的潜力，并强调对这些相关工作的信任需求。

    Time series analysis is essential for comprehending the complexities inherent in various real-world systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including modality switching and time series question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts
    
[^197]: 用于优化物理信息神经网络的架构策略

    Architectural Strategies for the optimization of Physics-Informed Neural Networks

    [https://arxiv.org/abs/2402.02711](https://arxiv.org/abs/2402.02711)

    本研究从神经架构的角度研究了物理信息神经网络（PINNs）的优化。通过利用神经切向核（NTK），我们发现高斯激活函数在有效训练PINNs时超过其他替代激活函数。此外，我们引入了一种经过预处理的神经架构，进一步增强了优化过程。这些发现在多个PDEs的验证中得到了证实。

    

    物理信息神经网络（PINNs）通过将深度学习与基础物理原理结合起来，为解决偏微分方程（PDEs）中的前向和反向问题提供了一个有希望的途径。尽管PINNs在实践中取得了显著的成功，但它们在一系列PDEs的训练中也因其困难而声名狼藉。在这项工作中，我们从神经架构的角度深入探讨了PINN优化的复杂性。利用神经切向核（NTK），我们的研究揭示了在有效训练PINNs时，高斯激活函数优于几种替代激活函数。借鉴数值线性代数的见解，我们引入了一种经过预处理的神经架构，展示了这种定制架构如何增强优化过程。我们的理论发现通过对科学文献中已有的PDEs进行严格验证得到了证实。

    Physics-informed neural networks (PINNs) offer a promising avenue for tackling both forward and inverse problems in partial differential equations (PDEs) by incorporating deep learning with fundamental physics principles. Despite their remarkable empirical success, PINNs have garnered a reputation for their notorious training challenges across a spectrum of PDEs. In this work, we delve into the intricacies of PINN optimization from a neural architecture perspective. Leveraging the Neural Tangent Kernel (NTK), our study reveals that Gaussian activations surpass several alternate activations when it comes to effectively training PINNs. Building on insights from numerical linear algebra, we introduce a preconditioned neural architecture, showcasing how such tailored architectures enhance the optimization process. Our theoretical findings are substantiated through rigorous validation against established PDEs within the scientific literature.
    
[^198]: 多任务模型合并的表征手术

    Representation Surgery for Multi-Task Model Merging

    [https://arxiv.org/abs/2402.02705](https://arxiv.org/abs/2402.02705)

    该论文提出了一种名为“Surgery”的表征手术解决方案，用于减少多任务模型合并中的表示偏差。该方法通过一个轻量级的任务专用模块，针对合并模型的表示进行修正，以提高合并模型的性能。

    

    多任务学习（MTL）将多个任务的信息压缩到一个统一的骨干模型中，以提高计算效率和泛化能力。最近的研究直接合并多个独立训练的模型来执行MTL，而不是收集它们的原始数据进行联合训练，从而极大地扩展了MTL的应用场景。然而，通过可视化现有模型合并方案的表示分布，我们发现合并模型往往面临表示偏差的困境。也就是说，合并模型与个体模型之间的表示分布存在明显的差异，导致合并MTL性能较差。在本文中，我们提出了一个称为“Surgery”的表征手术解决方案，以减少合并模型中的表示偏差。具体而言，“Surgery”是一个轻量级的任务专用模块，它以合并模型的表示为输入，并试图输出其中包含的偏差。

    Multi-task learning (MTL) compresses the information from multiple tasks into a unified backbone to improve computational efficiency and generalization. Recent work directly merges multiple independently trained models to perform MTL instead of collecting their raw data for joint training, greatly expanding the application scenarios of MTL. However, by visualizing the representation distribution of existing model merging schemes, we find that the merged model often suffers from the dilemma of representation bias. That is, there is a significant discrepancy in the representation distribution between the merged and individual models, resulting in poor performance of merged MTL. In this paper, we propose a representation surgery solution called "Surgery" to reduce representation bias in the merged model. Specifically, Surgery is a lightweight task-specific module that takes the representation of the merged model as input and attempts to output the biases contained in the representation fr
    
[^199]: 理解影响视觉强化学习中泛化差距的因素：理论和实证证据

    Understanding What Affects Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence

    [https://arxiv.org/abs/2402.02701](https://arxiv.org/abs/2402.02701)

    本文通过理论和实证研究，揭示了在测试环境具有干扰因素时影响视觉强化学习中泛化差距的关键因素。结果表明，最小化训练和测试环境之间的表示距离是减少泛化差距最关键的因素。

    

    最近，有许多努力致力于在视觉强化学习中学习对连续控制有用的策略。在这种场景下，学习一个具有泛化能力的策略非常重要，因为测试环境可能与训练环境不同，例如在部署过程中存在干扰因素。许多实际算法被提出来解决这个问题。然而，据我们所知，它们中没有一种算法能够从理论上解释泛化差距的影响因素以及为什么他们的方法有效。在本文中，我们通过在测试环境具有干扰因素时理论上回答影响泛化差距的关键因素来解决这个问题。我们的理论表明，最小化训练和测试环境之间的表示距离（与人类直觉一致）对于减少泛化差距的效益至关重要。我们的理论结果得到了DM数据的实证证据的支持。

    Recently, there are many efforts attempting to learn useful policies for continuous control in visual reinforcement learning (RL). In this scenario, it is important to learn a generalizable policy, as the testing environment may differ from the training environment, e.g., there exist distractors during deployment. Many practical algorithms are proposed to handle this problem. However, to the best of our knowledge, none of them provide a theoretical understanding of what affects the generalization gap and why their proposed methods work. In this paper, we bridge this issue by theoretically answering the key factors that contribute to the generalization gap when the testing environment has distractors. Our theories indicate that minimizing the representation distance between training and testing environments, which aligns with human intuition, is the most critical for the benefit of reducing the generalization gap. Our theoretical results are supported by the empirical evidence in the DM
    
[^200]: 线性上下文马尔可夫决策过程的样本复杂性表征

    Sample Complexity Characterization for Linear Contextual MDPs

    [https://arxiv.org/abs/2402.02700](https://arxiv.org/abs/2402.02700)

    本文研究了线性上下文马尔可夫决策过程（CMDPs）的样本复杂性表征，并提出了两种模型的新颖算法，证明它们具有所需的多项式样本复杂性。其中，对于第一个模型，通过去除可达性假设，改进了现有结果。

    

    上下文马尔可夫决策过程（CMDPs）描述了一类强化学习问题，其中转移内核和奖励函数可以随时间变化，并由一个上下文变量索引的不同MDPs。虽然CMDPs作为一个重要的框架，可以模拟具有时变环境的许多实际应用，但它们在理论上很少有研究。在本文中，我们研究了两个线性函数逼近模型下的CMDPs：模型I具有上下文变化表示和所有上下文公共线性权重；以及模型II具有所有上下文的公共表示和上下文变化的线性权重。对于这两个模型，我们提出了新颖的基于模型的算法，并证明它们具有所需多项式样本复杂性的保证的ε-次优间隙。特别是，将我们对第一个模型的结果实例化为表格CMDP，通过去除可达性假设，改进了现有结果。我们对第二个模型的结果。

    Contextual Markov decision processes (CMDPs) describe a class of reinforcement learning problems in which the transition kernels and reward functions can change over time with different MDPs indexed by a context variable. While CMDPs serve as an important framework to model many real-world applications with time-varying environments, they are largely unexplored from theoretical perspective. In this paper, we study CMDPs under two linear function approximation models: Model I with context-varying representations and common linear weights for all contexts; and Model II with common representations for all contexts and context-varying linear weights. For both models, we propose novel model-based algorithms and show that they enjoy guaranteed $\epsilon$-suboptimality gap with desired polynomial sample complexity. In particular, instantiating our result for the first model to the tabular CMDP improves the existing result by removing the reachability assumption. Our result for the second mode
    
[^201]: 针对鲁棒性说话人验证的对抗性数据增强

    Adversarial Data Augmentation for Robust Speaker Verification

    [https://arxiv.org/abs/2402.02699](https://arxiv.org/abs/2402.02699)

    本论文提出了一种称为对抗性数据增强的方法，它通过结合数据增强和对抗性学习的方式来提高深度说话人模型的鲁棒性。通过使用一个额外的增强分类器来分类不同类型的增强，网络能够生成对该分类器具有欺骗性的说话人嵌入，从而提高了在面对不相关声学变化时的泛化性。

    

    数据增强（DA）由于其易于实现和显著的有效性，在深度说话人模型中广泛受到欢迎。它通过模拟真实的声学变化来丰富训练数据，使深度神经网络能够学习与说话人相关的表示，同时忽略不相关的声学变化，从而提高鲁棒性和泛化性。然而，普通的DA存在一个潜在问题，即增强残差，即不同类型的增强引起的不必要失真。为了解决这个问题，本文提出了一种称为对抗性数据增强（A-DA）的新方法，该方法结合了DA和对抗性学习。具体而言，它涉及一个额外的增强分类器来分类用于数据增强的各种增强类型。这种对抗性学习使网络能够生成可以欺骗增强分类器的说话人嵌入，从而使学习到的说话人嵌入在面对不同类型增强时更具鲁棒性。

    Data augmentation (DA) has gained widespread popularity in deep speaker models due to its ease of implementation and significant effectiveness. It enriches training data by simulating real-life acoustic variations, enabling deep neural networks to learn speaker-related representations while disregarding irrelevant acoustic variations, thereby improving robustness and generalization. However, a potential issue with the vanilla DA is augmentation residual, i.e., unwanted distortion caused by different types of augmentation. To address this problem, this paper proposes a novel approach called adversarial data augmentation (A-DA) which combines DA with adversarial learning. Specifically, it involves an additional augmentation classifier to categorize various augmentation types used in data augmentation. This adversarial learning empowers the network to generate speaker embeddings that can deceive the augmentation classifier, making the learned speaker embeddings more robust in the face of 
    
[^202]: 超越期望: 现实中实现随机优势学习

    Beyond Expectations: Learning with Stochastic Dominance Made Practical

    [https://arxiv.org/abs/2402.02698](https://arxiv.org/abs/2402.02698)

    这项工作首次尝试建立了一个随机优势学习的通用框架，并推广了随机优势的概念以使其能够在任意两个随机变量之间进行比较。同时，我们还开发了一种有效的计算方法来处理连续性评估的问题。

    

    随机优势模型对决策时具有风险厌恶偏好的不确定结果进行建模，相比于仅仅依赖期望值，自然地捕捉了底层不确定性的内在结构。尽管在理论上具有吸引力，但随机优势在机器学习中的应用却很少，主要是由于以下挑战：$\textbf{i)}$ 随机优势的原始概念仅提供了$\textit{部分序}$，因此不能作为最优性准则；和 $\textbf{ii)}$ 由于评估随机优势的连续性本质，目前还缺乏高效的计算方法。在这项工作中，我们首次尝试建立一个与随机优势学习相关的通用框架。我们首先将随机优势概念推广，使得任意两个随机变量之间的比较成为可能。接下来我们开发了一个有效的计算方法，以解决评估随机优势的连续性问题。

    Stochastic dominance models risk-averse preferences for decision making with uncertain outcomes, which naturally captures the intrinsic structure of the underlying uncertainty, in contrast to simply resorting to the expectations. Despite theoretically appealing, the application of stochastic dominance in machine learning has been scarce, due to the following challenges: $\textbf{i)}$, the original concept of stochastic dominance only provides a $\textit{partial order}$, therefore, is not amenable to serve as an optimality criterion; and $\textbf{ii)}$, an efficient computational recipe remains lacking due to the continuum nature of evaluating stochastic dominance.%, which barriers its application for machine learning.   In this work, we make the first attempt towards establishing a general framework of learning with stochastic dominance. We first generalize the stochastic dominance concept to enable feasible comparisons between any arbitrary pair of random variables. We next develop a 
    
[^203]: 深度均衡模型与高维高斯混合模型中不太深的显式模型几乎等价

    Deep Equilibrium Models are Almost Equivalent to Not-so-deep Explicit Models for High-dimensional Gaussian Mixtures

    [https://arxiv.org/abs/2402.02697](https://arxiv.org/abs/2402.02697)

    本文通过对深度均衡模型和显式神经网络模型进行理论分析和实验证明，在高维高斯混合数据下，可以通过设计浅显式网络来实现与给定深度均衡模型相同的特征光谱行为。

    

    深度均衡模型（DEQs）作为典型的隐式神经网络，在各种任务上取得了显着的成功。然而，我们对隐式DEQ和显式神经网络模型之间的连接和差异缺乏理论上的理解。在本文中，我们借鉴最近在随机矩阵理论方面的进展，对高维高斯混合模型输入数据下，隐式DEQ的共轭核（CK）和神经切向核（NTK）矩阵的特征光谱进行了深入分析。我们在这个设置中证明了这些隐式-CKs和NTKs的光谱行为取决于DEQ激活函数和初始权重方差，但仅通过一组四个非线性方程。作为这一理论结果的直接影响，我们证明可以精心设计一个浅显式网络来产生与给定DEQ相同的CK或NTK。尽管这里是针对高斯混合数据推导的，经验结果表明

    Deep equilibrium models (DEQs), as a typical implicit neural network, have demonstrated remarkable success on various tasks. There is, however, a lack of theoretical understanding of the connections and differences between implicit DEQs and explicit neural network models. In this paper, leveraging recent advances in random matrix theory (RMT), we perform an in-depth analysis on the eigenspectra of the conjugate kernel (CK) and neural tangent kernel (NTK) matrices for implicit DEQs, when the input data are drawn from a high-dimensional Gaussian mixture. We prove, in this setting, that the spectral behavior of these Implicit-CKs and NTKs depend on the DEQ activation function and initial weight variances, but only via a system of four nonlinear equations. As a direct consequence of this theoretical result, we demonstrate that a shallow explicit network can be carefully designed to produce the same CK or NTK as a given DEQ. Despite derived here for Gaussian mixture data, empirical results 
    
[^204]: 对负责任机器学习的因果特征选择的研究

    Causal Feature Selection for Responsible Machine Learning

    [https://arxiv.org/abs/2402.02696](https://arxiv.org/abs/2402.02696)

    本调查论文研究了负责任机器学习中的因果特征选择，强调了其对解释性、公平性、对抗鲁棒性和域泛化的作用。

    

    机器学习（ML）已成为许多实际应用的重要方面。因此，负责任机器学习的需求已经出现，重点是将ML模型与伦理和社会价值相一致，同时增强其可靠性和可信度。负责任的ML涉及许多问题。本调查涉及四个主要问题：可解释性，公平性，对抗鲁棒性和域泛化。特征选择在负责任的ML任务中起着重要作用。然而，仅基于变量之间的统计相关性构建模型可能会导致带有偏见和性能下降的虚假模式。本调查专注于当前对因果特征选择的研究：什么是因果特征选择以及它如何加强负责任ML的四个方面。通过识别对结果产生因果影响并区分因果和相关性，因果特征选择被认为是确保ML模型在伦理和社会上负责任的独特方法。

    Machine Learning (ML) has become an integral aspect of many real-world applications. As a result, the need for responsible machine learning has emerged, focusing on aligning ML models to ethical and social values, while enhancing their reliability and trustworthiness. Responsible ML involves many issues. This survey addresses four main issues: interpretability, fairness, adversarial robustness, and domain generalization. Feature selection plays a pivotal role in the responsible ML tasks. However, building upon statistical correlations between variables can lead to spurious patterns with biases and compromised performance. This survey focuses on the current study of causal feature selection: what it is and how it can reinforce the four aspects of responsible ML. By identifying features with causal impacts on outcomes and distinguishing causality from correlation, causal feature selection is posited as a unique approach to ensuring ML models to be ethically and socially responsible in hi
    
[^205]: 利用类别概率进行黑盒子句级攻击

    Exploiting Class Probabilities for Black-box Sentence-level Attacks

    [https://arxiv.org/abs/2402.02695](https://arxiv.org/abs/2402.02695)

    该论文研究了在黑盒子句级攻击中利用类别概率的有效性，并开发了一种新的算法进行攻击。通过与基线方法进行对比，进行了广泛的评估。

    

    句级攻击是针对文本分类器的对抗性句子生成方法，这些句子与正确分类的句子同义，但被分类器错误地分类。在黑盒设置下，分类器只能通过对查询输入的反馈进行访问，这主要以类别概率的形式提供。尽管利用类别概率可以获得更强大的攻击效果，但由于在句级攻击中使用类别概率存在挑战，现有的攻击方法要么不使用反馈，要么仅使用类别标签。为了克服这些挑战，我们开发了一种新的算法，使用类别概率进行黑盒句级攻击，并研究了在攻击成功率上使用类别概率的有效性，并探讨了在黑盒句级攻击中使用类别概率是否值得或可行。我们在各种分类器和基准数据集上对提出的攻击方法进行了广泛评估，并与基线进行了对比。

    Sentence-level attacks craft adversarial sentences that are synonymous with correctly-classified sentences but are misclassified by the text classifiers. Under the black-box setting, classifiers are only accessible through their feedback to queried inputs, which is predominately available in the form of class probabilities. Even though utilizing class probabilities results in stronger attacks, due to the challenges of using them for sentence-level attacks, existing attacks use either no feedback or only the class labels. Overcoming the challenges, we develop a novel algorithm that uses class probabilities for black-box sentence-level attacks, investigate the effectiveness of using class probabilities on the attack's success, and examine the question if it is worthy or practical to use class probabilities by black-box sentence-level attacks. We conduct extensive evaluations of the proposed attack comparing with the baselines across various classifiers and benchmark datasets.
    
[^206]: IEEE ICME 2024大挑战赛: 半监督领域转移下的声场分类描述

    Description on IEEE ICME 2024 Grand Challenge: Semi-supervised Acoustic Scene Classification under Domain Shift

    [https://arxiv.org/abs/2402.02694](https://arxiv.org/abs/2402.02694)

    这项研究介绍了ICME 2024 Grand Challenge中的半监督领域转移下的声场分类任务，该任务探索了不同区域之间领域转移的挑战，同时研究了如何利用未标记数据来提升声场分类模型的性能。

    

    声场分类是计算声场分析中一个关键的研究问题，旨在识别环境的独特声学特征。声场分类任务面临的一个挑战是训练和测试数据之间的分布差异所引起的领域转移。虽然近年来的声场分类挑战已经在设备通用性方面取得了实质性进展，但涉及时间、空间、文化和语言等特征的不同区域之间领域转移的挑战目前尚未得到充分探讨。此外，考虑到现实世界中未标记的声场数据的丰富性，研究利用这些未标记数据的可能方法是很重要的。因此，我们在ICME 2024大挑战赛中引入了半监督领域转移下的声场分类任务。

    Acoustic scene classification (ASC) is a crucial research problem in computational auditory scene analysis, and it aims to recognize the unique acoustic characteristics of an environment. One of the challenges of the ASC task is domain shift caused by a distribution gap between training and testing data. Since 2018, ASC challenges have focused on the generalization of ASC models across different recording devices. Although this task in recent years has achieved substantial progress in device generalization, the challenge of domain shift between different regions, involving characteristics such as time, space, culture, and language, remains insufficiently explored at present. In addition, considering the abundance of unlabeled acoustic scene data in the real world, it is important to study the possible ways to utilize these unlabelled data. Therefore, we introduce the task Semi-supervised Acoustic Scene Classification under Domain Shift in the ICME 2024 Grand Challenge. We encourage par
    
[^207]: 使用图神经网络的链接预测的统计保证

    Statistical Guarantees for Link Prediction using Graph Neural Networks

    [https://arxiv.org/abs/2402.02692](https://arxiv.org/abs/2402.02692)

    本文提出了一种线性图神经网络（LG-GNN）架构，通过计算边缘概率来预测图中的链接，并推导了其在链接预测任务中的性能统计保证。这种架构对于稀疏和稠密图都适用，并在真实和合成数据集上验证了其优势。

    

    本文针对由图上生成的图网络中的链接预测任务，推导了图神经网络（GNN）性能的统计保证。我们提出了一个线性GNN架构（LG-GNN），可以产生对潜在边缘概率的一致估计。我们对均方误差进行了界定，并对LG-GNN在检测高概率边缘的能力给出了保证。我们的保证适用于稀疏和稠密图。最后，我们展示了经典GCN架构的一些缺点，并在真实和合成数据集上验证了我们的结果。

    This paper derives statistical guarantees for the performance of Graph Neural Networks (GNNs) in link prediction tasks on graphs generated by a graphon. We propose a linear GNN architecture (LG-GNN) that produces consistent estimators for the underlying edge probabilities. We establish a bound on the mean squared error and give guarantees on the ability of LG-GNN to detect high-probability edges. Our guarantees hold for both sparse and dense graphs. Finally, we demonstrate some of the shortcomings of the classical GCN architecture, as well as verify our results on real and synthetic datasets.
    
[^208]: 泊松过程用于贝叶斯优化

    Poisson Process for Bayesian Optimization

    [https://arxiv.org/abs/2402.02687](https://arxiv.org/abs/2402.02687)

    提出了一种基于泊松过程的新型排名替代模型，引入了称为泊松过程贝叶斯优化（PoPBO）的高效BO框架，并从经典的LCB和EI模型中得出了两个定制的收集函数以适应它。

    

    贝叶斯优化是一种高效的黑盒优化器，通过概率替代模型建立黑盒函数的绝对函数响应。已经提出了许多方法来构建这种模型，包括基于树结构的Parzen估计方法(TPE)、随机森林(SMAC)和高斯过程(GP)。然而，很少有方法来估计候选项的相对排名，相对排名可以比绝对函数响应更具鲁棒性，并且在函数响应难以处理但偏好可以获取时更具实用性。为此，我们提出了一种基于泊松过程的新型排名替代模型，并引入了一种高效的BO框架，称为泊松过程贝叶斯优化(PoPBO)。进一步从经典的LCB和EI模型中得出了两个定制的收集函数以适应它。与经典的GP-BO方法相比，我们的PoPBO计算成本较低，对噪声具有更好的鲁棒性。

    BayesianOptimization(BO) is a sample-efficient black-box optimizer, and extensive methods have been proposed to build the absolute function response of the black-box function through a probabilistic surrogate model, including Tree-structured Parzen Estimator (TPE), random forest (SMAC), and Gaussian process (GP). However, few methods have been explored to estimate the relative rankings of candidates, which can be more robust to noise and have better practicality than absolute function responses, especially when the function responses are intractable but preferences can be acquired. To this end, we propose a novel ranking-based surrogate model based on the Poisson process and introduce an efficient BO framework, namely Poisson Process Bayesian Optimization (PoPBO). Two tailored acquisition functions are further derived from classic LCB and EI to accommodate it. Compared to the classic GP-BO method, our PoPBO has lower computation costs and better robustness to noise, which is verified b
    
[^209]: 多区域马尔可夫高斯过程：一种发现多个脑区之间方向性通讯的高效方法

    Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions

    [https://arxiv.org/abs/2402.02686](https://arxiv.org/abs/2402.02686)

    本研究提出了一种名为多区域马尔可夫高斯过程的方法，将高斯过程和线性动态系统相结合，有效地发现了多个脑区之间的方向性通讯。通过建立LDS与多输出GP之间的联系，该模型实现了线性推断并提供了可解释的低维表示。

    

    研究不同脑区之间复杂的相互作用对神经科学至关重要。各种统计方法已经探索了多个脑区之间潜在的通讯。两个主要的类别是高斯过程（GP）和线性动态系统（LDS），每个方法都具有独特的优势。基于GP的方法有效地发现了潜在变量，如频带和通讯方向。相反，基于LDS的方法在计算效率上高，但在潜在表示方面缺乏强大的表达能力。在本研究中，我们通过创建一个与多输出GP相对应的LDS，即多区域马尔可夫高斯过程（MRM-GP），将这两种方法合二为一。我们的工作首次建立了LDS和多输出GP之间的联系，在神经记录的潜在空间中明确建模了频率和相位延迟。因此，该模型在时间点上实现了线性推断成本，并提供了可解释的低维表示。

    Studying the complex interactions between different brain regions is crucial in neuroscience. Various statistical methods have explored the latent communication across multiple brain regions. Two main categories are the Gaussian Process (GP) and Linear Dynamical System (LDS), each with unique strengths. The GP-based approach effectively discovers latent variables such as frequency bands and communication directions. Conversely, the LDS-based approach is computationally efficient but lacks powerful expressiveness in latent representation. In this study, we merge both methodologies by creating an LDS mirroring a multi-output GP, termed Multi-Region Markovian Gaussian Process (MRM-GP). Our work is the first to establish a connection between an LDS and a multi-output GP that explicitly models frequencies and phase delays within the latent space of neural recordings. Consequently, the model achieves a linear inference cost over time points and provides an interpretable low-dimensional repre
    
[^210]: 等变对称破缺集

    Equivariant Symmetry Breaking Sets

    [https://arxiv.org/abs/2402.02681](https://arxiv.org/abs/2402.02681)

    这里是中文总结出的一句话要点: 该论文提出了一种全等变的对称破缺框架，通过引入对称破缺集来破坏等变神经网络中的对称性。这种方法通用且适用于任何群的等变性。

    

    等变神经网络（ENN）已被证明在涉及潜在对称性的应用中非常有效。通过设计，ENN在给定更高对称性输入时无法产生较低对称性输出。然而，在许多物理系统中会发生自发对称破缺，我们可以从一个初始高度对称的状态获得一个较不对称的稳定状态。因此，我们必须了解如何系统地在ENN中破坏对称性。在这项工作中，我们提出了一种全等变的新型对称破缺框架。我们强调我们的方法是通用的，并适用于任何群的等变性。为了实现这一目标，我们引入了对称破缺集（SBS）的概念。我们不是重新设计现有的网络，而是设计了一组对称破缺对象，根据输入和输出的对称性将其输入到我们的网络中。我们展示了在这些集合上定义等变性的一种自然方式，它提供了额外的约束。通过最小化... (the abstract is incomplete and cut off)

    Equivariant neural networks (ENNs) have been shown to be extremely effective in applications involving underlying symmetries. By construction ENNs cannot produce lower symmetry outputs given a higher symmetry input. However, spontaneous symmetry breaking occurs in many physical systems and we may obtain a less symmetric stable state from an initial highly symmetric one. Hence, it is imperative that we understand how to systematically break symmetry in ENNs. In this work, we propose a novel symmetry breaking framework that is fully equivariant. We emphasize that our approach is general and applicable to equivariance under any group. To achieve this, we introduce the idea of symmetry breaking sets (SBS). Rather than redesign existing networks, we design sets of symmetry breaking objects which we feed into our network based on the symmetry of our inputs and outputs. We show there is a natural way to define equivariance on these sets, which gives an additional constraint. Minimizing the si
    
[^211]: 大型语言模型存在地理偏见

    Large Language Models are Geographically Biased

    [https://arxiv.org/abs/2402.02680](https://arxiv.org/abs/2402.02680)

    本文研究了大型语言模型的地理偏见，并展示了其对地理空间预测的系统错误，通过零射击地理空间预测来评估其对世界的认知。

    

    大型语言模型（LLMs）内在地含有其训练语料库中的偏见，这可能导致社会伤害的持续存在。随着这些基础模型的影响力不断增长，理解和评估它们的偏见对于实现公正和准确性至关重要。本文提出通过地理视角研究LLMs对我们所生活的世界的认知。这种方法特别强大，因为对人类生活中诸多与地理空间相关的方面（如文化、种族、语言、政治和宗教）有着明显的真实性。我们展示了各种问题地理偏见，我们将其定义为地理空间预测中的系统错误。首先，我们证明LLMs能够进行精确的零射击地理空间预测，以评级的形式呈现，其与真实情况之间呈现出强烈的单调相关性（Spearman's ρ最高可达0.89）。然后，我们展示了LLMs在多个客观和子领域上表现出共同的偏见。

    Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and sub
    
[^212]: 使用因果发现解释黑盒机器学习模型的反事实解释与信誉评级应用

    Counterfactual Explanations of Black-box Machine Learning Models using Causal Discovery with Applications to Credit Rating

    [https://arxiv.org/abs/2402.02678](https://arxiv.org/abs/2402.02678)

    本研究提出了一种新的可解释人工智能框架，通过使用反事实概率和额外的因果结构先验信息，克服了因果图未知的问题，可以解释黑盒机器学习模型并应用于信贷评级等领域。

    

    可解释的人工智能（XAI）有助于阐明机器学习算法的内部机制，通过展示其预测基础来增强其可靠性。几种XAI模型考虑使用因果关系来解释模型，通过研究预测模型的输入输出关系和特征之间的依赖关系。这些模型大多基于反事实概率来解释，并假设因果图已知。然而，这种假设增加了这些模型在实际数据应用中的复杂性，因为大多数情况下特征之间的因果关系是未知的。因此，本研究提出了一种新颖的XAI框架，放宽了因果图已知的约束。该框架利用反事实概率和关于因果结构的额外先验信息，通过因果发现方法估计出的因果图与黑盒分类模型进行集成。

    Explainable artificial intelligence (XAI) has helped elucidate the internal mechanisms of machine learning algorithms, bolstering their reliability by demonstrating the basis of their predictions. Several XAI models consider causal relationships to explain models by examining the input-output relationships of prediction models and the dependencies between features. The majority of these models have been based their explanations on counterfactual probabilities, assuming that the causal graph is known. However, this assumption complicates the application of such models to real data, given that the causal relationships between features are unknown in most cases. Thus, this study proposed a novel XAI framework that relaxed the constraint that the causal graph is known. This framework leveraged counterfactual probabilities and additional prior information on causal structure, facilitating the integration of a causal graph estimated through causal discovery methods and a black-box classifica
    
[^213]: 使用zkSNARKs进行可验证的机器学习模型评估

    Verifiable evaluations of machine learning models using zkSNARKs

    [https://arxiv.org/abs/2402.02675](https://arxiv.org/abs/2402.02675)

    本论文提出了一种使用zkSNARKs进行机器学习模型评估的方法。通过模型推理和零知识计算证明，可以提供可验证的评估证明，验证模型在公开输入上的性能和公平性指标。这有助于解决闭源商业机器学习模型评估可信性的问题。

    

    在越来越多闭源商业机器学习模型的世界中，开发者的模型评估必须被当作面值接受。这些评估结果，无论是任务准确性、偏差评估还是安全检查，传统上无法通过重新执行黑箱模型输出的基准测试来进行验证。本研究提出了一种使用zkSNARKs进行可验证模型评估的方法。通过zkSNARKs进行模型推理，得到的模型输出的零知识计算证明可以打包成可验证的评估证明，显示具有固定私有权重的模型在公开输入上达到了所述的性能或公平性指标。这些可验证的评估证明可以在任何标准神经网络模型上进行，计算要求各不相同。我们首次在一系列真实模型上展示了这一点，并突出了关键挑战和设计解决方案。

    In a world of increasing closed-source commercial machine learning models, model evaluations from developers must be taken at face value. These benchmark results, whether over task accuracy, bias evaluations, or safety checks, are traditionally impossible to verify by a model end-user without the costly or impossible process of re-performing the benchmark on black-box model outputs. This work presents a method of verifiable model evaluation using model inference through zkSNARKs. The resulting zero-knowledge computational proofs of model outputs over datasets can be packaged into verifiable evaluation attestations showing that models with fixed private weights achieve stated performance or fairness metrics over public inputs. These verifiable attestations can be performed on any standard neural network model with varying compute requirements. For the first time, we demonstrate this across a sample of real-world models and highlight key challenges and design solutions. This presents a n
    
[^214]: 对分布式数据的条件平均治疗效果估计：一种保护隐私的方法

    Estimation of conditional average treatment effects on distributed data: A privacy-preserving approach

    [https://arxiv.org/abs/2402.02672](https://arxiv.org/abs/2402.02672)

    本论文提出了一种数据协作双机器学习（DC-DML）方法，该方法可以在保护分布式数据隐私的情况下估计条件平均治疗效果（CATE）模型。通过数值实验验证了该方法的有效性。该方法的三个主要贡献是：实现了对分布式数据上的非迭代通信的半参数CATE模型的估计和测试，提高了模型的鲁棒性。

    

    在医学和社会科学等各个领域中，对条件平均治疗效果（CATEs）的估计是一个重要的课题。如果分布在多个参与方之间的数据可以集中，可以对CATEs进行高精度的估计。然而，如果这些数据包含隐私信息，则很难进行数据聚合。为了解决这个问题，我们提出了数据协作双机器学习（DC-DML）方法，该方法可以在保护分布式数据隐私的情况下估计CATE模型，并通过数值实验对该方法进行了评估。我们的贡献总结如下三点。首先，我们的方法能够在分布式数据上进行非迭代通信的半参数CATE模型的估计和测试。半参数或非参数的CATE模型能够比参数模型更稳健地进行估计和测试，对于模型偏差的鲁棒性更强。然而，据我们所知，目前还没有提出有效的通信方法来估计和测试这些模型。

    Estimation of conditional average treatment effects (CATEs) is an important topic in various fields such as medical and social sciences. CATEs can be estimated with high accuracy if distributed data across multiple parties can be centralized. However, it is difficult to aggregate such data if they contain privacy information. To address this issue, we proposed data collaboration double machine learning (DC-DML), a method that can estimate CATE models with privacy preservation of distributed data, and evaluated the method through numerical experiments. Our contributions are summarized in the following three points. First, our method enables estimation and testing of semi-parametric CATE models without iterative communication on distributed data. Semi-parametric or non-parametric CATE models enable estimation and testing that is more robust to model mis-specification than parametric models. However, to our knowledge, no communication-efficient method has been proposed for estimating and 
    
[^215]: 基于效用的强化学习：统一单目标和多目标强化学习

    Utility-Based Reinforcement Learning: Unifying Single-objective and Multi-objective Reinforcement Learning

    [https://arxiv.org/abs/2402.02665](https://arxiv.org/abs/2402.02665)

    基于效用的强化学习范式将多目标强化学习引入到单目标强化学习中，具有多个潜在益处，并探讨了算法意义。

    

    多目标强化学习（MORL）的研究引入了基于效用的范式，该范式利用环境奖励和定义用户从这些奖励中获得的效用的函数。本文将这种范式扩展到单目标强化学习（RL）的背景下，并概述了多个潜在益处，包括能够在不确定目标相关的任务之间进行多策略学习、风险感知的强化学习、折扣和安全强化学习。我们还探讨了采用基于效用的方法的算法意义。

    Research in multi-objective reinforcement learning (MORL) has introduced the utility-based paradigm, which makes use of both environmental rewards and a function that defines the utility derived by the user from those rewards. In this paper we extend this paradigm to the context of single-objective reinforcement learning (RL), and outline multiple potential benefits including the ability to perform multi-policy learning across tasks relating to uncertain objectives, risk-aware RL, discounting, and safe RL. We also examine the algorithmic implications of adopting a utility-based approach.
    
[^216]: 计算上的公正并非人口统计数据的平等，以及其他观察

    Counterfactual Fairness Is Not Demographic Parity, and Other Observations

    [https://arxiv.org/abs/2402.02663](https://arxiv.org/abs/2402.02663)

    这里是中文总结出的一句话要点：文章探讨了因果概念与纯粹概率概念之间的等价性，并发现计算上的公正并不等同于人口统计数据的平等。同时还纠正了一些有关计算上的公正的误解。

    

    需要谨慎对待在因果概念与纯粹概率概念之间进行等价性的断言。在本简短的文章中，我对最近一个声称计算上的公正等同于人口统计数据的平等的主张进行了审查。仔细研究后发现该主张不成立。我将借此机会解决一些关于计算上的公正的更广泛误解。

    Blanket statements of equivalence between causal concepts and purely probabilistic concepts should be approached with care. In this short note, I examine a recent claim that counterfactual fairness is equivalent to demographic parity. The claim fails to hold up upon closer examination. I will take the opportunity to address some broader misunderstandings about counterfactual fairness.
    
[^217]: 提升零样本泛化能力的图像-字幕编码方法

    Image-Caption Encoding for Improving Zero-Shot Generalization

    [https://arxiv.org/abs/2402.02662](https://arxiv.org/abs/2402.02662)

    本研究提出了一种名为图像-字幕编码（ICE）的方法，通过在评估时对图像和字幕条件下的预测进行一致性约束，来改善图像分类模型的分布外泛化能力。

    

    最近，视觉-语言模型在对比方法与生成方法相结合的基础上，在零样本图像分类等下游推断任务上取得了最先进的结果。然而，图像分类模型的一个持续存在的问题是其在分布外泛化能力的不足。我们首先展示了当一个分布外数据点被错误分类时，正确类别通常可以在前K个预测类别中找到。为了将模型预测导向前K个预测类别中的正确类别，我们提出了图像-字幕编码（ICE）方法，这是一种直接在评估时只在图像条件和字幕条件下进行一致性约束的简单方法。直观地说，我们利用生成的字幕的独特属性来指导我们在前K个预测类别中寻找正确类别标签的局部搜索。我们展示了我们的方法可以与其他方法轻松结合。

    Recent advances in vision-language models have combined contrastive approaches with generative methods to achieve state-of-the-art (SOTA) on downstream inference tasks like zero-shot image classification. However, a persistent issue of these models for image classification is their out-of-distribution (OOD) generalization capabilities. We first show that when an OOD data point is misclassified, the correct class can be typically found in the Top-K predicted classes. In order to steer the model prediction toward the correct class within the top predicted classes, we propose the Image-Caption Encoding (ICE) method, a straightforward approach that directly enforces consistency between the image-conditioned and caption-conditioned predictions at evaluation time only. Intuitively, we take advantage of unique properties of the generated captions to guide our local search for the correct class label within the Top-K predicted classes. We show that our method can be easily combined with other 
    
[^218]: 多步问题求解中的验证器：关于模型引导的过程监督的实证分析

    Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision

    [https://arxiv.org/abs/2402.02658](https://arxiv.org/abs/2402.02658)

    本文介绍了一种名为模型引导的过程监督（MiPS）的新方法，该方法通过对推理模型的解决方案进行抽样完成来自动进行数据整理，从而避免了昂贵的人工注释。实证结果表明，与之前的工作相反，我们应优先选择验证器预测得分高的验证。这种方法显著改进了PaLM 2在数学和编码任务上的性能。

    

    过程监督使用训练好的验证器来评估推理器生成的中间步骤，已经在多步问题求解中展示出了显著的改进。在本文中，为了避免在验证器训练数据上进行昂贵的人工注释，我们引入了模型引导的过程监督（MiPS），这是一种自动化数据整理的新方法。MiPS通过对推理模型的解决方案进行抽样完成，并获得一个准确率，其中准确完成的比例定义为准确率。推理器中的错误会导致MiPS低估中间步骤的准确率，因此，我们建议并通过实验证明，与之前的工作相反，应优先选择验证器预测得分高的验证，而不是低的。我们的方法显著提高了PaLM 2在数学和编码任务上的性能（GSM8K上的准确率+0.67％，数学上的准确率+4.16％，MBPP上的准确率+0.92％与输出s相比。）

    Process supervision, using a trained verifier to evaluate the intermediate steps generated by reasoner, has demonstrated significant improvements in multi-step problem solving. In this paper, to avoid expensive human annotation effort on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation. MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions. Errors in the reasoner would cause MiPS to underestimate the accuracy of intermediate steps, therefore, we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior work. Our approach significantly improves the performance of PaLM 2 on math and coding tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with an output s
    
[^219]: 使用原型的混合学习方法进行外部分布检测

    Learning with Mixture of Prototypes for Out-of-Distribution Detection

    [https://arxiv.org/abs/2402.02653](https://arxiv.org/abs/2402.02653)

    本文提出了一种使用多个原型进行模型学习的方法，用于外部分布的检测任务。通过这种方法，可以更好地捕捉数据中的多样性，并提高模型的性能。

    

    外部分布（OOD）检测旨在检测远离内部分布（ID）训练数据的测试样本，这对于在真实世界中安全部署机器学习模型至关重要。基于距离的OOD检测方法通过测量样本与ID类别中心或原型的距离来识别未知OOD样本。然而，现有方法在学习表示时依赖于过于简化的数据假设，例如，用一个中心类原型建模每个类的ID数据，或者使用不适用于OOD检测的损失函数，忽视了数据中的自然多样性。将每个类别的数据样本强制紧凑地围绕一个原型进行建模会导致对真实数据建模不够充分，性能受限。为解决这些问题，我们提出了一种使用多个原型模型的原型学习与混合（PALM）方法。

    Out-of-distribution (OOD) detection aims to detect testing samples far away from the in-distribution (ID) training data, which is crucial for the safe deployment of machine learning models in the real world. Distance-based OOD detection methods have emerged with enhanced deep representation learning. They identify unseen OOD samples by measuring their distances from ID class centroids or prototypes. However, existing approaches learn the representation relying on oversimplified data assumptions, e.g, modeling ID data of each class with one centroid class prototype or using loss functions not designed for OOD detection, which overlook the natural diversities within the data. Naively enforcing data samples of each class to be compact around only one prototype leads to inadequate modeling of realistic data and limited performance. To tackle these issues, we propose PrototypicAl Learning with a Mixture of prototypes (PALM) which models each class with multiple prototypes to capture the sam
    
[^220]: 视觉-语言模型为强化学习提供可提示的表示

    Vision-Language Models Provide Promptable Representations for Reinforcement Learning

    [https://arxiv.org/abs/2402.02651](https://arxiv.org/abs/2402.02651)

    本论文提出一种利用预训练的视觉-语言模型作为可提示的表示，为强化学习提供世界知识，使得代理能够更快地学习新的行为。在实验中，我们发现使用这种表示训练的策略在复杂环境下表现更好，优于通用图像表示和遵循指示的方法。

    

    人类可以通过利用背景世界知识快速学习新的行为。相比之下，利用强化学习训练的代理通常需要从零开始学习行为。因此，我们提出了一种新的方法，利用在互联网规模数据上预训练的视觉-语言模型（VLMs）中编码的大量通用和可索引的世界知识来进行具象的强化学习。我们通过将VLMs用作可提示表示来初始化策略：这些嵌入在视觉观察中具有基础，并根据VLM的内部知识编码语义特征，通过提供任务上下文和辅助信息来触发。我们在Minecraft和Habitat中的视觉复杂、长期的强化学习任务上评估了我们的方法。我们发现，使用通用型VLMs提取的嵌入训练的策略胜过使用通用的、不可提示的图像嵌入训练的策略。我们还发现我们的方法胜过遵循指示的元策略。

    Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following met
    
[^221]: 通过状态扩展和随机排列的方法进行变分DAG估计

    Variational DAG Estimation via State Augmentation With Stochastic Permutations

    [https://arxiv.org/abs/2402.02644](https://arxiv.org/abs/2402.02644)

    使用状态扩展和随机排列进行变分DAG估计的方法可以超越竞争的贝叶斯和非贝叶斯基准方法，从而在估计贝叶斯网络结构方面取得更好的性能。

    

    从观测数据中估计贝叶斯网络的结构，即有向无环图（DAG），是一个在统计和计算上都很困难的问题，在因果发现等领域有着重要应用。贝叶斯方法在解决这个任务方面是一个有希望的方向，因为它们允许进行不确定性量化，并处理众所周知的可识别性问题。从概率推断的角度来看，主要的挑战是（i）表示满足DAG约束的图的分布和（ii）估计底层组合空间的后验概率。我们提出了一种方法，通过在DAG和排列的扩展空间上构建联合分布来解决这些挑战。我们通过变分推断进行后验估计，在其中利用了离散分布的连续松弛。我们展示了我们的方法在一系列合成和实际数据上能够超越竞争的贝叶斯和非贝叶斯基准方法。

    Estimating the structure of a Bayesian network, in the form of a directed acyclic graph (DAG), from observational data is a statistically and computationally hard problem with essential applications in areas such as causal discovery. Bayesian approaches are a promising direction for solving this task, as they allow for uncertainty quantification and deal with well-known identifiability issues. From a probabilistic inference perspective, the main challenges are (i) representing distributions over graphs that satisfy the DAG constraint and (ii) estimating a posterior over the underlying combinatorial space. We propose an approach that addresses these challenges by formulating a joint distribution on an augmented space of DAGs and permutations. We carry out posterior estimation via variational inference, where we exploit continuous relaxations of discrete distributions. We show that our approach can outperform competitive Bayesian and non-Bayesian benchmarks on a range of synthetic and re
    
[^222]: LLM增强型数据管理

    LLM-Enhanced Data Management

    [https://arxiv.org/abs/2402.02643](https://arxiv.org/abs/2402.02643)

    LLMDB是一种LLM增强的数据管理范式，通过LLM的微调和提示工程嵌入领域特定知识，解决了虚构、高成本和低准确性的挑战，并实现了泛化能力和高推理能力。

    

    近年来，针对优化数据管理问题的机器学习（ML）技术得到了广泛研究和广泛部署。然而，传统的ML方法在泛化能力（适应不同情景）和推理能力（理解上下文）方面存在局限性。幸运的是，大型语言模型（LLMs）表现出高度的泛化能力和人类竞争能力，对于数据管理任务（如数据库诊断、数据库调优）具有潜在的应用前景。然而，现有的LLMs存在一些限制：虚构、高成本以及对复杂任务的低准确性。为了应对这些挑战，我们设计了LLMDB，一种使用LLM增强的数据管理范式，具有良好的泛化能力和高推理能力，同时避免了虚构，降低了LLM成本，提高了准确性。LLMDB通过LLM的微调和提示工程嵌入领域特定知识以避免虚构。LLMDB通过减少LLMs的高成本 addresses challenges: hallucination, high cost, low accuracy-

    Machine learning (ML) techniques for optimizing data management problems have been extensively studied and widely deployed in recent five years. However traditional ML methods have limitations on generalizability (adapting to different scenarios) and inference ability (understanding the context). Fortunately, large language models (LLMs) have shown high generalizability and human-competitive abilities in understanding context, which are promising for data management tasks (e.g., database diagnosis, database tuning). However, existing LLMs have several limitations: hallucination, high cost, and low accuracy for complicated tasks. To address these challenges, we design LLMDB, an LLM-enhanced data management paradigm which has generalizability and high inference ability while avoiding hallucination, reducing LLM cost, and achieving high accuracy. LLMDB embeds domain-specific knowledge to avoid hallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high cost of LLMs by 
    
[^223]: $C^*$-代数机器学习：迈向新的方向

    $C^*$-Algebraic Machine Learning: Moving in a New Direction

    [https://arxiv.org/abs/2402.02637](https://arxiv.org/abs/2402.02637)

    $C^*$-代数机器学习是将$C^*$-代数与机器学习结合的新研究方向，它通过统一现有的学习策略，并构建更多元化和信息丰富的数据模型的新框架，为机器学习提供了一种新的方法。

    

    机器学习与数学的几个领域（如统计学、概率论和线性代数）有着长期的合作传统。我们提出了机器学习研究的一个新方向：$C^*$-代数机器学习，这是$C^*$-代数和机器学习之间的交流和相互滋养。$C^*$-代数是复数空间的自然推广的数学概念，它使我们能够统一现有的学习策略，并构建一个更多元化和信息丰富的数据模型的新框架。我们解释了在机器学习中使用$C^*$-代数的原因和方法，并提供了在核方法和神经网络背景下设计$C^*$-代数学习模型的技术考虑。此外，我们讨论了$C^*$-代数机器学习中的开放问题和挑战，并提出了我们对未来发展和应用的思考。

    Machine learning has a long collaborative tradition with several fields of mathematics, such as statistics, probability and linear algebra. We propose a new direction for machine learning research: $C^*$-algebraic ML $-$ a cross-fertilization between $C^*$-algebra and machine learning. The mathematical concept of $C^*$-algebra is a natural generalization of the space of complex numbers. It enables us to unify existing learning strategies, and construct a new framework for more diverse and information-rich data models. We explain why and how to use $C^*$-algebras in machine learning, and provide technical considerations that go into the design of $C^*$-algebraic learning models in the contexts of kernel methods and neural networks. Furthermore, we discuss open questions and challenges in $C^*$-algebraic ML and give our thoughts for future development and applications.
    
[^224]: 大型语言模型能否学习独立的因果机制？

    Can Large Language Models Learn Independent Causal Mechanisms?

    [https://arxiv.org/abs/2402.02636](https://arxiv.org/abs/2402.02636)

    本论文研究在大型语言模型中学习独立因果机制的方法，以增强模型在分布变化下的鲁棒性和泛化能力。

    

    尽管大型语言模型（LLMs）在语言建模和复杂推理任务中表现出色，但在不常见的环境设置或分布变化的任务中，LLMs的泛化能力仍然不足。目前通常通过增加训练数据来缓解这个问题。然而，这种方法是脆弱的，因为任务的范围可能无法预测或可能会发生变化，并且使用新数据更新模型通常需要大量的额外训练。相反，那些学习抽象变量和因果关系的系统，如因果模型，可以表现出对分布变化的更强稳健性。其中一个原因是存在并使用独立因果机制（ICMs），表示只稀疏交互的高层概念。在这项工作中，我们应用因果性的两个概念，在LLMs中学习ICMs。我们开发了一个由多个稀疏交互的语言模型组成的新LLM架构。

    Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability. This issue has usually been alleviated by feeding more training data into the LLM. However, this method is brittle, as the scope of tasks may not be readily predictable or may evolve, and updating the model with new data generally requires extensive additional training. By contrast, systems, such as causal models, that learn abstract variables and causal relationships can demonstrate increased robustness against changes in the distribution. One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact. In this work, we apply two concepts from causality to learn ICMs within LLMs. We develop a new LLM architecture composed of multiple sparsely interacting language
    
[^225]: 图像修复的关键图变换器

    Key-Graph Transformer for Image Restoration

    [https://arxiv.org/abs/2402.02634](https://arxiv.org/abs/2402.02634)

    我们提出了一种关键图变换器（KGT）用于高效地进行图像修复，通过选择性地连接关键节点，减少了计算负担，并且在6个图像修复任务上展示了最先进的性能。

    

    在有效的图像修复中捕捉全局信息至关重要，但将这些提示集成到基于变换器的方法中会导致计算负担加重，尤其是在高输入分辨率下。此外，变换器中的自注意机制容易考虑与无关对象或区域的不必要全局提示，从而引入计算效率低下的问题。为了应对这些挑战，我们在本文中引入了关键图变换器（KGT）。具体而言，KGT将图像块特征视为图节点。所提出的关键图构造器通过仅选择关键节点而不是所有节点来高效地形成稀疏但具有代表性的关键图。然后，在每个窗口内，只在所选择节点之间引导关键图的指导下进行关键图注意操作，其计算复杂性线性。对6个图像修复任务的大量实验验证了所提出的KGT在性能上的领先地位，展示了双方面的进展。

    While it is crucial to capture global information for effective image restoration (IR), integrating such cues into transformer-based methods becomes computationally expensive, especially with high input resolution. Furthermore, the self-attention mechanism in transformers is prone to considering unnecessary global cues from unrelated objects or regions, introducing computational inefficiencies. In response to these challenges, we introduce the Key-Graph Transformer (KGT) in this paper. Specifically, KGT views patch features as graph nodes. The proposed Key-Graph Constructor efficiently forms a sparse yet representative Key-Graph by selectively connecting essential nodes instead of all the nodes. Then the proposed Key-Graph Attention is conducted under the guidance of the Key-Graph only among selected nodes with linear computational complexity within each window. Extensive experiments across 6 IR tasks confirm the proposed KGT's state-of-the-art performance, showcasing advancements both
    
[^226]: 预测低资源语言机器翻译性能：领域相似性的作用

    Predicting Machine Translation Performance on Low-Resource Languages: The Role of Domain Similarity

    [https://arxiv.org/abs/2402.02633](https://arxiv.org/abs/2402.02633)

    该论文研究了预测低资源语言机器翻译性能的关键因素，发现领域相似性对于预测模型性能具有最重要的影响。

    

    对于低资源语言（LRLs），细调和测试多语言大型语言模型是昂贵且具有挑战性的。然而，先前的研究主要关注高资源语言，忽视了LRLs和跨领域的转变。针对LRLs，我们研究了三个因素：细调语料库的大小，细调语料库与测试语料库之间的领域相似性，以及源语言和目标语言之间的语言相似性。我们采用经典回归模型评估这些因素对模型性能的影响。我们的结果表明领域相似性对于预测机器翻译模型的性能具有最重要的影响。

    Fine-tuning and testing a multilingual large language model is expensive and challenging for low-resource languages (LRLs). While previous studies have predicted the performance of natural language processing (NLP) tasks using machine learning methods, they primarily focus on high-resource languages, overlooking LRLs and shifts across domains. Focusing on LRLs, we investigate three factors: the size of the fine-tuning corpus, the domain similarity between fine-tuning and testing corpora, and the language similarity between source and target languages. We employ classical regression models to assess how these factors impact the model's performance. Our results indicate that domain similarity has the most critical impact on predicting the performance of Machine Translation models.
    
[^227]: 学习理解：通过Mobius变换识别相互作用

    Learning to Understand: Identifying Interactions via the Mobius Transform

    [https://arxiv.org/abs/2402.02631](https://arxiv.org/abs/2402.02631)

    本文研究了通过Mobius变换识别相互作用的问题，提出了一种算法可以在非零系数较少的情况下精确恢复Mobius变换，并揭示了群体理论和信息论之间的联系。

    

    机器学习中最基本的问题之一是找到我们学习的函数的可解释表示。Mobius变换是一个有用的工具，因为它的系数对应于输入变量集合上的唯一重要性得分。Mobius变换与Shapley值的概念密切相关（在某些情况下是等价的），后者是一种广泛使用的博弈论重要性概念。本文关注的是在$n$个输入之间的所有$2^n$个可能交互之中，非零Mobius系数（和因此输入之间的相互作用）的比例小于非零系数总数的（典型）情况。当有$K = O(2^{n \delta})$个，其中$\delta \leq \frac{1}{3}$的非零系数以均匀随机方式选择时，我们的算法可以在$O(Kn)$个样本和$O(Kn^2)$的时间内完全恢复Mobius变换，并且随着$K \rightarrow \infty$，误差趋于零。这是第一个非自适应算法实现这一点。我们还发现了群体理论和信息论的令人惊讶的联系。

    One of the most fundamental problems in machine learning is finding interpretable representations of the functions we learn. The Mobius transform is a useful tool for this because its coefficients correspond to unique importance scores on sets of input variables. The Mobius Transform is strongly related (and in some cases equivalent) to the concept of Shapley value, which is a widely used game-theoretic notion of importance. This work focuses on the (typical) regime where the fraction of non-zero Mobius coefficients (and thus interactions between inputs) is small compared to the set of all $2^n$ possible interactions between $n$ inputs. When there are $K = O(2^{n \delta})$ with $\delta \leq \frac{1}{3}$ non-zero coefficients chosen uniformly at random, our algorithm exactly recovers the Mobius transform in $O(Kn)$ samples and $O(Kn^2)$ time with vanishing error as $K \rightarrow \infty$, the first non-adaptive algorithm to do so. We also uncover a surprising connection between group te
    
[^228]: 对多种基于循环神经网络的符号规则提取方法的稳定性分析

    Stability Analysis of Various Symbolic Rule Extraction Methods from Recurrent Neural Network

    [https://arxiv.org/abs/2402.02627](https://arxiv.org/abs/2402.02627)

    本文分析了基于循环神经网络的两种规则提取方法：量化和等价查询。实验结果表明，O2RNN和基于量化的规则提取方法在稳定性和性能方面优于其他方法。

    

    本文分析了两种竞争的规则提取方法：量化和等价查询。我们使用量化方法（k-means和SOM）从3600个RNN模型中提取了18000个DFA，使用等价查询（$L^{*}$）方法从3600个DFA模型中提取了10个初始化种子。我们从7个Tomita语法和4个Dyck语法中抽样得到了数据集，并使用4个RNN单元（LSTM、GRU、O2RNN和MIRNN）进行训练。我们的实验观察结果表明，O2RNN和基于量化的规则提取方法表现优于其他方法。当神经网络完全训练时，$L^{*}$在Tomita语言上的表现与量化方法类似。然而，对于部分训练的RNN，$L^{*}$在DFA状态数量上表现不稳定，例如对于Tomita 5和Tomita 6语言，$L^{*}$产生了超过100个状态的结果。相反，量化方法的规则状态数量非常接近真实的DFA。在RNN中，O2RNN和quantization-based规则提取方法表现出色。

    This paper analyzes two competing rule extraction methodologies: quantization and equivalence query. We trained $3600$ RNN models, extracting $18000$ DFA with a quantization approach (k-means and SOM) and $3600$ DFA by equivalence query($L^{*}$) methods across $10$ initialization seeds. We sampled the datasets from $7$ Tomita and $4$ Dyck grammars and trained them on $4$ RNN cells: LSTM, GRU, O2RNN, and MIRNN. The observations from our experiments establish the superior performance of O2RNN and quantization-based rule extraction over others. $L^{*}$, primarily proposed for regular grammars, performs similarly to quantization methods for Tomita languages when neural networks are perfectly trained. However, for partially trained RNNs, $L^{*}$ shows instability in the number of states in DFA, e.g., for Tomita 5 and Tomita 6 languages, $L^{*}$ produced more than $100$ states. In contrast, quantization methods result in rules with number of states very close to ground truth DFA. Among RNN c
    
[^229]: 特征中的位置偏差

    Position bias in features

    [https://arxiv.org/abs/2402.02626](https://arxiv.org/abs/2402.02626)

    本文提出了一种用于搜索引擎的动态排名系统的特征，该特征可以准确估计文档相关性，但是在存在位置偏差的情况下会有高方差，并强调了准确估计位置偏差的必要性，建议同时使用有偏和无偏的位置偏差特征。

    

    搜索引擎中建模文件相关性的目的是在后续搜索中更好地排名。文档特定的历史点击率可以作为动态排名系统中的重要特征，随着我们积累更多样本，系统会进行更新。本文描述了几种这样的特征的属性，并在控制实验中对其进行了测试。将反向倾向加权方法扩展到文档上可以产生对文档相关性的无偏估计。这个特征可以准确地近似相关性，在理想情况下可以实现接近最优的排名。然而，它具有高方差，且随着位置偏差的程度增加而增加。此外，不准确的位置偏差估计会导致性能不佳。在几种情况下，这个特征的表现可能不如偏倚的点击率。本文强调了准确的位置偏差估计的必要性，并独特地建议同时使用有偏和无偏的位置偏差特征。

    The purpose of modeling document relevance for search engines is to rank better in subsequent searches. Document-specific historical click-through rates can be important features in a dynamic ranking system which updates as we accumulate more sample. This paper describes the properties of several such features, and tests them in controlled experiments. Extending the inverse propensity weighting method to documents creates an unbiased estimate of document relevance. This feature can approximate relevance accurately, leading to near-optimal ranking in ideal circumstances. However, it has high variance that is increasing with respect to the degree of position bias. Furthermore, inaccurate position bias estimation leads to poor performance. Under several scenarios this feature can perform worse than biased click-through rates. This paper underscores the need for accurate position bias estimation, and is unique in suggesting simultaneous use of biased and unbiased position bias features.
    
[^230]: 用多个时间视角增强Transformer RNNs

    Enhancing Transformer RNNs with Multiple Temporal Perspectives

    [https://arxiv.org/abs/2402.02625](https://arxiv.org/abs/2402.02625)

    引入了多个时间视角的概念，用于增强Transformer RNNs对顺序数据的理解能力，在参数数量最小增加的情况下取得了显著的改进。

    

    我们引入了多个时间视角的概念，这是一种适用于循环神经网络（RNN）架构的新方法，用于增强其对顺序数据的理解。该方法涉及维护先前遇到的文本的多样时间视图，显著丰富了语言模型解释上下文的能力。为了展示这种方法的有效性，我们将其纳入了Receptance Weighted Key Value（RWKV）架构，解决了该架构在单个隐藏状态中保留所有历史信息的固有挑战。值得注意的是，即使参数数量增加最少（仅为最初参数数量的0.04%），也实现了此改进。此外，多个时间视角所需的额外参数经过微小的计算开销进行微调，避免了完全预训练的需要。由此产生的模型在提示推断过程中保持了线性的计算复杂度。

    We introduce the concept of multiple temporal perspectives, a novel approach applicable to Recurrent Neural Network (RNN) architectures for enhancing their understanding of sequential data. This method involves maintaining diverse temporal views of previously encountered text, significantly enriching the language models' capacity to interpret context. To show the efficacy of this approach, we incorporate it into the Receptance Weighted Key Value (RWKV) architecture, addressing its inherent challenge of retaining all historical information within a single hidden state. Notably, this improvement is achieved with a minimal increase in the number of parameters --even as little as $0.04\%$ of the original number of parameters. Further, the additional parameters necessary for the multiple temporal perspectives are fine-tuned with minimal computational overhead, avoiding the need for a full pre-training. The resulting model maintains linear computational complexity during prompt inference, en
    
[^231]: 一种安全的强化学习驱动的权重变化模型预测控制用于自动驾驶车辆运动控制

    A Safe Reinforcement Learning driven Weights-varying Model Predictive Control for Autonomous Vehicle Motion Control

    [https://arxiv.org/abs/2402.02624](https://arxiv.org/abs/2402.02624)

    提出了一种安全的强化学习驱动的权重变化模型预测控制方法，用于自动驾驶车辆运动控制。该方法通过在运行时动态调整代价函数权重，可以提供上下文相关的最优闭环控制性能。

    

    确定模型预测控制(MPC)的最佳代价函数参数以优化多个控制目标是一项具有挑战性且耗时的任务。多目标贝叶斯优化(BO)技术通过确定MPC的 Pareto 最优参数集解决了这个问题，然而，当MPC的操作条件上下文在其运行过程中发生变化时，单个参数集可能无法提供最优的闭环控制性能，因此需要在运行时调整代价函数权重。深度强化学习(RL)算法能够自动学习上下文相关的最优参数集，并在权重变化的MPC中进行动态调整。然而，在连续动作空间中从头学习代价函数权重可能导致不安全的操作状态。为了解决这个问题，我们提出了一种新方法，将RL的动作限制在安全学习空间内，该空间表示经过预优化的BO Pareto最优权重的目录。

    Determining the optimal cost function parameters of Model Predictive Control (MPC) to optimize multiple control objectives is a challenging and time-consuming task. Multiobjective Bayesian Optimization (BO) techniques solve this problem by determining a Pareto optimal parameter set for an MPC with static weights. However, a single parameter set may not deliver the most optimal closed-loop control performance when the context of the MPC operating conditions changes during its operation, urging the need to adapt the cost function weights at runtime. Deep Reinforcement Learning (RL) algorithms can automatically learn context-dependent optimal parameter sets and dynamically adapt for a Weightsvarying MPC (WMPC). However, learning cost function weights from scratch in a continuous action space may lead to unsafe operating states. To solve this, we propose a novel approach limiting the RL actions within a safe learning space representing a catalog of pre-optimized BO Pareto-optimal weight se
    
[^232]: DenseFormer: 通过深度加权平均增强Transformer中的信息流

    DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging

    [https://arxiv.org/abs/2402.02622](https://arxiv.org/abs/2402.02622)

    DenseFormer是对Transformer的简单修改，通过在每个transformer块之后进行深度加权平均，提高了模型的困惑度。学到的加权平均权重揭示了信息流的连贯模式，使得DenseFormer具有更高的数据效率，并且在相同困惑度下胜过传统的Transformer模型。

    

    从Vaswani等人（2017）的Transformer架构现已普遍应用于各个应用领域，从自然语言处理到语音处理和图像理解。我们提出了DenseFormer，这是对标准架构的简单修改，提高了模型的困惑度，而不增加其大小-对于拥有100B参数范围的大规模模型，只需添加几千个参数。我们的方法在每个transformer块之后依靠额外的平均步骤，计算当前和过去表示的加权平均-我们将这个操作称为深度加权平均（DWA）。学到的DWA权重展现了信息流的连贯模式，揭示了来自远层的激活的强大且结构化的重复使用。实验证明DenseFormer具有更高的数据效率，能够达到比更深的transformer模型相同的困惑度，并且在相同困惑度下，这些新模型在性能上超过了transformer基准模型。

    The transformer architecture from Vaswani et al. (2017) is now ubiquitous across application domains, from natural language processing to speech processing and image understanding. We propose DenseFormer, a simple modification to the standard architecture that improves the perplexity of the model without increasing its size -- adding a few thousand parameters for large-scale models in the 100B parameters range. Our approach relies on an additional averaging step after each transformer block, which computes a weighted average of current and past representations -- we refer to this operation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit coherent patterns of information flow, revealing the strong and structured reuse of activations from distant layers. Experiments demonstrate that DenseFormer is more data efficient, reaching the same perplexity of much deeper transformer models, and that for the same perplexity, these new models outperform transformer baselines in terms
    
[^233]: 通过重复使用经过验证的电路增加语言模型的可信度

    Increasing Trust in Language Models through the Reuse of Verified Circuits

    [https://arxiv.org/abs/2402.02619](https://arxiv.org/abs/2402.02619)

    本文介绍了一种通过重复使用经过验证的电路来增加语言模型的可信度的方法。研究者通过构建数学和逻辑规范的框架，并对一个n位整数加法模型进行完全验证。他们插入训练好的加法模型到一个未经训练的模型中，通过训练组合模型执行加法和减法。他们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了减法模型的验证。

    

    语言模型（LMs）在各种预测任务中的应用越来越广泛，但它们的训练经常忽略罕见的边界情况，降低了它们的可靠性。在本文中，我们定义了一个严格的可信度标准，即任务算法和电路实现必须经过验证，考虑到边界情况，并且没有已知的故障模式。我们展示了通过使用数学和逻辑规范的框架来构建变压器模型，可以训练出满足这一标准的模型。在本文中，我们对一个n位整数加法模型进行了完全验证。为了展示经过验证的模块的重复使用性，我们将训练好的整数加法模型插入到一个未经训练的模型中，并训练组合模型同时执行加法和减法。我们发现加法电路在这两个任务中得到了广泛的重复使用，从而简化了更复杂的减法模型的验证。我们讨论了如何将经过验证的任务模块插入到语言模型中，以利用模型的重复使用来提高可验证性和可信度。

    Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of languag
    
[^234]: 逆推强化学习中悲观主义的优势

    The Virtues of Pessimism in Inverse Reinforcement Learning

    [https://arxiv.org/abs/2402.02616](https://arxiv.org/abs/2402.02616)

    本论文提出了一种使用离线强化学习算法来加速逆推强化学习中强化学习子程序的替代方法，通过保持与专家数据分布接近的悲观主义策略，提高了逆推强化学习的样本效率。

    

    逆推强化学习（IRL）是从专家演示中学习复杂行为的强大框架。然而，传统上需要在内循环中反复解决计算成本昂贵的强化学习（RL）问题。通过利用专家演示来减少探索负担在内循环的RL中非常可取。例如，最近的工作通过将学习者重置到专家状态来指导学习者在高回报专家状态下工作。然而，这样的方法在真实世界中不可行。在这项工作中，我们考虑了一种加速IRL中RL子程序的替代方法：悲观主义，即保持与专家数据分布接近，通过离线RL算法来实现。我们在离线RL和IRL之间形成了一个连接，使我们能够使用任意离线RL算法来提高IRL的样本效率。我们通过实验证明了我们的理论，展示了样本效率和探索负担之间的强相关性。

    Inverse Reinforcement Learning (IRL) is a powerful framework for learning complex behaviors from expert demonstrations. However, it traditionally requires repeatedly solving a computationally expensive reinforcement learning (RL) problem in its inner loop. It is desirable to reduce the exploration burden by leveraging expert demonstrations in the inner-loop RL. As an example, recent work resets the learner to expert states in order to inform the learner of high-reward expert states. However, such an approach is infeasible in the real world. In this work, we consider an alternative approach to speeding up the RL subroutine in IRL: \emph{pessimism}, i.e., staying close to the expert's data distribution, instantiated via the use of offline RL algorithms. We formalize a connection between offline RL and IRL, enabling us to use an arbitrary offline RL algorithm to improve the sample efficiency of IRL. We validate our theory experimentally by demonstrating a strong correlation between the ef
    
[^235]: PuzzleBench：LLMs能否解决困难的一阶组合推理问题？

    PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?

    [https://arxiv.org/abs/2402.02611](https://arxiv.org/abs/2402.02611)

    本研究通过PuzzleBench数据集探索了LLMs解决困难的一阶组合推理问题的能力，并提出了Puzzle-LM方法，该方法将LLMs与符号求解器和程序解释器相结合，使其能够有效地推理这类问题。

    

    最近的研究探索了使用LLMs进行推理任务，重点是相对简单的问题，如逻辑问答。在我们的工作中，我们希望解决更复杂的问题，显著扩展这些模型的功能。特别是，我们探讨LLMs是否能够解决困难的一阶组合推理问题，一个例子是流行的数独谜题。这些问题有一个由自然语言描述的基础一阶结构，并且可以实例化为不同大小的实例。此外，这些问题在计算上是密集型的，需要多个推理步骤才能达到解决方案。我们提出了PuzzleBench，一个包含31个这样具有挑战性的谜题的数据集。我们观察到，即使在符号求解器的帮助下，LLMs在我们的基准测试中表现得相当糟糕。作为回应，我们提出了一种新的方法，Puzzle-LM，它将LLMs与符号求解器和程序解释器相结合，使它们能够推理这类问题。

    Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such
    
[^236]: 加速逆强化学习与专家引导

    Accelerating Inverse Reinforcement Learning with Expert Bootstrapping

    [https://arxiv.org/abs/2402.02608](https://arxiv.org/abs/2402.02608)

    本文提出两个简单的方法来加速逆强化学习，通过更好地使用专家示范，减少对内部强化学习循环中的硬探索的需求。这些方法在MaxEntIRL的基础上取得了显著的增益。

    

    现有的逆强化学习方法（例如MaxEntIRL，f-IRL）在候选奖励函数上进行搜索，并在内循环中解决强化学习问题。这造成了一个比较奇怪的倒置，一个更难的问题，强化学习，位于一个相对较易的问题，模仿学习的内循环中。在这项工作中，我们展示了更好地利用专家示范可以减少内部强化学习循环中对硬探索的需求，从而加速学习。具体而言，我们提出了两个简单的方法：（1）将专家转场放入内部强化学习算法（例如Soft-Actor Critic）的回放缓冲区中，直接向学习者提供高奖励状态的信息，而不是通过广泛的探索来发现它们，（2）在Q值引导中使用专家行为，以改进目标Q值估计并更准确地描述高价值的专家状态。我们的方法在MaxEntIRL的基础上取得了显著的增益。

    Existing inverse reinforcement learning methods (e.g. MaxEntIRL, $f$-IRL) search over candidate reward functions and solve a reinforcement learning problem in the inner loop. This creates a rather strange inversion where a harder problem, reinforcement learning, is in the inner loop of a presumably easier problem, imitation learning. In this work, we show that better utilization of expert demonstrations can reduce the need for hard exploration in the inner RL loop, hence accelerating learning. Specifically, we propose two simple recipes: (1) placing expert transitions into the replay buffer of the inner RL algorithm (e.g. Soft-Actor Critic) which directly informs the learner about high reward states instead of forcing the learner to discover them through extensive exploration, and (2) using expert actions in Q value bootstrapping in order to improve the target Q value estimates and more accurately describe high value expert states. Our methods show significant gains over a MaxEntIRL ba
    
[^237]: 绕过基于深度学习的恶意软件检测器的混淆方法：一种基于深度强化学习的方法

    Evading Deep Learning-Based Malware Detectors via Obfuscation: A Deep Reinforcement Learning Approach

    [https://arxiv.org/abs/2402.02600](https://arxiv.org/abs/2402.02600)

    该论文提出了一种基于深度强化学习的方法，通过使用开源加密工具和强化学习框架成功地混淆恶意软件，规避了最先进的恶意软件检测器，并且在规避率方面超越了使用高级修改方法的技术。

    

    对抗性恶意软件生成（AMG）是生成对抗性恶意软件变种以加强基于深度学习（DL）的恶意软件检测器的重要工具，它已成为主动式网络防御的关键技术。然而，现有的大多数方法仅提供对可执行文件的微小扰动或添加，并没有探索全文件混淆。在本研究中，我们展示了一个开源加密工具结合强化学习（RL）框架可以成功地混淆恶意软件以规避最先进的恶意软件检测引擎，并超越使用高级修改方法的技术。我们的结果表明，与广泛使用的最先进的基于强化学习的方法相比，所提出的方法将规避率提高了27%-49%。

    Adversarial Malware Generation (AMG), the gen- eration of adversarial malware variants to strengthen Deep Learning (DL)-based malware detectors has emerged as a crucial tool in the development of proactive cyberdefense. However, the majority of extant works offer subtle perturbations or additions to executable files and do not explore full-file obfuscation. In this study, we show that an open-source encryption tool coupled with a Reinforcement Learning (RL) framework can successfully obfuscate malware to evade state-of-the-art malware detection engines and outperform techniques that use advanced modification methods. Our results show that the proposed method improves the evasion rate from 27%-49% compared to widely- used state-of-the-art reinforcement learning-based methods.
    
[^238]: 双内点优化学习

    Dual Interior-Point Optimization Learning

    [https://arxiv.org/abs/2402.02596](https://arxiv.org/abs/2402.02596)

    本文介绍了双内点优化学习和双超梯度学习两种方法，用于学习带有有界变量的参数线性规划的对偶可行解。这些方法通过预测约束对应的对偶变量，确保对偶可行性，并且能够提供高保真度的对偶可行解和有效的对偶界限。

    

    本文引入了双内点学习（DIPL）和双超梯度学习（DSL），以学习带有有界变量的参数线性规划的对偶可行解，这在许多行业中都是普遍存在的。DIPL模拟了一种新颖的对偶内点算法，而DSL则模拟了经典的对偶超梯度上升算法。通过预测与约束关联的对偶变量，DIPL和DSL保证对偶可行性，然后利用对于约束界限的对偶的灵活性。DIPL和DSL通过提供质量证明来补充现有的原始学习方法。实验证明，它们能够为大规模最优功率流问题产生高保真度的对偶可行解，并在0.5%的优化差距下提供有效的对偶界限。

    This paper introduces Dual Interior Point Learning (DIPL) and Dual Supergradient Learning (DSL) to learn dual feasible solutions to parametric linear programs with bounded variables, which are pervasive across many industries. DIPL mimics a novel dual interior point algorithm while DSL mimics classical dual supergradient ascent. DIPL and DSL ensure dual feasibility by predicting dual variables associated with the constraints then exploiting the flexibility of the duals of the bound constraints. DIPL and DSL complement existing primal learning methods by providing a certificate of quality. They are shown to produce high-fidelity dual-feasible solutions to large-scale optimal power flow problems providing valid dual bounds under 0.5% optimality gap.
    
[^239]: 在量化噪声环境中利用连续可微激活函数进行学习的优化

    Leveraging Continuously Differentiable Activation Functions for Learning in Quantized Noisy Environments

    [https://arxiv.org/abs/2402.02593](https://arxiv.org/abs/2402.02593)

    在量化噪声环境中，利用连续可微激活函数进行学习可以减轻模拟量化误差，为计算机视觉、信号处理等多个机器学习领域的硬件实现提供了指导。

    

    实际世界中的模拟系统固有地受到噪声的影响，这可能会阻碍各种深度学习模型的收敛性和准确性。我们证明了像GELU和SiLU这样的可微激活函数可以稳健地传播梯度，有助于减轻普遍存在于所有模拟系统中的模拟量化误差。我们在量化噪声存在的情况下进行了卷积、线性和Transformer网络的分析和训练。我们能够证明，与传统的修正线性激活函数相比，连续可微激活函数在抗噪声方面具有显著优势。与ReLU相比，在接近零时梯度误差高出100倍。我们的研究结果为选择适当的激活函数提供了指导，以实现在计算机视觉、信号处理等多个机器学习领域中具有高性能和可靠性的硬件实现。

    Real-world analog systems intrinsically suffer from noise that can impede model convergence and accuracy on a variety of deep learning models. We demonstrate that differentiable activations like GELU and SiLU enable robust propagation of gradients which help to mitigate analog quantization error that is ubiquitous to all analog systems. We perform analysis and training of convolutional, linear, and transformer networks in the presence of quantized noise. Here, we are able to demonstrate that continuously differentiable activation functions are significantly more noise resilient over conventional rectified activations. As in the case of ReLU, the error in gradients are 100x higher than those in GELU near zero. Our findings provide guidance for selecting appropriate activations to realize performant and reliable hardware implementations across several machine learning domains such as computer vision, signal processing, and beyond.
    
[^240]: 统一训练通用时间序列预测Transformer

    Unified Training of Universal Time Series Forecasting Transformers

    [https://arxiv.org/abs/2402.02592](https://arxiv.org/abs/2402.02592)

    本研究提出了一种改进的时间序列Transformer架构，名为Moirai，以解决时间序列预测中的跨频率学习、适应多变量时间序列以及解决大规模数据固有的不同分布特性等挑战，从而实现了统一训练通用时间序列预测Transformer。

    

    传统上，时间序列预测的深度学习在一个数据集中一模型的框架下运作，限制了其能够利用大型预训练模型的突破性影响的潜力。通用预测的概念，源于在大量时间序列数据集上进行预训练，设想一个能够处理各种下游预测任务的单一大型时间序列模型。然而，构建这样的模型对于时间序列数据存在独特的挑战，包括：i) 跨频率学习，ii) 适应多变量时间序列中任意数量的变量，以及iii) 解决大规模数据固有的不同分布特性。为了解决这些挑战，我们对传统的时间序列Transformer架构进行了新颖的增强，提出了基于Masked Encoder的通用时间序列预测Transformer（Moirai）。在我们新引入的大规模开放时间序列存档（LOTSA）数据集上进行训练。

    Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) fea
    
[^241]: 从有噪声输入和有噪声输出数据中合成控制器的研究

    Controller Synthesis from Noisy-Input Noisy-Output Data

    [https://arxiv.org/abs/2402.02588](https://arxiv.org/abs/2402.02588)

    本论文研究了从受噪声影响的输入输出数据中合成动态输出反馈控制器的问题，并提出了一种能稳定所有与数据一致的可能系统的控制器设计方法，同时还解决了将结果扩展到通用多输入多输出系统的问题。

    

    本文考虑使用仅受测量噪声影响的输入输出数据，为线性系统合成一个动态输出反馈控制器的问题。为处理输入输出数据，引入了原系统的辅助表示。通过利用辅助系统的结构，我们设计了一个能稳定所有与数据一致的可能系统的控制器。值得注意的是，我们还提供了一个新颖的解决方案，将结果扩展到通用的多输入多输出系统。数值例子对所得的结果进行了说明。

    We consider the problem of synthesizing a dynamic output-feedback controller for a linear system, using solely input-output data corrupted by measurement noise. To handle input-output data, an auxiliary representation of the original system is introduced. By exploiting the structure of the auxiliary system, we design a controller that robustly stabilizes all possible systems consistent with data. Notably, we also provide a novel solution to extend the results to generic multi-input multi-output systems. The findings are illustrated by numerical examples.
    
[^242]: ClipFormer:用于减轻存储器电阻交叉点上变压器写入噪声的键-值剪辑

    ClipFormer: Key-Value Clipping of Transformers on Memristive Crossbars for Write Noise Mitigation

    [https://arxiv.org/abs/2402.02586](https://arxiv.org/abs/2402.02586)

    ClipFormer算法用于减轻变压器在交叉栏上的写入噪声对注意力层中键值矩阵的影响，提高推理准确性。

    

    变压器已经在从自然语言处理到计算机视觉的各种现实应用中带来了革命。然而，由于其庞大的模型大小，传统的冯·诺依曼计算范例在加速变压器时面临着内存和带宽限制。因此，基于非易失性存储器（NVM）的内存计算（IMC）交叉栏，由于其能够以高能效进行高度并行的矩阵-向量乘法（MVM），已成为加速变压器的一种有前景的解决方案。然而，交叉栏中的模拟MVM操作引入了非理想性，如随机读写噪声，这些噪声会影响部署的变压器的推理准确性。具体来说，我们发现预训练的视觉变压器（ViTs）由于写噪声对注意力层中的动态生成的键（K）和值（V）矩阵的影响而容易受到交叉栏的影响，在先前的研究中没有考虑到这种影响。因此，我们提出创新的ClipFormer算法来解决这个问题。

    Transformers have revolutionized various real-world applications from natural language processing to computer vision. However, traditional von-Neumann computing paradigm faces memory and bandwidth limitations in accelerating transformers owing to their massive model sizes. To this end, In-memory Computing (IMC) crossbars based on Non-volatile Memories (NVMs), due to their ability to perform highly parallelized Matrix-Vector-Multiplications (MVMs) with high energy-efficiencies, have emerged as a promising solution for accelerating transformers. However, analog MVM operations in crossbars introduce non-idealities, such as stochastic read & write noise, which affect the inference accuracy of the deployed transformers. Specifically, we find pre-trained Vision Transformers (ViTs) to be vulnerable on crossbars due to the impact of write noise on the dynamically-generated Key (K) and Value (V) matrices in the attention layers, an effect not accounted for in prior studies. We, thus, propose Cl
    
[^243]: DiffEditor: 提升扩散式图像编辑的准确性和灵活性

    DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image Editing

    [https://arxiv.org/abs/2402.02583](https://arxiv.org/abs/2402.02583)

    DiffEditor提出了一种解决现有扩散式图像编辑的准确性和灵活性问题的方法，通过引入图像提示和细节操作，以及结合随机微分方程和梯度引导，有效地提高了编辑效果。

    

    近年来，大规模的文本到图像扩散模型已经在图像生成领域取得了革命性的进展。尽管具备多样且高质量的生成能力，但将这些能力应用于细粒度图像编辑仍然具有挑战性。本文中，我们提出了DiffEditor来改善现有扩散式图像编辑中存在的两个问题：（1）在复杂场景中，编辑结果通常缺乏编辑准确性并且会出现意外的伪影；（2）缺乏灵活性以协调编辑操作，例如引入新的内容。我们的解决方案是在细粒度图像编辑中引入图像提示，与文本提示一起更好地描述编辑内容。为了增加灵活性同时保持内容的一致性，我们将随机微分方程（SDE）局部结合到常微分方程（ODE）采样中。此外，我们还将基于区域得分的梯度引导和时间旅行策略结合到扩散采样中。

    Large-scale Text-to-Image (T2I) diffusion models have revolutionized image generation over the last few years. Although owning diverse and high-quality generation capabilities, translating these abilities to fine-grained image editing remains challenging. In this paper, we propose DiffEditor to rectify two weaknesses in existing diffusion-based image editing: (1) in complex scenarios, editing results often lack editing accuracy and exhibit unexpected artifacts; (2) lack of flexibility to harmonize editing operations, e.g., imagine new content. In our solution, we introduce image prompts in fine-grained image editing, cooperating with the text prompt to better describe the editing content. To increase the flexibility while maintaining content consistency, we locally combine stochastic differential equation (SDE) into the ordinary differential equation (ODE) sampling. In addition, we incorporate regional score-based gradient guidance and a time travel strategy into the diffusion sampling
    
[^244]: 误差估计下PSF错误估计和星系种群偏差对使用CNN进行精确剪切测量的影响

    Impact of PSF misestimation and galaxy population bias on precision shear measurement using a CNN

    [https://arxiv.org/abs/2402.02578](https://arxiv.org/abs/2402.02578)

    本文研究了使用卷积神经网络进行精确剪切测量，并探究了点扩散函数错误估计和星系种群偏差对测量精度的影响。

    

    遥远星系的弱引力透镜提供了探测暗能量的强大手段。本研究旨在研究卷积神经网络（CNN）在精确剪切估计中的应用。特别地，我们使用了一个浅层的CNN，研究了点扩散函数（PSF）错误估计以及“星系种群偏差”（包括“分布偏差”和“形态偏差”）对下一代调查的精确度要求的影响。我们模拟了一个包含噪声的圆盘状和椭圆状星系种群，并采用了一个代表类似欧几里得调查的PSF。我们假设估计剪切和真实剪切之间存在线性关系，并使用非常规的损失函数来减轻噪声偏差的影响，并测量了乘法（m）和加法（c）偏差。当我们使用不正确的星系椭圆度分布或尺寸-星等关系，或错误比例时，我们测量了$m$和$c$。

    Weak gravitational lensing of distant galaxies provides a powerful probe of dark energy. The aim of this study is to investigate the application of convolutional neural networks (CNNs) to precision shear estimation. In particular, using a shallow CNN, we explore the impact of point spread function (PSF) misestimation and `galaxy population bias' (including `distribution bias' and `morphology bias'), focusing on the accuracy requirements of next generation surveys. We simulate a population of noisy disk and elliptical galaxies and adopt a PSF that is representative of a Euclid-like survey. We quantify the accuracy achieved by the CNN assuming a linear relationship between the estimated and true shears and measure the multiplicative ($m$) and additive ($c$) biases. We make use of an unconventional loss function to mitigate the effects of noise bias and measure $m$ and $c$ when we use either: (i) an incorrect galaxy ellipticity distribution or size-magnitude relation, or the wrong ratio o
    
[^245]: Gazebo植物：使用Cosserat Rods模拟植物与机器人的交互

    Gazebo Plants: Simulating Plant-Robot Interaction with Cosserat Rods

    [https://arxiv.org/abs/2402.02570](https://arxiv.org/abs/2402.02570)

    本研究针对机器人与植物的交互问题，提出了一个基于Cosserat杆的Gazebo仿真平台插件，有效解决了物理引擎对非刚性物体的限制，有助于农业机器人训练和优化。

    

    机器人采摘有潜力对农业生产力产生积极影响，降低成本，改善食品质量，提高可持续性，并解决劳动力短缺问题。在农业机器人领域飞速发展的情况下，在虚拟环境中训练机器人已成为必要。生成训练数据以自动化底层计算机视觉任务（如图像分割、目标检测和分类）也严重依赖于这些虚拟环境，因为往往需要合成数据来克服真实数据集的短缺和缺乏多样性。然而，常用的物理引擎如ODE、Simbody、Bullet和DART在机器人领域主要支持刚体的运动和碰撞交互，这个固有的限制阻碍了对非刚性物体（如植物和作物）的处理实验和进展。在本文中，我们提出了一个基于Cosserat杆的Gazebo仿真平台插件。

    Robotic harvesting has the potential to positively impact agricultural productivity, reduce costs, improve food quality, enhance sustainability, and to address labor shortage. In the rapidly advancing field of agricultural robotics, the necessity of training robots in a virtual environment has become essential. Generating training data to automatize the underlying computer vision tasks such as image segmentation, object detection and classification, also heavily relies on such virtual environments as synthetic data is often required to overcome the shortage and lack of variety of real data sets. However, physics engines commonly employed within the robotics community, such as ODE, Simbody, Bullet, and DART, primarily support motion and collision interaction of rigid bodies. This inherent limitation hinders experimentation and progress in handling non-rigid objects such as plants and crops. In this contribution, we present a plugin for the Gazebo simulation platform based on Cosserat ro
    
[^246]: 关于满足Polyak-Lojasiewicz条件的有限和平滑优化问题的复杂性研究

    On the Complexity of Finite-Sum Smooth Optimization under the Polyak-{\L}ojasiewicz Condition

    [https://arxiv.org/abs/2402.02569](https://arxiv.org/abs/2402.02569)

    本文研究了满足Polyak-Lojasiewicz条件的有限和平滑优化问题的复杂性。在单机情况下，为了找到ε次优解，任何梯度方法都需要至少Ω(n+κ√nlog(1/ε))个IFO调用。在分布式情况下，最小化PL函数的问题的下界为Ω(κ/√γlog(1/ε))，Ω((κ+τκ/√γ)log(1/ε))和Ω(n+κ√...

    

    本文研究了形式为$\min_{{\bf x}\in{\mathbb R}^d} f({\bf x})\triangleq \frac{1}{n}\sum_{i=1}^n f_i({\bf x})$的优化问题，其中$f(\cdot)$满足参数$\mu$的Polyak-Lojasiewicz（PL）条件，而$\{f_i(\cdot)\}_{i=1}^n$是$L$均方平滑的。我们证明了任何梯度方法在寻找$\epsilon$-次优解时都需要至少$\Omega(n+\kappa\sqrt{n}\log(1/\epsilon))$个增量一阶预言机（IFO）调用，其中$\kappa\triangleq L/\mu$是问题的条件数。这个结果几乎与已知的最佳一阶方法的IFO复杂度的上界相匹配。我们还研究了在分布式环境中最小化PL函数的问题，其中个体$f_1(\cdot),\dots,f_n(\cdot)$位于一个由$n$个节点组成的连通网络上。我们提供了$\Omega(\kappa/\sqrt{\gamma}\,\log(1/\epsilon))$，$\Omega((\kappa+\tau\kappa/\sqrt{\gamma}\,)\log(1/\epsilon))$和$\Omega\big(n+\kappa\sqrt{...

    This paper considers the optimization problem of the form $\min_{{\bf x}\in{\mathbb R}^d} f({\bf x})\triangleq \frac{1}{n}\sum_{i=1}^n f_i({\bf x})$, where $f(\cdot)$ satisfies the Polyak--{\L}ojasiewicz (PL) condition with parameter $\mu$ and $\{f_i(\cdot)\}_{i=1}^n$ is $L$-mean-squared smooth. We show that any gradient method requires at least $\Omega(n+\kappa\sqrt{n}\log(1/\epsilon))$ incremental first-order oracle (IFO) calls to find an $\epsilon$-suboptimal solution, where $\kappa\triangleq L/\mu$ is the condition number of the problem. This result nearly matches upper bounds of IFO complexity for best-known first-order methods. We also study the problem of minimizing the PL function in the distributed setting such that the individuals $f_1(\cdot),\dots,f_n(\cdot)$ are located on a connected network of $n$ agents. We provide lower bounds of $\Omega(\kappa/\sqrt{\gamma}\,\log(1/\epsilon))$, $\Omega((\kappa+\tau\kappa/\sqrt{\gamma}\,)\log(1/\epsilon))$ and $\Omega\big(n+\kappa\sqrt{
    
[^247]: DefInt：一种用于高效处理混合大型语言模型推理的默认干预框架

    DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models

    [https://arxiv.org/abs/2402.02563](https://arxiv.org/abs/2402.02563)

    DefInt提出了一种默认干预框架，通过默认使用较小规模的语言模型生成推理思路，然后通过反思推理干预解决复杂推理问题，从而提高混合大型语言模型的效率和准确性。

    

    大型语言模型（LLMs）在各种任务中展示出令人印象深刻的新能力，但在处理复杂推理问题方面仍面临挑战。以往的研究如连锁推理（CoT）和思维树（ToT）主要关注提高准确性，但忽视了不断增加的标记成本，这对于具有巨大解空间的开放性实际任务来说可能特别问题。受人类认知的双过程理论的启发，我们提出了一种默认干预框架（DefInt），以释放混合LLMs的协同潜力。默认情况下，DefInt使用较小规模的语言模型生成低成本的推理思路，类似于“系统1”产生的快速直觉。如果这些直觉被认为低置信度，则DefInt将调用放大的语言模型的反思推理作为“系统2”的干预，可以覆盖默认思考并纠正推理过程。实验在五个实际数据集上展示了DefInt论文中的有效性。

    Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
    
[^248]: 基于基础模型的聚类优化主动学习初始化方法

    Foundation Model Makes Clustering a Better Initialization for Active Learning

    [https://arxiv.org/abs/2402.02561](https://arxiv.org/abs/2402.02561)

    本研究提出了一种基于基础模型和聚类方法的主动学习初始化方案，用于选择最具信息量的样本。基础模型是通过自监督训练在大规模数据集上训练得到的，并能生成适用于各种下游任务的紧凑嵌入表示。

    

    主动学习是从未标记的数据集中选择最具信息量的样本进行标注，以满足有限的标注预算。尽管已经有许多方法针对初始化模型后的样本选择进行了研究，但对于主动学习必不可少的初始化阶段，却没有引起足够的关注。先前的研究中大多数都采用随机抽样或者简单的聚类方法。然而，随机抽样容易产生波动，而简单聚类在处理高维数据（如图像数据）时收敛速度慢。在本研究中，我们提出将基础模型与聚类方法结合，用于选择主动学习初始化阶段的样本。基础模型是指在自监督范式下在大规模数据集上训练的模型，能够生成信息丰富且紧凑的嵌入表示，用于各种下游任务。

    Active learning selects the most informative samples from the unlabeled dataset to annotate in the context of a limited annotation budget. While numerous methods have been proposed for subsequent sample selection based on an initialized model, scant attention has been paid to the indispensable phase of active learning: selecting samples for model initialization. Most of the previous studies resort to random sampling or naive clustering. However, random sampling is prone to fluctuation, and naive clustering suffers from convergence speed, particularly when dealing with high-dimensional data such as imaging data. In this work, we propose to integrate foundation models with clustering methods to select samples for active learning initialization. Foundation models refer to those trained on massive datasets by the self-supervised paradigm and capable of generating informative and compacted embeddings for various downstream tasks. Leveraging these embeddings to replace raw features such as p
    
[^249]: 提升生物医学NLI模型的鲁棒性：一种基于探测的临床试验方法

    Enhancing Robustness in Biomedical NLI Models: A Probing Approach for Clinical Trials

    [https://arxiv.org/abs/2402.02558](https://arxiv.org/abs/2402.02558)

    本研究旨在提升生物医学自然语言推断（NLI）模型的鲁棒性，通过一种基于探测的方法研究了临床试验和模型对自然逻辑的理解。

    

    大型语言模型在各个领域和行业中产生了革命性的影响，如对话式人工智能、内容生成、信息检索、商业智能和医学等。在医学领域中，一个主要的应用是分析和调查与蕴含任务相关的临床试验。然而，人们发现大型语言模型容易产生捷径学习、事实不一致和性能下降的问题，即便在上下文变化很小的情况下。虽然进行了对抗性和鲁棒性测试以确保模型的输出完整性，但模糊性仍然存在。为了确保推理的完整性和正确的句法语义理解，我们使用了探测方法。在这里，我使用了记忆探测方法来研究在临床试验上训练的Sci-five模型。我调查了该模型在自然逻辑方面学习到的特征。为了实现目标，我训练了任务特定的探测器。使用这些探测器进行了研究。

    Large Language Models have revolutionized various fields and industries, such as Conversational AI, Content Generation, Information Retrieval, Business Intelligence, and Medical, to name a few. One major application in the field of medical is to analyze and investigate clinical trials for entailment tasks.However, It has been observed that Large Language Models are susceptible to shortcut learning, factual inconsistency, and performance degradation with little variation in context. Adversarial and robust testing is performed to ensure the integrity of models output. But, ambiguity still persists. In order to ensure the integrity of the reasoning performed and investigate the model has correct syntactic and semantic understanding probing is used. Here, I used mnestic probing to investigate the Sci-five model, trained on clinical trial. I investigated the model for feature learnt with respect to natural logic. To achieve the target, I trained task specific probes. Used these probes to in
    
[^250]: DeSparsify：对视觉Transformer中的Token稀疏化机制进行的对抗攻击

    DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms in Vision Transformers

    [https://arxiv.org/abs/2402.02554](https://arxiv.org/abs/2402.02554)

    本文提出了一种对抗攻击方法DeSparsify，针对使用Token稀疏化机制的视觉Transformer，通过精心制作的对抗样本欺骗稀疏化机制，导致最坏情况的性能，以此耗尽操作系统的资源并保持隐蔽性。

    

    视觉Transformer在计算机视觉领域做出了巨大贡献，展现出在各种任务（如图像分类、目标检测）中的最先进性能。然而，它们的高计算要求随使用的Token数量呈二次增长。为解决这个问题，提出了Token稀疏化技术。这些技术采用了一种依赖输入的策略，将无关的Token从计算流程中丢弃，提高模型的效率。然而，它们的动态性和平均情况假设使它们容易受到一种新的威胁 - 经过精心制作的对抗样本，能够欺骗稀疏化机制，导致最坏情况的性能。在本文中，我们提出了一种攻击方法DeSparsify，针对使用Token稀疏化机制的视觉Transformer的可用性。该攻击旨在耗尽操作系统的资源，同时保持隐蔽性。

    Vision transformers have contributed greatly to advancements in the computer vision domain, demonstrating state-of-the-art performance in diverse tasks (e.g., image classification, object detection). However, their high computational requirements grow quadratically with the number of tokens used. Token sparsification techniques have been proposed to address this issue. These techniques employ an input-dependent strategy, in which uninformative tokens are discarded from the computation pipeline, improving the model's efficiency. However, their dynamism and average-case assumption makes them vulnerable to a new threat vector - carefully crafted adversarial examples capable of fooling the sparsification mechanism, resulting in worst-case performance. In this paper, we present DeSparsify, an attack targeting the availability of vision transformers that use token sparsification mechanisms. The attack aims to exhaust the operating system's resources, while maintaining its stealthiness. Our e
    
[^251]: Neur2BiLO: 神经双层优化

    Neur2BiLO: Neural Bilevel Optimization

    [https://arxiv.org/abs/2402.02552](https://arxiv.org/abs/2402.02552)

    Neur2BiLO是一个针对双层优化问题的框架，通过将神经网络近似引入到混合整数规划中，可以快速生成高质量的解决方案。

    

    双层优化处理嵌套问题，在这些问题中，领导者首先做出决策以最小化自己的目标函数，同时考虑到追随者的最好反应。整数变量约束的双层问题特别难以处理。尽管已经提出了用于混合整数线性双层优化的精确求解器，但它们在问题规模较大时往往无法扩展，并且难以推广到非线性情况。另一方面，问题特定的算法（精确和启发式）局限于特定范围。在以数据驱动的环境下，我们提出的框架Neur2BiLO将通过监督回归训练的领导者或追随者的值函数的神经网络近似嵌入到易于解决的混合整数规划中。 Neur2BiLO作为一种启发式算法，可以快速生成高质量的解决方案，适用于双层背包拦截问题，即“关键n个问题”。

    Bilevel optimization deals with nested problems in which a leader takes the first decision to minimize their objective function while accounting for a follower's best-response reaction. Constrained bilevel problems with integer variables are particularly notorious for their hardness. While exact solvers have been proposed for mixed-integer linear bilevel optimization, they tend to scale poorly with problem size and are hard to generalize to the non-linear case. On the other hand, problem-specific algorithms (exact and heuristic) are limited in scope. Under a data-driven setting in which similar instances of a bilevel problem are solved routinely, our proposed framework, Neur2BiLO, embeds a neural network approximation of the leader's or follower's value function, trained via supervised regression, into an easy-to-solve mixed-integer program. Neur2BiLO serves as a heuristic that produces high-quality solutions extremely fast for the bilevel knapsack interdiction problem, the "critical n
    
[^252]: 基于深度强化学习的障碍物避障轨迹规划器与鲁棒低级控制的机器人操作器

    Obstacle Avoidance Deep Reinforcement Learning-Based Trajectory Planner with Robust Low-Level Control for Robotic Manipulators

    [https://arxiv.org/abs/2402.02551](https://arxiv.org/abs/2402.02551)

    这篇论文提出了一种基于深度强化学习和鲁棒低级控制的机器人操作器的障碍物避障轨迹规划方法，该方法通过与环境的交互积极参与学习，绕过了计算复杂性，同时解决了非重复和随机的避障任务。

    

    在机器人领域，现代策略往往是基于学习的，其特点是黑盒性质复杂，缺乏解释性，可能在确保稳定性和安全性方面带来挑战。为了解决这些问题，我们提出了将无障碍深度强化学习（DRL）轨迹规划器与新颖的自动调谐低级和关节级控制策略集成在一起，并通过与环境的交互积极参与学习阶段。这种方法绕过了与计算相关的复杂性，同时解决了非重复和随机的避障任务。首先，利用无模型DRL代理在关节级推理任务空间中进行速度限制和无障碍运动规划，然后将该规划输入到稳健的子系统自适应控制器中，产生所需的扭矩，而杜鹃搜索优化（CSO）算法增强了控制增益以最小化。

    In robotics, contemporary strategies are learning-based, characterized by a complex black-box nature and a lack of interpretability, which may pose challenges in ensuring stability and safety. To address these issues, we propose integrating an obstacle-free deep reinforcement learning (DRL) trajectory planner with a novel auto-tuning low- and joint-level control strategy, all while actively engaging in the learning phase through interactions with the environment. This approach circumvents the complexities associated with computations while also addressing nonrepetitive and random obstacle avoidance tasks. First, a model-free DRL agent to plan velocity-bounded and obstacle-free motion is employed for a manipulator with 'n' degrees of freedom (DoF) in task space through joint-level reasoning. This plan is then input into a robust subsystem-based adaptive controller, which produces the necessary torques, while the Cuckoo Search Optimization (CSO) algorithm enhances control gains to minimi
    
[^253]: 大型语言模型是否适合基于表格的事实检查？

    Are Large Language Models Table-based Fact-Checkers?

    [https://arxiv.org/abs/2402.02549](https://arxiv.org/abs/2402.02549)

    本研究初步探讨了大型语言模型在基于表格的事实检查方面的潜力。实验结果表明，通过提示工程，大型语言模型在零样本和少样本的情况下可以实现可接受的表现。

    

    基于表格的事实验证（TFV）旨在提取语句和结构化表格之间的蕴涵关系。现有基于小规模模型的TFV方法在标注数据不足和零样本能力薄弱方面存在问题。近年来，大型语言模型（LLMs）在研究领域引起了广泛关注。它们在几个自然语言处理任务上展示了强大的零样本和上下文学习能力，但它们在TFV领域的潜力还不清楚。在本文中，我们进行了关于LLMs是否适合作为基于表格的事实检查器的初步研究。具体来说，我们设计了多样化的提示语来探索上下文学习如何帮助LLMs在TFV方面，即零样本和少样本TFV能力。此外，我们精心设计和构建了TFV指导以研究LLMs的指导调整带来的性能改进。实验结果表明，通过提示工程，LLMs在零样本和少样本TFV方面可以达到可接受的结果，而指导调整则进一步提升了性能。

    Table-based Fact Verification (TFV) aims to extract the entailment relation between statements and structured tables. Existing TFV methods based on small-scaled models suffer from insufficient labeled data and weak zero-shot ability. Recently, the appearance of Large Language Models (LLMs) has gained lots of attraction in research fields. They have shown powerful zero-shot and in-context learning abilities on several NLP tasks, but their potential on TFV is still unknown. In this work, we implement a preliminary study about whether LLMs are table-based fact-checkers. In detail, we design diverse prompts to explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and few-shot TFV capability. Besides, we carefully design and construct TFV instructions to study the performance gain brought by the instruction tuning of LLMs. Experimental results demonstrate that LLMs can achieve acceptable results on zero-shot and few-shot TFV with prompt engineering, while instruction-tun
    
[^254]: 使用深度学习对网球动作进行分类

    Classification of Tennis Actions Using Deep Learning

    [https://arxiv.org/abs/2402.02545](https://arxiv.org/abs/2402.02545)

    本研究使用深度学习探索了对网球动作进行分类的潜力和挑战，并通过使用学术网球数据集THETIS进行了训练和评估。最佳模型达到了74%的泛化准确率，为网球动作分类提供了良好的性能。然后，对最佳模型进行了错误分析，并指出了改进网球数据集的方向。同时，讨论了数据集和当前公开可用的网球数据集的限制，并提出了未来的研究方向。

    

    深度学习的最新进展使得在视频中识别特定事件变得更加精确。这在网球等运动中具有很大的相关性，例如可以自动收集比赛统计信息，或者回放特定感兴趣的动作以用于游戏策略或球员提高。在本文中，我们研究了使用深度学习进行网球动作分类的潜力和挑战。我们基于深度学习架构SlowFast训练和评估了三个不同大小的模型，并使用学术网球数据集THETIS进行测试。最佳模型达到了74%的泛化准确率，证明了网球动作分类的良好性能。我们为最佳模型提供了错误分析，并指出改进网球数据集的方向。我们讨论了数据集的局限性，目前公开可用的网球数据集的一般限制以及未来需要采取的步骤以取得进展。

    Recent advances of deep learning makes it possible to identify specific events in videos with greater precision. This has great relevance in sports like tennis in order to e.g., automatically collect game statistics, or replay actions of specific interest for game strategy or player improvements. In this paper, we investigate the potential and the challenges of using deep learning to classify tennis actions. Three models of different size, all based on the deep learning architecture SlowFast were trained and evaluated on the academic tennis dataset THETIS. The best models achieve a generalization accuracy of 74 %, demonstrating a good performance for tennis action classification. We provide an error analysis for the best model and pinpoint directions for improvement of tennis datasets in general. We discuss the limitations of the data set, general limitations of current publicly available tennis data-sets, and future steps needed to make progress.
    
[^255]: LHRS-Bot：利用VGI增强的大型多模态语言模型赋能遥感领域

    LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model

    [https://arxiv.org/abs/2402.02544](https://arxiv.org/abs/2402.02544)

    LHRS-Bot 是一个利用自愿地理信息(VGI)增强的大型多模态语言模型，旨在解决近期MLLM在遥感领域中未对多样的地理景观和物体进行充分考虑的问题。通过引入多层次视觉-语言对齐策略和课程学习方法，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。

    

    大型语言模型（LLMs）的革命性能力开创了多模态大型语言模型（MLLMs）并促进了在各个专业领域的多样化应用。然而，在遥感（RS）领域中，近期的MLLM努力未能充分考虑到遥感图像中多样的地理景观和物体。为了弥补这一差距，我们构建了一个大规模的RS图像-文本数据集LHRS-Align，以及一个信息丰富的RS特定指导数据集LHRS-Instruct，利用丰富的自愿地理信息（VGI）和全球可用的RS图像。在此基础上，我们引入了LHRS-Bot，一种针对RS图像理解的MLLM，通过一种新颖的多层次视觉-语言对齐策略和课程学习方法。全面的实验证明，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。

    The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.
    
[^256]: CompeteSMoE - 通过竞争实现稀疏专家混合模型的有效训练

    CompeteSMoE - Effective Training of Sparse Mixture of Experts via Competition

    [https://arxiv.org/abs/2402.02526](https://arxiv.org/abs/2402.02526)

    本文提出了CompeteSMoE方法，通过竞争机制解决了稀疏专家混合模型的表示崩溃问题，实现了有效的训练，在大型语言模型上取得了强大性能提升。

    

    稀疏专家混合模型（SMoE）为超越增加网络深度或宽度的模型复杂性提供了一种吸引人的解决方案。然而，有效训练SMoE的挑战在于表示崩溃问题，导致参数冗余和有限的表示能力。在本研究中，我们提出了一种竞争机制来解决表示崩溃的基本挑战。通过只将输入路由到具有最高神经响应的专家，我们展示了在温和假设下，竞争享有与最优估计器相同的收敛速率。我们进一步提出了CompeteSMoE，一种通过部署一个简单的路由器来预测竞争结果的有效且高效的大型语言模型训练算法。因此，CompeteSMoE在竞争路由策略方面获得了强大的性能提升，同时具有较低的计算开销。我们在两个Transformer架构上进行了广泛的实证评估。

    Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, effective training of SMoE has proven to be challenging due to the representation collapse issue, which causes parameter redundancy and limited representation potentials. In this work, we propose a competition mechanism to address this fundamental challenge of representation collapse. By routing inputs only to experts with the highest neural response, we show that, under mild assumptions, competition enjoys the same convergence rate as the optimal estimator. We further propose CompeteSMoE, an effective and efficient algorithm to train large language models by deploying a simple router that predicts the competition outcomes. Consequently, CompeteSMoE enjoys strong performance gains from the competition routing policy while having low computation overheads. Our extensive empirical evaluations on two transformer architectures a
    
[^257]: 非主动自适应采样中的绝对收敛和误差阈值

    Absolute convergence and error thresholds in non-active adaptive sampling

    [https://arxiv.org/abs/2402.02522](https://arxiv.org/abs/2402.02522)

    提出了一种计算非主动自适应采样中绝对收敛和误差阈值的方法，可以确定模型何时不再增加质量，并提供了一个接近条件来估算模型实现目标的接近度，从而支持模型选择中学习参数的微调。

    

    非主动自适应采样是一种从训练数据中构建机器学习模型的方法，它可以动态和自动地确定保证的样本大小。在这个背景下，无论所采用的调度和生成弱预测器的策略如何，我们描述了一种计算绝对收敛和误差阈值的方法。我们不仅可以确定模型的质量何时不再增加，还提供了一个接近条件来绝对地估算模型实现这一目标的接近度，从而支持在模型选择中进行学习参数的微调。该技术在工作假设方面证明了其正确性和完备性，同时增强了采样方案的鲁棒性。测试结果符合我们的预期，并以自然语言处理领域中词性标注生成为案例研究来说明这一提议。

    Non-active adaptive sampling is a way of building machine learning models from a training data base which are supposed to dynamically and automatically derive guaranteed sample size. In this context and regardless of the strategy used in both scheduling and generating of weak predictors, a proposal for calculating absolute convergence and error thresholds is described. We not only make it possible to establish when the quality of the model no longer increases, but also supplies a proximity condition to estimate in absolute terms how close it is to achieving such a goal, thus supporting decision making for fine-tuning learning parameters in model selection. The technique proves its correctness and completeness with respect to our working hypotheses, in addition to strengthening the robustness of the sampling scheme. Tests meet our expectations and illustrate the proposal in the domain of natural language processing, taking the generation of part-of-speech taggers as case study.
    
[^258]: SIMPL:一种简单高效的自动驾驶多智能体运动预测基线

    SIMPL: A Simple and Efficient Multi-agent Motion Prediction Baseline for Autonomous Driving

    [https://arxiv.org/abs/2402.02519](https://arxiv.org/abs/2402.02519)

    本文提出了一种简单高效的自动驾驶多智能体运动预测基线(SIMPL)，通过引入紧凑高效的全局特征融合模块以及使用Bernstein基多项式对连续轨迹参数化的方法，实现了实时准确的运动预测，为下游规划任务提供了有价值的数据。

    

    本文提出了一种简单高效的自动驾驶车辆的运动预测基线，名为SIMPL。与传统的以智能体为中心的方法相比，虽然具有高准确性但计算重复，以及以场景为中心的方法虽然准确性和泛化性有所妥协，SIMPL可以实时、准确地预测所有相关交通参与者的运动。为了在准确性和推理速度上实现改进，我们提出了一种紧凑高效的全局特征融合模块，以对称方式执行定向消息传递，使网络能够在单次前向传递中预测所有道路使用者的未来运动，并减轻视角转换带来的准确性损失。此外，我们还研究了使用Bernstein基多项式对连续轨迹参数化，允许在任何所需时间点评估轨迹和其高阶导数，这对下游规划任务非常有价值。作为一个强大的基准方法，SIMPL打破了模式并提供了准确结果。

    This paper presents a Simple and effIcient Motion Prediction baseLine (SIMPL) for autonomous vehicles. Unlike conventional agent-centric methods with high accuracy but repetitive computations and scene-centric methods with compromised accuracy and generalizability, SIMPL delivers real-time, accurate motion predictions for all relevant traffic participants. To achieve improvements in both accuracy and inference speed, we propose a compact and efficient global feature fusion module that performs directed message passing in a symmetric manner, enabling the network to forecast future motion for all road users in a single feed-forward pass and mitigating accuracy loss caused by viewpoint shifting. Additionally, we investigate the continuous trajectory parameterization using Bernstein basis polynomials in trajectory decoding, allowing evaluations of states and their higher-order derivatives at any desired time point, which is valuable for downstream planning tasks. As a strong baseline, SIMP
    
[^259]: 潜在图扩散：一种在图上生成和预测的统一框架

    Latent Graph Diffusion: A Unified Framework for Generation and Prediction on Graphs

    [https://arxiv.org/abs/2402.02518](https://arxiv.org/abs/2402.02518)

    本文提出了一种统一框架，能够使用一个模型解决各级别和各类型的图学习任务，通过潜在图扩散模型生成和预测节点、边和图级别的特征，具有可证明的保证，并在实验证明了其有效性。

    

    本文提出了第一个框架，可以使用一个模型解决各级别（节点、边和图）和各类型（生成、回归和分类）的图学习任务。首先，我们提出了潜在图扩散（LGD），一种能够同时生成节点、边和图级别特征的生成模型。通过将图结构和特征嵌入潜在空间，利用强大的编码器进行解码，然后在潜在空间中训练扩散模型，我们实现了这个目标。LGD还可以通过特殊设计的交叉注意力机制进行条件生成。然后，我们将回归和分类等预测任务形式化为（条件）生成，这使得我们的LGD能够通过可证明的保证来解决各级别和各类型的任务。通过大量的实验证明了我们框架的有效性，其中我们的模型在各项指标上取得了最先进或高度竞争力的结果。

    In this paper, we propose the first framework that enables solving graph learning tasks of all levels (node, edge and graph) and all types (generation, regression and classification) with one model. We first propose Latent Graph Diffusion (LGD), a generative model that can generate node, edge, and graph-level features of all categories simultaneously. We achieve this goal by embedding the graph structures and features into a latent space leveraging a powerful encoder which can also be decoded, then training a diffusion model in the latent space. LGD is also capable of conditional generation through a specifically designed cross-attention mechanism. Then we formulate prediction tasks including regression and classification as (conditional) generation, which enables our LGD to solve tasks of all levels and all types with provable guarantees. We verify the effectiveness of our framework with extensive experiments, where our models achieve state-of-the-art or highly competitive results acr
    
[^260]: 适应性调度用于自适应采样在构建词性标注器中

    Adaptive scheduling for adaptive sampling in POS taggers construction

    [https://arxiv.org/abs/2402.02516](https://arxiv.org/abs/2402.02516)

    本论文提出了一种自适应调度的自适应采样方法，用于在构建词性标注器中加快训练速度，同时提高采样的鲁棒性。该方法分析学习曲线的形状，在任何时候增加或减少实例，以确保获得学习能力的净增益。

    

    我们引入了一种自适应调度的自适应采样作为构建词性标注器的机器学习的新方法。目标是在大规模数据集上加快训练速度，同时不显著损失性能。与之前使用随机、固定或定期增加实例之间间隔的方法不同，我们的方法在几何上分析学习曲线的形状，结合功能模型，在任何时候增加或减少实例。该算法在我们的工作假设上被证明是形式上正确的。也就是说，给定一个案例，下一个案例是最近的，确保从前者中获得学习能力的净增益，可以调节此条件的要求水平。我们还通过更加关注在训练数据库中临时性能膨胀的区域，提高了采样的鲁棒性，从而防止学习提前停止。

    We introduce an adaptive scheduling for adaptive sampling as a novel way of machine learning in the construction of part-of-speech taggers. The goal is to speed up the training on large data sets, without significant loss of performance with regard to an optimal configuration. In contrast to previous methods using a random, fixed or regularly rising spacing between the instances, ours analyzes the shape of the learning curve geometrically in conjunction with a functional model to increase or decrease it at any time. The algorithm proves to be formally correct regarding our working hypotheses. Namely, given a case, the following one is the nearest ensuring a net gain of learning ability from the former, it being possible to modulate the level of requirement for this condition. We also improve the robustness of sampling by paying greater attention to those regions of the training data base subject to a temporary inflation in performance, thus preventing the learning from stopping prematu
    
[^261]: 学习曲线建模及其在词性标注中的应用

    Modeling of learning curves with applications to pos tagging

    [https://arxiv.org/abs/2402.02515](https://arxiv.org/abs/2402.02515)

    该论文提出了一种估计学习曲线演化的算法，可以通过部分训练数据结果来预测学习过程中达到期望准确度所需的时间。

    

    介绍了一种基于部分训练数据结果和使用功能策略的算法，用于估计整个训练数据集上的学习曲线的演化。我们通过迭代逼近所需时间点的待求值，独立于所使用的学习技术，并且在经过一定的过程点（称为预测级别）后。该提案在工作假设方面被证明是形式上正确的，并且包含一个可靠的近似条件。这使得用户可以基于最终可实现的准确度来设定收敛阈值，这扩展了停止准则的概念，即使存在扭曲观察结果，也似乎是有效的。我们的目标是评估培训工作量，支持决策过程来减少学习过程中所需的人力和计算资源。该提案在至少三个操作程序中很有兴趣。第一个是预测学习过程中达到期望准确度所需的时间。

    An algorithm to estimate the evolution of learning curves on the whole of a training data base, based on the results obtained from a portion and using a functional strategy, is introduced. We approximate iteratively the sought value at the desired time, independently of the learning technique used and once a point in the process, called prediction level, has been passed. The proposal proves to be formally correct with respect to our working hypotheses and includes a reliable proximity condition. This allows the user to fix a convergence threshold with respect to the accuracy finally achievable, which extends the concept of stopping criterion and seems to be effective even in the presence of distorting observations.   Our aim is to evaluate the training effort, supporting decision making in order to reduce the need for both human and computational resources during the learning process. The proposal is of interest in at least three operational procedures. The first is the anticipation of
    
[^262]: 基于高斯伪标签的形态注意力深度监督在非对比度CT腹主动脉分割中的应用

    Deep Supervision by Gaussian Pseudo-label-based Morphological Attention for Abdominal Aorta Segmentation in Non-Contrast CTs

    [https://arxiv.org/abs/2402.02514](https://arxiv.org/abs/2402.02514)

    本文提出了一种基于高斯伪标签的形态注意力深度监督方法，应用于非对比度CT腹主动脉分割任务中。通过该方法，可以在训练过程中保留主动脉的形态特征并减轻模糊边界的影响，降低过拟合的风险。

    

    针对非对比度CT影像中腹部主动脉的分割问题是计算机辅助内腔导航的一个复杂任务，特别是在不适合使用对比剂的情况下。虽然最新的深度学习分割模型已经被提出来处理这个任务，但它们都是基于人工注释的强标签进行训练。然而，在非对比度CT中由于主动脉边界的固有模糊，强标签的可靠性可能受到损害，从而导致过拟合风险。本文介绍了一种基于高斯伪标签的方法，通过深度监督将其集成到传统的深度学习模型中，以实现形态注意力（MA）的增强。由于高斯伪标签保留了主动脉的形态特征，而没有明确地表示其边界分布，我们认为它在训练过程中可以保持主动脉的形态学，同时减轻模糊边界的负面影响，降低过拟合的风险。

    The segmentation of the abdominal aorta in non-contrast CT images is a non-trivial task for computer-assisted endovascular navigation, particularly in scenarios where contrast agents are unsuitable. While state-of-the-art deep learning segmentation models have been proposed recently for this task, they are trained on manually annotated strong labels. However, the inherent ambiguity in the boundary of the aorta in non-contrast CT may undermine the reliability of strong labels, leading to potential overfitting risks. This paper introduces a Gaussian-based pseudo label, integrated into conventional deep learning models through deep supervision, to achieve Morphological Attention (MA) enhancement. As the Gaussian pseudo label retains the morphological features of the aorta without explicitly representing its boundary distribution, we suggest that it preserves aortic morphology during training while mitigating the negative impact of ambiguous boundaries, reducing the risk of overfitting. It
    
[^263]: 在神经网络中通过相关在线指标来提前停止

    Early stopping by correlating online indicators in neural networks

    [https://arxiv.org/abs/2402.02513](https://arxiv.org/abs/2402.02513)

    本文提出了一种在神经网络中最小化泛化误差的新技术，通过利用一系列在线指标的时间相关性，找到过拟合现象，并提供了可靠的提前停止条件，从而提高了预测能力。

    

    为了最小化神经网络中的泛化误差，引入了一种新颖的技术来在训练学习者时识别过拟合现象。这使得支持可靠和可信的提前停止条件成为可能，从而提高了该类型建模的预测能力。我们的提议利用一系列在线指标的时间相关性，即用于指示一组假设是否满足的特征函数，与从金丝雀判断中构建的一系列独立停止条件相联系，以评估过拟合的存在。通过这种方式，我们为决策提供了形式化的基础，以中断学习过程。与之前专注于单一标准的方法相反，我们利用独立评估之间的附带效应，寻求更广泛的操作范围和更大的诊断可靠性。

    In order to minimize the generalization error in neural networks, a novel technique to identify overfitting phenomena when training the learner is formally introduced. This enables support of a reliable and trustworthy early stopping condition, thus improving the predictive power of that type of modeling. Our proposal exploits the correlation over time in a collection of online indicators, namely characteristic functions for indicating if a set of hypotheses are met, associated with a range of independent stopping conditions built from a canary judgment to evaluate the presence of overfitting. That way, we provide a formal basis for decision making in terms of interrupting the learning process.   As opposed to previous approaches focused on a single criterion, we take advantage of subsidiarities between independent assessments, thus seeking both a wider operating range and greater diagnostic reliability. With a view to illustrating the effectiveness of the halting condition described, 
    
[^264]: PoCo: 来自和为异构机器人学习的策略组合

    PoCo: Policy Composition from and for Heterogeneous Robot Learning

    [https://arxiv.org/abs/2402.02511](https://arxiv.org/abs/2402.02511)

    PoCo是一种策略组合方法，通过组合不同模态和领域的数据分布，实现了从异构数据中训练通用机器人策略的目标。该方法可以实现场景级和任务级的广义操作技能学习，并在推理时通过任务级组合和分析成本函数进行策略行为的自适应调整。

    

    从异构数据中训练通用的机器人策略以处理不同任务是一个重大挑战。现有的机器人数据集在颜色、深度、触觉和姿态感等不同模态上存在差异，并在模拟、真实机器人和人类视频等不同领域收集。目前的方法通常会收集并汇集一个领域的所有数据，以训练一个单一策略来处理任务和领域的异构性，这是非常昂贵和困难的。在这项工作中，我们提出了一种灵活的方法，称为策略组合，通过组合用扩散模型表示的不同数据分布，将跨不同模态和领域的信息结合起来，以学习场景级和任务级的广义操作技能。我们的方法可以在多任务操作中使用任务级组合，并与分析成本函数组合，以在推理时调整策略行为。我们在模拟、人类和实际机器人数据上训练我们的方法。

    Training general robotic policies from heterogeneous data for different tasks is a significant challenge. Existing robotic datasets vary in different modalities such as color, depth, tactile, and proprioceptive information, and collected in different domains such as simulation, real robots, and human videos. Current methods usually collect and pool all data from one domain to train a single policy to handle such heterogeneity in tasks and domains, which is prohibitively expensive and difficult. In this work, we present a flexible approach, dubbed Policy Composition, to combine information across such diverse modalities and domains for learning scene-level and task-level generalized manipulation skills, by composing different data distributions represented with diffusion models. Our method can use task-level composition for multi-task manipulation and be composed with analytic cost functions to adapt policy behaviors at inference time. We train our method on simulation, human, and real 
    
[^265]: 分层联合学习中的设备调度和分配在物联网中的应用

    Device Scheduling and Assignment in Hierarchical Federated Learning for Internet of Things

    [https://arxiv.org/abs/2402.02506](https://arxiv.org/abs/2402.02506)

    在物联网中，分层联合学习（HFL）通过将模型聚合分配给多个边缘服务器来解决联合学习中的网络拥塞问题。本文提出了一种改进的K-Center算法用于设备调度，并引入了基于深度强化学习的方法来优化设备分配。

    

    联合学习（FL）是物联网中一种有前景的机器学习方法，但在IoT设备数量增加时，必须解决网络拥塞问题。分层联合学习（HFL）通过将模型聚合分配给多个边缘服务器来缓解这个问题。然而，通信开销仍然是一个挑战，特别是在所有IoT设备同时参与训练过程的情况下。为了可扩展性，实际的HFL方案选择一部分IoT设备参与训练，因此引入了设备调度的概念。在此设置中，只有选择的IoT设备被安排参与全局训练，每个设备被分配给一个边缘服务器。现有的HFL分配方法主要基于搜索机制，这些机制在寻找最优分配时存在高延迟的问题。本文提出了一种改进的K-Center算法用于设备调度，并引入了基于深度强化学习的方法。

    Federated Learning (FL) is a promising machine learning approach for Internet of Things (IoT), but it has to address network congestion problems when the population of IoT devices grows. Hierarchical FL (HFL) alleviates this issue by distributing model aggregation to multiple edge servers. Nevertheless, the challenge of communication overhead remains, especially in scenarios where all IoT devices simultaneously join the training process. For scalability, practical HFL schemes select a subset of IoT devices to participate in the training, hence the notion of device scheduling. In this setting, only selected IoT devices are scheduled to participate in the global training, with each of them being assigned to one edge server. Existing HFL assignment methods are primarily based on search mechanisms, which suffer from high latency in finding the optimal assignment. This paper proposes an improved K-Center algorithm for device scheduling and introduces a deep reinforcement learning-based appr
    
[^266]: 点云问题:重新思考不同观测空间对机器人学习的影响

    Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning

    [https://arxiv.org/abs/2402.02500](https://arxiv.org/abs/2402.02500)

    通过广泛实验发现基于点云的方法在机器人学习中表现出更好的性能，特别是在各种预训练和泛化任务中。结果表明，点云观测模态对于复杂机器人任务是有价值的。

    

    在这项研究中，我们探讨了不同观测空间对机器人学习的影响，重点关注了三种主要模态：RGB，RGB-D和点云。通过在超过17个不同接触丰富的操作任务上进行广泛实验，涉及两个基准和仿真器，我们观察到了一个显著的趋势：基于点云的方法，即使是最简单的设计，通常在性能上超过了其RGB和RGB-D的对应物。这在从头开始训练和利用预训练的两种情况下都是一致的。此外，我们的研究结果表明，点云观测在相机视角、照明条件、噪声水平和背景外观等各种几何和视觉线索方面，都能提高策略零样本泛化能力。研究结果表明，三维点云是复杂机器人任务中有价值的观测模态。我们将公开所有的代码和检查点，希望我们的观点能帮助解决问题。

    In this study, we explore the influence of different observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. Through extensive experimentation on over 17 varied contact-rich manipulation tasks, conducted across two benchmarks and simulators, we have observed a notable trend: point cloud-based methods, even those with the simplest designs, frequently surpass their RGB and RGB-D counterparts in performance. This remains consistent in both scenarios: training from scratch and utilizing pretraining. Furthermore, our findings indicate that point cloud observations lead to improved policy zero-shot generalization in relation to various geometry and visual clues, including camera viewpoints, lighting conditions, noise levels and background appearance. The outcomes suggest that 3D point cloud is a valuable observation modality for intricate robotic tasks. We will open-source all our codes and checkpoints, hoping that our insights can help de
    
[^267]: 机器人Trajectron：基于轨迹预测的机器人操作共享控制

    Robot Trajectron: Trajectory Prediction-based Shared Control for Robot Manipulation

    [https://arxiv.org/abs/2402.02499](https://arxiv.org/abs/2402.02499)

    本论文提出了一种新的轨迹预测和共享控制方法，通过预测机器人的运动轨迹和辅助操作者的意图，可以减轻操作者的认知负荷和提高操作效率。

    

    我们解决了以下问题：(a)如何预测手臂到达动作的轨迹，基于运动的开始的几秒钟；(b)如何利用这个预测器来促进共享控制操作任务，通过在预期运动方向上辅助操作者来减轻认知负荷。我们提出了一种新颖的意图估计器，名为Robot Trajectron (RT)，它根据机器人最近的位置、速度和加速度历史来产生机器人预期轨迹的概率表示。考虑到手臂动力学特性，RT可以更好地捕捉操作者的意图，相比其他只使用手臂位置的状态最先进模型，RT特别适合在操作者意图易受变化的任务中提供辅助。我们提出了一种新颖的共享控制解决方案，将RT的预测能力与潜在目标位置的表示相结合。实验证明RT在意图估计和

    We address the problem of (a) predicting the trajectory of an arm reaching motion, based on a few seconds of the motion's onset, and (b) leveraging this predictor to facilitate shared-control manipulation tasks, easing the cognitive load of the operator by assisting them in their anticipated direction of motion. Our novel intent estimator, dubbed the \emph{Robot Trajectron} (RT), produces a probabilistic representation of the robot's anticipated trajectory based on its recent position, velocity and acceleration history. Taking arm dynamics into account allows RT to capture the operator's intent better than other SOTA models that only use the arm's position, making it particularly well-suited to assist in tasks where the operator's intent is susceptible to change. We derive a novel shared-control solution that combines RT's predictive capacity to a representation of the locations of potential reaching targets. Our experiments demonstrate RT's effectiveness in both intent estimation and 
    
[^268]: Weisfeiler Leman用于欧几里得等变机器学习

    Weisfeiler Leman for Euclidean Equivariant Machine Learning

    [https://arxiv.org/abs/2402.02484](https://arxiv.org/abs/2402.02484)

    本文扩展了2-WL测试的适用范围，包括点云中的位置和速度，并提出了一种简单修改的PPGN架构，以获得一个可近似所有连续等变函数的通用等变架构。

    

    k-Weisfeiler Leman (k-WL)图同构测试层次结构是评估图神经网络(GNNs)表达能力的常用方法。最近，证明了2-WL测试在编码3D点云数据的加权图上是完备的。因此，具有与2-WL测试等价的表达能力的GNNs可以被证明在点云上是通用的。然而，这个结果仅限于点云上的不变连续函数。本文通过三个方面对这一结果进行了扩展:首先，我们展示了2-WL测试可以扩展到包括位置和速度的点云，这在应用中经常遇到。其次，我们展示了PPGN (Maron等人，2019)可以在低复杂度下在所有点云上一致地模拟2-WL。最后，我们展示了对这个PPGN架构的简单修改可以用来获得一个可近似所有连续等变函数的通用等变架构。构建

    The $k$-Weifeiler-Leman ($k$-WL) graph isomorphism test hierarchy is a common method for assessing the expressive power of graph neural networks (GNNs). Recently, the $2$-WL test was proven to be complete on weighted graphs which encode $3\mathrm{D}$ point cloud data. Consequently, GNNs whose expressive power is equivalent to the $2$-WL test are provably universal on point clouds. Yet, this result is limited to invariant continuous functions on point clouds.   In this paper we extend this result in three ways: Firstly, we show that $2$-WL tests can be extended to point clouds which include both positions and velocity, a scenario often encountered in applications. Secondly, we show that PPGN (Maron et al., 2019) can simulate $2$-WL uniformly on all point clouds with low complexity. Finally, we show that a simple modification of this PPGN architecture can be used to obtain a universal equivariant architecture that can approximate all continuous equivariant functions uniformly.   Building
    
[^269]: BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback

    BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback

    [https://arxiv.org/abs/2402.02479](https://arxiv.org/abs/2402.02479)

    BRAIn是一种基于贝叶斯奖励条件化缩减推断的自然语言生成方法，通过反馈来改进RLHF，在LLM对齐中表现出较好的可扩展性和性能。

    

    在人类反馈的强化学习领域，继Proximal Policy Optimization (PPO)取得成功之后，提出了一种新的方法，如Sequence Likelihood Calibration (SLiC)和Direct Policy Optimization (DPO)，这些方法是离线的，并且以间接的方式使用奖励。这些技术，特别是DPO，由于其可扩展性和性能，最近已经成为LLM对齐的首选工具。然而，它们遗漏了PPO方法的重要特征。诸如SLiC或RRHF的方法仅利用奖励模型(RM)进行排序/偏好，丢失了细粒度信息，忽略了RM的参数形式(例如Bradley-Terry、Plackett-Luce)；而诸如DPO的方法甚至不使用单独的奖励模型。在这项工作中，我们提出了一种新颖的方法，命名为BRAIn，它将RM作为分布匹配方法的一部分重新引入。BRAIn考虑到了LLM分布在假设输出质量良好的条件下，并应用B...

    Following the success of Proximal Policy Optimization (PPO) for Reinforcement Learning from Human Feedback (RLHF), new techniques such as Sequence Likelihood Calibration (SLiC) and Direct Policy Optimization (DPO) have been proposed that are offline in nature and use rewards in an indirect manner. These techniques, in particular DPO, have recently become the tools of choice for LLM alignment due to their scalability and performance. However, they leave behind important features of the PPO approach. Methods such as SLiC or RRHF make use of the Reward Model (RM) only for ranking/preference, losing fine-grained information and ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce), while methods such as DPO do not use even a separate reward model. In this work, we propose a novel approach, named BRAIn, that re-introduces the RM as part of a distribution matching approach.BRAIn considers the LLM distribution conditioned on the assumption of output goodness and applies B
    
[^270]: 为什么双曲神经网络有效？关于层级表示能力的研究

    Why are hyperbolic neural networks effective? A study on hierarchical representation capability

    [https://arxiv.org/abs/2402.02478](https://arxiv.org/abs/2402.02478)

    本文通过大规模实验对为什么双曲神经网络有效进行了全面分析，并提出了几种预训练策略来增强层级表示能力，从而改进下游任务的性能。

    

    双曲神经网络（HNNs）在双曲空间中运作，在过去几年中被广泛应用，其动机是双曲空间中存在一种优化嵌入，能够比欧几里得空间更准确地保留数据的层级关系（称为层级表示能力，HRC）。然而，没有证据表明HNN可以达到理论上的最优嵌入，这导致了许多研究建立在错误的动机上。本文提出了一个评估HRC的基准，并通过大规模实验对HNN为何有效进行了全面分析。受到分析结果的启发，我们提出了几种预训练策略来增强HRC并提高下游任务的性能，进一步验证了分析的可靠性。实验证明，HNN无法实现理论上的最优嵌入。HRC受优化目标和层级结构的显著影响，

    Hyperbolic Neural Networks (HNNs), operating in hyperbolic space, have been widely applied in recent years, motivated by the existence of an optimal embedding in hyperbolic space that can preserve data hierarchical relationships (termed Hierarchical Representation Capability, HRC) more accurately than Euclidean space. However, there is no evidence to suggest that HNNs can achieve this theoretical optimal embedding, leading to much research being built on flawed motivations. In this paper, we propose a benchmark for evaluating HRC and conduct a comprehensive analysis of why HNNs are effective through large-scale experiments. Inspired by the analysis results, we propose several pre-training strategies to enhance HRC and improve the performance of downstream tasks, further validating the reliability of the analysis. Experiments show that HNNs cannot achieve the theoretical optimal embedding. The HRC is significantly affected by the optimization objectives and hierarchical structures, and 
    
[^271]: TimeSiam：一种用于孪生时间序列建模的预训练框架

    TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling

    [https://arxiv.org/abs/2402.02475](https://arxiv.org/abs/2402.02475)

    TimeSiam是一种用于时间序列建模的预训练框架，通过使用Siamese网络和简单的数据增强方法，能够捕捉时间序列数据的内在时间相关性，并学习内部时序依赖的表示。

    

    最近，时间序列的预训练引起了广泛关注，因为它能够降低标注的成本并受益于各种下游任务。先前的方法主要基于在视觉或语言领域广为认可的预训练技术，如遮蔽建模和对比学习。然而，随机遮蔽时间序列或计算序列之间的相似性将导致丢失或忽视时间序列数据中关键的内在时间相关性。为了强调时间相关性建模，本文提出了一种名为TimeSiam的简单但有效的自监督预训练框架，用于基于孪生网络的时间序列。具体而言，TimeSiam对孪生编码器进行预训练，以捕捉随机采样的过去和当前子序列之间的内在时间相关性。通过简单的数据增强方法（例如遮蔽），TimeSiam可以从多样化的增强子序列中受益，并通过从过去到当前的重建来学习内部时序依赖的表示。此外，可学习的...

    Time series pre-training has recently garnered wide attention for its potential to reduce labeling expenses and benefit various downstream tasks. Prior methods are mainly based on pre-training techniques well-acknowledged in vision or language, such as masked modeling and contrastive learning. However, randomly masking time series or calculating series-wise similarity will distort or neglect inherent temporal correlations crucial in time series data. To emphasize temporal correlation modeling, this paper proposes TimeSiam as a simple but effective self-supervised pre-training framework for Time series based on Siamese networks. Concretely, TimeSiam pre-trains Siamese encoders to capture intrinsic temporal correlations between randomly sampled past and current subseries. With a simple data augmentation method (e.g.~masking), TimeSiam can benefit from diverse augmented subseries and learn internal time-dependent representations through a past-to-current reconstruction. Moreover, learnabl
    
[^272]: 基于上下文感知探索的快速适应未知同伴

    Fast Peer Adaptation with Context-aware Exploration

    [https://arxiv.org/abs/2402.02468](https://arxiv.org/abs/2402.02468)

    本文提出了一种基于上下文感知的探索方法，用于快速适应具有不同策略的未知同伴。通过奖励智能体在历史上下文中有效识别同伴行为模式，该方法能够促进智能体积极探索和快速适应，从而在不确定同伴策略时收集信息反馈，并在有信心时利用上下文执行最佳反应。

    

    在多智能体游戏中，快速适应具有不同策略的未知同伴是一个关键挑战。为了做到这一点，智能体能够高效地探索和识别同伴的策略至关重要，因为这是适应中进行最佳反应的先决条件。然而，当游戏是部分可观测且时间跨度很长时，探索未知同伴的策略是困难的。在本文中，我们提出了一种同伴识别奖励，根据智能体在历史环境下（例如多个回合的观察）如何有效地识别同伴的行为模式来奖励学习智能体。这个奖励激励智能体学习一种基于上下文的策略，以实现有效探索和快速适应，即在对同伴策略不确定时积极寻找和收集信息反馈，并在有信心时利用上下文执行最佳反应。我们在不同的测试场景上进行了评估。

    Fast adapting to unknown peers (partners or opponents) with different strategies is a key challenge in multi-agent games. To do so, it is crucial for the agent to efficiently probe and identify the peer's strategy, as this is the prerequisite for carrying out the best response in adaptation. However, it is difficult to explore the strategies of unknown peers, especially when the games are partially observable and have a long horizon. In this paper, we propose a peer identification reward, which rewards the learning agent based on how well it can identify the behavior pattern of the peer over the historical context, such as the observation over multiple episodes. This reward motivates the agent to learn a context-aware policy for effective exploration and fast adaptation, i.e., to actively seek and collect informative feedback from peers when uncertain about their policies and to exploit the context to perform the best response when confident. We evaluate our method on diverse testbeds 
    
[^273]: 一张图值千言：使用纯Transformer将图形欧拉化

    A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer

    [https://arxiv.org/abs/2402.02464](https://arxiv.org/abs/2402.02464)

    这篇论文介绍了GraphsGPT，它使用纯Transformer将非欧几里德图形转换为在欧几里德空间中可学习的图形单词，并通过解码器将图形单词重新构建为原始图形，保证了信息的等价性。预训练的GraphsGPT在图形表示学习和图形生成方面取得了突出成果。

    

    我们能否将非欧几里德图形建模为纯语言甚至欧几里德向量，同时保留其固有信息？非欧几里德性质一直是图形建模中的长期挑战。尽管最近的GNN和Graphformer努力将图形编码为欧几里德向量，但从向量中恢复出原始图形仍然是一个挑战。我们引入了GraphsGPT，它具有一个将非欧几里德图形转换为在欧几里德空间中可学习图形单词的Graph2Seq编码器，以及一个从图形单词重构原始图形以确保信息等价性的GraphGPT解码器。我们在100M个分子上预训练了GraphsGPT，并得到一些有趣的发现：(1) 预训练的Graph2Seq在图形表示学习方面表现出色，在8/9个图形分类和回归任务上取得了最新成果。(2) 预训练的GraphGPT作为一个强大的图形生成器，其能够进行无条件和有条件的图形生成。(3) Graph2Seq+Gr

    Can we model non-Euclidean graphs as pure language or even Euclidean vectors while retaining their inherent information? The non-Euclidean property have posed a long term challenge in graph modeling. Despite recent GNN and Graphformer efforts encoding graphs as Euclidean vectors, recovering original graph from the vectors remains a challenge. We introduce GraphsGPT, featuring a Graph2Seq encoder that transforms non-Euclidean graphs into learnable graph words in a Euclidean space, along with a GraphGPT decoder that reconstructs the original graph from graph words to ensure information equivalence. We pretrain GraphsGPT on 100M molecules and yield some interesting findings: (1) Pretrained Graph2Seq excels in graph representation learning, achieving state-of-the-art results on 8/9 graph classification and regression tasks. (2) Pretrained GraphGPT serves as a strong graph generator, demonstrated by its ability to perform both unconditional and conditional graph generation. (3) Graph2Seq+Gr
    
[^274]: 一种快速的Lasso和Logistic Lasso方法

    A Fast Method for Lasso and Logistic Lasso

    [https://arxiv.org/abs/2402.02463](https://arxiv.org/abs/2402.02463)

    本论文提出了一种快速解决Lasso和Logistic Lasso问题的方法，通过采用主动集方法和适当的求解器，成功实现了加速。在压缩感知、Lasso回归和Logistic Lasso回归实验中，与传统方法相比，我们的方法平均能提高约30倍的速度。

    

    我们提出了一种快速解决压缩感知、Lasso回归和Logistic Lasso回归问题的方法，该方法使用主动集方法迭代运行适当的求解器。我们设计了一种更新主动集的策略，相比于单次调用多个求解器（包括Matlab的lassoglm和glmnet以及用于稀疏重构的梯度投影算法GPSR），能够实现大幅加速。对于压缩感知，我们的方法与GPSR的混合平均速度提高了31.41倍（对于高斯系列）和25.64倍（对于二进制系列）。在Lasso回归实验中，我们的方法与GPSR的混合平均速度提高了30.67倍。在Logistic Lasso回归的实验中，我们的方法与lassoglm的混合平均速度提高了11.95倍，与glmnet的混合平均速度提高了1.40倍。

    We propose a fast method for solving compressed sensing, Lasso regression, and Logistic Lasso regression problems that iteratively runs an appropriate solver using an active set approach. We design a strategy to update the active set that achieves a large speedup over a single call of several solvers, including gradient projection for sparse reconstruction (GPSR), lassoglm of Matlab, and glmnet. For compressed sensing, the hybrid of our method and GPSR is 31.41 times faster than GPSR on average for Gaussian ensembles and 25.64 faster on average for binary ensembles. For Lasso regression, the hybrid of our method and GPSR achieves a 30.67-fold average speedup in our experiments. In our experiments on Logistic Lasso regression, the hybrid of our method and lassoglm gives an 11.95-fold average speedup, and the hybrid of our method and glmnet gives a 1.40-fold average speedup.
    
[^275]: 医疗保健中多模态机器学习方法的综述

    Review of multimodal machine learning approaches in healthcare

    [https://arxiv.org/abs/2402.02460](https://arxiv.org/abs/2402.02460)

    这篇综述主要介绍医疗保健领域中多模态机器学习方法的最新研究进展。通过综合分析最近的文献，探讨了临床诊断中各种数据模态的应用以及融合技术的评估。重点关注影像数据的应用，并介绍了现有的多模态数据集和训练方法。

    

    在医疗保健领域，机器学习方法传统上注重使用单一模态的数据，限制了其有效复制临床实践中整合多种信息来源以改善决策的能力。临床医生通常依赖各种数据来源，包括患者的人口统计信息、实验室数据、生命体征和各种影像数据模态来做出明智的决策并对其发现进行上下文化。机器学习的最新进展促进了多模态数据的更高效融合，从而产生更好地代表医生方法的应用。在这里，我们提供了医疗保健中多模态机器学习方法的综述，全面概述了最近的文献。我们讨论了临床诊断中使用的各种数据模态，特别强调影像数据。我们评估了融合技术，探索了现有的多模态数据集，以及研究常见的训练方法。

    Machine learning methods in healthcare have traditionally focused on using data from a single modality, limiting their ability to effectively replicate the clinical practice of integrating multiple sources of information for improved decision making. Clinicians typically rely on a variety of data sources including patients' demographic information, laboratory data, vital signs and various imaging data modalities to make informed decisions and contextualise their findings. Recent advances in machine learning have facilitated the more efficient incorporation of multimodal data, resulting in applications that better represent the clinician's approach. Here, we provide a review of multimodal machine learning approaches in healthcare, offering a comprehensive overview of recent literature. We discuss the various data modalities used in clinical diagnosis, with a particular emphasis on imaging data. We evaluate fusion techniques, explore existing multimodal datasets and examine common traini
    
[^276]: 在最小化迹因子分析中 - 这首老歌以新的方式演唱

    On Minimum Trace Factor Analysis - An Old Song Sung to a New Tune

    [https://arxiv.org/abs/2402.02459](https://arxiv.org/abs/2402.02459)

    本文提出了最小化迹因子分析（MTFA）的放松版本，该方法能够有效降低因异方差噪声造成的过拟合问题，并解决了在因子分析和谱方法中常见的异常情况和病态诅咒问题。

    

    维度降低方法，例如主成分分析（PCA）和因子分析，在数据科学中是很常用的。然而，对于具有显著异方差噪声的数据，寻找稳健的低维逼近存在明显且被广泛接受的挑战。本文介绍了最小化迹因子分析（MTFA）的放松版本，这是一种凸优化方法，其根源可以追溯到1940年Ledermann的工作。这种放松方法在不过度拟合异方差扰动方面特别有效，解决了因素分析中经常被提到的Heywood案例和最近发现的现有谱方法中"病态诅咒"问题。我们在所得低秩子空间的精确度和所提算法的收敛速度上提供了理论保证。我们发现了与现有方法（包括HeteroPCA，Lasso和Soft-Impute）的一些有趣联系，以填补一些空白。

    Dimensionality reduction methods, such as principal component analysis (PCA) and factor analysis, are central to many problems in data science. There are, however, serious and well-understood challenges to finding robust low dimensional approximations for data with significant heteroskedastic noise. This paper introduces a relaxed version of Minimum Trace Factor Analysis (MTFA), a convex optimization method with roots dating back to the work of Ledermann in 1940. This relaxation is particularly effective at not overfitting to heteroskedastic perturbations and addresses the commonly cited Heywood cases in factor analysis and the recently identified "curse of ill-conditioning" for existing spectral methods. We provide theoretical guarantees on the accuracy of the resulting low rank subspace and the convergence rate of the proposed algorithm to compute that matrix. We develop a number of interesting connections to existing methods, including HeteroPCA, Lasso, and Soft-Impute, to fill an i
    
[^277]: 通过大型语言模型（LLMs）发现更有效的张量网络结构搜索算法

    Discovering More Effective Tensor Network Structure Search Algorithms via Large Language Models (LLMs)

    [https://arxiv.org/abs/2402.02456](https://arxiv.org/abs/2402.02456)

    通过大型语言模型（LLMs），我们开发了GPTN-SS算法，用于自动设计更有效的张量网络结构搜索算法。实验证明，这些算法在探索和开发之间取得了更好的平衡，并在搜索高质量的TN结构方面展现出卓越的性能。

    

    张量网络结构搜索（TN-SS）旨在搜索适合表示高维问题的张量网络（TN）结构，极大地促进了TN在各种机器学习应用中的效果。然而，使用现有算法找到满意的TN结构仍然具有挑战性。为了开发更有效的算法并避免人力密集型的开发过程，我们利用大型语言模型（LLMs）中嵌入的知识来自动设计TN-SS算法。我们的方法称为GPTN-SS，利用了一种精心设计的基于LLM的提示系统，以类似进化的方式运行。从真实数据中得出的实验结果表明，GPTN-SS可以有效地利用现有方法获得的见解，开发出更好地平衡探索和开发之间关系的新型TN-SS算法。这些算法在搜索高质量TN结构方面表现出优秀的性能。

    Tensor network structure search (TN-SS), aiming at searching for suitable tensor network (TN) structures in representing high-dimensional problems, largely promotes the efficacy of TN in various machine learning applications. Nonetheless, finding a satisfactory TN structure using existing algorithms remains challenging. To develop more effective algorithms and avoid the human labor-intensive development process, we explore the knowledge embedded in large language models (LLMs) for the automatic design of TN-SS algorithms. Our approach, dubbed GPTN-SS, leverages an elaborate crafting LLM-based prompting system that operates in an evolutionary-like manner. The experimental results, derived from real-world data, demonstrate that GPTN-SS can effectively leverage the insights gained from existing methods to develop novel TN-SS algorithms that achieve a better balance between exploration and exploitation. These algorithms exhibit superior performance in searching the high-quality TN structur
    
[^278]: 关于初始化对深度线性网络中隐式偏差的作用

    On the Role of Initialization on the Implicit Bias in Deep Linear Networks

    [https://arxiv.org/abs/2402.02454](https://arxiv.org/abs/2402.02454)

    本研究探索了深度学习中隐式偏差的起源，重点研究了由权重初始化引起的隐式偏差。研究结果揭示了初始化对优化和泛化的悖论的作用，为深度学习的性能提供了更全面的理解。

    

    尽管深度学习在实践中取得了成功，但我们对其有效性的理论了解仍然有限。一个显著的悖论是，传统观点不鼓励完美拟合数据，而深度神经网络则被设计成做这件事，然而它们能有效地泛化。本研究着重探索与此现象相关的隐式偏差。已经确定了各种隐式偏差的来源，例如步长、权重初始化、优化算法和参数数量。在这项工作中，我们专注于研究由权重初始化引起的隐式偏差。为此，我们在各种情况下研究了解决欠定线性系统的问题，评估了在使用深度网络解决这些系统时初始化对隐式正则化的影响。我们的研究结果阐明了初始化在优化和泛化悖论中的作用，有助于更全面地理解深度学习的性能。

    Despite Deep Learning's (DL) empirical success, our theoretical understanding of its efficacy remains limited. One notable paradox is that while conventional wisdom discourages perfect data fitting, deep neural networks are designed to do just that, yet they generalize effectively. This study focuses on exploring this phenomenon attributed to the implicit bias at play. Various sources of implicit bias have been identified, such as step size, weight initialization, optimization algorithm, and number of parameters. In this work, we focus on investigating the implicit bias originating from weight initialization. To this end, we examine the problem of solving underdetermined linear systems in various contexts, scrutinizing the impact of initialization on the implicit regularization when using deep networks to solve such systems. Our findings elucidate the role of initialization in the optimization and generalization paradoxes, contributing to a more comprehensive understanding of DL's perf
    
[^279]: 在低资源场景下建模PoS标记器的研究

    Surfing the modeling of PoS taggers in low-resource scenarios

    [https://arxiv.org/abs/2402.02449](https://arxiv.org/abs/2402.02449)

    在低资源实验场景中，我们评估了早期学习曲线估计作为选择最合适模型的实用方法，并研究了在资源贫乏环境中的可靠性。

    

    最近深度结构技术在自然语言处理中的应用趋势揭示了庞大模型的局限性。这使得传统机器学习算法重新引起了人们的兴趣，在特定情境下仍然表现出竞争力，特别是在低资源环境设置中。同时，模型选择已成为一个重要任务，以在合理成本内提升性能，尤其是当涉及到训练和/或计算资源稀缺的领域时。在这种背景下，我们评估早期学习曲线估计作为在资源贫乏环境中选择最合适模型的实用机制。基于先前在训练和验证资源充足条件下评估的形式化逼近模型，我们研究了这种方法在不同且更具挑战性的操作环境中的可靠性。

    The recent trend towards the application of deep structured techniques has revealed the limits of huge models in natural language processing. This has reawakened the interest in traditional machine learning algorithms, which have proved still to be competitive in certain contexts, in particular low-resource settings. In parallel, model selection has become an essential task to boost performance at reasonable cost, even more so when we talk about processes involving domains where the training and/or computational resources are scarce. Against this backdrop, we evaluate the early estimation of learning curves as a practical mechanism for selecting the most appropriate model in scenarios characterized by the use of non-deep learners in resource-lean settings. On the basis of a formal approximation model previously evaluated under conditions of wide availability of training and validation resources, we study the reliability of such an approach in a different and much more demanding operati
    
[^280]: 打破MLPerf训练：优化BERT的案例研究

    Breaking MLPerf Training: A Case Study on Optimizing BERT

    [https://arxiv.org/abs/2402.02447](https://arxiv.org/abs/2402.02447)

    通过改进负载平衡、通信和优化器等各个组件，我们提出了用于快速大规模训练BERT模型的新方法，实现了新水平的BERT训练性能。

    

    加速大规模分布式训练具有挑战性，需要改进包括负载平衡、通信、优化器等训练的各个组件。我们提出了用于快速大规模训练BERT模型的新方法，通过改进每个组件，从而实现了BERT训练性能的新水平。在分布式BERT训练中，负载平衡至关重要，因为其训练数据集根据不同长度的样本进行了特征化。与分布式训练的规模成正比的通信成本需要通过有用的计算来隐藏。此外，优化器，如ADAM、LAMB等，需要在大规模分布式训练的背景下进行仔细重新评估。我们提出了两个新想法，即基于数据集分层的本地预排序进行负载平衡和全约减之前的按桶梯度裁剪，从而使我们能够从梯度计算和同步的重叠中获益。

    Speeding up the large-scale distributed training is challenging in that it requires improving various components of training including load balancing, communication, optimizers, etc. We present novel approaches for fast large-scale training of BERT model which individually ameliorates each component thereby leading to a new level of BERT training performance. Load balancing is imperative in distributed BERT training since its training datasets are characterized by samples with various lengths. Communication cost, which is proportional to the scale of distributed training, needs to be hidden by useful computation. In addition, the optimizers, e.g., ADAM, LAMB, etc., need to be carefully re-evaluated in the context of large-scale distributed training. We propose two new ideas, (1) local presorting based on dataset stratification for load balancing and (2) bucket-wise gradient clipping before allreduce which allows us to benefit from the overlap of gradient computation and synchronization
    
[^281]: LQER: 低秩量化误差重建用于LLMs

    LQER: Low-Rank Quantization Error Reconstruction for LLMs

    [https://arxiv.org/abs/2402.02446](https://arxiv.org/abs/2402.02446)

    LQER使用低秩逼近和激活引起的尺度矩阵，实现了对LLMs的近乎无损量化，无需知识蒸馏或梯度优化，并大幅减少硬件资源的使用。

    

    大型语言模型（LLMs）的训练后量化是具有挑战性的。在这项工作中，我们介绍了低秩量化误差减少（LQER）方法，该方法结合了量化和低秩逼近来恢复模型的能力。LQER利用激活引起的尺度矩阵将量化误差的奇异值分布推向期望的分布，从而实现了在各种LLMs和下游任务上近乎无损的W4A8量化，无需知识蒸馏、网格搜索或基于梯度的迭代优化。与现有方法不同，LQER的计算模式消除了从不规则内存位置收集高精度权重所需的专用Scatter和Gather过程。我们的W4A8 LLMs在六个热门下游任务上实现了近乎无损的性能，同时使用的硬件资源比领先的最新方法少1.36倍。一旦论文被接受，我们将开源我们的框架。

    Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\times$ fewer hardware resources than the leading state-of-the-art method. We will open-source our framework once the paper is accepted.
    
[^282]: BECLR：增强批量对比式少样本学习

    BECLR: Batch Enhanced Contrastive Few-Shot Learning

    [https://arxiv.org/abs/2402.02444](https://arxiv.org/abs/2402.02444)

    本论文提出了一种增强的批量对比式少样本学习方法（BECLR），通过引入动态聚类内存（DyCE）模块和迭代最优传输的分布对齐策略（OpTA），在预训练和推理阶段分别解决了正样本采样和样本偏差问题，提高了无监督少样本学习性能。

    

    在深度表示学习时代，快速从很少的标记样本中学习是区分机器和人类的基本特点。无监督少样本学习（U-FSL）希望通过在训练时丢弃对注释的依赖来弥合这一差距。受对比学习方法在U-FSL领域的成功启发，我们从预训练和后续推理阶段结构性地解决了它们的缺点。我们提出了一种新颖的动态聚类内存（DyCE）模块，以提升预训练阶段正样本采样的高度可分离潜在表示空间，并将隐式类别级别的洞察力融入无监督对比学习中。然后，我们解决了一直被忽视但却至关重要的少样本推理阶段的样本偏差问题。我们提出了一种基于迭代最优传输的分布对齐（OpTA）策略，并证明它能够有效解决这个问题，特别是在低样本场景下。

    Learning quickly from very few labeled samples is a fundamental attribute that separates machines and humans in the era of deep representation learning. Unsupervised few-shot learning (U-FSL) aspires to bridge this gap by discarding the reliance on annotations at training time. Intrigued by the success of contrastive learning approaches in the realm of U-FSL, we structurally approach their shortcomings in both pretraining and downstream inference stages. We propose a novel Dynamic Clustered mEmory (DyCE) module to promote a highly separable latent representation space for enhancing positive sampling at the pretraining phase and infusing implicit class-level insights into unsupervised contrastive learning. We then tackle the, somehow overlooked yet critical, issue of sample bias at the few-shot inference stage. We propose an iterative Optimal Transport-based distribution Alignment (OpTA) strategy and demonstrate that it efficiently addresses the problem, especially in low-shot scenarios
    
[^283]: 一种用于基于ReLU的非线性矩阵分解的动量加速算法

    A Momentum Accelerated Algorithm for ReLU-based Nonlinear Matrix Decomposition

    [https://arxiv.org/abs/2402.02442](https://arxiv.org/abs/2402.02442)

    提出了一种针对基于ReLU的非线性矩阵分解问题的动量加速算法，解决了现有模型中过拟合的问题，并通过实验验证了算法的有效性。

    

    最近，由于与神经网络的密切联系，人们对非线性矩阵分解（NMD）的探索日益增多。NMD旨在从稀疏的非负矩阵中找到一个低秩矩阵，其中每个元素都经过非线性函数，典型的选择是ReLU激活函数。为了解决现有的ReLU-NMD模型中的过拟合问题，我们提出了一种Tikhonov正则化的ReLU-NMD模型，称为ReLU-NMD-T。随后，我们引入了一种用于处理ReLU-NMD-T模型的动量加速算法。与大多数现有研究工作的一个显著特点是，我们的算法同时结合了正动量和负动量参数。我们对实际数据集进行的数值实验表明了所提出模型和算法的有效性。此外，代码可在https://github.com/nothing2wang/NMD-TM找到。

    Recently, there has been a growing interest in the exploration of Nonlinear Matrix Decomposition (NMD) due to its close ties with neural networks. NMD aims to find a low-rank matrix from a sparse nonnegative matrix with a per-element nonlinear function. A typical choice is the Rectified Linear Unit (ReLU) activation function. To address over-fitting in the existing ReLU-based NMD model (ReLU-NMD), we propose a Tikhonov regularized ReLU-NMD model, referred to as ReLU-NMD-T. Subsequently, we introduce a momentum accelerated algorithm for handling the ReLU-NMD-T model. A distinctive feature, setting our work apart from most existing studies, is the incorporation of both positive and negative momentum parameters in our algorithm. Our numerical experiments on real-world datasets show the effectiveness of the proposed model and algorithm. Moreover, the code is available at https://github.com/nothing2wang/NMD-TM.
    
[^284]: TopoX: 一个用于拓扑域上的机器学习的Python软件包套件

    TopoX: A Suite of Python Packages for Machine Learning on Topological Domains

    [https://arxiv.org/abs/2402.02441](https://arxiv.org/abs/2402.02441)

    TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。

    

    我们介绍了topox，一个提供可靠且用户友好的Python软件包套件，用于在拓扑域（扩展了图的领域）上进行计算和机器学习：超图、单纯、胞腔、路径和组合复合体。topox由三个软件包组成：toponetx用于构建和计算这些域，包括节点、边和高阶单元的处理；topoembedx提供了将拓扑域嵌入到向量空间的方法，类似于流行的基于图的嵌入算法，如node2vec；topomodelx建立在PyTorch之上，为拓扑域上的神经网络提供了一套全面的高阶消息传递功能工具箱。topox的源代码经过广泛的文档化和单元测试，并在https://github.com/pyt-team以MIT许可证的形式提供。

    We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
    
[^285]: DiffStitch: 使用基于扩散的轨迹拼接提升离线强化学习

    DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching

    [https://arxiv.org/abs/2402.02439](https://arxiv.org/abs/2402.02439)

    DiffStitch是一种使用基于扩散的轨迹拼接提升离线强化学习的方法。它通过有效地连接低奖励轨迹和高奖励轨迹，形成全局最优轨迹，以提高离线强化学习算法的性能。

    

    在离线强化学习中，学习策略的性能高度依赖于离线数据集的质量。然而，在许多情况下，离线数据集只包含了非常有限的最佳轨迹，这给离线强化学习算法带来了挑战，因为智能体必须获得到达高奖励区域的能力。为了解决这个问题，我们引入了基于扩散的轨迹拼接（DiffStitch），这是一个新颖的基于扩散的数据增强流水线，它可以系统地生成轨迹之间的拼接转换。DiffStitch可以有效地连接低奖励轨迹和高奖励轨迹，形成全局最优轨迹，以解决离线强化学习算法所面临的挑战。在D4RL数据集上进行的实证实验表明，DiffStitch在各种强化学习方法中都具有有效性。值得注意的是，DiffStitch在一步方法（IQL）、模仿学习方法（TD3+BC）和轨迹方法（PPO）的性能方面都有显著的改进。

    In offline reinforcement learning (RL), the performance of the learned policy highly depends on the quality of offline datasets. However, in many cases, the offline dataset contains very limited optimal trajectories, which poses a challenge for offline RL algorithms as agents must acquire the ability to transit to high-reward regions. To address this issue, we introduce Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data augmentation pipeline that systematically generates stitching transitions between trajectories. DiffStitch effectively connects low-reward trajectories with high-reward trajectories, forming globally optimal trajectories to address the challenges faced by offline RL algorithms. Empirical experiments conducted on D4RL datasets demonstrate the effectiveness of DiffStitch across RL methodologies. Notably, DiffStitch demonstrates substantial enhancements in the performance of one-step methods (IQL), imitation learning methods (TD3+BC), and traje
    
[^286]: 基于截断ANOVA分解的快速可解释支持向量分类法

    Fast and interpretable Support Vector Classification based on the truncated ANOVA decomposition

    [https://arxiv.org/abs/2402.02438](https://arxiv.org/abs/2402.02438)

    基于截断ANOVA分解的快速可解释支持向量分类法能够通过使用特征映射和少量维度的多变量基函数来快速且准确地进行高维散乱数据的分类。

    

    支持向量机（SVM）是在散乱数据上进行分类的重要工具，在高维空间中通常需要处理许多数据点。我们提出使用基于三角函数或小波的特征映射来解决SVM的原始形式。在小维度设置中，快速傅里叶变换（FFT）和相关方法是处理所考虑基函数的强大工具。随着维度的增长，由于维数灾难，传统的基于FFT的方法变得低效。因此，我们限制自己使用多变量基函数，每个基函数只依赖于少数几个维度。这是由于效应的稀疏性和最近关于函数从散乱数据中的截断方差分解的重建的结果所带来的动机，使得生成的模型在特征的重要性以及它们的耦合方面具有可解释性。

    Support Vector Machines (SVMs) are an important tool for performing classification on scattered data, where one usually has to deal with many data points in high-dimensional spaces. We propose solving SVMs in primal form using feature maps based on trigonometric functions or wavelets. In small dimensional settings the Fast Fourier Transform (FFT) and related methods are a powerful tool in order to deal with the considered basis functions. For growing dimensions the classical FFT-based methods become inefficient due to the curse of dimensionality. Therefore, we restrict ourselves to multivariate basis functions, each one of them depends only on a small number of dimensions. This is motivated by the well-known sparsity of effects and recent results regarding the reconstruction of functions from scattered data in terms of truncated analysis of variance (ANOVA) decomposition, which makes the resulting model even interpretable in terms of importance of the features as well as their coupling
    
[^287]: 不确定性感知者

    Uncertainty-Aware Perceiver

    [https://arxiv.org/abs/2402.02433](https://arxiv.org/abs/2402.02433)

    不确定性感知者是对感知者模型的五种变体，它们能够获得不确定性估计并在多个指标上取得显著的性能提升。

    

    感知者模型对于输入之间的关系没有过多的建模假设，其在内存和计算时间上具有二次可扩展性。事实上，感知者模型在准确度方面超过或与ResNet-50和ViT相竞争。然而，感知者模型没有考虑到预测的不确定性和校准问题。感知者模型在三个数据集、三个模型、一个评估指标和一个超参数设置上都具有一定的泛化性能。最糟糕的是，相对于其他模型，感知者模型的性能改进幅度很小。此外，感知者模型对于架构先验的减少并不实质性，并不能等同于其质量。因此，我发明了五种变体的感知者模型，即不确定性感知者，它们可以获得不确定性估计并在三个指标上进行测量。通过在CIFAR-10和CIFAR-100上进行实验，不确定性感知者相比感知者模型显著提高了性能。

    The Perceiver makes few architectural assumptions about the relationship among its inputs with quadratic scalability on its memory and computation time. Indeed, the Perceiver model outpaces or is competitive with ResNet-50 and ViT in terms of accuracy to some degree. However, the Perceiver does not take predictive uncertainty and calibration into account. The Perceiver also generalizes its performance on three datasets, three models, one evaluation metric, and one hyper-parameter setting. Worst of all, the Perceiver's relative performance improvement against other models is marginal. Furthermore, its reduction of architectural prior is not substantial; is not equivalent to its quality. Thereby, I invented five mutations of the Perceiver, the Uncertainty-Aware Perceivers, that obtain uncertainty estimates and measured their performance on three metrics. Experimented with CIFAR-10 and CIFAR-100, the Uncertainty-Aware Perceivers make considerable performance enhancement compared to the Pe
    
[^288]: 学习相互激励以实现手对手和人对人交互识别

    Learning Mutual Excitation for Hand-to-Hand and Human-to-Human Interaction Recognition

    [https://arxiv.org/abs/2402.02431](https://arxiv.org/abs/2402.02431)

    本文介绍了一种学习相互激励的图卷积网络（me-GCN），用于手对手和人对人交互识别。通过堆叠相互激励图卷积层（me-GC），该网络能够自适应地建模成对实体之间的互相约束，并提取和合并深度特征。

    

    识别交互动作，包括手对手交互和人对人交互，在视频分析和人机交互领域具有广泛的应用。考虑到图卷积在建模骨骼数据的拓扑感知特征方面的成功，最近的方法通常将图卷积应用于独立实体，并在交互动作识别时使用后期融合，这几乎无法建模成对实体之间的互相语义关系。为此，我们通过堆叠相互激励图卷积（me-GC）层，提出了一种相互激励图卷积网络（me-GCN）。具体来说，me-GC使用相互拓扑激励模块首先从单个实体中提取邻接矩阵，然后自适应地对它们之间的相互约束进行建模。此外，me-GC进一步使用相互特征激励模块从成对实体中提取和合并深度特征。

    Recognizing interactive actions, including hand-to-hand interaction and human-to-human interaction, has attracted increasing attention for various applications in the field of video analysis and human-robot interaction. Considering the success of graph convolution in modeling topology-aware features from skeleton data, recent methods commonly operate graph convolution on separate entities and use late fusion for interactive action recognition, which can barely model the mutual semantic relationships between pairwise entities. To this end, we propose a mutual excitation graph convolutional network (me-GCN) by stacking mutual excitation graph convolution (me-GC) layers. Specifically, me-GC uses a mutual topology excitation module to firstly extract adjacency matrices from individual entities and then adaptively model the mutual constraints between them. Moreover, me-GC extends the above idea and further uses a mutual feature excitation module to extract and merge deep features from pairw
    
[^289]: 利用低层次表示进行超快速道路分割

    Exploiting Low-level Representations for Ultra-Fast Road Segmentation

    [https://arxiv.org/abs/2402.02430](https://arxiv.org/abs/2402.02430)

    本研究利用低层次特征表示进行道路分割，在主流网络模型的主要阶段实现了大部分道路像素的准确表示，提出了以低层次特征为主导的道路分割网络（LFD-RoadSeg）。

    

    实现嵌入式平台上的实时性和准确性一直是道路分割方法的追求。为此，他们提出了许多轻量级网络。然而，他们忽视了道路是“物质”（背景或环境元素）而不是“东西”（特定可识别的对象）的事实，这激发了我们探索用低层次特征而不是高层次特征来表示道路的可行性。令人惊讶的是，我们发现主流网络模型的主要阶段足以表示大部分像素的道路进行分割。在此基础上，我们提出了一个以低层次特征为主导的道路分割网络（LFD-RoadSeg）。具体而言，LFD-RoadSeg采用了双边结构。首先设计了空间细节分支，通过ResNet-18的第一阶段提取道路的低层次特征表示。然后设计了上下文语义分支，以抑制低层次特征中错误地将无纹理区域误认为道路。

    Achieving real-time and accuracy on embedded platforms has always been the pursuit of road segmentation methods. To this end, they have proposed many lightweight networks. However, they ignore the fact that roads are "stuff" (background or environmental elements) rather than "things" (specific identifiable objects), which inspires us to explore the feasibility of representing roads with low-level instead of high-level features. Surprisingly, we find that the primary stage of mainstream network models is sufficient to represent most pixels of the road for segmentation. Motivated by this, we propose a Low-level Feature Dominated Road Segmentation network (LFD-RoadSeg). Specifically, LFD-RoadSeg employs a bilateral structure. The spatial detail branch is firstly designed to extract low-level feature representation for the road by the first stage of ResNet-18. To suppress texture-less regions mistaken as the road in the low-level feature, the context semantic branch is then designed to ext
    
[^290]: 朝着基于信息理论的离线元强化学习的框架

    Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning

    [https://arxiv.org/abs/2402.02429](https://arxiv.org/abs/2402.02429)

    本研究提出了一个统一的信息理论框架，将离线元强化学习中的不同方法整合起来，并提出了一种新算法UNICORN，展现了在广泛的任务上显著的泛化能力。

    

    离线元强化学习（OMRL）作为离线RL和元RL的结合，在实现RL智能体进行多任务学习和快速适应以及安全获取知识方面表现出巨大的潜力。其中，基于上下文的OMRL（COMRL）作为一种流行的范式，旨在学习一个基于有效任务表示的通用策略。在本文中，通过研究COMRL领域的几个关键里程碑，我们提议将这些看似独立的方法整合到一个统一的信息理论框架中。最重要的是，我们展示了现有的COMRL算法本质上是通过实现各种近似界限来优化任务变量$\boldsymbol{M}$和其潜在表示$\boldsymbol{Z}$之间的相互信息目标。基于理论洞察力和信息瓶颈原理，我们提出了一种新算法UNICORN，展现了在广泛的R问题谱上的显著泛化能力。

    As a marriage between offline RL and meta-RL, the advent of offline meta-reinforcement learning (OMRL) has shown great promise in enabling RL agents to multi-task and quickly adapt while acquiring knowledge safely. Among which, Context-based OMRL (COMRL) as a popular paradigm, aims to learn a universal policy conditioned on effective task representations. In this work, by examining several key milestones in the field of COMRL, we propose to integrate these seemingly independent methodologies into a unified information theoretic framework. Most importantly, we show that the pre-existing COMRL algorithms are essentially optimizing the same mutual information objective between the task variable $\boldsymbol{M}$ and its latent representation $\boldsymbol{Z}$ by implementing various approximate bounds. Based on the theoretical insight and the information bottleneck principle, we arrive at a novel algorithm dubbed UNICORN, which exhibits remarkable generalization across a broad spectrum of R
    
[^291]: 自动驾驶的混合预测综合规划

    Hybrid-Prediction Integrated Planning for Autonomous Driving

    [https://arxiv.org/abs/2402.02426](https://arxiv.org/abs/2402.02426)

    本论文介绍了一个混合预测综合规划（HPP）系统，通过引入边缘条件下的占用预测和博弈论运动预测器，解决了自动驾驶系统中预测和规划模块整合所面临的挑战。

    

    自动驾驶系统需要全面理解并预测周围环境，以在复杂场景中做出明智的决策。最近学习型系统的进展凸显了整合预测和规划模块的重要性。然而，这种整合带来了三个主要挑战：单一预测的固有权衡，预测模式之间的一致性，以及预测和规划中的社交一致性。为了解决这些挑战，我们引入了一个混合预测综合规划（HPP）系统，该系统具有三个创新设计的模块。首先，我们引入了边缘条件下的占用预测，以使联合占用与代理感知保持一致。我们提出的MS-OccFormer模块通过代理方向运动预测实现了多阶段的占用预测对齐。其次，我们提出了一个博弈论运动预测器GTFormer，来模拟个体之间的交互未来。

    Autonomous driving systems require the ability to fully understand and predict the surrounding environment to make informed decisions in complex scenarios. Recent advancements in learning-based systems have highlighted the importance of integrating prediction and planning modules. However, this integration has brought forth three major challenges: inherent trade-offs by sole prediction, consistency between prediction patterns, and social coherence in prediction and planning. To address these challenges, we introduce a hybrid-prediction integrated planning (HPP) system, which possesses three novelly designed modules. First, we introduce marginal-conditioned occupancy prediction to align joint occupancy with agent-wise perceptions. Our proposed MS-OccFormer module achieves multi-stage alignment per occupancy forecasting with consistent awareness from agent-wise motion predictions. Second, we propose a game-theoretic motion predictor, GTFormer, to model the interactive future among indivi
    
[^292]: EuLagNet: 拉格朗日动力学的欧拉预测

    EuLagNet: Eulerian Fluid Prediction with Lagrangian Dynamics

    [https://arxiv.org/abs/2402.02425](https://arxiv.org/abs/2402.02425)

    EuLagNet提出了一种新的拉格朗日引导范式，通过跟踪多尺度关键粒子的运动来捕捉多尺度流体动力学。这种方法克服了由于欧拉观察而导致的流体动力学困难，为准确预测未来的流体提供了一种有效的方法。

    

    准确预测未来的流体对气象学、海洋学和空气动力学等广泛领域至关重要。然而，由于流体通常从欧拉角度观察，其活跃和复杂的动力学在静止的网格中严重被掩盖和混淆，给预测带来了巨大挑战。本文引入了一种新的拉格朗日引导范式来解决复杂的流体动力学。我们提出了以拉格朗日动力学为导向的欧拉-拉格朗日双重递归网络（EuLagNet），通过跟踪自适应采样的多尺度关键粒子的运动并随时间积累动力学信息来捕捉多尺度流体动力学。具体地，我们提出了一个EuLag块，用于在每个时刻和尺度上传递学习到的欧拉和拉格朗日特征，其中跟踪粒子的运动是从欧拉观察中推断出来的，它们积累的动力学信息被纳入到预测模型中。

    Accurately predicting the future fluid is important to extensive areas, such as meteorology, oceanology and aerodynamics. However, since the fluid is usually observed from an Eulerian perspective, its active and intricate dynamics are seriously obscured and confounded in static grids, bringing horny challenges to the prediction. This paper introduces a new Lagrangian-guided paradigm to tackle the tanglesome fluid dynamics. Instead of solely predicting the future based on Eulerian observations, we propose the Eulerian-Lagrangian Dual Recurrent Network (EuLagNet), which captures multiscale fluid dynamics by tracking movements of adaptively sampled key particles on multiple scales and integrating dynamics information over time. Concretely, a EuLag Block is presented to communicate the learned Eulerian and Lagrangian features at each moment and scale, where the motion of tracked particles is inferred from Eulerian observations and their accumulated dynamics information is incorporated into
    
[^293]: Uni-RLHF: 用于多样化人类反馈的强化学习通用平台和基准套件

    Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback

    [https://arxiv.org/abs/2402.02423](https://arxiv.org/abs/2402.02423)

    Uni-RLHF是一个通用的强化学习平台和基准套件，致力于处理多样化的人类反馈，解决了在RLHF中量化进展的挑战，并提供了用户友好的注释界面和离线基准实现。

    

    强化学习与人类反馈（RLHF）通过与人类偏好对齐，避免了昂贵的手动奖励设计，已经受到了广泛关注。考虑到不同环境中不同学习方法和多样化的人类反馈类型对RLHF的进步进行量化是具有挑战性的，因为缺乏标准化的注释平台和广泛使用的统一基准。为了填补这个空白，我们引入了Uni-RLHF，这是一个为RLHF量身定制的综合系统实现。它旨在提供一个完整的从真实人类反馈到实际问题发展的工作流。Uni-RLHF包含三个部分：1）通用的多反馈注释平台，2）大规模的众包反馈数据集，3）模块化的离线RLHF基准实现。Uni-RLHF开发了一个用户友好的注释界面，适用于各种反馈类型，并与主要的强化学习框架兼容。

    Reinforcement Learning with Human Feedback (RLHF) has received significant attention for performing tasks without the need for costly manual reward design by aligning human preferences. It is crucial to consider diverse human feedback types and various learning methods in different environments. However, quantifying progress in RLHF with diverse feedback is challenging due to the lack of standardized annotation platforms and widely used unified benchmarks. To bridge this gap, we introduce Uni-RLHF, a comprehensive system implementation tailored for RLHF. It aims to provide a complete workflow from real human feedback, fostering progress in the development of practical problems. Uni-RLHF contains three packages: 1) a universal multi-feedback annotation platform, 2) large-scale crowdsourced feedback datasets, and 3) modular offline RLHF baseline implementations. Uni-RLHF develops a user-friendly annotation interface tailored to various feedback types, compatible with a wide range of main
    
[^294]: 可解释的贝叶斯多透视生成检索

    eXplainable Bayesian Multi-Perspective Generative Retrieval

    [https://arxiv.org/abs/2402.02418](https://arxiv.org/abs/2402.02418)

    本文将不确定性校准和可解释性整合到一个检索流程中，通过引入贝叶斯方法和多透视检索来校准不确定性。使用解释方法分析黑盒模型行为，并将重要性分数作为补充相关性分数，提升基础模型性能。实验证明我们的方法在问答和事实检验任务上显著提高了性能。

    

    现代确定性检索流水线注重实现最先进的性能，但在决策过程中通常缺乏可解释性。这些模型在评估不确定性时面临挑战，导致过于自信的预测。为了克服这些限制，我们将不确定性校准和可解释性整合到检索流程中。具体而言，我们引入了贝叶斯方法和多透视检索来校准检索流程中的不确定性。我们结合LIME和SHAP等技术来分析黑盒重排序模型的行为。从这些解释方法中得出的重要性分数作为补充相关性分数，以增强基础重排序模型。我们在问答和事实检验任务上评估通过不确定性校准和可解释性重排实现的性能提升。我们的方法在三个KILT数据集上展示出了明显的性能改善。

    Modern deterministic retrieval pipelines prioritize achieving state-of-the-art performance but often lack interpretability in decision-making. These models face challenges in assessing uncertainty, leading to overconfident predictions. To overcome these limitations, we integrate uncertainty calibration and interpretability into a retrieval pipeline. Specifically, we introduce Bayesian methodologies and multi-perspective retrieval to calibrate uncertainty within a retrieval pipeline. We incorporate techniques such as LIME and SHAP to analyze the behavior of a black-box reranker model. The importance scores derived from these explanation methodologies serve as supplementary relevance scores to enhance the base reranker model. We evaluate the resulting performance enhancements achieved through uncertainty calibration and interpretable reranking on Question Answering and Fact Checking tasks. Our methods demonstrate substantial performance improvements across three KILT datasets.
    
[^295]: Aligner: 通过弱到强校正实现高效对齐

    Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction

    [https://arxiv.org/abs/2402.02416](https://arxiv.org/abs/2402.02416)

    Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。

    

    对于大型语言模型（LLMs），通过强化学习来进行对齐的努力主要是通过人类反馈的强化学习方法进行的。然而，强化学习面临着主要的挑战，包括训练奖励模型、演员-评论家工程以及重要的是，需要访问LLM参数。在这里，我们介绍了一种新的高效对齐范式Aligner，它通过学习对齐和未对齐答案之间的校正残差来绕过整个强化学习过程。我们的Aligner具有几个关键优势。首先，它是一个基于自监督学习的自动回归seq2seq模型，通过训练查询-答案-校正数据集，提供了一种参数高效的对齐解决方案，并且对资源需求较少。其次，Aligner实现了从弱到强的泛化；通过Aligner的监督信号来微调大型预训练模型，可以显著提升性能。第三，Aligner作为一个模型不可知的即插即用模块，可以直接应用于…

    Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
    
[^296]: GLaPE：大型语言模型的无依赖于金标签的提示评估与优化

    GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language Model

    [https://arxiv.org/abs/2402.02408](https://arxiv.org/abs/2402.02408)

    GLaPE提出了一种无依赖于金标签的提示评估方法，通过自一致性作为初始评估分数，进一步改进了产生相同答案的提示的得分的互相一致性，提供了与准确性相一致的可靠评估，即使在没有金标签的情况下。

    

    尽管大型语言模型（LLMs）取得了快速进展，但它们的任务性能仍然对提示设计敏感。最近的研究探索了利用LLM自身作为优化器来识别最大化任务准确性的最优提示。然而，在评估提示时，这些方法严重依赖于难以获取的手动标注的金标签，以计算每个候选提示的任务准确性，这阻碍了广泛的实施和通用性。为了克服这一限制，本研究提出了一种无依赖于金标签的提示评估方法（GLaPE），以减少对金标签的依赖。受到自一致性和答案准确性之间的相关性的启发，我们将自一致性作为初始评估分数。随后，我们对产生相同答案的提示进行得分的互相一致性的改进。实验结果表明，GLaPE在没有金标签的情况下提供了与准确性相一致的可靠评估。此外，对于六个任务，GLaPE在绝大部分情况下得到的评估结果与使用真实金标签评估的结果相似。

    Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design. Recent studies have explored leveraging the LLM itself as an optimizer to identify optimal prompts that maximize task accuracy. However, when evaluating prompts, such approaches heavily rely on elusive manually annotated gold labels to calculate task accuracy for each candidate prompt, which hinders the widespread implementation and generality. To overcome the limitation, this work proposes a gold label-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold labels. Motivated by the observed correlation between self-consistency and the accuracy of the answer, we adopt self-consistency as the initial evaluation score. Subsequently, we refine the scores of prompts producing identical answers to be mutually consistent. Experimental results show that GLaPE provides reliable evaluations uniform with accuracy, even in the absence of gold labels. Moreover, on six p
    
[^297]: 通过数据集的多面体结构定义神经网络架构

    Defining Neural Network Architecture through Polytope Structures of Dataset

    [https://arxiv.org/abs/2402.02407](https://arxiv.org/abs/2402.02407)

    本文通过定义上下界确定神经网络宽度，与数据集的几何复杂性相关。同时开发了一种算法，可以从训练好的神经网络中推断数据集的多面体结构。

    

    当前神经网络的理论和实证研究表明，复杂的数据集需要大型网络架构进行彻底分类，然而这种关系的具体性质仍不清楚。本文通过定义神经网络宽度的上下界来解决这个问题，这些界限是由所讨论数据集的多面体结构所确定的。我们还深入探讨了这些原则在单纯复合体和特定多样曲面形状上的应用，解释了网络宽度需求如何根据数据集的几何复杂性而变化。此外，我们开发了一种算法来研究一种相反情况，即可以从相应的训练神经网络推断出数据集的多面体结构。通过我们的算法，我们确定了流行的数据集（如MNIST、Fashion-MNIST和CIFAR10）可以用只有少数面的两个多面体有效地表示。

    Current theoretical and empirical research in neural networks suggests that complex datasets require large network architectures for thorough classification, yet the precise nature of this relationship remains unclear. This paper tackles this issue by defining upper and lower bounds for neural network widths, which are informed by the polytope structure of the dataset in question. We also delve into the application of these principles to simplicial complexes and specific manifold shapes, explaining how the requirement for network width varies in accordance with the geometric complexity of the dataset. Moreover, we develop an algorithm to investigate a converse situation where the polytope structure of a dataset can be inferred from its corresponding trained neural networks. Through our algorithm, it is established that popular datasets such as MNIST, Fashion-MNIST, and CIFAR10 can be efficiently encapsulated using no more than two polytopes with a small number of faces.
    
[^298]: FreDF: 在频域中学习预测

    FreDF: Learning to Forecast in Frequency Domain

    [https://arxiv.org/abs/2402.02399](https://arxiv.org/abs/2402.02399)

    FreDF是一种在频域中学习预测的方法，解决了时间序列建模中标签序列的自相关问题，相比现有方法有更好的性能表现，并且与各种预测模型兼容。

    

    时间序列建模在历史序列和标签序列中都面临自相关的挑战。当前的研究主要集中在处理历史序列中的自相关问题，但往往忽视了标签序列中的自相关存在。具体来说，新兴的预测模型主要遵循直接预测（DF）范式，在标签序列中假设条件独立性下生成多步预测。这种假设忽视了标签序列中固有的自相关性，从而限制了基于DF的模型的性能。针对这一问题，我们引入了频域增强直接预测（FreDF），通过在频域中学习预测来避免标签自相关的复杂性。我们的实验证明，FreDF在性能上大大超过了包括iTransformer在内的现有最先进方法，并且与各种预测模型兼容。

    Time series modeling is uniquely challenged by the presence of autocorrelation in both historical and label sequences. Current research predominantly focuses on handling autocorrelation within the historical sequence but often neglects its presence in the label sequence. Specifically, emerging forecast models mainly conform to the direct forecast (DF) paradigm, generating multi-step forecasts under the assumption of conditional independence within the label sequence. This assumption disregards the inherent autocorrelation in the label sequence, thereby limiting the performance of DF-based models. In response to this gap, we introduce the Frequency-enhanced Direct Forecast (FreDF), which bypasses the complexity of label autocorrelation by learning to forecast in the frequency domain. Our experiments demonstrate that FreDF substantially outperforms existing state-of-the-art methods including iTransformer and is compatible with a variety of forecast models.
    
[^299]: DeLLMa:一个用于大型语言模型下决策的框架

    DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models

    [https://arxiv.org/abs/2402.02392](https://arxiv.org/abs/2402.02392)

    DeLLMa是一个旨在提高不确定环境下决策精度的框架，通过多步骤的脚手架程序，借鉴决策理论和效用理论的原则，可以显著提高大型语言模型的决策性能。

    

    大型语言模型（LLMs）在商业、工程和医学等领域被广泛应用，这些领域往往面临决策不确定性的问题，这是一个关键但具有挑战性的任务。本文表明，在决策问题上直接使用LLMs往往效果较差，尤其是在问题复杂性增加时。为了克服这个限制，我们提出了DeLLMa（Decision-making Large Language Model assistant）框架，旨在提高不确定环境下的决策精度。DeLLMa包括一个多步骤的脚手架程序，借鉴了决策理论和效用理论的原则，提供了一个最优的、可审计的决策过程。我们在涉及真实农业和金融数据的决策环境中验证了我们的框架。结果表明，DeLLMa可以显著提高LLMs的决策性能，准确性可提高高达40%以上。

    Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over co
    
[^300]: 使用辅助验证的迭代上下文学习生成面向解决方案的基于Agent的模型

    Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning

    [https://arxiv.org/abs/2402.02388](https://arxiv.org/abs/2402.02388)

    本文提出了SAGE，一个通用的面向解决方案的ABM生成框架，利用辅助验证和迭代上下文学习，自动生成针对特定问题的模型和解决方案。

    

    基于Agent的模型（ABM）作为一种重要的范式，用于提出和验证针对复杂系统所提出的假设解决方案或政策，并实现各种目标。这个过程需要大量的工作和跨学科的专业知识。具有跨领域知识和编程能力的大型语言模型（LLM）有潜力减轻这个过程的困难。然而，LLM擅长处理序列信息，这对于分析ABM中的复杂交互和非线性动力学来说是具有挑战性的。另外，由于LLM缺乏自我评估能力，仅仅依靠LLM是无法有效完成这个过程的。在本文中，我们提出了SAGE，这是一个通用的面向解决方案的ABM生成框架，用于自动生成针对特定问题的模型和解决方案。

    Agent-based models (ABMs) stand as an essential paradigm for proposing and validating hypothetical solutions or policies aimed at addressing challenges posed by complex systems and achieving various objectives. This process demands labor-intensive endeavors and multidisciplinary expertise. Large language models (LLMs) encapsulating cross-domain knowledge and programming proficiency could potentially alleviate the difficulty of this process. However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process. In this paper, we present SAGE, a general solution-oriented ABM generation framework designed for automatic modeling and generating solutions for targeted problems. Unlike approaches reliant on expert handcrafting or resource-intensive neural netw
    
[^301]: 重新审视视觉调整中提示词的力量

    Revisiting the Power of Prompt for Visual Tuning

    [https://arxiv.org/abs/2402.02382](https://arxiv.org/abs/2402.02382)

    本研究提出了一种改进的视觉调整方法，通过采用下游令牌原型对提示进行初始化，进一步优化令牌构造，有效解决了视觉提示调整中的挑战，并取得了比其他方法更好的性能。

    

    视觉提示调整（VPT）是一种很有前景的解决方案，它利用可学习的提示词来定制预训练模型，用于下游任务。然而，VPT及其变种经常遇到诸如提示初始化、提示长度和自监督预训练中性能不佳等挑战，阻碍了成功的上下文适应。本研究从探索训练过程中提示词与补丁令牌之间的相关性演变开始。受到提示令牌与补丁令牌之间往往具有高互信息的观察启发，我们提出利用下游令牌原型对提示进行初始化。该策略性初始化明显提高了微调性能，相比于VPT，无需增加计算开销，我们进一步优化令牌构造，使用简化的流程保持了出色的性能。详尽的实验表明，我们提出的方法胜过了其他方法。

    Visual prompt tuning (VPT) is a promising solution incorporating learnable prompt tokens to customize pre-trained models for downstream tasks. However, VPT and its variants often encounter challenges like prompt initialization, prompt length, and subpar performance in self-supervised pretraining, hindering successful contextual adaptation. This study commences by exploring the correlation evolvement between prompts and patch tokens during proficient training. Inspired by the observation that the prompt tokens tend to share high mutual information with patch tokens, we propose initializing prompts with downstream token prototypes. The strategic initialization, a stand-in for the previous initialization, substantially improves performance in fine-tuning. To refine further, we optimize token construction with a streamlined pipeline that maintains excellent performance with almost no increase in computational expenses compared to VPT. Exhaustive experiments show our proposed approach outpe
    
[^302]: NOAH: 学习用于图像分类的成对对象类别注意力

    NOAH: Learning Pairwise Object Category Attentions for Image Classification

    [https://arxiv.org/abs/2402.02377](https://arxiv.org/abs/2402.02377)

    NOAH是一种新型的深度神经网络头部结构，通过使用成对对象类别注意力(POCA)来增强分类性能。它能够替代现有的DNN头部，提高分类准确率，并保持模型效率。

    

    现代图像分类任务的深度神经网络(DNN)通常由两部分组成：用于特征提取的主干和用于特征编码和类别预测的头部。我们观察到，主流DNN的头部结构采用类似的特征编码流水线，利用全局特征依赖性而忽略了局部特征。在本文中，我们重新审视了特征编码问题，并提出了一种依赖于称为成对对象类别注意力(POCA)的新型点积注意力的Non-glObal Attentive Head (NOAH)，有效地利用密集的特定类别的空间注意力来增强分类性能。NOAH引入了一种整洁的特征分割、转换和合并操作的组合，以在局部到全局范围内学习POCA。作为一种易于替代各种类型DNN现有头部的设计，NOAH能够在保持类似的模型效率的同时提高分类性能。我们验证了其有效性。

    A modern deep neural network (DNN) for image classification tasks typically consists of two parts: a backbone for feature extraction, and a head for feature encoding and class predication. We observe that the head structures of mainstream DNNs adopt a similar feature encoding pipeline, exploiting global feature dependencies while disregarding local ones. In this paper, we revisit the feature encoding problem, and propose Non-glObal Attentive Head (NOAH) that relies on a new form of dot-product attention called pairwise object category attention (POCA), efficiently exploiting spatially dense category-specific attentions to augment classification performance. NOAH introduces a neat combination of feature split, transform and merge operations to learn POCAs at local to global scales. As a drop-in design, NOAH can be easily used to replace existing heads of various types of DNNs, improving classification performance while maintaining similar model efficiency. We validate the effectiveness 
    
[^303]: AutoTimes: 基于大型语言模型的自回归时间序列预测器

    AutoTimes: Autoregressive Time Series Forecasters via Large Language Models

    [https://arxiv.org/abs/2402.02370](https://arxiv.org/abs/2402.02370)

    AutoTimes是一种基于大型语言模型的自回归时间序列预测器，利用语言模型的转换能力来处理时间序列数据，实现了与先前模型相当的性能。

    

    由于大规模时间序列的有限可用性和可扩展预训练的不充分探索，时间序列的基础模型尚未完全发展。基于时间序列和自然语言的相似顺序结构，越来越多的研究证明了利用大型语言模型(LLM)进行时间序列的可行性。然而，先前的方法可能忽视了时间序列和自然语言对齐的一致性，导致对LLM潜力的利用不足。为了充分利用从语言建模中学到的通用令牌转换，我们提出了AutoTimes，将LLM重新用作自回归时间序列预测器，这与LLM的获取和利用一致，而无需更新参数。由此产生的预测器可以处理灵活的系列长度，并实现与流行模型相当的性能。此外，我们提出了基于令牌的提示方法，利用相应的时间戳来进行预测。

    Foundation models of time series have not been fully developed due to the limited availability of large-scale time series and the underexploration of scalable pre-training. Based on the similar sequential structure of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, prior methods may overlook the consistency in aligning time series and natural language, resulting in insufficient utilization of the LLM potentials. To fully exploit the general-purpose token transitions learned from language modeling, we propose AutoTimes to repurpose LLMs as Autoregressive Time series forecasters, which is consistent with the acquisition and utilization of LLMs without updating the parameters. The consequent forecasters can handle flexible series lengths and achieve competitive performance as prevalent models. Further, we present token-wise prompting that utilizes corresponding timestamps to make ou
    
[^304]: 计时器: 用于大规模时间序列分析的Transformer模型

    Timer: Transformers for Time Series Analysis at Scale

    [https://arxiv.org/abs/2402.02368](https://arxiv.org/abs/2402.02368)

    本文旨在早期开发大规模时间序列模型（LTSM），通过预训练和GPT风格架构，克服深度模型在小样本场景中的性能瓶颈，并实现在时间序列分析中的大样本泛化能力、可扩展性和任务普适性。

    

    深度学习在时间序列分析方面做出了显著贡献。然而，在现实世界的小样本场景中，深度模型可能遇到性能瓶颈，这可能由于当前基准测试中小模型的性能饱和而隐蔽。同时，通过大规模预训练，大模型在这些场景中展示了巨大的能力。随着大型语言模型的出现，取得了持续的进展，在少样本泛化能力、可扩展性和任务普适性方面展现了前所未有的能力，但这些能力在时间序列模型中不存在。为了改变目前在特定数据集上从头开始训练小模型的做法，本文旨在早期开发大规模时间序列模型（LTSM）。在预训练期间，我们策划了包含10亿个时间点的大规模数据集，将异构时间序列统一为单序列序列（S3）格式，并开发了面向LTSM的GPT风格架构。

    Deep learning has contributed remarkably to the advancement of time series analysis. Still, deep models can encounter performance bottlenecks in real-world small-sample scenarios, which can be concealed due to the performance saturation with small models on current benchmarks. Meanwhile, large models have demonstrated great powers in these scenarios through large-scale pre-training. Continuous progresses have been achieved as the emergence of large language models, exhibiting unprecedented ability in few-shot generalization, scalability, and task generality, which is however absent in time series models. To change the current practices of training small models on specific datasets from scratch, this paper aims at an early development of large time series models (LTSM). During pre-training, we curate large-scale datasets with up to 1 billion time points, unify heterogeneous time series into single-series sequence (S3) format, and develop the GPT-style architecture toward LTSMs. To meet 
    
[^305]: Transolver：一种用于一般几何体上求解偏微分方程的快速Transformer求解器

    Transolver: A Fast Transformer Solver for PDEs on General Geometries

    [https://arxiv.org/abs/2402.02366](https://arxiv.org/abs/2402.02366)

    Transolver是一种快速的Transformer求解器，通过使用物理注意力和灵活形状的片段来学习离散化几何形状背后隐藏的内在物理状态。它能够有效地捕捉复杂几何形状下的复杂物理相关性，从而提供了具备内生几何生成能力的求解器。

    

    Transformer已在各个领域实现了很多里程碑式的成就，最近开始应用于求解偏微分方程（PDEs）。然而，由于PDEs通常被离散化成具有复杂几何形状的大规模网格，对Transformer来说直接从大量单个点中捕捉复杂的物理相关性是具有挑战性的。为了超越肤浅而笨重的网格，我们基于一个更基础的思想提出了Transolver，即学习离散化几何形状背后隐藏的内在物理状态。具体来说，我们提出了一种新的物理注意力机制，将离散化的域自适应地划分为一系列可学习的灵活形状的片段，具有相似物理状态的网格点将被归属于同一个片段。通过计算从片段编码的具有物理意识的记号的注意力，Transovler能够有效地捕捉复杂几何形状下的复杂物理相关性，从而使求解器具备内生几何生成的能力。

    Transformers have empowered many milestones across various fields and have recently been applied to solve partial differential equations (PDEs). However, since PDEs are typically discretized into large-scale meshes with complex geometries, it is challenging for Transformers to capture intricate physical correlations directly from massive individual points. Going beyond superficial and unwieldy meshes, we present Transolver based on a more foundational idea, which is learning intrinsic physical states hidden behind discretized geometries. Specifically, we propose a new Physics-Attention to adaptively split the discretized domain into a series of learnable slices of flexible shapes, where mesh points under similar physical states will be ascribed to the same slice. By calculating attention to physics-aware tokens encoded from slices, Transovler can effectively capture intricate physical correlations under complex geometrics, which also empowers the solver with endogenetic geometry-genera
    
[^306]: 在上下文中学习的发展景观

    The Developmental Landscape of In-Context Learning

    [https://arxiv.org/abs/2402.02364](https://arxiv.org/abs/2402.02364)

    在transformers模型中，我们展示了在上下文学习中的离散发展阶段，并引入了两种方法来检测这些阶段的关键里程碑。我们使用行为和结构度量验证了这些方法的有效性。

    

    我们展示了在transformers中，当它们通过语言建模或线性回归任务进行训练时，上下文学习是如何以离散的发展阶段出现的。我们引入了两种方法来检测分隔这些阶段的关键里程碑，通过探测参数空间和函数空间中种群损失的几何特征。我们使用一系列行为和结构度量研究这些新方法揭示的阶段，以建立它们的有效性。

    We show that in-context learning emerges in transformers in discrete developmental stages, when they are trained on either language modeling or linear regression tasks. We introduce two methods for detecting the milestones that separate these stages, by probing the geometry of the population loss in both parameter space and function space. We study the stages revealed by these new methods using a range of behavioral and structural metrics to establish their validity.
    
[^307]: 神经网络内部的对称性统一：Transformer, 前馈和神经ODE

    Unification of Symmetries Inside Neural Networks: Transformer, Feedforward and Neural ODE

    [https://arxiv.org/abs/2402.02362](https://arxiv.org/abs/2402.02362)

    本研究通过应用物理学中的规范对称性原理，将其应用于神经网络架构，发现各种机器学习模型的参数冗余可以解释为规范对称性。并证明了前馈神经网络中的参数冗余在神经ODE中升级为微分同胚，并找到了Transformer模型与神经ODE及其规范对称性的自然对应关系。

    

    理解神经网络内部运作，包括transformers，仍然是机器学习中最具挑战性的难题之一。本研究将物理学中的规范对称性原理应用于神经网络架构，提出了一种新颖的方法。通过将模型函数视为物理可观测量，发现各种机器学习模型的参数冗余可以解释为规范对称性。我们在神经ODE中数学形式化了参数冗余，并发现它们的规范对称性由时空微分同胚给出，这在爱因斯坦的引力理论中起着基础性作用。我们将神经ODE视为连续版本的前馈神经网络，证明了前馈神经网络中的参数冗余确实在神经ODE中升级为微分同胚。我们进一步将分析扩展到Transformer模型，找到了与神经ODE及其规范对称性的自然对应关系。

    Understanding the inner workings of neural networks, including transformers, remains one of the most challenging puzzles in machine learning. This study introduces a novel approach by applying the principles of gauge symmetries, a key concept in physics, to neural network architectures. By regarding model functions as physical observables, we find that parametric redundancies of various machine learning models can be interpreted as gauge symmetries. We mathematically formulate the parametric redundancies in neural ODEs, and find that their gauge symmetries are given by spacetime diffeomorphisms, which play a fundamental role in Einstein's theory of gravity. Viewing neural ODEs as a continuum version of feedforward neural networks, we show that the parametric redundancies in feedforward neural networks are indeed lifted to diffeomorphisms in neural ODEs. We further extend our analysis to transformer models, finding natural correspondences with neural ODEs and their gauge symmetries. The
    
[^308]: Pruner:一种具有双重感知能力的高效跨平台张量编译器

    Pruner: An Efficient Cross-Platform Tensor Compiler with Dual Awareness

    [https://arxiv.org/abs/2402.02361](https://arxiv.org/abs/2402.02361)

    Pruner是一种高效跨平台张量编译器，通过参数化静态分析器（PSA）和模式感知成本模型（PaCM）实现张量程序优化，并使用动量转移学习（MTL）策略实现了跨平台适应性。

    

    对深度学习加速器（DLAs）上的张量程序优化对于有效的模型部署至关重要。虽然基于搜索的深度学习编译器（DLC）与手动方法相比取得了显著的性能提升，但仍然面临着搜索效率低和跨平台适应性差的挑战。在本文中，我们提出了Pruner，遵循硬件/软件协同设计原则来分层提升张量程序优化。Pruner由两个主要组件组成：参数化静态分析器（PSA）和模式感知成本模型（PaCM）。前者作为一种硬件感知和公式化的性能分析工具，引导搜索空间的修剪，而后者根据关键的数据流模式实现了对张量程序的性能预测。此外，为了保证有效的跨平台适应性，我们设计了一个动量转移学习（MTL）策略。

    Tensor program optimization on Deep Learning Accelerators (DLAs) is critical for efficient model deployment. Although search-based Deep Learning Compilers (DLCs) have achieved significant performance gains compared to manual methods, they still suffer from the persistent challenges of low search efficiency and poor cross-platform adaptability. In this paper, we propose $\textbf{Pruner}$, following hardware/software co-design principles to hierarchically boost tensor program optimization. Pruner comprises two primary components: a Parameterized Static Analyzer ($\textbf{PSA}$) and a Pattern-aware Cost Model ($\textbf{PaCM}$). The former serves as a hardware-aware and formulaic performance analysis tool, guiding the pruning of the search space, while the latter enables the performance prediction of tensor programs according to the critical data-flow patterns. Furthermore, to ensure effective cross-platform adaptation, we design a Momentum Transfer Learning ($\textbf{MTL}$) strategy using
    
[^309]: 具有更快超线性收敛速率的增量拟牛顿方法

    Incremental Quasi-Newton Methods with Faster Superlinear Convergence Rates

    [https://arxiv.org/abs/2402.02359](https://arxiv.org/abs/2402.02359)

    本文提出了一种更高效的增量拟牛顿方法，通过引入对称秩一更新和块更新的方式，实现了无条件数的局部超线性收敛速率，并且在数值实验中取得了显著优于基线方法的结果。

    

    本文考虑了有限和优化问题，其中每个分量函数都是强凸的，并且具有Lipschitz连续的梯度和Hessian矩阵。最近提出的增量拟牛顿方法基于BFGS更新，达到了依赖于问题条件数的局部超线性收敛速率。本文提出了一种更高效的拟牛顿方法，通过将对称秩一更新结合到增量框架中，从而实现了无条件数的局部超线性收敛速率。此外，我们可以通过在Hessian近似值上应用块更新来提升我们的方法，从而导致更快的局部收敛速率。数值实验表明，所提出的方法明显优于基线方法。

    We consider the finite-sum optimization problem, where each component function is strongly convex and has Lipschitz continuous gradient and Hessian. The recently proposed incremental quasi-Newton method is based on BFGS update and achieves a local superlinear convergence rate that is dependent on the condition number of the problem. This paper proposes a more efficient quasi-Newton method by incorporating the symmetric rank-1 update into the incremental framework, which results in the condition-number-free local superlinear convergence rate. Furthermore, we can boost our method by applying the block update on the Hessian approximation, which leads to an even faster local convergence rate. The numerical experiments show the proposed methods significantly outperform the baseline methods.
    
[^310]: 多模态因果结构学习与根因分析

    Multi-modal Causal Structure Learning and Root Cause Analysis

    [https://arxiv.org/abs/2402.02357](https://arxiv.org/abs/2402.02357)

    这种方法融合了多种模态的数据，利用对比学习提取模态间的关系，提高了根因定位的效果。

    

    有效的根因分析（RCA）对于迅速恢复服务、最小化损失以及确保复杂系统的顺利运行和管理至关重要。先前基于数据驱动的RCA方法，特别是采用因果发现技术的方法，主要关注构建依赖或因果图以追溯根因。然而，这些方法常常不够理想，因为它们仅依赖于单一模态的数据，导致了次优的解决方案。在这项工作中，我们提出了Mulan，一种统一的多模态因果结构学习方法，用于根因定位。我们利用一个针对日志定制的语言模型来促进日志表示学习，将日志序列转换为时间序列数据。为了探索不同模态之间的复杂关系，我们提出了一种基于对比学习的方法，在共享潜在空间中提取模态不变和模态特定的表示。此外，我们引入了一种新颖的关键绩效

    Effective root cause analysis (RCA) is vital for swiftly restoring services, minimizing losses, and ensuring the smooth operation and management of complex systems. Previous data-driven RCA methods, particularly those employing causal discovery techniques, have primarily focused on constructing dependency or causal graphs for backtracking the root causes. However, these methods often fall short as they rely solely on data from a single modality, thereby resulting in suboptimal solutions. In this work, we propose Mulan, a unified multi-modal causal structure learning method for root cause localization. We leverage a log-tailored language model to facilitate log representation learning, converting log sequences into time-series data. To explore intricate relationships across different modalities, we propose a contrastive learning-based approach to extract modality-invariant and modality-specific representations within a shared latent space. Additionally, we introduce a novel key performa
    
[^311]: 分散的非凸求和优化

    Decentralized Sum-of-Nonconvex Optimization

    [https://arxiv.org/abs/2402.02356](https://arxiv.org/abs/2402.02356)

    本论文研究了分散场景下的非凸求和优化问题，并提出了一种具有加速效果的随机分散一阶算法。数值实验证实了该算法的有效性。

    

    我们考虑最小化非凸函数的求和问题，即将非凸组分的平均值作为凸函数。现有的针对这类问题的随机算法只关注单台机器和集中式场景。本文研究了分散场景下的非凸求和优化问题。我们对该问题的PMGT-SVRG算法进行了新的理论分析，并证明了他们方法的线性收敛性。然而，PMGT-SVRG算法的收敛速度与条件数呈线性依赖关系，在条件数较差的问题中是不可取的。为了解决这个问题，我们提出了一种加速的随机分散一阶算法，将加速、梯度追踪和多一致性混合技术融入SVRG算法中。所提方法的收敛速度与条件数呈平方根依赖关系。数值实验证实了我们方法的有效性。

    We consider the optimization problem of minimizing the sum-of-nonconvex function, i.e., a convex function that is the average of nonconvex components. The existing stochastic algorithms for such a problem only focus on a single machine and the centralized scenario. In this paper, we study the sum-of-nonconvex optimization in the decentralized setting. We present a new theoretical analysis of the PMGT-SVRG algorithm for this problem and prove the linear convergence of their approach. However, the convergence rate of the PMGT-SVRG algorithm has a linear dependency on the condition number, which is undesirable for the ill-conditioned problem. To remedy this issue, we propose an accelerated stochastic decentralized first-order algorithm by incorporating the techniques of acceleration, gradient tracking, and multi-consensus mixing into the SVRG algorithm. The convergence rate of the proposed method has a square-root dependency on the condition number. The numerical experiments validate the 
    
[^312]: Symbol:通过符号方程学习生成灵活的黑盒优化器

    Symbol: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning

    [https://arxiv.org/abs/2402.02355](https://arxiv.org/abs/2402.02355)

    本研究提出了一种名为\textsc{Symbol}的新框架，通过符号方程学习来自动发现黑盒优化器。实验结果表明，\textsc{Symbol}生成的优化器在超越现有基准线的同时，还展现出出色的零样本泛化能力。

    

    最近的MetaBBO方法利用神经网络来元学习传统黑盒优化器的配置。尽管这些方法取得了成功，但它们不可避免地受到预定义手工优化器的限制。本文中，我们提出了一种名为\textsc{Symbol}的新框架，通过符号方程学习来促进黑盒优化器的自动发现。具体而言，我们提出了一个符号方程生成器(SEG)，允许为特定任务和优化步骤动态生成闭式优化规则。在\textsc{Symbol}内部，我们基于强化学习开发了三种不同的策略，以便高效地元学习SEG。大量实验证明，\textsc{Symbol}生成的优化器不仅超越了最先进的BBO和MetaBBO基准线，而且在完全不同问题的全新任务上表现出了异常的零样本泛化能力。

    Recent Meta-learning for Black-Box Optimization (MetaBBO) methods harness neural networks to meta-learn configurations of traditional black-box optimizers. Despite their success, they are inevitably restricted by the limitations of predefined hand-crafted optimizers. In this paper, we present \textsc{Symbol}, a novel framework that promotes the automated discovery of black-box optimizers through symbolic equation learning. Specifically, we propose a Symbolic Equation Generator (SEG) that allows closed-form optimization rules to be dynamically generated for specific tasks and optimization steps. Within \textsc{Symbol}, we then develop three distinct strategies based on reinforcement learning, so as to meta-learn the SEG efficiently. Extensive experiments reveal that the optimizers generated by \textsc{Symbol} not only surpass the state-of-the-art BBO and MetaBBO baselines, but also exhibit exceptional zero-shot generalization abilities across entirely unseen tasks with different problem
    
[^313]: 分类和回归问题中潜在模型性能改进的范式：一个概念验证

    A Paradigm for Potential Model Performance Improvement in Classification and Regression Problems. A Proof of Concept

    [https://arxiv.org/abs/2402.02354](https://arxiv.org/abs/2402.02354)

    本文提出了一种方法来增强模型预测性能，通过生成多个辅助模型捕捉属性之间的关系，并生成额外的信息列来潜在地增强目标预测。

    

    本文提出了一种旨在增强模型预测性能的方法论。该方法涉及生成多个辅助模型，以捕捉属性之间的关系。这些信息用于在数据集中生成额外的信息列，从而潜在地增强目标预测。文中提供了一个概念验证案例和相关代码。

    A methodology that seeks to enhance model prediction performance is presented. The method involves generating multiple auxiliary models that capture relationships between attributes as a function of each other. Such information serves to generate additional informative columns in the dataset that can potentially enhance target prediction. A proof of case and related code is provided.
    
[^314]: 感知干扰的下行低地球轨道卫星网络中的新型随机接入协议

    Interference-Aware Emergent Random Access Protocol for Downlink LEO Satellite Networks

    [https://arxiv.org/abs/2402.02350](https://arxiv.org/abs/2402.02350)

    本文提出了一个多智能体深度强化学习（MADRL）框架，用于训练下行低地球轨道（LEO）卫星网络的多址接入协议。我们的方法集中压缩了新型随机接入信令（Ce2RACH），通过联合学习的额外信令消息交换来减轻卫星间干扰，并实现了高达36.65％的网络吞吐量提升。

    

    本文提出了一个多智能体深度强化学习（MADRL）框架，用于训练下行低地球轨道（LEO）卫星网络的多址接入协议。通过改进现有的学习协议，即新型随机接入信道（eRACH），我们提出的方法，被命名为集中压缩的新型随机接入信令（Ce2RACH），可以通过在MADRL训练过程中联合学习的额外信令消息交换来减轻卫星间干扰。模拟结果表明，与eRACH相比，Ce2RACH的网络吞吐量高达36.65％，而信令消息的成本与用户数量线性增加。

    In this article, we propose a multi-agent deep reinforcement learning (MADRL) framework to train a multiple access protocol for downlink low earth orbit (LEO) satellite networks. By improving the existing learned protocol, emergent random access channel (eRACH), our proposed method, coined centralized and compressed emergent signaling for eRACH (Ce2RACH), can mitigate inter-satellite interference by exchanging additional signaling messages jointly learned through the MADRL training process. Simulations demonstrate that Ce2RACH achieves up to 36.65% higher network throughput compared to eRACH, while the cost of signaling messages increase linearly with the number of users.
    
[^315]: Riemannian Preconditioned LoRA用于基础模型微调的研究

    Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models

    [https://arxiv.org/abs/2402.02347](https://arxiv.org/abs/2402.02347)

    本研究通过引入Riemannian预条件器，增强了LoRA微调过程中的优化步骤。实验结果表明，使用该预条件器可以显著提升SGD和AdamW的收敛性和可靠性，并使训练过程更加稳健。此外，理论分析证明了在凸参数化下使用该预条件器微调ReLU网络的收敛速度与数据矩阵的条件数无关。这个新的Riemannian预条件器在经典的低秩矩阵恢复中已经有过研究。

    

    在这项工作中，我们研究了在LoRA微调过程中引入Riemannian预条件器来提升其优化步骤的效果。具体来说，我们在每个梯度步骤中引入了一个$r\times r$的预条件器，其中$r$是LoRA的秩。这个预条件器对现有的优化器代码只需要做出很小的改变，并且几乎没有存储和运行时开销。我们对大型语言模型和文本到图像扩散模型进行了实验，结果表明，使用我们的预条件器，SGD和AdamW的收敛性和可靠性都可以显著提升。此外，训练过程对于学习率等超参数的选择变得更加稳健。从理论上讲，我们证明了使用我们的预条件器在凸参数化下微调两层ReLU网络的收敛速度与数据矩阵的条件数无关。这个新的Riemannian预条件器在经典的低秩矩阵恢复中已经有过研究。

    In this work we study the enhancement of Low Rank Adaptation (LoRA) fine-tuning procedure by introducing a Riemannian preconditioner in its optimization step. Specifically, we introduce an $r\times r$ preconditioner in each gradient step where $r$ is the LoRA rank. This preconditioner requires a small change to existing optimizer code and creates virtually minuscule storage and runtime overhead. Our experimental results with both large language models and text-to-image diffusion models show that with our preconditioner, the convergence and reliability of SGD and AdamW can be significantly enhanced. Moreover, the training process becomes much more robust to hyperparameter choices such as learning rate. Theoretically, we show that fine-tuning a two-layer ReLU network in the convex paramaterization with our preconditioner has convergence rate independent of condition number of the data matrix. This new Riemannian preconditioner, previously explored in classic low-rank matrix recovery, is 
    
[^316]: 闭环无监督表示解缠的β-VAE蒸馏与扩散概率反馈

    Closed-Loop Unsupervised Representation Disentanglement with $\beta$-VAE Distillation and Diffusion Probabilistic Feedback

    [https://arxiv.org/abs/2402.02346](https://arxiv.org/abs/2402.02346)

    本文提出了闭环无监督表示解缠方法CL-Dis，使用扩散自动编码器（Diff-AE）和β-VAE共同提取语义解缠表示，以解决表示解缠面临的问题。

    

    表示解缠可能有助于AI根本上理解现实世界，从而使判别和生成任务受益。目前至少有三个未解决的核心问题：（i）过于依赖标签注释和合成数据-导致在自然情景下泛化能力较差；（ii）启发式/手工制作的解缠约束使得难以自适应地实现最佳训练权衡；（iii）缺乏合理的评估指标，特别是对于真实的无标签数据。为了解决这些挑战，我们提出了一种被称为CL-Dis的闭环无监督表示解缠方法。具体地，我们使用基于扩散的自动编码器（Diff-AE）作为骨干，并使用β-VAE作为副驾驶员来提取语义解缠的表示。扩散模型的强大生成能力和VAE模型的良好解缠能力是互补的。为了加强解缠，使用VAE潜变量。

    Representation disentanglement may help AI fundamentally understand the real world and thus benefit both discrimination and generation tasks. It currently has at least three unresolved core issues: (i) heavy reliance on label annotation and synthetic data -- causing poor generalization on natural scenarios; (ii) heuristic/hand-craft disentangling constraints make it hard to adaptively achieve an optimal training trade-off; (iii) lacking reasonable evaluation metric, especially for the real label-free data. To address these challenges, we propose a \textbf{C}losed-\textbf{L}oop unsupervised representation \textbf{Dis}entanglement approach dubbed \textbf{CL-Dis}. Specifically, we use diffusion-based autoencoder (Diff-AE) as a backbone while resorting to $\beta$-VAE as a co-pilot to extract semantically disentangled representations. The strong generation ability of diffusion model and the good disentanglement ability of VAE model are complementary. To strengthen disentangling, VAE-latent 
    
[^317]: Stereographic Spherical Sliced Wasserstein Distances - 应用于球形概率分布比较的立体投影球面切片瓦瑟斯坦距离

    Stereographic Spherical Sliced Wasserstein Distances

    [https://arxiv.org/abs/2402.02345](https://arxiv.org/abs/2402.02345)

    本文提出了一种快速且高度并行的用于比较球形测度的距离，使用了立体投影和广义Radon变换，称之为立体投影球面切片瓦瑟斯坦（S3W）距离。通过仔细处理立体投影引起的距离畸变，并进行了理论分析，证明了该方法在速度和效果上的优势。

    

    在地质学、医学领域、计算机视觉和深度表示学习等各个领域，比较球形概率分布是非常重要的。基于最优传输的距离，比如瓦瑟斯坦距离，对于比较概率测度已经引发了活跃的研究，以开发计算效率高的球形概率测度的变体。本文介绍了一种高速且高度并行化的用于比较球形测度的距离，使用了立体投影和广义Radon变换，我们称之为立体投影球面切片瓦瑟斯坦（S3W）距离。我们仔细处理了立体投影引起的距离畸变，并对我们提出的度量及其具有旋转不变性的变体进行了广泛的理论分析。最后，我们评估了所提出的度量的性能，并将其与最近的基线进行了比较，从遥感和处理效率两个方面进行了评估。

    Comparing spherical probability distributions is of great interest in various fields, including geology, medical domains, computer vision, and deep representation learning. The utility of optimal transport-based distances, such as the Wasserstein distance, for comparing probability measures has spurred active research in developing computationally efficient variations of these distances for spherical probability measures. This paper introduces a high-speed and highly parallelizable distance for comparing spherical measures using the stereographic projection and the generalized Radon transform, which we refer to as the Stereographic Spherical Sliced Wasserstein (S3W) distance. We carefully address the distance distortion caused by the stereographic projection and provide an extensive theoretical analysis of our proposed metric and its rotationally invariant variation. Finally, we evaluate the performance of the proposed metrics and compare them with recent baselines in terms of both spe
    
[^318]: MetaOptimize：一个优化步长和其他元参数的框架

    MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters

    [https://arxiv.org/abs/2402.02342](https://arxiv.org/abs/2402.02342)

    MetaOptimize是一个框架，通过动态调整学习率来优化机器学习算法中的元参数，以提高训练效率和模型性能。

    

    本文解决了机器学习算法中优化元参数（即超参数）的挑战，这是影响训练效率和模型性能的关键因素。我们引入了MetaOptimize框架，摆脱了计算昂贵的传统元参数搜索方法，通过动态调整元参数，特别是步长（也称为学习率），来训练模型。具体而言，MetaOptimize可以适用于任何一阶优化算法，在训练过程中实时调整步长，通过未来损失的折现总和来最小化一种特定形式的遗憾。我们还介绍了MetaOptimize的低复杂度变体，结合其适应多个优化算法的能力，展示了在各种机器学习应用中与手工设计的学习率计划相媲美的性能。

    This paper addresses the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance. Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses. We also introduce low complexity variants of MetaOptimize that, in conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning rate schedules across various machine learning applications.
    
[^319]: 从视觉提示中学习语义代理，为深度度量学习中的参数高效微调

    Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning

    [https://arxiv.org/abs/2402.02340](https://arxiv.org/abs/2402.02340)

    本论文提出了一种基于学习视觉提示的参数高效微调方法，能够在深度度量学习任务中使预训练模型适应本地数据域并保留先前获得的知识。

    

    深度度量学习(DML)一直是机器学习社区关注的重点目标。现有解决方案集中于对传统图像数据集上进行预训练模型的微调。由于最近从更大规模数据集训练的预训练模型取得成功，将该模型适应本地数据域的DML任务，同时保留先前获得的知识，是具有挑战性的。在本文中，我们研究了用于DML任务的预训练模型的参数高效微调方法。特别是，我们提出了一种基于学习预训练视觉转换器(ViT)中的视觉提示(VPT)的新颖有效的框架。基于传统的基于代理的DML范例，我们通过将输入图像和ViT的语义信息结合到代理中来优化每个类别的视觉提示。我们证明了我们的新逼近方法在语义信息方面优于代表能力。

    Deep Metric Learning (DML) has long attracted the attention of the machine learning community as a key objective. Existing solutions concentrate on fine-tuning the pre-trained models on conventional image datasets. As a result of the success of recent pre-trained models trained from larger-scale datasets, it is challenging to adapt the model to the DML tasks in the local data domain while retaining the previously gained knowledge. In this paper, we investigate parameter-efficient methods for fine-tuning the pre-trained model for DML tasks. In particular, we propose a novel and effective framework based on learning Visual Prompts (VPT) in the pre-trained Vision Transformers (ViT). Based on the conventional proxy-based DML paradigm, we augment the proxy by incorporating the semantic information from the input image and the ViT, in which we optimize the visual prompts for each class. We demonstrate that our new approximations with semantic information are superior to representative capabi
    
[^320]: 不确定性感知的3D人体姿势估计测试时间优化

    Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation

    [https://arxiv.org/abs/2402.02339](https://arxiv.org/abs/2402.02339)

    本文提出了一种不确定性感知的测试时间优化（UAO）框架，通过量化关节点的不确定性来缓解过拟合问题，提高3D人体姿势估计的性能。

    

    尽管数据驱动方法在3D人体姿势估计方面取得了成功，但它们常常受到域间差异的限制，表现出有限的泛化能力。相比之下，基于优化的方法在特定情况下进行微调方面表现优秀，但整体表现通常不如数据驱动方法。我们观察到先前的基于优化的方法通常依赖于投影约束，这仅仅确保了在2D空间中的对齐，可能导致过拟合问题。为了解决这个问题，我们提出了一种不确定性感知的测试时间优化 (UAO) 框架，它保留了预训练模型的先验信息，并利用关节点的不确定性来缓解过拟合问题。具体而言，在训练阶段，我们设计了一个有效的2D到3D网络，用于估计相应的3D姿势，并量化每个3D关节点的不确定性。对于测试时的优化，所提出的优化框架冻结预训练模型，并仅优化少量关键参数，以提高性能。

    Although data-driven methods have achieved success in 3D human pose estimation, they often suffer from domain gaps and exhibit limited generalization. In contrast, optimization-based methods excel in fine-tuning for specific cases but are generally inferior to data-driven methods in overall performance. We observe that previous optimization-based methods commonly rely on projection constraint, which only ensures alignment in 2D space, potentially leading to the overfitting problem. To address this, we propose an Uncertainty-Aware testing-time Optimization (UAO) framework, which keeps the prior information of pre-trained model and alleviates the overfitting problem using the uncertainty of joints. Specifically, during the training phase, we design an effective 2D-to-3D network for estimating the corresponding 3D pose while quantifying the uncertainty of each 3D joint. For optimization during testing, the proposed optimization framework freezes the pre-trained model and optimizes only a 
    
[^321]: 大型语言模型用于网络的适应性

    Large Language Model Adaptation for Networking

    [https://arxiv.org/abs/2402.02338](https://arxiv.org/abs/2402.02338)

    本文首次研究了使用大型语言模型（LLM）适应网络问题的方法，通过利用LLM的预训练知识和强大推理能力，实现了“一模型适用于所有”的目标，并取得了更好的性能和更强的泛化能力。

    

    现在许多网络任务都使用深度学习（DL）来解决复杂的预测和系统优化问题。然而，目前基于DL的算法的设计哲学需要进行大量的工程开销，因为需要为不同的网络任务手动设计深度神经网络（DNN）。此外，DNN在未见过的数据分布/环境上的泛化性能较差。在大型语言模型（LLM）的最新成功的推动下，本文首次研究了LLM用于网络的适应性，以探索更可持续的设计哲学。凭借海量的预训练知识和强大的推理能力，LLM可以作为基础模型，并且有望在各种任务中实现“一模型适用于所有”，并具有更好的性能和更强的泛化能力。在本文中，我们提出了NetLLM，这是第一个有效地将LLM应用于解决网络问题的适应性框架。NetLLM解决了许多实际挑战。

    Many networking tasks now employ deep learning (DL) to solve complex prediction and system optimization problems. However, current design philosophy of DL-based algorithms entails intensive engineering overhead due to the manual design of deep neural networks (DNNs) for different networking tasks. Besides, DNNs tend to achieve poor generalization performance on unseen data distributions/environments.   Motivated by the recent success of large language models (LLMs), for the first time, this work studies the LLM adaptation for networking to explore a more sustainable design philosophy. With the massive pre-trained knowledge and powerful inference ability, LLM can serve as the foundation model, and is expected to achieve "one model for all" with even better performance and stronger generalization for various tasks. In this paper, we present NetLLM, the first LLM adaptation framework that efficiently adapts LLMs to solve networking problems. NetLLM addresses many practical challenges in L
    
[^322]: 深度表格学习需要算术特征交互

    Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning

    [https://arxiv.org/abs/2402.02334](https://arxiv.org/abs/2402.02334)

    本文研究了深度表格学习中算术特征交互的必要性，通过引入AMFormer模型，实现了在细粒度表格数据建模、训练数据效率和泛化方面的优越性能。

    

    直到最近，关于深度模型在表格数据上的有效归纳偏见的问题仍然没有答案。本文调查了算术特征交互对于深度表格学习的必要性假设。为了测试这一观点，我们创建了一个具有轻微特征交互假设的合成表格数据集，并研究了一种改进的Transformer架构，使其能够进行算术特征交互，称为AMFormer。结果显示，AMFormer在细粒度表格数据建模、训练数据效率和泛化方面优于强对手。这归因于其并行的加性和乘性注意力操作符和基于提示的优化，这有助于在具有算术工程特征的扩展空间中分离表格样本。我们在真实世界数据上进行了广泛的实验，也验证了AMFormer的一致有效性、效率和合理性，表明它已经建立了强有力的归纳能力。

    Until recently, the question of the effective inductive bias of deep models on tabular data has remained unanswered. This paper investigates the hypothesis that arithmetic feature interaction is necessary for deep tabular learning. To test this point, we create a synthetic tabular dataset with a mild feature interaction assumption and examine a modified transformer architecture enabling arithmetical feature interactions, referred to as AMFormer. Results show that AMFormer outperforms strong counterparts in fine-grained tabular data modeling, data efficiency in training, and generalization. This is attributed to its parallel additive and multiplicative attention operators and prompt-based optimization, which facilitate the separation of tabular samples in an extended space with arithmetically-engineered features. Our extensive experiments on real-world data also validate the consistent effectiveness, efficiency, and rationale of AMFormer, suggesting it has established a strong inductive
    
[^323]: 生成型人工智能中的版权保护：技术视角

    Copyright Protection in Generative AI: A Technical Perspective

    [https://arxiv.org/abs/2402.02333](https://arxiv.org/abs/2402.02333)

    本文从技术角度全面概述了在生成型人工智能中的版权保护问题，包括数据版权和模型版权两个方面，并提出了一些创新的方法和技术。

    

    近年来，生成型人工智能（Generative AI）取得了快速发展，扩展了其创建文本、图像、音频和代码等合成内容的能力。这些深度生成模型（Deep Generative Models，DGMs）生成的内容高保真度和真实性引发了重大的版权问题。关于如何有效保护DGMs中的版权问题，已经进行了各种法律辩论。本文从技术角度提供了版权保护的全面概述。我们从两个不同的视角来进行研究：一是与数据所有者所持有的源数据相关的版权，二是与模型构建者所维护的生成模型相关的版权。对于数据版权，我们深入探讨了数据所有者如何保护其内容，并在不侵犯这些权利的情况下使用DGMs。对于模型版权，我们的讨论延伸到防止模型盗窃和识别特定模型生成的输出的策略。最后，我们强调了一些创新的方法和技术来处理这些版权问题。

    Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight 
    
[^324]: Minusformer: 通过渐进学习残差来改进时间序列预测

    Minusformer: Improving Time Series Forecasting by Progressively Learning Residuals

    [https://arxiv.org/abs/2402.02332](https://arxiv.org/abs/2402.02332)

    Minusformer通过逐步学习残差来改进时间序列预测。它将Transformer模型中信息聚合机制从加法改为减法，并通过辅助输出分支逐层学习监督信号的残差，从而提高模型的灵活性和对抗过拟合的能力。

    

    在本文中，我们发现普遍存在的时间序列（TS）预测模型容易严重过拟合。为了应对这个问题，我们采用去冗余的方法逐步恢复TS的内在价值以用于未来的时间段。具体而言，我们通过将信息聚合机制从加法转变为减法来改进传统的Transformer模型。然后，我们在原模型的每个模块中加入一个辅助输出分支，构建一条通往最终预测的高速公路。该分支中后续模块的输出将减去先前学习到的结果，使模型能够逐层学习监督信号的残差。这种设计促进了输入和输出流的逐步学习驱动隐式分解，使模型具有更高的灵活性、可解释性和对抗过拟合的韧性。由于模型中的所有聚合都是减号，因此被称为Minusformer。

    In this paper, we find that ubiquitous time series (TS) forecasting models are prone to severe overfitting. To cope with this problem, we embrace a de-redundancy approach to progressively reinstate the intrinsic values of TS for future intervals. Specifically, we renovate the vanilla Transformer by reorienting the information aggregation mechanism from addition to subtraction. Then, we incorporate an auxiliary output branch into each block of the original model to construct a highway leading to the ultimate prediction. The output of subsequent modules in this branch will subtract the previously learned results, enabling the model to learn the residuals of the supervision signal, layer by layer. This designing facilitates the learning-driven implicit progressive decomposition of the input and output streams, empowering the model with heightened versatility, interpretability, and resilience against overfitting. Since all aggregations in the model are minus signs, which is called Minusfor
    
[^325]: 基于神经网络的数据驱动算法设计及其在分支定界中的应用

    Data-driven algorithm design using neural networks with applications to branch-and-cut

    [https://arxiv.org/abs/2402.02328](https://arxiv.org/abs/2402.02328)

    本文介绍了一种基于神经网络的数据驱动算法设计方法，并将其应用于分支定界框架中，用于解决混合整数优化问题。

    

    数据驱动算法设计是一种使用统计和机器学习技术从一类算法中选择在某个（未知）问题实例分布上表现最佳的算法的范例。本文在这一研究领域的最新工作的基础上引入了一个新的思路，即不仅仅选择一个具有最佳性能的单一算法，而是允许根据问题实例选择算法。具体而言，给定一组代表性的问题实例样本，我们学习一个神经网络，将问题实例映射到最合适的算法。我们将这一思路形式化，并根据最近的数据驱动算法设计的工作，推导出了这个学习问题的严格样本复杂度界限。然后，我们将这种方法应用到混合整数优化的分支定界框架中，以做出良好的决策。

    Data-driven algorithm design is a paradigm that uses statistical and machine learning techniques to select from a class of algorithms for a computational problem an algorithm that has the best expected performance with respect to some (unknown) distribution on the instances of the problem. We build upon recent work in this line of research by introducing the idea where, instead of selecting a single algorithm that has the best performance, we allow the possibility of selecting an algorithm based on the instance to be solved. In particular, given a representative sample of instances, we learn a neural network that maps an instance of the problem to the most appropriate algorithm {\em for that instance}. We formalize this idea and derive rigorous sample complexity bounds for this learning problem, in the spirit of recent work in data-driven algorithm design. We then apply this approach to the problem of making good decisions in the branch-and-cut framework for mixed-integer optimization 
    
[^326]: 动量在隐式逐步优化中对目标函数的平滑作用的角色

    Role of Momentum in Smoothing Objective Function in Implicit Graduated Optimization

    [https://arxiv.org/abs/2402.02325](https://arxiv.org/abs/2402.02325)

    这篇论文揭示了具有动量的随机梯度下降算法平滑了目标函数，影响程度由多个超参数决定，同时提供了对动量改善泛化能力的理论解释和新见解。

    

    虽然具有动量的随机梯度下降（SGD）具有快速收敛和良好的泛化能力，但对此缺乏理论解释。本文展示了具有动量的SGD平滑了目标函数，其程度由学习率、批大小、动量因子、随机梯度的方差以及梯度范数的上界确定。这一理论发现揭示了为什么动量改善了泛化能力，并提供了关于动量因子等超参数作用的新见解。我们还提出了一种利用SGD动量平滑特性的隐式逐步优化算法，并提供了实验结果支持我们的观点，即SGD动量平滑了目标函数。

    While stochastic gradient descent (SGD) with momentum has fast convergence and excellent generalizability, a theoretical explanation for this is lacking. In this paper, we show that SGD with momentum smooths the objective function, the degree of which is determined by the learning rate, the batch size, the momentum factor, the variance of the stochastic gradient, and the upper bound of the gradient norm. This theoretical finding reveals why momentum improves generalizability and provides new insights into the role of the hyperparameters, including momentum factor. We also present an implicit graduated optimization algorithm that exploits the smoothing properties of SGD with momentum and provide experimental results supporting our assertion that SGD with momentum smooths the objective function.
    
[^327]: 动态增量优化用于最佳子集选择

    Dynamic Incremental Optimization for Best Subset Selection

    [https://arxiv.org/abs/2402.02322](https://arxiv.org/abs/2402.02322)

    本文研究了一类$\ell_0$正则化问题的对偶形式，并提出了一种高效的原对偶算法，通过充分利用对偶范围估计和增量策略，提高了最佳子集选择问题的解决方案的效率和统计性质。

    

    最佳子集选择被认为是稀疏学习问题的“黄金标准”。已经提出了各种优化技术来攻击这个非光滑非凸问题。本文研究了一类$\ell_0$正则化问题的对偶形式。基于原始问题和对偶问题的结构，我们提出了一种高效的原对偶算法。通过充分利用对偶范围估计和增量策略，我们的算法潜在地减少了冗余计算并改进了最佳子集选择的解决方案。理论分析和对合成和真实数据集的实验验证了所提出解决方案的效率和统计性质。

    Best subset selection is considered the `gold standard' for many sparse learning problems. A variety of optimization techniques have been proposed to attack this non-smooth non-convex problem. In this paper, we investigate the dual forms of a family of $\ell_0$-regularized problems. An efficient primal-dual algorithm is developed based on the primal and dual problem structures. By leveraging the dual range estimation along with the incremental strategy, our algorithm potentially reduces redundant computation and improves the solutions of best subset selection. Theoretical analysis and experiments on synthetic and real-world datasets validate the efficiency and statistical properties of the proposed solutions.
    
[^328]: 无噪声结构图的主动学习

    Active Learning for Graphs with Noisy Structures

    [https://arxiv.org/abs/2402.02321](https://arxiv.org/abs/2402.02321)

    本论文提出了一个针对有噪声图的主动学习框架GALClean，通过采用迭代方法实现数据选择和图的清理，解决了选标记数据和获取干净图结构这两个相互依赖的任务。

    

    图神经网络（GNN）在节点分类等任务中取得了显著的成功，这很大程度上依赖于足够标记的节点的可用性。然而，标记大规模图的巨大成本使得人们将重点放在了图上的主动学习上，该方法旨在通过有效的数据选择来最大化下游模型的性能。值得注意的是，大多数现有方法都假设了可靠的图拓扑，而实际情况下往往会出现有噪声的图。鉴于此，设计一个成功的适用于有噪声图的主动学习框架是非常必要但具有挑战性的，因为选择标记数据和获取干净的图是两个相互依赖的任务：选择高质量的数据需要干净的图结构，而清理噪声图结构需要足够标记的数据。考虑到上述复杂性，我们提出了一个主动学习框架GALClean，该框架专门设计成采用迭代方法来进行数据选择和图的清理。

    Graph Neural Networks (GNNs) have seen significant success in tasks such as node classification, largely contingent upon the availability of sufficient labeled nodes. Yet, the excessive cost of labeling large-scale graphs led to a focus on active learning on graphs, which aims for effective data selection to maximize downstream model performance. Notably, most existing methods assume reliable graph topology, while real-world scenarios often present noisy graphs. Given this, designing a successful active learning framework for noisy graphs is highly needed but challenging, as selecting data for labeling and obtaining a clean graph are two tasks naturally interdependent: selecting high-quality data requires clean graph structure while cleaning noisy graph structure requires sufficient labeled data. Considering the complexity mentioned above, we propose an active learning framework, GALClean, which has been specifically designed to adopt an iterative approach for conducting both data sele
    
[^329]: Spin: 一种具备GPU加速的高效安全计算框架

    Spin: An Efficient Secure Computation Framework with GPU Acceleration

    [https://arxiv.org/abs/2402.02320](https://arxiv.org/abs/2402.02320)

    Spin是一个GPU加速的多方计算(MPC)框架，支持多个计算方和不诚实多数对抗设置。该框架提出了针对机器学习关键的非线性函数的优化协议，并进行了针对Transformer模型的注意力的新颖优化，以实现高效且安全的计算。

    

    准确性和效率对于多方计算（MPC）框架仍然是挑战。Spin是一个支持多个计算方和不诚实多数对抗设置的GPU加速的MPC框架。我们提出了针对机器学习关键的非线性函数的优化协议，以及针对Transformer模型的基本单元注意力的几种新颖优化，使Spin能够在不牺牲安全性的情况下进行非常规CNN训练和Transformer推断。在后端层面，Spin利用GPU、CPU和RDMA启用的智能网络卡进行加速。全面的评估表明，Spin在深度神经网络训练方面比最先进技术快两倍。对于具有1890万参数的Transformer模型的推断，我们的注意力特定优化使Spin能够实现更好的效率、更少的通信和更好的准确性。

    Accuracy and efficiency remain challenges for multi-party computation (MPC) frameworks. Spin is a GPU-accelerated MPC framework that supports multiple computation parties and a dishonest majority adversarial setup. We propose optimized protocols for non-linear functions that are critical for machine learning, as well as several novel optimizations specific to attention that is the fundamental unit of Transformer models, allowing Spin to perform non-trivial CNNs training and Transformer inference without sacrificing security. At the backend level, Spin leverages GPU, CPU, and RDMA-enabled smart network cards for acceleration. Comprehensive evaluations demonstrate that Spin can be up to $2\times$ faster than the state-of-the-art for deep neural network training. For inference on a Transformer model with 18.9 million parameters, our attention-specific optimizations enable Spin to achieve better efficiency, less communication, and better accuracy.
    
[^330]: 指令调整数据集的多样性度量和子集选择

    Diversity Measurement and Subset Selection for Instruction Tuning Datasets

    [https://arxiv.org/abs/2402.02318](https://arxiv.org/abs/2402.02318)

    本文提出了一种用于指令调整数据集的多样性度量和子集选择的方法，通过使用确定性点过程来捕捉数据集的多样性和质量，并通过对数行列式距离来衡量数据集的多样性。实验证明，在归一化的权重梯度空间中，提出的多样性度量与指令遵循性能相关，并可以用于指导数据选择和分析数据集构建策略。

    

    我们的目标是选择数据子集，以更有效地对大型语言模型进行微调，以更好地遵循指令。之前的研究强调了数据集构建中多样性的重要性，但依赖于启发式方法，如任务数量。在本文中，我们使用确定性点过程来捕捉指令调整数据集的多样性和质量，以进行子集选择。我们提出用对数行列式距离来衡量数据集的多样性，即感兴趣的数据集与最大多样性参考数据集之间的距离。我们的实验表明，归一化的权重梯度空间中所提出的多样性度量与下游指令遵循性能相关。因此，它可以用于指导何时最有帮助地进行数据选择，并分析数据集构建策略。我们在各种指令调整数据集上展示了我们方法的实用性。

    We aim to select data subsets for the fine-tuning of large language models to more effectively follow instructions. Prior work has emphasized the importance of diversity in dataset curation but relied on heuristics such as the number of tasks. In this paper, we use determinantal point processes to capture the diversity and quality of instruction tuning datasets for subset selection. We propose to measure dataset diversity with log determinant distance that is the distance between the dataset of interest and a maximally diverse reference dataset. Our experiments demonstrate that the proposed diversity measure in the normalized weight gradient space is correlated with downstream instruction-following performance. Consequently, it can be used to inform when data selection is the most helpful and to analyze dataset curation strategies. We demonstrate the utility of our approach on various instruction tuning datasets.
    
[^331]: INViT:一种具有不变嵌套视图转换器的可泛化解决路由问题的方法

    INViT: A Generalizable Routing Problem Solver with Invariant Nested View Transformer

    [https://arxiv.org/abs/2402.02317](https://arxiv.org/abs/2402.02317)

    INViT是一种具有不变嵌套视图转换器的解决路由问题的方法，通过强制嵌套设计和不变的视图，在编码器内部提高学习求解器的泛化能力，从而实现了在具有不同分布和不同问题规模的TSP和CVRP问题上卓越的泛化性能。

    

    最近，深度强化学习在学习解决路由问题的快速启发式方法方面取得了有希望的结果。与此同时，大多数求解器在推广到未见过的分布或具有不同规模的分布时遇到了困难。为解决这个问题，我们提出了一种新的架构，称为不变嵌套视图转换器（INViT），它旨在通过在编码器内部强制一个嵌套设计以及不变的视图来促进学习求解器的泛化能力。它应用了修改的策略梯度算法，并结合了数据增强。我们证明了所提出的INViT在具有不同分布和不同问题规模的TSP和CVRP问题上取得了卓越的泛化性能。

    Recently, deep reinforcement learning has shown promising results for learning fast heuristics to solve routing problems. Meanwhile, most of the solvers suffer from generalizing to an unseen distribution or distributions with different scales. To address this issue, we propose a novel architecture, called Invariant Nested View Transformer (INViT), which is designed to enforce a nested design together with invariant views inside the encoders to promote the generalizability of the learned solver. It applies a modified policy gradient algorithm enhanced with data augmentations. We demonstrate that the proposed INViT achieves a dominant generalization performance on both TSP and CVRP problems with various distributions and different problem scales.
    
[^332]: 你的扩散模型实际上是一个可证明鲁棒的分类器

    Your Diffusion Model is Secretly a Certifiably Robust Classifier

    [https://arxiv.org/abs/2402.02316](https://arxiv.org/abs/2402.02316)

    这项研究提出了一种新的扩散分类器家族，称为噪声扩散分类器（NDCs），其具有最新的可证明的鲁棒性。通过将扩散分类器推广到分类高斯受损数据，并将其与随机平滑技术相结合，构建了具有非常量Lipschitzness的平滑分类器。这些NDCs显示出卓越的认证鲁棒性。

    

    近期，扩散模型被作为鲁棒分类的生成器分类器所采用。然而，对于扩散分类器鲁棒性的综合理论理解仍然缺乏，这让我们怀疑它们是否会容易受到未来更强攻击的影响。在本研究中，我们提出了一种新的扩散分类器家族，命名为噪声扩散分类器（NDCs），其具有最新的可证明的鲁棒性。具体来说，我们通过推导这些分布的证据下界（ELBOs），利用ELBO近似似然度量，并使用贝叶斯定理计算分类概率，将扩散分类器推广到分类高斯受损数据。我们将这些推广的扩散分类器与随机平滑技术相结合，构建具有非常量Lipschitzness的平滑分类器。实验结果表明我们提出的NDCs在鲁棒性方面具有卓越的认证能力。值得注意的是，我们是第一个达到80%的...

    Diffusion models are recently employed as generative classifiers for robust classification. However, a comprehensive theoretical understanding of the robustness of diffusion classifiers is still lacking, leading us to question whether they will be vulnerable to future stronger attacks. In this study, we propose a new family of diffusion classifiers, named Noised Diffusion Classifiers~(NDCs), that possess state-of-the-art certified robustness. Specifically, we generalize the diffusion classifiers to classify Gaussian-corrupted data by deriving the evidence lower bounds (ELBOs) for these distributions, approximating the likelihood using the ELBO, and calculating classification probabilities via Bayes' theorem. We integrate these generalized diffusion classifiers with randomized smoothing to construct smoothed classifiers possessing non-constant Lipschitzness. Experimental results demonstrate the superior certified robustness of our proposed NDCs. Notably, we are the first to achieve 80\%
    
[^333]: 通过修正的缩放定律选择大型语言模型进行微调

    Selecting Large Language Model to Fine-tune via Rectified Scaling Law

    [https://arxiv.org/abs/2402.02314](https://arxiv.org/abs/2402.02314)

    该论文研究了在资源受限的情况下如何选择合适的预训练语言模型进行微调的问题。通过引入修正的缩放定律和预学习数据大小的概念，作者提出了一种新颖的模型选择算法，可以选择接近最优的模型。

    

    在日益增长的语言模型生态系统中，在众多选项中选择最合适的预训练模型进行微调成为了一个挑战。在资源受限的情况下，微调所有模型然后再进行选择是不现实的。在本文中，我们将这个资源受限的选择任务转化为预测微调性能，并且展示其与缩放定律之间的自然联系。与预训练不同，我们发现微调的缩放曲线不仅包括众所周知的“功率阶段”，还包括以前未被观察到的“预功率阶段”。我们还解释了为什么现有的缩放定律无法理论和实证地捕捉到这种相变现象。为了解决这个问题，我们将“预学习数据大小”概念引入到我们的修正缩放定律中，这克服了理论上的限制，并更好地适应实验结果。通过利用我们的定律，我们提出了一种新颖的语言模型选择算法，可以选择接近最优的模型。

    The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundr
    
[^334]: 对多模态大型语言模型的越狱攻击

    Jailbreaking Attack against Multimodal Large Language Model

    [https://arxiv.org/abs/2402.02309](https://arxiv.org/abs/2402.02309)

    本文提出了一种针对多模态大型语言模型的越狱攻击方法，通过找到“图像越狱提示”，实现在多个未知提示和图像上对语言模型的越狱，并展示了与单模态语言模型的越狱方法之间的联系。同时，通过构建的方法，将该越狱方法应用于单模态语言模型，较现有方法更高效。

    

    本文重点研究针对多模态大型语言模型（MLLMs）的越狱攻击，旨在引导MLLM生成对有害用户查询不当回应。提出了一种基于最大似然的算法来找到“图像越狱提示”（imgJP），实现对MLLM在多个未知提示和图像上的越狱（即数据通用属性）。我们的方法表现出较强的模型可转移性，生成的imgJP可以以黑盒方式转移到各种模型上进行越狱，包括MiniGPT-v2、LLaVA、InstructBLIP和mPLUG-Owl2。此外，我们揭示了MLLM越狱和LLM越狱之间的联系。因此，我们引入了一种基于构建的方法，利用我们的方法进行LLM越狱，展示了比当前最先进的方法更高的效率。代码链接在这里。注意：一些语言模型生成的内容可能对某些读者具有冒犯性。

    This paper focuses on jailbreaking attacks against multi-modal large language models (MLLMs), seeking to elicit MLLMs to generate objectionable responses to harmful user queries. A maximum likelihood-based algorithm is proposed to find an \emph{image Jailbreaking Prompt} (imgJP), enabling jailbreaks against MLLMs across multiple unseen prompts and images (i.e., data-universal property). Our approach exhibits strong model-transferability, as the generated imgJP can be transferred to jailbreak various models, including MiniGPT-v2, LLaVA, InstructBLIP, and mPLUG-Owl2, in a black-box manner. Moreover, we reveal a connection between MLLM-jailbreaks and LLM-jailbreaks. As a result, we introduce a construction-based method to harness our approach for LLM-jailbreaks, demonstrating greater efficiency than current state-of-the-art methods. The code is available here. \textbf{Warning: some content generated by language models may be offensive to some readers.}
    
[^335]: 由端到端深度学习模型加强的高效数值波传播

    Efficient Numerical Wave Propagation Enhanced by an End-to-End Deep Learning Model

    [https://arxiv.org/abs/2402.02304](https://arxiv.org/abs/2402.02304)

    本文提出了一个由端到端深度学习模型加强的高效数值波传播方法，通过结合数值求解器和深度学习组件，优化算法架构、数据生成和并行时间算法，实现了在保持速度的同时显著提高性能。

    

    在多个科学和工程领域，从地震建模到医学成像，对于高频波传播的高保真和高效解决方案的需求非常重要。最近在波传播模型中的一项进展利用足够准确的细求解器输出来训练神经网络，以提高快速但不准确的粗求解器的准确性。稳定且快速的求解器还允许使用并行时间算法Parareal来提取和纠正高频波组成部分。在本文中，我们在Nguyen和Tsai（2023）的工作基础上，提出了一个新颖的统一系统，将数值求解器与深度学习组件整合到端到端框架中。在提出的设置中，我们研究了神经网络架构、数据生成算法和Parareal方案的改进。我们的结果表明，这种协调的结构在不牺牲速度的情况下显著提高了性能，并且证明了

    In a variety of scientific and engineering domains, ranging from seismic modeling to medical imaging, the need for high-fidelity and efficient solutions for high-frequency wave propagation holds great significance. Recent advances in wave modeling use sufficiently accurate fine solver outputs to train neural networks that enhance the accuracy of a fast but inaccurate coarse solver. A stable and fast solver further allows the use of Parareal, a parallel-in-time algorithm to retrieve and correct high-frequency wave components. In this paper we build upon the work of Nguyen and Tsai (2023) and present a novel unified system that integrates a numerical solver with deep learning components into an end-to-end framework. In the proposed setting, we investigate refinements to the neural network architecture, data generation algorithm and Parareal scheme. Our results show that the cohesive structure significantly improves performance without sacrificing speed, and demonstrate the importance of 
    
[^336]: AI增强的侧信道分析的回顾和比较

    A Review and Comparison of AI Enhanced Side Channel Analysis

    [https://arxiv.org/abs/2402.02299](https://arxiv.org/abs/2402.02299)

    本文回顾和比较了AI增强的侧信道分析技术，重点研究了基于深度学习的模型攻击，并研究了一些新的由深度学习技术增强的方法。

    

    侧信道分析（SCA）对现代计算系统的隐私和安全构成了明确的威胁。绝大多数通信都是通过密码算法进行保护的。这些算法在密码学角度上往往是可证明安全的，但是它们在实际硬件上的实现会引入漏洞。攻击者可以利用这些漏洞来进行SCA，并恢复机密信息，如密钥或内部状态。随着机器学习，尤其是深度学习增强攻击变得更加普遍，SCA的威胁也大大增加。在这项工作中，我们将研究最新的深度学习技术在侧信道分析中的应用，以及其背后的理论和实施方法。我们的重点将放在使用深度学习技术进行的模型攻击上，同时还将研究一些新兴的由深度学习技术增强的方法，如非模型攻击、人工迹线生成等。

    Side Channel Analysis (SCA) presents a clear threat to privacy and security in modern computing systems. The vast majority of communications are secured through cryptographic algorithms. These algorithms are often provably-secure from a cryptographical perspective, but their implementation on real hardware introduces vulnerabilities. Adversaries can exploit these vulnerabilities to conduct SCA and recover confidential information, such as secret keys or internal states. The threat of SCA has greatly increased as machine learning, and in particular deep learning, enhanced attacks become more common. In this work, we will examine the latest state-of-the-art deep learning techniques for side channel analysis, the theory behind them, and how they are conducted. Our focus will be on profiling attacks using deep learning techniques, but we will also examine some new and emerging methodologies enhanced by deep learning techniques, such as non-profiled attacks, artificial trace generation, and
    
[^337]: 基于去噪扩散的非线性系统控制方法

    Denoising Diffusion-Based Control of Nonlinear Systems

    [https://arxiv.org/abs/2402.02297](https://arxiv.org/abs/2402.02297)

    我们提出了一种基于去噪扩散概率模型的新方法，用于控制非线性动力系统，通过学习反向控制系统使得终止状态属于目标集合，我们证明了对于控制仿射系统且满足可控性条件时，控制系统能够完全追踪正向过程的轨迹，并通过数值实验验证了我们的理论结果。

    

    我们提出了一种基于去噪扩散概率模型（DDPMs）的新方法，用于控制非线性动力系统。DDPMs是目前生成模型中的最先进技术，在各种采样任务中取得了成功。在我们的框架中，我们将反馈控制问题视为在控制系统约束下从目标集合中绘制样本的生成任务。DDPMs的正向过程通过添加噪声构建从目标集合起始的轨迹。我们学习如何反向控制动力系统，使得终止状态属于目标集合。对于没有漂移的控制仿射系统，我们证明了当可控性的李括号条件成立时，控制系统能够完全追踪正向过程的轨迹。我们在各种非线性系统上进行了数值研究，并验证了我们的理论结果。我们还在一个物理学实验案例中进行了超出理论结果的数值实验。

    We propose a novel approach based on Denoising Diffusion Probabilistic Models (DDPMs) to control nonlinear dynamical systems. DDPMs are the state-of-art of generative models that have achieved success in a wide variety of sampling tasks. In our framework, we pose the feedback control problem as a generative task of drawing samples from a target set under control system constraints. The forward process of DDPMs constructs trajectories originating from a target set by adding noise. We learn to control a dynamical system in reverse such that the terminal state belongs to the target set. For control-affine systems without drift, we prove that the control system can exactly track the trajectory of the forward process in reverse, whenever the the Lie bracket based condition for controllability holds. We numerically study our approach on various nonlinear systems and verify our theoretical results. We also conduct numerical experiments for cases beyond our theoretical results on a physics-eng
    
[^338]: 球形数据的拟合度和聚类：R和Python中的QuadratiK软件包

    Goodness-of-Fit and Clustering of Spherical Data: the QuadratiK package in R and Python

    [https://arxiv.org/abs/2402.02290](https://arxiv.org/abs/2402.02290)

    QuadratiK软件包是一个在R和Python中实现的数据分析工具，它提供了一套全面的拟合度测试和基于核方法的聚类技术，特别适用于处理球形数据。

    

    我们介绍了QuadratiK软件包，该软件包包含了创新的数据分析方法。该软件包在R和Python中实现，提供了一套全面的适应度拟合测试和基于核方法的二次距离的聚类技术，从而弥合了统计学和机器学习文献之间的差距。我们的软件实现了单样本、双样本和k样本适应度拟合测试，提供了一种高效且数学上合理的方法来评估概率分布的拟合度。我们的软件扩展了功能，包括基于泊松核密度的$d$维球上均匀性测试，以及从泊松核密度中生成随机样本的算法。特别值得注意的是，我们的软件还包括一种针对球形数据而特别量身定制的独特聚类算法，该算法利用了球面上基于泊松核密度的混合模型。同时，我们的软件还包括其他图形功能。

    We introduce the QuadratiK package that incorporates innovative data analysis methodologies. The presented software, implemented in both R and Python, offers a comprehensive set of goodness-of-fit tests and clustering techniques using kernel-based quadratic distances, thereby bridging the gap between the statistical and machine learning literatures. Our software implements one, two and k-sample tests for goodness of fit, providing an efficient and mathematically sound way to assess the fit of probability distributions. Expanded capabilities of our software include supporting tests for uniformity on the $d$-dimensional Sphere based on Poisson kernel densities, and algorithms for generating random samples from Poisson kernel densities. Particularly noteworthy is the incorporation of a unique clustering algorithm specifically tailored for spherical data that leverages a mixture of Poisson-kernel-based densities on the sphere. Alongside this, our software includes additional graphical func
    
[^339]: 图机器学习基础的未来方向

    Future Directions in Foundations of Graph Machine Learning

    [https://arxiv.org/abs/2402.02287](https://arxiv.org/abs/2402.02287)

    图机器学习领域的未来方向应该是发展一个更加均衡的理论，从更完整的角度探究图神经网络的表达能力、泛化和优化之间的相互关系。

    

    随着图数据在不同学科（从生命科学到社会科学和工程科学）上的广泛应用，图机器学习，尤其是使用图神经网络（GNNs），引起了人们浓厚的兴趣。尽管在实际应用中取得了成功，但我们对GNNs性质的理论理解仍然非常不完整。最近的理论发展主要集中在阐明GNNs粗粒度表达能力方面，主要采用组合技巧。然而，这些研究与实践并不完全一致，特别是在使用随机一阶优化技术训练GNNs时，对GNNs的泛化行为的理解。在这篇定位论文中，我们认为图机器学习领域需要将注意力转移到发展一个更加均衡的图机器学习理论上来，重点关注表达能力、泛化和优化的相互关系的更全面的理解。

    Machine learning on graphs, especially using graph neural networks (GNNs), has seen a surge in interest due to the wide availability of graph data across a broad spectrum of disciplines, from life to social and engineering sciences. Despite their practical success, our theoretical understanding of the properties of GNNs remains highly incomplete. Recent theoretical advancements primarily focus on elucidating the coarse-grained expressive power of GNNs, predominantly employing combinatorial techniques. However, these studies do not perfectly align with practice, particularly in understanding the generalization behavior of GNNs when trained with stochastic first-order optimization techniques. In this position paper, we argue that the graph machine learning community needs to shift its attention to developing a more balanced theory of graph machine learning, focusing on a more thorough understanding of the interplay of expressive power, generalization, and optimization.
    
[^340]: 多级特征聚合和递归对齐网络用于实时语义分割

    Multi-Level Feature Aggregation and Recursive Alignment Network for Real-Time Semantic Segmentation

    [https://arxiv.org/abs/2402.02286](https://arxiv.org/abs/2402.02286)

    该论文提出了一种在实时推理速度下实现高分割准确性的多级特征聚合和递归对齐网络。使用ResNet-18作为骨干，通过多级特征聚合模块和递归对齐模块来提高模型性能。

    

    实时语义分割对于实际应用非常重要。然而，许多方法都着重于降低计算复杂性和模型大小，但同时牺牲了准确性。在一些场景下，如自主导航和驾驶员辅助系统，准确性和速度同样重要。为了解决这个问题，我们提出了一种新颖的多级特征聚合和递归对齐网络（MFARANet），旨在实现高分割准确性和实时推理速度。我们使用ResNet-18作为骨干来保证效率，并提出了三个核心组件来弥补浅骨干引起的模型容量减少。具体而言，我们首先设计多级特征聚合模块（MFAM），将编码器中的分层特征聚合到每个尺度，以便于后续的空间对齐和多尺度推理。然后，我们通过结合基于流的对齐来建立递归对齐模块（RAM）。

    Real-time semantic segmentation is a crucial research for real-world applications. However, many methods lay particular emphasis on reducing the computational complexity and model size, while largely sacrificing the accuracy. In some scenarios, such as autonomous navigation and driver assistance system, accuracy and speed are equally important. To tackle this problem, we propose a novel Multi-level Feature Aggregation and Recursive Alignment Network (MFARANet), aiming to achieve high segmentation accuracy at real-time inference speed. We employ ResNet-18 as the backbone to ensure efficiency, and propose three core components to compensate for the reduced model capacity due to the shallow backbone. Specifically, we first design Multi-level Feature Aggregation Module (MFAM) to aggregate the hierarchical features in the encoder to each scale to benefit subsequent spatial alignment and multi-scale inference. Then, we build Recursive Alignment Module (RAM) by combining the flow-based alignm
    
[^341]: SynthDST: 少样本对话状态跟踪所需的全部是合成数据

    SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking

    [https://arxiv.org/abs/2402.02285](https://arxiv.org/abs/2402.02285)

    SynthDST是一个针对对话状态跟踪设计的数据生成框架，利用合成数据来实现少样本提示，通过使用少量手工对话模板和对话模式，它能够生成自然、连贯和流畅的带有DST注释的对话，并使Join连通率提升4-5％.

    

    在上下文学习中，大型语言模型（LLM）已成为对话状态跟踪（DST）研究的一个有希望的方向。然而，表现最好的上下文学习方法涉及检索和添加类似的示例到提示中，需要访问标记的训练数据。在多个领域和应用中获取这样的训练数据非常耗时、昂贵，有时是不可行的。虽然零样本学习不需要训练数据，但在少样本设置中明显落后。因此，“我们是否可以为任何对话模式有效地生成合成数据，以实现少样本提示？”针对这个问题，我们提出了一个名为\method的数据生成框架，专门针对DST，利用LLM。我们的方法只需要对话模式和一些手工对话模板，就能合成自然、连贯和流畅的带有DST注释的对话。使用{\method}的少样本学习结果显示，Join连通率提升了4-5％。

    In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST). However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible. While zero-shot learning requires no training data, it significantly lags behind the few-shot setup. Thus, `\textit{Can we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?}' Addressing this question, we propose \method, a data generation framework tailored for DST, utilizing LLMs. Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations. Few-shot learning using data from {\method} results in $4-5%$ improvement in Join
    
[^342]: 因果贝叶斯优化通过外源分布学习

    Causal Bayesian Optimization via Exogenous Distribution Learning

    [https://arxiv.org/abs/2402.02277](https://arxiv.org/abs/2402.02277)

    本文引入了一种新的方法，通过学习外源变量的分布，提高了结构化因果模型的近似精度，并将因果贝叶斯优化扩展到更一般的因果方案。

    

    在结构化因果模型中，将目标变量最大化作为操作目标是一个重要的问题。现有的因果贝叶斯优化（CBO）方法要么依赖于改变因果结构以最大化奖励的硬干预，要么引入动作节点到内生变量中，以调整数据生成机制以实现目标。本文引入了一种新的方法来学习外源变量的分布，这在现有方法中通常被忽略或通过期望进行边缘化。外源分布学习提高了通常通过有限观测数据训练的代理模型中的结构化因果模型的近似精度。此外，学习到的外源分布将现有的CBO扩展到超出加性噪声模型（ANM）的一般因果方案。恢复外源变量使我们能够为噪声或未观测到的隐藏变量使用更灵活的先验。引入了一种新的CBO方法。

    Maximizing a target variable as an operational objective in a structured causal model is an important problem. Existing Causal Bayesian Optimization (CBO) methods either rely on hard interventions that alter the causal structure to maximize the reward; or introduce action nodes to endogenous variables so that the data generation mechanisms are adjusted to achieve the objective. In this paper, a novel method is introduced to learn the distribution of exogenous variables, which is typically ignored or marginalized through expectation by existing methods.   Exogenous distribution learning improves the approximation accuracy of structured causal models in a surrogate model that is usually trained with limited observational data. Moreover, the learned exogenous distribution extends existing CBO to general causal schemes beyond Additive Noise Models (ANM). The recovery of exogenous variables allows us to use a more flexible prior for noise or unobserved hidden variables. A new CBO method is 
    
[^343]: SudokuSens: 使用生成方法增强IoT感知应用的深度学习鲁棒性

    SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing Applications using a Generative Approach

    [https://arxiv.org/abs/2402.02275](https://arxiv.org/abs/2402.02275)

    SudokuSens是一种生成框架，用于通过自动生成训练数据来提高IoT感知应用的深度学习模型鲁棒性。该框架通过模仿实际数据采集过程中未遇到的实验配置，增加了数据的多样性。

    

    本文介绍了SudokuSens，一种用于机器学习驱动的物联网（IoT）应用中自动生成训练数据的生成框架，生成的合成数据模仿实际传感器数据采集过程中未遇到的实验配置。该框架提高了深度学习模型的鲁棒性，并适用于数据采集成本高昂的IoT应用。该工作的动机是因为IoT时间序列数据纠缠了观察对象的特征以及周围环境的混杂内在属性和动态环境干扰。为了在IoT训练数据中加入足够的多样性，因此需要考虑与对象数量和可能遇到的环境条件成倍增加的训练用例的组合爆炸。我们的框架大大减少了这些多样性。

    This paper introduces SudokuSens, a generative framework for automated generation of training data in machine-learning-based Internet-of-Things (IoT) applications, such that the generated synthetic data mimic experimental configurations not encountered during actual sensor data collection. The framework improves the robustness of resulting deep learning models, and is intended for IoT applications where data collection is expensive. The work is motivated by the fact that IoT time-series data entangle the signatures of observed objects with the confounding intrinsic properties of the surrounding environment and the dynamic environmental disturbances experienced. To incorporate sufficient diversity into the IoT training data, one therefore needs to consider a combinatorial explosion of training cases that are multiplicative in the number of objects considered and the possible environmental conditions in which such objects may be encountered. Our framework substantially reduces these mult
    
[^344]: InceptionCapsule: 采用自注意力机制的Inception-Resnet和CapsuleNet用于医学图像分类

    InceptionCapsule: Inception-Resnet and CapsuleNet with self-attention for medical image Classification

    [https://arxiv.org/abs/2402.02274](https://arxiv.org/abs/2402.02274)

    InceptionCapsule方法采用了自注意力机制、迁移学习和Inception-ResNet模型，在医学图像分类中解决了初始权重选择和特征提取的问题，取得了较高的准确率。

    

    在深度神经网络中，初始权重的选择对于网络的性能至关重要，因为随机选择的权重可能产生不同的输出，增加过拟合和欠拟合的概率。另一方面，基于向量的特征提取方法需要丰富的向量以实现更准确的分类。本文提出了InceptionCapsule方法来解决这两个问题。该方法利用迁移学习和Inception-ResNet模型，避免随机选择权重，从ImageNet中获取初始权重。同时，利用Inception中间层的输出生成丰富的向量。提取的向量被传递给装备有注意力机制的胶囊网络进行学习。使用Kvasir数据集和BUSI with GT数据集对这种方法进行了评估。该模型在5类分类上实现了97.62%的准确率，在8类分类上实现了94.30%的准确率。

    Initial weighting is significant in deep neural networks because the random selection of weights produces different outputs and increases the probability of overfitting and underfitting. On the other hand, vector-based approaches to extract vector features need rich vectors for more accurate classification. The InceptionCapsule approach is presented to alleviate these two problems. This approach uses transfer learning and the Inception-ResNet model to avoid random selection of weights, which takes initial weights from ImageNet. It also uses the output of Inception middle layers to generate rich vectors. Extracted vectors are given to a capsule network for learning, which is equipped with an attention technique. Kvasir data and BUSI with the GT dataset were used to evaluate this approach. This model was able to achieve 97.62 accuracies in 5-class classification and also achieved 94.30 accuracies in 8-class classification on Kvasir. In the BUSI with GT dataset, the proposed approach achi
    
[^345]: 具有新知识的联邦学习: 基础，进展和未来展望

    Federated Learning with New Knowledge: Fundamentals, Advances, and Futures

    [https://arxiv.org/abs/2402.02268](https://arxiv.org/abs/2402.02268)

    本文系统地探讨了具有新知识的联邦学习，包括如何有效地融入新特征、任务、模型和算法，并分析了新知识到达的形式和时间对纳入过程的影响。同时讨论了联邦学习具有新知识时的潜在未来发展方向。

    

    联邦学习（FL）是一种隐私保护的分布式学习方法，在隐私保护日益重视的时代迅速发展。正是这种快速发展趋势，以及现实世界中对联邦学习的不断新需求的出现，促使我们专注于一个非常重要的问题：具有新知识的联邦学习。主要挑战在于如何将各种新知识有效地融入现有的FL系统中，并使这些系统降低成本，延长寿命，并促进可持续发展。在本文中，我们系统地定义了FL中新知识的主要来源，包括新特征、任务、模型和算法。对于每个来源，我们深入分析和讨论如何将新知识纳入现有的FL系统，并研究新知识到达的形式和时间对纳入过程的影响。此外，我们全面讨论了联邦学习具有新知识时的潜在未来发展方向。

    Federated Learning (FL) is a privacy-preserving distributed learning approach that is rapidly developing in an era where privacy protection is increasingly valued. It is this rapid development trend, along with the continuous emergence of new demands for FL in the real world, that prompts us to focus on a very important problem: Federated Learning with New Knowledge. The primary challenge here is to effectively incorporate various new knowledge into existing FL systems and evolve these systems to reduce costs, extend their lifespan, and facilitate sustainable development. In this paper, we systematically define the main sources of new knowledge in FL, including new features, tasks, models, and algorithms. For each source, we thoroughly analyze and discuss how to incorporate new knowledge into existing FL systems and examine the impact of the form and timing of new knowledge arrival on the incorporation process. Furthermore, we comprehensively discuss the potential future directions for
    
[^346]: MixedNUTS: 通过非线性混合分类器实现无需训练的准确性和鲁棒性平衡

    MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers

    [https://arxiv.org/abs/2402.02263](https://arxiv.org/abs/2402.02263)

    MixedNUTS是一种无需训练的方法，通过非线性混合分类器的转换和概率混合来实现准确性和鲁棒性的平衡。

    

    鲁棒性往往牺牲了准确性，阻碍了鲁棒分类模型在实际应用中的使用。基于训练的解决方案在与已训练的大型高性能模型兼容性方面存在限制，因此需要探索无需训练的集成方法。我们观察到鲁棒模型在干净数据和对抗数据上的正确预测比错误预测更自信，我们推测通过增强这种“良性置信度特性”可以在集成环境中实现准确性和鲁棒性的平衡。为了实现这一点，我们提出了“MixedNUTS”，一种无需训练的方法，利用仅有三个参数的非线性转换来处理鲁棒分类器和标准非鲁棒分类器的输出Logits，并通过高效算法进行优化。然后，MixedNUTS将转换后的Logits转换为概率，并将它们混合作为最终的输出。在CIFAR-10、CIFAR-100和ImageNet数据集上进行了实验。

    Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet da
    
[^347]: XTSFormer: 跨时空尺度的Transformer用于不规则时间事件预测

    XTSFormer: Cross-Temporal-Scale Transformer for Irregular Time Event Prediction

    [https://arxiv.org/abs/2402.02258](https://arxiv.org/abs/2402.02258)

    XTSFormer是一个用于不规则时间事件预测的跨时空尺度的Transformer模型，通过新颖的循环感知时间位置编码和分层的多尺度时间注意机制来解决不规则时间间隔、循环、周期性和多尺度事件交互等挑战。

    

    事件预测旨在基于历史事件序列预测未来事件的时间和类型。尽管其重要性，但存在几个挑战，包括连续事件之间时间间隔的不规则性、循环、周期性和多尺度事件交互，以及长事件序列的高计算成本。现有的神经时间点过程（TPP）方法不能捕捉事件交互的多尺度特性，而这在许多实际应用中（如临床事件数据）很常见。为了解决这些问题，我们提出了跨时空尺度的Transformer（XTSFormer），特别适用于不规则时间事件数据。我们的模型包含两个关键组成部分：一种新颖的基于特征的循环感知时间位置编码（FCPE），能够灵活捕捉时间的循环性质，以及一个分层的多尺度时间注意机制。这些尺度由自底向上的聚类算法确定。

    Event prediction aims to forecast the time and type of a future event based on a historical event sequence. Despite its significance, several challenges exist, including the irregularity of time intervals between consecutive events, the existence of cycles, periodicity, and multi-scale event interactions, as well as the high computational costs for long event sequences. Existing neural temporal point processes (TPPs) methods do not capture the multi-scale nature of event interactions, which is common in many real-world applications such as clinical event data. To address these issues, we propose the cross-temporal-scale transformer (XTSFormer), designed specifically for irregularly timed event data. Our model comprises two vital components: a novel Feature-based Cycle-aware Time Positional Encoding (FCPE) that adeptly captures the cyclical nature of time, and a hierarchical multi-scale temporal attention mechanism. These scales are determined by a bottom-up clustering algorithm. Extens
    
[^348]: 频率解释了大型语言模型尺寸、训练数据量和惊讶程度适应阅读时间的反相关关系

    Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times

    [https://arxiv.org/abs/2402.02255](https://arxiv.org/abs/2402.02255)

    本研究发现，当基于Transformer的语言模型变得越来越大并且在大量数据上进行训练时，模型的惊讶估计与自然人阅读时间的适应性下降。而词频是解释这种适应性下降的关键因素，较大模型变体过度准确地预测了人群中最不频繁的词汇，而较大模型的训练过程中更准确地学习了罕见的词汇，这解释了训练数据量和模型尺寸对适应阅读时间的不利影响。

    

    最近的研究表明，随着基于Transformer的语言模型变得越来越大并在大量数据上进行训练，它们的惊讶估计与自然人阅读时间的适应性下降。本研究通过一系列分析显示，词频是这两个趋势背后的关键解释因素。首先，来自四个语言模型家族在四个语料库上的残差误差显示，模型尺寸与适应阅读时间之间的反相关在最不频繁的词汇子集上最为显著，这是由较大模型变体过度准确的预测所推动。此外，训练动态显示，在后期训练步骤中，所有模型变体学习预测罕见的词汇，并且较大模型变体的预测更为准确，这解释了训练数据量和模型尺寸对适应阅读时间的负面影响。最后，特征归因分析证明较大的模型变体能够更好地预测罕见的词汇。

    Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades. The current work presents a series of analyses showing that word frequency is a key explanatory factor underlying these two trends. First, residual errors from four language model families on four corpora show that the inverse correlation between model size and fit to reading times is the strongest on the subset of least frequent words, which is driven by excessively accurate predictions of larger model variants. Additionally, training dynamics reveal that during later training steps, all model variants learn to predict rare words and that larger model variants do so more accurately, which explains the detrimental effect of both training data amount and model size on fit to reading times. Finally, a feature attribution analysis demonstrates that larger model variants are able t
    
[^349]: 基于教师-学生学习的无线供能通信中低复杂度中继选择

    Teacher-Student Learning based Low Complexity Relay Selection in Wireless Powered Communications

    [https://arxiv.org/abs/2402.02254](https://arxiv.org/abs/2402.02254)

    本文研究了在非线性能量收集条件下多源多中继射频能量收集网络中的联合中继选择、调度和功率控制问题。提出了两种基于卷积神经网络的架构，分别采用传统的2D卷积块和Inception块，以提高网络性能。

    

    射频能量收集网络是大规模物联网的关键因素，通过为能量有限的设备提供可控和长距离的能量传输。中继器在能量或信息传输方面的帮助已被证明可以显著提高这些网络的性能。本文研究了非线性能量收集条件下多源多中继射频能量收集网络中的联合中继选择、调度和功率控制问题。首先，我们针对给定的中继选择获取了调度和功率控制问题的最优解。然后，将中继选择问题构建为一个分类问题，提出了两种基于卷积神经网络（CNN）的架构。第一种架构采用传统的2D卷积块，并从层之间引入跳跃连接的优势；第二种架构使用Inception块替换它们，以减小可训练的参数大小而不会牺牲性能。

    Radio Frequency Energy Harvesting (RF-EH) networks are key enablers of massive Internet-of-things by providing controllable and long-distance energy transfer to energy-limited devices. Relays, helping either energy or information transfer, have been demonstrated to significantly improve the performance of these networks. This paper studies the joint relay selection, scheduling, and power control problem in multiple-source-multiple-relay RF-EH networks under nonlinear EH conditions. We first obtain the optimal solution to the scheduling and power control problem for the given relay selection. Then, the relay selection problem is formulated as a classification problem, for which two convolutional neural network (CNN) based architectures are proposed. While the first architecture employs conventional 2D convolution blocks and benefits from skip connections between layers; the second architecture replaces them with inception blocks, to decrease trainable parameter size without sacrificing 
    
[^350]: 不要重复标记：在有限预算下比较二元分类器时，数量胜过质量

    Don't Label Twice: Quantity Beats Quality when Comparing Binary Classifiers on a Budget

    [https://arxiv.org/abs/2402.02249](https://arxiv.org/abs/2402.02249)

    在比较两个二元分类器的准确性时，通过收集更多样本的单个标签而不是汇总多个噪声标签能更好地利用预算。

    

    我们研究了如何更好地利用有限预算来比较两个二元分类器的准确性。通常的做法是通过多次收集和汇总给定数据点的多个噪声标签，通过多数投票形成一个不太噪声的标签。我们证明了一个与常识相反的定理。如果目标是确定两个分类器中的较好者，我们展示了更好的做法是将预算用于收集更多样本的单个标签。我们的结果来自于对Cram\'er定理的非平凡应用，这是大偏差理论中的一个重要工具。我们讨论了我们的工作对机器学习基准设计的影响，其中它们推翻了一些历史上的建议。此外，我们的结果提供了比Hoeffding界更优的样本大小界限。

    We study how to best spend a budget of noisy labels to compare the accuracy of two binary classifiers. It's common practice to collect and aggregate multiple noisy labels for a given data point into a less noisy label via a majority vote. We prove a theorem that runs counter to conventional wisdom. If the goal is to identify the better of two classifiers, we show it's best to spend the budget on collecting a single label for more samples. Our result follows from a non-trivial application of Cram\'er's theorem, a staple in the theory of large deviations. We discuss the implications of our work for the design of machine learning benchmarks, where they overturn some time-honored recommendations. In addition, our results provide sample size bounds superior to what follows from Hoeffding's bound.
    
[^351]: ExTTNet:一种用于从发票图像中提取表格文字的深度学习算法

    ExTTNet: A Deep Learning Algorithm for Extracting Table Texts from Invoice Images

    [https://arxiv.org/abs/2402.02246](https://arxiv.org/abs/2402.02246)

    ExTTNet是一种深度学习算法，能够自动从发票图像中提取表格文字。使用光学字符识别（OCR）技术获取文本，并通过特征提取方法提高准确度。通过多层神经网络模型进行训练，最终获得了0.92的F1分数。

    

    在这项工作中，通过一个被称为ExTTNet的深度学习模型，自动地从发票图像中提取产品表格。首先使用光学字符识别（OCR）技术从发票图像中获取文本。在此过程中，使用了Tesseract OCR引擎 [37]。然后，通过使用特征提取方法增加现有特征的数量来提高准确度。根据每个OCR获得的文本是否是表格元素，进行标记处理。在本研究中，使用了多层人工神经网络模型进行训练。使用Nvidia RTX 3090显卡进行训练，耗时162分钟。训练的结果，F1分数为0.92。

    In this work, product tables in invoices are obtained autonomously via a deep learning model, which is named as ExTTNet. Firstly, text is obtained from invoice images using Optical Character Recognition (OCR) techniques. Tesseract OCR engine [37] is used for this process. Afterwards, the number of existing features is increased by using feature extraction methods to increase the accuracy. Labeling process is done according to whether each text obtained as a result of OCR is a table element or not. In this study, a multilayer artificial neural network model is used. The training has been carried out with an Nvidia RTX 3090 graphics card and taken $162$ minutes. As a result of the training, the F1 score is $0.92$.
    
[^352]: 重访基于生成对抗网络的不平衡数据集二进制语义分割

    Revisiting Generative Adversarial Networks for Binary Semantic Segmentation on Imbalanced Datasets

    [https://arxiv.org/abs/2402.02245](https://arxiv.org/abs/2402.02245)

    这项工作提出了一种基于生成对抗网络的深度学习框架，用于像素级别的路面异常区域检测。通过两个训练阶段和多尺度特征表示，该框架增强了生成器从异构输入中估计概率特征图的能力，并通过引入几种注意机制，解决了在严重不平衡数据集上训练模型时性能恶化的问题。

    

    异常路面表面状况检测旨在通过算法自动检测代表异常状态（如裂缝）的像素。最近，深度学习模型在相关领域取得了杰出的性能。然而，大多数现有的深度学习相关解决方案很少在多样化数据集上实现稳定的性能。为解决这个问题，在这项工作中，我们提出了一个基于条件生成对抗网络的深度学习框架，用于像素级别的路面异常区域检测。具体而言，我们的框架通过两个训练阶段和多尺度特征表示来提高生成器从异构输入中估计概率特征图的能力。此外，将几种注意机制纳入所提出的框架中，以减轻模型训练在严重不平衡数据集上性能恶化的问题。

    Anomalous pavement surface conditions detection aims to detect pixels representing anomalous states, such as cracks, on pavement surface images automatically by algorithms. Recently, deep learning models have been intensively applied to related topics with outstanding performance. However, most existing deep learning-related solutions rarely achieve a stable performance on diverse datasets. To address this issue, in this work, we propose a deep learning framework based on conditional Generative Adversarial Networks for anomalous region detection on pavement images at the pixel level. In particular, the proposed framework is developed to enhance the generator's ability to estimate the probability feature map from heterogeneous inputs with two training stages and multiscale feature representation. Moreover, several attention mechanisms are incorporated into the proposed framework to mitigate the performance deterioration of model training on severely imbalanced datasets. We implement exp
    
[^353]: 超越极限：扩展大型语言模型中上下文长度的技术综述

    Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models

    [https://arxiv.org/abs/2402.02244](https://arxiv.org/abs/2402.02244)

    这篇论文综述了近期为扩展大型语言模型中上下文长度而设计的技术和方法，并回顾了包括架构修改在内的多种技术，使得语言模型可以更有效地理解长上下文。

    

    近期，大型语言模型（LLMs）展现出了令人惊异的能力，包括理解上下文、进行逻辑推理和生成响应。然而，这是以严格的计算和内存要求为代价的，限制了它们有效支持长输入序列的能力。本综述全面回顾了最近为扩展LLMs序列长度而设计的技术和方法，从而增强其对长上下文理解的能力。具体而言，我们回顾和分类了各种技术，包括修改位置编码和修改注意机制等架构修改，旨在增强对更长序列的处理，同时避免计算需求的成比例增加。本研究探讨的多样方法可以在LLMs的不同阶段（即训练、微调和推理）中利用。这使得LLMs可以有效地处理长序列并提升对长上下文的理解能力。

    Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational requirements. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to effic
    
[^354]: 面向预训练视觉模型的参数高效微调：一项综述

    Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey

    [https://arxiv.org/abs/2402.02242](https://arxiv.org/abs/2402.02242)

    本综述调研了面向预训练视觉模型的参数高效微调方法，通过最小参数修改超越全面微调的性能，提供了全面的概述和未来方向，并提供了丰富的资源收藏。

    

    大规模预训练的视觉模型（PVMs）展示了在各种下游视觉任务中的适应能力潜力。然而，随着最先进的PVMs达到数十亿甚至数万亿个参数，标准的全面微调范式由于高计算和存储需求变得不可持续。作为响应，研究人员正在探索参数高效微调（PEFT），旨在以最小参数修改超越全面微调的性能。本综述提供了视觉PEFT的全面概述和未来方向，对最新进展进行了系统审查。首先，我们提供了PEFT的正式定义，并讨论了模型预训练方法。然后，我们将现有方法分为三类：基于添加的、基于部分的和基于统一的。最后，我们介绍了常用的数据集和应用，并提出了潜在的未来研究挑战。该综述还提供了丰富的资源收藏。

    Large-scale pre-trained vision models (PVMs) have shown great potential for adaptability across various downstream vision tasks. However, with state-of-the-art PVMs growing to billions or even trillions of parameters, the standard full fine-tuning paradigm is becoming unsustainable due to high computational and storage demands. In response, researchers are exploring parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance of full fine-tuning with minimal parameter modifications. This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements. First, we provide a formal definition of PEFT and discuss model pre-training methods. We then categorize existing methods into three categories: addition-based, partial-based, and unified-based. Finally, we introduce the commonly used datasets and applications and suggest potential future research challenges. A comprehensive collection of resources is
    
[^355]: 分布约简：用格罗莫夫-瓦瑟斯坦投影统一降维和聚类

    Distributional Reduction: Unifying Dimensionality Reduction and Clustering with Gromov-Wasserstein Projection

    [https://arxiv.org/abs/2402.02239](https://arxiv.org/abs/2402.02239)

    本文提出了一种新的分布约简方法，利用格罗莫夫-瓦瑟斯坦投影统一了降维和聚类，通过优化问题同时解决降维和聚类，实验证明了该方法在多个领域表现出卓越性能。

    

    无监督学习旨在捕捉潜在的大规模和高维数据集的结构。传统上，这涉及使用降维方法将数据投影到可解释的空间上，或将数据点组织成有意义的聚类。在实践中，这些方法通常是按顺序使用的，而不能保证聚类与降维相一致。在这项工作中，我们提出了一个新的观点：使用分布。通过利用最优输运的工具，特别是格罗莫夫-瓦瑟斯坦距离，我们将聚类和降维统一为一个称为分布约简的单一框架。这使我们能够通过单个优化问题同时解决聚类和降维。通过全面的实验证明了我们方法的多功能性和解释性，并表明它在各种图像和基因组数据集上优于现有方法。

    Unsupervised learning aims to capture the underlying structure of potentially large and high-dimensional datasets. Traditionally, this involves using dimensionality reduction methods to project data onto interpretable spaces or organizing points into meaningful clusters. In practice, these methods are used sequentially, without guaranteeing that the clustering aligns well with the conducted dimensionality reduction. In this work, we offer a fresh perspective: that of distributions. Leveraging tools from optimal transport, particularly the Gromov-Wasserstein distance, we unify clustering and dimensionality reduction into a single framework called distributional reduction. This allows us to jointly address clustering and dimensionality reduction with a single optimization problem. Through comprehensive experiments, we highlight the versatility and interpretability of our method and show that it outperforms existing approaches across a variety of image and genomics datasets.
    
[^356]: 带有差分隐私的联邦学习

    Federated Learning with Differential Privacy

    [https://arxiv.org/abs/2402.02230](https://arxiv.org/abs/2402.02230)

    本论文研究了带有差分隐私的联邦学习对模型性能的影响，发现在分布式和差分隐私设置下，非独立同分布和小型数据集的性能下降最为严重。

    

    联邦学习作为一种分布式机器学习方法，能够显著保护客户的私密数据，避免在不同参与方之间共享。然而，通过分析来自客户端上传的参数权重，私密信息仍然可能泄露。在本报告中，我们展示了关于客户数量和差分隐私机制对不同数据类型模型性能的实证基准测试。我们的结果表明，在分布式和差分隐私设置下，非独立同分布和小型数据集的性能下降最为严重。

    Federated learning (FL), as a type of distributed machine learning, is capable of significantly preserving client's private data from being shared among different parties. Nevertheless, private information can still be divulged by analyzing uploaded parameter weights from clients. In this report, we showcase our empirical benchmark of the effect of the number of clients and the addition of differential privacy (DP) mechanisms on the performance of the model on different types of data. Our results show that non-i.i.d and small datasets have the highest decrease in performance in a distributed and differentially private setting.
    
[^357]: 高维情况下，普通贝叶斯优化算法表现出色

    Vanilla Bayesian Optimization Performs Great in High Dimension

    [https://arxiv.org/abs/2402.02229](https://arxiv.org/abs/2402.02229)

    本文研究了高维情况下贝叶斯优化算法的问题，并提出了一种改进方法，通过对先验假设进行简单的缩放，使普通贝叶斯优化在高维任务中表现出色。

    

    长期以来，高维问题一直被认为是贝叶斯优化算法的软肋。受到维度噪音的刺激，许多算法旨在通过对目标应用各种简化假设来提高其性能。本文通过识别导致普通贝叶斯优化在高维任务中不适用的退化现象，并进一步展示了现有算法如何通过降低模型复杂度来应对这些退化现象。此外，我们还提出了一种对普通贝叶斯优化算法中典型先验假设的改进方法，该方法在不对目标施加结构性限制的情况下将复杂性降低到可管理的水平。我们的修改方法——通过维度对高斯过程长度先验进行简单的缩放——揭示了标准贝叶斯优化在高维情况下的显著改进，明确表明其效果远远超出以往的预期。

    High-dimensional problems have long been considered the Achilles' heel of Bayesian optimization algorithms. Spurred by the curse of dimensionality, a large collection of algorithms aim to make it more performant in this setting, commonly by imposing various simplifying assumptions on the objective. In this paper, we identify the degeneracies that make vanilla Bayesian optimization poorly suited to high-dimensional tasks, and further show how existing algorithms address these degeneracies through the lens of lowering the model complexity. Moreover, we propose an enhancement to the prior assumptions that are typical to vanilla Bayesian optimization algorithms, which reduces the complexity to manageable levels without imposing structural restrictions on the objective. Our modification - a simple scaling of the Gaussian process lengthscale prior with the dimensionality - reveals that standard Bayesian optimization works drastically better than previously thought in high dimensions, clearly
    
[^358]: 重思出发点：通过协作预训练增强联邦学习的性能和公平性

    Rethinking the Starting Point: Enhancing Performance and Fairness of Federated Learning via Collaborative Pre-Training

    [https://arxiv.org/abs/2402.02225](https://arxiv.org/abs/2402.02225)

    本文提出了一种名为CoPreFL的协作预训练方法，该方法通过设计一个可适应任何联邦学习任务的预训练模型来提高性能和公平性。大量实验证实了该方法在提供可靠的初始化方面的有效性。

    

    大多数现有的联邦学习方法假设训练从一个随机初始化的模型开始。最近的研究实证了利用预训练模型可以为联邦学习提供有益的初始化。在本文中，我们提出了一种协作预训练方法CoPreFL，该方法通过策略性地设计一个预训练模型，为任何下游联邦学习任务提供良好的初始化。我们的预训练算法的关键思想是模仿下游分布式场景的元学习过程，使其能够适应任何未知的联邦学习任务。CoPreFL的预训练优化过程也在平均性能和公平性之间取得了平衡，旨在通过智能初始化来解决下游联邦学习任务中的竞争挑战。大量实验结果验证了我们的预训练方法为任何未知的下游联邦学习任务提供了可靠的初始化，从而提高了平均性能。

    Most existing federated learning (FL) methodologies have assumed training begins from a randomly initialized model. Recently, several studies have empirically demonstrated that leveraging a pre-trained model can offer advantageous initializations for FL. In this paper, we propose a collaborative pre-training approach, CoPreFL, which strategically designs a pre-trained model to serve as a good initialization for any downstream FL task. The key idea of our pre-training algorithm is a meta-learning procedure which mimics downstream distributed scenarios, enabling it to adapt to any unforeseen FL task. CoPreFL's pre-training optimization procedure also strikes a balance between average performance and fairness, with the aim of addressing these competing challenges in downstream FL tasks through intelligent initializations. Extensive experimental results validate that our pre-training method provides a robust initialization for any unseen downstream FL task, resulting in enhanced average pe
    
[^359]: 图基础模型

    Graph Foundation Models

    [https://arxiv.org/abs/2402.02216](https://arxiv.org/abs/2402.02216)

    图基础模型是图领域中一个新兴的研究课题，旨在开发一个能够跨不同图和任务进行泛化的图模型。我们提出了一种新的 GFM 发展视角，通过倡导“图词汇表”来编码图的不变性，有助于推进未来的GFM设计。

    

    图基础模型（Graph Foundation Model，GFM）是图领域中一个新兴的研究课题，旨在开发一个能够跨不同图和任务进行泛化的图模型。然而，目前还没有实现一个多功能的GFM。构建GFM的关键挑战在于如何能在具有不同结构模式的图之间实现正向迁移。受计算机视觉（CV）和自然语言处理（NLP）领域基础模型的启发，我们提出了一种新的 GFM 发展视角，通过倡导“图词汇表”，即潜藏于图中的基本可迁移单元来编码图的不变性。我们从网络分析、理论基础和稳定性等重要方面来建立图词汇表。这种词汇表的视角有助于按照神经缩放定律推进未来的GFM设计。

    Graph Foundation Model (GFM) is a new trending research topic in the graph domain, aiming to develop a graph model capable of generalizing across different graphs and tasks. However, a versatile GFM has not yet been achieved. The key challenge in building GFM is how to enable positive transfer across graphs with diverse structural patterns. Inspired by the existing foundation models in the CV and NLP domains, we propose a novel perspective for the GFM development by advocating for a ``graph vocabulary'', in which the basic transferable units underlying graphs encode the invariance on graphs. We ground the graph vocabulary construction from essential aspects including network analysis, theoretical foundations, and stability. Such a vocabulary perspective can potentially advance the future GFM design following the neural scaling laws.
    
[^360]: 最短路径和最小斯坦纳树之间的查询决策回归

    Query-decision Regression between Shortest Path and Minimum Steiner Tree

    [https://arxiv.org/abs/2402.02211](https://arxiv.org/abs/2402.02211)

    该论文探讨了最短路径和最小斯坦纳树之间的查询决策回归问题，提出了两种有原则的学习框架，并通过实验研究证明了问题可得到较好的解决。

    

    考虑一个具有未知权重的图，如果我们知道一些节点子集对应的最小斯坦纳树，我们能否找到一对节点的最短路径？也就是说，对于一个固定的潜在决策系统（例如加权图），我们试图通过利用与另一个优化问题（例如最小斯坦纳树问题）相关的信息来解决一个优化问题（例如最短路径问题）。在本文中，我们研究了这样一个原型问题，称为“具有任务转移的查询决策回归”，重点研究了最短路径问题和最小斯坦纳树问题。我们提供了关于构建评分模型的可实现假设空间的理论见解，并提出了两种有原则的学习框架。我们的实验研究表明，这类问题可以在具有统计显著性的程度上解决。

    Considering a graph with unknown weights, can we find the shortest path for a pair of nodes if we know the minimal Steiner trees associated with some subset of nodes? That is, with respect to a fixed latent decision-making system (e.g., a weighted graph), we seek to solve one optimization problem (e.g., the shortest path problem) by leveraging information associated with another optimization problem (e.g., the minimal Steiner tree problem). In this paper, we study such a prototype problem called \textit{query-decision regression with task shifts}, focusing on the shortest path problem and the minimum Steiner tree problem. We provide theoretical insights regarding the design of realizable hypothesis spaces for building scoring models, and present two principled learning frameworks. Our experimental studies show that such problems can be solved to a decent extent with statistical significance.
    
[^361]: 隐式神经表示的可平铺材质纹理

    Implicit Neural Representation of Tileable Material Textures

    [https://arxiv.org/abs/2402.02208](https://arxiv.org/abs/2402.02208)

    本文研究了一种使用正弦神经网络表示周期性可平铺纹理的隐式神经表示方法。该方法通过使用傅立叶级数初始化神经网络的第一层，实现了在任意空间坐标上直接评估纹理，避免了插值。通过添加正则化项和泊松方程，确保生成可平铺的纹理，并能高效重建高分辨率纹理。应用于反锯齿表面能够提供高视觉保真度和锐度的效果。

    

    我们探索使用正弦神经网络来表示周期性可平铺纹理。我们的方法通过用整数频率初始化正弦神经网络的第一层，利用傅立叶级数。我们证明正弦层的组合只生成具有周期P的整数频率。因此，我们的网络学习了周期模式的连续表示，可以直接在任意空间坐标上进行评估，无需插值。为了使结果模式可平铺，我们在损失函数中添加了基于泊松方程的正则化项。我们提出的神经隐式表示紧凑且能够高效重建具有多个层次细节的高分辨率纹理，并具有高视觉保真度和锐度。我们展示了在反锯齿表面领域中应用我们方法的案例。

    We explore sinusoidal neural networks to represent periodic tileable textures. Our approach leverages the Fourier series by initializing the first layer of a sinusoidal neural network with integer frequencies with a period $P$. We prove that the compositions of sinusoidal layers generate only integer frequencies with period $P$. As a result, our network learns a continuous representation of a periodic pattern, enabling direct evaluation at any spatial coordinate without the need for interpolation. To enforce the resulting pattern to be tileable, we add a regularization term, based on the Poisson equation, to the loss function. Our proposed neural implicit representation is compact and enables efficient reconstruction of high-resolution textures with high visual fidelity and sharpness across multiple levels of detail. We present applications of our approach in the domain of anti-aliased surface.
    
[^362]: 安全微调几乎零成本：大规模语言模型视觉基线

    Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models

    [https://arxiv.org/abs/2402.02207](https://arxiv.org/abs/2402.02207)

    该论文研究了大规模语言模型在安全微调过程中存在的问题，并提出了一种可行的解决方案。他们首先策划了一个包含各种有害类别的视觉语言安全指令遵循数据集VLGuard，并通过实验证明将该数据集集成到视觉语言微调中或进行事后微调，可以有效地对齐VLLMs的安全性。这对模型的有益性几乎没有影响甚至有所提升。这项研究的贡献是提供了一个有价值的资源，用于对现有VLLMs进行安全测试、训练新模型或保护预训练VLLMs。

    

    当前的大规模语言模型（VLLMs）具有非凡的能力，但容易生成有害内容，并且容易受到简单越狱攻击的影响。我们的初步分析发现，这是由于在视觉语言指令微调过程中存在有害数据，并且VLLM的微调会导致对底层LLM之前学习的安全对齐性的遗忘。为了解决这个问题，我们首先策划了一个包含各种有害类别的视觉语言安全指令遵循数据集VLGuard。我们的实验表明，将该数据集集成到标准的视觉语言微调中或将其用于事后微调，可以有效地对齐VLLMs的安全性。这种对齐是在对模型的有益性几乎没有影响甚至有所提升的情况下实现的。我们安全微调数据集的多样性使其成为对现有VLLMs进行安全测试，训练新模型或保护预训练VLLMs的宝贵资源。

    Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue, we first curate a vision-language safe instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on, or even enhancement of, the models' helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs, training new models or safeguarding pre-trained VLLMs. Empi
    
[^363]: 多模态协同编排通过多任务贝叶斯优化探索组合库中的结构-性质关系

    Multimodal Co-orchestration for Exploring Structure-Property Relationships in Combinatorial Libraries via Multi-Task Bayesian Optimization

    [https://arxiv.org/abs/2402.02198](https://arxiv.org/abs/2402.02198)

    本研究提出并实现了一种多模态协同编排的方法，通过多任务贝叶斯优化来探索组合库中的结构-性质关系。该方法利用变分自动编码器的降维和表示学习来控制复杂可观测量的测量。

    

    自动化和自主仪器的快速发展带来了多模态工具的协同编排机会，这些工具配备了多种顺序检测方法，或者多个表征工具来同时探索相同样品。这可以以组合库为例，可以通过多个工具同时在多个位置探索，或者在自动合成系统中进行下游表征。在协同编排方法中，从一种模态中获得的信息应该加速其他模态的发现。相应地，协同编排代理应该根据预期的知识增益和测量成本来选择测量模态。在这里，我们提出并实施了一种用于进行具有复杂可观测量（如光谱或图像）的测量的协同编排方法。该方法依赖于变分自动编码器的降维和表示学习来控制潜在空间。

    The rapid growth of automated and autonomous instrumentations brings forth an opportunity for the co-orchestration of multimodal tools, equipped with multiple sequential detection methods, or several characterization tools to explore identical samples. This can be exemplified by the combinatorial libraries that can be explored in multiple locations by multiple tools simultaneously, or downstream characterization in automated synthesis systems. In the co-orchestration approaches, information gained in one modality should accelerate the discovery of other modalities. Correspondingly, the orchestrating agent should select the measurement modality based on the anticipated knowledge gain and measurement cost. Here, we propose and implement a co-orchestration approach for conducting measurements with complex observables such as spectra or images. The method relies on combining dimensionality reduction by variational autoencoders with representation learning for control over the latent space 
    
[^364]: 并行大规模排序和选择问题的样本高效聚类及征服方法

    Sample-Efficient Clustering and Conquer Procedures for Parallel Large-Scale Ranking and Selection

    [https://arxiv.org/abs/2402.02196](https://arxiv.org/abs/2402.02196)

    我们提出了一种新颖的并行大规模排序和选择问题的聚类及征服方法，通过利用相关信息进行聚类以提高样本效率，在大规模AI应用中表现优异。

    

    我们提出了一种新颖的"聚类和征服"方法，用于解决并行大规模排序和选择问题，通过利用相关信息进行聚类，以打破样本效率的瓶颈。在并行计算环境中，基于相关性的聚类可以实现O(p)的样本复杂度减少速度，这是理论上可达到的最佳减少速度。我们提出的框架是通用的，在固定预算和固定精度的范式下，可以无缝集成各种常见的排序和选择方法。它可以在无需高精确度相关估计和精确聚类的情况下实现改进。在大规模人工智能应用中，如神经结构搜索，我们的无筛选版本的方法惊人地超过了完全顺序化的基准，表现出更高的样本效率。这表明利用有价值的结构信息，如相关性，是绕过传统方法的一条可行路径。

    We propose novel "clustering and conquer" procedures for the parallel large-scale ranking and selection (R&S) problem, which leverage correlation information for clustering to break the bottleneck of sample efficiency. In parallel computing environments, correlation-based clustering can achieve an $\mathcal{O}(p)$ sample complexity reduction rate, which is the optimal reduction rate theoretically attainable. Our proposed framework is versatile, allowing for seamless integration of various prevalent R&S methods under both fixed-budget and fixed-precision paradigms. It can achieve improvements without the necessity of highly accurate correlation estimation and precise clustering. In large-scale AI applications such as neural architecture search, a screening-free version of our procedure surprisingly surpasses fully-sequential benchmarks in terms of sample efficiency. This suggests that leveraging valuable structural information, such as correlation, is a viable path to bypassing the trad
    
[^365]: 在组合优化问题中寻找多样化解决方案的连续张量放松方法

    Continuous Tensor Relaxation for Finding Diverse Solutions in Combinatorial Optimization Problems

    [https://arxiv.org/abs/2402.02190](https://arxiv.org/abs/2402.02190)

    本研究提出了连续张量放松方法(CTRA)，用于在组合优化问题中寻找多样化的解决方案。CTRA通过对离散决策变量进行连续放松，解决了寻找多样化解决方案的挑战。

    

    在组合优化问题中，寻找最佳解是最常见的目标。然而，在实际场景中，单一解决方案可能不适用，因为目标函数和约束条件只是原始现实世界情况的近似值。为了解决这个问题，寻找具有不同特征的多样化解决方案和约束严重性的变化成为自然的方向。这种策略提供了在后处理过程中选择合适解决方案的灵活性。然而，发现这些多样化解决方案比确定单一解决方案更具挑战性。为了克服这一挑战，本研究引入了连续张量松弛退火 (CTRA) 方法，用于基于无监督学习的组合优化求解器。CTRA通过扩展连续松弛方法，将离散决策变量转换为连续张量，同时解决了多个问题。该方法找到了不同特征的多样化解决方案和约束严重性的变化。

    Finding the best solution is the most common objective in combinatorial optimization (CO) problems. However, a single solution may not be suitable in practical scenarios, as the objective functions and constraints are only approximations of original real-world situations. To tackle this, finding (i) "heterogeneous solutions", diverse solutions with distinct characteristics, and (ii) "penalty-diversified solutions", variations in constraint severity, are natural directions. This strategy provides the flexibility to select a suitable solution during post-processing. However, discovering these diverse solutions is more challenging than identifying a single solution. To overcome this challenge, this study introduces Continual Tensor Relaxation Annealing (CTRA) for unsupervised-learning-based CO solvers. CTRA addresses various problems simultaneously by extending the continual relaxation approach, which transforms discrete decision variables into continual tensors. This method finds heterog
    
[^366]: 进化引导的生成流网络

    Evolution Guided Generative Flow Networks

    [https://arxiv.org/abs/2402.02186](https://arxiv.org/abs/2402.02186)

    本文提出了进化引导的生成流网络（EGFN），用于处理长时间跨度和稀疏奖励的挑战。通过使用进化算法训练一组代理参数，并将结果轨迹存储在优先回放缓冲区中，我们的方法在训练GFlowNets代理时展现出了很高的效果。

    

    生成流网络（GFlowNets）是一类概率生成模型，用于学习按照奖励比例对组合对象进行采样。GFlowNets的一个重大挑战是在处理长时间跨度和稀疏奖励时有效训练它们。为了解决这个问题，我们提出了进化引导的生成流网络（EGFN），这是对GFlowNets训练的一种简单但强大的增强方法，使用进化算法（EA）进行训练。我们的方法可以在任何GFlowNets训练目标的基础上工作，通过使用EA训练一组代理参数，将结果轨迹存储在优先回放缓冲区中，并使用存储的轨迹训练GFlowNets代理。我们在广泛的玩具和真实世界基准任务上进行了深入研究，展示了我们方法在处理长轨迹和稀疏奖励方面的有效性。

    Generative Flow Networks (GFlowNets) are a family of probabilistic generative models that learn to sample compositional objects proportional to their rewards. One big challenge of GFlowNets is training them effectively when dealing with long time horizons and sparse rewards. To address this, we propose Evolution guided generative flow networks (EGFN), a simple but powerful augmentation to the GFlowNets training using Evolutionary algorithms (EA). Our method can work on top of any GFlowNets training objective, by training a set of agent parameters using EA, storing the resulting trajectories in the prioritized replay buffer, and training the GFlowNets agent using the stored trajectories. We present a thorough investigation over a wide range of toy and real-world benchmark tasks showing the effectiveness of our method in handling long trajectories and sparse rewards.
    
[^367]: 跨域推荐中的扩散模型

    Diffusion Cross-domain Recommendation

    [https://arxiv.org/abs/2402.02182](https://arxiv.org/abs/2402.02182)

    该论文研究了跨域推荐中的扩散模型，通过从辅助领域中添加数据来解决冷启动用户数据稀疏问题。其中，映射模块起着关键的作用，确定了跨域推荐模型的性能。

    

    对于推荐系统来说，给与冷启动用户高质量的推荐结果一直是一个挑战。解决目标领域中冷启动用户数据稀疏问题的一个潜在解决方案是从辅助领域添加数据。找到一种合适的方法从辅助领域提取知识并将其转移到目标领域是跨域推荐（CDR）研究的主要目标之一。在现有的方法中，映射方法是一种流行的方法来实现跨域推荐模型（CDRs）。对于这种类型的模型，映射模块起着将数据从一个域转换到另一个域的作用。它主要决定了映射方法CDRs的性能。最近，扩散概率模型（DPMs）在图像合成相关任务中取得了令人印象深刻的成功。它们涉及从添加噪声的样本中恢复图像，这可以看作是具有出色性能的数据转换过程。

    It is always a challenge for recommender systems to give high-quality outcomes to cold-start users. One potential solution to alleviate the data sparsity problem for cold-start users in the target domain is to add data from the auxiliary domain. Finding a proper way to extract knowledge from an auxiliary domain and transfer it into a target domain is one of the main objectives for cross-domain recommendation (CDR) research. Among the existing methods, mapping approach is a popular one to implement cross-domain recommendation models (CDRs). For models of this type, a mapping module plays the role of transforming data from one domain to another. It primarily determines the performance of mapping approach CDRs. Recently, diffusion probability models (DPMs) have achieved impressive success for image synthesis related tasks. They involve recovering images from noise-added samples, which can be viewed as a data transformation process with outstanding performance. To further enhance the perfo
    
[^368]: 通过优化抽象的方式进行Slate Bandit策略的离策略评估

    Off-Policy Evaluation of Slate Bandit Policies via Optimizing Abstraction

    [https://arxiv.org/abs/2402.02171](https://arxiv.org/abs/2402.02171)

    我们提出了一种名为潜在IPS（LIPS）的新的Slate Bandit OPE估计器，通过在低维度的Slate抽象空间中定义重要性权重，并通过数据驱动的方式优化Slate抽象来减小偏差和方差。

    

    我们研究了Slate上下文强盗问题中的离策略评估（OPE），其中一个策略选择称为slates的多维动作。这个问题在推荐系统、搜索引擎、营销以及医疗应用中广泛存在，然而，由于动作空间大，典型的逆倾向评分（IPS）估计器存在较大的方差，使得有效的OPE成为一个重大挑战。伪逆（PI）估计器已被引入以减小方差问题，通过假设奖励函数线性，但这可能导致显著的偏差，因为这个假设在观测数据中很难验证并且经常会被实质性违反。为了解决之前估计器的局限性，我们开发了一种新的Slate Bandit OPE估计器，称为潜在IPS（LIPS），它在低维度的Slate抽象空间中定义了重要性权重，我们通过数据驱动的方式优化Slate抽象来最小化LIPS的偏差和方差。

    We study off-policy evaluation (OPE) in the problem of slate contextual bandits where a policy selects multi-dimensional actions known as slates. This problem is widespread in recommender systems, search engines, marketing, to medical applications, however, the typical Inverse Propensity Scoring (IPS) estimator suffers from substantial variance due to large action spaces, making effective OPE a significant challenge. The PseudoInverse (PI) estimator has been introduced to mitigate the variance issue by assuming linearity in the reward function, but this can result in significant bias as this assumption is hard-to-verify from observed data and is often substantially violated. To address the limitations of previous estimators, we develop a novel estimator for OPE of slate bandits, called Latent IPS (LIPS), which defines importance weights in a low-dimensional slate abstraction space where we optimize slate abstractions to minimize the bias and variance of LIPS in a data-driven way. By do
    
[^369]: 跨域动态链接预测的一种图模型

    One Graph Model for Cross-domain Dynamic Link Prediction

    [https://arxiv.org/abs/2402.02168](https://arxiv.org/abs/2402.02168)

    DyExpert是一种用于跨域链接预测的动态图模型，通过明确建模历史演化过程并结合链接预测，它可以学习特定下游图的演化模式，并在各个领域上取得了最先进的性能。

    

    本研究提出了DyExpert，一种用于跨域链接预测的动态图模型。它可以明确地建模历史演化过程，学习特定下游图的演化模式，并进而进行特定模式的链接预测。DyExpert采用了解码器优化的transformer，并通过结合演化建模和链接预测的“条件链接生成”实现了高效的并行训练和推断。DyExpert在包含6百万个动态边的广泛动态图上进行训练。在八个未训练的图上进行了大量实验，结果显示DyExpert在跨域链接预测中取得了最先进的性能。与相同设置下的先进基准相比，DyExpert在八个图上的平均精确度提高了11.40％。更令人印象深刻的是，在六个未训练的图上，它超过了八个先进基线的全监督性能。

    This work proposes DyExpert, a dynamic graph model for cross-domain link prediction. It can explicitly model historical evolving processes to learn the evolution pattern of a specific downstream graph and subsequently make pattern-specific link predictions. DyExpert adopts a decode-only transformer and is capable of efficiently parallel training and inference by \textit{conditioned link generation} that integrates both evolution modeling and link prediction. DyExpert is trained by extensive dynamic graphs across diverse domains, comprising 6M dynamic edges. Extensive experiments on eight untrained graphs demonstrate that DyExpert achieves state-of-the-art performance in cross-domain link prediction. Compared to the advanced baseline under the same setting, DyExpert achieves an average of 11.40% improvement Average Precision across eight graphs. More impressive, it surpasses the fully supervised performance of 8 advanced baselines on 6 untrained graphs.
    
[^370]: Vi(E)va LLM！一个用于评估和解释生成AI可视化的概念模型栈

    Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting Generative AI-based Visualizations

    [https://arxiv.org/abs/2402.02167](https://arxiv.org/abs/2402.02167)

    这篇论文介绍了一个名为EvaLLM的概念模型栈，用于评估和解释基于生成AI的可视化。它解决了使用大型语言模型生成可视化时遇到的问题，并提出了一种理论评估的方法。

    

    自动生成可视化是一项古老的任务，多年来越来越受到研究和实践社区的关注。最近，大型语言模型（LLM）已成为支持与可视化相关的生成任务的有趣选择，并展示了初步的有希望的结果。与此同时，存在诸多问题，如指导LLM生成所需结果的多种方式，引导生成的不同视角（基于代码、基于图像、基于语法），以及即使在可视化生成任务中也存在的幻觉，使得它们的使用不如预期的那样可行。在类似为LLM进行基准测试的倡议下，本文针对通过LLM对生成的可视化建模的评估问题进行了研究。我们提出了一个理论上的评估模型栈，EvaLLM，它将评估工作分解为其原子组成部分，并对其性质进行了描述，并概述了如何进行评估的概述。

    The automatic generation of visualizations is an old task that, through the years, has shown more and more interest from the research and practitioner communities. Recently, large language models (LLM) have become an interesting option for supporting generative tasks related to visualization, demonstrating initial promising results. At the same time, several pitfalls, like the multiple ways of instructing an LLM to generate the desired result, the different perspectives leading the generation (code-based, image-based, grammar-based), and the presence of hallucinations even for the visualization generation task, make their usage less affordable than expected. Following similar initiatives for benchmarking LLMs, this paper copes with the problem of modeling the evaluation of a generated visualization through an LLM. We propose a theoretical evaluation stack, EvaLLM, that decomposes the evaluation effort in its atomic components, characterizes their nature, and provides an overview of how
    
[^371]: 对抗鲁棒Q学习的最优化研究与贝尔曼无穷误差

    Towards Optimal Adversarial Robust Q-learning with Bellman Infinity-error

    [https://arxiv.org/abs/2402.02165](https://arxiv.org/abs/2402.02165)

    本文研究了对抗鲁棒Q学习的最优化，并证明了一种确定性且稳态的最优鲁棒策略存在，该策略与贝尔曼最优策略一致。同时，阐明了在最小化贝尔曼误差以获得最优鲁棒策略时使用$L^{\infty}$-范数的必要性。

    

    构建鲁棒策略对于抵御影响深度强化学习（DRL）智能体的攻击或干扰至关重要。最近的研究探索了状态对抗鲁棒性，并暗示了缺乏最优鲁棒策略（ORP）的潜在问题，这给设定严格的鲁棒性约束带来了挑战。本文进一步研究了ORP：首先，我们引入了策略一致性假设（CAP），该假设指出马尔可夫决策过程中的最优动作在微小扰动下保持一致，得到了实证和理论证据的支持。在CAP的基础上，我们关键地证明了一种确定性且稳态的ORP的存在，该ORP与贝尔曼最优策略一致。此外，我们还阐明了在最小化贝尔曼误差以获得ORP时，$L^{\infty}$-范数的必要性。这一发现澄清了先前针对贝尔曼最优策略使用$L^{1}$-范数的DRL算法的脆弱性，并激励我们训练一种一致性对抗鲁棒深度Q网络（CA）

    Establishing robust policies is essential to counter attacks or disturbances affecting deep reinforcement learning (DRL) agents. Recent studies explore state-adversarial robustness and suggest the potential lack of an optimal robust policy (ORP), posing challenges in setting strict robustness constraints. This work further investigates ORP: At first, we introduce a consistency assumption of policy (CAP) stating that optimal actions in the Markov decision process remain consistent with minor perturbations, supported by empirical and theoretical evidence. Building upon CAP, we crucially prove the existence of a deterministic and stationary ORP that aligns with the Bellman optimal policy. Furthermore, we illustrate the necessity of $L^{\infty}$-norm when minimizing Bellman error to attain ORP. This finding clarifies the vulnerability of prior DRL algorithms that target the Bellman optimal policy with $L^{1}$-norm and motivates us to train a Consistent Adversarial Robust Deep Q-Network (CA
    
[^372]: 一个贝叶斯聚类有效性指数

    A Bayesian cluster validity index

    [https://arxiv.org/abs/2402.02162](https://arxiv.org/abs/2402.02162)

    该论文提出了一个基于贝叶斯方法的聚类有效性指数，该指数根据现有的基础指数定义，并用于检测次优聚类数，通过与其他指数进行比较，验证了其有效性。

    

    在应用聚类算法时，选择聚类数是关键步骤之一。为了完成这个任务，引入了各种聚类有效性指数（CVIs）。大多数聚类有效性指数都被定义为检测数据集中隐藏的最优聚类数。然而，用户有时并不期望获得最优聚类数，而是更适合他们应用的次优聚类数。这促使我们引入了一种基于现有基础指数的贝叶斯聚类有效性指数（BCVI）。该指数基于狄利克雷或广义狄利克雷先验定义，得到相同的后验分布。然后我们基于Wiroonsri指数（WI）和Wiroonsri-Preedasawakul指数（WP）作为硬聚类和软聚类的基础指数来测试我们的BCVI。我们将它们的结果与原始的基础指数以及一些其他存在的CVIs（包括Davies and Bouldin (DB)，Starczewski (STR)）进行比较。

    Selecting the number of clusters is one of the key processes when applying clustering algorithms. To fulfill this task, various cluster validity indices (CVIs) have been introduced. Most of the cluster validity indices are defined to detect the optimal number of clusters hidden in a dataset. However, users sometimes do not expect to get the optimal number of groups but a secondary one which is more reasonable for their applications. This has motivated us to introduce a Bayesian cluster validity index (BCVI) based on existing underlying indices. This index is defined based on either Dirichlet or Generalized Dirichlet priors which result in the same posterior distribution. Our BCVI is then tested based on the Wiroonsri index (WI), and the Wiroonsri-Preedasawakul index (WP) as underlying indices for hard and soft clustering, respectively. We compare their outcomes with the original underlying indices, as well as a few more existing CVIs including Davies and Bouldin (DB), Starczewski (STR)
    
[^373]: 评估越野自动驾驶分割模型对抗攻击的抗干扰性：以数据集为中心的分析

    Evaluating the Robustness of Off-Road Autonomous Driving Segmentation against Adversarial Attacks: A Dataset-Centric analysis

    [https://arxiv.org/abs/2402.02154](https://arxiv.org/abs/2402.02154)

    本研究评估了越野自动驾驶分割模型对抗攻击的鲁棒性，并提出了一个稳健数据集来研究非稳健特征对对抗攻击的影响。

    

    本研究调查了在越野自动驾驶领域中，语义分割模型对抗输入扰动的脆弱性。尽管在通用条件下表现良好，但最先进的分类器往往容易受到甚至是较小的扰动的影响，最终导致高置信度的不准确预测。以往的研究将重点放在通过修改架构并使用嘈杂的输入图像进行训练来使模型更加稳健，但却没有探索数据集在对抗攻击中的影响。本研究旨在通过研究越野数据集中的非稳健特征的影响，并比较对不同分割网络架构的对抗攻击的影响来填补这一空白。为了实现这一目标，我们创建了一个稳健的数据集，其中只包含稳健特征，并在该稳健数据集上训练网络。我们对研究结果进行了定性和定量分析，这些结果具有重要性。

    This study investigates the vulnerability of semantic segmentation models to adversarial input perturbations, in the domain of off- road autonomous driving. Despite good performance in generic conditions, the state-of-the-art classifiers are often susceptible to (even) small perturbations, ultimately resulting in inaccurate predic- tions with high confidence. Prior research has directed their focus on making models more robust by modifying the architecture and training with noisy input images, but has not explored the influence of datasets in adversarial attacks. Our study aims to address this gap by examining the impact of non-robust features in off-road datasets and comparing the effects of adversarial attacks on different seg- mentation network architectures. To enable this, a robust dataset is created consisting of only robust features and training the net- works on this robustified dataset. We present both qualitative and quantitative analysis of our findings, which have important
    
[^374]: 论文题目：为什么“摸着石头过河”方法主导推荐系统实践；呼吁摒弃反乌托邦思维

    Position Paper: Why the Shooting in the Dark Method Dominates Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking

    [https://arxiv.org/abs/2402.02152](https://arxiv.org/abs/2402.02152)

    这篇论文质疑了推荐系统实践中目前常用的“摸着石头过河”方法，呼吁摒弃反乌托邦思维。论文提出了使用深度学习堆栈的非标准用法，以解锁奖励优化的推荐系统的潜力。

    

    应用推荐系统研究处于一种奇特的境地。尽管在通过A/B测试来衡量性能方面有一个非常严格的协议，但找到要测试的“B”的最佳方法并没有明确地针对性能，而是针对一个代理指标。因此，一个A/B测试的成功或失败完全取决于所提出的代理指标是否与性能相关性更好。没有原则可以在离线情况下确定一个代理指标是否比另一个更好，这使得从业者们摸不着头脑。本论文的目的是质疑这种反乌托邦思维，并主张深度学习堆栈的非标准用法实际上有潜力解锁优化奖励的推荐系统。

    Applied recommender systems research is in a curious position. While there is a very rigorous protocol for measuring performance by A/B testing, best practice for finding a `B' to test does not explicitly target performance but rather targets a proxy measure. The success or failure of a given A/B test then depends entirely on if the proposed proxy is better correlated to performance than the previous proxy. No principle exists to identify if one proxy is better than another offline, leaving the practitioners shooting in the dark. The purpose of this position paper is to question this anti-Utopian thinking and argue that a non-standard use of the deep learning stacks actually has the potential to unlock reward optimizing recommendation.
    
[^375]: 改进无需重训练的传播模型用于逆问题，使用最优后验协方差

    Improving Diffusion Models for Inverse Problems Using Optimal Posterior Covariance

    [https://arxiv.org/abs/2402.02149](https://arxiv.org/abs/2402.02149)

    本文提出了一种改进无需重训练的传播模型的方法，通过优化后验协方差，提供了一种零样本解决方案，用于嘈杂的线性逆问题。根据最近的方法等价于对给定扩散噪声图像的干净图像的不可计算后验分布进行各向同性高斯近似的发现，我们提出了一种通用即插即用的后验协方差优化方法。为了实现无需重新训练的最优后验协方差，我们提供了基于两种方法的通用解决方案，这两种方法专门设计用于利用具有和不具有反向协方差的预训练模型。

    

    最近的传播模型为嘈杂的线性逆问题提供了有希望的零样本解决方案，无需为特定的逆问题重新训练。本文从条件抽样的反向传播过程中近似后验均值的角度，提出了对现有零样本方法的第一个统一解释。我们揭示了最近的方法等价于对给定扩散噪声图像的干净图像的不可计算后验分布进行各向同性高斯近似，唯一的差别是各向同性后验协方差的手工设计。受到这一发现的启示，我们提出了一种基于最大似然估计的通用即插即用后验协方差优化方法，以改进最近的方法。为了实现无需重新训练的最优后验协方差，我们提供了基于两种方法的通用解决方案，这两种方法专门设计用于利用具有和不具有反向协方差的预训练模型。

    Recent diffusion models provide a promising zero-shot solution to noisy linear inverse problems without retraining for specific inverse problems. In this paper, we propose the first unified interpretation for existing zero-shot methods from the perspective of approximating the conditional posterior mean for the reverse diffusion process of conditional sampling. We reveal that recent methods are equivalent to making isotropic Gaussian approximations to intractable posterior distributions over clean images given diffused noisy images, with the only difference in the handcrafted design of isotropic posterior covariances. Inspired by this finding, we propose a general plug-and-play posterior covariance optimization based on maximum likelihood estimation to improve recent methods. To achieve optimal posterior covariance without retraining, we provide general solutions based on two approaches specifically designed to leverage pre-trained models with and without reverse covariances. Experimen
    
[^376]: 利用深度集成森林实现基于MODIS MAIAC AOD的PM2.5高分辨率制图于伊朗德黑兰

    Using Deep Ensemble Forest for High Resolution Mapping of PM2.5 from MODIS MAIAC AOD in Tehran, Iran

    [https://arxiv.org/abs/2402.02139](https://arxiv.org/abs/2402.02139)

    本文评估了深度集成森林方法在从AOD数据估计PM2.5浓度方面的潜力，并发现其相比深度学习方法和随机森林等数据驱动方法具有更高的准确度。

    

    由于污染源复杂且地面空气质量监测站数量不足，高分辨率的细颗粒物（PM2.5）浓度制图是德黑兰市面临的挑战。作为替代，可以使用高分辨率的卫星气溶胶光学深度（AOD）数据来制图。为了达到这个目的，文献中使用了不同的数据驱动方法。最近，深度学习方法展示了其从AOD数据估计PM2.5的能力。然而，这些方法在解决从卫星AOD数据估计PM2.5的问题上存在一些弱点。本文评估了深度集成森林方法在从AOD数据估计PM2.5浓度方面的潜力。结果表明，深度集成森林方法（R2 = 0.74）比深度学习方法（R2 = 0.67）以及经典的数据驱动方法如随机森林（R2 = 0.68）具有更高的PM2.5估计精度。

    High resolution mapping of PM2.5 concentration over Tehran city is challenging because of the complicated behavior of numerous sources of pollution and the insufficient number of ground air quality monitoring stations. Alternatively, high resolution satellite Aerosol Optical Depth (AOD) data can be employed for high resolution mapping of PM2.5. For this purpose, different data-driven methods have been used in the literature. Recently, deep learning methods have demonstrated their ability to estimate PM2.5 from AOD data. However, these methods have several weaknesses in solving the problem of estimating PM2.5 from satellite AOD data. In this paper, the potential of the deep ensemble forest method for estimating the PM2.5 concentration from AOD data was evaluated. The results showed that the deep ensemble forest method with R2 = 0.74 gives a higher accuracy of PM2.5 estimation than deep learning methods (R2 = 0.67) as well as classic data-driven methods such as random forest (R2 = 0.68).
    
[^377]: 基于语法的进化方法用于具有特定领域操作和集成多样性的自动化工作流组合

    Grammar-based evolutionary approach for automated workflow composition with domain-specific operators and ensemble diversity

    [https://arxiv.org/abs/2402.02124](https://arxiv.org/abs/2402.02124)

    本文介绍了一种名为EvoFlow的基于语法的进化方法，用于自动化工作流组合。EvoFlow通过增强工作流结构的灵活性，并引入多样性集成策略，使从业者能够选择最适合其特定需求的算法。实验证明，EvoFlow在多个AutoML基准测试数据集上取得显著的性能提升，并在不同领域的实际应用中表现出优势。

    

    从原始数据中提取有价值和新颖见解的过程涉及一系列复杂的步骤。在自动机器学习（AutoML）领域中，重点研究自动化该过程的方面，特别是选择算法和优化超参数的任务。AutoML中一个特别具有挑战性的任务是自动化工作流组合（AWC）。AWC旨在识别最有效的数据预处理和机器学习算法序列，以及它们在特定数据集上的最佳超参数。然而，现有的AWC方法在组合工作流中的算法数量和方式方面存在限制。为了解决这一问题，本文引入了一种名为EvoFlow的基于语法的进化方法用于AWC。EvoFlow增强了设计工作流结构的灵活性，使从业者能够选择最适合他们特定需求的算法。EvoFlow通过集成两个创新特性脱颖而出。首先，它使用一种基于语法的表示方法来描述工作流，允许从预先定义的领域特定操作中选择适当的算法。其次，它引入多样性集成策略来鼓励工作流中不同算法的组合，以提高性能和鲁棒性。实验证明，EvoFlow在多个AutoML基准测试数据集上取得了显著的性能提升，并且在不同领域的实际应用中表现出了优势。

    The process of extracting valuable and novel insights from raw data involves a series of complex steps. In the realm of Automated Machine Learning (AutoML), a significant research focus is on automating aspects of this process, specifically tasks like selecting algorithms and optimising their hyper-parameters. A particularly challenging task in AutoML is automatic workflow composition (AWC). AWC aims to identify the most effective sequence of data preprocessing and ML algorithms, coupled with their best hyper-parameters, for a specific dataset. However, existing AWC methods are limited in how many and in what ways they can combine algorithms within a workflow.   Addressing this gap, this paper introduces EvoFlow, a grammar-based evolutionary approach for AWC. EvoFlow enhances the flexibility in designing workflow structures, empowering practitioners to select algorithms that best fit their specific requirements. EvoFlow stands out by integrating two innovative features. First, it emplo
    
[^378]: 通过使用深度学习生成合成的SAR-光学数据来提高农作物分类准确性

    Enhancing crop classification accuracy by synthetic SAR-Optical data generation using deep learning

    [https://arxiv.org/abs/2402.02121](https://arxiv.org/abs/2402.02121)

    通过使用深度学习生成合成的SAR-光学数据，可以增强农作物分类的准确性。然而，传统数据生成方法面临训练数据不平衡的挑战。

    

    农作物分类使用遥感数据已成为近几十年来的一个重要研究领域。研究表明，融合SAR和光学影像可以显著提高分类的准确性。然而，该领域面临的一个主要挑战是训练数据的有限可用性，这会对分类器的性能造成不利影响。在农业地区，主要作物通常包括一两种特定类型，而其他作物则很少。因此，在收集训练样本以创建农产品地图时，主要作物的样本充裕，形成大部分类别。相反，其他作物的样本很少，代表着少数类别。解决这个问题需要克服传统数据生成方法的一些挑战和弱点。这些方法已被用来解决训练数据不平衡的问题。然而，它们仍然面临着限制。

    Crop classification using remote sensing data has emerged as a prominent research area in recent decades. Studies have demonstrated that fusing SAR and optical images can significantly enhance the accuracy of classification. However, a major challenge in this field is the limited availability of training data, which adversely affects the performance of classifiers. In agricultural regions, the dominant crops typically consist of one or two specific types, while other crops are scarce. Consequently, when collecting training samples to create a map of agricultural products, there is an abundance of samples from the dominant crops, forming the majority classes. Conversely, samples from other crops are scarce, representing the minority classes. Addressing this issue requires overcoming several challenges and weaknesses associated with traditional data generation methods. These methods have been employed to tackle the imbalanced nature of the training data. Nevertheless, they still face lim
    
[^379]: 在分布式在线优化中处理延迟反馈的投影无关方法

    Handling Delayed Feedback in Distributed Online Optimization : A Projection-Free Approach

    [https://arxiv.org/abs/2402.02114](https://arxiv.org/abs/2402.02114)

    本研究提出了两种投影无关算法，用于处理分布式在线优化中的延迟反馈问题。这些算法在中心化和分布式设置中都能达到O(\sqrt{B})的遗憾界，对于延迟设置中的OCO问题是最优的。

    

    随着本地不断产生大量数据，边缘学习变得越来越重要。在这种范式下，需要简单（以便本地设备可以执行）和可靠的算法，以应对不确定性和网络问题，尤其是延迟。本研究探讨了在对抗性延迟反馈下的在线凸优化问题。我们提出了两种投影无关算法，用于中心化和分布式设置，并仔细设计以实现O(\sqrt{B})的遗憾界，其中B是延迟总和，对于延迟设置中的OCO问题来说是最优的，同时仍然是投影无关的。我们进行了广泛的理论研究，并通过与现有算法在实际问题上的比较来实验验证我们算法的性能。

    Learning at the edges has become increasingly important as large quantities of data are continually generated locally. Among others, this paradigm requires algorithms that are simple (so that they can be executed by local devices), robust (again uncertainty as data are continually generated), and reliable in a distributed manner under network issues, especially delays. In this study, we investigate the problem of online convex optimization under adversarial delayed feedback. We propose two projection-free algorithms for centralised and distributed settings in which they are carefully designed to achieve a regret bound of O(\sqrt{B}) where B is the sum of delay, which is optimal for the OCO problem in the delay setting while still being projection-free. We provide an extensive theoretical study and experimentally validate the performance of our algorithms by comparing them with existing ones on real-world problems.
    
[^380]: 加速贝叶斯优化中的前瞻：多层蒙特卡洛就够了

    Accelerating Look-ahead in Bayesian Optimization: Multilevel Monte Carlo is All you Need

    [https://arxiv.org/abs/2402.02111](https://arxiv.org/abs/2402.02111)

    本文利用多层蒙特卡洛方法加速贝叶斯优化中的前瞻过程，并证明在涉及嵌套期望和最大化的问题中具有优势。

    

    我们利用多层蒙特卡洛(MLMC)来提高涉及嵌套期望和最大化的多步前瞻贝叶斯优化(BO)方法的性能。普通蒙特卡洛的复杂度在嵌套操作中会降低，而MLMC能够以规范蒙特卡洛收敛速度解决这类问题，而且不依赖于维度和平滑性假设。我们的理论研究主要关注一步和两步前瞻采集函数的近似改进，但正如我们所讨论的，这种方法在多种方面是可推广的，包括超越BO的背景。我们通过数值验证了我们的发现，并在几个基准示例中展示了MLMC在BO中的优势。代码在这里获取：https://github.com/Shangda-Yang/MLMCBO。

    We leverage multilevel Monte Carlo (MLMC) to improve the performance of multi-step look-ahead Bayesian optimization (BO) methods that involve nested expectations and maximizations. The complexity rate of naive Monte Carlo degrades for nested operations, whereas MLMC is capable of achieving the canonical Monte Carlo convergence rate for this type of problem, independently of dimension and without any smoothness assumptions. Our theoretical study focuses on the approximation improvements for one- and two-step look-ahead acquisition functions, but, as we discuss, the approach is generalizable in various ways, including beyond the context of BO. Findings are verified numerically and the benefits of MLMC for BO are illustrated on several benchmark examples. Code is available here https://github.com/Shangda-Yang/MLMCBO.
    
[^381]: 复合主动学习：在具备理论保证的多领域主动学习中迈进

    Composite Active Learning: Towards Multi-Domain Active Learning with Theoretical Guarantees

    [https://arxiv.org/abs/2402.02110](https://arxiv.org/abs/2402.02110)

    复合主动学习为多领域主动学习提供了具备理论保证的方法，并且考虑了不同领域之间的相似性和数据分布变化。

    

    主动学习（AL）旨在通过选择信息量最大的数据点进行标注，从而在固定的标注预算内提高模型性能。现有的AL方法通常只适用于单一领域的情况，即所有数据都来自同一领域（例如，同一数据集）。然而，许多现实任务往往涉及多个领域。例如，在视觉识别中，通常希望训练一个能够在不同环境（例如，不同背景）中工作的图像分类器，其中每个环境的图像构成一个领域。这种多领域AL设置对于以前的方法来说是具有挑战性的，因为它们（1）在分配标注预算时忽视了不同领域之间的相似性，（2）无法处理不同领域之间的数据分布变化。在本文中，我们提出了第一个通用方法，称为复合主动学习（CAL），用于多领域AL。我们的方法显式地考虑了问题中的领域层级和实例层级信息；CAL首先分配

    Active learning (AL) aims to improve model performance within a fixed labeling budget by choosing the most informative data points to label. Existing AL focuses on the single-domain setting, where all data come from the same domain (e.g., the same dataset). However, many real-world tasks often involve multiple domains. For example, in visual recognition, it is often desirable to train an image classifier that works across different environments (e.g., different backgrounds), where images from each environment constitute one domain. Such a multi-domain AL setting is challenging for prior methods because they (1) ignore the similarity among different domains when assigning labeling budget and (2) fail to handle distribution shift of data across different domains. In this paper, we propose the first general method, dubbed composite active learning (CAL), for multi-domain AL. Our approach explicitly considers the domain-level and instance-level information in the problem; CAL first assigns
    
[^382]: 学习依赖类型的结构感知表示

    Learning Structure-Aware Representations of Dependent Types

    [https://arxiv.org/abs/2402.02104](https://arxiv.org/abs/2402.02104)

    本文扩展了Agda生态系统到机器学习领域，并发布了一种新颖的依赖类型编程语言的证明数据集。通过提出一种基于结构而非命名原则的新颖神经架构，我们能够准确地表示依赖类型程序，并在前提选择任务中取得了强大的初步结果。

    

    Agda是一种依赖类型编程语言和证明助手，在证明形式化和编程语言理论中起着关键作用。本文将Agda生态系统扩展到机器学习领域，并反过来使机器学习从业者能够使用Agda相关资源。我们介绍并发布了一种新颖的Agda程序证明数据集，该数据集既详尽又广泛，可以支持各种机器学习应用，这是首个这样的数据集。利用数据集的超高分辨率，详细展示了亚型级别的证明状态，我们提出了一种基于结构而不是命名原则准确表示依赖类型程序的新颖神经架构。我们在前提选择设置中实例化和评估我们的架构，取得了强大的初步结果。

    Agda is a dependently-typed programming language and a proof assistant, pivotal in proof formalization and programming language theory. This paper extends the Agda ecosystem into machine learning territory, and, vice versa, makes Agda-related resources available to machine learning practitioners. We introduce and release a novel dataset of Agda program-proofs that is elaborate and extensive enough to support various machine learning applications -- the first of its kind. Leveraging the dataset's ultra-high resolution, detailing proof states at the sub-type level, we propose a novel neural architecture targeted at faithfully representing dependently-typed programs on the basis of structural rather than nominal principles. We instantiate and evaluate our architecture in a premise selection setup, where it achieves strong initial results.
    
[^383]: 视觉语言模型中的心理现象记忆

    D\'ej\`a Vu Memorization in Vision-Language Models

    [https://arxiv.org/abs/2402.02103](https://arxiv.org/abs/2402.02103)

    这项研究提出了一种新方法来衡量视觉语言模型中的记忆现象，并发现对于使用图像-标题对进行训练的VLMs，模型确实会保留关于训练图像中的个别对象的信息，文本随机化可以在很大程度上减轻记忆现象而对模型的下游任务性能影响较小。

    

    视觉语言模型（VLM）作为最先进的表示学习解决方案出现，具有诸多下游应用，如图像分类、检索和生成。一个自然的问题是这些模型是否会记忆训练数据，这也对泛化有着影响。我们提出了一种衡量VLMs中记忆的新方法，称之为心理现象记忆。对于在图像-标题对上训练的VLMs，我们展示了该模型确实保留了关于训练图像中个别对象的信息，超出了从相关性或图像标题中可以推断出的范畴。我们在样本和总体水平上评估了心理现象记忆，并展示了OpenCLIP在多达5000万个图像-标题对上训练时的显著性。最后，我们展示了文本随机化在很大程度上减轻了记忆，同时对模型的下游任务性能产生了适度影响。

    Vision-Language Models (VLMs) have emerged as the state-of-the-art representation learning solution, with myriads of downstream applications such as image classification, retrieval and generation. A natural question is whether these models memorize their training data, which also has implications for generalization. We propose a new method for measuring memorization in VLMs, which we call d\'ej\`a vu memorization. For VLMs trained on image-caption pairs, we show that the model indeed retains information about individual objects in the training images beyond what can be inferred from correlations or the image caption. We evaluate d\'ej\`a vu memorization at both sample and population level, and show that it is significant for OpenCLIP trained on as many as 50M image-caption pairs. Finally, we show that text randomization considerably mitigates memorization while only moderately impacting the model's downstream task performance.
    
[^384]: 分析多语言语言模型中跨语言知识转移的评估

    Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models

    [https://arxiv.org/abs/2402.02099](https://arxiv.org/abs/2402.02099)

    分析了多语言语言模型中跨语言知识转移的评估方法和设置，发现高性能主要归因于非语言知识的因素，如任务和表层知识，并且跨语言传输的主要是数据工件和偏见，尤其是对于低资源语言。

    

    最近在大规模数据集上训练的多语言语言模型似乎显示出在跨语言知识转移和下游任务上取得了很高的性能。然而，我们对当前的评估基准和设置能够准确衡量零-shot跨语言知识转移的程度表示质疑。在这项工作中，我们通过引入更具挑战性的设置，涉及多语言实例，挑战了高零-shot性能在目标任务中反映高跨语言能力的假设。通过广泛的实验和分析，我们展示了多语言模型的高性能主要归因于不需要转移实际语言知识的因素，如任务和表层知识。更具体地说，我们观察到跨语言传输的主要是数据工件和偏见，特别是对于低资源语言。我们的发现突显了被忽视的缺点。

    Recent advances in training multilingual language models on large datasets seem to have shown promising results in knowledge transfer across languages and achieve high performance on downstream tasks. However, we question to what extent the current evaluation benchmarks and setups accurately measure zero-shot cross-lingual knowledge transfer. In this work, we challenge the assumption that high zero-shot performance on target tasks reflects high cross-lingual ability by introducing more challenging setups involving instances with multiple languages. Through extensive experiments and analysis, we show that the observed high performance of multilingual models can be largely attributed to factors not requiring the transfer of actual linguistic knowledge, such as task- and surface-level knowledge. More specifically, we observe what has been transferred across languages is mostly data artifacts and biases, especially for low-resource languages. Our findings highlight the overlooked drawbacks
    
[^385]: 自注意网络在QK特征值谱集中时进行定位

    Self-attention Networks Localize When QK-eigenspectrum Concentrates

    [https://arxiv.org/abs/2402.02098](https://arxiv.org/abs/2402.02098)

    本文研究了自注意网络中的注意力定位问题，通过QK特征值谱的集中定位现象来解决不同观点之间的矛盾。

    

    自注意机制在现代机器学习中非常流行。它具有适应性选择输入序列中的标记，并通过调节注意力定位的程度来实现，这被很多研究人员认为是强大模型性能的基础，但也复杂化了学习动力学的基本机制。近年来，主要有两种观点将注意力定位与模型性能联系起来。一种观点是秩坍缩，即自注意块嵌入的标记在不同的标记之间变得非常相似，导致网络表达能力降低。另一种观点是熵坍缩，即注意概率接近非均匀且熵低，使得学习动力学更容易陷入平台期。这两种失效模式似乎相互矛盾，因为秩和熵坍缩分别与均匀和非均匀注意力相关。为此，我们对QK特征值谱的集中定位进行了表征。

    The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the 
    
[^386]: 利用新颖性共享解决分散式多智能体协同探索问题

    Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing

    [https://arxiv.org/abs/2402.02097](https://arxiv.org/abs/2402.02097)

    提出了一种名为MACE的简单而有效的多智能体协同探索方法，通过共享局部新颖性来近似全局新颖性，并引入加权互信息来衡量智能体行动对其他智能体的影响，从而促进多智能体之间的协同探索。实验证明，MACE在稀疏奖励的多智能体环境中取得了出色的性能。

    

    分散式协作式多智能体强化学习中的探索面临两个挑战。一是全局状态的新颖性不可用，而局部观察的新颖性存在偏差。另一个挑战是智能体如何协调地进行探索。为了解决这些挑战，我们提出了一种简单但有效的多智能体协同探索方法MACE。通过仅传播局部新颖性，智能体可以考虑其他智能体的局部新颖性来近似全局新颖性。此外，我们新引入了加权互信息来衡量一个智能体的行动对其他智能体累计新颖性的影响。我们将其作为内在回报来鼓励智能体对其他智能体的探索产生更大的影响，从而促进协同探索。实验证明，MACE在三种稀疏奖励的多智能体环境中表现出优异的性能。

    Exploration in decentralized cooperative multi-agent reinforcement learning faces two challenges. One is that the novelty of global states is unavailable, while the novelty of local observations is biased. The other is how agents can explore in a coordinated way. To address these challenges, we propose MACE, a simple yet effective multi-agent coordinated exploration method. By communicating only local novelty, agents can take into account other agents' local novelty to approximate the global novelty. Further, we newly introduce weighted mutual information to measure the influence of one agent's action on other agents' accumulated novelty. We convert it as an intrinsic reward in hindsight to encourage agents to exert more influence on other agents' exploration and boost coordinated exploration. Empirically, we show that MACE achieves superior performance in three multi-agent environments with sparse rewards.
    
[^387]: 眼见未必为实：无害扰动空间的探索

    Seeing is not always believing: The Space of Harmless Perturbations

    [https://arxiv.org/abs/2402.02095](https://arxiv.org/abs/2402.02095)

    在深度神经网络中，我们发现了一种无害扰动空间的存在，这种扰动不会影响网络对原始图像的输出。我们证明了在输入维度超过输出维度的情况下，存在一个连续的无害扰动子空间。我们还解决了一族通用扰动，这些扰动一致地影响网络输出。我们的工作揭示了深度神经网络与人类感知之间的差异，即深度神经网络对人类认为重要的扰动可能不会影响其识别能力。

    

    在深度神经网络的背景下，我们揭示了一种无害扰动空间的存在，即扰动会使网络输出完全不变。无论这些扰动在应用于图像时的大小如何，只要它们位于无害扰动空间内，就不会对原始图像的网络输出产生影响。具体而言，对于网络中的任何线性层，输入维度$n$超过输出维度$m$的情况下，我们证明了连续无害扰动子空间的存在，其维度为$(n-m)$。受此启发，我们解决了一族一致影响网络输出的通用扰动，而不论它们的大小如何。基于这些理论发现，我们探索了无害扰动在保护隐私数据使用方面的应用。我们的工作揭示了深度神经网络与人类感知之间的差异，即被人类捕捉到的重要扰动可能不会影响深度神经网络的识别能力。

    In the context of deep neural networks, we expose the existence of a harmless perturbation space, where perturbations leave the network output entirely unaltered. Perturbations within this harmless perturbation space, regardless of their magnitude when applied to images, exhibit no impact on the network's outputs of the original images. Specifically, given any linear layer within the network, where the input dimension $n$ exceeds the output dimension $m$, we demonstrate the existence of a continuous harmless perturbation subspace with a dimension of $(n-m)$. Inspired by this, we solve for a family of general perturbations that consistently influence the network output, irrespective of their magnitudes. With these theoretical findings, we explore the application of harmless perturbations for privacy-preserving data usage. Our work reveals the difference between DNNs and human perception that the significant perturbations captured by humans may not affect the recognition of DNNs. As a re
    
[^388]: 风险敏感扩散：从带噪声样本学习潜在分布

    Risk-Sensitive Diffusion: Learning the Underlying Distribution from Noisy Samples

    [https://arxiv.org/abs/2402.02081](https://arxiv.org/abs/2402.02081)

    该论文提出了一种风险敏感的扩散模型，通过调整噪声样本的分布，减少误导并从中获取信息，有效地应对噪声样本的存在，并在合成和真实世界数据集上取得了良好的效果。

    

    我们发现，扩散模型在存在噪声样本时很脆弱，限制了它们在大量环境中的潜力，这些环境不像图像合成那样干净。受到我们对噪声样本与干净样本在扩散过程中分布差异的发现的启发，我们引入了风险敏感的SDE，这是一个由风险（即数据“脏乱度”）参数化的随机微分方程，用于调整噪声样本的分布，减少误导并从中获取信息。风险敏感的SDE的最优表达式取决于特定的噪声分布，我们导出了最小化高斯和一般非高斯扰动误导的参数化。我们在合成和真实世界数据集（如医学时间序列）上进行了大量实验，证明了我们的模型有效地。

    While achieving remarkable performances, we show that diffusion models are fragile to the presence of noisy samples, limiting their potential in the vast amount of settings where, unlike image synthesis, we are not blessed with clean data. Motivated by our finding that such fragility originates from the distribution gaps between noisy and clean samples along the diffusion process, we introduce risk-sensitive SDE, a stochastic differential equation that is parameterized by the risk (i.e., data "dirtiness") to adjust the distributions of noisy samples, reducing misguidance while benefiting from their contained information. The optimal expression for risk-sensitive SDE depends on the specific noise distribution, and we derive its parameterizations that minimize the misguidance of noisy samples for both Gaussian and general non-Gaussian perturbations. We conduct extensive experiments on both synthetic and real-world datasets (e.g., medical time series), showing that our model effectively r
    
[^389]: 使用无雅各布逆向传播训练隐式网络进行图像去模糊

    Training Implicit Networks for Image Deblurring using Jacobian-Free Backpropagation

    [https://arxiv.org/abs/2402.02065](https://arxiv.org/abs/2402.02065)

    本文介绍了使用无雅各布逆向传播方法来训练隐式网络进行图像去模糊的研究。实验结果表明，该方法在降低计算成本的同时，具有与最先进的前馈网络和现有的隐式网络相媲美的效果。

    

    最近在应用隐式网络解决成像反问题方面的努力取得了与前馈网络相比具有竞争力甚至优越的结果。这些隐式网络在反向传播过程中只需要固定内存，而不受层数影响。然而，它们并不容易训练。梯度计算需要耗费大量计算资源，因为需要通过固定点进行反向传播。特别是，这个过程需要解决一个由固定点迭代中的特征数决定大小的大型线性方程组。本文探索了最近提出的一种方法，称为无雅各布逆向传播（JFB），在图像去模糊问题中的应用。我们的结果表明，JFB在降低计算成本的同时，与优化方案、最先进的前馈网络和现有的隐式网络相媲美。

    Recent efforts in applying implicit networks to solve inverse problems in imaging have achieved competitive or even superior results when compared to feedforward networks. These implicit networks only require constant memory during backpropagation, regardless of the number of layers. However, they are not necessarily easy to train. Gradient calculations are computationally expensive because they require backpropagating through a fixed point. In particular, this process requires solving a large linear system whose size is determined by the number of features in the fixed point iteration. This paper explores a recently proposed method, Jacobian-free Backpropagation (JFB), a backpropagation scheme that circumvents such calculation, in the context of image deblurring problems. Our results show that JFB is comparable against fine-tuned optimization schemes, state-of-the-art (SOTA) feedforward networks, and existing implicit networks at a reduced computational cost.
    
[^390]: 打破LLM推理的顺序依赖：使用前瞻解码

    Break the Sequential Dependency of LLM Inference Using Lookahead Decoding

    [https://arxiv.org/abs/2402.02057](https://arxiv.org/abs/2402.02057)

    本文介绍了一种称为前瞻解码的精确、并行解码算法，通过交换每步操作数以减少总解码步骤的数量，加速了大型语言模型（LLM）的解码过程。它不需要辅助模型或数据存储，并且与并发内存高效的注意力机制兼容。实验证明，在代码补全任务中，前瞻解码可将自回归解码加速1.8倍，并且在多个GPU上实现强扩展性。

    

    大型语言模型（LLM）的自回归解码受到内存带宽限制，导致延迟较高，并且浪费了现代加速器的并行处理能力。现有的加速LLM解码的方法通常需要草稿模型（例如，推测解码），这样的模型不易获取且无法推广。在本文中，我们介绍了前瞻解码，一种精确的并行解码算法，可以加速LLM解码，而无需辅助模型或数据存储。它允许交换每步log（FLOPs）以减少总解码步骤的数量，在单个或多个现代加速器上更易于并行化，并且与并发内存高效的注意力机制（例如FlashAttention）兼容。我们的前瞻解码实现可以在MT-bench上加速自回归解码1.8倍，并在多个GPU上实现强扩展性，代码补全任务上加速4倍。我们的代码可在https://github.com/hao-ai-lab/LookaheadDecoding找到。

    Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step log(FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of Lookahead decoding can speed up autoregressive decoding by up to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code completion tasks. Our code is avialable at https://github.com/hao-ai-lab/LookaheadDecoding
    
[^391]: 方差对齐分数: 一种简单但难以超越的多模式对比学习数据选择方法

    Variance Alignment Score: A Simple But Tough-to-Beat Data Selection Method for Multimodal Contrastive Learning

    [https://arxiv.org/abs/2402.02055](https://arxiv.org/abs/2402.02055)

    提出了一种简单但有理论原则支持的度量方法——方差对齐分数(Variance Alignment Score，VAS)，用于解决数据选择问题。该方法通过最大化总的VAS来选择最具信息量的样本。

    

    近年来，数据选择已经成为大规模视觉-语言模型预训练的核心问题，特别是在嘈杂的网络抽样数据集上。一种广泛采用的策略是为每个样本分配质量分数，如CLIP相似度，并保留具有最高分数的数据对。然而，这些方法对数据分布是无知的，始终无法选择最具信息量的样本。为了解决这个问题，我们提出了一种简单但有理论原则支持的度量方法，名为方差对齐分数(Variance Alignment Score，VAS)，它的形式是$\langle \Sigma_{\text{test}}, \Sigma_i\rangle$。这里，$\Sigma_{\text{test}}$表示我们希望对齐的目标（交叉）协方差矩阵，可能基于先验知识，而$\Sigma_i$表示第$i$个样本的单模态或多模态表示的张量积。我们进一步设计了一种新的数据选择方法，最大化总的VAS。我们在简化的设置下进行了理论分析来证明这个方法的理论优势。

    In recent years, data selection has emerged as a core issue for large-scale visual-language model pretraining, especially on noisy web-curated datasets. One widely adopted strategy assigns quality scores such as CLIP similarity for each sample and retains the data pairs with the highest scores. However, these approaches are agnostic of data distribution and always fail to select the most informative samples. To solve this problem, we propose a simple yet theoretically principled metric named Variance Alignment Score (VAS), which has the form $\langle \Sigma_{\text{test}}, \Sigma_i\rangle$. Here, $\Sigma_{\text{test}}$ represents the target (cross-)covariance matrix we aim to align, potentially based on prior knowledge, while $\Sigma_i$ denotes the tensor product of single or multi-modal representations for the $i$-th sample. We further design a new data selection method that maximizes the total VAS. We provide theoretical analysis in a simplified setting to demonstrate the theoretical 
    
[^392]: 图上的神经缩放定律

    Neural Scaling Laws on Graphs

    [https://arxiv.org/abs/2402.02054](https://arxiv.org/abs/2402.02054)

    本论文在图上深入研究了神经缩放定律，从模型和数据两个角度进行了探索。对于模型缩放，发现了缩放定律崩溃和过拟合之间的关系，以及深度图模型的模型深度对缩放行为的影响。对于数据缩放，提出了图数量不适合作为衡量缩放定律中图数据量的指标。

    

    深度图模型（例如图神经网络和图变换器）已成为利用各种类型图的知识的重要技术。然而，深度图模型的缩放特性尚未得到系统研究，对通过扩大模型和数据集大小来实现大型图模型的可行性产生了疑问。在这项工作中，我们从模型和数据的角度深入探索了图上的神经缩放定律。我们首先验证了这些定律在图上的有效性，并建立了描述缩放行为的公式。对于模型缩放，我们研究了缩放定律崩溃现象，并确定了过拟合可能是原因。此外，我们揭示了深度图模型的模型深度可以影响模型缩放行为，这与其他领域（如计算机视觉和自然语言处理）的观察结果不同。对于数据缩放，我们建议图数量无法有效衡量图数据量的缩放定律，因为...

    Deep graph models (e.g., graph neural networks and graph transformers) have become important techniques for leveraging knowledge across various types of graphs. Yet, the scaling properties of deep graph models have not been systematically investigated, casting doubt on the feasibility of achieving large graph models through enlarging the model and dataset sizes. In this work, we delve into neural scaling laws on graphs from both model and data perspectives. We first verify the validity of such laws on graphs, establishing formulations to describe the scaling behaviors. For model scaling, we investigate the phenomenon of scaling law collapse and identify overfitting as the potential reason. Moreover, we reveal that the model depth of deep graph models can impact the model scaling behaviors, which differ from observations in other domains such as CV and NLP. For data scaling, we suggest that the number of graphs can not effectively metric the graph data volume in scaling law since the si
    
[^393]: 使用孔雀交配概念进行入侵检测系统中的特征选择

    Feature Selection using the concept of Peafowl Mating in IDS

    [https://arxiv.org/abs/2402.02052](https://arxiv.org/abs/2402.02052)

    本研究在入侵检测系统中提出了一种结合孔雀交配概念的特征选择算法，以实现高效的云环境安全保证。

    

    云计算作为一种基于互联网的服务，依赖于共享计算资源，具有较高的适用性。云计算提供基础设施、平台和软件服务。该技术的流行是由于其出色的性能、高水平的计算能力、低成本的服务、可扩展性、可用性和灵活性。在云环境中，数据的可获取性和开放性使其容易受到网络攻击的威胁。为了检测攻击并确保信息安全，使用入侵检测系统。本文提出了一个一致而高效的入侵检测系统，以实现在云环境中更高的安全性保证水平。本文将孔雀的交配行为纳入到一个优化算法中，该算法进而用作特征选择算法。该算法用于减小云数据的巨大大小，使入侵检测系统能够正常工作。

    Cloud computing has high applicability as an Internet based service that relies on sharing computing resources. Cloud computing provides services that are Infrastructure based, Platform based and Software based. The popularity of this technology is due to its superb performance, high level of computing ability, low cost of services, scalability, availability and flexibility. The obtainability and openness of data in cloud environment make it vulnerable to the world of cyber-attacks. To detect the attacks Intrusion Detection System is used, that can identify the attacks and ensure information security. Such a coherent and proficient Intrusion Detection System is proposed in this paper to achieve higher certainty levels regarding safety in cloud environment. In this paper, the mating behavior of peafowl is incorporated into an optimization algorithm which in turn is used as a feature selection algorithm. The algorithm is used to reduce the huge size of cloud data so that the IDS can work
    
[^394]: 基于功能链接神经网络的非线性子空间聚类

    Nonlinear subspace clustering by functional link neural networks

    [https://arxiv.org/abs/2402.02051](https://arxiv.org/abs/2402.02051)

    本研究提出了一种基于功能链接神经网络的非线性子空间聚类方法，通过将数据样本转化为非线性域并利用学习机制构建自表示矩阵，同时引入局部相似性正则化以增强分组效应，改善聚类结果质量，并且在保证高计算效率的前提下获得良好的聚类准确性。

    

    研究表明，基于前馈神经网络的非线性子空间聚类方法比一些先进的子空间聚类算法提供更好的聚类准确性。然而，这种方法在效果和计算成本之间需要进行平衡。在本研究中，我们采用功能链接神经网络将数据样本转化为非线性域。随后，我们通过学习机制利用映射样本构建自表示矩阵。由于功能链接神经网络是一个单层神经网络，我们提出的方法在保证良好的聚类性能的同时具有较高的计算效率。通过引入局部相似性正则化以增强分组效应，我们提出的方法进一步改善了聚类结果的质量。此外，我们还引入了一个凸组合子空间聚类方案，它结合了线性子空间聚类方法。

    Nonlinear subspace clustering based on a feed-forward neural network has been demonstrated to provide better clustering accuracy than some advanced subspace clustering algorithms. While this approach demonstrates impressive outcomes, it involves a balance between effectiveness and computational cost. In this study, we employ a functional link neural network to transform data samples into a nonlinear domain. Subsequently, we acquire a self-representation matrix through a learning mechanism that builds upon the mapped samples. As the functional link neural network is a single-layer neural network, our proposed method achieves high computational efficiency while ensuring desirable clustering performance. By incorporating the local similarity regularization to enhance the grouping effect, our proposed method further improves the quality of the clustering results. Additionally, we introduce a convex combination subspace clustering scheme, which combining a linear subspace clustering method 
    
[^395]: 机器学习生成代码的质量和信任

    Quality and Trust in LLM-generated Code

    [https://arxiv.org/abs/2402.02047](https://arxiv.org/abs/2402.02047)

    本论文研究了机器学习生成代码的质量和信任问题，提出了校准的重要性，并探讨了如何确定模型生成代码的正确性。

    

    机器学习模型广泛应用，但常常会出错。用户需要可靠的指示，以确定给定模型的输出是否可信，从而可以做出理性决策是否使用该输出。例如，可以将输出与置信度相关联；如果置信度与正确性的可能性强相关，则称该模型为良好校准。在这种情况下，高置信度的输出可以安全接受，低置信度的输出可以拒绝。校准迄今主要在非生成性（例如分类）环境中进行研究，特别是在软件工程领域。然而，生成代码很容易出错：开发人员需要知道何时直接使用、经过仔细审查后使用或丢弃模型生成的代码，因此在生成环境中，校准非常重要。然而，生成代码的正确性概念并不简单，因此校准也是如此。

    Machine learning models are widely used but can also often be wrong. Users would benefit from a reliable indication of whether a given output from a given model should be trusted, so a rational decision can be made whether to use the output or not. For example, outputs can be associated with a confidence measure; if this confidence measure is strongly associated with likelihood of correctness, then the model is said to be well-calibrated. In this case, for example, high-confidence outputs could be safely accepted, and low-confidence outputs rejected.   Calibration has so far been studied in non-generative (e.g., classification) settings, especially in Software Engineering. However, generated code can quite often be wrong: Developers need to know when they should e.g., directly use, use after careful review, or discard model-generated code; thus Calibration is vital in generative settings. However, the notion of correctness of generated code is non-trivial, and thus so is Calibration. I
    
[^396]: 用于流式向量搜索的局部自适应量化

    Locally-Adaptive Quantization for Streaming Vector Search

    [https://arxiv.org/abs/2402.02044](https://arxiv.org/abs/2402.02044)

    本文研究了局部自适应量化在流式相似度搜索中的应用，引入了Turbo LVQ和multi-means LVQ两种改进方法，分别提升了搜索性能28%和27%。实验证明，LVQ及其新变体使得向量搜索变得极快。

    

    在大量向量集合中找到与给定查询最相似的向量嵌入一直是无数实际应用的关键组成部分。最近引入的检索增强生成就是其中最突出的例子之一。对于许多这些应用，数据库通过插入新数据和删除过时数据而随时间演变。在这些情况下，检索问题被称为流式相似度搜索。虽然局部自适应向量量化（LVQ）作为一种高效的向量压缩方法，在非演化数据库中具有最先进的搜索性能，但其在流式场景中的实用性尚未确定。在这项工作中，我们研究了LVQ在流式相似度搜索中的应用。为了支持我们的评估，我们引入了两种LVQ的改进：Turbo LVQ和multi-means LVQ，分别提升了其搜索性能高达28%和27%。我们的研究表明，LVQ及其新变体使得向量搜索变得极快。

    Retrieving the most similar vector embeddings to a given query among a massive collection of vectors has long been a key component of countless real-world applications. The recently introduced Retrieval-Augmented Generation is one of the most prominent examples. For many of these applications, the database evolves over time by inserting new data and removing outdated data. In these cases, the retrieval problem is known as streaming similarity search. While Locally-Adaptive Vector Quantization (LVQ), a highly efficient vector compression method, yields state-of-the-art search performance for non-evolving databases, its usefulness in the streaming setting has not been yet established. In this work, we study LVQ in streaming similarity search. In support of our evaluation, we introduce two improvements of LVQ: Turbo LVQ and multi-means LVQ that boost its search performance by up to 28% and 27%, respectively. Our studies show that LVQ and its new variants enable blazing fast vector search,
    
[^397]: 一种用于智能和选择性传感器数据传输的插件式微型人工智能模块

    A Plug-in Tiny AI Module for Intelligent and Selective Sensor Data Transmission

    [https://arxiv.org/abs/2402.02043](https://arxiv.org/abs/2402.02043)

    提出了一种插件式微型AI模块，可以将智能和选择性传感器数据传输集成到物联网应用中。通过放置高效的机器学习模型靠近传感器，实现对数据传输的智能控制，筛选有价值的数据，并通过调节传输频率丢弃无关信息。通过量化和优化近传感器模型，实现实时传感器控制。定制训练过程和利用时间信息的“懒惰”传感器停用策略提升框架性能。该方法独立于其他物联网框架，可与之共存。

    

    物联网应用中利用机器学习分析传感器生成的数据，但当前感知系统缺乏有针对性的智能，导致数据产生量巨大、计算和通信成本增加。为了解决这个挑战，我们提出了一种新颖的感知模块，通过将高效的机器学习模型放置在传感器附近，为感知框架提供智能数据传输能力。该模型及时反馈给感知系统，只传输有价值的数据，通过调节数据传输的频率丢弃无关信息。近传感器模型经过量化和优化，实现实时传感器控制。为了提升框架性能，定制了训练过程，并引入利用时间信息的“懒惰”传感器停用策略。该方法与其他物联网框架正交，可以保证共存。

    Applications in the Internet of Things (IoT) utilize machine learning to analyze sensor-generated data. However, a major challenge lies in the lack of targeted intelligence in current sensing systems, leading to vast data generation and increased computational and communication costs. To address this challenge, we propose a novel sensing module to equip sensing frameworks with intelligent data transmission capabilities by integrating a highly efficient machine learning model placed near the sensor. This model provides prompt feedback for the sensing system to transmit only valuable data while discarding irrelevant information by regulating the frequency of data transmission. The near-sensor model is quantized and optimized for real-time sensor control. To enhance the framework's performance, the training process is customized and a "lazy" sensor deactivation strategy utilizing temporal information is introduced. The suggested method is orthogonal to other IoT frameworks and can be cons
    
[^398]: 学习通过原始-对偶策略梯度算法对无限时域平均回报受限MDP进行参数化通用策略

    Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm

    [https://arxiv.org/abs/2402.02042](https://arxiv.org/abs/2402.02042)

    该论文研究了无限时域平均回报受限MDPs的参数化通用策略，并提出了一种基于原始-对偶策略梯度算法，可在保证低遗憾的情况下管理约束条件，达到全局最优策略。算法的分析表明，其目标遗憾和约束违反均为 $\tilde{\mathcal{O}}({T}^{3/4})$。

    

    本文探索了无限时域平均回报受限马尔科夫决策过程（CMDP）的领域。据我们所知，这项工作是首次研究具有通用策略参数化的平均回报CMDP的遗憾和约束违规分析。为了解决这个挑战，我们提出了一种基于原始对偶的策略梯度算法，能够灵活地管理约束条件，并确保低遗憾保证以实现全局最优策略。特别地，我们证明了我们提出的算法在目标遗憾和约束违反上具有 $\tilde{\mathcal{O}}({T}^{3/4})$ 的界限。

    This paper explores the realm of infinite horizon average reward Constrained Markov Decision Processes (CMDP). To the best of our knowledge, this work is the first to delve into the regret and constraint violation analysis of average reward CMDPs with a general policy parametrization. To address this challenge, we propose a primal dual based policy gradient algorithm that adeptly manages the constraints while ensuring a low regret guarantee toward achieving a global optimal policy. In particular, we demonstrate that our proposed algorithm achieves $\tilde{\mathcal{O}}({T}^{3/4})$ objective regret and $\tilde{\mathcal{O}}({T}^{3/4})$ constraint violation bounds.
    
[^399]: 用于神经密度比估计的$\alpha$-散度损失函数

    $\alpha$-Divergence Loss Function for Neural Density Ratio Estimation

    [https://arxiv.org/abs/2402.02041](https://arxiv.org/abs/2402.02041)

    本文提出了一种应用于神经密度比估计的$\alpha$-散度损失函数($\alpha$-Div)，通过简洁实现和稳定优化解决了现有方法中存在的优化问题。实验证明了这种损失函数的稳定性，并提出了对DRE任务的估计准确性的研究，同时给出了样本要求的解决方案。

    

    最近，神经网络在机器学习中的基础技术密度比估计(DRE)方面取得了最先进的结果。然而，现有方法因DRE的损失函数而出现了优化问题：KL散度需要大样本，训练损失梯度消失，损失函数梯度有偏。因此，本文提出了一种提供简洁实现和稳定优化的$\alpha$-散度损失函数($\alpha$-Div)。此外，还给出了对所提出的损失函数的技术验证。实验证明了所提出的损失函数的稳定性，并研究了DRE任务的估计准确性。此外，本研究还提出了使用所提出的损失函数进行DRE的样本要求，以$L_1$误差的上界联系起来，该上界将高维度DRE任务中的维度诅咒作为一个共同问题。

    Recently, neural networks have produced state-of-the-art results for density-ratio estimation (DRE), a fundamental technique in machine learning. However, existing methods bear optimization issues that arise from the loss functions of DRE: a large sample requirement of Kullback--Leibler (KL)-divergence, vanishing of train loss gradients, and biased gradients of the loss functions. Thus, an $\alpha$-divergence loss function ($\alpha$-Div) that offers concise implementation and stable optimization is proposed in this paper. Furthermore, technical justifications for the proposed loss function are presented. The stability of the proposed loss function is empirically demonstrated and the estimation accuracy of DRE tasks is investigated. Additionally, this study presents a sample requirement for DRE using the proposed loss function in terms of the upper bound of $L_1$ error, which connects a curse of dimensionality as a common problem in high-dimensional DRE tasks.
    
[^400]: 用分布式代理解释图神经网络

    Interpreting Graph Neural Networks with In-Distributed Proxies

    [https://arxiv.org/abs/2402.02036](https://arxiv.org/abs/2402.02036)

    该论文提出了一种翻译图神经网络中可解释子图的代理图的新方法，解决了训练数据分布与可解释子图集之间的分布偏移问题。

    

    图神经网络（GNN）已成为图数据处理的重要组成部分，在关键领域广泛应用。在高风险应用中部署GNN的不断增长需求需要用户在决策过程中能够解释其原因。解释GNN的流行范式是通过比较它们与原始图的标签来识别可解释的子图。由于训练集中原始图与可解释子图集之间存在显著的分布偏移，导致无法准确预测子图的标签，这是一个具有挑战性的任务。为了解决这个问题，在本文中，我们提出了一种新的方法，用于生成与训练数据分布相符的可解释子图的代理图。我们引入了一个使用图生成器生成代理图的参数化方法。基于信息论设计了一个新的训练目标，以确保代理图不仅遵循训练数据的分布，而且便于解释。

    Graph Neural Networks (GNNs) have become a building block in graph data processing, with wide applications in critical domains. The growing needs to deploy GNNs in high-stakes applications necessitate explainability for users in the decision-making processes. A popular paradigm for the explainability of GNNs is to identify explainable subgraphs by comparing their labels with the ones of original graphs. This task is challenging due to the substantial distributional shift from the original graphs in the training set to the set of explainable subgraphs, which prevents accurate prediction of labels with the subgraphs. To address it, in this paper, we propose a novel method that generates proxy graphs for explainable subgraphs that are in the distribution of training data. We introduce a parametric method that employs graph generators to produce proxy graphs. A new training objective based on information theory is designed to ensure that proxy graphs not only adhere to the distribution of 
    
[^401]: 深度神经网络中针对后门攻击的通用后训练反向工程防御方法

    Universal Post-Training Reverse-Engineering Defense Against Backdoors in Deep Neural Networks

    [https://arxiv.org/abs/2402.02034](https://arxiv.org/abs/2402.02034)

    本文提出了一种针对深度神经网络中后门攻击的通用后训练反向工程防御方法，通过依赖内部特征图来检测和反向工程后门，并识别其目标类别，具有广泛适用性和低计算开销。

    

    针对深度神经网络分类器的后门攻击，提出了各种防御方法。通用方法旨在可靠地检测和/或减轻后门攻击，而反向工程方法通常明确假设其中一种。本文提出了一种新的检测器，它依赖于被防守的DNN的内部特征图来检测和反向工程后门，并识别其目标类别；它可以在后训练时操作（无需访问训练数据集）；对于不同的嵌入机制（即通用的）非常有效；并且具有低计算开销，因此可扩展。我们对基准CIFAR-10图像分类器的不同攻击进行了检测方法的评估。

    A variety of defenses have been proposed against backdoors attacks on deep neural network (DNN) classifiers. Universal methods seek to reliably detect and/or mitigate backdoors irrespective of the incorporation mechanism used by the attacker, while reverse-engineering methods often explicitly assume one. In this paper, we describe a new detector that: relies on internal feature map of the defended DNN to detect and reverse-engineer the backdoor and identify its target class; can operate post-training (without access to the training dataset); is highly effective for various incorporation mechanisms (i.e., is universal); and which has low computational overhead and so is scalable. Our detection approach is evaluated for different attacks on a benchmark CIFAR-10 image classifier.
    
[^402]: RobustTSF: 关于具有异常值的鲁棒时间序列预测的理论与设计

    RobustTSF: Towards Theory and Design of Robust Time Series Forecasting with Anomalies

    [https://arxiv.org/abs/2402.02032](https://arxiv.org/abs/2402.02032)

    本文研究了具有异常值的时间序列预测的理论与设计，提出了一种简单高效的算法来学习鲁棒的预测模型，并通过广泛的实验验证了方法的高鲁棒性和优越性能。

    

    时间序列预测是许多实际应用中重要而前沿的任务。然而，大多数时间序列预测技术假设训练数据没有异常值，这种假设在实践中是不现实的，因为收集的时间序列数据可能被污染。如果直接使用带有异常值的时间序列进行训练，预测模型将表现较差。因此，从受污染数据中自动学习鲁棒的预测模型是至关重要的。本文首先对三种异常进行了统计定义，然后在这些异常存在时从理论和实验上分析了损失鲁棒性和样本鲁棒性。基于我们的分析，我们提出了一种简单高效的算法来学习一个鲁棒的预测模型。广泛的实验表明我们的方法具有很高的鲁棒性，并且优于所有现有方法。代码可在https://github.com/haochenglouis/RobustTSF获取。

    Time series forecasting is an important and forefront task in many real-world applications. However, most of time series forecasting techniques assume that the training data is clean without anomalies. This assumption is unrealistic since the collected time series data can be contaminated in practice. The forecasting model will be inferior if it is directly trained by time series with anomalies. Thus it is essential to develop methods to automatically learn a robust forecasting model from the contaminated data. In this paper, we first statistically define three types of anomalies, then theoretically and experimentally analyze the loss robustness and sample robustness when these anomalies exist. Based on our analyses, we propose a simple and efficient algorithm to learn a robust forecasting model. Extensive experiments show that our method is highly robust and outperforms all existing approaches. The code is available at https://github.com/haochenglouis/RobustTSF.
    
[^403]: 多级逼真的物理约束神经网络用于动力系统

    Multi-fidelity physics constrained neural networks for dynamical systems

    [https://arxiv.org/abs/2402.02031](https://arxiv.org/abs/2402.02031)

    本文提出了一种多级逼真度的物理约束神经网络，通过定制的多逼真度自编码器将具有不同逼真度水平的数据融合到一个统一的潜在空间中。

    

    物理约束神经网络通常被用来增强预测的稳健性，相比纯数据驱动模型而言，通过在模型训练过程中包含物理约束损失来实现。然而，物理约束神经网络面临的主要挑战之一是训练复杂性，特别是对于高维系统而言。事实上，传统的物理约束模型依赖于单一逼真度数据，需要在高维场中评估物理约束，这引入了计算困难。此外，由于神经网络的固定输入大小，使用多逼真度的训练数据也会变得繁琐。在本文中，我们提出了多尺度物理约束神经网络（MSPCNN），通过定制的多逼真度自编码器，将具有不同逼真度水平的数据融合到一个统一的潜在空间中，从而提供了一种新的方法论。

    Physics-constrained neural networks are commonly employed to enhance prediction robustness compared to purely data-driven models, achieved through the inclusion of physical constraint losses during the model training process. However, one of the major challenges of physics-constrained neural networks consists of the training complexity especially for high-dimensional systems. In fact, conventional physics-constrained models rely on singular-fidelity data necessitating the assessment of physical constraints within high-dimensional fields, which introduces computational difficulties. Furthermore, due to the fixed input size of the neural networks, employing multi-fidelity training data can also be cumbersome. In this paper, we propose the Multi-Scale Physics-Constrained Neural Network (MSPCNN), which offers a novel methodology for incorporating data with different levels of fidelity into a unified latent space through a customised multi-fidelity autoencoder. Additionally, multiple decode
    
[^404]: ScribFormer: Transformer使得基于涂鸦的医学图像分割的CNN工作更好

    ScribFormer: Transformer Makes CNN Work Better for Scribble-based Medical Image Segmentation

    [https://arxiv.org/abs/2402.02029](https://arxiv.org/abs/2402.02029)

    ScribFormer是一种新的CNN-Transformer混合解决方案，用于涂鸦监督的医学图像分割。它通过融合CNN学习的局部特征和Transformer获得的全局表示，有效克服了现有方法在涂鸦注释中学习全局形状信息的局限性。

    

    最近的涂鸦监督分割方法通常采用具有编码器-解码器架构的CNN框架。尽管这个框架有多个好处，但是由于卷积层只能捕捉具有局部感受野的小范围特征依赖关系，它很难从涂鸦注释提供的有限信息中学习全局形状信息。为了解决这个问题，本文提出了一种新的CNN-Transformer混合解决方案，用于涂鸦监督的医学图像分割，称为ScribFormer。所提出的ScribFormer模型具有三个分支结构，即CNN分支，Transformer分支和注意力引导的类激活图（ACAM）分支的混合。具体而言，CNN分支与Transformer分支合作，将从CNN学习的局部特征与从Transformer获得的全局表示相融合，可以有效克服现有涂鸦监督分割方法的局限性。

    Most recent scribble-supervised segmentation methods commonly adopt a CNN framework with an encoder-decoder architecture. Despite its multiple benefits, this framework generally can only capture small-range feature dependency for the convolutional layer with the local receptive field, which makes it difficult to learn global shape information from the limited information provided by scribble annotations. To address this issue, this paper proposes a new CNN-Transformer hybrid solution for scribble-supervised medical image segmentation called ScribFormer. The proposed ScribFormer model has a triple-branch structure, i.e., the hybrid of a CNN branch, a Transformer branch, and an attention-guided class activation map (ACAM) branch. Specifically, the CNN branch collaborates with the Transformer branch to fuse the local features learned from CNN with the global representations obtained from Transformer, which can effectively overcome limitations of existing scribble-supervised segmentation m
    
[^405]: 时间序列的不可学习样本

    Unlearnable Examples For Time Series

    [https://arxiv.org/abs/2402.02028](https://arxiv.org/abs/2402.02028)

    本研究提出了一种新的UE生成方法，用于保护时间序列数据免受深度学习模型的未授权训练。通过选择性应用最小化误差噪声，我们使特定时间序列段对DNN模型不可学习，同时对人类观察者不可察觉。

    

    不可学习的样本（UEs）是指经过修改使得深度神经网络（DNNs）无法学习的训练样本。这些样本通常是通过添加最小化误差的噪声生成的，这些噪声可以欺骗DNN模型认为从数据中没有任何（无误差）可学习的内容。UE的概念被提出作为对个人数据的未经授权的数据利用的对策。虽然UE在图像上已被广泛研究，但如何针对时间序列数据创建有效的UE仍不清楚。在这项工作中，我们引入了第一个UE生成方法，以保护时间序列数据免受深度学习模型的未授权训练。为此，我们提出了一种新形式的最小化误差噪声，可以\emph{选择性地}应用于时间序列的特定段落，使它们对DNN模型不可学习，同时对人类观察者来说不可察觉。通过对各种时间序列数据集进行广泛实验，我们证明了所提出的UE生成方法是有效的。

    Unlearnable examples (UEs) refer to training samples modified to be unlearnable to Deep Neural Networks (DNNs). These examples are usually generated by adding error-minimizing noises that can fool a DNN model into believing that there is nothing (no error) to learn from the data. The concept of UE has been proposed as a countermeasure against unauthorized data exploitation on personal data. While UE has been extensively studied on images, it is unclear how to craft effective UEs for time series data. In this work, we introduce the first UE generation method to protect time series data from unauthorized training by deep learning models. To this end, we propose a new form of error-minimizing noise that can be \emph{selectively} applied to specific segments of time series, rendering them unlearnable to DNN models while remaining imperceptible to human observers. Through extensive experiments on a wide range of time series datasets, we demonstrate that the proposed UE generation method is 
    
[^406]: 安全强化学习中的约束形式综述

    A Survey of Constraint Formulations in Safe Reinforcement Learning

    [https://arxiv.org/abs/2402.02025](https://arxiv.org/abs/2402.02025)

    本文综述了安全强化学习中的约束形式，包括对每种形式特别设计的算法。同时，揭示了常见问题形式之间的数学相互关系。

    

    在将强化学习（RL）应用于现实世界问题时，确保安全性至关重要。因此，安全RL成为一种从实验数据中安全优化代理策略的基本而强大的范例。基于约束准则的安全RL方法被广泛采用，它解决了在安全约束下最大化预期累积奖励的问题。虽然近年来在RL中实现安全性的尝试激增，但由于约束表示的多样性和对它们之间关系的讨论很少，对该领域的系统性了解仍然困难。为了解决这一知识差距，我们提供了对代表性约束形式的全面回顾，以及针对每种形式特别设计的算法的精选。此外，我们揭示了揭示常见问题形式之间的数学相互关系的理论基础。最后，我们讨论了当前研究的一些挑战和未来方向。

    Ensuring safety is critical when applying reinforcement learning (RL) to real-world problems. Consequently, safe RL emerges as a fundamental and powerful paradigm for safely optimizing an agent's policy from experimental data. A popular safe RL approach is based on a constrained criterion, which solves the problem of maximizing expected cumulative reward under safety constraints. Though there has been recently a surge of such attempts to achieve safety in RL, a systematic understanding of the field is difficult due to 1) the diversity of constraint representations and 2) little discussion of their interrelations. To address this knowledge gap, we provide a comprehensive review of representative constraint formulations, along with a curated selection of algorithms specifically designed for each formulation. Furthermore, we elucidate the theoretical underpinnings that reveal the mathematical mutual relations among common problem formulations. We conclude with a discussion of the current 
    
[^407]: 自监督对比预测

    Self-Supervised Contrastive Forecasting

    [https://arxiv.org/abs/2402.02023](https://arxiv.org/abs/2402.02023)

    该论文介绍了一种通过采用对比学习和增强的分解架构，并结合全局自相关性的自监督方法来解决长期预测中的挑战。实验证明，该方法在九个长期基准上的多个实验中胜过了14个基线模型。

    

    长期预测由于处理长序列的时间和内存复杂性而面临独特挑战。现有方法依赖于滑动窗口来处理长序列，难以有效捕捉部分在短窗口内被捕捉到的长期变化（即外窗口变化）。本文介绍了一种新颖的方法，通过采用对比学习和增强的分解架构，专门设计用于聚焦长期变化，从而克服了这个限制。为此，我们的对比损失将整个时间序列中的全局自相关性纳入考虑，以自监督方式构建正负对。当与我们的分解网络结合使用时，我们的对比学习显著提高了长期预测性能。广泛的实验表明，我们的方法在九个长期基准上的多个实验中胜过了14个基线模型。

    Long-term forecasting presents unique challenges due to the time and memory complexity of handling long sequences. Existing methods, which rely on sliding windows to process long sequences, struggle to effectively capture long-term variations that are partially caught within the short window (i.e., outer-window variations). In this paper, we introduce a novel approach that overcomes this limitation by employing contrastive learning and enhanced decomposition architecture, specifically designed to focus on long-term variations. To this end, our contrastive loss incorporates global autocorrelation held in the whole time series, which facilitates the construction of positive and negative pairs in a self-supervised manner. When combined with our decomposition networks, our contrastive learning significantly improves long-term forecasting performance. Extensive experiments demonstrate that our approach outperforms 14 baseline models in multiple experiments over nine long-term benchmarks, es
    
[^408]: ECG诊断中的迁移学习：有效吗？

    Transfer Learning in ECG Diagnosis: Is It Effective?

    [https://arxiv.org/abs/2402.02021](https://arxiv.org/abs/2402.02021)

    本研究首次对心电图诊断中的迁移学习进行了广泛的经验研究，发现微调对于小型数据集是较好的选择，当数据集足够大时，从头开始训练可以达到可比性能，但需要更长的训练时间。同时，迁移学习与卷积神经网络具有更好的兼容性。

    

    在真实世界的场景中，深度学习在心电图诊断中的应用往往受到大规模、标记良好的数据集的稀缺性的限制，因此使用迁移学习来利用从更大的数据集中学到的特征。然而，关于迁移学习始终优于从头开始训练的普遍假设从未被系统验证过。在本研究中，我们通过对多标签心电图分类中进行微调与从头开始训练的性能进行比较，涵盖了各种心电图数据集和深度神经网络，进行了第一次广泛的经验性研究来验证迁移学习的有效性。我们证实，对于小型的下游数据集来说，微调是更好的选择；然而，当数据集足够大时，从头开始训练可以达到可比性能，尽管需要更长的训练时间来迎头赶上。此外，我们发现，迁移学习与卷积神经网络更好的兼容性。

    The adoption of deep learning in ECG diagnosis is often hindered by the scarcity of large, well-labeled datasets in real-world scenarios, leading to the use of transfer learning to leverage features learned from larger datasets. Yet the prevailing assumption that transfer learning consistently outperforms training from scratch has never been systematically validated. In this study, we conduct the first extensive empirical study on the effectiveness of transfer learning in multi-label ECG classification, by investigating comparing the fine-tuning performance with that of training from scratch, covering a variety of ECG datasets and deep neural networks. We confirm that fine-tuning is the preferable choice for small downstream datasets; however, when the dataset is sufficiently large, training from scratch can achieve comparable performance, albeit requiring a longer training time to catch up. Furthermore, we find that transfer learning exhibits better compatibility with convolutional ne
    
[^409]: 位置论文：超级计算研究和LLMs的现状与挑战

    Position Paper: The Landscape and Challenges of HPC Research and LLMs

    [https://arxiv.org/abs/2402.02018](https://arxiv.org/abs/2402.02018)

    运用语言模型技术于高性能计算任务中具有巨大潜力

    

    最近，语言模型（LMs），特别是大规模的语言模型（LLMs），已经彻底改变了深度学习领域。编码器-解码器模型和基于提示的技术均展现出在自然语言处理和基于代码的任务中巨大的潜力。在过去几年中，许多研究实验室和机构在高性能计算方面投入了大量资源，达到或突破了超级计算的性能水平。本文提出，将这些基于语言模型的技术调整和应用于高性能计算任务中将会非常有益。本研究阐述了我们上述观点的理由，并强调了现有想法在HPC任务中的改进和应用。

    Recently, language models (LMs), especially large language models (LLMs), have revolutionized the field of deep learning. Both encoder-decoder models and prompt-based techniques have shown immense potential for natural language processing and code-based tasks. Over the past several years, many research labs and institutions have invested heavily in high-performance computing, approaching or breaching exascale performance levels. In this paper, we posit that adapting and utilizing such language model-based techniques for tasks in high-performance computing (HPC) would be very beneficial. This study presents our reasoning behind the aforementioned position and highlights how existing ideas can be improved and adapted for HPC tasks.
    
[^410]: 无需奖励的条件监督学习在离线强化学习中的价值增强

    Value-Aided Conditional Supervised Learning for Offline RL

    [https://arxiv.org/abs/2402.02017](https://arxiv.org/abs/2402.02017)

    该论文提出了一种称为价值增强的条件监督学习方法，通过将RCSL的稳定性与基于价值的方法的连接能力相结合，动态地根据轨迹回报将价值帮助注入损失函数中。实验证明，该方法不仅优于现有方法，而且在各种离线强化学习任务中实现了最高的轨迹回报，推动了离线强化学习的发展。

    

    离线强化学习通过基于回报的条件监督学习（RCSL）和基于价值的方法取得了显著进展，但每种方法都存在一些实际挑战。为了解决这些挑战，我们提出了价值增强的条件监督学习（VCS）方法，该方法将RCSL的稳定性与基于价值的方法的连接能力有效地结合在一起。通过神经切线核分析，VCS可以动态地根据轨迹回报将价值帮助注入RCSL的损失函数中，以区分价值函数可能无法实现稳定连接的实例。我们的实证研究表明，VCS不仅显著优于RCSL和基于价值的方法，而且在各种离线强化学习基准测试中始终实现了或经常超过最高的轨迹回报。这一突破为离线强化学习开辟了新的道路，推动了可实现的极限，并促进了进一步的创新。

    Offline reinforcement learning (RL) has seen notable advancements through return-conditioned supervised learning (RCSL) and value-based methods, yet each approach comes with its own set of practical challenges. Addressing these, we propose Value-Aided Conditional Supervised Learning (VCS), a method that effectively synergizes the stability of RCSL with the stitching ability of value-based methods. Based on the Neural Tangent Kernel analysis to discern instances where value function may not lead to stable stitching, VCS injects the value aid into the RCSL's loss function dynamically according to the trajectory return. Our empirical studies reveal that VCS not only significantly outperforms both RCSL and value-based methods but also consistently achieves, or often surpasses, the highest trajectory returns across diverse offline RL benchmarks. This breakthrough in VCS paves new paths in offline RL, pushing the limits of what can be achieved and fostering further innovations.
    
[^411]: GenFormer: 一种基于深度学习的生成多元随机过程的方法

    GenFormer: A Deep-Learning-Based Approach for Generating Multivariate Stochastic Processes

    [https://arxiv.org/abs/2402.02010](https://arxiv.org/abs/2402.02010)

    GenFormer是一种基于深度学习的方法，用于生成多元随机过程。它能保留目标统计特性，包括边际分布，并能在具有挑战性的应用中近似捕捉到其他期望的统计特性。应用于风速数据模拟的实验中，GenFormer模型用于计算风险管理的超越概率。

    

    随机生成器对于生成保持目标统计特性的合成实现非常重要。我们提出了GenFormer，一个用于时空多元随机过程的随机生成器。它采用基于Transformer的深度学习模型构建，学习了一个将马尔可夫状态序列映射到时间序列值的映射关系。GenFormer模型生成的合成数据保留了目标边际分布，并在涉及大量空间位置和长时间模拟的挑战性应用中近似捕捉到其他期望的统计特性。我们将GenFormer模型应用于在佛罗里达州的各个站点模拟合成风速数据，以计算风险管理的超越概率。

    Stochastic generators are essential to produce synthetic realizations that preserve target statistical properties. We propose GenFormer, a stochastic generator for spatio-temporal multivariate stochastic processes. It is constructed using a Transformer-based deep learning model that learns a mapping between a Markov state sequence and time series values. The synthetic data generated by the GenFormer model preserves the target marginal distributions and approximately captures other desired statistical properties even in challenging applications involving a large number of spatial locations and a long simulation horizon. The GenFormer model is applied to simulate synthetic wind speed data at various stations in Florida to calculate exceedance probabilities for risk management.
    
[^412]: 具有过多风险的鲁棒多任务学习

    Robust Multi-Task Learning with Excess Risks

    [https://arxiv.org/abs/2402.02009](https://arxiv.org/abs/2402.02009)

    提出了一种具有过多风险的多任务学习（ExcessMTL）方法，根据任务到收敛的距离来更新任务权重，以克服存在标签噪声时现有方法的限制。

    

    多任务学习（MTL）通过优化所有任务损失的凸组合来考虑为多个任务学习一个联合模型。为了解决优化问题，现有方法使用自适应权重更新方案，根据各自的损失动态调整任务权重，以优先考虑困难任务。然而，在存在标签噪声的情况下，这些算法会面临巨大挑战，因为过多的权重往往被分配给具有相对较大贝叶斯最优误差的噪声任务，从而掩盖其他任务并导致整体性能下降。为了克服这个限制，我们提出了具有过多风险的多任务学习（ExcessMTL），这是一种基于过多风险的任务平衡方法，通过任务到收敛的距离来更新任务权重。直观来说，ExcessMTL将更高的权重分配给较差训练的距离收敛较远的任务。为了估计过多风险，我们开发了一种高效而准确的方法。

    Multi-task learning (MTL) considers learning a joint model for multiple tasks by optimizing a convex combination of all task losses. To solve the optimization problem, existing methods use an adaptive weight updating scheme, where task weights are dynamically adjusted based on their respective losses to prioritize difficult tasks. However, these algorithms face a great challenge whenever label noise is present, in which case excessive weights tend to be assigned to noisy tasks that have relatively large Bayes optimal errors, thereby overshadowing other tasks and causing performance to drop across the board. To overcome this limitation, we propose Multi-Task Learning with Excess Risks (ExcessMTL), an excess risk-based task balancing method that updates the task weights by their distances to convergence instead. Intuitively, ExcessMTL assigns higher weights to worse-trained tasks that are further from convergence. To estimate the excess risks, we develop an efficient and accurate method 
    
[^413]: 通过单类分类理解时间序列异常状态检测

    Understanding Time Series Anomaly State Detection through One-Class Classification

    [https://arxiv.org/abs/2402.02007](https://arxiv.org/abs/2402.02007)

    本文重新定义了时间序列异常检测问题为时间序列异常状态检测问题，并使用单类分类方法进行解决。

    

    长期以来，时间序列异常检测的研究主要集中在发现给定时间序列中的异常值。诚然，这与一些实际问题是一致的，但在其他实际应用场景中，人们关心的是：假设给定一个标准时间序列，如何判断另一个测试时间序列是否偏离标准时间序列，这更类似于讨论的单类分类（OCC）问题。因此，在本文中，我们尝试通过OCC重新理解和定义时间序列异常检测问题，我们称之为“时间序列异常状态检测问题”。我们首先使用随机过程和假设检验严格定义了“时间序列异常状态检测问题”及其对应的异常情况。然后，我们使用时间序列分类数据集构建相应问题的人工数据集。我们编制了38种异常检测算法并纠正了其中一些算法。

    For a long time, research on time series anomaly detection has mainly focused on finding outliers within a given time series. Admittedly, this is consistent with some practical problems, but in other practical application scenarios, people are concerned about: assuming a standard time series is given, how to judge whether another test time series deviates from the standard time series, which is more similar to the problem discussed in one-class classification (OCC). Therefore, in this article, we try to re-understand and define the time series anomaly detection problem through OCC, which we call 'time series anomaly state detection problem'. We first use stochastic processes and hypothesis testing to strictly define the 'time series anomaly state detection problem', and its corresponding anomalies. Then, we use the time series classification dataset to construct an artificial dataset corresponding to the problem. We compile 38 anomaly detection algorithms and correct some of the algori
    
[^414]: PresAIse，一种企业预测性人工智能解决方案

    PresAIse, An Enterprises Prescriptive AI Solution

    [https://arxiv.org/abs/2402.02006](https://arxiv.org/abs/2402.02006)

    PresAIse是一种企业预测性人工智能解决方案，通过提供因果推断方法、可解释的决策制定方法和大型语言模型（LLMs）的集成，旨在解决企业预测性人工智能面临的数据限制、建议可解释性和数据科学家与业务用户之间的合作障碍等挑战。

    

    预测性人工智能代表了决策制定中的一次转型性变革，提供因果洞察和可操作的建议。尽管具有巨大潜力，但企业采用常常面临几个挑战。首先，观察数据的限制使得准确的因果推断成为了良好决策制定的前提条件。其次，建议的可解释性对于企业决策制定至关重要。第三个挑战是数据科学家和业务用户之间的隔离，阻碍了有效的合作。本文概述了IBM研究的一项倡议，旨在通过提供一套预测性 AI 解决方案来解决其中一些挑战。利用来自各种研究论文的见解，解决方案套件包括可扩展的因果推断方法、可解释的决策制定方法以及通过对话框架来弥合沟通隔阂的大型语言模型 (LLMs) 的集成。

    Prescriptive AI represents a transformative shift in decision-making, offering causal insights and actionable recommendations. Despite its huge potential, enterprise adoption often faces several challenges. The first challenge is caused by the limitations of observational data for accurate causal inference which is typically a prerequisite for good decision-making. The second pertains to the interpretability of recommendations, which is crucial for enterprise decision-making settings. The third challenge is the silos between data scientists and business users, hindering effective collaboration. This paper outlines an initiative from IBM Research, aiming to address some of these challenges by offering a suite of prescriptive AI solutions. Leveraging insights from various research papers, the solution suite includes scalable causal inference methods, interpretable decision-making approaches, and the integration of large language models (LLMs) to bridge communication gaps via a conversati
    
[^415]: 基于拓扑信息的图形变换器

    Topology-Informed Graph Transformer

    [https://arxiv.org/abs/2402.02005](https://arxiv.org/abs/2402.02005)

    TIGT是一种基于拓扑信息的新型图形变换器，通过增强区分图同构性的能力和提高图形变换器性能，实现了对图同构性的检测和整体性能的增强。

    

    变形器在自然语言处理和视觉领域中取得了突破性的成果，为与图神经网络（GNN）的集成铺平了道路。增强图形变换器的一个关键挑战是增强区分图的同构性的区分能力，这在提高它们的预测性能中起到关键作用。为了解决这个挑战，我们引入了一种新的变形器——“基于拓扑信息的图形变换器（TIGT）”，它增强了检测图同构性的区分能力和图形变换器的整体性能。TIGT由四个组件组成：一个使用基于图的循环子图的非同构卷上的拓扑位置嵌入层，以确保唯一的图表示；一个双路径消息传递层，以明确地编码拓扑特征；一个全局注意机制；和一个图信息层，用于重新校准通道级的图特征。

    Transformers have revolutionized performance in Natural Language Processing and Vision, paving the way for their integration with Graph Neural Networks (GNNs). One key challenge in enhancing graph transformers is strengthening the discriminative power of distinguishing isomorphisms of graphs, which plays a crucial role in boosting their predictive performances. To address this challenge, we introduce 'Topology-Informed Graph Transformer (TIGT)', a novel transformer enhancing both discriminative power in detecting graph isomorphisms and the overall performance of Graph Transformers. TIGT consists of four components: A topological positional embedding layer using non-isomorphic universal covers based on cyclic subgraphs of graphs to ensure unique graph representation: A dual-path message-passing layer to explicitly encode topological characteristics throughout the encoder layers: A global attention mechanism: And a graph information layer to recalibrate channel-wise graph features for be
    
[^416]: 图结构压缩综述

    A Survey on Graph Condensation

    [https://arxiv.org/abs/2402.02000](https://arxiv.org/abs/2402.02000)

    本综述调查了图结构压缩（GC）的最新研究，提出了GC的形式化定义，并将现有方法按照目标和公式分类。同时，综述还包括了对数据集和评估指标的全面分析。

    

    大规模图数据的分析在计算效率和资源需求方面面临着重大挑战。最近，图结构压缩（GC）作为一种解决方案出现，以应对不断增加的图数据。GC的动机是将大图缩小为较小的图，同时保留下游任务所需的关键信息。为了更好地理解GC，并将其区别于其他相关主题，我们提出了GC的正式定义，并建立了一个系统分类体系，将现有方法根据目标分为三种类型，并将生成压缩图的公式分类为修改原始图或合成全新图两类。此外，我们的综述还包括对该领域中数据集和评估指标的全面分析。最后，我们通过讨论挑战和限制，概述未来的方向，并提供简明的指导。

    Analytics on large-scale graphs have posed significant challenges to computational efficiency and resource requirements. Recently, Graph condensation (GC) has emerged as a solution to address challenges arising from the escalating volume of graph data. The motivation of GC is to reduce the scale of large graphs to smaller ones while preserving essential information for downstream tasks. For a better understanding of GC and to distinguish it from other related topics, we present a formal definition of GC and establish a taxonomy that systematically categorizes existing methods into three types based on its objective, and classify the formulations to generate the condensed graphs into two categories as modifying the original graphs or synthetic completely new ones. Moreover, our survey includes a comprehensive analysis of datasets and evaluation metrics in this field. Finally, we conclude by addressing challenges and limitations, outlining future directions, and offering concise guidelin
    
[^417]: 边缘上基于新颖超维度计算的在线时间序列预测框架

    A Novel Hyperdimensional Computing Framework for Online Time Series Forecasting on the Edge

    [https://arxiv.org/abs/2402.01999](https://arxiv.org/abs/2402.01999)

    本文提出了一种新颖的超维度计算框架，用于解决在线时间序列预测问题。通过将非线性低维数据映射到高维空间进行线性超维度预测，实现了快速、高效和轻量级的预测，同时适应了时间序列数据的变化。

    

    近年来，已经开发出在线和离线的深度学习模型用于时间序列预测。然而，离线深度预测模型不能有效适应时间序列数据的变化，而在线深度预测模型常常昂贵且具有复杂的训练过程。在本文中，我们重新定义了在线非线性时间序列预测问题，将其作为线性超维度时间序列预测的问题。非线性低维时间序列数据被映射到高维（超维度）空间进行线性超维度预测，实现了快速、高效和轻量级的在线时间序列预测。我们的框架TSF-HD使用了一种新颖的共训练框架，用于超维度映射和线性超维度预测器，以适应时间序列分布的变化。TSF-HD在短期和长期时间序列预测方面优于现有技术，并降低了推断延迟。我们的代码已经公开发布。

    In recent years, both online and offline deep learning models have been developed for time series forecasting. However, offline deep forecasting models fail to adapt effectively to changes in time-series data, while online deep forecasting models are often expensive and have complex training procedures. In this paper, we reframe the online nonlinear time-series forecasting problem as one of linear hyperdimensional time-series forecasting. Nonlinear low-dimensional time-series data is mapped to high-dimensional (hyperdimensional) spaces for linear hyperdimensional prediction, allowing fast, efficient and lightweight online time-series forecasting. Our framework, TSF-HD, adapts to time-series distribution shifts using a novel co-training framework for its hyperdimensional mapping and its linear hyperdimensional predictor. TSF-HD is shown to outperform the state of the art, while having reduced inference latency, for both short-term and long-term time series forecasting. Our code is publi
    
[^418]: 在线均匀风险时间抽样：第一次近似算法，具有全置信区间集成的学习增强

    Online Uniform Risk Times Sampling: First Approximation Algorithms, Learning Augmentation with Full Confidence Interval Integration

    [https://arxiv.org/abs/2402.01995](https://arxiv.org/abs/2402.01995)

    本文首次引入在线均匀风险时间抽样问题，并提出了两种在线近似算法，一种带有学习增强，一种没有学习增强。通过竞争比分析，我们提供了严格的理论性能保证。我们通过合成实验和实际案例研究评估了算法的性能。

    

    在数字健康领域，将有限的治疗预算分配到可用的风险时间上是减少用户疲劳的关键策略。然而，由于未知的实际风险时间数量，这一策略遇到了显著的障碍，现有方法在理论保证方面还不足够。本文首次将在线均匀风险时间抽样问题引入近似算法框架。我们提出了两种在线近似算法，一种带有学习增强，一种没有学习增强，并使用竞争比分析为它们提供了严格的理论性能保证。我们使用合成实验和HeartSteps移动应用的实际案例研究评估了我们算法的性能。

    In digital health, the strategy of allocating a limited treatment budget across available risk times is crucial to reduce user fatigue. This strategy, however, encounters a significant obstacle due to the unknown actual number of risk times, a factor not adequately addressed by existing methods lacking theoretical guarantees. This paper introduces, for the first time, the online uniform risk times sampling problem within the approximation algorithm framework. We propose two online approximation algorithms for this problem, one with and one without learning augmentation, and provide rigorous theoretical performance guarantees for them using competitive ratio analysis. We assess the performance of our algorithms using both synthetic experiments and a real-world case study on HeartSteps mobile applications.
    
[^419]: 在线转移学习用于RSV病例检测

    Online Transfer Learning for RSV Case Detection

    [https://arxiv.org/abs/2402.01987](https://arxiv.org/abs/2402.01987)

    这项研究介绍了一种名为PVAW的在线多源转移学习方法，通过动态加权机制实现了对序列流行病学数据的自适应调整，并在分析RSV数据的应用中取得了显著的模型性能改进。

    

    转移学习已经成为机器学习中的一个关键技术，在各种真实世界应用中表现出高效性。然而，当将这种方法应用于序列流行病学数据时，会面临一个重要挑战，即标注信息匮乏。为了解决这个挑战，我们引入了一种新颖的在线多源转移学习方法，称为预测体积自适应加权（PVAW）。PVAW在整合模型中创造性地实现了动态加权机制，可以根据每个源模型和目标模型的相关性和贡献度自动调整权重。我们通过在匹兹堡大学医学中心收集的多个季节的呼吸道合胞病毒（RSV）数据上应用PVAW，证明了其有效性。我们的方法在模型性能上显著优于现有基线，突出了在线转移学习在处理这一问题上的潜力。

    Transfer learning has become a pivotal technique in machine learning, renowned for its effectiveness in various real-world applications. However, a significant challenge arises when applying this approach to sequential epidemiological data, often characterized by a scarcity of labeled information. To address this challenge, we introduce Predictive Volume-Adaptive Weighting (PVAW), a novel online multi-source transfer learning method. PVAW innovatively implements a dynamic weighting mechanism within an ensemble model, allowing for the automatic adjustment of weights based on the relevance and contribution of each source and target model. We demonstrate the effectiveness of PVAW through its application in analyzing Respiratory Syncytial Virus (RSV) data, collected over multiple seasons at the University of Pittsburgh Medical Center. Our method showcases significant improvements in model performance over existing baselines, highlighting the potential of online transfer learning in handlin
    
[^420]: 自我去偏差的大型语言模型：零-shot识别和降低刻板印象

    Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes

    [https://arxiv.org/abs/2402.01981](https://arxiv.org/abs/2402.01981)

    本研究利用大型语言模型的零-shot能力提出了一种自我去偏差的技术，通过解释和重启两种方法，成功减少了九个不同社会群体的刻板印象程度，该技术不需要对训练数据、模型参数或解码策略进行修改，希望能启发其他零-shot偏见缓解技术的研究。

    

    大型语言模型(LLMs)在语言生成和理解方面表现出了显著的进展，但也容易展示出有害的社会偏见。尽管已经提出了许多偏见缓解技术，但大多数需要对训练数据、模型参数或解码策略进行修改，这在没有可训练模型的情况下可能是不可行的。在这项工作中，我们利用LLMs的零-shot能力，引入了一种称为零-shot自我去偏差的技术来减少刻板印象。通过两种方法，即解释自我去偏差和重启自我去偏差，我们展示了自我去偏差可以显著减少九个不同社会群体的刻板印象程度，只依赖于LLM自身和简单的提示，其中解释正确地识别出无效的假设，而重启则产生了最大的偏见减少效果。我们希望这项工作能够开启对其他零-shot偏见缓解技术的研究。

    Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mi
    
[^421]: 结构感知的E(3) 不变性分子构型聚合网络

    Structure-Aware E(3)-Invariant Molecular Conformer Aggregation Networks

    [https://arxiv.org/abs/2402.01975](https://arxiv.org/abs/2402.01975)

    本论文提出了一种结构感知的E(3) 不变性分子构型聚合网络，将分子的2D表示与其多个构型的表示整合在一起，通过使用一个新型的2D-3D聚合机制，融合Gromov-Wasserstein变量问题，并使用高效的在线构型生成方法。

    

    一个分子的2D表示由其原子、原子属性和分子的共价键组成。分子的3D（几何）表示称为构型，由其原子类型和笛卡尔坐标组成。每个构型都具有潜在能量，能量越低，其在自然界中出现的可能性越大。大多数现有的分子性质预测的机器学习方法要么只考虑2D分子图，要么只考虑3D构型结构表示。受到最近关于在2D图表示和构型集合中使用集成的研究启发，我们提出了E(3) 不变性分子构型聚合网络。该方法将分子的2D表示与其多个构型的表示整合在一起。与以往的研究相反，我们提出了一种基于可微分求解器的新型2D-3D聚合机制，用于融合Gromov-Wasserstein变量问题，并使用高效的在线构型生成方法。

    A molecule's 2D representation consists of its atoms, their attributes, and the molecule's covalent bonds. A 3D (geometric) representation of a molecule is called a conformer and consists of its atom types and Cartesian coordinates. Every conformer has a potential energy, and the lower this energy, the more likely it occurs in nature. Most existing machine learning methods for molecular property prediction consider either 2D molecular graphs or 3D conformer structure representations in isolation. Inspired by recent work on using ensembles of conformers in conjunction with 2D graph representations, we propose E(3)-invariant molecular conformer aggregation networks. The method integrates a molecule's 2D representation with that of multiple of its conformers. Contrary to prior work, we propose a novel 2D--3D aggregation mechanism based on a differentiable solver for the \emph{Fused Gromov-Wasserstein Barycenter} problem and the use of an efficient online conformer generation method based 
    
[^422]: 组合T-learning和DR-learning：一个用于高效估计因果对比的框架

    Combining T-learning and DR-learning: a framework for oracle-efficient estimation of causal contrasts

    [https://arxiv.org/abs/2402.01972](https://arxiv.org/abs/2402.01972)

    这篇论文介绍了高效插件学习的框架，能够有效估计异质因果对比，并解决了其他学习策略的一些缺点。该框架构建了人口风险函数的高效插件估计器，具有稳定性和鲁棒性。

    

    我们引入了高效插件（EP）学习，这是一种用于估计异质因果对比的新框架，例如条件平均处理效应和条件相对风险。 EP学习框架享有与Neyman正交学习策略（如DR-learning和R-learning）相同的oracle效率，同时解决了它们的一些主要缺点，包括（i）实际适用性可能受到损失函数非凸性的阻碍； （ii）它们可能因违反界限的倒数概率加权和伪结果而导致性能和稳定性差。为了避免这些缺点，EP学习者构建了因果对比的人口风险函数的高效插件估计器，从而继承了T-learning等插件估计策略的稳定性和鲁棒性特性。在合理条件下，基于经验风险最小化的EP学习者具有oracle效率，表现出渐近等价的性质。

    We introduce efficient plug-in (EP) learning, a novel framework for the estimation of heterogeneous causal contrasts, such as the conditional average treatment effect and conditional relative risk. The EP-learning framework enjoys the same oracle-efficiency as Neyman-orthogonal learning strategies, such as DR-learning and R-learning, while addressing some of their primary drawbacks, including that (i) their practical applicability can be hindered by loss function non-convexity; and (ii) they may suffer from poor performance and instability due to inverse probability weighting and pseudo-outcomes that violate bounds. To avoid these drawbacks, EP-learner constructs an efficient plug-in estimator of the population risk function for the causal contrast, thereby inheriting the stability and robustness properties of plug-in estimation strategies like T-learning. Under reasonable conditions, EP-learners based on empirical risk minimization are oracle-efficient, exhibiting asymptotic equivalen
    
[^423]: 机器学习路径损耗预测的模拟增强数据增强方法

    Simulation-Enhanced Data Augmentation for Machine Learning Pathloss Prediction

    [https://arxiv.org/abs/2402.01969](https://arxiv.org/abs/2402.01969)

    本文提出了一种模拟增强数据增强方法，用于改善机器学习路径损耗预测中数据有限的挑战。该方法通过结合来自蜂窝覆盖模拟器生成的合成数据和独立收集的真实世界数据集，提供了关键的真实值用于模型训练。通过使用特征工程和高效稳健的梯度提升机器学习算法CatBoost，该方法显著提高了路径损耗预测的性能。

    

    机器学习（ML）对于路径损耗预测提供了一种有前景的解决方案。然而，由于数据的有限可用性，其有效性可能会降低。为了减轻这些挑战，本文引入了一种新颖的模拟增强数据增强方法，用于机器学习路径损耗预测。我们的方法将从蜂窝覆盖模拟器生成的合成数据与独立收集的现实世界数据集相结合。这些数据集通过在不同环境（包括农场、丘陵地带和住宅区）开展广泛的测量活动来收集。这种全面的数据收集为模型训练提供了至关重要的真实值。我们还设计了一组信道特征，包括从LiDAR数据集中导出的地理属性。然后，我们使用这些特征来训练我们的预测模型，结合了高效且稳健的梯度提升机器学习算法CatBoost。在我们的研究中，模拟数据的集成显著改善了路径损耗预测的性能。

    Machine learning (ML) offers a promising solution to pathloss prediction. However, its effectiveness can be degraded by the limited availability of data. To alleviate these challenges, this paper introduces a novel simulation-enhanced data augmentation method for ML pathloss prediction. Our method integrates synthetic data generated from a cellular coverage simulator and independently collected real-world datasets. These datasets were collected through an extensive measurement campaign in different environments, including farms, hilly terrains, and residential areas. This comprehensive data collection provides vital ground truth for model training. A set of channel features was engineered, including geographical attributes derived from LiDAR datasets. These features were then used to train our prediction model, incorporating the highly efficient and robust gradient boosting ML algorithm, CatBoost. The integration of synthetic data, as demonstrated in our study, significantly improves t
    
[^424]: 对上下文感知多agent系统的调查：技术、挑战和未来发展方向

    A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions

    [https://arxiv.org/abs/2402.01968](https://arxiv.org/abs/2402.01968)

    这篇论文调查了上下文感知多代理系统的技术、挑战和未来发展方向，并提供了一个综合的概述。它介绍了上下文感知系统和多代理系统的特性，以及集成这些系统的通用过程。

    

    随着新兴主题的兴起，自主代理的研究兴趣正在增加。大型语言模型的显著成就已经展示了在自主代理中达到人类智能的巨大潜力。然而，挑战在于使这些代理能够在动态环境中学习、推理和导航不确定性。当处理动态情况时，上下文意识成为强化多agent系统的关键因素。尽管现有的研究专注于上下文感知系统和多agent系统，但缺乏全面概述如何将上下文感知系统与多agent系统集成的综合调查。为了填补这个空白，本调查提供了对最先进的上下文感知多agent系统的全面概述。首先，我们概述了促进这些系统之间集成的上下文感知系统和多 agent 系统的特性。随后，我们提出了一个通用的过程来建模上下文感知和多agent系统的集成。

    Research interest in autonomous agents is on the rise as an emerging topic. The notable achievements of Large Language Models (LLMs) have demonstrated the considerable potential to attain human-like intelligence in autonomous agents. However, the challenge lies in enabling these agents to learn, reason, and navigate uncertainties in dynamic environments. Context awareness emerges as a pivotal element in fortifying multi-agent systems when dealing with dynamic situations. Despite existing research focusing on both context-aware systems and multi-agent systems, there is a lack of comprehensive surveys outlining techniques for integrating context-aware systems with multi-agent systems. To address this gap, this survey provides a comprehensive overview of state-of-the-art context-aware multi-agent systems. First, we outline the properties of both context-aware systems and multi-agent systems that facilitate integration between these systems. Subsequently, we propose a general process for c
    
[^425]: 通过凸优化对基于神经网络的生成扩散模型进行分析

    Analyzing Neural Network-Based Generative Diffusion Models through Convex Optimization

    [https://arxiv.org/abs/2402.01965](https://arxiv.org/abs/2402.01965)

    本研究通过凸优化方法分析了基于神经网络的生成扩散模型，揭示了这些模型在非渐近设置下的精确预测分数函数和收敛结果。

    

    扩散模型在最先进的图像、视频和音频生成中变得广泛使用。基于分数的扩散模型在这些方法中脱颖而出，需要估计输入数据分布的分数函数。在本研究中，我们提出了一个理论框架，通过将分数匹配和去噪分数匹配重新构建为凸优化的形式，来分析两层神经网络的扩散模型。尽管现有的扩散理论主要是渐近的，但我们对神经网络的扩散模型给出了精确的预测分数函数，并建立了有限数据情况下的收敛结果。这项工作有助于理解神经网络的非渐近设置中学习到的扩散模型。

    Diffusion models are becoming widely used in state-of-the-art image, video and audio generation. Score-based diffusion models stand out among these methods, necessitating the estimation of score function of the input data distribution. In this study, we present a theoretical framework to analyze two-layer neural network-based diffusion models by reframing score matching and denoising score matching as convex optimization. Though existing diffusion theory is mainly asymptotic, we characterize the exact predicted score function and establish the convergence result for neural network-based diffusion models with finite data. This work contributes to understanding what neural network-based diffusion model learns in non-asymptotic settings.
    
[^426]: 不需回顾：一种高效可扩展的时态网络表示学习方法

    No Need to Look Back: An Efficient and Scalable Approach for Temporal Network Representation Learning

    [https://arxiv.org/abs/2402.01964](https://arxiv.org/abs/2402.01964)

    本论文提出了一种高效可扩展的时态网络表示学习方法，该方法通过前向最近采样策略和GPU可执行的大小受限哈希表实现了对查询的快速响应和最小化推理延迟。

    

    时态图表示学习（TGRL）对于建模实际网络中复杂动态系统至关重要。传统的TGRL方法虽然有效，但计算需求和推理延迟较高。这主要是由于在进行模型推理时，通过回溯每个节点的交互历史来进行时态邻居的低效采样所致。本文介绍了一种新颖的高效TGRL框架，名为No-Looking-Back（NLB）。NLB采用了“前向最近采样”策略，绕过了回溯历史交互的需求。该策略通过使用针对每个节点的GPU可执行的大小受限哈希表记录下采样后的最近交互，实现对查询的快速响应和最小化推理延迟。该哈希表的维护具有高效性，复杂度为$O(1)$。NLB与GPU处理完全兼容，最大化了可编程性、并行性和能效。实证评估表明...

    Temporal graph representation learning (TGRL) is crucial for modeling complex, dynamic systems in real-world networks. Traditional TGRL methods, though effective, suffer from high computational demands and inference latency. This is mainly induced by their inefficient sampling of temporal neighbors by backtracking the interaction history of each node when making model inference. This paper introduces a novel efficient TGRL framework, No-Looking-Back (NLB). NLB employs a "forward recent sampling" strategy, which bypasses the need for backtracking historical interactions. This strategy is implemented using a GPU-executable size-constrained hash table for each node, recording down-sampled recent interactions, which enables rapid response to queries with minimal inference latency. The maintenance of this hash table is highly efficient, with $O(1)$ complexity. NLB is fully compatible with GPU processing, maximizing programmability, parallelism, and power efficiency. Empirical evaluations de
    
[^427]: 使用标签自编码器改进大规模k最近邻文本分类

    Improving Large-Scale k-Nearest Neighbor Text Categorization with Label Autoencoders

    [https://arxiv.org/abs/2402.01963](https://arxiv.org/abs/2402.01963)

    本文提出了一种使用标签自编码器改进大规模k最近邻文本分类的方法，通过将大的标签空间映射到一个缩小的潜在空间，并从该潜在空间中重建出预测的标签，以解决在大型文档集合中处理自动语义索引的问题。

    

    本文介绍了一种多标签惰性学习方法，用于在复杂和结构化标签词汇具有高相关性的大型文档集合中处理自动语义索引。所提出的方法是传统k最近邻算法的演化，它使用一个大的自编码器来将大的标签空间映射到一个缩小的潜在空间，并从该潜在空间中重建出预测的标签。我们在MEDLINE生物医学文档集合的大部分上评估了我们的提议，该集合使用医学主题词（MeSH）词汇表作为控制词汇。在我们的实验中，我们提出并评估了几种文档表示方法和不同的标签自编码器配置。

    In this paper, we introduce a multi-label lazy learning approach to deal with automatic semantic indexing in large document collections in the presence of complex and structured label vocabularies with high inter-label correlation. The proposed method is an evolution of the traditional k-Nearest Neighbors algorithm which uses a large autoencoder trained to map the large label space to a reduced size latent space and to regenerate the predicted labels from this latent space. We have evaluated our proposal in a large portion of the MEDLINE biomedical document collection which uses the Medical Subject Headings (MeSH) thesaurus as a controlled vocabulary. In our experiments we propose and evaluate several document representation approaches and different label autoencoder configurations.
    
[^428]: 通过符合预测实现运算器学习的校准不确定性量化

    Calibrated Uncertainty Quantification for Operator Learning via Conformal Prediction

    [https://arxiv.org/abs/2402.01960](https://arxiv.org/abs/2402.01960)

    通过符合预测方法，提出了一种校准不确定性量化的分位数神经运算器，能够在函数定义域上同时量化不确定性，无需分布假设，实验结果表明其在2D Darcy流动和3D车辆表面压力预测任务上优于基线方法。

    

    运算器学习在科学和工程应用中越来越被采用，其中很多应用需要校准的不确定性量化。由于运算器学习的输出是连续函数，在整个定义域上同时量化不确定性是具有挑战性的。当前的方法只考虑一个点的校准，或者针对一个标量函数进行校准，或者做出强大的假设，比如假设高斯性。我们提出了一种风险控制的分位数神经运算器,一种无分布、有限样本的函数校准符合预测方法。我们提供了一个理论上的校准保证，即覆盖率，其定义为函数定义域内真实值位于预测不确定性球内的预期百分比。在一个2D Darcy流动和一个3D车辆表面压力预测任务上的实证结果验证了我们的理论结果，表明校准的覆盖率和有效的不确定性区间优于基线方法。

    Operator learning has been increasingly adopted in scientific and engineering applications, many of which require calibrated uncertainty quantification. Since the output of operator learning is a continuous function, quantifying uncertainty simultaneously at all points in the domain is challenging. Current methods consider calibration at a single point or over one scalar function or make strong assumptions such as Gaussianity. We propose a risk-controlling quantile neural operator, a distribution-free, finite-sample functional calibration conformal prediction method. We provide a theoretical calibration guarantee on the coverage rate, defined as the expected percentage of points on the function domain whose true value lies within the predicted uncertainty ball. Empirical results on a 2D Darcy flow and a 3D car surface pressure prediction tasks validate our theoretical results, demonstrating calibrated coverage and efficient uncertainty bands outperforming baseline methods. In particula
    
[^429]: OPSurv：用于生存分析的正交多项式积分算法

    OPSurv: Orthogonal Polynomials Quadrature Algorithm for Survival Analysis

    [https://arxiv.org/abs/2402.01955](https://arxiv.org/abs/2402.01955)

    本文介绍了一种用于生存分析的新方法OPSurv，通过正交多项式积分算法提供连续时间功能性输出。该方法利用累积发生函数的初始零条件和正交多项式的分解，实现了对每个风险事件的功能近似系数的学习，并通过高斯-勒让德积分法构建累积发生函数的估计值，有效地解决了竞争风险场景中的过度拟合问题。

    

    本文介绍了用于生存分析中单一风险和竞争风险场景的Orthogonal Polynomials Quadrature Algorithm for Survival Analysis（OPSurv）方法，该方法提供了连续时间功能性输出。OPSurv利用累积发生函数的初始零条件和使用正交多项式对概率密度进行唯一分解，从而为每个风险事件学习功能近似系数，并通过高斯-勒让德积分法构建累积发生函数的估计值。该方法有效地对抗过度拟合，尤其在竞争风险场景中，增强模型的表达能力和控制性。本文还详细介绍了OPSurv的经验验证和理论证明，突出了其作为竞争风险生存分析的一项重要进展的鲁棒性能。

    This paper introduces the Orthogonal Polynomials Quadrature Algorithm for Survival Analysis (OPSurv), a new method providing time-continuous functional outputs for both single and competing risks scenarios in survival analysis. OPSurv utilizes the initial zero condition of the Cumulative Incidence function and a unique decomposition of probability densities using orthogonal polynomials, allowing it to learn functional approximation coefficients for each risk event and construct Cumulative Incidence Function estimates via Gauss--Legendre quadrature. This approach effectively counters overfitting, particularly in competing risks scenarios, enhancing model expressiveness and control. The paper further details empirical validations and theoretical justifications of OPSurv, highlighting its robust performance as an advancement in survival analysis with competing risks.
    
[^430]: 有效图形数据估值的优先约束冬季价值

    Precedence-Constrained Winter Value for Effective Graph Data Valuation

    [https://arxiv.org/abs/2402.01943](https://arxiv.org/abs/2402.01943)

    提出了一种名为PC-Winter的优先约束冬季价值方法，用于有效地评估复杂图形数据的价值。通过解决复杂的图形结构以及计算挑战，PC-Winter方法在各种数据集和任务中展示了其有效性。

    

    数据估值对于量化数据的价值、评估数据质量和确定公平补偿至关重要。现有的数据估值方法在评估欧几里德数据的价值方面已被证明有效，但在应用于越来越受欢迎的图形数据时存在局限性。特别是，图形数据估值引入了独特的挑战，主要源于节点之间复杂的依赖关系和价值估计成本的指数增长。为了解决图形数据估值的问题，我们提出了一种创新的解决方案，称为优先约束冬季价值(Precedence-Constrained Winter, PC-Winter)，以考虑复杂的图形结构。此外，我们还开发了多种策略来解决计算挑战，实现对PC-Winter的高效近似。大量的实验证明了PC-Winter在各种数据集和任务上的有效性。

    Data valuation is essential for quantifying data's worth, aiding in assessing data quality and determining fair compensation. While existing data valuation methods have proven effective in evaluating the value of Euclidean data, they face limitations when applied to the increasingly popular graph-structured data. Particularly, graph data valuation introduces unique challenges, primarily stemming from the intricate dependencies among nodes and the exponential growth in value estimation costs. To address the challenging problem of graph data valuation, we put forth an innovative solution, Precedence-Constrained Winter (PC-Winter) Value, to account for the complex graph structure. Furthermore, we develop a variety of strategies to address the computational challenges and enable efficient approximation of PC-Winter. Extensive experiments demonstrate the effectiveness of PC-Winter across diverse datasets and tasks.
    
[^431]: 准确和安全交易的数字微模型

    Digits micro-model for accurate and secure transactions

    [https://arxiv.org/abs/2402.01931](https://arxiv.org/abs/2402.01931)

    本研究展示了数字微模型在准确和安全交易中的潜力，这些轻量级模型在数字识别特定任务上表现良好，并使用较少的训练时间和内存资源。

    

    自动语音识别（ASR）系统在金融领域中被用于提升呼叫者体验，通过实现自然语言理解和实现高效直观的互动。ASR系统的增加使用要求这些系统具有非常低的错误率。目前主要的ASR模型用于数字数据收集是大型通用商用模型- Google语音转文字（STT）或亚马逊转录-或开源（OpenAI的Whisper）。这些ASR模型通过数十万小时的音频数据进行训练，需要大量资源运行。尽管最近大型语音识别模型取得了进展，我们强调了小型专门的“微”模型的潜力。这样的轻量级模型可以通过少于80分钟的训练时间表现良好于数字识别特定任务中，与Whisper或Google STT等通用模型竞争，同时使用至少一个数量级更少的内存资源。另外，不同于更大的语音模型，这些数字微模型可以提供更准确和安全的交易。

    Automatic Speech Recognition (ASR) systems are used in the financial domain to enhance the caller experience by enabling natural language understanding and facilitating efficient and intuitive interactions. Increasing use of ASR systems requires that such systems exhibit very low error rates. The predominant ASR models to collect numeric data are large, general-purpose commercial models -- Google Speech-to-text (STT), or Amazon Transcribe -- or open source (OpenAI's Whisper). Such ASR models are trained on hundreds of thousands of hours of audio data and require considerable resources to run. Despite recent progress large speech recognition models, we highlight the potential of smaller, specialized "micro" models. Such light models can be trained perform well on number recognition specific tasks, competing with general models like Whisper or Google STT while using less than 80 minutes of training time and occupying at least an order of less memory resources. Also, unlike larger speech 
    
[^432]: 减少不完全合作博弈中的乐观偏误

    Reducing Optimism Bias in Incomplete Cooperative Games

    [https://arxiv.org/abs/2402.01930](https://arxiv.org/abs/2402.01930)

    本文提出了一个框架，旨在通过优化揭示联盟价值的顺序来减少不完全合作博弈中的乐观偏误。

    

    合作博弈理论在当代人工智能中具有广泛的应用，包括解释性机器学习、资源分配和协同决策等领域。然而，指定一个合作博弈需要为指数多个联盟分配价值，并且在实践中获得一个联盟价值可能会消耗大量资源。然而，简单地不公开某些联盟的价值会引入关于个体对集体大联盟的贡献的模糊性。这种模糊性经常导致玩家持有过于乐观的期望，其源于内在偏见或战略考虑，进而常常导致集体要求超过实际的大联盟价值。本文提出了一个框架，旨在优化揭示联盟价值的顺序，以实现有效地缩小合作博弈中玩家期望与可实现结果之间的差距。

    Cooperative game theory has diverse applications in contemporary artificial intelligence, including domains like interpretable machine learning, resource allocation, and collaborative decision-making. However, specifying a cooperative game entails assigning values to exponentially many coalitions, and obtaining even a single value can be resource-intensive in practice. Yet simply leaving certain coalition values undisclosed introduces ambiguity regarding individual contributions to the collective grand coalition. This ambiguity often leads to players holding overly optimistic expectations, stemming from either inherent biases or strategic considerations, frequently resulting in collective claims exceeding the actual grand coalition value. In this paper, we present a framework aimed at optimizing the sequence for revealing coalition values, with the overarching goal of efficiently closing the gap between players' expectations and achievable outcomes in cooperative games. Our contributio
    
[^433]: 样本、估计、聚合：因果发现基础模型的一种方法

    Sample, estimate, aggregate: A recipe for causal discovery foundation models

    [https://arxiv.org/abs/2402.01929](https://arxiv.org/abs/2402.01929)

    本文提出一种因果发现框架，通过深度学习模型预训练与经典发现算法的结合，实现了快速、准确地推断因果结构，并在实验中展示了与现有方法相比更好的表现和推理速度。

    

    因果发现是从数据中推断因果结构的任务，它可以加速科学研究、指导决策等。然而，现有因果发现算法的每个数据集的特性使它们变得缓慢、需要大量数据并且脆弱。受基础模型的启发，我们提出了一种因果发现框架，其中深度学习模型预训练用于处理在较小的变量子集上运行的经典发现算法的预测。这种方法可以利用以下观察结果：经典算法的输出在小问题上计算速度快，对（边际）数据结构具有信息量，且它们的输出结构作为对象在数据集之间可以进行比较。我们的方法在合成和实际数据集上实现了最先进的性能，可以推广到训练期间未见过的数据生成机制，并且提供比现有模型快几个数量级的推理速度。

    Causal discovery, the task of inferring causal structure from data, promises to accelerate scientific research, inform policy making, and more. However, the per-dataset nature of existing causal discovery algorithms renders them slow, data hungry, and brittle. Inspired by foundation models, we propose a causal discovery framework where a deep learning model is pretrained to resolve predictions from classical discovery algorithms run over smaller subsets of variables. This method is enabled by the observations that the outputs from classical algorithms are fast to compute for small problems, informative of (marginal) data structure, and their structure outputs as objects remain comparable across datasets. Our method achieves state-of-the-art performance on synthetic and realistic datasets, generalizes to data generating mechanisms not seen during training, and offers inference speeds that are orders of magnitude faster than existing models.
    
[^434]: 机器学习中的强健反事实解释：一项调查

    Robust Counterfactual Explanations in Machine Learning: A Survey

    [https://arxiv.org/abs/2402.01928](https://arxiv.org/abs/2402.01928)

    这项调查回顾了机器学习中强健反事实解释的研究，并分析了其对强健性的考虑。该调查讨论了现有解决方案及其局限性，为未来的发展提供了基础。

    

    反事实解释（CEs）被认为非常适合为受机器学习模型预测影响的对象提供算法上的补救措施。尽管CEs对受影响的个体有益，但最近的研究揭示了获取CEs的最新方法相关的严重问题。由于缺乏强健性可能会损害CEs的有效性，因此有必要采取技术来减轻这个风险。在这项调查中，我们回顾了强健CEs这一迅速发展的领域的研究，并对它们考虑的强健性形式进行了深入分析。我们还讨论了现有解决方案及其局限性，为未来的发展提供了坚实的基础。

    Counterfactual explanations (CEs) are advocated as being ideally suited to providing algorithmic recourse for subjects affected by the predictions of machine learning models. While CEs can be beneficial to affected individuals, recent work has exposed severe issues related to the robustness of state-of-the-art methods for obtaining CEs. Since a lack of robustness may compromise the validity of CEs, techniques to mitigate this risk are in order. In this survey, we review works in the rapidly growing area of robust CEs and perform an in-depth analysis of the forms of robustness they consider. We also discuss existing solutions and their limitations, providing a solid foundation for future developments.
    
[^435]: 从弱监督中学习的通用框架

    A General Framework for Learning from Weak Supervision

    [https://arxiv.org/abs/2402.01922](https://arxiv.org/abs/2402.01922)

    本文介绍了一个通用框架，利用新算法从各种弱监督中学习，通过使用非确定性有限自动机和前向-后向算法来简化计算要求，并将时间复杂度降低到线性尺度。

    

    弱监督学习通常面临着适用于具有多样化弱监督的各种场景和由于现有算法的复杂性而导致的可扩展性挑战，从而阻碍了实际部署。本文介绍了一个利用一种新算法来从弱监督中学习的通用框架（GLWS）。GLWS的核心是一个期望最大化（EM）的公式，灵活地适应了各种弱监督来源，包括实例的部分标签、聚合统计、成对观察和无标注数据。此外，我们还提供了一个先进的算法，使用非确定性有限自动机（NFA）以及前向-后向算法，显著简化了EM计算的需求，从而将时间复杂度从现有解决方案中通常所需的二次或阶乘复杂度降低到线性尺度。因此，从任意弱监督中学习的问题转化为了对它们进行NFA建模。GLWS不仅可以增强+

    Weakly supervised learning generally faces challenges in applicability to various scenarios with diverse weak supervision and in scalability due to the complexity of existing algorithms, thereby hindering the practical deployment. This paper introduces a general framework for learning from weak supervision (GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization (EM) formulation, adeptly accommodating various weak supervision sources, including instance partial labels, aggregate statistics, pairwise observations, and unlabeled data. We further present an advanced algorithm that significantly simplifies the EM computational demands using a Non-deterministic Finite Automaton (NFA) along with a forward-backward algorithm, which effectively reduces time complexity from quadratic or factorial often required in existing solutions to linear scale. The problem of learning from arbitrary weak supervision is therefore converted to the NFA modeling of them. GLWS not only enha
    
[^436]: 对奖励模型学习的偏好污染攻击

    Preference Poisoning Attacks on Reward Model Learning

    [https://arxiv.org/abs/2402.01920](https://arxiv.org/abs/2402.01920)

    对于从偏好比较中学习奖励模型的方法存在偏好污染攻击的漏洞，攻击者可以通过翻转少量偏好比较来对目标结果进行操纵。我们提出了两类算法方法，并证明了这些攻击在实施恶意行为方面的有效性。

    

    从两两比较中学习效用或奖励模型是许多应用领域的基础组成部分。这些方法从本质上需要从人们那里收集偏好信息，而反馈通常是匿名提供的。由于偏好是主观的，没有可以比较的黄金标准；然而，对偏好学习的高影响系统的依赖性为恶意行为者倾向于扭曲以达到其目的而采集的数据创造了强烈的动机。我们通过考虑一种威胁模型系统地调查了这种漏洞的性质和程度，其中攻击者可以翻转少量偏好比较，以促进或贬低目标结果。首先，我们提出了两类用于这些攻击的算法方法：基于原则的梯度框架和几种变种的按距离排名的方法。接下来，我们展示了这两类最佳攻击在成功实施恶意行为方面的效果。

    Learning utility, or reward, models from pairwise comparisons is a fundamental component in a number of application domains. These approaches inherently entail collecting preference information from people, with feedback often provided anonymously. Since preferences are subjective, there is no gold standard to compare against; yet, reliance of high-impact systems on preference learning creates a strong motivation for malicious actors to skew data collected in this fashion to their ends. We investigate the nature and extent of this vulnerability systematically by considering a threat model in which an attacker can flip a small subset of preference comparisons with the goal of either promoting or demoting a target outcome. First, we propose two classes of algorithmic approaches for these attacks: a principled gradient-based framework, and several variants of rank-by-distance methods. Next, we demonstrate the efficacy of best attacks in both these classes in successfully achieving malicio
    
[^437]: CoLe和LYS在BioASQ MESINESP8任务中的表征分配的相似度方法

    CoLe and LYS at BioASQ MESINESP8 Task: similarity based descriptor assignment in Spanish

    [https://arxiv.org/abs/2402.01916](https://arxiv.org/abs/2402.01916)

    这篇论文介绍了基于相似度的描述符分配方法在BioASQ MESINESP8任务中的应用，通过使用传统的信息检索工具，并提出了适用于西班牙语等语言的方法。

    

    在本文中，我们描述了我们参与了BioASQ生物医学语义指标挑战赛的MESINESP任务。参与的系统仅使用了传统的信息检索工具。我们评估了从IBECS/LILACS文档中提取索引术语的多种方法，以便存储在Apache Lucene索引中。这些索引表示使用要注释的文章内容进行查询，并从检索到的文档创建一个候选标签的排序列表。我们还评估了一种有限的标签全集方法，该方法通过将具有高共现得分的DeCS标签配对并创建元标签，以及一种基于标签个人资料匹配的替代方法。在官方运行中获得的结果似乎证实了这种方法在西班牙语等语言中的适用性。

    In this paper, we describe our participation in the MESINESP Task of the BioASQ biomedical semantic indexing challenge. The participating system follows an approach based solely on conventional information retrieval tools. We have evaluated various alternatives for extracting index terms from IBECS/LILACS documents in order to be stored in an Apache Lucene index. Those indexed representations are queried using the contents of the article to be annotated and a ranked list of candidate labels is created from the retrieved documents. We also have evaluated a sort of limited Label Powerset approach which creates meta-labels joining pairs of DeCS labels with high co-occurrence scores, and an alternative method based on label profile matching. Results obtained in official runs seem to confirm the suitability of this approach for languages like Spanish.
    
[^438]: 从PEFT到DEFT：用于减少变压器中激活密度的参数高效微调

    From PEFT to DEFT: Parameter Efficient Finetuning for Reducing Activation Density in Transformers

    [https://arxiv.org/abs/2402.01911](https://arxiv.org/abs/2402.01911)

    本论文提出了一种用于减少变压器模型中激活密度的参数高效微调方法 DEFT。研究发现预训练模型中存在激活稀疏性，并通过引入新的密度损失来促进更高的激活稀疏性。通过应用主流的PEFT技术，包括QLoRA、LoRA、Adapter、Prompt/Prefix Tuning，实验证明了该方法在不同下游任务中的有效性。

    

    预训练语言模型（PLMs）已成为下游任务微调的事实上的起点。然而，随着模型规模的增加，传统的全参数微调变得困难。为了解决这个问题，参数高效微调（PEFT）方法作为有效适应PLMs的手段而变得流行。与此同时，最近的研究揭示了变压器中多层感知（MLP）模块的中间输出中存在的激活稀疏性。低激活密度能够在支持稀疏感知硬件上实现高效模型推断。基于这一观察，我们在工作中提出了一种新的密度损失，鼓励预训练模型中更高的激活稀疏性（等价于更低的激活密度）。我们通过利用包括QLoRA、LoRA、Adapter、Prompt/Prefix Tuning在内的主流PEFT技术，展示了我们方法的有效性，以促进在多样的下游任务中实现高效的模型适应。

    Pretrained Language Models (PLMs) have become the de facto starting point for fine-tuning on downstream tasks. However, as model sizes continue to increase, traditional fine-tuning of all parameters becomes challenging. To address this, parameter-efficient fine-tuning (PEFT) methods have gained popularity as a means to adapt PLMs effectively. In parallel, recent studies have revealed the presence of activation sparsity within the intermediate outputs of the multilayer perception (MLP) blocks in transformers. Low activation density enables efficient model inference on sparsity-aware hardware. Building upon this insight, in this work, we propose a novel density loss that encourages higher activation sparsity (equivalently, lower activation density) in the pre-trained models. We demonstrate the effectiveness of our approach by utilizing mainstream PEFT techniques including QLoRA, LoRA, Adapter, Prompt/Prefix Tuning to facilitate efficient model adaptation across diverse downstream tasks. 
    
[^439]: 关于大型基础模型的灾难性继承问题

    On Catastrophic Inheritance of Large Foundation Models

    [https://arxiv.org/abs/2402.01909](https://arxiv.org/abs/2402.01909)

    这篇论文讨论了大型基础模型（LFMs）中的灾难性继承问题，指出了从有偏见的大规模预训练数据到LFMs在下游任务中的行为的弱点和限制。我们提出了UIM框架，旨在理解LFMs的灾难性继承问题，并解释其中的含义。

    

    大型基础模型（LFMs）声称具有惊人的性能，然而人们对它们在机器学习以及其他各个学科中的神秘和难以解释的潜力提出了极大关切。在这篇立场论文中，我们提出了一个被忽视的问题，即LFMs中根深蒂固的灾难性继承问题，描述了从有偏见的大规模预训练数据到LFMs在下游任务中的行为的弱点和限制，包括受损、长尾、有噪音、超出分布等样本。这种继承可能对下游应用产生灾难性影响，如偏见、缺乏泛化能力、性能下降、安全漏洞、隐私泄露和价值误差。我们讨论了这个问题背后的挑战，并提出了UIM框架，来理解LFMs的灾难性继承问题，包括来自预训练和下游适应的继承内容的解释。

    Large foundation models (LFMs) are claiming incredible performances. Yet great concerns have been raised about their mythic and uninterpreted potentials not only in machine learning, but also in various other disciplines. In this position paper, we propose to identify a neglected issue deeply rooted in LFMs: Catastrophic Inheritance, describing the weaknesses and limitations inherited from biased large-scale pre-training data to behaviors of LFMs on the downstream tasks, including samples that are corrupted, long-tailed, noisy, out-of-distributed, to name a few. Such inheritance can potentially cause catastrophes to downstream applications, such as bias, lack of generalization, deteriorated performance, security vulnerability, privacy leakage, and value misalignment. We discuss the challenges behind this issue and propose UIM, a framework to Understand the catastrophic inheritance of LFMs from both pre-training and downstream adaptation, Interpret the implications of catastrophic inher
    
[^440]: EBV: 基于原理挖掘和预测蜜蜂时间序列的电子蜜蜂兽医

    EBV: Electronic Bee-Veterinarian for Principled Mining and Forecasting of Honeybee Time Series

    [https://arxiv.org/abs/2402.01902](https://arxiv.org/abs/2402.01902)

    EBV是一种基于原理、有效、可解释和可扩展的方法，用于挖掘和预测蜜蜂的温度时间序列数据。

    

    蜜蜂对传粉和食物生产至关重要。在诸多因素中，极端温度（例如气候变化）对蜜蜂健康特别危险。预测这种极端情况将使养蜂人可以采取早期预防措施。因此，给定来自蜂箱的传感器（温度）时间序列数据，我们如何找到模式并进行预测？预测对于发现意外行为并向养蜂人员发出警告至关重要。在这种情况下，什么是正确的预测模型？ARIMA、RNNs还是其他一些方法？我们提出了EBV（电子蜜蜂兽医）方法，具有以下有益特性：(i) 基于原理：它基于物理学中的扩散方程和控制理论中的反馈环控制器；(ii) 有效：在多个真实世界的时间序列上运行良好；(iii) 可解释：它只需要一些养蜂人员可以轻松理解和信任的参数（例如蜜蜂强度）；(iv) 可扩展：它可以适应大规模数据处理。

    Honeybees are vital for pollination and food production. Among many factors, extreme temperature (e.g., due to climate change) is particularly dangerous for bee health. Anticipating such extremities would allow beekeepers to take early preventive action. Thus, given sensor (temperature) time series data from beehives, how can we find patterns and do forecasting? Forecasting is crucial as it helps spot unexpected behavior and thus issue warnings to the beekeepers. In that case, what are the right models for forecasting? ARIMA, RNNs, or something else?   We propose the EBV (Electronic Bee-Veterinarian) method, which has the following desirable properties: (i) principled: it is based on a) diffusion equations from physics and b) control theory for feedback-loop controllers; (ii) effective: it works well on multiple, real-world time sequences, (iii) explainable: it needs only a handful of parameters (e.g., bee strength) that beekeepers can easily understand and trust, and (iv) scalable: it
    
[^441]: 使用Bellman残差最小化的分布式离线策略评估

    Distributional Off-policy Evaluation with Bellman Residual Minimization

    [https://arxiv.org/abs/2402.01900](https://arxiv.org/abs/2402.01900)

    这篇论文研究了使用Bellman残差最小化的方法来解决分布式离线策略评估问题，并提出了一种称为能量Bellman残差最小化（EBRM）的方法来估计返回分布。在可实现性假设下，建立了EBRM估计器的有限样本误差界。

    

    我们考虑分布式离线策略评估的问题，它是许多分布式强化学习（DRL）算法的基础。与大多数现有的方法（依赖于最大值-扩展的统计距离，如最大值Wasserstein距离）不同，我们研究用于量化分布式Bellman残差的期望-扩展的统计距离，并且证明它可以上界估计返回分布的期望误差。基于这个有吸引力的性质，通过将Bellman残差最小化框架推广到DRL，我们提出了一种称为能量Bellman残差最小化（EBRM）的方法来估计返回分布。我们在可实现性假设下建立了EBRM估计器的有限样本误差界。此外，我们引入了一种基于多步引导过程的方法的变体，以实现多步扩展。通过选择适当的步长，我们获得了更好的误差界。

    We consider the problem of distributional off-policy evaluation which serves as the foundation of many distributional reinforcement learning (DRL) algorithms. In contrast to most existing works (that rely on supremum-extended statistical distances such as supremum-Wasserstein distance), we study the expectation-extended statistical distance for quantifying the distributional Bellman residuals and show that it can upper bound the expected error of estimating the return distribution. Based on this appealing property, by extending the framework of Bellman residual minimization to DRL, we propose a method called Energy Bellman Residual Minimizer (EBRM) to estimate the return distribution. We establish a finite-sample error bound for the EBRM estimator under the realizability assumption. Furthermore, we introduce a variant of our method based on a multi-step bootstrapping procedure to enable multi-step extension. By selecting an appropriate step level, we obtain a better error bound for thi
    
[^442]: 基于基础模型的神经符号学习和推理的作用

    The Role of Foundation Models in Neuro-Symbolic Learning and Reasoning

    [https://arxiv.org/abs/2402.01889](https://arxiv.org/abs/2402.01889)

    基于基础模型的神经符号学习和推理在NeSy任务中表现出更好的性能，并且减少了标注和手动工程的工作量。

    

    神经符号人工智能（NeSy）有望确保AI系统的安全部署，因为可解释的符号技术提供了正式的行为保证。挑战在于如何有效地整合神经和符号计算，以便从原始数据中进行学习和推理。现有的顺序训练神经和符号组件的流水线需要大量标注，而端到端方法在符号基础问题的组合爆炸方面具有可扩展性的限制。在本文中，我们利用基础模型中的隐性知识来增强NeSy任务的性能，同时减少数据标注和人工工程量。我们引入了一种新的架构，称为NeSyGPT，它通过微调视觉-语言基础模型来从原始数据中提取符号特征，然后学习一个高度表达的答案集程序来解决下游任务。我们的综合评估表明，NeSyGPT具有... (剩余部分请自行翻译)

    Neuro-Symbolic AI (NeSy) holds promise to ensure the safe deployment of AI systems, as interpretable symbolic techniques provide formal behaviour guarantees. The challenge is how to effectively integrate neural and symbolic computation, to enable learning and reasoning from raw data. Existing pipelines that train the neural and symbolic components sequentially require extensive labelling, whereas end-to-end approaches are limited in terms of scalability, due to the combinatorial explosion in the symbol grounding problem. In this paper, we leverage the implicit knowledge within foundation models to enhance the performance in NeSy tasks, whilst reducing the amount of data labelling and manual engineering. We introduce a new architecture, called NeSyGPT, which fine-tunes a vision-language foundation model to extract symbolic features from raw data, before learning a highly expressive answer set program to solve a downstream task. Our comprehensive evaluation demonstrates that NeSyGPT has 
    
[^443]: 基于f-散度原理的领域自适应：一个改进的框架

    On f-Divergence Principled Domain Adaptation: An Improved Framework

    [https://arxiv.org/abs/2402.01887](https://arxiv.org/abs/2402.01887)

    本文改进了基于f-散度的无监督领域自适应（UDA）框架，引入了f-领域差异度量指标，并通过去除绝对值函数和引入缩放参数，提出了新的目标误差和样本复杂度界限，从而使得我们能够恢复以前的KL结果，将算法和理论之间的差距缩小，并通过定位技术开发了快速率的泛化界限。实验结果证明了基于f-DD的领域学习算法在流行的UDA基准测试中表现出了卓越的性能。

    

    无监督领域自适应（UDA）在解决机器学习中的分布偏移问题中起着至关重要的作用。在本文中，我们通过改进Acuna等人（2021年）提出的UDA的理论基础，对其基于f-散度的差异度进行了改进，并引入了一个新的度量指标，即f-领域差异（f-DD）。通过去除绝对值函数并引入一个缩放参数，f-DD产生了新的目标误差和样本复杂度界限，使我们能够恢复以前基于KL的结果，并弥合了Acuna等人（2021年）中提出的算法和理论之间的差距。利用定位技术，我们还开发了一种快速率的泛化界限。实证结果表明，在流行的UDA基准测试中，基于f-DD的领域学习算法表现出优越性能。

    Unsupervised domain adaptation (UDA) plays a crucial role in addressing distribution shifts in machine learning. In this work, we improve the theoretical foundations of UDA proposed by Acuna et al. (2021) by refining their f-divergence-based discrepancy and additionally introducing a new measure, f-domain discrepancy (f-DD). By removing the absolute value function and incorporating a scaling parameter, f-DD yields novel target error and sample complexity bounds, allowing us to recover previous KL-based results and bridging the gap between algorithms and theory presented in Acuna et al. (2021). Leveraging a localization technique, we also develop a fast-rate generalization bound. Empirical results demonstrate the superior performance of f-DD-based domain learning algorithms over previous works in popular UDA benchmarks.
    
[^444]: 通过估计演示者的专业知识的逆向强化学习

    Inverse Reinforcement Learning by Estimating Expertise of Demonstrators

    [https://arxiv.org/abs/2402.01886](https://arxiv.org/abs/2402.01886)

    本文介绍了一个新颖的框架，IRLEED，它通过估计演示者的专业知识来解决模仿学习中的次优和异质演示的问题。IRLEED通过结合演示者次优性的普适模型和最大熵IRL框架，有效地从多样的次优演示中得出最佳策略。

    

    在模仿学习中，利用次优和异质的演示提出了一个重大挑战，因为现实世界数据的性质各不相同。然而，标准的模仿学习算法将这些数据集视为同质的，从而继承了次优演示的缺陷。先前处理这个问题的方法通常依赖于不切实际的假设，如高质量的数据子集、置信度排名或明确的环境知识。本文介绍了IRLEED（通过估计演示者的专业知识的逆向强化学习），这是一个新颖的框架，能够克服这些障碍，而不需要先前对演示者专业知识进行了解。IRLEED通过将演示者次优性的普适模型与最大熵IRL框架相结合，来处理奖励偏差和行动方差，从而有效地从多样的次优演示中得出最优策略。在在线和离线实验中进行了验证。

    In Imitation Learning (IL), utilizing suboptimal and heterogeneous demonstrations presents a substantial challenge due to the varied nature of real-world data. However, standard IL algorithms consider these datasets as homogeneous, thereby inheriting the deficiencies of suboptimal demonstrators. Previous approaches to this issue typically rely on impractical assumptions like high-quality data subsets, confidence rankings, or explicit environmental knowledge. This paper introduces IRLEED, Inverse Reinforcement Learning by Estimating Expertise of Demonstrators, a novel framework that overcomes these hurdles without prior knowledge of demonstrator expertise. IRLEED enhances existing Inverse Reinforcement Learning (IRL) algorithms by combining a general model for demonstrator suboptimality to address reward bias and action variance, with a Maximum Entropy IRL framework to efficiently derive the optimal policy from diverse, suboptimal demonstrations. Experiments in both online and offline I
    
[^445]: 基于大规模语言模型的超参数优化的技术

    Large Language Model Agent for Hyper-Parameter Optimization

    [https://arxiv.org/abs/2402.01881](https://arxiv.org/abs/2402.01881)

    基于大规模语言模型的AgentHPO技术通过自动化超参数优化，在机器学习任务中大大减少了试验次数，简化了设置过程，提升了解释性和用户信任。

    

    超参数优化在现代机器学习中至关重要，需要专业知识、大量实验以及高计算和人力资源。尽管自动化机器学习（AutoML）取得了一些进展，但试验效率、设置复杂性和互操作性方面仍存在挑战。为了解决这些问题，我们引入了一种新的范式，利用大规模语言模型（LLMs）来自动化不同机器学习任务的超参数优化，称为AgentHPO（LLM Agent-based Hyperparameter Optimization）。具体来说，AgentHPO自主处理任务信息，根据历史试验对特定超参数（HPs）进行实验，并进行迭代优化。与传统的AutoML方法相比，这种类似人类的优化过程极大地减少了所需的试验次数，简化了设置过程，并提升了解释性和用户信任。

    Hyperparameter optimization is critical in modern machine learning, requiring expert knowledge, numerous trials, and high computational and human resources. Despite the advancements in Automated Machine Learning (AutoML), challenges in terms of trial efficiency, setup complexity, and interoperability still persist. To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks, which is named AgentHPO (short for LLM Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the task information autonomously, conducts experiments with specific hyperparameters (HPs), and iteratively optimizes them based on historical trials. This human-like optimization process largely reduces the number of required trials, simplifies the setup process, and enhances interpretability and user trust, compared to traditional AutoML methods. Extensive empirical experiments conducted o
    
[^446]: $\sigma$-zero: 基于梯度的$\ell_0$-范数对抗样本优化

    $\sigma$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial Examples

    [https://arxiv.org/abs/2402.01879](https://arxiv.org/abs/2402.01879)

    该论文提出了一种新的基于梯度的$\ell_0$范数攻击方法$\sigma$-zero，其利用了$\ell_0$范数的可微近似和自适应投影运算符，能够在非凸和非可微的约束下优化，从而评估深度网络对稀疏$\ell_0$范数攻击的鲁棒性。

    

    评估深度网络对基于梯度攻击的对抗鲁棒性是具有挑战性的。虽然大多数攻击考虑$\ell_2$和$\ell_\infty$范数约束来制造输入扰动，但只有少数研究了稀疏的$\ell_1$和$\ell_0$范数攻击。特别是，由于在非凸且非可微约束上进行优化的固有复杂性，$\ell_0$范数攻击是研究最少的。然而，使用这些攻击评估对抗鲁棒性可以揭示在更传统的$\ell_2$和$\ell_\infty$范数攻击中未能测试出的弱点。在这项工作中，我们提出了一种新颖的$\ell_0$范数攻击，称为$\sigma$-zero，它利用了$\ell_0$范数的一个特殊可微近似来促进基于梯度的优化，并利用自适应投影运算符动态调整损失最小化和扰动稀疏性之间的权衡。通过在MNIST、CIFAR10和ImageNet数据集上进行广泛评估，包括...

    Evaluating the adversarial robustness of deep networks to gradient-based attacks is challenging. While most attacks consider $\ell_2$- and $\ell_\infty$-norm constraints to craft input perturbations, only a few investigate sparse $\ell_1$- and $\ell_0$-norm attacks. In particular, $\ell_0$-norm attacks remain the least studied due to the inherent complexity of optimizing over a non-convex and non-differentiable constraint. However, evaluating adversarial robustness under these attacks could reveal weaknesses otherwise left untested with more conventional $\ell_2$- and $\ell_\infty$-norm attacks. In this work, we propose a novel $\ell_0$-norm attack, called $\sigma$-zero, which leverages an ad hoc differentiable approximation of the $\ell_0$ norm to facilitate gradient-based optimization, and an adaptive projection operator to dynamically adjust the trade-off between loss minimization and perturbation sparsity. Extensive evaluations using MNIST, CIFAR10, and ImageNet datasets, involving
    
[^447]: LiPO: 通过学习排序进行列表型偏好优化

    LiPO: Listwise Preference Optimization through Learning-to-Rank

    [https://arxiv.org/abs/2402.01878](https://arxiv.org/abs/2402.01878)

    本研究提出了一种名为LiPO的框架，用于将语言模型对齐问题定义为一个列表型排序问题。通过从排名列表中学习，该框架可以使策略更有效地学习到可行的响应。

    

    将语言模型与人工反馈进行对齐是控制其在实际应用中行为的关键。最近的一些策略优化方法，如DPO和SLiC，成为传统的来自人类反馈的增强学习方法的有希望的替代方案。实际上，人工反馈通常以对多个响应进行排序的格式提供，以摊销阅读提示的成本。多个响应也可以通过奖励模型或AI反馈进行排序。缺少关于直接适应响应列表的研究。在这项工作中，我们将语言模型对齐问题定义为一个列表型排序问题，并描述了列表型偏好优化（LiPO）框架，在给定提示的情况下，策略可以从一个排名列表中更有效地学习可行响应。这种观点与学习排序（LTR）形成明确的联系，其中大多数现有的偏好优化工作可以映射到现有的排名目标，特别是

    Aligning language models (LMs) with curated human feedback is critical to control their behaviors in real-world applications. Several recent policy optimization methods, such as DPO and SLiC, serve as promising alternatives to the traditional Reinforcement Learning from Human Feedback (RLHF) approach. In practice, human feedback often comes in a format of a ranked list over multiple responses to amortize the cost of reading prompt. Multiple responses can also be ranked by reward models or AI feedback. There lacks such a study on directly fitting upon a list of responses. In this work, we formulate the LM alignment as a listwise ranking problem and describe the Listwise Preference Optimization (LiPO) framework, where the policy can potentially learn more effectively from a ranked list of plausible responses given the prompt. This view draws an explicit connection to Learning-to-Rank (LTR), where most existing preference optimization work can be mapped to existing ranking objectives, esp
    
[^448]: 移动试衣间：基于扩散模型的设备内虚拟试穿

    Mobile Fitting Room: On-device Virtual Try-on via Diffusion Models

    [https://arxiv.org/abs/2402.01877](https://arxiv.org/abs/2402.01877)

    移动试衣间是第一个基于设备内扩散模型的虚拟试穿系统，解决了高质量服装放置和模型压缩等技术挑战，实现了隐私保护和用户定制。它为时尚电子商务企业提供了有价值的服务和用户友好的虚拟试穿体验。

    

    在日益增长的时尚电子商务领域，需要交互式和用户友好的界面来进行虚拟试穿衣服。传统的试穿方法在适应不同的背景、姿势和主体方面存在困难。虽然利用最近扩散模型的进展而产生了更高质量的图像生成，但移动界面交付的以人为中心的维度和隐私问题大部分仍未被探索。我们提出了移动试衣间，这是第一个基于设备内扩散的虚拟试穿系统。为了解决多个相互关联的技术挑战，如高质量服装放置和移动设备的模型压缩，我们提出了一种新颖的技术流程和界面设计，实现隐私保护和用户定制。一个使用场景突显了我们的工具如何为顾客提供无缝、互动的虚拟试穿体验，并为时尚电子商务企业提供有价值的服务。

    The growing digital landscape of fashion e-commerce calls for interactive and user-friendly interfaces for virtually trying on clothes. Traditional try-on methods grapple with challenges in adapting to diverse backgrounds, poses, and subjects. While newer methods, utilizing the recent advances of diffusion models, have achieved higher-quality image generation, the human-centered dimensions of mobile interface delivery and privacy concerns remain largely unexplored. We present Mobile Fitting Room, the first on-device diffusion-based virtual try-on system. To address multiple inter-related technical challenges such as high-quality garment placement and model compression for mobile devices, we present a novel technical pipeline and an interface design that enables privacy preservation and user customization. A usage scenario highlights how our tool can provide a seamless, interactive virtual try-on experience for customers and provide a valuable service for fashion e-commerce businesses.
    
[^449]: 集合就是你所需要的：用于HL-LHC的超快速喷注分类在FPGAs上的研究

    Sets are all you need: Ultrafast jet classification on FPGAs for HL-LHC

    [https://arxiv.org/abs/2402.01876](https://arxiv.org/abs/2402.01876)

    该论文研究了在FPGA上进行高性能喷注分类的机器学习算法，通过量化感知的训练和高效的硬件实现，实现了低延迟和低资源消耗，为高亮度阶段标记的模型提供了初始设计。

    

    我们研究了各种基于机器学习的算法，用于在可编程逻辑门阵列上进行准确的喷注风味分类，并展示了延迟和资源消耗如何随着输入大小和算法选择而变化。这些架构为在CERN LHC的高亮度阶段标记所能使用的模型提供了一个初始设计。高亮度升级将导致质子-质子碰撞的瞬时亮度增加五倍，进而产生更高的数据量和复杂性，例如喷注组成部分的可用性。通过量化感知的训练和高效的硬件实现，我们展示了复杂架构（如深度集合和交互网络）的推断时间可以达到O（100）ns，并且计算资源成本较低。

    We study various machine learning based algorithms for performing accurate jet flavor classification on field-programmable gate arrays and demonstrate how latency and resource consumption scale with the input size and choice of algorithm. These architectures provide an initial design for models that could be used for tagging at the CERN LHC during its high-luminosity phase. The high-luminosity upgrade will lead to a five-fold increase in its instantaneous luminosity for proton-proton collisions and, in turn, higher data volume and complexity, such as the availability of jet constituents. Through quantization-aware training and efficient hardware implementations, we show that O(100) ns inference of complex architectures such as deep sets and interaction networks is feasible at a low computational resource cost.
    
[^450]: RL/LLM分类树：回顾强化学习和大语言模型之间的协同关系

    The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models

    [https://arxiv.org/abs/2402.01874](https://arxiv.org/abs/2402.01874)

    这项工作回顾了将强化学习和大语言模型结合起来的研究，并提出了一个新的分类方法，以审视这两个领域之间的协同关系。

    

    在这项工作中，我们回顾了将强化学习（RL）和大语言模型（LLM）结合起来的研究，并提出了一种新的三类分类方法，该分类方法基于这两种模型类型之间的交互方式。第一类是RL4LLM，包括利用RL改进与自然语言处理相关任务上LLM性能的研究。L4LLM分为两个子类，取决于RL是直接微调现有LLM还是改进LLM的提示。在第二类LLM4RL中，LLM辅助训练一个与自然语言无关的RL模型。我们进一步根据LLM辅助或替代RL训练框架的组件（奖励塑造、目标生成和策略函数）对LLM4RL进行了细分。最后，在第三类RL+LLM中，一个LLM和一个RL代理被嵌入其中。

    In this work, we review research studies that combine Reinforcement Learning (RL) and Large Language Models (LLMs), two areas that owe their momentum to the development of deep neural networks. We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other. The first class, RL4LLM, includes studies where RL is leveraged to improve the performance of LLMs on tasks related to Natural Language Processing. L4LLM is divided into two sub-categories depending on whether RL is used to directly fine-tune an existing LLM or to improve the prompt of the LLM. In the second class, LLM4RL, an LLM assists the training of an RL model that performs a task that is not inherently related to natural language. We further break down LLM4RL based on the component of the RL training framework that the LLM assists or replaces, namely reward shaping, goal generation, and policy function. Finally, in the third class, RL+LLM, an LLM and an RL agent are embedde
    
[^451]: APIServe: 高效支持大型语言模型推理的API工具

    APIServe: Efficient API Support for Large-Language Model Inferencing

    [https://arxiv.org/abs/2402.01869](https://arxiv.org/abs/2402.01869)

    APIServe是针对API增强的大型语言模型推理的一个高效工具，它最大限度地减少了由 API 调用引起的 GPU 资源浪费，并提高了整体服务吞吐量。

    

    大型语言模型越来越多地与外部工具和API集成，如ChatGPT插件，以扩展其能力以外的语言中心任务。然而，当前的LLM推理系统是为独立的LLM设计的。它们将API调用视为新请求，导致不必要的重新计算已经计算过的上下文，这占了总模型前向时间的37-40%。本文提出了APIServe，这是针对API增强的LLM推理框架。APIServe最大限度地减少了由API调用引起的GPU资源浪费，并将节省的内存用于服务更多的请求。与现有的LLM推理系统相比，APIServe将整体服务吞吐量提升了1.6倍，每秒完成的请求增加了2倍。

    Large language models are increasingly integrated with external tools and APIs like ChatGPT plugins to extend their capability beyond language-centric tasks. However, today's LLM inference systems are designed for standalone LLMs. They treat API calls as new requests, causing unnecessary recomputation of already computed contexts, which accounts for 37-40% of total model forwarding time. This paper presents APIServe, the first LLM inference framework targeting API-augmented LLMs. APISERVE minimizes the GPU resource waste caused by API calls and dedicates saved memory for serving more requests. APISERVE improves the overall serving throughput by 1.6x and completes 2x more requests per second compared to the state-of-the-art LLM inference systems.
    
[^452]: 训练PINNs的挑战：从损失函数空间角度探究

    Challenges in Training PINNs: A Loss Landscape Perspective

    [https://arxiv.org/abs/2402.01868](https://arxiv.org/abs/2402.01868)

    本文探讨了训练PINNs的挑战，强调了损失函数空间在训练过程中的作用，引入了新颖的二阶优化器NNCG并优化了PINN性能，为训练PINNs提供了有价值的洞见和更强大的优化策略。

    

    本文通过研究物理信息神经网络（PINNs）的训练挑战，强调了损失函数空间在训练过程中的作用。我们分析了在最小化PINN损失函数方面的困难，特别是由于残差项中的微分算子引起的病态条件。我们比较了基于梯度的优化器Adam、L-BFGS以及它们的组合Adam+L-BFGS的性能，表明Adam+L-BFGS更优，并介绍了一种新颖的二阶优化器NysNewton-CG（NNCG），显著提高了PINN的性能。从理论上，我们阐明了病态微分算子与PINN损失中的病态条件之间的联系，并展示了结合一阶和二阶优化方法的好处。我们的工作为训练PINNs提供了有价值的洞见和更强大的优化策略，可以提高PINNs在解决困难的偏微分方程中的实用性。

    This paper explores challenges in training Physics-Informed Neural Networks (PINNs), emphasizing the role of the loss landscape in the training process. We examine difficulties in minimizing the PINN loss function, particularly due to ill-conditioning caused by differential operators in the residual term. We compare gradient-based optimizers Adam, L-BFGS, and their combination Adam+L-BFGS, showing the superiority of Adam+L-BFGS, and introduce a novel second-order optimizer, NysNewton-CG (NNCG), which significantly improves PINN performance. Theoretically, our work elucidates the connection between ill-conditioned differential operators and ill-conditioning in the PINN loss and shows the benefits of combining first- and second-order optimization methods. Our work presents valuable insights and more powerful optimization strategies for training PINNs, which could improve the utility of PINNs for solving difficult partial differential equations.
    
[^453]: 利用大型语言模型在提示式弱监督中进行结构学习

    Leveraging Large Language Models for Structure Learning in Prompted Weak Supervision

    [https://arxiv.org/abs/2402.01867](https://arxiv.org/abs/2402.01867)

    这项工作利用大型语言模型在提示式弱监督中学习标注函数之间的统计依赖结构，通过结构细化模块提高了提示式弱监督流程的效果。

    

    提示式弱监督（PromptedWS）将预训练的大型语言模型（LLMs）应用于弱监督框架中的标注函数（LFs），以获取大规模标记数据集。我们进一步扩展了LLMs在循环中的使用，以解决弱监督中的一个关键挑战：学习监督源之间的统计依赖结构。在这项工作中，我们询问LLM这些提示式LFs有多相似。我们提出了一个结构细化模块，它是一种基于提示相似性的简单而有效的方法，利用嵌入空间中的内在结构。结构细化模块的核心是标注函数移除（LaRe）和相关结构生成（CosGen）。与之前从弱标签中学习依赖关系的方法相比，我们的方法找到了对LFs内在且不太依赖数据的依赖关系。我们展示了我们的结构细化模块如何改进提示式弱监督流程。

    Prompted weak supervision (PromptedWS) applies pre-trained large language models (LLMs) as the basis for labeling functions (LFs) in a weak supervision framework to obtain large labeled datasets. We further extend the use of LLMs in the loop to address one of the key challenges in weak supervision: learning the statistical dependency structure among supervision sources. In this work, we ask the LLM how similar are these prompted LFs. We propose a Structure Refining Module, a simple yet effective first approach based on the similarities of the prompts by taking advantage of the intrinsic structure in the embedding space. At the core of Structure Refining Module are Labeling Function Removal (LaRe) and Correlation Structure Generation (CosGen). Compared to previous methods that learn the dependencies from weak labels, our method finds the dependencies which are intrinsic to the LFs and less dependent on the data. We show that our Structure Refining Module improves the PromptedWS pipeline
    
[^454]: 我的模型会忘记什么？语言模型改进中的被遗忘实例预测

    What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement

    [https://arxiv.org/abs/2402.01865](https://arxiv.org/abs/2402.01865)

    本文研究了语言模型更新中的遗忘现象，提出了一种预测上游实例遗忘的方法，以改进重播过程的可控性和解释性。根据预训练实例的预-softmax对数几率分数变化与在线学习实例的相似性，提出了一种部分可解释的预测模型，在BART模型上表现良好但在T5模型上失败。此外，还展示了基于内积的黑盒分类器。

    

    在实际应用中，语言模型会出现错误。然而，仅仅通过将模型更新为纠正错误实例，会导致灾难性的遗忘，更新后的模型在指导微调或上游训练阶段中学到的实例上出现错误。随机重播上游数据的效果不令人满意，往往伴随着较高的方差和较差的可控性。为了改善重播过程的可控性和解释性，我们试图预测由于模型更新而遗忘的上游实例。我们根据一组在线学习的实例和相应被遗忘的上游预训练实例训练预测模型。我们提出了一种部分可解释的预测模型，该模型基于这样的观察结果：预训练实例的预-softmax对数几率分数的变化类似于在线学习实例的变化，这在BART模型上表现出不错的效果，但在T5模型上失败。我们进一步展示了基于内积的黑盒分类器

    Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase. Randomly replaying upstream data yields unsatisfactory performance and often comes with high variance and poor controllability. To this end, we try to forecast upstream examples that will be forgotten due to a model update for improved controllability of the replay process and interpretability. We train forecasting models given a collection of online learned examples and corresponding forgotten upstream pre-training examples. We propose a partially interpretable forecasting model based on the observation that changes in pre-softmax logit scores of pretraining examples resemble that of online learned examples, which performs decently on BART but fails on T5 models. We further show a black-box classifier based on inner products 
    
[^455]: DFML：分散式联邦互联学习

    DFML: Decentralized Federated Mutual Learning

    [https://arxiv.org/abs/2402.01863](https://arxiv.org/abs/2402.01863)

    DFML是一个无服务器的分散式联邦互联学习框架，能够有效地处理模型和数据的异质性，并通过相互学习在客户端之间传授知识，以获得更快的收敛速度和更高的全局准确性。

    

    在现实设备领域中，联邦学习（FL）中的集中式服务器存在通信瓶颈和容易受到单点故障的挑战。此外，现有设备固有地表现出模型和数据的异质性。现有工作缺乏一个能够适应此异质性且不施加架构限制或假定公共数据可用的分散式FL（DFL）框架。为了解决这些问题，我们提出了一个分散式联邦互联学习（DFML）框架，该框架是无服务器的，支持非限制性的异构模型，并避免依赖公共数据。DFML通过相互学习在客户端之间传授知识，并循环改变监督和提取信号的数量来有效处理模型和数据的异质性。广泛的实验结果表明，DFML在收敛速度和全局准确性方面具有一致的有效性，优于普遍存在的方法。

    In the realm of real-world devices, centralized servers in Federated Learning (FL) present challenges including communication bottlenecks and susceptibility to a single point of failure. Additionally, contemporary devices inherently exhibit model and data heterogeneity. Existing work lacks a Decentralized FL (DFL) framework capable of accommodating such heterogeneity without imposing architectural restrictions or assuming the availability of public data. To address these issues, we propose a Decentralized Federated Mutual Learning (DFML) framework that is serverless, supports nonrestrictive heterogeneous models, and avoids reliance on public data. DFML effectively handles model and data heterogeneity through mutual learning, which distills knowledge between clients, and cyclically varying the amount of supervision and distillation signals. Extensive experimental results demonstrate consistent effectiveness of DFML in both convergence speed and global accuracy, outperforming prevalent b
    
[^456]: 参数特征迁移：一次性联邦学习与基础模型

    Parametric Feature Transfer: One-shot Federated Learning with Foundation Models

    [https://arxiv.org/abs/2402.01862](https://arxiv.org/abs/2402.01862)

    FedPFT 使用参数特征迁移提高了一次性联邦学习的准确性和通信效率，并在不同数据异质性设置中显示出改进效果。

    

    在一次性联邦学习中，客户端在一轮通信中共同训练一个全局模型。现有的一次性联邦学习方法在增强通信效率的同时损失了准确性。本文介绍了FedPFT（带参数特征迁移的联邦学习），这是一种利用基础模型的可迁移性来提高一次性联邦学习的准确性和通信效率的方法。该方法涉及从基础模型中提取的每个客户端参数模型（具体来说是高斯混合模型）的特征进行迁移。随后，每个参数模型被用来生成用于训练分类器头的合成特征。在八个数据集上的实验结果表明，FedPFT在集中和分散的联邦学习场景中以及在协变量转移和任务转移等多样的数据异质性设置中，增强了通信-准确性的边界，改进了最高达20.6%。

    In one-shot federated learning (FL), clients collaboratively train a global model in a single round of communication. Existing approaches for one-shot FL enhance communication efficiency at the expense of diminished accuracy. This paper introduces FedPFT (Federated Learning with Parametric Feature Transfer), a methodology that harnesses the transferability of foundation models to enhance both accuracy and communication efficiency in one-shot FL. The approach involves transferring per-client parametric models (specifically, Gaussian mixtures) of features extracted from foundation models. Subsequently, each parametric model is employed to generate synthetic features for training a classifier head. Experimental results on eight datasets demonstrate that FedPFT enhances the communication-accuracy frontier in both centralized and decentralized FL scenarios, as well as across diverse data-heterogeneity settings such as covariate shift and task shift, with improvements of up to 20.6%. Additio
    
[^457]: 使用大型多模态模型解释生成模型的潜在表示

    Explaining latent representations of generative models with large multimodal models

    [https://arxiv.org/abs/2402.01858](https://arxiv.org/abs/2402.01858)

    该研究提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。对于不同生成模型的解释，我们量化了解释的不确定性，通过多模态模型进行了性能评估，且定性地展示了潜在因素的变化效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。

    

    学习可解释的数据生成潜在因素表示是人工智能发展的重要课题。随着大型多模态模型的兴起，它可以将图像与文本对齐以生成答案。在本研究中，我们提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。我们进一步量化评估了我们生成的解释的不确定性，在多个大型多模态模型之间评估了解释生成的性能，并定性地可视化了每个潜在因素的变化，以学习不同生成模型对解释的解缠效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。

    Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.
    
[^458]: 评估基于Foundation模型集成联邦学习的鲁棒性、隐私和公平性的立场论文

    Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models

    [https://arxiv.org/abs/2402.01857](https://arxiv.org/abs/2402.01857)

    本文评估了基于Foundation模型集成联邦学习中鲁棒性、隐私和公平性的挑战和问题，并提出了应对策略和研究方向。

    

    联邦学习（FL）是分散式机器学习的重大突破，但面临诸多挑战，如数据可用性有限和计算资源的变化性，这可能会限制模型的性能和可伸缩性。将Foundation模型（FM）集成到FL中，可以解决这些问题，通过预训练和数据增强增加数据丰富性并减少计算需求。然而，这种集成引入了鲁棒性、隐私和公平性方面的新问题，在现有研究中尚未得到充分解决。我们通过系统评估FM-FL集成对这些方面的影响，进行了初步调查。我们分析了其中的权衡取舍，揭示了该集成引入的威胁和问题，并提出了一套用于应对这些挑战的标准和策略。此外，我们还鉴定了可能解决这些问题的一些前景方向和研究方向。

    Federated Learning (FL), while a breakthrough in decentralized machine learning, contends with significant challenges such as limited data availability and the variability of computational resources, which can stifle the performance and scalability of the models. The integration of Foundation Models (FMs) into FL presents a compelling solution to these issues, with the potential to enhance data richness and reduce computational demands through pre-training and data augmentation. However, this incorporation introduces novel issues in terms of robustness, privacy, and fairness, which have not been sufficiently addressed in the existing research. We make a preliminary investigation into this field by systematically evaluating the implications of FM-FL integration across these dimensions. We analyze the trade-offs involved, uncover the threats and issues introduced by this integration, and propose a set of criteria and strategies for navigating these challenges. Furthermore, we identify po
    
[^459]: SPDE先验在端到端神经数据同化方案的不确定性量化中的应用

    SPDE priors for uncertainty quantification of end-to-end neural data assimilation schemes

    [https://arxiv.org/abs/2402.01855](https://arxiv.org/abs/2402.01855)

    SPDE先验在最优插值中的应用及其与神经网络的联合学习问题，为大规模地球物理数据集的时空插值提供了一种新的方法。

    

    大规模地球物理数据集的时空插值通常通过最优插值(Optimal Interpolation，OI)和更复杂的基于模型或数据驱动的数据同化技术来处理。在过去的十年中，随机偏微分方程(Spatio-temporal Partial Differential Equations，SPDE)和高斯马尔科夫随机场(Gaussian Markov Random Fields，GMRF)之间的联系开辟了一条新的途径，用于处理最优插值中的大数据集和物理诱导协方差矩阵。深度学习社区的最新进展也使得可以将这个问题视为嵌入数据同化变分框架的神经网络体系结构的联合学习问题。重建任务被视为一个包含在变分内部成本中的先验学习问题和后者的基于梯度的最小化：先验模型和求解器都被表示为具有自动微分的神经网络，可以通过最小化损失函数来训练，该损失函数通常被表示为一些真实值和重建值之间的均方误差。

    The spatio-temporal interpolation of large geophysical datasets has historically been adressed by Optimal Interpolation (OI) and more sophisticated model-based or data-driven DA techniques. In the last ten years, the link established between Stochastic Partial Differential Equations (SPDE) and Gaussian Markov Random Fields (GMRF) opened a new way of handling both large datasets and physically-induced covariance matrix in Optimal Interpolation. Recent advances in the deep learning community also enables to adress this problem as neural architecture embedding data assimilation variational framework. The reconstruction task is seen as a joint learning problem of the prior involved in the variational inner cost and the gradient-based minimization of the latter: both prior models and solvers are stated as neural networks with automatic differentiation which can be trained by minimizing a loss function, typically stated as the mean squared error between some ground truth and the reconstructi
    
[^460]: 通过偏好学习将废物收集规划专家知识融入适应性函数中

    Capturing waste collection planning expert knowledge in a fitness function through preference learning

    [https://arxiv.org/abs/2402.01849](https://arxiv.org/abs/2402.01849)

    本文提出了一种通过偏好学习将废物收集规划专家知识融入适应性函数的方法，通过制定关键绩效指标和偏好判断，实现了全局优化的废物收集流程。

    

    本文针对COGERSA废物收集流程进行研究。目前，专家们通过试错机制手动设计该流程。但该流程并不是全局优化的，因为它是根据委员会需求逐步和局部地构建的。规划优化算法通常可以解决这个问题，但需要一个适应性函数来评估路线规划的质量。然而，由于流程的复杂性，即使专家也无法直接提出一个适应性函数。因此，本文的目标是通过偏好框架建立一个适应性函数，利用现有的专家知识和专业能力。根据专家的意见，精心确定了几个关键绩效指标和偏好判断，以学习一个有前途的适应性函数。特别地，它们的可加性属性使任务更加可行，因为它允许我们处理路线而不是路线规划。

    This paper copes with the COGERSA waste collection process. Up to now, experts have been manually designed the process using a trial and error mechanism. This process is not globally optimized, since it has been progressively and locally built as council demands appear. Planning optimization algorithms usually solve it, but they need a fitness function to evaluate a route planning quality. The drawback is that even experts are not able to propose one in a straightforward way due to the complexity of the process. Hence, the goal of this paper is to build a fitness function though a preference framework, taking advantage of the available expert knowledge and expertise. Several key performance indicators together with preference judgments are carefully established according to the experts for learning a promising fitness function. Particularly, the additivity property of them makes the task be much more affordable, since it allows to work with routes rather than with route plannings. Besi
    
[^461]: 具有干扰的多臂赌博机问题

    Multi-Armed Bandits with Interference

    [https://arxiv.org/abs/2402.01845](https://arxiv.org/abs/2402.01845)

    这篇论文研究了在在线平台中与干扰进行的实验。在多臂赌博机问题中，学习者分配不同的臂给每个实验单元，根据单元之间的空间距离和对手选择的匹配函数来决定每个单元在每轮的回报。研究发现，转换政策能够实现最佳的预期遗憾，但任何转换政策都会遭受一定的遗憾现象。

    

    在当代在线平台上，与干扰进行实验是一个重大挑战。以往有关干扰实验的研究集中在政策的最终输出上，而对于累计性能则了解不足。为了填补这一空白，我们引入了“具有干扰的多臂赌博机”（MABI）问题，在时间段为T轮的情况下，学习者为N个实验单元中的每个分配一个臂。每个单元在每一轮的回报取决于“所有”单元的治疗方式，而单元之间的空间距离会导致单元的影响力逐渐衰减。此外，我们使用了一个通用设置，其中回报函数由对手选择，并且在轮次和单元之间可以任意变化。我们首先证明了转换政策能够对最佳固定臂政策实现最优的“预期”遗憾，遗憾值为$O(\sqrt T)$。然而，任何一个转换政策的遗憾（作为一个随机变量）都会遭受一定的遗憾现象。

    Experimentation with interference poses a significant challenge in contemporary online platforms. Prior research on experimentation with interference has concentrated on the final output of a policy. The cumulative performance, while equally crucial, is less well understood. To address this gap, we introduce the problem of {\em Multi-armed Bandits with Interference} (MABI), where the learner assigns an arm to each of $N$ experimental units over a time horizon of $T$ rounds. The reward of each unit in each round depends on the treatments of {\em all} units, where the influence of a unit decays in the spatial distance between units. Furthermore, we employ a general setup wherein the reward functions are chosen by an adversary and may vary arbitrarily across rounds and units. We first show that switchback policies achieve an optimal {\em expected} regret $\tilde O(\sqrt T)$ against the best fixed-arm policy. Nonetheless, the regret (as a random variable) for any switchback policy suffers 
    
[^462]: SynthCLIP: 我们准备好开始完全合成的CLIP训练了吗？

    SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?

    [https://arxiv.org/abs/2402.01832](https://arxiv.org/abs/2402.01832)

    SynthCLIP是一种新的框架，用于训练完全合成的CLIP模型，通过生成大规模的合成图片和标题数据集，在性能上可以与在真实数据上训练的CLIP模型相媲美。

    

    我们提出了SynthCLIP，一种新颖的用于训练完全合成的CLIP模型的框架，与之前依赖真实数据的方法有着显著区别。借助最近的文本到图像生成网络和大型语言模型，我们能够生成任意规模的图像和相应的标题的合成数据集，无需人为干预。通过大规模的训练，SynthCLIP实现了与在真实数据集上训练的CLIP模型相当的性能。我们还介绍了SynthCI-30M，一个纯粹合成的数据集，包含3000万张带标题的图片。我们的代码、训练模型和生成的数据已经在https://github.com/hammoudhasan/SynthCLIP发布。

    We present SynthCLIP, a novel framework for training CLIP models with entirely synthetic text-image pairs, significantly departing from previous methods relying on real data. Leveraging recent text-to-image (TTI) generative networks and large language models (LLM), we are able to generate synthetic datasets of images and corresponding captions at any scale, with no human intervention. With training at scale, SynthCLIP achieves performance comparable to CLIP models trained on real datasets. We also introduce SynthCI-30M, a purely synthetic dataset comprising 30 million captioned images. Our code, trained models, and generated data are released at https://github.com/hammoudhasan/SynthCLIP
    
[^463]: Audio Flamingo: 一种具备弱监督学习和对话能力的新型音频语言模型

    Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities

    [https://arxiv.org/abs/2402.01831](https://arxiv.org/abs/2402.01831)

    Audio Flamingo是一种新型音频语言模型，具备强大的音频理解能力、通过上下文学习和检索快速适应未见过的任务的能力以及强大的多轮对话能力，并且通过广泛的评估达到了最优成绩。

    

    对大型语言模型（LLMs）进行增强，以理解音频——包括非语音声音和非言语的语音——对LLMs的多样化真实世界应用至关重要。本文提出了一种名为Audio Flamingo的新型音频语言模型，具备强大的音频理解能力、通过上下文学习和检索快速适应未见过的任务的能力以及强大的多轮对话能力。我们引入了一系列训练技术、架构设计和数据策略，以增强我们的模型具备这些功能。广泛的音频理解任务评估验证了我们方法的有效性，并创造了新的最优成绩基准。

    Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks.
    
[^464]: LLM中的同行评审方法：开放环境下LLMs的自动评估方法

    Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment

    [https://arxiv.org/abs/2402.01830](https://arxiv.org/abs/2402.01830)

    本文提出了一种新的无监督评估方法，利用同行评审机制在开放环境中衡量LLMs。通过为每个LLM分配可学习的能力参数，以最大化各个LLM的能力和得分的一致性。结果表明，高层次的LLM能够更准确地评估其他模型的答案，并能够获得更高的响应得分。

    

    现有的大型语言模型（LLMs）评估方法通常集中于在一些有人工注释的封闭环境和特定领域基准上测试性能。本文探索了一种新颖的无监督评估方法，利用同行评审机制自动衡量LLMs。在这个设置中，开源和闭源的LLMs处于同一环境中，能够回答未标记的问题并互相评估，每个LLM的响应得分由其他匿名的LLMs共同决定。为了获取这些模型之间的能力层次结构，我们为每个LLM分配一个可学习的能力参数来调整最终排序结果。我们将其形式化为一个受约束的优化问题，旨在最大化每个LLM的能力和得分的一致性。背后的关键假设是高层次的LLM能够比低层次的LLM更准确地评估其他模型的答案，而高层次的LLM也可以达到较高的响应得分。

    Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking. We formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. The key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. Moreover
    
[^465]: 使用深度学习和自然语言处理预测蛋白质序列中的ATP结合位点

    Predicting ATP binding sites in protein sequences using Deep Learning and Natural Language Processing

    [https://arxiv.org/abs/2402.01829](https://arxiv.org/abs/2402.01829)

    本研究使用深度学习和自然语言处理方法，开发了一种预测蛋白质序列中ATP结合位点的方法，实验证明了与最先进的基准模型相比的改进效果。

    

    在生物学和医学领域中，预测基因中的ATP-蛋白结合位点具有重要意义。过去的研究主要通过费时且资源密集的实验室实验进行。多年来，研究人员一直在探索计算方法，利用先进的深度学习和自然语言处理算法来实现相同的目标。在本文中，我们提出了一种分类ATP-蛋白结合位点的方法。我们主要使用PSSMs和几个单词嵌入作为特征进行了各种实验。我们使用了2D CNN和LightGBM分类器作为我们主要的深度学习算法。MP3Vec和BERT模型也在我们的研究中进行了测试。我们的实验证明了与最先进的基准模型相比的改进效果。

    Predicting ATP-Protein Binding sites in genes is of great significance in the field of Biology and Medicine. The majority of research in this field has been conducted through time- and resource-intensive 'wet experiments' in laboratories. Over the years, researchers have been investigating computational methods computational methods to accomplish the same goals, utilising the strength of advanced Deep Learning and NLP algorithms. In this paper, we propose to develop methods to classify ATP-Protein binding sites. We conducted various experiments mainly using PSSMs and several word embeddings as features. We used 2D CNNs and LightGBM classifiers as our chief Deep Learning Algorithms. The MP3Vec and BERT models have also been subjected to testing in our study. The outcomes of our experiments demonstrated improvement over the state-of-the-art benchmarks.
    
[^466]: 通过特征选择和声学词袋模型识别通过口语语言的认知能力下降

    Identification of Cognitive Decline from Spoken Language through Feature Selection and the Bag of Acoustic Words Model

    [https://arxiv.org/abs/2402.01824](https://arxiv.org/abs/2402.01824)

    通过特征选择和声学词袋模型，该研究旨在开发一种自动机器学习技术，通过分析自然口语语言的非词汇声学属性，实现快速准确诊断记忆障碍症状，从而提前识别认知能力下降的早期迹象。

    

    记忆障碍是老年人功能和日常活动衰退的一个重要因素。确认疾病、开始药物治疗以减慢其进程，并开展旨在维护和康复认知能力的职业治疗需要医学诊断。早期识别记忆障碍症状，特别是认知能力下降，对保障人群的幸福起着重要作用。与语音产生相关的特征已知与说话者的认知能力和变化有关。临床环境中缺乏标准化的语音测试导致对自然口语语言的自动机器学习技术的开发越来越受关注。在需要快速、经济高效、可扩展的解决方案用于快速诊断疾病时，非词汇但声学属性的口语语言已被证明是有用的。该工作提出了一种方法...

    Memory disorders are a central factor in the decline of functioning and daily activities in elderly individuals. The confirmation of the illness, initiation of medication to slow its progression, and the commencement of occupational therapy aimed at maintaining and rehabilitating cognitive abilities require a medical diagnosis. The early identification of symptoms of memory disorders, especially the decline in cognitive abilities, plays a significant role in ensuring the well-being of populations. Features related to speech production are known to connect with the speaker's cognitive ability and changes. The lack of standardized speech tests in clinical settings has led to a growing emphasis on developing automatic machine learning techniques for analyzing naturally spoken language. Non-lexical but acoustic properties of spoken language have proven useful when fast, cost-effective, and scalable solutions are needed for the rapid diagnosis of a disease. The work presents an approach rel
    
[^467]: 生态合理的元学习推断解释人类类别学习

    Ecologically rational meta-learned inference explains human category learning

    [https://arxiv.org/abs/2402.01821](https://arxiv.org/abs/2402.01821)

    本研究提出了一种叫做生态合理的元学习推断（ERMI）的模型，通过使用大型语言模型生成与现实世界任务统计一致的认知任务，并通过元学习框架推导适应这些任务的理性主体。实验证明，ERMI模型在定性和定量上都更好地解释了人类的数据。

    

    生态合理性是指人类作为适应环境的理性主体。然而，由于两个方面的挑战，测试这个理论仍然具有挑战性：定义哪些任务在生态上是有效的以及为这些任务建立合理的模型。在这项工作中，我们证明了大型语言模型可以生成与现实世界任务统计一致的认知任务，特别是类别学习任务，从而解决了第一个挑战。我们通过利用元学习框架推导适应这些任务的合理性主体来解决第二个挑战，从而导致了一类模型，称为生态合理的元学习推断（ERMI）。ERMI在两个不同实验中以定量方式比其他七个认知模型更好地解释了人类数据。此外，它在定性上与人类行为相匹配：（1）它发现了与人类发现困难的相同任务，（2）它变得更依赖于基于样本的策略。

    Ecological rationality refers to the notion that humans are rational agents adapted to their environment. However, testing this theory remains challenging due to two reasons: the difficulty in defining what tasks are ecologically valid and building rational models for these tasks. In this work, we demonstrate that large language models can generate cognitive tasks, specifically category learning tasks, that match the statistics of real-world tasks, thereby addressing the first challenge. We tackle the second challenge by deriving rational agents adapted to these tasks using the framework of meta-learning, leading to a class of models called ecologically rational meta-learned inference (ERMI). ERMI quantitatively explains human data better than seven other cognitive models in two different experiments. It additionally matches human behavior on a qualitative level: (1) it finds the same tasks difficult that humans find difficult, (2) it becomes more reliant on an exemplar-based strategy 
    
[^468]: LLMs无法规划，但可以在LLM-Modulo框架中帮助规划

    LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks

    [https://arxiv.org/abs/2402.01817](https://arxiv.org/abs/2402.01817)

    LLMs无法独自进行规划或自我验证，但在规划/推理任务中可以作为通用近似知识源发挥更大作用。

    

    关于大型语言模型（LLMs）在规划和推理任务中的角色存在很大的困惑。一方面有人过于乐观地声称只需正确提示或自我验证策略，LLMs就能完成这些任务。另一方面，也有人过于悲观地认为LLMs在规划/推理任务中仅能作为问题规范的简单翻译器，并将问题交给外部符号求解器。在这篇立场文章中，我们认为这两种极端观点都是错误的。我们认为自回归LLMs本身不能进行规划或自我验证（毕竟这是一种推理形式），并对文献中的误解原因进行了一些阐述。我们还将辩称LLMs应该被视为具有更有意义的角色的通用近似知识源，能在规划/推理任务中发挥更大的作用。

    There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end forma
    
[^469]: 将LLMs的分解能力融入到紧凑语言模型中

    Distilling LLMs' Decomposition Abilities into Compact Language Models

    [https://arxiv.org/abs/2402.01812](https://arxiv.org/abs/2402.01812)

    本研究将LLMs的分解能力通过离线强化学习融入到紧凑模型中，通过开发AI生成的数据集和建立基准，突出了紧凑模型在复制复杂问题解决能力方面的潜力。

    

    大型语言模型（LLMs）展示了其推理能力，但其庞大的大小带来了可扩展性挑战，并限制了进一步的定制。相比之下，紧凑模型提供了定制化培训，但在解决复杂推理任务方面往往不足。本研究着重于使用离线强化学习将LLMs的分解能力融入到紧凑模型中。我们利用LLM能力的进步，提供反馈并生成专门用于训练紧凑模型的特定任务数据集。通过开发一个由AI生成的数据集和建立基准，我们的工作主要贡献在于强调了紧凑模型在复制复杂问题解决能力方面的潜力。

    Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization. In contrast, compact models offer customized training but often fall short in solving complex reasoning tasks. This study focuses on distilling the LLMs' decomposition skills into compact models using offline reinforcement learning. We leverage the advancements in the LLM`s capabilities to provide feedback and generate a specialized task-specific dataset for training compact models. The development of an AI-generated dataset and the establishment of baselines constitute the primary contributions of our work, underscoring the potential of compact models in replicating complex problem-solving skills.
    
[^470]: 分布鲁棒优化方法在公平信用评分中的应用

    A Distributionally Robust Optimisation Approach to Fair Credit Scoring

    [https://arxiv.org/abs/2402.01811](https://arxiv.org/abs/2402.01811)

    本文研究了如何在信用评分中应用分布鲁棒优化方法，并对现有技术的鲁棒性效果进行了实证评估。

    

    信用评分被欧洲委员会和美国总统办公室归为高风险分类任务，关键问题是基于可能偏向某些群体的模型进行贷款批准决策可能造成的潜在风险。为解决这一问题，近期的信用评分研究考虑了机器学习领域提出的一系列增强公平性的技术来减少分类系统中的偏见和不公平对待。然而，尽管公平性的定义或实施方法各有不同，这些技术大多忽视了结果的鲁棒性。这可能导致在训练集中有效纠正不公平对待，但在生成样本外的分类时会再次产生不公平对待。因此，在本文中，我们将研究如何将分布鲁棒优化(DRO)方法应用于信用评分，并为此对现有技术的鲁棒性效果进行实证评估。

    Credit scoring has been catalogued by the European Commission and the Executive Office of the US President as a high-risk classification task, a key concern being the potential harms of making loan approval decisions based on models that would be biased against certain groups. To address this concern, recent credit scoring research has considered a range of fairness-enhancing techniques put forward by the machine learning community to reduce bias and unfair treatment in classification systems. While the definition of fairness or the approach they follow to impose it may vary, most of these techniques, however, disregard the robustness of the results. This can create situations where unfair treatment is effectively corrected in the training set, but when producing out-of-sample classifications, unfair treatment is incurred again. Instead, in this paper, we will investigate how to apply Distributionally Robust Optimisation (DRO) methods to credit scoring, thereby empirically evaluating h
    
[^471]: 近确定性回归中的错误规范化不确定性

    Misspecification uncertainties in near-deterministic regression

    [https://arxiv.org/abs/2402.01810](https://arxiv.org/abs/2402.01810)

    该论文研究了近确定性回归中错误规范化的不确定性问题，并提出了一种组合模型，以准确预测和控制参数不确定性。

    

    期望损失是模型泛化误差的上界，可用于学习的鲁棒PAC-Bayes边界。然而，损失最小化被认为忽略了错误规范化，即模型不能完全复制观测结果。这导致大数据或欠参数化极限下对参数不确定性的显著低估。我们分析近确定性、错误规范化和欠参数化替代模型的泛化误差，这是科学和工程中广泛相关的一个领域。我们证明后验分布必须覆盖每个训练点，以避免发散的泛化误差，并导出一个符合这个约束的组合模型。对于线性模型，这种高效的方法产生的额外开销最小。这种高效方法在模型问题上进行了演示，然后应用于原子尺度机器学习中的高维数据集。

    The expected loss is an upper bound to the model generalization error which admits robust PAC-Bayes bounds for learning. However, loss minimization is known to ignore misspecification, where models cannot exactly reproduce observations. This leads to significant underestimates of parameter uncertainties in the large data, or underparameterized, limit. We analyze the generalization error of near-deterministic, misspecified and underparametrized surrogate models, a regime of broad relevance in science and engineering. We show posterior distributions must cover every training point to avoid a divergent generalization error and derive an ensemble {ansatz} that respects this constraint, which for linear models incurs minimal overhead. The efficient approach is demonstrated on model problems before application to high dimensional datasets in atomistic machine learning. Parameter uncertainties from misspecification survive in the underparametrized limit, giving accurate prediction and boundin
    
[^472]: PhenoLinker: 使用异构图神经网络进行表型-基因连接预测和解释的研究

    PhenoLinker: Phenotype-Gene Link Prediction and Explanation using Heterogeneous Graph Neural Networks

    [https://arxiv.org/abs/2402.01809](https://arxiv.org/abs/2402.01809)

    PhenoLinker通过使用异构图神经网络进行表型-基因关联预测和解释，为发现新的关联和理解人类遗传变异的后果提供了一种新的方法。

    

    将给定的人类表型与遗传变异相关联仍然是生物学中的一项关键挑战。我们提出了一个名为PhenoLinker的新系统，能够使用异构信息网络和基于卷积神经网络的图模型来为表型-基因关系分配一个得分，并且能够提供对预测结果的解释。该系统可以帮助发现新的关联，并理解人类遗传变异的后果。

    The association of a given human phenotype to a genetic variant remains a critical challenge for biology. We present a novel system called PhenoLinker capable of associating a score to a phenotype-gene relationship by using heterogeneous information networks and a convolutional neural network-based model for graphs, which can provide an explanation for the predictions. This system can aid in the discovery of new associations and in the understanding of the consequences of human genetic variation.
    
[^473]: 一个基于拍卖的联邦学习模型交易市场

    An Auction-based Marketplace for Model Trading in Federated Learning

    [https://arxiv.org/abs/2402.01802](https://arxiv.org/abs/2402.01802)

    本论文提出了一个基于拍卖的联邦学习模型交易市场，允许模型的买卖并通过适当定价和激励机制来提高模型性能和实现最大交易量。

    

    联邦学习（FL）越来越被认识到在使用本地分布数据训练模型方面的效力。然而，在这个合作过程中共享数据的适当估值仍未得到足够解决。在这项工作中，我们将FL设想为一个模型交易的市场，客户既是买家也是卖家，参与模型交易。这个FL市场允许客户通过出售自己的模型赚取货币奖励，并通过购买他人的模型来提高本地模型的性能。我们提出了一个基于拍卖的解决方案，以确保基于性能增益的适当定价。激励机制被设计出来鼓励客户真实地揭示他们对模型的价值评估。此外，我们引入了一个强化学习（RL）框架用于市场运营，旨在实现在动态和不断变化的市场状况下的最大交易量。在四个数据集上进行的实验结果表明，所提出的FL市场可以实现高交易收益和公平的模型定价。

    Federated learning (FL) is increasingly recognized for its efficacy in training models using locally distributed data. However, the proper valuation of shared data in this collaborative process remains insufficiently addressed. In this work, we frame FL as a marketplace of models, where clients act as both buyers and sellers, engaging in model trading. This FL market allows clients to gain monetary reward by selling their own models and improve local model performance through the purchase of others' models. We propose an auction-based solution to ensure proper pricing based on performance gain. Incentive mechanisms are designed to encourage clients to truthfully reveal their model valuations. Furthermore, we introduce a reinforcement learning (RL) framework for marketing operations, aiming to achieve maximum trading volumes under the dynamic and evolving market status. Experimental results on four datasets demonstrate that the proposed FL market can achieve high trading revenue and fai
    
[^474]: 大规模语言模型用于时间序列：一项调研

    Large Language Models for Time Series: A Survey

    [https://arxiv.org/abs/2402.01801](https://arxiv.org/abs/2402.01801)

    本调研论文深入探讨了大规模语言模型（LLM）在时间序列分析中的应用方法。通过解决LLM与数值型时间序列数据之间的差异挑战，揭示了LLM在时间序列领域的潜力，并提出了直接提示、量化、对齐、利用视觉方式和结合工具等方法。此外，还提供了对应用领域、评估方法和未来研究方向的讨论。

    

    大规模语言模型（LLM）在自然语言处理和计算机视觉等领域得到了广泛应用。LLM不仅仅局限于文本、图像和图形，还具有对时间序列数据进行分析的重要潜力，可以在气候、物联网、医疗、交通、音频和金融等领域受益。本调研论文对利用LLM进行时间序列分析的各种方法进行了深入探讨和详细分类。我们解决了LLM原始文本数据训练与数值型时间序列数据之间的差异挑战，并探索了将LLM的知识转移和提取到数值时间序列分析的策略。我们详细介绍了各种方法，包括（1）直接提示LLM，（2）时间序列量化，（3）对齐技术，（4）利用视觉方式作为桥接机制，和（5）结合LLM与工具。此外，本调研还提供了一系列涉及应用领域、评估方法和未来研究方向的讨论。

    Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) alignment techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey off
    
[^475]: 更快更轻的LLMs：当前挑战和未来发展的调查

    Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward

    [https://arxiv.org/abs/2402.01799](https://arxiv.org/abs/2402.01799)

    本调查文章概述了在提高LLM推理效果方面的最新方法和进展，通过实验评估不同压缩技术的有效性，并提出改进LLM推理效率的潜在未来方向。

    

    尽管LLMs表现出色，但由于推理过程中需要大量的计算和内存资源，它们的普及面临着挑战。最近在模型压缩和系统级优化方法方面的进展旨在增强LLM推理效果。本调查提供了这些方法的概述，强调了最近的发展。通过对LLaMA(/2)-7B的实验，我们评估了各种压缩技术，为在统一环境中高效部署LLM提供了实践见解。对LLaMA(/2)-7B的实证分析突出了这些方法的有效性。基于调查结果，我们确定了当前的局限性，并讨论了改善LLM推理效率的潜在未来方向。我们在https://github.com/nyunAI/Faster-LLM-Survey发布了用于复现本文结果的代码库。

    Despite the impressive performance of LLMs, their widespread adoption faces challenges due to substantial computational and memory requirements during inference. Recent advancements in model compression and system-level optimization methods aim to enhance LLM inference. This survey offers an overview of these methods, emphasizing recent developments. Through experiments on LLaMA(/2)-7B, we evaluate various compression techniques, providing practical insights for efficient LLM deployment in a unified setting. The empirical analysis on LLaMA(/2)-7B highlights the effectiveness of these methods. Drawing from survey insights, we identify current limitations and discuss potential future directions to improve LLM inference efficiency. We release the codebase to reproduce the results presented in this paper at https://github.com/nyunAI/Faster-LLM-Survey
    
[^476]: 改进的量化策略用于管理分布式学习中的重尾梯度

    Improved Quantization Strategies for Managing Heavy-tailed Gradients in Distributed Learning

    [https://arxiv.org/abs/2402.01798](https://arxiv.org/abs/2402.01798)

    本论文介绍了一种针对重尾梯度的新型压缩方案，在分布式学习中解决了梯度分布不均匀的挑战，并且将梯度截断和量化相结合，通过优化关键参数的值来减小量化引起的误差。

    

    梯度压缩已经成为分布式学习中解决通信效率挑战的关键技术。然而，在分布式深度学习中，观察到梯度分布呈重尾分布，离群值显著影响压缩策略的设计。现有的参数量化方法在忽略重尾特征时会出现性能下降。在本文中，我们引入了一种新的针对重尾梯度的压缩方案，将梯度截断与量化巧妙结合。该方案巧妙地实现在一个通信受限的分布式随机梯度下降（SGD）框架中。我们考虑了一类遵循幂律分布的重尾梯度，旨在最小化由量化引起的误差，从而确定两个关键参数的最优值：截断阈值和量化密度。

    Gradient compression has surfaced as a key technique to address the challenge of communication efficiency in distributed learning. In distributed deep learning, however, it is observed that gradient distributions are heavy-tailed, with outliers significantly influencing the design of compression strategies. Existing parameter quantization methods experience performance degradation when this heavy-tailed feature is ignored. In this paper, we introduce a novel compression scheme specifically engineered for heavy-tailed gradients, which effectively combines gradient truncation with quantization. This scheme is adeptly implemented within a communication-limited distributed Stochastic Gradient Descent (SGD) framework. We consider a general family of heavy-tail gradients that follow a power-law distribution, we aim to minimize the error resulting from quantization, thereby determining optimal values for two critical parameters: the truncation threshold and the quantization density. We provid
    
[^477]: 鲁棒支持向量机的锥优化方法

    Robust support vector machines via conic optimization

    [https://arxiv.org/abs/2402.01797](https://arxiv.org/abs/2402.01797)

    本文通过混合整数优化技术提出了一种新的损失函数，用于学习鲁棒支持向量机。与现有的方法相比，该方法更好地逼近了0-1损失，同时保持了学习问题的凸性。在实验中表现与标准SVMs相竞争，在存在异常值的情况下表现更优秀。

    

    我们考虑了在不确定性条件下学习鲁棒支持向量机的问题。文献中已经证明，典型的损失函数，包括铰链损失，对数据扰动和异常值非常敏感，在该设置下表现不佳。相比之下，使用0-1损失或适当的非凸逼近可以得到鲁棒的估计，但代价是计算成本较高。我们使用混合整数优化技术导出了一个新的损失函数，与现有的选择相比更好地逼近了0-1损失，同时保持了学习问题的凸性。在我们的计算结果中，我们展示了所提出的估计器在无异常值的情况下与带铰链损失的标准SVMs相竞争，在存在异常值的情况下更优秀。

    We consider the problem of learning support vector machines robust to uncertainty. It has been established in the literature that typical loss functions, including the hinge loss, are sensible to data perturbations and outliers, thus performing poorly in the setting considered. In contrast, using the 0-1 loss or a suitable non-convex approximation results in robust estimators, at the expense of large computational costs. In this paper we use mixed-integer optimization techniques to derive a new loss function that better approximates the 0-1 loss compared with existing alternatives, while preserving the convexity of the learning problem. In our computational results, we show that the proposed estimator is competitive with the standard SVMs with the hinge loss in outlier-free regimes and better in the presence of outliers.
    
[^478]: 探索用于病理语音特征预测的迁移学习：层选择的影响

    Exploring transfer learning for pathological speech feature prediction: Impact of layer selection

    [https://arxiv.org/abs/2402.01796](https://arxiv.org/abs/2402.01796)

    本研究探索了用于病理语音特征预测的迁移学习，发现选择适当的层能显著提高性能并且学得的加权和具有更好的泛化能力。

    

    利用人工智能对临床语音进行自动客观评估，并促进语音障碍的诊断和治疗具有重要意义。本研究探索了迁移学习，在预测病理语音存在性的下游任务中，重点分析了层选择的影响。我们发现选择最佳层能显著提高性能（平均平衡准确率增加12.4%），尽管最佳层因预测特征而异，并且并不总是对未见数据泛化良好。学得的加权和在分布内与平均最佳层具有可比性的性能，并且在分布外数据上具有更好的泛化能力。

    There is interest in leveraging AI to conduct automatic, objective assessments of clinical speech, in turn facilitating diagnosis and treatment of speech disorders. We explore transfer learning, focusing on the impact of layer selection, for the downstream task of predicting the presence of pathological speech. We find that selecting an optimal layer offers large performance improvements (12.4% average increase in balanced accuracy), though the best layer varies by predicted feature and does not always generalize well to unseen data. A learned weighted sum offers comparable performance to the average best layer in-distribution and has better generalization for out-of-distribution data.
    
[^479]: 基于邻域覆盖和相似性的自动驾驶车辆少样本场景测试

    Few-Shot Scenario Testing for Autonomous Vehicles Based on Neighborhood Coverage and Similarity

    [https://arxiv.org/abs/2402.01795](https://arxiv.org/abs/2402.01795)

    本文提出了一个基于邻域覆盖和相似性的少样本场景测试框架，用于解决大规模部署之前自动驾驶车辆测试和评估中的不确定性和挑战。

    

    在大规模部署之前，对自动驾驶车辆（AVs）进行测试和评估其安全性能是至关重要的。然而，由于测试成本或时间的限制，针对特定AV模型的可接受测试成本通常会被极限制地降低。现有的测试方法严格限制的测试数目会导致测试结果的显著不确定性或挑战。本文首次将这个问题定义为“少样本测试（FST）”问题，并提出了一个系统的FST框架来解决这个挑战。为了减轻小规模测试场景中的不确定性，并优化场景利用，我们将FST问题建模为一个优化问题，并基于邻域覆盖和相似性寻找一个小的场景集合。通过利用先验信息中的代理模型（SMs），我们动态调整测试场景集合和其贡献。

    Testing and evaluating the safety performance of autonomous vehicles (AVs) is essential before the large-scale deployment. Practically, the acceptable cost of testing specific AV model can be restricted within an extremely small limit because of testing cost or time. With existing testing methods, the limitations imposed by strictly restricted testing numbers often result in significant uncertainties or challenges in quantifying testing results. In this paper, we formulate this problem for the first time the "few-shot testing" (FST) problem and propose a systematic FST framework to address this challenge. To alleviate the considerable uncertainty inherent in a small testing scenario set and optimize scenario utilization, we frame the FST problem as an optimization problem and search for a small scenario set based on neighborhood coverage and similarity. By leveraging the prior information on surrogate models (SMs), we dynamically adjust the testing scenario set and the contribution of 
    
[^480]: 变分量子电路增强的生成对抗网络

    Variational Quantum Circuits Enhanced Generative Adversarial Network

    [https://arxiv.org/abs/2402.01791](https://arxiv.org/abs/2402.01791)

    这项工作提出了一种基于变分量子电路的混合量子-经典架构（QC-GAN），通过在手写图像生成任务上表现出更好的性能来改进传统GAN。这个架构在收敛时具有更少的训练参数和迭代次数，并且利用了量子电路的纠缠和表达能力。

    

    生成对抗网络（GAN）是一种广泛应用于生成高质量图像、视频和音频内容等各种应用领域的机器学习框架。然而，对于大型神经网络，训练GAN可能会变得计算密集。在这项工作中，我们提出了一种改进GAN的混合量子-经典架构（QC-GAN）。通过在手写图像生成任务上使用MindSpore Quantum与传统GAN进行基准测试，我们对性能进行了数值检验。QC-GAN的生成器由量子变分电路和一层神经网络组成，判别器由传统神经网络组成。借助量子电路的纠缠和表达能力，我们的混合架构在收敛时比传统GAN具有更好的性能（Frechet Inception Distance），并且训练参数和迭代次数更少。我们还展示了其优越性。

    Generative adversarial network (GAN) is one of the widely-adopted machine-learning frameworks for a wide range of applications such as generating high-quality images, video, and audio contents. However, training a GAN could become computationally expensive for large neural networks. In this work, we propose a hybrid quantum-classical architecture for improving GAN (denoted as QC-GAN). The performance was examed numerically by benchmarking with a classical GAN using MindSpore Quantum on the task of hand-written image generation. The generator of the QC-GAN consists of a quantum variational circuit together with a one-layer neural network, and the discriminator consists of a traditional neural network. Leveraging the entangling and expressive power of quantum circuits, our hybrid architecture achieved better performance (Frechet Inception Distance) than the classical GAN, with much fewer training parameters and number of iterations for convergence. We have also demonstrated the superiori
    
[^481]: 一种用于机制可解释性的图形张量符号化的介绍

    An introduction to graphical tensor notation for mechanistic interpretability

    [https://arxiv.org/abs/2402.01790](https://arxiv.org/abs/2402.01790)

    图形张量符号化是一种简单的表示张量操作的方法，对于理解深度学习系统和理解神经网络行为具有重要意义。本论文介绍了图形张量符号化的方法，并将其应用于不同的分解和解释语言模型的基础方法中。

    

    图形张量符号化是一种简单的表示张量线性操作的方法，源自物理学。现代深度学习几乎完全由张量操作组成，因此理解张量操作对于理解这些系统非常重要。尤其是在试图反向工程神经网络学习的算法以理解其行为时，这一点尤为重要，这个领域被称为机制可解释性。在张量间进行的操作往往让人混淆，并且很难抓住整体结构，但图形张量符号化使得快速解析和发现有趣的等价关系更加容易。本文的前半部分介绍了这种符号化方法并将其应用于一些分解方法（SVD，CP，Tucker和张量网络分解），后半部分将其应用于一些现有的用于理解语言模型的基础方法，大致遵循

    Graphical tensor notation is a simple way of denoting linear operations on tensors, originating from physics. Modern deep learning consists almost entirely of operations on or between tensors, so easily understanding tensor operations is quite important for understanding these systems. This is especially true when attempting to reverse-engineer the algorithms learned by a neural network in order to understand its behavior: a field known as mechanistic interpretability. It's often easy to get confused about which operations are happening between tensors and lose sight of the overall structure, but graphical tensor notation makes it easier to parse things at a glance and see interesting equivalences. The first half of this document introduces the notation and applies it to some decompositions (SVD, CP, Tucker, and tensor network decompositions), while the second half applies it to some existing some foundational approaches for mechanistically understanding language models, loosely follow
    
[^482]: 在文本到图像模型中的危害放大

    Harm Amplification in Text-to-Image Models

    [https://arxiv.org/abs/2402.01787](https://arxiv.org/abs/2402.01787)

    我们的研究提出了危害放大现象并发展了量化危害放大的方法，考虑模型输出的危害与用户输入的情境。我们还实证地研究了不同的方法在真实场景中的应用，并量化了由危害放大引起的性别之间的影响差异。

    

    文本到图像 (T2I) 模型已成为生成式人工智能的重要进展，然而，存在安全问题，即使用户输入看似安全的提示，这些模型也可能生成有害图像。这种现象称为危害放大，它比对抗提示更具潜在风险，使用户无意间遭受伤害。本文首先提出了危害放大的形式定义，并进一步贡献于开发用于量化危害放大的方法，考虑模型输出的危害与用户输入的情境。我们还经验性地研究了如何应用这些方法模拟真实世界的部署场景，包括量化由危害放大引起的不同性别之间的影响差异。我们的工作旨在为研究者提供工具去解决这个问题。

    Text-to-image (T2I) models have emerged as a significant advancement in generative AI; however, there exist safety concerns regarding their potential to produce harmful image outputs even when users input seemingly safe prompts. This phenomenon, where T2I models generate harmful representations that were not explicit in the input, poses a potentially greater risk than adversarial prompts, leaving users unintentionally exposed to harms. Our paper addresses this issue by first introducing a formal definition for this phenomenon, termed harm amplification. We further contribute to the field by developing methodologies to quantify harm amplification in which we consider the harm of the model output in the context of user input. We then empirically examine how to apply these different methodologies to simulate real-world deployment scenarios including a quantification of disparate impacts across genders resulting from harm amplification. Together, our work aims to offer researchers tools to
    
[^483]: COA-GPT：用于军事行动中加速行动方案开发的生成式预训练变压器

    COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations

    [https://arxiv.org/abs/2402.01786](https://arxiv.org/abs/2402.01786)

    COA-GPT是一种利用大型语言模型快速高效生成有效行动方案的算法，它融合了军事学说和领域专业知识，并在军事游戏中的实验中展示了其快速生成战略合理COAs的优势。

    

    军事行动中行动方案（COAs）的开发传统上是一个耗时且复杂的过程。针对这一挑战，本研究介绍了COA-GPT，一种利用大型语言模型（LLMs）快速高效生成有效COAs的新算法。COA-GPT通过上下文学习将军事学说和领域专业知识融入到LLMs中，允许指挥官输入任务信息（包括文本和图像格式），并获得与战略对齐的COAs以供审查和批准。独特的是，COA-GPT不仅加速了COA的开发，在几秒钟内生成初始COAs，还能根据指挥官的反馈实时精细化改进。本研究在《星际争霸II》游戏的军事相关场景中评估了COA-GPT，将其性能与最先进的强化学习算法进行了比较。我们的结果表明COA-GPT在更快生成战略合理的COAs方面具有优势。

    The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, 
    
[^484]: DoubleMLDeep: 利用多模态数据对因果效应进行估计

    DoubleMLDeep: Estimation of Causal Effects with Multimodal Data

    [https://arxiv.org/abs/2402.01785](https://arxiv.org/abs/2402.01785)

    本文提出了一个利用文本和图像在因果推断和治疗效应估计中的双机器学习框架，并提出了一种生成半合成数据集的方法用于评估因果效应估计的性能。这些方法和架构在半合成数据集上进行了评估，并与标准方法进行了比较，显示了直接使用文本和图像进行因果研究的潜在好处。

    

    本文探讨了在因果推断和治疗效应估计中使用非结构化的多模态数据，即文本和图像。我们提出了一种适应于双机器学习（DML）框架，特别是部分线性模型的神经网络架构。我们论文的另一个贡献是提出了一种生成半合成数据集的新方法，该方法可用于评估在文本和图像作为混淆因素的情况下因果效应估计的性能。我们在半合成数据集上评估并与标准方法进行比较，突出了直接在因果研究中使用文本和图像的潜在好处。我们的研究结果对经济学、市场营销、金融、医学和数据科学等领域的研究人员和从业者具有重要意义，他们希望使用非传统数据估计因果数量。

    This paper explores the use of unstructured, multimodal data, namely text and images, in causal inference and treatment effect estimation. We propose a neural network architecture that is adapted to the double machine learning (DML) framework, specifically the partially linear model. An additional contribution of our paper is a new method to generate a semi-synthetic dataset which can be used to evaluate the performance of causal effect estimation in the presence of text and images as confounders. The proposed methods and architectures are evaluated on the semi-synthetic dataset and compared to standard approaches, highlighting the potential benefit of using text and images directly in causal studies. Our findings have implications for researchers and practitioners in economics, marketing, finance, medicine and data science in general who are interested in estimating causal quantities using non-traditional data.
    
[^485]: 在线疫苗关注的分层多标签分类

    Hierarchical Multi-Label Classification of Online Vaccine Concerns

    [https://arxiv.org/abs/2402.01783](https://arxiv.org/abs/2402.01783)

    本文研究了在线疫苗关注的分层多标签分类任务，使用大型语言模型在零样本设置下检测疫苗关注，同时探索了不同提示策略的成本和准确性权衡，并提供了指导当前应用程序系统设计的具体经验教训。

    

    疫苗关注是一个不断发展的目标，可以在COVID-19大流行中快速变化。通过识别疫苗关注和错误信息的长期趋势，可以帮助公共卫生努力在资源或信息宣传上进行战略性分配。我们在零样本设置中使用大型语言模型（LLM）探索在在线讨论中检测疫苗关注的任务，无需昂贵的训练数据集。由于实时监控在线来源需要大规模推理，我们探索了不同提示策略的成本和准确性之间的权衡，并提供了可以为当前应用程序的系统设计选择提供信息的具体经验教训。对不同提示策略的分析表明，通过LLM多次进行分类，每次通过布尔问题判断文本是否提到疫苗关注，效果最好。我们的结果表明，GPT-4能够明显优于其他模型。

    Vaccine concerns are an ever-evolving target, and can shift quickly as seen during the COVID-19 pandemic. Identifying longitudinal trends in vaccine concerns and misinformation might inform the healthcare space by helping public health efforts strategically allocate resources or information campaigns. We explore the task of detecting vaccine concerns in online discourse using large language models (LLMs) in a zero-shot setting without the need for expensive training datasets. Since real-time monitoring of online sources requires large-scale inference, we explore cost-accuracy trade-offs of different prompting strategies and offer concrete takeaways that may inform choices in system designs for current applications. An analysis of different prompting strategies reveals that classifying the concerns over multiple passes through the LLM, each consisting a boolean question whether the text mentions a vaccine concern or not, works the best. Our results indicate that GPT-4 can strongly outpe
    
[^486]: 使用不同局部性对脉冲神经网络学习方法进行基准测试

    Benchmarking Spiking Neural Network Learning Methods with Varying Locality

    [https://arxiv.org/abs/2402.01782](https://arxiv.org/abs/2402.01782)

    本研究使用不同局部性对脉冲神经网络学习方法进行基准测试，并发现这些方法在性能和生物学合理性之间存在权衡。此外，研究还探讨了SNN的隐式循环特性。

    

    脉冲神经网络（SNN）提供更真实的神经动力学，在多个机器学习任务中已经显示出与人工神经网络（ANN）相当的性能。信息在SNN中以脉冲形式进行处理，采用事件驱动机制，显著降低了能源消耗。然而，由于脉冲机制的非可微性，训练SNN具有挑战性。传统方法如时间反向传播（BPTT）已经显示出一定的效果，但在计算和存储成本方面存在问题，并且在生物学上不可行。相反，最近的研究提出了具有不同局部性的替代学习方法，在分类任务中取得了成功。本文表明，这些方法在训练过程中有相似之处，同时在生物学合理性和性能之间存在权衡。此外，本研究还探讨了SNN的隐式循环特性，并进行了调查。

    Spiking Neural Networks (SNNs), providing more realistic neuronal dynamics, have shown to achieve performance comparable to Artificial Neural Networks (ANNs) in several machine learning tasks. Information is processed as spikes within SNNs in an event-based mechanism that significantly reduces energy consumption. However, training SNNs is challenging due to the non-differentiable nature of the spiking mechanism. Traditional approaches, such as Backpropagation Through Time (BPTT), have shown effectiveness but comes with additional computational and memory costs and are biologically implausible. In contrast, recent works propose alternative learning methods with varying degrees of locality, demonstrating success in classification tasks. In this work, we show that these methods share similarities during the training process, while they present a trade-off between biological plausibility and performance. Further, this research examines the implicitly recurrent nature of SNNs and investigat
    
[^487]: 当基准成为目标：揭示大型语言模型排行榜的敏感性

    When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards

    [https://arxiv.org/abs/2402.01781](https://arxiv.org/abs/2402.01781)

    依赖基准排行榜的大型语言模型评估存在较高敏感性，微小的扰动会导致排名的显著变化。研究结果提供了几个最佳实践建议，包括选择混合评分方法来提高答案选择的性能。

    

    基于基准排名的大型语言模型(LLM)排行榜经常被用来指导实践者在模型选择中。通常，发布的排行榜排名被直接接受 - 我们表明这是一个（潜在昂贵的）错误。在现有的排行榜下，LLM的相对性能对（通常微小的）细节非常敏感。我们展示了对于流行的多项选择题基准（例如MMLU），对基准的微小扰动，如改变选项顺序或答案选择方法，会导致排名变化达到8个位置。我们通过对三个广泛的基准扰动类别进行系统实验并确定这一行为的来源来解释这一现象。我们的分析得出了几个最佳实践建议，包括选择优化的混合评分方法来进行答案选择。我们的研究强调了依赖简单基准评估的风险，并为更健壮的模型评估提供了指导道路。

    Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple choice question benchmarks (e.g. MMLU) minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust
    
[^488]: 带有随机去噪正则化的即插即用图像恢复

    Plug-and-Play image restoration with Stochastic deNOising REgularization

    [https://arxiv.org/abs/2402.01779](https://arxiv.org/abs/2402.01779)

    本论文提出了一种新的即插即用图像恢复框架，称为随机去噪正则化（SNORE）。该框架在恰当噪声水平的图像上应用去噪器，并基于随机正则化提供了解决病态逆问题的随机梯度下降算法。实验结果表明，SNORE在去模糊和修复任务中与最先进的方法具有竞争力。

    

    即插即用（PnP）算法是一类迭代算法，通过结合物理模型和深度神经网络进行正则化来解决图像反演问题。尽管这些算法能够产生令人印象深刻的图像恢复结果，但它们依赖于在迭代过程中越来越少噪音的图像上的一种非标准的去噪器使用方法，这与基于扩散模型（DM）的最新算法相矛盾，在这些算法中，去噪器仅应用于重新加噪的图像上。我们提出了一种新的PnP框架，称为随机去噪正则化（SNORE），它仅在噪声水平适当的图像上应用去噪器。它基于显式的随机正则化，从而导致了一种解决病态逆问题的随机梯度下降算法。我们提供了该算法及其退火扩展的收敛分析。在实验上，我们证明SNORE在去模糊和修复任务上与最先进的方法相竞争。

    Plug-and-Play (PnP) algorithms are a class of iterative algorithms that address image inverse problems by combining a physical model and a deep neural network for regularization. Even if they produce impressive image restoration results, these algorithms rely on a non-standard use of a denoiser on images that are less and less noisy along the iterations, which contrasts with recent algorithms based on Diffusion Models (DM), where the denoiser is applied only on re-noised images. We propose a new PnP framework, called Stochastic deNOising REgularization (SNORE), which applies the denoiser only on images with noise of the adequate level. It is based on an explicit stochastic regularization, which leads to a stochastic gradient descent algorithm to solve ill-posed inverse problems. A convergence analysis of this algorithm and its annealing extension is provided. Experimentally, we prove that SNORE is competitive with respect to state-of-the-art methods on deblurring and inpainting tasks, 
    
[^489]: 语音识别导论

    Introduction to speech recognition

    [https://arxiv.org/abs/2402.01778](https://arxiv.org/abs/2402.01778)

    该论文介绍了使用Matlab实现的语音识别系统，通过语音建模、强大的计算机算法和机器学习方法，成功实现了对三个单词进行正确分类的任务。

    

    本文包含了使用Matlab进行讲座和实践实验的内容，实现了一个能够正确分类三个单词（one，two和three）的系统，并且只使用了一个非常小的数据库。为了达到这样的性能，它使用了特定的语音建模，强大的计算机算法（动态时间规整和Dijkstra算法）以及机器学习（最近邻）。本文还介绍了一些机器学习评估指标。

    This document contains lectures and practical experimentations using Matlab and implementing a system which is actually correctly classifying three words (one, two and three) with the help of a very small database. To achieve this performance, it uses speech modeling specificities, powerful computer algorithms (dynamic time warping and Dijktra's algorithm) and machine learning (nearest neighbor). This document introduces also some machine learning evaluation metrics.
    
[^490]: 解开多语言机器翻译中目标端转移和正则化的作用

    Disentangling the Roles of Target-Side Transfer and Regularization in Multilingual Machine Translation

    [https://arxiv.org/abs/2402.01772](https://arxiv.org/abs/2402.01772)

    本文通过大规模研究展示了多语言机器翻译中目标端转移的动态影响。我们发现，语言相似的辅助目标语言具有强大的正向知识转移能力，并且随着相似目标语言规模的增加，转移效果进一步增强。同时，远离的辅助目标语言也可以意外地对主要语言对产生正向转移效果。

    

    多语言机器翻译(MMT)在不同语言对之间的知识转移中受益。然而，一对多翻译相比于多对一翻译的改进仅有微小甚至可忽略不计。这种性能差异引发了一个问题：在一对多MT中，正向转移在目标端的作用程度如何。在本文中，我们进行了一项大规模研究，通过语言相似性和语料库大小这两个维度变化辅助目标语言，以展示知识转移对主要语言对的动态影响。我们发现，语言相似的辅助目标语言表现出强大的正向知识转移能力。随着相似目标语言规模的增加，正向转移进一步增强，使主要语言对受益。同时，我们发现远离的辅助目标语言也可以出乎意料地使主要语言对受益，即使正向转移最小也是如此。

    Multilingual Machine Translation (MMT) benefits from knowledge transfer across different language pairs. However, improvements in one-to-many translation compared to many-to-one translation are only marginal and sometimes even negligible. This performance discrepancy raises the question of to what extent positive transfer plays a role on the target-side for one-to-many MT. In this paper, we conduct a large-scale study that varies the auxiliary target side languages along two dimensions, i.e., linguistic similarity and corpus size, to show the dynamic impact of knowledge transfer on the main language pairs. We show that linguistically similar auxiliary target languages exhibit strong ability to transfer positive knowledge. With an increasing size of similar target languages, the positive transfer is further enhanced to benefit the main language pairs. Meanwhile, we find distant auxiliary target languages can also unexpectedly benefit main language pairs, even with minimal positive trans
    
[^491]: BlackMamba: 混合专家模型的状态空间模型

    BlackMamba: Mixture of Experts for State-Space Models

    [https://arxiv.org/abs/2402.01771](https://arxiv.org/abs/2402.01771)

    BlackMamba是一种结合了Mamba SSM和MoE的新型架构，它具有竞争力的性能和较低的推断和训练成本，在大规模语言建模领域具有潜在应用价值。

    

    最近的研究表明，状态空间模型（SSMs）在大规模语言建模基准测试中表现出与transformer竞争力的性能，同时，其时间和内存复杂度与序列长度成线性关系。最近发布的SSM模型Mamba在语言建模和处理长序列任务方面表现出色。与此同时，专家混合模型（MoE）在显著降低推断计算和延迟成本的同时，也增加了更大的内存占用。本文提出了一种名为BlackMamba的新型架构，将Mamba SSM与MoE相结合，以获得两者的好处。我们证明BlackMamba在Mamba和transformer基准测试中表现出竞争力，并在推断和训练FLOPs方面表现出色。我们在自定义数据集的300B标记上全面训练并开源了340M/1.5B和630M/2.8B的BlackMamba模型。我们展示了BlackMamba继承并结合了这两种模型的优势。

    State-space models (SSMs) have recently demonstrated competitive performance to transformers at large-scale language modeling benchmarks while achieving linear time and memory complexity as a function of sequence length. Mamba, a recently released SSM model, shows impressive performance in both language modeling and long sequence processing tasks. Simultaneously, mixture-of-expert (MoE) models have shown remarkable performance while significantly reducing the compute and latency costs of inference at the expense of a larger memory footprint. In this paper, we present BlackMamba, a novel architecture that combines the Mamba SSM with MoE to obtain the benefits of both. We demonstrate that BlackMamba performs competitively against both Mamba and transformer baselines, and outperforms in inference and training FLOPs. We fully train and open-source 340M/1.5B and 630M/2.8B BlackMamba models on 300B tokens of a custom dataset. We show that BlackMamba inherits and combines both of the benefits
    
[^492]: 增强的物理信息神经网络用于动态泊松-纳宁斯特-普朗克系统

    Enriched Physics-informed Neural Networks for Dynamic Poisson-Nernst-Planck Systems

    [https://arxiv.org/abs/2402.01768](https://arxiv.org/abs/2402.01768)

    本文提出了一种增强的物理信息神经网络算法（EPINNs），用于解决具有强耦合和非线性特征的动态泊松-纳宁斯特-普朗克系统，该方法通过添加自适应损失权重和采用重采样策略来提高解算效率和精度。

    

    本文提出了一种无网格深度学习算法，称为增强的物理信息神经网络（EPINNs），用于求解具有强耦合和非线性特征的动态泊松-纳宁斯特-普朗克（PNP）方程。EPINNs将传统的物理信息神经网络作为基本框架，并加入了自适应损失权重来平衡损失函数，通过根据最大似然估计在每次迭代中更新参数自动分配损失的权重。在EPINNs中采用重采样策略来加速损失函数的收敛。同时，采用了GPU并行计算技术来加速求解过程。通过四个示例来证明所提方法的有效性和适用性。数值结果表明，与传统的数值方法相比，新方法在解决这种耦合非线性系统时具有更好的适用性。更重要的是，EPINNs更加准确。

    This paper proposes a meshless deep learning algorithm, enriched physics-informed neural networks (EPINNs), to solve dynamic Poisson-Nernst-Planck (PNP) equations with strong coupling and nonlinear characteristics. The EPINNs takes the traditional physics-informed neural networks as the foundation framework, and adds the adaptive loss weight to balance the loss functions, which automatically assigns the weights of losses by updating the parameters in each iteration based on the maximum likelihood estimate. The resampling strategy is employed in the EPINNs to accelerate the convergence of loss function. Meanwhile, the GPU parallel computing technique is adopted to accelerate the solving process. Four examples are provided to demonstrate the validity and effectiveness of the proposed method. Numerical results indicate that the new method has better applicability than traditional numerical methods in solving such coupled nonlinear systems. More importantly, the EPINNs is more accurate, st
    
[^493]: HiQA：一种用于大规模文档问答的分层上下文增强的RAG模型

    HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA

    [https://arxiv.org/abs/2402.01767](https://arxiv.org/abs/2402.01767)

    HiQA是一个先进的多文档问答框架，使用分层的上下文增强和多路径检索机制，解决了大规模文档问答中的检索准确性问题，并在多文档环境中展示了最先进的性能。

    

    随着利用外部工具的语言模型代理迅速发展，使用补充文档和检索增强生成（RAG）方法的问答（QA）方法学取得了重要进展。这种进步提高了语言模型的回答质量，并减轻了幻觉的出现。然而，当面临大量无法区分的文档时，这些方法在检索准确性方面表现有限，给实际应用带来了显著挑战。针对这些新兴的挑战，我们提出了HiQA，这是一个先进的多文档问答（MDQA）框架，将级联的元数据整合到内容中，同时具备多路径检索机制。我们还发布了一个名为MasQA的基准来评估和研究MDQA。最后，HiQA在多文档环境中展示了最先进的性能。

    As language model agents leveraging external tools rapidly evolve, significant progress has been made in question-answering(QA) methodologies utilizing supplementary documents and the Retrieval-Augmented Generation (RAG) approach. This advancement has improved the response quality of language models and alleviates the appearance of hallucination. However, these methods exhibit limited retrieval accuracy when faced with massive indistinguishable documents, presenting notable challenges in their practical application. In response to these emerging challenges, we present HiQA, an advanced framework for multi-document question-answering (MDQA) that integrates cascading metadata into content as well as a multi-route retrieval mechanism. We also release a benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA demonstrates the state-of-the-art performance in multi-document environments.
    
[^494]: LLM投票：人类选择和AI集体决策

    LLM Voting: Human Choices and AI Collective Decision Making

    [https://arxiv.org/abs/2402.01766](https://arxiv.org/abs/2402.01766)

    本文研究了大型语言模型（LLMs），特别是OpenAI的GPT4和LLaMA2的投票行为，并揭示了LLMs与人类在决策和偏见方面的差异。研究发现，在投票辅助中使用LLMs可能会导致更同质化的集体结果，强调了谨慎将LLMs整合到民主过程中的必要性。

    

    本文研究了大型语言模型（LLMs），特别是OpenAI的GPT4和LLaMA2的投票行为，并与人类投票模式进行了对比。我们的方法包括进行人类投票实验以建立人类偏好的基准，并与LLM代理进行平行实验。研究聚焦于集体结果和个体偏好，揭示了人类和LLMs之间在决策和固有偏见方面的差异。我们观察到LLMs在偏好多样性和一致性之间存在权衡，相比人类选民的多样偏好，LLMs有更趋向于一致选择的倾向。这一发现表明，在投票辅助中使用LLMs可能会导致更同质化的集体结果，强调了谨慎将LLMs整合到民主过程中的必要性。

    This paper investigates the voting behaviors of Large Language Models (LLMs), particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting patterns. Our approach included a human voting experiment to establish a baseline for human preferences and a parallel experiment with LLM agents. The study focused on both collective outcomes and individual preferences, revealing differences in decision-making and inherent biases between humans and LLMs. We observed a trade-off between preference diversity and alignment in LLMs, with a tendency towards more uniform choices as compared to the diverse preferences of human voters. This finding indicates that LLMs could lead to more homogenized collective outcomes when used in voting assistance, underscoring the need for cautious integration of LLMs into democratic processes.
    
[^495]: 当大型语言模型遇上向量数据库：一项综述

    When Large Language Models Meet Vector Databases: A Survey

    [https://arxiv.org/abs/2402.01763](https://arxiv.org/abs/2402.01763)

    本综述论文深入分析了大型语言模型和向量数据库之间的交叉点，大型语言模型的突破带来了新的挑战，而向量数据库提供了潜在的解决方案，可以显著增强人工智能系统管理和利用多样数据的能力。

    

    最近大型语言模型的突破在人类文字处理和生成方面开启了新的领域。然而，随着它们的显著增长，大型语言模型面临着包括幻觉、偏见、实时知识更新以及在商业环境中实施和维护的高成本等重要挑战。而另一种日益流行的工具，向量数据库则为这些挑战提供了潜在的解决方案。这些数据库擅长处理高维数据，并且对于高效的信息检索和语义搜索等任务至关重要。通过与大型语言模型的整合，它们显著增强了人工智能系统管理和更有效地利用多样数据的能力。本综述论文对大型语言模型和向量数据库之间的交叉点进行了深入而独特的分析。

    The recent burst in Large Language Models has opened new frontiers in human-like text processing and generation. However, alongside their remarkable growth, Large Language Models have encountered critical challenges including issues of hallucination, bias, real-time knowledge updates, and the high costs of implementation and maintenance in commercial settings. Vector Databases, another increasingly popular tool, offer potential solutions to these challenges. These databases are adept at handling high-dimensional data and are crucial for tasks such as efficient information retrieval and semantic search. By integrating with Large Language Models, they significantly enhance AI systems' ability to manage and utilize diverse data more effectively. This survey paper provides an in-depth and unique analysis of the intersection between Large Language Models and Vector Databases.
    
[^496]: 在大语言模型时代重新思考可解释性

    Rethinking Interpretability in the Era of Large Language Models

    [https://arxiv.org/abs/2402.01761](https://arxiv.org/abs/2402.01761)

    大语言模型具有以自然语言解释的能力，能够重新定义解释性，并且在多个应用中展示出巨大潜力。

    

    可解释的机器学习在过去十年中成为一个热门领域，受到越来越大的数据集和深度神经网络的崛起的推动。与此同时，大语言模型（LLMs）在各种任务中展示出了卓越的能力，为重新思考可解释机器学习的机会提供了可能。值得注意的是，以自然语言解释的能力使得LLMs能够扩展给人类的规模和复杂性上的模式。然而，这些新的能力也带来了新的挑战，比如虚构的解释和巨大的计算成本。在这篇立场论文中，我们首先回顾了评估新兴LLM解释领域的现有方法（包括解释LLM和使用LLM进行解释）。我们认为，尽管存在局限性，LLMs能够重新定义解释性，涵盖更广泛的应用领域，包括对LLMs本身的审计。

    Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs.   In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We high
    
[^497]: 系统性文献综述：用于幽默风格分类的计算方法

    Systematic Literature Review: Computational Approaches for Humour Style Classification

    [https://arxiv.org/abs/2402.01759](https://arxiv.org/abs/2402.01759)

    这项研究通过系统性文献综述展示了计算方法在幽默风格分类中的应用情况，并指出了当前的研究空白和有希望的方向。

    

    理解各种幽默风格对于理解幽默的多面性及其在心理学和人工智能等领域的影响至关重要。这种理解揭示了依据所采用的风格，幽默可以对个人的健康和人际关系产生治疗或有害的影响。虽然专门研究基于计算的幽默风格分析的研究仍然比较少见，但在相关任务中，特别是二元幽默和讽刺识别方面，已有大量研究蓬勃发展。在这项系统性文献综述中，我们调查了应用于这些相关任务的计算技术的现状，并揭示了它们与幽默风格分析的基本相关性。通过这项研究，我们揭示了常见的方法，阐明了各种数据集和评估指标，并有效地引导幽默研究的复杂领域。我们的努力确定了潜在的研究空白，并提出了有希望的方向。

    Understanding various humour styles is essential for comprehending the multifaceted nature of humour and its impact on fields such as psychology and artificial intelligence. This understanding has revealed that humour, depending on the style employed, can either have therapeutic or detrimental effects on an individual's health and relationships. Although studies dedicated exclusively to computational-based humour style analysis remain somewhat rare, an expansive body of research thrives within related task, particularly binary humour and sarcasm recognition. In this systematic literature review (SLR), we survey the landscape of computational techniques applied to these related tasks and also uncover their fundamental relevance to humour style analysis. Through this study, we unveil common approaches, illuminate various datasets and evaluation metrics, and effectively navigate the complex terrain of humour research. Our efforts determine potential research gaps and outlined promising di
    
[^498]: SpecDiff-GAN：一种用于语音和音乐合成的谱形状噪声扩散GAN

    SpecDiff-GAN: A Spectrally-Shaped Noise Diffusion GAN for Speech and Music Synthesis

    [https://arxiv.org/abs/2402.01753](https://arxiv.org/abs/2402.01753)

    SpecDiff-GAN 是一种基于 HiFi-GAN 的神经声码器，通过前向扩散过程来提高训练稳定性，并通过使用谱形状噪声分布使鉴别器任务更具挑战性。实验证实该模型在语音和音乐合成方面具有优越的音频质量和效率。

    

    生成对抗网络（GAN）模型可以合成高质量的音频信号，同时确保快速采样生成。然而，它们很难训练，并且容易出现模式坍缩和发散等问题。本文介绍了一种基于HiFi-GAN的神经声码器——SpecDiff-GAN，该模型最初用于从mel频谱图进行语音合成。在我们的模型中，通过一个前向扩散过程来增强训练稳定性，该过程在输入鉴别器之前向真实样本和伪造样本注入来自高斯分布的噪声。我们进一步改进了模型，通过利用谱形状噪声分布来使鉴别器的任务更具挑战性。然后，我们在几个数据集上展示了我们提出的模型在语音和音乐合成方面的优点。我们的实验证实，与几个基准模型相比，我们的模型在音频质量和效率方面表现出色。

    Generative adversarial network (GAN) models can synthesize highquality audio signals while ensuring fast sample generation. However, they are difficult to train and are prone to several issues including mode collapse and divergence. In this paper, we introduce SpecDiff-GAN, a neural vocoder based on HiFi-GAN, which was initially devised for speech synthesis from mel spectrogram. In our model, the training stability is enhanced by means of a forward diffusion process which consists in injecting noise from a Gaussian distribution to both real and fake samples before inputting them to the discriminator. We further improve the model by exploiting a spectrally-shaped noise distribution with the aim to make the discriminator's task more challenging. We then show the merits of our proposed model for speech and music synthesis on several datasets. Our experiments confirm that our model compares favorably in audio quality and efficiency compared to several baselines.
    
[^499]: 通过分析音频识别辱骂言论和假消息，辨别僞作和仇恨言论在僧伽罗语YouTube视频中

    Identifying False Content and Hate Speech in Sinhala YouTube Videos by Analyzing the Audio

    [https://arxiv.org/abs/2402.01752](https://arxiv.org/abs/2402.01752)

    该研究提出了一种通过分析音频来辨别僞作和仇恨言论在僧伽罗语YouTube视频中的解决方案。这些方法包括评估视频是否包含虚假信息以及检测其中是否存在仇恨言论。

    

    YouTube面临着全球范围内虚假信息和仇恨言论的传播危机。为解决这些问题，YouTube已实施严格规定，禁止上传包含虚假信息或宣传仇恨言论的内容。虽然已经进行了许多降低冒犯性英语内容的研究，但对僧伽罗语内容的研究仍然相对较少。本研究旨在通过提出解决方案，减少僧伽罗语YouTube视频中暴力和虚假信息的传播。该方法包括开发一个评级系统，通过比较标题和描述与音频内容，评估视频是否包含虚假信息，并检测其中是否存在仇恨言论。方法包括使用Pytube库进行音频提取，使用经过微调的Whisper模型进行音频转录，使用distilroberta-base模型进行仇恨言论检测和文本分类。

    YouTube faces a global crisis with the dissemination of false information and hate speech. To counter these issues, YouTube has implemented strict rules against uploading content that includes false information or promotes hate speech. While numerous studies have been conducted to reduce offensive English-language content, there's a significant lack of research on Sinhala content. This study aims to address the aforementioned gap by proposing a solution to minimize the spread of violence and misinformation in Sinhala YouTube videos. The approach involves developing a rating system that assesses whether a video contains false information by comparing the title and description with the audio content and evaluating whether the video includes hate speech. The methodology encompasses several steps, including audio extraction using the Pytube library, audio transcription via the fine-tuned Whisper model, hate speech detection employing the distilroberta-base model and a text classification L
    
[^500]: 迈向城市智能：城市基础模型综述与展望

    Towards Urban General Intelligence: A Review and Outlook of Urban Foundation Models

    [https://arxiv.org/abs/2402.01749](https://arxiv.org/abs/2402.01749)

    本文综述了城市基础模型在智能城市发展中的重要性和潜力，并提出了一个以数据为中心的分类方法。这个新兴领域面临着一些挑战，如缺乏清晰的定义和系统性的综述，需要进一步的研究和解决方案。

    

    机器学习技术现已成为智能城市服务进步的核心，对提高城市环境的效率、可持续性和宜居性起到至关重要的作用。最近出现的ChatGPT等基础模型在机器学习和人工智能领域标志着一个革命性的转变。它们在上下文理解、问题解决和适应各种任务方面的无与伦比的能力表明，将这些模型整合到城市领域中可能对智能城市的发展产生变革性影响。尽管对城市基础模型（UFMs）的兴趣日益增长，但这个新兴领域面临着一些挑战，如缺乏清晰的定义、系统性的综述和可普遍化的解决方案。为此，本文首先介绍了UFM的概念，并讨论了构建它们所面临的独特挑战。然后，我们提出了一个以数据为中心的分类方法，对当前与UFM相关的工作进行了分类。

    Machine learning techniques are now integral to the advancement of intelligent urban services, playing a crucial role in elevating the efficiency, sustainability, and livability of urban environments. The recent emergence of foundation models such as ChatGPT marks a revolutionary shift in the fields of machine learning and artificial intelligence. Their unparalleled capabilities in contextual understanding, problem solving, and adaptability across a wide range of tasks suggest that integrating these models into urban domains could have a transformative impact on the development of smart cities. Despite growing interest in Urban Foundation Models~(UFMs), this burgeoning field faces challenges such as a lack of clear definitions, systematic reviews, and universalizable solutions. To this end, this paper first introduces the concept of UFM and discusses the unique challenges involved in building them. We then propose a data-centric taxonomy that categorizes current UFM-related works, base
    
[^501]: 大型多模型(LMMs)作为AI原生无线系统的通用基础模型

    Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems

    [https://arxiv.org/abs/2402.01748](https://arxiv.org/abs/2402.01748)

    本文提出了大型多模型(LMMs)作为AI原生无线系统的通用基础模型的设计框架，通过处理多模态感知数据、通过因果推理和检索增强生成(RAG)将物理符号表示与无线系统联系起来，并通过无线环境反馈实现可教导性，从而促进动态网络配置。

    

    最近，大型语言模型(LLMs)和基础模型被宣称为6G系统的改变者。然而，目前关于无线网络的LLMs的努力仅限于直接应用现有的为自然语言处理(NLP)应用设计的语言模型。为了解决这一挑战，并创建以无线为中心的基础模型，本文提出了一个全面的视野，介绍了如何设计针对部署人工智能(AI)原生网络的通用基础模型。与基于NLP的基础模型不同，所提出的框架通过三个关键能力促进了大型多模型(LMMs)的设计：1) 处理多模态感知数据，2) 通过因果推理和检索增强生成(RAG)将物理符号表示与现实世界的无线系统联系起来，3) 通过无线环境反馈实现可教导性，以促进动态网络配置。

    Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language mod- els that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network a
    
[^502]: 使用生成性人工智能处理智能辅导系统学习者性能稀疏数据的3DG框架

    3DG: A Framework for Using Generative AI for Handling Sparse Learner Performance Data From Intelligent Tutoring Systems

    [https://arxiv.org/abs/2402.01746](https://arxiv.org/abs/2402.01746)

    3DG框架是一种通过将数据表示为三维张量并结合张量分解和生成性人工智能模型，来处理智能辅导系统中学习者性能稀疏数据的创新方法。

    

    学习者的学习表现数据（如测验分数和尝试次数）对于理解学习者参与度和知识掌握水平非常重要。然而，从智能辅导系统（ITS）收集的学习表现数据常常稀疏，影响学习者建模和知识评估的准确性。为了解决这个问题，我们引入了3DG框架（三维张量稠密生成与生成性模型结合），这是一种结合了张量分解和先进生成性模型（包括生成对抗网络（GAN）和生成预训练变换器（GPT））的创新方法，用于增强数据填补和扩充。该框架首先将数据表示为三维张量，捕捉学习者、问题和尝试次数这三个维度。然后，通过张量分解对数据进行稠密处理，并利用针对个体学习模式的聚类进行生成性人工智能模型的扩充。应用于AutoTutor的数据中。

    Learning performance data (e.g., quiz scores and attempts) is significant for understanding learner engagement and knowledge mastery level. However, the learning performance data collected from Intelligent Tutoring Systems (ITSs) often suffers from sparsity, impacting the accuracy of learner modeling and knowledge assessments. To address this, we introduce the 3DG framework (3-Dimensional tensor for Densification and Generation), a novel approach combining tensor factorization with advanced generative models, including Generative Adversarial Network (GAN) and Generative Pre-trained Transformer (GPT), for enhanced data imputation and augmentation. The framework operates by first representing the data as a three-dimensional tensor, capturing dimensions of learners, questions, and attempts. It then densifies the data through tensor factorization and augments it using Generative AI models, tailored to individual learning patterns identified via clustering. Applied to data from an AutoTutor
    
[^503]: 通过分层图解释揭示分子成分

    Unveiling Molecular Moieties through Hierarchical Graph Explainability

    [https://arxiv.org/abs/2402.01744](https://arxiv.org/abs/2402.01744)

    本论文提出了一种使用图神经网络和分层可解释人工智能技术的方法，能够准确预测生物活性并找到与之相关的最重要的成分。

    

    背景：图神经网络（GNN）作为一种强大的工具，在支持体外虚拟筛选方面已经出现多年。在这项工作中，我们提出了一种使用图卷积架构实现高精度多靶标筛选的GNN。我们还设计了一种分层可解释人工智能（XAI）技术，通过利用信息传递机制，在原子、环和整个分子层面上直接捕获信息，从而找到与生物活性预测相关的最重要的成分。结果：我们在支持虚拟筛选方面的二十个细胞周期依赖性激酶靶标上报道了一种最先进的GNN分类器。我们的分类器超越了作者提出的先前最先进方法。此外，我们还设计了一个仅针对CDK1的高灵敏度版本的GNN，以使用我们的解释器来避免多类别模型固有的偏差。分层解释器已经由一位专家化学家在19个CDK1批准药物上进行了验证。

    Background: Graph Neural Networks (GNN) have emerged in very recent years as a powerful tool for supporting in silico Virtual Screening. In this work we present a GNN which uses Graph Convolutional architectures to achieve very accurate multi-target screening. We also devised a hierarchical Explainable Artificial Intelligence (XAI) technique to catch information directly at atom, ring, and whole molecule level by leveraging the message passing mechanism. In this way, we find the most relevant moieties involved in bioactivity prediction. Results: We report a state-of-the-art GNN classifier on twenty Cyclin-dependent Kinase targets in support of VS. Our classifier outperforms previous SOTA approaches proposed by the authors. Moreover, a CDK1-only high-sensitivity version of the GNN has been designed to use our explainer in order to avoid the inherent bias of multi-class models. The hierarchical explainer has been validated by an expert chemist on 19 approved drugs on CDK1. Our explainer 
    
[^504]: 优化LLM使用成本的研究

    Towards Optimizing the Costs of LLM Usage

    [https://arxiv.org/abs/2402.01742](https://arxiv.org/abs/2402.01742)

    本论文提出了一种优化LLM使用成本的方法，通过估计输出质量并解决优化问题，实现在质量和延迟方面保持成本在预算范围内或最小化成本。

    

    生成式人工智能，特别是LLM在现今广泛应用于各种文件处理任务中，如问答和摘要。然而，不同的LLM在不同任务上具有不同的能力、成本、标记化和延迟。实际上，企业已经在为各自的用例运营或使用LLM而承担巨大的成本。在这项工作中，我们提出通过估计LLM的输出质量（而无需实际调用LLM），然后解决LLM选择的优化例程，以在质量和延迟方面保持成本在预算范围内或最小化成本。我们提出了一个模型来预测LLM在摘要等文件处理任务中的输出质量，随后采用LP取整算法来优化LLM的选择。我们从理论和实证的角度研究了在质量和成本之间权衡的优化问题。

    Generative AI and LLMs in particular are heavily used nowadays for various document processing tasks such as question answering and summarization. However, different LLMs come with different capabilities for different tasks as well as with different costs, tokenization, and latency. In fact, enterprises are already incurring huge costs of operating or using LLMs for their respective use cases.   In this work, we propose optimizing the usage costs of LLMs by estimating their output quality (without actually invoking the LLMs), and then solving an optimization routine for the LLM selection to either keep costs under a budget, or minimize the costs, in a quality and latency aware manner. We propose a model to predict the output quality of LLMs on document processing tasks like summarization, followed by an LP rounding algorithm to optimize the selection of LLMs. We study optimization problems trading off the quality and costs, both theoretically and empirically. We further propose a sente
    
[^505]: OpenMoE：开源混合专家语言模型的早期努力

    OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models

    [https://arxiv.org/abs/2402.01739](https://arxiv.org/abs/2402.01739)

    OpenMoE是一种开源的混合专家语言模型，通过训练和发布一系列具有可复现性的解码器模型，我们确认了MoE模型相比密集模型具有更有利的成本效益平衡，并且进行了对路由机制的深入分析，得出了三个重要发现。

    

    为了帮助开源社区更好地理解基于混合专家(MoE)的大型语言模型(LLM)，我们训练并发布了OpenMoE，一系列完全开放源码和可复现的仅解码器MoE LLM，参数范围从650M到34B，训练数据超过1T个标记。我们的研究证实，MoE-based LLM可以提供比密集LLM更有利的成本效益平衡，突出了未来LLM开发的潜在有效性。本研究的另一个重要贡献是对我们的OpenMoE模型中的路由机制进行深入分析，得到了三个重要发现：上下文无关专业化、早期路由学习和末尾降低。我们发现，MoE模型中的路由决策主要基于标记ID，与上下文相关性很小。标记到专家的分配在预训练阶段早期确定，并且基本保持不变。这种不完全的路由可能导致...

    To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development.   One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can resu
    
[^506]: CFTM: 连续时间分数话题模型

    CFTM: Continuous time fractional topic model

    [https://arxiv.org/abs/2402.01734](https://arxiv.org/abs/2402.01734)

    CFTM是一种新的动态主题建模方法，通过使用分数布朗运动来识别随时间变化的主题和词分布的正负相关性，揭示长期依赖性或粗糙度。实证研究结果表明，该模型能够有效地识别和跟踪主题的长期依赖性或粗糙度。

    

    本文提出了连续时间分数话题模型（cFTM），一种新的动态主题建模方法。该方法利用分数布朗运动（fBm）有效地识别主题和词分布随时间的正负相关性，揭示长期依赖性或粗糙度。我们的理论分析表明，cFTM可以捕捉到主题和词分布中的这些长期依赖性或粗糙度，反映了fBm的主要特征。此外，我们证明了cFTM的参数估计过程与传统主题模型LDA的相当。为了证明cFTM的性质，我们使用经济新闻文章进行了实证研究。这些测试的结果支持该模型能够识别和跟踪主题随时间的长期依赖性或粗糙度。

    In this paper, we propose the Continuous Time Fractional Topic Model (cFTM), a new method for dynamic topic modeling. This approach incorporates fractional Brownian motion~(fBm) to effectively identify positive or negative correlations in topic and word distribution over time, revealing long-term dependency or roughness. Our theoretical analysis shows that the cFTM can capture these long-term dependency or roughness in both topic and word distributions, mirroring the main characteristics of fBm. Moreover, we prove that the parameter estimation process for the cFTM is on par with that of LDA, traditional topic models. To demonstrate the cFTM's property, we conduct empirical study using economic news articles. The results from these tests support the model's ability to identify and track long-term dependency or roughness in topics over time.
    
[^507]: CERM: 通过情感分析进行基于文献的上下文感知发现

    CERM: Context-aware Literature-based Discovery via Sentiment Analysis

    [https://arxiv.org/abs/2402.01724](https://arxiv.org/abs/2402.01724)

    CERM是一个通过情感分析进行基于文献的上下文感知发现的系统，旨在理解食品与健康之间的关系。通常情况下，基于食材营养成分或基于标记数据的计算模型已被用于食谱推荐和分析系统。然而，本研究提出了一种增强模型，通过捕捉食材与生物医学概念之间的固有关系，利用标记和未标记的数据来更好地支持食品相关研究。

    

    鉴于生物医学出版物的丰富，我们引入了一项情感分析任务来理解食品与健康之间的关系。之前将健康纳入食谱推荐和分析系统的尝试主要集中在食材营养成分上，或者利用基于标记数据的基本计算模型进行训练。捕捉食材和生物医学概念之间固有关系的增强模型对于食品相关研究更有益处，鉴于生物医学文本中的丰富信息。考虑到昂贵的数据标记过程，这些模型应该有效利用标记和未标记的数据。本文介绍了一项名为实体关系情感分析（ERSA）的新任务，该任务基于实体对捕捉文本的情感。ERSA扩展了广泛研究的基于方面的情感分析（ABSA）任务。具体而言，我们的研究集中在应用于生物医学文本的ERSA任务上，重点关注(entity-ent

    Driven by the abundance of biomedical publications, we introduce a sentiment analysis task to understand food-health relationship. Prior attempts to incorporate health into recipe recommendation and analysis systems have primarily focused on ingredient nutritional components or utilized basic computational models trained on curated labeled data. Enhanced models that capture the inherent relationship between food ingredients and biomedical concepts can be more beneficial for food-related research, given the wealth of information in biomedical texts. Considering the costly data labeling process, these models should effectively utilize both labeled and unlabeled data. This paper introduces Entity Relationship Sentiment Analysis (ERSA), a new task that captures the sentiment of a text based on an entity pair. ERSA extends the widely studied Aspect Based Sentiment Analysis (ABSA) task. Specifically, our study concentrates on the ERSA task applied to biomedical texts, focusing on (entity-ent
    
[^508]: 基于深度学习的阿姆哈拉语常见问题解答聊天机器人

    Deep Learning Based Amharic Chatbot for FAQs in Universities

    [https://arxiv.org/abs/2402.01720](https://arxiv.org/abs/2402.01720)

    本文提出了一个基于深度学习的阿姆哈拉语常见问题解答聊天机器人模型，可以帮助大学生解答常见问题，通过使用自然语言处理和深度学习技术，采用多种机器学习模型算法进行分析和分类，取得了最好的成绩。

    

    大学生常常花费大量时间向管理员或教师寻求常见问题的答案。这对双方来说都很繁琐，需要找到一个解决方案。为此，本文提出了一个聊天机器人模型，利用自然语言处理和深度学习技术，在阿姆哈拉语中回答常见问题。聊天机器人是通过人工智能模拟人类对话的计算机程序，作为虚拟助手处理问题和其他任务。所提出的聊天机器人程序使用标记化、规范化、去除停用词和词干提取对阿姆哈拉语输入句子进行分析和分类。采用了三种机器学习模型算法来分类标记和检索合适的回答：支持向量机（SVM）、多项式朴素贝叶斯和通过TensorFlow、Keras和NLTK实现的深度神经网络。深度学习模型取得了最好的成绩。

    University students often spend a considerable amount of time seeking answers to common questions from administrators or teachers. This can become tedious for both parties, leading to a need for a solution. In response, this paper proposes a chatbot model that utilizes natural language processing and deep learning techniques to answer frequently asked questions (FAQs) in the Amharic language. Chatbots are computer programs that simulate human conversation through the use of artificial intelligence (AI), acting as a virtual assistant to handle questions and other tasks. The proposed chatbot program employs tokenization, normalization, stop word removal, and stemming to analyze and categorize Amharic input sentences. Three machine learning model algorithms were used to classify tokens and retrieve appropriate responses: Support Vector Machine (SVM), Multinomial Na\"ive Bayes, and deep neural networks implemented through TensorFlow, Keras, and NLTK. The deep learning model achieved the be
    
[^509]: 在大型语言模型中测量道德不一致性

    Measuring Moral Inconsistencies in Large Language Models

    [https://arxiv.org/abs/2402.01719](https://arxiv.org/abs/2402.01719)

    本研究提出了一种新的信息论度量方法，称为语义图熵（SGE），用于测量道德情景中大型语言模型（LLM）的一致性。与现有的一致性度量方法相比，SGE在五个LLMs上与人类判断更好地相关，为研究LLM不一致性的根本原因提供了新的思路。

    

    如果语义等价的提示产生语义等价的响应，那么大型语言模型(LLM)被认为是一致的。尽管最近的进展展示了LLMs在对话系统中令人印象深刻的能力，但我们表明即使是最先进的LLMs在生成方面也存在高度不一致性，这对它们的可靠性提出了质疑。先前的研究尝试用任务特定的准确度来衡量这一点。然而，这种方法对于没有“正确”答案的道德情景（例如，道路交运问题）是不合适的。为了解决这个问题，我们提出了一种新的信息论度量方法，称为语义图熵（SGE），来衡量LLM在道德情景中的一致性。我们利用“经验法则”（RoTs）来解释模型的决策策略，并进一步增强我们的度量方法。与现有的一致性度量方法相比，SGE与人类判断在五个LLMs上更好地相关。在未来，我们的目标是调查LLM不一致性的根本原因。

    A Large Language Model~(LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracies. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no ``correct'' answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy~(SGE) to measure the consistency of an LLM in moral scenarios. We leverage ``Rules of Thumb''~(RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies 
    
[^510]: Bloom-认知和情感分析层次分类在课程讨论论坛中的应用

    Bloom-epistemic and sentiment analysis hierarchical classification in course discussion forums

    [https://arxiv.org/abs/2402.01716](https://arxiv.org/abs/2402.01716)

    本研究提出了一种称为布鲁姆-认知和情感分析（BE-Sent）的层次化方法，用于评估教育讨论论坛中的情绪和布鲁姆的认知分类。方法包括数据收集、文本预处理、情感分析和认知分类。研究结果有助于了解学生在学习过程中的进展情况和知识水平。

    

    在线讨论论坛广泛被用于讲师和学生之间的积极文本交流，以及检查学生在学习过程中的进展情况。本研究的目标是比较适合的机器学习模型，以评估教育讨论论坛中基于文本评论的情绪和布鲁姆的认知分类。我们提出的方法被称为布鲁姆认知和情感分析的层次化方法（BE-Sent）。研究方法包括三个主要步骤。第一步是从内部讨论论坛和YouTube频道的评论中收集数据。下一步是对文本进行预处理，对文本进行标注并清除不重要的单词。此外，对已成功清理的文本数据集，将在每个句子中进行情感分析和认知分类。情感分析分为三个类别：积极的，消极的和中性的。布鲁姆（Bloom）的认知分类是根据认知过程的六个层次进行的，从低层次的知识记忆到高层次的评价和创造。

    Online discussion forums are widely used for active textual interaction between lecturers and students, and to see how the students have progressed in a learning process. The objective of this study is to compare appropriate machine-learning models to assess sentiments and Bloom\'s epistemic taxonomy based on textual comments in educational discussion forums. Our proposed method is called the hierarchical approach of Bloom-Epistemic and Sentiment Analysis (BE-Sent). The research methodology consists of three main steps. The first step is the data collection from the internal discussion forum and YouTube comments of a Web Programming channel. The next step is text preprocessing to annotate the text and clear unimportant words. Furthermore, with the text dataset that has been successfully cleaned, sentiment analysis and epistemic categorization will be done in each sentence of the text. Sentiment analysis is divided into three categories: positive, negative, and neutral. Bloom\'s epistem
    
[^511]: TrICy: 通过意图感知的关注-复制机制引导的数据到文本生成

    TrICy: Trigger-guided Data-to-text Generation with Intent aware Attention-Copy

    [https://arxiv.org/abs/2402.01714](https://arxiv.org/abs/2402.01714)

    TrICy是一种轻量级框架，利用意图和触发器引导数据到文本生成任务，并通过关注-复制机制有效地处理了词汇表之外的词汇。实验结果表明其在不同数据集上取得了良好的性能。

    

    数据到文本（D2T）生成是许多自然语言理解（NLU）应用中的关键任务，也是面向任务导向对话系统的基础。在可以直接与用户设备上的本地数据一起工作的会话型人工智能解决方案中，利用大型预训练语言模型（PLMs）的架构由于高内存占用而无法在设备上部署。为此，我们提出了TrICy，一种新颖的轻量级框架，用于增强D2T任务，根据上下文中的意图生成文本序列，并可进一步由用户提供的触发器进行引导。我们利用关注-复制机制准确地预测了词汇表之外的词汇（OOV）。对E2E NLG数据集（BLEU：66.43％，ROUGE-L：70.14％），WebNLG数据集（BLEU：Seen 64.08％，Unseen 52.35％）和与文本消息应用相关的自定义数据集的性能分析展示了我们的架构的有效性。此外，我们展示了通过利用可选的触发器输入，数据

    Data-to-text (D2T) generation is a crucial task in many natural language understanding (NLU) applications and forms the foundation of task-oriented dialog systems. In the context of conversational AI solutions that can work directly with local data on the user's device, architectures utilizing large pre-trained language models (PLMs) are impractical for on-device deployment due to a high memory footprint. To this end, we propose TrICy, a novel lightweight framework for an enhanced D2T task that generates text sequences based on the intent in context and may further be guided by user-provided triggers. We leverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words accurately. Performance analyses on E2E NLG dataset (BLEU: 66.43%, ROUGE-L: 70.14%), WebNLG dataset (BLEU: Seen 64.08%, Unseen 52.35%), and our Custom dataset related to text messaging applications, showcase our architecture's effectiveness. Moreover, we show that by leveraging an optional trigger input, data
    
[^512]: 使用结构化纵向电子健康记录数据促使大型语言模型进行零样本临床预测

    Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data

    [https://arxiv.org/abs/2402.01713](https://arxiv.org/abs/2402.01713)

    本研究探索了将大型语言模型（LLMs）应用于结构化纵向电子健康记录（EHR）数据的可行性，并着重研究了其零样本能力。通过考虑EHR特征和临床上下文，我们的方法在MIMIC-IV和TJH数据集上取得了良好的实验结果。

    

    结构化纵向电子健康记录（EHR）数据的固有复杂性使其与传统上为自然语言处理而设计的大型语言模型（LLM）整合时面临重大挑战。受新疾病爆发时迅速决策的紧迫需求的驱使，本研究调查了类似GPT-4的LLM对EHR数据的适应性。我们特别关注它们的零样本能力，即在没有明确训练的情况下进行预测。针对EHR数据的纵向、稀疏和知识注入的特点，我们的提示方法考虑了特定的EHR特征，如单位和参考范围，并采用了与临床上下文相一致的上下文学习策略。通过在MIMIC-IV和TJH数据集上进行全面实验，我们证明了LLM能够通过我们的方法进行零样本临床预测，有效应对了EHR数据的挑战。

    The inherent complexity of structured longitudinal Electronic Health Records (EHR) data poses a significant challenge when integrated with Large Language Models (LLMs), which are traditionally tailored for natural language processing. Motivated by the urgent need for swift decision-making during new disease outbreaks, where traditional predictive models often fail due to a lack of historical data, this research investigates the adaptability of LLMs, like GPT-4, to EHR data. We particularly focus on their zero-shot capabilities, which enable them to make predictions in scenarios in which they haven't been explicitly trained. In response to the longitudinal, sparse, and knowledge-infused nature of EHR data, our prompting approach involves taking into account specific EHR characteristics such as units and reference ranges, and employing an in-context learning strategy that aligns with clinical contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets demonstrate that with o
    
[^513]: 基于大型语言模型的社会感知式合成数据生成用于自杀意念检测

    Socially Aware Synthetic Data Generation for Suicidal Ideation Detection Using Large Language Models

    [https://arxiv.org/abs/2402.01712](https://arxiv.org/abs/2402.01712)

    通过利用生成式AI模型，我们提出了一种基于社会感知的合成数据生成方法，用于自杀意念检测。与传统模型相比，我们的方法在实际数据集上表现出有竞争力的性能。

    

    自杀意念检测是一个重要的研究领域，对于改进心理健康支持系统具有巨大潜力。然而，在自杀相关数据周围的敏感性导致难以访问到大规模的、注释的数据集，这是训练有效的机器学习模型所必需的。为了解决这个限制，我们引入了一种创新的策略，利用ChatGPT，Flan-T5和Llama等生成式AI模型的能力，为自杀意念检测创建合成数据。我们的数据生成方法基于从心理学文献中提取的社会因素，并旨在确保涵盖与自杀意念相关的基本信息。在我们的研究中，我们与基于BERT系列结构的现有NLP分类模型进行了基准测试。当在现实世界的UMD数据集上训练时，这些传统模型的F1分数通常在0.75到0.87之间。我们的合成数据驱动方法，提供了一种有效的替代选择，产生具有竞争力的性能。

    Suicidal ideation detection is a vital research area that holds great potential for improving mental health support systems. However, the sensitivity surrounding suicide-related data poses challenges in accessing large-scale, annotated datasets necessary for training effective machine learning models. To address this limitation, we introduce an innovative strategy that leverages the capabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to create synthetic data for suicidal ideation detection. Our data generation approach is grounded in social factors extracted from psychology literature and aims to ensure coverage of essential information related to suicidal ideation. In our study, we benchmarked against state-of-the-art NLP classification models, specifically, those centered around the BERT family structures. When trained on the real-world dataset, UMD, these conventional models tend to yield F1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, i
    
[^514]: 探索教育公平：揭示乔治亚州成就差距的机器学习方法

    Exploring Educational Equity: A Machine Learning Approach to Unravel Achievement Disparities in Georgia

    [https://arxiv.org/abs/2402.01710](https://arxiv.org/abs/2402.01710)

    COVID-19疫情对乔治亚州K-12教育系统造成了严重的不平等现象，尤其是在种族和民族成就差距方面。研究利用机器学习方法分析了不同人口统计学特征、地区和学科的学生成绩，发现了英语和数学熟练程度的显著下降，并且进一步探讨了社会经济地位和地理因素对学业成就的影响。

    

    COVID-19疫情显著加剧了乔治亚州K-12教育系统现有的不平等现象，尤其是在种族和民族成就差距方面。利用机器学习方法，该研究对不同人口统计学特征、地区和学科的学生成绩进行了全面分析。研究发现，在疫情期间英语和数学的熟练程度显著下降，分数分布收缩并对经济困境和黑人学生产生更大影响。直接证明的百分比（Directly Certified Percentage）代表的社会经济地位成为最关键的因素，此外还从教师资源（如教师工资）和教学开支中获得了其他洞见。研究还发现了城市和农村环境之间的成就差异，以及不同县之间的变化，凸显了地理因素的影响。

    The COVID-19 pandemic has significantly exacerbated existing educational disparities in Georgia's K-12 system, particularly in terms of racial and ethnic achievement gaps. Utilizing machine learning methods, the study conducts a comprehensive analysis of student achievement rates across different demographics, regions, and subjects. The findings highlight a significant decline in proficiency in English and Math during the pandemic, with a noticeable contraction in score distribution and a greater impact on economically disadvantaged and Black students. Socio-economic status, as represented by the Directly Certified Percentage -- the percentage of students eligible for free lunch, emerges as the most crucial factor, with additional insights drawn from faculty resources such as teacher salaries and expenditure on instruction. The study also identifies disparities in achievement rates between urban and rural settings, as well as variations across counties, underscoring the influence of ge
    
[^515]: MULTIVERSE: 在不同世界中揭示大型语言模型对齐问题

    MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds

    [https://arxiv.org/abs/2402.01706](https://arxiv.org/abs/2402.01706)

    本文通过构建多种语境，使用领域特定语言描述可能世界，并利用编译器，发现了大型语言模型在不同语境下的对齐问题。这种方法成本较低，能够更全面地研究LLM对齐问题。

    

    大型语言模型（LLM）对齐旨在确保LLM的输出与人类价值相匹配。研究人员通过一系列越狱技术展示了对齐问题的严重性，这些技术可以在对话中诱使LLMs产生恶意内容。通常需要大量的人类智能或计算资源才能找到相应的越狱提示。本文报告了LLMs在不同语境下对齐水平的差异。因此，通过系统地构建许多被称为世界的语境、利用描述可能世界（如时间、地点、角色、行为和语言）的领域特定语言和相应的编译器，我们能够以较低成本揭示潜在的对齐问题。鉴于我们方法的低成本，我们能够对不同世界中LLM对齐问题进行大规模研究。我们的结果表明，我们的方法在效果上优于现有的越狱技术。

    Large Language Model (LLM) alignment aims to ensure that LLM outputs match with human values. Researchers have demonstrated the severity of alignment problems with a large spectrum of jailbreak techniques that can induce LLMs to produce malicious content during conversations. Finding the corresponding jailbreaking prompts usually requires substantial human intelligence or computation resources. In this paper, we report that LLMs have different levels of alignment in various contexts. As such, by systematically constructing many contexts, called worlds, leveraging a Domain Specific Language describing possible worlds (e.g., time, location, characters, actions and languages) and the corresponding compiler, we can cost-effectively expose latent alignment issues. Given the low cost of our method, we are able to conduct a large scale study regarding LLM alignment issues in different worlds. Our results show that our method outperforms the-state-of-the-art jailbreaking techniques on both eff
    
[^516]: 超越行为主义的表征伤害：度量和减轻计划

    Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation

    [https://arxiv.org/abs/2402.01705](https://arxiv.org/abs/2402.01705)

    本研究超越行为主义的定义范围，提出了一种度量和减轻表征性伤害的框架，强调了大型语言模型在实施这些伤害时的脆弱性，并提出了减轻措施的建议。

    

    算法伤害通常被分为配置性或表征性。本研究专门针对后者，重点在于对当前表征性伤害定义的审查，以确定其中包含什么和不包含什么。这个分析促使我们扩展超越行为主义的定义范围，包括对认知和情感状态的伤害。本文概述了度量的高级要求：确定实施这种方法所需的专业知识，并通过案例研究进行说明。我们的工作凸显了大型语言模型在实施表征性伤害时的独特脆弱性，特别是当这些伤害未被度量和减轻时。该研究通过提出减轻措施并界定何时使用它们来结束。这项研究的总体目标是建立一个框架，扩大表征性伤害的定义，并将公平研究的见解转化为实际的度量方法。

    Algorithmic harms are commonly categorized as either allocative or representational. This study specifically addresses the latter, focusing on an examination of current definitions of representational harms to discern what is included and what is not. This analysis motivates our expansion beyond behavioral definitions to encompass harms to cognitive and affective states. The paper outlines high-level requirements for measurement: identifying the necessary expertise to implement this approach and illustrating it through a case study. Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated. The work concludes by presenting proposed mitigations and delineating when to employ them. The overarching aim of this research is to establish a framework for broadening the definition of representational harms and to translate insights from fairness research into practical measurement 
    
[^517]: 一种多角度的机器学习方法用于评估洛杉矶警察与司机的互动

    A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles

    [https://arxiv.org/abs/2402.01703](https://arxiv.org/abs/2402.01703)

    该研究提出了一种多角度的机器学习方法，用于分析洛杉矶警察与司机的互动。该方法利用多模态的数据包括音频、视频和文字信息，旨在提供对复杂和有争议的警民互动的分析工具。

    

    政府官员与市民之间的互动影响公共福祉和民主社会的正当性。警察是国家最显而易见、最接触市民的代理人，在交通站停期间，他们每年与公众互动超过2000万次。如今，这些互动经常被戴在身上的摄像机记录下来，这被视为提高警察问责制和改善警民互动的手段。然而，由于缺乏可靠的自动化工具来分析这些复杂而有争议的警民互动，这些记录的及时分析受到了阻碍。本文提出了一种新的多角度、多模态机器学习（ML）工具的方法，用于分析来自这些身上摄像机记录的音频、视频和文字信息。我们的方法首先确定与不同利益相关者最相关的沟通方面，包括共同感知互动的标志标记以及具有这些标记的符号。

    Interactions between the government officials and civilians affect public wellbeing and the state legitimacy that is necessary for the functioning of democratic society. Police officers, the most visible and contacted agents of the state, interact with the public more than 20 million times a year during traffic stops. Today, these interactions are regularly recorded by body-worn cameras (BWCs), which are lauded as a means to enhance police accountability and improve police-public interactions. However, the timely analysis of these recordings is hampered by a lack of reliable automated tools that can enable the analysis of these complex and contested police-public interactions. This article proposes an approach to developing new multi-perspective, multimodal machine learning (ML) tools to analyze the audio, video, and transcript information from this BWC footage. Our approach begins by identifying the aspects of communication most salient to different stakeholders, including both commun
    
[^518]: HiGen: 层次感知的层级文本分类序列生成

    HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text Classification

    [https://arxiv.org/abs/2402.01696](https://arxiv.org/abs/2402.01696)

    HiGen提出了一个基于文本生成的框架，利用语言模型来编码动态文本表示，在层次文本分类中考虑了文档各个部分的相关性，并引入了一个层级引导的损失函数。此外，还提供了一个新颖的用于HTC的数据集ENZYME。

    

    层次文本分类（HTC）是多标签文本分类中的一个复杂子任务，其特点是具有层级标签分类法和数据不平衡。最佳性能模型旨在通过结合文档和层级标签信息来学习静态表示。然而，文档各个部分的相关性可能因层级水平的不同而变化，需要动态的文档表示。为了解决这个问题，我们提出了HiGen，一个利用语言模型编码动态文本表示的基于文本生成的框架。我们引入了一种层级引导的损失函数，以捕捉文本和标签名称语义之间的关系。我们的方法采用了一个特定任务的预训练策略，将语言模型调整到领域知识上，并显著提高了对样本有限的类别的性能。此外，我们还提供了一个命名为ENZYME的新颖和有价值的用于HTC的数据集，该数据集由来自PubMed的文章组成，旨在预测...

    Hierarchical text classification (HTC) is a complex subtask under multi-label text classification, characterized by a hierarchical label taxonomy and data imbalance. The best-performing models aim to learn a static representation by combining document and hierarchical label information. However, the relevance of document sections can vary based on the hierarchy level, necessitating a dynamic document representation. To address this, we propose HiGen, a text-generation-based framework utilizing language models to encode dynamic text representations. We introduce a level-guided loss function to capture the relationship between text and label name semantics. Our approach incorporates a task-specific pretraining strategy, adapting the language model to in-domain knowledge and significantly enhancing performance for classes with limited examples. Furthermore, we present a new and valuable dataset called ENZYME, designed for HTC, which comprises articles from PubMed with the goal of predicti
    
[^519]: 语言引导的世界模型：一种基于模型的人工智能控制方法

    Language-Guided World Models: A Model-Based Approach to AI Control

    [https://arxiv.org/abs/2402.01695](https://arxiv.org/abs/2402.01695)

    语言引导的世界模型（LWMs）是一种基于模型的人工智能控制方法，它通过阅读语言描述来捕捉环境动态，提高了代理的沟通效率，并允许人类通过简洁的语言反馈同时改变他们在多个任务上的行为。

    

    将概率世界模型安装到人工智能代理中，为人类与这些代理沟通和控制打开了一个高效的渠道。除了更新代理策略，人类还可以修改他们的内部世界模型，以影响代理的决策。然而，当前现有的世界模型难以适应人类，因为它们缺乏自然的通信界面。为了解决这个问题，我们开发了语言引导的世界模型（LWMs），它们可以通过阅读语言描述来捕捉环境动态。这些模型提高了代理的沟通效率，使人类能够通过简洁的语言反馈同时改变他们在多个任务上的行为。它们还使代理能够从最初用于指导人类的文本中进行自我学习。为了促进LWMs的发展，我们设计了一个基于MESSENGER游戏（Hanjie等人，2021）的挑战基准，需要对新场景进行组合泛化。

    Installing probabilistic world models into artificial agents opens an efficient channel for humans to communicate with and control these agents. In addition to updating agent policies, humans can modify their internal world models in order to influence their decisions. The challenge, however, is that currently existing world models are difficult for humans to adapt because they lack a natural communication interface. Aimed at addressing this shortcoming, we develop Language-Guided World Models (LWMs), which can capture environment dynamics by reading language descriptions. These models enhance agent communication efficiency, allowing humans to simultaneously alter their behavior on multiple tasks with concise language feedback. They also enable agents to self-learn from texts originally written to instruct humans. To facilitate the development of LWMs, we design a challenging benchmark based on the game of MESSENGER (Hanjie et al., 2021), requiring compositional generalization to new l
    
[^520]: ARGS: 对齐作为奖励导向的搜索

    ARGS: Alignment as Reward-Guided Search

    [https://arxiv.org/abs/2402.01694](https://arxiv.org/abs/2402.01694)

    ARGS是一个对齐作为奖励导向的搜索框架，通过在解码过程中将模型的概率预测调整为奖励信号，实现生成具有语义多样性且与人类偏好对齐的文本。与基线相比，在不同任务和模型维度下，ARGS具有持续的奖励增益，表现出很好的性能。

    

    将大规模语言模型与人类目标对齐是至关重要的，然而常见的方法包括RLHF在训练过程中存在不稳定和资源密集的问题。为应对这一挑战，我们引入了一个新的框架ARGS，即对齐作为奖励导向的搜索，它将对齐融入到解码过程中，消除了昂贵的RL训练的需求。通过使用奖励信号调整模型的概率预测，ARGS生成具有语义多样性的文本，同时与人类偏好对齐，为对齐语言模型提供了一种有前景且灵活的解决方案。值得注意的是，在不同的对齐任务和不同的模型维度下，ARGS相对于基线显示出持续的奖励改进。例如，采用相同的贪婪解码策略，我们的方法相对于基线提高了19.56%的平均奖励，并在GPT-4评估中获得了64.33%的偏好或并列分数。我们相信，我们的框架强调了解码的创新性和效果。

    Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training. In response to this challenge, we introduce ARGS, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the decoding process, eliminating the need for expensive RL training. By adjusting the model's probabilistic predictions using a reward signal, ARGS generates texts with semantic diversity while being aligned with human preferences, offering a promising and flexible solution for aligning language models. Notably, ARGS demonstrates consistent enhancements in average reward compared to baselines across diverse alignment tasks and various model dimensions. For example, under the same greedy-based decoding strategy, our method improves the average reward by 19.56% relative to the baseline and secures a preference or tie score of 64.33% in GPT-4 evaluation. We believe that our framework, emphasizing deco
    
[^521]: 通过自监督表示混合和嵌入初始化实现跨语言TTS自适应的最大数据效率

    Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by Self-Supervised Representation Mixing and Embedding Initialization

    [https://arxiv.org/abs/2402.01692](https://arxiv.org/abs/2402.01692)

    本文提出了一种有效的转移学习框架，用于通过最少的标记和未标记数据实现语言自适应。该方法使用自监督特征进行预训练，在细调期间替换伪标签噪声部分，并结合嵌入初始化技巧，有效利用未标记数据的信息。实验证明，即使在仅有很少的数据情况下，该框架也能合成可理解的未知语言语音，并超越传统技术。这一研究结果展示了该高效语言自适应框架的潜力。

    

    本文提出了一种有效的转移学习框架，用于文本到语音系统中的语言自适应，重点是使用最少的标记和未标记数据实现语言自适应。虽然许多工作侧重于减少标记数据的使用，但很少有人考虑尽量减少未标记数据的使用。通过在预训练阶段利用自监督特征，替换细调期间伪标签中的噪声部分，并结合嵌入初始化技巧，我们的方法与传统方法相比利用了更多未标记数据的信息。实验结果表明，我们的框架能够使用仅4个标记数据和15分钟未标记数据合成可理解的未知语言语音。我们的方法即使在更多数据可用的情况下，仍然超过了传统技术。这些发现突显了我们的高效语言自适应框架的潜力。

    This paper presents an effective transfer learning framework for language adaptation in text-to-speech systems, with a focus on achieving language adaptation using minimal labeled and unlabeled data. While many works focus on reducing the usage of labeled data, very few consider minimizing the usage of unlabeled data. By utilizing self-supervised features in the pretraining stage, replacing the noisy portion of pseudo labels with these features during fine-tuning, and incorporating an embedding initialization trick, our method leverages more information from unlabeled data compared to conventional approaches. Experimental results show that our framework is able to synthesize intelligible speech in unseen languages with only 4 utterances of labeled data and 15 minutes of unlabeled data. Our methodology continues to surpass conventional techniques, even when a greater volume of data is accessible. These findings highlight the potential of our data-efficient language adaptation framework.
    
[^522]: 基于信息损失的语言学方法用于检测轻度认知障碍

    Linguistic-Based Mild Cognitive Impairment Detection Using Informative Loss

    [https://arxiv.org/abs/2402.01690](https://arxiv.org/abs/2402.01690)

    本论文提出了一种基于自然语言处理技术的深度学习方法，用于在老年人中区分轻度认知障碍和正常认知条件。该方法使用了句子嵌入和句子交叉注意力模块，通过分析视频访谈的转录文本，提取时序特征进行分类。同时，引入了一种新的损失函数来建立稳健的模型。

    

    本论文提出了一种使用自然语言处理（NLP）技术的深度学习方法，以区分老年人中的轻度认知障碍（MCI）和正常认知（NC）条件。我们提出了一个框架，该框架分析了在I-CONECT研究项目中收集的视频访谈中生成的转录文本，该项目是一项旨在通过视频聊天改善认知功能的随机对照试验。我们提出的NLP框架包括两个基于Transformer的模块，即句子嵌入（SE）和句子交叉注意力（SCA）。首先，SE模块捕捉每个句子中单词之间的上下文关系。接下来，SCA模块提取句子序列的时序特征。然后，这些特征由多层感知器（MLP）用于将被试分为MCI或NC。为了建立一个稳健的模型，我们提出了一种新的损失函数，称为信息损失（InfoLoss），该函数通过观察每个句子序列来考虑熵的减少。

    This paper presents a deep learning method using Natural Language Processing (NLP) techniques, to distinguish between Mild Cognitive Impairment (MCI) and Normal Cognitive (NC) conditions in older adults. We propose a framework that analyzes transcripts generated from video interviews collected within the I-CONECT study project, a randomized controlled trial aimed at improving cognitive functions through video chats. Our proposed NLP framework consists of two Transformer-based modules, namely Sentence Embedding (SE) and Sentence Cross Attention (SCA). First, the SE module captures contextual relationships between words within each sentence. Subsequently, the SCA module extracts temporal features from a sequence of sentences. This feature is then used by a Multi-Layer Perceptron (MLP) for the classification of subjects into MCI or NC. To build a robust model, we propose a novel loss function, called InfoLoss, that considers the reduction in entropy by observing each sequence of sentences
    
[^523]: “我应该使用哪个LLM？”：评估用于印度本科计算机科学学生任务的LLMs

    "Which LLM should I use?": Evaluating LLMs for tasks performed by Undergraduate Computer Science Students in India

    [https://arxiv.org/abs/2402.01687](https://arxiv.org/abs/2402.01687)

    本研究评估了各种LLMs在本科计算机科学学生常见任务中的效果，并指导学生选择适合他们的LLM.

    

    本研究评估了各种大型语言模型（LLMs）在本科计算机科学学生常见任务中的效果。尽管计算机教育界的许多研究已经探讨了使用LLMs执行各种任务的可能性，但缺乏对不同LLMs进行综合比较和评估哪个LLMs对不同任务最有效的研究。我们的研究系统地评估了一些公开可用的LLMs，例如Google Bard，ChatGPT，GitHub Copilot Chat和Microsoft Copilot在本科计算机科学学生常遇到的不同任务中的表现。这些任务包括代码生成，解释，项目构思，内容生成，课程作业和电子邮件撰写。计算机科学的高年级和低年级学生进行了这些任务的评估，并提供了对模型的优点和局限性的见解。该研究旨在指导学生选择适合他们的LLM。

    This study evaluates the effectiveness of various large language models (LLMs) in performing tasks common among undergraduate computer science students. Although a number of research studies in the computing education community have explored the possibility of using LLMs for a variety of tasks, there is a lack of comprehensive research comparing different LLMs and evaluating which LLMs are most effective for different tasks. Our research systematically assesses some of the publicly available LLMs such as Google Bard, ChatGPT, GitHub Copilot Chat, and Microsoft Copilot across diverse tasks commonly encountered by undergraduate computer science students. These tasks include code generation, explanation, project ideation, content generation, class assignments, and email composition. Evaluation for these tasks was carried out by junior and senior students in computer science, and provides insights into the models' strengths and limitations. This study aims to guide students in selecting su
    
[^524]: 使用CGC-LORA算法在LLMs中实现1 + N多任务微调模式的框架

    A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm

    [https://arxiv.org/abs/2402.01684](https://arxiv.org/abs/2402.01684)

    提出了一个使用CGC-LORA算法在LLMs中实现1 + N多任务微调模式的统一框架，旨在解决高计算成本和摇摆问题。

    

    随着自然语言处理领域中大型语言模型（LLMs）的不断演进，人们为了有效地微调常见的预训练LLMs以完成各种任务，在一个或多个特定领域中进行了大量努力。在实践中，有两种主要的适应方式：（i）多个独立模型：使用每个任务的相应训练样本对预训练LLMs进行独立的微调；（ii）集成模型：使用所有任务的样本来联合微调预训练LLMs 。为了同时解决高计算成本和摇摆问题，我们提出了一个统一的框架，使用一种新颖的定制门控（CGC）低秩自适应（LoRA）算法在LLMs中实现了1 + N多任务微调模式。我们的工作旨在充分利用MTL（即CGC）和PEFT（即LoRA）方案。对于给定的任务集群，我们设计了一个创新的层，其中包含...

    With the productive evolution of large language models (LLMs) in the field of natural language processing (NLP), tons of effort has been made to effectively fine-tune common pre-trained LLMs to fulfill a variety of tasks in one or multiple specific domain. In practice, there are two prevailing ways, in which the adaptation can be achieved: (i) Multiple Independent Models: Pre-trained LLMs are fine-tuned a few times independently using the corresponding training samples from each task. (ii) An Integrated Model: Samples from all tasks are employed to fine-tune a pre-trianed LLM unitedly. To address the high computing cost and seesawing issue simultaneously, we propose a unified framework that implements a 1 + N mutli-task fine-tuning pattern in LLMs using a novel Customized Gate Control (CGC) Low-rank Adaptation (LoRA) algorithm. Our work aims to take an advantage of both MTL (i.e., CGC) and PEFT (i.e., LoRA) scheme. For a given cluster of tasks, we design an innovative layer that contai
    
[^525]: 智能辅导系统中的先决条件结构发现

    Prerequisite Structure Discovery in Intelligent Tutoring Systems

    [https://arxiv.org/abs/2402.01672](https://arxiv.org/abs/2402.01672)

    本研究提出了一种在智能辅导系统中整合知识结构和知识追踪的方法，通过学习者轨迹发现潜在的先决条件结构，并应用于内容推荐和评估推荐算法。

    

    本文讨论了在智能辅导系统中，知识结构（KS）和知识追踪（KT）对提高教育内容推荐的重要性。KS表示不同知识组件（KCs）之间的关系，而KT根据学习者的过去历史来预测其成功。本研究的贡献包括提出了一个将KS作为可学习参数合并到KT模型中的方法，从而能够从学习者轨迹中发现潜在的KS。通过使用发现的KS进行内容推荐，并利用模拟学生评估推荐算法，评估揭示的KS的质量。

    This paper addresses the importance of Knowledge Structure (KS) and Knowledge Tracing (KT) in improving the recommendation of educational content in intelligent tutoring systems. The KS represents the relations between different Knowledge Components (KCs), while KT predicts a learner's success based on her past history. The contribution of this research includes proposing a KT model that incorporates the KS as a learnable parameter, enabling the discovery of the underlying KS from learner trajectories. The quality of the uncovered KS is assessed by using it to recommend content and evaluating the recommendation algorithm with simulated students.
    
[^526]: 智能辅导系统中的性能和动机的改进：结合机器学习和学习者选择

    Improved Performances and Motivation in Intelligent Tutoring Systems: Combining Machine Learning and Learner Choice

    [https://arxiv.org/abs/2402.01669](https://arxiv.org/abs/2402.01669)

    本研究通过结合机器学习和学生选择，改进了智能辅导系统的性能和动机。使用ZPDES算法，该系统能够最大化学习进展，并在实地研究中提高了不同学生群体的学习成绩。研究还探讨了学生选择对学习效率和动机的影响。

    

    在学校中，大规模的课堂规模给个性化学习带来了挑战，教育技术，尤其是智能辅导系统（ITS）试图解决这个问题。在这个背景下，基于学习进展假设（LPH）和多臂赌博机器学习技术的ZPDES算法对最大化学习进展（LP）的练习进行排序。该算法在之前的实地研究中已经显示出将学习表现提升到更广泛的学生群体中，与手工设计的课程相比。然而，其动机影响尚未评估。此外，ZPDES不允许学生发表选择意见。这种缺乏机构的限制与关注建模好奇驱动学习的LPH理论不一致。我们在这里研究了这种选择可能性的引入如何影响学习效率和动机。给定的选择与练习难度正交的维度有关，作为一种有趣的特性。

    Large class sizes pose challenges to personalized learning in schools, which educational technologies, especially intelligent tutoring systems (ITS), aim to address. In this context, the ZPDES algorithm, based on the Learning Progress Hypothesis (LPH) and multi-armed bandit machine learning techniques, sequences exercises that maximize learning progress (LP). This algorithm was previously shown in field studies to boost learning performances for a wider diversity of students compared to a hand-designed curriculum. However, its motivational impact was not assessed. Also, ZPDES did not allow students to express choices. This limitation in agency is at odds with the LPH theory concerned with modeling curiosity-driven learning. We here study how the introduction of such choice possibilities impact both learning efficiency and motivation. The given choice concerns dimensions that are orthogonal to exercise difficulty, acting as a playful feature.   In an extensive field study (265 7-8 years
    
[^527]: 基于知识驱动的深度学习范式在6G无线网络优化中的应用

    Knowledge-Driven Deep Learning Paradigms for Wireless Network Optimization in 6G

    [https://arxiv.org/abs/2402.01665](https://arxiv.org/abs/2402.01665)

    基于知识驱动的深度学习范式在6G无线网络优化中的应用通过整合领域知识和神经网络构建来解决复杂的网络优化问题。

    

    在第六代（6G）网络中，需要通过多维异构资源满足新兴的大规模用户和动态网络环境的多样化服务。由此引发的大规模复杂的网络优化问题超出了基于模型的理论方法的能力范围，因为计算复杂度极高且处理时间很长。尽管数据驱动的深度学习（DL）具有快速在线推理和通用逼近能力，但它严重依赖丰富的训练数据且缺乏解释性。为了解决这些问题，出现了一种新的范式，称为知识驱动的DL，旨在将已证明的领域知识整合到神经网络的构建中，从而充分利用两种方法的优势。本文系统地回顾了知识驱动的DL在无线网络中的应用。具体而言，提出了一个综合的知识驱动的DL在无线网络中的框架，

    In the sixth-generation (6G) networks, newly emerging diversified services of massive users in dynamic network environments are required to be satisfied by multi-dimensional heterogeneous resources. The resulting large-scale complicated network optimization problems are beyond the capability of model-based theoretical methods due to the overwhelming computational complexity and the long processing time. Although with fast online inference and universal approximation ability, data-driven deep learning (DL) heavily relies on abundant training data and lacks interpretability. To address these issues, a new paradigm called knowledge-driven DL has emerged, aiming to integrate proven domain knowledge into the construction of neural networks, thereby exploiting the strengths of both methods. This article provides a systematic review of knowledge-driven DL in wireless networks. Specifically, a holistic framework of knowledge-driven DL in wireless networks is proposed, where knowledge sources, 
    
[^528]: 杀手级应用：低速大规模AI武器

    Killer Apps: Low-Speed, Large-Scale AI Weapons

    [https://arxiv.org/abs/2402.01663](https://arxiv.org/abs/2402.01663)

    本文研究了AI武器的概念、部署、检测和潜在对策，强调了在信息领域内基于AI的心理操纵的潜力，以及其对全球个人、组织和社会的威胁。

    

    人工智能（AI）和机器学习（ML）的不断进步，特别是由OpenAI、Meta和Anthropic等组织开发的尖端生成式预训练转换器（GPT）模型的发展，给战争和安全带来了新的挑战和机会。目前关注的主要是AI在武器系统中的整合以及在动能冲突中快速决策中的作用。然而，同样重要但经常被忽视的一个方面是在信息领域中基于AI的心理操纵在互联网规模内的潜力。这些能力可能对全球个人、组织和社会造成重大威胁。本文探讨了AI武器的概念、部署、检测和潜在对策。

    The accelerating advancements in Artificial Intelligence (AI) and Machine Learning (ML), highlighted by the development of cutting-edge Generative Pre-trained Transformer (GPT) models by organizations such as OpenAI, Meta, and Anthropic, present new challenges and opportunities in warfare and security. Much of the current focus is on AI's integration within weapons systems and its role in rapid decision-making in kinetic conflict. However, an equally important but often overlooked aspect is the potential of AI-based psychological manipulation at internet scales within the information domain. These capabilities could pose significant threats to individuals, organizations, and societies globally. This paper explores the concept of AI weapons, their deployment, detection, and potential countermeasures.
    
[^529]: 研究数据叙事对仪表盘用户数据理解的影响

    Untersuchung der Wirkung von Data Storytelling auf das Datenverstaendnis von Dashboard-Nutzern

    [https://arxiv.org/abs/2402.01658](https://arxiv.org/abs/2402.01658)

    本研究通过实证数据分析验证了数据叙事对组织绩效的正面影响，这部分是由于所传达的决策质量。这为进一步研究数据叙事的前因和后果提供了理论基础。

    

    随着大数据和商业分析的广泛应用，数据叙事作为一种有效的手段，用于向观众传达分析洞察，以支持决策和提高业务绩效，逐渐受到关注。然而，关于数据叙事对数据理解的影响，目前缺乏实证证据。本研究通过实证数据分析验证了数据叙事作为一种构造在用户数据理解方面的影响。研究结果显示，数据叙事能力与组织绩效呈正相关，部分原因在于所传达的决策质量。这些结果为进一步探讨数据叙事的潜在前因与后果提供了理论基础。

    With the increasing use of big data and business analytics, data storytelling has gained popularity as an effective means of communicating analytical insights to audiences to support decision making and improve business performance. However, there is little empirical evidence on the impact of data storytelling on data understanding. This study validates the concept of data storytelling as a construct in terms of its impact on users' data understanding. Based on empirical data analysis, the results of this study show that data storytelling competence is positively associated with organizational performance, which is partly due to the quality of the decision is conveyed. These results provide a theoretical basis for further investigation of potential antecedents and consequences of data storytelling.
    
[^530]: 在全球视角下基于深度学习的在线课程学生表现预测方法的挑战

    A Deep Learning Approach Towards Student Performance Prediction in Online Courses: Challenges Based on a Global Perspective

    [https://arxiv.org/abs/2402.01655](https://arxiv.org/abs/2402.01655)

    该论文研究了基于深度学习的方法在全球范围内预测在线课程学生表现的挑战，并证明了深度学习模型在此方面具有有希望的性能。

    

    使用传统分析方法来分析和评估学生在任何学习环境中的进展是耗时且压力巨大的。由于教育界对于整合互联网技术以及向电子学习、混合学习或在线学习模式的转变的关注，学生人数的增加进一步加剧了这种情况。因此，学生表现预测成为了近年来一个活跃的研究领域。为了解决这个问题，机器学习和数据挖掘技术被提出作为一个可行的解决方案。为此，该工作提出了使用深度学习技术（CNN和RNN-LSTM）来预测在线课程交付的中期阶段学生的表现，并使用来自世界上三个不同地区的三个不同数据集进行了实验。实验结果表明，深度学习模型在性能上表现出色，超过了其他优化的传统机器学习模型。

    Analyzing and evaluating students' progress in any learning environment is stressful and time consuming if done using traditional analysis methods. This is further exasperated by the increasing number of students due to the shift of focus toward integrating the Internet technologies in education and the focus of academic institutions on moving toward e-Learning, blended, or online learning models. As a result, the topic of student performance prediction has become a vibrant research area in recent years. To address this, machine learning and data mining techniques have emerged as a viable solution. To that end, this work proposes the use of deep learning techniques (CNN and RNN-LSTM) to predict the students' performance at the midpoint stage of the online course delivery using three distinct datasets collected from three different regions of the world. Experimental results show that deep learning models have promising performance as they outperform other optimized traditional ML models
    
[^531]: 用户中心的AI分析在慢性健康状况管理中的应用

    User-Centric AI Analytics for Chronic Health Conditions Management

    [https://arxiv.org/abs/2402.01652](https://arxiv.org/abs/2402.01652)

    本论文介绍了AI分析在管理慢性健康状况中的应用，特别是在无药物治疗方法中的挑战。研究呈现了用户中心的方法以及相关的研究问题和下一步工作。

    

    在健康信息学中，AI分析的应用近年来迅速增长。在本专题讨论中，我们探讨了AI分析在管理糖尿病、肥胖等慢性健康状况中的应用。我们重点关注在无药物治疗方法中管理这些状况所面临的挑战，由于个体状况的差异。这些差异导致研究进入了用户中心的方法，引发了各种研究问题。在本短文中，我们提供了近期和正在进行的研究工作的示例，并结论我们认为的下一步和一些尚未解决的研究问题。

    The use of AI analytics in health informatics has seen a rapid growth in recent years. In this talk, we look at AI analytics use in managing chronic health conditions such as diabetes, obesity, etc. We focus on the challenges in managing these conditions especially with drug-free approaches due to the variations in individual circumstances. These variations directed the research into user-centric approach leading to variety of research questions. In this short paper, we give examples from recent and current research work and conclude with what, in our opinion, to be the next steps and some remaining open research questions.
    
[^532]: 利用LSTM神经网络算法预测OECD成员国和伊朗的进口

    Forecasting Imports in OECD Member Countries and Iran by Using Neural Network Algorithms of LSTM

    [https://arxiv.org/abs/2402.01648](https://arxiv.org/abs/2402.01648)

    这项研究利用LSTM神经网络算法预测了选定的OECD成员国和伊朗在2021年到2025年的进口情况，结果表明其准确性达到了99%。

    

    人工神经网络（ANN）作为人工智能的一个分支，在许多应用中展示了其高价值，并被用作一种适合的预测方法。因此，本研究旨在利用ANN预测选定的OECD成员国和伊朗在2021年到2025年的20个季度的进口情况。从1970年至2019年，通过使用世界银行、世界贸易组织、国际货币基金组织等有效资源收集了这些国家进口相关的数据，并将数据转化为季度数据，以增加收集数据量，以提高网络性能和准确性。本研究使用LSTM在PyCharm中分析数据。其中75%的数据被视为训练数据，25%被视为测试数据，分析结果以99%的准确率进行预测，显示了输出的有效性和可靠性。由于进口是一个消费函数，而消费受到...

    Artificial Neural Networks (ANN) which are a branch of artificial intelligence, have shown their high value in lots of applications and are used as a suitable forecasting method. Therefore, this study aims at forecasting imports in OECD member selected countries and Iran for 20 seasons from 2021 to 2025 by means of ANN. Data related to the imports of such countries collected over 50 years from 1970 to 2019 from valid resources including World Bank, WTO, IFM,the data turned into seasonal data to increase the number of collected data for better performance and high accuracy of the network by using Diz formula that there were totally 200 data related to imports. This study has used LSTM to analyse data in Pycharm. 75% of data considered as training data and 25% considered as test data and the results of the analysis were forecasted with 99% accuracy which revealed the validity and reliability of the output. Since the imports is consumption function and since the consumption is influenced 
    
[^533]: 打造您自己的机器人朋友：面向普及且引人入胜的人工智能教育的开源学习模块

    Build Your Own Robot Friend: An Open-Source Learning Module for Accessible and Engaging AI Education

    [https://arxiv.org/abs/2402.01647](https://arxiv.org/abs/2402.01647)

    本文开发了一个面向大学和高中学生的开源学习模块，允许学生从零开始构建自己的机器人伙伴，并提供实践经验和入门知识，包括机器人技术、机器学习、软件工程和机械工程等。同时，该模块着重强调以人为本的人工智能，使学生能够更好地理解和开发。

    

    随着人工智能在社会和全球经济中的日益重要作用，人工智能教育和素养已成为大学和K-12教育中必要的组成部分，以培养学生面对人工智能驱动的社会。然而，当前的人工智能课程尚未足够普及和引人入胜，无法满足来自不同教育目标和不同社会经济背景的学生和学校的需求。在这项工作中，我们开发了一个面向大学和高中学生的开源学习模块，允许学生从零开始构建自己的机器人伙伴。这个开放平台可以提供关于人工智能各个方面的实践经验和入门知识，包括机器人技术、机器学习、软件工程和机械工程等。由于社交辅助机器人伙伴的社交和个人性质，该模块还特别强调以人为本的人工智能，使学生能够更好地理解和开发。

    As artificial intelligence (AI) is playing an increasingly important role in our society and global economy, AI education and literacy have become necessary components in college and K-12 education to prepare students for an AI-powered society. However, current AI curricula have not yet been made accessible and engaging enough for students and schools from all socio-economic backgrounds with different educational goals. In this work, we developed an open-source learning module for college and high school students, which allows students to build their own robot companion from the ground up. This open platform can be used to provide hands-on experience and introductory knowledge about various aspects of AI, including robotics, machine learning (ML), software engineering, and mechanical engineering. Because of the social and personal nature of a socially assistive robot companion, this module also puts a special emphasis on human-centered AI, enabling students to develop a better understa
    
[^534]: L-TUNING：用于LLMs中的提示和前缀的同步标签调整

    L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs

    [https://arxiv.org/abs/2402.01643](https://arxiv.org/abs/2402.01643)

    本文介绍了L-Tuning，一种在自然语言推理（NLI）框架内的高效微调方法，通过对预训练的LLM中的标签标记进行微调，提高了训练准确性和效率，并增强了模型的训练细微差别。

    

    高效地针对特定任务对大型语言模型（LLMs）进行微调在自然语言处理中面临着重大挑战。传统方法，如提示或前缀调整，通常依赖于任意标记进行训练，从而导致训练时间延长并且通用标记在各种类别标签中使用。为了解决这些问题，本文引入了L-Tuning，这是一种在自然语言推理（NLI）框架内设计的用于分类任务的高效微调方法。与传统方法不同，L-Tuning专注于通过预训练的LLM处理的标签标记的微调，从而利用其预先存在的语义知识。这种技术不仅提高了微调的准确性和效率，还促进了为每个类别生成不同的标签嵌入，增强了模型的训练细微差别。我们的实验结果表明，使用L-Tuning可以显著提高训练效率和分类准确性。

    Efficiently fine-tuning Large Language Models (LLMs) for specific tasks presents a considerable challenge in natural language processing. Traditional methods, like prompt or prefix tuning, typically rely on arbitrary tokens for training, leading to prolonged training times and generalized token use across various class labels. To address these issues, this paper introduces L-Tuning, an efficient fine-tuning approach designed for classification tasks within the Natural Language Inference (NLI) framework. Diverging from conventional methods, L-Tuning focuses on the fine-tuning of label tokens processed through a pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This technique not only improves the fine-tuning accuracy and efficiency but also facilitates the generation of distinct label embeddings for each class, enhancing the model's training nuance. Our experimental results indicate a significant improvement in training efficiency and classification accuracy with 
    
[^535]: 机器生成文本的检测：文献综述

    Detection of Machine-Generated Text: Literature Survey

    [https://arxiv.org/abs/2402.01642](https://arxiv.org/abs/2402.01642)

    这项论文对机器生成文本的检测进行了文献综述，指出了语言模型生成的虚假文本大量存在于公共领域，因此需要采取预防措施来应对其可能带来的危险影响。

    

    由于语言模型能够快速轻松地产生虚假文本，公共领域中出现了大量此类内容。在不断提升的复杂度和写作风格下，几乎无法区分人类撰写和机器生成的内容。因此，与人工作者相比，语言模型生成的作品引起了巨大的媒体关注并引起了争议。对于先进语言模型可能对社会产生的影响的担忧也应运而生，需要对这些过程有更充分的了解。自然语言生成（NLG）和生成预训练转换器（GPT）模型已经在各个领域引起了革命性的影响：其范围不仅渗透到新闻报道和客户服务，还涉及到学术界。为了减轻使用这些模型可能带来的危险影响，必须采取预防措施，例如为人类操作员提供区分虚假文本和真实文本的能力。

    Since language models produce fake text quickly and easily, there is an oversupply of such content in the public domain. The degree of sophistication and writing style has reached a point where differentiating between human authored and machine-generated content is nearly impossible. As a result, works generated by language models rather than human authors have gained significant media attention and stirred controversy.Concerns regarding the possible influence of advanced language models on society have also arisen, needing a fuller knowledge of these processes. Natural language generation (NLG) and generative pre-trained transformer (GPT) models have revolutionized a variety of sectors: the scope not only permeated throughout journalism and customer service but also reached academia. To mitigate the hazardous implications that may arise from the use of these models, preventative measures must be implemented, such as providing human agents with the capacity to distinguish between artif
    
[^536]: 正式-LLM：将形式语言和自然语言集成于可控的LLM智能体中

    Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents

    [https://arxiv.org/abs/2402.00798](https://arxiv.org/abs/2402.00798)

    本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。

    

    最近，对于大型语言模型（LLMs）的进展使得人工智能智能体能够自动生成和执行解决复杂任务的多步计划。然而，由于LLM的内容生成过程几乎无法控制，当前的LLM智能体经常生成无效或不可执行的计划，这损害了生成计划的性能并破坏了用户对LLM智能体的信任。为应对这个问题，本文提出了一种新颖的“正式-LLM”框架，用于LLM智能体，通过将自然语言的表达力和形式语言的精确性进行整合。具体而言，该框架允许人类用户将他们对计划过程的要求或约束表达为自动机。然后，在自动机的监督下，使用基于堆栈的LLM计划生成过程来确保生成的计划满足约束条件，从而使计划过程可控。我们在基准任务和实际的真实任务上进行了实验，并且obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.

    Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
    
[^537]: 开发基于深度强化学习的多智能体自适应框架用于动态投资组合风险管理

    Developing A Multi-Agent and Self-Adaptive Framework with Deep Reinforcement Learning for Dynamic Portfolio Risk Management

    [https://arxiv.org/abs/2402.00515](https://arxiv.org/abs/2402.00515)

    本研究开发了一种基于深度强化学习的多智能体自适应框架，用于动态投资组合风险管理。通过两个协同反应的智能体，平衡整体投资组合回报和潜在风险，解决了在复杂金融市场环境下的投资策略问题。

    

    近年来，深度学习和强化学习方法已被用作反应性智能体以在高度动荡的金融市场环境下快速学习并响应新的投资策略，用于投资组合管理。然而，在复杂的金融行业之间存在非常复杂的关联性和不断变化的趋势的情况下，深度学习或强化学习基于的智能体可能会偏向于最大化新制定的投资组合的总回报，而忽视其在全球或区域部门的各种市场条件动荡下的潜在风险。因此，提出了一种名为MASA的多智能体自适应框架，通过两个协同和反应的智能体采用复杂的多智能体强化学习方法来仔细动态平衡整体投资组合回报和潜在风险之间的权衡。

    Deep or reinforcement learning (RL) approaches have been adapted as reactive agents to quickly learn and respond with new investment strategies for portfolio management under the highly turbulent financial market environments in recent years. In many cases, due to the very complex correlations among various financial sectors, and the fluctuating trends in different financial markets, a deep or reinforcement learning based agent can be biased in maximising the total returns of the newly formulated investment portfolio while neglecting its potential risks under the turmoil of various market conditions in the global or regional sectors. Accordingly, a multi-agent and self-adaptive framework namely the MASA is proposed in which a sophisticated multi-agent reinforcement learning (RL) approach is adopted through two cooperating and reactive agents to carefully and dynamically balance the trade-off between the overall portfolio returns and their potential risks. Besides, a very flexible and p
    
[^538]: LLaMA和ChatGPT嵌入在分子嵌入中的比较分析

    Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding

    [https://arxiv.org/abs/2402.00024](https://arxiv.org/abs/2402.00024)

    LLaMA和ChatGPT比较分析了它们在SMILES字符串嵌入中的性能，在分子性质预测和药物-药物相互作用预测中，LLaMA相对于ChatGPT表现更好并且与现有方法相当。

    

    目的：ChatGPT和LLaMA等大型语言模型在化学信息学领域越来越受到重视，特别是在解释Simplified Molecular Input Line Entry System (SMILES)方面。这些语言模型可以将SMILES字符串解码为向量表示，为理解化学图提供了一种新的方法。方法：我们研究了ChatGPT和LLaMA在嵌入SMILES字符串方面的性能。我们的评估集中在两个关键应用领域：分子性质（MP）预测和药物-药物相互作用（DDI）预测，这在药物开发和医疗保健中至关重要。结果：我们发现，使用LLaMA生成的SMILES嵌入在MP和DDI预测任务中表现优于ChatGPT。值得注意的是，基于LLaMA的SMILES嵌入在这两个预测任务中显示了与现有方法相当的结果。结论：在化学信息学中应用LLMs，特别是在利用SMILES进行嵌入方面，是可行的。

    Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly recognized for their potential in the field of cheminformatics, particularly in interpreting Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs can decode SMILES strings into vector representations, providing a novel approach to understanding chemical graphs.   Methods: We investigate the performance of ChatGPT and LLaMA in embedding SMILES strings. Our evaluation focuses on two key applications: molecular property (MP) prediction and drug-drug interaction (DDI) prediction, both essential in drug development and healthcare.   Results: We find that SMILES embeddings generated using LLaMA outperform those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to existing methods in both prediction tasks.   Conclusion: The application of LLMs in cheminformatics, particularly in utilizi
    
[^539]: EEG-GPT: 探索大型语言模型在EEG分类和解读中的能力

    EEG-GPT: Exploring Capabilities of Large Language Models for EEG Classification and Interpretation

    [https://arxiv.org/abs/2401.18006](https://arxiv.org/abs/2401.18006)

    EEG-GPT是一种利用大型语言模型来分类和解读EEG的方法，它能够实现多尺度电生理理解和分类，且在few-shot学习范式中表现出色。

    

    在应用于脑电图（EEG）的传统机器学习（ML）方法中，往往是有限的聚焦，仅仅孤立地关注跨越不同时间尺度（从毫秒的瞬时尖峰到持续几分钟的癫痫发作）和空间尺度（从局部高频振荡到全局睡眠活动）的特定脑活动。这种孤立的方法限制了发展出具有多尺度电生理理解和分类能力的EEG ML模型。此外，典型的ML EEG方法采用黑匣子方法，限制了其在临床环境中的可解释性和可信度。因此，我们提出了EEG-GPT，一种统一的EEG分类方法，利用大型语言模型（LLM）的进展。EEG-GPT在仅利用2％的训练数据的few-shot学习范式中，达到了与当前最先进的深度学习方法相当的优异性能，能够对正常和异常的EEG进行分类。此外，

    In conventional machine learning (ML) approaches applied to electroencephalography (EEG), this is often a limited focus, isolating specific brain activities occurring across disparate temporal scales (from transient spikes in milliseconds to seizures lasting minutes) and spatial scales (from localized high-frequency oscillations to global sleep activity). This siloed approach limits the development EEG ML models that exhibit multi-scale electrophysiological understanding and classification capabilities. Moreover, typical ML EEG approaches utilize black-box approaches, limiting their interpretability and trustworthiness in clinical contexts. Thus, we propose EEG-GPT, a unifying approach to EEG classification that leverages advances in large language models (LLM). EEG-GPT achieves excellent performance comparable to current state-of-the-art deep learning methods in classifying normal from abnormal EEG in a few-shot learning paradigm utilizing only 2% of training data. Furthermore, it off
    
[^540]: 使用远程连接通知的Transformer进行高效的次季节天气预报

    Efficient Subseasonal Weather Forecast using Teleconnection-informed Transformers

    [https://arxiv.org/abs/2401.17870](https://arxiv.org/abs/2401.17870)

    提出了一种使用远程连接通知的Transformer模型来实现高效的次季节天气预报，该方法通过利用预训练模型和集成远程连接通知的时间模块来改进预测能力。

    

    次季节预报对农业、水资源管理和灾害预警至关重要，但由于大气的混沌性，面临着挑战。最近机器学习领域的进展通过实现与数值模型相当的预测能力，革新了天气预报。然而，训练这些基础模型需要数千个GPU的计算时间，这导致相当多的碳排放，并限制了其更广泛的应用。此外，机器学习模型往往通过产生平滑的结果来愚弄像素误差评分，这些结果缺乏物理一致性和气象意义。为了解决上述问题，我们提出了一种远程连接通知的Transformer。我们的架构利用预训练的Pangu模型来获得良好的初始权重，并集成了一个远程连接通知的时间模块，以提高在延长的时间范围内的可预测性。值得注意的是，通过调整Pangu模型的1.1%参数，我们的方法改进了预测能力。

    Subseasonal forecasting, which is pivotal for agriculture, water resource management, and early warning of disasters, faces challenges due to the chaotic nature of the atmosphere. Recent advances in machine learning (ML) have revolutionized weather forecasting by achieving competitive predictive skills to numerical models. However, training such foundation models requires thousands of GPU days, which causes substantial carbon emissions and limits their broader applicability. Moreover, ML models tend to fool the pixel-wise error scores by producing smoothed results which lack physical consistency and meteorological meaning. To deal with the aforementioned problems, we propose a teleconnection-informed transformer. Our architecture leverages the pretrained Pangu model to achieve good initial weights and integrates a teleconnection-informed temporal module to improve predictability in an extended temporal range. Remarkably, by adjusting 1.1% of the Pangu model's parameters, our method enh
    
[^541]: IGCN：用于多模态数据的综合图卷积网络

    IGCN: Integrative Graph Convolutional Networks for Multi-modal Data

    [https://arxiv.org/abs/2401.17612](https://arxiv.org/abs/2401.17612)

    IGCN是用于多模态数据的综合神经网络，通过综合分析多模态数据来获得更好的学习表示，并提高模型的可解释性。

    

    最近图神经网络（GNN）的进展已经导致了图数据建模的显著增长，用于包含各种类型节点和边的多模态数据。尽管最近已经开发了一些用于网络结构化数据的综合预测解决方案，但这些方法存在一些限制。对于涉及多模态数据的节点分类任务，某些数据模态在预测一个类时表现更好，而其他数据模态可能在预测不同类别时表现出色。因此，为了获得更好的学习表示，需要先进的计算方法来进行多模态数据的综合分析。此外，现有的综合工具缺乏对其特定预测背后原理的全面和连贯理解，使其无法用于提高模型的可解释性。为了解决这些限制，我们引入了一种新的用于多模态数据网络的综合神经网络方法，名为综合图卷积网络（IGCN）。

    Recent advances in Graph Neural Networks (GNN) have led to a considerable growth in graph data modeling for multi-modal data which contains various types of nodes and edges. Although some integrative prediction solutions have been developed recently for network-structured data, these methods have some restrictions. For a node classification task involving multi-modal data, certain data modalities may perform better when predicting one class, while others might excel in predicting a different class. Thus, to obtain a better learning representation, advanced computational methodologies are required for the integrative analysis of multi-modal data. Moreover, existing integrative tools lack a comprehensive and cohesive understanding of the rationale behind their specific predictions, making them unsuitable for enhancing model interpretability. Addressing these restrictions, we introduce a novel integrative neural network approach for multi-modal data networks, named Integrative Graph Convo
    
[^542]: 鲁棒的提示优化用于对抗语言模型的破解攻击

    Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks

    [https://arxiv.org/abs/2401.17263](https://arxiv.org/abs/2401.17263)

    该论文提出了一种鲁棒的提示优化算法（RPO）用于对抗语言模型的破解攻击，通过梯度优化来确保输出的无害性，并成功降低了攻击成功率。

    

    尽管在人工智能对齐方面取得了一些进展，但语言模型（LM）仍然容易受到对抗性攻击或破解攻击的影响，其中对手修改输入提示以诱导有害行为。虽然已经提出了一些防御方法，但它们仅关注狭窄的威胁模型，并不能提供强大的防御。为了实现强大的防御，我们首次提出了用于对抗破解攻击的对抗目标，并提出了一种名为鲁棒提示优化（RPO）的算法，该算法利用基于梯度的令牌优化来确保输出的无害性。通过这种方法，我们得到了一个易于访问的后缀，显著改善了对破解攻击的强韧性，包括优化过程中出现的破解攻击以及未知的破解攻击，将攻击成功率从84%降低到8.66%，在20个破解攻击中。此外，我们还发现RPO对正常LM使用的影响较小，在适应性攻击下仍然有效，并且可以迁移到黑盒模型中，降低攻击成功率。

    Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
    
[^543]: “越大越好？”重新思考长期时间序列预测中的有效模型规模

    The Bigger the Better? Rethinking the Effective Model Scale in Long-term Time Series Forecasting

    [https://arxiv.org/abs/2401.11929](https://arxiv.org/abs/2401.11929)

    本研究通过调查条件相关性和自相关性，揭示了输入数据中的冗余性，并提出了HDformer，这是一种轻量级的Transformer变种，利用蒸馏技术和快速网络连接层来降低模型复杂性。

    

    长期时间序列预测（LTSF）是时间序列分析中的一个重要前沿，其特点是关注于大量输入序列，与传统方法中的有限长度相比有所不同。尽管更长的序列本质上传达了更丰富的信息，可能提高了预测的精度，但目前的技术往往通过提高模型复杂性来应对。这些复杂的模型可以膨胀为数百万个参数，包括位置编码、前馈网络和自注意机制等参数密集型元素。然而，这种复杂性导致了禁止性的模型规模，特别是考虑到时间序列数据的语义简单性。出于追求简洁性的动机，我们的研究利用条件相关性和自相关性作为调查工具，揭示了输入数据中的显著冗余。借助这些见解，我们引入了HDformer，这是一种轻量级的Transformer变体，经过增强，使用蒸馏技术和快速网络连接层来降低模型复杂性。

    Long-term time series forecasting (LTSF) represents a critical frontier in time series analysis, distinguished by its focus on extensive input sequences, in contrast to the constrained lengths typical of traditional approaches. While longer sequences inherently convey richer information, potentially enhancing predictive precision, prevailing techniques often respond by escalating model complexity. These intricate models can inflate into millions of parameters, incorporating parameter-intensive elements like positional encodings, feed-forward networks and self-attention mechanisms. This complexity, however, leads to prohibitive model scale, particularly given the time series data's semantic simplicity. Motivated by the pursuit of parsimony, our research employs conditional correlation and auto-correlation as investigative tools, revealing significant redundancies within the input data. Leveraging these insights, we introduce the HDformer, a lightweight Transformer variant enhanced with 
    
[^544]: BanglaNet: 使用卷积神经网络集合的方法对孟加拉手写字符进行识别

    BanglaNet: Bangla Handwritten Character Recognition using Ensembling of Convolutional Neural Network

    [https://arxiv.org/abs/2401.08035](https://arxiv.org/abs/2401.08035)

    本文提出了BanglaNet，使用卷积神经网络集合的方法对孟加拉手写字符进行识别。实验结果表明，在三个基准数据集上，与最新的CNN研究相比，取得了显著的识别准确率提升。

    

    手写字符识别是一项重要任务，因为它有很多应用。孟加拉手写字符的识别任务尤其具有挑战性，因为孟加拉字符具有连写的特点，并且具有多种书写方式的复合字符。本文提出了一种基于多个卷积神经网络（CNN）集合的分类模型——BanglaNet，用于对孟加拉基本字符、复合字符、数字符号和修饰符进行分类。基于Inception、ResNet和DenseNet等最新CNN模型的三种不同模型根据增强和非增强输入进行训练。最后，将这些模型进行平均或集成，得到最终模型。针对三个基准孟加拉手写字符数据集（CMATERdb、BanglaLekha-Isolated和Ekush），进行了严格的实验，与一些最新的基于CNN的研究相比，取得了显著的识别准确率提升。

    Handwritten character recognition is a crucial task because of its abundant applications. The recognition task of Bangla handwritten characters is especially challenging because of the cursive nature of Bangla characters and the presence of compound characters with more than one way of writing. In this paper, a classification model based on the ensembling of several Convolutional Neural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic characters, compound characters, numerals, and modifiers. Three different models based on the idea of state-of-the-art CNN models like Inception, ResNet, and DenseNet have been trained with both augmented and non-augmented inputs. Finally, all these models are averaged or ensembled to get the finishing model. Rigorous experimentation on three benchmark Bangla handwritten characters datasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited significant recognition accuracies compared to some recent CNN-based research. T
    
[^545]: 为管理低空领域授权而构建公平和公正的软件系统的工程方法

    Towards Engineering Fair and Equitable Software Systems for Managing Low-Altitude Airspace Authorizations

    [https://arxiv.org/abs/2401.07353](https://arxiv.org/abs/2401.07353)

    本文旨在为管理低空领域授权而构建公平和公正的软件系统进行工程方法研究。研究结果表明，飞行特征和环境条件被认为是最重要的因素，但还应考虑飞行员和无人机的能力。此外，许多受访者对使用机器学习算法批准或拒绝飞行请求表示反对。

    

    小型无人机系统（sUAS）已在广泛的应用领域中得到广泛应用。这引入了共享领域内的操作复杂性和报告的事件增加，引发了安全担忧。为此，美国联邦航空管理局（FAA）正在开发无人机交通管理（UTM）系统，以基于sUAS预测能够安全完成任务的能力来控制对空域的访问。然而，一个完全自动化的系统，能够快速批准或拒绝飞行请求，可能存在偏见，并必须考虑多样化利益相关者的安全性、透明性和公平性。在本文中，我们提出了一项初步研究，探讨了应考虑在自动化系统中的因素的利益相关者的观点。结果表明，飞行特征和环境条件被认为是最重要的，但飞行员和无人机的能力也应该被考虑。此外，几个受访者表示他们对使用机器学习算法批准或拒绝飞行请求的不满。

    Small Unmanned Aircraft Systems (sUAS) have gained widespread adoption across a diverse range of applications. This has introduced operational complexities within shared airspaces and an increase in reported incidents, raising safety concerns. In response, the U.S. Federal Aviation Administration (FAA) is developing a UAS Traffic Management (UTM) system to control access to airspace based on an sUAS's predicted ability to safely complete its mission. However, a fully automated system capable of swiftly approving or denying flight requests can be prone to bias and must consider safety, transparency, and fairness to diverse stakeholders. In this paper, we present an initial study that explores stakeholders' perspectives on factors that should be considered in an automated system. Results indicate flight characteristics and environmental conditions were perceived as most important but pilot and drone capabilities should also be considered. Further, several respondents indicated an aversio
    
[^546]: 使用机器学习技术在物联网系统中进行恶意软件检测

    Malware Detection in IOT Systems Using Machine Learning Techniques

    [https://arxiv.org/abs/2312.17683](https://arxiv.org/abs/2312.17683)

    本研究提出了一种基于CNN-LSTM混合模型的物联网恶意软件识别方法，通过与已有方法的比较和评估，展示了该方法的高准确率和突出性能，为增强物联网安全性提供了潜力。

    

    物联网环境中的恶意软件检测需要强大的方法学。本研究引入了一个CNN-LSTM混合模型用于物联网恶意软件识别，并评估其在已有方法上的表现。借助K折交叉验证，所提出的方法实现了95.5%的准确率，超过了现有方法。CNN算法促进了更优秀的学习模型构建，而LSTM分类器在分类中表现出增强的准确性。与流行技术进行的比较分析表明了所提出模型的有效性，突出了其增强物联网安全性的潜力。研究呼吁进一步探索SVM作为替代方法，强调了分布式检测策略的必要性，并强调了预测分析对于更强大的物联网安全的重要性。该研究为在物联网生态系统中开发更具韧性的安全措施提供了平台。

    Malware detection in IoT environments necessitates robust methodologies. This study introduces a CNN-LSTM hybrid model for IoT malware identification and evaluates its performance against established methods. Leveraging K-fold cross-validation, the proposed approach achieved 95.5% accuracy, surpassing existing methods. The CNN algorithm enabled superior learning model construction, and the LSTM classifier exhibited heightened accuracy in classification. Comparative analysis against prevalent techniques demonstrated the efficacy of the proposed model, highlighting its potential for enhancing IoT security. The study advocates for future exploration of SVMs as alternatives, emphasizes the need for distributed detection strategies, and underscores the importance of predictive analyses for a more powerful IOT security. This research serves as a platform for developing more resilient security measures in IoT ecosystems.
    
[^547]: 面向印刷多层感知机的定制近似乘积累加和激活技术

    Bespoke Approximation of Multiplication-Accumulation and Activation Targeting Printed Multilayer Perceptrons

    [https://arxiv.org/abs/2312.17612](https://arxiv.org/abs/2312.17612)

    本研究提出了一种针对印刷电子技术中的限制的自动化框架，用于设计超低功耗的多层感知机（MLP）分类器。

    

    印刷电子技术具有独特的特性，使其成为实现真正无处不在计算的重要技术。本研究提出了一种自动化框架，用于设计超低功耗的多层感知机（MLP）分类器，通过利用近似计算和定制化设计的原则来克服印刷电子技术中的限制。

    Printed Electronics (PE) feature distinct and remarkable characteristics that make them a prominent technology for achieving true ubiquitous computing. This is particularly relevant in application domains that require conformal and ultra-low cost solutions, which have experienced limited penetration of computing until now. Unlike silicon-based technologies, PE offer unparalleled features such as non-recurring engineering costs, ultra-low manufacturing cost, and on-demand fabrication of conformal, flexible, non-toxic, and stretchable hardware. However, PE face certain limitations due to their large feature sizes, that impede the realization of complex circuits, such as machine learning classifiers. In this work, we address these limitations by leveraging the principles of Approximate Computing and Bespoke (fully-customized) design. We propose an automated framework for designing ultra-low power Multilayer Perceptron (MLP) classifiers which employs, for the first time, a holistic approac
    
[^548]: AdaNAS：自我监督的神经架构搜索用于集合降雨预报的自适应后处理方法

    AdaNAS: Adaptively Post-processing with Self-supervised Neural Architecture Search for Ensemble Rainfall Forecasts

    [https://arxiv.org/abs/2312.16046](https://arxiv.org/abs/2312.16046)

    AdaNAS是一个自适应后处理方法，利用自我监督神经架构搜索对集合降雨预报进行处理，能够提高降雨预测的准确性。它采用面向降雨的搜索空间和降雨层级规则化函数，有效消除噪声数据的影响。

    

    以往关于数值天气预报(NWP)降雨预报的后处理研究主要关注统计方面的内容，较少涉及基于学习的方面。虽然一些手动设计的模型被提出来提高准确性，但它们是定制的网络，需要反复尝试和验证，耗费大量的时间和人力成本。因此，本研究提出了一种无需重要手动工作的自我监督神经架构搜索(NAS)方法，称为AdaNAS，用于进行降雨预报后处理和高准确性的降雨预测。此外，我们设计了一个面向降雨的搜索空间，显著改进了高降雨区域的预报。此外，还提出了一个降雨层级规则化函数，以消除训练过程中噪声数据的影响。在大规模预测实验中，根据\emph{无雨}，\emph{小雨}，\emph{中雨}，\emph{大雨}和\emph{暴雨}的情况进行了验证。

    Previous post-processing studies on rainfall forecasts using numerical weather prediction (NWP) mainly focus on statistics-based aspects, while learning-based aspects are rarely investigated. Although some manually-designed models are proposed to raise accuracy, they are customized networks, which need to be repeatedly tried and verified, at a huge cost in time and labor. Therefore, a self-supervised neural architecture search (NAS) method without significant manual efforts called AdaNAS is proposed in this study to perform rainfall forecast post-processing and predict rainfall with high accuracy. In addition, we design a rainfall-aware search space to significantly improve forecasts for high-rainfall areas. Furthermore, we propose a rainfall-level regularization function to eliminate the effect of noise data during the training. Validation experiments have been performed under the cases of \emph{None}, \emph{Light}, \emph{Moderate}, \emph{Heavy} and \emph{Violent} on a large-scale pre
    
[^549]: 一种针对不平衡线性分类的扩展非对称sigmoid和感知机(SIGTRON)

    An extended asymmetric sigmoid with Perceptron (SIGTRON) for imbalanced linear classification

    [https://arxiv.org/abs/2312.16043](https://arxiv.org/abs/2312.16043)

    本文提出了一个新的多项式参数化sigmoid函数(SIGTRON)，并且介绍了其伴随的SIC模型。相比传统的成本敏感学习模型，在给定的训练数据集接近良好平衡的条件下，所提出的SIC模型对于数据集的变化更加适应，并通过创建倾斜的超平面方程来实现。

    

    本文提出了一种新的多项式参数化sigmoid函数，称为SIGTRON，它是一种扩展的非对称sigmoid函数和感知机的结合，以及它的伴随凸模型SIGTRON-不平衡分类(SIC)模型，该模型使用了虚拟SIGTRON产生的凸损失函数。与传统的$\pi$-加权成本敏感学习模型相比，SIC模型在损失函数上没有外部的$\pi$-权重，而是在虚拟的SIGTRON产生的损失函数中有内部参数。因此，当给定的训练数据集接近良好平衡的条件时，我们展示了所提出的SIC模型对数据集的变化更加适应，比如训练集和测试集之间比例不平衡的不一致性。这种适应是通过创建一个倾斜的超平面方程来实现的。另外，我们提出了一个基于拟牛顿优化(L-BFGS)框架的虚拟凸损失，通过开发一个基于区间的二分线性搜索算法来实现。

    This article presents a new polynomial parameterized sigmoid called SIGTRON, which is an extended asymmetric sigmoid with Perceptron, and its companion convex model called SIGTRON-imbalanced classification (SIC) model that employs a virtual SIGTRON-induced convex loss function. In contrast to the conventional $\pi$-weighted cost-sensitive learning model, the SIC model does not have an external $\pi$-weight on the loss function but has internal parameters in the virtual SIGTRON-induced loss function. As a consequence, when the given training dataset is close to the well-balanced condition, we show that the proposed SIC model is more adaptive to variations of the dataset, such as the inconsistency of the scale-class-imbalance ratio between the training and test datasets. This adaptation is achieved by creating a skewed hyperplane equation. Additionally, we present a quasi-Newton optimization(L-BFGS) framework for the virtual convex loss by developing an interval-based bisection line sear
    
[^550]: BiSwift: 边缘多流视频分析的带宽编排器

    BiSwift: Bandwidth Orchestrator for Multi-Stream Video Analytics on Edge

    [https://arxiv.org/abs/2312.15740](https://arxiv.org/abs/2312.15740)

    BiSwift是一个用于边缘多流视频分析的带宽编排器，它通过一个创新的自适应混合编解码器和全局带宽控制器，实现了高并发实时视频分析以及多个流之间的准确性公平性。

    

    用于监控和道路交通的高清（HD）摄像头经历了巨大的增长，实时分析需要大量的计算资源。最近，将帧从前端设备卸载到后端边缘服务器已经显示出了很大的潜力。在多流竞争环境下，高效的带宽管理和适当的调度对于保证推理准确性和吞吐量非常重要。为了实现这个目标，我们提出了BiSwift，一种双级框架，通过集成多级流水线的创新自适应混合编解码器和全局带宽控制器来扩展并发实时视频分析，并为多个视频流提供优化的调度。低级别的前后端协同机制（称为自适应混合编解码器）在本地优化准确性和加速单个流的端到端视频分析。上层调度器通过全局带宽控制器，旨在实现多个流之间的准确性公平性。

    High-definition (HD) cameras for surveillance and road traffic have experienced tremendous growth, demanding intensive computation resources for real-time analytics. Recently, offloading frames from the front-end device to the back-end edge server has shown great promise. In multi-stream competitive environments, efficient bandwidth management and proper scheduling are crucial to ensure both high inference accuracy and high throughput. To achieve this goal, we propose BiSwift, a bi-level framework that scales the concurrent real-time video analytics by a novel adaptive hybrid codec integrated with multi-level pipelines, and a global bandwidth controller for multiple video streams. The lower-level front-back-end collaborative mechanism (called adaptive hybrid codec) locally optimizes the accuracy and accelerates end-to-end video analytics for a single stream. The upper-level scheduler aims to accuracy fairness among multiple streams via the global bandwidth controller. The evaluation of
    
[^551]: 更快速的Switchback实验方法

    Faster Rates for Switchback Experiments

    [https://arxiv.org/abs/2312.15574](https://arxiv.org/abs/2312.15574)

    本研究提出了一种更快速的Switchback实验方法，通过使用整个时间块，以 $\sqrt{\log T/T}$ 的速率估计全局平均处理效应。

    

    Switchback实验设计中，一个单独的单元（例如整个系统）在交替的时间块中暴露于一个随机处理，处理并行处理了跨单元和时间干扰问题。Hu和Wager（2022）最近提出了一种截断块起始的处理效应估计器，并在Markov条件下证明了用于估计全局平均处理效应（GATE）的$T^{-1/3}$速率，他们声称这个速率是最优的，并建议将注意力转向不同（且依赖设计）的估计量，以获得更快的速率。对于相同的设计，我们提出了一种替代估计器，使用整个块，并惊人地证明，在相同的假设下，它实际上达到了原始的设计独立GATE估计量的$\sqrt{\log T/T}$的估计速率。

    Switchback experimental design, wherein a single unit (e.g., a whole system) is exposed to a single random treatment for interspersed blocks of time, tackles both cross-unit and temporal interference. Hu and Wager (2022) recently proposed a treatment-effect estimator that truncates the beginnings of blocks and established a $T^{-1/3}$ rate for estimating the global average treatment effect (GATE) in a Markov setting with rapid mixing. They claim this rate is optimal and suggest focusing instead on a different (and design-dependent) estimand so as to enjoy a faster rate. For the same design we propose an alternative estimator that uses the whole block and surprisingly show that it in fact achieves an estimation rate of $\sqrt{\log T/T}$ for the original design-independent GATE estimand under the same assumptions.
    
[^552]: 结构化概率编码

    Structured Probabilistic Coding

    [https://arxiv.org/abs/2312.13933](https://arxiv.org/abs/2312.13933)

    结构化概率编码（SPC）是一种新的监督式表示学习框架，通过编码和预测任务的信息来学习紧凑且信息丰富的表示，提高语言模型的泛化能力和语言理解能力，并通过结构化正则化实现更好的覆盖率。

    

    本论文提出了一种新的监督式表示学习框架，即结构化概率编码（SPC），用于从与目标任务相关的输入中学习紧凑和信息丰富的表示。SPC是一种仅有编码器的概率编码技术，具有来自目标空间的结构化正则化。它可以提高预训练语言模型的泛化能力，以实现更好的语言理解。具体而言，我们的概率编码在一个模块中同时进行信息编码和任务预测，以更充分地利用输入数据中的有效信息。它使用输出空间的变分推断来减少随机性和不确定性。此外，为了更好地控制概率表示的学习过程，在潜在空间中提出了结构化正则化，以促进类别之间的均匀性。通过正则化项，SPC可以保持潜在编码的高斯结构，并实现更好的覆盖率。

    This paper presents a new supervised representation learning framework, namely structured probabilistic coding (SPC), to learn compact and informative representations from input related to the target task. SPC is an encoder-only probabilistic coding technology with a structured regularization from the target space. It can enhance the generalization ability of pre-trained language models for better language understanding. Specifically, our probabilistic coding simultaneously performs information encoding and task prediction in one module to more fully utilize the effective information from input data. It uses variational inference in the output space to reduce randomness and uncertainty. Besides, to better control the learning process of probabilistic representations, a structured regularization is proposed to promote uniformity across classes in the latent space. With the regularization term, SPC can preserve the Gaussian structure of the latent code and achieve better coverage of the 
    
[^553]: 多模态联邦学习中通过原型遮罩和对比处理缺失模态的方法

    Multimodal Federated Learning with Missing Modality via Prototype Mask and Contrast

    [https://arxiv.org/abs/2312.13508](https://arxiv.org/abs/2312.13508)

    本文介绍了一种利用原型遮罩和对比处理来解决多模态联邦学习中缺失模态问题的方法，该方法可以有效缓解由于模态缺失引起的全局模型性能下降。

    

    在现实场景中，多模态联邦学习经常面临复杂的模态缺失问题，这对于构建联邦框架和显著降低模型推理准确性产生限制。现有解决方案通常涉及在客户端开发模态特定的编码器，并在服务器上训练模态融合模块来解决缺失模态的问题。然而，这些方法主要仅适用于具有单模态客户端或完全多模态客户端的特定场景，在复杂的模态缺失场景中很难有效泛化。本文将原型库引入基于FedAvg的联邦学习框架，从而使框架能够在训练和测试期间缓解由模态缺失引起的全局模型性能下降。所提出的方法利用原型作为表示缺失模态的遮罩，以制定任务校准的策略。

    In real-world scenarios, multimodal federated learning often faces the practical challenge of intricate modality missing, which poses constraints on building federated frameworks and significantly degrades model inference accuracy. Existing solutions for addressing missing modalities generally involve developing modality-specific encoders on clients and training modality fusion modules on servers. However, these methods are primarily constrained to specific scenarios with either unimodal clients or complete multimodal clients, struggling to generalize effectively in the intricate modality missing scenarios. In this paper, we introduce a prototype library into the FedAvg-based Federated Learning framework, thereby empowering the framework with the capability to alleviate the global model performance degradation resulting from modality missing during both training and testing. The proposed method utilizes prototypes as masks representing missing modalities to formulate a task-calibrated 
    
[^554]: 在随机森林中节点数和树数之间的权衡

    On the Trade-off between the Number of Nodes and the Number of Trees in a Random Forest

    [https://arxiv.org/abs/2312.11540](https://arxiv.org/abs/2312.11540)

    本文研究了在随机森林中使用较小的决策树集合来代表一个决策树集合的问题，并给出了多数函数和分类错误的表示方法，同时讨论了k-选-n函数的相关结果。

    

    本文致力于随机森林的预测阶段，并研究了使用较小的决策树集合代表一个决策树集合的问题，其中仅考虑二进制决策问题和简单决策树，内部节点只查询单个变量的布尔值。作为主要结果，我们证明了如果n-T是常数，则可以通过一个包含T（<n）个决策树的集合（每个决策树大小为多项式）来表示n个变量的多数函数，其中n和T必须是奇数（以避免决胜局）。我们还表明，如果n-T是常数并且允许小的分类错误，则可以用一个包含T个决策树的集合（每个决策树大小为多项式）来表示一个包含n个决策树的集合。还介绍了k-选-n函数的相关结果。

    In this paper, we focus on the prediction phase of a random forest and study the problem of representing a bag of decision trees using a smaller bag of decision trees, where we only consider binary decision problems on the binary domain and simple decision trees in which an internal node is limited to querying the Boolean value of a single variable. As a main result, we show that the majority function of $n$ variables can be represented by a bag of $T$ ($< n$) decision trees each with polynomial size if $n-T$ is a constant, where $n$ and $T$ must be odd (in order to avoid the tie break). We also show that a bag of $n$ decision trees can be represented by a bag of $T$ decision trees each with polynomial size if $n-T$ is a constant and a small classification error is allowed. A related result on the $k$-out-of-$n$ functions is presented too.
    
[^555]: 光谱状态空间模型

    Spectral State Space Models

    [https://arxiv.org/abs/2312.06837](https://arxiv.org/abs/2312.06837)

    本文提出了一种称为光谱状态空间模型的序列预测架构，通过学习具有光谱滤波算法的线性动态系统实现。这些模型具有可证明的鲁棒性和固定卷积滤波器，适用于需要非常长程记忆的预测任务。

    

    本文研究了具有长程依赖关系的预测任务的序列建模。我们提出了一种基于学习具有光谱滤波算法的线性动态系统的新形式化状态空间模型（SSMs）。这导致了一种称为光谱状态空间模型的新颖序列预测架构。光谱状态空间模型具有两个主要优势。首先，它们具有可证明的鲁棒性属性，因为它们的性能既不依赖于底层动力学的频谱，也不依赖于问题的维度。其次，这些模型是通过固定的卷积滤波器构建的，不需要学习，同时在理论和实践中仍然优于SSMs。基于光谱过滤算法的Spectral state space models在合成动态系统和各种模态的长程预测任务上进行了评估。这些评估支持了光谱滤波在需要非常长程记忆的任务中的理论优势。

    This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model.   Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice.   The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.
    
[^556]: 使用反应器模型的高效并行强化学习框架

    Efficient Parallel Reinforcement Learning Framework using the Reactor Model

    [https://arxiv.org/abs/2312.04704](https://arxiv.org/abs/2312.04704)

    本论文提出了一种使用反应器模型的高效并行强化学习框架，可以有效解决现有框架在同步和输入/输出方面的低效问题。

    

    并行强化学习(Reinforcement Learning, RL)框架对于将RL工作负载映射到多个计算资源非常重要，可以加快样本生成、值估计和策略改进的速度。这些计算模式要求无缝集成训练、服务和模拟工作负载。现有的框架，如Ray，在RL任务中对单个节点上的角色之间的输入/输出和同步要求不够高效。在本研究中，我们提出了一种实现反应器模型的解决方案，该模型强制一组角色具有固定的通信模式。这使得调度程序可以消除需要同步的工作，例如每个角色的锁的获取和释放，或发送和处理协调相关的消息。我们的框架Lingua Franca (LF) 是一种基于反应器模型的协调语言，还在Python中支持真正的并行处理，并提供统一的接口。

    Parallel Reinforcement Learning (RL) frameworks are essential for mapping RL workloads to multiple computational resources, allowing for faster generation of samples, estimation of values, and policy improvement. These computational paradigms require a seamless integration of training, serving, and simulation workloads. Existing frameworks, such as Ray, are not managing this orchestration efficiently, especially in RL tasks that demand intensive input/output and synchronization between actors on a single node. In this study, we have proposed a solution implementing the reactor model, which enforces a set of actors to have a fixed communication pattern. This allows the scheduler to eliminate work needed for synchronization, such as acquiring and releasing locks for each actor or sending and processing coordination-related messages. Our framework, Lingua Franca (LF), a coordination language based on the reactor model, also supports true parallelism in Python and provides a unified interf
    
[^557]: GraphMETRO：通过混合对齐专家来减轻复杂的图分布变化

    GraphMETRO: Mitigating Complex Graph Distribution Shifts via Mixture of Aligned Experts

    [https://arxiv.org/abs/2312.04693](https://arxiv.org/abs/2312.04693)

    GraphMETRO是一种通过混合对齐专家来减轻复杂图分布变化的图神经网络架构，通过门控模型和多个专家模型对不同分布变化进行建模，并设计了新颖的目标函数进行平滑优化，在真实世界数据集上获得了显著的性能提升。

    

    图数据固有复杂且异构，导致自然多样性的分布变化很高。然而，如何构建能够在真实世界中普遍适用于复杂非合成分布变化的机器学习架构仍不清楚。在这里，我们开发了GraphMETRO，一种图神经网络架构，可可靠地建模自然多样性并捕捉复杂的分布变化。GraphMETRO采用了一种混合专家（MoE）架构，其中包含一个门控模型和多个专家模型，每个专家模型针对特定的分布变化产生不变表示，门控模型则识别变化组件。此外，我们设计了一种新颖的目标函数，将不同专家模型的表示进行对齐以确保平滑优化。GraphMETRO在由复杂和自然的真实世界分布变化组成的GOOD基准测试的四个数据集上取得了最先进的结果，提高了67％。

    Graph data are inherently complex and heterogeneous, leading to a high natural diversity of distributional shifts. However, it remains unclear how to build machine learning architectures that generalize to complex non-synthetic distributional shifts naturally occurring in the real world. Here we develop GraphMETRO, a Graph Neural Network architecture, that reliably models natural diversity and captures complex distributional shifts. GraphMETRO employs a Mixture-of-Experts (MoE) architecture with a gating model and multiple expert models, where each expert model targets a specific distributional shift to produce a shift-invariant representation, and the gating model identifies shift components. Additionally, we design a novel objective that aligns the representations from different expert models to ensure smooth optimization. GraphMETRO achieves state-of-the-art results on four datasets from GOOD benchmark comprised of complex and natural real-world distribution shifts, improving by 67%
    
[^558]: 通过高阶导数总结，将牛顿法应用于神经网络的改进

    Adapting Newton's Method to Neural Networks through a Summary of Higher-Order Derivatives

    [https://arxiv.org/abs/2312.03885](https://arxiv.org/abs/2312.03885)

    本论文通过计算高阶导数，将牛顿法应用于神经网络中，提出了一个适用于各种架构的深度神经网络的二阶优化方法。

    

    我们考虑了一种应用于向量变量$\boldsymbol{\theta}$上的函数$\mathcal{L}$的基于梯度的优化方法，在这种情况下，$\boldsymbol{\theta}$被表示为元组$(\mathbf{T}_1, \cdots, \mathbf{T}_S)$的张量。该框架包括许多常见的用例，例如通过梯度下降来训练神经网络。首先，我们提出了一种计算成本低廉的技术，通过自动微分和计算技巧，提供关于$\mathcal{L}$及其张量$\mathbf{T}_s$之间相互作用的高阶信息。其次，我们利用这种技术来建立一个二阶优化方法，适用于训练各种架构的深度神经网络。这个二阶方法利用了$\boldsymbol{\theta}$被分割为张量$(\mathbf{T}_1, \cdots, \mathbf{T}_S)$的分区结构，因此不需要计算$\mathcal{L}$的Hessian矩阵。

    We consider a gradient-based optimization method applied to a function $\mathcal{L}$ of a vector of variables $\boldsymbol{\theta}$, in the case where $\boldsymbol{\theta}$ is represented as a tuple of tensors $(\mathbf{T}_1, \cdots, \mathbf{T}_S)$. This framework encompasses many common use-cases, such as training neural networks by gradient descent. First, we propose a computationally inexpensive technique providing higher-order information on $\mathcal{L}$, especially about the interactions between the tensors $\mathbf{T}_s$, based on automatic differentiation and computational tricks. Second, we use this technique at order 2 to build a second-order optimization method which is suitable, among other things, for training deep neural networks of various architectures. This second-order method leverages the partition structure of $\boldsymbol{\theta}$ into tensors $(\mathbf{T}_1, \cdots, \mathbf{T}_S)$, in such a way that it requires neither the computation of the Hessian of $\mathcal{
    
[^559]: Elijah: 通过分布变化消除扩散模型中的后门注入

    Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift

    [https://arxiv.org/abs/2312.00050](https://arxiv.org/abs/2312.00050)

    Elijah是一种用于扩散模型的后门检测和消除框架，能够准确检测后门并将其效果减少至接近零，而不会显著牺牲模型的实用性。

    

    扩散模型(DM)因其能够从噪声中生成高质量图像而成为最先进的生成模型。然而，最近的研究表明，它们容易受到后门攻击的影响。当一个数据输入（例如一些高斯噪声）被注入触发器（例如一个白色斑点）时，带有后门的模型总是生成目标图像（例如不恰当的照片）。然而，有效的防御策略来减轻DM中的后门问题尚未得到充分探索。为了弥合这一差距，我们提出了第一个适用于DM的后门检测和消除框架。我们使用13种采样器对包括DDPM、NCSN和LDM在内的数百个DM进行评估，针对3种现有的后门攻击。广泛的实验证明，我们的方法可以接近100%的检测准确率，同时将后门效果减少至接近零，而不会显著牺牲模型的实用性。

    Diffusion models (DM) have become state-of-the-art generative models because of their capability to generate high-quality images from noises without adversarial training. However, they are vulnerable to backdoor attacks as reported by recent studies. When a data input (e.g., some Gaussian noise) is stamped with a trigger (e.g., a white patch), the backdoored model always generates the target image (e.g., an improper photo). However, effective defense strategies to mitigate backdoors from DMs are underexplored. To bridge this gap, we propose the first backdoor detection and removal framework for DMs. We evaluate our framework Elijah on hundreds of DMs of 3 types including DDPM, NCSN and LDM, with 13 samplers against 3 existing backdoor attacks. Extensive experiments show that our approach can have close to 100% detection accuracy and reduce the backdoor effects to close to zero without significantly sacrificing the model utility.
    
[^560]: 在Web上揭示语言模型代理在顺序任务组合中的局限性

    Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web

    [https://arxiv.org/abs/2311.18751](https://arxiv.org/abs/2311.18751)

    本文介绍了语言模型代理 (LMA) 在多步决策任务上的有希望的范例，在基本任务上具有出色的性能，但在组合任务上表现不佳。通过平衡数据分布，我们训练了一个新模型 HTML-T5++，在现实应用中取得了超越人类的性能，并在新基准测试中实现了最佳零-shot性能。

    

    最近，语言模型代理(LMA)作为一种在多步决策任务上的有希望的范例出现，通常表现优于人类和其他强化学习代理。尽管有这种希望，但它们在通常涉及任务组合的现实应用中的性能仍未得到充分探索。在这项工作中，我们引入了一个新的基准，叫做CompWoB-反映更现实假设的50个组合性网站自动化任务。我们发现，虽然现有的提示型LMA（gpt-3.5-turbo或gpt-4）在基本任务上实现了94.0％的平均成功率，但在组合任务上降至24.9％的成功率。另一方面，只在基本任务上进行微调的转移性LMA表现出更小的泛化性差距，从85.4％下降到54.8％。通过平衡任务之间的数据分布，我们训练了一个新模型HTML-T5++，在MiniWoB上超过了人类水平的性能（95.2％），并在CompWoB上实现了最佳的零-shot性能（61.5%）。

    Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise o
    
[^561]: 通过熵率最小化实现可预测的强化学习动态

    Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization

    [https://arxiv.org/abs/2311.18703](https://arxiv.org/abs/2311.18703)

    该论文提出了一种名为PA-RL的方法，通过最小化熵率来引导强化学习智能体展现可预测的行为。研究展示了如何利用平均替代奖励实现确定性策略，并在动态模型的基础上近似计算值函数。

    

    在强化学习中，智能体没有动机展示可预测的行为，通常通过策略熵正则化推动智能体在探索上随机化其行为。从人的角度来看，这使得强化学习智能体很难解释和预测；从安全角度来看，更难以进行形式化验证。我们提出了一种新的方法，称为可预测性感知强化学习（PA-RL），用于引导智能体展现可预测的行为，其利用状态序列熵率作为可预测性度量。我们展示了如何将熵率制定为平均奖励目标，并且由于其熵奖励函数依赖于策略，我们引入了一个动作相关的替代熵，以利用PG方法。我们证明了最小化平均替代奖励的确定性策略存在，并且最小化了实际熵率。我们还展示了如何在学习到的动态模型的基础上近似计算与值函数。

    In Reinforcement Learning (RL), agents have no incentive to exhibit predictable behaviors, and are often pushed (through e.g. policy entropy regularization) to randomize their actions in favor of exploration. From a human perspective, this makes RL agents hard to interpret and predict, and from a safety perspective, even harder to formally verify. We propose a novel method to induce predictable behavior in RL agents, referred to as Predictability-Aware RL (PA-RL), which employs the state sequence entropy rate as a predictability measure. We show how the entropy rate can be formulated as an average reward objective, and since its entropy reward function is policy-dependent, we introduce an action-dependent surrogate entropy enabling the use of PG methods. We prove that deterministic policies minimizing the average surrogate reward exist and also minimize the actual entropy rate, and show how, given a learned dynamical model, we are able to approximate the value function associated to th
    
[^562]: 利用指数尺度的深度强化ReLU网络初始化和训练

    Compelling ReLU Network Initialization and Training to Leverage Exponential Scaling with Depth

    [https://arxiv.org/abs/2311.18022](https://arxiv.org/abs/2311.18022)

    该论文提出了一种新的训练策略，通过重新参数化网络权重，使得神经网络的指数数量的激活模式得以展现，从而得到远远超过随机初始化的结果。

    

    ReLU激活的神经网络可以看作是分段线性函数的组合。对于这样的网络，随着深度的增加，表达在输入域上的不同线性区域的数量有可能以指数级增长，但当初始参数选择随机时，不太可能出现这种情况。这种不良的尺度能够导致即使是简单函数也需要使用过大的模型来近似。为了解决这个问题，我们引入了一种新的训练策略：首先以一种方式重新参数化网络权重，使得指数数量的激活模式得以展现。在这些新参数上进行训练可以得到一个初始解，稍后通过更新底层模型权重来改进。这种方法使我们能够产生比随机初始化对应的函数逼近好几个数量级的结果。

    A neural network with ReLU activations may be viewed as a composition of piecewise linear functions. For such networks, the number of distinct linear regions expressed over the input domain has the potential to scale exponentially with depth, but it is not expected to do so when the initial parameters are chosen randomly. This poor scaling can necessitate the use of overly large models to approximate even simple functions. To address this issue, we introduce a novel training strategy: we first reparameterize the network weights in a manner that forces an exponential number of activation patterns to manifest. Training first on these new parameters provides an initial solution that can later be refined by updating the underlying model weights. This approach allows us to produce function approximations that are several orders of magnitude better than their randomly initialized counterparts.
    
[^563]: 在过参数化下分析锐度感知最小化

    Analyzing Sharpness-aware Minimization under Overparameterization

    [https://arxiv.org/abs/2311.17539](https://arxiv.org/abs/2311.17539)

    本文分析了在过参数化条件下的锐度感知最小化方法。通过实证和理论结果，发现过参数化对锐度感知最小化具有重要影响，并且在过参数化增加的情况下，锐度感知最小化仍然受益。

    

    在训练过参数化的神经网络时，尽管训练损失相同，但可以得到具有不同泛化能力的极小值。有证据表明，极小值的锐度与其泛化误差之间存在相关性，因此已经做出了更多努力开发一种优化方法，以显式地找到扁平极小值作为更具有泛化能力的解。然而，至今为止，关于过参数化对锐度感知最小化（SAM）策略的影响的研究还不多。在这项工作中，我们分析了在不同程度的过参数化下的SAM，并提出了实证和理论结果，表明过参数化对SAM具有重要影响。具体而言，我们进行了广泛的数值实验，涵盖了各个领域，并表明存在一种一致的趋势，即SAM在过参数化增加的情况下仍然受益。我们还发现了一些令人信服的案例，说明了过参数化的影响。

    Training an overparameterized neural network can yield minimizers of different generalization capabilities despite the same level of training loss. With evidence that suggests a correlation between sharpness of minima and their generalization errors, increasing efforts have been made to develop an optimization method to explicitly find flat minima as more generalizable solutions. However, this sharpness-aware minimization (SAM) strategy has not been studied much yet as to whether and how it is affected by overparameterization.   In this work, we analyze SAM under overparameterization of varying degrees and present both empirical and theoretical results that indicate a critical influence of overparameterization on SAM. Specifically, we conduct extensive numerical experiments across various domains, and show that there exists a consistent trend that SAM continues to benefit from increasing overparameterization. We also discover compelling cases where the effect of overparameterization is
    
[^564]: ChatTraffic：通过扩散模型实现文本到交通生成

    ChatTraffic: Text-to-Traffic Generation via Diffusion Model

    [https://arxiv.org/abs/2311.16203](https://arxiv.org/abs/2311.16203)

    本研究提出了一种名为ChatTraffic的扩散模型，用于实现文本到交通生成。通过将生成模型与描述交通系统的文本结合，该方法能够解决传统交通预测方法中的两个挑战，并得到与真实交通数据一致的合成数据。

    

    交通预测是智能交通系统中最重要的基础之一。传统的交通预测方法只依赖历史交通数据来预测交通趋势，面临两个主要挑战：1）对异常事件不敏感；2）在长期预测方面性能有限。本文探讨了如何将生成模型与描述交通系统的文本结合起来用于交通生成，将此任务命名为文本到交通生成（TTG）。TTG任务的关键挑战是如何将文本与道路网络的空间结构和交通数据相关联，用于生成交通情况。为此，我们提出了ChatTraffic，这是第一个用于文本到交通生成的扩散模型。为了保证合成数据与真实数据的一致性，我们用图卷积网络（GCN）来扩展扩散模型，以提取交通数据的空间相关性。此外，我们构建了一个包含...

    Traffic prediction is one of the most significant foundations in Intelligent Transportation Systems (ITS). Traditional traffic prediction methods rely only on historical traffic data to predict traffic trends and face two main challenges. 1) insensitivity to unusual events. 2) limited performance in long-term prediction. In this work, we explore how generative models combined with text describing the traffic system can be applied for traffic generation, and name the task Text-to-Traffic Generation (TTG). The key challenge of the TTG task is how to associate text with the spatial structure of the road network and traffic data for generating traffic situations. To this end, we propose ChatTraffic, the first diffusion model for text-to-traffic generation. To guarantee the consistency between synthetic and real data, we augment a diffusion model with the Graph Convolutional Network (GCN) to extract spatial correlations of traffic data. In addition, we construct a large dataset containing t
    
[^565]: 单变量径向基函数层：面向低维输入的脑启发式深度神经网络层

    Univariate Radial Basis Function Layers: Brain-inspired Deep Neural Layers for Low-Dimensional Inputs

    [https://arxiv.org/abs/2311.16148](https://arxiv.org/abs/2311.16148)

    该研究提出了一种新颖的深度神经网络层称为单变量径向基函数层，模拟了大脑中感觉神经元的处理方式，通过对每个输入维度进行专门处理，提供了一种适用于低维输入的替代方法，并在低维函数回归和强化学习任务中证明了其有效性。

    

    深度神经网络已成为函数逼近的标准工具，大多数介绍的架构都是针对高维输入数据开发的。然而，许多实际问题具有低维输入，标准的多层感知机（MLP）是默认选择，而对于专用架构的调查则缺失。我们提出了一种新颖的DNN层，称为单变量径向基函数（U-RBF）层作为替代方法。类似于大脑中的感觉神经元，U-RBF层使用一组神经元处理每个单独的输入维度，其激活依赖于不同的首选输入值。我们在低维函数回归和强化学习任务中验证了其有效性与MLP相比。结果表明，当目标函数变得复杂且难以逼近时，U-RBF尤为有优势。

    Deep Neural Networks (DNNs) became the standard tool for function approximation with most of the introduced architectures being developed for high-dimensional input data. However, many real-world problems have low-dimensional inputs for which standard Multi-Layer Perceptrons (MLPs) are the default choice. An investigation into specialized architectures is missing. We propose a novel DNN layer called Univariate Radial Basis Function (U-RBF) layer as an alternative. Similar to sensory neurons in the brain, the U-RBF layer processes each individual input dimension with a population of neurons whose activations depend on different preferred input values. We verify its effectiveness compared to MLPs in low-dimensional function regressions and reinforcement learning tasks. The results show that the U-RBF is especially advantageous when the target function becomes complex and difficult to approximate.
    
[^566]: 用于评估潜在表示多样性的度量空间大小

    Metric Space Magnitude for Evaluating the Diversity of Latent Representations

    [https://arxiv.org/abs/2311.16054](https://arxiv.org/abs/2311.16054)

    基于度量空间大小的潜在表示多样性度量，可稳定计算，能够进行多尺度比较，在多个领域和任务中展现出优越性能。

    

    度量空间的大小是一种近期建立的不变性，能够在多个尺度上提供空间的“有效大小”的衡量，并捕捉到许多几何属性。我们发展了一系列基于大小的潜在表示内在多样性度量，形式化了有限度量空间大小函数之间的新颖不相似性概念。我们的度量在数据扰动下保证稳定，可以高效计算，并且能够对潜在表示进行严格的多尺度比较。我们展示了我们的度量在实验套件中的实用性和卓越性能，包括不同领域和任务的多样性评估、模式崩溃检测以及用于文本、图像和图形数据的生成模型评估。

    The magnitude of a metric space is a recently-established invariant, providing a measure of the 'effective size' of a space across multiple scales while also capturing numerous geometrical properties. We develop a family of magnitude-based measures of the intrinsic diversity of latent representations, formalising a novel notion of dissimilarity between magnitude functions of finite metric spaces. Our measures are provably stable under perturbations of the data, can be efficiently calculated, and enable a rigorous multi-scale comparison of latent representations. We show the utility and superior performance of our measures in an experimental suite that comprises different domains and tasks, including the evaluation of diversity, the detection of mode collapse, and the evaluation of generative models for text, image, and graph data.
    
[^567]: 数据多样性对鲁棒指令调整至关重要

    Data Diversity Matters for Robust Instruction Tuning

    [https://arxiv.org/abs/2311.14736](https://arxiv.org/abs/2311.14736)

    数据多样性对鲁棒指令调整非常重要，我们提出了一种新算法(QDIT)，通过同时控制数据集的多样性和质量，我们深入研究了多样性和质量对指令调整性能的影响，并得出了两个关键观点。

    

    最近的研究表明，通过精选高质量且多样化的指令调整数据集，我们可以显著提高指令跟随能力。然而，创建这样的数据集非常困难，大多数研究依赖于手动精选或专有语言模型。自动数据精选很困难，因为仍不清楚如何为指令调整定义多样性，多样性和质量如何相互关联，以及如何优化数据集的质量和多样性。为解决这些问题，我们提出了一种新算法，质量-多样性指令调整(QDIT)。QDIT提供了一种简单的方法来同时控制数据集的多样性和质量，使我们能够深入研究多样性和质量对指令调整性能的影响。从这项研究中，我们得出了两个关键观点：(1)数据多样性和质量之间存在自然的权衡关系，(2)增加数据多样性显著提高最坏情况下的指令跟随性能。

    Recent works have shown that by curating high quality and diverse instruction tuning datasets, we can significantly improve instruction-following capabilities. However, creating such datasets is difficult and most works rely on manual curation or proprietary language models. Automatic data curation is difficult as it is still not clear how we can define diversity for instruction tuning, how diversity and quality depend on one other, and how we can optimize dataset quality and diversity. To resolve these issue, we propose a new algorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simple method to simultaneously control dataset diversity and quality, allowing us to conduct an in-depth study on the effect of diversity and quality on instruction tuning performance. From this study we draw two key insights (1) there is a natural tradeoff between data diversity and quality and (2) increasing data diversity significantly improves the worst case instruction following perform
    
[^568]: 一种超长Token注意力近似的单次流算法

    One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space

    [https://arxiv.org/abs/2311.14652](https://arxiv.org/abs/2311.14652)

    本文研究了在超长上下文下内存效率的问题，提出一种用于超长Token注意力近似的单次流算法，通过构建矩阵$U_1, U_2$加速注意力计算，解决了部署大型语言模型时的计算资源问题。

    

    注意力计算同时具有$O(n^2)$的时间复杂度和$O(n^2)$的空间复杂度，这使得在需要大量计算资源的流应用中部署大型语言模型(Large Language Models，LLMs)变得困难。在最近的OpenAI DevDay（2023年11月6日），OpenAI发布了一种能够支持128K长文档的新模型，在我们的论文中，我们关注的是当上下文长度$n$远大于128K ($n \gg 2^d$)时的内存有效问题。考虑到具有 Query、Key 和 Value 矩阵$Q, K, V \in \mathbb{R}^{n \times d}$的单层自注意力，多项式方法近似了注意力输出$T \in \mathbb{R}^{n \times d}$。它通过构建$U_1, U_2 \in \mathbb{R}^{n \times t}$在$n^{1+o(1)}$次时间执行内加速注意力计算${\sf Attn}(Q, K, V)$。尽管如此，计算近似的注意力矩阵$U_1U_2^\top \in \mathbb{R}^{n \times n}$仍需要$O(n^2)$的空间。

    Attention computation takes both the time complexity of $O(n^2)$ and the space complexity of $O(n^2)$ simultaneously, which makes deploying Large Language Models (LLMs) in streaming applications that involve long contexts requiring substantial computational resources. In recent OpenAI DevDay (Nov 6, 2023), OpenAI released a new model that is able to support a 128K-long document, in our paper, we focus on the memory-efficient issue when context length $n$ is much greater than 128K ($n \gg 2^d$). Considering a single-layer self-attention with Query, Key, and Value matrices $Q, K, V \in \mathbb{R}^{n \times d}$, the polynomial method approximates the attention output $T \in \mathbb{R}^{n \times d}$. It accomplishes this by constructing $U_1, U_2 \in \mathbb{R}^{n \times t}$ to expedite attention ${\sf Attn}(Q, K, V)$ computation within $n^{1+o(1)}$ time executions. Despite this, computing the approximated attention matrix $U_1U_2^\top \in \mathbb{R}^{n \times n}$ still necessitates $O(n^2
    
[^569]: 假设简化和数据自适应的后预测推断

    Assumption-lean and Data-adaptive Post-Prediction Inference

    [https://arxiv.org/abs/2311.14220](https://arxiv.org/abs/2311.14220)

    这项工作介绍了一种假设简化和数据自适应的后预测推断（POP-Inf）过程，可以有效且有力地基于机器学习预测结果进行统计推断。

    

    现代科学研究面临的主要挑战是黄金标准数据的有限可用性，而获取这些数据既耗费时间又费力。随着机器学习（ML）的快速发展，科学家们依赖于ML算法使用易得的协变量来预测这些黄金标准结果。然而，这些预测结果常常直接用于后续的统计分析中，忽略了预测过程引入的不精确性和异质性。这可能导致虚假的正面结果和无效的科学结论。在这项工作中，我们介绍了一种假设简化和数据自适应的后预测推断（POP-Inf）过程，它允许基于ML预测结果进行有效和有力的推断。它的“假设简化”属性保证在广泛的统计量上不基于ML预测做出可靠的统计推断。它的“数据自适应”特性保证了相较于现有方法的效率提高。

    A primary challenge facing modern scientific research is the limited availability of gold-standard data which can be both costly and labor-intensive to obtain. With the rapid development of machine learning (ML), scientists have relied on ML algorithms to predict these gold-standard outcomes with easily obtained covariates. However, these predicted outcomes are often used directly in subsequent statistical analyses, ignoring imprecision and heterogeneity introduced by the prediction procedure. This will likely result in false positive findings and invalid scientific conclusions. In this work, we introduce an assumption-lean and data-adaptive Post-Prediction Inference (POP-Inf) procedure that allows valid and powerful inference based on ML-predicted outcomes. Its "assumption-lean" property guarantees reliable statistical inference without assumptions on the ML-prediction, for a wide range of statistical quantities. Its "data-adaptive'" feature guarantees an efficiency gain over existing
    
[^570]: 以采样为导向: Langevin动力学的预测编码

    Sample as You Infer: Predictive Coding With Langevin Dynamics

    [https://arxiv.org/abs/2311.13664](https://arxiv.org/abs/2311.13664)

    本文提出了以采样为导向的Langevin动力学的预测编码算法，通过对PC推理过程注入高斯噪声实现过阻尼的Langevin采样，并改进了结果编码器自由训练方法，通过编码器网络提供摊销的热启动。此外，还验证了一种轻量级且易于计算的预处理形式，使得算法具有更好的性能和鲁棒性。

    

    我们提出了一种新颖的算法，用于在通用深度生成模型中学习参数，该算法建立在计算神经科学的预测编码(PC)框架之上。我们的方法修改了标准的PC算法，使其性能与标准变分自动编码器(VAE)训练的性能相当甚至超过。通过将高斯噪声注入PC推理过程中，我们重新将其构想为过阻尼的Langevin采样，从而方便对紧凑证据下界(ELBO)进行优化。我们改进了结果编码器自由训练方法，通过将编码器网络纳入其中，为我们的Langevin采样提供了一种摊销的热启动，并测试了三种不同的目标。最后，为了增加对采样步长的鲁棒性，并减少对曲率的敏感性，我们验证了一种轻量级且易于计算的预处理形式，受到Riemann Manifold Langevin和SGD文献中的自适应优化器的启发。我们与...

    We present a novel algorithm for parameter learning in generic deep generative models that builds upon the predictive coding (PC) framework of computational neuroscience. Our approach modifies the standard PC algorithm to bring performance on-par and exceeding that obtained from standard variational auto-encoder (VAE) training. By injecting Gaussian noise into the PC inference procedure we re-envision it as an overdamped Langevin sampling, which facilitates optimisation with respect to a tight evidence lower bound (ELBO). We improve the resultant encoder-free training method by incorporating an encoder network to provide an amortised warm-start to our Langevin sampling and test three different objectives for doing so. Finally, to increase robustness to the sampling step size and reduce sensitivity to curvature, we validate a lightweight and easily computable form of preconditioning, inspired by Riemann Manifold Langevin and adaptive optimizers from the SGD literature. We compare agains
    
[^571]: 基于分解的多目标强化学习：分类和框架

    Multi-Objective Reinforcement Learning Based on Decomposition: A Taxonomy and Framework

    [https://arxiv.org/abs/2311.12495](https://arxiv.org/abs/2311.12495)

    基于分解的多目标强化学习（MORL/D）是一种新的方法，连接了强化学习和多目标优化领域。该论文提出了一个全面的MORL/D分类法，为现有和潜在的MORL工作提供了结构化的基础。

    

    多目标强化学习（MORL）通过寻求权衡冲突目标的策略扩展了传统的强化学习。对MORL的不断兴趣引发了各种各样的研究和解决方法，通常借鉴了基于分解的多目标优化的现有知识。然而，现有文献中缺乏基于RL和MOO/D的清晰分类。因此，MORL研究人员在尝试将贡献归类到更广泛的背景中时遇到困难，因为缺乏标准化的分类法。为了解决这个问题，本文介绍了基于分解的多目标强化学习（MORL/D），这是一种将RL和MOO文献联系起来的新方法。提出了一个全面的MORL/D分类法，为分类现有和潜在的MORL工作提供了结构化的基础。然后使用该分类法对MORL研究进行细致审查，增强了清晰度和简洁性。

    Multi-objective reinforcement learning (MORL) extends traditional RL by seeking policies making different compromises among conflicting objectives. The recent surge of interest in MORL has led to diverse studies and solving methods, often drawing from existing knowledge in multi-objective optimization based on decomposition (MOO/D). Yet, a clear categorization based on both RL and MOO/D is lacking in the existing literature. Consequently, MORL researchers face difficulties when trying to classify contributions within a broader context due to the absence of a standardized taxonomy. To tackle such an issue, this paper introduces multi-objective reinforcement learning based on decomposition (MORL/D), a novel methodology bridging the literature of RL and MOO. A comprehensive taxonomy for MORL/D is presented, providing a structured foundation for categorizing existing and potential MORL works. The introduced taxonomy is then used to scrutinize MORL research, enhancing clarity and concisenes
    
[^572]: 从一般环境中学习因果表示：可辨识性和内在歧义

    Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity

    [https://arxiv.org/abs/2311.12267](https://arxiv.org/abs/2311.12267)

    该论文研究了从一般环境中学习因果表示的问题，提供了基于这种环境生成的数据的可辨识性结果，并指出了受到围绕节点歧义的限制。同时提出了一个算法可以恢复出地面真实模型

    

    我们研究因果表示学习，即从低级观测数据（如文本和图像）中恢复高级潜在变量及其因果关系的任务，假设可以访问从多个环境生成的观察结果。之前关于因果表示可辨识性的结果通常假设可以访问单节点干预，但实际上这是不切实际的，因为潜在变量本身就未知。在本研究中，我们提供了基于来自一般环境的数据的第一个可辨识性结果。我们展示了对于线性因果模型，虽然可以完全恢复因果图，但潜在变量只能被识别到受到围绕节点歧义（SNA）的程度上。我们提供了我们保证的对应对，证明了在我们的设置中SNA基本上是不可避免的。我们还提出了一个算法LiNGCReL，可以被证明可以恢复出地面真实模型

    We study causal representation learning, the task of recovering high-level latent variables and their causal relationships in the form of a causal graph from low-level observed data (such as text and images), assuming access to observations generated from multiple environments. Prior results on the identifiability of causal representations typically assume access to single-node interventions which is rather unrealistic in practice, since the latent variables are unknown in the first place. In this work, we provide the first identifiability results based on data that stem from general environments. We show that for linear causal models, while the causal graph can be fully recovered, the latent variables are only identified up to the surrounded-node ambiguity (SNA) \citep{varici2023score}. We provide a counterpart of our guarantee, showing that SNA is basically unavoidable in our setting. We also propose an algorithm, \texttt{LiNGCReL} which provably recovers the ground-truth model up to
    
[^573]: 重新思考注意力：探索将浅层前馈神经网络作为Transformers中注意力层的替代方法

    Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers

    [https://arxiv.org/abs/2311.10642](https://arxiv.org/abs/2311.10642)

    本研究探索使用浅层前馈神经网络替代注意力机制，通过知识蒸馏方法训练，实验证明了这种"无注意力的Transformers"可以与原始架构的性能媲美，并揭示了其简化复杂架构的潜力。

    

    本研究分析了使用标准的浅层前馈网络来模仿Transformer模型中注意力机制的有效性。我们使用知识蒸馏的方法，将Transformer中的关键元素替换为简单的前馈网络，并使用原始组件进行训练。我们在IWSLT2017数据集上进行实验证明了这种“无注意力的Transformers”可以与原始架构的性能媲美。通过严谨的实验和不同替代网络类型和大小的尝试，我们提供了支持我们方法可行性的见解。这不仅揭示了浅层前馈网络在模仿注意力机制方面的适应性，而且强调了它们在简化序列任务的复杂架构方面的潜力。

    This work presents an analysis of the effectiveness of using standard shallow feed-forward networks to mimic the behavior of the attention mechanism in the original Transformer model, a state-of-the-art architecture for sequence-to-sequence tasks. We substitute key elements of the attention mechanism in the Transformer with simple feed-forward networks, trained using the original components via knowledge distillation. Our experiments, conducted on the IWSLT2017 dataset, reveal the capacity of these "attentionless Transformers" to rival the performance of the original architecture. Through rigorous ablation studies, and experimenting with various replacement network types and sizes, we offer insights that support the viability of our approach. This not only sheds light on the adaptability of shallow feed-forward networks in emulating attention mechanisms but also underscores their potential to streamline complex architectures for sequence-to-sequence tasks.
    
[^574]: 基于惊喜性驱动的稳健可解释的非参数学习中的k-NN算法

    Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning

    [https://arxiv.org/abs/2311.10246](https://arxiv.org/abs/2311.10246)

    本论文提出了一种基于惊喜性驱动的稳健可解释的k-NN算法，通过使用信息论的角度对传统算法进行新的阐释，实现了在非参数学习中的分类、回归、密度估计和异常检测等任务。

    

    非参数学习是机器学习中的一个基本概念，旨在捕捉数据中的复杂模式和关系，而不对潜在的数据分布做出强烈的假设。在这一范式下，最为著名的算法之一是k最近邻（k-NN）算法。在这项工作中，我们通过使用机器学习在安全关键应用中的应用，从信息论的角度对传统的最近邻算法进行了新的阐释，并提出了一种稳健可解释的框架，用于分类、回归、密度估计和异常检测等任务。我们可以通过计算增加特征时的条件熵来确定数据点的权重和特征的贡献，而无需进行显式的模型训练。这使我们能够通过提供详细的数据点影响权重来计算特征的贡献。

    Nonparametric learning is a fundamental concept in machine learning that aims to capture complex patterns and relationships in data without making strong assumptions about the underlying data distribution. Owing to simplicity and familiarity, one of the most well-known algorithms under this paradigm is the $k$-nearest neighbors ($k$-NN) algorithm. Driven by the usage of machine learning in safety-critical applications, in this work, we shed new light on the traditional nearest neighbors algorithm from the perspective of information theory and propose a robust and interpretable framework for tasks such as classification, regression, density estimation, and anomaly detection using a single model. We can determine data point weights as well as feature contributions by calculating the conditional entropy for adding a feature without the need for explicit model training. This allows us to compute feature contributions by providing detailed data point influence weights with perfect attributi
    
[^575]: 语言模型与人脑的差异

    Divergences between Language Models and Human Brains

    [https://arxiv.org/abs/2311.09308](https://arxiv.org/abs/2311.09308)

    该论文系统地探索了语言模型（LMs）和人类大脑在语言处理方面的差异，发现在社交/情感智能和物理常识领域，LMs无法很好地捕捉到人类的表现，但在这些领域对LMs进行微调可以提高其性能。

    

    机器和人类是否以相似的方式处理语言？最近的研究暗示肯定，发现大脑信号可以通过语言模型（LMs）的内部表示有效地进行预测。尽管这样的结果被认为反映了LMs和人类大脑之间的共享计算原理，但LMs和人类在语言表示和使用上也存在明显的差异。在这项工作中，我们通过检查LM表示和人类大脑对语言的响应之间的差异，通过采用两个数据集对受试者阅读和听叙述故事的方式，系统地探索了人类和机器语言处理之间的分歧。通过数据驱动的方法，我们确定了两个领域，即社交/情感智能和物理常识，这些领域在LMs中无法很好地捕捉到。然后，我们使用人类行为实验验证了这些领域，并证明在这些领域对LMs进行微调可以改善其性能。

    Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve th
    
[^576]: 数据污染问题: 一种检测和估计大型语言模型中污染的工具

    Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models

    [https://arxiv.org/abs/2311.06233](https://arxiv.org/abs/2311.06233)

    这个工具使用数据污染问题（DCQ）的方法来检测和估计大型语言模型中的数据污染。在DCQ中，我们创建了每个数据集实例的扰动版本，并让语言模型从中选择原始实例，通过词级扰动来区分选项。这种方法利用了语言模型在预训练阶段暴露于原始实例时的固有特性。

    

    我们提出了数据污染问题（DCQ），这是一种简单而有效的方法，用于检测大型语言模型（LLM）中的数据污染并估计其数量。具体而言，我们将数据污染检测视为一系列的多项选择问题，并设计了一种测验形式，其中创建了每个数据集实例的三个扰动版本。这些变化仅包括词级扰动。生成的扰动版本与原始实例一起形成DCQ中的选项，额外的选项适应了提供的选择都不正确的可能性。鉴于在选择之间唯一的区别信号是与原始实例的确切措辞相关，如果在预训练阶段已经接触到原始实例，语言模型当被要求从选项中识别原始实例时，倾向于选择原始实例--这是语言模型固有的特性。在使用GPT-4/3.5进行多个数据集的测试中，我们的结果完全缺少准确性。

    We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions and devise a quiz format wherein three perturbed versions of each dataset instance are created. These changes only include word-level perturbations. The generated perturbed versions, along with the original instance, form the options in the DCQ, with an extra option accommodating the possibility that none of the provided choices is correct. Given that the only distinguishing signal among the choices is the exact wording relative to the original instance, an LLM, when tasked with identifying the original instance from the choices, gravitates towards the original one if it has been exposed to it in its pre-training phase--a trait intrinsic to LLMs. Tested over several datasets with GPT-4/3.5, our findings--while fully lacking acc
    
[^577]: PowerFlowNet: 使用消息传递图神经网络进行功率流近似

    PowerFlowNet: Power Flow Approximation Using Message Passing Graph Neural Networks

    [https://arxiv.org/abs/2311.03415](https://arxiv.org/abs/2311.03415)

    PowerFlowNet 是一种使用消息传递图神经网络进行功率流近似的新型架构，与传统的牛顿-拉夫逊方法相比，在简单的系统中速度提高了4倍，在实际的法国高电压网络中提高了145倍，同时在性能和执行时间方面明显优于其他传统方法。

    

    准确且高效的功率流分析对于现代电力网络的运行和规划至关重要。因此，需要能够为小型和大型电力网络提供准确和快速解的可扩展算法。由于电力网络可以被解释为一个图，图神经网络(GNNs)已经成为通过利用底层图结构的信息共享来改善功率流近似的准确性和速度的一种有前景的方法。在这项研究中，我们介绍了PowerFlowNet，一个新颖的GNN架构，用于功率流近似，在简单的IEEE 14总线系统中与传统的牛顿-拉夫逊方法展示了相似的性能，但在法国高电压网络(6470rte)的真实情况下实现了4倍的速度提升。同时，与其他传统的近似方法(如直流松弛法)相比，在性能和执行时间方面显著优于它们，从而实现了优越的表现。

    Accurate and efficient power flow (PF) analysis is crucial in modern electrical networks' operation and planning. Therefore, there is a need for scalable algorithms that can provide accurate and fast solutions for both small and large scale power networks. As the power network can be interpreted as a graph, Graph Neural Networks (GNNs) have emerged as a promising approach for improving the accuracy and speed of PF approximations by exploiting information sharing via the underlying graph structure. In this study, we introduce PowerFlowNet, a novel GNN architecture for PF approximation that showcases similar performance with the traditional Newton-Raphson method but achieves it 4 times faster in the simple IEEE 14-bus system and 145 times faster in the realistic case of the French high voltage network (6470rte). Meanwhile, it significantly outperforms other traditional approximation methods, such as the DC relaxation method, in terms of performance and execution time; therefore, making P
    
[^578]: 基于一阶逻辑约束的多任务核机器学习

    Multitask Kernel-based Learning with First-Order Logic Constraints

    [https://arxiv.org/abs/2311.03340](https://arxiv.org/abs/2311.03340)

    本文提出了一个通用框架，将有监督和无监督示例与一阶逻辑背景知识整合到核机器学习中，实现多任务学习。通过将一阶逻辑约束转化为连续实现的形式，有效处理基于核的谓词的输出。

    

    本文提出了一个通用的框架，将由一系列一阶逻辑子句表达的背景知识与有监督和无监督示例整合到核机器中。特别地，我们考虑了一个多任务学习方案，在该方案中，定义在一组对象上的多个谓词需要从示例中共同学习，并对其值的合法配置施加一系列的FOL约束。这些谓词是定义在输入对象表示的特征空间上的，并且可以是已知的事先定义好的，也可以由适当的基于核的学习器进行近似。我们提出了一种通用的方法，将FOL子句转化为一个连续实现，能够处理由基于核的谓词计算出的输出。该学习问题被视为半监督任务，需要在损失函数的原属上进行优化，该损失函数包括对有监督示例的拟合损失度量、正则化项和

    In this paper we propose a general framework to integrate supervised and unsupervised examples with background knowledge expressed by a collection of first-order logic clauses into kernel machines. In particular, we consider a multi-task learning scheme where multiple predicates defined on a set of objects are to be jointly learned from examples, enforcing a set of FOL constraints on the admissible configurations of their values. The predicates are defined on the feature spaces, in which the input objects are represented, and can be either known a priori or approximated by an appropriate kernel-based learner. A general approach is presented to convert the FOL clauses into a continuous implementation that can deal with the outputs computed by the kernel-based predicates. The learning problem is formulated as a semi-supervised task that requires the optimization in the primal of a loss function that combines a fitting loss measure on the supervised examples, a regularization term, and a 
    
[^579]: 语言模型就像超级马里奥：通过吸收同源模型的能力来实现免费午餐

    Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch

    [https://arxiv.org/abs/2311.03099](https://arxiv.org/abs/2311.03099)

    本文揭示了语言模型可以通过吸收同源模型的参数来获得新的能力，而无需重新训练或使用GPU。作者提出了DARE技术来稀疏化参数并将多个同源模型合并为一个模型。实验证明，DARE可以轻松删除大部分参数并实现多任务融合。

    

    在本文中，我们揭示了语言模型(LMs)可以通过吸收同源模型的参数来获得新的能力，而无需重新训练或使用GPU。我们首先引入了DARE来将大多数delta参数（即微调和预训练参数之间的差异）设置为零，而不会影响监督微调(SFT) LMs的能力，DARE通过随机删除比率为p的delta参数，并通过1/(1 - p)重新缩放剩余参数来近似原始嵌入。然后，我们将DARE作为一种通用的即插即用技术来稀疏化多个SFT同源模型的delta参数，以减轻参数干扰，并通过参数融合将它们合并为一个模型。我们通过编码器和解码器为基础的LM进行实验，结果表明：（1）SFT delta参数值范围通常很小（在0.005以内），具有极高的冗余，DARE可以轻松删除90%甚至99%的参数。（2）DARE可以将多个任务特定的LM合并为一个LM，并有驾驶技能

    In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio p And REscales the remaining ones by 1/(1 - p) to approximate the original embeddings. Then, we use DARE as a versatile plug-and-play technique to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.005) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them. (2) DARE can merge multiple task-specific LMs into one LM with dive
    
[^580]: 集群网络干扰下的个体化策略评估与学习

    Individualized Policy Evaluation and Learning under Clustered Network Interference

    [https://arxiv.org/abs/2311.02467](https://arxiv.org/abs/2311.02467)

    本文研究了集群网络干扰下个体化策略评估与学习的问题，提出了一种只假设半参数结构模型的方法，能够更准确地评估和学习最优的个体化处理规则。

    

    尽管现在有很多关于政策评估和学习的文献，但大部分之前的工作都假设一个个体的处理分配不会影响另一个个体的结果。不幸的是，忽视干扰可能导致评估偏误和无效的学习策略。例如，处理有很多朋友的有影响力的个体可能产生正向溢出效应，从而改善个体化处理规则（ITR）的整体性能。我们考虑在集群网络干扰（也称为部分干扰）下评估和学习最优ITR的问题，在该问题中，单位聚类从一个总体中抽样，并且在每个聚类中单位之间可能互相影响。与以前的方法强制限制溢出效应不同，所提出的方法只假设半参数结构模型，每个单位的结果是聚类中的个体处理的加法函数。

    While there now exists a large literature on policy evaluation and learning, much of prior work assumes that the treatment assignment of one unit does not affect the outcome of another unit. Unfortunately, ignoring interference may lead to biased policy evaluation and ineffective learned policies. For example, treating influential individuals who have many friends can generate positive spillover effects, thereby improving the overall performance of an individualized treatment rule (ITR). We consider the problem of evaluating and learning an optimal ITR under clustered network interference (also known as partial interference) where clusters of units are sampled from a population and units may influence one another within each cluster. Unlike previous methods that impose strong restrictions on spillover effects, the proposed methodology only assumes a semiparametric structural model where each unit's outcome is an additive function of individual treatments within the cluster. Under this 
    
[^581]: 对抗样本不是真正的特征

    Adversarial Examples Are Not Real Features

    [https://arxiv.org/abs/2310.18936](https://arxiv.org/abs/2310.18936)

    该论文从多个学习范式的角度重新审视对抗样本的理论，发现非鲁棒特征在多种无监督学习范式中的效用较差，揭示了这些特征并不像鲁棒特征或自然特征那样真正有用。

    

    对抗样本的存在多年来一直是一个谜团，并引起了广泛的关注。一种由Ilyas等人提出的著名理论从数据的角度解释了对抗性脆弱性，即通过展示可以从对抗样本中提取非鲁棒特征，并且这些特征单独用于分类是有用的。然而，这种解释仍然相当反直觉，因为非鲁棒特征对于人类来说大部分是噪声特征。在本文中，我们从更大的背景下重新审视了这个理论，结合了多个学习范式。值得注意的是，我们发现与在监督学习中的良好效用相反，当将非鲁棒特征转移到其他无监督学习范式时（如对比学习、遮挡图像建模和扩散模型），它们的效用变差。这揭示了非鲁棒特征并不像在这些范式之间享有良好可迁移性的鲁棒特征或自然特征那样真正有用。同时，对于鲁棒特征而言，在自监督学习中的效用与监督学习中的效用相当。

    The existence of adversarial examples has been a mystery for years and attracted much interest. A well-known theory by \citet{ilyas2019adversarial} explains adversarial vulnerability from a data perspective by showing that one can extract non-robust features from adversarial examples and these features alone are useful for classification. However, the explanation remains quite counter-intuitive since non-robust features are mostly noise features to humans. In this paper, we re-examine the theory from a larger context by incorporating multiple learning paradigms. Notably, we find that contrary to their good usefulness under supervised learning, non-robust features attain poor usefulness when transferred to other self-supervised learning paradigms, such as contrastive learning, masked image modeling, and diffusion models. It reveals that non-robust features are not really as useful as robust or natural features that enjoy good transferability between these paradigms. Meanwhile, for robus
    
[^582]: 重新思考基于偏差-方差分解的半监督不平衡节点分类问题

    Rethinking Semi-Supervised Imbalanced Node Classification from Bias-Variance Decomposition

    [https://arxiv.org/abs/2310.18765](https://arxiv.org/abs/2310.18765)

    本文提出了一种新的方法来解决图神经网络中的类别不平衡问题。该方法将不平衡节点分类和偏差-方差分解相结合，利用图扩充技术估计方差，并通过正则项减轻不平衡的影响。在多个基准数据集上进行的测试结果表明，该方法在各种不平衡场景中优于现有最先进的方法，并为解决GNN中的不平衡节点分类问题提供了一种新颖的理论视角。

    

    本文介绍了一种新的方法来解决图神经网络（GNN）中的类别不平衡问题。我们的方法将不平衡节点分类和偏差-方差分解相结合，建立了一个将数据不平衡与模型方差密切相关的理论框架。我们还利用图扩充技术来估计方差，并设计了一个正则项来减轻不平衡的影响。我们在多个基准数据集上进行了详尽的测试，包括自然不平衡的数据集和公开划分的类别不平衡数据集，结果表明我们的方法在各种不平衡场景中优于现有最先进的方法。该工作为解决GNN中的不平衡节点分类问题提供了一种新颖的理论视角。

    This paper introduces a new approach to address the issue of class imbalance in graph neural networks (GNNs) for learning on graph-structured data. Our approach integrates imbalanced node classification and Bias-Variance Decomposition, establishing a theoretical framework that closely relates data imbalance to model variance. We also leverage graph augmentation technique to estimate the variance, and design a regularization term to alleviate the impact of imbalance. Exhaustive tests are conducted on multiple benchmarks, including naturally imbalanced datasets and public-split class-imbalanced datasets, demonstrating that our approach outperforms state-of-the-art methods in various imbalanced scenarios. This work provides a novel theoretical perspective for addressing the problem of imbalanced node classification in GNNs.
    
[^583]: MicroNAS: 针对微控制器上的时间序列分类的记忆和延迟约束的硬件感知神经架构搜索

    MicroNAS: Memory and Latency Constrained Hardware-Aware Neural Architecture Search for Time Series Classification on Microcontrollers

    [https://arxiv.org/abs/2310.18384](https://arxiv.org/abs/2310.18384)

    MicroNAS是针对微控制器上时间序列分类的硬件感知神经架构搜索系统，能够生成满足用户定义的执行延迟和内存消耗限制的优化架构。

    

    设计特定领域的神经网络是一项耗时、容易出错且昂贵的任务。神经结构搜索(NAS)的存在是为了简化特定领域模型的开发，但文献中缺乏关于微控制器上的时间序列分类方面的研究。因此，我们将可微分神经结构搜索(DNAS)的概念应用于解决资源受限的微控制器上的时间序列分类问题。我们引入MicroNAS，这是一个领域特定的硬件感知神经架构搜索系统，集成了DNAS、延迟查找表、动态卷积和专门设计用于微控制器上的时间序列分类的新型搜索空间。所得到的系统具有硬件感知能力，可以生成满足用户定义的执行延迟和峰值内存消耗限制的神经网络架构。我们在不同的微控制器和标准基准数据集上进行了广泛的研究，证明MicroNAS可以找到适用于微控制器的架构，实现性能(F1-score)的最优化。

    Designing domain specific neural networks is a time-consuming, error-prone, and expensive task. Neural Architecture Search (NAS) exists to simplify domain-specific model development but there is a gap in the literature for time series classification on microcontrollers. Therefore, we adapt the concept of differentiable neural architecture search (DNAS) to solve the time-series classification problem on resource-constrained microcontrollers (MCUs). We introduce MicroNAS, a domain-specific HW-NAS system integration of DNAS, Latency Lookup Tables, dynamic convolutions and a novel search space specifically designed for time-series classification on MCUs. The resulting system is hardware-aware and can generate neural network architectures that satisfy user-defined limits on the execution latency and peak memory consumption. Our extensive studies on different MCUs and standard benchmark datasets demonstrate that MicroNAS finds MCU-tailored architectures that achieve performance (F1-score) ne
    
[^584]: HelmFluid：学习Helmholtz动力学进行可解释的流体预测

    HelmFluid: Learning Helmholtz Dynamics for Interpretable Fluid Prediction

    [https://arxiv.org/abs/2310.10565](https://arxiv.org/abs/2310.10565)

    本文提出了HelmFluid，一个精确且可解释的流体预测器。通过学习Helmholtz动力学，将流体动力学分解为更可解的无旋和无散部分，并结合多尺度多头积分架构进行集成，HelmFluid能够更准确地预测流体的未来行为。

    

    流体预测是一个长期存在的挑战，由于固有的高维非线性动力学。先前的方法通常利用深度模型的非线性建模能力直接估计未来预测的速度场。然而，直接学习表面速度场而跳过固有的物理特性将导致模型难以生成精确或具有物理可靠性的结果。本文提出了HelmFluid，针对流体的精确和可解释的预测器。受Helmholtz定理的启发，我们设计了HelmDynamics模块，用于学习Helmholtz动力学，将流体动力学分解为更可解的无旋和无散部分，物理上对应于流体的势函数和流函数。通过将HelmDynamics模块嵌入到多尺度多头积分架构中，HelmFluid可以在多个空间尺度上沿时间维度集成学到的Helmholtz动力学，从而产生未来的流体。与先前的方法相比，HelmFluid能够更好地捕捉流体的物理特性，并实现准确的流体预测。

    Fluid prediction is a long-standing challenge due to the intrinsic high-dimensional non-linear dynamics. Previous methods usually utilize the non-linear modeling capability of deep models to directly estimate velocity fields for future prediction. However, skipping over inherent physical properties but directly learning superficial velocity fields will overwhelm the model from generating precise or physics-reliable results. In this paper, we propose the HelmFluid toward an accurate and interpretable predictor for fluid. Inspired by the Helmholtz theorem, we design a HelmDynamics block to learn Helmholtz dynamics, which decomposes fluid dynamics into more solvable curl-free and divergence-free parts, physically corresponding to potential and stream functions of fluid. By embedding the HelmDynamics block into a Multiscale Multihead Integral Architecture, HelmFluid can integrate learned Helmholtz dynamics along temporal dimension in multiple spatial scales to yield future fluid. Compared 
    
[^585]: 神经距离场的零水平集编码器

    Zero-Level-Set Encoder for Neural Distance Fields

    [https://arxiv.org/abs/2310.06644](https://arxiv.org/abs/2310.06644)

    本文提出了一种用于嵌入3D形状的神经网络架构，通过多尺度混合系统和连续可微的解码器，不仅能够生成有效的有符号距离场，还能够在训练和推断中仅使用零水平集的知识。同时，还提出了针对曲面法线不存在情况的损失函数修改。

    

    神经形状表示通常指使用神经网络来表示3D几何，例如，在特定空间位置计算有符号距离或占据值。本文提出了一种新颖的编码器-解码器神经网络，用于在单次前向传递中嵌入3D形状。我们的架构基于多尺度混合系统，包括基于图形和基于体素的组件，以及连续可微的解码器。此外，该网络经过训练以解决Eikonal方程，仅需要零水平集的知识进行训练和推断。这意味着，与大多数之前的工作相比，我们的网络能够输出有效的有符号距离场，而无需明确的非零距离值或形状占据的先验知识。我们还提出了一种损失函数的修改，以解决曲面法线不存在的情况，例如，非封闭曲面和非流形几何的上下文。总体上，这可以帮助减少必要的先验知识。

    Neural shape representation generally refers to representing 3D geometry using neural networks, e.g., to compute a signed distance or occupancy value at a specific spatial position. In this paper, we present a novel encoder-decoder neural network for embedding 3D shapes in a single forward pass. Our architecture is based on a multi-scale hybrid system incorporating graph-based and voxel-based components, as well as a continuously differentiable decoder. Furthermore, the network is trained to solve the Eikonal equation and only requires knowledge of the zero-level set for training and inference. This means that in contrast to most previous work, our network is able to output valid signed distance fields without explicit prior knowledge of non-zero distance values or shape occupancy. We further propose a modification of the loss function in case that surface normals are not well defined, e.g., in the context of non-watertight surfaces and non-manifold geometry. Overall, this can help red
    
[^586]: 用规划标记引导语言模型的数学推理

    Guiding Language Model Math Reasoning with Planning Tokens

    [https://arxiv.org/abs/2310.05707](https://arxiv.org/abs/2310.05707)

    本论文介绍了一种通过引入规划标记来引导语言模型进行数学推理的方法。这种方法在保持推理过程中的一致性方面具有显著的准确性提升，而增加的训练参数很少。

    

    大型语言模型（LLMs）近来因其进行复杂推理任务的能力（如思维链推理）而引起了广泛关注。然而，大多数现有的增强模型推理能力方法过于依赖数据驱动方法，忽视了模型推理能力的结构化方面。我们发现，虽然LLMs可以很好地处理个别推理步骤，但在整个推理链上保持一致性方面却存在困难。为了解决这个问题，我们在每个推理步骤的开始处引入规划标记，作为模型的引导，并将它们的嵌入添加到模型参数中。我们的方法对于可训练参数的增加非常小（仅为0.001%），可以通过完全微调或更高效的参数方案来应用。我们通过将其应用于三种不同的LLMs，在三个数学单词问题数据集上展示了我们方法的有效性，相对于标准方法，准确性显著提高。

    Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. We find that while LLMs can manage individual reasoning steps well, they struggle with maintaining consistency across an entire reasoning chain. To solve this, we introduce planning tokens at the start of each reasoning step, serving as a guide for the model, and add their embeddings to the model parameters. Our approach requires a negligible increase in trainable parameters (just 0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets w.r.t. standard f
    
[^587]: 本文描述了一组季节性和非季节性时间序列模型，可以看作是加法和乘法指数平滑模型的推广，用于建模增长速度介于线性和指数之间的时间序列。这些模型的开发是基于快速增长、波动性较大的时间序列，具有从加法到乘法平滑变化的全局趋势，与线性局部趋势相结合。在我们的模型中，季节性(如果有使用)是乘法的，误差始终是加法的，但具有异方差性，并可通过参数sigma增长。我们利用最先进的贝叶斯拟合技术准确拟合这些比标准指数平滑模型更复杂、更灵活的模型。当应用于M3竞赛数据集时，我们的模型表现优于竞赛中的最佳算法和其他基准模型，据我们所知，取得了每个序列的最佳结果。

    Local and Global Trend Bayesian Exponential Smoothing Models

    [https://arxiv.org/abs/2309.13950](https://arxiv.org/abs/2309.13950)

    本文描述了一组季节性和非季节性时间序列模型，可以用于建模增长速度介于线性和指数之间的时间序列。模型包括全局趋势从加法到乘法的平滑变化，与线性局部趋势相结合，并采用乘法季节性和异方差的加法误差。通过贝叶斯拟合技术，该模型在M3竞赛数据集上表现优于其他模型。

    

    本文描述了一组季节性和非季节性时间序列模型，这些模型可以被视为加法和乘法指数平滑模型的推广，用来建模增长速度介于线性和指数之间的时间序列。我们的模型具有全局趋势，可以从加法平滑平滑地转变为乘法平滑，并与线性局部趋势相结合。我们的模型中的季节性是乘法的，误差始终是加法的，但具有异方差性，并可以通过参数sigma增长。我们利用最先进的贝叶斯拟合技术准确拟合这些比标准指数平滑模型更复杂、更灵活的模型。在应用于M3竞赛数据集时，我们的模型表现优于竞赛中的最佳算法和其他基准模型，据我们所知，取得了每个序列的最佳结果。

    This paper describes a family of seasonal and non-seasonal time series models that can be viewed as generalisations of additive and multiplicative exponential smoothing models, to model series that grow faster than linear but slower than exponential. Their development is motivated by fast-growing, volatile time series. In particular, our models have a global trend that can smoothly change from additive to multiplicative, and is combined with a linear local trend. Seasonality when used is multiplicative in our models, and the error is always additive but is heteroscedastic and can grow through a parameter sigma. We leverage state-of-the-art Bayesian fitting techniques to accurately fit these models that are more complex and flexible than standard exponential smoothing models. When applied to the M3 competition data set, our models outperform the best algorithms in the competition as well as other benchmarks, thus achieving to the best of our knowledge the best results of per-series univ
    
[^588]: DiffusionWorldViewer：揭示和拓宽生成式文本到图像模型反映的世界观

    DiffusionWorldViewer: Exposing and Broadening the Worldview Reflected by Generative Text-to-Image Models

    [https://arxiv.org/abs/2309.09944](https://arxiv.org/abs/2309.09944)

    本文章提出了DiffusionWorldViewer，一个交互界面，旨在揭示和拓宽生成式文本到图像模型的世界观。通过在输出的不同人口统计数据之间揭示世界观，并提供编辑工具，帮助用户在生成的图像中代表他们多样化的观点，并挑战当前模型中反映的有限世界观。

    

    生成式文本到图像（TTI）模型可以从简短的文本描述中生成高质量图像，在学术和创意领域被广泛使用。与人类一样，TTI模型有一个世界观，即从训练数据和任务中学习到的对世界的认知，这会影响它们为给定提示生成的图像。然而，TTI模型的世界观通常对用户隐藏，这使用户难以建立对TTI输出的直觉，并且它们通常与用户的世界观不一致，导致输出的图像不符合用户的期望。为此，我们介绍了DiffusionWorldViewer，一个交互界面，可在输出的不同人口统计数据之间揭示TTI模型的世界观，并提供编辑工具以使输出图像与用户的观点一致。在对18位多样化TTI用户进行的用户研究中，我们发现DiffusionWorldViewer帮助用户在生成的图像中代表他们多样化的观点，并挑战当前TTI模型中反映的有限世界观。

    Generative text-to-image (TTI) models produce high-quality images from short textual descriptions and are widely used in academic and creative domains. Like humans, TTI models have a worldview, a conception of the world learned from their training data and task that influences the images they generate for a given prompt. However, the worldviews of TTI models are often hidden from users, making it challenging for users to build intuition about TTI outputs, and they are often misaligned with users' worldviews, resulting in output images that do not match user expectations. In response, we introduce DiffusionWorldViewer, an interactive interface that exposes a TTI model's worldview across output demographics and provides editing tools for aligning output images with user perspectives. In a user study with 18 diverse TTI users, we find that DiffusionWorldViewer helps users represent their varied viewpoints in generated images and challenge the limited worldview reflected in current TTI mod
    
[^589]: 无限时域平均奖励马尔可夫决策过程中策略梯度算法的遗憾分析

    Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes

    [https://arxiv.org/abs/2309.01922](https://arxiv.org/abs/2309.01922)

    本文提出了一种基于策略梯度的算法用于无限时域平均奖励马尔可夫决策过程，并证明了其全局收敛性和近似O(T^3/4)的遗憾界。

    

    本文考虑了无限时域平均奖励马尔可夫决策过程（MDP）。与现有的相关工作不同，我们的方法利用了通用策略梯度算法的能力，解放了它在假设线性MDP结构的限制下。我们提出了一种基于策略梯度的算法，并展示了其全局收敛性质。然后，我们证明了该算法具有近似O(T^3/4)的遗憾。值得注意的是，本文首次探索了在平均奖励场景下，对于通用参数化策略梯度算法的遗憾界计算。

    In this paper, we consider an infinite horizon average reward Markov Decision Process (MDP). Distinguishing itself from existing works within this context, our approach harnesses the power of the general policy gradient-based algorithm, liberating it from the constraints of assuming a linear MDP structure. We propose a policy gradient-based algorithm and show its global convergence property. We then prove that the proposed algorithm has $\tilde{\mathcal{O}}({T}^{3/4})$ regret. Remarkably, this paper marks a pioneering effort by presenting the first exploration into regret-bound computation for the general parameterized policy gradient algorithm in the context of average reward scenarios.
    
[^590]: 成本有效的在线决策：一种组合多臂赌博机方法

    Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed Bandit Approach

    [https://arxiv.org/abs/2308.10699](https://arxiv.org/abs/2308.10699)

    本论文提供了一种基于组合多臂赌博机的成本有效的在线决策框架，并利用后验抽样或BayesUCB进行探索。实验结果证明了该框架在实际问题中的适用性。

    

    在线决策在许多实际应用中起关键作用。在许多情况下，决策是基于对传入数据点进行一系列测试来进行的。然而，执行所有测试可能是昂贵的，并且并非总是可行的。在本文中，我们提出了一种基于组合多臂赌博机的在线决策问题的新形式，并考虑了执行测试的（可能是随机的）成本。基于这种形式化，我们提供了一种新的成本有效的在线决策框架，可以利用后验抽样或BayesUCB进行探索。我们对用于成本有效在线决策的Thompson抽样进行了理论分析，并提供了各种实验结果，证明我们的框架适用于实际问题。

    Online decision making plays a crucial role in numerous real-world applications. In many scenarios, the decision is made based on performing a sequence of tests on the incoming data points. However, performing all tests can be expensive and is not always possible. In this paper, we provide a novel formulation of the online decision making problem based on combinatorial multi-armed bandits and take the (possibly stochastic) cost of performing tests into account. Based on this formulation, we provide a new framework for cost-efficient online decision making which can utilize posterior sampling or BayesUCB for exploration. We provide a theoretical analysis of Thompson Sampling for cost-efficient online decision making, and present various experimental results that demonstrate the applicability of our framework to real-world problems.
    
[^591]: UniAP: 通过混合整数二次规划统一跨层和内层自动并行化

    UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming

    [https://arxiv.org/abs/2307.16375](https://arxiv.org/abs/2307.16375)

    UniAP是一种新型的自动并行化方法，通过混合整数二次规划统一跨层和内层的自动并行化。与现有方法相比，UniAP在吞吐量方面表现更好，并且减少了策略优化时间。

    

    分布式学习常用于训练深度学习模型，特别是大型模型。在分布式学习中，手动并行化方法需要大量人力，并且灵活性有限。因此，最近提出了自动并行化方法来自动化并行策略优化过程。现有的自动并行化方法存在次优解的问题，因为它们不会同时优化跨层并行化和内层并行化这两个类别的并行策略。在本文中，我们提出了一种名为UniAP的新型自动并行化方法，通过混合整数二次规划统一跨层和内层的自动并行化。据我们所知，UniAP是第一种能够同时优化这两个类别的并行策略以求得最优解的并行化方法。实验结果表明，UniAP在吞吐量方面胜过了最先进的方法，提高了最多1.71倍，并减少了策略优化的时间。

    Distributed learning is commonly used for training deep learning models, especially large models. In distributed learning, manual parallelism (MP) methods demand considerable human effort and have limited flexibility. Hence, automatic parallelism (AP) methods have recently been proposed for automating the parallel strategy optimization process. Existing AP methods suffer from sub-optimal solutions because they do not jointly optimize the two categories of parallel strategies (i.e., inter-layer parallelism and intra-layer parallelism). In this paper, we propose a novel AP method called UniAP, which unifies inter- and intra-layer automatic parallelism by mixed integer quadratic programming. To the best of our knowledge, UniAP is the first parallel method that can jointly optimize the two categories of parallel strategies to find an optimal solution. Experimental results show that UniAP outperforms state-of-the-art methods by up to 1.71$\times$ in throughput and reduces strategy optimizat
    
[^592]: 量子神经熵估计

    Quantum Neural Estimation of Entropies

    [https://arxiv.org/abs/2307.01171](https://arxiv.org/abs/2307.01171)

    本文提出了一种量子神经网络算法，可以准确估计量子系统中的熵度量，并表明其在下游任务中有潜在应用价值。

    

    熵度量量子系统中信息和相关性的量。在实践中，当量子态未知且仅有其副本可用时，必须进行熵度量的估计。本文提出了一种用于估计冯诺依曼熵、R\'enyi熵、测量相对熵和测量R\'enyi相对熵的变分量子算法。我们的方法首先通过一个量子电路和一个经典神经网络对感兴趣的度量进行参数化，然后在参数空间上优化得到的目标。我们使用无噪声量子模拟器进行了数值模拟。该算法对所测试的各种熵度量提供了准确的估计，表明它是在下游任务中使用的一种有希望的方法。

    Entropy measures quantify the amount of information and correlation present in a quantum system. In practice, when the quantum state is unknown and only copies thereof are available, one must resort to the estimation of such entropy measures. Here we propose a variational quantum algorithm for estimating the von Neumann and R\'enyi entropies, as well as the measured relative entropy and measured R\'enyi relative entropy. Our approach first parameterizes a variational formula for the measure of interest by a quantum circuit and a classical neural network, and then optimizes the resulting objective over parameter space. Numerical simulations of our quantum algorithm are provided, using a noiseless quantum simulator. The algorithm provides accurate estimates of the various entropy measures for the examples tested, which renders it as a promising approach for usage in downstream tasks.
    
[^593]: 稀疏序列微多普勒重建的注意力优化展开

    Attention-Refined Unrolling for Sparse Sequential micro-Doppler Reconstruction

    [https://arxiv.org/abs/2306.14233](https://arxiv.org/abs/2306.14233)

    提出了STAR，一种基于神经网络的方法，能够从高度不完整的信道测量中重建人体运动的微多普勒序列。

    

    对人体运动的微多普勒特征的重建是细粒度活动识别无线感知的关键。在联合通信和感知系统中，与专用雷达感知系统不同，必须在感知准确性和通信开销之间取得适当的折衷。因此，微多普勒必须从从通信数据包中获取的不完整的信道估计窗口中重建。现有的方法利用压缩感知，但在只有少量信道测量可用的情况下（这在实际通信模式中经常发生）时产生非常差的重建结果。此外，它们需要大量迭代才能收敛，限制了它们在实时系统中的使用。在这项工作中，我们提出并验证了STAR，一种基于神经网络的方法，即使在高度不完整的信道测量条件下，也能重建人体运动的微多普勒序列。

    The reconstruction of micro-Doppler signatures of human movements is a key enabler for fine-grained activity recognition wireless sensing. In Joint Communication and Sensing (JCS) systems, unlike in dedicated radar sensing systems, a suitable trade-off between sensing accuracy and communication overhead has to be attained. It follows that the micro-Doppler has to be reconstructed from incomplete windows of channel estimates obtained from communication packets. Existing approaches exploit compressed sensing, but produce very poor reconstructions when only a few channel measurements are available, which is often the case with real communication patterns. In addition, the large number of iterations they need to converge hinders their use in real-time systems. In this work, we propose and validate STAR, a neural network that reconstructs micro-Doppler sequences of human movement even from highly incomplete channel measurements. STAR is based upon a new architectural design that combines a 
    
[^594]: 上下文优化方法在不确定决策中的应用综述

    A Survey of Contextual Optimization Methods for Decision Making under Uncertainty

    [https://arxiv.org/abs/2306.10374](https://arxiv.org/abs/2306.10374)

    本综述文章对上下文优化领域进行了综述，介绍了将预测算法和优化技术结合解决不确定决策问题的研究进展。文章通过研究单阶段和两阶段随机规划问题，确定了三种主要的从数据中学习策略的框架，并讨论了它们的优点和缺点。

    

    最近，运筹学和机器学习社区对于将预测算法和优化技术结合起来以解决面临不确定性的决策问题产生了极大兴趣。这催生了上下文优化领域，其中开发出数据驱动的程序，为决策者提供最新更新信息的最佳操作。在运筹学和机器学习文献中提出了许多模型和方法，包括数据驱动优化、规定性优化、预测性随机规划、策略优化、(智能)预测/估计-优化、以决策为导向的学习、(基于任务的)端到端学习/预测/优化等等。本综述文章关注单阶段和两阶段随机规划问题，确定了三种主要的从数据中学习策略的框架，并讨论了它们的优点和缺点。

    Recently there has been a surge of interest in operations research (OR) and the machine learning (ML) community in combining prediction algorithms and optimization techniques to solve decision-making problems in the face of uncertainty. This gave rise to the field of contextual optimization, under which data-driven procedures are developed to prescribe actions to the decision-maker that make the best use of the most recently updated information. A large variety of models and methods have been presented in both OR and ML literature under a variety of names, including data-driven optimization, prescriptive optimization, predictive stochastic programming, policy optimization, (smart) predict/estimate-then-optimize, decision-focused learning, (task-based) end-to-end learning/forecasting/optimization, etc. Focusing on single and two-stage stochastic programming problems, this review article identifies three main frameworks for learning policies from data and discusses their strengths and li
    
[^595]: 数据偏差调查中的特征重要性差异

    Feature Importance Disparities for Data Bias Investigations

    [https://arxiv.org/abs/2303.01704](https://arxiv.org/abs/2303.01704)

    本文介绍了一种新的方法，通过给定一个数据集，利用特征重要性差异（FID）来调查数据偏差，并展示了实验证据支持该方法的有效性。

    

    广泛认为，分类器中的下游偏差的一种原因是训练数据中存在的偏差。纠正这种偏差可能涉及到依赖于上下文的干预措施，例如在子集上训练单独的模型，在收集过程中删除具有偏差的特征，甚至进行真实世界的实验以确定偏差源。尽管需要进行这样的数据偏差调查，但目前很少有自动化方法可以辅助从业人员进行这些工作。在本文中，我们提出了一种给定数据集$X$，包括保护和不保护的特征，结果$y$，以及一个预测给定$X$的回归器$h$的元组$(f_j, g)$, 其中$g$对应于训练数据集$(X, y)$的子集，使得第$j$个特征$f_j$在子组$g$中的影响要比整体数据集中大得多（或者小得多），我们将其称为特征重要性差异（FID）。我们在4个数据集和4个常见特征重要性中展示了这一点。

    It is widely held that one cause of downstream bias in classifiers is bias present in the training data. Rectifying such biases may involve context-dependent interventions such as training separate models on subgroups, removing features with bias in the collection process, or even conducting real-world experiments to ascertain sources of bias. Despite the need for such data bias investigations, few automated methods exist to assist practitioners in these efforts. In this paper, we present one such method that given a dataset $X$ consisting of protected and unprotected features, outcomes $y$, and a regressor $h$ that predicts $y$ given $X$, outputs a tuple $(f_j, g)$, with the following property: $g$ corresponds to a subset of the training dataset $(X, y)$, such that the $j^{th}$ feature $f_j$ has much larger (or smaller) influence in the subgroup $g$, than on the dataset overall, which we call feature importance disparity (FID). We show across $4$ datasets and $4$ common feature import
    
[^596]: 带有分布式量化流的GFlowNets

    Distributional GFlowNets with Quantile Flows

    [https://arxiv.org/abs/2302.05793](https://arxiv.org/abs/2302.05793)

    本文提出了一种带分布式量化流的GFlowNets模型，通过将流函数转化为分布，在训练过程中提供更多信息的学习信号。通过量化函数参数化每个边流，我们提出的算法可以学习风险敏感的策略，实现对风险不确定性场景的处理，并在现有基准上取得了显著改进。

    

    生成式流网络（GFlowNets）是一种新的概率采样器系列，其中代理通过一系列决策步骤学习生成复杂组合结构的随机策略。尽管受强化学习启发，当前的GFlowNet框架在适用性上相对有限，无法处理奖励函数中的随机性。在这项工作中，我们采用分布式范式来处理GFlowNets，将每个流函数转化为一个分布，从而在训练过程中提供更多信息的学习信号。通过通过量化函数对每个边流进行参数化，我们提出的“量化匹配” GFlowNet学习算法能够学习风险敏感的策略，这是处理风险不确定性场景的基本组成部分。此外，我们发现与之前的方法相比，分布式方法由于我们增强的训练算法，可以在现有基准上实现显着改进。

    Generative Flow Networks (GFlowNets) are a new family of probabilistic samplers where an agent learns a stochastic policy for generating complex combinatorial structure through a series of decision-making steps. Despite being inspired from reinforcement learning, the current GFlowNet framework is relatively limited in its applicability and cannot handle stochasticity in the reward function. In this work, we adopt a distributional paradigm for GFlowNets, turning each flow function into a distribution, thus providing more informative learning signals during training. By parameterizing each edge flow through their quantile functions, our proposed \textit{quantile matching} GFlowNet learning algorithm is able to learn a risk-sensitive policy, an essential component for handling scenarios with risk uncertainty. Moreover, we find that the distributional approach can achieve substantial improvement on existing benchmarks compared to prior methods due to our enhanced training algorithm, even i
    
[^597]: 一种用于文本生成的重新参数化离散扩散模型的研究

    A Reparameterized Discrete Diffusion Model for Text Generation

    [https://arxiv.org/abs/2302.05737](https://arxiv.org/abs/2302.05737)

    本文提出了一种重新参数化离散扩散模型，该模型在文本生成方面表现出更好的灵活性、训练技术和生成效果，实验证明其较现有的扩散模型有显著的改进。

    

    本文研究了应用于自然语言生成的离散扩散概率模型。我们推导出了从离散扩散过程中采样的另一种等价形式，并利用这一洞见开发了一族重新参数化离散扩散模型。这个派生的通用框架非常灵活，为离散扩散模型中的生成过程提供了新的视角，并具备更有效的训练和解码技术。我们进行了大量实验证明我们模型的文本生成能力，在现有的扩散模型上取得了显著的改进。

    This work studies discrete diffusion probabilistic models with applications to natural language generation. We derive an alternative yet equivalent formulation of the sampling from discrete diffusion processes and leverage this insight to develop a family of reparameterized discrete diffusion models. The derived generic framework is highly flexible, offers a fresh perspective of the generation process in discrete diffusion models, and features more effective training and decoding techniques. We conduct extensive experiments to evaluate the text generation capability of our model, demonstrating significant improvements over existing diffusion models.
    
[^598]: 基于模型的聚类中的正则化和优化

    Regularization and Optimization in Model-Based Clustering

    [https://arxiv.org/abs/2302.02450](https://arxiv.org/abs/2302.02450)

    本论文针对基于模型的聚类中的正则化和优化提出了解决的方法，通过设计更有效的通用GMM优化算法以及结合正则化策略，解决了局部最小值数量较多以及解决方案过度拟合数据的问题。

    

    由于它们的概念简单性，k-means算法的变体被广泛应用于无监督聚类分析。然而，这些算法的主要缺点是它们基本上将相同的球状高斯混合适用于与这种分布大相径庭的数据。相比之下，通用的高斯混合模型（GMM）可以适应更丰富的结构，但需要估计每个簇表示协方差矩阵的二次数量的参数。这带来了两个主要问题：（i）由于局部最小值数量较多，底层的优化问题具有挑战性，（ii）它们的解决方案可能过度拟合数据。在这项工作中，我们设计了既能解决这两个问题的搜索策略。我们开发了更有效的通用GMM优化算法，并将这些算法与避免过拟合的正则化策略相结合。通过广泛的计算分析，我们观察到单独进行优化或正则化不会解决这些问题。

    Due to their conceptual simplicity, k-means algorithm variants have been extensively used for unsupervised cluster analysis. However, one main shortcoming of these algorithms is that they essentially fit a mixture of identical spherical Gaussians to data that vastly deviates from such a distribution. In comparison, general Gaussian Mixture Models (GMMs) can fit richer structures but require estimating a quadratic number of parameters per cluster to represent the covariance matrices. This poses two main issues: (i) the underlying optimization problems are challenging due to their larger number of local minima, and (ii) their solutions can overfit the data. In this work, we design search strategies that circumvent both issues. We develop more effective optimization algorithms for general GMMs, and we combine these algorithms with regularization strategies that avoid overfitting. Through extensive computational analyses, we observe that optimization or regularization in isolation does not
    
[^599]: 在异质隐私约束下的联邦学习中数据的公允价值

    The Fair Value of Data Under Heterogeneous Privacy Constraints in Federated Learning

    [https://arxiv.org/abs/2301.13336](https://arxiv.org/abs/2301.13336)

    本文提出了一种在给定隐私级别下基于公正定义的公平补偿用户数据的方法，并且考虑了隐私约束，是第一个明确考虑隐私约束的数据公平概念。同时，本文还研究了具有不同隐私级别、数据量和异质程度的公平分配下用户获得的补偿金额，并且讨论了平台被迫设计公平激励措施时的情况。

    

    现代数据聚合通常涉及平台从多个具有不同隐私选项的用户收集数据。平台必须解决如何向用户分配激励的问题，以说服他们共享他们的数据。本文根据一个公平的金额来补偿用户的数据，基于一个公平定义的公平概念，类似于著名的Shapley值。据我们所知，这是第一个明确考虑隐私约束的数据公平概念。我们还为平台制定了一个具有用户隐私级别选项的异质联邦学习问题。通过研究这个问题，我们研究了在不同隐私级别、数据量和异质程度下公平分配下用户获得的补偿金额。我们还讨论了当平台被迫设计公平激励措施时会发生什么。在一定的条件下，我们发现当隐私程度

    Modern data aggregation often involves a platform collecting data from a network of users with various privacy options. Platforms must solve the problem of how to allocate incentives to users to convince them to share their data. This paper puts forth an idea for a \textit{fair} amount to compensate users for their data at a given privacy level based on an axiomatic definition of fairness, along the lines of the celebrated Shapley value. To the best of our knowledge, these are the first fairness concepts for data that explicitly consider privacy constraints. We also formulate a heterogeneous federated learning problem for the platform with privacy level options for users. By studying this problem, we investigate the amount of compensation users receive under fair allocations with different privacy levels, amounts of data, and degrees of heterogeneity. We also discuss what happens when the platform is forced to design fair incentives. Under certain conditions we find that when privacy s
    
[^600]: 不训练权重就能拥有更高效的图神经网络：发现未训练的GNN表现

    You Can Have Better Graph Neural Networks by Not Training Weights at All: Finding Untrained GNNs Tickets

    [https://arxiv.org/abs/2211.15335](https://arxiv.org/abs/2211.15335)

    通过发现未经训练的稀疏子网络，我们可以在初始化时实现与完全训练的GNNs相媲美的性能，同时有效缓解GNNs过度平滑问题，为实现更深层GNNs提供了强大的工具。

    

    最近的研究令人印象深刻地证明，在随机初始化的卷积神经网络（CNN）中存在一个子网络，它在初始化时无需对网络的权重进行任何优化（即未训练网络）就能达到完全训练的稠密网络的性能。然而，在图神经网络（GNNs）中是否存在这样的未训练子网络仍然是个谜。在这篇论文中，我们首次探索了发现匹配的未训练GNNs的方法。通过稀疏性作为核心工具，我们可以在初始化时找到与完全训练的稠密GNNs性能匹配的未训练稀疏子网络。除了这个令人鼓舞的结果，我们还展示了发现的未训练子网络可以显著减轻GNNs过度平滑的问题，从而成为实现更深层GNNs而无需繁琐的强大工具。我们还观察到这样的稀疏未训练子网络具有较高的广泛适用性。

    Recent works have impressively demonstrated that there exists a subnetwork in randomly initialized convolutional neural networks (CNNs) that can match the performance of the fully trained dense networks at initialization, without any optimization of the weights of the network (i.e., untrained networks). However, the presence of such untrained subnetworks in graph neural networks (GNNs) still remains mysterious. In this paper we carry out the first-of-its-kind exploration of discovering matching untrained GNNs. With sparsity as the core tool, we can find \textit{untrained sparse subnetworks} at the initialization, that can match the performance of \textit{fully trained dense} GNNs. Besides this already encouraging finding of comparable performance, we show that the found untrained subnetworks can substantially mitigate the GNN over-smoothing problem, hence becoming a powerful tool to enable deeper GNNs without bells and whistles. We also observe that such sparse untrained subnetworks ha
    
[^601]: 关于差分隐私持续计数的几乎严格的误差界限

    Almost Tight Error Bounds on Differentially Private Continual Counting

    [https://arxiv.org/abs/2211.05006](https://arxiv.org/abs/2211.05006)

    这项研究提出了一种新颖的机制，能够在差分隐私持续计数中减小误差，并证明其均方误差比二进制机制小一个因子10。研究还给出了几乎严格的常数界限，以及对私有学习算法超出风险的上界。

    

    私有联邦学习的首个大规模部署在持续发布模型中使用差分隐私计数作为子程序 (标题为 "带有正式差分隐私保证的联邦学习")。在这种情况下，对误差的具体界限非常重要，以减小隐私参数。持续计数的标准机制是二进制机制。我们提出了一种新颖的机制，并且证明其均方误差既是渐近优化的，又比二进制机制的误差小一个因子10。我们还通过给出非渐近的下界和上界，证明了我们分析中的常数几乎是严格的，只在低阶项的常数中有所不同。我们的算法是计数矩阵的矩阵机制，并且每次发布都需要常数时间。我们还利用我们对计数矩阵的明确因式分解，给出了Denisov等人的私有学习算法的超出风险的上界 (NeurIPS 2022)。

    The first large-scale deployment of private federated learning uses differentially private counting in the continual release model as a subroutine (Google AI blog titled "Federated Learning with Formal Differential Privacy Guarantees"). In this case, a concrete bound on the error is very relevant to reduce the privacy parameter. The standard mechanism for continual counting is the binary mechanism. We present a novel mechanism and show that its mean squared error is both asymptotically optimal and a factor 10 smaller than the error of the binary mechanism. We also show that the constants in our analysis are almost tight by giving non-asymptotic lower and upper bounds that differ only in the constants of lower-order terms. Our algorithm is a matrix mechanism for the counting matrix and takes constant time per release. We also use our explicit factorization of the counting matrix to give an upper bound on the excess risk of the private learning algorithm of Denisov et al. (NeurIPS 2022).
    
[^602]: 采用爆发传播的多模态语音增强

    Multimodal Speech Enhancement Using Burst Propagation

    [https://arxiv.org/abs/2209.03275](https://arxiv.org/abs/2209.03275)

    本论文提出了一种采用爆发传播的多模态语音增强解决方案，通过学习噪声信号和视觉刺激之间的相关性，放大相关信息并抑制噪声，从而赋予语音含义。

    

    本论文提出了一种名为MBURST的新颖的多模态解决方案，用于音频-视觉语音增强，并考虑了有关前额叶皮层和其他脑区金字塔细胞的最新神经学发现。所谓的爆发传播通过反馈方式实现了几个准则，以更符合生物学的方式解决信任分配问题：通过反馈控制塑性的符号和幅度，通过不同的权重连接在各层之间多路复用反馈和前馈信息，近似反馈和前馈连接，并线性化反馈信号。MBURST利用这些功能学习噪声信号和视觉刺激之间的相关性，从而通过放大相关信息和抑制噪声赋予语音以含义。在Grid Corpus和基于CHiME3的数据集上进行的实验表明，MBURST能够复现类似的掩模重建，与多模态反向传播基准方法相比。

    This paper proposes the MBURST, a novel multimodal solution for audio-visual speech enhancements that consider the most recent neurological discoveries regarding pyramidal cells of the prefrontal cortex and other brain regions. The so-called burst propagation implements several criteria to address the credit assignment problem in a more biologically plausible manner: steering the sign and magnitude of plasticity through feedback, multiplexing the feedback and feedforward information across layers through different weight connections, approximating feedback and feedforward connections, and linearizing the feedback signals. MBURST benefits from such capabilities to learn correlations between the noisy signal and the visual stimuli, thus attributing meaning to the speech by amplifying relevant information and suppressing noise. Experiments conducted over a Grid Corpus and CHiME3-based dataset show that MBURST can reproduce similar mask reconstructions to the multimodal backpropagation-bas
    
[^603]: ANAct: 自适应归一化的激活函数

    ANAct: Adaptive Normalization for Activation Functions

    [https://arxiv.org/abs/2208.13315](https://arxiv.org/abs/2208.13315)

    本文研究了激活函数对神经网络前向和反向传播的负面影响，并提出了一种自适应归一化的激活函数方法ANAct来保持一致的梯度差异，通过实验证明了其有效性。

    

    本文研究了激活函数对前向和反向传播的负面影响，以及如何抵消这种影响。首先，我们考察了激活函数对神经网络前向和反向传播的影响，并推导出了梯度差异的一般形式，扩展了此领域的先前工作。我们尝试使用小批量统计来动态更新归一化因子，以确保在训练过程中始终保持归一化属性，而不仅仅考虑权重初始化后的神经网络状态。其次，我们提出了ANAct方法，该方法对激活函数进行归一化，以在各层之间保持一致的梯度差异，并通过实验证明了其有效性。我们观察到收敛速度与归一化属性大致相关。我们将ANAct与几种常见的激活函数在卷积神经网络和残差网络上进行比较，并显示ANAct可以持续改善它们的性能。

    In this paper, we investigate the negative effect of activation functions on forward and backward propagation and how to counteract this effect. First, We examine how activation functions affect the forward and backward propagation of neural networks and derive a general form for gradient variance that extends the previous work in this area. We try to use mini-batch statistics to dynamically update the normalization factor to ensure the normalization property throughout the training process, rather than only accounting for the state of the neural network after weight initialization. Second, we propose ANAct, a method that normalizes activation functions to maintain consistent gradient variance across layers and demonstrate its effectiveness through experiments. We observe that the convergence rate is roughly related to the normalization property. We compare ANAct with several common activation functions on CNNs and residual networks and show that ANAct consistently improves their perfo
    
[^604]: 加速算法用于约束非凸-非凹极小-极大优化和共单调包含

    Accelerated Algorithms for Constrained Nonconvex-Nonconcave Min-Max Optimization and Comonotone Inclusion

    [https://arxiv.org/abs/2206.05248](https://arxiv.org/abs/2206.05248)

    本论文提出了针对约束共单调极小-极大优化和共单调包含问题的加速算法，扩展了现有算法并实现了较优的收敛速率，同时证明了算法的收敛性。

    

    我们研究了约束共单调极小-极大优化，一类结构化的非凸-非凹极小-极大优化问题以及它们对共单调包含的推广。在我们的第一个贡献中，我们将最初由Yoon和Ryu（2021）提出的无约束极小-极大优化的Extra Anchored Gradient（EAG）算法扩展到约束共单调极小-极大优化和共单调包含问题，并实现了所有一阶方法中的最优收敛速率$O\left(\frac{1}{T}\right)$。此外，我们证明了算法的迭代收敛到解集中的一个点。在我们的第二个贡献中，我们将由Lee和Kim（2021）开发的快速额外梯度（FEG）算法扩展到约束共单调极小-极大优化和共单调包含，并实现了相同的$O\left(\frac{1}{T}\right)$收敛速率。这个速率适用于文献中研究过的最广泛的共单调包含问题集合。我们的分析基于s的内容。

    We study constrained comonotone min-max optimization, a structured class of nonconvex-nonconcave min-max optimization problems, and their generalization to comonotone inclusion. In our first contribution, we extend the Extra Anchored Gradient (EAG) algorithm, originally proposed by Yoon and Ryu (2021) for unconstrained min-max optimization, to constrained comonotone min-max optimization and comonotone inclusion, achieving an optimal convergence rate of $O\left(\frac{1}{T}\right)$ among all first-order methods. Additionally, we prove that the algorithm's iterations converge to a point in the solution set. In our second contribution, we extend the Fast Extra Gradient (FEG) algorithm, as developed by Lee and Kim (2021), to constrained comonotone min-max optimization and comonotone inclusion, achieving the same $O\left(\frac{1}{T}\right)$ convergence rate. This rate is applicable to the broadest set of comonotone inclusion problems yet studied in the literature. Our analyses are based on s
    
[^605]: 将子图转化为节点让简单的图神经网络在子图表示学习上更强大和高效

    Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning

    [https://arxiv.org/abs/2204.04510](https://arxiv.org/abs/2204.04510)

    提出了一种将子图转化为节点的方法来学习子图的表示，该方法不仅显著降低了内存和计算成本，还捕捉了子图的局部和全局结构，并在多个基准测试上表现出色。

    

    子图表示学习已经成为一个重要的问题，并且通常使用专门的图神经网络来处理大型全局图。这些模型需要大量的内存和计算资源，但挑战子图的层次结构建模。在本文中，我们提出了子图到节点（S2N）转换的新颖公式，用于学习子图的表示。具体而言，给定全局图中的一组子图，我们通过粗略地将子图转换成节点来构建一个新的图。通过理论和实证证据，S2N不仅相比最先进的模型显著减少了内存和计算成本，而且通过捕捉子图的局部和全局结构也在性能上超过了它们。通过利用图粗化方法，我们的方法甚至在数据稀缺的情况下也优于基线模型。我们在八个基准测试上的实验表明，调整模型后效果出色。

    Subgraph representation learning has emerged as an important problem, but it is by default approached with specialized graph neural networks on a large global graph. These models demand extensive memory and computational resources but challenge modeling hierarchical structures of subgraphs. In this paper, we propose Subgraph-To-Node (S2N) translation, a novel formulation for learning representations of subgraphs. Specifically, given a set of subgraphs in the global graph, we construct a new graph by coarsely transforming subgraphs into nodes. Demonstrating both theoretical and empirical evidence, S2N not only significantly reduces memory and computational costs compared to state-of-the-art models but also outperforms them by capturing both local and global structures of the subgraph. By leveraging graph coarsening methods, our method outperforms baselines even in a data-scarce setting with insufficient subgraphs. Our experiments on eight benchmarks demonstrate that fined-tuned models w
    
[^606]: 领航神经空间：重新审视概念激活向量以克服方向差异

    Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence

    [https://arxiv.org/abs/2202.03482](https://arxiv.org/abs/2202.03482)

    本文重新审视了概念激活向量（CAVs）在建模人类可理解的概念中的应用，并引入了基于模式的CAVs来提供更准确的概念方向。

    

    随着对于理解神经网络预测策略的兴趣日益增长，概念激活向量（CAVs）已成为一种流行的工具，用于在潜在空间中建模人类可理解的概念。通常，CAVs是通过利用线性分类器来计算的，该分类器优化具有给定概念和无给定概念的样本的潜在表示的可分离性。然而，在本文中我们展示了这种以可分离性为导向的计算方法会导致与精确建模概念方向的实际目标发散的解决方案。这种差异可以归因于分散方向的显著影响，即与概念无关的信号，这些信号被线性模型的滤波器（即权重）捕获以优化类别可分性。为了解决这个问题，我们引入基于模式的CAVs，仅关注概念信号，从而提供更准确的概念方向。我们评估了各种CAV方法与真实概念方向的对齐程度。

    With a growing interest in understanding neural network prediction strategies, Concept Activation Vectors (CAVs) have emerged as a popular tool for modeling human-understandable concepts in the latent space. Commonly, CAVs are computed by leveraging linear classifiers optimizing the separability of latent representations of samples with and without a given concept. However, in this paper we show that such a separability-oriented computation leads to solutions, which may diverge from the actual goal of precisely modeling the concept direction. This discrepancy can be attributed to the significant influence of distractor directions, i.e., signals unrelated to the concept, which are picked up by filters (i.e., weights) of linear models to optimize class-separability. To address this, we introduce pattern-based CAVs, solely focussing on concept signals, thereby providing more accurate concept directions. We evaluate various CAV methods in terms of their alignment with the true concept dire
    
[^607]: 可实现学习就是你所需要的一切

    Realizable Learning is All You Need

    [https://arxiv.org/abs/2111.04746](https://arxiv.org/abs/2111.04746)

    可实现学习与无偏学习的等价性是学习理论中的基本现象，我们提出了第一个独立于模型的框架来解释这个等价性，它可以适用于各种设置，并拓展了我们对各种学习情况的理解。

    

    可实现学习与无偏学习的等价性是学习理论中的基本现象。从经典的PAC学习和回归到最近的趋势如对抗鲁棒学习，令人惊讶的是我们仍然缺乏一个统一的理论；传统的等价性证明往往是零散的，并且依赖于强的模型特定假设，如均匀收敛和样本压缩。在这项工作中，我们提供了第一个独立于模型的框架，解释了可实现学习与无偏学习的等价性：一个三行代码的黑盒简化，统一和拓展了我们对各种设置的理解。这包括了没有已知可学习性描述的模型，如具有任意分布假设和更一般的损失函数的学习，以及一系列其他流行的设置，如鲁棒学习、部分学习、公平学习和统计查询模型。

    The equivalence of realizable and agnostic learnability is a fundamental phenomenon in learning theory. With variants ranging from classical settings like PAC learning and regression to recent trends such as adversarially robust learning, it's surprising that we still lack a unified theory; traditional proofs of the equivalence tend to be disparate, and rely on strong model-specific assumptions like uniform convergence and sample compression.   In this work, we give the first model-independent framework explaining the equivalence of realizable and agnostic learnability: a three-line blackbox reduction that simplifies, unifies, and extends our understanding across a wide variety of settings. This includes models with no known characterization of learnability such as learning with arbitrary distributional assumptions and more general loss functions, as well as a host of other popular settings such as robust learning, partial learning, fair learning, and the statistical query model.   Mor
    
[^608]: 在连续治疗下的多重稳健因果中介分析

    Multiply Robust Causal Mediation Analysis with Continuous Treatments

    [https://arxiv.org/abs/2105.09254](https://arxiv.org/abs/2105.09254)

    本文提出了一种适用于连续治疗环境的多重稳健因果中介分析估计器，采用了核平滑方法，并具有多重稳健性和渐近正态性。

    

    在许多应用中，研究人员对治疗或暴露对感兴趣的结果的直接和间接的因果效应。中介分析为鉴定和估计这些因果效应提供了一个严谨的框架。对于二元治疗，Tchetgen Tchetgen和Shpitser (2012)提出了直接和间接效应的高效估计器，基于参数的影响函数。这些估计器具有良好的性质，如多重稳健性和渐近正态性，同时允许对干扰参数进行低于根号n的收敛速度。然而，在涉及连续治疗的情况下，这些基于影响函数的估计器没有准备好应用，除非进行强参数假设。在这项工作中，我们利用核平滑方法提出了一种适用于连续治疗环境的估计器，受到Tchetgen Tchetgen的影响函数估计器的启发。

    In many applications, researchers are interested in the direct and indirect causal effects of a treatment or exposure on an outcome of interest. Mediation analysis offers a rigorous framework for identifying and estimating these causal effects. For binary treatments, efficient estimators for the direct and indirect effects are presented in Tchetgen Tchetgen and Shpitser (2012) based on the influence function of the parameter of interest. These estimators possess desirable properties, such as multiple-robustness and asymptotic normality, while allowing for slower than root-n rates of convergence for the nuisance parameters. However, in settings involving continuous treatments, these influence function-based estimators are not readily applicable without making strong parametric assumptions. In this work, utilizing a kernel-smoothing approach, we propose an estimator suitable for settings with continuous treatments inspired by the influence function-based estimator of Tchetgen Tchetgen an
    
[^609]: 一个对线性模型进行严格介绍的书籍

    A rigorous introduction to linear models

    [https://arxiv.org/abs/2105.04240](https://arxiv.org/abs/2105.04240)

    本书旨在向读者提供对线性模型及其理论的严格介绍，并总结了线性模型在回归问题中的重要性和应用。

    

    本书旨在向读者介绍线性模型及其背后的理论。我们的目标是为读者提供一个严谨的介绍，前提是读者具有普通最小二乘法的先前经验。在机器学习中，输出通常是输入的非线性函数。深度学习甚至旨在找到具有许多层的非线性依赖关系，这需要大量计算。然而，大多数算法都是基于简单的线性模型构建的。我们从不同的角度描述线性模型，找到模型背后的性质和理论。线性模型是回归问题中的主要技术，最主要的工具是最小二乘逼近，它最小化了平方误差的和。当我们有兴趣找到最小化相应的期望平方误差的回归函数时，这是一个自然的选择。本书主要总结了线性模型背后的目的和重要理论的意义，例如概率分布、推导和估计方法等等。

    This book is meant to provide an introduction to linear models and the theories behind them. Our goal is to give a rigorous introduction to the readers with prior exposure to ordinary least squares. In machine learning, the output is usually a nonlinear function of the input. Deep learning even aims to find a nonlinear dependence with many layers, which require a large amount of computation. However, most of these algorithms build upon simple linear models. We then describe linear models from different perspectives and find the properties and theories behind the models. The linear model is the main technique in regression problems, and the primary tool for it is the least squares approximation, which minimizes a sum of squared errors. This is a natural choice when we're interested in finding the regression function which minimizes the corresponding expected squared error. This book is primarily a summary of purpose, significance of important theories behind linear models, e.g., distrib
    
[^610]: 农作物类型语义分割的上下文自对比预训练

    Context-self contrastive pretraining for crop type semantic segmentation

    [https://arxiv.org/abs/2104.04310](https://arxiv.org/abs/2104.04310)

    这项研究提出了一种针对农作物类型语义分割任务的上下文自对比预训练方法，通过学习嵌入空间的相似度度量，突出语义边界。实验结果表明，这种方法改善了该任务的最新技术表现，并提供了一个大规模的密集注释数据集以及数据生成流程。

    

    本文提出了一种基于对比学习的全监督预训练方案，特别适用于密集分类任务。所提出的Context-Self对比损失（CSCL）通过在训练样本的每个位置和其局部上下文之间使用相似度度量来学习嵌入空间，使语义边界突出。针对卫星图像时间序列（SITS）中的农作物类型语义分割，我们发现地块边界的性能是一个关键瓶颈，并解释了CSCL如何解决该问题的潜在原因，改进了这一任务的最新技术表现。此外，我们利用来自Sentinel-2（S2）卫星任务的图像编制了目前我们所知的最大的密集注释的农作物类型和地块身份的SITS数据集，并将其连同数据生成流程公开。使用该数据，我们发现即使是最小的预训练，CSCL也能提高所有相应基准线，并展示一个先进的性能结果。

    In this paper, we propose a fully supervised pre-training scheme based on contrastive learning particularly tailored to dense classification tasks. The proposed Context-Self Contrastive Loss (CSCL) learns an embedding space that makes semantic boundaries pop-up by use of a similarity metric between every location in a training sample and its local context. For crop type semantic segmentation from Satellite Image Time Series (SITS) we find performance at parcel boundaries to be a critical bottleneck and explain how CSCL tackles the underlying cause of that problem, improving the state-of-the-art performance in this task. Additionally, using images from the Sentinel-2 (S2) satellite missions we compile the largest, to our knowledge, SITS dataset densely annotated by crop type and parcel identities, which we make publicly available together with the data generation pipeline. Using that data we find CSCL, even with minimal pre-training, to improve all respective baselines and present a pro
    
[^611]: 来自噪声二进制反馈的最优聚类

    Optimal Clustering from Noisy Binary Feedback

    [https://arxiv.org/abs/1910.06002](https://arxiv.org/abs/1910.06002)

    本论文研究了通过二进制用户反馈进行聚类的问题，并提出了一种算法来最小化聚类恢复错误率。

    

    我们研究了通过二进制用户反馈来进行聚类的问题。这样的问题在大规模标记任务中以最小的用户工作量解决的众包平台上出现。例如，在一些最近的reCAPTCHA系统中，用户的点击（二进制答案）可以用来有效地标记图像。在我们的推理问题中，项目被分成最初未知的不重叠的聚类。为了恢复这些聚类，学习者按顺序向用户呈现一系列项目，每个项目都附有一个从固定有限集合中选择的具有二进制答案的问题。对于这些项目中的每一个，用户提供的是一个由项目聚类、问题和一个描述对项目进行分类的难度的项目特定参数决定期望的噪声答案。目标是设计一种算法，具有最小的聚类恢复错误率。我们得到了任何算法满足的问题特定的信息理论下界，用于错误率。

    We study the problem of clustering a set of items from binary user feedback. Such a problem arises in crowdsourcing platforms solving large-scale labeling tasks with minimal effort put on the users. For example, in some of the recent reCAPTCHA systems, users clicks (binary answers) can be used to efficiently label images. In our inference problem, items are grouped into initially unknown non-overlapping clusters. To recover these clusters, the learner sequentially presents to users a finite list of items together with a question with a binary answer selected from a fixed finite set. For each of these items, the user provides a noisy answer whose expectation is determined by the item cluster and the question and by an item-specific parameter characterizing the {\it hardness} of classifying the item. The objective is to devise an algorithm with a minimal cluster recovery error rate. We derive problem-specific information-theoretical lower bounds on the error rate satisfied by any algorit
    
[^612]: 无知回归问题中的不可知样本压缩方案

    Agnostic Sample Compression Schemes for Regression

    [https://arxiv.org/abs/1810.01864](https://arxiv.org/abs/1810.01864)

    本文在绝对值损失函数为 $\ell_p$ 的不确定回归设置中构建了一种通用的逼近样本压缩方案，对于线性回归可以实现线性维度大小的压缩，对于 $\ell_1$ 和 $\ell_\infty$ 损失函数可以实现线性维度大小的有效完全样本压缩方案；同时，证明了其他 $\ell_p$ 损失函数不存在有限尺寸的完全不可知压缩方案的结果，并提出了开放问题。

    

    我们在绝对值损失函数为 $\ell_p$ 的不确定回归设置中获得了第一个有限样本压缩的积极结果，其中 $p \in [1, \infty]$。我们构建了一种通用的逼近样本压缩方案，适用于展示了指数级大小的fat-shattering维度但与样本数量无关的实值函数类。值得注意的是，在线性回归中，我们构造了一个线性维度大小的逼近压缩。此外，在$\ell_1$和$\ell_\infty$损失函数中，我们甚至可以展示出一个线性维度大小的有效完全样本压缩方案。我们进一步证明了对于其他每一个 $\ell_p$ 损失函数，其中 $p \in (1,\infty)$，不存在有限尺寸的完全不可知压缩方案。这进一步改进和推广了David、Moran和Yehudayoff对于$\ell_2$损失的负面结果。我们最后提出了一般性的开放问题：对于 $\ell_1$ 损失的不可知回归问题，是否每个函数类都存在尺寸为...的完全压缩方案？

    We obtain the first positive results for bounded sample compression in the agnostic regression setting with the $\ell_p$ loss, where $p\in [1,\infty]$. We construct a generic approximate sample compression scheme for real-valued function classes exhibiting exponential size in the fat-shattering dimension but independent of the sample size. Notably, for linear regression, an approximate compression of size linear in the dimension is constructed. Moreover, for $\ell_1$ and $\ell_\infty$ losses, we can even exhibit an efficient exact sample compression scheme of size linear in the dimension. We further show that for every other $\ell_p$ loss, $p\in (1,\infty)$, there does not exist an exact agnostic compression scheme of bounded size. This refines and generalizes a negative result of David, Moran, and Yehudayoff for the $\ell_2$ loss. We close by posing general open questions: for agnostic regression with $\ell_1$ loss, does every function class admits an exact compression scheme of size 
    
[^613]: 通过增加表示来编码时间统计空间先验

    Encoding Temporal Statistical-space Priors via Augmented Representation. (arXiv:2401.16808v1 [cs.LG])

    [http://arxiv.org/abs/2401.16808](http://arxiv.org/abs/2401.16808)

    通过增加表示的方式编码时间统计空间先验，以应对时间序列数据建模中的挑战。我们的方法在两个数据集上的实证推广性能明显优于五个最新的基准方法。具有高度模块化性质的方法适用于各种场景。

    

    时间序列数据建模仍然是一个普遍存在的问题，因为时间维度与许多领域密切相关。尽管在时间序列预测方面取得了显著进展，但高噪声信号比、非正态性、非平稳性和数据缺乏仍然是挑战从业者的问题。为此，我们利用一种简单的表示增强技术来克服这些挑战。我们的增强表示在每个时间步骤上作为统计空间先验进行编码。作为响应，我们将我们的方法命名为统计空间增强表示（SSAR）。基于高维数据生成过程，启发了我们的表示增强。我们在两个数据集上对两个下游时间学习算法的经验泛化性能进行了严格的检查。我们的方法明显击败了五个最新的基准线。此外，我们的方法具有高度模块化的性质，可以轻松应用于各种情况。最后，我们提供了全面的理论视角。

    Modeling time series data remains a pervasive issue as the temporal dimension is inherent to numerous domains. Despite significant strides in time series forecasting, high noise-to-signal ratio, non-normality, non-stationarity, and lack of data continue challenging practitioners. In response, we leverage a simple representation augmentation technique to overcome these challenges. Our augmented representation acts as a statistical-space prior encoded at each time step. In response, we name our method Statistical-space Augmented Representation (SSAR). The underlying high-dimensional data-generating process inspires our representation augmentation. We rigorously examine the empirical generalization performance on two data sets with two downstream temporal learning algorithms. Our approach significantly beats all five up-to-date baselines. Moreover, the highly modular nature of our approach can easily be applied to various settings. Lastly, fully-fledged theoretical perspectives are availa
    
[^614]: 从零开始构建一个大型语言模型

    Engineering A Large Language Model From Scratch. (arXiv:2401.16736v1 [cs.CL])

    [http://arxiv.org/abs/2401.16736](http://arxiv.org/abs/2401.16736)

    Atinuke是一种基于Transformer的神经网络，通过在处理时序数据的层与注意机制交织在一起，模拟人类语言，从而优化各种语言任务的性能。

    

    深度学习在自然语言处理（NLP）领域的普及导致了能够理解和生成人类语言的创新技术的开发和发布。Atinuke是一种基于Transformer的神经网络，通过利用独特的配置，在各种语言任务上优化性能。该架构通过将处理时序数据的层与注意机制交织在一起，从而在输入和输出之间建立有意义的关联。由于其拓扑结构和超参数调整的配置，它可以提取特征并学习复杂的映射，从而模仿人类语言。Atinuke是模块化、可扩展的，并可以与现有的机器学习流程无缝集成。softmax、嵌入和多头注意力等高级矩阵操作使得对文本、声音和视觉信号的细致处理成为可能。通过将现代深度学习技术与软件设计原则和数学方法相结合

    The proliferation of deep learning in natural language processing (NLP) has led to the development and release of innovative technologies capable of understanding and generating human language with remarkable proficiency. Atinuke, a Transformer-based neural network, optimises performance across various language tasks by utilising a unique configuration. The architecture interweaves layers for processing sequential data with attention mechanisms to draw meaningful affinities between inputs and outputs. Due to the configuration of its topology and hyperparameter tuning, it can emulate human-like language by extracting features and learning complex mappings. Atinuke is modular, extensible, and integrates seamlessly with existing machine learning pipelines. Advanced matrix operations like softmax, embeddings, and multi-head attention enable nuanced handling of textual, acoustic, and visual signals. By unifying modern deep learning techniques with software design principles and mathematical
    
[^615]: Context-Former：基于潜在条件序列建模的拼接技术

    Context-Former: Stitching via Latent Conditioned Sequence Modeling. (arXiv:2401.16452v1 [cs.LG])

    [http://arxiv.org/abs/2401.16452](http://arxiv.org/abs/2401.16452)

    Context-Former是一种集成了基于情境信息的模仿学习和序列建模的方法，通过拼接次优轨迹片段来改善决策，并提高了Decision Transformer的性能。

    

    离线强化学习（RL）算法可以通过拼接次优轨迹来改善决策，从而获得更优的结果。这种能力是使RL能够学习优于行为策略的策略的关键因素。另一方面，Decision Transformer（DT）将决策建模为序列建模，展示了在离线RL基准测试中竞争性的性能，然而，最近的研究表明DT缺乏拼接能力，因此提高DT性能需要利用拼接能力。为了赋予DT拼接能力，我们将轨迹拼接抽象为专家匹配，并引入了我们的方法ContextFormer，通过模拟有限数量的专家轨迹的表示来集成基于情境信息的模仿学习（IL）和序列建模，以拼接次优轨迹片段。为了验证我们的观点，我们从两个角度进行实验证明：

    Offline reinforcement learning (RL) algorithms can improve the decision making via stitching sub-optimal trajectories to obtain more optimal ones. This capability is a crucial factor in enabling RL to learn policies that are superior to the behavioral policy. On the other hand, Decision Transformer (DT) abstracts the decision-making as sequence modeling, showcasing competitive performance on offline RL benchmarks, however, recent studies demonstrate that DT lacks of stitching capability, thus exploit stitching capability for DT is vital to further improve its performance. In order to endow stitching capability to DT, we abstract trajectory stitching as expert matching and introduce our approach, ContextFormer, which integrates contextual information-based imitation learning (IL) and sequence modeling to stitch sub-optimal trajectory fragments by emulating the representations of a limited number of expert trajectories. To validate our claim, we conduct experiments from two perspectives:
    
[^616]: 通过网络流量分析和机器学习技术来缓解勒索软件威胁

    Ransomware threat mitigation through network traffic analysis and machine learning techniques. (arXiv:2401.15285v1 [cs.CR])

    [http://arxiv.org/abs/2401.15285](http://arxiv.org/abs/2401.15285)

    本文介绍了一种通过分析网络流量和使用机器学习算法来识别和检测勒索软件的方法，该方法在实践中表现出高水平的精度和准确性。

    

    近年来，使用勒索软件进行网络攻击的情况明显增加。攻击者利用这种恶意软件侵入网络并损害计算机系统。这给各种组织带来了重大和长期的损害，包括政府、私营公司和普通用户。这些攻击通常导致敏感信息的丢失或泄露，正常运营的中断以及持久的漏洞。本文侧重于一种在计算机网络中识别和辨别勒索软件的方法。该方法依赖于使用机器学习算法和分析网络流量的模式。通过收集和研究这些流量，然后应用机器学习模型，我们可以准确地识别和检测勒索软件。实施这种方法的结果表明，机器学习算法可以基于网络流量准确地定位勒索软件，实现高水平的精度和准确性。

    In recent years, there has been a noticeable increase in cyberattacks using ransomware. Attackers use this malicious software to break into networks and harm computer systems. This has caused significant and lasting damage to various organizations, including government, private companies, and regular users. These attacks often lead to the loss or exposure of sensitive information, disruptions in normal operations, and persistent vulnerabilities. This paper focuses on a method for recognizing and identifying ransomware in computer networks. The approach relies on using machine learning algorithms and analyzing the patterns of network traffic. By collecting and studying this traffic, and then applying machine learning models, we can accurately identify and detect ransomware. The results of implementing this method show that machine learning algorithms can effectively pinpoint ransomware based on network traffic, achieving high levels of precision and accuracy.
    
[^617]: 朝着稳定的利益相关方一致化机器学习偏好迈进

    Towards Stable Preferences for Stakeholder-aligned Machine Learning. (arXiv:2401.15268v1 [cs.LG])

    [http://arxiv.org/abs/2401.15268](http://arxiv.org/abs/2401.15268)

    本研究旨在通过数据驱动的方法，整合个人和团体的偏好，解决肾脏分配中的紧迫挑战，并评估偏好的稳定性。

    

    针对肾脏分配的紧迫挑战，即需求增长与利益相关方价值的结合，本研究旨在开发一个以数据驱动的解决方案，以解决这个问题。该研究的主要目标是创建一种学习个人和团体关于肾脏分配的偏好的方法。通过利用“成对肾脏患者在线调查”的数据，结合两个不同的数据集，并在个人、团体和稳定性三个层面上进行评估，我们使用机器学习分类器并通过几种度量方法进行评估。个人层面模型预测个体参与者的偏好，团体层面模型汇总参与者偏好，稳定性层面模型是团体升级的扩展，评估这些偏好随时间的稳定性。通过将利益相关方的偏好纳入肾脏分配过程，我们希望推动伦理维度的发展。

    In response to the pressing challenge of kidney allocation, characterized by growing demands for organs, this research sets out to develop a data-driven solution to this problem, which also incorporates stakeholder values. The primary objective of this study is to create a method for learning both individual and group-level preferences pertaining to kidney allocations. Drawing upon data from the 'Pairwise Kidney Patient Online Survey.' Leveraging two distinct datasets and evaluating across three levels - Individual, Group and Stability - we employ machine learning classifiers assessed through several metrics. The Individual level model predicts individual participant preferences, the Group level model aggregates preferences across participants, and the Stability level model, an extension of the Group level, evaluates the stability of these preferences over time. By incorporating stakeholder preferences into the kidney allocation process, we aspire to advance the ethical dimensions of o
    
[^618]: 在临床文本中预测实体修饰语的迁移学习：以阿片类物质使用障碍病例检测为应用

    Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection. (arXiv:2401.15222v1 [cs.CL])

    [http://arxiv.org/abs/2401.15222](http://arxiv.org/abs/2401.15222)

    本研究提出了一种使用多任务学习和迁移学习的方法，在临床文本中预测实体修饰语。实验结果表明，该方法在公开数据集和新数据集上均取得了最新技术的结果。

    

    背景：从临床文本中提取的实体的语义可能会受到修饰语的显著改变，包括实体的否定、不确定性、条件性、严重性和主观性。现有的确定临床实体修饰语的模型涉及使用正则表达式或特征权重，这些权重是独立训练每个修饰语的。方法：我们开发并评估了一个多任务变换器架构设计，在公开可用的SemEval 2015任务14语料库和一个新的阿片类物质使用障碍（OUD）数据集上共同学习和预测修饰语，该数据集包含与SemEval共享的修饰语以及OUD特定的新修饰语。我们评估了我们的多任务学习方法与以前发表的系统的效果，并评估了仅共享部分临床修饰语时的临床实体修饰语的迁移学习的可行性。结果：我们的方法在来自SemEval 2015的ShARe语料库上取得了最新技术的结果。

    Background: The semantics of entities extracted from a clinical text can be dramatically altered by modifiers, including entity negation, uncertainty, conditionality, severity, and subject. Existing models for determining modifiers of clinical entities involve regular expression or features weights that are trained independently for each modifier.  Methods: We develop and evaluate a multi-task transformer architecture design where modifiers are learned and predicted jointly using the publicly available SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that contains modifiers shared with SemEval as well as novel modifiers specific for OUD. We evaluate the effectiveness of our multi-task learning approach versus previously published systems and assess the feasibility of transfer learning for clinical entity modifiers when only a portion of clinical modifiers are shared.  Results: Our approach achieved state-of-the-art results on the ShARe corpus from SemEval 2015 T
    
[^619]: EAGLE: 推测采样需要重新思考特征不确定性

    EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. (arXiv:2401.15077v1 [cs.LG])

    [http://arxiv.org/abs/2401.15077](http://arxiv.org/abs/2401.15077)

    EAGLE是一个无损加速语言模型推理的框架，通过在次顶层特征层面上自回归推理，并解决采样不确定性问题，实现了比传统方法更快3倍的速度。

    

    自回归解码使得大型语言模型（LLMs）的推理变得耗时。我们提出了一个简单的框架，EAGLE（用于提高语言模型效率的外推算法），实现了无损加速。与传统的推测采样方法不同，EAGLE在更规律的（次顶层）特征层面上自回归进行编写，并通过整合提前一个时间步的标记来解决下一个特征预测问题中的采样不确定性。EAGLE所提供的加速是无损的：它不需要微调目标LLM，并且生成的文本与原始的自回归解码的分布相同。截至本文提交时，EAGLE是已知推测采样家族中速度最快的框架。在MT-bench上，EAGLE比原始解码快3倍，比Lookahead快2倍，比Medusa快1.6倍。使用gpt-fast，EAGLE平均每秒达到160个标记与LLaMA2-Chat搭配。

    Auto-regressive decoding makes the inference of Large Language Models (LLMs) time-consuming. We propose a simple framework, EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), for lossless acceleration. Unlike traditional speculative sampling methods, EAGLE operates the drafting process auto-regressively at the more regular (second-top-layer) feature level and addresses the sampling uncertainty issues in the next-feature prediction problems by integrating tokens from one time step ahead. The acceleration provided by EAGLE is lossless: it involves no fine-tuning of the target LLM, and the generated text maintains the same distribution as that of vanilla auto-regressive decoding. As of the submission of this paper, EAGLE is the fastest known framework within the speculative sampling family. On MT-bench, EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x faster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s with LLaMA2-Chat 
    
[^620]: 基于机器学习的胶质瘤组织切片分析：一项综述

    Machine learning-based analysis of glioma tissue sections: a review. (arXiv:2401.15022v1 [eess.IV])

    [http://arxiv.org/abs/2401.15022](http://arxiv.org/abs/2401.15022)

    机器学习技术在胶质瘤组织切片分析中具有诊断和预测的潜力，当前研究聚焦于成人型弥漫性胶质瘤的苏木精和伊红染色组织切片，以及对该疾病的分类、分级、分子标记预测和生存预测等临床任务。

    

    近年来，胶质瘤的诊断变得越来越复杂。使用现代机器学习技术对胶质瘤组织进行组织学评估，为诊断和预测结果提供了新的机会。为了对当前研究的现状进行概述，本综述对70个公开可得的研究论文进行了研究，这些论文关于使用机器学习分析染色的胶质瘤组织切片，涵盖了分类（16/70），分级（23/70），分子标记预测（13/70）和生存预测（27/70）等诊断任务。所有的研究都在方法学方面及其临床适用性方面进行了评估。发现当前研究的重点是对成人型弥漫性胶质瘤的苏木精和伊红染色组织切片进行评估。多数研究（49/70）基于公开的胶质母细胞瘤和低级别胶质瘤数据集，仅有少数研究使用其他数据集。

    In recent years, the diagnosis of gliomas has become increasingly complex. Histological assessment of glioma tissue using modern machine learning techniques offers new opportunities to support diagnosis and outcome prediction. To give an overview of the current state of research, this review examines 70 publicly available research studies on machine learning-based analysis of stained human glioma tissue sections, covering the diagnostic tasks of subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and survival prediction (27/70). All studies were reviewed with regard to methodological aspects as well as clinical applicability. It was found that the focus of current research is the assessment of hematoxylin and eosin-stained tissue sections of adult-type diffuse gliomas. The majority of studies (49/70) are based on the publicly available glioblastoma and low-grade glioma datasets from The Cancer Genome Atlas (TCGA) and only a few studies employed other datasets in is
    
[^621]: 利用Ricci流引导的自编码器学习时变动力学

    Ricci flow-guided autoencoders in learning time-dependent dynamics. (arXiv:2401.14591v1 [cs.LG])

    [http://arxiv.org/abs/2401.14591](http://arxiv.org/abs/2401.14591)

    利用Ricci流引导的自编码器方法能够学习非线性动力学，尤其是偏微分方程。该方法通过在训练中学习流形，并使用Ricci流使流形潜空间逐步适应动力学的变化，从而获得更好的表示能力。在实验中，我们展示了该方法在具有周期性和随机性的PDE上的应用，并评估了在分布内和外推场景中的误差。

    

    我们提出了一种基于流形的自编码器方法，用于学习时间上的非线性动力学，尤其是偏微分方程（PDE），其中流形潜空间根据Ricci流发展。这可以通过在物理信息设置中模拟Ricci流来实现，并且可以匹配流形量，以便实现Ricci流。使用我们的方法，流形是作为训练过程的一部分学习的，因此可以识别出理想的几何形状，同时演变也能在静态方法上引起更宽容的潜在表示。我们在一系列数值实验中展示了我们的方法，包括具有周期性和随机性等理想特征的PDE，并在分布内和外推场景中进行误差评估。

    We present a manifold-based autoencoder method for learning nonlinear dynamics in time, notably partial differential equations (PDEs), in which the manifold latent space evolves according to Ricci flow. This can be accomplished by simulating Ricci flow in a physics-informed setting, and manifold quantities can be matched so that Ricci flow is empirically achieved. With our methodology, the manifold is learned as part of the training procedure, so ideal geometries may be discerned, while the evolution simultaneously induces a more accommodating latent representation over static methods. We present our method on a range of numerical experiments consisting of PDEs that encompass desirable characteristics such as periodicity and randomness, remarking error on in-distribution and extrapolation scenarios.
    
[^622]: 设计你自己的宇宙：一种物理信息引导的无偏方法来增强图神经网络

    Design Your Own Universe: A Physics-Informed Agnostic Method for Enhancing Graph Neural Networks. (arXiv:2401.14580v1 [cs.LG])

    [http://arxiv.org/abs/2401.14580](http://arxiv.org/abs/2401.14580)

    本文提出了一种物理信息引导的无偏方法来增强图神经网络，通过引入附加节点和使用正负权重重连连接来丰富图结构，以解决过度平滑和过度压缩的问题。

    

    物理信息引导的图神经网络通过缓解常见的GNN挑战（如过度平滑化、过度压缩和异质适应）在学习图结构数据方面取得了显著的性能。尽管取得了这些进展，仍然在开发一种简单而有效的范式来适当地整合处理所有这些挑战的先前方法。在本文中，我们将GNN的传播与物理粒子系统进行类比，提出了一种模型无关的增强框架。该框架通过引入附加节点和使用正负权重重连连接来丰富图结构，受节点标记信息的指导。我们理论上验证了通过我们的方法增强的GNN可以有效地避免过度平滑问题，并对过度压缩具有鲁棒性。此外，我们对重连图进行了谱分析，证明了相应的GNN可以...

    Physics-informed Graph Neural Networks have achieved remarkable performance in learning through graph-structured data by mitigating common GNN challenges such as over-smoothing, over-squashing, and heterophily adaption. Despite these advancements, the development of a simple yet effective paradigm that appropriately integrates previous methods for handling all these challenges is still underway. In this paper, we draw an analogy between the propagation of GNNs and particle systems in physics, proposing a model-agnostic enhancement framework. This framework enriches the graph structure by introducing additional nodes and rewiring connections with both positive and negative weights, guided by node labeling information. We theoretically verify that GNNs enhanced through our approach can effectively circumvent the over-smoothing issue and exhibit robustness against over-squashing. Moreover, we conduct a spectral analysis on the rewired graph to demonstrate that the corresponding GNNs can f
    
[^623]: 具有分布式固定设计量子芯片和量子信道的联邦学习

    Federated learning with distributed fixed design quantum chips and quantum channels. (arXiv:2401.13421v1 [quant-ph])

    [http://arxiv.org/abs/2401.13421](http://arxiv.org/abs/2401.13421)

    本论文提出了一种具有分布式固定设计量子芯片和量子信道的量子联邦学习模型，通过量子态的传递和聚合梯度来更新参数，提供更高的隐私保护和指数级的效率。

    

    经过客户端的精心设计查询，经典联邦学习中的隐私可以被突破。然而，由于数据中的测量会导致信息的丢失，量子通信信道被认为更加安全，因为可以检测到这种信息丢失。因此，量子版本的联邦学习可以提供更多的隐私保护。此外，通过量子信道发送N维数据向量需要发送log N个纠缠态量子比特，如果数据向量作为量子态获取，这可以提供指数级的效率。在本文中，我们提出了一种量子联邦学习模型，其中基于由集中式服务器发送的量子态，操作固定设计的量子芯片。基于接收到的叠加态，客户端计算并将其本地梯度作为量子态发送到服务器，服务器将这些梯度聚合以更新参数。由于服务器不发送模型信息，

    The privacy in classical federated learning can be breached through the use of local gradient results by using engineered queries from the clients. However, quantum communication channels are considered more secure because the use of measurements in the data causes some loss of information, which can be detected. Therefore, the quantum version of federated learning can be used to provide more privacy. Additionally, sending an $N$ dimensional data vector through a quantum channel requires sending $\log N$ entangled qubits, which can provide exponential efficiency if the data vector is obtained as quantum states.  In this paper, we propose a quantum federated learning model where fixed design quantum chips are operated based on the quantum states sent by a centralized server. Based on the coming superposition states, the clients compute and then send their local gradients as quantum states to the server, where they are aggregated to update parameters. Since the server does not send model
    
[^624]: 通过分解梯度下降实现低胞状秩张量恢复

    Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent. (arXiv:2401.11940v1 [cs.LG])

    [http://arxiv.org/abs/2401.11940](http://arxiv.org/abs/2401.11940)

    本文提出了一种通过分解梯度下降方法解决低胞状秩张量恢复问题的高效方法，该方法通过将大张量分解为两个较小的因子张量，在减少计算成本和存储需求的同时，确保了收敛性。

    

    本文研究了从少量被破坏的线性测量中恢复具有低胞状秩结构的张量的问题。传统方法需要计算张量奇异值分解（t-SVD），这是一种计算密集的过程，使它们难以处理大规模张量。为了解决这个挑战，我们提出了一种基于类似于Burer-Monteiro（BM）方法的分解过程的高效低胞状秩张量恢复方法。具体而言，我们的基本方法涉及将一个大张量分解为两个较小的因子张量，然后通过分解梯度下降（FGD）来解决问题。该策略消除了t-SVD计算的需要，从而减少了计算成本和存储需求。我们提供了严格的理论分析，以保证FGD在无噪声和有噪声情况下的收敛性。

    This paper considers the problem of recovering a tensor with an underlying low-tubal-rank structure from a small number of corrupted linear measurements. Traditional approaches tackling such a problem require the computation of tensor Singular Value Decomposition (t-SVD), that is a computationally intensive process, rendering them impractical for dealing with large-scale tensors. Aim to address this challenge, we propose an efficient and effective low-tubal-rank tensor recovery method based on a factorization procedure akin to the Burer-Monteiro (BM) method. Precisely, our fundamental approach involves decomposing a large tensor into two smaller factor tensors, followed by solving the problem through factorized gradient descent (FGD). This strategy eliminates the need for t-SVD computation, thereby reducing computational costs and storage requirements. We provide rigorous theoretical analysis to ensure the convergence of FGD under both noise-free and noisy situations. Additionally, it 
    
[^625]: LoMA: 无损压缩的内存注意力

    LoMA: Lossless Compressed Memory Attention. (arXiv:2401.09486v1 [cs.LG])

    [http://arxiv.org/abs/2401.09486](http://arxiv.org/abs/2401.09486)

    LoMA是一种无损压缩的内存注意力方法，可以有效地处理长文本并减少资源消耗。

    

    处理长文本是大型语言模型（LLMs）最重要的能力之一，但随着文本长度的增加，资源消耗也急剧增加。目前，通过压缩KV缓存来减少资源消耗是一种常见的方法。尽管存在许多现有的压缩方法，但它们都有一个共同的缺点：压缩是有损的。也就是说，在压缩过程中信息不可避免地会丢失。如果压缩率很高，丢失重要信息的概率会大大增加。我们提出了一种新方法，无损压缩的内存注意力（LoMA），可以根据一组压缩比率将信息无损压缩成特殊的内存令牌KV对。我们的实验证明，LoMA具有出色的性能，可以高效训练且具有非常有效的性能。

    The ability to handle long texts is one of the most important capabilities of Large Language Models (LLMs), but as the text length increases, the consumption of resources also increases dramatically. At present, reducing resource consumption by compressing the KV cache is a common approach. Although there are many existing compression methods, they share a common drawback: the compression is not lossless. That is, information is inevitably lost during the compression process. If the compression rate is high, the probability of losing important information increases dramatically. We propose a new method, Lossless Compressed Memory Attention (LoMA), which allows for lossless compression of information into special memory token KV pairs according to a set compression ratio. Our experiments have achieved remarkable results, demonstrating that LoMA can be efficiently trained and has very effective performance.
    
[^626]: DCRMTA: 无偏的多触点归因的因果表示

    DCRMTA: Unbiased Causal Representation for Multi-touch Attribution. (arXiv:2401.08875v1 [cs.LG])

    [http://arxiv.org/abs/2401.08875](http://arxiv.org/abs/2401.08875)

    DCRMTA提出了一种无偏的多触点归因方法，通过建立转化预测模型和构建对照触点序列来减轻偏差的影响。

    

    多触点归因（MTA）在实现对每个广告触点对于转化行为的贡献的公正估计方面起着关键作用，深刻影响预算分配和广告推荐。传统的多触点归因方法首先构建一个转化预测模型，通过历史数据学习触点序列和用户购买行为之间的内在关系。在此基础上，从原始序列子集中构建对照触点序列，并使用预测模型估计转化，从而计算广告贡献。这些方法的一个隐含假设是转化预测模型的无偏性。然而，由于用户偏好和互联网推荐机制（如过去的购物记录导致的广告推荐同质化）引起的混杂变量因素，转化中很容易产生偏差。

    Multi-touch attribution (MTA) currently plays a pivotal role in achieving a fair estimation of the contributions of each advertising touchpoint to-wards conversion behavior, deeply influencing budget allocation and advertising recommenda-tion. Traditional multi-touch attribution methods initially build a conversion prediction model, an-ticipating learning the inherent relationship be-tween touchpoint sequences and user purchasing behavior through historical data. Based on this, counterfactual touchpoint sequences are con-structed from the original sequence subset, and conversions are estimated using the prediction model, thus calculating advertising contributions. A covert assumption of these methods is the un-biased nature of conversion prediction models. However, due to confounding variables factors arising from user preferences and internet recom-mendation mechanisms such as homogenization of ad recommendations resulting from past shop-ping records, bias can easily occur in conversi
    
[^627]: 我们真的需要数据吗？

    Do We Really Even Need Data?. (arXiv:2401.08702v1 [stat.ME])

    [http://arxiv.org/abs/2401.08702](http://arxiv.org/abs/2401.08702)

    本文探讨了使用预训练算法的预测作为因变量的统计挑战，并着重阐述了三个可能的错误来源。

    

    随着人工智能和机器学习工具的普及，科学家在数据收集方面面临着新的障碍（例如成本上升、调查响应率下降），研究人员越来越多地使用预训练算法的预测作为因变量。虽然从财务和后勤的角度来看这样做具有吸引力，但使用标准的推论工具可能会在替换真实的未观察到的结果值时误代表自变量与所关心的结果之间的联系。本文对这种所谓“后预测推断”问题的统计挑战进行了表征，并阐明了可能存在的三个错误来源：（i）预测结果与其真实未观察到的对应物之间的关系，（ii）机器学习模型对重新采样或对训练数据的不确定性的鲁棒性，以及（iii）适当地将预测结果的偏差和不确定性传播到最终的推断中。

    As artificial intelligence and machine learning tools become more accessible, and scientists face new obstacles to data collection (e.g. rising costs, declining survey response rates), researchers increasingly use predictions from pre-trained algorithms as outcome variables. Though appealing for financial and logistical reasons, using standard tools for inference can misrepresent the association between independent variables and the outcome of interest when the true, unobserved outcome is replaced by a predicted value. In this paper, we characterize the statistical challenges inherent to this so-called ``post-prediction inference'' problem and elucidate three potential sources of error: (i) the relationship between predicted outcomes and their true, unobserved counterparts, (ii) robustness of the machine learning model to resampling or uncertainty about the training data, and (iii) appropriately propagating not just bias but also uncertainty from predictions into the ultimate inference
    
[^628]: 隐私保护下自适应实验中CATE的差分隐私估计

    Differentially Private Estimation of CATE in Adaptive Experiment. (arXiv:2401.08224v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2401.08224](http://arxiv.org/abs/2401.08224)

    本研究提出在自适应实验中使用差分隐私估计条件平均处理效果(CATE)，平衡社会福利损失和统计功率，并通过匹配上下界以及帕累托最优性概念来获得最优解。

    

    自适应实验广泛应用于临床试验和其他场景中估计条件平均处理效果(CATE)。虽然实验的主要目标是最大化估计精度，但由于社会福利的要求，为患者提供具有优越结果的治疗也是至关重要的，这可以通过环境批次框架中的遗憾来衡量。这两个目标经常导致对比优化分配机制。此外，在包含敏感数据（如患者健康记录）的临床场景中出现隐私问题。因此，治疗分配机制必须纳入强大的隐私保护措施。在本文中，我们研究了环境批次实验中社会福利损失和统计功率之间的权衡。我们为多目标优化问题提出了匹配的上下界，并采用帕累托最优性的概念来数学地刻画最优解。

    Adaptive experiment is widely adopted to estimate conditional average treatment effect (CATE) in clinical trials and many other scenarios. While the primary goal in experiment is to maximize estimation accuracy, due to the imperative of social welfare, it's also crucial to provide treatment with superior outcomes to patients, which is measured by regret in contextual bandit framework. These two objectives often lead to contrast optimal allocation mechanism. Furthermore, privacy concerns arise in clinical scenarios containing sensitive data like patients health records. Therefore, it's essential for the treatment allocation mechanism to incorporate robust privacy protection measures. In this paper, we investigate the tradeoff between loss of social welfare and statistical power in contextual bandit experiment. We propose a matched upper and lower bound for the multi-objective optimization problem, and then adopt the concept of Pareto optimality to mathematically characterize the optimal
    
[^629]: 使用自适应网络的方法对加密货币价值进行先进预测

    An adaptive network-based approach for advanced forecasting of cryptocurrency values. (arXiv:2401.05441v1 [q-fin.ST])

    [http://arxiv.org/abs/2401.05441](http://arxiv.org/abs/2401.05441)

    本文提出了一种使用自适应网络的方法，通过历史数据对比特币、以太坊和比特币支配度、以太坊支配度进行预测，具有较高的预测准确性。

    

    本文描述了一种使用自适应网络基于模糊推理系统 (ANFIS) 预测未来七天加密货币价格的架构。在每日时间框架下考虑了比特币 (BTC)、以太坊 (ETH)、比特币支配度 (BTC.D) 和以太坊支配度 (ETH.D)的历史数据。所使用的方法包括混合和反向传播算法，以及网格划分、减法聚类和模糊C均值聚类 (FCM) 算法用于数据聚类。通过统计评估标准比较了本文设计的架构性能与不同输入和神经网络模型。最终，该方法能够在短时间内预测数字货币的价格。

    This paper describes an architecture for predicting the price of cryptocurrencies for the next seven days using the Adaptive Network Based Fuzzy Inference System (ANFIS). Historical data of cryptocurrencies and indexes that are considered are Bitcoin (BTC), Ethereum (ETH), Bitcoin Dominance (BTC.D), and Ethereum Dominance (ETH.D) in a daily timeframe. The methods used to teach the data are hybrid and backpropagation algorithms, as well as grid partition, subtractive clustering, and Fuzzy C-means clustering (FCM) algorithms, which are used in data clustering. The architectural performance designed in this paper has been compared with different inputs and neural network models in terms of statistical evaluation criteria. Finally, the proposed method can predict the price of digital currencies in a short time.
    
[^630]: CreINNs: Credal-Set Interval Neural Networks用于分类任务中的不确定性估计

    CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation in Classification Tasks. (arXiv:2401.05043v1 [cs.LG])

    [http://arxiv.org/abs/2401.05043](http://arxiv.org/abs/2401.05043)

    CreINNs是一种用于分类任务的Credal-Set Interval Neural Networks，通过保留传统的区间神经网络结构，捕捉权重不确定性，并使用概率区间的数学框架预测可信区间。实验结果表明，CreINNs在不确定性估计方面优于变分贝叶斯神经网络和深度集成，并且具有较低的计算复杂度和模型大小。

    

    不确定性估计对于提高神经网络的可靠性越来越有吸引力。在这项工作中，我们提出了新颖的Credal-Set Interval Neural Networks（CreINNs），用于分类任务。CreINNs保留了传统的区间神经网络结构，通过确定性区间捕捉权重的不确定性，同时使用概率区间的数学框架预测可信区间。在一个超出分发检测基准（CIFAR10 vs SVHN）上的实验验证中，CreINNs相比于变分贝叶斯神经网络（BNNs）和深度集成（DEs），在认知不确定性估计方面表现出色。此外，与变分BNNs相比，CreINNs的计算复杂度显著降低，并且比DEs具有较小的模型大小。

    Uncertainty estimation is increasingly attractive for improving the reliability of neural networks. In this work, we present novel credal-set interval neural networks (CreINNs) designed for classification tasks. CreINNs preserve the traditional interval neural network structure, capturing weight uncertainty through deterministic intervals, while forecasting credal sets using the mathematical framework of probability intervals. Experimental validations on an out-of-distribution detection benchmark (CIFAR10 vs SVHN) showcase that CreINNs outperform epistemic uncertainty estimation when compared to variational Bayesian neural networks (BNNs) and deep ensembles (DEs). Furthermore, CreINNs exhibit a notable reduction in computational complexity compared to variational BNNs and demonstrate smaller model sizes than DEs.
    
[^631]: 自扩展LLM:无需调整的LLM上下文窗口。

    LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. (arXiv:2401.01325v1 [cs.CL])

    [http://arxiv.org/abs/2401.01325](http://arxiv.org/abs/2401.01325)

    本研究提出了一种名为Self-Extend的方法，通过自身扩展现有LLMs的上下文窗口，无需调整，充分利用LLMs处理长上下文的固有能力。

    

    本研究揭示了LLM在处理长上下文时的固有能力，而无需进行精调。在训练过程中，训练序列的有限长度可能限制了大型语言模型（LLMs）在推理过程中对长输入序列的应用。在本研究中，我们认为现有的LLMs本身具有处理长上下文的固有能力。基于这一观点，我们建议通过自身扩展LLMs的上下文窗口，以充分利用其固有能力。我们提出了Self-Extend方法来激发LLMs的长上下文处理潜力。基本思想是构建双层注意信息：群组级和邻居级。这两个级别通过原始模型的自注意力计算，这意味着所提方法不需要任何训练。只需修改四行代码，所提方法就可以轻松扩展现有LLMs的上下文窗口，而无需进行任何精调。我们进行了全面的实验证明，结果表明所提方法可以+摘要减掉文章最后一句話

    This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can 
    
[^632]: 消除差距：基于模型预测控制的可验证模型无关二次规划控制器的学习

    Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming Controllers Inspired by Model Predictive Control. (arXiv:2312.05332v3 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2312.05332](http://arxiv.org/abs/2312.05332)

    本文提出了一种新的参数化控制器类，利用深度强化学习训练其控制器的参数，从而消除了常见控制器中的可验证性和性能保证的限制。该控制器类似于模型预测控制问题的二次规划求解器，具有可验证的属性，并且在控制性能和鲁棒性方面与其他控制器相媲美。同时，该控制器的计算效率显著优于传统的模型预测控制。

    

    本文介绍了一种新的参数化控制器类，受到模型预测控制（MPC）的启发。该控制器类似于线性MPC问题的二次规划（QP）求解器，但控制器的参数是通过深度强化学习（DRL）进行训练，而不是从系统模型中推导出来的。这种方法解决了常见控制器中使用MLP或其他通用神经网络架构的DRL的可验证性和性能保证的局限性，并且所学习的控制器具有与MPC类似的持续可行性和渐近稳定性等可验证属性。另一方面，数值实验表明，所提出的控制器在控制性能上与MPC和MLP控制器相匹配，并且对建模不确定性和噪声具有更优的鲁棒性。此外，所提出的控制器在计算效率上明显优于MPC。

    In this paper, we introduce a new class of parameterized controllers, drawing inspiration from Model Predictive Control (MPC). The controller resembles a Quadratic Programming (QP) solver of a linear MPC problem, with the parameters of the controller being trained via Deep Reinforcement Learning (DRL) rather than derived from system models. This approach addresses the limitations of common controllers with Multi-Layer Perceptron (MLP) or other general neural network architecture used in DRL, in terms of verifiability and performance guarantees, and the learned controllers possess verifiable properties like persistent feasibility and asymptotic stability akin to MPC. On the other hand, numerical examples illustrate that the proposed controller empirically matches MPC and MLP controllers in terms of control performance and has superior robustness against modeling uncertainty and noises. Furthermore, the proposed controller is significantly more computationally efficient compared to MPC a
    
[^633]: SASSL:通过神经风格迁移增强自监督学习

    SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer. (arXiv:2312.01187v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.01187](http://arxiv.org/abs/2312.01187)

    SASSL提出了一种基于神经风格迁移的增强技术，通过解耦语义和风格属性，在自监督学习中生成多样化的增强样本，从而提升了图像分类性能。

    

    自监督学习依赖于数据增强来从无标签图像中提取有意义的表征。现有的最先进的增强流水线包括了各种原始的转换，但通常忽略了自然图像的结构。因此，增强样本可能显示出退化的语义信息和低风格多样性，从而影响到自监督表征的下游性能。为了克服这个问题，我们提出了一种名为SASSL的新型增强技术，它基于神经风格迁移。该方法将图像中的语义和风格属性解耦，并仅对风格应用转换，保持内容，生成多样化的增强样本，更好地保留它们的语义属性。实验结果显示，与广为接受的MoCo v2相比，我们的技术在ImageNet上的top-1分类性能提升超过2%。

    Self-supervised learning relies heavily on data augmentation to extract meaningful representations from unlabeled images. While existing state-of-the-art augmentation pipelines incorporate a wide range of primitive transformations, these often disregard natural image structure. Thus, augmented samples can exhibit degraded semantic information and low stylistic diversity, affecting downstream performance of self-supervised representations. To overcome this, we propose SASSL: Style Augmentations for Self Supervised Learning, a novel augmentation technique based on Neural Style Transfer. The method decouples semantic and stylistic attributes in images and applies transformations exclusively to the style while preserving content, generating diverse augmented samples that better retain their semantic properties. Experimental results show our technique achieves a top-1 classification performance improvement of more than 2% on ImageNet compared to the well-established MoCo v2. We also measure
    
[^634]: 通过整数规划对温顺函数进行分段多项式回归

    Piecewise polynomial regression of tame functions via integer programming. (arXiv:2311.13544v1 [math.OC] CROSS LISTED)

    [http://arxiv.org/abs/2311.13544](http://arxiv.org/abs/2311.13544)

    本论文提出了使用整数规划对温顺函数进行分段多项式回归的方法，这可以用于估计包含在许多应用中的温顺函数，并且展示了令人期待的计算结果。

    

    我们考虑估计属于一类特定的非光滑函数的函数的任务，即所谓的温顺函数。这些函数出现在各种应用中：深度学习的训练、混合整数规划的价值函数或小分子的波函数。我们展示了温顺函数在任何完全维度的立方体上可用分段多项式来逼近。然后我们提出了第一个分段多项式回归的混合整数规划形式。这些方法可用于估计温顺函数。我们展示了令人期待的计算结果。

    We consider the task of estimating functions belonging to a specific class of nonsmooth functions, namely so-called tame functions. These functions appear in a wide range of applications: training deep learning, value functions of mixed-integer programs, or wave functions of small molecules. We show that tame functions are approximable by piecewise polynomials on any full-dimensional cube. We then present the first ever mixed-integer programming formulation of piecewise polynomial regression. Together, these can be used to estimate tame functions. We demonstrate promising computational results.
    
[^635]: Anytime-Competitive Reinforcement Learning with Policy Prior. (arXiv:2311.01568v1 [cs.LG]) （具有策略先验的任意时刻竞争性强化学习）

    Anytime-Competitive Reinforcement Learning with Policy Prior. (arXiv:2311.01568v1 [cs.LG])

    [http://arxiv.org/abs/2311.01568](http://arxiv.org/abs/2311.01568)

    本文研究了具有策略先验的任意时刻竞争性强化学习问题（A-CMDP）。我们提出了一种新的算法ACRL，该算法可以保证任意时刻的成本约束，并在实验中验证了其回报性能和成本约束保证。

    

    本文研究了任意时刻竞争性马尔可夫决策过程（A-CMDP）的问题。现有的关于受限制马尔可夫决策过程（CMDP）的研究旨在在随机动态中优化期望回报同时约束期望成本，但是特定情节中的成本仍然可能非常高。相反，A-CMDP的目标是在任意一轮的任何情节中，优化期望回报同时保证有界的成本，并针对策略先验进行了保证。我们提出了一种新的算法，称为Anytime-Competitive Reinforcement Learning（ACRL），它可以证明地保证了任意时刻的成本约束。遗憾分析表明，该策略在任意的竞争性约束下渐近地与最优回报相匹配。在碳智能计算应用中的实验验证了ACRL的回报性能和成本约束保证。

    This paper studies the problem of Anytime-Competitive Markov Decision Process (A-CMDP). Existing works on Constrained Markov Decision Processes (CMDPs) aim to optimize the expected reward while constraining the expected cost over random dynamics, but the cost in a specific episode can still be unsatisfactorily high. In contrast, the goal of A-CMDP is to optimize the expected reward while guaranteeing a bounded cost in each round of any episode against a policy prior. We propose a new algorithm, called Anytime-Competitive Reinforcement Learning (ACRL), which provably guarantees the anytime cost constraints. The regret analysis shows the policy asymptotically matches the optimal reward achievable under the anytime competitive constraints. Experiments on the application of carbon-intelligent computing verify the reward performance and cost constraint guarantee of ACRL.
    
[^636]: 从视觉-语言基础模型中提取对抗性鲁棒性的框架

    Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models. (arXiv:2311.01441v1 [cs.LG])

    [http://arxiv.org/abs/2311.01441](http://arxiv.org/abs/2311.01441)

    本文提出了一个概念简单且轻量级的框架，通过结合知识蒸馏和数据增强的方法来提高视觉模型的鲁棒性，并从预训练的基础模型中获得鲁棒教师模型的知识。借助离散对抗蒸馏方法，我们生成更有信息量的对抗样本，取得了在对抗性样本上鲁棒性显著提升的结果。此外，我们提供了理论框架来支持在知识蒸馏和数据增强设置中使用鲁棒教师模型，并展示了在不同学生模型上的显著性能提升。我们的方法在计算负载方面的开销较小，并可以与其他数据增强方法轻松结合。

    

    我们提出了一个概念简单且轻量级的框架，通过知识蒸馏和数据增强的结合来提高视觉模型的鲁棒性。我们通过从预训练的基础模型中进行蒸馏，展示了在对抗性样本上获得的鲁棒性增益，以此反驳了更大的模型不一定会成为更好的教师的猜想。我们还提出了离散对抗蒸馏（Discrete Adversarial Distillation，DAD）方法，利用鲁棒的教师模型生成对抗样本，并通过VQGAN将其离散化，从而创造出比标准数据增强技术更有信息量的样本。我们提供了一个理论框架，用于在知识蒸馏和数据增强的设置中使用鲁棒的教师模型，并在不同的学生模型中展示了在对抗性样本上的鲁棒性和干净准确性的显著提升。值得注意的是，与类似技术相比，我们的方法增加了少量的计算负载，并且可以轻松与其他数据增强方法相结合。

    We propose a conceptually simple and lightweight framework for improving the robustness of vision models through the combination of knowledge distillation and data augmentation. We address the conjecture that larger models do not make for better teachers by showing strong gains in out-of-distribution robustness when distilling from pretrained foundation models. Following this finding, we propose Discrete Adversarial Distillation (DAD), which leverages a robust teacher to generate adversarial examples and a VQGAN to discretize them, creating more informative samples than standard data augmentation techniques. We provide a theoretical framework for the use of a robust teacher in the knowledge distillation with data augmentation setting and demonstrate strong gains in out-of-distribution robustness and clean accuracy across different student architectures. Notably, our method adds minor computational overhead compared to similar techniques and can be easily combined with other data augmen
    
[^637]: Vision-Language Foundation Models作为有效的机器人模仿者

    Vision-Language Foundation Models as Effective Robot Imitators. (arXiv:2311.01378v1 [cs.RO])

    [http://arxiv.org/abs/2311.01378](http://arxiv.org/abs/2311.01378)

    该论文介绍了一种利用视觉语言基础模型进行机器人操作的方法，通过在语言条件的操作数据集上进行微调，实现了在低性能平台上的有效模仿控制。

    

    最近在视觉语言基础模型方面的进展显示出它们理解多模态数据和解决复杂的视觉语言任务（包括机器人操作）的能力。我们寻求一种简单的方式来利用现有的视觉语言模型（VLMs）在机器人数据上进行简单微调。为此，我们提出了一个简单而新颖的视觉语言操作框架，名为RoboFlamingo，它建立在开源的VLMs，OpenFlamingo之上。与以前的工作不同，RoboFlamingo利用预训练的VLMs进行单步视觉语言理解，使用显式策略头模拟顺序历史信息，并只在语言条件的操作数据集上进行微调。这种分解为RoboFlamingo提供了在低性能平台上进行开环控制和部署的灵活性。通过在测试基准上大幅超过现有技术水平，我们展示了RoboFlamingo可以成为一种有效的机器人模仿者。

    Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective an
    
[^638]: 基于能源的法律领域文本分类常见方法的比较分析

    An energy-based comparative analysis of common approaches to text classification in the Legal domain. (arXiv:2311.01256v1 [cs.CL])

    [http://arxiv.org/abs/2311.01256](http://arxiv.org/abs/2311.01256)

    本研究通过在法律领域的文本分类任务上进行比较分析，综合考虑性能和能源消耗等指标，探讨了大型语言模型与传统方法的优劣，并强调了在性能相近的情况下应重视生产成本、能源消耗和碳足迹等方面的考量。

    

    大部分机器学习研究评估最佳解决方案的性能。然而，在追求最佳性能的竞争中，经常忽视许多重要因素，而事实上，这些因素应该被仔细考虑。实际上，有时不同方法之间的性能差距可以忽略不计，而生产成本、能源消耗和碳足迹等因素必须考虑在内。大型语言模型（LLMs）被广泛应用于学术界和工业界的NLP问题。在这项工作中，我们在LexGLUE基准上对LLM和传统方法（例如SVM）进行了详细的定量比较，同时考虑性能（标准指标）和其他指标，如时间、耗能和成本，总之就是碳足迹。在我们的分析中，我们分别考虑了原型设计阶段（通过训练-验证-测试迭代进行模型选择）和生产阶段。

    Most Machine Learning research evaluates the best solutions in terms of performance. However, in the race for the best performing model, many important aspects are often overlooked when, on the contrary, they should be carefully considered. In fact, sometimes the gaps in performance between different approaches are neglectable, whereas factors such as production costs, energy consumption, and carbon footprint must take into consideration. Large Language Models (LLMs) are extensively adopted to address NLP problems in academia and industry. In this work, we present a detailed quantitative comparison of LLM and traditional approaches (e.g. SVM) on the LexGLUE benchmark, which takes into account both performance (standard indices) and alternative metrics such as timing, power consumption and cost, in a word: the carbon-footprint. In our analysis, we considered the prototyping phase (model selection by training-validation-test iterations) and in-production phases separately, since they fol
    
[^639]: 强化学习的扩散模型: 一份综述

    Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v1 [cs.LG])

    [http://arxiv.org/abs/2311.01223](http://arxiv.org/abs/2311.01223)

    强化学习中的扩散模型已经成为一种突出的生成模型，通过在样本质量和训练稳定性方面的优势改进了强化学习解决方案。该综述提供了这一新兴领域发展的概述，并探讨了扩散模型在强化学习中的分类法和应用。

    

    扩散模型作为一种突出的生成模型类别已经出现，超越了以往方法在样本质量和训练稳定性方面的优势。最近的研究表明，扩散模型在改进强化学习（RL）解决方案方面具有优势，包括作为轨迹规划器、表达能力丰富的策略类别、数据合成器等。本综述旨在提供该新兴领域发展的概述，并希望能启发新的研究方向。首先，我们审查了当前RL算法遇到的一些挑战。然后，我们根据扩散模型在RL中所扮演的角色，提出了现有方法的分类法，并探讨了如何解决现有挑战。我们进一步概述了扩散模型在各种与RL相关任务中的成功应用，并讨论了当前方法的局限性。最后，我们总结了这项综述，并提出了对未来研究方向的见解，重点是提高模型性能和应用扩散模型的方法。

    Diffusion models have emerged as a prominent class of generative models, surpassing previous methods regarding sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions, including as trajectory planners, expressive policy classes, data synthesizers, etc. This survey aims to provide an overview of the advancements in this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by current RL algorithms. Then, we present a taxonomy of existing methods based on the roles played by diffusion models in RL and explore how the existing challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks while discussing the limitations of current approaches. Finally, we conclude the survey and offer insights into future research directions, focusing on enhancing model performance and applying diffusion m
    
[^640]: Jina Embeddings 2: 面向长篇文档的8192-Token通用文本嵌入模型

    Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])

    [http://arxiv.org/abs/2310.19923](http://arxiv.org/abs/2310.19923)

    Jina Embeddings 2是一个能够处理长篇文档的文本嵌入模型，突破了传统512个标记限制，提供了高达8192个标记的容量。

    

    文本嵌入模型已经成为将句子转化为固定大小特征向量的强大工具，这些向量包含了语义信息。尽管这些模型对于信息检索、语义聚类和文本重排序等任务至关重要，但大多数现有的开源模型，尤其是基于BERT等架构构建的模型，难以表示长篇文档，并且常常会进行截断。为了缓解这个挑战，一种常见的方法是将文档分割成更小的段落进行嵌入。然而，这种策略会导致更大的向量集合，进而增加内存消耗，并且在向量搜索时会出现计算密集和延迟升高的问题。为了解决这些挑战，我们介绍了Jina Embeddings 2，这是一个开源的文本嵌入模型，可以容纳高达8192个标记。该模型旨在突破传统的512个标记限制，能够灵活处理长篇文档。

    Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
    
[^641]: 解决A/B测试中数据训练循环引起的干扰：一种加权训练方法

    Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach. (arXiv:2310.17496v1 [stat.ME])

    [http://arxiv.org/abs/2310.17496](http://arxiv.org/abs/2310.17496)

    该论文提出了一种名为加权训练的方法，用于解决A/B测试中由数据训练循环引起的干扰。该方法通过训练模型预测每个数据点在实验组或控制组中出现的概率，并在模型训练过程中应用加权损失，从而实现了最小方差的估计结果，并且不会引起训练分布的变化。

    

    在现代推荐系统中，标准流程涉及使用历史数据训练机器学习模型来预测用户行为并持续改进推荐。然而，这些数据训练循环可能在A/B测试中引入干扰，其中控制组和实验组算法生成的数据，可能具有不同的分布，被合并在一起。为了解决这些挑战，我们提出了一种新颖的方法，称为加权训练。该方法包括训练一个模型来预测每个数据点出现在实验组或控制组数据中的概率，并在模型训练过程中应用加权损失。我们通过模拟研究证明了这种方法在所有估计量中具有最小的方差，且不会导致训练分布发生变化。我们通过模拟研究证明了与其他方法相比，我们的方法具有较低的偏差和方差。

    In modern recommendation systems, the standard pipeline involves training machine learning models on historical data to predict user behaviors and improve recommendations continuously. However, these data training loops can introduce interference in A/B tests, where data generated by control and treatment algorithms, potentially with different distributions, are combined. To address these challenges, we introduce a novel approach called weighted training. This approach entails training a model to predict the probability of each data point appearing in either the treatment or control data and subsequently applying weighted losses during model training. We demonstrate that this approach achieves the least variance among all estimators without causing shifts in the training distributions. Through simulation studies, we demonstrate the lower bias and variance of our approach compared to other methods.
    
[^642]: 具有参数化图的图神经网络

    Graph Neural Networks with a Distribution of Parametrized Graphs. (arXiv:2310.16401v1 [cs.LG])

    [http://arxiv.org/abs/2310.16401](http://arxiv.org/abs/2310.16401)

    这篇论文提出了一种使用参数化和生成多个图来解决图神经网络仅使用单个观察图的挑战的方法。在节点分类和图回归任务上，通过最大似然估计网络参数，在多个图上使用马尔可夫链蒙特卡洛方法，结合PAC-Bayesian理论的原则，取得了性能改进。

    

    传统上，图神经网络是使用单个观察到的图进行训练的。然而，观察到的图仅代表了一种可能的实现。在许多应用中，图可能会遇到不确定性，例如存在错误或缺失的边，以及提供很少信息价值的边权重。为了解决这些挑战并捕捉先前在观察到的图中缺失的附加信息，我们引入潜在变量来参数化和生成多个图。我们基于多个图的期望最大化（EM）框架，使用最大似然估计网络参数。具体而言，我们使用马尔可夫链蒙特卡洛（MCMC）方法迭代地确定图的分布，结合PAC-Bayesian理论的原则。数值实验在异质图的节点分类和化学数据集的图回归上改进了性能。

    Traditionally, graph neural networks have been trained using a single observed graph. However, the observed graph represents only one possible realization. In many applications, the graph may encounter uncertainties, such as having erroneous or missing edges, as well as edge weights that provide little informative value. To address these challenges and capture additional information previously absent in the observed graph, we introduce latent variables to parameterize and generate multiple graphs. We obtain the maximum likelihood estimate of the network parameters in an Expectation-Maximization (EM) framework based on the multiple graphs. Specifically, we iteratively determine the distribution of the graphs using a Markov Chain Monte Carlo (MCMC) method, incorporating the principles of PAC-Bayesian theory. Numerical experiments demonstrate improvements in performance against baseline models on node classification for heterogeneous graphs and graph regression on chemistry datasets.
    
[^643]: 关于离散去噪扩散模型内在隐私属性的研究

    On the Inherent Privacy Properties of Discrete Denoising Diffusion Models. (arXiv:2310.15524v1 [cs.LG])

    [http://arxiv.org/abs/2310.15524](http://arxiv.org/abs/2310.15524)

    本研究探索了离散扩散模型在隐私保护方面的潜力，提供了关于训练数据集中每个数据点的隐私泄露的洞察，以及通过数据预处理减少合成数据集生成中隐私风险的方法。

    

    隐私问题导致合成数据集的创建激增，扩散模型成为一种有前景的方法。虽然以前的研究已经对这些模型进行了经验评估，但在提供数学特征化其隐私保护能力方面存在差距。为了解决这个问题，我们提出了离散扩散模型（DDMs）内在隐私保护的开创性理论研究，用于离散数据集生成。对于每个数据点的每个实例差异隐私（pDP），我们的框架阐明了给定训练数据集中每个数据点的潜在隐私泄露，从而为通过DDMs降低合成数据集生成的隐私风险提供了洞察。我们的界限还表明，使用$s$个大小的数据点进行训练会导致隐私泄露从$(\epsilon, \mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP到$(\epsilon, \mathcal{O}(\frac{1}{s\epsilon}))$-pDP的激增。

    Privacy concerns have led to a surge in the creation of synthetic datasets, with diffusion models emerging as a promising avenue. Although prior studies have performed empirical evaluations on these models, there has been a gap in providing a mathematical characterization of their privacy-preserving capabilities. To address this, we present the pioneering theoretical exploration of the privacy preservation inherent in discrete diffusion models (DDMs) for discrete dataset generation. Focusing on per-instance differential privacy (pDP), our framework elucidates the potential privacy leakage for each data point in a given training dataset, offering insights into data preprocessing to reduce privacy risks of the synthetic dataset generation via DDMs. Our bounds also show that training with $s$-sized data points leads to a surge in privacy leakage from $(\epsilon, \mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP to $(\epsilon, \mathcal{O}(\frac{1}{s\epsilon}))$-pDP during the transition from the pu
    
[^644]: DoGE: 使用泛化估计进行领域重新加权

    DoGE: Domain Reweighting with Generalization Estimation. (arXiv:2310.15393v1 [cs.LG])

    [http://arxiv.org/abs/2310.15393](http://arxiv.org/abs/2310.15393)

    DoGE提出了一种基于泛化估计的领域重新加权方法。通过使用梯度估计函数评估每个领域对泛化目标的贡献，重新调整了预训练数据中不同领域的采样概率。实验结果表明，该方法在提高大型语言模型的泛化能力方面取得了显著效果。

    

    预训练数据语料库的覆盖范围和组成对大型语言模型的泛化能力有着重要影响。传统上，预训练语料库由各种来源领域（如CommonCrawl、Wikipedia、Github等）按照特定的采样概率（领域权重）组成。然而，当前的方法缺乏一种基于最终泛化目标优化领域权重的原则方法。我们提出了一种称为DOmain reweighting with Generalization Estimation（DoGE）的方法，其中我们重新调整了每个领域的采样概率，根据它对最终泛化目标的贡献进行了基于梯度的泛化估计函数评估。首先，我们使用最小最大优化训练了一个小规模的代理模型来获取重新加权的领域权重。在每一步中，通过镜像下降法更新领域权重以最大化整体的泛化增益。最后，我们使用获得的领域权重来训练一个规模更大的完整语言模型。

    The coverage and composition of the pretraining data corpus significantly impacts the generalization ability of large language models. Conventionally, the pretraining corpus is composed of various source domains (e.g. CommonCrawl, Wikipedia, Github etc.) according to certain sampling probabilities (domain weights). However, current methods lack a principled way to optimize domain weights for ultimate goal for generalization. We propose DOmain reweighting with Generalization Estimation (DoGE), where we reweigh the sampling probability from each domain based on its contribution to the final generalization objective assessed by a gradient-based generalization estimation function. First, we train a small-scale proxy model with a min-max optimization to obtain the reweighted domain weights. At each step, the domain weights are updated to maximize the overall generalization gain by mirror descent. Finally we use the obtained domain weights to train a larger scale full-size language model. On
    
[^645]: 无监督联邦学习：具有对抗攻击鲁棒性的异构混合模型的联邦梯度EM算法

    Unsupervised Federated Learning: A Federated Gradient EM Algorithm for Heterogeneous Mixture Models with Robustness against Adversarial Attacks. (arXiv:2310.15330v1 [stat.ML])

    [http://arxiv.org/abs/2310.15330](http://arxiv.org/abs/2310.15330)

    本文介绍了一种针对带有异构混合比例的混合模型的无监督学习的新型联邦梯度EM算法，在适用于普通混合模型的全面有限样本理论基础上，对高斯混合模型（GMM）和混合回归（MoRs）进行了具体的估计误差分析。该算法具有适应未知任务相似性、抵抗对少部分数据源的对抗攻击、保护本地数据隐私以及计算和通信效率等关键优势。

    

    尽管有监督的联邦学习方法取得了显著的成功，但无监督的联邦学习领域相对较少探索。在本文中，我们介绍了一种针对带有异构混合比例的混合模型的无监督学习的新型联邦梯度EM算法。我们首先提出了适用于普通混合模型的全面有限样本理论，然后将这一通用理论应用于高斯混合模型（GMM）和混合回归（MoRs）以描述模型参数和混合比例的显式估计误差。我们提出的联邦梯度EM算法具有以下几个关键优势：适应未知任务相似性、对少部分数据源的对抗攻击具有弹性、保护本地数据隐私以及计算和通信效率。

    While supervised federated learning approaches have enjoyed significant success, the domain of unsupervised federated learning remains relatively underexplored. In this paper, we introduce a novel federated gradient EM algorithm designed for the unsupervised learning of mixture models with heterogeneous mixture proportions across tasks. We begin with a comprehensive finite-sample theory that holds for general mixture models, then apply this general theory on Gaussian Mixture Models (GMMs) and Mixture of Regressions (MoRs) to characterize the explicit estimation error of model parameters and mixture proportions. Our proposed federated gradient EM algorithm demonstrates several key advantages: adaptability to unknown task similarity, resilience against adversarial attacks on a small fraction of data sources, protection of local data privacy, and computational and communication efficiency.
    
[^646]: RealFM: 一个真实机制，激励数据贡献和设备参与

    RealFM: A Realistic Mechanism to Incentivize Data Contribution and Device Participation. (arXiv:2310.13681v1 [cs.GT])

    [http://arxiv.org/abs/2310.13681](http://arxiv.org/abs/2310.13681)

    RealFM 是一个真实的联邦机制，旨在解决现实环境中联邦学习中的驻泊者问题，通过模拟设备效用、激励数据贡献和设备参与，并提供了非线性关系的模型准确性和效用，从而改善了服务器和设备的效用和数据贡献。

    

    边缘设备参与联邦学习（FL）通常在设备-服务器通信的视角下进行研究（例如设备掉线），并假设边缘设备有持续参与FL的愿望。因此，在实际环境中实施当前的FL框架存在缺陷，许多框架遇到了驻泊者问题。为了将FL推向更真实的环境，我们提出了RealFM：第一个真正的联邦机制，它（1）实际地模拟设备效用，（2）激励数据贡献和设备参与，（3）可证明地消除了驻泊者现象。RealFM不需要数据共享，并允许模型准确性和效用之间存在非线性关系，相比于不参与和其他FL机制的设备，提高了服务器和参与设备的效用和数据贡献。在真实数据上，RealFM提高了设备和服务器的效用以及数据贡献，最多可达...

    Edge device participation in federating learning (FL) has been typically studied under the lens of device-server communication (e.g., device dropout) and assumes an undying desire from edge devices to participate in FL. As a result, current FL frameworks are flawed when implemented in real-world settings, with many encountering the free-rider problem. In a step to push FL towards realistic settings, we propose RealFM: the first truly federated mechanism which (1) realistically models device utility, (2) incentivizes data contribution and device participation, and (3) provably removes the free-rider phenomena. RealFM does not require data sharing and allows for a non-linear relationship between model accuracy and utility, which improves the utility gained by the server and participating devices compared to non-participating devices as well as devices participating in other FL mechanisms. On real-world data, RealFM improves device and server utility, as well as data contribution, by up t
    
[^647]: 等变深度权重空间对齐

    Equivariant Deep Weight Space Alignment. (arXiv:2310.13397v1 [cs.LG])

    [http://arxiv.org/abs/2310.13397](http://arxiv.org/abs/2310.13397)

    本论文提出了一个名为Deep-Align的新框架，用于学习解决权重对齐问题，以加速对齐过程并提高其质量。

    

    深度网络的排列对称性使得简单操作如模型平均和相似度估计变得困难。在许多情况下，对齐网络的权重，即找到它们之间最优排列，是必要的。更一般地说，权重对齐对于广泛的应用非常重要，从模型合并，通过探索深度神经网络的优化空间，到定义神经网络之间有意义的距离函数。不幸的是，权重对齐是一个NP-hard问题。先前的研究主要集中在解决对齐问题的松弛版本，导致方法耗时或者次优解。为了加速对齐过程并提高其质量，我们提出了一个名为Deep-Align的新框架，旨在学习解决权重对齐问题。为此，我们首先证明了权重对齐遵循两个基本对称性，然后提出了一个深度架构。

    Permutation symmetries of deep networks make simple operations like model averaging and similarity estimation challenging. In many cases, aligning the weights of the networks, i.e., finding optimal permutations between their weights, is necessary. More generally, weight alignment is essential for a wide range of applications, from model merging, through exploring the optimization landscape of deep neural networks, to defining meaningful distance functions between neural networks. Unfortunately, weight alignment is an NP-hard problem. Prior research has mainly focused on solving relaxed versions of the alignment problem, leading to either time-consuming methods or sub-optimal solutions. To accelerate the alignment process and improve its quality, we propose a novel framework aimed at learning to solve the weight alignment problem, which we name Deep-Align. To that end, we first demonstrate that weight alignment adheres to two fundamental symmetries and then, propose a deep architecture 
    
[^648]: 无限时间无折扣奖励马尔可夫决策过程自然策略梯度算法的改进样本复杂度分析

    Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes. (arXiv:2310.11677v1 [cs.LG])

    [http://arxiv.org/abs/2310.11677](http://arxiv.org/abs/2310.11677)

    本论文提出了一种加速自然策略梯度算法（ANPG），用于解决无限时间无折扣奖励马尔可夫决策过程。ANPG实现了样本和迭代复杂度的显著改进，克服了现有算法的局限性，并达到了最新的技术成果。

    

    本文考虑设计样本高效的学习算法，用于无限时间无折扣奖励马尔可夫决策过程。具体地，我们提出了加速自然策略梯度（ANPG）算法，利用加速随机梯度下降过程来获取自然策略梯度。ANPG算法在一般参数化情况下实现了O(ε^{-2})的样本复杂度和O(ε^{-1})的迭代复杂度，其中ε定义了最优性误差。这将样本复杂度提高了一个log(1/ε)的因子。ANPG是一个一阶算法，并且不需要现有文献中可能无法验证的重要性采样(IS)权重方差上界的假设。在无Hessian和无IS算法类中，ANPG超过了已知样本复杂度的一个O(ε^{-\frac{1}{2}})的因子，并同时达到了它们的最新技术成果。

    We consider the problem of designing sample efficient learning algorithms for infinite horizon discounted reward Markov Decision Process. Specifically, we propose the Accelerated Natural Policy Gradient (ANPG) algorithm that utilizes an accelerated stochastic gradient descent process to obtain the natural policy gradient. ANPG achieves $\mathcal{O}({\epsilon^{-2}})$ sample complexity and $\mathcal{O}(\epsilon^{-1})$ iteration complexity with general parameterization where $\epsilon$ defines the optimality error. This improves the state-of-the-art sample complexity by a $\log(\frac{1}{\epsilon})$ factor. ANPG is a first-order algorithm and unlike some existing literature, does not require the unverifiable assumption that the variance of importance sampling (IS) weights is upper bounded. In the class of Hessian-free and IS-free algorithms, ANPG beats the best-known sample complexity by a factor of $\mathcal{O}(\epsilon^{-\frac{1}{2}})$ and simultaneously matches their state-of-the-art it
    
[^649]: 一种仅解码器的时间序列预测基础模型

    A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])

    [http://arxiv.org/abs/2310.10688](http://arxiv.org/abs/2310.10688)

    本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。

    

    受到自然语言处理中大型语言模型的最新进展的启发，我们设计了一种用于预测的时间序列基础模型，其在各种公共数据集上的开箱即用的零样本性能接近每个个别数据集上最先进的监督预测模型的准确性。我们的模型基于在大型时间序列语料库上预训练的补丁解码器式注意力模型，并可以适用于不同的预测历史长度、预测长度和时间粒度。

    Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
    
[^650]: 从挫折中获得智慧：通过错误分析对齐大型语言模型

    Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10477](http://arxiv.org/abs/2310.10477)

    该论文介绍了一种基于错误分析的对齐策略，通过暴露大型语言模型的错误输出并进行评估，以理解内部原因。通过这种方法，有毒回应可以转化为模型对齐的指导调谐语料，从而提高模型的安全性并训练其进行自我批评。

    

    大型语言模型（LLMs）的快速发展既带来了机遇，也带来了挑战，特别是在意外生成有害和有毒回应方面。传统的对齐方法致力于引导LLMs朝着期望的性能发展并保护它们免受恶意内容的侵害，而本研究提出了一种基于错误分析的全新对齐策略，通过有意暴露LLMs的缺陷输出并进行深入评估，以完全理解内部原因，通过自然语言分析。因此，有毒回应可以转化为模型对齐的指导调谐语料，LLMs不仅可以避免生成有缺陷的回应，还可以训练其进行自我批评，发挥其辨别有毒内容的内在能力。实验结果表明，所提出的方法在安全指令遵循方面优于传统的对齐技术，同时还保持了卓越的效率。

    The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
    
[^651]: Mirage: 图分类的模型无关图蒸馏

    Mirage: Model-Agnostic Graph Distillation for Graph Classification. (arXiv:2310.09486v1 [cs.LG])

    [http://arxiv.org/abs/2310.09486](http://arxiv.org/abs/2310.09486)

    Mirage是一种模型无关的图蒸馏算法，旨在构建一个更小的合成训练集，以在资源有限的环境中实现图分类，并克服了现有算法对完整数据集的依赖性和对建模流程变化的敏感性。

    

    GNNs和其他深度学习模型一样，对数据和计算需求量很大。急需在大型数据集上扩展GNN的训练，以便在资源有限的环境中使用它们。图蒸馏是为此目的而努力，旨在从原始训练数据构建一个更小的合成训练集，而不会显著影响模型性能。虽然初步工作取得了一些进展，但这项工作基于两个关键观察：(1)现有的图蒸馏算法本身依赖于使用完整数据集进行训练，这就破坏了图蒸馏的前提。(2)蒸馏过程对目标GNN架构和超参数具有特异性，因此对建模流程的变化不具备鲁棒性。我们通过设计一种名为Mirage的图分类蒸馏算法来避免这些限制。Mirage建立在一个洞察的基础上，即一个消息传递的GNN将输入图分解为计算的多重集合。

    GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called Mirage for graph classification. Mirage is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation 
    
[^652]: 使用稀疏自编码器解释RLHF调整的语言模型中的奖励模型

    Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders. (arXiv:2310.08164v1 [cs.LG])

    [http://arxiv.org/abs/2310.08164](http://arxiv.org/abs/2310.08164)

    通过使用稀疏自编码器，我们提出了一种解释RLHF调整的语言模型中学习的奖励函数的新方法。这个方法能够通过比较自编码器隐藏空间来识别学习奖励模型准确性的独特特征，并提供了奖励完整性的抽象近似值。

    

    通过稀疏自编码器，我们提出了一种解释RLHF调整的语言模型中学习的奖励函数的新方法。我们的方法利用基本语言模型和经过RLHF调整的版本的激活来训练自编码器集合，并通过比较自编码器隐藏空间来识别反映学习奖励模型准确性的独特特征。为了量化这一点，我们构建了一个情景，调整的语言模型学习令牌-奖励映射以最大化奖励。这是首次应用稀疏自编码器来解释学习奖励和广泛检查语言模型中的奖励学习。我们的方法提供了奖励完整性的抽象近似值，这为确保指定目标和模型行为之间的一致性提供了一个有前景的技术。

    Large language models (LLMs) aligned to human preferences via reinforcement learning from human feedback (RLHF) underpin many commercial applications. However, how RLHF impacts LLM internals remains opaque. We propose a novel method to interpret learned reward functions in RLHF-tuned LLMs using sparse autoencoders. Our approach trains autoencoder sets on activations from a base LLM and its RLHF-tuned version. By comparing autoencoder hidden spaces, we identify unique features that reflect the accuracy of the learned reward model. To quantify this, we construct a scenario where the tuned LLM learns token-reward mappings to maximize reward. This is the first application of sparse autoencoders for interpreting learned rewards and broadly inspecting reward learning in LLMs. Our method provides an abstract approximation of reward integrity. This presents a promising technique for ensuring alignment between specified objectives and model behaviors.
    
[^653]: 两层神经网络中一次梯度下降的非线性特征学习理论

    A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks. (arXiv:2310.07891v1 [stat.ML])

    [http://arxiv.org/abs/2310.07891](http://arxiv.org/abs/2310.07891)

    这篇论文提出了一种关于两层神经网络中非线性特征学习的理论。通过一步梯度下降训练的过程中引入不同的多项式特征，该方法能够学习到目标函数的非线性组件，而更新的神经网络的性能则由这些特征所决定。

    

    特征学习被认为是深度神经网络成功的基本原因之一。在特定条件下已经严格证明，在两层全连接神经网络中，第一层进行一步梯度下降，然后在第二层进行岭回归可以导致特征学习；特征矩阵的谱中会出现分离的一维组件，称为“spike”。然而，使用固定梯度下降步长时，这个“spike”仅提供了目标函数的线性组件的信息，因此学习非线性组件是不可能的。我们展示了当学习率随样本大小增长时，这样的训练实际上引入了多个一维组件，每个组件对应一个特定的多项式特征。我们进一步证明了更新的神经网络的极限大维度和大样本训练和测试误差完全由这些“spike”所决定。

    Feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. It is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer followed by ridge regression on the second layer can lead to feature learning; characterized by the appearance of a separated rank-one component -- spike -- in the spectrum of the feature matrix. However, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. We show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. We further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. By 
    
[^654]: 宽深度神经网络的鲁棒过拟合的理论分析：一种NTK方法

    Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach. (arXiv:2310.06112v1 [cs.LG])

    [http://arxiv.org/abs/2310.06112](http://arxiv.org/abs/2310.06112)

    本文理论分析了宽深度神经网络的鲁棒过拟合现象，并提出了一种名为Adv-NTK的AT算法来增强神经网络的鲁棒性。

    

    对抗训练(AT)是增强深度神经网络(DNNs)鲁棒性的经典方法。然而，最近的研究实验证明它存在鲁棒过拟合现象，即长时间的AT可能对DNNs的鲁棒性产生不利影响。本文对DNNs的鲁棒过拟合提出了一个理论解释。具体而言，我们将神经切向核(NTK)理论非平凡地扩展到AT，并证明了通过AT训练的宽DNN可以很好地近似为一个线性化的DNN。此外，对于平方损失，可以推导出线性化DNN的闭式AT动力学，揭示了一种新的AT退化现象：长期的AT将导致宽DNN退化为没有AT的DNN，从而引起鲁棒过拟合。根据我们的理论结果，我们进一步设计了一种名为Adv-NTK的方法，这是第一种针对无限宽的DNNs的AT算法。在实际数据集上的实验结果表明，Adv-NTK可以帮助无限宽的DNNs提升鲁棒性。

    Adversarial training (AT) is a canonical method for enhancing the robustness of deep neural networks (DNNs). However, recent studies empirically demonstrated that it suffers from robust overfitting, i.e., a long time AT can be detrimental to the robustness of DNNs. This paper presents a theoretical explanation of robust overfitting for DNNs. Specifically, we non-trivially extend the neural tangent kernel (NTK) theory to AT and prove that an adversarially trained wide DNN can be well approximated by a linearized DNN. Moreover, for squared loss, closed-form AT dynamics for the linearized DNN can be derived, which reveals a new AT degeneration phenomenon: a long-term AT will result in a wide DNN degenerates to that obtained without AT and thus cause robust overfitting. Based on our theoretical results, we further design a method namely Adv-NTK, the first AT algorithm for infinite-width DNNs. Experiments on real-world datasets show that Adv-NTK can help infinite-width DNNs enhance comparab
    
[^655]: Entropy-MCMC: 轻松从平坦盆地进行采样

    Entropy-MCMC: Sampling from Flat Basins with Ease. (arXiv:2310.05401v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.05401](http://arxiv.org/abs/2310.05401)

    本文提出了一种Entropy-MCMC的方法，通过引入一个辅助的引导变量来在平坦盆地中进行采样，以解决深度神经网络后验分布的多模态问题，并证明了该方法的收敛性。

    

    贝叶斯深度学习依赖于对后验分布的质量估计。然而，深度神经网络的后验分布在性质上是高度多模态的，局部模式表现出不同的泛化性能。在有限的计算资源下，从原始后验分布中进行采样可能会导致次优性能，因为一些样本可能会陷入“坏”模式并出现过拟合。基于观察到低泛化误差的“好”模式通常存在于能量景观的平坦盆地中，我们提出通过偏置采样朝向这些平坦区域的后验。具体而言，我们引入了一个辅助引导变量，其稳态分布类似于平滑后验分布，并且没有尖锐的模态，以引导MCMC采样器在平坦的盆地中采样。通过将此引导变量与模型参数相结合，我们创建了一个简单的联合分布，可以在最小计算开销下实现高效采样。我们证明了我们的元算法的收敛性。

    Bayesian deep learning counts on the quality of posterior distribution estimation. However, the posterior of deep neural networks is highly multi-modal in nature, with local modes exhibiting varying generalization performance. Given a practical budget, sampling from the original posterior can lead to suboptimal performance, as some samples may become trapped in "bad" modes and suffer from overfitting. Leveraging the observation that "good" modes with low generalization error often reside in flat basins of the energy landscape, we propose to bias sampling on the posterior toward these flat regions. Specifically, we introduce an auxiliary guiding variable, the stationary distribution of which resembles a smoothed posterior free from sharp modes, to lead the MCMC sampler to flat basins. By integrating this guiding variable with the model parameter, we create a simple joint distribution that enables efficient sampling with minimal computational overhead. We prove the convergence of our met
    
[^656]: 通过区分位置和上下文来揭示Transformers中的隐藏几何

    Uncovering hidden geometry in Transformers via disentangling position and context. (arXiv:2310.04861v1 [cs.LG])

    [http://arxiv.org/abs/2310.04861](http://arxiv.org/abs/2310.04861)

    本文通过分解transformer的隐藏状态，揭示了其在语义理解中的隐含几何结构。

    

    Transformers广泛用于从输入令牌中提取复杂的语义意义，然而它们通常作为黑盒模型运行。本文提出了一种简单而信息丰富的方法，将训练好的transformer的隐藏状态（或嵌入）分解为可解释的组件。对于任何层，输入序列样本的嵌入向量由一个张量表示 $\boldsymbol{h} \in \mathbb{R}^{C \times T \times d}$。给定在序列（或上下文） $c \le C$ 的位置 $t \le T$ 处的嵌入向量 $\boldsymbol{h}_{c,t} \in \mathbb{R}^d$，提取均值效果得到分解形式 \[ \boldsymbol{h}_{c,t} = \boldsymbol{\mu} + \mathbf{pos}_t + \mathbf{ctx}_c + \mathbf{resid}_{c,t} \] 其中 $\boldsymbol{\mu}$ 是全局均值向量，$\mathbf{pos}_t$ 和 $\mathbf{ctx}_c$ 分别是跨上下文和跨位置的均值向量，$\mathbf{resid}_{c,t}$ 是残余向量。针对流行的transformer架构和多样的文本数据集，经验结果表明...

    Transformers are widely used to extract complex semantic meanings from input tokens, yet they usually operate as black-box models. In this paper, we present a simple yet informative decomposition of hidden states (or embeddings) of trained transformers into interpretable components. For any layer, embedding vectors of input sequence samples are represented by a tensor $\boldsymbol{h} \in \mathbb{R}^{C \times T \times d}$. Given embedding vector $\boldsymbol{h}_{c,t} \in \mathbb{R}^d$ at sequence position $t \le T$ in a sequence (or context) $c \le C$, extracting the mean effects yields the decomposition \[ \boldsymbol{h}_{c,t} = \boldsymbol{\mu} + \mathbf{pos}_t + \mathbf{ctx}_c + \mathbf{resid}_{c,t} \] where $\boldsymbol{\mu}$ is the global mean vector, $\mathbf{pos}_t$ and $\mathbf{ctx}_c$ are the mean vectors across contexts and across positions respectively, and $\mathbf{resid}_{c,t}$ is the residual vector. For popular transformer architectures and diverse text datasets, empirica
    
[^657]: 用单一IMU和分层机器学习模型监测年长者的奥塔哥锻炼

    Otago Exercises Monitoring for Older Adults by a Single IMU and Hierarchical Machine Learning Models. (arXiv:2310.03512v1 [cs.LG])

    [http://arxiv.org/abs/2310.03512](http://arxiv.org/abs/2310.03512)

    本研究提出了一种使用单一IMU和分层机器学习模型监测年长者奥塔哥锻炼的准确系统。利用深度学习模型判断患者是在进行OEP还是日常生活活动，可以监测OEP的参与情况。

    

    奥塔哥锻炼计划(OEP)是一种用于改善年长者脆弱、肌少症和平衡的康复计划。准确地监测患者参与OEP的情况具有挑战性，因为自我报告(日记)通常不可靠。随着可穿戴传感器的发展，利用可穿戴传感器进行人体活动识别(HAR)的系统已经改变了医疗保健。然而，它们在OEP的使用仍然表现出有限的性能。本研究旨在构建一个不显眼且准确的系统，以监测年长者参与OEP的情况。数据是从佩戴单个腰部IMU的年长者身上收集的。收集了两组数据，一组是在实验室环境中，一组是在患者家中。提出了一个分层系统，分为两个阶段：1)使用深度学习模型，利用10分钟的滑动窗口识别患者是在进行OEP还是日常生活活动(ADLs)；2)基于第一阶段，使用6秒的滑动窗口重新识别OEP与ADLs。

    Otago Exercise Program (OEP) is a rehabilitation program for older adults to improve frailty, sarcopenia, and balance. Accurate monitoring of patient involvement in OEP is challenging, as self-reports (diaries) are often unreliable. With the development of wearable sensors, Human Activity Recognition (HAR) systems using wearable sensors have revolutionized healthcare. However, their usage for OEP still shows limited performance. The objective of this study is to build an unobtrusive and accurate system to monitor OEP for older adults. Data was collected from older adults wearing a single waist-mounted Inertial Measurement Unit (IMU). Two datasets were collected, one in a laboratory setting, and one at the homes of the patients. A hierarchical system is proposed with two stages: 1) using a deep learning model to recognize whether the patients are performing OEP or activities of daily life (ADLs) using a 10-minute sliding window; 2) based on stage 1, using a 6-second sliding window to re
    
[^658]: Memoria: 用于类人顺序处理的海比安记忆体架构

    Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing. (arXiv:2310.03052v1 [cs.LG])

    [http://arxiv.org/abs/2310.03052](http://arxiv.org/abs/2310.03052)

    Memoria 是一个通用记忆网络，应用海比安理论来增强神经网络中的长期依赖。通过存储和检索信息，并使用根据海布规则变化的连接权重，Memoria 在诸如 BERT 和 GPT 之类的流行 Transformer 模型上显著改进了考虑长期依赖的能力。

    

    Transformer 在多个领域和任务中取得了成功。然而，由于其有限的容量，Transformer 很难处理长输入序列。虽然增加输入长度是一个解决方案，但无止境地增加长度是不现实的。此外，与 Transformer 不同，人类有选择性地记住和使用仅与输入相关的信息，而不是从头到尾处理所有原始数据。我们引入了 Memoria，一个应用海比安记忆形成理论的通用记忆网络，用于增强神经网络中的长期依赖。Memoria 在工作记忆、短期记忆和长期记忆的多个记忆层级上存储和检索称为 engram 的信息，使用根据海布规则变化的连接权重。通过与诸如 BERT 和 GPT 等流行的基于 Transformer 的模型进行实验，我们提出 Memoria 显著提高了在各种任务中考虑长期依赖的能力。结果

    Transformers have demonstrated their success in various domains and tasks. However, Transformers struggle with long input sequences due to their limited capacity. While one solution is to increase input length, endlessly stretching the length is unrealistic. Furthermore, humans selectively remember and use only relevant information from inputs, unlike Transformers which process all raw data from start to end. We introduce Memoria, a general memory network that applies Hebbian theory which is a major theory explaining human memory formulation to enhance long-term dependencies in neural networks. Memoria stores and retrieves information called engram at multiple memory levels of working memory, short-term memory, and long-term memory, using connection weights that change according to Hebb's rule. Through experiments with popular Transformer-based models like BERT and GPT, we present that Memoria significantly improves the ability to consider long-term dependencies in various tasks. Resul
    
[^659]: 学习温度条件下尺度标量化的GFlowNets

    Learning to Scale Logits for Temperature-Conditional GFlowNets. (arXiv:2310.02823v1 [cs.LG])

    [http://arxiv.org/abs/2310.02823](http://arxiv.org/abs/2310.02823)

    这项研究提出了一种名为LSL-GFN的新型架构设计，可以大大加速温度条件下GFlowNets的训练，从而提高GFlowNets的探索和利用能力。

    

    GFlowNets是一种概率模型，通过学习随机策略来顺序生成组合结构，例如分子图。它们的训练目标是按比例采样具有相应温度调节的对象的奖励。在GFlowNets中，温度条件下的GFlowNets代表了一系列由温度索引的策略，每个策略与相应的温度调节奖励函数相关联。温度条件下的GFlowNets的主要优势在于通过调整温度来控制对GFlowNets的探索和利用。我们提出了一种名为学习温度条件下尺度标量化的GFlowNets（LSL-GFN）的新型架构设计，它极大地加速了温度条件下GFlowNets的训练。它基于一个思想，即之前提出的温度条件方法在深度网络的训练中引入了数值挑战，因为不同的温度可能导致非常不同的情况。

    GFlowNets are probabilistic models that learn a stochastic policy that sequentially generates compositional structures, such as molecular graphs. They are trained with the objective of sampling such objects with probability proportional to the object's reward. Among GFlowNets, the temperature-conditional GFlowNets represent a family of policies indexed by temperature, and each is associated with the correspondingly tempered reward function. The major benefit of temperature-conditional GFlowNets is the controllability of GFlowNets' exploration and exploitation through adjusting temperature. We propose Learning to Scale Logits for temperature-conditional GFlowNets (LSL-GFN), a novel architectural design that greatly accelerates the training of temperature-conditional GFlowNets. It is based on the idea that previously proposed temperature-conditioning approaches introduced numerical challenges in the training of the deep network because different temperatures may give rise to very differe
    
[^660]: 通过最大化梯度多样性来解决联邦优化中的混合异构性

    Tackling Hybrid Heterogeneity on Federated Optimization via Gradient Diversity Maximization. (arXiv:2310.02702v1 [cs.LG])

    [http://arxiv.org/abs/2310.02702](http://arxiv.org/abs/2310.02702)

    本文探讨了混合异构性如何影响联邦优化，并提出了一种通过最大化梯度多样性来减轻混合异构性负面影响的方法。

    

    联邦学习是一种分布式机器学习范式，其中数据样本被分散和分布在多个客户端之间。这些样本可能表现出统计异质性，即数据分布在客户端之间不是独立和相同的。此外，系统异质性，即客户端计算能力的变化，会给联邦学习带来偏差。统计和系统异质性的综合效应可以显著降低联邦优化的效率。然而，混合异构性的影响并没有得到严谨的讨论。本文通过研究服务器端优化，探讨了混合异构性如何影响联邦优化。理论结果表明，在服务器更新方向上自适应地最大化梯度多样性可以帮助减轻混合异构性的潜在负面影响。为此，我们引入了一种新颖的基于服务器端梯度的优化器。

    Federated learning refers to a distributed machine learning paradigm in which data samples are decentralized and distributed among multiple clients. These samples may exhibit statistical heterogeneity, which refers to data distributions are not independent and identical across clients. Additionally, system heterogeneity, or variations in the computational power of the clients, introduces biases into federated learning. The combined effects of statistical and system heterogeneity can significantly reduce the efficiency of federated optimization. However, the impact of hybrid heterogeneity is not rigorously discussed. This paper explores how hybrid heterogeneity affects federated optimization by investigating server-side optimization. The theoretical results indicate that adaptively maximizing gradient diversity in server update direction can help mitigate the potential negative consequences of hybrid heterogeneity. To this end, we introduce a novel server-side gradient-based optimizer \
    
[^661]: 探索通过减少自适应无偏客户采样方差的联邦优化

    Exploring Federated Optimization by Reducing Variance of Adaptive Unbiased Client Sampling. (arXiv:2310.02698v1 [cs.LG])

    [http://arxiv.org/abs/2310.02698](http://arxiv.org/abs/2310.02698)

    本文通过减少自适应无偏客户采样方差，探索了联邦优化中的一系列自适应客户采样技术，并提出了一种名为K-Vib的新型采样器，显著提高了联邦学习性能。

    

    联邦学习系统通常对一部分客户进行采样来进行训练过程。值得注意的是，基于来自采样客户的信息建立全局模型的全局估计方差与联邦优化质量密切相关。本文探讨了一系列“免费”的自适应客户采样技术，其中服务器构建了有前途的采样概率和可靠的全局估计，而无需额外的本地通信和计算。我们捕捉了采样过程中的一个小变体，并相应改进了全局估计。在此基础上，我们提出了一种名为K-Vib的新型采样器，它解决了在联邦优化中遵循客户采样的在线凸优化问题。它在通信预算K的情况下实现了改进的线性速率上升，具有遗憾边界$\tilde{\mathcal{O}}\big(N^{\frac{1}{3}}T^{\frac{2}{3}}/K^{\frac{4}{}3}\big)$。结果是，它显著提高了联邦学习性能。

    Federated Learning (FL) systems usually sample a fraction of clients to conduct a training process. Notably, the variance of global estimates for updating the global model built on information from sampled clients is highly related to federated optimization quality. This paper explores a line of "free" adaptive client sampling techniques in federated optimization, where the server builds promising sampling probability and reliable global estimates without requiring additional local communication and computation. We capture a minor variant in the sampling procedure and improve the global estimation accordingly. Based on that, we propose a novel sampler called K-Vib, which solves an online convex optimization respecting client sampling in federated optimization. It achieves improved a linear speed up on regret bound $\tilde{\mathcal{O}}\big(N^{\frac{1}{3}}T^{\frac{2}{3}}/K^{\frac{4}{3}}\big)$ with communication budget $K$. As a result, it significantly improves the performance of federat
    
[^662]: 3D物理系统中学习对称性破缺的松弛八面体群卷积

    Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems. (arXiv:2310.02299v1 [cs.LG])

    [http://arxiv.org/abs/2310.02299](http://arxiv.org/abs/2310.02299)

    本文介绍了一种用于建模3D物理系统的松弛八面体群卷积技术，它可以在保持数据一致的最高等变性水平的同时，发现物理系统中微妙的对称性破缺因素。

    

    深度等价模型利用对称性提高样本效率和泛化性能。然而，在许多这些模型中，完美对称性的假设有时可能会限制性能，特别是当数据与这些对称性不完全一致时。因此，我们在本文中引入了用于建模3D物理系统的松弛八面体群卷积。这种灵活的卷积技术能够在保持与数据一致的最高等变性水平的同时，发现物理系统中微妙的对称性破缺因素。实证结果验证了我们的方法不仅可以揭示相变中的对称性破缺因素，还可以在流体超分辨率任务中实现卓越性能。

    Deep equivariant models use symmetries to improve sample efficiency and generalization. However, the assumption of perfect symmetry in many of these models can sometimes be restrictive, especially when the data does not perfectly align with such symmetries. Thus, we introduce relaxed octahedral group convolution for modeling 3D physical systems in this paper. This flexible convolution technique provably allows the model to both maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in the physical systems. Empirical results validate that our approach can not only provide insights into the symmetry-breaking factors in phase transitions but also achieves superior performance in fluid super-resolution tasks.
    
[^663]: DeepZero: 将零阶优化应用于深度模型训练的扩展

    DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training. (arXiv:2310.02025v1 [cs.LG])

    [http://arxiv.org/abs/2310.02025](http://arxiv.org/abs/2310.02025)

    DeepZero是一个扩展零阶优化到深度神经网络训练的深度学习框架，通过创新的坐标梯度估计和稀疏诱导的零阶训练协议，实现了高准确性和计算效率的优化。

    

    在无法获取一阶信息时，零阶优化已成为解决机器学习问题的一种常用技术。然而，零阶优化的可扩展性仍然是一个待解决的问题：其应用主要局限在相对小规模的机器学习问题上。我们开发了DeepZero，一个基于零阶优化的深度学习框架，通过三个主要创新将零阶优化扩展到从零开始的深度神经网络训练中。

    Zeroth-order (ZO) optimization has become a popular technique for solving machine learning (ML) problems when first-order (FO) information is difficult or impossible to obtain. However, the scalability of ZO optimization remains an open problem: Its use has primarily been limited to relatively small-scale ML problems, such as sample-wise adversarial attack generation. To our best knowledge, no prior work has demonstrated the effectiveness of ZO optimization in training deep neural networks (DNNs) without a significant decrease in performance. To overcome this roadblock, we develop DeepZero, a principled ZO deep learning (DL) framework that can scale ZO optimization to DNN training from scratch through three primary innovations. First, we demonstrate the advantages of coordinate-wise gradient estimation (CGE) over randomized vector-wise gradient estimation in training accuracy and computational efficiency. Second, we propose a sparsity-induced ZO training protocol that extends the model
    
[^664]: 有效且参数高效的重复使用微调模型

    Effective and Parameter-Efficient Reusing Fine-Tuned Models. (arXiv:2310.01886v1 [cs.LG])

    [http://arxiv.org/abs/2310.01886](http://arxiv.org/abs/2310.01886)

    本文提出了一种有效且参数高效的方法，可以重复使用微调模型来处理下游任务，减轻存储和服务负担，并提出了PERU-FFT方法用于重复使用全面微调模型。

    

    许多在线提供的预训练大规模模型在传递到下游任务中变得非常有效。与此同时，各种在这些预训练模型上微调的任务特定模型也可供公众使用。在实践中，由于收集任务特定数据耗时且微调大规模预训练模型计算复杂，可以重复使用任务特定微调模型来处理下游任务。然而，为每个任务使用一个模型会给存储和服务带来巨大负担。最近，有许多无需训练且参数高效的方法被提出，将多个微调的任务特定模型重复使用到一个多任务模型中。然而，与为每个任务使用微调模型相比，这些方法表现出较大的准确性差距。本文中，我们提出了参数高效方法来重复使用微调模型。针对重复使用全面微调模型，我们提出了PERU-FFT，通过将稀疏任务向量注入到一个mer模型中来实现。

    Many pre-trained large-scale models provided online have become highly effective in transferring to downstream tasks. At the same time, various task-specific models fine-tuned on these pre-trained models are available online for public use. In practice, as collecting task-specific data is labor-intensive and fine-tuning the large pre-trained models is computationally expensive, one can reuse task-specific finetuned models to deal with downstream tasks. However, using a model per task causes a heavy burden on storage and serving. Recently, many training-free and parameter-efficient methods have been proposed for reusing multiple fine-tuned task-specific models into a single multi-task model. However, these methods exhibit a large accuracy gap compared with using a fine-tuned model per task. In this paper, we propose Parameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing Fully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task vector into a mer
    
[^665]: 在规划中结合空间和时间抽象以实现更好的泛化

    Combining Spatial and Temporal Abstraction in Planning for Better Generalization. (arXiv:2310.00229v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.00229](http://arxiv.org/abs/2310.00229)

    Skipper是一个基于模型的强化学习代理，利用时空抽象来在新情境中推广学到的技能。它自动将任务分解为子任务，实现稀疏决策和对环境相关部分的专注计算。实验结果表明，Skipper在零样本泛化方面具有显著优势。

    

    受到人类有意识规划的启发，我们提出了Skipper，这是一个利用时空抽象来推广在新情境中学到的技能的基于模型的强化学习代理。它自动将给定任务分解为更小、更可管理的子任务，从而实现稀疏决策和对环境相关部分的专注计算。这依赖于从回溯中学习得到的表示为有向图的抽象代理问题的提取。我们的理论分析在适当的假设下提供了性能保证，并确定了我们的方法在哪些方面有望提供帮助。针对泛化的实验验证了Skipper在零样本泛化方面与现有最先进的分层规划方法相比的显著优势。

    Inspired by human conscious planning, we propose Skipper, a model-based reinforcement learning agent utilizing spatio-temporal abstractions to generalize learned skills in novel situations. It automatically decomposes the given task into smaller, more manageable subtasks, and hence enables sparse decision-making and focused computation on the relevant parts of the environment. This relies on the extraction of an abstracted proxy problem represented as a directed graph, in which vertices and edges are learned end-to-end from hindsight. Our theoretical analyses provide performance guarantees under appropriate assumptions and establish where our approach is expected to be helpful. Generalization-focused experiments validate Skipper's significant advantage in zero-shot generalization, compared to existing state-of-the-art hierarchical planning methods.
    
[^666]: ResBit: 基于残差位向量的离散值表示方法

    ResBit: Residual Bit Vector for Categorical Values. (arXiv:2309.17196v1 [cs.LG])

    [http://arxiv.org/abs/2309.17196](http://arxiv.org/abs/2309.17196)

    本论文提出了一种名为ResBit的残差位向量方法，用于解决在深度学习中表示离散数据维度增加和无法恢复原始类别值的问题。

    

    长期以来，独热编码向量一直广泛应用于机器学习中，作为一种简单且通用的表示离散数据的方法。然而，这种方法会导致维度随着要表示的离散数据线性增加，这在深度学习中视为空间计算复杂性的问题，而深度学习需要大量的数据。最近，基于扩散模型的高表达能力，提出了一种用位序列表示离散数据的方法，即Analog Bits。然而，由于在生成任务中要表示的类别类型数量不一定是2的幂次，导致Analog Bits能够表示的范围与类别数据的范围存在差异。如果生成了这样的值，问题就是无法恢复原始的类别值。为了解决这个问题，我们提出了残差位向量（ResBit），它是一种分层的位表示方法。

    The one-hot vector has long been widely used in machine learning as a simple and generic method for representing discrete data. However, this method increases the number of dimensions linearly with the categorical data to be represented, which is problematic from the viewpoint of spatial computational complexity in deep learning, which requires a large amount of data. Recently, Analog Bits, a method for representing discrete data as a sequence of bits, was proposed on the basis of the high expressiveness of diffusion models. However, since the number of category types to be represented in a generation task is not necessarily at a power of two, there is a discrepancy between the range that Analog Bits can represent and the range represented as category data. If such a value is generated, the problem is that the original category value cannot be restored. To address this issue, we propose Residual Bit Vector (ResBit), which is a hierarchical bit representation. Although it is a general-p
    
[^667]: 控制组合优化的连续放松

    Controlling Continuous Relaxation for Combinatorial Optimization. (arXiv:2309.16965v1 [stat.ML])

    [http://arxiv.org/abs/2309.16965](http://arxiv.org/abs/2309.16965)

    本文研究了在相对密集的图上组合优化问题中物理启发的图神经网络（PI-GNN）求解器的表现。通过数值实验，我们发现PI-GNN求解器在学习早期可能陷入所有变量为零的局部解。为了解决这个问题，我们通过控制连续性和离散性提出了一种改进方法。

    

    最近在组合优化（CO）问题中，图神经网络（GNNs）显示出巨大潜力。通过无监督学习找到近似解的受物理启发的GNN（PI-GNN）求解器在大规模CO问题上引起了极大关注。然而，对于相对密集图上的CO问题，贪婪算法的性能恶化，但对于PI-GNN求解器的性能却没有太多讨论。此外，由于PI-GNN求解器采用了放松策略，学习后需要从连续空间人工转换回原始离散空间，可能会破坏解的鲁棒性。本文通过数值实验证明了PI-GNN求解器在密集图上的CO问题的学习早期可能陷入局部解的情况，其中所有变量都为零。然后，我们通过控制连续性和离散性来解决这些问题。

    Recent advancements in combinatorial optimization (CO) problems emphasize the potential of graph neural networks (GNNs). The physics-inspired GNN (PI-GNN) solver, which finds approximate solutions through unsupervised learning, has attracted significant attention for large-scale CO problems. Nevertheless, there has been limited discussion on the performance of the PI-GNN solver for CO problems on relatively dense graphs where the performance of greedy algorithms worsens. In addition, since the PI-GNN solver employs a relaxation strategy, an artificial transformation from the continuous space back to the original discrete space is necessary after learning, potentially undermining the robustness of the solutions. This paper numerically demonstrates that the PI-GNN solver can be trapped in a local solution, where all variables are zero, in the early stage of learning for CO problems on the dense graphs. Then, we address these problems by controlling the continuity and discreteness of rela
    
[^668]: AtomSurf：蛋白质结构上的学习的表面表示

    AtomSurf : Surface Representation for Learning on Protein Structures. (arXiv:2309.16519v1 [cs.LG])

    [http://arxiv.org/abs/2309.16519](http://arxiv.org/abs/2309.16519)

    本文研究了将蛋白质作为3D网格的表面表示，并提出了一种结合图表面的协同方法，既有竞争优势，又有实际应用潜力。

    

    近期Cryo-EM和蛋白质结构预测算法的进展使得大规模蛋白质结构可获得，为基于机器学习的功能注释铺平了道路。几何深度学习领域关注创建适用于几何数据的方法。从蛋白质结构中学习的一个重要方面是将这些结构表示为几何对象（如网格、图或表面）并应用适合这种表示形式的学习方法。给定方法的性能将取决于表示和相应的学习方法。在本文中，我们研究将蛋白质表示为$\textit{3D mesh surfaces}$并将其纳入已建立的表示基准中。我们的第一个发现是，尽管有着有希望的初步结果，但仅单独表面表示似乎无法与3D网格竞争。在此基础上，我们提出了一种协同方法，将表面表示与图表面结合起来。

    Recent advancements in Cryo-EM and protein structure prediction algorithms have made large-scale protein structures accessible, paving the way for machine learning-based functional annotations.The field of geometric deep learning focuses on creating methods working on geometric data. An essential aspect of learning from protein structures is representing these structures as a geometric object (be it a grid, graph, or surface) and applying a learning method tailored to this representation. The performance of a given approach will then depend on both the representation and its corresponding learning method.  In this paper, we investigate representing proteins as $\textit{3D mesh surfaces}$ and incorporate them into an established representation benchmark. Our first finding is that despite promising preliminary results, the surface representation alone does not seem competitive with 3D grids. Building on this, we introduce a synergistic approach, combining surface representations with gra
    
[^669]: 在在线CMDPs中，无模型、遗憾最优的最佳策略识别

    Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs. (arXiv:2309.15395v1 [cs.LG])

    [http://arxiv.org/abs/2309.15395](http://arxiv.org/abs/2309.15395)

    本文提出了一种无模型的算法，名为PRI，用于在线CMDPs中的最佳策略识别问题。该算法基于CMDPs的有限随机性属性，能够以低遗憾并以高概率识别出最优策略。

    

    本文考虑了在线约束马尔科夫决策过程（CMDPs）中的最佳策略识别（BPI）问题。我们对具有低遗憾并且以高概率识别最优策略的无模型算法感兴趣。现有的在线CMDPs的无模型算法在次线性遗憾和违约时没有提供任何对最优策略的收敛保证，并且只在从以前使用的策略中随机均匀抽样时提供平均性能保证。本文提出了一种新的算法，名为PRUNING-REFINEMENT-IDENTIFICATION（PRI），基于我们发现的CMDPs的一个基本结构性质，称为有限随机性。该属性表明对于具有N约束的CMDP，存在一个最优策略，其中至多有N个随机决策。所提出的算法首先识别出在哪个步骤和哪个状态需要进行随机决策，然后对这些决策的分布进行微调。

    This paper considers the best policy identification (BPI) problem in online Constrained Markov Decision Processes (CMDPs). We are interested in algorithms that are model-free, have low regret, and identify an optimal policy with a high probability. Existing model-free algorithms for online CMDPs with sublinear regret and constraint violation do not provide any convergence guarantee to an optimal policy and provide only average performance guarantees when a policy is uniformly sampled at random from all previously used policies. In this paper, we develop a new algorithm, named Pruning-Refinement-Identification (PRI), based on a fundamental structural property of CMDPs we discover, called limited stochasticity. The property says for a CMDP with $N$ constraints, there exists an optimal policy with at most $N$ stochastic decisions.  The proposed algorithm first identifies at which step and in which state a stochastic decision has to be taken and then fine-tunes the distributions of these s
    
[^670]: 通过利用图像-文本辅助任务提高社交媒体帖子的多模态分类

    Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text Auxiliary tasks. (arXiv:2309.07794v1 [cs.CL])

    [http://arxiv.org/abs/2309.07794](http://arxiv.org/abs/2309.07794)

    本研究通过引入图像-文本辅助任务，有效地提高了社交媒体帖子的多模态分类，通过两个辅助损失函数对图像-文本表示进行调整，捕捉底层依赖关系和语义对应关系，实现了一致的改进。

    

    有效地利用社交媒体帖子中的多模态信息对情感分析、讽刺检测和仇恨言论分类等多个下游任务至关重要。然而，由于匹配的图像-文本对中存在隐藏或互补信息的独特跨模态语义，将文本和图像信息结合起来是具有挑战性的。在本研究中，我们旨在通过在微调任何预训练的多模态模型时联合使用两个辅助损失函数来直接建模这一问题。图像-文本对比（ITC）将一篇帖子的图像-文本表示更加靠近，并将其与其他帖子分离开来，捕捉底层依赖关系。图像-文本匹配（ITM）通过惩罚不相关的对来促进理解图像和文本之间的语义对应关系。我们将这些目标与五个多模态模型相结合，证明了在四个热门社交媒体数据集上的一致改进。

    Effectively leveraging multimodal information from social media posts is essential to various downstream tasks such as sentiment analysis, sarcasm detection and hate speech classification. However, combining text and image information is challenging because of the idiosyncratic cross-modal semantics with hidden or complementary information present in matching image-text pairs. In this work, we aim to directly model this by proposing the use of two auxiliary losses jointly with the main task when fine-tuning any pre-trained multimodal model. Image-Text Contrastive (ITC) brings image-text representations of a post closer together and separates them from different posts, capturing underlying dependencies. Image-Text Matching (ITM) facilitates the understanding of semantic correspondence between images and text by penalizing unrelated pairs. We combine these objectives with five multimodal models, demonstrating consistent improvements across four popular social media datasets. Furthermore,
    
[^671]: 无监督的对比一致排序与语言模型

    Unsupervised Contrast-Consistent Ranking with Language Models. (arXiv:2309.06991v1 [cs.LG])

    [http://arxiv.org/abs/2309.06991](http://arxiv.org/abs/2309.06991)

    无监督的对比一致排序与语言模型，通过训练一个受逻辑约束引导的探测模型，实现在多个语句中始终映射到对比的真-假极点的排序任务。

    

    语言模型包含基于排序的知识，并且是处理上下文排名任务的强大解决者。最近的研究关注于配对、点对和列表提示技术，以揭示语言模型的排序知识。然而，我们发现，即使在仔细校准和限制解码的情况下，基于提示的技术在产生的排序中也不总是自洽的。这促使我们探索一种受无监督探测方法Contrast-Consistent Search（CCS）启发的替代方法。这个想法是训练一个受逻辑约束引导的探测模型：模型对一个语句及其否定的表示必须在多个语句中始终映射到对比的真-假极点。我们假设类似的约束适用于所有项通过一致性对相关排序任务。

    Language models contain ranking-based knowledge and are powerful solvers of in-context ranking tasks. For instance, they may have parametric knowledge about the ordering of countries by size or may be able to rank reviews by sentiment. Recent work focuses on pairwise, pointwise, and listwise prompting techniques to elicit a language model's ranking knowledge. However, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce. This motivates us to explore an alternative approach that is inspired by an unsupervised probing method called Contrast-Consistent Search (CCS). The idea is to train a probing model guided by a logical constraint: a model's representation of a statement and its negation must be mapped to contrastive true-false poles consistently across multiple statements. We hypothesize that similar constraints apply to ranking tasks where all items are related via consistent pair
    
[^672]: 自适应扩散：通过潜在扩散模型进行样本自适应重建

    Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models. (arXiv:2309.06642v1 [eess.IV])

    [http://arxiv.org/abs/2309.06642](http://arxiv.org/abs/2309.06642)

    本文提出了一种自适应扩散方法，通过潜在扩散模型实现样本自适应重建。该方法解决了现有求解器在适应重建任务困难程度、推理时间和资源分配方面的不足。

    

    反问题在许多应用中出现，其目标是从嘈杂和可能是（非）线性的观测中恢复出一个干净的信号。重建问题的困难取决于多个因素，如原始信号的结构，退化的严重程度，重建模型的隐式偏差以及上述因素之间复杂的交互。这导致重建任务的困难在样本间存在自然的变化，这在现代技术中经常被忽视。最近，基于扩散的反问题求解器在各种重建任务中取得了新的最先进水平。然而，它们的缺点是计算复杂，难以实施。本文的关键观察是大多数现有求解器缺乏根据重建任务的困难程度自适应计算能力的能力，导致推理时间长，性能不佳且资源分配浪费。我们提出了一个新的自适应扩散方法来解决这个问题。

    Inverse problems arise in a multitude of applications, where the goal is to recover a clean signal from noisy and possibly (non)linear observations. The difficulty of a reconstruction problem depends on multiple factors, such as the structure of the ground truth signal, the severity of the degradation, the implicit bias of the reconstruction model and the complex interactions between the above factors. This results in natural sample-by-sample variation in the difficulty of a reconstruction task, which is often overlooked by contemporary techniques. Recently, diffusion-based inverse problem solvers have established new state-of-the-art in various reconstruction tasks. However, they have the drawback of being computationally prohibitive. Our key observation in this paper is that most existing solvers lack the ability to adapt their compute power to the difficulty of the reconstruction task, resulting in long inference times, subpar performance and wasteful resource allocation. We propose
    
[^673]: 专业性与广泛性：关于基础模型微调中灾难性遗忘的实证研究

    Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models. (arXiv:2309.06256v1 [cs.LG])

    [http://arxiv.org/abs/2309.06256](http://arxiv.org/abs/2309.06256)

    本研究实证了基础模型微调中的灾难性遗忘现象，微调过程中追求专业性会导致模型的广泛性损失。

    

    基础模型，包括视觉语言模型(VLMs)和大型语言模型(LLMs)，具有处理多样分布和任务的广泛性，这源于它们广泛的预训练数据集。对基础模型进行微调是提高任务性能或调整模型行为与人类期望一致的常见做法，使其获得专业性。然而，用于微调的小型数据集可能无法充分覆盖预训练过程中遇到的多样分布和任务。因此，追求微调过程中的专业性可能导致模型的广泛性损失，这与深度学习中的灾难性遗忘(Catastrophic Forgetting, CF)相关。在本研究中，我们展示了这种现象在VLMs和LLMs中的存在。例如，对像CLIP这样的VLM进行在ImageNet上的微调会导致处理多样分布的广泛性损失，对医学领域的Galactica进行微调则会导致遵循指令的能力损失。

    Foundation models, including Vision Language Models (VLMs) and Large Language Models (LLMs), possess the $generality$ to handle diverse distributions and tasks, which stems from their extensive pre-training datasets. The fine-tuning of foundation models is a common practice to enhance task performance or align the model's behavior with human expectations, allowing them to gain $speciality$. However, the small datasets used for fine-tuning may not adequately cover the diverse distributions and tasks encountered during pre-training. Consequently, the pursuit of speciality during fine-tuning can lead to a loss of {generality} in the model, which is related to catastrophic forgetting (CF) in deep learning. In this study, we demonstrate this phenomenon in both VLMs and LLMs. For instance, fine-tuning VLMs like CLIP on ImageNet results in a loss of generality in handling diverse distributions, and fine-tuning LLMs like Galactica in the medical domain leads to a loss in following instructions
    
[^674]: 数据效率、维度约简和广义对称信息瓶颈

    Data efficiency, dimensionality reduction, and the generalized symmetric information bottleneck. (arXiv:2309.05649v1 [cs.IT] CROSS LISTED)

    [http://arxiv.org/abs/2309.05649](http://arxiv.org/abs/2309.05649)

    广义对称信息瓶颈是一种同时压缩两个随机变量以保留信息的维度约简技术，相较于逐个压缩变量，它需要更少的数据来达到相同的误差。

    

    对称信息瓶颈（SIB）是一种维度约简技术，它是更常见的信息瓶颈的扩展，同时压缩两个随机变量以保留它们的压缩版本之间的信息。我们引入了广义对称信息瓶颈（GSIB），探索了不同功能形式的同时约简成本。然后，我们探索了同时压缩的数据集大小需求。我们通过推导涉及损失函数的统计波动的界限和均方根估计来实现这一点。我们表明，在典型情况下，与逐个压缩变量相比，同时的GSIB压缩在达到相同误差时需要更少的数据。我们认为这是一个更一般的原则的例子，即同时压缩比独立压缩输入变量更具数据效率。

    The Symmetric Information Bottleneck (SIB), an extension of the more familiar Information Bottleneck, is a dimensionality reduction technique that simultaneously compresses two random variables to preserve information between their compressed versions. We introduce the Generalized Symmetric Information Bottleneck (GSIB), which explores different functional forms of the cost of such simultaneous reduction. We then explore the dataset size requirements of such simultaneous compression. We do this by deriving bounds and root-mean-squared estimates of statistical fluctuations of the involved loss functions. We show that, in typical situations, the simultaneous GSIB compression requires qualitatively less data to achieve the same errors compared to compressing variables one at a time. We suggest that this is an example of a more general principle that simultaneous compression is more data efficient than independent compression of each of the input variables.
    
[^675]: 通过多元宇宙分析评估模型设计决策对算法公平性的影响：一切，无处不在，全方位评估

    Everything, Everywhere All in One Evaluation: Using Multiverse Analysis to Evaluate the Influence of Model Design Decisions on Algorithmic Fairness. (arXiv:2308.16681v1 [stat.ML])

    [http://arxiv.org/abs/2308.16681](http://arxiv.org/abs/2308.16681)

    通过多元宇宙分析评估模型设计决策对算法公平性的影响，可以揭示算法决策系统中设计决策的关键作用。

    

    全球范围内的许多系统都利用算法决策来（部分）自动化以前由人类进行的决策。当设计良好时，这些系统承诺更客观的决策，同时节省大量资源，节约人力。然而，当算法决策系统设计不良时，可能会导致对社会群体进行歧视的不公平决策。算法决策系统的下游效应在很大程度上取决于系统设计和实施过程中的决策，因为数据中的偏见可能会在建模过程中缓解或加强。许多这些设计决策是隐含进行的，不知道它们确切地如何影响最终系统。因此，明确算法决策系统设计中的决策并了解这些决策如何影响结果系统的公平性非常重要。为了研究这个问题，我们借鉴了心理学领域的见解，并引入了多元宇宙分析方法。

    A vast number of systems across the world use algorithmic decision making (ADM) to (partially) automate decisions that have previously been made by humans. When designed well, these systems promise more objective decisions while saving large amounts of resources and freeing up human time. However, when ADM systems are not designed well, they can lead to unfair decisions which discriminate against societal groups. The downstream effects of ADMs critically depend on the decisions made during the systems' design and implementation, as biases in data can be mitigated or reinforced along the modeling pipeline. Many of these design decisions are made implicitly, without knowing exactly how they will influence the final system. It is therefore important to make explicit the decisions made during the design of ADM systems and understand how these decisions affect the fairness of the resulting system.  To study this issue, we draw on insights from the field of psychology and introduce the metho
    
[^676]: Matbench Discovery - 一种用于评估机器学习晶体稳定性预测的框架

    Matbench Discovery -- An evaluation framework for machine learning crystal stability prediction. (arXiv:2308.14920v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2308.14920](http://arxiv.org/abs/2308.14920)

    Matbench Discovery是一个评估机器学习晶体稳定性预测的框架，在热力学稳定性预测方面的测试中，CHGNet表现最佳。

    

    Matbench Discovery通过模拟机器学习能源模型在高通量搜索稳定无机晶体方面的应用，解决了热力学稳定性和形成能之间的差异以及域内与域外性能之间的脱节问题。此外，我们还发布了一个Python包，以便于未来模型的提交，并提供了一个在线排行榜，进一步洞察各种性能指标之间的权衡。通过对热力学稳定性预测的测试集F1得分进行排名，我们发现CHGNet > M3GNet > MACE > ALIGNN > MEGNet > CGCNN > CGCNN+P > Wrenformer > BOWSR > Voronoi tessellation fingerprints with random forest。

    Matbench Discovery simulates the deployment of machine learning (ML) energy models in a high-throughput search for stable inorganic crystals. We address the disconnect between (i) thermodynamic stability and formation energy and (ii) in-domain vs out-of-distribution performance. Alongside this paper, we publish a Python package to aid with future model submissions and a growing online leaderboard with further insights into trade-offs between various performance metrics. To answer the question which ML methodology performs best at materials discovery, our initial release explores a variety of models including random forests, graph neural networks (GNN), one-shot predictors, iterative Bayesian optimizers and universal interatomic potentials (UIP). Ranked best-to-worst by their test set F1 score on thermodynamic stability prediction, we find CHGNet > M3GNet > MACE > ALIGNN > MEGNet > CGCNN > CGCNN+P > Wrenformer > BOWSR > Voronoi tessellation fingerprints with random forest. The top 3 mod
    
[^677]: 微调可能削弱基础模型；保留特征可能是解决方案

    Fine-tuning can cripple your foundation model; preserving features may be the solution. (arXiv:2308.13320v1 [cs.LG])

    [http://arxiv.org/abs/2308.13320](http://arxiv.org/abs/2308.13320)

    在微调过程中，基础模型可能会遗忘概念，我们提出了一种名为LDIFS的方法，用于解决这个问题，该方法在实验证明效果显著。

    

    预训练的基础模型主要由于其巨大的容量和对从互联网上爬取的大量训练数据的暴露，享有存储关于许多现实世界概念的知识的优势。这些模型通常在下游数据集上进行微调，以产生出色的最新性能。然而，我们观察到，与预训练模型相比，微调模型在与下游任务不同的任务上识别概念的能力显著降低。这显然是不可取的，因为在首次学习这些概念时，投入了大量的时间和金钱。我们将这种不可取的现象称为“概念遗忘”，通过实验证明大多数端到端微调方法都严重受到这种副作用的影响。为此，我们还提出了一个相当简单的解决方法，即设计了一种名为LDIFS的方法。

    Pre-trained foundation models, owing primarily to their enormous capacity and exposure to vast amount of training data scraped from the internet, enjoy the advantage of storing knowledge about plenty of real-world concepts. Such models are typically fine-tuned on downstream datasets to produce remarkable state-of-the-art performances. While various fine-tuning methods have been devised and are shown to be highly effective, we observe that a fine-tuned model's ability to recognize concepts on tasks $\textit{different}$ from the downstream one is reduced significantly compared to its pre-trained counterpart. This is clearly undesirable as a huge amount of time and money went into learning those very concepts in the first place. We call this undesirable phenomenon "concept forgetting" and via experiments show that most end-to-end fine-tuning approaches suffer heavily from this side effect. To this end, we also propose a rather simple fix to this problem by designing a method called LDIFS 
    
[^678]: ESG主导的DLT研究的演化：对文献进行NLP分析

    Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature. (arXiv:2308.12420v1 [cs.IR])

    [http://arxiv.org/abs/2308.12420](http://arxiv.org/abs/2308.12420)

    本研究通过NLP分析了ESG主导的DLT研究的演化，通过构建引用网络和命名实体识别任务，对DLT在ESG背景下的发展进行了文献综述。

    

    分布式账本技术(DLT)迅速发展，需要全面了解其各个组成部分。然而，针对DLT的环境、可持续性和治理(ESG)组成部分的系统文献综述还不足。为填补这一空白，我们选择了107篇种子文献，构建了一个包含63,083个参考文献的引用网络，并将其精炼为24,539篇文献的语料库进行分析。然后，我们根据一个已建立的技术分类法从46篇论文中标记了命名实体，并通过找出DLT的ESG要素来完善这个分类法。利用基于transformer的语言模型，我们对一个预先训练的语言模型进行了细化调整，用于命名实体识别任务，使用我们标记的数据集。我们利用我们调整后的语言模型对语料库进行了精简，得到了505篇关键论文，通过命名实体和时间图分析，促进了对DLT在ESG背景下的演化的文献综述。

    Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating comprehensive insights into their diverse components. However, a systematic literature review that emphasizes the Environmental, Sustainability, and Governance (ESG) components of DLT remains lacking. To bridge this gap, we selected 107 seed papers to build a citation network of 63,083 references and refined it to a corpus of 24,539 publications for analysis. Then, we labeled the named entities in 46 papers according to twelve top-level categories derived from an established technology taxonomy and enhanced the taxonomy by pinpointing DLT's ESG elements. Leveraging transformer-based language models, we fine-tuned a pre-trained language model for a Named Entity Recognition (NER) task using our labeled dataset. We used our fine-tuned language model to distill the corpus to 505 key papers, facilitating a literature review via named entities and temporal graph analysis on DLT evolution in the context of ESG. Our con
    
[^679]: 我看到的东西有多安全？基于图像控制的自治安全性预测的校准预测

    How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy. (arXiv:2308.12252v1 [cs.LG])

    [http://arxiv.org/abs/2308.12252](http://arxiv.org/abs/2308.12252)

    本文提出了一种基于生成世界模型的学习流水线族，通过克服学习安全知情表示和分布漂移下缺失安全标签的挑战，实现了在线安全预测。这些流水线具有统计校准保证的安全机会预测能力。

    

    端到端学习已经成为开发自治系统的主要范 paradigm。不幸的是，随着其性能和便利性，安全保证面临着更大的挑战。挑战的一个关键因素是缺乏低维可解释动态状态的概念，传统的保证方法都围绕这一概念展开。本文针对在线安全预测问题，提出了一种基于生成世界模型的可配置学习流水线族，不需要低维状态。为了实现这些流水线，我们克服了学习安全知情潜在表示和预测引起的分布漂移下的缺失安全标签的挑战。这些流水线基于符合性预测，对其安全机会预测提供了统计校准保证。我们对提出的学习流水线在两个图像控制系统的案例研究上进行了广泛评估：赛车和汽车。

    End-to-end learning has emerged as a major paradigm for developing autonomous systems. Unfortunately, with its performance and convenience comes an even greater challenge of safety assurance. A key factor of this challenge is the absence of the notion of a low-dimensional and interpretable dynamical state, around which traditional assurance methods revolve. Focusing on the online safety prediction problem, this paper proposes a configurable family of learning pipelines based on generative world models, which do not require low-dimensional states. To implement these pipelines, we overcome the challenges of learning safety-informed latent representations and missing safety labels under prediction-induced distribution shift. These pipelines come with statistical calibration guarantees on their safety chance predictions based on conformal prediction. We perform an extensive evaluation of the proposed learning pipelines on two case studies of image-controlled systems: a racing car and a car
    
[^680]: 使用层次结构距离捕捉多层次图结构的变压器

    Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances. (arXiv:2308.11129v1 [cs.LG])

    [http://arxiv.org/abs/2308.11129](http://arxiv.org/abs/2308.11129)

    本论文提出了一种层次距离结构编码（HDSE）方法，用于捕捉多层次图结构。经过在12个真实世界数据集上的实验，证明了该方法在10个基准数据集上实验效果达到了最先进水平。

    

    图变压器需要强大的归纳偏差来得出有意义的注意力分数。然而，当前的提议很少涉及捕捉更长距离、层次结构或社区结构的方法，而这些在分子、社交网络和引用网络等各种图形中都会出现。在本文中，我们提出了一种层次距离结构编码（HDSE）方法，用于建模图中节点之间的层次距离，重点关注其多层次、层次化的性质。特别是，这产生了一个可以灵活与现有图变压器集成的框架，可以与其他位置表示同时应用。通过在12个真实世界数据集上进行大量实验，我们证明了我们的HDSE方法成功提升了各种类型的基线变压器，在10个基准数据集上获得了最先进的实证性能。

    Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
    
[^681]: 自然语言是图表所需要的全部内容

    Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.07134](http://arxiv.org/abs/2308.07134)

    本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。

    

    大规模预训练语言模型的出现，如ChatGPT，已经在人工智能的各个研究领域中引起了革命。基于Transformer的大型语言模型（LLMs）逐渐取代了CNN和RNN，将计算机视觉和自然语言处理领域统一起来。与相对独立存在的数据（如图像、视频或文本）相比，图表是一种包含丰富结构和关系信息的数据类型。同时，作为最具表现力的媒介之一，自然语言在描述复杂结构方面表现出色。然而，将图表学习问题纳入生成式语言建模框架的现有工作仍然非常有限。随着大型语言模型的重要性不断增长，探索LLMs是否也可以替代GNNs成为图表的基础模型变得至关重要。在本文中，我们提出了InstructGLM（结构化语言模型）算法，系统地设计高度可扩展的模型来处理图表学习问题。

    The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
    
[^682]: 贝叶斯流网络

    Bayesian Flow Networks. (arXiv:2308.07037v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.07037](http://arxiv.org/abs/2308.07037)

    本文介绍了贝叶斯流网络（BFNs），一种新的生成模型，它通过贝叶斯推断修改了一组独立分布的参数，并将其作为输入传递给神经网络来生成另一个相互依赖的分布。该方法不需要前向过程，适用于连续和离散数据，并具有优化数据压缩的功能。

    

    本文介绍了贝叶斯流网络（BFNs），一种新的生成模型。在BFNs中，独立分布的参数会在嘈杂的数据样本的影响下通过贝叶斯推断进行修改，然后作为输入传递给神经网络，该神经网络输出一个相互依赖的分布。从简单的先验开始，通过迭代更新这两个分布可以得到一个类似于扩散模型反向过程的生成过程；不过，这个过程在概念上更简单，无需前向过程。对于连续、离散化和离散数据，推导出了离散和连续时间的损失函数，以及样本生成过程。值得注意的是，对于离散数据，网络的输入位于概率单纯形上，因此本质上是可微分的，为基于梯度的样本引导和在语言建模等离散领域进行少量步骤生成铺平了道路。损失函数直接优化了数据压缩，并且不放置限制。

    This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no
    
[^683]: 虚假网站：在规模上追踪和影响虚假新闻故事的传播

    Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale. (arXiv:2308.02068v1 [cs.SI])

    [http://arxiv.org/abs/2308.02068](http://arxiv.org/abs/2308.02068)

    本研究利用日常抓取的1,404个不可靠新闻网站以及大型语言模型和聚类算法，提出了一个自动分析网络生态系统中传播的叙述的系统。通过识别55,301个叙述，描述了2022年传播最广泛的叙述，并确定了最具影响力的起源和放大叙述的网站。该系统可用于检测来自不可靠新闻网站的新叙述，并帮助事实核查组织更快地应对错误信息。

    

    虚假信息、宣传和彻头彻尾的谎言在网络上大量传播，其中一些叙述对公共健康、选举和个人安全产生危险的现实影响。然而，尽管虚假信息的影响，研究界在追踪在线平台上的新闻叙述方面主要缺乏自动化和程序化的方法。在这项研究中，利用对1,404个不可靠新闻网站的日常抓取、大型语言模型MPNet和DP-Means聚类，我们介绍了一个系统来自动分离和分析在线生态系统中传播的叙述。我们在这些1,404个网站上识别了55,301个叙述，描述了2022年传播最广泛的叙述，并确定了起源和放大叙述的最具影响力的网站。最后，我们展示了如何利用我们的系统来检测源自不可靠新闻网站的新叙述，并帮助Politifact、路透社和美联社等事实核查组织更快地应对错误信息。

    Misinformation, propaganda, and outright lies proliferate on the web, with some narratives having dangerous real-world consequences on public health, elections, and individual safety. However, despite the impact of misinformation, the research community largely lacks automated and programmatic approaches for tracking news narratives across online platforms. In this work, utilizing daily scrapes of 1,404 unreliable news websites, the large-language model MPNet, and DP-Means clustering, we introduce a system to automatically isolate and analyze the narratives spread within online ecosystems. Identifying 55,301 narratives on these 1,404 websites, we describe the most prevalent narratives spread in 2022 and identify the most influential websites that originate and magnify narratives. Finally, we show how our system can be utilized to detect new narratives originating from unreliable news websites and aid fact-checkers like Politifact, Reuters, and AP News in more quickly addressing misinfo
    
[^684]: 深度学习中的校准：最新研究综述

    Calibration in Deep Learning: A Survey of the State-of-the-Art. (arXiv:2308.01222v1 [cs.LG])

    [http://arxiv.org/abs/2308.01222](http://arxiv.org/abs/2308.01222)

    本文回顾了深度学习中的校准方法的最新发展，并提供了对其原理的理解。研究表明，现代深度神经网络在预测能力上表现出色，但校准性较差，导致模型预测不可靠。因此，需要一些新的方法来改善模型的校准性。

    

    在构建可靠、鲁棒的安全关键应用的人工智能系统中，深度神经模型的校准起着重要作用。最近的研究表明，具有高预测能力的现代神经网络的校准性较差，产生不可靠的模型预测。尽管深度学习模型在各种基准测试中取得了显著的性能，但对模型的校准性和可靠性的研究相对较少。理想的深度模型不仅应具有高预测性能，还应具有良好的校准性。最近提出了一些使用不同机制进行深度模型校准的方法。在本综述中，我们回顾了最新的校准方法，并解释了它们执行模型校准的原理。首先，我们从模型校准的定义开始，解释了模型校准不准确的根本原因。然后，我们介绍了可以衡量模型校准性的关键指标。接下来，我们总结了一些校准方法的方法和实践。

    Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibrat
    
[^685]: 基于差分进化算法的Transformer神经网络模型用于负荷预测的超参数选择

    Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting. (arXiv:2307.15299v1 [cs.NE])

    [http://arxiv.org/abs/2307.15299](http://arxiv.org/abs/2307.15299)

    本研究使用差分进化算法选择Transformer神经网络模型的优化超参数，以提高负荷预测的准确性。

    

    精确的负荷预测在众多领域都起着重要作用，但准确捕捉动力系统的复杂动态仍然是传统统计模型面临的挑战。因此，时间序列模型（ARIMA）和深度学习模型（ANN，LSTM，GRU等）经常被使用，并且通常能够取得更好的成功率。本文分析了最近开发的Transformer-based神经网络模型在负荷预测中的效果。Transformer模型有望改进负荷预测，因为它们能够通过其Attention机制学习到长期依赖关系。我们运用了几种元启发式算法，如差分进化，以寻找Transformer-based神经网络的最优超参数，以产生精确的预测。差分进化为非可微分、多目标或约束优化问题提供了可扩展、强健和全局的解决方案。我们的工作比较了所提出的基于Transformer的神经网络与其他模型在负荷预测上的性能。

    Accurate load forecasting plays a vital role in numerous sectors, but accurately capturing the complex dynamics of dynamic power systems remains a challenge for traditional statistical models. For these reasons, time-series models (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonly deployed and often experience higher success. In this paper, we analyze the efficacy of the recently developed Transformer-based Neural Network model in Load forecasting. Transformer models have the potential to improve Load forecasting because of their ability to learn long-range dependencies derived from their Attention Mechanism. We apply several metaheuristics namely Differential Evolution to find the optimal hyperparameters of the Transformer-based Neural Network to produce accurate forecasts. Differential Evolution provides scalable, robust, global solutions to non-differentiable, multi-objective, or constrained optimization problems. Our work compares the proposed Transformer based Ne
    
[^686]: 将路径相关的NJ-ODE扩展到有噪声的观测和相关观测框架

    Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework. (arXiv:2307.13147v1 [stat.ML])

    [http://arxiv.org/abs/2307.13147](http://arxiv.org/abs/2307.13147)

    该论文研究了将路径相关的NJ-ODE方法扩展到具有噪声观测和相关观测框架的问题。研究提出了两种扩展方法，并提供了理论保证和实证示例。

    

    路径相关的神经跳跃ODE (PD-NJ-ODE) 是一种用于预测具有不规则和不完整观测的连续时间随机过程的模型。具体而言，该方法通过学习给定不规则采样的不完整过去观测的最优预测。迄今为止，假设过程本身和坐标分别观测时间是独立的，并且假设观测是无噪声的。在这项工作中，我们讨论了两种扩展来解除这些限制，并提供了理论保证以及它们的实证示例。

    The Path-Dependent Neural Jump ODE (PD-NJ-ODE) is a model for predicting continuous-time stochastic processes with irregular and incomplete observations. In particular, the method learns optimal forecasts given irregularly sampled time series of incomplete past observations. So far the process itself and the coordinate-wise observation times were assumed to be independent and observations were assumed to be noiseless. In this work we discuss two extensions to lift these restrictions and provide theoretical guarantees as well as empirical examples for them.
    
[^687]: 大数据-供应链管理框架的预测：数据预处理和机器学习技术

    Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques. (arXiv:2307.12971v1 [cs.LG])

    [http://arxiv.org/abs/2307.12971](http://arxiv.org/abs/2307.12971)

    本文介绍了一种新的大数据-供应链管理框架，通过数据预处理和机器学习技术实现供应链预测，优化操作管理、透明度，并讨论了幻影库存对预测的不利影响。

    

    本文旨在系统地识别和比较分析最先进的供应链预测策略和技术。提出了一个新的框架，将大数据分析应用于供应链管理中，包括问题识别、数据来源、探索性数据分析、机器学习模型训练、超参数调优、性能评估和优化，以及预测对人力、库存和整个供应链的影响。首先讨论了根据供应链策略收集数据的需求以及如何收集数据。文章讨论了根据周期或供应链目标需要不同类型的预测。推荐使用供应链绩效指标和误差测量系统来优化表现最佳的模型。还讨论了幻影库存对预测的不利影响以及管理决策依赖供应链绩效指标来确定模型性能参数和改进运营管理、透明度的问题。

    This article intends to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies. A novel framework has been proposed incorporating Big Data Analytics in SC Management (problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization), forecasting effects on human-workforce, inventory, and overall SC. Initially, the need to collect data according to SC strategy and how to collect them has been discussed. The article discusses the need for different types of forecasting according to the period or SC objective. The SC KPIs and the error-measurement systems have been recommended to optimize the top-performing model. The adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and 
    
[^688]: 全球降水即时预测：基于GPM的集成多卫星检索的U-Net卷积LSTM架构

    Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture. (arXiv:2307.10843v1 [cs.LG])

    [http://arxiv.org/abs/2307.10843](http://arxiv.org/abs/2307.10843)

    本文提出了一种基于U-Net和卷积LSTM的深度学习架构，能够准确预测全球降水的即时情况，尤其在极端降水方面的预测准确率更高。

    

    本文提出了一种深度学习架构，用于每30分钟全球近4小时的降水即时预测。该架构融合了U-Net和卷积长短期记忆(LSTM)神经网络，并使用来自集成多卫星检索(GPM)和全球预测系统(GFS)的关键降水驱动数据进行训练。研究了不同训练损失函数（包括均方差回归和聚焦损失分类）对降水现在预测质量的影响。结果表明，回归网络在捕捉轻度降水（小于1.6 mm/hr）方面表现良好，但分类网络在极端降水（大于8 mm/hr）的预测方面，以关键成功指数(CSI)衡量，可以胜过回归网络。使用Wasserstein距离表明，分类网络预测的降水具有更接近的类别概率。

    This paper presents a deep learning architecture for nowcasting of precipitation almost globally every 30 min with a 4-hour lead time. The architecture fuses a U-Net and a convolutional long short-term memory (LSTM) neural network and is trained using data from the Integrated MultisatellitE Retrievals for GPM (IMERG) and a few key precipitation drivers from the Global Forecast System (GFS). The impacts of different training loss functions, including the mean-squared error (regression) and the focal-loss (classification), on the quality of precipitation nowcasts are studied. The results indicate that the regression network performs well in capturing light precipitation (below 1.6 mm/hr), but the classification network can outperform the regression network for nowcasting of precipitation extremes (>8 mm/hr), in terms of the critical success index (CSI).. Using the Wasserstein distance, it is shown that the predicted precipitation by the classification network has a closer class probabili
    
[^689]: HAT-CL: 用于连续学习的基于任务的硬注意力PyTorch库

    HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning. (arXiv:2307.09653v1 [cs.LG])

    [http://arxiv.org/abs/2307.09653](http://arxiv.org/abs/2307.09653)

    HAT-CL是一个基于任务的硬注意力PyTorch库，以提供对连续学习中的灾难性遗忘现象的解决方案。它通过改善HAT的可用性和兼容性问题，并提供对现有网络复用的支持，实现了对PyTorch模块的自动化梯度操作和转换。此外，HAT-CL还引入了新颖的掩码操作技术。

    

    连续学习中的灾难性遗忘现象，即神经网络在学习新任务时丧失先前获得的知识，给人们带来了重大挑战。硬注意力任务(HAT)机制在减轻这个问题方面已经显示出潜力，但其实际实现受到了可用性和兼容性问题的影响，并且缺乏对现有网络复用的支持。在本文中，我们介绍了HAT-CL，这是HAT机制的用户友好、与PyTorch兼容的重新设计。HAT-CL不仅自动化了梯度操作，还简化了PyTorch模块转化为HAT模块的过程。它通过提供一套全面的模块，可以无缝地集成到现有的架构中。此外，HAT-CL还提供了与TIMM库平滑集成的可用的HAT网络。除了对HAT的重新设计和重新实现之外，我们还介绍了用于HAT的新颖的掩码操作技术。

    Catastrophic forgetting, the phenomenon in which a neural network loses previously obtained knowledge during the learning of new tasks, poses a significant challenge in continual learning. The Hard-Attention-to-the-Task (HAT) mechanism has shown potential in mitigating this problem, but its practical implementation has been complicated by issues of usability and compatibility, and a lack of support for existing network reuse. In this paper, we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT mechanism. HAT-CL not only automates gradient manipulation but also streamlines the transformation of PyTorch modules into HAT modules. It achieves this by providing a comprehensive suite of modules that can be seamlessly integrated into existing architectures. Additionally, HAT-CL offers ready-to-use HAT networks that are smoothly integrated with the TIMM library. Beyond the redesign and reimplementation of HAT, we also introduce novel mask manipulation techniques for HAT,
    
[^690]: 高失真条件下单位范数向量的最优压缩

    Optimal Compression of Unit Norm Vectors in the High Distortion Regime. (arXiv:2307.07941v1 [cs.IT])

    [http://arxiv.org/abs/2307.07941](http://arxiv.org/abs/2307.07941)

    本研究探讨了在高失真情况下，将单位范数向量压缩到最少比特数的最优方法。研究发现，简单的压缩方案在这种情况下几乎是最优的。

    

    受到通信高效分布式学习的需求的驱动，我们研究了将单位范数向量压缩到最少比特数的方法，同时允许一定程度的失真恢复。这个问题在速率-失真/覆盖编码文献中已经被研究过，但我们的重点仅限于“高失真”情况。我们在最坏情况下考虑了这个问题，没有任何关于向量的先验信息，但允许使用随机压缩映射。我们研究了有偏和无偏压缩方法，并确定了最优压缩比率。结果表明，简单的压缩方案在这种情况下几乎是最优的。虽然结果是新旧问题的混合，但为了完整起见，它们在本文中予以整理。

    Motivated by the need for communication-efficient distributed learning, we investigate the method for compressing a unit norm vector into the minimum number of bits, while still allowing for some acceptable level of distortion in recovery. This problem has been explored in the rate-distortion/covering code literature, but our focus is exclusively on the "high-distortion" regime. We approach this problem in a worst-case scenario, without any prior information on the vector, but allowing for the use of randomized compression maps. Our study considers both biased and unbiased compression methods and determines the optimal compression rates. It turns out that simple compression schemes are nearly optimal in this scenario. While the results are a mix of new and known, they are compiled in this paper for completeness.
    
[^691]: 通过填充和置换指纹编码的方法，对差分隐私算法提供平滑下界

    Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes. (arXiv:2307.07604v1 [cs.CR])

    [http://arxiv.org/abs/2307.07604](http://arxiv.org/abs/2307.07604)

    本论文提出了一种通过填充和置换指纹编码的方法来产生困难实例，从而在各种情景下提供平滑下界。这方法适用于差分隐私平均问题和近似k.

    

    指纹编码方法是最广泛用于确定约束差分隐私算法的样本复杂度或错误率的方法。然而，对于许多差分隐私问题，我们并不知道适当的下界，并且即使对于我们知道的问题，下界也不平滑，并且通常在误差大于某个阈值时变得无意义。在这项工作中，我们通过将填充和置换转换应用于指纹编码，提出了一种生成困难实例的简单方法。我们通过在不同情景下提供新的下界来说明这种方法的适用性：1. 低准确度情景下差分隐私平均问题的紧密下界，这尤其意味着新的私有1簇问题的下界 2. 近似k

    Fingerprinting arguments, first introduced by Bun, Ullman, and Vadhan (STOC 2014), are the most widely used method for establishing lower bounds on the sample complexity or error of approximately differentially private (DP) algorithms. Still, there are many problems in differential privacy for which we don't know suitable lower bounds, and even for problems that we do, the lower bounds are not smooth, and usually become vacuous when the error is larger than some threshold.  In this work, we present a simple method to generate hard instances by applying a padding-and-permuting transformation to a fingerprinting code. We illustrate the applicability of this method by providing new lower bounds in various settings:  1. A tight lower bound for DP averaging in the low-accuracy regime, which in particular implies a new lower bound for the private 1-cluster problem introduced by Nissim, Stemmer, and Vadhan (PODS 2016).  2. A lower bound on the additive error of DP algorithms for approximate k
    
[^692]: DRAGON: 一种基于对话的带有视觉语言关联的辅助导航机器人

    DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])

    [http://arxiv.org/abs/2307.06924](http://arxiv.org/abs/2307.06924)

    DRAGON是一种基于对话的导航机器人，能够理解用户的指令并通过语言与用户沟通，为视力受损者提供导航和环境描述的帮助。

    

    视力受损者在理解和导航周围空间方面存在困难。目前的导航技术要么只关注导航，要么提供有限的关于环境的沟通。受到最近在视觉语言关联和语义导航方面的进展的启发，我们提出了DRAGON，一种由对话系统驱动的导航机器人，并具有将环境与自然语言关联的能力。通过理解用户的指令，DRAGON能够引导用户到地图上的目标地标，描述环境，并通过视觉观察回答问题。通过有效利用对话，机器人可以将用户的自由形式描述与环境中的地标关联起来，并通过口语提供语义信息给用户。我们在日常室内环境中进行了盲目参与者的用户研究。我们的结果表明，DRAGON能够与用户顺畅地沟通，

    Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, pr
    
[^693]: 经过长距离步骤的梯度下降的可证明更快收敛速度

    Provably Faster Gradient Descent via Long Steps. (arXiv:2307.06324v1 [math.OC])

    [http://arxiv.org/abs/2307.06324](http://arxiv.org/abs/2307.06324)

    本研究通过计算机辅助分析技术，证明了非常数步长策略下的梯度下降方法经过长距离步骤可以实现更快的收敛速度。

    

    本研究通过计算机辅助分析技术，建立了经过长距离步骤的梯度下降的可证明更快收敛速度。我们的理论允许非常数步长策略，通过分析多次迭代的整体效果而不是典型的一次迭代归纳使用的，从而有可能破坏下降。我们表明，长距离步骤，可能在短期内增加目标值，但在长期内带来更快的收敛速度。此外，我们还提出了一个关于梯度下降更快收敛速度的猜想，并进行了简单的数值验证。

    This work establishes provably faster convergence rates for gradient descent via a computer-assisted analysis technique. Our theory allows nonconstant stepsize policies with frequent long steps potentially violating descent by analyzing the overall effect of many iterations at once rather than the typical one-iteration inductions used in most first-order method analyses. We show that long steps, which may increase the objective value in the short term, lead to provably faster convergence in the long term. A conjecture towards proving a faster $O(1/T\log T)$ rate for gradient descent is also motivated along with simple numerical validation.
    
[^694]: SITTA: 一种用于图像描述的语义图像文本对齐方法

    SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])

    [http://arxiv.org/abs/2307.05591](http://arxiv.org/abs/2307.05591)

    SITTA是一种用于图像描述的语义图像文本对齐方法，通过构建线性映射成功地将多模态模型和语言模型的嵌入空间对齐，实现了丰富的语言能力和良好的图像-语言映射。

    

    对图像的文本和语义理解对于生成适当的描述非常重要。这需要检测图像中的对象，建模它们之间的关系，评估场景的语义，并将提取的知识表示在语言空间中。为了在保证良好的图像-语言映射的同时实现丰富的语言能力，预训练的语言模型（LMs）被条件化为预训练的多模态（图像-文本）模型，允许使用图像输入。这要求将多模态模型的视觉编码器中检测到的语义与生成性LM的语言表示进行对齐。然而，如何最好地将视觉编码器检测到的语义传递给LM还不清楚。我们介绍了两种构建线性映射的新方法，成功地将两个预训练模型的嵌入空间之间的语义转移。第一种方法是将多模态语言编码器的嵌入空间与生成性LM的嵌入空间进行对齐。

    Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embe
    
[^695]: 快速经验场景

    Fast Empirical Scenarios. (arXiv:2307.03927v1 [stat.ML])

    [http://arxiv.org/abs/2307.03927](http://arxiv.org/abs/2307.03927)

    该论文提出了两种快速的经验场景提取算法，一种识别之前未观察到的场景并提供场景的协方差矩阵表示，另一种从已实现的世界状态中选择重要的数据点，并与高阶样本矩一致，这些算法计算效率高且适用于一致的基于场景的建模和高维数值积分。

    

    我们希望从大型和高维面板数据中提取一小部分与样本矩一致的代表性场景。在两种新算法中，第一种算法识别之前未观察到的场景，并提供了一种基于场景的协方差矩阵表示。第二种算法从已实现的世界状态中选择重要的数据点，并与高阶样本矩信息一致。这两种算法计算效率高，并可用于一致的基于场景的建模和高维数值积分。广泛的数值基准测试研究和在投资组合优化中的应用支持所提出的算法。

    We seek to extract a small number of representative scenarios from large and high-dimensional panel data that are consistent with sample moments. Among two novel algorithms, the first identifies scenarios that have not been observed before, and comes with a scenario-based representation of covariance matrices. The second proposal picks important data points from states of the world that have already realized, and are consistent with higher-order sample moment information. Both algorithms are efficient to compute, and lend themselves to consistent scenario-based modeling and high-dimensional numerical integration. Extensive numerical benchmarking studies and an application in portfolio optimization favor the proposed algorithms.
    
[^696]: 如何检测文本到图像扩散模型中的未授权数据使用

    How to Detect Unauthorized Data Usages in Text-to-image Diffusion Models. (arXiv:2307.03108v1 [cs.CV])

    [http://arxiv.org/abs/2307.03108](http://arxiv.org/abs/2307.03108)

    本文提出了一种方法，通过在训练的文本到图像扩散模型中植入注入的记忆化内容，来检测未授权数据使用。该方法修改了受保护的图像数据集，添加了对人眼不可察觉但模型可以捕捉和记忆的内容，通过分析模型对注入内容的记忆来判断模型是否存在生成类似图像的能力。

    

    最近的文本到图像扩散模型在生成高质量图像方面表现出令人惊讶的性能。然而，对于训练过程中的未授权数据使用引起了关注。一个例子是当模型训练者收集了一个特定艺术家创建的一系列图像，并试图训练一个能够生成类似图像的模型，而没有获得艺术家的许可。为了解决这个问题，我们提出了一种方法，通过将注入的记忆化内容植入保护数据集上训练的文本到图像扩散模型中，来检测此类未授权数据使用。具体地，我们通过在图像上添加独特的内容，例如对人类视觉不可察觉但能够被扩散模型捕捉和记忆的隐秘图像包装函数，来修改受保护的图像数据集。通过分析模型是否对注入的内容进行记忆化，我们可以判断模型是否存在这一记忆（即是否存在生成类似图像的能力）。

    Recent text-to-image diffusion models have shown surprising performance in generating high-quality images. However, concerns have arisen regarding the unauthorized usage of data during the training process. One example is when a model trainer collects a set of images created by a particular artist and attempts to train a model capable of generating similar images without obtaining permission from the artist. To address this issue, it becomes crucial to detect unauthorized data usage. In this paper, we propose a method for detecting such unauthorized data usage by planting injected memorization into the text-to-image diffusion models trained on the protected dataset. Specifically, we modify the protected image dataset by adding unique contents on the images such as stealthy image wrapping functions that are imperceptible to human vision but can be captured and memorized by diffusion models. By analyzing whether the model has memorization for the injected content (i.e., whether the gener
    
[^697]: 使用基于图形平滑的Gibbs采样优化蛋白质适应性。

    Optimizing protein fitness using Gibbs sampling with Graph-based Smoothing. (arXiv:2307.00494v1 [q-bio.BM])

    [http://arxiv.org/abs/2307.00494](http://arxiv.org/abs/2307.00494)

    使用基于图形平滑的Gibbs采样方法（GGS）优化蛋白质适应性，消除了突变距离的限制，同时提高了搜索效率。该方法在发现高适应性蛋白质方面达到了最先进水平。

    

    能够设计出在给定任务上具有更高适应性的新型蛋白质对许多医学领域来说都是革命性的。然而，通过穷举搜索海量序列空间是不可行的。以前的方法将搜索限制在从参考序列的小突变半径范围内，但这样的启发式方法极大地限制了设计空间。我们的工作旨在消除突变距离的限制，同时实现高效的探索。我们提出了基于图形平滑的Gibbs采样（GGS），它通过迭代应用带有梯度的Gibbs来提出有利的突变，并使用基于图形平滑的方法去除导致假阳性的噪声梯度。我们的方法在训练集中发现了高适应性蛋白质，最多具有8个突变。我们通过研究GFP和AAV设计问题、消融试验和基准模型来阐明结果。

    The ability to design novel proteins with higher fitness on a given task would be revolutionary for many fields of medicine. However, brute-force search through the combinatorially large space of sequences is infeasible. Prior methods constrain search to a small mutational radius from a reference sequence, but such heuristics drastically limit the design space. Our work seeks to remove the restriction on mutational distance while enabling efficient exploration. We propose Gibbs sampling with Graph-based Smoothing (GGS) which iteratively applies Gibbs with gradients to propose advantageous mutations using graph-based smoothing to remove noisy gradients that lead to false positives. Our method is state-of-the-art in discovering high-fitness proteins with up to 8 mutations from the training set. We study the GFP and AAV design problems, ablations, and baselines to elucidate the results. Code: https://github.com/kirjner/GGS
    
[^698]: 通过重新加权优化轨迹增强对抗训练

    Enhancing Adversarial Training via Reweighting Optimization Trajectory. (arXiv:2306.14275v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14275](http://arxiv.org/abs/2306.14275)

    本文提出了一种名为“加权优化轨迹（WOT）”的新方法，通过优化历史轨迹，解决了对抗训练中的鲁棒泛化问题。

    

    尽管对抗训练已成为提高深度神经网络鲁棒性的事实上的方法，但众所周知，简单的对抗训练遭受了令人畏缩的鲁棒过拟合问题，导致鲁棒泛化效果不佳。近年来已经提出了一些方法来解决这些缺点，如额外的规范化、对抗权重扰动和更多数据训练。然而，鲁棒泛化的改进仍然远不理想。在本文中，我们从全新的角度解决这一挑战--优化历史轨迹的精细化。我们提出了一种名为“加权优化轨迹（WOT）”的新方法，利用对抗训练的优化轨迹在时间上的特点。我们进行了大量实验证明了WOT在各种最新对抗攻击下的有效性。结果显示，WOT与现有方法完美融合。

    Despite the fact that adversarial training has become the de facto method for improving the robustness of deep neural networks, it is well-known that vanilla adversarial training suffers from daunting robust overfitting, resulting in unsatisfactory robust generalization. A number of approaches have been proposed to address these drawbacks such as extra regularization, adversarial weights perturbation, and training with more data over the last few years. However, the robust generalization improvement is yet far from satisfactory. In this paper, we approach this challenge with a brand new perspective -- refining historical optimization trajectories. We propose a new method named \textbf{Weighted Optimization Trajectories (WOT)} that leverages the optimization trajectories of adversarial training in time. We have conducted extensive experiments to demonstrate the effectiveness of WOT under various state-of-the-art adversarial attacks. Our results show that WOT integrates seamlessly with t
    
[^699]: TrustGuard: 基于GNN的动态支持鲁棒且可解释的信任评估

    TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG])

    [http://arxiv.org/abs/2306.13339](http://arxiv.org/abs/2306.13339)

    TrustGuard是一种基于GNN的信任评估模型，支持信任动态性，抗击鲁棒并提供解释能力，它的实验结果在准确性、鲁棒性和可解释性方面都优于其他方法。

    

    信任评估评估实体之间的信任关系并促进决策。机器学习由于其学习能力而表现出巨大的潜力，因此对信任评估具有重要意义。近年来，作为一种新的机器学习范 paradigm，图神经网络（GNN）在处理图形数据方面表现出优越性。这激发了研究人员探索将其用于信任评估，因为实体之间的信任关系可以建模为图形。但是，使用GNN的当前信任评估方法未能完全满足信任的动态性，忽略了攻击对信任评估的不利影响，并且无法提供令人信服的评估结果解释。为解决这些问题，在本文中，我们提出了TrustGuard ：一种支持信任动态性、抗击鲁棒且通过可视化提供解释的精确信任评估模型。具体而言，TrustGuard 设计了一个由动态感知节点嵌入层、图卷积层、注意机制层和信任预测层组成的分层架构。为了评估提出的模型的有效性，我们对真实数据集进行了实验，并将TrustGuard与其他最先进的方法进行了比较。实验结果表明，TrustGuard 在准确性、鲁棒性和可解释性方面均优于其他方法。

    Trust evaluation assesses trust relationships between entities and facilitates decision-making. Machine Learning (ML) shows great potential for trust evaluation owing to its learning capabilities. In recent years, Graph Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in dealing with graph data. This has motivated researchers to explore their use in trust evaluation, as trust relationships among entities can be modeled as a graph. However, current trust evaluation methods that employ GNNs fail to fully satisfy the dynamicity nature of trust, overlook the adverse effects of attacks on trust evaluation, and cannot provide convincing explanations on evaluation results. To address these problems, in this paper, we propose TrustGuard, a GNN-based accurate trust evaluation model that supports trust dynamicity, is robust against typical attacks, and provides explanations through visualization. Specifically, TrustGuard is designed with a layered architecture that con
    
[^700]: 走向量子联邦学习

    Towards Quantum Federated Learning. (arXiv:2306.09912v1 [cs.LG])

    [http://arxiv.org/abs/2306.09912](http://arxiv.org/abs/2306.09912)

    量子联邦学习通过将量子计算和联邦学习原理相结合，旨在利用量子技术增强学习过程中的隐私、安全和效率，并通过独特的分类法分类总结了这一快速发展领域的技术特点和未来研究方向。

    

    量子联邦学习是一个新兴的交叉学科领域，将量子计算和联邦学习的原理相结合，旨在利用量子技术增强学习过程中的隐私、安全和效率。目前尚无关于该交叉学科领域的全面调查。本文对量子联邦学习进行了全面细致的探讨。我们旨在提供对量子联邦学习的原理、技术以及新兴应用的全面理解。我们讨论了这一快速发展领域的现状，确定了整合这些技术所面临的挑战和机遇，并概述了未来的方向和开放性研究问题。我们提出了一种独特的分类法，将量子联邦学习技术按其特征和所采用的量子技术分类。随着量子联邦学习领域的不断发展，我们可以预计将在各个行业实现更多的突破和应用。

    Quantum Federated Learning (QFL) is an emerging interdisciplinary field that merges the principles of Quantum Computing (QC) and Federated Learning (FL), with the goal of leveraging quantum technologies to enhance privacy, security, and efficiency in the learning process. Currently, there is no comprehensive survey for this interdisciplinary field. This review offers a thorough, holistic examination of QFL. We aim to provide a comprehensive understanding of the principles, techniques, and emerging applications of QFL. We discuss the current state of research in this rapidly evolving field, identify challenges and opportunities associated with integrating these technologies, and outline future directions and open research questions. We propose a unique taxonomy of QFL techniques, categorized according to their characteristics and the quantum techniques employed. As the field of QFL continues to progress, we can anticipate further breakthroughs and applications across various industries,
    
[^701]: 具有理论保障的差分隐私域自适应算法

    Differentially Private Domain Adaptation with Theoretical Guarantees. (arXiv:2306.08838v1 [cs.LG])

    [http://arxiv.org/abs/2306.08838](http://arxiv.org/abs/2306.08838)

    该论文提出了两种具有差分隐私保障的自适应算法，用于在受隐私约束且有限标记数据条件下，从公开源领域到目标领域进行监督域自适应。该算法能够解决一般的优化问题，并具有有利的理论学习保证。

    

    在许多应用中，学习者可用的标记数据受到隐私约束并相对有限。为了为目标领域导出更准确的预测器，通常有利于利用来自与目标领域相近的另一领域的公开标记数据。这是从公共源领域到私有目标领域的现代监督域自适应问题。我们提出了两种 $(\epsilon, \delta)$-差分隐私自适应算法，用于监督性自适应。对于其我们使用了一般的优化问题，该优化问题最近被证明具有有利的理论学习保证。我们的第一个算法是为具有线性预测器的回归设计的，并显示为解决凸优化问题。我们的第二个算法是一种更一般的解决方案，用于可能是非凸但Lipschitz和平滑的损失函数。虽然我们的主要目标是进行理论分析，但我们也报告了几个实验的结果。

    In many applications, the labeled data at the learner's disposal is subject to privacy constraints and is relatively limited. To derive a more accurate predictor for the target domain, it is often beneficial to leverage publicly available labeled data from an alternative domain, somewhat close to the target domain. This is the modern problem of supervised domain adaptation from a public source to a private target domain. We present two $(\epsilon, \delta)$-differentially private adaptation algorithms for supervised adaptation, for which we make use of a general optimization problem, recently shown to benefit from favorable theoretical learning guarantees. Our first algorithm is designed for regression with linear predictors and shown to solve a convex optimization problem. Our second algorithm is a more general solution for loss functions that may be non-convex but Lipschitz and smooth. While our main objective is a theoretical analysis, we also report the results of several experiment
    
[^702]: 大规模密集随机Kronecker图的分析和近似推断

    Analysis and Approximate Inference of Large and Dense Random Kronecker Graphs. (arXiv:2306.08489v1 [stat.ML])

    [http://arxiv.org/abs/2306.08489](http://arxiv.org/abs/2306.08489)

    本文对大规模密集随机Kronecker图进行了分析和近似推断，提出了“去噪声和求解”元算法，用于近似推断图参数，并具有较低的计算复杂度和性能保证。

    

    随机图模型在科学和工业中发挥着越来越重要的作用，并在各种领域中得到应用，包括社交和交通网络、推荐系统和分子遗传学。本文对\cite{leskovec2010kronecker}中提出的随机Kronecker图模型进行了深入的分析，当图顶点数量$N$很大时。基于最近在随机矩阵理论方面的进展，我们证明，在密集的情况下，随机Kronecker图邻接矩阵近似遵循一个信号加噪声模型，其中信号矩阵的秩很小（最多为$\log N$阶），在图参数中是线性的，而随机的噪声矩阵具有四分之一圆形奇异值分布。这个观察允许我们提出了一种“去噪声和求解”元算法来近似推断图参数，具有较低的计算复杂度和（渐近的）性能保证。通过图i的数值实验进行了验证。

    Random graph models are playing an increasingly important role in science and industry, and finds their applications in a variety of fields ranging from social and traffic networks, to recommendation systems and molecular genetics. In this paper, we perform an in-depth analysis of the random Kronecker graph model proposed in \cite{leskovec2010kronecker}, when the number of graph vertices $N$ is large. Built upon recent advances in random matrix theory, we show, in the dense regime, that the random Kronecker graph adjacency matrix follows approximately a signal-plus-noise model, with a small-rank (of order at most $\log N$) signal matrix that is linear in the graph parameters and a random noise matrix having a quarter-circle-form singular value distribution. This observation allows us to propose a ``denoise-and-solve'' meta algorithm to approximately infer the graph parameters, with reduced computational complexity and (asymptotic) performance guarantee. Numerical experiments of graph i
    
[^703]: SqueezeLLM：密集稀疏量化

    SqueezeLLM: Dense-and-Sparse Quantization. (arXiv:2306.07629v1 [cs.CL])

    [http://arxiv.org/abs/2306.07629](http://arxiv.org/abs/2306.07629)

    本文提出了一种基于训练后的量化框架——SqueezeLLM，它不仅可以实现高达3位的无损压缩，而且在相同的内存约束下实现更高的量化性能。

    

    生成式大型语言模型(LLMs)已经证明在广泛领域的任务中取得了非凡的成果。但是由于其前所未有的资源需求，将这些模型用于推理一直是一个巨大的挑战。这导致现有的部署框架需要使用多GPU推理管道，这通常是复杂和昂贵的，或者使用更小且性能更低的模型。在这项工作中，我们证明了用于LLMs生成推断的主要瓶颈是内存带宽，而不是计算，尤其是单个批次推理。虽然通过使用减少精度来表示模型权重，量化已经成为一种有前途的解决方案，但是以前的努力通常导致性能下降。为了解决这个问题，我们引入SqueezeLLM，这是一种基于训练后的量化框架，不仅可以实现高达3位的无损压缩，而且在相同的内存约束下实现更高的量化性能。

    Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing model weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint
    
[^704]: 通过神经表面渲染，在混乱场景中学习任意视角的6DoF机器人抓取

    Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural Surface Rendering. (arXiv:2306.07392v1 [cs.RO])

    [http://arxiv.org/abs/2306.07392](http://arxiv.org/abs/2306.07392)

    通过神经表面渲染，NeuGraspNet能够在混乱场景中有效地从任意视角预测6DoF抓取质量，并能够在遮挡的场景中采样抓取候选项。

    

    机器人操作在智能辅助等各种应用领域中至关重要。其中一个主要挑战是在杂乱的环境中从任何视角有效地抓取对象，而不需要额外的场景探索。我们引入了NeuGraspNet，一种新颖的6DoF抓取检测方法，利用了神经体积表示和表面渲染的最新进展。我们的方法学习了全局（场景级别）和局部（抓取级别）神经表面表示，使得即使在场景的未见部分，也能有效地预测6DoF抓取质量。此外，我们将抓取重新解释为一个局部的神经表面渲染问题，使得模型能够编码机器人末端执行器和对象表面几何之间的交互。NeuGraspNet在单个视角上运行，并且可以在遮挡的场景中采样抓取候选项，表现出优于现有隐式和半隐式基线模型的性能。

    Robotic manipulation is critical for admitting robotic agents to various application domains, like intelligent assistance. A major challenge therein is the effective 6DoF grasping of objects in cluttered environments from any viewpoint without requiring additional scene exploration. We introduce $\textit{NeuGraspNet}$, a novel method for 6DoF grasp detection that leverages recent advances in neural volumetric representations and surface rendering. Our approach learns both global (scene-level) and local (grasp-level) neural surface representations, enabling effective and fully implicit 6DoF grasp quality prediction, even in unseen parts of the scene. Further, we reinterpret grasping as a local neural surface rendering problem, allowing the model to encode the interaction between the robot's end-effector and the object's surface geometry. NeuGraspNet operates on single viewpoints and can sample grasp candidates in occluded scenes, outperforming existing implicit and semi-implicit baselin
    
[^705]: 长期序列预测是否需要复杂的注意力机制和额外的长输入数据？

    Does Long-Term Series Forecasting Need Complex Attention and Extra Long Inputs?. (arXiv:2306.05035v1 [cs.LG])

    [http://arxiv.org/abs/2306.05035](http://arxiv.org/abs/2306.05035)

    本论文介绍了一种新的轻量级周期-注意机制，名为Periodformer，解决了长期序列预测中的两个主要问题，并证明了Transformer-based方法不需要额外长的输入序列来保证性能。

    

    随着基于Transformer的模型在各种时间序列任务上取得了令人印象深刻的性能，长期序列预测（LTSF）任务在近年来也受到了广泛关注。然而，由于基于Transformer的方法所固有的计算复杂性和需要长序列，它在LTSF任务上的应用仍然存在两个主要问题需要进一步研究：1）这些方法设计的稀疏注意机制是否实际上缩短了真实设备上的运行时间；2）这些模型是否需要额外长的输入序列来保证它们的性能？本论文的答案是否定的。因此，为更好地解决这两个问题，我们设计了一种轻量级的周期-注意机制（Periodformer），通过显式周期性和内置的接近性来重新设计长期子序列和短期子序列的聚合。同时，我们还嵌入了一个门控机制到Periodformer中以调整注意力的影响。

    As Transformer-based models have achieved impressive performance on various time series tasks, Long-Term Series Forecasting (LTSF) tasks have also received extensive attention in recent years. However, due to the inherent computational complexity and long sequences demanding of Transformer-based methods, its application on LTSF tasks still has two major issues that need to be further investigated: 1) Whether the sparse attention mechanism designed by these methods actually reduce the running time on real devices; 2) Whether these models need extra long input sequences to guarantee their performance? The answers given in this paper are negative. Therefore, to better copy with these two issues, we design a lightweight Period-Attention mechanism (Periodformer), which renovates the aggregation of long-term subseries via explicit periodicity and short-term subseries via built-in proximity. Meanwhile, a gating mechanism is embedded into Periodformer to regulate the influence of the attention
    
[^706]: 分层级别激活机制

    Layer-level activation mechanism. (arXiv:2306.04940v1 [cs.LG])

    [http://arxiv.org/abs/2306.04940](http://arxiv.org/abs/2306.04940)

    去噪声更好，表现更好的分层级别激活机制

    

    本文提出了一种新颖的激活机制，旨在建立分层级别激活功能（LayerAct）。这些功能旨在通过减少输入偏移所导致的激活输出的分层级波动来降低传统元素级激活功能的噪音鲁棒性。此外，LayerAct功能实现了类似于零的平均激活输出，而不限制激活输出空间。我们进行了分析和实验，证明LayerAct功能在噪声鲁棒性方面优于元素级激活功能，并且经验证明这些功能的平均激活结果类似于零。在三个基准图像分类任务的实验结果表明，在处理嘈杂的图像数据集时，LayerAct功能比元素级激活功能表现更好，而在大多数情况下，清洁数据集的表现也是优越的。

    In this work, we propose a novel activation mechanism aimed at establishing layer-level activation (LayerAct) functions. These functions are designed to be more noise-robust compared to traditional element-level activation functions by reducing the layer-level fluctuation of the activation outputs due to shift in inputs. Moreover, the LayerAct functions achieve a zero-like mean activation output without restricting the activation output space. We present an analysis and experiments demonstrating that LayerAct functions exhibit superior noise-robustness compared to element-level activation functions, and empirically show that these functions have a zero-like mean activation. Experimental results on three benchmark image classification tasks show that LayerAct functions excel in handling noisy image datasets, outperforming element-level activation functions, while the performance on clean datasets is also superior in most cases.
    
[^707]: 利用观测偏差提高矩阵补全的方法研究

    Exploiting Observation Bias to Improve Matrix Completion. (arXiv:2306.04775v1 [cs.LG])

    [http://arxiv.org/abs/2306.04775](http://arxiv.org/abs/2306.04775)

    本研究利用观测偏差来改进矩阵补全问题，提出一个简单的两阶段算法，实现了与对未观测协变量的监督学习性能相当的结果。

    

    我们考虑了一种变形的矩阵补全问题，其中输入数据以偏差的方式呈现，类似于Ma和Chen所引入的模型。我们的目标是利用偏差与感兴趣的结果之间的共享信息来改进预测。为此，我们提出了一个简单的两阶段算法：（i）将观测模式解释为完全观测的噪声矩阵，我们对观测模式应用传统的矩阵补全方法来估计潜在因素之间的距离； (ii)我们对恢复的特征应用监督学习来填补缺失观察。我们建立了有限样本误差率，这些误差率与相应的监督学习参数率相竞争，这表明我们的学习性能与使用未观测协变量相当。实证评估使用真实世界数据集反映了类似的表现。

    We consider a variant of matrix completion where entries are revealed in a biased manner, adopting a model akin to that introduced by Ma and Chen. Instead of treating this observation bias as a disadvantage, as is typically the case, our goal is to exploit the shared information between the bias and the outcome of interest to improve predictions. Towards this, we propose a simple two-stage algorithm: (i) interpreting the observation pattern as a fully observed noisy matrix, we apply traditional matrix completion methods to the observation pattern to estimate the distances between the latent factors; (ii) we apply supervised learning on the recovered features to impute missing observations. We establish finite-sample error rates that are competitive with the corresponding supervised learning parametric rates, suggesting that our learning performance is comparable to having access to the unobserved covariates. Empirical evaluation using a real-world dataset reflects similar performance g
    
[^708]: 公平多智能体赌博机的最优算法研究

    Optimal Fair Multi-Agent Bandits. (arXiv:2306.04498v1 [cs.LG])

    [http://arxiv.org/abs/2306.04498](http://arxiv.org/abs/2306.04498)

    本文针对多智能体之间公平多臂赌博机学习问题提出了一种算法，通过分布式拍卖算法学习样本最优匹配，使用一种新的利用阶段和一种基于顺序统计的遗憾分析实现，相较于先前的结果遗憾阶数从$O(\log T \log\log T)$到了$O\left(N^3 \log N \log T \right)$，能够更好地处理多个智能体之间的依赖关系。

    

    本文研究了在多个不相互通信的智能体之间进行公平的多臂赌博机学习的问题，这些智能体只有在同时访问同一个臂时才提供碰撞信息。我们提出了一种算法，其遗憾为$O\left(N^3 \log N \log T \right)$（假设奖励有界，但未知上界）。这大大改进了之前结果，其遗憾阶数为$O(\log T \log\log T)$，并且对智能体数量具有指数依赖性。结果是通过使用分布式拍卖算法来学习样本最优匹配，一种新的利用阶段，其长度来自于观察到的样本，以及一种基于顺序统计的遗憾分析实现的。仿真结果显示了遗憾对$\log T$的依存关系。

    In this paper, we study the problem of fair multi-agent multi-arm bandit learning when agents do not communicate with each other, except collision information, provided to agents accessing the same arm simultaneously. We provide an algorithm with regret $O\left(N^3 \log N \log T \right)$ (assuming bounded rewards, with unknown bound). This significantly improves previous results which had regret of order $O(\log T \log\log T)$ and exponential dependence on the number of agents. The result is attained by using a distributed auction algorithm to learn the sample-optimal matching, a new type of exploitation phase whose length is derived from the observed samples, and a novel order-statistics-based regret analysis. Simulation results present the dependence of the regret on $\log T$.
    
[^709]: 如何跨越云和大陆培训深度学习模型？一项实验研究。

    How Can We Train Deep Learning Models Across Clouds and Continents? An Experimental Study. (arXiv:2306.03163v1 [cs.LG])

    [http://arxiv.org/abs/2306.03163](http://arxiv.org/abs/2306.03163)

    本文通过实验研究，探究了在不同大陆、云供应商和数据中心范围内，使用分布式数据并行点深度学习训练是否是更具成本效益的选择，并比较了其与集中式训练的可扩展性潜力。

    

    在云端或专用硬件上训练深度学习模型是昂贵的。一种更具成本效益的选择是提供点实例的高超规模云，这是一个便宜但短暂的选择，用于替代按需资源。由于点实例的可用性可能会因日期、大陆和云供应商不同而发生变化，因此在全球范围内分配资源可能更具成本效益。但是，尚未调查地理分布式数据并行点深度学习训练是否是集中式训练的更具成本效益的替代方案。本文旨在回答一个问题：深度学习模型能否在覆盖不同数据中心和云提供商的点 VM 全球市场上以更具成本效益的方式进行训练？为了提供指导，我们广泛评估了不同区域、大陆和云对代表性 CV 和 NLP 模型的成本和吞吐量影响。为了进一步扩展当前的培训选择，我们比较了可扩展性潜力。

    Training deep learning models in the cloud or on dedicated hardware is expensive. A more cost-efficient option are hyperscale clouds offering spot instances, a cheap but ephemeral alternative to on-demand resources. As spot instance availability can change depending on the time of day, continent, and cloud provider, it could be more cost-efficient to distribute resources over the world. Still, it has not been investigated whether geo-distributed, data-parallel spot deep learning training could be a more cost-efficient alternative to centralized training.  This paper aims to answer the question: Can deep learning models be cost-efficiently trained on a global market of spot VMs spanning different data centers and cloud providers? To provide guidance, we extensively evaluate the cost and throughput implications of training in different zones, continents, and clouds for representative CV and NLP models. To expand the current training options further, we compare the scalability potential f
    
[^710]: 将NP困难的最小最大路径问题作为具有公平背景的顺序生成来解决

    Solving NP-hard Min-max Routing Problems as Sequential Generation with Equity Context. (arXiv:2306.02689v1 [cs.LG])

    [http://arxiv.org/abs/2306.02689](http://arxiv.org/abs/2306.02689)

    本文提出了一个新的深度学习框架Equity-Transformer来解决大规模的最小最大路径问题。该模型利用可扩展的深度学习模型进行顺序决策，并生成考虑公平工作负载的顺序动作。研究显示，Equity-Transformer在两个代表性最小最大路径问题中具有卓越的性能。

    

    最小最大路径问题旨在最小化所有代理商协同访问所有城市的最大旅游长度，即完成时间。这些问题包括有影响力的实际应用，但被认为是NP困难的。现有方法面临挑战，特别是在需要协调众多代理商覆盖数千个城市的大规模问题中。本文提出了一个新的深度学习框架来解决大规模的最小最大路径问题。我们将多个代理商的同时决策建模为顺序生成过程，允许利用可扩展的深度学习模型进行顺序决策。在顺序近似问题中，我们提出了一个可扩展的上下文Transformer模型Equity-Transformer，它生成考虑其他代理商之间公平工作负载的顺序动作。Equity-Transformer的有效性通过其在两个代表性最小最大路径问题中具有卓越的性能得到证明。

    Min-max routing problems aim to minimize the maximum tour length among agents as they collaboratively visit all cities, i.e., the completion time. These problems include impactful real-world applications but are known as NP-hard. Existing methods are facing challenges, particularly in large-scale problems that require the coordination of numerous agents to cover thousands of cities. This paper proposes a new deep-learning framework to solve large-scale min-max routing problems. We model the simultaneous decision-making of multiple agents as a sequential generation process, allowing the utilization of scalable deep-learning models for sequential decision-making. In the sequentially approximated problem, we propose a scalable contextual Transformer model, Equity-Transformer, which generates sequential actions considering an equitable workload among other agents. The effectiveness of Equity-Transformer is demonstrated through its superior performance in two representative min-max routing 
    
[^711]: 关于ReLU网络的大小无关样本复杂度

    On Size-Independent Sample Complexity of ReLU Networks. (arXiv:2306.01992v1 [cs.LG])

    [http://arxiv.org/abs/2306.01992](http://arxiv.org/abs/2306.01992)

    本文研究了ReLU神经网络的样本复杂度，给出了一个现有方法精细化的结果，实现了无深度依赖性的上界。

    

    我们从泛化的角度研究了学习ReLU神经网络的样本复杂度。在权重矩阵上给定范数约束的情况下，一个常见的方法是估计相关函数类的Rademacher复杂度。之前Golowich-Rakhlin-Shamir (2020)获得了一个不依赖于网络大小的（与Frobenius范数的乘积成比例）上界，除了一个平方根深度的因子。我们给出了一个精细化的结果，通常根本没有明显的深度依赖性。

    We study the sample complexity of learning ReLU neural networks from the point of view of generalization. Given norm constraints on the weight matrices, a common approach is to estimate the Rademacher complexity of the associated function class. Previously Golowich-Rakhlin-Shamir (2020) obtained a bound independent of the network size (scaling with a product of Frobenius norms) except for a factor of the square-root depth. We give a refinement which often has no explicit depth-dependence at all.
    
[^712]: 组合优化中对称探索是免费的！

    Symmetric Exploration in Combinatorial Optimization is Free!. (arXiv:2306.01276v1 [cs.LG])

    [http://arxiv.org/abs/2306.01276](http://arxiv.org/abs/2306.01276)

    该论文提出了一种免费的技术，通过利用对称性提高了基于DRL的组合优化求解器的性能，无需额外的目标函数评估，适用于广泛的组合优化任务，并在多种任务上进行实证评估证实了其有效性。

    

    最近，深度强化学习（DRL）在解决组合优化（CO）问题方面已经显示出潜力。然而，他们经常需要大量的目标函数评估，这在现实场景中可能耗时。为了解决这个问题，我们提出了一种“免费”的技术，通过利用对称性来增强任何深度强化学习（DRL）求解器的性能，而无需额外的目标函数评估。我们的关键思想是通过保留奖励的变换来增强基于DRL的组合优化求解器的训练。该算法可能具有影响力，因为它简单，易于与现有求解器集成，并适用于广泛的组合优化任务。在NP难的路线优化，调度优化和新型分子优化的广泛实证评估结果表明，我们的方法轻松提高了最先进的DRL算法的样本效率。

    Recently, deep reinforcement learning (DRL) has shown promise in solving combinatorial optimization (CO) problems. However, they often require a large number of evaluations on the objective function, which can be time-consuming in real-world scenarios. To address this issue, we propose a "free" technique to enhance the performance of any deep reinforcement learning (DRL) solver by exploiting symmetry without requiring additional objective function evaluations. Our key idea is to augment the training of DRL-based combinatorial optimization solvers by reward-preserving transformations. The proposed algorithm is likely to be impactful since it is simple, easy to integrate with existing solvers, and applicable to a wide range of combinatorial optimization tasks. Extensive empirical evaluations on NP-hard routing optimization, scheduling optimization, and de novo molecular optimization confirm that our method effortlessly improves the sample efficiency of state-of-the-art DRL algorithms. Ou
    
[^713]: 为什么在对抗训练中会同时出现干净泛化和强健过拟合现象？

    Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training. (arXiv:2306.01271v1 [cs.LG])

    [http://arxiv.org/abs/2306.01271](http://arxiv.org/abs/2306.01271)

    对抗训练是训练深度神经网络抗击对抗扰动的标准方法, 其学习机制导致干净泛化和强健过拟合现象同时发生。

    

    对抗训练是训练深度神经网络抗击对抗扰动的标准方法。与在标准深度学习环境中出现惊人的干净泛化能力类似，通过对抗训练训练的神经网络也能很好地泛化到未见过的干净数据。然而，与干净泛化不同的是，尽管对抗训练能够实现低鲁棒训练误差，仍存在显著的鲁棒泛化距离，这促使我们探索在学习过程中导致干净泛化和强健过拟合现象同时发生的机制。本文提供了对抗训练中这种现象的理论理解。首先，我们提出了对抗训练的理论框架，分析了特征学习过程，解释了对抗训练如何导致网络学习者进入到干净泛化和强健过拟合状态。具体来说，我们证明了，通过迫使学习器成为强预测网络，对抗训练将导致干净泛化和鲁棒过拟合现象同时发生。

    Adversarial training is a standard method to train deep neural networks to be robust to adversarial perturbation. Similar to surprising $\textit{clean generalization}$ ability in the standard deep learning setting, neural networks trained by adversarial training also generalize well for $\textit{unseen clean data}$. However, in constrast with clean generalization, while adversarial training method is able to achieve low $\textit{robust training error}$, there still exists a significant $\textit{robust generalization gap}$, which promotes us exploring what mechanism leads to both $\textit{clean generalization and robust overfitting (CGRO)}$ during learning process. In this paper, we provide a theoretical understanding of this CGRO phenomenon in adversarial training. First, we propose a theoretical framework of adversarial training, where we analyze $\textit{feature learning process}$ to explain how adversarial training leads network learner to CGRO regime. Specifically, we prove that, u
    
[^714]: STEVE-1: 一个用于Minecraft中文本-行为生成的生成模型

    STEVE-1: A Generative Model for Text-to-Behavior in Minecraft. (arXiv:2306.00937v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.00937](http://arxiv.org/abs/2306.00937)

    STEVE-1 是一种新的生成模型，能够在Minecraft中跟随各种短期开放型文本和视觉指令。STEVE-1利用预先训练的模型和最佳实践，通过自监督的行为克隆和回顾重新标记来微调，避免了昂贵的人工注释。

    

    建立对文本指令做出响应的AI模型对于连续性决策任务来说是具有挑战性的。本文介绍了一种名为STEVE-1的Minecraft指令调整型视频预训练模型，展示了DALL-E 2中使用的unCLIP方法也对创建指令跟随连续决策代理非常有效。STEVE-1分为两个步骤进行训练：首先是将预先训练的VPT模型适应MineCLIP的潜在空间中的指令，然后训练一个先验模型以从文本预测潜在代码。这使我们能够通过自监督的行为克隆和回顾重新标记来微调VPT，避免需要昂贵的人工文本注释。通过利用VPT和MineCLIP等预先训练的模型，并采用文本条件的图像生成的最佳实践，STEVE-1的训练成本仅为60美元，并且可以在Minecraft中遵循各种短期开放型文本和视觉指令。STEVE-1为开放的指令跟随连续决策代理设定了一个新的标准。

    Constructing AI models that respond to text instructions is challenging, especially for sequential decision-making tasks. This work introduces an instruction-tuned Video Pretraining (VPT) model for Minecraft called STEVE-1, demonstrating that the unCLIP approach, utilized in DALL-E 2, is also effective for creating instruction-following sequential decision-making agents. STEVE-1 is trained in two steps: adapting the pretrained VPT model to follow commands in MineCLIP's latent space, then training a prior to predict latent codes from text. This allows us to finetune VPT through self-supervised behavioral cloning and hindsight relabeling, bypassing the need for costly human text annotations. By leveraging pretrained models like VPT and MineCLIP and employing best practices from text-conditioned image generation, STEVE-1 costs just $60 to train and can follow a wide range of short-horizon open-ended text and visual instructions in Minecraft. STEVE-1 sets a new bar for open-ended instructi
    
[^715]: 基于最近邻的大语言模型的测试时间训练

    Test-Time Training on Nearest Neighbors for Large Language Models. (arXiv:2305.18466v1 [cs.CL])

    [http://arxiv.org/abs/2305.18466](http://arxiv.org/abs/2305.18466)

    该论文提出了一种基于最近邻的测试时间训练方法，通过检索和微调少量邻居的文本数据，该方法在大语言模型上显著提高了性能。

    

    最近的许多工作都旨在在测试时从数据库中检索相关信息以增强语言模型。我们通过直接在测试时使用其标准训练设置对检索到的数据对模型进行微调，避免了提示工程的需要。为此，我们建立了一个基于“Pile”数据集的文本嵌入的大规模分布式最近邻索引。给定一个语言模型的查询，我们的系统检索查询的邻居，并在对应于这些邻居的文本数据上微调模型。令人惊讶的是，检索和训练仅20个邻居，每个邻居仅进行一次梯度迭代，就显著提高了在“Pile”基准测试中超过二十个语言建模任务的性能。例如，测试时间训练显著缩小了小型GPT2模型和GPTNeo模型之间的性能差距，后者是专门对“Pile”进行收敛训练的，体积却是前者的十倍以上。然而，其方法的成功还取决于充分的索引质量和大小。

    Many recent efforts aim to augment language models with relevant information retrieved from a database at test time. We avoid the need for prompt engineering by directly fine-tuning the model on data retrieved at test time using its standard training setup. For this purpose, we build a large-scale distributed nearest neighbor index based on text embeddings of the Pile dataset. Given a query to a language model, our system retrieves the neighbors of the query and fine-tunes the model on the text data corresponding to those neighbors. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than twenty language modeling tasks in the Pile benchmark. For example, test-time training significantly narrows the performance gap between a small GPT2 model and a GPTNeo model, more than ten times larger, that was specifically trained to convergence on the Pile. Sufficient index quality and size, however, are
    
[^716]: 用于强化学习顺序实验设计的交叉熵估计器

    Cross-Entropy Estimators for Sequential Experiment Design with Reinforcement Learning. (arXiv:2305.18435v1 [cs.LG])

    [http://arxiv.org/abs/2305.18435](http://arxiv.org/abs/2305.18435)

    这篇论文提出了一个基于交叉熵估计器的备选下界估计方法，这个方法不需要对比样本，可以更精确地估计高信息增益，允许学习更优秀的设计策略，并且与隐式概率模型兼容。

    

    强化学习可有效地学习设计实验序列的摊销设计策略。然而，当前的方法依赖于期望信息增益的对比估计器，需要指数级的对比样本来达到无偏估计。我们提出了一种基于联合模型分布和灵活的提议分布的备选下界估计器。提议分布逼近模型参数在实验历史和设计策略条件下给定的真实后验分布。我们的估计器不需要对比样本，可以实现更准确的高信息增益估计，允许学习更优秀的设计策略，并且与隐式概率模型兼容。我们评估了我们算法在各种任务中的性能，包括连续和离散设计以及显式和隐式可能性。

    Reinforcement learning can effectively learn amortised design policies for designing sequences of experiments. However, current methods rely on contrastive estimators of expected information gain, which require an exponential number of contrastive samples to achieve an unbiased estimation. We propose an alternative lower bound estimator, based on the cross-entropy of the joint model distribution and a flexible proposal distribution. This proposal distribution approximates the true posterior of the model parameters given the experimental history and the design policy. Our estimator requires no contrastive samples, can achieve more accurate estimates of high information gains, allows learning of superior design policies, and is compatible with implicit probabilistic models. We assess our algorithm's performance in various tasks, including continuous and discrete designs and explicit and implicit likelihoods.
    
[^717]: 基于内核KL散度的自监督学习方法Kernel-SSL

    Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])

    [http://arxiv.org/abs/2305.17326](http://arxiv.org/abs/2305.17326)

    本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。

    

    对比学习通常将一个正锚点样本与许多负样本进行比较，来完成自监督学习（SSL）。相反，非对比学习，例如BYOL、SimSiam和Barlow Twins等方法，在没有显式使用负样本的情况下完成SSL。受对比学习现有分析的启发，我们提供了多种现有非对比学习方法的再生核希尔伯特空间（RKHS）理解。随后，我们提出了一种新的损失函数Kernel-SSL，直接优化RKHS中的均值嵌入和协方差算子。实验中，我们的方法Kernel-SSL在线性评估设置下在ImageNet数据集上大幅优于最先进的方法。具体来说，在进行100个epoch的预训练时，我们的方法比SimCLR表现提高了4.6%。

    Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
    
[^718]: 拉普拉斯逼近神经加性模型：贝叶斯推理提高解释性

    Laplace-Approximated Neural Additive Models: Improving Interpretability with Bayesian Inference. (arXiv:2305.16905v1 [stat.ML])

    [http://arxiv.org/abs/2305.16905](http://arxiv.org/abs/2305.16905)

    本文提出了拉普拉斯逼近神经加性模型，该模型从贝叶斯角度考虑加性结构，在恢复的特征交互中提供可信区间，提供可处理的边缘似然估计，可用于执行隐式特征选择并对特征对进行排名。

    

    深度神经网络（DNN）在许多领域取得了成功应用，但它们的黑盒性质阻碍了解释性。神经加性模型（NAM）解决了这个问题，将网络分为加性子网络，从而使输入特征和预测之间的交互变得明显。在本文中，我们从贝叶斯角度考虑加性结构，并开发了一个实用的拉普拉斯逼近方法。这种方法在以下三个方面提高了可解释性：a）它通过估计子网络的函数空间不确定性为恢复的特征交互提供可信区间；b）它提供可处理的边缘似然估计，可用于通过经验贝叶斯过程执行特征的隐式选择；c）它可用于对特征对进行排名，作为精细调整的交互模型候选。我们在几个基准数据集上实证表明，我们提出的拉普拉斯逼近神经加性模型（LA-NAM）提高了NAM模型的可解释性，并进一步揭示了学习到的子网络的交互结构。

    Deep neural networks (DNNs) have found successful applications in many fields, but their black-box nature hinders interpretability. This is addressed by the neural additive model (NAM), in which the network is divided into additive sub-networks, thus making apparent the interaction between input features and predictions. In this paper, we approach the additive structure from a Bayesian perspective and develop a practical Laplace approximation. This enhances interpretability in three primary ways: a) It provides credible intervals for the recovered feature interactions by estimating function-space uncertainty of the sub-networks; b) it yields a tractable estimate of the marginal likelihood, which can be used to perform an implicit selection of features through an empirical Bayes procedure; and c) it can be used to rank feature pairs as candidates for second-order interactions in fine-tuned interaction models. We show empirically that our proposed Laplace-approximated NAM (LA-NAM) improv
    
[^719]: 神经不完全分解：学习共轭梯度法的预处理器

    Neural incomplete factorization: learning preconditioners for the conjugate gradient method. (arXiv:2305.16368v1 [math.OC])

    [http://arxiv.org/abs/2305.16368](http://arxiv.org/abs/2305.16368)

    本文提出了一种名为神经不完全分解的新方法，利用自监督训练的图神经网络生成适用于特定问题域的有效预处理器。其通过替换传统手工预处理器显着提高了收敛和计算效率，在合成和真实问题上进行的实验均表现出竞争力。

    

    本文提出了一种新型的数据驱动方法，用于加速科学计算和优化中遇到的大规模线性方程组求解。我们的方法利用自监督训练图神经网络，生成适用于特定问题域的有效预处理器。通过替换与共轭梯度法一起使用的传统手工预处理器，我们的方法（称为神经不完全分解）显着加速了收敛和计算效率。我们的方法的核心是一种受稀疏矩阵理论启发的新型消息传递块，它与寻找矩阵的稀疏分解的目标相一致。我们在合成问题和来自科学计算的真实问题上评估了我们的方法。我们的结果表明，神经不完全分解始终优于最常见的通用预处理器，包括不完全的Cholesky方法，在收敛速度和计算效率方面表现出竞争力。

    In this paper, we develop a novel data-driven approach to accelerate solving large-scale linear equation systems encountered in scientific computing and optimization. Our method utilizes self-supervised training of a graph neural network to generate an effective preconditioner tailored to the specific problem domain. By replacing conventional hand-crafted preconditioners used with the conjugate gradient method, our approach, named neural incomplete factorization (NeuralIF), significantly speeds-up convergence and computational efficiency. At the core of our method is a novel message-passing block, inspired by sparse matrix theory, that aligns with the objective to find a sparse factorization of the matrix. We evaluate our proposed method on both a synthetic and a real-world problem arising from scientific computing. Our results demonstrate that NeuralIF consistently outperforms the most common general-purpose preconditioners, including the incomplete Cholesky method, achieving competit
    
[^720]: 一个降维人类分类的理性模型

    A Rational Model of Dimension-reduced Human Categorization. (arXiv:2305.14383v1 [cs.LG])

    [http://arxiv.org/abs/2305.14383](http://arxiv.org/abs/2305.14383)

    提出了一个基于层次化混合模型的分类模型，以及基于生成过程的低维潜在空间的分类解释，该模型学习类别表示和特征集合，适用于高维刺激下，支持零-shot学习，并验证了该模型。

    

    认知科学中现有的模型通常假设人类在心理空间中进行分级概括行为，但是在自然环境中，这些模型中的类别表示可能会受到维度诅咒的影响。人们一般依赖于一组可行但足够的特征来理解复杂的环境。本文提出了一个基于层次化概率主成分混合模型的分类模型，同时学习类别表示和经济的特征集合。该模型捕捉了人类分类中的维度偏差并支持零-shot学习。我们进一步在低维潜在空间内利用生成过程，提供高维刺激下更好的分类解释。我们通过模拟和行为实验验证了模型。

    Existing models in cognitive science typically assume human categorization as graded generalization behavior in a multidimensional psychological space. However, category representations in these models may suffer from the curse of dimensionality in a natural setting. People generally rely on a tractable yet sufficient set of features to understand the complex environment. We propose a rational model of categorization based on a hierarchical mixture of probabilistic principal components, that simultaneously learn category representations and an economical collection of features. The model captures dimensional biases in human categorization and supports zero-shot learning. We further exploit a generative process within a low-dimensional latent space to provide a better account of categorization with high-dimensional stimuli. We validate the model with simulation and behavioral experiments.
    
[^721]: 通过重新标记最小训练子集来翻转预测

    Relabel Minimal Training Subset to Flip a Prediction. (arXiv:2305.12809v1 [cs.LG])

    [http://arxiv.org/abs/2305.12809](http://arxiv.org/abs/2305.12809)

    本文利用扩展影响函数提出了一种有效的识别和重新标记最小训练子集的方法，并证明其始终能够成功翻转测试结果，同时还提供了挑战模型预测、评估模型鲁棒性和洞察训练集偏差等多重作用。

    

    Yang等人发现，仅删除1%的训练数据就可能导致预测结果翻转。鉴于机器学习模型中存在噪声数据的普遍性，本文提出了一个问题：在模型训练之前通过重新标记一个小的训练数据子集可否导致测试结果翻转？本文利用扩展影响函数提出了一种有效的识别和重新标记这种子集的方法，并证明了其始终能够产生成功的结果。这种机制有多重作用：（1）提供了一种补充方法，可以通过恢复可能错误标记的训练数据来挑战模型预测；（2）评估模型的鲁棒性，因为本文发现子集的大小与训练集中噪声数据的比例之间存在显著关系；（3）提供了洞察训练集偏差的见解。据我们所知，这项工作代表了对识别最小训练子集问题的第一次研究。

    Yang et al. (2023) discovered that removing a mere 1% of training points can often lead to the flipping of a prediction. Given the prevalence of noisy data in machine learning models, we pose the question: can we also result in the flipping of a test prediction by relabeling a small subset of the training data before the model is trained? In this paper, utilizing the extended influence function, we propose an efficient procedure for identifying and relabeling such a subset, demonstrating consistent success. This mechanism serves multiple purposes: (1) providing a complementary approach to challenge model predictions by recovering potentially mislabeled training points; (2) evaluating model resilience, as our research uncovers a significant relationship between the subset's size and the ratio of noisy data in the training set; and (3) offering insights into bias within the training set. To the best of our knowledge, this work represents the first investigation into the problem of identi
    
[^722]: Q-malizing流和无穷小密度比估计

    Q-malizing flow and infinitesimal density ratio estimation. (arXiv:2305.11857v1 [stat.ML])

    [http://arxiv.org/abs/2305.11857](http://arxiv.org/abs/2305.11857)

    研究提出了一种可以从一个数据分布P传输到任意访问通过有限样本的Q的流模型。这个模型通过神经ODE模型进行，可以进行无穷小DRE。

    

    连续的正则化流在生成任务中被广泛使用，其中流网络从数据分布P传输到正态分布。一种能够从P传输到任意Q的流模型，其中P和Q都可通过有限样本访问，将在各种应用兴趣中使用，特别是在最近开发的望远镜密度比估计中（DRE），它需要构建中间密度以在P和Q之间建立桥梁。在这项工作中，我们提出了这样的“Q-malizing流”，通过神经ODE模型进行，该模型通过经验样本的可逆传输从P到Q（反之亦然），并通过最小化传输成本进行正则化。训练好的流模型使我们能够沿与时间参数化的log密度进行无穷小DRE，通过训练附加的连续时间流网络使用分类损失来估计log密度的时间偏导数。通过积分时间得分网络

    Continuous normalizing flows are widely used in generative tasks, where a flow network transports from a data distribution $P$ to a normal distribution. A flow model that can transport from $P$ to an arbitrary $Q$, where both $P$ and $Q$ are accessible via finite samples, would be of various application interests, particularly in the recently developed telescoping density ratio estimation (DRE) which calls for the construction of intermediate densities to bridge between $P$ and $Q$. In this work, we propose such a ``Q-malizing flow'' by a neural-ODE model which is trained to transport invertibly from $P$ to $Q$ (and vice versa) from empirical samples and is regularized by minimizing the transport cost. The trained flow model allows us to perform infinitesimal DRE along the time-parametrized $\log$-density by training an additional continuous-time flow network using classification loss, which estimates the time-partial derivative of the $\log$-density. Integrating the time-score network
    
[^723]: 使用不平衡图册的状态表示学习

    State Representation Learning Using an Unbalanced Atlas. (arXiv:2305.10267v1 [cs.LG])

    [http://arxiv.org/abs/2305.10267](http://arxiv.org/abs/2305.10267)

    本文介绍了一种使用不平衡图册（UA）方法的状态表示学习，该方法可以超越最先进的自监督学习方法。

    

    流形假说认为，高维数据通常位于较低维的流形上，并且利用该流形作为目标空间可以产生更有效的表示。虽然存在许多传统的基于流形的技术用于降维，但它们在自监督学习中的应用进展缓慢。最近的MSIMCLR方法将流形编码与SimCLR相结合，但需要极低的目标编码维度才能胜过SimCLR，从而限制了其适用性。本文介绍了一种使用不平衡图册（UA）的新型学习方法，能够超越最先进的自监督学习方法。我们通过系统地调整时空DeepInfomax（ST-DIM）框架以与我们提议的UA模式保持一致，并在整个过程中采用严谨的科学方法来精心研究和设计了使用UA的DeepInfomax（DIM-UA）方法。

    The manifold hypothesis posits that high-dimensional data often lies on a lower-dimensional manifold and that utilizing this manifold as the target space yields more efficient representations. While numerous traditional manifold-based techniques exist for dimensionality reduction, their application in self-supervised learning has witnessed slow progress. The recent MSIMCLR method combines manifold encoding with SimCLR but requires extremely low target encoding dimensions to outperform SimCLR, limiting its applicability. This paper introduces a novel learning paradigm using an unbalanced atlas (UA), capable of surpassing state-of-the-art self-supervised learning approaches. We meticulously investigated and engineered the DeepInfomax with an unbalanced atlas (DIM-UA) method by systematically adapting the Spatiotemporal DeepInfomax (ST-DIM) framework to align with our proposed UA paradigm, employing rigorous scientific methodologies throughout the process. The efficacy of DIM-UA is demons
    
[^724]: 小型语言模型更适合作为黑匣子机器生成文本检测器

    Smaller Language Models are Better Black-box Machine-Generated Text Detectors. (arXiv:2305.09859v1 [cs.CL])

    [http://arxiv.org/abs/2305.09859](http://arxiv.org/abs/2305.09859)

    本文研究发现，小型语言模型更适用于作为通用文本检测器，可以更加精确地检测出机器生成的文本，而检测器和生成模型是否具有相同的架构或语料库并不会对检测性能产生显著影响。

    

    随着流畅的生成语言模型的出现，它们可以生成与人类写作的非常相似的令人信服的话语，因此区分一段文本是由机器生成的还是人类写作的变得更加具有挑战性和重要性，因为这样的模型可以用于传播错误信息、虚假新闻、虚假评论并模仿某些作者和人物。为此，已经提出了许多检测机器生成文本的方法。其中大部分方法需要访问目标模型的 logits，或需要可以从目标模型中进行采样的能力。其中一种黑匣子检测方法依赖于观察到生成文本在生成器的似然函数下是局部最优的，而人类写作的文本则不是。我们发现，总体而言，较小且部分训练的模型更适合作为通用文本检测器：它们可以更精确地检测来自小型和大型模型的生成文本。有趣的是，我们发现检测器和生成模型是否具有相同的架构或相同的语料库对检测性能没有显著影响。

    With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generat
    
[^725]: 基于抽象语法树的异构有向超图神经网络用于代码分类

    Heterogeneous Directed Hypergraph Neural Network over abstract syntax tree (AST) for Code Classification. (arXiv:2305.04228v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2305.04228](http://arxiv.org/abs/2305.04228)

    本研究提出了使用异构有向超图表示AST，并使用异构有向超图神经网络处理图形进行代码分类，超过了现有方法。

    

    代码分类是程序理解和自动编码中的一个难题。由于程序的模糊语法和复杂语义，大多数现有研究使用基于抽象语法树（AST）和图神经网络（GNN）的技术创建代码表示用于代码分类。这些技术利用代码的结构和语义信息，但只考虑节点之间的成对关系，忽略了AST中节点之间已经存在的高阶相关性，可能导致代码结构信息的丢失。本研究提出使用异构有向超图（HDHG）表示AST，并使用异构有向超图神经网络（HDHGN）处理图形。HDHG保留了节点之间的高阶相关性，并更全面地编码了AST的语义和结构信息。HDHGN通过聚合不同节点的特征并使用不同的函数对其进行处理来对AST进行建模。在四个数据集上的实验表明，HDHG和HDHGN在代码分类任务中超越了现有方法。

    Code classification is a difficult issue in program understanding and automatic coding. Due to the elusive syntax and complicated semantics in programs, most existing studies use techniques based on abstract syntax tree (AST) and graph neural network (GNN) to create code representations for code classification. These techniques utilize the structure and semantic information of the code, but they only take into account pairwise associations and neglect the high-order correlations that already exist between nodes in the AST, which may result in the loss of code structural information. On the other hand, while a general hypergraph can encode high-order data correlations, it is homogeneous and undirected which will result in a lack of semantic and structural information such as node types, edge types, and directions between child nodes and parent nodes when modeling AST. In this study, we propose to represent AST as a heterogeneous directed hypergraph (HDHG) and process the graph by hetero
    
[^726]: 基于半监督学习的高维贝叶斯优化及优化无标签数据采样

    High-dimensional Bayesian Optimization via Semi-supervised Learning with Optimized Unlabeled Data Sampling. (arXiv:2305.02614v1 [cs.LG])

    [http://arxiv.org/abs/2305.02614](http://arxiv.org/abs/2305.02614)

    本文提出基于半监督学习的高维贝叶斯优化方法，利用特定的未标记数据采样、参数化采样分布的优化及动态选择无标记数据等策略，解决了高维贝叶斯优化难以处理的问题。

    

    贝叶斯优化（BO）是一种寻找黑箱函数全局最优解的强大工具。虽然黑箱函数的评估成本往往很高，但减少昂贵标记数据的使用是理想的。本文首次提出了一种教师-学生模型，利用半监督学习在BO环境下利用大量未标记的数据。其中，关键在于选择验证和未标记数据以提高BO的表现。为了优化无标签数据的采样，我们采用黑箱参数化采样分布，将其优化为所采用双层优化框架的一部分。更进一步，通过从动态适应的极值分布中选择未标签数据，我们证明了BO的性能可以进一步提高。我们的BO方法在学习后的低维潜在空间中运行，使其可扩展到高维问题。

    Bayesian optimization (BO) is a powerful tool for seeking the global optimum of black-box functions. While evaluations of the black-box functions can be highly costly, it is desirable to reduce the use of expensive labeled data. For the first time, we introduce a teacher-student model to exploit semi-supervised learning that can make use of large amounts of unlabelled data under the context of BO. Importantly, we show that the selection of the validation and unlabeled data is key to the performance of BO. To optimize the sampling of unlabeled data, we employ a black-box parameterized sampling distribution optimized as part of the employed bi-level optimization framework. Taking one step further, we demonstrate that the performance of BO can be further improved by selecting unlabeled data from a dynamically fitted extreme value distribution. Our BO method operates in a learned latent space with reduced dimensionality, making it scalable to high-dimensional problems. The proposed approac
    
[^727]: 基于反Lipschitz约束的解码器网络控制后验坍塌

    Controlling Posterior Collapse by an Inverse Lipschitz Constraint on the Decoder Network. (arXiv:2304.12770v1 [cs.LG])

    [http://arxiv.org/abs/2304.12770](http://arxiv.org/abs/2304.12770)

    本文提出了一种基于反Lipschitz约束的解码器网络，可以简单明了地控制广泛的VAE模型的后验坍塌程度，并带有具体的理论保证。

    

    变分自编码器（VAE）是深度生成模型中取得巨大成功的一种。然而，在实践中，它们存在一个称为后验坍塌的问题，当编码器与没有考虑输入数据的潜在结构的先验重合或坍塌时就会发生。本文介绍了一种基于反Lipschitz神经网络的解码器，基于这个架构，提供了一种新方法，可以简单明了地控制广泛的VAE模型的后验坍塌程度，并带有具体的理论保证。我们还通过几个数值实验证明了我们方法的有效性。

    Variational autoencoders (VAEs) are one of the deep generative models that have experienced enormous success over the past decades. However, in practice, they suffer from a problem called posterior collapse, which occurs when the encoder coincides, or collapses, with the prior taking no information from the latent structure of the input data into consideration. In this work, we introduce an inverse Lipschitz neural network into the decoder and, based on this architecture, provide a new method that can control in a simple and clear manner the degree of posterior collapse for a wide range of VAE models equipped with a concrete theoretical guarantee. We also illustrate the effectiveness of our method through several numerical experiments.
    
[^728]: 稀疏深度神经网络中梯度下降的点对点收敛定理

    Pointwise convergence theorem of gradient descent in sparse deep neural network. (arXiv:2304.08172v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.08172](http://arxiv.org/abs/2304.08172)

    本文研究了稀疏深度神经网络中梯度下降的点对点收敛定理，针对非光滑指示函数构造了一种特殊形状的DNN，实现了梯度下降过程的点对点收敛。

    

    深度神经网络（DNN）的理论结构逐渐得到了阐明。Imaizumi-Fukumizu（2019）和Suzuki（2019）指出，当目标函数为非光滑函数时，DNN的学习能力优于先前的理论。然而，据作者所知，迄今为止的众多研究尝试在没有任何统计论证的情况下进行数学研究，探究真正能够引发梯度下降的DNN架构的点对点收敛性，这一尝试似乎更贴近实际DNN。本文将目标函数限制为非光滑指示函数，并在ReLU-DNN中构造了一个稀疏且具有特殊形状的DNN，从而实现了梯度下降过程中的点对点收敛。

    The theoretical structure of deep neural network (DNN) has been clarified gradually. Imaizumi-Fukumizu (2019) and Suzuki (2019) clarified that the learning ability of DNN is superior to the previous theories when the target function is non-smooth functions. However, as far as the author is aware, none of the numerous works to date attempted to mathematically investigate what kind of DNN architectures really induce pointwise convergence of gradient descent (without any statistical argument), and this attempt seems to be closer to the practical DNNs. In this paper we restrict target functions to non-smooth indicator functions, and construct a deep neural network inducing pointwise convergence provided by gradient descent process in ReLU-DNN. The DNN has a sparse and a special shape, with certain variable transformations.
    
[^729]: SAMM（Segment Any Medical Model）：用于SAM的3D Slicer集成

    SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM. (arXiv:2304.05622v1 [eess.IV])

    [http://arxiv.org/abs/2304.05622](http://arxiv.org/abs/2304.05622)

    介绍了Segment Any Medical Model (SAMM)，它是用于3D Slicer的SAM的扩展。SAMM在医学图像分割上表现良好，在实时性和通用性方面都有很好的性能，可以推断出掩模。

    

    Segment Anything Model（SAM）是一个新的图像分割工具，使用迄今为止最大的分割数据集进行训练。该模型表明它可以创建高质量的图像分割掩模，具有良好的实时性和通用性。然而，在医学图像上的性能需要进一步验证。为了协助在医学图像上开发，评估和利用SAM，我们介绍了Segment Any Medical Model（SAMM），它是SAM在3D Slicer上的扩展。3D Slicer是一个广泛使用于医学影像处理和可视化软件的开源软件。这个开源扩展程序及其演示已发布在GitHub上（https://github.com/bingogome/samm）。SAMM在完整周期中实现了0.6秒的延迟，并可以实时推断出图像掩模。

    The Segment Anything Model (SAM) is a new image segmentation tool trained with the largest segmentation dataset at this time. The model has demonstrated that it can create high-quality masks for image segmentation with good promptability and generalizability. However, the performance of the model on medical images requires further validation. To assist with the development, assessment, and utilization of SAM on medical images, we introduce Segment Any Medical Model (SAMM), an extension of SAM on 3D Slicer, a widely-used open-source image processing and visualization software that has been extensively used in the medical imaging community. This open-source extension to 3D Slicer and its demonstrations are posted on GitHub (https://github.com/bingogome/samm). SAMM achieves 0.6-second latency of a complete cycle and can infer image masks in nearly real-time.
    
[^730]: GLADE：用于非配对超分辨率各向异性MRI的梯度损失增强退化增强

    GLADE: Gradient Loss Augmented Degradation Enhancement for Unpaired Super-Resolution of Anisotropic MRI. (arXiv:2303.11831v1 [cs.CV])

    [http://arxiv.org/abs/2303.11831](http://arxiv.org/abs/2303.11831)

    本文提出了一种可用于加速全腹MRI扫描的新方法GLADE，它通过使用梯度映射损失来合成高分辨率等向性3D腹部MR图像。

    

    我们提出了一种新方法，在非配对的情况下，从各向异性3D图像中合成高分辨率等向性3D腹部MR图像。通过使用修改后的CycleGAN架构，并使用梯度映射损失，我们利用来自各向异性体积高分辨率（面内）数据的不重叠的补丁，强制网络生成器增加低分辨率（面外）切片的分辨率。这将使在短时间内以高分辨率等向性图像进行全腹扫描成为可能。

    We present a novel approach to synthesise high-resolution isotropic 3D abdominal MR images, from anisotropic 3D images in an unpaired fashion. Using a modified CycleGAN architecture with a gradient mapping loss, we leverage disjoint patches from the high-resolution (in-plane) data of an anisotropic volume to enforce the network generator to increase the resolution of the low-resolution (through-plane) slices. This will enable accelerated whole-abdomen scanning with high-resolution isotropic images within short breath-hold times.
    
[^731]: 在资源受限的环境中运行关键的机器学习模型

    Operating critical machine learning models in resource constrained regimes. (arXiv:2303.10181v1 [cs.LG])

    [http://arxiv.org/abs/2303.10181](http://arxiv.org/abs/2303.10181)

    本文分享了在关键场景下使用机器学习模型时资源消耗和性能之间的权衡方法。考虑到模型在全球诊所中的部署，机器学习界正在为改进模型效率而努力。

    

    机器学习方法，特别是深度学习，在医学图像分析和计算机辅助干预方面的最新突破，使其得到快速发展。深度学习模型在训练数据，计算和能源成本方面的资源消耗是巨大的。这些巨大的资源成本可能会阻碍这些模型在全球诊所中的部署。为了解决这个问题，机器学习界正在努力引入资源效率的概念。例如，使用量化来减轻内存消耗。虽然大多数这些方法已被证明可以减少资源利用，但可能会以性能为代价。在这项工作中，我们探讨了资源消耗和性能之间的权衡，特别是在诊所等关键环境中使用的模型方面。

    The accelerated development of machine learning methods, primarily deep learning, are causal to the recent breakthroughs in medical image analysis and computer aided intervention. The resource consumption of deep learning models in terms of amount of training data, compute and energy costs are known to be massive. These large resource costs can be barriers in deploying these models in clinics, globally. To address this, there are cogent efforts within the machine learning community to introduce notions of resource efficiency. For instance, using quantisation to alleviate memory consumption. While most of these methods are shown to reduce the resource utilisation, they could come at a cost in performance. In this work, we probe into the trade-off between resource consumption and performance, specifically, when dealing with models that are used in critical settings such as in clinics.
    
[^732]: 基于像素的混合交通控制与协调方法

    Mixed Traffic Control and Coordination from Pixels. (arXiv:2302.09167v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2302.09167](http://arxiv.org/abs/2302.09167)

    本研究考虑利用图像观察作为替代方法来进行混合交通控制。

    

    交通拥堵是我们社会中一直存在的问题。传统的交通控制方法在缓解当前拥堵程度方面已经失效，因此研究人员开始探索通过机器人车辆进行交通控制的想法，考虑到不同级别自主性车辆的不断涌现。这引起了混合交通控制的出现，其中机器人车辆通过强化学习算法来调节人驾驶车辆。本研究考虑利用图像观察作为混合交通控制的替代方法：1）图像通过卫星图像、车内摄像系统和交通监控系统普遍存在；2）图像不需要更新现有道路基础设施，并且不需要向可能不愿意配合的人类驾驶员传递信息。

    Traffic congestion is a persistent problem in our society. Existing methods for traffic control have proven futile in alleviating current congestion levels leading researchers to explore ideas with robot vehicles given the increased emergence of vehicles with different levels of autonomy on our roads. This gives rise to mixed traffic control, where robot vehicles regulate human-driven vehicles through reinforcement learning (RL). However, most existing studies use precise observations that involve global information, such as environment outflow, and local information, i.e., vehicle positions and velocities. Obtaining this information requires updating existing road infrastructure with vast sensor environments and communication to potentially unwilling human drivers. We consider image observations as the alternative for mixed traffic control via RL: 1) images are ubiquitous through satellite imagery, in-car camera systems, and traffic monitoring systems; 2) images do not require a compl
    
[^733]: Sneaky Spikes: 用神经形态数据在脉冲神经网络中揭示隐蔽的后门攻击

    Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data. (arXiv:2302.06279v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.06279](http://arxiv.org/abs/2302.06279)

    本文研究使用神经形态数据和多样化的刺激在脉冲神经网络中的后门攻击问题。

    

    深度神经网络（DNN）在各种任务中显示出了卓越的性能，包括图像和语音识别。然而，最大化DNN的效果需要通过训练对众多超参数和网络参数进行精细优化。此外，高性能的DNN涉及许多参数，在训练过程中消耗大量能源。为了克服这些挑战，研究人员转向了脉冲神经网络（SNN），其提供了增强的能源效率和生物学可行的数据处理能力，使其非常适合感知数据任务，特别是在神经形态数据方面。尽管存在优势，SNN与DNN一样，容易受到各种威胁的影响，包括对抗性示例和后门攻击。然而，关于SNN在理解和对抗这些攻击方面，仍需要进一步探索。本文深入研究了使用神经形态数据和多样化的刺激的SNN中的后门攻击。

    Deep neural networks (DNNs) have demonstrated remarkable performance across various tasks, including image and speech recognition. However, maximizing the effectiveness of DNNs requires meticulous optimization of numerous hyperparameters and network parameters through training. Moreover, high-performance DNNs entail many parameters, which consume significant energy during training. In order to overcome these challenges, researchers have turned to spiking neural networks (SNNs), which offer enhanced energy efficiency and biologically plausible data processing capabilities, rendering them highly suitable for sensory data tasks, particularly in neuromorphic data. Despite their advantages, SNNs, like DNNs, are susceptible to various threats, including adversarial examples and backdoor attacks. Yet, the field of SNNs still needs to be explored in terms of understanding and countering these attacks.  This paper delves into backdoor attacks in SNNs using neuromorphic datasets and diverse trig
    
[^734]: 带有目标预测扩散混合的图生成

    Graph Generation with Destination-Predicting Diffusion Mixture. (arXiv:2302.03596v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03596](http://arxiv.org/abs/2302.03596)

    本文介绍了一种名为目标预测扩散混合的方法，用于解决传统扩散模型不能很好地建模图拓扑结构的问题，并在图生成任务上取得了最先进的性能表现。

    

    生成图是理解其非欧几里得结构复杂性的真实任务的主要挑战。虽然扩散模型在图生成方面最近取得了显着成功，但它们不适合建模图的结构信息，因为学习去噪声样本不能明确地捕捉图的拓扑结构。为了解决这个限制，我们提出了一个新颖的生成框架，通过预测扩散过程的目标，即具有正确拓扑信息的原始图作为数据的加权平均值，建模了图的拓扑结构。具体而言，我们将生成过程设计为一个以数据分布中的终点为条件的扩散过程混合，它将过程推向预测的目标，并实现快速收敛。我们引入了预测目标的新型无仿真训练目标，并进一步讨论了将这种策略纳入图生成任务中的优势。我们在几个基准数据集上的实验表明，我们提出的方法在各种评估指标上均实现了最先进的性能。

    Generation of graphs is a major challenge for real-world tasks that require understanding the complex nature of their non-Euclidean structures. Although diffusion models have achieved notable success in graph generation recently, they are ill-suited for modeling the structural information of graphs since learning to denoise the noisy samples does not explicitly capture the graph topology. To tackle this limitation, we propose a novel generative framework that models the topology of graphs by predicting the destination of the diffusion process, which is the original graph that has the correct topology information, as a weighted mean of data. Specifically, we design the generative process as a mixture of diffusion processes conditioned on the endpoint in the data distribution, which drives the process toward the predicted destination, resulting in rapid convergence. We introduce new simulation-free training objectives for predicting the destination, and further discuss the advantages of 
    
[^735]: FedEBA+：基于熵的模型实现公平和有效联邦学习

    FedEBA+: Towards Fair and Effective Federated Learning via Entropy-Based Model. (arXiv:2301.12407v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12407](http://arxiv.org/abs/2301.12407)

    FedEBA+是一种新的联邦学习算法，它采用公平聚合方案和对齐更新方法，在同时提高全局模型性能的同时提高公平性。实验证明FedEBA+优于其他公平性联邦学习方法。

    

    确保公平性是联邦学习中至关重要的方面，它使模型在所有客户端上保持一致表现。然而，设计一种可以同时提高全局模型性能和促进公平的联邦学习算法仍然是一个艰巨的挑战，因为实现后者通常需要与前者的权衡。为了解决这一问题，我们提出了一种新的联邦学习算法FedEBA+，它在同时提高全局模型性能的同时提高公平性，该算法采用公平聚合方案和对齐更新方法。此外，我们提供了理论收敛分析，证明了FedEBA+的公平性。大量实验表明FedEBA+在公平性和全局模型性能方面均优于其他SOTA的公平联邦学习方法。

    Ensuring fairness is a crucial aspect of Federated Learning (FL), which enables the model to perform consistently across all clients. However, designing an FL algorithm that simultaneously improves global model performance and promotes fairness remains a formidable challenge, as achieving the latter often necessitates a trade-off with the former.To address this challenge, we propose a new FL algorithm, FedEBA+, which enhances fairness while simultaneously improving global model performance. FedEBA+ incorporates a fair aggregation scheme that assigns higher weights to underperforming clients and an alignment update method. In addition, we provide theoretical convergence analysis and show the fairness of FedEBA+. Extensive experiments demonstrate that FedEBA+ outperforms other SOTA fairness FL methods in terms of both fairness and global model performance.
    
[^736]: 运动感知标记选择实现高效视频表征学习

    Efficient Video Representation Learning via Motion-Aware Token Selection. (arXiv:2211.10636v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.10636](http://arxiv.org/abs/2211.10636)

    该论文提出了一种新的运动感知标记选择方法，针对视频中不同补丁的信息密度，选择包含丰富动态特性的标记，放弃无效的标记，从而大大降低计算和存储需求，实现了在单台机器上进行预训练和微调而不影响性能。

    

    最近出现的蒙版视频建模技术通过在视频的自我监督学习中获得了显着的优势。然而，由于随机蒙版策略导致预测无效的标记/帧，这些技术需要大量的计算和存储，需要昂贵的计算机和大量显卡进行训练。我们利用视频补丁中的不均匀信息密度，并提出一种新的标记选择方法：MATS：运动感知标记选择，在自监督预训练和微调过程中找到包含丰富动态特性的标记，并放弃无效的标记，我们还提出了自适应帧选择策略，使模型能够关注最重要和因果性的帧，并使计算和存储需求得到显着降低，使得在单台机器上进行预训练和微调而不影响性能。

    Recently emerged Masked Video Modeling techniques demonstrated their potential by significantly outperforming previous methods in self-supervised learning for video. However, they require an excessive amount of computations and memory while predicting uninformative tokens/frames due to random masking strategies, requiring excessive computing power for training. (e.g., over 16 nodes with 128 NVIDIA A100 GPUs). To resolve this issue, we exploit the unequal information density among the patches in videos and propose a new token selection method, MATS: Motion-Aware Token Selection, that finds tokens containing rich motion features and drops uninformative ones during both self-supervised pre-training and fine-tuning. We further present an adaptive frame selection strategy that allows the model to focus on informative and causal frames with minimal redundancy. Our method significantly reduces computation and memory requirements, enabling the pre-training and fine-tuning on a single machine w
    
[^737]: 使用机器学习算法研究血凝素序列在流感病毒宿主预测中的应用

    Dive into Machine Learning Algorithms for Influenza Virus Host Prediction with Hemagglutinin Sequences. (arXiv:2207.13842v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.13842](http://arxiv.org/abs/2207.13842)

    本研究使用机器学习算法，以血凝素序列为基础，利用位置特异性评分矩阵和词嵌入的方法，通过多种评估指标在不同分类水平上评估了机器学习算法，并发现5-grams-transformer神经网络是最有效的算法，可以准确预测流感病毒序列的起源。

    

    流感病毒突变迅速，对公众健康，特别是脆弱群体构成威胁。在历史上，甲型流感病毒在不同物种之间引发过大流行。确定病毒的起源至关重要，以防止疫情的传播。最近，越来越多的人对使用机器学习算法进行病毒序列的快速准确预测产生了兴趣。本研究使用真实测试数据集和各种评估指标以不同分类水平评估机器学习算法。由于血凝素是免疫反应中的主要蛋白质，只使用血凝素序列，并以位置特异性评分矩阵和词嵌入表示。结果表明，5-grams-transformer神经网络是预测病毒序列起源最有效的算法，在较高分类水平上大约有99.54％的AUCPR，98.01％的F1得分和96.60％的MCC。

    Influenza viruses mutate rapidly and can pose a threat to public health, especially to those in vulnerable groups. Throughout history, influenza A viruses have caused pandemics between different species. It is important to identify the origin of a virus in order to prevent the spread of an outbreak. Recently, there has been increasing interest in using machine learning algorithms to provide fast and accurate predictions for viral sequences. In this study, real testing data sets and a variety of evaluation metrics were used to evaluate machine learning algorithms at different taxonomic levels. As hemagglutinin is the major protein in the immune response, only hemagglutinin sequences were used and represented by position-specific scoring matrix and word embedding. The results suggest that the 5-grams-transformer neural network is the most effective algorithm for predicting viral sequence origins, with approximately 99.54% AUCPR, 98.01% F1 score and 96.60% MCC at a higher classification l
    

