# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes.](http://arxiv.org/abs/2401.05335) | InseRF是一种在NeRF重建的3D场景中进行生成物体插入的新方法，通过用户提供的文本描述和2D边界框，实现对于现有方法难以实现的生成新物体的需求。 |
| [^2] | [Arrival Time Prediction for Autonomous Shuttle Services in the Real World: Evidence from Five Cities.](http://arxiv.org/abs/2401.05322) | 本研究提出了一个自动穿梭巴士的到达时间预测系统，利用分别用于停留时间和运行时间预测的模型，并利用实际数据进行验证。通过集成空间数据和使用层次化模型处理绕过站点的情况，得到了可靠的AT预测结果。 |
| [^3] | [Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL Networks.](http://arxiv.org/abs/2401.05308) | 该研究介绍了一种针对高空平台站（HAPS）使能的垂直异构网络中数据分布不均问题的战略客户选择策略，通过利用用户的网络流量行为预测和分类，优先选择数据呈现相似模式的客户参与，以提高联合学习（FL）模型的训练效果。 |
| [^4] | [Can Probabilistic Feedback Drive User Impacts in Online Platforms?.](http://arxiv.org/abs/2401.05304) | 这项工作探讨了内容推荐系统可能对用户产生的负面影响，并指出这种影响不仅可能由平台目标与用户福利不一致引起，还可能由学习算法对不同内容的反馈率差异造成，提出了使用多臂赌博机框架和概率反馈的解决方法。 |
| [^5] | [Synthesis of pulses from particle detectors with a Generative Adversarial Network (GAN).](http://arxiv.org/abs/2401.05295) | 通过使用生成对抗网络（GAN），我们提出了一种模型，可以在粒子探测器缺乏脉冲的情况下生成具有相同形状的脉冲，并与真实数据分布相匹配。 |
| [^6] | [AUTOACT: Automatic Agent Learning from Scratch via Self-Planning.](http://arxiv.org/abs/2401.05268) | AUTOACT是一个自动代理学习框架，通过自主规划合成轨迹，不依赖于大规模数据和闭源模型，能够实现更好或类似的性能。 |
| [^7] | [ReACT: Reinforcement Learning for Controller Parametrization using B-Spline Geometries.](http://arxiv.org/abs/2401.05251) | 本研究提出了一种使用深度强化学习和N维B样条几何进行控制器参数化的方法。通过自主决定控制器参数的调整，该方法可以简化对于复杂和非线性系统的控制器参数化过程。 |
| [^8] | [Reliability Analysis of Complex Systems using Subset Simulations with Hamiltonian Neural Networks.](http://arxiv.org/abs/2401.05244) | 该论文提出了一种使用基于Hamilton神经网络的Monte Carlo采样的子集模拟方法来进行复杂系统可靠性分析。该方法通过将优越的采样和高效的梯度评估相结合，实现了高接受率和低计算成本。在不同的可靠性问题上展示了其显著的准确性，并与传统的Hamilton Monte Carlo方法进行了比较。此方法在梯度评估方面存在一定限制。 |
| [^9] | [Decoupling Decision-Making in Fraud Prevention through Classifier Calibration for Business Logic Action.](http://arxiv.org/abs/2401.05240) | 该论文研究了通过分类器校准来实现反欺诈预防中的决策解耦。通过使用校准策略，他们发现等距和贝塔校准方法在训练和测试数据之间发生变化的场景下表现突出。这些结果为优化解耦努力的从业者提供了宝贵的见解。 |
| [^10] | [Taming "data-hungry" reinforcement learning? Stability in continuous state-action spaces.](http://arxiv.org/abs/2401.05233) | 本文介绍了一个在连续状态-动作空间中分析强化学习的新框架，并证明了在离线和在线设置中具有快速收敛速度。分析发现了两个稳定性属性，与值函数和/或策略变化如何影响贝尔曼算子和占据度测度相关。这些属性在许多连续状态-动作马尔可夫决策过程中成立，并且在线性函数逼近方法下自然产生。分析还揭示了离线和在线强化学习中悲观主义和乐观主义的角色，以及离线强化学习和迁移学习之间的联系。 |
| [^11] | [Do Vision and Language Encoders Represent the World Similarly?.](http://arxiv.org/abs/2401.05224) | 通过分析视觉和语言模型的潜在空间结构，发现未对齐和对齐的编码器的表示空间在语义上是相似的。我们提出了两种方法来匹配未对齐编码器，无需训练即可实现匹配。 |
| [^12] | [Invariant Causal Prediction with Locally Linear Models.](http://arxiv.org/abs/2401.05218) | 本文扩展了ICP原则，考虑了在不同环境下具有局部线性模型的不变因果预测任务。通过提供因果父节点的可辨识性条件和引入LoLICaP方法，实现了在观察数据中识别目标变量的因果父节点。 |
| [^13] | [Error estimation for physics-informed neural networks with implicit Runge-Kutta methods.](http://arxiv.org/abs/2401.05211) | 本研究提出使用高阶隐式龙格-库塔方法来估计物理知识神经网络的误差，在轨迹的多个点上提供精确的误差估计。通过与神经网络的预测误差高度相关，增加IRK方法的阶数可以进一步改善估计的准确性。 |
| [^14] | [Tailoring Frictional Properties of Surfaces Using Diffusion Models.](http://arxiv.org/abs/2401.05206) | 本研究介绍了一种使用机器学习模型设计表面摩擦性能的方法，通过训练模型可以直接得到符合摩擦要求的表面设计。 |
| [^15] | [Experiment Planning with Function Approximation.](http://arxiv.org/abs/2401.05193) | 本研究探讨了在上下文关联赌博问题中使用函数逼近进行实验规划的问题，并提出了两种与函数逼近兼容的实验规划策略。 |
| [^16] | [Federated Unlearning: A Survey on Methods, Design Guidelines, and Evaluation Metrics.](http://arxiv.org/abs/2401.05146) | 这篇综述论文介绍了联邦遗忘的概念和挑战，以及解决这些问题的方法和设计准则，旨在为联邦学习中保护用户隐私和防止恶意攻击提供解决方案。 |
| [^17] | [Machine Learning to Promote Translational Research: Predicting Patent and Clinical Trial Inclusion in Dementia Research.](http://arxiv.org/abs/2401.05145) | 通过利用机器学习预测痴呆研究的转化潜力，这项开创性研究有望解决痴呆研究从基础发现到实际应用转化速度较慢的问题。 |
| [^18] | [Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving Vision Transformer.](http://arxiv.org/abs/2401.05126) | 本论文提出了一种高效的领域适应方法，用于训练和测试隐私保护的视觉Transformer模型，并避免了使用加密图像导致的性能下降。实验结果表明，在图像分类任务上，该方法在CIFAR-10和ImageNet数据集上表现出更高的准确度。 |
| [^19] | [Photonics for Sustainable Computing.](http://arxiv.org/abs/2401.05121) | 光子集成电路在多个应用中得以应用，尤其是在机器学习推断方面具有高能量效率。然而，硬件制造和基础设施会造成大量碳排放，因此需要对光子学的制造和运营碳成本进行考虑，以确定其在可持续未来中是否可行。 |
| [^20] | [Noise-robust zero-shot text-to-speech synthesis conditioned on self-supervised speech-representation model with adapters.](http://arxiv.org/abs/2401.05111) | 本论文提出了一种基于自监督学习的抗噪零样本文本到语音合成方法。通过在自监督学习模型中引入适配器，并使用带有噪声参考语音对TTS模型进行微调，以及采用语音增强前端，我们实现了高质量的语音合成，对参考语音中的噪声具有高度鲁棒性。 |
| [^21] | [Any-Way Meta Learning.](http://arxiv.org/abs/2401.05097) | 本文通过引入“任意方式”学习范式解决了元学习中固定基数的限制，并通过利用从标签分配中出现的“标签等价性”来提高模型的性能和稳定性。同时提出了一个新的方法来弥补标签等价性带来的语义信息差距。 |
| [^22] | [Hierarchical Classification of Transversal Skills in Job Ads Based on Sentence Embeddings.](http://arxiv.org/abs/2401.05073) | 本论文提出了一个基于深度学习模型的分类框架，用于识别职位广告要求和横向技能集之间的相关性，并预测个别工作描述所需的技能。通过使用层次分类和多标签策略，并采用增强技术解决数据不平衡问题，该方法在欧洲就业市场具有良好的效果。 |
| [^23] | [MISS: Multiclass Interpretable Scoring Systems.](http://arxiv.org/abs/2401.05069) | 本文提出了一种全新的机器学习方法，构建了多类可解释评分系统（MISS），用于解决多类分类问题，该方法可将评分系统转换为类别概率，具有高效的训练和优化能力。 |
| [^24] | [Singer Identity Representation Learning using Self-Supervised Techniques.](http://arxiv.org/abs/2401.05064) | 本论文提出了一个框架，通过使用自监督技术在大量的独立音轨上训练，以提取适用于唱歌相关任务的高质量歌手身份表示。实验证明，这些表示在多个数据集上优于现有的基线方法，并具备领域外泛化能力。 |
| [^25] | [Content-Aware Depth-Adaptive Image Restoration.](http://arxiv.org/abs/2401.05049) | 这项工作提出了一种模块化的图像修复流程，通过利用现有模型、对象级别的修复和用户自定义的步骤顺序，以及深度感知优化生成的图像，从而实现了对整个修复过程的完全用户控制。这个系统具有很强的适应性，可以针对特定的对象类别实现图像修复。 |
| [^26] | [CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation in Classification Tasks.](http://arxiv.org/abs/2401.05043) | CreINNs是一种用于分类任务的Credal-Set Interval Neural Networks，通过保留传统的区间神经网络结构，捕捉权重不确定性，并使用概率区间的数学框架预测可信区间。实验结果表明，CreINNs在不确定性估计方面优于变分贝叶斯神经网络和深度集成，并且具有较低的计算复杂度和模型大小。 |
| [^27] | [Learning to Configure Mathematical Programming Solvers by Mathematical Programming.](http://arxiv.org/abs/2401.05041) | 通过数学规划学习来配置数学规划求解器，解决了参数依赖关系的问题。 |
| [^28] | [An Information Theoretic Approach to Interaction-Grounded Learning.](http://arxiv.org/abs/2401.05015) | 本文提出了一个基于变分信息的互动基础学习（VI-IGL）方法，用于在强化学习任务中强制执行条件独立性假设。该方法通过学习奖励解码器来最大化上下文-动作（X，A）和反馈变量Y之间的条件互信息。 |
| [^29] | [HiMTM: Hierarchical Multi-Scale Masked Time Series Modeling for Long-Term Forecasting.](http://arxiv.org/abs/2401.05012) | HiMTM是一种面向长期预测的分层多尺度屏蔽时间序列建模方法，包括分层多尺度变压器，解耦的编码器-解码器，多尺度屏蔽重构和跨尺度注意微调等组成部分。 |
| [^30] | [Temporal Analysis of World Disaster Risk:A Machine Learning Approach to Cluster Dynamics.](http://arxiv.org/abs/2401.05007) | 本研究通过机器学习方法对全球灾害风险进行了时间分析，发现尽管有持续的努力，全球仍然分为高易感性和中等易感性两个主要集群。 |
| [^31] | [AdaFed: Fair Federated Learning via Adaptive Common Descent Direction.](http://arxiv.org/abs/2401.04993) | AdaFed是一种通过自适应公共下降方向实现公平的联邦学习方法，通过调整服务器的更新方向来确保所有客户端的损失函数减小，并且更大值的客户端的减小速率更高。 |
| [^32] | [Structure-Preserving Physics-Informed Neural Networks With Energy or Lyapunov Structure.](http://arxiv.org/abs/2401.04986) | 这项研究提出了一种保持结构的物理信息神经网络(PINNs)方法，通过设计保持结构的损失函数和应用于图像识别任务，提高了PINNs的性能和应用范围。 |
| [^33] | [Invertible Solution of Neural Differential Equations for Analysis of Irregularly-Sampled Time Series.](http://arxiv.org/abs/2401.04979) | 我们提出了一种可逆解决非规则采样时间序列的神经微分方程分析方法，通过引入神经流的概念，我们的方法既保证了可逆性又降低了计算负担，并且在分类和插值任务中表现出了优异的性能。 |
| [^34] | [Closed-Form Interpretation of Neural Network Classifiers with Symbolic Regression Gradients.](http://arxiv.org/abs/2401.04978) | 本文提出了一种解释神经网络分类器的闭式表示的方法，使其适用于自动化科学发现。这种方法通过将神经网络嵌入到一组基于相同量的等价类中，并通过找到该等价类与符号回归搜索空间中的方程的交集来解释神经网络。 |
| [^35] | [ConvConcatNet: a deep convolutional neural network to reconstruct mel spectrogram from the EEG.](http://arxiv.org/abs/2401.04965) | 本论文提出了一种名为ConvConcatNet的深度卷积神经网络方法，用于从EEG中重建Mel频谱，并在Auditory EEG Challenge的Task 2中取得了第一名，重建和目标Mel频谱之间的皮尔逊相关系数达到0.0420。 |
| [^36] | [Why Change Your Controller When You Can Change Your Planner: Drag-Aware Trajectory Generation for Quadrotor Systems.](http://arxiv.org/abs/2401.04960) | 本文研究了四旋翼器在受到阻力力影响下的轨迹生成和控制设计问题。通过保持控制器固定，只改变轨迹生成部分，我们提出了一种考虑风阻的规划方法，以提高四旋翼器系统的轨迹跟踪能力。 |
| [^37] | [Advancing ECG Diagnosis Using Reinforcement Learning on Global Waveform Variations Related to P Wave and PR Interval.](http://arxiv.org/abs/2401.04938) | 本研究利用Q学习强化算法在PhysioNet/Computing in Cardiology Challenge（CinC）提供的多种ECG数据集上应用，研究了P波和PR间期在Lead II和Lead V1上的不同变异。通过Q-Agent的分类能够在平均准确率为90.4％的情况下对患者的心搏样本进行分类，并且具有较低的平均汉明损失率和较快的分类时间。 |
| [^38] | [Fully Decentralized Cooperative Multi-Agent Reinforcement Learning: A Survey.](http://arxiv.org/abs/2401.04934) | 本文系统地回顾了完全分散的合作多智能体强化学习方法，并讨论了两种不同设置下的算法以及未来的研究方向。 |
| [^39] | [Rethinking Test-time Likelihood: The Likelihood Path Principle and Its Application to OOD Detection.](http://arxiv.org/abs/2401.04933) | 通过引入似然路径原理和新的理论工具，本研究针对变分自编码器（VAEs）的条件似然性提供了非渐近可证明的超出分布（OOD）检测保证。 |
| [^40] | [Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks.](http://arxiv.org/abs/2401.04929) | 本文介绍了一种基于学习的难度校准的成员推理攻击方法，旨在显著提高低FPR下的TPR，以验证训练模型是否保护隐私。 |
| [^41] | [Relaxed Contrastive Learning for Federated Learning.](http://arxiv.org/abs/2401.04928) | 我们提出了一种放松的对比学习框架，用于解决联邦学习中的数据异构性挑战。我们的方法通过引入放松的对比学习损失，防止表示坍缩，增强特征的可传递性，从而实现了显著的性能提升。 |
| [^42] | [Inconsistency-Based Data-Centric Active Open-Set Annotation.](http://arxiv.org/abs/2401.04923) | NEAT是一种数据中心的主动学习方法，旨在解决主动开放集标注问题。它通过利用标签的可聚类性来识别已知类别，并从已知和未知类别的未标记数据中选择具有信息量的数据进行标注。 |
| [^43] | [SPT: Spectral Transformer for Red Giant Stars Age and Mass Estimation.](http://arxiv.org/abs/2401.04900) | 我们开发了一种叫做光谱变换器（SPT）的框架来预测红巨星的年龄和质量，其中关键的组成部分是为光谱设计的多头哈达玛自注意机制，同时还使用了马氏距离和蒙特卡洛Dropout来解决问题并分析不确定性。 |
| [^44] | [Nonparametric Partial Disentanglement via Mechanism Sparsity: Sparse Actions, Interventions and Sparse Temporal Dependencies.](http://arxiv.org/abs/2401.04890) | 本研究引入了一种称为机制稀疏性正则化的解缠原则，通过同时学习潜在因素和解释它们的稀疏因果图模型来实现解缠。这项工作通过非参数化可辨识性理论证明了这一原则，并提供了一种图形准则来保证完全解缠。 |
| [^45] | [Feature Network Methods in Machine Learning and Applications.](http://arxiv.org/abs/2401.04874) | 机器学习中的特征网络方法及应用是将学习任务中的特征连接成图形结构，并通过函数操作生成新的特征。这种方法在图像处理和计算生物学中具有应用价值。 |
| [^46] | [Knowledge-aware Graph Transformer for Pedestrian Trajectory Prediction.](http://arxiv.org/abs/2401.04872) | 这篇论文提出了一种知识感知图转换器结构，通过设计自注意机制和领域自适应模块来提高行人轨迹预测性能，并引入考虑跨数据集序列的额外指标进行评估。 |
| [^47] | [User Embedding Model for Personalized Language Prompting.](http://arxiv.org/abs/2401.04858) | 本研究提出了一种新的用户嵌入模块，可以更有效地处理长时间的用户历史记录，并在推荐系统中取得了显著的改进。 |
| [^48] | [Transportation Market Rate Forecast Using Signature Transform.](http://arxiv.org/abs/2401.04857) | 本论文提出了一种基于特征变换的新型统计方法，用于解决交通市场利率的预测挑战。该方法具有通用的非线性属性和特征变换核函数，能够高效生成特征，并在预测过程中准确识别季节性和制度转换。 |
| [^49] | [A Good Score Does not Lead to A Good Generative Model.](http://arxiv.org/abs/2401.04856) | 本文通过反例证明，在某些情况下，即使评分函数学习良好，基于评分的生成模型（SGMs）仍然无法生成接近真实数据分布的样本，并且只能产生训练数据点的高斯模糊样本。 |
| [^50] | [LPAC: Learnable Perception-Action-Communication Loops with Applications to Coverage Control.](http://arxiv.org/abs/2401.04855) | 提出了一种可学习的感知-行动-通信(LPAC)架构，使用卷积神经网络处理环境感知，图神经网络实现机器人之间的信息交流，浅层多层感知机计算机器人的动作。使用集中式显微算法训练模型，实现机器人群体的协作。 |
| [^51] | [Graph Learning-based Fleet Scheduling for Urban Air Mobility under Operational Constraints, Varying Demand & Uncertainties.](http://arxiv.org/abs/2401.04851) | 本文提出了一种基于图学习的方法，用于在线规划城市空中移动机队的时间表和目的地，考虑到操作限制、需求变化和不确定性等问题。通过构建新的策略架构和使用图胶囊转换网络、转换器层和Multi-head Attention-based解码器等组件，该方法能够解决现有机队规划实施中存在的复杂性，具有更高的现实性和逼真度。 |
| [^52] | [On the Correctness of the Generalized Isotonic Recursive Partitioning Algorithm.](http://arxiv.org/abs/2401.04847) | 本文通过深入分析广义等增递归分割算法（GIRP），在可分离凸损失和不可微损失的情况下，解决了等增回归问题的存在性和唯一性，并提出了递归二分分割的方法来找到解。 |
| [^53] | [T-PRIME: Transformer-based Protocol Identification for Machine-learning at the Edge.](http://arxiv.org/abs/2401.04837) | T-PRIME是一个基于Transformer的边缘机器学习协议识别方法，通过注意机制学习传输帧的结构设计，克服了传统方法的局限性。实验证明其在深度学习硬件限制下的实时可行性，并证明了其优于传统方法和最先进的神经网络。 |
| [^54] | [GNNShap: Fast and Accurate GNN Explanations using Shapley Values.](http://arxiv.org/abs/2401.04829) | GNNShap是一种使用Shapley值的解释方法，能够快速而准确地解释图神经网络的预测结果。相较于其他方法，GNNShap通过抽样、并行化计算等技术提高了解释速度和精细度。 |
| [^55] | [First 100 days of pandemic; an interplay of pharmaceutical, behavioral and digital interventions -- A study using agent based modeling.](http://arxiv.org/abs/2401.04795) | 本研究通过Agent-Based模型模拟了药物、行为和数字干预的相互作用，并建议综合运用这些干预措施应对大流行疫情。通过分析发现，最初的100天对决定疫情发展至关重要，强调了迅速决策和高效政策制定的重要性。 |
| [^56] | [Hyperbolic Machine Learning Moment Closures for the BGK Equations.](http://arxiv.org/abs/2401.04783) | 这篇论文介绍了一种使用神经网络训练的双曲线闭包模型，用于BGK动力模型的Grad矩展开，以实现最高矩的梯度的精确闭合关系。 |
| [^57] | [Generative neural networks for characteristic functions.](http://arxiv.org/abs/2401.04778) | 本论文研究了利用生成神经网络模拟特征函数的问题，并通过构建一个普适且无需假设的生成神经网络来解决。研究基于最大均值差异度量，并提出了有关逼近质量的有限样本保证。 |
| [^58] | [How predictable is language model benchmark performance?.](http://arxiv.org/abs/2401.04757) | 本研究通过对十一个最近的模型架构在五个数量级的计算规模上进行了大规模的语言模型性能研究，发现将许多个体任务和评估聚合在一起的平均基准性能可以合理预测，但在个别任务中的预测性能仍存在挑战。 |
| [^59] | [Identifying Best Practice Melting Patterns in Induction Furnaces: A Data-Driven Approach Using Time Series KMeans Clustering and Multi-Criteria Decision Making.](http://arxiv.org/abs/2401.04751) | 本研究通过时间序列KMeans聚类的数据驱动方法，识别出了感应炉中的最佳熔化模式，并利用多准则决策方法确定了最佳实践簇。 |
| [^60] | [LogFormer: A Pre-train and Tuning Pipeline for Log Anomaly Detection.](http://arxiv.org/abs/2401.04749) | LogFormer是一种预训练和调优流程，能够提高日志异常检测在不同领域之间的泛化能力。它通过在源领域上进行预训练并利用共享参数将知识转移到目标领域，同时引入Log-Attention模块来补充被日志配对忽略的信息。 |
| [^61] | [Convolutional Neural Network Ensemble Learning for Hyperspectral Imaging-based Blackberry Fruit Ripeness Detection in Uncontrolled Farm Environment.](http://arxiv.org/abs/2401.04748) | 本文提出了一种基于卷积神经网络集成学习的方法，用于在无控制农场环境下检测黑莓果实的细微成熟特征。最近的研究已经开始使用深度学习技术来提取黑莓果实图像的特征，以判断其成熟度。本研究通过使用预训练的VGG16模型，构建了一个多输入CNN进行集成分类，以解决黑莓果实成熟度检测中的困难。 |
| [^62] | [Skin Cancer Segmentation and Classification Using Vision Transformer for Automatic Analysis in Dermatoscopy-based Non-invasive Digital System.](http://arxiv.org/abs/2401.04746) | 本论文提出了一种使用Vision Transformer进行皮肤癌分割和分类的方法，采用自注意机制捕捉复杂的空间依赖关系，实现了优于传统深度学习架构的性能，并展示了高准确率和潜在的有效性。 |
| [^63] | [Testing Spintronics Implemented Monte Carlo Dropout-Based Bayesian Neural Networks.](http://arxiv.org/abs/2401.04744) | 本文提出了一种基于蒙特卡洛Dropout的贝叶斯神经网络测试框架，用于分析自旋电子学中模糊不确定性估计和准确性，为资源受限的安全关键应用提供了有效的方法。 |
| [^64] | [Masked AutoEncoder for Graph Clustering without Pre-defined Cluster Number k.](http://arxiv.org/abs/2401.04741) | 本论文提出了一种新的图聚类方法，通过加入掩码自编码器和改进的基于密度的聚类算法，能够在没有预定义的聚类数情况下实现高效的图聚类，并具有良好的泛化能力。 |
| [^65] | [A case study of Generative AI in MSX Sales Copilot: Improving seller productivity with a real-time question-answering system for content recommendation.](http://arxiv.org/abs/2401.04732) | 本论文设计了一个实时问答系统，通过LLM嵌入与销售材料进行匹配，提供给销售人员实时推荐，从而提高销售人员的工作效率。这一解决方案可以在几秒钟内返回最相关的内容推荐，即使对于大规模数据集也是如此。这一推荐系统已成功集成到微软销售人员每日使用的Dynamics CRM的生产版本中。 |
| [^66] | [RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation.](http://arxiv.org/abs/2401.04679) | RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。 |
| [^67] | [Deep Efficient Private Neighbor Generation for Subgraph Federated Learning.](http://arxiv.org/abs/2401.04336) | 本文提出了FedDEP，用于解决子图联邦学习中的信息传播不完整的问题，并提出了一系列新颖的技术设计，包括深度邻居生成和高效的私密领域生成。 |
| [^68] | [Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules.](http://arxiv.org/abs/2401.04130) | 这项工作介绍了PLUTO:一种插拔式模块化的测试时领域适应策略，通过预先训练一系列针对不同源领域的模块，有效地创建了一个"模块存储库"。采用无监督的测试时自适应方法，从存储库中选择稀疏的相关模块的子集，并创建选中模块的加权组合，实现了对新领域的自适应。 |
| [^69] | [Structure-focused Neurodegeneration Convolutional Neural Network for Modeling and Classification of Alzheimer's Disease.](http://arxiv.org/abs/2401.03922) | 本论文提出了一种结构化神经退行性卷积神经网络，用于AD和MCI的识别。该网络考虑了局部结构特征，可以更准确地进行早期AD的诊断。 |
| [^70] | [Contextual Fixed-Budget Best Arm Identification: Adaptive Experimental Design with Policy Learning.](http://arxiv.org/abs/2401.03756) | 该论文研究了个性化治疗推荐的问题，提出了一个上下文固定预算的最佳臂识别模型，通过自适应实验设计和策略学习来推荐最佳治疗方案，并通过最坏情况下的期望简单遗憾来衡量推荐的有效性。 |
| [^71] | [An exploratory study on automatic identification of assumptions in the development of deep learning frameworks.](http://arxiv.org/abs/2401.03653) | 本研究以构建一个新的最大假设数据集为基础，针对深度学习框架开发中手动识别假设的问题进行了探索性研究。在该研究中，我们发现手动识别假设的成本高，因此探讨了使用传统机器学习模型和流行的深度学习模型来识别假设的性能。 |
| [^72] | [Predicting the Skies: A Novel Model for Flight-Level Passenger Traffic Forecasting.](http://arxiv.org/abs/2401.03397) | 本研究提出了一种新颖的多模态深度学习方法用于预测航班层次的乘客流量，相比传统模型取得了显著的准确性提升。该模型有效整合了循环神经网络和卷积神经网络，利用数据内部的时间模式和空间关系来增强预测性能。 |
| [^73] | [Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving.](http://arxiv.org/abs/2401.03160) | 本文提出了一种增强的人机协作强化学习方法，通过将人类智能注入到AI中实现混合交通编队中的安全高效自动驾驶。该方法将人类专家作为导师，允许代理在不确定环境中进行探索，同时在危险情况下接管控制以避免事故，并指导代理减小交通流干扰，优化交通流效果。 |
| [^74] | [Decentralized Multi-Agent Active Search and Tracking when Targets Outnumber Agents.](http://arxiv.org/abs/2401.03154) | 该论文介绍了一种处理多目标跟踪的分散式多智能体算法，该算法能在智能体数量少于目标数量时实现主动搜索和跟踪，并使用异步智能体通信来协调动作。 |
| [^75] | [Powerformer: A Section-adaptive Transformer for Power Flow Adjustment.](http://arxiv.org/abs/2401.02771) | Powerformer是一种适应不同传输区段的变压器架构，用于学习稳健电力系统状态表示。它通过开发专用的区段自适应注意机制，并引入图神经网络传播和多因素注意机制来提供更加稳健的状态表示。在三个不同的电力系统场景上进行了广泛评估。 |
| [^76] | [Mapping Walnut water Stress with High Resolution Multispectral UAV Imagery and Machine Learning.](http://arxiv.org/abs/2401.01375) | 本研究提出了一种利用高分辨率多光谱无人机影像和机器学习进行核桃水分胁迫测绘的方法，通过结合无人机影像和天气数据，使用随机森林模型有效估计了核桃树的茎水势，为核桃精准灌溉管理提供了重要的参考依据。 |
| [^77] | [Comparative study of clustering models for multivariate time series from connected medical devices.](http://arxiv.org/abs/2312.17286) | 这项研究比较了两种针对多变量时间序列的聚类模型，通过预测未来值并形成聚类空间来创建患者资料，其中一种模型可以处理动态群组归属。 |
| [^78] | [Bellman Optimal Step-size Straightening of Flow-Matching Models.](http://arxiv.org/abs/2312.16414) | 本论文介绍了一种名为BOSS的技术，通过优化步长和生成路径，提升了低资源场景下流匹配生成模型的图像质量和资源利用效率。 |
| [^79] | [I-CEE: Tailoring Explanations of Image Classification Models to User Expertise.](http://arxiv.org/abs/2312.12102) | I-CEE是一个人为中心的框架，为用户专业知识定制了图像分类模型的解释，通过提供信息丰富的示例图像、局部解释和模型决策来帮助用户理解模型的决策。 |
| [^80] | [Non-Euclidean Spatial Graph Neural Network.](http://arxiv.org/abs/2312.10808) | 本文提出了一种新的通用框架，用于学习嵌入在非欧几里德流形空间中的空间网络的表示，通过提取边上的消息将图拓扑和空间几何结合起来。 |
| [^81] | [Federated Learning with Instance-Dependent Noisy Label.](http://arxiv.org/abs/2312.10324) | 本研究针对具有实例相关噪声的联邦学习问题，提出了一种名为FedBeat的新算法。该算法通过使用实例相关噪声转移矩阵构建全局分类器，包括联邦数据提取和转移矩阵估计两个步骤。 |
| [^82] | [Calibrated One Round Federated Learning with Bayesian Inference in the Predictive Space.](http://arxiv.org/abs/2312.09817) | 本研究提出了一个名为$\beta$-Predictive Bayes的贝叶斯联邦学习算法，在预测后验的混合和乘积之间进行插值，通过调整参数$\beta$来解决现有方法中过于自信的预测问题。 |
| [^83] | [KwaiAgents: Generalized Information-seeking Agent System with Large Language Models.](http://arxiv.org/abs/2312.04889) | 本文介绍了 KwaiAgents，这是一个基于大型语言模型的通用信息搜索智能体系统。该系统能够利用语言模型作为认知核心，理解用户的查询，行为准则并参考外部文档，以提供高质量的知识和信息。 |
| [^84] | [Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility via Contextual Bandit.](http://arxiv.org/abs/2312.03038) | 我们提出了一种基于样本的动态分层Transformer模型(DHT)，通过解决上下文Bandit问题动态配置层和头的数量。与之前的工作不同，DHT不仅在训练中能够自适应优化网络架构，而且具有灵活的网络架构，用于高效的推断。 |
| [^85] | [FedEmb: A Vertical and Hybrid Federated Learning Algorithm using Network And Feature Embedding Aggregation.](http://arxiv.org/abs/2312.00102) | 本论文提出了一种名为FedEmb的通用算法，用于进行垂直和混合的基于DNN的联邦学习，具有更高的推理准确率、隐私保护性能更强以及较低的通信带宽需求。实验证明，FedEmb是一种有效的方法来解决分布式问题，并在有限的隐私泄露下提高推理准确度。 |
| [^86] | [A density estimation perspective on learning from pairwise human preferences.](http://arxiv.org/abs/2311.14115) | 研究提出了一个从密度估计的角度解释学习成对人类偏好的方法，并证明通过这种方法训练奖励函数可以有效地模拟注释者的隐含偏好分布。 |
| [^87] | [Deep Neural Decision Forest: A Novel Approach for Predicting Recovery or Decease of COVID-19 Patients with Clinical and RT-PCR.](http://arxiv.org/abs/2311.13925) | 该研究介绍了一种利用临床和RT-PCR数据结合深度学习算法来预测COVID-19患者康复或死亡风险的新方法。 |
| [^88] | [Speak Like a Native: Prompting Large Language Models in a Native Style.](http://arxiv.org/abs/2311.13538) | 本文提出了一种名为AlignedCoT的新颖有效方法，通过将上下文示例与大型语言模型（LLMs）的母语风格对齐，提高了LLMs的推理能力和性能。 |
| [^89] | [Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization.](http://arxiv.org/abs/2310.17759) | 该论文研究了在凸优化中算法的可重现性和梯度复杂度问题。他们挑战了之前的观点，证明了对于平滑凸优化和平滑凸凹极小极大问题，可以实现最优的可重现性和接近最优的收敛保证。他们还证明了在不同的oracle设置下，与不同类型的oracle相匹配的算法达到了最优性。 |
| [^90] | [Constructing and Machine Learning Calabi-Yau Five-folds.](http://arxiv.org/abs/2310.15966) | 本论文构建并机器学习了Calabi-Yau五折面，在多个复杂射影空间的乘积中，找到了27068个不同的空间，并计算了它们的欧拉数和同调数据。通过使用神经网络进行监督学习，可以高效地学习到$h^{1,1}$。 |
| [^91] | [KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models.](http://arxiv.org/abs/2310.15872) | 本文提出了一种称为基赫霍夫网络的神经网络模型，利用基赫霍夫电流定律与消息传递神经网络和连续深度网络建立连接。在MNIST数据集上，基赫霍夫网络可以实现接近98.86%的测试准确度，且具有在硬件上实现的潜力。无论网络参数数量如何，其正向计算都可以在1/f秒内完成，具有快速计算的硬件特性。 |
| [^92] | [Generalizing Medical Image Representations via Quaternion Wavelet Networks.](http://arxiv.org/abs/2310.10224) | 本文提出了一种名为QUAVE的四元数小波网络，可以从医学图像中提取显著特征。该网络可以与现有的医学图像分析或综合任务结合使用，并推广了对单通道数据的采用。通过四元数小波变换和加权处理，QUAVE能够处理具有较大变化的医学数据。 |
| [^93] | [Molecular De Novo Design through Transformer-based Reinforcement Learning.](http://arxiv.org/abs/2310.05365) | 本文提出了一种基于Transformer的强化学习方法，通过精细调整生成模型，能够在分子的全新设计中生成具有所需性质的分子结构，展现出优越的性能。 |
| [^94] | [Unified speech and gesture synthesis using flow matching.](http://arxiv.org/abs/2310.05181) | 本文提出了一种使用流匹配进行统一的语音和手势合成的架构，相比于先前的技术，它更简单、占用内存更小，并能够同时生成语音和手势模态。新的训练机制在少量步骤中实现了更好的合成质量，并通过主观测试证明了在语音自然度、手势人类化和跨模态适当性方面的改进。 |
| [^95] | [Improving Automatic VQA Evaluation Using Large Language Models.](http://arxiv.org/abs/2310.02567) | 提出使用大型语言模型改进自动视觉问答（VQA）评估的方法，将VQA评估格式化为回答评分任务，通过指令调整大型语言模型在准确度上评分候选答案，证明该方法与人类判断相关性优于现有度量方法。 |
| [^96] | [Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy.](http://arxiv.org/abs/2309.13500) | 这项研究介绍了一种创新的策略，将有符号图神经网络（SGNNs）和大型语言模型（LLM）的潜力协同起来，用于预测学生在学习者提供的问题上的表现。该方法利用有符号二分图全面建模学生回答，并采用对比学习框架增强了噪声的鲁棒性。 |
| [^97] | [Multiplying poles to avoid unwanted points in root finding and optimization.](http://arxiv.org/abs/2309.11475) | 通过增加极点来避免根查找和优化中不需要的点，方法是将代价函数除以到目标点的距离函数的适当幂。 |
| [^98] | [Analysis of the Memorization and Generalization Capabilities of AI Agents: Are Continual Learners Robust?.](http://arxiv.org/abs/2309.10149) | 本文提出了一种新的连续学习框架，旨在在动态环境下实现鲁棒的泛化能力并保留过去的知识。通过使用有限容量的内存来保存先前观察到的环境信息，并从内存中采样数据点来获得对未知变化鲁棒的预测器。该分析展示了记忆和泛化之间的权衡，而实验证明了所提出的算法的优越性。 |
| [^99] | [RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud.](http://arxiv.org/abs/2309.09737) | RaTrack是一种针对雷达跟踪的创新解决方案，通过运动分割和聚类以及运动估计模块，实现了对移动物体的精确跟踪，优于最先进性能。 |
| [^100] | [Fully-Connected Spatial-Temporal Graph for Multivariate Time-Series Data.](http://arxiv.org/abs/2309.05305) | 本论文提出了一种全连接的时空图神经网络（FC-STGNN）方法，用于有效地建模多变量时序数据中的时空依赖性。该方法能够捕捉不同时间戳上不同传感器之间的相关性，提供了一种全面建模时空依赖性的新途径。 |
| [^101] | [Graph-Aware Contrasting for Multivariate Time-Series Classification.](http://arxiv.org/abs/2309.05202) | 该论文提出了一种图感知对比学习方法，用于改进多元时间序列(MTS)分类任务。现有的对比学习方法忽视了MTS数据中的空间一致性，该方法通过图扩增和对比学习来保持传感器的稳定性和相关性，从而提高了对数据的表示能力。 |
| [^102] | [CenTime: Event-Conditional Modelling of Censoring in Survival Analysis.](http://arxiv.org/abs/2309.03851) | CenTime是一种新的生存分析方法，通过创新的事件条件审查机制直接估计事件发生的时间，在处理未被审查的数据时具有良好的鲁棒性和准确性。 |
| [^103] | [Matcha-TTS: A fast TTS architecture with conditional flow matching.](http://arxiv.org/abs/2309.03199) | Matcha-TTS是一种快速TTS架构，使用最优传输条件流匹配训练，具有高质量输出和快速合成步骤。它不需要外部对齐，与其他模型相比，具有最小的内存占用，速度更快，并在听觉测试中获得了最高的评分。 |
| [^104] | [Deep learning in medical image registration: introduction and survey.](http://arxiv.org/abs/2309.00727) | 这篇文章介绍了医学图像配准和使用深度学习的综述。它包括了图像配准的定义和符号表示，以及各种图像变换和医学图像配准算法。还讨论了基于图谱的配准和多阶段图像配准技术，以及评估指标和数据集。 |
| [^105] | [Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale Geological Carbon Storage.](http://arxiv.org/abs/2308.09113) | 多保真度傅里叶神经算子用于解决大规模地质碳储存问题，通过利用经济性更高的多保真度训练数据集，能够以与高保真度模型相当的准确性进行预测。 |
| [^106] | [Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning.](http://arxiv.org/abs/2308.07520) | 这篇论文研究了非线性、反馈和因果结构学习中的一致性问题，并提出了一个弱于强可靠性的k-Triangle Faithfulness的替代定义。 |
| [^107] | [Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season.](http://arxiv.org/abs/2308.05281) | 该研究通过社交媒体数据和SIR模型研究了2020年西部美国火灾季的灾害响应。研究发现Twitter用户主要关注健康影响、损失和撤离三个主题，并使用SIR理论探索了这些主题在Twitter上的传播规模和速度。 |
| [^108] | [Evaluating Pedestrian Trajectory Prediction Methods for the Application in Autonomous Driving.](http://arxiv.org/abs/2308.05194) | 本论文评估了行人轨迹预测方法在自动驾驶应用中的可行性，并发现简单模型在生成单个轨迹时仍然具有竞争力，某些通常被认为有用的特征对整体性能影响较小。 |
| [^109] | [SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling.](http://arxiv.org/abs/2308.04365) | SLEM是一种路径建模技术，通过集成机器学习超级学习者，实现了一致且无偏的因果效应估计，并在处理非线性关系时超过了传统的结构方程模型。 |
| [^110] | [Is Last Layer Re-Training Truly Sufficient for Robustness to Spurious Correlations?.](http://arxiv.org/abs/2308.00473) | 通过对最后一层进行重新训练，Deep Feature Reweighting（DFR）方法可以提高模型在存在虚假相关性的数据中的准确性，但其应用于实际医学数据时存在一定局限性。 |
| [^111] | [MSQNet: Actor-agnostic Action Recognition with Multi-modal Query.](http://arxiv.org/abs/2307.10763) | MSQNet是一种无关演员的多模态多标签动作识别方法，通过使用视觉和文本模态来更好地表示动作类别，克服了现有方法中针对特定演员的限制。 |
| [^112] | [Learning Curves for Heterogeneous Feature-Subsampled Ridge Ensembles.](http://arxiv.org/abs/2307.03176) | 通过引入异构特征集成，本文研究了在线性回归设置中构建的岭回归集成的学习曲线，结果表明异构特征集成具有较好的性能。 |
| [^113] | [How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model.](http://arxiv.org/abs/2307.02129) | 本文研究了深度神经网络学习组合性数据的问题，通过对随机层次模型进行分类任务，发现深度CNN学习这个任务所需的训练数据数量随着类别数、组合数和迭代次数的增加而渐进增加。 |
| [^114] | [BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting.](http://arxiv.org/abs/2307.00142) | 本文提出了BuildingsBench，这是一个包含900K座建筑物的大规模数据集，旨在解决短期负荷预测中数据集不足的问题。通过该数据集，我们进行了两个任务的基准评估，并发现经过合成预训练的模型具有良好的泛化能力。 |
| [^115] | [The Challenge of Quickly Determining the Quality of a Single-Photon Source.](http://arxiv.org/abs/2306.15683) | 该研究通过使用数据增强技术，结合实验数据和自举样本，提出了一种快速评估单光子源质量的方法，并揭示了多光子发射事件概率的不确定性对质量评估的重要影响。 |
| [^116] | [TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support.](http://arxiv.org/abs/2306.13339) | TrustGuard是一种基于GNN的信任评估模型，支持信任动态性，抗击鲁棒并提供解释能力，它的实验结果在准确性、鲁棒性和可解释性方面都优于其他方法。 |
| [^117] | [Online Dynamic Submodular Optimization.](http://arxiv.org/abs/2306.10835) | 该论文介绍了在线动态子模规划优化问题，并提出了在线子模贪婪算法（OSGA）和在线子模映射梯度下降（OSPGD）算法以解决此类问题。实验结果表明，这些算法在不同的电力系统中表现良好。 |
| [^118] | [$K$-Nearest-Neighbor Resampling for Off-Policy Evaluation in Stochastic Control.](http://arxiv.org/abs/2306.04836) | 提出了一种新颖的$K$最近邻重采样方法，用于估算历史数据中由不同策略生成的决策过程的性能，解决了离线策略评估中的反事实估计问题。 |
| [^119] | [Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination.](http://arxiv.org/abs/2305.16963) | 这个论文介绍了一个基于Pointnet++架构的神经网络模型，用于解决稀疏不规则点云中叶片和木材的区分问题。 |
| [^120] | [FedZero: Leveraging Renewable Excess Energy in Federated Learning.](http://arxiv.org/abs/2305.15092) | FedZero是一个在联邦学习中利用可再生多余能源的系统，通过能源和负载预测来调度训练任务，将碳排放降低到零。 |
| [^121] | [Evidence Networks: simple losses for fast, amortized, neural Bayesian model comparison.](http://arxiv.org/abs/2305.11241) | 本论文提出了一种名为证据网络的方法，能够在处理似然函数或先验函数与嵌套抽样无法胜任的情况下实现贝叶斯模型比较。与传统方法不同的是，该方法使用了新的损失函数，使得我们能够更快速地、更有效地估算贝叶斯因子。 |
| [^122] | [DualFL: A Duality-based Federated Learning Algorithm with Communication Acceleration in the General Convex Regime.](http://arxiv.org/abs/2305.10294) | DualFL是一种基于对偶的联邦学习算法，通过具体对偶形式解决分布式优化问题，并保证了即使使用不精确的本地解决方案也可以实现最佳通信复杂度。 |
| [^123] | [Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer.](http://arxiv.org/abs/2305.09480) | 本文提出了一种深度生成模型，可以一次性地共同设计抗体CDR的1D序列和3D结构，解决几何建模和低效推断的问题。 |
| [^124] | [Blockwise Principal Component Analysis for monotone missing data imputation and dimensionality reduction.](http://arxiv.org/abs/2305.06042) | 基于块的主成分分析处理单调缺失数据的插值与降维框架，可以显著地减少插补时间，适用于大数据集。 |
| [^125] | [Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions.](http://arxiv.org/abs/2305.05400) | 本研究探讨了使用随机Lp范数失真对图像分类器的训练和测试数据进行增强，并评估模型对不可感知随机失真的稳健性，发现稳健性可能会提高模型在随机失真方面的性能，但也可能会损害L∞范数的稳健性。 |
| [^126] | [Learning Robust Deep Equilibrium Models.](http://arxiv.org/abs/2304.12707) | 本论文提出一种名为LyaDEQ的鲁棒DEQ模型，通过Lyapunov理论提供了保证的稳定性以抵抗微小的初始扰动，并在不同的固定点之间加入全连接层以避免不良对抗性防御。 |
| [^127] | [MERMAIDE: Learning to Align Learners using Model-Based Meta-Learning.](http://arxiv.org/abs/2304.04668) | MERMAIDE是一个基于模型的元学习框架，用于训练主体快速适应具有不同学习策略和奖励函数的超出分布代理，以实现理想结果，并且能够在少量样本中适应并减少干预次数。 |
| [^128] | [Physics-guided adversarial networks for artificial digital image correlation data generation.](http://arxiv.org/abs/2303.15939) | 本文提出一种使用具有物理引导鉴别器的生成式对抗网络来生成人造DIC位移数据的方法， 以训练更精确可靠的机器学习模型，从而实现更准确可靠的疲劳裂纹增长评估的发展。 |
| [^129] | [Closed-Loop Koopman Operator Approximation.](http://arxiv.org/abs/2303.15318) | 本文提出了一种闭环Koopman算子逼近法，通过利用Koopman算子的线性性质和对控制器和闭环系统结构的了解，可以同时识别闭环和装置系统。 |
| [^130] | [Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging.](http://arxiv.org/abs/2303.06783) | 本文提出了一种异步分散的联邦终身学习 (ADFLL) 方法，通过同时训练多个任务，克服了传统联邦学习中的潜在问题，并在医学成像中的地标定位方面展现了出色的性能。 |
| [^131] | [Memory-adaptive Depth-wise Heterogenous Federated Learning.](http://arxiv.org/abs/2303.04887) | 这项研究介绍了一种名为FeDepth的内存自适应深度学习解决方案，它根据每个客户端的内存预算将完整模型自适应地分解成块，并依次训练这些块，以解决联邦学习中异构设备的内存限制问题。 |
| [^132] | [Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging.](http://arxiv.org/abs/2302.11510) | 本文提出了一种使用核心集进行选择性经验回放压缩的技术，可以提升终身学习的效率，应用于医学影像领域。 |
| [^133] | [Statistical Complexity and Optimal Algorithms for Non-linear Ridge Bandits.](http://arxiv.org/abs/2302.06025) | 本文探讨了非线性Ridge Bandits中独特的学习现象，推导出了最优烧录成本的上下限和整个烧录期间的学习轨迹的统计算法，并证明了UCB和基于回归神经元的算法都是次优解。 |
| [^134] | [Pathologies of Predictive Diversity in Deep Ensembles.](http://arxiv.org/abs/2302.00704) | 本文发现在高容量的神经网络集成中，鼓励预测多样性并不总是有效的，甚至反而会损害性能。相反地，阻止预测多样性往往是无害的，这与先前的直觉相反。 |
| [^135] | [Case-Base Neural Networks: survival analysis with time-varying, higher-order interactions.](http://arxiv.org/abs/2301.06535) | 案例基础神经网络（CBNNs）是一种新的生存分析方法，它可以同时模拟时间变化的交互和复杂的基线风险。 |
| [^136] | [SemPPL: Predicting pseudo-labels for better contrastive representations.](http://arxiv.org/abs/2301.05158) | 该论文提出了一种新的半监督学习方法SemPPL，通过预测伪标签来改善对比表示，从而解决了计算机视觉中学习大量无监督数据和少量监督数据的问题。 |
| [^137] | [t-SMILES: A Scalable Fragment-based Molecular Representation Framework for De Novo Molecule Generation.](http://arxiv.org/abs/2301.01829) | 本研究提出了一种可扩展的基于碎片的分子表示框架 t-SMILES，通过引入 t-SMILES 可以显著改善分子的表示效果，并在多种任务中表现出色，优于其他经典模型。 |
| [^138] | [Convergent autoencoder approximation of low bending and low distortion manifold embeddings.](http://arxiv.org/abs/2208.10193) | 这项研究提出了一种新的正则化方法，用于学习自动编码器的编码器部分，该方法偏好等距、外在平坦嵌入，并允许单独对编码器进行训练。研究通过蒙特卡洛积分计算损失函数，使用局部Riemannian距离和局部Riemannian均值评估输入流形上的点对。 |
| [^139] | [GANDALF: Gated Adaptive Network for Deep Automated Learning of Features.](http://arxiv.org/abs/2207.08548) | GANDALF是一种用于表格数据的高性能深度学习架构，具有解释性和计算效率高的特点。通过引入门控特征学习单元，GANDALF能够实现更好的性能，并且在多个公开基准测试中优于或与其他最先进的方法持平。 |
| [^140] | [A Reinforcement Learning Approach to Sensing Design in Resource-Constrained Wireless Networked Control Systems.](http://arxiv.org/abs/2204.00703) | 本文提出了一种基于强化学习的方法，用于在资源受限的无线网络控制系统中进行感知设计。通过在智能传感器间进行决策，可以在延迟和准确性之间进行权衡，并获得最优的感知设计策略。 |
| [^141] | [Generalized Optimistic Methods for Convex-Concave Saddle Point Problems.](http://arxiv.org/abs/2202.09674) | 本文通过将乐观梯度方法解释为对邻近点法的近似，提出了一个广义乐观方法，可以处理具有复合目标函数的约束鞍点问题，并且可以使用Bregman距离处理任意范数。此外，我们还开发了一个回溯线搜索方案，以选择步长，而不需要了解平滑系数。我们的方法在使用一阶、二阶和更高阶数学规则时，给出了已知的全局迭代复杂度界限。 |
| [^142] | [A Theoretical View of Linear Backpropagation and Its Convergence.](http://arxiv.org/abs/2112.11018) | 本文从理论上分析了线性反向传播（LinBP）在神经网络相关的学习任务中的性质，在对抗攻击和模型训练中，LinBP在相同的超参数设置下能够实现更快的收敛。 |
| [^143] | [Adaptive joint distribution learning.](http://arxiv.org/abs/2110.04829) | 该论文提出了一种自适应联合分布学习的框架，可以从大量数据点中估计低维、归一化和正的Radon-Nikodym导数模型，并在不同学习问题上取得了良好的结果。 |
| [^144] | [Hierarchical Correlation Clustering and Tree Preserving Embedding.](http://arxiv.org/abs/2002.07756) | 本文提出了一种分层相关聚类方法，可应用于正负配对不相似度，并研究了使用此方法进行无监督表征学习的方法。 |

# 详细

[^1]: InseRF: 基于文本驱动的神经3D场景中的生成物体插入方法

    InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes. (arXiv:2401.05335v1 [cs.CV])

    [http://arxiv.org/abs/2401.05335](http://arxiv.org/abs/2401.05335)

    InseRF是一种在NeRF重建的3D场景中进行生成物体插入的新方法，通过用户提供的文本描述和2D边界框，实现对于现有方法难以实现的生成新物体的需求。

    

    我们引入了InseRF，一种在3D场景的NeRF重建中进行生成物体插入的新方法。基于用户提供的文本描述和参考视点中的2D边界框，InseRF在3D场景中生成新的物体。最近，基于文本到图像扩散模型在3D生成建模中使用强大的先验知识，使得3D场景编辑的方法发生了深刻的转变。现有的方法主要通过风格和外观的改变或者移除现有物体来有效编辑3D场景。然而，对于这种方法，生成新的物体仍然是一个挑战，我们在本研究中解决了这个问题。具体而言，我们提出了将3D物体插入与参考视图中的2D物体插入进行对接的方法。然后，使用单视图物体重建方法将2D编辑提升为3D。然后，根据单目深度估计方法的先验知识将重建的物体插入到场景中。我们在不同的3D场景上对我们的方法进行了评估。

    We introduce InseRF, a novel method for generative object insertion in the NeRF reconstructions of 3D scenes. Based on a user-provided textual description and a 2D bounding box in a reference viewpoint, InseRF generates new objects in 3D scenes. Recently, methods for 3D scene editing have been profoundly transformed, owing to the use of strong priors of text-to-image diffusion models in 3D generative modeling. Existing methods are mostly effective in editing 3D scenes via style and appearance changes or removing existing objects. Generating new objects, however, remains a challenge for such methods, which we address in this study. Specifically, we propose grounding the 3D object insertion to a 2D object insertion in a reference view of the scene. The 2D edit is then lifted to 3D using a single-view object reconstruction method. The reconstructed object is then inserted into the scene, guided by the priors of monocular depth estimation methods. We evaluate our method on various 3D scene
    
[^2]: 在真实世界中为自动穿梭巴士服务的到达时间预测: 来自五个城市的证据

    Arrival Time Prediction for Autonomous Shuttle Services in the Real World: Evidence from Five Cities. (arXiv:2401.05322v1 [cs.LG])

    [http://arxiv.org/abs/2401.05322](http://arxiv.org/abs/2401.05322)

    本研究提出了一个自动穿梭巴士的到达时间预测系统，利用分别用于停留时间和运行时间预测的模型，并利用实际数据进行验证。通过集成空间数据和使用层次化模型处理绕过站点的情况，得到了可靠的AT预测结果。

    

    随着共享、连接和协作的自动驾驶车辆的出现，城市移动性正处于转型的边缘。然而，要想被客户接受，对它们的准点性的信任至关重要。许多试点项目没有固定时间表，因此可靠的到达时间（AT）预测的重要性得到了增强。本研究提出了一个针对自动穿梭巴士的AT预测系统，利用分别用于停留时间和运行时间预测的模型，并利用来自五个城市的实际数据进行验证。除了常用的方法如XGBoost外，我们还探索了使用图神经网络（GNN）集成空间数据的益处。为了准确处理穿梭巴士绕过站点的情况，我们提出了一个层次化模型，结合了随机森林分类器和GNN。最终的AT预测结果很有前景，即使预测数个站点之前也显示出较低的误差。然而，并没有单一的模型显露出普遍优势，我们提供了关于模型特征的见解。

    Urban mobility is on the cusp of transformation with the emergence of shared, connected, and cooperative automated vehicles. Yet, for them to be accepted by customers, trust in their punctuality is vital. Many pilot initiatives operate without a fixed schedule, thus enhancing the importance of reliable arrival time (AT) predictions. This study presents an AT prediction system for autonomous shuttles, utilizing separate models for dwell and running time predictions, validated on real-world data from five cities. Alongside established methods such as XGBoost, we explore the benefits of integrating spatial data using graph neural networks (GNN). To accurately handle the case of a shuttle bypassing a stop, we propose a hierarchical model combining a random forest classifier and a GNN. The results for the final AT prediction are promising, showing low errors even when predicting several stops ahead. Yet, no single model emerges as universally superior, and we provide insights into the chara
    
[^3]: 面对HAPS使能的FL网络中的非独立同分布问题，战略客户选择的研究

    Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL Networks. (arXiv:2401.05308v1 [cs.NI])

    [http://arxiv.org/abs/2401.05308](http://arxiv.org/abs/2401.05308)

    该研究介绍了一种针对高空平台站（HAPS）使能的垂直异构网络中数据分布不均问题的战略客户选择策略，通过利用用户的网络流量行为预测和分类，优先选择数据呈现相似模式的客户参与，以提高联合学习（FL）模型的训练效果。

    

    在由高空平台站（HAPS）使能的垂直异构网络中部署联合学习（FL）为各种不同通信和计算能力的客户提供了参与的机会。这种多样性不仅提高了FL模型的训练精度，还加快了其收敛速度。然而，在这些广阔的网络中应用FL存在显著的非独立同分布问题。这种数据异质性往往导致收敛速度较慢和模型训练性能的降低。我们的研究引入了一种针对此问题的客户选择策略，利用用户网络流量行为进行预测和分类。该策略通过战略性选择数据呈现相似模式的客户参与，同时优先考虑用户隐私。

    The deployment of federated learning (FL) within vertical heterogeneous networks, such as those enabled by high-altitude platform station (HAPS), offers the opportunity to engage a wide array of clients, each endowed with distinct communication and computational capabilities. This diversity not only enhances the training accuracy of FL models but also hastens their convergence. Yet, applying FL in these expansive networks presents notable challenges, particularly the significant non-IIDness in client data distributions. Such data heterogeneity often results in slower convergence rates and reduced effectiveness in model training performance. Our study introduces a client selection strategy tailored to address this issue, leveraging user network traffic behaviour. This strategy involves the prediction and classification of clients based on their network usage patterns while prioritizing user privacy. By strategically selecting clients whose data exhibit similar patterns for participation
    
[^4]: 能否用概率反馈推动在线平台对用户产生影响？

    Can Probabilistic Feedback Drive User Impacts in Online Platforms?. (arXiv:2401.05304v1 [cs.LG])

    [http://arxiv.org/abs/2401.05304](http://arxiv.org/abs/2401.05304)

    这项工作探讨了内容推荐系统可能对用户产生的负面影响，并指出这种影响不仅可能由平台目标与用户福利不一致引起，还可能由学习算法对不同内容的反馈率差异造成，提出了使用多臂赌博机框架和概率反馈的解决方法。

    

    常见的解释是内容推荐系统对用户产生负面影响是由于平台目标与用户福利之间的不对齐。在这项工作中，我们展示了平台目标不一致并不是对用户产生意外影响的唯一潜在原因：即使平台目标完全与用户福利一致，学习算法也可能对用户产生负面影响。这些用户影响的来源是不同内容可能以不同的速率产生可观察的用户反应（反馈信息）；这些反馈速率可能与影响用户体验的内容属性（如争议性或创作者的人口相似度）相关。由于反馈速率的差异可能会影响学习算法与不同内容的交互频率，学习算法可能会无意中推广具有某些特定属性的内容。使用多臂赌博机框架与概率反馈，我们提出了一种新的方法来解决这一问题。

    A common explanation for negative user impacts of content recommender systems is misalignment between the platform's objective and user welfare. In this work, we show that misalignment in the platform's objective is not the only potential cause of unintended impacts on users: even when the platform's objective is fully aligned with user welfare, the platform's learning algorithm can induce negative downstream impacts on users. The source of these user impacts is that different pieces of content may generate observable user reactions (feedback information) at different rates; these feedback rates may correlate with content properties, such as controversiality or demographic similarity of the creator, that affect the user experience. Since differences in feedback rates can impact how often the learning algorithm engages with different content, the learning algorithm may inadvertently promote content with certain such properties. Using the multi-armed bandit framework with probabilistic f
    
[^5]: 利用生成对抗网络（GAN）从粒子探测器合成脉冲

    Synthesis of pulses from particle detectors with a Generative Adversarial Network (GAN). (arXiv:2401.05295v1 [physics.ins-det])

    [http://arxiv.org/abs/2401.05295](http://arxiv.org/abs/2401.05295)

    通过使用生成对抗网络（GAN），我们提出了一种模型，可以在粒子探测器缺乏脉冲的情况下生成具有相同形状的脉冲，并与真实数据分布相匹配。

    

    为了解决在开发相关的电子设备期间粒子探测器可能缺乏或完全没有脉冲的问题，我们提出了一个模型，可以在不失去真实脉冲特征的情况下生成它们。该模型基于人工神经网络，即生成对抗网络（GAN）。我们描述了所提出的网络架构，其训练方法和使用来自显示闪烁晶体由${}^{137}$Cs和${}^{22}$Na放射源接收的真实脉冲来训练GAN的方法。生成器被安装在Xilinx的片上系统（SoC）中。我们展示了该网络能够生成与真实脉冲相同形状的脉冲，甚至与原始脉冲高度直方图数据的数据分布相匹配。

    To address the possible lack or total absence of pulses from particle detectors during the development of its associate electronics, we propose a model that can generate them without losing the features of the real ones. This model is based on artificial neural networks, namely Generative Adversarial Networks (GAN). We describe the proposed network architecture, its training methodology and the approach to train the GAN with real pulses from a scintillator receiving radiation from sources of ${}^{137}$Cs and ${}^{22}$Na. The Generator was installed in a Xilinx's System-On-Chip (SoC). We show how the network is capable of generating pulses with the same shape as the real ones that even match the data distributions in the original pulse-height histogram data.
    
[^6]: AUTOACT：通过自主规划实现的自动代理学习

    AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])

    [http://arxiv.org/abs/2401.05268](http://arxiv.org/abs/2401.05268)

    AUTOACT是一个自动代理学习框架，通过自主规划合成轨迹，不依赖于大规模数据和闭源模型，能够实现更好或类似的性能。

    

    语言代理在各种复杂任务上取得了相当的性能。尽管在这个领域进行了不断的探索，但现有的语言代理系统仍然面临昂贵、不可重复的数据依赖问题，并且面临将单一模型应用于多个功能的挑战。为此，我们介绍了AutoAct，这是一个自动代理学习框架，不依赖于大规模带注释的数据和来自闭源模型（如GPT-4）的合成轨迹。给定有限的数据和工具库，AutoAct首先自动合成规划轨迹，不需要人类或强闭源模型的任何辅助。然后，AutoAct利用分工策略，根据目标任务信息和合成轨迹自动区分，产生一个子代理组来完成任务。我们进行了多种LLMs的广泛实验，结果显示AutoAct在性能上优于或与其相当。

    Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
    
[^7]: ReACT: 使用B样条几何的强化学习在控制器参数化中的应用

    ReACT: Reinforcement Learning for Controller Parametrization using B-Spline Geometries. (arXiv:2401.05251v1 [cs.LG])

    [http://arxiv.org/abs/2401.05251](http://arxiv.org/abs/2401.05251)

    本研究提出了一种使用深度强化学习和N维B样条几何进行控制器参数化的方法。通过自主决定控制器参数的调整，该方法可以简化对于复杂和非线性系统的控制器参数化过程。

    

    强大且高效的控制器对于工业应用至关重要。然而，对于复杂和非线性系统导出控制器参数是具有挑战性且耗时的。为了便于自动控制器参数化，本研究提出了一种新的方法，使用深度强化学习（DRL）和N维B样条几何（BSG）。我们关注参数变化系统的控制，这是一类行为复杂且取决于运行条件的系统。对于这一系统类别，由于其已知的设计原理，增益调度控制结构被广泛应用于各个行业的应用中。为了简化对于这些控制结构的控制器参数化任务，我们部署了一个DRL代理。基于控制系统观测，代理自主决定如何调整控制器参数。通过引入BSG来映射可能依赖于多个变量的控制器参数，我们使得适应过程更加高效。

    Robust and performant controllers are essential for industrial applications. However, deriving controller parameters for complex and nonlinear systems is challenging and time-consuming. To facilitate automatic controller parametrization, this work presents a novel approach using deep reinforcement learning (DRL) with N-dimensional B-spline geometries (BSGs). We focus on the control of parameter-variant systems, a class of systems with complex behavior which depends on the operating conditions. For this system class, gain-scheduling control structures are widely used in applications across industries due to well-known design principles. Facilitating the expensive controller parametrization task regarding these control structures, we deploy an DRL agent. Based on control system observations, the agent autonomously decides how to adapt the controller parameters. We make the adaptation process more efficient by introducing BSGs to map the controller parameters which may depend on numerous 
    
[^8]: 使用Hamilton神经网络的子集模拟进行复杂系统可靠性分析

    Reliability Analysis of Complex Systems using Subset Simulations with Hamiltonian Neural Networks. (arXiv:2401.05244v1 [stat.ML])

    [http://arxiv.org/abs/2401.05244](http://arxiv.org/abs/2401.05244)

    该论文提出了一种使用基于Hamilton神经网络的Monte Carlo采样的子集模拟方法来进行复杂系统可靠性分析。该方法通过将优越的采样和高效的梯度评估相结合，实现了高接受率和低计算成本。在不同的可靠性问题上展示了其显著的准确性，并与传统的Hamilton Monte Carlo方法进行了比较。此方法在梯度评估方面存在一定限制。

    

    我们提出了一种新的使用基于Hamilton神经网络的Monte Carlo采样的子集模拟方法，用于可靠性分析。所提出的策略将Hamilton Monte Carlo方法的优越采样与使用Hamilton神经网络进行计算高效梯度评估相结合。这种组合特别有优势，因为神经网络结构保持了Hamiltonian的特性，而Hamiltonian定义了Hamilton Monte Carlo采样器的接受准则。因此，这种策略在低计算成本下可以实现高接受率。我们的方法使用子集模拟来估计小概率失效。然而，在低概率样本区域中，梯度评估尤其具有挑战性。我们展示了所提出策略的显著准确性，并将其效率与传统的Hamilton Monte Carlo方法进行比较。我们注意到，这种方法在梯度评估方面存在限制。

    We present a new Subset Simulation approach using Hamiltonian neural network-based Monte Carlo sampling for reliability analysis. The proposed strategy combines the superior sampling of the Hamiltonian Monte Carlo method with computationally efficient gradient evaluations using Hamiltonian neural networks. This combination is especially advantageous because the neural network architecture conserves the Hamiltonian, which defines the acceptance criteria of the Hamiltonian Monte Carlo sampler. Hence, this strategy achieves high acceptance rates at low computational cost. Our approach estimates small failure probabilities using Subset Simulations. However, in low-probability sample regions, the gradient evaluation is particularly challenging. The remarkable accuracy of the proposed strategy is demonstrated on different reliability problems, and its efficiency is compared to the traditional Hamiltonian Monte Carlo method. We note that this approach can reach its limitations for gradient es
    
[^9]: 通过分类器校准实现反欺诈预防的决策解耦

    Decoupling Decision-Making in Fraud Prevention through Classifier Calibration for Business Logic Action. (arXiv:2401.05240v1 [cs.LG])

    [http://arxiv.org/abs/2401.05240](http://arxiv.org/abs/2401.05240)

    该论文研究了通过分类器校准来实现反欺诈预防中的决策解耦。通过使用校准策略，他们发现等距和贝塔校准方法在训练和测试数据之间发生变化的场景下表现突出。这些结果为优化解耦努力的从业者提供了宝贵的见解。

    

    机器学习模型通常专注于特定目标，比如创建分类器，通常基于商业环境中已知的人群特征分布。然而，计算个体特征的模型随时间而适应，以提高精度，引入解耦的概念：从点评估转向数据分布。我们使用校准策略作为解耦机器学习（ML）分类器与基于得分的业务逻辑框架中的行动的策略。为了评估这些策略，我们使用一个真实的商业场景和多个ML模型进行了比较分析。我们的发现突出了这种方法的权衡和性能影响，并为寻求优化解耦努力的从业者提供了有价值的见解。特别是在训练和测试数据之间存在转变的情况下，等距和贝塔校准方法表现出色。

    Machine learning models typically focus on specific targets like creating classifiers, often based on known population feature distributions in a business context. However, models calculating individual features adapt over time to improve precision, introducing the concept of decoupling: shifting from point evaluation to data distribution. We use calibration strategies as strategy for decoupling machine learning (ML) classifiers from score-based actions within business logic frameworks. To evaluate these strategies, we perform a comparative analysis using a real-world business scenario and multiple ML models. Our findings highlight the trade-offs and performance implications of the approach, offering valuable insights for practitioners seeking to optimize their decoupling efforts. In particular, the Isotonic and Beta calibration methods stand out for scenarios in which there is shift between training and testing data.
    
[^10]: 驯服“数据饥渴”的强化学习？在连续状态-动作空间中的稳定性

    Taming "data-hungry" reinforcement learning? Stability in continuous state-action spaces. (arXiv:2401.05233v1 [cs.LG])

    [http://arxiv.org/abs/2401.05233](http://arxiv.org/abs/2401.05233)

    本文介绍了一个在连续状态-动作空间中分析强化学习的新框架，并证明了在离线和在线设置中具有快速收敛速度。分析发现了两个稳定性属性，与值函数和/或策略变化如何影响贝尔曼算子和占据度测度相关。这些属性在许多连续状态-动作马尔可夫决策过程中成立，并且在线性函数逼近方法下自然产生。分析还揭示了离线和在线强化学习中悲观主义和乐观主义的角色，以及离线强化学习和迁移学习之间的联系。

    

    我们引入了一个在连续状态-动作空间中分析强化学习的新框架，并将其用于证明离线和在线设置中的快速收敛速度。我们的分析突出了两个关键的稳定性属性，涉及值函数和/或策略变化如何影响贝尔曼算子和占据度测度。我们认为这些属性在许多连续状态-动作马尔可夫决策过程中成立，并演示了在使用线性函数逼近方法时如何自然地产生这些属性。我们的分析提供了关于离线和在线强化学习中悲观主义和乐观主义的新视角，并强调了离线强化学习和迁移学习之间的联系。

    We introduce a novel framework for analyzing reinforcement learning (RL) in continuous state-action spaces, and use it to prove fast rates of convergence in both off-line and on-line settings. Our analysis highlights two key stability properties, relating to how changes in value functions and/or policies affect the Bellman operator and occupation measures. We argue that these properties are satisfied in many continuous state-action Markov decision processes, and demonstrate how they arise naturally when using linear function approximation methods. Our analysis offers fresh perspectives on the roles of pessimism and optimism in off-line and on-line RL, and highlights the connection between off-line RL and transfer learning.
    
[^11]: 视觉和语言编码器是否以相似方式表示世界？

    Do Vision and Language Encoders Represent the World Similarly?. (arXiv:2401.05224v1 [cs.CV])

    [http://arxiv.org/abs/2401.05224](http://arxiv.org/abs/2401.05224)

    通过分析视觉和语言模型的潜在空间结构，发现未对齐和对齐的编码器的表示空间在语义上是相似的。我们提出了两种方法来匹配未对齐编码器，无需训练即可实现匹配。

    

    已经成为视觉语言任务中事实上的模型的对齐的文本-图像编码器（如CLIP）已经取得了令人印象深刻的表现。此外，模态特定的编码器在各自领域中也取得了令人印象深刻的表现。这引出了一个核心问题：由于它们基本上表示同一个物理世界，单模态的视觉和语言编码器之间是否存在对齐？通过使用中心核对齐（CKA）分析图像-标题基准上视觉和语言模型的潜在空间结构，我们发现未对齐和对齐的编码器的表示空间在语义上是相似的。在像CLIP这样的对齐编码器中缺乏统计相似性的情况下，我们显示了可能存在无需任何训练的未对齐编码器的匹配。我们将这视为利用图之间的语义相似性的有种子图匹配问题，并提出了两种方法 - 快速二次分配问题优化和一种基于新颖的局部CKA度量的匹配/检索方法。

    Aligned text-image encoders such as CLIP have become the de facto model for vision-language tasks. Furthermore, modality-specific encoders achieve impressive performances in their respective domains. This raises a central question: does an alignment exist between uni-modal vision and language encoders since they fundamentally represent the same physical world? Analyzing the latent spaces structure of vision and language models on image-caption benchmarks using the Centered Kernel Alignment (CKA), we find that the representation spaces of unaligned and aligned encoders are semantically similar. In the absence of statistical similarity in aligned encoders like CLIP, we show that a possible matching of unaligned encoders exists without any training. We frame this as a seeded graph-matching problem exploiting the semantic similarity between graphs and propose two methods - a Fast Quadratic Assignment Problem optimization, and a novel localized CKA metric-based matching/retrieval. We demons
    
[^12]: 具有局部线性模型的不变因果预测

    Invariant Causal Prediction with Locally Linear Models. (arXiv:2401.05218v1 [cs.LG])

    [http://arxiv.org/abs/2401.05218](http://arxiv.org/abs/2401.05218)

    本文扩展了ICP原则，考虑了在不同环境下具有局部线性模型的不变因果预测任务。通过提供因果父节点的可辨识性条件和引入LoLICaP方法，实现了在观察数据中识别目标变量的因果父节点。

    

    本文考虑通过观察数据，从一组候选变量中识别出目标变量的因果父节点的任务。我们的主要假设是候选变量在不同的环境中被观察到，这些环境可以对应于机器的不同设置或者动态过程中的不同时间间隔等。在一定的假设条件下，不同的环境可以被视为对观察系统的干预。我们假设目标变量和协变量之间存在线性关系，在每个环境下可能不同，但因果结构在不同环境中是不变的。这是Peters等人[2016]提出的ICP（不变因果预测）原则的扩展，后者假设所有环境下存在一个固定的线性关系。在我们提出的设置下，我们给出了因果父节点可辨识性的充分条件，并引入了一个名为LoLICaP的实用方法。

    We consider the task of identifying the causal parents of a target variable among a set of candidate variables from observational data. Our main assumption is that the candidate variables are observed in different environments which may, for example, correspond to different settings of a machine or different time intervals in a dynamical process. Under certain assumptions different environments can be regarded as interventions on the observed system. We assume a linear relationship between target and covariates, which can be different in each environment with the only restriction that the causal structure is invariant across environments. This is an extension of the ICP ($\textbf{I}$nvariant $\textbf{C}$ausal $\textbf{P}$rediction) principle by Peters et al. [2016], who assumed a fixed linear relationship across all environments. Within our proposed setting we provide sufficient conditions for identifiability of the causal parents and introduce a practical method called LoLICaP ($\text
    
[^13]: 使用隐式龙格-库塔方法的物理知识神经网络的误差估计

    Error estimation for physics-informed neural networks with implicit Runge-Kutta methods. (arXiv:2401.05211v1 [physics.comp-ph])

    [http://arxiv.org/abs/2401.05211](http://arxiv.org/abs/2401.05211)

    本研究提出使用高阶隐式龙格-库塔方法来估计物理知识神经网络的误差，在轨迹的多个点上提供精确的误差估计。通过与神经网络的预测误差高度相关，增加IRK方法的阶数可以进一步改善估计的准确性。

    

    准确近似动力系统的轨迹能够实现对其分析、预测和控制。基于神经网络的近似具有快速评估和长时间步长的准确性，因此受到了广泛关注。与传统的龙格-库塔方法等数值近似方案相比，基于神经网络的近似误差估计相对困难。在这项工作中，我们提出使用神经网络的预测结果在高阶隐式龙格-库塔 (IRK)方法中。隐式方程组中的残差可以与神经网络的预测误差相关联，因此我们可以在轨迹的多个点上提供误差估计。我们发现，这个误差估计与神经网络的预测误差高度相关，并且增加IRK方法的阶数可以改善这个估计。我们以物理知识神经网络 (PINNs)在逻辑方程上进行了这个估计方法的演示。

    The ability to accurately approximate trajectories of dynamical systems enables their analysis, prediction, and control. Neural network (NN)-based approximations have attracted significant interest due to fast evaluation with good accuracy over long integration time steps. In contrast to established numerical approximation schemes such as Runge-Kutta methods, the estimation of the error of the NN-based approximations proves to be difficult. In this work, we propose to use the NN's predictions in a high-order implicit Runge-Kutta (IRK) method. The residuals in the implicit system of equations can be related to the NN's prediction error, hence, we can provide an error estimate at several points along a trajectory. We find that this error estimate highly correlates with the NN's prediction error and that increasing the order of the IRK method improves this estimate. We demonstrate this estimation methodology for Physics-Informed Neural Network (PINNs) on the logistic equation as an illust
    
[^14]: 使用扩散模型定制表面摩擦性能

    Tailoring Frictional Properties of Surfaces Using Diffusion Models. (arXiv:2401.05206v1 [physics.comp-ph])

    [http://arxiv.org/abs/2401.05206](http://arxiv.org/abs/2401.05206)

    本研究介绍了一种使用机器学习模型设计表面摩擦性能的方法，通过训练模型可以直接得到符合摩擦要求的表面设计。

    

    本文介绍了一种使用条件生成机器学习模型（具体是扩散去噪概率模型DDPM）精确设计表面摩擦性能的方法。我们创建了一个由分子动力学模拟确定摩擦性质的合成表面数据集，通过训练DDPM来预测与所期望的摩擦性能相对应的表面结构。与传统的试错和数值优化方法不同，我们的方法能够以高准确性和高效率直接得到符合指定摩擦标准的表面设计。这一材料表面工程的进展展示了机器学习在减少表面设计过程中迭代性质的潜力。我们的研究结果不仅为精确定制表面性能提供了一条新途径，而且还提出了在表面特性关键的材料科学领域的更广泛应用。

    This Letter introduces an approach for precisely designing surface friction properties using a conditional generative machine learning model, specifically a diffusion denoising probabilistic model (DDPM). We created a dataset of synthetic surfaces with frictional properties determined by molecular dynamics simulations, which trained the DDPM to predict surface structures from desired frictional outcomes. Unlike traditional trial-and-error and numerical optimization methods, our approach directly yields surface designs meeting specified frictional criteria with high accuracy and efficiency. This advancement in material surface engineering demonstrates the potential of machine learning in reducing the iterative nature of surface design processes. Our findings not only provide a new pathway for precise surface property tailoring but also suggest broader applications in material science where surface characteristics are critical.
    
[^15]: 使用函数逼近进行实验规划

    Experiment Planning with Function Approximation. (arXiv:2401.05193v1 [cs.LG])

    [http://arxiv.org/abs/2401.05193](http://arxiv.org/abs/2401.05193)

    本研究探讨了在上下文关联赌博问题中使用函数逼近进行实验规划的问题，并提出了两种与函数逼近兼容的实验规划策略。

    

    我们研究了在上下文关联赌博问题中使用函数逼近进行实验规划的问题。在存在部署自适应算法的显著开销的情况下，例如当执行数据收集策略需要分布式或需要人工参与时，提前生成一组数据收集策略是至关重要的。我们研究了一个大型上下文数据集可用但奖励数据不可用的情景，学习者可以利用该数据集设计一个有效的数据收集策略。虽然当奖励是线性的时候，这个问题已经被广泛研究，但对于更复杂的奖励模型，仍然缺乏结果。在这项工作中，我们提出了两种与函数逼近兼容的实验规划策略。第一种是逃避者规划和采样过程，可以根据逃避者维度的奖励函数类获得最优性保证。对于第二种策略，我们证明了一个...

    We study the problem of experiment planning with function approximation in contextual bandit problems. In settings where there is a significant overhead to deploying adaptive algorithms -- for example, when the execution of the data collection policies is required to be distributed, or a human in the loop is needed to implement these policies -- producing in advance a set of policies for data collection is paramount. We study the setting where a large dataset of contexts but not rewards is available and may be used by the learner to design an effective data collection strategy. Although when rewards are linear this problem has been well studied, results are still missing for more complex reward models. In this work we propose two experiment planning strategies compatible with function approximation. The first is an eluder planning and sampling procedure that can recover optimality guarantees depending on the eluder dimension of the reward function class. For the second, we show that a 
    
[^16]: 联邦遗忘：方法、设计准则和评估指标的综述

    Federated Unlearning: A Survey on Methods, Design Guidelines, and Evaluation Metrics. (arXiv:2401.05146v1 [cs.LG])

    [http://arxiv.org/abs/2401.05146](http://arxiv.org/abs/2401.05146)

    这篇综述论文介绍了联邦遗忘的概念和挑战，以及解决这些问题的方法和设计准则，旨在为联邦学习中保护用户隐私和防止恶意攻击提供解决方案。

    

    联邦学习使得多个参与方能够协同训练一个机器学习模型，通过保留数据在本地存储，从而维护了用户和机构的隐私。与集中化原始数据不同，联邦学习通过交换本地优化的模型参数来逐步构建全局模型。尽管联邦学习更加符合新兴规定，如欧洲通用数据保护条例（GDPR），但在此背景下确保遗忘权——允许联邦学习参与方从学习的模型中删除他们的数据贡献仍然不明确。此外，人们认识到恶意客户端可能通过更新将后门注入全局模型，例如对特制数据示例进行错误预测。因此，需要机制来确保个人有可能在聚合后移除他们的数据并清除恶意贡献，而不损害已获得的"全

    Federated Learning (FL) enables collaborative training of a Machine Learning (ML) model across multiple parties, facilitating the preservation of users' and institutions' privacy by keeping data stored locally. Instead of centralizing raw data, FL exchanges locally refined model parameters to build a global model incrementally. While FL is more compliant with emerging regulations such as the European General Data Protection Regulation (GDPR), ensuring the right to be forgotten in this context - allowing FL participants to remove their data contributions from the learned model - remains unclear. In addition, it is recognized that malicious clients may inject backdoors into the global model through updates, e.g. to generate mispredictions on specially crafted data examples. Consequently, there is the need for mechanisms that can guarantee individuals the possibility to remove their data and erase malicious contributions even after aggregation, without compromising the already acquired "g
    
[^17]: 机器学习推动转化研究：预测痴呆研究中的专利和临床试验收录

    Machine Learning to Promote Translational Research: Predicting Patent and Clinical Trial Inclusion in Dementia Research. (arXiv:2401.05145v1 [cs.LG])

    [http://arxiv.org/abs/2401.05145](http://arxiv.org/abs/2401.05145)

    通过利用机器学习预测痴呆研究的转化潜力，这项开创性研究有望解决痴呆研究从基础发现到实际应用转化速度较慢的问题。

    

    预计到2040年，痴呆症将影响英国160万人，并每年耗费250亿英镑，给社会带来日益增长的挑战。本研究是一项开创性的尝试，利用机器学习来预测痴呆研究的转化潜力，并希望解决尽管痴呆症对社会和经济具有重要影响，但基础发现转化为实际应用的速度较慢的问题。我们使用Dimensions数据库提取了1990-2023年之间43,091篇英国痴呆研究出版物的数据，包括元数据（作者、出版年等）、论文中提到的概念以及论文摘要。为了为机器学习准备数据，我们应用了诸如one hot编码和/或词嵌入等方法。我们训练了一个CatBoost分类器来预测一篇论文是否会被引用在未来的专利或临床试验中。我们训练了几种不同的模型变体。结合元数据、概念和摘要的嵌入模型产生了最高的per

    Projected to impact 1.6 million people in the UK by 2040 and costing {\pounds}25 billion annually, dementia presents a growing challenge to society. This study, a pioneering effort to predict the translational potential of dementia research using machine learning, hopes to address the slow translation of fundamental discoveries into practical applications despite dementia's significant societal and economic impact. We used the Dimensions database to extract data from 43,091 UK dementia research publications between the years 1990-2023, specifically metadata (authors, publication year etc.), concepts mentioned in the paper, and the paper abstract. To prepare the data for machine learning we applied methods such as one hot encoding and/or word embeddings. We trained a CatBoost Classifier to predict if a publication will be cited in a future patent or clinical trial. We trained several model variations. The model combining metadata, concept, and abstract embeddings yielded the highest per
    
[^18]: 高效领域适应下的隐私保护视觉Transformer的精调方法

    Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving Vision Transformer. (arXiv:2401.05126v1 [cs.CV])

    [http://arxiv.org/abs/2401.05126](http://arxiv.org/abs/2401.05126)

    本论文提出了一种高效的领域适应方法，用于训练和测试隐私保护的视觉Transformer模型，并避免了使用加密图像导致的性能下降。实验结果表明，在图像分类任务上，该方法在CIFAR-10和ImageNet数据集上表现出更高的准确度。

    

    我们提出了一种新颖的方法，用于使用视觉Transformer（ViT）进行隐私保护的深度神经网络（DNN）。该方法不仅可以用视觉保护的图像训练模型并进行测试，而且还可以避免使用加密图像导致的性能下降，而传统方法不能避免图像加密的影响。通过领域适应方法，可以高效地对使用加密图像的ViT进行精细调整。在实验中，该方法在CIFAR-10和ImageNet数据集上的图像分类任务中表现出优于传统方法的分类准确度。

    We propose a novel method for privacy-preserving deep neural networks (DNNs) with the Vision Transformer (ViT). The method allows us not only to train models and test with visually protected images but to also avoid the performance degradation caused from the use of encrypted images, whereas conventional methods cannot avoid the influence of image encryption. A domain adaptation method is used to efficiently fine-tune ViT with encrypted images. In experiments, the method is demonstrated to outperform conventional methods in an image classification task on the CIFAR-10 and ImageNet datasets in terms of classification accuracy.
    
[^19]: 光子学用于可持续计算

    Photonics for Sustainable Computing. (arXiv:2401.05121v1 [cs.ET])

    [http://arxiv.org/abs/2401.05121](http://arxiv.org/abs/2401.05121)

    光子集成电路在多个应用中得以应用，尤其是在机器学习推断方面具有高能量效率。然而，硬件制造和基础设施会造成大量碳排放，因此需要对光子学的制造和运营碳成本进行考虑，以确定其在可持续未来中是否可行。

    

    光子集成电路在光学收发器、激光雷达、生物传感、光子量子计算和机器学习等多个应用中得到了应用。特别是随着机器学习模型规模的指数增长，基于光子的加速器因能够比基于CMOS的加速器以多个数量级更高的能量效率进行机器学习推断而受到特别关注，成为可持续解决方案。然而，最近的研究显示，硬件制造和基础设施对计算设备的碳足迹有着显著贡献，甚至超过了使用过程中产生的排放。例如，2019年苹果公司的总碳排放量中，制造过程占据了74％。这促使我们产生一个问题 - 如果我们考虑光子学的制造和运营碳成本，它是否确实是可持续未来的途径？因此，在本文中，我们建立了一个碳足迹模型...

    Photonic integrated circuits are finding use in a variety of applications including optical transceivers, LIDAR, bio-sensing, photonic quantum computing, and Machine Learning (ML). In particular, with the exponentially increasing sizes of ML models, photonics-based accelerators are getting special attention as a sustainable solution because they can perform ML inferences with multiple orders of magnitude higher energy efficiency than CMOS-based accelerators. However, recent studies have shown that hardware manufacturing and infrastructure contribute significantly to the carbon footprint of computing devices, even surpassing the emissions generated during their use. For example, the manufacturing process accounts for 74% of the total carbon emissions from Apple in 2019. This prompts us to ask -- if we consider both the embodied (manufacturing) and operational carbon cost of photonics, is it indeed a viable avenue for a sustainable future? So, in this paper, we build a carbon footprint m
    
[^20]: 基于自监督语音表征模型的抗噪零样本文本到语音合成方法

    Noise-robust zero-shot text-to-speech synthesis conditioned on self-supervised speech-representation model with adapters. (arXiv:2401.05111v1 [cs.SD])

    [http://arxiv.org/abs/2401.05111](http://arxiv.org/abs/2401.05111)

    本论文提出了一种基于自监督学习的抗噪零样本文本到语音合成方法。通过在自监督学习模型中引入适配器，并使用带有噪声参考语音对TTS模型进行微调，以及采用语音增强前端，我们实现了高质量的语音合成，对参考语音中的噪声具有高度鲁棒性。

    

    基于自监督学习（SSL）语音表征提取的说话人嵌入向量的零样本文本到语音（TTS）方法可以非常准确地复制说话人的特征。然而，当参考语音中含有噪声时，该方法的语音合成质量会下降。本文提出了一种抗噪零样本TTS方法。我们将适配器引入到SSL模型中，并使用带有噪声参考语音的TTS模型进行微调。此外，为了进一步提高性能，我们采用了语音增强（SE）前端。通过这些改进，我们提出的基于SSL的零样本TTS在噪声参考语音下实现了高质量的语音合成。通过客观和主观评估，我们确认了该方法对参考语音中的噪声具有高度的鲁棒性，并且与SE结合有效地运作。

    The zero-shot text-to-speech (TTS) method, based on speaker embeddings extracted from reference speech using self-supervised learning (SSL) speech representations, can reproduce speaker characteristics very accurately. However, this approach suffers from degradation in speech synthesis quality when the reference speech contains noise. In this paper, we propose a noise-robust zero-shot TTS method. We incorporated adapters into the SSL model, which we fine-tuned with the TTS model using noisy reference speech. In addition, to further improve performance, we adopted a speech enhancement (SE) front-end. With these improvements, our proposed SSL-based zero-shot TTS achieved high-quality speech synthesis with noisy reference speech. Through the objective and subjective evaluations, we confirmed that the proposed method is highly robust to noise in reference speech, and effectively works in combination with SE.
    
[^21]: 任意方式元学习

    Any-Way Meta Learning. (arXiv:2401.05097v1 [cs.LG])

    [http://arxiv.org/abs/2401.05097](http://arxiv.org/abs/2401.05097)

    本文通过引入“任意方式”学习范式解决了元学习中固定基数的限制，并通过利用从标签分配中出现的“标签等价性”来提高模型的性能和稳定性。同时提出了一个新的方法来弥补标签等价性带来的语义信息差距。

    

    尽管元学习在快速适应性方面表现出了很大的潜力，但它受到固定基数的限制。当面临训练过程中未见过的基数不同的任务时，模型就无法胜任。本文通过利用从随机数值标签分配中出现的“标签等价性”来解决这一挑战。质疑“真正的”元学习的定义，我们引入了“任意方式”学习范式，这是一种创新的模型训练方法，使模型摆脱了固定基数的限制。令人惊讶的是，这个模型不仅在性能、收敛速度和稳定性方面与传统的固定方式模型相匹配，而且通常表现得更好。这颠覆了关于领域泛化的已有观念。此外，我们认为固有的标签等价性自然地缺乏语义信息。为了弥补标签等价性带来的这种语义信息差距，我们进一步提出了一个新的方法。

    Although meta-learning seems promising performance in the realm of rapid adaptability, it is constrained by fixed cardinality. When faced with tasks of varying cardinalities that were unseen during training, the model lacks its ability. In this paper, we address and resolve this challenge by harnessing `label equivalence' emerged from stochastic numeric label assignments during episodic task sampling. Questioning what defines ``true" meta-learning, we introduce the ``any-way" learning paradigm, an innovative model training approach that liberates model from fixed cardinality constraints. Surprisingly, this model not only matches but often outperforms traditional fixed-way models in terms of performance, convergence speed, and stability. This disrupts established notions about domain generalization. Furthermore, we argue that the inherent label equivalence naturally lacks semantic information. To bridge this semantic information gap arising from label equivalence, we further propose a m
    
[^22]: 基于句子嵌入的职位广告中横向技能的层次分类

    Hierarchical Classification of Transversal Skills in Job Ads Based on Sentence Embeddings. (arXiv:2401.05073v1 [cs.LG])

    [http://arxiv.org/abs/2401.05073](http://arxiv.org/abs/2401.05073)

    本论文提出了一个基于深度学习模型的分类框架，用于识别职位广告要求和横向技能集之间的相关性，并预测个别工作描述所需的技能。通过使用层次分类和多标签策略，并采用增强技术解决数据不平衡问题，该方法在欧洲就业市场具有良好的效果。

    

    本文提出了一个分类框架，旨在识别职位广告要求和横向技能集之间的相关性，重点是使用深度学习模型预测个别工作描述所需的技能。该方法涉及数据收集、预处理和使用ESCO（欧洲技能、能力和职业）分类系统进行标注。在技能识别方面，采用了层次分类和多标签策略，而增强技术则解决了数据不平衡问题，提高了模型的稳健性。通过比较使用英语特定和多语言句子嵌入模型得到的结果，发现其准确度相近。实验案例研究详细说明了神经网络配置、超参数和交叉验证结果，突显了层次化方法的功效以及多语言模型适用于多样化的欧洲就业市场。因此，提供了一种新的层次分类方法。

    This paper proposes a classification framework aimed at identifying correlations between job ad requirements and transversal skill sets, with a focus on predicting the necessary skills for individual job descriptions using a deep learning model. The approach involves data collection, preprocessing, and labeling using ESCO (European Skills, Competences, and Occupations) taxonomy. Hierarchical classification and multi-label strategies are used for skill identification, while augmentation techniques address data imbalance, enhancing model robustness. A comparison between results obtained with English-specific and multi-language sentence embedding models reveals close accuracy. The experimental case studies detail neural network configurations, hyperparameters, and cross-validation results, highlighting the efficacy of the hierarchical approach and the suitability of the multi-language model for the diverse European job market. Thus, a new approach is proposed for the hierarchical classifi
    
[^23]: MISS: 多类可解释评分系统

    MISS: Multiclass Interpretable Scoring Systems. (arXiv:2401.05069v1 [cs.LG])

    [http://arxiv.org/abs/2401.05069](http://arxiv.org/abs/2401.05069)

    本文提出了一种全新的机器学习方法，构建了多类可解释评分系统（MISS），用于解决多类分类问题，该方法可将评分系统转换为类别概率，具有高效的训练和优化能力。

    

    本文提出了一种新颖的机器学习方法，用于构建多类可解释评分系统（MISS）- 一种完全数据驱动的方法，用于生成单一、稀疏和用户友好的多类分类问题的评分系统。评分系统通常用作医疗保健、刑事司法和其他领域的决策支持模型，其中预测的解释性和易用性至关重要。以前用于数据驱动评分的方法，如SLIM（超稀疏线性整数模型），仅限于二元分类任务，而对于多类领域的扩展主要通过一对多技术实现。我们的方法产生的评分可以通过softmax函数轻松转换为类别概率。我们演示了降维和启发式技术，提高了训练效率并减少了最佳模型之间的差距，该差距可以证明模型的最佳性。

    In this work, we present a novel, machine-learning approach for constructing Multiclass Interpretable Scoring Systems (MISS) - a fully data-driven methodology for generating single, sparse, and user-friendly scoring systems for multiclass classification problems. Scoring systems are commonly utilized as decision support models in healthcare, criminal justice, and other domains where interpretability of predictions and ease of use are crucial. Prior methods for data-driven scoring, such as SLIM (Supersparse Linear Integer Model), were limited to binary classification tasks and extensions to multiclass domains were primarily accomplished via one-versus-all-type techniques. The scores produced by our method can be easily transformed into class probabilities via the softmax function. We demonstrate techniques for dimensionality reduction and heuristics that enhance the training efficiency and decrease the optimality gap, a measure that can certify the optimality of the model. Our approach 
    
[^24]: 使用自监督技术进行歌手身份表示学习

    Singer Identity Representation Learning using Self-Supervised Techniques. (arXiv:2401.05064v1 [cs.SD])

    [http://arxiv.org/abs/2401.05064](http://arxiv.org/abs/2401.05064)

    本论文提出了一个框架，通过使用自监督技术在大量的独立音轨上训练，以提取适用于唱歌相关任务的高质量歌手身份表示。实验证明，这些表示在多个数据集上优于现有的基线方法，并具备领域外泛化能力。

    

    在使用语音数据创建声音身份表示方面已经取得了重要进展。然而，对于唱歌声音，还没有取得同样的进展。为了弥补这一差距，我们提出了一个框架，用于训练歌手身份编码器，以提取适用于各种唱歌相关任务（如唱歌声音相似性和合成）的表示。我们在大量的独立音轨上探索了不同的自监督学习技术，并在训练期间应用数据增强，以确保表示对音高和内容变化不变。我们在多个数据集上评估了所得到表示的质量，特别注重领域外泛化能力。我们提出的框架在44.1 kHz条件下产生了高质量的嵌入，优于说话人验证和wav2vec 2.0预训练基线，并在唱歌声音上操作。我们发布了我们的代码和...

    Significant strides have been made in creating voice identity representations using speech data. However, the same level of progress has not been achieved for singing voices. To bridge this gap, we suggest a framework for training singer identity encoders to extract representations suitable for various singing-related tasks, such as singing voice similarity and synthesis. We explore different self-supervised learning techniques on a large collection of isolated vocal tracks and apply data augmentations during training to ensure that the representations are invariant to pitch and content variations. We evaluate the quality of the resulting representations on singer similarity and identification tasks across multiple datasets, with a particular emphasis on out-of-domain generalization. Our proposed framework produces high-quality embeddings that outperform both speaker verification and wav2vec 2.0 pre-trained baselines on singing voice while operating at 44.1 kHz. We release our code and
    
[^25]: 内容感知的深度自适应图像修复

    Content-Aware Depth-Adaptive Image Restoration. (arXiv:2401.05049v1 [cs.CV])

    [http://arxiv.org/abs/2401.05049](http://arxiv.org/abs/2401.05049)

    这项工作提出了一种模块化的图像修复流程，通过利用现有模型、对象级别的修复和用户自定义的步骤顺序，以及深度感知优化生成的图像，从而实现了对整个修复过程的完全用户控制。这个系统具有很强的适应性，可以针对特定的对象类别实现图像修复。

    

    这项工作的重点是建立一个模块化的流程，利用现有模型来系统地修复图像，而不是从头开始创建新的修复模型。修复是在对象级别进行的，每个对象使用其对应的类别标签信息进行重新生成。该方法通过提供完全用户控制整个修复过程而脱颖而出。用户可以选择专门的修复步骤模型，自定义步骤的顺序以满足他们的需求，并利用深度感知对生成的图像进行进一步的优化。研究提供了两种不同的图像修复实施路径，以比较它们各自的优势和限制。这个多功能系统最吸引人的方面是它的适应性。这种适应性使用户能够针对特定的对象类别（包括医学图像）提供在这些对象类别上训练的模型。

    This work prioritizes building a modular pipeline that utilizes existing models to systematically restore images, rather than creating new restoration models from scratch. Restoration is carried out at an object-specific level, with each object regenerated using its corresponding class label information. The approach stands out by providing complete user control over the entire restoration process. Users can select models for specialized restoration steps, customize the sequence of steps to meet their needs, and refine the resulting regenerated image with depth awareness. The research provides two distinct pathways for implementing image regeneration, allowing for a comparison of their respective strengths and limitations. The most compelling aspect of this versatile system is its adaptability. This adaptability enables users to target particular object categories, including medical images, by providing models that are trained on those object classes.
    
[^26]: CreINNs: Credal-Set Interval Neural Networks用于分类任务中的不确定性估计

    CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation in Classification Tasks. (arXiv:2401.05043v1 [cs.LG])

    [http://arxiv.org/abs/2401.05043](http://arxiv.org/abs/2401.05043)

    CreINNs是一种用于分类任务的Credal-Set Interval Neural Networks，通过保留传统的区间神经网络结构，捕捉权重不确定性，并使用概率区间的数学框架预测可信区间。实验结果表明，CreINNs在不确定性估计方面优于变分贝叶斯神经网络和深度集成，并且具有较低的计算复杂度和模型大小。

    

    不确定性估计对于提高神经网络的可靠性越来越有吸引力。在这项工作中，我们提出了新颖的Credal-Set Interval Neural Networks（CreINNs），用于分类任务。CreINNs保留了传统的区间神经网络结构，通过确定性区间捕捉权重的不确定性，同时使用概率区间的数学框架预测可信区间。在一个超出分发检测基准（CIFAR10 vs SVHN）上的实验验证中，CreINNs相比于变分贝叶斯神经网络（BNNs）和深度集成（DEs），在认知不确定性估计方面表现出色。此外，与变分BNNs相比，CreINNs的计算复杂度显著降低，并且比DEs具有较小的模型大小。

    Uncertainty estimation is increasingly attractive for improving the reliability of neural networks. In this work, we present novel credal-set interval neural networks (CreINNs) designed for classification tasks. CreINNs preserve the traditional interval neural network structure, capturing weight uncertainty through deterministic intervals, while forecasting credal sets using the mathematical framework of probability intervals. Experimental validations on an out-of-distribution detection benchmark (CIFAR10 vs SVHN) showcase that CreINNs outperform epistemic uncertainty estimation when compared to variational Bayesian neural networks (BNNs) and deep ensembles (DEs). Furthermore, CreINNs exhibit a notable reduction in computational complexity compared to variational BNNs and demonstrate smaller model sizes than DEs.
    
[^27]: 学习通过数学规划来配置数学规划求解器

    Learning to Configure Mathematical Programming Solvers by Mathematical Programming. (arXiv:2401.05041v1 [math.OC])

    [http://arxiv.org/abs/2401.05041](http://arxiv.org/abs/2401.05041)

    通过数学规划学习来配置数学规划求解器，解决了参数依赖关系的问题。

    

    我们讨论了为特定问题实例找到一个好的数学规划求解器配置的问题，并提出了一个两阶段的解决方法。在第一阶段，我们学习了问题实例、配置和配置求解器在问题实例上的性能之间的关系。学习一个好的求解器配置的一个特殊困难是参数设置可能不是全部独立的；这需要强制（硬）约束条件，而许多广泛使用的监督学习方法不能原生地实现。在我们的方法的第二阶段，我们解决了这个问题，使用学到的信息构建和解决了一个优化问题，其中显式地表示了配置参数设置的依赖/一致性约束。我们讨论了在两个不同实例上使用这种方法的计算结果，这些实例是在水库谷地短期规划中出现的问题。

    We discuss the issue of finding a good mathematical programming solver configuration for a particular instance of a given problem, and we propose a two-phase approach to solve it. In the first phase we learn the relationships between the instance, the configuration and the performance of the configured solver on the given instance. A specific difficulty of learning a good solver configuration is that parameter settings may not all be independent; this requires enforcing (hard) constraints, something that many widely used supervised learning methods cannot natively achieve. We tackle this issue in the second phase of our approach, where we use the learnt information to construct and solve an optimization problem having an explicit representation of the dependency/consistency constraints on the configuration parameter settings. We discuss computational results for two different instantiations of this approach on a unit commitment problem arising in the short-term planning of hydro valley
    
[^28]: 信息论方法在基于互动学习中的应用

    An Information Theoretic Approach to Interaction-Grounded Learning. (arXiv:2401.05015v1 [cs.LG])

    [http://arxiv.org/abs/2401.05015](http://arxiv.org/abs/2401.05015)

    本文提出了一个基于变分信息的互动基础学习（VI-IGL）方法，用于在强化学习任务中强制执行条件独立性假设。该方法通过学习奖励解码器来最大化上下文-动作（X，A）和反馈变量Y之间的条件互信息。

    

    最近的几篇论文研究了强化学习（RL）中学习者试图从一些反馈变量中推断出未观测到的奖励的问题。互动基础学习（IGL）是这种基于反馈的强化学习任务的一个例子，学习者通过与环境的互动推断出潜在的二值奖励来优化返回。在IGL设置中，RL文献中使用的一个相关假设是，在给定潜在奖励R的情况下，反馈变量Y在上下文-动作（X，A）上是条件独立的。在本文中，我们提出了一种基于变分信息的IGL（VI-IGL）方法，以应用于IGL的RL问题中，以强制执行条件独立性假设。VI-IGL框架使用基于条件互信息（MI）的信息目标来学习奖励解码器，该信息目标衡量了从环境中观察到的上下文-动作（X，A）和反馈变量Y之间的条件互信息。

    Reinforcement learning (RL) problems where the learner attempts to infer an unobserved reward from some feedback variables have been studied in several recent papers. The setting of Interaction-Grounded Learning (IGL) is an example of such feedback-based reinforcement learning tasks where the learner optimizes the return by inferring latent binary rewards from the interaction with the environment. In the IGL setting, a relevant assumption used in the RL literature is that the feedback variable $Y$ is conditionally independent of the context-action $(X,A)$ given the latent reward $R$. In this work, we propose Variational Information-based IGL (VI-IGL) as an information-theoretic method to enforce the conditional independence assumption in the IGL-based RL problem. The VI-IGL framework learns a reward decoder using an information-based objective based on the conditional mutual information (MI) between the context-action $(X,A)$ and the feedback variable $Y$ observed from the environment.
    
[^29]: HiMTM: 面向长期预测的分层多尺度屏蔽时间序列建模

    HiMTM: Hierarchical Multi-Scale Masked Time Series Modeling for Long-Term Forecasting. (arXiv:2401.05012v1 [cs.LG])

    [http://arxiv.org/abs/2401.05012](http://arxiv.org/abs/2401.05012)

    HiMTM是一种面向长期预测的分层多尺度屏蔽时间序列建模方法，包括分层多尺度变压器，解耦的编码器-解码器，多尺度屏蔽重构和跨尺度注意微调等组成部分。

    

    在现实世界中，时间序列预测是至关重要且具有挑战性的。近期对于时间序列基础模型的兴趣激增，这些模型适应各种不同的下游任务，这是值得注意的。然而，现有方法往往忽视了时间序列的多尺度性质，这是进行精确预测所必需的。为了弥合这一差距，我们提出了HiMTM，一种专门针对长期预测设计的分层多尺度屏蔽时间序列建模方法。具体包括四个主要组成部分：（1）分层多尺度变压器（HMT）捕捉不同尺度的时间信息；（2）解耦的编码器-解码器（DED）让编码器专注于特征提取，而解码器专注于假设任务；（3）多尺度屏蔽重构（MMR）为预训练提供多阶段的监督信号；（4）跨尺度注意微调（CSA-FT）以捕捉不同尺度之间的依赖性进行预测。综上所述，这些部件共同构成了模型。

    Time series forecasting is crucial and challenging in the real world. The recent surge in interest regarding time series foundation models, which cater to a diverse array of downstream tasks, is noteworthy. However, existing methods often overlook the multi-scale nature of time series, an aspect crucial for precise forecasting. To bridge this gap, we propose HiMTM, a hierarchical multi-scale masked time series modeling method designed for long-term forecasting. Specifically, it comprises four integral components: (1) hierarchical multi-scale transformer (HMT) to capture temporal information at different scales; (2) decoupled encoder-decoder (DED) forces the encoder to focus on feature extraction, while the decoder to focus on pretext tasks; (3) multi-scale masked reconstruction (MMR) provides multi-stage supervision signals for pre-training; (4) cross-scale attention fine-tuning (CSA-FT) to capture dependencies between different scales for forecasting. Collectively, these components en
    
[^30]: 世界灾害风险的时间分析：一种基于机器学习的集群动态方法

    Temporal Analysis of World Disaster Risk:A Machine Learning Approach to Cluster Dynamics. (arXiv:2401.05007v1 [cs.LG])

    [http://arxiv.org/abs/2401.05007](http://arxiv.org/abs/2401.05007)

    本研究通过机器学习方法对全球灾害风险进行了时间分析，发现尽管有持续的努力，全球仍然分为高易感性和中等易感性两个主要集群。

    

    评估行动的影响对于管理至关重要。本文评估了全球范围内减轻风险和创建安全环境所考虑的努力的影响。我们通过观察在特定短时间内改善的概率来衡量这种影响。利用世界风险指数，我们对2011年至2021年全球灾害风险动态进行了时间分析。通过世界风险指数的视角进行的这种时间探索揭示了灾害风险的复杂动态。我们发现，尽管进行了持续努力，全球格局仍然分为两个主要集群：高易感性和中等易感性，与地理位置无关。通过半监督方法使用标签扩散算法进行了该聚类，准确率达到了98%。我们还发现，在本研究所考虑的期间（一年、三年和五年），通过监督学习预测的集群持续存在。

    he evaluation of the impact of actions undertaken is essential in management. This paper assesses the impact of efforts considered to mitigate risk and create safe environments on a global scale. We measure this impact by looking at the probability of improvement over a specific short period of time. Using the World Risk Index, we conduct a temporal analysis of global disaster risk dynamics from 2011 to 2021. This temporal exploration through the lens of the World Risk Index provides insights into the complex dynamics of disaster risk. We found that, despite sustained efforts, the global landscape remains divided into two main clusters: high susceptibility and moderate susceptibility, regardless of geographical location. This clustering was achieved using a semi-supervised approach through the Label Spreading algorithm, with 98% accuracy. We also found that the prediction of clusters achieved through supervised learning on the period considered in this study (one, three, and five years
    
[^31]: AdaFed：通过自适应公共下降方向实现公平的联邦学习

    AdaFed: Fair Federated Learning via Adaptive Common Descent Direction. (arXiv:2401.04993v1 [cs.LG])

    [http://arxiv.org/abs/2401.04993](http://arxiv.org/abs/2401.04993)

    AdaFed是一种通过自适应公共下降方向实现公平的联邦学习方法，通过调整服务器的更新方向来确保所有客户端的损失函数减小，并且更大值的客户端的减小速率更高。

    

    联邦学习是一种有前途的技术，通过该技术，一些边缘设备/客户端在服务器的协调下共同训练一个机器学习模型。不公平的模型学习是联邦学习中的一个关键问题，训练的模型可能对某些设备产生不公平的优势或劣势。为了解决这个问题，本文提出了AdaFed。AdaFed的目标是找到服务器更新方向，在这个方向上，所有客户端的损失函数都在减小，并且更重要的是，损失函数值较大的客户端的减小速率更高。AdaFed根据本地梯度和损失函数的值自适应地调整这个公共方向。我们通过一系列联邦数据集验证了AdaFed的有效性，并证明AdaFed优于最先进的公平联邦学习方法。

    Federated learning (FL) is a promising technology via which some edge devices/clients collaboratively train a machine learning model orchestrated by a server. Learning an unfair model is known as a critical problem in federated learning, where the trained model may unfairly advantage or disadvantage some of the devices. To tackle this problem, in this work, we propose AdaFed. The goal of AdaFed is to find an updating direction for the server along which (i) all the clients' loss functions are decreasing; and (ii) more importantly, the loss functions for the clients with larger values decrease with a higher rate. AdaFed adaptively tunes this common direction based on the values of local gradients and loss functions. We validate the effectiveness of AdaFed on a suite of federated datasets, and demonstrate that AdaFed outperforms state-of-the-art fair FL methods.
    
[^32]: 保持结构的物理信息神经网络：基于能量或李雅普诺夫结构

    Structure-Preserving Physics-Informed Neural Networks With Energy or Lyapunov Structure. (arXiv:2401.04986v1 [cs.LG])

    [http://arxiv.org/abs/2401.04986](http://arxiv.org/abs/2401.04986)

    这项研究提出了一种保持结构的物理信息神经网络(PINNs)方法，通过设计保持结构的损失函数和应用于图像识别任务，提高了PINNs的性能和应用范围。

    

    最近，使用物理信息神经网络(PINNs)解决微分方程的兴趣不断增加。然而，如何以适当的方式保持能量和稳定性等结构，尚未得到确认。这种限制可能是PINNs学习过程效率不高和数值结果可能表现出非物理行为的潜在原因。此外，对于它们在下游任务上的应用，研究还很少。为了解决这些问题，我们提出了保持结构的PINNs，以提高它们的性能并扩展它们在下游任务中的应用。首先，通过利用对物理系统的先验知识，设计了一个保持结构的损失函数，帮助PINN学习潜在结构。其次，我们提出了一个利用保持结构的PINN进行鲁棒图像识别的框架。在这里，保持底层系统的李雅普诺夫结构确保系统的稳定性。

    Recently, there has been growing interest in using physics-informed neural networks (PINNs) to solve differential equations. However, the preservation of structure, such as energy and stability, in a suitable manner has yet to be established. This limitation could be a potential reason why the learning process for PINNs is not always efficient and the numerical results may suggest nonphysical behavior. Besides, there is little research on their applications on downstream tasks. To address these issues, we propose structure-preserving PINNs to improve their performance and broaden their applications for downstream tasks. Firstly, by leveraging prior knowledge about the physical system, a structure-preserving loss function is designed to assist the PINN in learning the underlying structure. Secondly, a framework that utilizes structure-preserving PINN for robust image recognition is proposed. Here, preserving the Lyapunov structure of the underlying system ensures the stability of the sy
    
[^33]: 可逆解决非规则采样时间序列的神经微分方程分析方法

    Invertible Solution of Neural Differential Equations for Analysis of Irregularly-Sampled Time Series. (arXiv:2401.04979v1 [cs.LG])

    [http://arxiv.org/abs/2401.04979](http://arxiv.org/abs/2401.04979)

    我们提出了一种可逆解决非规则采样时间序列的神经微分方程分析方法，通过引入神经流的概念，我们的方法既保证了可逆性又降低了计算负担，并且在分类和插值任务中表现出了优异的性能。

    

    为了处理非规则和不完整的时间序列数据的复杂性，我们提出了一种基于神经微分方程（NDE）的可逆解决方案。虽然基于NDE的方法是分析非规则采样时间序列的一种强大方法，但它们通常不能保证在其标准形式下进行可逆变换。我们的方法建议使用具有神经流的神经控制微分方程（Neural CDEs）的变种，该方法在保持较低的计算负担的同时确保了可逆性。此外，它还可以训练双重潜在空间，增强了对动态时间动力学的建模能力。我们的研究提出了一个先进的框架，在分类和插值任务中都表现出色。我们方法的核心是一个经过精心设计的增强型双重潜在状态架构，用于在各种时间序列任务中提高精度。实证分析表明，我们的方法明显优于现有模型。

    To handle the complexities of irregular and incomplete time series data, we propose an invertible solution of Neural Differential Equations (NDE)-based method. While NDE-based methods are a powerful method for analyzing irregularly-sampled time series, they typically do not guarantee reversible transformations in their standard form. Our method suggests the variation of Neural Controlled Differential Equations (Neural CDEs) with Neural Flow, which ensures invertibility while maintaining a lower computational burden. Additionally, it enables the training of a dual latent space, enhancing the modeling of dynamic temporal dynamics. Our research presents an advanced framework that excels in both classification and interpolation tasks. At the core of our approach is an enhanced dual latent states architecture, carefully designed for high precision across various time series tasks. Empirical analysis demonstrates that our method significantly outperforms existing models. This work significan
    
[^34]: 用符号回归梯度解释神经网络分类器的闭式表示

    Closed-Form Interpretation of Neural Network Classifiers with Symbolic Regression Gradients. (arXiv:2401.04978v1 [cs.LG])

    [http://arxiv.org/abs/2401.04978](http://arxiv.org/abs/2401.04978)

    本文提出了一种解释神经网络分类器的闭式表示的方法，使其适用于自动化科学发现。这种方法通过将神经网络嵌入到一组基于相同量的等价类中，并通过找到该等价类与符号回归搜索空间中的方程的交集来解释神经网络。

    

    我提出了一个统一的框架来解释神经网络分类器，以实现自动科学发现。与基于神经网络的回归不同，对于分类而言，即使神经网络本身的分类基于可以表示为闭式方程的量，也一般无法找到从神经网络到符号方程的一对一映射。在本文中，我将训练好的神经网络嵌入到一个等价类中，这个等价类的分类函数的决策都基于相同的量。我通过找到这个等价类与由符号回归搜索空间定义的可读的方程的交集来解释神经网络。这种方法不限于分类器或完整的神经网络，还可以应用于隐藏层或潜在空间中的任意神经元，或简化解释神经网络回归器的过程。

    I introduce a unified framework for interpreting neural network classifiers tailored toward automated scientific discovery. In contrast to neural network-based regression, for classification, it is in general impossible to find a one-to-one mapping from the neural network to a symbolic equation even if the neural network itself bases its classification on a quantity that can be written as a closed-form equation. In this paper, I embed a trained neural network into an equivalence class of classifying functions that base their decisions on the same quantity. I interpret neural networks by finding an intersection between this equivalence class and human-readable equations defined by the search space of symbolic regression. The approach is not limited to classifiers or full neural networks and can be applied to arbitrary neurons in hidden layers or latent spaces or to simplify the process of interpreting neural network regressors.
    
[^35]: ConvConcatNet: 一个用于从脑电图 (EEG) 重建 Mel 频谱的深度卷积神经网络

    ConvConcatNet: a deep convolutional neural network to reconstruct mel spectrogram from the EEG. (arXiv:2401.04965v1 [eess.SP])

    [http://arxiv.org/abs/2401.04965](http://arxiv.org/abs/2401.04965)

    本论文提出了一种名为ConvConcatNet的深度卷积神经网络方法，用于从EEG中重建Mel频谱，并在Auditory EEG Challenge的Task 2中取得了第一名，重建和目标Mel频谱之间的皮尔逊相关系数达到0.0420。

    

    为了研究大脑对语音的处理，通常使用简单的线性模型来建立脑信号和语音特征之间的关系。然而，这些线性模型不适用于模拟大脑这样高度动态和复杂的非线性系统。尽管最近已经开发出了使用神经网络的非线性方法，但从未见过的受试者的 EEG 中重建未知刺激仍然是一项极具挑战性的任务。本研究提出了一种新的方法 ConvConcatNet，通过结合深度卷积神经网络和广泛的串接操作，从 EEG 中重建 Mel 频谱。使用我们的 ConvConcatNet 模型，重建的 Mel 频谱与目标 Mel 频谱之间的皮尔逊相关系数可达到 0.0420，在听觉 EEG 挑战赛的任务2中排名第一。我们的代码和模型将在 Github 上提供：https://github.com/xuxiran/ConvConcatNet

    To investigate the processing of speech in the brain, simple linear models are commonly used to establish a relationship between brain signals and speech features. However, these linear models are ill-equipped to model a highly dynamic and complex non-linear system like the brain. Although non-linear methods with neural networks have been developed recently, reconstructing unseen stimuli from unseen subjects' EEG is still a highly challenging task. This work presents a novel method, ConvConcatNet, to reconstruct mel-specgrams from EEG, in which the deep convolution neural network and extensive concatenation operation were combined. With our ConvConcatNet model, the Pearson correlation between the reconstructed and the target mel-spectrogram can achieve 0.0420, which was ranked as No.1 in the Task 2 of the Auditory EEG Challenge. The codes and models to implement our work will be available on Github: https://github.com/xuxiran/ConvConcatNet
    
[^36]: 当你能改变你的规划器为何要改变你的控制器：针对四旋翼系统的考虑风阻轨迹生成

    Why Change Your Controller When You Can Change Your Planner: Drag-Aware Trajectory Generation for Quadrotor Systems. (arXiv:2401.04960v1 [cs.RO])

    [http://arxiv.org/abs/2401.04960](http://arxiv.org/abs/2401.04960)

    本文研究了四旋翼器在受到阻力力影响下的轨迹生成和控制设计问题。通过保持控制器固定，只改变轨迹生成部分，我们提出了一种考虑风阻的规划方法，以提高四旋翼器系统的轨迹跟踪能力。

    

    受到四旋翼器在载荷传递中的越来越广泛应用的启发，我们考虑了一个四旋翼器的联合轨迹生成和反馈控制设计问题，考虑了四旋翼器在空气动力学力的作用下的运动。携带的载荷产生的未建模空气动力学阻力力可能导致灾难性的后果。之前的工作将空气动力学效应建模为控制问题中的剩余动力学或外部干扰，引导出一种可能造成灾难的反应性政策。此外，在硬件平台上重新设计控制器和调整控制增益是一项费时的工作。在本文中，我们认为保持控制器固定，只改变轨迹生成部分能够提高在受到阻力力影响下的四旋翼器系统的轨迹跟踪能力。为了实现这一点，我们通过对最优四旋翼器控制问题应用适当的松弛，引入了一个衡量控制器跟踪参考轨迹能力的跟踪成本函数，从而形成一个考虑风阻的规划问题。

    Motivated by the increasing use of quadrotors for payload delivery, we consider a joint trajectory generation and feedback control design problem for a quadrotor experiencing aerodynamic wrenches. Unmodeled aerodynamic drag forces from carried payloads can lead to catastrophic outcomes. Prior work model aerodynamic effects as residual dynamics or external disturbances in the control problem leading to a reactive policy that could be catastrophic. Moreover, redesigning controllers and tuning control gains on hardware platforms is a laborious effort. In this paper, we argue that adapting the trajectory generation component keeping the controller fixed can improve trajectory tracking for quadrotor systems experiencing drag forces. To achieve this, we formulate a drag-aware planning problem by applying a suitable relaxation to an optimal quadrotor control problem, introducing a tracking cost function which measures the ability of a controller to follow a reference trajectory. This tracking
    
[^37]: 利用强化学习提升ECG诊断：针对P波和PR间期的全局波形变异(arXiv：2401.04938v1 [eess.SP])

    Advancing ECG Diagnosis Using Reinforcement Learning on Global Waveform Variations Related to P Wave and PR Interval. (arXiv:2401.04938v1 [eess.SP])

    [http://arxiv.org/abs/2401.04938](http://arxiv.org/abs/2401.04938)

    本研究利用Q学习强化算法在PhysioNet/Computing in Cardiology Challenge（CinC）提供的多种ECG数据集上应用，研究了P波和PR间期在Lead II和Lead V1上的不同变异。通过Q-Agent的分类能够在平均准确率为90.4％的情况下对患者的心搏样本进行分类，并且具有较低的平均汉明损失率和较快的分类时间。

    

    通过心电图（ECG）分析可靠地诊断心脏病情，关键要准确检测P波和测量PR间期。然而，由于ECG信号中存在的全局变异，实现跨多样化人群的一致和可推广的诊断存在挑战。本文重点研究将Q学习强化算法应用于PhysioNet/Computing in Cardiology Challenge（CinC）中可用的各种ECG数据集。研究了Lead II和Lead V1上P波和PR间期的多样性，包含五个ECG心搏类型，包括正常窦性心律、心房扑动、心房纤颤、一度房室传导阻滞和左房扩大。Q-Agent对8,867名患者的71,672个心搏样本进行了分类，平均准确率为90.4％，通过错误分类的平均汉明损失率仅为9.6％。在处理约40,000个样本的第100个episode中，平均分类时间为0.04秒。

    The reliable diagnosis of cardiac conditions through electrocardiogram (ECG) analysis critically depends on accurately detecting P waves and measuring the PR interval. However, achieving consistent and generalizable diagnoses across diverse populations presents challenges due to the inherent global variations observed in ECG signals. This paper is focused on applying the Q learning reinforcement algorithm to the various ECG datasets available in the PhysioNet/Computing in Cardiology Challenge (CinC). Five ECG beats, including Normal Sinus Rhythm, Atrial Flutter, Atrial Fibrillation, 1st Degree Atrioventricular Block, and Left Atrial Enlargement, are included to study variations of P waves and PR Interval on Lead II and Lead V1. Q-Agent classified 71,672 beat samples in 8,867 patients with an average accuracy of 90.4% and only 9.6% average hamming loss over misclassification. The average classification time at the 100th episode containing around 40,000 samples is 0.04 seconds. An averag
    
[^38]: 完全分散的合作多智能体强化学习：一项调查

    Fully Decentralized Cooperative Multi-Agent Reinforcement Learning: A Survey. (arXiv:2401.04934v1 [cs.MA])

    [http://arxiv.org/abs/2401.04934](http://arxiv.org/abs/2401.04934)

    本文系统地回顾了完全分散的合作多智能体强化学习方法，并讨论了两种不同设置下的算法以及未来的研究方向。

    

    合作多智能体强化学习是解决许多实际合作任务的强大工具，但现实应用的限制可能要求以完全分散的方式训练智能体。由于缺乏其他智能体的信息，要在完全分散的情况下得出可以收敛到最优联合策略的算法是具有挑战性的。因此，这个研究领域还没有得到深入的研究。在本文中，我们系统地回顾了两种设置的完全分散方法：最大化所有智能体的共享奖励和最大化所有智能体的个体奖励的总和，并讨论了开放问题和未来的研究方向。

    Cooperative multi-agent reinforcement learning is a powerful tool to solve many real-world cooperative tasks, but restrictions of real-world applications may require training the agents in a fully decentralized manner. Due to the lack of information about other agents, it is challenging to derive algorithms that can converge to the optimal joint policy in a fully decentralized setting. Thus, this research area has not been thoroughly studied. In this paper, we seek to systematically review the fully decentralized methods in two settings: maximizing a shared reward of all agents and maximizing the sum of individual rewards of all agents, and discuss open questions and future research directions.
    
[^39]: 重新思考测试时似然性：似然路径原理及其在OOD检测中的应用

    Rethinking Test-time Likelihood: The Likelihood Path Principle and Its Application to OOD Detection. (arXiv:2401.04933v1 [cs.LG])

    [http://arxiv.org/abs/2401.04933](http://arxiv.org/abs/2401.04933)

    通过引入似然路径原理和新的理论工具，本研究针对变分自编码器（VAEs）的条件似然性提供了非渐近可证明的超出分布（OOD）检测保证。

    

    虽然似然性在理论上很有吸引力，但是通过深度生成模型（DGM）估计的似然性在实践中经常出现问题，对于超出分布（OOD）检测表现不佳。最近的一些工作开始考虑替代性评分，并取得了更好的性能。然而，这些方法并没有提供可证明的保证，也不清楚它们的选择是否提取了足够的信息。我们尝试改变这种情况，通过对变分自编码器（VAEs）进行案例研究。首先，我们引入了似然路径（LPath）原理，推广了似然性原理。这将搜索有用的摘要统计量缩小到VAEs条件似然性的最小充分统计量。其次，引入了新的理论工具，如几乎有效支持、基本距离和共-Lipschitz性，我们为某些最小充分统计量的摘要提供了非渐近可证明的OOD检测保证。相应的LPath算法证明了这一点。

    While likelihood is attractive in theory, its estimates by deep generative models (DGMs) are often broken in practice, and perform poorly for out of distribution (OOD) Detection. Various recent works started to consider alternative scores and achieved better performances. However, such recipes do not come with provable guarantees, nor is it clear that their choices extract sufficient information.  We attempt to change this by conducting a case study on variational autoencoders (VAEs). First, we introduce the likelihood path (LPath) principle, generalizing the likelihood principle. This narrows the search for informative summary statistics down to the minimal sufficient statistics of VAEs' conditional likelihoods. Second, introducing new theoretic tools such as nearly essential support, essential distance and co-Lipschitzness, we obtain non-asymptotic provable OOD detection guarantees for certain distillation of the minimal sufficient statistics. The corresponding LPath algorithm demons
    
[^40]: 基于学习的难度校准提升成员推理攻击的能力

    Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks. (arXiv:2401.04929v1 [cs.CR])

    [http://arxiv.org/abs/2401.04929](http://arxiv.org/abs/2401.04929)

    本文介绍了一种基于学习的难度校准的成员推理攻击方法，旨在显著提高低FPR下的TPR，以验证训练模型是否保护隐私。

    

    机器学习模型，特别是深度神经网络，目前是各种应用的重要组成部分，从医疗保健到金融。然而，使用敏感数据来训练这些模型引发了对隐私和安全的担忧。一种验证训练模型是否保护隐私的方法是成员推理攻击（MIA），它允许对手确定特定数据点是否是模型的训练数据集的一部分。虽然已经在文献中提出了一系列的MIA，但只有少数能够在低假阳性率（FPR）区域（0.01%~1%）实现较高的真阳性率（TPR）。这是实际应用于实际场景中的MIA必须考虑的关键因素。在本文中，我们提出了一种新颖的MIA方法，旨在显著提高低FPR的TPR。我们的方法名为基于学习的难度校准（LDC-MIA），通过使用神经网络分类器将数据记录以其难度级别进行表征。

    Machine learning models, in particular deep neural networks, are currently an integral part of various applications, from healthcare to finance. However, using sensitive data to train these models raises concerns about privacy and security. One method that has emerged to verify if the trained models are privacy-preserving is Membership Inference Attacks (MIA), which allows adversaries to determine whether a specific data point was part of a model's training dataset. While a series of MIAs have been proposed in the literature, only a few can achieve high True Positive Rates (TPR) in the low False Positive Rate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA to be practically useful in real-world settings. In this paper, we present a novel approach to MIA that is aimed at significantly improving TPR at low FPRs. Our method, named learning-based difficulty calibration for MIA(LDC-MIA), characterizes data records by their hardness levels using a neural network clas
    
[^41]: 放松的对比学习用于联邦学习

    Relaxed Contrastive Learning for Federated Learning. (arXiv:2401.04928v1 [cs.LG])

    [http://arxiv.org/abs/2401.04928](http://arxiv.org/abs/2401.04928)

    我们提出了一种放松的对比学习框架，用于解决联邦学习中的数据异构性挑战。我们的方法通过引入放松的对比学习损失，防止表示坍缩，增强特征的可传递性，从而实现了显著的性能提升。

    

    我们提出了一种新颖的对比学习框架，以有效解决联邦学习中的数据异构性挑战。我们首先分析了本地训练中客户端之间梯度更新的不一致性，并建立其与特征表示分布的依赖关系，从而导出了监督对比学习（SCL）目标来减轻局部偏差。此外，我们还展示了在联邦学习中对SCL的朴素应用会导致表示坍缩，导致收敛缓慢和性能提升有限。为了解决这个问题，我们引入了一种放松的对比学习损失，对每个类别内过于相似的样本对施加发散惩罚。这种策略可以防止表示坍缩，增强特征的可传递性，促进协作训练，并导致显著的性能提升。我们的框架在所有现有的联邦学习方法中表现出巨大的优势。

    We propose a novel contrastive learning framework to effectively address the challenges of data heterogeneity in federated learning. We first analyze the inconsistency of gradient updates across clients during local training and establish its dependence on the distribution of feature representations, leading to the derivation of the supervised contrastive learning (SCL) objective to mitigate local deviations. In addition, we show that a na\"ive adoption of SCL in federated learning leads to representation collapse, resulting in slow convergence and limited performance gains. To address this issue, we introduce a relaxed contrastive learning loss that imposes a divergence penalty on excessively similar sample pairs within each class. This strategy prevents collapsed representations and enhances feature transferability, facilitating collaborative training and leading to significant performance improvements. Our framework outperforms all existing federated learning approaches by huge marg
    
[^42]: 基于不一致性的数据中心主动开放集标注

    Inconsistency-Based Data-Centric Active Open-Set Annotation. (arXiv:2401.04923v1 [cs.LG])

    [http://arxiv.org/abs/2401.04923](http://arxiv.org/abs/2401.04923)

    NEAT是一种数据中心的主动学习方法，旨在解决主动开放集标注问题。它通过利用标签的可聚类性来识别已知类别，并从已知和未知类别的未标记数据中选择具有信息量的数据进行标注。

    

    主动学习是一种常用的方法，可减少训练深度神经网络所需的标记工作量。然而，当前主动学习方法的效果受到其封闭世界假设的限制，即假设未标记数据池中的所有数据来自一组预定义的已知类别。在实际情况下，这种假设通常是无效的，因为未标记数据中可能存在未知类别，导致主动开放集标注问题。数据中存在未知类别会显著影响现有主动学习方法的性能，因为其引入了不确定性。为了解决这个问题，我们提出了一种新颖的基于数据的主动学习方法NEAT，它可以主动标注开放集数据。NEAT旨在从已知和未知类别的未标记数据池中标记已知类别的数据，利用标签的可聚类性来识别未标记池中的已知类别，并选择具有信息量的数据进行标注。

    Active learning is a commonly used approach that reduces the labeling effort required to train deep neural networks. However, the effectiveness of current active learning methods is limited by their closed-world assumptions, which assume that all data in the unlabeled pool comes from a set of predefined known classes. This assumption is often not valid in practical situations, as there may be unknown classes in the unlabeled data, leading to the active open-set annotation problem. The presence of unknown classes in the data can significantly impact the performance of existing active learning methods due to the uncertainty they introduce. To address this issue, we propose a novel data-centric active learning method called NEAT that actively annotates open-set data. NEAT is designed to label known classes data from a pool of both known and unknown classes unlabeled data. It utilizes the clusterability of labels to identify the known classes from the unlabeled pool and selects informative
    
[^43]: SPT: 光谱变换器用于红巨星年龄和质量估计

    SPT: Spectral Transformer for Red Giant Stars Age and Mass Estimation. (arXiv:2401.04900v1 [astro-ph.SR])

    [http://arxiv.org/abs/2401.04900](http://arxiv.org/abs/2401.04900)

    我们开发了一种叫做光谱变换器（SPT）的框架来预测红巨星的年龄和质量，其中关键的组成部分是为光谱设计的多头哈达玛自注意机制，同时还使用了马氏距离和蒙特卡洛Dropout来解决问题并分析不确定性。

    

    红巨星的年龄和质量对于理解银河系的结构和演化至关重要。传统的等时线方法在估算中存在限制，因为在赫罗图中等时线重叠，而天体声学虽然更精确，但需要高精度、长期观测。为了应对这些挑战，我们开发了一种新颖的框架——光谱变换器（Spectral Transformer，SPT），通过红巨星的光谱预测其与天体声学一致的年龄和质量。SPT的关键组成部分是为光谱特别设计的多头哈达玛自注意机制，可以捕捉不同波长上的复杂关系。此外，我们引入了基于马氏距离的损失函数来解决尺度不平衡和交互模式损失，并结合了蒙特卡洛Dropout来定量分析预测的不确定性。在来自LAMOST的3,880个红巨星光谱上进行训练和测试，SPT取得了令人瞩目的年龄

    The age and mass of red giants are essential for understanding the structure and evolution of the Milky Way. Traditional isochrone methods for these estimations are inherently limited due to overlapping isochrones in the Hertzsprung-Russell diagram, while asteroseismology, though more precise, requires high-precision, long-term observations. In response to these challenges, we developed a novel framework, Spectral Transformer (SPT), to predict the age and mass of red giants aligned with asteroseismology from their spectra. A key component of SPT, the Multi-head Hadamard Self-Attention mechanism, designed specifically for spectra, can capture complex relationships across different wavelength. Further, we introduced a Mahalanobis distance-based loss function to address scale imbalance and interaction mode loss, and incorporated Monte Carlo dropout for quantitative analysis of prediction uncertainty.Trained and tested on 3,880 red giant spectra from LAMOST, the SPT achieved remarkable age
    
[^44]: 通过机制稀疏性进行非参数化部分解缠: 稀疏动作, 干预和稀疏时间依赖性

    Nonparametric Partial Disentanglement via Mechanism Sparsity: Sparse Actions, Interventions and Sparse Temporal Dependencies. (arXiv:2401.04890v1 [stat.ML])

    [http://arxiv.org/abs/2401.04890](http://arxiv.org/abs/2401.04890)

    本研究引入了一种称为机制稀疏性正则化的解缠原则，通过同时学习潜在因素和解释它们的稀疏因果图模型来实现解缠。这项工作通过非参数化可辨识性理论证明了这一原则，并提供了一种图形准则来保证完全解缠。

    

    这项工作引入一种新的解缠原则，即机制稀疏规则，该规则适用于感兴趣的潜在因素在观察辅助变量和/或过去潜在因素上稀疏依赖的情况。我们提出了一种表示学习方法，通过同时学习潜在因素和解释它们的稀疏因果图模型来引导解缠。我们开发了一个非参数化可辨识性理论来形式化这一原则，并证明通过将学习到的因果图稀疏化，可以恢复潜在因素。更确切地说，我们展示了一种新的等价关系"一致性"来描述能够保持一些潜在因素纠缠的部分解缠过程。为了描述纠缠的结构，我们引入了纠缠图和图保持函数的概念。我们还提供了一个图形准则，用于保证完全解缠。

    This work introduces a novel principle for disentanglement we call mechanism sparsity regularization, which applies when the latent factors of interest depend sparsely on observed auxiliary variables and/or past latent factors. We propose a representation learning method that induces disentanglement by simultaneously learning the latent factors and the sparse causal graphical model that explains them. We develop a nonparametric identifiability theory that formalizes this principle and shows that the latent factors can be recovered by regularizing the learned causal graph to be sparse. More precisely, we show identifiablity up to a novel equivalence relation we call "consistency", which allows some latent factors to remain entangled (hence the term partial disentanglement). To describe the structure of this entanglement, we introduce the notions of entanglement graphs and graph preserving functions. We further provide a graphical criterion which guarantees complete disentanglement, that
    
[^45]: 机器学习中的特征网络方法及应用

    Feature Network Methods in Machine Learning and Applications. (arXiv:2401.04874v1 [stat.ML])

    [http://arxiv.org/abs/2401.04874](http://arxiv.org/abs/2401.04874)

    机器学习中的特征网络方法及应用是将学习任务中的特征连接成图形结构，并通过函数操作生成新的特征。这种方法在图像处理和计算生物学中具有应用价值。

    

    机器学习中的特征网络是一个将学习任务中的特征基于相似性连接起来的图形表示。这种网络表示允许我们将特征向量视为网络上的函数。通过利用傅里叶分析和函数分析中的函数操作，我们可以轻松地生成新的特征，利用特征向量上所施加的图形结构。这样的网络结构在图像处理和计算生物学中已经被隐式研究过。因此，我们将特征网络描述为被施加在特征向量上的图形结构，并在机器学习中提供应用。其中一个应用涉及基于图形的卷积神经网络的推广，涉及具有不同深度或复杂度的特征的层次化结构化深度学习。这还扩展到能够生成有用的新的多层级特征的学习算法。此外，我们还讨论了特征的使用。

    A machine learning (ML) feature network is a graph that connects ML features in learning tasks based on their similarity. This network representation allows us to view feature vectors as functions on the network. By leveraging function operations from Fourier analysis and from functional analysis, one can easily generate new and novel features, making use of the graph structure imposed on the feature vectors. Such network structures have previously been studied implicitly in image processing and computational biology. We thus describe feature networks as graph structures imposed on feature vectors, and provide applications in machine learning. One application involves graph-based generalizations of convolutional neural networks, involving structured deep learning with hierarchical representations of features that have varying depth or complexity. This extends also to learning algorithms that are able to generate useful new multilevel features. Additionally, we discuss the use of featur
    
[^46]: 知识感知图转换器用于行人轨迹预测

    Knowledge-aware Graph Transformer for Pedestrian Trajectory Prediction. (arXiv:2401.04872v1 [cs.CV])

    [http://arxiv.org/abs/2401.04872](http://arxiv.org/abs/2401.04872)

    这篇论文提出了一种知识感知图转换器结构，通过设计自注意机制和领域自适应模块来提高行人轨迹预测性能，并引入考虑跨数据集序列的额外指标进行评估。

    

    预测行人运动轨迹对于自动驾驶车辆的路径规划和运动控制至关重要。由于不同环境下人类运动的不确定性，准确预测群体轨迹是具有挑战性的。最近的深度学习预测方法主要利用轨迹历史和行人之间的交互等信息进行训练。然而，这种方法限制了在不同场景下的预测性能，因为训练数据集之间的差异没有得到很好地融合。为了克服这个限制，本文提出了一种图转换器结构来提高预测性能，捕捉数据集中不同场所和场景之间的差异。具体而言，设计了一种自注意机制和领域自适应模块来提高模型的泛化能力。此外，引入了考虑跨数据集序列的额外指标以进行轨迹预测性能的评估。

    Predicting pedestrian motion trajectories is crucial for path planning and motion control of autonomous vehicles. Accurately forecasting crowd trajectories is challenging due to the uncertain nature of human motions in different environments. For training, recent deep learning-based prediction approaches mainly utilize information like trajectory history and interactions between pedestrians, among others. This can limit the prediction performance across various scenarios since the discrepancies between training datasets have not been properly incorporated. To overcome this limitation, this paper proposes a graph transformer structure to improve prediction performance, capturing the differences between the various sites and scenarios contained in the datasets. In particular, a self-attention mechanism and a domain adaption module have been designed to improve the generalization ability of the model. Moreover, an additional metric considering cross-dataset sequences is introduced for tra
    
[^47]: 个性化语言提示的用户嵌入模型

    User Embedding Model for Personalized Language Prompting. (arXiv:2401.04858v1 [cs.CL])

    [http://arxiv.org/abs/2401.04858](http://arxiv.org/abs/2401.04858)

    本研究提出了一种新的用户嵌入模块，可以更有效地处理长时间的用户历史记录，并在推荐系统中取得了显著的改进。

    

    对于提升推荐系统的模型，建模长时间的历史记录起到了关键作用，能够捕捉用户不断演变的偏好，从而得到更准确和个性化的推荐。本研究致力于解决自然语言偏好理解中建模长用户历史记录的挑战。具体地，我们引入了一种新的用户嵌入模块(UEM)，通过将用户历史记录以嵌入形式压缩和表示，将其作为对语言模型的软提示。我们的实验表明，与传统的基于文本的提示方法相比，这种方法在处理显著更长的历史记录方面具有卓越的能力，并在预测性能方面取得了实质性的改进。该研究的主要贡献在于展示了使用表示为嵌入的用户信号来偏置语言模型的能力。

    Modeling long histories plays a pivotal role in enhancing recommendation systems, allowing to capture user's evolving preferences, resulting in more precise and personalized recommendations. In this study we tackle the challenges of modeling long user histories for preference understanding in natural language. Specifically, we introduce a new User Embedding Module (UEM) that efficiently processes user history in free-form text by compressing and representing them as embeddings, to use them as soft prompts to a LM. Our experiments demonstrate the superior capability of this approach in handling significantly longer histories compared to conventional text based prompting methods, yielding substantial improvements in predictive performance. The main contribution of this research is to demonstrate the ability to bias language models with user signals represented as embeddings.
    
[^48]: 使用特征变换进行交通市场利率预测

    Transportation Market Rate Forecast Using Signature Transform. (arXiv:2401.04857v1 [cs.LG])

    [http://arxiv.org/abs/2401.04857](http://arxiv.org/abs/2401.04857)

    本论文提出了一种基于特征变换的新型统计方法，用于解决交通市场利率的预测挑战。该方法具有通用的非线性属性和特征变换核函数，能够高效生成特征，并在预测过程中准确识别季节性和制度转换。

    

    目前，亚马逊在交通市场利率预测上依赖第三方，尽管这些预测质量差且缺乏可解释性。虽然交通市场利率通常很难准确预测，但我们开发了一种基于特征变换的新型统计技术来解决这些挑战，并构建了一个预测和自适应模型来预测市场利率。这种新技术基于特征变换的两个关键属性。第一个是其通用的非线性，它线性化特征空间，从而将预测问题转化为线性回归分析；第二个是特征变换核函数，它允许在时间序列数据之间进行计算有效的相似性比较。结合起来，这些属性允许进行高效的特征生成，并在预测过程中更精确地识别季节性和制度转换。模型的初步结果显示，这种新方法可以改善市场利率的预测性能。

    Currently, Amazon relies on third parties for transportation marketplace rate forecasts, despite the poor quality and lack of interpretability of these forecasts. While transportation marketplace rates are typically very challenging to forecast accurately, we have developed a novel signature-based statistical technique to address these challenges and built a predictive and adaptive model to forecast marketplace rates. This novel technique is based on two key properties of the signature transform. The first is its universal nonlinearity which linearizes the feature space and hence translates the forecasting problem into a linear regression analysis; the second is the signature kernel which allows for comparing computationally efficiently similarities between time series data. Combined, these properties allow for efficient feature generation and more precise identification of seasonality and regime switching in the forecasting process. Preliminary result by the model shows that this new 
    
[^49]: 一个好的评分并不会导致一个好的生成模型

    A Good Score Does not Lead to A Good Generative Model. (arXiv:2401.04856v1 [cs.LG])

    [http://arxiv.org/abs/2401.04856](http://arxiv.org/abs/2401.04856)

    本文通过反例证明，在某些情况下，即使评分函数学习良好，基于评分的生成模型（SGMs）仍然无法生成接近真实数据分布的样本，并且只能产生训练数据点的高斯模糊样本。

    

    基于评分的生成模型（SGMs）是生成建模中的一种主要方法，以其能够从复杂的高维数据分布中生成高质量样本而闻名。该方法在经验上取得了成功，并且有着严格的理论收敛性质的支持。特别是已经证明，如果学习到的底层评分函数良好，SGMs能够生成接近真实数据分布的样本，这表明了SGM作为生成模型的成功之处。本文提供了一个反例。通过样本复杂度的分析，我们提供了一个特定的设置，其中评分函数学习得很好。然而，在这个设置中，SGMs只能输出训练数据点的高斯模糊样本，模拟核密度估计的效果。这一发现与最近的一系列发现相一致，揭示了SGMs可能表现出强大的记忆效应并且无法生成样本的问题。

    Score-based Generative Models (SGMs) is one leading method in generative modeling, renowned for their ability to generate high-quality samples from complex, high-dimensional data distributions. The method enjoys empirical success and is supported by rigorous theoretical convergence properties. In particular, it has been shown that SGMs can generate samples from a distribution that is close to the ground-truth if the underlying score function is learned well, suggesting the success of SGM as a generative model. We provide a counter-example in this paper. Through the sample complexity argument, we provide one specific setting where the score function is learned well. Yet, SGMs in this setting can only output samples that are Gaussian blurring of training data points, mimicking the effects of kernel density estimation. The finding resonates a series of recent finding that reveal that SGMs can demonstrate strong memorization effect and fail to generate.
    
[^50]: LPAC: 可学习的感知-行动-通信循环及其在覆盖控制中的应用

    LPAC: Learnable Perception-Action-Communication Loops with Applications to Coverage Control. (arXiv:2401.04855v1 [cs.RO])

    [http://arxiv.org/abs/2401.04855](http://arxiv.org/abs/2401.04855)

    提出了一种可学习的感知-行动-通信(LPAC)架构，使用卷积神经网络处理环境感知，图神经网络实现机器人之间的信息交流，浅层多层感知机计算机器人的动作。使用集中式显微算法训练模型，实现机器人群体的协作。

    

    覆盖控制是指导机器人群体协同监测未知的感兴趣特征或现象的问题。在有限的通信和感知能力的分散设置中，这个问题具有挑战性。本文提出了一种可学习的感知-行动-通信(LPAC)架构来解决覆盖控制问题。在该解决方案中，卷积神经网络(CNN)处理了环境的局部感知；图神经网络(GNN)实现了邻近机器人之间的相关信息通信；最后，浅层多层感知机(MLP)计算机器人的动作。通信模块中的GNN通过计算应该与邻居通信哪些信息以及如何利用接收到的信息采取适当的行动来实现机器人群体的协作。我们使用一个知晓整个环境的集中式显微算法来进行模型的训练。

    Coverage control is the problem of navigating a robot swarm to collaboratively monitor features or a phenomenon of interest not known a priori. The problem is challenging in decentralized settings with robots that have limited communication and sensing capabilities. This paper proposes a learnable Perception-Action-Communication (LPAC) architecture for the coverage control problem. In the proposed solution, a convolution neural network (CNN) processes localized perception of the environment; a graph neural network (GNN) enables communication of relevant information between neighboring robots; finally, a shallow multi-layer perceptron (MLP) computes robot actions. The GNN in the communication module enables collaboration in the robot swarm by computing what information to communicate with neighbors and how to use received information to take appropriate actions. We train models using imitation learning with a centralized clairvoyant algorithm that is aware of the entire environment. Eva
    
[^51]: 基于图学习的城市空中移动机队调度在操作限制、需求变化和不确定性下的研究

    Graph Learning-based Fleet Scheduling for Urban Air Mobility under Operational Constraints, Varying Demand & Uncertainties. (arXiv:2401.04851v1 [cs.MA])

    [http://arxiv.org/abs/2401.04851](http://arxiv.org/abs/2401.04851)

    本文提出了一种基于图学习的方法，用于在线规划城市空中移动机队的时间表和目的地，考虑到操作限制、需求变化和不确定性等问题。通过构建新的策略架构和使用图胶囊转换网络、转换器层和Multi-head Attention-based解码器等组件，该方法能够解决现有机队规划实施中存在的复杂性，具有更高的现实性和逼真度。

    

    本文提出了一种基于图强化学习的方法,用于在线规划电动飞行器的时间表和目的地,该飞行器组成了一个跨多个垂直港口运营的城市空中移动（UAM）机队。这个机队调度问题的制定考虑了时间变化的需求、垂直港口容量、飞行器容量和空域安全准则的约束，以及起飞延误、天气引起的路线关闭和情况未知的飞行器停机时间的不确定性。这样的制定方式比现有的UAM机队规划实施更加复杂，可能增加了更多的现实性。为了解决这些复杂性，构建了一个新的策略架构，主要组成部分包括：作为图的抽象，用于编码垂直港口和飞行器机队状态的图胶囊转换网络；编码需求和乘客票价的时间序列信息的转换器层；以及使用编码器的Multi-head Attention-based解码器。

    This paper develops a graph reinforcement learning approach to online planning of the schedule and destinations of electric aircraft that comprise an urban air mobility (UAM) fleet operating across multiple vertiports. This fleet scheduling problem is formulated to consider time-varying demand, constraints related to vertiport capacity, aircraft capacity and airspace safety guidelines, uncertainties related to take-off delay, weather-induced route closures, and unanticipated aircraft downtime. Collectively, such a formulation presents greater complexity, and potentially increased realism, than in existing UAM fleet planning implementations. To address these complexities, a new policy architecture is constructed, primary components of which include: graph capsule conv-nets for encoding vertiport and aircraft-fleet states both abstracted as graphs; transformer layers encoding time series information on demand and passenger fare; and a Multi-head Attention-based decoder that uses the enco
    
[^52]: 关于广义等增递归分割算法的正确性研究

    On the Correctness of the Generalized Isotonic Recursive Partitioning Algorithm. (arXiv:2401.04847v1 [stat.ML])

    [http://arxiv.org/abs/2401.04847](http://arxiv.org/abs/2401.04847)

    本文通过深入分析广义等增递归分割算法（GIRP），在可分离凸损失和不可微损失的情况下，解决了等增回归问题的存在性和唯一性，并提出了递归二分分割的方法来找到解。

    

    本文深入分析了广义等增递归分割算法（GIRP），该算法用于拟合可分离凸损失下的等增模型，该算法由Luss和Rosset提出 [J. Comput. Graph. Statist., 23 (2014), pp. 192--201] 并由Painsky和Rosset [IEEE Trans. Pattern Anal. Mach. Intell., 38 (2016), pp. 308-321] 扩展适用于不可微损失。GIRP算法具有吸引人的特点，即在算法的每一步中，中间解满足等增约束。文章以一个例子开始，展示了文献中描述的GIRP算法可能无法产生等增模型的情况，表明必须仔细讨论等增回归问题的解的存在性和唯一性。文章接着展示，可能存在许多解之一，可以通过对观察数据集进行递归二分分割来找到解。一个小的修改

    This paper presents an in-depth analysis of the generalized isotonic recursive partitioning (GIRP) algorithm for fitting isotonic models under separable convex losses, proposed by Luss and Rosset [J. Comput. Graph. Statist., 23 (2014), pp. 192--201] for differentiable losses and extended by Painsky and Rosset [IEEE Trans. Pattern Anal. Mach. Intell., 38 (2016), pp. 308-321] for nondifferentiable losses. The GIRP algorithm poseses an attractive feature that in each step of the algorithm, the intermediate solution satisfies the isotonicity constraint. The paper begins with an example showing that the GIRP algorithm as described in the literature may fail to produce an isotonic model, suggesting that the existence and uniqueness of the solution to the isotonic regression problem must be carefully addressed. It proceeds with showing that, among possibly many solutions, there indeed exists a solution that can be found by recursive binary partitioning of the set of observed data. A small mod
    
[^53]: T-PRIME: 基于Transformer的边缘机器学习协议识别

    T-PRIME: Transformer-based Protocol Identification for Machine-learning at the Edge. (arXiv:2401.04837v1 [cs.LG])

    [http://arxiv.org/abs/2401.04837](http://arxiv.org/abs/2401.04837)

    T-PRIME是一个基于Transformer的边缘机器学习协议识别方法，通过注意机制学习传输帧的结构设计，克服了传统方法的局限性。实验证明其在深度学习硬件限制下的实时可行性，并证明了其优于传统方法和最先进的神经网络。

    

    频谱共享允许相同标准（例如802.11系列）或不同标准（例如LTE和DVB）的不同协议在重叠的频段中共存。随着这种范式的推广，无线系统必须在故意损坏前导码、极低信噪比和挑战性的信道条件下实时识别活动发射器和未经授权的波形。通过设计T-PRIME：一种基于Transformer的机器学习方法，我们克服了在这种情况下基于相关性的前导码匹配方法的局限性。T-PRIME通过其注意机制学习传输帧的结构设计，查看超出前导码的序列模式。本文提出了三个贡献：首先，对比Transformer模型并证明其优于传统方法和最先进的神经网络。其次，严格分析了T-PRIME在深度学习硬件限制下的实时可行性。

    Spectrum sharing allows different protocols of the same standard (e.g., 802.11 family) or different standards (e.g., LTE and DVB) to coexist in overlapping frequency bands. As this paradigm continues to spread, wireless systems must also evolve to identify active transmitters and unauthorized waveforms in real time under intentional distortion of preambles, extremely low signal-to-noise ratios and challenging channel conditions. We overcome limitations of correlation-based preamble matching methods in such conditions through the design of T-PRIME: a Transformer-based machine learning approach. T-PRIME learns the structural design of transmitted frames through its attention mechanism, looking at sequence patterns that go beyond the preamble alone. The paper makes three contributions: First, it compares Transformer models and demonstrates their superiority over traditional methods and state-of-the-art neural networks. Second, it rigorously analyzes T-PRIME's real-time feasibility on Deep
    
[^54]: GNNShap: 使用Shapley值快速而准确解释GNN的论文

    GNNShap: Fast and Accurate GNN Explanations using Shapley Values. (arXiv:2401.04829v1 [cs.LG])

    [http://arxiv.org/abs/2401.04829](http://arxiv.org/abs/2401.04829)

    GNNShap是一种使用Shapley值的解释方法，能够快速而准确地解释图神经网络的预测结果。相较于其他方法，GNNShap通过抽样、并行化计算等技术提高了解释速度和精细度。

    

    图神经网络(GNN)是一种在科学领域中具有广泛应用的图机器学习模型。然而，GNN被认为是黑盒模型，很难理解模型如何进行预测。基于博弈论的Shapley值方法在其他领域中被广泛应用于解释模型，但在图领域中研究较少。一些研究已经提出了基于Shapley值的GNN解释方法，然而它们存在一些限制：它们只考虑了有限的样本来近似Shapley值；有些方法主要关注小和大的联盟大小，并且它们比其他解释方法慢了一个数量级，使得它们在中等规模的图中无法应用。在这项工作中，我们提出了GNNShap，它提供边的解释，因为它们对图提供了更自然和精细的解释。我们通过对所有联盟大小进行抽样、在GPU上并行抽样和加速模型等方面克服了这些限制。

    Graph neural networks (GNNs) are popular machine learning models for graphs with many applications across scientific domains. However, GNNs are considered black box models, and it is challenging to understand how the model makes predictions. Game theory-based Shapley value approaches are popular explanation methods in other domains but are not well-studied for graphs. Some studies have proposed Shapley value-based GNN explanations, yet they have several limitations: they consider limited samples to approximate Shapley values; some mainly focus on small and large coalition sizes, and they are an order of magnitude slower than other explanation methods, making them inapplicable to even moderate-size graphs. In this work, we propose GNNShap, which provides explanations for edges since they provide more natural explanations for graphs and more fine-grained explanations. We overcome the limitations by sampling from all coalition sizes, parallelizing the sampling on GPUs, and speeding up mod
    
[^55]: 大流行的前100天；药物、行为和数字干预的相互作用——基于Agent-Based模型的研究。

    First 100 days of pandemic; an interplay of pharmaceutical, behavioral and digital interventions -- A study using agent based modeling. (arXiv:2401.04795v1 [cs.MA])

    [http://arxiv.org/abs/2401.04795](http://arxiv.org/abs/2401.04795)

    本研究通过Agent-Based模型模拟了药物、行为和数字干预的相互作用，并建议综合运用这些干预措施应对大流行疫情。通过分析发现，最初的100天对决定疫情发展至关重要，强调了迅速决策和高效政策制定的重要性。

    

    大流行，特别是最近的COVID-19疫情爆发，对公共卫生和全球经济产生了影响。因此，我们需要深入了解疾病发展趋势和高效的应对策略，以应对潜在的未来疫情爆发。本文强调了代理人模型（Agent-Based Models，ABM）在捕捉复杂的感染动态和理解干预措施影响的潜力。我们模拟了反映现实政策采纳中的挑战的真实药物、行为和数字干预，并提出了这些干预的整体组合用于大流行疫情应对。通过这些模拟，我们根据华盛顿州金斯县的真实社会人口统计数据和地理普查数据，研究了大规模人口的新兴行为趋势。我们的分析揭示了最初100天在决定大流行疫情走势方面的关键作用，强调了快速决策和高效政策制定的重要性。

    Pandemics, notably the recent COVID-19 outbreak, have impacted both public health and the global economy. A profound understanding of disease progression and efficient response strategies is thus needed to prepare for potential future outbreaks. In this paper, we emphasize the potential of Agent-Based Models (ABM) in capturing complex infection dynamics and understanding the impact of interventions. We simulate realistic pharmaceutical, behavioral, and digital interventions that mirror challenges in real-world policy adoption and suggest a holistic combination of these interventions for pandemic response. Using these simulations, we study the trends of emergent behavior on a large-scale population based on real-world socio-demographic and geo-census data from Kings County in Washington. Our analysis reveals the pivotal role of the initial 100 days in dictating a pandemic's course, emphasizing the importance of quick decision-making and efficient policy development. Further, we highligh
    
[^56]: BGK方程的双曲线机器学习矩闭包

    Hyperbolic Machine Learning Moment Closures for the BGK Equations. (arXiv:2401.04783v1 [math.NA])

    [http://arxiv.org/abs/2401.04783](http://arxiv.org/abs/2401.04783)

    这篇论文介绍了一种使用神经网络训练的双曲线闭包模型，用于BGK动力模型的Grad矩展开，以实现最高矩的梯度的精确闭合关系。

    

    我们使用神经网络（NN）在BGK动力模型的矩数据上进行训练，引入了对Bhatnagar-Gross-Krook（BGK）动力模型的Grad矩展开的双曲线闭包。这个闭包是基于我们在输运封闭中导出的自由流极限的精确封闭关系而提出的。这个精确封闭关系将最高矩的梯度与四个较低矩的梯度相关联。与我们过去的工作一样，这里介绍的模型通过较低矩的梯度系数来学习最高矩的梯度。这意味着得到的双曲系统在最高矩上并非守恒。为了稳定性，神经网络的输出层被设计成强制双曲性和Galileo不变性。这确保模型能够在NN的训练窗口之外运行。与我们以前处理线性模型的辐射输运工作不同，BGK模型的非线性性要求更高级的训练。

    We introduce a hyperbolic closure for the Grad moment expansion of the Bhatnagar-Gross-Krook's (BGK) kinetic model using a neural network (NN) trained on BGK's moment data. This closure is motivated by the exact closure for the free streaming limit that we derived in our paper on closures in transport \cite{Huang2022-RTE1}. The exact closure relates the gradient of the highest moment to the gradient of four lower moments. As with our past work, the model presented here learns the gradient of the highest moment in terms of the coefficients of gradients for all lower ones. By necessity, this means that the resulting hyperbolic system is not conservative in the highest moment. For stability, the output layers of the NN are designed to enforce hyperbolicity and Galilean invariance. This ensures the model can be run outside of the training window of the NN. Unlike our previous work on radiation transport that dealt with linear models, the BGK model's nonlinearity demanded advanced training 
    
[^57]: 利用生成神经网络模拟特征函数

    Generative neural networks for characteristic functions. (arXiv:2401.04778v1 [stat.ML])

    [http://arxiv.org/abs/2401.04778](http://arxiv.org/abs/2401.04778)

    本论文研究了利用生成神经网络模拟特征函数的问题，并通过构建一个普适且无需假设的生成神经网络来解决。研究基于最大均值差异度量，并提出了有关逼近质量的有限样本保证。

    

    在这项工作中，我们提供了一个模拟算法来从一个（多元）特征函数中模拟，该特征函数仅以黑盒格式可访问。我们构建了一个生成神经网络，其损失函数利用最大均值差异度量的特定表示，直接结合目标特征函数。这种构造具有普遍性，不依赖于维度，并且不需要对给定特征函数进行任何假设。此外，还得出了关于最大均值差异度量的逼近质量的有限样本保证。该方法在一个短期模拟研究中进行了说明。

    In this work, we provide a simulation algorithm to simulate from a (multivariate) characteristic function, which is only accessible in a black-box format. We construct a generative neural network, whose loss function exploits a specific representation of the Maximum-Mean-Discrepancy metric to directly incorporate the targeted characteristic function. The construction is universal in the sense that it is independent of the dimension and that it does not require any assumptions on the given characteristic function. Furthermore, finite sample guarantees on the approximation quality in terms of the Maximum-Mean Discrepancy metric are derived. The method is illustrated in a short simulation study.
    
[^58]: 语言模型基准性能有多可预测？

    How predictable is language model benchmark performance?. (arXiv:2401.04757v1 [cs.LG])

    [http://arxiv.org/abs/2401.04757](http://arxiv.org/abs/2401.04757)

    本研究通过对十一个最近的模型架构在五个数量级的计算规模上进行了大规模的语言模型性能研究，发现将许多个体任务和评估聚合在一起的平均基准性能可以合理预测，但在个别任务中的预测性能仍存在挑战。

    

    我们对十一个最近的模型架构在五个数量级的计算规模上进行了大规模的语言模型性能研究。我们发现，将许多个体任务和评估聚合在一起，就像常用的BIG-Bench数据集一样，平均基准性能在训练计算规模的函数下是可以合理预测的。具体来说，当在计算中扩大一个数量级时，我们观察到BIG-Bench Hard的平均绝对误差为6个百分点（pp）。相比之下，在计算中扩大一个数量级的个别BIG-Bench任务的外推平均误差为18pp。不过，个别任务的性能仍然比随机结果更可预测。总之，我们的研究表明，计算规模提供了一种有前景的方法来预测多样化基准中的AI能力，但在特定任务中预测性能仍然存在挑战。

    We investigate large language model performance across five orders of magnitude of compute scaling in eleven recent model architectures. We show that average benchmark performance, aggregating over many individual tasks and evaluations as in the commonly-used BIG-Bench dataset, is decently predictable as a function of training compute scale. Specifically, when extrapolating BIG-Bench Hard performance across one order of magnitude in compute, we observe average absolute errors of 6 percentage points (pp). By contrast, extrapolation for individual BIG-Bench tasks across an order of magnitude in compute yields higher average errors of 18pp. Nonetheless, individual task performance remains significantly more predictable than chance. Overall, our work suggests compute scaling provides a promising basis to forecast AI capabilities in diverse benchmarks, though predicting performance in specific tasks poses challenges.
    
[^59]: 识别感应炉最佳熔化模式：基于时间序列KMeans聚类和多标准决策方法的数据驱动方法

    Identifying Best Practice Melting Patterns in Induction Furnaces: A Data-Driven Approach Using Time Series KMeans Clustering and Multi-Criteria Decision Making. (arXiv:2401.04751v1 [cs.LG])

    [http://arxiv.org/abs/2401.04751](http://arxiv.org/abs/2401.04751)

    本研究通过时间序列KMeans聚类的数据驱动方法，识别出了感应炉中的最佳熔化模式，并利用多准则决策方法确定了最佳实践簇。

    

    提高工业生产过程的能源效率对于竞争力和符合气候政策至关重要。本文介绍了一种数据驱动方法，用于识别感应炉中的最佳熔化模式。通过时间序列KMeans聚类，可以根据温度曲线将熔化模式分类为不同的簇。使用弯曲拐点法，确定了12个簇，代表了熔化模式的范围。针对每个簇建立了熔化时间、能源特定性能和碳成本等性能参数，指示了炉子的效率和环境影响。利用简单加权、乘法指数加权、TOPSIS法、改进的TOPSIS法和VlseKriterijumska Optimizacija I Kompromisno Resenje等多准则决策方法来确定最佳实践簇。研究成功地识别了最佳实践簇。

    Improving energy efficiency in industrial production processes is crucial for competitiveness, and compliance with climate policies. This paper introduces a data-driven approach to identify optimal melting patterns in induction furnaces. Through time-series K-means clustering the melting patterns could be classified into distinct clusters based on temperature profiles. Using the elbow method, 12 clusters were identified, representing the range of melting patterns. Performance parameters such as melting time, energy-specific performance, and carbon cost were established for each cluster, indicating furnace efficiency and environmental impact. Multiple criteria decision-making methods including Simple Additive Weighting, Multiplicative Exponential Weighting, Technique for Order of Preference by Similarity to Ideal Solution, modified TOPSIS, and VlseKriterijumska Optimizacija I Kompromisno Resenje were utilized to determine the best-practice cluster. The study successfully identified the 
    
[^60]: LogFormer：一种适用于日志异常检测的预训练和调优流程

    LogFormer: A Pre-train and Tuning Pipeline for Log Anomaly Detection. (arXiv:2401.04749v1 [cs.LG])

    [http://arxiv.org/abs/2401.04749](http://arxiv.org/abs/2401.04749)

    LogFormer是一种预训练和调优流程，能够提高日志异常检测在不同领域之间的泛化能力。它通过在源领域上进行预训练并利用共享参数将知识转移到目标领域，同时引入Log-Attention模块来补充被日志配对忽略的信息。

    

    日志异常检测是人工智能运维（AIOps）领域的关键组成部分。考虑到不同领域的日志数据，在实际工业场景中重新训练整个网络以适应未知领域是低效的。然而，先前的深度模型仅关注于在同一领域中提取日志序列的语义信息，导致在多领域日志上的泛化能力较差。为了解决这个问题，我们提出了一种基于Transformer的Log异常检测统一框架(LogFormer)，以改善在不同领域之间的泛化能力，并建立了包括预训练和基于adapter的调优阶段的两阶段流程。具体而言，我们的模型首先在源领域上进行预训练，以获取日志数据的共享语义知识。然后，通过共享参数将这种知识转移到目标领域。此外，我们提出了Log-Attention模块，用于补充被日志配对忽略的信息。该方法是经过实验证明的。

    Log anomaly detection is a key component in the field of artificial intelligence for IT operations (AIOps). Considering log data of variant domains, retraining the whole network for unknown domains is inefficient in real industrial scenarios. However, previous deep models merely focused on extracting the semantics of log sequences in the same domain, leading to poor generalization on multi-domain logs. To alleviate this issue, we propose a unified Transformer-based framework for Log anomaly detection (LogFormer) to improve the generalization ability across different domains, where we establish a two-stage process including the pre-training and adapter-based tuning stage. Specifically, our model is first pre-trained on the source domain to obtain shared semantic knowledge of log data. Then, we transfer such knowledge to the target domain via shared parameters. Besides, the Log-Attention module is proposed to supplement the information ignored by the log-paring. The proposed method is ev
    
[^61]: 基于卷积神经网络集成学习的无控制农场环境下的高光谱图像黑莓果实成熟度检测

    Convolutional Neural Network Ensemble Learning for Hyperspectral Imaging-based Blackberry Fruit Ripeness Detection in Uncontrolled Farm Environment. (arXiv:2401.04748v1 [cs.CV])

    [http://arxiv.org/abs/2401.04748](http://arxiv.org/abs/2401.04748)

    本文提出了一种基于卷积神经网络集成学习的方法，用于在无控制农场环境下检测黑莓果实的细微成熟特征。最近的研究已经开始使用深度学习技术来提取黑莓果实图像的特征，以判断其成熟度。本研究通过使用预训练的VGG16模型，构建了一个多输入CNN进行集成分类，以解决黑莓果实成熟度检测中的困难。

    

    多年来，果实成熟度估计模型一直依赖于光谱指数特征或基于颜色的特征，如均值、标准差、偏度、颜色矩和/或直方图。最近，一些研究开始探索使用深度学习技术从黑莓果实的图像中提取特征来判断其成熟度。然而，黑莓果实在成熟时没有明显可靠的可见性特征，因此对采摘者来说具有很大的困难。为了解决这个工程应用挑战，本文提出了一种新颖的多输入卷积神经网络（CNN）集成分类器，用于检测黑莓果实成熟度的细微特征。多输入CNN是由在ImageNet数据集上训练的预训练视觉几何组16层深度卷积网络（VGG16）模型创建的。

    Fruit ripeness estimation models have for decades depended on spectral index features or colour-based features, such as mean, standard deviation, skewness, colour moments, and/or histograms for learning traits of fruit ripeness. Recently, few studies have explored the use of deep learning techniques to extract features from images of fruits with visible ripeness cues. However, the blackberry (Rubus fruticosus) fruit does not show obvious and reliable visible traits of ripeness when mature and therefore poses great difficulty to fruit pickers. The mature blackberry, to the human eye, is black before, during, and post-ripening. To address this engineering application challenge, this paper proposes a novel multi-input convolutional neural network (CNN) ensemble classifier for detecting subtle traits of ripeness in blackberry fruits. The multi-input CNN was created from a pre-trained visual geometry group 16-layer deep convolutional network (VGG16) model trained on the ImageNet dataset. Th
    
[^62]: 使用Vision Transformer进行皮肤癌分割和分类，用于基于皮肤镜的非侵入式数字系统的自动分析

    Skin Cancer Segmentation and Classification Using Vision Transformer for Automatic Analysis in Dermatoscopy-based Non-invasive Digital System. (arXiv:2401.04746v1 [eess.IV])

    [http://arxiv.org/abs/2401.04746](http://arxiv.org/abs/2401.04746)

    本论文提出了一种使用Vision Transformer进行皮肤癌分割和分类的方法，采用自注意机制捕捉复杂的空间依赖关系，实现了优于传统深度学习架构的性能，并展示了高准确率和潜在的有效性。

    

    皮肤癌是全球健康关注的问题，需要早期和准确的诊断以提高患者预后。本研究引入了一种创新的皮肤癌分类方法，采用了Vision Transformer，这是一种在各种图像分析任务中取得成功的先进深度学习架构。利用HAM10000数据集的10,015个精确注释的皮肤病变图像，模型经过预处理以提高鲁棒性。适应于皮肤癌分类任务的Vision Transformer利用自注意机制捕捉复杂的空间依赖关系，相较于传统深度学习架构具有卓越性能。Segment Anything Model有助于精确分割癌变区域，达到高IOU和Dice系数。广泛的实验证明了该模型的优越性，特别是基于Google的ViT patch-32变体，其准确率达到96.15%，展示了作为有效工具的潜力。

    Skin cancer is a global health concern, necessitating early and accurate diagnosis for improved patient outcomes. This study introduces a groundbreaking approach to skin cancer classification, employing the Vision Transformer, a state-of-the-art deep learning architecture renowned for its success in diverse image analysis tasks. Utilizing the HAM10000 dataset of 10,015 meticulously annotated skin lesion images, the model undergoes preprocessing for enhanced robustness. The Vision Transformer, adapted to the skin cancer classification task, leverages the self-attention mechanism to capture intricate spatial dependencies, achieving superior performance over traditional deep learning architectures. Segment Anything Model aids in precise segmentation of cancerous areas, attaining high IOU and Dice Coefficient. Extensive experiments highlight the model's supremacy, particularly the Google-based ViT patch-32 variant, which achieves 96.15% accuracy and showcases potential as an effective tool
    
[^63]: 基于蒙特卡洛Dropout的贝叶斯神经网络在自旋电子学中的测试

    Testing Spintronics Implemented Monte Carlo Dropout-Based Bayesian Neural Networks. (arXiv:2401.04744v1 [cs.ET])

    [http://arxiv.org/abs/2401.04744](http://arxiv.org/abs/2401.04744)

    本文提出了一种基于蒙特卡洛Dropout的贝叶斯神经网络测试框架，用于分析自旋电子学中模糊不确定性估计和准确性，为资源受限的安全关键应用提供了有效的方法。

    

    贝叶斯神经网络（BayNNs）可以内在地估计预测的不确定性，方便决策制定。基于Dropout的BayNNs越来越多地应用于自旋电子学计算内存架构中，用于资源受限但性能要求高的安全关键应用。尽管不确定性估计很重要，但现有的工作忽视了Dropout生成和BayNN计算的可靠性对于目标应用同样重要。然而，与传统的神经网络相比，测试BayNNs更具挑战性，因为它们具有随机性质。在本文中，我们首次提出了基于自旋电子学Dropout模块的非理想性模型，并分析了它们对不确定性估计和准确性的影响。此外，我们还提出了一种基于可重复性排名的测试框架，用于Dropout-based BayNN，可以达到100%的故障覆盖率，同时仅使用0.2%的训练数据作为测试向量。

    Bayesian Neural Networks (BayNNs) can inherently estimate predictive uncertainty, facilitating informed decision-making. Dropout-based BayNNs are increasingly implemented in spintronics-based computation-in-memory architectures for resource-constrained yet high-performance safety-critical applications. Although uncertainty estimation is important, the reliability of Dropout generation and BayNN computation is equally important for target applications but is overlooked in existing works. However, testing BayNNs is significantly more challenging compared to conventional NNs, due to their stochastic nature. In this paper, we present for the first time the model of the non-idealities of the spintronics-based Dropout module and analyze their impact on uncertainty estimates and accuracy. Furthermore, we propose a testing framework based on repeatability ranking for Dropout-based BayNN with up to $100\%$ fault coverage while using only $0.2\%$ of training data as test vectors.
    
[^64]: 无预定义聚类数k的图聚类的Masked AutoEncoder

    Masked AutoEncoder for Graph Clustering without Pre-defined Cluster Number k. (arXiv:2401.04741v1 [cs.LG])

    [http://arxiv.org/abs/2401.04741](http://arxiv.org/abs/2401.04741)

    本论文提出了一种新的图聚类方法，通过加入掩码自编码器和改进的基于密度的聚类算法，能够在没有预定义的聚类数情况下实现高效的图聚类，并具有良好的泛化能力。

    

    最近，基于自编码器结构的图聚类算法因其高效性能和低训练成本而受到广泛关注。然而，对于基于GCN或GAT的现有图自编码聚类算法，它们不仅缺乏良好的泛化能力，而且难以自动确定由此类自编码模型聚类的簇数。为了解决这个问题，我们提出了一种新的框架，称为Masked Autoencoders进行图聚类（GCMA）。它采用基于图屏蔽方法的融合自编码器进行图的融合编码。它引入我们改进的基于密度的聚类算法作为第二个解码器，在多目标重建时解码。通过解码掩码嵌入，我们的模型可以捕获更广义和全面的知识。在提高泛化能力的同时可以输出聚类数和聚类结果。作为一种非参数类方法，我们进行了大量的实验...

    Graph clustering algorithms with autoencoder structures have recently gained popularity due to their efficient performance and low training cost. However, for existing graph autoencoder clustering algorithms based on GCN or GAT, not only do they lack good generalization ability, but also the number of clusters clustered by such autoencoder models is difficult to determine automatically. To solve this problem, we propose a new framework called Graph Clustering with Masked Autoencoders (GCMA). It employs our designed fusion autoencoder based on the graph masking method for the fusion coding of graph. It introduces our improved density-based clustering algorithm as a second decoder while decoding with multi-target reconstruction. By decoding the mask embedding, our model can capture more generalized and comprehensive knowledge. The number of clusters and clustering results can be output end-to-end while improving the generalization ability. As a nonparametric class method, extensive exper
    
[^65]: MSX销售协同助手中生成式人工智能的案例研究: 通过实时问答系统改善销售人员的工作效率以实现内容推荐

    A case study of Generative AI in MSX Sales Copilot: Improving seller productivity with a real-time question-answering system for content recommendation. (arXiv:2401.04732v1 [cs.IR])

    [http://arxiv.org/abs/2401.04732](http://arxiv.org/abs/2401.04732)

    本论文设计了一个实时问答系统，通过LLM嵌入与销售材料进行匹配，提供给销售人员实时推荐，从而提高销售人员的工作效率。这一解决方案可以在几秒钟内返回最相关的内容推荐，即使对于大规模数据集也是如此。这一推荐系统已成功集成到微软销售人员每日使用的Dynamics CRM的生产版本中。

    

    本文设计了一个实时问答系统，专门为销售人员提供有关材料/文档的实时推荐，以便与客户分享或在电话中参考。通过使用Seismic销售资料的相对较大规模的多样化数据集，我们展示了如何使用卖方查询的LLM嵌入与相关内容相匹配。我们通过以详细的方式设计提示语，并利用可用的丰富的文档和销售者元特征集，实现了这一目标。通过使用具有交叉编码器重排序器架构的双编码器，我们展示了该解决方案在仅几秒钟内即可返回最相关的内容推荐，即使对于大规模数据集也是如此。我们的推荐系统已部署为用于实时推理的AML端点，并已集成到Copilot界面中，该界面现已部署在每日由微软销售人员使用的Dynamics CRM的生产版本中(MSX）。

    In this paper, we design a real-time question-answering system specifically targeted for helping sellers get relevant material/documentation they can share live with their customers or refer to during a call. Taking the Seismic content repository as a relatively large scale example of a diverse dataset of sales material, we demonstrate how LLM embeddings of sellers' queries can be matched with the relevant content. We achieve this by engineering prompts in an elaborate fashion that makes use of the rich set of meta-features available for documents and sellers. Using a bi-encoder with cross-encoder re-ranker architecture, we show how the solution returns the most relevant content recommendations in just a few seconds even for large datasets. Our recommender system is deployed as an AML endpoint for real-time inferencing and has been integrated into a Copilot interface that is now deployed in the production version of the Dynamics CRM, known as MSX, used daily by Microsoft sellers.
    
[^66]: RoSA: 通过鲁棒适应实现准确的参数高效微调

    RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])

    [http://arxiv.org/abs/2401.04679](http://arxiv.org/abs/2401.04679)

    RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。

    

    我们研究了在大语言模型 (LLMs) 的背景下，能够在有限的计算和内存预算下提供良好准确性的参数高效微调 (PEFT) 方法。我们提出了一种新的PEFT方法，称为RoSA，受鲁棒主成分分析 (PCA) 的启发，它在一组固定的预训练权重上共同训练$\textit{低秩}$和$\textit{高度稀疏}$的组件，以高效近似完全微调（FFT）解决方案的性能。我们展示了RoSA在一系列具有挑战性的生成任务上的性能，例如小学数学和SQL查询生成，这些任务需要进行微调以获得良好性能，我们证明了在相同的参数预算下，RoSA优于LoRA和纯粹的稀疏微调。我们通过稀疏GPU内核为RoSA提供系统支持，以补充训练算法，从而实现内存和计算效率的训练。我们的代码将在https://github.com/IST-DASLab上提供。

    We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
    
[^67]: 深度高效的私密领域生成用于子图联邦学习

    Deep Efficient Private Neighbor Generation for Subgraph Federated Learning. (arXiv:2401.04336v1 [cs.LG])

    [http://arxiv.org/abs/2401.04336](http://arxiv.org/abs/2401.04336)

    本文提出了FedDEP，用于解决子图联邦学习中的信息传播不完整的问题，并提出了一系列新颖的技术设计，包括深度邻居生成和高效的私密领域生成。

    

    在现实应用中，巨大图通常以非中心化子图的形式由多个数据所有者分散存储。为了保护数据隐私，在不损害数据隐私的前提下，考虑到子图联邦学习（subgraph FL）场景是很自然的，其中每个本地客户端持有整个全局图的子图，以获取全局一般化的图挖掘模型。为了解决由于缺少跨子图邻居而导致的局部子图上的信息传播不完整的独特挑战，以前的工作通过缺失邻居生成器和GNN的联合FL来增加本地邻域。然而，它们在FL的效用性、效率性和隐私目标方面存在深层次的限制。在这项工作中，我们提出了FedDEP来全面解决子图FL中的这些挑战。FedDEP包括一系列新颖的技术设计：(1) 利用潜在缺失邻居的GNN嵌入进行深度邻居生成；(2) Effic...

    Behemoth graphs are often fragmented and separately stored by multiple data owners as distributed subgraphs in many realistic applications. Without harming data privacy, it is natural to consider the subgraph federated learning (subgraph FL) scenario, where each local client holds a subgraph of the entire global graph, to obtain globally generalized graph mining models. To overcome the unique challenge of incomplete information propagation on local subgraphs due to missing cross-subgraph neighbors, previous works resort to the augmentation of local neighborhoods through the joint FL of missing neighbor generators and GNNs. Yet their technical designs have profound limitations regarding the utility, efficiency, and privacy goals of FL. In this work, we propose FedDEP to comprehensively tackle these challenges in subgraph FL. FedDEP consists of a series of novel technical designs: (1) Deep neighbor generation through leveraging the GNN embeddings of potential missing neighbors; (2) Effic
    
[^68]: 无监督的测试时自适应：通过插入和播放变换器模块

    Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules. (arXiv:2401.04130v1 [cs.LG])

    [http://arxiv.org/abs/2401.04130](http://arxiv.org/abs/2401.04130)

    这项工作介绍了PLUTO:一种插拔式模块化的测试时领域适应策略，通过预先训练一系列针对不同源领域的模块，有效地创建了一个"模块存储库"。采用无监督的测试时自适应方法，从存储库中选择稀疏的相关模块的子集，并创建选中模块的加权组合，实现了对新领域的自适应。

    

    参数高效调优(PET)方法，如LoRA、Adapter和Visual Prompt Tuning(VPT)，通过调整变换器模型中的小模块，在使适应新领域方面取得了成功。然而，在测试过程中遇到的领域数量可能非常大，数据通常是无标签的。因此，适应新领域是具有挑战性的，也不现实为每个这样的领域生成定制的调整模块。为了应对这些挑战，本文引入了PLUTO：一种插拔模块化的测试时领域适应策略。我们预训练了一系列模块，每个模块专为不同的源领域进行了专门设计，有效地创建了一个"模块存储库"。给定一个带有少样本无标签数据的目标域，我们引入了一种无监督的测试时自适应(TTA)方法，来(1)从库中选择出稀疏的相关模块的子集，并且(2)在不调整权重的情况下创建选中模块的加权组合。这种插拔式的特性使得它可===

    Parameter-efficient tuning (PET) methods such as LoRA, Adapter, and Visual Prompt Tuning (VPT) have found success in enabling adaptation to new domains by tuning small modules within a transformer model. However, the number of domains encountered during test time can be very large, and the data is usually unlabeled. Thus, adaptation to new domains is challenging; it is also impractical to generate customized tuned modules for each such domain. Toward addressing these challenges, this work introduces PLUTO: a Plug-and-pLay modUlar Test-time domain adaptatiOn strategy. We pre-train a large set of modules, each specialized for different source domains, effectively creating a ``module store''. Given a target domain with few-shot unlabeled data, we introduce an unsupervised test-time adaptation (TTA) method to (1) select a sparse subset of relevant modules from this store and (2) create a weighted combination of selected modules without tuning their weights. This plug-and-play nature enable
    
[^69]: 结构化神经退行性卷积神经网络用于阿尔茨海默病建模和分类

    Structure-focused Neurodegeneration Convolutional Neural Network for Modeling and Classification of Alzheimer's Disease. (arXiv:2401.03922v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2401.03922](http://arxiv.org/abs/2401.03922)

    本论文提出了一种结构化神经退行性卷积神经网络，用于AD和MCI的识别。该网络考虑了局部结构特征，可以更准确地进行早期AD的诊断。

    

    阿尔茨海默病（AD）作为主要的痴呆形式，对全球构成了一个不断增长的挑战，并强调了准确和早期诊断的紧迫性。临床技术中，放射科医生在使用磁共振成像（MRI）区分轻度认知障碍（MCI）和AD时遇到了困难，因为这些方法不一致且不可靠。已经证明，机器学习有望提供早期AD诊断的可能性。然而，现有的模型主要关注细粒度的局部特征，而没有考虑到可以提供大脑皮层神经退行信息的局部结构特征。因此，本文提出了一种机器学习（ML）框架，该框架集成了Gamma校正（一种图像增强技术），并使用一种名为SNeurodCNN的结构化神经退行性卷积神经网络（CNN）架构，用于区分AD和MCI。该ML框架利用了中矢状面和旁矢状面的大脑图像视点。

    Alzheimer's disease (AD), the predominant form of dementia, poses a growing global challenge and underscores the urgency of accurate and early diagnosis. The clinical technique radiologists adopt for distinguishing between mild cognitive impairment (MCI) and AD using Machine Resonance Imaging (MRI) encounter hurdles because they are not consistent and reliable. Machine learning has been shown to offer promise for early AD diagnosis. However, existing models focused on focal fine-grain features without considerations to focal structural features that give off information on neurodegeneration of the brain cerebral cortex. Therefore, this paper proposes a machine learning (ML) framework that integrates Gamma correction, an image enhancement technique, and includes a structure-focused neurodegeneration convolutional neural network (CNN) architecture called SNeurodCNN for discriminating between AD and MCI. The ML framework leverages the mid-sagittal and para-sagittal brain image viewpoints 
    
[^70]: 上下文固定预算的最佳臂识别：适应性实验设计与策略学习

    Contextual Fixed-Budget Best Arm Identification: Adaptive Experimental Design with Policy Learning. (arXiv:2401.03756v1 [cs.LG])

    [http://arxiv.org/abs/2401.03756](http://arxiv.org/abs/2401.03756)

    该论文研究了个性化治疗推荐的问题，提出了一个上下文固定预算的最佳臂识别模型，通过自适应实验设计和策略学习来推荐最佳治疗方案，并通过最坏情况下的期望简单遗憾来衡量推荐的有效性。

    

    个性化治疗推荐是基于证据的决策中的关键任务。在这项研究中，我们将这个任务作为一个带有上下文信息的固定预算最佳臂识别（Best Arm Identification, BAI）问题来进行建模。在这个设置中，我们考虑了一个给定多个治疗臂的自适应试验。在每一轮中，决策者观察一个刻画实验单位的上下文（协变量），并将该单位分配给其中一个治疗臂。在实验结束时，决策者推荐一个在给定上下文条件下预计产生最高期望结果的治疗臂（最佳治疗臂）。该决策的有效性通过最坏情况下的期望简单遗憾（策略遗憾）来衡量，该遗憾表示在给定上下文条件下，最佳治疗臂和推荐治疗臂的条件期望结果之间的最大差异。我们的初始步骤是推导最坏情况下期望简单遗憾的渐近下界，该下界还暗示着解决该问题的一些思路。

    Individualized treatment recommendation is a crucial task in evidence-based decision-making. In this study, we formulate this task as a fixed-budget best arm identification (BAI) problem with contextual information. In this setting, we consider an adaptive experiment given multiple treatment arms. At each round, a decision-maker observes a context (covariate) that characterizes an experimental unit and assigns the unit to one of the treatment arms. At the end of the experiment, the decision-maker recommends a treatment arm estimated to yield the highest expected outcome conditioned on a context (best treatment arm). The effectiveness of this decision is measured in terms of the worst-case expected simple regret (policy regret), which represents the largest difference between the conditional expected outcomes of the best and recommended treatment arms given a context. Our initial step is to derive asymptotic lower bounds for the worst-case expected simple regret, which also implies idea
    
[^71]: 关于深度学习框架开发中自动识别假设的探索性研究

    An exploratory study on automatic identification of assumptions in the development of deep learning frameworks. (arXiv:2401.03653v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2401.03653](http://arxiv.org/abs/2401.03653)

    本研究以构建一个新的最大假设数据集为基础，针对深度学习框架开发中手动识别假设的问题进行了探索性研究。在该研究中，我们发现手动识别假设的成本高，因此探讨了使用传统机器学习模型和流行的深度学习模型来识别假设的性能。

    

    利益相关方在深度学习框架开发中经常做出假设。这些假设涉及各种软件构件（例如需求、设计决策和技术债务），可能会被证明无效，从而导致系统故障。现有的假设管理方法和工具通常依赖于手动识别假设。然而，假设分散在深度学习框架开发的各种源头（例如代码注释、提交、拉取请求和问题）中，手动识别假设成本较高（例如时间和资源消耗）。为了解决深度学习框架开发中手动识别假设的问题，我们构建了一个新的并且最大的假设数据集（称为AssuEval），该数据集收集自GitHub上的TensorFlow和Keras代码库；我们探讨了七个传统的机器学习模型（例如支持向量机、分类回归树）和一个流行的深度学习模型的性能。

    Stakeholders constantly make assumptions in the development of deep learning (DL) frameworks. These assumptions are related to various types of software artifacts (e.g., requirements, design decisions, and technical debt) and can turn out to be invalid, leading to system failures. Existing approaches and tools for assumption management usually depend on manual identification of assumptions. However, assumptions are scattered in various sources (e.g., code comments, commits, pull requests, and issues) of DL framework development, and manually identifying assumptions has high costs (e.g., time and resources). To overcome the issues of manually identifying assumptions in DL framework development, we constructed a new and largest dataset (i.e., AssuEval) of assumptions collected from the TensorFlow and Keras repositories on GitHub; explored the performance of seven traditional machine learning models (e.g., Support Vector Machine, Classification and Regression Trees), a popular DL model (i
    
[^72]: 预测天空：一种用于航班层次的乘客流量预测的新模型

    Predicting the Skies: A Novel Model for Flight-Level Passenger Traffic Forecasting. (arXiv:2401.03397v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.03397](http://arxiv.org/abs/2401.03397)

    本研究提出了一种新颖的多模态深度学习方法用于预测航班层次的乘客流量，相比传统模型取得了显著的准确性提升。该模型有效整合了循环神经网络和卷积神经网络，利用数据内部的时间模式和空间关系来增强预测性能。

    

    准确预测航班层次的乘客流量在航空公司运营中至关重要，影响从定价到路线优化等关键决策。本研究引入了一种新颖的多模态深度学习方法来解决航班层次乘客流量预测的挑战，相比传统模型，取得了显著的准确性提升。利用美国航空公司的大量数据集，我们的模型吸收了历史流量数据、票价关闭信息和每个航班特定的季节性属性。我们提出的神经网络将循环神经网络（RNN）和卷积神经网络（CNN）的优势进行了整合，利用数据内部的时间模式和空间关系来增强预测性能。我们成功的一个关键因素是全面的数据处理策略。我们构建了3D张量来表示数据，应用了精细的掩蔽策略来模拟真实世界的动态，并采用数据增强方法。

    Accurate prediction of flight-level passenger traffic is of paramount importance in airline operations, influencing key decisions from pricing to route optimization. This study introduces a novel, multimodal deep learning approach to the challenge of predicting flight-level passenger traffic, yielding substantial accuracy improvements compared to traditional models. Leveraging an extensive dataset from American Airlines, our model ingests historical traffic data, fare closure information, and seasonality attributes specific to each flight. Our proposed neural network integrates the strengths of Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN), exploiting the temporal patterns and spatial relationships within the data to enhance prediction performance. Crucial to the success of our model is a comprehensive data processing strategy. We construct 3D tensors to represent data, apply careful masking strategies to mirror real-world dynamics, and employ data augmentatio
    
[^73]: 人作为AI导师：增强人机协作强化学习以实现安全高效的自动驾驶

    Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])

    [http://arxiv.org/abs/2401.03160](http://arxiv.org/abs/2401.03160)

    本文提出了一种增强的人机协作强化学习方法，通过将人类智能注入到AI中实现混合交通编队中的安全高效自动驾驶。该方法将人类专家作为导师，允许代理在不确定环境中进行探索，同时在危险情况下接管控制以避免事故，并指导代理减小交通流干扰，优化交通流效果。

    

    尽管自动驾驶车辆（AVs）取得了重大进展，但确保AVs的安全性和交通流效率的驾驶策略的发展尚未得到充分探索。在本文中，我们提出了一种增强的人机协作强化学习方法，称为基于人作为AI导师的深度强化学习（HAIM-DRL）框架，以在混合交通编队中实现安全高效的自动驾驶。从人类学习过程中汲取灵感，我们首先引入了一种创新的学习范式，有效地将人类智能注入到AI中，称为人作为AI导师（HAIM）。在这个范式中，人类专家作为导师为AI代理提供帮助。在允许代理在不确定环境中进行充分探索的同时，人类专家可以在危险情况下接管控制，并展示正确的行动以避免潜在事故。另一方面，可以指导代理减小交通流干扰，从而优化交通流效果。

    Despite significant progress in autonomous vehicles (AVs), the development of driving policies that ensure both the safety of AVs and traffic flow efficiency has not yet been fully explored. In this paper, we propose an enhanced human-in-the-loop reinforcement learning method, termed the Human as AI mentor-based deep reinforcement learning (HAIM-DRL) framework, which facilitates safe and efficient autonomous driving in mixed traffic platoon. Drawing inspiration from the human learning process, we first introduce an innovative learning paradigm that effectively injects human intelligence into AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves as a mentor to the AI agent. While allowing the agent to sufficiently explore uncertain environments, the human expert can take control in dangerous situations and demonstrate correct actions to avoid potential accidents. On the other hand, the agent could be guided to minimize traffic flow disturbance, thereby optimizi
    
[^74]: 分散式多智能体主动搜索和跟踪当目标超过智能体数量时

    Decentralized Multi-Agent Active Search and Tracking when Targets Outnumber Agents. (arXiv:2401.03154v1 [cs.RO])

    [http://arxiv.org/abs/2401.03154](http://arxiv.org/abs/2401.03154)

    该论文介绍了一种处理多目标跟踪的分散式多智能体算法，该算法能在智能体数量少于目标数量时实现主动搜索和跟踪，并使用异步智能体通信来协调动作。

    

    多智能体多目标跟踪在野生动物巡逻、安全监控或环境监测等领域有广泛应用。现有算法常常做出一些限制性假设：目标数量和初始位置已知，或者智能体已被预分配到监控环境的不重叠分区，减轻了探索的负担。然而，当智能体数量少于目标数量时，这种假设会限制算法的适用性，因为智能体无法持续跟踪其视野中的目标。此外，多智能体跟踪算法还假设智能体间观测的同步，或者需要一个中央控制器来协调联合动作。相反，我们关注于分散式多智能体、多目标、同时主动搜索和跟踪的设置，其中智能体间通信是异步的。我们提出的算法DecSTER使用了一种基于概率假设密度滤波器的顺序蒙特卡洛实现。

    Multi-agent multi-target tracking has a wide range of applications, including wildlife patrolling, security surveillance or environment monitoring. Such algorithms often make restrictive assumptions: the number of targets and/or their initial locations may be assumed known, or agents may be pre-assigned to monitor disjoint partitions of the environment, reducing the burden of exploration. This also limits applicability when there are fewer agents than targets, since agents are unable to continuously follow the targets in their fields of view. Multi-agent tracking algorithms additionally assume inter-agent synchronization of observations, or the presence of a central controller to coordinate joint actions. Instead, we focus on the setting of decentralized multi-agent, multi-target, simultaneous active search-and-tracking with asynchronous inter-agent communication. Our proposed algorithm DecSTER uses a sequential monte carlo implementation of the probability hypothesis density filter fo
    
[^75]: Powerformer：适应不同传输区段的变压器架构用于电力流调整

    Powerformer: A Section-adaptive Transformer for Power Flow Adjustment. (arXiv:2401.02771v1 [cs.LG])

    [http://arxiv.org/abs/2401.02771](http://arxiv.org/abs/2401.02771)

    Powerformer是一种适应不同传输区段的变压器架构，用于学习稳健电力系统状态表示。它通过开发专用的区段自适应注意机制，并引入图神经网络传播和多因素注意机制来提供更加稳健的状态表示。在三个不同的电力系统场景上进行了广泛评估。

    

    本文提出了一种专为学习稳健电力系统状态表示而量身定制的变压器架构，旨在优化跨不同传输区段的电力调度以进行电力流调整。具体而言，我们的提出的方法名为Powerformer，开发了一种专用的区段自适应注意机制，与传统变压器中使用的自注意分离开来。该机制有效地将电力系统状态与传输区段信息整合在一起，有助于开发稳健的状态表示。此外，通过考虑电力系统的图拓扑和母线节点的电气属性，我们引入了两种定制策略来进一步增强表达能力：图神经网络传播和多因素注意机制。我们在三个电力系统场景（包括IEEE 118节点系统、中国实际300节点系统和一个大型系统）上进行了广泛的评估。

    In this paper, we present a novel transformer architecture tailored for learning robust power system state representations, which strives to optimize power dispatch for the power flow adjustment across different transmission sections. Specifically, our proposed approach, named Powerformer, develops a dedicated section-adaptive attention mechanism, separating itself from the self-attention used in conventional transformers. This mechanism effectively integrates power system states with transmission section information, which facilitates the development of robust state representations. Furthermore, by considering the graph topology of power system and the electrical attributes of bus nodes, we introduce two customized strategies to further enhance the expressiveness: graph neural network propagation and multi-factor attention mechanism. Extensive evaluations are conducted on three power system scenarios, including the IEEE 118-bus system, a realistic 300-bus system in China, and a large-
    
[^76]: 使用高分辨率多光谱无人机影像和机器学习来进行核桃水分胁迫的测绘

    Mapping Walnut water Stress with High Resolution Multispectral UAV Imagery and Machine Learning. (arXiv:2401.01375v1 [cs.CV])

    [http://arxiv.org/abs/2401.01375](http://arxiv.org/abs/2401.01375)

    本研究提出了一种利用高分辨率多光谱无人机影像和机器学习进行核桃水分胁迫测绘的方法，通过结合无人机影像和天气数据，使用随机森林模型有效估计了核桃树的茎水势，为核桃精准灌溉管理提供了重要的参考依据。

    

    有效监测核桃的水分状态和胁迫水平对于加利福尼亚州重要农作物核桃的精准灌溉管理至关重要。本研究提出了一个利用随机森林（RF）模型结合无人机航拍的高分辨率多光谱遥感影像和天气数据来测绘茎水势（SWP）的机器学习方法。从2017年到2018年，使用一架装备有七波段多光谱相机的无人机，在一个商业核桃园进行了五次飞行，同时伴随对抽样核桃植株的地面测量。RF回归模型利用来自正射无人机影像和天气数据的植被指数，有效地估计了地面测量的SWPs，达到了0.63的R^2值和0.80巴的平均绝对误差（MAE）。天气数据的整合尤为重要，以整合不同飞行日期的数据。显著的变量

    Effective monitoring of walnut water status and stress level across the whole orchard is an essential step towards precision irrigation management of walnuts, a significant crop in California. This study presents a machine learning approach using Random Forest (RF) models to map stem water potential (SWP) by integrating high-resolution multispectral remote sensing imagery from Unmanned Aerial Vehicle (UAV) flights with weather data. From 2017 to 2018, five flights of an UAV equipped with a seven-band multispectral camera were conducted over a commercial walnut orchard, paired with concurrent ground measurements of sampled walnut plants. The RF regression model, utilizing vegetation indices derived from orthomosaiced UAV imagery and weather data, effectively estimated ground-measured SWPs, achieving an $R^2$ of 0.63 and a mean absolute error (MAE) of 0.80 bars. The integration of weather data was particularly crucial for consolidating data across various flight dates. Significant variab
    
[^77]: 连接医疗设备的多变量时间序列的聚类模型的比较研究

    Comparative study of clustering models for multivariate time series from connected medical devices. (arXiv:2312.17286v2 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2312.17286](http://arxiv.org/abs/2312.17286)

    这项研究比较了两种针对多变量时间序列的聚类模型，通过预测未来值并形成聚类空间来创建患者资料，其中一种模型可以处理动态群组归属。

    

    在医疗保健领域，患者数据通常以多变量时间序列的形式收集，可以全面地反映患者随时间变化的健康状况。虽然这些数据可能是稀疏的，但连接的设备可以增加数据频率。目标是从这些时间序列中创建患者资料。在没有标签的情况下，可以使用预测模型来预测未来值，同时形成一个潜在的聚类空间，并以预测性能为评价指标。我们在Withing的数据集上比较了两个模型，M AGMAC LUST可以对整个时间序列进行聚类，而DGM${}^2$允许个体的群组归属随时间变化（动态聚类）。

    In healthcare, patient data is often collected as multivariate time series, providing a comprehensive view of a patient's health status over time. While this data can be sparse, connected devices may enhance its frequency. The goal is to create patient profiles from these time series. In the absence of labels, a predictive model can be used to predict future values while forming a latent cluster space, evaluated based on predictive performance. We compare two models on Withing's datasets, M AGMAC LUST which clusters entire time series and DGM${}^2$ which allows the group affiliation of an individual to change over time (dynamic clustering).
    
[^78]: Bellman最佳步长直线化流匹配模型

    Bellman Optimal Step-size Straightening of Flow-Matching Models. (arXiv:2312.16414v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.16414](http://arxiv.org/abs/2312.16414)

    本论文介绍了一种名为BOSS的技术，通过优化步长和生成路径，提升了低资源场景下流匹配生成模型的图像质量和资源利用效率。

    

    流匹配是一个强大的框架，用于在各种应用中生成高质量的样本，尤其是图像合成。然而，这些模型的强大计算需求，尤其在微调过程和采样过程中，给低资源场景带来了重大挑战。本文引入了Bellman最佳步长直线化（BOSS）技术来提炼流匹配生成模型：它针对的是在计算预算约束下进行少数步骤的高效图像采样。首先，该技术涉及一个动态规划算法，优化预训练网络的步长。然后，它通过优化速度网络以匹配最佳步长来改进生成路径。广泛的实验评估表明，BOSS在资源利用和图像质量方面都具有显著的优势。我们的结果显示，BOSS在图像生成任务中取得了实质性的收益。

    Flow matching is a powerful framework for generating high-quality samples in various applications, especially image synthesis. However, the intensive computational demands of these models, especially during the fine-tuning process and sampling processes, pose significant challenges for low-resource scenarios. This paper introduces Bellman Optimal Step-size Straightening (BOSS) technique for distilling flow-matching generative models: it aims specifically for a few-step efficient image sampling while adhering to a computational budget constraint. First, this technique involves a dynamic programming algorithm that optimizes the step sizes of the pretrained network. Then, it refines the velocity network to match the optimal step sizes, aiming to straighten the generation paths. Extensive experimental evaluations across image generation tasks demonstrate the efficacy of BOSS in terms of both resource utilization and image quality. Our results reveal that BOSS achieves substantial gains in 
    
[^79]: I-CEE: 将图像分类模型的解释定制为用户专业知识

    I-CEE: Tailoring Explanations of Image Classification Models to User Expertise. (arXiv:2312.12102v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.12102](http://arxiv.org/abs/2312.12102)

    I-CEE是一个人为中心的框架，为用户专业知识定制了图像分类模型的解释，通过提供信息丰富的示例图像、局部解释和模型决策来帮助用户理解模型的决策。

    

    有效解释黑盒机器学习模型的决策对于依赖它们的人工智能系统的负责任部署至关重要。识别到其重要性，可以生成这些解释的可解释人工智能（XAI）领域提供了几种技术。然而，在这一不断发展的工作中，对用户（解释对象）的关注相对较少，大多数XAI技术产生的是“一刀切”的解释。为了弥合这一差距，实现更加以人为中心的XAI，我们提出了I-CEE，这是一个为用户专业知识定制图像分类解释的框架。受到现有工作的启发，I-CEE通过为用户提供信息丰富的训练数据子集（即示例图像）、相应的局部解释和模型决策来解释图像分类模型的决策。然而，与此前的工作不同的是，I-CEE模拟了示例图像的信息量依赖于用户专业知识的情况，从而为不同的用户提供不同的示例。

    Effectively explaining decisions of black-box machine learning models is critical to responsible deployment of AI systems that rely on them. Recognizing their importance, the field of explainable AI (XAI) provides several techniques to generate these explanations. Yet, there is relatively little emphasis on the user (the explainee) in this growing body of work and most XAI techniques generate "one-size-fits-all" explanations. To bridge this gap and achieve a step closer towards human-centered XAI, we present I-CEE, a framework that provides Image Classification Explanations tailored to User Expertise. Informed by existing work, I-CEE explains the decisions of image classification models by providing the user with an informative subset of training data (i.e., example images), corresponding local explanations, and model decisions. However, unlike prior work, I-CEE models the informativeness of the example images to depend on user expertise, resulting in different examples for different u
    
[^80]: 非欧几里德空间图神经网络

    Non-Euclidean Spatial Graph Neural Network. (arXiv:2312.10808v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.10808](http://arxiv.org/abs/2312.10808)

    本文提出了一种新的通用框架，用于学习嵌入在非欧几里德流形空间中的空间网络的表示，通过提取边上的消息将图拓扑和空间几何结合起来。

    

    空间网络是其图拓扑受嵌入空间约束的网络。理解耦合的空间图属性对于从空间网络中提取强大表示非常重要。因此，仅仅结合个别的空间和网络表示无法揭示空间网络的潜在交互机制。此外，现有的空间网络表示学习方法只能考虑嵌入在欧几里德空间中的网络，无法很好地利用不规则和非均匀的非欧几里德空间中所携带的丰富几何信息。为了解决这个问题，本文提出了一个新的通用框架来学习嵌入在非欧几里德流形空间中的空间网络的表示。具体而言，我们提出了一种基于消息传递的神经网络来结合图拓扑和空间几何，其中空间几何被提取为边上的消息。我们在理论上保证了...

    Spatial networks are networks whose graph topology is constrained by their embedded spatial space. Understanding the coupled spatial-graph properties is crucial for extracting powerful representations from spatial networks. Therefore, merely combining individual spatial and network representations cannot reveal the underlying interaction mechanism of spatial networks. Besides, existing spatial network representation learning methods can only consider networks embedded in Euclidean space, and can not well exploit the rich geometric information carried by irregular and non-uniform non-Euclidean space. In order to address this issue, in this paper we propose a novel generic framework to learn the representation of spatial networks that are embedded in non-Euclidean manifold space. Specifically, a novel message-passing-based neural network is proposed to combine graph topology and spatial geometry, where spatial geometry is extracted as messages on the edges. We theoretically guarantee tha
    
[^81]: 具有实例相关噪声标签的联邦学习

    Federated Learning with Instance-Dependent Noisy Label. (arXiv:2312.10324v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.10324](http://arxiv.org/abs/2312.10324)

    本研究针对具有实例相关噪声的联邦学习问题，提出了一种名为FedBeat的新算法。该算法通过使用实例相关噪声转移矩阵构建全局分类器，包括联邦数据提取和转移矩阵估计两个步骤。

    

    具有噪声标签的联邦学习是一个重要挑战。现有的处理中心化学习中噪声标签的方法在联邦学习环境中往往失效，主要原因是数据集规模小和客户端数据的异质性。虽然已经尝试了一些解决具有噪声标签的联邦学习的方法，但主要集中在涉及类条件噪声的情况。本文主要研究联邦学习中更具挑战性和实际问题，即实例相关噪声。我们提出了一种新颖的算法FedBeat（联邦学习与贝叶斯集成辅助转移矩阵估计），旨在使用实例相关噪声转移矩阵（IDNTM）构建全局统一的分类器。该算法包括三个协同步骤：（1）联邦数据提取步骤，通过贝叶斯模型集成方法构建弱全局模型并提取高置信度数据；（2）联邦转移矩阵估计步骤； in

    Federated learning (FL) with noisy labels poses a significant challenge. Existing methods designed for handling noisy labels in centralized learning tend to lose their effectiveness in the FL setting, mainly due to the small dataset size and the heterogeneity of client data. While some attempts have been made to tackle FL with noisy labels, they primarily focused on scenarios involving class-conditional noise. In this paper, we study the more challenging and practical issue of instance-dependent noise (IDN) in FL. We introduce a novel algorithm called FedBeat (Federated Learning with Bayesian Ensemble-Assisted Transition Matrix Estimation). FedBeat aims to build a global statistically consistent classifier using the IDN transition matrix (IDNTM), which encompasses three synergistic steps: (1) A federated data extraction step that constructs a weak global model and extracts high-confidence data using a Bayesian model ensemble method. (2) A federated transition matrix estimation step in 
    
[^82]: 在预测空间中带有贝叶斯推断的校准一轮联邦学习

    Calibrated One Round Federated Learning with Bayesian Inference in the Predictive Space. (arXiv:2312.09817v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.09817](http://arxiv.org/abs/2312.09817)

    本研究提出了一个名为$\beta$-Predictive Bayes的贝叶斯联邦学习算法，在预测后验的混合和乘积之间进行插值，通过调整参数$\beta$来解决现有方法中过于自信的预测问题。

    

    联邦学习是指在分布在客户端中的数据集上训练模型，每个客户端的数据集是本地化且可能是异质的。在联邦学习中，小而噪声的数据集很常见，强调了需要能够表示预测的不确定性的良好校准模型。最接近实现这一目标的联邦学习技术是贝叶斯联邦学习方法，它从局部后验中收集参数样本，并将它们聚合以近似全局后验。为了提高更大模型的可扩展性，贝叶斯方法通常是通过将局部预测后验相乘来近似全局预测后验。本研究表明，这种方法会导致系统性的过于自信的预测结果。为了解决这个问题，我们提出了一种称为$\beta$-Predictive Bayes的贝叶斯联邦学习算法，它在预测后验的混合和乘积之间进行插值，使用一个可调参数$\beta$来实现。

    Federated Learning (FL) involves training a model over a dataset distributed among clients, with the constraint that each client's dataset is localized and possibly heterogeneous. In FL, small and noisy datasets are common, highlighting the need for well-calibrated models that represent the uncertainty of predictions. The closest FL techniques to achieving such goals are the Bayesian FL methods which collect parameter samples from local posteriors, and aggregate them to approximate the global posterior. To improve scalability for larger models, one common Bayesian approach is to approximate the global predictive posterior by multiplying local predictive posteriors. In this work, we demonstrate that this method gives systematically overconfident predictions, and we remedy this by proposing $\beta$-Predictive Bayes, a Bayesian FL algorithm that interpolates between a mixture and product of the predictive posteriors, using a tunable parameter $\beta$. This parameter is tuned to improve th
    
[^83]: KwaiAgents：基于大型语言模型的通用信息搜索智能体系统

    KwaiAgents: Generalized Information-seeking Agent System with Large Language Models. (arXiv:2312.04889v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.04889](http://arxiv.org/abs/2312.04889)

    本文介绍了 KwaiAgents，这是一个基于大型语言模型的通用信息搜索智能体系统。该系统能够利用语言模型作为认知核心，理解用户的查询，行为准则并参考外部文档，以提供高质量的知识和信息。

    

    人类由于好奇心的驱使，不断探索和理解周围的世界，从而发明了各种工具来满足这种好奇心。尽管人类无法在大脑中处理和记忆大量信息，但在批判思维、规划、反思以及利用现有工具与世界进行交互和解释方面卓越出色，使其能够高效地寻找答案。最近大型语言模型（LLM）的进步表明，机器可能也具备类似于人类的能力，即使参数数量受限，也能展示强大的能力。在本文中，我们介绍了 KwaiAgents，这是一个基于LLM的通用信息搜索智能体系统。在 KwaiAgents 中，我们提出了一种利用LLM作为认知核心的智能体系统，它能够理解用户的查询、行为准则和参考外部文档。智能体还可以更新查询结果，与用户进行互动，并提供高质量的知识和信息。

    Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents. The agent can also update an
    
[^84]: 基于样本的动态分层Transformer通过上下文Bandit实现层和头的灵活性

    Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility via Contextual Bandit. (arXiv:2312.03038v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.03038](http://arxiv.org/abs/2312.03038)

    我们提出了一种基于样本的动态分层Transformer模型(DHT)，通过解决上下文Bandit问题动态配置层和头的数量。与之前的工作不同，DHT不仅在训练中能够自适应优化网络架构，而且具有灵活的网络架构，用于高效的推断。

    

    Transformer模型需要固定数量的层和头，这使得它们对单个样本的复杂性不灵活，并且在训练和推断中都很昂贵。为了解决这个问题，我们提出了一种基于样本的动态分层Transformer（DHT）模型，它的层和头可以通过解决上下文Bandit问题动态配置。为了确定层数和头数，我们使用了均匀置信上界，而在给定头数量的情况下，我们采用组合Thompson抽样来选择特定的头组合。与之前只关注压缩训练网络以用于推断的工作不同，DHT不仅在训练期间能够自适应优化底层网络架构，而且还具有灵活的网络架构，以实现高效的推断。据我们所知，这是第一个全面的、没有任何额外辅助神经网络实现动态系统的数据驱动动态transformer模型。

    Transformer requires a fixed number of layers and heads which makes them inflexible to the complexity of individual samples and expensive in training and inference. To address this, we propose a sample-based Dynamic Hierarchical Transformer (DHT) model whose layers and heads can be dynamically configured with single data samples via solving contextual bandit problems. To determine the number of layers and heads, we use the Uniform Confidence Bound while we deploy combinatorial Thompson Sampling in order to select specific head combinations given their number. Different from previous work that focuses on compressing trained networks for inference only, DHT is not only advantageous for adaptively optimizing the underlying network architecture during training but also has a flexible network for efficient inference. To the best of our knowledge, this is the first comprehensive data-driven dynamic transformer without any additional auxiliary neural networks that implement the dynamic system
    
[^85]: FedEmb:一种使用网络和特征嵌入聚合的垂直和混合联邦学习算法

    FedEmb: A Vertical and Hybrid Federated Learning Algorithm using Network And Feature Embedding Aggregation. (arXiv:2312.00102v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.00102](http://arxiv.org/abs/2312.00102)

    本论文提出了一种名为FedEmb的通用算法，用于进行垂直和混合的基于DNN的联邦学习，具有更高的推理准确率、隐私保护性能更强以及较低的通信带宽需求。实验证明，FedEmb是一种有效的方法来解决分布式问题，并在有限的隐私泄露下提高推理准确度。

    

    联邦学习(FL)是一种新兴的去中心化训练机器学习模型的范例，它在分布式客户端上进行，而不将数据传输给中央服务器。学习方案可以是水平的、垂直的或混合的(垂直和水平都有)。大部分现有的基于深度神经网络(DNN)建模的研究工作都集中在水平数据分布上，而垂直和混合方案研究较少。本文提出了一种通用算法FedEmb,用于建模垂直和混合的基于DNN的学习。我们算法的思想具有更高的推理准确率、更强的隐私保护性能和更低的客户端-服务器通信带宽需求，与现有工作相比。实验结果表明，FedEmb是一种有效的方法来解决分割特征和主题空间分散问题，在有限的隐私暴露下，显示了0.3%到4.2%的推理准确度提高，适用于存储在本地客户端的数据集。

    Federated learning (FL) is an emerging paradigm for decentralized training of machine learning models on distributed clients, without revealing the data to the central server. The learning scheme may be horizontal, vertical or hybrid (both vertical and horizontal). Most existing research work with deep neural network (DNN) modelling is focused on horizontal data distributions, while vertical and hybrid schemes are much less studied. In this paper, we propose a generalized algorithm FedEmb, for modelling vertical and hybrid DNN-based learning. The idea of our algorithm is characterised by higher inference accuracy, stronger privacy-preserving properties, and lower client-server communication bandwidth demands as compared with existing work. The experimental results show that FedEmb is an effective method to tackle both split feature & subject space decentralized problems, shows 0.3% to 4.2% inference accuracy improvement with limited privacy revealing for datasets stored in local client
    
[^86]: 从成对人类偏好学习的密度估计视角

    A density estimation perspective on learning from pairwise human preferences. (arXiv:2311.14115v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.14115](http://arxiv.org/abs/2311.14115)

    研究提出了一个从密度估计的角度解释学习成对人类偏好的方法，并证明通过这种方法训练奖励函数可以有效地模拟注释者的隐含偏好分布。

    

    从人类反馈中学习（LHF）--尤其是从成对偏好学习--最近在训练大型语言模型（LLM）中变得至关重要，并成为许多研究的主题。最近的工作大多将其框架为一种强化学习问题，通过成对偏好数据学习奖励函数，并将LLM视为一个策略，并在额外的正则化约束下进行调整以最大化奖励。我们提出了一种替代解释，它以成对偏好的生成过程为中心，并将LHF视为一个密度估计问题。我们提供了理论和实证结果，表明对于通过偏好行为分布方程定义的一类生成过程，通过成对偏好训练奖励函数有效地模拟了注释者的隐含偏好分布。最后，我们讨论并提出了关于“标注者错误”的研究结果--即错误的情况。

    Learning from human feedback (LHF) -- and in particular learning from pairwise preferences -- has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on "annotator misspecification" -failure cases where wro
    
[^87]: 深度神经决策森林：一种用于预测COVID-19患者康复或死亡的新方法，结合临床和RT-PCR数据

    Deep Neural Decision Forest: A Novel Approach for Predicting Recovery or Decease of COVID-19 Patients with Clinical and RT-PCR. (arXiv:2311.13925v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2311.13925](http://arxiv.org/abs/2311.13925)

    该研究介绍了一种利用临床和RT-PCR数据结合深度学习算法来预测COVID-19患者康复或死亡风险的新方法。

    

    尽管世界卫生组织宣布大流行已经结束，但COVID-19仍然被视为一种地方性疾病。这次大流行以前所未有的方式打乱了人们的生活并导致广泛的发病率和死亡率。因此，紧急医生有必要确定高风险死亡患者，以便优先考虑医院设备的分配，尤其是在医疗资源有限的地区。尽管存在哪种数据最准确的预测的问题，但患者收集到的数据对于预测COVID-19病例的结果是有益的。因此，本研究旨在实现两个主要目标。首先，我们想要检查深度学习算法是否能够预测患者的死亡率。其次，我们研究了临床和RT-PCR对预测的影响，以确定哪个更可靠。我们定义了四个不同特征集的阶段，并使用可解释的深度学习方法构建了相应的模型。

    COVID-19 continues to be considered an endemic disease in spite of the World Health Organization's declaration that the pandemic is over. This pandemic has disrupted people's lives in unprecedented ways and caused widespread morbidity and mortality. As a result, it is important for emergency physicians to identify patients with a higher mortality risk in order to prioritize hospital equipment, especially in areas with limited medical services. The collected data from patients is beneficial to predict the outcome of COVID-19 cases, although there is a question about which data makes the most accurate predictions. Therefore, this study aims to accomplish two main objectives. First, we want to examine whether deep learning algorithms can predict a patient's morality. Second, we investigated the impact of Clinical and RT-PCR on prediction to determine which one is more reliable. We defined four stages with different feature sets and used interpretable deep learning methods to build appropr
    
[^88]: 学会说母语：以母语风格激发大型语言模型的能力

    Speak Like a Native: Prompting Large Language Models in a Native Style. (arXiv:2311.13538v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.13538](http://arxiv.org/abs/2311.13538)

    本文提出了一种名为AlignedCoT的新颖有效方法，通过将上下文示例与大型语言模型（LLMs）的母语风格对齐，提高了LLMs的推理能力和性能。

    

    大型语言模型（LLMs）与上下文学习（ICL）已成为许多自然语言处理任务的现代工具选择。然而，上下文示例的文本风格如何影响LLMs的性能仍然不足。本文提出了一种名为AlignedCoT的新颖有效的方法，通过将上下文示例与LLMs的母语风格对齐来提高LLMs的推理能力。 "母语"是指LLMs的固有特征，可以通过零-shot场景探测。 AlignedCoT广泛适用于ICL方法，可以轻松与最先进的技术结合，进一步提高LLMs的性能。我们在数学问答、常识推理和文本理解等多个基准测试上进行了广泛而全面的实验。实证结果表明，我们的AlignedCoT相比精心手工制作的演示文稿显著提高了性能。

    In-context learning (ICL) with large language models (LLMs) has become the modern tools of choice for many natural language processing tasks. However, how the text style of in-context examples influences the performance of LLMs still remains under-explored. This paper presents a novel and effective approach, named \textbf{AlignedCoT}, to improve the reasoning capability of LLMs by aligning the in-context examples with the native style of LLMs.''Native'' refers to the inherent characteristic of LLMs which can be probed by zero-shot scenarios.AlignedCoT is widely applicable to ICL methods, making it easy to combine with state-of-the-art techniques to further improve the LLMs' performance. We conduct extensive and comprehensive experiments on several benchmarks on mathematical question-answering, common-sense reasoning, and text understanding. The empirical results demonstrate that our AlignedCoT significantly improves performance over the carefully handcrafted demonstrations. Specificall
    
[^89]: 在凸优化中的算法可重现性和梯度复杂度的最优保证

    Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization. (arXiv:2310.17759v1 [cs.LG])

    [http://arxiv.org/abs/2310.17759](http://arxiv.org/abs/2310.17759)

    该论文研究了在凸优化中算法的可重现性和梯度复杂度问题。他们挑战了之前的观点，证明了对于平滑凸优化和平滑凸凹极小极大问题，可以实现最优的可重现性和接近最优的收敛保证。他们还证明了在不同的oracle设置下，与不同类型的oracle相匹配的算法达到了最优性。

    

    算法可重现性衡量了机器学习算法在训练过程中稍微改变时输出的偏差。之前的研究表明，一阶方法需要在收敛速度（梯度复杂度）和更好的可重现性之间做出权衡。在这项工作中，我们挑战了这种看法，并展示了在各种容易出错的oracle设置下，对于平滑凸优化和平滑凸凹极小极大问题，可以实现最优的可重现性和接近最优的收敛保证。尤其是，在不精确的初始化oracle给定情况下，我们基于正则化的算法实现了最优的可重现性和接近最优的梯度复杂度-对于最小化和最小最大优化。对于不精确的梯度oracle，接近最优的保证也适用于最小最大优化。此外，对于随机梯度oracle，我们证明了随机梯度下降上升在可重现性和收敛速度方面都是最优的。

    Algorithmic reproducibility measures the deviation in outputs of machine learning algorithms upon minor changes in the training process. Previous work suggests that first-order methods would need to trade-off convergence rate (gradient complexity) for better reproducibility. In this work, we challenge this perception and demonstrate that both optimal reproducibility and near-optimal convergence guarantees can be achieved for smooth convex minimization and smooth convex-concave minimax problems under various error-prone oracle settings. Particularly, given the inexact initialization oracle, our regularization-based algorithms achieve the best of both worlds optimal reproducibility and near-optimal gradient complexity - for minimization and minimax optimization. With the inexact gradient oracle, the near-optimal guarantees also hold for minimax optimization. Additionally, with the stochastic gradient oracle, we show that stochastic gradient descent ascent is optimal in terms of both re
    
[^90]: 构建和机器学习Calabi-Yau五折面

    Constructing and Machine Learning Calabi-Yau Five-folds. (arXiv:2310.15966v2 [hep-th] UPDATED)

    [http://arxiv.org/abs/2310.15966](http://arxiv.org/abs/2310.15966)

    本论文构建并机器学习了Calabi-Yau五折面，在多个复杂射影空间的乘积中，找到了27068个不同的空间，并计算了它们的欧拉数和同调数据。通过使用神经网络进行监督学习，可以高效地学习到$h^{1,1}$。

    

    我们在四个或更少复杂射影空间的乘积中构造了所有可能的完全交叉Calabi-Yau五折面，并带有最多四个约束。我们得到了27068个空间，它们相互之间没有行和列排序的关联，并确定了它们的欧拉数。在所有空间中排除3909个产品流形后，我们计算了12433个案例的同调数据，即非产品空间的53.7％，得到了2375个不同的霍奇钻石。包含上述所有信息的数据集可在https://www.dropbox.com/scl/fo/z7ii5idt6qxu36e0b8azq/h?rlkey=0qfhx3tykytduobpld510gsfy&dl=0获得。讨论了不变量的分布，并与低维类比进行了比较。通过分类器和回归器（全连接和卷积）神经网络对同调数据进行了监督式机器学习。我们发现$h^{1,1}$可以非常高效地学习。

    We construct all possible complete intersection Calabi-Yau five-folds in a product of four or less complex projective spaces, with up to four constraints. We obtain $27068$ spaces, which are not related by permutations of rows and columns of the configuration matrix, and determine the Euler number for all of them. Excluding the $3909$ product manifolds among those, we calculate the cohomological data for $12433$ cases, i.e. $53.7 \%$ of the non-product spaces, obtaining $2375$ different Hodge diamonds. The dataset containing all the above information is available at https://www.dropbox.com/scl/fo/z7ii5idt6qxu36e0b8azq/h?rlkey=0qfhx3tykytduobpld510gsfy&dl=0 . The distributions of the invariants are presented, and a comparison with the lower-dimensional analogues is discussed. Supervised machine learning is performed on the cohomological data, via classifier and regressor (both fully connected and convolutional) neural networks. We find that $h^{1,1}$ can be learnt very efficiently, with
    
[^91]: KirchhoffNet：一种连接消息传递和连续深度模型的电路桥接神经网络

    KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models. (arXiv:2310.15872v1 [cs.LG])

    [http://arxiv.org/abs/2310.15872](http://arxiv.org/abs/2310.15872)

    本文提出了一种称为基赫霍夫网络的神经网络模型，利用基赫霍夫电流定律与消息传递神经网络和连续深度网络建立连接。在MNIST数据集上，基赫霍夫网络可以实现接近98.86%的测试准确度，且具有在硬件上实现的潜力。无论网络参数数量如何，其正向计算都可以在1/f秒内完成，具有快速计算的硬件特性。

    

    在本文中，我们利用了模拟电路的基本原理基赫霍夫电流定律，引入了一类独特的神经网络模型，称为基赫霍夫网络。基赫霍夫网络与消息传递神经网络和连续深度网络建立了密切联系。我们证明，即使在没有任何传统层（如卷积、池化或线性层）的情况下，基赫霍夫网络在MNIST数据集上取得了98.86%的测试准确度，与最先进的结果相当。让基赫霍夫网络更加有趣的是其在硬件领域的潜力。当代深度神经网络通常部署在GPU上。相反，基赫霍夫网络可以通过模拟电路来实现。此外，我们证明了无论在基赫霍夫网络内有多少参数，其正向计算都可以在1/f秒内完成，其中f表示硬件的时钟频率。这种特性表明，基赫霍夫网络具有潜力实现快速计算的硬件。

    In this paper, we exploit a fundamental principle of analog electronic circuitry, Kirchhoff's current law, to introduce a unique class of neural network models that we refer to as KirchhoffNet. KirchhoffNet establishes close connections with message passing neural networks and continuous-depth networks. We demonstrate that even in the absence of any traditional layers (such as convolution, pooling, or linear layers), KirchhoffNet attains 98.86% test accuracy on the MNIST dataset, comparable with state of the art (SOTA) results. What makes KirchhoffNet more intriguing is its potential in the realm of hardware. Contemporary deep neural networks are conventionally deployed on GPUs. In contrast, KirchhoffNet can be physically realized by an analog electronic circuit. Moreover, we justify that irrespective of the number of parameters within a KirchhoffNet, its forward calculation can always be completed within 1/f seconds, with f representing the hardware's clock frequency. This characteris
    
[^92]: 通过四元数小波网络推广医学图像表示

    Generalizing Medical Image Representations via Quaternion Wavelet Networks. (arXiv:2310.10224v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2310.10224](http://arxiv.org/abs/2310.10224)

    本文提出了一种名为QUAVE的四元数小波网络，可以从医学图像中提取显著特征。该网络可以与现有的医学图像分析或综合任务结合使用，并推广了对单通道数据的采用。通过四元数小波变换和加权处理，QUAVE能够处理具有较大变化的医学数据。

    

    鉴于来自不同来源和各种任务的数据集日益增加，神经网络的普适性成为一个广泛研究的领域。当处理医学数据时，这个问题尤为广泛，因为缺乏方法论标准导致不同的成像中心或使用不同设备和辅助因素获取的数据存在较大变化。为了克服这些限制，我们引入了一种新颖的、普适的、数据-和任务不可知的框架，能够从医学图像中提取显著特征。所提出的四元数小波网络（QUAVE）可以很容易地与任何现有的医学图像分析或综合任务相结合，并且可以结合实际、四元数或超复值模型，推广它们对单通道数据的采用。QUAVE首先通过四元数小波变换提取不同的子带，得到低频/近似频带和高频/细粒度特征。然后，它对最有代表性的特征进行加权处理，从而减少了特征重要性不均匀性。

    Neural network generalizability is becoming a broad research field due to the increasing availability of datasets from different sources and for various tasks. This issue is even wider when processing medical data, where a lack of methodological standards causes large variations being provided by different imaging centers or acquired with various devices and cofactors. To overcome these limitations, we introduce a novel, generalizable, data- and task-agnostic framework able to extract salient features from medical images. The proposed quaternion wavelet network (QUAVE) can be easily integrated with any pre-existing medical image analysis or synthesis task, and it can be involved with real, quaternion, or hypercomplex-valued models, generalizing their adoption to single-channel data. QUAVE first extracts different sub-bands through the quaternion wavelet transform, resulting in both low-frequency/approximation bands and high-frequency/fine-grained features. Then, it weighs the most repr
    
[^93]: 通过基于Transformer的强化学习进行分子的全新设计

    Molecular De Novo Design through Transformer-based Reinforcement Learning. (arXiv:2310.05365v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05365](http://arxiv.org/abs/2310.05365)

    本文提出了一种基于Transformer的强化学习方法，通过精细调整生成模型，能够在分子的全新设计中生成具有所需性质的分子结构，展现出优越的性能。

    

    本文介绍了一种通过精细调整基于Transformer的生成模型用于分子的全新设计的方法。利用Transformer相对于循环神经网络（RNN）的优越序列学习能力，我们的模型可以有效地生成具有所需性质的分子结构。与传统的基于RNN的模型相比，我们提出的方法在生成预测对多种生物靶点具有活性的化合物方面表现出卓越性能，捕捉了分子结构序列的长期依赖性。该模型的有效性在许多任务中得到了证明，包括生成与查询结构类似的分子和生成具有特定属性的化合物，在性能上优于基线的基于RNN的方法。我们的方法可以用于桥接化学、从单个分子开始扩展库，并生成具有高预测活性的化合物。

    In this work, we introduce a method to fine-tune a Transformer-based generative model for molecular de novo design. Leveraging the superior sequence learning capacity of Transformers over Recurrent Neural Networks (RNNs), our model can generate molecular structures with desired properties effectively. In contrast to the traditional RNN-based models, our proposed method exhibits superior performance in generating compounds predicted to be active against various biological targets, capturing long-term dependencies in the molecular structure sequence. The model's efficacy is demonstrated across numerous tasks, including generating analogues to a query structure and producing compounds with particular attributes, outperforming the baseline RNN-based methods. Our approach can be used for scaffold hopping, library expansion starting from a single molecule, and generating compounds with high predicted activity against biological targets.
    
[^94]: 使用流匹配进行统一的语音和手势合成

    Unified speech and gesture synthesis using flow matching. (arXiv:2310.05181v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2310.05181](http://arxiv.org/abs/2310.05181)

    本文提出了一种使用流匹配进行统一的语音和手势合成的架构，相比于先前的技术，它更简单、占用内存更小，并能够同时生成语音和手势模态。新的训练机制在少量步骤中实现了更好的合成质量，并通过主观测试证明了在语音自然度、手势人类化和跨模态适当性方面的改进。

    

    随着文本转语音技术在朗读任务中达到了显著的自然性，人们越来越关注语言和非语言交流行为的多模态合成，例如自发语言和相关的身体手势。本文提出了一种新颖的统一架构，用于从文本中联合合成语音声学和基于骨骼的三维手势运动，训练使用最优传输条件流匹配（OT-CFM）。所提出的架构比先前的最新技术更简单，占用的内存更小，并且能够捕捉语音和手势的联合分布，在一个单一过程中生成两种模态。与以前相比，新的训练机制在更少的步骤（网络评估）中实现了更好的合成质量。单一和多模态的主观测试表明，与现有基准相比，语音自然度、手势人类化和跨模态的适当性都得到了改善。

    As text-to-speech technologies achieve remarkable naturalness in read-aloud tasks, there is growing interest in multimodal synthesis of verbal and non-verbal communicative behaviour, such as spontaneous speech and associated body gestures. This paper presents a novel, unified architecture for jointly synthesising speech acoustics and skeleton-based 3D gesture motion from text, trained using optimal-transport conditional flow matching (OT-CFM). The proposed architecture is simpler than the previous state of the art, has a smaller memory footprint, and can capture the joint distribution of speech and gestures, generating both modalities together in one single process. The new training regime, meanwhile, enables better synthesis quality in much fewer steps (network evaluations) than before. Uni- and multimodal subjective tests demonstrate improved speech naturalness, gesture human-likeness, and cross-modal appropriateness compared to existing benchmarks. Please see https://shivammehta25.g
    
[^95]: 使用大型语言模型改进自动VQA评估

    Improving Automatic VQA Evaluation Using Large Language Models. (arXiv:2310.02567v1 [cs.CV])

    [http://arxiv.org/abs/2310.02567](http://arxiv.org/abs/2310.02567)

    提出使用大型语言模型改进自动视觉问答（VQA）评估的方法，将VQA评估格式化为回答评分任务，通过指令调整大型语言模型在准确度上评分候选答案，证明该方法与人类判断相关性优于现有度量方法。

    

    在提出视觉问答（VQA）任务8年后，准确率仍然是自动评估的主要指标。在IID评估设置中，VQA准确度一直很有效。然而，我们的社区正在转向开放式生成模型和OOD评估。在这种新的范式中，现有的VQA准确度指标过于严格，低估了VQA系统的性能。因此，有必要开发更强大的自动VQA度量，作为人类判断的代理。在这项工作中，我们提出利用指令调整大型语言模型（LLM）的上下文学习能力来构建更好的VQA度量。我们将VQA评估格式化为一个回答评分任务，即指令调整的大型语言模型被指示根据一组参考答案评分候选答案的准确性。我们证明所提出的度量与人类判断相关性优于现有度量在几个VQA模型和基准测试中。

    8 years after the visual question answering (VQA) task was proposed, accuracy remains the primary metric for automatic evaluation. VQA Accuracy has been effective so far in the IID evaluation setting. However, our community is undergoing a shift towards open-ended generative models and OOD evaluation. In this new paradigm, the existing VQA Accuracy metric is overly stringent and underestimates the performance of VQA systems. Thus, there is a need to develop more robust automatic VQA metrics that serve as a proxy for human judgment. In this work, we propose to leverage the in-context learning capabilities of instruction-tuned large language models (LLMs) to build a better VQA metric. We formulate VQA evaluation as an answer-rating task where the LLM is instructed to score the accuracy of a candidate answer given a set of reference answers. We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks. We ho
    
[^96]: 在学习者提供的问题上增强学生表现预测的SGNN-LLM协同

    Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy. (arXiv:2309.13500v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13500](http://arxiv.org/abs/2309.13500)

    这项研究介绍了一种创新的策略，将有符号图神经网络（SGNNs）和大型语言模型（LLM）的潜力协同起来，用于预测学生在学习者提供的问题上的表现。该方法利用有符号二分图全面建模学生回答，并采用对比学习框架增强了噪声的鲁棒性。

    

    通过学生内容创作，学习者合作具有可扩展教育的巨大潜力。然而，预测学生在学习者提供的问题上的表现，对于个性化学习体验至关重要，由于学生生成的数据中固有的噪声，这是具有挑战性的。此外，传统的基于图的方法可以捕获学生和问题交互的复杂网络，但在冷启动条件下，其中学生对问题的有限参与导致数据稀疏，这些方法往往表现不佳。为了解决这两个挑战，我们引入了一种创新策略，将整合有符号图神经网络（SGNNs）和大型语言模型（LLM）的潜力协同起来。我们的方法利用有符号二分图全面建模学生回答，并采用对比学习框架增强了噪声的鲁棒性。此外，LLM的贡献在于生成基础问题嵌入，特别是证明了其优势。

    Learnersourcing offers great potential for scalable education through student content creation. However, predicting student performance on learnersourced questions, which is essential for personalizing the learning experience, is challenging due to the inherent noise in student-generated data. Moreover, while conventional graph-based methods can capture the complex network of student and question interactions, they often fall short under cold start conditions where limited student engagement with questions yields sparse data. To address both challenges, we introduce an innovative strategy that synergizes the potential of integrating Signed Graph Neural Networks (SGNNs) and Large Language Model (LLM) embeddings. Our methodology employs a signed bipartite graph to comprehensively model student answers, complemented by a contrastive learning framework that enhances noise resilience. Furthermore, LLM's contribution lies in generating foundational question embeddings, proving especially adv
    
[^97]: 避免在根查找和优化中出现不需要的点的方法：通过增加极点

    Multiplying poles to avoid unwanted points in root finding and optimization. (arXiv:2309.11475v1 [math.OC])

    [http://arxiv.org/abs/2309.11475](http://arxiv.org/abs/2309.11475)

    通过增加极点来避免根查找和优化中不需要的点，方法是将代价函数除以到目标点的距离函数的适当幂。

    

    在根查找和优化中，存在许多情况下，我们可能无法保证自己选择的方法构造的序列收敛于一个闭集合A（在这里，我们并不假设A有其他附加属性，如凸性或连通性）。在这种情况下，我们希望有一个机制来避免在算法的下一次运行中再次遇到这个点z*。在本文中，我们提出了一种新的方法来实现这一目标：我们将代价函数除以到A的距离函数的适当幂。这个想法受到了在一维函数中尝试找到所有根的启发。我们首先解释了在代价函数的最小值恰好为0的情况下这种方法的启发式方法，然后解释了如果最小值不为零该如何进行（同时允许正的最小值）。

    In root finding and optimization, there are many cases where there is a closed set $A$ one does not the sequence constructed by one's favourite method will converge to A (here, we do not assume extra properties on $A$ such as being convex or connected). For example, if one wants to find roots, and one chooses initial points in the basin of attraction for 1 root $x^*$ (a fact which one may not know before hand), then one will always end up in that root. In this case, one would like to have a mechanism to avoid this point $z^*$ in the next runs of one's algorithm.  In this paper, we propose a new method aiming to achieve this: we divide the cost function by an appropriate power of the distance function to $A$. This idea is inspired by how one would try to find all roots of a function in 1 variable. We first explain the heuristic for this method in the case where the minimum of the cost function is exactly 0, and then explain how to proceed if the minimum is non-zero (allowing both positi
    
[^98]: AI代理的记忆和泛化能力分析：连续学习者是否具有鲁棒性？

    Analysis of the Memorization and Generalization Capabilities of AI Agents: Are Continual Learners Robust?. (arXiv:2309.10149v1 [cs.LG])

    [http://arxiv.org/abs/2309.10149](http://arxiv.org/abs/2309.10149)

    本文提出了一种新的连续学习框架，旨在在动态环境下实现鲁棒的泛化能力并保留过去的知识。通过使用有限容量的内存来保存先前观察到的环境信息，并从内存中采样数据点来获得对未知变化鲁棒的预测器。该分析展示了记忆和泛化之间的权衡，而实验证明了所提出的算法的优越性。

    

    在连续学习中，AI代理（例如自动驾驶车辆或机器人）在动态环境下从非稳态数据流中学习。对于这类应用的实际部署，保证对未知环境的鲁棒性以及保留过去的经验是非常重要的。本文提出了一种新的连续学习框架，旨在在动态环境下实现鲁棒的泛化能力并保留过去的知识。考虑到连续学习代理使用有限容量的内存来保存先前观察到的环境信息，以减轻遗忘问题。然后，从内存中采样数据点，以估计环境变化的风险分布，从而得到对未知变化具有鲁棒性的预测器。对所提出的框架的泛化和记忆性能进行了理论分析。该分析展示了随内存大小的记忆和泛化之间的权衡。实验证明了所提出的算法的优越性。

    In continual learning (CL), an AI agent (e.g., autonomous vehicles or robotics) learns from non-stationary data streams under dynamic environments. For the practical deployment of such applications, it is important to guarantee robustness to unseen environments while maintaining past experiences. In this paper, a novel CL framework is proposed to achieve robust generalization to dynamic environments while retaining past knowledge. The considered CL agent uses a capacity-limited memory to save previously observed environmental information to mitigate forgetting issues. Then, data points are sampled from the memory to estimate the distribution of risks over environmental change so as to obtain predictors that are robust with unseen changes. The generalization and memorization performance of the proposed framework are theoretically analyzed. This analysis showcases the tradeoff between memorization and generalization with the memory size. Experiments show that the proposed algorithm outpe
    
[^99]: RaTrack: 带有4D雷达点云的运动物体检测与跟踪

    RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud. (arXiv:2309.09737v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.09737](http://arxiv.org/abs/2309.09737)

    RaTrack是一种针对雷达跟踪的创新解决方案，通过运动分割和聚类以及运动估计模块，实现了对移动物体的精确跟踪，优于最先进性能。

    

    移动自主性依赖于对动态环境的精确感知。在3D世界中稳定地跟踪移动物体因此对于轨迹预测、避障和路径规划等应用起着关键作用。虽然大多数现有方法利用LiDAR或相机进行多目标跟踪（MOT），但4D成像雷达的能力仍然很少被探索。认识到4D雷达数据中的雷达噪声和点稀疏性所带来的挑战，我们介绍了RaTrack，这是一种专门针对基于雷达的跟踪的创新解决方案。我们的方法摒弃了对特定对象类型和3D边界框的依赖，而是专注于运动分割和聚类，并配以运动估计模块。在View-of-Delft数据集上进行评估时，RaTrack展示出了优于最先进性能的运动物体跟踪精度。

    Mobile autonomy relies on the precise perception of dynamic environments. Robustly tracking moving objects in 3D world thus plays a pivotal role for applications like trajectory prediction, obstacle avoidance, and path planning. While most current methods utilize LiDARs or cameras for Multiple Object Tracking (MOT), the capabilities of 4D imaging radars remain largely unexplored. Recognizing the challenges posed by radar noise and point sparsity in 4D radar data, we introduce RaTrack, an innovative solution tailored for radar-based tracking. Bypassing the typical reliance on specific object types and 3D bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. Evaluated on the View-of-Delft dataset, RaTrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art.
    
[^100]: 全连接的时空图用于多变量时序数据

    Fully-Connected Spatial-Temporal Graph for Multivariate Time-Series Data. (arXiv:2309.05305v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.05305](http://arxiv.org/abs/2309.05305)

    本论文提出了一种全连接的时空图神经网络（FC-STGNN）方法，用于有效地建模多变量时序数据中的时空依赖性。该方法能够捕捉不同时间戳上不同传感器之间的相关性，提供了一种全面建模时空依赖性的新途径。

    

    多变量时序（MTS）数据在各个应用领域都很重要。由于其序列性和多源性（多个传感器），MTS数据固有地展现了时空依赖性，包括时间戳之间的时间相关性和每个时间戳中传感器之间的空间相关性。为了有效利用这些信息，基于图神经网络的方法（GNNs）被广泛采用。然而，现有的方法分别捕获空间依赖性和时间依赖性，无法捕捉不同时间戳上不同传感器之间的相关性。忽视这样的相关性限制了在MTS数据中全面建模时空依赖性，从而限制了现有GNNs学习有效的表示。为了解决这个限制，我们提出了一种新方法，称为全连接的时空图神经网络（FC-STGNN），包括两个关键组成部分，即FC图构建和FC图卷积。

    Multivariate Time-Series (MTS) data is crucial in various application fields. With its sequential and multi-source (multiple sensors) properties, MTS data inherently exhibits Spatial-Temporal (ST) dependencies, involving temporal correlations between timestamps and spatial correlations between sensors in each timestamp. To effectively leverage this information, Graph Neural Network-based methods (GNNs) have been widely adopted. However, existing approaches separately capture spatial dependency and temporal dependency and fail to capture the correlations between Different sEnsors at Different Timestamps (DEDT). Overlooking such correlations hinders the comprehensive modelling of ST dependencies within MTS data, thus restricting existing GNNs from learning effective representations. To address this limitation, we propose a novel method called Fully-Connected Spatial-Temporal Graph Neural Network (FC-STGNN), including two key components namely FC graph construction and FC graph convolutio
    
[^101]: 多元时间序列分类中的图感知对比学习

    Graph-Aware Contrasting for Multivariate Time-Series Classification. (arXiv:2309.05202v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.05202](http://arxiv.org/abs/2309.05202)

    该论文提出了一种图感知对比学习方法，用于改进多元时间序列(MTS)分类任务。现有的对比学习方法忽视了MTS数据中的空间一致性，该方法通过图扩增和对比学习来保持传感器的稳定性和相关性，从而提高了对数据的表示能力。

    

    对比学习作为一种自监督学习范式，在多元时间序列（MTS）分类中变得流行起来。它确保了未标记样本的不同视图之间的一致性，然后学习这些样本的有效表示。现有的对比学习方法主要集中在通过时间扩增和对比技术实现时间一致性，旨在保护MTS数据的时间模式不受扰动。然而，它们忽视了需要确保传感器的稳定性和它们之间相关性的空间一致性。由于MTS数据通常来自多个传感器，确保空间一致性对于对比学习在MTS数据上的整体性能是至关重要的。因此，我们提出了一种图感知对比学习方法，用于实现MTS数据的空间一致性。具体而言，我们提出了包括节点和边扩增在内的图扩增，以保持传感器的稳定性和它们之间的相关性，然后应用图对比目标进行学习。

    Contrastive learning, as a self-supervised learning paradigm, becomes popular for Multivariate Time-Series (MTS) classification. It ensures the consistency across different views of unlabeled samples and then learns effective representations for these samples. Existing contrastive learning methods mainly focus on achieving temporal consistency with temporal augmentation and contrasting techniques, aiming to preserve temporal patterns against perturbations for MTS data. However, they overlook spatial consistency that requires the stability of individual sensors and their correlations. As MTS data typically originate from multiple sensors, ensuring spatial consistency becomes essential for the overall performance of contrastive learning on MTS data. Thus, we propose Graph-Aware Contrasting for spatial consistency across MTS data. Specifically, we propose graph augmentations including node and edge augmentations to preserve the stability of sensors and their correlations, followed by grap
    
[^102]: CenTime: 事件条件模型在生存分析中的应用

    CenTime: Event-Conditional Modelling of Censoring in Survival Analysis. (arXiv:2309.03851v1 [cs.LG])

    [http://arxiv.org/abs/2309.03851](http://arxiv.org/abs/2309.03851)

    CenTime是一种新的生存分析方法，通过创新的事件条件审查机制直接估计事件发生的时间，在处理未被审查的数据时具有良好的鲁棒性和准确性。

    

    生存分析是一种有价值的工具，可以基于基线观测来估计特定事件（如死亡或癌症复发）发生的时间。这在医疗保健中非常有用，可以根据患者数据预测临床重要事件的预后。然而，现有方法常常存在局限性；有些方法只关注将患者按生存能力进行排名，忽视了对实际事件时间的估计；而其他方法将问题视为分类任务，忽视了事件的时间顺序结构。此外，有效利用被审查样本（训练数据点，其中确切事件时间不可知）对于提高模型的预测准确性至关重要。在本文中，我们引入了CenTime，一种新的生存分析方法，直接估计事件发生的时间。我们的方法具有创新的事件条件审查机制，即使没有未被审查的数据，也能表现出良好的鲁棒性。我们证明了我们的方法在准确性上的优势。

    Survival analysis is a valuable tool for estimating the time until specific events, such as death or cancer recurrence, based on baseline observations. This is particularly useful in healthcare to prognostically predict clinically important events based on patient data. However, existing approaches often have limitations; some focus only on ranking patients by survivability, neglecting to estimate the actual event time, while others treat the problem as a classification task, ignoring the inherent time-ordered structure of the events. Furthermore, the effective utilization of censored samples - training data points where the exact event time is unknown - is essential for improving the predictive accuracy of the model. In this paper, we introduce CenTime, a novel approach to survival analysis that directly estimates the time to event. Our method features an innovative event-conditional censoring mechanism that performs robustly even when uncensored data is scarce. We demonstrate that ou
    
[^103]: Matcha-TTS: 一种具有条件流匹配的快速TTS架构

    Matcha-TTS: A fast TTS architecture with conditional flow matching. (arXiv:2309.03199v1 [eess.AS])

    [http://arxiv.org/abs/2309.03199](http://arxiv.org/abs/2309.03199)

    Matcha-TTS是一种快速TTS架构，使用最优传输条件流匹配训练，具有高质量输出和快速合成步骤。它不需要外部对齐，与其他模型相比，具有最小的内存占用，速度更快，并在听觉测试中获得了最高的评分。

    

    我们介绍了Matcha-TTS，一种新的编码器-解码器架构，用于高速TTS声学建模，该模型使用最优传输条件流匹配（OT-CFM）进行训练。这使得基于ODE的解码器能够在比使用得分匹配进行训练的模型更少的合成步骤中产生高质量的输出。精心设计的选择确保每个合成步骤的运行速度快。该方法是概率的、非自回归的，并且可以自主学习说话，无需外部对齐。与强大的预训练基线模型相比，Matcha-TTS系统具有最小的内存占用，与最快模型在长语音片段上的速度相当，并在一项听觉测试中获得了最高的评分。请访问https://shivammehta25.github.io/Matcha-TTS/ 查看音频示例、代码和预训练模型。

    We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS acoustic modelling, trained using optimal-transport conditional flow matching (OT-CFM). This yields an ODE-based decoder capable of high output quality in fewer synthesis steps than models trained using score matching. Careful design choices additionally ensure each synthesis step is fast to run. The method is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments. Compared to strong pre-trained baseline models, the Matcha-TTS system has the smallest memory footprint, rivals the speed of the fastest models on long utterances, and attains the highest mean opinion score in a listening test. Please see https://shivammehta25.github.io/Matcha-TTS/ for audio examples, code, and pre-trained models.
    
[^104]: 医学图像配准中的深度学习：介绍与综述

    Deep learning in medical image registration: introduction and survey. (arXiv:2309.00727v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2309.00727](http://arxiv.org/abs/2309.00727)

    这篇文章介绍了医学图像配准和使用深度学习的综述。它包括了图像配准的定义和符号表示，以及各种图像变换和医学图像配准算法。还讨论了基于图谱的配准和多阶段图像配准技术，以及评估指标和数据集。

    

    图像配准（IR）是一种将图像变形以使其相对于参考空间对齐的过程，使医务人员能够在标准化的参考框架中检查各种医学图像，如具有相同的旋转和缩放。本文以一个简单的数值示例介绍了图像配准，并提供了图像配准的定义以及空间方向的符号表示。本综述涵盖了各种图像变换的方面，包括仿射、可变形、可逆和双向变换，以及医学图像配准算法，如Voxelmorph、Demons、SyN、Iterative Closest Point和SynthMorph。还探讨了基于图谱的配准和多阶段图像配准技术，包括粗-细和金字塔方法。此外，本综述还讨论了医学图像配准的分类体系、数据集和评估指标，如基于相关性的度量和基于分割的度量。

    Image registration (IR) is a process that deforms images to align them with respect to a reference space, making it easier for medical practitioners to examine various medical images in a standardized reference frame, such as having the same rotation and scale. This document introduces image registration using a simple numeric example. It provides a definition of image registration along with a space-oriented symbolic representation. This review covers various aspects of image transformations, including affine, deformable, invertible, and bidirectional transformations, as well as medical image registration algorithms such as Voxelmorph, Demons, SyN, Iterative Closest Point, and SynthMorph. It also explores atlas-based registration and multistage image registration techniques, including coarse-fine and pyramid approaches. Furthermore, this survey paper discusses medical image registration taxonomies, datasets, evaluation measures, such as correlation-based metrics, segmentation-based me
    
[^105]: 多保真度傅里叶神经算子用于快速建模大规模地质碳储存

    Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale Geological Carbon Storage. (arXiv:2308.09113v1 [stat.ML])

    [http://arxiv.org/abs/2308.09113](http://arxiv.org/abs/2308.09113)

    多保真度傅里叶神经算子用于解决大规模地质碳储存问题，通过利用经济性更高的多保真度训练数据集，能够以与高保真度模型相当的准确性进行预测。

    

    深度学习的代理模型已广泛应用于地质碳储存（GCS）问题，以加快预测储压和二氧化碳云层移动。然而，由于高计算成本，大规模三维问题的可用训练数据始终有限。因此，我们提出使用多保真度傅里叶神经算子来解决大规模GCS问题，利用更具经济性的多保真度训练数据集。傅里叶神经算子具有良好的网格不变性，简化了不同离散数据集之间的迁移学习过程。我们首先在一个GCS储层模型上进行模型有效性测试，该模型被划分为110,000个网格单元。多保真度模型的预测准确度可与高保真度模型的训练进行比较。

    Deep learning-based surrogate models have been widely applied in geological carbon storage (GCS) problems to accelerate the prediction of reservoir pressure and CO2 plume migration. Large amounts of data from physics-based numerical simulators are required to train a model to accurately predict the complex physical behaviors associated with this process. In practice, the available training data are always limited in large-scale 3D problems due to the high computational cost. Therefore, we propose to use a multi-fidelity Fourier Neural Operator to solve large-scale GCS problems with more affordable multi-fidelity training datasets. The Fourier Neural Operator has a desirable grid-invariant property, which simplifies the transfer learning procedure between datasets with different discretization. We first test the model efficacy on a GCS reservoir model being discretized into 110k grid cells. The multi-fidelity model can predict with accuracy comparable to a high-fidelity model trained wi
    
[^106]: 非线性、反馈和因果结构学习中的一致性问题研究

    Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning. (arXiv:2308.07520v1 [stat.ML])

    [http://arxiv.org/abs/2308.07520](http://arxiv.org/abs/2308.07520)

    这篇论文研究了非线性、反馈和因果结构学习中的一致性问题，并提出了一个弱于强可靠性的k-Triangle Faithfulness的替代定义。

    

    因果发现的目标是从观测数据中找到学习因果结构的自动化搜索方法。有些情况下，感兴趣的因果机制的所有变量都已经被测量，任务是预测一个变量对另一个变量的影响。相反，有时主要关注的变量并非直接可观察，而是通过它们在数据中的表现来推理出来的。这些被称为潜在变量。一个广泛被知道的例子是心理构造的智商，因为无法直接测量，所以研究人员尝试通过各种指标如智商测试来评估。在这种情况下，因果发现算法可以揭示潜在变量之间和潜在变量与观察变量之间的因果连接，从而发现潜在的模式和结构。这篇论文主要研究因果发现中的两个问题：提供了一个弱于强可靠性的k-Triangle Faithfulness的替代定义，并提出了对统计一致性的新要求。

    The goal of Causal Discovery is to find automated search methods for learning causal structures from observational data. In some cases all variables of the interested causal mechanism are measured, and the task is to predict the effects one measured variable has on another. In contrast, sometimes the variables of primary interest are not directly observable but instead inferred from their manifestations in the data. These are referred to as latent variables. One commonly known example is the psychological construct of intelligence, which cannot directly measured so researchers try to assess through various indicators such as IQ tests. In this case, casual discovery algorithms can uncover underlying patterns and structures to reveal the causal connections between the latent variables and between the latent and observed variables. This thesis focuses on two questions in causal discovery: providing an alternative definition of k-Triangle Faithfulness that (i) is weaker than strong faithfu
    
[^107]: 通过社交媒体数据和易感-感染-康复（SIR）模型研究灾害响应：以2020年西部美国火灾季为案例研究

    Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season. (arXiv:2308.05281v1 [cs.SI])

    [http://arxiv.org/abs/2308.05281](http://arxiv.org/abs/2308.05281)

    该研究通过社交媒体数据和SIR模型研究了2020年西部美国火灾季的灾害响应。研究发现Twitter用户主要关注健康影响、损失和撤离三个主题，并使用SIR理论探索了这些主题在Twitter上的传播规模和速度。

    

    有效的灾害响应对受影响的社区至关重要。应急人员和决策者在灾害期间在了解社区所面临问题的可靠和及时的指标上将受益于社交媒体提供的丰富数据来源。社交媒体可以反映公众关注和需求，为决策者提供有价值的洞见，以了解不断演变的情况并优化资源配置。我们使用双向编码器表示转换（BERT）主题建模对Twitter数据进行主题聚类。然后，我们进行了时间-空间分析，研究了这些主题在2020年美国西部火灾季期间在不同地区的分布情况。我们的结果显示，Twitter用户主要关注三个主题：“健康影响”，“损失”，“撤离”。我们使用易感-感染-康复（SIR）理论来探索主题在Twitter上的传播规模和速度。结果清晰地显示了主题传播的情况。

    Effective disaster response is critical for affected communities. Responders and decision-makers would benefit from reliable, timely measures of the issues impacting their communities during a disaster, and social media offers a potentially rich data source. Social media can reflect public concerns and demands during a disaster, offering valuable insights for decision-makers to understand evolving situations and optimize resource allocation. We used Bidirectional Encoder Representations from Transformers (BERT) topic modeling to cluster topics from Twitter data. Then, we conducted a temporal-spatial analysis to examine the distribution of these topics across different regions during the 2020 western U.S. wildfire season. Our results show that Twitter users mainly focused on three topics:"health impact," "damage," and "evacuation." We used the Susceptible-Infected-Recovered (SIR) theory to explore the magnitude and velocity of topic diffusion on Twitter. The results displayed a clear re
    
[^108]: 评估用于自动驾驶中的行人轨迹预测方法

    Evaluating Pedestrian Trajectory Prediction Methods for the Application in Autonomous Driving. (arXiv:2308.05194v1 [cs.LG])

    [http://arxiv.org/abs/2308.05194](http://arxiv.org/abs/2308.05194)

    本论文评估了行人轨迹预测方法在自动驾驶应用中的可行性，并发现简单模型在生成单个轨迹时仍然具有竞争力，某些通常被认为有用的特征对整体性能影响较小。

    

    本文评估了行人轨迹预测领域的最新方法与常速模型在自动驾驶应用中的适用性。评估在广泛使用的ETH/UCY数据集上进行，报告了平均位移误差（ADE）和最终位移误差（FDE）。为了符合实际应用的要求，对初始模型的输入特征进行了修改。进行了消融研究，以研究观察到的运动历史对预测性能的影响，从而建立更好的理解。此外，还测量了每个模型的推理时间，以评估面对不同数量代理时每个模型的可扩展性。结果表明，在生成单个轨迹时，简单模型仍然具有竞争力，某些通常被认为有用的特征对整体性能影响很小。

    In this paper, the state of the art in the field of pedestrian trajectory prediction is evaluated alongside the constant velocity model (CVM) with respect to its applicability in autonomous vehicles. The evaluation is conducted on the widely-used ETH/UCY dataset where the Average Displacement Error (ADE) and the Final Displacement Error (FDE) are reported. To align with requirements in real-world applications, modifications are made to the input features of the initially proposed models. An ablation study is conducted to examine the influence of the observed motion history on the prediction performance, thereby establishing a better understanding of its impact. Additionally, the inference time of each model is measured to evaluate the scalability of each model when confronted with varying amounts of agents. The results demonstrate that simple models remain competitive when generating single trajectories, and certain features commonly thought of as useful have little impact on the overa
    
[^109]: SLEM：机器学习用于路径建模和因果推断的超级学习者方程模型

    SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling. (arXiv:2308.04365v1 [stat.ML])

    [http://arxiv.org/abs/2308.04365](http://arxiv.org/abs/2308.04365)

    SLEM是一种路径建模技术，通过集成机器学习超级学习者，实现了一致且无偏的因果效应估计，并在处理非线性关系时超过了传统的结构方程模型。

    

    因果推断是科学的关键目标，使研究人员能够通过观察数据得出关于对假定干预的预测的有意义的结论。路径模型、结构方程模型(SEMs)以及更一般的有向无环图(DAGs)能够明确地指定关于现象背后的因果结构的假设。与DAGs不同，SEMs假设线性关系，这可能导致函数错误规范，从而阻碍研究人员进行可靠的效果大小估计。相反，我们提出了超级学习者方程模型（SLEM），一种集成了机器学习超级学习者集成的路径建模技术。我们通过实证研究，证明了SLEM能够提供一致且无偏的因果效应估计，在与SEMs进行线性模型比较时表现出竞争力，并且在处理非线性关系时优于SEMs。

    Causal inference is a crucial goal of science, enabling researchers to arrive at meaningful conclusions regarding the predictions of hypothetical interventions using observational data. Path models, Structural Equation Models (SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to unambiguously specify assumptions regarding the causal structure underlying a phenomenon. Unlike DAGs, which make very few assumptions about the functional and parametric form, SEM assumes linearity. This can result in functional misspecification which prevents researchers from undertaking reliable effect size estimation. In contrast, we propose Super Learner Equation Modeling, a path modeling technique integrating machine learning Super Learner ensembles. We empirically demonstrate its ability to provide consistent and unbiased estimates of causal effects, its competitive performance for linear models when compared with SEM, and highlight its superiority over SEM when dealing with non
    
[^110]: 最后一层的训练是否足以应对虚假相关性？

    Is Last Layer Re-Training Truly Sufficient for Robustness to Spurious Correlations?. (arXiv:2308.00473v1 [cs.LG])

    [http://arxiv.org/abs/2308.00473](http://arxiv.org/abs/2308.00473)

    通过对最后一层进行重新训练，Deep Feature Reweighting（DFR）方法可以提高模型在存在虚假相关性的数据中的准确性，但其应用于实际医学数据时存在一定局限性。

    

    以经验风险最小化（ERM）训练的模型已被知晓学会依赖虚假特征，即它们的预测基于与类别标签强相关但缺乏因果推理的非期望辅助特征。这种行为尤其在相关类别的样本组中，可能没有这些虚假特征或者相反类别的样本中存在这些虚假特征时，导致准确性的下降。最近提出的深度特征重加权（DFR）方法提高了这些最差样本组的准确性。基于ERM模型可以足够好地学习核心特征的主要论点，DFR只需对分类模型的最后一层进行小规模平衡数据集的重新训练。在本研究中，我们检验了DFR在医学领域真实数据中的适用性。此外，我们对最后一层重新训练有效性背后的推理进行了调查，结果表明尽管DFR具有提高最差样本组准确性的潜力，但其实现方式存在局限性。

    Models trained with empirical risk minimization (ERM) are known to learn to rely on spurious features, i.e., their prediction is based on undesired auxiliary features which are strongly correlated with class labels but lack causal reasoning. This behavior particularly degrades accuracy in groups of samples of the correlated class that are missing the spurious feature or samples of the opposite class but with the spurious feature present. The recently proposed Deep Feature Reweighting (DFR) method improves accuracy of these worst groups. Based on the main argument that ERM mods can learn core features sufficiently well, DFR only needs to retrain the last layer of the classification model with a small group-balanced data set. In this work, we examine the applicability of DFR to realistic data in the medical domain. Furthermore, we investigate the reasoning behind the effectiveness of last-layer retraining and show that even though DFR has the potential to improve the accuracy of the wors
    
[^111]: MSQNet: 无关演员的多模态动作识别

    MSQNet: Actor-agnostic Action Recognition with Multi-modal Query. (arXiv:2307.10763v1 [cs.CV])

    [http://arxiv.org/abs/2307.10763](http://arxiv.org/abs/2307.10763)

    MSQNet是一种无关演员的多模态多标签动作识别方法，通过使用视觉和文本模态来更好地表示动作类别，克服了现有方法中针对特定演员的限制。

    

    现有的动作识别方法通常是针对特定演员的，因为演员之间具有固有的拓扑和显着差异。这就需要特定演员的姿态估计（例如人类与动物），导致模型设计复杂性和高维护成本。此外，它们通常只关注学习视觉模态和单标签分类，忽视了其他可用信息源（例如类名文本）和多个动作的同时发生。为了克服这些限制，我们提出了一种新的方法，称为“无关演员的多模态多标签动作识别”，为包括人类和动物在内的各种类型的演员提供了统一的解决方案。我们进一步在基于Transformer的目标检测框架（例如DETR）中提出了一种新颖的多模态语义查询网络（MSQNet）模型，通过利用视觉和文本模态更好地表示动作类别。消除了演员特定性的限制。

    Existing action recognition methods are typically actor-specific due to the intrinsic topological and apparent differences among the actors. This requires actor-specific pose estimation (e.g., humans vs. animals), leading to cumbersome model design complexity and high maintenance costs. Moreover, they often focus on learning the visual modality alone and single-label classification whilst neglecting other available information sources (e.g., class name text) and the concurrent occurrence of multiple actions. To overcome these limitations, we propose a new approach called 'actor-agnostic multi-modal multi-label action recognition,' which offers a unified solution for various types of actors, including humans and animals. We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better. The elimination of actor-spec
    
[^112]: 异构特征子采样的Ridge Ensemble的学习曲线

    Learning Curves for Heterogeneous Feature-Subsampled Ridge Ensembles. (arXiv:2307.03176v1 [stat.ML])

    [http://arxiv.org/abs/2307.03176](http://arxiv.org/abs/2307.03176)

    通过引入异构特征集成，本文研究了在线性回归设置中构建的岭回归集成的学习曲线，结果表明异构特征集成具有较好的性能。

    

    特征包装是一种旨在通过在随机子样本或特征投影上训练估计器来减少预测方差的成熟集成方法。通常，集成选择是同质的，即估计器可用的特征维数在整个集成中是均匀的。在这里，我们介绍了异构特征集成方法，其中的估计器基于变动的特征维数，并研究其在线性回归设置中的性能。我们研究了一个线性预测器的集成，每个预测器使用部分可用特征进行岭回归拟合。我们允许这些子集中包含的特征数量有所变化。利用统计物理中的复制技巧，我们推导了具有确定性线性掩模的岭回归集成的学习曲线。对于具有各向同性特征噪声的等相相关数据，我们得到了学习曲线的显式表达式。利用这些推导表达式，我们研究了集成在不同特征维数下的性能。

    Feature bagging is a well-established ensembling method which aims to reduce prediction variance by training estimators in an ensemble on random subsamples or projections of features. Typically, ensembles are chosen to be homogeneous, in the sense the the number of feature dimensions available to an estimator is uniform across the ensemble. Here, we introduce heterogeneous feature ensembling, with estimators built on varying number of feature dimensions, and consider its performance in a linear regression setting. We study an ensemble of linear predictors, each fit using ridge regression on a subset of the available features. We allow the number of features included in these subsets to vary. Using the replica trick from statistical physics, we derive learning curves for ridge ensembles with deterministic linear masks. We obtain explicit expressions for the learning curves in the case of equicorrelated data with an isotropic feature noise. Using the derived expressions, we investigate t
    
[^113]: 深度神经网络如何学习组合性数据：随机层次模型

    How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model. (arXiv:2307.02129v1 [cs.LG])

    [http://arxiv.org/abs/2307.02129](http://arxiv.org/abs/2307.02129)

    本文研究了深度神经网络学习组合性数据的问题，通过对随机层次模型进行分类任务，发现深度CNN学习这个任务所需的训练数据数量随着类别数、组合数和迭代次数的增加而渐进增加。

    

    学习一般高维任务是非常困难的，因为它需要与维度成指数增长的训练数据数量。然而，深度卷积神经网络（CNN）在克服这一挑战方面显示出了卓越的成功。一种普遍的假设是可学习任务具有高度结构化，CNN利用这种结构建立了数据的低维表示。然而，我们对它们需要多少训练数据以及这个数字如何取决于数据结构知之甚少。本文回答了针对一个简单的分类任务的这个问题，该任务旨在捕捉真实数据的相关方面：随机层次模型。在这个模型中，$n_c$个类别中的每一个对应于$m$个同义组合的高层次特征，并且这些特征又通过一个重复$L$次的迭代过程由子特征组成。我们发现，需要深度CNN学习这个任务的训练数据数量$P^*$（i）随着$n_c m^L$的增长而渐进地增长，这只有...

    Learning generic high-dimensional tasks is notably hard, as it requires a number of training data exponential in the dimension. Yet, deep convolutional neural networks (CNNs) have shown remarkable success in overcoming this challenge. A popular hypothesis is that learnable tasks are highly structured and that CNNs leverage this structure to build a low-dimensional representation of the data. However, little is known about how much training data they require, and how this number depends on the data structure. This paper answers this question for a simple classification task that seeks to capture relevant aspects of real data: the Random Hierarchy Model. In this model, each of the $n_c$ classes corresponds to $m$ synonymic compositions of high-level features, which are in turn composed of sub-features through an iterative process repeated $L$ times. We find that the number of training data $P^*$ required by deep CNNs to learn this task (i) grows asymptotically as $n_c m^L$, which is only
    
[^114]: BuildingsBench：一个包含900K座建筑物的大规模数据集和短期负荷预测基准

    BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting. (arXiv:2307.00142v1 [cs.LG])

    [http://arxiv.org/abs/2307.00142](http://arxiv.org/abs/2307.00142)

    本文提出了BuildingsBench，这是一个包含900K座建筑物的大规模数据集，旨在解决短期负荷预测中数据集不足的问题。通过该数据集，我们进行了两个任务的基准评估，并发现经过合成预训练的模型具有良好的泛化能力。

    

    针对短期负荷预测(STLF)中缺乏开放、大规模、高建筑多样性数据集的问题，本文提出了BuildingsBench，包括1)包含900K个模拟建筑的大规模数据集Buildings-900K，以模拟美国的建筑库存，以及2)拥有来自7个开放数据集的超过1900个真实住宅和商业建筑物的评估平台。BuildingsBench为两个未被充分探索的任务提供了基准：零-shot STLF，其中预训练模型在未见过的建筑上进行评估而无需微调；以及迁移学习，其中预训练模型在目标建筑上进行微调。本次基准分析的主要发现是，经过合成预训练的模型意外地具有良好的泛化能力。

    Short-term forecasting of residential and commercial building energy consumption is widely used in power systems and continues to grow in importance. Data-driven short-term load forecasting (STLF), although promising, has suffered from a lack of open, large-scale datasets with high building diversity. This has hindered exploring the pretrain-then-finetune paradigm for STLF. To help address this, we present BuildingsBench, which consists of 1) Buildings-900K, a large-scale dataset of 900K simulated buildings representing the U.S. building stock, and 2) an evaluation platform with over 1,900 real residential and commercial buildings from 7 open datasets. BuildingsBench benchmarks two under-explored tasks: zero-shot STLF, where a pretrained model is evaluated on unseen buildings without fine-tuning, and transfer learning, where a pretrained model is fine-tuned on a target building. The main finding of our benchmark analysis is that synthetically pretrained models generalize surprisingly w
    
[^115]: 快速确定单光子源质量的挑战

    The Challenge of Quickly Determining the Quality of a Single-Photon Source. (arXiv:2306.15683v1 [physics.optics])

    [http://arxiv.org/abs/2306.15683](http://arxiv.org/abs/2306.15683)

    该研究通过使用数据增强技术，结合实验数据和自举样本，提出了一种快速评估单光子源质量的方法，并揭示了多光子发射事件概率的不确定性对质量评估的重要影响。

    

    近期文献提出了新颖方法来快速评估单光子源（SPS）的质量，例如量子点，以解决实验验证通过光强干涉仪验证效率低和耗时长的问题。然而，由于缺乏不确定性讨论和可重复性细节，这些方法的可靠性引发了关切。本研究对从发射波长为1.3μm，由80MHz激光器激发的InGaAs/GaAs外延量子点获得的八组数据进行了研究。该研究通过采用数据增强的机器学习技术，将实验数据与自举样本相结合，引入了一种新颖的贡献。对合成样本的高效直方图拟合导出的SPS质量指标，即多光子发射事件的概率进行分析，揭示了由描述探测率的泊松过程中随机变异性贡献的显著不确定性。忽视这个因素可能导致对SPS质量的错误评估。

    Novel methods for rapidly estimating single-photon source (SPS) quality, e.g. of quantum dots, have been promoted in recent literature to address the expensive and time-consuming nature of experimental validation via intensity interferometry. However, the frequent lack of uncertainty discussions and reproducible details raises concerns about their reliability. This study investigates one such proposal on eight datasets obtained from an InGaAs/GaAs epitaxial quantum dot that emits at 1.3 {\mu}m and is excited by an 80 MHz laser. The study introduces a novel contribution by employing data augmentation, a machine learning technique, to supplement experimental data with bootstrapped samples. Analysis of the SPS quality metric, i.e. the probability of multi-photon emission events, as derived from efficient histogram fitting of the synthetic samples, reveals significant uncertainty contributed by stochastic variability in the Poisson processes that describe detection rates. Ignoring this sou
    
[^116]: TrustGuard: 基于GNN的动态支持鲁棒且可解释的信任评估

    TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG])

    [http://arxiv.org/abs/2306.13339](http://arxiv.org/abs/2306.13339)

    TrustGuard是一种基于GNN的信任评估模型，支持信任动态性，抗击鲁棒并提供解释能力，它的实验结果在准确性、鲁棒性和可解释性方面都优于其他方法。

    

    信任评估评估实体之间的信任关系并促进决策。机器学习由于其学习能力而表现出巨大的潜力，因此对信任评估具有重要意义。近年来，作为一种新的机器学习范 paradigm，图神经网络（GNN）在处理图形数据方面表现出优越性。这激发了研究人员探索将其用于信任评估，因为实体之间的信任关系可以建模为图形。但是，使用GNN的当前信任评估方法未能完全满足信任的动态性，忽略了攻击对信任评估的不利影响，并且无法提供令人信服的评估结果解释。为解决这些问题，在本文中，我们提出了TrustGuard ：一种支持信任动态性、抗击鲁棒且通过可视化提供解释的精确信任评估模型。具体而言，TrustGuard 设计了一个由动态感知节点嵌入层、图卷积层、注意机制层和信任预测层组成的分层架构。为了评估提出的模型的有效性，我们对真实数据集进行了实验，并将TrustGuard与其他最先进的方法进行了比较。实验结果表明，TrustGuard 在准确性、鲁棒性和可解释性方面均优于其他方法。

    Trust evaluation assesses trust relationships between entities and facilitates decision-making. Machine Learning (ML) shows great potential for trust evaluation owing to its learning capabilities. In recent years, Graph Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in dealing with graph data. This has motivated researchers to explore their use in trust evaluation, as trust relationships among entities can be modeled as a graph. However, current trust evaluation methods that employ GNNs fail to fully satisfy the dynamicity nature of trust, overlook the adverse effects of attacks on trust evaluation, and cannot provide convincing explanations on evaluation results. To address these problems, in this paper, we propose TrustGuard, a GNN-based accurate trust evaluation model that supports trust dynamicity, is robust against typical attacks, and provides explanations through visualization. Specifically, TrustGuard is designed with a layered architecture that con
    
[^117]: 在线动态子模规划优化

    Online Dynamic Submodular Optimization. (arXiv:2306.10835v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2306.10835](http://arxiv.org/abs/2306.10835)

    该论文介绍了在线动态子模规划优化问题，并提出了在线子模贪婪算法（OSGA）和在线子模映射梯度下降（OSPGD）算法以解决此类问题。实验结果表明，这些算法在不同的电力系统中表现良好。

    

    我们提出了一种新的具有可证明性能的算法，用于处理满足一般约束条件和动态环境下的在线二元优化问题。我们考虑了目标函数为子模规划的问题子集。我们提出了在线子模贪婪算法（OSGA），该算法通过求解先前轮损失函数的近似值来避免原问题的NP-困难性。我们将OSGA扩展为通用的近似函数。我们证明了OSGA在时间长度和累积轮次最优变化方面具有与在线凸优化中最严格边界相似的动态遗憾界。对于没有近似解或需要更简单的实现的情况，我们设计了在线子模映射梯度下降（OSPGD）算法，利用Lova\'sz扩展。我们得到了类似于传统在线梯度下降（OGD）的遗憾界。最后，我们在两个电力系统中对算法进行了数值测试。

    We propose new algorithms with provable performance for online binary optimization subject to general constraints and in dynamic settings. We consider the subset of problems in which the objective function is submodular. We propose the online submodular greedy algorithm (OSGA) which solves to optimality an approximation of the previous round loss function to avoid the NP-hardness of the original problem. We extend OSGA to a generic approximation function. We show that OSGA has a dynamic regret bound similar to the tightest bounds in online convex optimization with respect to the time horizon and the cumulative round optimum variation. For instances where no approximation exists or a computationally simpler implementation is desired, we design the online submodular projected gradient descent (OSPGD) by leveraging the Lova\'sz extension. We obtain a regret bound that is akin to the conventional online gradient descent (OGD). Finally, we numerically test our algorithms in two power system
    
[^118]: $K$最近邻重采样用于随机控制中的离线策略评估

    $K$-Nearest-Neighbor Resampling for Off-Policy Evaluation in Stochastic Control. (arXiv:2306.04836v1 [stat.ML])

    [http://arxiv.org/abs/2306.04836](http://arxiv.org/abs/2306.04836)

    提出了一种新颖的$K$最近邻重采样方法，用于估算历史数据中由不同策略生成的决策过程的性能，解决了离线策略评估中的反事实估计问题。

    

    本文提出了一种新颖的$K$最近邻重采样方法，用于估算历史数据中由不同策略生成的决策过程的性能。我们专注于依赖于当前状态的反馈策略，这种策略适用于具有连续状态-动作空间和所选动作影响下的系统固有随机性的环境。这些设置在许多高风险应用程序中很常见，并在随机控制的上下文中积极研究。我们的过程利用了类似的状态/动作对（在度量意义下）与类似的奖励和状态转换相关。这使得我们的重采样过程通过类似于蒙特卡罗方法的轨迹模拟来解决离线策略评估（OPE）中的反事实估计问题。与其他OPE方法相比，我们的算法不需要优化，可以通过基于树的最近邻搜索高效实现，并且本质上是可并行化的。我们提供理论性能保证，并在基准环境下展示了我们算法的优越实验性能。

    We propose a novel $K$-nearest neighbor resampling procedure for estimating the performance of a policy from historical data containing realized episodes of a decision process generated under a different policy. We focus on feedback policies that depend deterministically on the current state in environments with continuous state-action spaces and system-inherent stochasticity effected by chosen actions. Such settings are common in a wide range of high-stake applications and are actively investigated in the context of stochastic control. Our procedure exploits that similar state/action pairs (in a metric sense) are associated with similar rewards and state transitions. This enables our resampling procedure to tackle the counterfactual estimation problem underlying off-policy evaluation (OPE) by simulating trajectories similarly to Monte Carlo methods. Compared to other OPE methods, our algorithm does not require optimization, can be efficiently implemented via tree-based nearest neighbo
    
[^119]: 稀疏不规则点云的叶片/木材区分的语义分割

    Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination. (arXiv:2305.16963v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.16963](http://arxiv.org/abs/2305.16963)

    这个论文介绍了一个基于Pointnet++架构的神经网络模型，用于解决稀疏不规则点云中叶片和木材的区分问题。

    

    LiDAR（光探测与测距）已成为遥感工具箱中不可或缺的一部分，用于生物圈监测。特别是，LiDAR提供了以前所未有的准确度来绘制森林叶面积，而叶面积一直是影响植被与大气间气体交换模型的重要不确定性源。无人机（UAV）易于部署，因此可以频繁重访以跟踪植被对气候变化的响应。然而，装载在UAV上的微型传感器通常只提供有限密度的点云，而且由于逐渐增强的遮挡，从顶部到底部的密度呈现明显下降的趋势。在这种情况下，由于强烈的类别不平衡和空间上的不规则采样密度，将叶片点区分出来木材点面临着巨大挑战。在这里，我们介绍了一个基于Pointnet ++架构的神经网络模型

    LiDAR (Light Detection and Ranging) has become an essential part of the remote sensing toolbox used for biosphere monitoring. In particular, LiDAR provides the opportunity to map forest leaf area with unprecedented accuracy, while leaf area has remained an important source of uncertainty affecting models of gas exchanges between the vegetation and the atmosphere. Unmanned Aerial Vehicles (UAV) are easy to mobilize and therefore allow frequent revisits to track the response of vegetation to climate change. However, miniature sensors embarked on UAVs usually provide point clouds of limited density, which are further affected by a strong decrease in density from top to bottom of the canopy due to progressively stronger occlusion. In such a context, discriminating leaf points from wood points presents a significant challenge due in particular to strong class imbalance and spatially irregular sampling intensity. Here we introduce a neural network model based on the Pointnet ++ architecture 
    
[^120]: FedZero：利用可再生多余能源在联邦学习中

    FedZero: Leveraging Renewable Excess Energy in Federated Learning. (arXiv:2305.15092v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15092](http://arxiv.org/abs/2305.15092)

    FedZero是一个在联邦学习中利用可再生多余能源的系统，通过能源和负载预测来调度训练任务，将碳排放降低到零。

    

    联邦学习（FL）是一种新兴的机器学习技术，可以在数据孤岛或边缘设备之间进行分布式模型训练，而无需共享数据。然而，与集中式模型训练相比，FL不可避免地引入了效率低下的问题，这将进一步增加未来机器学习的高能耗和相关的碳排放。减少FL的碳足迹的一个思路是根据电网中某些时段和地点可能出现的可再生多余能源的可用性进行调度训练任务。然而，在存在这种波动和不可靠的资源的情况下，现有的FL调度器不能始终保证快速、高效和公平的训练。我们提出了一种名为FedZero的FL系统，它仅在可再生多余能源和计算基础设施的闲置容量上运行，将训练的操作碳排放有效降低到零。通过能源和负载预测，FedZero利用多余的可再生能源的时空可用性来进行调度。

    Federated Learning (FL) is an emerging machine learning technique that enables distributed model training across data silos or edge devices without data sharing. Yet, FL inevitably introduces inefficiencies compared to centralized model training, which will further increase the already high energy usage and associated carbon emissions of machine learning in the future. One idea to reduce FL's carbon footprint is to schedule training jobs based on the availability of renewable excess energy that can occur at certain times and places in the grid. However, in the presence of such volatile and unreliable resources, existing FL schedulers cannot always ensure fast, efficient, and fair training.  We propose FedZero, an FL system that operates exclusively on renewable excess energy and spare capacity of compute infrastructure to effectively reduce a training's operational carbon emissions to zero. Using energy and load forecasts, FedZero leverages the spatio-temporal availability of excess re
    
[^121]: 证据网络：用简单的损失函数快速、分摊式地进行神经贝叶斯模型比较

    Evidence Networks: simple losses for fast, amortized, neural Bayesian model comparison. (arXiv:2305.11241v1 [cs.LG])

    [http://arxiv.org/abs/2305.11241](http://arxiv.org/abs/2305.11241)

    本论文提出了一种名为证据网络的方法，能够在处理似然函数或先验函数与嵌套抽样无法胜任的情况下实现贝叶斯模型比较。与传统方法不同的是，该方法使用了新的损失函数，使得我们能够更快速地、更有效地估算贝叶斯因子。

    

    证据网络可在当现有的方法（如嵌套抽样）失败、似然函数或先验函数难以处理或不知道的情况下实现贝叶斯模型比较。贝叶斯模型比较可看作一个优化问题。虽然用贝叶斯法进行最优分类的解释已经众所周知，但在这里，我们改变了视角，提出了一系列损失函数，以产生快速、分摊式的神经估计器，直接估算方便的贝叶斯因子的函数。这减少了估算单个模型概率时的数字不准确性。我们介绍了渗漏奇 parity-odd power（l-POP）变换，引导了新的“l-Pop-Exponential”的损失函数。我们探讨了在不同模型中对数据概率进行神经密度估计，结果表明这种方法比证据网络的精度和可扩展性都要低。多种实际和人造例子证明了证据网络的优越性。

    Evidence Networks can enable Bayesian model comparison when state-of-the-art methods (e.g. nested sampling) fail and even when likelihoods or priors are intractable or unknown. Bayesian model comparison, i.e. the computation of Bayes factors or evidence ratios, can be cast as an optimization problem. Though the Bayesian interpretation of optimal classification is well-known, here we change perspective and present classes of loss functions that result in fast, amortized neural estimators that directly estimate convenient functions of the Bayes factor. This mitigates numerical inaccuracies associated with estimating individual model probabilities. We introduce the leaky parity-odd power (l-POP) transform, leading to the novel ``l-POP-Exponential'' loss function. We explore neural density estimation for data probability in different models, showing it to be less accurate and scalable than Evidence Networks. Multiple real-world and synthetic examples illustrate that Evidence Networks are e
    
[^122]: DualFL：一种基于对偶的Federated Learning算法及在一般凸情形下加速通讯

    DualFL: A Duality-based Federated Learning Algorithm with Communication Acceleration in the General Convex Regime. (arXiv:2305.10294v1 [cs.LG])

    [http://arxiv.org/abs/2305.10294](http://arxiv.org/abs/2305.10294)

    DualFL是一种基于对偶的联邦学习算法，通过具体对偶形式解决分布式优化问题，并保证了即使使用不精确的本地解决方案也可以实现最佳通信复杂度。

    

    我们提出了一种名为DualFL（Dualized Federated Learning）的新型训练算法，用于解决联邦学习的分布式优化问题。我们的方法基于联邦学习问题的特定对偶形式。DualFL在不同的光滑性和强凸性设置下实现通讯加速。此外，它在理论上保证使用不精确的本地求解器，即使是使用不精确的本地解决方案，也可以保持其最佳通信复杂度。DualFL是第一个实现通讯加速的联邦学习算法，即使成本函数既非光滑也非强凸，也可以使用。数值结果表明，DualFL的实际性能与最先进的联邦学习算法相当，并且对超参数调整是稳健的。

    We propose a novel training algorithm called DualFL (Dualized Federated Learning), for solving a distributed optimization problem in federated learning. Our approach is based on a specific dual formulation of the federated learning problem. DualFL achieves communication acceleration under various settings on smoothness and strong convexity of the problem. Moreover, it theoretically guarantees the use of inexact local solvers, preserving its optimal communication complexity even with inexact local solutions. DualFL is the first federated learning algorithm that achieves communication acceleration, even when the cost function is either nonsmooth or non-strongly convex. Numerical results demonstrate that the practical performance of DualFL is comparable to those of state-of-the-art federated learning algorithms, and it is robust with respect to hyperparameter tuning.
    
[^123]: 交叉门控多层感知机下的蛋白质复合物不变嵌入是一种一次性抗体设计器

    Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.09480](http://arxiv.org/abs/2305.09480)

    本文提出了一种深度生成模型，可以一次性地共同设计抗体CDR的1D序列和3D结构，解决几何建模和低效推断的问题。

    

    抗体是由免疫系统产生的针对外来物质或抗原的重要蛋白质。抗体的特异性由其互补决定区（CDR）决定，CDR位于抗体链的可变区域中，形成与抗原结合的位点。以往的研究利用复杂的技术生成CDR，但它们遭受了几何建模不足的问题。此外，常见的迭代精化策略导致了低效的推断。本文提出了一种深度生成模型，可以一次性地共同设计CDR的1D序列和3D结构。为了实现这一目标，我们将抗体CDR设计分为两个阶段：（i）蛋白质结构的几何建模和（ii）序列结构共学习。我们开发了一种蛋白质复合物不变嵌入，可捕捉蛋白质骨架原子（包括Cα、N、C和O原子）之间的内部和外部组分相互作用，以实现全面的几何建模。

    Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
    
[^124]: 基于块的主成分分析处理单调缺失数据的插值与降维

    Blockwise Principal Component Analysis for monotone missing data imputation and dimensionality reduction. (arXiv:2305.06042v1 [cs.LG])

    [http://arxiv.org/abs/2305.06042](http://arxiv.org/abs/2305.06042)

    基于块的主成分分析处理单调缺失数据的插值与降维框架，可以显著地减少插补时间，适用于大数据集。

    

    单调缺失数据是数据分析中常见的问题，插值和降维的组合可能会面临计算复杂度高的问题，特别是对于不断增长的数据集。为了解决这个问题，我们提出了一种叫BPI的框架，它利用了块内主成分分析的方法处理单调缺失数据的插值以及降维问题。该框架可以与各种插补技术一起使用，并且与插补后的降维相比，其可以显著地减少插补时间。通过实验验证，该框架具有较高效率并且有效。此外，我们的实验结果还表明，将MICE直接应用于缺失数据可能不是最佳方法。

    Monotone missing data is a common problem in data analysis. However, imputation combined with dimensionality reduction can be computationally expensive, especially with the increasing size of datasets. To address this issue, we propose a Blockwise principal component analysis Imputation (BPI) framework for dimensionality reduction and imputation of monotone missing data. The framework conducts Principal Component Analysis (PCA) on the observed part of each monotone block of the data and then imputes on merging the obtained principal components using a chosen imputation technique. BPI can work with various imputation techniques and can significantly reduce imputation time compared to conducting dimensionality reduction after imputation. This makes it a practical and efficient approach for large datasets with monotone missing data. Our experiments validate the improvement in speed. In addition, our experiments also show that while applying MICE imputation directly on missing data may not
    
[^125]: 使用随机Lp范数失真探究图像分类器的腐败稳健性

    Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions. (arXiv:2305.05400v1 [cs.LG])

    [http://arxiv.org/abs/2305.05400](http://arxiv.org/abs/2305.05400)

    本研究探讨了使用随机Lp范数失真对图像分类器的训练和测试数据进行增强，并评估模型对不可感知随机失真的稳健性，发现稳健性可能会提高模型在随机失真方面的性能，但也可能会损害L∞范数的稳健性。

    

    稳健性是机器学习分类器实现安全和可靠的基本属性。在对图像分类模型的对抗稳健性和形式稳健性验证领域中，稳健性通常被定义为在Lp范数距离内对所有输入变化的稳定性。然而，对随机失真的稳健性通常通过在现实世界中观察到的变化来改进和评估，而很少考虑数学定义的Lp范数失真。本研究探讨了使用随机Lp范数失真来增强图像分类器的训练和测试数据。我们借鉴了对抗稳健性领域的方法来评估模型对不可感知随机失真的稳健性。我们实证和理论上研究了在不同Lp范数之间稳健性是否可转移，并得出结论，哪些Lp范数的失真应该用来训练和评估模型。我们发现训练数据增强可能会提高模型在随机失真方面的性能，但也可能会损害L∞范数的稳健性。

    Robustness is a fundamental property of machine learning classifiers to achieve safety and reliability. In the fields of adversarial robustness and formal robustness verification of image classification models, robustness is commonly defined as the stability to all input variations within an Lp-norm distance. However, robustness to random corruptions is usually improved and evaluated using variations observed in the real-world, while mathematically defined Lp-norm corruptions are rarely considered. This study investigates the use of random Lp-norm corruptions to augment the training and test data of image classifiers. We adapt an approach from the field of adversarial robustness to assess the model robustness to imperceptible random corruptions. We empirically and theoretically investigate whether robustness is transferable across different Lp-norms and derive conclusions on which Lp-norm corruptions a model should be trained and evaluated on. We find that training data augmentation wi
    
[^126]: 学习鲁棒的深度平衡模型

    Learning Robust Deep Equilibrium Models. (arXiv:2304.12707v1 [cs.LG])

    [http://arxiv.org/abs/2304.12707](http://arxiv.org/abs/2304.12707)

    本论文提出一种名为LyaDEQ的鲁棒DEQ模型，通过Lyapunov理论提供了保证的稳定性以抵抗微小的初始扰动，并在不同的固定点之间加入全连接层以避免不良对抗性防御。

    

    深度平衡(DEQ)模型已成为深度学习中一种有前途的隐式层模型，它通过解决单个非线性层的固定点来放弃了传统深度。尽管这些模型很成功，但对于这些模型的固定点的稳定性仍然知之甚少。最近，将Lyapunov理论应用于另一种类型的隐式层模型——神经ODE，可以赋予其对抗鲁棒性。通过将DEQ模型视为非线性动态系统，我们提出了一种名为LyaDEQ的鲁棒DEQ模型，通过Lyapunov理论提供了保证的稳定性。我们方法的关键是确保DEQ模型的固定点是Lyapunov稳定的，这使得LyaDEQ模型能够抵抗微小的初始扰动。为了避免由于Lyapunov稳定的固定点彼此靠近而导致的不良对抗性防御，我们在Lyapunov稳定性模块之后加入了一个正交的全连接层，以分离不同的固定点。我们在各种基准上评估了LyaDEQ模型。

    Deep equilibrium (DEQ) models have emerged as a promising class of implicit layer models in deep learning, which abandon traditional depth by solving for the fixed points of a single nonlinear layer. Despite their success, the stability of the fixed points for these models remains poorly understood. Recently, Lyapunov theory has been applied to Neural ODEs, another type of implicit layer model, to confer adversarial robustness. By considering DEQ models as nonlinear dynamic systems, we propose a robust DEQ model named LyaDEQ with guaranteed provable stability via Lyapunov theory. The crux of our method is ensuring the fixed points of the DEQ models are Lyapunov stable, which enables the LyaDEQ models to resist the minor initial perturbations. To avoid poor adversarial defense due to Lyapunov-stable fixed points being located near each other, we add an orthogonal fully connected layer after the Lyapunov stability module to separate different fixed points. We evaluate LyaDEQ models on se
    
[^127]: MERMAIDE: 使用基于模型的元学习方法来学习对齐学习者

    MERMAIDE: Learning to Align Learners using Model-Based Meta-Learning. (arXiv:2304.04668v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.04668](http://arxiv.org/abs/2304.04668)

    MERMAIDE是一个基于模型的元学习框架，用于训练主体快速适应具有不同学习策略和奖励函数的超出分布代理，以实现理想结果，并且能够在少量样本中适应并减少干预次数。

    

    我们研究了一个主体如何高效有效地干预之前未见过的学习代理的奖励，以实现理想的结果。这对于许多现实世界的情景（如拍卖或税收）是相关的，因为主体可能不知道真实人的学习行为和奖励。此外，主体应该能够在少量样本中适应，并且尽量减少干预的次数，因为干预通常是昂贵的。我们引入了MERMAIDE，这是一个基于模型的元学习框架，用于训练一个能够快速适应具有不同学习策略和奖励函数的超出分布代理的主体。我们逐步验证了这种方法。首先，在具有最佳响应代理的斯塔克贝格设置中，我们展示了元学习在测试时能够快速收敛到理论已知的斯塔克贝格均衡，尽管噪声观测严重增加了样本复杂性。然后，我们展示了我们的基于模型的元学习方法成本效益高

    We study how a principal can efficiently and effectively intervene on the rewards of a previously unseen learning agent in order to induce desirable outcomes. This is relevant to many real-world settings like auctions or taxation, where the principal may not know the learning behavior nor the rewards of real people. Moreover, the principal should be few-shot adaptable and minimize the number of interventions, because interventions are often costly. We introduce MERMAIDE, a model-based meta-learning framework to train a principal that can quickly adapt to out-of-distribution agents with different learning strategies and reward functions. We validate this approach step-by-step. First, in a Stackelberg setting with a best-response agent, we show that meta-learning enables quick convergence to the theoretically known Stackelberg equilibrium at test time, although noisy observations severely increase the sample complexity. We then show that our model-based meta-learning approach is cost-eff
    
[^128]: 物理引导的对抗神经网络用于人造数字图像相关数据生成

    Physics-guided adversarial networks for artificial digital image correlation data generation. (arXiv:2303.15939v1 [eess.IV])

    [http://arxiv.org/abs/2303.15939](http://arxiv.org/abs/2303.15939)

    本文提出一种使用具有物理引导鉴别器的生成式对抗网络来生成人造DIC位移数据的方法， 以训练更精确可靠的机器学习模型，从而实现更准确可靠的疲劳裂纹增长评估的发展。

    

    数字图像相关（DIC）已成为评估力学实验的有价值工具，特别是疲劳裂纹增长实验。评估需要准确的裂纹路径和裂纹尖端位置信息，由于固有噪声和伪影的原因，这很难获得。机器学习模型在给定标记的DIC位移数据的情况下识别此相关信息非常成功。为了训练具有广泛泛化能力的强大模型，需要大数据。然而，由于实验昂贵且耗时，材料科学和工程领域的数据通常很少。 我们提出了一种使用具有物理引导鉴别器的生成式对抗网络来生成人造DIC位移数据的方法。为了决定数据样本是真实还是假的，该鉴别器另外接收导出的von Mises等效应变。我们显示，这种物理引导方法相比传统GAN产生了更准确和稳健的结果。我们的方法允许产生大量的人造DIC数据，以训练机器学习模型，从而实现更准确可靠的疲劳裂纹增长评估的发展。

    Digital image correlation (DIC) has become a valuable tool in the evaluation of mechanical experiments, particularly fatigue crack growth experiments. The evaluation requires accurate information of the crack path and crack tip position, which is difficult to obtain due to inherent noise and artefacts. Machine learning models have been extremely successful in recognizing this relevant information given labelled DIC displacement data. For the training of robust models, which generalize well, big data is needed. However, data is typically scarce in the field of material science and engineering because experiments are expensive and time-consuming. We present a method to generate synthetic DIC displacement data using generative adversarial networks with a physics-guided discriminator. To decide whether data samples are real or fake, this discriminator additionally receives the derived von Mises equivalent strain. We show that this physics-guided approach leads to improved results in terms 
    
[^129]: 闭环Koopman算子逼近法

    Closed-Loop Koopman Operator Approximation. (arXiv:2303.15318v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2303.15318](http://arxiv.org/abs/2303.15318)

    本文提出了一种闭环Koopman算子逼近法，通过利用Koopman算子的线性性质和对控制器和闭环系统结构的了解，可以同时识别闭环和装置系统。

    

    本文提出了一种方法，用于在已知控制器的情况下，识别反馈控制系统的Koopman模型。Koopman算子通过将非线性系统视为无限维线性系统，可以通过无限数量的映射函数进行重写。通过选择有限数量的映射函数并在映射空间中解决回归问题，可以识别出Koopman算子的有限维近似。现有方法旨在识别开环系统。然而，对于某些系统（如不稳定系统），以开环方式运行实验是不切实际或不可能的。所提出的方法利用Koopman算子的线性性质，结合控制器和闭环系统的结构，同时识别闭环和装置系统。通过实验证明了所提出的闭环Koopman算子逼近法的优势。

    This paper proposes a method to identify a Koopman model of a feedback-controlled system given a known controller. The Koopman operator allows a nonlinear system to be rewritten as an infinite-dimensional linear system by viewing it in terms of an infinite set of lifting functions. A finite-dimensional approximation of the Koopman operator can be identified from data by choosing a finite subset of lifting functions and solving a regression problem in the lifted space. Existing methods are designed to identify open-loop systems. However, it is impractical or impossible to run experiments on some systems, such as unstable systems, in an open-loop fashion. The proposed method leverages the linearity of the Koopman operator, along with knowledge of the controller and the structure of the closed-loop system, to simultaneously identify the closed-loop and plant systems. The advantages of the proposed closed-loop Koopman operator approximation method are demonstrated experimentally using a ro
    
[^130]: 异步分散的联邦终身学习在医学成像中的地标定位

    Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging. (arXiv:2303.06783v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06783](http://arxiv.org/abs/2303.06783)

    本文提出了一种异步分散的联邦终身学习 (ADFLL) 方法，通过同时训练多个任务，克服了传统联邦学习中的潜在问题，并在医学成像中的地标定位方面展现了出色的性能。

    

    联邦学习是机器学习领域的一个新发展，它允许设备的系统在一个或多个任务上进行训练，而无需将其数据共享到单个位置或设备。然而，该框架仍然需要一个集中的全局模型来将个体模型合并为一个，并且设备进行同步训练，这两者都可能成为使用联邦学习的潜在瓶颈。在本文中，我们提出了一种新颖的异步分散的联邦终身学习 (ADFLL) 方法，它继承了联邦学习的优点，可以同时在多个任务上进行训练，而无需中央节点或同步训练。从而克服了传统联邦学习的潜在缺点。我们在脑肿瘤分割 (BRATS) 数据集上展示了卓越的性能，用于定位多个图像序列和图像方向的左室。我们的框架允许代理商以平均距离达到最佳性能。

    Federated learning is a recent development in the machine learning area that allows a system of devices to train on one or more tasks without sharing their data to a single location or device. However, this framework still requires a centralized global model to consolidate individual models into one, and the devices train synchronously, which both can be potential bottlenecks for using federated learning. In this paper, we propose a novel method of asynchronous decentralized federated lifelong learning (ADFLL) method that inherits the merits of federated learning and can train on multiple tasks simultaneously without the need for a central node or synchronous training. Thus, overcoming the potential drawbacks of conventional federated learning. We demonstrate excellent performance on the brain tumor segmentation (BRATS) dataset for localizing the left ventricle on multiple image sequences and image orientation. Our framework allows agents to achieve the best performance with a mean dis
    
[^131]: 可变深度异构联邦学习的内存自适应模型

    Memory-adaptive Depth-wise Heterogenous Federated Learning. (arXiv:2303.04887v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04887](http://arxiv.org/abs/2303.04887)

    这项研究介绍了一种名为FeDepth的内存自适应深度学习解决方案，它根据每个客户端的内存预算将完整模型自适应地分解成块，并依次训练这些块，以解决联邦学习中异构设备的内存限制问题。

    

    联邦学习是一种有前途的范式，允许多个客户端在不共享本地数据的情况下协同训练模型。然而，在联邦学习中存在异构设备，如手机和物联网设备的内存能力不同，会限制模型能够训练的规模和性能。主要解决内存限制的方法集中在减少宽度的技术上，即不同客户端在本地训练减宽度的子网络，然后服务器聚合这些子网络。由于处理聚合阶段中不同子网络宽度变化的负面影响，这些方法产生的全局模型会受到性能的降低。在本文中，我们介绍了一种称为FeDepth的内存自适应深度学习解决方案，它根据每个客户端的内存预算将完整模型自适应地分解成块，并依次训练这些块，以获取更好的性能和可扩展性。

    Federated learning is a promising paradigm that allows multiple clients to collaboratively train a model without sharing the local data. However, the presence of heterogeneous devices in federated learning, such as mobile phones and IoT devices with varying memory capabilities, would limit the scale and hence the performance of the model could be trained. The mainstream approaches to address memory limitations focus on width-slimming techniques, where different clients train subnetworks with reduced widths locally and then the server aggregates the subnetworks. The global model produced from these methods suffers from performance degradation due to the negative impact of the actions taken to handle the varying subnetwork widths in the aggregation phase. In this paper, we introduce a memory-adaptive depth-wise learning solution in FL called FeDepth, which adaptively decomposes the full model into blocks according to the memory budgets of each client and trains blocks sequentially to obt
    
[^132]: 使用核心集的选择性经验回放压缩用于医学影像领域中的终身深度强化学习

    Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging. (arXiv:2302.11510v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11510](http://arxiv.org/abs/2302.11510)

    本文提出了一种使用核心集进行选择性经验回放压缩的技术，可以提升终身学习的效率，应用于医学影像领域。

    

    选择性经验回放是将深度强化学习与终身学习结合的一种流行策略。选择性经验回放旨在重述以前任务中的选择性经验，以避免灾难性遗忘。此外，基于选择性经验回放的技术是模型不可知的，并允许在不同模型之间共享经验。然而，存储所有以前任务的经验会使得使用选择性经验回放的终身学习变得计算上非常昂贵和不切实际，特别是随着任务数量的增加。

    Selective experience replay is a popular strategy for integrating lifelong learning with deep reinforcement learning. Selective experience replay aims to recount selected experiences from previous tasks to avoid catastrophic forgetting. Furthermore, selective experience replay based techniques are model agnostic and allow experiences to be shared across different models. However, storing experiences from all previous tasks make lifelong learning using selective experience replay computationally very expensive and impractical as the number of tasks increase. To that end, we propose a reward distribution-preserving coreset compression technique for compressing experience replay buffers stored for selective experience replay.  We evaluated the coreset compression technique on the brain tumor segmentation (BRATS) dataset for the task of ventricle localization and on the whole-body MRI for localization of left knee cap, left kidney, right trochanter, left lung, and spleen. The coreset lifel
    
[^133]: 非线性Ridge Bandits的统计复杂度和最优算法

    Statistical Complexity and Optimal Algorithms for Non-linear Ridge Bandits. (arXiv:2302.06025v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.06025](http://arxiv.org/abs/2302.06025)

    本文探讨了非线性Ridge Bandits中独特的学习现象，推导出了最优烧录成本的上下限和整个烧录期间的学习轨迹的统计算法，并证明了UCB和基于回归神经元的算法都是次优解。

    

    本文考虑了一种顺序决策问题，其中平均结果是所选择动作的非线性函数。与线性模型相比，非线性模型有两种奇特现象：首先，除了具有标准参数率的“学习阶段”以进行估计或后悔外，还有一个由非线性函数确定的固定成本的“烧录期”; 其次，实现最小烧录成本需要新的探索算法。针对一类名为ridge函数的特殊非线性函数，我们通过微分方程推导了最优烧录成本的上下限，此外还推导了整个烧录期间的学习轨迹的上下限。特别地，一种两阶段算法先找到一个好的初始行动，然后将问题视为局部线性，这是统计上最优的。相反，几种经典算法，例如UCB和依赖于回归神经元的算法，其可证明是次优的。

    We consider the sequential decision-making problem where the mean outcome is a non-linear function of the chosen action. Compared with the linear model, two curious phenomena arise in non-linear models: first, in addition to the "learning phase" with a standard parametric rate for estimation or regret, there is an "burn-in period" with a fixed cost determined by the non-linear function; second, achieving the smallest burn-in cost requires new exploration algorithms. For a special family of non-linear functions named ridge functions in the literature, we derive upper and lower bounds on the optimal burn-in cost, and in addition, on the entire learning trajectory during the burn-in period via differential equations. In particular, a two-stage algorithm that first finds a good initial action and then treats the problem as locally linear is statistically optimal. In contrast, several classical algorithms, such as UCB and algorithms relying on regression oracles, are provably suboptimal.
    
[^134]: 深度集成中的预测多样性病态

    Pathologies of Predictive Diversity in Deep Ensembles. (arXiv:2302.00704v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00704](http://arxiv.org/abs/2302.00704)

    本文发现在高容量的神经网络集成中，鼓励预测多样性并不总是有效的，甚至反而会损害性能。相反地，阻止预测多样性往往是无害的，这与先前的直觉相反。

    

    传统的结果表明，鼓励预测多样性可以提高低容量模型的集成性能，例如通过Bagging或Boosting。然而，在本文中，我们证明这些直觉在高容量的神经网络集成（深度集成）中并不适用，事实上，往往相反。通过对近600个神经网络分类集成的大规模研究，我们考察了一系列平衡组件模型性能与预测多样性之间关系的干预措施。虽然这样的干预措施可以改善小规模神经网络集成的性能（符合标准直觉），但它们却会损害在实践中最常用的大规模神经网络集成的性能。令人惊讶的是，在大型网络集成中，阻止预测多样性往往是无害的，完全颠覆了标准的直觉。即使多样性促进的干预措施不牺牲组件模型的性能（例如使用异构架构和训练策略）...

    Classic results establish that encouraging predictive diversity improves performance in ensembles of low-capacity models, e.g. through bagging or boosting. Here we demonstrate that these intuitions do not apply to high-capacity neural network ensembles (deep ensembles), and in fact the opposite is often true. In a large scale study of nearly 600 neural network classification ensembles, we examine a variety of interventions that trade off component model performance for predictive diversity. While such interventions can improve the performance of small neural network ensembles (in line with standard intuitions), they harm the performance of the large neural network ensembles most often used in practice. Surprisingly, we also find that discouraging predictive diversity is often benign in large-network ensembles, fully inverting standard intuitions. Even when diversity-promoting interventions do not sacrifice component model performance (e.g. using heterogeneous architectures and training
    
[^135]: 案例基础神经网络：具有时间变化的高阶交互的生存分析

    Case-Base Neural Networks: survival analysis with time-varying, higher-order interactions. (arXiv:2301.06535v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.06535](http://arxiv.org/abs/2301.06535)

    案例基础神经网络（CBNNs）是一种新的生存分析方法，它可以同时模拟时间变化的交互和复杂的基线风险。

    

    神经网络基于生存分析方法可以模拟数据驱动的协变量交互。虽然这些方法可以比回归方法提供更好的预测性能，但并不是所有的方法都可以模拟时间变化的交互和复杂的基线风险。为了解决这个问题，我们提出了一种称为案例基础神经网络（CBNNs）的新方法，它将案例基础抽样框架与灵活的神经网络结构相结合。通过使用一种新颖的抽样方案和数据增强来自然地考虑到截尾，我们构建了一个可以接受时间输入的前馈神经网络。CBNNs通过预测在给定时刻事件发生的概率来估计危险函数。我们通过模拟和三个案例研究使用两个时间依赖指标比较CBNNs与回归和神经网络基于生存分析方法的性能。首先，我们通过涉及复杂基线风险和时间变化交互的模拟来评估所有方法，其中包括CBNNs。

    Neural network-based survival methods can model data-driven covariate interactions. While these methods can provide better predictive performance than regression-based approaches, not all can model time-varying interactions and complex baseline hazards. To address this, we propose Case-Base Neural Networks (CBNNs) as a new approach that combines the case-base sampling framework with flexible neural network architectures. Using a novel sampling scheme and data augmentation to naturally account for censoring, we construct a feed-forward neural network that may take time as an input. CBNNs predict the probability of an event occurring at a given moment to estimate the hazard function. We compare the performance of CBNNs to regression and neural network-based survival methods in a simulation and three case studies using two time-dependent metrics. First, we examine performance on a simulation involving a complex baseline hazard and time-varying interactions to assess all methods, with CBNN
    
[^136]: SemPPL: 预测伪标签以改善对比表示

    SemPPL: Predicting pseudo-labels for better contrastive representations. (arXiv:2301.05158v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.05158](http://arxiv.org/abs/2301.05158)

    该论文提出了一种新的半监督学习方法SemPPL，通过预测伪标签来改善对比表示，从而解决了计算机视觉中学习大量无监督数据和少量监督数据的问题。

    

    从大量无监督数据和少量监督数据中学习是计算机视觉中一个重要的开放问题。我们提出了一种新的半监督学习方法，Semantic Positives via Pseudo-Labels (SemPPL)，它结合了有标签和无标签的数据来学习信息丰富的表示。我们的方法扩展了自监督对比学习，通过选择正样本的新方法，来区分两个样本是否代表相同的基础数据。为了丰富正样本集，我们利用少量已有的真实标签通过学习到的有标签数据的嵌入来预测缺失的标签，通过k最近邻分类器实现。我们将具有相同伪标签的数据点扩展为正样本，并称之为语义正样本。我们同时学习表示和预测自动增强的伪标签，形成一个循环。

    Learning from large amounts of unsupervised data and a small amount of supervision is an important open problem in computer vision. We propose a new semi-supervised learning method, Semantic Positives via Pseudo-Labels (SemPPL), that combines labelled and unlabelled data to learn informative representations. Our method extends self-supervised contrastive learning -where representations are shaped by distinguishing whether two samples represent the same underlying datum (positives) or not (negatives) -- with a novel approach to selecting positives. To enrich the set of positives, we leverage the few existing ground-truth labels to predict the missing ones through a $k$-nearest neighbours classifier by using the learned embeddings of the labelled data. We thus extend the set of positives with datapoints having the same pseudo-label and call these semantic positives. We jointly learn the representation and predict bootstrapped pseudo-labels. This creates a reinforcing cycle. Strong init
    
[^137]: t-SMILES：用于全新分子生成的可扩展基于碎片的分子表示框架

    t-SMILES: A Scalable Fragment-based Molecular Representation Framework for De Novo Molecule Generation. (arXiv:2301.01829v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01829](http://arxiv.org/abs/2301.01829)

    本研究提出了一种可扩展的基于碎片的分子表示框架 t-SMILES，通过引入 t-SMILES 可以显著改善分子的表示效果，并在多种任务中表现出色，优于其他经典模型。

    

    分子的有效表示是影响人工智能模型性能的关键因素。本研究引入了一种灵活的、基于碎片的多尺度分子表示框架 t-SMILES（基于树的SMILES），该框架包含三种代码算法：TSSA（带有共享原子的t-SMILES）、TSDY（带有虚拟原子的t-SMILES）和TSID（带有ID的t-SMILES）。它使用从分子图的碎片形成的全二叉树上进行广度优先搜索得到的SMILES类型字符串来描述分子。通过使用JTVAE、BRICS、MMPA和Scaffold进行系统评估，显示了构建多代码分子描述系统的可行性，各种描述相互补充，提高整体性能。此外，在资源有限的数据集上表现出色，无论模型是原始的、数据增强的还是预训练微调的。它在goa等任务中明显优于经典的SMILES、DeepSMILES、SELFIES和基准模型。

    Effective representation of molecules is a crucial factor affecting the performance of artificial intelligence models. This study introduces a flexible, fragment-based, multiscale molecular representation framework called t-SMILES (tree-based SMILES) with three code algorithms: TSSA (t-SMILES with Shared Atom), TSDY (t-SMILES with Dummy Atom) and TSID (t-SMILES with ID). It describes molecules using SMILES-type strings obtained by performing a breadth-first search on a full binary tree formed from a fragmented molecular graph. Systematic evaluations using JTVAE, BRICS, MMPA, and Scaffold show the feasibility to construct a multi-code molecular description system, where various descriptions complement each other, enhancing the overall performance. Additionally, it exhibits impressive performance on low-resource datasets, whether the model is original, data augmented, or pre-training fine-tuned. It significantly outperforms classical SMILES, DeepSMILES, SELFIES and baseline models in goa
    
[^138]: 具有低弯曲和低畸变的流形嵌入的收敛自动编码器逼近

    Convergent autoencoder approximation of low bending and low distortion manifold embeddings. (arXiv:2208.10193v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2208.10193](http://arxiv.org/abs/2208.10193)

    这项研究提出了一种新的正则化方法，用于学习自动编码器的编码器部分，该方法偏好等距、外在平坦嵌入，并允许单独对编码器进行训练。研究通过蒙特卡洛积分计算损失函数，使用局部Riemannian距离和局部Riemannian均值评估输入流形上的点对。

    

    自动编码器由编码器和解码器组成，在机器学习中广泛用于高维数据的降维。编码器将输入数据流形嵌入到较低维的潜在空间中，而解码器则表示反向映射，通过潜在空间中的流形给出数据流形的参数化。良好的嵌入流形的规则性和结构性可以极大地简化进一步的数据处理任务，如聚类分析或数据插值。我们提出并分析了一种新颖的正则化方法，用于学习自动编码器的编码器部分：一种偏好等距、外在平坦嵌入的损失函数，允许单独对编码器进行训练。为了进行训练，假设可以评估输入流形上附近点对的局部Riemannian距离和局部Riemannian均值。通过蒙特卡洛积分和不同的采样策略计算损失函数。

    Autoencoders, which consist of an encoder and a decoder, are widely used in machine learning for dimension reduction of high-dimensional data. The encoder embeds the input data manifold into a lower-dimensional latent space, while the decoder represents the inverse map, providing a parametrization of the data manifold by the manifold in latent space. A good regularity and structure of the embedded manifold may substantially simplify further data processing tasks such as cluster analysis or data interpolation. We propose and analyze a novel regularization for learning the encoder component of an autoencoder: a loss functional that prefers isometric, extrinsically flat embeddings and allows to train the encoder on its own. To perform the training it is assumed that for pairs of nearby points on the input manifold their local Riemannian distance and their local Riemannian average can be evaluated. The loss functional is computed via Monte Carlo integration with different sampling strategi
    
[^139]: GANDALF: 用于深度自动化特征学习的门控自适应网络

    GANDALF: Gated Adaptive Network for Deep Automated Learning of Features. (arXiv:2207.08548v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.08548](http://arxiv.org/abs/2207.08548)

    GANDALF是一种用于表格数据的高性能深度学习架构，具有解释性和计算效率高的特点。通过引入门控特征学习单元，GANDALF能够实现更好的性能，并且在多个公开基准测试中优于或与其他最先进的方法持平。

    

    我们提出了一种新颖的高性能、可解释、参数和计算效率高的深度学习架构，用于表格数据，称为GANDALF（Gated Adaptive Network for Deep Automated Learning of Features）。GANDALF依赖于一个具有门控机制和内置特征选择的新的表格处理单元，称为门控特征学习单元（GFLU），作为特征表示学习单元。通过对多个公开的基准测试进行实验证明，GANDALF在性能上优于或与XGBoost、SAINT、FT-Transformers等最先进的方法持平。我们已经将代码在github.com/manujosephv/pytorch_tabular上以MIT许可证的形式提供。

    We propose a novel high-performance, interpretable, and parameter \& computationally efficient deep learning architecture for tabular data, Gated Adaptive Network for Deep Automated Learning of Features (GANDALF). GANDALF relies on a new tabular processing unit with a gating mechanism and in-built feature selection called Gated Feature Learning Unit (GFLU) as a feature representation learning unit. We demonstrate that GANDALF outperforms or stays at-par with SOTA approaches like XGBoost, SAINT, FT-Transformers, etc. by experiments on multiple established public benchmarks. We have made available the code at github.com/manujosephv/pytorch_tabular under MIT License.
    
[^140]: 一种基于强化学习的资源受限无线网络控制系统中的感知设计方法

    A Reinforcement Learning Approach to Sensing Design in Resource-Constrained Wireless Networked Control Systems. (arXiv:2204.00703v5 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2204.00703](http://arxiv.org/abs/2204.00703)

    本文提出了一种基于强化学习的方法，用于在资源受限的无线网络控制系统中进行感知设计。通过在智能传感器间进行决策，可以在延迟和准确性之间进行权衡，并获得最优的感知设计策略。

    

    本文考虑了一种智能传感器（代理器件）组成的无线网络，用于监测动态过程并将测量结果发送给执行全局监测和决策的基站。智能传感器具备感知和计算能力，可以发送原始测量数据或在传输前进行处理。代理器件资源的限制导致了延迟和准确性之间的基本权衡。一方面，原始测量数据具有不准确但产生快的特点。另一方面，为了在资源受限平台上生成准确的测量数据，需要耗费相当的计算延迟。此外，如果处理后的数据也经过了压缩，则由于无线通信引起的延迟可能更高。因此，决定网络中的传感器何时何地传输原始测量数据或利用耗时的本地处理是具有挑战性的。为了解决这个设计问题，本文提出一种基于强化学习的方法来学习一个最优的感知设计策略。

    In this paper, we consider a wireless network of smart sensors (agents) that monitor a dynamical process and send measurements to a base station that performs global monitoring and decision-making. Smart sensors are equipped with both sensing and computation, and can either send raw measurements or process them prior to transmission. Constrained agent resources raise a fundamental latency-accuracy trade-off. On the one hand, raw measurements are inaccurate but fast to produce. On the other hand, data processing on resource-constrained platforms generates accurate measurements at the cost of non-negligible computation latency. Further, if processed data are also compressed, latency caused by wireless communication might be higher for raw measurements. Hence, it is challenging to decide when and where sensors in the network should transmit raw measurements or leverage time-consuming local processing. To tackle this design problem, we propose a Reinforcement Learning approach to learn an 
    
[^141]: 广义乐观方法用于凸凹鞍点问题的研究。

    Generalized Optimistic Methods for Convex-Concave Saddle Point Problems. (arXiv:2202.09674v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2202.09674](http://arxiv.org/abs/2202.09674)

    本文通过将乐观梯度方法解释为对邻近点法的近似，提出了一个广义乐观方法，可以处理具有复合目标函数的约束鞍点问题，并且可以使用Bregman距离处理任意范数。此外，我们还开发了一个回溯线搜索方案，以选择步长，而不需要了解平滑系数。我们的方法在使用一阶、二阶和更高阶数学规则时，给出了已知的全局迭代复杂度界限。

    

    乐观梯度方法在解决凸凹鞍点问题方面越来越受欢迎。为了分析其迭代复杂度，最近的一项工作提出了一个有趣的观点，将这个方法解释为对邻近点法的近似。在本文中，我们遵循这一方法，并将乐观的思想精炼为一个广义乐观方法，其中包括乐观梯度方法作为一种特殊情况。我们的通用框架可以处理具有复合目标函数的约束鞍点问题，并且可以使用Bregman距离处理任意范数。此外，我们开发了一个回溯线搜索方案，以选择步长，而不需要了解平滑系数。我们利用一阶、二阶和更高阶数学规则来实现我们的方法，并给出了已知的全局迭代复杂度界限。对于我们的一阶方法，我们证明了平均迭代在$O(1/N)$的速度下收敛。

    The optimistic gradient method has seen increasing popularity for solving convex-concave saddle point problems. To analyze its iteration complexity, a recent work [arXiv:1906.01115] proposed an interesting perspective that interprets this method as an approximation to the proximal point method. In this paper, we follow this approach and distill the underlying idea of optimism to propose a generalized optimistic method, which includes the optimistic gradient method as a special case. Our general framework can handle constrained saddle point problems with composite objective functions and can work with arbitrary norms using Bregman distances. Moreover, we develop a backtracking line search scheme to select the step sizes without knowledge of the smoothness coefficients. We instantiate our method with first-, second- and higher-order oracles and give best-known global iteration complexity bounds. For our first-order method, we show that the averaged iterates converge at a rate of $O(1/N)$
    
[^142]: 线性反向传播及其收敛性的理论视角

    A Theoretical View of Linear Backpropagation and Its Convergence. (arXiv:2112.11018v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.11018](http://arxiv.org/abs/2112.11018)

    本文从理论上分析了线性反向传播（LinBP）在神经网络相关的学习任务中的性质，在对抗攻击和模型训练中，LinBP在相同的超参数设置下能够实现更快的收敛。

    

    反向传播（BP）被广泛用于计算深度神经网络（DNN）中的梯度。它通常与随机梯度下降（SGD）或其变种一起使用，在许多机器学习任务中被视为事实上的选择，包括DNN训练和对抗攻击/防御。最近，Guo等人提出了一种名为LinBP的线性BP变体，用于生成更具传递性的对抗样本，以进行黑盒攻击。虽然经验证明在黑盒攻击中有效，但对这种方法的理论研究和收敛性分析较为缺乏。本文作为Guo等人论文的补充和扩展，提供了对LinBP在涉及神经网络的学习任务中的理论分析，包括对抗攻击和模型训练。我们展示了令人惊讶的是，在相同的超参数设置下，与BP相比，LinBP在这些任务中能够实现更快的收敛。我们验证了我们的理论。

    Backpropagation (BP) is widely used for calculating gradients in deep neural networks (DNNs). Applied often along with stochastic gradient descent (SGD) or its variants, BP is considered as a de-facto choice in a variety of machine learning tasks including DNN training and adversarial attack/defense. Recently, a linear variant of BP named LinBP was introduced for generating more transferable adversarial examples for performing black-box attacks, by Guo et al. Although it has been shown empirically effective in black-box attacks, theoretical studies and convergence analyses of such a method is lacking. This paper serves as a complement and somewhat an extension to Guo et al.'s paper, by providing theoretical analyses on LinBP in neural-network-involved learning tasks, including adversarial attack and model training. We demonstrate that, somewhat surprisingly, LinBP can lead to faster convergence in these tasks in the same hyper-parameter settings, compared to BP. We confirm our theoreti
    
[^143]: 自适应联合分布学习

    Adaptive joint distribution learning. (arXiv:2110.04829v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.04829](http://arxiv.org/abs/2110.04829)

    该论文提出了一种自适应联合分布学习的框架，可以从大量数据点中估计低维、归一化和正的Radon-Nikodym导数模型，并在不同学习问题上取得了良好的结果。

    

    我们开发了一个新的框架，用于将联合概率分布嵌入张量积再生核希尔伯特空间（RKHS）中。我们的框架可以容纳一个低维、归一化和正的Radon-Nikodym导数模型，该模型可以从多达数百万个数据点的样本大小中进行估计，减轻了RKHS建模的固有限制。我们的方法自然产生了定义良好的归一化和正的条件分布。嵌入计算速度快且适用于从预测到分类的各种学习问题。我们的理论结果得到了有益的数值结果的支持。

    We develop a new framework for embedding joint probability distributions in tensor product reproducing kernel Hilbert spaces (RKHS). Our framework accommodates a low-dimensional, normalized and positive model of a Radon-Nikodym derivative, which we estimate from sample sizes of up to several million data points, alleviating the inherent limitations of RKHS modeling. Well-defined normalized and positive conditional distributions are natural by-products to our approach. The embedding is fast to compute and accommodates learning problems ranging from prediction to classification. Our theoretical findings are supplemented by favorable numerical results.
    
[^144]: 分层相关聚类和维持树结构嵌入

    Hierarchical Correlation Clustering and Tree Preserving Embedding. (arXiv:2002.07756v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2002.07756](http://arxiv.org/abs/2002.07756)

    本文提出了一种分层相关聚类方法，可应用于正负配对不相似度，并研究了使用此方法进行无监督表征学习的方法。

    

    我们提出了一种分层相关聚类方法，扩展了著名的相关聚类方法，可以产生适用于正负配对不相似度的分层聚类。接下来，我们研究了使用这种分层相关聚类的无监督表征学习。为此，我们首先研究将相应的分层嵌入用于维持树结构嵌入和特征提取。然后，我们研究了最小最大距离度量扩展到相关聚类的方法，作为另一种表征学习范式。最后，我们在多个数据集上展示了我们方法的性能。

    We propose a hierarchical correlation clustering method that extends the well-known correlation clustering to produce hierarchical clusters applicable to both positive and negative pairwise dissimilarities. Then, in the following, we study unsupervised representation learning with such hierarchical correlation clustering. For this purpose, we first investigate embedding the respective hierarchy to be used for tree-preserving embedding and feature extraction. Thereafter, we study the extension of minimax distance measures to correlation clustering, as another representation learning paradigm. Finally, we demonstrate the performance of our methods on several datasets.
    

