# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Anomaly Detection in Satellite Videos using Diffusion Models.](http://arxiv.org/abs/2306.05376) | 本文提出基于扩散模型的卫星视频异常检测方法，效果优于其他方法，能够快速捕捉移动异常，具有实时检测灾害的应用前景。 |
| [^2] | [Sequential Graph Neural Networks for Source Code Vulnerability Identification.](http://arxiv.org/abs/2306.05375) | 本文提出了一个经过适当策划的 C/C++ 源代码漏洞数据集，以帮助开发模型，同时还提出了一种基于图神经网络的学习框架，称为 SEquential Graph Neural Network（SEGNN），用于学习大量代码语义表示。在公开数据集上的实验结果表明，SEGNN 方法在准确性和 F1得分方面优于现有最先进的方法。 |
| [^3] | [A Computational Analysis of Oral Argument in the Supreme Court.](http://arxiv.org/abs/2306.05373) | 本文提出了一个计算分析的方法来研究最高法院的口头辩论。结果发现，口头辩论是一个高度结构化和战略性的仪式，律师使用各种修辞策略来推进他们的案件，而法官则通过提问和评论不仅塑造他们面前的案件，而且塑造了法院更广泛的法律原则。 |
| [^4] | [Ordinal Potential-based Player Rating.](http://arxiv.org/abs/2306.05366) | 该论文提出了一种使用序数势函数的玩家评级方法，通过可逆映射的嵌套计算，能够保持传递性。 |
| [^5] | [Subject clustering by IF-PCA and several recent methods.](http://arxiv.org/abs/2306.05363) | 本文结合了无监督深度学习方法和有影响特征PCA方法，提出了IF-VAE作为一种新的主题聚类方法，该方法在基因芯片和单细胞RNA-seq数据集方面的表现较好。 |
| [^6] | [Trustworthy Sensor Fusion against Inaudible Command Attacks in Advanced Driver-Assistance System.](http://arxiv.org/abs/2306.05358) | 本文提出了多模态融合框架（MFF），利用VGG系列神经网络实现异构音频-视觉模态的融合。在实验中，MFF实现了92.25%的检测准确率，并证明MFF在不同模型不确定性条件下具有可靠性。 |
| [^7] | [Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models.](http://arxiv.org/abs/2306.05357) | 本论文提出了一种无监督的方法，用于从图像中自动地发现不同的生成概念，并且这些生成概念可以被用于重新组合和生成新的艺术和混合图像，并作为一种表示用于下游的分类任务。 |
| [^8] | [A Crystal-Specific Pre-Training Framework for Crystal Material Property Prediction.](http://arxiv.org/abs/2306.05344) | 本论文提出了一种基于自监督学习的晶体特异性预训练框架，该框架设计了一种互斥掩码策略，缓解了晶体物性预测中存在的标记受限问题，并增强了表示学习。同时，该框架还考虑了晶体结构中的周期性不变性，开发了周期性不变的多图模块和周期特性学习。 |
| [^9] | [Federated Learning under Covariate Shifts with Generalization Guarantees.](http://arxiv.org/abs/2306.05325) | 本文提出了具有泛化保证的联邦学习方法FTW-ERM，在处理协变量转移方面表现出优势。 |
| [^10] | [Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application.](http://arxiv.org/abs/2306.05323) | 该研究创建了意大利神经精神命名实体识别数据集，并使用巨型语言模型开发出多中心识别模型，整体 F1得分为84.77%。该模型将帮助临床从业者从非结构化的医疗记录中自动提取信息。 |
| [^11] | [Real-time whole-heart electromechanical simulations using Latent Neural Ordinary Differential Equations.](http://arxiv.org/abs/2306.05321) | 该论文使用潜变量神经常微分方程(LNODEs)实现了实时全心脏电机仿真，并开发出了一个代理模型，可快速预测对不同肌力干预的压力-容积曲线，为患者特异性诊断和治疗优化提供了潜在快速工具。 |
| [^12] | [RNN-Based GNSS Positioning using Satellite Measurement Features and Pseudorange Residuals.](http://arxiv.org/abs/2306.05319) | 本文利用循环神经网络和自定义矩阵处理卫星测量特征和伪距残差，以提高全球导航卫星系统(GNSS)的定位精度 |
| [^13] | [A framework for dynamically training and adapting deep reinforcement learning models to different, low-compute, and continuously changing radiology deployment environments.](http://arxiv.org/abs/2306.05310) | 本研究开发了三种图像核心算法，以对选择性经验回放强化学习的医学图像进行压缩和去噪，以适应不断变化的计算和成像环境。最大熵核心算法以较低压缩比获得最佳表现。 |
| [^14] | [Are fairness metric scores enough to assess discrimination biases in machine learning?.](http://arxiv.org/abs/2306.05307) | 本文在机器学习算法在文本数据上对性别歧视的偏见的度量标准的局限性上进行了实验研究。 |
| [^15] | [Bayesian Optimisation of Functions on Graphs.](http://arxiv.org/abs/2306.05304) | 本论文提出了一种在通用的大规模和潜在未知图上定义函数的贝叶斯优化算法，并通过学习适当的图内核，适应目标函数行为。 |
| [^16] | [Correlated Noise in Epoch-Based Stochastic Gradient Descent: Implications for Weight Variances.](http://arxiv.org/abs/2306.05300) | 研究挑战了在时间上是不相关的假设，并强调了epoch-based噪声相关性对离散时间带动量的SGD的权重方差的影响。 |
| [^17] | [Deep Learning with Partially Labeled Data for Radio Map Reconstruction.](http://arxiv.org/abs/2306.05294) | 本篇论文解决了利用侧面信息重建接收信号强度地图的问题，采用部分标记数据的深度学习。使用额外的侧面信息可以提高在三个主要城市数据集中重建结果的准确性。 |
| [^18] | [Safe Collaborative Filtering.](http://arxiv.org/abs/2306.05292) | 本论文提出了一个安全的协同过滤算法，通过最小化条件风险价值，提高低满意度用户的推荐质量。在实际数据集中表现出色，同时也保持总体推荐质量。 |
| [^19] | [One shot learning based drivers head movement identification using a millimetre wave radar sensor.](http://arxiv.org/abs/2306.05291) | 通过使用小型毫米波雷达传感器从司机的头部动作中收集信号，并基于一次学习技术设计分类器，可以高效地识别司机的不同动作类型，实验准确率超过95%。 |
| [^20] | [Predictive and diagnosis models of stroke from hemodynamic signal monitoring.](http://arxiv.org/abs/2306.05289) | 本文基于血流动力学数据开发了实时诊断和预测模型，能够准确诊断中风亚型，预测患者的死亡率和中风复发。 |
| [^21] | [JGAT: a joint spatio-temporal graph attention model for brain decoding.](http://arxiv.org/abs/2306.05286) | 本文提出了一种联合时空图形注意力网络框架JGAT，集成了来自功能磁共振成像（fMRI）和扩散加权成像(DWI)的多模态数据，并保留动态信息，可用于大脑解码任务。 |
| [^22] | [Unsupervised Statistical Feature-Guided Diffusion Model for Sensor-based Human Activity Recognition.](http://arxiv.org/abs/2306.05285) | 本文提出了一种基于非监督统计特征引导扩散模型的传感器人体活动识别方法，通过生成多样化和代表性的合成传感器数据，从而解决了真实世界传感器数据的稀缺性和注释困难性，并在实验中获得了良好的性能表现。 |
| [^23] | [Simple and Controllable Music Generation.](http://arxiv.org/abs/2306.05284) | 本文提出了 MusicGen，一个单一的语言模型，可以在条件描述或旋律特征控制下生成高质量的样本，并且在标准的文本到音乐基准上的实证研究中，该方法优于其他基线模型。 |
| [^24] | [A Method for Detecting Murmurous Heart Sounds based on Self-similar Properties.](http://arxiv.org/abs/2306.05283) | 该研究提出了一种基于心音自相似和复杂特性的判别性多尺度特征集，以小波域中的心音信号为基础。当应用于一个公开的心音数据集时，我们提出的基于小波的多尺度特征表现出与现有方法相当的性能。 |
| [^25] | [Federated Linear Contextual Bandits with User-level Differential Privacy.](http://arxiv.org/abs/2306.05275) | 本文研究了带有用户级差分隐私的联邦线性上下文强化学习模型，为CDP提出了近乎最优的联邦算法\robin，在LDP下证明了学习必须承受至少一个遗憾膨胀因子。 |
| [^26] | [Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models.](http://arxiv.org/abs/2306.05272) | 本论文提出了一种新的图像聚类流程，利用大型预训练模型的强大特征表示，在规模上有效地对图像进行聚类，并通过优化速率降低目标和 CLIP 的图像-文本绑定，成功地提高了聚类的准确性和自标记算法的效果。 |
| [^27] | [Factorized Contrastive Learning: Going Beyond Multi-view Redundancy.](http://arxiv.org/abs/2306.05268) | 本论文提出了FactorCL，一种新的多模态表示学习方法，不仅考虑跨模态共享信息，还能捕捉跨模态唯一的任务相关信息。 |
| [^28] | [Representing and Learning Functions Invariant Under Crystallographic Groups.](http://arxiv.org/abs/2306.05261) | 本研究提出了线性和非线性表达形式，用于描述具有晶体对称性的函数。线性表达形式提供了晶体对称性下的基函数，而非线性表达将轨道空间嵌入到有限维欧几里得空间中进行描述。 |
| [^29] | [Comprehensive evaluation of deep and graph learning on drug-drug interactions prediction.](http://arxiv.org/abs/2306.05257) | 本论文综合评估了使用深度学习和图学习在药物相互作用预测方面的可行性，并总结了基于化学结构、基于网络、基于自然语言处理和混合方法，提供了一个易于访问的指南。 |
| [^30] | [Unscented Autoencoder.](http://arxiv.org/abs/2306.05256) | 本文提出了一种新的自编码器模型，名为非线性无损自编码器（UAE），它使用确定性采样的有限一组统计量来提高后验表示，从而获得更高质量的重建。同时，本文使用Wasserstein分布度量替换KL散度，实现更尖锐的后验。 |
| [^31] | [Toward more accurate and generalizable brain deformation estimators for traumatic brain injury detection with unsupervised domain adaptation.](http://arxiv.org/abs/2306.05255) | 本研究提出了一种脑部变形估计器，该估计器结合了非监督领域适应和深度神经网络的方法，可以更准确、更通用地预测脑最大主应变和MPS速率。该模型显著提高了预测精度，对早期TBI检测有重要意义。 |
| [^32] | [Matching Latent Encoding for Audio-Text based Keyword Spotting.](http://arxiv.org/abs/2306.05245) | 本文提出了一种基于音频文本的端到端模型来进行灵活的关键词检测，使用一种新型的动态规划算法来将音频序列最优地分割成与基于单词的文本序列相同的长度，以实现不同长度多词关键词的语义对齐。 |
| [^33] | [Ownership Protection of Generative Adversarial Networks.](http://arxiv.org/abs/2306.05233) | 本文提出一种新型的GAN拥有权保护方法，它基于目标模型及其盗版模型共同特征，能够直接适用于所有训练良好的GAN，最终实验结果表明其具有最佳的保护性能。 |
| [^34] | [Boosting Adversarial Transferability by Achieving Flat Local Maxima.](http://arxiv.org/abs/2306.05225) | 本文提出了一种近似优化方法来实现平坦局部极小值，该方法可以提高对抗性转移能力，并在实验中证实了该方法的有效性。 |
| [^35] | [Boosting-based Construction of BDDs for Linear Threshold Functions and Its Application to Verification of Neural Networks.](http://arxiv.org/abs/2306.05211) | 本文提出了一种基于Boosting的新方法，用于构造任何线性阈值函数的有序BDD，并可以在O(n2^{n})的时间复杂度内实现。该方法已应用于神经网络对抗攻击的鲁棒性验证中。 |
| [^36] | [PriSampler: Mitigating Property Inference of Diffusion Models.](http://arxiv.org/abs/2306.05208) | 本文是第一项针对扩散模型属性推断攻击的隐私研究，攻击者将从模型中提取训练集的敏感全局属性，结果表明各种扩散模型及其取样器容易受到攻击的影响。 |
| [^37] | [EMO: Episodic Memory Optimization for Few-Shot Meta-Learning.](http://arxiv.org/abs/2306.05189) | EMO是一种元学习的情节记忆优化方案，通过在外部存储器中记录过去任务的梯度历史，实现小样本学习，无论提供的梯度信息是否可靠，都可以推动参数更新朝着正确的方向前进。 |
| [^38] | [On the Identification and Optimization of Nonsmooth Superposition Operators in Semilinear Elliptic PDEs.](http://arxiv.org/abs/2306.05185) | 本文研究了在低正则性情况下，如何识别半线性椭圆PDE中的Nemytskii算子以及如何解决这个优化问题。这对于进行有关PDE的神经网络训练问题有很好的启示作用。 |
| [^39] | [Improving Long Context Document-Level Machine Translation.](http://arxiv.org/abs/2306.05183) | 该论文提出了一种新的受限注意力机制来提高长篇文本机器翻译的质量。 |
| [^40] | [Interactive Fashion Content Generation Using LLMs and Latent Diffusion Models.](http://arxiv.org/abs/2306.05182) | 该论文提出了一种利用LLMs和潜在扩散模型生成时尚内容的方法，通过简单的提示进行互动并生成时尚图像，从而帮助时尚设计师进行实时可视化和进一步改进，这种方法能够捕捉数据的基本结构。 |
| [^41] | [Large-scale Dataset Pruning with Dynamic Uncertainty.](http://arxiv.org/abs/2306.05175) | 本文提出了一种探索预测不确定性和训练动态的简单且有效的大规模数据集修剪方法，以产生信息量丰富的子集用于训练深度模型，实验结果表明优于现有技术，在ImageNet-1K和ImageNet-21K上实现了75％的无损压缩比。 |
| [^42] | [FLEdge: Benchmarking Federated Machine Learning Applications in Edge Computing Systems.](http://arxiv.org/abs/2306.05172) | FLEdge是一个面向边缘计算系统中FL工作量的基准测试，通过研究硬件异构性、能量效率和隐私级别对FL系统训练的影响，以及客户端退出对最新FL策略的影响，提供了训练最先进的FL工作负载的新见解。 |
| [^43] | [Decision S4: Efficient Sequence-Based RL via State Spaces Layers.](http://arxiv.org/abs/2306.05167) | 本论文提出了基于状态空间层的高效序列强化学习方法，包含一种离线和一种在线训练过程，可以从长程依赖中受益，取得了较好的效果。 |
| [^44] | [Bayesian Optimization of Expensive Nested Grey-Box Functions.](http://arxiv.org/abs/2306.05150) | 本文提出基于乐观主义的算法来解决嵌套黑白箱函数优化问题，相比传统黑箱优化方法显著提高全局最优解速度。 |
| [^45] | [Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean.](http://arxiv.org/abs/2306.05144) | Mesogeos是一个地中海地区的大规模多用途数据集，用于数据驱动的野火建模。它集成了历史野火记录和野火驱动因素，具有机器学习的潜力，提供了短期野火危险预测和最终烧毁区域估计两个可用于演示潜力的数据集。 |
| [^46] | [Genomic Interpreter: A Hierarchical Genomic Deep Neural Network with 1D Shifted Window Transformer.](http://arxiv.org/abs/2306.05143) | 本文提出了一种名为“基因组解释器”的模型，可以对基因组数据进行预测并发现基因调控的层次依赖关系，性能优于现有模型。 |
| [^47] | [Conformal Prediction for Federated Uncertainty Quantification Under Label Shift.](http://arxiv.org/abs/2306.05131) | 该论文提出了一种基于分位数回归的新型联邦合规性预测方法，该方法利用重要性加权有效地解决了代理之间的标签漂移问题，并为预测集的有效覆盖和差分隐私提供了理论保证。广泛的实验结果表明，该方法优于目前的竞争对手。 |
| [^48] | [A Meta-Generation framework for Industrial System Generation.](http://arxiv.org/abs/2306.05123) | 该论文提出了一个基于实际工业系统特征的使用案例，可作为基准用于评估和比较不同的深层生成模型架构。作者还提出了一种Meta-VAE，可以准确生成具有潜在设计约束的多组分工业系统。 |
| [^49] | [On Search Strategies for Document-Level Neural Machine Translation.](http://arxiv.org/abs/2306.05116) | 本文探讨了如何在搜索过程中最好地利用文档级神经机器翻译模型，比较了文献中的不同解码方案和作者提出的方案，并发现针对某些语言现象时，常用的解码策略不能够取得较好的性能。 |
| [^50] | [Hybrid Graph: A Unified Graph Representation with Datasets and Benchmarks for Complex Graphs.](http://arxiv.org/abs/2306.05108) | 本论文介绍了混合图的概念及其在高阶图建模中的应用，同时提出了混合图数据集及全面的评估框架，这为图神经网络在复杂图上的性能提供了全面的解决方案。 |
| [^51] | [Sy-CON: Symmetric Contrastive Loss for Continual Self-Supervised Representation Learning.](http://arxiv.org/abs/2306.05101) | Sy-CON是一种新的损失函数，用于持续自监督学习，它由两个损失组成，可以自然地找到良好的可塑性和稳定性之间的权衡，超过了现有持续学习方法。 |
| [^52] | [Re-aligning Shadow Models can Improve White-box Membership Inference Attacks.](http://arxiv.org/abs/2306.05093) | 系统分析了影子模型不对齐问题的原因，并通过对抗性训练或实例加权方法重新对齐影子模型，从而提高了白盒成员隐私攻击的效果。 |
| [^53] | [The ART of Conversation: Measuring Phonetic Convergence and Deliberate Imitation in L2-Speech with a Siamese RNN.](http://arxiv.org/abs/2306.05088) | 本文提出了一种孪生RNN结构用于测量L2-L2交互中语音音质收敛。该模型可以有效捕捉语音收敛和说话者的模仿能力的动态，并能够处理由L1引起的说话者差异。 |
| [^54] | [The Importance of Time in Causal Algorithmic Recourse.](http://arxiv.org/abs/2306.05082) | 因果算法补救需要纳入时间维度，以提高推荐的合理性和可靠性。 |
| [^55] | [Enhancing Robustness of AI Offensive Code Generators via Data Augmentation.](http://arxiv.org/abs/2306.05079) | 本论文提出了一种方法，通过在代码描述中引入扰动来增强AI攻击性代码生成器的鲁棒性，并证明数据增强可有效提高代码生成器对扰动和非扰动的代码描述的性能。 |
| [^56] | [A Causal Framework for Decomposing Spurious Variations.](http://arxiv.org/abs/2306.05071) | 本论文提出了一种因果框架，用于分解伪变异，包括虚假性图表和理论框架，能够在真假相关关系上帮助区分直接效应和间接效应。 |
| [^57] | [Shedding light on underrepresentation and Sampling Bias in machine learning.](http://arxiv.org/abs/2306.05068) | 文章讨论了机器学习中的偏见对公正性的影响，提出抽样偏差的变体：样本量偏差和代表性不足偏差，揭示歧视可以分解为方差、偏差和噪声，并挑战了通常被接受的缓解方法。 |
| [^58] | [Improving Visual Prompt Tuning for Self-supervised Vision Transformers.](http://arxiv.org/abs/2306.05067) | 本文研究了自监督视觉Transformer中视觉提示调整方法的提升，并确定了提示记号插入后续图块而非第一个图块的最佳位置。提出的方法能有效地减少确定最佳提示记号块位置的成本。 |
| [^59] | [Causal Fairness for Outcome Control.](http://arxiv.org/abs/2306.05066) | 本论文研究了一种特定的决策任务叫做结果控制，针对涉及到刑事司法、福利、临床决策以及公共卫生等多个方面，提出了因果公平的概念以及一组因果工具和技术来推断结果控制中的公平性。 |
| [^60] | [Precision-aware Latency and Energy Balancing on Multi-Accelerator Platforms for DNN Inference.](http://arxiv.org/abs/2306.05060) | 介绍了ODiMO，一种硬件感知的工具，通过精细映射将DNN分割并在不同加速器上并行执行，以在维持准确性的前提下降低推断的能耗和延迟。 |
| [^61] | [Neuro-Symbolic Approaches for Context-Aware Human Activity Recognition.](http://arxiv.org/abs/2306.05058) | 本文提出了一种采用语义损失函数的新方法，避免在分类过程中使用符号推理，可以解决现有上下文感知HAR的NeSy方法在部署和泛化能力方面的局限性。 |
| [^62] | [Magnitude Attention-based Dynamic Pruning.](http://arxiv.org/abs/2306.05056) | 本文提出一种基于幅值注意力的动态剪枝方法(MAP)，这种方法通过在前向和反向路径中应用权重的重要性动态地探索稀疏模型结构，从而实现了从冗余到有效的稀疏网络的无缝转换，得到了既具有高性能又经过精简的模型。 |
| [^63] | [A Gradient-based Approach for Online Robust Deep Neural Network Training with Noisy Labels.](http://arxiv.org/abs/2306.05046) | 本文提出了一种基于梯度的OGRS方法，用于在数据流场景下在线学习深度神经网络并处理噪声标签问题，能够自动选择不同干净比率的数据样本，无需改变参数设置。 |
| [^64] | [Non-autoregressive Conditional Diffusion Models for Time Series Prediction.](http://arxiv.org/abs/2306.05043) | 本文提出了一种名为TimeDiff的非自回归扩散模型来实现高质量时间序列预测，通过引入两种新颖的条件机制 - 未来混合和自回归初始化。实验结果表明，TimeDiff在各种强基线模型中取得了最佳整体性能。 |
| [^65] | [Energy-Efficient Downlink Semantic Generative Communication with Text-to-Image Generators.](http://arxiv.org/abs/2306.05041) | 本论文提出了一种语义生成通信框架，在该框架中，生成型用户利用文本到图像生成器来创建本地图像，从而减少基站下行传输能量消耗，但会增加用户能源消耗。通过生成用户选择算法，总能耗可以降低高达54%。 |
| [^66] | [Does Long-Term Series Forecasting Need Complex Attention and Extra Long Inputs?.](http://arxiv.org/abs/2306.05035) | 本论文介绍了一种新的轻量级周期-注意机制，名为Periodformer，解决了长期序列预测中的两个主要问题，并证明了Transformer-based方法不需要额外长的输入序列来保证性能。 |
| [^67] | [Scalable and Adaptive Log-based Anomaly Detection with Expert in the Loop.](http://arxiv.org/abs/2306.05032) | 本文提出了一种适用于大规模云系统的准确、轻量级、自适应的基于日志的异常检测框架——SeaLog。该方法利用一种基于 Trie 结构的动态增长检测代理，可以接收人类专家反馈，能够在不断变化的日志数据中实现高准确度的实时异常检测，从而减少了手动验证的工作量。 |
| [^68] | [Generalizable Lightweight Proxy for Robust NAS against Diverse Perturbations.](http://arxiv.org/abs/2306.05031) | 本研究提出了一种轻量级的、零成本代理，通过考虑干净图像和受扰动图像的特征、参数和梯度的一致性，实现了高效、快速地搜索鲁棒NAS架构。 |
| [^69] | [Multi-level Multiple Instance Learning with Transformer for Whole Slide Image Classification.](http://arxiv.org/abs/2306.05029) | 本文提出了一种基于Transformer的多级多示例学习（MMIL-Transformer）方法，该方法能够有效地处理涉及大量实例的MIL任务。 |
| [^70] | [Posterior Collapse in Linear Conditional and Hierarchical Variational Autoencoders.](http://arxiv.org/abs/2306.05023) | 本文研究了高度相似的变分后验分布和先验分布之间的后验崩溃现象，特别地，通过对线性条件VAE和分层VAE进行分析，证明了这种现象是由于潜在变量层次关系不清晰而引起的。 |
| [^71] | [Mixed-TD: Efficient Neural Network Accelerator with Layer-Specific Tensor Decomposition.](http://arxiv.org/abs/2306.05021) | 本文提出了一种基于层特定张量分解的神经网络加速器Mixed-TD，采用混合方式的SVD和CPD方法，实现了高压缩率，同时保持了与原始神经网络类似的准确性，并通过动态映射方法实现了对可用片上存储器和计算资源的有效利用。 |
| [^72] | [Non-Intrusive Load Monitoring (NILM) using Deep Neural Networks: A Review.](http://arxiv.org/abs/2306.05017) | 本文综述了基于深度学习算法的最新NILM方法，比较了它们的性能表现，为需求侧管理、能耗监测提供了一种精确的非侵入式负载监测方法。 |
| [^73] | [Learning Closed-form Equations for Subgrid-scale Closures from High-fidelity Data: Promises and Challenges.](http://arxiv.org/abs/2306.05014) | 本文发现了非线性梯度模型（NGM），它是可解析地使用Taylor级数拓展导出的闭合形式，从而实现对地球系统复杂过程的子网格尺度（SGS）闭合/参数化。 |
| [^74] | [Attention Weighted Mixture of Experts with Contrastive Learning for Personalized Ranking in E-commerce.](http://arxiv.org/abs/2306.05011) | 电子商务个性化排名专家混合模型，利用注意力机制和MoE框架进行特征交互建模，采用对比学习提升历史行为较少的长尾用户个性化排名结果。 |
| [^75] | [COURIER: Contrastive User Intention Reconstruction for Large-Scale Pre-Train of Image Features.](http://arxiv.org/abs/2306.05001) | 该论文提出了一种基于对比学习的推荐方法，以获得更好的个性化推荐表现。 |
| [^76] | [Ambulance Demand Prediction via Convolutional Neural Networks.](http://arxiv.org/abs/2306.04994) | 本研究提出一种新的卷积神经网络(CNN)体系结构，通过将时间序列数据转化为热力图，预测救护车需求。同时采用特征选择和超参数优化框架，结合历史救护车需求和外部信息，如天气、事件、节日和时间。 |
| [^77] | [Beyond Probability Partitions: Calibrating Neural Networks with Semantic Aware Grouping.](http://arxiv.org/abs/2306.04985) | 这篇论文提出了一种更为普适的校准误差定义——分区校准误差（PCE），指出了分区划分是各种校准误差指标之间的关键区别。作者提出了一个命题：准确的模型应该在任何分区上都具有校准性，而不仅仅是预测概率分区。通过语义相关的分区函数，作者证明了分区函数的粒度与模型准确性和校准之间的关系。 |
| [^78] | [G$^2$uardFL: Safeguarding Federated Learning Against Backdoor Attacks through Attributed Client Graph Clustering.](http://arxiv.org/abs/2306.04984) | 本论文提出了G$^2$uardFL，这是一个基于属性化客户端图聚类的联邦学习保护框架，能够有效识别恶意客户端，即使恶意客户端数量高达50％。 |
| [^79] | [CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification.](http://arxiv.org/abs/2306.04979) | CoCo是一种耦合对比图表示学习框架，其中包含一个图卷积网络和一个分层图内核网络，通过耦合对比学习减少领域差异，用于无监督领域自适应图分类。 |
| [^80] | [Conservative Prediction via Data-Driven Confidence Minimization.](http://arxiv.org/abs/2306.04974) | 该论文提出了一种可以在处理不常见样本时推迟到人类判断的保守模型方法。该方法使用基于数据驱动置信度最小化（DCM）的算法，在辅助数据集中选择感兴趣的OOD（Out-of-Distribution）区域的样本，进而实现可靠地分离ID（In-Distribution）和OOD输入。 |
| [^81] | [A Melting Pot of Evolution and Learning.](http://arxiv.org/abs/2306.04971) | 这篇综述介绍了我们小组在进化算法与机器学习、深度学习的融合方面的八个重要作品。 |
| [^82] | [Leveraging Language Identification to Enhance Code-Mixed Text Classification.](http://arxiv.org/abs/2306.04964) | 本研究提出了一种改进代码混合文本分类的流程，包括数据预处理、词级语言识别、语言增强和模型训练等步骤，用于下游任务，如情感分析。我们探索了词级交错和句子后置的语言信息插入方法，以提高基于BERT的模型在低资源代码混合印地语-英语数据集上的性能。 |
| [^83] | [arXiv4TGC: Large-Scale Datasets for Temporal Graph Clustering.](http://arxiv.org/abs/2306.04962) | arXiv4TGC提供了一组适用于大规模时态图聚类的新颖学术数据集，解决了缺乏可靠数据集来评估聚类性能的挑战。 |
| [^84] | [Recovering Simultaneously Structured Data via Non-Convex Iteratively Reweighted Least Squares.](http://arxiv.org/abs/2306.04961) | 该论文提出了一种非凸迭代加权最小二乘法用于同时恢复行稀疏和低秩的数据矩阵，能够在最小样本复杂度的情况下局部二次收敛到同时结构化的数据矩阵，并在实验中表现出有利的经验收敛性。 |
| [^85] | [Adaptive Fake Audio Detection with Low-Rank Model Squeezing.](http://arxiv.org/abs/2306.04956) | 本文提出了一种适应新出现假音频类型的低秩适应矩阵训练方法，有效地缓解了传统微调方法的局限性，并能保留现有模型对已知假音频类型的预测准确性。 |
| [^86] | [Degraded Polygons Raise Fundamental Questions of Neural Network Perception.](http://arxiv.org/abs/2306.04955) | 本文研究了神经网络在识别具有不同程度边缘降解的规则多边形时的性能和行为，发现存在基本问题，揭示了人机视觉差距的另一个角度。 |
| [^87] | [Entropy-based Training Methods for Scalable Neural Implicit Sampler.](http://arxiv.org/abs/2306.04952) | 本文提出了一种高效且可扩展的神经隐式采样器，并引入了KL训练法和Fisher训练法来训练它，实现了低计算成本下生成大批量样本。 |
| [^88] | [Robust Learning with Progressive Data Expansion Against Spurious Correlation.](http://arxiv.org/abs/2306.04949) | 本文通过理论分析和实验证明了，在存在虚假特征的情况下，数据组的不平衡和易于学习的虚假特征可能导致模型学习虚假特征。作者提出了一种新的训练算法PDE，它逐步扩展训练数据的大小可以提高了模型鲁棒性，有效地增强了其最劣组性能，实验证实在合成和真实世界的基准数据集上的超越了R模型等其它模型。 |
| [^89] | [ShuttleSet: A Human-Annotated Stroke-Level Singles Dataset for Badminton Tactical Analysis.](http://arxiv.org/abs/2306.04948) | ShuttleSet是一份羽毛球单打比赛的拍级数据集，其中包含104场比赛、3,685轮比赛、36,492个拍击，并涵盖了27名排名前列的男子和女子单打选手。这些拍级记录将促进人工智能在体育分析领域的发展。 |
| [^90] | [A modified model for topic detection from a corpus and a new metric evaluating the understandability of topics.](http://arxiv.org/abs/2306.04941) | 本文提出一种改进的神经网络模型用于检测语料库中的主题，并提出了一种新的指标用于评估主题的可读性。 |
| [^91] | [Layer-level activation mechanism.](http://arxiv.org/abs/2306.04940) | 去噪声更好，表现更好的分层级别激活机制 |
| [^92] | [InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding.](http://arxiv.org/abs/2306.04933) | 本文提出了一种新的信息论框架，通过最大化提示和其他模型参数之间互信息来优化软提示调整，从而开发了更加高效、准确和稳健的软提示调整方法InfoPrompt。该方法通过两个新型损失函数，发现合适的提示初始化，并从提示令牌中学习足够的任务相关信息，同时鼓励预训练语言模型的输出表示更加关注任务相关信息。 |
| [^93] | [When to Show a Suggestion? Integrating Human Feedback in AI-Assisted Programming.](http://arxiv.org/abs/2306.04930) | 本研究利用先前数据的干预措施提高基于AI的代码推荐系统的有效性，提出了一个CDHF框架来整合人类反馈，预测建议接受程度并决定何时展示哪些建议。 |
| [^94] | [covLLM: Large Language Models for COVID-19 Biomedical Literature.](http://arxiv.org/abs/2306.04926) | 使用大型语言模型开发了一种名为covLLM的工具，用于协助临床医生评估COVID-19文献。covLLM可以汇总和提取相关信息，帮助医生更好地应对COVID-19疫情。 |
| [^95] | [Prefer to Classify: Improving Text Classifiers via Auxiliary Preference Learning.](http://arxiv.org/abs/2306.04925) | 本文提出了一种新的文本分类方法，使用偏好分类学习辅助数据来提高模型准确性。通过比较输入文本对之间的偏好关系，这种方法能够为模型提供额外的训练信号。 |
| [^96] | [Exact Optimality of Communication-Privacy-Utility Tradeoffs in Distributed Mean Estimation.](http://arxiv.org/abs/2306.04924) | 本研究针对均值估计问题，探讨了在通信和本地差分隐私约束下的精确最优方法，提出了利用旋转对称的共享随机码书，并通过$k$-closest编码实现了随机旋转的单纯形$c$的精确最优。 |
| [^97] | [Unconstrained Online Learning with Unbounded Losses.](http://arxiv.org/abs/2306.04923) | 本论文提出了一种算法，可用于解决无界域和非Lipschitz损失的在线学习问题，并提供了一个遗憾的度量，以衡量该算法的性能。此外，我们还利用该算法开发了一种新的鞍点优化算法，即使在没有有意义的曲率的情况下，也能够在无界领域中收敛于对偶间隙。最后，我们提供了一种算法，在无界域和非Lipschitz损失的情况下实现了非平凡的动态遗憾，以及相匹配的下界。 |
| [^98] | [Efficient and Equivariant Graph Networks for Predicting Quantum Hamiltonian.](http://arxiv.org/abs/2306.04922) | 本文提出了一种名为QHNet的SE(3)-等变网络，具有高效和等变性。与最先进的方法相比，QHNet在更快的速度下实现了与其可比的表现，并且消耗的内存少了50％。 |
| [^99] | [Unsupervised Cross-Domain Soft Sensor Modelling via A Deep Bayesian Particle Flow Framework.](http://arxiv.org/abs/2306.04919) | 该论文提出了一种基于深度粒子流贝叶斯框架的跨领域软测量无监督建模方法，能够解决标签缺失、领域适应性和数据时间一致性等问题，提高软测量建模的准确性。 |
| [^100] | [A Cover Time Study of a non-Markovian Algorithm.](http://arxiv.org/abs/2306.04902) | 本文研究了一个遍历算法中的覆盖时间问题，通过一种基于计数的负反馈策略，实现在任意图中局部提高搜索效率，并在特殊的图形中实现更小的覆盖时间。 |
| [^101] | [Generalization Performance of Transfer Learning: Overparameterized and Underparameterized Regimes.](http://arxiv.org/abs/2306.04901) | 该研究探讨了转移学习在不同相似度条件下的表现，使用线性回归模型分析了两种参数传递选项的比较，并通过理论分析模型误差的特征来检验泛化性能如何随着参数数量的变化而变化。 |
| [^102] | [Understanding Masked Autoencoders via Hierarchical Latent Variable Models.](http://arxiv.org/abs/2306.04898) | 本文正式刻画和证明了现有的经验性见解，并为MAE的训练和超参数调整提供了指导，以实现更好的表示学习。 |
| [^103] | [Solution of physics-based inverse problems using conditional generative adversarial networks with full gradient penalty.](http://arxiv.org/abs/2306.04895) | 本研究提出了一种新型的基于深度学习的方法，利用样本和物理学的正向模型生成训练数据，得到条件Wasserstein生成式对抗网络（cWGAN）的概率分布，可用于解决困难的概率反问题。 |
| [^104] | [A Bayesian Framework for learning governing Partial Differential Equation from Data.](http://arxiv.org/abs/2306.04894) | 本文介绍了一种采用变分贝叶斯和稀疏线性回归相结合的方法，用于从预定义的基函数字典中学习相关基本知识，并发现偏微分方程。 |
| [^105] | [In-Context Learning through the Bayesian Prism.](http://arxiv.org/abs/2306.04891) | 这篇论文研究了大型语言模型中的上下文学习现象，并通过实验证据展示了Transformer模型在多种设置下表现出贝叶斯预测器的行为。作者还探讨了上下文学习与贝叶斯学习框架之间的联系，并提出了一个线性回归任务来验证这种联系。 |
| [^106] | [ShaDDR: Real-Time Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering.](http://arxiv.org/abs/2306.04889) | 本文介绍了一种基于示例的深度生成神经网络 ShaDDR，可以通过几何细节化和条件纹理生成应用于输入的粗略体素形状，生成高分辨率贴图的 3D 形状。生成实时且精度高，风格可以通过学习的潜在代码进行控制。 |
| [^107] | [Multi-task Bioassay Pre-training for Protein-ligand Binding Affinity Prediction.](http://arxiv.org/abs/2306.04886) | 本文提出一种多任务生物测定预训练框架（MBP）用于基于结构的蛋白质-配体结合亲和力预测，通过构建一个预训练数据集ChEMBL-Dock，解决了高质量训练数据稀缺以及不同标签和实验条件导致的噪声问题。 |
| [^108] | [Faster Approximation Algorithms for Parameterized Graph Clustering and Edge Labeling.](http://arxiv.org/abs/2306.04884) | 本文提出了针对参数化聚类LambdaCC的更快逼近算法，同时引入了一个概括之前边标记问题的新的参数化边标记问题，该方法可扩展性比以前的算法高几个数量级。 |
| [^109] | [Augmenting Hessians with Inter-Layer Dependencies for Mixed-Precision Post-Training Quantization.](http://arxiv.org/abs/2306.04879) | 本文提出一种混合精度后训练量化方法，分配不同的数值精度以减少内存占用和改善延迟，同时通过增强Hessian与其他层之间的依赖关系来解决传统方法不足以确定层敏感性排序的问题。实验证明该方法优于现有的混合精度后训练量化方法。 |
| [^110] | [Trojan Model Detection Using Activation Optimization.](http://arxiv.org/abs/2306.04877) | 本文提出了一种新颖的特洛伊模型检测方法，通过激活优化为模型创建签名，然后训练分类器来检测特洛伊模型。该方法在两个公共数据集上实现了最先进的性能。 |
| [^111] | [Instructed Diffuser with Temporal Condition Guidance for Offline Reinforcement Learning.](http://arxiv.org/abs/2306.04875) | 本文提出一种有效的时间条件扩散模型 TCD，通过提取序列数据中的时间信息并将其用于生成，实现了更好的控制生成效果。 |
| [^112] | [Expanding Scope: Adapting English Adversarial Attacks to Chinese.](http://arxiv.org/abs/2306.04874) | 本文研究将英文的对抗攻击方法适用于中文上，并证明了这些方法可以生成高质量的中文对抗实例。通过关注中文的语言特点，生成的对抗实例可以实现高流畅度和语义一致性，从而可以用来提高中文NLP模型的对抗鲁棒性。 |
| [^113] | [City-wide Origin-Destination Matrix Generation via Graph Denoising Diffusion.](http://arxiv.org/abs/2306.04873) | 本文提出了一种基于图去噪扩散的方法，从网络的角度生成城市全域的起点与终点矩阵，并通过学习区域层面的城市特征来设计出图去噪扩散方法的条件概率分布。 |
| [^114] | [A Systematic Literature Review on Client Selection in Federated Learning.](http://arxiv.org/abs/2306.04862) | 联邦学习中客户端选择的主要挑战是异质性、资源分配、通信成本和公平性。客户选择方案旨在解决这些挑战，最常用的指标是测试准确性与通信轮次。 |
| [^115] | [Island-based Random Dynamic Voltage Scaling vs ML-Enhanced Power Side-Channel Attacks.](http://arxiv.org/abs/2306.04859) | 本文介绍了一种基于岛屿的随机动态电压调节（iRDVS）方法，用于防范功率侧信道攻击，并通过实验验证了其有效性。 |
| [^116] | [Interpreting and Improving Diffusion Models Using the Euclidean Distance Function.](http://arxiv.org/abs/2306.04848) | 本文利用欧几里得距离函数解释去噪扩散模型，并提出了一种新的采样器。采样器表现出了最先进的FID得分，并能够生成高质量的样本。 |
| [^117] | [Embedding stochastic differential equations into neural networks via dual processes.](http://arxiv.org/abs/2306.04847) | 该论文提出了一种新方法，通过将随机微分方程的信息直接与神经网络的权重进行比较，构建用于预测随机微分方程期望的神经网络。这种方法避免了过度拟合问题，并在原点附近的输入有着准确度。 |
| [^118] | [Classical Verification of Quantum Learning.](http://arxiv.org/abs/2306.04843) | 本文提出了一个经典验证量子学习的框架，以便经典客户委托学习给不可信的量子服务器，并成功解决了对仗学习奇偶性和傅里叶稀疏函数不可信量子证明的问题。 |
| [^119] | [$K$-Nearest-Neighbor Resampling for Off-Policy Evaluation in Stochastic Control.](http://arxiv.org/abs/2306.04836) | 提出了一种新颖的$K$最近邻重采样方法，用于估算历史数据中由不同策略生成的决策过程的性能，解决了离线策略评估中的反事实估计问题。 |
| [^120] | [Empowering Counterfactual Reasoning over Graph Neural Networks through Inductivity.](http://arxiv.org/abs/2306.04835) | 本研究引入了一种归纳算法INDUCE来增强图神经网络的反事实推理能力，改进了现有算法中存在的限制，通过在多个数据集上进行广泛的实验证明了其可行性。 |
| [^121] | [A Semi-supervised Object Detection Algorithm for Underwater Imagery.](http://arxiv.org/abs/2306.04834) | 该研究提出了一种基于变分自编码器的半监督框架，将人造物体视为异常来检测它们，通过在低维潜在空间中聚类图像数据并提取可能包含异常特征的图像以及设计一个基于提取图像重建质量差的异常得分方法，人类操作员可以更有效地检测水下目标。 |
| [^122] | [Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities.](http://arxiv.org/abs/2306.04829) | 本研究提出了一种新方法，利用预训练的自监督特征和时间特征相似性损失，实现了对真实世界视频的物体中心学习，在合成MOVi数据集上取得了最先进的性能。同时，本模型是首个能够扩展到无约束视频数据集的物体中心视频模型。 |
| [^123] | [Fast and Effective GNN Training with Linearized Random Spanning Trees.](http://arxiv.org/abs/2306.04828) | 本文提出了一种基于线性化随机生成树的GNN训练框架，在多个真实世界的图形基准测试中表现得比其他经典算法更快且更准确。 |
| [^124] | [Sparse Linear Centroid-Encoder: A Convex Method for Feature Selection.](http://arxiv.org/abs/2306.04824) | SLCE是用线性变换将点重构为类别质心并使用$\ell_1$范数惩罚从输入数据中滤除不必要特征的特征选择新方法。该方法为多类数据使用单个模型，具有与最先进方法相当的性能，同时具有显着较少的特征数。 |
| [^125] | [SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States.](http://arxiv.org/abs/2306.04817) | 本文提出了一种跨状态的图形相似性驱动的模块推理框架，可以同时考虑数据中的状态间和状态内关系，并允许状态之间的会话计数和持续时间的差异。它可以提取非正交组件，并且能够识别特定状态与状态非特定模块。 |
| [^126] | [Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning.](http://arxiv.org/abs/2306.04815) | 本文通过研究SGD训练损失中的尖峰现象，提出了“投石机”优化现象，通过增加与真实预测器的平均梯度外积对齐来促进特征学习，并证明较小的批量大小可提高泛化性能。 |
| [^127] | [Correlative Information Maximization: A Biologically Plausible Approach to Supervised Deep Neural Networks without Weight Symmetry.](http://arxiv.org/abs/2306.04810) | 本文提出了一种无需权重对称的有监督深度神经网络的生物合理方法，该方法利用相关信息最大化在层激活之间描述生物神经网络中的信号传播。通过坐标下降优化相应的目标和均方误差损失函数，可以产生一个更生物真实的神经网络结构。 |
| [^128] | [Privately generating tabular data using language models.](http://arxiv.org/abs/2306.04803) | 通过使用语言模型训练每一行数据作为一个句子并添加差分隐私，可以在多个数据集中生成具有竞争力的合成数据，以实现私人数据生成。 |
| [^129] | [A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises.](http://arxiv.org/abs/2306.04802) | 本论文综述了医疗知识图谱(HKGs)的构建流程、关键技术和利用方法以及现有资源，并深入探讨了HKG在各种医疗领域的变革性影响。 |
| [^130] | [Feature Selection using Sparse Adaptive Bottleneck Centroid-Encoder.](http://arxiv.org/abs/2306.04795) | 该论文提出一种基于稀疏自适应瓶颈质心编码器的特征选择模型，该模型通过提取有区别的特征组以及在重构类别质心的同时减少同类分散度、增加不同类别质心的分离性，从而实现过滤输入数据中不必要特征的功能。 |
| [^131] | [On the Joint Interaction of Models, Data, and Features.](http://arxiv.org/abs/2306.04793) | 本文提出了一种新工具--交互张量用于通过特征对模型和数据之间的交互进行经验分析，并提出了一个特征学习的概念框架，可以解释一些经验观察现象，表明深度学习可能受益于探索传统IID（独立同分布）假设之外的数据。 |
| [^132] | [Absformer: Transformer-based Model for Unsupervised Multi-Document Abstractive Summarization.](http://arxiv.org/abs/2306.04787) | 本文提出了一种新的用于无监督多文档抽象摘要生成的Transformer-based方法Absformer，它使用掩码语言建模（MLM）目标进行预训练聚类文档并生成抽象摘要，实验结果显示，它在基准数据集上优于几种最先进的无监督抽象MDS方法。 |
| [^133] | [Interpretable Deep Clustering.](http://arxiv.org/abs/2306.04785) | 本文提出了一种可解释的深度学习框架，通过自我监督的方式从数据点中标识信息量丰富的特征，设计了一个模型和门矩阵来预测可解释的实例和聚类级别的聚类分配，并在合成和实际数据中验证了其可靠性和可解释性。 |
| [^134] | [Learning to Navigate in Turbulent Flows with Aerial Robot Swarms: A Cooperative Deep Reinforcement Learning Approach.](http://arxiv.org/abs/2306.04781) | 本文提出了一种基于深度强化学习的多机器人控制器，通过嵌套控制架构将轨迹跟踪控制与湍流补偿分离，以解决在湍流风况下无人机群组协同运动的问题。 |
| [^135] | [Loss Functions for Behavioral Game Theory.](http://arxiv.org/abs/2306.04778) | 本文研究了行为博弈论家在损失函数选择上的差异，并构建了一组满足特定公理的损失函数集合，其中平方L2误差是实践中唯一可接受的损失函数，并建议行为博弈论家继续使用它。 |
| [^136] | [Invariant Causal Set Covering Machines.](http://arxiv.org/abs/2306.04777) | 本文提出了一种名为不变因果集覆盖机的算法，它避免了产生虚假关联，可以在多项式时间内识别感兴趣变量的因果父节点。 |
| [^137] | [Exploiting Observation Bias to Improve Matrix Completion.](http://arxiv.org/abs/2306.04775) | 本研究利用观测偏差来改进矩阵补全问题，提出一个简单的两阶段算法，实现了与对未观测协变量的监督学习性能相当的结果。 |
| [^138] | [Enabling tabular deep learning when $d \gg n$ with an auxiliary knowledge graph.](http://arxiv.org/abs/2306.04766) | 该论文提出了 PLATO 方法，通过使用描述输入特征的辅助 KG 来规范 MLP，在 $d \gg n$ 的表格数据上实现了强大的性能。 |
| [^139] | [Context-Aware Self-Supervised Learning of Whole Slide Images.](http://arxiv.org/abs/2306.04763) | 本文提出了一种全切片图像使用图卷积网络进行上下文感知自监督学习的新的两阶段学习技术，该技术可提高癌症诊断的效率和准确性。 |
| [^140] | [Analysis, Identification and Prediction of Parkinson's disease sub-types and progression through Machine Learning.](http://arxiv.org/abs/2306.04748) | 本文使用监督和无监督机器学习方法鉴别出三种不同的帕金森病亚型，并生成患者个性化的症状进展预测，有助于制定针对性干预和改善患者预后。 |
| [^141] | [Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning.](http://arxiv.org/abs/2306.04746) | 该论文提出了一种新算法，使用大型语言模型（LLMs）输出进行下游统计分析，以实现有效的下游统计推断，并降低标签获取的研究成本80％，同时保证CSS研究的统计属性。 |
| [^142] | [Automatic retrieval of corresponding US views in longitudinal examinations.](http://arxiv.org/abs/2306.04739) | 本文提出了一种自监督对比学习方法，以自动检索不同时间扫描的相似超声肌肉视图。使用从67名ICU患者获取的数据对比了三种不同的模型，结果表明该方法在视图检索任务中表现良好。 |
| [^143] | [Soft-prompt Tuning for Large Language Models to Evaluate Bias.](http://arxiv.org/abs/2306.04735) | 本文使用软提示调整来量化大型语言模型中的偏差，避免手动设计提示导致的人为偏差注入。通过分组公平性检查模型对不同敏感属性的偏见，发现了有趣的偏差模式。 |
| [^144] | [Stochastic Natural Thresholding Algorithms.](http://arxiv.org/abs/2306.04730) | 该论文提出了一种随机自然阈值算法，用于稀疏信号恢复，并扩展了现有的自然阈值法，可以提高算法的计算效率和性能。 |
| [^145] | [Don't trust your eyes: on the (un)reliability of feature visualizations.](http://arxiv.org/abs/2306.04719) | 本文探讨了神经网络如何从像素中提取模式的问题，并研究了特征可视化的可靠性。实验证据表明，由于优化过程中固有的限制，特征可视化能够可靠理解的功能集非常有限，对于解释神经网络如何处理自然图像的解释能力产生怀疑。 |
| [^146] | [Neural Symbolic Regression using Control Variables.](http://arxiv.org/abs/2306.04718) | 提出了一种利用控制变量的神经符号回归方法，可以分解多变量符号回归为单变量回归，并从底部向上组合，解决了符号回归存在的精度和可扩展性问题。 |
| [^147] | [Differentiable Earth Mover's Distance for Data Compression at the High-Luminosity LHC.](http://arxiv.org/abs/2306.04712) | 本文利用可微分的快速逼近方法，训练了一个编码器神经网络用于高亮LHC数据的压缩，同时保留了数据内与粒子探测器中的能量沉积分布相关的信息。 |
| [^148] | [Improved statistical benchmarking of digital pathology models using pairwise frames evaluation.](http://arxiv.org/abs/2306.04709) | 该论文提出一种嵌套成对帧评估方法用于相对基准测试，通过比较候选模型和病理学家注释之间的一致性以及病理学家注释之间的一致性，解决了使用手动病理学家注释进行模型验证的基本问题。 |
| [^149] | [Robust-DefReg: A Robust Deformable Point Cloud Registration Method based on Graph Convolutional Neural Networks.](http://arxiv.org/abs/2306.04701) | 本文提出了一种稳健的非刚性点云配准方法Robust-DefReg，使用基于图卷积神经网络的端到端粗到细配准策略，并在解决不同难度问题上均表现出高精度和稳健性。 |
| [^150] | [ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models.](http://arxiv.org/abs/2306.04695) | 本文提出ConceptBed数据集和评估指标CCD，用于评估文本到图像模型的概念学习和合成能力。 |
| [^151] | [Multiscale Flow for Robust and Optimal Cosmological Analysis.](http://arxiv.org/abs/2306.04689) | 用多尺度流进行二维宇宙学数据的生成和建模，可识别不同尺度的信息并显著胜过现有方法。 |
| [^152] | [Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models.](http://arxiv.org/abs/2306.04675) | 系统地研究了图像生成模型的评估，发现常见的评价指标如FID等不能很好地体现扩散模型的感知真实性，建议使用SwAV特征提取器结合FID进行评估。 |
| [^153] | [SMRVIS: Point cloud extraction from 3-D ultrasound for non-destructive testing.](http://arxiv.org/abs/2306.04668) | 提出了将超声体积点云提取作为图像分割问题的解决方案，并通过快速原型开发验证效果。 |
| [^154] | [Neural Embeddings for Protein Graphs.](http://arxiv.org/abs/2306.04667) | 该论文提出了一种新方法，通过学习编码器函数在几何向量空间中嵌入蛋白质图，生成具有结构和序列感知能力的蛋白质表示，实现了在比较蛋白质结构和蛋白质结构分类任务中的显着加速和卓越结果。 |
| [^155] | [Estimating Uncertainty in PET Image Reconstruction via Deep Posterior Sampling.](http://arxiv.org/abs/2306.04664) | 本文提出了一种基于深度学习的方法，通过后验采样量化PET图像重建中的不确定性，该方法可以更准确地估计不确定性并提高重建质量。 |
| [^156] | [U-PASS: an Uncertainty-guided deep learning Pipeline for Automated Sleep Staging.](http://arxiv.org/abs/2306.04663) | U-PASS是一种基于不确定性引导的深度学习管道，具有估计不确定性的能力，针对临床应用进行设计。在睡眠分期中表现出极高的性能，通过优化训练数据集和积极寻找信息样本实现最先进的性能。 |
| [^157] | [Understanding Place Identity with Generative AI.](http://arxiv.org/abs/2306.04662) | 本研究测试了生成式人工智能在捕捉城市地方认同方面的潜力，结果表明它可以生成多样化的逼真表现，但不能过度依赖，同时挖掘了它的潜力和局限性。 |
| [^158] | [Adaptive Frequency Green Light Optimal Speed Advisory based on Hybrid Actor-Critic Reinforcement Learning.](http://arxiv.org/abs/2306.04660) | 本文提出了一种基于混合Actor-Critic强化学习的自适应频率绿灯最佳速度建议模型，通过使用离散和连续演员网络来优化绿灯最佳速度建议系统的频率和加速度曲线，设计了新的奖励函数来平衡减少停车和最小化速度变化的权衡。实验结果显示，该模型可以有效地减少旅行时间和燃料消耗，并优于现有的最先进的GLOSA系统。 |
| [^159] | [Mathematics-assisted directed evolution and protein engineering.](http://arxiv.org/abs/2306.04658) | 这篇论文介绍了定向进化在蛋白质工程中的应用，机器学习和拓扑数据分析方法对其实现起到了关键作用。 |
| [^160] | [Modulation Classification Through Deep Learning Using Resolution Transformed Spectrograms.](http://arxiv.org/abs/2306.04655) | 本文提出了通过生成不同调制类型的频谱图像，利用深度学习自动进行调制分类的方案，并通过对频谱图进行分辨率转换来提高转换速度和减少计算负载。 |
| [^161] | [From Data to Action: Exploring AI and IoT-driven Solutions for Smarter Cities.](http://arxiv.org/abs/2306.04653) | 本文介绍了一种智能城市管理系统，利用人工智能和物联网技术来分析交通、能源消耗和城市设施维护等数据，提供可行的解决方案，以增强城市的安全性、能源效率和可持续性。 |
| [^162] | [On training locally adaptive CP.](http://arxiv.org/abs/2306.04648) | 本文提出了一种新的Conformal Prediction方法，使用可训练的变量变换重新定义符合度量，使得预测区间在保持边际有效的同时具有对象属性相关的大小。通过训练可最大化间隔效率。 |
| [^163] | [Compressed Sensing: A Discrete Optimization Approach.](http://arxiv.org/abs/2306.04647) | 本文中提出了一种离散优化方法来解决压缩感知问题，该方法在二次锥松弛下，可以找到最稀疏的向量，得到了可靠的最优解。 |
| [^164] | [Improve State-Level Wheat Yield Forecasts in Kazakhstan on GEOGLAM's EO Data by Leveraging A Simple Spatial-Aware Technique.](http://arxiv.org/abs/2306.04646) | 本文提出了一种称为州级加性偏差的简单技术，可显著提高使用遥感数据进行小麦产量预测的机器学习性能，尤其是解决哈萨克斯坦州级产量异质性问题，将RMSE降低了8.9\% ~ 28.37\%。 |
| [^165] | [Special Session: Approximation and Fault Resiliency of DNN Accelerators.](http://arxiv.org/abs/2306.04645) | 本文研究了DNN加速器的逼近和容错能力。使用AxC算术电路敏捷地模拟硬件上的错误，而不需要在DNN上执行故障注入。提出了一种基于GPU的快速评估框架，并通过检查故障传播和掩蔽，进行了精细的容错分析。 |
| [^166] | [DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models.](http://arxiv.org/abs/2306.04642) | 本文提出了一种针对生成式扩散模型的版权保护数字水印方法DiffusionShield，保护图像免受侵权，简单易学，难以检测，可广泛应用于各行各业。 |
| [^167] | [Generalizable Low-Resource Activity Recognition with Diverse and Discriminative Representation Learning.](http://arxiv.org/abs/2306.04641) | 提出了一种新方法DDLearn，通过构建自监督学习任务，在考虑多样化和区分化学习的基础上提高通用低资源人类活动识别的性能。 |
| [^168] | [Faithful Knowledge Distillation.](http://arxiv.org/abs/2306.04431) | 本文研究了知识蒸馏中教师和学生之间的相对校准问题，提出了一个忠实的模仿框架来解决学生置信度和软标签的问题，并提供了一种实证和认证的方法来评估学生模型的鲁棒性。 |
| [^169] | [Revising deep learning methods in parking lot occupancy detection.](http://arxiv.org/abs/2306.04288) | 本文提出了一种基于EfficientNet架构的停车位占用检测算法，并在5个不同的数据集上进行了评估，性能得到提高。 |
| [^170] | [Self-Adjusting Weighted Expected Improvement for Bayesian Optimization.](http://arxiv.org/abs/2306.04262) | 本文提出了一种新的自适应加权期望改进方法（SAWEI），可以自动平衡探索不确定区域和利用有承诺区域之间的权衡。在COCO基准测试中，该方法表现出有利的性能。 |
| [^171] | [Adversarial Sample Detection Through Neural Network Transport Dynamics.](http://arxiv.org/abs/2306.04252) | 本文提出基于神经网络动力学的对抗样本检测器，并通过规范化向量场的训练方法来提高其干净输入和异常输入的区分度。实验结果表明，该方法在不同攻击下的表现优于其他检测器，同时也提高了对抗检测器的测试准确率。 |
| [^172] | [Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL.](http://arxiv.org/abs/2306.04220) | 本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。 |
| [^173] | [Designing Decision Support Systems Using Counterfactual Prediction Sets.](http://arxiv.org/abs/2306.03928) | 本文提出了一种基于反事实预测集的决策支持系统设计方法，不同于传统的单一标签预测，它使用符合预测器构建预测集，并引导人类专家从中选择标签值。 |
| [^174] | [Emotion-Conditioned Melody Harmonization with Hierarchical Variational Autoencoder.](http://arxiv.org/abs/2306.03718) | 提出了一种基于LSTM的分层变分自编码器(LHVAE)，研究情感条件对旋律和声编配的影响，同时提高了生成的和声质量并捕捉了丰富的和弦进行变化。实验证明，该模型在感知情感表达方面优于现有方法。 |
| [^175] | [Continual Learning with Pretrained Backbones by Tuning in the Input Space.](http://arxiv.org/abs/2306.02947) | 本论文提出了一种新颖的策略，使得通过微调不仅可以避免更新预训练部分从而损失宝贵的知识，并且还可以学习一组新的可学习参数将输入数据转换为有效信息。 |
| [^176] | [Message-passing selection: Towards interpretable GNNs for graph classification.](http://arxiv.org/abs/2306.02081) | 本文提出了一种可解释的GNN推理范式MSInterpreter，其中包括消息传递选择方案(MSScheme)，通过计算由结构和节点嵌入组成的消息聚合路径的权重因子，实现对GNN自我解释。在图分类基准测试中表明其有效性。 |
| [^177] | [SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization.](http://arxiv.org/abs/2306.01981) | SGEM提出了一种新的测试时自适应框架，利用波束搜索和广义熵最小化调整预训练ASR模型，取得了三个主流ASR模型在不同领域转变时的最新性能。 |
| [^178] | [Responsible Design Patterns for Machine Learning Pipelines.](http://arxiv.org/abs/2306.01788) | 本文提出了一种综合框架，将负责任设计模式纳入机器学习流程中，以确保AI系统的伦理性和公正性。这个框架包括新的负责任AI设计模式，并指导AI开发人员、数据科学家和决策者在AI开发和部署中实施伦理实践。 |
| [^179] | [Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization.](http://arxiv.org/abs/2306.01103) | 本文提出了一种考虑标签和环境因果独立性的方法来解决图形超出分布（OOD）泛化问题，通过敌对训练策略来联合优化属性以获得有效结果，实验证明LECI显着优于之前的方法。 |
| [^180] | [Human-Aligned Calibration for AI-Assisted Decision Making.](http://arxiv.org/abs/2306.00074) | 本文通过引入一种基于主动询问决策者个人偏好的置信度构造方法，解决了现有置信度对于决策者信任决策的不准确问题，从而提高决策的准确性和效率。 |
| [^181] | [Parity Calibration.](http://arxiv.org/abs/2305.18655) | 本文介绍一种新的校准预测目标——parity校准，其考虑时间序列中未来观测值的增加或减少。我们使用在线二进制校准方法实现了parity校准，并在流行病学、天气预报和核聚变控制等领域中表明该方法的有效性。 |
| [^182] | [A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets.](http://arxiv.org/abs/2305.18486) | 本文对基准数据集上 ChatGPT 的性能进行了全面的评估，包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务。研究旨在验证 ChatGPT 的优势和弱点，并为使用语言模型的未来研究提供见解。 |
| [^183] | [Distilling BlackBox to Interpretable models for Efficient Transfer Learning.](http://arxiv.org/abs/2305.17303) | 本论文提出了一种方法可以将黑盒模型转化成为可解释模型并在目标领域成功进行迁移学习，从而实现高效迁移学习。 |
| [^184] | [Stability of implicit neural networks for long-term forecasting in dynamical systems.](http://arxiv.org/abs/2305.17155) | 本文提出了一种基于隐式数值方案稳定性特性的神经网络，加入了硬性约束来保证其权重稳定性，取得了较好的长期预测结果。 |
| [^185] | [Parallel Sampling of Diffusion Models.](http://arxiv.org/abs/2305.16317) | 本文提出了一种新方法，ParaDiGMS，可以通过并行处理多个步骤来加速预训练扩散模型的采样。ParaDiGMS是第一个使计算速度和采样效率实现平衡的扩散采样方法，并与现有方法兼容。 |
| [^186] | [Can Self-Supervised Neural Representations Pre-Trained on Human Speech distinguish Animal Callers?.](http://arxiv.org/abs/2305.14035) | 本文研究了利用人类语音自监督神经表示学习来分析生物声学信号的交叉可迁移性，在狨猴叫声识别和检测中取得了成功，未来该方法可有效应用于该领域的研究。 |
| [^187] | [Capturing Conversion Rate Fluctuation during Sales Promotions: A Novel Historical Data Reuse Approach.](http://arxiv.org/abs/2305.12837) | 本论文提出了一种名为HDR的新方法，通过重复使用历史促销数据，来捕捉促销转化模式，达到更好地适应促销模式的目的。 |
| [^188] | [RGCVAE: Relational Graph Conditioned Variational Autoencoder for Molecule Design.](http://arxiv.org/abs/2305.11699) | 本文提出了RGCVAE，一种基于关系图条件化的可变自编码器，可以有效、高效地进行分子设计，并在两个数据集上表现出了先进的生成性能和快速的训练速度。 |
| [^189] | [Differentially Private Online Item Pricing.](http://arxiv.org/abs/2305.11362) | 本文介绍了一种差分隐私算法，可在保护买家隐私的同时实现重复、不限供应物品拍卖中的收益最大化，是第一个提供隐私保证的$O(\sqrt{T}\log{T})$亏损子线性的方法。 |
| [^190] | [A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes.](http://arxiv.org/abs/2305.08841) | 本文提出了一种乐观的近端策略优化算法，用于解决带有全信息反馈的周期性对抗性线性MDP，在随机线性MDP和带有全信息的敌对线性MDP中，达到了最先进的后悔边界，并具有新颖的多批次更新机制。 |
| [^191] | [Statistical Inference for Fairness Auditing.](http://arxiv.org/abs/2305.03712) | 本文介绍了一种公平性审计的统计推断方法，可用于评估黑盒模型在敏感子群体上的表现，并通过自举方法限制多个群体表现差异，方法通用性强且适用面广。 |
| [^192] | [The Ideal Continual Learner: An Agent That Never Forgets.](http://arxiv.org/abs/2305.00316) | 本文提出了一个新的持续学习框架——理想的持续学习者（ICL），通过构建避免灾难性遗忘，统一了多个持续学习方法，并为这些方法的优缺点提供了新的理论见解。 |
| [^193] | [Controlled Text Generation with Natural Language Instructions.](http://arxiv.org/abs/2304.14293) | InstructCTG是一个可以通过自然语言描述和演示来控制文本生成并满足不同约束条件的框架，它有效地解决了现有搜索或得分方法所存在的问题。 |
| [^194] | [Learning Trajectories are Generalization Indicators.](http://arxiv.org/abs/2304.12579) | 本文研究了DNN的学习轨迹与其在优化后的泛化能力的联系，提出了一种基于学习轨迹复杂性和训练集偏置和多样性比率的新的泛化上界，并通过实验验证了其有效性。 |
| [^195] | [SkinGPT: A Dermatology Diagnostic System with Vision Large Language Model.](http://arxiv.org/abs/2304.10691) | SkinGPT是一个基于迷你GPT-4的精细调整版本和内部皮肤图像集合结合的皮肤科诊断系统，利用高级视觉大语言模型，解决了皮肤科医生不足、准确诊断皮肤科图片难度大，以及提供用户友好的诊断报告困难等三个挑战。 |
| [^196] | [Long-Term Fairness with Unknown Dynamics.](http://arxiv.org/abs/2304.09362) | 本文提出一种新方法，通过在未知动态下追求长期公平性，实现算法的动态适应和权衡，可为分类器-人群系统推向更理想的平衡。 |
| [^197] | [A Simple Proof of the Mixing of Metropolis-Adjusted Langevin Algorithm under Smoothness and Isoperimetry.](http://arxiv.org/abs/2304.04095) | 本文证明了，在光滑和等周条件下，MALA的混合时间仅与Hessian矩阵的trace有关，而与其算子范数和log-concave没有关系。 |
| [^198] | [Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark.](http://arxiv.org/abs/2304.03279) | 本文介绍了 MACHIAVELLI 基准测试，用于衡量人工智能代理是否表现出马基雅维利行为，发现了最大化奖励和行为的道德性之间存在权衡，并探索了基于语言模型的方法来减轻这种权衡。 |
| [^199] | [How Regional Wind Characteristics Affect CNN-based wind predictions: Insights from Spatiotemporal Correlation Analysis.](http://arxiv.org/abs/2304.01545) | 本研究探讨了使用3D卷积神经网络（3D-CNN）对时空数据进行风速预测的精度，并发现使用周围区域的空间数据进行3D-CNN训练可以比仅使用单点信息获得更好的预测性能。 |
| [^200] | [When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective!.](http://arxiv.org/abs/2303.16458) | 本文提出了一个通用框架W2PGNN，旨在回答何时预训练图神经网络的关键问题。使用新的角度探索了从预训练数据到下游数据的复杂生成机制。 |
| [^201] | [Opening Up the Neural Network Classifier for Shap Score Computation.](http://arxiv.org/abs/2303.06516) | 本文提出了一种高效计算机器学习模型分类中Shap解释分数的方法，通过将二进制神经网络转换为布尔电路，并使用知识编译技术，将电路视为开放式模型，通过最近的高效算法计算Shap分数，相比于将BNN视为黑盒模型直接计算Shap，性能有了显著的提高。 |
| [^202] | [DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference.](http://arxiv.org/abs/2303.06171) | 本文提出了一种新的DP-Fast MH算法，用于大规模贝叶斯推断，具有精确、快速和隐私保护的特点。 |
| [^203] | [Polynomial Time and Private Learning of Unbounded Gaussian Mixture Models.](http://arxiv.org/abs/2303.04288) | 本文开发了一种技术，将高维高斯混合模型的私有参数估计问题降低到非私有算法，从而使现有的非私有算法能够在隐私保护的条件下使用，提供了第一个多项式时间的私有学习GMMs算法。 |
| [^204] | [Learning to Influence Human Behavior with Offline Reinforcement Learning.](http://arxiv.org/abs/2303.02265) | 本文提出了一种通过离线强化学习学习影响人类行为的方法，可以提高人类在协作任务中的表现。 |
| [^205] | [CrystalBox: Future-Based Explanations for DRL Network Controllers.](http://arxiv.org/abs/2302.13483) | CrystalBox是一种解释DRL网络控制器行为的框架，它能够使用未来关键网络性能指标的影响来生成简明而富有表现力的解释。 |
| [^206] | [Generalization of Auto-Regressive Hidden Markov Models to Non-Linear Dynamics and Unit Quaternion Observation Space.](http://arxiv.org/abs/2302.11834) | 本文提出了自回归隐马尔可夫模型的两个推广，一个是笛卡尔空间中更一般的自回归动力学，另一个是单位四元数空间中的线性动力学。此扩展允许描述观察状态的更复杂动力学。 |
| [^207] | [Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC.](http://arxiv.org/abs/2302.11552) | 该论文提出了一种基于能量扩散模型和MCMC的组合生成方法，旨在解决现有技术在组合生成中的失败问题，并提出了新的成功的解决方案。 |
| [^208] | [Balanced Audiovisual Dataset for Imbalance Analysis.](http://arxiv.org/abs/2302.10912) | 本文通过估计样本级模态差异将现有数据集分成不同的子集并探索了模态偏差的影响。结果发现，存在不平衡算法的多模态模型在特定子集上的表现一直比单模态的差，与模态偏差相一致。 |
| [^209] | [Infinite Action Contextual Bandits with Reusable Data Exhaust.](http://arxiv.org/abs/2302.08551) | 本文提出了一种无限动作上下文臂的在线学习算法，它能够生成具有明确定义的重要性权重，使平滑遗憾在生产场景中更易于采用。 |
| [^210] | [Global and Preference-based Optimization with Mixed Variables using Piecewise Affine Surrogates.](http://arxiv.org/abs/2302.04686) | 本文提出了一种基于分段仿射代理构建的全局和基于偏好优化算法，可解决线性约束的混合变量问题，算法通过两种探索函数可有效搜索可行域 |
| [^211] | [Q-Diffusion: Quantizing Diffusion Models.](http://arxiv.org/abs/2302.04304) | 本文提出了一种特定于扩散模型的量化方法，通过时间步骤感知校准和拆分快捷路来加速图像生成，解决了扩散模型中后训练量化所面临的难题。 |
| [^212] | [Combining Variational Autoencoders and Physical Bias for Improved Microscopy Data Analysis.](http://arxiv.org/abs/2302.04216) | 本文提出了一种结合了变分自编码器和物理偏差的方法，能够从显微镜数据中识别出物理上有意义的区域和特征。 |
| [^213] | [DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets.](http://arxiv.org/abs/2302.04178) | DynGFN是一种借助RNA速度技术进行基因调控网络推断的方法，能够捕捉网络结构的不确定性，并在准确度上超过现有方法。 |
| [^214] | [Principlism Guided Responsible Data Curation.](http://arxiv.org/abs/2302.03629) | 研究针对人本计算机视觉数据集的负责任数据管理建议，采用预防性反思的观点，遵循原则主义的伦理框架，解决隐私和偏差问题。 |
| [^215] | [Robust Subtask Learning for Compositional Generalization.](http://arxiv.org/abs/2302.02984) | 本文提出了两种RL算法来解决子任务是否能执行任何任务的问题，并在两个多任务环境中进行了评估，表明这些算法在鲁棒性和泛化性方面优于基线算法。 |
| [^216] | [Target-based Surrogates for Stochastic Optimization.](http://arxiv.org/abs/2302.02607) | 本文提出了一个基于目标的优化框架，可以构造出有效的代理函数来代替计算昂贵的梯度，实现在目标空间的最优化，摊销梯度计算的成本，进而提出了SSO算法进行投影随机梯度下降，以达到损失的稳定点。 |
| [^217] | [A Lipschitz Bandits Approach for Continuous Hyperparameter Optimization.](http://arxiv.org/abs/2302.01539) | BLiE是一种用于超参数优化的算法，只假设目标函数具有Lipschitz连续性。理论和实验证明BLiE优于现有算法，并且可以应用于搜索扩散模型的噪声调度。 |
| [^218] | [Neural Insights for Digital Marketing Content Design.](http://arxiv.org/abs/2302.01416) | 本文提出了一种基于神经网络的数字营销内容设计评分和提取见解系统，该系统可以为营销人员提供基于历史数据的见解和设计建议，以改善其创意过程并显着提高客户参与度。 |
| [^219] | [Robust online active learning.](http://arxiv.org/abs/2302.00422) | 本文提出了一种自适应方法，用于鲁棒的在线主动学习，并在受污染的数据流中证明了其性能表现优异，同时确保了稳定性并减少异常值的负面影响。 |
| [^220] | [Graph-based Time-Series Anomaly Detection: A Survey.](http://arxiv.org/abs/2302.00058) | 本文综述了基于图的时间序列异常检测，主要探讨了图表示学习的潜力和最先进的图异常检测技术在时间序列中的应用。 |
| [^221] | [Regret Bounds for Markov Decision Processes with Recursive Optimized Certainty Equivalents.](http://arxiv.org/abs/2301.12601) | 本文提出了基于递归优化等价性的马尔可夫决策过程的风险敏感强化学习公式，设计了一种有效学习算法，并且推导了算法的遗憾上界和极小-最大下界，表明该算法实现的遗憾率对于情歌数和动作数具有最优依赖性。 |
| [^222] | [Causal Bandits without Graph Learning.](http://arxiv.org/abs/2301.11401) | 本文针对因果图未知的情况，提出了一种在因果赌博机问题中使用原子干预找到奖励节点父节点的有效算法，并扩展至奖励节点具有多个父节点的情况。同时，还得出了算法执行期望干预次数的精确方程，并证明在特定图形条件下，该算法可以快速执行对数。 |
| [^223] | [Deep Learning Meets Sparse Regularization: A Signal Processing Perspective.](http://arxiv.org/abs/2301.09554) | 本文介绍了一个新的数学框架，恰当地表征了被训练对数据拟合的神经网络的功能属性。这个框架解释了深度神经网络的一些优秀性能、网络体系结构中跳跃连接和低秩权重矩阵的使用、稀疏在神经网络中的作用以及为什么神经网络可作为优秀的特征提取器。 |
| [^224] | [Solving PDEs with Unmeasurable Source Terms Using Coupled Physics-Informed Neural Network with Recurrent Prediction in Soft Sensor Modeling.](http://arxiv.org/abs/2301.08618) | 提出了一种带循环预测学习策略的耦合物理信息神经网络（CPINN），用于软传感建模中的含非测量源项的PDEs求解，证明了CPINN具有满足PDEs解的近似容量，提高了软传感建模在时空工业系统中的性能表现。 |
| [^225] | [Leveraging Diffusion For Strong and High Quality Face Morphing Attacks.](http://arxiv.org/abs/2301.04218) | 本文提出了一种利用扩散技术提高图像保真度的人脸变形攻击，通过将两种特征结合提高了攻击的准确性和生物特征识别系统的易受性。 |
| [^226] | [MSCDA: Multi-level Semantic-guided Contrast Improves Unsupervised Domain Adaptation for Breast MRI Segmentation in Small Datasets.](http://arxiv.org/abs/2301.02554) | 本文提出了MSCDA框架来解决乳腺MRI分割中领域偏移的问题，其使用了多级语义引导对比和自训练方法，以对齐特征表示。通过像素对比、质心对比以及类别跨域采样策略的应用，MSCDA在小数据集无监督域自适应中表现出了优异的性能。 |
| [^227] | [Learning to Maximize Mutual Information for Dynamic Feature Selection.](http://arxiv.org/abs/2301.00557) | 本文提出一种基于互信息的动态特征选择方法，使用摊销优化进行学习。在实验中取得了比现有方法更好效果。 |
| [^228] | [BTS: Bifold Teacher-Student in Semi-Supervised Learning for Indoor Two-Room Presence Detection Under Time-Varying CSI.](http://arxiv.org/abs/2212.10802) | 本文提出了一种基于半监督学习的双折叠师生网络，该网络通过利用部分标记和未标记的数据集智能地学习空间和时间特征，有效地解决了基于CSI的室内存在检测受到环境变化和有监督学习方法需要耗时标注的问题。 |
| [^229] | [Simplicity Bias Leads to Amplified Performance Disparities.](http://arxiv.org/abs/2212.06641) | 模型在训练和测试过程中会优先考虑简单部分而非复杂部分，导致性能差异扩大，即使数据集平衡且没有群体/标签关联。困难差异是一个依赖于模型的量，并在常用模型中进一步放大。这个问题可以通过几种方法得到缓解。 |
| [^230] | [Differentially Private Adaptive Optimization with Delayed Preconditioners.](http://arxiv.org/abs/2212.00309) | 本篇论文提出了一种称为DP^2的方法，用于解决差分隐私自适应优化中的隐私噪声问题。该方法通过使用具有延迟但噪声较小的预条件器来实现自适应性的好处，并在凸和非凸问题上提供收敛保证，经实验证明可以将收敛速度提高多达4倍。 |
| [^231] | [CRONOS: Colorization and Contrastive Learning for Device-Free NLoS Human Presence Detection using Wi-Fi CSI.](http://arxiv.org/abs/2211.10354) | 本文介绍了一种名为CRONOS的系统，可以通过彩色化和对比学习来基于Wi-Fi CSI实现无人设备NLoS人体检测，可以区分房间中的移动人员和空置。实验结果表明该系统在NLoS条件下能够准确地检测出房间中的人物存在。 |
| [^232] | [A fermion neural network with efficient optimization and quantum applicability.](http://arxiv.org/abs/2211.05793) | 本文提出了一种费米子神经网络（FNN），它将输入作为初始层，输出物理特性，建立了一种高效的优化方法，可应用于具有相互作用的硬量子系统，而且能够精确地确定拓扑相和紧凑电荷序，其量子特性带来多种优势。 |
| [^233] | [Dynamic Interpretable Change Point Detection.](http://arxiv.org/abs/2211.03991) | 提出了一种动态的可解释的变点检测方法 TiVaCPD，该方法使用时间变化图形套索和聚合核最大均值差异测试相结合，能够有效地捕获多维时间序列中的各种复杂变化类型。 |
| [^234] | [Unrolled Graph Learning for Multi-Agent Collaboration.](http://arxiv.org/abs/2210.17101) | 提出一种受人类协作启发的分布式多智能体学习模型，智能体可以自主检测适合的合作者，并参考合作者的模型以获得更好的性能，使用协作图实现成对协作关系指示，通过展开图学习网络以更灵活适应各种情况地学习潜在合作者之间的相似特征。 |
| [^235] | [A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models.](http://arxiv.org/abs/2210.12023) | 该研究提出了一种基于因果框架的新方法，用于确定语言模型在数学推理任务中各种因素对输出解决方案的因果影响，研究结果显示GPT-3 Davinci模型（175B）在鲁棒性方面取得了显着改善。 |
| [^236] | [AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation.](http://arxiv.org/abs/2210.07535) | AutoMoE提出了一种能够在计算约束下设计异构MoE的框架，它在神经机器翻译任务中实现了高效且最先进的性能。 |
| [^237] | [FARE: Provably Fair Representation Learning with Practical Certificates.](http://arxiv.org/abs/2210.07213) | FARE是第一个提供实际公平性证书的FRL方法，通过限制编码器的表示空间实现实际保证并保持准确度-公平度权衡。 |
| [^238] | [Stochastic noise can be helpful for variational quantum algorithms.](http://arxiv.org/abs/2210.06723) | 本文证明随机性可以自然地避免变分量子算法中的严格鞍点问题，这一认识可以帮助我们更好地理解近期变分量子算法的概念。 |
| [^239] | [On the Robustness of Random Forest Against Untargeted Data Poisoning: An Ensemble-Based Approach.](http://arxiv.org/abs/2209.14013) | 本文提出了一种基于集合的方法来提高随机森林对非有目标数据污染的鲁棒性 |
| [^240] | [A Comprehensive Benchmark for COVID-19 Predictive Modeling Using Electronic Health Records in Intensive Care.](http://arxiv.org/abs/2209.07805) | 本文提出了两个针对COVID-19患者的临床预测任务：Outcome-specific length-of-stay prediction 和 Early mortality prediction，旨在填补临床应用和传统预测任务之间的差距，并提供一个基准测试框架，以实现公平比较各种模型。 |
| [^241] | [Scalable Set Encoding with Universal Mini-Batch Consistency and Unbiased Full Set Gradient Approximation.](http://arxiv.org/abs/2208.12401) | 本文提出了一种可扩展的集合编码方法UMBC，可以与任意非MBC组件相结合，同时仍满足MBC；同时提出了一种高效的MBC训练算法，可以为任何集合大小在训练和测试时都具有恒定的内存开销，给出完整集合梯度的无偏近似。 |
| [^242] | [A Hybrid Self-Supervised Learning Framework for Vertical Federated Learning.](http://arxiv.org/abs/2208.08934) | 本文提出了一种名为FedHSSL的联邦混合自监督学习框架，通过跨方视角和本地视角相结合，以解决纵向联邦学习中数据不足的问题，并且可以通过部分模型聚合进一步提高联合模型的性能。 |
| [^243] | [Classification of Stress via Ambulatory ECG and GSR Data.](http://arxiv.org/abs/2208.04705) | 本文利用机器学习分类器和生理数据对步态环境下的压力进行了检测，为个体监控心理健康提供了可能。 |
| [^244] | [Mitigating Propagation Failures in Physics-informed Neural Networks using Retain-Resample-Release (R3) Sampling.](http://arxiv.org/abs/2207.02338) | 本文提出了一种新的Retain-Resample-Release采样（R3）算法，通过减轻传播失败，使得物理信息神经网络（PINN）能够成功地从边界点传播解决方案到内部点。 |
| [^245] | [Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework.](http://arxiv.org/abs/2207.01955) | 本文提出了一种新颖的主动顾问演员-评论家框架，Ask-AC，它替换了传统的被动监督信号机制，实现了定制化和高效的信息交换，其中的两个互补组件允许代理主动寻求顾问干预和识别漏掉的不稳定状态。 |
| [^246] | [Efficient computation of rankings from pairwise comparisons.](http://arxiv.org/abs/2207.00076) | 本文提出了一种比已有迭代算法运行速度快100倍的基于成对比较的排名高效计算方法。 |
| [^247] | [Evaluating Self-Supervised Learning for Molecular Graph Embeddings.](http://arxiv.org/abs/2206.08005) | 评估分子图嵌入的自监督学习方法的MOLGRAPHEVAL揭示了在不同的下游任务中GSSL方法性能存在显著的不一致性，并为未来研究提供了新的方向。 |
| [^248] | [Quadratic models for understanding neural network dynamics.](http://arxiv.org/abs/2205.11787) | 神经二次模型可以展示出神经网络在大学习率情况下的“弹弓阶段”，并且在泛化特性上与神经网络有相似之处，是分析神经网络的有效工具。 |
| [^249] | [Transition to Linearity of General Neural Networks with Directed Acyclic Graph Architecture.](http://arxiv.org/abs/2205.11786) | 本文阐明具有任意有向无环图的神经网络，在宽度无限增大的情况下有线性转化的趋势。结果揭示了转化为线性的数学结构，并推广了一系列关于标准架构神经切向核的线性转化或恒定性的最新研究。 |
| [^250] | [Toward Enhanced Robustness in Unsupervised Graph Representation Learning: A Graph Information Bottleneck Perspective.](http://arxiv.org/abs/2201.08557) | 提出一种基于信息瓶颈原理的无偏鲁棒图信息瓶颈(RGIB)方法，尝试通过保留良性图中的原始信息并消除对抗性图中的对抗性信息，来学习对抗性扰动下的鲁棒节点表示。 |
| [^251] | [Value Functions Factorization with Latent State Information Sharing in Decentralized Multi-Agent Policy Gradients.](http://arxiv.org/abs/2201.01247) | 在QMIX方法的基础上，提出了LSF-SAC框架，其中包括一个潜在信息共享机制，可显著扩展价值函数分解的能力，同时在完全分散执行中保持了有效性。 |
| [^252] | [Multitask Learning and Bandits via Robust Statistics.](http://arxiv.org/abs/2112.14233) | 本研究探讨了多任务学习以及Bandits方法的健壮统计学实现，提出了一种新颖的两阶段多任务学习估计器，该估计器以一种样本高效的方式利用共享全局参数和稀疏实例特定术语的结构。 |
| [^253] | [Beyond Parallel Pancakes: Quasi-Polynomial Time Guarantees for Non-Spherical Gaussian Mixtures.](http://arxiv.org/abs/2112.05445) | 本文针对具有未知均值和未知协方差的$k\geq2$的高斯组分混合物，提出了一种准多项式时间内可靠区分该混合物与纯高斯分布的算法，并且该算法仅需要在多项式下界混合重量的情况下运行。 |
| [^254] | [Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework.](http://arxiv.org/abs/2110.15317) | 该论文提出了一种针对文本对抗攻击的框架，通过在嵌入层上持续优化扰动并放大这些扰动，使用遮罩语言模型头对最终扰动的潜在代表进行解码，以获取可能的对抗样本，进行了广泛的评估，并在各种语言任务上取得了制造近乎不可察觉的通用和定向文本对抗样本的最新技术水平。 |
| [^255] | [Approximate Newton policy gradient algorithms.](http://arxiv.org/abs/2110.02398) | 本文提出了一种用于政策梯度算法的近似牛顿方法，包括自然策略梯度算法和全新的策略梯度算法，具有快速收敛的优势。 |
| [^256] | [Reinforcement Learning Policies in Continuous-Time Linear Systems.](http://arxiv.org/abs/2109.07630) | 本研究旨在解决模型不确定性下连续时间线性系统最优操作的问题，提出在线策略学习最优操作，平衡探索与开发的挑战，实现飞行控制任务的有效性，并证明了在不精确的系统动态下的尖锐稳定性结果，以及次优情况下微小后悔的详细说明。 |
| [^257] | [Cyclic Coordinate Dual Averaging with Extrapolation.](http://arxiv.org/abs/2102.13244) | 本文提出了一种新的块坐标方法，适用于变分不等式问题，并提供了收敛边界。通过使用这种方法，我们得到了关于Mahalanobis范数的梯度Lipschitz条件。这为复合凸优化问题和凸-凹极小极大优化问题的解决提供了新的工具。 |
| [^258] | [Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2102.03479) | 本文研究了QMIX算法的变体和单调性约束的实现技巧，发现标准化优化对SMAC环境的表现有显著影响；单调性约束能提高SMAC和DEPP的采样效率。 |
| [^259] | [Attentional-Biased Stochastic Gradient Descent.](http://arxiv.org/abs/2012.06951) | 本文提出了一种名为ABSGD的简单而有效的方法，用于解决深度学习中的数据不平衡或标签噪声问题。我们为每个样本分配一个个别重要性权重，该权重系统地与数据的损失值成比例，并且可以捕捉到每个类中个别示例之间的多样性。与现有的加权方案相比，我们的方法可以减轻计算负担并在分布鲁棒优化 (DRO) 框架中解释为正则化参数。 |
| [^260] | [MALTS: Matching After Learning to Stretch.](http://arxiv.org/abs/1811.07415) | 我们提出了一个灵活的匹配算法MALTS，通过学习可解释的、基于重要协变量的距离度量，在因果推断中生成高质量的几乎精确匹配。 |

# 详细

[^1]: 基于扩散模型的卫星视频异常检测

    Anomaly Detection in Satellite Videos using Diffusion Models. (arXiv:2306.05376v1 [cs.CV])

    [http://arxiv.org/abs/2306.05376](http://arxiv.org/abs/2306.05376)

    本文提出基于扩散模型的卫星视频异常检测方法，效果优于其他方法，能够快速捕捉移动异常，具有实时检测灾害的应用前景。

    

    异常检测的定义是识别意外事件。使用卫星数据实时检测极端事件（如野火、气旋或洪水）在灾害管理中变得至关重要。尽管有几个地球观测卫星提供有关灾害的信息，但地球静止轨道上的卫星每分钟能够提供数据，从实质上创建了太空视频。已经提出了许多技术来识别监视视频中的异常，然而可用的数据集没有动态行为，所以我们讨论了一种异常框架，可以处理非常高频的数据集以查找非常快速移动的异常。在这项工作中，我们提出了一种扩散模型，它不需要任何运动组件来捕捉快速移动的异常，且表现优于其他基线方法。

    The definition of anomaly detection is the identification of an unexpected event. Real-time detection of extreme events such as wildfires, cyclones, or floods using satellite data has become crucial for disaster management. Although several earth-observing satellites provide information about disasters, satellites in the geostationary orbit provide data at intervals as frequent as every minute, effectively creating a video from space. There are many techniques that have been proposed to identify anomalies in surveillance videos; however, the available datasets do not have dynamic behavior, so we discuss an anomaly framework that can work on very high-frequency datasets to find very fast-moving anomalies. In this work, we present a diffusion model which does not need any motion component to capture the fast-moving anomalies and outperforms the other baseline methods.
    
[^2]: 面向源代码漏洞识别的顺序图神经网络

    Sequential Graph Neural Networks for Source Code Vulnerability Identification. (arXiv:2306.05375v1 [cs.CR])

    [http://arxiv.org/abs/2306.05375](http://arxiv.org/abs/2306.05375)

    本文提出了一个经过适当策划的 C/C++ 源代码漏洞数据集，以帮助开发模型，同时还提出了一种基于图神经网络的学习框架，称为 SEquential Graph Neural Network（SEGNN），用于学习大量代码语义表示。在公开数据集上的实验结果表明，SEGNN 方法在准确性和 F1得分方面优于现有最先进的方法。

    

    漏洞识别对于网络安全至关重要。在大型应用中查找和修复易受攻击的函数非常有帮助。然而，由于缺乏可靠和充分管理的数据集和学习模型，这项任务相当具有挑战性。现有的解决方案通常依赖于人类专业知识来注释数据集或指定特征，这容易出现错误。此外，学习模型具有较高的误报率。为了弥补这个差距，本文提出了一个经过适当策划的C/C++源代码漏洞数据集CVEFunctionGraphEmbeddings（CVEFGE），以帮助开发模型。CVEFGE自动从CVE数据库中爬取，其中包含真实和公开披露的源代码漏洞。我们还提出了一种基于图神经网络的学习框架，称为SEquential Graph Neural Network（SEGNN），用于学习大量代码语义表示。SEGNN由两个堆叠的图神经网络（GNN）解码器层组成，用于模拟源代码中函数的结构和语义信息。我们在公开数据集上评估了我们的模型，结果表明，我们的方法在准确性和F1得分方面优于现有最先进的方法。

    Vulnerability identification constitutes a task of high importance for cyber security. It is quite helpful for locating and fixing vulnerable functions in large applications. However, this task is rather challenging owing to the absence of reliable and adequately managed datasets and learning models. Existing solutions typically rely on human expertise to annotate datasets or specify features, which is prone to error. In addition, the learning models have a high rate of false positives. To bridge this gap, in this paper, we present a properly curated C/C++ source code vulnerability dataset, denoted as CVEFunctionGraphEmbeddings (CVEFGE), to aid in developing models. CVEFGE is automatically crawled from the CVE database, which contains authentic and publicly disclosed source code vulnerabilities. We also propose a learning framework based on graph neural networks, denoted SEquential Graph Neural Network (SEGNN) for learning a large number of code semantic representations. SEGNN consists
    
[^3]: 最高法院口头辩论的计算分析

    A Computational Analysis of Oral Argument in the Supreme Court. (arXiv:2306.05373v1 [cs.CY])

    [http://arxiv.org/abs/2306.05373](http://arxiv.org/abs/2306.05373)

    本文提出了一个计算分析的方法来研究最高法院的口头辩论。结果发现，口头辩论是一个高度结构化和战略性的仪式，律师使用各种修辞策略来推进他们的案件，而法官则通过提问和评论不仅塑造他们面前的案件，而且塑造了法院更广泛的法律原则。

    

    作为最高法院决策过程中最为公开的部分，口头辩论在大众媒体中受到了非常多的关注。然而，作为一个机构，口头辩论的基本功能和作用仍然不为人们所熟知。过去的研究往往集中在口头辩论的离散量化属性上，如对每位辩护律师提问的次数、法官任命总统的党派和上诉案件的意识形态影响等，但这些研究很少涉及到口头辩论的具体过程和更广泛的制度和规范作用。本文提出了一个基于1955年至2018年语料库的最高法院口头辩论的计算分析方法，强调口头辩论的质性和交互方面，旨在揭示在辩论中律师和法官所采用的修辞策略的模式和倾向性。我们的研究表明，口头辩论是一个高度结构化和战略性的仪式，律师使用各种修辞策略来推进他们的案件，而法官则通过提问和评论不仅塑造他们面前的案件，而且塑造了法院更广泛的法律原则。

    As the most public component of the Supreme Court's decision-making process, oral argument receives an out-sized share of attention in the popular media. Despite its prominence, however, the basic function and operation of oral argument as an institution remains poorly understood, as political scientists and legal scholars continue to debate even the most fundamental questions about its role.  Past study of oral argument has tended to focus on discrete, quantifiable attributes of oral argument, such as the number of questions asked to each advocate, the party of the Justices' appointing president, or the ideological implications of the case on appeal. Such studies allow broad generalizations about oral argument and judicial decision making: Justices tend to vote in accordance with their ideological preferences, and they tend to ask more questions when they are skeptical of a party's position. But they tell us little about the actual goings on at oral argument -- the running dialog betw
    
[^4]: 序数势函数的玩家评级方法

    Ordinal Potential-based Player Rating. (arXiv:2306.05366v1 [cs.GT])

    [http://arxiv.org/abs/2306.05366](http://arxiv.org/abs/2306.05366)

    该论文提出了一种使用序数势函数的玩家评级方法，通过可逆映射的嵌套计算，能够保持传递性。

    

    如果对于任意纯策略$x$、$y$和$z$，如果$x$比$y$更好，$y$比$z$更好，则$x$比$z$更好，则两个对称的零和博弈是可传递的。最近观察到，Elo评级未能保持策略之间的传递关系，因此不能正确提取游戏的传递组件。我们的第一个贡献是表明当在正确的空间中计算Elo评级时，Elo评级确实能够保持传递性。具体而言，我们首先使用合适的可逆映射$\varphi$将游戏应用于$\varphi$，然后计算Elo评级，最后通过应用$\varphi^{-1}$回到原始空间。我们将可传递游戏的表征为势游戏的一个弱变体，其势函数是加性可分离的。利用这一洞见，我们引入了传递序数的概念，即将可传递游戏的收益转化为其差异所需的最小可逆映射数。

    A two-player symmetric zero-sum game is transitive if for any pure strategies $x$, $y$, $z$, if $x$ is better than $y$, and $y$ is better than $z$, then $x$ is better than $z$. It was recently observed that the Elo rating fails at preserving transitive relations among strategies and therefore cannot correctly extract the transitive component of a game. Our first contribution is to show that the Elo rating actually does preserve transitivity when computed in the right space. Precisely, using a suitable invertible mapping $\varphi$, we first apply $\varphi$ to the game, then compute Elo ratings, then go back to the original space by applying $\varphi^{-1}$. We provide a characterization of transitive games as a weak variant of ordinal potential games with additively separable potential functions. Leveraging this insight, we introduce the concept of transitivity order, the minimum number of invertible mappings required to transform the payoff of a transitive game into (differences of) its
    
[^5]: 通过IF-PCA和一些最新的方法进行主题聚类

    Subject clustering by IF-PCA and several recent methods. (arXiv:2306.05363v1 [stat.ME])

    [http://arxiv.org/abs/2306.05363](http://arxiv.org/abs/2306.05363)

    本文结合了无监督深度学习方法和有影响特征PCA方法，提出了IF-VAE作为一种新的主题聚类方法，该方法在基因芯片和单细胞RNA-seq数据集方面的表现较好。

    

    主题聚类是一个非常感兴趣的问题，其中包括使用测量特征将受试者（如患者或细胞）聚类成多个组的方法。近年来，提出了许多方法，其中无监督深度学习（UDL）受到了极大关注。本文结合了流行的UDL方法变分自动编码器（VAE）和最新的有影响特征PCA（IF-PCA）想法，提出了IF-VAE作为一种新的主题聚类方法，详细研究了该方法，并将其与其他方法进行比较。实验结果表明，IF-VAE显著优于VAE，但仍不如IF-PCA。另外，在8个单细胞数据集上，IF-PCA稍微优于Seurat和SC3。

    Subject clustering (i.e., the use of measured features to cluster subjects, such as patients or cells, into multiple groups) is a problem of great interest. In recent years, many approaches were proposed, among which unsupervised deep learning (UDL) has received a great deal of attention. Two interesting questions are (a) how to combine the strengths of UDL and other approaches, and (b) how these approaches compare to one other.  We combine Variational Auto-Encoder (VAE), a popular UDL approach, with the recent idea of Influential Feature PCA (IF-PCA), and propose IF-VAE as a new method for subject clustering. We study IF-VAE and compare it with several other methods (including IF-PCA, VAE, Seurat, and SC3) on $10$ gene microarray data sets and $8$ single-cell RNA-seq data sets. We find that IF-VAE significantly improves over VAE, but still underperforms IF-PCA. We also find that IF-PCA is quite competitive, which slightly outperforms Seurat and SC3 over the $8$ single-cell data sets. 
    
[^6]: 面向高级驾驶辅助系统中听不见的指令攻击的可信传感器融合

    Trustworthy Sensor Fusion against Inaudible Command Attacks in Advanced Driver-Assistance System. (arXiv:2306.05358v1 [cs.CR])

    [http://arxiv.org/abs/2306.05358](http://arxiv.org/abs/2306.05358)

    本文提出了多模态融合框架（MFF），利用VGG系列神经网络实现异构音频-视觉模态的融合。在实验中，MFF实现了92.25%的检测准确率，并证明MFF在不同模型不确定性条件下具有可靠性。

    

    自动驾驶车辆遭受恶意攻击的担忧日益增加。其中，听不见的语音指令攻击构成了重大威胁，因为语音指令已经在自动驾驶系统中得到了应用。如何通过实证研究来抵御这些听不见的攻击仍然是一个未解决的问题。以往的研究探讨运用基于深度学习的多模态融合来进行防御，却没有考虑模型可信度的不确定性。随着深度学习应用于越来越敏感的任务，不确定性测量在帮助提高模型稳健性方面尤为重要，特别是在关键任务场景中。本文提出了多模态融合框架（MFF）作为智能安全系统来防范听不见的语音指令攻击。MFF利用VGG系列神经网络融合异构音频-视觉模态，并在比较融合方法的实证研究中实现了92.25%的检测准确率。此外，对MFF进行了大量实验，以证明其在不同模型不确定性条件下的可靠性。

    There are increasing concerns about malicious attacks on autonomous vehicles. In particular, inaudible voice command attacks pose a significant threat as voice commands become available in autonomous driving systems. How to empirically defend against these inaudible attacks remains an open question. Previous research investigates utilizing deep learning-based multimodal fusion for defense, without considering the model uncertainty in trustworthiness. As deep learning has been applied to increasingly sensitive tasks, uncertainty measurement is crucial in helping improve model robustness, especially in mission-critical scenarios. In this paper, we propose the Multimodal Fusion Framework (MFF) as an intelligent security system to defend against inaudible voice command attacks. MFF fuses heterogeneous audio-vision modalities using VGG family neural networks and achieves the detection accuracy of 92.25% in the comparative fusion method empirical study. Additionally, extensive experiments on
    
[^7]: 无监督的文本到图像生成模型下的组合式概念发现

    Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models. (arXiv:2306.05357v1 [cs.CV])

    [http://arxiv.org/abs/2306.05357](http://arxiv.org/abs/2306.05357)

    本论文提出了一种无监督的方法，用于从图像中自动地发现不同的生成概念，并且这些生成概念可以被用于重新组合和生成新的艺术和混合图像，并作为一种表示用于下游的分类任务。

    

    文本到图像生成模型使得在不同领域实现高分辨率的图像合成成为可能，但需要用户指定他们想要生成的内容。本文考虑了相反的问题——在给出的不同图像集合中，我们能否发现代表每个图像的生成概念？我们提出了一种无监督的方法来从一组图像中发现生成的概念，将绘画中不同的艺术风格，对象和照明从厨房场景中分解出来，并通过给定的ImageNet图像发现图像类。我们展示了这样的生成概念能够准确地表示图像的内容，能够重新组合和组成以生成新的艺术和混合图像，并可以作为下游分类任务的一种表示来使用。

    Text-to-image generative models have enabled high-resolution image synthesis across different domains, but require users to specify the content they wish to generate. In this paper, we consider the inverse problem -- given a collection of different images, can we discover the generative concepts that represent each image? We present an unsupervised approach to discover generative concepts from a collection of images, disentangling different art styles in paintings, objects, and lighting from kitchen scenes, and discovering image classes given ImageNet images. We show how such generative concepts can accurately represent the content of images, be recombined and composed to generate new artistic and hybrid images, and be further used as a representation for downstream classification tasks.
    
[^8]: 水晶材料性能预测的晶体特异性预训练框架

    A Crystal-Specific Pre-Training Framework for Crystal Material Property Prediction. (arXiv:2306.05344v1 [cs.LG])

    [http://arxiv.org/abs/2306.05344](http://arxiv.org/abs/2306.05344)

    本论文提出了一种基于自监督学习的晶体特异性预训练框架，该框架设计了一种互斥掩码策略，缓解了晶体物性预测中存在的标记受限问题，并增强了表示学习。同时，该框架还考虑了晶体结构中的周期性不变性，开发了周期性不变的多图模块和周期特性学习。

    

    水晶物性预测是开发新材料的重要方面。然而，为了加速晶体的研究，需要解决两个技术难题：其一，标记晶体物性本质上是困难的，因为需要进行昂贵耗时的物理模拟或实验；其二，晶体遵循特定的量子化学原理，即周期性不变性，往往无法被现有的机器学习方法捕捉到。为了克服这些挑战，我们提出了一种基于自监督学习的晶体特异性预训练框架，用于学习晶体表示。框架设计了一种互斥掩码策略，增强了表示学习，缓解了晶体物性预测中存在的标记受限问题。此外，我们还考虑了晶体结构中的周期性不变性，开发了周期性不变的多图模块和周期特性学习。

    Crystal property prediction is a crucial aspect of developing novel materials. However, there are two technical challenges to be addressed for speeding up the investigation of crystals. First, labeling crystal properties is intrinsically difficult due to the high cost and time involved in physical simulations or lab experiments. Second, crystals adhere to a specific quantum chemical principle known as periodic invariance, which is often not captured by existing machine learning methods. To overcome these challenges, we propose the crystal-specific pre-training framework for learning crystal representations with self-supervision. The framework designs a mutex mask strategy for enhancing representation learning so as to alleviate the limited labels available for crystal property prediction. Moreover, we take into account the specific periodic invariance in crystal structures by developing a periodic invariance multi-graph module and periodic attribute learning within our framework. This 
    
[^9]: 在协变量转移下具有泛化保证的联邦学习

    Federated Learning under Covariate Shifts with Generalization Guarantees. (arXiv:2306.05325v1 [cs.LG])

    [http://arxiv.org/abs/2306.05325](http://arxiv.org/abs/2306.05325)

    本文提出了具有泛化保证的联邦学习方法FTW-ERM，在处理协变量转移方面表现出优势。

    

    本文解决了联邦学习中客户端内和客户端间协变量转移的问题，重点关注总体泛化性能。为了处理协变量转移，我们制定了一种新的全局模型训练范式，并提出了Federated Importance-Weighted Empirical Risk Minimization（FTW-ERM），改进了密度比匹配方法，而不需要完美知识来处理真实比率的上确界。我们还提出了通信高效的变体FITW-ERM，其隐私保证与经典ERM在FL中的隐私保证相同。我们在理论上证明了FTW-ERM在某些设置下比经典ERM达到更小的泛化误差。实验结果表明，在具有数据分布在客户端之间发生偏移的挑战性失衡联邦设置方面，FTW-ERM优于现有的FL基线。

    This paper addresses intra-client and inter-client covariate shifts in federated learning (FL) with a focus on the overall generalization performance. To handle covariate shifts, we formulate a new global model training paradigm and propose Federated Importance-Weighted Empirical Risk Minimization (FTW-ERM) along with improving density ratio matching methods without requiring perfect knowledge of the supremum over true ratios. We also propose the communication-efficient variant FITW-ERM with the same level of privacy guarantees as those of classical ERM in FL. We theoretically show that FTW-ERM achieves smaller generalization error than classical ERM under certain settings. Experimental results demonstrate the superiority of FTW-ERM over existing FL baselines in challenging imbalanced federated settings in terms of data distribution shifts across clients.
    
[^10]: 巨型语言模型在意大利生物医学信息提取方面的应用：方法论研究和实际应用的多中心实践

    Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application. (arXiv:2306.05323v1 [cs.CL])

    [http://arxiv.org/abs/2306.05323](http://arxiv.org/abs/2306.05323)

    该研究创建了意大利神经精神命名实体识别数据集，并使用巨型语言模型开发出多中心识别模型，整体 F1得分为84.77%。该模型将帮助临床从业者从非结构化的医疗记录中自动提取信息。

    

    医院引入计算机化医疗记录有助于减少手写和信息提取等繁琐操作。然而，由于从非结构化文本医疗记录中提取数据需要时间和精力，因此医疗记录中包含的数据仍然被充分利用程度低。自然语言处理的子领域信息提取可以帮助临床从业者克服这一限制，使用自动化文本挖掘流程。在这项工作中，我们创建了意大利神经精神命名实体识别数据集 PsyNIT，并使用它来开发这一任务的巨型语言模型。此外，我们还进行了多个实验，使用三个外部独立数据集来实现有效的多中心模型，整体 F1 得分为 84.77%，精确率为 83.16%，召回率为 86.44%。我们学到的经验是: (i) 一致的注释过程的关键作用和 (ii) 结合经典方法和“少量训练”的 fine-tuning 策略。

    The introduction of computerized medical records in hospitals has reduced burdensome operations like manual writing and information fetching. However, the data contained in medical records are still far underutilized, primarily because extracting them from unstructured textual medical records takes time and effort. Information Extraction, a subfield of Natural Language Processing, can help clinical practitioners overcome this limitation, using automated text-mining pipelines. In this work, we created the first Italian neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to develop a Large Language Model for this task. Moreover, we conducted several experiments with three external independent datasets to implement an effective multicenter model, with overall F1-score 84.77%, Precision 83.16%, Recall 86.44%. The lessons learned are: (i) the crucial role of a consistent annotation process and (ii) a fine-tuning strategy that combines classical methods with a "few-shot" a
    
[^11]: 使用潜变量神经常微分方程的实时全心脏电机仿真

    Real-time whole-heart electromechanical simulations using Latent Neural Ordinary Differential Equations. (arXiv:2306.05321v1 [math.NA])

    [http://arxiv.org/abs/2306.05321](http://arxiv.org/abs/2306.05321)

    该论文使用潜变量神经常微分方程(LNODEs)实现了实时全心脏电机仿真，并开发出了一个代理模型，可快速预测对不同肌力干预的压力-容积曲线，为患者特异性诊断和治疗优化提供了潜在快速工具。

    

    心脏数字孪生提供了一种基于物理和生理学的框架，以提供预测性和个性化医学。然而，高保真度的多尺度心脏模型仍然是采用的障碍，因为它们的广泛计算成本以及需要大量的模型评估进行特定于患者的个性化。基于人工智能的方法可以使创建快速准确的全心数字孪生成为可能。在这项工作中，我们使用潜变量神经常微分方程(LNODEs)来学习心力衰竭患者的时间压力-容积动力学。我们基于LNODEs的代理模型是从400个三维零维全心闭环电机仿真中进行训练的，同时考虑43个模型参数，描述单个细胞到整个器官和心血管血液动力学。经过训练的LNODE提供了3D-0D模型在潜空间中的紧凑且高效的表示，通过前馈全连接的人工神经网络。我们展示了LNODE的代理可以对不同的肌力介入做出实时预测，提供了一种可能的快速工具，用于特定于患者的诊断和治疗优化。

    Cardiac digital twins provide a physics and physiology informed framework to deliver predictive and personalized medicine. However, high-fidelity multi-scale cardiac models remain a barrier to adoption due to their extensive computational costs and the high number of model evaluations needed for patient-specific personalization. Artificial Intelligence-based methods can make the creation of fast and accurate whole-heart digital twins feasible. In this work, we use Latent Neural Ordinary Differential Equations (LNODEs) to learn the temporal pressure-volume dynamics of a heart failure patient. Our surrogate model based on LNODEs is trained from 400 3D-0D whole-heart closed-loop electromechanical simulations while accounting for 43 model parameters, describing single cell through to whole organ and cardiovascular hemodynamics. The trained LNODEs provides a compact and efficient representation of the 3D-0D model in a latent space by means of a feedforward fully-connected Artificial Neural 
    
[^12]: 基于RNN的GNSS定位系统使用卫星测量特征和伪距残差

    RNN-Based GNSS Positioning using Satellite Measurement Features and Pseudorange Residuals. (arXiv:2306.05319v1 [eess.SP])

    [http://arxiv.org/abs/2306.05319](http://arxiv.org/abs/2306.05319)

    本文利用循环神经网络和自定义矩阵处理卫星测量特征和伪距残差，以提高全球导航卫星系统(GNSS)的定位精度

    

    在全球导航卫星系统(GNSS)中，可用卫星的数量增加导致在选择最准确的伪距贡献方面面临许多挑战，因为有偏差的测量对定位精度影响很大，特别是在单时刻场景中。该文利用机器学习在预测链路测量质量因子方面的潜力，从而优化测量加权。为此，我们使用了一个定制的矩阵，由条件伪距残差和每个链路卫星指标(例如载波噪声功率密度比及其经验统计量、卫星高度、载波相位锁定时间)组成。然后，将该矩阵作为输入提供给循环神经网络(RNN)（即长短期记忆(LSTM)网络）进行处理。

    In the Global Navigation Satellite System (GNSS) context, the growing number of available satellites has lead to many challenges when it comes to choosing the most accurate pseudorange contributions, given the strong impact of biased measurements on positioning accuracy, particularly in single-epoch scenarios. This work leverages the potential of machine learning in predicting link-wise measurement quality factors and, hence, optimize measurement weighting. For this purpose, we use a customized matrix composed of heterogeneous features such as conditional pseudorange residuals and per-link satellite metrics (e.g., carrier-to-noise power density ratio and its empirical statistics, satellite elevation, carrier phase lock time). This matrix is then fed as an input to a recurrent neural network (RNN) (i.e., a long-short term memory (LSTM) network). Our experimental results on real data, obtained from extensive field measurements, demonstrate the high potential of our proposed solution bein
    
[^13]: 动态训练和适应深度强化学习模型到不同的、低计算的和不断变化的放射学部署环境的框架

    A framework for dynamically training and adapting deep reinforcement learning models to different, low-compute, and continuously changing radiology deployment environments. (arXiv:2306.05310v1 [cs.LG])

    [http://arxiv.org/abs/2306.05310](http://arxiv.org/abs/2306.05310)

    本研究开发了三种图像核心算法，以对选择性经验回放强化学习的医学图像进行压缩和去噪，以适应不断变化的计算和成像环境。最大熵核心算法以较低压缩比获得最佳表现。

    

    尽管Deep Reinforcement Learning在医学影像方面得到广泛研究，但这些模型的训练和部署通常需要强大的GPU。由于影像环境的快速演变和边缘设备的普及，算法需要持续学习和适应不断变化的环境，为低计算设备进行适配。为此，我们开发了三种图像核心算法，以对选择性经验回放强化学习的医学图像进行压缩和去噪。我们在全身DIXON水和DIXON脂肪MRI图像上实施了邻域平均核心算法、邻域灵敏度采样核心算法和最大熵核心算法。所有三种核心算法都能够以27倍的压缩率在两种成像环境下定位5个解剖标志：左膝、右转子、左肾、脾脏和肺。最大熵核心算法在平均距离误差方面获得了最佳表现，为$11.97\pm12.02$。

    While Deep Reinforcement Learning has been widely researched in medical imaging, the training and deployment of these models usually require powerful GPUs. Since imaging environments evolve rapidly and can be generated by edge devices, the algorithm is required to continually learn and adapt to changing environments, and adjust to low-compute devices. To this end, we developed three image coreset algorithms to compress and denoise medical images for selective experience replayed-based lifelong reinforcement learning. We implemented neighborhood averaging coreset, neighborhood sensitivity-based sampling coreset, and maximum entropy coreset on full-body DIXON water and DIXON fat MRI images. All three coresets produced 27x compression with excellent performance in localizing five anatomical landmarks: left knee, right trochanter, left kidney, spleen, and lung across both imaging environments. Maximum entropy coreset obtained the best performance of $11.97\pm 12.02$ average distance error,
    
[^14]: 机器学习中公平度指标评估歧视偏差是否足够？

    Are fairness metric scores enough to assess discrimination biases in machine learning?. (arXiv:2306.05307v1 [cs.CL])

    [http://arxiv.org/abs/2306.05307](http://arxiv.org/abs/2306.05307)

    本文在机器学习算法在文本数据上对性别歧视的偏见的度量标准的局限性上进行了实验研究。

    

    本文提出了新的实验，揭示了当前用于评估机器学习算法在文本数据上对性别歧视的偏见的度量标准的局限性。我们以Bios数据集为例，学习预测个人的职业。这种预测任务在商业自然语言处理（NLP）应用程序中很常见，例如自动工作推荐。

    This paper presents novel experiments shedding light on the shortcomings of current metrics for assessing biases of gender discrimination made by machine learning algorithms on textual data. We focus on the Bios dataset, and our learning task is to predict the occupation of individuals, based on their biography. Such prediction tasks are common in commercial Natural Language Processing (NLP) applications such as automatic job recommendations. We address an important limitation of theoretical discussions dealing with group-wise fairness metrics: they focus on large datasets, although the norm in many industrial NLP applications is to use small to reasonably large linguistic datasets for which the main practical constraint is to get a good prediction accuracy. We then question how reliable are different popular measures of bias when the size of the training set is simply sufficient to learn reasonably accurate predictions. Our experiments sample the Bios dataset and learn more than 200 m
    
[^15]: 图上函数的贝叶斯优化

    Bayesian Optimisation of Functions on Graphs. (arXiv:2306.05304v1 [cs.LG])

    [http://arxiv.org/abs/2306.05304](http://arxiv.org/abs/2306.05304)

    本论文提出了一种在通用的大规模和潜在未知图上定义函数的贝叶斯优化算法，并通过学习适当的图内核，适应目标函数行为。

    

    图结构数据的不断涌现推动了在图节点集上定义函数的优化任务。传统的图搜索算法可用于此，但它们可能样本效率低下，并且不利用关于函数值的信息；另一方面，贝叶斯优化是一类有前途的黑盒求解器，具有更高的样本效率，但它很少被应用于这样的新颖设置。为了填补这一空白，我们提出了一种新颖的贝叶斯优化框架，该框架优化在通用，大规模和潜在的未知图上定义的函数。通过学习适当的图内核，我们的框架具有适应目标函数行为的优点。局部建模方法进一步保证了我们方法的效率。在合成和真实世界图上的大量实验表明了所提出的优化框架的有效性。

    The increasing availability of graph-structured data motivates the task of optimising over functions defined on the node set of graphs. Traditional graph search algorithms can be applied in this case, but they may be sample-inefficient and do not make use of information about the function values; on the other hand, Bayesian optimisation is a class of promising black-box solvers with superior sample efficiency, but it has been scarcely been applied to such novel setups. To fill this gap, we propose a novel Bayesian optimisation framework that optimises over functions defined on generic, large-scale and potentially unknown graphs. Through the learning of suitable kernels on graphs, our framework has the advantage of adapting to the behaviour of the target function. The local modelling approach further guarantees the efficiency of our method. Extensive experiments on both synthetic and real-world graphs demonstrate the effectiveness of the proposed optimisation framework.
    
[^16]: 基于Epoch的随机梯度下降中相关噪声：权重方差的影响

    Correlated Noise in Epoch-Based Stochastic Gradient Descent: Implications for Weight Variances. (arXiv:2306.05300v1 [cs.LG])

    [http://arxiv.org/abs/2306.05300](http://arxiv.org/abs/2306.05300)

    研究挑战了在时间上是不相关的假设，并强调了epoch-based噪声相关性对离散时间带动量的SGD的权重方差的影响。

    

    随机梯度下降（SGD）已成为神经网络优化的基石，但认为SGD引入的噪声在时间上是不相关的，尽管epoch-based训练是无处不在的。在这项工作中，我们对此进行了挑战，并调查了epoch-based噪声相关性对离散时间带动量的SGD的稳态分布的影响，限于二次损失。我们的主要贡献有两个：首先，我们计算训练epoch时噪声的精确自相关性，假设该噪声独立于权重向量中的小波动;其次，我们探索epoch-based学习方案引入的相关性对SGD动态的影响。我们发现，在曲率大于一个超参数相关值的方向上，还原了不相关噪声的结果。然而，在相对平坦的方向上，权重方差显着减小。我们使用简单的二维图例对这些结果进行了直观解释。总的来说，我们的工作提供了关于epoch-based SGD中相关噪声影响的见解，可以指导设计更有效的优化算法。

    Stochastic gradient descent (SGD) has become a cornerstone of neural network optimization, yet the noise introduced by SGD is often assumed to be uncorrelated over time, despite the ubiquity of epoch-based training. In this work, we challenge this assumption and investigate the effects of epoch-based noise correlations on the stationary distribution of discrete-time SGD with momentum, limited to a quadratic loss. Our main contributions are twofold: first, we calculate the exact autocorrelation of the noise for training in epochs under the assumption that the noise is independent of small fluctuations in the weight vector; second, we explore the influence of correlations introduced by the epoch-based learning scheme on SGD dynamics. We find that for directions with a curvature greater than a hyperparameter-dependent crossover value, the results for uncorrelated noise are recovered. However, for relatively flat directions, the weight variance is significantly reduced. We provide an intui
    
[^17]: 部分标记数据的深度学习用于射频地图重建

    Deep Learning with Partially Labeled Data for Radio Map Reconstruction. (arXiv:2306.05294v1 [eess.SP])

    [http://arxiv.org/abs/2306.05294](http://arxiv.org/abs/2306.05294)

    本篇论文解决了利用侧面信息重建接收信号强度地图的问题，采用部分标记数据的深度学习。使用额外的侧面信息可以提高在三个主要城市数据集中重建结果的准确性。

    

    本篇论文解决了基于位置相关的无线电测量和利用关于本地区域的侧面知识（例如城市规划，地形高度，网关位置）的问题，以重建接收信号强度地图。根据此类先前侧面信息的数量，我们采用神经架构搜索来找到最佳神经网络模型的最佳架构。我们证明，在三个对应于三个主要城市的数据集中，使用额外的侧面信息可以增强接收信号强度地图重建的最终准确性，特别是在网关附近的子区域，因为通常观察到平均接收信号功率的变化较大。

    In this paper, we address the problem of Received Signal Strength map reconstruction based on location-dependent radio measurements and utilizing side knowledge about the local region; for example, city plan, terrain height, gateway position. Depending on the quantity of such prior side information, we employ Neural Architecture Search to find an optimized Neural Network model with the best architecture for each of the supposed settings. We demonstrate that using additional side information enhances the final accuracy of the Received Signal Strength map reconstruction on three datasets that correspond to three major cities, particularly in sub-areas near the gateways where larger variations of the average received signal power are typically observed.
    
[^18]: 安全的协同过滤

    Safe Collaborative Filtering. (arXiv:2306.05292v1 [cs.IR])

    [http://arxiv.org/abs/2306.05292](http://arxiv.org/abs/2306.05292)

    本论文提出了一个安全的协同过滤算法，通过最小化条件风险价值，提高低满意度用户的推荐质量。在实际数据集中表现出色，同时也保持总体推荐质量。

    

    对于现代机器学习任务，例如算法公平性、类别不平衡和风险敏感的决策制定，优秀的尾部性能非常重要，因为它确保了对数据集中具有挑战性的样本的有效处理。尾部性能也是个性化推荐系统成功的重要决定因素，以减少对低满意度用户的流失风险。本研究介绍了一种“安全”的协同过滤方法，该方法优先考虑低满意度用户的推荐质量，而不是关注平均表现。我们的方法最小化条件风险价值（CVaR），表示用户损失尾部的平均风险。为了克服网络规模的推荐系统的计算难题，我们开发了一个强大而实用的算法，扩展了最可扩展的方法隐式交替最小二乘法（iALS）。在实际数据集的经验证明，我们的方法具有出色的尾部性能，同时保持了总体推荐质量。

    Excellent tail performance is crucial for modern machine learning tasks, such as algorithmic fairness, class imbalance, and risk-sensitive decision making, as it ensures the effective handling of challenging samples within a dataset. Tail performance is also a vital determinant of success for personalised recommender systems to reduce the risk of losing users with low satisfaction. This study introduces a "safe" collaborative filtering method that prioritises recommendation quality for less-satisfied users rather than focusing on the average performance. Our approach minimises the conditional value at risk (CVaR), which represents the average risk over the tails of users' loss. To overcome computational challenges for web-scale recommender systems, we develop a robust yet practical algorithm that extends the most scalable method, implicit alternating least squares (iALS). Empirical evaluation on real-world datasets demonstrates the excellent tail performance of our approach while maint
    
[^19]: 基于毫米波雷达传感器的一次学习式司机头部动作识别

    One shot learning based drivers head movement identification using a millimetre wave radar sensor. (arXiv:2306.05291v1 [eess.SP])

    [http://arxiv.org/abs/2306.05291](http://arxiv.org/abs/2306.05291)

    通过使用小型毫米波雷达传感器从司机的头部动作中收集信号，并基于一次学习技术设计分类器，可以高效地识别司机的不同动作类型，实验准确率超过95%。

    

    聚焦于交通安全，监测司机是否专注于驾驶是必要的。本文基于毫米波雷达应用，利用安装在车辆方向盘处的小型毫米波雷达从司机不同的头部动作收集信号，再通过设计基于雷达传感器的分类器来区分不同的动作类型。鉴于数据集较小，本文提出使用一次学习方法分类司机的四种头部动作。实验结果表明，使用该方法进行分类的准确率超过了95%。

    Concentration of drivers on traffic is a vital safety issue; thus, monitoring a driver being on road becomes an essential requirement. The key purpose of supervision is to detect abnormal behaviours of the driver and promptly send warnings to him her for avoiding incidents related to traffic accidents. In this paper, to meet the requirement, based on radar sensors applications, the authors first use a small sized millimetre wave radar installed at the steering wheel of the vehicle to collect signals from different head movements of the driver. The received signals consist of the reflection patterns that change in response to the head movements of the driver. Then, in order to distinguish these different movements, a classifier based on the measured signal of the radar sensor is designed. However, since the collected data set is not large, in this paper, the authors propose One shot learning to classify four cases of driver's head movements. The experimental results indicate that the pr
    
[^20]: 从血流动力学信号监测中预测和诊断中风的模型

    Predictive and diagnosis models of stroke from hemodynamic signal monitoring. (arXiv:2306.05289v1 [eess.SP])

    [http://arxiv.org/abs/2306.05289](http://arxiv.org/abs/2306.05289)

    本文基于血流动力学数据开发了实时诊断和预测模型，能够准确诊断中风亚型，预测患者的死亡率和中风复发。

    

    本文提出了一种新颖和有前景的急性中风临床管理方法。利用机器学习技术，我们的研究成功开发了从血流动力学数据中实时诊断和预测模型。这些模型能够在30分钟内诊断中风亚型，在监测的前3小时内预测死亡率，并且在仅15分钟的监测内预测中风复发。那些难以获取CT扫描的患者，以及抵达专科医院中风单元的所有患者都将从这些积极的结果中受益。实时开发的模型得到的结果如下：中风诊断准确率约为$98\%$（$97.8\%$敏感性，$99.5\%$特异性），预测死亡率的精度为$99.8\%$（$99.8\%$敏感性，$99.9\%$特异性），预测中风复发的精确度为$98\%$（$98\%$敏感性，$99\%$特异性）。

    This work presents a novel and promising approach to the clinical management of acute stroke. Using machine learning techniques, our research has succeeded in developing accurate diagnosis and prediction real-time models from hemodynamic data. These models are able to diagnose stroke subtype with 30 minutes of monitoring, to predict the exitus during the first 3 hours of monitoring, and to predict the stroke recurrence in just 15 minutes of monitoring. Patients with difficult access to a \acrshort{CT} scan, and all patients that arrive at the stroke unit of a specialized hospital will benefit from these positive results. The results obtained from the real-time developed models are the following: stroke diagnosis around $98\%$ precision ($97.8\%$ Sensitivity, $99.5\%$ Specificity), exitus prediction with $99.8\%$ precision ($99.8\%$ Sens., $99.9\%$ Spec.) and $98\%$ precision predicting stroke recurrence ($98\%$ Sens., $99\%$ Spec.).
    
[^21]: JGAT: 一种用于大脑解码的联合时空图形注意力模型

    JGAT: a joint spatio-temporal graph attention model for brain decoding. (arXiv:2306.05286v1 [q-bio.NC])

    [http://arxiv.org/abs/2306.05286](http://arxiv.org/abs/2306.05286)

    本文提出了一种联合时空图形注意力网络框架JGAT，集成了来自功能磁共振成像（fMRI）和扩散加权成像(DWI)的多模态数据，并保留动态信息，可用于大脑解码任务。

    

    大脑神经网络的解码一直是神经科学中令人着迷的课题，可以更全面地理解不同类型的脑部疾病和认知刺激。结合来自多模态成像技术的不同类型的连接（例如功能连接和结构连接）可以考虑它们的互补信息，因此具有更好的解码能力的潜力。然而，传统的整合功能连接和结构连接的方法忽略了动态变化，这很可能导致过度概括大脑神经网络。在本文中，我们提出了一种联合核图形注意力网络（JGAT），这是一种新的多模态时空图形注意力网络框架，同时保留动态信息，集成了功能磁共振成像（fMRI）和扩散加权成像（DWI）数据。我们使用JGAT在四个独立数据集上进行了脑解码任务。

    The decoding of brain neural networks has been an intriguing topic in neuroscience for a well-rounded understanding of different types of brain disorders and cognitive stimuli. Integrating different types of connectivity, e.g., Functional Connectivity (FC) and Structural Connectivity (SC), from multi-modal imaging techniques can take their complementary information into account and therefore have the potential to get better decoding capability. However, traditional approaches for integrating FC and SC overlook the dynamical variations, which stand a great chance to over-generalize the brain neural network. In this paper, we propose a Joint kernel Graph Attention Network (JGAT), which is a new multi-modal temporal graph attention network framework. It integrates the data from functional Magnetic Resonance Images (fMRI) and Diffusion Weighted Imaging (DWI) while preserving the dynamic information at the same time. We conduct brain-decoding tasks with our JGAT on four independent datasets
    
[^22]: 基于非监督统计特征引导扩散模型的传感器人体活动识别

    Unsupervised Statistical Feature-Guided Diffusion Model for Sensor-based Human Activity Recognition. (arXiv:2306.05285v1 [eess.SP])

    [http://arxiv.org/abs/2306.05285](http://arxiv.org/abs/2306.05285)

    本文提出了一种基于非监督统计特征引导扩散模型的传感器人体活动识别方法，通过生成多样化和代表性的合成传感器数据，从而解决了真实世界传感器数据的稀缺性和注释困难性，并在实验中获得了良好的性能表现。

    

    从传感器数据中识别人类活动是各个领域中的重要任务，但获得多样化和标记传感器数据仍然具有挑战性和成本高昂。本文提出了一种基于非监督统计特征引导扩散模型的传感器人体活动识别方法。所提出的方法旨在生成合成时间序列传感器数据，而不依赖于标记数据，从而解决了与真实世界传感器数据相关的稀缺性和注释困难性。通过将扩散模型的条件设置为统计信息（例如平均值、标准偏差、Z得分和偏度），我们生成了多样化和代表性的合成传感器数据。我们在公共人类活动识别数据集上进行了实验，并将所提出的方法与传统过采样方法和最先进的生成式对抗网络方法进行了比较。实验结果表明，所提出的方法可以改善人体活动识别的性能，并且优于其他方法。

    Recognizing human activities from sensor data is a vital task in various domains, but obtaining diverse and labeled sensor data remains challenging and costly. In this paper, we propose an unsupervised statistical feature-guided diffusion model for sensor-based human activity recognition. The proposed method aims to generate synthetic time-series sensor data without relying on labeled data, addressing the scarcity and annotation difficulties associated with real-world sensor data. By conditioning the diffusion model on statistical information such as mean, standard deviation, Z-score, and skewness, we generate diverse and representative synthetic sensor data. We conducted experiments on public human activity recognition datasets and compared the proposed method to conventional oversampling methods and state-of-the-art generative adversarial network methods. The experimental results demonstrate that the proposed method can improve the performance of human activity recognition and outper
    
[^23]: 简单且可控的音乐生成

    Simple and Controllable Music Generation. (arXiv:2306.05284v1 [cs.SD])

    [http://arxiv.org/abs/2306.05284](http://arxiv.org/abs/2306.05284)

    本文提出了 MusicGen，一个单一的语言模型，可以在条件描述或旋律特征控制下生成高质量的样本，并且在标准的文本到音乐基准上的实证研究中，该方法优于其他基线模型。

    

    本研究解决了条件音乐生成的问题。我们介绍了MusicGen，它是一个单一的语言模型，可以操作多个压缩离散音乐表示流，即令牌。与以往的工作不同，MusicGen由一个单一阶段的Transformer LM和高效的令牌交错模式组成，消除了级联多个模型的需要，例如分层或上采样。采用这种方法，我们展示了MusicGen如何在条件描述或旋律特征的控制下生成高质量的样本。我们进行了广泛的实证评估，考虑了自动和人为研究，展示了所提出的方法优于标准文本到音乐基准上评估的基线。通过消融研究，我们阐明了MusicGen所包含组件的重要性。音乐样本、代码和模型可以在https://github.com/fac找到。

    We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/fac
    
[^24]: 基于自相似特性的心脏杂音检测方法

    A Method for Detecting Murmurous Heart Sounds based on Self-similar Properties. (arXiv:2306.05283v1 [eess.SP])

    [http://arxiv.org/abs/2306.05283](http://arxiv.org/abs/2306.05283)

    该研究提出了一种基于心音自相似和复杂特性的判别性多尺度特征集，以小波域中的心音信号为基础。当应用于一个公开的心音数据集时，我们提出的基于小波的多尺度特征表现出与现有方法相当的性能。

    

    心脏杂音是由血液通过心脏流动时产生的一种非正常声音。它可能是严重心脏病的症状，因此检测心脏杂音对于识别和管理心血管疾病至关重要。本研究提出了一种基于心音自相似和复杂特性的判别性多尺度特征集，以小波域中的心音信号为基础。自相似性特征通过评估分形行为进行表征，而复杂性则通过计算小波熵进行探索。我们使用一组标准分类器评估了这些特征在检测心脏杂音方面的诊断性能。当应用于一个公开的心音数据集时，我们提出的基于小波的多尺度特征表现出与现有方法相当的性能。

    A heart murmur is an atypical sound produced by the flow of blood through the heart. It can be a sign of a serious heart condition, so detecting heart murmurs is critical for identifying and managing cardiovascular diseases. However, current methods for identifying murmurous heart sounds do not fully utilize the valuable insights that can be gained by exploring intrinsic properties of heart sound signals. To address this issue, this study proposes a new discriminatory set of multiscale features based on the self-similarity and complexity properties of heart sounds, as derived in the wavelet domain. Self-similarity is characterized by assessing fractal behaviors, while complexity is explored by calculating wavelet entropy. We evaluated the diagnostic performance of these proposed features for detecting murmurs using a set of standard classifiers. When applied to a publicly available heart sound dataset, our proposed wavelet-based multiscale features achieved comparable performance to ex
    
[^25]: 带有用户级差分隐私的联邦线性上下文强化学习

    Federated Linear Contextual Bandits with User-level Differential Privacy. (arXiv:2306.05275v1 [cs.LG])

    [http://arxiv.org/abs/2306.05275](http://arxiv.org/abs/2306.05275)

    本文研究了带有用户级差分隐私的联邦线性上下文强化学习模型，为CDP提出了近乎最优的联邦算法\robin，在LDP下证明了学习必须承受至少一个遗憾膨胀因子。

    

    本文研究了在用户级差分隐私（DP）概念下的联邦线性上下文强化学习。我们首先介绍了一个统一的联邦强化学习框架，可以适应顺序决策设置中DP的各种定义。然后在联邦强化学习框架中正式引入了用户级中心DP和本地DP，并研究了联邦线性上下文强化学习模型中学习遗憾和相应DP保证之间的基本权衡。对于CDP，我们提出了一种称为\robin的联邦算法，并通过推导在满足用户级DP时的几乎匹配的上界和下界遗憾界，证明其在客户端数量$M$和隐私预算$\varepsilon$方面是近乎最优的。对于LDP，我们获得了几个下界，表明在用户级$(\varepsilon,\delta)$-LDP下学习必须至少承受一个遗憾膨胀因子至少为{$\min\{1/\varepsilon,M\}$或$\min\{1/\sqrt{\varepsilon},\sq

    This paper studies federated linear contextual bandits under the notion of user-level differential privacy (DP). We first introduce a unified federated bandits framework that can accommodate various definitions of DP in the sequential decision-making setting. We then formally introduce user-level central DP (CDP) and local DP (LDP) in the federated bandits framework, and investigate the fundamental trade-offs between the learning regrets and the corresponding DP guarantees in a federated linear contextual bandits model. For CDP, we propose a federated algorithm termed as \robin and show that it is near-optimal in terms of the number of clients $M$ and the privacy budget $\varepsilon$ by deriving nearly-matching upper and lower regret bounds when user-level DP is satisfied. For LDP, we obtain several lower bounds, indicating that learning under user-level $(\varepsilon,\delta)$-LDP must suffer a regret blow-up factor at least {$\min\{1/\varepsilon,M\}$ or $\min\{1/\sqrt{\varepsilon},\sq
    
[^26]: 利用预训练模型的速率降低原则进行图像聚类

    Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models. (arXiv:2306.05272v1 [cs.CV])

    [http://arxiv.org/abs/2306.05272](http://arxiv.org/abs/2306.05272)

    本论文提出了一种新的图像聚类流程，利用大型预训练模型的强大特征表示，在规模上有效地对图像进行聚类，并通过优化速率降低目标和 CLIP 的图像-文本绑定，成功地提高了聚类的准确性和自标记算法的效果。

    

    大型预训练模型的出现已经在视觉表示学习和自然语言处理方面带来了范式转变，但是聚类未标记的图像作为一种基本和经典的机器学习问题，仍然缺乏有效的解决方案，特别是对于大规模数据集。在本文中，我们提出了一种新的图像聚类流程，利用 CLIP 等大型预训练模型的强大特征表示，在规模上有效地对图像进行聚类。我们展示了预训练特征通过进一步优化速率降低目标，更具有结构性。由此产生的特征可以显著提高聚类的准确性，例如从 ImageNet-1k 的 57％提高到 66％。此外，通过利用 CLIP 的图像-文本绑定，我们展示了新的聚类方法如何导致简单而有效的自标记算法，成功地应用于未标记的大型数据集，例如 MS-COCO 和 LAION-Aesthetics。

    The advent of large pre-trained models has brought about a paradigm shift in both visual representation learning and natural language processing. However, clustering unlabeled images, as a fundamental and classic machine learning problem, still lacks effective solution, particularly for large-scale datasets. In this paper, we propose a novel image clustering pipeline that leverages the powerful feature representation of large pre-trained models such as CLIP and cluster images effectively and efficiently at scale. We show that the pre-trained features are significantly more structured by further optimizing the rate reduction objective. The resulting features may significantly improve the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore, by leveraging CLIP's image-text binding, we show how the new clustering method leads to a simple yet effective self-labeling algorithm that successfully works on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We wi
    
[^27]: 分解对比学习：超越多视角冗余

    Factorized Contrastive Learning: Going Beyond Multi-view Redundancy. (arXiv:2306.05268v1 [cs.LG])

    [http://arxiv.org/abs/2306.05268](http://arxiv.org/abs/2306.05268)

    本论文提出了FactorCL，一种新的多模态表示学习方法，不仅考虑跨模态共享信息，还能捕捉跨模态唯一的任务相关信息。

    

    在广泛的多模态任务中，对比学习已成为一种特别吸引人的方法，因为它可以成功地学习具有丰富未标记数据的表示，只需配对信息（例如，图像标题或视频音频对）。这些方法的基础是多视角冗余的假设——跨模态间共享信息对于下游任务是必要且足够的。然而，在许多现实世界的情况下，任务相关信息也包含在跨模态唯一区域中：一种仅存在于一个模态中但与任务仍然相关的信息。如何学习自我监督的多模态表示以捕获与下游任务相关的共享和唯一信息？本文提出了一种新的多模态表示学习方法FactorCL，以超越多视角冗余。FactorCL的基础是三个新的贡献：（1）将任务相关信息分解为共享和唯一表示，（2）限制共享和唯一成分之间的交互，（3）使用因子正则化促进表示学习。

    In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) cap
    
[^28]: 描述和学习晶体对称群下不变函数

    Representing and Learning Functions Invariant Under Crystallographic Groups. (arXiv:2306.05261v1 [stat.ML])

    [http://arxiv.org/abs/2306.05261](http://arxiv.org/abs/2306.05261)

    本研究提出了线性和非线性表达形式，用于描述具有晶体对称性的函数。线性表达形式提供了晶体对称性下的基函数，而非线性表达将轨道空间嵌入到有限维欧几里得空间中进行描述。

    

    晶体对称群描述了自然和科学中遇到的晶体和其他重复结构的对称性。本文推导出函数的线性和非线性表示形式，这些函数同时具有（1）平滑和（2）在此类群下不变的特性。线性表示将傅里叶基广义化为具有晶体对称性的基函数。我们证明了这种基函数对于每个晶体对称群都存在，对应$L_2$空间具有正交性，在纯平移群的情况下，可恢复为标准傅里叶基。非线性表示将该群的轨道空间嵌入到有限维欧几里得空间中。我们证明了每个晶体对称群都存在这样的嵌入，而且这个嵌入通过一个称为轨积流形的流形的广义化函数因子化。我们还描述了一些算法，可以根据该群的标准描述计算出傅里叶基和嵌入映射。

    Crystallographic groups describe the symmetries of crystals and other repetitive structures encountered in nature and the sciences. These groups include the wallpaper and space groups. We derive linear and nonlinear representations of functions that are (1) smooth and (2) invariant under such a group. The linear representation generalizes the Fourier basis to crystallographically invariant basis functions. We show that such a basis exists for each crystallographic group, that it is orthonormal in the relevant $L_2$ space, and recover the standard Fourier basis as a special case for pure shift groups. The nonlinear representation embeds the orbit space of the group into a finite-dimensional Euclidean space. We show that such an embedding exists for every crystallographic group, and that it factors functions through a generalization of a manifold called an orbifold. We describe algorithms that, given a standardized description of the group, compute the Fourier basis and an embedding map.
    
[^29]: 深度学习和图学习在药物相互作用预测中的综合评估

    Comprehensive evaluation of deep and graph learning on drug-drug interactions prediction. (arXiv:2306.05257v1 [cs.LG])

    [http://arxiv.org/abs/2306.05257](http://arxiv.org/abs/2306.05257)

    本论文综合评估了使用深度学习和图学习在药物相互作用预测方面的可行性，并总结了基于化学结构、基于网络、基于自然语言处理和混合方法，提供了一个易于访问的指南。

    

    人工智能及深度学习和图学习模型的最新进展和成就在生物医学应用中已经被证明是有用的，特别是在药物相互作用(DDIs)相关的应用。DDIs是指在人体中存在另一个药物时，对一种药物作用的改变，它在药物发现和临床研究中起着重要的作用。通过传统的临床试验和实验来预测DDIs是一项昂贵且耗时的过程。为了正确应用先进的AI和深度学习，开发者和用户面临着各种挑战，如数据资源的可用性和编码以及计算方法的设计。本文总结了基于化学结构、基于网络、基于自然语言处理(NLP)和混合方法，为具有不同领域知识的广泛研究和发展社区提供了最新和易于访问的指南。我们介绍了广泛使用的分子表示方法，并描述了理论框架。

    Recent advances and achievements of artificial intelligence (AI) as well as deep and graph learning models have established their usefulness in biomedical applications, especially in drug-drug interactions (DDIs). DDIs refer to a change in the effect of one drug to the presence of another drug in the human body, which plays an essential role in drug discovery and clinical research. DDIs prediction through traditional clinical trials and experiments is an expensive and time-consuming process. To correctly apply the advanced AI and deep learning, the developer and user meet various challenges such as the availability and encoding of data resources, and the design of computational methods. This review summarizes chemical structure based, network based, NLP based and hybrid methods, providing an updated and accessible guide to the broad researchers and development community with different domain knowledge. We introduce widely-used molecular representation and describe the theoretical frame
    
[^30]: 非线性无损自编码器

    Unscented Autoencoder. (arXiv:2306.05256v1 [cs.LG])

    [http://arxiv.org/abs/2306.05256](http://arxiv.org/abs/2306.05256)

    本文提出了一种新的自编码器模型，名为非线性无损自编码器（UAE），它使用确定性采样的有限一组统计量来提高后验表示，从而获得更高质量的重建。同时，本文使用Wasserstein分布度量替换KL散度，实现更尖锐的后验。

    

    变分自编码器（VAE）是具有潜在变量的深度生成模型中的开创性方法。我们将其重建过程解释为来自潜在后验分布的样本的非线性转换，并应用无损卡尔曼滤波器（UKF）领域中使用的众所周知的分布近似——无损变换（UT）。确定性采样的有限一组称为sigma点的统计量提供比重参数技巧的普遍噪声缩放更具信息量且方差更小的后验表示，同时确保更高质量的重建。我们进一步通过将Kullback-Leibler（KL）散度替换为Wasserstein分布度量来提高性能，从而允许更尖锐的后验。受到这两个组件的启发，我们得出了一种新颖的确定性采样VAE，即非线性无损自编码器（UAE），该自编码器仅使用针对每个样本后验的类似正规化的项进行训练。

    The Variational Autoencoder (VAE) is a seminal approach in deep generative modeling with latent variables. Interpreting its reconstruction process as a nonlinear transformation of samples from the latent posterior distribution, we apply the Unscented Transform (UT) -- a well-known distribution approximation used in the Unscented Kalman Filter (UKF) from the field of filtering. A finite set of statistics called sigma points, sampled deterministically, provides a more informative and lower-variance posterior representation than the ubiquitous noise-scaling of the reparameterization trick, while ensuring higher-quality reconstruction. We further boost the performance by replacing the Kullback-Leibler (KL) divergence with the Wasserstein distribution metric that allows for a sharper posterior. Inspired by the two components, we derive a novel, deterministic-sampling flavor of the VAE, the Unscented Autoencoder (UAE), trained purely with regularization-like terms on the per-sample posterior
    
[^31]: 面向非监督领域自适应的更准确和通用的脑部变形估计器，用于创伤性脑损伤检测。

    Toward more accurate and generalizable brain deformation estimators for traumatic brain injury detection with unsupervised domain adaptation. (arXiv:2306.05255v1 [cs.LG])

    [http://arxiv.org/abs/2306.05255](http://arxiv.org/abs/2306.05255)

    本研究提出了一种脑部变形估计器，该估计器结合了非监督领域适应和深度神经网络的方法，可以更准确、更通用地预测脑最大主应变和MPS速率。该模型显著提高了预测精度，对早期TBI检测有重要意义。

    

    机器学习头部模型（MLHMs）被开发用于估计脑部变形，以早期检测创伤性脑损伤（TBI）。然而，对模拟碰撞的过度拟合以及由不同头部碰撞数据集的分布偏移引起的通用性缺失阻碍了当前MLHMs的广泛临床应用。我们提出了一种脑部变形估计器，该估计器将非监督领域适应与深度神经网络相结合，以预测整个脑最大主应变（MPS）和MPS速率（MPSR）。我们使用领域正则化组件分析（DRCA）和循环GAN方法，针对12,780个模拟头部撞击，对来自302个大学橄榄球（CF）撞击和457个综合武术（MMA）撞击的现场头部撞击进行了非监督领域适应。新模型提高了MPS / MPSR估计准确性，其中DRCA方法在预测精度方面明显优于其他领域适应方法（p <0.001）：MPS RMSE：0.027（CF）和0.037（MMA）;

    Machine learning head models (MLHMs) are developed to estimate brain deformation for early detection of traumatic brain injury (TBI). However, the overfitting to simulated impacts and the lack of generalizability caused by distributional shift of different head impact datasets hinders the broad clinical applications of current MLHMs. We propose brain deformation estimators that integrates unsupervised domain adaptation with a deep neural network to predict whole-brain maximum principal strain (MPS) and MPS rate (MPSR). With 12,780 simulated head impacts, we performed unsupervised domain adaptation on on-field head impacts from 302 college football (CF) impacts and 457 mixed martial arts (MMA) impacts using domain regularized component analysis (DRCA) and cycle-GAN-based methods. The new model improved the MPS/MPSR estimation accuracy, with the DRCA method significantly outperforming other domain adaptation methods in prediction accuracy (p<0.001): MPS RMSE: 0.027 (CF) and 0.037 (MMA); 
    
[^32]: 基于音频文本匹配的关键词检测

    Matching Latent Encoding for Audio-Text based Keyword Spotting. (arXiv:2306.05245v1 [eess.AS])

    [http://arxiv.org/abs/2306.05245](http://arxiv.org/abs/2306.05245)

    本文提出了一种基于音频文本的端到端模型来进行灵活的关键词检测，使用一种新型的动态规划算法来将音频序列最优地分割成与基于单词的文本序列相同的长度，以实现不同长度多词关键词的语义对齐。

    

    结合音频与文本嵌入进行关键词检测已经取得了高质量的结果，但如何对不同序列长度的多词关键词进行语义对齐仍然是一个挑战。本文提出了一种基于音频文本的端到端模型，使用学习到的音频和文本嵌入来进行灵活的关键词检测。我们的架构使用一种新型的动态规划算法Dynamic Sequence Partitioning（DSP），通过口语内容的单调对齐将音频序列最优地分割成与基于单词的文本序列相同的长度。我们的模型由一个编码器块来获取音频和文本嵌入，一个投影器块将各自的嵌入投影到一个共同的潜在空间，和一个音频文本匹配器，其中包含一种新颖的DSP算法，用于将音频和文本嵌入对齐，以确定口语内容是否与文本相同。

    Using audio and text embeddings jointly for Keyword Spotting (KWS) has shown high-quality results, but the key challenge of how to semantically align two embeddings for multi-word keywords of different sequence lengths remains largely unsolved. In this paper, we propose an audio-text-based end-to-end model architecture for flexible keyword spotting (KWS), which builds upon learned audio and text embeddings. Our architecture uses a novel dynamic programming-based algorithm, Dynamic Sequence Partitioning (DSP), to optimally partition the audio sequence into the same length as the word-based text sequence using the monotonic alignment of spoken content. Our proposed model consists of an encoder block to get audio and text embeddings, a projector block to project individual embeddings to a common latent space, and an audio-text aligner containing a novel DSP algorithm, which aligns the audio and text embeddings to determine if the spoken content is the same as the text. Experimental result
    
[^33]: 生成对抗网络的拥有权保护

    Ownership Protection of Generative Adversarial Networks. (arXiv:2306.05233v1 [cs.CR])

    [http://arxiv.org/abs/2306.05233](http://arxiv.org/abs/2306.05233)

    本文提出一种新型的GAN拥有权保护方法，它基于目标模型及其盗版模型共同特征，能够直接适用于所有训练良好的GAN，最终实验结果表明其具有最佳的保护性能。

    

    生成对抗网络（GAN）在图像合成方面取得了显著的成功，使得GAN模型本身对于合法的模型所有者具有商业价值。因此，技术保护GAN的知识产权至关重要。先前的研究需要篡改训练集或训练过程，并且它们对新兴的模型提取攻击不具有鲁棒性。本文提出一种基于目标模型及其盗版模型共同特征的新型拥有权保护方法。我们的方法可以直接适用于所有训练良好的GAN，因为它不需要重新训练目标模型。大量实验结果表明，与最先进的方法相比，我们的新方法可以实现最佳的保护性能。最后，我们证明了我们的方法对于模型提取攻击的生成次数、生成样本的数量、不同数据集以及自适应攻击的有效性。

    Generative adversarial networks (GANs) have shown remarkable success in image synthesis, making GAN models themselves commercially valuable to legitimate model owners. Therefore, it is critical to technically protect the intellectual property of GANs. Prior works need to tamper with the training set or training process, and they are not robust to emerging model extraction attacks. In this paper, we propose a new ownership protection method based on the common characteristics of a target model and its stolen models. Our method can be directly applicable to all well-trained GANs as it does not require retraining target models. Extensive experimental results show that our new method can achieve the best protection performance, compared to the state-of-the-art methods. Finally, we demonstrate the effectiveness of our method with respect to the number of generations of model extraction attacks, the number of generated samples, different datasets, as well as adaptive attacks.
    
[^34]: 实现平坦局部极小值以提高对抗性转移的效果

    Boosting Adversarial Transferability by Achieving Flat Local Maxima. (arXiv:2306.05225v1 [cs.CV])

    [http://arxiv.org/abs/2306.05225](http://arxiv.org/abs/2306.05225)

    本文提出了一种近似优化方法来实现平坦局部极小值，该方法可以提高对抗性转移能力，并在实验中证实了该方法的有效性。

    

    基于转移的攻击采用在替代模型上生成的对抗性示例来攻击各种模型，在物理世界中适用并吸引了越来越多的关注。在这项工作中，受到平坦局部极小值与良好的泛化之间的相关性启发，我们假设并经验性地验证了，在平坦局部区域的对抗性示例倾向于具有良好的转移能力，通过在原始损失函数中引入惩罚梯度范数。由于直接优化梯度正则化范数在生成对抗性示例时计算代价昂贵且难以处理，因此我们提出了一种近似优化方法，以简化目标函数的梯度更新。我们随机采样一个示例，并采用一阶梯度来逼近二阶黑塞矩阵，从而使计算更加容易。

    Transfer-based attack adopts the adversarial examples generated on the surrogate model to attack various models, making it applicable in the physical world and attracting increasing interest. Recently, various adversarial attacks have emerged to boost adversarial transferability from different perspectives. In this work, inspired by the fact that flat local minima are correlated with good generalization, we assume and empirically validate that adversarial examples at a flat local region tend to have good transferability by introducing a penalized gradient norm to the original loss function. Since directly optimizing the gradient regularization norm is computationally expensive and intractable for generating adversarial examples, we propose an approximation optimization method to simplify the gradient update of the objective function. Specifically, we randomly sample an example and adopt the first-order gradient to approximate the second-order Hessian matrix, which makes computing more 
    
[^35]: 基于Boosting的线性阈值函数BDD构造及其在神经网络验证中的应用

    Boosting-based Construction of BDDs for Linear Threshold Functions and Its Application to Verification of Neural Networks. (arXiv:2306.05211v1 [cs.LG])

    [http://arxiv.org/abs/2306.05211](http://arxiv.org/abs/2306.05211)

    本文提出了一种基于Boosting的新方法，用于构造任何线性阈值函数的有序BDD，并可以在O(n2^{n})的时间复杂度内实现。该方法已应用于神经网络对抗攻击的鲁棒性验证中。

    

    由于神经网络的复杂结构和行为，理解其特征具有重要意义但也很困难。先前的一些工作提出将神经网络转换为等价的布尔表达式，并应用验证技术来研究感兴趣的特征。然而，该转换的时间复杂度是瓶颈。本文提出了一种基于Boosting技术的新方法，用于构造任何线性阈值函数的有序BDD，并且可以在时间复杂度O(n2^{n})内实现。我们证明了该方法的有效性，应用于神经网络对抗攻击的鲁棒性验证。

    Understanding the characteristics of neural networks is important but difficult due to their complex structures and behaviors. Some previous work proposes to transform neural networks into equivalent Boolean expressions and apply verification techniques for characteristics of interest. This approach is promising since rich results of verification techniques for circuits and other Boolean expressions can be readily applied. The bottleneck is the time complexity of the transformation. More precisely, (i) each neuron of the network, i.e., a linear threshold function, is converted to a Binary Decision Diagram (BDD), and (ii) they are further combined into some final form, such as Boolean circuits. For a linear threshold function with $n$ variables, an existing method takes $O(n2^{\frac{n}{2}})$ time to construct an ordered BDD of size $O(2^{\frac{n}{2}})$ consistent with some variable ordering. However, it is non-trivial to choose a variable ordering producing a small BDD among $n!$ candid
    
[^36]: PriSampler: 缓解扩散模型的属性推断问题

    PriSampler: Mitigating Property Inference of Diffusion Models. (arXiv:2306.05208v1 [cs.CR])

    [http://arxiv.org/abs/2306.05208](http://arxiv.org/abs/2306.05208)

    本文是第一项针对扩散模型属性推断攻击的隐私研究，攻击者将从模型中提取训练集的敏感全局属性，结果表明各种扩散模型及其取样器容易受到攻击的影响。

    

    扩散模型在数据合成方面取得了巨大成功。这些成功也促使扩散模型应用于敏感数据，例如人脸数据，但这可能带来严重的隐私问题。本文系统地介绍了针对扩散模型的属性推断攻击的第一项隐私研究，其中攻击者旨在从扩散模型中提取训练集的敏感全局属性，例如某些敏感属性的训练数据比例。具体而言，我们考虑了最实用的攻击场景：攻击者只能获得合成数据。在现实场景下，我们对不同类型的取样器和扩散模型进行了属性推断攻击的评估。广泛的评估范围表明，各种扩散模型及其取样器都容易受到属性推断攻击的影响。此外，对现成的预训练扩散模型进行一项案例研究也展示了攻击的实际效果。

    Diffusion models have been remarkably successful in data synthesis. Such successes have also driven diffusion models to apply to sensitive data, such as human face data, but this might bring about severe privacy concerns. In this work, we systematically present the first privacy study about property inference attacks against diffusion models, in which adversaries aim to extract sensitive global properties of the training set from a diffusion model, such as the proportion of the training data for certain sensitive properties. Specifically, we consider the most practical attack scenario: adversaries are only allowed to obtain synthetic data. Under this realistic scenario, we evaluate the property inference attacks on different types of samplers and diffusion models. A broad range of evaluations shows that various diffusion models and their samplers are all vulnerable to property inference attacks. Furthermore, one case study on off-the-shelf pre-trained diffusion models also demonstrates
    
[^37]: EMO：用于小样本元学习的情节记忆优化

    EMO: Episodic Memory Optimization for Few-Shot Meta-Learning. (arXiv:2306.05189v1 [cs.LG])

    [http://arxiv.org/abs/2306.05189](http://arxiv.org/abs/2306.05189)

    EMO是一种元学习的情节记忆优化方案，通过在外部存储器中记录过去任务的梯度历史，实现小样本学习，无论提供的梯度信息是否可靠，都可以推动参数更新朝着正确的方向前进。

    

    小样本元学习由于任务训练样本数量的限制对梯度下降优化提出了挑战。为了解决这个问题，本文提出了一种元学习的情节记忆优化方案，称为EMO。EMO受到人类从脑内记忆中回忆过去学习经验的能力的启发，将过去任务的梯度历史记录在外部存储器中，以增强记忆的方式进行小样本学习。通过学习保留和回忆过去训练任务的学习过程，即使仅有有限数量的示例提供了不可靠的梯度，EMO也可以推动参数更新朝着正确的方向前进。我们在理论上证明了该算法对于平滑、强凸目标函数会收敛。EMO是通用的、灵活的、与模型无关的优化器，可无缝嵌入现有的基于优化的小样本元学习方法。实证结果表明EMO可以提高准确性和收敛速度。

    Few-shot meta-learning presents a challenge for gradient descent optimization due to the limited number of training samples per task. To address this issue, we propose an episodic memory optimization for meta-learning, we call \emph{EMO}, which is inspired by the human ability to recall past learning experiences from the brain's memory. EMO retains the gradient history of past experienced tasks in external memory, enabling few-shot learning in a memory-augmented way. By learning to retain and recall the learning process of past training tasks, EMO nudges parameter updates in the right direction, even when the gradients provided by a limited number of examples are uninformative. We prove theoretically that our algorithm converges for smooth, strongly convex objectives. EMO is generic, flexible, and model-agnostic, making it a simple plug-and-play optimizer that can be seamlessly embedded into existing optimization-based few-shot meta-learning approaches. Empirical results show that EMO 
    
[^38]: 关于半线性椭圆PDE中非光滑超定算子的识别和优化

    On the Identification and Optimization of Nonsmooth Superposition Operators in Semilinear Elliptic PDEs. (arXiv:2306.05185v1 [math.OC])

    [http://arxiv.org/abs/2306.05185](http://arxiv.org/abs/2306.05185)

    本文研究了在低正则性情况下，如何识别半线性椭圆PDE中的Nemytskii算子以及如何解决这个优化问题。这对于进行有关PDE的神经网络训练问题有很好的启示作用。

    

    我们研究了一个无限维优化问题，目的是识别半线性椭圆偏微分方程(PDE)非线性部分中的Nemytskii算子，该算子将PDE解与给定的期望状态之间的距离最小化。与以前的工作不同，我们在低正则性情况下考虑了这个识别问题，在这种情况下引起Nemytskii算子函数先验仅被认为是H^1_{loc}(\mathbb{R})的元素。这使得研究的问题类成为一种适合从严格分析的角度来处理学习有关PDE的培训问题的出发点，其中未知的超定算子是通过使用非光滑激活函数(ReLU，leaky-ReLU等)的神经网络逼近的。我们证明，尽管控制的正则性较低，但可以为局部极小值导出经典的站点系统，并通过梯度投影法解决所考虑的问题。

    We study an infinite-dimensional optimization problem that aims to identify the Nemytskii operator in the nonlinear part of a prototypical semilinear elliptic partial differential equation (PDE) which minimizes the distance between the PDE-solution and a given desired state. In contrast to previous works, we consider this identification problem in a low-regularity regime in which the function inducing the Nemytskii operator is a-priori only known to be an element of $H^1_{loc}(\mathbb{R})$. This makes the studied problem class a suitable point of departure for the rigorous analysis of training problems for learning-informed PDEs in which an unknown superposition operator is approximated by means of a neural network with nonsmooth activation functions (ReLU, leaky-ReLU, etc.). We establish that, despite the low regularity of the controls, it is possible to derive a classical stationarity system for local minimizers and to solve the considered problem by means of a gradient projection me
    
[^39]: 提高长篇文本机器翻译的质量

    Improving Long Context Document-Level Machine Translation. (arXiv:2306.05183v1 [cs.CL])

    [http://arxiv.org/abs/2306.05183](http://arxiv.org/abs/2306.05183)

    该论文提出了一种新的受限注意力机制来提高长篇文本机器翻译的质量。

    

    对于神经机器翻译（NMT）来说，文本级别的上下文对于提高翻译的一致性、凝聚性、模棱两可输入的翻译以及其他语言现象至关重要。虽然已经有许多关于文档级别 NMT 的相关论文出版，但大多数将系统限制在本地上下文，通常只包括前一两个句子作为更多信息。这可能足以解决一些曖昧性输入，但可能不足以捕捉文档级别信息，例如话题或对话风格。当将上下文大小增加到本地上下文之外时，会面临两个挑战：（i）内存使用将呈指数增长（ii）翻译性能开始降低。我们认为广泛使用的注意机制是这两个问题的原因。因此，我们提出了一种受限的注意力变体，将注意力集中在序列的最相关部分，同时控制对齐权重的总和。

    Document-level context for neural machine translation (NMT) is crucial to improve the translation consistency and cohesion, the translation of ambiguous inputs, as well as several other linguistic phenomena. Many works have been published on the topic of document-level NMT, but most restrict the system to only local context, typically including just the one or two preceding sentences as additional information. This might be enough to resolve some ambiguous inputs, but it is probably not sufficient to capture some document-level information like the topic or style of a conversation. When increasing the context size beyond just the local context, there are two challenges: (i) the~memory usage increases exponentially (ii) the translation performance starts to degrade. We argue that the widely-used attention mechanism is responsible for both issues. Therefore, we propose a constrained attention variant that focuses the attention on the most relevant parts of the sequence, while simultaneou
    
[^40]: 利用LLMs和潜在扩散模型的互动时尚内容生成

    Interactive Fashion Content Generation Using LLMs and Latent Diffusion Models. (arXiv:2306.05182v1 [cs.CV])

    [http://arxiv.org/abs/2306.05182](http://arxiv.org/abs/2306.05182)

    该论文提出了一种利用LLMs和潜在扩散模型生成时尚内容的方法，通过简单的提示进行互动并生成时尚图像，从而帮助时尚设计师进行实时可视化和进一步改进，这种方法能够捕捉数据的基本结构。

    

    时尚图像生成旨在合成世界各地流行的不同时尚图片，通过为时尚设计师提供特定设计偏好在真实生活中的基本定制结构以及进一步改进的看法，帮助设计师进行实时可视化。此外，用户可以通过简单的提示进行互动并生成时尚图像。最近，扩散模型由于其灵活性和从高斯噪声生成逼真图像而被视为生成模型。潜在扩散模型是一种生成模型，使用扩散过程来模拟复杂数据（如图像、音频或文本）的生成。它们被称为“潜在”，因为它们学习数据的隐藏表示或潜在变量，捕捉其基本结构。

    Fashionable image generation aims to synthesize images of diverse fashion prevalent around the globe, helping fashion designers in real-time visualization by giving them a basic customized structure of how a specific design preference would look in real life and what further improvements can be made for enhanced customer satisfaction. Moreover, users can alone interact and generate fashionable images by just giving a few simple prompts. Recently, diffusion models have gained popularity as generative models owing to their flexibility and generation of realistic images from Gaussian noise. Latent diffusion models are a type of generative model that use diffusion processes to model the generation of complex data, such as images, audio, or text. They are called "latent" because they learn a hidden representation, or latent variable, of the data that captures its underlying structure. We propose a method exploiting the equivalence between diffusion models and energy-based models (EBMs) and 
    
[^41]: 动态不确定性下的大规模数据集修剪方法

    Large-scale Dataset Pruning with Dynamic Uncertainty. (arXiv:2306.05175v1 [cs.LG])

    [http://arxiv.org/abs/2306.05175](http://arxiv.org/abs/2306.05175)

    本文提出了一种探索预测不确定性和训练动态的简单且有效的大规模数据集修剪方法，以产生信息量丰富的子集用于训练深度模型，实验结果表明优于现有技术，在ImageNet-1K和ImageNet-21K上实现了75％的无损压缩比。

    

    在许多学习任务，例如图像分类中，收集更大的数据集并在其上训练更大的模型是推动技术前进的关键因素。由此产生的计算成本逐渐变得难以承受。本文研究了如何修剪大规模数据集，从而产生信息量丰富的子集，用于训练复杂的深度模型，且性能下降可以忽略不计。我们提出了一种简单但有效的数据集修剪方法，通过探索预测不确定性和训练动态来实现。据我们所知，这是第一篇在大规模数据集（ImageNet-1K和ImageNet-21K）和先进模型（Swin Transformer和ConvNeXt）上研究数据集修剪的工作。广泛的实验结果表明，我们的方法优于现有技术，在ImageNet-1K和ImageNet-21K上实现了75％的无损压缩比。代码和修剪后的数据集可在https://github.com/BAAI-DCAI/Dataset-Pruning上获得。

    The state of the art of many learning tasks, e.g., image classification, is advanced by collecting larger datasets and then training larger models on them. As the outcome, the increasing computational cost is becoming unaffordable. In this paper, we investigate how to prune the large-scale datasets, and thus produce an informative subset for training sophisticated deep models with negligible performance drop. We propose a simple yet effective dataset pruning method by exploring both the prediction uncertainty and training dynamics. To our knowledge, this is the first work to study dataset pruning on large-scale datasets, i.e., ImageNet-1K and ImageNet-21K, and advanced models, i.e., Swin Transformer and ConvNeXt. Extensive experimental results indicate that our method outperforms the state of the art and achieves 75% lossless compression ratio on both ImageNet-1K and ImageNet-21K. The code and pruned datasets are available at https://github.com/BAAI-DCAI/Dataset-Pruning.
    
[^42]: FLEdge：边缘计算系统中联邦机器学习应用的基准测试

    FLEdge: Benchmarking Federated Machine Learning Applications in Edge Computing Systems. (arXiv:2306.05172v1 [cs.LG])

    [http://arxiv.org/abs/2306.05172](http://arxiv.org/abs/2306.05172)

    FLEdge是一个面向边缘计算系统中FL工作量的基准测试，通过研究硬件异构性、能量效率和隐私级别对FL系统训练的影响，以及客户端退出对最新FL策略的影响，提供了训练最先进的FL工作负载的新见解。

    

    近年来，联邦机器学习（FL）备受关注。 FL基准测试主要在模拟系统或数据中心环境中进行探索，忽略了与边缘计算密切相关的实际系统设置。 我们通过引入面向边缘计算系统中FL工作量的基准测试FLEdge来弥补这一研究差距。我们系统地研究了硬件异构性、训练过程中的能量效率以及各种不同隐私级别对FL系统训练的影响。为了使这个基准测试适用于实际场景，我们评估了客户端退出对具有高达50％失效率的最新FL策略的影响。 FLEdge提供了新的见解，例如，在旧GPU加速的嵌入式设备上训练最先进的FL工作负载比在现代服务器级GPU上训练高达3倍的能量效率。

    Federated Machine Learning (FL) has received considerable attention in recent years. FL benchmarks are predominantly explored in either simulated systems or data center environments, neglecting the setups of real-world systems, which are often closely linked to edge computing. We close this research gap by introducing FLEdge, a benchmark targeting FL workloads in edge computing systems. We systematically study hardware heterogeneity, energy efficiency during training, and the effect of various differential privacy levels on training in FL systems. To make this benchmark applicable to real-world scenarios, we evaluate the impact of client dropouts on state-of-the-art FL strategies with failure rates as high as 50%. FLEdge provides new insights, such as that training state-of-the-art FL workloads on older GPU-accelerated embedded devices is up to 3x more energy efficient than on modern server-grade GPUs.
    
[^43]: Decision S4：基于状态空间层的高效序列强化学习方法

    Decision S4: Efficient Sequence-Based RL via State Spaces Layers. (arXiv:2306.05167v1 [cs.LG])

    [http://arxiv.org/abs/2306.05167](http://arxiv.org/abs/2306.05167)

    本论文提出了基于状态空间层的高效序列强化学习方法，包含一种离线和一种在线训练过程，可以从长程依赖中受益，取得了较好的效果。

    

    最近，序列学习方法被应用于离线强化学习问题，其中包括使用Transformer的开创性工作——Decision Transformer。由于Transformer参数重，无法利用超过固定窗口大小的历史信息，且不能使用复现计算，因此我们开始研究S4模型系列的适用性。该模型基于状态空间层构建，已被证明在建模长程依赖方面优于Transformer。在本研究中，我们提出了两个主要算法：（i）一种离线训练过程，可以使用轨迹数据，同时仍然保持S4模型的训练效率；（ii）一种基于新颖稳定的演员 - 评论家机制，以循环方式训练的在线训练过程，可以从长程依赖中受益。我们的结果表明，在各种基准环境下，我们的方法优于多种Decision Transformer的变体以及传统的LSTM和GRU模型。

    Recently, sequence learning methods have been applied to the problem of off-policy Reinforcement Learning, including the seminal work on Decision Transformers, which employs transformers for this task. Since transformers are parameter-heavy, cannot benefit from history longer than a fixed window size, and are not computed using recurrence, we set out to investigate the suitability of the S4 family of models, which are based on state-space layers and have been shown to outperform transformers, especially in modeling long-range dependencies. In this work we present two main algorithms: (i) an off-policy training procedure that works with trajectories, while still maintaining the training efficiency of the S4 model. (ii) An on-policy training procedure that is trained in a recurrent manner, benefits from long-range dependencies, and is based on a novel stable actor-critic mechanism. Our results indicate that our method outperforms multiple variants of decision transformers, as well as the
    
[^44]: 昂贵嵌套灰盒函数的贝叶斯优化

    Bayesian Optimization of Expensive Nested Grey-Box Functions. (arXiv:2306.05150v1 [cs.LG])

    [http://arxiv.org/abs/2306.05150](http://arxiv.org/abs/2306.05150)

    本文提出基于乐观主义的算法来解决嵌套黑白箱函数优化问题，相比传统黑箱优化方法显著提高全局最优解速度。

    

    我们考虑优化灰盒目标函数的问题，即由黑箱和白箱函数组成的嵌套函数。给出了这种灰盒问题的一般形式，涵盖了现有的灰盒优化公式作为特殊情况。我们设计了一种基于乐观主义的算法来解决这个问题。在一定的正则性假设下，我们的算法实现了与标准黑箱贝叶斯优化算法相似的后悔边界，但乘以依赖于所考虑函数的Lipschitz常数的常数乘项。我们进一步将我们的方法扩展到约束情况，并讨论了几个特殊情况。对于常用的核函数，后悔边界使我们能够推导到最优解的收敛速度。实验结果表明，与标准黑箱优化相比，我们的灰盒优化方法在实践中显着提高了寻找全局最优解的速度。

    We consider the problem of optimizing a grey-box objective function, i.e., nested function composed of both black-box and white-box functions. A general formulation for such grey-box problems is given, which covers the existing grey-box optimization formulations as special cases. We then design an optimism-driven algorithm to solve it. Under certain regularity assumptions, our algorithm achieves similar regret bound as that for the standard black-box Bayesian optimization algorithm, up to a constant multiplicative term depending on the Lipschitz constants of the functions considered. We further extend our method to the constrained case and discuss several special cases. For the commonly used kernel functions, the regret bounds allow us to derive a convergence rate to the optimal solution. Experimental results show that our grey-box optimization method empirically improves the speed of finding the global optimal solution significantly, as compared to the standard black-box optimization 
    
[^45]: Mesogeos: 地中海区域数据驱动野火建模的多用途数据集

    Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean. (arXiv:2306.05144v1 [cs.CV])

    [http://arxiv.org/abs/2306.05144](http://arxiv.org/abs/2306.05144)

    Mesogeos是一个地中海地区的大规模多用途数据集，用于数据驱动的野火建模。它集成了历史野火记录和野火驱动因素，具有机器学习的潜力，提供了短期野火危险预测和最终烧毁区域估计两个可用于演示潜力的数据集。

    

    我们介绍了Mesogeos，这是一个大规模的多用途数据集，用于地中海地区的野火建模。Mesogeos集成了代表野火驱动因素（气象、植被、人类活动）和17年（2006-2022年）野火点燃和烧毁区域的历史记录的变量。它被设计为云友好型时空数据集（即数据立方体），在1km x 1km x 1天的分辨率下对所有变量进行了协调。数据立方体的结构为各种野火建模任务的机器学习(ML)使用提供了机会。我们提取出两个ML可用的数据集，以展示这个潜力：(1)短期野火危险预测和(2)在点火位置的情况下估计最终烧毁区域。我们定义了适当的指标和基线来评估每个跟踪中模型的性能。通过发布数据立方体，以及用于创建ML数据集和模型的代码，我们鼓励社区促进实施。

    We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire modeling in the Mediterranean. Mesogeos integrates variables representing wildfire drivers (meteorology, vegetation, human activity) and historical records of wildfire ignitions and burned areas for 17 years (2006-2022). It is designed as a cloud-friendly spatio-temporal dataset, namely a datacube, harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The datacube structure offers opportunities to assess machine learning (ML) usage in various wildfire modeling tasks. We extract two ML-ready datasets that establish distinct tracks to demonstrate this potential: (1) short-term wildfire danger forecasting and (2) final burned area estimation given the point of ignition. We define appropriate metrics and baselines to evaluate the performance of models in each track. By publishing the datacube, along with the code to create the ML datasets and models, we encourage the community to foster the implementatio
    
[^46]: 基因组解释器：一种带有1D移动窗口变换器的层次基因组深度神经网络

    Genomic Interpreter: A Hierarchical Genomic Deep Neural Network with 1D Shifted Window Transformer. (arXiv:2306.05143v1 [cs.LG])

    [http://arxiv.org/abs/2306.05143](http://arxiv.org/abs/2306.05143)

    本文提出了一种名为“基因组解释器”的模型，可以对基因组数据进行预测并发现基因调控的层次依赖关系，性能优于现有模型。

    

    随着基因组数据量和质量的增加，提取新的洞见需要可解释的机器学习模型。本文提出了一种新的基因组测定预测结构：基因组解释器。该模型在基因组测定预测任务中表现优于现有模型。我们的模型可以识别基因组位点的层次依赖关系。这是通过1D-Swin进行实现的，这是我们设计的一种用于建模长范围层次数据的新型变换器块。在一个包含38,171个17K碱基对的DNA片段的数据集上进行评估，基因组解释器在染色质可达性和基因表达预测方面表现出卓越的性能，并揭示了基因调控的潜在“语法”。

    Given the increasing volume and quality of genomics data, extracting new insights requires interpretable machine-learning models. This work presents Genomic Interpreter: a novel architecture for genomic assay prediction. This model outperforms the state-of-the-art models for genomic assay prediction tasks. Our model can identify hierarchical dependencies in genomic sites. This is achieved through the integration of 1D-Swin, a novel Transformer-based block designed by us for modelling long-range hierarchical data. Evaluated on a dataset containing 38,171 DNA segments of 17K base pairs, Genomic Interpreter demonstrates superior performance in chromatin accessibility and gene expression prediction and unmasks the underlying `syntax' of gene regulation.
    
[^47]: 面向标签漂移的联邦不确定性量化的合规性预测

    Conformal Prediction for Federated Uncertainty Quantification Under Label Shift. (arXiv:2306.05131v1 [stat.ML])

    [http://arxiv.org/abs/2306.05131](http://arxiv.org/abs/2306.05131)

    该论文提出了一种基于分位数回归的新型联邦合规性预测方法，该方法利用重要性加权有效地解决了代理之间的标签漂移问题，并为预测集的有效覆盖和差分隐私提供了理论保证。广泛的实验结果表明，该方法优于目前的竞争对手。

    

    联邦学习（FL）是一种机器学习框架，许多客户端协作训练模型，同时保持训练数据分散。尽管近年来在FL方面取得了进展，但未对不确定性量化主题（UQ）进行部分处理。在UQ方法中，合规性预测（CP）方法在最小假设下提供无分布保证。我们基于分位数回归开发了一种新的联邦合规性预测方法，并考虑了隐私约束。该方法利用重要性加权有效地解决了代理之间的标签漂移问题，并为预测集的有效覆盖和差分隐私提供了理论保证。广泛的实验证明，该方法优于目前的竞争对手。

    Federated Learning (FL) is a machine learning framework where many clients collaboratively train models while keeping the training data decentralized. Despite recent advances in FL, the uncertainty quantification topic (UQ) remains partially addressed. Among UQ methods, conformal prediction (CP) approaches provides distribution-free guarantees under minimal assumptions. We develop a new federated conformal prediction method based on quantile regression and take into account privacy constraints. This method takes advantage of importance weighting to effectively address the label shift between agents and provides theoretical guarantees for both valid coverage of the prediction sets and differential privacy. Extensive experimental studies demonstrate that this method outperforms current competitors.
    
[^48]: 工业系统生成的元生成框架

    A Meta-Generation framework for Industrial System Generation. (arXiv:2306.05123v1 [cs.LG])

    [http://arxiv.org/abs/2306.05123](http://arxiv.org/abs/2306.05123)

    该论文提出了一个基于实际工业系统特征的使用案例，可作为基准用于评估和比较不同的深层生成模型架构。作者还提出了一种Meta-VAE，可以准确生成具有潜在设计约束的多组分工业系统。

    

    生成设计是工业界越来越重要的工具。它允许设计师和工程师轻松探索广泛的设计选项，提供了一个更便宜、更快速的试错方法替代方案。由于其提供的灵活性，深度生成模型在生成设计技术中越来越受欢迎。然而，开发和评估这些模型可能具有挑战性。领域缺乏可访问的基准来客观地评估和比较不同的深层生成模型架构。此外，普通的深度生成模型似乎无法准确生成由潜在设计约束控制的多组分工业系统。为了应对这些挑战，我们提出了一个工业界灵感的使用案例，该案例融入了实际工业系统的特征。该使用案例可以快速生成并用作基准。我们提出了一种Meta-VAE，能够产生多组分的工业系统

    Generative design is an increasingly important tool in the industrial world. It allows the designers and engineers to easily explore vast ranges of design options, providing a cheaper and faster alternative to the trial and failure approaches. Thanks to the flexibility they offer, Deep Generative Models are gaining popularity amongst Generative Design technologies. However, developing and evaluating these models can be challenging. The field lacks accessible benchmarks, in order to evaluate and compare objectively different Deep Generative Models architectures. Moreover, vanilla Deep Generative Models appear to be unable to accurately generate multi-components industrial systems that are controlled by latent design constraints. To address these challenges, we propose an industry-inspired use case that incorporates actual industrial system characteristics. This use case can be quickly generated and used as a benchmark. We propose a Meta-VAE capable of producing multi-component industria
    
[^49]: 关于文档级神经机器翻译的搜索策略

    On Search Strategies for Document-Level Neural Machine Translation. (arXiv:2306.05116v1 [cs.CL])

    [http://arxiv.org/abs/2306.05116](http://arxiv.org/abs/2306.05116)

    本文探讨了如何在搜索过程中最好地利用文档级神经机器翻译模型，比较了文献中的不同解码方案和作者提出的方案，并发现针对某些语言现象时，常用的解码策略不能够取得较好的性能。

    

    与句子级系统相比，文档级神经机器翻译（NMT）模型能够在一份文件中产生更一致的输出，并且能够更好地解决输入中的歧义。在文档级NMT上已经有许多研究，尤其是着重于修改模型架构或训练策略以更好地适应额外的上下文输入。然而，在大多数研究中，如何通过已训练好的模型执行搜索的问题很少被讨论，有时甚至根本不被提及。本研究旨在回答如何在解码中最好地利用上下文感知翻译模型的问题。我们从最流行的文档级NMT方法开始，比较了文献中的不同解码方案和我们提出的方案。比较中，我们使用了标准自动评价指标以及针对三个标准文档级翻译基准测试的特定语言现象。我们发现，大多数常用的解码策略在针对某些语言现象时并不能取得较好的性能。

    Compared to sentence-level systems, document-level neural machine translation (NMT) models produce a more consistent output across a document and are able to better resolve ambiguities within the input. There are many works on document-level NMT, mostly focusing on modifying the model architecture or training strategy to better accommodate the additional context-input. On the other hand, in most works, the question on how to perform search with the trained model is scarcely discussed, sometimes not mentioned at all. In this work, we aim to answer the question how to best utilize a context-aware translation model in decoding. We start with the most popular document-level NMT approach and compare different decoding schemes, some from the literature and others proposed by us. In the comparison, we are using both, standard automatic metrics, as well as specific linguistic phenomena on three standard document-level translation benchmarks. We find that most commonly used decoding strategies 
    
[^50]: 混合图：一种用于复杂图的统一图表示及数据集和基准的方法

    Hybrid Graph: A Unified Graph Representation with Datasets and Benchmarks for Complex Graphs. (arXiv:2306.05108v1 [cs.LG])

    [http://arxiv.org/abs/2306.05108](http://arxiv.org/abs/2306.05108)

    本论文介绍了混合图的概念及其在高阶图建模中的应用，同时提出了混合图数据集及全面的评估框架，这为图神经网络在复杂图上的性能提供了全面的解决方案。

    

    图被广泛用于封装各种数据格式，但是现实世界中的网络通常涉及复杂的节点关系，不仅仅是成对的关系。虽然已经开发和使用了超图和分层图来解决复杂的节点关系，但它们在实践中无法完全表示这些复杂性。此外，尽管已经提出了许多用于更高阶图的表示学习的图神经网络（GNN），但它们通常只在简单的图数据集上进行评估。因此，需要一个统一的更高阶图建模方法，并且需要一组包含全面数据集的可访问的评估框架，以完全了解这些算法在复杂图上的性能。在本文中，我们引入了混合图的概念，这是一个更高阶图的统一定义，并提出了混合图贝奇马克（HGB）。HGB包含各个领域的23个真实混合图数据集（如生物学、社交媒体和交通），并为GNN在复杂图上提供了全面的评估框架。

    Graphs are widely used to encapsulate a variety of data formats, but real-world networks often involve complex node relations beyond only being pairwise. While hypergraphs and hierarchical graphs have been developed and employed to account for the complex node relations, they cannot fully represent these complexities in practice. Additionally, though many Graph Neural Networks (GNNs) have been proposed for representation learning on higher-order graphs, they are usually only evaluated on simple graph datasets. Therefore, there is a need for a unified modelling of higher-order graphs, and a collection of comprehensive datasets with an accessible evaluation framework to fully understand the performance of these algorithms on complex graphs. In this paper, we introduce the concept of hybrid graphs, a unified definition for higher-order graphs, and present the Hybrid Graph Benchmark (HGB). HGB contains 23 real-world hybrid graph datasets across various domains such as biology, social media
    
[^51]: Sy-CON：用于持续自监督表示学习的对称对比损失

    Sy-CON: Symmetric Contrastive Loss for Continual Self-Supervised Representation Learning. (arXiv:2306.05101v1 [cs.LG])

    [http://arxiv.org/abs/2306.05101](http://arxiv.org/abs/2306.05101)

    Sy-CON是一种新的损失函数，用于持续自监督学习，它由两个损失组成，可以自然地找到良好的可塑性和稳定性之间的权衡，超过了现有持续学习方法。

    

    我们引入了一种新颖的通用损失函数，称为Symmetric Contrastive（Sy-CON）损失，用于有效的持续自监督学习（CSSL）。我们首先认为，传统的持续学习损失形式由单个任务特定损失（用于可塑性）和一个正则化器（用于稳定性）组成，对于基于对比损失的CSSL来说可能不理想，因为在对比学习方法中，任务特定损失会遭受负样本多样性降低的困扰，而正则化器可能会阻碍学习新的有区别的表示。为此，我们提出Sy-CON，它由两个损失（一个用于可塑性，另一个用于稳定性）组成，对当前和过去模型的负样本嵌入具有对称依赖关系。我们认为，我们的模型可以自然地找到良好的可塑性和稳定性之间的权衡，而无需任何显式的超参数调整。我们通过对几种持续学习基准的外部和内部实验验证了我们方法的有效性，并表明Sy-CON在稳定性和表征质量方面始终优于现有的持续学习方法。

    We introduce a novel and general loss function, called Symmetric Contrastive (Sy-CON) loss, for effective continual self-supervised learning (CSSL). We first argue that the conventional loss form of continual learning which consists of single task-specific loss (for plasticity) and a regularizer (for stability) may not be ideal for contrastive loss based CSSL that focus on representation learning. Our reasoning is that, in contrastive learning based methods, the task-specific loss would suffer from decreasing diversity of negative samples and the regularizer may hinder learning new distinctive representations. To that end, we propose Sy-CON that consists of two losses (one for plasticity and the other for stability) with symmetric dependence on current and past models' negative sample embeddings. We argue our model can naturally find good trade-off between the plasticity and stability without any explicit hyperparameter tuning. We validate the effectiveness of our approach through exte
    
[^52]: 重新对齐影子模型能够提高白盒成员隐私攻击的效果。

    Re-aligning Shadow Models can Improve White-box Membership Inference Attacks. (arXiv:2306.05093v1 [cs.CR])

    [http://arxiv.org/abs/2306.05093](http://arxiv.org/abs/2306.05093)

    系统分析了影子模型不对齐问题的原因，并通过对抗性训练或实例加权方法重新对齐影子模型，从而提高了白盒成员隐私攻击的效果。

    

    机器学习模型已被证明泄露了有关其训练数据集的敏感信息。随着模型的日益普及，被用于设备上，自动化任务和驱动新应用，人们开始关注白盒访问模型参数，而不是仅提供对模型的查询访问的黑盒设置，这增加了攻击面。将黑盒到白盒设置的影子建模技术直接扩展到白盒设置中，通常表现不如仅进行黑盒攻击。其中一个关键原因是深度神经网络的已知特征——不对齐。本文首次对影子模型不对齐的原因进行了系统分析，并表明采用不同的权重初始化是影子模型不对齐的主要原因。其次，我们将模型融合文献中先前开发的多种重新对齐技术扩展到影子建模上下文中，目标是重新对齐......

    Machine learning models have been shown to leak sensitive information about their training datasets. As models are being increasingly used, on devices, to automate tasks and power new applications, there have been concerns that such white-box access to its parameters, as opposed to the black-box setting which only provides query access to the model, increases the attack surface. Directly extending the shadow modelling technique from the black-box to the white-box setting has been shown, in general, not to perform better than black-box only attacks. A key reason is misalignment, a known characteristic of deep neural networks. We here present the first systematic analysis of the causes of misalignment in shadow models and show the use of a different weight initialisation to be the main cause of shadow model misalignment. Second, we extend several re-alignment techniques, previously developed in the model fusion literature, to the shadow modelling context, where the goal is to re-align th
    
[^53]: 对话的艺术：使用孪生RNN测量L2语音中的语音收敛和故意模仿

    The ART of Conversation: Measuring Phonetic Convergence and Deliberate Imitation in L2-Speech with a Siamese RNN. (arXiv:2306.05088v1 [cs.CL])

    [http://arxiv.org/abs/2306.05088](http://arxiv.org/abs/2306.05088)

    本文提出了一种孪生RNN结构用于测量L2-L2交互中语音音质收敛。该模型可以有效捕捉语音收敛和说话者的模仿能力的动态，并能够处理由L1引起的说话者差异。

    

    本文提出了一种孪生递归神经网络（RNN）结构，用于测量L2-L2交互中语音音质的收敛。我们通过添加20名母语为斯洛伐克语的英语学习者来扩展交替阅读任务（ART）数据集。我们训练和测试了该模型，从三个不同的母语组，即意大利语（9对），法语（10对）和斯洛伐克语（10对）中测量L2英语语音的语音收敛。结果表明，孪生RNN模型可以有效捕捉语音收敛和说话者的模仿能力的动态。此外，这是一个文本无关的模型，可以扩展，并能够处理由L1引起的说话者差异。

    Phonetic convergence describes the automatic and unconscious speech adaptation of two interlocutors in a conversation. This paper proposes a Siamese recurrent neural network (RNN) architecture to measure the convergence of the holistic spectral characteristics of speech sounds in an L2-L2 interaction. We extend an alternating reading task (the ART) dataset by adding 20 native Slovak L2 English speakers. We train and test the Siamese RNN model to measure phonetic convergence of L2 English speech from three different native language groups: Italian (9 dyads), French (10 dyads) and Slovak (10 dyads). Our results indicate that the Siamese RNN model effectively captures the dynamics of phonetic convergence and the speaker's imitation ability. Moreover, this text-independent model is scalable and capable of handling L1-induced speaker variability.
    
[^54]: 因果算法补救中时间的重要性

    The Importance of Time in Causal Algorithmic Recourse. (arXiv:2306.05082v1 [cs.AI])

    [http://arxiv.org/abs/2306.05082](http://arxiv.org/abs/2306.05082)

    因果算法补救需要纳入时间维度，以提高推荐的合理性和可靠性。

    

    算法补救在决策制定中具有实践意义，可以提供有利于改变不利决策的实现方案。尽管这些方法有了改进，但无法纳入时间维度仍然是这些方法面临的重大局限。在这项工作中，我们提出了将时间维度纳入因果算法补救方法中以提高推荐的合理性和可靠性的必要性。

    The application of Algorithmic Recourse in decision-making is a promising field that offers practical solutions to reverse unfavorable decisions. However, the inability of these methods to consider potential dependencies among variables poses a significant challenge due to the assumption of feature independence. Recent advancements have incorporated knowledge of causal dependencies, thereby enhancing the quality of the recommended recourse actions. Despite these improvements, the inability to incorporate the temporal dimension remains a significant limitation of these approaches. This is particularly problematic as identifying and addressing the root causes of undesired outcomes requires understanding time-dependent relationships between variables. In this work, we motivate the need to integrate the temporal dimension into causal algorithmic recourse methods to enhance recommendations' plausibility and reliability. The experimental evaluation highlights the significance of the role of 
    
[^55]: 通过数据增强提升AI攻击性代码生成器的鲁棒性

    Enhancing Robustness of AI Offensive Code Generators via Data Augmentation. (arXiv:2306.05079v1 [cs.LG])

    [http://arxiv.org/abs/2306.05079](http://arxiv.org/abs/2306.05079)

    本论文提出了一种方法，通过在代码描述中引入扰动来增强AI攻击性代码生成器的鲁棒性，并证明数据增强可有效提高代码生成器对扰动和非扰动的代码描述的性能。

    

    本研究提出了一种将扰动添加到安全性代码上下文中的代码描述中的方法，即来自善意开发者的自然语言输入（NL），并分析了扰动如何以及在什么程度上影响AI攻击性代码生成器的性能。我们的实验表明，NL描述中的扰动高度影响代码生成器的性能。为了增强代码生成器的鲁棒性，我们使用该方法执行数据增强，即增加训练数据的变异性和多样性，并证明其对扰动和非扰动的代码描述的有效性。

    In this work, we present a method to add perturbations to the code descriptions, i.e., new inputs in natural language (NL) from well-intentioned developers, in the context of security-oriented code, and analyze how and to what extent perturbations affect the performance of AI offensive code generators. Our experiments show that the performance of the code generators is highly affected by perturbations in the NL descriptions. To enhance the robustness of the code generators, we use the method to perform data augmentation, i.e., to increase the variability and diversity of the training data, proving its effectiveness against both perturbed and non-perturbed code descriptions.
    
[^56]: 分解伪变异的因果框架

    A Causal Framework for Decomposing Spurious Variations. (arXiv:2306.05071v1 [stat.ME])

    [http://arxiv.org/abs/2306.05071](http://arxiv.org/abs/2306.05071)

    本论文提出了一种因果框架，用于分解伪变异，包括虚假性图表和理论框架，能够在真假相关关系上帮助区分直接效应和间接效应。

    

    数据科学中一个根本性的挑战是解释为什么事情以特定的方式发生，或通过哪些机制某个变量$X$对另一个变量$Y$施加影响。本文提出了一种形式化工具，用于在马尔可夫和半马尔可夫系统中分解伪变异，引入了虚假性图表的概念，提出了一种理论框架，将伪效应分解为直接效应和间接效应，以识别生成这些虚假关系的基础因果机制。

    One of the fundamental challenges found throughout the data sciences is to explain why things happen in specific ways, or through which mechanisms a certain variable $X$ exerts influences over another variable $Y$. In statistics and machine learning, significant efforts have been put into developing machinery to estimate correlations across variables efficiently. In causal inference, a large body of literature is concerned with the decomposition of causal effects under the rubric of mediation analysis. However, many variations are spurious in nature, including different phenomena throughout the applied sciences. Despite the statistical power to estimate correlations and the identification power to decompose causal effects, there is still little understanding of the properties of spurious associations and how they can be decomposed in terms of the underlying causal mechanisms. In this manuscript, we develop formal tools for decomposing spurious variations in both Markovian and Semi-Mark
    
[^57]: 揭示机器学习中的代表性不足和抽样偏差问题

    Shedding light on underrepresentation and Sampling Bias in machine learning. (arXiv:2306.05068v1 [cs.LG])

    [http://arxiv.org/abs/2306.05068](http://arxiv.org/abs/2306.05068)

    文章讨论了机器学习中的偏见对公正性的影响，提出抽样偏差的变体：样本量偏差和代表性不足偏差，揭示歧视可以分解为方差、偏差和噪声，并挑战了通常被接受的缓解方法。

    

    准确地衡量歧视对于忠实地评估训练好的机器学习（ML）模型的公正性至关重要。存在任何测量歧视的偏见都会导致现有差距的放大或低估。存在几种偏见来源，假设机器学习导致的偏见在不同的群体（例如女性与男性、白人与黑人等）之间平等分配。然而，如果偏见在不同的群体中分别存在，可能会加剧对特定亚群体的歧视。抽样偏差是文献中描述由抽样程序引起的偏差的术语。在本文中，我们试图通过引入明确定义的抽样偏差变体，即样本量偏差和代表性不足偏差，来消除这个术语的歧义。我们还展示了如何将歧视分解为方差、偏差和噪声。最后，我们挑战了通常被接受的缓解方法，即通过“简单地”从训练数据中删除敏感属性（例如种族或性别）来解决歧视问题。

    Accurately measuring discrimination is crucial to faithfully assessing fairness of trained machine learning (ML) models. Any bias in measuring discrimination leads to either amplification or underestimation of the existing disparity. Several sources of bias exist and it is assumed that bias resulting from machine learning is born equally by different groups (e.g. females vs males, whites vs blacks, etc.). If, however, bias is born differently by different groups, it may exacerbate discrimination against specific sub-populations. Sampling bias, is inconsistently used in the literature to describe bias due to the sampling procedure. In this paper, we attempt to disambiguate this term by introducing clearly defined variants of sampling bias, namely, sample size bias (SSB) and underrepresentation bias (URB). We show also how discrimination can be decomposed into variance, bias, and noise. Finally, we challenge the commonly accepted mitigation approach that discrimination can be addressed b
    
[^58]: 提高自监督视觉Transformer可视化提示调整的方法

    Improving Visual Prompt Tuning for Self-supervised Vision Transformers. (arXiv:2306.05067v1 [cs.LG])

    [http://arxiv.org/abs/2306.05067](http://arxiv.org/abs/2306.05067)

    本文研究了自监督视觉Transformer中视觉提示调整方法的提升，并确定了提示记号插入后续图块而非第一个图块的最佳位置。提出的方法能有效地减少确定最佳提示记号块位置的成本。

    

    视觉提示调整（VPT）是一种适用于下游任务的预训练视觉Transformer（ViTs）的有效调整方法。它利用额外的可学习记号，称为提示，来指导冻结的预训练ViTs。虽然VPT在受监督的视觉Transformer中显示了其适用性，但在自监督情况下常常表现不佳。通过实证观察，我们推断VPT的有效性在很大程度上取决于提示记号与ViT图块相互作用的方式。具体来说，当提示记号插入后续图块而不是第一个图块时，VPT在MAE和MoCo v3图像分类任务中显示出了改进的性能。这些观察表明，存在适用于插入提示记号的最佳块的位置。不幸的是，确定每个自监督ViT内用于不同未来场景的提示的最佳块是一个昂贵的过程。为了缓解这个问题，我们提出了一种简单而有效的方法。

    Visual Prompt Tuning (VPT) is an effective tuning method for adapting pretrained Vision Transformers (ViTs) to downstream tasks. It leverages extra learnable tokens, known as prompts, which steer the frozen pretrained ViTs. Although VPT has demonstrated its applicability with supervised vision transformers, it often underperforms with self-supervised ones. Through empirical observations, we deduce that the effectiveness of VPT hinges largely on the ViT blocks with which the prompt tokens interact. Specifically, VPT shows improved performance on image classification tasks for MAE and MoCo v3 when the prompt tokens are inserted into later blocks rather than the first block. These observations suggest that there exists an optimal location of blocks for the insertion of prompt tokens. Unfortunately, identifying the optimal blocks for prompts within each self-supervised ViT for diverse future scenarios is a costly process. To mitigate this problem, we propose a simple yet effective method t
    
[^59]: 结果控制的因果公平性

    Causal Fairness for Outcome Control. (arXiv:2306.05066v1 [cs.AI])

    [http://arxiv.org/abs/2306.05066](http://arxiv.org/abs/2306.05066)

    本论文研究了一种特定的决策任务叫做结果控制，针对涉及到刑事司法、福利、临床决策以及公共卫生等多个方面，提出了因果公平的概念以及一组因果工具和技术来推断结果控制中的公平性。

    

    随着社会向基于人工智能的决策基础设施的过渡，越来越多的曾由人类控制的决策现在被委托给了自动化系统。尽管这样的发展使社会的各个方面更有效率，但大量的证据表明，需要非常小心地使这种自动化决策系统变得公平和公正，即考虑到诸如性别、种族和宗教等敏感属性。本文研究了一种特定的决策任务，称为结果控制，其中自动化系统旨在优化一个结果变量Y，同时保持公平和公正。对于这样一个设置的兴趣范围从与刑事司法和福利有关的干预措施，一直到临床决策和公共卫生。我们通过因果镜片首先分析了利益的概念，该概念捕捉了一个特定个体从积极决策中获得了多少好处，对照事实的公平性，捕捉了如果涉及到不同的敏感属性，问题所在，以及充分性的概念，它捕捉了需要多少干预来改善因果系统。然后，我们介绍了一组因果工具和技术来推断结果控制中的公平性。我们通过算法公平性的案例研究展示了我们的方法的有用性，其中我们控制了贷款决策过程中的不公平歧视。

    As society transitions towards an AI-based decision-making infrastructure, an ever-increasing number of decisions once under control of humans are now delegated to automated systems. Even though such developments make various parts of society more efficient, a large body of evidence suggests that a great deal of care needs to be taken to make such automated decision-making systems fair and equitable, namely, taking into account sensitive attributes such as gender, race, and religion. In this paper, we study a specific decision-making task called outcome control in which an automated system aims to optimize an outcome variable $Y$ while being fair and equitable. The interest in such a setting ranges from interventions related to criminal justice and welfare, all the way to clinical decision-making and public health. In this paper, we first analyze through causal lenses the notion of benefit, which captures how much a specific individual would benefit from a positive decision, counterfac
    
[^60]: 面向DNN推断的多加速器平台上精度感知的延迟和能量平衡

    Precision-aware Latency and Energy Balancing on Multi-Accelerator Platforms for DNN Inference. (arXiv:2306.05060v1 [cs.LG])

    [http://arxiv.org/abs/2306.05060](http://arxiv.org/abs/2306.05060)

    介绍了ODiMO，一种硬件感知的工具，通过精细映射将DNN分割并在不同加速器上并行执行，以在维持准确性的前提下降低推断的能耗和延迟。

    

    在边缘计算中需要低延迟和低功耗地执行深度神经网络(DNN)，这促使了新的异构片上系统(System-on-Chips, SoCs)的开发，其封装了各种硬件加速器。如何最优地将DNN映射到这样的多加速器系统中仍然是一个开放的问题。我们提出ODiMO，一种硬件感知的工具，它在片上的不同加速器之间执行细粒度映射，分割单个层并并行执行它们，以降低推断的能量消耗或延迟，同时考虑每个加速器的量化精度以维护准确性。在三个流行的数据集/DNN对上追求准确性与能量或延迟空间中的帕累托最优网络，并部署在DIANA异构超低功耗边缘AI SoC上。我们展示了ODiMO相对于手动启发式映射可以降低能量/延迟高达33%/31%，而准确度下降有限(-0.53%/-0.32%)。

    The need to execute Deep Neural Networks (DNNs) at low latency and low power at the edge has spurred the development of new heterogeneous Systems-on-Chips (SoCs) encapsulating a diverse set of hardware accelerators. How to optimally map a DNN onto such multi-accelerator systems is an open problem. We propose ODiMO, a hardware-aware tool that performs a fine-grain mapping across different accelerators on-chip, splitting individual layers and executing them in parallel, to reduce inference energy consumption or latency, while taking into account each accelerator's quantization precision to maintain accuracy. Pareto-optimal networks in the accuracy vs. energy or latency space are pursued for three popular dataset/DNN pairs, and deployed on the DIANA heterogeneous ultra-low power edge AI SoC. We show that ODiMO reduces energy/latency by up to 33%/31% with limited accuracy drop (-0.53%/-0.32%) compared to manual heuristic mappings.
    
[^61]: 上下文感知的人类活动识别的神经符号方法

    Neuro-Symbolic Approaches for Context-Aware Human Activity Recognition. (arXiv:2306.05058v1 [cs.LG])

    [http://arxiv.org/abs/2306.05058](http://arxiv.org/abs/2306.05058)

    本文提出了一种采用语义损失函数的新方法，避免在分类过程中使用符号推理，可以解决现有上下文感知HAR的NeSy方法在部署和泛化能力方面的局限性。

    

    深度学习模型是传感器人类活动识别的标准解决方案，但是它们的部署常常受到标记数据不足和模型的不透明性的限制。神经符号人工智能（NeSy）为缓解这些问题提供了一个有趣的研究方向，即将关于上下文信息的知识注入到HAR深度学习分类器中。然而，现有的上下文感知HAR的NeSy方法需要在分类过程中使用计算复杂的符号推理器，使它们不太适合在资源受限的设备（例如移动设备）上部署。此外，用于上下文感知的HAR的NeSy方法从未在野外数据集上进行评估，它们在现实世界场景中的泛化能力也是有问题的。在本文中，我们提出了一种基于语义损失函数的新方法，通过在训练阶段注入知识约束来避免在分类过程中进行符号推理。我们在手稿和

    Deep Learning models are a standard solution for sensor-based Human Activity Recognition (HAR), but their deployment is often limited by labeled data scarcity and models' opacity. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate these issues by infusing knowledge about context information into HAR deep learning classifiers. However, existing NeSy methods for context-aware HAR require computationally expensive symbolic reasoners during classification, making them less suitable for deployment on resource-constrained devices (e.g., mobile devices). Additionally, NeSy approaches for context-aware HAR have never been evaluated on in-the-wild datasets, and their generalization capabilities in real-world scenarios are questionable. In this work, we propose a novel approach based on a semantic loss function that infuses knowledge constraints in the HAR model during the training phase, avoiding symbolic reasoning during classification. Our results on scripted and 
    
[^62]: 基于幅值注意力的动态剪枝方法

    Magnitude Attention-based Dynamic Pruning. (arXiv:2306.05056v1 [cs.CV])

    [http://arxiv.org/abs/2306.05056](http://arxiv.org/abs/2306.05056)

    本文提出一种基于幅值注意力的动态剪枝方法(MAP)，这种方法通过在前向和反向路径中应用权重的重要性动态地探索稀疏模型结构，从而实现了从冗余到有效的稀疏网络的无缝转换，得到了既具有高性能又经过精简的模型。

    

    现有的剪枝方法只会根据特定标准利用每个权重的重要性来搜索稀疏结构，但在训练过程中不会利用。本文提出一种新颖的方法——基于幅值注意力的动态剪枝方法(MAP)。该方法在前向和反向路径中应用权重的重要性动态地探索稀疏模型结构。通过基于权重的幅值定义幅值注意力，以连续实值数的形式使得从冗余到有效的稀疏网络的无缝转换成为可能，从而促进了高效的探索。此外，注意力机制确保了在稀疏网络中对重要层的更有效的更新。在训练的后期阶段，我们的方法从探索转向了开发，仅基于所探索出的结构更新由重要权重组成的稀疏模型，从而得到了既具有高性能又经过精简的模型。

    Existing pruning methods utilize the importance of each weight based on specified criteria only when searching for a sparse structure but do not utilize it during training. In this work, we propose a novel approach \textbf{M}agnitude \textbf{A}ttention-based Dynamic \textbf{P}runing (MAP) method, which applies the importance of weights throughout both the forward and backward paths to explore sparse model structures dynamically. Magnitude attention is defined based on the magnitude of weights as continuous real-valued numbers enabling a seamless transition from a redundant to an effective sparse network by promoting efficient exploration. Additionally, the attention mechanism ensures more effective updates for important layers within the sparse network. In later stages of training, our approach shifts from exploration to exploitation, exclusively updating the sparse model composed of crucial weights based on the explored structure, resulting in pruned models that not only achieve per
    
[^63]: 一种基于梯度的在线鲁棒深度神经网络训练方法：考虑噪声标签下的数据选择

    A Gradient-based Approach for Online Robust Deep Neural Network Training with Noisy Labels. (arXiv:2306.05046v1 [cs.LG])

    [http://arxiv.org/abs/2306.05046](http://arxiv.org/abs/2306.05046)

    本文提出了一种基于梯度的OGRS方法，用于在数据流场景下在线学习深度神经网络并处理噪声标签问题，能够自动选择不同干净比率的数据样本，无需改变参数设置。

    

    带有噪声标签的学习是许多实际应用中可扩展的训练问题，但是很少有之前的研究考虑在数据流场景下进行在线学习。本文提出了一种新颖的梯度方法，用于检测在线模型学习参数的噪声标签问题，称为Online Gradient-based Robust Selection（OGRS）。与使用离线训练样本选择所需的在每次训练之前估计数据集的干净比率的方法相比，OGRS可以在渐变更新步骤中自动选择不同干净比率的数据样本，而不需要改变参数设置。在训练过程中，OGRS方法在每次迭代中选择干净样本，并将所选样本馈送到增量更新模型参数。我们提供了详细的理论分析，证明了数据选择过程正在收敛。

    Learning with noisy labels is an important topic for scalable training in many real-world scenarios. However, few previous research considers this problem in the online setting, where the arrival of data is streaming. In this paper, we propose a novel gradient-based approach to enable the detection of noisy labels for the online learning of model parameters, named Online Gradient-based Robust Selection (OGRS). In contrast to the previous sample selection approach for the offline training that requires the estimation of a clean ratio of the dataset before each epoch of training, OGRS can automatically select clean samples by steps of gradient update from datasets with varying clean ratios without changing the parameter setting. During the training process, the OGRS method selects clean samples at each iteration and feeds the selected sample to incrementally update the model parameters. We provide a detailed theoretical analysis to demonstrate data selection process is converging to the 
    
[^64]: 基于非自回归条件扩散模型的时间序列预测研究

    Non-autoregressive Conditional Diffusion Models for Time Series Prediction. (arXiv:2306.05043v1 [cs.LG])

    [http://arxiv.org/abs/2306.05043](http://arxiv.org/abs/2306.05043)

    本文提出了一种名为TimeDiff的非自回归扩散模型来实现高质量时间序列预测，通过引入两种新颖的条件机制 - 未来混合和自回归初始化。实验结果表明，TimeDiff在各种强基线模型中取得了最佳整体性能。

    

    最近，去噪扩散模型在图像、音频和文本生成方面取得了重大突破。然而，如何适应其强大的建模能力来建模时间序列仍然是一个未解之谜。在本文中，我们提出了TimeDiff，一种非自回归扩散模型，通过引入两种新颖的条件机制 - 未来混合和自回归初始化，实现高质量时间序列预测。类似于teacher forcing，未来混合允许使用部分真实未来预测结果进行条件，而自回归初始化有助于更好地初始化模型并获得基本的时间序列模式，如短期趋势。在九个真实世界数据集上进行了大量实验。结果表明，TimeDiff始终优于现有的时间序列扩散模型，并在各种现有的强基线模型（包括transformers和FiLM）中取得了最佳的整体性能。

    Recently, denoising diffusion models have led to significant breakthroughs in the generation of images, audio and text. However, it is still an open question on how to adapt their strong modeling ability to model time series. In this paper, we propose TimeDiff, a non-autoregressive diffusion model that achieves high-quality time series prediction with the introduction of two novel conditioning mechanisms: future mixup and autoregressive initialization. Similar to teacher forcing, future mixup allows parts of the ground-truth future predictions for conditioning, while autoregressive initialization helps better initialize the model with basic time series patterns such as short-term trends. Extensive experiments are performed on nine real-world datasets. Results show that TimeDiff consistently outperforms existing time series diffusion models, and also achieves the best overall performance across a variety of the existing strong baselines (including transformers and FiLM).
    
[^65]: 能源高效的下行语义生成通信与文本到图像生成器

    Energy-Efficient Downlink Semantic Generative Communication with Text-to-Image Generators. (arXiv:2306.05041v1 [cs.LG])

    [http://arxiv.org/abs/2306.05041](http://arxiv.org/abs/2306.05041)

    本论文提出了一种语义生成通信框架，在该框架中，生成型用户利用文本到图像生成器来创建本地图像，从而减少基站下行传输能量消耗，但会增加用户能源消耗。通过生成用户选择算法，总能耗可以降低高达54%。

    

    本文引入了一种新颖的语义生成通信 (SGC) 框架，在该框架中，生成型用户利用文本到图像 (T2I) 生成器从下载的文本提示本地创建图像，而非生成型用户直接从基站 (BS) 下载图像。虽然生成型用户帮助减少了基站下行传输能量，但它们消耗了额外的能源用于图像生成和上传它们的生成器状态信息 (GSI)。我们制定了一个生成用户选择算法，来解决最小化基站和用户总能量消耗的问题。仿真结果证实，与所有非生成型用户的基线相比，我们提出的算法将总能量消耗降低了高达 54%。

    In this paper, we introduce a novel semantic generative communication (SGC) framework, where generative users leverage text-to-image (T2I) generators to create images locally from downloaded text prompts, while non-generative users directly download images from a base station (BS). Although generative users help reduce downlink transmission energy at the BS, they consume additional energy for image generation and for uploading their generator state information (GSI). We formulate the problem of minimizing the total energy consumption of the BS and the users, and devise a generative user selection algorithm. Simulation results corroborate that our proposed algorithm reduces total energy by up to 54% compared to a baseline with all non-generative users.
    
[^66]: 长期序列预测是否需要复杂的注意力机制和额外的长输入数据？

    Does Long-Term Series Forecasting Need Complex Attention and Extra Long Inputs?. (arXiv:2306.05035v1 [cs.LG])

    [http://arxiv.org/abs/2306.05035](http://arxiv.org/abs/2306.05035)

    本论文介绍了一种新的轻量级周期-注意机制，名为Periodformer，解决了长期序列预测中的两个主要问题，并证明了Transformer-based方法不需要额外长的输入序列来保证性能。

    

    随着基于Transformer的模型在各种时间序列任务上取得了令人印象深刻的性能，长期序列预测（LTSF）任务在近年来也受到了广泛关注。然而，由于基于Transformer的方法所固有的计算复杂性和需要长序列，它在LTSF任务上的应用仍然存在两个主要问题需要进一步研究：1）这些方法设计的稀疏注意机制是否实际上缩短了真实设备上的运行时间；2）这些模型是否需要额外长的输入序列来保证它们的性能？本论文的答案是否定的。因此，为更好地解决这两个问题，我们设计了一种轻量级的周期-注意机制（Periodformer），通过显式周期性和内置的接近性来重新设计长期子序列和短期子序列的聚合。同时，我们还嵌入了一个门控机制到Periodformer中以调整注意力的影响。

    As Transformer-based models have achieved impressive performance on various time series tasks, Long-Term Series Forecasting (LTSF) tasks have also received extensive attention in recent years. However, due to the inherent computational complexity and long sequences demanding of Transformer-based methods, its application on LTSF tasks still has two major issues that need to be further investigated: 1) Whether the sparse attention mechanism designed by these methods actually reduce the running time on real devices; 2) Whether these models need extra long input sequences to guarantee their performance? The answers given in this paper are negative. Therefore, to better copy with these two issues, we design a lightweight Period-Attention mechanism (Periodformer), which renovates the aggregation of long-term subseries via explicit periodicity and short-term subseries via built-in proximity. Meanwhile, a gating mechanism is embedded into Periodformer to regulate the influence of the attention
    
[^67]: 可扩展自适应的基于日志的异常检测，并带有专家反馈

    Scalable and Adaptive Log-based Anomaly Detection with Expert in the Loop. (arXiv:2306.05032v1 [cs.SE])

    [http://arxiv.org/abs/2306.05032](http://arxiv.org/abs/2306.05032)

    本文提出了一种适用于大规模云系统的准确、轻量级、自适应的基于日志的异常检测框架——SeaLog。该方法利用一种基于 Trie 结构的动态增长检测代理，可以接收人类专家反馈，能够在不断变化的日志数据中实现高准确度的实时异常检测，从而减少了手动验证的工作量。

    

    系统日志在维护软件系统可靠性方面起着关键作用。已有研究探索了自动基于日志的异常检测，并在基准数据集上取得了显著的准确性。然而，当应用于大规模云系统时，这些解决方案由于高资源消耗和缺乏对不断变化的日志数据的适应性而面临限制。因此，本文提出了一种准确，轻量级和自适应的基于日志的异常检测框架——SeaLog。我们的方法引入了一种基于 Trie 的检测代理 (TDA)，它利用一种轻量级、动态增长的 Trie 结构进行实时异常检测。为了增强 TDA 对不断变化的日志数据的准确性，我们使其能够从专家获得反馈。有趣的是，我们的研究发现，现代的大语言模型，如 ChatGPT，可以提供与人类专家相当一致性的反馈，这可能有助于减少手动验证的工作量。我们对该方法进行了全面的实验评估，并证明了其在大规模环境下的可扩展性和高效性。

    System logs play a critical role in maintaining the reliability of software systems. Fruitful studies have explored automatic log-based anomaly detection and achieved notable accuracy on benchmark datasets. However, when applied to large-scale cloud systems, these solutions face limitations due to high resource consumption and lack of adaptability to evolving logs. In this paper, we present an accurate, lightweight, and adaptive log-based anomaly detection framework, referred to as SeaLog. Our method introduces a Trie-based Detection Agent (TDA) that employs a lightweight, dynamically-growing trie structure for real-time anomaly detection. To enhance TDA's accuracy in response to evolving log data, we enable it to receive feedback from experts. Interestingly, our findings suggest that contemporary large language models, such as ChatGPT, can provide feedback with a level of consistency comparable to human experts, which can potentially reduce manual verification efforts. We extensively 
    
[^68]: 鲁棒性神经架构搜索中的可推广轻量级代理

    Generalizable Lightweight Proxy for Robust NAS against Diverse Perturbations. (arXiv:2306.05031v1 [cs.LG])

    [http://arxiv.org/abs/2306.05031](http://arxiv.org/abs/2306.05031)

    本研究提出了一种轻量级的、零成本代理，通过考虑干净图像和受扰动图像的特征、参数和梯度的一致性，实现了高效、快速地搜索鲁棒NAS架构。

    

    最近的神经架构搜索（NAS）框架已成功在给定条件下（例如性能或延迟）找到最佳架构。然而，它们只针对干净图像的性能寻找最佳架构，而在实践中，对于各种类型的扰动或损坏的鲁棒性非常重要。尽管存在几个通过将对抗性训练集成到单次NAS中来解决这个问题的鲁棒性NAS框架，但它们的局限性在于它们只考虑对抗攻击的鲁棒性，并需要大量计算资源才能为单个任务发现最佳架构，这使它们在实际应用中不切实际。为了解决这些挑战，我们提出了一种新颖的轻量级鲁棒零成本代理，该代理在初始化状态下考虑干净图像和受扰动图像的特征、参数和梯度的一致性。我们的方法有助于快速、高效地搜索鲁棒NAS架构。

    Recent neural architecture search (NAS) frameworks have been successful in finding optimal architectures for given conditions (e.g., performance or latency). However, they search for optimal architectures in terms of their performance on clean images only, while robustness against various types of perturbations or corruptions is crucial in practice. Although there exist several robust NAS frameworks that tackle this issue by integrating adversarial training into one-shot NAS, however, they are limited in that they only consider robustness against adversarial attacks and require significant computational resources to discover optimal architectures for a single task, which makes them impractical in real-world scenarios. To address these challenges, we propose a novel lightweight robust zero-cost proxy that considers the consistency across features, parameters, and gradients of both clean and perturbed images at the initialization state. Our approach facilitates an efficient and rapid sea
    
[^69]: 基于Transformer的多级多示例学习用于全扫描图像分类

    Multi-level Multiple Instance Learning with Transformer for Whole Slide Image Classification. (arXiv:2306.05029v1 [cs.CV])

    [http://arxiv.org/abs/2306.05029](http://arxiv.org/abs/2306.05029)

    本文提出了一种基于Transformer的多级多示例学习（MMIL-Transformer）方法，该方法能够有效地处理涉及大量实例的MIL任务。

    

    全扫描图像（WSI）是一种高分辨率的组织扫描图像，被广泛用于计算机辅助诊断（CAD）。由于极高的分辨率和区域级别注释的有限性，对于基于WSI的数字诊断使用深度学习方法是具有挑战性的。多示例学习（MIL）是解决弱注释问题的强有力工具，而Transformer已在视觉任务领域表现出了巨大的成功。结合两者将为基于深度学习的图像诊断提供新的见解。然而，由于单级MIL的限制和注意机制对序列长度的限制，将Transformer直接应用于基于WSI的MIL任务并不实用。为了解决这个问题，我们提出了一种多级MIL与Transformer（MMIL-Transformer）方法。通过引入层次结构到MIL中，这种方法能够有效地处理涉及大量实例的MIL任务。

    Whole slide image (WSI) refers to a type of high-resolution scanned tissue image, which is extensively employed in computer-assisted diagnosis (CAD). The extremely high resolution and limited availability of region-level annotations make it challenging to employ deep learning methods for WSI-based digital diagnosis. Multiple instance learning (MIL) is a powerful tool to address the weak annotation problem, while Transformer has shown great success in the field of visual tasks. The combination of both should provide new insights for deep learning based image diagnosis. However, due to the limitations of single-level MIL and the attention mechanism's constraints on sequence length, directly applying Transformer to WSI-based MIL tasks is not practical. To tackle this issue, we propose a Multi-level MIL with Transformer (MMIL-Transformer) approach. By introducing a hierarchical structure to MIL, this approach enables efficient handling of MIL tasks that involve a large number of instances.
    
[^70]: 线性条件VAE和分层VAE中的后验崩溃现象

    Posterior Collapse in Linear Conditional and Hierarchical Variational Autoencoders. (arXiv:2306.05023v1 [stat.ML])

    [http://arxiv.org/abs/2306.05023](http://arxiv.org/abs/2306.05023)

    本文研究了高度相似的变分后验分布和先验分布之间的后验崩溃现象，特别地，通过对线性条件VAE和分层VAE进行分析，证明了这种现象是由于潜在变量层次关系不清晰而引起的。

    

    在变分自编码器（VAE）中，后验崩溃现象指的是变分后验分布与先验分布的相似度过高，导致编码器提取的潜在变量保存的输入数据信息较少，无法为解码器的数据重建过程产生有意义的表示。尽管该现象一直是VAEs性能的研究热点，但是对于后验崩溃的理论却相对薄弱，特别是在非标准的VAEs中。本文通过对两类重要而常见又较少研究的VAEs进行非平凡的理论分析，即具有两个潜在变量层次的线性条件VAE和分层VAE，提升了对后验崩溃的理论认识，证明了其成因。

    The posterior collapse phenomenon in variational autoencoders (VAEs), where the variational posterior distribution closely matches the prior distribution, can hinder the quality of the learned latent variables. As a consequence of posterior collapse, the latent variables extracted by the encoder in VAEs preserve less information from the input data and thus fail to produce meaningful representations as input to the reconstruction process in the decoder. While this phenomenon has been an actively addressed topic related to VAEs performance, the theory for posterior collapse remains underdeveloped, especially beyond the standard VAEs. In this work, we advance the theoretical understanding of posterior collapse to two important and prevalent yet less studied classes of VAEs: conditional VAEs and hierarchical VAEs. Specifically, via a non-trivial theoretical analysis of linear conditional VAEs and hierarchical VAEs with two levels of latent, we prove that the cause of posterior collapses i
    
[^71]: Mixed-TD: 基于层特定张量分解的高效神经网络加速器

    Mixed-TD: Efficient Neural Network Accelerator with Layer-Specific Tensor Decomposition. (arXiv:2306.05021v1 [cs.LG])

    [http://arxiv.org/abs/2306.05021](http://arxiv.org/abs/2306.05021)

    本文提出了一种基于层特定张量分解的神经网络加速器Mixed-TD，采用混合方式的SVD和CPD方法，实现了高压缩率，同时保持了与原始神经网络类似的准确性，并通过动态映射方法实现了对可用片上存储器和计算资源的有效利用。

    

    神经网络设计相当多样化，从VGG到ResNet，从卷积神经网络到变换器。为了设计效率高的加速器，许多工作采用了基于数据流的、层间流水线结构的体系结构，针对每一层进行了自定义硬件设计，实现了超高的吞吐量和低延迟。神经网络部署到此类数据流体系结构加速器上通常受可用片上内存的限制，因为预加载神经网络的权重到片上以最大化系统性能是理想的。为了解决这个问题，网络通常会通过修剪、量化和张量分解等方法进行压缩。本文提出了一种将CNN映射到FPGA上的框架，基于一种新颖的张量分解方法Mixed-TD。该方法采用了混合方式的层特定奇异值分解（SVD）和典型多项式分解（CPD），在保持原始神经网络准确性的同时实现了高压缩率。Mixed-TD框架采用动态映射方法，以实现有效地利用可用的片上存储器和计算资源。实验结果表明，Mixed-TD框架在减少内存占用和计算周期方面能够显著提高性能，同时保持与原始未压缩神经网络相似的准确性。

    Neural Network designs are quite diverse, from VGG-style to ResNet-style, and from Convolutional Neural Networks to Transformers. Towards the design of efficient accelerators, many works have adopted a dataflow-based, inter-layer pipelined architecture, with a customised hardware towards each layer, achieving ultra high throughput and low latency. The deployment of neural networks to such dataflow architecture accelerators is usually hindered by the available on-chip memory as it is desirable to preload the weights of neural networks on-chip to maximise the system performance. To address this, networks are usually compressed before the deployment through methods such as pruning, quantization and tensor decomposition. In this paper, a framework for mapping CNNs onto FPGAs based on a novel tensor decomposition method called Mixed-TD is proposed. The proposed method applies layer-specific Singular Value Decomposition (SVD) and Canonical Polyadic Decomposition (CPD) in a mixed manner, achi
    
[^72]: 使用深度神经网络的非侵入式负载监测（NILM）: 一项综述。

    Non-Intrusive Load Monitoring (NILM) using Deep Neural Networks: A Review. (arXiv:2306.05017v1 [eess.SP])

    [http://arxiv.org/abs/2306.05017](http://arxiv.org/abs/2306.05017)

    本文综述了基于深度学习算法的最新NILM方法，比较了它们的性能表现，为需求侧管理、能耗监测提供了一种精确的非侵入式负载监测方法。

    

    需求侧管理现在涵盖了更多的居民负载。为了有效地应用需求响应策略，定期观察各种家用电器对总能耗的贡献至关重要。非侵入式负载监测（NILM），也称负载分解，是一种将总能耗剖分为家庭内单个电器负载轮廓的方法。NILM有多个应用，包括需求侧管理、能耗监测和分析。各种方法，包括机器学习和深度学习，已被用于实现和改进NILM算法。本文综述了一些基于深度学习的最近NILM方法，并介绍了最精确的住宅负载方法。它总结了NILM评估的公共数据库，并使用标准性能指标比较方法。

    Demand-side management now encompasses more residential loads. To efficiently apply demand response strategies, it's essential to periodically observe the contribution of various domestic appliances to total energy consumption. Non-intrusive load monitoring (NILM), also known as load disaggregation, is a method for decomposing the total energy consumption profile into individual appliance load profiles within the household. It has multiple applications in demand-side management, energy consumption monitoring, and analysis. Various methods, including machine learning and deep learning, have been used to implement and improve NILM algorithms. This paper reviews some recent NILM methods based on deep learning and introduces the most accurate methods for residential loads. It summarizes public databases for NILM evaluation and compares methods using standard performance metrics.
    
[^73]: 从高保真度数据中学习子网格尺度闭合形式方程：机遇与挑战

    Learning Closed-form Equations for Subgrid-scale Closures from High-fidelity Data: Promises and Challenges. (arXiv:2306.05014v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2306.05014](http://arxiv.org/abs/2306.05014)

    本文发现了非线性梯度模型（NGM），它是可解析地使用Taylor级数拓展导出的闭合形式，从而实现对地球系统复杂过程的子网格尺度（SGS）闭合/参数化。

    

    在发现地球系统复杂过程的子网格尺度（SGS）闭合/参数化的可解释性闭合形式方程上，越来越多的人表现出浓厚兴趣。本研究使用广泛的库应用通用的方程发现技术，从经过滤波的二维强迫湍流和瑞利 - 贝纳德对流的直接数值模拟中学习闭合形式。在常见的滤波器范围内，我们强有力地发现了动量和热通量的相同形式的闭合形式。这些闭合形式取决于被过滤变量（速度、温度）的梯度的非线性组合，其中的常数独立于流体/流动特性，仅依赖于过滤器类型/大小。我们表明，这些闭合形式是非线性梯度模型（NGM），可以使用Taylor级数展开分析地导出。事实上，我们认为，使用常见的（无物理信息的）方程发现算法时，无论是什么系统/物理学，发现的闭合形式始终与Taylor级数一致。

    There is growing interest in discovering interpretable, closed-form equations for subgrid-scale (SGS) closures/parameterizations of complex processes in Earth system. Here, we apply a common equation-discovery technique with expansive libraries to learn closures from filtered direct numerical simulations of 2D forced turbulence and Rayleigh-B\'enard convection (RBC). Across common filters, we robustly discover closures of the same form for momentum and heat fluxes. These closures depend on nonlinear combinations of gradients of filtered variables (velocity, temperature), with constants that are independent of the fluid/flow properties and only depend on filter type/size. We show that these closures are the nonlinear gradient model (NGM), which is derivable analytically using Taylor-series expansions. In fact, we suggest that with common (physics-free) equation-discovery algorithms, regardless of the system/physics, discovered closures are always consistent with the Taylor-series. Like 
    
[^74]: 基于对比学习加权注意力的电子商务个性化排名专家混合模型

    Attention Weighted Mixture of Experts with Contrastive Learning for Personalized Ranking in E-commerce. (arXiv:2306.05011v1 [cs.IR])

    [http://arxiv.org/abs/2306.05011](http://arxiv.org/abs/2306.05011)

    电子商务个性化排名专家混合模型，利用注意力机制和MoE框架进行特征交互建模，采用对比学习提升历史行为较少的长尾用户个性化排名结果。

    

    排名模型在电子商务搜索和推荐中起着至关重要的作用。有效的排名模型应该根据用户喜好为每个用户提供个性化的排名列表。现有算法通常从用户行为序列中提取用户表示向量，然后将该向量与其他特征一起馈入前馈神经网络（FFN）进行特征交互，并最终生成个性化排名得分。尽管过去取得了巨大的进展，但仍有改进的空间。首先，不同用户的个性化特征交互模式没有明确建模。其次，由于数据稀疏，大多数现有算法在具有少量历史行为的长尾用户上的个性化排名结果较差。为了克服这两个挑战，我们提出了基于对比学习加权注意力的个性化排名专家混合模型（AW-MoE）。首先，AW-MoE利用MoE框架捕获个性化特征交互模式，

    Ranking model plays an essential role in e-commerce search and recommendation. An effective ranking model should give a personalized ranking list for each user according to the user preference. Existing algorithms usually extract a user representation vector from the user behavior sequence, then feed the vector into a feed-forward network (FFN) together with other features for feature interactions, and finally produce a personalized ranking score. Despite tremendous progress in the past, there is still room for improvement. Firstly, the personalized patterns of feature interactions for different users are not explicitly modeled. Secondly, most of existing algorithms have poor personalized ranking results for long-tail users with few historical behaviors due to the data sparsity. To overcome the two challenges, we propose Attention Weighted Mixture of Experts (AW-MoE) with contrastive learning for personalized ranking. Firstly, AW-MoE leverages the MoE framework to capture personalized 
    
[^75]: COURIER: 基于对比学习的大规模图像特征预训练中的用户意图还原

    COURIER: Contrastive User Intention Reconstruction for Large-Scale Pre-Train of Image Features. (arXiv:2306.05001v1 [cs.CV])

    [http://arxiv.org/abs/2306.05001](http://arxiv.org/abs/2306.05001)

    该论文提出了一种基于对比学习的推荐方法，以获得更好的个性化推荐表现。

    

    随着多媒体互联网的发展，视觉特征已成为影响用户兴趣的重要因素。因此，将视觉特征融入点击率（CTR）预测的进一步提升方向是颇具前景的。但我们发现，仅将已有预训练方法训练得到的图像嵌入注入到模型中仅能产生较小的改进。我们将这种现象归因于以下两个原因：首先，预训练方法是为了明确的、重点放在语义特征上的计算机视觉任务而设计的，无法学习到个性化推荐中的个人兴趣; 其次，预训练的只包含语义信息的图像嵌入与我们在CTR预测任务中所已有的类别和物品标题等语义特征相比有较小的信息增益。我们认为需要一种针对推荐任务的预训练方法以获得更多改进。为此，我们提出了一种基于对比学习的推荐方法。

    With the development of the multi-media internet, visual characteristics have become an important factor affecting user interests. Thus, incorporating visual features is a promising direction for further performance improvements in click-through rate (CTR) prediction. However, we found that simply injecting the image embeddings trained with established pre-training methods only has marginal improvements. We attribute the failure to two reasons: First, The pre-training methods are designed for well-defined computer vision tasks concentrating on semantic features, and they cannot learn personalized interest in recommendations. Secondly, pre-trained image embeddings only containing semantic information have little information gain, considering we already have semantic features such as categories and item titles as inputs in the CTR prediction task. We argue that a pre-training method tailored for recommendation is necessary for further improvements. To this end, we propose a recommendatio
    
[^76]: 通过卷积神经网络预测救护车需求

    Ambulance Demand Prediction via Convolutional Neural Networks. (arXiv:2306.04994v1 [cs.LG])

    [http://arxiv.org/abs/2306.04994](http://arxiv.org/abs/2306.04994)

    本研究提出一种新的卷积神经网络(CNN)体系结构，通过将时间序列数据转化为热力图，预测救护车需求。同时采用特征选择和超参数优化框架，结合历史救护车需求和外部信息，如天气、事件、节日和时间。

    

    减少响应时间对于紧急医疗服务来说非常重要，可降低患者的等待时间，提高他们的生存率。许多模型可用于优化诸如救护车分配和调度等运营任务。将精确的需求预测包含在此类模型中可提高运营决策制定。在此背景下，我们提出了一种新的卷积神经网络(CNN)体系结构，它将时间序列数据转化为热力图以预测救护车需求。应用这种预测需要纳入影响救护车需求的外部特征。我们通过提供一个灵活的、通用的CNN体系结构，允许包含具有不同尺寸的外部特征，为现有文献做出了贡献。此外，我们提供了一个特征选择和超参数优化框架，利用贝叶斯优化。我们整合历史救护车需求和外部信息，如天气，事件，节日和时间。

    Minimizing response times is crucial for emergency medical services to reduce patients' waiting times and to increase their survival rates. Many models exist to optimize operational tasks such as ambulance allocation and dispatching. Including accurate demand forecasts in such models can improve operational decision-making. Against this background, we present a novel convolutional neural network (CNN) architecture that transforms time series data into heatmaps to predict ambulance demand. Applying such predictions requires incorporating external features that influence ambulance demands. We contribute to the existing literature by providing a flexible, generic CNN architecture, allowing for the inclusion of external features with varying dimensions. Additionally, we provide a feature selection and hyperparameter optimization framework utilizing Bayesian optimization. We integrate historical ambulance demand and external information such as weather, events, holidays, and time. To show t
    
[^77]: 超越概率划分：语义感知分组校准神经网络

    Beyond Probability Partitions: Calibrating Neural Networks with Semantic Aware Grouping. (arXiv:2306.04985v1 [cs.LG])

    [http://arxiv.org/abs/2306.04985](http://arxiv.org/abs/2306.04985)

    这篇论文提出了一种更为普适的校准误差定义——分区校准误差（PCE），指出了分区划分是各种校准误差指标之间的关键区别。作者提出了一个命题：准确的模型应该在任何分区上都具有校准性，而不仅仅是预测概率分区。通过语义相关的分区函数，作者证明了分区函数的粒度与模型准确性和校准之间的关系。

    

    研究表明，深度网络往往对其预测过于乐观，导致预测误差被低估。由于数据的有限性，现有研究已经提出了各种基于模型预测概率的方法来对数据进行分组并评估校准误差。本文提出了一种更加通用的校准误差定义，称为分区校准误差（PCE），揭示了这些校准误差指标之间的关键区别在于如何将数据空间划分。我们提出了一个直观的命题，即准确的模型应该在任何分区上都具有校准性，这表明输入空间分区可以扩展到不仅仅是预测概率分区，还可以包括与输入直接相关的分区。通过语义相关的分区函数，我们证明了模型准确性和校准之间的关系在于分区函数的粒度。

    Research has shown that deep networks tend to be overly optimistic about their predictions, leading to an underestimation of prediction errors. Due to the limited nature of data, existing studies have proposed various methods based on model prediction probabilities to bin the data and evaluate calibration error. We propose a more generalized definition of calibration error called Partitioned Calibration Error (PCE), revealing that the key difference among these calibration error metrics lies in how the data space is partitioned. We put forth an intuitive proposition that an accurate model should be calibrated across any partition, suggesting that the input space partitioning can extend beyond just the partitioning of prediction probabilities, and include partitions directly related to the input. Through semantic-related partitioning functions, we demonstrate that the relationship between model accuracy and calibration lies in the granularity of the partitioning function. This highlight
    
[^78]: G$^2$uardFL: 通过属性化客户端图聚类来防御后门攻击的联邦学习保护框架

    G$^2$uardFL: Safeguarding Federated Learning Against Backdoor Attacks through Attributed Client Graph Clustering. (arXiv:2306.04984v1 [cs.CR])

    [http://arxiv.org/abs/2306.04984](http://arxiv.org/abs/2306.04984)

    本论文提出了G$^2$uardFL，这是一个基于属性化客户端图聚类的联邦学习保护框架，能够有效识别恶意客户端，即使恶意客户端数量高达50％。

    

    作为协同范式，联邦学习（FL）使客户端能够进行集体模型训练而不交换各自的本地数据。然而，FL仍然容易受到后门攻击的影响，攻击者会通过篡改模型权重注入有毒数据，从而得到针对特定样本的攻击者选择的预测结果。现有的对策主要基于异常检测，但由于量化客户模型相似性的不足，这些对策可能会错误地拒绝合法权重，同时接受恶意权重。其他防御机制仅在面对少量恶意客户端，例如少于10％的恶意客户端时才有效。为了解决这些漏洞，我们提出了G$^2$uardFL，这是一个保护框架，它将检测恶意客户端视为一个属性图聚类问题，从而保护FL系统。该框架采用客户端图聚类技术，根据模型权重的相似性将客户端分类为正常或恶意。通过采用对客户端固有属性进行编码的属性标签，G$^2$uardFL在识别受损客户端方面优于现有的防御机制，而不排除合法客户端。实验结果表明，即使有50％的客户端是恶意的，G$^2$uardFL也能显著降低后门攻击成功率。

    As a collaborative paradigm, Federated Learning (FL) empowers clients to engage in collective model training without exchanging their respective local data. Nevertheless, FL remains vulnerable to backdoor attacks in which an attacker compromises malicious clients, and injects poisoned model weights into the aggregation process to yield attacker-chosen predictions for particular samples. Existing countermeasures, mainly based on anomaly detection, may erroneously reject legitimate weights while accepting malicious ones, which is due to inadequacies in quantifying client model similarities. Other defense mechanisms prove effective exclusively when confronted with a restricted number of malicious clients, e.g., less than 10%. To address these vulnerabilities, we present G$^2$uardFL, a protective framework that reframes the detection of malicious clients as an attributed graph clustering problem, thereby safeguarding FL systems. This framework employs a client graph clustering technique to
    
[^79]: CoCo: 一种用于无监督领域自适应图分类的耦合对比框架

    CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification. (arXiv:2306.04979v1 [cs.LG])

    [http://arxiv.org/abs/2306.04979](http://arxiv.org/abs/2306.04979)

    CoCo是一种耦合对比图表示学习框架，其中包含一个图卷积网络和一个分层图内核网络，通过耦合对比学习减少领域差异，用于无监督领域自适应图分类。

    

    虽然图神经网络在图分类中取得了显著成果，但它们通常需要大量特定任务的标签，这可能需要极大的代价来获得。一种可靠的解决方案是探索其他标注图以增强目标域的无监督学习，但如何将图神经网络应用到领域适应中仍未解决，因为对图拓扑的不充分探索以及相当大的领域偏差。本文提出了一种称为CoCo（Coupled Contrastive Graph Representation Learning）方案，该方案从耦合学习分支中提取拓扑信息，并通过耦合对比学习减少领域差异。CoCo包含一个图卷积网络分支和分层图内核网络分支，分别用隐式和显式方式探索图拓扑。此外，我们将耦合分支结合到一个全面的多视角对比学习框架中，

    Although graph neural networks (GNNs) have achieved impressive achievements in graph classification, they often need abundant task-specific labels, which could be extensively costly to acquire. A credible solution is to explore additional labeled graphs to enhance unsupervised learning on the target domain. However, how to apply GNNs to domain adaptation remains unsolved owing to the insufficient exploration of graph topology and the significant domain discrepancy. In this paper, we propose \underline{Co}upled \underline{Co}ntrastive Graph Representation Learning (\method{}), which extracts the topological information from coupled learning branches and reduces the domain discrepancy with coupled contrastive learning. \method{} contains a graph convolutional network branch and a hierarchical graph kernel network branch, which explore graph topology in implicit and explicit manners. Besides, we incorporate coupled branches into a holistic multi-view contrastive learning framework, which 
    
[^80]: 基于数据驱动置信度最小化的保守预测

    Conservative Prediction via Data-Driven Confidence Minimization. (arXiv:2306.04974v1 [cs.LG])

    [http://arxiv.org/abs/2306.04974](http://arxiv.org/abs/2306.04974)

    该论文提出了一种可以在处理不常见样本时推迟到人类判断的保守模型方法。该方法使用基于数据驱动置信度最小化（DCM）的算法，在辅助数据集中选择感兴趣的OOD（Out-of-Distribution）区域的样本，进而实现可靠地分离ID（In-Distribution）和OOD输入。

    

    机器学习模型的错误代价很高，特别是在诸如医疗保健等安全关键领域，这种错误可能会阻止机器学习的部署。在这些情况下，具有保守性的模型——当它们可能出现错误时可以推迟到人类判断——可能会提供解决方案。然而，检测异常或复杂示例明显具有挑战性，因为无法预测所有可能的测试输入。为了解决这个问题，先前的工作提出了在辅助伪OOD数据集上最小化模型置信度的方法。我们在理论上分析了置信度最小化的影响，并表明辅助数据集的选择是关键的。具体而言，如果辅助数据集包括来自感兴趣的OOD区域的样本，置信度最小化可以通过预测置信度可靠地分离ID和OOD输入。受到这一结果的启示，我们提出了基于数据驱动置信度最小化（DCM）的算法。

    Errors of machine learning models are costly, especially in safety-critical domains such as healthcare, where such mistakes can prevent the deployment of machine learning altogether. In these settings, conservative models -- models which can defer to human judgment when they are likely to make an error -- may offer a solution. However, detecting unusual or difficult examples is notably challenging, as it is impossible to anticipate all potential inputs at test time. To address this issue, prior work has proposed to minimize the model's confidence on an auxiliary pseudo-OOD dataset. We theoretically analyze the effect of confidence minimization and show that the choice of auxiliary dataset is critical. Specifically, if the auxiliary dataset includes samples from the OOD region of interest, confidence minimization provably separates ID and OOD inputs by predictive confidence. Taking inspiration from this result, we present data-driven confidence minimization (DCM), which minimizes confid
    
[^81]: 进化和学习的融合：八个作品综述

    A Melting Pot of Evolution and Learning. (arXiv:2306.04971v1 [cs.NE])

    [http://arxiv.org/abs/2306.04971](http://arxiv.org/abs/2306.04971)

    这篇综述介绍了我们小组在进化算法与机器学习、深度学习的融合方面的八个重要作品。

    

    本文概述了我们小组的八个最近作品，这些作品成功地将进化算法与机器学习和深度学习相融合：1.使用进化符号回归进行二元和多项分类，2.优雅的集成：一种新颖的分类集成算法，3.Python版演化计算工具包，4.基于深度学习的图像分类的激活函数的进化，5.遗传算法和新奇搜索的自适应组合，用于深度神经进化，6.用于在深度网络中生成对抗性实例的进化、无梯度、查询高效、黑盒算法，7.深度神经网络中的挫败性解释，8.隐形斑点：自然黑匣子对目标探测器的对抗攻击。

    We survey eight recent works by our group, involving the successful blending of evolutionary algorithms with machine learning and deep learning: 1. Binary and Multinomial Classification through Evolutionary Symbolic Regression, 2. Classy Ensemble: A Novel Ensemble Algorithm for Classification, 3. EC-KitY: Evolutionary Computation Tool Kit in Python, 4. Evolution of Activation Functions for Deep Learning-Based Image Classification, 5. Adaptive Combination of a Genetic Algorithm and Novelty Search for Deep Neuroevolution, 6. An Evolutionary, Gradient-Free, Query-Efficient, Black-Box Algorithm for Generating Adversarial Instances in Deep Networks, 7. Foiling Explanations in Deep Neural Networks, 8. Patch of Invisibility: Naturalistic Black-Box Adversarial Attacks on Object Detectors.
    
[^82]: 利用语言识别技术提升代码混合文本分类

    Leveraging Language Identification to Enhance Code-Mixed Text Classification. (arXiv:2306.04964v1 [cs.CL])

    [http://arxiv.org/abs/2306.04964](http://arxiv.org/abs/2306.04964)

    本研究提出了一种改进代码混合文本分类的流程，包括数据预处理、词级语言识别、语言增强和模型训练等步骤，用于下游任务，如情感分析。我们探索了词级交错和句子后置的语言信息插入方法，以提高基于BERT的模型在低资源代码混合印地语-英语数据集上的性能。

    

    在同一段文本中使用多种语言叫做代码混合。当前社交媒体平台上，特别是英语和地方语言混合使用的数据越来越多。现有的深度学习模型没有充分利用代码混合文本中的隐性语言信息。本研究旨在通过尝试不同的语言增强方法，提高基于BERT的模型在低资源代码混合印地语-英语数据集上的性能。我们提出了一种改进代码混合系统的流程，包括数据预处理、词级语言识别、语言增强和模型训练等步骤，用于下游任务，如情感分析。在BERT模型中进行语言增强时，我们探索了词级交错和句子后置的语言信息插入方法。我们测试了原始BERT模型和经过代码混合改进的HingBERT在各自基准数据集上的性能。

    The usage of more than one language in the same text is referred to as Code Mixed. It is evident that there is a growing degree of adaption of the use of code-mixed data, especially English with a regional language, on social media platforms. Existing deep-learning models do not take advantage of the implicit language information in the code-mixed text. Our study aims to improve BERT-based models performance on low-resource Code-Mixed Hindi-English Datasets by experimenting with language augmentation approaches. We propose a pipeline to improve code-mixed systems that comprise data preprocessing, word-level language identification, language augmentation, and model training on downstream tasks like sentiment analysis. For language augmentation in BERT models, we explore word-level interleaving and post-sentence placement of language information. We have examined the performance of vanilla BERT-based models and their code-mixed HingBERT counterparts on respective benchmark datasets, comp
    
[^83]: arXiv4TGC：用于时态图聚类的大规模数据集

    arXiv4TGC: Large-Scale Datasets for Temporal Graph Clustering. (arXiv:2306.04962v1 [cs.AI])

    [http://arxiv.org/abs/2306.04962](http://arxiv.org/abs/2306.04962)

    arXiv4TGC提供了一组适用于大规模时态图聚类的新颖学术数据集，解决了缺乏可靠数据集来评估聚类性能的挑战。

    

    时态图聚类是时态图学习中的重要任务，其重点是在时态图上的节点聚类，并由于时态图方法的机制，为大规模图结构提供了更大的灵活性。然而，时态图聚类的发展目前存在一个显著问题：缺乏适合和可靠的大规模时态图数据集来评估聚类性能。为解决这一挑战，我们构建了arXiv4TGC，这是一组用于大规模时态图聚类的新颖学术数据集。其中，最大的数据集arXivLarge包含130万个有标签可用节点和1000万个时态边缘。

    Temporal graph clustering (TGC) is a crucial task in temporal graph learning. Its focus is on node clustering on temporal graphs, and it offers greater flexibility for large-scale graph structures due to the mechanism of temporal graph methods. However, the development of TGC is currently constrained by a significant problem: the lack of suitable and reliable large-scale temporal graph datasets to evaluate clustering performance. In other words, most existing temporal graph datasets are in small sizes, and even large-scale datasets contain only a limited number of available node labels. It makes evaluating models for large-scale temporal graph clustering challenging. To address this challenge, we build arXiv4TGC, a set of novel academic datasets (including arXivAI, arXivCS, arXivMath, arXivPhy, and arXivLarge) for large-scale temporal graph clustering. In particular, the largest dataset, arXivLarge, contains 1.3 million labeled available nodes and 10 million temporal edges. We further 
    
[^84]: 通过非凸迭代加权最小二乘法同时恢复结构化数据

    Recovering Simultaneously Structured Data via Non-Convex Iteratively Reweighted Least Squares. (arXiv:2306.04961v1 [cs.LG])

    [http://arxiv.org/abs/2306.04961](http://arxiv.org/abs/2306.04961)

    该论文提出了一种非凸迭代加权最小二乘法用于同时恢复行稀疏和低秩的数据矩阵，能够在最小样本复杂度的情况下局部二次收敛到同时结构化的数据矩阵，并在实验中表现出有利的经验收敛性。

    

    我们提出了一种新的算法，用于从线性观测中恢复遵循多个异构低维结构的数据。针对同时行稀疏和低秩的数据矩阵，我们提出并分析了一种迭代加权最小二乘（IRLS）算法，能够利用这两种结构。特别地，它优化了一种非凸稀疏性和秩的组合代理，其中平衡被建入到算法中。我们证明了在最小样本复杂度的情况下（最多常数和一个对数因子），迭代方式局部二次收敛到同时结构化数据矩阵，这对于组合凸代理而言是不可能的。在实验中，我们展示了IRLS方法表现出了有利的经验收敛性，从比最先进的方法更少的测量中确定了同时行稀疏和低秩矩阵。

    We propose a new algorithm for the problem of recovering data that adheres to multiple, heterogeneous low-dimensional structures from linear observations. Focusing on data matrices that are simultaneously row-sparse and low-rank, we propose and analyze an iteratively reweighted least squares (IRLS) algorithm that is able to leverage both structures. In particular, it optimizes a combination of non-convex surrogates for row-sparsity and rank, a balancing of which is built into the algorithm. We prove locally quadratic convergence of the iterates to a simultaneously structured data matrix in a regime of minimal sample complexity (up to constants and a logarithmic factor), which is known to be impossible for a combination of convex surrogates. In experiments, we show that the IRLS method exhibits favorable empirical convergence, identifying simultaneously row-sparse and low-rank matrices from fewer measurements than state-of-the-art methods.
    
[^85]: 低秩模型压缩的自适应假音频检测

    Adaptive Fake Audio Detection with Low-Rank Model Squeezing. (arXiv:2306.04956v1 [cs.SD])

    [http://arxiv.org/abs/2306.04956](http://arxiv.org/abs/2306.04956)

    本文提出了一种适应新出现假音频类型的低秩适应矩阵训练方法，有效地缓解了传统微调方法的局限性，并能保留现有模型对已知假音频类型的预测准确性。

    

    假音频技术的不断发展需要开发鲁棒性检测方法来准确识别新出现的假音频。本文提出了一种创新方法，通过训练适应于新出现的假音频类型的低秩适应矩阵来缓解传统的微调方法的限制。在推断阶段，这些适应矩阵与现有模型相结合生成最终的预测输出。

    The rapid advancement of spoofing algorithms necessitates the development of robust detection methods capable of accurately identifying emerging fake audio. Traditional approaches, such as finetuning on new datasets containing these novel spoofing algorithms, are computationally intensive and pose a risk of impairing the acquired knowledge of known fake audio types. To address these challenges, this paper proposes an innovative approach that mitigates the limitations associated with finetuning. We introduce the concept of training low-rank adaptation matrices tailored specifically to the newly emerging fake audio types. During the inference stage, these adaptation matrices are combined with the existing model to generate the final prediction output. Extensive experimentation is conducted to evaluate the efficacy of the proposed method. The results demonstrate that our approach effectively preserves the prediction accuracy of the existing model for known fake audio types. Furthermore, o
    
[^86]: 论神经网络对降解多边形的感知存在的基本问题

    Degraded Polygons Raise Fundamental Questions of Neural Network Perception. (arXiv:2306.04955v1 [cs.CV])

    [http://arxiv.org/abs/2306.04955](http://arxiv.org/abs/2306.04955)

    本文研究了神经网络在识别具有不同程度边缘降解的规则多边形时的性能和行为，发现存在基本问题，揭示了人机视觉差距的另一个角度。

    

    现代计算机视觉系统往往表现出与人类不一致的行为：从对抗攻击到图像损坏，深度学习视觉模型在各种环境中都表现不佳，然而人类却能够很好地解决这些问题。本文从另一个角度研究了人机视觉差距。我们重新审视了恢复受损图像的任务，该任务在人类视觉的“识别组件”理论中首次引入，研究了神经网络在分类具有不同程度边缘降解的规则多边形时的性能和行为。为此，我们使用了自动化形状可恢复性测试，快速生成了大规模数据集，将历史上手动创建图像可恢复性实验的方法进行了现代化改进。我们进一步研究了神经网络识别多边形的能力以及其相关问题。

    It is well-known that modern computer vision systems often exhibit behaviors misaligned with those of humans: from adversarial attacks to image corruptions, deep learning vision models suffer in a variety of settings that humans capably handle. In light of these phenomena, here we introduce another, orthogonal perspective studying the human-machine vision gap. We revisit the task of recovering images under degradation, first introduced over 30 years ago in the Recognition-by-Components theory of human vision. Specifically, we study the performance and behavior of neural networks on the seemingly simple task of classifying regular polygons at varying orders of degradation along their perimeters. To this end, we implement the Automated Shape Recoverability Test for rapidly generating large-scale datasets of perimeter-degraded regular polygons, modernizing the historically manual creation of image recoverability experiments. We then investigate the capacity of neural networks to recognize
    
[^87]: 基于熵的训练方法用于可扩展的神经隐式采样器

    Entropy-based Training Methods for Scalable Neural Implicit Sampler. (arXiv:2306.04952v1 [stat.ML])

    [http://arxiv.org/abs/2306.04952](http://arxiv.org/abs/2306.04952)

    本文提出了一种高效且可扩展的神经隐式采样器，并引入了KL训练法和Fisher训练法来训练它，实现了低计算成本下生成大批量样本。

    

    高效地从非标准目标分布中采样是科学计算和机器学习中的一个基本问题。传统方法如马尔科夫蒙特卡洛（MCMC）可保证从这些分布中渐进无偏采样，但在处理高维目标时计算效率低下，需要多次迭代生成一批样本。本文提出了一种高效且可扩展的神经隐式采样器，通过利用直接将易于采样的潜在向量映射到目标样本的神经变换，可以在低计算成本下生成大批量样本。为了训练神经隐式采样器，我们引入了两种新方法：KL训练法和Fisher训练法。前者最小化Kullback-Leibler散度，而后者则最小化Fisher散度。

    Efficiently sampling from un-normalized target distributions is a fundamental problem in scientific computing and machine learning. Traditional approaches like Markov Chain Monte Carlo (MCMC) guarantee asymptotically unbiased samples from such distributions but suffer from computational inefficiency, particularly when dealing with high-dimensional targets, as they require numerous iterations to generate a batch of samples. In this paper, we propose an efficient and scalable neural implicit sampler that overcomes these limitations. Our sampler can generate large batches of samples with low computational costs by leveraging a neural transformation that directly maps easily sampled latent vectors to target samples without the need for iterative procedures. To train the neural implicit sampler, we introduce two novel methods: the KL training method and the Fisher training method. The former minimizes the Kullback-Leibler divergence, while the latter minimizes the Fisher divergence. By empl
    
[^88]: 通过逐步扩展数据对抗虚假相关性的鲁棒学习

    Robust Learning with Progressive Data Expansion Against Spurious Correlation. (arXiv:2306.04949v1 [cs.LG])

    [http://arxiv.org/abs/2306.04949](http://arxiv.org/abs/2306.04949)

    本文通过理论分析和实验证明了，在存在虚假特征的情况下，数据组的不平衡和易于学习的虚假特征可能导致模型学习虚假特征。作者提出了一种新的训练算法PDE，它逐步扩展训练数据的大小可以提高了模型鲁棒性，有效地增强了其最劣组性能，实验证实在合成和真实世界的基准数据集上的超越了R模型等其它模型。

    

    虽然深度学习模型在各种任务中表现出了卓越的性能，但它们易于学习与真实标签无关的虚假特征，而不是真正与标签相关的核心特征。在本文中，我们在已有线性模型分析的基础上，从理论上检查了存在虚假特征时两层非线性卷积神经网络的学习过程。我们的分析表明，数据组不平衡和易于学习的虚假特征可能在学习过程中导致虚假特征的支配。基于此，我们提出了一种名为PDE的新的训练算法，该算法有效地增强了模型的鲁棒性，以获得更好的最差组性能。PDE从一组平衡的训练数据子集开始，并逐步扩展其大小以促进核心特征的学习。在合成和真实世界基准数据集上的实验证明了我们的方法在模型（例如R）上的优异性能。

    While deep learning models have shown remarkable performance in various tasks, they are susceptible to learning non-generalizable spurious features rather than the core features that are genuinely correlated to the true label. In this paper, beyond existing analyses of linear models, we theoretically examine the learning process of a two-layer nonlinear convolutional neural network in the presence of spurious features. Our analysis suggests that imbalanced data groups and easily learnable spurious features can lead to the dominance of spurious features during the learning process. In light of this, we propose a new training algorithm called PDE that efficiently enhances the model's robustness for a better worst-group performance. PDE begins with a group-balanced subset of training data and progressively expands it to facilitate the learning of the core features. Experiments on synthetic and real-world benchmark datasets confirm the superior performance of our method on models such as R
    
[^89]: ShuttleSet: 一份人工标注的羽毛球单打比赛的拍级数据集，用于战术分析。

    ShuttleSet: A Human-Annotated Stroke-Level Singles Dataset for Badminton Tactical Analysis. (arXiv:2306.04948v1 [cs.LG])

    [http://arxiv.org/abs/2306.04948](http://arxiv.org/abs/2306.04948)

    ShuttleSet是一份羽毛球单打比赛的拍级数据集，其中包含104场比赛、3,685轮比赛、36,492个拍击，并涵盖了27名排名前列的男子和女子单打选手。这些拍级记录将促进人工智能在体育分析领域的发展。

    

    随着体育分析的最新进展，深度学习方法已经展示出挖掘球员战术洞察力以提高表现质量和球迷参与度的有效性。这归因于公共基础真实数据集的可用性。虽然有一些用于行动检测的回合比赛的可用数据集，但这些数据集严重缺乏结构化的来源数据和拍级记录，因为这些需要来自领域专家的高成本标记工作，并且很难使用自动技术检测到。因此，当现有模型应用于更具挑战性的结构化回合序列时，人工智能方法的开发受到重大制约。在本文中，我们介绍了 ShuttleSet，这是最大的公开羽毛球单打比赛的拍级记录数据集。它包括2018年至2021年间的44场比赛中的104盘比赛，3,685轮比赛和36,492个拍击，并涵盖了27名排名前列的男子和女子单打选手。

    With the recent progress in sports analytics, deep learning approaches have demonstrated the effectiveness of mining insights into players' tactics for improving performance quality and fan engagement. This is attributed to the availability of public ground-truth datasets. While there are a few available datasets for turn-based sports for action detection, these datasets severely lack structured source data and stroke-level records since these require high-cost labeling efforts from domain experts and are hard to detect using automatic techniques. Consequently, the development of artificial intelligence approaches is significantly hindered when existing models are applied to more challenging structured turn-based sequences. In this paper, we present ShuttleSet, the largest publicly-available badminton singles dataset with annotated stroke-level records. It contains 104 sets, 3,685 rallies, and 36,492 strokes in 44 matches between 2018 and 2021 with 27 top-ranking men's singles and wome
    
[^90]: 一种改良的语料主题检测模型及评估主题可读性的新指标

    A modified model for topic detection from a corpus and a new metric evaluating the understandability of topics. (arXiv:2306.04941v1 [cs.CL])

    [http://arxiv.org/abs/2306.04941](http://arxiv.org/abs/2306.04941)

    本文提出一种改进的神经网络模型用于检测语料库中的主题，并提出了一种新的指标用于评估主题的可读性。

    

    本论文提出了一种改良的神经网络模型用于从语料库中检测主题，并且提出了一种新的指标用于评估所检测到的主题。新模型建立在嵌入式主题模型基础上，并加入了一些改进，如文档聚类等。实验表明，该模型在处理各种长度的文档时表现良好。新的指标可以更有效地计算主题可读性，提供了有关所检测到主题的可理解程度的不同信息。

    This paper presents a modified neural model for topic detection from a corpus and proposes a new metric to evaluate the detected topics. The new model builds upon the embedded topic model incorporating some modifications such as document clustering. Numerical experiments suggest that the new model performs favourably regardless of the document's length. The new metric, which can be computed more efficiently than widely-used metrics such as topic coherence, provides variable information regarding the understandability of the detected topics.
    
[^91]: 分层级别激活机制

    Layer-level activation mechanism. (arXiv:2306.04940v1 [cs.LG])

    [http://arxiv.org/abs/2306.04940](http://arxiv.org/abs/2306.04940)

    去噪声更好，表现更好的分层级别激活机制

    

    本文提出了一种新颖的激活机制，旨在建立分层级别激活功能（LayerAct）。这些功能旨在通过减少输入偏移所导致的激活输出的分层级波动来降低传统元素级激活功能的噪音鲁棒性。此外，LayerAct功能实现了类似于零的平均激活输出，而不限制激活输出空间。我们进行了分析和实验，证明LayerAct功能在噪声鲁棒性方面优于元素级激活功能，并且经验证明这些功能的平均激活结果类似于零。在三个基准图像分类任务的实验结果表明，在处理嘈杂的图像数据集时，LayerAct功能比元素级激活功能表现更好，而在大多数情况下，清洁数据集的表现也是优越的。

    In this work, we propose a novel activation mechanism aimed at establishing layer-level activation (LayerAct) functions. These functions are designed to be more noise-robust compared to traditional element-level activation functions by reducing the layer-level fluctuation of the activation outputs due to shift in inputs. Moreover, the LayerAct functions achieve a zero-like mean activation output without restricting the activation output space. We present an analysis and experiments demonstrating that LayerAct functions exhibit superior noise-robustness compared to element-level activation functions, and empirically show that these functions have a zero-like mean activation. Experimental results on three benchmark image classification tasks show that LayerAct functions excel in handling noisy image datasets, outperforming element-level activation functions, while the performance on clean datasets is also superior in most cases.
    
[^92]: InfoPrompt：用信息论软提示调整自然语言理解的方法

    InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding. (arXiv:2306.04933v1 [cs.CL])

    [http://arxiv.org/abs/2306.04933](http://arxiv.org/abs/2306.04933)

    本文提出了一种新的信息论框架，通过最大化提示和其他模型参数之间互信息来优化软提示调整，从而开发了更加高效、准确和稳健的软提示调整方法InfoPrompt。该方法通过两个新型损失函数，发现合适的提示初始化，并从提示令牌中学习足够的任务相关信息，同时鼓励预训练语言模型的输出表示更加关注任务相关信息。

    

    软提示调整在广泛的少样本任务中取得了卓越的性能。但是，提示调整的性能对初始化的提示非常敏感。本文发现，传统的提示调整方法不能从提示令牌中编码和学习足够的任务相关信息。我们开发了一种信息论框架，将软提示调整形式化为最大化提示和其他模型参数（或编码表示）之间互信息的优化问题。这种新颖的观点有助于我们开发一个更高效、准确和稳健的软提示调整方法InfoPrompt。在这个框架下，我们开发了两个基于互信息的新型损失函数，用于（i）发现下游任务的合适提示初始化，并从提示令牌中学习足够的任务相关信息，（ii）鼓励预训练语言模型的输出表示更加关注任务相关信息。

    Soft prompt tuning achieves superior performances across a wide range of few-shot tasks. However, the performances of prompt tuning can be highly sensitive to the initialization of the prompts. We also empirically observe that conventional prompt tuning methods cannot encode and learn sufficient task-relevant information from prompt tokens. In this work, we develop an information-theoretic framework that formulates soft prompt tuning as maximizing mutual information between prompts and other model parameters (or encoded representations). This novel view helps us to develop a more efficient, accurate and robust soft prompt tuning method InfoPrompt. With this framework, we develop two novel mutual information based loss functions, to (i) discover proper prompt initialization for the downstream tasks and learn sufficient task-relevant information from prompt tokens and (ii) encourage the output representation from the pretrained language model to be more aware of the task-relevant informa
    
[^93]: 何时展示建议？在AI辅助编程中整合人类反馈

    When to Show a Suggestion? Integrating Human Feedback in AI-Assisted Programming. (arXiv:2306.04930v1 [cs.HC])

    [http://arxiv.org/abs/2306.04930](http://arxiv.org/abs/2306.04930)

    本研究利用先前数据的干预措施提高基于AI的代码推荐系统的有效性，提出了一个CDHF框架来整合人类反馈，预测建议接受程度并决定何时展示哪些建议。

    

    基于AI的代码推荐系统，如Copilot和CodeWhisperer，提供程序员环境（例如IDE）内的代码建议，旨在提高他们的生产力。由于在这些场景中，程序员接受和拒绝建议，因此理想情况下，该系统应使用此反馈以促进这一目标。本研究利用程序员与Copilot交互的先前数据，开发可以节省程序员时间的干预措施。我们提出了一个实用理论框架，用于建模与程序员的交互，并决定何时展示哪些建议。我们的框架“基于人类反馈的条件建议展示”（CDHF）基于对程序员操作的预测模型。使用535名程序员的数据，我们构建了可以预测建议接受程度的模型。在对通过AI辅助编程解决的真实世界编程任务的回顾性评估中，我们发现CDHF能够实现有利的权衡。我们的发现表明，整合人类反馈可以显著提高基于AI的代码推荐系统的有效性。

    AI powered code-recommendation systems, such as Copilot and CodeWhisperer, provide code suggestions inside a programmer's environment (e.g., an IDE) with the aim to improve their productivity. Since, in these scenarios, programmers accept and reject suggestions, ideally, such a system should use this feedback in furtherance of this goal. In this work we leverage prior data of programmers interacting with Copilot to develop interventions that can save programmer time. We propose a utility theory framework, which models this interaction with programmers and decides when and which suggestions to display. Our framework Conditional suggestion Display from Human Feedback (CDHF) is based on predictive models of programmer actions. Using data from 535 programmers we build models that predict the likelihood of suggestion acceptance. In a retrospective evaluation on real-world programming tasks solved with AI-assisted programming, we find that CDHF can achieve favorable tradeoffs. Our findings s
    
[^94]: covLLM：用于COVID-19生物医学文献的大型语言模型

    covLLM: Large Language Models for COVID-19 Biomedical Literature. (arXiv:2306.04926v1 [cs.CL])

    [http://arxiv.org/abs/2306.04926](http://arxiv.org/abs/2306.04926)

    使用大型语言模型开发了一种名为covLLM的工具，用于协助临床医生评估COVID-19文献。covLLM可以汇总和提取相关信息，帮助医生更好地应对COVID-19疫情。

    

    虽然新冠病毒的研究在不断增加，但COVID-19大流行导致了美国110万人的死亡。这些新发现在转化为临床干预方案方面缓慢，导致患者预后较差和不必要的死亡。其中一种原因是临床医生因患者过多而难以跟上新冠病毒文献的速度。发展一个使用大型语言模型（LLM）评估冠状病毒文献的工具，即神经网络用于自然语言处理，可能是一个解决方案。LLMs可用于汇总和提取用户指定的信息。较大范围和先进的LLMs和预处理的冠状病毒文献数据库提供了通过冠状病毒文献特定LLM（covLLM）协助临床医生评估冠状病毒文献的机会，该工具直接输入研究文章和用户查询以返回答案。在使用COVID-19开放研究数据集（CORD-19）的过程中，我们开发和评估了covLLM，展示了它在总结和从冠状病毒文献中提取信息方面的实用性。

    The COVID-19 pandemic led to 1.1 million deaths in the United States, despite the explosion of coronavirus research. These new findings are slow to translate to clinical interventions, leading to poorer patient outcomes and unnecessary deaths. One reason is that clinicians, overwhelmed by patients, struggle to keep pace with the rate of new coronavirus literature. A potential solution is developing a tool for evaluating coronavirus literature using large language models (LLMs) -- neural networks that are deployed for natural language processing. LLMs can be used to summarize and extract user-specified information. The greater availability and advancement of LLMs and pre-processed coronavirus literature databases provide the opportunity to assist clinicians in evaluating coronavirus literature through a coronavirus literature specific LLM (covLLM), a tool that directly takes an inputted research article and a user query to return an answer. Using the COVID-19 Open Research Dataset (CORD
    
[^95]: 偏好分类：通过辅助偏好学习改进文本分类器

    Prefer to Classify: Improving Text Classifiers via Auxiliary Preference Learning. (arXiv:2306.04925v1 [cs.CL])

    [http://arxiv.org/abs/2306.04925](http://arxiv.org/abs/2306.04925)

    本文提出了一种新的文本分类方法，使用偏好分类学习辅助数据来提高模型准确性。通过比较输入文本对之间的偏好关系，这种方法能够为模型提供额外的训练信号。

    

    大量人工标注的基准数据集推动了深度神经网络在各种自然语言处理任务中的成功。为了增强现有基准数据集的效果，收集新的输入输出对通常过于昂贵和具有挑战性，特别是考虑到它们对提高当前模型精度的边际影响。相反，对基准数据集中现有输入文本的附加或补充标注可能是一种有效的额外人工成本支付方式。本文研究了任务特定的输入文本对之间的偏好关系作为这种辅助数据标注的新替代方式。从任务相关的“成对”比较中，辅助偏好学习使模型学习到一种额外的信息性训练信号，这种信号无法通过“实例级”的任务标签来捕捉。为此，我们提出了一种新的多任务学习框架，称为prefer-to-classify （P2C），它可以同时享受实例级和偏好级任务。

    The development of largely human-annotated benchmarks has driven the success of deep neural networks in various NLP tasks. To enhance the effectiveness of existing benchmarks, collecting new additional input-output pairs is often too costly and challenging, particularly considering their marginal impact on improving the current model accuracy. Instead, additional or complementary annotations on the existing input texts in the benchmarks can be preferable as an efficient way to pay the additional human cost. In this paper, we investigate task-specific preferences between pairs of input texts as a new alternative way for such auxiliary data annotation. From 'pair-wise' comparisons with respect to the task, the auxiliary preference learning enables the model to learn an additional informative training signal that cannot be captured with 'instance-wise' task labels. To this end, we propose a novel multi-task learning framework, called prefer-to-classify (P2C), which can enjoy the cooperati
    
[^96]: 分布式均值估计中的通信隐私效用权衡的精确最优性研究

    Exact Optimality of Communication-Privacy-Utility Tradeoffs in Distributed Mean Estimation. (arXiv:2306.04924v1 [cs.LG])

    [http://arxiv.org/abs/2306.04924](http://arxiv.org/abs/2306.04924)

    本研究针对均值估计问题，探讨了在通信和本地差分隐私约束下的精确最优方法，提出了利用旋转对称的共享随机码书，并通过$k$-closest编码实现了随机旋转的单纯形$c$的精确最优。

    

    本文研究了在通信和本地差分隐私约束下的均值估计问题。虽然以前的研究已经提出了相同问题的\emph{阶}-最优算法（即当我们花费更多比特时渐进最优），但在非渐进设置下仍然没有实现\emph{精确}最优性。在本文中，我们迈出了一步，描述了在共享随机性存在的情况下的\emph{精确}-最优方法，并确定了几个\emph{精确}最优的必要条件。我们证明了其中一个必要条件是利用旋转对称的共享随机码书。基于此，我们提出了一种随机化机制，其中码书是随机旋转的单纯形——满足\emph{精确}-最优码书的必要属性。该机制基于我们证明的$k$最近编码，对于随机旋转的单纯形$c$来说是\emph{精确}-最优的。

    We study the mean estimation problem under communication and local differential privacy constraints. While previous work has proposed \emph{order}-optimal algorithms for the same problem (i.e., asymptotically optimal as we spend more bits), \emph{exact} optimality (in the non-asymptotic setting) still has not been achieved. In this work, we take a step towards characterizing the \emph{exact}-optimal approach in the presence of shared randomness (a random variable shared between the server and the user) and identify several necessary conditions for \emph{exact} optimality. We prove that one of the necessary conditions is to utilize a rotationally symmetric shared random codebook. Based on this, we propose a randomization mechanism where the codebook is a randomly rotated simplex -- satisfying the necessary properties of the \emph{exact}-optimal codebook. The proposed mechanism is based on a $k$-closest encoding which we prove to be \emph{exact}-optimal for the randomly rotated simplex c
    
[^97]: 无约束在线学习和无界损失的算法

    Unconstrained Online Learning with Unbounded Losses. (arXiv:2306.04923v1 [cs.LG])

    [http://arxiv.org/abs/2306.04923](http://arxiv.org/abs/2306.04923)

    本论文提出了一种算法，可用于解决无界域和非Lipschitz损失的在线学习问题，并提供了一个遗憾的度量，以衡量该算法的性能。此外，我们还利用该算法开发了一种新的鞍点优化算法，即使在没有有意义的曲率的情况下，也能够在无界领域中收敛于对偶间隙。最后，我们提供了一种算法，在无界域和非Lipschitz损失的情况下实现了非平凡的动态遗憾，以及相匹配的下界。

    

    在线学习算法通常需要一个或多个有界性假设：即域是有界的，损失是Lipschitz的或两者都有。在本文中，我们为具有无界域和非Lipschitz损失的在线学习开发了一个新的设置。针对该场景，我们提供了一种算法，可以保证在任何满足子梯度满足$\|g_{t}\|\le G+L\|w_{t}\|$的问题中，其遗憾的度量值$R_{T}(u)\le \tilde O(G\|u\|\sqrt{T}+L\|u\|^{2}\sqrt{T})$，并且表明除非有进一步 假设，否则该界限是不能进一步改进的。

    Algorithms for online learning typically require one or more boundedness assumptions: that the domain is bounded, that the losses are Lipschitz, or both. In this paper, we develop a new setting for online learning with unbounded domains and non-Lipschitz losses. For this setting we provide an algorithm which guarantees $R_{T}(u)\le \tilde O(G\|u\|\sqrt{T}+L\|u\|^{2}\sqrt{T})$ regret on any problem where the subgradients satisfy $\|g_{t}\|\le G+L\|w_{t}\|$, and show that this bound is unimprovable without further assumptions. We leverage this algorithm to develop new saddle-point optimization algorithms that converge in duality gap in unbounded domains, even in the absence of meaningful curvature. Finally, we provide the first algorithm achieving non-trivial dynamic regret in an unbounded domain for non-Lipschitz losses, as well as a matching lower bound. The regret of our dynamic regret algorithm automatically improves to a novel $L^{*}$ bound when the losses are smooth.
    
[^98]: 高效和等变图网络预测量子哈密顿

    Efficient and Equivariant Graph Networks for Predicting Quantum Hamiltonian. (arXiv:2306.04922v1 [cs.LG])

    [http://arxiv.org/abs/2306.04922](http://arxiv.org/abs/2306.04922)

    本文提出了一种名为QHNet的SE(3)-等变网络，具有高效和等变性。与最先进的方法相比，QHNet在更快的速度下实现了与其可比的表现，并且消耗的内存少了50％。

    

    我们考虑用于量子化学和凝聚态物理中的哈密顿矩阵预测。效率和等变性是两个重要但冲突的因素。在本研究中，我们提出了一种SE(3)-等变网络，称为QHNet，既实现了效率又实现了等变性。我们的关键进展在于QHNet架构的创新设计，它不仅遵守基本的对称性，还可以通过92％的张量积数量来减少数量。此外，QHNet在涉及更多原子类型时，可以防止通道维度的指数增长。我们在MD17数据集上进行了实验，包括四个分子系统。实验结果表明，我们的QHNet可以在显著更快的速度下实现与最先进方法可比拟的性能。此外，由于其简化的架构，我们的QHNet消耗的内存少了50％。我们的代码可作为AIRS库的一部分公开使用（\url{https://github.com/divelab/AIRS}）。

    We consider the prediction of the Hamiltonian matrix, which finds use in quantum chemistry and condensed matter physics. Efficiency and equivariance are two important, but conflicting factors. In this work, we propose a SE(3)-equivariant network, named QHNet, that achieves efficiency and equivariance. Our key advance lies at the innovative design of QHNet architecture, which not only obeys the underlying symmetries, but also enables the reduction of number of tensor products by 92\%. In addition, QHNet prevents the exponential growth of channel dimension when more atom types are involved. We perform experiments on MD17 datasets, including four molecular systems. Experimental results show that our QHNet can achieve comparable performance to the state of the art methods at a significantly faster speed. Besides, our QHNet consumes 50\% less memory due to its streamlined architecture. Our code is publicly available as part of the AIRS library (\url{https://github.com/divelab/AIRS}).
    
[^99]: 一种基于深度贝叶斯粒子流框架的跨领域软测量无监督建模方法

    Unsupervised Cross-Domain Soft Sensor Modelling via A Deep Bayesian Particle Flow Framework. (arXiv:2306.04919v1 [cs.LG])

    [http://arxiv.org/abs/2306.04919](http://arxiv.org/abs/2306.04919)

    该论文提出了一种基于深度粒子流贝叶斯框架的跨领域软测量无监督建模方法，能够解决标签缺失、领域适应性和数据时间一致性等问题，提高软测量建模的准确性。

    

    数据驱动的软测量对于通过可靠的状态推断实现精确感知至关重要。然而，由于存在标签缺失、领域适应性和数据时间一致性等问题，开发具有代表性的软测量模型具有挑战性。为了解决这些问题，我们提出了一种基于深度粒子流贝叶斯 (DPFB) 框架，用于在无目标状态标签情况下进行跨领域软测量建模。具体来说，首先制定了一个顺序贝叶斯目标，以执行潜在的跨领域软感知问题的最大似然估计。在框架核心，我们结合物理学启发的粒子流，通过优化顺序贝叶斯目标来执行模型提取的潜在和隐藏特征的精确贝叶斯更新。由此，这些贡献使得该框架能够学习一个有机的近似后验特征表示，能够表征复杂的跨领域系统动力学并实现有效的软测量建模。

    Data-driven soft sensors are essential for achieving accurate perception through reliable state inference. However, developing representative soft sensor models is challenged by issues such as missing labels, domain adaptability, and temporal coherence in data. To address these challenges, we propose a deep Particle Flow Bayes (DPFB) framework for cross-domain soft sensor modeling in the absence of target state labels. In particular, a sequential Bayes objective is first formulated to perform the maximum likelihood estimation underlying the cross-domain soft sensing problem. At the core of the framework, we incorporate a physics-inspired particle flow that optimizes the sequential Bayes objective to perform an exact Bayes update of the model extracted latent and hidden features. As a result, these contributions enable the proposed framework to learn a cohesive approximate posterior feature representation capable of characterizing complex cross-domain system dynamics and performing effe
    
[^100]: 一种非马尔可夫算法的覆盖时间研究

    A Cover Time Study of a non-Markovian Algorithm. (arXiv:2306.04902v1 [cs.DS])

    [http://arxiv.org/abs/2306.04902](http://arxiv.org/abs/2306.04902)

    本文研究了一个遍历算法中的覆盖时间问题，通过一种基于计数的负反馈策略，实现在任意图中局部提高搜索效率，并在特殊的图形中实现更小的覆盖时间。

    

    在给定的图中，覆盖时间是访问所有节点所需的期望步数。更小的覆盖时间意味着遍历算法的探索效率更高。尽管对于随机游走算法已有大量研究，但对于任何非马尔可夫方法尚无覆盖时间结果。本研究从理论角度出发，表明负反馈策略（一种基于计数的探索方法）比朴素的随机漫步搜索策略更好。特别地，前者可以在任意图中局部提高搜索效率。它还可以在特殊但重要的图形中实现更小的覆盖时间，包括团簇图和树图等。此外，我们将结果与强化学习文献联系起来，以揭示为什么经典的UCB和MCTS算法如此有用。各种数值结果证实了我们的理论发现。

    Given a traversal algorithm, cover time is the expected number of steps needed to visit all nodes in a given graph. A smaller cover time means a higher exploration efficiency of traversal algorithm. Although random walk algorithms have been studied extensively in the existing literature, there has been no cover time result for any non-Markovian method. In this work, we stand on a theoretical perspective and show that the negative feedback strategy (a count-based exploration method) is better than the naive random walk search. In particular, the former strategy can locally improve the search efficiency for an arbitrary graph. It also achieves smaller cover times for special but important graphs, including clique graphs, tree graphs, etc. Moreover, we make connections between our results and reinforcement learning literature to give new insights on why classical UCB and MCTS algorithms are so useful. Various numerical results corroborate our theoretical findings.
    
[^101]: 转移学习的泛化性能：过参数化和欠参数化的情况

    Generalization Performance of Transfer Learning: Overparameterized and Underparameterized Regimes. (arXiv:2306.04901v1 [cs.LG])

    [http://arxiv.org/abs/2306.04901](http://arxiv.org/abs/2306.04901)

    该研究探讨了转移学习在不同相似度条件下的表现，使用线性回归模型分析了两种参数传递选项的比较，并通过理论分析模型误差的特征来检验泛化性能如何随着参数数量的变化而变化。

    

    转移学习是一种使用源任务中获得的知识并将其应用于目标任务，以实现改善性能和降低训练成本的有用技术。评估转移学习的有效性依赖于理解源任务和目标任务之间的相似性。在实际应用中，任务通常表现出部分相似性，其中某些方面相似而另一些方面则不同或无关。为了研究部分相似性对转移学习性能的影响，我们专注于具有两组不同特征的线性回归模型：在任务间共享的公共部分和任务特定的部分。我们的研究探索了各种类型的转移学习，涵盖了两个参数传递选项。通过建立学习模型误差的理论特征，我们比较这些转移学习选项，特别关注泛化性能如何随学习模型参数数量的变化而变化。

    Transfer learning is a useful technique for achieving improved performance and reducing training costs by leveraging the knowledge gained from source tasks and applying it to target tasks. Assessing the effectiveness of transfer learning relies on understanding the similarity between the ground truth of the source and target tasks. In real-world applications, tasks often exhibit partial similarity, where certain aspects are similar while others are different or irrelevant. To investigate the impact of partial similarity on transfer learning performance, we focus on a linear regression model with two distinct sets of features: a common part shared across tasks and a task-specific part. Our study explores various types of transfer learning, encompassing two options for parameter transfer. By establishing a theoretical characterization on the error of the learned model, we compare these transfer learning options, particularly examining how generalization performance changes with the numbe
    
[^102]: 通过分层潜变量模型理解遮蔽自编码器

    Understanding Masked Autoencoders via Hierarchical Latent Variable Models. (arXiv:2306.04898v1 [cs.LG])

    [http://arxiv.org/abs/2306.04898](http://arxiv.org/abs/2306.04898)

    本文正式刻画和证明了现有的经验性见解，并为MAE的训练和超参数调整提供了指导，以实现更好的表示学习。

    

    遮蔽自编码器（MAE）是一种基于遮蔽图像区域重构的简单有效的自监督学习框架，近来在各种视觉任务中取得了显著的成功。尽管MAE存在有趣的经验性观察结果，但仍然缺乏理论上的原则性理解。在本文中，我们正式刻画和证明了现有的经验性见解，并对MAE提供了理论保证。我们将底层数据生成过程建模为一个分层潜变量模型，并表明在合理的假设下，MAE能够证明地识别出一组潜在变量，解释了MAE如何从像素中提取高层信息。此外，我们展示了MAE中的关键超参数（遮蔽比率和补丁大小）如何决定要恢复哪些真实潜变量，从而影响表示中的语义信息水平。具体而言，极大或极小的遮蔽比率会导致表示质量差，而不同的补丁大小可以捕获不同级别的对象信息。我们的理论结果为MAE的训练和超参数调整提供了指导，以实现更好的表示学习。

    Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical insights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small maski
    
[^103]: 使用带有全梯度惩罚的条件生成式对抗网络解决基于物理反问题。

    Solution of physics-based inverse problems using conditional generative adversarial networks with full gradient penalty. (arXiv:2306.04895v1 [stat.ML])

    [http://arxiv.org/abs/2306.04895](http://arxiv.org/abs/2306.04895)

    本研究提出了一种新型的基于深度学习的方法，利用样本和物理学的正向模型生成训练数据，得到条件Wasserstein生成式对抗网络（cWGAN）的概率分布，可用于解决困难的概率反问题。

    

    解决概率反问题是一项具有挑战性的任务，特别是当推断向量的维度很大且其先验信息是由一组样本组成时。本研究提出了一种基于深度学习的新方法，应用于解决这类问题。该方法利用从先验分布中绘制的推断向量样本和基于物理的正向模型来生成条件Wasserstein生成式对抗网络（cWGAN）的训练数据。cWGAN学习给定测量的推断向量的概率分布，并生成来自该分布的样本。本文所开发的cWGAN与早期版本不同之处在于其评论家必须在推断向量和测量向量方面都是1-Lipschitz。这导致了一个带有全梯度惩罚的损失项。

    The solution of probabilistic inverse problems for which the corresponding forward problem is constrained by physical principles is challenging. This is especially true if the dimension of the inferred vector is large and the prior information about it is in the form of a collection of samples. In this work, a novel deep learning based approach is developed and applied to solving these types of problems. The approach utilizes samples of the inferred vector drawn from the prior distribution and a physics-based forward model to generate training data for a conditional Wasserstein generative adversarial network (cWGAN). The cWGAN learns the probability distribution for the inferred vector conditioned on the measurement and produces samples from this distribution. The cWGAN developed in this work differs from earlier versions in that its critic is required to be 1-Lipschitz with respect to both the inferred and the measurement vectors and not just the former. This leads to a loss term with
    
[^104]: 一种用于从数据学习控制偏微分方程的贝叶斯框架

    A Bayesian Framework for learning governing Partial Differential Equation from Data. (arXiv:2306.04894v1 [stat.ML])

    [http://arxiv.org/abs/2306.04894](http://arxiv.org/abs/2306.04894)

    本文介绍了一种采用变分贝叶斯和稀疏线性回归相结合的方法，用于从预定义的基函数字典中学习相关基本知识，并发现偏微分方程。

    

    发现偏微分方程（PDE）是一项既涉及理论又涉及实证方法的挑战性任务。虽然机器学习方法已经被用来解决此问题，但是现有方法常常在存在噪声的情况下难以准确地识别潜在的方程式。在本研究中，我们提出了一种结合变分贝叶斯和稀疏线性回归的发现PDE的新方法。PDE发现问题被视为从预定义基函数字典中学习相关基础的问题。为了加速整个过程，提出了一种基于变分贝叶斯的方法来发现偏微分方程。为了确保稀疏性，我们采用了针尖和板条先验。我们在包括Burgers、Korteweg-de Vries、Kuramoto Sivashinsky、波动方程和热方程（1D和2D）在内的几个示例中展示了我们策略的功效。我们的方法提供了一种有希望的途径。

    The discovery of partial differential equations (PDEs) is a challenging task that involves both theoretical and empirical methods. Machine learning approaches have been developed and used to solve this problem; however, it is important to note that existing methods often struggle to identify the underlying equation accurately in the presence of noise. In this study, we present a new approach to discovering PDEs by combining variational Bayes and sparse linear regression. The problem of PDE discovery has been posed as a problem to learn relevant basis from a predefined dictionary of basis functions. To accelerate the overall process, a variational Bayes-based approach for discovering partial differential equations is proposed. To ensure sparsity, we employ a spike and slab prior. We illustrate the efficacy of our strategy in several examples, including Burgers, Korteweg-de Vries, Kuramoto Sivashinsky, wave equation, and heat equation (1D as well as 2D). Our method offers a promising ave
    
[^105]: 基于贝叶斯原理的上下文学习

    In-Context Learning through the Bayesian Prism. (arXiv:2306.04891v1 [cs.LG])

    [http://arxiv.org/abs/2306.04891](http://arxiv.org/abs/2306.04891)

    这篇论文研究了大型语言模型中的上下文学习现象，并通过实验证据展示了Transformer模型在多种设置下表现出贝叶斯预测器的行为。作者还探讨了上下文学习与贝叶斯学习框架之间的联系，并提出了一个线性回归任务来验证这种联系。

    

    上下文学习是大型语言模型中令人惊讶且有用的特性之一。它的工作原理是一个活跃的研究领域。近期，人们设计了一些风格化的类元学习的设置，它们使用语言建模损失函数对来自函数类的输入输出对$(x, f(x))$ 进行训练，并观察模型对同一类中未见过的函数的泛化能力。这一研究线路中的一个主要发现是，对于诸如线性回归等几个问题，训练好的 Transformer 学习了上下文学习算法。然而，导致这种行为的归纳偏差并不清楚。拥有无限的训练数据和计算能力的模型是贝叶斯预测器：它学习了预训练分布。已经证明，高容量的 Transformer 模型在线性回归任务上模拟贝叶斯预测器的行为。在本文中，我们展示了Transformer在多种设置下表现出理想学习者的行为的经验证据，包括外推和求解微分方程。我们探讨了上下文学习和贝叶斯学习框架之间的联系，认为这些模型学习了合理函数的先验概率，而不仅仅是最小化语言建模损失。最后，我们提出了一个简单的线性回归任务来进一步探究这种联系，证明使用真实的贝叶斯先验进行训练的模型比使用固定先验或没有先验训练的模型表现更好。

    In-context learning is one of the surprising and useful features of large language models. How it works is an active area of research. Recently, stylized meta-learning-like setups have been devised that train these models on a sequence of input-output pairs $(x, f(x))$ from a function class using the language modeling loss and observe generalization to unseen functions from the same class. One of the main discoveries in this line of research has been that for several problems such as linear regression, trained transformers learn algorithms for learning functions in context. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. It has been shown that high-capacity transformers mimic the Bayesian predictor for linear regression. In this paper, we show empirical evidence of transformers exhibiting the behavior of this ideal learne
    
[^106]: ShaDDR: 基于示例的实时几何和纹理生成

    ShaDDR: Real-Time Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering. (arXiv:2306.04889v1 [cs.CV])

    [http://arxiv.org/abs/2306.04889](http://arxiv.org/abs/2306.04889)

    本文介绍了一种基于示例的深度生成神经网络 ShaDDR，可以通过几何细节化和条件纹理生成应用于输入的粗略体素形状，生成高分辨率贴图的 3D 形状。生成实时且精度高，风格可以通过学习的潜在代码进行控制。

    

    本文介绍了 ShaDDR，一种基于示例的深度生成神经网络，通过几何细节化和条件纹理生成应用于输入的粗略体素形状，生成高分辨率贴图的 3D 形状。在少量详细和纹理的范例形状上训练，我们的方法通过多分辨率体素上采样学习几何细节化，并且通过与几个视图的范例纹理图像进行可微渲染，在体素表面生成纹理。生成是实时的，仅需不到 1 秒即可生成分辨率高达 512^3 的 3D 模型。生成的形状保留了输入粗略体素模型的整体结构，而生成的几何细节和纹理的风格可以通过学习的潜在代码进行操纵。在实验中，我们展示了我们的方法可以生成比以前的作品更真实且几何细节和纹理更干净的高分辨率形状。

    We present ShaDDR, an example-based deep generative neural network which produces a high-resolution textured 3D shape through geometry detailization and conditional texture generation applied to an input coarse voxel shape. Trained on a small set of detailed and textured exemplar shapes, our method learns to detailize the geometry via multi-resolution voxel upsampling and generate textures on voxel surfaces via differentiable rendering against exemplar texture images from a few views. The generation is real-time, taking less than 1 second to produce a 3D model with voxel resolutions up to 512^3. The generated shape preserves the overall structure of the input coarse voxel model, while the style of the generated geometric details and textures can be manipulated through learned latent codes. In the experiments, we show that our method can generate higher-resolution shapes with plausible and improved geometric details and clean textures compared to prior works. Furthermore, we showcase th
    
[^107]: 多任务生物测定预训练用于蛋白质-配体结合亲和力预测

    Multi-task Bioassay Pre-training for Protein-ligand Binding Affinity Prediction. (arXiv:2306.04886v1 [q-bio.BM])

    [http://arxiv.org/abs/2306.04886](http://arxiv.org/abs/2306.04886)

    本文提出一种多任务生物测定预训练框架（MBP）用于基于结构的蛋白质-配体结合亲和力预测，通过构建一个预训练数据集ChEMBL-Dock，解决了高质量训练数据稀缺以及不同标签和实验条件导致的噪声问题。

    

    蛋白质-配体结合亲和力（PLBA）预测是药物发现中的基本任务。最近，各种基于深度学习的模型通过将蛋白质-配体复合物的三维结构作为输入并取得了惊人的进展来预测结合亲和力。然而，由于高质量训练数据的稀缺性，当前模型的泛化能力仍然有限。此外，不同的生物测定使用不同的亲和力测量标签（即IC50，Ki，Kd），不同的实验条件不可避免地会引入系统噪声，这对构建高精度亲和力预测模型构成了重大挑战。为了解决这些问题，我们（1）提出了多任务生物测定预训练（MBP），一种用于基于结构的PLBA预测的预训练框架;（2）构建了一个包含超过300k个实验测定亲和力标签和约2.8M个对接三维结构的预训练数据集，称为ChEMBL-Dock。

    Protein-ligand binding affinity (PLBA) prediction is the fundamental task in drug discovery. Recently, various deep learning-based models predict binding affinity by incorporating the three-dimensional structure of protein-ligand complexes as input and achieving astounding progress. However, due to the scarcity of high-quality training data, the generalization ability of current models is still limited. In addition, different bioassays use varying affinity measurement labels (i.e., IC50, Ki, Kd), and different experimental conditions inevitably introduce systematic noise, which poses a significant challenge to constructing high-precision affinity prediction models. To address these issues, we (1) propose Multi-task Bioassay Pre-training (MBP), a pre-training framework for structure-based PLBA prediction; (2) construct a pre-training dataset called ChEMBL-Dock with more than 300k experimentally measured affinity labels and about 2.8M docked three-dimensional structures. By introducing m
    
[^108]: 参数化图聚类和边标记的更快逼近算法

    Faster Approximation Algorithms for Parameterized Graph Clustering and Edge Labeling. (arXiv:2306.04884v1 [cs.DS])

    [http://arxiv.org/abs/2306.04884](http://arxiv.org/abs/2306.04884)

    本文提出了针对参数化聚类LambdaCC的更快逼近算法，同时引入了一个概括之前边标记问题的新的参数化边标记问题，该方法可扩展性比以前的算法高几个数量级。

    

    图聚类是网络分析中的基本任务之一，其目标是检测与彼此连接紧密、但与其余图稀疏连接的节点集。本文提出了一种更快的逼近算法，应用于一个名为LambdaCC的NP困难参数化聚类框架，该框架由可调节的分辨率参数控制，并且概括了许多其他聚类目标，例如模块化、最稀疏割和聚类删除。以前的LambdaCC算法要么是没有逼近保证的启发式算法，要么是计算复杂的逼近算法。我们提出了可以纯粹利用组合方法实现的快速新逼近算法。这些算法基于我们介绍的新参数化边标记问题，这个问题概括了之前基于强三元闭合原理的边标记问题，并在社交网络分析中有着独立的重要性。我们的方法比以前的逼近算法可扩展性高几个数量级。

    Graph clustering is a fundamental task in network analysis where the goal is to detect sets of nodes that are well-connected to each other but sparsely connected to the rest of the graph. We present faster approximation algorithms for an NP-hard parameterized clustering framework called LambdaCC, which is governed by a tunable resolution parameter and generalizes many other clustering objectives such as modularity, sparsest cut, and cluster deletion. Previous LambdaCC algorithms are either heuristics with no approximation guarantees, or computationally expensive approximation algorithms. We provide fast new approximation algorithms that can be made purely combinatorial. These rely on a new parameterized edge labeling problem we introduce that generalizes previous edge labeling problems that are based on the principle of strong triadic closure and are of independent interest in social network analysis. Our methods are orders of magnitude more scalable than previous approximation algorit
    
[^109]: 增强具有层间依赖性的Hessians用于混合精度后训练量化

    Augmenting Hessians with Inter-Layer Dependencies for Mixed-Precision Post-Training Quantization. (arXiv:2306.04879v1 [cs.LG])

    [http://arxiv.org/abs/2306.04879](http://arxiv.org/abs/2306.04879)

    本文提出一种混合精度后训练量化方法，分配不同的数值精度以减少内存占用和改善延迟，同时通过增强Hessian与其他层之间的依赖关系来解决传统方法不足以确定层敏感性排序的问题。实验证明该方法优于现有的混合精度后训练量化方法。

    

    随着模型复杂性和参数数量的增加，有效地为低延迟服务神经网络模型变得更加具有挑战性。模型量化提供了一种解决方案，可以同时减少内存占用和计算需求。然而，过度的量化可能会导致由于模型不同层之间对数字缺陷的敏感性差异而导致模型准确度的不可接受损失。为了解决这个问题，我们提出了一种混合精度后训练量化（PTQ）方法，根据网络中张量的特定需求为它们分配不同的数值精度，从而减少内存占用和改善延迟，同时保持模型准确度。以前的工作依赖于层次Hessian信息来确定数值精度，但正如我们所证明的那样，Hessian估计通常不足以确定层敏感性的有效排序。我们通过增强估计的Hessian与其他层之间的依赖关系来解决这个问题。我们的实验显示，我们的方法在一系列神经网络架构的混合精度PTQ方面优于先前的最先进方法。

    Efficiently serving neural network models with low latency is becoming more challenging due to increasing model complexity and parameter count. Model quantization offers a solution which simultaneously reduces memory footprint and compute requirements. However, aggressive quantization may lead to an unacceptable loss in model accuracy owing to differences in sensitivity to numerical imperfection across different layers in the model. To address this challenge, we propose a mixed-precision post training quantization (PTQ) approach that assigns different numerical precisions to tensors in a network based on their specific needs, for a reduced memory footprint and improved latency while preserving model accuracy. Previous works rely on layer-wise Hessian information to determine numerical precision, but as we demonstrate, Hessian estimation is typically insufficient in determining an effective ordering of layer sensitivities. We address this by augmenting the estimated Hessian with additio
    
[^110]: 使用激活优化进行特洛伊模型检测

    Trojan Model Detection Using Activation Optimization. (arXiv:2306.04877v1 [cs.CV])

    [http://arxiv.org/abs/2306.04877](http://arxiv.org/abs/2306.04877)

    本文提出了一种新颖的特洛伊模型检测方法，通过激活优化为模型创建签名，然后训练分类器来检测特洛伊模型。该方法在两个公共数据集上实现了最先进的性能。

    

    由于数据的不可用性或大规模，以及训练机器学习模型的高计算和人力成本，通常会在可能的情况下依赖于开源预训练模型。但是，从安全的角度来看，这种做法非常令人担忧。预训练模型可能会被感染特洛伊攻击，在这种攻击中，攻击者嵌入一个触发器在模型中，使得当触发器存在于输入中时，攻击者可以控制模型的行为。本文提出了一种新颖的特洛伊模型检测方法的初步工作。我们的方法根据激活优化为模型创建签名。然后训练分类器来检测特洛伊模型并给出其签名。我们的方法在两个公共数据集上实现了最先进的性能。

    Due to data's unavailability or large size, and the high computational and human labor costs of training machine learning models, it is a common practice to rely on open source pre-trained models whenever possible. However, this practice is worry some from the security perspective. Pre-trained models can be infected with Trojan attacks, in which the attacker embeds a trigger in the model such that the model's behavior can be controlled by the attacker when the trigger is present in the input. In this paper, we present our preliminary work on a novel method for Trojan model detection. Our method creates a signature for a model based on activation optimization. A classifier is then trained to detect a Trojan model given its signature. Our method achieves state of the art performance on two public datasets.
    
[^111]: 指导扩散器结合时间条件的离线强化学习

    Instructed Diffuser with Temporal Condition Guidance for Offline Reinforcement Learning. (arXiv:2306.04875v1 [cs.LG])

    [http://arxiv.org/abs/2306.04875](http://arxiv.org/abs/2306.04875)

    本文提出一种有效的时间条件扩散模型 TCD，通过提取序列数据中的时间信息并将其用于生成，实现了更好的控制生成效果。

    

    最近的研究表明了扩散模型在计算机视觉和自然语言处理领域的潜力。除了传统的监督学习领域，扩散模型还通过将决策制定为序列生成的方式，在强化学习中表现出强大的竞争力。然而，将顺序数据的时间信息纳入扩散模型，并利用它来指导更好的生成仍然是一个开放性的挑战。本文提出了一个有效的时间条件扩散模型，称为 Temporally-Composable Diffuser (TCD)，它从交互中提取时间信息，通过细化时间条件进行控制生成，并比较了不同时间条件的全面讨论。

    Recent works have shown the potential of diffusion models in computer vision and natural language processing. Apart from the classical supervised learning fields, diffusion models have also shown strong competitiveness in reinforcement learning (RL) by formulating decision-making as sequential generation. However, incorporating temporal information of sequential data and utilizing it to guide diffusion models to perform better generation is still an open challenge. In this paper, we take one step forward to investigate controllable generation with temporal conditions that are refined from temporal information. We observe the importance of temporal conditions in sequential generation in sufficient explorative scenarios and provide a comprehensive discussion and comparison of different temporal conditions. Based on the observations, we propose an effective temporally-conditional diffusion model coined Temporally-Composable Diffuser (TCD), which extracts temporal information from interact
    
[^112]: 扩大范围：将英文对抗攻击方法适应到中文上的研究

    Expanding Scope: Adapting English Adversarial Attacks to Chinese. (arXiv:2306.04874v1 [cs.CL])

    [http://arxiv.org/abs/2306.04874](http://arxiv.org/abs/2306.04874)

    本文研究将英文的对抗攻击方法适用于中文上，并证明了这些方法可以生成高质量的中文对抗实例。通过关注中文的语言特点，生成的对抗实例可以实现高流畅度和语义一致性，从而可以用来提高中文NLP模型的对抗鲁棒性。

    

    最近的研究表明，自然语言处理(NLP)的预测模型容易受到对抗攻击。多数现有的研究着眼于设计攻击方式来评估英语语境下的NLP模型的鲁棒性。然而学术界对其它语言的NLP解决方案需求日益增长。因此，我们自然产生一个问题：当前最先进的对抗攻击方法是否能够泛化到其它语言中？本文研究了如何将在英文环境下的最先进的对抗攻击算法适应到中文上。我们的实验表明，当结合正确的文本分割和语言限制时，先前针对英文NLP的攻击方法也能够在中文中生成高质量的对抗性例子。此外，我们还证明了，通过关注中文的形态和音系，生成的对抗实例可以实现高流畅度和语义一致性，从而可以用来提高中文NLP模型的对抗鲁棒性。

    Recent studies have revealed that NLP predictive models are vulnerable to adversarial attacks. Most existing studies focused on designing attacks to evaluate the robustness of NLP models in the English language alone. Literature has seen an increasing need for NLP solutions for other languages. We, therefore, ask one natural question: whether state-of-the-art (SOTA) attack methods generalize to other languages. This paper investigates how to adapt SOTA adversarial attack algorithms in English to the Chinese language. Our experiments show that attack methods previously applied to English NLP can generate high-quality adversarial examples in Chinese when combined with proper text segmentation and linguistic constraints. In addition, we demonstrate that the generated adversarial examples can achieve high fluency and semantic consistency by focusing on the Chinese language's morphology and phonology, which in turn can be used to improve the adversarial robustness of Chinese NLP models.
    
[^113]: 基于图去噪扩散的城市全域起点与终点矩阵生成

    City-wide Origin-Destination Matrix Generation via Graph Denoising Diffusion. (arXiv:2306.04873v1 [cs.LG])

    [http://arxiv.org/abs/2306.04873](http://arxiv.org/abs/2306.04873)

    本文提出了一种基于图去噪扩散的方法，从网络的角度生成城市全域的起点与终点矩阵，并通过学习区域层面的城市特征来设计出图去噪扩散方法的条件概率分布。

    

    起点终点矩阵估计了区域之间的出行人数，即城市中的人流量，广泛应用于城市规划、交通等领域。本文提出了一种基于图去噪扩散的方法，从网络的角度生成城市全域的起点与终点矩阵，并通过学习区域层面的城市特征来设计出图去噪扩散方法的条件概率分布。为了克服涵盖数千个区域的城市全域起点和终点矩阵的学习难度，我们将其分解成了若干小块、独立的部分。

    The Origin-Destination~(OD) matrix provides an estimation of number of individuals traveling between regions, i.e., mobility flow in the city, which is widely-used in urban planning, transportation, etc. Given various city characteristics of urban regions, generating the city-wide OD matrix without using historical flow information has become increasingly appealing to both researchers and practitioners. However, existing works are limited in independent generation of each element, i.e., flow, in OD matrix, overlooking the element relations within the matrix that can be well formulated as a network. In this paper, we instead propose to generate the city-wide OD matrix from the network perspective, and design a graph denoising diffusion method to learn the conditional joint probability distribution of all elements in the OD matrix given city characteristics at region level. To overcome the learning difficulty of the city-wide OD matrix covering over thousands of regions, we decompose the
    
[^114]: “联邦学习中客户端选择的系统性文献综述”

    A Systematic Literature Review on Client Selection in Federated Learning. (arXiv:2306.04862v1 [cs.LG])

    [http://arxiv.org/abs/2306.04862](http://arxiv.org/abs/2306.04862)

    联邦学习中客户端选择的主要挑战是异质性、资源分配、通信成本和公平性。客户选择方案旨在解决这些挑战，最常用的指标是测试准确性与通信轮次。

    

    由于对机器学习中隐私的担忧，联邦学习（FL）在2017年被发明，其中客户端（如移动设备）训练模型并将更新发送到集中式服务器。随机选择客户端进行FL可能会对学习性能造成伤害，因为原因各异。许多研究提出了解决FL客户端选择挑战的方法。然而，这一主题的系统性文献综述（SLR）不存在。本文的SLR调查了联邦学习中客户端选择的现状，并回答了挑战、解决方案和评估解决方案时使用的指标。我们对47篇主要研究进行了系统综述。在客户端选择方面，主要挑战是异质性、资源分配、通信成本和公平性。客户选择方案旨在通过专注于上述一项或几项挑战来改进原始的随机选择算法。最常用的指标是测试准确性与通信轮次。

    With the arising concerns of privacy within machine learning, federated learning (FL) was invented in 2017, in which the clients, such as mobile devices, train a model and send the update to the centralized server. Choosing clients randomly for FL can harm learning performance due to different reasons. Many studies have proposed approaches to address the challenges of client selection of FL. However, no systematic literature review (SLR) on this topic existed. This SLR investigates the state of the art of client selection in FL and answers the challenges, solutions, and metrics to evaluate the solutions. We systematically reviewed 47 primary studies. The main challenges found in client selection are heterogeneity, resource allocation, communication costs, and fairness. The client selection schemes aim to improve the original random selection algorithm by focusing on one or several of the aforementioned challenges. The most common metric used is testing accuracy versus communication rou
    
[^115]: 基于岛屿的随机动态电压调节与ML增强型功率侧信道攻击

    Island-based Random Dynamic Voltage Scaling vs ML-Enhanced Power Side-Channel Attacks. (arXiv:2306.04859v1 [cs.CR])

    [http://arxiv.org/abs/2306.04859](http://arxiv.org/abs/2306.04859)

    本文介绍了一种基于岛屿的随机动态电压调节（iRDVS）方法，用于防范功率侧信道攻击，并通过实验验证了其有效性。

    

    本文描述和分析了一种基于岛屿的随机动态电压调节（iRDVS）方法，用于防范功率侧信道攻击。我们首先分析了独立电压岛的数量对信噪比和轨迹错位的影响。作为我们对错位的分析的一部分，我们提出了一种新颖的基于无监督机器学习（ML）的攻击，对于具有三个或更少独立电压的系统很有效。我们的结果表明，带有四个电压岛的iRDVS在200k加密跟踪下无法被破解，说明iRDVS可以有效。我们最后通过描述一个12纳米FinFet工艺下的iRDVS测试芯片来结束讲话，其中包括三个变体的AES-256加速器，所有这些加速器均源自同一RTL。这包括一个同步核心，一个没有保护的异步核心，以及一个使用异步逻辑采用iRDVS技术的核心。芯片的实验室测量表明，两个未受保护的变体都失败了。

    In this paper, we describe and analyze an island-based random dynamic voltage scaling (iRDVS) approach to thwart power side-channel attacks. We first analyze the impact of the number of independent voltage islands on the resulting signal-to-noise ratio and trace misalignment. As part of our analysis of misalignment, we propose a novel unsupervised machine learning (ML) based attack that is effective on systems with three or fewer independent voltages. Our results show that iRDVS with four voltage islands, however, cannot be broken with 200k encryption traces, suggesting that iRDVS can be effective. We finish the talk by describing an iRDVS test chip in a 12nm FinFet process that incorporates three variants of an AES-256 accelerator, all originating from the same RTL. This included a synchronous core, an asynchronous core with no protection, and a core employing the iRDVS technique using asynchronous logic. Lab measurements from the chips indicated that both unprotected variants failed 
    
[^116]: 利用欧几里得距离函数解释和改进扩散模型

    Interpreting and Improving Diffusion Models Using the Euclidean Distance Function. (arXiv:2306.04848v1 [cs.LG])

    [http://arxiv.org/abs/2306.04848](http://arxiv.org/abs/2306.04848)

    本文利用欧几里得距离函数解释去噪扩散模型，并提出了一种新的采样器。采样器表现出了最先进的FID得分，并能够生成高质量的样本。

    

    去噪直觉上与投影有关。事实上，在流形假设下，添加随机噪声近似等价于正交扰动。因此，学习去噪近似于学习投影。本文利用这一观察结果，将去噪扩散模型解释为应用于欧几里得距离函数的近似梯度下降。随后，我们基于对去噪器投影误差的简单假设，提供DDIM（Denoising Diffusion Implicit Models）采样器的简单收敛分析。最后，我们基于理论结果的洞见提出一种基于对DDIM的两个简单修改的新采样器。仅需要5-10个函数评估，我们的采样器就能在预训练的CIFAR-10和CelebA模型上达到最先进的FID得分，并且可以在潜在扩散模型上生成高质量的样本。

    Denoising is intuitively related to projection. Indeed, under the manifold hypothesis, adding random noise is approximately equivalent to orthogonal perturbation. Hence, learning to denoise is approximately learning to project. In this paper, we use this observation to reinterpret denoising diffusion models as approximate gradient descent applied to the Euclidean distance function. We then provide straight-forward convergence analysis of the DDIM sampler under simple assumptions on the projection-error of the denoiser. Finally, we propose a new sampler based on two simple modifications to DDIM using insights from our theoretical results. In as few as 5-10 function evaluations, our sampler achieves state-of-the-art FID scores on pretrained CIFAR-10 and CelebA models and can generate high quality samples on latent diffusion models.
    
[^117]: 通过双重过程将随机微分方程嵌入到神经网络中

    Embedding stochastic differential equations into neural networks via dual processes. (arXiv:2306.04847v1 [cs.LG])

    [http://arxiv.org/abs/2306.04847](http://arxiv.org/abs/2306.04847)

    该论文提出了一种新方法，通过将随机微分方程的信息直接与神经网络的权重进行比较，构建用于预测随机微分方程期望的神经网络。这种方法避免了过度拟合问题，并在原点附近的输入有着准确度。

    

    我们提出了一种新方法来构建用于预测随机微分方程期望的神经网络。该方法不需要输入和输出数据集；相反，从时间演化方程获得的信息，即相应的双重过程，直接与神经网络中的权重进行比较。作为演示，我们构建了用于Ornstein-Uhlenbeck过程和噪声van der Pol系统的神经网络。使用该方法学习的网络的显着特征是在原点附近的输入的准确度。因此，可以避免过度拟合问题，因为学习的网络不依赖于训练数据集。

    We propose a new approach to constructing a neural network for predicting expectations of stochastic differential equations. The proposed method does not need data sets of inputs and outputs; instead, the information obtained from the time-evolution equations, i.e., the corresponding dual process, is directly compared with the weights in the neural network. As a demonstration, we construct neural networks for the Ornstein-Uhlenbeck process and the noisy van der Pol system. The remarkable feature of learned networks with the proposed method is the accuracy of inputs near the origin. Hence, it would be possible to avoid the overfitting problem because the learned network does not depend on training data sets.
    
[^118]: 量子学习的经典验证

    Classical Verification of Quantum Learning. (arXiv:2306.04843v1 [quant-ph])

    [http://arxiv.org/abs/2306.04843](http://arxiv.org/abs/2306.04843)

    本文提出了一个经典验证量子学习的框架，以便经典客户委托学习给不可信的量子服务器，并成功解决了对仗学习奇偶性和傅里叶稀疏函数不可信量子证明的问题。

    

    量子数据访问和量子处理可以使某些经典难以处理的学习任务变得可行。然而，量子能力在不久的将来只能提供给少数人使用。因此，需要可靠的方案，允许经典客户委托学习给不可信的量子服务器，以促进广泛获得量子学习的优势。本文基于最近引入的古典机器学习交互证明系统框架，构建了一个经典验证量子学习的框架。我们展示了一些经典学习者无法自行高效求解的学习问题，但当与不可信的量子证明者交互时，他们可以高效且可靠地解决这些问题。具体而言，我们考虑了关于具有均匀输入边缘密度的对仗学习奇偶性和傅里叶稀疏函数的问题。我们提出了一个新的量子数据访问模型，称为“叠加混合量子样例”。

    Quantum data access and quantum processing can make certain classically intractable learning tasks feasible. However, quantum capabilities will only be available to a select few in the near future. Thus, reliable schemes that allow classical clients to delegate learning to untrusted quantum servers are required to facilitate widespread access to quantum learning advantages. Building on a recently introduced framework of interactive proof systems for classical machine learning, we develop a framework for classical verification of quantum learning. We exhibit learning problems that a classical learner cannot efficiently solve on their own, but that they can efficiently and reliably solve when interacting with an untrusted quantum prover. Concretely, we consider the problems of agnostic learning parities and Fourier-sparse functions with respect to distributions with uniform input marginal. We propose a new quantum data access model that we call "mixture-of-superpositions" quantum example
    
[^119]: $K$最近邻重采样用于随机控制中的离线策略评估

    $K$-Nearest-Neighbor Resampling for Off-Policy Evaluation in Stochastic Control. (arXiv:2306.04836v1 [stat.ML])

    [http://arxiv.org/abs/2306.04836](http://arxiv.org/abs/2306.04836)

    提出了一种新颖的$K$最近邻重采样方法，用于估算历史数据中由不同策略生成的决策过程的性能，解决了离线策略评估中的反事实估计问题。

    

    本文提出了一种新颖的$K$最近邻重采样方法，用于估算历史数据中由不同策略生成的决策过程的性能。我们专注于依赖于当前状态的反馈策略，这种策略适用于具有连续状态-动作空间和所选动作影响下的系统固有随机性的环境。这些设置在许多高风险应用程序中很常见，并在随机控制的上下文中积极研究。我们的过程利用了类似的状态/动作对（在度量意义下）与类似的奖励和状态转换相关。这使得我们的重采样过程通过类似于蒙特卡罗方法的轨迹模拟来解决离线策略评估（OPE）中的反事实估计问题。与其他OPE方法相比，我们的算法不需要优化，可以通过基于树的最近邻搜索高效实现，并且本质上是可并行化的。我们提供理论性能保证，并在基准环境下展示了我们算法的优越实验性能。

    We propose a novel $K$-nearest neighbor resampling procedure for estimating the performance of a policy from historical data containing realized episodes of a decision process generated under a different policy. We focus on feedback policies that depend deterministically on the current state in environments with continuous state-action spaces and system-inherent stochasticity effected by chosen actions. Such settings are common in a wide range of high-stake applications and are actively investigated in the context of stochastic control. Our procedure exploits that similar state/action pairs (in a metric sense) are associated with similar rewards and state transitions. This enables our resampling procedure to tackle the counterfactual estimation problem underlying off-policy evaluation (OPE) by simulating trajectories similarly to Monte Carlo methods. Compared to other OPE methods, our algorithm does not require optimization, can be efficiently implemented via tree-based nearest neighbo
    
[^120]: 增强图神经网络的反事实推理能力通过归纳性

    Empowering Counterfactual Reasoning over Graph Neural Networks through Inductivity. (arXiv:2306.04835v1 [cs.LG])

    [http://arxiv.org/abs/2306.04835](http://arxiv.org/abs/2306.04835)

    本研究引入了一种归纳算法INDUCE来增强图神经网络的反事实推理能力，改进了现有算法中存在的限制，通过在多个数据集上进行广泛的实验证明了其可行性。

    

    图神经网络（GNN）有多种实际应用，例如药物发现、推荐引擎和芯片设计。但是，GNN缺乏透明度，因为它们无法提供可理解的解释来支持其预测。为了解决这个问题，使用了反事实推理。其主要目标是对GNN的输入图进行最小更改，以改变其预测结果。虽然已经提出了几种算法来解释GNN的反事实结果，但它们大多存在两个主要缺点。首先，它们只考虑边删除作为扰动。其次，反事实解释模型是传导性的，意味着它们不能推广到未见过的数据。在本研究中，我们引入了一种称为INDUCE的归纳算法来克服这些限制。通过在多个数据集上进行广泛的实验，我们证明了包括边添加在内的改进可获得比现有方法更好的反事实结果。此外，归纳模型在未见过的数据上也展现了良好的泛化性。

    Graph neural networks (GNNs) have various practical applications, such as drug discovery, recommendation engines, and chip design. However, GNNs lack transparency as they cannot provide understandable explanations for their predictions. To address this issue, counterfactual reasoning is used. The main goal is to make minimal changes to the input graph of a GNN in order to alter its prediction. While several algorithms have been proposed for counterfactual explanations of GNNs, most of them have two main drawbacks. Firstly, they only consider edge deletions as perturbations. Secondly, the counterfactual explanation models are transductive, meaning they do not generalize to unseen data. In this study, we introduce an inductive algorithm called INDUCE, which overcomes these limitations. By conducting extensive experiments on several datasets, we demonstrate that incorporating edge additions leads to better counterfactual results compared to the existing methods. Moreover, the inductive mo
    
[^121]: 一种半监督的水下图像目标检测算法

    A Semi-supervised Object Detection Algorithm for Underwater Imagery. (arXiv:2306.04834v1 [cs.CV])

    [http://arxiv.org/abs/2306.04834](http://arxiv.org/abs/2306.04834)

    该研究提出了一种基于变分自编码器的半监督框架，将人造物体视为异常来检测它们，通过在低维潜在空间中聚类图像数据并提取可能包含异常特征的图像以及设计一个基于提取图像重建质量差的异常得分方法，人类操作员可以更有效地检测水下目标。

    

    在自主水下机器人（AUV）采集的水下图像中检测人造物体是许多海底应用的关键要求。实际世界中的AUV图像数据集往往非常大且未标记。此外，这些数据集通常不平衡，包含很少的感兴趣对象的实例，特别是在搜索场景中的异常对象时。因此，难以拟合能够可靠检测这些对象的模型。考虑到这些因素，我们建议将人造物体视为异常值，并通过基于变分自编码器（VAE）的半监督框架来检测它们。我们开发了一种方法，将图像数据聚类到一个学习到的低维潜在空间中，并提取可能包含异常特征的图像。我们还设计了一种基于提取图像的重建质量差的区域的异常得分。我们证明，通过在大型图像数据集上应用这两种方法，可以向人类操作员显示候选物体，从而大大提高水下目标检测的效率。

    Detection of artificial objects from underwater imagery gathered by Autonomous Underwater Vehicles (AUVs) is a key requirement for many subsea applications. Real-world AUV image datasets tend to be very large and unlabelled. Furthermore, such datasets are typically imbalanced, containing few instances of objects of interest, particularly when searching for unusual objects in a scene. It is therefore, difficult to fit models capable of reliably detecting these objects. Given these factors, we propose to treat artificial objects as anomalies and detect them through a semi-supervised framework based on Variational Autoencoders (VAEs). We develop a method which clusters image data in a learned low-dimensional latent space and extracts images that are likely to contain anomalous features. We also devise an anomaly score based on extracting poorly reconstructed regions of an image. We demonstrate that by applying both methods on large image datasets, human operators can be shown candidate an
    
[^122]: 利用预测时间特征相似性的物体中心学习实现对真实世界视频的分析

    Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities. (arXiv:2306.04829v1 [cs.CV])

    [http://arxiv.org/abs/2306.04829](http://arxiv.org/abs/2306.04829)

    本研究提出了一种新方法，利用预训练的自监督特征和时间特征相似性损失，实现了对真实世界视频的物体中心学习，在合成MOVi数据集上取得了最先进的性能。同时，本模型是首个能够扩展到无约束视频数据集的物体中心视频模型。

    

    无监督的基于视频的物体中心学习是从大规模无标签视频集合中学习结构化表示的有前途的途径。然而，以前的方法只能在受限领域内缩放到真实世界的数据集。最近的研究表明，预训练的自监督特征的重建会导致在不受约束的真实世界图像数据集上的物体中心表示。基于这种方法，我们提出了一种利用这些预训练特征的新方法，形式为时间特征相似性损失。该损失编码图像块之间的时间相关性，并自然地引入运动偏差来发现物体。我们证明，这种损失导致了在具有挑战性的合成MOVi数据集上的最先进性能。当与特征重建损失结合使用时，我们的模型是首个能够扩展到无约束视频数据集（如YouTube-VIS）的物体中心视频模型。

    Unsupervised video-based object-centric learning is a promising avenue to learn structured representations from large, unlabeled video collections, but previous approaches have only managed to scale to real-world datasets in restricted domains. Recently, it was shown that the reconstruction of pre-trained self-supervised features leads to object-centric representations on unconstrained real-world image datasets. Building on this approach, we propose a novel way to use such pre-trained features in the form of a temporal feature similarity loss. This loss encodes temporal correlations between image patches and is a natural way to introduce a motion bias for object discovery. We demonstrate that this loss leads to state-of-the-art performance on the challenging synthetic MOVi datasets. When used in combination with the feature reconstruction loss, our model is the first object-centric video model that scales to unconstrained video datasets such as YouTube-VIS.
    
[^123]: 线性化随机生成树与GNN的快速高效训练

    Fast and Effective GNN Training with Linearized Random Spanning Trees. (arXiv:2306.04828v1 [cs.LG])

    [http://arxiv.org/abs/2306.04828](http://arxiv.org/abs/2306.04828)

    本文提出了一种基于线性化随机生成树的GNN训练框架，在多个真实世界的图形基准测试中表现得比其他经典算法更快且更准确。

    

    我们提出了一种新的有效和可扩展的框架，用于在给定图形结构数据的监督节点分类任务中训练GNN。我们的方法通过线性化从输入网络中提取的随机生成树得到一系列路径图来逐步精细化权重更新操作。路径图被设计为保留原始图的基本拓扑和节点信息。同时，路径图的稀疏性使得GNN训练更轻便，除了可扩展性外，还有助于缓解过度压缩和过度平滑等经典训练问题。我们在多个真实世界的图形基准测试上进行了广泛的实验研究，并将我们的框架应用于图形卷积网络，与众所周知的基线相比，同时提高了训练速度和测试精度。

    We present a new effective and scalable framework for training GNNs in supervised node classification tasks, given graph-structured data. Our approach increasingly refines the weight update operations on a sequence of path graphs obtained by linearizing random spanning trees extracted from the input network. The path graphs are designed to retain essential topological and node information of the original graph. At the same time, the sparsity of path graphs enables a much lighter GNN training which, besides scalability, helps in mitigating classical training issues, like over-squashing and over-smoothing. We carry out an extensive experimental investigation on a number of real-world graph benchmarks, where we apply our framework to graph convolutional networks, showing simultaneous improvement of both training speed and test accuracy, as compared to well-known baselines.
    
[^124]: 稀疏线性质心编码器：一种特征选择的凸方法

    Sparse Linear Centroid-Encoder: A Convex Method for Feature Selection. (arXiv:2306.04824v1 [cs.LG])

    [http://arxiv.org/abs/2306.04824](http://arxiv.org/abs/2306.04824)

    SLCE是用线性变换将点重构为类别质心并使用$\ell_1$范数惩罚从输入数据中滤除不必要特征的特征选择新方法。该方法为多类数据使用单个模型，具有与最先进方法相当的性能，同时具有显着较少的特征数。

    

    我们提出了一种新的特征选择技术，称为稀疏线性质心编码器（SLCE）。该算法使用一个线性变换将一个点重构为其类别的质心，并同时使用$\ell_1$范数惩罚从输入数据中滤除不必要的特征。优化问题的原始公式是非凸的，但我们提出了一个两步法，其中每一步都是凸的。在第一步中，我们解决线性质心编码器，它是一个矩阵$A$上的凸优化问题。在第二步中，我们只在对角线矩阵$B$上搜索稀疏解，同时保持$A$不变。与其他线性方法（例如稀疏支持向量机和Lasso）不同，稀疏线性质心编码器对于多类数据使用单个模型。我们对所提出的模型进行了深入的实证分析，并表明它促进了各种数据集（包括高维生物数据）上的稀疏性。我们的实验结果表明，SLCE在分类准确率方面具有与最先进方法相当的性能，同时具有显着较少的特征数。

    We present a novel feature selection technique, Sparse Linear Centroid-Encoder (SLCE). The algorithm uses a linear transformation to reconstruct a point as its class centroid and, at the same time, uses the $\ell_1$-norm penalty to filter out unnecessary features from the input data. The original formulation of the optimization problem is nonconvex, but we propose a two-step approach, where each step is convex. In the first step, we solve the linear Centroid-Encoder, a convex optimization problem over a matrix $A$. In the second step, we only search for a sparse solution over a diagonal matrix $B$ while keeping $A$ fixed. Unlike other linear methods, e.g., Sparse Support Vector Machines and Lasso, Sparse Linear Centroid-Encoder uses a single model for multi-class data. We present an in-depth empirical analysis of the proposed model and show that it promotes sparsity on various data sets, including high-dimensional biological data. Our experimental results show that SLCE has a performan
    
[^125]: SiBBlInGS: 使用跨状态的图形相似性驱动模块推理的建模块方法

    SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States. (arXiv:2306.04817v1 [stat.ML])

    [http://arxiv.org/abs/2306.04817](http://arxiv.org/abs/2306.04817)

    本文提出了一种跨状态的图形相似性驱动的模块推理框架，可以同时考虑数据中的状态间和状态内关系，并允许状态之间的会话计数和持续时间的差异。它可以提取非正交组件，并且能够识别特定状态与状态非特定模块。

    

    对于多维时间序列来说，提取有意义的模块是发现复杂系统中有价值见解的关键。本文提出了一种基于图形相似性驱动的模块推理框架(SiBBlInGS)，用于发现模块，同时考虑到数据中的状态间和状态内关系，能够提取非正交组件，并允许状态之间的会话计数和持续时间差异。此外，SiBBlInGS还允许跨状态变化模块结构和每次试验的时间变异，并可识别特定状态与状态非特定模块。

    Interpretable methods for extracting meaningful building blocks (BBs) underlying multi-dimensional time series are vital for discovering valuable insights in complex systems. Existing techniques, however, encounter limitations that restrict their applicability to real-world systems, like reliance on orthogonality assumptions, inadequate incorporation of inter- and intra-state variability, and incapability to handle sessions of varying duration. Here, we present a framework for Similarity-driven Building Block Inference using Graphs across States (SiBBlInGS). SiBBlInGS employs a graph-based dictionary learning approach for BB discovery, simultaneously considers both inter- and intra-state relationships in the data, can extract non-orthogonal components, and allows for variations in session counts and duration across states. Additionally, SiBBlInGS allows for cross-state variations in BB structure and per-trial temporal variability, can identify state-specific vs state-invariant BBs, and
    
[^126]: SGD中的投石机：训练损失中的尖峰及其通过特征学习对泛化的影响

    Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning. (arXiv:2306.04815v1 [cs.LG])

    [http://arxiv.org/abs/2306.04815](http://arxiv.org/abs/2306.04815)

    本文通过研究SGD训练损失中的尖峰现象，提出了“投石机”优化现象，通过增加与真实预测器的平均梯度外积对齐来促进特征学习，并证明较小的批量大小可提高泛化性能。

    

    本文首先解释了神经网络在使用随机梯度下降（SGD）进行训练时为什么经常出现训练损失尖峰的现象。我们提供了证据表明，SGD训练损失中的尖峰是“投石机”，这是一种优化现象，最初在[Lewkowycz等人，2020年]的大学习率GD中观察到。我们通过实验证明这些投石机出现在由正切内核的前几个特征向量所张成的低维子空间中，适用于GD和SGD。其次，我们提出了一个解释，即投石机如何通过增加与真实预测器的平均梯度外积（AGOP）对齐来促进特征学习，从而实现更好的泛化。此外，我们证明，在SGD中，更小的批量大小会导致更多的投石机出现，从而提高AGOP对齐和测试性能。

    In this paper, we first present an explanation regarding the common occurrence of spikes in the training loss when neural networks are trained with stochastic gradient descent (SGD). We provide evidence that the spikes in the training loss of SGD are "catapults", an optimization phenomenon originally observed in GD with large learning rates in [Lewkowycz et al. 2020]. We empirically show that these catapults occur in a low-dimensional subspace spanned by the top eigenvectors of the tangent kernel, for both GD and SGD. Second, we posit an explanation for how catapults lead to better generalization by demonstrating that catapults promote feature learning by increasing alignment with the Average Gradient Outer Product (AGOP) of the true predictor. Furthermore, we demonstrate that a smaller batch size in SGD induces a larger number of catapults, thereby improving AGOP alignment and test performance.
    
[^127]: 相关信息最大化：一种无需权重对称的有监督深度神经网络的生物合理方法

    Correlative Information Maximization: A Biologically Plausible Approach to Supervised Deep Neural Networks without Weight Symmetry. (arXiv:2306.04810v1 [cs.NE])

    [http://arxiv.org/abs/2306.04810](http://arxiv.org/abs/2306.04810)

    本文提出了一种无需权重对称的有监督深度神经网络的生物合理方法，该方法利用相关信息最大化在层激活之间描述生物神经网络中的信号传播。通过坐标下降优化相应的目标和均方误差损失函数，可以产生一个更生物真实的神经网络结构。

    

    反向传播算法在训练大规模人工神经网络方面取得了显著的成功，但其生物合理性受到争议，现在仍然存在一个开放的问题，即大脑是否采用类似于它的监督学习机制。在本文中，我们提出了在层激活之间进行相关信息最大化的替代规范方法，以描述生物神经网络中信号在前向和后向方向上传播的机制。这种新框架解决了有关传统人工神经网络和反向传播算法生物合理性的许多问题。相应目标的坐标下降优化，与均方误差损失函数相结合，可以产生一个神经网络结构，模拟一种具有树突处理和侧抑制神经元的更生物真实的多室金字塔形神经元网络。

    The backpropagation algorithm has experienced remarkable success in training large-scale artificial neural networks, however, its biological-plausibility is disputed, and it remains an open question whether the brain employs supervised learning mechanisms akin to it. Here, we propose correlative information maximization between layer activations as an alternative normative approach to describe the signal propagation in biological neural networks in both forward and backward directions. This new framework addresses many concerns about the biological-plausibility of conventional artificial neural networks and the backpropagation algorithm. The coordinate descent-based optimization of the corresponding objective, combined with the mean square error loss function for fitting labeled supervision data, gives rise to a neural network structure that emulates a more biologically realistic network of multi-compartment pyramidal neurons with dendritic processing and lateral inhibitory neurons. Fu
    
[^128]: 使用语言模型私下生成表格数据

    Privately generating tabular data using language models. (arXiv:2306.04803v1 [cs.LG])

    [http://arxiv.org/abs/2306.04803](http://arxiv.org/abs/2306.04803)

    通过使用语言模型训练每一行数据作为一个句子并添加差分隐私，可以在多个数据集中生成具有竞争力的合成数据，以实现私人数据生成。

    

    在以隐私为先的世界中，私下生成合成数据是非常重要的。我们提出并研究了一种简单的方法，将表格中的每一行视为一个句子，并训练差分隐私语言模型。我们展示了这种方法在建模多个数据集的表格数据时取得了具有竞争力的结果，即使是在有利于基于边缘分布的替代方法的小规模情况下也是如此。

    Privately generating synthetic data from a table is an important brick of a privacy-first world. We propose and investigate a simple approach of treating each row in a table as a sentence and training a language model with differential privacy. We show this approach obtains competitive results in modelling tabular data across multiple datasets, even at small scales that favor alternative methods based on marginal distributions.
    
[^129]: 医疗知识图谱综述：资源、应用和前景

    A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])

    [http://arxiv.org/abs/2306.04802](http://arxiv.org/abs/2306.04802)

    本论文综述了医疗知识图谱(HKGs)的构建流程、关键技术和利用方法以及现有资源，并深入探讨了HKG在各种医疗领域的变革性影响。

    

    医疗知识图谱(HKGs)已成为组织医学知识的有结构且可解释的有为工具，提供了医学概念及其关系的全面视图。然而，数据异质性和覆盖范围有限等挑战仍然存在，强调了在HKG领域需要进一步研究的必要性。本综述是HKG的第一份综合概述。我们总结了HKG构建的流程和关键技术（即从头开始和通过集成），以及常见的利用方法（即基于模型和非基于模型）。为了为研究人员提供有价值的资源，我们根据它们捕获的数据类型和应用领域（该资源存储于https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase）组织了现有的HKG，并提供了相关的统计信息。在应用部分，我们深入探讨了HKG在各种医疗领域的变革性影响。

    Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
    
[^130]: 使用稀疏自适应瓶颈质心编码器进行特征选择

    Feature Selection using Sparse Adaptive Bottleneck Centroid-Encoder. (arXiv:2306.04795v1 [cs.LG])

    [http://arxiv.org/abs/2306.04795](http://arxiv.org/abs/2306.04795)

    该论文提出一种基于稀疏自适应瓶颈质心编码器的特征选择模型，该模型通过提取有区别的特征组以及在重构类别质心的同时减少同类分散度、增加不同类别质心的分离性，从而实现过滤输入数据中不必要特征的功能。

    

    我们引入了一种新的非线性模型，即稀疏自适应瓶颈质心编码器（SABCE），用于确定能够区分两个或多个类别的特征。该算法旨在提取有区别的特征组，并在环境空间中重构类别质心，同时在瓶颈层使用附加惩罚项来减少同类分散度并增加不同类别质心的分离性。模型具有促进稀疏性的层（SPL），与输入层具有一对一的连接。除了主要目标，我们还最小化稀疏层的$l_{2,1}$-范数，从而过滤掉输入数据中的不必要的特征。在训练期间，我们通过将质心和稀疏层的权重的Hadamard积来更新类别质心，从而忽略目标中的无关特征。因此，该方法学习重建类别质心的关键组件，而不是整个质心。

    We introduce a novel nonlinear model, Sparse Adaptive Bottleneck Centroid-Encoder (SABCE), for determining the features that discriminate between two or more classes. The algorithm aims to extract discriminatory features in groups while reconstructing the class centroids in the ambient space and simultaneously use additional penalty terms in the bottleneck layer to decrease within-class scatter and increase the separation of different class centroids. The model has a sparsity-promoting layer (SPL) with a one-to-one connection to the input layer. Along with the primary objective, we minimize the $l_{2,1}$-norm of the sparse layer, which filters out unnecessary features from input data. During training, we update class centroids by taking the Hadamard product of the centroids and weights of the sparse layer, thus ignoring the irrelevant features from the target. Therefore the proposed method learns to reconstruct the critical components of class centroids rather than the whole centroids.
    
[^131]: 论模型、数据和特征的联合交互

    On the Joint Interaction of Models, Data, and Features. (arXiv:2306.04793v1 [cs.LG])

    [http://arxiv.org/abs/2306.04793](http://arxiv.org/abs/2306.04793)

    本文提出了一种新工具--交互张量用于通过特征对模型和数据之间的交互进行经验分析，并提出了一个特征学习的概念框架，可以解释一些经验观察现象，表明深度学习可能受益于探索传统IID（独立同分布）假设之外的数据。

    

    从数据中学习特征是深度学习的一个定义性特征，但是我们对于特征在深度学习中所起的作用的理解仍然很有限。为了弥补这一空白，我们引入了一种新工具，交互张量，用于通过特征对模型和数据之间的交互进行经验分析。通过交互张量，我们对特征在数据中的分布以及不同随机种子的模型学习不同特征等方面做出了几个关键观察。基于这些观察结果，我们提出了一个特征学习的概念框架。在这个框架下，可以通过闭式形式推导出单个假设的期望准确率和一对假设的一致性。我们证明了所提出的框架可以解释一些经验观察现象，包括最近发现的广义化不同等式（GDE），它可以使用仅无标签数据来估计泛化误差。此外，我们的理论表明，深度学习可能受益于探索传统IID（独立同分布）假设之外的数据。

    Learning features from data is one of the defining characteristics of deep learning, but our theoretical understanding of the role features play in deep learning is still rudimentary. To address this gap, we introduce a new tool, the interaction tensor, for empirically analyzing the interaction between data and model through features. With the interaction tensor, we make several key observations about how features are distributed in data and how models with different random seeds learn different features. Based on these observations, we propose a conceptual framework for feature learning. Under this framework, the expected accuracy for a single hypothesis and agreement for a pair of hypotheses can both be derived in closed-form. We demonstrate that the proposed framework can explain empirically observed phenomena, including the recently discovered Generalization Disagreement Equality (GDE) that allows for estimating the generalization error with only unlabeled data. Further, our theory
    
[^132]: 基于Transformer的无监督多文档抽象摘要模型

    Absformer: Transformer-based Model for Unsupervised Multi-Document Abstractive Summarization. (arXiv:2306.04787v1 [cs.CL])

    [http://arxiv.org/abs/2306.04787](http://arxiv.org/abs/2306.04787)

    本文提出了一种新的用于无监督多文档抽象摘要生成的Transformer-based方法Absformer，它使用掩码语言建模（MLM）目标进行预训练聚类文档并生成抽象摘要，实验结果显示，它在基准数据集上优于几种最先进的无监督抽象MDS方法。

    

    多文档摘要（MDS）是将多个文档中的文本总结成简洁概括的任务。所生成的摘要通过用少数几句话提供重要内容，可以省去阅读多个文档的时间。抽象MDS旨在使用自然语言生成技术为多个文档生成连贯、流畅的摘要。本文考虑仅有文档而没有摘要的无监督抽象MDS环境，并提出Absformer，这是一种用于无监督抽象摘要生成的新型基于Transformer的方法。我们的方法包括第一步，使用掩码语言建模（MLM）目标作为预训练任务，以训练Transformer编码器，将文档聚类为语义相似的组；第二步，训练一个Transformer解码器，为文档集群生成抽象摘要。据我们所知，这是第一份提出将聚类用于文档组合的无监督抽象MDS方法的工作。我们在基准数据集上的实验结果显示，我们的方法优于几种最先进的无监督抽象MDS方法。

    Multi-document summarization (MDS) refers to the task of summarizing the text in multiple documents into a concise summary. The generated summary can save the time of reading many documents by providing the important content in the form of a few sentences. Abstractive MDS aims to generate a coherent and fluent summary for multiple documents using natural language generation techniques. In this paper, we consider the unsupervised abstractive MDS setting where there are only documents with no groundtruh summaries provided, and we propose Absformer, a new Transformer-based method for unsupervised abstractive summary generation. Our method consists of a first step where we pretrain a Transformer-based encoder using the masked language modeling (MLM) objective as the pretraining task in order to cluster the documents into semantically similar groups; and a second step where we train a Transformer-based decoder to generate abstractive summaries for the clusters of documents. To our knowledge
    
[^133]: 可解释的深度聚类

    Interpretable Deep Clustering. (arXiv:2306.04785v1 [cs.LG])

    [http://arxiv.org/abs/2306.04785](http://arxiv.org/abs/2306.04785)

    本文提出了一种可解释的深度学习框架，通过自我监督的方式从数据点中标识信息量丰富的特征，设计了一个模型和门矩阵来预测可解释的实例和聚类级别的聚类分配，并在合成和实际数据中验证了其可靠性和可解释性。

    

    聚类是一项广泛应用于数据分析中的基础学习任务。例如，生物学家经常使用聚类分配来分析基因组序列、医疗记录或图像。由于下游分析通常在聚类级别上执行，因此从业者寻求可靠且可解释的聚类模型。本文提出了一种新的深度学习框架，它可以预测可解释的实例和聚类级别的聚类分配。首先，我们提出一个自我监督的过程来从每个数据点中标识出一组信息量丰富的特征子集。然后，我们设计了一个模型，用于预测聚类分配和一个门矩阵，用于引导聚类级别的特征选择。我们证明了所提出的方法可以使用合成和实际数据可靠地预测聚类分配。此外，我们验证了我们的模型可以在实例和聚类级别上产生可解释的结果。

    Clustering is a fundamental learning task widely used as a first step in data analysis. For example, biologists often use cluster assignments to analyze genome sequences, medical records, or images. Since downstream analysis is typically performed at the cluster level, practitioners seek reliable and interpretable clustering models. We propose a new deep-learning framework that predicts interpretable cluster assignments at the instance and cluster levels. First, we present a self-supervised procedure to identify a subset of informative features from each data point. Then, we design a model that predicts cluster assignments and a gate matrix that leads to cluster-level feature selection. We show that the proposed method can reliably predict cluster assignments using synthetic and real data. Furthermore, we verify that our model leads to interpretable results at a sample and cluster level.
    
[^134]: 无人机群体学习协同导航在湍流中的应用：一种基于深度强化学习的合作方法

    Learning to Navigate in Turbulent Flows with Aerial Robot Swarms: A Cooperative Deep Reinforcement Learning Approach. (arXiv:2306.04781v1 [cs.RO])

    [http://arxiv.org/abs/2306.04781](http://arxiv.org/abs/2306.04781)

    本文提出了一种基于深度强化学习的多机器人控制器，通过嵌套控制架构将轨迹跟踪控制与湍流补偿分离，以解决在湍流风况下无人机群组协同运动的问题。

    

    在湍流环境中的空中操作是一个具有挑战性的问题，由于气流的混沌行为，这个问题变得更加复杂，特别是当一组无人机试图在湍流风况下实现协调运动时。本文提出了一种新的多机器人控制器来在湍流中导航，通过嵌套控制架构将轨迹跟踪控制与湍流补偿分离。与以往的方法不同的是，我们的方法不会在特定的时间和空间学习补偿空气流动。相反，我们的方法基于湍流对团队的影响来学习补偿空气流动，在实现过程中采用了基于图卷积神经网络（GCNN）的深度强化学习方法，可以通过处理团队中风流的时空相关性来实现更好的风力补偿效果。我们的方法可以很好地适用于大型机器人团队，因为每个机器人只使用局部信息。

    Aerial operation in turbulent environments is a challenging problem due to the chaotic behavior of the flow. This problem is made even more complex when a team of aerial robots is trying to achieve coordinated motion in turbulent wind conditions. In this paper, we present a novel multi-robot controller to navigate in turbulent flows, decoupling the trajectory-tracking control from the turbulence compensation via a nested control architecture. Unlike previous works, our method does not learn to compensate for the air-flow at a specific time and space. Instead, our method learns to compensate for the flow based on its effect on the team. This is made possible via a deep reinforcement learning approach, implemented via a Graph Convolutional Neural Network (GCNN)-based architecture, which enables robots to achieve better wind compensation by processing the spatial-temporal correlation of wind flows across the team. Our approach scales well to large robot teams -- as each robot only uses in
    
[^135]: 行为博弈论的损失函数研究

    Loss Functions for Behavioral Game Theory. (arXiv:2306.04778v1 [cs.LG])

    [http://arxiv.org/abs/2306.04778](http://arxiv.org/abs/2306.04778)

    本文研究了行为博弈论家在损失函数选择上的差异，并构建了一组满足特定公理的损失函数集合，其中平方L2误差是实践中唯一可接受的损失函数，并建议行为博弈论家继续使用它。

    

    行为博弈论家们使用实验数据评估人类行为的预测模型，但是他们在损失函数的选择上存在很大的差异，错误率、负对数似然、交叉熵、Brier得分和L2误差都是常见的选择。本文试图提供一个原则性的答案，解决哪些损失函数适用于这个任务的问题，并规范化了认为损失函数应该满足的准则。我们构建了一组损失函数，称为“对角线有界Bregman散度”，满足所有这些公理，并包括平方L2误差。实际上，平方L2误差是相对常用的唯一可接受的损失函数。因此，我们建议行为博弈论家继续使用它。

    Behavioral game theorists all use experimental data to evaluate predictive models of human behavior. However, they differ greatly in their choice of loss function for these evaluations, with error rate, negative log-likelihood, cross-entropy, Brier score, and L2 error all being common choices. We attempt to offer a principled answer to the question of which loss functions make sense for this task, formalizing desiderata that we argue loss functions should satisfy. We construct a family of loss functions, which we dub "diagonal bounded Bregman divergences", that satisfy all of these axioms and includes the squared L2 error. In fact, the squared L2 error is the only acceptable loss that is relatively commonly used in practice; we thus recommend its continued use to behavioral game theorists.
    
[^136]: 不变因果集覆盖机

    Invariant Causal Set Covering Machines. (arXiv:2306.04777v1 [cs.LG])

    [http://arxiv.org/abs/2306.04777](http://arxiv.org/abs/2306.04777)

    本文提出了一种名为不变因果集覆盖机的算法，它避免了产生虚假关联，可以在多项式时间内识别感兴趣变量的因果父节点。

    

    基于规则的模型，如决策树，因其可解释的特性受到从业者的欢迎。然而，产生这种模型的学习算法往往容易受到虚假关联的影响，因此不能保证提取的是具有因果关系的洞见。在这项工作中，我们借鉴了不变因果预测文献中的思想，提出了不变的因果集覆盖机，这是一种经典的集覆盖机算法的扩展，用于二值规则的合取/析取，可以证明它避免了虚假关联。我们理论上和实践上证明，我们的方法可以在多项式时间内识别感兴趣变量的因果父节点。

    Rule-based models, such as decision trees, appeal to practitioners due to their interpretable nature. However, the learning algorithms that produce such models are often vulnerable to spurious associations and thus, they are not guaranteed to extract causally-relevant insights. In this work, we build on ideas from the invariant causal prediction literature to propose Invariant Causal Set Covering Machines, an extension of the classical Set Covering Machine algorithm for conjunctions/disjunctions of binary-valued rules that provably avoids spurious associations. We demonstrate both theoretically and empirically that our method can identify the causal parents of a variable of interest in polynomial time.
    
[^137]: 利用观测偏差提高矩阵补全的方法研究

    Exploiting Observation Bias to Improve Matrix Completion. (arXiv:2306.04775v1 [cs.LG])

    [http://arxiv.org/abs/2306.04775](http://arxiv.org/abs/2306.04775)

    本研究利用观测偏差来改进矩阵补全问题，提出一个简单的两阶段算法，实现了与对未观测协变量的监督学习性能相当的结果。

    

    我们考虑了一种变形的矩阵补全问题，其中输入数据以偏差的方式呈现，类似于Ma和Chen所引入的模型。我们的目标是利用偏差与感兴趣的结果之间的共享信息来改进预测。为此，我们提出了一个简单的两阶段算法：（i）将观测模式解释为完全观测的噪声矩阵，我们对观测模式应用传统的矩阵补全方法来估计潜在因素之间的距离； (ii)我们对恢复的特征应用监督学习来填补缺失观察。我们建立了有限样本误差率，这些误差率与相应的监督学习参数率相竞争，这表明我们的学习性能与使用未观测协变量相当。实证评估使用真实世界数据集反映了类似的表现。

    We consider a variant of matrix completion where entries are revealed in a biased manner, adopting a model akin to that introduced by Ma and Chen. Instead of treating this observation bias as a disadvantage, as is typically the case, our goal is to exploit the shared information between the bias and the outcome of interest to improve predictions. Towards this, we propose a simple two-stage algorithm: (i) interpreting the observation pattern as a fully observed noisy matrix, we apply traditional matrix completion methods to the observation pattern to estimate the distances between the latent factors; (ii) we apply supervised learning on the recovered features to impute missing observations. We establish finite-sample error rates that are competitive with the corresponding supervised learning parametric rates, suggesting that our learning performance is comparable to having access to the unobserved covariates. Empirical evaluation using a real-world dataset reflects similar performance g
    
[^138]: 用辅助知识图谱实现在 $d \gg n$ 情况下的表格深度学习

    Enabling tabular deep learning when $d \gg n$ with an auxiliary knowledge graph. (arXiv:2306.04766v1 [cs.LG])

    [http://arxiv.org/abs/2306.04766](http://arxiv.org/abs/2306.04766)

    该论文提出了 PLATO 方法，通过使用描述输入特征的辅助 KG 来规范 MLP，在 $d \gg n$ 的表格数据上实现了强大的性能。

    

    机器学习模型在具有丰富标记样本的数据集上表现出很强的性能，但对于具有非常高维特征但样本数有限（即 $d \gg n$ 的表格数据），机器学习模型很难实现强大的性能。在这里，作者的主要洞见在于输入特征通常具有丰富的辅助领域信息，这些信息可以被组织成异构的知识图谱。作者提出了 PLATO 方法，使用描述输入特征的辅助 KG 来规范一个 MLP，在 $d \gg n$ 的表格数据上实现了强大的性能。

    Machine learning models exhibit strong performance on datasets with abundant labeled samples. However, for tabular datasets with extremely high $d$-dimensional features but limited $n$ samples (i.e. $d \gg n$), machine learning models struggle to achieve strong performance due to the risk of overfitting. Here, our key insight is that there is often abundant, auxiliary domain information describing input features which can be structured as a heterogeneous knowledge graph (KG). We propose PLATO, a method that achieves strong performance on tabular data with $d \gg n$ by using an auxiliary KG describing input features to regularize a multilayer perceptron (MLP). In PLATO, each input feature corresponds to a node in the auxiliary KG. In the MLP's first layer, each input feature also corresponds to a weight vector. PLATO is based on the inductive bias that two input features corresponding to similar nodes in the auxiliary KG should have similar weight vectors in the MLP's first layer. PLATO
    
[^139]: 全切片图像的上下文感知自监督学习

    Context-Aware Self-Supervised Learning of Whole Slide Images. (arXiv:2306.04763v1 [eess.IV])

    [http://arxiv.org/abs/2306.04763](http://arxiv.org/abs/2306.04763)

    本文提出了一种全切片图像使用图卷积网络进行上下文感知自监督学习的新的两阶段学习技术，该技术可提高癌症诊断的效率和准确性。

    

    把全切片图像 (WSIs) 呈现为图形，将为癌症诊断提供更高效和准确的学习框架。 由于单个WSI由数十亿像素组成，而计算病理学所需的广泛注释的数据集不足，因此使用典型的深度学习方法，如卷积神经网络 (CNN)，从WSIs中学习是具有挑战性的。此外，WSIs下采样可能导致丢失对癌症检测至关重要的数据。 本文提出了一种新的两阶段学习技术。 鉴于上下文（例如瘤周围的拓扑特征）可能持有癌症分级和诊断的重要信息，因此捕捉WSI中所有区域之间的依赖关系的图形表示非常直观。使用图卷积网络(GCN)从肿瘤和相邻组织中捕捉上下文，并使用自我监督学习通过未标记的数据来增强训练。更具体地说，该方法是使用GCN的WSIs上下文感知自监督学习，可实现高效和准确的癌症诊断。

    Presenting whole slide images (WSIs) as graph will enable a more efficient and accurate learning framework for cancer diagnosis. Due to the fact that a single WSI consists of billions of pixels and there is a lack of vast annotated datasets required for computational pathology, the problem of learning from WSIs using typical deep learning approaches such as convolutional neural network (CNN) is challenging. Additionally, WSIs down-sampling may lead to the loss of data that is essential for cancer detection. A novel two-stage learning technique is presented in this work. Since context, such as topological features in the tumor surroundings, may hold important information for cancer grading and diagnosis, a graph representation capturing all dependencies among regions in the WSI is very intuitive. Graph convolutional network (GCN) is deployed to include context from the tumor and adjacent tissues, and self-supervised learning is used to enhance training through unlabeled data. More speci
    
[^140]: 机器学习分析、识别和预测帕金森病亚型和进展

    Analysis, Identification and Prediction of Parkinson's disease sub-types and progression through Machine Learning. (arXiv:2306.04748v1 [cs.LG])

    [http://arxiv.org/abs/2306.04748](http://arxiv.org/abs/2306.04748)

    本文使用监督和无监督机器学习方法鉴别出三种不同的帕金森病亚型，并生成患者个性化的症状进展预测，有助于制定针对性干预和改善患者预后。

    

    帕金森病（PD）是一种常见的神经退行性疾病，患者表现多种多样，但对其潜在原因和症状进展的了解还很有限。帕金森进展标志物计划（PPMI）收集了多个患者群的详细纵向数据以确定生物标志物并辅助干预方法的制定。尽管已经有超过110个机器学习研究使用PPMI数据库，但大部分研究仅聚焦于监督模型用于诊断预测，限制了对患者多样性和进展的认识。本文通过结合监督和无监督的机器学习方法来鉴别亚型，准确预测帕金森患者疾病进程。本研究利用PPMI数据库中5年的纵向数据并在之前研究的基础上，整合无监督的病人聚类和病人当前和未来症状的预测。我们的方法鉴别出三个不同的帕金森病亚型，并为个体患者生成个性化的症状进展预测。这种方法提高了预测疾病进展的准确性，帮助制定个性化干预方法并改善患者的预后。

    Parkinson's disease (PD) is a prevalent neurodegenerative disorder with varying patient trajectories, yet little is understood about the underlying causes and symptom progression. The Parkinson's Progression Markers Initiative (PPMI) has collected comprehensive longitudinal data from diverse patient cohorts to identify biomarkers and aid in the development of interventions. Despite over 110 machine learning studies using the PPMI database, the majority have focused on supervised models for diagnosis prediction, which has limited impact on understanding patient variability and progression. This paper addresses this gap by combining supervised and unsupervised machine learning methods to identify subtypes that accurately predict disease progression in Parkinson's patients. Building upon previous work, we replicate and extend the study by integrating unsupervised patient clustering and prediction of present and future symptoms using 5 additional years of longitudinal data from the Progres
    
[^141]: 使用大型语言模型注释进行社会科学中的有效下游统计推断: 基于设计的半监督学习

    Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning. (arXiv:2306.04746v1 [stat.ME])

    [http://arxiv.org/abs/2306.04746](http://arxiv.org/abs/2306.04746)

    该论文提出了一种新算法，使用大型语言模型（LLMs）输出进行下游统计分析，以实现有效的下游统计推断，并降低标签获取的研究成本80％，同时保证CSS研究的统计属性。

    

    在计算社会科学（CSS）中，研究人员通过分析文档来解释社会和政治现象。在大多数情况下，CSS研究人员首先获取文档的标签，然后使用可解释的回归分析来解释标签。大型语言模型（LLMs）的最近进展可以通过在规模上便宜地注释文档来降低CSS研究成本，但这些替代标签通常是不完美和有偏的。我们提出了一种新算法，用于使用LLMs的输出进行下游统计分析，同时保证与CSS研究基本相关的统计属性-如渐近无偏性和正确的不确定性量化。我们表明，直接在下游统计分析中使用LLM预测的替代标签会导致实质性偏差和无效置信区间，即使替代准确性高达80-90％。为了解决这个问题，我们基于无偏机器学习提出了基于设计的半监督学习（D-SSL）算法，该算法将LLM注释与有针对性的采样相结合，以实现有效的下游统计推断。我们的方法可以将标签获取的CSS研究成本降低80％，而不影响统计分析的有效性。模拟研究和实际数据示例表明，与直接使用LLM预测标签相比，D-SSL可以将回归估计的准确性提高多达40％。

    In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. The recent advancements in large language models (LLMs) can lower costs for CSS research by annotating documents cheaply at scale, but such surrogate labels are often imperfect and biased. We present a new algorithm for using outputs from LLMs for downstream statistical analyses while guaranteeing statistical properties -- like asymptotic unbiasedness and proper uncertainty quantification -- which are fundamental to CSS research. We show that direct use of LLM-predicted surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80--90\%. To address this, we build on debiased machine learning to propose the design-based semi-supervised le
    
[^142]: 长期检查中依据时间检索相应的超声图像的自动化方法。

    Automatic retrieval of corresponding US views in longitudinal examinations. (arXiv:2306.04739v1 [cs.LG])

    [http://arxiv.org/abs/2306.04739](http://arxiv.org/abs/2306.04739)

    本文提出了一种自监督对比学习方法，以自动检索不同时间扫描的相似超声肌肉视图。使用从67名ICU患者获取的数据对比了三种不同的模型，结果表明该方法在视图检索任务中表现良好。

    

    在重症监护病房(ICU)长期卧床的危重病患者中，骨骼肌萎缩是常见的情况。必须通过物理治疗恢复肌肉质量，然后才能出院。超声成像通常用于测量肌肉尺寸的变化，以评估恢复过程。然而，由于扫描通常在不同的日期进行，而且可能由不同的操作员进行，因此这些手动测量容易产生较大的变异性。在本文中，我们提出了一种自监督对比学习方法，以自动检索不同时间扫描的相似超声肌肉视图。使用从67名ICU患者获取的数据对比了三种不同的模型。结果表明，我们的对比模型在视图检索任务中表现优于监督基准模型，具有73.52%的AUC值，当与自动分割模型结合使用时，其横截面积误差为5.7%+/-0.24%。

    Skeletal muscle atrophy is a common occurrence in critically ill patients in the intensive care unit (ICU) who spend long periods in bed. Muscle mass must be recovered through physiotherapy before patient discharge and ultrasound imaging is frequently used to assess the recovery process by measuring the muscle size over time. However, these manual measurements are subject to large variability, particularly since the scans are typically acquired on different days and potentially by different operators. In this paper, we propose a self-supervised contrastive learning approach to automatically retrieve similar ultrasound muscle views at different scan times. Three different models were compared using data from 67 patients acquired in the ICU. Results indicate that our contrastive model outperformed a supervised baseline model in the task of view retrieval with an AUC of 73.52% and when combined with an automatic segmentation model achieved 5.7%+/-0.24% error in cross-sectional area. Furth
    
[^143]: 大型语言模型的软提示调整方法用于评估偏差

    Soft-prompt Tuning for Large Language Models to Evaluate Bias. (arXiv:2306.04735v1 [cs.CL])

    [http://arxiv.org/abs/2306.04735](http://arxiv.org/abs/2306.04735)

    本文使用软提示调整来量化大型语言模型中的偏差，避免手动设计提示导致的人为偏差注入。通过分组公平性检查模型对不同敏感属性的偏见，发现了有趣的偏差模式。

    

    近年来，大型语言模型的提示功能因无需标记数据即可产生良好结果而备受青睐。然而，这需要进行提示调整以获得引导更好模型性能的最佳提示。本文中，我们探讨了在情感分类任务中使用软提示调整来量化大型语言模型（LLMs）如Open Pre-trained Transformers（OPT）和Galactica语言模型中的偏差。由于这些模型是在可能偏向某些人群的真实数据上训练的，因此识别这些潜在问题非常重要。使用软提示来评估偏差给我们带来了额外的优势，可以避免手动设计提示导致的人为偏差注入。我们使用分组公平性（偏差）检查模型对不同敏感属性的偏见，并找到了有趣的偏差模式。由于LLMs已在各种应用中被用于工业中，因此我们对其进行的偏见评估具有实际意义。

    Prompting large language models has gained immense popularity in recent years due to the advantage of producing good results even without the need for labelled data. However, this requires prompt tuning to get optimal prompts that lead to better model performances. In this paper, we explore the use of soft-prompt tuning on sentiment classification task to quantify the biases of large language models (LLMs) such as Open Pre-trained Transformers (OPT) and Galactica language model. Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues. Using soft-prompts to evaluate bias gives us the extra advantage of avoiding the human-bias injection that can be caused by manually designed prompts. We check the model biases on different sensitive attributes using the group fairness (bias) and find interesting bias patterns. Since LLMs have been used in the industry in various applications, i
    
[^144]: 随机自然阈值算法

    Stochastic Natural Thresholding Algorithms. (arXiv:2306.04730v1 [eess.SP])

    [http://arxiv.org/abs/2306.04730](http://arxiv.org/abs/2306.04730)

    该论文提出了一种随机自然阈值算法，用于稀疏信号恢复，并扩展了现有的自然阈值法，可以提高算法的计算效率和性能。

    

    稀疏信号恢复是各种应用中最基本的问题之一，包括医学成像和遥感。许多基于硬阈值算子族的贪婪算法已被开发用于解决稀疏信号恢复问题。最近，提出了自然阈值法（NT），具有更好的计算效率。本文通过将NT从带线性测量的确定版本扩展到具有一般目标函数的随机版本，提出并讨论了随机自然阈值算法的收敛性保证。我们还进行了各种数值实验，对线性和非线性测量进行了演示，以展示StoNT的性能。

    Sparse signal recovery is one of the most fundamental problems in various applications, including medical imaging and remote sensing. Many greedy algorithms based on the family of hard thresholding operators have been developed to solve the sparse signal recovery problem. More recently, Natural Thresholding (NT) has been proposed with improved computational efficiency. This paper proposes and discusses convergence guarantees for stochastic natural thresholding algorithms by extending the NT from the deterministic version with linear measurements to the stochastic version with a general objective function. We also conduct various numerical experiments on linear and nonlinear measurements to demonstrate the performance of StoNT.
    
[^145]: 不要相信你的眼睛：关于特征可视化的（不）可靠性。

    Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])

    [http://arxiv.org/abs/2306.04719](http://arxiv.org/abs/2306.04719)

    本文探讨了神经网络如何从像素中提取模式的问题，并研究了特征可视化的可靠性。实验证据表明，由于优化过程中固有的限制，特征可视化能够可靠理解的功能集非常有限，对于解释神经网络如何处理自然图像的解释能力产生怀疑。

    

    神经网络是如何从像素中提取模式的？特征可视化通过优化来可视化高激活的模式，试图回答这个重要问题。如今，可视化方法构成了我们对神经网络内部工作的了解的基础，作为一种机械式的可解释性。在这里，我们问：特征可视化有多可靠？我们通过开发网络电路来诈骗特征可视化，使其显示完全与自然输入的正常网络行为毫无联系的任意模式。然后，我们提供证据表明在标准，未操纵网络中发生了类似的现象：特征可视化与标准输入处理非常不同，对神经网络如何处理自然图像的解释能力产生怀疑。我们通过理论证明支撑这一经验发现，由于优化过程中固有的限制，可以通过特征可视化可靠理解的功能集极其有限。

    How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
    
[^146]: 控制变量的神经符号回归

    Neural Symbolic Regression using Control Variables. (arXiv:2306.04718v1 [cs.LG])

    [http://arxiv.org/abs/2306.04718](http://arxiv.org/abs/2306.04718)

    提出了一种利用控制变量的神经符号回归方法，可以分解多变量符号回归为单变量回归，并从底部向上组合，解决了符号回归存在的精度和可扩展性问题。

    

    符号回归是一种从数据中发现分析数学表达式的强有力技术，由于其良好的结果可解释性，在自然科学中应用广泛。然而，现有的方法在处理涉及多个变量的复杂方程时面临可伸缩性问题。为了应对这一挑战，我们提出了 SRCV，一种新颖的神经符号回归方法，利用控制变量来增强精度和可扩展性。核心思想是将多变量符号回归分解为一组单变量 SR 问题，然后从底部向上组合。该方法包括四个步骤。首先，我们使用深度神经网络（DNN）从观察到的数据中学习数据生成器。其次，使用数据生成器通过控制输入变量来生成特定变量的样本。第三，应用单变量符号回归来估计相应的数学表达式。

    Symbolic regression (SR) is a powerful technique for discovering the analytical mathematical expression from data, finding various applications in natural sciences due to its good interpretability of results. However, existing methods face scalability issues when dealing with complex equations involving multiple variables. To address this challenge, we propose SRCV, a novel neural symbolic regression method that leverages control variables to enhance both accuracy and scalability. The core idea is to decompose multi-variable symbolic regression into a set of single-variable SR problems, which are then combined in a bottom-up manner. The proposed method involves a four-step process. First, we learn a data generator from observed data using deep neural networks (DNNs). Second, the data generator is used to generate samples for a certain variable by controlling the input variables. Thirdly, single-variable symbolic regression is applied to estimate the corresponding mathematical expressio
    
[^147]: 可微的地球移动距离在高亮LHC数据压缩中的应用

    Differentiable Earth Mover's Distance for Data Compression at the High-Luminosity LHC. (arXiv:2306.04712v1 [hep-ex])

    [http://arxiv.org/abs/2306.04712](http://arxiv.org/abs/2306.04712)

    本文利用可微分的快速逼近方法，训练了一个编码器神经网络用于高亮LHC数据的压缩，同时保留了数据内与粒子探测器中的能量沉积分布相关的信息。

    

    地球移动距离(EMD)是图像识别和分类的有用指标，但其通常实现不可微分或过于缓慢，无法用作通过梯度下降训练其他算法的损失函数。本文训练了一个卷积神经网络(CNN)，学习了可微分的、快速的EMD的逼近方法，并证明它可以用作计算密集的EMD实现的替代品。我们将这种可微分的逼近方法应用于用于数据压缩的类自编码神经网络(encoder NN)的训练，这些数据来自欧洲核子研究组织的高亮LHC。编码器NN的目标是在保留与粒子探测器中的能量沉积分布相关的信息的同时压缩数据。我们证明，使用可微的EMD CNN训练的编码器NN的性能超越基于平均平方误差的损失函数的训练。

    The Earth mover's distance (EMD) is a useful metric for image recognition and classification, but its usual implementations are not differentiable or too slow to be used as a loss function for training other algorithms via gradient descent. In this paper, we train a convolutional neural network (CNN) to learn a differentiable, fast approximation of the EMD and demonstrate that it can be used as a substitute for computing-intensive EMD implementations. We apply this differentiable approximation in the training of an autoencoder-inspired neural network (encoder NN) for data compression at the high-luminosity LHC at CERN. The goal of this encoder NN is to compress the data while preserving the information related to the distribution of energy deposits in particle detectors. We demonstrate that the performance of our encoder NN trained using the differentiable EMD CNN surpasses that of training with loss functions based on mean squared error.
    
[^148]: 论文标题：使用成对帧评估改进数字病理模型的统计基准测试

    Improved statistical benchmarking of digital pathology models using pairwise frames evaluation. (arXiv:2306.04709v1 [cs.CV])

    [http://arxiv.org/abs/2306.04709](http://arxiv.org/abs/2306.04709)

    该论文提出一种嵌套成对帧评估方法用于相对基准测试，通过比较候选模型和病理学家注释之间的一致性以及病理学家注释之间的一致性，解决了使用手动病理学家注释进行模型验证的基本问题。

    

    嵌套成对帧是一种相对基准测试方法，可将细胞或组织数字病理模型与手动病理学家注释的抽样补丁集进行比较。该方法比较了候选模型和病理学家注释之间的一致性以及病理学家注释之间的一致性，从而解决了使用手动病理学家注释作为模型验证的基础数据的大小和注释者变异的基本问题。我们为组织分类、细胞分类和细胞计数预测任务实现了嵌套成对帧评估，并展示了在H＆E染色黑素瘤数据集上部署的细胞和组织模型的结果。

    Nested pairwise frames is a method for relative benchmarking of cell or tissue digital pathology models against manual pathologist annotations on a set of sampled patches. At a high level, the method compares agreement between a candidate model and pathologist annotations with agreement among pathologists' annotations. This evaluation framework addresses fundamental issues of data size and annotator variability in using manual pathologist annotations as a source of ground truth for model validation. We implemented nested pairwise frames evaluation for tissue classification, cell classification, and cell count prediction tasks and show results for cell and tissue models deployed on an H&E-stained melanoma dataset.
    
[^149]: 基于图卷积神经网络的稳健可变形点云配准方法：Robust-DefReg

    Robust-DefReg: A Robust Deformable Point Cloud Registration Method based on Graph Convolutional Neural Networks. (arXiv:2306.04701v1 [cs.CV])

    [http://arxiv.org/abs/2306.04701](http://arxiv.org/abs/2306.04701)

    本文提出了一种稳健的非刚性点云配准方法Robust-DefReg，使用基于图卷积神经网络的端到端粗到细配准策略，并在解决不同难度问题上均表现出高精度和稳健性。

    

    点云配准是计算机视觉中的一个基本问题，旨在估计相应点集之间的变换。尤其是非刚性配准需要解决多种变形、噪声、异常值和数据不完整性等挑战。本文介绍了一种基于图卷积网络的稳健非刚性点云配准方法Robust-DefReg。Robust-DefReg是一个端到端的粗到细配准方法，利用粗配准和细配准方法的优点。该方法学习全局特征以找到源与目标点云之间的对应关系，实现适当的初始对准和随后的细配准。在所有挑战中同时实现高准确性和稳健性报道得更少，这成为Robust-DefReg方法的一个关键目标。该方法在大规模变形情况下实现了高精度。

    Point cloud registration is a fundamental problem in computer vision that aims to estimate the transformation between corresponding sets of points. Non-rigid registration, in particular, involves addressing challenges including various levels of deformation, noise, outliers, and data incompleteness. This paper introduces Robust-DefReg, a robust non-rigid point cloud registration method based on graph convolutional networks (GCNNs). Robust-DefReg is a coarse-to-fine registration approach within an end-to-end pipeline, leveraging the advantages of both coarse and fine methods. The method learns global features to find correspondences between source and target point clouds, to enable appropriate initial alignment, and subsequently fine registration. The simultaneous achievement of high accuracy and robustness across all challenges is reported less frequently in existing studies, making it a key objective of the Robust-DefReg method. The proposed method achieves high accuracy in large defo
    
[^150]: ConceptBed: 评估文本到图像扩散模型的概念学习能力

    ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models. (arXiv:2306.04695v1 [cs.CV])

    [http://arxiv.org/abs/2306.04695](http://arxiv.org/abs/2306.04695)

    本文提出ConceptBed数据集和评估指标CCD，用于评估文本到图像模型的概念学习和合成能力。

    

    理解视觉概念并从图像中复制和组合这些概念的能力是计算机视觉的一个核心目标。最近文本到图像（T2I）模型的进展使得通过学习大量图像及其描述来生成高清晰度和逼真的图像质量成为可能。然而，评估T2I模型的重点在于照片般的真实感和有限的视觉理解定性量度。为了量化T2I模型在学习和合成新的视觉概念方面的能力，我们引入了ConceptBed，一个包含284个独特视觉概念、5K个独特概念组合和33K个组合文本提示的大规模数据集。除了数据集，我们提出了一个评估指标Concept Confidence Deviation（CCD），它利用oracle概念分类器的置信度来衡量T2I生成器生成的概念与地面真实图像中包含的概念之间的对齐度。我们评估的视觉概念是对象或者...

    The ability to understand visual concepts and replicate and compose these concepts from images is a central goal for computer vision. Recent advances in text-to-image (T2I) models have lead to high definition and realistic image quality generation by learning from large databases of images and their descriptions. However, the evaluation of T2I models has focused on photorealism and limited qualitative measures of visual understanding. To quantify the ability of T2I models in learning and synthesizing novel visual concepts, we introduce ConceptBed, a large-scale dataset that consists of 284 unique visual concepts, 5K unique concept compositions, and 33K composite text prompts. Along with the dataset, we propose an evaluation metric, Concept Confidence Deviation (CCD), that uses the confidence of oracle concept classifiers to measure the alignment between concepts generated by T2I generators and concepts contained in ground truth images. We evaluate visual concepts that are either object
    
[^151]: 多尺度流用于鲁棒和最优宇宙学分析

    Multiscale Flow for Robust and Optimal Cosmological Analysis. (arXiv:2306.04689v1 [astro-ph.CO])

    [http://arxiv.org/abs/2306.04689](http://arxiv.org/abs/2306.04689)

    用多尺度流进行二维宇宙学数据的生成和建模，可识别不同尺度的信息并显著胜过现有方法。

    

    本文提出了多尺度流(Convolutional Normalizing Flow)用于生成二维宇宙学数据，并对之进行建模和分析。该模型通过小波基础分解宇宙学场，然后将不同级别的小波分量分别建模。通过逐项求和得出原始宇宙学场的对数可能性，从而分离不同尺度的信息并识别其中未知的尺度相关系统性。

    We propose Multiscale Flow, a generative Normalizing Flow that creates samples and models the field-level likelihood of two-dimensional cosmological data such as weak lensing. Multiscale Flow uses hierarchical decomposition of cosmological fields via a wavelet basis, and then models different wavelet components separately as Normalizing Flows. The log-likelihood of the original cosmological field can be recovered by summing over the log-likelihood of each wavelet term. This decomposition allows us to separate the information from different scales and identify distribution shifts in the data such as unknown scale-dependent systematics. The resulting likelihood analysis can not only identify these types of systematics, but can also be made optimal, in the sense that the Multiscale Flow can learn the full likelihood at the field without any dimensionality reduction. We apply Multiscale Flow to weak lensing mock datasets for cosmological inference, and show that it significantly outperform
    
[^152]: 揭示生成模型评价度量的缺陷及其不公平对待扩散模型的现象

    Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. (arXiv:2306.04675v1 [cs.LG])

    [http://arxiv.org/abs/2306.04675](http://arxiv.org/abs/2306.04675)

    系统地研究了图像生成模型的评估，发现常见的评价指标如FID等不能很好地体现扩散模型的感知真实性，建议使用SwAV特征提取器结合FID进行评估。

    

    我们系统地研究了许多种基于图像的生成模型，包括语义多样的数据集，以理解和改进用于评估它们的特征提取器和度量。使用心理物理学的最佳实践，我们进行了迄今为止最大规模的生成模型评估实验，通过对生成样本进行人类感知图像真实性的测量，发现没有任何现有的度量能与人类评估强相关。我们比较了用于评估生成模型整体性能、保真度、多样性和记忆能力的16个现代指标，发现以人类为基准的扩散模型的最先进的感知真实性不反映在常见的度量指标，如FID中。这种差异并不能通过生成样本的多样性来解释，尽管其中一个原因是过度依赖于Inception-V3。通过研究替代的自监督特征提取器，我们解决了这些缺陷，发现个别弱Downstream任务编码的语义信息最能解释图像真实性，建议在评估生成模型时使用SwAV特征提取器结合FID。

    We systematically study a wide variety of image-based generative models spanning semantically-diverse datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 16 modern metrics for evaluating the overall performance, fidelity, diversity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individu
    
[^153]: SMRVIS：用于非破坏性测试的三维超声点云提取

    SMRVIS: Point cloud extraction from 3-D ultrasound for non-destructive testing. (arXiv:2306.04668v1 [eess.IV])

    [http://arxiv.org/abs/2306.04668](http://arxiv.org/abs/2306.04668)

    提出了将超声体积点云提取作为图像分割问题的解决方案，并通过快速原型开发验证效果。

    

    我们提出了将超声体积的点云提取作为图像分割问题的解决方案。通过这个便捷的公式，我们开发了一个快速原型来探索U-Net架构的各种变体，并对其进行了评估。本报告记录了使用5个标记超声体积和84个未标记体积的训练数据集完成的实验结果，这个工作是作为一个名为“超声图像分析的深度学习”开放挑战的提交的一部分。源代码已在Github上\url{https://github.com/lisatwyw/smrvis}公开共享给研究社区。

    We propose to formulate point cloud extraction from ultrasound volumes as an image segmentation problem. Through this convenient formulation, a quick prototype exploring various variants of the U-Net architecture was developed and evaluated. This report documents the experimental results compiled using a training dataset of 5 labelled ultrasound volumes and 84 unlabelled volumes that got completed in a two-week period as part of a challenge submission to an open challenge entitled ``Deep Learning in Ultrasound Image Analysis''. Source code is shared with the research community at this GitHub URL \url{https://github.com/lisatwyw/smrvis}.
    
[^154]: 蛋白质图的神经嵌入

    Neural Embeddings for Protein Graphs. (arXiv:2306.04667v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.04667](http://arxiv.org/abs/2306.04667)

    该论文提出了一种新方法，通过学习编码器函数在几何向量空间中嵌入蛋白质图，生成具有结构和序列感知能力的蛋白质表示，实现了在比较蛋白质结构和蛋白质结构分类任务中的显着加速和卓越结果。

    

    蛋白质在生物体中扮演着非常重要的角色，因此，开发用于蛋白质表示的高效计算方法对推进大规模生物研究至关重要。目前，大多数方法难以高效集成蛋白质序列和结构中包含的大量信息。在本文中，我们提出了一种新的框架，通过学习保持蛋白质图结构距离的编码器函数，在几何向量空间中嵌入蛋白质图。利用图神经网络（GNN）和大型语言模型（LLM），提出的框架生成具有结构和序列感知能力的蛋白质表示。我们证明了我们的嵌入在比较蛋白质结构的任务中取得了成功，同时与基于结构对齐的传统方法相比，提供了显着加速。我们的框架在蛋白质结构分类任务中取得了卓越的结果；特别是，我们的方法可以在大规模蛋白质数据集上进行训练。

    Proteins perform much of the work in living organisms, and consequently the development of efficient computational methods for protein representation is essential for advancing large-scale biological research. Most current approaches struggle to efficiently integrate the wealth of information contained in the protein sequence and structure. In this paper, we propose a novel framework for embedding protein graphs in geometric vector spaces, by learning an encoder function that preserves the structural distance between protein graphs. Utilizing Graph Neural Networks (GNNs) and Large Language Models (LLMs), the proposed framework generates structure- and sequence-aware protein representations. We demonstrate that our embeddings are successful in the task of comparing protein structures, while providing a significant speed-up compared to traditional approaches based on structural alignment. Our framework achieves remarkable results in the task of protein structure classification; in partic
    
[^155]: 通过深度后验采样估计PET图像重建的不确定性

    Estimating Uncertainty in PET Image Reconstruction via Deep Posterior Sampling. (arXiv:2306.04664v1 [eess.IV])

    [http://arxiv.org/abs/2306.04664](http://arxiv.org/abs/2306.04664)

    本文提出了一种基于深度学习的方法，通过后验采样量化PET图像重建中的不确定性，该方法可以更准确地估计不确定性并提高重建质量。

    

    正电子发射断层扫描（PET）是一种重要的功能医学成像技术，常用于评估某些脑部疾病，其重建问题是无论是否实际的。PET成像中绝大多数重建方法，包括迭代方法和深度学习方法，都会返回单个估计值而不量化关联的不确定性。由于无法确定性和噪声，单个解决方案可能会误导或不准确。因此，在PET图像重建中提供不确定性的度量可以帮助医务人员做出关键决策。本文提出了一种基于深度学习的方法，通过后验采样量化PET图像重建中的不确定性。该方法基于训练一种条件生成对抗网络，其生成器近似于在贝叶斯反演的后验中进行采样。 生成器是在传统重建方法获得的低剂量PET扫描和同一受试者的高质量磁共振（MR）图像的重建的条件下进行的。该方法在模拟数据和实际数据上进行了评估，并与其他不确定性评估方法进行了比较。结果表明，与其他方法相比，所提出的方法提供了更准确的不确定性估计和更好的重建质量。

    Positron emission tomography (PET) is an important functional medical imaging technique often used in the evaluation of certain brain disorders, whose reconstruction problem is ill-posed. The vast majority of reconstruction methods in PET imaging, both iterative and deep learning, return a single estimate without quantifying the associated uncertainty. Due to ill-posedness and noise, a single solution can be misleading or inaccurate. Thus, providing a measure of uncertainty in PET image reconstruction can help medical practitioners in making critical decisions. This paper proposes a deep learning-based method for uncertainty quantification in PET image reconstruction via posterior sampling. The method is based on training a conditional generative adversarial network whose generator approximates sampling from the posterior in Bayesian inversion. The generator is conditioned on reconstruction from a low-dose PET scan obtained by a conventional reconstruction method and a high-quality mag
    
[^156]: U-PASS：一种基于不确定性引导的自动睡眠分期深度学习管道

    U-PASS: an Uncertainty-guided deep learning Pipeline for Automated Sleep Staging. (arXiv:2306.04663v1 [eess.SP])

    [http://arxiv.org/abs/2306.04663](http://arxiv.org/abs/2306.04663)

    U-PASS是一种基于不确定性引导的深度学习管道，具有估计不确定性的能力，针对临床应用进行设计。在睡眠分期中表现出极高的性能，通过优化训练数据集和积极寻找信息样本实现最先进的性能。

    

    随着机器学习在诸如医疗保健等关键领域中越来越普及，确保机器学习系统的安全性和可靠性变得至关重要。可靠性的关键组成部分是能够估计不确定性，这可以使识别自信度高和低的区域并帮助最小化错误的风险。在本研究中，我们提出了一种名为 U-PASS 的机器学习管道，该管道专为临床应用而设计，在整个过程中，包括数据采集、训练和模型部署，都结合了不确定性估计。训练过程分为监督的预训练步骤和半监督的微调步骤。我们将此不确定性引导的深度学习管道应用于具有挑战性的睡眠分期问题，并证明了它在每个阶段都系统地提高了性能。通过优化训练数据集、积极寻找信息样本并将最不确定的样本推迟给专家，我们在两个睡眠基准数据集上实现了最先进的性能，并超过了商业睡眠分期系统。

    As machine learning becomes increasingly prevalent in critical fields such as healthcare, ensuring the safety and reliability of machine learning systems becomes paramount. A key component of reliability is the ability to estimate uncertainty, which enables the identification of areas of high and low confidence and helps to minimize the risk of error. In this study, we propose a machine learning pipeline called U-PASS tailored for clinical applications that incorporates uncertainty estimation at every stage of the process, including data acquisition, training, and model deployment. The training process is divided into a supervised pre-training step and a semi-supervised finetuning step. We apply our uncertainty-guided deep learning pipeline to the challenging problem of sleep staging and demonstrate that it systematically improves performance at every stage. By optimizing the training dataset, actively seeking informative samples, and deferring the most uncertain samples to an expert, 
    
[^157]: 通过生成式人工智能理解地方认同

    Understanding Place Identity with Generative AI. (arXiv:2306.04662v1 [cs.LG])

    [http://arxiv.org/abs/2306.04662](http://arxiv.org/abs/2306.04662)

    本研究测试了生成式人工智能在捕捉城市地方认同方面的潜力，结果表明它可以生成多样化的逼真表现，但不能过度依赖，同时挖掘了它的潜力和局限性。

    

    研究人员不断利用新形式的数据，旨在了解人们如何认知建筑环境并建立城市的集体地方认同。生成式人工智能（AI）模型的最新进展使得可以从大量数据中学习并生成逼真的表现形式。本研究旨在测试生成式AI作为文本和视觉信息来源的潜力，以捕捉经过过滤的城市地方认同。我们向ChatGPT和DALL-E2两个生成式AI模型提出关于31个全球城市地方认同的问题。由于生成式AI引发了其可靠性的伦理问题，我们进行了交叉验证以检查结果是否显示出与真实城市环境相似的模式。

    Researchers are constantly leveraging new forms of data with the goal of understanding how people perceive the built environment and build the collective place identity of cities. Latest advancements in generative artificial intelligence (AI) models have enabled the production of realistic representations learned from vast amounts of data. In this study, we aim to test the potential of generative AI as the source of textual and visual information in capturing the place identity of cities assessed by filtered descriptions and images. We asked questions on the place identity of a set of 31 global cities to two generative AI models, ChatGPT and DALL-E2. Since generative AI has raised ethical concerns regarding its trustworthiness, we performed cross-validation to examine whether the results show similar patterns to real urban settings. In particular, we compared the outputs with Wikipedia data for text and images searched from Google for image. Our results indicate that generative AI mode
    
[^158]: 基于混合Actor-Critic强化学习的自适应频率绿灯最佳速度建议

    Adaptive Frequency Green Light Optimal Speed Advisory based on Hybrid Actor-Critic Reinforcement Learning. (arXiv:2306.04660v1 [cs.LG])

    [http://arxiv.org/abs/2306.04660](http://arxiv.org/abs/2306.04660)

    本文提出了一种基于混合Actor-Critic强化学习的自适应频率绿灯最佳速度建议模型，通过使用离散和连续演员网络来优化绿灯最佳速度建议系统的频率和加速度曲线，设计了新的奖励函数来平衡减少停车和最小化速度变化的权衡。实验结果显示，该模型可以有效地减少旅行时间和燃料消耗，并优于现有的最先进的GLOSA系统。

    

    绿灯最佳速度建议（GLOSA）系统建议车辆速度，以帮助它们在绿色时间通过路口，从而通过最小化在路口停车和怠速时间来减少交通拥堵和燃料消耗。但是，以前的研究集中于优化GLOSA算法，忽略了GLOSA系统的速度建议频率。具体而言，一些研究在每个决策步骤提供速度建议，导致冗余建议，而其他人仅为车辆计算最佳速度，无法适应动态交通。在本文中，我们提出了一种基于混合PPO（H-PPO）的自适应频率GLOSA（AF-GLOSA）模型，其采用了一个actor-critic架构和一个混合actor网络。混合演员网络包括一个离散演员，输出咨询频率和一个连续演员，输出加速度曲线。此外，我们设计了一种新的奖励函数来平衡减少停车和最小化速度变化的权衡。实验结果表明，所提出的AF-GLOSA模型可以有效地减少旅行时间和燃料消耗，并优于现有的最先进的GLOSA系统。

    Green Light Optimal Speed Advisory (GLOSA) system suggests speeds to vehicles to assist them in passing through intersections during green intervals, thus reducing traffic congestion and fuel consumption by minimizing the number of stops and idle times at intersections. However, previous research has focused on optimizing the GLOSA algorithm, neglecting the frequency of speed advisory by the GLOSA system. Specifically, some studies provide speed advisory profile at each decision step, resulting in redundant advisory, while others calculate the optimal speed for the vehicle only once, which cannot adapt to dynamic traffic. In this paper, we propose an Adaptive Frequency GLOSA (AF-GLOSA) model based on Hybrid Proximal Policy Optimization (H-PPO), which employs an actor-critic architecture with a hybrid actor network. The hybrid actor network consists of a discrete actor that outputs advisory frequency and a continuous actor that outputs acceleration profiles. Additionally, we design a no
    
[^159]: 数学辅助下的定向进化与蛋白质工程

    Mathematics-assisted directed evolution and protein engineering. (arXiv:2306.04658v1 [q-bio.BM])

    [http://arxiv.org/abs/2306.04658](http://arxiv.org/abs/2306.04658)

    这篇论文介绍了定向进化在蛋白质工程中的应用，机器学习和拓扑数据分析方法对其实现起到了关键作用。

    

    定向进化是一种改变蛋白质工程的分子生物学技术，可以通过创建具有期望特性和功能的蛋白质来实现。然而，由于巨大的突变空间，即突变空间随氨基酸数N呈$20^N$的指数增长，无法进行整个蛋白质库的深度突变扫描。这导致AI辅助的定向进化（AIDE）或AI辅助的蛋白质工程（AIPE）作为新兴研究领域的快速增长。借助先进的自然语言处理技术，包括长短期记忆、自编码器和Transformer，基于序列的嵌入已成为AIDE和AIPE中的主流方法。持久拉普拉斯人，在拓扑数据分析（TDA）中的一种新兴技术，为基于结构的嵌入提供了一个出色的选择。我们认为，一类持久拓扑拉普拉斯人（PTLs），包括持久拉普拉斯人、持久路径拉普拉斯人、持久量子化拉普拉斯人，可以推进基于拓扑和代数的AIDE和AIPE中的数学方法学习。

    Directed evolution is a molecular biology technique that is transforming protein engineering by creating proteins with desirable properties and functions. However, it is experimentally impossible to perform the deep mutational scanning of the entire protein library due to the enormous mutational space, which scales as $20^N$ , where N is the number of amino acids. This has led to the rapid growth of AI-assisted directed evolution (AIDE) or AI-assisted protein engineering (AIPE) as an emerging research field. Aided with advanced natural language processing (NLP) techniques, including long short-term memory, autoencoder, and transformer, sequence-based embeddings have been dominant approaches in AIDE and AIPE. Persistent Laplacians, an emerging technique in topological data analysis (TDA), have made structure-based embeddings a superb option in AIDE and AIPE. We argue that a class of persistent topological Laplacians (PTLs), including persistent Laplacians, persistent path Laplacians, pe
    
[^160]: 利用分辨率转换频谱图的深度学习进行调制分类

    Modulation Classification Through Deep Learning Using Resolution Transformed Spectrograms. (arXiv:2306.04655v1 [eess.SP])

    [http://arxiv.org/abs/2306.04655](http://arxiv.org/abs/2306.04655)

    本文提出了通过生成不同调制类型的频谱图像，利用深度学习自动进行调制分类的方案，并通过对频谱图进行分辨率转换来提高转换速度和减少计算负载。

    

    调制分类是信号处理中的一个重要步骤，在通信领域得到广泛应用。由于频率随时间变化对于不同调制格式的无线电信号仍然是一个重要的区别，因此可以通过将1-D无线电信号转换成频域来进行特征提取。本文提出了一种通过生成十一种不同调制类型的频谱图像，利用卷积神经网络（CNN）的现代架构来进行自动调制分类（AMC）的方案。此外，我们对频谱图进行分辨率转换，导致计算负载减少了多达99.61％，从接收到的I/Q数据转换速度加快了8倍。该AMC方案在CPU和GPU上实现，用于识别数字和模拟信号调制方案。性能评估基于现有的CNN模型，包括SqueezeNet、Resnet-50和InceptionR。

    Modulation classification is an essential step of signal processing and has been regularly applied in the field of tele-communication. Since variations of frequency with respect to time remains a vital distinction among radio signals having different modulation formats, these variations can be used for feature extraction by converting 1-D radio signals into frequency domain. In this paper, we propose a scheme for Automatic Modulation Classification (AMC) using modern architectures of Convolutional Neural Networks (CNN), through generating spectrum images of eleven different modulation types. Additionally, we perform resolution transformation of spectrograms that results up to 99.61% of computational load reduction and 8x faster conversion from the received I/Q data. This proposed AMC is implemented on CPU and GPU, to recognize digital as well as analogue signal modulation schemes on signals. The performance is evaluated on existing CNN models including SqueezeNet, Resnet-50, InceptionR
    
[^161]: 从数据到行动：探索智能城市的人工智能和物联网驱动解决方案

    From Data to Action: Exploring AI and IoT-driven Solutions for Smarter Cities. (arXiv:2306.04653v1 [cs.LG])

    [http://arxiv.org/abs/2306.04653](http://arxiv.org/abs/2306.04653)

    本文介绍了一种智能城市管理系统，利用人工智能和物联网技术来分析交通、能源消耗和城市设施维护等数据，提供可行的解决方案，以增强城市的安全性、能源效率和可持续性。

    

    智能城市的出现需要利用先进技术，如物联网（IoT）和人工智能（AI），并承诺释放城市潜力，使其更具可持续性、效率和居民友好性。本文介绍了一种智能城市管理系统，提供基于数据的方法来处理三个用例：（i）分析交通信息，以减少交通事故风险并改善驾驶员和行人的安全性，（ii）确定何时何地可以减少能源消耗以提高成本节省，以及（iii）检测城市道路和人行道上的维护问题，以及洪水和火灾等灾害的开始。在阿维罗市的案例研究中，展示了该系统在生成可操作的见解方面的有效性，增强了安全性、能源效率和可持续性，并突出了AI和IoT驱动的智能城市发展解决方案的潜力。

    The emergence of smart cities demands harnessing advanced technologies like the Internet of Things (IoT) and Artificial Intelligence (AI) and promises to unlock cities' potential to become more sustainable, efficient, and ultimately livable for their inhabitants. This work introduces an intelligent city management system that provides a data-driven approach to three use cases: (i) analyze traffic information to reduce the risk of traffic collisions and improve driver and pedestrian safety, (ii) identify when and where energy consumption can be reduced to improve cost savings, and (iii) detect maintenance issues like potholes in the city's roads and sidewalks, as well as the beginning of hazards like floods and fires. A case study in Aveiro City demonstrates the system's effectiveness in generating actionable insights that enhance security, energy efficiency, and sustainability, while highlighting the potential of AI and IoT-driven solutions for smart city development.
    
[^162]: 论训练本地自适应的排名预测

    On training locally adaptive CP. (arXiv:2306.04648v1 [cs.LG])

    [http://arxiv.org/abs/2306.04648](http://arxiv.org/abs/2306.04648)

    本文提出了一种新的Conformal Prediction方法，使用可训练的变量变换重新定义符合度量，使得预测区间在保持边际有效的同时具有对象属性相关的大小。通过训练可最大化间隔效率。

    

    本文解决了使符合性预测（CP）间隔本地自适应的问题。大多数现有方法集中于通过划分或重新加权校准集来近似间隔的对象条件有效性。我们的策略是新的且概念上不同。我们不是重新加权校准数据，而是通过可训练的变量变换$A \to \phi_X(A)$重新定义符合度量，该变换明确地取决于对象属性$X$。在某些条件下，如果$\phi_X$对于任何$X$在$A$中是单调的，则变换将生成保证是边际有效且具有$X$相关大小的预测区间。我们描述了如何对$\phi_X$进行参数化和训练以最大化间隔效率。与其他CP-aware训练方法相反，目标函数是平滑的，可以通过标准梯度方法进行最小化而无需进行逼近。

    We address the problem of making Conformal Prediction (CP) intervals locally adaptive. Most existing methods focus on approximating the object-conditional validity of the intervals by partitioning or re-weighting the calibration set. Our strategy is new and conceptually different. Instead of re-weighting the calibration data, we redefine the conformity measure through a trainable change of variables, $A \to \phi_X(A)$, that depends explicitly on the object attributes, $X$. Under certain conditions and if $\phi_X$ is monotonic in $A$ for any $X$, the transformations produce prediction intervals that are guaranteed to be marginally valid and have $X$-dependent sizes. We describe how to parameterize and train $\phi_X$ to maximize the interval efficiency. Contrary to other CP-aware training methods, the objective function is smooth and can be minimized through standard gradient methods without approximations.
    
[^163]: 压缩感知：离散优化方法

    Compressed Sensing: A Discrete Optimization Approach. (arXiv:2306.04647v1 [eess.SP])

    [http://arxiv.org/abs/2306.04647](http://arxiv.org/abs/2306.04647)

    本文中提出了一种离散优化方法来解决压缩感知问题，该方法在二次锥松弛下，可以找到最稀疏的向量，得到了可靠的最优解。

    

    本文研究了压缩感知问题，即找到最稀疏的向量，该向量满足一组线性测量，同时达到一定的数值容限。压缩感知是统计学、运筹学和机器学习中的核心问题，应用于信号处理、数据压缩和图像重建等领域。我们引入了一个带有$\ell_2$正则化的压缩感知问题，将其作为混合整数二次锥规划来重新定义。我们推导出此问题的二次锥松弛，并展示了在正则化参数的温和限制下，得到的松弛等价于深入研究的基础追踪去噪问题。我们提出了一个半定松弛来加强二次锥松弛，开发了一种定制的分支定界算法，利用我们的二次锥松弛来解决压缩感知问题的实例，以确证的最优解。我们的数值结果表明，我们的方法产生的解决方案是精确的，并且优于其他方法。

    We study the Compressed Sensing (CS) problem, which is the problem of finding the most sparse vector that satisfies a set of linear measurements up to some numerical tolerance. CS is a central problem in Statistics, Operations Research and Machine Learning which arises in applications such as signal processing, data compression and image reconstruction. We introduce an $\ell_2$ regularized formulation of CS which we reformulate as a mixed integer second order cone program. We derive a second order cone relaxation of this problem and show that under mild conditions on the regularization parameter, the resulting relaxation is equivalent to the well studied basis pursuit denoising problem. We present a semidefinite relaxation that strengthens the second order cone relaxation and develop a custom branch-and-bound algorithm that leverages our second order cone relaxation to solve instances of CS to certifiable optimality. Our numerical results show that our approach produces solutions that 
    
[^164]: 利用简单的空间感知技术，在GEOGLAM的EO数据上改进哈萨克斯坦的州级小麦产量预测。

    Improve State-Level Wheat Yield Forecasts in Kazakhstan on GEOGLAM's EO Data by Leveraging A Simple Spatial-Aware Technique. (arXiv:2306.04646v1 [cs.LG])

    [http://arxiv.org/abs/2306.04646](http://arxiv.org/abs/2306.04646)

    本文提出了一种称为州级加性偏差的简单技术，可显著提高使用遥感数据进行小麦产量预测的机器学习性能，尤其是解决哈萨克斯坦州级产量异质性问题，将RMSE降低了8.9\% ~ 28.37\%。

    

    精准的产量预测对于制定食品安全的知情政策和长期决策至关重要。遥感数据和机器学习算法在提供从田间到国家尺度的作物状态的全面和及时视图方面发挥着关键作用。然而，机器学习算法的预测精度常常受到空间异质性的影响，这是由于遥感数据中没有反映的外在因素，如作物管理策略的差异性。在本文中，我们提出并研究了一种简单的技术，称为州级加性偏差，以明确解决哈萨克斯坦的跨地区产量异质性。与基线机器学习模型（随机森林、CatBoost、XGBoost）相比，我们的方法将总体RMSE降低了8.9\%，将最高州级RMSE降低了28.37\%。州级加性偏差的有效性表明，通过明确解决使用遥感数据进行作物产量预测的空间异质性，可以显著提高机器学习的性能。

    Accurate yield forecasting is essential for making informed policies and long-term decisions for food security. Earth Observation (EO) data and machine learning algorithms play a key role in providing a comprehensive and timely view of crop conditions from field to national scales. However, machine learning algorithms' prediction accuracy is often harmed by spatial heterogeneity caused by exogenous factors not reflected in remote sensing data, such as differences in crop management strategies. In this paper, we propose and investigate a simple technique called state-wise additive bias to explicitly address the cross-region yield heterogeneity in Kazakhstan. Compared to baseline machine learning models (Random Forest, CatBoost, XGBoost), our method reduces the overall RMSE by 8.9\% and the highest state-wise RMSE by 28.37\%. The effectiveness of state-wise additive bias indicates machine learning's performance can be significantly improved by explicitly addressing the spatial heterogene
    
[^165]: 特别会议：DNN加速器的逼近和容错能力

    Special Session: Approximation and Fault Resiliency of DNN Accelerators. (arXiv:2306.04645v1 [cs.LG])

    [http://arxiv.org/abs/2306.04645](http://arxiv.org/abs/2306.04645)

    本文研究了DNN加速器的逼近和容错能力。使用AxC算术电路敏捷地模拟硬件上的错误，而不需要在DNN上执行故障注入。提出了一种基于GPU的快速评估框架，并通过检查故障传播和掩蔽，进行了精细的容错分析。

    

    深度学习，特别是深度神经网络（DNN），现今在许多场合广泛使用，包括自动驾驶等安全关键应用。在这种情况下，除了能源效率和性能外，可靠性也扮演着至关重要的角色，因为系统故障可能会危及人类生命。作为任何其他设备一样，运行DNN的硬件架构的可靠性必须经过评估，通常是通过昂贵的故障注入活动。本文研究了DNN加速器的逼近和容错能力。我们建议使用近似（AxC）算术电路，以在硬件上敏捷地模拟错误，而无需在DNN上执行故障注入。为了允许快速评估AxC DNN，我们开发了一个高效的基于GPU的模拟框架。此外，我们通过检查网络中的故障传播和掩蔽，提出了一种精细的容错分析。

    Deep Learning, and in particular, Deep Neural Network (DNN) is nowadays widely used in many scenarios, including safety-critical applications such as autonomous driving. In this context, besides energy efficiency and performance, reliability plays a crucial role since a system failure can jeopardize human life. As with any other device, the reliability of hardware architectures running DNNs has to be evaluated, usually through costly fault injection campaigns. This paper explores the approximation and fault resiliency of DNN accelerators. We propose to use approximate (AxC) arithmetic circuits to agilely emulate errors in hardware without performing fault injection on the DNN. To allow fast evaluation of AxC DNN, we developed an efficient GPU-based simulation framework. Further, we propose a fine-grain analysis of fault resiliency by examining fault propagation and masking in networks
    
[^166]: DiffusionShield：一种针对生成式扩散模型的版权保护数字水印方法

    DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models. (arXiv:2306.04642v1 [cs.CR])

    [http://arxiv.org/abs/2306.04642](http://arxiv.org/abs/2306.04642)

    本文提出了一种针对生成式扩散模型的版权保护数字水印方法DiffusionShield，保护图像免受侵权，简单易学，难以检测，可广泛应用于各行各业。

    

    近来，生成式扩散模型(GDMs)在学习和生成图像方面展示了其卓越的能力。大量的GDMs社区自然而然地出现，并促进了GDMs在各个领域的多样化应用。然而，这种无限制的扩散引起了有关版权保护的严重关注。例如，艺术家(包括画家和摄影师)越来越担心GDMs可以毫不费力地复制他们独特的创意作品而无需授权。为了应对这些挑战，我们介绍了一种针对GDMs量身定制的新型水印方案——DiffusionShield。通过将所有权信息编码成一个不可察觉的水印并将其注入图像中，DiffusionShield保护图像免受GDMs侵权。它的水印可以被GDMs轻松地学习并重现在它们生成的图像中。通过从生成的图像中检测水印，可以揭露版权侵权行为。

    Recently, Generative Diffusion Models (GDMs) have showcased their remarkable capabilities in learning and generating images. A large community of GDMs has naturally emerged, further promoting the diversified applications of GDMs in various fields. However, this unrestricted proliferation has raised serious concerns about copyright protection. For example, artists including painters and photographers are becoming increasingly concerned that GDMs could effortlessly replicate their unique creative works without authorization. In response to these challenges, we introduce a novel watermarking scheme, DiffusionShield, tailored for GDMs. DiffusionShield protects images from copyright infringement by GDMs through encoding the ownership information into an imperceptible watermark and injecting it into the images. Its watermark can be easily learned by GDMs and will be reproduced in their generated images. By detecting the watermark from generated images, copyright infringement can be exposed w
    
[^167]: 多样化和区分化表示学习的通用低资源活动识别

    Generalizable Low-Resource Activity Recognition with Diverse and Discriminative Representation Learning. (arXiv:2306.04641v1 [cs.CV])

    [http://arxiv.org/abs/2306.04641](http://arxiv.org/abs/2306.04641)

    提出了一种新方法DDLearn，通过构建自监督学习任务，在考虑多样化和区分化学习的基础上提高通用低资源人类活动识别的性能。

    

    人类活动识别是一项时间序列分类任务，重点是从人类传感器读数中识别动作模式。然而，获取充足的数据是训练通用人类活动识别模型的关键难点，这有助于在线网络应用程序的定制和优化。本文提出了一种新的方法DDLearn，通过构建自监督学习任务，同时考虑差异性和歧视性学习，从而实现了通用低资源人类活动识别。

    Human activity recognition (HAR) is a time series classification task that focuses on identifying the motion patterns from human sensor readings. Adequate data is essential but a major bottleneck for training a generalizable HAR model, which assists customization and optimization of online web applications. However, it is costly in time and economy to collect large-scale labeled data in reality, i.e., the low-resource challenge. Meanwhile, data collected from different persons have distribution shifts due to different living habits, body shapes, age groups, etc. The low-resource and distribution shift challenges are detrimental to HAR when applying the trained model to new unseen subjects. In this paper, we propose a novel approach called Diverse and Discriminative representation Learning (DDLearn) for generalizable low-resource HAR. DDLearn simultaneously considers diversity and discrimination learning. With the constructed self-supervised learning task, DDLearn enlarges the data dive
    
[^168]: 忠实知识蒸馏

    Faithful Knowledge Distillation. (arXiv:2306.04431v1 [cs.LG])

    [http://arxiv.org/abs/2306.04431](http://arxiv.org/abs/2306.04431)

    本文研究了知识蒸馏中教师和学生之间的相对校准问题，提出了一个忠实的模仿框架来解决学生置信度和软标签的问题，并提供了一种实证和认证的方法来评估学生模型的鲁棒性。

    

    知识蒸馏是一种压缩神经网络使其能够在资源受限的系统中部署的成功方法，但过去的研究忽略了教师与学生之间在软置信度方面的相对校准问题。本文聚焦于一个教师-学生对中两个关键问题：（i）教师和学生是否在接近正确分类的数据样本时存在分歧，（ii）在数据样本周围，经过蒸馏的学生是否像教师一样自信。这些都是在安全关键环境中考虑从鲁棒教师中训练较小学生网络的部署时非常关键的问题。为了解决这些问题，我们引入了一个忠实的模仿框架来讨论置信度的相对校准，并提供实证和认证方法来评估学生的训练。

    Knowledge distillation (KD) has received much attention due to its success in compressing networks to allow for their deployment in resource-constrained systems. While the problem of adversarial robustness has been studied before in the KD setting, previous works overlook what we term the relative calibration of the student network with respect to its teacher in terms of soft confidences. In particular, we focus on two crucial questions with regard to a teacher-student pair: (i) do the teacher and student disagree at points close to correctly classified dataset examples, and (ii) is the distilled student as confident as the teacher around dataset examples? These are critical questions when considering the deployment of a smaller student network trained from a robust teacher within a safety-critical setting. To address these questions, we introduce a faithful imitation framework to discuss the relative calibration of confidences, as well as provide empirical and certified methods to eva
    
[^169]: 基于停车场占用检测的深度学习方法的改进

    Revising deep learning methods in parking lot occupancy detection. (arXiv:2306.04288v1 [cs.LG])

    [http://arxiv.org/abs/2306.04288](http://arxiv.org/abs/2306.04288)

    本文提出了一种基于EfficientNet架构的停车位占用检测算法，并在5个不同的数据集上进行了评估，性能得到提高。

    

    停车场引导系统作为智能城市发展的一部分已经成为一个流行的趋势。这些系统的关键部分是允许驾驶员在感兴趣的区域搜索可用停车位的算法。传统的方法是基于神经网络分类器应用于摄像头记录。然而，现有的系统在特定的视觉条件下缺乏泛化能力和适当的测试。在本研究中，我们广泛评估了最先进的停车位占用检测算法，将它们的预测质量与最近出现的视觉Transformer进行比较，并基于EfficientNet架构提出了一个新的管道。进行的计算实验已经证明，在我们的模型的情况下，性能有了提高，该模型在5种不同数据集上进行了评估。

    Parking guidance systems have recently become a popular trend as a part of the smart cities' paradigm of development. The crucial part of such systems is the algorithm allowing drivers to search for available parking lots across regions of interest. The classic approach to this task is based on the application of neural network classifiers to camera records. However, existing systems demonstrate a lack of generalization ability and appropriate testing regarding specific visual conditions. In this study, we extensively evaluate state-of-the-art parking lot occupancy detection algorithms, compare their prediction quality with the recently emerged vision transformers, and propose a new pipeline based on EfficientNet architecture. Performed computational experiments have demonstrated the performance increase in the case of our model, which was evaluated on 5 different datasets.
    
[^170]: 基于贝叶斯优化的自适应加权期望改进方法

    Self-Adjusting Weighted Expected Improvement for Bayesian Optimization. (arXiv:2306.04262v1 [cs.LG])

    [http://arxiv.org/abs/2306.04262](http://arxiv.org/abs/2306.04262)

    本文提出了一种新的自适应加权期望改进方法（SAWEI），可以自动平衡探索不确定区域和利用有承诺区域之间的权衡。在COCO基准测试中，该方法表现出有利的性能。

    

    贝叶斯优化（BO）是一种基于代理模型，对小型评估预算的黑箱问题进行优化的高效算法类。BO管道本身高度可配置，涉及初始设计、代理模型和获取功能（AF）的许多不同设计选择。然而，我们对如何为手头问题选择合适的组件的理解非常有限。在本文中，我们的重点是AF的定义，其主要目的是平衡对高不确定性区域和对好解决方案有高承诺的区域之间的探索和利用的权衡。我们提出了自适应加权期望改进方法（SAWEI），其中我们让探索 - 利用权衡以数据驱动的方式进行自适应，基于BO的收敛准则。在COCO基准平台的无噪声黑箱BBOB函数上，我们的方法相对于手工制作的基线表现出有利的任何时间性能，并且是一个稳健的SP。

    Bayesian Optimization (BO) is a class of surrogate-based, sample-efficient algorithms for optimizing black-box problems with small evaluation budgets. The BO pipeline itself is highly configurable with many different design choices regarding the initial design, surrogate model, and acquisition function (AF). Unfortunately, our understanding of how to select suitable components for a problem at hand is very limited. In this work, we focus on the definition of the AF, whose main purpose is to balance the trade-off between exploring regions with high uncertainty and those with high promise for good solutions. We propose Self-Adjusting Weighted Expected Improvement (SAWEI), where we let the exploration-exploitation trade-off self-adjust in a data-driven manner, based on a convergence criterion for BO. On the noise-free black-box BBOB functions of the COCO benchmarking platform, our method exhibits a favorable any-time performance compared to handcrafted baselines and serves as a robust def
    
[^171]: 基于神经网络动力学的对抗样本检测方法

    Adversarial Sample Detection Through Neural Network Transport Dynamics. (arXiv:2306.04252v1 [cs.LG])

    [http://arxiv.org/abs/2306.04252](http://arxiv.org/abs/2306.04252)

    本文提出基于神经网络动力学的对抗样本检测器，并通过规范化向量场的训练方法来提高其干净输入和异常输入的区分度。实验结果表明，该方法在不同攻击下的表现优于其他检测器，同时也提高了对抗检测器的测试准确率。

    

    本文提出了一种基于神经网络离散动态系统的对抗样本检测器。通过比较输入通过层遵循的离散向量场，该检测器可以将干净输入从异常输入中区分出来。我们还展示了在训练过程中对该向量场进行规范化可以使网络在数据分布的支持上更加规则化，从而使干净输入的激活更容易与异常输入的激活区分开来。在实验中，我们将我们的检测器与其他检测器进行了比较，并展示了规范化网络动态可以提高将内部嵌入用作输入的对抗检测器的性能，同时也可以提高测试准确率。

    We propose a detector of adversarial samples that is based on the view of neural networks as discrete dynamic systems. The detector tells clean inputs from abnormal ones by comparing the discrete vector fields they follow through the layers. We also show that regularizing this vector field during training makes the network more regular on the data distribution's support, thus making the activations of clean inputs more distinguishable from those of abnormal ones. Experimentally, we compare our detector favorably to other detectors on seen and unseen attacks, and show that the regularization of the network's dynamics improves the performance of adversarial detectors that use the internal embeddings as inputs, while also improving test accuracy.
    
[^172]: 在表面之下寻找：利用基本对称性实现高效离线强化学习

    Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])

    [http://arxiv.org/abs/2306.04220](http://arxiv.org/abs/2306.04220)

    本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。

    

    离线强化学习通过从预先收集的数据集中学习策略来解决与环境交互的实际问题。然而，现有的离线强化学习算法的性能严重依赖于数据集的规模和状态-动作空间覆盖范围。真实世界数据的收集通常是昂贵和难以控制的，导致数据集小且覆盖范围狭窄，从而对离线强化学习的实际部署提出了重大挑战。在本文中，我们提供了一个新的见解，即利用系统动力学的基本对称性可以在小数据集下显著提高离线强化学习的性能。具体来说，我们提出了一个时间反演对称(T-symmetry)强制的动力学模型(TDM)，建立了一对正向和反向潜在动力学之间的一致性。TDM为小数据集提供了良好的表示，并基于T-symmetry的符合性提供了一种新的OOD样本的可靠性度量。

    Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
    
[^173]: 使用反事实预测集设计决策支持系统

    Designing Decision Support Systems Using Counterfactual Prediction Sets. (arXiv:2306.03928v1 [cs.LG])

    [http://arxiv.org/abs/2306.03928](http://arxiv.org/abs/2306.03928)

    本文提出了一种基于反事实预测集的决策支持系统设计方法，不同于传统的单一标签预测，它使用符合预测器构建预测集，并引导人类专家从中选择标签值。

    

    分类任务的决策支持系统通常被设计用于预测地面实况标签的值。然而，由于它们的预测并不完美，这些系统还需要让人类专家了解何时以及如何使用这些预测来更新自己的预测。不幸的是，这被证明是具有挑战性的。最近有人认为，另一种类型的决策支持系统可能会避开这个挑战。这些系统不是提供单个标签预测，而是使用符合预测器构建一组标签预测值，即预测集，并强制要求专家从预测集中预测一个标签值。然而，这些系统的设计和评估迄今仍依赖于样式化的专家模型，这引发了人们对它们的承诺的质疑。本文从在线学习的角度重新审视了这种系统的设计，并开发了一种不需要。

    Decision support systems for classification tasks are predominantly designed to predict the value of the ground truth labels. However, since their predictions are not perfect, these systems also need to make human experts understand when and how to use these predictions to update their own predictions. Unfortunately, this has been proven challenging. In this context, it has been recently argued that an alternative type of decision support systems may circumvent this challenge. Rather than providing a single label prediction, these systems provide a set of label prediction values constructed using a conformal predictor, namely a prediction set, and forcefully ask experts to predict a label value from the prediction set. However, the design and evaluation of these systems have so far relied on stylized expert models, questioning their promise. In this paper, we revisit the design of this type of systems from the perspective of online learning and develop a methodology that does not requi
    
[^174]: 带有分层变分自编码器的情感条件旋律和声编配研究

    Emotion-Conditioned Melody Harmonization with Hierarchical Variational Autoencoder. (arXiv:2306.03718v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2306.03718](http://arxiv.org/abs/2306.03718)

    提出了一种基于LSTM的分层变分自编码器(LHVAE)，研究情感条件对旋律和声编配的影响，同时提高了生成的和声质量并捕捉了丰富的和弦进行变化。实验证明，该模型在感知情感表达方面优于现有方法。

    

    现有的旋律和声编配模型在提高生成的和声质量方面取得了很大进展，但大多数忽略了音乐中的情感。同时，以前的方法所生成的和声变化性不足。为了解决这些问题，我们提出了一种新颖的基于LSTM的分层变分自编码器(LHVAE)，研究情感条件对旋律和声编配的影响，同时提高了生成的和声质量并捕捉了丰富的和弦进行变化。具体来说，LHVAE在不同层次（乐章和小节级别）上融合了潜在变量和情感条件，以建模全局和局部音乐属性。另外，我们在每个步骤中引入了基于注意力的旋律上下文向量，以更好地学习旋律和和声之间的对应关系。客观评估的实验结果表明，我们提出的模型胜过了其他基于LSTM的模型。通过主观评估，我们证明了我们的模型可以生成符合给定情感的和声进行，并在感知情感表达方面优于现有方法。

    Existing melody harmonization models have made great progress in improving the quality of generated harmonies, but most of them ignored the emotions beneath the music. Meanwhile, the variability of harmonies generated by previous methods is insufficient. To solve these problems, we propose a novel LSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the influence of emotional conditions on melody harmonization, while improving the quality of generated harmonies and capturing the abundant variability of chord progressions. Specifically, LHVAE incorporates latent variables and emotional conditions at different levels (piece- and bar-level) to model the global and local music properties. Additionally, we introduce an attention-based melody context vector at each step to better learn the correspondence between melodies and harmonies. Experimental results of the objective evaluation show that our proposed model outperforms other LSTM-based models. Through subjective evalu
    
[^175]: 通过调整输入空间进行预训练骨干网络的持续学习

    Continual Learning with Pretrained Backbones by Tuning in the Input Space. (arXiv:2306.02947v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02947](http://arxiv.org/abs/2306.02947)

    本论文提出了一种新颖的策略，使得通过微调不仅可以避免更新预训练部分从而损失宝贵的知识，并且还可以学习一组新的可学习参数将输入数据转换为有效信息。

    

    将深度学习模型适应非稳态环境的内在困难限制了神经网络在实际任务中的应用。本文提出了一种新的策略，通过避免更新网络的预训练部分并学习不仅是通常的分类头，而且还有一组新引入的可以学习参数，这些参数负责转换输入数据。这个过程允许网络有效地利用已学习的知识，并使微调过程更为有效。

    The intrinsic difficulty in adapting deep learning models to non-stationary environments limits the applicability of neural networks to real-world tasks. This issue is critical in practical supervised learning settings, such as the ones in which a pre-trained model computes projections toward a latent space where different task predictors are sequentially learned over time. As a matter of fact, incrementally fine-tuning the whole model to better adapt to new tasks usually results in catastrophic forgetting, with decreasing performance over the past experiences and losing valuable knowledge from the pre-training stage. In this paper, we propose a novel strategy to make the fine-tuning procedure more effective, by avoiding to update the pre-trained part of the network and learning not only the usual classification head, but also a set of newly-introduced learnable parameters that are responsible for transforming the input data. This process allows the network to effectively leverage the 
    
[^176]: 信息传递选择：面向图分类的可解释GNN的研究

    Message-passing selection: Towards interpretable GNNs for graph classification. (arXiv:2306.02081v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02081](http://arxiv.org/abs/2306.02081)

    本文提出了一种可解释的GNN推理范式MSInterpreter，其中包括消息传递选择方案(MSScheme)，通过计算由结构和节点嵌入组成的消息聚合路径的权重因子，实现对GNN自我解释。在图分类基准测试中表明其有效性。

    

    本文致力于开发一个可解释的GNN推理范式，称为MSInterpreter，它可以作为即插即用的方案轻松应用于各种GNN基线。与大多数现有的解释方法不同，MSInterpreter提供了一种消息传递选择方案(MSScheme)，以选择GNN的关键路径进行消息聚合，旨在实现自我解释而不是事后解释。具体而言，精心设计的MSScheme的目的是通过考虑香草结构和节点嵌入组件来计算消息聚合路径的权重因子，其中结构基旨在权衡节点引起的子结构之间的权重因子；另一方面，节点嵌入基着眼于通过单层GNN获得的节点嵌入来计算权重因子。最后，我们在图分类基准测试中展示了这种方法的有效性。

    In this paper, we strive to develop an interpretable GNNs' inference paradigm, termed MSInterpreter, which can serve as a plug-and-play scheme readily applicable to various GNNs' baselines. Unlike the most existing explanation methods, MSInterpreter provides a Message-passing Selection scheme(MSScheme) to select the critical paths for GNNs' message aggregations, which aims at reaching the self-explaination instead of post-hoc explanations. In detail, the elaborate MSScheme is designed to calculate weight factors of message aggregation paths by considering the vanilla structure and node embedding components, where the structure base aims at weight factors among node-induced substructures; on the other hand, the node embedding base focuses on weight factors via node embeddings obtained by one-layer GNN.Finally, we demonstrate the effectiveness of our approach on graph classification benchmarks.
    
[^177]: SGEM：通过序列级广义熵最小化实现自动语音识别的测试时自适应

    SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization. (arXiv:2306.01981v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2306.01981](http://arxiv.org/abs/2306.01981)

    SGEM提出了一种新的测试时自适应框架，利用波束搜索和广义熵最小化调整预训练ASR模型，取得了三个主流ASR模型在不同领域转变时的最新性能。

    

    在许多实际情况下，自动语音识别（ASR）模型经常暴露于数据分布的变化，导致错误的预测。为了解决这个问题，最近提出了一种现有的测试时自适应（TTA）方法，可以在没有源数据的情况下调整预训练的ASR模型以适应未标记的测试实例。尽管有了不错的性能提升，但这项工作仅依赖于简单的贪心解码，并在帧级别上跨越时间步长进行调整，这在模型输出的序列性质下可能不是最优的。出于这个动机，我们提出了一个新的TTA框架，称为SGEM，用于一般ASR模型。为了处理序列输出，SGEM首先利用波束搜索来探索候选输出标志，并选择最可信的标志。然后，它利用广义熵最小化和负抽样作为无监督目标来适应模型。在各种领域的转变下，SGEM实现了三种主流ASR模型的最新性能。

    Automatic speech recognition (ASR) models are frequently exposed to data distribution shifts in many real-world scenarios, leading to erroneous predictions. To tackle this issue, an existing test-time adaptation (TTA) method has recently been proposed to adapt the pre-trained ASR model on unlabeled test instances without source data. Despite decent performance gain, this work relies solely on naive greedy decoding and performs adaptation across timesteps at a frame level, which may not be optimal given the sequential nature of the model output. Motivated by this, we propose a novel TTA framework, dubbed SGEM, for general ASR models. To treat the sequential output, SGEM first exploits beam search to explore candidate output logits and selects the most plausible one. Then, it utilizes generalized entropy minimization and negative sampling as unsupervised objectives to adapt the model. SGEM achieves state-of-the-art performance for three mainstream ASR models under various domain shifts.
    
[^178]: 机器学习流程的负责任设计模式

    Responsible Design Patterns for Machine Learning Pipelines. (arXiv:2306.01788v1 [cs.SE])

    [http://arxiv.org/abs/2306.01788](http://arxiv.org/abs/2306.01788)

    本文提出了一种综合框架，将负责任设计模式纳入机器学习流程中，以确保AI系统的伦理性和公正性。这个框架包括新的负责任AI设计模式，并指导AI开发人员、数据科学家和决策者在AI开发和部署中实施伦理实践。

    

    将道德实践整合到人工智能(AI)开发过程中对于确保AI的安全、公平和负责任操作至关重要。AI伦理涉及将伦理原则应用于AI系统的整个生命周期。这对于减轻与AI相关的潜在风险和伤害（如算法偏见）至关重要。为实现这一目标，机器学习流程中的负责任设计模式（RDPs）对于确保伦理和公平结果至关重要。在本文中，我们提出了一个综合框架，将RDPs纳入ML流程中，以减轻风险并确保AI系统的伦理发展。我们的框架包括新的负责任AI设计模式，这些模式通过对AI伦理和数据管理专家的调查确定，并通过专家反馈的实际情况进行验证。该框架指导AI开发人员、数据科学家和决策者在AI开发和部署中实施伦理实践。

    Integrating ethical practices into the AI development process for artificial intelligence (AI) is essential to ensure safe, fair, and responsible operation. AI ethics involves applying ethical principles to the entire life cycle of AI systems. This is essential to mitigate potential risks and harms associated with AI, such as algorithm biases. To achieve this goal, responsible design patterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee ethical and fair outcomes. In this paper, we propose a comprehensive framework incorporating RDPs into ML pipelines to mitigate risks and ensure the ethical development of AI systems. Our framework comprises new responsible AI design patterns for ML pipelines identified through a survey of AI ethics and data management experts and validated through real-world scenarios with expert feedback. The framework guides AI developers, data scientists, and policy-makers to implement ethical practices in AI development and deploy responsibl
    
[^179]: 在图形的超出分布泛化中学习标签和环境因果独立性

    Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization. (arXiv:2306.01103v1 [cs.LG])

    [http://arxiv.org/abs/2306.01103](http://arxiv.org/abs/2306.01103)

    本文提出了一种考虑标签和环境因果独立性的方法来解决图形超出分布（OOD）泛化问题，通过敌对训练策略来联合优化属性以获得有效结果，实验证明LECI显着优于之前的方法。

    

    我们解决了图形的超出分布（OOD）泛化问题。现有的图形OOD算法要么依赖于受限的假设，要么无法利用训练数据中的环境信息。在这项工作中，我们提出同时纳入标签和环境因果独立（LECI），充分利用标签和环境信息，从而解决之前的方法在识别因果和不变子图时面临的挑战。我们进一步开发了一种敌对训练策略，以联合优化这两个属性，用于具有理论保证的导致子图发现。广泛的实验和分析表明，LECI在合成和真实数据集上都显着优于之前的方法，将LECI确立为图形OOD泛化的实用有效解决方案。

    We tackle the problem of graph out-of-distribution (OOD) generalization. Existing graph OOD algorithms either rely on restricted assumptions or fail to exploit environment information in training data. In this work, we propose to simultaneously incorporate label and environment causal independence (LECI) to fully make use of label and environment information, thereby addressing the challenges faced by prior methods on identifying causal and invariant subgraphs. We further develop an adversarial training strategy to jointly optimize these two properties for casual subgraph discovery with theoretical guarantees. Extensive experiments and analysis show that LECI significantly outperforms prior methods on both synthetic and real-world datasets, establishing LECI as a practical and effective solution for graph OOD generalization.
    
[^180]: 人类对齐校准用于AI辅助决策制定

    Human-Aligned Calibration for AI-Assisted Decision Making. (arXiv:2306.00074v1 [cs.LG])

    [http://arxiv.org/abs/2306.00074](http://arxiv.org/abs/2306.00074)

    本文通过引入一种基于主动询问决策者个人偏好的置信度构造方法，解决了现有置信度对于决策者信任决策的不准确问题，从而提高决策的准确性和效率。

    

    当使用二元分类器提供决策支持时，它通常提供标签预测和置信度值。然后，决策者应使用置信度值来校准对预测的信任程度。在这种情况下，人们经常认为置信度值应对预测标签与实际标签匹配的概率进行良好校准的估计。然而，多条实证证据表明，决策者难以使用这些置信度值很好地确定何时信任预测。本文的目标首先是理解为什么，然后研究如何构建更有用的置信度值。我们首先认为，在广泛类的效用函数中，存在数据分布，对于这些分布，理性决策者通常难以使用以上置信度值发现最佳决策政策——最佳的决策者需要人类对齐。然后，我们引入了一种基于主动询问决策者他们在所面临的二元分类任务的决策上的个人偏好的新方法来构造置信度值。我们表明，该方法产生的置信度值比使用标准置信度度量导致更好的决策。

    Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. In this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. We first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values -- an optimal decision maker wou
    
[^181]: Parity校准

    Parity Calibration. (arXiv:2305.18655v1 [cs.LG])

    [http://arxiv.org/abs/2305.18655](http://arxiv.org/abs/2305.18655)

    本文介绍一种新的校准预测目标——parity校准，其考虑时间序列中未来观测值的增加或减少。我们使用在线二进制校准方法实现了parity校准，并在流行病学、天气预报和核聚变控制等领域中表明该方法的有效性。

    

    在序列回归设置中，决策者可能更关注未来观测值是否比当前值增加或减少，而不是未来观测值的实际值。在此背景下，我们引入了平等校准的概念，它捕捉了时间序列增减事件的校准预测目标。平等概率可以从输出的预测分布中提取，但我们显示这种策略导致理论上的不可预测性和差劲的实际性能。然后我们发现，虽然原任务是回归，但平等校准可以被表达为二进制校准。基于这种联系，我们使用在线二进制校准方法实现了平等校准。我们通过流行病学、天气预报和基于模型的核聚变控制的实际案例展示了我们方法的有效性。

    In a sequential regression setting, a decision-maker may be primarily concerned with whether the future observation will increase or decrease compared to the current one, rather than the actual value of the future observation. In this context, we introduce the notion of parity calibration, which captures the goal of calibrated forecasting for the increase-decrease (or "parity") event in a timeseries. Parity probabilities can be extracted from a forecasted distribution for the output, but we show that such a strategy leads to theoretical unpredictability and poor practical performance. We then observe that although the original task was regression, parity calibration can be expressed as binary calibration. Drawing on this connection, we use an online binary calibration method to achieve parity calibration. We demonstrate the effectiveness of our approach on real-world case studies in epidemiology, weather forecasting, and model-based control in nuclear fusion.
    
[^182]: 基准数据集上 ChatGPT 的系统研究和全面评估

    A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])

    [http://arxiv.org/abs/2305.18486](http://arxiv.org/abs/2305.18486)

    本文对基准数据集上 ChatGPT 的性能进行了全面的评估，包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务。研究旨在验证 ChatGPT 的优势和弱点，并为使用语言模型的未来研究提供见解。

    

    最近，如 ChatGPT 这样的大型语言模型（LLM）的开发引起了很多关注。然而，由于难以将该模型生成的产出与基本事实进行比较，因此其在基准学术数据集上的评估仍未充分探索。本文旨在对 ChatGPT 在包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务中的表现进行彻底评估。具体而言，我们在 140 个任务中评估了 ChatGPT，并分析了其在这些数据集中生成的 255K 次响应，这使我们的工作成为了在 NLP 基准测试中对 ChatGPT 进行的最大评估。简而言之，我们的研究旨在验证 ChatGPT 在各种任务中的优势和弱点，并为使用 LLM 的未来研究提供见解。我们还报告了一种新的迸发能力，即遵循多个查询指令。

    The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
    
[^183]: 从黑盒模型到可解释模型的转化，用于高效的迁移学习

    Distilling BlackBox to Interpretable models for Efficient Transfer Learning. (arXiv:2305.17303v1 [cs.CV])

    [http://arxiv.org/abs/2305.17303](http://arxiv.org/abs/2305.17303)

    本论文提出了一种方法可以将黑盒模型转化成为可解释模型并在目标领域成功进行迁移学习，从而实现高效迁移学习。

    

    建立具有普适性的AI模型是医疗领域面临的主要挑战之一。神经网络（NN）模型即使输入分布轻微移位（例如扫描仪类型），也会受到影响，而放射科医生则依赖于异常性的通用描述性规则。微调模型以将知识从一个领域转移到另一个领域需要大量标记数据。在本文中，我们开发了一种可解释的模型，它可以在计算成本最小的情况下，高效地针对未知的目标域进行微调。我们认为NN的可解释组件大致是域不变的。然而，可解释模型通常表现不及它们的BB变体。在源域中我们先使用人类理解的概念从BB开始，将其提炼成一组浅显易懂的interpretable模型。由于每个interpretable模型都覆盖了数据的一个子集，具有一组interpretable模型的混合可以实现与BB相当的性能。

    Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (\eg scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this paper, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain-invariant. However, interpretable models typically underperform compared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a \emph{mixture} of shallow interpretable models using human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Fu
    
[^184]: 动力学系统中隐式神经网络的长期预测稳定性

    Stability of implicit neural networks for long-term forecasting in dynamical systems. (arXiv:2305.17155v1 [cs.LG])

    [http://arxiv.org/abs/2305.17155](http://arxiv.org/abs/2305.17155)

    本文提出了一种基于隐式数值方案稳定性特性的神经网络，加入了硬性约束来保证其权重稳定性，取得了较好的长期预测结果。

    

    预测长时间范围内的物理信号是偏微分方程研究中最具挑战性的任务之一。为了规避传统求解器的限制，提出了许多不同的深度学习方法。它们都基于自回归方法并展示出稳定性问题。受隐式数值方案的稳定性特性启发，我们引入了一个稳定的自回归隐式神经网络。我们根据数值方案的稳定性定义，开发了一种理论来保证网络预测的稳定性。它导致我们对其权重添加了硬性约束，并在潜空间中传播动态。我们的实验结果验证了我们的稳定性，展示了在两个输运偏微分方程的长期预测上改进的结果。

    Forecasting physical signals in long time range is among the most challenging tasks in Partial Differential Equations (PDEs) research. To circumvent limitations of traditional solvers, many different Deep Learning methods have been proposed. They are all based on auto-regressive methods and exhibit stability issues. Drawing inspiration from the stability property of implicit numerical schemes, we introduce a stable auto-regressive implicit neural network. We develop a theory based on the stability definition of schemes to ensure the stability in forecasting of this network. It leads us to introduce hard constraints on its weights and propagate the dynamics in the latent space. Our experimental results validate our stability property, and show improved results at long-term forecasting for two transports PDEs.
    
[^185]: 扩散模型的并行采样

    Parallel Sampling of Diffusion Models. (arXiv:2305.16317v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16317](http://arxiv.org/abs/2305.16317)

    本文提出了一种新方法，ParaDiGMS，可以通过并行处理多个步骤来加速预训练扩散模型的采样。ParaDiGMS是第一个使计算速度和采样效率实现平衡的扩散采样方法，并与现有方法兼容。

    

    扩散模型是强大的生成模型，但采样速度缓慢，通常需要进行1000次顺序去噪步骤才能得到一个样本。因此，本文探索了一种交换计算机处理速度和采样效率的方法。通过猜测未来的去噪步骤的解决方案并逐步细化至收敛的Picard迭代，我们展示了惊人的发现：尽管去噪步骤有顺序性，但仍然可以并行采样。基于这一洞见，我们提出了ParaDiGMS，这是一种通过以并行方式去噪多个步骤加速预训练扩散模型采样的新方法。ParaDiGMS是第一个在计算处理速度和采样效率上实现平衡的扩散采样方法，甚至还兼容现有的方法。

    Diffusion models are powerful generative models but suffer from slow sampling, often taking 1000 sequential denoising steps for one sample. As a result, considerable efforts have been directed toward reducing the number of denoising steps, but these methods hurt sample quality. Instead of reducing the number of denoising steps (trading quality for speed), in this paper we explore an orthogonal approach: can we run the denoising steps in parallel (trading compute for speed)? In spite of the sequential nature of the denoising steps, we show that surprisingly it is possible to parallelize sampling via Picard iterations, by guessing the solution of future denoising steps and iteratively refining until convergence. With this insight, we present ParaDiGMS, a novel method to accelerate the sampling of pretrained diffusion models by denoising multiple steps in parallel. ParaDiGMS is the first diffusion sampling method that enables trading compute for speed and is even compatible with existing 
    
[^186]: 自监督神经表示是否能够在预训练人类语音后区分动物呼叫者？

    Can Self-Supervised Neural Representations Pre-Trained on Human Speech distinguish Animal Callers?. (arXiv:2305.14035v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14035](http://arxiv.org/abs/2305.14035)

    本文研究了利用人类语音自监督神经表示学习来分析生物声学信号的交叉可迁移性，在狨猴叫声识别和检测中取得了成功，未来该方法可有效应用于该领域的研究。

    

    自监督学习模型仅使用给定信号的内在结构，独立于声学领域，将信号转换为嵌入空间中的基本信息。这意味着这些表示的效用不仅局限于人类语音建模。本文基于此探讨了通过预训练人类语音自监督神经表示学习来分析生物声学信号的交叉可迁移性。我们使用11个预先训练的各种预训练任务的自监督模型对叫声进行辨别和检测研究，结果表明嵌入空间包含有意义的呼叫者信息，可成功区分不同狨猴叫的个体身份，而无需微调。这表明在生物声学领域，预先训练于人类语音的表示能够被有效地应用于该领域，并为该领域的未来研究提供有价值的见解。

    Self-supervised learning (SSL) models use only the intrinsic structure of a given signal, independent of its acoustic domain, to extract essential information from the input to an embedding space. This implies that the utility of such representations is not limited to modeling human speech alone. Building on this understanding, this paper explores the cross-transferability of SSL neural representations learned from human speech to analyze bio-acoustic signals. We conduct a caller discrimination analysis and a caller detection study on Marmoset vocalizations using eleven SSL models pre-trained with various pretext tasks. The results show that the embedding spaces carry meaningful caller information and can successfully distinguish the individual identities of Marmoset callers without fine-tuning. This demonstrates that representations pre-trained on human speech can be effectively applied to the bio-acoustics domain, providing valuable insights for future investigations in this field.
    
[^187]: 捕捉促销期间的转化率波动：一种新颖的历史数据再利用方法

    Capturing Conversion Rate Fluctuation during Sales Promotions: A Novel Historical Data Reuse Approach. (arXiv:2305.12837v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2305.12837](http://arxiv.org/abs/2305.12837)

    本论文提出了一种名为HDR的新方法，通过重复使用历史促销数据，来捕捉促销转化模式，达到更好地适应促销模式的目的。

    

    转化率（CVR）预测是在线推荐系统的核心组件之一，已经提出了各种方法以获得准确和一致的CVR估计。然而，我们观察到，即使训练良好的CVR预测模型，在促销期间也经常表现出次优的性能。这主要归因于数据分布转移问题，其中传统方法不再起作用。因此，我们寻求开发替代建模技术用于CVR预测。观察到不同促销之间存在相似的购买模式，我们提出了重用历史促销数据以捕捉促销转化模式的方法。因此，我们提出了一种新颖的历史数据再利用（HDR）方法，该方法首先检索历史上相似的促销数据，然后使用获取的数据微调CVR预测模型以更好地适应促销模式。HDR由三个组件组成：自动数据

    Conversion rate (CVR) prediction is one of the core components in online recommender systems, and various approaches have been proposed to obtain accurate and well-calibrated CVR estimation. However, we observe that a well-trained CVR prediction model often performs sub-optimally during sales promotions. This can be largely ascribed to the problem of the data distribution shift, in which the conventional methods no longer work. To this end, we seek to develop alternative modeling techniques for CVR prediction. Observing similar purchase patterns across different promotions, we propose reusing the historical promotion data to capture the promotional conversion patterns. Herein, we propose a novel \textbf{H}istorical \textbf{D}ata \textbf{R}euse (\textbf{HDR}) approach that first retrieves historically similar promotion data and then fine-tunes the CVR prediction model with the acquired data for better adaptation to the promotion mode. HDR consists of three components: an automated data 
    
[^188]: RGCVAE：基于关系图条件化可变自编码器的分子设计

    RGCVAE: Relational Graph Conditioned Variational Autoencoder for Molecule Design. (arXiv:2305.11699v1 [cs.LG])

    [http://arxiv.org/abs/2305.11699](http://arxiv.org/abs/2305.11699)

    本文提出了RGCVAE，一种基于关系图条件化的可变自编码器，可以有效、高效地进行分子设计，并在两个数据集上表现出了先进的生成性能和快速的训练速度。

    

    确定表现出某些预定特性的分子是难以解决的问题。近年来，深度生成模型已被用于分子生成。深度图变分自编码器是最强大的机器学习工具之一，可用于解决此问题。然而，现有方法往往难以捕捉真实的数据分布，并且倾向于计算成本高。在本文中，我们提出了一种高效且有效的基于关系图同构网络的图变分自编码器RGCVAE：（i）利用全新的强大关系图同构网络的编码网络；（ii）一种新颖的概率解码组件。在两个广泛采用的数据集上，与数种最先进的VAE方法相比，RGCVAE表现出最先进的分子生成性能，同时训练速度显著加快。

    Identifying molecules that exhibit some pre-specified properties is a difficult problem to solve. In the last few years, deep generative models have been used for molecule generation. Deep Graph Variational Autoencoders are among the most powerful machine learning tools with which it is possible to address this problem. However, existing methods struggle in capturing the true data distribution and tend to be computationally expensive. In this work, we propose RGCVAE, an efficient and effective Graph Variational Autoencoder based on: (i) an encoding network exploiting a new powerful Relational Graph Isomorphism Network; (ii) a novel probabilistic decoding component. Compared to several state-of-the-art VAE methods on two widely adopted datasets, RGCVAE shows state-of-the-art molecule generation performance while being significantly faster to train.
    
[^189]: 差分隐私在线物品定价

    Differentially Private Online Item Pricing. (arXiv:2305.11362v1 [cs.GT])

    [http://arxiv.org/abs/2305.11362](http://arxiv.org/abs/2305.11362)

    本文介绍了一种差分隐私算法，可在保护买家隐私的同时实现重复、不限供应物品拍卖中的收益最大化，是第一个提供隐私保证的$O(\sqrt{T}\log{T})$亏损子线性的方法。

    

    本文探讨了在保护买方隐私的同时，实现重复、不限供应物品拍卖中的收益最大化问题。我们提出了一种新颖的算法，它与买方的输入对（商品选择和出价）具有差分隐私性质。值得注意的是，我们的算法是第一个提供隐私保证的$O(\sqrt{T}\log{T})$亏损子线性(regret)的方法。我们的方法基于指数权重元算法，通过小的随机扰动缓解了收益函数不连续的问题。由于与指数机制的结构相似，因此我们的方法固有地保证了差分隐私。我们还将我们的算法扩展到逐轮策略性出价的情况。内在的差分隐私性质使我们能够在最小修改的情况下适应这种情况，并确保其亏损子线性。

    This work addresses the problem of revenue maximization in a repeated, unlimited supply item-pricing auction while preserving buyer privacy. We present a novel algorithm that provides differential privacy with respect to the buyer's input pair: item selection and bid. Notably, our algorithm is the first to offer a sublinear $O(\sqrt{T}\log{T})$ regret with a privacy guarantee. Our method is based on an exponential weights meta-algorithm, and we mitigate the issue of discontinuities in revenue functions via small random perturbations. As a result of its structural similarity to the exponential mechanism, our method inherently secures differential privacy. We also extend our algorithm to accommodate scenarios where buyers strategically bid over successive rounds. The inherent differential privacy allows us to adapt our algorithm with minimal modification to ensure a sublinear regret in this setting.
    
[^190]: 乐观的近端策略优化在线性马尔可夫决策过程中的理论分析

    A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes. (arXiv:2305.08841v1 [cs.LG])

    [http://arxiv.org/abs/2305.08841](http://arxiv.org/abs/2305.08841)

    本文提出了一种乐观的近端策略优化算法，用于解决带有全信息反馈的周期性对抗性线性MDP，在随机线性MDP和带有全信息的敌对线性MDP中，达到了最先进的后悔边界，并具有新颖的多批次更新机制。

    

    近端策略优化（PPO）算法是强化学习领域中最成功的方法之一。尽管PPO很成功，但是对于PPO及其乐观变种是否能有效解决线性马尔可夫决策过程(MDPs)的理论理解仍不足。为了填补这一空白，我们提出了一种乐观的近端策略优化算法，用于带有全信息反馈的周期敌对线性MDP，并为其建立了一个$\tilde{\mathcal{O}}(d^{3/4}H^2K^{3/4})$的后悔值。其中$d$是线性MDPs的环境维数，$H$是每个周期的长度，$K$是周期数。与现有的基于策略的算法相比，在随机线性MDP和带有全信息的敌对线性MDP中，我们实现了当今最先进的后悔边界。此外，我们的算法设计具有新颖的多批次更新机制。

    The proximal policy optimization (PPO) algorithm stands as one of the most prosperous methods in the field of reinforcement learning (RL). Despite its success, the theoretical understanding of PPO remains deficient. Specifically, it is unclear whether PPO or its optimistic variants can effectively solve linear Markov decision processes (MDPs), which are arguably the simplest models in RL with function approximation. To bridge this gap, we propose an optimistic variant of PPO for episodic adversarial linear MDPs with full-information feedback, and establish a $\tilde{\mathcal{O}}(d^{3/4}H^2K^{3/4})$ regret for it. Here $d$ is the ambient dimension of linear MDPs, $H$ is the length of each episode, and $K$ is the number of episodes. Compared with existing policy-based algorithms, we achieve the state-of-the-art regret bound in both stochastic linear MDPs and adversarial linear MDPs with full information. Additionally, our algorithm design features a novel multi-batched updating mechanism
    
[^191]: 公平性审计的统计推断方法

    Statistical Inference for Fairness Auditing. (arXiv:2305.03712v1 [stat.ME])

    [http://arxiv.org/abs/2305.03712](http://arxiv.org/abs/2305.03712)

    本文介绍了一种公平性审计的统计推断方法，可用于评估黑盒模型在敏感子群体上的表现，并通过自举方法限制多个群体表现差异，方法通用性强且适用面广。

    

    在将黑盒模型用于高风险问题之前，评估模型在敏感子群体上的表现非常重要。本文提出了一种“公平性审计”的任务框架，并通过多重假设检验将其表述。我们展示了如何使用自举方法以统计保证的方式同时限制多个群体表现差异。我们的方法适用于几乎任何性能度量或群体公平性标准，并且可以处理非常丰富的、甚至是无限的子群体集合。此外，我们将方法推广到了多个潜在重叠标准下的模型表现审计。我们在合成和真实数据集上展示了我们方法的有效性。

    Before deploying a black-box model in high-stakes problems, it is important to evaluate the model's performance on sensitive subpopulations. For example, in a recidivism prediction task, we may wish to identify demographic groups for which our prediction model has unacceptably high false positive rates or certify that no such groups exist. In this paper, we frame this task, often referred to as "fairness auditing," in terms of multiple hypothesis testing. We show how the bootstrap can be used to simultaneously bound performance disparities over a collection of groups with statistical guarantees. Our methods can be used to flag subpopulations affected by model underperformance, and certify subpopulations for which the model performs adequately. Crucially, our audit is model-agnostic and applicable to nearly any performance metric or group fairness criterion. Our methods also accommodate extremely rich -- even infinite -- collections of subpopulations. Further, we generalize beyond subpo
    
[^192]: 理想的不断学习者: 不会遗忘的代理

    The Ideal Continual Learner: An Agent That Never Forgets. (arXiv:2305.00316v1 [cs.LG])

    [http://arxiv.org/abs/2305.00316](http://arxiv.org/abs/2305.00316)

    本文提出了一个新的持续学习框架——理想的持续学习者（ICL），通过构建避免灾难性遗忘，统一了多个持续学习方法，并为这些方法的优缺点提供了新的理论见解。

    

    持续学习的目标是找到一个模型，解决按顺序呈现给学习者的多个学习任务。在这种情况下，一个主要的挑战是，当学习新任务时，学习者可能会忘记如何解决先前的任务，这种现象称为灾难性遗忘。为了应对这一挑战，已经提出了许多实用的方法，包括基于记忆、基于正则化和基于扩展的方法。然而，对这些方法的严格理论理解仍然是困难的。本文旨在通过提出一种新的持续学习框架——理想的持续学习者（ICL），从理论与实践之间的鸿沟中跨越过去，ICL的构建保证避免灾难性遗忘，统一了多个成熟的持续学习方法，并为这些方法的优缺点提供了新的理论见解。我们还为ICL推导出了泛化界限，这使我们能够在理论上量化如何控制模型的复杂度来减轻灾难性遗忘。

    The goal of continual learning is to find a model that solves multiple learning tasks which are presented sequentially to the learner. A key challenge in this setting is that the learner may forget how to solve a previous task when learning a new task, a phenomenon known as catastrophic forgetting. To address this challenge, many practical methods have been proposed, including memory-based, regularization-based, and expansion-based methods. However, a rigorous theoretical understanding of these methods remains elusive. This paper aims to bridge this gap between theory and practice by proposing a new continual learning framework called Ideal Continual Learner (ICL), which is guaranteed to avoid catastrophic forgetting by construction. We show that ICL unifies multiple well-established continual learning methods and gives new theoretical insights into the strengths and weaknesses of these methods. We also derive generalization bounds for ICL which allow us to theoretically quantify how r
    
[^193]: 用自然语言指令控制文本生成

    Controlled Text Generation with Natural Language Instructions. (arXiv:2304.14293v1 [cs.CL])

    [http://arxiv.org/abs/2304.14293](http://arxiv.org/abs/2304.14293)

    InstructCTG是一个可以通过自然语言描述和演示来控制文本生成并满足不同约束条件的框架，它有效地解决了现有搜索或得分方法所存在的问题。

    

    大型语言模型可以产生流畅的文本，并能根据自然语言指令解决各种任务，无需特定的训练。然而，控制它们的生成以满足不同应用程序所需的各种约束条件是非常困难的。本文提供了一个带约束调节的文本生成框架——InstructCTG，该框架通过基于自然语言描述和约束演示来纳入不同的约束条件。我们首先通过一系列现成的NLP工具和简单的启发式方法来提取自然文本的潜在约束条件。此外，我们将这些约束条件转化为自然语言指令，以形成弱监督的训练数据。通过添加自然语言约束描述和少量演示，我们对预训练语言模型进行了微调，以纳入各种类型的约束条件。与现有基于搜索或得分的方法相比，InstructCTG 更加有效。

    Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present InstructCTG, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf NLP tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Compared to existing search-based or score-based methods, InstructCT
    
[^194]: 学习轨迹是泛化指标

    Learning Trajectories are Generalization Indicators. (arXiv:2304.12579v1 [cs.LG])

    [http://arxiv.org/abs/2304.12579](http://arxiv.org/abs/2304.12579)

    本文研究了DNN的学习轨迹与其在优化后的泛化能力的联系，提出了一种基于学习轨迹复杂性和训练集偏置和多样性比率的新的泛化上界，并通过实验验证了其有效性。

    

    本文旨在研究深度神经网络（DNN）的学习轨迹与其在广泛使用的梯度下降和随机梯度下降算法优化时对应的泛化能力之间的联系。本文构建了线性近似函数来模拟轨迹信息，并在此基础上提出了一种基于更丰富轨迹信息的新的泛化上界。我们提出的泛化上界依赖于学习轨迹的复杂性以及训练集的偏置和多样性比之间的比率。实验结果表明，该方法可以有效地捕捉不同训练步骤、学习率和标签噪声水平下的泛化趋势。

    The aim of this paper is to investigate the connection between learning trajectories of the Deep Neural Networks (DNNs) and their corresponding generalization capabilities when being optimized with broadly used gradient descent and stochastic gradient descent algorithms. In this paper, we construct Linear Approximation Function to model the trajectory information and we propose a new generalization bound with richer trajectory information based on it. Our proposed generalization bound relies on the complexity of learning trajectory and the ratio between the bias and diversity of training set. Experimental results indicate that the proposed method effectively captures the generalization trend across various training steps, learning rates, and label noise levels.
    
[^195]: SkinGPT: 一种基于视觉大语言模型的皮肤科诊断系统

    SkinGPT: A Dermatology Diagnostic System with Vision Large Language Model. (arXiv:2304.10691v1 [eess.IV])

    [http://arxiv.org/abs/2304.10691](http://arxiv.org/abs/2304.10691)

    SkinGPT是一个基于迷你GPT-4的精细调整版本和内部皮肤图像集合结合的皮肤科诊断系统，利用高级视觉大语言模型，解决了皮肤科医生不足、准确诊断皮肤科图片难度大，以及提供用户友好的诊断报告困难等三个挑战。

    

    皮肤和皮下疾病是全球非致命疾病负担的主要原因之一，影响了大部分人口。然而，皮肤科诊断领域存在三个主要挑战：皮肤科医生不足、准确诊断皮肤科图片难度大以及提供用户友好的诊断报告困难。最近大型语言模型的进展显示出了在临床应用中的潜力。然而，当前的大型语言模型在处理图像上存在困难，并且使用ChatGPT的API上传数据会存在潜在的隐私问题。在本文中，我们提出了SkinGPT，这是第一个利用高级视觉大语言模型的皮肤科诊断系统。SkinGPT是首个采用内部皮肤图像集合和迷你GPT-4的精细调整版本结合而成的系统。

    Skin and subcutaneous diseases are among the major causes of the nonfatal disease burden worldwide, affecting a significant proportion of the population. However, there are three major challenges in the field of dermatology diagnosis. Firstly, there is a shortage of dermatologists available to diagnose patients. Secondly, accurately diagnosing dermatological pictures can be challenging. Lastly, providing user-friendly diagnostic reports can be difficult. Recent advancements in the field of large language models (LLMs) have shown potential for clinical applications. However, current LLMs have difficulty processing images, and there are potential privacy concerns associated with using ChatGPT's API for uploading data. In this paper, we propose SkinGPT, which is the first dermatology diagnostic system that utilizes an advanced vision-based large language model. SkinGPT is the first system of its kind, incorporating a fine-tuned version of MiniGPT-4 with a vast collection of in-house skin 
    
[^196]: 未知动态下的长期公平性

    Long-Term Fairness with Unknown Dynamics. (arXiv:2304.09362v1 [cs.LG])

    [http://arxiv.org/abs/2304.09362](http://arxiv.org/abs/2304.09362)

    本文提出一种新方法，通过在未知动态下追求长期公平性，实现算法的动态适应和权衡，可为分类器-人群系统推向更理想的平衡。

    

    尽管机器学习可能会强化社会不平等，但它也可以用于动态地寻求公平的结果。在本文中，我们在在线强化学习的上下文中规范了长期公正性。该公式可以容纳动态控制目标，例如推动人群状态中固有的平等，这些目标不能被纳入到公平性的静态公式中。我们证明这种方法允许算法通过牺牲短期激励，将分类器-人群系统推向更理想的平衡，以适应未知的动态系统。针对这种情况，我们开发了一种算法，该算法适应了最近的在线学习工作。我们证明，这种算法在累积损失和公平性违规的累积性（作为人群之间的统计规律）上实现了同时的概率界限。我们将我们的算法与基准的细微分类器的重复训练以及一个解题器进行比较，在两个协调目标之间实现了更好的权衡。

    While machine learning can myopically reinforce social inequalities, it may also be used to dynamically seek equitable outcomes. In this paper, we formalize long-term fairness in the context of online reinforcement learning. This formulation can accommodate dynamical control objectives, such as driving equity inherent in the state of a population, that cannot be incorporated into static formulations of fairness. We demonstrate that this framing allows an algorithm to adapt to unknown dynamics by sacrificing short-term incentives to drive a classifier-population system towards more desirable equilibria. For the proposed setting, we develop an algorithm that adapts recent work in online learning. We prove that this algorithm achieves simultaneous probabilistic bounds on cumulative loss and cumulative violations of fairness (as statistical regularities between demographic groups). We compare our proposed algorithm to the repeated retraining of myopic classifiers, as a baseline, and to a d
    
[^197]: 元卡洛调整朗之万(MALA)在光滑且等周条件下的混合简单证明

    A Simple Proof of the Mixing of Metropolis-Adjusted Langevin Algorithm under Smoothness and Isoperimetry. (arXiv:2304.04095v1 [stat.ML])

    [http://arxiv.org/abs/2304.04095](http://arxiv.org/abs/2304.04095)

    本文证明了，在光滑和等周条件下，MALA的混合时间仅与Hessian矩阵的trace有关，而与其算子范数和log-concave没有关系。

    

    本文研究了在$\mathbb{R}^d$上样本目标密度的元卡洛调整朗之万（MALA）的混合时间。我们假设目标密度满足$\psi_\mu$-等周和它的黑塞矩阵的trace和算子范数分别受$L$和$\Upsilon$的限制。我们的主要结论是，从热启动开始，为了达到$\epsilon$总变差距离，MALA在$O\left(\frac{(L\Upsilon)^{\frac12}}{\psi_\mu^2} \log\left(\frac{1}{\epsilon}\right)\right)$次迭代中混合。值得注意的是，该结果不仅适用于对数凹采样设置，而且混合时间仅取决于$\Upsilon$，而不是其上界$Ld$。在$m$-强对数凹和$L$-光滑采样设置中，我们的界限恢复了以前的MALA的最小值混合界限。

    We study the mixing time of Metropolis-Adjusted Langevin algorithm (MALA) for sampling a target density on $\mathbb{R}^d$. We assume that the target density satisfies $\psi_\mu$-isoperimetry and that the operator norm and trace of its Hessian are bounded by $L$ and $\Upsilon$ respectively. Our main result establishes that, from a warm start, to achieve $\epsilon$-total variation distance to the target density, MALA mixes in $O\left(\frac{(L\Upsilon)^{\frac12}}{\psi_\mu^2} \log\left(\frac{1}{\epsilon}\right)\right)$ iterations. Notably, this result holds beyond the log-concave sampling setting and the mixing time depends on only $\Upsilon$ rather than its upper bound $L d$. In the $m$-strongly logconcave and $L$-log-smooth sampling setting, our bound recovers the previous minimax mixing bound of MALA~\cite{wu2021minimax}.
    
[^198]: 奖励是否合理？在 MACHIAVELLI 基准测试中衡量奖励与道德行为之间的权衡

    Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])

    [http://arxiv.org/abs/2304.03279](http://arxiv.org/abs/2304.03279)

    本文介绍了 MACHIAVELLI 基准测试，用于衡量人工智能代理是否表现出马基雅维利行为，发现了最大化奖励和行为的道德性之间存在权衡，并探索了基于语言模型的方法来减轻这种权衡。

    

    传统上，人工智能代理被训练成最大化奖励，这可能会激励追求权力和欺骗行为，类似于语言模型中的下一个标记预测可能会激励有害行为。那么代理是否自然而然地学会了马基雅维利行为？我们如何在 GPT-4 等通用模型中衡量这些行为呢？为回答这些问题，我们引入了 MACHIAVELLI 基准测试，该测试涵盖了超过一百万个多样化的情景，重点关注社会决策制定，用于衡量人工代理是否表现出马基雅维利行为。我们数学化了数十种有害行为，并使用我们的注释来评估代理倾向于追求权力，造成功能不良和违反伦理的倾向。我们观察到最大化奖励和行为的道德性之间存在一些紧张关系。为了改善这种权衡，我们研究了基于语言模型的方法，以使代理趋向于采取更少的有害行为。我们的结果显示，MACHIAVELLI 是评估人工代理马基雅维利行为水平的有用基准测试。

    Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results sh
    
[^199]: 区域风力特征如何影响基于CNN的风速预测：来自时空相关性分析的见解。

    How Regional Wind Characteristics Affect CNN-based wind predictions: Insights from Spatiotemporal Correlation Analysis. (arXiv:2304.01545v1 [cs.LG])

    [http://arxiv.org/abs/2304.01545](http://arxiv.org/abs/2304.01545)

    本研究探讨了使用3D卷积神经网络（3D-CNN）对时空数据进行风速预测的精度，并发现使用周围区域的空间数据进行3D-CNN训练可以比仅使用单点信息获得更好的预测性能。

    

    本研究探讨了时空数据维度对利用人工神经网络构建的风速预测模型精度的影响。尽管以前的研究表明，加入空间数据可以提高风速预测模型的精度，但很少有研究探讨了基于神经网络的预测模型中不同空间尺度改进的程度。此外，对于这些模型的最佳时间长度的输入数据的研究也很有限。为了解决这个问题，本研究在使用3D卷积神经网络（3D-CNN）预测风速时，采用具有不同时空维度的数据作为输入，并评估其预测性能。结果表明，使用周围区域的空间数据进行3D-CNN训练可以比仅使用单点信息获得更好的预测性能。此外，多时间数据对预测性能的影响更为显著。

    This study investigates the impact of spatiotemporal data dimensions on the precision of a wind forecasting model developed using an artificial neural network. Although previous studies have shown that incorporating spatial data can enhance the accuracy of wind forecasting models, few investigations have explored the extent of the improvement owing to different spatial scales in neural network-based predictive models. Additionally, there are limited studies on the optimal temporal length of the input data for these models. To address this gap, this study employs data with various spatiotemporal dimensions as inputs when forecasting wind using 3D-Convolutional Neural Networks (3D-CNN) and assesses their predictive performance. The results indicate that using spatial data of the surrounding area for 3D-CNN training can achieve better predictive performance than using only single-point information. Additionally, multi-time data had a more positive effect on the predictive performance than
    
[^200]: 何时预训练图神经网络？基于数据生成视角的回答！

    When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective!. (arXiv:2303.16458v1 [cs.LG])

    [http://arxiv.org/abs/2303.16458](http://arxiv.org/abs/2303.16458)

    本文提出了一个通用框架W2PGNN，旨在回答何时预训练图神经网络的关键问题。使用新的角度探索了从预训练数据到下游数据的复杂生成机制。

    

    最近，图预训练在学术界引起了广泛关注，旨在从未标记的图数据中学习可转移知识，以提高下游性能。尽管最近的尝试，但负面迁移是将图预训练模型应用于下游任务时的重大问题。现有工作通过设计多种图预训练和微调策略，致力于解决何时预训练和如何预训练的问题。然而，有时候无论策略如何先进，“预训练和微调”范式仍然无法带来明显的好处。本文引入了一个通用框架W2PGNN来回答何时预训练的关键问题（即我们在什么情况下可以利用图预训练），然后再进行费力的预训练或微调。我们从一个新的角度探索了从预训练数据到下游数据的复杂生成机制。

    Recently, graph pre-training has attracted wide research attention, which aims to learn transferable knowledge from unlabeled graph data so as to improve downstream performance. Despite these recent attempts, the negative transfer is a major issue when applying graph pre-trained models to downstream tasks. Existing works made great efforts on the issue of what to pre-train and how to pre-train by designing a number of graph pre-training and fine-tuning strategies. However, there are indeed cases where no matter how advanced the strategy is, the "pre-train and fine-tune" paradigm still cannot achieve clear benefits. This paper introduces a generic framework W2PGNN to answer the crucial question of when to pre-train (i.e., in what situations could we take advantage of graph pre-training) before performing effortful pre-training or fine-tuning. We start from a new perspective to explore the complex generative mechanisms from the pre-training data to downstream data. In particular, W2PGNN 
    
[^201]: 打开神经网络分类器以计算Shap分数

    Opening Up the Neural Network Classifier for Shap Score Computation. (arXiv:2303.06516v1 [cs.AI])

    [http://arxiv.org/abs/2303.06516](http://arxiv.org/abs/2303.06516)

    本文提出了一种高效计算机器学习模型分类中Shap解释分数的方法，通过将二进制神经网络转换为布尔电路，并使用知识编译技术，将电路视为开放式模型，通过最近的高效算法计算Shap分数，相比于将BNN视为黑盒模型直接计算Shap，性能有了显著的提高。

    This paper proposes an efficient method for computing Shap explanation scores in machine learning model classification by transforming binary neural networks into Boolean circuits and treating the resulting circuit as an open-box model, which leads to a significant improvement in performance compared to computing Shap directly on the BNN treated as a black-box model.

    我们解决了使用机器学习模型进行分类的Shap解释分数的高效计算问题。为此，我们展示了将二进制神经网络（BNN）转换为确定性和可分解的布尔电路，使用知识编译技术。所得到的电路被视为开放式模型，通过最近的高效算法计算Shap分数。详细的实验表明，与将BNN视为黑盒模型直接计算Shap相比，性能有了显著的提高。

    We address the problem of efficiently computing Shap explanation scores for classifications with machine learning models. With this goal, we show the transformation of binary neural networks (BNNs) for classification into deterministic and decomposable Boolean circuits, for which knowledge compilation techniques are used. The resulting circuit is treated as an open-box model, to compute Shap scores by means of a recent efficient algorithm for this class of circuits. Detailed experiments show a considerable gain in performance in comparison with computing Shap directly on the BNN treated as a black-box model.
    
[^202]: DP-Fast MH: 大规模贝叶斯推断的私有、快速、准确的Metropolis-Hastings算法

    DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference. (arXiv:2303.06171v1 [cs.LG])

    [http://arxiv.org/abs/2303.06171](http://arxiv.org/abs/2303.06171)

    本文提出了一种新的DP-Fast MH算法，用于大规模贝叶斯推断，具有精确、快速和隐私保护的特点。

    This paper proposes a new DP-Fast MH algorithm for large-scale Bayesian inference, which is accurate, fast, and privacy-preserving.

    贝叶斯推断提供了一个从复杂数据中学习和在不确定性下推理的原则性框架。它已经广泛应用于机器学习任务，如医学诊断、药物设计和政策制定。在这些常见应用中，数据可能非常敏感。差分隐私（DP）提供了具有强大最坏情况隐私保证的数据分析工具，并已发展成为隐私保护数据分析的主要方法。在本文中，我们研究了Metropolis-Hastings（MH）算法，这是最基本的MCMC方法之一，用于差分隐私下的大规模贝叶斯推断。虽然大多数现有的私有MCMC算法为了获得隐私而牺牲了准确性和效率，但我们提供了第一个精确且快速的DP MH算法，大多数迭代中仅使用一个小批量的数据。我们进一步揭示了隐私、可扩展性（即批量大小）和效率（即收敛速度）之间的三重权衡，从理论上说明了这一点。

    Bayesian inference provides a principled framework for learning from complex data and reasoning under uncertainty. It has been widely applied in machine learning tasks such as medical diagnosis, drug design, and policymaking. In these common applications, the data can be highly sensitive. Differential privacy (DP) offers data analysis tools with powerful worst-case privacy guarantees and has been developed as the leading approach in privacy-preserving data analysis. In this paper, we study Metropolis-Hastings (MH), one of the most fundamental MCMC methods, for large-scale Bayesian inference under differential privacy. While most existing private MCMC algorithms sacrifice accuracy and efficiency to obtain privacy, we provide the first exact and fast DP MH algorithm, using only a minibatch of data in most iterations. We further reveal, for the first time, a three-way trade-off among privacy, scalability (i.e. the batch size), and efficiency (i.e. the convergence rate), theoretically char
    
[^203]: 多项式时间和私有学习无限高斯混合模型

    Polynomial Time and Private Learning of Unbounded Gaussian Mixture Models. (arXiv:2303.04288v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.04288](http://arxiv.org/abs/2303.04288)

    本文开发了一种技术，将高维高斯混合模型的私有参数估计问题降低到非私有算法，从而使现有的非私有算法能够在隐私保护的条件下使用，提供了第一个多项式时间的私有学习GMMs算法。

    

    本文研究了使用$k$个组件和$d$维高斯混合模型(GMMs)私下估计参数的问题。我们开发了一种技术，将此问题减少到非私有算法。这使我们能够以黑盒方式私有化现有的非私有算法，同时在样本复杂度和运行时间方面仅产生少量开销。作为我们框架的主要应用，我们使用Moitra和Valiant [MV10]的非私有算法开发了一个$(\varepsilon, \delta)$-差分隐私算法来学习GMMs。结果，这提供了第一个不对参数进行任何有界性假设的私有学习GMMs的样本复杂度上界和第一个多项式时间算法。作为我们分析的一部分，我们证明了高维高斯分布的总变差距离的紧密(高达常数因子)下界，这可以独立地得到研究。

    We study the problem of privately estimating the parameters of $d$-dimensional Gaussian Mixture Models (GMMs) with $k$ components. For this, we develop a technique to reduce the problem to its non-private counterpart. This allows us to privatize existing non-private algorithms in a blackbox manner, while incurring only a small overhead in the sample complexity and running time. As the main application of our framework, we develop an $(\varepsilon, \delta)$-differentially private algorithm to learn GMMs using the non-private algorithm of Moitra and Valiant [MV10] as a blackbox. Consequently, this gives the first sample complexity upper bound and first polynomial time algorithm for privately learning GMMs without any boundedness assumptions on the parameters. As part of our analysis, we prove a tight (up to a constant factor) lower bound on the total variation distance of high-dimensional Gaussians which can be of independent interest.
    
[^204]: 通过离线强化学习学习影响人类行为

    Learning to Influence Human Behavior with Offline Reinforcement Learning. (arXiv:2303.02265v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.02265](http://arxiv.org/abs/2303.02265)

    本文提出了一种通过离线强化学习学习影响人类行为的方法，可以提高人类在协作任务中的表现。

    This paper proposes a method of learning to influence human behavior through offline reinforcement learning, which can improve human performance in collaborative tasks.

    在现实世界中，学习代理与人类互动是最复杂的设置之一，因为人类往往由于复杂的偏见而表现出次优的、不可预测的行为。在这种情况下与人类互动的代理最终会影响这些人所采取的行动。我们的目标是使代理能够利用这种影响来提高人类在协作任务中的表现，随着任务的展开。与以前的工作不同，我们不假设与人员进行在线培训（这往往太昂贵和不安全），也不假设有高保真度环境模拟器的访问权限。我们的想法是，通过采用各种先前观察到的人类-人类交互数据并将其标记为任务奖励，离线强化学习（RL）可以学习组合行为的组件，并发现导致更理想的人类行为的行动。首先，我们展示了离线RL可以学习策略来影响和改善人类行为，尽管这些策略可能与人类的期望不同。

    In the real world, some of the most complex settings for learned agents involve interaction with humans, who often exhibit suboptimal, unpredictable behavior due to sophisticated biases. Agents that interact with people in such settings end up influencing the actions that these people take. Our goal in this work is to enable agents to leverage that influence to improve the human's performance in collaborative tasks, as the task unfolds. Unlike prior work, we do not assume online training with people (which tends to be too expensive and unsafe), nor access to a high fidelity simulator of the environment. Our idea is that by taking a variety of previously observed human-human interaction data and labeling it with the task reward, offline reinforcement learning (RL) can learn to combine components of behavior, and uncover actions that lead to more desirable human actions. First, we show that offline RL can learn strategies to influence and improve human behavior, despite those strategies 
    
[^205]: CrystalBox：基于未来的DRL网络控制器的解释器

    CrystalBox: Future-Based Explanations for DRL Network Controllers. (arXiv:2302.13483v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13483](http://arxiv.org/abs/2302.13483)

    CrystalBox是一种解释DRL网络控制器行为的框架，它能够使用未来关键网络性能指标的影响来生成简明而富有表现力的解释。

    

    解释性不足是限制高效深度强化学习(DRL)控制器实际采用的关键因素。网络解释性强化学习迄今使用引人注目的输入特征来解释控制器的行为。然而，这些基于特征的解决方案不能完全解释控制器的决策过程。通常，运营商有兴趣了解控制器对未来性能的影响，而基于特征的解决方案无法捕捉。在本文中，我们提出了CrystalBox，这是一个框架，通过关键网络性能指标的未来影响来解释控制器的行为。CrystalBox采用一种新颖的基于学习的方法，生成简洁而富有表现力的解释。我们使用DRL网络控制器的奖励组件作为解释的基础，这是运营商有意义的关键绩效指标。CrystalBox是通用的，可以适用于离散和连续控制环境。

    Lack of explainability is a key factor limiting the practical adoption of high-performant Deep Reinforcement Learning (DRL) controllers. Explainable RL for networking hitherto used salient input features to interpret a controller's behavior. However, these feature-based solutions do not completely explain the controller's decision-making process. Often, operators are interested in understanding the impact of a controller's actions on performance in the future, which feature-based solutions cannot capture.  In this paper, we present CrystalBox, a framework that explains a controller's behavior in terms of the future impact on key network performance metrics. CrystalBox employs a novel learning-based approach to generate succinct and expressive explanations. We use reward components of the DRL network controller, which are key performance metrics meaningful to operators, as the basis for explanations. CrystalBox is generalizable and can work across both discrete and continuous control en
    
[^206]: 自回归隐马尔可夫模型在非线性动力学和单位四元数观测空间中的推广

    Generalization of Auto-Regressive Hidden Markov Models to Non-Linear Dynamics and Unit Quaternion Observation Space. (arXiv:2302.11834v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.11834](http://arxiv.org/abs/2302.11834)

    本文提出了自回归隐马尔可夫模型的两个推广，一个是笛卡尔空间中更一般的自回归动力学，另一个是单位四元数空间中的线性动力学。此扩展允许描述观察状态的更复杂动力学。

    

    潜变量模型广泛应用于机器人技术、语音识别和经济学等不同领域的时间序列非监督分割。其中最广泛使用的潜变量模型是自回归隐马尔可夫模型(ARHMM)，它将由马尔可夫链动力学控制的潜在模态与观测状态的线性自回归动力学相结合。本文提出了ARHMM的两个推广。首先，我们提出了笛卡尔空间中更一般的自回归动力学，它被描述为非线性基函数的线性组合。其次，我们提出了单位四元数空间中的线性动力学，以正确描述方向。这些扩展允许描述观察状态的更复杂动力学。虽然此扩展是针对ARHMM提出的，但易于扩展到其他具有观测空间中自回归动力学的潜变量模型，例如自回归隐半马尔可夫模型。

    Latent variable models are widely used to perform unsupervised segmentation of time series in different context such as robotics, speech recognition, and economics. One of the most widely used latent variable model is the Auto-Regressive Hidden Markov Model (ARHMM), which combines a latent mode governed by a Markov chain dynamics with a linear Auto-Regressive dynamics of the observed state.  In this work, we propose two generalizations of the ARHMM. First, we propose a more general AR dynamics in Cartesian space, described as a linear combination of non-linear basis functions. Second, we propose a linear dynamics in unit quaternion space, in order to properly describe orientations. These extensions allow to describe more complex dynamics of the observed state.  Although this extension is proposed for the ARHMM, it can be easily extended to other latent variable models with AR dynamics in the observed space, such as Auto-Regressive Hidden semi-Markov Models.
    
[^207]: 减少、重复利用、回收：基于能量扩散模型和MCMC的组合生成

    Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC. (arXiv:2302.11552v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11552](http://arxiv.org/abs/2302.11552)

    该论文提出了一种基于能量扩散模型和MCMC的组合生成方法，旨在解决现有技术在组合生成中的失败问题，并提出了新的成功的解决方案。

    

    自从扩散模型问世以来，它在许多领域中已经迅速成为生成模型的主要方法。它们可以被解释为学习一系列时变的对数概率密度函数的梯度。这种解释已经激发了基于分类器和无分类器指导的思想成为后续控制扩散模型的方法。在这项工作中，我们建立在这些想法的基础上，利用扩散模型的分数-based解释，探索了用于涉及组合生成和指导的条件、修改和重复使用扩散模型的替代方法。特别是，我们调查了为什么某些类型的组合使用当前技术失败，并介绍了一些解决方案。我们得出结论，采样者(而不是模型)对此失败负有责任，并提出了新的采样器，受MCMC的启发，使组合生成成功。此外，我们提出了一种基于能量的扩散模型参数化方法，它使得逼近目标分布更加容易。

    Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the 
    
[^208]: 平衡的音频视觉数据集用于不平衡分析

    Balanced Audiovisual Dataset for Imbalance Analysis. (arXiv:2302.10912v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10912](http://arxiv.org/abs/2302.10912)

    本文通过估计样本级模态差异将现有数据集分成不同的子集并探索了模态偏差的影响。结果发现，存在不平衡算法的多模态模型在特定子集上的表现一直比单模态的差，与模态偏差相一致。

    

    不平衡问题在机器学习领域广泛存在，多模态学习领域也存在由于样本模态间固有差异导致的不平衡问题。近期的工作尝试从算法角度解决模态不平衡问题，然而，它们没有完全分析数据集中模态偏差所产生的影响。具体地说，现有的多模态数据集通常是针对特定任务收集的，其中一个模态在大多数情况下往往比其他模态表现更好。在本文中，为了全面探索模态偏差的影响，我们首先通过估计样本级模态差异将现有数据集分成不同的子集。我们惊奇地发现：存在不平衡算法的多模态模型在特定子集上的表现一直比单模态的差，与模态偏差相一致。为了进一步探索模态偏差的影响，分析现有不平衡算法的有效性，

    The imbalance problem is widespread in the field of machine learning, which also exists in multimodal learning areas caused by the intrinsic discrepancy between modalities of samples. Recent works have attempted to solve the modality imbalance problem from algorithm perspective, however, they do not fully analyze the influence of modality bias in datasets. Concretely, existing multimodal datasets are usually collected under specific tasks, where one modality tends to perform better than other ones in most conditions. In this work, to comprehensively explore the influence of modality bias, we first split existing datasets into different subsets by estimating sample-wise modality discrepancy. We surprisingly find that: the multimodal models with existing imbalance algorithms consistently perform worse than the unimodal one on specific subsets, in accordance with the modality bias. To further explore the influence of modality bias and analyze the effectiveness of existing imbalance algori
    
[^209]: 可复用数据集的无限动作上下文臂的在线学习算法

    Infinite Action Contextual Bandits with Reusable Data Exhaust. (arXiv:2302.08551v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08551](http://arxiv.org/abs/2302.08551)

    本文提出了一种无限动作上下文臂的在线学习算法，它能够生成具有明确定义的重要性权重，使平滑遗憾在生产场景中更易于采用。

    

    对于无限动作上下文臂，平滑遗憾和回归结果导致了最先进的在线性能，计算成本独立于动作集合：然而，结果的数据剩余量没有明确定义的重要性权重。这挫败了下游的数据科学流程，如离线模型选择。在本文中，我们描述了一种在线算法，具有相当的平滑遗憾保证，但生成具有明确定义的重要性权重：作为交换，在线计算成本增加，但仅至于平滑度（即仍独立于动作集合）。这消除了采用平滑遗憾在生产场景中的关键障碍。

    For infinite action contextual bandits, smoothed regret and reduction to regression results in state-of-the-art online performance with computational cost independent of the action set: unfortunately, the resulting data exhaust does not have well-defined importance-weights. This frustrates the execution of downstream data science processes such as offline model selection. In this paper we describe an online algorithm with an equivalent smoothed regret guarantee, but which generates well-defined importance weights: in exchange, the online computational cost increases, but only to order smoothness (i.e., still independent of the action set). This removes a key obstacle to adoption of smoothed regret in production scenarios.
    
[^210]: 利用分段仿射代理实现混合变量的全局和优先级优化

    Global and Preference-based Optimization with Mixed Variables using Piecewise Affine Surrogates. (arXiv:2302.04686v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.04686](http://arxiv.org/abs/2302.04686)

    本文提出了一种基于分段仿射代理构建的全局和基于偏好优化算法，可解决线性约束的混合变量问题，算法通过两种探索函数可有效搜索可行域

    

    在存在复杂限制条件的情况下，涉及混合变量（即数值和分类性的变量）的优化问题可能难以解决。此外，当目标函数是复杂模拟或实验的结果时，评估代价可能很高。本文提出了一种新颖的代理全局优化算法，基于对可行样本上目标函数的分段仿射代理构建来解决线性约束的混合变量问题，可解决中到大规模问题（编码后约100个变量和20个约束）。我们介绍了两种探索函数来通过混合整数线性规划求解器有效地搜索可行域。我们还提供了一种基于偏好的算法版本，当只能获得样本间的成对比较而未量化底层要最小化的目标函数时，可使用该算法。这两种算法进行了测试。

    Optimization problems involving mixed variables, i.e., variables of numerical and categorical nature, can be challenging to solve, especially in the presence of complex constraints. Moreover, when the objective function is the result of a complicated simulation or experiment, it may be expensive to evaluate. This paper proposes a novel surrogate-based global optimization algorithm to solve linearly constrained mixed-variable problems up to medium-large size (around 100 variables after encoding and 20 constraints) based on constructing a piecewise affine surrogate of the objective function over feasible samples. We introduce two types of exploration functions to efficiently search the feasible domain via mixed-integer linear programming solvers. We also provide a preference-based version of the algorithm, which can be used when only pairwise comparisons between samples can be acquired while the underlying objective function to minimize remains unquantified. The two algorithms are tested
    
[^211]: Q-Diffusion: 量化扩散模型

    Q-Diffusion: Quantizing Diffusion Models. (arXiv:2302.04304v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.04304](http://arxiv.org/abs/2302.04304)

    本文提出了一种特定于扩散模型的量化方法，通过时间步骤感知校准和拆分快捷路来加速图像生成，解决了扩散模型中后训练量化所面临的难题。

    

    扩散模型通过使用深度神经网络进行迭代噪声估计，在图像合成方面取得了巨大成功。然而，噪声估计模型的慢推断、高内存消耗和计算强度妨碍了扩散模型的有效采用。虽然后训练量化（PTQ）被认为是其他任务的首选压缩方法，但它不能直接用于扩散模型。我们提出了一种新颖的PTQ方法，专门针对扩散模型的独特多时间步骤管道和模型架构，压缩噪声估计网络以加速生成过程。我们认为扩散模型量化的关键难点是噪声估计网络在多个时间步骤上的变化输出分布和噪声估计网络中快捷层的双峰激活分布。我们通过时间步长感知校准和拆分快捷路来应对这些挑战。

    Diffusion models have achieved great success in image synthesis through iterative noise estimation using deep neural networks. However, the slow inference, high memory consumption, and computation intensity of the noise estimation model hinder the efficient adoption of diffusion models. Although post-training quantization (PTQ) is considered a go-to compression method for other tasks, it does not work out-of-the-box on diffusion models. We propose a novel PTQ method specifically tailored towards the unique multi-timestep pipeline and model architecture of the diffusion models, which compresses the noise estimation network to accelerate the generation process. We identify the key difficulty of diffusion model quantization as the changing output distributions of noise estimation networks over multiple time steps and the bimodal activation distribution of the shortcut layers within the noise estimation network. We tackle these challenges with timestep-aware calibration and split shortcut 
    
[^212]: 将变分自编码器与物理偏差相结合以改善显微镜数据分析

    Combining Variational Autoencoders and Physical Bias for Improved Microscopy Data Analysis. (arXiv:2302.04216v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04216](http://arxiv.org/abs/2302.04216)

    本文提出了一种结合了变分自编码器和物理偏差的方法，能够从显微镜数据中识别出物理上有意义的区域和特征。

    

    电子显微镜和扫描探针显微镜产生大量的图像或高光谱数据，例如EELS或4D STEM，这些数据包含材料的各种结构、物理和化学性质信息。为了从这些数据中提取有价值的见解，重要的是要识别出数据中物理上分离的区域，如相、铁电变体和它们之间的边界。为了推导易于解释的特征分析，以原则性和无监督的方式与明确定义的边界相结合，本文提出了一种物理增强的机器学习方法，将变分自编码器的能力与物理驱动的损失函数相结合，该损失函数寻求在对应于潜在表示的图像中最小化不连续性的总长度。我们的方法应用于各种材料，包括NiO-LSMO、BiFeO3和石墨烯。结果展示了我们的方法在显微镜数据中识别物理上有意义的区域和特征的有效性。

    Electron and scanning probe microscopy produce vast amounts of data in the form of images or hyperspectral data, such as EELS or 4D STEM, that contain information on a wide range of structural, physical, and chemical properties of materials. To extract valuable insights from these data, it is crucial to identify physically separate regions in the data, such as phases, ferroic variants, and boundaries between them. In order to derive an easily interpretable feature analysis, combining with well-defined boundaries in a principled and unsupervised manner, here we present a physics augmented machine learning method which combines the capability of Variational Autoencoders to disentangle factors of variability within the data and the physics driven loss function that seeks to minimize the total length of the discontinuities in images corresponding to latent representations. Our method is applied to various materials, including NiO-LSMO, BiFeO3, and graphene. The results demonstrate the effe
    
[^213]: DynGFN: 借助RNA速度技术进行贝叶斯基因调控网络推断的GFlowNets方法

    DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets. (arXiv:2302.04178v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04178](http://arxiv.org/abs/2302.04178)

    DynGFN是一种借助RNA速度技术进行基因调控网络推断的方法，能够捕捉网络结构的不确定性，并在准确度上超过现有方法。

    

    细胞生物学的一个重要挑战是推断基因调控网络（GRN），该网络描述了控制基因表达和细胞功能的基因及其产物之间的相互作用。本文借助RNA速度技术开发了一种方法DynGFN，该方法训练生成流网络，使用RNA速度数据执行基因调控网络的贝叶斯推断，并捕捉网络结构的不确定性。

    One of the grand challenges of cell biology is inferring the gene regulatory network (GRN) which describes interactions between genes and their products that control gene expression and cellular function. We can treat this as a causal discovery problem but with two non-standard challenges: (1) regulatory networks are inherently cyclic so we should not model a GRN as a directed acyclic graph (DAG), and (2) observations have significant measurement noise, so for typical sample sizes there will always be a large equivalence class of graphs that are likely given the data, and we want methods that capture this uncertainty. Existing methods either focus on challenge (1), identifying cyclic structure from dynamics, or on challenge (2) learning complex Bayesian posteriors over DAGs, but not both. In this paper we leverage the fact that it is possible to estimate the "velocity" of gene expression with RNA velocity techniques to develop an approach that addresses both challenges. Because we have
    
[^214]: 基于原则主义的负责任数据管理

    Principlism Guided Responsible Data Curation. (arXiv:2302.03629v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.03629](http://arxiv.org/abs/2302.03629)

    研究针对人本计算机视觉数据集的负责任数据管理建议，采用预防性反思的观点，遵循原则主义的伦理框架，解决隐私和偏差问题。

    

    人本计算机视觉数据整理实践经常忽略隐私和偏差问题，导致数据集撤回和不公平的模型。此外，通过非同意网络爬取构建的人本计算机视觉数据集缺乏全面的公平性和鲁棒性评估所必需的元数据。当前的解决方法后期解决问题，缺乏说服力的采用理由或未能提供适当应用的合适背景。我们的研究着重于针对人本计算机视觉数据集的主动领域特定建议，解决隐私和偏差问题。我们采用反思的观点，并借鉴了现有的实践和指南，遵循原则主义的伦理框架。

    Human-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. Further, HCCV datasets constructed through nonconsensual web scraping lack the necessary metadata for comprehensive fairness and robustness evaluations. Current remedies address issues post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations for curating HCCV datasets, addressing privacy and bias. We adopt an ante hoc reflective perspective and draw from current practices and guidelines, guided by the ethical framework of principlism.
    
[^215]: 组合泛化的鲁棒子任务学习

    Robust Subtask Learning for Compositional Generalization. (arXiv:2302.02984v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02984](http://arxiv.org/abs/2302.02984)

    本文提出了两种RL算法来解决子任务是否能执行任何任务的问题，并在两个多任务环境中进行了评估，表明这些算法在鲁棒性和泛化性方面优于基线算法。

    

    组合强化学习是培养执行复杂任务的策略的一种有前途的方法。在传统方法中，一个高层任务被分解成一系列子任务，并训练单独的策略来执行每个子任务。本文关注的问题是以一种能够执行任何任务的方式训练子任务策略；这里定义的任务是由一系列子任务组成的。我们旨在最大化所有任务的最坏情况表现，而不是平均情况表现。我们把该问题表述为一个二人零和博弈，其中对手挑选子任务序列。我们提出了两个RL算法来解决这个问题：一个是现有多智能体RL算法的改编，适用于我们的情况，另一个是异步版本，能够并行训练子任务策略。我们在两个具有连续状态和动作的多任务环境中评估了我们的方法，并展示了我们的算法在鲁棒性和泛化性方面优于现有的基线算法。

    Compositional reinforcement learning is a promising approach for training policies to perform complex long-horizon tasks. Typically, a high-level task is decomposed into a sequence of subtasks and a separate policy is trained to perform each subtask. In this paper, we focus on the problem of training subtask policies in a way that they can be used to perform any task; here, a task is given by a sequence of subtasks. We aim to maximize the worst-case performance over all tasks as opposed to the average-case performance. We formulate the problem as a two agent zero-sum game in which the adversary picks the sequence of subtasks. We propose two RL algorithms to solve this game: one is an adaptation of existing multi-agent RL algorithms to our setting and the other is an asynchronous version which enables parallel training of subtask policies. We evaluate our approach on two multi-task environments with continuous states and actions and demonstrate that our algorithms outperform state-of-th
    
[^216]: 基于目标的随机优化代理

    Target-based Surrogates for Stochastic Optimization. (arXiv:2302.02607v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02607](http://arxiv.org/abs/2302.02607)

    本文提出了一个基于目标的优化框架，可以构造出有效的代理函数来代替计算昂贵的梯度，实现在目标空间的最优化，摊销梯度计算的成本，进而提出了SSO算法进行投影随机梯度下降，以达到损失的稳定点。

    

    我们考虑最小化函数，这些函数计算（可能是随机的）梯度是昂贵的。这种函数在强化学习、模仿学习和对抗性训练中都很普遍。我们的目标优化框架使用（昂贵的）梯度计算来在\emph{目标空间}（例如用于分类的线性模型输出的logits）中构造代理函数，可以有效地进行最小化。这允许对模型进行多个参数更新，摊销梯度计算的成本。在完全批处理的设置中，我们证明我们的代理是损失的全局上界，并且可以使用黑盒优化算法（局部地）最小化。我们证明，最终的主导最小化算法确保收敛到损失的稳定点。接下来，我们在随机设置中实例化我们的框架，并提出SSO算法，可以视为目标空间中的投影随机梯度下降。

    We consider minimizing functions for which it is expensive to compute the (possibly stochastic) gradient. Such functions are prevalent in reinforcement learning, imitation learning and adversarial training. Our target optimization framework uses the (expensive) gradient computation to construct surrogate functions in a \emph{target space} (e.g. the logits output by a linear model for classification) that can be minimized efficiently. This allows for multiple parameter updates to the model, amortizing the cost of gradient computation. In the full-batch setting, we prove that our surrogate is a global upper-bound on the loss, and can be (locally) minimized using a black-box optimization algorithm. We prove that the resulting majorization-minimization algorithm ensures convergence to a stationary point of the loss. Next, we instantiate our framework in the stochastic setting and propose the $SSO$ algorithm, which can be viewed as projected stochastic gradient descent in the target space. 
    
[^217]: 一种用于连续超参数优化的Lipschitz乐观策略算法

    A Lipschitz Bandits Approach for Continuous Hyperparameter Optimization. (arXiv:2302.01539v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01539](http://arxiv.org/abs/2302.01539)

    BLiE是一种用于超参数优化的算法，只假设目标函数具有Lipschitz连续性。理论和实验证明BLiE优于现有算法，并且可以应用于搜索扩散模型的噪声调度。

    

    在机器学习中，超参数优化（HPO）是最关键的问题之一，因为超参数的选择对最终模型的性能有重要影响。虽然有许多HPO算法，但它们要么没有理论保证，要么需要强的假设。为此，我们介绍了BLiE——一种基于Lipschitz乐观策略的HPO算法，它只假设目标函数具有Lipschitz连续性。BLiE利用目标函数的景观以自适应地搜索超参数空间。理论上，我们证明了$(i)$ BLiE发现具有$O(\frac{1}{\epsilon})^{d_z+\beta}$个总预算的$\epsilon$最优超参数，其中$d_z$和$\beta$是问题内在的；$(ii)$ BLiE具有高度可并行性。实验上，我们证明了BLiE在基准任务上优于现有的HPO算法。我们还应用BLiE搜索扩散模型的噪声调度。与默认调度相比较，BLiE表现出更好的性能。

    One of the most critical problems in machine learning is HyperParameter Optimization (HPO), since choice of hyperparameters has a significant impact on final model performance. Although there are many HPO algorithms, they either have no theoretical guarantees or require strong assumptions. To this end, we introduce BLiE -- a Lipschitz-bandit-based algorithm for HPO that only assumes Lipschitz continuity of the objective function. BLiE exploits the landscape of the objective function to adaptively search over the hyperparameter space. Theoretically, we show that $(i)$ BLiE finds an $\epsilon$-optimal hyperparameter with $O \left( \frac{1}{\epsilon} \right)^{d_z + \beta}$ total budgets, where $d_z$ and $\beta$ are problem intrinsic; $(ii)$ BLiE is highly parallelizable. Empirically, we demonstrate that BLiE outperforms the state-of-the-art HPO algorithms on benchmark tasks. We also apply BLiE to search for noise schedule of diffusion models. Comparison with the default schedule shows tha
    
[^218]: 数字营销内容设计的神经洞察

    Neural Insights for Digital Marketing Content Design. (arXiv:2302.01416v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01416](http://arxiv.org/abs/2302.01416)

    本文提出了一种基于神经网络的数字营销内容设计评分和提取见解系统，该系统可以为营销人员提供基于历史数据的见解和设计建议，以改善其创意过程并显着提高客户参与度。

    

    在数字营销中，尝试新的网站内容是提高客户参与度的关键杠杆之一。但是，创建成功的营销内容是一项手动且耗时的过程，缺乏明确的指导原则。本文旨在通过基于历史数据为营销人员提供基于AI驱动的可行性见解，以改进其创意过程，从而将内容创建和在线实验之间的循环闭合起来。我们提出了一个基于神经网络的系统，该系统对营销内容设计进行评分和提取见解，即多模态神经网络预测营销内容的吸引力，后处理归因方法为营销人员生成有针对性的见解，以改善特定营销位置的内容。我们的见解不仅指出了当前给定内容的优势和劣势，还根据历史数据提供了设计建议。我们表明，我们的评分模型和见解在数量和质量上均表现良好，并且可以显着提高真实网站上的客户参与度。

    In digital marketing, experimenting with new website content is one of the key levers to improve customer engagement. However, creating successful marketing content is a manual and time-consuming process that lacks clear guiding principles. This paper seeks to close the loop between content creation and online experimentation by offering marketers AI-driven actionable insights based on historical data to improve their creative process. We present a neural-network-based system that scores and extracts insights from a marketing content design, namely, a multimodal neural network predicts the attractiveness of marketing contents, and a post-hoc attribution method generates actionable insights for marketers to improve their content in specific marketing locations. Our insights not only point out the advantages and drawbacks of a given current content, but also provide design recommendations based on historical data. We show that our scoring model and insights work well both quantitatively 
    
[^219]: 鲁棒的在线主动学习策略

    Robust online active learning. (arXiv:2302.00422v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.00422](http://arxiv.org/abs/2302.00422)

    本文提出了一种自适应方法，用于鲁棒的在线主动学习，并在受污染的数据流中证明了其性能表现优异，同时确保了稳定性并减少异常值的负面影响。

    

    在许多工业应用中，获得标记的观测数据并不简单，通常需要人工专家干预或使用昂贵的测试设备。在这种情况下，主动学习可以大大提高拟合模型时最信息数据点的建议。减少模型开发所需的观测数据数量可以减轻训练所需的计算负担和标记相关的操作支出。特别是在线主动学习，在需要在极短时间内决定是否获取数据点标记的高容量生产过程中非常有用。然而，尽管最近致力于开发在线主动学习策略，但在存在异常值的情况下这些方法的行为仍未得到彻底研究。在这项工作中，我们调查了在线主动线性回归在受污染的数据流中的性能，并提出了一种自适应方法，用于鲁棒的在线主动学习，同时保证稳定性并减少异常值的负面影响。

    In many industrial applications, obtaining labeled observations is not straightforward as it often requires the intervention of human experts or the use of expensive testing equipment. In these circumstances, active learning can be highly beneficial in suggesting the most informative data points to be used when fitting a model. Reducing the number of observations needed for model development alleviates both the computational burden required for training and the operational expenses related to labeling. Online active learning, in particular, is useful in high-volume production processes where the decision about the acquisition of the label for a data point needs to be taken within an extremely short time frame. However, despite the recent efforts to develop online active learning strategies, the behavior of these methods in the presence of outliers has not been thoroughly examined. In this work, we investigate the performance of online active linear regression in contaminated data strea
    
[^220]: 基于图的时间序列异常检测：综述(arXiv：2302.00058v2 [cs.LG]更新)

    Graph-based Time-Series Anomaly Detection: A Survey. (arXiv:2302.00058v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00058](http://arxiv.org/abs/2302.00058)

    本文综述了基于图的时间序列异常检测，主要探讨了图表示学习的潜力和最先进的图异常检测技术在时间序列中的应用。

    

    随着技术的发展，许多系统持续收集大量时间序列数据，如电子商务、网络安全、车辆维护和医疗监测等领域，时间序列异常检测已成为重要的任务。但由于需要同时考虑变量内部和变量间的依赖性，这一任务非常具有挑战性。近年来，基于图的方法在解决该领域的难题方面取得了重要进展。本综述全面而最新地回顾了基于图的时间序列异常检测(G-TSAD)。首先探讨了图表示学习在时间序列数据中的巨大潜力，然后在时间序列背景下回顾了最先进的图异常检测技术，并讨论了它们的优点和缺点。最后，讨论了这些技术如何应用于实际系统中。

    With the recent advances in technology, a wide range of systems continue to collect a large amount of data over time and thus generate time series. Time-Series Anomaly Detection (TSAD) is an important task in various time-series applications such as e-commerce, cybersecurity, vehicle maintenance, and healthcare monitoring. However, this task is very challenging as it requires considering both the intra-variable dependency and the inter-variable dependency, where a variable can be defined as an observation in time series data. Recent graph-based approaches have made impressive progress in tackling the challenges of this field. In this survey, we conduct a comprehensive and up-to-date review of Graph-based TSAD (G-TSAD). First, we explore the significant potential of graph representation learning for time-series data. Then, we review state-of-the-art graph anomaly detection techniques in the context of time series and discuss their strengths and drawbacks. Finally, we discuss the technic
    
[^221]: 基于递归优化等价性的马尔可夫决策过程的遗憾边界研究

    Regret Bounds for Markov Decision Processes with Recursive Optimized Certainty Equivalents. (arXiv:2301.12601v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12601](http://arxiv.org/abs/2301.12601)

    本文提出了基于递归优化等价性的马尔可夫决策过程的风险敏感强化学习公式，设计了一种有效学习算法，并且推导了算法的遗憾上界和极小-最大下界，表明该算法实现的遗憾率对于情歌数和动作数具有最优依赖性。

    

    优化等价性（OCE）是一类风险测量，涵盖了重要的实例，如熵风险，条件风险价值和均值方差模型。在本文中，我们提出了一个新的基于表格马尔可夫决策过程和递归OCE的情节化风险敏感强化学习公式。我们设计了一个基于值迭代和上置信界的有效学习算法。我们推导了所提出算法的遗憾上界，同时建立了一个极小-最大下界。我们的界限表明，所提出的算法实现的遗憾率对于情歌数和动作数具有最优依赖性。

    The optimized certainty equivalent (OCE) is a family of risk measures that cover important examples such as entropic risk, conditional value-at-risk and mean-variance models. In this paper, we propose a new episodic risk-sensitive reinforcement learning formulation based on tabular Markov decision processes with recursive OCEs. We design an efficient learning algorithm for this problem based on value iteration and upper confidence bound. We derive an upper bound on the regret of the proposed algorithm, and also establish a minimax lower bound. Our bounds show that the regret rate achieved by our proposed algorithm has optimal dependence on the number of episodes and the number of actions.
    
[^222]: 无需图形学习的因果赌博机问题

    Causal Bandits without Graph Learning. (arXiv:2301.11401v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.11401](http://arxiv.org/abs/2301.11401)

    本文针对因果图未知的情况，提出了一种在因果赌博机问题中使用原子干预找到奖励节点父节点的有效算法，并扩展至奖励节点具有多个父节点的情况。同时，还得出了算法执行期望干预次数的精确方程，并证明在特定图形条件下，该算法可以快速执行对数。

    

    本文研究当因果图不知道的情况下的因果赌博机问题，并开发了一种使用原子干预找到奖励节点的父节点的有效算法。我们推导出算法执行的期望干预次数的精确方程，并表明，在某些图形条件下，它可以快速执行对数，或者在更一般的假设下，执行更慢但仍然是变量数的亚线性级别。我们正式表明我们的算法是最优的，因为它符合我们为执行原子干预的任何算法建立的通用下界。最后，我们将我们的算法扩展到奖励节点具有多个父节点的情况。使用这个算法和来自赌博文献的标准算法可以导致改进的遗憾界。

    We study the causal bandit problem when the causal graph is unknown and develop an efficient algorithm for finding the parent node of the reward node using atomic interventions. We derive the exact equation for the expected number of interventions performed by the algorithm and show that under certain graphical conditions it could perform either logarithmically fast or, under more general assumptions, slower but still sublinearly in the number of variables. We formally show that our algorithm is optimal as it meets the universal lower bound we establish for any algorithm that performs atomic interventions. Finally, we extend our algorithm to the case when the reward node has multiple parents. Using this algorithm together with a standard algorithm from bandit literature leads to improved regret bounds.
    
[^223]: 深度学习与稀疏正则化：信号处理的观点

    Deep Learning Meets Sparse Regularization: A Signal Processing Perspective. (arXiv:2301.09554v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.09554](http://arxiv.org/abs/2301.09554)

    本文介绍了一个新的数学框架，恰当地表征了被训练对数据拟合的神经网络的功能属性。这个框架解释了深度神经网络的一些优秀性能、网络体系结构中跳跃连接和低秩权重矩阵的使用、稀疏在神经网络中的作用以及为什么神经网络可作为优秀的特征提取器。

    

    深度学习在实践中取得了巨大成功，大多数最先进的机器学习方法都基于神经网络。然而，缺乏一个严格的数学理论，能够充分解释深度神经网络的惊人性能。本文介绍了一个相对较新的数学框架，提供了对深度学习的更深入理解的开端。这个框架恰当地表征了被训练对数据拟合的神经网络的功能属性。支持这个框架的关键数学工具包括变换域稀疏正则化、计算机断层摄影的Radon变换和逼近理论，这些技术都深深根植于信号处理。这个框架解释了加权衰减正则化在神经网络训练中的作用，网络体系结构中跳跃连接和低秩权重矩阵的使用，稀疏在神经网络中的作用，并解释了为什么神经网络可作为优秀的特征提取器。

    Deep learning has been wildly successful in practice and most state-of-the-art machine learning methods are based on neural networks. Lacking, however, is a rigorous mathematical theory that adequately explains the amazing performance of deep neural networks. In this article, we present a relatively new mathematical framework that provides the beginning of a deeper understanding of deep learning. This framework precisely characterizes the functional properties of neural networks that are trained to fit to data. The key mathematical tools which support this framework include transform-domain sparse regularization, the Radon transform of computed tomography, and approximation theory, which are all techniques deeply rooted in signal processing. This framework explains the effect of weight decay regularization in neural network training, the use of skip connections and low-rank weight matrices in network architectures, the role of sparsity in neural networks, and explains why neural networ
    
[^224]: 使用循环预测耦合物理信息神经网络在软传感建模中求解含非测量源项的PDEs

    Solving PDEs with Unmeasurable Source Terms Using Coupled Physics-Informed Neural Network with Recurrent Prediction in Soft Sensor Modeling. (arXiv:2301.08618v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08618](http://arxiv.org/abs/2301.08618)

    提出了一种带循环预测学习策略的耦合物理信息神经网络（CPINN），用于软传感建模中的含非测量源项的PDEs求解，证明了CPINN具有满足PDEs解的近似容量，提高了软传感建模在时空工业系统中的性能表现。

    

    针对含非测量源项的非齐次偏微分方程（PDEs）在软传感建模中难以很好解决的问题，本文提出了一种带循环预测（RP）学习策略的耦合物理信息神经网络（CPINN）来进行软传感建模。首先，提出了包含NetU和NetG的CPINN，其中NetU用于近似研究PDEs的解，NetG用于正则化NetU的训练，两个网络被集成到一个数据-物理混合损失函数中。随后，在理论上证明了所提出的CPINN具有满足PDEs解的近似容量。此外，我们提出了一种分层训练策略来优化和耦合两个网络以实现CPINN的参数。其次，通过在NetU中引入循环机制来进行未来状态的预测，提出了NetU-RP，以提高软传感建模在时空工业系统中的性能表现。最后，通过模拟振动位移系统的例子，演示了所提出的CPINN-RP方法在求解含非测量源项的PDEs和软传感建模应用中的有效性。

    Nonhomogeneous partial differential equations (PDEs) are an applicable model in soft sensor modeling for describing spatiotemporal industrial systems with unmeasurable source terms, which cannot be well solved by existing physics-informed neural networks (PINNs). To this end, a coupled PINN (CPINN) with a recurrent prediction (RP) learning strategy (CPINN-RP) is proposed for soft sensor modeling in spatiotemporal industrial processes, such as vibration displacement. First, CPINN containing NetU and NetG is proposed. NetU is used to approximate the solutions to PDEs under study and NetG is used to regularize the training of NetU. The two networks are integrated into a data-physics-hybrid loss function. Then, we theoretically prove that the proposed CPINN has a satisfying approximation capacity to the PDEs solutions. Besides the theoretical aspects, we propose a hierarchical training strategy to optimize and couple the two networks to achieve the parameters of CPINN. Secondly, NetU-RP is
    
[^225]: 利用扩散技术进行高保真度人脸变形攻击

    Leveraging Diffusion For Strong and High Quality Face Morphing Attacks. (arXiv:2301.04218v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.04218](http://arxiv.org/abs/2301.04218)

    本文提出了一种利用扩散技术提高图像保真度的人脸变形攻击，通过将两种特征结合提高了攻击的准确性和生物特征识别系统的易受性。

    

    人脸变形攻击旨在通过呈现由两个不同身份的生物特征组成的变形图像来欺骗人脸识别系统，以期望触发一个错误的接受，从而对生物特征系统构成重大威胁。本文提出了一种利用基于扩散的架构来改进图像视觉保真度的变形攻击，并提高变形攻击表示两种特征的能力。通过Frechet Inception Distance (FID)来评估所提出的攻击的视觉保真度，同时进行了大量实验来测量FR系统对所提出攻击的易受性。还测试了一种变形攻击检测器来检测所提出攻击的能力。

    Face morphing attacks seek to deceive a Face Recognition (FR) system by presenting a morphed image consisting of the biometric qualities from two different identities with the aim of triggering a false acceptance with one of the two identities, thereby presenting a significant threat to biometric systems. The success of a morphing attack is dependent on the ability of the morphed image to represent the biometric characteristics of both identities that were used to create the image. We present a novel morphing attack that uses a Diffusion-based architecture to improve the visual fidelity of the image and the ability of the morphing attack to represent characteristics from both identities. We demonstrate the effectiveness of the proposed attack by evaluating its visual fidelity via the Frechet Inception Distance (FID). Also, extensive experiments are conducted to measure the vulnerability of FR systems to the proposed attack. The ability of a morphing attack detector to detect the propos
    
[^226]: MSCDA: 多级语义引导对比提高了小数据集无监督域自适应的乳腺MRI分割

    MSCDA: Multi-level Semantic-guided Contrast Improves Unsupervised Domain Adaptation for Breast MRI Segmentation in Small Datasets. (arXiv:2301.02554v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2301.02554](http://arxiv.org/abs/2301.02554)

    本文提出了MSCDA框架来解决乳腺MRI分割中领域偏移的问题，其使用了多级语义引导对比和自训练方法，以对齐特征表示。通过像素对比、质心对比以及类别跨域采样策略的应用，MSCDA在小数据集无监督域自适应中表现出了优异的性能。

    

    近十年来，深度学习在乳腺磁共振成像中应用于组织分割方面受到越来越多的关注，但由于不同类型的设备、采集协议以及生物学上的异质性所导致的领域偏移仍然是实现临床应用的一个重要而具有挑战性的障碍。本文提出了一种新的多级语义引导对比域自适应（MSCDA）框架来以无监督的方式解决这个问题。我们的方法将自训练与对比学习相结合，以调整不同领域之间的特征表示。特别地，我们将像素到像素、像素到质心和质心到质心对比融入对比损失中，以更好地利用不同层次图像背后的语义信息。为了解决数据不平衡问题，我们采用类别跨域采样策略从目标图像中采样锚点，并构建混合对比学习损失。我们在三个广泛使用的乳腺MRI数据集上评估了我们的方法。实验结果表明，我们提出的方法在只有少量有标签数据可用时，优于几种最先进的无监督域自适应方法。

    Deep learning (DL) applied to breast tissue segmentation in magnetic resonance imaging (MRI) has received increased attention in the last decade, however, the domain shift which arises from different vendors, acquisition protocols, and biological heterogeneity, remains an important but challenging obstacle on the path towards clinical implementation. In this paper, we propose a novel Multi-level Semantic-guided Contrastive Domain Adaptation (MSCDA) framework to address this issue in an unsupervised manner. Our approach incorporates self-training with contrastive learning to align feature representations between domains. In particular, we extend the contrastive loss by incorporating pixel-to-pixel, pixel-to-centroid, and centroid-to-centroid contrasts to better exploit the underlying semantic information of the image at different levels. To resolve the data imbalance problem, we utilize a category-wise cross-domain sampling strategy to sample anchors from target images and build a hybri
    
[^227]: 学习最大化互信息用于动态特征选择

    Learning to Maximize Mutual Information for Dynamic Feature Selection. (arXiv:2301.00557v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.00557](http://arxiv.org/abs/2301.00557)

    本文提出一种基于互信息的动态特征选择方法，使用摊销优化进行学习。在实验中取得了比现有方法更好效果。

    

    特征选择有助于减少机器学习中的数据采集成本，但标准方法是使用静态特征子集来训练模型。本文考虑了动态特征选择（DFS）问题，即模型根据现有信息顺序查询特征。DFS通常采用强化学习方法处理，但本文探索了一种更简单的方法，即基于条件互信息贪婪地选择特征。虽然这种方法在理论上十分有吸引力，但需要对数据分布进行完全访问。因此，我们提出了一种基于摊销优化的学习方法。实验证明，该方法在训练到最优状态后可以恢复贪心策略，并且在我们的实验中优于许多现有的特征选择方法，从而验证其作为这个问题的简单而强大的解决方法。

    Feature selection helps reduce data acquisition costs in ML, but the standard approach is to train models with static feature subsets. Here, we consider the dynamic feature selection (DFS) problem where a model sequentially queries features based on the presently available information. DFS is often addressed with reinforcement learning, but we explore a simpler approach of greedily selecting features based on their conditional mutual information. This method is theoretically appealing but requires oracle access to the data distribution, so we develop a learning approach based on amortized optimization. The proposed method is shown to recover the greedy policy when trained to optimality, and it outperforms numerous existing feature selection methods in our experiments, thus validating it as a simple but powerful approach for this problem.
    
[^228]: BTS：基于半监督学习的室内两房间存在检测中的双折叠师生网络

    BTS: Bifold Teacher-Student in Semi-Supervised Learning for Indoor Two-Room Presence Detection Under Time-Varying CSI. (arXiv:2212.10802v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.10802](http://arxiv.org/abs/2212.10802)

    本文提出了一种基于半监督学习的双折叠师生网络，该网络通过利用部分标记和未标记的数据集智能地学习空间和时间特征，有效地解决了基于CSI的室内存在检测受到环境变化和有监督学习方法需要耗时标注的问题。

    

    近年来，基于有监督学习和信道状态信息（CSI）的室内人体存在检测引起了广泛的关注。然而，现有的研究依赖于CSI的空间信息，容易受到环境变化的影响，如物体移动、大气因素和机器重启，从而降低了预测精度。此外，基于有监督学习的方法需要进行耗时的标注来重新训练模型。因此，使用半监督学习方案设计一个连续监控的模型生命周期是必要的。在本文中，我们构思了一种双折叠师生（BTS）学习方法来检测存在于系统中的存在。该方法结合了半监督学习，利用部分标记和未标记的数据集。所提出的原始对偶师生网络从标记和未标记的CSI中智能地学习空间和时间特征。此外，增强的惩罚损失函数利用熵和距离测量来区分深层特征，降低噪声的影响。

    In recent years, indoor human presence detection based on supervised learning (SL) and channel state information (CSI) has attracted much attention. However, the existing studies that rely on spatial information of CSI are susceptible to environmental changes, such as object movement, atmospheric factors, and machine rebooting, which degrade prediction accuracy. Moreover, SL-based methods require time-consuming labeling for retraining models. Therefore, it is imperative to design a continuously monitored model life-cycle using a semi-supervised learning (SSL) based scheme. In this paper, we conceive a bifold teacher-student (BTS) learning approach for presence detection systems that combines SSL by utilizing partially labeled and unlabeled datasets. The proposed primal-dual teacher-student network intelligently learns spatial and temporal features from labeled and unlabeled CSI. Additionally, the enhanced penalized loss function leverages entropy and distance measures to distinguish dr
    
[^229]: 简单性偏见导致表现差异扩大

    Simplicity Bias Leads to Amplified Performance Disparities. (arXiv:2212.06641v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06641](http://arxiv.org/abs/2212.06641)

    模型在训练和测试过程中会优先考虑简单部分而非复杂部分，导致性能差异扩大，即使数据集平衡且没有群体/标签关联。困难差异是一个依赖于模型的量，并在常用模型中进一步放大。这个问题可以通过几种方法得到缓解。

    

    给定的模型将会把数据集中哪些部分视为困难？最近的研究表明，SGD训练的模型具有偏向简单的偏见，导致它们优先学习多数类，或依赖有害的表面相关性。我们在这里展示了偏好"简单"的情况远不止于此：一个模型可能会优先考虑任何它发现简单的类别或数据集中的任何组-以牺牲它发现复杂的部分的性能差异测量为代价。当不同复杂度水平的子集与人口统计学群体相一致时，我们将此称为困难差异，即使数据集是平衡的，也缺乏群体/标签关联。我们展示了困难差异是一个依赖于模型的量，并在通常的平均表现得分所选择的常用模型中进一步放大。我们量化了一个扩增因子，以便在固定数据集上比较不同模型的差异。最后，我们提出了两个建议，可以减少困难差异。

    Which parts of a dataset will a given model find difficult? Recent work has shown that SGD-trained models have a bias towards simplicity, leading them to prioritize learning a majority class, or to rely upon harmful spurious correlations. Here, we show that the preference for "easy" runs far deeper: A model may prioritize any class or group of the dataset that it finds simple-at the expense of what it finds complex-as measured by performance difference on the test set. When subsets with different levels of complexity align with demographic groups, we term this difficulty disparity, a phenomenon that occurs even with balanced datasets that lack group/label associations. We show how difficulty disparity is a model-dependent quantity, and is further amplified in commonly-used models as selected by typical average performance scores. We quantify an amplification factor across a range of settings in order to compare disparity of different models on a fixed dataset. Finally, we present two r
    
[^230]: 延迟预条件器的差分隐私自适应优化

    Differentially Private Adaptive Optimization with Delayed Preconditioners. (arXiv:2212.00309v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00309](http://arxiv.org/abs/2212.00309)

    本篇论文提出了一种称为DP^2的方法，用于解决差分隐私自适应优化中的隐私噪声问题。该方法通过使用具有延迟但噪声较小的预条件器来实现自适应性的好处，并在凸和非凸问题上提供收敛保证，经实验证明可以将收敛速度提高多达4倍。

    

    差分隐私的噪音可能会抵消在差分隐私模型训练中使用自适应优化器的好处。先前的研究通常通过使用辅助信息（例如，公共数据）来增强自适应优化的效果来解决这个问题。在本文中，我们探索了在没有辅助数据的情况下，评估和有效适应私有自适应优化的梯度几何技术。受到自适应方法可以容忍陈旧预条件器的观察启发，我们提出了延迟预条件差分隐私自适应训练（DP ^ 2），这是一种简单的方法，构建具有延迟但噪音较小的预条件器，以更好地实现自适应性的好处。理论上，我们为我们的方法提供了针对凸和非凸问题的收敛保证，并分析了延迟和隐私噪声减少之间的权衡。在实证方面，我们在几个实际数据集上探索了 DP ^ 2，证明它可以将收敛速度提高多达4倍。

    Privacy noise may negate the benefits of using adaptive optimizers in differentially private model training. Prior works typically address this issue by using auxiliary information (e.g., public data) to boost the effectiveness of adaptive optimization. In this work, we explore techniques to estimate and efficiently adapt to gradient geometry in private adaptive optimization without auxiliary data. Motivated by the observation that adaptive methods can tolerate stale preconditioners, we propose differentially private adaptive training with delayed preconditioners (DP^2), a simple method that constructs delayed but less noisy preconditioners to better realize the benefits of adaptivity. Theoretically, we provide convergence guarantees for our method for both convex and non-convex problems, and analyze trade-offs between delay and privacy noise reduction. Empirically, we explore DP^2 across several real-world datasets, demonstrating that it can improve convergence speed by as much as 4x 
    
[^231]: CRONOS：基于Wi-Fi CSI的无人设备NLoS人体检测的彩色化和对比学习

    CRONOS: Colorization and Contrastive Learning for Device-Free NLoS Human Presence Detection using Wi-Fi CSI. (arXiv:2211.10354v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.10354](http://arxiv.org/abs/2211.10354)

    本文介绍了一种名为CRONOS的系统，可以通过彩色化和对比学习来基于Wi-Fi CSI实现无人设备NLoS人体检测，可以区分房间中的移动人员和空置。实验结果表明该系统在NLoS条件下能够准确地检测出房间中的人物存在。

    

    近年来，对于全面智能化服务和应用的需求迅速增长。通过传感器或摄像头进行无人检测已被广泛采用，但存在隐私问题以及对静止人员的错误检测。为了解决这些缺点，商用Wi-Fi设备捕获的信道状态信息(CSI)提供了丰富的信号特征，以进行准确的检测。然而，现有系统在非直视(NLoS)和静态场景下存在分类不准确的问题，例如当一个人静止站在房间角落时。在本文中，我们提出了一个名为CRONOS(基于彩色化和对比度学习增强的NLoS人体存在检测)的系统，它生成动态的复发图(RPs)和颜色编码的CSI比率以区分房间中的移动人员和空置。我们还结合监督对比学习来检索实质性的表征，其中咨询损失被制定为区分同类和异类的嵌入点之间距离度量的损失。在数据集上的实验结果表明，提出的系统在NLoS条件下能够准确地检测出房间中的人物存在。

    In recent years, the demand for pervasive smart services and applications has increased rapidly. Device-free human detection through sensors or cameras has been widely adopted, but it comes with privacy issues as well as misdetection for motionless people. To address these drawbacks, channel state information (CSI) captured from commercialized Wi-Fi devices provides rich signal features for accurate detection. However, existing systems suffer from inaccurate classification under a non-line-of-sight (NLoS) and stationary scenario, such as when a person is standing still in a room corner. In this work, we propose a system called CRONOS (Colorization and Contrastive Learning Enhanced NLoS Human Presence Detection), which generates dynamic recurrence plots (RPs) and color-coded CSI ratios to distinguish mobile people from vacancy in a room, respectively. We also incorporate supervised contrastive learning to retrieve substantial representations, where consultation loss is formulated to dif
    
[^232]: 一种具有高效优化和量子适用性的费米子神经网络

    A fermion neural network with efficient optimization and quantum applicability. (arXiv:2211.05793v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2211.05793](http://arxiv.org/abs/2211.05793)

    本文提出了一种费米子神经网络（FNN），它将输入作为初始层，输出物理特性，建立了一种高效的优化方法，可应用于具有相互作用的硬量子系统，而且能够精确地确定拓扑相和紧凑电荷序，其量子特性带来多种优势。

    

    经典人工神经网络已在机器学习领域取得了广泛成功。本文提出了一种费米子神经网络（FNN），其物理特性（例如局部态密度或条件电导）在输入作为初始层后作为输出。与反向传播类似，我们建立了一种高效优化方法，使FNN在具有挑战性的机器学习基准测试上表现出竞争性能。FNN也直接应用于量子系统，包括具有相互作用的硬系统，并在无预处理或假设的情况下提供原位分析。在机器学习之后，FNN精确地确定拓扑相和紧凑电荷序。它们的量子特性也带来了各种优势：量子相关性使网络连接更加通用，并且可以深入了解消失的梯度问题，量子纠缠则为可解释的机器学习打开了新的途径等。

    Classical artificial neural networks have witnessed widespread successes in machine-learning applications. Here, we propose fermion neural networks (FNNs) whose physical properties, such as local density of states or conditional conductance, serve as outputs, once the inputs are incorporated as an initial layer. Comparable to back-propagation, we establish an efficient optimization, which entitles FNNs to competitive performance on challenging machine-learning benchmarks. FNNs also directly apply to quantum systems, including hard ones with interactions, and offer in-situ analysis without preprocessing or presumption. Following machine learning, FNNs precisely determine topological phases and emergent charge orders. Their quantum nature also brings various advantages: quantum correlation entitles more general network connectivity and insight into the vanishing gradient problem, quantum entanglement opens up novel avenues for interpretable machine learning, etc.
    
[^233]: 动态可解释的变点检测

    Dynamic Interpretable Change Point Detection. (arXiv:2211.03991v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.03991](http://arxiv.org/abs/2211.03991)

    提出了一种动态的可解释的变点检测方法 TiVaCPD，该方法使用时间变化图形套索和聚合核最大均值差异测试相结合，能够有效地捕获多维时间序列中的各种复杂变化类型。

    

    在金融和医疗保健等各个领域中，确定时间序列中的变点变得至关重要，有利于指导更好的决策，并及时应对潜在的风险或机会。现有的变点检测方法在追踪多维特征的联合分布中的变化方面具有局限性。此外，它们不能有效地在同一时间序列中进行概括，因为不同类型的变点可能需要不同的检测方法。随着多维时间序列的数量不断增长，捕获各种复杂变点的类型已经成为必不可少的。为了克服现有方法的局限性，我们提出了TiVaCPD，一种方法，它使用时间变化图形套索（TVGL）来识别多维特征之间的相关模式随时间的变化，并将其与聚合核最大均值差异（MMD）测试相结合，以识别潜在变点。

    Identifying change points (CPs) in a time series is crucial to guide better decision making across various fields like finance and healthcare and facilitating timely responses to potential risks or opportunities. Existing Change Point Detection (CPD) methods have a limitation in tracking changes in the joint distribution of multidimensional features. In addition, they fail to generalize effectively within the same time series as different types of CPs may require different detection methods. As the volume of multidimensional time series continues to grow, capturing various types of complex CPs such as changes in the correlation structure of the time-series features has become essential. To overcome the limitations of existing methods, we propose TiVaCPD, an approach that uses a Time-Varying Graphical Lasso (TVGL) to identify changes in correlation patterns between multidimensional features over time, and combines that with an aggregate Kernel Maximum Mean Discrepancy (MMD) test to iden
    
[^234]: 多智能体协作的展开图学习

    Unrolled Graph Learning for Multi-Agent Collaboration. (arXiv:2210.17101v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17101](http://arxiv.org/abs/2210.17101)

    提出一种受人类协作启发的分布式多智能体学习模型，智能体可以自主检测适合的合作者，并参考合作者的模型以获得更好的性能，使用协作图实现成对协作关系指示，通过展开图学习网络以更灵活适应各种情况地学习潜在合作者之间的相似特征。

    

    多智能体学习越来越受到关注，以应对数据交换受限的分布式机器学习场景。然而，现有的多智能体学习模型通常考虑在智能体之间固定强制性的协作关系下的数据融合，这不如人类协作那样灵活和自治。为弥补这一缺口，我们提出了一个受人类协作启发的分布式多智能体学习模型，在该模型中，智能体可以自主检测适合的合作者，并参考合作者的模型以获得更好的性能。为实现这种适应性协作，我们使用协作图来指示成对协作关系。协作图可以通过基于不同智能体之间的模型相似度的图学习技术来获得。由于模型相似性不能通过固定的图形优化来描述，我们设计了一个展开图学习网络，以更适应各种情况地学习潜在合作者之间的相似特征。

    Multi-agent learning has gained increasing attention to tackle distributed machine learning scenarios under constrictions of data exchanging. However, existing multi-agent learning models usually consider data fusion under fixed and compulsory collaborative relations among agents, which is not as flexible and autonomous as human collaboration. To fill this gap, we propose a distributed multi-agent learning model inspired by human collaboration, in which the agents can autonomously detect suitable collaborators and refer to collaborators' model for better performance. To implement such adaptive collaboration, we use a collaboration graph to indicate the pairwise collaborative relation. The collaboration graph can be obtained by graph learning techniques based on model similarity between different agents. Since model similarity can not be formulated by a fixed graphical optimization, we design a graph learning network by unrolling, which can learn underlying similar features among potent
    
[^235]: 一种量化语言模型数学推理鲁棒性的因果框架

    A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models. (arXiv:2210.12023v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.12023](http://arxiv.org/abs/2210.12023)

    该研究提出了一种基于因果框架的新方法，用于确定语言模型在数学推理任务中各种因素对输出解决方案的因果影响，研究结果显示GPT-3 Davinci模型（175B）在鲁棒性方面取得了显着改善。

    

    最近，语言模型在解决困难数学推理问题方面取得了一些令人印象深刻的成果，同时，这些模型的鲁棒性也备受质疑。最近的研究表明，当生成解决方案时，模型可能会依赖于问题描述中的浅层模式。我们提出了一个新的框架，建立在行为测试的思想基础上，它能够确定输入中各种因素，例如问题文本的表面形式、操作数和数学运算符对输出解决方案的因果影响。通过在直观推理过程中描述因果图，将行为分析根据鲁棒性和对输入空间的直接干预敏感性研究语言模型的行为。我们将这个框架应用于数学题的测试中。我们的分析显示，鲁棒性似乎并不会随着模型大小的增加而不断改善，但与较小的模型相比，GPT-3 Davinci模型（175B）在鲁棒性方面取得了显着改善。我们提供了一种系统而可解释的方法来理解语言模型在数学推理任务中的行为。

    We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution. Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution. By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems. Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramati
    
[^236]: AutoMoE：自适应计算的异构专家混合体，在神经机器翻译中实现高效。

    AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation. (arXiv:2210.07535v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.07535](http://arxiv.org/abs/2210.07535)

    AutoMoE提出了一种能够在计算约束下设计异构MoE的框架，它在神经机器翻译任务中实现了高效且最先进的性能。

    

    在神经机器翻译（NMT）任务中，专家混合体（MoE）模型获得了最先进的性能。MoE的现有工作主要考虑同质设计，其中相同数量的相同大小的专家均匀地放置在整个网络中。此外，现有的MoE工作没有考虑计算约束（例如FLOPs、延迟）来指导其设计。为此，我们开发了AutoMoE--一个在计算约束下设计异构MoE的框架。AutoMoE利用神经架构搜索（NAS）来获取高效的稀疏MoE子变压器，具有4倍推理速度优化（CPU）和FLOPs减少，相对于手动设计的Transformer，在NMT基准数据集上实现了BLEU分数的平稳性。采用密集和稀疏激活的Transformer模块的异构搜索空间（例如有多少专家？在哪里放置它们？它们的大小应该是多少？）允许更好地探索模型设计空间，最大限度地提高资源利用率。

    Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE -- a framework for designing heterogeneous MoE's under computational constraints. AutoMoE leverages Neural Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, with parity in BLEU score over dense Transformer and within 1 BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for NMT. Heterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows
    
[^237]: FARE: 具有实际证书的可证明公平表示学习

    FARE: Provably Fair Representation Learning with Practical Certificates. (arXiv:2210.07213v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07213](http://arxiv.org/abs/2210.07213)

    FARE是第一个提供实际公平性证书的FRL方法，通过限制编码器的表示空间实现实际保证并保持准确度-公平度权衡。

    

    公平性表示学习（FRL）是一种常用的方法，旨在通过数据预处理来产生公平分类器。最近的监管指令强调需要提供实际证书的FRL方法，即在任何预处理数据训练的下游分类器的不公平性上提供可证明的上限，从而直接在实际情况下提供保证。创建这样的FRL方法是一个重要的挑战，目前尚未解决。在这项工作中，我们解决了这一挑战，并引入了FARE（具有受限编码器的公平性），这是第一个具有实际公平性证书的FRL方法。FARE基于我们的关键洞见，即限制编码器的表示空间可以推导出实际的保证，同时仍然允许适当的准确度-公平度权衡，比如我们基于公平树提出的方法。为了产生实际的证书，我们开发和应用了一种统计过程，计算有限样本的h值。

    Fair representation learning (FRL) is a popular class of methods aiming to produce fair classifiers via data preprocessing. Recent regulatory directives stress the need for FRL methods that provide practical certificates, i.e., provable upper bounds on the unfairness of any downstream classifier trained on preprocessed data, which directly provides assurance in a practical scenario. Creating such FRL methods is an important challenge that remains unsolved. In this work, we address that challenge and introduce FARE (Fairness with Restricted Encoders), the first FRL method with practical fairness certificates. FARE is based on our key insight that restricting the representation space of the encoder enables the derivation of practical guarantees, while still permitting favorable accuracy-fairness tradeoffs for suitable instantiations, such as one we propose based on fair trees. To produce a practical certificate, we develop and apply a statistical procedure that computes a finite sample h
    
[^238]: 随机噪声对变分量子算法有帮助。

    Stochastic noise can be helpful for variational quantum algorithms. (arXiv:2210.06723v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2210.06723](http://arxiv.org/abs/2210.06723)

    本文证明随机性可以自然地避免变分量子算法中的严格鞍点问题，这一认识可以帮助我们更好地理解近期变分量子算法的概念。

    

    鞍点是对于一阶梯度下降算法的一个重要挑战。在经典机器学习的概念中，可以通过随机梯度下降方法避免鞍点。本文提出了证据表明，可以通过利用随机性来自然地避免变分量子算法中的鞍点问题。我们证明了收敛保证，并在数值模拟和量子硬件上提供了实际的例子。我们认为，变分算法的自然随机性可以有助于避免严格的鞍点，即至少具有一个负Hessian特征值的鞍点。这个见解表明一定程度的随机噪声可以帮助我们更好地理解近期变分量子算法的概念。

    Saddle points constitute a crucial challenge for first-order gradient descent algorithms. In notions of classical machine learning, they are avoided for example by means of stochastic gradient descent methods. In this work, we provide evidence that the saddle points problem can be naturally avoided in variational quantum algorithms by exploiting the presence of stochasticity. We prove convergence guarantees and present practical examples in numerical simulations and on quantum hardware. We argue that the natural stochasticity of variational algorithms can be beneficial for avoiding strict saddle points, i.e., those saddle points with at least one negative Hessian eigenvalue. This insight that some levels of shot noise could help is expected to add a new perspective to notions of near-term variational quantum algorithms.
    
[^239]: 随机森林对非有目标数据污染的鲁棒性：一种基于集合的方法

    On the Robustness of Random Forest Against Untargeted Data Poisoning: An Ensemble-Based Approach. (arXiv:2209.14013v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14013](http://arxiv.org/abs/2209.14013)

    本文提出了一种基于集合的方法来提高随机森林对非有目标数据污染的鲁棒性

    

    机器学习正在变得无处不在。从金融到医学，机器学习模型正在提高决策过程，甚至在某些任务中表现优于人类。然而，在模型和相应预测的安全性方面，相对预测质量取得了巨大进展的情况并没有相应的得到保证。在过去的十年中，对污染攻击和防御的研究得到了越来越多的关注，

    Machine learning is becoming ubiquitous. From finance to medicine, machine learning models are boosting decision-making processes and even outperforming humans in some tasks. This huge progress in terms of prediction quality does not however find a counterpart in the security of such models and corresponding predictions, where perturbations of fractions of the training set (poisoning) can seriously undermine the model accuracy. Research on poisoning attacks and defenses received increasing attention in the last decade, leading to several promising solutions aiming to increase the robustness of machine learning. Among them, ensemble-based defenses, where different models are trained on portions of the training set and their predictions are then aggregated, provide strong theoretical guarantees at the price of a linear overhead. Surprisingly, ensemble-based defenses, which do not pose any restrictions on the base model, have not been applied to increase the robustness of random forest mo
    
[^240]: 一项使用电子病历在重症监护室中进行 COVID-19 预测建模的全面基准测试

    A Comprehensive Benchmark for COVID-19 Predictive Modeling Using Electronic Health Records in Intensive Care. (arXiv:2209.07805v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07805](http://arxiv.org/abs/2209.07805)

    本文提出了两个针对COVID-19患者的临床预测任务：Outcome-specific length-of-stay prediction 和 Early mortality prediction，旨在填补临床应用和传统预测任务之间的差距，并提供一个基准测试框架，以实现公平比较各种模型。

    

    COVID-19 疫情给全球医疗保健系统带来了沉重的负担，造成巨大的社会破坏和经济损失。许多深度学习模型已经被提出来使用电子病历数据进行临床预测任务，例如 COVID-19 患者在重症监护室中的死亡风险预测。尽管在某些临床应用中取得了初步的成功，但目前缺乏可公平比较各种模型的基准测试结果，以便为实际临床使用选择最佳模型。此外，传统预测任务的制定与重症监护室的实际临床实践存在差异。为填补这些差距，我们提出了两个临床预测任务：针对 COVID-19 重症监护室患者的特定结果预测和早期死亡预测。这两个任务是根据天真的住院时间和死亡率预测任务进行调整的，以适应 COVID-19 的临床实践。

    The COVID-19 pandemic has posed a heavy burden to the healthcare system worldwide and caused huge social disruption and economic loss. Many deep learning models have been proposed to conduct clinical predictive tasks such as mortality prediction for COVID-19 patients in intensive care units using Electronic Health Record (EHR) data. Despite their initial success in certain clinical applications, there is currently a lack of benchmarking results to achieve a fair comparison so that we can select the optimal model for clinical use. Furthermore, there is a discrepancy between the formulation of traditional prediction tasks and real-world clinical practice in intensive care. To fill these gaps, we propose two clinical prediction tasks, Outcome-specific length-of-stay prediction and Early mortality prediction for COVID-19 patients in intensive care units. The two tasks are adapted from the naive length-of-stay and mortality prediction tasks to accommodate the clinical practice for COVID-19 
    
[^241]: 具有通用迷你批量一致性和无偏完全集合梯度近似的可扩展集合编码。

    Scalable Set Encoding with Universal Mini-Batch Consistency and Unbiased Full Set Gradient Approximation. (arXiv:2208.12401v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.12401](http://arxiv.org/abs/2208.12401)

    本文提出了一种可扩展的集合编码方法UMBC，可以与任意非MBC组件相结合，同时仍满足MBC；同时提出了一种高效的MBC训练算法，可以为任何集合大小在训练和测试时都具有恒定的内存开销，给出完整集合梯度的无偏近似。

    

    近期，关于集合函数的小批量一致性(MBC)的研究引起了人们对于保证将一个分割的集合的部分顺序处理和聚合，而保证所有分割的输出相同的需求的关注。然而，现有的MBC架构的限制导致了具有有限表达能力的模型。此外，先前的研究没有解决在需要完整集合梯度的情况下如何处理训练中的大型集合。为了解决这些问题，我们提出了一种可用于任意非-MBC组件相结合的通用MBC (UMBC) 类集合函数，同时仍满足MBC，使得MBC设置中可以使用更广泛的功能类。此外，我们提出了一种高效的MBC训练算法，它能够为任何集合大小在训练和测试时都具有恒定的内存开销，给出完整集合梯度的无偏近似。我们进行了广泛的实验，包括图像完成、文本分类、无监督聚类等。

    Recent work on mini-batch consistency (MBC) for set functions has brought attention to the need for sequentially processing and aggregating chunks of a partitioned set while guaranteeing the same output for all partitions. However, existing constraints on MBC architectures lead to models with limited expressive power. Additionally, prior work has not addressed how to deal with large sets during training when the full set gradient is required. To address these issues, we propose a Universally MBC (UMBC) class of set functions which can be used in conjunction with arbitrary non-MBC components while still satisfying MBC, enabling a wider range of function classes to be used in MBC settings. Furthermore, we propose an efficient MBC training algorithm which gives an unbiased approximation of the full set gradient and has a constant memory overhead for any set size for both train- and test-time. We conduct extensive experiments including image completion, text classification, unsupervised cl
    
[^242]: 一种混合自监督学习框架用于纵向联邦学习

    A Hybrid Self-Supervised Learning Framework for Vertical Federated Learning. (arXiv:2208.08934v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.08934](http://arxiv.org/abs/2208.08934)

    本文提出了一种名为FedHSSL的联邦混合自监督学习框架，通过跨方视角和本地视角相结合，以解决纵向联邦学习中数据不足的问题，并且可以通过部分模型聚合进一步提高联合模型的性能。

    

    纵向联邦学习（VFL）是联邦学习（FL）的一种变体，近年来备受关注。然而，传统的VFL方法可能会遭遇数据不足的问题，因为它们仅利用经过对齐和标记的样本（属于不同方），经常忽略了大多数未对齐和未标记的样本。这个数据不足阻碍了联邦一方的努力。本文提出了一种名为FedHSSL的联邦混合自监督学习框架，利用方间视角（即分散的特征）和各方内未对齐样本的本地视角（即数据增强）来提高VFL联合模型的表示学习能力。FedHSSL通过部分模型聚合进一步利用各方共同的特征来提高联合模型的性能。

    Vertical federated learning (VFL), a variant of Federated Learning (FL), has recently drawn increasing attention as the VFL matches the enterprises' demands of leveraging more valuable features to achieve better model performance. However, conventional VFL methods may run into data deficiency as they exploit only aligned and labeled samples (belonging to different parties), leaving often the majority of unaligned and unlabeled samples unused. The data deficiency hampers the effort of the federation.  In this work, we propose a Federated Hybrid Self-Supervised Learning framework, named FedHSSL, that utilizes cross-party views (i.e., dispersed features) of samples aligned among parties and local views (i.e., augmentation) of unaligned samples within each party to improve the representation learning capability of the VFL joint model. FedHSSL further exploits invariant features across parties to boost the performance of the joint model through partial model aggregation. FedHSSL, as a frame
    
[^243]: 通过步态心电图和皮肤电反应数据分类压力

    Classification of Stress via Ambulatory ECG and GSR Data. (arXiv:2208.04705v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2208.04705](http://arxiv.org/abs/2208.04705)

    本文利用机器学习分类器和生理数据对步态环境下的压力进行了检测，为个体监控心理健康提供了可能。

    

    在医疗保健领域，检测压力并使个人监控其心理健康和福祉是具有挑战性的。可穿戴技术的进步现在使连续的生理数据收集成为可能。这些数据可以通过心理生理分析提供有关心理健康和行为状态的洞察。然而，由于收集到的数据数量巨大，需要自动化分析来及时提供结果。机器学习已经显示在受控实验室环境中为健康应用提供生理数据的自动分类方面的功效。然而，无法控制的步态环境提供了额外的挑战，需要进一步建模来克服。本文从经验角度评估了使用机器学习分类器来检测压力的几种方法，这些方法使用自我报告压力注释记录的生理数据在步态环境中进行了记录。在SMILE数据集的一部分训练部分中，能够评估这些方法。

    In healthcare, detecting stress and enabling individuals to monitor their mental health and wellbeing is challenging. Advancements in wearable technology now enable continuous physiological data collection. This data can provide insights into mental health and behavioural states through psychophysiological analysis. However, automated analysis is required to provide timely results due to the quantity of data collected. Machine learning has shown efficacy in providing an automated classification of physiological data for health applications in controlled laboratory environments. Ambulatory uncontrolled environments, however, provide additional challenges requiring further modelling to overcome. This work empirically assesses several approaches utilising machine learning classifiers to detect stress using physiological data recorded in an ambulatory setting with self-reported stress annotations. A subset of the training portion SMILE dataset enables the evaluation of approaches before su
    
[^244]: 利用Retain-Resample-Release (R3)采样方法减轻物理信息神经网络中的传播失败

    Mitigating Propagation Failures in Physics-informed Neural Networks using Retain-Resample-Release (R3) Sampling. (arXiv:2207.02338v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.02338](http://arxiv.org/abs/2207.02338)

    本文提出了一种新的Retain-Resample-Release采样（R3）算法，通过减轻传播失败，使得物理信息神经网络（PINN）能够成功地从边界点传播解决方案到内部点。

    

    尽管物理信息神经网络（PINN）在近似偏微分方程（PDE）方面取得了成功，但在涉及复杂PDE的问题中，PINN有时会无法收敛到正确的解。这反映在近期关于“失败模式”特征的几项研究中，尽管缺乏关于PINN失败模式和采样策略之间连接的深入理解。在本文中，我们提出了一种新的PINN失败模式的视角，即假设训练PINN基于从初始和/或边界条件点到内部点的解“传播”的成功。我们发现，如果存在传播失败，则采用糟糕采样策略的PINN可能会卡在平凡解上，表现为高度不平衡的PDE残差场。为了减轻传播失败，我们提出了一种新的Retain-Resample-Release采样（R3）算法，该算法可以在高PD区域逐步累积重合点。

    Despite the success of physics-informed neural networks (PINNs) in approximating partial differential equations (PDEs), PINNs can sometimes fail to converge to the correct solution in problems involving complicated PDEs. This is reflected in several recent studies on characterizing the "failure modes" of PINNs, although a thorough understanding of the connection between PINN failure modes and sampling strategies is missing. In this paper, we provide a novel perspective of failure modes of PINNs by hypothesizing that training PINNs relies on successful "propagation" of solution from initial and/or boundary condition points to interior points. We show that PINNs with poor sampling strategies can get stuck at trivial solutions if there are propagation failures, characterized by highly imbalanced PDE residual fields. To mitigate propagation failures, we propose a novel Retain-Resample-Release sampling (R3) algorithm that can incrementally accumulate collocation points in regions of high PD
    
[^245]: Ask-AC: 一种循环中的主动顾问演员-评论家框架

    Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework. (arXiv:2207.01955v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.01955](http://arxiv.org/abs/2207.01955)

    本文提出了一种新颖的主动顾问演员-评论家框架，Ask-AC，它替换了传统的被动监督信号机制，实现了定制化和高效的信息交换，其中的两个互补组件允许代理主动寻求顾问干预和识别漏掉的不稳定状态。

    

    尽管交互式强化学习方案取得了很多有希望的结果，但目前的方案仍然依赖于来自顾问专家的被动监督信号，形式包括持续监控或预定义规则，这不可避免地导致了一种麻烦而昂贵的学习过程。在本文中，我们介绍了一种新的主动顾问演员-评论家框架，称为Ask-AC，它用一个双向的学习者主动机制替换了单向的顾问指导机制，从而实现了学习者和顾问之间的定制化和有效的信息交换。Ask-AC 的核心是两个互补的组件，分别是动作请求者和自适应状态选择器，可以方便地纳入各种离散的演员-评论家架构中。前者允许代理主动寻求不确定状态下的顾问干预，后者则可以识别漏掉的不稳定状态。

    Despite the promising results achieved, state-of-the-art interactive reinforcement learning schemes rely on passively receiving supervision signals from advisor experts, in the form of either continuous monitoring or pre-defined rules, which inevitably result in a cumbersome and expensive learning process. In this paper, we introduce a novel initiative advisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the unilateral advisor-guidance mechanism with a bidirectional learner-initiative one, and thereby enables a customized and efficacious message exchange between learner and advisor. At the heart of Ask-AC are two complementary components, namely action requester and adaptive state selector, that can be readily incorporated into various discrete actor-critic architectures. The former component allows the agent to initiatively seek advisor intervention in the presence of uncertain states, while the latter identifies the unstable states potentially missed by the for
    
[^246]: 基于成对比较的排名高效计算

    Efficient computation of rankings from pairwise comparisons. (arXiv:2207.00076v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.00076](http://arxiv.org/abs/2207.00076)

    本文提出了一种比已有迭代算法运行速度快100倍的基于成对比较的排名高效计算方法。

    

    本文研究了使用Bradley-Terry模型通过个体、团队或对象之间的成对比较来确定排名。该模型中排名的估计通常使用Zermelo近一百年前首次引入的简单迭代算法进行。本文提出了一种替代且同样简单的迭代方法，可证明它返回的结果是相同的，但速度更快。在一些情况下，速度可能快了一百倍。我们通过应用于一系列示例数据集来演示该算法，并导出有关其收敛性的许多结果。

    We study the ranking of individuals, teams, or objects, based on pairwise comparisons between them, using the Bradley-Terry model. Estimates of rankings within this model are commonly made using a simple iterative algorithm first introduced by Zermelo almost a century ago. Here we describe an alternative and similarly simple iteration that provably returns identical results but does so much faster -- over a hundred times faster in some cases. We demonstrate this algorithm with applications to a range of example data sets and derive a number of results regarding its convergence.
    
[^247]: 评估分子图嵌入的自监督学习

    Evaluating Self-Supervised Learning for Molecular Graph Embeddings. (arXiv:2206.08005v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.08005](http://arxiv.org/abs/2206.08005)

    评估分子图嵌入的自监督学习方法的MOLGRAPHEVAL揭示了在不同的下游任务中GSSL方法性能存在显著的不一致性，并为未来研究提供了新的方向。

    

    图形自监督学习（GSSL）为获取嵌入提供了一个强大的途径，无需专家标注，这种能力对于分子图具有深刻的影响，因为潜在分子的数量惊人，并且获取标签的成本很高。然而，GSSL方法不是为了在特定领域内进行优化，而是为了在各种下游任务之间进行可转移性。这种广泛适用性使它们的评估变得复杂。为了解决这一挑战，我们提出了“分子图表示评估”（MOLGRAPHEVAL），生成具有可解释和多样化属性的分子图嵌入的详细剖析。MOLGRAPHEVAL提供了一组探测任务，分为三类：（i）通用图形，（ii）分子亚结构和（iii）嵌入空间属性。通过利用MOLGRAPHEVAL来基准化现有的GSSL方法，对比当前的下游数据集以及我们的任务套件，我们发现GSSL方法性能存在显著的不一致性，并为未来的研究提供了新的方向。

    Graph Self-Supervised Learning (GSSL) provides a robust pathway for acquiring embeddings without expert labelling, a capability that carries profound implications for molecular graphs due to the staggering number of potential molecules and the high cost of obtaining labels. However, GSSL methods are designed not for optimisation within a specific domain but rather for transferability across a variety of downstream tasks. This broad applicability complicates their evaluation. Addressing this challenge, we present "Molecular Graph Representation Evaluation" (MOLGRAPHEVAL), generating detailed profiles of molecular graph embeddings with interpretable and diversified attributes. MOLGRAPHEVAL offers a suite of probing tasks grouped into three categories: (i) generic graph, (ii) molecular substructure, and (iii) embedding space properties. By leveraging MOLGRAPHEVAL to benchmark existing GSSL methods against both current downstream datasets and our suite of tasks, we uncover significant inco
    
[^248]: 用于理解神经网络动态的二次模型

    Quadratic models for understanding neural network dynamics. (arXiv:2205.11787v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.11787](http://arxiv.org/abs/2205.11787)

    神经二次模型可以展示出神经网络在大学习率情况下的“弹弓阶段”，并且在泛化特性上与神经网络有相似之处，是分析神经网络的有效工具。

    

    当神经网络的宽度增加时，可以用线性模型来逼近神经网络，但宽神经网络的某些特性不能被线性模型捕捉。在这项工作中，我们展示了最近提出的神经二次模型可以展示“弹弓阶段”[Lewkowycz等人，2020]，当使用大学习率训练此类模型时会出现。接着，我们经验证明，神经二次模型的行为与神经网络在泛化特性上有相似之处，尤其是在弹弓阶段范围内。我们的分析进一步表明，二次模型可以成为分析神经网络的有效工具。

    While neural networks can be approximated by linear models as their width increases, certain properties of wide neural networks cannot be captured by linear models. In this work we show that recently proposed Neural Quadratic Models can exhibit the "catapult phase" [Lewkowycz et al. 2020] that arises when training such models with large learning rates. We then empirically show that the behaviour of neural quadratic models parallels that of neural networks in generalization, especially in the catapult phase regime. Our analysis further demonstrates that quadratic models can be an effective tool for analysis of neural networks.
    
[^249]: 具有有向无环图架构的普通神经网络的线性转换

    Transition to Linearity of General Neural Networks with Directed Acyclic Graph Architecture. (arXiv:2205.11786v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.11786](http://arxiv.org/abs/2205.11786)

    本文阐明具有任意有向无环图的神经网络，在宽度无限增大的情况下有线性转化的趋势。结果揭示了转化为线性的数学结构，并推广了一系列关于标准架构神经切向核的线性转化或恒定性的最新研究。

    

    本文展示，随着其“宽度”接近无穷大，与任意有向无环图相关的前馈神经网络会发生线性转换。这些普通网络的宽度由其神经元的最小入度（除了输入和第一层之外）来刻画。我们的结果确定了转换到线性所基于的数学结构，并概括了一些旨在表征标准架构下神经切向核的线性转换或恒定性的最近研究工作。

    In this paper we show that feedforward neural networks corresponding to arbitrary directed acyclic graphs undergo transition to linearity as their "width" approaches infinity. The width of these general networks is characterized by the minimum in-degree of their neurons, except for the input and first layers. Our results identify the mathematical structure underlying transition to linearity and generalize a number of recent works aimed at characterizing transition to linearity or constancy of the Neural Tangent Kernel for standard architectures.
    
[^250]: 增强非监督图表示学习的鲁棒性：基于图信息瓶颈的视角

    Toward Enhanced Robustness in Unsupervised Graph Representation Learning: A Graph Information Bottleneck Perspective. (arXiv:2201.08557v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.08557](http://arxiv.org/abs/2201.08557)

    提出一种基于信息瓶颈原理的无偏鲁棒图信息瓶颈(RGIB)方法，尝试通过保留良性图中的原始信息并消除对抗性图中的对抗性信息，来学习对抗性扰动下的鲁棒节点表示。

    

    最近的研究表明，图神经网络(GNN)易受到对抗性攻击。大多数现有的鲁棒图学习方法基于标签信息度量模型的鲁棒性，这使得它们在没有标签信息时难以实现。一种直接的方向是利用典型的非监督图表示学习(UGRL)中广泛使用的Infomax技术来学习鲁棒的无监督表示。然而，直接将Infomax技术从典型的UGRL移植到鲁棒的UGRL可能涉及一种有偏的假设。鉴于Infomax的局限性，我们提出了一种新的无偏鲁棒UGRL方法——Robust Graph Information Bottleneck (RGIB)，它基于Information Bottleneck (IB)原理。我们的RGIB试图通过保留良性图中的原始信息并消除对抗性图中的对抗性信息，来学习对抗性扰动下的鲁棒节点表示。实现这一目标有两个主要挑战：1）如何衡量图中的原始信息和对抗信息的数量，以及2）如何在保留原始信息和消除对抗信息之间取得平衡。我们提出了一种新的目标来解决这些挑战，并表明RGIB在几个基准数据集上表现优于现有的图鲁棒性方法，包括针对性和非针对性攻击。

    Recent studies have revealed that GNNs are vulnerable to adversarial attacks. Most existing robust graph learning methods measure model robustness based on label information, rendering them infeasible when label information is not available. A straightforward direction is to employ the widely used Infomax technique from typical Unsupervised Graph Representation Learning (UGRL) to learn robust unsupervised representations. Nonetheless, directly transplanting the Infomax technique from typical UGRL to robust UGRL may involve a biased assumption. In light of the limitation of Infomax, we propose a novel unbiased robust UGRL method called Robust Graph Information Bottleneck (RGIB), which is grounded in the Information Bottleneck (IB) principle. Our RGIB attempts to learn robust node representations against adversarial perturbations by preserving the original information in the benign graph while eliminating the adversarial information in the adversarial graph. There are mainly two challeng
    
[^251]: 带有潜在状态信息共享的分散式多智能体策略梯度中的价值函数分解

    Value Functions Factorization with Latent State Information Sharing in Decentralized Multi-Agent Policy Gradients. (arXiv:2201.01247v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2201.01247](http://arxiv.org/abs/2201.01247)

    在QMIX方法的基础上，提出了LSF-SAC框架，其中包括一个潜在信息共享机制，可显著扩展价值函数分解的能力，同时在完全分散执行中保持了有效性。

    

    通过集中训练和分散执行的价值函数分解方法有望解决合作多智能体强化学习任务。在这个领域中，QMIX是一种方法，已经成为最先进的技术，并在StarCraft II微观管理基准测试中取得了最佳性能。然而，QMIX中的单个智能体估计的单调混合被认为限制了它能表示的联合动作Q值的范围，同时全局状态信息不足以进行单个智能体值函数估计，通常会导致结果次优。为此，我们提出了LSF-SAC，这是一个新颖的框架，具有基于变分推理的信息共享机制作为额外的状态信息，以辅助个体智能体在价值函数分解中。我们证明了这种潜在的个体状态信息共享可以显著扩展值函数分解的能力，同时在LSF-SAC中仍然可以通过软限制实现完全分散执行。

    Value function factorization via centralized training and decentralized execution is promising for solving cooperative multi-agent reinforcement tasks. One of the approaches in this area, QMIX, has become state-of-the-art and achieved the best performance on the StarCraft II micromanagement benchmark. However, the monotonic-mixing of per agent estimates in QMIX is known to restrict the joint action Q-values it can represent, as well as the insufficient global state information for single agent value function estimation, often resulting in suboptimality. To this end, we present LSF-SAC, a novel framework that features a variational inference-based information-sharing mechanism as extra state information to assist individual agents in the value function factorization. We demonstrate that such latent individual state information sharing can significantly expand the power of value function factorization, while fully decentralized execution can still be maintained in LSF-SAC through a soft-
    
[^252]: 多任务学习和Bandits通过健壮统计学

    Multitask Learning and Bandits via Robust Statistics. (arXiv:2112.14233v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.14233](http://arxiv.org/abs/2112.14233)

    本研究探讨了多任务学习以及Bandits方法的健壮统计学实现，提出了一种新颖的两阶段多任务学习估计器，该估计器以一种样本高效的方式利用共享全局参数和稀疏实例特定术语的结构。

    

    决策者经常同时面对许多相关但异质的学习问题。在此工作中，我们研究了一种自然的设置，其中每个学习实例中的未知参数可以分解为共享全局参数加上稀疏的实例特定术语。我们提出了一种新颖的两阶段多任务学习估计器，以一种样本高效的方式利用这种结构，使用健壮统计学（在相似实例上学习）和LASSO回归（去偏差结果）的独特组合。我们的估计器提供了改进的样本复杂度界限。

    Decision-makers often simultaneously face many related but heterogeneous learning problems. For instance, a large retailer may wish to learn product demand at different stores to solve pricing or inventory problems, making it desirable to learn jointly for stores serving similar customers; alternatively, a hospital network may wish to learn patient risk at different providers to allocate personalized interventions, making it desirable to learn jointly for hospitals serving similar patient populations. Motivated by real datasets, we study a natural setting where the unknown parameter in each learning instance can be decomposed into a shared global parameter plus a sparse instance-specific term. We propose a novel two-stage multitask learning estimator that exploits this structure in a sample-efficient way, using a unique combination of robust statistics (to learn across similar instances) and LASSO regression (to debias the results). Our estimator yields improved sample complexity bound
    
[^253]: 超越平行平面：非球形高斯混合的准多项式时间保证

    Beyond Parallel Pancakes: Quasi-Polynomial Time Guarantees for Non-Spherical Gaussian Mixtures. (arXiv:2112.05445v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.05445](http://arxiv.org/abs/2112.05445)

    本文针对具有未知均值和未知协方差的$k\geq2$的高斯组分混合物，提出了一种准多项式时间内可靠区分该混合物与纯高斯分布的算法，并且该算法仅需要在多项式下界混合重量的情况下运行。

    

    本文考虑具有未知则和未知方差（对于所有组分都相同）的$k\geq2$个高斯组分混合物，这些组分是良好分离的，即不同组分的统计重叠最多为$k^{-C}$，其中$C\geq1$为足够大的常数。我们展示了只有在允许混合权为指数级小数时才会出现这种难度，并且对于多项式下界混合重量，准多项式时间内可以实现非平凡算法保证。具体而言，我们开发了一种基于平方和方法的算法，其运行时间准多项式，并且可靠地区分一个由$k\geq2$个良好分离的高斯组分混合物和一个纯高斯分布。作为证书，该算法输出此区分的一个符号。

    We consider mixtures of $k\geq 2$ Gaussian components with unknown means and unknown covariance (identical for all components) that are well-separated, i.e., distinct components have statistical overlap at most $k^{-C}$ for a large enough constant $C\ge 1$. Previous statistical-query [DKS17] and lattice-based [BRST21, GVV22] lower bounds give formal evidence that even distinguishing such mixtures from (pure) Gaussians may be exponentially hard (in $k$).  We show that this kind of hardness can only appear if mixing weights are allowed to be exponentially small, and that for polynomially lower bounded mixing weights non-trivial algorithmic guarantees are possible in quasi-polynomial time. Concretely, we develop an algorithm based on the sum-of-squares method with running time quasi-polynomial in the minimum mixing weight. The algorithm can reliably distinguish between a mixture of $k\ge 2$ well-separated Gaussian components and a (pure) Gaussian distribution. As a certificate, the algori
    
[^254]: 桥接计算机视觉与自然语言处理间的鸿沟！一种基于梯度的文本对抗攻击框架。

    Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework. (arXiv:2110.15317v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2110.15317](http://arxiv.org/abs/2110.15317)

    该论文提出了一种针对文本对抗攻击的框架，通过在嵌入层上持续优化扰动并放大这些扰动，使用遮罩语言模型头对最终扰动的潜在代表进行解码，以获取可能的对抗样本，进行了广泛的评估，并在各种语言任务上取得了制造近乎不可察觉的通用和定向文本对抗样本的最新技术水平。

    

    尽管深度学习在各种任务上取得了最近的成功，但在小扰动的对抗样本上仍表现不佳。优化类的对抗攻击方法虽已在计算机视觉领域得到了很好的研究，但由于文本的离散性质，直接将这些方法应用于自然语言处理是不切实际的。为了解决这个问题，我们提出了一个统一的框架，将现有的视觉域优化类对抗攻击方法扩展到文本对抗样本的制造上。在这个框架中，我们通过在嵌入层上持续优化扰动，并在前向传播过程中放大这些扰动。随后，使用遮罩语言模型头对最终扰动的潜在代表进行解码，以获取可能的对抗样本。在本文中，我们使用了一种名为文本投影梯度下降（T-PGD）的攻击算法来实例化我们的框架。我们发现即使在文本领域中常见的使用代理梯度信息的情况下，我们的算法也具有很好的效果。此外，我们对各种语言任务进行了广泛的评估，包括情感分析、文本分类和命名实体识别。实验结果表明，我们提出的框架实现了制造近乎不可察觉的通用和定向文本对抗样本的最新技术水平。

    Despite recent success on various tasks, deep learning techniques still perform poorly on adversarial examples with small perturbations. While optimization-based methods for adversarial attacks are well-explored in the field of computer vision, it is impractical to directly apply them in natural language processing due to the discrete nature of the text. To address the problem, we propose a unified framework to extend the existing optimization-based adversarial attack methods in the vision domain to craft textual adversarial samples. In this framework, continuously optimized perturbations are added to the embedding layer and amplified in the forward propagation process. Then the final perturbed latent representations are decoded with a masked language model head to obtain potential adversarial samples. In this paper, we instantiate our framework with an attack algorithm named Textual Projected Gradient Descent (T-PGD). We find our algorithm effective even using proxy gradient informati
    
[^255]: 近似牛顿策略梯度算法

    Approximate Newton policy gradient algorithms. (arXiv:2110.02398v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.02398](http://arxiv.org/abs/2110.02398)

    本文提出了一种用于政策梯度算法的近似牛顿方法，包括自然策略梯度算法和全新的策略梯度算法，具有快速收敛的优势。

    

    最近几年，政策梯度算法被广泛应用于马尔可夫决策过程和强化学习问题中。常常使用各种熵函数进行正则化，以鼓励探索和提高稳定性。本文提出了一种用于熵正则化策略梯度算法的近似牛顿方法。在Shannon熵的情况下，所得到的算法复制了自然策略梯度算法。对于其他熵函数，这种方法得到了全新的策略梯度算法。我们证明了所有这些算法具有牛顿类型的二次收敛性，并且相应的梯度流全局收敛于最优解。我们使用合成和工业规模的示例来证明，所提出的近似牛顿方法通常在单位数迭代中收敛，并且往往比其他最先进的算法快几个数量级。

    Policy gradient algorithms have been widely applied to Markov decision processes and reinforcement learning problems in recent years. Regularization with various entropy functions is often used to encourage exploration and improve stability. This paper proposes an approximate Newton method for the policy gradient algorithm with entropy regularization. In the case of Shannon entropy, the resulting algorithm reproduces the natural policy gradient algorithm. For other entropy functions, this method results in brand-new policy gradient algorithms. We prove that all these algorithms enjoy Newton-type quadratic convergence and that the corresponding gradient flow converges globally to the optimal solution. We use synthetic and industrial-scale examples to demonstrate that the proposed approximate Newton method typically converges in single-digit iterations, often orders of magnitude faster than other state-of-the-art algorithms.
    
[^256]: 连续时间线性系统中的强化学习策略

    Reinforcement Learning Policies in Continuous-Time Linear Systems. (arXiv:2109.07630v3 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2109.07630](http://arxiv.org/abs/2109.07630)

    本研究旨在解决模型不确定性下连续时间线性系统最优操作的问题，提出在线策略学习最优操作，平衡探索与开发的挑战，实现飞行控制任务的有效性，并证明了在不精确的系统动态下的尖锐稳定性结果，以及次优情况下微小后悔的详细说明。

    

    符合随机微分方程的线性动态系统是经典模型。尽管已经有许多关于已知系统最优控制的文献，但在模型不确定性下，该问题在技术上非常困难且几乎没有结果。本文着手研究此问题，并旨在学习（并同时部署）最小化二次成本函数的最优操作。事实上，本研究首次全面解决了平衡连续系统的探索与开发的关键挑战。我们提出在线策略，通过谨慎地随机参数估计快速学习最优操作，并建立其性能保证：后悔界随时间的平方根乘以参数数量增长。对飞行控制任务的策略实施证明了其功效。此外，我们证明了对于不精确的系统动态，具有尖锐的稳定性结果，并严格说明了次优情况下的微小后悔。

    Linear dynamical systems that obey stochastic differential equations are canonical models. While optimal control of known systems has a rich literature, the problem is technically hard under model uncertainty and there are hardly any results. We initiate study of this problem and aim to learn (and simultaneously deploy) optimal actions for minimizing a quadratic cost function. Indeed, this work is the first that comprehensively addresses the crucial challenge of balancing exploration versus exploitation in continuous-time systems. We present online policies that learn optimal actions fast by carefully randomizing the parameter estimates, and establish their performance guarantees: a regret bound that grows with square-root of time multiplied by the number of parameters. Implementation of the policy for a flight-control task demonstrates its efficacy. Further, we prove sharp stability results for inexact system dynamics and tightly specify the infinitesimal regret caused by sub-optimal 
    
[^257]: 循环坐标双平均与外推法

    Cyclic Coordinate Dual Averaging with Extrapolation. (arXiv:2102.13244v4 [math.OC] UPDATED)

    [http://arxiv.org/abs/2102.13244](http://arxiv.org/abs/2102.13244)

    本文提出了一种新的块坐标方法，适用于变分不等式问题，并提供了收敛边界。通过使用这种方法，我们得到了关于Mahalanobis范数的梯度Lipschitz条件。这为复合凸优化问题和凸-凹极小极大优化问题的解决提供了新的工具。

    

    循环块坐标方法是一类基本的优化方法，广泛应用于实践，并作为统计学习标准软件包的一部分实现。然而，它们的收敛性通常不是很清楚，迄今为止它们的良好实践表现还没有被现有的收敛性分析所解释。在这项工作中，我们引入了一种适用于单调算子的广泛类别变分不等式（VI）问题的新的块坐标方法。该类别包括复合凸优化问题和凸-凹极小极大优化问题作为特例，并且尚未被现有工作所解决。所得到的收敛界与全梯度方法的最优收敛界相匹配，但是提供了一种基于Mahalanobis范数的新型梯度Lipschitz条件。 对于$m$坐标块，我们界限中所需的梯度 Lipschitz常数永远不会大于一个$\sqrt{m}$的系数。

    Cyclic block coordinate methods are a fundamental class of optimization methods widely used in practice and implemented as part of standard software packages for statistical learning. Nevertheless, their convergence is generally not well understood and so far their good practical performance has not been explained by existing convergence analyses. In this work, we introduce a new block coordinate method that applies to the general class of variational inequality (VI) problems with monotone operators. This class includes composite convex optimization problems and convex-concave min-max optimization problems as special cases and has not been addressed by the existing work. The resulting convergence bounds match the optimal convergence bounds of full gradient methods, but are provided in terms of a novel gradient Lipschitz condition w.r.t.~a Mahalanobis norm. For $m$ coordinate blocks, the resulting gradient Lipschitz constant in our bounds is never larger than a factor $\sqrt{m}$ compare
    
[^258]: 重新思考合作多智能体强化学习中的实现技巧和单调性约束

    Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v19 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.03479](http://arxiv.org/abs/2102.03479)

    本文研究了QMIX算法的变体和单调性约束的实现技巧，发现标准化优化对SMAC环境的表现有显著影响；单调性约束能提高SMAC和DEPP的采样效率。

    

    许多复杂的多智能体系统，如机器人集群控制和自主车辆协调，都可以被建模为多智能体强化学习（MARL）任务。QMIX是一种广泛使用的MARL算法，已被用作基准环境，例如星际争霸多智能体挑战赛（SMAC）和升级版的Predator-Prey（DEPP）。最近，QMIX的变体旨在放松QMIX的单调性约束，从而提高SMAC的性能。本文研究了这些变体的代码级优化和单调性约束。(1)我们发现这些变体的改进受到各种代码级优化的显著影响；(2)实验结果表明，带有标准化优化的QMIX在SMAC中的表现优于其他算法；(3)除了这些算法中的常识，单调性约束还可以提高SMAC和DEPP的采样效率。我们还讨论了为什么单调性约束在纯合作MARL中效果良好。

    Many complex multi-agent systems such as robot swarms control and autonomous vehicle coordination can be modeled as Multi-Agent Reinforcement Learning (MARL) tasks. QMIX, a widely popular MARL algorithm, has been used as a baseline for the benchmark environments, e.g., Starcraft Multi-Agent Challenge (SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX target relaxing the monotonicity constraint of QMIX, allowing for performance improvement in SMAC. In this paper, we investigate the code-level optimizations of these variants and the monotonicity constraint. (1) We find that such improvements of the variants are significantly affected by various code-level optimizations. (2) The experiment results show that QMIX with normalized optimizations outperforms other works in SMAC; (3) beyond the common wisdom from these works, the monotonicity constraint can improve sample efficiency in SMAC and DEPP. We also discuss why monotonicity constraints work well in purely coopera
    
[^259]: 注意力偏向的随机梯度下降

    Attentional-Biased Stochastic Gradient Descent. (arXiv:2012.06951v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2012.06951](http://arxiv.org/abs/2012.06951)

    本文提出了一种名为ABSGD的简单而有效的方法，用于解决深度学习中的数据不平衡或标签噪声问题。我们为每个样本分配一个个别重要性权重，该权重系统地与数据的损失值成比例，并且可以捕捉到每个类中个别示例之间的多样性。与现有的加权方案相比，我们的方法可以减轻计算负担并在分布鲁棒优化 (DRO) 框架中解释为正则化参数。

    

    在本文中，我们提出了一种名为ABSGD的简单而有效的方法，用于解决深度学习中的数据不平衡或标签噪声问题。我们的方法是对动量SGD的简单修改，在小批量中为每个样本分配一个个别重要性权重。样本数据的个体级别权重系统地与数据的缩放损失值的指数成比例，其中缩放因子在分布鲁棒优化 (DRO) 框架中被解释为正则化参数。根据缩放因子是正还是负，ABSGD保证收敛到信息正则化的最小值最大值或最小值最小值 DRO 问题的静态点。与现有的类级加权方案相比，我们的方法可以捕捉到每个类中个别示例之间的多样性。与需要三次反向传播的元学习使用的现有个体级加权方法相比，我们的方法可以减轻计算负担。

    In this paper, we present a simple yet effective provable method (named ABSGD) for addressing the data imbalance or label noise problem in deep learning. Our method is a simple modification to momentum SGD where we assign an individual importance weight to each sample in the mini-batch. The individual-level weight of sampled data is systematically proportional to the exponential of a scaled loss value of the data, where the scaling factor is interpreted as the regularization parameter in the framework of distributionally robust optimization (DRO). Depending on whether the scaling factor is positive or negative, ABSGD is guaranteed to converge to a stationary point of an information-regularized min-max or min-min DRO problem, respectively. Compared with existing class-level weighting schemes, our method can capture the diversity between individual examples within each class. Compared with existing individual-level weighting methods using meta-learning that require three backward propaga
    
[^260]: MALTS: 学习拉伸后匹配的算法

    MALTS: Matching After Learning to Stretch. (arXiv:1811.07415v9 [stat.ME] UPDATED)

    [http://arxiv.org/abs/1811.07415](http://arxiv.org/abs/1811.07415)

    我们提出了一个灵活的匹配算法MALTS，通过学习可解释的、基于重要协变量的距离度量，在因果推断中生成高质量的几乎精确匹配。

    

    我们介绍了一个灵活的框架，用于生成高质量的几乎精确匹配的因果推断。以往的匹配工作大多使用特定的距离度量，这通常导致低质量的匹配，特别是在存在不相关协变量的情况下。在这项工作中，我们学习一个可解释的匹配距离度量，它导致更高质量的匹配。学习到的距离度量根据每个协变量对于结果预测的贡献来拉伸协变量空间。这种拉伸意味着在重要协变量上不匹配会产生比在不相关协变量上不匹配更大的惩罚。我们学习灵活的距离度量的能力使得匹配是可解释的并且有助于估计有条件的平均处理效应。

    We introduce a flexible framework that produces high-quality almost-exact matches for causal inference. Most prior work in matching uses ad-hoc distance metrics, often leading to poor quality matches, particularly when there are irrelevant covariates. In this work, we learn an interpretable distance metric for matching, which leads to substantially higher quality matches. The learned distance metric stretches the covariate space according to each covariate's contribution to outcome prediction: this stretching means that mismatches on important covariates carry a larger penalty than mismatches on irrelevant covariates. Our ability to learn flexible distance metrics leads to matches that are interpretable and useful for the estimation of conditional average treatment effects.
    

