# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Efficient Generalized Low-Rank Tensor Contextual Bandits.](http://arxiv.org/abs/2311.01771) | 本文提出了一种新颖的广义低秩张量情境赌博算法，并引入了G-LowTESTR算法来实现探索和利用之间的权衡。 |
| [^2] | [Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data.](http://arxiv.org/abs/2311.00136) | Neuroformer是一个多模态和多任务的生成预训练模型，旨在处理系统神经科学中大规模的多模态数据。模型经过训练后能准确预测神经回路活动并推断神经回路连接性，同时能用于预测行为。 |
| [^3] | [Visible to Thermal image Translation for improving visual task in low light conditions.](http://arxiv.org/abs/2310.20190) | 本文提出了一种新的方法来改善低光条件下的视觉任务，利用可见光图像到热红外图像的翻译。通过使用生成网络和检测网络，实现了可见光图像到热红外图像的转换，并展示了该方法的可行性和效果。这对于安防和监控应用具有重要意义。 |
| [^4] | [Unpaired MRI Super Resolution with Self-Supervised Contrastive Learning.](http://arxiv.org/abs/2310.15767) | 本文提出了一种利用自监督对比学习的无配对MRI超分辨率方法，可以在有限的训练数据下提高SR性能，改善MRI分辨率。 |
| [^5] | [Training A Semantic Communication System with Federated Learning.](http://arxiv.org/abs/2310.13236) | 该论文介绍了用联邦学习训练语义通信系统的方法，通过利用用户数据而不泄露隐私的方式，减少了通信开销，并提高了网络资源利用率。 |
| [^6] | [Understanding Addition in Transformers.](http://arxiv.org/abs/2310.13121) | 本文通过对经过训练进行整数加法的单层Transformer模型的深入分析，揭示了该模型将任务分为并行的、特定于数字的流，并针对不同的数字位置采用不同的算法，同时还发现了一种罕见的高损失的使用情况。这些发现对于机制可解释性、人工智能安全性和对齐性等方面的研究具有重要贡献。 |
| [^7] | [An effective theory of collective deep learning.](http://arxiv.org/abs/2310.12802) | 通过竞争局部学习和单元之间的扩散耦合，我们引入了一个简化模型，预测了集体学习中的无序-有序-无序相变，并验证了这个理论。 |
| [^8] | [Detecting and Mitigating Algorithmic Bias in Binary Classification using Causal Modeling.](http://arxiv.org/abs/2310.12421) | 本文提出了使用因果建模来检测和减轻算法偏见的新方法，并在性别偏见和二元分类问题上进行了实证研究。通过交叉验证实验证明了因果模型在减轻性别偏见方面的有效性，并轻微提高了整体分类准确率。 |
| [^9] | [Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models.](http://arxiv.org/abs/2310.10378) | 本论文研究了多语言预训练语言模型中事实知识的跨语言一致性，提出了一种新的度量方法，并通过分析模型大小、语言配对等因素发现了影响一致性的因素。实验结果表明，增加模型大小可以提高准确性，但不会改善跨语言一致性。 |
| [^10] | [Implicit Variational Inference for High-Dimensional Posteriors.](http://arxiv.org/abs/2310.06643) | 本文提出了一种隐变分推断的方法，使用神经采样器指定隐含分布，在高维空间中近似复杂的多峰和相关后验分布。通过引入局部线性化的约束，避免了依赖额外的网络和不稳定对抗目标的问题。此外，还提出了一种新的采样器架构，首次实现了对数百万个潜变量的隐含分布。实证分析表明，该方法可以恢复大型贝叶斯神经网络中层之间的相关性，这对于网络的性能至关重要。 |
| [^11] | [Exploit the antenna response consistency to define the alignment criteria for CSI data.](http://arxiv.org/abs/2310.06328) | 本论文提出了一个解决方案，利用天线响应一致性（ARC）来定义适当的对准标准，以解决在WiFi人体活动识别中的自我监督学习算法在CSI数据上无法达到预期性能的问题。 |
| [^12] | [Understanding Deep Gradient Leakage via Inversion Influence Functions.](http://arxiv.org/abs/2309.13016) | 本文提出了一种新的方法I²F，可以有效近似深度梯度泄露攻击，并建立了恢复图像和私有梯度之间的连接。通过这个方法，我们能够更好地理解和应对深度梯度泄露攻击。 |
| [^13] | [ATMS: Algorithmic Trading-Guided Market Simulation.](http://arxiv.org/abs/2309.01784) | 本文提出了一种算法交易引导的市场模拟方法(ATMS)，通过优化提出的度量标准，该方法能够适应交易活动的序列和动态特性。实验结果表明，在半真实市场上，该方法取得了良好的效果。 |
| [^14] | [Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair.](http://arxiv.org/abs/2309.00608) | 这篇论文提出了一种框架，利用完成引擎来进一步支持大型语言模型在自动化程序修复中合成更多有效的修补程序。 |
| [^15] | [Incorporating Neuro-Inspired Adaptability for Continual Learning in Artificial Intelligence.](http://arxiv.org/abs/2308.14991) | 本研究引入了神经启发的适应性解决方案，以实现人工智能的持续学习。通过模拟果蝇学习系统，我们提出了一种可以灵活适应变化的通用方法，改善了学习可塑性，并确保解决方案的兼容性。 |
| [^16] | [Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language.](http://arxiv.org/abs/2308.05061) | 本文提出了一种使用传感器数据、方程和自然语言提示上下文中运算符学习的方法。通过整合人类知识和语言描述，该方法不仅扩展了物理信息学习的灵活性和普适性，而且显著提高了学习性能和减少了数据需求。 |
| [^17] | [Non-Convex Bilevel Optimization with Time-Varying Objective Functions.](http://arxiv.org/abs/2308.03811) | 本论文研究了在线双层优化问题，提出了一种基于窗口平均的单循环在线双层优化器（SOBOW），它能在处理函数变化和真实超梯度不可用的情况下高效更新决策。 |
| [^18] | [A theory of data variability in Neural Network Bayesian inference.](http://arxiv.org/abs/2307.16695) | 本文提出了一个泛化性的场论形式体系，用于研究神经网络在无限宽隐藏层的极限情况，并通过计算非线性和深度非线性网络的泛化特性，阐明了数据的变异性对网络行为的影响。 |
| [^19] | [Efficiently Sampling the PSD Cone with the Metric Dikin Walk.](http://arxiv.org/abs/2307.12943) | 本文通过对Dikin步行方法进行分析并适应一般度量，为带约束的PSD锥体抽样问题提供了一种有效的解决方案，并提出了优化的自共轭矩阵函数概念。 |
| [^20] | [Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment.](http://arxiv.org/abs/2307.07980) | 本文研究了在拜占庭攻击下的分布式在线学习中的对手行为。在拜占庭和恶意对手存在的情况下，尽管可以控制线性对手遗憾的常数，但分布式在线梯度下降只能实现线性对手遗憾上界。然而，在环境不完全对抗性的情况下，可以实现次线性的随机遗憾。 |
| [^21] | [FDAPT: Federated Domain-adaptive Pre-training for Language Models.](http://arxiv.org/abs/2307.06933) | FDAPT是一种联邦领域自适应预训练的方法，在保护数据隐私的同时，能够通过利用敏感和分布式数据来增强模型适应能力。对于IID和非IID情况下的下游任务，FDAPT能够维持与中央基线相竞争的性能。提出的FFDAPT算法进一步提高了计算效率，并展现出与标准FDAPT类似的下游任务性能。此外，我们也确定了这个新研究领域的有希望的未来研究方向。 |
| [^22] | [Identifiability Guarantees for Causal Disentanglement from Soft Interventions.](http://arxiv.org/abs/2307.06250) | 本文研究了从软干预中确保因果分解的可识别性。通过开发一种自编码变分贝叶斯算法，我们展示了在给定一般化的忠诚性概念的情况下，即使存在未观测到的因果变量，仍然可以恢复潜在的因果模型，并在无限数据的极限情况下预测未见组合的干预效果。 |
| [^23] | [Metropolis Sampling for Constrained Diffusion Models.](http://arxiv.org/abs/2307.05439) | 本文提出了一种基于城市采样的简单加噪方案，用于解决受约束的扩散模型的生成建模问题。相较于现有的采样器，该方案在计算效率和实证性能方面都有明显提升，并且被证明是反射布朗运动的有效离散化。 |
| [^24] | [Learning to Group Auxiliary Datasets for Molecule.](http://arxiv.org/abs/2307.04052) | 本文提出了一种名为MolGroup的方法，通过将图结构相似性和任务相似性相结合，预测每个辅助分子数据集的潜在好处，以解决合作使用辅助数据集时的负迁移问题。 |
| [^25] | [TD Convergence: An Optimization Perspective.](http://arxiv.org/abs/2306.17750) | 本研究从优化的视角研究了时差(TD)学习算法的收敛行为，在经典反例中确定了影响算法收敛或发散的两个力量，并在线性逼近和平方损失以外的情况下证明了TD的收敛性。这一研究为TD在强化学习领域的成功应用提供了理论解释。 |
| [^26] | [Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing.](http://arxiv.org/abs/2306.12929) | 本文提出了一种称为“Helper-Head”的方法，可以通过教授注意头忽略输入和输出的某些部分来消除离群值，从而实现对transformer的可量化。实验结果表明，该方法在低、高比特宽度设置上均优于现有方法，在两个流行的语言建模基准上实现了最先进的结果。 |
| [^27] | [Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset.](http://arxiv.org/abs/2306.11167) | 这项研究探索了大型语言模型（LLMs）对创造性问题解决的能力，并发现大型语言模型容易被误导，出现固定效应和Einstellung范式。 |
| [^28] | [Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?.](http://arxiv.org/abs/2306.09955) | 本论文研究了二层ReLU网络在使用梯度下降和合页损失处理噪声数据进行二分类中的良性过拟合，通过对干净数据余量的条件的确定，得出了三种不同的训练结果，能够在训练过程中对神经元动态变化做出精细描述，并发现了两个不同的训练阶段。 |
| [^29] | [TopP\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models.](http://arxiv.org/abs/2306.08013) | 本文提出了一种鲁棒可靠的生成模型评估指标TopP\&R，通过引入拓扑和统计处理进行严格的支持估计。TopP\&R仅保留具有一定置信水平的具有拓扑和统计上重要性的特征，对于噪声特征具有强大的鲁棒性，并提供了统计一致性。 |
| [^30] | [Intervention Generalization: A View from Factor Graph Models.](http://arxiv.org/abs/2306.04027) | 本文提出了一种基于因子图模型的“干预因子模型”(IFM)方法，仅基于对操纵系统分布的因子分解的最小假设，以实现从过去的实验到新的条件的跃迁。 |
| [^31] | [Decentralized SGD and Average-direction SAM are Asymptotically Equivalent.](http://arxiv.org/abs/2306.02913) | 分散SGD和平均方向SAM在渐近意义下是等价的，D-SGD表现出梯度平滑效应和锐度正则化效应，而且可以提高后验评估，并证明了潜在的泛化能力 |
| [^32] | [Training neural operators to preserve invariant measures of chaotic attractors.](http://arxiv.org/abs/2306.01187) | 本文提出了一种基于神经算子的训练框架，通过最优传输距离或KSD损失来确保神经算子能够在混沌系统上复现其统计或结构特性。 |
| [^33] | [Contrastive Training of Complex-Valued Autoencoders for Object Discovery.](http://arxiv.org/abs/2305.15001) | 介绍了一种架构修改和新颖对比学习方法，大大改进了同步性基模型，首次获得了一类同步性基模型，在多物体彩色数据集中无监督地发现物体。 |
| [^34] | [Counterfactually Comparing Abstaining Classifiers.](http://arxiv.org/abs/2305.10564) | 本文提出一种新的方法和视角来评估和比较放弃分类器，将放弃预测视为缺失数据。我们定义了放弃分类器的反事实得分，指的是分类器没有放弃预测时的预测性能期望。 |
| [^35] | [Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees.](http://arxiv.org/abs/2305.01588) | 本文提出了针对梯度剪切的收敛保证机制，不再需要特定的阈值和强噪声假设，同时可以独立于步长选择，从而提高了收敛的自由度。 |
| [^36] | [MLRegTest: A Benchmark for the Machine Learning of Regular Languages.](http://arxiv.org/abs/2304.07687) | 本文提出了一个名为MLRegTest的新基准测试，其包含了来自1,800个正则语言的数据集。该测试根据逻辑复杂度和逻辑文字种类组织语言，并可以帮助我们了解机器学习系统在学习不同种类的长距离依赖方面的性能。 |
| [^37] | [Variational operator learning: A unified paradigm for training neural operators and solving partial differential equations.](http://arxiv.org/abs/2304.04234) | 本文提出了变分算子学习（VOL）的范式，同时训练神经算子和解决偏微分方程（PDE）。使用正反传递循环和自动微分实现了变分操作，通过最速下降法和共轭梯度法进行神经算子的简单但有效的训练。实验结果非常好。 |
| [^38] | [PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining.](http://arxiv.org/abs/2303.08789) | PLEX提出了一种新的机器人操纵预训练方法，利用任务不可知的视觉运动轨迹和大量的任务条件下的物体操作视频，在学习通用的操纵例程的同时，通过视频演示学习如何在这个特征空间中规划各种任务。 |
| [^39] | [Penalising the biases in norm regularisation enforces sparsity.](http://arxiv.org/abs/2303.01353) | 本研究表明，控制神经网络参数的范数可以获得良好的泛化性能。对神经网络中偏差项的范数进行惩罚可以实现稀疏估计量。 |
| [^40] | [Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive.](http://arxiv.org/abs/2301.12534) | 本文通过人工和机器审核员的共情冒犯和噪声审计研究了冒犯性言论检测中的差异性。结果表明，审核员之间存在广泛的分歧，并且人工审核员和大型语言模型分类器无法预测其他审核员的回应。这对于内容审核具有重要意义。 |
| [^41] | [Efficient Activation Function Optimization through Surrogate Modeling.](http://arxiv.org/abs/2301.05785) | 本文提出了一种基于代理建模的方法，通过扩展的基准测试空间在较少的函数评估次数中发现了优化的高效激活函数架构，并在多个标准基准测试中实现了最先进的性能。 |
| [^42] | [Why Batch Normalization Damage Federated Learning on Non-IID Data?.](http://arxiv.org/abs/2301.02982) | 本文通过首次的收敛性分析发现，非独立同分布数据中，在批归一化中局部和全局统计参数不匹配导致了梯度偏差，从而影响了联邦学习的收敛性。 |
| [^43] | [Optimization on Manifolds via Graph Gaussian Processes.](http://arxiv.org/abs/2210.10962) | 本论文结合流形学习和高斯过程，利用流形样本点云定义图高斯过程代理模型，通过选择查询点在流形上优化目标函数，具有良好性能。 |
| [^44] | [Principled Pruning of Bayesian Neural Networks through Variational Free Energy Minimization.](http://arxiv.org/abs/2210.09134) | 本文提出了一种基于变分自由能最小化的贝叶斯模型简化方法，用于对贝叶斯神经网络进行原则性剪枝。通过引入迭代剪枝算法，解决了直接应用贝叶斯模型简化的近似误差问题，并在实验证明了该方法的有效性和优势。 |
| [^45] | [Automatically Score Tissue Images Like a Pathologist by Transfer Learning.](http://arxiv.org/abs/2209.05954) | 该算法通过选择性迁移学习从多个小辅助集中提取知识，从具有“相似”特征的组织图像中学习染色模式，以实现像病理学家一样自动评分组织图像的目标。 |
| [^46] | [A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs.](http://arxiv.org/abs/2206.04798) | 本论文提出了A*Net，一种基于路径的知识图谱推理方法，通过学习优先级函数，实现了对重要节点和边的选择，从而提高了训练和推理的效率。实验证明A*Net在性能和效率之间取得了平衡，在知识图谱推理中具有竞争力，并在大规模数据集上获得了新的最先进结果。 |
| [^47] | [Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation.](http://arxiv.org/abs/2204.07028) | 本文提出了一种基于分布式知识一致性的无代理数据联邦蒸馏算法，解决了客户端模型异质性引起的知识差异问题，从而提高了模型表示的准确性。 |
| [^48] | [Comparing Sequential Forecasters.](http://arxiv.org/abs/2110.00115) | 本文提出了一种比较序贯预测器的方法，通过设计新的序贯推断程序来估计预测得分的时变差异，这种方法避免了对预测和结果生成方式的不可验证假设。 |
| [^49] | [On the approximation capability of GNNs in node classification/regression tasks.](http://arxiv.org/abs/2106.08992) | 本文研究GNN在节点分类/回归任务中的逼近能力，发现在概率上GNN对于任何有限域上的可测函数都是通用逼近器。我们的研究还揭示了GNN的表达能力与其深度和宽度的变化有关，并且结果可扩展用于大型图。 |
| [^50] | [Simple steps are all you need: Frank-Wolfe and generalized self-concordant functions.](http://arxiv.org/abs/2105.13913) | 本论文介绍了一种简单的Frank-Wolfe算法变体，利用广义自协调函数的特性，在不需要使用二阶信息或估计局部平滑度参数的情况下，以$\mathcal{O}(1/t)$的收敛速度达到了优化目标。 |

# 详细

[^1]: 高效的广义低秩张量情境赌博算法

    Efficient Generalized Low-Rank Tensor Contextual Bandits. (arXiv:2311.01771v1 [cs.LG])

    [http://arxiv.org/abs/2311.01771](http://arxiv.org/abs/2311.01771)

    本文提出了一种新颖的广义低秩张量情境赌博算法，并引入了G-LowTESTR算法来实现探索和利用之间的权衡。

    

    本文旨在构建一种新颖的赌博算法，能够充分利用多维数据和奖励函数的固有非线性特性，提供高可用和负责任的决策服务。为此，我们引入了一种广义低秩张量情境赌博模型，其中一个动作由三个特征向量组成，因此可以用张量表示。在这个模型中，奖励是通过将动作的特征张量与一个固定但未知的参数张量的内积应用于广义线性函数来确定的，而这个参数张量具有较低的管状秩。为了实现探索和利用之间的权衡，我们引入了一种名为“广义低秩张量探索子空间然后细化”的新算法（G-LowTESTR）。该算法首先收集原始数据，以探索嵌入在决策情境中的本质低秩张量子空间信息，然后将原始概率转换为可解释的结构化概率。

    In this paper, we aim to build a novel bandits algorithm that is capable of fully harnessing the power of multi-dimensional data and the inherent non-linearity of reward functions to provide high-usable and accountable decision-making services. To this end, we introduce a generalized low-rank tensor contextual bandits model in which an action is formed from three feature vectors, and thus can be represented by a tensor. In this formulation, the reward is determined through a generalized linear function applied to the inner product of the action's feature tensor and a fixed but unknown parameter tensor with a low tubal rank. To effectively achieve the trade-off between exploration and exploitation, we introduce a novel algorithm called "Generalized Low-Rank Tensor Exploration Subspace then Refine" (G-LowTESTR). This algorithm first collects raw data to explore the intrinsic low-rank tensor subspace information embedded in the decision-making scenario, and then converts the original prob
    
[^2]: Neuroformer：用于脑数据的多模态和多任务生成预训练模型

    Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data. (arXiv:2311.00136v1 [q-bio.NC])

    [http://arxiv.org/abs/2311.00136](http://arxiv.org/abs/2311.00136)

    Neuroformer是一个多模态和多任务的生成预训练模型，旨在处理系统神经科学中大规模的多模态数据。模型经过训练后能准确预测神经回路活动并推断神经回路连接性，同时能用于预测行为。

    

    最先进的系统神经科学实验产生了大规模的多模态数据，这些数据集需要新的分析工具。受到视觉和语言领域大规模预训练模型成功的启发，我们将大规模的细胞分辨率神经元尖峰数据的分析重新构建为一个自回归的时空生成问题。Neuroformer是一个多模态、多任务的生成预训练transformer（GPT）模型，专为处理系统神经科学数据的复杂性而设计。它与特征大小呈线性扩展，并且可以处理任意数量的模态，适应下游任务，比如预测行为。我们首先在模拟数据集上训练了Neuroformer，并发现它既能准确预测模拟神经回路活动，也能内在地推断出底层神经回路连接性，包括方向。当预训练用于解码神经响应时，该模型能预测小鼠行为。

    State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an autoregressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pretrained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mo
    
[^3]: 低光条件下改善视觉任务的可见光到热红外图像翻译

    Visible to Thermal image Translation for improving visual task in low light conditions. (arXiv:2310.20190v1 [cs.CV])

    [http://arxiv.org/abs/2310.20190](http://arxiv.org/abs/2310.20190)

    本文提出了一种新的方法来改善低光条件下的视觉任务，利用可见光图像到热红外图像的翻译。通过使用生成网络和检测网络，实现了可见光图像到热红外图像的转换，并展示了该方法的可行性和效果。这对于安防和监控应用具有重要意义。

    

    在低光条件下，通过使用可见光图像很难完成一些视觉任务，如行人检测和图像翻译。热红外图像中物体的热变化可以用来克服这个问题。本文提出了一个端到端的框架，包括一个生成网络和一个检测网络，用于将可见光图像翻译成热红外图像，并将生成的热红外图像与真实数据进行比较。我们使用Parrot Anafi Thermal无人机在两个不同位置收集了图像。然后，我们创建了一个双流网络，对图像数据进行了预处理、增强，从头开始训练了生成器和鉴别器模型。研究结果表明，使用GAN将可见光训练数据转换为热红外数据是可行的。因此，现在可以更快速、更经济地生成热红外数据，这对于安防和监控应用非常有用。

    Several visual tasks, such as pedestrian detection and image-to-image translation, are challenging to accomplish in low light using RGB images. Heat variation of objects in thermal images can be used to overcome this. In this work, an end-to-end framework, which consists of a generative network and a detector network, is proposed to translate RGB image into Thermal ones and compare generated thermal images with real data. We have collected images from two different locations using the Parrot Anafi Thermal drone. After that, we created a two-stream network, preprocessed, augmented, the image data, and trained the generator and discriminator models from scratch. The findings demonstrate that it is feasible to translate RGB training data to thermal data using GAN. As a result, thermal data can now be produced more quickly and affordably, which is useful for security and surveillance applications.
    
[^4]: 无配对MRI超分辨率与自监督对比学习

    Unpaired MRI Super Resolution with Self-Supervised Contrastive Learning. (arXiv:2310.15767v1 [eess.IV])

    [http://arxiv.org/abs/2310.15767](http://arxiv.org/abs/2310.15767)

    本文提出了一种利用自监督对比学习的无配对MRI超分辨率方法，可以在有限的训练数据下提高SR性能，改善MRI分辨率。

    

    高分辨率（HR）磁共振成像（MRI）在临床环境中提高诊断准确性至关重要。然而，MRI分辨率的固有限制限制了其广泛应用。基于深度学习的图像超分辨率（SR）方法展现了提升MRI分辨率的潜力，而无需额外成本。然而，这些方法通常需要大量HR MRI图像进行训练，而这可能难以获取。在本文中，我们提出了一种无配对MRI SR方法，利用自监督对比学习来提高有限训练数据下的SR性能。我们的方法利用真实的HR图像和人工生成的SR图像构建正负样本对，从而促进辨别性特征的学习。本研究呈现的实证结果突出了峰值信噪比和结构相似性指数的显著提高，即使缺乏HR图像数据。

    High-resolution (HR) magnetic resonance imaging (MRI) is crucial for enhancing diagnostic accuracy in clinical settings. Nonetheless, the inherent limitation of MRI resolution restricts its widespread applicability. Deep learning-based image super-resolution (SR) methods exhibit promise in improving MRI resolution without additional cost. However, these methods frequently require a substantial number of HR MRI images for training, which can be challenging to acquire. In this paper, we propose an unpaired MRI SR approach that employs self-supervised contrastive learning to enhance SR performance with limited training data. Our approach leverages both authentic HR images and synthetically generated SR images to construct positive and negative sample pairs, thus facilitating the learning of discriminative features. Empirical results presented in this study underscore significant enhancements in the peak signal-to-noise ratio and structural similarity index, even when a paucity of HR image
    
[^5]: 用联邦学习训练语义通信系统

    Training A Semantic Communication System with Federated Learning. (arXiv:2310.13236v1 [cs.LG])

    [http://arxiv.org/abs/2310.13236](http://arxiv.org/abs/2310.13236)

    该论文介绍了用联邦学习训练语义通信系统的方法，通过利用用户数据而不泄露隐私的方式，减少了通信开销，并提高了网络资源利用率。

    

    语义通信因其减少数据冗余的功能成为下一代通信系统的重要支柱。大多数语义通信系统使用先进的深度学习模型构建，其性能严重依赖于数据的可用性。这些研究假设有丰富的训练数据可用，这是不现实的。实际上，数据主要是由用户生成的。由于隐私和安全问题，数据传输受到限制，这对于传统的集中式训练方案是必要的。为了解决这个挑战，我们在联邦学习（FL）环境中探索语义通信，利用用户数据而不泄露隐私。此外，我们设计了一个系统来解决通信开销问题，通过减少每个全局轮次传送的信息量。这样，我们可以为资源受限的设备节省大量带宽，减少整体网络流量。最后，我们提出了...

    Semantic communication has emerged as a pillar for the next generation of communication systems due to its capabilities in alleviating data redundancy. Most semantic communication systems are built using advanced deep learning models whose performance heavily depends on data availability. These studies assume that an abundance of training data is available, which is unrealistic. In practice, data is mainly created on the user side. Due to privacy and security concerns, the transmission of data is restricted, which is necessary for conventional centralized training schemes. To address this challenge, we explore semantic communication in federated learning (FL) setting that utilizes user data without leaking privacy. Additionally, we design our system to tackle the communication overhead by reducing the quantity of information delivered in each global round. In this way, we can save significant bandwidth for resource-limited devices and reduce overall network traffic. Finally, we propose
    
[^6]: 理解Transformer中的加法

    Understanding Addition in Transformers. (arXiv:2310.13121v1 [cs.LG])

    [http://arxiv.org/abs/2310.13121](http://arxiv.org/abs/2310.13121)

    本文通过对经过训练进行整数加法的单层Transformer模型的深入分析，揭示了该模型将任务分为并行的、特定于数字的流，并针对不同的数字位置采用不同的算法，同时还发现了一种罕见的高损失的使用情况。这些发现对于机制可解释性、人工智能安全性和对齐性等方面的研究具有重要贡献。

    

    了解像Transformer这样的机器学习模型的内部工作方式对于其安全和道德使用至关重要。本文对经过训练进行整数加法的单层Transformer模型进行了深入分析。我们揭示了该模型将任务分为并行的、特定于数字的流，并针对不同的数字位置采用不同的算法。我们的研究还发现该模型开始计算较晚，但执行速度非常快。我们还发现了一种罕见的高损失的使用情况，并予以解释。总体而言，我们详细解释了该模型的算法。这些发现通过严格测试和数学建模得到了验证，对于机制可解释性、人工智能安全性和对齐性等广泛研究做出了贡献。我们的方法为分析更复杂的任务和多层Transformer模型打开了大门。

    Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.
    
[^7]: 一种集体深度学习的有效理论

    An effective theory of collective deep learning. (arXiv:2310.12802v1 [physics.soc-ph])

    [http://arxiv.org/abs/2310.12802](http://arxiv.org/abs/2310.12802)

    通过竞争局部学习和单元之间的扩散耦合，我们引入了一个简化模型，预测了集体学习中的无序-有序-无序相变，并验证了这个理论。

    

    揭示耦合的人工神经网络系统中集体学习的出现是对物理学、机器学习、神经科学和社会学的广泛影响的一项努力。我们引入了一个简化模型，通过考虑各个神经网络单元参数的局部学习动态和单元之间的扩散耦合之间的竞争，将几个最近的分散算法进行了压缩。我们通过一个与具有淬灭随机性的Ginzburg-Landau模型类似的线性网络的有效理论，推导出了我们模型的粗粒化行为。这个框架预测了参数解的（深度依赖的）无序-有序-无序相变，揭示了集体学习相的开始，以及深度引起的临界点延迟和微观学习路径的鲁棒形状。我们在现实中验证了我们的理论。

    Unraveling the emergence of collective learning in systems of coupled artificial neural networks is an endeavor with broader implications for physics, machine learning, neuroscience and society. Here we introduce a minimal model that condenses several recent decentralized algorithms by considering a competition between two terms: the local learning dynamics in the parameters of each neural network unit, and a diffusive coupling among units that tends to homogenize the parameters of the ensemble. We derive the coarse-grained behavior of our model via an effective theory for linear networks that we show is analogous to a deformed Ginzburg-Landau model with quenched disorder. This framework predicts (depth-dependent) disorder-order-disorder phase transitions in the parameters' solutions that reveal the onset of a collective learning phase, along with a depth-induced delay of the critical point and a robust shape of the microscopic learning path. We validate our theory in realistic ensembl
    
[^8]: 使用因果建模检测和减轻二元分类中的算法偏见

    Detecting and Mitigating Algorithmic Bias in Binary Classification using Causal Modeling. (arXiv:2310.12421v1 [cs.LG])

    [http://arxiv.org/abs/2310.12421](http://arxiv.org/abs/2310.12421)

    本文提出了使用因果建模来检测和减轻算法偏见的新方法，并在性别偏见和二元分类问题上进行了实证研究。通过交叉验证实验证明了因果模型在减轻性别偏见方面的有效性，并轻微提高了整体分类准确率。

    

    本文提出使用因果建模来检测和减轻算法偏见。我们简要介绍了因果建模的概念并概述了我们的方法。我们使用了UC Irvine机器学习库中可下载的成年人数据集，分别开发了（1）一个被视为黑箱的预测模型和（2）一个用于减轻偏见的因果模型。本文重点关注性别偏见和二元分类问题。我们展示了预测模型中性别偏见的统计显著性（p<0.05），并通过交叉验证展示了因果模型在减轻性别偏见方面的有效性。此外，我们还展示了整体分类准确率的轻微提升。我们的创新方法直观易懂，并且可以使用现有统计软件工具（如R中的“lavaan”）实现。因此，它提高了可解释性并促进了信任。

    This paper proposes the use of causal modeling to detect and mitigate algorithmic bias. We provide a brief description of causal modeling and a general overview of our approach. We then use the Adult dataset, which is available for download from the UC Irvine Machine Learning Repository, to develop (1) a prediction model, which is treated as a black box, and (2) a causal model for bias mitigation. In this paper, we focus on gender bias and the problem of binary classification. We show that gender bias in the prediction model is statistically significant at the 0.05 level. We demonstrate the effectiveness of the causal model in mitigating gender bias by cross-validation. Furthermore, we show that the overall classification accuracy is improved slightly. Our novel approach is intuitive, easy-to-use, and can be implemented using existing statistical software tools such as "lavaan" in R. Hence, it enhances explainability and promotes trust.
    
[^9]: 跨语言多语言模型中事实知识的跨语言一致性

    Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10378](http://arxiv.org/abs/2310.10378)

    本论文研究了多语言预训练语言模型中事实知识的跨语言一致性，提出了一种新的度量方法，并通过分析模型大小、语言配对等因素发现了影响一致性的因素。实验结果表明，增加模型大小可以提高准确性，但不会改善跨语言一致性。

    

    多语言大规模预训练语言模型（PLM）显示存储了大量的事实知识，但在不同语言之间存在较大的变化。为了确保不同语言背景的用户从同一个模型中获得一致的反馈，我们研究了各种多语言PLM中事实知识的跨语言一致性（CLC）。为此，我们提出了一种基于排序的一致性（RankC）度量，用于独立于准确性评估跨语言间的知识一致性。利用这个度量方法，我们对决定CLC的因素进行了深入分析，包括模型层面和语言对层面。在其他结果中，我们发现增加模型大小可以提高大多数语言中的事实探测准确性，但不能改善跨语言一致性。最后，我们通过模型编辑在PLMs中插入新的事实关联进行了一个CLC的案例研究。对一小部分事实进行了实验。

    Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts 
    
[^10]: 高维后验推断的隐变分推断

    Implicit Variational Inference for High-Dimensional Posteriors. (arXiv:2310.06643v1 [cs.LG])

    [http://arxiv.org/abs/2310.06643](http://arxiv.org/abs/2310.06643)

    本文提出了一种隐变分推断的方法，使用神经采样器指定隐含分布，在高维空间中近似复杂的多峰和相关后验分布。通过引入局部线性化的约束，避免了依赖额外的网络和不稳定对抗目标的问题。此外，还提出了一种新的采样器架构，首次实现了对数百万个潜变量的隐含分布。实证分析表明，该方法可以恢复大型贝叶斯神经网络中层之间的相关性，这对于网络的性能至关重要。

    

    在变分推断中，贝叶斯模型的好处在于准确捕捉真实的后验分布。我们提出使用指定隐含分布的神经采样器，这对于近似高维空间中复杂多峰和相关后验分布非常适用。我们的方法通过局部线性化神经采样器引入新的约束，这与现有方法不同，现有方法依赖于额外的鉴别器网络和不稳定的对抗目标。此外，我们提出了一种新的采样器架构，首次实现了对数百万个潜变量的隐含分布，通过使用可微分的数值近似来解决计算上的问题。我们的实证分析表明，我们的方法能够在大型贝叶斯神经网络中恢复层之间的相关性，这是网络性能关键但臭名昭著的属性。

    In variational inference, the benefits of Bayesian models rely on accurately capturing the true posterior distribution. We propose using neural samplers that specify implicit distributions, which are well-suited for approximating complex multimodal and correlated posteriors in high-dimensional spaces. Our approach advances inference using implicit distributions by introducing novel bounds that come about by locally linearising the neural sampler. This is distinct from existing methods that rely on additional discriminator networks and unstable adversarial objectives. Furthermore, we present a new sampler architecture that, for the first time, enables implicit distributions over millions of latent variables, addressing computational concerns by using differentiable numerical approximations. Our empirical analysis indicates our method is capable of recovering correlations across layers in large Bayesian neural networks, a property that is crucial for a network's performance but notorious
    
[^11]: 利用天线响应一致性定义CSI数据的对准标准

    Exploit the antenna response consistency to define the alignment criteria for CSI data. (arXiv:2310.06328v1 [cs.LG])

    [http://arxiv.org/abs/2310.06328](http://arxiv.org/abs/2310.06328)

    本论文提出了一个解决方案，利用天线响应一致性（ARC）来定义适当的对准标准，以解决在WiFi人体活动识别中的自我监督学习算法在CSI数据上无法达到预期性能的问题。

    

    自我监督学习（SSL）用于基于WiFi的人体活动识别（HAR）由于能够解决标注数据不足的挑战而具有很大的潜力。然而，直接将原本设计用于其他领域的SSL算法，特别是对比学习，移植到CSI数据上往往无法达到预期的性能。我们将这个问题归因于对准标准不当，这破坏了特征空间和输入空间之间的语义距离一致性。为了解决这个挑战，我们引入了``Anetenna Response Consistency (ARC)''作为定义合适对准标准的解决方案。ARC的设计在保留输入空间的语义信息的同时，引入了对现实世界噪声的鲁棒性。我们从CSI数据结构的角度分析了ARC，并展示了其最优解导致了从输入CSI数据到特征映射中的动作向量的直接映射。

    Self-supervised learning (SSL) for WiFi-based human activity recognition (HAR) holds great promise due to its ability to address the challenge of insufficient labeled data. However, directly transplanting SSL algorithms, especially contrastive learning, originally designed for other domains to CSI data, often fails to achieve the expected performance. We attribute this issue to the inappropriate alignment criteria, which disrupt the semantic distance consistency between the feature space and the input space. To address this challenge, we introduce \textbf{A}netenna \textbf{R}esponse \textbf{C}onsistency (ARC) as a solution to define proper alignment criteria. ARC is designed to retain semantic information from the input space while introducing robustness to real-world noise. We analyze ARC from the perspective of CSI data structure, demonstrating that its optimal solution leads to a direct mapping from input CSI data to action vectors in the feature map. Furthermore, we provide extensi
    
[^12]: 通过反演影响函数理解深度梯度泄露

    Understanding Deep Gradient Leakage via Inversion Influence Functions. (arXiv:2309.13016v1 [cs.LG])

    [http://arxiv.org/abs/2309.13016](http://arxiv.org/abs/2309.13016)

    本文提出了一种新的方法I²F，可以有效近似深度梯度泄露攻击，并建立了恢复图像和私有梯度之间的连接。通过这个方法，我们能够更好地理解和应对深度梯度泄露攻击。

    

    深度梯度泄露（DGL）是一种非常有效的攻击方法，可以从梯度向量中恢复私有训练图像。这种攻击对于具有敏感数据的客户端分布式学习提出了重要的隐私挑战，其中客户端需要共享梯度。防御此类攻击需要但缺乏对隐私泄露发生的时间和方式的理解，主要是因为深度网络的黑盒特性。在本文中，我们提出了一种新颖的反演影响函数（I²F），通过隐式解决DGL问题，建立了恢复图像和私有梯度之间的闭式连接。与直接解决DGL相比，I²F在分析深度网络时具有可扩展性，仅需要梯度和雅可比向量乘积的预言访问。我们通过实验证明，I²F在不同的模型架构、数据集、攻击实现和基于噪声的防御中都能有效近似DGL。我们通过这种新颖的工具，能够更好地了解深度梯度泄露的机理和应对方法。

    Deep Gradient Leakage (DGL) is a highly effective attack that recovers private training images from gradient vectors. This attack casts significant privacy challenges on distributed learning from clients with sensitive data, where clients are required to share gradients. Defending against such attacks requires but lacks an understanding of when and how privacy leakage happens, mostly because of the black-box nature of deep networks. In this paper, we propose a novel Inversion Influence Function (I$^2$F) that establishes a closed-form connection between the recovered images and the private gradients by implicitly solving the DGL problem. Compared to directly solving DGL, I$^2$F is scalable for analyzing deep networks, requiring only oracle access to gradients and Jacobian-vector products. We empirically demonstrate that I$^2$F effectively approximated the DGL generally on different model architectures, datasets, attack implementations, and noise-based defenses. With this novel tool, we 
    
[^13]: ATMS: 算法交易引导的市场模拟

    ATMS: Algorithmic Trading-Guided Market Simulation. (arXiv:2309.01784v1 [cs.LG])

    [http://arxiv.org/abs/2309.01784](http://arxiv.org/abs/2309.01784)

    本文提出了一种算法交易引导的市场模拟方法(ATMS)，通过优化提出的度量标准，该方法能够适应交易活动的序列和动态特性。实验结果表明，在半真实市场上，该方法取得了良好的效果。

    

    有效构建算法交易策略通常依赖于市场模拟器，然而现有方法很难适应交易活动的序列和动态特性。本文通过提出一个衡量市场差异的度量标准来填补这一空白。该度量标准通过算法交易代理和市场之间的交互来评估底层市场的因果效应差异。最重要的是，我们引入了算法交易引导的市场模拟(ATMS)，通过优化我们提出的度量标准。受SeqGAN的启发，ATMS将模拟器形式化为强化学习中的随机策略，以考虑交易的序列特性。此外，ATMS利用策略梯度更新来绕过对提出的度量标准的微分，这涉及到非可微分操作，如从市场中删除订单。通过在半真实市场上进行大量的实验

    The effective construction of an Algorithmic Trading (AT) strategy often relies on market simulators, which remains challenging due to existing methods' inability to adapt to the sequential and dynamic nature of trading activities. This work fills this gap by proposing a metric to quantify market discrepancy. This metric measures the difference between a causal effect from underlying market unique characteristics and it is evaluated through the interaction between the AT agent and the market. Most importantly, we introduce Algorithmic Trading-guided Market Simulation (ATMS) by optimizing our proposed metric. Inspired by SeqGAN, ATMS formulates the simulator as a stochastic policy in reinforcement learning (RL) to account for the sequential nature of trading. Moreover, ATMS utilizes the policy gradient update to bypass differentiating the proposed metric, which involves non-differentiable operations such as order deletion from the market. Through extensive experiments on semi-real marke
    
[^14]: Copiloting the Copilots: 将大型语言模型与完成引擎融合用于自动化程序修复

    Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair. (arXiv:2309.00608v1 [cs.SE])

    [http://arxiv.org/abs/2309.00608](http://arxiv.org/abs/2309.00608)

    这篇论文提出了一种框架，利用完成引擎来进一步支持大型语言模型在自动化程序修复中合成更多有效的修补程序。

    

    在自动化程序修复中，对于通用编程语言中的实际系统合成正确的修补程序可能具有挑战性。最近的大型语言模型（LLMs）已被证明对开发人员在各种编码任务中具有帮助，并且已直接应用于修补程序的合成。然而，大多数LLMs将程序视为令牌序列，这意味着它们对目标编程语言的底层语义约束一无所知。这导致生成了大量静态无效的修补程序，阻碍了该技术的实用性。因此，我们提出了Repilot，一种在修复过程中通过合成更多有效修补程序从而进一步支持AI“副驾驶员”（即LLMs）的框架。我们的关键见解是，许多LLMs以自回归方式生成输出（即逐个令牌生成），类似于人类编写程序，这可以通过完成引擎显著提升和引导。Repilot协同合成了修补程序。

    During Automated Program Repair (APR), it can be challenging to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful "copilots" in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a framework to further copilot the AI "copilots" (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthe
    
[^15]: 在人工智能中引入神经启发的适应性，以实现持续学习

    Incorporating Neuro-Inspired Adaptability for Continual Learning in Artificial Intelligence. (arXiv:2308.14991v1 [cs.LG])

    [http://arxiv.org/abs/2308.14991](http://arxiv.org/abs/2308.14991)

    本研究引入了神经启发的适应性解决方案，以实现人工智能的持续学习。通过模拟果蝇学习系统，我们提出了一种可以灵活适应变化的通用方法，改善了学习可塑性，并确保解决方案的兼容性。

    

    持续学习旨在赋予人工智能（AI）对真实世界的强大适应能力。为此，一个理想的解决方案应该在记忆稳定性和学习可塑性之间保持适当平衡，并获得足够的兼容性来捕捉观测到的分布。现有的进展主要集中在保持记忆稳定性以克服灾难性遗忘，但仍难以像生物智能（BI）那样灵活地适应增量变化。通过建模一个能够主动调节遗忘的稳健果蝇学习系统，并利用多个学习模块，我们提出了一种通用方法，通过在参数分布中适当衰减旧记忆来改善学习可塑性，并相应地协调多学习者架构来确保解决方案的兼容性。通过广泛的理论和实证验证，我们的方法不仅明显提高了持续学习的性能，特别是在突触调节方面。

    Continual learning aims to empower artificial intelligence (AI) with strong adaptability to the real world. For this purpose, a desirable solution should properly balance memory stability with learning plasticity, and acquire sufficient compatibility to capture the observed distributions. Existing advances mainly focus on preserving memory stability to overcome catastrophic forgetting, but remain difficult to flexibly accommodate incremental changes as biological intelligence (BI) does. By modeling a robust Drosophila learning system that actively regulates forgetting with multiple learning modules, here we propose a generic approach that appropriately attenuates old memories in parameter distributions to improve learning plasticity, and accordingly coordinates a multi-learner architecture to ensure solution compatibility. Through extensive theoretical and empirical validation, our approach not only clearly enhances the performance of continual learning, especially over synaptic regula
    
[^16]: 使用传感器数据、方程和自然语言提示上下文中的运算符学习

    Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language. (arXiv:2308.05061v1 [cs.LG])

    [http://arxiv.org/abs/2308.05061](http://arxiv.org/abs/2308.05061)

    本文提出了一种使用传感器数据、方程和自然语言提示上下文中运算符学习的方法。通过整合人类知识和语言描述，该方法不仅扩展了物理信息学习的灵活性和普适性，而且显著提高了学习性能和减少了数据需求。

    

    在科学机器学习领域中，上下文中的运算符学习已经展示出了在推理阶段从提示数据中学习运算符的显著潜力，而无需进行权重更新。然而，当前模型对传感器数据的过度依赖可能会无意中忽视运算符的宝贵的人类洞察力。为了解决这个问题，我们将上下文中的运算符学习转化为一种多模式范式。我们提出使用“标题”来整合通过自然语言描述和方程式表达的运算符的人类知识。我们演示了这种方法不仅扩展了物理信息学习的灵活性和普遍性，而且还显著提高了学习性能并减少了数据需求。此外，我们引入了一种更高效的多模式上下文运算符学习的神经网络架构，称为“ICON-LM”，基于类似于语言模型的架构。

    In the growing domain of scientific machine learning, in-context operator learning has demonstrated notable potential in learning operators from prompted data during inference stage without weight updates. However, the current model's overdependence on sensor data, may inadvertently overlook the invaluable human insight into the operator. To address this, we present a transformation of in-context operator learning into a multi-modal paradigm. We propose the use of "captions" to integrate human knowledge about the operator, expressed through natural language descriptions and equations. We illustrate how this method not only broadens the flexibility and generality of physics-informed learning, but also significantly boosts learning performance and reduces data needs. Furthermore, we introduce a more efficient neural network architecture for multi-modal in-context operator learning, referred to as "ICON-LM", based on a language-model-like architecture. We demonstrate the viability of "ICO
    
[^17]: 非凸双层优化和时变目标函数

    Non-Convex Bilevel Optimization with Time-Varying Objective Functions. (arXiv:2308.03811v1 [math.OC])

    [http://arxiv.org/abs/2308.03811](http://arxiv.org/abs/2308.03811)

    本论文研究了在线双层优化问题，提出了一种基于窗口平均的单循环在线双层优化器（SOBOW），它能在处理函数变化和真实超梯度不可用的情况下高效更新决策。

    

    双层优化已经成为机器学习问题中的一个强大工具，然而，当前的非凸双层优化考虑的是离线数据集和静态函数，在新兴的在线应用中，这可能无法很好地处理流式数据和时变函数。在这项工作中，我们研究了在线双层优化（OBO），其中函数可以时变，并且代理不断根据在线流数据更新决策。为了处理OBO中的函数变化和真实超梯度不可用的问题，我们提出了一种基于窗口平均的单循环在线双层优化器（SOBOW），它根据内层窗口平均的最近超梯度估计值来更新外层的决策。与现有算法相比，SOBOW计算效率高，并且不需要知道先前的函数。为了处理单循环更新和函数变化带来的独特技术困难，我们还提出了一种深度学习方法，重点是通过增加稀疏性以及结构和全局信息来减少预测误差。

    Bilevel optimization has become a powerful tool in a wide variety of machine learning problems. However, the current nonconvex bilevel optimization considers an offline dataset and static functions, which may not work well in emerging online applications with streaming data and time-varying functions. In this work, we study online bilevel optimization (OBO) where the functions can be time-varying and the agent continuously updates the decisions with online streaming data. To deal with the function variations and the unavailability of the true hypergradients in OBO, we propose a single-loop online bilevel optimizer with window averaging (SOBOW), which updates the outer-level decision based on a window average of the most recent hypergradient estimations stored in the memory. Compared to existing algorithms, SOBOW is computationally efficient and does not need to know previous functions. To handle the unique technical difficulties rooted in single-loop update and function variations for 
    
[^18]: 神经网络贝叶斯推理中的数据变异性理论

    A theory of data variability in Neural Network Bayesian inference. (arXiv:2307.16695v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2307.16695](http://arxiv.org/abs/2307.16695)

    本文提出了一个泛化性的场论形式体系，用于研究神经网络在无限宽隐藏层的极限情况，并通过计算非线性和深度非线性网络的泛化特性，阐明了数据的变异性对网络行为的影响。

    

    贝叶斯推理和核方法在机器学习中已经得到了很好的应用。特别是神经网络高斯过程通过使用核和推理方法提供了一种研究神经网络在无限宽隐藏层的极限情况的概念。本文在这个极限的基础上建立了一个场论形式体系，涵盖了无限宽网络的泛化特性。我们系统地计算了具有异质条目的核矩阵的线性、非线性和深度非线性网络的泛化特性。与目前使用的谱方法相比，我们通过从输入数据的统计特性推导出泛化特性，阐明了输入维度、训练数据集的大小以及数据的变异性之间的相互作用。我们表明数据的变异性导致了一种非高斯作用，类似于($\varphi^3+\varphi^4$)-理论。在一个合成任务和MNIST上使用我们的形式体系，我们获得了一个均匀的核。

    Bayesian inference and kernel methods are well established in machine learning. The neural network Gaussian process in particular provides a concept to investigate neural networks in the limit of infinitely wide hidden layers by using kernel and inference methods. Here we build upon this limit and provide a field-theoretic formalism which covers the generalization properties of infinitely wide networks. We systematically compute generalization properties of linear, non-linear, and deep non-linear networks for kernel matrices with heterogeneous entries. In contrast to currently employed spectral methods we derive the generalization properties from the statistical properties of the input, elucidating the interplay of input dimensionality, size of the training data set, and variability of the data. We show that data variability leads to a non-Gaussian action reminiscent of a ($\varphi^3+\varphi^4$)-theory. Using our formalism on a synthetic task and on MNIST we obtain a homogeneous kernel
    
[^19]: 用度量的Dikin步骤有效地抽样PSD锥体

    Efficiently Sampling the PSD Cone with the Metric Dikin Walk. (arXiv:2307.12943v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2307.12943](http://arxiv.org/abs/2307.12943)

    本文通过对Dikin步行方法进行分析并适应一般度量，为带约束的PSD锥体抽样问题提供了一种有效的解决方案，并提出了优化的自共轭矩阵函数概念。

    

    半定规划代表了高效计算的前沿。尽管在半定最优化上已取得了很大进展，如今内点法能够在实践中解决中等规模的问题，但是抽样半定解的基本问题仍然是一个巨大的挑战。直接应用已知的多项式时间算法来抽样一般凸体的方法导致运行时间过长。此外，已知的通用方法需要昂贵的舍入阶段作为预处理。本文分析了Dikin步行，并首先将其适应于一般度量，然后为带有仿射约束的PSD锥体设计合适的度量。所得到的混合时间和每步复杂度相当小，并且通过适当选择度量，可以使其对约束的依赖关系变为多对数级的。我们介绍了一个优化的自共轭矩阵函数概念，并给出了组合规则。

    Semi-definite programs represent a frontier of efficient computation. While there has been much progress on semi-definite optimization, with moderate-sized instances currently solvable in practice by the interior-point method, the basic problem of sampling semi-definite solutions remains a formidable challenge. The direct application of known polynomial-time algorithms for sampling general convex bodies to semi-definite sampling leads to a prohibitively high running time. In addition, known general methods require an expensive rounding phase as pre-processing. Here we analyze the Dikin walk, by first adapting it to general metrics, then devising suitable metrics for the PSD cone with affine constraints. The resulting mixing time and per-step complexity are considerably smaller, and by an appropriate choice of the metric, the dependence on the number of constraints can be made polylogarithmic. We introduce a refined notion of self-concordant matrix functions and give rules for combining
    
[^20]: 拜占庭鲁棒的分布式在线学习：在拜占庭环境中应对对手的恶意行为

    Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment. (arXiv:2307.07980v1 [cs.LG])

    [http://arxiv.org/abs/2307.07980](http://arxiv.org/abs/2307.07980)

    本文研究了在拜占庭攻击下的分布式在线学习中的对手行为。在拜占庭和恶意对手存在的情况下，尽管可以控制线性对手遗憾的常数，但分布式在线梯度下降只能实现线性对手遗憾上界。然而，在环境不完全对抗性的情况下，可以实现次线性的随机遗憾。

    

    本文研究了在拜占庭攻击下的分布式在线学习。在线学习算法的性能通常通过（对手的）遗憾来评估，在环境提供对手损失时评估一步决策的质量，而期望得到一个次线性的上界。但我们证明，即使使用一类最先进的鲁棒聚合规则，在拜占庭环境和存在拜占庭参与者的情况下，分布式在线梯度下降只能实现线性的对手遗憾上界，这是紧密的。这是拜占庭攻击的必然结果，尽管我们可以将线性对手遗憾的常数控制在合理的水平上。有趣的是，当环境不是完全对抗性的，即诚实参与者的损失是独立同分布的（i.i.d.），我们表明，与前面提到的对手遗憾相反，可以实现次线性的随机遗憾。

    This paper studies distributed online learning under Byzantine attacks. The performance of an online learning algorithm is often characterized by (adversarial) regret, which evaluates the quality of one-step-ahead decision-making when an environment provides adversarial losses, and a sublinear bound is preferred. But we prove that, even with a class of state-of-the-art robust aggregation rules, in an adversarial environment and in the presence of Byzantine participants, distributed online gradient descent can only achieve a linear adversarial regret bound, which is tight. This is the inevitable consequence of Byzantine attacks, even though we can control the constant of the linear adversarial regret to a reasonable level. Interestingly, when the environment is not fully adversarial so that the losses of the honest participants are i.i.d. (independent and identically distributed), we show that sublinear stochastic regret, in contrast to the aforementioned adversarial regret, is possible
    
[^21]: FDAPT: 面向语言模型的联邦领域自适应预训练

    FDAPT: Federated Domain-adaptive Pre-training for Language Models. (arXiv:2307.06933v1 [cs.LG])

    [http://arxiv.org/abs/2307.06933](http://arxiv.org/abs/2307.06933)

    FDAPT是一种联邦领域自适应预训练的方法，在保护数据隐私的同时，能够通过利用敏感和分布式数据来增强模型适应能力。对于IID和非IID情况下的下游任务，FDAPT能够维持与中央基线相竞争的性能。提出的FFDAPT算法进一步提高了计算效率，并展现出与标准FDAPT类似的下游任务性能。此外，我们也确定了这个新研究领域的有希望的未来研究方向。

    

    将领域自适应预训练（DAPT）与联邦学习（FL）相结合可以通过利用更敏感和分布式数据来增强模型适应能力，同时保护数据隐私。然而，目前关于这种方法的研究还很少。因此，我们进行了第一次全面的实证研究，以评估联邦领域自适应预训练（FDAPT）的性能。我们证明了FDAPT在IID和非IID情况下都能维持与中央基线相竞争的下游任务性能。此外，我们提出了一种新算法，冻结的联邦领域自适应预训练（FFDAPT）。FFDAPT平均提高了12.1%的计算效率，并且在标准FDAPT的情况下展现出类似的下游任务性能，一般性能波动保持在1%以下。最后，通过对我们的工作进行批判性评估，我们确定了这个新的研究领域的有希望的未来研究方向。

    Combining Domain-adaptive Pre-training (DAPT) with Federated Learning (FL) can enhance model adaptation by leveraging more sensitive and distributed data while preserving data privacy. However, few studies have focused on this method. Therefore, we conduct the first comprehensive empirical study to evaluate the performance of Federated Domain-adaptive Pre-training (FDAPT). We demonstrate that FDAPT can maintain competitive downstream task performance to the centralized baseline in both IID and non-IID situations. Furthermore, we propose a novel algorithm, Frozen Federated Domain-adaptive Pre-training (FFDAPT). FFDAPT improves the computational efficiency by 12.1% on average and exhibits similar downstream task performance to standard FDAPT, with general performance fluctuations remaining less than 1%. Finally, through a critical evaluation of our work, we identify promising future research directions for this new research area.
    
[^22]: 从软干预中确保因果分解的可识别性

    Identifiability Guarantees for Causal Disentanglement from Soft Interventions. (arXiv:2307.06250v1 [stat.ML])

    [http://arxiv.org/abs/2307.06250](http://arxiv.org/abs/2307.06250)

    本文研究了从软干预中确保因果分解的可识别性。通过开发一种自编码变分贝叶斯算法，我们展示了在给定一般化的忠诚性概念的情况下，即使存在未观测到的因果变量，仍然可以恢复潜在的因果模型，并在无限数据的极限情况下预测未见组合的干预效果。

    

    因果分解旨在通过潜在变量的相关性揭示数据的表征，其通过因果模型相互关联。如果解释数据的潜在模型是唯一的，那么这种表示是可识别的。本文关注的是当存在不配对的观测和干预数据时的情况，每个干预都会改变一个潜在变量的机制。当因果变量完全观测到时，在诚实性假设下，已经开发出了统计一致的算法来识别因果模型。我们在这里展示，即使存在未观测到的因果变量，在给定一般化的忠诚性概念的情况下仍然可以实现可识别性。我们的结果保证了我们可以恢复潜在的因果模型，预测未见组合的干预效果，在无限数据的极限情况下。我们通过开发一种自编码变分贝叶斯算法和ap来实现我们的因果分解框架。

    Causal disentanglement aims to uncover a representation of data using latent variables that are interrelated through a causal model. Such a representation is identifiable if the latent model that explains the data is unique. In this paper, we focus on the scenario where unpaired observational and interventional data are available, with each intervention changing the mechanism of a latent variable. When the causal variables are fully observed, statistically consistent algorithms have been developed to identify the causal model under faithfulness assumptions. We here show that identifiability can still be achieved with unobserved causal variables, given a generalized notion of faithfulness. Our results guarantee that we can recover the latent causal model up to an equivalence class and predict the effect of unseen combinations of interventions, in the limit of infinite data. We implement our causal disentanglement framework by developing an autoencoding variational Bayes algorithm and ap
    
[^23]: 受限扩散模型的城市采样

    Metropolis Sampling for Constrained Diffusion Models. (arXiv:2307.05439v1 [cs.LG])

    [http://arxiv.org/abs/2307.05439](http://arxiv.org/abs/2307.05439)

    本文提出了一种基于城市采样的简单加噪方案，用于解决受约束的扩散模型的生成建模问题。相较于现有的采样器，该方案在计算效率和实证性能方面都有明显提升，并且被证明是反射布朗运动的有效离散化。

    

    去噪扩散模型最近已经成为生成建模的主要范式。它们对黎曼流形的扩展使得它们能够在自然科学中的一系列问题上得到应用。然而，在许多实际情况中，这些流形由一组约束定义，并且不被现有的（黎曼）扩散模型方法所覆盖。最近的工作通过使用基于对数障碍方法或反射布朗运动的新型加噪过程来解决这个问题。然而，随着约束复杂度的增加，相关的采样器计算负担较重。在本文中，我们引入了一种基于城市采样的替代简单加噪方案，与早期的采样器相比，计算效率和实证性能都有很大提升。在独立的兴趣方面，我们证明了这个新过程对应于反射布朗运动的有效离散化。

    Denoising diffusion models have recently emerged as the predominant paradigm for generative modelling. Their extension to Riemannian manifolds has facilitated their application to an array of problems in the natural sciences. Yet, in many practical settings, such manifolds are defined by a set of constraints and are not covered by the existing (Riemannian) diffusion model methodology. Recent work has attempted to address this issue by employing novel noising processes based on logarithmic barrier methods or reflected Brownian motions. However, the associated samplers are computationally burdensome as the complexity of the constraints increases. In this paper, we introduce an alternative simple noising scheme based on Metropolis sampling that affords substantial gains in computational efficiency and empirical performance compared to the earlier samplers. Of independent interest, we prove that this new process corresponds to a valid discretisation of the reflected Brownian motion. We dem
    
[^24]: 学习将辅助数据集分组用于分子问题

    Learning to Group Auxiliary Datasets for Molecule. (arXiv:2307.04052v1 [q-bio.BM])

    [http://arxiv.org/abs/2307.04052](http://arxiv.org/abs/2307.04052)

    本文提出了一种名为MolGroup的方法，通过将图结构相似性和任务相似性相结合，预测每个辅助分子数据集的潜在好处，以解决合作使用辅助数据集时的负迁移问题。

    

    小分子数据集中有限的注释可用性对机器学习模型提出了挑战。为了解决这个问题，一种常见的策略是与额外的辅助数据集合作。然而，拥有更多的数据并不总是能保证改进。当目标数据集中的知识与辅助分子数据集中的知识不同或相互矛盾时，负迁移可能会发生。鉴于此，当共同训练时，确定可以使目标数据集受益的辅助分子数据集仍然是一个关键而未解决的问题。通过经验分析，我们观察到将图结构相似性和任务相似性相结合可以作为确定高亲和性辅助数据集的更可靠指标。在此基础上，我们提出了MolGroup，它将数据集亲和性分为任务亲和性和结构亲和性，以预测每个辅助分子数据集的潜在好处。MolGroup通过利用路由机制来实现。

    The limited availability of annotations in small molecule datasets presents a challenge to machine learning models. To address this, one common strategy is to collaborate with additional auxiliary datasets. However, having more data does not always guarantee improvements. Negative transfer can occur when the knowledge in the target dataset differs or contradicts that of the auxiliary molecule datasets. In light of this, identifying the auxiliary molecule datasets that can benefit the target dataset when jointly trained remains a critical and unresolved problem. Through an empirical analysis, we observe that combining graph structure similarity and task similarity can serve as a more reliable indicator for identifying high-affinity auxiliary datasets. Motivated by this insight, we propose MolGroup, which separates the dataset affinity into task and structure affinity to predict the potential benefits of each auxiliary molecule dataset. MolGroup achieves this by utilizing a routing mecha
    
[^25]: TD收敛性：一个优化的视角

    TD Convergence: An Optimization Perspective. (arXiv:2306.17750v1 [cs.LG])

    [http://arxiv.org/abs/2306.17750](http://arxiv.org/abs/2306.17750)

    本研究从优化的视角研究了时差(TD)学习算法的收敛行为，在经典反例中确定了影响算法收敛或发散的两个力量，并在线性逼近和平方损失以外的情况下证明了TD的收敛性。这一研究为TD在强化学习领域的成功应用提供了理论解释。

    

    我们研究了著名的时差(TD)学习算法的收敛特性。通过优化的视角来看待算法，我们首先论证了TD可以被视为一种迭代优化算法，其中每次迭代时要最小化的函数都会发生变化。通过仔细研究TD在经典反例中的发散行为，我们确定了决定算法收敛或发散行为的两个力量。我们还将这一优化视角推广到了比线性逼近和平方损失更广泛的设置中，证明了TD的收敛性取决于这两个力量之间的相互作用。我们的结果为TD在强化学习中的成功应用提供了理论上的解释。

    We study the convergence behavior of the celebrated temporal-difference (TD) learning algorithm. By looking at the algorithm through the lens of optimization, we first argue that TD can be viewed as an iterative optimization algorithm where the function to be minimized changes per iteration. By carefully investigating the divergence displayed by TD on a classical counter example, we identify two forces that determine the convergent or divergent behavior of the algorithm. We next formalize our discovery in the linear TD setting with quadratic loss and prove that convergence of TD hinges on the interplay between these two forces. We extend this optimization perspective to prove convergence of TD in a much broader setting than just linear approximation and squared loss. Our results provide a theoretical explanation for the successful application of TD in reinforcement learning.
    
[^26]: 可量化Transformer：通过帮助注意力头“什么也不做”去除离群值

    Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing. (arXiv:2306.12929v1 [cs.LG])

    [http://arxiv.org/abs/2306.12929](http://arxiv.org/abs/2306.12929)

    本文提出了一种称为“Helper-Head”的方法，可以通过教授注意头忽略输入和输出的某些部分来消除离群值，从而实现对transformer的可量化。实验结果表明，该方法在低、高比特宽度设置上均优于现有方法，在两个流行的语言建模基准上实现了最先进的结果。

    

    过去几年里，Transformer模型已经被广泛应用于各个领域，特别是大型语言模型已经显著推进了人工智能领域的发展。由于其规模，这些网络的能力已经大大增强，但这是以极大的计算成本为代价的。量化是减少神经网络计算时间和存储器消耗的最有效方法之一。然而，许多研究表明，现代transformer模型往往学习到其激活中的强离群值，这使得它们难以量化。为保持可接受的性能，这些离群值的存在需要将激活置于更高的比特宽度或使用不同的数字格式，进行额外的微调或其他变通方法。本文展示了强离群值与特定注意头行为的相关性，这些头试图学习“无操作”或仅仅是部分残差更新。为了实现注意力头中需要的精确零位，我们引入了一个称为“Helper-Head”的方法，教授注意力头忽略输入和输出的某些部分。我们还引入了一种利用这些额外信息的量化技术，可以使用低精度量化甚至是强离群数据。在几个基准数据集上的实验证明，我们的方法在低、高比特宽度设置上均优于现有方法，在两个流行的语言建模基准上实现了最先进的结果。

    Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI significantly. Due to their size, the capability of these networks has increased tremendously, but this has come at the cost of a significant increase in necessary compute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize. To retain acceptable performance, the existence of these outliers requires activations to be in higher bitwidth or the use of different numeric formats, extra fine-tuning, or other workarounds. We show that strong outliers are related to very specific behavior of attention heads that try to learn a "no-op" or just a partial update of the residual. To achieve the exact zeros needed in the attention 
    
[^27]: 大型语言模型被误导：使用Only Connect Wall数据集探索创造性问题解决和Einstellung效应。

    Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset. (arXiv:2306.11167v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11167](http://arxiv.org/abs/2306.11167)

    这项研究探索了大型语言模型（LLMs）对创造性问题解决的能力，并发现大型语言模型容易被误导，出现固定效应和Einstellung范式。

    

    自从人工智能诞生以来，对人类仿真智能的追求一直是人工智能研究的持久话题。最新一代的大型语言模型（LLM）的技术演进和新兴能力将这个主题从学术界带到了文化时代。尽管最近的NLP评估基准任务测试了人类仿真行为的一些方面（例如BIG-bench的“类人行为”任务），但几乎没有一个任务考察创造性问题解决能力。人类的创造性问题解决是认知神经科学中研究较为深入的主题，标准化测试主要使用将线索词之间的（异构）连接能力作为创造性的度量。在这样的任务中，暗示性的误导性刺激-被称为“诱导误解”的干扰因素-通过固定效应和Einstellung范式阻碍了人类的表现。在认知神经科学的研究中，通过事先让参与者接触到有相似拼写的错误因素来实验性地诱导这样的固定。

    The quest for human imitative AI has been an enduring topic in AI research since its inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to the cultural zeitgeist. While recent NLP evaluation benchmark tasks test some aspects of human-imitative behaviour (e.g., BIG-bench's 'human-like behavior' tasks), few, if not none, examine creative problem solving abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use the ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incor
    
[^28]: 使用合页损失在噪声数据上训练浅层ReLU网络：我们何时过度拟合且其是否良性？

    Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?. (arXiv:2306.09955v1 [cs.LG])

    [http://arxiv.org/abs/2306.09955](http://arxiv.org/abs/2306.09955)

    本论文研究了二层ReLU网络在使用梯度下降和合页损失处理噪声数据进行二分类中的良性过拟合，通过对干净数据余量的条件的确定，得出了三种不同的训练结果，能够在训练过程中对神经元动态变化做出精细描述，并发现了两个不同的训练阶段。

    

    我们研究了使用梯度下降和合页损失在噪声数据上训练的二层ReLU网络在二分类中的良性过拟合现象。我们特别考虑了线性可分数据，其中相对较小比例的标签被损坏或翻转。我们确定了干净数据余量的条件，产生了三种不同的训练结果：良性过拟合，在这种情况下将达到零损失，并且具有很高的概率测试数据被正确分类；过拟合，在这种情况下将达到零损失，但测试数据被错误分类的概率受到常数下限的约束；以及不过拟合，在这种情况下干净的点可以达到零损失，并且具有很高的概率测试数据被正确分类，但是不干净的点无法做出同样的预测。我们的分析提供了对神经元在训练过程中动态变化的一种精细描述，并揭示了两个不同的阶段：在第一个阶段中，干净点接近达到零损失，在第二个阶段中，干净点会振荡。

    We study benign overfitting in two-layer ReLU networks trained using gradient descent and hinge loss on noisy data for binary classification. In particular, we consider linearly separable data for which a relatively small proportion of labels are corrupted or flipped. We identify conditions on the margin of the clean data that give rise to three distinct training outcomes: benign overfitting, in which zero loss is achieved and with high probability test data is classified correctly; overfitting, in which zero loss is achieved but test data is misclassified with probability lower bounded by a constant; and non-overfitting, in which clean points, but not corrupt points, achieve zero loss and again with high probability test data is classified correctly. Our analysis provides a fine-grained description of the dynamics of neurons throughout training and reveals two distinct phases: in the first phase clean points achieve close to zero loss, in the second phase clean points oscillate on the
    
[^29]: TopP\&R: 具有鲁棒性的支持估计方法，用于评估生成模型中的保真度和多样性

    TopP\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])

    [http://arxiv.org/abs/2306.08013](http://arxiv.org/abs/2306.08013)

    本文提出了一种鲁棒可靠的生成模型评估指标TopP\&R，通过引入拓扑和统计处理进行严格的支持估计。TopP\&R仅保留具有一定置信水平的具有拓扑和统计上重要性的特征，对于噪声特征具有强大的鲁棒性，并提供了统计一致性。

    

    本文提出了一种鲁棒可靠的生成模型评估指标，通过引入拓扑和统计处理进行严格的支持估计。现有的度量标准，如Inception Score（IS），Fr\'echet Inception Distance（FID）以及Precision and Recall（P\&R）的变体，严重依赖于从样本特征估计的支持。然而，尽管评估的质量完全取决于其可靠性，但其估计的可靠性并没有得到严肃的讨论（并被忽视）。本文提出了拓扑精度和召回率（TopP\&R，发音为“topper”），它提供了一种系统的方法来估计支持，仅保留具有一定置信水平的具有拓扑和统计上重要性的特征。这不仅使TopP\&R对于噪声特征具有强大的鲁棒性，而且还提供了统计一致性。我们的理论和实验结果表明，TopP\&R对于离群值和非独立同分布具有鲁棒性。

    We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&R is robust to outliers and non-independent and identically distributed
    
[^30]: 因子图模型视角下的干预泛化

    Intervention Generalization: A View from Factor Graph Models. (arXiv:2306.04027v1 [stat.ML])

    [http://arxiv.org/abs/2306.04027](http://arxiv.org/abs/2306.04027)

    本文提出了一种基于因子图模型的“干预因子模型”(IFM)方法，仅基于对操纵系统分布的因子分解的最小假设，以实现从过去的实验到新的条件的跃迁。

    

    因果推断的一个目标是从过去的实验和观察数据推广到新的条件。在训练数据中提供足够多的实验的情况下，理论上可能最终学习从新的实验条件到感兴趣的结果的映射，但是处理大量可能的干预组合空间很困难。在典型的稀疏实验设计下，如果不依赖于重的规则化或先验分布，这种映射是不适当的。这样的假设可能是可靠的，也可能是不可靠的，很难辩护或测试。本文从因子图模型的语言角度深入探讨如何保证从过去的实验到新的条件的跃迁，仅基于对操纵系统分布的因子分解的最小假设。假设的“干预因子模型”可能并不总是有用的，但是它很方便地处理了大量可能的干预空间。

    One of the goals of causal inference is to generalize from past experiments and observational data to novel conditions. While it is in principle possible to eventually learn a mapping from a novel experimental condition to an outcome of interest, provided a sufficient variety of experiments is available in the training data, coping with a large combinatorial space of possible interventions is hard. Under a typical sparse experimental design, this mapping is ill-posed without relying on heavy regularization or prior distributions. Such assumptions may or may not be reliable, and can be hard to defend or test. In this paper, we take a close look at how to warrant a leap from past experiments to novel conditions based on minimal assumptions about the factorization of the distribution of the manipulated system, communicated in the well-understood language of factor graph models. A postulated $\textit{interventional factor model}$ (IFM) may not always be informative, but it conveniently abs
    
[^31]: 分散化SGD和平均方向SAM在渐近意义下是等价的

    Decentralized SGD and Average-direction SAM are Asymptotically Equivalent. (arXiv:2306.02913v1 [cs.LG])

    [http://arxiv.org/abs/2306.02913](http://arxiv.org/abs/2306.02913)

    分散SGD和平均方向SAM在渐近意义下是等价的，D-SGD表现出梯度平滑效应和锐度正则化效应，而且可以提高后验评估，并证明了潜在的泛化能力

    

    分散随机梯度下降（D-SGD）允许在没有中央服务器的控制下，大量设备同时进行协作学习。然而，现有理论认为，分散化不可避免地削弱了泛化能力。本文挑战传统信念，提出了完全新的角度来理解分散学习。我们证明了在一般非凸非-$\beta$-平滑设置下，D-SGD隐式地最小化了平均方向锐度感知最小化（SAM）算法的损失函数。这种惊人的渐近等价揭示了内在的正则化-优化权衡以及分散化的三个优点：（1）D-SGD中存在一个自由的不确定性评估机制，可以提高后验估计；（2）D-SGD表现出梯度平滑效应；（3）D-SGD的锐度正则化效应不会随着总批处理大小的增加而减少，这证明了潜在的泛化能力

    Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive devices simultaneously without the control of a central server. However, existing theories claim that decentralization invariably undermines generalization. In this paper, we challenge the conventional belief and present a completely new perspective for understanding decentralized learning. We prove that D-SGD implicitly minimizes the loss function of an average-direction Sharpness-aware minimization (SAM) algorithm under general non-convex non-$\beta$-smooth settings. This surprising asymptotic equivalence reveals an intrinsic regularization-optimization trade-off and three advantages of decentralization: (1) there exists a free uncertainty evaluation mechanism in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient smoothing effect; and (3) the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the potential generalization b
    
[^32]: 训练神经算子以保持混沌吸引子的不变测度

    Training neural operators to preserve invariant measures of chaotic attractors. (arXiv:2306.01187v1 [cs.LG])

    [http://arxiv.org/abs/2306.01187](http://arxiv.org/abs/2306.01187)

    本文提出了一种基于神经算子的训练框架，通过最优传输距离或KSD损失来确保神经算子能够在混沌系统上复现其统计或结构特性。

    

    混沌系统使得长时间预测变得困难，因为初始条件的微小扰动会导致轨迹以指数速度发散。在这种情况下，神经算子训练为最小化平方误差损失，虽然能够准确地进行短期预测，但常常无法再长时间内复制动力学的统计或结构特性，并且可能产生退化的结果。本文提出了一种替代框架，旨在保持表征动态不变统计属性的混沌吸引子的不变测度。具体来说，在多环境设置中（每个样本轨迹都受略微不同动态的控制），我们考虑了两种新的处理嘈杂数据的方法。首先，我们提出了一种基于观测到的动态与神经算子输出之间的最优传输距离的损失。这种方法需要专家对基础物理的专业知识来确定最优传输距离矩阵。

    Chaotic systems make long-horizon forecasts difficult because small perturbations in initial conditions cause trajectories to diverge at an exponential rate. In this setting, neural operators trained to minimize squared error losses, while capable of accurate short-term forecasts, often fail to reproduce statistical or structural properties of the dynamics over longer time horizons and can yield degenerate results. In this paper, we propose an alternative framework designed to preserve invariant measures of chaotic attractors that characterize the time-invariant statistical properties of the dynamics. Specifically, in the multi-environment setting (where each sample trajectory is governed by slightly different dynamics), we consider two novel approaches to training with noisy data. First, we propose a loss based on the optimal transport distance between the observed dynamics and the neural operator outputs. This approach requires expert knowledge of the underlying physics to determine 
    
[^33]: 复数值自编码器对物体发现的对比训练

    Contrastive Training of Complex-Valued Autoencoders for Object Discovery. (arXiv:2305.15001v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15001](http://arxiv.org/abs/2305.15001)

    介绍了一种架构修改和新颖对比学习方法，大大改进了同步性基模型，首次获得了一类同步性基模型，在多物体彩色数据集中无监督地发现物体。

    

    当前最先进的物体中心模型使用插槽和注意力路由进行绑定。然而，这类模型有几个概念性的局限性：插槽的数量是硬编码的；所有插槽的容量相等；训练成本高昂；插槽内没有目标级别的关系因素。原则上，基于同步性的模型可以通过使用复数值激活在其相位分量中存储绑定信息来解决这些限制。然而，这类基于同步性的模型的工作示例只是最近才有，而且实际上仍然限于玩具灰度数据集的同时存储不到三个物体。在这里，我们介绍了架构修改和一种新颖的对比学习方法，极大地改进了最先进的同步性基模型。我们首次获得了一类同步性基模型，能够在多物体彩色数据集中以无监督的方式发现物体。

    Current state-of-the-art object-centric models use slots and attention-based routing for binding. However, this class of models has several conceptual limitations: the number of slots is hardwired; all slots have equal capacity; training has high computational cost; there are no object-level relational factors within slots. Synchrony-based models in principle can address these limitations by using complex-valued activations which store binding information in their phase components. However, working examples of such synchrony-based models have been developed only very recently, and are still limited to toy grayscale datasets and simultaneous storage of less than three objects in practice. Here we introduce architectural modifications and a novel contrastive learning method that greatly improve the state-of-the-art synchrony-based model. For the first time, we obtain a class of synchrony-based models capable of discovering objects in an unsupervised manner in multi-object color datasets 
    
[^34]: 对放弃分类器进行反事实比较

    Counterfactually Comparing Abstaining Classifiers. (arXiv:2305.10564v1 [stat.ML])

    [http://arxiv.org/abs/2305.10564](http://arxiv.org/abs/2305.10564)

    本文提出一种新的方法和视角来评估和比较放弃分类器，将放弃预测视为缺失数据。我们定义了放弃分类器的反事实得分，指的是分类器没有放弃预测时的预测性能期望。

    

    放弃分类器可以选择在不确定时放弃对输入的预测。这些分类器在高风险决策问题中越来越受欢迎，因为它们可以保留不确定的预测，以提高其可靠性和安全性。然而，在评估黑盒放弃分类器时，我们缺乏一个原则性的方法来考虑分类器在它的放弃预测上的预测结果。当放射科医生不确定其诊断或当驾驶员在自动驾驶汽车中不注意时，这些缺失的预测结果至关重要。本文引入了一种新的方法和视角来评估和比较放弃分类器，将放弃预测视为缺失数据。我们的评估方法围绕着定义一个放弃分类器的反事实得分，即分类器没有放弃的情况下的预测性能的期望。我们指定了条件... (此处省略)

    Abstaining classifiers have the option to abstain from making predictions on inputs that they are unsure about. These classifiers are becoming increasingly popular in high-stake decision-making problems, as they can withhold uncertain predictions to improve their reliability and safety. When evaluating black-box abstaining classifier(s), however, we lack a principled approach that accounts for what the classifier would have predicted on its abstentions. These missing predictions are crucial when, e.g., a radiologist is unsure of their diagnosis or when a driver is inattentive in a self-driving car. In this paper, we introduce a novel approach and perspective to the problem of evaluating and comparing abstaining classifiers by treating abstentions as missing data. Our evaluation approach is centered around defining the counterfactual score of an abstaining classifier, defined as the expected performance of the classifier had it not been allowed to abstain. We specify the conditions unde
    
[^35]: 重新审视梯度剪切：随机偏差和紧密收敛性保证。

    Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees. (arXiv:2305.01588v1 [cs.LG])

    [http://arxiv.org/abs/2305.01588](http://arxiv.org/abs/2305.01588)

    本文提出了针对梯度剪切的收敛保证机制，不再需要特定的阈值和强噪声假设，同时可以独立于步长选择，从而提高了收敛的自由度。

    

    梯度剪切是标准（随机）梯度下降的一种流行修改方法，每次迭代将梯度范数限制在某个值c>0。它被广泛用于稳定深度学习模型的训练( Goodfellow et al., 2016 )或强制实施差分隐私( Abadi et al., 2016 )。尽管剪切机制受欢迎且简单，但其收敛保证通常需要特定的$c$值和强噪声假设。在本文中，我们给出了收敛保证，显示了对任意剪辑阈值的精确依赖，并且表明我们的保证在确定性和随机梯度下都是紧密的。特别地，我们表明(i)对于确定性的梯度下降，剪辑阈值仅影响收敛的高阶项，(ii)在随机设置中，即使对于任意小的步长，也不能保证收敛到真正的最优解在标准的噪声假设下，我们给出了机器学习特定的随机噪声假设，在此假设下，收敛是保证的，剪切阈值$c$可以独立于步长选择。

    Gradient clipping is a popular modification to standard (stochastic) gradient descent, at every iteration limiting the gradient norm to a certain value $c >0$. It is widely used for example for stabilizing the training of deep learning models (Goodfellow et al., 2016), or for enforcing differential privacy (Abadi et al., 2016). Despite popularity and simplicity of the clipping mechanism, its convergence guarantees often require specific values of $c$ and strong noise assumptions.  In this paper, we give convergence guarantees that show precise dependence on arbitrary clipping thresholds $c$ and show that our guarantees are tight with both deterministic and stochastic gradients. In particular, we show that (i) for deterministic gradient descent, the clipping threshold only affects the higher-order terms of convergence, (ii) in the stochastic setting convergence to the true optimum cannot be guaranteed under the standard noise assumption, even under arbitrary small step-sizes. We give ma
    
[^36]: MLRegTest：机器学习正则语言的基准测试

    MLRegTest: A Benchmark for the Machine Learning of Regular Languages. (arXiv:2304.07687v1 [cs.LG])

    [http://arxiv.org/abs/2304.07687](http://arxiv.org/abs/2304.07687)

    本文提出了一个名为MLRegTest的新基准测试，其包含了来自1,800个正则语言的数据集。该测试根据逻辑复杂度和逻辑文字种类组织语言，并可以帮助我们了解机器学习系统在学习不同种类的长距离依赖方面的性能。

    

    评估机器学习系统对已知分类器的学习能力允许细致地检查它们可以学习哪些模式，并在将它们应用于未知分类器的学习时建立信心。本文提出了一个名为MLRegTest的新的序列分类机器学习系统基准测试，其中包含来自1,800个正则语言的训练、开发和测试集。不同类型的形式语言代表着不同种类的长距离依赖，并正确地识别序列中的长距离依赖是机器学习系统成功泛化的已知挑战。MLRegTest根据它们的逻辑复杂度（单调二阶，一阶，命题或单项式表达式）和逻辑文字的种类（字符串，定级字符串，子序列或两者的组合）组织其语言。逻辑复杂度和文字的选择提供了一种系统方法来理解不同种类的长距离依赖和机器学习系统在处理它们时的性能。

    Evaluating machine learning (ML) systems on their ability to learn known classifiers allows fine-grained examination of the patterns they can learn, which builds confidence when they are applied to the learning of unknown classifiers. This article presents a new benchmark for ML systems on sequence classification called MLRegTest, which contains training, development, and test sets from 1,800 regular languages.  Different kinds of formal languages represent different kinds of long-distance dependencies, and correctly identifying long-distance dependencies in sequences is a known challenge for ML systems to generalize successfully. MLRegTest organizes its languages according to their logical complexity (monadic second order, first order, propositional, or monomial expressions) and the kind of logical literals (string, tier-string, subsequence, or combinations thereof). The logical complexity and choice of literal provides a systematic way to understand different kinds of long-distance d
    
[^37]: 变分算子学习：一种训练神经算子和解决偏微分方程的统一方法

    Variational operator learning: A unified paradigm for training neural operators and solving partial differential equations. (arXiv:2304.04234v1 [cs.LG])

    [http://arxiv.org/abs/2304.04234](http://arxiv.org/abs/2304.04234)

    本文提出了变分算子学习（VOL）的范式，同时训练神经算子和解决偏微分方程（PDE）。使用正反传递循环和自动微分实现了变分操作，通过最速下降法和共轭梯度法进行神经算子的简单但有效的训练。实验结果非常好。

    

    本论文提出了一种基于变分方法的新范式，为训练神经算子和用变分形式解决偏微分方程（PDE）提供了一个统一的框架，称为变分算子学习（VOL）。我们首先从神经算子给出的节点解预测中推导出系统的函数逼近，并通过自动微分进行变分操作，构建正反传递循环来推导线性系统的残差。在每次迭代中，我们提供最速下降法（SD）和共轭梯度法（CG）的一个或多个更新步骤，作为训练神经算子的一种简单而有效的更新方法。实验结果显示，所提出的VOL可以学习到在稳定传热和变刚度弹性PDE中各种解算子，结果令人满意，误差较小。该方法几乎实现无标签训练。

    Based on the variational method, we propose a novel paradigm that provides a unified framework of training neural operators and solving partial differential equations (PDEs) with the variational form, which we refer to as the variational operator learning (VOL). We first derive the functional approximation of the system from the node solution prediction given by neural operators, and then conduct the variational operation by automatic differentiation, constructing a forward-backward propagation loop to derive the residual of the linear system. One or several update steps of the steepest decent method (SD) and the conjugate gradient method (CG) are provided in every iteration as a cheap yet effective update for training the neural operators. Experimental results show the proposed VOL can learn a variety of solution operators in PDEs of the steady heat transfer and the variable stiffness elasticity with satisfactory results and small error. The proposed VOL achieves nearly label-free tra
    
[^38]: PLEX：利用可用数据进行机器人操纵预训练的最大化

    PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining. (arXiv:2303.08789v1 [cs.RO])

    [http://arxiv.org/abs/2303.08789](http://arxiv.org/abs/2303.08789)

    PLEX提出了一种新的机器人操纵预训练方法，利用任务不可知的视觉运动轨迹和大量的任务条件下的物体操作视频，在学习通用的操纵例程的同时，通过视频演示学习如何在这个特征空间中规划各种任务。

    

    丰富的表征是实现机器人操纵的关键，但现有的模型架构需要大量数据来学习。不幸的是，理想的机器人操纵训练数据，即各种已注释任务的专家视觉-动作演示，是稀缺的。在本文中，我们提出了一种基于变压器的架构PLEX，它是从任务不可知视觉运动轨迹中学习的，伴随着大量的任务条件下的物体操作视频——这是一种数量可观的与机器人相关的数据。PLEX背后的关键见解是，在观察和行动方面的轨迹下，有助于诱导潜在的特征空间，并训练机器人执行与任务不相关的操作例程，而多样化的仅为视频演示仅可以有效地教会机器人如何在这个特征空间中规划各种任务。与大多数机器人操纵预培训作品不同，PLEX学习了一种可推广的感觉运动多任务策略。

    A rich representation is key to general robotic manipulation, but existing model architectures require a lot of data to learn it. Unfortunately, ideal robotic manipulation training data, which comes in the form of expert visuomotor demonstrations for a variety of annotated tasks, is scarce. In this work we propose PLEX, a transformer-based architecture that learns from task-agnostic visuomotor trajectories accompanied by a much larger amount of task-conditioned object manipulation videos -- a type of robotics-relevant data available in quantity. The key insight behind PLEX is that the trajectories with observations and actions help induce a latent feature space and train a robot to execute task-agnostic manipulation routines, while a diverse set of video-only demonstrations can efficiently teach the robot how to plan in this feature space for a wide variety of tasks. In contrast to most works on robotic manipulation pretraining, PLEX learns a generalizable sensorimotor multi-task polic
    
[^39]: 对正则化中的偏差进行惩罚将使稀疏化

    Penalising the biases in norm regularisation enforces sparsity. (arXiv:2303.01353v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.01353](http://arxiv.org/abs/2303.01353)

    本研究表明，控制神经网络参数的范数可以获得良好的泛化性能。对神经网络中偏差项的范数进行惩罚可以实现稀疏估计量。

    

    当训练神经网络时，通过控制参数的范数往往可以获得良好的泛化性能。然而，正则化参数的范数和所得估计量之间的关系在理论上尚未完全理解。本文针对具有单一隐藏层和一维数据的神经网络，展示了表示函数所需的参数范数由其二阶导数的总变差加权得到，其中所加权的因子为$\sqrt{1+x^2}$。值得注意的是，当不对偏差项的范数进行正则化时，这个加权因子会消失。这个额外的加权因子的存在非常重要，因为它被证明可以强制实现最小范数内插器的唯一性和稀疏性（在拐点数量上）。相反，省略偏差的范数则会导致非稀疏解。因此，在正则化中对偏差项进行惩罚，无论是显式还是隐式地，都会导致稀疏估计量。

    Controlling the parameters' norm often yields good generalisation when training neural networks. Beyond simple intuitions, the relation between regularising parameters' norm and obtained estimators remains theoretically misunderstood. For one hidden ReLU layer networks with unidimensional data, this work shows the parameters' norm required to represent a function is given by the total variation of its second derivative, weighted by a $\sqrt{1+x^2}$ factor. Notably, this weighting factor disappears when the norm of bias terms is not regularised. The presence of this additional weighting factor is of utmost significance as it is shown to enforce the uniqueness and sparsity (in the number of kinks) of the minimal norm interpolator. Conversely, omitting the bias' norm allows for non-sparse solutions. Penalising the bias terms in the regularisation, either explicitly or implicitly, thus leads to sparse estimators.
    
[^40]: 人工和机器关于什么是冒犯存在较大分歧的共情冒犯和噪声审计：统一主观冒犯的人类和机器差异

    Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive. (arXiv:2301.12534v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.12534](http://arxiv.org/abs/2301.12534)

    本文通过人工和机器审核员的共情冒犯和噪声审计研究了冒犯性言论检测中的差异性。结果表明，审核员之间存在广泛的分歧，并且人工审核员和大型语言模型分类器无法预测其他审核员的回应。这对于内容审核具有重要意义。

    

    冒犯性言论检测是内容审核的一个关键组成部分。然而，什么是冒犯性的可以是高度主观的。本文研究了当涉及到现实世界社交网站政治言论时，人工和机器审核员对于什么是冒犯性的存在分歧。我们发现（1）审核员之间（包括人工和机器）存在广泛分歧；和（2）人工审核员和大型语言模型分类器无法预测其他审核员基于他们的政治倾向如何回应。对于（1），我们进行了一个前所未有规模的噪声审计，结合了机器和人工回答。对于（2），我们介绍了一个首创的共情冒犯的数据集。我们的噪声审计揭示了不同机器审核员之间的审核结果差异很大。我们与人工审核员进行的实验表明，政治倾向结合敏感问题会影响到一对一的冒犯，以及共情冒犯。数据集可通过https://github.com/Homan-Lab/voic获得。

    Offensive speech detection is a key component of content moderation. However, what is offensive can be highly subjective. This paper investigates how machine and human moderators disagree on what is offensive when it comes to real-world social web political discourse. We show that (1) there is extensive disagreement among the moderators (humans and machines); and (2) human and large-language-model classifiers are unable to predict how other human raters will respond, based on their political leanings. For (1), we conduct a noise audit at an unprecedented scale that combines both machine and human responses. For (2), we introduce a first-of-its-kind dataset of vicarious offense. Our noise audit reveals that moderation outcomes vary wildly across different machine moderators. Our experiments with human moderators suggest that political leanings combined with sensitive issues affect both first-person and vicarious offense. The dataset is available through https://github.com/Homan-Lab/voic
    
[^41]: 通过代理建模实现高效的激活函数优化

    Efficient Activation Function Optimization through Surrogate Modeling. (arXiv:2301.05785v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05785](http://arxiv.org/abs/2301.05785)

    本文提出了一种基于代理建模的方法，通过扩展的基准测试空间在较少的函数评估次数中发现了优化的高效激活函数架构，并在多个标准基准测试中实现了最先进的性能。

    

    精心设计的激活函数可以提高神经网络在许多机器学习任务中的性能。然而，人类很难构建最优激活函数，而当前的激活函数搜索算法过于昂贵。本文通过三个步骤旨在改进现有技术：首先，通过使用2,913个系统生成的激活函数从头训练卷积、残差和视觉变换器架构来创建 Act-Bench-CNN、Act-Bench-ResNet 和 Act-Bench-ViT 基准数据集。第二，开发了基于代理的方法用于优化基准空间，发现与模型预测分布和激活函数输出分布相关联的 Fisher 信息矩阵的频谱对性能的预测性很高。第三，使用代理在较少的函数评估次数中发现了改进的激活函数架构，同时在几个标准基准测试中实现了最先进的性能。

    Carefully designed activation functions can improve the performance of neural networks in many machine learning tasks. However, it is difficult for humans to construct optimal activation functions, and current activation function search algorithms are prohibitively expensive. This paper aims to improve the state of the art through three steps: First, the benchmark datasets Act-Bench-CNN, Act-Bench-ResNet, and Act-Bench-ViT were created by training convolutional, residual, and vision transformer architectures from scratch with 2,913 systematically generated activation functions. Second, a characterization of the benchmark space was developed, leading to a new surrogate-based method for optimization. More specifically, the spectrum of the Fisher information matrix associated with the model's predictive distribution at initialization and the activation function's output distribution were found to be highly predictive of performance. Third, the surrogate was used to discover improved activ
    
[^42]: 为什么批归一化会损害非独立同分布数据上的联邦学习？

    Why Batch Normalization Damage Federated Learning on Non-IID Data?. (arXiv:2301.02982v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.02982](http://arxiv.org/abs/2301.02982)

    本文通过首次的收敛性分析发现，非独立同分布数据中，在批归一化中局部和全局统计参数不匹配导致了梯度偏差，从而影响了联邦学习的收敛性。

    

    作为一种有前景的分布式学习范式，联邦学习（FL）涉及在网络边缘训练深度神经网络（DNN）模型，同时保护边缘客户端的隐私。为了训练大规模的DNN模型，批归一化（BN）被认为是一种简单有效的加速训练和改善泛化能力的方法。然而，最近的研究发现，在非独立同分布数据的情况下，BN会显著损害FL的性能。尽管已经提出了一些FL算法来解决这个问题，但它们的性能仍然明显低于集中式方案。此外，它们没有提供关于BN如何损害FL收敛性的理论解释。在本文中，我们提出了第一个收敛性分析，以展示在非独立同分布数据下，BN中局部和全局统计参数之间的不匹配导致了局部和全局模型之间的梯度偏差，从而影响了联邦学习的收敛性。

    As a promising distributed learning paradigm, federated learning (FL) involves training deep neural network (DNN) models at the network edge while protecting the privacy of the edge clients. To train a large-scale DNN model, batch normalization (BN) has been regarded as a simple and effective means to accelerate the training and improve the generalization capability. However, recent findings indicate that BN can significantly impair the performance of FL in the presence of non-i.i.d. data. While several FL algorithms have been proposed to address this issue, their performance still falls significantly when compared to the centralized scheme. Furthermore, none of them have provided a theoretical explanation of how the BN damages the FL convergence. In this paper, we present the first convergence analysis to show that under the non-i.i.d. data, the mismatch between the local and global statistical parameters in BN causes the gradient deviation between the local and global models, which, 
    
[^43]: 基于图高斯过程的流形优化

    Optimization on Manifolds via Graph Gaussian Processes. (arXiv:2210.10962v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.10962](http://arxiv.org/abs/2210.10962)

    本论文结合流形学习和高斯过程，利用流形样本点云定义图高斯过程代理模型，通过选择查询点在流形上优化目标函数，具有良好性能。

    

    本文将流形学习技术与高斯过程上限置信度算法相结合，以优化流形上的目标函数。我们的方法是针对在无法获得完整流形表示且查询目标昂贵的应用场景而设计的。我们依靠流形样本点云来定义用于目标函数的图高斯过程代理模型。使用先前所有查询的后验分布逐步选择查询点。我们在查询次数和点云大小方面建立了遗憾上限。数值实验补充了理论，并说明了我们的方法的性能。

    This paper integrates manifold learning techniques within a \emph{Gaussian process upper confidence bound} algorithm to optimize an objective function on a manifold. Our approach is motivated by applications where a full representation of the manifold is not available and querying the objective is expensive. We rely on a point cloud of manifold samples to define a graph Gaussian process surrogate model for the objective. Query points are sequentially chosen using the posterior distribution of the surrogate model given all previous queries. We establish regret bounds in terms of the number of queries and the size of the point cloud. Several numerical examples complement the theory and illustrate the performance of our method.
    
[^44]: 通过变分自由能最小化对贝叶斯神经网络进行原则性剪枝

    Principled Pruning of Bayesian Neural Networks through Variational Free Energy Minimization. (arXiv:2210.09134v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09134](http://arxiv.org/abs/2210.09134)

    本文提出了一种基于变分自由能最小化的贝叶斯模型简化方法，用于对贝叶斯神经网络进行原则性剪枝。通过引入迭代剪枝算法，解决了直接应用贝叶斯模型简化的近似误差问题，并在实验证明了该方法的有效性和优势。

    

    贝叶斯模型简化提供了一种有效的方法，用于比较模型的所有嵌套子模型的性能，而无需重新评估这些子模型。迄今为止，贝叶斯模型简化主要应用于计算神经科学社区的简单模型。本文提出并应用了基于变分自由能最小化的贝叶斯模型简化方法，用于对贝叶斯神经网络进行原则性剪枝。然而，直接应用贝叶斯模型简化会产生近似误差。因此，本文提出了一种新颖的迭代剪枝算法，以缓解直接应用贝叶斯模型简化所引起的问题，并在公开可用的UCI数据集上通过实验证明其效果对不同推理算法。这种新颖的参数剪枝方案解决了信号处理社区使用的当前最先进的剪枝方法的缺点。所提出的方法具有明确的停止准则和m.

    Bayesian model reduction provides an efficient approach for comparing the performance of all nested sub-models of a model, without re-evaluating any of these sub-models. Until now, Bayesian model reduction has been applied mainly in the computational neuroscience community on simple models. In this paper, we formulate and apply Bayesian model reduction to perform principled pruning of Bayesian neural networks, based on variational free energy minimization. Direct application of Bayesian model reduction, however, gives rise to approximation errors. Therefore, a novel iterative pruning algorithm is presented to alleviate the problems arising with naive Bayesian model reduction, as supported experimentally on the publicly available UCI datasets for different inference algorithms. This novel parameter pruning scheme solves the shortcomings of current state-of-the-art pruning methods that are used by the signal processing community. The proposed approach has a clear stopping criterion and m
    
[^45]: 利用迁移学习像病理学家一样自动评分组织图像

    Automatically Score Tissue Images Like a Pathologist by Transfer Learning. (arXiv:2209.05954v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05954](http://arxiv.org/abs/2209.05954)

    该算法通过选择性迁移学习从多个小辅助集中提取知识，从具有“相似”特征的组织图像中学习染色模式，以实现像病理学家一样自动评分组织图像的目标。

    

    癌症是全球第二大死亡原因。早期诊断癌症可以挽救很多生命。病理学家必须手动查看组织微阵列 (TMA) 图像以识别肿瘤，这可能会耗费时间、不一致且主观。现有的自动检测肿瘤的算法要么没有达到病理学家的准确水平，要么需要大量的人工参与。主要挑战是具有不同形状、大小和位置的 TMA 图像可能具有相同的得分。由于医疗组织中的隐私问题和限制，学习 TMA 图像的染色模式受到严重限制。来自不同癌症类型的 TMA 图像可能具有共同的特征，提供了有价值的信息，但直接使用会损害准确性。通过选择性迁移学习来自多个小辅助集的知识，所提出的算法能够提取显示“类似”的组织图像的知识，从而在TMA图像评分方面取得了很好的成果。

    Cancer is the second leading cause of death in the world. Diagnosing cancer early on can save many lives. Pathologists have to look at tissue microarray (TMA) images manually to identify tumors, which can be time-consuming, inconsistent and subjective. Existing algorithms that automatically detect tumors have either not achieved the accuracy level of a pathologist or require substantial human involvements. A major challenge is that TMA images with different shapes, sizes, and locations can have the same score. Learning staining patterns in TMA images requires a huge number of images, which are severely limited due to privacy concerns and regulations in medical organizations. TMA images from different cancer types may have common characteristics that could provide valuable information, but using them directly harms the accuracy. By selective transfer learning from multiple small auxiliary sets, the proposed algorithm is able to extract knowledge from tissue images showing a ``similar" s
    
[^46]: A*Net：基于路径的知识图谱推理方法的可扩展性研究

    A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs. (arXiv:2206.04798v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.04798](http://arxiv.org/abs/2206.04798)

    本论文提出了A*Net，一种基于路径的知识图谱推理方法，通过学习优先级函数，实现了对重要节点和边的选择，从而提高了训练和推理的效率。实验证明A*Net在性能和效率之间取得了平衡，在知识图谱推理中具有竞争力，并在大规模数据集上获得了新的最先进结果。

    

    长期以来，对大规模知识图谱的推理一直由嵌入方法主导。虽然基于路径的方法具有嵌入方法所缺乏的归纳能力，但其可扩展性受到指数级路径数量的限制。在这里，我们提出了A*Net，一种适用于知识图谱推理的可扩展路径方法。受到A*算法最短路径问题的启发，我们的A*Net学习了一个优先级函数，在每次迭代中选择重要的节点和边，以减少训练和推理的时间和内存占用。选择的节点和边的比例可以指定，以在性能和效率之间进行权衡。在传导性和归纳性知识图推理基准测试中的实验证明，A*Net在仅访问每次迭代中的10%节点和10%边的情况下，实现了与现有最先进基于路径方法竞争的性能。在一个百万级数据集ogbl-wikikg2上，A*Net不仅取得了新的最先进结果，还实现了收敛。

    Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A* algorithm for shortest path problems, our A*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A*Net not only achieves a new state-of-the-art result, but also converges 
    
[^47]: 探索无代理数据的分布式知识一致性在联邦蒸馏中的应用

    Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation. (arXiv:2204.07028v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.07028](http://arxiv.org/abs/2204.07028)

    本文提出了一种基于分布式知识一致性的无代理数据联邦蒸馏算法，解决了客户端模型异质性引起的知识差异问题，从而提高了模型表示的准确性。

    

    联邦学习 (FL) 是一种保护隐私的机器学习范 paradigm, 在此服务器周期性地收集客户端的本地模型参数, 而不组装其私有数据. 有限的通讯和个性化需求对FL提出了严峻挑战. 联邦蒸馏 (FD) 被提出同时解决上述两个问题, 与此服务器和客户端之间交换知识, 支持异构本地模型同时显著减少通讯开销. 然而，大多数现有的FD方法需要一个代理数据集，而这在现实中通常是不可用的. 一些最近的无代理数据的FD方法可以消除额外的公共数据的需求, 但由于客户端模型的异质性而产生了明显的差异, 导致服务器上的模型表示不明确，并且不可避免地降低了准确性.

    Federated learning (FL) is a privacy-preserving machine learning paradigm in which the server periodically aggregates local model parameters from clients without assembling their private data.  Constrained communication and personalization requirements pose severe challenges to FL. Federated distillation (FD) is proposed to simultaneously address the above two problems, which exchanges knowledge between the server and clients, supporting heterogeneous local models while significantly reducing communication overhead. However, most existing FD methods require a proxy dataset, which is often unavailable in reality.  A few recent proxy-data-free FD approaches can eliminate the need for additional public data, but suffer from remarkable discrepancy among local knowledge due to client-side model heterogeneity, leading to ambiguous representation on the server and inevitable accuracy degradation.  To tackle this issue, we propose a proxy-data-free FD algorithm based on distributed knowledge c
    
[^48]: 比较序贯预测器

    Comparing Sequential Forecasters. (arXiv:2110.00115v5 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2110.00115](http://arxiv.org/abs/2110.00115)

    本文提出了一种比较序贯预测器的方法，通过设计新的序贯推断程序来估计预测得分的时变差异，这种方法避免了对预测和结果生成方式的不可验证假设。

    

    考虑两个预测器，在一段时间内对一系列事件进行单次预测。我们提出一个相对基础的问题：在不假设预测和结果生成方式的情况下，我们如何比较这些预测器，无论是在线还是事后比较？在本文中，我们通过设计用于估计时变预测得分差异的新型序贯推断程序，对这个问题给出了严格的答案。为此，我们采用置信序列（CS），它是一系列置信区间，可以连续监测并在任意数据依赖停时（“anytime-valid”）下有效。我们的置信序列的宽度是自适应的，适应了得分差异的底层方差。它们的构建基于博弈论统计框架，在这个框架中，我们进一步确定了用于顺序检验弱零假设的e过程和p过程，即一个预测器平均表现是否优于另一个预测器。

    Consider two forecasters, each making a single prediction for a sequence of events over time. We ask a relatively basic question: how might we compare these forecasters, either online or post-hoc, while avoiding unverifiable assumptions on how the forecasts and outcomes were generated? In this paper, we present a rigorous answer to this question by designing novel sequential inference procedures for estimating the time-varying difference in forecast scores. To do this, we employ confidence sequences (CS), which are sequences of confidence intervals that can be continuously monitored and are valid at arbitrary data-dependent stopping times ("anytime-valid"). The widths of our CSs are adaptive to the underlying variance of the score differences. Underlying their construction is a game-theoretic statistical framework, in which we further identify e-processes and p-processes for sequentially testing a weak null hypothesis -- whether one forecaster outperforms another on average (rather tha
    
[^49]: GNN在节点分类/回归任务中的逼近能力研究

    On the approximation capability of GNNs in node classification/regression tasks. (arXiv:2106.08992v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.08992](http://arxiv.org/abs/2106.08992)

    本文研究GNN在节点分类/回归任务中的逼近能力，发现在概率上GNN对于任何有限域上的可测函数都是通用逼近器。我们的研究还揭示了GNN的表达能力与其深度和宽度的变化有关，并且结果可扩展用于大型图。

    

    图神经网络(GNNs)是一种广泛的用于图处理的连接式模型。最近的研究表明，GNN可以逐步逼近关于图的任意函数，这取决于由Weisfeiler--Lehman(WL)测试定义的图等价关系。然而，这些结果存在一些局限性，一方面因为它们是用Stone-Weierstrass定理来推导得出的，这种方法本质上只是一种存在性证明，另一方面因为它们假设目标函数是连续的。此外，所有当前的结果都是专门针对图分类/回归任务的，而节点分类/回归问题也非常常见。在本文中，我们提出了一种新的方法来证明GNN的逼近能力，克服了这些局限性。我们发现，对于节点分类/回归任务，无论目标函数是连续的还是离散的，GNN在概率上都是通用逼近器。具体而言，我们发现，在一些温和条件下，任何有限域上的可测函数都可以通过一个只有一层隐藏层、ReLU激活函数和共享所有节点的边权值的GNN进行逼近。我们的分析也揭示了GNN的表达能力随其深度和宽度的变化及其结果针对大型图的可扩展性。

    Graph Neural Networks (GNNs) are a broad class of connectionist models for graph processing. Recent studies have shown that GNNs can approximate any function on graphs, modulo the equivalence relation on graphs defined by the Weisfeiler--Lehman (WL) test. However, these results suffer from some limitations, both because they were derived using the Stone--Weierstrass theorem -- which is existential in nature, -- and because they assume that the target function to be approximated must be continuous. Furthermore, all current results are dedicated to graph classification/regression tasks, where the GNN must produce a single output for the whole graph, while also node classification/regression problems, in which an output is returned for each node, are very common. In this paper, we propose an alternative way to demonstrate the approximation capability of GNNs that overcomes these limitations. Indeed, we show that GNNs are universal approximators in probability for node classification/regre
    
[^50]: 只需简单步骤：Frank-Wolfe算法和广义自协调函数

    Simple steps are all you need: Frank-Wolfe and generalized self-concordant functions. (arXiv:2105.13913v6 [math.OC] UPDATED)

    [http://arxiv.org/abs/2105.13913](http://arxiv.org/abs/2105.13913)

    本论文介绍了一种简单的Frank-Wolfe算法变体，利用广义自协调函数的特性，在不需要使用二阶信息或估计局部平滑度参数的情况下，以$\mathcal{O}(1/t)$的收敛速度达到了优化目标。

    

    广义自协调是许多重要学习问题的目标函数中存在的一个关键特性。我们建立了一个简单的Frank-Wolfe变体的收敛速度，该变体使用了开环步长策略$\gamma_t=2/(t+2)$，对于这类函数在原始间隙和Frank-Wolfe间隙方面获得了$\mathcal{O}(1/t)$的收敛速度，其中$t$是迭代次数。这避免了使用二阶信息或需要估计先前工作的局部平滑度参数。我们还展示了不同常见情况下的改进收敛速度，例如，当所考虑的可行域是均匀凸的或者是多面体的时候。

    Generalized self-concordance is a key property present in the objective function of many important learning problems. We establish the convergence rate of a simple Frank-Wolfe variant that uses the open-loop step size strategy $\gamma_t = 2/(t+2)$, obtaining a $\mathcal{O}(1/t)$ convergence rate for this class of functions in terms of primal gap and Frank-Wolfe gap, where $t$ is the iteration count. This avoids the use of second-order information or the need to estimate local smoothness parameters of previous work. We also show improved convergence rates for various common cases, e.g., when the feasible region under consideration is uniformly convex or polyhedral.
    

