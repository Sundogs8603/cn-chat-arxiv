# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization](https://arxiv.org/abs/2401.18079) | KVQuant是一种解决LLM推理中大量内存消耗的KV缓存量化方法，通过引入新颖的量化方法，包括分通道键量化、RoPE前量化键和非均匀KV缓存量化，准确表示超低精度的KV激活。 |
| [^2] | [Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?](https://arxiv.org/abs/2401.18070) | 该研究调查了语言模型在解决算术问题时与人类学习者的认知偏见。研究发现，当前最先进的语言模型在文本理解和解决方案规划阶段表现出与人类类似的偏见。 |
| [^3] | [RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/abs/2401.18059) | 本论文提出了一种递归嵌入、聚类和摘要的方法，通过构建不同摘要级别的树，从下往上整合并检索长度较长的文档，对传统检索增强的语言模型进行改进，实现了在复杂的多步推理问答任务上的最先进的结果提升。 |
| [^4] | [LongAlign: A Recipe for Long Context Alignment of Large Language Models](https://arxiv.org/abs/2401.18058) | LongAlign是一种用于大型语言模型的长上下文对齐的方法，通过指导微调和使用打包、排序和损失加权策略，它在长上下文任务中表现优异，相比现有的方法提高了多达30\%的性能。 |
| [^5] | [Rank Supervised Contrastive Learning for Time Series Classification](https://arxiv.org/abs/2401.18057) | Rank Supervised Contrastive Learning (RankSCL) proposes a targeted data augmentation method and a novel rank loss to improve time series classification by utilizing fine-grained relative similarity information. |
| [^6] | [Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based Action Recognition](https://arxiv.org/abs/2401.18054) | 本研究关注连续图学习中的GNN，并提出了基于骨骼动作识别的连续图学习基准。我们研究了CGL方法在类别和任务顺序上的敏感性以及在不同架构下的敏感性。 |
| [^7] | [Epidemic Modeling using Hybrid of Time-varying SIRD, Particle Swarm Optimization, and Deep Learning](https://arxiv.org/abs/2401.18047) | 该论文开发了一个集流行病建模、粒子群优化和深度学习于一体的混合模型，用于处理非稳态模式和流行病的多个波动。模型主要满足三个目标：周期性估计模型参数、考虑所有方面的影响以提高预测准确性、利用深度学习预测模型参数。 |
| [^8] | [Variable selection for Na\"ive Bayes classification](https://arxiv.org/abs/2401.18039) | 该论文提出了一种稀疏版本的Na\"ive Bayes分类器，通过考虑特征间的相关结构实现了稀疏性，并支持不同的性能度量来指导特征选择。 |
| [^9] | [Optimizing contrastive learning for cortical folding pattern detection](https://arxiv.org/abs/2401.18035) | 本研究使用对比学习方法优化了深度学习模型，能够检测髓样皮层的折叠模式，为解决皮层折叠的变异性和与个体行为特征以及病理学的关系问题提供了一种新的方法。 |
| [^10] | [Prompt-Driven LLM Safeguarding via Directed Representation Optimization](https://arxiv.org/abs/2401.18018) | 通过研究模型表示的影响，我们发现安全提示并没有明显增强恶意和无害查询之间的区分，并提出了一种名为DRO的方法，用于自动优化安全提示。 |
| [^11] | [Causal Discovery by Kernel Deviance Measures with Heterogeneous Transforms](https://arxiv.org/abs/2401.18017) | 本文提出了一种通过异构变换的核偏差测量来解决因果关系发现中的挑战，以捕捉因果关系和效应之间的高阶结构变异的主要标记。 |
| [^12] | [Causal Coordinated Concurrent Reinforcement Learning](https://arxiv.org/abs/2401.18012) | 这项工作提出了一种用于并发强化学习的新算法框架，通过数据共享和协同探索来学习更高效和表现更好的策略。算法中利用因果推断算法提取控制个体差异的模型参数，并提出了一种基于相似性度量的数据共享方案，展示了更快的学习速度和多样化动作选择的有效性。 |
| [^13] | [EEG-GPT: Exploring Capabilities of Large Language Models for EEG Classification and Interpretation](https://arxiv.org/abs/2401.18006) | EEG-GPT是一种利用大型语言模型来分类和解读EEG的方法，它能够实现多尺度电生理理解和分类，且在few-shot学习范式中表现出色。 |
| [^14] | [Multilinear Operator Networks](https://arxiv.org/abs/2401.17992) | 该论文提出了一种名为MONet的模型，该模型仅依赖多线性算子来进行图像识别，通过捕捉输入元素的高次交互，优于以前的多项式网络，并与现代架构性能相近。这一研究可以激发对完全使用多线性操作的模型的进一步研究。 |
| [^15] | [Understanding polysemanticity in neural networks through coding theory](https://arxiv.org/abs/2401.17975) | 通过应用编码理论和神经科学工具，本文提出了一种新颖的实践方法来理解神经网络中的多语义性，并对多语义神经元对学习性能的优势进行了解释。 |
| [^16] | [MelNet: A Real-Time Deep Learning Algorithm for Object Detection](https://arxiv.org/abs/2401.17972) | MelNet是一种用于实时目标检测的深度学习算法，经过训练后，在KITTI数据集上表现出良好的性能，同时也展示了迁移学习和定制模型在目标检测中的有效性。 |
| [^17] | [CONCORD: Towards a DSL for Configurable Graph Code Representation](https://arxiv.org/abs/2401.17967) | CONCORD是一种面向可配置图形代码表示的领域特定语言，通过实现减少图形大小复杂性的启发式方法，能够自动生成代码表示，并在代码异味检测中展示出效果。 |
| [^18] | [Convergence Analysis for General Probability Flow ODEs of Diffusion Models in Wasserstein Distances](https://arxiv.org/abs/2401.17958) | 本文提供了在2-Wasserstein距离中的一般类概率流ODE抽样器的非渐近收敛性分析，假设得分估计准确。 |
| [^19] | [LOCOST: State-Space Models for Long Document Abstractive Summarization](https://arxiv.org/abs/2401.17919) | LOCOST是一种基于状态空间模型的编码器-解码器架构，用于处理长文档的抽象摘要生成。与基于稀疏注意模式的最先进模型相比，LOCOST具有更低的计算复杂度，并且能够在训练和推断期间节省大量内存。在评估中，LOCOST在长文档摘要化任务上达到了93-96%的性能水平，并且能够处理超过600K个标记的输入文本。 |
| [^20] | [Graph Attention-based Reinforcement Learning for Trajectory Design and Resource Assignment in Multi-UAV Assisted Communication](https://arxiv.org/abs/2401.17880) | 本论文提出了一种基于图注意力的多智能体可信区间（GA-MATR）强化学习框架，用于解决多无人机辅助通信中的路径设计和资源分配问题。该框架通过引入图循环网络和注意机制，能够处理复杂的通信网络拓扑结构，提取有用的信息和模式。 |
| [^21] | [Efficient Subseasonal Weather Forecast using Teleconnection-informed Transformers](https://arxiv.org/abs/2401.17870) | 提出了一种使用远程连接通知的Transformer模型来实现高效的次季节天气预报，该方法通过利用预训练模型和集成远程连接通知的时间模块来改进预测能力。 |
| [^22] | [Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model](https://arxiv.org/abs/2401.17868) | 本文介绍了一种名为Conv-LoRA的参数高效微调方法，它通过在SAM的基础上整合轻量级卷积参数和低秩调整，将图像相关的归纳偏见注入ViT编码器，以提高SAM在特定领域的分割能力和高级图像语义学习能力。 |
| [^23] | [Manipulating Predictions over Discrete Inputs in Machine Teaching](https://arxiv.org/abs/2401.17865) | 本文研究离散域中的机器教学，在操纵学生模型的预测方面具有显著的数值优势，可用于矫正错误的预测或恶意操纵模型实现个人利益。 |
| [^24] | [A Cross-View Hierarchical Graph Learning Hypernetwork for Skill Demand-Supply Joint Prediction](https://arxiv.org/abs/2401.17838) | 本文提出了一个跨视角分层图学习超网络（CHGH）框架，用于联合预测技能需求和供应。框架包括跨视角图编码器、层次图编码器和条件超解码器，能够捕捉不同技能之间的关系和需求供应的内在联系。 |
| [^25] | [Predicting the Future with Simple World Models](https://arxiv.org/abs/2401.17835) | 本研究提出了一种正则化方案，通过简化模型的潜在动态，使得世界模型更加可预测。该模型在未来潜在状态预测、视频预测和规划中展现出良好的性能。 |
| [^26] | [Privacy-preserving data release leveraging optimal transport and particle gradient descent](https://arxiv.org/abs/2401.17823) | 该研究提出了一种基于边际的保护隐私数据合成方法PrivPGD，利用了最优输运和粒子梯度下降的工具。该方法在不同领域的数据集上表现出色，具有高度的可扩展性和灵活性，并可以满足特定的领域约束条件。 |
| [^27] | [SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering](https://arxiv.org/abs/2401.17809) | 提出了一种主题词嵌入修改框架（SWEA），通过在推理阶段修改主题的表示来编辑知识，保护模型的原始权重，避免不可逆的损害和额外的推理开销。 |
| [^28] | [Distillation Enhanced Time Series Forecasting Network with Momentum Contrastive Learning](https://arxiv.org/abs/2401.17802) | 本论文提出了一种创新的蒸馏增强框架，用于长序列时间序列预测。通过设计可学习的数据增强机制和带有动量更新的对比学习任务，能够充分利用时间序列数据的复杂性，并获得更鲁棒的表示。 |
| [^29] | [Graph Transformers without Positional Encodings](https://arxiv.org/abs/2401.17791) | 本文介绍了一种不需要位置编码的图变压器模型，该模型通过注意机制本身包含图结构信息，并通过实验证明了其有效性。 |
| [^30] | [RADIN: Souping on a Budget](https://arxiv.org/abs/2401.17790) | 本文提出了一种名为RADIN的低成本集成模型优化方法，通过使用平均集合logit性能来近似soup性能，实现了加速模型soup的效果。 |
| [^31] | [Robustly overfitting latents for flexible neural image compression](https://arxiv.org/abs/2401.17789) | 这项研究提出了一种鲁棒的过拟合潜变量方法来改进神经图像压缩模型，通过使用SGA+，可以显著提高性能并减少对超参数选择的敏感性。 |
| [^32] | [Vision-Assisted Digital Twin Creation for mmWave Beam Management](https://arxiv.org/abs/2401.17781) | 这项研究提出了一种基于视觉的数字孪生创建技术，通过使用单个摄像头和位置信息，有效解决了在毫米波系统中3D数字孪生的精度要求，并展示了在波束获取任务中的性能优势。 |
| [^33] | [A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with Uniform PAC Guarantees](https://arxiv.org/abs/2401.17780) | 本文介绍了一种带有均匀PAC保证的策略梯度原始对偶算法，用于在线约束马尔可夫决策过程（CMDP）问题。该算法同时保证了收敛到最优策略、次线性遗憾和多项式样本复杂度，并在实证研究中验证了其优越性能。 |
| [^34] | [Regularized Linear Discriminant Analysis Using a Nonlinear Covariance Matrix Estimator](https://arxiv.org/abs/2401.17760) | 本文研究了使用非线性协方差矩阵估计器的正则化线性判别分析方法，以解决特征空间维度高于训练数据大小时数据协方差矩阵病态导致效率低下的问题。 |
| [^35] | [PF-GNN: Differentiable particle filtering based approximation of universal graph representations](https://arxiv.org/abs/2401.17752) | PF-GNN是一种可微的基于粒子滤波的通用图表示逼近算法，通过引入精确同构求解技术指导学习过程，从而提高了图神经网络的表达能力。 |
| [^36] | [Algorithmic Robust Forecast Aggregation](https://arxiv.org/abs/2401.17743) | 该论文提出了一种算法框架用于算法鲁棒的预测聚合，旨在找到与全知聚合相比具有最小最坏情况遗憾的聚合器。数值实验证明了该算法框架的优越性能。 |
| [^37] | [Operator learning without the adjoint](https://arxiv.org/abs/2401.17739) | 本论文提出了一种不需要探测伴随算子的算子学习方法，通过在Fourier基上进行投影来逼近一类非自伴随的无限维紧算子，并应用于恢复椭圆型偏微分算子的格林函数。这是第一个试图填补算子学习理论与实践差距的无需伴随算子分析。 |
| [^38] | [Harnessing Smartwatch Microphone Sensors for Cough Detection and Classification](https://arxiv.org/abs/2401.17738) | 本研究利用智能手表的麦克风传感器监测和分类了各种咳嗽，通过结构化处理和专门的1D卷积神经网络模型，成功实现了98.49%和98.2%的准确率，并成功识别出四种不同的咳嗽类型。 |
| [^39] | [Hierarchical Bias-Driven Stratification for Interpretable Causal Effect Estimation](https://arxiv.org/abs/2401.17737) | BICauseTree是一种基于层级偏差驱动分层的可解释因果效应估计方法，通过使用决策树进行平衡、减少偏差和确定目标人群定义，提供了可解释性和透明性。 |
| [^40] | [Towards Physical Plausibility in Neuroevolution Systems](https://arxiv.org/abs/2401.17733) | 本研究关注神经进化系统中物理合理性的问题，提出了一种旨在最大化人工神经网络模型准确性同时最小化功耗的方法，通过引入新的突变策略和训练技术来优化模型表现和节能效果。 |
| [^41] | [Predicting suicidal behavior among Indian adults using childhood trauma, mental health questionnaires and machine learning cascade ensembles](https://arxiv.org/abs/2401.17705) | 该研究使用机器学习方法，结合儿童创伤、心理健康参数和其他行为因素，预测印度成年人的自杀行为。结果表明，级联集成学习方法能有效分类和预测自杀行为。 |
| [^42] | [Datacube segmentation via Deep Spectral Clustering](https://arxiv.org/abs/2401.17695) | 通过应用深度聚类算法对数据立方体像素的光谱属性进行无监督聚类，可以实现数据立方体的图像分割和统计解释。 |
| [^43] | [Convergence analysis of t-SNE as a gradient flow for point cloud on a manifold](https://arxiv.org/abs/2401.17675) | 我们在本论文提出了关于t-SNE算法的收敛性分析，证明了t-SNE生成的点是有界的，并得出了KL散度最小值的存在性。 |
| [^44] | [An attempt to generate new bridge types from latent space of energy-based model](https://arxiv.org/abs/2401.17657) | 使用能量模型进行桥梁创新，通过博弈论解释损失函数，并利用朗之万动力学技术生成能量值较低的新样本，建立基于能量的桥梁生成模型。 |
| [^45] | [A primer on synthetic health data](https://arxiv.org/abs/2401.17653) | 深层生成模型的进展使得创造逼真合成健康数据集成为可能，这些合成数据集可以在不公开敏感信息的情况下进行安全数据共享，并支持各种倡议和项目构思。然而，评估合成数据集与原始数据集的相似性和预测效用，以及解决隐私和法规问题仍然是挑战。 |
| [^46] | [Spatial-and-Frequency-aware Restoration method for Images based on Diffusion Models](https://arxiv.org/abs/2401.17629) | 本文提出了一种名为SaFaRI的基于空间和频率感知的扩散模型，用于图像恢复。在各种噪声逆问题上，SaFaRI在ImageNet数据集和FFHQ数据集上实现了最先进的性能。 |
| [^47] | [Generative AI to Generate Test Data Generators](https://arxiv.org/abs/2401.17626) | 本研究评估了生成式人工智能在不同领域生成测试数据的能力，并证明它能够在不同的集成级别上生成逼真的测试数据生成器。 |
| [^48] | [Graph Multi-Similarity Learning for Molecular Property Prediction](https://arxiv.org/abs/2401.17615) | 提出了图多相似性学习 (GraphMSL)框架，通过引入连续尺度的多相似性度量，包括自相似度和相对相似性，以及对不同化学特征进行融合，提高了分子属性预测的效果和适用性。 |
| [^49] | [IGCN: Integrative Graph Convolutional Networks for Multi-modal Data](https://arxiv.org/abs/2401.17612) | IGCN是用于多模态数据的综合神经网络，通过综合分析多模态数据来获得更好的学习表示，并提高模型的可解释性。 |
| [^50] | [Propagation and Pitfalls: Reasoning-based Assessment of Knowledge Editing through Counterfactual Tasks](https://arxiv.org/abs/2401.17585) | 该论文针对现有的知识编辑方法在推理能力方面的限制，通过引入ReCoE数据集进行了深入分析。研究发现所有的模型编辑方法在该数据集上表现较差，尤其在特定的推理方案中。此外，通过对编辑模型思维链生成的分析，揭示了现有方法的不足之处，包括对事实编辑、事实回忆能力和连贯性的考量。 |
| [^51] | [Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion](https://arxiv.org/abs/2401.17583) | 本文介绍了一种名为敏捷但安全（ABS）的学习控制框架，能够实现四足机器人的敏捷且无碰撞行走。该框架通过一个学习得到的控制论到达-避免值网络来实现策略切换，并通过协作运行的敏捷策略和恢复策略，使机器人能够高速且安全地导航。 |
| [^52] | [Graph Contrastive Learning with Cohesive Subgraph Awareness](https://arxiv.org/abs/2401.17580) | 本研究提出了一种名为CTAug的新框架，将内聚子图意识无缝整合到图对比学习中。通过改进图拓扑增强和图学习过程，提高了对各种图的表征学习性能。 |
| [^53] | [Scavenging Hyena: Distilling Transformers into Long Convolution Models](https://arxiv.org/abs/2401.17574) | 本文介绍了一种通过使用知识蒸馏将Transformer模型中的注意力头替换为Hyena，从而提高效率并处理长上下文信息的方法，超越了传统预训练方法，在准确性和效率方面取得了优秀的结果。这一技术为追求可持续的AI解决方案做出了贡献，实现了计算能力和环境影响的平衡。 |
| [^54] | [Tensor-based process control and monitoring for semiconductor manufacturing with unstable disturbances](https://arxiv.org/abs/2401.17573) | 本文提出了一种基于张量的工艺控制和监控方法，用于半导体制造过程中高维度基于图像的叠加误差的复杂结构。该方法通过有限的控制配方减小叠加误差，并设计了稳定的张量数据控制器来处理高维度扰动。 |
| [^55] | [Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators](https://arxiv.org/abs/2401.17548) | 本文提出了一种新方法，LIFT，通过利用通道相关性和领先指标，为多元时间序列预测提供准确的预测。LIFT方法可以无缝与任意时间序列预测方法协作，大量实验证明了其有效性。 |
| [^56] | [Effective Multi-Stage Training Model For Edge Computing Devices In Intrusion Detection](https://arxiv.org/abs/2401.17546) | 这项研究提出了一种针对边缘计算设备在入侵检测中的有效多阶段训练模型，通过增强的剪枝方法和模型压缩技术，提升了系统的效果，同时保持了高准确性。 |
| [^57] | [Trainable Fixed-Point Quantization for Deep Learning Acceleration on FPGAs](https://arxiv.org/abs/2401.17544) | 本论文提出了一种可训练的固定点量化方法QFX，实现了基于FPGA的深度学习加速。QFX可以自动学习二进制点位置，并引入了无乘法的量化策略以最小化硬件资源的使用。 |
| [^58] | [Data-Effective Learning: A Comprehensive Medical Benchmark](https://arxiv.org/abs/2401.17542) | 这项研究引入了一个综合基准，用于评估医学领域的数据有效学习。该基准包括大量的医疗数据样本、基准方法和新的评估指标，能够准确评估数据有效学习的性能。 |
| [^59] | [Towards Understanding Variants of Invariant Risk Minimization through the Lens of Calibration](https://arxiv.org/abs/2401.17541) | 本研究通过比较分析使用不同近似方法的不变风险最小化（IRM）技术，以期望校准误差（ECE）作为关键指标，观察到基于信息瓶颈的IRM在压缩关键特征上表现最佳。 |
| [^60] | [Enhancing Score-Based Sampling Methods with Ensembles](https://arxiv.org/abs/2401.17539) | 这项研究通过引入集成方法来增强基于分数的抽样方法，成功开发了一种利用粒子集合动态计算近似反扩散漂移的无梯度抽样技术，并在多个示例中展示了其有效性，包括对复杂概率分布的建模和在地球物理科学中的应用。 |
| [^61] | [Game-Theoretic Unlearnable Example Generator](https://arxiv.org/abs/2401.17523) | 本论文研究了从游戏论域的角度来进行不可学习示例攻击的方法。研究发现，博弈均衡给出了最强大的毒攻击，并提出了一种名为游戏论域不可学习示例（GUE）的新攻击方法。 |
| [^62] | [Arrows of Time for Large Language Models](https://arxiv.org/abs/2401.17505) | 这篇论文通过研究自回归大型语言模型的时间方向性，发现了模型在建模自然语言能力上存在时间上的不对称性。从信息理论的角度来看，这种差异理论上是不应该存在的。通过稀疏性和计算复杂性的考虑，提供了一个理论框架来解释这种不对称性的出现。 |
| [^63] | [CaMU: Disentangling Causal Effects in Deep Model Unlearning](https://arxiv.org/abs/2401.17504) | 本研究对深度模型的“遗忘”进行了因果分析，并提出了一种新的方法来解决移除已遗忘数据时对剩余数据信息损失的问题。 |
| [^64] | [Pixel to Elevation: Learning to Predict Elevation Maps at Long Range using Images for Autonomous Offroad Navigation](https://arxiv.org/abs/2401.17484) | 本研究提出了一种学习方法，可以通过车载视角图像实时预测长距离地形高程地图。该方法包括transformer-based编码器、方向感知的位置编码和历史增强的可学习地图嵌入。通过学习视角图像与鸟瞰图高程地图之间的关联，结合车辆姿态信息和视觉图像特征，实现更好的地图预测时序一致性。 |
| [^65] | [Detecting mental disorder on social media: a ChatGPT-augmented explainable approach](https://arxiv.org/abs/2401.17477) | 本文提出了一种利用大型语言模型，可解释人工智能和对话代理器ChatGPT相结合的新方法，以解决通过社交媒体检测抑郁症的可解释性挑战。通过将Twitter特定变体BERTweet与自解释模型BERT-XDD相结合，并借助ChatGPT将技术解释转化为人类可读的评论，实现了解释能力的同时提高了可解释性。这种方法可以为发展社会负责任的数字平台，促进早期干预做出贡献。 |
| [^66] | [Rendering Wireless Environments Useful for Gradient Estimators: A Zero-Order Stochastic Federated Learning Method](https://arxiv.org/abs/2401.17460) | 提出了一种新颖的零阶随机联邦学习方法，通过利用无线通信通道的特性，在学习算法中考虑了无线通道，避免了资源的浪费和分析难度。 |
| [^67] | [Liquid Democracy for Low-Cost Ensemble Pruning](https://arxiv.org/abs/2401.17443) | 本文介绍了一种利用液态民主实现低成本集合剪枝的方法。通过液态民主的委派机制识别和移除冗余分类器，成功降低了集合训练的计算成本，并比某些增强方法具有更高的准确性。同时，本文还展示了计算社会选择文献框架在非传统领域问题中的应用。 |
| [^68] | [Explaining Predictive Uncertainty by Exposing Second-Order Effects](https://arxiv.org/abs/2401.17441) | 该论文研究发现，预测不确定性主要受到单个特征或特征之间乘积相互作用的二阶影响的影响。作者提出了一种基于这些二阶影响来解释预测不确定性的新方法。该方法通过简单的协方差计算对一阶解释进行处理，可以将常见的归因技术转化为强大的二阶不确定性解释器。作者通过量化评估验证了该方法解释的准确性，并展示了整体实用性。 |
| [^69] | [Can Large Language Models Replace Economic Choice Prediction Labs?](https://arxiv.org/abs/2401.17435) | 该论文研究大型语言模型是否能够取代经济实验室进行选择预测，并通过相关实验证明了其可行性。 |
| [^70] | [Superiority of Multi-Head Attention in In-Context Linear Regression](https://arxiv.org/abs/2401.17426) | 多头注意力在上下文线性回归任务中表现出优于单头注意力的性能，通过理论分析证明了多头注意力在大嵌入维度情况下有更小的预测损失，并且在各种数据分布设置下都显示出优势。 |
| [^71] | [Application of Neural Networks for the Reconstruction of Supernova Neutrino Energy Spectra Following Fast Neutrino Flavor Conversions](https://arxiv.org/abs/2401.17424) | 本研究利用物理信息驱动的神经网络（PINNs），基于多能量中微子气体中中微子角分布的前两个矩，预测了快速 flavor 转换的结果，取得了较低的预测误差。 |
| [^72] | [Through-Wall Imaging based on WiFi Channel State Information](https://arxiv.org/abs/2401.17417) | 本研究提出了一种通过WiFi信道状态信息实现穿墙成像的创新方法，可以将室内环境可视化监测到房间边界之外，无需摄像机，具有广泛的实际应用潜力。 |
| [^73] | [Solving Boltzmann Optimization Problems with Deep Learning](https://arxiv.org/abs/2401.17408) | 本文的贡献是一种新颖的机器学习方法来解决基于伊辛硬件的优化问题。 |
| [^74] | [Step-size Optimization for Continual Learning](https://arxiv.org/abs/2401.17401) | 本文研究了连续学习中的步长优化问题，指出传统算法忽视了对整体目标函数的影响，而随机元梯度下降算法能够明确优化步长向量，在简单问题中表现更优。 |
| [^75] | [Timeseries Suppliers Allocation Risk Optimization via Deep Black Litterman Model](https://arxiv.org/abs/2401.17350) | 通过深度黑石贝莱曼模型和时空图神经网络，我们优化了供应商选择和订单分配，同时解决了零阶情况下的可信度问题，实现了准确的预测和精确的置信区间。 |
| [^76] | [Reproducibility, energy efficiency and performance of pseudorandom number generators in machine learning: a comparative study of python, numpy, tensorflow, and pytorch implementations](https://arxiv.org/abs/2401.17345) | 本研究比较了机器学习中常用的伪随机数生成器在统计质量、数值可重复性、时间效率和能源消耗等方面与原始C实现的差异。 |
| [^77] | [A Latent Space Metric for Enhancing Prediction Confidence in Earth Observation Data](https://arxiv.org/abs/2401.17342) | 这项研究提出了一种新的方法来估计利用地球观测数据进行回归任务时机器学习模型预测的置信度。通过利用潜在空间表示来推导置信度度量，建立了潜在表示中的欧几里得距离与个体蚊子种群预测的绝对误差之间的相关性。该方法在意大利威尼托地区和德国上莱茵河谷的地区得到了验证，并表现出较高的可靠性和可信度。 |
| [^78] | [Decentralized Federated Learning: A Survey on Security and Privacy](https://arxiv.org/abs/2401.17319) | 去中心化联邦学习架构允许保护隐私，但也引入了新的安全和隐私威胁，该综述对去中心化联邦学习中的威胁、对手和防御机制进行了研究。 |
| [^79] | [Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You](https://arxiv.org/abs/2401.16092) | 多语言文本到图像生成模型存在性别偏见；通过MAGBIG评估模型时，发现模型对不同语言具有重要差异；我们呼吁研究多语言模型领域消除性别偏见。 |
| [^80] | [Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization](https://arxiv.org/abs/2401.15496) | 本文提出了Baichuan2-Sum模型，通过指导微调Baichuan2-7B模型进行对话摘要，并应用NEFTune技术改进训练过程。实验证明该模型在CSDS和SAMSUM数据集上取得了新的最先进结果。 |
| [^81] | [Wind speed super-resolution and validation: from ERA5 to CERRA via diffusion models](https://arxiv.org/abs/2401.15469) | 本论文提出了一种利用扩散模型以数据驱动的方式近似CERRA的降尺度的方法，通过利用ERA5数据集进行风速超分辨率任务。 |
| [^82] | [Training and Comparison of nnU-Net and DeepMedic Methods for Autosegmentation of Pediatric Brain Tumors](https://arxiv.org/abs/2401.08404) | 本研究对比了nnU-Net和DeepMedic两种基于深度学习的3D分割模型，在儿童脑肿瘤自动分割中的性能。通过使用儿童特定的多机构脑肿瘤数据进行训练，得到了较高的分割准确性和敏感性。 |
| [^83] | [Efficient Learning of Long-Range and Equivariant Quantum Systems](https://arxiv.org/abs/2312.17019) | 本文研究了学习量子系统基态的高效方法，特别是在存在长程和等变特性的情况下。我们扩展了现有结果，使其适用于分子和原子系统中的长程相互作用，并提供了具有指数级复杂度的误差依赖性。 |
| [^84] | [Computational Tradeoffs of Optimization-Based Bound Tightening in ReLU Networks](https://arxiv.org/abs/2312.16699) | 本研究探讨了在ReLU网络中使用基于优化的界限收紧的计算权衡，提供了根据网络结构、正则化和舍入的实施指南。 |
| [^85] | [Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes](https://arxiv.org/abs/2312.06353) | 本论文提出了一种名为FedKSeed的方法，使用零阶优化和有限的随机种子集合，实现了通信成本较低的联邦全参数调整。该方法使得在终端设备上可以进行亿级语言模型的联邦全参数调整，具有较高的性能表现。 |
| [^86] | [Injecting linguistic knowledge into BERT for Dialogue State Tracking](https://arxiv.org/abs/2311.15623) | 本文提出了一种方法，在对话状态跟踪任务中，通过无监督的知识提取方法将语言知识注入到BERT中，以提高性能和可解释性。这种方法无需额外的训练数据，通过简单的神经模块实现。该方法使用的特征提取工具与对话的句法和语义模式相关，有助于理解DST模型的决策过程。 |
| [^87] | [ECNR: Efficient Compressive Neural Representation of Time-Varying Volumetric Datasets](https://arxiv.org/abs/2311.12831) | ECNR是一种针对时变数据的高效压缩神经表示方法，通过使用多尺度结构和多个小型MLP，以及深度压缩策略，可以显著加速训练和推理过程。 |
| [^88] | [Privacy Risks Analysis and Mitigation in Federated Learning for Medical Images](https://arxiv.org/abs/2311.06643) | 本研究提出了一种用于医学数据隐私风险分析和FL中缓解策略的全面框架（MedPFL）。研究发现，使用FL处理医学图像存在重大隐私风险，攻击者可以准确重构私密医学图像。为了缓解隐私攻击，研究提出了防御方法，并评估了其效果。 |
| [^89] | [A Specialized Semismooth Newton Method for Kernel-Based Optimal Transport](https://arxiv.org/abs/2310.14087) | 提出了一种专门的半光滑牛顿方法，用于解决基于核的最优输运问题，以提高计算效率并可扩展到大样本量。 |
| [^90] | [A RelEntLess Benchmark for Modelling Graded Relations between Named Entities](https://arxiv.org/abs/2305.15002) | 本文提出了一个用于模拟命名实体之间分级关系的无情基准，使用大型语言模型进行填补，以对实体对根据其满足程度进行排序。通过评估最先进的关系嵌入策略和多个LLM，我们发现了重要的创新和贡献。 |
| [^91] | [Domain-Generalizable Multiple-Domain Clustering](https://arxiv.org/abs/2301.13530) | 本研究解决了无监督领域通用化问题，并提出了一个两阶段的训练框架，该框架使用自助预训练和伪标签的多头聚类预测来提高准确性。 |
| [^92] | [On the Generalizability of ECG-based Stress Detection Models](https://arxiv.org/abs/2210.06225) | 本文研究了基于心电图(ECG)的压力检测模型的泛化能力，探讨了深度学习模型和基于心电图特征的模型在不同压力场景下的应用程度。 |
| [^93] | [An Empathetic AI Coach for Self-Attachment Therapy](https://arxiv.org/abs/2209.08316) | 本文介绍了一个用于自述疗法的共情人工智能辅导系统，通过深度学习和规则进行情绪识别和生成流畅、共情的对话，达到更高的用户参与度和实用性。通过非临床试验验证了框架的有效性，并提供改进设计和性能的指导方针。 |
| [^94] | [Variational Transfer Learning using Cross-Domain Latent Modulation](https://arxiv.org/abs/2205.15523) | 本研究提出了一种变分自编码器框架中的跨领域潜在调制机制，通过从一领域获取深层表示并影响另一领域的潜在变量，实现了有效的转移学习。在多个转移学习基准任务中，我们的模型展示了竞争性能。 |
| [^95] | [Learning to Predict Gradients for Semi-Supervised Continual Learning](https://arxiv.org/abs/2201.09196) | 本论文提出了一种新的半监督连续学习方法，通过学习预测未标记数据上的梯度，使其能够适应有监督连续学习方法，从而解决了学习和连续学习过程中的灾难性遗忘问题。 |
| [^96] | [Efficiently Solving High-Order and Nonlinear ODEs with Rational Fraction Polynomial: the Ratio Net](https://arxiv.org/abs/2105.11309) | 这项研究引入了一种新的神经网络结构——分数神经网络，用于解决高阶和非线性常微分方程。实证试验表明，该方法相比于现有方法，具有更高的效率。 |
| [^97] | [Exploration of Interpretability Techniques for Deep COVID-19 Classification using Chest X-ray Images](https://arxiv.org/abs/2006.02570) | 本文探讨了使用胸部X光图像进行COVID-19分类的可解释性技术，使用五种不同的深度学习模型进行分类，同时研究了这些模型的可解释性方法。 |
| [^98] | [Causal Machine Learning for Cost-Effective Allocation of Development Aid.](http://arxiv.org/abs/2401.16986) | 本文提出了一个因果机器学习框架，用于预测援助分配的异质化治疗效果，以支持有效的援助分配决策。 |
| [^99] | [Widely Linear Matched Filter: A Lynchpin towards the Interpretability of Complex-valued CNNs.](http://arxiv.org/abs/2401.16729) | 本研究提出了广义线性匹配滤波器（WLMF）范例来实现复杂值CNN的可解释性，解决了在复杂值数据中匹配滤波的难题，并分析了其性能。与标准线性对应物（SLMF）相比，WLMF在输出信噪比方面具有理论上的优势。 |
| [^100] | [Rademacher Complexity of Neural ODEs via Chen-Fliess Series.](http://arxiv.org/abs/2401.16655) | 本文通过Chen-Fliess序列展开将连续深度神经ODE模型转化为单层、无限宽度的网络，并利用此框架推导出了将初始条件映射到某个终端时间的ODE模型的Rademacher复杂度的紧凑表达式。 |
| [^101] | [High-Quality Image Restoration Following Human Instructions.](http://arxiv.org/abs/2401.16468) | 本论文提出了一种使用人类编写的指令来指导图像恢复模型的方法，并在多个恢复任务上取得了最先进的结果，为基于文本指导的图像恢复和增强研究提供了一个新的基准。 |
| [^102] | [Integral Operator Approaches for Scattered Data Fitting on Spheres.](http://arxiv.org/abs/2401.15294) | 本文提出了一种积分算子方法来解决球面上的散点数据拟合问题，通过研究加权谱滤波算法的逼近性能，成功推导出了带权重谱滤波算法的最优误差估计。这种方法可以避免一些现有方法中存在的问题，同时提供了一种优化算法的解决方案。 |
| [^103] | [Adaptive Block sparse regularization under arbitrary linear transform.](http://arxiv.org/abs/2401.15292) | 我们提出了一种在任意线性变换下重构具有块稀疏性的信号的方法，相比现有方法扩大了应用范围，并通过数值实验证明了其有效性。 |
| [^104] | [PrivStream: An Algorithm for Streaming Differentially Private Data.](http://arxiv.org/abs/2401.14577) | PrivStream是一种用于流式差分隐私数据的算法，可以解决离线应用中的隐私保护和数据效用问题。算法可以针对空间数据集进行合成流数据生成，并提供了通用的在线选择性计数框架，验证了算法的实用性。 |
| [^105] | [Improving Antibody Humanness Prediction using Patent Data.](http://arxiv.org/abs/2401.14442) | 本研究利用专利数据提高了抗体人性预测的能力，通过多阶段、多损失的训练过程以及弱监督对比学习的方法，成功地预测了抗体序列的人性评分。 |
| [^106] | [Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models.](http://arxiv.org/abs/2401.14440) | 这份论文研究发现，最先进的NLI模型对微小的语义保持表面形式变化非常敏感，导致推断结果不一致。其行为与对组合语义的有效理解不同，这对当前NLI模型的可靠性提出了挑战。 |
| [^107] | [Generative Design of Crystal Structures by Point Cloud Representations and Diffusion Model.](http://arxiv.org/abs/2401.13192) | 本研究提出了一种基于点云和扩散模型的晶体结构生成设计框架，并通过重建输入结构和生成全新材料的实验证明了其有效性和潜力。 |
| [^108] | [Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities.](http://arxiv.org/abs/2401.11143) | 该论文提出了一个名为GAAM的多头高斯自适应注意力机制，用于增强跨多个模态的信息聚合。通过将可学习的均值和方差纳入注意力机制中，GAAM能够动态地重新调整特征的重要性，从而在处理非平稳数据时取得了显著的性能提升，超过了目前现有的注意力技术。该方法的适应性强且参数数量较少，具有改进现有注意力框架的潜力。 |
| [^109] | [Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes.](http://arxiv.org/abs/2401.04286) | 本文扩展了之前的结果，证明了基于宽而深的ReLU神经网络和逻辑损失训练的分类规则具有普适一致性，并给出了一类概率测度条件下基于神经网络的分类器实现极小极限收敛速率的充分条件。 |
| [^110] | [HAAQI-Net: A non-intrusive neural music quality assessment model for hearing aids.](http://arxiv.org/abs/2401.01145) | HAAQI-Net是一种适用于助听器用户的非侵入性神经音质评估模型，通过使用BLSTM和注意力机制，以及预训练的BEATs进行声学特征提取，能够快速且准确地预测音乐的HAAQI得分，相比传统方法具有更高的性能和更低的推理时间。 |
| [^111] | [Fast Cell Library Characterization for Design Technology Co-Optimization Based on Graph Neural Networks.](http://arxiv.org/abs/2312.12784) | 提出了一种基于图神经网络的快速准确芯片库特征化的机器学习模型，通过结合芯片结构，在各种工艺参数下预测精度高，并且相较于传统方法具有100倍的加速。 |
| [^112] | [Vanishing Gradients in Reinforcement Finetuning of Language Models.](http://arxiv.org/abs/2310.20703) | 本研究发现在强化微调（RFT）中存在梯度消失的问题，当模型下奖励的标准差较小时，输入的期望梯度会消失，导致奖励最大化缓慢。初始监督微调（SFT）阶段是克服这个问题的最有希望的方法。 |
| [^113] | [Over-the-air Federated Policy Gradient.](http://arxiv.org/abs/2310.16592) | 本文提出了一种过空中联合策略梯度算法，通过无线信道广播携带本地信息的模拟信号实现更新策略参数，研究了噪声和信道失真对算法收敛性的影响，并通过仿真结果验证了算法的有效性。 |
| [^114] | [Fundamental Limits of Membership Inference Attacks on Machine Learning Models.](http://arxiv.org/abs/2310.13786) | 本文探讨了机器学习模型上成员推断攻击的基本限制，包括推导了效果和成功率的统计量，并提供了几种情况下的界限。这使得我们能够根据样本数量和其他结构参数推断潜在攻击的准确性。 |
| [^115] | [Graph Neural Networks with polynomial activations have limited expressivity.](http://arxiv.org/abs/2310.13139) | 本文证明了具有多项式激活函数的图神经网络无法表达GC2查询，与常用的非多项式激活函数存在分离，这回答了一个开放问题。 |
| [^116] | [PPG to ECG Signal Translation for Continuous Atrial Fibrillation Detection via Attention-based Deep State-Space Modeling.](http://arxiv.org/abs/2309.15375) | 通过基于注意力的深度状态空间建模，我们提出了一种不受个体限制的方法，将PPG信号转换为ECG，用于连续性心房颤动检测。 |
| [^117] | [Associative Transformer Is A Sparse Representation Learner.](http://arxiv.org/abs/2309.12862) | 关联变换器（AiT）是一种采用低秩显式记忆和关联记忆的稀疏表示学习器，通过联合端到端训练实现模块特化和注意力瓶颈的形成。 |
| [^118] | [A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data.](http://arxiv.org/abs/2308.13352) | 这篇论文介绍了一个完全无监督的机器学习框架，用于处理在训练数据中含有异常样本的异常检测任务。 |
| [^119] | [Try with Simpler -- An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection.](http://arxiv.org/abs/2308.12612) | 本论文通过改进传统的主成分分析方法，优化了基于日志的异常检测技术，以提高其效果，从而使其与深度学习方法相媲美。 |
| [^120] | [Zero-delay Consistent Signal Reconstruction from Streamed Multivariate Time Series.](http://arxiv.org/abs/2308.12459) | 本文介绍了一种从流式多元时间序列中一致地重建信号的方法，同时减少了零延迟信号重建的粗糙度。 (arXiv:2308.12459v1 [eess.SP]) |
| [^121] | [ConcatPlexer: Additional Dim1 Batching for Faster ViTs.](http://arxiv.org/abs/2308.11199) | 本文提出了一种名为ConcatPlexer的方法，通过在视觉识别中使用附加的Dim1批处理（即连接）来提高吞吐量，同时准确性受到的影响较小。 |
| [^122] | [Intrinsic Motivation via Surprise Memory.](http://arxiv.org/abs/2308.04836) | 该论文提出了一种新的计算模型，通过意外记忆作为内在奖励的基础，在强化学习中解决了现有方法的局限性。实验结果表明，通过结合意外预测器的意外记忆在稀疏奖励环境中表现出高效的探索行为，并显著提升了最终性能。 |
| [^123] | [RCT Rejection Sampling for Causal Estimation Evaluation.](http://arxiv.org/abs/2307.15176) | 该论文提出了一种名为RCT拒绝抽样的新抽样算法，用于因果估计评估。该方法通过子抽样随机控制试验(RCT)创建混淆的观测数据集，并使用RCT的平均因果效应作为基准真实值，以进行有效比较。 |
| [^124] | [Variational Autoencoding of Dental Point Clouds.](http://arxiv.org/abs/2307.10895) | 本论文介绍了一种新颖的点云变分自编码器（VF-Net）用于牙科点云数据的处理，该模型在各种任务中具有显著的性能，包括网格生成、形状完整和表示学习。 |
| [^125] | [Deep Network Approximation: Beyond ReLU to Diverse Activation Functions.](http://arxiv.org/abs/2307.06555) | 本文研究了深度神经网络在多种激活函数下的表达能力，证明了可以通过在有界集合上构建一个宽度为6N、深度为2L的varrho激活网络来逼近一个宽度为N、深度为L的ReLU网络，从而将对ReLU网络的逼近结果推广到其他激活函数。 |
| [^126] | [Predicting small molecules solubilities on endpoint devices using deep ensemble neural networks.](http://arxiv.org/abs/2307.05318) | 这项工作提出了一种使用深度集成神经网络在端点设备上预测小分子溶解度的方法，通过静态网站运行，同时具备预测不确定性，并实现了令人满意的结果。 |
| [^127] | [What do self-supervised speech models know about words?.](http://arxiv.org/abs/2307.00162) | 通过对自我监督的语音模型进行分析，发现这些模型在不同层中编码了不同的语言信息，也学习了类似音素的子词单元。与单词相关的信息主要在中间的模型层中，同时一些低级信息在更高的层中也得以保留。 |
| [^128] | [Uncertainty Quantification via Spatial-Temporal Tweedie Model for Zero-inflated and Long-tail Travel Demand Prediction.](http://arxiv.org/abs/2306.09882) | 本文提出了一种新型的时空Tweedie模型STTD，旨在解决高分辨率OD矩阵中稀疏和长尾特征的问题，并成功量化预测不确定性，具有很高的应用前景。 |
| [^129] | [Some Primal-Dual Theory for Subgradient Methods for Strongly Convex Optimization.](http://arxiv.org/abs/2305.17323) | 本文提出了一种强凸优化的次梯度法原始对偶理论，可以实现简单的、最佳的停止准则和优化证明，同时可以适用于各种步长的选择和非Lipschitz病态问题，保证了这些方法次线性收敛速度。 |
| [^130] | [Make Transformer Great Again for Time Series Forecasting: Channel Aligned Robust Dual Transformer.](http://arxiv.org/abs/2305.12095) | 本文提出了一种通道对齐鲁棒双Transformer模型，通过双Transformer结构和鲁棒损失函数的引入，解决了Transformer在时间序列预测中的关键缺点，显著提高了预测精度和效率。 |
| [^131] | [Utilizing Reinforcement Learning for de novo Drug Design.](http://arxiv.org/abs/2303.17615) | 本文开发了一个统一的框架，利用强化学习生成预测具有活性的新药分子。在需要结构多样性的情况下，同时使用高分和低分子来更新策略是有利的。使用所有生成的分子可以提高性能稳定性，而off-policy算法有潜力提高生成分子的结构多样性。 |
| [^132] | [Bayesian Self-Supervised Contrastive Learning.](http://arxiv.org/abs/2301.11673) | 本文提出了一种新的自监督对比损失——BCL损失，通过重要性权重修正导致的偏差，设计所需的采样分布来采样难以得到的真实负样本，修正伪负样本，采矿难负样本以提高编码器训练的准确性。 |
| [^133] | [StructCoder: Structure-Aware Transformer for Code Generation.](http://arxiv.org/abs/2206.05239) | 本文提出了一个结构感知的Transformer模型，通过引入AST和DFG辅助任务，旨在解决现有代码生成模型在面对代码语法和语义时的训练不足问题。 |
| [^134] | [What Is Fairness? Philosophical Considerations and Implications For FairML.](http://arxiv.org/abs/2205.09622) | 本文探讨了公平性的哲学概念，提出了公平性和预测性能不是不可调和的对立面，并强调从数据收集到最终模型评估都需纳入伦理考虑。 |
| [^135] | [Hyperspectral Pixel Unmixing with Latent Dirichlet Variational Autoencoder.](http://arxiv.org/abs/2203.01327) | 本研究提出了一种使用潜在狄利克雷变分自编码器进行高光谱像素解混的方法，可以通过迁移学习范式训练模型并在真实数据上进行像素解混，取得了最新的最好结果。 |
| [^136] | [An adaptation of InfoMap to absorbing random walks using absorption-scaled graphs.](http://arxiv.org/abs/2112.10953) | 我们使用吸收比例缩放图和马尔可夫时间扫描改进了InfoMap算法，检测网络上密集连接的节点社区，此方法适应节点具有不同移除率的情况，社区结构与不考虑节点吸收率的方法可能有显著不同，并对易感-感染-恢复（SI）模型产生重要影响。 |

# 详细

[^1]: KVQuant: 以KV缓存量化实现1000万上下文长度LLM推理

    KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization

    [https://arxiv.org/abs/2401.18079](https://arxiv.org/abs/2401.18079)

    KVQuant是一种解决LLM推理中大量内存消耗的KV缓存量化方法，通过引入新颖的量化方法，包括分通道键量化、RoPE前量化键和非均匀KV缓存量化，准确表示超低精度的KV激活。

    

    LLM在文档分析和摘要等需要大窗口上下文的应用中越来越受到关注，在推理过程中，KV缓存激活成为记忆消耗的主要贡献者。量化是一种压缩KV缓存激活的有效方法，然而现有的解决方案无法准确表示超低精度（如低于4位）的激活。本文提出了KVQuant，通过引入新颖的方法量化缓存的KV激活来解决这个问题，包括：(i)分通道键量化，在量化键激活时调整维度以更好地匹配分布；(ii)RoPE前量化键，在旋转位置嵌入之前量化键激活以减轻其对量化的影响；(iii)非均匀KV缓存量化，在每层推导出权重感知的非均匀数据类型，以更好地表示不同层的敏感性。

    LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better 
    
[^2]: 语言模型在解决问题时是否表现出与人类学习者相同的认知偏见？

    Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?

    [https://arxiv.org/abs/2401.18070](https://arxiv.org/abs/2401.18070)

    该研究调查了语言模型在解决算术问题时与人类学习者的认知偏见。研究发现，当前最先进的语言模型在文本理解和解决方案规划阶段表现出与人类类似的偏见。

    

    越来越多的人对使用大型语言模型（LLMs）作为认知模型感兴趣。为了达到这个目的，了解LLMs能够模拟哪些认知特性以及哪些不能模拟是至关重要的。在这项研究中，我们研究了LLMs在解决算术问题时与儿童已知认知偏见的相关性。通过调查学习科学文献，我们提出问题解决过程可以分为三个明确的步骤：文本理解、解决方案规划和解决方案执行。我们为每个步骤构建了测试，以了解当前最先进的LLMs可以如何忠实地模拟这个过程的哪些部分。我们使用一种神经符号方法为每个测试生成了一组新的单词问题，该方法可以对问题特征进行精细控制。我们发现，LLMs在文本理解和解决方案规划两个解决过程的步骤中，不论是否经过指导调整，都表现出与人类类似的偏见。

    There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which cognitive properties are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand which parts of this process can be faithfully modeled by current state-of-the-art LLMs. We generate a novel set of word problems for each of these tests, using a neuro-symbolic method that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but 
    
[^3]: RAPTOR: 递归抽象处理用于树状检索

    RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval

    [https://arxiv.org/abs/2401.18059](https://arxiv.org/abs/2401.18059)

    本论文提出了一种递归嵌入、聚类和摘要的方法，通过构建不同摘要级别的树，从下往上整合并检索长度较长的文档，对传统检索增强的语言模型进行改进，实现了在复杂的多步推理问答任务上的最先进的结果提升。

    

    检索增强的语言模型可以更好地适应世界状态的变化，并结合长尾知识。然而，大多数现有的方法仅从检索语料库中检索短连续块，限制了对整体文档上下文的整体理解。我们引入了一种新颖的方法，即递归嵌入、聚类和摘要文本块，从下往上构建具有不同摘要级别的树。在推理时，我们的RAPTOR模型从这棵树中检索，将不同抽象级别的信息整合到长度较长的文档中。控制实验表明，使用递归摘要的检索在几个任务上比传统检索增强的语言模型有显著改进。在涉及复杂的多步推理的问答任务上，我们展示了最先进的结果；例如，通过将RAPTOR检索与GPT-4的使用相结合，我们可以将QuALITY基准测试的最佳性能提高20％。

    Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20%
    
[^4]: LongAlign：大型语言模型的长上下文对齐方法

    LongAlign: A Recipe for Long Context Alignment of Large Language Models

    [https://arxiv.org/abs/2401.18058](https://arxiv.org/abs/2401.18058)

    LongAlign是一种用于大型语言模型的长上下文对齐的方法，通过指导微调和使用打包、排序和损失加权策略，它在长上下文任务中表现优异，相比现有的方法提高了多达30\%的性能。

    

    将大型语言模型扩展到有效处理长上下文的能力需要对相似长度的输入序列进行指导微调。为了解决这个问题，我们提出了LongAlign - 一种用于长上下文对齐的指导数据、训练和评估方法。首先，我们使用自我指导方法构建长指令跟随数据集。为了确保数据多样性，它涵盖了来自不同长上下文来源的广泛任务。其次，我们采用打包和排序批处理策略，加速在具有不同长度分布的数据上的受监督微调。此外，我们还开发了一种损失加权方法，在打包训练过程中平衡损失对不同序列的贡献。第三，我们引入了LongBench-Chat基准，用于评估10k-100k长度的查询的指令跟随能力。实验结果表明，LongAlign在长上下文任务中比现有的LLMs方法的性能提高了多达30\%，同时也保持了其熟练性。

    Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length. To address this, we present LongAlign -- a recipe of the instruction data, training, and evaluation for long context alignment. First, we construct a long instruction-following dataset using Self-Instruct. To ensure the data diversity, it covers a broad range of tasks from various long context sources. Second, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions. Additionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training. Third, we introduce the LongBench-Chat benchmark for evaluating instruction-following capabilities on queries of 10k-100k in length. Experiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30\%, while also maintaining their proficienc
    
[^5]: 排名监督对比学习用于时间序列分类

    Rank Supervised Contrastive Learning for Time Series Classification

    [https://arxiv.org/abs/2401.18057](https://arxiv.org/abs/2401.18057)

    Rank Supervised Contrastive Learning (RankSCL) proposes a targeted data augmentation method and a novel rank loss to improve time series classification by utilizing fine-grained relative similarity information.

    

    最近，各种对比学习技术已经被开发用于时间序列数据的分类，并展现出很好的性能。一种常见的范式是利用适当的数据增强和构建可行的正样本，使得编码器能够在特征空间中将相似的数据点映射得更近，将不相似的数据点映射得更远，从而产生稳健而有区分性的表示。尽管该方法非常有效，在标记样本有限的情况下，正样本的细粒度相对相似性信息（例如排名）往往被忽略。为此，我们提出了一种称为Rank Supervised Contrastive Learning（RankSCL）的方法来进行时间序列分类。与传统的对比学习框架不同，RankSCL在嵌入空间中以一种有针对性的方式进行数据增强，并采用特定的过滤规则来选择更具信息量的正样本和负样本对。此外，还开发了一种新的排名损失函数，来为不同的样本赋予不同的权重。

    Recently, various contrastive learning techniques have been developed to categorize time series data and exhibit promising performance. A general paradigm is to utilize appropriate augmentations and construct feasible positive samples such that the encoder can yield robust and discriminative representations by mapping similar data points closer together in the feature space while pushing dissimilar data points farther apart. Despite its efficacy, the fine-grained relative similarity (e.g., rank) information of positive samples is largely ignored, especially when labeled samples are limited. To this end, we present Rank Supervised Contrastive Learning (RankSCL) to perform time series classification. Different from conventional contrastive learning frameworks, RankSCL augments raw data in a targeted way in the embedding space and adopts certain filtering rules to select more informative positive and negative pairs of samples. Moreover, a novel rank loss is developed to assign different w
    
[^6]: 为基于骨骼的动作识别的连续图学习评估敏感度

    Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based Action Recognition

    [https://arxiv.org/abs/2401.18054](https://arxiv.org/abs/2401.18054)

    本研究关注连续图学习中的GNN，并提出了基于骨骼动作识别的连续图学习基准。我们研究了CGL方法在类别和任务顺序上的敏感性以及在不同架构下的敏感性。

    

    连续学习（CL）是一个旨在构建能够在不从头开始重新训练的情况下，连续地积累不同任务知识的机器学习模型的研究领域。先前的研究表明，预训练图神经网络（GNN）在微调后可能会导致负迁移（Hu等，2020），这与CL密切相关。因此，我们关注研究CGL设置下的GNN。我们提出了第一个空时图连续学习基准，并在这种新颖的设置中使用它来评估知名的CGL方法。该基准基于N-UCLA和NTU-RGB+D数据集，用于骨骼动作识别。除了对标准性能指标进行基准测试外，我们还研究了CGL方法的类别和任务顺序敏感性，即学习顺序对每个类/任务性能的影响，以及在不同宽度和深度下使用骨干GNN的CGL方法的架构敏感性。我们揭示了任务

    Continual learning (CL) is the research field that aims to build machine learning models that can accumulate knowledge continuously over different tasks without retraining from scratch. Previous studies have shown that pre-training graph neural networks (GNN) may lead to negative transfer (Hu et al., 2020) after fine-tuning, a setting which is closely related to CL. Thus, we focus on studying GNN in the continual graph learning (CGL) setting. We propose the first continual graph learning benchmark for spatio-temporal graphs and use it to benchmark well-known CGL methods in this novel setting. The benchmark is based on the N-UCLA and NTU-RGB+D datasets for skeleton-based action recognition. Beyond benchmarking for standard performance metrics, we study the class and task-order sensitivity of CGL methods, i.e., the impact of learning order on each class/task's performance, and the architectural sensitivity of CGL methods with backbone GNN at various widths and depths. We reveal that task
    
[^7]: 使用时变SIRD，粒子群优化和深度学习的流行病建模

    Epidemic Modeling using Hybrid of Time-varying SIRD, Particle Swarm Optimization, and Deep Learning

    [https://arxiv.org/abs/2401.18047](https://arxiv.org/abs/2401.18047)

    该论文开发了一个集流行病建模、粒子群优化和深度学习于一体的混合模型，用于处理非稳态模式和流行病的多个波动。模型主要满足三个目标：周期性估计模型参数、考虑所有方面的影响以提高预测准确性、利用深度学习预测模型参数。

    

    如果传播模式是稳定的话，流行病学模型是最适合用于建模流行病的。为了处理非稳态模式和流行病的多个波动，我们开发了一个包括流行病建模、粒子群优化和深度学习的混合模型。该模型主要满足三个目标以实现更好的预测：1. 周期性估计模型参数。2. 利用数据拟合和参数优化来考虑所有方面的影响。3. 基于深度学习的模型参数预测。在我们的模型中，我们使用一组常微分方程（ODEs）来进行易感-感染-恢复-死亡（SIRD）流行病建模，使用粒子群优化（PSO）进行模型参数优化，使用堆叠LSTM预测模型参数。单次或首次估计模型参数无法建模流行病的多个波动。因此，我们定期（每周）估计模型参数。我们使用PSO来确定最佳的参数值。

    Epidemiological models are best suitable to model an epidemic if the spread pattern is stationary. To deal with non-stationary patterns and multiple waves of an epidemic, we develop a hybrid model encompassing epidemic modeling, particle swarm optimization, and deep learning. The model mainly caters to three objectives for better prediction: 1. Periodic estimation of the model parameters. 2. Incorporating impact of all the aspects using data fitting and parameter optimization 3. Deep learning based prediction of the model parameters. In our model, we use a system of ordinary differential equations (ODEs) for Susceptible-Infected-Recovered-Dead (SIRD) epidemic modeling, Particle Swarm Optimization (PSO) for model parameter optimization, and stacked-LSTM for forecasting the model parameters. Initial or one time estimation of model parameters is not able to model multiple waves of an epidemic. So, we estimate the model parameters periodically (weekly). We use PSO to identify the optimum v
    
[^8]: Na\"ive Bayes分类的变量选择

    Variable selection for Na\"ive Bayes classification

    [https://arxiv.org/abs/2401.18039](https://arxiv.org/abs/2401.18039)

    该论文提出了一种稀疏版本的Na\"ive Bayes分类器，通过考虑特征间的相关结构实现了稀疏性，并支持不同的性能度量来指导特征选择。

    

    在多变量分析中，经典的Na\"ive Bayes分类方法已被证明是一种易于处理和高效的方法。然而，特征通常是相关的，这违反了Na\"ive Bayes条件独立性的假设，可能会损害该方法的性能。此外，数据集通常具有大量的特征，这可能会使结果的解释变得复杂，并减慢该方法的执行。在本文中，我们提出了一种稀疏版本的Na\"ive Bayes分类器，它具有三个特点。首先，通过考虑协变量的相关结构实现了稀疏性。其次，可以使用不同的性能度量来指导特征的选择。第三，可以包括对更感兴趣的组别的性能约束。我们的提案可以实现智能搜索，具有竞争力的运行时间，同时在分类的性能度量方面具有灵活性。

    The Na\"ive Bayes has proven to be a tractable and efficient method for classification in multivariate analysis. However, features are usually correlated, a fact that violates the Na\"ive Bayes' assumption of conditional independence, and may deteriorate the method's performance. Moreover, datasets are often characterized by a large number of features, which may complicate the interpretation of the results as well as slow down the method's execution.   In this paper we propose a sparse version of the Na\"ive Bayes classifier that is characterized by three properties. First, the sparsity is achieved taking into account the correlation structure of the covariates. Second, different performance measures can be used to guide the selection of features. Third, performance constraints on groups of higher interest can be included. Our proposal leads to a smart search, which yields competitive running times, whereas the flexibility in terms of performance measure for classification is integrate
    
[^9]: 优化对比学习以检测髓样皮层折叠模式

    Optimizing contrastive learning for cortical folding pattern detection

    [https://arxiv.org/abs/2401.18035](https://arxiv.org/abs/2401.18035)

    本研究使用对比学习方法优化了深度学习模型，能够检测髓样皮层的折叠模式，为解决皮层折叠的变异性和与个体行为特征以及病理学的关系问题提供了一种新的方法。

    

    人类大脑的皮层有许多凸起和沟槽，称为回状脑回和沟状脑沟。虽然主要皮层褶皱存在着高度个体间的一致性，但当我们检查具体的形状和折叠模式的细节时情况并非如此。由于这种复杂性，描述皮层折叠的变异性并将其与受试者的行为特征或病理学联系起来仍然是一个未解决的科学问题。传统的方法包括基于几何距离标记一些特定的模式，要么手动标记，要么半自动标记，但是现代深度学习技术与数以万计的受试者的MRI图像数据集的最近可用性使其特别具有吸引力。在这里，我们建立了一个自监督深度学习模型来检测扣带区域的折叠模式。我们使用基于拓扑的增强训练了一个对比自监督模型（SimCLR），使用了人体连通计划（1101个受试者）和UKBioBank（21070个受试者）数据集。

    The human cerebral cortex has many bumps and grooves called gyri and sulci. Even though there is a high inter-individual consistency for the main cortical folds, this is not the case when we examine the exact shapes and details of the folding patterns. Because of this complexity, characterizing the cortical folding variability and relating them to subjects' behavioral characteristics or pathologies is still an open scientific problem. Classical approaches include labeling a few specific patterns, either manually or semi-automatically, based on geometric distances, but the recent availability of MRI image datasets of tens of thousands of subjects makes modern deep-learning techniques particularly attractive. Here, we build a self-supervised deep-learning model to detect folding patterns in the cingulate region. We train a contrastive self-supervised model (SimCLR) on both Human Connectome Project (1101 subjects) and UKBioBank (21070 subjects) datasets with topological-based augmentation
    
[^10]: 通过定向表示优化实现的安全提示驱动的大型语言模型(LLM)保护

    Prompt-Driven LLM Safeguarding via Directed Representation Optimization

    [https://arxiv.org/abs/2401.18018](https://arxiv.org/abs/2401.18018)

    通过研究模型表示的影响，我们发现安全提示并没有明显增强恶意和无害查询之间的区分，并提出了一种名为DRO的方法，用于自动优化安全提示。

    

    在大型语言模型(LLM)中，使用安全提示在模型输入之前是一种常见的保护实践，以使其不遵从包含恶意意图的查询。然而，安全提示的工作机制尚未完全理解，这妨碍了自动优化其以改善LLM安全性的潜力。针对这个问题，我们从模型表示的角度调查了安全提示的影响。我们发现在模型的表示空间中，有害和无害的查询可以在很大程度上区分开来，但安全提示并没有明显增强这一区分。相反，不同安全提示导致查询的表示朝着相似的方向移动，使得模型即使在查询无害时也更容易拒绝提供协助。受到这些发现的启发，我们提出了一种名为DRO（定向表示优化）的方法，用于自动安全提示优化。DRO将安全提示视为要优化的表示方向。

    Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
    
[^11]: 通过异构变换的核偏差测量来进行因果关系发现

    Causal Discovery by Kernel Deviance Measures with Heterogeneous Transforms

    [https://arxiv.org/abs/2401.18017](https://arxiv.org/abs/2401.18017)

    本文提出了一种通过异构变换的核偏差测量来解决因果关系发现中的挑战，以捕捉因果关系和效应之间的高阶结构变异的主要标记。

    

    在一组随机变量中发现因果关系是科学的基本目标，并且最近也被认为是实现真正机器智能的必要组成部分。其中一类因果关系发现技术基于这样一种论点：因果方向和反因果方向之间存在固有的结构不对称性，可以利用这种不对称性来确定因果关系的方向。然而，如何捕捉因果关系和效应之间的差异仍然是一个挑战，许多当前最先进的算法提出了通过比较条件分布的核均值嵌入的范数来解决这个问题。本文认为，基于RKHS嵌入的这种方法不足以捕捉涉及条件分布的高阶结构变异的主要标记的因果效应不对称性。因此，我们提出了一种引入了异构变换的核内在不变性测量（KIIM-HT）方法。

    The discovery of causal relationships in a set of random variables is a fundamental objective of science and has also recently been argued as being an essential component towards real machine intelligence. One class of causal discovery techniques are founded based on the argument that there are inherent structural asymmetries between the causal and anti-causal direction which could be leveraged in determining the direction of causation. To go about capturing these discrepancies between cause and effect remains to be a challenge and many current state-of-the-art algorithms propose to compare the norms of the kernel mean embeddings of the conditional distributions. In this work, we argue that such approaches based on RKHS embeddings are insufficient in capturing principal markers of cause-effect asymmetry involving higher-order structural variabilities of the conditional distributions. We propose Kernel Intrinsic Invariance Measure with Heterogeneous Transform (KIIM-HT) which introduces 
    
[^12]: 因果协同并发强化学习

    Causal Coordinated Concurrent Reinforcement Learning

    [https://arxiv.org/abs/2401.18012](https://arxiv.org/abs/2401.18012)

    这项工作提出了一种用于并发强化学习的新算法框架，通过数据共享和协同探索来学习更高效和表现更好的策略。算法中利用因果推断算法提取控制个体差异的模型参数，并提出了一种基于相似性度量的数据共享方案，展示了更快的学习速度和多样化动作选择的有效性。

    

    在这项工作中，我们提出了一种新颖的算法框架，用于数据共享和协同探索，以在并发强化学习（CRL）环境下学习更高效和表现更好的策略。与其他假设所有代理都在相同环境下行动的工作相比，我们放宽了这一限制，而是考虑每个代理在共享全局结构但也存在个体差异的环境中行动的情况。我们的算法利用了一个因果推断算法，即加性噪声模型 - 混合模型（ANM-MM），通过独立性强化提取控制个体差异的模型参数。我们提出了一种基于提取的模型参数相似性度量的数据共享方案，并在一组自回归、摆杆和倒立摆任务上展示了更快的学习速度，最后我们展示了多样化动作选择的有效性。

    In this work, we propose a novel algorithmic framework for data sharing and coordinated exploration for the purpose of learning more data-efficient and better performing policies under a concurrent reinforcement learning (CRL) setting. In contrast to other work which make the assumption that all agents act under identical environments, we relax this restriction and instead consider the formulation where each agent acts within an environment which shares a global structure but also exhibits individual variations. Our algorithm leverages a causal inference algorithm in the form of Additive Noise Model - Mixture Model (ANM-MM) in extracting model parameters governing individual differentials via independence enforcement. We propose a new data sharing scheme based on a similarity measure of the extracted model parameters and demonstrate superior learning speeds on a set of autoregressive, pendulum and cart-pole swing-up tasks and finally, we show the effectiveness of diverse action selecti
    
[^13]: EEG-GPT: 探索大型语言模型在EEG分类和解读中的能力

    EEG-GPT: Exploring Capabilities of Large Language Models for EEG Classification and Interpretation

    [https://arxiv.org/abs/2401.18006](https://arxiv.org/abs/2401.18006)

    EEG-GPT是一种利用大型语言模型来分类和解读EEG的方法，它能够实现多尺度电生理理解和分类，且在few-shot学习范式中表现出色。

    

    在应用于脑电图（EEG）的传统机器学习（ML）方法中，往往是有限的聚焦，仅仅孤立地关注跨越不同时间尺度（从毫秒的瞬时尖峰到持续几分钟的癫痫发作）和空间尺度（从局部高频振荡到全局睡眠活动）的特定脑活动。这种孤立的方法限制了发展出具有多尺度电生理理解和分类能力的EEG ML模型。此外，典型的ML EEG方法采用黑匣子方法，限制了其在临床环境中的可解释性和可信度。因此，我们提出了EEG-GPT，一种统一的EEG分类方法，利用大型语言模型（LLM）的进展。EEG-GPT在仅利用2％的训练数据的few-shot学习范式中，达到了与当前最先进的深度学习方法相当的优异性能，能够对正常和异常的EEG进行分类。此外，

    In conventional machine learning (ML) approaches applied to electroencephalography (EEG), this is often a limited focus, isolating specific brain activities occurring across disparate temporal scales (from transient spikes in milliseconds to seizures lasting minutes) and spatial scales (from localized high-frequency oscillations to global sleep activity). This siloed approach limits the development EEG ML models that exhibit multi-scale electrophysiological understanding and classification capabilities. Moreover, typical ML EEG approaches utilize black-box approaches, limiting their interpretability and trustworthiness in clinical contexts. Thus, we propose EEG-GPT, a unifying approach to EEG classification that leverages advances in large language models (LLM). EEG-GPT achieves excellent performance comparable to current state-of-the-art deep learning methods in classifying normal from abnormal EEG in a few-shot learning paradigm utilizing only 2% of training data. Furthermore, it off
    
[^14]: 多线性算子网络

    Multilinear Operator Networks

    [https://arxiv.org/abs/2401.17992](https://arxiv.org/abs/2401.17992)

    该论文提出了一种名为MONet的模型，该模型仅依赖多线性算子来进行图像识别，通过捕捉输入元素的高次交互，优于以前的多项式网络，并与现代架构性能相近。这一研究可以激发对完全使用多线性操作的模型的进一步研究。

    

    尽管深度神经网络在图像识别方面具有显著的能力，但对激活函数的依赖仍然是一个基本未探索的领域，并且有待消除。另一方面，多项式网络是一类不需要激活函数的模型，但其性能仍未与现代架构相媲美。在本文中，我们旨在弥合这一差距，并提出了仅依赖多线性算子的MONet模型。MONet的核心层称为Mu-Layer，捕捉了输入元素的乘法交互。MONet捕捉了输入元素的高次交互，并通过一系列图像识别和科学计算基准测试展示了我们方法的有效性。所提出的模型优于以前的多项式网络，并与现代架构性能相近。我们相信MONet可以激发对完全使用多线性操作的模型进一步的研究。

    Despite the remarkable capabilities of deep neural networks in image recognition, the dependence on activation functions remains a largely unexplored area and has yet to be eliminated. On the other hand, Polynomial Networks is a class of models that does not require activation functions, but have yet to perform on par with modern architectures. In this work, we aim close this gap and propose MONet, which relies solely on multilinear operators. The core layer of MONet, called Mu-Layer, captures multiplicative interactions of the elements of the input token. MONet captures high-degree interactions of the input elements and we demonstrate the efficacy of our approach on a series of image recognition and scientific computing benchmarks. The proposed model outperforms prior polynomial networks and performs on par with modern architectures. We believe that MONet can inspire further research on models that use entirely multilinear operations.
    
[^15]: 通过编码理论了解神经网络中的多语义性

    Understanding polysemanticity in neural networks through coding theory

    [https://arxiv.org/abs/2401.17975](https://arxiv.org/abs/2401.17975)

    通过应用编码理论和神经科学工具，本文提出了一种新颖的实践方法来理解神经网络中的多语义性，并对多语义神经元对学习性能的优势进行了解释。

    

    尽管付出了大量努力，神经网络可解释性仍然是一个难以捉摸的目标，以前的研究未能对大多数单个神经元对网络输出的影响提供简洁的解释。这个限制是由于大多数神经元的多语义性，即一个给定的神经元参与多个不相关的网络状态，使解释该神经元变得复杂。在本文中，我们应用神经科学和信息论中开发的工具，提出了一种新颖的网络可解释性实践方法，并对多语义性和编码密度提出了理论见解。我们通过检查激活的协方差矩阵的特征谱来推断网络代码的冗余水平。此外，我们展示了随机投影如何揭示网络是否具有平滑的或不可微的代码，从而解释了代码的可解释性。这个相同的框架解释了多语义神经元对学习性能的优势并解释了

    Despite substantial efforts, neural network interpretability remains an elusive goal, with previous research failing to provide succinct explanations of most single neurons' impact on the network output. This limitation is due to the polysemantic nature of most neurons, whereby a given neuron is involved in multiple unrelated network states, complicating the interpretation of that neuron. In this paper, we apply tools developed in neuroscience and information theory to propose both a novel practical approach to network interpretability and theoretical insights into polysemanticity and the density of codes. We infer levels of redundancy in the network's code by inspecting the eigenspectrum of the activation's covariance matrix. Furthermore, we show how random projections can reveal whether a network exhibits a smooth or non-differentiable code and hence how interpretable the code is. This same framework explains the advantages of polysemantic neurons to learning performance and explains
    
[^16]: MelNet:一种用于实时目标检测的深度学习算法

    MelNet: A Real-Time Deep Learning Algorithm for Object Detection

    [https://arxiv.org/abs/2401.17972](https://arxiv.org/abs/2401.17972)

    MelNet是一种用于实时目标检测的深度学习算法，经过训练后，在KITTI数据集上表现出良好的性能，同时也展示了迁移学习和定制模型在目标检测中的有效性。

    

    本研究介绍了一种名为MelNet的新型深度学习算法，用于目标检测。MelNet利用KITTI数据集进行了目标检测的训练。经过300个训练轮次，MelNet达到了0.732的mAP（平均精度）。此外，还对三个备选模型（YOLOv5、EfficientDet和Faster-RCNN-MobileNetv3）在KITTI数据集上进行了训练，并与MelNet进行了对比。结果表明，在某些情况下，采用迁移学习是有效的。值得注意的是，预先在知名数据集（如ImageNet、COCO和Pascal VOC）上训练的现有模型产生了更好的结果。另一个发现强调了根据特定情景创建新模型并在特定数据集上进行训练的可行性。这项研究表明，仅在KITTI数据集上训练的MelNet在150个轮次后也超过了EfficientDet。因此，训练后，MelNet的性能与EfficientDet接近。

    In this study, a novel deep learning algorithm for object detection, named MelNet, was introduced. MelNet underwent training utilizing the KITTI dataset for object detection. Following 300 training epochs, MelNet attained an mAP (mean average precision) score of 0.732. Additionally, three alternative models -YOLOv5, EfficientDet, and Faster-RCNN-MobileNetv3- were trained on the KITTI dataset and juxtaposed with MelNet for object detection.   The outcomes underscore the efficacy of employing transfer learning in certain instances. Notably, preexisting models trained on prominent datasets (e.g., ImageNet, COCO, and Pascal VOC) yield superior results. Another finding underscores the viability of creating a new model tailored to a specific scenario and training it on a specific dataset. This investigation demonstrates that training MelNet exclusively on the KITTI dataset also surpasses EfficientDet after 150 epochs. Consequently, post-training, MelNet's performance closely aligns with that
    
[^17]: CONCORD: 面向可配置图形代码表示的领域特定语言

    CONCORD: Towards a DSL for Configurable Graph Code Representation

    [https://arxiv.org/abs/2401.17967](https://arxiv.org/abs/2401.17967)

    CONCORD是一种面向可配置图形代码表示的领域特定语言，通过实现减少图形大小复杂性的启发式方法，能够自动生成代码表示，并在代码异味检测中展示出效果。

    

    深度学习被广泛应用于挖掘大型代码库中的隐藏模式。为了实现这一目标，构建一个能够捕捉源代码的相关特征和特性的格式至关重要。基于图形的表示因其能够模拟结构和语义信息而受到关注。然而，现有工具在跨不同编程语言构建图形方面缺乏灵活性，限制了它们的使用。此外，这些工具的输出通常缺乏互操作性，并导致图形神经网络训练速度较慢且不可扩展。我们引入了CONCORD，一个用于构建可定制图形表示的领域特定语言。它实现了减少图形大小复杂性的启发式方法。我们通过代码异味检测作为一个示例用例展示了其有效性，并显示：首先，CONCORD可以根据指定的配置自动产生代码表示；其次，我们的方法可以通过减少图形复杂性来提高图形神经网络的训练速度和可扩展性。

    Deep learning is widely used to uncover hidden patterns in large code corpora. To achieve this, constructing a format that captures the relevant characteristics and features of source code is essential. Graph-based representations have gained attention for their ability to model structural and semantic information. However, existing tools lack flexibility in constructing graphs across different programming languages, limiting their use. Additionally, the output of these tools often lacks interoperability and results in excessively large graphs, making graph-based neural networks training slower and less scalable.   We introduce CONCORD, a domain-specific language to build customizable graph representations. It implements reduction heuristics to reduce graphs' size complexity. We demonstrate its effectiveness in code smell detection as an illustrative use case and show that: first, CONCORD can produce code representations automatically per the specified configuration, and second, our he
    
[^18]: 在Wasserstein距离中的扩散模型的一般概率流ODE的收敛性分析

    Convergence Analysis for General Probability Flow ODEs of Diffusion Models in Wasserstein Distances

    [https://arxiv.org/abs/2401.17958](https://arxiv.org/abs/2401.17958)

    本文提供了在2-Wasserstein距离中的一般类概率流ODE抽样器的非渐近收敛性分析，假设得分估计准确。

    

    基于概率流常微分方程（ODE）的基于得分的生成模型在各种应用中取得了显著的成功。虽然文献中提出了各种快速的基于ODE的抽样器并在实践中使用，但对概率流ODE的收敛性属性的理论理解仍然非常有限。在本文中，我们提供了适用于2-Wasserstein距离中的一般类概率流ODE抽样器的首个非渐近收敛性分析结果，假设准确的得分估计。接下来，我们考虑了各种示例，并确定了相应基于ODE的抽样器的迭代复杂度的结果。

    Score-based generative modeling with probability flow ordinary differential equations (ODEs) has achieved remarkable success in a variety of applications. While various fast ODE-based samplers have been proposed in the literature and employed in practice, the theoretical understandings about convergence properties of the probability flow ODE are still quite limited. In this paper, we provide the first non-asymptotic convergence analysis for a general class of probability flow ODE samplers in 2-Wasserstein distance, assuming accurate score estimates. We then consider various examples and establish results on the iteration complexity of the corresponding ODE-based samplers.
    
[^19]: LOCOST: 长文档抽象摘要化的状态空间模型

    LOCOST: State-Space Models for Long Document Abstractive Summarization

    [https://arxiv.org/abs/2401.17919](https://arxiv.org/abs/2401.17919)

    LOCOST是一种基于状态空间模型的编码器-解码器架构，用于处理长文档的抽象摘要生成。与基于稀疏注意模式的最先进模型相比，LOCOST具有更低的计算复杂度，并且能够在训练和推断期间节省大量内存。在评估中，LOCOST在长文档摘要化任务上达到了93-96%的性能水平，并且能够处理超过600K个标记的输入文本。

    

    状态空间模型是编码长序列和捕捉长期依赖的低复杂度替代方案，我们提出了LOCOST：一种基于状态空间模型的编码器-解码器架构，用于具有长上下文输入的条件文本生成。这种架构的计算复杂度为O（L log L），可以处理比基于稀疏注意模式的最先进模型更长的序列。我们在一系列长文档抽象摘要化任务上评估了我们的模型。该模型在性能水平上达到了与相同大小的最优稀疏变压器相当的93-96%，同时在训练期间节省了高达50%的内存，在推断期间节省了高达87%的内存。此外，LOCOST有效地处理超过600K个标记的输入文本，为完整书摘要化设定了新的最新结果，并为长输入处理开辟了新的视角。

    State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of $O(L \log L)$, this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.
    
[^20]: 基于图注意力的多无人机辅助通信中路径设计和资源分配的增强学习

    Graph Attention-based Reinforcement Learning for Trajectory Design and Resource Assignment in Multi-UAV Assisted Communication

    [https://arxiv.org/abs/2401.17880](https://arxiv.org/abs/2401.17880)

    本论文提出了一种基于图注意力的多智能体可信区间（GA-MATR）强化学习框架，用于解决多无人机辅助通信中的路径设计和资源分配问题。该框架通过引入图循环网络和注意机制，能够处理复杂的通信网络拓扑结构，提取有用的信息和模式。

    

    在多个无人机辅助下行通信中，无人机基站（UAV BSs）在未知环境中实现路径设计和资源分配是一项具有挑战性的任务。通信网络中无人机基站之间的合作和竞争导致了马尔可夫博弈问题。多智能体强化学习是解决上述决策问题的重要方案。然而，仍存在许多共同问题，如系统的不稳定性和历史数据利用率低，限制了其应用。本文提出了一种新颖的图注意力多智能体可信区间（GA-MATR）强化学习框架来解决多无人机辅助通信问题。引入图循环网络来处理和分析通信网络的复杂拓扑结构，从观测信息中提取有用的信息和模式。注意机制为传递的信息提供了额外的加权。

    In the multiple unmanned aerial vehicle (UAV)- assisted downlink communication, it is challenging for UAV base stations (UAV BSs) to realize trajectory design and resource assignment in unknown environments. The cooperation and competition between UAV BSs in the communication network leads to a Markov game problem. Multi-agent reinforcement learning is a significant solution for the above decision-making. However, there are still many common issues, such as the instability of the system and low utilization of historical data, that limit its application. In this paper, a novel graph-attention multi-agent trust region (GA-MATR) reinforcement learning framework is proposed to solve the multi-UAV assisted communication problem. Graph recurrent network is introduced to process and analyze complex topology of the communication network, so as to extract useful information and patterns from observational information. The attention mechanism provides additional weighting for conveyed informatio
    
[^21]: 使用远程连接通知的Transformer进行高效的次季节天气预报

    Efficient Subseasonal Weather Forecast using Teleconnection-informed Transformers

    [https://arxiv.org/abs/2401.17870](https://arxiv.org/abs/2401.17870)

    提出了一种使用远程连接通知的Transformer模型来实现高效的次季节天气预报，该方法通过利用预训练模型和集成远程连接通知的时间模块来改进预测能力。

    

    次季节预报对农业、水资源管理和灾害预警至关重要，但由于大气的混沌性，面临着挑战。最近机器学习领域的进展通过实现与数值模型相当的预测能力，革新了天气预报。然而，训练这些基础模型需要数千个GPU的计算时间，这导致相当多的碳排放，并限制了其更广泛的应用。此外，机器学习模型往往通过产生平滑的结果来愚弄像素误差评分，这些结果缺乏物理一致性和气象意义。为了解决上述问题，我们提出了一种远程连接通知的Transformer。我们的架构利用预训练的Pangu模型来获得良好的初始权重，并集成了一个远程连接通知的时间模块，以提高在延长的时间范围内的可预测性。值得注意的是，通过调整Pangu模型的1.1%参数，我们的方法改进了预测能力。

    Subseasonal forecasting, which is pivotal for agriculture, water resource management, and early warning of disasters, faces challenges due to the chaotic nature of the atmosphere. Recent advances in machine learning (ML) have revolutionized weather forecasting by achieving competitive predictive skills to numerical models. However, training such foundation models requires thousands of GPU days, which causes substantial carbon emissions and limits their broader applicability. Moreover, ML models tend to fool the pixel-wise error scores by producing smoothed results which lack physical consistency and meteorological meaning. To deal with the aforementioned problems, we propose a teleconnection-informed transformer. Our architecture leverages the pretrained Pangu model to achieve good initial weights and integrates a teleconnection-informed temporal module to improve predictability in an extended temporal range. Remarkably, by adjusting 1.1% of the Pangu model's parameters, our method enh
    
[^22]: 卷积与LoRA相遇: 用于Segment Anything模型的参数高效微调

    Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model

    [https://arxiv.org/abs/2401.17868](https://arxiv.org/abs/2401.17868)

    本文介绍了一种名为Conv-LoRA的参数高效微调方法，它通过在SAM的基础上整合轻量级卷积参数和低秩调整，将图像相关的归纳偏见注入ViT编码器，以提高SAM在特定领域的分割能力和高级图像语义学习能力。

    

    Segment Anything模型（SAM）是图像分割的基础框架。在典型场景中，它展现出了非常出色的零样本泛化能力，但在应用于医学图像和遥感等专业领域时，其优势减弱。为了解决这个局限性，本文引入了Conv-LoRA，一种简单而有效的参数高效微调方法。通过将超轻量级卷积参数与低秩调整（LoRA）相结合，Conv-LoRA能够将与图像相关的归纳偏见注入普通的ViT编码器中，进一步强化SAM的局部先验假设。值得注意的是，Conv-LoRA不仅保留了SAM的广泛分割知识，还恢复了其学习高级图像语义能力，这一能力受到SAM的前景-背景分割预训练的限制。在跨越多个领域的各种基准测试中进行的全面实验强调了Conv-LoRA在将SAM适应到实际语义分割任务中的优越性。

    The Segment Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM's local prior assumption. Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM's foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA's superiority in adapting SAM to real-world semantic s
    
[^23]: 在机器教学中操纵离散输入的预测

    Manipulating Predictions over Discrete Inputs in Machine Teaching

    [https://arxiv.org/abs/2401.17865](https://arxiv.org/abs/2401.17865)

    本文研究离散域中的机器教学，在操纵学生模型的预测方面具有显著的数值优势，可用于矫正错误的预测或恶意操纵模型实现个人利益。

    

    机器教学通常涉及创建一个最优（通常是最小的）数据集，以帮助模型（被称为“学生”）根据教师给出的特定目标实现特定目标。尽管在连续域中广泛存在，但在离散域中对机器教学的有效性的研究相对较少。本文主要研究离散域中的机器教学，具体地说，是通过有效地改变训练数据来操纵学生模型的预测，以实现教师的目标。我们将这个任务形式化为一个组合优化问题，并通过提出一个迭代搜索算法来解决它。我们的算法在教师试图纠正错误的预测以改善学生模型，或恶意操纵模型以错误分类某些特定样本到与其个人利益一致的目标类别的场景中具有显著的数值优势。实验结果表明，我们提出的算法具有超级

    Machine teaching often involves the creation of an optimal (typically minimal) dataset to help a model (referred to as the `student') achieve specific goals given by a teacher. While abundant in the continuous domain, the studies on the effectiveness of machine teaching in the discrete domain are relatively limited. This paper focuses on machine teaching in the discrete domain, specifically on manipulating student models' predictions based on the goals of teachers via changing the training data efficiently. We formulate this task as a combinatorial optimization problem and solve it by proposing an iterative searching algorithm. Our algorithm demonstrates significant numerical merit in the scenarios where a teacher attempts at correcting erroneous predictions to improve the student's models, or maliciously manipulating the model to misclassify some specific samples to the target class aligned with his personal profits. Experimental results show that our proposed algorithm can have super
    
[^24]: 跨视角分层图学习超网络用于技能需求供应联合预测

    A Cross-View Hierarchical Graph Learning Hypernetwork for Skill Demand-Supply Joint Prediction

    [https://arxiv.org/abs/2401.17838](https://arxiv.org/abs/2401.17838)

    本文提出了一个跨视角分层图学习超网络（CHGH）框架，用于联合预测技能需求和供应。框架包括跨视角图编码器、层次图编码器和条件超解码器，能够捕捉不同技能之间的关系和需求供应的内在联系。

    

    技术和产业的迅速变化导致技能需求动态变化，因此员工和雇主预测这种变化以在劳动市场保持竞争优势至关重要。现有的研究要么依赖于领域专家知识，要么将技能演变视为简化的时间序列预测问题。然而，这两种方法都忽视了不同技能之间复杂的关系以及技能需求和供应变化之间的内在联系。本文提出了一种用于联合技能需求和供应预测的跨视角分层图学习超网络（CHGH）框架。具体而言，CHGH是一个编码器-解码器网络，包括：i) 一个跨视角图编码器用于捕捉技能需求和供应之间的相互关联，ii) 一个层次图编码器用于从集群角度建模技能的共同演变，iii) 一个条件超解码器用于共同预测需求和供应变量。

    The rapidly changing landscape of technology and industries leads to dynamic skill requirements, making it crucial for employees and employers to anticipate such shifts to maintain a competitive edge in the labor market. Existing efforts in this area either rely on domain-expert knowledge or regarding skill evolution as a simplified time series forecasting problem. However, both approaches overlook the sophisticated relationships among different skills and the inner-connection between skill demand and supply variations. In this paper, we propose a Cross-view Hierarchical Graph learning Hypernetwork (CHGH) framework for joint skill demand-supply prediction. Specifically, CHGH is an encoder-decoder network consisting of i) a cross-view graph encoder to capture the interconnection between skill demand and supply, ii) a hierarchical graph encoder to model the co-evolution of skills from a cluster-wise perspective, and iii) a conditional hyper-decoder to jointly predict demand and supply va
    
[^25]: 用简单的世界模型预测未来

    Predicting the Future with Simple World Models

    [https://arxiv.org/abs/2401.17835](https://arxiv.org/abs/2401.17835)

    本研究提出了一种正则化方案，通过简化模型的潜在动态，使得世界模型更加可预测。该模型在未来潜在状态预测、视频预测和规划中展现出良好的性能。

    

    世界模型可以用紧凑的潜在空间表示潜在高维度像素观察，从而使得对环境动态建模成为可能。然而，这些模型推导出的潜在动态可能仍然非常复杂。通过简化模型来抽象环境的动态可以带来几个好处。如果潜在动态简单，模型可能更好地推广到新的转换，并发现有用的环境状态的潜在表示。我们提出了一种简化世界模型潜在动态的正则化方案。我们的模型，简约潜在空间模型（PLSM），最小化了潜在状态与其之间产生的动态之间的互信息。这使得动态在状态上具有软性不变性，并使得智能体行为的影响更加可预测。我们将PLSM与用于i)未来潜在状态预测、ii)视频预测和iii)规划的三种不同模型类结合。研究结果表明，我们的正则化模型可以提供更好的预测效果。

    World models can represent potentially high-dimensional pixel observations in compact latent spaces, making it tractable to model the dynamics of the environment. However, the latent dynamics inferred by these models may still be highly complex. Abstracting the dynamics of the environment with simple models can have several benefits. If the latent dynamics are simple, the model may generalize better to novel transitions, and discover useful latent representations of environment states. We propose a regularization scheme that simplifies the world model's latent dynamics. Our model, the Parsimonious Latent Space Model (PLSM), minimizes the mutual information between latent states and the dynamics that arise between them. This makes the dynamics softly state-invariant, and the effects of the agent's actions more predictable. We combine the PLSM with three different model classes used for i) future latent state prediction, ii) video prediction, and iii) planning. We find that our regulariz
    
[^26]: 采用最优输运和粒子梯度下降的保护隐私数据发布方法

    Privacy-preserving data release leveraging optimal transport and particle gradient descent

    [https://arxiv.org/abs/2401.17823](https://arxiv.org/abs/2401.17823)

    该研究提出了一种基于边际的保护隐私数据合成方法PrivPGD，利用了最优输运和粒子梯度下降的工具。该方法在不同领域的数据集上表现出色，具有高度的可扩展性和灵活性，并可以满足特定的领域约束条件。

    

    我们提出了一种新颖的方法，用于保护关键领域（如医疗保健和政府）中隐私的表格数据差分私有数据合成的任务。目前的最先进方法主要使用基于边际的方法，从私有边际估计生成数据集。在本文中，我们引入了PrivPGD，一种基于边际的私有数据合成的新一代方法，利用了最优输运和粒子梯度下降的工具。我们的算法在大范围数据集上优于现有方法，并具有高度可扩展性和灵活性，可以结合其他领域特定的约束条件。

    We present a novel approach for differentially private data synthesis of protected tabular datasets, a relevant task in highly sensitive domains such as healthcare and government. Current state-of-the-art methods predominantly use marginal-based approaches, where a dataset is generated from private estimates of the marginals. In this paper, we introduce PrivPGD, a new generation method for marginal-based private data synthesis, leveraging tools from optimal transport and particle gradient descent. Our algorithm outperforms existing methods on a large range of datasets while being highly scalable and offering the flexibility to incorporate additional domain-specific constraints.
    
[^27]: SWEA:通过主题词嵌入修改改变大型语言模型中的事实知识

    SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering

    [https://arxiv.org/abs/2401.17809](https://arxiv.org/abs/2401.17809)

    提出了一种主题词嵌入修改框架（SWEA），通过在推理阶段修改主题的表示来编辑知识，保护模型的原始权重，避免不可逆的损害和额外的推理开销。

    

    模型编辑近来引起了广泛关注。目前的模型编辑方法主要涉及修改模型参数或向现有模型添加附加模块。然而，前者会对LLM造成不可逆的影响，而后者会产生额外的推理开销，并且模糊的向量匹配并不总是可靠的。为了解决这些问题，我们提出了一种可扩展的主题词嵌入修改（SWEA）框架，它在推理阶段修改主题的表示，并实现编辑知识的目标。SWEA在模型外部使用精确的关键匹配，并进行可靠的主题词嵌入修改，从而保护模型的原始权重而不增加推理开销。然后，我们提出优化抑制融合方法，首先优化编辑目标的嵌入向量，然后抑制知识嵌入维度（KED）以获得最终融合的嵌入。我们因此提出了SWEAOS元方法。

    Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS met
    
[^28]: 带有动量对比学习的蒸馏增强时间序列预测网络

    Distillation Enhanced Time Series Forecasting Network with Momentum Contrastive Learning

    [https://arxiv.org/abs/2401.17802](https://arxiv.org/abs/2401.17802)

    本论文提出了一种创新的蒸馏增强框架，用于长序列时间序列预测。通过设计可学习的数据增强机制和带有动量更新的对比学习任务，能够充分利用时间序列数据的复杂性，并获得更鲁棒的表示。

    

    对比表示学习在时间序列分析中非常重要，可以缓解数据噪声、不完整性以及监督信号稀疏性等问题。然而，现有的对比学习框架通常聚焦于时间内部特征，未能充分利用时间序列数据的复杂性。为解决这个问题，我们提出了DE-TSMCL，一种创新的用于长序列时间序列预测的蒸馏增强框架。具体来说，我们设计了一种可学习的数据增强机制，用于自适应地学习是否屏蔽时间戳以获得优化的子序列。然后，我们提出了一种带有动量更新的对比学习任务，探索时间序列的样本间和时间内部的相关性，从而学习未标记时间序列的潜在结构特征。同时，我们设计了一个监督任务，以学习更鲁棒的表示并促进对比学习过程。最后，我们联合优化上述两个任务。

    Contrastive representation learning is crucial in time series analysis as it alleviates the issue of data noise and incompleteness as well as sparsity of supervision signal. However, existing constrastive learning frameworks usually focus on intral-temporal features, which fails to fully exploit the intricate nature of time series data. To address this issue, we propose DE-TSMCL, an innovative distillation enhanced framework for long sequence time series forecasting. Specifically, we design a learnable data augmentation mechanism which adaptively learns whether to mask a timestamp to obtain optimized sub-sequences. Then, we propose a contrastive learning task with momentum update to explore inter-sample and intra-temporal correlations of time series to learn the underlying structure feature on the unlabeled time series. Meanwhile, we design a supervised task to learn more robust representations and facilitate the contrastive learning process. Finally, we jointly optimize the above two 
    
[^29]: 不带位置编码的图变压器

    Graph Transformers without Positional Encodings

    [https://arxiv.org/abs/2401.17791](https://arxiv.org/abs/2401.17791)

    本文介绍了一种不需要位置编码的图变压器模型，该模型通过注意机制本身包含图结构信息，并通过实验证明了其有效性。

    

    最近，用于图表示学习的变压器越来越受欢迎，在各种数据集上取得了最先进的性能，无论是单独使用还是与消息传递图神经网络（MP-GNN）结合。将图归纳偏见融入天然与结构无关的变压器架构中，以结构或位置编码（PEs）的形式，是实现这些令人印象深刻的结果的关键。然而，设计这样的编码是棘手的，人们已经提出了不同的尝试来设计这样的编码，包括拉普拉斯特征向量、相对随机行走概率（RRWP）、空间编码、中心度编码、边缘编码等。在这项工作中，我们认为这些编码可能根本不需要，只要注意机制本身包含有关图结构的信息。我们介绍了Eigenformer，它使用一种新颖的谱感知注意机制，了解图的拉普拉斯谱，并通过实验证明

    Recently, Transformers for graph representation learning have become increasingly popular, achieving state-of-the-art performance on a wide-variety of datasets, either alone or in combination with message-passing graph neural networks (MP-GNNs). Infusing graph inductive-biases in the innately structure-agnostic transformer architecture in the form of structural or positional encodings (PEs) is key to achieving these impressive results. However, designing such encodings is tricky and disparate attempts have been made to engineer such encodings including Laplacian eigenvectors, relative random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge encodings etc. In this work, we argue that such encodings may not be required at all, provided the attention mechanism itself incorporates information about the graph structure. We introduce Eigenformer, which uses a novel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of the graph, and empirically show
    
[^30]: RADIN: 低成本集成模型优化方法

    RADIN: Souping on a Budget

    [https://arxiv.org/abs/2401.17790](https://arxiv.org/abs/2401.17790)

    本文提出了一种名为RADIN的低成本集成模型优化方法，通过使用平均集合logit性能来近似soup性能，实现了加速模型soup的效果。

    

    模型Soup通过结合使用不同超参数微调的模型来扩展随机权重平均（SWA），但是其采用受到了子集选择问题的计算挑战的限制。在本文中，我们提出了一种通过使用平均集合logit性能来近似soup性能从而加速soup模型的方法。理论上的洞察验证了在任何混合比例下，集合logit与权重平均soup之间的一致性。我们的调整资源soup构造（RADIN）过程通过允许灵活的评估预算，使用户能够根据自己的资源调整探索预算，同时提高低预算下的性能，相比之前的贪心方法提高了4％（在ImageNet上）。

    Model Soups, extending Stochastic Weights Averaging (SWA), combine models fine-tuned with different hyperparameters. Yet, their adoption is hindered by computational challenges due to subset selection issues. In this paper, we propose to speed up model soups by approximating soups performance using averaged ensemble logits performances. Theoretical insights validate the congruence between ensemble logits and weight averaging soups across any mixing ratios. Our Resource ADjusted soups craftINg (RADIN) procedure stands out by allowing flexible evaluation budgets, enabling users to adjust his budget of exploration adapted to his resources while increasing performance at lower budget compared to previous greedy approach (up to 4% on ImageNet).
    
[^31]: 弹性神经图像压缩中的鲁棒过拟合潜变量

    Robustly overfitting latents for flexible neural image compression

    [https://arxiv.org/abs/2401.17789](https://arxiv.org/abs/2401.17789)

    这项研究提出了一种鲁棒的过拟合潜变量方法来改进神经图像压缩模型，通过使用SGA+，可以显著提高性能并减少对超参数选择的敏感性。

    

    神经图像压缩取得了很大的进展。最先进的模型基于变分自编码器，胜过了传统模型。神经压缩模型学会将图像编码为量化的潜变量表示，然后将其高效地发送给解码器，解码器再将量化的潜变量解码为重建图像。虽然这些模型在实践中取得了成功，但由于优化不完美以及编码器和解码器容量的限制，它们导致了次优结果。最近的研究表明，如何利用随机Gumbel退火（SGA）来改进预训练的神经图像压缩模型的潜变量。我们通过引入SGA+扩展了这个想法，SGA+包含了三种不同的方法，这些方法都建立在SGA的基础上。此外，我们对我们提出的方法进行了详细分析，展示了它们如何改进性能，并且证明它们对超参数选择不敏感。此外，我们还展示了如何将每个方法扩展到三个而不是两个。

    Neural image compression has made a great deal of progress. State-of-the-art models are based on variational autoencoders and are outperforming classical models. Neural compression models learn to encode an image into a quantized latent representation that can be efficiently sent to the decoder, which decodes the quantized latent into a reconstructed image. While these models have proven successful in practice, they lead to sub-optimal results due to imperfect optimization and limitations in the encoder and decoder capacity. Recent work shows how to use stochastic Gumbel annealing (SGA) to refine the latents of pre-trained neural image compression models. We extend this idea by introducing SGA+, which contains three different methods that build upon SGA. Further, we give a detailed analysis of our proposed methods, show how they improve performance, and show that they are less sensitive to hyperparameter choices. Besides, we show how each method can be extended to three- instead of two
    
[^32]: 基于视觉的毫米波波束管理数字孪生创建技术

    Vision-Assisted Digital Twin Creation for mmWave Beam Management

    [https://arxiv.org/abs/2401.17781](https://arxiv.org/abs/2401.17781)

    这项研究提出了一种基于视觉的数字孪生创建技术，通过使用单个摄像头和位置信息，有效解决了在毫米波系统中3D数字孪生的精度要求，并展示了在波束获取任务中的性能优势。

    

    在通信网络的背景下，数字孪生技术提供了一种重复无线电频率（RF）传播环境以及系统行为的方式，允许基于模拟来优化部署系统的性能。数字孪生技术应用于毫米波系统的一个关键挑战是现有通道模拟器对3D数字孪生精度的严格要求，降低了技术在实际应用中的可行性。我们提出了一种实际的数字孪生创建流程和通道模拟器，仅依赖于单个安装的摄像头和位置信息。我们使用DeepSense6G挑战的真实数据集，在波束获取的下游子任务中，展示了与不明确建模3D环境的方法相比的性能优势。

    In the context of communication networks, digital twin technology provides a means to replicate the radio frequency (RF) propagation environment as well as the system behaviour, allowing for a way to optimize the performance of a deployed system based on simulations. One of the key challenges in the application of Digital Twin technology to mmWave systems is the prevalent channel simulators' stringent requirements on the accuracy of the 3D Digital Twin, reducing the feasibility of the technology in real applications. We propose a practical Digital Twin creation pipeline and a channel simulator, that relies only on a single mounted camera and position information. We demonstrate the performance benefits compared to methods that do not explicitly model the 3D environment, on downstream sub-tasks in beam acquisition, using the real-world dataset of the DeepSense6G challenge
    
[^33]: 一种带有均匀PAC保证的约束MDPs的策略梯度原始对偶算法

    A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with Uniform PAC Guarantees

    [https://arxiv.org/abs/2401.17780](https://arxiv.org/abs/2401.17780)

    本文介绍了一种带有均匀PAC保证的策略梯度原始对偶算法，用于在线约束马尔可夫决策过程（CMDP）问题。该算法同时保证了收敛到最优策略、次线性遗憾和多项式样本复杂度，并在实证研究中验证了其优越性能。

    

    我们研究了一种基于原始对偶强化学习算法的在线约束马尔可夫决策过程（CMDP）问题，其中代理探索的最优策略在满足约束的同时最大化回报。尽管在实践中被广泛使用，但现有的理论文献仅提供次线性遗憾保证，并未能确保收敛到最优策略。在本文中，我们引入了一种新颖的带有均匀可能近似正确性（Uniform-PAC）保证的策略梯度原始对偶算法，同时确保收敛到最优策略、次线性遗憾和多项式样本复杂度以实现任何目标精度。值得注意的是，这是在线CMDP问题的第一个Uniform-PAC算法。除了理论保证外，我们还在一个简单的CMDP中进行了实证研究，证明了我们的算法收敛到最优策略，而现有算法表现出振荡性能表现。

    We study a primal-dual reinforcement learning (RL) algorithm for the online constrained Markov decision processes (CMDP) problem, wherein the agent explores an optimal policy that maximizes return while satisfying constraints. Despite its widespread practical use, the existing theoretical literature on primal-dual RL algorithms for this problem only provides sublinear regret guarantees and fails to ensure convergence to optimal policies. In this paper, we introduce a novel policy gradient primal-dual algorithm with uniform probably approximate correctness (Uniform-PAC) guarantees, simultaneously ensuring convergence to optimal policies, sublinear regret, and polynomial sample complexity for any target accuracy. Notably, this represents the first Uniform-PAC algorithm for the online CMDP problem. In addition to the theoretical guarantees, we empirically demonstrate in a simple CMDP that our algorithm converges to optimal policies, while an existing algorithm exhibits oscillatory perform
    
[^34]: 使用非线性协方差矩阵估计器的正则化线性判别分析方法

    Regularized Linear Discriminant Analysis Using a Nonlinear Covariance Matrix Estimator

    [https://arxiv.org/abs/2401.17760](https://arxiv.org/abs/2401.17760)

    本文研究了使用非线性协方差矩阵估计器的正则化线性判别分析方法，以解决特征空间维度高于训练数据大小时数据协方差矩阵病态导致效率低下的问题。

    

    线性判别分析（LDA）是一种广泛使用的数据分类技术。该方法在许多分类问题中具有良好的性能，但在数据协方差矩阵病态条件下效率低下。这通常发生在特征空间的维度高于或接近训练数据大小的情况下。为了应对这种情况，提出了基于正则化线性估计器的正则化LDA（RLDA）方法。RLDA方法的性能已得到充分研究，并已提出了最优的正则化方案。本文研究了与非线性（NL）协方差矩阵估计器相一致的正半定 Ridge 型逆协方差矩阵估计器的能力。通过重新制定利用线性估计方法的最优分类器的得分函数，得到了该估计器，最终形成了所提出的NL-RLDA分类器。

    Linear discriminant analysis (LDA) is a widely used technique for data classification. The method offers adequate performance in many classification problems, but it becomes inefficient when the data covariance matrix is ill-conditioned. This often occurs when the feature space's dimensionality is higher than or comparable to the training data size. Regularized LDA (RLDA) methods based on regularized linear estimators of the data covariance matrix have been proposed to cope with such a situation. The performance of RLDA methods is well studied, with optimal regularization schemes already proposed. In this paper, we investigate the capability of a positive semidefinite ridge-type estimator of the inverse covariance matrix that coincides with a nonlinear (NL) covariance matrix estimator. The estimator is derived by reformulating the score function of the optimal classifier utilizing linear estimation methods, which eventually results in the proposed NL-RLDA classifier. We derive asymptot
    
[^35]: PF-GNN: 可微的基于粒子滤波的通用图表示逼近

    PF-GNN: Differentiable particle filtering based approximation of universal graph representations

    [https://arxiv.org/abs/2401.17752](https://arxiv.org/abs/2401.17752)

    PF-GNN是一种可微的基于粒子滤波的通用图表示逼近算法，通过引入精确同构求解技术指导学习过程，从而提高了图神经网络的表达能力。

    

    消息传递的图神经网络（GNN）在表示图同构性的1-WL颜色精炼测试方面受到了限制。其他更具表达能力的模型要么计算成本高，要么需要预处理来提取结构特征。在这项工作中，我们提出了一种通过使用精确同构求解技术来指导学习过程，以使GNN成为通用模型。该技术在个体化和精炼（IR）范式下操作，通过人为引入不对称性并进一步精炼着色，当1-WL停止时。同构求解器生成一个颜色着色的搜索树，其叶子节点能唯一标识图。然而，搜索树的大小呈指数级增长，并且需要手工设计的修剪技术，这在学习角度上是不可取的。我们采用概率视角，并通过从根节点到叶子节点的搜索树中采样多条路径，来近似颜色着色的搜索树（即嵌入）。以学习更有辨别性的表示。

    Message passing Graph Neural Networks (GNNs) are known to be limited in expressive power by the 1-WL color-refinement test for graph isomorphism. Other more expressive models either are computationally expensive or need preprocessing to extract structural features from the graph. In this work, we propose to make GNNs universal by guiding the learning process with exact isomorphism solver techniques which operate on the paradigm of Individualization and Refinement (IR), a method to artificially introduce asymmetry and further refine the coloring when 1-WL stops. Isomorphism solvers generate a search tree of colorings whose leaves uniquely identify the graph. However, the tree grows exponentially large and needs hand-crafted pruning techniques which are not desirable from a learning perspective. We take a probabilistic view and approximate the search tree of colorings (i.e. embeddings) by sampling multiple paths from root to leaves of the search tree. To learn more discriminative represe
    
[^36]: 算法鲁棒的预测聚合

    Algorithmic Robust Forecast Aggregation

    [https://arxiv.org/abs/2401.17743](https://arxiv.org/abs/2401.17743)

    该论文提出了一种算法框架用于算法鲁棒的预测聚合，旨在找到与全知聚合相比具有最小最坏情况遗憾的聚合器。数值实验证明了该算法框架的优越性能。

    

    预测聚合通过结合多个预测器的预测结果来提高准确性。然而，对于预测器信息结构的缺乏了解阻碍了最优聚合的实现。在给定信息结构族的情况下，算法鲁棒的预测聚合旨在找到与全知聚合相比具有最小最坏情况遗憾的聚合器。先前的算法鲁棒预测聚合方法依赖于经验观察和参数调整。我们提出了一种算法框架用于算法鲁棒的预测聚合。我们的框架针对具有有限信息结构族的普遍信息聚合问题提供了高效的逼近方案。在Arieli等人（2018）研究的情境下，其中两个代理接收与二进制状态有关的独立信号，我们的框架还可以通过对聚合器施加Lipschitz条件或对代理报告施加离散条件来提供高效的逼近方案。数值实验证明了我们的算法框架的优越性能。

    Forecast aggregation combines the predictions of multiple forecasters to improve accuracy. However, the lack of knowledge about forecasters' information structure hinders optimal aggregation. Given a family of information structures, robust forecast aggregation aims to find the aggregator with minimal worst-case regret compared to the omniscient aggregator. Previous approaches for robust forecast aggregation rely on heuristic observations and parameter tuning. We propose an algorithmic framework for robust forecast aggregation. Our framework provides efficient approximation schemes for general information aggregation with a finite family of possible information structures. In the setting considered by Arieli et al. (2018) where two agents receive independent signals conditioned on a binary state, our framework also provides efficient approximation schemes by imposing Lipschitz conditions on the aggregator or discrete conditions on agents' reports. Numerical experiments demonstrate the 
    
[^37]: 不需要伴随算子的算子学习

    Operator learning without the adjoint

    [https://arxiv.org/abs/2401.17739](https://arxiv.org/abs/2401.17739)

    本论文提出了一种不需要探测伴随算子的算子学习方法，通过在Fourier基上进行投影来逼近一类非自伴随的无限维紧算子，并应用于恢复椭圆型偏微分算子的格林函数。这是第一个试图填补算子学习理论与实践差距的无需伴随算子分析。

    

    算子学习中存在一个谜团：如何在没有探测伴随算子的情况下从数据中恢复非自伴随算子？目前的实际方法表明，在仅使用由算子的正向作用生成的数据的情况下，可以准确地恢复算子，而不需要访问伴随算子。然而，以直观的方式看，似乎有必要采样伴随算子的作用。在本文中，我们部分解释了这个谜团，通过证明在不查询伴随算子的情况下，可以通过在Fourier基上进行投影来逼近一类非自伴随的无限维紧算子。然后，我们将该结果应用于恢复椭圆型偏微分算子的格林函数，并导出一个无需伴随算子的样本复杂度界限。虽然现有的理论证明了算子学习的低样本复杂度，但我们的是第一个试图填补理论与实践差距的无需伴随算子的分析。

    There is a mystery at the heart of operator learning: how can one recover a non-self-adjoint operator from data without probing the adjoint? Current practical approaches suggest that one can accurately recover an operator while only using data generated by the forward action of the operator without access to the adjoint. However, naively, it seems essential to sample the action of the adjoint. In this paper, we partially explain this mystery by proving that without querying the adjoint, one can approximate a family of non-self-adjoint infinite-dimensional compact operators via projection onto a Fourier basis. We then apply the result to recovering Green's functions of elliptic partial differential operators and derive an adjoint-free sample complexity bound. While existing theory justifies low sample complexity in operator learning, ours is the first adjoint-free analysis that attempts to close the gap between theory and practice.
    
[^38]: 利用智能手表麦克风传感器进行咳嗽检测和分类的研究

    Harnessing Smartwatch Microphone Sensors for Cough Detection and Classification

    [https://arxiv.org/abs/2401.17738](https://arxiv.org/abs/2401.17738)

    本研究利用智能手表的麦克风传感器监测和分类了各种咳嗽，通过结构化处理和专门的1D卷积神经网络模型，成功实现了98.49%和98.2%的准确率，并成功识别出四种不同的咳嗽类型。

    

    本研究探讨了利用内置麦克风传感器的智能手表监测咳嗽并检测各种咳嗽类型的潜力。我们进行了一项涉及32名参与者的研究，并以受控方式收集了9个小时的音频数据。之后，我们采用结构化方法对这些数据进行了处理，得到了223个阳性咳嗽样本。我们通过增强技术进一步改进了数据集，并采用了专门的1D卷积神经网络模型。该模型在非步行时具有令人印象深刻的准确率98.49%，在步行时为98.2%，表明智能手表可以检测到咳嗽。此外，我们的研究成功利用聚类技术识别出了四种不同的咳嗽类型。

    This study investigates the potential of using smartwatches with built-in microphone sensors for monitoring coughs and detecting various cough types. We conducted a study involving 32 participants and collected 9 hours of audio data in a controlled manner. Afterward, we processed this data using a structured approach, resulting in 223 positive cough samples. We further improved the dataset through augmentation techniques and employed a specialized 1D CNN model. This model achieved an impressive accuracy rate of 98.49% while non-walking and 98.2% while walking, showing smartwatches can detect cough. Moreover, our research successfully identified four distinct types of coughs using clustering techniques.
    
[^39]: 基于层级偏差驱动分层的可解释因果效应估计

    Hierarchical Bias-Driven Stratification for Interpretable Causal Effect Estimation

    [https://arxiv.org/abs/2401.17737](https://arxiv.org/abs/2401.17737)

    BICauseTree是一种基于层级偏差驱动分层的可解释因果效应估计方法，通过使用决策树进行平衡、减少偏差和确定目标人群定义，提供了可解释性和透明性。

    

    可解释性和透明性对于将观察数据中的因果效应模型纳入政策决策至关重要。它们可以在缺乏真实标签来评估这些模型准确性的情况下提供对模型的信任。到目前为止，透明的因果效应估计尝试包括将事后解释方法应用于不可解释的黑盒模型。在这里，我们提出了BICauseTree：一种可解释的平衡方法，用于识别局部发生自然实验的聚类。我们的方法基于带有自定义目标函数的决策树，以改进平衡和减少处理分配偏差。因此，它还可以检测出存在正性违规的子群体，排除它们，并提供基于协变量的目标人群定义，我们可以从中推断并推广。我们使用合成和真实数据集评估该方法的性能，并探索其偏差可解释性的权衡。

    Interpretability and transparency are essential for incorporating causal effect models from observational data into policy decision-making. They can provide trust for the model in the absence of ground truth labels to evaluate the accuracy of such models. To date, attempts at transparent causal effect estimation consist of applying post hoc explanation methods to black-box models, which are not interpretable. Here, we present BICauseTree: an interpretable balancing method that identifies clusters where natural experiments occur locally. Our approach builds on decision trees with a customized objective function to improve balancing and reduce treatment allocation bias. Consequently, it can additionally detect subgroups presenting positivity violations, exclude them, and provide a covariate-based definition of the target population we can infer from and generalize to. We evaluate the method's performance using synthetic and realistic datasets, explore its bias-interpretability tradeoff, 
    
[^40]: 在神经进化系统中迈向物理合理性

    Towards Physical Plausibility in Neuroevolution Systems

    [https://arxiv.org/abs/2401.17733](https://arxiv.org/abs/2401.17733)

    本研究关注神经进化系统中物理合理性的问题，提出了一种旨在最大化人工神经网络模型准确性同时最小化功耗的方法，通过引入新的突变策略和训练技术来优化模型表现和节能效果。

    

    使用人工智能模型，特别是深度神经网络，越来越多地增加了训练和推理过程中的功耗，引发了对更加节能算法和硬件解决方案的需求，并引起了环境担忧。本研究致力于解决机器学习中推理阶段日益增长的能源消耗问题。即使稍微减少电力使用也可能导致显著的能源节约，使用户、公司和环境都受益。我们的方法着重于在神经进化框架中最大化人工神经网络模型的准确性，同时最小化其功耗。为此，我们在适应度函数中考虑了功耗。我们引入一种新的突变策略，以随机方式重新引入层模块，具有节能模块的选择机会更高。我们还引入了一种训练两个独立模型的新技术。

    The increasing usage of Artificial Intelligence (AI) models, especially Deep Neural Networks (DNNs), is increasing the power consumption during training and inference, posing environmental concerns and driving the need for more energy-efficient algorithms and hardware solutions. This work addresses the growing energy consumption problem in Machine Learning (ML), particularly during the inference phase. Even a slight reduction in power usage can lead to significant energy savings, benefiting users, companies, and the environment. Our approach focuses on maximizing the accuracy of Artificial Neural Network (ANN) models using a neuroevolutionary framework whilst minimizing their power consumption. To do so, power consumption is considered in the fitness function. We introduce a new mutation strategy that stochastically reintroduces modules of layers, with power-efficient modules having a higher chance of being chosen. We introduce a novel technique that allows training two separate models
    
[^41]: 使用儿童创伤、心理健康问卷和机器学习级联集成预测印度成年人的自杀行为

    Predicting suicidal behavior among Indian adults using childhood trauma, mental health questionnaires and machine learning cascade ensembles

    [https://arxiv.org/abs/2401.17705](https://arxiv.org/abs/2401.17705)

    该研究使用机器学习方法，结合儿童创伤、心理健康参数和其他行为因素，预测印度成年人的自杀行为。结果表明，级联集成学习方法能有效分类和预测自杀行为。

    

    在年轻成年人中，自杀是印度的主要死因，占到了惊人的国家自杀率约16%。近年来，机器学习算法已经开始利用各种行为特征来预测自杀行为。但到目前为止，机器学习算法在预测印度背景下的自杀行为的有效性尚未在文献中探讨。在本研究中，基于儿童创伤、不同的心理健康指标和其他行为因素，开发了不同的机器学习算法和集成来预测自杀行为。数据集来源于印度某个健康中心的391名个体。通过标准化问卷获取了关于他们的儿童创伤、心理健康和其他心理健康问题的信息。结果显示，使用支持向量机、决策树和随机森林的级联集成学习方法能够将自杀行为进行分类并进行预测。

    Among young adults, suicide is India's leading cause of death, accounting for an alarming national suicide rate of around 16%. In recent years, machine learning algorithms have emerged to predict suicidal behavior using various behavioral traits. But to date, the efficacy of machine learning algorithms in predicting suicidal behavior in the Indian context has not been explored in literature. In this study, different machine learning algorithms and ensembles were developed to predict suicide behavior based on childhood trauma, different mental health parameters, and other behavioral factors. The dataset was acquired from 391 individuals from a wellness center in India. Information regarding their childhood trauma, psychological wellness, and other mental health issues was acquired through standardized questionnaires. Results revealed that cascade ensemble learning methods using a support vector machine, decision trees, and random forest were able to classify suicidal behavior with an ac
    
[^42]: 通过深度光谱聚类实现数据立方体分割

    Datacube segmentation via Deep Spectral Clustering

    [https://arxiv.org/abs/2401.17695](https://arxiv.org/abs/2401.17695)

    通过应用深度聚类算法对数据立方体像素的光谱属性进行无监督聚类，可以实现数据立方体的图像分割和统计解释。

    

    扩展视觉技术在物理学中无处不在。然而，由此类分析产生的数据立方体在解释上往往具有挑战，因为很难从组成数据立方体的光谱中辨别出相关信息。此外，数据立方体光谱的巨大维度对于统计解释来说是一个复杂的任务；然而，这种复杂性包含了大量的统计信息，可以以无监督的方式利用，以描绘出所研究案例的一些基本特性，例如，可以通过在适当定义的低维嵌入空间中对数据立方体光谱进行（深度）聚类来获得图像分割。为了解决这个问题，我们探索了在编码空间中应用无监督聚类方法的可能性，即对数据立方体像素的光谱属性进行深度聚类。通过专门训练的统计维度缩减器进行统计维度缩减

    Extended Vision techniques are ubiquitous in physics. However, the data cubes steaming from such analysis often pose a challenge in their interpretation, due to the intrinsic difficulty in discerning the relevant information from the spectra composing the data cube.   Furthermore, the huge dimensionality of data cube spectra poses a complex task in its statistical interpretation; nevertheless, this complexity contains a massive amount of statistical information that can be exploited in an unsupervised manner to outline some essential properties of the case study at hand, e.g.~it is possible to obtain an image segmentation via (deep) clustering of data-cube's spectra, performed in a suitably defined low-dimensional embedding space.   To tackle this topic, we explore the possibility of applying unsupervised clustering methods in encoded space, i.e. perform deep clustering on the spectral properties of datacube pixels. A statistical dimensional reduction is performed by an ad hoc trained 
    
[^43]: t-SNE作为流形上点云的梯度流的收敛分析

    Convergence analysis of t-SNE as a gradient flow for point cloud on a manifold

    [https://arxiv.org/abs/2401.17675](https://arxiv.org/abs/2401.17675)

    我们在本论文提出了关于t-SNE算法的收敛性分析，证明了t-SNE生成的点是有界的，并得出了KL散度最小值的存在性。

    

    我们提出了关于t-SNE算法有界性的理论基础。t-SNE采用梯度下降迭代和KL散度作为目标函数，旨在在高维空间中找到一组点，使其与原始数据点的相似度较高，最小化KL散度。在对采样数据集进行弱收敛假设的情况下，研究了t-SNE的属性，如困惑度和相似度，探究了t-SNE生成的点在连续梯度流下的行为。通过证明t-SNE生成的点保持有界，我们利用这个结论来建立KL散度的最小值存在性。

    We present a theoretical foundation regarding the boundedness of the t-SNE algorithm. t-SNE employs gradient descent iteration with Kullback-Leibler (KL) divergence as the objective function, aiming to identify a set of points that closely resemble the original data points in a high-dimensional space, minimizing KL divergence. Investigating t-SNE properties such as perplexity and affinity under a weak convergence assumption on the sampled dataset, we examine the behavior of points generated by t-SNE under continuous gradient flow. Demonstrating that points generated by t-SNE remain bounded, we leverage this insight to establish the existence of a minimizer for KL divergence.
    
[^44]: 从能量模型的潜在空间生成新的桥梁类型的尝试

    An attempt to generate new bridge types from latent space of energy-based model

    [https://arxiv.org/abs/2401.17657](https://arxiv.org/abs/2401.17657)

    使用能量模型进行桥梁创新，通过博弈论解释损失函数，并利用朗之万动力学技术生成能量值较低的新样本，建立基于能量的桥梁生成模型。

    

    使用能量模型进行桥梁创新。通过博弈论解释损失函数，逻辑清晰，公式简单明了。因此避免使用最大似然估计来解释损失函数，并消除了解决标准化分母的蒙特卡洛方法的需求。假设桥梁类型的种群符合玻尔兹曼分布，构建神经网络表示能量函数。利用朗之万动力学技术生成能量值较低的新样本，从而建立基于能量的桥梁生成模型。通过对三跨梁桥、拱桥、斜拉桥和悬索桥的对称结构图像数据集进行能量函数训练，精确计算真实和伪造样本的能量值。从潜在空间进行采样，利用梯度下降算法，能量函数将采样点转化为能量得分低的样本，从而生成新的桥梁。

    Use energy-based model for bridge-type innovation. The loss function is explained by the game theory, the logic is clear and the formula is simple and clear. Thus avoid the use of maximum likelihood estimation to explain the loss function and eliminate the need for Monte Carlo methods to solve the normalized denominator. Assuming that the bridge-type population follows a Boltzmann distribution, a neural network is constructed to represent the energy function. Use Langevin dynamics technology to generate a new sample with low energy value, thus a generative model of bridge-type based on energy is established. Train energy function on symmetric structured image dataset of three span beam bridge, arch bridge, cable-stayed bridge, and suspension bridge to accurately calculate the energy values of real and fake samples. Sampling from latent space, using gradient descent algorithm, the energy function transforms the sampling points into low energy score samples, thereby generating new bridge
    
[^45]: 合成健康数据初探

    A primer on synthetic health data

    [https://arxiv.org/abs/2401.17653](https://arxiv.org/abs/2401.17653)

    深层生成模型的进展使得创造逼真合成健康数据集成为可能，这些合成数据集可以在不公开敏感信息的情况下进行安全数据共享，并支持各种倡议和项目构思。然而，评估合成数据集与原始数据集的相似性和预测效用，以及解决隐私和法规问题仍然是挑战。

    

    深层生成模型的最新进展极大地扩展了创造逼真合成健康数据集的潜力。这些合成数据集旨在在不公开病人身份或敏感信息的情况下保留从敏感健康数据集中获得的特征、模式和总体科学结论。因此，合成数据可以促进安全数据共享，支持一系列倡议，包括开发新的预测模型、先进的健康IT平台以及一般项目构思和假设发展。然而，许多问题和挑战仍然存在，包括如何与原始真实数据集进行一致的评估合成数据集的相似性和预测效用，以及分享时对隐私的风险。额外的法规和治理问题尚未得到广泛解决。在这个初探中，我们对合成健康数据的现状进行了梳理，包括生成和评估方法和工具，以及现有部署示例。

    Recent advances in deep generative models have greatly expanded the potential to create realistic synthetic health datasets. These synthetic datasets aim to preserve the characteristics, patterns, and overall scientific conclusions derived from sensitive health datasets without disclosing patient identity or sensitive information. Thus, synthetic data can facilitate safe data sharing that supports a range of initiatives including the development of new predictive models, advanced health IT platforms, and general project ideation and hypothesis development. However, many questions and challenges remain, including how to consistently evaluate a synthetic dataset's similarity and predictive utility in comparison to the original real dataset and risk to privacy when shared. Additional regulatory and governance issues have not been widely addressed. In this primer, we map the state of synthetic health data, including generation and evaluation methods and tools, existing examples of deployme
    
[^46]: 基于扩散模型的空间和频率感知图像恢复方法

    Spatial-and-Frequency-aware Restoration method for Images based on Diffusion Models

    [https://arxiv.org/abs/2401.17629](https://arxiv.org/abs/2401.17629)

    本文提出了一种名为SaFaRI的基于空间和频率感知的扩散模型，用于图像恢复。在各种噪声逆问题上，SaFaRI在ImageNet数据集和FFHQ数据集上实现了最先进的性能。

    

    扩散模型最近成为一种有前途的图像恢复（IR）框架，因为它们能够产生高质量的重建结果并且与现有方法兼容。现有的解决IR中噪声逆问题的方法通常仅考虑像素级的数据保真度。在本文中，我们提出了一种名为SaFaRI的基于空间和频率感知的扩散模型，用于处理带有高斯噪声的IR问题。我们的模型鼓励图像在空间和频率域中保持数据保真度，从而提高重建质量。我们全面评估了我们的模型在各种噪声逆问题上的性能，包括修复、降噪和超分辨率。我们的细致评估表明，SaFaRI在ImageNet数据集和FFHQ数据集上实现了最先进的性能，以LPIPS和FID指标超过了现有的零样本IR方法。

    Diffusion models have recently emerged as a promising framework for Image Restoration (IR), owing to their ability to produce high-quality reconstructions and their compatibility with established methods. Existing methods for solving noisy inverse problems in IR, considers the pixel-wise data-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware diffusion model for IR with Gaussian noise. Our model encourages images to preserve data-fidelity in both the spatial and frequency domains, resulting in enhanced reconstruction quality. We comprehensively evaluate the performance of our model on a variety of noisy inverse problems, including inpainting, denoising, and super-resolution. Our thorough evaluation demonstrates that SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS and FID metrics.
    
[^47]: 生成式人工智能生成测试数据生成器

    Generative AI to Generate Test Data Generators

    [https://arxiv.org/abs/2401.17626](https://arxiv.org/abs/2401.17626)

    本研究评估了生成式人工智能在不同领域生成测试数据的能力，并证明它能够在不同的集成级别上生成逼真的测试数据生成器。

    

    生成假数据是现代软件测试的重要维度之一，众多数据伪造库的数量和重要性证明了这一点。然而，伪造库的开发者无法满足不同自然语言和领域所需生成的广泛数据范围。本文评估了生成式人工智能在不同领域生成测试数据的能力。我们设计了三种类型的大型语言模型（LLMs）提示，它们在不同的集成级别上执行测试数据生成任务：1）原始测试数据生成，2）合成特定语言的程序以生成有用的测试数据，3）生成使用尖端伪造库的程序。我们通过提示LLMs为11个领域生成测试数据来评估我们的方法。结果表明，在所有三个集成级别上，LLMs能够成功地生成逼真的测试数据生成器。

    Generating fake data is an essential dimension of modern software testing, as demonstrated by the number and significance of data faking libraries. Yet, developers of faking libraries cannot keep up with the wide range of data to be generated for different natural languages and domains. In this paper, we assess the ability of generative AI for generating test data in different domains. We design three types of prompts for Large Language Models (LLMs), which perform test data generation tasks at different levels of integrability: 1) raw test data generation, 2) synthesizing programs in a specific language that generate useful test data, and 3) producing programs that use state-of-the-art faker libraries. We evaluate our approach by prompting LLMs to generate test data for 11 domains. The results show that LLMs can successfully generate realistic test data generators in a wide range of domains at all three levels of integrability.
    
[^48]: 分子属性预测的图多相似性学习

    Graph Multi-Similarity Learning for Molecular Property Prediction

    [https://arxiv.org/abs/2401.17615](https://arxiv.org/abs/2401.17615)

    提出了图多相似性学习 (GraphMSL)框架，通过引入连续尺度的多相似性度量，包括自相似度和相对相似性，以及对不同化学特征进行融合，提高了分子属性预测的效果和适用性。

    

    有效的分子表示学习对于分子属性预测至关重要。对分子表示学习而言，对比学习是一种明显的自监督方法，它依赖于建立正负对。然而，这种二进制相似性分类过于简化了复杂分子关系的性质，并忽视了分子之间相对相似性的程度，给表示学习的效果和泛化性带来挑战。为了应对这一挑战，我们提出了用于分子属性预测的图多相似性学习(GraphMSL)框架。GraphMSL将广义多相似性度量融入到连续尺度中，包括自相似度和相对相似性。单模多相似性度量来自于各种化学模态，而这些度量的融合形式则显著增强了GraphMSL的效果。此外，融合的灵活性能够适应不同任务需求，使得GraphMSL在分子属性预测中具备更广的适用性。

    Effective molecular representation learning is essential for molecular property prediction. Contrastive learning, a prominent self-supervised approach for molecular representation learning, relies on establishing positive and negative pairs. However, this binary similarity categorization oversimplifies the nature of complex molecular relationships and overlooks the degree of relative similarities among molecules, posing challenges to the effectiveness and generality of representation learning. In response to this challenge, we propose the Graph Multi-Similarity Learning for Molecular Property Prediction (GraphMSL) framework. GraphMSL incorporates a generalized multi-similarity metric in a continuous scale, capturing self-similarity and relative similarities. The unimodal multi-similarity metrics are derived from various chemical modalities, and the fusion of these metrics into a multimodal form significantly enhances the effectiveness of GraphMSL. In addition, the flexibility of fusion
    
[^49]: IGCN：用于多模态数据的综合图卷积网络

    IGCN: Integrative Graph Convolutional Networks for Multi-modal Data

    [https://arxiv.org/abs/2401.17612](https://arxiv.org/abs/2401.17612)

    IGCN是用于多模态数据的综合神经网络，通过综合分析多模态数据来获得更好的学习表示，并提高模型的可解释性。

    

    最近图神经网络（GNN）的进展已经导致了图数据建模的显著增长，用于包含各种类型节点和边的多模态数据。尽管最近已经开发了一些用于网络结构化数据的综合预测解决方案，但这些方法存在一些限制。对于涉及多模态数据的节点分类任务，某些数据模态在预测一个类时表现更好，而其他数据模态可能在预测不同类别时表现出色。因此，为了获得更好的学习表示，需要先进的计算方法来进行多模态数据的综合分析。此外，现有的综合工具缺乏对其特定预测背后原理的全面和连贯理解，使其无法用于提高模型的可解释性。为了解决这些限制，我们引入了一种新的用于多模态数据网络的综合神经网络方法，名为综合图卷积网络（IGCN）。

    Recent advances in Graph Neural Networks (GNN) have led to a considerable growth in graph data modeling for multi-modal data which contains various types of nodes and edges. Although some integrative prediction solutions have been developed recently for network-structured data, these methods have some restrictions. For a node classification task involving multi-modal data, certain data modalities may perform better when predicting one class, while others might excel in predicting a different class. Thus, to obtain a better learning representation, advanced computational methodologies are required for the integrative analysis of multi-modal data. Moreover, existing integrative tools lack a comprehensive and cohesive understanding of the rationale behind their specific predictions, making them unsuitable for enhancing model interpretability. Addressing these restrictions, we introduce a novel integrative neural network approach for multi-modal data networks, named Integrative Graph Convo
    
[^50]: 传播与陷阱：通过反事实任务评估基于推理的知识编辑的困境

    Propagation and Pitfalls: Reasoning-based Assessment of Knowledge Editing through Counterfactual Tasks

    [https://arxiv.org/abs/2401.17585](https://arxiv.org/abs/2401.17585)

    该论文针对现有的知识编辑方法在推理能力方面的限制，通过引入ReCoE数据集进行了深入分析。研究发现所有的模型编辑方法在该数据集上表现较差，尤其在特定的推理方案中。此外，通过对编辑模型思维链生成的分析，揭示了现有方法的不足之处，包括对事实编辑、事实回忆能力和连贯性的考量。

    

    当前的知识编辑方法在有效传播更新的相互关联事实方面面临困难。在这项工作中，我们深入探讨了阻碍准确推理模型中更新知识适当传播的障碍。为了支持我们的分析，我们引入了一种新颖的基于推理的基准——ReCoE（基于推理的反事实编辑数据集），涵盖了现实世界中的六种常见推理方案。我们对现有的知识编辑技术进行了全面分析，包括输入增强、微调和定位编辑。我们发现所有模型编辑方法在这个数据集上的表现都明显较低，尤其是在某些推理方案中。我们通过对编辑模型的思维链生成的分析，从推理的角度揭示了现有知识编辑方法不足的关键原因，包括对事实编辑、事实回忆能力以及生成的连贯性的方面。我们将公开我们的基准数据集和代码。

    Current approaches of knowledge editing struggle to effectively propagate updates to interconnected facts. In this work, we delve into the barriers that hinder the appropriate propagation of updated knowledge within these models for accurate reasoning. To support our analysis, we introduce a novel reasoning-based benchmark -- ReCoE (Reasoning-based Counterfactual Editing dataset) -- which covers six common reasoning schemes in real world. We conduct a thorough analysis of existing knowledge editing techniques, including input augmentation, finetuning, and locate-and-edit. We found that all model editing methods show notably low performance on this dataset, especially in certain reasoning schemes. Our analysis over the chain-of-thought generation of edited models further uncover key reasons behind the inadequacy of existing knowledge editing methods from a reasoning standpoint, involving aspects on fact-wise editing, fact recall ability, and coherence in generation. We will make our ben
    
[^51]: 敏捷但安全：学习无碰撞高速四足机器人行走

    Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion

    [https://arxiv.org/abs/2401.17583](https://arxiv.org/abs/2401.17583)

    本文介绍了一种名为敏捷但安全（ABS）的学习控制框架，能够实现四足机器人的敏捷且无碰撞行走。该框架通过一个学习得到的控制论到达-避免值网络来实现策略切换，并通过协作运行的敏捷策略和恢复策略，使机器人能够高速且安全地导航。

    

    在杂乱环境中行走的四足机器人必须既敏捷以提高任务执行效率，又要确保安全，避免与障碍物或人碰撞。现有的研究要么开发保守的控制器（速度小于1.0 m/s）以确保安全，要么专注于敏捷性而未考虑潜在致命的碰撞。本文介绍了敏捷但安全（ABS）的学习控制框架，为四足机器人实现了敏捷且无碰撞的行走。ABS包括一个敏捷策略来在障碍物中执行灵活的动作技能，并且有一个恢复策略来避免失败，共同实现高速且无碰撞的导航。ABS中的策略切换由一个学习得到的控制论到达-避免值网络控制，该网络也指导恢复策略作为目标函数，从而在闭环中保护机器人。训练过程涉及敏捷策略、到达-避免值网络、恢复策略和外感知表征的学习。

    Legged robots navigating cluttered environments must be jointly agile for efficient task execution and safe to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (< 1.0 m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an exteroception repre
    
[^52]: 具有内聚子图意识的图对比学习

    Graph Contrastive Learning with Cohesive Subgraph Awareness

    [https://arxiv.org/abs/2401.17580](https://arxiv.org/abs/2401.17580)

    本研究提出了一种名为CTAug的新框架，将内聚子图意识无缝整合到图对比学习中。通过改进图拓扑增强和图学习过程，提高了对各种图的表征学习性能。

    

    图对比学习（GCL）已成为学习各种图表征的先进策略，包括社交和生物医学网络。GCL广泛使用随机图拓扑增强，如均匀节点丢失，生成增强图。然而，这种随机增强可能严重损害图的内在属性并恶化后续的表征学习过程。我们认为，在图增强和学习过程中引入内聚子图意识有可能提高GCL的性能。为此，我们提出了一种称为CTAug的新颖统一框架，以无缝地将内聚意识整合到各种现有的GCL机制中。具体来说，CTAug包括两个专门的模块：拓扑增强增强和图学习增强。前者生成谨慎保留内聚性质的增强图，而后者增强了图的学习能力。

    Graph contrastive learning (GCL) has emerged as a state-of-the-art strategy for learning representations of diverse graphs including social and biomedical networks. GCL widely uses stochastic graph topology augmentation, such as uniform node dropping, to generate augmented graphs. However, such stochastic augmentations may severely damage the intrinsic properties of a graph and deteriorate the following representation learning process. We argue that incorporating an awareness of cohesive subgraphs during the graph augmentation and learning processes has the potential to enhance GCL performance. To this end, we propose a novel unified framework called CTAug, to seamlessly integrate cohesion awareness into various existing GCL mechanisms. In particular, CTAug comprises two specialized modules: topology augmentation enhancement and graph learning enhancement. The former module generates augmented graphs that carefully preserve cohesion properties, while the latter module bolsters the grap
    
[^53]: Scavenging Hyena: 将Transformer模型精炼为长卷积模型

    Scavenging Hyena: Distilling Transformers into Long Convolution Models

    [https://arxiv.org/abs/2401.17574](https://arxiv.org/abs/2401.17574)

    本文介绍了一种通过使用知识蒸馏将Transformer模型中的注意力头替换为Hyena，从而提高效率并处理长上下文信息的方法，超越了传统预训练方法，在准确性和效率方面取得了优秀的结果。这一技术为追求可持续的AI解决方案做出了贡献，实现了计算能力和环境影响的平衡。

    

    大型语言模型（LLMs）的快速发展，以GPT-4等架构为典范，重塑了自然语言处理的领域。本文介绍了一种开创性方法，用于解决LLM预训练中的效率问题，提出了使用知识蒸馏进行跨架构迁移的方法。借鉴高效的Hyena机制的见解，我们的方法通过使用Hyena来替换Transformer模型中的注意力头，提供了一种经济有效的替代传统预训练的方法，同时要面对处理长上下文信息的挑战，这是二次注意力机制固有的。与传统的压缩方法不同，我们的技术不仅提高了推理速度，还在准确性和效率方面超越了预训练。在LLM不断发展的时代，我们的工作为追求可持续的人工智能解决方案作出了贡献，实现了计算能力和环境影响之间的平衡。

    The rapid evolution of Large Language Models (LLMs), epitomized by architectures like GPT-4, has reshaped the landscape of natural language processing. This paper introduces a pioneering approach to address the efficiency concerns associated with LLM pre-training, proposing the use of knowledge distillation for cross-architecture transfer. Leveraging insights from the efficient Hyena mechanism, our method replaces attention heads in transformer models by Hyena, offering a cost-effective alternative to traditional pre-training while confronting the challenge of processing long contextual information, inherent in quadratic attention mechanisms. Unlike conventional compression-focused methods, our technique not only enhances inference speed but also surpasses pre-training in terms of both accuracy and efficiency. In the era of evolving LLMs, our work contributes to the pursuit of sustainable AI solutions, striking a balance between computational power and environmental impact.
    
[^54]: 基于张量的半导体制造过程控制与监控研究

    Tensor-based process control and monitoring for semiconductor manufacturing with unstable disturbances

    [https://arxiv.org/abs/2401.17573](https://arxiv.org/abs/2401.17573)

    本文提出了一种基于张量的工艺控制和监控方法，用于半导体制造过程中高维度基于图像的叠加误差的复杂结构。该方法通过有限的控制配方减小叠加误差，并设计了稳定的张量数据控制器来处理高维度扰动。

    

    随着制造系统中安装传感器的发展和普及，制造过程中收集到了复杂的数据，给传统的过程控制方法带来了挑战。本文提出了一种新颖的工艺控制和监控方法，用于半导体制造过程中高维度基于图像的叠加误差（以张量形式建模）的复杂结构。所提出的方法旨在使用有限的控制配方减小叠加误差。首先建立了一个高维度的过程模型，并提出了不同的张量-向量回归算法来估计模型中的参数，以减轻维度灾难。然后，基于张量参数的估计，设计了具有稳定性的指数加权移动平均（EWMA）张量数据控制器，其稳定性在理论上得到了保证。考虑到低维度的控制配方不能弥补所有高维度扰动的事实。

    With the development and popularity of sensors installed in manufacturing systems, complex data are collected during manufacturing processes, which brings challenges for traditional process control methods. This paper proposes a novel process control and monitoring method for the complex structure of high-dimensional image-based overlay errors (modeled in tensor form), which are collected in semiconductor manufacturing processes. The proposed method aims to reduce overlay errors using limited control recipes. We first build a high-dimensional process model and propose different tensor-on-vector regression algorithms to estimate parameters in the model to alleviate the curse of dimensionality. Then, based on the estimate of tensor parameters, the exponentially weighted moving average (EWMA) controller for tensor data is designed whose stability is theoretically guaranteed. Considering the fact that low-dimensional control recipes cannot compensate for all high-dimensional disturbances o
    
[^55]: 重新思考多元时间序列预测的通道相关性：从领先指标中学习

    Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators

    [https://arxiv.org/abs/2401.17548](https://arxiv.org/abs/2401.17548)

    本文提出了一种新方法，LIFT，通过利用通道相关性和领先指标，为多元时间序列预测提供准确的预测。LIFT方法可以无缝与任意时间序列预测方法协作，大量实验证明了其有效性。

    

    最近，独立于通道的方法在多元时间序列（MTS）预测中取得了最先进的性能。尽管这些方法减少了过拟合的风险，但它们错过了利用通道相关性进行准确预测的潜在机会。我们认为，在变量之间存在局部平稳的领先-滞后关系，即一些滞后变量在短时间内可能遵循领先指标。利用这种通道相关性是有益的，因为领先指标提供了先进信息，可以用来减少滞后变量的预测难度。在本文中，我们提出了一种名为LIFT的新方法，该方法首先在每个时间步骤高效地估计领先指标及其领先步骤，然后巧妙地允许滞后变量利用来自领先指标的先进信息。LIFT作为一个插件，可以与任意时间序列预测方法无缝协作。进行了大量实验证明了LIFT方法的有效性。

    Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments o
    
[^56]: 针对边缘计算设备在入侵检测中的有效多阶段训练模型

    Effective Multi-Stage Training Model For Edge Computing Devices In Intrusion Detection

    [https://arxiv.org/abs/2401.17546](https://arxiv.org/abs/2401.17546)

    这项研究提出了一种针对边缘计算设备在入侵检测中的有效多阶段训练模型，通过增强的剪枝方法和模型压缩技术，提升了系统的效果，同时保持了高准确性。

    

    在不断扩张和互联环境中，入侵检测面临着重大挑战。随着恶意代码的不断进化和攻击方法的复杂化，已经提出了各种先进的基于深度学习的检测方法。然而，入侵检测模型的复杂性和准确性仍需要进一步提升，以使其更适应各种资源受限的设备，特别是嵌入在边缘计算系统中的设备。本研究引入了一种三阶段训练模型，加上增强的剪枝方法和模型压缩技术。目标是提高系统的效果，同时保持入侵检测的高准确性水平。在UNSW-NB15数据集上进行的实证评估表明，该解决方案显著减小了模型的规模，同时保持了与类似提案相当的准确性水平。

    Intrusion detection poses a significant challenge within expansive and persistently interconnected environments. As malicious code continues to advance and sophisticated attack methodologies proliferate, various advanced deep learning-based detection approaches have been proposed. Nevertheless, the complexity and accuracy of intrusion detection models still need further enhancement to render them more adaptable to diverse system categories, particularly within resource-constrained devices, such as those embedded in edge computing systems. This research introduces a three-stage training paradigm, augmented by an enhanced pruning methodology and model compression techniques. The objective is to elevate the system's effectiveness, concurrently maintaining a high level of accuracy for intrusion detection. Empirical assessments conducted on the UNSW-NB15 dataset evince that this solution notably reduces the model's dimensions, while upholding accuracy levels equivalent to similar proposals.
    
[^57]: 可训练的固定点量化加速深度学习在FPGA上的实施

    Trainable Fixed-Point Quantization for Deep Learning Acceleration on FPGAs

    [https://arxiv.org/abs/2401.17544](https://arxiv.org/abs/2401.17544)

    本论文提出了一种可训练的固定点量化方法QFX，实现了基于FPGA的深度学习加速。QFX可以自动学习二进制点位置，并引入了无乘法的量化策略以最小化硬件资源的使用。

    

    量化是在资源受限设备（如嵌入式FPGA）上部署深度学习模型的重要技术。先前的工作主要关注于对矩阵乘法进行量化，而将BatchNorm或shortcut等其他层保留为浮点形式，尽管在FPGA上固定点算术更高效。一种常见的做法是对预训练模型进行微调以进行FPGA部署，但可能会降低准确性。本文提出了一种新颖的可训练固定点量化方法QFX，该方法在模型训练过程中自动学习二进制点位置。此外，我们引入了一种无乘法的量化策略，以最小化DSP的使用。QFX是基于PyTorch的库，可以在反向传播过程中以可微分的方式有效地模拟固定点算术，并得到FPGA HLS支持。通过使用QFX训练的模型，可以轻松地通过HLS部署，并产生与浮点模型相同的数值结果。

    Quantization is a crucial technique for deploying deep learning models on resource-constrained devices, such as embedded FPGAs. Prior efforts mostly focus on quantizing matrix multiplications, leaving other layers like BatchNorm or shortcuts in floating-point form, even though fixed-point arithmetic is more efficient on FPGAs. A common practice is to fine-tune a pre-trained model to fixed-point for FPGA deployment, but potentially degrading accuracy.   This work presents QFX, a novel trainable fixed-point quantization approach that automatically learns the binary-point position during model training. Additionally, we introduce a multiplier-free quantization strategy within QFX to minimize DSP usage. QFX is implemented as a PyTorch-based library that efficiently emulates fixed-point arithmetic, supported by FPGA HLS, in a differentiable manner during backpropagation. With minimal effort, models trained with QFX can readily be deployed through HLS, producing the same numerical results as
    
[^58]: 数据有效学习：一项综合医学基准研究

    Data-Effective Learning: A Comprehensive Medical Benchmark

    [https://arxiv.org/abs/2401.17542](https://arxiv.org/abs/2401.17542)

    这项研究引入了一个综合基准，用于评估医学领域的数据有效学习。该基准包括大量的医疗数据样本、基准方法和新的评估指标，能够准确评估数据有效学习的性能。

    

    数据有效学习旨在以最有效的方式利用数据来训练AI模型，其涉及关注数据质量而非数量的策略，确保用于训练的数据具有高信息价值。数据有效学习在加快AI训练、减少计算成本和节省数据存储方面发挥着重要作用，这在近年来医学数据的数量超出了许多人的预期时尤为重要。然而，由于缺乏标准和综合的基准研究，医学领域的数据有效学习研究还不够深入。为了填补这一空白，本文引入了一个专门用于评估医学领域数据有效学习的综合基准。该基准包括来自31个医疗中心数百万个数据样本的数据集(DataDEL)，用于比较的基准方法(MedDEL)，以及一个用于客观衡量数据有效学习性能的新评估指标(NormDEL)。我们进行了广泛的实证实验和比较，证明了我们的基准在评估数据有效学习方面的有效性和适用性。

    Data-effective learning aims to use data in the most impactful way to train AI models, which involves strategies that focus on data quality rather than quantity, ensuring the data used for training has high informational value. Data-effective learning plays a profound role in accelerating AI training, reducing computational costs, and saving data storage, which is very important as the volume of medical data in recent years has grown beyond many people's expectations. However, due to the lack of standards and comprehensive benchmark, research on medical data-effective learning is poorly studied. To address this gap, our paper introduces a comprehensive benchmark specifically for evaluating data-effective learning in the medical field. This benchmark includes a dataset with millions of data samples from 31 medical centers (DataDEL), a baseline method for comparison (MedDEL), and a new evaluation metric (NormDEL) to objectively measure data-effective learning performance. Our extensive e
    
[^59]: 透过校准的视角理解不变风险最小化的变体

    Towards Understanding Variants of Invariant Risk Minimization through the Lens of Calibration

    [https://arxiv.org/abs/2401.17541](https://arxiv.org/abs/2401.17541)

    本研究通过比较分析使用不同近似方法的不变风险最小化（IRM）技术，以期望校准误差（ECE）作为关键指标，观察到基于信息瓶颈的IRM在压缩关键特征上表现最佳。

    

    传统的机器学习模型假设训练和测试数据是独立且同分布的。然而，在现实世界的应用中，测试分布往往与训练不同。这个问题被称为越域泛化，在常规模型面临挑战。不变风险最小化（IRM）作为一个解决方案出现，旨在识别在不同环境中保持不变的特征，以增强越域鲁棒性。然而，IRM的复杂性，特别是其双层优化，导致了各种近似方法的开发。我们的研究调查了这些近似IRM技术，使用期望校准误差（ECE）作为关键指标。ECE可以衡量模型预测的可靠性，它是衡量模型是否有效捕捉到环境不变特征的指标。通过对具有分布变化的数据集进行比较分析，我们观察到基于信息瓶颈的IRM在压缩了...（接下部分摘要超过200字，提取前200字）

    Machine learning models traditionally assume that training and test data are independently and identically distributed. However, in real-world applications, the test distribution often differs from training. This problem, known as out-of-distribution generalization, challenges conventional models. Invariant Risk Minimization (IRM) emerges as a solution, aiming to identify features invariant across different environments to enhance out-of-distribution robustness. However, IRM's complexity, particularly its bi-level optimization, has led to the development of various approximate methods. Our study investigates these approximate IRM techniques, employing the Expected Calibration Error (ECE) as a key metric. ECE, which measures the reliability of model prediction, serves as an indicator of whether models effectively capture environment-invariant features. Through a comparative analysis of datasets with distributional shifts, we observe that Information Bottleneck-based IRM, which condenses
    
[^60]: 用集成方法增强基于分数的抽样方法

    Enhancing Score-Based Sampling Methods with Ensembles

    [https://arxiv.org/abs/2401.17539](https://arxiv.org/abs/2401.17539)

    这项研究通过引入集成方法来增强基于分数的抽样方法，成功开发了一种利用粒子集合动态计算近似反扩散漂移的无梯度抽样技术，并在多个示例中展示了其有效性，包括对复杂概率分布的建模和在地球物理科学中的应用。

    

    我们在基于分数的抽样方法中引入了集成方法，以开发利用粒子集合动态计算近似反扩散漂移的无梯度抽样技术。我们介绍了底层方法，并强调它与生成扩散模型和先前介绍的Föllmer抽样器的关系。我们通过各种示例证明了集成策略的有效性，涵盖了从低维到中等维度的抽样问题，包括多峰和高度非高斯概率分布，并与传统方法如NUTS进行比较。我们的发现突出了在梯度不可用的情况下，集成策略在建模复杂概率分布方面的潜力。最后，我们展示了在地球物理科学的贝叶斯反演问题中的应用。

    We introduce ensembles within score-based sampling methods to develop gradient-free approximate sampling techniques that leverage the collective dynamics of particle ensembles to compute approximate reverse diffusion drifts. We introduce the underlying methodology, emphasizing its relationship with generative diffusion models and the previously introduced F\"ollmer sampler. We demonstrate the efficacy of ensemble strategies through various examples, ranging from low- to medium-dimensionality sampling problems, including multi-modal and highly non-Gaussian probability distributions, and provide comparisons to traditional methods like NUTS. Our findings highlight the potential of ensemble strategies for modeling complex probability distributions in situations where gradients are unavailable. Finally, we showcase its application in the context of Bayesian inversion problems within the geophysical sciences.
    
[^61]: 游戏论域不可学习示例生成器

    Game-Theoretic Unlearnable Example Generator

    [https://arxiv.org/abs/2401.17523](https://arxiv.org/abs/2401.17523)

    本论文研究了从游戏论域的角度来进行不可学习示例攻击的方法。研究发现，博弈均衡给出了最强大的毒攻击，并提出了一种名为游戏论域不可学习示例（GUE）的新攻击方法。

    

    不可学习示例攻击是一种数据毒化攻击，旨在通过向训练样本添加微不可察觉的扰动，降低深度学习的干净测试准确性，这可以被定义为一个二层优化问题。然而，直接解决这个优化问题对于深度神经网络来说是难以处理的。在本文中，我们从博弈论的角度对不可学习示例攻击进行了研究，将攻击形式化为一个非零和Stackelberg博弈。首先，在正常设置和对抗训练设置下证明了博弈均衡的存在性。结果表明，在使用特定损失函数时，博弈均衡给出了最强大的毒攻击，即在相同假设空间内，受害者的测试准确率最低。其次，我们提出了一种新的攻击方法，称为游戏论域不可学习示例（GUE），它主要包括三个梯度。（1）通过直接求解均衡获得毒攻击。

    Unlearnable example attacks are data poisoning attacks aiming to degrade the clean test accuracy of deep learning by adding imperceptible perturbations to the training samples, which can be formulated as a bi-level optimization problem. However, directly solving this optimization problem is intractable for deep neural networks. In this paper, we investigate unlearnable example attacks from a game-theoretic perspective, by formulating the attack as a nonzero sum Stackelberg game. First, the existence of game equilibria is proved under the normal setting and the adversarial training setting. It is shown that the game equilibrium gives the most powerful poison attack in that the victim has the lowest test accuracy among all networks within the same hypothesis space, when certain loss functions are used. Second, we propose a novel attack method, called the Game Unlearnable Example (GUE), which has three main gradients. (1) The poisons are obtained by directly solving the equilibrium of the
    
[^62]: 大型语言模型中的时间箭头

    Arrows of Time for Large Language Models

    [https://arxiv.org/abs/2401.17505](https://arxiv.org/abs/2401.17505)

    这篇论文通过研究自回归大型语言模型的时间方向性，发现了模型在建模自然语言能力上存在时间上的不对称性。从信息理论的角度来看，这种差异理论上是不应该存在的。通过稀疏性和计算复杂性的考虑，提供了一个理论框架来解释这种不对称性的出现。

    

    我们通过时间方向性的视角研究了自回归大型语言模型的概率建模。我们在实证上发现这类模型在建模自然语言能力上存在时间上的不对称性：预测下一个记号和预测前一个记号时的平均对数困惑度存在差异。这种差异既微妙又在不同的模态（语言、模型大小、训练时间等）下非常一致。从信息理论的角度来看，这在理论上是令人惊讶的，不应该存在这样的差异。我们提供了一个理论框架，解释了这种不对称性如何出现在稀疏性和计算复杂性考虑中，并概述了我们的结果带来的一些展望。

    We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
    
[^63]: CaMU：深度模型“遗忘”的因果效应解析

    CaMU: Disentangling Causal Effects in Deep Model Unlearning

    [https://arxiv.org/abs/2401.17504](https://arxiv.org/abs/2401.17504)

    本研究对深度模型的“遗忘”进行了因果分析，并提出了一种新的方法来解决移除已遗忘数据时对剩余数据信息损失的问题。

    

    机器“遗忘”需要在消除已遗忘数据的信息的同时，保留剩余数据的必要信息。尽管该领域近年来取得了一些进展，但现有的方法主要关注消除已遗忘数据的效果，而没有考虑到这可能对剩余数据的信息造成的负面影响，导致数据移除后显著性能下降。虽然一些方法尝试在移除后修复剩余数据的性能，但被遗忘的信息也可能在修复后再次出现。这个问题是由于“遗忘”和剩余数据的错综复杂的相互交织造成的。在充分区分这两类数据对模型的影响之前，现有的算法冒着要么不能充分消除已遗忘数据的风险，要么损失剩余数据的宝贵信息的风险。为了解决这个不足，本研究对“遗忘”进行了因果分析，并引入了一种新的方法。

    Machine unlearning requires removing the information of forgetting data while keeping the necessary information of remaining data. Despite recent advancements in this area, existing methodologies mainly focus on the effect of removing forgetting data without considering the negative impact this can have on the information of the remaining data, resulting in significant performance degradation after data removal. Although some methods try to repair the performance of remaining data after removal, the forgotten information can also return after repair. Such an issue is due to the intricate intertwining of the forgetting and remaining data. Without adequately differentiating the influence of these two kinds of data on the model, existing algorithms take the risk of either inadequate removal of the forgetting data or unnecessary loss of valuable information from the remaining data. To address this shortcoming, the present study undertakes a causal analysis of the unlearning and introduces 
    
[^64]: 像素到高程：使用图像学习预测自主越野导航中的长距离高程地图

    Pixel to Elevation: Learning to Predict Elevation Maps at Long Range using Images for Autonomous Offroad Navigation

    [https://arxiv.org/abs/2401.17484](https://arxiv.org/abs/2401.17484)

    本研究提出了一种学习方法，可以通过车载视角图像实时预测长距离地形高程地图。该方法包括transformer-based编码器、方向感知的位置编码和历史增强的可学习地图嵌入。通过学习视角图像与鸟瞰图高程地图之间的关联，结合车辆姿态信息和视觉图像特征，实现更好的地图预测时序一致性。

    

    在离线机器人任务中，长距离理解地形拓扑对于成功至关重要，特别是在高速导航时。目前，几何映射主要依赖于LiDAR传感器，但在更远距离的映射时提供的测量数较少。为了解决这个挑战，我们提出了一种新颖的基于学习的方法，能够仅使用实时车载视角图像预测长距离地形高程地图。我们的方法由三个主要元素组成。首先，引入了一个基于transformer的编码器，该编码器学习车载视角图像与先前的鸟瞰图高程地图预测之间的跨视图关联。其次，提出了一种方向感知的位置编码，将3D车辆姿态信息与多视角视觉图像特征相结合，用于处理复杂的非结构化地形。最后，提出了一种历史增强的可学习地图嵌入，以实现高程地图预测之间的更好时序一致性。

    Understanding terrain topology at long-range is crucial for the success of off-road robotic missions, especially when navigating at high-speeds. LiDAR sensors, which are currently heavily relied upon for geometric mapping, provide sparse measurements when mapping at greater distances. To address this challenge, we present a novel learning-based approach capable of predicting terrain elevation maps at long-range using only onboard egocentric images in real-time. Our proposed method is comprised of three main elements. First, a transformer-based encoder is introduced that learns cross-view associations between the egocentric views and prior bird-eye-view elevation map predictions. Second, an orientation-aware positional encoding is proposed to incorporate the 3D vehicle pose information over complex unstructured terrain with multi-view visual image features. Lastly, a history-augmented learn-able map embedding is proposed to achieve better temporal consistency between elevation map predi
    
[^65]: 在社交媒体上检测心理障碍：基于ChatGPT的可解释方法

    Detecting mental disorder on social media: a ChatGPT-augmented explainable approach

    [https://arxiv.org/abs/2401.17477](https://arxiv.org/abs/2401.17477)

    本文提出了一种利用大型语言模型，可解释人工智能和对话代理器ChatGPT相结合的新方法，以解决通过社交媒体检测抑郁症的可解释性挑战。通过将Twitter特定变体BERTweet与自解释模型BERT-XDD相结合，并借助ChatGPT将技术解释转化为人类可读的评论，实现了解释能力的同时提高了可解释性。这种方法可以为发展社会负责任的数字平台，促进早期干预做出贡献。

    

    在数字时代，社交媒体上表达的抑郁症状的频率引起了严重关注，迫切需要先进的方法来及时检测。本文通过提出一种新颖的方法，将大型语言模型（LLM）与可解释的人工智能（XAI）和ChatGPT等对话代理器有效地结合起来，以应对可解释性抑郁症检测的挑战。在我们的方法中，通过将Twitter特定变体BERTweet与一种新型的自解释模型BERT-XDD相结合，实现了解释能力，该模型能够通过掩码注意力提供分类和解释。使用ChatGPT将技术解释转化为可读性强的评论，进一步增强了可解释性。通过引入一种有效且模块化的可解释抑郁症检测方法，我们的方法可以为发展社会负责任的数字平台做出贡献，促进早期干预。

    In the digital era, the prevalence of depressive symptoms expressed on social media has raised serious concerns, necessitating advanced methodologies for timely detection. This paper addresses the challenge of interpretable depression detection by proposing a novel methodology that effectively combines Large Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and conversational agents like ChatGPT. In our methodology, explanations are achieved by integrating BERTweet, a Twitter-specific variant of BERT, into a novel self-explanatory model, namely BERT-XDD, capable of providing both classification and explanations via masked attention. The interpretability is further enhanced using ChatGPT to transform technical explanations into human-readable commentaries. By introducing an effective and modular approach for interpretable depression detection, our methodology can contribute to the development of socially responsible digital platforms, fostering early intervention and
    
[^66]: 使无线环境对梯度估计器有用：一种零阶随机联邦学习方法

    Rendering Wireless Environments Useful for Gradient Estimators: A Zero-Order Stochastic Federated Learning Method

    [https://arxiv.org/abs/2401.17460](https://arxiv.org/abs/2401.17460)

    提出了一种新颖的零阶随机联邦学习方法，通过利用无线通信通道的特性，在学习算法中考虑了无线通道，避免了资源的浪费和分析难度。

    

    联邦学习（FL）是一种新颖的机器学习方法，允许多个边缘设备协同训练模型，而无需公开原始数据。然而，当设备和服务器通过无线信道通信时，该方法面临着通信和计算瓶颈。通过利用一个通信高效的框架，我们提出了一种新颖的零阶（ZO）方法，采用一点梯度估计器，利用无线通信通道的特性，而无需知道通道状态系数。这是第一种将无线通道包含在学习算法本身中的方法，而不是浪费资源来分析和消除其影响。这项工作的两个主要困难是，在FL中，目标函数通常不是凸的，这使得将FL扩展到ZO方法具有挑战性，以及包括影响的难度。

    Federated learning (FL) is a novel approach to machine learning that allows multiple edge devices to collaboratively train a model without disclosing their raw data. However, several challenges hinder the practical implementation of this approach, especially when devices and the server communicate over wireless channels, as it suffers from communication and computation bottlenecks in this case. By utilizing a communication-efficient framework, we propose a novel zero-order (ZO) method with a one-point gradient estimator that harnesses the nature of the wireless communication channel without requiring the knowledge of the channel state coefficient. It is the first method that includes the wireless channel in the learning algorithm itself instead of wasting resources to analyze it and remove its impact. The two main difficulties of this work are that in FL, the objective function is usually not convex, which makes the extension of FL to ZO methods challenging, and that including the impa
    
[^67]: 低成本集合剪枝的液态民主

    Liquid Democracy for Low-Cost Ensemble Pruning

    [https://arxiv.org/abs/2401.17443](https://arxiv.org/abs/2401.17443)

    本文介绍了一种利用液态民主实现低成本集合剪枝的方法。通过液态民主的委派机制识别和移除冗余分类器，成功降低了集合训练的计算成本，并比某些增强方法具有更高的准确性。同时，本文还展示了计算社会选择文献框架在非传统领域问题中的应用。

    

    我们认为，集合学习和一种代理投票模式——液态民主之间存在着强烈的联系，可以通过液态民主来降低集合训练的成本。我们提出了一种增量训练的过程，通过液态民主的启发，通过委派机制来识别和移除集合中的冗余分类器。通过分析和大量实验，我们展示了这个过程大大降低了训练的计算成本，相比于训练一个完整的集合。通过精选底层的委派机制，避免了分类器群体的权重集中，提高了准确性，而且，这项工作也展示了计算社会选择文献中的框架如何应用于非传统领域的问题。

    We argue that there is a strong connection between ensemble learning and a delegative voting paradigm -- liquid democracy -- that can be leveraged to reduce ensemble training costs. We present an incremental training procedure that identifies and removes redundant classifiers from an ensemble via delegation mechanisms inspired by liquid democracy. Through both analysis and extensive experiments we show that this process greatly reduces the computational cost of training compared to training a full ensemble. By carefully selecting the underlying delegation mechanism, weight centralization in the classifier population is avoided, leading to higher accuracy than some boosting methods. Furthermore, this work serves as an exemplar of how frameworks from computational social choice literature can be applied to problems in nontraditional domains.
    
[^68]: 揭示二阶影响来解释预测不确定性

    Explaining Predictive Uncertainty by Exposing Second-Order Effects

    [https://arxiv.org/abs/2401.17441](https://arxiv.org/abs/2401.17441)

    该论文研究发现，预测不确定性主要受到单个特征或特征之间乘积相互作用的二阶影响的影响。作者提出了一种基于这些二阶影响来解释预测不确定性的新方法。该方法通过简单的协方差计算对一阶解释进行处理，可以将常见的归因技术转化为强大的二阶不确定性解释器。作者通过量化评估验证了该方法解释的准确性，并展示了整体实用性。

    

    可解释的人工智能（Explainable AI）使复杂的机器学习黑箱变得透明，特别是可以确定模型用来进行预测的特征。然而，关于解释预测不确定性，即为什么模型“不确定”，目前研究较少。我们的研究发现，预测不确定性主要由涉及单个特征或特征之间的乘积相互作用的二阶影响所主导。我们提出了一种基于这些二阶影响来解释预测不确定性的新方法。在计算上，我们的方法简化成对一组一阶解释进行简单协方差计算。我们的方法具有普遍适用性，可以将常见的归因技术（LRP，Gradient x Input等）转化为强大的二阶不确定性解释器，称为CovLRP，CovGI等。我们通过系统量化评估验证了我们方法产生解释的准确性，展示了我们方法的整体实用性。

    Explainable AI has brought transparency into complex ML blackboxes, enabling, in particular, to identify which features these models use for their predictions. So far, the question of explaining predictive uncertainty, i.e. why a model 'doubts', has been scarcely studied. Our investigation reveals that predictive uncertainty is dominated by second-order effects, involving single features or product interactions between them. We contribute a new method for explaining predictive uncertainty based on these second-order effects. Computationally, our method reduces to a simple covariance computation over a collection of first-order explanations. Our method is generally applicable, allowing for turning common attribution techniques (LRP, Gradient x Input, etc.) into powerful second-order uncertainty explainers, which we call CovLRP, CovGI, etc. The accuracy of the explanations our method produces is demonstrated through systematic quantitative evaluations, and the overall usefulness of our m
    
[^69]: 大型语言模型能否取代经济选择预测实验室？

    Can Large Language Models Replace Economic Choice Prediction Labs?

    [https://arxiv.org/abs/2401.17435](https://arxiv.org/abs/2401.17435)

    该论文研究大型语言模型是否能够取代经济实验室进行选择预测，并通过相关实验证明了其可行性。

    

    经济选择预测是一项具有挑战性的重要任务，往往受限于获取人类选择数据的困难。实验经济学研究在很大程度上专注于简单的选择环境。最近，人工智能界以两种方式为该努力做出了贡献：考虑大型语言模型是否可以代替人类在上述简单选择预测环境中，以及通过机器学习视角研究更复杂但仍严格的实验经济学环境，包括不完全信息、重复博弈和基于自然语言交流的说服游戏。这引发了一个重要的灵感：大型语言模型是否能够完全模拟经济环境，并生成用于高效人类选择预测的数据，替代复杂的经济实验室研究？我们在这个主题上开创了研究，并展示了其可行性。特别是，我们表明仅在大型语言模型生成的数据上训练的模型可以有效地进行预测。

    Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
    
[^70]: 多头注意力在上下文线性回归中的优势

    Superiority of Multi-Head Attention in In-Context Linear Regression

    [https://arxiv.org/abs/2401.17426](https://arxiv.org/abs/2401.17426)

    多头注意力在上下文线性回归任务中表现出优于单头注意力的性能，通过理论分析证明了多头注意力在大嵌入维度情况下有更小的预测损失，并且在各种数据分布设置下都显示出优势。

    

    我们通过理论分析在线性回归任务的上下文学习中，使用softmax注意力的transformer的性能。与现有文献主要关注单头/多头注意力的收敛性不同，我们的研究着重比较它们的性能。我们进行了精确的理论分析，证明了具有较大嵌入维度的多头注意力比单头注意力表现更好。当上下文示例数量D增加时，使用单头/多头注意力的预测损失为O(1/D)，而多头注意力的乘法常数较小。除了最简单的数据分布设置，我们考虑了更多情景，例如噪声标签，局部示例，相关特征和先验知识。我们观察到，总的来说，多头注意力优于单头注意力。我们的结果验证了多头注意力设计的有效性。

    We present a theoretical analysis of the performance of transformer with softmax attention in in-context learning with linear regression tasks. While the existing literature predominantly focuses on the convergence of transformers with single-/multi-head attention, our research centers on comparing their performance. We conduct an exact theoretical analysis to demonstrate that multi-head attention with a substantial embedding dimension performs better than single-head attention. When the number of in-context examples D increases, the prediction loss using single-/multi-head attention is in O(1/D), and the one for multi-head attention has a smaller multiplicative constant. In addition to the simplest data distribution setting, we consider more scenarios, e.g., noisy labels, local examples, correlated features, and prior knowledge. We observe that, in general, multi-head attention is preferred over single-head attention. Our results verify the effectiveness of the design of multi-head at
    
[^71]: 应用神经网络重建快速中微子换 flavor 后的能谱

    Application of Neural Networks for the Reconstruction of Supernova Neutrino Energy Spectra Following Fast Neutrino Flavor Conversions

    [https://arxiv.org/abs/2401.17424](https://arxiv.org/abs/2401.17424)

    本研究利用物理信息驱动的神经网络（PINNs），基于多能量中微子气体中中微子角分布的前两个矩，预测了快速 flavor 转换的结果，取得了较低的预测误差。

    

    中微子在极密度的天体环境中，如核心坍缩超新星和中子星合并中，可以进行快速的 flavor 转换。本研究探索了多能量中微子气体中的快速 flavor 转换，发现当快速 flavor 转换的增长率显著超过真空哈密顿量的增长率时，所有中微子（无论其能量如何）都共享一个由能量积分中微子谱决定的存活概率。然后，我们使用物理信息驱动的神经网络（PINNs）来预测在这种多能量中微子气体中快速 flavor 转换的渐近结果。这些预测基于每个能量 bin 的中微子角分布的前两个矩，通常可以从最先进的超新星和中子星模拟中获得。我们的 PINNs 对于预测电子通道中的中微子数量和中微子矩的相对绝对误差分别达到不到6％和不到18％的误差。

    Neutrinos can undergo fast flavor conversions (FFCs) within extremely dense astrophysical environments such as core-collapse supernovae (CCSNe) and neutron star mergers (NSMs). In this study, we explore FFCs in a \emph{multi-energy} neutrino gas, revealing that when the FFC growth rate significantly exceeds that of the vacuum Hamiltonian, all neutrinos (regardless of energy) share a common survival probability dictated by the energy-integrated neutrino spectrum. We then employ physics-informed neural networks (PINNs) to predict the asymptotic outcomes of FFCs within such a multi-energy neutrino gas. These predictions are based on the first two moments of neutrino angular distributions for each energy bin, typically available in state-of-the-art CCSN and NSM simulations. Our PINNs achieve errors as low as $\lesssim6\%$ and $\lesssim 18\%$ for predicting the number of neutrinos in the electron channel and the relative absolute error in the neutrino moments, respectively.
    
[^72]: 基于WiFi信道状态信息的穿墙成像

    Through-Wall Imaging based on WiFi Channel State Information

    [https://arxiv.org/abs/2401.17417](https://arxiv.org/abs/2401.17417)

    本研究提出了一种通过WiFi信道状态信息实现穿墙成像的创新方法，可以将室内环境可视化监测到房间边界之外，无需摄像机，具有广泛的实际应用潜力。

    

    本研究提出了一种创新的方法，通过WiFi信道状态信息（CSI）在穿墙场景中合成图像。利用WiFi的优势，如成本效益，光照不变性和穿墙能力，我们的方法实现了对室内环境的可视化监测，越过房间边界，无需摄像机。更一般地，它通过解锁执行基于图像的下游任务（例如，视觉活动识别）的选项，提高了WiFi CSI的可解释性。为了实现从WiFi CSI到图像的跨模态转换，我们依赖于一个适应我们问题特定的多模态变分自编码器（VAE）。我们通过架构配置的剔除研究和重建图像的定量/定性评估对我们提出的方法进行了广泛评估。我们的结果证明了我们方法的可行性，并突显了其在实际应用中的潜力。

    This work presents a seminal approach for synthesizing images from WiFi Channel State Information (CSI) in through-wall scenarios. Leveraging the strengths of WiFi, such as cost-effectiveness, illumination invariance, and wall-penetrating capabilities, our approach enables visual monitoring of indoor environments beyond room boundaries and without the need for cameras. More generally, it improves the interpretability of WiFi CSI by unlocking the option to perform image-based downstream tasks, e.g., visual activity recognition. In order to achieve this crossmodal translation from WiFi CSI to images, we rely on a multimodal Variational Autoencoder (VAE) adapted to our problem specifics. We extensively evaluate our proposed methodology through an ablation study on architecture configuration and a quantitative/qualitative assessment of reconstructed images. Our results demonstrate the viability of our method and highlight its potential for practical applications.
    
[^73]: 用深度学习解决玻尔兹曼优化问题

    Solving Boltzmann Optimization Problems with Deep Learning

    [https://arxiv.org/abs/2401.17408](https://arxiv.org/abs/2401.17408)

    本文的贡献是一种新颖的机器学习方法来解决基于伊辛硬件的优化问题。

    

    在高性能计算（HPC）效率的指数级扩展几十年后，即将结束。基于晶体管的互补金属氧化物半导体（CMOS）技术已接近物理极限，进一步微型化将变得不可能。未来的HPC效率提升将必然依赖于新技术和计算范式。伊辛模型显示出作为高能效计算未来框架的特殊潜力。伊辛系统能够在接近热力学极限的能量消耗下运行计算。伊辛系统可以同时充当逻辑和存储器。因此，通过消除昂贵的数据移动，它们有可能显著降低 CMOS 计算所固有的能源成本。创建基于伊辛的硬件的挑战在于优化能够在根本上不确定的硬件上产生正确结果的有用电路。本文的贡献是一种新颖的机器学习方法，一种综合了经典计算和深度学习的方法，用于解决Ising硬件的优化问题。

    Decades of exponential scaling in high performance computing (HPC) efficiency is coming to an end. Transistor based logic in complementary metal-oxide semiconductor (CMOS) technology is approaching physical limits beyond which further miniaturization will be impossible. Future HPC efficiency gains will necessarily rely on new technologies and paradigms of compute. The Ising model shows particular promise as a future framework for highly energy efficient computation. Ising systems are able to operate at energies approaching thermodynamic limits for energy consumption of computation. Ising systems can function as both logic and memory. Thus, they have the potential to significantly reduce energy costs inherent to CMOS computing by eliminating costly data movement. The challenge in creating Ising-based hardware is in optimizing useful circuits that produce correct results on fundamentally nondeterministic hardware. The contribution of this paper is a novel machine learning approach, a com
    
[^74]: 连续学习的步长优化

    Step-size Optimization for Continual Learning

    [https://arxiv.org/abs/2401.17401](https://arxiv.org/abs/2401.17401)

    本文研究了连续学习中的步长优化问题，指出传统算法忽视了对整体目标函数的影响，而随机元梯度下降算法能够明确优化步长向量，在简单问题中表现更优。

    

    在连续学习中，学习者需要在整个生命周期内不断学习数据。一个关键问题是决定要保留什么知识和放弃什么知识。在神经网络中，可以通过使用步长向量来缩放梯度样本对网络权重的改变程度来实现。常见的算法，如RMSProp和Adam，使用启发式方法，特别是标准化，来适应这个步长向量。在本文中，我们展示了这些启发式方法忽视了它们对整体目标函数的适应效果，例如将步长向量远离更好的步长向量。另一方面，像IDBD（Sutton，1992）这样的随机元梯度下降算法，明确地针对整体目标函数优化步长向量。在简单问题上，我们展示了IDBD能够持续改善步长向量，而RMSProp和Adam则不行。我们解释了这两种方法以及它们各自的局限性之间的差异。

    In continual learning, a learner has to keep learning from the data over its whole life time. A key issue is to decide what knowledge to keep and what knowledge to let go. In a neural network, this can be implemented by using a step-size vector to scale how much gradient samples change network weights. Common algorithms, like RMSProp and Adam, use heuristics, specifically normalization, to adapt this step-size vector. In this paper, we show that those heuristics ignore the effect of their adaptation on the overall objective function, for example by moving the step-size vector away from better step-size vectors. On the other hand, stochastic meta-gradient descent algorithms, like IDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to the overall objective function. On simple problems, we show that IDBD is able to consistently improve step-size vectors, where RMSProp and Adam do not. We explain the differences between the two approaches and their respective limitat
    
[^75]: 通过深度黑石贝莱曼模型优化时间序列供应商分配风险

    Timeseries Suppliers Allocation Risk Optimization via Deep Black Litterman Model

    [https://arxiv.org/abs/2401.17350](https://arxiv.org/abs/2401.17350)

    通过深度黑石贝莱曼模型和时空图神经网络，我们优化了供应商选择和订单分配，同时解决了零阶情况下的可信度问题，实现了准确的预测和精确的置信区间。

    

    我们介绍了BL模型和Perspective矩阵，以优化供应商选择和订单分配，重点关注时间和空间动态。我们使用时空图神经网络开发了供应商关系网络，增强了对复杂供应商相互依赖关系的理解。此外，我们还通过Masked Ranking机制解决了零阶情况下的可信度问题，提高了供应商排序效率。与传统模型相比，我们的模型在两个数据集上展现了优越的结果。我们使用真实数据集进行的评估突出了DBLM在提供准确预测和精确置信区间方面的优势，特别是在高分辨率情景下。

    We introduce the BL model and the Perspective Matrix to optimize supplier selection and order allocation, focusing on both temporal and spatial dynamics. Our development of a Supplier Relationship Network, using a Spatio-Temporal Graph Neural Network, enhances the understanding of complex supplier interdependencies. Additionally, we address credibility issues in zero-order scenarios with a Masked Ranking Mechanism, improving supplier ranking efficiency. Our model demonstrates superior results on two datasets compared to the traditional models. Our evaluations using real-world datasets highlight DBLM's superiority in providing accurate predictions and precise confidence intervals, particularly in high-resolution scenarios.
    
[^76]: 机器学习中伪随机数生成器的可重复性、能效和性能：对Python、NumPy、TensorFlow和PyTorch实现的比较研究

    Reproducibility, energy efficiency and performance of pseudorandom number generators in machine learning: a comparative study of python, numpy, tensorflow, and pytorch implementations

    [https://arxiv.org/abs/2401.17345](https://arxiv.org/abs/2401.17345)

    本研究比较了机器学习中常用的伪随机数生成器在统计质量、数值可重复性、时间效率和能源消耗等方面与原始C实现的差异。

    

    伪随机数生成器(PRNGs)在机器学习技术中已经无处不在，因为它们在众多方法中都非常有意思。机器学习领域有着巨大的潜力，在各个领域中取得突破性进展，比如最近在大型语言模型(LLMs)中的突破。然而，尽管越来越受关注，但仍然存在一些持续关注的问题，包括与可重复性和能源消耗相关的问题。可重复性对于强大的科学研究和可解释性至关重要，而能效则强调了保护有限全球资源的必要性。本研究深入探讨了机器学习语言、库和框架中最主要的伪随机数生成器(PRNGs)与各自算法原始C实现相比，在统计质量和数值可重复性方面的表现。此外，我们还旨在评估时间效率和能源消耗。

    Pseudo-Random Number Generators (PRNGs) have become ubiquitous in machine learning technologies because they are interesting for numerous methods. The field of machine learning holds the potential for substantial advancements across various domains, as exemplified by recent breakthroughs in Large Language Models (LLMs). However, despite the growing interest, persistent concerns include issues related to reproducibility and energy consumption. Reproducibility is crucial for robust scientific inquiry and explainability, while energy efficiency underscores the imperative to conserve finite global resources. This study delves into the investigation of whether the leading Pseudo-Random Number Generators (PRNGs) employed in machine learning languages, libraries, and frameworks uphold statistical quality and numerical reproducibility when compared to the original C implementation of the respective PRNG algorithms. Additionally, we aim to evaluate the time efficiency and energy consumption of 
    
[^77]: 提高地球观测数据预测置信度的潜在空间度量

    A Latent Space Metric for Enhancing Prediction Confidence in Earth Observation Data

    [https://arxiv.org/abs/2401.17342](https://arxiv.org/abs/2401.17342)

    这项研究提出了一种新的方法来估计利用地球观测数据进行回归任务时机器学习模型预测的置信度。通过利用潜在空间表示来推导置信度度量，建立了潜在表示中的欧几里得距离与个体蚊子种群预测的绝对误差之间的相关性。该方法在意大利威尼托地区和德国上莱茵河谷的地区得到了验证，并表现出较高的可靠性和可信度。

    

    本研究提出了一种新的方法，用于估计机器学习模型预测的置信度，特别是在利用地球观测数据进行回归任务时，重点关注蚊子种群（MA）估计。我们利用变分自动编码器的架构，通过EO数据的潜在空间表示来推导置信度度量。这种方法对于建立潜在表示中的欧几里得距离与个体MA预测的绝对误差（AE）之间的相关性至关重要。我们的研究重点关注了意大利威尼托地区和德国上莱茵河谷的EO数据集，这些地区受蚊子种群的影响显著。一个重要的发现是MA预测的AE与所提出的置信度度量之间存在0.46的显著相关性。这个相关性意味着这是一个稳健的、新的度量方法，用于量化AI模型在该背景下的预测可靠性和提高可信度。

    This study presents a new approach for estimating confidence in machine learning model predictions, specifically in regression tasks utilizing Earth Observation (EO) data, with a particular focus on mosquito abundance (MA) estimation. We take advantage of a Variational AutoEncoder architecture, to derive a confidence metric by the latent space representations of EO datasets. This methodology is pivotal in establishing a correlation between the Euclidean distance in latent representations and the Absolute Error (AE) in individual MA predictions. Our research focuses on EO datasets from the Veneto region in Italy and the Upper Rhine Valley in Germany, targeting areas significantly affected by mosquito populations. A key finding is a notable correlation of 0.46 between the AE of MA predictions and the proposed confidence metric. This correlation signifies a robust, new metric for quantifying the reliability and enhancing the trustworthiness of the AI model's predictions in the context of 
    
[^78]: 去中心化联邦学习：安全与隐私综述

    Decentralized Federated Learning: A Survey on Security and Privacy

    [https://arxiv.org/abs/2401.17319](https://arxiv.org/abs/2401.17319)

    去中心化联邦学习架构允许保护隐私，但也引入了新的安全和隐私威胁，该综述对去中心化联邦学习中的威胁、对手和防御机制进行了研究。

    

    联邦学习由于其保护隐私等优势，在近年来迅速发展并受到广泛关注。然而，在这种架构中，模型更新和梯度的交换为网络中的恶意用户提供了新的攻击面，可能危及模型性能以及用户和数据的隐私。因此，去中心化联邦学习的主要动机之一是通过去除服务器并通过区块链等技术进行补偿来消除与服务器相关的威胁。然而，这种优势却以挑战系统面临新的隐私威胁为代价。因此，对这种新范 paradigm，并进行全面的安全分析是必要的。这项调查研究了去中心化联邦学习中可能存在的威胁和对手变化，并概述了潜在的防御机制。还考虑了去中心化联邦学习的可信度和验证性。

    Federated learning has been rapidly evolving and gaining popularity in recent years due to its privacy-preserving features, among other advantages. Nevertheless, the exchange of model updates and gradients in this architecture provides new attack surfaces for malicious users of the network which may jeopardize the model performance and user and data privacy. For this reason, one of the main motivations for decentralized federated learning is to eliminate server-related threats by removing the server from the network and compensating for it through technologies such as blockchain. However, this advantage comes at the cost of challenging the system with new privacy threats. Thus, performing a thorough security analysis in this new paradigm is necessary. This survey studies possible variations of threats and adversaries in decentralized federated learning and overviews the potential defense mechanisms. Trustability and verifiability of decentralized federated learning are also considered 
    
[^79]: 多语言文本到图像生成放大了性别刻板印象，并且修正工程可能无法帮助您

    Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You

    [https://arxiv.org/abs/2401.16092](https://arxiv.org/abs/2401.16092)

    多语言文本到图像生成模型存在性别偏见；通过MAGBIG评估模型时，发现模型对不同语言具有重要差异；我们呼吁研究多语言模型领域消除性别偏见。

    

    最近，文本到图像生成模型在图像质量、灵活性和文本对齐方面取得了令人惊讶的结果，并因此在越来越多的应用中得到应用。通过改善多语言能力，更多的社群现在可以访问这种技术。然而，正如我们将展示的那样，多语言模型与单语模型一样受到(性别)偏见的困扰。此外，人们自然期望这些模型在不同语言之间提供类似的结果，但事实并非如此，不同语言之间存在重要的差异。因此，我们提出了一个旨在促进没有性别偏见的多语言模型研究的新基准MAGBIG。我们研究了多语言T2I模型是否通过MAGBIG放大了性别偏见。为此，我们使用多语言提示请求特定职业或特质的人像图像(使用形容词)。我们的结果不仅表明模型偏离了规范的假设，...

    Text-to-image generation models have recently achieved astonishing results in image quality, flexibility, and text alignment and are consequently employed in a fast-growing number of applications. Through improvements in multilingual abilities, a larger community now has access to this kind of technology. Yet, as we will show, multilingual models suffer similarly from (gender) biases as monolingual models. Furthermore, the natural expectation is that these models will provide similar results across languages, but this is not the case and there are important differences between languages. Thus, we propose a novel benchmark MAGBIG intending to foster research in multilingual models without gender bias. We investigate whether multilingual T2I models magnify gender bias with MAGBIG. To this end, we use multilingual prompts requesting portrait images of persons of a certain occupation or trait (using adjectives). Our results show not only that models deviate from the normative assumption th
    
[^80]: Baichuan2-Sum: 使用指导微调Baichuan2-7B模型进行对话摘要

    Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization

    [https://arxiv.org/abs/2401.15496](https://arxiv.org/abs/2401.15496)

    本文提出了Baichuan2-Sum模型，通过指导微调Baichuan2-7B模型进行对话摘要，并应用NEFTune技术改进训练过程。实验证明该模型在CSDS和SAMSUM数据集上取得了新的最先进结果。

    

    巨大的语言模型（LLM）如Llama、Baichuan和Bloom模型在许多自然语言任务中展现出了令人瞩目的能力。然而，对于对话摘要任务，该任务旨在为对话中的不同角色生成摘要，大多数最先进的方法都是基于小模型（例如Bart和Bert）进行的。现有方法尝试在小模型上添加任务指定的优化，如向模型添加全局-局部中心度得分。在本文中，我们提出了一种指导微调模型：Baichuan2-Sum，用于面向角色的对话摘要。通过为不同角色设置不同的指令，模型可以从对话交互中学习并输出期望的摘要。此外，我们还应用了NEFTune技术，在训练过程中添加合适的噪声以提高结果。实验证明，所提出的模型在两个公开的对话摘要数据集CSDS和SAMSUM上取得了新的最先进结果。

    Large language models (LLMs) like Llama, Baichuan and Bloom models show remarkable ability with instruction fine-tuning in many natural language tasks. Nevertheless, for the dialogue summarization task, which aims to generate summaries for different roles in dialogue, most of the state-of-the-art methods conduct on small models (e.g Bart and Bert). Existing methods try to add task specified optimization on small models like adding global-local centrality score to models. In this paper, we propose an instruction fine-tuning model: Baichuan2-Sum, for role-oriented diaglouge summarization. By setting different instructions for different roles, the model can learn from the dialogue interactions and output the expected summaries. Furthermore, we applied NEFTune technique to add suitable noise during training to improve the results. The experiments demonstrate that the proposed model achieves the new state-of-the-art results on two public dialogue summarization datasets: CSDS and SAMSUM. We 
    
[^81]: 风速超分辨率与验证：从ERA5到CERRA通过扩散模型

    Wind speed super-resolution and validation: from ERA5 to CERRA via diffusion models

    [https://arxiv.org/abs/2401.15469](https://arxiv.org/abs/2401.15469)

    本论文提出了一种利用扩散模型以数据驱动的方式近似CERRA的降尺度的方法，通过利用ERA5数据集进行风速超分辨率任务。

    

    Copernicus区域再分析数据集CERRA是一个高分辨率的欧洲区域再分析数据集。近年来，在各种与气候相关的任务中它显示出了显著的实用性，包括天气预报、气候变化研究、可再生能源预测、资源管理、空气质量风险评估以及罕见事件的预测等。不幸的是，由于获取所需外部数据和生成过程中计算量大，CERRA的可用性滞后于当前日期两年。作为解决方案，本文提出了一种新的方法，使用扩散模型以数据驱动的方式近似CERRA的降尺度，而无需额外信息。通过利用边界条件由低分辨率ERA5数据集提供的CERRA数据，我们将其视为超分辨率任务。以意大利周围的风速为重点，我们的模型在现有CERRA数据上进行训练。

    The Copernicus Regional Reanalysis for Europe, CERRA, is a high-resolution regional reanalysis dataset for the European domain. In recent years it has shown significant utility across various climate-related tasks, ranging from forecasting and climate change research to renewable energy prediction, resource management, air quality risk assessment, and the forecasting of rare events, among others. Unfortunately, the availability of CERRA is lagging two years behind the current date, due to constraints in acquiring the requisite external data and the intensive computational demands inherent in its generation. As a solution, this paper introduces a novel method using diffusion models to approximate CERRA downscaling in a data-driven manner, without additional informations. By leveraging the lower resolution ERA5 dataset, which provides boundary conditions for CERRA, we approach this as a super-resolution task. Focusing on wind speed around Italy, our model, trained on existing CERRA data,
    
[^82]: nnU-Net和DeepMedic方法在儿童脑肿瘤自动分割中的训练和比较

    Training and Comparison of nnU-Net and DeepMedic Methods for Autosegmentation of Pediatric Brain Tumors

    [https://arxiv.org/abs/2401.08404](https://arxiv.org/abs/2401.08404)

    本研究对比了nnU-Net和DeepMedic两种基于深度学习的3D分割模型，在儿童脑肿瘤自动分割中的性能。通过使用儿童特定的多机构脑肿瘤数据进行训练，得到了较高的分割准确性和敏感性。

    

    脑肿瘤是最常见的固体肿瘤，也是儿童癌症相关死亡的主要原因。在手术和治疗计划、反应评估和监测中，肿瘤分割是必不可少的。然而，手动分割耗时且操作者间的变异性高，因此需要更高效的方法。我们使用儿童特定的多机构脑肿瘤数据通过多参数MRI扫描对两种基于深度学习的3D分割模型DeepMedic和nnU-Net进行了比较。对339名儿童患者（内部组和外部组分别为293例和46例）的多参数术前MRI扫描进行预处理和手动分割，划分为增强肿瘤（ET）、非增强肿瘤（NET）、囊性组分（CC）和周围水肿（ED）四个肿瘤亚区。在训练后，通过Dice分数、敏感度等指标评估了两种模型在内部和外部测试集上的性能。

    Brain tumors are the most common solid tumors and the leading cause of cancer-related death among children. Tumor segmentation is essential in surgical and treatment planning, and response assessment and monitoring. However, manual segmentation is time-consuming and has high inter-operator variability, underscoring the need for more efficient methods. We compared two deep learning-based 3D segmentation models, DeepMedic and nnU-Net, after training with pediatric-specific multi-institutional brain tumor data using based on multi-parametric MRI scans.Multi-parametric preoperative MRI scans of 339 pediatric patients (n=293 internal and n=46 external cohorts) with a variety of tumor subtypes, were preprocessed and manually segmented into four tumor subregions, i.e., enhancing tumor (ET), non-enhancing tumor (NET), cystic components (CC), and peritumoral edema (ED). After training, performance of the two models on internal and external test sets was evaluated using Dice scores, sensitivity,
    
[^83]: 高效学习长程和等变量量子系统

    Efficient Learning of Long-Range and Equivariant Quantum Systems

    [https://arxiv.org/abs/2312.17019](https://arxiv.org/abs/2312.17019)

    本文研究了学习量子系统基态的高效方法，特别是在存在长程和等变特性的情况下。我们扩展了现有结果，使其适用于分子和原子系统中的长程相互作用，并提供了具有指数级复杂度的误差依赖性。

    

    本文考虑了量子多体物理学中的一个基本任务-找到和学习量子哈密顿量的基态及其性质。最近的研究探讨了通过学习数据来预测几何局部可观测量的基态期望值的任务。对于短程缺陷哈密顿量，得到了样本复杂度在量子位数的对数和误差的准多项式之间的关系。在这里，我们将这些结果扩展到超出哈密顿量和观测量的局部要求，这是由分子和原子系统中的长程相互作用的相关性所驱动的。对于指数大于系统维数两倍的幂律衰减相互作用，我们得到了相同的高效对数标度关于量子位数的依赖性，但误差的依赖性恶化到了指数级。此外，我们还展示了在相互作用超图的自同构群下等变的学习算法。

    In this work, we consider a fundamental task in quantum many-body physics - finding and learning ground states of quantum Hamiltonians and their properties. Recent works have studied the task of predicting the ground state expectation value of sums of geometrically local observables by learning from data. For short-range gapped Hamiltonians, a sample complexity that is logarithmic in the number of qubits and quasipolynomial in the error was obtained. Here we extend these results beyond the local requirements on both Hamiltonians and observables, motivated by the relevance of long-range interactions in molecular and atomic systems. For interactions decaying as a power law with exponent greater than twice the dimension of the system, we recover the same efficient logarithmic scaling with respect to the number of qubits, but the dependence on the error worsens to exponential. Further, we show that learning algorithms equivariant under the automorphism group of the interaction hypergraph a
    
[^84]: 计算中基于优化的界限收紧在ReLU网络中的权衡

    Computational Tradeoffs of Optimization-Based Bound Tightening in ReLU Networks

    [https://arxiv.org/abs/2312.16699](https://arxiv.org/abs/2312.16699)

    本研究探讨了在ReLU网络中使用基于优化的界限收紧的计算权衡，提供了根据网络结构、正则化和舍入的实施指南。

    

    在过去的十年中，使用混合整数线性规划（MILP）模型来表示具有修正线性单元（ReLU）激活的神经网络的使用变得越来越广泛。这使得可以使用MILP技术来测试或压力测试它们的行为，为了对它们的训练进行敌对改进，并将它们嵌入到利用它们的预测能力的优化模型中。其中许多MILP模型依赖于激活界限，即每个神经元输入值的界限。在本研究中，我们探讨了界限的紧密度与解决结果MILP模型的计算工作之间的权衡。我们提供了基于网络结构、正则化和舍入的实现这些模型的指南。

    The use of Mixed-Integer Linear Programming (MILP) models to represent neural networks with Rectified Linear Unit (ReLU) activations has become increasingly widespread in the last decade. This has enabled the use of MILP technology to test-or stress-their behavior, to adversarially improve their training, and to embed them in optimization models leveraging their predictive power. Many of these MILP models rely on activation bounds. That is, bounds on the input values of each neuron. In this work, we explore the tradeoff between the tightness of these bounds and the computational effort of solving the resulting MILP models. We provide guidelines for implementing these models based on the impact of network structure, regularization, and rounding.
    
[^85]: 使用通信成本低于18千字节的联邦全参数调整亿级语言模型

    Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes

    [https://arxiv.org/abs/2312.06353](https://arxiv.org/abs/2312.06353)

    本论文提出了一种名为FedKSeed的方法，使用零阶优化和有限的随机种子集合，实现了通信成本较低的联邦全参数调整。该方法使得在终端设备上可以进行亿级语言模型的联邦全参数调整，具有较高的性能表现。

    

    预训练的大型语言模型（LLM）需要通过细化调整来提高对自然语言指令的响应能力。联邦学习提供了一种在不牺牲数据隐私的情况下，利用终端设备上丰富的数据对LLM进行细化调整的方法。大多数现有的LLM联邦细化调整方法依赖于参数高效的细化调整技术，但可能无法达到全参数调整可能达到的性能高度。然而，由于巨大的通信成本，LLM的联邦全参数调整是一个非常困难的问题。本研究介绍了FedKSeed，它使用随机种子的有限集合进行零阶优化。它显著降低了服务器和终端之间的传输要求，仅需传输几个随机种子和标量梯度，仅占用几千字节的空间，使得在终端设备上能够进行亿级LLM的联邦全参数调整。在此基础上，我们开发了一种策略，实现了概率差异化种子采样，优先考虑一些种子，从而进一步提高了联邦全参数调整的效果。

    Pre-trained large language models (LLMs) need fine-tuning to improve their responsiveness to natural language instructions. Federated learning offers a way to fine-tune LLMs using the abundant data on end devices without compromising data privacy. Most existing federated fine-tuning methods for LLMs rely on parameter-efficient fine-tuning techniques, which may not reach the performance height possible with full-parameter tuning. However, federated full-parameter tuning of LLMs is a non-trivial problem due to the immense communication cost. This work introduces FedKSeed that employs zeroth-order optimization with a finite set of random seeds. It significantly reduces transmission requirements between the server and clients to just a few random seeds and scalar gradients, amounting to only a few thousand bytes, making federated full-parameter tuning of billion-sized LLMs possible on devices. Building on it, we develop a strategy enabling probability-differentiated seed sampling, prioriti
    
[^86]: 将语言知识注入到BERT中用于对话状态跟踪

    Injecting linguistic knowledge into BERT for Dialogue State Tracking

    [https://arxiv.org/abs/2311.15623](https://arxiv.org/abs/2311.15623)

    本文提出了一种方法，在对话状态跟踪任务中，通过无监督的知识提取方法将语言知识注入到BERT中，以提高性能和可解释性。这种方法无需额外的训练数据，通过简单的神经模块实现。该方法使用的特征提取工具与对话的句法和语义模式相关，有助于理解DST模型的决策过程。

    

    对话状态跟踪(DST)模型通常采用复杂的神经网络架构，需要大量的训练数据，其推理过程缺乏透明性。本文提出了一种方法，通过无监督框架提取语言知识，然后利用这些知识来增强BERT在DST任务中的性能和可解释性。知识提取过程计算经济高效，不需要注释或额外的训练数据。注入提取的知识只需要添加简单的神经模块。我们使用凸多面体模型(CPM)作为DST任务的特征提取工具，并表明所获取的特征与对话中的句法和语义模式相关。这种相关性有助于全面理解影响DST模型决策过程的语言特征。我们在不同的DST任务上对这个框架进行基准测试，并展示了其效果。

    Dialogue State Tracking (DST) models often employ intricate neural network architectures, necessitating substantial training data, and their inference processes lack transparency. This paper proposes a method that extracts linguistic knowledge via an unsupervised framework and subsequently utilizes this knowledge to augment BERT's performance and interpretability in DST tasks. The knowledge extraction procedure is computationally economical and does not necessitate annotations or additional training data. The injection of the extracted knowledge necessitates the addition of only simple neural modules. We employ the Convex Polytopic Model (CPM) as a feature extraction tool for DST tasks and illustrate that the acquired features correlate with the syntactic and semantic patterns in the dialogues. This correlation facilitates a comprehensive understanding of the linguistic features influencing the DST model's decision-making process. We benchmark this framework on various DST tasks and ob
    
[^87]: ECNR: 高效压缩神经表示的时变体积数据

    ECNR: Efficient Compressive Neural Representation of Time-Varying Volumetric Datasets

    [https://arxiv.org/abs/2311.12831](https://arxiv.org/abs/2311.12831)

    ECNR是一种针对时变数据的高效压缩神经表示方法，通过使用多尺度结构和多个小型MLP，以及深度压缩策略，可以显著加速训练和推理过程。

    

    由于其概念简单性和广泛适用性，压缩神经表示已经成为管理大规模体积数据集的传统压缩方法的有希望的替代方案。当前的神经压缩实践利用单个大型多层感知器（MLP）来对全局体积进行编码，导致训练和推理速度慢。本文提出了一种用于时变数据压缩的高效压缩神经表示（ECNR）解决方案，利用拉普拉斯金字塔进行自适应信号拟合。遵循多尺度的结构，我们在每个尺度上利用多个小型MLP来拟合本地内容或残差块。通过将相似的块分配给相同大小的MLP，通过大小统一化，我们实现了MLP之间的平衡并行化，从而显著加速训练和推理。与多尺度结构协同工作的是，我们量身定制了一种深度压缩策略来压缩结果模型。我们展示了ECNR的效果，有多个...

    Due to its conceptual simplicity and generality, compressive neural representation has emerged as a promising alternative to traditional compression methods for managing massive volumetric datasets. The current practice of neural compression utilizes a single large multilayer perceptron (MLP) to encode the global volume, incurring slow training and inference. This paper presents an efficient compressive neural representation (ECNR) solution for time-varying data compression, utilizing the Laplacian pyramid for adaptive signal fitting. Following a multiscale structure, we leverage multiple small MLPs at each scale for fitting local content or residual blocks. By assigning similar blocks to the same MLP via size uniformization, we enable balanced parallelization among MLPs to significantly speed up training and inference. Working in concert with the multiscale structure, we tailor a deep compression strategy to compact the resulting model. We show the effectiveness of ECNR with multiple 
    
[^88]: 针对医学图像联邦学习的隐私风险分析与缓解

    Privacy Risks Analysis and Mitigation in Federated Learning for Medical Images

    [https://arxiv.org/abs/2311.06643](https://arxiv.org/abs/2311.06643)

    本研究提出了一种用于医学数据隐私风险分析和FL中缓解策略的全面框架（MedPFL）。研究发现，使用FL处理医学图像存在重大隐私风险，攻击者可以准确重构私密医学图像。为了缓解隐私攻击，研究提出了防御方法，并评估了其效果。

    

    联邦学习（FL）在医学领域中用于分析医学图像，被认为是一种有效的技术，用于保护敏感患者数据和遵守隐私法规。然而，最近的研究揭示了FL的默认设置可能在隐私攻击下泄露私密训练数据。因此，目前还不清楚FL在医学领域是否存在此类隐私风险，以及如何缓解这些风险。本文首先提出了一种用于医学数据隐私风险分析和FL中缓解策略的全面框架（MedPFL），以分析隐私风险并制定有效的缓解策略以保护私密医学数据。其次，我们展示了使用FL处理医学图像存在的重大隐私风险，其中攻击者可以轻松地进行隐私攻击以准确重建私密医学图像。最后，我们展示了一种防御方法，即添加随机噪声以缓解隐私攻击，并评估了该方法的效果。

    Federated learning (FL) is gaining increasing popularity in the medical domain for analyzing medical images, which is considered an effective technique to safeguard sensitive patient data and comply with privacy regulations. However, several recent studies have revealed that the default settings of FL may leak private training data under privacy attacks. Thus, it is still unclear whether and to what extent such privacy risks of FL exist in the medical domain, and if so, "how to mitigate such risks?". In this paper, first, we propose a holistic framework for Medical data Privacy risk analysis and mitigation in Federated Learning (MedPFL) to analyze privacy risks and develop effective mitigation strategies in FL for protecting private medical data. Second, we demonstrate the substantial privacy risks of using FL to process medical images, where adversaries can easily perform privacy attacks to reconstruct private medical images accurately. Third, we show that the defense approach of addi
    
[^89]: 一种专用的半光滑牛顿方法用于基于核的最优输运问题

    A Specialized Semismooth Newton Method for Kernel-Based Optimal Transport

    [https://arxiv.org/abs/2310.14087](https://arxiv.org/abs/2310.14087)

    提出了一种专门的半光滑牛顿方法，用于解决基于核的最优输运问题，以提高计算效率并可扩展到大样本量。

    

    基于核的最优输运（OT）估计器提供了一种替代的功能估计方法来解决来自样本的OT问题。最近的研究表明，与插值（基于线性规划）OT估计器相比，这些估计器在高维度下比较概率测度时具有更高的统计效率。不幸的是，这种统计优势导致计算成本非常高昂：因为其计算依赖于短步长内点方法（SSIPM），在实践中迭代次数很大，这些估计器很快在样本量$n$方面变得难以处理。为了将这些估计器扩展到更大的$n$，我们提出了一种非光滑固定点模型来解决基于核的OT问题，并显示它可以通过一种专门的半光滑牛顿（SSN）方法有效地解决：我们通过探索问题的结构表明，在实践中，执行一个SSN步骤的每次迭代成本可以显著降低。

    Kernel-based optimal transport (OT) estimators offer an alternative, functional estimation procedure to address OT problems from samples. Recent works suggest that these estimators are more statistically efficient than plug-in (linear programming-based) OT estimators when comparing probability measures in high-dimensions~\citep{Vacher-2021-Dimension}. Unfortunately, that statistical benefit comes at a very steep computational price: because their computation relies on the short-step interior-point method (SSIPM), which comes with a large iteration count in practice, these estimators quickly become intractable w.r.t. sample size $n$. To scale these estimators to larger $n$, we propose a nonsmooth fixed-point model for the kernel-based OT problem, and show that it can be efficiently solved via a specialized semismooth Newton (SSN) method: We show, exploring the problem's structure, that the per-iteration cost of performing one SSN step can be significantly reduced in practice. We prove t
    
[^90]: 用于建模命名实体之间分级关系的无情基准

    A RelEntLess Benchmark for Modelling Graded Relations between Named Entities

    [https://arxiv.org/abs/2305.15002](https://arxiv.org/abs/2305.15002)

    本文提出了一个用于模拟命名实体之间分级关系的无情基准，使用大型语言模型进行填补，以对实体对根据其满足程度进行排序。通过评估最先进的关系嵌入策略和多个LLM，我们发现了重要的创新和贡献。

    

    诸如“受影响于”、“以...闻名”或“与...竞争”之类的关系本质上是分级的：我们可以根据实体对满足这些关系的程度对它们进行排名，但很难将满足和不满足这些关系的实体对划分开。这样的分级关系在许多应用中起着重要作用，然而现有的知识图谱通常不包含此类关系。本文考虑使用大型语言模型（LLM）来填补这个空白的可能性。为此，我们引入了一个新的基准，其中实体对必须根据其满足给定分级关系的程度进行排序。该任务被定义为少样本排序问题，模型只能访问关系的描述和五个原型实例。我们使用提出的基准来评估最先进的关系嵌入策略以及几个最近的LLM，包括公开可用的LLM和封闭模型，例如GPT-4。总体而言，我们发现了一个重要的创新和贡献。

    Relations such as "is influenced by", "is known for" or "is a competitor of" are inherently graded: we can rank entity pairs based on how well they satisfy these relations, but it is hard to draw a line between those pairs that satisfy them and those that do not. Such graded relations play a central role in many applications, yet they are typically not covered by existing Knowledge Graphs. In this paper, we consider the possibility of using Large Language Models (LLMs) to fill this gap. To this end, we introduce a new benchmark, in which entity pairs have to be ranked according to how much they satisfy a given graded relation. The task is formulated as a few-shot ranking problem, where models only have access to a description of the relation and five prototypical instances. We use the proposed benchmark to evaluate state-of-the-art relation embedding strategies as well as several recent LLMs, covering both publicly available LLMs and closed models such as GPT-4. Overall, we find a stro
    
[^91]: 通用多领域聚类问题的解决方法

    Domain-Generalizable Multiple-Domain Clustering

    [https://arxiv.org/abs/2301.13530](https://arxiv.org/abs/2301.13530)

    本研究解决了无监督领域通用化问题，并提出了一个两阶段的训练框架，该框架使用自助预训练和伪标签的多头聚类预测来提高准确性。

    

    本研究将无监督领域通用化问题推广到无标签样本的情况（完全无监督）。我们获得了来自多个源领域的未标记样本，并旨在学习一个共享的预测器，将示例分配到语义相关的聚类中。通过在以前未见的领域中预测聚类分配来进行评估。为实现这一目标，我们提出了一个两阶段的训练框架：（1）自助预训练用于提取领域不变的语义特征。（2）使用伪标签的多头聚类预测，该伪标签依赖于特征空间和聚类头预测，进一步利用了一种新颖的基于预测的标签平滑方案。我们的实验证明，与需要使用目标领域样本进行微调或某种程度的监督的基线模型相比，我们的模型更准确。我们的代码可在https://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering找到。

    This work generalizes the problem of unsupervised domain generalization to the case in which no labeled samples are available (completely unsupervised). We are given unlabeled samples from multiple source domains, and we aim to learn a shared predictor that assigns examples to semantically related clusters. Evaluation is done by predicting cluster assignments in previously unseen domains. Towards this goal, we propose a two-stage training framework: (1) self-supervised pre-training for extracting domain invariant semantic features. (2) multi-head cluster prediction with pseudo labels, which rely on both the feature space and cluster head prediction, further leveraging a novel prediction-based label smoothing scheme. We demonstrate empirically that our model is more accurate than baselines that require fine-tuning using samples from the target domain or some level of supervision. Our code is available at https://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering.
    
[^92]: 关于基于心电图(ECG)的压力检测模型的泛化能力研究

    On the Generalizability of ECG-based Stress Detection Models

    [https://arxiv.org/abs/2210.06225](https://arxiv.org/abs/2210.06225)

    本文研究了基于心电图(ECG)的压力检测模型的泛化能力，探讨了深度学习模型和基于心电图特征的模型在不同压力场景下的应用程度。

    

    压力在日常生活的许多方面都很普遍，包括工作、医疗和社交互动。许多研究已经研究了各种生物信号的手工特征，这些特征是压力的指标。最近，也提出了使用深度学习模型检测压力。通常，压力模型在相同的数据集上进行训练和验证，通常涉及一个压力场景。然而，为每个场景收集压力数据是不实际的。因此，研究这些模型的泛化能力，并确定它们在其他场景中的使用程度非常重要。在本文中，我们探索了基于心电图(ECG)的深度学习模型和基于心电图特征（即心率变异性(HRV)特征）的模型的泛化能力。为此，我们训练了三个HRV模型和两个使用ECG信号作为输入的深度学习模型。我们使用了两个流行的压力数据集（WESAD和SWELL-KW）的ECG信号。

    Stress is prevalent in many aspects of everyday life including work, healthcare, and social interactions. Many works have studied handcrafted features from various bio-signals that are indicators of stress. Recently, deep learning models have also been proposed to detect stress. Typically, stress models are trained and validated on the same dataset, often involving one stressful scenario. However, it is not practical to collect stress data for every scenario. So, it is crucial to study the generalizability of these models and determine to what extent they can be used in other scenarios. In this paper, we explore the generalization capabilities of Electrocardiogram (ECG)-based deep learning models and models based on handcrafted ECG features, i.e., Heart Rate Variability (HRV) features. To this end, we train three HRV models and two deep learning models that use ECG signals as input. We use ECG signals from two popular stress datasets - WESAD and SWELL-KW - differing in terms of stresso
    
[^93]: 一种自述疗法的共情人工智能辅导系统

    An Empathetic AI Coach for Self-Attachment Therapy

    [https://arxiv.org/abs/2209.08316](https://arxiv.org/abs/2209.08316)

    本文介绍了一个用于自述疗法的共情人工智能辅导系统，通过深度学习和规则进行情绪识别和生成流畅、共情的对话，达到更高的用户参与度和实用性。通过非临床试验验证了框架的有效性，并提供改进设计和性能的指导方针。

    

    本文提出了一个用于指导用户进行自述疗法的数字辅导系统的新数据集和计算策略。我们的框架通过将基于规则的对话代理与深度学习分类器相结合，可以识别用户文本回复中的潜在情绪，并采用深度学习辅助检索方法生成新颖、流畅和共情的话语。我们还设计了一组类似人类的角色供用户选择互动。我们的目标是在虚拟疗法会话中实现高水平的参与度。我们在一项非临床试验中对N=16名参与者进行了框架的有效性评估，这些参与者在五天内至少与代理进行了四次互动。结果显示，与简单的基于规则的框架相比，我们的平台在共情度、用户参与度和实用性方面被评价得更高。最后，我们提供了进一步改进设计和性能的指导方针。

    In this work, we present a new dataset and a computational strategy for a digital coach that aims to guide users in practicing the protocols of self-attachment therapy. Our framework augments a rule-based conversational agent with a deep-learning classifier for identifying the underlying emotion in a user's text response, as well as a deep-learning assisted retrieval method for producing novel, fluent and empathetic utterances. We also craft a set of human-like personas that users can choose to interact with. Our goal is to achieve a high level of engagement during virtual therapy sessions. We evaluate the effectiveness of our framework in a non-clinical trial with N=16 participants, all of whom have had at least four interactions with the agent over the course of five days. We find that our platform is consistently rated higher for empathy, user engagement and usefulness than the simple rule-based framework. Finally, we provide guidelines to further improve the design and performance 
    
[^94]: 变分转移学习中的跨领域潜在调制机制

    Variational Transfer Learning using Cross-Domain Latent Modulation

    [https://arxiv.org/abs/2205.15523](https://arxiv.org/abs/2205.15523)

    本研究提出了一种变分自编码器框架中的跨领域潜在调制机制，通过从一领域获取深层表示并影响另一领域的潜在变量，实现了有效的转移学习。在多个转移学习基准任务中，我们的模型展示了竞争性能。

    

    为了成功地将训练好的神经网络模型应用到新领域，强大的转移学习解决方案至关重要。我们提出了一种新的跨领域潜在调制机制，将其引入到变分自编码器框架中，以实现有效的转移学习。我们的关键思想是从一个数据领域获取深层表示，并用它来影响另一个领域的潜在变量的重新参数化。具体地说，通过统一的推理模型来提取源领域和目标领域的深层表示，并通过梯度反转进行对齐。然后将学习到的深层表示跨调制到另一领域的潜在编码中，并应用一致性约束。在包括一些无监督领域自适应和图像到图像转换的转移学习基准任务的实证验证中，我们的模型展示了竞争性能，并且得到了支持的证据。

    To successfully apply trained neural network models to new domains, powerful transfer learning solutions are essential. We propose to introduce a novel cross-domain latent modulation mechanism to a variational autoencoder framework so as to achieve effective transfer learning. Our key idea is to procure deep representations from one data domain and use it to influence the reparameterization of the latent variable of another domain. Specifically, deep representations of the source and target domains are first extracted by a unified inference model and aligned by employing gradient reversal. The learned deep representations are then cross-modulated to the latent encoding of the alternative domain, where consistency constraints are also applied. In the empirical validation that includes a number of transfer learning benchmark tasks for unsupervised domain adaptation and image-to-image translation, our model demonstrates competitive performance, which is also supported by evidence obtained
    
[^95]: 学习预测半监督连续学习的梯度

    Learning to Predict Gradients for Semi-Supervised Continual Learning

    [https://arxiv.org/abs/2201.09196](https://arxiv.org/abs/2201.09196)

    本论文提出了一种新的半监督连续学习方法，通过学习预测未标记数据上的梯度，使其能够适应有监督连续学习方法，从而解决了学习和连续学习过程中的灾难性遗忘问题。

    

    机器智能面临的一个关键挑战是在不遗忘先前获得的知识的情况下学习新的视觉概念。连续学习旨在解决这个挑战。然而，现有的有监督连续学习与人类智能之间存在差距，人类能够从标记和未标记的数据中学习。未标记数据如何影响学习和连续学习过程中的灾难性遗忘至今仍不清楚。为了探索这些问题，我们提出了一种新的半监督连续学习方法，该方法可以通用地应用于现有的连续学习模型。具体而言，一种新颖的梯度学习器从标记数据中学习预测未标记数据上的梯度。因此，未标记数据可以适应有监督连续学习方法。与传统的半监督设置不同的是，我们不假设与未标记数据相关联的潜在类别对学习过程是已知的。

    A key challenge for machine intelligence is to learn new visual concepts without forgetting the previously acquired knowledge. Continual learning is aimed towards addressing this challenge. However, there is a gap between existing supervised continual learning and human-like intelligence, where human is able to learn from both labeled and unlabeled data. How unlabeled data affects learning and catastrophic forgetting in the continual learning process remains unknown. To explore these issues, we formulate a new semi-supervised continual learning method, which can be generically applied to existing continual learning models. Specifically, a novel gradient learner learns from labeled data to predict gradients on unlabeled data. Hence, the unlabeled data could fit into the supervised continual learning method. Different from conventional semi-supervised settings, we do not hypothesize that the underlying classes, which are associated to the unlabeled data, are known to the learning process
    
[^96]: 使用分式比多项式高效解决高阶和非线性常微分方程的研究: 分数神经网络

    Efficiently Solving High-Order and Nonlinear ODEs with Rational Fraction Polynomial: the Ratio Net

    [https://arxiv.org/abs/2105.11309](https://arxiv.org/abs/2105.11309)

    这项研究引入了一种新的神经网络结构——分数神经网络，用于解决高阶和非线性常微分方程。实证试验表明，该方法相比于现有方法，具有更高的效率。

    

    最近使用神经网络解决常微分方程（ODE）的进展令人瞩目。神经网络在作为试探函数以及在函数空间内逼近解方面表现出色，并受梯度反向传播算法的帮助。然而，在解决复杂ODE的过程中仍然存在挑战，包括高阶和非线性情况，强调了提高效率和效果的需求。传统方法通常依赖于已有的知识集成来提高问题求解效率。相反，本研究采用了一种新的神经网络结构来构建试探函数，称为分数神经网络。该结构借鉴了分式比多项式逼近函数，特别是Pade逼近。通过实证试验，证明了该方法相比于现有方法，包括基于多项式和多层感知器的方法，具有更高的效率。

    Recent advances in solving ordinary differential equations (ODEs) with neural networks have been remarkable. Neural networks excel at serving as trial functions and approximating solutions within functional spaces, aided by gradient backpropagation algorithms. However, challenges remain in solving complex ODEs, including high-order and nonlinear cases, emphasizing the need for improved efficiency and effectiveness. Traditional methods have typically relied on established knowledge integration to improve problem-solving efficiency. In contrast, this study takes a different approach by introducing a new neural network architecture for constructing trial functions, known as ratio net. This architecture draws inspiration from rational fraction polynomial approximation functions, specifically the Pade approximant. Through empirical trials, it demonstrated that the proposed method exhibits higher efficiency compared to existing approaches, including polynomial-based and multilayer perceptron
    
[^97]: 探索使用胸部X光图像的深度COVID-19分类的可解释性技术

    Exploration of Interpretability Techniques for Deep COVID-19 Classification using Chest X-ray Images

    [https://arxiv.org/abs/2006.02570](https://arxiv.org/abs/2006.02570)

    本文探讨了使用胸部X光图像进行COVID-19分类的可解释性技术，使用五种不同的深度学习模型进行分类，同时研究了这些模型的可解释性方法。

    

    COVID-19的爆发以其相对快速的传播而震惊了整个世界，并对不同的领域提出了挑战。限制其传播的最有效方法之一是对感染者进行早期和准确的诊断。医学影像，如X光和计算机断层扫描(CT)，结合人工智能(AI)的潜力，在诊断过程中起着支持医务人员的关键作用。因此，本文使用五种不同的深度学习模型(ResNet18、ResNet34、InceptionV3、InceptionResNetV2和DenseNet161)及其集成，通过多数投票的方式，使用胸部X光图像对COVID-19、肺炎和健康对象进行分类。对于每个患者，进行多标签分类以预测其存在的多个病理。首先，通过局部可解释性方法-遮挡、显著性、输入X梯度、引导性反传、集成梯度来对每个网络的可解释性进行了详细研究。

    The outbreak of COVID-19 has shocked the entire world with its fairly rapid spread and has challenged different sectors. One of the most effective ways to limit its spread is the early and accurate diagnosing infected patients. Medical imaging, such as X-ray and Computed Tomography (CT), combined with the potential of Artificial Intelligence (AI), plays an essential role in supporting medical personnel in the diagnosis process. Thus, in this article five different deep learning models (ResNet18, ResNet34, InceptionV3, InceptionResNetV2 and DenseNet161) and their ensemble, using majority voting have been used to classify COVID-19, pneumoni{\ae} and healthy subjects using chest X-ray images. Multilabel classification was performed to predict multiple pathologies for each patient, if present. Firstly, the interpretability of each of the networks was thoroughly studied using local interpretability methods - occlusion, saliency, input X gradient, guided backpropagation, integrated gradients
    
[^98]: 用于成本效益优化的因果机器学习在发展援助分配中的应用

    Causal Machine Learning for Cost-Effective Allocation of Development Aid. (arXiv:2401.16986v1 [stat.ML])

    [http://arxiv.org/abs/2401.16986](http://arxiv.org/abs/2401.16986)

    本文提出了一个因果机器学习框架，用于预测援助分配的异质化治疗效果，以支持有效的援助分配决策。

    

    联合国的可持续发展目标提供了“无人被遗弃”的更美好未来蓝图，为了在2030年之前实现这些目标，贫穷国家需要大量的发展援助。本文提出了一个因果机器学习框架，用于预测援助分配的异质化治疗效果，以支持有效的援助分配决策。具体而言，我们的框架包括三个组成部分：（i）一个平衡自编码器，利用表示学习将高维国家特征嵌入，同时解决治疗选择偏差问题；（ii）一个反事实生成器，用于计算在不同援助规模下的反事实结果，以解决小样本问题；（iii）一个推断模型，用于预测异质化的治疗效果曲线。我们使用105个国家战略性发展援助数据（总额超过52亿美元），以结束HIV/AIDS为目标，证明了我们的框架的有效性。

    The Sustainable Development Goals (SDGs) of the United Nations provide a blueprint of a better future by 'leaving no one behind', and, to achieve the SDGs by 2030, poor countries require immense volumes of development aid. In this paper, we develop a causal machine learning framework for predicting heterogeneous treatment effects of aid disbursements to inform effective aid allocation. Specifically, our framework comprises three components: (i) a balancing autoencoder that uses representation learning to embed high-dimensional country characteristics while addressing treatment selection bias; (ii) a counterfactual generator to compute counterfactual outcomes for varying aid volumes to address small sample-size settings; and (iii) an inference model that is used to predict heterogeneous treatment-response curves. We demonstrate the effectiveness of our framework using data with official development aid earmarked to end HIV/AIDS in 105 countries, amounting to more than USD 5.2 billion. F
    
[^99]: 广义线性匹配滤波器：实现复杂值CNN可解释性之关键

    Widely Linear Matched Filter: A Lynchpin towards the Interpretability of Complex-valued CNNs. (arXiv:2401.16729v1 [cs.LG])

    [http://arxiv.org/abs/2401.16729](http://arxiv.org/abs/2401.16729)

    本研究提出了广义线性匹配滤波器（WLMF）范例来实现复杂值CNN的可解释性，解决了在复杂值数据中匹配滤波的难题，并分析了其性能。与标准线性对应物（SLMF）相比，WLMF在输出信噪比方面具有理论上的优势。

    

    最近关于实值卷积神经网络（CNNs）可解释性的研究揭示了通过匹配滤波器在数据中找到特征的任务与其直接和有物理含义的联系。然而，将这一范式应用于揭示复杂值CNNs可解释性遇到了一个巨大的障碍：广义非循环复杂值数据的匹配滤波器扩展，即广义线性匹配滤波器（WLMF），在文献中仅仅是隐含的。为了实现复杂值CNNs操作的可解释性，我们引入了一种广义WLMF范例，提供了解决方案并对其性能进行了分析。为了保证严谨性，我们的WLMF解决方案不对噪声的概率密度做任何假设。WLMF在输出信噪比方面相对于其标准严格线性对应物（SLMF）的理论优势被提供。

    A recent study on the interpretability of real-valued convolutional neural networks (CNNs) \cite{Stankovic_Mandic_2023CNN} has revealed a direct and physically meaningful link with the task of finding features in data through matched filters. However, applying this paradigm to illuminate the interpretability of complex-valued CNNs meets a formidable obstacle: the extension of matched filtering to a general class of noncircular complex-valued data, referred to here as the widely linear matched filter (WLMF), has been only implicit in the literature. To this end, to establish the interpretability of the operation of complex-valued CNNs, we introduce a general WLMF paradigm, provide its solution and undertake analysis of its performance. For rigor, our WLMF solution is derived without imposing any assumption on the probability density of noise. The theoretical advantages of the WLMF over its standard strictly linear counterpart (SLMF) are provided in terms of their output signal-to-noise-
    
[^100]: 通过Chen-Fliess序列，我们展示了如何将连续深度神经ODE模型构建为单层、无限宽度的网络。

    Rademacher Complexity of Neural ODEs via Chen-Fliess Series. (arXiv:2401.16655v1 [stat.ML])

    [http://arxiv.org/abs/2401.16655](http://arxiv.org/abs/2401.16655)

    本文通过Chen-Fliess序列展开将连续深度神经ODE模型转化为单层、无限宽度的网络，并利用此框架推导出了将初始条件映射到某个终端时间的ODE模型的Rademacher复杂度的紧凑表达式。

    

    本文将连续深度神经ODE模型使用Chen-Fliess序列展开为单层、无限宽度的网络。在这个网络中，输出的“权重”来自控制输入的特征序列，它由控制输入在单纯形上的迭代积分构成。而“特征”则基于受控ODE模型中输出函数相对于向量场的迭代李导数。本文的主要结果是，应用这个框架推导出了将初始条件映射到某个终端时间的ODE模型的Rademacher复杂度的紧凑表达式。这一结果利用了单层结构所带来的直接分析性质。最后，我们通过一些具体系统的例子实例化该界，并讨论了可能的后续工作。

    We show how continuous-depth neural ODE models can be framed as single-layer, infinite-width nets using the Chen--Fliess series expansion for nonlinear ODEs. In this net, the output ''weights'' are taken from the signature of the control input -- a tool used to represent infinite-dimensional paths as a sequence of tensors -- which comprises iterated integrals of the control input over a simplex. The ''features'' are taken to be iterated Lie derivatives of the output function with respect to the vector fields in the controlled ODE model. The main result of this work applies this framework to derive compact expressions for the Rademacher complexity of ODE models that map an initial condition to a scalar output at some terminal time. The result leverages the straightforward analysis afforded by single-layer architectures. We conclude with some examples instantiating the bound for some specific systems and discuss potential follow-up work.
    
[^101]: 遵循人类指令的高质量图像恢复

    High-Quality Image Restoration Following Human Instructions. (arXiv:2401.16468v1 [cs.CV])

    [http://arxiv.org/abs/2401.16468](http://arxiv.org/abs/2401.16468)

    本论文提出了一种使用人类编写的指令来指导图像恢复模型的方法，并在多个恢复任务上取得了最先进的结果，为基于文本指导的图像恢复和增强研究提供了一个新的基准。

    

    图像恢复是一个基本问题，涉及从退化观测中恢复出高质量的干净图像。全能图像恢复模型可以通过使用特定于退化类型的信息作为提示来有效地恢复各种类型和级别的退化图像，并引导恢复模型。我们提出了一种使用人类编写的指令来指导图像恢复模型的方法。在给定自然语言提示的情况下，我们的模型可以从退化图像中恢复出高质量的图像，并考虑多种退化类型。我们的方法InstructIR在图像去噪、雨水去除、去模糊、去雾和(低光)图像增强等多个恢复任务上取得了最先进的结果。InstructIR在之前的全能恢复方法上提高了1dB。此外，我们的数据集和结果为基于文本指导的图像恢复和增强的新研究提供了一个新的基准。我们提供了代码、数据集和模型。

    Image restoration is a fundamental problem that involves recovering a high-quality clean image from its degraded observation. All-In-One image restoration models can effectively restore images from various types and levels of degradation using degradation-specific information as prompts to guide the restoration model. In this work, we present the first approach that uses human-written instructions to guide the image restoration model. Given natural language prompts, our model can recover high-quality images from their degraded counterparts, considering multiple degradation types. Our method, InstructIR, achieves state-of-the-art results on several restoration tasks including image denoising, deraining, deblurring, dehazing, and (low-light) image enhancement. InstructIR improves +1dB over previous all-in-one restoration methods. Moreover, our dataset and results represent a novel benchmark for new research on text-guided image restoration and enhancement. Our code, datasets and models a
    
[^102]: 球面上散点数据拟合的积分算子方法

    Integral Operator Approaches for Scattered Data Fitting on Spheres. (arXiv:2401.15294v1 [math.NA])

    [http://arxiv.org/abs/2401.15294](http://arxiv.org/abs/2401.15294)

    本文提出了一种积分算子方法来解决球面上的散点数据拟合问题，通过研究加权谱滤波算法的逼近性能，成功推导出了带权重谱滤波算法的最优误差估计。这种方法可以避免一些现有方法中存在的问题，同时提供了一种优化算法的解决方案。

    

    本文着重研究了球面上的散点数据拟合问题。我们研究了一类加权谱滤波算法（包括Tikhonov正则化、Landaweber迭代、谱截断和迭代Tikhonov）在拟合可能存在的无界随机噪声的嘈杂数据时的逼近性能。为了分析这个问题，我们提出了一种积分算子方法，可以被看作是散点数据拟合领域中广泛使用的采样不等式方法和规范集方法的延伸。通过提供算子差异和数值积分规则之间的等价性，我们成功地推导出带权重谱滤波算法的Sobolev类型误差估计的最优结果。我们的误差估计不受文献中Tikhonov正则化的饱和现象、现有误差分析中的本地空间屏障和不同嵌入空间的影响。我们还提出了一种分而治之的方案，以提升加权谱滤波算法的效能。

    This paper focuses on scattered data fitting problems on spheres. We study the approximation performance of a class of weighted spectral filter algorithms, including Tikhonov regularization, Landaweber iteration, spectral cut-off, and iterated Tikhonov, in fitting noisy data with possibly unbounded random noise. For the analysis, we develop an integral operator approach that can be regarded as an extension of the widely used sampling inequality approach and norming set method in the community of scattered data fitting. After providing an equivalence between the operator differences and quadrature rules, we succeed in deriving optimal Sobolev-type error estimates of weighted spectral filter algorithms. Our derived error estimates do not suffer from the saturation phenomenon for Tikhonov regularization in the literature, native-space-barrier for existing error analysis and adapts to different embedding spaces. We also propose a divide-and-conquer scheme to equip weighted spectral filter 
    
[^103]: 在任意线性变换下的自适应块稀疏正则化

    Adaptive Block sparse regularization under arbitrary linear transform. (arXiv:2401.15292v1 [cs.LG])

    [http://arxiv.org/abs/2401.15292](http://arxiv.org/abs/2401.15292)

    我们提出了一种在任意线性变换下重构具有块稀疏性的信号的方法，相比现有方法扩大了应用范围，并通过数值实验证明了其有效性。

    

    我们提出了一种用于在未知块结构下的任意线性变换下的块稀疏信号重构方法。该方法是现有方法LOP-$\ell_2$/$\ell_1$的推广，可以在非可逆变换下重构具有块稀疏性的信号，而LOP-$\ell_2$/$\ell_1$不能。我们的工作扩大了块稀疏正则化的范围，使其能够在各种信号处理领域中应用更加灵活和强大。我们推导了一个迭代算法来求解该方法，并给出了其收敛到最优解的条件。数值实验验证了该方法的有效性。

    We propose a convex signal reconstruction method for block sparsity under arbitrary linear transform with unknown block structure. The proposed method is a generalization of the existing method LOP-$\ell_2$/$\ell_1$ and can reconstruct signals with block sparsity under non-invertible transforms, unlike LOP-$\ell_2$/$\ell_1$. Our work broadens the scope of block sparse regularization, enabling more versatile and powerful applications across various signal processing domains. We derive an iterative algorithm for solving proposed method and provide conditions for its convergence to the optimal solution. Numerical experiments demonstrate the effectiveness of the proposed method.
    
[^104]: PrivStream：一种用于流式差分隐私数据的算法

    PrivStream: An Algorithm for Streaming Differentially Private Data. (arXiv:2401.14577v1 [cs.DB])

    [http://arxiv.org/abs/2401.14577](http://arxiv.org/abs/2401.14577)

    PrivStream是一种用于流式差分隐私数据的算法，可以解决离线应用中的隐私保护和数据效用问题。算法可以针对空间数据集进行合成流数据生成，并提供了通用的在线选择性计数框架，验证了算法的实用性。

    

    许多差分隐私研究都着重于假设所有数据一次性可用的离线应用。但当这些算法应用于实际中的数据流，要么违反了隐私保证，要么导致了糟糕的效用。我们提出了一种针对空间数据集的差分隐私合成流数据生成算法，并提供了一种在线选择性计数的通用框架，该框架可用于查询应答和合成数据生成等多个任务。我们的算法在真实数据集和模拟数据集上进行了实验验证。

    Much of the research in differential privacy has focused on offline applications with the assumption that all data is available at once. When these algorithms are applied in practice to streams where data is collected over time, this either violates the privacy guarantees or results in poor utility. We derive an algorithm for differentially private synthetic streaming data generation, especially curated towards spatial datasets. Furthermore, we provide a general framework for online selective counting among a collection of queries which forms a basis for many tasks such as query answering and synthetic data generation. The utility of our algorithm is verified on both real-world and simulated datasets.
    
[^105]: 利用专利数据提高抗体人性预测能力

    Improving Antibody Humanness Prediction using Patent Data. (arXiv:2401.14442v1 [q-bio.QM])

    [http://arxiv.org/abs/2401.14442](http://arxiv.org/abs/2401.14442)

    本研究利用专利数据提高了抗体人性预测的能力，通过多阶段、多损失的训练过程以及弱监督对比学习的方法，成功地预测了抗体序列的人性评分。

    

    我们研究了利用专利数据来提高抗体人性预测的潜力，采用了多阶段、多损失的训练过程。抗体人性作为对抗体治疗的免疫反应的代理，是药物发现中的主要原因之一，在临床环境中使用抗体治疗面临着具有挑战性的障碍。我们将初始学习阶段视为一个弱监督对比学习问题，每个抗体序列与可能有多个功能标识符相关联，目标是学习一个编码器，根据其专利属性将它们分组。然后，我们冻结对比编码器的一部分，并继续使用交叉熵损失在专利数据上训练，以预测给定抗体序列的人性评分。我们通过对三个不同的免疫原性数据集进行推理，展示了专利数据和我们的方法的效用。我们的实证结果表明，l

    We investigate the potential of patent data for improving the antibody humanness prediction using a multi-stage, multi-loss training process. Humanness serves as a proxy for the immunogenic response to antibody therapeutics, one of the major causes of attrition in drug discovery and a challenging obstacle for their use in clinical settings. We pose the initial learning stage as a weakly-supervised contrastive-learning problem, where each antibody sequence is associated with possibly multiple identifiers of function and the objective is to learn an encoder that groups them according to their patented properties. We then freeze a part of the contrastive encoder and continue training it on the patent data using the cross-entropy loss to predict the humanness score of a given antibody sequence. We illustrate the utility of the patent data and our approach by performing inference on three different immunogenicity datasets, unseen during training. Our empirical results demonstrate that the l
    
[^106]: 语义敏感性和不一致的预测：衡量NLI模型的脆弱性

    Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models. (arXiv:2401.14440v1 [cs.CL])

    [http://arxiv.org/abs/2401.14440](http://arxiv.org/abs/2401.14440)

    这份论文研究发现，最先进的NLI模型对微小的语义保持表面形式变化非常敏感，导致推断结果不一致。其行为与对组合语义的有效理解不同，这对当前NLI模型的可靠性提出了挑战。

    

    最近对基于transformer的自然语言理解（NLU）模型的新能力进行的研究表明，它们具备对词汇和组合语义的理解。然而，我们提供了证据表明这些说法应该持保留态度：我们发现目前最先进的自然语言推理（NLI）模型对微小的保留语义的表面形式变化敏感，这导致推断过程中出现大量不一致的模型决策。值得注意的是，这种行为与对组合语义的有效和深入理解不同，而在标准基准测试中评估模型准确度或探究句法、单调性和逻辑鲁棒性推理时均不会出现。我们提出了一个新颖的框架来衡量语义敏感性的程度。为此，我们使用含有微小保留语义的表面形式输入噪声的对抗生成样例来评估NLI模型。

    Recent studies of the emergent capabilities of transformer-based Natural Language Understanding (NLU) models have indicated that they have an understanding of lexical and compositional semantics. We provide evidence that suggests these claims should be taken with a grain of salt: we find that state-of-the-art Natural Language Inference (NLI) models are sensitive towards minor semantics preserving surface-form variations, which lead to sizable inconsistent model decisions during inference. Notably, this behaviour differs from valid and in-depth comprehension of compositional semantics, however does neither emerge when evaluating model accuracy on standard benchmarks nor when probing for syntactic, monotonic, and logically robust reasoning. We propose a novel framework to measure the extent of semantic sensitivity. To this end, we evaluate NLI models on adversarially generated examples containing minor semantics-preserving surface-form input noise. This is achieved using conditional text
    
[^107]: 基于点云表示和扩散模型的晶体结构生成设计

    Generative Design of Crystal Structures by Point Cloud Representations and Diffusion Model. (arXiv:2401.13192v1 [cs.AI])

    [http://arxiv.org/abs/2401.13192](http://arxiv.org/abs/2401.13192)

    本研究提出了一种基于点云和扩散模型的晶体结构生成设计框架，并通过重建输入结构和生成全新材料的实验证明了其有效性和潜力。

    

    在材料设计中，高效地生成能量稳定的晶体结构一直是个挑战，主要是因为晶格中原子的巨大排列。为了促进稳定材料的发现，我们提出了一个用于生成可合成材料的框架，利用点云表示来编码复杂的结构信息。在这个框架的核心是引入扩散模型作为基础支柱。为了评估我们方法的有效性，我们使用它来重建训练数据集中的输入结构，并严格验证其高重建性能。此外，我们通过生成全新的材料，重点强调了基于点云的晶体扩散(PCCD)的巨大潜力，并展示了其可合成性。我们的研究在材料设计和合成的推进中，通过先进的生成设计方法，做出了显著贡献。

    Efficiently generating energetically stable crystal structures has long been a challenge in material design, primarily due to the immense arrangement of atoms in a crystal lattice. To facilitate the discovery of stable material, we present a framework for the generation of synthesizable materials, leveraging a point cloud representation to encode intricate structural information. At the heart of this framework lies the introduction of a diffusion model as its foundational pillar. To gauge the efficacy of our approach, we employ it to reconstruct input structures from our training datasets, rigorously validating its high reconstruction performance. Furthermore, we demonstrate the profound potential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely new materials, emphasizing their synthesizability. Our research stands as a noteworthy contribution to the advancement of materials design and synthesis through the cutting-edge avenue of generative design instead of the con
    
[^108]: 高斯自适应注意力是唯一所需的：跨多个模态的健壮上下文表示

    Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])

    [http://arxiv.org/abs/2401.11143](http://arxiv.org/abs/2401.11143)

    该论文提出了一个名为GAAM的多头高斯自适应注意力机制，用于增强跨多个模态的信息聚合。通过将可学习的均值和方差纳入注意力机制中，GAAM能够动态地重新调整特征的重要性，从而在处理非平稳数据时取得了显著的性能提升，超过了目前现有的注意力技术。该方法的适应性强且参数数量较少，具有改进现有注意力框架的潜力。

    

    我们提出了多头高斯自适应注意力机制（GAAM），一种新颖的概率注意力框架，并设计了高斯自适应变压器（GAT），旨在增强跨多个模态（包括语音、文本和视觉）的信息聚合。GAAM将可学习的均值和方差融入其注意力机制中，采用多头框架实现，使其能够集体建模任何概率分布，以动态重新调整特征重要性。该方法在处理高度非平稳数据时表现出显著改进，通过识别特征空间中的关键元素，超越了现有的注意力技术在模型性能上的状态（精度增加约20%）。GAAM与基于点积的注意力模型兼容，并具有相对较低的参数数量，展示了其适应性和提升现有注意力框架的潜力。在实证方面，GAAM表现出卓越的适应性和功效。

    We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
    
[^109]: 宽而深的ReLU神经网络的普适一致性以及Kolmogorov-Donoho最优函数类的极小极限收敛速率

    Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes. (arXiv:2401.04286v1 [stat.ML])

    [http://arxiv.org/abs/2401.04286](http://arxiv.org/abs/2401.04286)

    本文扩展了之前的结果，证明了基于宽而深的ReLU神经网络和逻辑损失训练的分类规则具有普适一致性，并给出了一类概率测度条件下基于神经网络的分类器实现极小极限收敛速率的充分条件。

    

    本文首先扩展了FL93的结果，并证明了基于宽而深的ReLU神经网络和逻辑损失训练的分类规则的普适一致性。与FL93中分解估计和经验误差的方法不同，我们根据一个广泛的神经网络能够插值任意数量的点的观察，直接分析分类风险。其次，我们给出了一类概率测度的充分条件，在这些条件下，基于神经网络的分类器实现了极小极限收敛速率。我们的结果源于实践者观察到神经网络通常被训练成达到0训练误差的事实，这也是我们提出的神经网络分类器的情况。我们的证明依赖于最近在经验风险最小化和深ReLU神经网络的逼近速率方面的发展，适用于不同的感兴趣函数类的应用。

    In this paper, we first extend the result of FL93 and prove universal consistency for a classification rule based on wide and deep ReLU neural networks trained on the logistic loss. Unlike the approach in FL93 that decomposes the estimation and empirical error, we directly analyze the classification risk based on the observation that a realization of a neural network that is wide enough is capable of interpolating an arbitrary number of points. Secondly, we give sufficient conditions for a class of probability measures under which classifiers based on neural networks achieve minimax optimal rates of convergence. Our result is motivated from the practitioner's observation that neural networks are often trained to achieve 0 training error, which is the case for our proposed neural network classifiers. Our proofs hinge on recent developments in empirical risk minimization and on approximation rates of deep ReLU neural networks for various function classes of interest. Applications to clas
    
[^110]: HAAQI-Net: 一种适用于助听器的非侵入性神经音质评估模型

    HAAQI-Net: A non-intrusive neural music quality assessment model for hearing aids. (arXiv:2401.01145v1 [eess.AS])

    [http://arxiv.org/abs/2401.01145](http://arxiv.org/abs/2401.01145)

    HAAQI-Net是一种适用于助听器用户的非侵入性神经音质评估模型，通过使用BLSTM和注意力机制，以及预训练的BEATs进行声学特征提取，能够快速且准确地预测音乐的HAAQI得分，相比传统方法具有更高的性能和更低的推理时间。

    

    本文介绍了HAAQI-Net，一种针对助听器用户定制的非侵入性深度学习音质评估模型。与传统方法如Hearing Aid Audio Quality Index (HAAQI) 不同，HAAQI-Net采用了带有注意力机制的双向长短期记忆网络(BLSTM)。该模型以评估的音乐样本和听力损失模式作为输入，生成预测的HAAQI得分。模型采用了预训练的来自音频变换器(BEATs)的双向编码器表示进行声学特征提取。通过将预测分数与真实分数进行比较，HAAQI-Net达到了0.9257的长期一致性相关(LCC)，0.9394的斯皮尔曼等级相关系数(SRCC)，和0.0080的均方误差(MSE)。值得注意的是，这种高性能伴随着推理时间的大幅减少：从62.52秒(HAAQI)减少到2.71秒(HAAQI-Net)，为助听器用户提供了高效的音质评估模型。

    This paper introduces HAAQI-Net, a non-intrusive deep learning model for music quality assessment tailored to hearing aid users. In contrast to traditional methods like the Hearing Aid Audio Quality Index (HAAQI), HAAQI-Net utilizes a Bidirectional Long Short-Term Memory (BLSTM) with attention. It takes an assessed music sample and a hearing loss pattern as input, generating a predicted HAAQI score. The model employs the pre-trained Bidirectional Encoder representation from Audio Transformers (BEATs) for acoustic feature extraction. Comparing predicted scores with ground truth, HAAQI-Net achieves a Longitudinal Concordance Correlation (LCC) of 0.9257, Spearman's Rank Correlation Coefficient (SRCC) of 0.9394, and Mean Squared Error (MSE) of 0.0080. Notably, this high performance comes with a substantial reduction in inference time: from 62.52 seconds (by HAAQI) to 2.71 seconds (by HAAQI-Net), serving as an efficient music quality assessment model for hearing aid users.
    
[^111]: 基于图神经网络的快速芯片库特征化用于设计技术共优化

    Fast Cell Library Characterization for Design Technology Co-Optimization Based on Graph Neural Networks. (arXiv:2312.12784v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.12784](http://arxiv.org/abs/2312.12784)

    提出了一种基于图神经网络的快速准确芯片库特征化的机器学习模型，通过结合芯片结构，在各种工艺参数下预测精度高，并且相较于传统方法具有100倍的加速。

    

    设计技术共优化在先进半导体工艺开发中实现功耗、性能和面积（PPA）的最佳化发挥着关键作用。芯片库特征化在设计技术共优化流程中至关重要，但传统方法耗时且昂贵。为了克服这些挑战，我们提出了一种基于图神经网络（GNN）的快速准确的芯片库特征化的机器学习模型。我们的模型考虑了芯片结构，并在各种工艺-电压-温度（PVT）角和技术参数上展示出高预测精度。在512个未见过的工艺角和一百万个测试数据点的验证中，我们的模型对于33种类型的单元的延迟、功率和输入引脚电容具有准确的预测，均方绝对百分比误差（MAPE）≤ 0.95%，与SPICE仿真相比加速了100倍。此外，我们还研究了系统级指标，如最差负松弛（WNS）、漏电功耗和动态...

    Design technology co-optimization (DTCO) plays a critical role in achieving optimal power, performance, and area (PPA) for advanced semiconductor process development. Cell library characterization is essential in DTCO flow, but traditional methods are time-consuming and costly. To overcome these challenges, we propose a graph neural network (GNN)-based machine learning model for rapid and accurate cell library characterization. Our model incorporates cell structures and demonstrates high prediction accuracy across various process-voltage-temperature (PVT) corners and technology parameters. Validation with 512 unseen technology corners and over one million test data points shows accurate predictions of delay, power, and input pin capacitance for 33 types of cells, with a mean absolute percentage error (MAPE) $\le$ 0.95% and a speed-up of 100X compared with SPICE simulations. Additionally, we investigate system-level metrics such as worst negative slack (WNS), leakage power, and dynamic 
    
[^112]: 强化微调语言模型中的梯度消失问题

    Vanishing Gradients in Reinforcement Finetuning of Language Models. (arXiv:2310.20703v1 [cs.LG])

    [http://arxiv.org/abs/2310.20703](http://arxiv.org/abs/2310.20703)

    本研究发现在强化微调（RFT）中存在梯度消失的问题，当模型下奖励的标准差较小时，输入的期望梯度会消失，导致奖励最大化缓慢。初始监督微调（SFT）阶段是克服这个问题的最有希望的方法。

    

    预训练的语言模型通过强化微调（RFT）与人类偏好和下游任务对齐，即使用策略梯度算法最大化（可能是学习得到的）奖励函数。本研究发现了RFT中的一个基本的优化障碍：我们证明了当模型下的奖励标准差较小时，输入的期望梯度会消失，即使期望奖励远离最优解。通过在RFT基准和控制环境中进行实验，以及理论分析，我们证明了由于小的奖励标准差导致的梯度消失问题普遍存在且有害，导致奖励最大化极其缓慢。最后，我们探索了克服RFT中梯度消失的方法。我们发现初始监督微调（SFT）阶段是最有希望的候选方法，并且揭示了它在RFT流程中的重要性。此外，我们还表明相对较小的训练数据集的SFT阶段可以有效克服梯度消失问题。

    Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which entails maximizing a (possibly learned) reward function using policy gradient algorithms. This work highlights a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small num
    
[^113]: 无线联合策略梯度的过空中聚合方法

    Over-the-air Federated Policy Gradient. (arXiv:2310.16592v1 [cs.LG])

    [http://arxiv.org/abs/2310.16592](http://arxiv.org/abs/2310.16592)

    本文提出了一种过空中联合策略梯度算法，通过无线信道广播携带本地信息的模拟信号实现更新策略参数，研究了噪声和信道失真对算法收敛性的影响，并通过仿真结果验证了算法的有效性。

    

    近年来，过空中聚合在大规模分布式学习、优化和感知中得到了广泛关注。本文提出了一种过空中联合策略梯度算法，其中所有的智能体同时向共享的无线信道广播携带本地信息的模拟信号，中央控制器使用接收到的汇总波形来更新策略参数。我们研究了噪声和信道失真对所提出算法收敛性的影响，并建立了通信和采样的复杂度来找到一个$\epsilon$-近似的稳定点。最后，我们通过一些仿真结果展示了该算法的有效性。

    In recent years, over-the-air aggregation has been widely considered in large-scale distributed learning, optimization, and sensing. In this paper, we propose the over-the-air federated policy gradient algorithm, where all agents simultaneously broadcast an analog signal carrying local information to a common wireless channel, and a central controller uses the received aggregated waveform to update the policy parameters. We investigate the effect of noise and channel distortion on the convergence of the proposed algorithm, and establish the complexities of communication and sampling for finding an $\epsilon$-approximate stationary point. Finally, we present some simulation results to show the effectiveness of the algorithm.
    
[^114]: 机器学习模型的成员推断攻击的基本限制

    Fundamental Limits of Membership Inference Attacks on Machine Learning Models. (arXiv:2310.13786v1 [stat.ML])

    [http://arxiv.org/abs/2310.13786](http://arxiv.org/abs/2310.13786)

    本文探讨了机器学习模型上成员推断攻击的基本限制，包括推导了效果和成功率的统计量，并提供了几种情况下的界限。这使得我们能够根据样本数量和其他结构参数推断潜在攻击的准确性。

    

    成员推断攻击（MIA）可以揭示特定数据点是否是训练数据集的一部分，可能暴露个人的敏感信息。本文探讨了关于机器学习模型上MIA的基本统计限制。具体而言，我们首先推导了统计量，该统计量决定了这种攻击的有效性和成功率。然后，我们研究了几种情况，并对这个感兴趣的统计量提供了界限。这使我们能够根据样本数量和学习模型的其他结构参数推断潜在攻击的准确性，在某些情况下可以直接从数据集中估计。

    Membership inference attacks (MIA) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. This article explores the fundamental statistical limitations associated with MIAs on machine learning models. More precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. Then, we investigate several situations for which we provide bounds on this quantity of interest. This allows us to infer the accuracy of potential attacks as a function of the number of samples and other structural parameters of learning models, which in some cases can be directly estimated from the dataset.
    
[^115]: 具有多项式激活函数的图神经网络具有有限的表达能力

    Graph Neural Networks with polynomial activations have limited expressivity. (arXiv:2310.13139v1 [cs.LG])

    [http://arxiv.org/abs/2310.13139](http://arxiv.org/abs/2310.13139)

    本文证明了具有多项式激活函数的图神经网络无法表达GC2查询，与常用的非多项式激活函数存在分离，这回答了一个开放问题。

    

    图神经网络（GNNs）的表达能力可以完全由适当的一阶逻辑片段来描述。换句话说，任何在标记图上解释的关于二元逻辑片段（GC2）的查询都可以使用一个大小仅取决于查询深度的GNN来表示。正如[Barcelo＆Al。，2020，Grohe，2021]指出的那样，这个描述适用于一组激活函数的家族，这表明GNN可以通过不同的激活函数选择来表达不同的逻辑层次结构。在本文中，我们证明了这样的层次结构的存在，证明了具有多项式激活函数的GNN无法表示GC2查询。这意味着多项式和常用的非多项式激活函数（如ReLU、sigmoid、双曲正切等）之间存在一个分离，并回答了[Grohe，2021]提出的一个悬而未决的问题。

    The expressivity of Graph Neural Networks (GNNs) can be entirely characterized by appropriate fragments of the first order logic. Namely, any query of the two variable fragment of graded modal logic (GC2) interpreted over labelled graphs can be expressed using a GNN whose size depends only on the depth of the query. As pointed out by [Barcelo & Al., 2020, Grohe, 2021 ], this description holds for a family of activation functions, leaving the possibibility for a hierarchy of logics expressible by GNNs depending on the chosen activation function. In this article, we show that such hierarchy indeed exists by proving that GC2 queries cannot be expressed by GNNs with polynomial activation functions. This implies a separation between polynomial and popular non polynomial activations (such as ReLUs, sigmoid and hyperbolic tan and others) and answers an open question formulated by [Grohe, 2021].
    
[^116]: 通过基于注意力的深度状态空间建模，将PPG信号转换为ECG，用于连续性心房颤动检测

    PPG to ECG Signal Translation for Continuous Atrial Fibrillation Detection via Attention-based Deep State-Space Modeling. (arXiv:2309.15375v1 [cs.LG])

    [http://arxiv.org/abs/2309.15375](http://arxiv.org/abs/2309.15375)

    通过基于注意力的深度状态空间建模，我们提出了一种不受个体限制的方法，将PPG信号转换为ECG，用于连续性心房颤动检测。

    

    电信号图（ECG或EKG）是一种测量心脏电活动的医学测试。ECG常用于诊断和监测各种心脏疾病，包括心律失常、心肌梗塞和心力衰竭。然而，传统的ECG需要临床测量，限制了其在医疗机构的应用。相比之下，单导联ECG已经在佩戴式设备上应用广泛。另一种ECG的替代方法是光浊度脉搏检测（PPG），它采用非侵入性、低成本的光学方法来测量心脏生理学，使其成为捕捉日常生活中重要心脏信号的合适选择。虽然ECG和PPG之间具有强烈的相关性，但后者并没有提供明显的临床诊断价值。在这里，我们提出了一种不受个体限制的基于注意力的深度状态空间模型，用于将PPG信号转换为ECG，从而实现连续性心房颤动检测。

    An electrocardiogram (ECG or EKG) is a medical test that measures the heart's electrical activity. ECGs are often used to diagnose and monitor a wide range of heart conditions, including arrhythmias, heart attacks, and heart failure. On the one hand, the conventional ECG requires clinical measurement, which restricts its deployment to medical facilities. On the other hand, single-lead ECG has become popular on wearable devices using administered procedures. An alternative to ECG is Photoplethysmography (PPG), which uses non-invasive, low-cost optical methods to measure cardiac physiology, making it a suitable option for capturing vital heart signs in daily life. As a result, it has become increasingly popular in health monitoring and is used in various clinical and commercial wearable devices. While ECG and PPG correlate strongly, the latter does not offer significant clinical diagnostic value. Here, we propose a subject-independent attention-based deep state-space model to translate P
    
[^117]: 关联变换器是一种稀疏表示学习器

    Associative Transformer Is A Sparse Representation Learner. (arXiv:2309.12862v1 [cs.LG])

    [http://arxiv.org/abs/2309.12862](http://arxiv.org/abs/2309.12862)

    关联变换器（AiT）是一种采用低秩显式记忆和关联记忆的稀疏表示学习器，通过联合端到端训练实现模块特化和注意力瓶颈的形成。

    

    在传统的Transformer模型中，出现了一种新兴的基于稀疏交互的注意力机制，这种机制与生物原理更为接近。包括Set Transformer和Perceiver在内的方法采用了与有限能力的潜在空间相结合的交叉注意力机制。基于最近对全局工作空间理论和关联记忆的神经科学研究，我们提出了关联变换器（AiT）。AiT引入了低秩显式记忆，既可以作为先验来指导共享工作空间的瓶颈注意力，又可以作为关联记忆的吸引子。通过联合端到端训练，这些先验自然地发展出模块的特化，每个模块对形成注意力瓶颈的归纳偏好有所贡献。瓶颈可以促进输入之间为将信息写入内存而进行竞争。我们展示了AiT是一种稀疏表示学习器。

    Emerging from the monolithic pairwise attention mechanism in conventional Transformer models, there is a growing interest in leveraging sparse interactions that align more closely with biological principles. Approaches including the Set Transformer and the Perceiver employ cross-attention consolidated with a latent space that forms an attention bottleneck with limited capacity. Building upon recent neuroscience studies of Global Workspace Theory and associative memory, we propose the Associative Transformer (AiT). AiT induces low-rank explicit memory that serves as both priors to guide bottleneck attention in the shared workspace and attractors within associative memory of a Hopfield network. Through joint end-to-end training, these priors naturally develop module specialization, each contributing a distinct inductive bias to form attention bottlenecks. A bottleneck can foster competition among inputs for writing information into the memory. We show that AiT is a sparse representation 
    
[^118]: 一个用于完全无监督异常检测的通用机器学习框架，适用于污染数据

    A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data. (arXiv:2308.13352v1 [cs.LG])

    [http://arxiv.org/abs/2308.13352](http://arxiv.org/abs/2308.13352)

    这篇论文介绍了一个完全无监督的机器学习框架，用于处理在训练数据中含有异常样本的异常检测任务。

    

    在各个领域和应用中，机器学习算法已经解决了异常检测（AD）任务。这些算法中大多数使用正常数据对一个基于残差的模型进行训练，并根据未见样本与学习到的正常范围的不相似性来分配异常分数。这些方法的基本假设是可以用无异常的数据进行训练。然而，在真实世界中的操作环境中，训练数据通常会与一定比例的异常样本混合。而利用污染数据进行训练必然会导致基于残差的算法的AD性能下降。本文介绍了一个完全无监督的用于AD任务的污染训练数据的改进框架。该框架是通用的，可应用于任何基于残差的机器学习模型。我们展示了该框架在两个多元时间数据集上的应用。

    Anomaly detection (AD) tasks have been solved using machine learning algorithms in various domains and applications. The great majority of these algorithms use normal data to train a residual-based model, and assign anomaly scores to unseen samples based on their dissimilarity with the learned normal regime. The underlying assumption of these approaches is that anomaly-free data is available for training. This is, however, often not the case in real-world operational settings, where the training data may be contaminated with a certain fraction of abnormal samples. Training with contaminated data, in turn, inevitably leads to a deteriorated AD performance of the residual-based algorithms.  In this paper we introduce a framework for a fully unsupervised refinement of contaminated training data for AD tasks. The framework is generic and can be applied to any residual-based machine learning model. We demonstrate the application of the framework to two public datasets of multivariate time s
    
[^119]: 尝试更简单-改进主成分分析在基于日志的异常检测中的评估

    Try with Simpler -- An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection. (arXiv:2308.12612v1 [cs.LG])

    [http://arxiv.org/abs/2308.12612](http://arxiv.org/abs/2308.12612)

    本论文通过改进传统的主成分分析方法，优化了基于日志的异常检测技术，以提高其效果，从而使其与深度学习方法相媲美。

    

    深度学习的快速发展激发了对增强基于日志的异常检测的兴趣。这种方法旨在从日志事件（日志消息模板）中提取意义，并开发先进的深度学习模型以进行异常检测。然而，这些深度学习方法面临着依赖训练数据、标签和计算资源的挑战，因为模型复杂性较高。相比之下，传统的机器学习和数据挖掘技术虽然不太依赖数据，更高效，但比深度学习方法效果较差。为了使基于日志的异常检测更实用，目标是增强传统技术以达到深度学习方法的效果。以前在不同领域（链接Stack Overflow上的问题）的研究表明，经过优化的传统技术可以与最先进的深度学习方法相媲美。受到这一概念的启发，我们进行了一项经验研究。我们通过整合轻量级正则化方法来优化无监督的主成分分析（PCA），一种传统技术。

    The rapid growth of deep learning (DL) has spurred interest in enhancing log-based anomaly detection. This approach aims to extract meaning from log events (log message templates) and develop advanced DL models for anomaly detection. However, these DL methods face challenges like heavy reliance on training data, labels, and computational resources due to model complexity. In contrast, traditional machine learning and data mining techniques are less data-dependent and more efficient but less effective than DL. To make log-based anomaly detection more practical, the goal is to enhance traditional techniques to match DL's effectiveness. Previous research in a different domain (linking questions on Stack Overflow) suggests that optimized traditional techniques can rival state-of-the-art DL methods. Drawing inspiration from this concept, we conducted an empirical study. We optimized the unsupervised PCA (Principal Component Analysis), a traditional technique, by incorporating lightweight se
    
[^120]: 从流式多元时间序列中实现零延迟一致信号重建

    Zero-delay Consistent Signal Reconstruction from Streamed Multivariate Time Series. (arXiv:2308.12459v1 [eess.SP])

    [http://arxiv.org/abs/2308.12459](http://arxiv.org/abs/2308.12459)

    本文介绍了一种从流式多元时间序列中一致地重建信号的方法，同时减少了零延迟信号重建的粗糙度。 (arXiv:2308.12459v1 [eess.SP])

    

    数字化现实世界的模拟信号通常涉及时间采样和幅度离散化。后续的信号重建不可避免地会产生一个与幅度分辨率和获取样本的时间密度有关的误差。从实施的角度来看，一致的信号重建方法在采样率增加时已被证明具有有益的误差衰减效果。尽管如此，这些结果是在离线设置下获得的。因此，关于从数据流中进行一致信号重建的方法存在研究空白。本文提出了一种在零延迟响应要求下一致地重建量化间隔的流式多元时间序列的方法。另一方面，先前的工作表明，利用单变量时间序列中的时间依赖性可以减少零延迟信号重建的粗糙度。本工作表明，在多元时间序列中存在着时空依赖性。

    Digitalizing real-world analog signals typically involves sampling in time and discretizing in amplitude. Subsequent signal reconstructions inevitably incur an error that depends on the amplitude resolution and the temporal density of the acquired samples. From an implementation viewpoint, consistent signal reconstruction methods have proven a profitable error-rate decay as the sampling rate increases. Despite that, these results are obtained under offline settings. Therefore, a research gap exists regarding methods for consistent signal reconstruction from data streams. This paper presents a method that consistently reconstructs streamed multivariate time series of quantization intervals under a zero-delay response requirement. On the other hand, previous work has shown that the temporal dependencies within univariate time series can be exploited to reduce the roughness of zero-delay signal reconstructions. This work shows that the spatiotemporal dependencies within multivariate time 
    
[^121]: ConcatPlexer：通过附加Dim1批处理以加快ViTs速度

    ConcatPlexer: Additional Dim1 Batching for Faster ViTs. (arXiv:2308.11199v1 [cs.CV])

    [http://arxiv.org/abs/2308.11199](http://arxiv.org/abs/2308.11199)

    本文提出了一种名为ConcatPlexer的方法，通过在视觉识别中使用附加的Dim1批处理（即连接）来提高吞吐量，同时准确性受到的影响较小。

    

    Transformer不仅在自然语言处理领域，还在计算机视觉领域取得了巨大成功，引发了各种创新的方法和应用。然而，Transformer卓越的性能和建模灵活性带来了计算成本的严重增加，因此有几项工作提出了减少这种负担的方法。受语言模型的一种减少成本的方法Data Multiplexing (DataMUX)的启发，我们提出了一种用于高效视觉识别的新方法，它采用了附加的Dim1批处理（即连接），在保证准确性的基础上大大提高了吞吐量。我们首先为视觉模型引入了DataMux的一种天然适应方法，图像多路复用器（Image Multiplexer），并设计了新的组件来克服其缺点，进而形成了我们最终的模型ConcatPlexer，在推理速度和准确度之间找到了平衡点。ConcatPlexer在ImageNet1K和CIFAR100数据集上进行了训练。

    Transformers have demonstrated tremendous success not only in the natural language processing (NLP) domain but also the field of computer vision, igniting various creative approaches and applications. Yet, the superior performance and modeling flexibility of transformers came with a severe increase in computation costs, and hence several works have proposed methods to reduce this burden. Inspired by a cost-cutting method originally proposed for language models, Data Multiplexing (DataMUX), we propose a novel approach for efficient visual recognition that employs additional dim1 batching (i.e., concatenation) that greatly improves the throughput with little compromise in the accuracy. We first introduce a naive adaptation of DataMux for vision models, Image Multiplexer, and devise novel components to overcome its weaknesses, rendering our final model, ConcatPlexer, at the sweet spot between inference speed and accuracy. The ConcatPlexer was trained on ImageNet1K and CIFAR100 dataset and
    
[^122]: 通过意外记忆实现内在动机

    Intrinsic Motivation via Surprise Memory. (arXiv:2308.04836v1 [cs.LG])

    [http://arxiv.org/abs/2308.04836](http://arxiv.org/abs/2308.04836)

    该论文提出了一种新的计算模型，通过意外记忆作为内在奖励的基础，在强化学习中解决了现有方法的局限性。实验结果表明，通过结合意外预测器的意外记忆在稀疏奖励环境中表现出高效的探索行为，并显著提升了最终性能。

    

    我们提出了一种新的计算模型，用于强化学习中的内在奖励，解决了现有基于意外的探索的局限性。奖励是意外的新颖性，而不是意外的规范。我们通过一个记忆网络中的检索错误来估计意外的新颖性，记忆存储和重建意外。我们的意外记忆（SM）增加了基于意外的内在动力学的能力，保持了对激动人心的探索的兴趣，同时减少了对不可预测或噪声观察的不必要的吸引力。我们的实验表明，结合各种意外预测器的SM展示了高效的探索行为，并显著提升了稀疏奖励环境中的最终性能，包括噪声电视、导航和具有挑战性的Atari游戏。

    We present a new computing model for intrinsic rewards in reinforcement learning that addresses the limitations of existing surprise-driven explorations. The reward is the novelty of the surprise rather than the surprise norm. We estimate the surprise novelty as retrieval errors of a memory network wherein the memory stores and reconstructs surprises. Our surprise memory (SM) augments the capability of surprise-based intrinsic motivators, maintaining the agent's interest in exciting exploration while reducing unwanted attraction to unpredictable or noisy observations. Our experiments demonstrate that the SM combined with various surprise predictors exhibits efficient exploring behaviors and significantly boosts the final performance in sparse reward environments, including Noisy-TV, navigation and challenging Atari games.
    
[^123]: RCT拒绝抽样用于因果估计评估

    RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v1 [cs.AI])

    [http://arxiv.org/abs/2307.15176](http://arxiv.org/abs/2307.15176)

    该论文提出了一种名为RCT拒绝抽样的新抽样算法，用于因果估计评估。该方法通过子抽样随机控制试验(RCT)创建混淆的观测数据集，并使用RCT的平均因果效应作为基准真实值，以进行有效比较。

    

    混淆是从观测数据中无偏估计因果效应的一个重要障碍。对于高维协变量的情况，如文本数据、基因组学或行为社会科学，研究人员提出了适应机器学习方法进行因果估计的调整方法。然而，这些调整方法的经验评估一直存在困难和限制。在这项工作中，我们基于一种有前景的经验评估策略，简化了评估设计，并使用真实数据：对随机控制试验(RCT)进行子抽样，以创建混淆的观测数据集，同时使用RCT的平均因果效应作为基准真实值。我们提出了一种新的抽样算法，称为RCT拒绝抽样，并提供了理论保证，以确保观测数据的因果识别成立，从而可以与基准RCT进行有效比较。通过使用合成数据，我们展示了我们的算法在...

    Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm in
    
[^124]: 牙科点云的变分自编码

    Variational Autoencoding of Dental Point Clouds. (arXiv:2307.10895v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.10895](http://arxiv.org/abs/2307.10895)

    本论文介绍了一种新颖的点云变分自编码器（VF-Net）用于牙科点云数据的处理，该模型在各种任务中具有显著的性能，包括网格生成、形状完整和表示学习。

    

    数字牙科学取得了重大进展，但仍面临许多挑战。本文介绍了FDI 16数据集，这是一个包含了大量牙齿网格和点云的数据集。此外，我们还提出了一种新颖的方法：变分FoldingNet（VF-Net），这是一种专为点云设计的完全概率变分自编码器。值得注意的是，先前的点云潜变量模型缺乏输入和输出点之间的一一对应关系。相反，它们依赖于优化Chamfer距离，这是一种缺乏归一化分布对应的度量，因此不适合概率建模。我们用合适的编码器取代了明确的最小化Chamfer距离，提高了计算效率，同时简化了概率扩展。这使得在各种任务中都可以直接应用，包括网格生成、形状完整和表示学习。在实证方面，我们提供了牙齿重建中较低的重建误差的证据。

    Digital dentistry has made significant advancements, yet numerous challenges remain. This paper introduces the FDI 16 dataset, an extensive collection of tooth meshes and point clouds. Additionally, we present a novel approach: Variational FoldingNet (VF-Net), a fully probabilistic variational autoencoder designed for point clouds. Notably, prior latent variable models for point clouds lack a one-to-one correspondence between input and output points. Instead, they rely on optimizing Chamfer distances, a metric that lacks a normalized distributional counterpart, rendering it unsuitable for probabilistic modeling. We replace the explicit minimization of Chamfer distances with a suitable encoder, increasing computational efficiency while simplifying the probabilistic extension. This allows for straightforward application in various tasks, including mesh generation, shape completion, and representation learning. Empirically, we provide evidence of lower reconstruction error in dental recon
    
[^125]: 深度网络逼近：从ReLU到多种激活函数

    Deep Network Approximation: Beyond ReLU to Diverse Activation Functions. (arXiv:2307.06555v1 [cs.LG])

    [http://arxiv.org/abs/2307.06555](http://arxiv.org/abs/2307.06555)

    本文研究了深度神经网络在多种激活函数下的表达能力，证明了可以通过在有界集合上构建一个宽度为6N、深度为2L的varrho激活网络来逼近一个宽度为N、深度为L的ReLU网络，从而将对ReLU网络的逼近结果推广到其他激活函数。

    

    本文探究了深度神经网络在多种激活函数下的表达能力。定义了一个激活函数集合A，包括大多数常用的激活函数，如ReLU、LeakyReLU、ReLU^2、ELU、SELU、Softplus、GELU、SiLU、Swish、Mish、Sigmoid、Tanh、Arctan、Softsign、dSiLU和SRS。我们证明了对于任意激活函数varrho∈A，可以通过一个宽度为6N、深度为2L的varrho激活网络在有界集合上以任意精度逼近一个宽度为N、深度为L的ReLU网络。这一发现使得大部分对于ReLU网络的逼近结果能够推广到其他激活函数，尽管需要稍大的常数代价。

    This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set $\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\mathtt{ReLU}$, $\mathtt{LeakyReLU}$, $\mathtt{ReLU}^2$, $\mathtt{ELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$, $\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, $\mathtt{Mish}$, $\mathtt{Sigmoid}$, $\mathtt{Tanh}$, $\mathtt{Arctan}$, $\mathtt{Softsign}$, $\mathtt{dSiLU}$, and $\mathtt{SRS}$. We demonstrate that for any activation function $\varrho\in \mathscr{A}$, a $\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\varrho$-activated network of width $6N$ and depth $2L$ on any bounded set. This finding enables the extension of most approximation results achieved with $\mathtt{ReLU}$ networks to a wide variety of other activation functions, at the cost of slightly larger constants.
    
[^126]: 使用深度集成神经网络在端点设备上预测小分子的溶解度

    Predicting small molecules solubilities on endpoint devices using deep ensemble neural networks. (arXiv:2307.05318v1 [physics.chem-ph])

    [http://arxiv.org/abs/2307.05318](http://arxiv.org/abs/2307.05318)

    这项工作提出了一种使用深度集成神经网络在端点设备上预测小分子溶解度的方法，通过静态网站运行，同时具备预测不确定性，并实现了令人满意的结果。

    

    水溶解度是一种有价值但难以预测的性质。使用一级原理方法计算溶解度需要考虑熵和焓的竞争效应，导致计算时间较长且准确性相对较差。基于数据驱动的方法，如深度学习，提供了更高的准确性和计算效率，但通常缺乏不确定性量化。此外，任何计算技术的易用性仍然是一个问题，导致群体贡献方法的持续流行。在这项工作中，我们使用一种具有预测不确定性的深度学习模型来解决这些问题，该模型在静态网站上运行（无需服务器）。这种方法将计算需求转移到网站访问者身上，而不需要安装，消除了支付和维护服务器的需求。我们的模型在溶解度预测上取得了令人满意的结果。此外，我们展示了如何创建平衡溶解度预测模型。

    Aqueous solubility is a valuable yet challenging property to predict. Computing solubility using first-principles methods requires accounting for the competing effects of entropy and enthalpy, resulting in long computations for relatively poor accuracy. Data-driven approaches, such as deep learning, offer improved accuracy and computational efficiency but typically lack uncertainty quantification. Additionally, ease of use remains a concern for any computational technique, resulting in the sustained popularity of group-based contribution methods. In this work, we addressed these problems with a deep learning model with predictive uncertainty that runs on a static website (without a server). This approach moves computing needs onto the website visitor without requiring installation, removing the need to pay for and maintain servers. Our model achieves satisfactory results in solubility prediction. Furthermore, we demonstrate how to create molecular property prediction models that balanc
    
[^127]: 自我监督的语音模型对单词的了解程度是什么？

    What do self-supervised speech models know about words?. (arXiv:2307.00162v1 [cs.CL])

    [http://arxiv.org/abs/2307.00162](http://arxiv.org/abs/2307.00162)

    通过对自我监督的语音模型进行分析，发现这些模型在不同层中编码了不同的语言信息，也学习了类似音素的子词单元。与单词相关的信息主要在中间的模型层中，同时一些低级信息在更高的层中也得以保留。

    

    在过去几年中，许多自我监督的语音模型（S3Ms）被引入，为各种语音任务提供了性能和数据效率的改进。有证据表明，不同的S3Ms在不同的层中编码语言信息，而且一些S3Ms似乎学习了类似于音素的子词单元。然而，这些模型捕捉更大的语言单元（如单词）的程度以及单词相关信息的编码位置仍然不清楚。在这项研究中，我们对来自三个S3Ms的不同层的单词片段表示进行了多种分析：wav2vec2、HuBERT和WavLM。我们利用规范相关分析（CCA），一种轻量级的分析工具，来衡量这些表示与单词级语言属性之间的相似性。我们发现最大的单词级语言内容往往出现在中间的模型层，而一些低级信息（如发音）也在更高的层中保留。

    Many self-supervised speech models (S3Ms) have been introduced over the last few years, producing performance and data efficiency improvements for a variety of speech tasks. Evidence is emerging that different S3Ms encode linguistic information in different layers, and also that some S3Ms appear to learn phone-like sub-word units. However, the extent to which these models capture larger linguistic units, such as words, and where word-related information is encoded, remains unclear. In this study, we conduct several analyses of word segment representations extracted from different layers of three S3Ms: wav2vec2, HuBERT, and WavLM. We employ canonical correlation analysis (CCA), a lightweight analysis tool, to measure the similarity between these representations and word-level linguistic properties. We find that the maximal word-level linguistic content tends to be found in intermediate model layers, while some lower-level information like pronunciation is also retained in higher layers 
    
[^128]: 时空Tweedie模型在预测存在零膨胀和长尾旅行需求中的应用及不确定性量化

    Uncertainty Quantification via Spatial-Temporal Tweedie Model for Zero-inflated and Long-tail Travel Demand Prediction. (arXiv:2306.09882v1 [cs.LG])

    [http://arxiv.org/abs/2306.09882](http://arxiv.org/abs/2306.09882)

    本文提出了一种新型的时空Tweedie模型STTD，旨在解决高分辨率OD矩阵中稀疏和长尾特征的问题，并成功量化预测不确定性，具有很高的应用前景。

    

    传统的时空深度学习模型难以解决高分辨率OD矩阵中稀疏和长尾特征的问题，从而难以量化预测不确定性，而这对于交通管理至关重要。为了解决这些挑战，本文提出了一种新颖的方法：空间-Tweedie图神经网络（STTD）。STTD将Tweedie分布作为传统的“零膨胀”模型的有力替代品，并利用空间和时间嵌入来参数化旅行需求分布。我们使用真实世界的数据集进行评估，结果表明STTD在高分辨率场景下提供了准确的预测和精确的置信区间，具有优越性。

    crucial for transportation management. However, traditional spatial-temporal deep learning models grapple with addressing the sparse and long-tail characteristics in high-resolution O-D matrices and quantifying prediction uncertainty. This dilemma arises from the numerous zeros and over-dispersed demand patterns within these matrices, which challenge the Gaussian assumption inherent to deterministic deep learning models. To address these challenges, we propose a novel approach: the Spatial-Temporal Tweedie Graph Neural Network (STTD). The STTD introduces the Tweedie distribution as a compelling alternative to the traditional 'zero-inflated' model and leverages spatial and temporal embeddings to parameterize travel demand distributions. Our evaluations using real-world datasets highlight STTD's superiority in providing accurate predictions and precise confidence intervals, particularly in high-resolution scenarios.
    
[^129]: 强凸优化的次梯度法的原始对偶理论

    Some Primal-Dual Theory for Subgradient Methods for Strongly Convex Optimization. (arXiv:2305.17323v1 [math.OC])

    [http://arxiv.org/abs/2305.17323](http://arxiv.org/abs/2305.17323)

    本文提出了一种强凸优化的次梯度法原始对偶理论，可以实现简单的、最佳的停止准则和优化证明，同时可以适用于各种步长的选择和非Lipschitz病态问题，保证了这些方法次线性收敛速度。

    

    本文考虑强凸但潜在非光滑非Lipschitz优化的（随机）次梯度法。我们提供了新的等价对偶描述（类似于对偶平均）来描述经典的次梯度法，近端次梯度法和切换次梯度法。这些等价性能够以 $O(1/T)$ 的速度收敛，同时能够在强凸优化问题上分别还提供了经典原始间隙和前人未曾分析的对偶间隙保证。因此，我们的理论为这些经典方法提供了简单的、最佳的停止准则和优化证明，而不需要额外的计算成本。我们的结果适用于近乎所有的步长选择和一系列的非Lipschitz病态问题，对于在这些情况下，次梯度法的早期迭代可能会出现指数级的发散，而之前的研究没有处理过这种问题。即使在这种不良操作的情况下，我们的理论仍然确保和 bounds 了这些方法的次线性收敛速度。

    We consider (stochastic) subgradient methods for strongly convex but potentially nonsmooth non-Lipschitz optimization. We provide new equivalent dual descriptions (in the style of dual averaging) for the classic subgradient method, the proximal subgradient method, and the switching subgradient method. These equivalences enable $O(1/T)$ convergence guarantees in terms of both their classic primal gap and a not previously analyzed dual gap for strongly convex optimization. Consequently, our theory provides these classic methods with simple, optimal stopping criteria and optimality certificates at no added computational cost. Our results apply under nearly any stepsize selection and for a range of non-Lipschitz ill-conditioned problems where the early iterations of the subgradient method may diverge exponentially quickly (a phenomenon which, to the best of our knowledge, no prior works address). Even in the presence of such undesirable behaviors, our theory still ensures and bounds eventu
    
[^130]: 使Transformer在时间序列预测中再次卓越：通道对齐鲁棒双Transformer

    Make Transformer Great Again for Time Series Forecasting: Channel Aligned Robust Dual Transformer. (arXiv:2305.12095v1 [cs.LG])

    [http://arxiv.org/abs/2305.12095](http://arxiv.org/abs/2305.12095)

    本文提出了一种通道对齐鲁棒双Transformer模型，通过双Transformer结构和鲁棒损失函数的引入，解决了Transformer在时间序列预测中的关键缺点，显著提高了预测精度和效率。

    

    最近的研究表明，深度学习方法，尤其是Transformer和MLP，在时间序列预测方面具有巨大的优势。尽管在NLP和CV方面获得了成功，但许多研究发现，与MLP相比，Transformer在时间序列预测方面的效果不佳。在本文中，我们设计了一种特殊的Transformer，即通道对齐鲁棒双Transformer（CARD），以解决Transformer在时间序列预测中的关键缺点。首先，CARD引入了双Transformer结构，使其能够捕捉信号之间的时间相关性和多个变量在时间上的动态依赖。其次，我们引入了一种用于时间序列预测的鲁棒损失函数，以减轻潜在的过度拟合问题。这种新的损失函数基于预测不确定性加权预测在有限时间内的重要性。我们对多个长期和短期预测数据集进行的评估表明，CARD在精度和效率方面显著优于现有的方法。

    Recent studies have demonstrated the great power of deep learning methods, particularly Transformer and MLP, for time series forecasting. Despite its success in NLP and CV, many studies found that Transformer is less effective than MLP for time series forecasting. In this work, we design a special Transformer, i.e., channel-aligned robust dual Transformer (CARD for short), that addresses key shortcomings of Transformer in time series forecasting. First, CARD introduces a dual Transformer structure that allows it to capture both temporal correlations among signals and dynamical dependence among multiple variables over time. Second, we introduce a robust loss function for time series forecasting to alleviate the potential overfitting issue. This new loss function weights the importance of forecasting over a finite horizon based on prediction uncertainties. Our evaluation of multiple long-term and short-term forecasting datasets demonstrates that CARD significantly outperforms state-of-th
    
[^131]: 利用强化学习进行de novo药物设计

    Utilizing Reinforcement Learning for de novo Drug Design. (arXiv:2303.17615v1 [q-bio.BM])

    [http://arxiv.org/abs/2303.17615](http://arxiv.org/abs/2303.17615)

    本文开发了一个统一的框架，利用强化学习生成预测具有活性的新药分子。在需要结构多样性的情况下，同时使用高分和低分子来更新策略是有利的。使用所有生成的分子可以提高性能稳定性，而off-policy算法有潜力提高生成分子的结构多样性。

    

    基于深度学习的药物设计方法在生成具有特定性质的新药分子方面表现出了强大的潜力。最近的研究利用强化学习实现了字符串生成新分子的显著性能提升。本文中，我们开发了一个统一的框架，利用强化学习进行de novo药物设计，系统地研究了各种on-policy和off-policy 强化学习算法和重播缓冲区，学习基于RNN的策略，生成预测对于多巴胺受体DRD2具有活性的新分子。我们的研究结果表明，在需要结构多样性的情况下，同时使用高分和低分分子来更新策略是有利的。对于on-policy算法，使用所有生成的分子可以提高性能稳定性。此外，当重放高、中和低分子时，off-policy算法有潜力提高生成分子的结构多样性。

    Deep learning-based approaches for generating novel drug molecules with specific properties have gained a lot of interest in the last years. Recent studies have demonstrated promising performance for string-based generation of novel molecules utilizing reinforcement learning. In this paper, we develop a unified framework for using reinforcement learning for de novo drug design, wherein we systematically study various on- and off-policy reinforcement learning algorithms and replay buffers to learn an RNN-based policy to generate novel molecules predicted to be active against the dopamine receptor DRD2. Our findings suggest that it is advantageous to use at least both top-scoring and low-scoring molecules for updating the policy when structural diversity is essential. Using all generated molecules at an iteration seems to enhance performance stability for on-policy algorithms. In addition, when replaying high, intermediate, and low-scoring molecules, off-policy algorithms display the pot
    
[^132]: 贝叶斯自学习对比学习

    Bayesian Self-Supervised Contrastive Learning. (arXiv:2301.11673v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11673](http://arxiv.org/abs/2301.11673)

    本文提出了一种新的自监督对比损失——BCL损失，通过重要性权重修正导致的偏差，设计所需的采样分布来采样难以得到的真实负样本，修正伪负样本，采矿难负样本以提高编码器训练的准确性。

    

    近年来，对比学习在多个领域表现出了出色的应用，然而其自监督版本仍存在许多激动人心的挑战。由于负样本从未标记的数据集中选择，因此随机选择的样本可能实际上是一个伪负样本，导致编码器训练不正确。本文提出了一种新的自监督对比损失——BCL损失，它仍然使用未标记数据的随机样本，同时通过重要性权重修正导致的偏差。关键思想是在贝叶斯框架下设计所需的采样分布，从而采样难以得到的真实负样本。突出优点在于所需的采样分布是一个参数结构，其中具有位置参数以纠正伪负样本以及具有浓度参数以采矿难负样本。实验证明BCL损失的有效性和优越性。

    Recent years have witnessed many successful applications of contrastive learning in diverse domains, yet its self-supervised version still remains many exciting challenges. As the negative samples are drawn from unlabeled datasets, a randomly selected sample may be actually a false negative to an anchor, leading to incorrect encoder training. This paper proposes a new self-supervised contrastive loss called the BCL loss that still uses random samples from the unlabeled data while correcting the resulting bias with importance weights. The key idea is to design the desired sampling distribution for sampling hard true negative samples under the Bayesian framework. The prominent advantage lies in that the desired sampling distribution is a parametric structure, with a location parameter for debiasing false negative and concentration parameter for mining hard negative, respectively. Experiments validate the effectiveness and superiority of the BCL loss.
    
[^133]: StructCoder: 面向代码生成的结构感知Transformer模型

    StructCoder: Structure-Aware Transformer for Code Generation. (arXiv:2206.05239v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05239](http://arxiv.org/abs/2206.05239)

    本文提出了一个结构感知的Transformer模型，通过引入AST和DFG辅助任务，旨在解决现有代码生成模型在面对代码语法和语义时的训练不足问题。

    

    近年来，使用深度学习来自动化软件工程任务的兴趣日益增长。本文解决了代码生成问题，目标在于在给定不同语言或自然语言描述的源代码的情况下生成目标代码。针对代码语法和语义的严格理解和生成需要一种更为严谨的训练策略。出于这个动机，我们开发了一个编码器-解码器Transformer模型，在此模型中，编码器和解码器都明确地受过训练，以识别源代码和目标代码的语法和数据流。我们不仅通过利用源代码的语法树和数据流图使编码器结构感知，还通过引入两个新的辅助任务——AST（抽象语法）和DFG（数据流图）帮助解码器保留目标代码的语法和数据流。

    There has been a recent surge of interest in automating software engineering tasks using deep learning. This paper addresses the problem of code generation where the goal is to generate target code given source code in a different language or a natural language description. Most of the state-of-the-art deep learning models for code generation use training strategies primarily designed for natural language. However, understanding and generating code requires a more rigorous comprehension of the code syntax and semantics. With this motivation, we develop an encoder-decoder Transformer model where both the encoder and decoder are explicitly trained to recognize the syntax and data flow in the source and target codes, respectively. We not only make the encoder structure-aware by leveraging the source code's syntax tree and data flow graph, but we also support the decoder in preserving the syntax and data flow of the target code by introducing two novel auxiliary tasks: AST (Abstract Syntax
    
[^134]: 什么是公平性？哲学的思考与对fairML的影响

    What Is Fairness? Philosophical Considerations and Implications For FairML. (arXiv:2205.09622v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09622](http://arxiv.org/abs/2205.09622)

    本文探讨了公平性的哲学概念，提出了公平性和预测性能不是不可调和的对立面，并强调从数据收集到最终模型评估都需纳入伦理考虑。

    

    在公平性人工智能(fairML)领域，通过定义衡量模型公平性的度量和提出确保训练模型数据具有低公平性度量值的方法，来减轻人工智能(ML)产生的相关不公平性问题。然而，公平的基本概念，即"公平是什么"，很少被讨论，这造成了公平性研究在哲学领域几个世纪的讨论与近期被应用于机器学习领域之间的鸿沟。本文试图通过形式化一致性公平概念和将哲学思考转化为ADM系统中ML模型训练和评估的形式框架，来架起这一鸿沟。我们指出，不公平性问题可能已经存在，即使没有受保护性属性的存在，强调公平性和预测性能不是不可调和的对立面，而是前者实现的必要条件。我们提出的框架强调将伦理考虑纳入ML管道的所有阶段，从数据收集到最终部署模型的评估。

    A growing body of literature in fairness-aware ML (fairML) aspires to mitigate machine learning (ML)-related unfairness in automated decision making (ADM) by defining metrics that measure fairness of an ML model and by proposing methods that ensure that trained ML models achieve low values in those measures. However, the underlying concept of fairness, i.e., the question of what fairness is, is rarely discussed, leaving a considerable gap between centuries of philosophical discussion and recent adoption of the concept in the ML community. In this work, we try to bridge this gap by formalizing a consistent concept of fairness and by translating the philosophical considerations into a formal framework for the training and evaluation of ML models in ADM systems. We derive that fairness problems can already arise without the presence of protected attributes, pointing out that fairness and predictive performance are not irreconcilable counterparts, but rather that the latter is necessary to
    
[^135]: 使用潜在狄利克雷变分自编码器进行高光谱像素解混技术

    Hyperspectral Pixel Unmixing with Latent Dirichlet Variational Autoencoder. (arXiv:2203.01327v4 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2203.01327](http://arxiv.org/abs/2203.01327)

    本研究提出了一种使用潜在狄利克雷变分自编码器进行高光谱像素解混的方法，可以通过迁移学习范式训练模型并在真实数据上进行像素解混，取得了最新的最好结果。

    

    我们提出了一种用于高光谱像素解混的方法。该方法假设(1)丰度可以编码为狄利克雷分布，(2)成分的光谱可以表示为多元正态分布。该方法在变分自编码器框架下解决了丰度估计和成分提取问题，其中狄利克雷瓶颈层建模丰度，解码器执行成分提取。该方法还可以利用迁移学习范式，模型仅在包含感兴趣的一个或多个成分的线性组合像素的合成数据上进行训练。在这种情况下，我们从美国地质调查局光谱库中检索出成分(光谱)。然后，训练好的模型可以在包含生成合成数据所使用的一部分成分的“实际数据”上进行像素解混。该模型在多个评价指标上达到了最先进的结果。

    We present a method for hyperspectral pixel {\it unmixing}. The proposed method assumes that (1) {\it abundances} can be encoded as Dirichlet distributions and (2) spectra of {\it endmembers} can be represented as multivariate Normal distributions. The method solves the problem of abundance estimation and endmember extraction within a variational autoencoder setting where a Dirichlet bottleneck layer models the abundances, and the decoder performs endmember extraction. The proposed method can also leverage transfer learning paradigm, where the model is only trained on synthetic data containing pixels that are linear combinations of one or more endmembers of interest. In this case, we retrieve endmembers (spectra) from the United States Geological Survey Spectral Library. The model thus trained can be subsequently used to perform pixel unmixing on "real data" that contains a subset of the endmembers used to generated the synthetic data. The model achieves state-of-the-art results on sev
    
[^136]: 使用吸收比例缩放图的InfoMap算法在吸收随机漫步中的应用

    An adaptation of InfoMap to absorbing random walks using absorption-scaled graphs. (arXiv:2112.10953v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2112.10953](http://arxiv.org/abs/2112.10953)

    我们使用吸收比例缩放图和马尔可夫时间扫描改进了InfoMap算法，检测网络上密集连接的节点社区，此方法适应节点具有不同移除率的情况，社区结构与不考虑节点吸收率的方法可能有显著不同，并对易感-感染-恢复（SI）模型产生重要影响。

    

    InfoMap算法是一种用于检测网络上密集连接的“社区”节点的流行方法。本文将其应用与吸收随机漫步过程，并使用吸收比例缩放图和马尔可夫时间扫描来适应节点具有不同移除率的情况。改进后的InfoMap算法检测到的社区结构可能与不考虑节点吸收率的方法不同，并对易感-感染-恢复（SI）模型具有重要影响。

    InfoMap is a popular approach for detecting densely connected "communities" of nodes in networks. To detect such communities, InfoMap uses random walks and ideas from information theory. Motivated by the dynamics of disease spread on networks, whose nodes may have heterogeneous disease-removal rates, we adapt InfoMap to absorbing random walks. To do this, we use absorption-scaled graphs, in which the edge weights are scaled according to absorption rates, along with Markov time sweeping. One of our adaptations of InfoMap converges to the standard version of InfoMap in the limit in which the node-absorption rates approach $0$. The community structure that we obtain using our adaptations of InfoMap can differ markedly from the community structure that one detects using methods that do not take node-absorption rates into account. Additionally, we demonstrate that the community structure that is induced by local dynamics can have important implications for susceptible-infected-recovered (SI
    

