# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Automatic Piano Transcription with Hierarchical Frequency-Time Transformer.](http://arxiv.org/abs/2307.04305) | 提出了一种名为hFT-Transformer的自动音乐转录方法，它采用了两级分层频率-时间Transformer架构，并在MAPS和MAESTRO v3.0.0数据集上展现出了最先进的性能。 |
| [^2] | [Edge Storage Management Recipe with Zero-Shot Data Compression for Road Anomaly Detection.](http://arxiv.org/abs/2307.04298) | 提出了一种基于预训练自编码器的简单而有效的数据压缩方法，用于边缘存储管理和道路异常检测，可以高效地存储高保真音频样本。 |
| [^3] | [Generalizing Graph ODE for Learning Complex System Dynamics across Environments.](http://arxiv.org/abs/2307.04287) | GG-ODE是一个机器学习框架，用于跨环境学习复杂系统动态，利用神经普通微分方程和图神经网络的参数化方式。 |
| [^4] | [Assessing the efficacy of large language models in generating accurate teacher responses.](http://arxiv.org/abs/2307.04274) | 本研究评估了大型语言模型在生成准确的教师回答中的能力。实验结果表明，在教师-学生聊天室语料库子集上，GPT-4的效果优于其他模型。研究还指出样本抽样和代表性等数据集特征可能影响生成模型的效果。 |
| [^5] | [ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey.](http://arxiv.org/abs/2307.04251) | ChatGPT是由OpenAI创建的一种大型语言模型（LLM），它在自然语言处理（NLP）领域引起了革命性的变革。它是第一个实现公众大规模与生成式人工智能（GAI）互动的关键技术，也引发了类似技术的研究兴趣和应用探索。 |
| [^6] | [Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks.](http://arxiv.org/abs/2307.04228) | 本论文提出了一种高效的贝叶斯行程时间层析成像方法，利用敏感性信息的多项式混沌展开和深度生成网络，以处理地质复杂性先验的挑战。 |
| [^7] | [Hierarchical Autoencoder-based Lossy Compression for Large-scale High-resolution Scientific Data.](http://arxiv.org/abs/2307.04216) | 本论文提出了一种基于分层自编码器的神经网络模型，能够显著压缩大规模高分辨率科学数据，并保持高重建质量。 |
| [^8] | [Generalized Action-based Ball Recovery Model using 360$^\circ$ data.](http://arxiv.org/abs/2307.04215) | 本研究通过使用360度数据，提出了一个广义的基于动作的球权恢复模型，探究球队在失去球权后如何尽快恢复球权，球队位置对球权恢复的影响以及在压力下更易失误的球员等问题。 |
| [^9] | [Investigating the Edge of Stability Phenomenon in Reinforcement Learning.](http://arxiv.org/abs/2307.04210) | 本研究探究了强化学习中的稳定边界现象，发现即使在非政策深度强化学习中，稳定边界现象仍然存在，这有助于我们更好地理解和优化深度强化学习算法。 |
| [^10] | [On the Challenges of Deploying Privacy-Preserving Synthetic Data in the Enterprise.](http://arxiv.org/abs/2307.04208) | 本文研究了在企业中部署保护隐私的合成数据的挑战，重点关注个人和高度敏感数据所引起的隐私问题。鉴别出了40多个挑战并将其系统化，提出了企业可以采用的战略和系统方法来应对这些挑战。 |
| [^11] | [Extending the Forward Forward Algorithm.](http://arxiv.org/abs/2307.04205) | 这个论文扩展了前向前向算法，首先在IMDb数据集上进行了情感分析任务，其次引入了金字塔优化策略来改进损失阈值，最后通过参数可视化得出了一些重要的洞察。 |
| [^12] | [Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory.](http://arxiv.org/abs/2307.04204) | 本文通过实证研究证明了梯度下降轨迹上的稳定边缘现象，并且对于特定的网络结构进行了轨迹对齐分析，建立了渐进尖锐化和稳定边缘现象，扩展了当前文献的研究结果。 |
| [^13] | [On the sample complexity of estimation in logistic regression.](http://arxiv.org/abs/2307.04191) | 本文研究了逻辑回归模型在标准正态协变量下的参数估计样本复杂度，发现样本复杂度曲线在逆温度方面有两个转折点，明确划分了低、中和高温度区域。 |
| [^14] | [Latent Graph Attention for Enhanced Spatial Context.](http://arxiv.org/abs/2307.04149) | 本文提出了一种称为潜在图注意力（LGA）的模块化框架，用于增强图像中的全局上下文。LGA在计算上简洁且稳定，能够在小规模体系结构中实现接近大规模体系结构的性能，使得轻量级体系结构在边缘设备上更加实用。 |
| [^15] | [A Survey and Approach to Chart Classification.](http://arxiv.org/abs/2307.04147) | 本文调查了当前图表分类技术的研究现状，并对基于机器学习、卷积神经网络和Transformer的方法进行了比较性能分析，以提高对图表的自动理解能力。 |
| [^16] | [On The Impact of Machine Learning Randomness on Group Fairness.](http://arxiv.org/abs/2307.04138) | 本文研究了在机器学习中，随机性对群体公平性的影响。研究发现，群体公平性指标的方差主要来自于在代表性不足的群体上的学习过程的高易变性，其中最主要的随机性来源是训练期间数据顺序的随机性。基于这些发现，可以通过改变数据顺序来控制模型的群体级准确性，而几乎不影响模型的整体性能。 |
| [^17] | [Carbon-Efficient Neural Architecture Search.](http://arxiv.org/abs/2307.04131) | 本文介绍了一种碳效率的神经架构搜索方法（CE-NAS），通过动态平衡能效抽样和能耗评估任务，实现了更好的碳和搜索效率。 |
| [^18] | [A Deep Learning Framework for Solving Hyperbolic Partial Differential Equations: Part I.](http://arxiv.org/abs/2307.04121) | 本研究开发了一种物理信息深度学习框架，用于近似解非线性PDEs，特别是具有支配性双曲特性的PDEs。该框架在不具有先验知识的情况下能够处理冲击或间断，并能够自然处理边界条件、熵条件和正则性要求。 |
| [^19] | [FILM: How can Few-Shot Image Classification Benefit from Pre-Trained Language Models?.](http://arxiv.org/abs/2307.04114) | 本文提出了一种使用预训练语言模型的新型少样本学习框架，通过对比学习，使用文本嵌入和度量模块来提高图像分类性能和可迁移性。 |
| [^20] | [Learning Space-Time Continuous Neural PDEs from Partially Observed States.](http://arxiv.org/abs/2307.04110) | 该论文介绍了一种从部分观测状态学习时空连续神经PDE的新方法，该方法通过新颖的编码器设计和高效的概率框架，克服了先前方法的局限，实现了对复杂部分观测数据的网格独立建模。 |
| [^21] | [Towards Assumption-free Bias Mitigation.](http://arxiv.org/abs/2307.04105) | 本文提出了一种无假设的偏差缓解方法，用于解决机器学习模型对特定人口群体显示歧视和不公平预测行为的问题。 |
| [^22] | [A generative flow for conditional sampling via optimal transport.](http://arxiv.org/abs/2307.04102) | 本论文提出了一种通过解决最优输运问题来描述条件分布的非参数生成模型，该模型使用块三角输运映射将参考样本迭代映射到目标样本，从而克服了参数偏差和基于梯度的优化器的限制。 |
| [^23] | [GNP Attack: Transferable Adversarial Examples via Gradient Norm Penalty.](http://arxiv.org/abs/2307.04099) | 本文提出了一种通过梯度范数惩罚实现可转移的对抗样本的方法，通过攻击多个深度学习模型和防御方法的实验证明，该方法非常有效。同时，该方法还可以与其他梯度方法结合使用，以实现更强大的攻击。 |
| [^24] | [Restricted Generative Projection for One-Class Classification and Anomaly Detection.](http://arxiv.org/abs/2307.04097) | 该论文提出了一种简单的框架，用于单类分类和异常检测。作者通过学习一种映射，将训练数据的未知分布转化为已知目标分布，以实现对异常数据的可靠区分。 |
| [^25] | [Class-Incremental Mixture of Gaussians for Deep Continual Learning.](http://arxiv.org/abs/2307.04094) | 这项工作提出了将高斯混合模型与持续学习框架相结合的类别增量方法，并成功地在深度特征提取器下进行了联合优化和调整。 |
| [^26] | [Properly Learning Decision Trees with Queries Is NP-Hard.](http://arxiv.org/abs/2307.04093) | 通过查询正确学习决策树被证明是一个NP难问题，这就填补了学习理论中长期存在的一个空白，并引入了一种简化和加强决策树最小化问题下界的方法。 |
| [^27] | [DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs.](http://arxiv.org/abs/2307.04090) | 本论文提出了一种利用语义知识图自动创建政策辩论案例的方法，通过在争论的语义知识图上进行限制最短路径遍历，有效构建高质量的辩论案例。研究结果表明，在美国竞赛辩论中，利用这种方法显著改进了已有数据集DebateSum，并贡献了新的例子和有用的元数据。通过使用txtai语义搜索和知识图工具链，创建和贡献了9个语义知识图，同时提出了一种独特的评估方法来确定哪个知识图更适合政策辩论案例生成。 |
| [^28] | [Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance.](http://arxiv.org/abs/2307.04081) | 本论文提出了一种通过自校准分类器引导的方法改进基于评分的条件生成模型，以提高使用少量标记数据的准确性和性能。通过将分类器作为无条件生成模型的另一种视角，并利用标记和未标记数据来校准分类器，实验证实该方法的有效性。 |
| [^29] | [Towards Fast and Scalable Private Inference.](http://arxiv.org/abs/2307.04077) | 私密推理是一种应用于神经网络的新兴技术，可以通过解决各种保护隐私计算的开销问题，实现用户数据的机密性和控制性保护。 |
| [^30] | [Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data.](http://arxiv.org/abs/2307.04075) | 本研究提出了一种基于注意力机制的无监督对比学习框架（AMUCL），用于分析癌症多组学数据并识别癌症新亚型。通过多头注意力机制和解耦对比学习模型（DMACL），该方法能够深度提取多组学数据特征并进行亚型聚类。 |
| [^31] | [Large-scale global optimization of ultra-high dimensional non-convex landscapes based on generative neural networks.](http://arxiv.org/abs/2307.04065) | 本论文提出了一种基于深度生成网络的非凸优化算法，通过演化网络输出分布函数，实现了在超高维度景观中的有效搜索。实验证明该方法在较少的函数评价次数下表现优于其他算法。 |
| [^32] | [Bidirectional Attention as a Mixture of Continuous Word Experts.](http://arxiv.org/abs/2307.04057) | 双向注意力模型具有混合专家权重，类似于连续词袋模型（CBOW）的统计模型，它在大型语言模型中起到了重要作用。 |
| [^33] | [Manifold Filter-Combine Networks.](http://arxiv.org/abs/2307.04056) | 这篇论文介绍了一类称为流形滤波-组合网络的大型流形神经网络。作者提出了一种基于构建数据驱动图的方法来实现这种网络，并提供了收敛到连续极限的充分条件，其收敛速度不依赖于滤波器数量。 |
| [^34] | [Contextual Dynamic Pricing with Strategic Buyers.](http://arxiv.org/abs/2307.04055) | 本文研究了具有策略性买家的情境动态定价问题，提出了一种策略动态定价策略，将买家的策略行为纳入在线学习中，以最大化卖方的累计收益。 |
| [^35] | [Learning to Group Auxiliary Datasets for Molecule.](http://arxiv.org/abs/2307.04052) | 本文提出了一种名为MolGroup的方法，通过将图结构相似性和任务相似性相结合，预测每个辅助分子数据集的潜在好处，以解决合作使用辅助数据集时的负迁移问题。 |
| [^36] | [Optimization-based Learning for Dynamic Load Planning in Trucking Service Networks.](http://arxiv.org/abs/2307.04050) | 本文为快递运营中的负载规划问题提出了一种基于优化的学习方法，全面考虑了负载和流程规划的挑战，并开发了一个决策支持工具。研究发现在网络中存在大量的对称性，导致优化求解器返回不同的解决方案，降低了规划人员对优化求解的信任度。 |
| [^37] | [Parallel Algorithms Align with Neural Execution.](http://arxiv.org/abs/2307.04049) | 这项研究发现，与顺序算法相比，并行算法能充分利用神经算法推理器的计算能力，从而减少训练时间并获得更好的预测性能。 |
| [^38] | [Sup-Norm Convergence of Deep Neural Network Estimator for Nonparametric Regression by Adversarial Training.](http://arxiv.org/abs/2307.04042) | 我们展示了使用对抗训练的深度神经网络估计器在非参数回归中的超范数收敛性。我们发现普通的对抗训练使得神经估计器不一致，但通过所提出的带修正的对抗训练，深度神经网络估计器在超范数意义下达到最优速率。我们的实验证实了这些理论发现。 |
| [^39] | [Designing a Direct Feedback Loop between Humans and Convolutional Neural Networks through Local Explanations.](http://arxiv.org/abs/2307.04036) | 这篇论文设计了DeepFuse，它是第一个实现用户和CNN之间直接反馈循环的交互设计，通过局部解释帮助CNN工程师系统地诊断和修改CNN的脆弱性。 |
| [^40] | [Learning Variational Neighbor Labels for Test-Time Domain Generalization.](http://arxiv.org/abs/2307.04033) | 本文提出了一种用于测试时领域泛化的方法，通过在测试时使用概率伪标签和变分邻居标签来推广源域训练的模型到目标领域，以提高模型的鲁棒性和准确性。 |
| [^41] | [Measuring the Success of Diffusion Models at Imitating Human Artists.](http://arxiv.org/abs/2307.04028) | 这项研究评估了扩散模型在模仿人类艺术家方面的成功程度，并提出将版权责任与模型能力联系起来可能是有用的。研究通过使用Contrastive Language-Image Pretrained (CLIP)编码器对图像进行分类来衡量模型模仿特定艺术家的能力。 |
| [^42] | [Robust Ranking Explanations.](http://arxiv.org/abs/2307.04024) | 这篇论文提出了一种名为R2ET的算法，通过计算解释厚度来衡量排名稳定性，并设计出可最大化解释厚度并锚定排名靠前特征的算法。实验证明R2ET在面对对抗攻击时具有更高的解释鲁棒性和保持准确性能力。 |
| [^43] | [Robust Learning-Based Incipient Slip Detection using the PapillArray Optical Tactile Sensor for Improved Robotic Gripping.](http://arxiv.org/abs/2307.04011) | 本文提出了一种使用PapillArray光学触觉传感器的学习方法，能够高效准确地检测早期滑动，实现了95.6%的检测成功率。通过引入数据增强方法，在不同环境下也保持了鲁棒的性能，成功率达到96.8%。 |
| [^44] | [Understanding the Efficacy of U-Net & Vision Transformer for Groundwater Numerical Modelling.](http://arxiv.org/abs/2307.04010) | 该论文通过对U-Net、U-Net + ViT和FNO等多种机器学习模型在地下水系统中的时间相关正向建模进行全面比较，发现U-Net和U-Net + ViT模型在准确性和效率方面优于FNO，尤其是在稀疏数据场景中。这些结果证明了U-Net模型在数据稀缺的实际地下水建模应用中的潜力。 |
| [^45] | [Polynomial Width is Sufficient for Set Representation with High-dimensional Features.](http://arxiv.org/abs/2307.04001) | 本研究通过两种集合元素嵌入层的探索，证明了多项式宽度对于高维特征的集合表示足够，并揭示了之前分析中的局限性。 |
| [^46] | [Efficient Model-Free Exploration in Low-Rank MDPs.](http://arxiv.org/abs/2307.03997) | 提出了第一个计算高效、无模型的低秩MDPs探索算法，允许通用函数逼近，不需要额外的结构假设。 |
| [^47] | [Building and Road Segmentation Using EffUNet and Transfer Learning Approach.](http://arxiv.org/abs/2307.03980) | 本论文提出了一种使用EffUNet和迁移学习方法进行建筑和道路分割的新架构。利用这种方法，在马萨诸塞州建筑物和道路数据集上取得了令人满意的分数。 |
| [^48] | [Fault Monitoring in Passive Optical Networks using Machine Learning Techniques.](http://arxiv.org/abs/2307.03945) | 本文提出了基于机器学习技术的无源光网络故障监测方法，并使用实验数据进行了验证。 |
| [^49] | [Rosko: Row Skipping Outer Products for Sparse Matrix Multiplication Kernels.](http://arxiv.org/abs/2307.03930) | Rosko提出了一种稀疏矩阵乘法（SpMM）内核方法，通过使用行跳过外积实现深度神经网络（DNN）的计算和内存访问需求的减少。它可以适应不同硬件特性，并与其他外积调度方法相结合，胜过自动调优和搜索的解决方案，并能在不同稀疏度的矩阵上实现高效的计算。 |
| [^50] | [Fairness-Aware Graph Neural Networks: A Survey.](http://arxiv.org/abs/2307.03929) | 该论文调查了公平感知图神经网络，讨论了提高GNNs公平性的技术，并介绍了公平度评估指标分类法。 |
| [^51] | [Fast Empirical Scenarios.](http://arxiv.org/abs/2307.03927) | 该论文提出了两种快速的经验场景提取算法，一种识别之前未观察到的场景并提供场景的协方差矩阵表示，另一种从已实现的世界状态中选择重要的数据点，并与高阶样本矩一致，这些算法计算效率高且适用于一致的基于场景的建模和高维数值积分。 |
| [^52] | [Training Physics-Informed Neural Networks via Multi-Task Optimization for Traffic Density Prediction.](http://arxiv.org/abs/2307.03920) | 该论文提出了一种基于多任务优化的PINN训练框架，用于交通密度预测，该框架通过创建和解决多个辅助任务来辅助求解主任务，实现了神经网络在面对有限训练数据时的泛化性能。 |
| [^53] | [Incorporating Deep Q -- Network with Multiclass Classification Algorithms.](http://arxiv.org/abs/2307.03908) | 本研究探索了将Deep Q-Network与多类分类算法相结合以提高分类准确性的方法，并在预测财务困境等领域进行了应用。 |
| [^54] | [ScriptWorld: Text Based Environment For Learning Procedural Knowledge.](http://arxiv.org/abs/2307.03906) | ScriptWorld是一个基于文本的环境，用于教授智能体关于真实世界日常任务和常识知识。它是第一个采用脚本数据集设计的互动式基于文本的游戏框架，在10个日常活动中提供了游戏环境，并通过对环境的详细分析，展示了引入脚本式真实世界任务可以有效提供常识知识并提高强化学习智能体的学习性能。 |
| [^55] | [Feature selection simultaneously preserving both class and cluster structures.](http://arxiv.org/abs/2307.03902) | 本文提出了一种基于神经网络的特征选择方法，同时考虑了类别判别和聚类结构保持的目标。实验证明该方法可以在具有显著类别和聚类结构差异的数据集上取得良好的分类和聚类性能。 |
| [^56] | [Active Learning in Physics: From 101, to Progress, and Perspective.](http://arxiv.org/abs/2307.03899) | 本文介绍了物理学中的主动学习方法，该方法利用未标记的样本并通过专家的注释来提升模型性能。最近，该方法在物理学领域引起了越来越多的关注。另外，本文还探讨了主动学习与量子机器学习的整合，设想了这两个领域的协同融合。 |
| [^57] | [Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining.](http://arxiv.org/abs/2307.03887) | 本研究通过引入奖励重新加权、重选和重新训练的方法，改进了原型零件网络的分类效果，解决了学习从图像的虚假或不一致的部分进行分类的问题。 |
| [^58] | [On Regularization and Inference with Label Constraints.](http://arxiv.org/abs/2307.03886) | 本文研究了在机器学习中将先验知识和符号规则以标签约束的形式表达的方法。通过比较正则化和约束推理两种常见的编码标签约束的策略，发现正则化缩小了泛化差距但引入了对次优模型的偏置，而约束推理通过纠正模型的违规行为将违规行为转化为优势。进一步探索了将这两种方法结合使用的可能，并提出了用约束推理来补偿正则化引入的偏置的条件，旨在提高模型复杂性和最优风险。 |
| [^59] | [Noisy Tensor Ring approximation for computing gradients of Variational Quantum Eigensolver for Combinatorial Optimization.](http://arxiv.org/abs/2307.03884) | 使用噪声张量环逼近方法计算组合优化的变分量子本征求解器梯度，解决了VQE算法受限于经典不可处理的梯度问题，提高了可扩展性。 |
| [^60] | [Large Language Models for Supply Chain Optimization.](http://arxiv.org/abs/2307.03875) | 这项研究研究了利用大型语言模型（LLMs）来帮助解释和解读供应链优化结果的方法。他们设计了一个框架，可以接受普通文本查询作为输入，并输出关于底层优化结果的洞察。通过定量回答假设情况，该框架在不放弃最先进的组合优化技术的情况下帮助企业运营者更好地理解和信任优化结果。 |
| [^61] | [Domain Adaptation using Silver Standard Labels for Ki-67 Scoring in Digital Pathology: A Step Closer to Widescale Deployment.](http://arxiv.org/abs/2307.03872) | 本研究提出了一种领域自适应流程，通过使用生成的银标准标签将目标领域数据与源领域数据相结合，以提高Ki-67评分的深度学习系统在领域外数据上的性能。 |
| [^62] | [When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment.](http://arxiv.org/abs/2307.03864) | Transformer在强化学习中的作用是增强记忆能力而不是改进信用分配。 |
| [^63] | [Memory-Immersed Collaborative Digitization for Area-Efficient Compute-in-Memory Deep Learning.](http://arxiv.org/abs/2307.03863) | 本论文提出了一种基于内存的合作数字化方案，用于减少深度学习推理中传统模拟数字转换器(ADC)的面积开销，提升并行性和减少外部内存访问。该方案通过在内存中形成内存内的数字-模数转换器(DAC)来实现面积高效的数字化，并且利用合作数字化来提高计算效率。 |
| [^64] | [Reinforcement and Deep Reinforcement Learning-based Solutions for Machine Maintenance Planning, Scheduling Policies, and Optimization.](http://arxiv.org/abs/2307.03860) | 基于强化学习和深度强化学习的机器维护规划、调度策略和优化解决方案可以通过利用连续的条件监控数据来开发智能维护规划器，实现降低成本、延长资产寿命和确保工作场所安全等目标。 |
| [^65] | [inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data.](http://arxiv.org/abs/2307.03854) | inTformer是一种基于时间嵌入的关注机制Transformer，用于使用连接车辆数据预测路口事故可能性。与其他深度学习模型相比，inTformer具有处理长期依赖性、并行处理数据序列以及解决梯度消失问题的优势。 |
| [^66] | [Optimal Learners for Realizable Regression: PAC Learning and Online Learning.](http://arxiv.org/abs/2307.03848) | 本论文研究了可实现回归问题的PAC学习和在线学习的统计复杂度，并提出了对于可学习性的必要条件和充分条件。 |
| [^67] | [RADAR: Robust AI-Text Detection via Adversarial Learning.](http://arxiv.org/abs/2307.03838) | 本论文提出了一种名为RADAR的新框架，通过对抗性学习实现了鲁棒的AI文本检测，以解决当前AI文本检测器对于大语言模型的改写不具备鲁棒性的问题。 |
| [^68] | [Effect of Intensity Standardization on Deep Learning for WML Segmentation in Multi-Centre FLAIR MRI.](http://arxiv.org/abs/2307.03827) | 本研究评估了多种MRI强度标准化方法在多中心FLAIR MRI白质病变分割中的效果，并提出了一个集成模型，将不同方法的预测结果进行组合。结果表明，强度标准化可以提高深度学习模型在新机构数据上的性能。 |
| [^69] | [A Combinatorial Characterization of Online Learning Games with Bounded Losses.](http://arxiv.org/abs/2307.03816) | 这项研究提出了一个新的尺度敏感的组合维度，称为顺序极小极大维度，并通过对有界损失的在线学习游戏进行研究，给出了对向量值回归和多标签分类的在线可学习性的紧密定量刻画。 |
| [^70] | [Controlling Chaotic Maps using Next-Generation Reservoir Computing.](http://arxiv.org/abs/2307.03813) | 这项工作将非线性系统控制技术与储层计算相结合，成功地在混沌H\'enon映射上展示了控制器对于控制系统的稳定、固定点的控制和任意期望状态的控制的性能，并且只需10个数据点进行训练，单次迭代就能控制到期望轨迹，并且具有鲁棒性。 |
| [^71] | [Formulation Graphs for Mapping Structure-Composition of Battery Electrolytes to Device Performance.](http://arxiv.org/abs/2307.03811) | 该论文提出了一种深度学习模型，Formulation Graph Convolution Network（F-GCN），它可以将电池电解质的结构组成关系映射到整个液体配方的性能，从而加快新化合物的发现和应用。 |
| [^72] | [URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates.](http://arxiv.org/abs/2307.03810) | URL基准是一个评估预训练模型可转移性和不确定性估计的方式，研究发现专注于表示本身不确定性或直接估计预测风险的方法效果优于基于概率的方法。 |
| [^73] | [A Theoretical Perspective on Subnetwork Contributions to Adversarial Robustness.](http://arxiv.org/abs/2307.03803) | 本文提出了一个新的理论框架，研究了子网络的对抗鲁棒性对整个网络的鲁棒性的贡献，并引入了半鲁棒性的概念进行度量。这有助于更好地理解对抗性攻击并促进更高效的对抗性训练。 |
| [^74] | [CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution.](http://arxiv.org/abs/2307.03798) | 本文展示了对比式语言-图像预训练（CLIP）模型的脆弱性，通过挖掘生成模型的潜在空间可以找到欺骗主图像，这些图像在许多不同的提示下能欺骗CLIP模型，而对人类来说是无法认出的。欺骗主图像在少量图像标题上的训练上可能适用于更多数量的语义相关的标题。两种可能的缓解策略被评估，并发现脆弱性与对比式预训练中的模态差距密切相关。 |
| [^75] | [Neural Abstraction-Based Controller Synthesis and Deployment.](http://arxiv.org/abs/2307.03783) | 本论文提出了一种使用神经网络表示来减轻抽象技术中高内存需求的方法，用于合成正确构建的控制器。此外，该论文还提供了一种新颖的训练算法，用于部署这些控制器。 |
| [^76] | [For Women, Life, Freedom: A Participatory AI-Based Social Web Analysis of a Watershed Moment in Iran's Gender Struggles.](http://arxiv.org/abs/2307.03764) | 本文通过对波斯语Twitter话语的计算分析，估算了马莎·阿米尼去世后伊朗社会对性别平等立场的转变。研究采用参与式人工智能方法，涉及伊朗女性在标注过程中的积极角色。结果发现，马莎·阿米尼的死亡引发了波斯语话语的极化，积极推文数量略多于负面推文数量。此外，研究还观察到政府和抗议者在Twitter上的存在差异。 |
| [^77] | [Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks.](http://arxiv.org/abs/2307.03761) | 该论文提出了一种基于图的方法，命名为DyGATAD，用于在异构传感器网络中进行异常检测。该方法利用动态图注意力机制来识别集体异常行为，其中异常行为可能由于系统内部相互关系的变化引起。这在工业物联网监控系统中具有重要的实际应用价值。 |
| [^78] | [A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection.](http://arxiv.org/abs/2307.03759) | 这项综述介绍了图神经网络在时间序列分析中的应用，包括预测、分类、异常检测和插值。图神经网络能够显式地建模时间序列和变量之间的关系，为时间序列数据分析带来了新的方法和技术。 |
| [^79] | [Federated Learning over a Wireless Network: Distributed User Selection through Random Access.](http://arxiv.org/abs/2307.03758) | 本研究提出了一种利用随机接入中的无线电资源竞争机制的网络内在方法来实现分布式用户选择进行联邦学习。通过操纵竞争窗口大小和使用训练数据偏差作为优先级依据，可以快速实现高效的收敛，同时保证公平性。 |
| [^80] | [FITS: Modeling Time Series with $10k$ Parameters.](http://arxiv.org/abs/2307.03756) | FITS是一种轻量而强大的时间序列分析模型，通过在复杂频率域中进行插值操作，丢弃对时间序列数据影响微小的高频分量，实现了与最先进模型相当的性能，并且具有较小的模型参数数量，适用于边缘设备。 |
| [^81] | [Programmable Synthetic Tabular Data Generation.](http://arxiv.org/abs/2307.03577) | 这项工作介绍了ProgSyn，第一个可编程的合成表格数据生成算法，它允许对生成的数据进行全面的自定义，并且通过预训练和微调生成模型来确保高质量的数据和遵守自定义规范。 |
| [^82] | [ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers.](http://arxiv.org/abs/2307.03493) | ITA是一种基于量化的Transformer的能效高的Attention和Softmax加速器。通过利用8位量化和仅基于整数值的创新Softmax实现，ITA实现了高能效的推理，在16.9 TOPS/W的能效上超过了最先进的Transformer加速器，并在5.93 TOPS/mm$^2$的面积效率上超越了它们。 |
| [^83] | [Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs.](http://arxiv.org/abs/2307.03393) | 本文探索了大规模语言模型（LLMs）在图学习中的潜力，并尝试了两种不同的流程：将LLMs作为增强器通过海量知识来增强节点的文本属性，并使用图神经网络（GNNs）生成预测，以及直接使用LLMs作为独立的预测器。 |
| [^84] | [Generalizing Backpropagation for Gradient-Based Interpretability.](http://arxiv.org/abs/2307.03056) | 本论文在深度神经网络的特征解释中，泛化了反向传播算法，以便更好地理解梯度图的可解释统计数据，如最高加权路径和熵。作者通过在合成数据集上的评估和应用于BERT的实验中验证了该方法的有效性。 |
| [^85] | [PUFFIN: A Path-Unifying Feed-Forward Interfaced Network for Vapor Pressure Prediction.](http://arxiv.org/abs/2307.02903) | PUFFIN是一种结合迁移学习和基于领域知识的归纳偏差节点的机器学习框架，用于改进蒸汽压力预测。通过利用归纳偏差和图嵌入的迁移学习，PUFFIN在预测中胜过不使用归纳偏差或使用通用描述符的替代策略。 |
| [^86] | [Learning to Solve Tasks with Exploring Prior Behaviours.](http://arxiv.org/abs/2307.02889) | 本文提出了一种基于示例的控制方法（IRDEC），通过内在奖励驱动以及探索获得先前行为，并与示范中的任务特定行为连接，从而解决具有稀疏奖励的任务。这种方法在三个导航任务上的性能优于其他基线方法。 |
| [^87] | [Data-driven Predictive Latency for 5G: A Theoretical and Experimental Analysis Using Network Measurements.](http://arxiv.org/abs/2307.02329) | 本研究利用实际网络数据对5G网络中的预测性延迟进行了全面分析，并提出了以Hypoexponential分布为基础的用户面延迟的分析表达式。通过机器学习领域的贝叶斯学习和图机器学习技术，我们进行了概率回归、异常检测和预测性预测的实验。测试结果表明，该预测框架适用于不同情景下的移动性、城市交通和社交聚会。 |
| [^88] | [Set Learning for Accurate and Calibrated Models.](http://arxiv.org/abs/2307.02245) | 提出了一种集合学习方法(OKO)来解决机器学习中的模型过度自信和校准不良问题，通过最小化集合的交叉熵误差，从而提高准确性和校准效果，并在有限的训练数据和类别不平衡情况下表现出更好的结果。 |
| [^89] | [ChiENN: Embracing Molecular Chirality with Graph Neural Networks.](http://arxiv.org/abs/2307.02198) | 这篇论文介绍了一种名为ChiENN的图神经网络层，通过引入对手性敏感的信息传递方案，使得图神经网络能够区分化学化合物和其对映体之间的差异。在化学信息学中，这种方法在药物发现领域的手性敏感分子性质预测任务中表现出超越当前最先进方法的优异性能。 |
| [^90] | [SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection.](http://arxiv.org/abs/2307.01750) | 本文提供了针对单领域广义目标检测的新框架SRCD，通过学习和维护自增广义跨领域样本的语义结构，提高模型的泛化能力。 |
| [^91] | [Online Learning and Solving Infinite Games with an ERM Oracle.](http://arxiv.org/abs/2307.01689) | 这项工作提出了一种仅依赖ERM预言机调用的在线学习算法，该算法在可实现情况下具有有限的遗憾，并在不可知情况下具有亚线性增长的遗憾。同时，还提供了类似的结果用于非参数博弈环境中的学习算法，即仅依赖最佳响应预言机的学习算法，并收敛到近似极小-极大均衡点。 |
| [^92] | [vONTSS: vMF based semi-supervised neural topic modeling with optimal transport.](http://arxiv.org/abs/2307.01226) | vONTSS是一种基于vMF和最优传输的半监督神经主题建模方法，它在分类准确率和多样性方面优于其他方法，并且支持无监督主题建模。实验证明，vONTSS比最近的NTM更快。 |
| [^93] | [Shared Growth of Graph Neural Networks via Free-direction Knowledge Distillation.](http://arxiv.org/abs/2307.00534) | 本文提出了一种基于自由方向知识蒸馏的图神经网络共同增长的框架，通过强化学习同时训练两个较浅的GNN模型，实现了它们之间的知识交流和共享。 |
| [^94] | [GenRec: Large Language Model for Generative Recommendation.](http://arxiv.org/abs/2307.00457) | 本文介绍了一种基于大型语言模型的创新推荐系统方法GenRec，通过直接生成目标推荐项而不是计算排名分数，利用LLM的表达能力和理解能力来生成相关推荐。 |
| [^95] | [Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis.](http://arxiv.org/abs/2306.17181) | 本论文提出了一种使用生成对抗网络（GAN）生成连续文本嵌入空间的方法（TESGAN），以解决传统GAN在自然语言生成中的限制。这种方法通过引入连续的文本嵌入空间取代离散的标记，使得生成器在通过反向传播更新梯度时更加有效。 |
| [^96] | [On the Predictive Accuracy of Neural Temporal Point Process Models for Continuous-time Event Data.](http://arxiv.org/abs/2306.17066) | 该论文通过全面大规模实验研究，系统评估了最先进的神经TPP模型在预测准确性方面的效果，并发现了关键因素。 |
| [^97] | [OSP: Boosting Distributed Model Training with 2-stage Synchronization.](http://arxiv.org/abs/2306.16926) | OSP是一种新的分布式模型训练方法，通过使用两阶段同步和本地梯度修正来提高通信效率，避免了精度损失。 |
| [^98] | [Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor Kernel Density Approach.](http://arxiv.org/abs/2306.16906) | 本论文介绍了一种新的数值数据填补方法，通过将最近邻估计和高斯核密度估计结合，能够有效处理多模态数据集中的缺失值，并提供比当前方法更高的概率估计。 |
| [^99] | [DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing.](http://arxiv.org/abs/2306.14435) | DragDiffusion是一个利用扩散模型进行交互式点基图像编辑的方法，通过优化扩散潜在实现精确的空间控制，以提高实际场景中的应用性。 |
| [^100] | [Sparse Modular Activation for Efficient Sequence Modeling.](http://arxiv.org/abs/2306.11197) | 本论文引入了稀疏模块激活 (SMA) 机制，用于高效的序列建模。这种机制可以动态地稀疏激活序列元素的子模块，减少计算和内存消耗。 |
| [^101] | [Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances.](http://arxiv.org/abs/2306.10006) | 本文提出了一种非监督学习的动画方法，可以通过文本或语音输入，实现基于真实动作表演的面部动画，并且可以不同程度地学习并合成不同的表演风格。 |
| [^102] | [Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis.](http://arxiv.org/abs/2306.09417) | Diff-TTSG是第一个基于扩散的概率模型，用于联合学习合成语音和手势，相比于先前最新技术的非概率方法，它可以更好地捕捉人类讲话和运动的变化，产生更逼真和多样化的集成语音和手势合成。 |
| [^103] | [Agent market orders representation through a contrastive learning approach.](http://arxiv.org/abs/2306.05987) | 通过对比学习方法，本研究构建了一个自监督学习模型，用于学习代理市场订单的表示。进一步地，我们使用K均值聚类算法对代理订单的学习表示向量进行聚类，以确定每个簇中的不同行为类型。 |
| [^104] | [Unsupervised Cross-Domain Soft Sensor Modelling via A Deep Bayesian Particle Flow Framework.](http://arxiv.org/abs/2306.04919) | 该论文提出了一种基于深度粒子流贝叶斯框架的跨领域软测量无监督建模方法，能够解决标签缺失、领域适应性和数据时间一致性等问题，提高软测量建模的准确性。 |
| [^105] | [A Survey on Generative Diffusion Models for Structured Data.](http://arxiv.org/abs/2306.04139) | 本文全面综述了在结构化数据领域中最近提出的扩散模型，介绍了其理论基础和应用场景。 |
| [^106] | [Emotion-Conditioned Melody Harmonization with Hierarchical Variational Autoencoder.](http://arxiv.org/abs/2306.03718) | 提出了一种基于LSTM的分层变分自编码器(LHVAE)，研究情感条件对旋律和声编配的影响，同时提高了生成的和声质量并捕捉了丰富的和弦进行变化。实验证明，该模型在感知情感表达方面优于现有方法。 |
| [^107] | [Decentralized SGD and Average-direction SAM are Asymptotically Equivalent.](http://arxiv.org/abs/2306.02913) | 分散SGD和平均方向SAM在渐近意义下是等价的，D-SGD表现出梯度平滑效应和锐度正则化效应，而且可以提高后验评估，并证明了潜在的泛化能力 |
| [^108] | [(Un)reasonable Allure of Ante-hoc Interpretability for High-stakes Domains: Transparency Is Necessary but Insufficient for Comprehensibility.](http://arxiv.org/abs/2306.02312) | 高风险领域对于前文解释性的追求是重要的，但该概念自身含义模糊不清，取决于操作环境和观察者判断。透明的预测模型可能仍需后续处理才能提供合适的解释性洞见，因此传统的前文解释性概念仍需要进一步明确和探索。 |
| [^109] | [Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations.](http://arxiv.org/abs/2306.01505) | 本文提出了一种监督式对抗性对比学习（SACL）框架，用于学习类别分布结构表示，通过联合类别分布对比学习目标，有效利用标签级特性一致性并保留细粒度的类内特性，实现了在对话情感识别中最先进的结果。 |
| [^110] | [Shift-Robust Molecular Relational Learning with Causal Substructure.](http://arxiv.org/abs/2305.18451) | 本文提出了一种鲁棒性强的分子关系学习模型CMRL，通过检测与化学反应因果相关的核心亚结构来应对分子关系学习中的数据分布变化。 |
| [^111] | [Conformal Prediction with Large Language Models for Multi-Choice Question Answering.](http://arxiv.org/abs/2305.18404) | 本研究探讨了如何利用符合性预测技术，在多项选择题回答任务中为语言模型提供不确定性量化。我们发现符合性预测的不确定性估计与预测准确性密切相关。 |
| [^112] | [The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs).](http://arxiv.org/abs/2305.17033) | 这个论文介绍了CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023挑战，该挑战是首个专注于儿童脑肿瘤的BraTS挑战，旨在评估儿童脑胶质瘤的体积分割算法的发展。儿童中枢神经系统肿瘤是儿童癌症相关死亡的主要原因，并且对这些实体的诊断和治疗存在一些挑战。 |
| [^113] | [Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks.](http://arxiv.org/abs/2305.16044) | 本文提出了噪声脉冲神经元网络（NSNN）和噪声驱动学习规则（NDL），展示了噪声可以作为计算和学习的资源，并为一般脉冲神经元网络提供了一个框架。研究还展示了NSNNs在图像分类和语音识别等实际任务中的适用性，表明它们是未来神经形态计算系统的潜在有力工具。 |
| [^114] | [Efficient Large-Scale Vision Representation Learning.](http://arxiv.org/abs/2305.13399) | 本文介绍了一种高效大规模的单模态视觉表示学习方法，解决了电商应用中视觉表示的挑战，并提供了消融研究和评估。 |
| [^115] | [Synthetic data, real errors: how (not) to publish and use synthetic data.](http://arxiv.org/abs/2305.09235) | 合成数据在机器学习领域受到了越来越多的关注，但是不完美的合成数据可能会导致下游机器学习任务中的潜在错误。为了改进这种情况，研究人员引入了深度生成集成（DGE）框架来近似生成过程模型参数的后验分布，以提高下游模型的训练和评估效果。 |
| [^116] | [SKI to go Faster: Accelerating Toeplitz Neural Networks via Asymmetric Kernels.](http://arxiv.org/abs/2305.09028) | 本论文提出使用非对称核（asymmetric kernels）实现Toeplitz神经网络（TNNs）的加速，通过稀疏加低秩Toeplitz矩阵分解、小型1D卷积和替换相对位置编码器（RPE）多层感知器（MLP）实现O（n）复杂度，针对因果模型，提出了“快速”因果屏蔽来抵消这种方法的限制。 |
| [^117] | [How to Index Item IDs for Recommendation Foundation Models.](http://arxiv.org/abs/2305.06569) | 本研究对推荐基础模型的项目索引问题进行了系统检查，提出了一种新的上下文感知索引方法，该方法在项目推荐准确性和文本生成质量方面具有优势。 |
| [^118] | [Conditional Graph Information Bottleneck for Molecular Relational Learning.](http://arxiv.org/abs/2305.01520) | 本文提出了一种新的关系学习框架，称为CGIB，通过检测其中的核心子图，预测对图对之间的相互作用行为。 |
| [^119] | [Variational Bayes Made Easy.](http://arxiv.org/abs/2304.14251) | 该论文提出了一个三步骤方法，简化了变分贝叶斯近似推断方法的推导过程。 |
| [^120] | [SREL: Severity Rating Ensemble Learning for Non-Destructive Fault Diagnosis of Cu Interconnects using S-parameter Patterns.](http://arxiv.org/abs/2304.10207) | 本研究利用S参数模式成功实现对Cu互连缺陷的非破坏性检测和诊断，在同时分析根本原因和严重性方面具有先进性，具备早期检测、高诊断准确度和噪声鲁棒性等优点。 |
| [^121] | [SimbaML: Connecting Mechanistic Models and Machine Learning with Augmented Data.](http://arxiv.org/abs/2304.04000) | SimbaML是一个机器学习工具，它通过机械模型补充真实世界的数据集，实现了从合成到真实数据的转移学习、数据增强、物理启发式机器学习方法的基准测试等多项功能。 |
| [^122] | [Graph Enabled Cross-Domain Knowledge Transfer.](http://arxiv.org/abs/2304.03452) | 稀缺知识对自动化决策造成了障碍，跨领域知识转移是通过融合来自不同领域的辅助信息，缓解不同领域知识差距的一种方法。 |
| [^123] | [Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers.](http://arxiv.org/abs/2304.00215) | 本文提出了一种基于分层Transformer的方法，即REPORT，能够同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性。它完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在基准数据集上实现了最先进的性能。 |
| [^124] | [Machine learning for discovering laws of nature.](http://arxiv.org/abs/2303.17607) | 模型基于达尔文自然选择，结合函数选择和运算符选择两个过程，通过从数据中学习构建理论，可自动发现和表示自然定律，成功应用于模拟多领域问题，并提供一种新方法解决描述自然定律的严格数学模型不足的问题。 |
| [^125] | [Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos.](http://arxiv.org/abs/2303.16897) | 该论文提出了一种物理驱动扩散模型，可以为silent视频剪辑合成高保真的冲击声，并使用额外的物理先验知识来指导冲击声合成过程。 |
| [^126] | [BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning.](http://arxiv.org/abs/2303.14773) | BlackVIP是一种针对大型预训练模型的黑盒视觉提示方法，它可以在没有参数可访问性的情况下高效地适应模型，并通过使用协调器和SPSA-GC组件实现。 |
| [^127] | [Adaptive Experimentation at Scale: Bayesian Algorithms for Flexible Batches.](http://arxiv.org/abs/2303.11582) | 本文提出了一个基于贝叶斯算法的自适应实验框架，可灵活处理任何批处理大小。通过正态近似指导可扩展自适应设计，采用残余时限优化选择采样分配，实现了最先进的性能。 |
| [^128] | [Object-Centric Slot Diffusion.](http://arxiv.org/abs/2303.10834) | 基于对象中心的槽扩散是一种将扩散模型整合到对象中心学习的新颖方法，它使用Latent Slot Diffusion (LSD)模型替换了传统的槽解码器，同时也是第一个能够无监督进行组合条件扩散的模型。 |
| [^129] | [NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects.](http://arxiv.org/abs/2303.07308) | NeuSE是一种用于对象的神经网络嵌入算法，支持实现与长期场景变化一致的对象空间理解。它可以编码完整的形状信息，并与物理世界中的对象同时变换，能够直接推断相对帧变换和相机姿态约束，并维持适应变化的轻量级对象地图。 |
| [^130] | [Local-Global Methods for Generalised Solar Irradiance Forecasting.](http://arxiv.org/abs/2303.06010) | 本文提出了使用局部-全局方法进行广义太阳辐射预测，并且证明了可以构建不需要实时数据和历史观测数据的系统。 |
| [^131] | [Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games.](http://arxiv.org/abs/2303.04865) | 本文提出了一种网络马尔可夫势博弈中的局部演员-评论家算法，通过局部化和函数逼近技术，实现了有限样本保证，样本复杂度不依赖于代理数量。 |
| [^132] | [A Comparative Study of Self-Supervised Speech Representations in Read and Spontaneous TTS.](http://arxiv.org/abs/2303.02719) | 这项研究比较了在朗读和自由说话语音合成中使用的自我监督语音表示，并发现在12层wav2vec2.0（ASR微调）的第9层表现最优。这项工作揭示了语音SSL如何改进TTS系统，并在具有挑战性的TTS生成任务中进行了比较。 |
| [^133] | [Auxiliary Functions as Koopman Observables: Data-Driven Polynomial Optimization for Dynamical Systems.](http://arxiv.org/abs/2303.01483) | 提出了一种基于数据的动力系统分析方法，使用辅助函数作为Koopman可观测量，不需要明确的模型发现，可以适用于确定性和随机动力学，具有收敛性和性能优势。 |
| [^134] | [OmniForce: On Human-Centered, Large Model Empowered and Cloud-Edge Collaborative AutoML System.](http://arxiv.org/abs/2303.00501) | OmniForce是一个基于人类中心、大模型和云边协同的自动机器学习系统，旨在解决开放环境下的AutoML问题。当前的AutoML系统和平台低效且计算复杂，而OmniForce通过人机交互技术改进了这一现状。 |
| [^135] | [Approximately Stationary Bandits with Knapsacks.](http://arxiv.org/abs/2302.14686) | 带有背包的掠夺者问题在随机和敌对情况下存在巨大的差距，尤其是在敌对情况下，当预算更加紧缺时保证性能变得更差。 |
| [^136] | [FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning.](http://arxiv.org/abs/2302.13485) | 本文介绍了一种名为FedCLIP的方法，用于在联邦学习中实现CLIP的快速泛化和个性化。该方法通过设计基于注意力的适配器，充分利用预训练模型信息，并确保模型适应特定任务的客户端。这种方法可以提高模型训练的效率和性能。 |
| [^137] | [Adaptive Sparse Gaussian Process.](http://arxiv.org/abs/2302.10325) | 这篇论文提出了第一个自适应稀疏高斯过程，能够在非平稳环境中进行高效的模型更新，并具有快速的推理收敛性。 |
| [^138] | [Magnetohydrodynamics with Physics Informed Neural Operators.](http://arxiv.org/abs/2302.08332) | 本文提出了一种使用物理信息神经算子模拟磁流体力学的方法，通过使用人工智能技术，在显著降低计算成本的同时，准确捕捉了磁流体力学模拟的物理特性。 |
| [^139] | [Spatially heterogeneous learning by a deep student machine.](http://arxiv.org/abs/2302.07419) | 本论文研究了一种深度学生机器的教师-学生设置，通过学生机器的集合来研究由具有大量可调参数的DNN的监督学习。研究表明DNN的学习在网络空间中相当异质。 |
| [^140] | [Generalization Bounds with Data-dependent Fractal Dimensions.](http://arxiv.org/abs/2302.02766) | 这项研究提出了基于数据依赖的分形维度的泛化界限，不需要Lipschitz假设，并能控制泛化误差和互信息项。 |
| [^141] | [Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities.](http://arxiv.org/abs/2302.01018) | 该论文总结了时态图的图神经网络的现状、挑战和机遇，提供了学习设置和任务的严格规范化以及一个新的分类法，并讨论了该领域最相关的开放挑战，从研究和应用角度讨论。 |
| [^142] | [Local transfer learning from one data space to another.](http://arxiv.org/abs/2302.00160) |  |
| [^143] | [Pre-Training Protein Encoder via Siamese Sequence-Structure Diffusion Trajectory Prediction.](http://arxiv.org/abs/2301.12068) | 本研究提出了一种名为DiffPreT的方法，通过序列-结构联合扩散建模来预训练蛋白质编码器。同时，通过一种称为SiamDiff的方法增强了DiffPreT，以捕捉蛋白质的不同构象之间的相关性。 |
| [^144] | [Learning-Rate-Free Learning by D-Adaptation.](http://arxiv.org/abs/2301.07733) | D-Adaptation是一种可以自动设置学习率的方法，针对最小化凸性Lipschitz函数，用于实现最优收敛速率，而无需超参数，也无需额外对数因子改进，能够在各种机器学习问题中自动匹配手动调整的学习率。 |
| [^145] | [On Sequential Bayesian Inference for Continual Learning.](http://arxiv.org/abs/2301.01828) | 这篇论文研究了顺序贝叶斯推断在连续学习中的应用。研究表明，尽管使用了真实后验，这种方法仍无法防止神经网络中的灾难性遗忘。同时，模型误差和任务数据不平衡也会导致连续学习性能的下降。 |
| [^146] | [Provable Robust Saliency-based Explanations.](http://arxiv.org/abs/2212.14106) | 本文提出了一种可证明鲁棒的基于显著性的解释方法，通过最大化解释厚度和稳定顶部显著特征，改进了解释的数值和统计稳定性。实验证明了该方法在各种网络和数据上的性能。 |
| [^147] | [AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security.](http://arxiv.org/abs/2212.06951) | 本研究分析了Twitter上与MEV相关的话题，结果表明推文讨论了深刻的伦理问题，包括安全、公平、情感情绪和对MEV解决方案的渴望。 |
| [^148] | [Is Conditional Generative Modeling all you need for Decision-Making?.](http://arxiv.org/abs/2211.15657) | 该论文研究了条件生成建模方法在解决顺序决策问题方面的应用，发现该方法在标准基准测试中表现优于传统离线强化学习方法。通过建模为回报条件扩散模型，可以避免动态规划的复杂性，并通过考虑约束和技能作为条件变量进一步提高性能。 |
| [^149] | [Label Alignment Regularization for Distribution Shift.](http://arxiv.org/abs/2211.14960) | 这篇论文提出了一种用于无监督领域自适应的正则化方法，通过鼓励目标域中的预测与其前几个奇异向量对齐来实现。与传统方法不同的是，这个方法通过正则化分类器与无监督目标数据对齐，而不是正则化表示。通过消除对最优联合风险假设的依赖，该方法展示了很好的效果。 |
| [^150] | [Learning and Testing Latent-Tree Ising Models Efficiently.](http://arxiv.org/abs/2211.13291) | 本文提出了学习和测试潜在树状Ising模型的高效算法，改进了先前工作的结果，并通过展示树状Ising模型叶节点分布的新颖局部化结果来实现这些算法。 |
| [^151] | [Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions.](http://arxiv.org/abs/2211.12316) | Transformers相对于循环模型更偏向于学习具有低敏感度的函数，尤其在稀疏布尔函数上，Transformers能够实现近乎完美的泛化，而LSTMs则表现出过拟合和较低的泛化精度。 |
| [^152] | [On the Pointwise Behavior of Recursive Partitioning and Its Implications for Heterogeneous Causal Effect Estimation.](http://arxiv.org/abs/2211.10805) | 本文质疑了递归划分在决策树学习中的应用，通过证明它们可能无法实现一致范数的多项式收敛速率。我们提出了随机森林来解决这个问题，将低性能的树转化为几乎最优的过程，但代价是失去了解释性，并引入了两个额外的调整参数。 |
| [^153] | [Efficient Compressed Ratio Estimation using Online Sequential Learning for Edge Computing.](http://arxiv.org/abs/2211.04284) | 本研究提出了一种基于AC-OSELM的高效RL方法，能够在边缘设备上估计合适的压缩比，从而实现数据的高效压缩，同时保持重构数据的准确性。 |
| [^154] | [The Benefits of Model-Based Generalization in Reinforcement Learning.](http://arxiv.org/abs/2211.02222) | 这篇论文研究了模型驱动泛化在强化学习中的效益，通过学习模型生成的数据可以提供更多有用的信息，从而提高强化学习的效果。 |
| [^155] | [Infusing known operators in convolutional neural networks for lateral strain imaging in ultrasound elastography.](http://arxiv.org/abs/2211.00172) | 该论文提出了一种将已知运算符注入卷积神经网络的方法，用于改善超声弹性成像中横向应变成像的质量。目前存在的问题包括低采样频率、运动受限和缺乏横向方向的相位信息。最近提出的一种基于物理启发的无监督正则化弹性成像方法已取得实质性改进，但仅在训练期间应用正则化，无法保证在测试期间横向应变处于可行范围内。 |
| [^156] | [Self-Improving Safety Performance of Reinforcement Learning Based Driving with Black-Box Verification Algorithms.](http://arxiv.org/abs/2210.16575) | 本研究提出了一种使用黑盒验证方法来增强强化学习驾驶的安全性能的自我改进的人工智能系统。该方法通过发现自动驾驶的失败场景并重新训练，以改善在训练中缺乏的安全关键场景的性能。模拟结果表明该方法在自适应巡航控制应用中取得了显著的效果。 |
| [^157] | [Preferential Subsampling for Stochastic Gradient Langevin Dynamics.](http://arxiv.org/abs/2210.16189) | 本文提出一种偏好子采样的方法来对随机梯度Langevin动力学进行优化，通过使用非均匀概率分布子采样对具有更大影响的数据点进行加权，同时还通过自适应调整子采样大小来提高梯度估计的准确性。实验证明这种方法可以在减少子采样数的同时保持相同的精度水平。 |
| [^158] | [Conditionally Risk-Averse Contextual Bandits.](http://arxiv.org/abs/2210.13573) | 设计出了第一个具有在线遗憾保证的风险厌恶的上下文赌博算法，并在多个实验场景中展示了其适用性。 |
| [^159] | [Analysis of Convolutions, Non-linearity and Depth in Graph Neural Networks using Neural Tangent Kernel.](http://arxiv.org/abs/2210.09809) | 本研究通过理论方法分析了图神经网络中的卷积、非线性和深度对网络性能的影响，同时对基于图拉普拉斯和邻接矩阵的归一化方法进行了比较，并揭示了线性GNN与非线性ReLU-GNN性能相当的现象缺乏严格的理论解释。 |
| [^160] | [Rejecting noise in Baikal-GVD data with neural networks.](http://arxiv.org/abs/2210.04653) | 本研究引入了一种神经网络，使用U-net-like结构和事件的时间结构，成功地将Baikal-GVD数据中的噪声和信号分离开来，并取得了高精确度和召回率的结果。 |
| [^161] | [On The Effects Of Data Normalisation For Domain Adaptation On EEG Data.](http://arxiv.org/abs/2210.01081) | 本文研究了数据标准化对脑电领域适应的影响，并在三个EEG数据集上进行了实验评估。 |
| [^162] | [Compositional Score Modeling for Simulation-based Inference.](http://arxiv.org/abs/2209.14249) | 本研究提出了一种基于条件评分建模的方法，可以有效处理基于多个观测条件下得到的后验分布，同时具有高样本效率和聚合多个观测值的优势。 |
| [^163] | [Obstacle Identification and Ellipsoidal Decomposition for Fast Motion Planning in Unknown Dynamic Environments.](http://arxiv.org/abs/2209.14233) | 本文提出一种基于椭球的障碍物识别与测速方法，并定义了基于椭球的特征向量，能够适用于带有静态和动态障碍物的环境，其运行速度比现有算法更快且不需要预先知道聚类数量。 |
| [^164] | [DynDepNet: Learning Time-Varying Dependency Structures from fMRI Data via Dynamic Graph Structure Learning.](http://arxiv.org/abs/2209.13513) | DynDepNet是一种学习fMRI数据中时变依赖结构的新方法，在性别分类任务中取得了最先进的结果。 |
| [^165] | [The Bayan Algorithm: Detecting Communities in Networks Through Exact and Approximate Optimization of Modularity.](http://arxiv.org/abs/2209.04562) | 提出了一种名为Bayan的社区检测算法，通过精确或近似优化模块度的方法，它能够返回最优或接近最优的分区，并且比其他算法快数倍，并能够在合成和真实网络数据集上准确地找到地面真实社区。 |
| [^166] | [Defend Data Poisoning Attacks on Voice Authentication.](http://arxiv.org/abs/2209.04547) | 本文展示了一种易于实施的语音认证系统数据中毒攻击，并提出了一种更加强大的防御方法，称为Guardian，它是一种基于卷积神经网络的鉴别器。 |
| [^167] | [A Data-dependent Approach for High Dimensional (Robust) Wasserstein Alignment.](http://arxiv.org/abs/2209.02905) | 本文提出了一种面向高维(鲁棒)Wasserstein对齐的数据相关方法，通过压缩高维度的几何模式来降低时间复杂度。 |
| [^168] | [Continual Learning, Fast and Slow.](http://arxiv.org/abs/2209.02370) | 这篇论文提出了DualNets，一个通用的持续学习框架，通过结合快速学习和慢速学习系统，实现了更好的深度神经网络的持续学习。 |
| [^169] | [The Value of Out-of-Distribution Data.](http://arxiv.org/abs/2208.10967) | 不同分布的数据可以对任务的泛化误差产生非单调的影响，使用少量不同分布的数据进行训练是有价值的。 |
| [^170] | [Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies.](http://arxiv.org/abs/2208.10264) | 该论文介绍了一种新型测试方法，称为图灵实验（TE），用于评估语言模型的人类行为模拟能力。论文通过模拟经济学、心理语言学和社会心理学实验，成功复制了已有研究，并揭示了一种“超准确度扭曲”的现象。 |
| [^171] | [Double Auctions with Two-sided Bandit Feedback.](http://arxiv.org/abs/2208.06536) | 双拍卖市场中引入了双向信息反馈，通过置信界竞标和平均定价，能够实现有效的价格发现和降低参与者的遗憾。 |
| [^172] | [Adaptive Domain Generalization via Online Disagreement Minimization.](http://arxiv.org/abs/2208.01996) | 本文提出了一个自适应领域泛化的通用框架，通过在线争议最小化对源模型进行适应性修改，从而改善了领域泛化算法的性能。 |
| [^173] | [Gradient-based Bi-level Optimization for Deep Learning: A Survey.](http://arxiv.org/abs/2207.11719) | 基于梯度的双层优化方法在深度学习领域中被广泛应用，特别适用于超参数优化和元知识提取。本文提供了基于梯度的双层优化的定义、适用标准以及实用指南，对于初学者尤其有帮助。 |
| [^174] | [On the Robustness of Bayesian Neural Networks to Adversarial Attacks.](http://arxiv.org/abs/2207.06154) | 本文研究了贝叶斯神经网络在对抗攻击下的鲁棒性问题，证明了在大数据、超参数化极限下，BNN的后验具有梯度攻击的鲁棒性，这对于解决深度学习在安全关键应用中的脆弱性问题具有重要意义。 |
| [^175] | [Indecision Trees: Learning Argument-Based Reasoning under Quantified Uncertainty.](http://arxiv.org/abs/2206.12252) | 本文介绍了一种修改决策树的方法，即犹豫树，该方法能够在不确定性下进行学习，推理和提供强健的标签分布，可用于其他推理系统。 |
| [^176] | [Survival Kernets: Scalable and Interpretable Deep Kernel Survival Analysis with an Accuracy Guarantee.](http://arxiv.org/abs/2206.10477) | Survival Kernets 是一种可扩展且可解释的深度核生存分析模型，能够在大规模数据集上进行模型解释和理论分析。它利用核函数估计个体的生存分布，通过训练集压缩方案进行数据分簇，因此具有较高的可视化能力和预测准确性保证。该模型在特定情况下的预测生存分布误差界限最优，且在测试时具有可扩展性。 |
| [^177] | [MetaFed: Federated Learning among Federations with Cyclic Knowledge Distillation for Personalized Healthcare.](http://arxiv.org/abs/2206.08516) | 本文提出了一个名为MetaFed的新型框架，旨在在不同的联邦之间实现可信的联邦学习。通过循环知识蒸馏，MetaFed能够获取每个联邦的个性化模型，无需中央服务器，并且实验证明MetaFed在精度上超过了现有方法。 |
| [^178] | [Mixed integer linear optimization formulations for learning optimal binary classification trees.](http://arxiv.org/abs/2206.04857) | 本研究提出了四个混合整数线性优化（MILO）模型，旨在设计最优的二叉分类树，通过最大化正确分类的数据点数量和最小化分支节点数量。 |
| [^179] | [DORA: Exploring outlier representations in Deep Neural Networks.](http://arxiv.org/abs/2206.04530) | 本文提出了一种名为DORA的数据不可知框架，用于分析深度神经网络中的表征空间，并可以识别不符合人类直观认知的表征。 |
| [^180] | [Conditionally Calibrated Predictive Distributions by Probability-Probability Map: Application to Galaxy Redshift Estimation and Probabilistic Forecasting.](http://arxiv.org/abs/2205.14568) | 本研究提出了一种名为Cal-PIT的方法，通过学习一个概率-概率映射，解决了预测分布的诊断和校准问题，来实现有条件校准。 |
| [^181] | [SeedGNN: Graph Neural Networks for Supervised Seeded Graph Matching.](http://arxiv.org/abs/2205.13679) | 本文提出了一种名为SeedGNN的图神经网络架构，用于有监督种子图匹配。相比于半监督方法，SeedGNN能够从训练集中学习如何利用少量种子节点来匹配未见图，并通过计算和利用不同跳数的证人信息以及使用易匹配的节点对改善匹配效果的方式实现了显著的性能改进。 |
| [^182] | [DDAC-SpAM: A Distributed Algorithm for Fitting High-dimensional Sparse Additive Models with Feature Division and Decorrelation.](http://arxiv.org/abs/2205.07932) | DDAC-SpAM是一种在高维稀疏加性模型中利用特征划分和去相关的分布式算法。去相关操作使得每个局部估计器能够恢复每个加性组分的稀疏模式，同时不对变量之间的相关性结构施加严格的约束。该算法在理论和实证分析中证明了其有效性和效率，为拟合稀疏加性模型提供了实际解决方案。 |
| [^183] | [KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource Language.](http://arxiv.org/abs/2205.02364) | 该研究开发了KenSwQuAD - 一份适用于斯瓦希里语低资源语言的问答数据集，以满足低资源语言中机器理解自然语言的需求。这个数据集是通过从斯瓦希里语低资源语言的原始故事文本中进行注释得到的，包含7,526个QA对。 |
| [^184] | [Out-of-distribution generalization for learning quantum dynamics.](http://arxiv.org/abs/2204.10268) | 该论文在量子机器学习中证明了学习未知酉的越域泛化能力，并提出了使用乘积态来学习酉对纠缠态的作用，从而推动了在近期量子硬件上学习量子动力学的前景，并为经典和量子电路的编译提供了新的方法。 |
| [^185] | [Quantifying Spatial Under-reporting Disparities in Resident Crowdsourcing.](http://arxiv.org/abs/2204.08620) | 本文研究了城市治理中居民众包的问题，并提出了一种准确测量报道率的方法，使不同的报道率不再成为城市治理下游解决事件速度方面的不公平根源。 |
| [^186] | [Jump-Start Reinforcement Learning.](http://arxiv.org/abs/2204.02372) | 本文介绍了一种元算法，可以使用离线数据、演示或现有策略来初始化强化学习策略，并且与任何强化学习方法兼容。通过使用导引策略来形成探索策略的初始状态课程，能够提高在模拟机器人任务中的性能。 |
| [^187] | [A PDE-Based Analysis of the Symmetric Two-Armed Bernoulli Bandit.](http://arxiv.org/abs/2202.05767) | 本研究通过关联线性热方程的解，得到了对称双臂伯努利赌博机问题的minmax最优遗憾和伪遗憾的领先项。新的结果改进了先前的研究，并提供了新的非渐近边界。 |
| [^188] | [Consistent Collaborative Filtering via Tensor Decomposition.](http://arxiv.org/abs/2201.11936) | 本文提出了一种通过张量分解来实现一致协同过滤的新模型，它能够扩展传统的用户-物品偏好计算方法，使得在评估物品相对偏好时产生物品之间的交互，具有潜在的非线性态度。 |
| [^189] | [Fast Interpretable Greedy-Tree Sums.](http://arxiv.org/abs/2201.11931) | FIGS是一种快速可解释的贪婪树求和算法，通过将逻辑规则与加法相结合，能够适应加性结构同时保持高度可解释性。在真实数据集上的实验表明，FIGS实现了最先进的预测性能，并在高风险领域如医学中展示了其实用性。 |
| [^190] | [Conservative Distributional Reinforcement Learning with Safety Constraints.](http://arxiv.org/abs/2201.07286) | 本文提出了一种名为保守分布最大后验策略优化 (CDMPO) 的离策略强化学习算法，通过使用分布强化学习方法来估计Q函数和C函数，以及使用保守的价值函数损失来减少约束违反次数。 |
| [^191] | [Federated Continual Learning for Socially Aware Robotics.](http://arxiv.org/abs/2201.05527) | 面向社交感知机器人的联邦连续学习方法结合了联邦学习和连续学习，通过分散学习和隐私保护，提升了社交机器人的个性化和用户适应性。 |
| [^192] | [SCORE: Approximating Curvature Information under Self-Concordant Regularization.](http://arxiv.org/abs/2112.07344) | 本文提出了SCORE框架，在无约束极小化问题中利用自对偶正则化近似曲率信息。我们还提出了GGN-SCORE算法，该算法通过更新极小化变量来加速收敛并改善模型泛化性能。 |
| [^193] | [Phase transitions in nonparametric regressions.](http://arxiv.org/abs/2112.03626) | 本文研究了非参数回归中的相变问题，根据最小极大MISE速率的不同情况，确定了不同范围内最佳的平滑度。同时，本文还提出了一组用于平滑函数类别的度量熵界限。 |
| [^194] | [Contextual Combinatorial Multi-output GP Bandits with Group Constraints.](http://arxiv.org/abs/2111.14778) | 这项研究提出了一种应用于联邦多臂赌博问题的新算法，该算法通过选择一组基础臂来最大化超级臂奖励，并同时满足组奖励约束。算法利用两输出高斯过程模型，为每个基础臂的结果提供更大的灵活性。 |
| [^195] | [GFlowNet Foundations.](http://arxiv.org/abs/2111.09266) | GFlowNets是一种生成流网络方法，用于在主动学习环境中采样多样化的候选集。它们具有估计联合概率分布和边际分布的能力，可以表示关于复合对象（如集合和图）的分布。通过单次训练的生成传递，GFlowNets分摊了计算昂贵的MCMC方法的工作。 |
| [^196] | [SCORE: Spurious COrrelation REduction for Offline Reinforcement Learning.](http://arxiv.org/abs/2110.12468) | 本文提出了SCORE算法，用于离线强化学习中的虚假相关性降低。通过引入退火行为克隆正则化器，SCORE实现了SoTA性能，并消除了次优性中的虚假相关性。 |
| [^197] | [A scalable and fast artificial neural network syndrome decoder for surface codes.](http://arxiv.org/abs/2110.05854) | 本研究报道了一种基于人工神经网络的表面码综合征解码器，能够解码任意形状和大小的表面码，并取得了具有竞争性或优越的性能，显示出惊人的加速作用，这是实现大规模量子纠错的重要进展。 |
| [^198] | [Sparse MoEs meet Efficient Ensembles.](http://arxiv.org/abs/2110.03360) | 本论文研究了神经网络集成和稀疏专家混合的结合，提出了一种名为E$^3$的高效稀疏MoEs集成方法，在减少计算复杂度的同时取得了在准确性、鲁棒性和不确定性方面的改进。 |
| [^199] | [Self-Supervised Learning to Prove Equivalence Between Straight-Line Programs via Rewrite Rules.](http://arxiv.org/abs/2109.10476) | 本研究针对自动合成一对程序之间语义等价性的证明问题，提出了一种基于Transformer模型的神经网络架构，通过一系列重写规则的应用序列实现程序的等价性证明。 |
| [^200] | [Insights from Generative Modeling for Neural Video Compression.](http://arxiv.org/abs/2107.13136) | 本论文基于生成模型，通过深度自回归和潜变量建模的方法，对神经视频编码算法进行了改进。提出了改进的时序自回归变换、改进的熵模型以及可变码率版本算法，并在高分辨率视频上取得了最先进的压缩性能。 |
| [^201] | [Lightweight Distributed Gaussian Process Regression for Online Machine Learning.](http://arxiv.org/abs/2105.04738) | 在这篇论文中，我们提出了一种轻量级分布式高斯过程回归（GPR）算法，用于协同学习一个共同的静态潜在函数。通过独立运行基于代理的GPR和协同执行分布式GPR，我们展示了有限的代理间通信可以改善学习性能，并通过蒙特卡罗模拟评估了算法的性能。 |
| [^202] | [Low-rank Tensor Estimation via Riemannian Gauss-Newton: Statistical Optimality and Second-Order Convergence.](http://arxiv.org/abs/2104.12031) | 本文提出了一种通过Riemannian Gauss-Newton方法进行低秩张量估计的方法，并证明了其在噪声环境下的局部二次收敛性和统计最优性。 |
| [^203] | [Novelty Detection in Sequential Data by Informed Clustering and Modeling.](http://arxiv.org/abs/2103.03943) | 通过对数据进行聚类并分解建模任务，实现对序列数据中的新颖性进行检测。这种方法可以更准确地进行建模，但在每个聚类中的训练数据量较少，对于离散序列来说尤为突出。 |
| [^204] | [Achieving Efficiency in Black Box Simulation of Distribution Tails with Self-structuring Importance Samplers.](http://arxiv.org/abs/2102.07060) | 本文提出了一种新颖的自结构重要性采样方法，通过复制在较不罕见的样本中观察到的浓度特性，隐式诱导出一种有效的IS分布，从而提高了估计性能度量的分布尾部的效率。 |
| [^205] | [Efficient Data-Driven Optimization with Noisy Data.](http://arxiv.org/abs/2102.04363) | 本文研究了在已知噪声源的情况下的数据驱动处方问题，并导出了在这个噪声情况下的高效数据驱动形式，并指出它们具有熵最优传输解释。 |
| [^206] | [Learning to extrapolate using continued fractions: Predicting the critical temperature of superconductor materials.](http://arxiv.org/abs/2012.03774) | 本论文提出了使用连分数来学习外推的方法，以预测超导体材料的临界温度。该方法适用于需要在扩展领域内准确近似的应用场景，尤其在设计新结构时最为重要。 |
| [^207] | [Automated Detection of Double Nuclei Galaxies using GOTHIC and the Discovery of a Large Sample of Dual AGN.](http://arxiv.org/abs/2011.12177) | 我们提出了一种名为GOTHIC的算法，用于检测双核星系，发现了许多双重活动星系核。我们在大样本中检测到了159个双重AGN，其中2个是三重AGN系统。 |
| [^208] | [Optimal Learning for Structured Bandits.](http://arxiv.org/abs/2007.07302) | 本文研究了结构化多臂赌博机问题，在存在结构信息的情况下，设计了一种能够利用结构信息以最小化后悔的算法。 |
| [^209] | [Testing Robustness Against Unforeseen Adversaries.](http://arxiv.org/abs/1908.08016) | 该论文提出了18种新的对抗攻击，并使用这些攻击创建了一个用于评估对各种未预料到的对手的鲁棒性的新基准。作者还发现了一系列防御策略，可以帮助克服训练期间未考虑到的对手的泛化差距。该研究的结果将为研究现实世界最坏情况下的鲁棒性提供有用工具，促进开发更强大的防御措施。 |

# 详细

[^1]: 基于分层频率-时间Transformer的自动钢琴转录

    Automatic Piano Transcription with Hierarchical Frequency-Time Transformer. (arXiv:2307.04305v1 [cs.SD])

    [http://arxiv.org/abs/2307.04305](http://arxiv.org/abs/2307.04305)

    提出了一种名为hFT-Transformer的自动音乐转录方法，它采用了两级分层频率-时间Transformer架构，并在MAPS和MAESTRO v3.0.0数据集上展现出了最先进的性能。

    

    考虑到长期的频率和时间依赖关系对于自动钢琴转录至关重要，特别是在确定复音钢琴内容中每个音符的精确起始和结束时间时。在这种情况下，我们可以依靠Transformer中自注意力机制的能力来捕捉这些频率和时间轴上的长期依赖关系。在这项工作中，我们提出了hFT-Transformer，这是一种使用两级分层频率-时间Transformer架构的自动音乐转录方法。第一个层次结构包括时间轴上的卷积块，频率轴上的Transformer编码器以及将频率轴上的维度转换的Transformer解码器。然后将输出馈送到第二层次结构，其中包含另一个时间轴上的Transformer编码器。我们使用广泛使用的MAPS和MAESTRO v3.0.0数据集对我们的方法进行评估，结果表明在所有测试数据上都展现出了最先进的性能。

    Taking long-term spectral and temporal dependencies into account is essential for automatic piano transcription. This is especially helpful when determining the precise onset and offset for each note in the polyphonic piano content. In this case, we may rely on the capability of self-attention mechanism in Transformers to capture these long-term dependencies in the frequency and time axes. In this work, we propose hFT-Transformer, which is an automatic music transcription method that uses a two-level hierarchical frequency-time Transformer architecture. The first hierarchy includes a convolutional block in the time axis, a Transformer encoder in the frequency axis, and a Transformer decoder that converts the dimension in the frequency axis. The output is then fed into the second hierarchy which consists of another Transformer encoder in the time axis. We evaluated our method with the widely used MAPS and MAESTRO v3.0.0 datasets, and it demonstrated state-of-the-art performance on all t
    
[^2]: 带有零-shot数据压缩的边缘存储管理配方用于道路异常检测

    Edge Storage Management Recipe with Zero-Shot Data Compression for Road Anomaly Detection. (arXiv:2307.04298v1 [cs.SD])

    [http://arxiv.org/abs/2307.04298](http://arxiv.org/abs/2307.04298)

    提出了一种基于预训练自编码器的简单而有效的数据压缩方法，用于边缘存储管理和道路异常检测，可以高效地存储高保真音频样本。

    

    最近的研究显示了基于边缘计算的道路异常检测系统，该系统可以同时进行数据收集。然而，边缘计算机的数据存储空间很小，但我们需要长时间存储收集到的音频样本，以便更新现有模型或开发新的方法。因此，我们应该考虑一种在保持高保真音频的同时进行高效存储管理的方法。从硬件角度来看，使用低分辨率麦克风可以直观地减小文件大小，但不推荐这种方法，因为它会从根本上削弱高频组件。另一方面，计算文件压缩方法可以将收集到的高分辨率音频编码为紧凑的代码，因此推荐使用该方法，因为它还提供相应的解码方法。受此启发，我们提出了一种简单而有效的基于预训练自编码器的数据压缩方法。

    Recent studies show edge computing-based road anomaly detection systems which may also conduct data collection simultaneously. However, the edge computers will have small data storage but we need to store the collected audio samples for a long time in order to update existing models or develop a novel method. Therefore, we should consider an approach for efficient storage management methods while preserving high-fidelity audio. A hardware-perspective approach, such as using a low-resolution microphone, is an intuitive way to reduce file size but is not recommended because it fundamentally cuts off high-frequency components. On the other hand, a computational file compression approach that encodes collected high-resolution audio into a compact code should be recommended because it also provides a corresponding decoding method. Motivated by this, we propose a way of simple yet effective pre-trained autoencoder-based data compression method. The pre-trained autoencoder is trained for the 
    
[^3]: 广义图ODE：跨环境学习复杂系统动态

    Generalizing Graph ODE for Learning Complex System Dynamics across Environments. (arXiv:2307.04287v1 [cs.LG])

    [http://arxiv.org/abs/2307.04287](http://arxiv.org/abs/2307.04287)

    GG-ODE是一个机器学习框架，用于跨环境学习复杂系统动态，利用神经普通微分方程和图神经网络的参数化方式。

    

    学习多智能体系统动态在各种实际应用中得到了广泛研究，如生物学中的分子动力学。现有的大多数模型都是基于观测到的历史数据学习单一系统动态并预测未来轨迹。然而，在实践中，我们可能观察到在不同环境中生成的多个系统，这些系统在温度和重力等潜在外因素上存在差异。一个简单的解决方案是学习多个环境特定模型，但它未能利用跨环境动态中的潜在共性，并在每个环境数据稀缺或有限时提供较差的预测结果。在这里，我们提出了GG-ODE（广义图普通微分方程），这是一个用于跨环境学习连续多智能体系统动态的机器学习框架。我们的模型使用由图神经网络参数化的神经普通微分方程来学习系统动态。

    Learning multi-agent system dynamics has been extensively studied for various real-world applications, such as molecular dynamics in biology. Most of the existing models are built to learn single system dynamics from observed historical data and predict the future trajectory. In practice, however, we might observe multiple systems that are generated across different environments, which differ in latent exogenous factors such as temperature and gravity. One simple solution is to learn multiple environment-specific models, but it fails to exploit the potential commonalities among the dynamics across environments and offers poor prediction results where per-environment data is sparse or limited. Here, we present GG-ODE (Generalized Graph Ordinary Differential Equations), a machine learning framework for learning continuous multi-agent system dynamics across environments. Our model learns system dynamics using neural ordinary differential equations (ODE) parameterized by Graph Neural Netwo
    
[^4]: 评估大型语言模型在生成准确的教师回答中的有效性

    Assessing the efficacy of large language models in generating accurate teacher responses. (arXiv:2307.04274v1 [cs.CL])

    [http://arxiv.org/abs/2307.04274](http://arxiv.org/abs/2307.04274)

    本研究评估了大型语言模型在生成准确的教师回答中的能力。实验结果表明，在教师-学生聊天室语料库子集上，GPT-4的效果优于其他模型。研究还指出样本抽样和代表性等数据集特征可能影响生成模型的效果。

    

    (Tack等人，2023年)在第18届创新使用NLP构建教育应用研讨会上组织了共享任务，即教育对话中教师语言的生成。本研究按照共享任务的结构，试图评估大型语言模型在向学生提供信息性和有益洞察力方面的生成能力，从而模拟知识渊博的教师的角色。为此，我们对几种基准生成模型进行了广泛的评估，包括GPT-4（少样本、上下文学习）、微调的GPT-2和微调的DialoGPT。此外，为了优化教学质量，我们使用强化学习对Flan-T5模型进行了微调。在教师-学生聊天室语料库子集上的实验结果表明，使用BERTScore和DialogRPT度量，GPT-4比其他微调模型更为有效。我们假设样本抽样和代表性等多个数据集特征可能影响生成模型的效果。

    (Tack et al., 2023) organized the shared task hosted by the 18th Workshop on Innovative Use of NLP for Building Educational Applications on generation of teacher language in educational dialogues. Following the structure of the shared task, in this study, we attempt to assess the generative abilities of large language models in providing informative and helpful insights to students, thereby simulating the role of a knowledgeable teacher. To this end, we present an extensive evaluation of several benchmarking generative models, including GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and fine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we fine-tuned the Flan-T5 model using reinforcement learning. Our experimental findings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of GPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT.  We hypothesize that several dataset characteristics, including sampling, representativen
    
[^5]: ChatGPT在生成式AI和大语言模型时代的应用：一份简洁的调查报告

    ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey. (arXiv:2307.04251v1 [cs.CL])

    [http://arxiv.org/abs/2307.04251](http://arxiv.org/abs/2307.04251)

    ChatGPT是由OpenAI创建的一种大型语言模型（LLM），它在自然语言处理（NLP）领域引起了革命性的变革。它是第一个实现公众大规模与生成式人工智能（GAI）互动的关键技术，也引发了类似技术的研究兴趣和应用探索。

    

    ChatGPT是由OpenAI创建的一种大型语言模型（LLM），经过精心训练并使用了大量数据。它在自然语言处理（NLP）领域引起了革命性的变革，并推动了LLM能力的边界。ChatGPT在大规模范围内实现了普遍公众与生成式人工智能（GAI）的互动，起到了关键作用。它还引发了开发类似技术和研究其应用和影响的兴趣。本文的主要目标是对ChatGPT及其演化的当前研究方向进行简明调查。我们同时考虑了ChatGPT的玻璃盒和黑盒视角，包括技术的组成部分和基本要素，以及其应用、影响和影响。玻璃盒方法着重于理解技术的内部运作，而黑盒方法将其视为一个复杂系统，因此研究其输入，

    ChatGPT is a large language model (LLM) created by OpenAI that has been carefully trained on a large amount of data. It has revolutionized the field of natural language processing (NLP) and has pushed the boundaries of LLM capabilities. ChatGPT has played a pivotal role in enabling widespread public interaction with generative artificial intelligence (GAI) on a large scale. It has also sparked research interest in developing similar technologies and investigating their applications and implications. In this paper, our primary goal is to provide a concise survey on the current lines of research on ChatGPT and its evolution. We considered both the glass box and black box views of ChatGPT, encompassing the components and foundational elements of the technology, as well as its applications, impacts, and implications. The glass box approach focuses on understanding the inner workings of the technology, and the black box approach embraces it as a complex system, and thus examines its inputs,
    
[^6]: 高效的贝叶斯行程时间层析成像方法，利用基于敏感性的多项式混沌展开和深度生成网络的地质复杂先验

    Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks. (arXiv:2307.04228v1 [physics.geo-ph])

    [http://arxiv.org/abs/2307.04228](http://arxiv.org/abs/2307.04228)

    本论文提出了一种高效的贝叶斯行程时间层析成像方法，利用敏感性信息的多项式混沌展开和深度生成网络，以处理地质复杂性先验的挑战。

    

    蒙特卡洛马尔可夫链（MCMC）方法常常面临两个根本性挑战：先验分布的准确刻画和似然函数的高效评估。在层析成像的贝叶斯研究中，主成分分析（PCA）在某些情况下可以方便地定义先验分布，并同时借助基于多项式混沌展开（PCE）的准确代理模型来替代计算密集的全物理正向求解器。当PCA无法直接提供定义先验分布的方式时，可以采用深度生成模型（例如变分自编码器（VAEs））等替代方法。然而，准确产生一个能够捕捉VAE的潜在参数与正向建模输出之间复杂非线性关系的代理模型是一个显著的挑战。

    Monte Carlo Markov Chain (MCMC) methods commonly confront two fundamental challenges: the accurate characterization of the prior distribution and the efficient evaluation of the likelihood. In the context of Bayesian studies on tomography, principal component analysis (PCA) can in some cases facilitate the straightforward definition of the prior distribution, while simultaneously enabling the implementation of accurate surrogate models based on polynomial chaos expansion (PCE) to replace computationally intensive full-physics forward solvers. When faced with scenarios where PCA does not offer a direct means of easily defining the prior distribution alternative methods like deep generative models (e.g., variational autoencoders (VAEs)), can be employed as viable options. However, accurately producing a surrogate capable of capturing the intricate non-linear relationship between the latent parameters of a VAE and the outputs of forward modeling presents a notable challenge. Indeed, while
    
[^7]: 基于分层自编码器的大规模高分辨率科学数据有损压缩

    Hierarchical Autoencoder-based Lossy Compression for Large-scale High-resolution Scientific Data. (arXiv:2307.04216v1 [cs.LG])

    [http://arxiv.org/abs/2307.04216](http://arxiv.org/abs/2307.04216)

    本论文提出了一种基于分层自编码器的神经网络模型，能够显著压缩大规模高分辨率科学数据，并保持高重建质量。

    

    有损压缩已成为许多领域中减小数据大小的重要技术。这种压缩方法对于大小在几个PB范围内的大规模科学数据尤为重要。虽然基于自编码器的模型已成功地用于压缩图像和视频，但这种神经网络在科学数据领域尚未广为关注。我们的工作提出了一个神经网络，不仅可以显著压缩大规模科学数据，还可以保持高重建质量。所提出的模型在公开的科学基准数据上进行了测试，并应用于一种大规模高分辨率的气候模拟数据集。我们的模型在几个基准数据集上实现了140的压缩比，同时保持重建质量。高分辨率社区地球系统模型(CESM) Version 1.3的模拟数据在压缩比达到200的同时进行了压缩。

    Lossy compression has become an important technique to reduce data size in many domains. This type of compression is especially valuable for large-scale scientific data, whose size ranges up to several petabytes. Although Autoencoder-based models have been successfully leveraged to compress images and videos, such neural networks have not widely gained attention in the scientific data domain. Our work presents a neural network that not only significantly compresses large-scale scientific data but also maintains high reconstruction quality. The proposed model is tested with scientific benchmark data available publicly and applied to a large-scale high-resolution climate modeling data set. Our model achieves a compression ratio of 140 on several benchmark data sets without compromising the reconstruction quality. Simulation data from the High-Resolution Community Earth System Model (CESM) Version 1.3 over 500 years are also being compressed with a compression ratio of 200 while the recon
    
[^8]: 广义的基于动作的球权恢复模型使用360度数据

    Generalized Action-based Ball Recovery Model using 360$^\circ$ data. (arXiv:2307.04215v1 [stat.AP])

    [http://arxiv.org/abs/2307.04215](http://arxiv.org/abs/2307.04215)

    本研究通过使用360度数据，提出了一个广义的基于动作的球权恢复模型，探究球队在失去球权后如何尽快恢复球权，球队位置对球权恢复的影响以及在压力下更易失误的球员等问题。

    

    尽管拥有更多的控球并不一定导致胜利，但像曼城、利物浦和利兹联这样的球队在过去几年里一直试图在失去球后尽快恢复球权。现在，世界上一些顶级教练应用高位逼抢的风格，像“五秒规则”这样的概念通常归功于瓜迪奥拉，正在扩散开来，成为许多球队过去几年来打球的基本部分。媒体经常听到“不让他们喘息”和“尽快拿回球权”等说法，但是哪些动作最有可能改变球权？球队的位置对球权恢复有什么影响？哪些球员在压力下更容易失误？我们能否评估那些并不像上述球队一样强烈逼抢控球球员的球队的防守动态？我们试图回答这些问题和其他问题。

    Even though having more possession does not necessarily lead to winning, teams like Manchester City, Liverpool, and Leeds United notably have tried to recover the ball quickly after they lost it over the past few years. Nowadays, some of the top managers in the world apply high-pressing styles, and concepts such as the five-second rule, usually credited to Guardiola, have been spreading out [9][10], becoming a fundamental part of how lots of teams have played over the recent years. Expressions like "don't let them breathe" and "get the ball back as soon as possible" are often heard in the media [4][5][6], but what are the actions that most lead to a change in possession? What is the influence of a team's positioning on the ball recovery? Which are the players that more often collapse when under pressure? Can we evaluate the defensive dynamics of teams that do not necessarily press the player in possession as intensely as those mentioned above? We try to answer those and other questions
    
[^9]: 探究在强化学习中的稳定边界现象

    Investigating the Edge of Stability Phenomenon in Reinforcement Learning. (arXiv:2307.04210v1 [cs.LG])

    [http://arxiv.org/abs/2307.04210](http://arxiv.org/abs/2307.04210)

    本研究探究了强化学习中的稳定边界现象，发现即使在非政策深度强化学习中，稳定边界现象仍然存在，这有助于我们更好地理解和优化深度强化学习算法。

    

    最近在理解全批量梯度下降优化动力学方面取得了进展，通过有动量的全批量梯度下降在监督学习中揭示了稳定边界现象。稳定边界现象发生在Hessian的主导特征值达到二次损失的发散阈值时，此后它开始在阈值周围振荡，并且损失开始表现出局部不稳定性但在较长时间内逐渐减小。本研究中，我们探索了稳定边界现象在强化学习中的体现，特别是在各种数据场景中的离线到在线强化学习的非政策Q学习算法。我们的实验发现，尽管与监督学习存在显著的差异，如数据分布的非稳定性和引导bootstrap的使用，稳定边界现象仍然可能在非政策深度强化学习中存在。与监督学习不同的是，监督学习删除了误差减小的稳态（误差减小阶段下美在稳定边界的情况）。

    Recent progress has been made in understanding optimisation dynamics in neural networks trained with full-batch gradient descent with momentum with the uncovering of the edge of stability phenomenon in supervised learning. The edge of stability phenomenon occurs as the leading eigenvalue of the Hessian reaches the divergence threshold of the underlying optimisation algorithm for a quadratic loss, after which it starts oscillating around the threshold, and the loss starts to exhibit local instability but decreases over long time frames. In this work, we explore the edge of stability phenomenon in reinforcement learning (RL), specifically off-policy Q-learning algorithms across a variety of data regimes, from offline to online RL. Our experiments reveal that, despite significant differences to supervised learning, such as non-stationarity of the data distribution and the use of bootstrapping, the edge of stability phenomenon can be present in off-policy deep RL. Unlike supervised learnin
    
[^10]: 关于在企业中部署保护隐私的合成数据的挑战

    On the Challenges of Deploying Privacy-Preserving Synthetic Data in the Enterprise. (arXiv:2307.04208v1 [cs.LG])

    [http://arxiv.org/abs/2307.04208](http://arxiv.org/abs/2307.04208)

    本文研究了在企业中部署保护隐私的合成数据的挑战，重点关注个人和高度敏感数据所引起的隐私问题。鉴别出了40多个挑战并将其系统化，提出了企业可以采用的战略和系统方法来应对这些挑战。

    

    生成式人工智能技术正获得空前的普及，由于其卓越的能力带来了兴奋和忧虑的混合感。本文研究了部署合成数据的挑战，这是生成式人工智能的一个子领域。我们将重点放在企业部署上，特别关注个人和高度敏感数据所引起的隐私问题。我们鉴别出了40多个挑战，并将它们系统化为五个主要组别 - i）生成，ii）基础设施和架构，iii）治理，iv）合规和监管，和v）采纳。此外，我们讨论了企业可以采用的战略和系统方法来有效应对这些挑战，并通过建立对实施解决方案的信任来实现他们的目标。

    Generative AI technologies are gaining unprecedented popularity, causing a mix of excitement and apprehension through their remarkable capabilities. In this paper, we study the challenges associated with deploying synthetic data, a subfield of Generative AI. Our focus centers on enterprise deployment, with an emphasis on privacy concerns caused by the vast amount of personal and highly sensitive data. We identify 40+ challenges and systematize them into five main groups -- i) generation, ii) infrastructure & architecture, iii) governance, iv) compliance & regulation, and v) adoption. Additionally, we discuss a strategic and systematic approach that enterprises can employ to effectively address the challenges and achieve their goals by establishing trust in the implemented solutions.
    
[^11]: 扩展前向前向算法

    Extending the Forward Forward Algorithm. (arXiv:2307.04205v1 [cs.LG])

    [http://arxiv.org/abs/2307.04205](http://arxiv.org/abs/2307.04205)

    这个论文扩展了前向前向算法，首先在IMDb数据集上进行了情感分析任务，其次引入了金字塔优化策略来改进损失阈值，最后通过参数可视化得出了一些重要的洞察。

    

    前向前向算法是Geoffrey Hinton于2022年11月提出的一种用于训练神经网络的新方法，作为对反向传播的替代方法。在这个项目中，我们在MNIST数据集上复制了Hinton的实验，并随后通过两个重要的贡献扩展了该方法的范围。首先，我们为前向前向网络在IMDb电影评论数据集上建立了一个基准性能。据我们所知，我们在这个情感分析任务上的结果标志着该算法在计算机视觉之外的首次扩展。其次，我们引入了一种新颖的金字塔优化策略，用于损失阈值，这是前向前向方法特有的超参数。我们的金字塔方法表明，一个好的阈值策略会导致测试错误率的差异高达8%。最后，我们对训练参数进行可视化，并得出了一些重要的洞察，例如权重的平均值和方差显著增加了10-20倍。

    The Forward Forward algorithm, proposed by Geoffrey Hinton in November 2022, is a novel method for training neural networks as an alternative to backpropagation. In this project, we replicate Hinton's experiments on the MNIST dataset, and subsequently extend the scope of the method with two significant contributions. First, we establish a baseline performance for the Forward Forward network on the IMDb movie reviews dataset. As far as we know, our results on this sentiment analysis task marks the first instance of the algorithm's extension beyond computer vision. Second, we introduce a novel pyramidal optimization strategy for the loss threshold - a hyperparameter specific to the Forward Forward method. Our pyramidal approach shows that a good thresholding strategy causes a difference of upto 8% in test error. 1 Lastly, we perform visualizations of the trained parameters and derived several significant insights, such as a notably larger (10-20x) mean and variance in the weights acquire
    
[^12]: 轨迹对齐：通过分叉理论理解稳定边缘现象

    Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory. (arXiv:2307.04204v1 [cs.LG])

    [http://arxiv.org/abs/2307.04204](http://arxiv.org/abs/2307.04204)

    本文通过实证研究证明了梯度下降轨迹上的稳定边缘现象，并且对于特定的网络结构进行了轨迹对齐分析，建立了渐进尖锐化和稳定边缘现象，扩展了当前文献的研究结果。

    

    Cohen等人（2021）通过实证研究梯度下降（GD）轨迹上损失Hessian的最大特征值（即锐度），观察到一种称为稳定边缘（EoS）的现象。锐度在培训的早期阶段增加（称为渐进尖锐化），最终接近阈值$2/\text{(步长)}$附近停滞。本文通过实证研究首先证明了当EoS现象发生时，不同的GD轨迹（经过适当的参数化）在一个特定的分叉图上对齐，而与初始化无关。然后，我们对一个二层全连接线性网络和一个使用单个数据点训练的单神经元非线性网络严格证明了这种轨迹对齐现象。我们的轨迹对齐分析建立了渐进尖锐化和EoS现象，涵盖并扩展了最近文献中的研究结果。

    Cohen et al. (2021) empirically study the evolution of the largest eigenvalue of the loss Hessian, also known as sharpness, along the gradient descent (GD) trajectory and observe a phenomenon called the Edge of Stability (EoS). The sharpness increases at the early phase of training (referred to as progressive sharpening), and eventually saturates close to the threshold of $2 / \text{(step size)}$. In this paper, we start by demonstrating through empirical studies that when the EoS phenomenon occurs, different GD trajectories (after a proper reparameterization) align on a specific bifurcation diagram independent of initialization. We then rigorously prove this trajectory alignment phenomenon for a two-layer fully-connected linear network and a single-neuron nonlinear network trained with a single data point. Our trajectory alignment analysis establishes both progressive sharpening and EoS phenomena, encompassing and extending recent findings in the literature.
    
[^13]: 关于逻辑回归中参数估计的样本复杂度研究

    On the sample complexity of estimation in logistic regression. (arXiv:2307.04191v1 [math.ST])

    [http://arxiv.org/abs/2307.04191](http://arxiv.org/abs/2307.04191)

    本文研究了逻辑回归模型在标准正态协变量下的参数估计样本复杂度，发现样本复杂度曲线在逆温度方面有两个转折点，明确划分了低、中和高温度区域。

    

    逻辑回归模型是噪声二元分类问题中最常见的数据生成模型之一。本文研究了在标准正态协变量下，以$\ell_2$误差为限，估计逻辑回归模型参数的样本复杂度，考虑了维度和逆温度的影响。逆温度控制了数据生成过程中的信噪比。虽然逻辑回归的广义界限和渐近性能已经有了深入研究，但关于参数估计的非渐近样本复杂度在之前的分析中没有讨论其与误差和逆温度的依赖关系。我们展示了样本复杂度曲线在逆温度方面具有两个转折点（或临界点），明确划分了低、中和高温度区域。

    The logistic regression model is one of the most popular data generation model in noisy binary classification problems. In this work, we study the sample complexity of estimating the parameters of the logistic regression model up to a given $\ell_2$ error, in terms of the dimension and the inverse temperature, with standard normal covariates. The inverse temperature controls the signal-to-noise ratio of the data generation process. While both generalization bounds and asymptotic performance of the maximum-likelihood estimator for logistic regression are well-studied, the non-asymptotic sample complexity that shows the dependence on error and the inverse temperature for parameter estimation is absent from previous analyses. We show that the sample complexity curve has two change-points (or critical points) in terms of the inverse temperature, clearly separating the low, moderate, and high temperature regimes.
    
[^14]: 增强空间上下文的潜在图注意力

    Latent Graph Attention for Enhanced Spatial Context. (arXiv:2307.04149v1 [cs.CV])

    [http://arxiv.org/abs/2307.04149](http://arxiv.org/abs/2307.04149)

    本文提出了一种称为潜在图注意力（LGA）的模块化框架，用于增强图像中的全局上下文。LGA在计算上简洁且稳定，能够在小规模体系结构中实现接近大规模体系结构的性能，使得轻量级体系结构在边缘设备上更加实用。

    

    图像中的全局上下文在图像到图像转换问题中非常有价值。传统的基于注意力和图模型在很大程度上捕捉到了全局上下文，但是这些方法计算上比较昂贵。此外，现有的方法只能学习图像中任意两个点之间的配对语义关系。在本文中，我们提出了一种称为潜在图注意力（LGA）的计算简洁（与节点数量呈线性关系）和稳定的模块化框架，用于将全局上下文纳入现有体系结构中，特别是增强小规模体系结构的性能，使得轻量级体系结构对于计算能力较低和能量需求较低的边缘设备更加有用。LGA使用局部连接图网络来在空间上传播信息，从而方便地构建远距离的两个空间点之间的语义一致关系。

    Global contexts in images are quite valuable in image-to-image translation problems. Conventional attention-based and graph-based models capture the global context to a large extent, however, these are computationally expensive. Moreover, the existing approaches are limited to only learning the pairwise semantic relation between any two points on the image. In this paper, we present Latent Graph Attention (LGA) a computationally inexpensive (linear to the number of nodes) and stable, modular framework for incorporating the global context in the existing architectures, especially empowering small-scale architectures to give performance closer to large size architectures, thus making the light-weight architectures more useful for edge devices with lower compute power and lower energy needs. LGA propagates information spatially using a network of locally connected graphs, thereby facilitating to construct a semantically coherent relation between any two spatially distant points that also 
    
[^15]: 一项关于图表分类的调查和方法

    A Survey and Approach to Chart Classification. (arXiv:2307.04147v1 [cs.CV])

    [http://arxiv.org/abs/2307.04147](http://arxiv.org/abs/2307.04147)

    本文调查了当前图表分类技术的研究现状，并对基于机器学习、卷积神经网络和Transformer的方法进行了比较性能分析，以提高对图表的自动理解能力。

    

    图表在文档中表示了一个重要的视觉信息源，并促进了对通常以数字形式传达的信息的深入理解和解释。在科学文献中，有许多图表，每个图表都有其风格上的差异。最近，文档理解领域开始解决自动图表理解的问题，其中涉及到图表分类。本文提出了对当前最先进的图表分类技术的调查，并讨论了可用数据集及其支持的图表类型。我们将这些贡献分为基于机器学习、卷积神经网络和Transformer的传统方法。此外，我们在最近发布的CHARTINFO UB-UNITECH PMC数据集上对基于卷积神经网络和Transformer的方法进行了广泛的比较性能分析，该数据集用于ICPR 2022的CHART-Infographics竞赛。该数据集包括15个不同的图表类别，包括22923个训练样本。

    Charts represent an essential source of visual information in documents and facilitate a deep understanding and interpretation of information typically conveyed numerically. In the scientific literature, there are many charts, each with its stylistic differences. Recently the document understanding community has begun to address the problem of automatic chart understanding, which begins with chart classification. In this paper, we present a survey of the current state-of-the-art techniques for chart classification and discuss the available datasets and their supported chart types. We broadly classify these contributions as traditional approaches based on ML, CNN, and Transformers. Furthermore, we carry out an extensive comparative performance analysis of CNN-based and transformer-based approaches on the recently published CHARTINFO UB-UNITECH PMC dataset for the CHART-Infographics competition at ICPR 2022. The data set includes 15 different chart categories, including 22,923 training i
    
[^16]: 关于机器学习随机性对群体公平性影响的论文

    On The Impact of Machine Learning Randomness on Group Fairness. (arXiv:2307.04138v1 [cs.LG])

    [http://arxiv.org/abs/2307.04138](http://arxiv.org/abs/2307.04138)

    本文研究了在机器学习中，随机性对群体公平性的影响。研究发现，群体公平性指标的方差主要来自于在代表性不足的群体上的学习过程的高易变性，其中最主要的随机性来源是训练期间数据顺序的随机性。基于这些发现，可以通过改变数据顺序来控制模型的群体级准确性，而几乎不影响模型的整体性能。

    

    机器学习中用于衡量群体公平性的统计指标反映了不同群体算法性能之间的差距。然而，这些指标在不同训练实例之间存在很高的方差，使得它们在公平性的经验评估中不可靠。是什么导致了这种高方差？我们研究了训练神经网络中不同随机因素对群体公平性的影响。我们发现群体公平性指标的方差根植于在代表性不足的群体上的学习过程的高易变性。此外，我们确定了随机性主要来源是训练期间数据顺序的随机性。基于这些发现，我们展示了如何通过改变单个时期的数据顺序来控制群体级准确性（即模型公平性），并且对模型整体性能几乎没有影响。

    Statistical measures for group fairness in machine learning reflect the gap in performance of algorithms across different groups. These measures, however, exhibit a high variance between different training instances, which makes them unreliable for empirical evaluation of fairness. What causes this high variance? We investigate the impact on group fairness of different sources of randomness in training neural networks. We show that the variance in group fairness measures is rooted in the high volatility of the learning process on under-represented groups. Further, we recognize the dominant source of randomness as the stochasticity of data order during training. Based on these findings, we show how one can control group-level accuracy (i.e., model fairness), with high efficiency and negligible impact on the model's overall performance, by simply changing the data order for a single epoch.
    
[^17]: 碳效率神经架构搜索

    Carbon-Efficient Neural Architecture Search. (arXiv:2307.04131v1 [cs.LG])

    [http://arxiv.org/abs/2307.04131](http://arxiv.org/abs/2307.04131)

    本文介绍了一种碳效率的神经架构搜索方法（CE-NAS），通过动态平衡能效抽样和能耗评估任务，实现了更好的碳和搜索效率。

    

    本文提出了一种新颖的神经架构搜索（NAS）方法，旨在在模型设计过程中降低能源成本并提高碳效率。所提出的框架称为碳效率NAS（CE-NAS），它由具有不同能源需求的NAS评估算法、多目标优化器和启发式GPU分配策略组成。CE-NAS根据当前的碳排放动态平衡了能效抽样和能耗评估任务。使用最近的NAS基准数据集和两个碳追踪数据，我们的追踪驱动模拟表明CE-NAS在碳和搜索效率方面优于三个基准方法。

    This work presents a novel approach to neural architecture search (NAS) that aims to reduce energy costs and increase carbon efficiency during the model design process. The proposed framework, called carbon-efficient NAS (CE-NAS), consists of NAS evaluation algorithms with different energy requirements, a multi-objective optimizer, and a heuristic GPU allocation strategy. CE-NAS dynamically balances energy-efficient sampling and energy-consuming evaluation tasks based on current carbon emissions. Using a recent NAS benchmark dataset and two carbon traces, our trace-driven simulations demonstrate that CE-NAS achieves better carbon and search efficiency than the three baselines.
    
[^18]: 用于求解双曲型偏微分方程的深度学习框架：第一部分

    A Deep Learning Framework for Solving Hyperbolic Partial Differential Equations: Part I. (arXiv:2307.04121v1 [cs.LG])

    [http://arxiv.org/abs/2307.04121](http://arxiv.org/abs/2307.04121)

    本研究开发了一种物理信息深度学习框架，用于近似解非线性PDEs，特别是具有支配性双曲特性的PDEs。该框架在不具有先验知识的情况下能够处理冲击或间断，并能够自然处理边界条件、熵条件和正则性要求。

    

    物理信息神经网络（PINNs）已经成为一种强大的工具，可以提供对偏微分方程（PDEs）解的稳健准确的近似。然而，在尝试近似具有支配性双曲特性的PDEs时，PINNs面临严重困难和挑战。本研究侧重于开发一种物理信息深度学习框架，用于近似解非线性PDEs，这些PDEs可以在不具有任何先验知识的情况下产生冲击或间断。该框架受到有限元法的启发，该方法在离散化域中求解节点处的解值，并使用这些节点值获得全局定义的解场。基于间断Galerkin方法的严格数学基础，该框架自然处理边界条件（Neumann / Dirichlet），熵条件和正则性要求。进行了多个数值实验。

    Physics informed neural networks (PINNs) have emerged as a powerful tool to provide robust and accurate approximations of solutions to partial differential equations (PDEs). However, PINNs face serious difficulties and challenges when trying to approximate PDEs with dominant hyperbolic character. This research focuses on the development of a physics informed deep learning framework to approximate solutions to nonlinear PDEs that can develop shocks or discontinuities without any a-priori knowledge of the solution or the location of the discontinuities. The work takes motivation from finite element method that solves for solution values at nodes in the discretized domain and use these nodal values to obtain a globally defined solution field. Built on the rigorous mathematical foundations of the discontinuous Galerkin method, the framework naturally handles imposition of boundary conditions (Neumann/Dirichlet), entropy conditions, and regularity requirements. Several numerical experiments
    
[^19]: 《FILM:如何让少样本图像分类从预训练语言模型中受益?》

    FILM: How can Few-Shot Image Classification Benefit from Pre-Trained Language Models?. (arXiv:2307.04114v1 [cs.LG])

    [http://arxiv.org/abs/2307.04114](http://arxiv.org/abs/2307.04114)

    本文提出了一种使用预训练语言模型的新型少样本学习框架，通过对比学习，使用文本嵌入和度量模块来提高图像分类性能和可迁移性。

    

    少样本学习旨在训练能够利用少量样本推广到新类别的模型。最近，一系列工作提出利用可访问的类别名称语义信息来增强少样本学习。然而，这些工作主要集中在改进标准少样本学习框架中的视觉原型和特征提取器等现有模块，限制了语义信息的充分利用。本文提出了一种基于对比学习的预训练语言模型的新型少样本学习框架。为了解决视觉特征和文本嵌入之间的对齐挑战，我们精心设计了框架的文本分支，并引入了度量模块来推广余弦相似度。为了更好的可迁移性，我们让度量模块适应不同的少样本任务，并采用MAML进行双层优化训练模型。

    Few-shot learning aims to train models that can be generalized to novel classes with only a few samples. Recently, a line of works are proposed to enhance few-shot learning with accessible semantic information from class names. However, these works focus on improving existing modules such as visual prototypes and feature extractors of the standard few-shot learning framework. This limits the full potential use of semantic information. In this paper, we propose a novel few-shot learning framework that uses pre-trained language models based on contrastive learning. To address the challenge of alignment between visual features and textual embeddings obtained from text-based pre-trained language model, we carefully design the textual branch of our framework and introduce a metric module to generalize the cosine similarity. For better transferability, we let the metric module adapt to different few-shot tasks and adopt MAML to train the model via bi-level optimization. Moreover, we conduct 
    
[^20]: 从部分观测状态中学习时空连续神经偏微分方程

    Learning Space-Time Continuous Neural PDEs from Partially Observed States. (arXiv:2307.04110v1 [cs.LG])

    [http://arxiv.org/abs/2307.04110](http://arxiv.org/abs/2307.04110)

    该论文介绍了一种从部分观测状态学习时空连续神经PDE的新方法，该方法通过新颖的编码器设计和高效的概率框架，克服了先前方法的局限，实现了对复杂部分观测数据的网格独立建模。

    

    我们介绍了一种新颖的独立于网格的模型，用于从具有噪声和部分观测的不规则时空网格上学习偏微分方程（PDE）。我们提出了一个具有高效概率框架和改进的编码器设计的时空连续潜在神经PDE模型，以提高数据效率和网格独立性。潜在状态动力学由结合了插值法和线方法的PDE模型所控制。我们采用了均摊变分推理来近似后验估计，并利用多重射击技术来提高训练速度和稳定性。我们的模型在复杂的合成和真实世界数据集上展现出最先进的性能，克服了先前方法的局限，并有效处理部分观测数据。所提出的模型表现优于最近的方法，显示了其推进数据驱动PDE建模和实现复杂部分观测的网格独立建模的潜力。

    We introduce a novel grid-independent model for learning partial differential equations (PDEs) from noisy and partial observations on irregular spatiotemporal grids. We propose a space-time continuous latent neural PDE model with an efficient probabilistic framework and a novel encoder design for improved data efficiency and grid independence. The latent state dynamics are governed by a PDE model that combines the collocation method and the method of lines. We employ amortized variational inference for approximate posterior estimation and utilize a multiple shooting technique for enhanced training speed and stability. Our model demonstrates state-of-the-art performance on complex synthetic and real-world datasets, overcoming limitations of previous approaches and effectively handling partially-observed data. The proposed model outperforms recent methods, showing its potential to advance data-driven PDE modeling and enabling robust, grid-independent modeling of complex partially-observe
    
[^21]: 无假设偏差缓解方法

    Towards Assumption-free Bias Mitigation. (arXiv:2307.04105v1 [cs.LG])

    [http://arxiv.org/abs/2307.04105](http://arxiv.org/abs/2307.04105)

    本文提出了一种无假设的偏差缓解方法，用于解决机器学习模型对特定人口群体显示歧视和不公平预测行为的问题。

    

    尽管机器学习模型具有令人印象深刻的预测能力，但它们对特定人口群体显示出歧视，并且存在不公平的预测行为。为了减轻这种歧视，广泛的研究致力于通过多种方法消除敏感属性的不平等分布。然而，由于隐私问题，真实场景中经常无法得到或缺少敏感属性。因此，一些现有的工作在无敏感属性的情况下减轻了偏差。这些研究面临挑战，要么是对敏感属性的预测不准确，要么是需要减轻与偏差相关的人工定义的非敏感属性的不平等分布。后者对敏感和非敏感属性之间的相关性有着强大的假设。由于数据分布和任务目标的差异，对非敏感属性的强假设可能无效，并需要领域专业知识。在本文中，我们提出了一种无假设的偏差缓解方法。

    Despite the impressive prediction ability, machine learning models show discrimination towards certain demographics and suffer from unfair prediction behaviors. To alleviate the discrimination, extensive studies focus on eliminating the unequal distribution of sensitive attributes via multiple approaches. However, due to privacy concerns, sensitive attributes are often either unavailable or missing in real-world scenarios. Therefore, several existing works alleviate the bias without sensitive attributes. Those studies face challenges, either in inaccurate predictions of sensitive attributes or the need to mitigate unequal distribution of manually defined non-sensitive attributes related to bias. The latter requires strong assumptions about the correlation between sensitive and non-sensitive attributes. As data distribution and task goals vary, the strong assumption on non-sensitive attributes may not be valid and require domain expertise. In this work, we propose an assumption-free fra
    
[^22]: 一种通过最优输运进行条件采样的生成流

    A generative flow for conditional sampling via optimal transport. (arXiv:2307.04102v1 [stat.ML])

    [http://arxiv.org/abs/2307.04102](http://arxiv.org/abs/2307.04102)

    本论文提出了一种通过解决最优输运问题来描述条件分布的非参数生成模型，该模型使用块三角输运映射将参考样本迭代映射到目标样本，从而克服了参数偏差和基于梯度的优化器的限制。

    

    条件分布的采样是贝叶斯推断和密度估计的基本任务。生成模型，如归一化流和生成对抗网络，通过学习将简单参考模型（如标准高斯分布）推向目标分布的输运映射，来描述条件分布。虽然这些方法成功地描述了许多非高斯问题，但它们的性能通常受到参数偏差和基于梯度的（对抗性）优化器学习这些转换的可靠性的限制。本文提出了一种非参数生成模型，通过迭代地将参考样本映射到目标样本来描述条件分布。该模型使用块三角输运映射，其组件被证明可以表征目标分布的条件分布。这些映射是通过解决带权 $L^2$ 损失函数的最优输运问题得到的，从而扩展了[Trigila and Tabak, 2016]中的数据驱动方法用于条件采样。

    Sampling conditional distributions is a fundamental task for Bayesian inference and density estimation. Generative models, such as normalizing flows and generative adversarial networks, characterize conditional distributions by learning a transport map that pushes forward a simple reference (e.g., a standard Gaussian) to a target distribution. While these approaches successfully describe many non-Gaussian problems, their performance is often limited by parametric bias and the reliability of gradient-based (adversarial) optimizers to learn these transformations. This work proposes a non-parametric generative model that iteratively maps reference samples to the target. The model uses block-triangular transport maps, whose components are shown to characterize conditionals of the target distribution. These maps arise from solving an optimal transport problem with a weighted $L^2$ cost function, thereby extending the data-driven approach in [Trigila and Tabak, 2016] for conditional sampling
    
[^23]: GNP攻击: 通过梯度范数惩罚实现可转移的对抗样本

    GNP Attack: Transferable Adversarial Examples via Gradient Norm Penalty. (arXiv:2307.04099v1 [cs.LG])

    [http://arxiv.org/abs/2307.04099](http://arxiv.org/abs/2307.04099)

    本文提出了一种通过梯度范数惩罚实现可转移的对抗样本的方法，通过攻击多个深度学习模型和防御方法的实验证明，该方法非常有效。同时，该方法还可以与其他梯度方法结合使用，以实现更强大的攻击。

    

    具有良好可转移性的对抗样本使得针对不同目标模型的黑盒攻击成为可能，而不需要关于目标模型的内部知识。先前的方法往往生成具有很少或没有可转移性的对抗样本；也就是说，它们容易过拟合于源模型的特定架构和特征表示，生成的对抗样本几乎对目标黑盒模型没有作用。在本文中，我们提出了一种新颖的方法，利用梯度范数惩罚（GNP）增强对抗样本的可转移性。它驱使损失函数优化过程收敛到损失函数空间中的一个平坦区域的局部最优点。通过攻击11个最先进的深度学习模型和6种高级防御方法，我们经验证明，GNP在生成具有高转移性的对抗样本方面非常有效。我们还证明，它非常灵活，可以轻松与其他基于梯度的方法结合，以实现更强大的基于转移的攻击。

    Adversarial examples (AE) with good transferability enable practical black-box attacks on diverse target models, where insider knowledge about the target models is not required. Previous methods often generate AE with no or very limited transferability; that is, they easily overfit to the particular architecture and feature representation of the source, white-box model and the generated AE barely work for target, black-box models. In this paper, we propose a novel approach to enhance AE transferability using Gradient Norm Penalty (GNP). It drives the loss function optimization procedure to converge to a flat region of local optima in the loss landscape. By attacking 11 state-of-the-art (SOTA) deep learning models and 6 advanced defense methods, we empirically show that GNP is very effective in generating AE with high transferability. We also demonstrate that it is very flexible in that it can be easily integrated with other gradient based methods for stronger transfer-based attacks.
    
[^24]: 限制生成投影在单类分类和异常检测中的应用

    Restricted Generative Projection for One-Class Classification and Anomaly Detection. (arXiv:2307.04097v1 [cs.LG])

    [http://arxiv.org/abs/2307.04097](http://arxiv.org/abs/2307.04097)

    该论文提出了一种简单的框架，用于单类分类和异常检测。作者通过学习一种映射，将训练数据的未知分布转化为已知目标分布，以实现对异常数据的可靠区分。

    

    我们提出了一个简单的框架用于单类分类和异常检测。核心思想是通过学习一种映射来将训练（正常）数据的未知分布转化为已知目标分布。目标分布应该足够简单、紧凑和信息丰富。简洁性是为了确保我们可以轻松地从分布中进行取样，紧凑性是为了确保正常数据和异常数据之间的决策边界清晰可靠，信息丰富性是为了确保转换后的数据保留原始数据的重要信息。因此，我们提出将截断高斯分布、超球面上的均匀分布、超球面之间的均匀分布作为目标分布。我们同时使得转换后的数据分布与目标分布之间的距离最小化，同时保持原始数据的重建误差足够小。在多个基准数据集上进行了比较研究。

    We present a simple framework for one-class classification and anomaly detection. The core idea is to learn a mapping to transform the unknown distribution of training (normal) data to a known target distribution. Crucially, the target distribution should be sufficiently simple, compact, and informative. The simplicity is to ensure that we can sample from the distribution easily, the compactness is to ensure that the decision boundary between normal data and abnormal data is clear and reliable, and the informativeness is to ensure that the transformed data preserve the important information of the original data. Therefore, we propose to use truncated Gaussian, uniform in hypersphere, uniform on hypersphere, or uniform between hyperspheres, as the target distribution. We then minimize the distance between the transformed data distribution and the target distribution while keeping the reconstruction error for the original data small enough. Comparative studies on multiple benchmark datas
    
[^25]: 深度持续学习中的类别增量高斯混合模型

    Class-Incremental Mixture of Gaussians for Deep Continual Learning. (arXiv:2307.04094v1 [cs.LG])

    [http://arxiv.org/abs/2307.04094](http://arxiv.org/abs/2307.04094)

    这项工作提出了将高斯混合模型与持续学习框架相结合的类别增量方法，并成功地在深度特征提取器下进行了联合优化和调整。

    

    针对静态数据的持续学习模型注重以顺序方式学习和保留到达的概念。在最通用的类别增量环境中，我们必须准备好处理一个接一个到来的类别，而没有任何更高级别的分组。这个要求使得之前提出的许多方法无效，并迫使研究者寻找更灵活的替代方法。在这项工作中，我们遵循以质心驱动的方法的思想，提出将高斯混合模型完全整合到持续学习框架中的想法。通过采用基于梯度的方法和设计能够学习有区分性特征并避免退化解的损失函数，我们成功地将混合模型与深度特征提取器结合起来，实现在潜空间中的联合优化和调整。此外，我们还展示了我们的模型可以在固定的特征提取器下有效地在无内存的场景中学习。

    Continual learning models for stationary data focus on learning and retaining concepts coming to them in a sequential manner. In the most generic class-incremental environment, we have to be ready to deal with classes coming one by one, without any higher-level grouping. This requirement invalidates many previously proposed methods and forces researchers to look for more flexible alternative approaches. In this work, we follow the idea of centroid-driven methods and propose end-to-end incorporation of the mixture of Gaussians model into the continual learning framework. By employing the gradient-based approach and designing losses capable of learning discriminative features while avoiding degenerate solutions, we successfully combine the mixture model with a deep feature extractor allowing for joint optimization and adjustments in the latent space. Additionally, we show that our model can effectively learn in memory-free scenarios with fixed extractors. In the conducted experiments, we
    
[^26]: 通过查询正确学习决策树是NP难问题

    Properly Learning Decision Trees with Queries Is NP-Hard. (arXiv:2307.04093v1 [cs.CC])

    [http://arxiv.org/abs/2307.04093](http://arxiv.org/abs/2307.04093)

    通过查询正确学习决策树被证明是一个NP难问题，这就填补了学习理论中长期存在的一个空白，并引入了一种简化和加强决策树最小化问题下界的方法。

    

    我们证明了通过查询正确学习决策树是一个NP难问题，解决了学习理论中一个长期存在的开放问题。我们的研究对于决策树复杂性的困难蒸馏概念进行了引入，并提出了一种通用方法来确定导致复杂性的一个小输入集合，从而简化和加强了对决策树最小化问题的众所周知的下界。

    We prove that it is NP-hard to properly PAC learn decision trees with queries, resolving a longstanding open problem in learning theory (Bshouty 1993; Guijarro-Lavin-Raghavan 1999; Mehta-Raghavan 2002; Feldman 2016). While there has been a long line of work, dating back to (Pitt-Valiant 1988), establishing the hardness of properly learning decision trees from random examples, the more challenging setting of query learners necessitates different techniques and there were no previous lower bounds. En route to our main result, we simplify and strengthen the best known lower bounds for a different problem of Decision Tree Minimization (Zantema-Bodlaender 2000; Sieling 2003).  On a technical level, we introduce the notion of hardness distillation, which we study for decision tree complexity but can be considered for any complexity measure: for a function that requires large decision trees, we give a general method for identifying a small set of inputs that is responsible for its complexity.
    
[^27]: DebateKG: 用语义知识图自动创建政策辩论案例

    DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs. (arXiv:2307.04090v1 [cs.CL])

    [http://arxiv.org/abs/2307.04090](http://arxiv.org/abs/2307.04090)

    本论文提出了一种利用语义知识图自动创建政策辩论案例的方法，通过在争论的语义知识图上进行限制最短路径遍历，有效构建高质量的辩论案例。研究结果表明，在美国竞赛辩论中，利用这种方法显著改进了已有数据集DebateSum，并贡献了新的例子和有用的元数据。通过使用txtai语义搜索和知识图工具链，创建和贡献了9个语义知识图，同时提出了一种独特的评估方法来确定哪个知识图更适合政策辩论案例生成。

    

    近期相关工作表明，自然语言处理系统在解决竞赛辩论中的问题方面具有应用性。竞赛辩论中最重要的任务之一是辩手创建高质量的辩论案例。我们展示了使用限制最短路径遍历在争论的语义知识图上构建有效的辩论案例的方法。我们在一个名为DebateSum的大规模数据集上研究了这种潜力，该数据集针对的是一种名为政策辩论的美国竞赛辩论类型。我们通过向数据集中引入53180个新的例子，并为每个例子提供进一步有用的元数据，显著改进了DebateSum。我们利用txtai语义搜索和知识图工具链基于这个数据集产生并贡献了9个语义知识图。我们创建了一种独特的评估方法，以确定在政策辩论案例生成的背景下哪个知识图更好。

    Recent work within the Argument Mining community has shown the applicability of Natural Language Processing systems for solving problems found within competitive debate. One of the most important tasks within competitive debate is for debaters to create high quality debate cases. We show that effective debate cases can be constructed using constrained shortest path traversals on Argumentative Semantic Knowledge Graphs. We study this potential in the context of a type of American Competitive Debate, called Policy Debate, which already has a large scale dataset targeting it called DebateSum. We significantly improve upon DebateSum by introducing 53180 new examples, as well as further useful metadata for every example, to the dataset. We leverage the txtai semantic search and knowledge graph toolchain to produce and contribute 9 semantic knowledge graphs built on this dataset. We create a unique method for evaluating which knowledge graphs are better in the context of producing policy deb
    
[^28]: 自校准分类器引导下，少量标记数据的基于评分的条件生成

    Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance. (arXiv:2307.04081v1 [cs.CV])

    [http://arxiv.org/abs/2307.04081](http://arxiv.org/abs/2307.04081)

    本论文提出了一种通过自校准分类器引导的方法改进基于评分的条件生成模型，以提高使用少量标记数据的准确性和性能。通过将分类器作为无条件生成模型的另一种视角，并利用标记和未标记数据来校准分类器，实验证实该方法的有效性。

    

    基于评分的生成模型（SGMs）是一种流行的深度生成模型家族，能够达到领先的图像生成质量。早期的研究已经将SGMs扩展到处理类条件生成，通过将无条件的SGM与经过训练的分类器的引导相结合。然而，这种分类器引导的SGMs在训练数量较少的标记数据时并不总能实现准确的条件生成。我们认为问题根源在于分类器的不可靠梯度和无法充分利用未标记数据。因此，我们提出通过让分类器自校准来改进分类器引导的SGMs。我们的关键思想是使用能量模型的原则将分类器转化为无条件SGM的另一种视角。然后，可以采用现有的无条件SGM损失函数来使用标记和未标记数据来校准分类器。实证结果验证了所提出方法显著改善了条件生成的性能。

    Score-based Generative Models (SGMs) are a popular family of deep generative models that achieves leading image generation quality. Earlier studies have extended SGMs to tackle class-conditional generation by coupling an unconditional SGM with the guidance of a trained classifier. Nevertheless, such classifier-guided SGMs do not always achieve accurate conditional generation, especially when trained with fewer labeled data. We argue that the issue is rooted in unreliable gradients of the classifier and the inability to fully utilize unlabeled data during training. We then propose to improve classifier-guided SGMs by letting the classifier calibrate itself. Our key idea is to use principles from energy-based models to convert the classifier as another view of the unconditional SGM. Then, existing loss for the unconditional SGM can be adopted to calibrate the classifier using both labeled and unlabeled data. Empirical results validate that the proposed approach significantly improves the
    
[^29]: 迈向快速可扩展的私密推理

    Towards Fast and Scalable Private Inference. (arXiv:2307.04077v1 [cs.CR])

    [http://arxiv.org/abs/2307.04077](http://arxiv.org/abs/2307.04077)

    私密推理是一种应用于神经网络的新兴技术，可以通过解决各种保护隐私计算的开销问题，实现用户数据的机密性和控制性保护。

    

    隐私和安全迅速成为了首要的设计限制条件。现在用户要求在谁可以看到他们的数据（机密性）以及如何使用它（控制）方面获得更多的保护。现有的用于安全的加密技术不够完善：它们在存储或通信时保护数据，但必须解密来进行计算。幸运的是，存在一种被称为保护隐私计算（PPC）的新计算范式。新兴的PPC技术可以用于安全的外包计算，或使两个参与方能够进行计算而不泄露用户的秘密数据。尽管在数字时代，这些技术具有革命性的潜力来改变用户的保护，但由于计算、通信和存储开销过高，实现一直受到限制。本文回顾了最近关于使用神经网络中的私密推理（PI）来解决各种PPC开销的努力。首先，介绍了问题和各种技术，包括

    Privacy and security have rapidly emerged as first order design constraints. Users now demand more protection over who can see their data (confidentiality) as well as how it is used (control). Here, existing cryptographic techniques for security fall short: they secure data when stored or communicated but must decrypt it for computation. Fortunately, a new paradigm of computing exists, which we refer to as privacy-preserving computation (PPC). Emerging PPC technologies can be leveraged for secure outsourced computation or to enable two parties to compute without revealing either users' secret data. Despite their phenomenal potential to revolutionize user protection in the digital age, the realization has been limited due to exorbitant computational, communication, and storage overheads.  This paper reviews recent efforts on addressing various PPC overheads using private inference (PI) in neural network as a motivating application. First, the problem and various technologies, including 
    
[^30]: 基于癌症多组学数据的癌症新亚型和治疗的多头注意力机制学习

    Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data. (arXiv:2307.04075v1 [cs.LG])

    [http://arxiv.org/abs/2307.04075](http://arxiv.org/abs/2307.04075)

    本研究提出了一种基于注意力机制的无监督对比学习框架（AMUCL），用于分析癌症多组学数据并识别癌症新亚型。通过多头注意力机制和解耦对比学习模型（DMACL），该方法能够深度提取多组学数据特征并进行亚型聚类。

    

    由于癌症的高异质性和临床特征，不同癌症亚型之间的多组学数据和临床特征存在显著差异。因此，癌症亚型的识别和发现对于癌症的诊断、治疗和预后至关重要。本研究提出了一种基于注意力机制的无监督对比学习框架（AMUCL），用于分析癌症多组学数据，从而识别和表征癌症亚型。AMUCL框架包括一个无监督的多头注意力机制，用于深度提取多组学数据特征。重要的是，提出了一种基于多头注意力机制的解耦对比学习模型（DMACL），用于学习多组学数据特征和聚类，并识别新的癌症亚型。这种无监督对比学习方法通过计算特征空间中样本之间的相似度来聚类亚型。

    Due to the high heterogeneity and clinical characteristics of cancer, there are significant differences in multi-omics data and clinical features among subtypes of different cancers. Therefore, the identification and discovery of cancer subtypes are crucial for the diagnosis, treatment, and prognosis of cancer. In this study, we proposed a generalization framework based on attention mechanisms for unsupervised contrastive learning (AMUCL) to analyze cancer multi-omics data for the identification and characterization of cancer subtypes. AMUCL framework includes a unsupervised multi-head attention mechanism, which deeply extracts multi-omics data features. Importantly, a decoupled contrastive learning model (DMACL) based on a multi-head attention mechanism is proposed to learn multi-omics data features and clusters and identify new cancer subtypes. This unsupervised contrastive learning method clusters subtypes by calculating the similarity between samples in the feature space and sample
    
[^31]: 基于生成性神经网络的超高维非凸景观的大规模全球优化

    Large-scale global optimization of ultra-high dimensional non-convex landscapes based on generative neural networks. (arXiv:2307.04065v1 [cs.LG])

    [http://arxiv.org/abs/2307.04065](http://arxiv.org/abs/2307.04065)

    本论文提出了一种基于深度生成网络的非凸优化算法，通过演化网络输出分布函数，实现了在超高维度景观中的有效搜索。实验证明该方法在较少的函数评价次数下表现优于其他算法。

    

    我们提出了一种基于深度生成网络训练的非凸优化算法元启发式，能够在连续的超高维景观中进行有效搜索。在网络训练过程中，利用采样的局部梯度群体，在定制损失函数中演化网络输出分布函数，使其趋向于高性能最优点。深度网络结构经过定制，能够在训练过程中逐步增长，从而使算法能够处理高维景观中的维度灾难特征。我们将这个概念应用于一系列标准优化问题，其中维度高达一千，结果表明我们的方法在较少的函数评价次数下表现更好，相比最先进的算法基准。我们还讨论了深度网络过参数化、损失函数设计和适当的网络架构选择在优化中的作用。

    We present a non-convex optimization algorithm metaheuristic, based on the training of a deep generative network, which enables effective searching within continuous, ultra-high dimensional landscapes. During network training, populations of sampled local gradients are utilized within a customized loss function to evolve the network output distribution function towards one peak at high-performing optima. The deep network architecture is tailored to support progressive growth over the course of training, which allows the algorithm to manage the curse of dimensionality characteristic of high-dimensional landscapes. We apply our concept to a range of standard optimization problems with dimensions as high as one thousand and show that our method performs better with fewer function evaluations compared to state-of-the-art algorithm benchmarks. We also discuss the role of deep network over-parameterization, loss function engineering, and proper network architecture selection in optimization,
    
[^32]: 双向注意力作为连续词专家的混合物

    Bidirectional Attention as a Mixture of Continuous Word Experts. (arXiv:2307.04057v1 [cs.CL])

    [http://arxiv.org/abs/2307.04057](http://arxiv.org/abs/2307.04057)

    双向注意力模型具有混合专家权重，类似于连续词袋模型（CBOW）的统计模型，它在大型语言模型中起到了重要作用。

    

    双向注意力由位置编码和屏蔽语言模型（MLM）目标组成的自注意力构成，已成为现代大型语言模型（LLMs）的关键组件。尽管它在实践中取得了成功，但很少有研究探讨它的统计基础：双向注意力隐含地拟合了什么统计模型？它与非注意机制的先驱有何不同？本文探讨了这些问题。关键观察是，重新参数化后，拟合单层单头双向注意力等于拟合具有混合专家权重的连续词袋（CBOW）模型。此外，具有多个头和多个层的双向注意力等价于堆叠的MoEs和MoEs的混合。这个统计观点揭示了MoE在双向注意力中的独特用途，这与其在处理异构性方面的实际有效性相一致。

    Bidirectional attention $\unicode{x2013}$ composed of self-attention with positional encodings and the masked language model (MLM) objective $\unicode{x2013}$ has emerged as a key component of modern large language models (LLMs). Despite its empirical success, few studies have examined its statistical underpinnings: What statistical model is bidirectional attention implicitly fitting? What sets it apart from its non-attention predecessors? We explore these questions in this paper. The key observation is that fitting a single-layer single-head bidirectional attention, upon reparameterization, is equivalent to fitting a continuous bag of words (CBOW) model with mixture-of-experts (MoE) weights. Further, bidirectional attention with multiple heads and multiple layers is equivalent to stacked MoEs and a mixture of MoEs, respectively. This statistical viewpoint reveals the distinct use of MoE in bidirectional attention, which aligns with its practical effectiveness in handling heterogeneous
    
[^33]: 流形滤波-组合网络

    Manifold Filter-Combine Networks. (arXiv:2307.04056v1 [stat.ML])

    [http://arxiv.org/abs/2307.04056](http://arxiv.org/abs/2307.04056)

    这篇论文介绍了一类称为流形滤波-组合网络的大型流形神经网络。作者提出了一种基于构建数据驱动图的方法来实现这种网络，并提供了收敛到连续极限的充分条件，其收敛速度不依赖于滤波器数量。

    

    我们介绍了一类大型流形神经网络(MNNs)，我们称之为流形滤波-组合网络。这个类别包括了Wang、Ruiz和Ribeiro之前的研究中考虑的MNNs，流形散射变换(一种基于小波的神经网络模型)，以及其他有趣的之前在文献中未考虑的示例，如Kipf和Welling的图卷积网络的流形等效。然后，我们考虑了一种基于构建数据驱动图的方法，用于在没有对流形有全局知识的情况下实现这样的网络，而只能访问有限数量的样本点。我们提供了网络在样本点数趋于无穷大时能够保证收敛到其连续极限的充分条件。与之前的工作(主要关注特定的MNN结构和图构建)不同，我们的收敛速度并不依赖于使用的滤波器数量。而且，它表现出线性的收敛速度。

    We introduce a large class of manifold neural networks (MNNs) which we call Manifold Filter-Combine Networks. This class includes as special cases, the MNNs considered in previous work by Wang, Ruiz, and Ribeiro, the manifold scattering transform (a wavelet-based model of neural networks), and other interesting examples not previously considered in the literature such as the manifold equivalent of Kipf and Welling's graph convolutional network. We then consider a method, based on building a data-driven graph, for implementing such networks when one does not have global knowledge of the manifold, but merely has access to finitely many sample points. We provide sufficient conditions for the network to provably converge to its continuum limit as the number of sample points tends to infinity. Unlike previous work (which focused on specific MNN architectures and graph constructions), our rate of convergence does not explicitly depend on the number of filters used. Moreover, it exhibits line
    
[^34]: 具有策略性买家的情境动态定价

    Contextual Dynamic Pricing with Strategic Buyers. (arXiv:2307.04055v1 [stat.ML])

    [http://arxiv.org/abs/2307.04055](http://arxiv.org/abs/2307.04055)

    本文研究了具有策略性买家的情境动态定价问题，提出了一种策略动态定价策略，将买家的策略行为纳入在线学习中，以最大化卖方的累计收益。

    

    个性化定价是企业常用的一种针对个体特征制定价格的策略。在这个过程中，买家也可以通过操纵特征数据来获取更低的价格，但这也会导致特定的操作成本。这种策略行为可能会阻碍企业最大化利润。本文研究了具有策略性买家的情境动态定价问题。卖方无法观察到买家的真实特征，而只能观察到买家根据策略行为操纵后的特征。此外，卖方只能观察到买家对产品的估值，而无法直接获取具体数值，只能得到一个二进制的响应，表示是否发生销售。鉴于这些挑战，我们提出了一种策略动态定价策略，将买家的策略行为纳入在线学习中，以最大化卖方的累计收益。首先证明了现有的不考虑策略性的定价策略的存在限制。

    Personalized pricing, which involves tailoring prices based on individual characteristics, is commonly used by firms to implement a consumer-specific pricing policy. In this process, buyers can also strategically manipulate their feature data to obtain a lower price, incurring certain manipulation costs. Such strategic behavior can hinder firms from maximizing their profits. In this paper, we study the contextual dynamic pricing problem with strategic buyers. The seller does not observe the buyer's true feature, but a manipulated feature according to buyers' strategic behavior. In addition, the seller does not observe the buyers' valuation of the product, but only a binary response indicating whether a sale happens or not. Recognizing these challenges, we propose a strategic dynamic pricing policy that incorporates the buyers' strategic behavior into the online learning to maximize the seller's cumulative revenue. We first prove that existing non-strategic pricing policies that neglect
    
[^35]: 学习将辅助数据集分组用于分子问题

    Learning to Group Auxiliary Datasets for Molecule. (arXiv:2307.04052v1 [q-bio.BM])

    [http://arxiv.org/abs/2307.04052](http://arxiv.org/abs/2307.04052)

    本文提出了一种名为MolGroup的方法，通过将图结构相似性和任务相似性相结合，预测每个辅助分子数据集的潜在好处，以解决合作使用辅助数据集时的负迁移问题。

    

    小分子数据集中有限的注释可用性对机器学习模型提出了挑战。为了解决这个问题，一种常见的策略是与额外的辅助数据集合作。然而，拥有更多的数据并不总是能保证改进。当目标数据集中的知识与辅助分子数据集中的知识不同或相互矛盾时，负迁移可能会发生。鉴于此，当共同训练时，确定可以使目标数据集受益的辅助分子数据集仍然是一个关键而未解决的问题。通过经验分析，我们观察到将图结构相似性和任务相似性相结合可以作为确定高亲和性辅助数据集的更可靠指标。在此基础上，我们提出了MolGroup，它将数据集亲和性分为任务亲和性和结构亲和性，以预测每个辅助分子数据集的潜在好处。MolGroup通过利用路由机制来实现。

    The limited availability of annotations in small molecule datasets presents a challenge to machine learning models. To address this, one common strategy is to collaborate with additional auxiliary datasets. However, having more data does not always guarantee improvements. Negative transfer can occur when the knowledge in the target dataset differs or contradicts that of the auxiliary molecule datasets. In light of this, identifying the auxiliary molecule datasets that can benefit the target dataset when jointly trained remains a critical and unresolved problem. Through an empirical analysis, we observe that combining graph structure similarity and task similarity can serve as a more reliable indicator for identifying high-affinity auxiliary datasets. Motivated by this insight, we propose MolGroup, which separates the dataset affinity into task and structure affinity to predict the potential benefits of each auxiliary molecule dataset. MolGroup achieves this by utilizing a routing mecha
    
[^36]: 基于优化的学习用于卡车运输服务网络中的动态负载规划

    Optimization-based Learning for Dynamic Load Planning in Trucking Service Networks. (arXiv:2307.04050v1 [cs.AI])

    [http://arxiv.org/abs/2307.04050](http://arxiv.org/abs/2307.04050)

    本文为快递运营中的负载规划问题提出了一种基于优化的学习方法，全面考虑了负载和流程规划的挑战，并开发了一个决策支持工具。研究发现在网络中存在大量的对称性，导致优化求解器返回不同的解决方案，降低了规划人员对优化求解的信任度。

    

    负载规划问题是快递运营中服务网络设计的一个关键挑战：它决定了在终端之间如何在时间上分配多少辆拖车（或负载）进行派遣。另一个关键挑战是确定一个流程计划，它指定了如何将包裹体积分配给计划的负载。本文考虑到了动态负载规划问题（DLPP），它同时考虑了负载和流程规划的挑战，以在操作日之前随着需求预测的变化而调整负载和流程。本文旨在开发一个决策支持工具，为网络中各个终端的规划人员提供决策支持。本文将DLPP形式化为一个MIP，并证明它在每个商品都可以通过主路径和备用路径进行路由的网络中有大量的对称性。因此，优化求解器可能会对密切相关的问题返回根本不同的解决方案，使规划人员感到困惑，降低对优化求解的信任度。

    The load planning problem is a critical challenge in service network design for parcel carriers: it decides how many trailers (or loads) to assign for dispatch over time between pairs of terminals. Another key challenge is to determine a flow plan, which specifies how parcel volumes are assigned to planned loads. This paper considers the Dynamic Load Planning Problem (DLPP) that considers both flow and load planning challenges jointly to adjust loads and flows as the demand forecast changes over time before the day of operations. The paper aims at developing a decision-support tool to inform planners making these decisions at terminals across the network. The paper formulates the DLPP as a MIP and shows that it admits a large number of symmetries in a network where each commodity can be routed through primary and alternate paths. As a result, an optimization solver may return fundamentally different solutions to closely related problems, confusing planners and reducing trust in optimiz
    
[^37]: 并行算法与神经执行同步

    Parallel Algorithms Align with Neural Execution. (arXiv:2307.04049v1 [cs.LG])

    [http://arxiv.org/abs/2307.04049](http://arxiv.org/abs/2307.04049)

    这项研究发现，与顺序算法相比，并行算法能充分利用神经算法推理器的计算能力，从而减少训练时间并获得更好的预测性能。

    

    神经算法推理器是并行处理器。教授它们顺序算法与其性质相矛盾，使得它们的计算中包含很多冗余。然而并行算法可以充分利用它们的计算能力，因此只需执行较少的层次。这大大减少了训练时间，我们观察到与CLRS框架上顺序实现的搜索、排序和寻找强连接组件相比，并行实现的训练时间大大缩短。此外，大多数情况下，并行版本的预测性能更好。

    Neural algorithmic reasoners are parallel processors. Teaching them sequential algorithms contradicts this nature, rendering a significant share of their computations redundant. Parallel algorithms however may exploit their full computational power, therefore requiring fewer layers to be executed. This drastically reduces training times, as we observe when comparing parallel implementations of searching, sorting and finding strongly connected components to their sequential counterparts on the CLRS framework. Additionally, parallel versions achieve strongly superior predictive performance in most cases.
    
[^38]: 使用对抗训练的深度神经网络估计器在非参数回归中的超范数收敛性

    Sup-Norm Convergence of Deep Neural Network Estimator for Nonparametric Regression by Adversarial Training. (arXiv:2307.04042v1 [stat.ML])

    [http://arxiv.org/abs/2307.04042](http://arxiv.org/abs/2307.04042)

    我们展示了使用对抗训练的深度神经网络估计器在非参数回归中的超范数收敛性。我们发现普通的对抗训练使得神经估计器不一致，但通过所提出的带修正的对抗训练，深度神经网络估计器在超范数意义下达到最优速率。我们的实验证实了这些理论发现。

    

    我们展示了使用一种新颖的对抗训练方案的深度神经网络估计器的超范数收敛性。针对非参数回归问题，已经证明使用深度神经网络的估计器在$L2$-范数意义下可以获得更好的性能。相比之下，由于神经网络模型的深度结构，使用最小二乘法的神经估计器很难达到超范数收敛。在本研究中，我们发展了一种对抗训练方案，并研究了深度神经网络估计器的超范数收敛性。首先，我们发现普通的对抗训练使得神经估计器不一致。其次，我们展示了通过所提出的带修正的对抗训练，深度神经网络估计器在超范数意义下达到最优速率。我们将我们的对抗训练扩展到了一般的损失函数和数据生成函数的设置。我们的实验支持了理论发现。

    We show the sup-norm convergence of deep neural network estimators with a novel adversarial training scheme. For the nonparametric regression problem, it has been shown that an estimator using deep neural networks can achieve better performances in the sense of the $L2$-norm. In contrast, it is difficult for the neural estimator with least-squares to achieve the sup-norm convergence, due to the deep structure of neural network models. In this study, we develop an adversarial training scheme and investigate the sup-norm convergence of deep neural network estimators. First, we find that ordinary adversarial training makes neural estimators inconsistent. Second, we show that a deep neural network estimator achieves the optimal rate in the sup-norm sense by the proposed adversarial training with correction. We extend our adversarial training to general setups of a loss function and a data-generating function. Our experiments support the theoretical findings.
    
[^39]: 设计一个人类和卷积神经网络之间的直接反馈循环通过局部解释

    Designing a Direct Feedback Loop between Humans and Convolutional Neural Networks through Local Explanations. (arXiv:2307.04036v1 [cs.HC])

    [http://arxiv.org/abs/2307.04036](http://arxiv.org/abs/2307.04036)

    这篇论文设计了DeepFuse，它是第一个实现用户和CNN之间直接反馈循环的交互设计，通过局部解释帮助CNN工程师系统地诊断和修改CNN的脆弱性。

    

    局部解释通过图像热图解释卷积神经网络（CNN）是如何得出他们的输出结果的。由于其直观明了的特点，该方法已成为诊断CNN的最受欢迎的可解释AI（XAI）方法之一。然而，通过我们的初步研究，我们捕捉到ML工程师对局部解释的价值和不可或缺的看法存在矛盾：一方面认为局部解释是构建CNN不可或缺的愿景，另一方面却又认为它是一个消耗能量的过程，因为它基于启发式的方法来检测脆弱性。此外，基于诊断所学到的脆弱性指导CNN的过程也非常具有挑战性。为了缓解这个差距，我们设计了DeepFuse，这是第一个能够实现用户和CNN之间直接反馈循环的交互设计，用于诊断和修改CNN的脆弱性，并使用局部解释帮助CNN工程师系统地搜索“不合理”的局部解释，并为那些被识别为不合理的解释标注新的边界。

    The local explanation provides heatmaps on images to explain how Convolutional Neural Networks (CNNs) derive their output. Due to its visual straightforwardness, the method has been one of the most popular explainable AI (XAI) methods for diagnosing CNNs. Through our formative study (S1), however, we captured ML engineers' ambivalent perspective about the local explanation as a valuable and indispensable envision in building CNNs versus the process that exhausts them due to the heuristic nature of detecting vulnerability. Moreover, steering the CNNs based on the vulnerability learned from the diagnosis seemed highly challenging. To mitigate the gap, we designed DeepFuse, the first interactive design that realizes the direct feedback loop between a user and CNNs in diagnosing and revising CNN's vulnerability using local explanations. DeepFuse helps CNN engineers to systemically search "unreasonable" local explanations and annotate the new boundaries for those identified as unreasonable 
    
[^40]: 学习用于测试时领域泛化的变分邻居标签

    Learning Variational Neighbor Labels for Test-Time Domain Generalization. (arXiv:2307.04033v1 [cs.LG])

    [http://arxiv.org/abs/2307.04033](http://arxiv.org/abs/2307.04033)

    本文提出了一种用于测试时领域泛化的方法，通过在测试时使用概率伪标签和变分邻居标签来推广源域训练的模型到目标领域，以提高模型的鲁棒性和准确性。

    

    本文致力于领域泛化，在未知的目标领域中只在源领域上进行训练模型。我们在源域上进行训练，然后在目标域上进行推理，利用无标签目标数据本身的价值。我们做出了三个贡献。首先，我们提出了目标样本的概率伪标签，以在测试时将源领域训练的模型推广到目标领域。我们将测试时的推广建模为变分推理问题，通过将伪标签建模为分布，考虑泛化过程中的不确定性，并减轻伪标签不准确性带来的误导信号。其次，我们学习了变分邻居标签，将邻近目标样本的信息纳入到生成更强鲁棒伪标签的过程中。第三，为了学习将更具代表性的目标信息纳入到生成更准确、更强鲁棒的变分邻居标签的能力中，我们

    This paper strives for domain generalization, where models are trained exclusively on source domains before being deployed at unseen target domains. We follow the strict separation of source training and target testing but exploit the value of the unlabeled target data itself during inference. We make three contributions. First, we propose probabilistic pseudo-labeling of target samples to generalize the source-trained model to the target domain at test time. We formulate the generalization at test time as a variational inference problem by modeling pseudo labels as distributions to consider the uncertainty during generalization and alleviate the misleading signal of inaccurate pseudo labels. Second, we learn variational neighbor labels that incorporate the information of neighboring target samples to generate more robust pseudo labels. Third, to learn the ability to incorporate more representative target information and generate more precise and robust variational neighbor labels, we 
    
[^41]: 评估扩散模型在模仿人类艺术家方面的成功程度

    Measuring the Success of Diffusion Models at Imitating Human Artists. (arXiv:2307.04028v1 [cs.CV])

    [http://arxiv.org/abs/2307.04028](http://arxiv.org/abs/2307.04028)

    这项研究评估了扩散模型在模仿人类艺术家方面的成功程度，并提出将版权责任与模型能力联系起来可能是有用的。研究通过使用Contrastive Language-Image Pretrained (CLIP)编码器对图像进行分类来衡量模型模仿特定艺术家的能力。

    

    现代扩散模型在人工智能图像生成方面取得了最先进的成果。它们的成功部分是因为在训练过程中使用了互联网规模的数据，其中经常包含有版权的作品。这引发了关于这些模型在多大程度上从人类艺术家的作品中学习、模仿或复制的问题。本研究认为，将版权责任与模型的能力联系起来，可能在生成模型的不断发展的环境中是有用的。具体而言，关于版权和生成系统的法律分析往往侧重于使用受保护数据进行训练。因此，数据、训练和系统之间的联系常常被掩盖。在我们的方法中，我们考虑了简单的图像分类技术，以衡量模型模仿特定艺术家的能力。具体来说，我们使用对比语言-图像预训练(CLIP)编码器以零样本的方式对图像进行分类。我们的过程首先提示模型模仿特定的艺术家，然后测试CLIP是否能够准确分类图像。

    Modern diffusion models have set the state-of-the-art in AI image generation. Their success is due, in part, to training on Internet-scale data which often includes copyrighted work. This prompts questions about the extent to which these models learn from, imitate, or copy the work of human artists. This work suggests that tying copyright liability to the capabilities of the model may be useful given the evolving ecosystem of generative models. Specifically, much of the legal analysis of copyright and generative systems focuses on the use of protected data for training. As a result, the connections between data, training, and the system are often obscured. In our approach, we consider simple image classification techniques to measure a model's ability to imitate specific artists. Specifically, we use Contrastive Language-Image Pretrained (CLIP) encoders to classify images in a zero-shot fashion. Our process first prompts a model to imitate a specific artist. Then, we test whether CLIP 
    
[^42]: 强大的排名解释。

    Robust Ranking Explanations. (arXiv:2307.04024v1 [cs.LG])

    [http://arxiv.org/abs/2307.04024](http://arxiv.org/abs/2307.04024)

    这篇论文提出了一种名为R2ET的算法，通过计算解释厚度来衡量排名稳定性，并设计出可最大化解释厚度并锚定排名靠前特征的算法。实验证明R2ET在面对对抗攻击时具有更高的解释鲁棒性和保持准确性能力。

    

    强大的机器学习模型解释对于建立人类对模型的信任至关重要。由于认知能力有限，大多数人只能解释排名前几个显著特征。因此，将排名靠前的显著特征对抗攻击的鲁棒性尤为关键，特别是那些针对更脆弱的梯度解释。现有的防守措施使用l_p-范数来提供鲁棒性，但其保护能力较弱。我们定义了解释厚度来衡量排名稳定性，并导出了可计算的替代上界来设计R2ET算法，以有效地最大化厚度并锚定排名靠前的显著特征。理论上，我们证明了R2ET与对抗性训练之间的关联性。实验中涵盖了广泛的网络架构和数据模式，包括脑网络，证明了R2ET在保持准确性的同时达到更高的解释鲁棒性。

    Robust explanations of machine learning models are critical to establish human trust in the models. Due to limited cognition capability, most humans can only interpret the top few salient features. It is critical to make top salient features robust to adversarial attacks, especially those against the more vulnerable gradient-based explanations. Existing defense measures robustness using $\ell_p$-norms, which have weaker protection power. We define explanation thickness for measuring salient features ranking stability, and derive tractable surrogate bounds of the thickness to design the \textit{R2ET} algorithm to efficiently maximize the thickness and anchor top salient features. Theoretically, we prove a connection between R2ET and adversarial training. Experiments with a wide spectrum of network architectures and data modalities, including brain networks, demonstrate that R2ET attains higher explanation robustness under stealthy attacks while retaining accuracy.
    
[^43]: 使用PapillArray光学触觉传感器的鲁棒学习基于早期滑动检测的改进机器人抓取

    Robust Learning-Based Incipient Slip Detection using the PapillArray Optical Tactile Sensor for Improved Robotic Gripping. (arXiv:2307.04011v1 [cs.RO])

    [http://arxiv.org/abs/2307.04011](http://arxiv.org/abs/2307.04011)

    本文提出了一种使用PapillArray光学触觉传感器的学习方法，能够高效准确地检测早期滑动，实现了95.6%的检测成功率。通过引入数据增强方法，在不同环境下也保持了鲁棒的性能，成功率达到96.8%。

    

    检测滑动的能力，特别是早期滑动的能力，使得机器人系统能够采取纠正措施，防止抓取的物体掉落。因此，滑动检测可以增强机器人抓取的整体安全性。然而，准确检测早期滑动仍然是一个重大挑战。在本文中，我们提出了一种新颖的基于学习的方法，利用PapillArray（Contactile，澳大利亚）触觉传感器来检测早期滑动。所得模型在识别与早期滑动相关的模式方面具有很高的效果，在离线数据集测试时实现了95.6%的检测成功率。此外，我们引入了几种数据增强方法来增强我们模型的鲁棒性。当将训练好的模型转移到与训练数据采集环境不同的机器人抓取环境时，我们的模型保持了鲁棒的性能，成功率为96.8%，为稳定几种实际抓取提供及时反馈。

    The ability to detect slip, particularly incipient slip, enables robotic systems to take corrective measures to prevent a grasped object from being dropped. Therefore, slip detection can enhance the overall security of robotic gripping. However, accurately detecting incipient slip remains a significant challenge. In this paper, we propose a novel learning-based approach to detect incipient slip using the PapillArray (Contactile, Australia) tactile sensor. The resulting model is highly effective in identifying patterns associated with incipient slip, achieving a detection success rate of 95.6% when tested with an offline dataset. Furthermore, we introduce several data augmentation methods to enhance the robustness of our model. When transferring the trained model to a robotic gripping environment distinct from where the training data was collected, our model maintained robust performance, with a success rate of 96.8%, providing timely feedback for stabilizing several practical gripping 
    
[^44]: 理解U-Net和Vision Transformer在地下水数值建模中的有效性

    Understanding the Efficacy of U-Net & Vision Transformer for Groundwater Numerical Modelling. (arXiv:2307.04010v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2307.04010](http://arxiv.org/abs/2307.04010)

    该论文通过对U-Net、U-Net + ViT和FNO等多种机器学习模型在地下水系统中的时间相关正向建模进行全面比较，发现U-Net和U-Net + ViT模型在准确性和效率方面优于FNO，尤其是在稀疏数据场景中。这些结果证明了U-Net模型在数据稀缺的实际地下水建模应用中的潜力。

    

    本文对多种机器学习模型进行了全面比较，包括U-Net，集成了Vision Transformers（ViT）的U-Net和Fourier神经运算符（FNO），用于地下水系统中的时间相关正向建模。通过对合成数据集的测试，证明了U-Net和U-Net + ViT模型在准确性和效率方面优于FNO，特别是在稀疏数据场景中。这些发现强调了U-Net模型在数据稀缺的实际应用中用于地下水建模的潜力。

    This paper presents a comprehensive comparison of various machine learning models, namely U-Net, U-Net integrated with Vision Transformers (ViT), and Fourier Neural Operator (FNO), for time-dependent forward modelling in groundwater systems. Through testing on synthetic datasets, it is demonstrated that U-Net and U-Net + ViT models outperform FNO in accuracy and efficiency, especially in sparse data scenarios. These findings underscore the potential of U-Net-based models for groundwater modelling in real-world applications where data scarcity is prevalent.
    
[^45]: 多项式宽度对于具有高维特征的集合表示足够

    Polynomial Width is Sufficient for Set Representation with High-dimensional Features. (arXiv:2307.04001v1 [cs.LG])

    [http://arxiv.org/abs/2307.04001](http://arxiv.org/abs/2307.04001)

    本研究通过两种集合元素嵌入层的探索，证明了多项式宽度对于高维特征的集合表示足够，并揭示了之前分析中的局限性。

    

    集合表示在深度学习中已经变得普遍，用于建模神经网络对输入顺序不敏感的归纳偏差。DeepSets是最常用的集合表示神经网络架构，它将每个集合元素嵌入到具有维度L的潜在空间中，然后进行求和池化以获得整个集合的嵌入，最后将整个集合的嵌入映射到输出。在这项工作中，我们研究了维度L对DeepSets表达能力的影响。之前的分析要么将高维特征过于简化为一维特征，要么局限于分析激活函数，从而脱离实际应用或导致L随着集合大小N和特征维度D呈指数增长。为了研究达到足够表达能力的最小L值，我们提出了两种集合元素嵌入层：（a）线性+幂激活（LP）和（b）线性+指数激活。

    Set representation has become ubiquitous in deep learning for modeling the inductive bias of neural networks that are insensitive to the input order. DeepSets is the most widely used neural network architecture for set representation. It involves embedding each set element into a latent space with dimension $L$, followed by a sum pooling to obtain a whole-set embedding, and finally mapping the whole-set embedding to the output. In this work, we investigate the impact of the dimension $L$ on the expressive power of DeepSets. Previous analyses either oversimplified high-dimensional features to be one-dimensional features or were limited to analytic activations, thereby diverging from practical use or resulting in $L$ that grows exponentially with the set size $N$ and feature dimension $D$. To investigate the minimal value of $L$ that achieves sufficient expressive power, we present two set-element embedding layers: (a) linear + power activation (LP) and (b) linear + exponential activatio
    
[^46]: 低秩MDP中高效的无模型探索

    Efficient Model-Free Exploration in Low-Rank MDPs. (arXiv:2307.03997v1 [cs.LG])

    [http://arxiv.org/abs/2307.03997](http://arxiv.org/abs/2307.03997)

    提出了第一个计算高效、无模型的低秩MDPs探索算法，允许通用函数逼近，不需要额外的结构假设。

    

    强化学习中一个主要的挑战是在需要泛化和函数逼近的高维领域中开发出实用、样本高效的探索算法。低秩马尔可夫决策过程（MDPs）——其中转移概率可以基于未知特征嵌入进行低秩分解——为带有函数逼近的强化学习提供了简单而富有表现力的框架，但现有算法要么计算复杂度很高，要么依赖于限制性的统计假设，如潜变量结构、对模型为基础的函数逼近的访问性或可达性。在这项工作中，我们提出了第一个经过验证的低秩MDPs探索的样本高效算法，该算法既计算高效又无模型，允许进行通用的函数逼近，并且不需要额外的结构假设。我们的算法VoX使用了一个广义优化设计的特征嵌入概念作为探索的效能。

    A major challenge in reinforcement learning is to develop practical, sample-efficient algorithms for exploration in high-dimensional domains where generalization and function approximation is required. Low-Rank Markov Decision Processes -- where transition probabilities admit a low-rank factorization based on an unknown feature embedding -- offer a simple, yet expressive framework for RL with function approximation, but existing algorithms are either (1) computationally intractable, or (2) reliant upon restrictive statistical assumptions such as latent variable structure, access to model-based function approximation, or reachability. In this work, we propose the first provably sample-efficient algorithm for exploration in Low-Rank MDPs that is both computationally efficient and model-free, allowing for general function approximation and requiring no additional structural assumptions. Our algorithm, VoX, uses the notion of a generalized optimal design for the feature embedding as an eff
    
[^47]: 使用EffUNet和迁移学习方法进行建筑和道路分割

    Building and Road Segmentation Using EffUNet and Transfer Learning Approach. (arXiv:2307.03980v1 [cs.CV])

    [http://arxiv.org/abs/2307.03980](http://arxiv.org/abs/2307.03980)

    本论文提出了一种使用EffUNet和迁移学习方法进行建筑和道路分割的新架构。利用这种方法，在马萨诸塞州建筑物和道路数据集上取得了令人满意的分数。

    

    在城市中，了解城市对象（如供水、铁路线、电力线路、建筑物、道路等）的信息对城市规划是必要的。特别是，政策制定者需要了解这些对象的分布、位置和容量，以做出有影响力的决策。本论文旨在通过卫星和无人机拍摄的航空图像对建筑物和道路进行分割。许多不同的架构已经被提出用于语义分割任务，其中UNet是其中之一。在本论文中，我们提出了一种基于谷歌新提出的EfficientNetV2作为特征提取的编码器和UNet解码器构建分割图的新架构。使用这种方法，我们在马萨诸塞州建筑物和道路数据集上实现了基准分数，分别为0.8365和0.9153的mIOU。

    In city, information about urban objects such as water supply, railway lines, power lines, buildings, roads, etc., is necessary for city planning. In particular, information about the spread of these objects, locations and capacity is needed for the policymakers to make impactful decisions. This thesis aims to segment the building and roads from the aerial image captured by the satellites and UAVs. Many different architectures have been proposed for the semantic segmentation task and UNet being one of them. In this thesis, we propose a novel architecture based on Google's newly proposed EfficientNetV2 as an encoder for feature extraction with UNet decoder for constructing the segmentation map. Using this approach we achieved a benchmark score for the Massachusetts Building and Road dataset with an mIOU of 0.8365 and 0.9153 respectively.
    
[^48]: 无源光网络中基于机器学习技术的故障监测

    Fault Monitoring in Passive Optical Networks using Machine Learning Techniques. (arXiv:2307.03945v1 [cs.LG])

    [http://arxiv.org/abs/2307.03945](http://arxiv.org/abs/2307.03945)

    本文提出了基于机器学习技术的无源光网络故障监测方法，并使用实验数据进行了验证。

    

    无源光网络系统容易出现多种故障，包括光纤切断和光网络单元（ONU）的发射机/接收机故障。光纤切断造成的任何服务中断都可能给服务提供商或运营商带来巨大的经济损失。在几乎等距离的分支终止情况下，识别故障ONU变得困难，因为分支的反射重叠，很难根据全局反射信号区别出故障的分支。随着网络规模的增加，无源光网络系统中故障监测的复杂性增加，导致监测不够可靠。为了解决这些挑战，我们在本文中提出了各种机器学习（ML）方法用于无源光网络系统的故障监测，并使用实验光时域反射计（OTDR）数据进行验证。

    Passive optical network (PON) systems are vulnerable to a variety of failures, including fiber cuts and optical network unit (ONU) transmitter/receiver failures. Any service interruption caused by a fiber cut can result in huge financial losses for service providers or operators. Identifying the faulty ONU becomes difficult in the case of nearly equidistant branch terminations because the reflections from the branches overlap, making it difficult to distinguish the faulty branch given the global backscattering signal. With increasing network size, the complexity of fault monitoring in PON systems increases, resulting in less reliable monitoring. To address these challenges, we propose in this paper various machine learning (ML) approaches for fault monitoring in PON systems, and we validate them using experimental optical time domain reflectometry (OTDR) data.
    
[^49]: Rosko: 使用行跳过外积进行稀疏矩阵乘法内核计算

    Rosko: Row Skipping Outer Products for Sparse Matrix Multiplication Kernels. (arXiv:2307.03930v1 [cs.LG])

    [http://arxiv.org/abs/2307.03930](http://arxiv.org/abs/2307.03930)

    Rosko提出了一种稀疏矩阵乘法（SpMM）内核方法，通过使用行跳过外积实现深度神经网络（DNN）的计算和内存访问需求的减少。它可以适应不同硬件特性，并与其他外积调度方法相结合，胜过自动调优和搜索的解决方案，并能在不同稀疏度的矩阵上实现高效的计算。

    

    我们提出了Rosko，即使用行跳过外积（Row Skipping Outer Products），用于在减少深度神经网络（DNN）的计算和内存访问需求方面推导出稀疏矩阵乘法（SpMM）内核。Rosko允许在程序执行期间跳过整行计算，并具有低稀疏管理开销。我们从分析上推导出适应给定硬件特性的稀疏CPU内核，以有效利用处理器核心并最小化数据移动，而无需自动调优或搜索空间探索。Rosko可以与其他外积调度方法集成，通过使用Rosko的打包格式来跳过不必要的计算，从而利用行跳过。在各种神经网络工作负载下，Rosko内核在真实硬件上胜过现有的自动调优和基于搜索的解决方案以及最先进的供应商优化库。对于在机器学习中通常出现的稀疏度从65％到99.8％的矩阵，Rosko内核可以实现...（摘要未完整）

    We propose Rosko -- row skipping outer products -- for deriving sparse matrix multiplication (SpMM) kernels in reducing computation and memory access requirements of deep neural networks (DNNs). Rosko allows skipping of entire row computations during program execution with low sparsity-management overheads. We analytically derive sparse CPU kernels that adapt to given hardware characteristics to effectively utilize processor cores and minimize data movement without the need for auto-tuning or search space exploration. Rosko can be integrated with other outer product scheduling methods, allowing them to leverage row skipping by using Rosko's packing format to skip unnecessary computation.  Rosko kernels outperform existing auto-tuning and search-based solutions as well as state-of-the-art vendor-optimized libraries on real hardware across a variety of neural network workloads. For matrices with sparsities ranging from 65% to 99.8% typically found in machine learning, Rosko kernels achie
    
[^50]: 公平感知图神经网络：一项调查研究

    Fairness-Aware Graph Neural Networks: A Survey. (arXiv:2307.03929v1 [cs.LG])

    [http://arxiv.org/abs/2307.03929](http://arxiv.org/abs/2307.03929)

    该论文调查了公平感知图神经网络，讨论了提高GNNs公平性的技术，并介绍了公平度评估指标分类法。

    

    由于其代表能力和在许多基本学习任务中的最先进预测性能，图神经网络(GNNs)变得越来越重要。尽管取得了成功，但GNNs由于基础图数据和庞大的GNN模型中心的基本聚合机制的结果，存在公平性问题。在本文中，我们考察并分类了提高GNNs公平性的公平技术。先前关于公平GNN模型和技术的工作在预处理步骤、训练过程中或后处理阶段是否关注提高公平性方面进行了讨论。此外，我们讨论了这些技术如何在适当的情况下共同使用，并强调了各自的优势和直觉。我们还介绍了一种直观的公平度评估指标分类法，包括图级公平性、邻域级公平性、嵌入级公平性和预测级公平性。

    Graph Neural Networks (GNNs) have become increasingly important due to their representational power and state-of-the-art predictive performance on many fundamental learning tasks. Despite this success, GNNs suffer from fairness issues that arise as a result of the underlying graph data and the fundamental aggregation mechanism that lies at the heart of the large class of GNN models. In this article, we examine and categorize fairness techniques for improving the fairness of GNNs. Previous work on fair GNN models and techniques are discussed in terms of whether they focus on improving fairness during a preprocessing step, during training, or in a post-processing phase. Furthermore, we discuss how such techniques can be used together whenever appropriate, and highlight the advantages and intuition as well. We also introduce an intuitive taxonomy for fairness evaluation metrics including graph-level fairness, neighborhood-level fairness, embedding-level fairness, and prediction-level fair
    
[^51]: 快速经验场景

    Fast Empirical Scenarios. (arXiv:2307.03927v1 [stat.ML])

    [http://arxiv.org/abs/2307.03927](http://arxiv.org/abs/2307.03927)

    该论文提出了两种快速的经验场景提取算法，一种识别之前未观察到的场景并提供场景的协方差矩阵表示，另一种从已实现的世界状态中选择重要的数据点，并与高阶样本矩一致，这些算法计算效率高且适用于一致的基于场景的建模和高维数值积分。

    

    我们希望从大型和高维面板数据中提取一小部分与样本矩一致的代表性场景。在两种新算法中，第一种算法识别之前未观察到的场景，并提供了一种基于场景的协方差矩阵表示。第二种算法从已实现的世界状态中选择重要的数据点，并与高阶样本矩信息一致。这两种算法计算效率高，并可用于一致的基于场景的建模和高维数值积分。广泛的数值基准测试研究和在投资组合优化中的应用支持所提出的算法。

    We seek to extract a small number of representative scenarios from large and high-dimensional panel data that are consistent with sample moments. Among two novel algorithms, the first identifies scenarios that have not been observed before, and comes with a scenario-based representation of covariance matrices. The second proposal picks important data points from states of the world that have already realized, and are consistent with higher-order sample moment information. Both algorithms are efficient to compute, and lend themselves to consistent scenario-based modeling and high-dimensional numerical integration. Extensive numerical benchmarking studies and an application in portfolio optimization favor the proposed algorithms.
    
[^52]: 基于多任务优化的物理知识神经网络训练用于交通密度预测

    Training Physics-Informed Neural Networks via Multi-Task Optimization for Traffic Density Prediction. (arXiv:2307.03920v1 [cs.NE])

    [http://arxiv.org/abs/2307.03920](http://arxiv.org/abs/2307.03920)

    该论文提出了一种基于多任务优化的PINN训练框架，用于交通密度预测，该框架通过创建和解决多个辅助任务来辅助求解主任务，实现了神经网络在面对有限训练数据时的泛化性能。

    

    物理知识神经网络（PINNs）是机器学习中的新兴研究领域，将某个给定数据集所描述的物理定律（如偏微分方程）结合到神经网络的训练中。PINNs中，神经网络用作PDE的解近似器，而PDE作为先验知识指导神经网络的训练，以应对训练数据有限的情况下实现神经网络的泛化性能。然而，由于损失函数由神经网络和物理定律两部分组成，训练PINNs是一项非常复杂的任务。在本研究中，我们提出了一种基于多任务优化（MTO）范式的新PINN训练框架。在该框架下，除了给定的（主）任务外，还创建了多个辅助任务并一起解决，通过自适应地传递求解一个任务中获得的有用知识，来辅助求解主任务。

    Physics-informed neural networks (PINNs) are a newly emerging research frontier in machine learning, which incorporate certain physical laws that govern a given data set, e.g., those described by partial differential equations (PDEs), into the training of the neural network (NN) based on such a data set. In PINNs, the NN acts as the solution approximator for the PDE while the PDE acts as the prior knowledge to guide the NN training, leading to the desired generalization performance of the NN when facing the limited availability of training data. However, training PINNs is a non-trivial task largely due to the complexity of the loss composed of both NN and physical law parts. In this work, we propose a new PINN training framework based on the multi-task optimization (MTO) paradigm. Under this framework, multiple auxiliary tasks are created and solved together with the given (main) task, where the useful knowledge from solving one task is transferred in an adaptive mode to assist in solv
    
[^53]: 将Deep Q-Network与多类分类算法相结合的研究

    Incorporating Deep Q -- Network with Multiclass Classification Algorithms. (arXiv:2307.03908v1 [cs.LG])

    [http://arxiv.org/abs/2307.03908](http://arxiv.org/abs/2307.03908)

    本研究探索了将Deep Q-Network与多类分类算法相结合以提高分类准确性的方法，并在预测财务困境等领域进行了应用。

    

    在本研究中，我们探索了如何利用Deep Q-Network (DQN)来提高多类分类算法的功能性。我们将使用来自Kaggle的基准数据集创建一个将DQN与现有有监督多类分类算法相结合的框架。本研究的发现将带来对如何使用深度强化学习策略来提高多类分类准确性的见解。这些策略已经在图像识别、自然语言处理和生物信息学等领域中得到了应用。本研究还侧重于预测公司的财务困境以及Deep Q-Network在多类分类中的广泛应用。识别可能遇到财务困境的企业是金融和风险管理领域中的重要任务。当企业面临严重挑战时，无法维持运营并履行财务责任，就被认为处于财务困境中。

    In this study, we explore how Deep Q-Network (DQN) might improve the functionality of multiclass classification algorithms. We will use a benchmark dataset from Kaggle to create a framework incorporating DQN with existing supervised multiclass classification algorithms. The findings of this study will bring insight into how deep reinforcement learning strategies may be used to increase multiclass classification accuracy. They have been used in a number of fields, including image recognition, natural language processing, and bioinformatics. This study is focused on the prediction of financial distress in companies in addition to the wider application of Deep Q-Network in multiclass classification. Identifying businesses that are likely to experience financial distress is a crucial task in the fields of finance and risk management. Whenever a business experiences serious challenges keeping its operations going and meeting its financial responsibilities, it is said to be in financial dist
    
[^54]: ScriptWorld: 用于学习过程性知识的基于文本的环境

    ScriptWorld: Text Based Environment For Learning Procedural Knowledge. (arXiv:2307.03906v1 [cs.CL])

    [http://arxiv.org/abs/2307.03906](http://arxiv.org/abs/2307.03906)

    ScriptWorld是一个基于文本的环境，用于教授智能体关于真实世界日常任务和常识知识。它是第一个采用脚本数据集设计的互动式基于文本的游戏框架，在10个日常活动中提供了游戏环境，并通过对环境的详细分析，展示了引入脚本式真实世界任务可以有效提供常识知识并提高强化学习智能体的学习性能。

    

    文本游戏为基于强化学习的智能体开发自然语言理解和常识知识提供了一个框架。现有的文本环境通常依赖于虚构的情景和角色来创建游戏框架，与真实世界场景相差甚远。本文介绍了ScriptWorld：一个基于文本的环境，用于教授智能体关于真实世界日常任务和常识知识。据我们所知，这是第一个采用脚本数据集设计的互动式基于文本的游戏框架，提供了10个日常活动的游戏环境，并对所提出的环境进行了详细分析。我们开发了基于强化学习的基准模型/智能体来玩ScriptWorld中的游戏。为了理解语言模型在这种环境中的作用，我们利用从预训练语言模型中获得的特征来辅助强化学习智能体。我们的实验表明，在ScriptWorld中引入脚本式的真实世界任务可以有效提供常识知识和提高强化学习智能体的学习性能。

    Text-based games provide a framework for developing natural language understanding and commonsense knowledge about the world in reinforcement learning based agents. Existing text-based environments often rely on fictional situations and characters to create a gaming framework and are far from real-world scenarios. In this paper, we introduce ScriptWorld: a text-based environment for teaching agents about real-world daily chores and hence imparting commonsense knowledge. To the best of our knowledge, it is the first interactive text-based gaming framework that consists of daily real-world human activities designed using scripts dataset. We provide gaming environments for 10 daily activities and perform a detailed analysis of the proposed environment. We develop RL-based baseline models/agents to play the games in Scriptworld. To understand the role of language models in such environments, we leverage features obtained from pre-trained language models in the RL agents. Our experiments sh
    
[^55]: 同时保留类别和聚类结构的特征选择方法

    Feature selection simultaneously preserving both class and cluster structures. (arXiv:2307.03902v1 [cs.LG])

    [http://arxiv.org/abs/2307.03902](http://arxiv.org/abs/2307.03902)

    本文提出了一种基于神经网络的特征选择方法，同时考虑了类别判别和聚类结构保持的目标。实验证明该方法可以在具有显著类别和聚类结构差异的数据集上取得良好的分类和聚类性能。

    

    当数据集的类别和聚类结构具有显著差异时，仅针对类别的特征选择会导致聚类性能不佳，同样地，仅考虑聚类结构的特征选择也会导致分类性能不佳。据我们所知，文献中尚无同时考虑类别判别和聚类结构保持的特征选择方法。本文通过提出一种基于神经网络的特征选择方法，旨在整合类别判别和结构保持两方面的目标来弥补这一空白。除了评估典型的分类问题外，我们还研究了该方法在高光谱图像中的波段选择方面的有效性。根据实验结果，我们可以声称，所提出的特征/波段选择方法可以选择适用于分类和聚类的特征子集。

    When a data set has significant differences in its class and cluster structure, selecting features aiming only at the discrimination of classes would lead to poor clustering performance, and similarly, feature selection aiming only at preserving cluster structures would lead to poor classification performance. To the best of our knowledge, a feature selection method that simultaneously considers class discrimination and cluster structure preservation is not available in the literature. In this paper, we have tried to bridge this gap by proposing a neural network-based feature selection method that focuses both on class discrimination and structure preservation in an integrated manner. In addition to assessing typical classification problems, we have investigated its effectiveness on band selection in hyperspectral images. Based on the results of the experiments, we may claim that the proposed feature/band selection can select a subset of features that is good for both classification an
    
[^56]: 物理学中的主动学习：从101到进展与视角

    Active Learning in Physics: From 101, to Progress, and Perspective. (arXiv:2307.03899v1 [quant-ph])

    [http://arxiv.org/abs/2307.03899](http://arxiv.org/abs/2307.03899)

    本文介绍了物理学中的主动学习方法，该方法利用未标记的样本并通过专家的注释来提升模型性能。最近，该方法在物理学领域引起了越来越多的关注。另外，本文还探讨了主动学习与量子机器学习的整合，设想了这两个领域的协同融合。

    

    主动学习（AL）是一类在当前人工智能时代之前存在的机器学习（ML）算法。与传统方法需要标记样本进行训练不同，AL通过迭代选择未标记的样本由专家进行注释。这个协议旨在优先选择最具信息量的样本，使得模型性能相对于使用全部标记样本进行训练来说有所提升。近年来，AL在物理学领域引起了越来越多的关注。本文对AL的理论进行全面且易于理解的介绍，并回顾了各个领域的最新进展。此外，我们探讨了AL与量子ML的潜在整合，设想了这两个领域的协同融合，而不仅仅把AL视为经典ML在量子领域的扩展。

    Active Learning (AL) is a family of machine learning (ML) algorithms that predates the current era of artificial intelligence. Unlike traditional approaches that require labeled samples for training, AL iteratively selects unlabeled samples to be annotated by an expert. This protocol aims to prioritize the most informative samples, leading to improved model performance compared to training with all labeled samples. In recent years, AL has gained increasing attention, particularly in the field of physics. This paper presents a comprehensive and accessible introduction to the theory of AL reviewing the latest advancements across various domains. Additionally, we explore the potential integration of AL with quantum ML, envisioning a synergistic fusion of these two fields rather than viewing AL as a mere extension of classical ML into the quantum realm.
    
[^57]: 通过奖励重新加权、重选和重新训练方法，改进了原型零件网络

    Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v1 [cs.LG])

    [http://arxiv.org/abs/2307.03887](http://arxiv.org/abs/2307.03887)

    本研究通过引入奖励重新加权、重选和重新训练的方法，改进了原型零件网络的分类效果，解决了学习从图像的虚假或不一致的部分进行分类的问题。

    

    近年来，人们致力于开发深度可解释的图像分类方法，能够清楚地将模型的输出归因于数据的特定特征。其中一种方法是原型零件网络（ProtoPNet），它基于输入的有意义部分来尝试分类图像。然而，这种方法经常学习从图像的虚假或不一致的部分进行分类。为了解决这个问题，我们受到强化学习与人类反馈（RLHF）的最新发展启发，通过在CUB-200-2011数据集上收集人类原型质量的1-5分级注释，构建一个学习识别非虚假原型的奖励模型。我们提出了重新加权、重选和重新训练的原型零件网络（R3-ProtoPNet），该网络在ProtoPNet训练循环中增加了三个额外的步骤。

    In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the prototypical part network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, this method often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. In place of a full RL update, we propose the reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The fir
    
[^58]: 关于正则化和标签约束推理的研究

    On Regularization and Inference with Label Constraints. (arXiv:2307.03886v1 [cs.LG])

    [http://arxiv.org/abs/2307.03886](http://arxiv.org/abs/2307.03886)

    本文研究了在机器学习中将先验知识和符号规则以标签约束的形式表达的方法。通过比较正则化和约束推理两种常见的编码标签约束的策略，发现正则化缩小了泛化差距但引入了对次优模型的偏置，而约束推理通过纠正模型的违规行为将违规行为转化为优势。进一步探索了将这两种方法结合使用的可能，并提出了用约束推理来补偿正则化引入的偏置的条件，旨在提高模型复杂性和最优风险。

    

    先验知识和符号规则在机器学习中通常以标签约束的形式表达，特别是在结构预测问题中。本文通过量化其对模型性能的影响，比较了机器学习流程中两种常见的编码标签约束的策略：带约束的正则化和约束推理。对于正则化，我们展示了它通过排除与约束不一致的模型来缩小泛化差距的效果。然而，正则化对小违规的偏好导致了对次优模型的偏置。对于约束推理，我们展示了它通过纠正模型的违规行为来减小总体风险，并将违规行为转化为优势。鉴于这些差异，我们进一步探索了将这两种方法结合使用，并提出了约束推理来补偿正则化引入的偏置的条件，旨在提高模型复杂性和最优风险。

    Prior knowledge and symbolic rules in machine learning are often expressed in the form of label constraints, especially in structured prediction problems. In this work, we compare two common strategies for encoding label constraints in a machine learning pipeline, regularization with constraints and constrained inference, by quantifying their impact on model performance. For regularization, we show that it narrows the generalization gap by precluding models that are inconsistent with the constraints. However, its preference for small violations introduces a bias toward a suboptimal model. For constrained inference, we show that it reduces the population risk by correcting a model's violation, and hence turns the violation into an advantage. Given these differences, we further explore the use of two approaches together and propose conditions for constrained inference to compensate for the bias introduced by regularization, aiming to improve both the model complexity and optimal risk.
    
[^59]: 使用噪声张量环逼近方法计算组合优化的变分量子本征求解器梯度

    Noisy Tensor Ring approximation for computing gradients of Variational Quantum Eigensolver for Combinatorial Optimization. (arXiv:2307.03884v1 [quant-ph])

    [http://arxiv.org/abs/2307.03884](http://arxiv.org/abs/2307.03884)

    使用噪声张量环逼近方法计算组合优化的变分量子本征求解器梯度，解决了VQE算法受限于经典不可处理的梯度问题，提高了可扩展性。

    

    变分量子算法，特别是量子近似优化和变分量子本征求解器（VQE），已经在组合优化领域展示出了潜在的计算优势。然而，这些算法受到经典不可处理的梯度问题的限制，从而限制了可扩展性。本研究通过提出一种经典梯度计算方法来解决VQE的可扩展性挑战，该方法利用参数偏移规则，但是使用张量环逼近来计算电路的期望值。电路中的参数化门通过在张量环的自由边上收缩矩阵来转换张量环。虽然单比特门不会改变环的结构，但两比特旋转的状态变换通过截断奇异值来评估，从而保持了张量环的结构并降低了计算复杂度。

    Variational Quantum algorithms, especially Quantum Approximate Optimization and Variational Quantum Eigensolver (VQE) have established their potential to provide computational advantage in the realm of combinatorial optimization. However, these algorithms suffer from classically intractable gradients limiting the scalability. This work addresses the scalability challenge for VQE by proposing a classical gradient computation method which utilizes the parameter shift rule but computes the expected values from the circuits using a tensor ring approximation. The parametrized gates from the circuit transform the tensor ring by contracting the matrix along the free edges of the tensor ring. While the single qubit gates do not alter the ring structure, the state transformations from the two qubit rotations are evaluated by truncating the singular values thereby preserving the structure of the tensor ring and reducing the computational complexity. This variation of the Matrix product state app
    
[^60]: 大型语言模型用于供应链优化

    Large Language Models for Supply Chain Optimization. (arXiv:2307.03875v1 [cs.AI])

    [http://arxiv.org/abs/2307.03875](http://arxiv.org/abs/2307.03875)

    这项研究研究了利用大型语言模型（LLMs）来帮助解释和解读供应链优化结果的方法。他们设计了一个框架，可以接受普通文本查询作为输入，并输出关于底层优化结果的洞察。通过定量回答假设情况，该框架在不放弃最先进的组合优化技术的情况下帮助企业运营者更好地理解和信任优化结果。

    

    传统上，供应链操作涉及各种复杂的决策问题。在过去几十年中，供应链受益于计算技术的进步，从手动处理过渡到自动化和成本效益优化。然而，企业运营者仍然需要花费大量精力来解释和解读优化结果给相关人士。受大型语言模型(LLMs)最近的进展的启发，我们研究了这种颠覆性技术如何帮助弥合供应链自动化和人类理解与信任之间的差距。我们设计了一个名为\name{}的框架，它接受普通文本查询作为输入，并输出关于底层优化结果的洞察。我们的框架并没有放弃最先进的组合优化技术，而是利用它来定量地回答假设情况（例如，如果我们使用供应商B而不是供应商A，成本会如何变化）。

    Supply chain operations traditionally involve a variety of complex decision making problems. Over the last few decades, supply chains greatly benefited from advances in computation, which allowed the transition from manual processing to automation and cost-effective optimization. Nonetheless, business operators still need to spend substantial efforts in \emph{explaining} and interpreting the optimization outcomes to stakeholders. Motivated by the recent advances in Large Language Models (LLMs), we study how this disruptive technology can help bridge the gap between supply chain automation and human comprehension and trust thereof. We design \name{} -- a framework that accepts as input queries in plain text, and outputs insights about the underlying optimization outcomes. Our framework does not forgo the state-of-the-art combinatorial optimization technology, but rather leverages it to quantitatively answer what-if scenarios (e.g., how would the cost change if we used supplier B instead
    
[^61]: 使用银标准标签在数字病理学中进行Ki-67评分的领域自适应: 进一步接近大规模部署

    Domain Adaptation using Silver Standard Labels for Ki-67 Scoring in Digital Pathology: A Step Closer to Widescale Deployment. (arXiv:2307.03872v1 [eess.IV])

    [http://arxiv.org/abs/2307.03872](http://arxiv.org/abs/2307.03872)

    本研究提出了一种领域自适应流程，通过使用生成的银标准标签将目标领域数据与源领域数据相结合，以提高Ki-67评分的深度学习系统在领域外数据上的性能。

    

    深度学习系统被提出来提高Ki-67 PI评分的客观性和效率。挑战在于，虽然深度学习技术非常准确，但在应用于领域外数据时性能下降。这对于临床应用来说是一个关键的挑战，因为模型通常是使用供应商可用的数据进行训练的，而这些数据不来自目标领域。为了解决这个问题，本研究提出了一个领域自适应流程，利用无监督框架在目标领域生成银标准（伪）标签，这些标签用于增加黄金标准（GS）源领域数据。研究在两个经过验证的Ki-67评分架构（UV-Net和piNET）上测试了五个训练策略：(1) 仅SS: 在目标银标准（SS）标签上训练，(2) 仅GS: 在源GS标签上训练，(3) 混合: 在目标SS和源GS标签上训练，(4) GS+SS: 在源GS标签上训练并在目标SS标签上进行微调，以及我们提出的元学习方法。

    Deep learning systems have been proposed to improve the objectivity and efficiency of Ki- 67 PI scoring. The challenge is that while very accurate, deep learning techniques suffer from reduced performance when applied to out-of-domain data. This is a critical challenge for clinical translation, as models are typically trained using data available to the vendor, which is not from the target domain. To address this challenge, this study proposes a domain adaptation pipeline that employs an unsupervised framework to generate silver standard (pseudo) labels in the target domain, which is used to augment the gold standard (GS) source domain data. Five training regimes were tested on two validated Ki-67 scoring architectures (UV-Net and piNET), (1) SS Only: trained on target silver standard (SS) labels, (2) GS Only: trained on source GS labels, (3) Mixed: trained on target SS and source GS labels, (4) GS+SS: trained on source GS labels and fine-tuned on target SS labels, and our proposed met
    
[^62]: 何时使用Transformer在强化学习中发光？从记忆和信用分配中解耦

    When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment. (arXiv:2307.03864v1 [cs.LG])

    [http://arxiv.org/abs/2307.03864](http://arxiv.org/abs/2307.03864)

    Transformer在强化学习中的作用是增强记忆能力而不是改进信用分配。

    

    强化学习算法面临两个不同的挑战：学习有效的过去和当前观测的表示，并确定行动如何影响未来的收益。这两个挑战都涉及到建模长期依赖关系。Transformer架构在解决涉及长期依赖关系的问题，包括在强化学习领域方面非常成功。然而，Transformer基于的强化学习方法表现强劲的原因尚不清楚：是因为它们学习了有效的记忆，还是因为它们执行了有效的信用分配？在引入记忆长度和信用分配长度的形式定义之后，我们设计了简单的可配置任务来测量这些不同的量。我们的实证结果表明，Transformer可以增强强化学习算法的记忆能力，扩展到需要记住1500步前观察的任务。然而，Transformer无法改进长期的信用分配。总之，我们的研究揭示了Transformer在强化学习中记忆和信用分配方面的不同作用。

    Reinforcement learning (RL) algorithms face two distinct challenges: learning effective representations of past and present observations, and determining how actions influence future returns. Both challenges involve modeling long-term dependencies. The transformer architecture has been very successful to solve problems that involve long-term dependencies, including in the RL domain. However, the underlying reason for the strong performance of Transformer-based RL methods remains unclear: is it because they learn effective memory, or because they perform effective credit assignment? After introducing formal definitions of memory length and credit assignment length, we design simple configurable tasks to measure these distinct quantities. Our empirical results reveal that Transformers can enhance the memory capacity of RL algorithms, scaling up to tasks that require memorizing observations $1500$ steps ago. However, Transformers do not improve long-term credit assignment. In summary, our
    
[^63]: 基于内存的合作数字化用于面积高效的计算内存深度学习

    Memory-Immersed Collaborative Digitization for Area-Efficient Compute-in-Memory Deep Learning. (arXiv:2307.03863v1 [cs.AR])

    [http://arxiv.org/abs/2307.03863](http://arxiv.org/abs/2307.03863)

    本论文提出了一种基于内存的合作数字化方案，用于减少深度学习推理中传统模拟数字转换器(ADC)的面积开销，提升并行性和减少外部内存访问。该方案通过在内存中形成内存内的数字-模数转换器(DAC)来实现面积高效的数字化，并且利用合作数字化来提高计算效率。

    

    本论文讨论了基于内存的合作数字化在计算内存(CiM)阵列之间的应用，以最小化传统模拟数字转换器(ADC)在深度学习推理中的面积开销。通过使用提出的方案，可以在有限的尺寸设计中容纳更多的CiM阵列，以提高并行性并减少外部内存访问。在数字化方案下，CiM阵列利用其寄生比特线形成内存内的电容数字-模数转换器(DAC)，以实现面积高效的逐次逼近(SA)数字化。当一个阵列计算输入和权重的标量积时，CiM阵列协同工作，其中一个相邻的阵列进行模拟领域的乘积数的数字化。本文讨论了CiM阵列之间的各种网络配置，其中利用提出的内存内方案可以有效地实现Flash、SA和其混合数字化步骤。结果使用65纳米CMO进行了验证。

    This work discusses memory-immersed collaborative digitization among compute-in-memory (CiM) arrays to minimize the area overheads of a conventional analog-to-digital converter (ADC) for deep learning inference. Thereby, using the proposed scheme, significantly more CiM arrays can be accommodated within limited footprint designs to improve parallelism and minimize external memory accesses. Under the digitization scheme, CiM arrays exploit their parasitic bit lines to form a within-memory capacitive digital-to-analog converter (DAC) that facilitates area-efficient successive approximation (SA) digitization. CiM arrays collaborate where a proximal array digitizes the analog-domain product-sums when an array computes the scalar product of input and weights. We discuss various networking configurations among CiM arrays where Flash, SA, and their hybrid digitization steps can be efficiently implemented using the proposed memory-immersed scheme. The results are demonstrated using a 65 nm CMO
    
[^64]: 基于强化学习和深度强化学习的机器维护规划、调度策略和优化解决方案

    Reinforcement and Deep Reinforcement Learning-based Solutions for Machine Maintenance Planning, Scheduling Policies, and Optimization. (arXiv:2307.03860v1 [cs.LG])

    [http://arxiv.org/abs/2307.03860](http://arxiv.org/abs/2307.03860)

    基于强化学习和深度强化学习的机器维护规划、调度策略和优化解决方案可以通过利用连续的条件监控数据来开发智能维护规划器，实现降低成本、延长资产寿命和确保工作场所安全等目标。

    

    系统和机器会经历各种故障模式，导致机器健康程度下降，因此需要维护操作来将它们恢复到能够执行预期功能的状态。由于维护任务是不可避免的，所以维护规划对于确保生产系统和其他行业的平稳运行至关重要。维护规划是一个决策问题，旨在制定最优的维护策略和计划，以帮助降低维护成本、延长资产寿命、最大化可用性，最终确保工作场所安全。强化学习是一种数据驱动的决策算法，越来越多地被应用于开发动态维护计划，同时利用系统和机器状态的连续信息。通过利用系统和机器的条件监控数据与强化学习相结合，可以开发智能的维护规划器，可以优化维护计划。

    Systems and machines undergo various failure modes that result in machine health degradation, so maintenance actions are required to restore them back to a state where they can perform their expected functions. Since maintenance tasks are inevitable, maintenance planning is essential to ensure the smooth operations of the production system and other industries at large. Maintenance planning is a decision-making problem that aims at developing optimum maintenance policies and plans that help reduces maintenance costs, extend asset life, maximize their availability, and ultimately ensure workplace safety. Reinforcement learning is a data-driven decision-making algorithm that has been increasingly applied to develop dynamic maintenance plans while leveraging the continuous information from condition monitoring of the system and machine states. By leveraging the condition monitoring data of systems and machines with reinforcement learning, smart maintenance planners can be developed, which
    
[^65]: inTformer: 一种基于时间嵌入的关注机制Transformer用于使用连接车辆数据的路口事故可能性预测

    inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data. (arXiv:2307.03854v1 [cs.LG])

    [http://arxiv.org/abs/2307.03854](http://arxiv.org/abs/2307.03854)

    inTformer是一种基于时间嵌入的关注机制Transformer，用于使用连接车辆数据预测路口事故可能性。与其他深度学习模型相比，inTformer具有处理长期依赖性、并行处理数据序列以及解决梯度消失问题的优势。

    

    实时事故可能性预测模型是主动交通安全管理系统的关键组成部分。多年来，许多研究尝试构建事故可能性预测模型，以提高交通安全，但主要集中在高速公路上。在大多数现有研究中，研究人员主要采用基于深度学习的框架来识别事故潜在风险。最近，Transformer已经成为一种潜在的深度神经网络，其基本原理是通过注意力机制来进行操作。Transformer在功能上比现有的深度学习模型（如LSTM，CNN等）具有几个优势。首先，Transformer可以轻松处理数据序列中的长期依赖性。其次，Transformer可以在训练期间并行处理数据序列中的所有元素。最后，Transformer不存在梯度消失的问题。认识到Transformer的巨大潜力，

    The real-time crash likelihood prediction model is an essential component of the proactive traffic safety management system. Over the years, numerous studies have attempted to construct a crash likelihood prediction model in order to enhance traffic safety, but mostly on freeways. In the majority of the existing studies, researchers have primarily employed a deep learning-based framework to identify crash potential. Lately, Transformer has emerged as a potential deep neural network that fundamentally operates through attention-based mechanisms. Transformer has several functional benefits over extant deep learning models such as Long Short-Term Memory (LSTM), Convolution Neural Network (CNN), etc. Firstly, Transformer can readily handle long-term dependencies in a data sequence. Secondly, Transformer can parallelly process all elements in a data sequence during training. Finally, Transformer does not have the vanishing gradient issue. Realizing the immense possibility of Transformer, th
    
[^66]: 可实现回归的最优学习算法：PAC学习和在线学习

    Optimal Learners for Realizable Regression: PAC Learning and Online Learning. (arXiv:2307.03848v1 [cs.LG])

    [http://arxiv.org/abs/2307.03848](http://arxiv.org/abs/2307.03848)

    本论文研究了可实现回归问题的PAC学习和在线学习的统计复杂度，并提出了对于可学习性的必要条件和充分条件。

    

    本研究旨在对可实现回归在PAC学习和在线学习的统计复杂度进行刻画。先前的研究已经证明了有限的fat shattering维度对于PAC学习的充分性以及有限的scaled Natarajan维度对于必要性的存在，但自从Simon 1997（SICOMP '97）的工作以来，对于更完整的刻画的进展甚少。为此，我们首先引入了一种最小化实例最优学习算法来对可实现回归进行学习，并提出了一种既定性又定量地刻画了哪些类的实数预测器可以被学习的新颖维度。然后，我们确定了一个与图维度相关的组合维度，该维度刻画了在可实现设置中的ERM可学习性。最后，我们根据与DS维度相关的组合维度建立了学习可行性的必要条件，并猜测它也可能是充分的。

    In this work, we aim to characterize the statistical complexity of realizable regression both in the PAC learning setting and the online learning setting.  Previous work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since the work of Simon 1997 (SICOMP '97). To this end, we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable. We then identify a combinatorial dimension related to the Graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in 
    
[^67]: RADAR: 通过对抗性学习实现鲁棒的AI文本检测

    RADAR: Robust AI-Text Detection via Adversarial Learning. (arXiv:2307.03838v1 [cs.CL])

    [http://arxiv.org/abs/2307.03838](http://arxiv.org/abs/2307.03838)

    本论文提出了一种名为RADAR的新框架，通过对抗性学习实现了鲁棒的AI文本检测，以解决当前AI文本检测器对于大语言模型的改写不具备鲁棒性的问题。

    

    最近大语言模型（LLMs）的进展以及ChatGPT类应用的普及已经模糊了人类和机器之间高质量文本生成的界限。然而，除了对我们的技术和社会预期的革命性变化外，区分LLM生成的文本（AI文本）和人类生成的文本的困难也带来了新的滥用和公平性挑战，例如虚假内容生成，抄袭以及对无辜作者的错误指控。尽管现有的研究表明当前的AI文本检测器对基于LLM的改写不具有鲁棒性，但本文旨在通过提出一种名为RADAR的新框架来弥合这一差距，该框架通过对抗性学习共同训练了一个鲁棒的AI文本检测器。RADAR基于一个改写器和一个检测器的对抗性训练。改写器的目标是生成逼真的内容以规避AI文本检测。RADAR使用来自检测器的反馈来更新改写器，反之亦然。

    Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusation of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a Robust AI-text Detector via Adversarial leaRning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic contents to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice vers
    
[^68]: 强度标准化对多中心FLAIR MRI中深度学习白质病变分割的影响

    Effect of Intensity Standardization on Deep Learning for WML Segmentation in Multi-Centre FLAIR MRI. (arXiv:2307.03827v1 [eess.IV])

    [http://arxiv.org/abs/2307.03827](http://arxiv.org/abs/2307.03827)

    本研究评估了多种MRI强度标准化方法在多中心FLAIR MRI白质病变分割中的效果，并提出了一个集成模型，将不同方法的预测结果进行组合。结果表明，强度标准化可以提高深度学习模型在新机构数据上的性能。

    

    深度学习方法在MRI中用于白质病变分割时，当应用于训练数据集之外的扫描仪或中心的数据时，性能会降低。由于当前模型不能直接应用于新机构的数据，这对于转化和广泛应用非常重要。在本研究中，我们评估了几种MRI强度标准化方法作为多中心液体衰减反转恢复（FLAIR）MRI白质病变分割的预处理步骤。我们评估了一种专门用于FLAIR MRI的方法IAMLAB，以及其他常见的标准化技术，如White-strip、Nyul和Z-score。我们提出了一个集成模型，将每个模型的预测结果进行组合。我们训练了一个跳跃连接UNet（SC UNet）模型，使用标准化图像和原始数据进行训练，并在多个维度上评估了分割性能。

    Deep learning (DL) methods for white matter lesion (WML) segmentation in MRI suffer a reduction in performance when applied on data from a scanner or centre that is out-of-distribution (OOD) from the training data. This is critical for translation and widescale adoption, since current models cannot be readily applied to data from new institutions. In this work, we evaluate several intensity standardization methods for MRI as a preprocessing step for WML segmentation in multi-centre Fluid-Attenuated Inversion Recovery (FLAIR) MRI. We evaluate a method specifically developed for FLAIR MRI called IAMLAB along with other popular normalization techniques such as White-strip, Nyul and Z-score. We proposed an Ensemble model that combines predictions from each of these models. A skip-connection UNet (SC UNet) was trained on the standardized images, as well as the original data and segmentation performance was evaluated over several dimensions. The training (in-distribution) data consists of a 
    
[^69]: 在有界损失下的在线学习游戏的组合特征化

    A Combinatorial Characterization of Online Learning Games with Bounded Losses. (arXiv:2307.03816v1 [cs.LG])

    [http://arxiv.org/abs/2307.03816](http://arxiv.org/abs/2307.03816)

    这项研究提出了一个新的尺度敏感的组合维度，称为顺序极小极大维度，并通过对有界损失的在线学习游戏进行研究，给出了对向量值回归和多标签分类的在线可学习性的紧密定量刻画。

    

    我们研究了假设类别对任意但有界损失函数的在线可学习性。我们提出了一个新的尺度敏感的组合维度，称为顺序极小极大维度，并证明它能够紧密定量地刻画在线可学习性。作为应用，我们给出了对两个常见学习场景的在线可学习性的首个定量刻画：向量值回归和多标签分类。

    We study the online learnability of hypothesis classes with respect to arbitrary, but bounded, loss functions. We give a new scale-sensitive combinatorial dimension, named the sequential Minimax dimension, and show that it gives a tight quantitative characterization of online learnability. As applications, we give the first quantitative characterization of online learnability for two natural learning settings: vector-valued regression and multilabel classification.
    
[^70]: 使用下一代储层计算控制混沌映射

    Controlling Chaotic Maps using Next-Generation Reservoir Computing. (arXiv:2307.03813v1 [cs.LG])

    [http://arxiv.org/abs/2307.03813](http://arxiv.org/abs/2307.03813)

    这项工作将非线性系统控制技术与储层计算相结合，成功地在混沌H\'enon映射上展示了控制器对于控制系统的稳定、固定点的控制和任意期望状态的控制的性能，并且只需10个数据点进行训练，单次迭代就能控制到期望轨迹，并且具有鲁棒性。

    

    在这项工作中，我们将非线性系统控制技术与最先进的储层计算相结合，储层计算是一种用于预测动力系统行为的机器学习方法。我们在混沌H\'enon映射上展示了控制器在一系列控制任务中的性能，包括在不稳定的固定点之间控制系统，将系统稳定到更高阶周期轨道，并控制系统到任意期望状态。我们展示了我们的控制器在这些任务中成功，并且仅需10个数据点进行训练，可以在单次迭代中将系统控制到期望轨迹，并且对噪声和建模误差具有鲁棒性。

    In this work, we combine nonlinear system control techniques with next-generation reservoir computing, a best-in-class machine learning approach for predicting the behavior of dynamical systems. We demonstrate the performance of the controller in a series of control tasks for the chaotic H\'enon map, including controlling the system between unstable fixed-points, stabilizing the system to higher order periodic orbits, and to an arbitrary desired state. We show that our controller succeeds in these tasks, requires only 10 data points for training, can control the system to a desired trajectory in a single iteration, and is robust to noise and modeling error.
    
[^71]: 将电池电解质的结构组成映射到器件性能的配方图

    Formulation Graphs for Mapping Structure-Composition of Battery Electrolytes to Device Performance. (arXiv:2307.03811v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2307.03811](http://arxiv.org/abs/2307.03811)

    该论文提出了一种深度学习模型，Formulation Graph Convolution Network（F-GCN），它可以将电池电解质的结构组成关系映射到整个液体配方的性能，从而加快新化合物的发现和应用。

    

    正在积极寻求先进的计算方法来解决发现和开发新的组合材料（如配方）所面临的挑战。一个广泛采用的方法是领域感知的高通量筛选，该方法可以将各个组分组合成配方。这种方法能够加速寻找目标应用的新化合物，但是在从精选化学空间中识别出合适的“配方”方面仍然主要是实验驱动的过程。我们报告了一种深度学习模型，称为Formulation Graph Convolution Network（F-GCN），可以将各个组分的结构组成关系映射到液体配方的性质。多个GCN并行组装，并根据配方中各个组成部分的摩尔百分比直观地对配方成分进行特征化。然后根据相应组成部分的摩尔百分比对所得的分子描述符进行缩放，接下来进行...

    Advanced computational methods are being actively sought for addressing the challenges associated with discovery and development of new combinatorial material such as formulations. A widely adopted approach involves domain informed high-throughput screening of individual components that can be combined into a formulation. This manages to accelerate the discovery of new compounds for a target application but still leave the process of identifying the right 'formulation' from the shortlisted chemical space largely a laboratory experiment-driven process. We report a deep learning model, Formulation Graph Convolution Network (F-GCN), that can map structure-composition relationship of the individual components to the property of liquid formulation as whole. Multiple GCNs are assembled in parallel that featurize formulation constituents domain-intuitively on the fly. The resulting molecular descriptors are scaled based on respective constituent's molar percentage in the formulation, followed
    
[^72]: URL：一种可转移不确定性估计的表示学习基准

    URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates. (arXiv:2307.03810v1 [cs.LG])

    [http://arxiv.org/abs/2307.03810](http://arxiv.org/abs/2307.03810)

    URL基准是一个评估预训练模型可转移性和不确定性估计的方式，研究发现专注于表示本身不确定性或直接估计预测风险的方法效果优于基于概率的方法。

    

    表示学习显著推动了该领域发展出能够作为从零开始迁移到新数据集时的有价值起点的预训练模型。随着对可靠机器学习和不确定性量化的需求不断增加，需要的预训练模型不仅能提供嵌入向量，还能提供可转移的不确定性估计。为了引导这样的模型的开发，我们提出了URL（Uncertainty-aware Representation Learning）基准。除了表示的可转移性之外，它还使用一种新颖的度量标准来测量不确定性估计的零样本可转移性。我们应用URL来评估11种在ImageNet上进行预训练并转移到8个下游数据集的不确定性量化器。我们发现，着重于表示本身的不确定性或直接估计预测风险的方法优于基于上游类别的概率的方法。然而，实现可转移的不确定性仍然是一个挑战。

    Representation learning has significantly driven the field to develop pretrained models that can act as a valuable starting point when transferring to new datasets. With the rising demand for reliable machine learning and uncertainty quantification, there is a need for pretrained models that not only provide embeddings but also transferable uncertainty estimates. To guide the development of such models, we propose the Uncertainty-aware Representation Learning (URL) benchmark. Besides the transferability of the representations, it also measures the zero-shot transferability of the uncertainty estimate using a novel metric. We apply URL to evaluate eleven uncertainty quantifiers that are pretrained on ImageNet and transferred to eight downstream datasets. We find that approaches that focus on the uncertainty of the representation itself or estimate the prediction risk directly outperform those that are based on the probabilities of upstream classes. Yet, achieving transferable uncertaint
    
[^73]: 对对抗鲁棒性中子网络贡献的理论视角

    A Theoretical Perspective on Subnetwork Contributions to Adversarial Robustness. (arXiv:2307.03803v1 [cs.LG])

    [http://arxiv.org/abs/2307.03803](http://arxiv.org/abs/2307.03803)

    本文提出了一个新的理论框架，研究了子网络的对抗鲁棒性对整个网络的鲁棒性的贡献，并引入了半鲁棒性的概念进行度量。这有助于更好地理解对抗性攻击并促进更高效的对抗性训练。

    

    对深度神经网络（DNN）对抗性攻击的鲁棒性已经进行了广泛研究，旨在更好地理解深度学习模型如何收敛，并确保这些模型在安全关键应用中的安全性。对抗训练是加强DNN对抗性攻击的一种方法，已经证明可以以应用计算昂贵的训练方法到整个模型的代价提供这种方法。为了更好地理解这些攻击并促进更高效的对抗性训练，本文提出了一个新的理论框架，研究了子网络的对抗鲁棒性对整个网络的鲁棒性的贡献。为此，首先引入了半鲁棒性的概念，它是子网络的对抗鲁棒性的度量。在此概念基础上，我们提供一个理论分析，证明如果子网络是半鲁棒的并且存在一个足够大的比例

    The robustness of deep neural networks (DNNs) against adversarial attacks has been studied extensively in hopes of both better understanding how deep learning models converge and in order to ensure the security of these models in safety-critical applications. Adversarial training is one approach to strengthening DNNs against adversarial attacks, and has been shown to offer a means for doing so at the cost of applying computationally expensive training methods to the entire model. To better understand these attacks and facilitate more efficient adversarial training, in this paper we develop a novel theoretical framework that investigates how the adversarial robustness of a subnetwork contributes to the robustness of the entire network. To do so we first introduce the concept of semirobustness, which is a measure of the adversarial robustness of a subnetwork. Building on this concept, we then provide a theoretical analysis to show that if a subnetwork is semirobust and there is a suffici
    
[^74]: CLIPMasterPrints: 使用潜在变量演化欺骗对比式语言-图像预训练

    CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution. (arXiv:2307.03798v1 [cs.CV])

    [http://arxiv.org/abs/2307.03798](http://arxiv.org/abs/2307.03798)

    本文展示了对比式语言-图像预训练（CLIP）模型的脆弱性，通过挖掘生成模型的潜在空间可以找到欺骗主图像，这些图像在许多不同的提示下能欺骗CLIP模型，而对人类来说是无法认出的。欺骗主图像在少量图像标题上的训练上可能适用于更多数量的语义相关的标题。两种可能的缓解策略被评估，并发现脆弱性与对比式预训练中的模态差距密切相关。

    

    以对比式语言-图像预训练（CLIP）为代表的同时利用视觉和文本数据的模型越来越重要。本文展示了尽管这些模型具有多功能性，但它们对于所谓的欺骗主图像是脆弱的。欺骗主图像能够最大化CLIP模型在许多不同的提示下的置信度评分，同时对于人类来说是无法认出的。我们展示了如何通过使用演化策略或随机梯度下降在生成模型的潜在空间中搜索欺骗主图像。我们研究了挖掘出的欺骗主图像的特性，并发现在少量图像标题上训练的图像可能适用于更多数量的语义相关的标题。此外，我们评估了两种可能的缓解策略，并发现对欺骗主例子的脆弱性与对比式预训练中的模态差距密切相关。

    Models leveraging both visual and textual data such as Contrastive Language-Image Pre-training (CLIP), are increasingly gaining importance. In this work, we show that despite their versatility, such models are vulnerable to what we refer to as fooling master images. Fooling master images are capable of maximizing the confidence score of a CLIP model for a significant number of widely varying prompts, while being unrecognizable for humans. We demonstrate how fooling master images can be mined by searching the latent space of generative models by means of an evolution strategy or stochastic gradient descent. We investigate the properties of the mined fooling master images, and find that images trained on a small number of image captions potentially generalize to a much larger number of semantically related captions. Further, we evaluate two possible mitigation strategies and find that vulnerability to fooling master examples is closely related to a modality gap in contrastive pre-trained
    
[^75]: 基于神经抽象的控制器合成和部署

    Neural Abstraction-Based Controller Synthesis and Deployment. (arXiv:2307.03783v1 [eess.SY])

    [http://arxiv.org/abs/2307.03783](http://arxiv.org/abs/2307.03783)

    本论文提出了一种使用神经网络表示来减轻抽象技术中高内存需求的方法，用于合成正确构建的控制器。此外，该论文还提供了一种新颖的训练算法，用于部署这些控制器。

    

    基于抽象的技术是一种吸引人的方法，用于合成满足高层时间要求的正确构建的控制器。成功应用这些技术的主要障碍是内存需求，在控制器合成和控制器部署期间都存在内存需求问题。我们提出了一种使用神经网络表示来减轻基于抽象的技术的高内存需求的内存高效方法。为了执行达到避免规范的合成，我们提出了一种依赖于系统前向和后向动力学的压缩神经网络表示的即时算法。与通常的神经表示应用不同，我们的技术保持端到端过程的完整性。为了确保这一点，我们纠正了训练后神经网络的输出，使得修正后的输出表示在有限抽象方面是准确的。对于部署，我们提供了一种新颖的训练算法以完成部署。

    Abstraction-based techniques are an attractive approach for synthesizing correct-by-construction controllers to satisfy high-level temporal requirements. A main bottleneck for successful application of these techniques is the memory requirement, both during controller synthesis and in controller deployment.  We propose memory-efficient methods for mitigating the high memory demands of the abstraction-based techniques using neural network representations. To perform synthesis for reach-avoid specifications, we propose an on-the-fly algorithm that relies on compressed neural network representations of the forward and backward dynamics of the system. In contrast to usual applications of neural representations, our technique maintains soundness of the end-to-end process. To ensure this, we correct the output of the trained neural network such that the corrected output representations are sound with respect to the finite abstraction. For deployment, we provide a novel training algorithm to 
    
[^76]: 对于女性而言，生活和自由：基于参与式人工智能的社交网络分析论文研究伊朗性别斗争中的分水岭时刻

    For Women, Life, Freedom: A Participatory AI-Based Social Web Analysis of a Watershed Moment in Iran's Gender Struggles. (arXiv:2307.03764v1 [cs.CY])

    [http://arxiv.org/abs/2307.03764](http://arxiv.org/abs/2307.03764)

    本文通过对波斯语Twitter话语的计算分析，估算了马莎·阿米尼去世后伊朗社会对性别平等立场的转变。研究采用参与式人工智能方法，涉及伊朗女性在标注过程中的积极角色。结果发现，马莎·阿米尼的死亡引发了波斯语话语的极化，积极推文数量略多于负面推文数量。此外，研究还观察到政府和抗议者在Twitter上的存在差异。

    

    本文通过对波斯语Twitter话语的计算分析，旨在估算马莎·阿米尼在警方拘留期间去世后对性别平等立场的转变。我们提出了一个集成主动学习流程来训练一个立场分类器。我们的创新在于伊朗女性以主动的角色参与在构建这个人工智能系统中的标注。我们的标注者不仅提供标签，还提供有价值的关键词以进行更有意义的语料库创建，并提供简短的示例文档以进行引导式抽样。我们的分析结果表明，马莎·阿米尼的死亡引发了极化的波斯语话语，对性别平等的负面和积极推文数量都增加了。积极推文的增加略大于负面推文的增加。我们还观察到，就账号创建时间而言，与政府对齐的Twitter账号和支持抗议的Twitter账号之间存在差异。

    In this paper, we present a computational analysis of the Persian language Twitter discourse with the aim to estimate the shift in stance toward gender equality following the death of Mahsa Amini in police custody. We present an ensemble active learning pipeline to train a stance classifier. Our novelty lies in the involvement of Iranian women in an active role as annotators in building this AI system. Our annotators not only provide labels, but they also suggest valuable keywords for more meaningful corpus creation as well as provide short example documents for a guided sampling step. Our analyses indicate that Mahsa Amini's death triggered polarized Persian language discourse where both fractions of negative and positive tweets toward gender equality increased. The increase in positive tweets was slightly greater than the increase in negative tweets. We also observe that with respect to account creation time, between the state-aligned Twitter accounts and pro-protest Twitter accounts
    
[^77]: 异构传感器网络中的异常检测的动态图注意力

    Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks. (arXiv:2307.03761v1 [cs.LG])

    [http://arxiv.org/abs/2307.03761](http://arxiv.org/abs/2307.03761)

    该论文提出了一种基于图的方法，命名为DyGATAD，用于在异构传感器网络中进行异常检测。该方法利用动态图注意力机制来识别集体异常行为，其中异常行为可能由于系统内部相互关系的变化引起。这在工业物联网监控系统中具有重要的实际应用价值。

    

    在数字化转型的时代，由工业物联网 (IIoT) 监控的系统通过异构传感器网络生成大量的多元时间序列 (MTS) 数据。虽然这些数据有助于条件监控和异常检测，但是传感器网络中日益复杂和相互依赖的关系也给异常检测带来了重大挑战。尽管在这个领域取得了一些进展，但主要集中在点异常和背景异常，对集体异常的关注较少。集体异常的一种常见变种是异常集体行为由系统内部的相互关系变化引起。这可能是由于异常环境条件（如过热）、由于网络攻击造成的不正确操作设置或系统级故障引起的。为了解决这些挑战，本文提出了 DyGATAD（一种动态图注意力的异常检测方法），采用基于图的方法来识别传感器网络中的异常行为。

    In the era of digital transformation, systems monitored by the Industrial Internet of Things (IIoTs) generate large amounts of Multivariate Time Series (MTS) data through heterogeneous sensor networks. While this data facilitates condition monitoring and anomaly detection, the increasing complexity and interdependencies within the sensor network pose significant challenges for anomaly detection. Despite progress in this field, much of the focus has been on point anomalies and contextual anomalies, with lesser attention paid to collective anomalies. A less addressed but common variant of collective anomalies is when the abnormal collective behavior is caused by shifts in interrelationships within the system. This can be due to abnormal environmental conditions like overheating, improper operational settings resulting from cyber-physical attacks, or system-level faults. To address these challenges, this paper proposes DyGATAD (Dynamic Graph Attention for Anomaly Detection), a graph-based
    
[^78]: 一项关于时间序列的图神经网络综述：预测、分类、插值和异常检测

    A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection. (arXiv:2307.03759v1 [cs.LG])

    [http://arxiv.org/abs/2307.03759](http://arxiv.org/abs/2307.03759)

    这项综述介绍了图神经网络在时间序列分析中的应用，包括预测、分类、异常检测和插值。图神经网络能够显式地建模时间序列和变量之间的关系，为时间序列数据分析带来了新的方法和技术。

    

    时间序列是记录动态系统测量结果的主要数据类型，通过物理传感器和在线过程（虚拟传感器）生成大量数据。时间序列分析对于揭示可用数据中所蕴含的丰富信息至关重要。随着图神经网络（GNN）的最新进展，基于GNN的时间序列分析方法也大幅增加。这些方法能够显式地建模时间序列和变量之间的关系，而传统和其他深度神经网络方法则面临困难。在本综述中，我们提供了一份全面的基于图神经网络的时间序列分析综述（GNN4TS），包括四个基本维度：预测、分类、异常检测和插值。我们旨在指导设计师和实践者了解、构建应用和推动GNN4TS的研究。首先，我们提供了一个全面的面向任务的GNN4TS分类体系。接下来，我们展示了...

    Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. Approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: Forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present a
    
[^79]: 在无线网络上的联邦学习：通过随机接入实现分布式用户选择

    Federated Learning over a Wireless Network: Distributed User Selection through Random Access. (arXiv:2307.03758v1 [cs.LG])

    [http://arxiv.org/abs/2307.03758](http://arxiv.org/abs/2307.03758)

    本研究提出了一种利用随机接入中的无线电资源竞争机制的网络内在方法来实现分布式用户选择进行联邦学习。通过操纵竞争窗口大小和使用训练数据偏差作为优先级依据，可以快速实现高效的收敛，同时保证公平性。

    

    用户选择对于降低联邦学习在无线网络上的通信成本变得至关重要。然而，集中式用户选择会引起额外的系统复杂性。本研究提出了一种利用随机接入中的无线电资源竞争机制的网络内在方法来进行分布式用户选择。以载波感知多路访问（CSMA）机制为随机接入的例子，我们通过操纵竞争窗口（CW）大小，在每轮训练中优先为某些用户获取无线电资源。训练数据偏差被用作带有用户选择的联邦学习的目标场景。优先级基于新训练的局部模型与上一轮的全局模型之间的距离。为了避免某些用户过度贡献，使用计数机制来确保公平性。使用各种数据集进行的模拟表明，该方法可以快速实现类似于集中式训练的收敛。

    User selection has become crucial for decreasing the communication costs of federated learning (FL) over wireless networks. However, centralized user selection causes additional system complexity. This study proposes a network intrinsic approach of distributed user selection that leverages the radio resource competition mechanism in random access. Taking the carrier sensing multiple access (CSMA) mechanism as an example of random access, we manipulate the contention window (CW) size to prioritize certain users for obtaining radio resources in each round of training. Training data bias is used as a target scenario for FL with user selection. Prioritization is based on the distance between the newly trained local model and the global model of the previous round. To avoid excessive contribution by certain users, a counting mechanism is used to ensure fairness. Simulations with various datasets demonstrate that this method can rapidly achieve convergence similar to that of the centralized 
    
[^80]: FITS：模拟具有10k个参数的时间序列

    FITS: Modeling Time Series with $10k$ Parameters. (arXiv:2307.03756v1 [cs.LG])

    [http://arxiv.org/abs/2307.03756](http://arxiv.org/abs/2307.03756)

    FITS是一种轻量而强大的时间序列分析模型，通过在复杂频率域中进行插值操作，丢弃对时间序列数据影响微小的高频分量，实现了与最先进模型相当的性能，并且具有较小的模型参数数量，适用于边缘设备。

    

    本文介绍了FITS，一种轻量而强大的时间序列分析模型。与直接处理原始时间域数据的现有模型不同，FITS基于在复杂频率域中进行插值的原理操作时间序列。通过丢弃对时间序列数据影响微小的高频分量，FITS在时间序列预测和异常检测任务中实现了与最先进模型相当的性能，同时具有近似10k个参数的显著紧凑大小。这种轻量级模型可以轻松地在边缘设备上进行训练和部署，为各种应用创造了机会。

    In this paper, we introduce FITS, a lightweight yet powerful model for time series analysis. Unlike existing models that directly process raw time-domain data, FITS operates on the principle that time series can be manipulated through interpolation in the complex frequency domain. By discarding high-frequency components with negligible impact on time series data, FITS achieves performance comparable to state-of-the-art models for time series forecasting and anomaly detection tasks, while having a remarkably compact size of only approximately $10k$ parameters. Such a lightweight model can be easily trained and deployed in edge devices, creating opportunities for various applications. The anonymous code repo is available in: \url{https://anonymous.4open.science/r/FITS}
    
[^81]: 可编程合成表格数据生成

    Programmable Synthetic Tabular Data Generation. (arXiv:2307.03577v1 [cs.LG])

    [http://arxiv.org/abs/2307.03577](http://arxiv.org/abs/2307.03577)

    这项工作介绍了ProgSyn，第一个可编程的合成表格数据生成算法，它允许对生成的数据进行全面的自定义，并且通过预训练和微调生成模型来确保高质量的数据和遵守自定义规范。

    

    由于隐私、数据质量和数据共享的限制，大量的表格数据仍然被低效利用。尽管训练一个能够生成类似原始分布的合成数据的生成模型可以解决其中一些问题，但大多数应用程序还需要额外的生成数据约束。现有的合成数据方法存在局限性，因为它们通常只处理特定的约束条件，例如差分隐私（DP）或增加公平性，并且缺乏一个可访问的接口来声明一般规范。在这项工作中，我们介绍了ProgSyn，这是第一个可编程的合成表格数据生成算法，它允许对生成的数据进行全面的自定义。为了确保高质量的数据并遵守自定义规范，ProgSyn在原始数据集上进行预训练生成模型，并在提供的规范上自动推导出可微分损失进行微调。这些规范可以使用统计和。

    Large amounts of tabular data remain underutilized due to privacy, data quality, and data sharing limitations. While training a generative model producing synthetic data resembling the original distribution addresses some of these issues, most applications require additional constraints from the generated data. Existing synthetic data approaches are limited as they typically only handle specific constraints, e.g., differential privacy (DP) or increased fairness, and lack an accessible interface for declaring general specifications. In this work, we introduce ProgSyn, the first programmable synthetic tabular data generation algorithm that allows for comprehensive customization over the generated data. To ensure high data quality while adhering to custom specifications, ProgSyn pre-trains a generative model on the original dataset and fine-tunes it on a differentiable loss automatically derived from the provided specifications. These can be programmatically declared using statistical and
    
[^82]: ITA:一种基于量化的Transformer的能效高的Attention和Softmax加速器

    ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers. (arXiv:2307.03493v1 [cs.AR])

    [http://arxiv.org/abs/2307.03493](http://arxiv.org/abs/2307.03493)

    ITA是一种基于量化的Transformer的能效高的Attention和Softmax加速器。通过利用8位量化和仅基于整数值的创新Softmax实现，ITA实现了高能效的推理，在16.9 TOPS/W的能效上超过了最先进的Transformer加速器，并在5.93 TOPS/mm$^2$的面积效率上超越了它们。

    

    Transformer网络已经成为自然语言处理任务的最先进方法，并在计算机视觉和音频处理等其他领域受到欢迎。然而，Transformer模型的高算术强度、大内存需求和复杂数据流依赖导致了其有效硬件加速面临新的挑战。在这项工作中，我们提出了ITA，一种针对嵌入式系统上高效推理的Transformer和相关模型的新型加速器架构，通过利用8位量化和仅基于整数值的创新Softmax实现。通过在流模式下实时计算，我们的Softmax实现最大程度地减少了数据移动和能量消耗。ITA在能效方面与最先进的Transformer加速器保持竞争力，达到了16.9 TOPS/W，同时在面积效率方面以5.93 TOPS/mm$^2$的成绩超越了它们，在22纳米完全耗尽的硅上。

    Transformer networks have emerged as the state-of-the-art approach for natural language processing tasks and are gaining popularity in other domains such as computer vision and audio processing. However, the efficient hardware acceleration of transformer models poses new challenges due to their high arithmetic intensities, large memory requirements, and complex dataflow dependencies. In this work, we propose ITA, a novel accelerator architecture for transformers and related models that targets efficient inference on embedded systems by exploiting 8-bit quantization and an innovative softmax implementation that operates exclusively on integer values. By computing on-the-fly in streaming mode, our softmax implementation minimizes data movement and energy consumption. ITA achieves competitive energy efficiency with respect to state-of-the-art transformer accelerators with 16.9 TOPS/W, while outperforming them in area efficiency with 5.93 TOPS/mm$^2$ in 22 nm fully-depleted silicon-on-insu
    
[^83]: 探索大规模语言模型（LLMs）在图学习中的潜力

    Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v1 [cs.LG])

    [http://arxiv.org/abs/2307.03393](http://arxiv.org/abs/2307.03393)

    本文探索了大规模语言模型（LLMs）在图学习中的潜力，并尝试了两种不同的流程：将LLMs作为增强器通过海量知识来增强节点的文本属性，并使用图神经网络（GNNs）生成预测，以及直接使用LLMs作为独立的预测器。

    

    图学习因其广泛的现实世界应用而引起了极大的关注。以文本节点属性为主的图学习最流行的流程主要依赖于图神经网络（GNN），并利用浅层文本嵌入作为初始节点表示，但存在通用知识和深刻语义理解方面的限制。近年来，大规模语言模型（LLMs）被证明具有广泛的常识和强大的语义理解能力，已经颠覆了现有的处理文本数据的工作流程。在本文中，我们旨在探索LLMs在图机器学习中的潜力，特别是节点分类任务，并研究两种可能的流程：LLMs作为增强器和LLMs作为预测器。前者利用LLMs通过其海量知识增强节点的文本属性，然后通过GNNs生成预测。后者试图直接使用LLMs作为独立的预测器。

    Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct 
    
[^84]: 泛化反向传播用于基于梯度的可解释性

    Generalizing Backpropagation for Gradient-Based Interpretability. (arXiv:2307.03056v1 [cs.LG])

    [http://arxiv.org/abs/2307.03056](http://arxiv.org/abs/2307.03056)

    本论文在深度神经网络的特征解释中，泛化了反向传播算法，以便更好地理解梯度图的可解释统计数据，如最高加权路径和熵。作者通过在合成数据集上的评估和应用于BERT的实验中验证了该方法的有效性。

    

    许多用于解释深度神经网络的流行特征归因方法依赖于计算模型输出对输入的梯度。虽然这些方法可以指示哪些输入特征可能对模型的预测很重要，但它们对模型本身的内部工作了解甚少。在本文中，我们观察到模型的梯度计算是使用半环的更一般形式的特例。这种观察使我们能够将反向传播算法泛化，以高效地计算关于神经网络梯度图的其他可解释统计数据，例如最高加权路径和熵。我们实现了这个泛化算法，在合成数据集上进行评估以更好地理解它计算的统计数据，并将其应用于研究BERT在主谓数一致性任务（SVA）上的行为。使用这种方法，我们验证了模型组件上通过的梯度流量反映了其重要性。

    Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model's output with respect to its inputs. While these methods can indicate which input features may be important for the model's prediction, they reveal little about the inner workings of the model itself. In this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings. This observation allows us to generalize the backpropagation algorithm to efficiently compute other interpretable statistics about the gradient graph of a neural network, such as the highest-weighted path and entropy. We implement this generalized algorithm, evaluate it on synthetic datasets to better understand the statistics it computes, and apply it to study BERT's behavior on the subject-verb number agreement task (SVA). With this method, we (a) validate that the amount of gradient flow through a component of a model reflects its impor
    
[^85]: PUFFIN: 用于蒸汽压力预测的路径统一前向接口网络

    PUFFIN: A Path-Unifying Feed-Forward Interfaced Network for Vapor Pressure Prediction. (arXiv:2307.02903v1 [physics.chem-ph])

    [http://arxiv.org/abs/2307.02903](http://arxiv.org/abs/2307.02903)

    PUFFIN是一种结合迁移学习和基于领域知识的归纳偏差节点的机器学习框架，用于改进蒸汽压力预测。通过利用归纳偏差和图嵌入的迁移学习，PUFFIN在预测中胜过不使用归纳偏差或使用通用描述符的替代策略。

    

    准确预测蒸汽压力对于工业和环境应用至关重要。然而，由于实验的资源和劳动强度，无法获得所有有兴趣的化合物的准确测量。当希望预测蒸汽压力的温度相关关系时，资源和劳动的需求进一步增加。在本文中，我们提出了PUFFIN（路径统一前向接口网络），这是一个将迁移学习与启发于领域知识（安托万方程）的新归纳偏差节点结合起来的机器学习框架，以改善蒸汽压力预测。通过利用归纳偏差和使用图嵌入的迁移学习，PUFFIN优于不使用归纳偏差或使用通用描述符的替代策略。该框架将领域特定知识的融入克服了数据可用性不足的限制，展示出了其在更广泛应用中的潜力。

    Accurately predicting vapor pressure is vital for various industrial and environmental applications. However, obtaining accurate measurements for all compounds of interest is not possible due to the resource and labor intensity of experiments. The demand for resources and labor further multiplies when a temperature-dependent relationship for predicting vapor pressure is desired. In this paper, we propose PUFFIN (Path-Unifying Feed-Forward Interfaced Network), a machine learning framework that combines transfer learning with a new inductive bias node inspired by domain knowledge (the Antoine equation) to improve vapor pressure prediction. By leveraging inductive bias and transfer learning using graph embeddings, PUFFIN outperforms alternative strategies that do not use inductive bias or that use generic descriptors of compounds. The framework's incorporation of domain-specific knowledge to overcome the limitation of poor data availability shows its potential for broader applications in 
    
[^86]: 学习探索先前行为来解决任务

    Learning to Solve Tasks with Exploring Prior Behaviours. (arXiv:2307.02889v1 [cs.RO])

    [http://arxiv.org/abs/2307.02889](http://arxiv.org/abs/2307.02889)

    本文提出了一种基于示例的控制方法（IRDEC），通过内在奖励驱动以及探索获得先前行为，并与示范中的任务特定行为连接，从而解决具有稀疏奖励的任务。这种方法在三个导航任务上的性能优于其他基线方法。

    

    在深度强化学习中，示范常被广泛用于解决具有稀疏奖励的任务。然而，现实世界场景中的任务往往具有与示范不同的初始条件，这就需要额外的先前行为。例如，假设我们得到了“从打开抽屉中拿取物体”的任务的示范，但在训练时抽屉是关闭的。如果没有掌握打开抽屉的先前行为，机器人很难解决这个任务。为了解决这个问题，我们提出了一种内在奖励驱动的基于示例的控制方法（IRDEC）。我们的方法可以赋予智能体探索和获取所需的先前行为的能力，并与示范中的任务特定行为连接，从而解决稀疏奖励任务，而无需额外展示先前行为示范。我们的方法在三个导航任务上的性能优于其他基线方法。

    Demonstrations are widely used in Deep Reinforcement Learning (DRL) for facilitating solving tasks with sparse rewards. However, the tasks in real-world scenarios can often have varied initial conditions from the demonstration, which would require additional prior behaviours. For example, consider we are given the demonstration for the task of \emph{picking up an object from an open drawer}, but the drawer is closed in the training. Without acquiring the prior behaviours of opening the drawer, the robot is unlikely to solve the task. To address this, in this paper we propose an Intrinsic Rewards Driven Example-based Control \textbf{(IRDEC)}. Our method can endow agents with the ability to explore and acquire the required prior behaviours and then connect to the task-specific behaviours in the demonstration to solve sparse-reward tasks without requiring additional demonstration of the prior behaviours. The performance of our method outperforms other baselines on three navigation tasks a
    
[^87]: 基于数据驱动的预测性5G延迟：利用网络测量进行理论和实验分析

    Data-driven Predictive Latency for 5G: A Theoretical and Experimental Analysis Using Network Measurements. (arXiv:2307.02329v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2307.02329](http://arxiv.org/abs/2307.02329)

    本研究利用实际网络数据对5G网络中的预测性延迟进行了全面分析，并提出了以Hypoexponential分布为基础的用户面延迟的分析表达式。通过机器学习领域的贝叶斯学习和图机器学习技术，我们进行了概率回归、异常检测和预测性预测的实验。测试结果表明，该预测框架适用于不同情景下的移动性、城市交通和社交聚会。

    

    随着5G服务和应用的出现，具有绑定延迟要求和保证服务质量（QoS）的需求迫使网络管理程序纳入自主和主动决策。我们研究的目标是利用移动网络运营商（MNOs）可以访问的真实网络数据，对5G网络中的预测性延迟进行全面分析。具体而言，（i）我们提出了作为Hypoexponential分布的用户面延迟的分析表达式，并通过与实证测量的比较分析进行验证，（ii）我们进行了概率回归、异常检测和预测性预测的实验结果，利用了机器学习领域中的新兴领域，如贝叶斯学习（BL）和图机器学习（GML）。我们使用从车辆移动、密集城区交通和社交聚会场景中收集的数据测试我们的预测框架。

    The advent of novel 5G services and applications with binding latency requirements and guaranteed Quality of Service (QoS) hastened the need to incorporate autonomous and proactive decision-making in network management procedures. The objective of our study is to provide a thorough analysis of predictive latency within 5G networks by utilizing real-world network data that is accessible to mobile network operators (MNOs). In particular, (i) we present an analytical formulation of the user-plane latency as a Hypoexponential distribution, which is validated by means of a comparative analysis with empirical measurements, and (ii) we conduct experimental results of probabilistic regression, anomaly detection, and predictive forecasting leveraging on emerging domains in Machine Learning (ML), such as Bayesian Learning (BL) and Machine Learning on Graphs (GML). We test our predictive framework using data gathered from scenarios of vehicular mobility, dense-urban traffic, and social gathering 
    
[^88]: 准确和校准模型的集合学习

    Set Learning for Accurate and Calibrated Models. (arXiv:2307.02245v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02245](http://arxiv.org/abs/2307.02245)

    提出了一种集合学习方法(OKO)来解决机器学习中的模型过度自信和校准不良问题，通过最小化集合的交叉熵误差，从而提高准确性和校准效果，并在有限的训练数据和类别不平衡情况下表现出更好的结果。

    

    模型过度自信和校准不良在机器学习中很常见，并且在应用标准经验风险最小化时很难解决。在这项工作中，我们提出了一种新的方法来缓解这些问题，我们称之为奇数-$k$-去除学习（OKO），它通过最小化集合的交叉熵误差而不是单个示例的误差来实现。这自然地使模型能够捕捉数据示例之间的相关性，并在有限的训练数据和类别不平衡的情况下实现更好的准确性和校准。令人惊讶的是，即使使用硬标签进行训练并且不进行任何额外的校准参数调整，如温度缩放，OKO通常也能获得更好的校准效果。我们提供了理论上的证明，证明OKO自然地能够获得更好的校准效果，并进行了广泛的实验分析以验证我们的理论发现。我们强调，OKO是一个通用的框架，可以很容易地适应许多不同的情境。

    Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets rather than for single examples. This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes. Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling. We provide theoretical justification, establishing that OKO naturally yields better calibration, and provide extensive experimental analyses that corroborate our theoretical findings. We emphasize that OKO is a general framework that can be easily adapted to many settings and the
    
[^89]: ChiENN: 用图神经网络实现对分子手性的拥抱

    ChiENN: Embracing Molecular Chirality with Graph Neural Networks. (arXiv:2307.02198v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02198](http://arxiv.org/abs/2307.02198)

    这篇论文介绍了一种名为ChiENN的图神经网络层，通过引入对手性敏感的信息传递方案，使得图神经网络能够区分化学化合物和其对映体之间的差异。在化学信息学中，这种方法在药物发现领域的手性敏感分子性质预测任务中表现出超越当前最先进方法的优异性能。

    

    图神经网络在许多深度学习问题中起着基础作用，尤其在化学信息学中。然而，典型的图神经网络不能捕捉手性的概念，这意味着它们无法区分化学化合物的3D图与其对映体（对映异构体）的差异。区分对映异构体的能力在药物发现中尤为重要，因为对映异构体可能具有非常不同的生物化学性质。在本文中，我们提出了一种在理论上得到了证明的信息传递方案，使得图神经网络对节点邻居的顺序敏感。我们将这个通用概念应用在分子手性的背景下，构建了可以附加到任何图神经网络模型的手性边缘神经网络（ChiENN）层，以实现对手性的意识。我们的实验证明，将ChiENN层添加到图神经网络中，优于当前最先进的方法，在对手性敏感的分子性质预测任务中表现出色。

    Graph Neural Networks (GNNs) play a fundamental role in many deep learning problems, in particular in cheminformatics. However, typical GNNs cannot capture the concept of chirality, which means they do not distinguish between the 3D graph of a chemical compound and its mirror image (enantiomer). The ability to distinguish between enantiomers is important especially in drug discovery because enantiomers can have very distinct biochemical properties. In this paper, we propose a theoretically justified message-passing scheme, which makes GNNs sensitive to the order of node neighbors. We apply that general concept in the context of molecular chirality to construct Chiral Edge Neural Network (ChiENN) layer which can be appended to any GNN model to enable chirality-awareness. Our experiments show that adding ChiENN layers to a GNN outperforms current state-of-the-art methods in chiral-sensitive molecular property prediction tasks.
    
[^90]: SRCD：多个复合领域的语义推理用于单领域广义目标检测

    SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection. (arXiv:2307.01750v1 [cs.CV])

    [http://arxiv.org/abs/2307.01750](http://arxiv.org/abs/2307.01750)

    本文提供了针对单领域广义目标检测的新框架SRCD，通过学习和维护自增广义跨领域样本的语义结构，提高模型的泛化能力。

    

    本文提供了一个针对单领域广义目标检测（即Single-DGOD）的新框架，其中我们对学习和维护自增广义跨领域样本的语义结构以增强模型的泛化能力感兴趣。与在多个源领域上训练的DGOD不同，Single-DGOD要仅使用一个单一源领域很难很好地泛化到多个目标领域。现有方法大多采用了与DGOD类似的处理方式，通过解耦合或压缩语义空间来学习域不变特征。然而，存在两个潜在的限制：1）由于极度稀缺的单领域数据，伪属性-标签相关性；2）通常忽略语义结构信息，即我们发现示例中的实例级语义关系的相似性对模型的泛化至关重要。在本文中，我们引入了用于Single-DGOD的复合领域的语义推理（SRCD）。

    This paper provides a novel framework for single-domain generalized object detection (i.e., Single-DGOD), where we are interested in learning and maintaining the semantic structures of self-augmented compound cross-domain samples to enhance the model's generalization ability. Different from DGOD trained on multiple source domains, Single-DGOD is far more challenging to generalize well to multiple target domains with only one single source domain. Existing methods mostly adopt a similar treatment from DGOD to learn domain-invariant features by decoupling or compressing the semantic space. However, there may have two potential limitations: 1) pseudo attribute-label correlation, due to extremely scarce single-domain data; and 2) the semantic structural information is usually ignored, i.e., we found the affinities of instance-level semantic relations in samples are crucial to model generalization. In this paper, we introduce Semantic Reasoning with Compound Domains (SRCD) for Single-DGOD. 
    
[^91]: 在线学习和使用ERM预言机解决无穷博弈问题

    Online Learning and Solving Infinite Games with an ERM Oracle. (arXiv:2307.01689v1 [cs.LG])

    [http://arxiv.org/abs/2307.01689](http://arxiv.org/abs/2307.01689)

    这项工作提出了一种仅依赖ERM预言机调用的在线学习算法，该算法在可实现情况下具有有限的遗憾，并在不可知情况下具有亚线性增长的遗憾。同时，还提供了类似的结果用于非参数博弈环境中的学习算法，即仅依赖最佳响应预言机的学习算法，并收敛到近似极小-极大均衡点。

    

    在基于在线学习的情况下，ERM足以达到接近最优泛化误差的目标，但在在线学习环境下并非如此，通常的概念类算法依赖计算效率较低的预言机，如标准最优算法(SOA)。在这项工作中，我们提出了一种仅依赖ERM预言机调用的在线二分类算法，并证明在可实现的情况下具有有限的遗憾(regret)，在不可知的情况下具有亚线性增长的遗憾。我们通过底层概念类的Littlestone和阈值维度来限制遗憾。我们获得了类似的结果用于非参数博弈，其中ERM预言机可以被理解为最佳响应预言机，根据其他玩家的游戏历史找到一个玩家的最佳响应。在这种情况下，我们提供了仅依赖最佳响应预言机的学习算法，并收敛到两人零和博弈的近似极小-极大均衡点。

    While ERM suffices to attain near-optimal generalization error in the stochastic learning setting, this is not known to be the case in the online learning setting, where algorithms for general concept classes rely on computationally inefficient oracles such as the Standard Optimal Algorithm (SOA). In this work, we propose an algorithm for online binary classification setting that relies solely on ERM oracle calls, and show that it has finite regret in the realizable setting and sublinearly growing regret in the agnostic setting. We bound the regret in terms of the Littlestone and threshold dimensions of the underlying concept class.  We obtain similar results for nonparametric games, where the ERM oracle can be interpreted as a best response oracle, finding the best response of a player to a given history of play of the other players. In this setting, we provide learning algorithms that only rely on best response oracles and converge to approximate-minimax equilibria in two-player zero
    
[^92]: vONTSS：基于vMF和最优传输的半监督神经主题建模

    vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])

    [http://arxiv.org/abs/2307.01226](http://arxiv.org/abs/2307.01226)

    vONTSS是一种基于vMF和最优传输的半监督神经主题建模方法，它在分类准确率和多样性方面优于其他方法，并且支持无监督主题建模。实验证明，vONTSS比最近的NTM更快。

    

    最近，受变分自编码器启发的神经主题模型（NTM）引起了很多研究兴趣，然而，由于整合人类知识的挑战，这些方法在实际应用中受到了限制。本研究提出了一种半监督神经主题建模方法vONTSS，该方法利用基于von Mises-Fisher（vMF）的变分自编码器和最优传输。在半监督设置中，当提供每个主题的少量关键词时，vONTSS生成潜在主题并优化主题-关键词质量和主题分类。实验证明，vONTSS在分类准确率和多样性方面优于现有的半监督主题建模方法。vONTSS还支持无监督主题建模。定量和定性实验证明，vONTSS在无监督设置下在多个方面优于最近的NTM：vONTSS在基准数据集上发现高度聚类和连贯的主题。它也比现有-手法快得多。

    Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state
    
[^93]: 通过自由方向知识蒸馏在图神经网络中实现共同增长

    Shared Growth of Graph Neural Networks via Free-direction Knowledge Distillation. (arXiv:2307.00534v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00534](http://arxiv.org/abs/2307.00534)

    本文提出了一种基于自由方向知识蒸馏的图神经网络共同增长的框架，通过强化学习同时训练两个较浅的GNN模型，实现了它们之间的知识交流和共享。

    

    知识蒸馏（KD）已被证明对提升图神经网络（GNNs）性能有效，其中典型目标是将深层教师GNN的知识蒸馏到更浅的学生GNN中。然而，由于众所周知的过参数化和过平滑问题，训练一个令人满意的深层GNN通常很具挑战性，导致在实际应用中无效的知识转移。在本文中，我们提出了一种基于强化学习的图神经网络自由方向知识蒸馏框架，称为FreeKD，它不再需要提供一个更深层次的优化良好的教师GNN。我们的核心思想是通过层次化的强化学习来协同学习两个较浅的GNN，以便在它们之间交换知识。由于我们观察到一个典型的GNN模型在训练过程中在不同节点表现出更好和更差的性能，我们设计了一种动态和自由方向的知识传递策略，它涉及

    Knowledge distillation (KD) has shown to be effective to boost the performance of graph neural networks (GNNs), where the typical objective is to distill knowledge from a deeper teacher GNN into a shallower student GNN. However, it is often quite challenging to train a satisfactory deeper GNN due to the well-known over-parametrized and over-smoothing issues, leading to invalid knowledge transfer in practical applications. In this paper, we propose the first Free-direction Knowledge Distillation framework via reinforcement learning for GNNs, called FreeKD, which is no longer required to provide a deeper well-optimized teacher GNN. Our core idea is to collaboratively learn two shallower GNNs in an effort to exchange knowledge between them via reinforcement learning in a hierarchical way. As we observe that one typical GNN model often exhibits better and worse performances at different nodes during training, we devise a dynamic and free-direction knowledge transfer strategy that involves 
    
[^94]: GenRec:大型语言模型在生成式推荐中的应用

    GenRec: Large Language Model for Generative Recommendation. (arXiv:2307.00457v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2307.00457](http://arxiv.org/abs/2307.00457)

    本文介绍了一种基于大型语言模型的创新推荐系统方法GenRec，通过直接生成目标推荐项而不是计算排名分数，利用LLM的表达能力和理解能力来生成相关推荐。

    

    近年来，大型语言模型(Large Language Model，LLM)已经成为各种自然语言处理任务的强大工具。然而，在生成式推荐范式下，它们在推荐系统中的潜力相对未被探索。本文提出了一种创新的基于文本数据的推荐系统方法，利用大型语言模型(LLM)来进行推荐。我们介绍了一种新颖的大型语言模型推荐系统(GenRec)，该系统利用LLM的表达能力直接生成目标推荐项，而不是像传统的判别式推荐系统一样逐个计算每个候选项的排名分数。GenRec利用LLM的理解能力来解释上下文、学习用户偏好并生成相关推荐。我们提出的方法利用大型语言模型中编码的丰富知识来完成推荐任务。我们首先制定了专门的提示，以增强LLM理解推荐任务的能力。

    In recent years, large language models (LLM) have emerged as powerful tools for diverse natural language processing tasks. However, their potential for recommender systems under the generative recommendation paradigm remains relatively unexplored. This paper presents an innovative approach to recommendation systems using large language models (LLMs) based on text data. In this paper, we present a novel LLM for generative recommendation (GenRec) that utilized the expressive power of LLM to directly generate the target item to recommend, rather than calculating ranking score for each candidate item one by one as in traditional discriminative recommendation. GenRec uses LLM's understanding ability to interpret context, learn user preferences, and generate relevant recommendation. Our proposed approach leverages the vast knowledge encoded in large language models to accomplish recommendation tasks. We first we formulate specialized prompts to enhance the ability of LLM to comprehend recomm
    
[^95]: 使用生成对抗网络生成无监督文本嵌入空间用于文本合成

    Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v1 [cs.CL])

    [http://arxiv.org/abs/2306.17181](http://arxiv.org/abs/2306.17181)

    本论文提出了一种使用生成对抗网络（GAN）生成连续文本嵌入空间的方法（TESGAN），以解决传统GAN在自然语言生成中的限制。这种方法通过引入连续的文本嵌入空间取代离散的标记，使得生成器在通过反向传播更新梯度时更加有效。

    

    生成对抗网络（GAN）是一种用于数据合成的模型，通过生成器和判别器的竞争来创建逼真的数据。尽管GAN在图像合成方面得到了广泛研究，但在自然语言生成方面存在固有的限制。因为自然语言由离散的标记组成，生成器在通过反向传播更新梯度时遇到困难；因此，大多数文本-GAN研究使用奖励系统以随机标记为基础生成句子。因此，先前研究中的生成器在对抗训练之前以自回归方式进行预训练，导致合成的句子重复训练数据。在本文中，我们使用类似原始GAN的框架来合成句子。更具体地说，我们提出了文本嵌入空间生成对抗网络（TESGAN），它生成连续的文本嵌入空间来解决梯度反向传播的问题。

    Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagat
    
[^96]: 关于神经时态点过程模型在连续时间事件数据上的预测准确性

    On the Predictive Accuracy of Neural Temporal Point Process Models for Continuous-time Event Data. (arXiv:2306.17066v1 [cs.LG])

    [http://arxiv.org/abs/2306.17066](http://arxiv.org/abs/2306.17066)

    该论文通过全面大规模实验研究，系统评估了最先进的神经TPP模型在预测准确性方面的效果，并发现了关键因素。

    

    时态点过程（TPP）是连续时间中建模异步事件序列的标准数学框架。然而，传统的TPP模型常常受限于强假设，限制了其对复杂真实世界事件动态的捕捉能力。为了克服这一限制，研究人员提出了神经TPP，利用神经网络参数化来提供更灵活和高效的建模。虽然最近的研究表明神经TPP的有效性，但它们通常缺乏统一的设置，依赖不同的基线、数据集和实验配置。这使得难以确定推动预测准确性改进的关键因素，阻碍了研究进展。为了弥补这一差距，我们提出了一项全面的大规模实验研究，系统评估了最先进的神经TPP模型的预测准确性。我们的研究涵盖了多个真实世界和合成事件序列数据集。

    Temporal Point Processes (TPPs) serve as the standard mathematical framework for modeling asynchronous event sequences in continuous time. However, classical TPP models are often constrained by strong assumptions, limiting their ability to capture complex real-world event dynamics. To overcome this limitation, researchers have proposed Neural TPPs, which leverage neural network parametrizations to offer more flexible and efficient modeling. While recent studies demonstrate the effectiveness of Neural TPPs, they often lack a unified setup, relying on different baselines, datasets, and experimental configurations. This makes it challenging to identify the key factors driving improvements in predictive accuracy, hindering research progress. To bridge this gap, we present a comprehensive large-scale experimental study that systematically evaluates the predictive accuracy of state-of-the-art neural TPP models. Our study encompasses multiple real-world and synthetic event sequence datasets, 
    
[^97]: OSP: 使用两阶段同步提升分布式模型训练

    OSP: Boosting Distributed Model Training with 2-stage Synchronization. (arXiv:2306.16926v1 [cs.DC])

    [http://arxiv.org/abs/2306.16926](http://arxiv.org/abs/2306.16926)

    OSP是一种新的分布式模型训练方法，通过使用两阶段同步和本地梯度修正来提高通信效率，避免了精度损失。

    

    分布式深度学习（DDL）是一个有前景的研究领域，旨在提高训练大规模数据集和模型的深度学习任务的效率。随着DDL节点的计算能力不断增强，节点之间的网络连接成为一个主要瓶颈。为了解决参数服务器式DDL中这个瓶颈问题，人们提出了各种梯度压缩和改进的模型同步方法。然而，这两种方法可能会导致丢失梯度而出现精度损失，并且对模型同步的吞吐量的提升有限。为了解决这些挑战，我们提出了一种新的模型同步方法，名为Overlapped Synchronization Parallel（OSP），它采用两阶段同步方法实现高效通信，并使用基于本地梯度的参数修正（LGP）来避免由过期参数引起的精度损失。OSP的原型使用PyTo实现。

    Distributed deep learning (DDL) is a promising research area, which aims to increase the efficiency of training deep learning tasks with large size of datasets and models. As the computation capability of DDL nodes continues to increase, the network connection between nodes is becoming a major bottleneck. Various methods of gradient compression and improved model synchronization have been proposed to address this bottleneck in Parameter-Server-based DDL. However, these two types of methods can result in accuracy loss due to discarded gradients and have limited enhancement on the throughput of model synchronization, respectively. To address these challenges, we propose a new model synchronization method named Overlapped Synchronization Parallel (OSP), which achieves efficient communication with a 2-stage synchronization approach and uses Local-Gradient-based Parameter correction (LGP) to avoid accuracy loss caused by stale parameters. The prototype of OSP has been implemented using PyTo
    
[^98]: 数值数据填补的多模态数据集:一种概率最近邻核密度方法

    Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor Kernel Density Approach. (arXiv:2306.16906v1 [stat.ML])

    [http://arxiv.org/abs/2306.16906](http://arxiv.org/abs/2306.16906)

    本论文介绍了一种新的数值数据填补方法，通过将最近邻估计和高斯核密度估计结合，能够有效处理多模态数据集中的缺失值，并提供比当前方法更高的概率估计。

    

    数值数据填补方法通过估计替换缺失的值以利用不完整的数据集。当前的填补方法试图最小化未观察到的真实值和填补值之间的误差。但是，在多模态或复杂分布存在的情况下，这种策略可能会产生伪像，导致填补效果较差。为了解决这个问题，我们引入了$k$NN$\times$KDE算法: 一种将最近邻估计($k$NN)和使用高斯核进行密度估计(KDE)结合的数据填补方法。我们使用人工和真实数据进行了与之前数据填补方法的比较，涉及了不同的数据缺失情况和不同的数据缺失率，并且展示了我们的方法可以处理复杂的原始数据结构，产生更低的数据填补误差，并提供比当前方法更高的概率估计。我们将代码以开源形式发布给社区：https://github.com/DeltaFloflo/knnxkde

    Numerical data imputation algorithms replace missing values by estimates to leverage incomplete data sets. Current imputation methods seek to minimize the error between the unobserved ground truth and the imputed values. But this strategy can create artifacts leading to poor imputation in the presence of multimodal or complex distributions. To tackle this problem, we introduce the $k$NN$\times$KDE algorithm: a data imputation method combining nearest neighbor estimation ($k$NN) and density estimation with Gaussian kernels (KDE). We compare our method with previous data imputation methods using artificial and real-world data with different data missing scenarios and various data missing rates, and show that our method can cope with complex original data structure, yields lower data imputation errors, and provides probabilistic estimates with higher likelihood than current methods. We release the code in open-source for the community: https://github.com/DeltaFloflo/knnxkde
    
[^99]: DragDiffusion: 利用扩散模型进行交互式点基图像编辑

    DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing. (arXiv:2306.14435v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.14435](http://arxiv.org/abs/2306.14435)

    DragDiffusion是一个利用扩散模型进行交互式点基图像编辑的方法，通过优化扩散潜在实现精确的空间控制，以提高实际场景中的应用性。

    

    精确可控的图像编辑是一个具有挑战性的任务，引起了广泛的关注。最近，DragGAN实现了一个交互式的基于点的图像编辑框架，并以像素级精度实现了令人印象深刻的编辑结果。然而，由于该方法基于生成对抗网络（GAN），其通用性受限于预先训练的GAN模型的容量。在这项工作中，我们将这样的编辑框架扩展到扩散模型，并提出了DragDiffusion。通过利用大规模预训练的扩散模型，我们极大地提高了交互式基于点的编辑在实际场景中的适用性。虽然大多数现有的基于扩散的图像编辑方法基于文本嵌入，DragDiffusion优化扩散潜在来实现精确的空间控制。尽管扩散模型以迭代方式生成图像，但我们凭经验表明，在一个单独的步骤中优化扩散潜在已足以生成连贯的结果，从而使得该方法成为可能。

    Precise and controllable image editing is a challenging task that has attracted significant attention. Recently, DragGAN enables an interactive point-based image editing framework and achieves impressive editing results with pixel-level precision. However, since this method is based on generative adversarial networks (GAN), its generality is upper-bounded by the capacity of the pre-trained GAN models. In this work, we extend such an editing framework to diffusion models and propose DragDiffusion. By leveraging large-scale pretrained diffusion models, we greatly improve the applicability of interactive point-based editing in real world scenarios. While most existing diffusion-based image editing methods work on text embeddings, DragDiffusion optimizes the diffusion latent to achieve precise spatial control. Although diffusion models generate images in an iterative manner, we empirically show that optimizing diffusion latent at one single step suffices to generate coherent results, enabl
    
[^100]: 稀疏模块激活用于高效的序列建模

    Sparse Modular Activation for Efficient Sequence Modeling. (arXiv:2306.11197v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11197](http://arxiv.org/abs/2306.11197)

    本论文引入了稀疏模块激活 (SMA) 机制，用于高效的序列建模。这种机制可以动态地稀疏激活序列元素的子模块，减少计算和内存消耗。

    

    线性状态空间模型 (SSM) 在各种序列建模任务中表现出了很强的性能，因为它们有效地编码了循环结构。然而，在更综合的任务中，如语言建模和机器翻译中，基于自注意力的模型仍然优于SSM。同时使用SSM和自注意力的混合模型通常显示出有希望的性能，但当前方法将注意力模块静态且均匀地应用于输入序列中的所有元素，导致了质量和效率之间的次优权衡。在这项工作中，我们引入了稀疏模块激活 (SMA)，这是一种通用机制，使神经网络能够以可微分的方式稀疏地动态激活序列元素的子模块。通过允许每个元素跳过非激活的子模块，SMA可以在序列建模的训练和推理阶段降低计算和内存消耗。作为SMA的一个特定实例，我们设计了一种新颖的神经网络模型。

    Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural a
    
[^101]: 基于现实表演的面部动画风格感知非监督学习

    Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances. (arXiv:2306.10006v1 [cs.CV])

    [http://arxiv.org/abs/2306.10006](http://arxiv.org/abs/2306.10006)

    本文提出了一种非监督学习的动画方法，可以通过文本或语音输入，实现基于真实动作表演的面部动画，并且可以不同程度地学习并合成不同的表演风格。

    

    本文提出了一种新的方法，基于混合形状几何、动态纹理和神经渲染，用于从真实动作表演中驱动面部模型的文本/语音动画。通过训练包括形状和纹理的VAE，我们得到了一个参数化模型，以精确捕捉和逼真合成潜在特征向量中的面部表情。我们的动画方法基于条件卷积神经网络，将文本或语音转换为一系列动画参数。与以往的方法不同，我们的动画模型以非监督的方式学习区分和合成不同的表演风格，只需要用于描述训练序列内容的语音标签。为了实现逼真的实时渲染，我们训练了一个U-Net，通过计算改进的像素颜色和前景遮罩来改善栅格化渲染。我们定性/定量地将我们的框架与最近的头部建模方法以及面部动画方法进行比较，并评估感知渲染/动画效果。

    This paper presents a novel approach for text/speech-driven animation of a photo-realistic head model based on blend-shape geometry, dynamic textures, and neural rendering. Training a VAE for geometry and texture yields a parametric model for accurate capturing and realistic synthesis of facial expressions from a latent feature vector. Our animation method is based on a conditional CNN that transforms text or speech into a sequence of animation parameters. In contrast to previous approaches, our animation model learns disentangling/synthesizing different acting-styles in an unsupervised manner, requiring only phonetic labels that describe the content of training sequences. For realistic real-time rendering, we train a U-Net that refines rasterization-based renderings by computing improved pixel colors and a foreground matte. We compare our framework qualitatively/quantitatively against recent methods for head modeling as well as facial animation and evaluate the perceived rendering/ani
    
[^102]: Diff-TTSG: 去噪概率集成语音和手势合成

    Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis. (arXiv:2306.09417v1 [eess.AS])

    [http://arxiv.org/abs/2306.09417](http://arxiv.org/abs/2306.09417)

    Diff-TTSG是第一个基于扩散的概率模型，用于联合学习合成语音和手势，相比于先前最新技术的非概率方法，它可以更好地捕捉人类讲话和运动的变化，产生更逼真和多样化的集成语音和手势合成。

    

    随着朗读语音合成实现高自然度评分，越来越多的研究开始关注合成自然言语。然而，人类面对面的自发对话既有口头的，也有非语言的（例如，共同言语手势）。最近才开始研究联合合成这两种模态在一个单一的系统中的好处。先前的最新技术使用非概率方法，无法捕捉人类讲话和运动的变化，并可能产生过度平滑的伪影和次优的合成质量。我们提出了第一个基于扩散的概率模型，称为 Diff-TTSG，共同学习合成语音和手势。我们的方法可以从头开始使用小型数据集进行训练。此外，我们描述了一组小心的单模态和多模态主观测试，用于评估集成语音和手势合成系统，并用它们来验证我们提出的方法。对于合成的样例而言，Diff-TTSG优于先前的最新技术，产生更逼真和多样化的集成语音和手势合成。

    With read-aloud speech synthesis achieving high naturalness scores, there is a growing research interest in synthesising spontaneous speech. However, human spontaneous face-to-face conversation has both spoken and non-verbal aspects (here, co-speech gestures). Only recently has research begun to explore the benefits of jointly synthesising these two modalities in a single system. The previous state of the art used non-probabilistic methods, which fail to capture the variability of human speech and motion, and risk producing oversmoothing artefacts and sub-optimal synthesis quality. We present the first diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to synthesise speech and gestures together. Our method can be trained on small datasets from scratch. Furthermore, we describe a set of careful uni- and multi-modal subjective tests for evaluating integrated speech and gesture synthesis systems, and use them to validate our proposed approach. For synthesised examp
    
[^103]: 一种基于对比学习方法的代理市场订单表示

    Agent market orders representation through a contrastive learning approach. (arXiv:2306.05987v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.05987](http://arxiv.org/abs/2306.05987)

    通过对比学习方法，本研究构建了一个自监督学习模型，用于学习代理市场订单的表示。进一步地，我们使用K均值聚类算法对代理订单的学习表示向量进行聚类，以确定每个簇中的不同行为类型。

    

    本研究通过访问Euronext的CAC40数据中的标记订单，分析代理在市场中根据其下达的订单的行为。本研究构建了一个自监督学习模型，使用三元组损失来有效地学习代理市场订单的表示。通过获取这个学习表示，各种下游任务变得可行。本研究使用K均值聚类算法对代理订单的学习表示向量进行聚类，以确定每个簇中的不同行为类型。

    Due to the access to the labeled orders on the CAC40 data from Euronext, we are able to analyse agents' behaviours in the market based on their placed orders. In this study, we construct a self-supervised learning model using triplet loss to effectively learn the representation of agent market orders. By acquiring this learned representation, various downstream tasks become feasible. In this work, we utilise the K-means clustering algorithm on the learned representation vectors of agent orders to identify distinct behaviour types within each cluster.
    
[^104]: 一种基于深度贝叶斯粒子流框架的跨领域软测量无监督建模方法

    Unsupervised Cross-Domain Soft Sensor Modelling via A Deep Bayesian Particle Flow Framework. (arXiv:2306.04919v1 [cs.LG])

    [http://arxiv.org/abs/2306.04919](http://arxiv.org/abs/2306.04919)

    该论文提出了一种基于深度粒子流贝叶斯框架的跨领域软测量无监督建模方法，能够解决标签缺失、领域适应性和数据时间一致性等问题，提高软测量建模的准确性。

    

    数据驱动的软测量对于通过可靠的状态推断实现精确感知至关重要。然而，由于存在标签缺失、领域适应性和数据时间一致性等问题，开发具有代表性的软测量模型具有挑战性。为了解决这些问题，我们提出了一种基于深度粒子流贝叶斯 (DPFB) 框架，用于在无目标状态标签情况下进行跨领域软测量建模。具体来说，首先制定了一个顺序贝叶斯目标，以执行潜在的跨领域软感知问题的最大似然估计。在框架核心，我们结合物理学启发的粒子流，通过优化顺序贝叶斯目标来执行模型提取的潜在和隐藏特征的精确贝叶斯更新。由此，这些贡献使得该框架能够学习一个有机的近似后验特征表示，能够表征复杂的跨领域系统动力学并实现有效的软测量建模。

    Data-driven soft sensors are essential for achieving accurate perception through reliable state inference. However, developing representative soft sensor models is challenged by issues such as missing labels, domain adaptability, and temporal coherence in data. To address these challenges, we propose a deep Particle Flow Bayes (DPFB) framework for cross-domain soft sensor modeling in the absence of target state labels. In particular, a sequential Bayes objective is first formulated to perform the maximum likelihood estimation underlying the cross-domain soft sensing problem. At the core of the framework, we incorporate a physics-inspired particle flow that optimizes the sequential Bayes objective to perform an exact Bayes update of the model extracted latent and hidden features. As a result, these contributions enable the proposed framework to learn a cohesive approximate posterior feature representation capable of characterizing complex cross-domain system dynamics and performing effe
    
[^105]: 结构化数据生成扩散模型综述

    A Survey on Generative Diffusion Models for Structured Data. (arXiv:2306.04139v1 [cs.LG])

    [http://arxiv.org/abs/2306.04139](http://arxiv.org/abs/2306.04139)

    本文全面综述了在结构化数据领域中最近提出的扩散模型，介绍了其理论基础和应用场景。

    

    最近，生成扩散模型（generative diffusion models）在深度生成模型领域取得了突破性成果，展现了在许多应用中的出色表现。与此同时，结构化数据（包括表格和时间序列数据）在深度学习研究界中受到的关注相对较少，尽管其无处不在且应用广泛。因此，与计算机视觉和自然语言处理等其他数据形式相比，利用扩散模型对结构化数据建模的文献及其综述仍然缺乏。因此，本文介绍了在结构化数据领域中最近提出的扩散模型的全面综述。首先，本综述提供了基于得分的扩散模型理论的简要概述，随后又详细描述了在数据驱动的通用任务和特定领域应用中使用结构化数据的大部分开创性工作的技术描述。最后，

    In recent years, generative diffusion models have achieved a rapid paradigm shift in deep generative models by showing groundbreaking performance across various applications. Meanwhile, structured data, encompassing tabular and time series data, has been received comparatively limited attention from the deep learning research community, despite its omnipresence and extensive applications. Thus, there is still a lack of literature and its review on structured data modelling via diffusion models, compared to other data modalities such as computer vision and natural language processing. Hence, in this paper, we present a comprehensive review of recently proposed diffusion models in the field of structured data. First, this survey provides a concise overview of the score-based diffusion model theory, subsequently proceeding to the technical descriptions of the majority of pioneering works using structured data in both data-driven general tasks and domain-specific applications. Thereafter, 
    
[^106]: 带有分层变分自编码器的情感条件旋律和声编配研究

    Emotion-Conditioned Melody Harmonization with Hierarchical Variational Autoencoder. (arXiv:2306.03718v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2306.03718](http://arxiv.org/abs/2306.03718)

    提出了一种基于LSTM的分层变分自编码器(LHVAE)，研究情感条件对旋律和声编配的影响，同时提高了生成的和声质量并捕捉了丰富的和弦进行变化。实验证明，该模型在感知情感表达方面优于现有方法。

    

    现有的旋律和声编配模型在提高生成的和声质量方面取得了很大进展，但大多数忽略了音乐中的情感。同时，以前的方法所生成的和声变化性不足。为了解决这些问题，我们提出了一种新颖的基于LSTM的分层变分自编码器(LHVAE)，研究情感条件对旋律和声编配的影响，同时提高了生成的和声质量并捕捉了丰富的和弦进行变化。具体来说，LHVAE在不同层次（乐章和小节级别）上融合了潜在变量和情感条件，以建模全局和局部音乐属性。另外，我们在每个步骤中引入了基于注意力的旋律上下文向量，以更好地学习旋律和和声之间的对应关系。客观评估的实验结果表明，我们提出的模型胜过了其他基于LSTM的模型。通过主观评估，我们证明了我们的模型可以生成符合给定情感的和声进行，并在感知情感表达方面优于现有方法。

    Existing melody harmonization models have made great progress in improving the quality of generated harmonies, but most of them ignored the emotions beneath the music. Meanwhile, the variability of harmonies generated by previous methods is insufficient. To solve these problems, we propose a novel LSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the influence of emotional conditions on melody harmonization, while improving the quality of generated harmonies and capturing the abundant variability of chord progressions. Specifically, LHVAE incorporates latent variables and emotional conditions at different levels (piece- and bar-level) to model the global and local music properties. Additionally, we introduce an attention-based melody context vector at each step to better learn the correspondence between melodies and harmonies. Experimental results of the objective evaluation show that our proposed model outperforms other LSTM-based models. Through subjective evalu
    
[^107]: 分散化SGD和平均方向SAM在渐近意义下是等价的

    Decentralized SGD and Average-direction SAM are Asymptotically Equivalent. (arXiv:2306.02913v1 [cs.LG])

    [http://arxiv.org/abs/2306.02913](http://arxiv.org/abs/2306.02913)

    分散SGD和平均方向SAM在渐近意义下是等价的，D-SGD表现出梯度平滑效应和锐度正则化效应，而且可以提高后验评估，并证明了潜在的泛化能力

    

    分散随机梯度下降（D-SGD）允许在没有中央服务器的控制下，大量设备同时进行协作学习。然而，现有理论认为，分散化不可避免地削弱了泛化能力。本文挑战传统信念，提出了完全新的角度来理解分散学习。我们证明了在一般非凸非-$\beta$-平滑设置下，D-SGD隐式地最小化了平均方向锐度感知最小化（SAM）算法的损失函数。这种惊人的渐近等价揭示了内在的正则化-优化权衡以及分散化的三个优点：（1）D-SGD中存在一个自由的不确定性评估机制，可以提高后验估计；（2）D-SGD表现出梯度平滑效应；（3）D-SGD的锐度正则化效应不会随着总批处理大小的增加而减少，这证明了潜在的泛化能力

    Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive devices simultaneously without the control of a central server. However, existing theories claim that decentralization invariably undermines generalization. In this paper, we challenge the conventional belief and present a completely new perspective for understanding decentralized learning. We prove that D-SGD implicitly minimizes the loss function of an average-direction Sharpness-aware minimization (SAM) algorithm under general non-convex non-$\beta$-smooth settings. This surprising asymptotic equivalence reveals an intrinsic regularization-optimization trade-off and three advantages of decentralization: (1) there exists a free uncertainty evaluation mechanism in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient smoothing effect; and (3) the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the potential generalization b
    
[^108]: 高风险领域对于前文解释性吸引力的（不）合理吸引力：透明度对于可理解性来说是必要但不足够的。

    (Un)reasonable Allure of Ante-hoc Interpretability for High-stakes Domains: Transparency Is Necessary but Insufficient for Comprehensibility. (arXiv:2306.02312v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02312](http://arxiv.org/abs/2306.02312)

    高风险领域对于前文解释性的追求是重要的，但该概念自身含义模糊不清，取决于操作环境和观察者判断。透明的预测模型可能仍需后续处理才能提供合适的解释性洞见，因此传统的前文解释性概念仍需要进一步明确和探索。

    

    前文解释性已成为可解释的人工智能在高风险领域（如医疗保健）中的追求，然而，这个概念很难捉摸，缺乏广泛接受的定义，并且取决于操作环境。它可以指的是那些结构符合特定领域约束的预测模型，或者是本质上透明的模型。后一种观念假设观察者对此质量进行判断，而前一种观念则假设他们具有技术和领域专业知识（从而使其他解释对象群体感到陌生）。此外，前文解释性与较不理想的后文可解释性之间的区别模糊，后者是指构建一个单独的解释模型的方法，而透明的预测模型仍可能需要（后续）处理才能产生适当的解释性洞见。因此，前文解释性是一个超负荷的概念，它包含一系列隐含的属性，我们将在本文中详细阐述。

    Ante-hoc interpretability has become the holy grail of explainable artificial intelligence for high-stakes domains such as healthcare; however, this notion is elusive, lacks a widely-accepted definition and depends on the operational context. It can refer to predictive models whose structure adheres to domain-specific constraints, or ones that are inherently transparent. The latter conceptualisation assumes observers who judge this quality, whereas the former presupposes them to have technical and domain expertise (thus alienating other groups of explainees). Additionally, the distinction between ante-hoc interpretability and the less desirable post-hoc explainability, which refers to methods that construct a separate explanatory model, is vague given that transparent predictive models may still require (post-)processing to yield suitable explanatory insights. Ante-hoc interpretability is thus an overloaded concept that comprises a range of implicit properties, which we unpack in this 
    
[^109]: 在对话情感识别中使用监督式对抗性对比学习

    Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations. (arXiv:2306.01505v1 [cs.CL])

    [http://arxiv.org/abs/2306.01505](http://arxiv.org/abs/2306.01505)

    本文提出了一种监督式对抗性对比学习（SACL）框架，用于学习类别分布结构表示，通过联合类别分布对比学习目标，有效利用标签级特性一致性并保留细粒度的类内特性，实现了在对话情感识别中最先进的结果。

    

    情感识别在对话中是提取泛化和稳健表示的一个重要挑战。为了解决这个问题，本文提出了一种监督式对抗性对比学习（SACL）框架，用于学习类别分布结构表示。该框架应用于对比感知对抗性训练以生成最坏情况的样本，并在原始和对抗样本上使用联合类别分布对比学习目标。它可以有效地利用标签级特性一致性并保留细粒度的类内特性。为了避免对上下文相关数据产生负面影响，我们设计了一个上下文对抗性训练策略，从上下文中学习更多不同的特征，并增强模型对上下文的容错性。在该框架下，我们开发了一个基于序列的方法SACL-LSTM，用于学习针对ERC的标签一致和上下文稳健的情感特征。在三个数据集上的实验证明，SACL-LSTM在对话情感识别方面实现了最先进的结果，优于现有的方法。

    Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations. The framework applies contrast-aware adversarial training to generate worst-case samples and uses a joint class-spread contrastive learning objective on both original and adversarial samples. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training strategy to learn more diverse features from context and enhance the model's context robustness. We develop a sequence-based method SACL-LSTM under this framework, to learn label-consistent and context-robust emotional features for ERC. Experiments on three datasets demonstrate that SACL-LSTM achieves state-of
    
[^110]: 具有因果亚结构的分子关系学习模型在数据分布变化时的鲁棒性

    Shift-Robust Molecular Relational Learning with Causal Substructure. (arXiv:2305.18451v1 [cs.LG])

    [http://arxiv.org/abs/2305.18451](http://arxiv.org/abs/2305.18451)

    本文提出了一种鲁棒性强的分子关系学习模型CMRL，通过检测与化学反应因果相关的核心亚结构来应对分子关系学习中的数据分布变化。

    

    最近，分子关系学习引起了分子科学领域的广泛关注，其目标是预测分子对之间的相互作用行为。本文提出了一种鲁棒性强的分子关系学习模型CMRL，它通过检测与化学反应因果相关的核心亚结构来应对分子关系学习中的数据分布变化。为此，我们首先假定基于分子科学领域知识的因果关系，并构建结构因果模型（SCM）来揭示变量之间的关系。基于SCM，我们引入了一个新的条件干预框架，其干预是基于成对分子条件的。使用条件干预框架，我们的模型成功地从因果亚结构中学习，并减轻了与化学反应虚假相关的快捷亚结构的混淆效应。本文在各种任务和真实和合成数据集上进行了广泛实验。

    Recently, molecular relational learning, whose goal is to predict the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. In this work, we propose CMRL that is robust to the distributional shift in molecular relational learning by detecting the core substructure that is causally related to chemical reactions. To do so, we first assume a causal relationship based on the domain knowledge of molecular sciences and construct a structural causal model (SCM) that reveals the relationship between variables. Based on the SCM, we introduce a novel conditional intervention framework whose intervention is conditioned on the paired molecule. With the conditional intervention framework, our model successfully learns from the causal substructure and alleviates the confounding effect of shortcut substructures that are spuriously correlated to chemical reactions. Extensive experiments on various tasks with real-world and sy
    
[^111]: 基于大语言模型的多项选择题答案确认预测研究

    Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v1 [cs.CL])

    [http://arxiv.org/abs/2305.18404](http://arxiv.org/abs/2305.18404)

    本研究探讨了如何利用符合性预测技术，在多项选择题回答任务中为语言模型提供不确定性量化。我们发现符合性预测的不确定性估计与预测准确性密切相关。

    

    随着大型语言模型的广泛开发，对它们进行健壮的不确定性量化技术将成为它们在高风险场景下安全部署的关键。本研究探讨了如何利用符合性预测技术，在多项选择题回答任务中为语言模型提供不确定性量化。我们发现符合性预测的不确定性估计与预测准确性密切相关。这种观察对于下游应用，如选择性分类和过滤低质量预测，可能会有用。我们还研究了符合性预测对于超出主题的问题的交换性假设，这可能是许多实际应用的更为现实的场景。本研究为在需要可靠保证错误率的安全关键情况下更加值得信赖和可靠地使用大型语言模型做出了贡献。

    As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.
    
[^112]: 《大脑肿瘤分割（BraTS）挑战2023：关注儿科（CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs）》

    The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs). (arXiv:2305.17033v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.17033](http://arxiv.org/abs/2305.17033)

    这个论文介绍了CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023挑战，该挑战是首个专注于儿童脑肿瘤的BraTS挑战，旨在评估儿童脑胶质瘤的体积分割算法的发展。儿童中枢神经系统肿瘤是儿童癌症相关死亡的主要原因，并且对这些实体的诊断和治疗存在一些挑战。

    

    儿童中枢神经系统肿瘤是儿童癌症相关死亡的最常见原因。儿童高级别胶质瘤的五年生存率不到20％。由于罕见，对这些实体的诊断通常会延迟，其治疗主要基于历史治疗理念，并且临床试验需要多机构合作。MICCAI大脑肿瘤分割（BraTS）挑战是一个里程碑式的社区基准事件，已经成功创建资源12年，用于成人胶质瘤的分割和分析。在这里，我们介绍了CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023挑战，这是首个专注于儿童脑肿瘤的BraTS挑战，其中包括多个国际合作组织专注于儿科神经肿瘤和临床试验的数据。BraTS-PEDs 2023挑战侧重于评估用于儿童脑胶质瘤的体积分割算法的发展。

    Pediatric tumors of the central nervous system are the most common cause of cancer-related death in children. The five-year survival rate for high-grade gliomas in children is less than 20\%. Due to their rarity, the diagnosis of these entities is often delayed, their treatment is mainly based on historic treatment concepts, and clinical trials require multi-institutional collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a landmark community benchmark event with a successful history of 12 years of resource creation for the segmentation and analysis of adult glioma. Here we present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which represents the first BraTS challenge focused on pediatric brain tumors with data acquired across multiple international consortia dedicated to pediatric neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on benchmarking the development of volumentric segmentation algorithms for pediatric brain gli
    
[^113]: 在脉冲神经网络中将噪声作为计算和学习资源

    Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2305.16044](http://arxiv.org/abs/2305.16044)

    本文提出了噪声脉冲神经元网络（NSNN）和噪声驱动学习规则（NDL），展示了噪声可以作为计算和学习的资源，并为一般脉冲神经元网络提供了一个框架。研究还展示了NSNNs在图像分类和语音识别等实际任务中的适用性，表明它们是未来神经形态计算系统的潜在有力工具。

    

    脉冲神经元网络是大脑非凡信息处理能力的基础，并已成为神经形态智能的支柱模型。本文介绍了噪声脉冲神经元网络（NSNN）和噪声驱动学习规则（NDL），采用带有噪声神经元动力学的脉冲神经元模型。该方法显示噪声可以作为计算和学习的资源，并理论上为一般脉冲神经元网络提供了一个框架。此外，NDL为代理梯度提供了深入的生物学合理性。通过将各种SNN架构和算法结合起来，我们展示了我们的方法表现出竞争性能，并且比确定性SNNs表现出更好的鲁棒性。此外，本文还展示了NSNNs在图像分类和语音识别等实际任务中的适用性，表明它们是未来神经形态计算系统的潜在有力工具。

    Networks of spiking neurons underpin the extraordinary information-processing capabilities of the brain and have emerged as pillar models in neuromorphic intelligence. Despite extensive research on spiking neural networks (SNNs), most are established on deterministic models. Integrating noise into SNNs leads to biophysically more realistic neural dynamics and may benefit model performance. This work presents the noisy spiking neural network (NSNN) and the noise-driven learning rule (NDL) by introducing a spiking neuron model incorporating noisy neuronal dynamics. Our approach shows how noise may act as a resource for computation and learning and theoretically provides a framework for general SNNs. Moreover, NDL provides an insightful biological rationale for surrogate gradients. By incorporating various SNN architectures and algorithms, we show that our approach exhibits competitive performance and improved robustness against challenging perturbations than deterministic SNNs. Additiona
    
[^114]: 高效大规模的视觉表示学习

    Efficient Large-Scale Vision Representation Learning. (arXiv:2305.13399v1 [cs.CV])

    [http://arxiv.org/abs/2305.13399](http://arxiv.org/abs/2305.13399)

    本文介绍了一种高效大规模的单模态视觉表示学习方法，解决了电商应用中视觉表示的挑战，并提供了消融研究和评估。

    

    本文介绍了我们的单模态视觉表示学习方法，了解产品内容的视觉表示对电商推荐、搜索和广告应用至关重要。我们详细介绍和对比了在低资源环境下有效微调大规模视觉表示学习模型的技术，包括多种预训练的骨干架构，包括卷积神经网络和视觉转换器系列。我们强调了电子商务应用在大规模情况下的挑战，并突出了更有效地训练、评估和提供视觉表示的努力。我们为几个下游任务提供了消融研究，包括我们的视觉相似广告推荐。我们评估了所得视觉表示在下游任务中的离线性能。为此，我们提出了一种新的文本到图像生成的离线评估方法，用于视觉相似推荐。

    In this article, we present our approach to single-modality vision representation learning. Understanding vision representations of product content is vital for recommendations, search, and advertising applications in e-commerce. We detail and contrast techniques used to fine tune large-scale vision representation learning models in an efficient manner under low-resource settings, including several pretrained backbone architectures, both in the convolutional neural network as well as the vision transformer family. We highlight the challenges for e-commerce applications at-scale and highlight the efforts to more efficiently train, evaluate, and serve visual representations. We present ablation studies for several downstream tasks, including our visually similar ad recommendations. We evaluate the offline performance of the derived visual representations in downstream tasks. To this end, we present a novel text-to-image generative offline evaluation method for visually similar recommenda
    
[^115]: 合成数据，真实误差：如何（不）发布和使用合成数据

    Synthetic data, real errors: how (not) to publish and use synthetic data. (arXiv:2305.09235v1 [cs.LG])

    [http://arxiv.org/abs/2305.09235](http://arxiv.org/abs/2305.09235)

    合成数据在机器学习领域受到了越来越多的关注，但是不完美的合成数据可能会导致下游机器学习任务中的潜在错误。为了改进这种情况，研究人员引入了深度生成集成（DGE）框架来近似生成过程模型参数的后验分布，以提高下游模型的训练和评估效果。

    

    通过生成模型生成合成数据在机器学习社区和其他领域越来越受到关注，这种方法承诺将来可以根据个体需求定制数据集。不幸的是，合成数据通常并不完美，可能导致下游任务中的潜在错误。在本文中，我们探讨了生成过程对下游机器学习任务的影响。我们展示了单纯的合成数据方法——将合成数据视为真实数据使用——会导致下游模型和分析无法很好地推广到真实数据。作为合成数据环境下更好的机器学习的第一步，我们引入了深度生成集成（DGE）——受到深度集成启发的框架，旨在隐式地近似生成过程模型参数的后验分布。DGE改善了下游模型的训练、评估和不确定性量化，平均而言远远优于单纯的方法。对于少数类和低密度区域，最大的改进效果得到了实现。

    Generating synthetic data through generative models is gaining interest in the ML community and beyond, promising a future where datasets can be tailored to individual needs. Unfortunately, synthetic data is usually not perfect, resulting in potential errors in downstream tasks. In this work we explore how the generative process affects the downstream ML task. We show that the naive synthetic data approach -- using synthetic data as if it is real -- leads to downstream models and analyses that do not generalize well to real data. As a first step towards better ML in the synthetic data regime, we introduce Deep Generative Ensemble (DGE) -- a framework inspired by Deep Ensembles that aims to implicitly approximate the posterior distribution over the generative process model parameters. DGE improves downstream model training, evaluation, and uncertainty quantification, vastly outperforming the naive approach on average. The largest improvements are achieved for minority classes and low-de
    
[^116]: SKI加速Toeplitz神经网络：通过非对称核实现加速

    SKI to go Faster: Accelerating Toeplitz Neural Networks via Asymmetric Kernels. (arXiv:2305.09028v1 [stat.ML])

    [http://arxiv.org/abs/2305.09028](http://arxiv.org/abs/2305.09028)

    本论文提出使用非对称核（asymmetric kernels）实现Toeplitz神经网络（TNNs）的加速，通过稀疏加低秩Toeplitz矩阵分解、小型1D卷积和替换相对位置编码器（RPE）多层感知器（MLP）实现O（n）复杂度，针对因果模型，提出了“快速”因果屏蔽来抵消这种方法的限制。

    

    Toeplitz神经网络（TNNs）是最近出现并取得令人印象深刻结果的序列模型。它们需要O(n log n)的计算复杂度和O(n)的相对位置编码器（RPE）多层感知器（MLP）和衰减偏差调用。我们的目标是减少它们。我们首先指出，RPE是一个非对称正定核，而Toeplitz矩阵是伪格拉姆矩阵。此外：1）学习的核在主对角线附近显示出刺状行为，而在其他位置则表现出平滑行为；2）RPE MLP较慢。对于双向模型，这促使我们进行稀疏加低秩Toeplitz矩阵分解。对于稀疏组件的操作，我们进行小型1D卷积。对于低秩组件，我们将RPE MLP替换为线性插值，并使用非对称有结构的内核插值（SKI）（Wilson等，2015）以实现O（n）复杂度：我们提供了严格的误差分析。对于因果模型，“快速”因果屏蔽（Katharopoulos等，2020）抵消了SKI的好处。

    Toeplitz Neural Networks (TNNs) (Qin et. al. 2023) are a recent sequence model with impressive results. They require O(n log n) computational complexity and O(n) relative positional encoder (RPE) multi-layer perceptron (MLP) and decay bias calls. We aim to reduce both. We first note that the RPE is a non-SPD (symmetric positive definite) kernel and the Toeplitz matrices are pseudo-Gram matrices. Further 1) the learned kernels display spiky behavior near the main diagonals with otherwise smooth behavior; 2) the RPE MLP is slow. For bidirectional models, this motivates a sparse plus low-rank Toeplitz matrix decomposition. For the sparse component's action, we do a small 1D convolution. For the low rank component, we replace the RPE MLP with linear interpolation and use asymmetric Structured Kernel Interpolation (SKI) (Wilson et. al. 2015) for O(n) complexity: we provide rigorous error analysis. For causal models, "fast" causal masking (Katharopoulos et. al. 2020) negates SKI's benefits. 
    
[^117]: 如何为推荐基础模型索引项目ID

    How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])

    [http://arxiv.org/abs/2305.06569](http://arxiv.org/abs/2305.06569)

    本研究对推荐基础模型的项目索引问题进行了系统检查，提出了一种新的上下文感知索引方法，该方法在项目推荐准确性和文本生成质量方面具有优势。

    

    推荐基础模型将推荐任务转换为自然语言任务，利用大型语言模型（LLM）进行推荐。它通过直接生成建议的项目而不是计算传统推荐模型中每个候选项目的排名得分，简化了推荐管道，避免了多段过滤的问题。为了避免在决定要推荐哪些项目时生成过长的文本，为推荐基础模型创建LLM兼容的项目ID是必要的。本研究系统地研究了推荐基础模型的项目索引问题，以P5为代表的主干模型，并使用各种索引方法复制其结果。我们首先讨论了几种微不足道的项目索引方法（如独立索引、标题索引和随机索引）的问题，并表明它们不适用于推荐基础模型，然后提出了一种新的索引方法，称为上下文感知索引。我们表明，这种索引方法在项目推荐准确性和文本生成质量方面优于其他索引方法。

    Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
    
[^118]: 分子关系学习的条件图信息瓶颈

    Conditional Graph Information Bottleneck for Molecular Relational Learning. (arXiv:2305.01520v1 [q-bio.MN])

    [http://arxiv.org/abs/2305.01520](http://arxiv.org/abs/2305.01520)

    本文提出了一种新的关系学习框架，称为CGIB，通过检测其中的核心子图，预测对图对之间的相互作用行为。

    

    分子关系学习是指学习分子之间的相互作用行为，因其广泛应用于分子科学而引起极大兴趣。近年来，图神经网络通过将分子建模为图结构，并考虑两个分子之间的原子级相互作用，在分子关系学习中取得了巨大成功。然而，现有的分子关系学习方法往往忽略了化学的本质。即化合物由多个互相作用的子结构组成，这些子结构会引起独特的化学反应。本文提出了一种新的关系学习框架，称为CGIB，通过检测其中的核心子图，预测对图对之间的相互作用行为。其主要思想是，在给定一对图的情况下，从一个图中找到包含有关所需任务的最小充分信息的子图，以此来预测这对图之间的信息瓶颈。

    Molecular relational learning, whose goal is to learn the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. Recently, graph neural networks have recently shown great success in molecular relational learning by modeling a molecule as a graph structure, and considering atom-level interactions between two molecules. Despite their success, existing molecular relational learning methods tend to overlook the nature of chemistry, i.e., a chemical compound is composed of multiple substructures such as functional groups that cause distinctive chemical reactions. In this work, we propose a novel relational learning framework, called CGIB, that predicts the interaction behavior between a pair of graphs by detecting core subgraphs therein. The main idea is, given a pair of graphs, to find a subgraph from a graph that contains the minimal sufficient information regarding the task at hand conditioned on the paired graph
    
[^119]: 简化变分贝叶斯方法的推导过程

    Variational Bayes Made Easy. (arXiv:2304.14251v1 [cs.LG])

    [http://arxiv.org/abs/2304.14251](http://arxiv.org/abs/2304.14251)

    该论文提出了一个三步骤方法，简化了变分贝叶斯近似推断方法的推导过程。

    

    变分贝叶斯方法是一种流行的近似推断方法，但其推导过程可能很繁琐。为了简化这个过程，我们给出了一个三步骤的方法，通过显式寻找关于已知分布期望的线性性，来确定后验分布形式。然后我们可以直接通过“读取”这些期望前的项，写出更新。这个方法使得推导更加简单，快速，简短和通用。

    Variational Bayes is a popular method for approximate inference but its derivation can be cumbersome. To simplify the process, we give a 3-step recipe to identify the posterior form by explicitly looking for linearity with respect to expectations of well-known distributions. We can then directly write the update by simply ``reading-off'' the terms in front of those expectations. The recipe makes the derivation easier, faster, shorter, and more general.
    
[^120]: SREL：基于S参数模式的铜互连非破坏故障诊断的严重性评级集成学习

    SREL: Severity Rating Ensemble Learning for Non-Destructive Fault Diagnosis of Cu Interconnects using S-parameter Patterns. (arXiv:2304.10207v1 [cs.LG])

    [http://arxiv.org/abs/2304.10207](http://arxiv.org/abs/2304.10207)

    本研究利用S参数模式成功实现对Cu互连缺陷的非破坏性检测和诊断，在同时分析根本原因和严重性方面具有先进性，具备早期检测、高诊断准确度和噪声鲁棒性等优点。

    

    随着处理器的工作频率和时钟速度逐年提高，互连对整个电子系统的可靠性和性能都产生了影响。检测和诊断互连故障对电子健康管理至关重要。然而，利用电信号作为预测因子的现有研究存在局限性，例如无法区分缺陷的根本原因，最终需要进行额外的破坏性评估，并容易受到噪声的干扰而导致误警。本文实现了对Cu互连缺陷的非破坏性检测和诊断，实现了早期检测、高诊断准确度和噪声鲁棒性。据我们所知，本研究首次利用电信号模式同时分析根本原因和严重性。在本文中，我们实验性地展示了S参数模式具有故障诊断能力。

    As operating frequencies and clock speeds in processors have increased over the years, interconnects affect both the reliability and performance of entire electronic systems. Fault detection and diagnosis of the interconnects are crucial for prognostics and health management (PHM) of electronics. However, existing research works utilizing electrical signals as prognostic factors have limitations, such as the inability to distinguish the root cause of defects, which eventually requires additional destructive evaluation, and vulnerability to noise that results in a false alarm. Herein, we realize the non-destructive detection and diagnosis of defects in Cu interconnects, achieving early detection, high diagnostic accuracy, and noise robustness. To the best of our knowledge, this study first simultaneously analyzes the root cause and severity using electrical signal patterns. In this paper, we experimentally show that S-parameter patterns have the ability for fault diagnosis and they are 
    
[^121]: SimbaML：使用增强数据连接机械模型和机器学习模型。

    SimbaML: Connecting Mechanistic Models and Machine Learning with Augmented Data. (arXiv:2304.04000v1 [cs.LG])

    [http://arxiv.org/abs/2304.04000](http://arxiv.org/abs/2304.04000)

    SimbaML是一个机器学习工具，它通过机械模型补充真实世界的数据集，实现了从合成到真实数据的转移学习、数据增强、物理启发式机器学习方法的基准测试等多项功能。

    

    训练复杂的机器学习模型需要大量数据集，但对于许多应用程序来说，收集这些数据集往往是困难或昂贵的。如果有系统动态的先验知识，则可以使用机械模型来补充真实世界的数据。我们提出了SimbaML（基于仿真的机器学习），这是一个开源工具，它通过普通微分方程模型生成逼真的合成数据集，并直接将其用于机器学习管道的分析和包含。 SimbaML方便地启用了从合成到真实数据的转移学习，数据增强，确定对数据收集的需求以及基准测试物理启发式机器学习方法。SimbaML可在https://pypi.org/project/simba-ml/上获取。

    Training sophisticated machine learning (ML) models requires large datasets that are difficult or expensive to collect for many applications. If prior knowledge about system dynamics is available, mechanistic representations can be used to supplement real-world data. We present SimbaML (Simulation-Based ML), an open-source tool that unifies realistic synthetic dataset generation from ordinary differential equation-based models and the direct analysis and inclusion in ML pipelines. SimbaML conveniently enables investigating transfer learning from synthetic to real-world data, data augmentation, identifying needs for data collection, and benchmarking physics-informed ML approaches. SimbaML is available from https://pypi.org/project/simba-ml/.
    
[^122]: 图结构支持的跨领域知识转移

    Graph Enabled Cross-Domain Knowledge Transfer. (arXiv:2304.03452v1 [cs.LG])

    [http://arxiv.org/abs/2304.03452](http://arxiv.org/abs/2304.03452)

    稀缺知识对自动化决策造成了障碍，跨领域知识转移是通过融合来自不同领域的辅助信息，缓解不同领域知识差距的一种方法。

    

    为了将机器学习应用于任何决策过程中，必须将给定的知识（例如自然语言，非结构化文本）转化为可以被其兼容语言和数据格式的机器学习模型理解和处理的表示向量。然而，经常遇到的困难是，首先给定的知识并不充分或可靠。在这种情况下，人们会寻求融合来自单独领域的辅助信息来缓解好的表示学习和感兴趣领域的稀缺知识之间的差距。这种方法被称为跨领域知识转移。研究这个问题是至关重要的，因为在许多情况下，从在线医疗平台分析到金融市场风险量化，都存在稀缺知识的共性，这为我们从自动化决策中受益留下了障碍。从机器学习的角度来看，半监督学习范式利用了这种跨领域转移的思想。

    To leverage machine learning in any decision-making process, one must convert the given knowledge (for example, natural language, unstructured text) into representation vectors that can be understood and processed by machine learning model in their compatible language and data format. The frequently encountered difficulty is, however, the given knowledge is not rich or reliable enough in the first place. In such cases, one seeks to fuse side information from a separate domain to mitigate the gap between good representation learning and the scarce knowledge in the domain of interest. This approach is named Cross-Domain Knowledge Transfer. It is crucial to study the problem because of the commonality of scarce knowledge in many scenarios, from online healthcare platform analyses to financial market risk quantification, leaving an obstacle in front of us benefiting from automated decision making. From the machine learning perspective, the paradigm of semi-supervised learning takes advanta
    
[^123]: 基于分层Transformer的关系路径和上下文归纳关系预测方法

    Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])

    [http://arxiv.org/abs/2304.00215](http://arxiv.org/abs/2304.00215)

    本文提出了一种基于分层Transformer的方法，即REPORT，能够同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性。它完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在基准数据集上实现了最先进的性能。

    

    在知识图谱中进行关系预测是一个重要的研究课题。现有的嵌入式方法主要依赖于转导设置，缺乏归纳能力，无法推广到新的实体上进行推理。本文提出了一种新方法，通过使用统一的分层Transformer框架，即REPORT，同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性，这种方法完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在实验中，REPORT表现优于所有基线方法，甚至在两个完全归纳的数据集的八个版本子集上也是如此。此外，REPORT能够将推理推广到训练和推理中没有公共实体的新实体上，并在基准数据集上实现了最先进的性能。

    Relation prediction on knowledge graphs (KGs) is a key research topic. Dominant embedding-based methods mainly focus on the transductive setting and lack the inductive ability to generalize to new entities for inference. Existing methods for inductive reasoning mostly mine the connections between entities, i.e., relational paths, without considering the nature of head and tail entities contained in the relational context. This paper proposes a novel method that captures both connections between entities and the intrinsic nature of entities, by simultaneously aggregating RElational Paths and cOntext with a unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely on relation semantics and can naturally generalize to the fully-inductive setting, where KGs for training and inference have no common entities. In the experiments, REPORT performs consistently better than all baselines on almost all the eight version subsets of two fully-inductive datasets. Moreover. REPO
    
[^124]: 发现自然定律的机器学习

    Machine learning for discovering laws of nature. (arXiv:2303.17607v1 [cs.LG])

    [http://arxiv.org/abs/2303.17607](http://arxiv.org/abs/2303.17607)

    模型基于达尔文自然选择，结合函数选择和运算符选择两个过程，通过从数据中学习构建理论，可自动发现和表示自然定律，成功应用于模拟多领域问题，并提供一种新方法解决描述自然定律的严格数学模型不足的问题。

    

    微观粒子遵循量子力学的原理——那么宏观和微观世界之间的明确界限在哪里呢？正是这个“解释问题”促使薛定谔提出了他著名的思想实验（一只同时死亡和活着的猫），引发了关于量子测量问题的激烈争论，但至今仍没有令人满意的答案。这正是描述自然定律的严格数学模型的不足之处。我们提出了一个基于达尔文自然选择的计算模型来描述和理解自然定律。实际上，无论是宏观粒子、微观电子还是安全问题，它们都可以被认为是一个实体，这个实体随着时间的推移变化，可以用状态和值组成的数据序列来描述。观察者可以从这个数据序列中学习，构建理论（通常由函数和微分方程组成）。我们不再使用用户的经验或逻辑来建模，而是使用数据。计算模型的核心基于两个过程：函数选择和运算符选择。函数选择过程类似于达尔文的进化，允许具有优势特征的函数生存和繁殖；而运算符选择过程捕捉了自然定律的相互依存性，可以平衡自然界中不同函数的优势。该方法使我们能够从数据中自动发现和表示自然定律，并已成功应用于模拟量子力学、经典力学和系统生物学。

    A microscopic particle obeys the principles of quantum mechanics -- so where is the sharp boundary between the macroscopic and microscopic worlds? It was this "interpretation problem" that prompted Schr\"odinger to propose his famous thought experiment (a cat that is simultaneously both dead and alive) and sparked a great debate about the quantum measurement problem, and there is still no satisfactory answer yet. This is precisely the inadequacy of rigorous mathematical models in describing the laws of nature. We propose a computational model to describe and understand the laws of nature based on Darwin's natural selection. In fact, whether it's a macro particle, a micro electron or a security, they can all be considered as an entity, the change of this entity over time can be described by a data series composed of states and values. An observer can learn from this data series to construct theories (usually consisting of functions and differential equations). We don't model with the us
    
[^125]: 物理驱动的扩散模型用于从视频中合成冲击声

    Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos. (arXiv:2303.16897v1 [cs.CV])

    [http://arxiv.org/abs/2303.16897](http://arxiv.org/abs/2303.16897)

    该论文提出了一种物理驱动扩散模型，可以为silent视频剪辑合成高保真的冲击声，并使用额外的物理先验知识来指导冲击声合成过程。

    

    对物体相互作用发出的声音进行建模对于实际世界和虚拟世界中的沉浸式感官体验至关重要。传统的冲击声合成方法使用物理模拟来获得一组能够表示和合成声音的物理参数。然而，它们需要物体的细节和冲击位置，这在真实世界中很少可用，并且无法应用于从普通视频中合成冲击声。另一方面，现有的视频驱动深度学习方法只能捕捉到视觉内容和冲击声之间的弱对应关系，因为它们缺乏物理知识。在这项工作中，我们提出了一种物理驱动的扩散模型，可以为静态视频剪辑合成高保真的冲击声。除了视频内容外，我们还提出使用额外的物理先验知识来指导冲击声合成过程，这些先验包括既可控制物理参数，同时也能保证音效质量的噪声扰动。

    Modeling sounds emitted from physical object interactions is critical for immersive perceptual experiences in real and virtual worlds. Traditional methods of impact sound synthesis use physics simulation to obtain a set of physics parameters that could represent and synthesize the sound. However, they require fine details of both the object geometries and impact locations, which are rarely available in the real world and can not be applied to synthesize impact sounds from common videos. On the other hand, existing video-driven deep learning-based approaches could only capture the weak correspondence between visual content and impact sounds since they lack of physics knowledge. In this work, we propose a physics-driven diffusion model that can synthesize high-fidelity impact sound for a silent video clip. In addition to the video content, we propose to use additional physics priors to guide the impact sound synthesis procedure. The physics priors include both physics parameters that are
    
[^126]: BlackVIP: 针对稳健迁移学习的黑盒视觉提示

    BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning. (arXiv:2303.14773v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.14773](http://arxiv.org/abs/2303.14773)

    BlackVIP是一种针对大型预训练模型的黑盒视觉提示方法，它可以在没有参数可访问性的情况下高效地适应模型，并通过使用协调器和SPSA-GC组件实现。

    

    随着大规模预训练模型（PTMs）的兴起，将这些模型微调为众多下游任务变得至关重要。因此，大型模型的参数效率转移学习（PETL）引起了极大关注。虽然最近的PETL方法展示了令人印象深刻的性能，但它们依赖乐观的假设：1）PTM的整个参数集是可用的，2）具备足够大的内存容量进行微调。然而，在大多数实际应用中，PTMs作为黑盒API或专有软件提供，没有明确参数可访问性。此外，满足现代PTMs的大内存要求也很困难。在这项工作中，我们提出了黑盒视觉提示（BlackVIP），它可以有效地适应PTMs，而不需要关于模型架构和参数的知识。BlackVIP包括两个组件：1）协调器和2）带梯度校正的同时扰动随机逼近（SPSA-GC）。

    With the surge of large-scale pre-trained models (PTMs), fine-tuning these models to numerous downstream tasks becomes a crucial problem. Consequently, parameter efficient transfer learning (PETL) of large models has grasped huge attention. While recent PETL methods showcase impressive performance, they rely on optimistic assumptions: 1) the entire parameter set of a PTM is available, and 2) a sufficiently large memory capacity for the fine-tuning is equipped. However, in most real-world applications, PTMs are served as a black-box API or proprietary software without explicit parameter accessibility. Besides, it is hard to meet a large memory requirement for modern PTMs. In this work, we propose black-box visual prompting (BlackVIP), which efficiently adapts the PTMs without knowledge about model architectures and parameters. BlackVIP has two components; 1) Coordinator and 2) simultaneous perturbation stochastic approximation with gradient correction (SPSA-GC). The Coordinator designs 
    
[^127]: 大规模适应性实验：灵活批处理的贝叶斯算法

    Adaptive Experimentation at Scale: Bayesian Algorithms for Flexible Batches. (arXiv:2303.11582v1 [cs.LG])

    [http://arxiv.org/abs/2303.11582](http://arxiv.org/abs/2303.11582)

    本文提出了一个基于贝叶斯算法的自适应实验框架，可灵活处理任何批处理大小。通过正态近似指导可扩展自适应设计，采用残余时限优化选择采样分配，实现了最先进的性能。

    

    标准的贝叶斯算法假定持续重新分配测量工作，这在实现过程中存在延迟反馈和基础设施/组织难题等挑战。本文针对仅有少数重新分配阶段的实际情况，其中测量结果是以批处理形式测量的，提出了一种新的适应性实验框架，可灵活处理任何批处理大小。我们的主要观察是，在统计推断中普遍使用的正态近似也可以指导可扩展自适应设计。通过推导渐进顺序实验，我们制定了一种动态规划，可以利用平均回报的先验信息。动态规划的状态转移相对于采样分配是可微的，允许使用基于梯度的方法进行规划和策略优化。我们提出了一种简单的迭代规划方法，即残余时限优化，通过优化平衡探索和利用的规划目标来选择采样分配。在合成和真实世界基准测试问题上的实验结果表明，我们的框架实现了最先进的性能，同时具有模块化和易用性。

    Standard bandit algorithms that assume continual reallocation of measurement effort are challenging to implement due to delayed feedback and infrastructural/organizational difficulties. Motivated by practical instances involving a handful of reallocation epochs in which outcomes are measured in batches, we develop a new adaptive experimentation framework that can flexibly handle any batch size. Our main observation is that normal approximations universal in statistical inference can also guide the design of scalable adaptive designs. By deriving an asymptotic sequential experiment, we formulate a dynamic program that can leverage prior information on average rewards. State transitions of the dynamic program are differentiable with respect to the sampling allocations, allowing the use of gradient-based methods for planning and policy optimization. We propose a simple iterative planning method, Residual Horizon Optimization, which selects sampling allocations by optimizing a planning obj
    
[^128]: 基于对象中心的槽扩散

    Object-Centric Slot Diffusion. (arXiv:2303.10834v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.10834](http://arxiv.org/abs/2303.10834)

    基于对象中心的槽扩散是一种将扩散模型整合到对象中心学习的新颖方法，它使用Latent Slot Diffusion (LSD)模型替换了传统的槽解码器，同时也是第一个能够无监督进行组合条件扩散的模型。

    

    最近基于Transformer的图像生成模型在处理复杂场景的对象中心学习中取得了成功，这突显了强大的图像生成器的重要性。然而，尽管扩散模型在图像生成方面具有较高的表达能力，但它们在对象中心学习中的整合在这个领域中仍然较少探索。在本文中，我们探讨了将扩散模型整合到对象中心学习中的可行性和潜力，并研究了这种方法的优点和缺点。我们引入了Latent Slot Diffusion (LSD)，这是一种新颖的模型，它具有两个目标：首先，它是第一个将传统的槽解码器替换为以对象槽为条件的潜在扩散模型的对象中心学习模型；其次，它也是第一个不需要像文本这样的监督注释而能够无监督地进行组合条件扩散的模型。通过对各种对象中心任务的实验，包括首次在FFHQ数据集中的应用。

    The recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in t
    
[^129]: NeuSE: Neural SE(3)-等变嵌入用于一致的对象空间理解

    NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects. (arXiv:2303.07308v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.07308](http://arxiv.org/abs/2303.07308)

    NeuSE是一种用于对象的神经网络嵌入算法，支持实现与长期场景变化一致的对象空间理解。它可以编码完整的形状信息，并与物理世界中的对象同时变换，能够直接推断相对帧变换和相机姿态约束，并维持适应变化的轻量级对象地图。

    

    我们提出了NeuSE，这是一种用于对象的新颖的神经网络SE(3)-等变嵌入，并展示了它如何支持对象SLAM以实现与长期场景变化一致的空间理解。NeuSE是从部分对象观测中创建的一组潜在对象嵌入，在与物理世界中的对象同时SE(3)-等变变换的同时，它作为一个紧凑的点云替代完整的对象模型，编码了完整的形状信息。通过NeuSE，可以直接从推断的潜在代码中获得相对帧变换。我们提出的使用NeuSE进行对象形状和姿态特征化的SLAM范式可以独立运行或与典型的SLAM系统结合使用。它直接推断与普通SLAM姿态图优化兼容的SE(3)相机姿态约束，同时还维持了一个轻量级的以对象为中心的地图，能够适应现实世界的变化。我们的方法在包含改变的对象的合成和真实世界序列上进行了评估。

    We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and illustrate how it supports object SLAM for consistent spatial understanding with long-term scene changes. NeuSE is a set of latent object embeddings created from partial object observations. It serves as a compact point cloud surrogate for complete object models, encoding full shape information while transforming SE(3)-equivariantly in tandem with the object in the physical world. With NeuSE, relative frame transforms can be directly derived from inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape and pose characterization, can operate independently or in conjunction with typical SLAM systems. It directly infers SE(3) camera pose constraints that are compatible with general SLAM pose graph optimization, while also maintaining a lightweight object-centric map that adapts to real-world changes. Our approach is evaluated on synthetic and real-world sequences featuring changed objects 
    
[^130]: 通过局部-全局方法进行广义太阳辐射预测

    Local-Global Methods for Generalised Solar Irradiance Forecasting. (arXiv:2303.06010v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06010](http://arxiv.org/abs/2303.06010)

    本文提出了使用局部-全局方法进行广义太阳辐射预测，并且证明了可以构建不需要实时数据和历史观测数据的系统。

    

    随着太阳能使用量的增加，准确及时的预测对于运营平稳的电网运营商至关重要。目前有许多提出的方法用于预测太阳辐射/太阳能产量。然而，许多方法将问题表述为时间序列，依赖于对感兴趣位置附近观测数据的实时访问来生成预测。这要求同时具备实时数据流和足够的历史观测数据。在本文中，我们提出了使用全局方法以一种泛化的方式训练我们的模型，使其能够为未知位置生成预测。我们将这种方法应用于传统机器学习和最先进的方法。利用分布在英国境内的20个位置和广泛可得的天气数据，我们证明了可以构建不需要访问此数据的系统。我们同时利用和比较了卫星和地面观测数据（例如温度，压强）。

    As the use of solar power increases, having accurate and timely forecasts will be essential for smooth grid operators. There are many proposed methods for forecasting solar irradiance / solar power production. However, many of these methods formulate the problem as a time-series, relying on near real-time access to observations at the location of interest to generate forecasts. This requires both access to a real-time stream of data and enough historical observations for these methods to be deployed. In this paper, we propose the use of Global methods to train our models in a generalised way, enabling them to generate forecasts for unseen locations. We apply this approach to both classical ML and state of the art methods. Using data from 20 locations distributed throughout the UK and widely available weather data, we show that it is possible to build systems that do not require access to this data. We utilise and compare both satellite and ground observations (e.g. temperature, pressur
    
[^131]: 网络马尔可夫势博弈中局部演员-评论家算法的收敛速度

    Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games. (arXiv:2303.04865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04865](http://arxiv.org/abs/2303.04865)

    本文提出了一种网络马尔可夫势博弈中的局部演员-评论家算法，通过局部化和函数逼近技术，实现了有限样本保证，样本复杂度不依赖于代理数量。

    

    我们在网络马尔可夫势博弈中引入一类与网络中的节点相关的势博弈。每个代理都有自己的局部势函数，每个代理的奖励只取决于邻域中代理的状态和动作。在这个背景下，我们提出了一种局部演员-评论家算法。该算法具有可扩展性，因为每个代理只使用局部信息，不需要访问全局状态。此外，该算法通过使用函数逼近来克服维度灾难。我们的主要结果在局部化误差和函数逼近误差的情况下提供了有限样本保证。具体而言，我们通过平均纳什遗憾度量，实现了一个$\tilde{\mathcal {O}}(\tilde{\epsilon}^{-4})$的样本复杂度。这是第一个不依赖于代理数量的多智能体竞争博弈的有限样本界。

    We introduce a class of networked Markov potential games in which agents are associated with nodes in a network. Each agent has its own local potential function, and the reward of each agent depends only on the states and actions of the agents within a neighborhood. In this context, we propose a localized actor-critic algorithm. The algorithm is scalable since each agent uses only local information and does not need access to the global state. Further, the algorithm overcomes the curse of dimensionality through the use of function approximation. Our main results provide finite-sample guarantees up to a localization error and a function approximation error. Specifically, we achieve an $\tilde{\mathcal{O}}(\tilde{\epsilon}^{-4})$ sample complexity measured by the averaged Nash regret. This is the first finite-sample bound for multi-agent competitive games that does not depend on the number of agents.
    
[^132]: 自我监督语音表示在朗读和自由说话语音合成中的比较研究

    A Comparative Study of Self-Supervised Speech Representations in Read and Spontaneous TTS. (arXiv:2303.02719v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2303.02719](http://arxiv.org/abs/2303.02719)

    这项研究比较了在朗读和自由说话语音合成中使用的自我监督语音表示，并发现在12层wav2vec2.0（ASR微调）的第9层表现最优。这项工作揭示了语音SSL如何改进TTS系统，并在具有挑战性的TTS生成任务中进行了比较。

    

    最近的研究探索了使用自我监督学习（SSL）语音表示，如wav2vec2.0作为标准两阶段语音合成（TTS）中的表示介质，以取代惯常使用的mel频谱图。然而，目前尚不清楚哪种语音SSL适用于TTS，并且朗读和自由说话TTS之间的性能是否有所不同，后者可能更具挑战性。本研究旨在通过在朗读和自由说话语料库上测试几种语音SSL，包括同一个SSL的不同层，在保持TTS模型架构和训练设置恒定的情况下，解答这些问题。听测试的结果显示，在朗读和自由说话的TTS中，12层wav2vec2.0（ASR微调）的第9层胜过其他被测试的SSL和mel频谱图。我们的工作揭示了语音SSL如何可以方便地改进当前的TTS系统，以及在具有挑战性的TTS生成任务中SSL之间的比较。

    Recent work has explored using self-supervised learning (SSL) speech representations such as wav2vec2.0 as the representation medium in standard two-stage TTS, in place of conventionally used mel-spectrograms. It is however unclear which speech SSL is the better fit for TTS, and whether or not the performance differs between read and spontaneous TTS, the later of which is arguably more challenging. This study aims at addressing these questions by testing several speech SSLs, including different layers of the same SSL, in two-stage TTS on both read and spontaneous corpora, while maintaining constant TTS model architecture and training settings. Results from listening tests show that the 9th layer of 12-layer wav2vec2.0 (ASR finetuned) outperforms other tested SSLs and mel-spectrogram, in both read and spontaneous TTS. Our work sheds light on both how speech SSL can readily improve current TTS systems, and how SSLs compare in the challenging generative task of TTS. Audio examples can be 
    
[^133]: 辅助函数作为Koopman可观测量：基于数据的动力系统多项式优化方法

    Auxiliary Functions as Koopman Observables: Data-Driven Polynomial Optimization for Dynamical Systems. (arXiv:2303.01483v2 [math.DS] UPDATED)

    [http://arxiv.org/abs/2303.01483](http://arxiv.org/abs/2303.01483)

    提出了一种基于数据的动力系统分析方法，使用辅助函数作为Koopman可观测量，不需要明确的模型发现，可以适用于确定性和随机动力学，具有收敛性和性能优势。

    

    我们提出了一种灵活的基于数据的动力系统分析方法，不需要明确的模型发现。该方法源于从数据中逼近Koopman算子的技术，并且以一个可以通过数值求解的半定规划问题来实现。此外，该方法不关心数据是通过确定性还是随机过程生成的，因此用户无需进行任何调整即可应用于不同的情况。严格的收敛性结果证明了该方法的适用性，并将文献中类似的结果进行了扩展和统一。通过对确定性和随机动力学的吸引子上发现Lyapunov函数、执行遍历优化以及界定极值的示例，证明了该方法的性能。

    We present a flexible data-driven method for dynamical system analysis that does not require explicit model discovery. The method is rooted in well-established techniques for approximating the Koopman operator from data and is implemented as a semidefinite program that can be solved numerically. Furthermore, the method is agnostic of whether data is generated through a deterministic or stochastic process, so its implementation requires no prior adjustments by the user to accommodate these different scenarios. Rigorous convergence results justify the applicability of the method, while also extending and uniting similar results from across the literature. Examples on discovering Lyapunov functions, performing ergodic optimization, and bounding extrema over attractors for both deterministic and stochastic dynamics exemplify these convergence results and demonstrate the performance of the method.
    
[^134]: OmniForce: 基于人类中心、大模型和云边协同的自动机器学习系统

    OmniForce: On Human-Centered, Large Model Empowered and Cloud-Edge Collaborative AutoML System. (arXiv:2303.00501v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00501](http://arxiv.org/abs/2303.00501)

    OmniForce是一个基于人类中心、大模型和云边协同的自动机器学习系统，旨在解决开放环境下的AutoML问题。当前的AutoML系统和平台低效且计算复杂，而OmniForce通过人机交互技术改进了这一现状。

    

    自动化机器学习 (AutoML) 旨在以最少人力投入构建机器学习模型。虽然在AutoML领域进行了大量研究，以便在构建人工智能 (AI) 应用程序时将人类排除在外，但鲜有文献关注AutoML如何在开放环境场景中工作，例如训练和更新大型模型、工业供应链或工业虚拟世界。在这些场景中，人们在搜索过程中经常面临开放环路问题：他们必须不断收集数据、更新数据和模型、满足开发和部署环境的要求、支持大规模设备、修改评估指标等。使用纯数据驱动方法解决开放环境问题需要大量数据、计算资源和来自专职数据工程师的努力，使得当前的AutoML系统和平台低效且计算复杂。人机交互技术是解决这个问题的关键。

    Automated machine learning (AutoML) seeks to build ML models with minimal human effort. While considerable research has been conducted in the area of AutoML in general, aiming to take humans out of the loop when building artificial intelligence (AI) applications, scant literature has focused on how AutoML works well in open-environment scenarios such as the process of training and updating large models, industrial supply chains or the industrial metaverse, where people often face open-loop problems during the search process: they must continuously collect data, update data and models, satisfy the requirements of the development and deployment environment, support massive devices, modify evaluation metrics, etc. Addressing the open-environment issue with pure data-driven approaches requires considerable data, computing resources, and effort from dedicated data engineers, making current AutoML systems and platforms inefficient and computationally intractable. Human-computer interaction i
    
[^135]: 带有背包的近似稳定掠夺者

    Approximately Stationary Bandits with Knapsacks. (arXiv:2302.14686v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14686](http://arxiv.org/abs/2302.14686)

    带有背包的掠夺者问题在随机和敌对情况下存在巨大的差距，尤其是在敌对情况下，当预算更加紧缺时保证性能变得更差。

    

    带有背包的掠夺者问题（BwK）是在全局预算约束下将掠夺者问题进行泛化的研究，近年来引起了广泛关注。之前的研究关注的是两个极端之一：随机BwK，在每一轮中，奖励和资源的消耗从一个独立同分布的分布中采样；而敌对BwK，则由对手选择这些参数。这两种情况下的可实现保证存在巨大的差距：在随机情况下可以达到无悔学习，而在敌对情况下只能达到基于竞争比的保证，其中竞争比取决于预算或同时取决于时间和资源数量。这种差距之所以如此巨大，在敌对BwK的典型情况下，保证性能变得更差时，预算更加紧缺。虽然已知存在“两全其美”类型的算法（单个算法可以在两个极端情况下提供最佳的可实现保证），它们的界限则会变差。

    Bandits with Knapsacks (BwK), the generalization of the Bandits problem under global budget constraints, has received a lot of attention in recent years. Previous work has focused on one of the two extremes: Stochastic BwK where the rewards and consumptions of the resources of each round are sampled from an i.i.d. distribution, and Adversarial BwK where these parameters are picked by an adversary. Achievable guarantees in the two cases exhibit a massive gap: No-regret learning is achievable in the stochastic case, but in the adversarial case only competitive ratio style guarantees are achievable, where the competitive ratio depends either on the budget or on both the time and the number of resources. What makes this gap so vast is that in Adversarial BwK the guarantees get worse in the typical case when the budget is more binding. While ``best-of-both-worlds'' type algorithms are known (single algorithms that provide the best achievable guarantee in each extreme case), their bounds deg
    
[^136]: FedCLIP：联邦学习中用于CLIP的快速泛化和个性化方法

    FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning. (arXiv:2302.13485v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13485](http://arxiv.org/abs/2302.13485)

    本文介绍了一种名为FedCLIP的方法，用于在联邦学习中实现CLIP的快速泛化和个性化。该方法通过设计基于注意力的适配器，充分利用预训练模型信息，并确保模型适应特定任务的客户端。这种方法可以提高模型训练的效率和性能。

    

    近年来，联邦学习（FL）作为一种保护隐私计算的新范式已经出现。然而，FL面临两个关键挑战，限制了其实际性能：数据分布异质性和大型基础模型带来的高资源成本。具体而言，不同客户端中的非独立同分布数据使得现有的FL算法难以收敛，而高资源成本（包括计算和通信成本）增加了在实际场景中的部署难度。本文提出了一种有效而简单的方法，名为FedCLIP，用于实现联邦学习中CLIP的快速泛化和个性化。具体而言，我们设计了一个基于注意力的适配器来适应大型模型CLIP，其余操作仅依赖于适配器。轻量级适配器可以充分利用预训练模型信息，并确保模型在特定任务的客户端中具有自适应性。同时，小规模操作可以缓解计算负担和通信压力，提高模型训练的效率和性能。

    Federated learning (FL) has emerged as a new paradigm for privacy-preserving computation in recent years. Unfortunately, FL faces two critical challenges that hinder its actual performance: data distribution heterogeneity and high resource costs brought by large foundation models. Specifically, the non-IID data in different clients make existing FL algorithms hard to converge while the high resource costs, including computational and communication costs that increase the deployment difficulty in real-world scenarios. In this paper, we propose an effective yet simple method, named FedCLIP, to achieve fast generalization and personalization for CLIP in federated learning. Concretely, we design an attention-based adapter for the large model, CLIP, and the rest operations merely depend on adapters. Lightweight adapters can make the most use of pretrained model information and ensure models be adaptive for clients in specific tasks. Simultaneously, small-scale operations can mitigate the co
    
[^137]: 自适应稀疏高斯过程

    Adaptive Sparse Gaussian Process. (arXiv:2302.10325v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10325](http://arxiv.org/abs/2302.10325)

    这篇论文提出了第一个自适应稀疏高斯过程，能够在非平稳环境中进行高效的模型更新，并具有快速的推理收敛性。

    

    自适应学习对于非平稳环境中的学习机器是必要的，因为它需要忘记过去的数据分布。高效的算法需要紧凑的模型更新，以便不随着新数据的到来而增加计算负担，并以最低的计算成本进行在线参数更新。现有的解决方案只是部分满足这些需求。在这里，我们提出了第一个能够解决所有这些问题的自适应稀疏高斯过程（GP）。我们首先通过遗忘因子重新定义了变分稀疏GP算法，使其具有自适应性。接下来，为了使模型推理尽可能简单，我们建议每当出现新样本时同时更新稀疏GP模型的一个单个引导点和其他模型参数。结果，该算法呈现出推理过程的快速收敛性，即使在高度非平稳的环境中也能进行高效的模型更新（只需一次推理迭代）。试验结果表明，该算法在多种数据集上表现出了良好的性能。

    Adaptive learning is necessary for non-stationary environments where the learning machine needs to forget past data distribution. Efficient algorithms require a compact model update to not grow in computational burden with the incoming data and with the lowest possible computational cost for online parameter updating. Existing solutions only partially cover these needs. Here, we propose the first adaptive sparse Gaussian Process (GP) able to address all these issues. We first reformulate a variational sparse GP algorithm to make it adaptive through a forgetting factor. Next, to make the model inference as simple as possible, we propose updating a single inducing point of the sparse GP model together with the remaining model parameters every time a new sample arrives. As a result, the algorithm presents a fast convergence of the inference process, which allows an efficient model update (with a single inference iteration) even in highly non-stationary environments. Experimental results d
    
[^138]: 磁流体力学与物理信息神经算子

    Magnetohydrodynamics with Physics Informed Neural Operators. (arXiv:2302.08332v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2302.08332](http://arxiv.org/abs/2302.08332)

    本文提出了一种使用物理信息神经算子模拟磁流体力学的方法，通过使用人工智能技术，在显著降低计算成本的同时，准确捕捉了磁流体力学模拟的物理特性。

    

    对于建模多尺度和多物理复杂系统，通常需要使用能够充分发挥极端规模计算的科学软件。尽管近年来取得了重大进展，但这些模拟仍然需要大量计算和时间。在这里，我们探讨了使用人工智能来加速复杂系统建模的方法，其计算成本仅为传统方法的一小部分，并首次将物理信息神经算子应用于二维不可压缩磁流体力学模拟。我们的人工智能模型将张量傅里叶神经算子作为其基础，并使用TensorLY软件包进行实现。我们的结果表明，物理信息神经算子可以准确捕捉描述雷诺数$Re\leq250$的层流磁流体力学模拟的物理特性。我们还探讨了将我们的人工智能代理用于湍流流动的适用性，并讨论了多种可能的方法论。

    The modeling of multi-scale and multi-physics complex systems typically involves the use of scientific software that can optimally leverage extreme scale computing. Despite major developments in recent years, these simulations continue to be computationally intensive and time consuming. Here we explore the use of AI to accelerate the modeling of complex systems at a fraction of the computational cost of classical methods, and present the first application of physics informed neural operators to model 2D incompressible magnetohydrodynamics simulations. Our AI models incorporate tensor Fourier neural operators as their backbone, which we implemented with the TensorLY package. Our results indicate that physics informed neural operators can accurately capture the physics of magnetohydrodynamics simulations that describe laminar flows with Reynolds numbers $Re\leq250$. We also explore the applicability of our AI surrogates for turbulent flows, and discuss a variety of methodologies that may
    
[^139]: 通过深度学生机器实现空间异质性学习

    Spatially heterogeneous learning by a deep student machine. (arXiv:2302.07419v3 [cond-mat.dis-nn] UPDATED)

    [http://arxiv.org/abs/2302.07419](http://arxiv.org/abs/2302.07419)

    本论文研究了一种深度学生机器的教师-学生设置，通过学生机器的集合来研究由具有大量可调参数的DNN的监督学习。研究表明DNN的学习在网络空间中相当异质。

    

    尽管深度神经网络（DNN）取得了非凡的成功，但由于具有大量可调参数，其仍然是黑匣子。为了研究DNN的隐藏层，本文通过一种统计力学方法称为教师-学生设置，研究了由宽度为N，深度为L，由具有c个输入的感知机组成的DNN的监督学习。我们考虑了一个学生机器的集合，该集合可以精确重现由教师机器提供的M组N维输入/输出关系。我们使用副本方法（H. Yoshino（2020））理论分析了集合，并进行了贪婪的Monte Carlo模拟。对于高维数据$N \gg 1$，理论在'密集极限' $N \gg c \gg 1$ 和 $M \gg 1$ 且固定$\alpha=M/c$时变得精确。理论和模拟都表明，DNN的学习在网络空间中相当异质：机器的配置在靠近输入/输出的层内更加相关。

    Despite the spectacular successes, deep neural networks (DNN) with a huge number of adjustable parameters remain largely black boxes. To shed light on the hidden layers of DNN, we study supervised learning by a DNN of width $N$ and depth $L$ consisting of perceptrons with $c$ inputs by a statistical mechanics approach called the teacher-student setting. We consider an ensemble of student machines that exactly reproduce $M$ sets of $N$ dimensional input/output relations provided by a teacher machine. We analyze the ensemble theoretically using a replica method (H. Yoshino (2020)) and numerically performing greedy Monte Carlo simulations. The replica theory which works on high dimensional data $N \gg 1$ becomes exact in 'dense limit' $N \gg c \gg 1$ and $M \gg 1$ with fixed $\alpha=M/c$. Both the theory and the simulation suggest learning by the DNN is quite heterogeneous in the network space: configurations of the machines are more correlated within the layers closer to the input/output
    
[^140]: 数据依赖分形维度的泛化界限

    Generalization Bounds with Data-dependent Fractal Dimensions. (arXiv:2302.02766v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.02766](http://arxiv.org/abs/2302.02766)

    这项研究提出了基于数据依赖的分形维度的泛化界限，不需要Lipschitz假设，并能控制泛化误差和互信息项。

    

    在统计学习中，为现代神经网络提供泛化保证是一项关键任务。最近，一些研究尝试使用分形几何的工具来分析这种情况下的泛化误差。尽管这些工作成功地引入了新的数学工具来理解泛化，但它们严重依赖于Lipschitz连续性假设，而这一假设通常不适用于神经网络，并且可能使界限变得无效。在这项工作中，我们解决了这个问题，并且证明了不需要任何Lipschitz假设的基于分形几何的泛化界限。为了实现这个目标，我们在学习理论中基于经典的覆盖论证，并引入了数据依赖的分形维度。尽管引入了大量的技术复杂性，但这个新概念使我们能够控制泛化误差（在固定或随机的假设空间上）以及特定的互信息（MI）项。

    Providing generalization guarantees for modern neural networks has been a crucial task in statistical learning. Recently, several studies have attempted to analyze the generalization error in such settings by using tools from fractal geometry. While these works have successfully introduced new mathematical tools to apprehend generalization, they heavily rely on a Lipschitz continuity assumption, which in general does not hold for neural networks and might make the bounds vacuous. In this work, we address this issue and prove fractal geometry-based generalization bounds without requiring any Lipschitz assumption. To achieve this goal, we build up on a classical covering argument in learning theory and introduce a data-dependent fractal dimension. Despite introducing a significant amount of technical complications, this new notion lets us control the generalization error (over either fixed or random hypothesis spaces) along with certain mutual information (MI) terms. To provide a clearer
    
[^141]: 时态图的图神经网络：现状、挑战和机遇综述

    Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities. (arXiv:2302.01018v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01018](http://arxiv.org/abs/2302.01018)

    该论文总结了时态图的图神经网络的现状、挑战和机遇，提供了学习设置和任务的严格规范化以及一个新的分类法，并讨论了该领域最相关的开放挑战，从研究和应用角度讨论。

    

    图神经网络（GNNs）已经成为学习（静态）图结构数据的主要范例。然而，许多真实世界的系统是动态的，因为图和节点/边属性随着时间而变化。近年来，基于GNN的时态图模型已经成为扩展GNN能力的有前途的研究领域。在这项工作中，我们提供了第一个关于时态GNN的现状全面的概述，引入了学习设置和任务的严格规范化以及一个新的分类法，以表示和处理时态方面的现有方法。我们从研究和应用角度讨论了该领域最相关的开放挑战，结束了这项研究。

    Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.
    
[^142]: 从一个数据空间到另一个数据空间的本地迁移学习

    Local transfer learning from one data space to another. (arXiv:2302.00160v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00160](http://arxiv.org/abs/2302.00160)

    

    

    流形学习中的一个基本问题是在从支持在高维欧几里得空间中的低维子流形上随机选择的数据上近似一个函数关系。流形本质上由数据集本身定义，并且通常设计为数据在某种意义上在流形上稠密。数据空间的概念是一个抽象的流形，封装了允许进行函数逼近的基本属性。迁移学习（元学习）问题是利用在一个数据集上学习一个函数来学习在另一个数据集上的类似函数。在函数逼近方面，这意味着将一个数据空间上的函数（基本数据空间）提升到另一个数据空间（目标数据空间）。这个观点使我们能够将应用数学中的一些逆问题（如逆Radon变换）与迁移学习联系起来。本文探讨了这种提升问题。

    A fundamental problem in manifold learning is to approximate a functional relationship in a data chosen randomly from a probability distribution supported on a low dimensional sub-manifold of a high dimensional ambient Euclidean space. The manifold is essentially defined by the data set itself and, typically, designed so that the data is dense on the manifold in some sense. The notion of a data space is an abstraction of a manifold encapsulating the essential properties that allow for function approximation. The problem of transfer learning (meta-learning) is to use the learning of a function on one data set to learn a similar function on a new data set. In terms of function approximation, this means lifting a function on one data space (the base data space) to another (the target data space). This viewpoint enables us to connect some inverse problems in applied mathematics (such as inverse Radon transform) with transfer learning. In this paper we examine the question of such lifting w
    
[^143]: 通过孪生序列-结构扩散轨迹预测进行蛋白质编码器的预训练

    Pre-Training Protein Encoder via Siamese Sequence-Structure Diffusion Trajectory Prediction. (arXiv:2301.12068v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12068](http://arxiv.org/abs/2301.12068)

    本研究提出了一种名为DiffPreT的方法，通过序列-结构联合扩散建模来预训练蛋白质编码器。同时，通过一种称为SiamDiff的方法增强了DiffPreT，以捕捉蛋白质的不同构象之间的相关性。

    

    最近，自监督预训练方法在蛋白质领域引起了关注，大多数方法要么集中于蛋白质序列，要么集中于蛋白质结构，忽视了它们的联合分布的探索，而联合分布对于全面了解蛋白质功能非常重要，因为它能够整合共同演化信息和结构特性。本研究受到去噪扩散模型在生成任务中的成功启发，提出了DiffPreT方法，通过序列-结构联合扩散建模来预训练蛋白质编码器。DiffPreT引导编码器从扰动的序列和结构中恢复出原始蛋白质序列和结构，从而获得了序列和结构的联合分布。考虑到蛋白质构象的重要变化，我们通过一种称为孪生扩散轨迹预测（SiamDiff）的方法增强了DiffPreT，以捕捉蛋白质的不同构象之间的相关性。

    Self-supervised pre-training methods on proteins have recently gained attention, with most approaches focusing on either protein sequences or structures, neglecting the exploration of their joint distribution, which is crucial for a comprehensive understanding of protein functions by integrating co-evolutionary information and structural characteristics. In this work, inspired by the success of denoising diffusion models in generative tasks, we propose the DiffPreT approach to pre-train a protein encoder by sequence-structure joint diffusion modeling. DiffPreT guides the encoder to recover the native protein sequences and structures from the perturbed ones along the joint diffusion trajectory, which acquires the joint distribution of sequences and structures. Considering the essential protein conformational variations, we enhance DiffPreT by a method called Siamese Diffusion Trajectory Prediction (SiamDiff) to capture the correlation between different conformers of a protein. SiamDiff 
    
[^144]: 通过D适应实现学习率自由学习

    Learning-Rate-Free Learning by D-Adaptation. (arXiv:2301.07733v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.07733](http://arxiv.org/abs/2301.07733)

    D-Adaptation是一种可以自动设置学习率的方法，针对最小化凸性Lipschitz函数，用于实现最优收敛速率，而无需超参数，也无需额外对数因子改进，能够在各种机器学习问题中自动匹配手动调整的学习率。

    

    D适应是一种自动设置学习率的方法，可以渐近地实现最优收敛速率，用于最小化凸性Lipschitz函数，无需回溯或线性搜索，并且每步无需进行额外的函数值或梯度评估。我们的方法是这一类问题的第一个无超参数且收敛速率无需额外对数因子改进的方法。我们针对SGD和Adam变体展示了广泛的实验，其中该方法自动匹配手动调整的学习率，在十多个不同的机器学习问题中应用，包括大规模的视觉和语言问题。开源实现在 \url{https://github.com/facebookresearch/dadaptation}.

    D-Adaptation is an approach to automatically setting the learning rate which asymptotically achieves the optimal rate of convergence for minimizing convex Lipschitz functions, with no back-tracking or line searches, and no additional function value or gradient evaluations per step. Our approach is the first hyper-parameter free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems.  An open-source implementation is available at \url{https://github.com/facebookresearch/dadaptation}.
    
[^145]: 关于连续学习的顺序贝叶斯推断

    On Sequential Bayesian Inference for Continual Learning. (arXiv:2301.01828v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01828](http://arxiv.org/abs/2301.01828)

    这篇论文研究了顺序贝叶斯推断在连续学习中的应用。研究表明，尽管使用了真实后验，这种方法仍无法防止神经网络中的灾难性遗忘。同时，模型误差和任务数据不平衡也会导致连续学习性能的下降。

    

    顺序贝叶斯推断可用于连续学习，以防止过去任务的灾难性遗忘，并在学习新任务时提供信息丰富的先验。我们重新审视顺序贝叶斯推断，并测试是否有访问真实后验的保证可以防止贝叶斯神经网络中的灾难性遗忘。为了做到这一点，我们使用哈密顿蒙特卡洛执行顺序贝叶斯推断。我们通过对哈密顿蒙特卡洛样本拟合密度估计器，将后验传播为新任务的先验。我们发现这种方法无法防止灾难性遗忘，证明了在神经网络中执行顺序贝叶斯推断的困难。然后，我们研究了顺序贝叶斯推断和连续学习的简单分析示例，并强调了模型错误说明问题，即使进行了准确的推断，也可能导致次优的连续学习性能。此外，我们讨论了任务数据不平衡可能导致遗忘的问题。

    Sequential Bayesian inference can be used for continual learning to prevent catastrophic forgetting of past tasks and provide an informative prior when learning new tasks. We revisit sequential Bayesian inference and test whether having access to the true posterior is guaranteed to prevent catastrophic forgetting in Bayesian neural networks. To do this we perform sequential Bayesian inference using Hamiltonian Monte Carlo. We propagate the posterior as a prior for new tasks by fitting a density estimator on Hamiltonian Monte Carlo samples. We find that this approach fails to prevent catastrophic forgetting demonstrating the difficulty in performing sequential Bayesian inference in neural networks. From there we study simple analytical examples of sequential Bayesian inference and CL and highlight the issue of model misspecification which can lead to sub-optimal continual learning performance despite exact inference. Furthermore, we discuss how task data imbalances can cause forgetting.
    
[^146]: 可证明鲁棒的基于显著性的解释

    Provable Robust Saliency-based Explanations. (arXiv:2212.14106v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.14106](http://arxiv.org/abs/2212.14106)

    本文提出了一种可证明鲁棒的基于显著性的解释方法，通过最大化解释厚度和稳定顶部显著特征，改进了解释的数值和统计稳定性。实验证明了该方法在各种网络和数据上的性能。

    

    机器学习模型的鲁棒解释对于建立人类对模型的信任至关重要。通过使用顶部-k的交集来评估解释的鲁棒性是常用的方法。然而，大多数现有的攻击和防御策略都基于$\ell_p$范数，从而在评估和优化目标之间存在不匹配。为此，我们定义了解释的厚度来衡量顶部-k显著特征排名的稳定性，并设计了基于一种新颖可行的替代目标的R2ET算法，以高效地最大化厚度并稳定顶部显著特征。在理论上，我们证明了R2ET和对抗训练之间的联系；通过使用一种新颖的多目标优化公式和泛化误差界，我们进一步证明了替代目标可以改进解释的数值和统计稳定性。通过对各种网络架构和数据模态进行实验，验证了R2ET的性能。

    Robust explanations of machine learning models are critical to establishing human trust in the models. The top-$k$ intersection is widely used to evaluate the robustness of explanations. However, most existing attacking and defense strategies are based on $\ell_p$ norms, thus creating a mismatch between the evaluation and optimization objectives. To this end, we define explanation thickness for measuring top-$k$ salient features ranking stability, and design the \textit{R2ET} algorithm based on a novel tractable surrogate to maximize the thickness and stabilize the top salient features efficiently. Theoretically, we prove a connection between R2ET and adversarial training; using a novel multi-objective optimization formulation and a generalization error bound, we further prove that the surrogate objective can improve both the numerical and statistical stability of the explanations. Experiments with a wide spectrum of network architectures and data modalities demonstrate that R2ET attai
    
[^147]: 区块链上的AI伦理: 基于Twitter数据的区块链安全主题分析

    AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security. (arXiv:2212.06951v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2212.06951](http://arxiv.org/abs/2212.06951)

    本研究分析了Twitter上与MEV相关的话题，结果表明推文讨论了深刻的伦理问题，包括安全、公平、情感情绪和对MEV解决方案的渴望。

    

    区块链使用分布式网络让计算机系统更安全，但当前的区块链设计在交易顺序方面存在公平性问题。矿工可以重新排序交易以生成利润，这被称为矿工可提取价值（MEV）问题。现有研究认为MEV是一个严重的安全问题，并提出了潜在的解决方案，包括知名的Flashbots。然而，以往的研究大多分析了区块链数据，这可能无法捕捉到MEV在更广泛的AI社会中的影响。因此，在这项研究中，我们应用自然语言处理（NLP）方法全面分析了MEV推文中的话题。我们收集了超过20,000个MEV和Flashbots标签的推文并分析了它们的话题。我们的结果显示，这些推文讨论了深刻的伦理问题，包括安全、公平、情感情绪和对MEV解决方案的渴望。我们还发现了区块链上MEV活动的共同运动。

    Blockchain has empowered computer systems to be more secure using a distributed network. However, the current blockchain design suffers from fairness issues in transaction ordering. Miners are able to reorder transactions to generate profits, the so-called miner extractable value (MEV). Existing research recognizes MEV as a severe security issue and proposes potential solutions, including prominent Flashbots. However, previous studies have mostly analyzed blockchain data, which might not capture the impacts of MEV in a much broader AI society. Thus, in this research, we applied natural language processing (NLP) methods to comprehensively analyze topics in tweets on MEV. We collected more than 20000 tweets with MEV and Flashbots hashtags and analyzed their topics. Our results show that the tweets discussed profound topics of ethical concern, including security, equity, emotional sentiments, and the desire for solutions to MEV. We also identify the co-movements of MEV activities on block
    
[^148]: 条件生成建模是否足以解决决策问题？

    Is Conditional Generative Modeling all you need for Decision-Making?. (arXiv:2211.15657v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15657](http://arxiv.org/abs/2211.15657)

    该论文研究了条件生成建模方法在解决顺序决策问题方面的应用，发现该方法在标准基准测试中表现优于传统离线强化学习方法。通过建模为回报条件扩散模型，可以避免动态规划的复杂性，并通过考虑约束和技能作为条件变量进一步提高性能。

    

    最近条件生成建模的改进使得仅凭语言描述就能生成高质量的图像成为可能。我们探讨这些方法是否可以直接解决顺序决策问题。我们从条件生成建模的视角来看待决策，而非强化学习。令人惊讶的是，我们发现我们的方法在标准基准测试中可以优于现有的离线强化学习方法。通过将策略建模为回报条件扩散模型，我们展示了如何避免动态规划的需要，进而消除传统离线强化学习所带来的许多复杂性。通过考虑两个其他的条件变量：约束和技能，我们进一步证明了将策略建模为条件扩散模型的优势。在训练过程中仅对单个约束或技能进行条件设置，导致测试时的行为表现。

    Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that 
    
[^149]: 分布偏移的标签对齐正则化

    Label Alignment Regularization for Distribution Shift. (arXiv:2211.14960v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14960](http://arxiv.org/abs/2211.14960)

    这篇论文提出了一种用于无监督领域自适应的正则化方法，通过鼓励目标域中的预测与其前几个奇异向量对齐来实现。与传统方法不同的是，这个方法通过正则化分类器与无监督目标数据对齐，而不是正则化表示。通过消除对最优联合风险假设的依赖，该方法展示了很好的效果。

    

    最近的研究强调了监督学习中的标签对齐属性（LAP），即数据集中所有标签的向量大部分在数据矩阵的前几个奇异向量的张成空间内。受到这一观察的启发，我们提出了一种无监督领域自适应的正则化方法，鼓励目标域中的预测与其前几个奇异向量对齐。与传统的领域适应方法专注于正则化表示不同，我们相反，通过在源域和目标域中使用LAP，用正则化分类器与无监督目标数据对齐。理论分析表明，在一定的假设下，我们的解决方案位于目标域数据的前几个右奇异向量的张成空间内，并与最优解对齐。通过消除经典领域适应理论中常见的最优联合风险假设的依赖，我们展示了该方法的有效性。

    Recent work has highlighted the label alignment property (LAP) in supervised learning, where the vector of all labels in the dataset is mostly in the span of the top few singular vectors of the data matrix. Drawing inspiration from this observation, we propose a regularization method for unsupervised domain adaptation that encourages alignment between the predictions in the target domain and its top singular vectors. Unlike conventional domain adaptation approaches that focus on regularizing representations, we instead regularize the classifier to align with the unsupervised target data, guided by the LAP in both the source and target domains. Theoretical analysis demonstrates that, under certain assumptions, our solution resides within the span of the top right singular vectors of the target domain data and aligns with the optimal solution. By removing the reliance on the commonly used optimal joint risk assumption found in classic domain adaptation theory, we showcase the effectivene
    
[^150]: 高效学习和测试潜在树状Ising模型

    Learning and Testing Latent-Tree Ising Models Efficiently. (arXiv:2211.13291v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.13291](http://arxiv.org/abs/2211.13291)

    本文提出了学习和测试潜在树状Ising模型的高效算法，改进了先前工作的结果，并通过展示树状Ising模型叶节点分布的新颖局部化结果来实现这些算法。

    

    我们提供了高效的学习和测试潜在树状Ising模型的算法，即只能在其叶节点处观测到的Ising模型。在学习方面，我们获得了一种高效的算法，可以学习到一个叶节点分布在总变异距离上与之接近的树状Ising模型，改进了先前工作的结果。在测试方面，我们提供了一种有效的算法，使用更少的样本来测试两个潜在树状Ising模型的叶节点分布是否在总变异距离上接近或远离。通过展示树状Ising模型叶节点分布的总变异距离的新颖局部化结果，我们获得了我们的算法。

    We provide time- and sample-efficient algorithms for learning and testing latent-tree Ising models, i.e. Ising models that may only be observed at their leaf nodes. On the learning side, we obtain efficient algorithms for learning a tree-structured Ising model whose leaf node distribution is close in Total Variation Distance, improving on the results of prior work. On the testing side, we provide an efficient algorithm with fewer samples for testing whether two latent-tree Ising models have leaf-node distributions that are close or far in Total Variation distance. We obtain our algorithms by showing novel localization results for the total variation distance between the leaf-node distributions of tree-structured Ising models, in terms of their marginals on pairs of leaves.
    
[^151]: Transformers中的简洁偏见及其学习稀疏布尔函数的能力

    Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions. (arXiv:2211.12316v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12316](http://arxiv.org/abs/2211.12316)

    Transformers相对于循环模型更偏向于学习具有低敏感度的函数，尤其在稀疏布尔函数上，Transformers能够实现近乎完美的泛化，而LSTMs则表现出过拟合和较低的泛化精度。

    

    尽管Transformers在自然语言处理任务上取得了广泛的成功，但最近的研究发现，与循环模型相比，它们在建模几种形式语言时遇到困难。这引发了一个问题，为什么Transformers在实践中表现良好，它们是否具有任何能使它们比循环模型更好地泛化的属性。在这项工作中，我们对布尔函数进行了广泛的实证研究，以证明以下内容：(i) 随机Transformers相对更偏向于具有低敏感度的函数。(ii) 当训练布尔函数时，Transformers和LSTMs都优先学习具有低敏感度的函数，最终Transformers收敛到具有更低敏感度的函数。(iii) 在具有低敏感度的稀疏布尔函数上，我们发现Transformers在存在噪音标签的情况下能够近乎完美地泛化，而LSTMs过拟合并且泛化精度较低。总体而言，我们的结果提供了强有力的可量化证据。

    Despite the widespread success of Transformers on NLP tasks, recent works have found that they struggle to model several formal languages when compared to recurrent models. This raises the question of why Transformers perform well in practice and whether they have any properties that enable them to generalize better than recurrent models. In this work, we conduct an extensive empirical study on Boolean functions to demonstrate the following: (i) Random Transformers are relatively more biased towards functions of low sensitivity. (ii) When trained on Boolean functions, both Transformers and LSTMs prioritize learning functions of low sensitivity, with Transformers ultimately converging to functions of lower sensitivity. (iii) On sparse Boolean functions which have low sensitivity, we find that Transformers generalize near perfectly even in the presence of noisy labels whereas LSTMs overfit and achieve poor generalization accuracy. Overall, our results provide strong quantifiable evidence
    
[^152]: 关于递归划分的逐点行为及其对异质因果效应估计的影响

    On the Pointwise Behavior of Recursive Partitioning and Its Implications for Heterogeneous Causal Effect Estimation. (arXiv:2211.10805v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.10805](http://arxiv.org/abs/2211.10805)

    本文质疑了递归划分在决策树学习中的应用，通过证明它们可能无法实现一致范数的多项式收敛速率。我们提出了随机森林来解决这个问题，将低性能的树转化为几乎最优的过程，但代价是失去了解释性，并引入了两个额外的调整参数。

    

    决策树学习在逐点推断中的应用日益增多。重要的应用包括异质因果治疗效应和动态政策决策，以及条件分位数回归和实验设计，在这些应用中，树的估计和推断是在特定的协变量值上进行的。在本文中，我们对使用决策树（通过自适应递归划分训练）进行此类目的提出了质疑，通过证明它们甚至可以在修剪的情况下无法实现一致范数的多项式收敛速率。相反，收敛速度可能是多项式对数级别的，或者在一些重要的特殊情况下，例如诚实回归树，完全失败。我们表明，随机森林可以解决这个问题，将低性能的树转化为几乎最优的过程，但代价是失去了解释性，并引入了两个额外的调整参数。随机森林的两个标志性特征是子采样和随机特征选择机制。

    Decision tree learning is increasingly being used for pointwise inference. Important applications include causal heterogenous treatment effects and dynamic policy decisions, as well as conditional quantile regression and design of experiments, where tree estimation and inference is conducted at specific values of the covariates. In this paper, we call into question the use of decision trees (trained by adaptive recursive partitioning) for such purposes by demonstrating that they can fail to achieve polynomial rates of convergence in uniform norm, even with pruning. Instead, the convergence may be poly-logarithmic or, in some important special cases, such as honest regression trees, fail completely. We show that random forests can remedy the situation, turning poor performing trees into nearly optimal procedures, at the cost of losing interpretability and introducing two additional tuning parameters. The two hallmarks of random forests, subsampling and the random feature selection mecha
    
[^153]: 基于在线连续学习的边缘计算高效压缩比估计

    Efficient Compressed Ratio Estimation using Online Sequential Learning for Edge Computing. (arXiv:2211.04284v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.04284](http://arxiv.org/abs/2211.04284)

    本研究提出了一种基于AC-OSELM的高效RL方法，能够在边缘设备上估计合适的压缩比，从而实现数据的高效压缩，同时保持重构数据的准确性。

    

    随着物联网的广泛应用，大量的传感器信息正在实时采集。因此，从边缘设备传输数据的通讯成本不断增加。压缩感知（CS）是一种可用于边缘设备的数据压缩方法，并因其可节省通讯成本而备受关注。在压缩感知中，估计合适的压缩比是重要的。现有的利用强化学习（RL）自适应估计获得数据的压缩比的方法，其计算成本常常很高。本研究开发了一种针对边缘设备的高效RL方法，称为actor-critic在线连续极限学习机（AC-OSELM），并利用AC-OSELM开发了一种在边缘设备上通过估计适当的压缩比压缩数据的系统。通过使用实际传感器数据进行实验，评估了所提出方法在估计压缩比和重构压缩数据方面的性能。结果表明我们提出的方法在保持重构压缩数据的准确性的同时，提供了比强化学习和传统方法更高效的压缩比估计方法。

    Owing to the widespread adoption of the Internet of Things, a vast amount of sensor information is being acquired in real time. Accordingly, the communication cost of data from edge devices is increasing. Compressed sensing (CS), a data compression method that can be used on edge devices, has been attracting attention as a method to reduce communication costs. In CS, estimating the appropriate compression ratio is important. There is a method to adaptively estimate the compression ratio for the acquired data using reinforcement learning (RL). However, the computational costs associated with existing RL methods that can be utilized on edges are often high. In this study, we developed an efficient RL method for edge devices, referred to as the actor--critic online sequential extreme learning machine (AC-OSELM), and a system to compress data by estimating an appropriate compression ratio on the edge using AC-OSELM. The performance of the proposed method in estimating the compression ratio
    
[^154]: 模型驱动泛化在强化学习中的效益

    The Benefits of Model-Based Generalization in Reinforcement Learning. (arXiv:2211.02222v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02222](http://arxiv.org/abs/2211.02222)

    这篇论文研究了模型驱动泛化在强化学习中的效益，通过学习模型生成的数据可以提供更多有用的信息，从而提高强化学习的效果。

    

    模型驱动的强化学习（RL）被广泛认为具有提高样本效率的潜力，因为它允许代理器合成大量的想象经验。经验回放（ER）可以被视为一种简单的模型，已被证明能够提高深度强化学习的稳定性和效率。理论上，一个学习的参数化模型可以通过从真实经验中泛化来增添数据集中的其他可信经验，从而改进ER。然而，考虑到学习的值函数也具有泛化能力，模型泛化为何更好并不明显。在这篇论文中，我们提供了理论和实证研究结果，揭示了学习模型生成的数据何时以及如何能够有效。首先，我们提供了一个简单的定理，解释了通过学习模型作为中间步骤比直接使用贝尔曼方程从数据中学习值函数如何缩小可能值函数集合的范围。其次，我们提供实证结果，证明通过学习模型生成的数据可以提供更多有用的信息，从而提高强化学习的效果。

    Model-Based Reinforcement Learning (RL) is widely believed to have the potential to improve sample efficiency by allowing an agent to synthesize large amounts of imagined experience. Experience Replay (ER) can be considered a simple kind of model, which has proved effective at improving the stability and efficiency of deep RL. In principle, a learned parametric model could improve on ER by generalizing from real experience to augment the dataset with additional plausible experience. However, given that learned value functions can also generalize, it is not immediately obvious why model generalization should be better. Here, we provide theoretical and empirical insight into when, and how, we can expect data generated by a learned model to be useful. First, we provide a simple theorem motivating how learning a model as an intermediate step can narrow down the set of possible value functions more than learning a value function directly from data using the Bellman equation. Second, we prov
    
[^155]: 将已知运算符注入卷积神经网络用于超声弹性成像中的横向应变成像

    Infusing known operators in convolutional neural networks for lateral strain imaging in ultrasound elastography. (arXiv:2211.00172v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2211.00172](http://arxiv.org/abs/2211.00172)

    该论文提出了一种将已知运算符注入卷积神经网络的方法，用于改善超声弹性成像中横向应变成像的质量。目前存在的问题包括低采样频率、运动受限和缺乏横向方向的相位信息。最近提出的一种基于物理启发的无监督正则化弹性成像方法已取得实质性改进，但仅在训练期间应用正则化，无法保证在测试期间横向应变处于可行范围内。

    

    卷积神经网络（CNN）已被用于超声弹性成像中的位移估计。该网络可以估计高质量的纵向应变（在纵向方向上的位移导数）。相比纵向应变，横向应变在泊松比成像和弹性重建中具有较差的质量。其主要原因包括采样频率低、运动受限以及横向方向缺乏相位信息。最近，基于物理启发的无监督正则化弹性成像（PICTURE）方法被提出。该方法考虑了运动物理规则定义的可行横向应变范围，并使用正则化策略改善了横向应变。尽管取得了实质性改进，但正则化仅在训练期间应用，因此无法保证在测试期间横向应变处于可行范围内。

    Convolutional Neural Networks (CNN) have been employed for displacement estimation in ultrasound elastography (USE). High-quality axial strains (derivative of the axial displacement in the axial direction) can be estimated by the proposed networks. In contrast to axial strain, lateral strain, which is highly required in Poisson's ratio imaging and elasticity reconstruction, has a poor quality. The main causes include low sampling frequency, limited motion, and lack of phase information in the lateral direction. Recently, physically inspired constraint in unsupervised regularized elastography (PICTURE) has been proposed. This method took into account the range of the feasible lateral strain defined by the rules of physics of motion and employed a regularization strategy to improve the lateral strains. Despite the substantial improvement, the regularization was only applied during the training; hence it did not guarantee during the test that the lateral strain is within the feasible rang
    
[^156]: 基于黑盒验证算法的自我改进强化学习驾驶安全性能的研究

    Self-Improving Safety Performance of Reinforcement Learning Based Driving with Black-Box Verification Algorithms. (arXiv:2210.16575v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.16575](http://arxiv.org/abs/2210.16575)

    本研究提出了一种使用黑盒验证方法来增强强化学习驾驶的安全性能的自我改进的人工智能系统。该方法通过发现自动驾驶的失败场景并重新训练，以改善在训练中缺乏的安全关键场景的性能。模拟结果表明该方法在自适应巡航控制应用中取得了显著的效果。

    

    本文提出了一种自我改进的人工智能系统，利用黑盒验证方法来增强基于强化学习的自主驾驶代理的安全性能。最近几年来，强化学习算法在自主驾驶应用中越来越受欢迎。然而，现有的强化学习算法的性能严重依赖于训练场景的多样性。在训练阶段缺乏安全关键的场景可能导致在真实驾驶应用中的泛化性能较差。我们提出了一个新的框架，通过黑盒验证方法探索训练集的弱点。发现自动驾驶失败场景后，通过迁移学习重新启动RL代理的训练，以改进先前不安全的场景的性能。模拟结果表明，我们的方法有效地发现了基于强化学习的自适应巡航控制（ACC）应用中行为决策的安全失败，并且取得了显著的效果。

    In this work, we propose a self-improving artificial intelligence system to enhance the safety performance of reinforcement learning (RL)-based autonomous driving (AD) agents using black-box verification methods. RL algorithms have become popular in AD applications in recent years. However, the performance of existing RL algorithms heavily depends on the diversity of training scenarios. A lack of safety-critical scenarios during the training phase could result in poor generalization performance in real-world driving applications. We propose a novel framework in which the weaknesses of the training set are explored through black-box verification methods. After discovering AD failure scenarios, the RL agent's training is re-initiated via transfer learning to improve the performance of previously unsafe scenarios. Simulation results demonstrate that our approach efficiently discovers safety failures of action decisions in RL-based adaptive cruise control (ACC) applications and significant
    
[^157]: 偏好子采样对于随机梯度Langevin动力学的研究

    Preferential Subsampling for Stochastic Gradient Langevin Dynamics. (arXiv:2210.16189v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.16189](http://arxiv.org/abs/2210.16189)

    本文提出一种偏好子采样的方法来对随机梯度Langevin动力学进行优化，通过使用非均匀概率分布子采样对具有更大影响的数据点进行加权，同时还通过自适应调整子采样大小来提高梯度估计的准确性。实验证明这种方法可以在减少子采样数的同时保持相同的精度水平。

    

    随机梯度MCMC（SGMCMC）通过使用小型、均匀加权的数据子样本构建对于对数后验梯度的无偏估计，为传统MCMC提供了可扩展的替代方法。虽然计算高效，但由此产生的梯度估计可能具有较高的方差，并且会影响采样器性能。传统上，方差控制问题通过构建更好的随机梯度估计器来解决，通常使用控制变量。我们提议使用离散的非均匀概率分布来偏好地子采样对于对梯度产生更大影响的数据点。此外，我们还提出在算法的每次迭代中自适应地调整子采样大小的方法，以便在难以估计梯度的样本空间中增加子采样大小。我们证明了这种方法可以在大幅减少平均子采样数的同时保持相同的精度水平。

    Stochastic gradient MCMC (SGMCMC) offers a scalable alternative to traditional MCMC, by constructing an unbiased estimate of the gradient of the log-posterior with a small, uniformly-weighted subsample of the data. While efficient to compute, the resulting gradient estimator may exhibit a high variance and impact sampler performance. The problem of variance control has been traditionally addressed by constructing a better stochastic gradient estimator, often using control variates. We propose to use a discrete, non-uniform probability distribution to preferentially subsample data points that have a greater impact on the stochastic gradient. In addition, we present a method of adaptively adjusting the subsample size at each iteration of the algorithm, so that we increase the subsample size in areas of the sample space where the gradient is harder to estimate. We demonstrate that such an approach can maintain the same level of accuracy while substantially reducing the average subsample s
    
[^158]: 有条件风险厌恶的上下文赌博问题

    Conditionally Risk-Averse Contextual Bandits. (arXiv:2210.13573v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.13573](http://arxiv.org/abs/2210.13573)

    设计出了第一个具有在线遗憾保证的风险厌恶的上下文赌博算法，并在多个实验场景中展示了其适用性。

    

    在风险厌恶的情况下，具有平均统计保证的上下文赌博问题是不足够的，因为它们可能通过牺牲最坏情况的表现来获得更好的平均性能。设计一个风险厌恶的上下文赌博算法具有挑战性，因为探索是必要的，但风险厌恶对整个奖励分布都很敏感；尽管如此，我们展示了第一个具有在线遗憾保证的风险厌恶的上下文赌博算法。我们在多种场景下进行了实验，这些场景中最坏情况的结果应该被避免，包括动态定价、库存管理和自我调整软件；其中还包括一个生产级的扩展数据处理系统。

    Contextual bandits with average-case statistical guarantees are inadequate in risk-averse situations because they might trade off degraded worst-case behaviour for better average performance. Designing a risk-averse contextual bandit is challenging because exploration is necessary but risk-aversion is sensitive to the entire distribution of rewards; nonetheless we exhibit the first risk-averse contextual bandit algorithm with an online regret guarantee. We conduct experiments from diverse scenarios where worst-case outcomes should be avoided, from dynamic pricing, inventory management, and self-tuning software; including a production exascale data processing system.
    
[^159]: 使用神经切线核对图神经网络中的卷积，非线性和深度进行分析

    Analysis of Convolutions, Non-linearity and Depth in Graph Neural Networks using Neural Tangent Kernel. (arXiv:2210.09809v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09809](http://arxiv.org/abs/2210.09809)

    本研究通过理论方法分析了图神经网络中的卷积、非线性和深度对网络性能的影响，同时对基于图拉普拉斯和邻接矩阵的归一化方法进行了比较，并揭示了线性GNN与非线性ReLU-GNN性能相当的现象缺乏严格的理论解释。

    

    图神经网络（GNN）的基本原理是通过使用“图卷积”来聚合相邻节点的结构信息，并选择合适的网络架构（例如深度和激活函数）。因此，理解每个设计选择对网络性能的影响至关重要。基于图拉普拉斯的卷积已成为主流选择，其中对邻接矩阵进行对称归一化是最广泛采用的方法。然而，一些经验研究表明，行归一化的邻接矩阵在节点分类方面表现更好。尽管GNN的使用非常广泛，但目前尚无严格的理论研究关于这些卷积的表示能力，无法解释这种行为。同样，线性GNN的性能与非线性ReLU-GNN的性能相当的经验观察也缺乏严格的理论支持。

    The fundamental principle of Graph Neural Networks (GNNs) is to exploit the structural information of the data by aggregating the neighboring nodes using a `graph convolution' in conjunction with a suitable choice for the network architecture, such as depth and activation functions. Therefore, understanding the influence of each of the design choice on the network performance is crucial. Convolutions based on graph Laplacian have emerged as the dominant choice with the symmetric normalization of the adjacency matrix as the most widely adopted one. However, some empirical studies show that row normalization of the adjacency matrix outperforms it in node classification. Despite the widespread use of GNNs, there is no rigorous theoretical study on the representation power of these convolutions, that could explain this behavior. Similarly, the empirical observation of the linear GNNs performance being on par with non-linear ReLU GNNs lacks rigorous theory.  In this work, we theoretically a
    
[^160]: 用神经网络排除Baikal-GVD数据中的噪声

    Rejecting noise in Baikal-GVD data with neural networks. (arXiv:2210.04653v2 [astro-ph.IM] UPDATED)

    [http://arxiv.org/abs/2210.04653](http://arxiv.org/abs/2210.04653)

    本研究引入了一种神经网络，使用U-net-like结构和事件的时间结构，成功地将Baikal-GVD数据中的噪声和信号分离开来，并取得了高精确度和召回率的结果。

    

    Baikal-GVD是一个安装在贝加尔湖淡水中的大型（约1 km³）水下中微子望远镜。深湖水环境充斥着背景光，可以被Baikal-GVD的光传感器检测到。我们引入了一种神经网络，用于有效地将这些噪声击中与通过探测器传播的相对论粒子的信号击中分离开来。该模型采用了类似U-net的结构，并利用了事件的时间（因果）结构。神经网络的指标在蒙特卡洛模拟数据集上达到了99％的信号纯度（精确度）和96％的生存效率（召回率）。我们将开发的方法与算法方法进行了比较，并讨论了其他可能的神经网络架构，包括基于图的架构。

    Baikal-GVD is a large ($\sim$1 km$^3$) underwater neutrino telescope installed in the fresh waters of Lake Baikal. The deep lake water environment is pervaded by background light, which is detectable by Baikal-GVD's photosensors. We introduce a neural network for an efficient separation of these noise hits from the signal ones, stemming from the propagation of relativistic particles through the detector. The model has a U-net-like architecture and employs temporal (causal) structure of events. The neural network's metrics reach up to 99\% signal purity (precision) and 96\% survival efficiency (recall) on Monte-Carlo simulated dataset. We compare the developed method with the algorithmic approach to rejecting the noise and discuss other possible architectures of neural networks, including graph-based ones.
    
[^161]: 数据标准化在脑电领域适应中的影响研究

    On The Effects Of Data Normalisation For Domain Adaptation On EEG Data. (arXiv:2210.01081v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01081](http://arxiv.org/abs/2210.01081)

    本文研究了数据标准化对脑电领域适应的影响，并在三个EEG数据集上进行了实验评估。

    

    在机器学习文献中，数据集转换问题一直是一个众所周知的问题，与机器学习标准假设不同的是，训练集和测试集中的数据可能遵循不同的概率分布，导致机器学习系统的泛化性能不佳。这个问题在脑机接口领域尤为突出，因为生物信号如脑电图信号通常被用于数据处理。然而，脑电信号在时间和不同受试者之间都非常不稳定。为了解决这个问题，许多提出的解决方法都基于最近的转移学习方法，如领域适应。然而，在许多情况下，改进的实际原因仍然模糊。本文着重研究数据标准化或标准化策略在领域适应方法中的影响。具体而言，本文使用SEED、DEAP和BCI竞赛IV 2a EEG数据集，对数据标准化策略应用于领域适应方法中的影响进行了实验评估。

    In the Machine Learning (ML) literature, a well-known problem is the Dataset Shift problem where, differently from the ML standard hypothesis, the data in the training and test sets can follow different probability distributions, leading ML systems toward poor generalisation performances. This problem is intensely felt in the Brain-Computer Interface (BCI) context, where bio-signals as Electroencephalographic (EEG) are often used. In fact, EEG signals are highly non-stationary both over time and between different subjects. To overcome this problem, several proposed solutions are based on recent transfer learning approaches such as Domain Adaption (DA). In several cases, however, the actual causes of the improvements remain ambiguous. This paper focuses on the impact of data normalisation, or standardisation strategies applied together with DA methods. In particular, using \textit{SEED}, \textit{DEAP}, and \textit{BCI Competition IV 2a} EEG datasets, we experimentally evaluated the impa
    
[^162]: 基于组合评分建模的基于模拟推断

    Compositional Score Modeling for Simulation-based Inference. (arXiv:2209.14249v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14249](http://arxiv.org/abs/2209.14249)

    本研究提出了一种基于条件评分建模的方法，可以有效处理基于多个观测条件下得到的后验分布，同时具有高样本效率和聚合多个观测值的优势。

    

    模拟推断的神经后验估计方法在处理基于多个观测条件下得到的后验分布时可能不太适用，因为它们倾向于需要大量的模拟器调用来学习准确的近似。相比之下，神经似然估计方法可以在学习了单独观测值后，处理推断时的多个观测值，但它们依赖于标准的推断方法，如MCMC或变分推断，这些推断方法有一定的性能缺陷。我们引入了一种基于条件评分建模的新方法，它融合了两种方法的优点。我们对由单个观测引起的（扩散的）后验分布建模，然后引入了一种组合学习分数以近似从目标后验分布中采样的方法。我们的方法在样本效率上具有优势，在推断时可以自然地聚合多个观测值，并避免了传统推断方法的缺点。

    Neural Posterior Estimation methods for simulation-based inference can be ill-suited for dealing with posterior distributions obtained by conditioning on multiple observations, as they tend to require a large number of simulator calls to learn accurate approximations. In contrast, Neural Likelihood Estimation methods can handle multiple observations at inference time after learning from individual observations, but they rely on standard inference methods, such as MCMC or variational inference, which come with certain performance drawbacks. We introduce a new method based on conditional score modeling that enjoys the benefits of both approaches. We model the scores of the (diffused) posterior distributions induced by individual observations, and introduce a way of combining the learned scores to approximately sample from the target posterior distribution. Our approach is sample-efficient, can naturally aggregate multiple observations at inference time, and avoids the drawbacks of standa
    
[^163]: 未知动态环境下快速运动规划的障碍物识别与椭球分解

    Obstacle Identification and Ellipsoidal Decomposition for Fast Motion Planning in Unknown Dynamic Environments. (arXiv:2209.14233v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.14233](http://arxiv.org/abs/2209.14233)

    本文提出一种基于椭球的障碍物识别与测速方法，并定义了基于椭球的特征向量，能够适用于带有静态和动态障碍物的环境，其运行速度比现有算法更快且不需要预先知道聚类数量。

    

    在无人系统中，避免与未知动态障碍物的碰撞是最重要的挑战之一。本文提出了一种通过椭球来识别障碍物，并估计其线性和角速度的方法。我们的方法基于一个任何物体都可以近似表示为椭球的理念。为了实现这一点，我们提出了一种基于变分贝叶斯估计高斯混合模型、Kyachiyan算法和优化算法的方法。与现有的基于优化的方法不同，我们提出的方法不需要知道聚类数目，并且可以实时操作。此外，我们定义了基于椭球的特征向量，以匹配给定的两个时间接近的点帧的障碍物。我们的方法可以应用于任何带有静态和动态障碍物的环境，包括具有旋转障碍物的环境。我们将我们的算法与其他聚类方法进行了比较，并展示了其优越性。

    Collision avoidance in the presence of dynamic obstacles in unknown environments is one of the most critical challenges for unmanned systems. In this paper, we present a method that identifies obstacles in terms of ellipsoids to estimate linear and angular obstacle velocities. Our proposed method is based on the idea of any object can be approximately expressed by ellipsoids. To achieve this, we propose a method based on variational Bayesian estimation of Gaussian mixture model, the Kyachiyan algorithm, and a refinement algorithm. Our proposed method does not require knowledge of the number of clusters and can operate in real-time, unlike existing optimization-based methods. In addition, we define an ellipsoid-based feature vector to match obstacles given two timely close point frames. Our method can be applied to any environment with static and dynamic obstacles, including the ones with rotating obstacles. We compare our algorithm with other clustering methods and show that when coupl
    
[^164]: DynDepNet:通过动态图结构学习从fMRI数据中学习时变的依赖关系结构

    DynDepNet: Learning Time-Varying Dependency Structures from fMRI Data via Dynamic Graph Structure Learning. (arXiv:2209.13513v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.13513](http://arxiv.org/abs/2209.13513)

    DynDepNet是一种学习fMRI数据中时变依赖结构的新方法，在性别分类任务中取得了最先进的结果。

    

    图神经网络（GNNs）在学习基于功能磁共振成像（fMRI）数据的大脑图表示方面取得了成功。然而，现有的GNN方法假设大脑图在时间上是静态的，并且在模型训练之前已知图邻接矩阵。这些假设与证据相矛盾，即大脑图在时间上是变化的，并且其连接结构取决于功能连接测量的选择。用噪声大脑图错误地表示fMRI数据会对GNN的性能产生不利影响。为了解决这个问题，我们提出了DynDepNet，一种通过下游预测任务所引发的fMRI数据的最优时变依赖结构的学习方法。对于性别分类任务，对真实世界的fMRI数据集进行的实验证明DynDepNet取得了最先进的结果，准确率分别比最佳基准线提高了约8个百分点和6个百分点。

    Graph neural networks (GNNs) have demonstrated success in learning representations of brain graphs derived from functional magnetic resonance imaging (fMRI) data. However, existing GNN methods assume brain graphs are static over time and the graph adjacency matrix is known prior to model training. These assumptions contradict evidence that brain graphs are time-varying with a connectivity structure that depends on the choice of functional connectivity measure. Incorrectly representing fMRI data with noisy brain graphs can adversely affect GNN performance. To address this, we propose DynDepNet, a novel method for learning the optimal time-varying dependency structure of fMRI data induced by downstream prediction tasks. Experiments on real-world fMRI datasets, for the task of sex classification, demonstrate that DynDepNet achieves state-of-the-art results, outperforming the best baseline in terms of accuracy by approximately 8 and 6 percentage points, respectively. Furthermore, analysis 
    
[^165]: Bayan算法：通过对模块度的精确和近似优化来检测网络中的社区

    The Bayan Algorithm: Detecting Communities in Networks Through Exact and Approximate Optimization of Modularity. (arXiv:2209.04562v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2209.04562](http://arxiv.org/abs/2209.04562)

    提出了一种名为Bayan的社区检测算法，通过精确或近似优化模块度的方法，它能够返回最优或接近最优的分区，并且比其他算法快数倍，并能够在合成和真实网络数据集上准确地找到地面真实社区。

    

    社区检测是网络科学中的经典问题，具有广泛的应用。在众多方法中，最常见的方法是最大化模块度。尽管启发式模块度最大化算法设计理念和广泛采用，但很少返回最佳分区或类似分区。我们提出了一种专门的算法Bayan，它返回具有最优或接近最优分区保证的分区。Bayan算法的核心是一种分支限界方案，它解决了问题的整数规划公式以达到最优或近似最优的目的。我们证明Bayan在合成基准和真实网络节点标签的检索地面真实社区方面具有独特的准确性和稳定性，比其他21种算法快数倍，可以找到最优分区的实例。

    Community detection is a classic problem in network science with extensive applications in various fields. Among numerous approaches, the most common method is modularity maximization. Despite their design philosophy and wide adoption, heuristic modularity maximization algorithms rarely return an optimal partition or anything similar. We propose a specialized algorithm, Bayan, which returns partitions with a guarantee of either optimality or proximity to an optimal partition. At the core of the Bayan algorithm is a branch-and-cut scheme that solves an integer programming formulation of the problem to optimality or approximate it within a factor. We demonstrate Bayan's distinctive accuracy and stability over 21 other algorithms in retrieving ground-truth communities in synthetic benchmarks and node labels in real networks. Bayan is several times faster than open-source and commercial solvers for modularity maximization making it capable of finding optimal partitions for instances that c
    
[^166]: 防御语音认证中的数据中毒攻击

    Defend Data Poisoning Attacks on Voice Authentication. (arXiv:2209.04547v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2209.04547](http://arxiv.org/abs/2209.04547)

    本文展示了一种易于实施的语音认证系统数据中毒攻击，并提出了一种更加强大的防御方法，称为Guardian，它是一种基于卷积神经网络的鉴别器。

    

    随着深度学习的进展，说话人验证已经取得了非常高的准确性，并且正在作为一种生物特征认证选项在我们日常生活的许多场景中越来越受欢迎，特别是在不断增长的网络服务市场中。与传统密码相比，"声音密码"更方便，因为它们使人们不必记忆不同的密码。然而，新的机器学习攻击正在使这些语音认证系统面临风险。在没有强大的安全保证的情况下，攻击者可以通过欺骗基于深度神经网络（DNN）的语音识别模型来访问合法用户的网络账户。在本文中，我们展示了一种易于实施的语音认证系统数据中毒攻击，这种攻击几乎难以被现有的防御机制捕捉到。因此，我们提出了一种更加强大的防御方法，称为Guardian，它是一种基于卷积神经网络的鉴别器。Guardian鉴别器结合了一系列新技术，包括...

    With the advances in deep learning, speaker verification has achieved very high accuracy and is gaining popularity as a type of biometric authentication option in many scenes of our daily life, especially the growing market of web services. Compared to traditional passwords, "vocal passwords" are much more convenient as they relieve people from memorizing different passwords. However, new machine learning attacks are putting these voice authentication systems at risk. Without a strong security guarantee, attackers could access legitimate users' web accounts by fooling the deep neural network (DNN) based voice recognition models. In this paper, we demonstrate an easy-to-implement data poisoning attack to the voice authentication system, which can hardly be captured by existing defense mechanisms. Thus, we propose a more robust defense method, called Guardian, which is a convolutional neural network-based discriminator. The Guardian discriminator integrates a series of novel techniques i
    
[^167]: 一种面向高维(鲁棒)Wasserstein对齐的数据相关方法

    A Data-dependent Approach for High Dimensional (Robust) Wasserstein Alignment. (arXiv:2209.02905v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.02905](http://arxiv.org/abs/2209.02905)

    本文提出了一种面向高维(鲁棒)Wasserstein对齐的数据相关方法，通过压缩高维度的几何模式来降低时间复杂度。

    

    许多实际问题可以被归结为两个几何模式的对齐。在计算机视觉领域，以往的许多研究都集中在对2D或3D模式的对齐上。最近，在高维度的对齐问题上发现了一些新的应用。然而，在算法方面的研究仍然相对有限。据我们所知，大多数现有方法只是对2D和3D情况的简单扩展，并且往往存在计算复杂性高的问题。在本文中，我们提出了一个有效的框架来压缩高维度的几何模式。任何现有的对齐方法都可以应用于压缩后的几何模式，从而显著降低时间复杂度。我们的思路来源于高维数据通常具有低内在维度的观察。我们的框架是一种“数据相关”方法，其复杂度取决于数据。

    Many real-world problems can be formulated as the alignment between two geometric patterns. Previously, a great amount of research focus on the alignment of 2D or 3D patterns in the field of computer vision. Recently, the alignment problem in high dimensions finds several novel applications in practice. However, the research is still rather limited in the algorithmic aspect. To the best of our knowledge, most existing approaches are just simple extensions of their counterparts for 2D and 3D cases, and often suffer from the issues such as high computational complexities. In this paper, we propose an effective framework to compress the high dimensional geometric patterns. Any existing alignment method can be applied to the compressed geometric patterns and the time complexity can be significantly reduced. Our idea is inspired by the observation that high dimensional data often has a low intrinsic dimension. Our framework is a ``data-dependent'' approach that has the complexity depending 
    
[^168]: 持续学习，快与慢

    Continual Learning, Fast and Slow. (arXiv:2209.02370v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.02370](http://arxiv.org/abs/2209.02370)

    这篇论文提出了DualNets，一个通用的持续学习框架，通过结合快速学习和慢速学习系统，实现了更好的深度神经网络的持续学习。

    

    根据神经科学中的补充学习系统（CLS）理论，人类通过两个互补系统有效地进行持续学习：一个以海马体为中心的快速学习系统，用于快速学习具体的个体经验；一个位于新皮质的慢速学习系统，用于逐渐获取关于环境的结构化知识。受到这一理论的启发，我们提出了DualNets（双网络），这是一个通用的持续学习框架，由一个快速学习系统和一个慢速学习系统组成，通过自监督学习（SSL）从特定任务中进行模式分离表示的监督学习和从任务无关的一般表示的表示学习。DualNets可以无缝地将这两种表示类型合并到一个整体框架中，以促进深度神经网络的更好持续学习。通过大量实验，我们展示了有希望的结果。

    According to the Complementary Learning Systems (CLS) theory~\cite{mcclelland1995there} in neuroscience, humans do effective \emph{continual learning} through two complementary systems: a fast learning system centered on the hippocampus for rapid learning of the specifics, individual experiences; and a slow learning system located in the neocortex for the gradual acquisition of structured knowledge about the environment. Motivated by this theory, we propose \emph{DualNets} (for Dual Networks), a general continual learning framework comprising a fast learning system for supervised learning of pattern-separated representation from specific tasks and a slow learning system for representation learning of task-agnostic general representation via Self-Supervised Learning (SSL). DualNets can seamlessly incorporate both representation types into a holistic framework to facilitate better continual learning in deep neural networks. Via extensive experiments, we demonstrate the promising results 
    
[^169]: 不同分布的数据价值

    The Value of Out-of-Distribution Data. (arXiv:2208.10967v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.10967](http://arxiv.org/abs/2208.10967)

    不同分布的数据可以对任务的泛化误差产生非单调的影响，使用少量不同分布的数据进行训练是有价值的。

    

    我们期望随着类似任务样本的增加，泛化误差会减小；而随着来自不同分布（OOD）任务样本的增加，泛化误差会增大。在这项工作中，我们展示了一个反直觉的现象：任务的泛化误差可以是样本从OOD任务中的数量的非单调函数。随着OOD样本数量的增加，目标任务的泛化误差在超过一个阈值之前会先减小后增大。换句话说，使用少量OOD数据进行训练是有价值的。我们在合成数据集上使用Fisher线性判别和计算机视觉基准数据集（如MNIST、CIFAR-10、CINIC-10、PACS和DomainNet）上的深度网络来展示和分析这一现象。在我们知道哪些样本属于OOD的理想情况下，我们展示了可以利用目标和OOD经验风险的适当加权目标来利用这些非单调趋势。尽管实际应用有限，但这表明如果我们能够检测到OOD样本，这种方法可能是有价值的。

    We expect the generalization error to improve with more samples from a similar task, and to deteriorate with more samples from an out-of-distribution (OOD) task. In this work, we show a counter-intuitive phenomenon: the generalization error of a task can be a non-monotonic function of the number of OOD samples. As the number of OOD samples increases, the generalization error on the target task improves before deteriorating beyond a threshold. In other words, there is value in training on small amounts of OOD data. We use Fisher's Linear Discriminant on synthetic datasets and deep networks on computer vision benchmarks such as MNIST, CIFAR-10, CINIC-10, PACS and DomainNet to demonstrate and analyze this phenomenon. In the idealistic setting where we know which samples are OOD, we show that these non-monotonic trends can be exploited using an appropriately weighted objective of the target and OOD empirical risk. While its practical utility is limited, this does suggest that if we can det
    
[^170]: 使用大型语言模型模拟多个人并复制人类实验研究

    Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies. (arXiv:2208.10264v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.10264](http://arxiv.org/abs/2208.10264)

    该论文介绍了一种新型测试方法，称为图灵实验（TE），用于评估语言模型的人类行为模拟能力。论文通过模拟经济学、心理语言学和社会心理学实验，成功复制了已有研究，并揭示了一种“超准确度扭曲”的现象。

    

    我们引入了一种称为图灵实验（TE）的新型测试，用于评估给定的语言模型（如GPT模型）在多大程度上可以模拟人类行为的不同方面。TE还可以揭示语言模型在模拟特定人类行为方面的一致性扭曲。与图灵测试不同，图灵实验需要模拟代表性参与人类受试者研究的样本。我们进行TEs，试图复制之前研究中确立的发现。我们设计了一种模拟TEs的方法，并举例说明其用于比较不同语言模型能够多大程度上复制经典经济学、心理语言学和社会心理学实验的能力：最终游戏、园路句子、米尔格拉姆电击实验和众人的智慧。在前三个TEs中，使用最新模型成功复制了现有的研究结果，而最后一个TE揭示了存在于最新模型中的“超准确度扭曲”。

    We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a "hyper-accuracy distortion" present in
    
[^171]: 双向信息反馈的双拍卖

    Double Auctions with Two-sided Bandit Feedback. (arXiv:2208.06536v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.06536](http://arxiv.org/abs/2208.06536)

    双拍卖市场中引入了双向信息反馈，通过置信界竞标和平均定价，能够实现有效的价格发现和降低参与者的遗憾。

    

    双拍卖使得多个买家和卖家在去中心化的环境下进行商品交易，从而支撑许多在线市场的运作。在这些市场中，买家和卖家通过竞标来竞争，但往往不事先知道自己的评估价值。由于分配和定价是通过竞标进行的，参与者的盈利能力以及市场的可持续性在很大程度上取决于通过重复互动来学习各自评估价值。我们在买家和卖家双方的信息反馈上对双拍卖市场进行了研究。我们展示了基于置信界竞标和“平均定价”的有效价格发现在参与者之间。特别地，买家和卖家的综合评估价值（也称为社会遗憾）在T轮中的遗憾为$O(\log(T)/\Delta)$，其中$\Delta$为最小价格差。此外，交换货物的买家和卖家在个体上达到了$O(\sqrt{T})$的遗憾。

    Double Auction enables decentralized transfer of goods between multiple buyers and sellers, thus underpinning functioning of many online marketplaces. Buyers and sellers compete in these markets through bidding, but do not often know their own valuation a-priori. As the allocation and pricing happens through bids, the profitability of participants, hence sustainability of such markets, depends crucially on learning respective valuations through repeated interactions. We initiate the study of Double Auction markets under bandit feedback on both buyers' and sellers' side. We show with confidence bound based bidding, and `Average Pricing' there is an efficient price discovery among the participants. In particular, the regret on combined valuation of the buyers and the sellers -- a.k.a. the social regret -- is $O(\log(T)/\Delta)$ in $T$ rounds, where $\Delta$ is the minimum price gap. Moreover, the buyers and sellers exchanging goods attain $O(\sqrt{T})$ regret, individually. The buyers an
    
[^172]: 自适应领域泛化通过在线争议最小化

    Adaptive Domain Generalization via Online Disagreement Minimization. (arXiv:2208.01996v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.01996](http://arxiv.org/abs/2208.01996)

    本文提出了一个自适应领域泛化的通用框架，通过在线争议最小化对源模型进行适应性修改，从而改善了领域泛化算法的性能。

    

    当部署和训练之间存在分布偏移时，深度神经网络的性能会显著下降。领域泛化（DG）旨在通过仅依赖一组源领域将模型安全地转移到未知的目标领域。尽管已经提出了各种DG方法，但最近的一项研究称为DomainBed揭示了大部分方法都没有超过简单的经验风险最小化（ERM）。为此，我们提出了一个通用框架，与现有的DG算法正交，并可以持续改进它们的性能。与先前的DG工作不同，该工作基于一个静态的源模型希望它是通用的，我们提出的AdaODM在测试时间自适应地修改源模型，以适应不同的目标领域。具体而言，我们在共享的领域通用特征提取器上创建多个领域特定分类器。特征提取器和分类器通过对抗训练的方式进行训练，其中特征提取器嵌入输入样本。

    Deep neural networks suffer from significant performance deterioration when there exists distribution shift between deployment and training. Domain Generalization (DG) aims to safely transfer a model to unseen target domains by only relying on a set of source domains. Although various DG approaches have been proposed, a recent study named DomainBed, reveals that most of them do not beat the simple Empirical Risk Minimization (ERM). To this end, we propose a general framework that is orthogonal to existing DG algorithms and could improve their performance consistently. Unlike previous DG works that stake on a static source model to be hopefully a universal one, our proposed AdaODM adaptively modifies the source model at test time for different target domains. Specifically, we create multiple domain-specific classifiers upon a shared domain-generic feature extractor. The feature extractor and classifiers are trained in an adversarial way, where the feature extractor embeds the input samp
    
[^173]: 基于梯度的双层优化在深度学习中的应用调查

    Gradient-based Bi-level Optimization for Deep Learning: A Survey. (arXiv:2207.11719v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.11719](http://arxiv.org/abs/2207.11719)

    基于梯度的双层优化方法在深度学习领域中被广泛应用，特别适用于超参数优化和元知识提取。本文提供了基于梯度的双层优化的定义、适用标准以及实用指南，对于初学者尤其有帮助。

    

    双层优化，特别是基于梯度的方法，在深度学习社区中被广泛应用于超参数优化和元知识提取。双层优化将一个问题嵌套在另一个问题之内，基于梯度的方法通过计算超梯度来解决外层任务，比传统方法如进化算法更高效。在这项调查中，我们首先给出了基于梯度的双层优化的形式化定义。接下来，我们界定了决定研究问题是否适合双层优化的标准，并提供了一个实用指南，指导将这些问题结构化为双层优化框架，这对于新手来说尤为有益。具体而言，有两种形式：单任务形式用于优化正则化参数和经过提取的数据，多任务形式用于提取元知识等。

    Bi-level optimization, especially the gradient-based category, has been widely used in the deep learning community including hyperparameter optimization and meta-knowledge extraction. Bi-level optimization embeds one problem within another and the gradient-based category solves the outer-level task by computing the hypergradient, which is much more efficient than classical methods such as the evolutionary algorithm. In this survey, we first give a formal definition of the gradient-based bi-level optimization. Next, we delineate criteria to determine if a research problem is apt for bi-level optimization and provide a practical guide on structuring such problems into a bi-level optimization framework, a feature particularly beneficial for those new to this domain. More specifically, there are two formulations: the single-task formulation to optimize hyperparameters such as regularization parameters and the distilled data, and the multi-task formulation to extract meta-knowledge such as 
    
[^174]: 对贝叶斯神经网络在对抗攻击下的鲁棒性的研究

    On the Robustness of Bayesian Neural Networks to Adversarial Attacks. (arXiv:2207.06154v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.06154](http://arxiv.org/abs/2207.06154)

    本文研究了贝叶斯神经网络在对抗攻击下的鲁棒性问题，证明了在大数据、超参数化极限下，BNN的后验具有梯度攻击的鲁棒性，这对于解决深度学习在安全关键应用中的脆弱性问题具有重要意义。

    

    在安全关键应用中，对抗攻击的脆弱性是深度学习广泛应用的主要障碍之一。尽管在实践和理论方面已经进行了大量努力，但训练出对抗攻击具有鲁棒性的深度学习模型仍然是一个未解决的问题。本文分析了大数据、超参数化极限下贝叶斯神经网络（BNNs）对抗攻击的几何性质。我们证明，在这个极限下，梯度攻击的脆弱性是由于数据分布的退化导致的，也就是当数据位于环境空间的一个低维子流形上时。作为直接结果，我们证明在这个极限下，BNN的后验对梯度攻击具有鲁棒性。关键是，我们证明了即使从后验中采样的每个神经网络对梯度攻击都具有脆弱性，损失函数对BNN后验分布的期望梯度仍然趋于零。在t上的实验结果表明了我们的发现。

    Vulnerability to adversarial attacks is one of the principal hurdles to the adoption of deep learning in safety-critical applications. Despite significant efforts, both practical and theoretical, training deep learning models robust to adversarial attacks is still an open problem. In this paper, we analyse the geometry of adversarial attacks in the large-data, overparameterized limit for Bayesian Neural Networks (BNNs). We show that, in the limit, vulnerability to gradient-based attacks arises as a result of degeneracy in the data distribution, i.e., when the data lies on a lower-dimensional submanifold of the ambient space. As a direct consequence, we demonstrate that in this limit BNN posteriors are robust to gradient-based adversarial attacks. Crucially, we prove that the expected gradient of the loss with respect to the BNN posterior distribution is vanishing, even when each neural network sampled from the posterior is vulnerable to gradient-based attacks. Experimental results on t
    
[^175]: 不确定性下学习基于论证的推理的犹豫树

    Indecision Trees: Learning Argument-Based Reasoning under Quantified Uncertainty. (arXiv:2206.12252v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.12252](http://arxiv.org/abs/2206.12252)

    本文介绍了一种修改决策树的方法，即犹豫树，该方法能够在不确定性下进行学习，推理和提供强健的标签分布，可用于其他推理系统。

    

    在现实世界中使用机器学习系统往往存在问题，比如不可解释的黑盒模型，不完美测量的确定性假设，或者只提供单一分类而不是概率分布。本文介绍了犹豫树，一种对决策树进行修改的方法，能够在不确定性下进行学习，能够进行不确定性推理，提供一个强健的标签分布，并可分解为一组逻辑论证用于其他推理系统。

    Using Machine Learning systems in the real world can often be problematic, with inexplicable black-box models, the assumed certainty of imperfect measurements, or providing a single classification instead of a probability distribution.  This paper introduces Indecision Trees, a modification to Decision Trees which learn under uncertainty, can perform inference under uncertainty, provide a robust distribution over the possible labels, and can be disassembled into a set of logical arguments for use in other reasoning systems.
    
[^176]: Survival Kernets: 可扩展且可解释的深度核生存分析模型，并具有准确性保证

    Survival Kernets: Scalable and Interpretable Deep Kernel Survival Analysis with an Accuracy Guarantee. (arXiv:2206.10477v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10477](http://arxiv.org/abs/2206.10477)

    Survival Kernets 是一种可扩展且可解释的深度核生存分析模型，能够在大规模数据集上进行模型解释和理论分析。它利用核函数估计个体的生存分布，通过训练集压缩方案进行数据分簇，因此具有较高的可视化能力和预测准确性保证。该模型在特定情况下的预测生存分布误差界限最优，且在测试时具有可扩展性。

    

    核生存分析模型通过核函数来估计个体的生存分布，核函数度量任意两个数据点之间的相似性。我们提出了一种新的深度核生存模型——生存kernet，该模型可以适用于大规模数据集，并且易于解释和进行理论分析。具体而言，训练数据根据一种最近发展的用于分类和回归的训练集压缩方案（称为核群）进行分簇。在测试时，每个数据点被表示为这些簇的加权组合，每个簇可以进行可视化展示。对于生存kernet的一个特殊情况，我们建立了一个有限样本误差界限，预测的生存分布在该界限下是最优的（除去一个对数因子）。在测试时具有可扩展性。

    Kernel survival analysis models estimate individual survival distributions with the help of a kernel function, which measures the similarity between any two data points. Such a kernel function can be learned using deep kernel survival models. In this paper, we present a new deep kernel survival model called a survival kernet, which scales to large datasets in a manner that is amenable to model interpretation and also theoretical analysis. Specifically, the training data are partitioned into clusters based on a recently developed training set compression scheme for classification and regression called kernel netting that we extend to the survival analysis setting. At test time, each data point is represented as a weighted combination of these clusters, and each such cluster can be visualized. For a special case of survival kernets, we establish a finite-sample error bound on predicted survival distributions that is, up to a log factor, optimal. Whereas scalability at test time is achiev
    
[^177]: MetaFed: 个性化医疗中基于循环知识蒸馏的跨联邦联邦学习

    MetaFed: Federated Learning among Federations with Cyclic Knowledge Distillation for Personalized Healthcare. (arXiv:2206.08516v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.08516](http://arxiv.org/abs/2206.08516)

    本文提出了一个名为MetaFed的新型框架，旨在在不同的联邦之间实现可信的联邦学习。通过循环知识蒸馏，MetaFed能够获取每个联邦的个性化模型，无需中央服务器，并且实验证明MetaFed在精度上超过了现有方法。

    

    联邦学习在建立模型时无需访问原始用户数据，尤其在医疗领域引起了越来越多的关注。然而在实际应用中，不同的联邦很少因为数据异构性和中央服务器的不信任/不存在等原因而共同工作。本文提出了一种名为MetaFed的新型框架，以促进不同联邦之间可信的联邦学习。通过所提出的循环知识蒸馏，MetaFed在没有中央服务器的情况下为每个联邦获取个性化模型。具体而言，MetaFed将每个联邦视为一个元分布，并以循环的方式聚合每个联邦的知识。训练分为两个部分：共同知识积累和个性化。对三个基准进行的综合实验表明，与基线方法相比（例如，与基础方法相比，对于PAMAP2，精度提高了10％+），没有服务器的MetaFed能够实现更高的准确性。

    Federated learning has attracted increasing attention to building models without accessing the raw user data, especially in healthcare. In real applications, different federations can seldom work together due to possible reasons such as data heterogeneity and distrust/inexistence of the central server. In this paper, we propose a novel framework called MetaFed to facilitate trustworthy FL between different federations. MetaFed obtains a personalized model for each federation without a central server via the proposed Cyclic Knowledge Distillation. Specifically, MetaFed treats each federation as a meta distribution and aggregates knowledge of each federation in a cyclic manner. The training is split into two parts: common knowledge accumulation and personalization. Comprehensive experiments on three benchmarks demonstrate that MetaFed without a server achieves better accuracy compared to state-of-the-art methods (e.g., 10%+ accuracy improvement compared to the baseline for PAMAP2) with f
    
[^178]: 学习最优二叉分类树的混合整数线性优化模型

    Mixed integer linear optimization formulations for learning optimal binary classification trees. (arXiv:2206.04857v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04857](http://arxiv.org/abs/2206.04857)

    本研究提出了四个混合整数线性优化（MILO）模型，旨在设计最优的二叉分类树，通过最大化正确分类的数据点数量和最小化分支节点数量。

    

    决策树是分类和回归的强大工具，在机器学习的快速发展领域吸引了许多研究人员的关注。决策树相对于其他方法的优点之一是其可解释性，通常优先于其他准确率较高但难以解释的方法。二叉分类树包括两种类型的节点：（i）分支节点，有两个孩子节点，在一组离散特征上评估数据点；（ii）叶节点，给出离散的预测结果。通过解决一个双目标优化问题，即（i）最大化正确分类的数据点数量，（ii）最小化分支节点数量，可以得到一个最优的二叉分类树。在本文中，我们提出了四个混合整数线性优化（MILO）模型来设计最优的二叉分类树：两个基于流的模型和两个基于割的模型。

    Decision trees are powerful tools for classification and regression that attract many researchers working in the burgeoning area of machine learning. One advantage of decision trees over other methods is their interpretability, which is often preferred over other higher accuracy methods that are relatively uninterpretable. A binary classification tree has two types of vertices: (i) branching vertices which have exactly two children and where datapoints are assessed on a set of discrete features; and (ii) leaf vertices at which datapoints are given a discrete prediction. An optimal binary classification tree can be obtained by solving a biobjective optimization problem that seeks to (i) maximize the number of correctly classified datapoints and (ii) minimize the number of branching vertices. In this paper, we propose four mixed integer linear optimization (MILO) formulations for designing optimal binary classification trees: two flow-based formulations and two-cut based formulations. We
    
[^179]: DORA：探索深度神经网络中的异常值表示

    DORA: Exploring outlier representations in Deep Neural Networks. (arXiv:2206.04530v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04530](http://arxiv.org/abs/2206.04530)

    本文提出了一种名为DORA的数据不可知框架，用于分析深度神经网络中的表征空间，并可以识别不符合人类直观认知的表征。

    

    尽管深度神经网络（DNN）在学习复杂抽象方面非常有效，但它们容易意外地从训练数据中学习到虚假的特征。为了确保模型的透明度，检查学习表示之间的关系至关重要，因为意外的概念往往表现为与所需的任务不符的异常。在这项工作中，我们介绍了DORA（Data-agnOstic Representation Analysis）：用于分析DNN表示空间的第一个数据不可知框架。我们的框架采用了所提出的表示之间的极端激活（EA）距离度量，在不访问任何数据的情况下利用网络内自说明能力。我们定量验证了度量的正确性和与人为定义的语义距离的一致性。EA距离与人类判断之间的一致性使我们能够确定表征，其基本概念被认为是不自然的。

    Although Deep Neural Networks (DNNs) are incredibly effective in learning complex abstractions, they are susceptible to unintentionally learning spurious artifacts from the training data. To ensure model transparency, it is crucial to examine the relationships between learned representations, as unintended concepts often manifest themselves to be anomalous to the desired task. In this work, we introduce DORA (Data-agnOstic Representation Analysis): the first data-agnostic framework for the analysis of the representation space of DNNs. Our framework employs the proposed Extreme-Activation (EA) distance measure between representations that utilizes self-explaining capabilities within the network without accessing any data. We quantitatively validate the metric's correctness and alignment with human-defined semantic distances. The coherence between the EA distance and human judgment enables us to identify representations whose underlying concepts would be considered unnatural by humans by
    
[^180]: 通过概率-概率映射实现有条件校准的预测分布：在银河红移估计和概率预测中的应用

    Conditionally Calibrated Predictive Distributions by Probability-Probability Map: Application to Galaxy Redshift Estimation and Probabilistic Forecasting. (arXiv:2205.14568v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.14568](http://arxiv.org/abs/2205.14568)

    本研究提出了一种名为Cal-PIT的方法，通过学习一个概率-概率映射，解决了预测分布的诊断和校准问题，来实现有条件校准。

    

    不确定性量化对于评估AI算法的预测能力至关重要。过去的研究致力于描述目标变量$y \in \mathbb{R}$在给定复杂输入特征$\mathbf{x} \in \mathcal{X}$的条件下的预测分布$F(y|\mathbf{x})$。然而，现有的预测分布（例如，归一化流和贝叶斯神经网络）往往缺乏条件校准，即给定输入$\mathbf{x}$的事件发生的概率与预测概率显著不同。当前的校准方法不能完全评估和实施有条件校准的预测分布。在这里，我们提出了一种名为Cal-PIT的方法，它通过从校准数据中学习一个概率-概率映射来同时解决预测分布的诊断和校准问题。关键思想是对概率积分变换分数进行$\mathbf{x}$的回归。估计的回归提供了对特征空间中条件覆盖的可解释诊断。

    Uncertainty quantification is crucial for assessing the predictive ability of AI algorithms. Much research has been devoted to describing the predictive distribution (PD) $F(y|\mathbf{x})$ of a target variable $y \in \mathbb{R}$ given complex input features $\mathbf{x} \in \mathcal{X}$. However, off-the-shelf PDs (from, e.g., normalizing flows and Bayesian neural networks) often lack conditional calibration with the probability of occurrence of an event given input $\mathbf{x}$ being significantly different from the predicted probability. Current calibration methods do not fully assess and enforce conditionally calibrated PDs. Here we propose \texttt{Cal-PIT}, a method that addresses both PD diagnostics and recalibration by learning a single probability-probability map from calibration data. The key idea is to regress probability integral transform scores against $\mathbf{x}$. The estimated regression provides interpretable diagnostics of conditional coverage across the feature space. 
    
[^181]: SeedGNN：用于有监督种子图匹配的图神经网络

    SeedGNN: Graph Neural Networks for Supervised Seeded Graph Matching. (arXiv:2205.13679v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13679](http://arxiv.org/abs/2205.13679)

    本文提出了一种名为SeedGNN的图神经网络架构，用于有监督种子图匹配。相比于半监督方法，SeedGNN能够从训练集中学习如何利用少量种子节点来匹配未见图，并通过计算和利用不同跳数的证人信息以及使用易匹配的节点对改善匹配效果的方式实现了显著的性能改进。

    

    越来越多的人对设计用于有监督种子图匹配的图神经网络（GNNs）表示兴趣，这旨在仅利用拓扑信息和少量种子节点来匹配两个无标签图。然而，大多数先前用于此任务的GNNs使用半监督方法，这要求大量种子并不能学习可转移到未见图的知识。相比之下，本文提出了一种新的有监督方法，可以从训练集中学习如何仅用少量种子来匹配未见图。我们的SeedGNN架构融合了几个新颖的设计，受有关种子图匹配的理论研究的启发：1）它可以学习计算和使用来自不同跳数的类似证人信息，以一种可以推广到不同大小的图的方式；2）它可以使用易匹配的节点对作为新的种子，在后续层次中改善匹配效果。我们在合成和真实世界的图上评估了SeedGNN，并展示了显著的性能改进。

    There is a growing interest in designing Graph Neural Networks (GNNs) for seeded graph matching, which aims to match two unlabeled graphs using only topological information and a small set of seed nodes. However, most previous GNNs for this task use a semi-supervised approach, which requires a large number of seeds and cannot learn knowledge that is transferable to unseen graphs. In contrast, this paper proposes a new supervised approach that can learn from a training set how to match unseen graphs with only a few seeds. Our SeedGNN architecture incorporates several novel designs, inspired by theoretical studies of seeded graph matching: 1) it can learn to compute and use witness-like information from different hops, in a way that can be generalized to graphs of different sizes; 2) it can use easily-matched node-pairs as new seeds to improve the matching in subsequent layers. We evaluate SeedGNN on synthetic and real-world graphs and demonstrate significant performance improvements ove
    
[^182]: DDAC-SpAM: 一种用特征划分和去相关性的方法拟合高维稀疏加性模型的分布式算法

    DDAC-SpAM: A Distributed Algorithm for Fitting High-dimensional Sparse Additive Models with Feature Division and Decorrelation. (arXiv:2205.07932v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.07932](http://arxiv.org/abs/2205.07932)

    DDAC-SpAM是一种在高维稀疏加性模型中利用特征划分和去相关的分布式算法。去相关操作使得每个局部估计器能够恢复每个加性组分的稀疏模式，同时不对变量之间的相关性结构施加严格的约束。该算法在理论和实证分析中证明了其有效性和效率，为拟合稀疏加性模型提供了实际解决方案。

    

    分布式统计学习已成为大规模数据分析的常用技术。现有的大部分工作都是关注观察值的划分，但我们提出了一种新的算法DDAC-SpAM，在高维稀疏加性模型中对特征进行划分。我们的方法包括三个步骤：划分、去相关和征服。去相关操作使每个局部估计器能够恢复每个加性组分的稀疏模式，而不对变量之间的相关性结构施加严格的约束。通过理论分析和在合成数据和真实数据上的实证结果，证明了该算法的有效性和效率。理论结果包括一致的稀疏模式恢复以及对每个加性函数组成部分的统计推断。我们的方法为拟合稀疏加性模型提供了实际可行的解决方案，在各个领域有着广泛的应用前景。

    Distributed statistical learning has become a popular technique for large-scale data analysis. Most existing work in this area focuses on dividing the observations, but we propose a new algorithm, DDAC-SpAM, which divides the features under a high-dimensional sparse additive model. Our approach involves three steps: divide, decorrelate, and conquer. The decorrelation operation enables each local estimator to recover the sparsity pattern for each additive component without imposing strict constraints on the correlation structure among variables. The effectiveness and efficiency of the proposed algorithm are demonstrated through theoretical analysis and empirical results on both synthetic and real data. The theoretical results include both the consistent sparsity pattern recovery as well as statistical inference for each additive functional component. Our approach provides a practical solution for fitting sparse additive models, with promising applications in a wide range of domains.
    
[^183]: KenSwQuAD - 一份适用于斯瓦希里语低资源语言的问答数据集

    KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource Language. (arXiv:2205.02364v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.02364](http://arxiv.org/abs/2205.02364)

    该研究开发了KenSwQuAD - 一份适用于斯瓦希里语低资源语言的问答数据集，以满足低资源语言中机器理解自然语言的需求。这个数据集是通过从斯瓦希里语低资源语言的原始故事文本中进行注释得到的，包含7,526个QA对。

    

    本研究的动机是为低资源语言开发问答数据集的需求，从而开发了斯瓦希里语问答数据集KenSwQuAD。该数据集是从斯瓦希里语低资源语言的原始故事文本中进行注释的，该语言主要在东非和世界其他地方使用。问答（QA）数据集对于机器理解自然语言的任务（如互联网搜索和对话系统）非常重要。机器学习系统需要训练数据，如本研究开发的黄金标准问答集。本项目聘请了注解员从Kencorpus项目收集的斯瓦希里语文本中制定QA对。该项目对总共2,585个文本进行了1,445个文本的注释，每个注释至少有5个QA对，最终形成了一个包含7,526个QA对的数据集。对12.5%的注释文本进行的质量保证确认了QA对的准确性。

    The need for Question Answering datasets in low resource languages is the motivation of this research, leading to the development of Kencorpus Swahili Question Answering Dataset, KenSwQuAD. This dataset is annotated from raw story texts of Swahili low resource language, which is a predominantly spoken in Eastern African and in other parts of the world. Question Answering (QA) datasets are important for machine comprehension of natural language for tasks such as internet search and dialog systems. Machine learning systems need training data such as the gold standard Question Answering set developed in this research. The research engaged annotators to formulate QA pairs from Swahili texts collected by the Kencorpus project, a Kenyan languages corpus. The project annotated 1,445 texts from the total 2,585 texts with at least 5 QA pairs each, resulting into a final dataset of 7,526 QA pairs. A quality assurance set of 12.5% of the annotated texts confirmed that the QA pairs were all correc
    
[^184]: 学习量子动力学的越域泛化

    Out-of-distribution generalization for learning quantum dynamics. (arXiv:2204.10268v3 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2204.10268](http://arxiv.org/abs/2204.10268)

    该论文在量子机器学习中证明了学习未知酉的越域泛化能力，并提出了使用乘积态来学习酉对纠缠态的作用，从而推动了在近期量子硬件上学习量子动力学的前景，并为经典和量子电路的编译提供了新的方法。

    

    泛化边界是评估量子机器学习（QML）训练数据要求的关键工具。最近的研究已经建立了量子神经网络（QNNs）在域内泛化的保证，其中训练和测试数据来自同一数据分布。然而，目前还没有关于QML中域外泛化的结果，其中我们要求训练出的模型在来自与训练分布不同的数据上表现良好。在这里，我们证明了对学习未知酉的越域泛化的能力。特别地，我们证明了即使仅仅训练到了乘积态，我们也可以学习到酉对纠缠态的作用。由于乘积态只需要使用单比特门就可以制备，这推动了在近期量子硬件上学习量子动力学的前景，并进一步为经典和量子电路的编译提供了新的方法。

    Generalization bounds are a critical tool to assess the training data requirements of Quantum Machine Learning (QML). Recent work has established guarantees for in-distribution generalization of quantum neural networks (QNNs), where training and testing data are drawn from the same data distribution. However, there are currently no results on out-of-distribution generalization in QML, where we require a trained model to perform well even on data drawn from a different distribution to the training distribution. Here, we prove out-of-distribution generalization for the task of learning an unknown unitary. In particular, we show that one can learn the action of a unitary on entangled states having trained only product states. Since product states can be prepared using only single-qubit gates, this advances the prospects of learning quantum dynamics on near term quantum hardware, and further opens up new methods for both the classical and quantum compilation of quantum circuits.
    
[^185]: 居民众包中空间欠报告差异的量化

    Quantifying Spatial Under-reporting Disparities in Resident Crowdsourcing. (arXiv:2204.08620v2 [stat.AP] UPDATED)

    [http://arxiv.org/abs/2204.08620](http://arxiv.org/abs/2204.08620)

    本文研究了城市治理中居民众包的问题，并提出了一种准确测量报道率的方法，使不同的报道率不再成为城市治理下游解决事件速度方面的不公平根源。

    

    现代城市治理在很大程度上依赖于众包(“协同生产”)，以识别 downed trees 和 power lines 等问题。一个主要问题是，居民不以相同的速率报告问题，不同的报告异质性直接转化为下游在解决事件的速度方面的差异。测量这样的欠报告是一个困难的统计任务，因为根据定义，我们不能观察到没有被报告的事件或报告事件第一次发生的时间。因此，不能单纯地区分低报道率和低基准真实事件率。我们开发了一种方法来识别(异质的)报道率，而不使用外部基准真实数据。我们的想法是，在相同事件的 $\textit{duplicate}$ 报告中的比率可以利用来消除报告率随事件发生而发生的不确定性。借助这个思想，我们将问题简化为标准的泊松率估计任务，尽管标题有很多技术术语，但论文讨论了如何通过众包来帮助城市治理，不同的报道率如何导致问题解决的差异。作者提出了一种准确测量报道率的方法，而不依赖于外部数据。他们利用多次报告的事件可以帮助区分事件是否发生以及其报道率。

    Modern city governance relies heavily on crowdsourcing ("co-production") to identify problems such as downed trees and power lines. A major concern is that residents do not report problems at the same rates, with reporting heterogeneity directly translating to downstream disparities in how quickly incidents can be addressed. Measuring such under-reporting is a difficult statistical task, as, by definition, we do not observe incidents that are not reported or when reported incidents first occurred. Thus, low reporting rates and low ground-truth incident rates cannot be naively distinguished. We develop a method to identify (heterogeneous) reporting rates, without using external ground truth data. Our insight is that rates on $\textit{duplicate}$ reports about the same incident can be leveraged to disambiguate whether an incident has occurred with its reporting rate once it has occurred. Using this idea, we reduce the question to a standard Poisson rate estimation task -- even though the
    
[^186]: 启动强化学习

    Jump-Start Reinforcement Learning. (arXiv:2204.02372v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.02372](http://arxiv.org/abs/2204.02372)

    本文介绍了一种元算法，可以使用离线数据、演示或现有策略来初始化强化学习策略，并且与任何强化学习方法兼容。通过使用导引策略来形成探索策略的初始状态课程，能够提高在模拟机器人任务中的性能。

    

    强化学习（RL）通过试错的方式提供了一个理论框架，从而不断改进智能体的行为。然而，从零开始高效地学习策略可能非常困难，特别是对于具有探索挑战的任务来说。在这种情况下，使用现有策略、离线数据或演示来初始化RL可能是可行的。然而，对RL进行这种初始化常常效果不佳，特别是对于基于值的方法。本文提出了一种可以使用离线数据、演示或现有策略来初始化RL策略，并且与任何RL方法兼容的元算法。具体而言，我们提出了一种名为Jump-Start Reinforcement Learning (JSRL)的算法，它利用两个策略来解决任务：一个导引策略和一个探索策略。通过使用导引策略为探索策略形成一个初始状态的课程，我们能够在一组模拟机器人任务中高效地提高性能。

    Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent's behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks with exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tas
    
[^187]: 基于PDE的对称双臂伯努利赌博机的分析

    A PDE-Based Analysis of the Symmetric Two-Armed Bernoulli Bandit. (arXiv:2202.05767v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.05767](http://arxiv.org/abs/2202.05767)

    本研究通过关联线性热方程的解，得到了对称双臂伯努利赌博机问题的minmax最优遗憾和伪遗憾的领先项。新的结果改进了先前的研究，并提供了新的非渐近边界。

    

    本研究探讨了一个版本的双臂伯努利赌博机问题，其中两个臂的平均值之和为1（即对称的双臂伯努利赌博机）。在臂之间的差距趋近于零且预测期数趋近于无穷大的情况下，我们通过将每个解与线性热方程的解关联，得到了该问题的minmax最优遗憾和伪遗憾的领先项。我们的结果改进了先前已知的结果；具体而言，在三种不同的差距缩放模式下，我们明确计算了这些领先项。此外，我们还得到了任何给定时间范围的新的非渐近边界。

    This work addresses a version of the two-armed Bernoulli bandit problem where the sum of the means of the arms is one (the symmetric two-armed Bernoulli bandit). In a regime where the gap between these means goes to zero and the number of prediction periods approaches infinity, we obtain the leading order terms of the minmax optimal regret and pseudoregret for this problem by associating each of them with a solution of a linear heat equation. Our results improve upon the previously known results; specifically, we explicitly compute these leading order terms in three different scaling regimes for the gap. Additionally, we obtain new non-asymptotic bounds for any given time horizon.
    
[^188]: 通过张量分解实现一致的协同过滤

    Consistent Collaborative Filtering via Tensor Decomposition. (arXiv:2201.11936v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2201.11936](http://arxiv.org/abs/2201.11936)

    本文提出了一种通过张量分解来实现一致协同过滤的新模型，它能够扩展传统的用户-物品偏好计算方法，使得在评估物品相对偏好时产生物品之间的交互，具有潜在的非线性态度。

    

    协同过滤是分析用户活动和构建物品推荐系统的事实标准。本文提出了一种基于隐式反馈的协同过滤新模型——切割反对称分解（SAD）。与传统技术不同，SAD通过对用户-物品交互的新颖三维张量视图引入了一个额外的物品的隐含向量。该向量将通过标准点乘计算出的用户-物品偏好扩展到一般内积，从而在评估物品的相对偏好时产生物品之间的交互。当向量折叠为1时，SAD降为最先进的协同过滤模型（SOTA），而本文允许从数据中估计其值。允许新物品向量的值与1不同具有深远的影响。这表明用户可能具有非线性态度。

    Collaborative filtering is the de facto standard for analyzing users' activities and building recommendation systems for items. In this work we develop Sliced Anti-symmetric Decomposition (SAD), a new model for collaborative filtering based on implicit feedback. In contrast to traditional techniques where a latent representation of users (user vectors) and items (item vectors) are estimated, SAD introduces one additional latent vector to each item, using a novel three-way tensor view of user-item interactions. This new vector extends user-item preferences calculated by standard dot products to general inner products, producing interactions between items when evaluating their relative preferences. SAD reduces to state-of-the-art (SOTA) collaborative filtering models when the vector collapses to 1, while in this paper we allow its value to be estimated from data. Allowing the values of the new item vector to be different from 1 has profound implications. It suggests users may have nonlin
    
[^189]: 快速可解释的贪婪树求和

    Fast Interpretable Greedy-Tree Sums. (arXiv:2201.11931v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.11931](http://arxiv.org/abs/2201.11931)

    FIGS是一种快速可解释的贪婪树求和算法，通过将逻辑规则与加法相结合，能够适应加性结构同时保持高度可解释性。在真实数据集上的实验表明，FIGS实现了最先进的预测性能，并在高风险领域如医学中展示了其实用性。

    

    现代机器学习取得了令人印象深刻的预测性能，但通常会牺牲解释性，这在高风险领域如医学中是一个关键考虑因素。在这种情况下，从业者通常使用高度可解释的决策树模型，但这些模型对加性结构存在归纳偏差。为了克服这种偏差，我们提出了快速可解释的贪婪树求和（FIGS），它将CART算法推广到同时增长可变数量的树进行求和。通过将逻辑规则与加法相结合，FIGS能够适应加性结构同时保持高度可解释性。对真实数据集的大量实验表明，FIGS实现了最先进的预测性能。为了展示FIGS在高风险领域的实用性，我们将FIGS改进为学习临床决策工具（CDIs），CDIs是指导临床决策的工具。具体来说，我们引入了FIGS的一个变种，称为G-FIGS，它考虑了加性结构的因素。

    Modern machine learning has achieved impressive prediction performance, but often sacrifices interpretability, a critical consideration in high-stakes domains such as medicine. In such settings, practitioners often use highly interpretable decision tree models, but these suffer from inductive bias against additive structure. To overcome this bias, we propose Fast Interpretable Greedy-Tree Sums (FIGS), which generalizes the CART algorithm to simultaneously grow a flexible number of trees in summation. By combining logical rules with addition, FIGS is able to adapt to additive structure while remaining highly interpretable. Extensive experiments on real-world datasets show that FIGS achieves state-of-the-art prediction performance. To demonstrate the usefulness of FIGS in high-stakes domains, we adapt FIGS to learn clinical decision instruments (CDIs), which are tools for guiding clinical decision-making. Specifically, we introduce a variant of FIGS known as G-FIGS that accounts for the 
    
[^190]: 具有安全约束的保守分布强化学习

    Conservative Distributional Reinforcement Learning with Safety Constraints. (arXiv:2201.07286v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.07286](http://arxiv.org/abs/2201.07286)

    本文提出了一种名为保守分布最大后验策略优化 (CDMPO) 的离策略强化学习算法，通过使用分布强化学习方法来估计Q函数和C函数，以及使用保守的价值函数损失来减少约束违反次数。

    

    安全探索可以被看作是一个受约束的马尔可夫决策问题，在这个问题中，预期的长期成本是受到约束的。先前的离策略算法通过引入拉格朗日松弛技术，将约束优化问题转换为相应的无约束对偶问题。然而，上述算法的成本函数提供了不准确的估计，并导致拉格朗日乘子学习的不稳定性。在本文中，我们提出了一种新颖的离策略强化学习算法，称为保守分布最大后验策略优化(CDMPO)。首先，为了准确判断当前情况是否满足约束条件，CDMPO采用分布强化学习方法来估计Q函数和C函数。然后，CDMPO使用保守的价值函数损失来减少探索过程中违约次数。此外，我们利用加权平均比例积分D进行优化。

    Safety exploration can be regarded as a constrained Markov decision problem where the expected long-term cost is constrained. Previous off-policy algorithms convert the constrained optimization problem into the corresponding unconstrained dual problem by introducing the Lagrangian relaxation technique. However, the cost function of the above algorithms provides inaccurate estimations and causes the instability of the Lagrange multiplier learning. In this paper, we present a novel off-policy reinforcement learning algorithm called Conservative Distributional Maximum a Posteriori Policy Optimization (CDMPO). At first, to accurately judge whether the current situation satisfies the constraints, CDMPO adapts distributional reinforcement learning method to estimate the Q-function and C-function. Then, CDMPO uses a conservative value function loss to reduce the number of violations of constraints during the exploration process. In addition, we utilize Weighted Average Proportional Integral D
    
[^191]: 面向社交感知机器人的联邦连续学习

    Federated Continual Learning for Socially Aware Robotics. (arXiv:2201.05527v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.05527](http://arxiv.org/abs/2201.05527)

    面向社交感知机器人的联邦连续学习方法结合了联邦学习和连续学习，通过分散学习和隐私保护，提升了社交机器人的个性化和用户适应性。

    

    从学习辅助到陪伴，社交机器人承诺提升日常生活的许多方面。然而，社交机器人尚未广泛应用，部分原因是它们不会根据新用户自适应行为，并且未提供足够的隐私保护。中心化学习通过阻止在线学习新经验和要求存储隐私敏感数据来限制了这些。在这项工作中，我们提出了一种分散学习的替代方案，改善了社交机器人的隐私和个性化。我们结合了两种机器学习方法，联邦学习和连续学习，以捕捉分布在物理上的机器人之间和在时间上的重复机器人相遇中的交互动态。我们定义了一组在分散机器人学习场景中应该平衡的标准。我们还开发了一个新的算法——弹性传输（Elastic Transfer），利用贝叶斯推断和在线学习来平衡个体存储和全局信息共享的需求。

    From learning assistance to companionship, social robots promise to enhance many aspects of daily life. However, social robots have not seen widespread adoption, in part because (1) they do not adapt their behavior to new users, and (2) they do not provide sufficient privacy protections. Centralized learning, whereby robots develop skills by gathering data on a server, contributes to these limitations by preventing online learning of new experiences and requiring storage of privacy-sensitive data. In this work, we propose a decentralized learning alternative that improves the privacy and personalization of social robots. We combine two machine learning approaches, Federated Learning and Continual Learning, to capture interaction dynamics distributed physically across robots and temporally across repeated robot encounters. We define a set of criteria that should be balanced in decentralized robot learning scenarios. We also develop a new algorithm -Elastic Transfer -- that leverages i
    
[^192]: SCORE: 在自对偶正则化下近似曲率信息

    SCORE: Approximating Curvature Information under Self-Concordant Regularization. (arXiv:2112.07344v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.07344](http://arxiv.org/abs/2112.07344)

    本文提出了SCORE框架，在无约束极小化问题中利用自对偶正则化近似曲率信息。我们还提出了GGN-SCORE算法，该算法通过更新极小化变量来加速收敛并改善模型泛化性能。

    

    在许多应用中，包含正则化函数的优化问题经常被求解。当我们寻求针对这些问题的二阶方法时，为了加速收敛，考虑到一些特定正则化函数的曲率信息可能是值得利用的。在本文中，我们提出了用于无约束极小化问题的SCORE（自对偶正则化）框架，该框架在牛顿减量框架中结合了二阶信息进行凸优化。我们提出了广义高斯-牛顿与自对偶正则化（GGN-SCORE）算法，该算法在每次接收到新的输入批次时更新极小化变量。该算法利用了Hessian矩阵中二阶信息的结构，从而减少了计算开销。GGN-SCORE演示了如何加速收敛，同时改善模型的泛化性能。

    Optimization problems that include regularization functions in their objectives are regularly solved in many applications. When one seeks second-order methods for such problems, it may be desirable to exploit specific properties of some of these regularization functions when accounting for curvature information in the solution steps to speed up convergence. In this paper, we propose the SCORE (self-concordant regularization) framework for unconstrained minimization problems which incorporates second-order information in the Newton-decrement framework for convex optimization. We propose the generalized Gauss-Newton with Self-Concordant Regularization (GGN-SCORE) algorithm that updates the minimization variables each time it receives a new input batch. The proposed algorithm exploits the structure of the second-order information in the Hessian matrix, thereby reducing computational overhead. GGN-SCORE demonstrates how to speed up convergence while also improving model generalization for 
    
[^193]: 非参数回归中的相变

    Phase transitions in nonparametric regressions. (arXiv:2112.03626v6 [math.ST] UPDATED)

    [http://arxiv.org/abs/2112.03626](http://arxiv.org/abs/2112.03626)

    本文研究了非参数回归中的相变问题，根据最小极大MISE速率的不同情况，确定了不同范围内最佳的平滑度。同时，本文还提出了一组用于平滑函数类别的度量熵界限。

    

    当已知单变量的未知回归函数的导数在绝对值上直到$(\gamma+1)$阶都受到一个公共常数的界限（即$(\gamma+1)$阶平滑度），文献中给出的均方误差（MISE）的最小极大速率为$\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$。本文表明：（i）如果$n\leq\left(\gamma+1\right)^{2\gamma+3}$，则最小极大MISE速率为$\frac{\log n}{n\log(\log n)}$，并且最佳利用的平滑度为大约$\max\left\{ \left\lfloor \frac{\log n}{2\log\left(\log n\right)}\right\rfloor ,\,1\right\}$；（ii）如果$n>\left(\gamma+1\right)^{2\gamma+3}$，则最小极大MISE速率为$\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$，并且最佳利用的平滑度为$\gamma+1$。本文的基本贡献是我们为平滑函数类别制定的一组度量熵界限。

    When the unknown regression function of a single variable is known to have derivatives up to the $(\gamma+1)$th order bounded in absolute values by a common constant everywhere or a.e. (i.e., $(\gamma+1)$th degree of smoothness), the minimax optimal rate of the mean integrated squared error (MISE) is stated as $\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$ in the literature. This paper shows that: (i) if $n\leq\left(\gamma+1\right)^{2\gamma+3}$, the minimax optimal MISE rate is $\frac{\log n}{n\log(\log n)}$ and the optimal degree of smoothness to exploit is roughly $\max\left\{ \left\lfloor \frac{\log n}{2\log\left(\log n\right)}\right\rfloor ,\,1\right\} $; (ii) if $n>\left(\gamma+1\right)^{2\gamma+3}$, the minimax optimal MISE rate is $\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$ and the optimal degree of smoothness to exploit is $\gamma+1$. The fundamental contribution of this paper is a set of metric entropy bounds we develop for smooth function classes. Some 
    
[^194]: 上下文组合多输出 GP Bandits 带有组约束

    Contextual Combinatorial Multi-output GP Bandits with Group Constraints. (arXiv:2111.14778v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.14778](http://arxiv.org/abs/2111.14778)

    这项研究提出了一种应用于联邦多臂赌博问题的新算法，该算法通过选择一组基础臂来最大化超级臂奖励，并同时满足组奖励约束。算法利用两输出高斯过程模型，为每个基础臂的结果提供更大的灵活性。

    

    在联邦多臂赌博问题中，最大化全局奖励同时满足保护客户的最低隐私要求是主要目标。为了建立这样的问题，我们考虑了一个具有组和变化动作集的组合上下文赌博设定，其中相似的基础臂以组的形式出现，并且必须在每一轮选择一组基础臂（称为超级臂），以最大化超级臂的奖励同时满足来自选择基础臂的组的奖励约束。为了增加灵活性，我们让每个基础臂具有两个结果，被建模为两输出高斯过程（GP）的输出，其中一个结果用于计算超级臂的奖励，另一个用于组的奖励。然后，我们提出了一种新颖的双-UCB GP-bandit 算法，称为阈值组合高斯过程上置信边界（TCGP-UCB），它在最大化累积超级臂奖励和满足组奖励约束之间找到平衡，并且可以调整为偏好一方。

    In federated multi-armed bandit problems, maximizing global reward while satisfying minimum privacy requirements to protect clients is the main goal. To formulate such problems, we consider a combinatorial contextual bandit setting with groups and changing action sets, where similar base arms arrive in groups and a set of base arms, called a super arm, must be chosen in each round to maximize super arm reward while satisfying the constraints of the rewards of groups from which base arms were chosen. To allow for greater flexibility, we let each base arm have two outcomes, modeled as the output of a two-output Gaussian process (GP), where one outcome is used to compute super arm reward and the other for group reward. We then propose a novel double-UCB GP-bandit algorithm, called Thresholded Combinatorial Gaussian Process Upper Confidence Bounds (TCGP-UCB), which balances between maximizing cumulative super arm reward and satisfying group reward constraints and can be tuned to prefer one
    
[^195]: GFlowNet基础

    GFlowNet Foundations. (arXiv:2111.09266v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.09266](http://arxiv.org/abs/2111.09266)

    GFlowNets是一种生成流网络方法，用于在主动学习环境中采样多样化的候选集。它们具有估计联合概率分布和边际分布的能力，可以表示关于复合对象（如集合和图）的分布。通过单次训练的生成传递，GFlowNets分摊了计算昂贵的MCMC方法的工作。

    

    生成流网络（GFlowNets）被引入为在主动学习环境中采样多样化的候选集的方法，其训练目标使其近似按照给定的奖励函数进行采样。本文展示了GFlowNets的一些额外的理论性质。它们可以用于估计联合概率分布和相应的边际分布，其中一些变量未指定，特别是可以表示关于复合对象（如集合和图）的分布。GFlowNets通过单次训练的生成传递来分摊通常由计算昂贵的MCMC方法完成的工作。它们还可以用于估计分区函数和自由能，给定一个子集（子图）的超集（超图）的条件概率，以及给定一个集合（图）的所有超集（超图）的边际分布。我们介绍了一些变体，使得可以估计熵的值。

    Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets. They can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entro
    
[^196]: SCORE：用于离线强化学习的虚假相关性降低

    SCORE: Spurious COrrelation REduction for Offline Reinforcement Learning. (arXiv:2110.12468v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.12468](http://arxiv.org/abs/2110.12468)

    本文提出了SCORE算法，用于离线强化学习中的虚假相关性降低。通过引入退火行为克隆正则化器，SCORE实现了SoTA性能，并消除了次优性中的虚假相关性。

    

    离线强化学习（RL）利用大规模数据集解决序贯决策问题。大多数现有论文只讨论了对抗分布外（OOD）行为的防御，而本文研究了更广泛的问题，即认知不确定性与决策之间的虚假相关性，这是导致次优性的一个重要因素。本文提出了一种实用有效且理论上可证明的算法：用于离线RL的虚假相关性降低（SCORE）。我们通过实验证明，SCORE在标准基准（D4RL）上的各种任务中以3.1倍加速率实现了SoTA性能。所提算法引入了一个退火行为克隆正则化器来帮助生成高质量的不确定性估计，这对于消除次优性中的虚假相关性至关重要。理论上，我们证明了所提方法的合理性，并证明了在温和的条件下其收敛到最优策略的次线性率。

    Offline reinforcement learning (RL) harnesses the power of massive datasets for resolving sequential decision problems. Most existing papers only discuss defending against out-of-distribution (OOD) actions while we investigate a broader issue, the spurious correlations between epistemic uncertainty and decision-making, an essential factor that causes suboptimality. In this paper, we propose Spurious COrrelation REduction (SCORE) for offline RL, a practically effective and theoretically provable algorithm. We empirically show that SCORE achieves the SoTA performance with 3.1x acceleration on various tasks in a standard benchmark (D4RL). The proposed algorithm introduces an annealing behavior cloning regularizer to help produce a high-quality estimation of uncertainty which is critical for eliminating spurious correlations from suboptimality. Theoretically, we justify the rationality of the proposed method and prove its convergence to the optimal policy with a sublinear rate under mild a
    
[^197]: 一种可扩展、快速的基于人工神经网络的表面码综合征解码器

    A scalable and fast artificial neural network syndrome decoder for surface codes. (arXiv:2110.05854v4 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2110.05854](http://arxiv.org/abs/2110.05854)

    本研究报道了一种基于人工神经网络的表面码综合征解码器，能够解码任意形状和大小的表面码，并取得了具有竞争性或优越的性能，显示出惊人的加速作用，这是实现大规模量子纠错的重要进展。

    

    表面码纠错提供了实现可扩展容错量子计算的高度有前途的途径。当作为稳定码进行操作时，表面码计算包括一个综合征解码步骤，其中使用测量的稳定码算符来确定物理量子比特中的错误的适当纠正。译码算法经历了实质性的发展，最近的工作纳入了机器学习（ML）技术。尽管初步结果令人鼓舞，但基于ML的综合征译码器仍然局限于具有低延迟的小规模演示，并且无法处理需要进行晶格手术和编织的边界条件和各种形状的表面码。在这里，我们报道了一种基于人工神经网络（ANN）的可扩展快速综合征解码器，能够解码任意形状和大小的表面码，并且可以处理数据量子比特受到极化误差模型的影响。基于对5000万个随机量子错误实例的严格训练，ANN解码器实现了与最先进的译码算法相比的竞争性或优越性能，同时保持高解码准确性，呈现出了显著的加速效果。我们的工作展示了使用基于机器学习的译码方法实现大规模量子纠错的重大进展。

    Surface code error correction offers a highly promising pathway to achieve scalable fault-tolerant quantum computing. When operated as stabilizer codes, surface code computations consist of a syndrome decoding step where measured stabilizer operators are used to determine appropriate corrections for errors in physical qubits. Decoding algorithms have undergone substantial development, with recent work incorporating machine learning (ML) techniques. Despite promising initial results, the ML-based syndrome decoders are still limited to small scale demonstrations with low latency and are incapable of handling surface codes with boundary conditions and various shapes needed for lattice surgery and braiding. Here, we report the development of an artificial neural network (ANN) based scalable and fast syndrome decoder capable of decoding surface codes of arbitrary shape and size with data qubits suffering from the depolarizing error model. Based on rigorous training over 50 million random qu
    
[^198]: 稀疏MoEs满足高效的模型集成

    Sparse MoEs meet Efficient Ensembles. (arXiv:2110.03360v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03360](http://arxiv.org/abs/2110.03360)

    本论文研究了神经网络集成和稀疏专家混合的结合，提出了一种名为E$^3$的高效稀疏MoEs集成方法，在减少计算复杂度的同时取得了在准确性、鲁棒性和不确定性方面的改进。

    

    基于子模型的聚合输出的机器学习模型，无论是在激活还是预测水平上，往往相对于单个模型显示出很强的性能。我们研究了两种流行模型的相互作用：神经网络集成和稀疏专家混合（稀疏MoEs）。首先，我们展示了这两种方法具有互补的特点，它们的结合是有益的。这包括对稀疏MoEs在不确定性相关基准测试中的全面评估。然后，我们提出了高效的专家集成（E$^3$），一种可扩展且简单的稀疏MoEs集成方法，它兼具两类模型的优点，同时使用的浮点运算（FLOPs）比深度集成少多达45％。大量实验证明了E$^3$在多个具有挑战性的基于Transformer的视觉基线模型上的准确性、对数似然、少样本学习、鲁棒性和不确定性改进。E$^3$在扩展到具有高达27亿参数的模型时不仅保持其效率，而且还能提供更好的性能。

    Machine learning models based on the aggregated outputs of submodels, either at the activation or prediction levels, often exhibit strong performance compared to individual models. We study the interplay of two popular classes of such models: ensembles of neural networks and sparse mixture of experts (sparse MoEs). First, we show that the two approaches have complementary features whose combination is beneficial. This includes a comprehensive evaluation of sparse MoEs in uncertainty related benchmarks. Then, we present Efficient Ensemble of Experts (E$^3$), a scalable and simple ensemble of sparse MoEs that takes the best of both classes of models, while using up to 45% fewer FLOPs than a deep ensemble. Extensive experiments demonstrate the accuracy, log-likelihood, few-shot learning, robustness, and uncertainty improvements of E$^3$ over several challenging vision Transformer-based baselines. E$^3$ not only preserves its efficiency while scaling to models with up to 2.7B parameters, b
    
[^199]: 自监督学习通过重写规则证明直线程序的等价性

    Self-Supervised Learning to Prove Equivalence Between Straight-Line Programs via Rewrite Rules. (arXiv:2109.10476v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.10476](http://arxiv.org/abs/2109.10476)

    本研究针对自动合成一对程序之间语义等价性的证明问题，提出了一种基于Transformer模型的神经网络架构，通过一系列重写规则的应用序列实现程序的等价性证明。

    

    我们致力于自动合成由一系列语句构成的程序之间语义等价性的证明的问题。我们使用抽象语法树（AST）表示程序，其中一组保持语义的重写规则可以应用于特定的AST模式，生成转换后的程序与原程序在语义上等价。在我们的系统中，如果存在一系列重写规则的应用序列将一个程序重写成另一个程序，那么这两个程序是等价的。我们提出了一种基于Transformer模型的神经网络架构，用于生成程序对之间等价性的证明。系统输出一系列重写操作，验证该序列的有效性只需检查其是否可以应用。如果神经网络没有生成有效的序列，系统将报告这两个程序为非等价的，从设计上确保没有错误地将程序报告为等价的情况发生。我们的系统已经完全实现于单个文法上。

    We target the problem of automatically synthesizing proofs of semantic equivalence between two programs made of sequences of statements. We represent programs using abstract syntax trees (AST), where a given set of semantics-preserving rewrite rules can be applied on a specific AST pattern to generate a transformed and semantically equivalent program. In our system, two programs are equivalent if there exists a sequence of application of these rewrite rules that leads to rewriting one program into the other. We propose a neural network architecture based on a transformer model to generate proofs of equivalence between program pairs. The system outputs a sequence of rewrites, and the validity of the sequence is simply checked by verifying it can be applied. If no valid sequence is produced by the neural network, the system reports the programs as non-equivalent, ensuring by design no programs may be incorrectly reported as equivalent. Our system is fully implemented for one single gramm
    
[^200]: 从生成模型中获取神经视频压缩的洞见

    Insights from Generative Modeling for Neural Video Compression. (arXiv:2107.13136v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2107.13136](http://arxiv.org/abs/2107.13136)

    本论文基于生成模型，通过深度自回归和潜变量建模的方法，对神经视频编码算法进行了改进。提出了改进的时序自回归变换、改进的熵模型以及可变码率版本算法，并在高分辨率视频上取得了最先进的压缩性能。

    

    最近的机器学习研究揭示了深度生成模型（如VAEs）与学习压缩中使用的率失真损失之间的联系，但大部分工作都集中在图像上。在相似的思路下，我们通过深度自回归和潜变量建模的视角来观察最近提出的神经视频编码算法。我们将这些编码器视为广义随机时序自回归变换的实例，并提出了受归一化流和结构先验启发的进一步改进方法。我们提出了几种架构，使得在高分辨率视频上获得了最先进的视频压缩性能，并讨论了它们的权衡和消融。特别地，我们提出了（i）改进的时序自回归变换，（ii）带有结构和时序依赖性的改进熵模型，以及（iii）我们算法的可变码率版本。由于我们的改进与大量现有模型兼容，我们提供了实证结果来验证其有效性。

    While recent machine learning research has revealed connections between deep generative models such as VAEs and rate-distortion losses used in learned compression, most of this work has focused on images. In a similar spirit, we view recently proposed neural video coding algorithms through the lens of deep autoregressive and latent variable modeling. We present these codecs as instances of a generalized stochastic temporal autoregressive transform, and propose new avenues for further improvements inspired by normalizing flows and structured priors. We propose several architectures that yield state-of-the-art video compression performance on high-resolution video and discuss their tradeoffs and ablations. In particular, we propose (i) improved temporal autoregressive transforms, (ii) improved entropy models with structured and temporal dependencies, and (iii) variable bitrate versions of our algorithms. Since our improvements are compatible with a large class of existing models, we prov
    
[^201]: 轻量级分布式高斯过程回归用于在线机器学习

    Lightweight Distributed Gaussian Process Regression for Online Machine Learning. (arXiv:2105.04738v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.04738](http://arxiv.org/abs/2105.04738)

    在这篇论文中，我们提出了一种轻量级分布式高斯过程回归（GPR）算法，用于协同学习一个共同的静态潜在函数。通过独立运行基于代理的GPR和协同执行分布式GPR，我们展示了有限的代理间通信可以改善学习性能，并通过蒙特卡罗模拟评估了算法的性能。

    

    本文研究了一个问题，即一组代理通过流数据协同学习一个共同的静态潜在函数。我们提出了一种轻量级分布式高斯过程回归（GPR）算法，考虑到代理的有限通信、计算和存储能力。每个代理独立地利用本地流数据运行基于代理的GPR来预测感兴趣的测试点；然后代理们协同执行分布式GPR，得到在共同稀疏的一组测试点上的全局预测；最后，每个代理将分布式GPR的结果与基于代理的GPR的结果融合，以优化其预测。通过量化预测方差和误差的瞬态和稳态性能，我们证明了有限的代理间通信可以改善学习性能，满足帕累托改进。利用蒙特卡罗模拟评估了所开发的算法。

    In this paper, we study the problem where a group of agents aim to collaboratively learn a common static latent function through streaming data. We propose a lightweight distributed Gaussian process regression (GPR) algorithm that is cognizant of agents' limited capabilities in communication, computation and memory. Each agent independently runs agent-based GPR using local streaming data to predict test points of interest; then the agents collaboratively execute distributed GPR to obtain global predictions over a common sparse set of test points; finally, each agent fuses results from distributed GPR with agent-based GPR to refine its predictions. By quantifying the transient and steady-state performances in predictive variance and error, we show that limited inter-agent communication improves learning performances in the sense of Pareto. Monte Carlo simulation is conducted to evaluate the developed algorithm.
    
[^202]: 通过Riemannian Gauss-Newton进行低秩张量估计：统计最优性和二阶收敛性

    Low-rank Tensor Estimation via Riemannian Gauss-Newton: Statistical Optimality and Second-Order Convergence. (arXiv:2104.12031v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2104.12031](http://arxiv.org/abs/2104.12031)

    本文提出了一种通过Riemannian Gauss-Newton方法进行低秩张量估计的方法，并证明了其在噪声环境下的局部二次收敛性和统计最优性。

    

    本文研究从一系列有噪线性测量中估计低Tucker秩张量的问题。该问题涵盖了许多具体的应用示例，包括张量回归、张量补全和张量PCA / SVD。我们考虑了一种高效的Riemannian Gauss-Newton（RGN）方法来估计低Tucker秩张量。与文献中对RGN的（超）线性收敛保证不同，我们证明了在噪声环境下RGN在低秩张量估计中的第一种局部二次收敛保证，并提供相应的估计误差上界。我们还提供了一个确定性估计误差下界，该下界与上界相匹配，证明了RGN的统计最优性。我们通过两个机器学习应用（张量回归和张量SVD）来说明RGN的优点。最后，我们提供了模拟结果来证实我们的理论发现。

    In this paper, we consider the estimation of a low Tucker rank tensor from a number of noisy linear measurements. The general problem covers many specific examples arising from applications, including tensor regression, tensor completion, and tensor PCA/SVD. We consider an efficient Riemannian Gauss-Newton (RGN) method for low Tucker rank tensor estimation. Different from the generic (super)linear convergence guarantee of RGN in the literature, we prove the first local quadratic convergence guarantee of RGN for low-rank tensor estimation in the noisy setting under some regularity conditions and provide the corresponding estimation error upper bounds. A deterministic estimation error lower bound, which matches the upper bound, is provided that demonstrates the statistical optimality of RGN. The merit of RGN is illustrated through two machine learning applications: tensor regression and tensor SVD. Finally, we provide the simulation results to corroborate our theoretical findings.
    
[^203]: 通过信息驱动聚类和建模实现序列数据的新颖性检测

    Novelty Detection in Sequential Data by Informed Clustering and Modeling. (arXiv:2103.03943v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.03943](http://arxiv.org/abs/2103.03943)

    通过对数据进行聚类并分解建模任务，实现对序列数据中的新颖性进行检测。这种方法可以更准确地进行建模，但在每个聚类中的训练数据量较少，对于离散序列来说尤为突出。

    

    在离散序列中进行新颖性检测是一项具有挑战性的任务，因为与生成正常数据的过程相比，偏差通常很小或故意隐藏。可以通过建模正常序列并测量新序列与模型预测的偏差来检测新颖性。然而，在许多应用中，数据是由几个不同的过程生成的，因此在所有数据上训练的模型往往过度泛化，新颖性仍然未被检测到。我们提出通过分解方法来应对这一挑战：通过对数据进行聚类，我们将问题进行分解，从而在每个聚类中获得更简单的建模任务，从而可以更准确地进行建模。然而，这会带来一个权衡，因为每个聚类的训练数据量减少了。这对于数据需求量较大的离散序列来说是一个特殊的问题。因此，这种方法的成功取决于聚类的质量，即个体学习问题是否足够

    Novelty detection in discrete sequences is a challenging task, since deviations from the process generating the normal data are often small or intentionally hidden. Novelties can be detected by modeling normal sequences and measuring the deviations of a new sequence from the model predictions. However, in many applications data is generated by several distinct processes so that models trained on all the data tend to over-generalize and novelties remain undetected. We propose to approach this challenge through decomposition: by clustering the data we break down the problem, obtaining simpler modeling task in each cluster which can be modeled more accurately. However, this comes at a trade-off, since the amount of training data per cluster is reduced. This is a particular problem for discrete sequences where state-of-the-art models are data-hungry. The success of this approach thus depends on the quality of the clustering, i.e., whether the individual learning problems are sufficiently s
    
[^204]: 使用自结构重要性采样实现黑盒模拟分布尾部的效率

    Achieving Efficiency in Black Box Simulation of Distribution Tails with Self-structuring Importance Samplers. (arXiv:2102.07060v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.07060](http://arxiv.org/abs/2102.07060)

    本文提出了一种新颖的自结构重要性采样方法，通过复制在较不罕见的样本中观察到的浓度特性，隐式诱导出一种有效的IS分布，从而提高了估计性能度量的分布尾部的效率。

    

    本文提出了一种新颖的重要性采样（IS）方案，用于估计采用丰富工具模拟的性能度量的分布尾部，这些工具包括线性规划、整数线性规划、分段线性/二次目标、使用深度神经网络指定的特征映射等。传统的明确识别有效的测度变化的方法在高度格式化的模型之外受到可行性和可扩展性的限制，因为它们需要与目标和概率分布精心调整。在所提出的方案中，通过一种基本转换克服了这个瓶颈，该转换可以通过复制在较不罕见的样本中观察到的浓度特性来在各种模型中隐式诱导出一个有效的IS分布。这种新颖的方法是通过发展一个大偏差原理来指导的，这种原理揭示了最优IS分布的自相似现象。

    This paper presents a novel Importance Sampling (IS) scheme for estimating distribution tails of performance measures modeled with a rich set of tools such as linear programs, integer linear programs, piecewise linear/quadratic objectives, feature maps specified with deep neural networks, etc. The conventional approach of explicitly identifying efficient changes of measure suffers from feasibility and scalability concerns beyond highly stylized models, due to their need to be tailored intricately to the objective and the underlying probability distribution. This bottleneck is overcome in the proposed scheme with an elementary transformation which is capable of implicitly inducing an effective IS distribution in a variety of models by replicating the concentration properties observed in less rare samples. This novel approach is guided by developing a large deviations principle that brings out the phenomenon of self-similarity of optimal IS distributions. The proposed sampler is the firs
    
[^205]: 高效的数据驱动优化与有噪数据

    Efficient Data-Driven Optimization with Noisy Data. (arXiv:2102.04363v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2102.04363](http://arxiv.org/abs/2102.04363)

    本文研究了在已知噪声源的情况下的数据驱动处方问题，并导出了在这个噪声情况下的高效数据驱动形式，并指出它们具有熵最优传输解释。

    

    在决策者面临的实际情况下，大多数情况下，可用于决策的数据都受到一定程度的测量噪声的影响。本文研究了在已知噪声源的情况下的数据驱动处方问题，并导出了在这个噪声情况下的高效数据驱动形式，并指出它们具有熵最优传输解释。最后，通过利用Strassen的经典表示结果，我们证明了这些有效的鲁棒形式在几种有趣的设置下是可行的。

    Classical Kullback-Leibler or entropic distances are known to enjoy certain desirable statistical properties in the context of decision-making with noiseless data. However, in most practical situations the data available to a decision maker is subject to a certain amount of measurement noise. We hence study here data-driven prescription problems in which the data is corrupted by a known noise source. We derive efficient data-driven formulations in this noisy regime and indicate that they enjoy an entropic optimal transport interpretation. Finally, we show that these efficient robust formulations are tractable in several interesting settings by exploiting a classical representation result by Strassen.
    
[^206]: 学习使用连分数进行外推：预测超导体材料的临界温度

    Learning to extrapolate using continued fractions: Predicting the critical temperature of superconductor materials. (arXiv:2012.03774v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2012.03774](http://arxiv.org/abs/2012.03774)

    本论文提出了使用连分数来学习外推的方法，以预测超导体材料的临界温度。该方法适用于需要在扩展领域内准确近似的应用场景，尤其在设计新结构时最为重要。

    

    在人工智能（AI）和机器学习（ML）领域中，使用有限的实例$S={(\mathbf{x^{(i)}},y^{(i)})}$来近似未知目标函数$y=f(\mathbf{x})$，其中$\mathbf{x^{(i)}}\in D$，$D$表示感兴趣的领域，是一个常见的目标。我们将$S$称为训练集，并旨在找到一个低复杂度的数学模型，可以有效地近似这个目标函数对于新的实例$\mathbf{x}$。因此，模型的泛化能力通过在一个单独的集合$T=\{\mathbf{x^{(j)}}\}\subset D$上进行评估，其中$T\neq S$，经常有$T\cap S=\emptyset$，以评估其在训练集之外的表现。然而，某些应用不仅需要在原始领域$D$内准确近似，还需要在包含$D$的扩展领域$D'$内准确近似。这在涉及新结构设计的场景中尤为重要，因为最小化近似误差至关重要。

    In the field of Artificial Intelligence (AI) and Machine Learning (ML), the approximation of unknown target functions $y=f(\mathbf{x})$ using limited instances $S={(\mathbf{x^{(i)}},y^{(i)})}$, where $\mathbf{x^{(i)}} \in D$ and $D$ represents the domain of interest, is a common objective. We refer to $S$ as the training set and aim to identify a low-complexity mathematical model that can effectively approximate this target function for new instances $\mathbf{x}$. Consequently, the model's generalization ability is evaluated on a separate set $T=\{\mathbf{x^{(j)}}\} \subset D$, where $T \neq S$, frequently with $T \cap S = \emptyset$, to assess its performance beyond the training set. However, certain applications require accurate approximation not only within the original domain $D$ but also in an extended domain $D'$ that encompasses $D$. This becomes particularly relevant in scenarios involving the design of new structures, where minimizing errors in approximations is crucial. For e
    
[^207]: 使用GOTHIC自动检测双核星系并发现一大批双重AGN

    Automated Detection of Double Nuclei Galaxies using GOTHIC and the Discovery of a Large Sample of Dual AGN. (arXiv:2011.12177v3 [astro-ph.GA] UPDATED)

    [http://arxiv.org/abs/2011.12177](http://arxiv.org/abs/2011.12177)

    我们提出了一种名为GOTHIC的算法，用于检测双核星系，发现了许多双重活动星系核。我们在大样本中检测到了159个双重AGN，其中2个是三重AGN系统。

    

    我们提出了一种新颖的算法GOTHIC（图形增强迭代爬山算法），用于检测具有两个或多个密集分离核的星系。我们的目标是在星系中检测双重或多重活动星系核（AGN）的样本。尽管星系合并很常见，但双重AGN的检测很罕见。它们的检测非常重要，因为它们帮助我们理解超大质量黑洞（SMBH）双星的形成、SMBH的增长和多核系统中的AGN反馈效应。因此，需要一种算法对现有的成像数据进行系统调查，以发现双核星系和双重AGN。我们在已知的DNG样本上测试了GOTHIC，并随后将其应用于SDSS DR16中大约100万个处于红移范围0至0.75的星系样本，并具有可用的光谱数据。我们在这个样本中检测到159个双重AGN，其中2个是三重AGN系统。

    We present a novel algorithm to detect double nuclei galaxies (DNG) called GOTHIC (Graph BOosted iterated HIll Climbing) - that detects whether a given image of a galaxy has two or more closely separated nuclei. Our aim is to detect samples of dual or multiple active galactic nuclei (AGN) in galaxies. Although galaxy mergers are common, the detection of dual AGN is rare. Their detection is very important as they help us understand the formation of supermassive black hole (SMBH) binaries, SMBH growth and AGN feedback effects in multiple nuclei systems. There is thus a need for an algorithm to do a systematic survey of existing imaging data for the discovery of DNGs and dual AGN. We have tested GOTHIC on a known sample of DNGs and subsequently applied it to a sample of a million SDSS DR16 galaxies lying in the redshift range of 0 to 0.75 approximately, and have available spectroscopic data. We have detected 159 dual AGN in this sample, of which 2 are triple AGN systems. Our results show 
    
[^208]: 结构化多臂赌博机的最优学习

    Optimal Learning for Structured Bandits. (arXiv:2007.07302v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.07302](http://arxiv.org/abs/2007.07302)

    本文研究了结构化多臂赌博机问题，在存在结构信息的情况下，设计了一种能够利用结构信息以最小化后悔的算法。

    

    本文研究了结构化的多臂赌博机问题，即在存在结构信息的不确定性环境下进行在线决策。在这个问题中，决策者需要在观察到不确定的奖励随时间变化时找出最佳行动方针。决策者已经了解到奖励分布属于一个凸紧致集合的某种凸结构信息。在有这种结构信息的情况下，决策者希望通过利用这些信息来最小化后悔（与提前知道最佳行动的基准策略相比的性能差异）。在没有结构信息的情况下，经典的上界置信区间（UCB）和汤姆逊取样算法已被证明具有最小后悔。然而，最近指出，这两种算法都无法利用复杂结构信息。

    We study structured multi-armed bandits, which is the problem of online decision-making under uncertainty in the presence of structural information. In this problem, the decision-maker needs to discover the best course of action despite observing only uncertain rewards over time. The decision-maker is aware of certain convex structural information regarding the reward distributions; that is, the decision-maker knows the reward distributions of the arms belong to a convex compact set. In the presence such structural information, they then would like to minimize their regret by exploiting this information, where the regret is its performance difference against a benchmark policy that knows the best action ahead of time. In the absence of structural information, the classical upper confidence bound (UCB) and Thomson sampling algorithms are well known to suffer minimal regret. As recently pointed out, neither algorithms are, however, capable of exploiting structural information that is com
    
[^209]: 针对未预料到的对手测试鲁棒性

    Testing Robustness Against Unforeseen Adversaries. (arXiv:1908.08016v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1908.08016](http://arxiv.org/abs/1908.08016)

    该论文提出了18种新的对抗攻击，并使用这些攻击创建了一个用于评估对各种未预料到的对手的鲁棒性的新基准。作者还发现了一系列防御策略，可以帮助克服训练期间未考虑到的对手的泛化差距。该研究的结果将为研究现实世界最坏情况下的鲁棒性提供有用工具，促进开发更强大的防御措施。

    

    在考虑现实世界的对抗环境时，防御者在训练期间不太可能对所有可能的对手进行训练，并且对手很可能使用逼真的对抗扭曲，而不限于小的L_p约束扰动。为了缩小研究和现实之间的差距，我们介绍了18种新的对抗攻击，并使用它们创建了ImageNet-UA，这是一个用于评估模型对各种未预料到的对手的鲁棒性的新基准。我们利用这个基准来识别一系列能够帮助克服这种泛化差距的防御策略，发现了可以提高对未预料到的攻击的鲁棒性的技术的丰富空间。我们希望ImageNet-UA的更多样性和逼真性将成为那些研究现实世界最坏情况的鲁棒性的人的有用工具，从而促进开发能够在训练期间看不到的攻击中进行泛化的更强大的防御措施。

    When considering real-world adversarial settings, defenders are unlikely to have access to the full range of deployment-time adversaries during training, and adversaries are likely to use realistic adversarial distortions that will not be limited to small L_p-constrained perturbations. To narrow in on this discrepancy between research and reality we introduce eighteen novel adversarial attacks, which we use to create ImageNet-UA, a new benchmark for evaluating model robustness against a wide range of unforeseen adversaries. We make use of our benchmark to identify a range of defense strategies which can help overcome this generalization gap, finding a rich space of techniques which can improve unforeseen robustness. We hope the greater variety and realism of ImageNet-UA will make it a useful tool for those working on real-world worst-case robustness, enabling development of more robust defenses which can generalize beyond attacks seen during training.
    

