# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [De Novo Drug Design with Joint Transformers.](http://arxiv.org/abs/2310.02066) | 提出了联合Transformer来解决全新药物设计的困难，同时生成新颖分子并预测目标属性。使用惩罚对数似然目标训练模型，在分子生成和预测精度方面取得了最先进的性能，相比于仅微调解码器的Transformer，降低了新采样分子的预测误差42%。通过联合Transformer生成的新颖分子在全新药物设计中优于其他基于SMILES的优化方法。 |
| [^2] | [VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores.](http://arxiv.org/abs/2310.02065) | VENOM提出了一种新的V:N:M格式，可以在NVIDIA的稀疏张量核心上实现任意N:M比率。使用Spatha稀疏库，可以实现高达37倍的加速，并且展示了一种二阶修剪技术来实现高稀疏比率的V:N:M形式，并几乎没有损失。 |
| [^3] | [Lessons Learned from EXMOS User Studies: A Technical Report Summarizing Key Takeaways from User Studies Conducted to Evaluate The EXMOS Platform.](http://arxiv.org/abs/2310.02063) | 本技术报告总结了两项用户研究的重要发现，研究探索了在支持医疗专家优化机器学习模型的系统中，全局模型和数据中心解释对于检测和解决潜在的数据相关问题的有效性。 |
| [^4] | [The Inhibitor: ReLU and Addition-Based Attention for Efficient Transformers.](http://arxiv.org/abs/2310.02041) | 本论文提出了一种"Inhibitor"机制，通过使用ReLU和加法注意力机制来增强计算效率。这种机制可以在资源受限的硬件或替代算法系统上实现更高效的执行和支持更大的量化Transformer模型。实验结果表明，与传统的点积注意力相比，该机制在预测得分上表现相当，并且可以实现显著的计算节省。这一创新可能在隐私保护的应用中发挥重要作用。 |
| [^5] | [aSAGA: Automatic Sleep Analysis with Gray Areas.](http://arxiv.org/abs/2310.02032) | 提出了一个名为aSAGA的自动睡眠分析模型，可以在临床和家庭睡眠研究中有效执行。模型通过使用不确定性映射来识别灰色区域，需要进行手动重新评估。 |
| [^6] | [OceanGPT: A Large Language Model for Ocean Science Tasks.](http://arxiv.org/abs/2310.02031) | OceanGPT是首个专为海洋科学任务设计的大型语言模型，通过DoInstruct框架实现自动获取海洋领域指导数据。这一模型的引入填补了海洋科学领域中对LLM的需求缺口，并为海洋科学研究提供了新的工具和方法。 |
| [^7] | [Between accurate prediction and poor decision making: the AI/ML gap.](http://arxiv.org/abs/2310.02029) | 本文指出，AI/ML界在预测准确性方面过于关注状态概率的估计，忽视了对效用的准确可靠估计，导致了期望和实际影响之间的差距。 |
| [^8] | [DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks.](http://arxiv.org/abs/2310.02027) | DeepHGCN是一个具有深层架构的双曲图卷积网络，通过引入新的双曲特征转换层和正则化技术，实现了计算效率的极大改进和过度平滑问题的显著减轻。 |
| [^9] | [Nash Regret Guarantees for Linear Bandits.](http://arxiv.org/abs/2310.02023) | 本研究在随机线性多臂老虎机框架中提出了一个更强化的遗憾上界，称为纳什遗憾，它通过将算法的表现量化为其在各个回合中所生成的集体福利来提供一个基于公平性的保证。 |
| [^10] | [Ranking a Set of Objects using Heterogeneous Workers: QUITE an Easy Problem.](http://arxiv.org/abs/2310.02016) | 使用QUITE算法，我们可以通过估计工人的可靠性和对象的质量，从一组由异构工人提供的噪声成对比较开始，对一组对象进行排序。与其他算法相比，QUITE表现出良好的性能，并且可以轻松地使其自适应。 |
| [^11] | [Spectral operator learning for parametric PDEs without data reliance.](http://arxiv.org/abs/2310.02013) | 这种方法通过光谱方法和深度神经网络相结合，实现了无需数据依赖的参数化PDE求解，可以准确学习和预测复杂的PDE解。 |
| [^12] | [Towards Training Without Depth Limits: Batch Normalization Without Gradient Explosion.](http://arxiv.org/abs/2310.02012) | 本研究提出了一种超越深度限制的训练方法，通过批归一化避免梯度爆炸。研究给出了一种具体构造的多层感知器，在任何深度都能保持优化的信号传播特性，并具有有界梯度。 |
| [^13] | [Decoding Human Activities: Analyzing Wearable Accelerometer and Gyroscope Data for Activity Recognition.](http://arxiv.org/abs/2310.02011) | 本文提出了一种用于活动识别的分层多结构方法，利用残差网络和残差MobileNet对静态和动态活动进行分类，然后通过加权合奏方法进行集成。 |
| [^14] | [fmeffects: An R Package for Forward Marginal Effects.](http://arxiv.org/abs/2310.02008) | fmeffects是第一个实现前向边际效应（FMEs）的R软件包。 |
| [^15] | [L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation.](http://arxiv.org/abs/2310.02003) | L2MAC是一种基于LLM的存储程序自动计算机，可以用于生成长且逻辑一致的代码。 |
| [^16] | [MUSCLE: Multi-task Self-supervised Continual Learning to Pre-train Deep Models for X-ray Images of Multiple Body Parts.](http://arxiv.org/abs/2310.02000) | MUSCLE提出了一种多任务自监督连续学习（MUSCLE）的预训练流程，用于多个身体部位的X-ray图像的深度模型。它通过汇集来自多个身体部位的X-ray图像进行表示学习，并采用连续学习过程来预训练骨干网络，以应对多个X-ray分析任务中的数据异质性、过拟合和灾难性遗忘问题。 |
| [^17] | [Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems.](http://arxiv.org/abs/2310.01991) | 本文探讨了LLM在数学应用题中的逆向推理能力，发现在逆向推理任务上，LLM模型的准确性显著下降。通过改进技术，如Rephrase和PAL-Tools，我们提高了模型的性能。 |
| [^18] | [Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data.](http://arxiv.org/abs/2310.01975) | 本文研究了两层ReLU卷积神经网络在XOR数据中的良性过拟合现象。实验证明，在一定条件下，通过梯度下降训练的过度参数化的ReLU卷积神经网络可以达到近乎贝叶斯最优准确率。 |
| [^19] | [Federated Wasserstein Distance.](http://arxiv.org/abs/2310.01973) | 本文介绍了一种在联合方式下计算两个分布的瓦塞斯坦距离的方法，利用瓦塞斯坦距离的几何性质和测地线性质，通过操作和交换测地线空间中的分布来逼近瓦塞斯坦距离。 |
| [^20] | [Epidemic Learning: Boosting Decentralized Learning with Randomized Communication.](http://arxiv.org/abs/2310.01972) | 流行学习是一种简单而强大的分散式学习算法，通过利用变化的通信拓扑结构实现了比传统方法更快的模型收敛速度，具有更好的收敛性能。 |
| [^21] | [Beyond Labeling Oracles: What does it mean to steal ML models?.](http://arxiv.org/abs/2310.01959) | 本文研究了模型提取攻击，发现攻击者往往不能节约数据和标注成本，因为攻击隐含地依赖于从受害模型的数据分布中采样的能力。攻击者的先前知识对攻击成功至关重要。 |
| [^22] | [Probabilistic Reach-Avoid for Bayesian Neural Networks.](http://arxiv.org/abs/2310.01951) | 本文讨论了贝叶斯神经网络在计算到达-避免概率和合成最优控制策略中的应用，通过利用区间传播和向后递归技术，计算出了概率的下界作为安全性保证。 |
| [^23] | [OOD Aware Supervised Contrastive Learning.](http://arxiv.org/abs/2310.01942) | 本文提出了一种利用Supervised Contrastive（SupCon）学习的强大表示的方法来学习对OOD数据具有鲁棒性的分类器，并通过扩展SupCon损失的对比项来实现。当辅助OOD数据不可用时，我们提出了特征混合技术来生成伪OOD特征。 |
| [^24] | [Causal Inference with Conditional Front-Door Adjustment and Identifiable Variational Autoencoder.](http://arxiv.org/abs/2310.01937) | 本文提出了条件前门调整和可辨识性变分自编码器来解决因果推断中的问题，通过放松限制和利用深度生成模型，从观测数据中学习未观测到的混淆变量的表示，从而保证因果效应的可辨识性。 |
| [^25] | [Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models.](http://arxiv.org/abs/2310.01929) | 本研究旨在探索和解锁文本到图像模型的文化视角，通过对TTI模型中嵌入的文化感知进行评估，揭示了这些模型的文化意识、文化区别和文化适应性。 |
| [^26] | [RoFormer for Position Aware Multiple Instance Learning in Whole Slide Image Classification.](http://arxiv.org/abs/2310.01924) | RoFormer模块可以为全幻灯片图像分类任务中的位置感知多实例学习提供准确自注意力和相对位置编码，克服了当前方法中对补丁独立性和排序不变性的假设，并解决了计算工作量过大的问题。 |
| [^27] | [Improved Automatic Diabetic Retinopathy Severity Classification Using Deep Multimodal Fusion of UWF-CFP and OCTA Images.](http://arxiv.org/abs/2310.01912) | 本研究提出了一种利用UWF-CFP和OCTA图像的深度多模态融合方法来改进自动糖尿病视网膜病变严重程度分类，将2D UWF-CFP图像和3D高分辨率6x6 mm^3 OCTA图像进行整合，并采用ResNet50和3D-ResNet50模型的融合以及Squeeze-and-Excitation（SE）块来增强相关特征，同时采用多模态Manifold Mixup来增加模型的泛化能力。 |
| [^28] | [Beyond the Benchmark: Detecting Diverse Anomalies in Videos.](http://arxiv.org/abs/2310.01904) | 该研究提出了超越基准的视频异常检测方法，引入了两个新的数据集，HMDB-AD和HMDB-Violence，挑战具有多样化动作异常的模型。此外，还提出了一种新的方法，多帧异常检测（MFAD），它建立在AI-VAD框架上，结合了单帧和两帧特征，以算法计算异常得分。 |
| [^29] | [FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations.](http://arxiv.org/abs/2310.01892) | 本文介绍了一种简单的过滤器增强方法来改进无监督节点表示学习的性能，通过捕捉不同特征频谱部分，我们展示了显著的改进，并减少了计算负载。同时，我们通过使用简单的随机 Fourier 特征投影来解决高维表示的计算问题，并在基准数据集上取得了良好的性能。 |
| [^30] | [Effective and Parameter-Efficient Reusing Fine-Tuned Models.](http://arxiv.org/abs/2310.01886) | 本文提出了一种有效且参数高效的方法，可以重复使用微调模型来处理下游任务，减轻存储和服务负担，并提出了PERU-FFT方法用于重复使用全面微调模型。 |
| [^31] | [Adaptive Hybrid Model for Enhanced Stock Market Predictions Using Improved VMD and Stacked Informer.](http://arxiv.org/abs/2310.01884) | 本研究提出了一种自适应混合模型，通过改进的VMD、特征工程和堆叠Informer，结合自适应损失函数，成功应用于股市预测。实验证明该模型在预测准确性、响应性和泛化能力方面优于传统和其他混合模型，对于小企业和特征工程的预测建模有潜在的优化方向和未来发展方向。 |
| [^32] | [AutoCast++: Enhancing World Event Prediction with Zero-shot Ranking-based Context Retrieval.](http://arxiv.org/abs/2310.01880) | AutoCast++是一个零-shot基于排名的上下文检索系统，用于从广泛的新闻文档集合中进行事件预测。 |
| [^33] | [Towards Stable Backdoor Purification through Feature Shift Tuning.](http://arxiv.org/abs/2310.01875) | 本文通过综合评估不同攻击场景下的微调方法，提出了一种稳定的后门净化方法。研究发现，在低污染率的情况下，后门和干净特征之间的纠缠会削弱调整方法的效果。 |
| [^34] | [DeepDecipher: Accessing and Investigating Neuron Activation in Large Language Models.](http://arxiv.org/abs/2310.01870) | DeepDecipher是一种用于探测大规模语言模型中神经元激活的API和接口，它通过提供先进的可解释性技术和易于使用的界面，使得对这些复杂模型的分析更加直观和可访问。 |
| [^35] | [Conditional Instrumental Variable Regression with Representation Learning for Causal Inference.](http://arxiv.org/abs/2310.01865) | 本文提出了一种使用条件工具变量回归和表示学习进行因果推断的方法，该方法能够解决由未观测潜变量引起的混淆偏差，并在无需线性假设的情况下平衡观察到的混淆变量。 |
| [^36] | [High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise.](http://arxiv.org/abs/2310.01860) | 提出了一种针对复合式和分布式优化问题的新的随机方法，通过剪裁梯度差值实现了紧致的高概率收敛性分析。 |
| [^37] | [Variational Gaussian approximation of the Kushner optimal filter.](http://arxiv.org/abs/2310.01859) | 本论文提出了一种基于变分高斯逼近的方法来近似解决Kushner方程，通过传播和贝叶斯更新概率密度相关的两个接近损失，利用Wasserstein度量和Fisher度量，通过隐式更新均值和协方差矩阵来解决最后的接近损失，从而得到了满足高斯流的随机微分方程，扩展了线性情况下的Kalman-Bucy和Riccati流。 |
| [^38] | [Score-based Data Assimilation for a Two-Layer Quasi-Geostrophic Model.](http://arxiv.org/abs/2310.01853) | 本论文评估了基于评分的数据同化方法在高维度的地球物理动力系统中的可扩展性，并通过在双层拟地转动模型上的实验证明了该方法的良好性能。 |
| [^39] | [Benchmarking and Improving Generator-Validator Consistency of Language Models.](http://arxiv.org/abs/2310.01846) | 本文提出了一种衡量生成和验证之间一致性的框架，发现目前最先进的语言模型GPT-4仅在76%的情况下是一致的。为了改进一致性，提出了一种通过经过筛选的生成器和验证器答案进行微调的方法，并将其称为一致性微调。该方法将Alpaca-30B的一致性从60%提高到93%，并且对未见过的任务和领域也具有泛化能力。除了改进一致性外，一致性微调还带来了其他性能的提升。 |
| [^40] | [Zero-Shot Refinement of Buildings' Segmentation Models using SAM.](http://arxiv.org/abs/2310.01845) | 本文提出了一种使用SAM进行建筑物分割模型的零-shot细化的方法，针对遥感图像应用中SAM性能不佳、无法进行识别的问题进行了处理。通过引入不同的提示来提升模型的泛化能力。 |
| [^41] | [Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation.](http://arxiv.org/abs/2310.01837) | 这篇论文扩展了基于CAM的可解释AI方法，使其适用于遥感图像分割。当前AI模型在高分辨率卫星图像上训练时缺乏透明度和可解释性，本文通过改进XAI分类算法，提供了解释图像分割的手段。 |
| [^42] | [EMBERSim: A Large-Scale Databank for Boosting Similarity Search in Malware Analysis.](http://arxiv.org/abs/2310.01835) | EMBERSim是一个大规模数据库，用于提高恶意软件分析中的相似性搜索。它通过从最大的恶意软件分类数据集EMBER开始，并将相似性信息和恶意软件类别标签相结合，解决了相似性研究领域的不足。 |
| [^43] | [Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation.](http://arxiv.org/abs/2310.01828) | 本论文介绍了一种可训练的噪声模型作为XAI评估方法，在遥感图像分割中的应用。在图像处理中提供深度神经网络的可解释性对于广泛采用和部署至关重要。虽然图像分割在计算机视觉应用中很重要，但在可解释性方面受到了相对较少的关注。 |
| [^44] | [Empirical Study of PEFT techniques for Winter Wheat Segmentation.](http://arxiv.org/abs/2310.01825) | 本研究通过使用PEFT技术，探索跨区域和跨年份的分布外推广性，以适应农作物监测的需求。 |
| [^45] | [Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI.](http://arxiv.org/abs/2310.01824) | Mini-BEHAVIOR是一个面向具身人工智能的基准，旨在挑战智能体解决类似于日常挑战的复杂活动，并通过过程生成实现了无限的任务变化和对开放式学习的支持。 |
| [^46] | [MIMO-NeRF: Fast Neural Rendering with Multi-input Multi-output Neural Radiance Fields.](http://arxiv.org/abs/2310.01821) | MIMO-NeRF是一种多输入多输出神经辐射场算法，通过使用MIMO MLP并进行分组映射，提高了渲染速度，并通过自监督学习方法减轻了部分模糊性。 |
| [^47] | [Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks.](http://arxiv.org/abs/2310.01820) | 本文研究了评估图神经网络解释性的鲁棒度的方法，并指出了现有度量方法的局限性。 |
| [^48] | [AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework.](http://arxiv.org/abs/2310.01818) | AutoLoRa是一个自动鲁棒微调框架，通过引入低秩分支和启发式策略，解决了现有鲁棒微调存在的梯度方向分歧和超参数敏感性问题。 |
| [^49] | [What Determines the Price of NFTs?.](http://arxiv.org/abs/2310.01815) | 该论文分析了在OpenSea交易的NFT收藏的链上和链下数据，探讨了NFT价格的影响因素。结果发现，尽管文本和图像数据可以解释收藏内部价格的变化，但这些特征不能推广到新的收藏品。 |
| [^50] | [Simulation-based Inference with the Generalized Kullback-Leibler Divergence.](http://arxiv.org/abs/2310.01808) | 本研究提出了一种基于广义Kullback-Leibler散度的仿真推断方法，通过考虑非归一化分布中的归一化常数，将神经后验估计与神经比值估计结合为一个目标，并研究了一种混合模型来实现最佳效果。 |
| [^51] | [Discrete, compositional, and symbolic representations through attractor dynamics.](http://arxiv.org/abs/2310.01807) | 这项工作探讨了如何通过模拟吸引子动力学来更加神经可行地实现离散化，从而将连续的表示空间划分为对应于符号序列的分区。通过引入符号空间结构，可以在丰富的感知输入的吸引子支持表示空间中实现组合性。 |
| [^52] | [GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking.](http://arxiv.org/abs/2310.01794) | 本研究通过基准测试系统评估了基于扰动的GNN解释性方法，发现帕累托最优方法在噪声存在的情况下表现出卓越效力和稳定性。 |
| [^53] | [Can large language models provide useful feedback on research papers? A large-scale empirical analysis.](http://arxiv.org/abs/2310.01783) | 这项研究通过大规模实证分析探讨了使用大型语言模型生成科学论文反馈的实用性。通过对GPT-4生成的反馈与人类同行评审的比较，发现大型语言模型在提供科学反馈方面具有潜力。 |
| [^54] | [SEA: Sparse Linear Attention with Estimated Attention Mask.](http://arxiv.org/abs/2310.01777) | 提出了SEA方法，可以通过估计注意力掩码实现线性复杂度的稀疏注意力，解决了transformer处理长序列时注意力操作复杂度高的问题，并保持了可解释性。 |
| [^55] | [A simple connection from loss flatness to compressed representations in neural networks.](http://arxiv.org/abs/2310.01770) | 该论文研究了深度神经网络中损失平坦性和神经表示压缩之间的关系，通过简单的数学关系，证明了损失平坦性与神经表示的压缩相关。 |
| [^56] | [How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization.](http://arxiv.org/abs/2310.01769) | 该论文研究了过参数化如何影响矩阵感知问题中梯度下降的收敛行为，在对称和非对称设置下给出了不同的收敛速度。 |
| [^57] | [Backdiff: a diffusion model for generalized transferable protein backmapping.](http://arxiv.org/abs/2310.01768) | BackDiff是一种用于蛋白质背映射的新的生成模型，它利用条件分数扩散模型和几何表示，旨在实现在各种粗粒化模型和蛋白质中的广义化和可靠化。 |
| [^58] | [Exploring Counterfactual Alignment Loss towards Human-centered AI.](http://arxiv.org/abs/2310.01766) | 该论文提出了一个基于反事实生成的以人为中心的框架，并引入了一种新的损失函数，用于保证反事实生成归因的特征与人类专家对齐。 |
| [^59] | [Data Cleaning and Machine Learning: A Systematic Literature Review.](http://arxiv.org/abs/2310.01765) | 本文系统综述了数据清洗与机器学习的关系，并总结了最新的数据清洗方法和ML的应用领域。在这两个领域存在着互相促进的关系，研究人员提出了未来的研究方向和建议。 |
| [^60] | [Sampling Multimodal Distributions with the Vanilla Score: Benefits of Data-Based Initialization.](http://arxiv.org/abs/2310.01762) | 这项研究证明了通过在经验分布上初始化并运行使用早期停止的Langevin扩散的Vanilla Score方法，可以对多模态分布进行采样。 |
| [^61] | [Linearization of ReLU Activation Function for Neural Network-Embedded Optimization:Optimal Day-Ahead Energy Scheduling.](http://arxiv.org/abs/2310.01758) | 本文研究了将非线性激活函数线性化的方法，特别关注修正线性单元（ReLU）函数。并针对神经网络嵌入优化问题提出并比较了四种定制的ReLU激活函数的线性化方法。 |
| [^62] | [Improved Algorithms for Adversarial Bandits with Unbounded Losses.](http://arxiv.org/abs/2310.01756) | 改进的算法用于解决对抗性多臂老虎机问题，无需先验知识，实现自适应且无需统一探索的遗憾界限，能够处理任意无界损失，并通过实验证明优于现有算法。 |
| [^63] | [CausalTime: Realistically Generated Time-series for Benchmarking of Causal Discovery.](http://arxiv.org/abs/2310.01753) | CausalTime引入了一种生成逼真时间序列的流程，能够生成与真实数据极其相似且带有基准因果图的时间序列，用于定量性能评估。该流程利用深度神经网络和正态流捕捉逼真的动态，提取假设的因果图，并生成适合算法评估的多样化时间序列。 |
| [^64] | [5G Network Slicing: Analysis of Multiple Machine Learning Classifiers.](http://arxiv.org/abs/2310.01747) | 本研究分析了多个机器学习分类器对于5G网络切片的准确性和精度。研究发现，逻辑回归模型、线性判别模型、k最近邻模型、决策树模型、随机森林模型、SVC BernoulliNB模型和GaussianNB模型都能够有效地检测网络切片，并为动态调整网络切片来满足不同服务需求提供了有效的方法。 |
| [^65] | [Randomized Dimension Reduction with Statistical Guarantees.](http://arxiv.org/abs/2310.01739) | 本论文提出了一种基于“矩阵抽样”的快速随机低秩分解算法，用于大型矩阵的维度降低，从而提高计算效率和数据利用率。 |
| [^66] | [Blending Imitation and Reinforcement Learning for Robust Policy Improvement.](http://arxiv.org/abs/2310.01737) | 本文提出了一种融合模仿学习和强化学习的方法，根据在线评估结果交替使用二者，以提高样本效率和学习效果。 |
| [^67] | [Nugget: Neural Agglomerative Embeddings of Text.](http://arxiv.org/abs/2310.01732) | Nugget是一种基于动态选择的输入令牌子集的文本嵌入方法，通过自编码和机器翻译等任务，将语言分割为有意义的单元，优于相关方法，在语义比较任务中表现出色，并且允许扩展语言模型的上下文窗口。 |
| [^68] | [Time-LLM: Time Series Forecasting by Reprogramming Large Language Models.](http://arxiv.org/abs/2310.01728) | 这项工作介绍了Time-LLM，一个重新编程的框架，通过重新利用大型语言模型，可以进行一般的时间序列预测。 |
| [^69] | [Large Language Models for Test-Free Fault Localization.](http://arxiv.org/abs/2310.01726) | 该论文提出了一种基于大语言模型的故障定位方法，称为LLMAO，具有无需测试覆盖信息就能定位有问题的代码行的能力。 |
| [^70] | [PrACTiS: Perceiver-Attentional Copulas for Time Series.](http://arxiv.org/abs/2310.01720) | PrACTiS combines perceiver architecture with copula structure to enhance time-series forecasting and reduce computational demands. It incorporates midpoint inference and local attention mechanisms to effectively capture dependencies within imputed samples. The copula-based attention and output variance testing mechanism capture the joint distribution of missing data and mitigate error propagation during prediction. |
| [^71] | [Ensemble Distillation for Unsupervised Constituency Parsing.](http://arxiv.org/abs/2310.01717) | 本论文提出了一种集成蒸馏的方法来提高无监督句法解析的性能，并且通过蒸馏将集成知识转移到一个学生模型中，解决了常见的多教师蒸馏方法中的过度平滑问题。 |
| [^72] | [Large Language Models as Analogical Reasoners.](http://arxiv.org/abs/2310.01714) | 本研究提出了一种新的提示方法，称为类比提示，用于自动引导大型语言模型的推理过程。通过在上下文中自动生成相关实例或知识，该方法在多种推理任务中表现出优异的性能。 |
| [^73] | [Generative Autoencoding of Dropout Patterns.](http://arxiv.org/abs/2310.01712) | 本论文提出了一种称为解读自编码器的生成模型，通过为训练数据集中的每个数据点分配独特的随机丢弃模式来进行训练，只依靠重构误差来提供更稳定的训练性能，并在CIFAR-10数据集上展示了与DCGAN相媲美的采样质量。 |
| [^74] | [On Representation Complexity of Model-based and Model-free Reinforcement Learning.](http://arxiv.org/abs/2310.01706) | 本研究在电路复杂度的角度探讨了基于模型和无模型强化学习的表示复杂性。理论上证明了某些MDP可以用恒定深度电路表示转移和奖励函数，但最优$Q$-函数的电路复杂度指数级增加。我们的理论揭示了为什么基于模型的算法通常比无模型的算法具有更好的样本复杂性。 |
| [^75] | [Transformers are efficient hierarchical chemical graph learners.](http://arxiv.org/abs/2310.01704) | 本论文介绍了SubFormer，这是一个图转换器，通过在子图上进行消息传递机制来降低标记数量并增强学习长程交互。在化学结构的分子属性预测基准上，SubFormer在计算成本的一小部分下与最先进的图转换器相竞争，并且能够以几分钟的时间进行训练。注意权重的解释显示SubFormer展现出有限的关注权重。 |
| [^76] | [Robustifying State-space Models for Long Sequences via Approximate Diagonalization.](http://arxiv.org/abs/2310.01698) | 本文提出了一种用于处理机器学习中不适定对角化问题的通用解决方案，通过引入基于拟谱理论的“扰动然后对角化”（PTD）方法，改善了长序列状态空间模型的鲁棒性。 |
| [^77] | [DANI: Fast Diffusion Aware Network Inference with Preserving Topological Structure Property.](http://arxiv.org/abs/2310.01696) | DANI是一种快速的、具有保持拓扑结构属性的扩散感知网络推断方法。 |
| [^78] | [Forecasting Tropical Cyclones with Cascaded Diffusion Models.](http://arxiv.org/abs/2310.01690) | 本研究利用级联扩散模型预测热带气旋轨迹和降水模式，通过整合多源数据实现准确的预测，对高度脆弱地区具有重要意义。 |
| [^79] | [From Stability to Chaos: Analyzing Gradient Descent Dynamics in Quadratic Regression.](http://arxiv.org/abs/2310.01687) | 本文通过对二次回归模型中梯度下降的动力学进行全面研究，发现动力学可以用一个特定的立方映射来概括，并详细划分了五个训练阶段。同时，通过实验也证明了这些阶段的推广性能。 |
| [^80] | [A Framework for Interpretability in Machine Learning for Medical Imaging.](http://arxiv.org/abs/2310.01685) | 本文提出了一个机器学习在医学影像中的可解释性框架，明确了解释性的目标和要素，以指导方法设计并改进实际应用。 |
| [^81] | [Designing User-Centric Behavioral Interventions to Prevent Dysglycemia with Novel Counterfactual Explanations.](http://arxiv.org/abs/2310.01684) | 这项研究设计了一种以用户为中心的行为干预方法，通过提供新颖的反事实解释来预防血糖异常，有望对社会产生重要影响。 |
| [^82] | [Commutative Width and Depth Scaling in Deep Neural Networks.](http://arxiv.org/abs/2310.01683) | 该论文研究了深度神经网络中宽度和深度的可交换缩放。通过分析神经函数的行为，并确定了宽度和深度趋近于无穷大时的可交换性条件，并研究了神经协方差核的可交换性。研究结果表明，在具有跳跃连接的深度神经网络中，当分支适当缩放以避免爆炸行为时，将宽度和深度趋近于无穷大会得到相同的协方差结构。 |
| [^83] | [Estimating and Implementing Conventional Fairness Metrics With Probabilistic Protected Features.](http://arxiv.org/abs/2310.01679) | 本文提出了一种方法，用于在只有有限受保护属性标签访问的情况下估计和减少公平违规行为。该方法可以估计现有模型的公平度量范围，并通过解决优化问题训练模型以限制公平违规。与现有方法不同的是，该方法利用了上下文信息。 |
| [^84] | [Score dynamics: scaling molecular dynamics with picosecond timesteps via conditional diffusion model.](http://arxiv.org/abs/2310.01678) | 该论文提出了Score dynamics (SD) 方法，通过条件扩散模型，可以使用1 ps的时间步长进行分子动力学模拟。基于图神经网络的Score dynamics模型展示了在丙氨酸二肽和短链烷烃案例中的效果。 |
| [^85] | [Locality-Aware Graph-Rewiring in GNNs.](http://arxiv.org/abs/2310.01668) | 本论文提出了一种在GNN中考虑局部性的图重连技术，以减少过度压缩、保持图的稀疏性，并改善长程交互的捕捉能力。 |
| [^86] | [Artemis: HE-Aware Training for Efficient Privacy-Preserving Machine Learning.](http://arxiv.org/abs/2310.01664) | Artemis是一种针对基于同态加密的隐私保护机器学习的高效DNN修剪技术，通过HE感知修剪策略最大化降低计算成本，并取得了显著的改进。 |
| [^87] | [Home Electricity Data Generator (HEDGE): An open-access tool for the generation of electric vehicle, residential demand, and PV generation profiles.](http://arxiv.org/abs/2310.01661) | 本文介绍了家庭电力数据生成器(HEDGE)，它是一款用于生成真实住宅能源数据的开放接入工具。通过使用真实的英国数据，HEDGE能够生成每日的光伏发电曲线、家庭电力负荷以及电动车消耗和在家可用性等曲线。这一工具填补了当前研究中缺乏可用数据的问题。 |
| [^88] | [PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels.](http://arxiv.org/abs/2310.01655) | 本文通过使用多项式函数和多项式草图，实现了一个快速注意力机制PolySketchFormer，以突破Transformer架构中注意力机制的二次复杂性难题，无需假设注意力矩阵具有稀疏结构，并提出了高效的基于块的算法。 |
| [^89] | [Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations.](http://arxiv.org/abs/2310.01651) | 本文揭示了大型语言和视觉-语言模型中的一个特定漏洞，即它们对于多项选择问答的排列敏感性，这对于模型可靠性分析非常重要。这些漏洞在各种模型规模和最新的模型中都存在。 |
| [^90] | [CoDBench: A Critical Evaluation of Data-driven Models for Continuous Dynamical Systems.](http://arxiv.org/abs/2310.01650) | CoDBench是一个对连续动力系统数据驱动模型进行全面评估的基准套件，涵盖了11种最先进的模型，并对4种不同类别的这些模型进行了全面评估。这有助于科学机器学习领域的决策制定。 |
| [^91] | [On Training Derivative-Constrained Neural Networks.](http://arxiv.org/abs/2310.01649) | 该论文研究了导数约束神经网络（DC NN），提出了一种整合RELU激活函数（IReLU）来改进DC NN的训练，并探究了去归一化和标签缩放对于稳定DC训练的作用。研究还评估了这些方法在物理相关设置中的效果，证明了使用IReLU激活函数、去归一化和标签缩放的现有架构更好地融入了导数约束提供的训练信号。 |
| [^92] | [Equivariant Adaptation of Large Pre-Trained Models.](http://arxiv.org/abs/2310.01647) | 本论文提出了一种适用于大型预训练网络的等变适应方法，通过使用一个简单的规范化网络来使输入转换为规范形式。然而，我们观察到规范定向可能与训练分布的方向不一致，从而影响性能。 |
| [^93] | [Deep Insights into Noisy Pseudo Labeling on Graph Data.](http://arxiv.org/abs/2310.01634) | 该论文深入研究了图数据上嘈杂伪标签的影响，通过理论分析展示了伪标签对图学习模型性能的边界和收敛性的影响，并提出了一种谨慎的伪标签方法。 |
| [^94] | [Imitation Learning from Observation through Optimal Transport.](http://arxiv.org/abs/2310.01632) | 本文提出了一种通过最优输运进行从观察中的模仿学习的方法，该方法不需要学习模型或对抗学习，可以与任何强化学习算法集成，并在各种连续控制任务上超过了现有最先进方法，在ILfO设置下实现了专家级的性能。 |
| [^95] | [Operator Learning Meets Numerical Analysis: Improving Neural Networks through Iterative Methods.](http://arxiv.org/abs/2310.01618) | 本文通过将深度学习与经典数值分析进行比较，将神经网络视为具有固定点的算子，并通过迭代方法的收敛证明，改进了神经网络的性能。通过实证评估，我们证明了通过网络算子进行迭代可以提高性能，并通过引入迭代图神经网络PIGN进一步展示了迭代的好处。 |
| [^96] | [Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity.](http://arxiv.org/abs/2310.01616) | 本文理论上探讨了强化学习中样本效率和适应性之间的关系，发现样本效率算法需要的批次数K具有Ω(log log d)的下界，其中n = O(poly(d))。 |
| [^97] | [Intractability of Learning the Discrete Logarithm with Gradient-Based Methods.](http://arxiv.org/abs/2310.01611) | 本文研究了使用梯度法学习离散对数的困难度，发现损失函数梯度在一个固定点附近聚集，且无论网络架构复杂性如何，都会导致学习离散对数奇偶比特的能力受限。 |
| [^98] | [Adversarial Contextual Bandits Go Kernelized.](http://arxiv.org/abs/2310.01609) | 本研究通过将损失函数纳入再生核希尔伯特空间，对对抗性线性上下文多臂赌博的在线学习进行了内核化研究，并提出了一种计算效率高的算法，能够在多种特征值衰减假设下实现接近最优的遗憾保证。在多项式特征值衰减和指数特征值衰减的情况下，遗憾上界分别为 $\widetilde{O}(KT^{\frac{1}{2}(1+\frac{1}{c})})$ 和 $\widetilde{O}(\sqrt{T})$。 |
| [^99] | [Solving the Quadratic Assignment Problem using Deep Reinforcement Learning.](http://arxiv.org/abs/2310.01604) | 本文使用深度强化学习方法解决二次分配问题，提出了一种新颖的双指针网络，可以在没有特定实例重新训练的情况下产生解决方案。 |
| [^100] | [Pool-Based Active Learning with Proper Topological Regions.](http://arxiv.org/abs/2310.01597) | 本文提出了一种基于正确拓扑区域的池式主动学习策略的元方法，可用于多类别分类任务。该方法在各种基准数据集上进行了实证研究，并与文献中的经典方法具有竞争力。 |
| [^101] | [Prescribed Fire Modeling using Knowledge-Guided Machine Learning for Land Management.](http://arxiv.org/abs/2310.01593) | 本论文介绍了使用知识引导的机器学习框架来进行规定火模拟，解决了传统方法中的物理不一致性和预测偏差等问题。 |
| [^102] | [An Investigation of Representation and Allocation Harms in Contrastive Learning.](http://arxiv.org/abs/2310.01583) | 该论文研究了对比学习中的表示和分配伤害问题，发现在自监督学习中，对比学习容易造成少数群体的表示和多数群体的表示合并，从而导致分配伤害。通过因果中介分析和随机块模型的解释，强调了研究和缓解这种表示伤害的重要性。 |
| [^103] | [On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?.](http://arxiv.org/abs/2310.01581) | 本论文研究了开源大型语言模型的安全性，指出对齐不能真正防止它们被滥用，可以通过简单的方式误导它们生成不期望的内容。 |
| [^104] | [Contraction Properties of the Global Workspace Primitive.](http://arxiv.org/abs/2310.01571) | 本文扩展了关于多区域递归神经网络的稳定性研究，并在全局工作空间模块化结构上证明了松散稳定性条件。实证结果显示全局工作空间稀疏组合网络在测试表现和韧性方面表现出较好的性能，强调了稳定性对于实现模块化RNN的重要性。 |
| [^105] | [Iterative Option Discovery for Planning, by Planning.](http://arxiv.org/abs/2310.01569) | 本文提出了一种迭代式选项发现方法，通过学习一组局部强策略来指导搜索算法，从而在强化学习和规划中应用于复杂领域。 |
| [^106] | [Causality-informed Rapid Post-hurricane Building Damage Detection in Large Scale from InSAR Imagery.](http://arxiv.org/abs/2310.01565) | 本文介绍了一种从InSAR图像中快速检测飓风后建筑损伤的方法，通过编码复杂的因果关系图来提高建筑损害信息的准确性。 |
| [^107] | [SmartPlay : A Benchmark for LLMs as Intelligent Agents.](http://arxiv.org/abs/2310.01557) | SmartPlay是一个用于评估LLMs作为智能Agent能力的基准，包括6个具有不同挑战的游戏，并测试了智能LLM Agent的多种关键能力。这不仅是一个评估LLM Agent整体性能的严格测试场地，还可以分析每个能力的表现。 |
| [^108] | [Harnessing the Power of Choices in Decision Tree Learning.](http://arxiv.org/abs/2310.01551) | 该研究提出了一种利用选择能力的决策树学习算法Top-$k$，相比于传统贪婪算法和最优决策树算法，在准确率和性能上取得更好的结果。 |
| [^109] | [On the near-optimality of betting confidence sets for bounded means.](http://arxiv.org/abs/2310.01547) | 本文为投注置信区间的改进性能提供了理论解释，并比较了经典方法中的宽度，发现投注置信区间具有较小的极限宽度。 |
| [^110] | [Fusing Models with Complementary Expertise.](http://arxiv.org/abs/2310.01542) | 该论文研究了融合具有互补专长的模型的问题，并将其应用于不同任务和领域中，通过监督学习的方式实现了显著的性能提升。 |
| [^111] | [Adversarial Client Detection via Non-parametric Subspace Monitoring in the Internet of Federated Things.](http://arxiv.org/abs/2310.01537) | 本文提出了一种基于非参数子空间监测的方法FedRR，用于对抗客户检测和控制误报率，以解决联邦物联网中的安全问题。 |
| [^112] | [Nowcasting day-ahead marginal emissions using multi-headed CNNs and deep generative models.](http://arxiv.org/abs/2310.01524) | 本论文提出了一种利用多头卷积神经网络和深度生成模型的方法，进行现场预测次日边际排放因素，以适应高灵活性和分布式能源资源渗透的电力系统。 |
| [^113] | [The Benefit of Noise-Injection for Dynamic Gray-Box Model Creation.](http://arxiv.org/abs/2310.01517) | 本文研究了噪声注入在动态灰箱模型创建中的益处。通过向训练数据集中注入噪声，可以解决模型的非线性、未建模的动态和局部极小值等挑战，在水-水换热器的动态模型测试中取得了显著的建模误差减小。 |
| [^114] | [Tensor Ring Optimized Quantum-Enhanced Tensor Neural Networks.](http://arxiv.org/abs/2310.01515) | 张量环优化的增强量子张量神经网络（TR-QNet）将串联纠缠门应用于张量网络，通过量子比特测量和随机梯度下降算法对其参数进行优化，实现了在多个数据集上的二元分类精度提升。 |
| [^115] | [CODA: Temporal Domain Generalization via Concept Drift Simulator.](http://arxiv.org/abs/2310.01508) | 该论文介绍了一个模型无关的时间域泛化方法，通过概念漂移模拟器来解决真实世界数据中的概念漂移问题。 |
| [^116] | [Quantum-Based Feature Selection for Multi-classification Problem in Complex Systems with Edge Computing.](http://arxiv.org/abs/2310.01443) | 本文提出了一种基于量子的多分类问题特征选择算法QReliefF，通过量子编码和相似性计算，可以有效降低算法复杂性并提高计算效率。 |
| [^117] | [Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets.](http://arxiv.org/abs/2310.01438) | 本文提出了一种灵活、可扩展且机器学习准备的多模态肿瘤学数据集(MINDS)框架，用于融合来自不同来源的数据，并提供了探索关系和构建大规模多模态机器学习模型的界面。 |
| [^118] | [Graph Neural Architecture Search with GPT-4.](http://arxiv.org/abs/2310.01436) | 本文将GPT-4集成到图神经网络架构搜索（GNAS）中，提出了一种新的GPT-4基于的GNAS方法（GPT4GNAS），通过设计新的提示来引导GPT-4生成更准确的图神经网络，实验证明嵌入GPT-4到GNAS中优于现有方法。 |
| [^119] | [Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile.](http://arxiv.org/abs/2310.01434) | 本文介绍了一种在移动设备上执行30亿参数的GPT LLM的创新方法，通过集成本地代码和模型量化技术实现了在低内存设备上的平稳运行，并解决了网络依赖性的问题。这种应用不仅具有通用助手功能，还可以实现文本到操作的无缝移动互动。 |
| [^120] | [AI-Aristotle: A Physics-Informed framework for Systems Biology Gray-Box Identification.](http://arxiv.org/abs/2310.01433) | AI-Aristotle是一种物理启发的框架，用于系统生物学中的参数估计和灰盒识别。它结合了X-TFC和PINNs技术，并使用符号回归技术进行参数发现和验证。通过在两个系统生物学基准问题上的测试，我们验证了AI-Aristotle的准确性、速度、灵活性和鲁棒性。 |
| [^121] | [REMEDI: REinforcement learning-driven adaptive MEtabolism modeling of primary sclerosing cholangitis DIsease progression.](http://arxiv.org/abs/2310.01426) | REMEDI是一个基于强化学习的框架，能够捕捉PSC疾病进展过程中胆汁酸动力学和机体自适应响应，有助于探索治疗方法。 |
| [^122] | [Borges and AI.](http://arxiv.org/abs/2310.01425) | 这篇论文主张通过探索乔治·路易斯·博尔赫斯的意象来理解大型语言模型和人工智能之间的关系。 |
| [^123] | [An Empirical Study of AI Generated Text Detection Tools.](http://arxiv.org/abs/2310.01423) | 本研究实证研究了AI生成文本检测工具在多领域ChatGPT材料中的有效性，并创建了一个多领域数据集来测试最先进API和工具的能力。 |
| [^124] | [Design Principles of Robust Multi-Armed Bandit Framework in Video Recommendations.](http://arxiv.org/abs/2310.01419) | 本文提出了一种鲁棒多臂赌博框架的设计原则，以解决推荐系统中的利用挑战和分布变化问题，并通过实验证明了其相对收益提升。 |
| [^125] | [Gramian Angular Fields for leveraging pretrained computer vision models with anomalous diffusion trajectories.](http://arxiv.org/abs/2310.01416) | 利用Gramian Angular Fields方法，我们提出了一种新的数据驱动方法，用于处理异常扩散轨迹，并成功应用于多个领域，包括物理学、化学、生物学和生态学。 |
| [^126] | [Representation Engineering: A Top-Down Approach to AI Transparency.](http://arxiv.org/abs/2310.01405) | 这项研究介绍了一种名为表示工程化（RepE）的自上而下方法，通过借鉴认知神经科学的见解，提供了一种增强AI系统透明性的解决方案。该方法将集群级别的表示放在分析的核心，为监测和操纵深度神经网络中的高级认知现象提供了新的方法，并展示了在解决与安全相关的问题上的潜力。 |
| [^127] | [On the Generalization of Training-based ChatGPT Detection Methods.](http://arxiv.org/abs/2310.01307) | 本研究综合调查了基于训练的ChatGPT检测方法在分布漂移下的泛化行为，包括提示、文本长度、主题和语言任务，在新数据集上揭示了有启示性的发现，为未来方法或数据的开发提供了指导。 |
| [^128] | [Faster and Accurate Neural Networks with Semantic Inference.](http://arxiv.org/abs/2310.01259) | 本研究提出了一种名为语义推理（SINF）的新框架，在减少计算负载的同时，通过聚类语义相似的类来提取子图，从而减少深度神经网络的计算负担，并在性能上有限损失。 |
| [^129] | [Graph Isomorphic Networks for Assessing Reliability of the Medium-Voltage Grid.](http://arxiv.org/abs/2310.01181) | 本论文提出使用图同构网络（GINs）进行中压电网的n-1评估，相比传统数学优化方法，GIN方法展示了更快和更可靠的电网评估，将预测时间缩短约1000倍。 |
| [^130] | [A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge Regression.](http://arxiv.org/abs/2310.00987) | 本研究通过推导任意有限秩核岭回归模型的尖锐的非渐近性上界和下界，填补了有限秩核岭回归保证的空白。 |
| [^131] | [Path Structured Multimarginal Schr\"odinger Bridge for Probabilistic Learning of Hardware Resource Usage by Control Software.](http://arxiv.org/abs/2310.00604) | 该论文研究了硬件资源使用的概率学习，通过解决路径结构多重边桥问题，可以预测控制软件的硬件资源利用情况，并在任意时间实现线性收敛。 |
| [^132] | [JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention.](http://arxiv.org/abs/2310.00535) | 本文提出了联合MLP/注意力（JoMA）动态，用于解析多层Transformer架构的训练过程。通过预测非线性激活情况下注意力的行为，我们解释了多层Transformer中标记的层次组合方法。实验证实了我们的理论发现。 |
| [^133] | [On the Stability of Iterative Retraining of Generative Models on their own Data.](http://arxiv.org/abs/2310.00429) | 本文研究了生成模型在混合数据集上训练对稳定性的影响，通过证明初始生成模型足够接近数据分布并且数据比例适当，迭代训练是稳定的。 |
| [^134] | [FedLPA: Personalized One-shot Federated Learning with Layer-Wise Posterior Aggregation.](http://arxiv.org/abs/2310.00339) | 本文提出了一种名为FedLPA的新颖方法，采用分层后验聚合的方式实现个性化单次联邦学习。FedLPA能够高效地将本地模型聚合到全局模型，解决了单次聚合在非相同训练数据分布下的性能问题。 |
| [^135] | [CausalImages: An R Package for Causal Inference with Earth Observation, Bio-medical, and Social Science Images.](http://arxiv.org/abs/2310.00233) | causalimages R 包提供了使用图像进行因果推断的新工具，并能够整合卫星和生物医学图像等新数据源，通过分解治疗效果的异质性和控制混淆变量来进行分析。 |
| [^136] | [Multi-Resolution Active Learning of Fourier Neural Operators.](http://arxiv.org/abs/2309.16971) | 提出了一种多分辨率主动学习的傅里叶神经算子（MRA-FNO），通过动态选择输入函数和分辨率来降低数据成本并优化学习效率。 |
| [^137] | [ONNXExplainer: an ONNX Based Generic Framework to Explain Neural Networks Using Shapley Values.](http://arxiv.org/abs/2309.16916) | ONNXExplainer是一个基于ONNX的通用框架，使用Shapley值解释神经网络。它提供了自动微分和优化方法，实现了一次性部署和高效的解释计算。 |
| [^138] | [Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients.](http://arxiv.org/abs/2309.16742) | 该论文研究了2型糖尿病患者白蛋白尿的早期预测问题，并开发了一种监督学习模型。通过使用不同的监督学习算法对184条数据进行训练，得出了预测结果。 |
| [^139] | [Robust Offline Reinforcement Learning -- Certify the Confidence Interval.](http://arxiv.org/abs/2309.16631) | 本文提出了一种使用随机平滑算法认证给定策略在离线环境中的鲁棒性的方法，证明了该算法的高效性，并得到实验证实。 |
| [^140] | [Bayesian Personalized Federated Learning with Shared and Personalized Uncertainty Representations.](http://arxiv.org/abs/2309.15499) | 本文提出了一种贝叶斯个性化联邦学习（BPFL）的框架，用于处理联邦学习系统中的客户端不确定性和异质性。这个框架通过分解和共同学习统计异质性客户端数据上的共享和个性化不确定表示。 |
| [^141] | [SEPT: Towards Efficient Scene Representation Learning for Motion Prediction.](http://arxiv.org/abs/2309.15289) | SEPT是一个利用自监督学习进行场景表示学习的建模框架，通过预训练的编码器捕捉轨迹的运动学特征、道路网络的空间结构以及道路和代理之间的交互作用，实现了在运动预测任务上的最先进性能。 |
| [^142] | [Tranformer-based classification of user queries for medical consultancy with respect to expert specialisation.](http://arxiv.org/abs/2309.14662) | 本研究利用RuBERT模型和Transformer技术，提出了一种用于医学咨询的用户查询分类方法，重点关注专家特长，表现出超过92%的性能，具有良好的泛化性能和实际应用价值。 |
| [^143] | [Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas.](http://arxiv.org/abs/2309.14610) | 本研究基于无监督图深度学习模型提出了集成城市洪水风险评级模型，能够捕捉区域之间的空间依赖关系和洪水危险与城市要素之间的复杂相互作用，揭示了城市地区的突发洪水风险概况 |
| [^144] | [Parallelizing non-linear sequential models over the sequence length.](http://arxiv.org/abs/2309.12252) | 本论文提出了一种并行算法，能够加速GPU对于顺序模型的评估速度，提高了3个数量级，而不降低输出准确性。该算法适用于各种架构，并在长时间序列分类问题中发现了门控循环单元的有效性。 |
| [^145] | [Intelligent machines work in unstructured environments by differential neural computing.](http://arxiv.org/abs/2309.08835) | 本研究提出了一种基于差分神经计算的智能机器方法，通过提取环境信息的主要特征并应用相应的编码刺激到记忆阻性器件，成功地实现了处理无结构环境信息的类人能力，并展现了良好的可扩展性和泛化性。该方法在物体抓取和自动驾驶等应用方面得到了验证。 |
| [^146] | [Towards Understanding Neural Collapse: The Effects of Batch Normalization and Weight Decay.](http://arxiv.org/abs/2309.04644) | 本文研究了批归一化和权重衰减对神经网络坍塌现象的影响，并提出了对坍塌现象进行度量的方法。通过理论和实验，证明了在接近最优时，批归一化和权重衰减能够促使神经网络坍塌的出现。 |
| [^147] | [Asymmetric Momentum: A Rethinking of Gradient Descent.](http://arxiv.org/abs/2309.02130) | Loss-Controlled Asymmetric Momentum (LCAM) is proposed as a simple and versatile optimization method that can adapt to all types of datasets by dividing the training process into different loss phases and using different momentum. Experimental results suggest that frequently-changing parameters should be accelerated in non-sparse gradients, challenging the conventional wisdom. |
| [^148] | [Selective Scene Text Removal.](http://arxiv.org/abs/2309.00410) | 本论文提出了一种选择性场景文本去除（SSTR）任务，该任务可以根据用户指定的目标词汇来去除场景图像中的文本，实验结果表明所提出的方法有效地实现了目标词汇的去除。 |
| [^149] | [Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis & Benchmark].](http://arxiv.org/abs/2308.12899) | 该论文提出了针对城市时空预测的统一数据管理和综合性能评估方法。其贡献包括引入“原子文件”作为统一存储格式，提供了城市时空预测模型技术进展的全面概述，并建立了性能排行榜和鉴定了有潜力的模型和数据集。 |
| [^150] | [Maintaining Plasticity via Regenerative Regularization.](http://arxiv.org/abs/2308.11958) | 本文提出了一种名为L2 Init的简单方法，通过将L2正则化应用于初始参数，来维持神经网络在处理非平稳数据流时的可塑性且易于实施。该方法使得参数能够迅速适应新任务并减轻可塑性的丢失。 |
| [^151] | [Utilizing Admissible Bounds for Heuristic Learning.](http://arxiv.org/abs/2308.11905) | 本文通过将可接受启发式作为截断高斯分布的参数，明确了在监督启发式学习中可接受启发式的作用，紧缩了假设空间。 |
| [^152] | [Data Race Detection Using Large Language Models.](http://arxiv.org/abs/2308.07505) | 本研究探索了一种基于大型语言模型的数据竞争检测方法，通过结合提示工程和微调技术，发现LLMs可以作为数据竞争检测的可行方法，但在需要详细信息时仍不如传统工具竞争。 |
| [^153] | [Deep Learning based Image Watermarking: A Brief Survey.](http://arxiv.org/abs/2308.04603) | 该论文调查了基于深度学习的图像水印技术的最新研究进展，将其分为嵌入器-提取器联合训练、深度网络作为特征变换和混合方案。对每个类别的研究方向进行了分析和总结，并讨论了未来的研究方向。 |
| [^154] | [ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs.](http://arxiv.org/abs/2307.16789) | ToolLLM是一个通用的工具使用框架，旨在促进大型语言模型掌握16000多个真实世界API。该框架包括数据构建、模型训练和评估。使用ChatGPT自动构建的ToolBench数据集用于指令调整，涵盖了各种工具使用情境和API。 |
| [^155] | [Deep Learning Meets Adaptive Filtering: A Stein's Unbiased Risk Estimator Approach.](http://arxiv.org/abs/2307.16708) | 本文通过算法展开的方法，将递归最小二乘法（RLS）和等变自适应源分离（EASI）两种算法转化为深度神经网络的层，通过利用训练过程有效地进行源信号估计。同时，通过使用基于Stein无偏风险估计（SURE）的损失函数训练，进一步提高了性能，实证评估证明了该方法的有效性。 |
| [^156] | [Corruption-Robust Lipschitz Contextual Search.](http://arxiv.org/abs/2307.13903) | 该论文研究了学习具有被篡改的二进制信号的Lipschitz函数的问题，提出了一种腐败鲁棒算法。该算法在不同损失函数下实现了不同程度的后悔。 |
| [^157] | [A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis.](http://arxiv.org/abs/2307.12856) | 这篇论文介绍了一种名为WebAgent的LLM驱动代理，通过自我经验学习，在真实网站上完成任务。该方法通过规划、总结和生成代码来提高在真实网站上的成功率。 |
| [^158] | [In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning.](http://arxiv.org/abs/2307.12375) | 大型语言模型（LLMs）在包含标签关系示例的上下文中的学习能力使其在下游任务中表现显著提高，但与传统学习方法不同。我们研究了上下文示例中的标签如何影响预测、预训练中学习到的标签关系如何与上下文示例相互作用以及上下文学习如何聚合标签信息。研究结果揭示了LLMs的工作机制及其对上下文信息的处理方式。 |
| [^159] | [(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs.](http://arxiv.org/abs/2307.10490) | 本论文展示了如何利用图像和声音在多模态LLMs中进行间接指令注入，攻击者通过生成对抗扰动并将其融入图像或音频录音中，以操纵模型输出特定文本和指导对话的行为。 |
| [^160] | [A Meta-Learning Based Precoder Optimization Framework for Rate-Splitting Multiple Access.](http://arxiv.org/abs/2307.08822) | 本文提出了一种基于元学习的预编码优化框架，通过利用紧凑神经网络过拟合来优化Rate-Splitting Multiple Access (RSMA)预编码，结合部分信道状态信息，从而绕过了其他训练数据的需求，同时在中等规模情况下达到了类似传统方法的平均和速率性能，在大规模情况下显著优于低复杂度算法。 |
| [^161] | [In-context Autoencoder for Context Compression in a Large Language Model.](http://arxiv.org/abs/2307.06945) | 在大型语言模型中，我们提出了一种称为In-context Autoencoder (ICAE)的上下文自编码器，它通过将长上下文压缩为有限数量的内存槽，实现了$4\times$的上下文压缩，并能够根据内存槽进行条件处理以响应各种提示。 |
| [^162] | [Suppressing unknown disturbances to dynamical systems using machine learning.](http://arxiv.org/abs/2307.03690) | 本文提出了一种使用机器学习的无模型方法，可以仅通过系统在已知强迫函数影响下的观测，识别和抑制未知系统的未知干扰。这项方法对训练函数有非常温和的限制，能够稳健地识别和抑制大类别的未知干扰。 |
| [^163] | [Empirical Analysis of a Segmentation Foundation Model in Prostate Imaging.](http://arxiv.org/abs/2307.03266) | 本文通过对前列腺成像中的新型基础模型UniverSeg进行了实证评估研究，并将其与传统方法进行了比较。结果表明基础模型可能是医学成像领域未来的方向。 |
| [^164] | [GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models.](http://arxiv.org/abs/2306.13649) | 本文提出了广义知识蒸馏（GKD），通过从学生中采样输出序列来缓解分布不匹配，并在优化替代KL等离散度方面处理模型欠规范，达到了在摘要任务上最先进的性能。 |
| [^165] | [Scaling MLPs: A Tale of Inductive Bias.](http://arxiv.org/abs/2306.13575) | 本文研究了多层感知器（MLP）在视觉任务中的性能极限，并探讨了MLP相较于其他深度学习模型的归纳偏差，旨在推进深度学习理论和实践的结合。 |
| [^166] | [OpenDataVal: a Unified Benchmark for Data Valuation.](http://arxiv.org/abs/2306.10577) | 本文介绍了一种名为OpenDataVal的基准测试框架，该框架整合了多种数据集和九种最先进的数据估值算法实现，并提供了四个下游机器学习任务来评估数据价值的质量。 |
| [^167] | [Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation.](http://arxiv.org/abs/2306.06192) | Ada-NAV是一种自适应轨迹优化策略学习方法，采用降低策略随机性的方法平衡探索与利用，提高机器人导航任务的采样效率。在真实世界的测试中表现优异，可以在更短的采样时间内取得更高的性能。 |
| [^168] | [Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models.](http://arxiv.org/abs/2306.05272) | 本论文提出了一种新的图像聚类流程，利用大型预训练模型的强大特征表示，在规模上有效地对图像进行聚类，并通过优化速率降低目标和 CLIP 的图像-文本绑定，成功地提高了聚类的准确性和自标记算法的效果。 |
| [^169] | [Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis.](http://arxiv.org/abs/2306.00814) | Vocos是一个新的模型，通过直接生成傅里叶谱系数，消除了时域和基于傅里叶变化的神经声码器在高质量音频合成中的差距，并实现了计算效率的大幅提升。 |
| [^170] | [HiGen: Hierarchical Graph Generative Networks.](http://arxiv.org/abs/2305.19337) | HiGen是一种新颖的图形生成网络，能够以粗到细的方式捕捉图形的层次结构，并使用多项式分布来生成具有整数值的子图的边权。它能够有效地捕捉图形的局部和全局属性，并实现了最先进的性能。 |
| [^171] | [On Diffusion Modeling for Anomaly Detection.](http://arxiv.org/abs/2305.18593) | 本文研究了扩散建模在无监督和半监督异常检测中的应用，发现去噪扩散概率模型表现很好但计算成本高，因此提出了一种替代方法——扩散时间概率模型，该模型能够通过较大的时间步长上的高后验密度识别异常，并通过深度神经网络提高效率。 |
| [^172] | [Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning.](http://arxiv.org/abs/2305.18403) | 本论文提出了一种名为LoRAPrune的框架，可以高效微调和部署大型预训练模型，通过利用低秩自适应的值和梯度来设计PEFT感知的剪枝准则，并提出了一个迭代剪枝过程来去除冗余参数，实验结果表明与最先进的方法相比，可以显著降低模型大小和推理时间，同时保持竞争性的准确性。 |
| [^173] | [Automated Search-Space Generation Neural Architecture Search.](http://arxiv.org/abs/2305.18030) | Automated Search-Space Generation Neural Architecture Search (ASGNAS) is an automated system that trains general DNNs covering all candidate connections and operations and produces high-performing sub-networks. It minimizes the need for human expertise and manual intervention by automatically generating the search space and utilizing a Hierarchical Half-Space Projected Gradient (H2SPG) to ensure network validity and performance. |
| [^174] | [The Representation Jensen-Shannon Divergence.](http://arxiv.org/abs/2305.16446) | 本文提出了一种基于表示的新型散度——表示Jensen-Shannon散度，通过将数据分布嵌入到RKHS中，并利用表示的协方差算子的频谱，实现对数据分布的估计，并提供了具有灵活性，可扩展性，可微分性的经验协方差矩阵估计函数和基于核矩阵的估计函数。 |
| [^175] | [Differentially Private Latent Diffusion Models.](http://arxiv.org/abs/2305.15759) | 本文提出使用差分隐私训练潜在扩散模型(LDMs)，通过预训练自编码器将高维像素空间转变为低维潜在空间实现更高效快速的DMs训练，并且通过只微调注意力模块减少了可训练参数的数量。 |
| [^176] | [Transferring Learning Trajectories of Neural Networks.](http://arxiv.org/abs/2305.14122) | 本研究提出了转移学习轨迹的算法，可将之前训练过的神经网络的学习轨迹应用在新的训练中，并能在任何直接训练之前实现非平凡的准确性。 |
| [^177] | [Causality-Aided Trade-off Analysis for Machine Learning Fairness.](http://arxiv.org/abs/2305.13057) | 本论文使用因果分析作为一种方法，通过分析机器学习流程中公平性参数与其他关键指标的权衡，提供了一种系统理解和决策基础，帮助开发者在提供公平机器学习服务时做出明智的决策。 |
| [^178] | [Evaluation Metrics for CNNs Compression.](http://arxiv.org/abs/2305.10616) | 本文提供了神经网络压缩的评估度量综述，从而为标准化神经网络压缩贡献力量。 |
| [^179] | [Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples.](http://arxiv.org/abs/2305.09241) | “无法学习的样本”提出一种对数据进行保护的方法，但它无法阻止未经授权的用户对保护后的数据进行利用。通过提出“可学习的未经授权示例”和一种新的纯化过程，我们可以实现对数据的更好保护。 |
| [^180] | [Efficient Bayesian inference using physics-informed invertible neural networks for inverse problems.](http://arxiv.org/abs/2304.12541) | 本文提出了一种使用物理信息可逆神经网络(PI-INN)解决贝叶斯反问题的新方法，该方法可以高效地进行抽样和准确的密度评估。研究通过残差项和独立性损失项确保了INN输出的统计独立性，并在多项实验中证明了其有效性和准确性。 |
| [^181] | [On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation.](http://arxiv.org/abs/2304.11328) | 本文提出了一种通过优化系数来加速流行的基于反向ODE解算的扩散采样过程的方法，优化方法为改进的积分逼近（IIA），在每个反向时间步长，我们建议针对某些选择的系数最小化MSE函数，给定预训练的扩散模型，只需要在一批样本上进行特定数量的神经功能评估（NFEs）一次IIA过程即可获得最佳解。 |
| [^182] | [Bridging Discrete and Backpropagation: Straight-Through and Beyond.](http://arxiv.org/abs/2304.08612) | 本文提出了一种新方法来逼近生成离散潜变量的参数的梯度，其中包括了一些数值方法，实现了二阶精度，取得了实验上的持续改进。 |
| [^183] | [On the Possibilities of AI-Generated Text Detection.](http://arxiv.org/abs/2304.04736) | 该研究探讨了AI生成文本检测的可能性，提出了精确的样本复杂度界限，并指出了设计更准确的检测方法和提高透明度的挑战。 |
| [^184] | [Adversarial Robustness of Learning-based Static Malware Classifiers.](http://arxiv.org/abs/2303.13372) | 本文提出了一种通用的对抗性补丁（UAP）攻击方法，只需附加一个相对较小的字节级补丁即可绕过MalConv分类器的检测，可以将恶意软件文件的检测率降低80％。同时，作者提出了窗口消除处理作为应对此种攻击的一种方法。 |
| [^185] | [The Probabilistic Stability of Stochastic Gradient Descent.](http://arxiv.org/abs/2303.13093) | 本文重新定义了随机梯度下降的稳定性，并使用概率稳定性来回答深度学习理论中的一个根本问题：SGD如何从数量庞大的可能严重过拟合的解中选择有意义的神经网络解。 |
| [^186] | [Visual Analytics of Multivariate Networks with Representation Learning and Composite Variable Construction.](http://arxiv.org/abs/2303.09590) | 本文提出了一种用于研究多变量网络的视觉分析工作流程，其中包括神经网络学习阶段、降维和优化阶段以及用户交互式可视化接口进行解释。关键的组合变量构建步骤将非线性特征重塑为线性特征，以方便检查和理解。案例研究表明该工作流程具有有效性和可理解性。 |
| [^187] | [Learning to Grow Artificial Hippocampi in Vision Transformers for Resilient Lifelong Learning.](http://arxiv.org/abs/2303.08250) | 本文提出了一种在Vision Transformer中学习成长人工海马的方法，以实现弹性终身学习。通过神经架构搜索进行维护，选取多头自注意力块中的最终线性投影层进行ArtiHippo的实现和成长。 |
| [^188] | [On the Unlikelihood of D-Separation.](http://arxiv.org/abs/2303.05628) | 本文提供了分析证据表明，在大型图中，d-分离是罕见的现象，除非图是极其稀疏的。对于因果发现的PC算法和UniformSGS算法进行了平均情况分析，并给出了上界，上界以指数速度衰减到0。 |
| [^189] | [Lumos: Heterogeneity-aware Federated Graph Learning over Decentralized Devices.](http://arxiv.org/abs/2303.00492) | Lumos是第一个支持节点级联邦图学习的框架，具有特征和度数保护功能。它采用了树构造器和去中心化节点聚合策略来提高图的表示能力。 |
| [^190] | [Infinite-Dimensional Diffusion Models.](http://arxiv.org/abs/2302.10130) | 该论文提出了一种在无限维度中直接制定扩散基于的生成模型的方法，相比于传统的先离散化再应用扩散模型的方法，这种方法能够避免参数细化导致算法性能下降，同时提供了维度无关的距离界限，为无限维扩散模型设计提供了准则。 |
| [^191] | [Koopman-based generalization bound: New aspect for full-rank weights.](http://arxiv.org/abs/2302.05825) | 我们提出了一种使用Koopman算子对全秩神经网络权重进行泛化的新界限，当权重矩阵的条件数较小时，该界限比现有基于范数的界限更紧。我们的界限不与现有界限相矛盾，而是对现有界限进行的补充。此外，我们的界限可以与现有界限结合以得到更紧的界限。该研究结果为理解全秩权重神经网络的泛化提供了新的视角，同时也为算子理论分析和神经网络泛化之间提供了连接。 |
| [^192] | [Chain of Hindsight Aligns Language Models with Feedback.](http://arxiv.org/abs/2302.02676) | 该研究提出了一种新颖的技术，即回顾链，可以轻松优化，并可以从任何形式的反馈中学习，而不受其极性的影响。 |
| [^193] | [Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data.](http://arxiv.org/abs/2302.00674) | 本文提出了一种在少样本学习过程中假定有辅助数据的训练范式FLAD，并针对自动混合辅助和目标数据的方法局限，提出了两种计算复杂度独立于辅助数据集数量的算法，通过FLAD和这两种算法的比较，可以发现这两种算法的表现更好。 |
| [^194] | [Deep Operator Learning Lessens the Curse of Dimensionality for PDEs.](http://arxiv.org/abs/2301.12227) | 本文指出，利用DNN对Banach空间上的Lipschitz算子进行深度算子学习可以减轻PDE中的维数灾难，包括椭圆方程、抛物线方程和Burgers方程，并可用于提供算子学习中的离散化不变性的见解。 |
| [^195] | [Efficient Activation Function Optimization through Surrogate Modeling.](http://arxiv.org/abs/2301.05785) | 本文提出了一种基于代理建模的方法，通过扩展的基准测试空间在较少的函数评估次数中发现了优化的高效激活函数架构，并在多个标准基准测试中实现了最先进的性能。 |
| [^196] | [Learning k-Level Sparse Neural Networks Using a New Generalized Weighted Group Sparse Envelope Regularization.](http://arxiv.org/abs/2212.12921) | 本论文提出了一种利用加权组稀疏包络正则化方法学习k级稀疏神经网络的高效方法，同时保证网络的硬件友好的结构化稀疏性，加快网络评估速度，而且能够在训练中预定义稀疏度水平，同时几乎不降低网络准确度甚至有可能提高。 |
| [^197] | [Expanding Small-Scale Datasets with Guided Imagination.](http://arxiv.org/abs/2211.13976) | 本论文提出了一个引导想象框架(GIF)，通过利用DALL-E2和Stable Diffusion (SD)等生成模型，从种子数据中扩充小规模数据集。该框架通过在先验模型的语义空间中优化种子数据潜在特征来创建逼真的图像，并引入了类别保持和样本多样性的标准来指导想象过程。 |
| [^198] | [Exploring Physical Latent Spaces for High-Resolution Flow Restoration.](http://arxiv.org/abs/2211.11298) | 本文探索了使用物理模拟的潜空间来训练深度神经网络模型，实现了对细致模拟的恢复。通过将模拟的自由度作为神经网络的工具，并使用物理降维潜空间进行学习，神经网络能够发现改进给定任务性能的替代动态。 |
| [^199] | [Square-root regret bounds for continuous-time episodic Markov decision processes.](http://arxiv.org/abs/2210.00832) | 本论文研究了连续时间情节马尔可夫决策过程的强化学习问题，提出了一种基于值迭代和上界置信区间的学习算法，并导出了该算法的最坏情况下预期遗憾的上界和下界，均为情节数的平方根阶。模拟实验结果证明了该算法的性能。 |
| [^200] | [A Machine Learning Approach to Solving Large Bilevel and Stochastic Programs: Application to Cycling Network Design.](http://arxiv.org/abs/2209.09404) | 我们提出了一种基于机器学习的新方法，用于求解涉及大量独立从属者的双层规划问题。该方法通过从一个采样的子集中估计未采样从属者的目标值来优化模型，实现了对一般从属者特征的表征学习。 |
| [^201] | [Learning Task Automata for Reinforcement Learning using Hidden Markov Models.](http://arxiv.org/abs/2208.11838) | 本文提出了一种学习非马尔可夫任务规范的新方法，通过从代理经验中学习，将其表示为有限状态的任务自动机。利用隐马尔可夫模型和新的提炼方法，使得代理能够解决稀疏和非马尔可夫奖励的强化学习任务。 |
| [^202] | [Bucketized Active Sampling for Learning ACOPF.](http://arxiv.org/abs/2208.07497) | 本文提出了一种新颖的主动学习框架——分桶主动采样（BAS），旨在在时间限制内训练最佳的OPF代理。BAS将输入分布分成桶，并使用收集函数确定下一次采样的位置。实验结果显示了BAS的好处。 |
| [^203] | [Physics-Constrained Deep Learning for Climate Downscaling.](http://arxiv.org/abs/2208.05424) | 本文提出了一种物理约束深度学习降尺度模型的方法，以保证模型在预测物理变量时满足守恒定律，并提高其性能。 |
| [^204] | [A general framework for multi-step ahead adaptive conformal heteroscedastic time series forecasting.](http://arxiv.org/abs/2207.14219) | 本文介绍了一种名为AEnbMIMOCQR的新颖算法，通过自适应集成的方式，在不需要数据拆分的情况下，以分布无关的方式生成多步鲍型预测区间。该方法考虑了异方差性，并对分布转变具有鲁棒性，在实验中表现优于其他竞争方法。 |
| [^205] | [Memory Population in Continual Learning via Outlier Elimination.](http://arxiv.org/abs/2207.01145) | 本文引入了一种名为内存异常值消除（MOE）的方法，通过选择来自标签同质子种群的样本来识别和消除内存缓冲区中的异常值，以解决连续学习中的遗忘问题。 |
| [^206] | [Distillation Decision Tree.](http://arxiv.org/abs/2206.04661) | 精馏决策树（DDT）是一种通过将黑盒模型中的知识精馏到决策树中来促进解释性的方法。该方法建立在知识精馏的理论基础上，并且在结构稳定性的条件下可以有效实现。 |
| [^207] | [Locally Invariant Explanations: Towards Stable and Unidirectional Explanations through Local Invariant Learning.](http://arxiv.org/abs/2201.12143) | 本文提出了一种基于局部不变性学习的模型无关解释方法，通过借鉴不变风险最小化原理，可以生成高保真度、稳定且单向的解释。方法基于博弈论的形式化，能够在局部例子附近准确捕捉黑盒函数的梯度符号变化，选择恰当的特征归因。 |
| [^208] | [Sci-Net: Scale Invariant Model for Buildings Segmentation from Aerial Imagery.](http://arxiv.org/abs/2111.06812) | 本文提出了一种尺度不变的神经网络架构Sci-Net，用于从广泛范围空间分辨率的航空图像中分割建筑物，并且在实验中表现出显著优越性能。 |
| [^209] | [Dual Correction Strategy for Ranking Distillation in Top-N Recommender System.](http://arxiv.org/abs/2109.03459) | 本文提出了一种双重修正策略（DCD），用于在推荐系统中更有效地将教师模型的排名信息转移到学生模型。这种方法不仅充分利用了学生模型的预测误差，还提供了更全面的视角，解决了松弛排名蒸馏方法的限制。 |
| [^210] | [Certifiers Make Neural Networks Vulnerable to Availability Attacks.](http://arxiv.org/abs/2108.11299) | 对神经网络进行认证以确保预测的鲁棒性是实现可靠AI的关键概念，然而这种方法存在安全风险，对手可以故意触发后备策略，使系统容易受到可用性攻击。 |
| [^211] | [Dynamics of specialization in neural modules under resource constraints.](http://arxiv.org/abs/2106.02626) | 本研究使用人工神经网络模拟实验，发现结构模块化并不一定能够确保功能专业化，在特定环境和资源限制下，才能够出现专业化现象。 |
| [^212] | [Deep Contrastive Patch-Based Subspace Learning for Camera Image Signal Processing.](http://arxiv.org/abs/2104.00253) | 本论文提出了一种称为补丁子空间学习自动编码器 (PSL-AE) 的深度神经网络模型，用于改进相机图像信号处理的鲁棒性，特别是在处理异质伪影（如图像降噪）时。 |
| [^213] | [Is Simple Uniform Sampling Effective for Center-Based Clustering with Outliers: When and Why?.](http://arxiv.org/abs/2103.00558) | 本文提出了一个简单的均匀抽样框架用于解决带有异常值的中心聚类问题，并通过引入“显著性”度量来解释其有效性。在假设给定实例是“显著”的情况下，样本大小可以独立于输入数据的大小和维度。该方法具有简单性和显著优势。 |
| [^214] | [Minimax Estimation of Distances on a Surface and Minimax Manifold Learning in the Isometric-to-Convex Setting.](http://arxiv.org/abs/2011.12478) | 研究通过表面重建来获得在光滑次流形上估计内在距离的极小最优性以及在等距问题中使用重建表面计算距离的Isomap变体的极小最优性。 |
| [^215] | [A Survey on Reinforcement Learning for Combinatorial Optimization.](http://arxiv.org/abs/2008.12248) | 该论文综述了强化学习在组合优化中的应用，特别关注了旅行推销员问题。通过比较和总结现代强化学习算法和传统方法的差异，论文展示了深度强化学习的优势，并强调了将深度学习机制与强化学习相结合的有效性。 |
| [^216] | [Unifying supervised learning and VAEs -- coverage, systematics and goodness-of-fit in normalizing-flow based neural network models for astro-particle reconstructions.](http://arxiv.org/abs/2008.05825) | 本研究将监督学习和VAEs统一于基于正态流的神经网络模型中，对天文粒子重建进行了覆盖、系统性和拟合好坏的研究，并通过KL散度目标实现了监督学习和VAEs的统一。利用条件正态化流的方法可以计算神经网络模型的拟合优度p值。 |
| [^217] | [Tensor Clustering with Planted Structures: Statistical Optimality and Computational Limits.](http://arxiv.org/abs/2005.10743) | 本文研究了带有假定结构的高阶聚类的统计和计算限制，确定了聚类存在性和聚类支持的临界值，并提出了相应的算法。在超图种植团问题和超图种植稠密子图恢复的计算困难猜想下，我们证明了在特定信噪比范围内无法使用多项式时间算法解决这些问题。 |

# 详细

[^1]: 通过联合Transformer进行全新药物设计

    De Novo Drug Design with Joint Transformers. (arXiv:2310.02066v1 [cs.LG])

    [http://arxiv.org/abs/2310.02066](http://arxiv.org/abs/2310.02066)

    提出了联合Transformer来解决全新药物设计的困难，同时生成新颖分子并预测目标属性。使用惩罚对数似然目标训练模型，在分子生成和预测精度方面取得了最先进的性能，相比于仅微调解码器的Transformer，降低了新采样分子的预测误差42%。通过联合Transformer生成的新颖分子在全新药物设计中优于其他基于SMILES的优化方法。

    

    全新药物设计需要同时生成超出训练数据的新颖分子并预测其目标属性，这对生成模型来说是一项艰巨任务。为了解决这个问题，我们提出了联合Transformer，它将Transformer的解码器、编码器和预测器结合为一个具有共享权重的联合生成模型。我们证明了使用惩罚对数似然目标来训练模型可以在分子生成方面实现最先进的性能，同时相比于仅微调解码器的Transformer，降低了新采样分子的预测误差42%。最后，我们提出了一种概率黑盒优化算法，通过使用联合Transformer生成具有改进目标属性的新颖分子，相比于训练数据，在全新药物设计中表现优于其他基于SMILES的优化方法。

    De novo drug design requires simultaneously generating novel molecules outside of training data and predicting their target properties, making it a hard task for generative models. To address this, we propose Joint Transformer that combines a Transformer decoder, a Transformer encoder, and a predictor in a joint generative model with shared weights. We show that training the model with a penalized log-likelihood objective results in state-of-the-art performance in molecule generation, while decreasing the prediction error on newly sampled molecules, as compared to a fine-tuned decoder-only Transformer, by 42%. Finally, we propose a probabilistic black-box optimization algorithm that employs Joint Transformer to generate novel molecules with improved target properties, as compared to the training data, outperforming other SMILES-based optimization methods in de novo drug design.
    
[^2]: VENOM：一种用于释放稀疏张量核心巨大潜力的矢量化N:M格式

    VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores. (arXiv:2310.02065v1 [cs.DC])

    [http://arxiv.org/abs/2310.02065](http://arxiv.org/abs/2310.02065)

    VENOM提出了一种新的V:N:M格式，可以在NVIDIA的稀疏张量核心上实现任意N:M比率。使用Spatha稀疏库，可以实现高达37倍的加速，并且展示了一种二阶修剪技术来实现高稀疏比率的V:N:M形式，并几乎没有损失。

    

    深度学习模型的成功和规模扩大要求更高的计算效率和功率。稀疏化可以实现模型更小和更高的计算效率，并且可以利用加速硬件。然而，要有效地利用稀疏向量单元的硬件支持，需要进行核心实现、修剪算法和存储格式。其中一个例子是NVIDIA的稀疏张量核心（SPTC），可以实现2倍的加速。然而，SPTC只支持2:4格式，限制了可实现的稀疏比率为50%。我们提出了V:N:M格式，可以在SPTC上执行任意N:M比率。为了有效地利用新格式，我们提出了Spatha，一个针对深度学习例程的高性能稀疏库。我们展示了Spatha比cuBLAS实现了高达37倍的加速。我们还展示了一种二阶修剪技术，可以实现高稀疏比率的V:N:M形式，并几乎没有损失。

    The increasing success and scaling of Deep Learning models demands higher computational efficiency and power. Sparsification can lead to both smaller models as well as higher compute efficiency, and accelerated hardware is becoming available. However, exploiting it efficiently requires kernel implementations, pruning algorithms, and storage formats, to utilize hardware support of specialized sparse vector units. An example of those are the NVIDIA's Sparse Tensor Cores (SPTCs), which promise a 2x speedup. However, SPTCs only support the 2:4 format, limiting achievable sparsity ratios to 50%. We present the V:N:M format, which enables the execution of arbitrary N:M ratios on SPTCs. To efficiently exploit the resulting format, we propose Spatha, a high-performance sparse-library for DL routines. We show that Spatha achieves up to 37x speedup over cuBLAS. We also demonstrate a second-order pruning technique that enables sparsification to high sparsity ratios with V:N:M and little to no los
    
[^3]: 来自EXMOS用户研究的经验教训：总结评估EXMOS平台的用户研究中的重要收获的技术报告

    Lessons Learned from EXMOS User Studies: A Technical Report Summarizing Key Takeaways from User Studies Conducted to Evaluate The EXMOS Platform. (arXiv:2310.02063v1 [cs.LG])

    [http://arxiv.org/abs/2310.02063](http://arxiv.org/abs/2310.02063)

    本技术报告总结了两项用户研究的重要发现，研究探索了在支持医疗专家优化机器学习模型的系统中，全局模型和数据中心解释对于检测和解决潜在的数据相关问题的有效性。

    

    在交互式机器学习系统领域，解释的提供在调试和增强预测模型的过程中起着重要的辅助作用。然而，不同的全局模型中心和数据中心解释在帮助领域专家检测和解决潜在数据相关问题以改进模型的目的上的有效性至今尚未得到大规模的探索。在这份技术报告中，我们总结了我们两项用户研究的主要发现。我们的研究涉及对基于数据中心和模型中心视角的全局解释在支持医疗专家通过自动和手动数据配置优化机器学习模型的系统中的影响进行全面研究。为了从经验上研究这些动态，我们进行了两项用户研究，包括涉及70位医疗专家的定量分析和涉及30位医疗专家的定性评估。

    In the realm of interactive machine-learning systems, the provision of explanations serves as a vital aid in the processes of debugging and enhancing prediction models. However, the extent to which various global model-centric and data-centric explanations can effectively assist domain experts in detecting and resolving potential data-related issues for the purpose of model improvement has remained largely unexplored. In this technical report, we summarise the key findings of our two user studies. Our research involved a comprehensive examination of the impact of global explanations rooted in both data-centric and model-centric perspectives within systems designed to support healthcare experts in optimising machine learning models through both automated and manual data configurations. To empirically investigate these dynamics, we conducted two user studies, comprising quantitative analysis involving a sample size of 70 healthcare experts and qualitative assessments involving 30 healthc
    
[^4]: 用于高效Transformer的"Inhibitor"：ReLU和加法注意力机制

    The Inhibitor: ReLU and Addition-Based Attention for Efficient Transformers. (arXiv:2310.02041v1 [cs.LG])

    [http://arxiv.org/abs/2310.02041](http://arxiv.org/abs/2310.02041)

    本论文提出了一种"Inhibitor"机制，通过使用ReLU和加法注意力机制来增强计算效率。这种机制可以在资源受限的硬件或替代算法系统上实现更高效的执行和支持更大的量化Transformer模型。实验结果表明，与传统的点积注意力相比，该机制在预测得分上表现相当，并且可以实现显著的计算节省。这一创新可能在隐私保护的应用中发挥重要作用。

    

    为了增强量化Transformer的计算效率，我们用只涉及加法和ReLU激活的替代机制来取代基于点积和Softmax的注意力机制。这样可以避免矩阵乘法中常需要的双精度扩展和昂贵的Softmax计算，但仍保留了传统的点积注意力的核心功能。这种方法可以在资源受限的硬件或同态加密等替代算法系统上实现更高效的执行和支持更大的量化Transformer模型。在四个常见的基准任务上的训练实验显示，测试集的预测得分与采用点积注意力的传统Transformer相当。我们的缩放实验还表明，在明文和加密下都可以实现显著的计算节省。特别是，我们相信本文介绍的基于ReLU和加法的注意力机制可能会实现隐私保护的A

    To enhance the computational efficiency of quantized Transformers, we replace the dot-product and Softmax-based attention with an alternative mechanism involving addition and ReLU activation only. This side-steps the expansion to double precision often required by matrix multiplication and avoids costly Softmax evaluations but maintains much of the core functionality of conventional dot-product attention. It can enable more efficient execution and support larger quantized Transformer models on resource-constrained hardware or alternative arithmetic systems like homomorphic encryption. Training experiments on four common benchmark tasks show test set prediction scores comparable to those of conventional Transformers with dot-product attention. Our scaling experiments also suggest significant computational savings, both in plaintext and under encryption. In particular, we believe that the ReLU and addition-based attention mechanism introduced in this paper may enable privacy-preserving A
    
[^5]: aSAGA: 带灰区域的自动睡眠分析

    aSAGA: Automatic Sleep Analysis with Gray Areas. (arXiv:2310.02032v1 [cs.LG])

    [http://arxiv.org/abs/2310.02032](http://arxiv.org/abs/2310.02032)

    提出了一个名为aSAGA的自动睡眠分析模型，可以在临床和家庭睡眠研究中有效执行。模型通过使用不确定性映射来识别灰色区域，需要进行手动重新评估。

    

    最先进的自动睡眠分期方法已经展示出与手动睡眠分期相当的可靠性和更高的时间效率。然而，完全自动的黑盒解决方案很难适应临床工作流程，可解释的自动方法和睡眠技术师的工作之间的交互仍不被充分研究和概念化。因此，我们提出了一个针对睡眠分析的人机交互概念，提出了一个名为aSAGA的自动睡眠分期模型，可以在临床多项式监测记录和家庭睡眠研究中有效执行。为了验证该模型，进行了广泛的测试，采用三个回顾性数据集的临床前验证方法进行，包括开放获取、临床和研究驱动的数据集。此外，我们验证了利用不确定性映射识别自动睡眠分析中的模糊区域，概念化为灰色区域，需要进行手动重新评估。结果表明，该模型在不同数据集上得到了验证。

    State-of-the-art automatic sleep staging methods have already demonstrated comparable reliability and superior time efficiency to manual sleep staging. However, fully automatic black-box solutions are difficult to adapt into clinical workflow and the interaction between explainable automatic methods and the work of sleep technologists remains underexplored and inadequately conceptualized. Thus, we propose a human-in-the-loop concept for sleep analysis, presenting an automatic sleep staging model (aSAGA), that performs effectively with both clinical polysomnographic recordings and home sleep studies. To validate the model, extensive testing was conducted, employing a preclinical validation approach with three retrospective datasets; open-access, clinical, and research-driven. Furthermore, we validate the utilization of uncertainty mapping to identify ambiguous regions, conceptualized as gray areas, in automatic sleep analysis that warrants manual re-evaluation. The results demonstrate t
    
[^6]: OceanGPT：用于海洋科学任务的大型语言模型

    OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])

    [http://arxiv.org/abs/2310.02031](http://arxiv.org/abs/2310.02031)

    OceanGPT是首个专为海洋科学任务设计的大型语言模型，通过DoInstruct框架实现自动获取海洋领域指导数据。这一模型的引入填补了海洋科学领域中对LLM的需求缺口，并为海洋科学研究提供了新的工具和方法。

    

    海洋科学是探索充满生命和生物多样性的海洋的科学，考虑到海洋覆盖了地球表面的70％以上，这一领域具有重要意义。最近，大型语言模型（LLM）的进展改变了科学的范式。尽管在其他领域取得了成功，但现有的LLM通常无法满足海洋学家等领域专家的需求，同时对LLM在海洋科学中的潜力尚未得到充分探索。这其中的根本原因可能是海洋数据的庞大而复杂的性质，以及对更高的粒度和丰富的知识的需求。为了解决这些问题，我们推出了首个海洋领域的LLM——OceanGPT，该模型擅长各种海洋科学任务。我们提出了一个新颖的框架DoInstruct，用于自动获取大量的海洋领域指导数据，它基于多智能体的协作生成指导。

    Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
    
[^7]: 在准确预测与糟糕决策之间：AI/ML的差距

    Between accurate prediction and poor decision making: the AI/ML gap. (arXiv:2310.02029v1 [cs.LG])

    [http://arxiv.org/abs/2310.02029](http://arxiv.org/abs/2310.02029)

    本文指出，AI/ML界在预测准确性方面过于关注状态概率的估计，忽视了对效用的准确可靠估计，导致了期望和实际影响之间的差距。

    

    智能代理依赖于AI/ML功能来预测可能行动的后果并优化策略。然而，研究界在解决预测准确性方面的努力非常强烈（且成功），以至于产生了这样的幻觉：学习者预测（或分类）越准确，最终决策就会越好。但这种假设只有在（人类或人工）决策者对可能行动的效用有完全了解的情况下才成立。本文认为，AI/ML界迄今为止采取了过于不平衡的方法，过分关注状态（或目标）概率的估计，而忽视了对效用的准确可靠估计。特别是，很少有证据证明错误的效用评估对决策策略的预期效用产生的影响。这种情况导致了期望和实际影响之间的实质性差距。

    Intelligent agents rely on AI/ML functionalities to predict the consequence of possible actions and optimise the policy. However, the effort of the research community in addressing prediction accuracy has been so intense (and successful) that it created the illusion that the more accurate the learner prediction (or classification) the better would have been the final decision. Now, such an assumption is valid only if the (human or artificial) decision maker has complete knowledge of the utility of the possible actions. This paper argues that AI/ML community has taken so far a too unbalanced approach by devoting excessive attention to the estimation of the state (or target) probability to the detriment of accurate and reliable estimations of the utility. In particular, few evidence exists about the impact of a wrong utility assessment on the resulting expected utility of the decision strategy. This situation is creating a substantial gap between the expectations and the effective impact
    
[^8]: DeepHGCN：朝着更深的双曲图卷积网络

    DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks. (arXiv:2310.02027v1 [cs.LG])

    [http://arxiv.org/abs/2310.02027](http://arxiv.org/abs/2310.02027)

    DeepHGCN是一个具有深层架构的双曲图卷积网络，通过引入新的双曲特征转换层和正则化技术，实现了计算效率的极大改进和过度平滑问题的显著减轻。

    

    双曲图卷积网络（HGCN）在提取分层图信息方面展示了巨大潜力。然而，由于昂贵的双曲操作和随着深度增加的过度平滑问题，现有的HGCN受限于浅层架构。尽管在GCNs中已经应用了一些方法来减轻过度平滑问题，但是开发双曲治疗方法面临着不同的挑战，因为操作必须经过精心设计以适应双曲性质。解决以上挑战，本文提出了DeepHGCN，这是第一个具有显著提高计算效率和大大减轻过度平滑效果的深层多层HGCN架构。DeepHGCN具有两个深层HGCN的关键因素：（1）一种新颖的双曲特征转换层，能够实现快速而准确的线性映射；（2）通过有效的双曲残差连接和权重和特征的正则化技术来促进。

    Hyperbolic graph convolutional networks (HGCN) have demonstrated significant potential in extracting information from hierarchical graphs. However, existing HGCNs are limited to shallow architectures, due to the expensive hyperbolic operations and the over-smoothing issue as depth increases. Although in GCNs, treatments have been applied to alleviate over-smoothing, developing a hyperbolic therapy presents distinct challenges since operations should be carefully designed to fit the hyperbolic nature. Addressing the above challenges, in this work, we propose DeepHGCN, the first deep multi-layer HGCN architecture with dramatically improved computational efficiency and substantially alleviated over-smoothing effect. DeepHGCN presents two key enablers of deep HGCNs: (1) a novel hyperbolic feature transformation layer that enables fast and accurate linear maps; and (2) Techniques such as hyperbolic residual connections and regularization for both weights and features facilitated by an effic
    
[^9]: 对于线性多臂老虎机，纳什遗憾的保证

    Nash Regret Guarantees for Linear Bandits. (arXiv:2310.02023v1 [cs.LG])

    [http://arxiv.org/abs/2310.02023](http://arxiv.org/abs/2310.02023)

    本研究在随机线性多臂老虎机框架中提出了一个更强化的遗憾上界，称为纳什遗憾，它通过将算法的表现量化为其在各个回合中所生成的集体福利来提供一个基于公平性的保证。

    

    我们在随机线性多臂老虎机框架中获得了对遗憾的一个更强化的上界，称为纳什遗憾，其定义为线性多臂老虎机算法积累的预期奖励的几何平均与（先验未知的）最优解之间的差异。由于几何平均对应于众所周知的纳什社会福利（NSW）函数，因此该公式将算法的表现量化为其在各个回合中所生成的集体福利。已知NSW满足公平公理，因此对纳什遗憾的上界提供了一个有原则的公平保证。我们考虑了时间跨度为$T$回合、臂集合为${X}$、维度为$d$的随机线性多臂老虎机问题。此外，我们还关注与每个臂相关的随机奖励是非负的、$\nu$-次泊松随机变量的设置。对于这个设置，我们开发了一个算法

    We obtain essentially tight upper bounds for a strengthened notion of regret in the stochastic linear bandits framework. The strengthening -- referred to as Nash regret -- is defined as the difference between the (a priori unknown) optimum and the geometric mean of expected rewards accumulated by the linear bandit algorithm. Since the geometric mean corresponds to the well-studied Nash social welfare (NSW) function, this formulation quantifies the performance of a bandit algorithm as the collective welfare it generates across rounds. NSW is known to satisfy fairness axioms and, hence, an upper bound on Nash regret provides a principled fairness guarantee.  We consider the stochastic linear bandits problem over a horizon of $T$ rounds and with set of arms ${X}$ in ambient dimension $d$. Furthermore, we focus on settings in which the stochastic reward -- associated with each arm in ${X}$ -- is a non-negative, $\nu$-sub-Poisson random variable. For this setting, we develop an algorithm th
    
[^10]: 使用异质工人对一组对象进行排序：一个相当简单的问题

    Ranking a Set of Objects using Heterogeneous Workers: QUITE an Easy Problem. (arXiv:2310.02016v1 [cs.LG])

    [http://arxiv.org/abs/2310.02016](http://arxiv.org/abs/2310.02016)

    使用QUITE算法，我们可以通过估计工人的可靠性和对象的质量，从一组由异构工人提供的噪声成对比较开始，对一组对象进行排序。与其他算法相比，QUITE表现出良好的性能，并且可以轻松地使其自适应。

    

    我们关注的是从一组不平等工人提供的噪声成对比较开始，对$N$个对象进行排序的问题，每个工人都具有特定的可靠性程度，这反映了她对对象对进行排序的能力。具体来说，我们假设对象具有内在质量，一个对象被优选另一个的概率取决于两个竞争者的质量差异和工人的可靠性。我们提出了QUITE，一个非自适应的排序算法，它同时估计工人的可靠性和对象的质量。我们将QUITE在不同场景下与之前提出的算法进行性能比较。最后，我们展示了QUITE如何自然地变得自适应。

    We focus on the problem of ranking $N$ objects starting from a set of noisy pairwise comparisons provided by a crowd of unequal workers, each worker being characterized by a specific degree of reliability, which reflects her ability to rank pairs of objects. More specifically, we assume that objects are endowed with intrinsic qualities and that the probability with which an object is preferred to another depends both on the difference between the qualities of the two competitors and on the reliability of the worker. We propose QUITE, a non-adaptive ranking algorithm that jointly estimates workers' reliabilities and qualities of objects. Performance of QUITE is compared in different scenarios against previously proposed algorithms. Finally, we show how QUITE can be naturally made adaptive.
    
[^11]: 无需数据依赖的参数PDE的光谱算子学习

    Spectral operator learning for parametric PDEs without data reliance. (arXiv:2310.02013v1 [cs.LG])

    [http://arxiv.org/abs/2310.02013](http://arxiv.org/abs/2310.02013)

    这种方法通过光谱方法和深度神经网络相结合，实现了无需数据依赖的参数化PDE求解，可以准确学习和预测复杂的PDE解。

    

    本文介绍了一种名为SCLON的基于算子学习的新方法，用于解决参数化偏微分方程（PDE），而无需利用数据。我们方法的核心是采用光谱方法，利用正交函数（如傅里叶级数和Legendre多项式）进行展开，从而能够在更少的网格点上实现准确的PDE解。通过将光谱方法的优点与深度神经网络的能力相结合，SCLON提供了一种革命性的策略。我们的方法不仅消除了通常需要大量数值计算的成对输入-输出训练数据的需求，还有效地学习和预测复杂参数化PDE的解，从奇异摄动对流扩散方程到Navier-Stokes方程。

    In this paper, we introduce the Spectral Coefficient Learning via Operator Network (SCLON), a novel operator learning-based approach for solving parametric partial differential equations (PDEs) without the need for data harnessing. The cornerstone of our method is the spectral methodology that employs expansions using orthogonal functions, such as Fourier series and Legendre polynomials, enabling accurate PDE solutions with fewer grid points. By merging the merits of spectral methods - encompassing high accuracy, efficiency, generalization, and the exact fulfillment of boundary conditions with the prowess of deep neural networks, SCLON offers a transformative strategy. Our approach not only eliminates the need for paired input-output training data, which typically requires extensive numerical computations, but also effectively learns and predicts solutions of complex parametric PDEs, ranging from singularly perturbed convection-diffusion equations to the Navier-Stokes equations. The 
    
[^12]: 超越深度限制的训练：批归一化避免梯度爆炸

    Towards Training Without Depth Limits: Batch Normalization Without Gradient Explosion. (arXiv:2310.02012v1 [cs.LG])

    [http://arxiv.org/abs/2310.02012](http://arxiv.org/abs/2310.02012)

    本研究提出了一种超越深度限制的训练方法，通过批归一化避免梯度爆炸。研究给出了一种具体构造的多层感知器，在任何深度都能保持优化的信号传播特性，并具有有界梯度。

    

    归一化层是深度神经网络的关键组成部分之一。一些理论研究表明，批归一化改善了信号传播，通过避免表示在层之间变得共线。然而，批归一化的均场理论也得出结论，这种好处是以深度梯度爆炸的代价为。在本研究中，我们受到批归一化的这两个方面的启发，提出了以下问题：“批归一化网络能否保持最优的信号传播特性，但避免梯度爆炸？”我们肯定地回答了这个问题，通过给出一种具体的构造方法，证明了具有线性激活和批归一化的多层感知器（MLP）在任何深度都具有有界梯度。基于Weingarten微积分，我们为该构造的MLP开发了一个严格的非渐近理论，给出了前向信号传播的精确特征化。

    Normalization layers are one of the key building blocks for deep neural networks. Several theoretical studies have shown that batch normalization improves the signal propagation, by avoiding the representations from becoming collinear across the layers. However, results on mean-field theory of batch normalization also conclude that this benefit comes at the expense of exploding gradients in depth. Motivated by these two aspects of batch normalization, in this study we pose the following question: "Can a batch-normalized network keep the optimal signal propagation properties, but avoid exploding gradients?" We answer this question in the affirmative by giving a particular construction of an Multi-Layer Perceptron (MLP) with linear activations and batch-normalization that provably has bounded gradients at any depth. Based on Weingarten calculus, we develop a rigorous and non-asymptotic theory for this constructed MLP that gives a precise characterization of forward signal propagation, wh
    
[^13]: 解码人类行为：分析可穿戴加速度计和陀螺仪数据进行活动识别

    Decoding Human Activities: Analyzing Wearable Accelerometer and Gyroscope Data for Activity Recognition. (arXiv:2310.02011v1 [cs.CV])

    [http://arxiv.org/abs/2310.02011](http://arxiv.org/abs/2310.02011)

    本文提出了一种用于活动识别的分层多结构方法，利用残差网络和残差MobileNet对静态和动态活动进行分类，然后通过加权合奏方法进行集成。

    

    一个人的运动或相对定位有效地产生了可以被计算机读取的原始电信号，通过应用各种操作技术来对不同的人类活动进行分类。本文提出了一种基于残差网络与残差MobileNet进行合奏的分层多结构方法，称为FusionActNet。所提出的方法涉及使用精心设计的残差块分别对静态和动态活动进行分类，因为它们具有明显而独特的特征。这些网络独立训练，得到两个专业的高精度模型。通过利用架构调整的算法优势，这些模型在特定超类中优秀地识别活动。然后，这两个残差网络通过加权合奏的残差MobileNet进行传递。随后，这个合奏能够有效区分一些特定的子类。

    A person's movement or relative positioning effectively generates raw electrical signals that can be read by computing machines to apply various manipulative techniques for the classification of different human activities. In this paper, a stratified multi-structural approach based on a Residual network ensembled with Residual MobileNet is proposed, termed as FusionActNet. The proposed method involves using carefully designed Residual blocks for classifying the static and dynamic activities separately because they have clear and distinct characteristics that set them apart. These networks are trained independently, resulting in two specialized and highly accurate models. These models excel at recognizing activities within a specific superclass by taking advantage of the unique algorithmic benefits of architectural adjustments. Afterward, these two ResNets are passed through a weighted ensemble-based Residual MobileNet. Subsequently, this ensemble proficiently discriminates between a sp
    
[^14]: fmeffects: 一个用于前向边际效应的R软件包

    fmeffects: An R Package for Forward Marginal Effects. (arXiv:2310.02008v1 [cs.LG])

    [http://arxiv.org/abs/2310.02008](http://arxiv.org/abs/2310.02008)

    fmeffects是第一个实现前向边际效应（FMEs）的R软件包。

    

    前向边际效应（FMEs）作为一种通用有效的模型不可知解释方法最近被引入。它们以“如果我们将$x$改变$h$，那么预测结果$\widehat{y}$会发生什么变化？”的形式提供易于理解和可操作的模型解释。本文介绍了fmeffects软件包，这是FMEs的第一个软件实现。讨论了相关的理论背景、软件包功能和处理方式，以及软件设计和未来扩展的选项。

    Forward marginal effects (FMEs) have recently been introduced as a versatile and effective model-agnostic interpretation method. They provide comprehensible and actionable model explanations in the form of: If we change $x$ by an amount $h$, what is the change in predicted outcome $\widehat{y}$? We present the R package fmeffects, the first software implementation of FMEs. The relevant theoretical background, package functionality and handling, as well as the software design and options for future extensions are discussed in this paper.
    
[^15]: L2MAC：大规模语言模型自动计算机用于无限代码生成

    L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v1 [cs.SE])

    [http://arxiv.org/abs/2310.02003](http://arxiv.org/abs/2310.02003)

    L2MAC是一种基于LLM的存储程序自动计算机，可以用于生成长且逻辑一致的代码。

    

    基于Transformer的大型语言模型（LLM）受到底层Transformer架构固定上下文窗口的限制，阻碍了它们生成长且逻辑一致的代码的能力。增强记忆的LLM是一个有前途的解决方案，但目前的方法无法处理长时间的代码生成任务，因为它们要么只关注于读取内存并将其演变为新内存的连接，要么使用非常专门的内存，无法适应其他领域。本文介绍了L2MAC，这是一种基于LLM的长且一致代码生成的实用存储程序自动计算机。它的内存有两个组成部分：指令注册表，其中填充了一个解决用户给定任务的提示程序，以及文件存储，其中包含最终和中间输出。每个指令由单独的LLM实例执行，其上下文由控制单元管理，能够精确读取和写入内存，以确保有效的整合。

    Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective inte
    
[^16]: MUSCLE: 多任务自监督连续学习用于多个身体部位的X-ray图像的深度模型预训练

    MUSCLE: Multi-task Self-supervised Continual Learning to Pre-train Deep Models for X-ray Images of Multiple Body Parts. (arXiv:2310.02000v1 [cs.CV])

    [http://arxiv.org/abs/2310.02000](http://arxiv.org/abs/2310.02000)

    MUSCLE提出了一种多任务自监督连续学习（MUSCLE）的预训练流程，用于多个身体部位的X-ray图像的深度模型。它通过汇集来自多个身体部位的X-ray图像进行表示学习，并采用连续学习过程来预训练骨干网络，以应对多个X-ray分析任务中的数据异质性、过拟合和灾难性遗忘问题。

    

    尽管自监督学习（SSL）算法已广泛用于深度模型的预训练，但很少有研究着眼于通过自监督预训练模型提高X-ray图像分析的表示学习。在本研究中，我们研究了一种新颖的自监督预训练流程，名为多任务自监督连续学习（MUSCLE），用于多种医学影像任务，如分类和分割，使用来自多个身体部位（包括头部、肺部和骨骼）的X-ray图像。具体而言，MUSCLE通过汇集来自多个身体部位的X-ray图像进行基于MoCo的表示学习，并采用精心设计的连续学习过程，以共同预训练用于各种X-ray分析任务的骨干网络。MUSCLE中使用了图像预处理、学习计划和正则化策略来解决多任务/数据集学习中的数据异质性、过拟合和灾难性遗忘问题。我们评估了MUSCLE在不同身体部位的X-ray图像上的性能，并与其他方法进行了比较。

    While self-supervised learning (SSL) algorithms have been widely used to pre-train deep models, few efforts [11] have been done to improve representation learning of X-ray image analysis with SSL pre-trained models. In this work, we study a novel self-supervised pre-training pipeline, namely Multi-task Self-super-vised Continual Learning (MUSCLE), for multiple medical imaging tasks, such as classification and segmentation, using X-ray images collected from multiple body parts, including heads, lungs, and bones. Specifically, MUSCLE aggregates X-rays collected from multiple body parts for MoCo-based representation learning, and adopts a well-designed continual learning (CL) procedure to further pre-train the backbone subject various X-ray analysis tasks jointly. Certain strategies for image pre-processing, learning schedules, and regularization have been used to solve data heterogeneity, overfitting, and catastrophic forgetting problems for multi-task/dataset learning in MUSCLE.We evalu
    
[^17]: 填空题：探索并增强LLM在数学应用题中的逆向推理能力

    Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems. (arXiv:2310.01991v1 [cs.CL])

    [http://arxiv.org/abs/2310.01991](http://arxiv.org/abs/2310.01991)

    本文探讨了LLM在数学应用题中的逆向推理能力，发现在逆向推理任务上，LLM模型的准确性显著下降。通过改进技术，如Rephrase和PAL-Tools，我们提高了模型的性能。

    

    虽然近期的文献中广泛探讨了正向推理（即给定问题找答案），但逆向推理相对较少被研究。我们对LLM在数学应用题中的逆向推理能力进行了探讨：给定一个数学问题和其答案，在问题中有些细节被省略了，LLM能否有效地还原出缺失的信息？本文正式定义了数学应用题中的逆向推理任务，并修改了三个数据集来评估这一任务：GSM8k、SVAMP和MultiArith。我们的研究结果显示，与正向推理相比，四个最先进的LLM模型（GPT4、GPT3.5、PaLM-2和LLaMa-2）在逆向推理上的准确性显著下降。利用该任务的特定格式，我们提出了三种改进性能的新技术：Rephrase将给定的问题重述为一个正向推理问题，PAL-Tools结合了程序辅助的LLM思想，生成一组方程式可以解决缺失的信息。

    While forward reasoning (i.e. find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information?  In this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that ca
    
[^18]: 两层ReLU卷积神经网络在XOR数据中的良性过拟合

    Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data. (arXiv:2310.01975v1 [cs.LG])

    [http://arxiv.org/abs/2310.01975](http://arxiv.org/abs/2310.01975)

    本文研究了两层ReLU卷积神经网络在XOR数据中的良性过拟合现象。实验证明，在一定条件下，通过梯度下降训练的过度参数化的ReLU卷积神经网络可以达到近乎贝叶斯最优准确率。

    

    现代深度学习模型通常是高度过度参数化的，以便能够过拟合训练数据。令人惊讶的是，这种过拟合的神经网络通常仍然能够达到很高的预测准确率。为了研究这种“良性过拟合”现象，最近一系列的工作从理论上研究了线性模型和两层神经网络的学习。然而，这些分析大多仍限于贝叶斯最优分类器为线性的非常简单的学习问题。在本研究中，我们研究了一类XOR类型的分类任务，并考虑了标签翻转的噪声。我们证明，在样本复杂度和信噪比满足一定条件的情况下，通过梯度下降训练的过度参数化的ReLU卷积神经网络可以达到近乎贝叶斯最优准确率。此外，我们还建立了一个匹配的下界结果，表明当满足前述条件不成立时，所获得卷积神经网络的预测准确率与贝叶斯最优相差一个绝对常数。

    Modern deep learning models are usually highly over-parameterized so that they can overfit the training data. Surprisingly, such overfitting neural networks can usually still achieve high prediction accuracy. To study this "benign overfitting" phenomenon, a line of recent works has theoretically studied the learning of linear models and two-layer neural networks. However, most of these analyses are still limited to the very simple learning problems where the Bayes-optimal classifier is linear. In this work, we investigate a class of XOR-type classification tasks with label-flipping noises. We show that, under a certain condition on the sample complexity and signal-to-noise ratio, an over-parameterized ReLU CNN trained by gradient descent can achieve near Bayes-optimal accuracy. Moreover, we also establish a matching lower bound result showing that when the previous condition is not satisfied, the prediction accuracy of the obtained CNN is an absolute constant away from the Bayes-optima
    
[^19]: 联合分布式瓦塞斯坦距离

    Federated Wasserstein Distance. (arXiv:2310.01973v1 [cs.LG])

    [http://arxiv.org/abs/2310.01973](http://arxiv.org/abs/2310.01973)

    本文介绍了一种在联合方式下计算两个分布的瓦塞斯坦距离的方法，利用瓦塞斯坦距离的几何性质和测地线性质，通过操作和交换测地线空间中的分布来逼近瓦塞斯坦距离。

    

    我们引入了一种在联合方式下计算两个分布的瓦塞斯坦距离的方法。具体而言，我们展示了如何在不访问样本的情况下，通过中央实体/服务器协调计算，估计存储在不同设备/客户端上的两个样本的瓦塞斯坦距离。为了实现这一目标，我们利用了瓦塞斯坦距离的几何性质，尤其是三角不等式，以及相关的测地线性质：我们的算法FedWad（联合瓦塞斯坦距离）通过操作和交换测地线空间中的分布来迭代逼近瓦塞斯坦距离，而不是输入样本。除了建立FedWad的收敛性质，我们还提供了在联合核心集和联合最优传输数据集距离上的实证结果，分别用于构建新的联合模型和提升性能。

    We introduce a principled way of computing the Wasserstein distance between two distributions in a federated manner. Namely, we show how to estimate the Wasserstein distance between two samples stored and kept on different devices/clients whilst a central entity/server orchestrates the computations (again, without having access to the samples). To achieve this feat, we take advantage of the geometric properties of the Wasserstein distance -- in particular, the triangle inequality -- and that of the associated {\em geodesics}: our algorithm, FedWad (for Federated Wasserstein Distance), iteratively approximates the Wasserstein distance by manipulating and exchanging distributions from the space of geodesics in lieu of the input samples. In addition to establishing the convergence properties of FedWad, we provide empirical results on federated coresets and federate optimal transport dataset distance, that we respectively exploit for building a novel federated model and for boosting perfor
    
[^20]: 流行学习：通过随机通信增强分散式学习

    Epidemic Learning: Boosting Decentralized Learning with Randomized Communication. (arXiv:2310.01972v1 [cs.LG])

    [http://arxiv.org/abs/2310.01972](http://arxiv.org/abs/2310.01972)

    流行学习是一种简单而强大的分散式学习算法，通过利用变化的通信拓扑结构实现了比传统方法更快的模型收敛速度，具有更好的收敛性能。

    

    我们提出了一种名为流行学习（EL）的简单而强大的分散式学习（DL）算法，利用不断变化的通信拓扑结构，以比传统的DL方法更快的模型收敛速度。在EL的每一轮中，每个节点将其模型更新发送给一个随机样本的$s$个其他节点（在$n$个节点的系统中）。我们对EL进行了广泛的理论分析，证明其变化的拓扑结构导致了比现有的（静态和动态）拓扑结构更好的收敛性能。对于平滑的非凸损失函数，EL的暂态迭代次数，即达到渐近线性加速所需的轮数，是$\mathcal{O}(\frac{n^3}{s^2})$，超过了已知的最佳界限$\mathcal{O}({n^3})$，增加了$s^2$倍，表明了随机通信在DL中的好处。我们在一个96个节点的网络中对EL进行了实证评估，并将其性能与现有的DL方法进行了比较。我们的结果显示，EL达到了更快的模型收敛速度和更好的收敛性能。

    We present Epidemic Learning (EL), a simple yet powerful decentralized learning (DL) algorithm that leverages changing communication topologies to achieve faster model convergence compared to conventional DL approaches. At each round of EL, each node sends its model updates to a random sample of $s$ other nodes (in a system of $n$ nodes). We provide an extensive theoretical analysis of EL, demonstrating that its changing topology culminates in superior convergence properties compared to the state-of-the-art (static and dynamic) topologies. Considering smooth non-convex loss functions, the number of transient iterations for EL, i.e., the rounds required to achieve asymptotic linear speedup, is in $\mathcal{O}(\frac{n^3}{s^2})$ which outperforms the best-known bound $\mathcal{O}({n^3})$ by a factor of $ s^2 $, indicating the benefit of randomized communication for DL. We empirically evaluate EL in a 96-node network and compare its performance with state-of-the-art DL approaches. Our resu
    
[^21]: 超越标签神谕：什么是模型窃取的含义？

    Beyond Labeling Oracles: What does it mean to steal ML models?. (arXiv:2310.01959v1 [cs.LG])

    [http://arxiv.org/abs/2310.01959](http://arxiv.org/abs/2310.01959)

    本文研究了模型提取攻击，发现攻击者往往不能节约数据和标注成本，因为攻击隐含地依赖于从受害模型的数据分布中采样的能力。攻击者的先前知识对攻击成功至关重要。

    

    模型提取攻击旨在通过只有查询访问权限来窃取训练好的模型，通常通过ML-as-a-Service提供的API来实现。由于数据难以获取，训练ML模型的成本很高，因此模型提取的主要动机是在比从头开始训练更少成本的情况下获取模型。关于模型提取的文献普遍声称或假设攻击者能够节约数据获取和标注成本。然而我们发现攻击者往往不能实现这一点，因为当前的攻击隐含地依赖于攻击者能够从受害模型的数据分布中采样。我们对影响模型提取成功的因素进行了全面评估，发现攻击者对受害者的先前知识，即对分布数据的访问，比攻击策略（决定向受害者模型API发出哪些查询）等其他因素更为重要。因此，一个希望开发同等水平的攻击者更重要的是获取对分布数据的先前知识。

    Model extraction attacks are designed to steal trained models with only query access, as is often provided through APIs that ML-as-a-Service providers offer. ML models are expensive to train, in part because data is hard to obtain, and a primary incentive for model extraction is to acquire a model while incurring less cost than training from scratch. Literature on model extraction commonly claims or presumes that the attacker is able to save on both data acquisition and labeling costs. We show that the attacker often does not. This is because current attacks implicitly rely on the adversary being able to sample from the victim model's data distribution. We thoroughly evaluate factors influencing the success of model extraction. We discover that prior knowledge of the attacker, i.e. access to in-distribution data, dominates other factors like the attack policy the adversary follows to choose which queries to make to the victim model API. Thus, an adversary looking to develop an equally 
    
[^22]: 贝叶斯神经网络的概率性到达-避免问题

    Probabilistic Reach-Avoid for Bayesian Neural Networks. (arXiv:2310.01951v1 [cs.LG])

    [http://arxiv.org/abs/2310.01951](http://arxiv.org/abs/2310.01951)

    本文讨论了贝叶斯神经网络在计算到达-避免概率和合成最优控制策略中的应用，通过利用区间传播和向后递归技术，计算出了概率的下界作为安全性保证。

    

    基于模型的强化学习旨在同时学习未知随机环境的动力学并综合出适用于其中的最佳策略。在这样的环境中，确保通过策略作出的序列决策的安全性和鲁棒性是安全关键场景下的一个关键挑战。本文研究了两个互补的问题：第一，对由贝叶斯神经网络（BNN）描述的动力学模型进行迭代预测的到达-避免概率的计算；第二，合成最优控制策略以满足给定的到达-避免规范（达到“目标”状态，同时避免一组“不安全”状态）和学习的BNN模型。我们的解决方案利用区间传播和向后递归技术来计算策略动作序列满足到达-避免规范的概率的下界。这样计算出的下界提供了安全性保证。

    Model-based reinforcement learning seeks to simultaneously learn the dynamics of an unknown stochastic environment and synthesise an optimal policy for acting in it. Ensuring the safety and robustness of sequential decisions made through a policy in such an environment is a key challenge for policies intended for safety-critical scenarios. In this work, we investigate two complementary problems: first, computing reach-avoid probabilities for iterative predictions made with dynamical models, with dynamics described by Bayesian neural network (BNN); second, synthesising control policies that are optimal with respect to a given reach-avoid specification (reaching a "target" state, while avoiding a set of "unsafe" states) and a learned BNN model. Our solution leverages interval propagation and backward recursion techniques to compute lower bounds for the probability that a policy's sequence of actions leads to satisfying the reach-avoid specification. Such computed lower bounds provide saf
    
[^23]: OOD感知的监督对比学习

    OOD Aware Supervised Contrastive Learning. (arXiv:2310.01942v1 [cs.LG])

    [http://arxiv.org/abs/2310.01942](http://arxiv.org/abs/2310.01942)

    本文提出了一种利用Supervised Contrastive（SupCon）学习的强大表示的方法来学习对OOD数据具有鲁棒性的分类器，并通过扩展SupCon损失的对比项来实现。当辅助OOD数据不可用时，我们提出了特征混合技术来生成伪OOD特征。

    

    针对机器学习模型在训练数据分布之外的样本进行识别的安全部署中，超出分布（OOD）检测是一个关键问题。大多数OOD研究都集中在用交叉熵（CE）训练的分类模型上，并试图解决其中固有的问题。在这项工作中，我们利用用Supervised Contrastive（SupCon）训练学到的强大表示，并提出了一种综合方法来学习对OOD数据具有鲁棒性的分类器。我们通过两个额外的对比项扩展了SupCon损失。第一个项将辅助OOD表示与ID表示分离，而不对辅助数据之间的相似性施加任何约束。第二个项将OOD特征远离现有的类别原型，同时将ID表示推向其相对应的类别原型。当辅助OOD数据不可用时，我们提出了特征混合技术来高效生成伪OOD特征。

    Out-of-Distribution (OOD) detection is a crucial problem for the safe deployment of machine learning models identifying samples that fall outside of the training distribution, i.e. in-distribution data (ID). Most OOD works focus on the classification models trained with Cross Entropy (CE) and attempt to fix its inherent issues. In this work we leverage powerful representation learned with Supervised Contrastive (SupCon) training and propose a holistic approach to learn a classifier robust to OOD data. We extend SupCon loss with two additional contrast terms. The first term pushes auxiliary OOD representations away from ID representations without imposing any constraints on similarities among auxiliary data. The second term pushes OOD features far from the existing class prototypes, while pushing ID representations closer to their corresponding class prototype. When auxiliary OOD data is not available, we propose feature mixing techniques to efficiently generate pseudo-OOD features. Our
    
[^24]: Causal Inference with Conditional Front-Door Adjustment and Identifiable Variational Autoencoder

    Causal Inference with Conditional Front-Door Adjustment and Identifiable Variational Autoencoder. (arXiv:2310.01937v1 [cs.LG])

    [http://arxiv.org/abs/2310.01937](http://arxiv.org/abs/2310.01937)

    本文提出了条件前门调整和可辨识性变分自编码器来解决因果推断中的问题，通过放松限制和利用深度生成模型，从观测数据中学习未观测到的混淆变量的表示，从而保证因果效应的可辨识性。

    

    因果推断中的一个重要且具有挑战性的问题是如何从观测数据中估计因果效应。当存在未观测到的混淆变量时，这个问题变得更加困难。前门调整是处理未观测到的混淆变量的一种实用方法。然而，标准前门调整的限制在实践中很难满足。本文通过提出条件前门调整的概念并发展保证条件前门调整的因果效应可辨识性的定理，放松了一些限制。此外，由于在实践中往往无法给定一个条件前门变量，因此希望能够从数据中学习该变量。通过利用深度生成模型的能力，我们提出了CFDiVAE来直接从数据中学习条件前门调整变量的表示，并正式证明了模型的可辨识性。在大量实验中进行了验证。

    An essential and challenging problem in causal inference is causal effect estimation from observational data. The problem becomes more difficult with the presence of unobserved confounding variables. The front-door adjustment is a practical approach for dealing with unobserved confounding variables. However, the restriction for the standard front-door adjustment is difficult to satisfy in practice. In this paper, we relax some of the restrictions by proposing the concept of conditional front-door (CFD) adjustment and develop the theorem that guarantees the causal effect identifiability of CFD adjustment. Furthermore, as it is often impossible for a CFD variable to be given in practice, it is desirable to learn it from data. By leveraging the ability of deep generative models, we propose CFDiVAE to learn the representation of the CFD adjustment variable directly from data with the identifiable Variational AutoEncoder and formally prove the model identifiability. Extensive experiments on
    
[^25]: 穿越文化鸿沟：探索和解锁文本到图像模型的文化视角

    Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v1 [cs.CL])

    [http://arxiv.org/abs/2310.01929](http://arxiv.org/abs/2310.01929)

    本研究旨在探索和解锁文本到图像模型的文化视角，通过对TTI模型中嵌入的文化感知进行评估，揭示了这些模型的文化意识、文化区别和文化适应性。

    

    文本到图像（TTI）模型，例如DALL-E和StableDiffusion，在通过文本提示生成图像的零射模式方面具有卓越的能力，近来备受关注。作为文化的媒介，语言在这些模型的多语言能力中起着关键作用，从而塑造了它们的文化机制。在本研究中，我们通过描述文化维度，文化领域和文化概念的三个层次来探索TTI模型中嵌入的文化感知。我们提出了一套全面的评估技术，包括使用CLIP空间进行内在评估，使用视觉问答（VQA）模型进行外在评估以及人类评估，以识别TTI文化感知。为了促进我们的研究，我们引入了CulText2I数据集，该数据集来自四个不同的TTI模型，涵盖了十种语言。我们的实验揭示了这些模型的文化意识、文化区别和

    Text-To-Image (TTI) models, exemplified by DALL-E and StableDiffusion, have recently gained prominence for their remarkable zero-shot capabilities in generating images guided by textual prompts. Language, as a conduit of culture, plays a pivotal role in these models' multilingual capabilities, which in turn shape their cultural agency. In this study, we explore the cultural perception embedded in TTI models by characterizing culture across three hierarchical tiers: cultural dimensions, cultural domains, and cultural concepts. We propose a comprehensive suite of evaluation techniques, including intrinsic evaluations using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA) model, and human assessments, to discern TTI cultural perceptions. To facilitate our research, we introduce the CulText2I dataset, derived from four diverse TTI models and spanning ten languages. Our experiments reveal insights into these models' cultural awareness, cultural distinctions, and the
    
[^26]: RoFormer对于全幻灯片图像分类中的位置感知多实例学习的应用

    RoFormer for Position Aware Multiple Instance Learning in Whole Slide Image Classification. (arXiv:2310.01924v1 [cs.CV])

    [http://arxiv.org/abs/2310.01924](http://arxiv.org/abs/2310.01924)

    RoFormer模块可以为全幻灯片图像分类任务中的位置感知多实例学习提供准确自注意力和相对位置编码，克服了当前方法中对补丁独立性和排序不变性的假设，并解决了计算工作量过大的问题。

    

    全幻灯片图像分类是计算病理学中的关键任务。然而，这种图像的几个十亿像素规模对于现有的深度学习方法仍然是一个重要挑战。当前的方法依赖于具有冻结特征提取器的多实例学习模型。鉴于每个图像中实例的数量较高，多实例学习方法通常假设补丁的独立性和排序不变性，忽略了组织结构和补丁之间的相关性。最近的研究开始研究实例之间的相关性，但是这种大量令牌的计算工作量仍然是一个限制因素。尤其是补丁的相对位置问题仍未得到解决。我们提出使用一种直接的编码模块，即RoFormer层，依靠内存高效的准确自注意力和相对位置编码。该模块可以在大型和任意形状的幻灯片补丁上进行完全自注意力计算和相对位置编码。

    Whole slide image (WSI) classification is a critical task in computational pathology. However, the gigapixel-size of such images remains a major challenge for the current state of deep-learning. Current methods rely on multiple-instance learning (MIL) models with frozen feature extractors. Given the the high number of instances in each image, MIL methods have long assumed independence and permutation-invariance of patches, disregarding the tissue structure and correlation between patches. Recent works started studying this correlation between instances but the computational workload of such a high number of tokens remained a limiting factor. In particular, relative position of patches remains unaddressed. We propose to apply a straightforward encoding module, namely a RoFormer layer , relying on memory-efficient exact self-attention and relative positional encoding. This module can perform full self-attention with relative position encoding on patches of large and arbitrary shaped WSIs
    
[^27]: 使用深度多模态融合的UWF-CFP和OCTA图像改进了自动糖尿病视网膜病变严重程度分类

    Improved Automatic Diabetic Retinopathy Severity Classification Using Deep Multimodal Fusion of UWF-CFP and OCTA Images. (arXiv:2310.01912v1 [eess.IV])

    [http://arxiv.org/abs/2310.01912](http://arxiv.org/abs/2310.01912)

    本研究提出了一种利用UWF-CFP和OCTA图像的深度多模态融合方法来改进自动糖尿病视网膜病变严重程度分类，将2D UWF-CFP图像和3D高分辨率6x6 mm^3 OCTA图像进行整合，并采用ResNet50和3D-ResNet50模型的融合以及Squeeze-and-Excitation（SE）块来增强相关特征，同时采用多模态Manifold Mixup来增加模型的泛化能力。

    

    糖尿病视网膜病变（DR）是糖尿病的一种常见且严重的并发症，影响全球数百万人，因此需要准确及时的诊断。最近的成像技术进展，如超广角彩色眼底照相（UWF-CFP）和光学相干断层扫描血管造影（OCTA），为早期检测DR提供了机会，但也面临着数据不一致的巨大挑战。本研究引入了一种新颖的多模态方法，利用这些成像模式显著提高DR分类的准确性。我们的方法通过ResNet50和3D-ResNet50模型的融合，使用Squeeze-and-Excitation（SE）块来增强相关特征，将2D UWF-CFP图像和3D高分辨率6x6 mm^3 OCTA（结构和流）图像进行整合。另外，为了增加模型的泛化能力，我们还将多模态Manifold Mixup应用于连接的多模态特征上。

    Diabetic Retinopathy (DR), a prevalent and severe complication of diabetes, affects millions of individuals globally, underscoring the need for accurate and timely diagnosis. Recent advancements in imaging technologies, such as Ultra-WideField Color Fundus Photography (UWF-CFP) imaging and Optical Coherence Tomography Angiography (OCTA), provide opportunities for the early detection of DR but also pose significant challenges given the disparate nature of the data they produce. This study introduces a novel multimodal approach that leverages these imaging modalities to notably enhance DR classification. Our approach integrates 2D UWF-CFP images and 3D high-resolution 6x6 mm$^3$ OCTA (both structure and flow) images using a fusion of ResNet50 and 3D-ResNet50 models, with Squeeze-and-Excitation (SE) blocks to amplify relevant features. Additionally, to increase the model's generalization capabilities, a multimodal extension of Manifold Mixup, applied to concatenated multimodal features, i
    
[^28]: 超越基准：检测视频中多样化的异常现象

    Beyond the Benchmark: Detecting Diverse Anomalies in Videos. (arXiv:2310.01904v1 [cs.CV])

    [http://arxiv.org/abs/2310.01904](http://arxiv.org/abs/2310.01904)

    该研究提出了超越基准的视频异常检测方法，引入了两个新的数据集，HMDB-AD和HMDB-Violence，挑战具有多样化动作异常的模型。此外，还提出了一种新的方法，多帧异常检测（MFAD），它建立在AI-VAD框架上，结合了单帧和两帧特征，以算法计算异常得分。

    

    视频异常检测（VAD）在现代监控系统中起着关键作用，旨在识别现实世界情况中的各种异常现象。然而，当前的基准数据集主要强调简单的单帧异常，如新物体检测。这种狭窄的焦点限制了VAD模型的发展。在这项研究中，我们提倡扩大VAD研究范围，包括超越传统基准边界的复杂异常现象。为此，我们引入了两个数据集，HMDB-AD和HMDB-Violence，用于挑战具有多样化动作异常的模型。这些数据集是从HMDB51动作识别数据集派生而来。我们还提出了一种新的方法，多帧异常检测（MFAD），它建立在AI-VAD框架上。AI-VAD利用单帧特征，如姿势估计和深度图像编码，以及两帧特征，如物体速度。然后应用密度估计算法计算异常得分。

    Video Anomaly Detection (VAD) plays a crucial role in modern surveillance systems, aiming to identify various anomalies in real-world situations. However, current benchmark datasets predominantly emphasize simple, single-frame anomalies such as novel object detection. This narrow focus restricts the advancement of VAD models. In this research, we advocate for an expansion of VAD investigations to encompass intricate anomalies that extend beyond conventional benchmark boundaries. To facilitate this, we introduce two datasets, HMDB-AD and HMDB-Violence, to challenge models with diverse action-based anomalies. These datasets are derived from the HMDB51 action recognition dataset. We further present Multi-Frame Anomaly Detection (MFAD), a novel method built upon the AI-VAD framework. AI-VAD utilizes single-frame features such as pose estimation and deep image encoding, and two-frame features such as object velocity. They then apply a density estimation algorithm to compute anomaly scores. 
    
[^29]: FiGURe: 使用过滤器增强的简单高效的无监督节点表示

    FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations. (arXiv:2310.01892v1 [cs.LG])

    [http://arxiv.org/abs/2310.01892](http://arxiv.org/abs/2310.01892)

    本文介绍了一种简单的过滤器增强方法来改进无监督节点表示学习的性能，通过捕捉不同特征频谱部分，我们展示了显著的改进，并减少了计算负载。同时，我们通过使用简单的随机 Fourier 特征投影来解决高维表示的计算问题，并在基准数据集上取得了良好的性能。

    

    使用对比学习方法学习的无监督节点表示在下游任务上表现出良好的性能。然而，这些方法依赖于模拟低通滤波器的增强，限制了在需要不同特征频谱部分的任务上的性能。本文提出了一种简单的基于过滤器的增强方法来捕捉特征频谱的不同部分。我们展示了使用这些增强方法的显著改进。此外，我们展示了在这些不同的过滤器增强之间共享相同权重是可能的，从而减少了计算负载。此外，先前的研究表明，在下游任务上获得良好的性能需要高维表示。在处理高维度数据时，特别是当涉及多个增强方法时，增加了计算量。我们通过使用简单的随机 Fourier 特征投影来减轻这个问题并恢复良好的性能。我们的方法 FiGURe 在一些基准数据集上实现了

    Unsupervised node representations learnt using contrastive learning-based methods have shown good performance on downstream tasks. However, these methods rely on augmentations that mimic low-pass filters, limiting their performance on tasks requiring different eigen-spectrum parts. This paper presents a simple filter-based augmentation method to capture different parts of the eigen-spectrum. We show significant improvements using these augmentations. Further, we show that sharing the same weights across these different filter augmentations is possible, reducing the computational load. In addition, previous works have shown that good performance on downstream tasks requires high dimensional representations. Working with high dimensions increases the computations, especially when multiple augmentations are involved. We mitigate this problem and recover good performance through lower dimensional embeddings using simple random Fourier feature projections. Our method, FiGURe achieves an ave
    
[^30]: 有效且参数高效的重复使用微调模型

    Effective and Parameter-Efficient Reusing Fine-Tuned Models. (arXiv:2310.01886v1 [cs.LG])

    [http://arxiv.org/abs/2310.01886](http://arxiv.org/abs/2310.01886)

    本文提出了一种有效且参数高效的方法，可以重复使用微调模型来处理下游任务，减轻存储和服务负担，并提出了PERU-FFT方法用于重复使用全面微调模型。

    

    许多在线提供的预训练大规模模型在传递到下游任务中变得非常有效。与此同时，各种在这些预训练模型上微调的任务特定模型也可供公众使用。在实践中，由于收集任务特定数据耗时且微调大规模预训练模型计算复杂，可以重复使用任务特定微调模型来处理下游任务。然而，为每个任务使用一个模型会给存储和服务带来巨大负担。最近，有许多无需训练且参数高效的方法被提出，将多个微调的任务特定模型重复使用到一个多任务模型中。然而，与为每个任务使用微调模型相比，这些方法表现出较大的准确性差距。本文中，我们提出了参数高效方法来重复使用微调模型。针对重复使用全面微调模型，我们提出了PERU-FFT，通过将稀疏任务向量注入到一个mer模型中来实现。

    Many pre-trained large-scale models provided online have become highly effective in transferring to downstream tasks. At the same time, various task-specific models fine-tuned on these pre-trained models are available online for public use. In practice, as collecting task-specific data is labor-intensive and fine-tuning the large pre-trained models is computationally expensive, one can reuse task-specific finetuned models to deal with downstream tasks. However, using a model per task causes a heavy burden on storage and serving. Recently, many training-free and parameter-efficient methods have been proposed for reusing multiple fine-tuned task-specific models into a single multi-task model. However, these methods exhibit a large accuracy gap compared with using a fine-tuned model per task. In this paper, we propose Parameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing Fully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task vector into a mer
    
[^31]: 自适应混合模型的改进VMD和堆叠Informer在增强股市预测中的应用

    Adaptive Hybrid Model for Enhanced Stock Market Predictions Using Improved VMD and Stacked Informer. (arXiv:2310.01884v1 [cs.LG])

    [http://arxiv.org/abs/2310.01884](http://arxiv.org/abs/2310.01884)

    本研究提出了一种自适应混合模型，通过改进的VMD、特征工程和堆叠Informer，结合自适应损失函数，成功应用于股市预测。实验证明该模型在预测准确性、响应性和泛化能力方面优于传统和其他混合模型，对于小企业和特征工程的预测建模有潜在的优化方向和未来发展方向。

    

    本文介绍了一种创新的自适应混合模型，利用改进的变分模态分解（VMD）、特征工程（FE）和堆叠Informer，并结合自适应损失函数，用于股市预测。通过严格的实验，所提出的模型，命名为VMGCformer（Adam+GC+enhanced informer），在处理股市数据的复杂动态和波动性方面展示出显著的熟练度。基于多个基准数据集得出的实验结果突显出该模型在预测准确性、响应性和泛化能力方面相对传统和其他混合模型的优势。本研究进一步强调了优化的潜在途径，并介绍了进一步增强预测建模的未来方向，特别是针对小企业和特征工程。

    This paper introduces an innovative adaptive hybrid model for stock market predictions, leveraging the capabilities of an enhanced Variational Mode Decomposition (VMD), Feature Engineering (FE), and stacked Informer integrated with an adaptive loss function. Through rigorous experimentation, the proposed model, termed Adam+GC+enhanced informer (We name it VMGCformer), demonstrates significant proficiency in addressing the intricate dynamics and volatile nature of stock market data. Experimental results, derived from multiple benchmark datasets, underscore the model's superiority in terms of prediction accuracy, responsiveness, and generalization capabilities over traditional and other hybrid models. The research further highlights potential avenues for optimization and introduces future directions to enhance predictive modeling, especially for small enterprises and feature engineering.
    
[^32]: AutoCast++：利用零-shot基于排名的上下文检索增强世界事件预测

    AutoCast++: Enhancing World Event Prediction with Zero-shot Ranking-based Context Retrieval. (arXiv:2310.01880v1 [cs.LG])

    [http://arxiv.org/abs/2310.01880](http://arxiv.org/abs/2310.01880)

    AutoCast++是一个零-shot基于排名的上下文检索系统，用于从广泛的新闻文档集合中进行事件预测。

    

    基于机器的实时事件预测因其在决策过程中能提供知情决策而受到关注。然而，传统的预测方法主要依赖于结构化数据，如时间序列，而最近语言模型的突破使得可以使用非结构化文本进行预测。本文介绍了AutoCast++，它是一个零-shot基于排名的上下文检索系统，旨在针对广泛的新闻文档集合进行事件预测。我们的方法首先根据零-shot的问题-段落相关性对文章进行重新排名，从而找到与语义相关的新闻。之后，所选的文章将进行零-shot处理用于事件预测。

    Machine-based prediction of real-world events is garnering attention due to its potential for informed decision-making. Whereas traditional forecasting predominantly hinges on structured data like time-series, recent breakthroughs in language models enable predictions using unstructured text. In particular, (Zou et al., 2022) unveils AutoCast, a new benchmark that employs news articles for answering forecasting queries. Nevertheless, existing methods still trail behind human performance. The cornerstone of accurate forecasting, we argue, lies in identifying a concise, yet rich subset of news snippets from a vast corpus. With this motivation, we introduce AutoCast++, a zero-shot ranking-based context retrieval system, tailored to sift through expansive news document collections for event forecasting. Our approach first re-ranks articles based on zero-shot question-passage relevance, honing in on semantically pertinent news. Following this, the chosen articles are subjected to zero-shot 
    
[^33]: 通过特征漂移调整实现稳定的后门净化

    Towards Stable Backdoor Purification through Feature Shift Tuning. (arXiv:2310.01875v1 [cs.LG])

    [http://arxiv.org/abs/2310.01875](http://arxiv.org/abs/2310.01875)

    本文通过综合评估不同攻击场景下的微调方法，提出了一种稳定的后门净化方法。研究发现，在低污染率的情况下，后门和干净特征之间的纠缠会削弱调整方法的效果。

    

    深度神经网络（DNN）容易受到后门攻击，攻击者可以通过篡改一小组训练样本来恶意操控模型行为。虽然提出了一系列防御方法来减轻这种威胁，但它们要么需要对训练过程进行复杂修改，要么严重依赖特定的模型架构，使得它们难以应用于现实世界的应用。因此，在本文中，我们从微调开始，通过对各种攻击场景的全面评估来探索最常见和易于部署的后门防御方法。通过初步实验观察发现，与高污染率的有希望的防御结果相比，普通的调整方法在低污染率场景下完全失效。我们的分析表明，在低污染率下，后门和干净特征之间的纠缠破坏了基于调整的效果。

    It has been widely observed that deep neural networks (DNN) are vulnerable to backdoor attacks where attackers could manipulate the model behavior maliciously by tampering with a small set of training samples. Although a line of defense methods is proposed to mitigate this threat, they either require complicated modifications to the training process or heavily rely on the specific model architecture, which makes them hard to deploy into real-world applications. Therefore, in this paper, we instead start with fine-tuning, one of the most common and easy-to-deploy backdoor defenses, through comprehensive evaluations against diverse attack scenarios. Observations made through initial experiments show that in contrast to the promising defensive results on high poisoning rates, vanilla tuning methods completely fail at low poisoning rate scenarios. Our analysis shows that with the low poisoning rate, the entanglement between backdoor and clean features undermines the effect of tuning-based 
    
[^34]: DeepDecipher: 访问和研究大规模语言模型中的神经元激活

    DeepDecipher: Accessing and Investigating Neuron Activation in Large Language Models. (arXiv:2310.01870v1 [cs.LG])

    [http://arxiv.org/abs/2310.01870](http://arxiv.org/abs/2310.01870)

    DeepDecipher是一种用于探测大规模语言模型中神经元激活的API和接口，它通过提供先进的可解释性技术和易于使用的界面，使得对这些复杂模型的分析更加直观和可访问。

    

    随着大规模语言模型（LLMs）的能力越来越强，对可解释和透明工具的紧迫需求逐渐增加。目前的方法难以实现，并且缺乏分析模型内部的可访问工具。为了弥补这一差距，我们提出了DeepDecipher - 一种用于探测转换器模型MLP层中神经元的API和接口。DeepDecipher将LLMs的先进可解释性技术的输出变得容易获取。易于使用的界面还使得对这些复杂模型的检查更加直观。本文概述了DeepDecipher的设计和能力。我们演示了如何分析神经元，比较模型，并获得有关模型行为的见解。例如，我们将DeepDecipher的功能与类似的工具如Neuroscope和OpenAI的Neuron Explainer进行对比。DeepDecipher实现了对LLMs的高效可扩展分析。通过提供最先进的可解释性方法，DeepDecipher使LLMs变得更加透明、可信和安全。

    As large language models (LLMs) become more capable, there is an urgent need for interpretable and transparent tools. Current methods are difficult to implement, and accessible tools to analyze model internals are lacking. To bridge this gap, we present DeepDecipher - an API and interface for probing neurons in transformer models' MLP layers. DeepDecipher makes the outputs of advanced interpretability techniques for LLMs readily available. The easy-to-use interface also makes inspecting these complex models more intuitive. This paper outlines DeepDecipher's design and capabilities. We demonstrate how to analyze neurons, compare models, and gain insights into model behavior. For example, we contrast DeepDecipher's functionality with similar tools like Neuroscope and OpenAI's Neuron Explainer. DeepDecipher enables efficient, scalable analysis of LLMs. By granting access to state-of-the-art interpretability methods, DeepDecipher makes LLMs more transparent, trustworthy, and safe. Research
    
[^35]: 带有表示学习的条件工具变量回归用于因果推断

    Conditional Instrumental Variable Regression with Representation Learning for Causal Inference. (arXiv:2310.01865v1 [cs.LG])

    [http://arxiv.org/abs/2310.01865](http://arxiv.org/abs/2310.01865)

    本文提出了一种使用条件工具变量回归和表示学习进行因果推断的方法，该方法能够解决由未观测潜变量引起的混淆偏差，并在无需线性假设的情况下平衡观察到的混淆变量。

    

    本文研究在存在未观测潜变量的观测数据中估计因果效应的挑战性问题。两阶段最小二乘法（TSLS）方法及其具有标准工具变量（IV）的变种常用于消除混淆偏差，包括由未观测潜变量引起的偏差，但它们依赖线性假设。此外，对于标准IV方法提出的无混淆工具的严格条件对于实践来说太强了。为了解决标准IV方法（线性假设和严格条件）的这些具有挑战性和实际问题，在本文中，我们使用条件IV（CIV）放松标准IV的无混淆工具条件，提出了一种非线性CIV回归与混淆均衡表示学习（CBRL.CIV），用于同时消除由未观测潜变量引起的混淆偏差和平衡观察到的混淆。

    This paper studies the challenging problem of estimating causal effects from observational data, in the presence of unobserved confounders. The two-stage least square (TSLS) method and its variants with a standard instrumental variable (IV) are commonly used to eliminate confounding bias, including the bias caused by unobserved confounders, but they rely on the linearity assumption. Besides, the strict condition of unconfounded instruments posed on a standard IV is too strong to be practical. To address these challenging and practical problems of the standard IV method (linearity assumption and the strict condition), in this paper, we use a conditional IV (CIV) to relax the unconfounded instrument condition of standard IV and propose a non-linear CIV regression with Confounding Balancing Representation Learning, CBRL.CIV, for jointly eliminating the confounding bias from unobserved confounders and balancing the observed confounders, without the linearity assumption. We theoretically de
    
[^36]: 高概率下具有重尾噪声的复合式和分布式随机最小化和变分不等式收敛性分析

    High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise. (arXiv:2310.01860v1 [math.OC])

    [http://arxiv.org/abs/2310.01860](http://arxiv.org/abs/2310.01860)

    提出了一种针对复合式和分布式优化问题的新的随机方法，通过剪裁梯度差值实现了紧致的高概率收敛性分析。

    

    近年来，对于具有轻微噪声假设的随机一阶优化方法的高概率分析受到了广泛关注。通常情况下，当噪声是重尾的时候，梯度剪裁是推导出良好的高概率保证的关键算法要素之一。然而，如果不加以处理地实现，剪裁操作会破坏常用的复合式和分布式优化方法（如Prox-SGD/Parallel SGD）的收敛性，即使在没有任何噪声的情况下也是如此。因此，许多高概率分析的工作仅考虑无约束的非分布式问题，现有的复合式/分布式问题的收敛性结果并不包括一些重要的特殊情况（如强凸问题），也不是最优的。为了解决这个问题，我们提出了基于梯度差值剪裁的复合式和分布式优化的新的随机方法，并证明了紧致的高概率收敛性结果（包括几乎所有的场景）。

    High-probability analysis of stochastic first-order optimization methods under mild assumptions on the noise has been gaining a lot of attention in recent years. Typically, gradient clipping is one of the key algorithmic ingredients to derive good high-probability guarantees when the noise is heavy-tailed. However, if implemented na\"ively, clipping can spoil the convergence of the popular methods for composite and distributed optimization (Prox-SGD/Parallel SGD) even in the absence of any noise. Due to this reason, many works on high-probability analysis consider only unconstrained non-distributed problems, and the existing results for composite/distributed problems do not include some important special cases (like strongly convex problems) and are not optimal. To address this issue, we propose new stochastic methods for composite and distributed optimization based on the clipping of stochastic gradient differences and prove tight high-probability convergence results (including nearly
    
[^37]: 变分高斯逼近Kushner最优滤波器

    Variational Gaussian approximation of the Kushner optimal filter. (arXiv:2310.01859v1 [stat.ML])

    [http://arxiv.org/abs/2310.01859](http://arxiv.org/abs/2310.01859)

    本论文提出了一种基于变分高斯逼近的方法来近似解决Kushner方程，通过传播和贝叶斯更新概率密度相关的两个接近损失，利用Wasserstein度量和Fisher度量，通过隐式更新均值和协方差矩阵来解决最后的接近损失，从而得到了满足高斯流的随机微分方程，扩展了线性情况下的Kalman-Bucy和Riccati流。

    

    在估计理论中，Kushner方程提供了给定连续时间观测的动态系统状态的概率密度的演化。在我们最近的工作的基础上，我们提出了一种通过可处理的变分高斯逼近来近似解决Kushner方程的新方法，其中涉及到与概率密度的传播和贝叶斯更新相关的两个接近近似损失。第一个是基于Wasserstein度量的接近损失，第二个是基于Fisher度量的接近损失。这个最后接近损失的解决方案由我们之前提出的均值和协方差的隐式更新给出。这两个变分更新可以融合并证明满足高斯均值和协方差矩阵的一组随机微分方程。这个高斯流与线性情况下的Kalman-Bucy和Riccati流一致，并在非线性情况下推广了它们。

    In estimation theory, the Kushner equation provides the evolution of the probability density of the state of a dynamical system given continuous-time observations. Building upon our recent work, we propose a new way to approximate the solution of the Kushner equation through tractable variational Gaussian approximations of two proximal losses associated with the propagation and Bayesian update of the probability density. The first is a proximal loss based on the Wasserstein metric and the second is a proximal loss based on the Fisher metric. The solution to this last proximal loss is given by implicit updates on the mean and covariance that we proposed earlier. These two variational updates can be fused and shown to satisfy a set of stochastic differential equations on the Gaussian's mean and covariance matrix. This Gaussian flow is consistent with the Kalman-Bucy and Riccati flows in the linear case and generalize them in the nonlinear one.
    
[^38]: 基于评分的数据同化在双层拟地转动模型中的应用

    Score-based Data Assimilation for a Two-Layer Quasi-Geostrophic Model. (arXiv:2310.01853v1 [stat.ML])

    [http://arxiv.org/abs/2310.01853](http://arxiv.org/abs/2310.01853)

    本论文评估了基于评分的数据同化方法在高维度的地球物理动力系统中的可扩展性，并通过在双层拟地转动模型上的实验证明了该方法的良好性能。

    

    数据同化解决了在给定嘈杂或不完整观测情况下，确定动力系统可行状态轨迹的问题。在地球科学中，由于地球物理动力系统的高维度性，往往超过了数百万维度，因此存在挑战。本文评估了基于评分的数据同化（SDA）这一新颖的数据同化方法在此类系统中的可扩展性。我们提出了针对评分网络架构的修改，旨在显著减少内存消耗和执行时间。我们在一个双层拟地转动模型中展示了有希望的结果。

    Data assimilation addresses the problem of identifying plausible state trajectories of dynamical systems given noisy or incomplete observations. In geosciences, it presents challenges due to the high-dimensionality of geophysical dynamical systems, often exceeding millions of dimensions. This work assesses the scalability of score-based data assimilation (SDA), a novel data assimilation method, in the context of such systems. We propose modifications to the score network architecture aimed at significantly reducing memory consumption and execution time. We demonstrate promising results for a two-layer quasi-geostrophic model.
    
[^39]: 基准测试和改进语言模型生成器验证器一致性

    Benchmarking and Improving Generator-Validator Consistency of Language Models. (arXiv:2310.01846v1 [cs.CL])

    [http://arxiv.org/abs/2310.01846](http://arxiv.org/abs/2310.01846)

    本文提出了一种衡量生成和验证之间一致性的框架，发现目前最先进的语言模型GPT-4仅在76%的情况下是一致的。为了改进一致性，提出了一种通过经过筛选的生成器和验证器答案进行微调的方法，并将其称为一致性微调。该方法将Alpaca-30B的一致性从60%提高到93%，并且对未见过的任务和领域也具有泛化能力。除了改进一致性外，一致性微调还带来了其他性能的提升。

    

    截至2023年9月，ChatGPT在被问到"7+8=15，对还是错"时，回答"错"，但在被问到"7+8"等于多少时，回答"15"。这种生成和验证答案之间的不一致在语言模型中很常见，破坏了人们的信任。在本文中，我们提出了一种衡量生成和验证之间一致性的框架（称为生成器验证器一致性），发现即使是最先进的GPT-4语言模型，仅在76%的情况下是一致的。为了改进语言模型的一致性，我们提出了在经过生成器和验证器答案筛选的基础上进行微调的方法，并称之为一致性微调。我们发现，这种方法将Alpaca-30B的一致性从60%提高到了93%，并且这种改进可以推广到未见过的任务和领域（例如，对于积极的风格转换，一致性的改进可以推广到未见过的风格，如幽默）。除了改进一致性外，一致性微调还改善了其他性能指标。

    As of September 2023, ChatGPT correctly answers "what is 7+8" with 15, but when asked "7+8=15, True or False" it responds with "False". This inconsistency between generating and validating an answer is prevalent in language models (LMs) and erodes trust. In this paper, we propose a framework for measuring the consistency between generation and validation (which we call generator-validator consistency, or GV-consistency), finding that even GPT-4, a state-of-the-art LM, is GV-consistent only 76% of the time. To improve the consistency of LMs, we propose to finetune on the filtered generator and validator responses that are GV-consistent, and call this approach consistency fine-tuning. We find that this approach improves GV-consistency of Alpaca-30B from 60% to 93%, and the improvement extrapolates to unseen tasks and domains (e.g., GV-consistency for positive style transfers extrapolates to unseen styles like humor). In addition to improving consistency, consistency fine-tuning improves 
    
[^40]: 使用SAM进行建筑物分割模型的零-shot细化

    Zero-Shot Refinement of Buildings' Segmentation Models using SAM. (arXiv:2310.01845v1 [cs.CV])

    [http://arxiv.org/abs/2310.01845](http://arxiv.org/abs/2310.01845)

    本文提出了一种使用SAM进行建筑物分割模型的零-shot细化的方法，针对遥感图像应用中SAM性能不佳、无法进行识别的问题进行了处理。通过引入不同的提示来提升模型的泛化能力。

    

    基础模型在各种任务中表现出色，但通常在常规基准测试中评估。将这些模型应用于特定领域，如遥感图像，仍然是一个未充分开发的领域。在遥感领域中，精确的建筑物实例分割对于城市规划等应用至关重要。虽然卷积神经网络（CNN）表现良好，但它们的泛化能力可能受限。为了实现这一目标，我们提出了一种新的方法，以使基础模型适应已有模型的泛化性能下降。在众多模型中，我们的重点在于Segment Anything Model（SAM），这是一种强大的基础模型，以其擅长无类别图像分割能力而闻名。我们首先确定了SAM的局限性，揭示了它在应用于遥感图像时性能不佳。此外，SAM不具备识别能力，因此无法对定位的对象进行分类和标记。为了解决这些限制，我们引入了不同的提示

    Foundation models have excelled in various tasks but are often evaluated on general benchmarks. The adaptation of these models for specific domains, such as remote sensing imagery, remains an underexplored area. In remote sensing, precise building instance segmentation is vital for applications like urban planning. While Convolutional Neural Networks (CNNs) perform well, their generalization can be limited. For this aim, we present a novel approach to adapt foundation models to address existing models' generalization dropback. Among several models, our focus centers on the Segment Anything Model (SAM), a potent foundation model renowned for its prowess in class-agnostic image segmentation capabilities. We start by identifying the limitations of SAM, revealing its suboptimal performance when applied to remote sensing imagery. Moreover, SAM does not offer recognition abilities and thus fails to classify and tag localized objects. To address these limitations, we introduce different promp
    
[^41]: 扩展基于CAM的可解释AI方法用于遥感图像分割

    Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation. (arXiv:2310.01837v1 [cs.CV])

    [http://arxiv.org/abs/2310.01837](http://arxiv.org/abs/2310.01837)

    这篇论文扩展了基于CAM的可解释AI方法，使其适用于遥感图像分割。当前AI模型在高分辨率卫星图像上训练时缺乏透明度和可解释性，本文通过改进XAI分类算法，提供了解释图像分割的手段。

    

    当前的基于AI的方法无法对所使用的数据、提取的特征和预测/推理操作提供可理解的物理解释。因此，使用高分辨率卫星图像训练的深度学习模型缺乏透明度和可解释性，只能被视为一个黑盒子，这限制了它们的广泛采用。专家需要帮助理解AI模型的复杂行为和基础决策过程。可解释人工智能（XAI）领域是一个新兴领域，提供了确保AI模型稳健、实用和可信赖部署的手段。已有一些XAI技术被提出用于图像分类任务，而对于图像分割的解释则基本上没有被探索。本文通过改进最新的XAI分类算法，并使其适用于多类图像分割，以弥补这一差距，其中我们主要关注高分辨率卫星图像中的建筑物分割。

    Current AI-based methods do not provide comprehensible physical interpretations of the utilized data, extracted features, and predictions/inference operations. As a result, deep learning models trained using high-resolution satellite imagery lack transparency and explainability and can be merely seen as a black box, which limits their wide-level adoption. Experts need help understanding the complex behavior of AI models and the underlying decision-making process. The explainable artificial intelligence (XAI) field is an emerging field providing means for robust, practical, and trustworthy deployment of AI models. Several XAI techniques have been proposed for image classification tasks, whereas the interpretation of image segmentation remains largely unexplored. This paper offers to bridge this gap by adapting the recent XAI classification algorithms and making them usable for muti-class image segmentation, where we mainly focus on buildings' segmentation from high-resolution satellite 
    
[^42]: EMBERSim: 用于提高恶意软件分析相似性搜索的大规模数据库

    EMBERSim: A Large-Scale Databank for Boosting Similarity Search in Malware Analysis. (arXiv:2310.01835v1 [cs.LG])

    [http://arxiv.org/abs/2310.01835](http://arxiv.org/abs/2310.01835)

    EMBERSim是一个大规模数据库，用于提高恶意软件分析中的相似性搜索。它通过从最大的恶意软件分类数据集EMBER开始，并将相似性信息和恶意软件类别标签相结合，解决了相似性研究领域的不足。

    

    近年来，从启发式的恶意软件检测向机器学习的转变，证明在当前充满敌对性威胁的环境下更加强大。虽然我们承认机器学习在挖掘日益增多的相似文件中的模式方面更加有优势，但我们还观察到在相似性研究领域中可用数据的明显缺乏。此外，我们观察到在少数相关工作中，重点放在量化恶意软件的相似性上，往往忽视了清洁数据。在检测绕过的背景下，这种单方面的量化尤其危险。我们提出解决二进制文件相似性研究空白领域的不足，从最大的恶意软件分类数据集EMBER开始。我们将EMBER与相似性信息以及恶意软件类别标签相结合，以支持相似性空间中的进一步研究。我们的贡献有三个方面：(1)我们发布EMBERSim，

    In recent years there has been a shift from heuristics-based malware detection towards machine learning, which proves to be more robust in the current heavily adversarial threat landscape. While we acknowledge machine learning to be better equipped to mine for patterns in the increasingly high amounts of similar-looking files, we also note a remarkable scarcity of the data available for similarity-targeted research. Moreover, we observe that the focus in the few related works falls on quantifying similarity in malware, often overlooking the clean data. This one-sided quantification is especially dangerous in the context of detection bypass. We propose to address the deficiencies in the space of similarity research on binary files, starting from EMBER - one of the largest malware classification data sets. We enhance EMBER with similarity information as well as malware class tags, to enable further research in the similarity space. Our contribution is threefold: (1) we publish EMBERSim, 
    
[^43]: 可训练的噪声模型作为XAI评估方法：在遥感图像分割中的应用

    Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation. (arXiv:2310.01828v1 [cs.CV])

    [http://arxiv.org/abs/2310.01828](http://arxiv.org/abs/2310.01828)

    本论文介绍了一种可训练的噪声模型作为XAI评估方法，在遥感图像分割中的应用。在图像处理中提供深度神经网络的可解释性对于广泛采用和部署至关重要。虽然图像分割在计算机视觉应用中很重要，但在可解释性方面受到了相对较少的关注。

    

    可解释的人工智能（XAI）已成为处理关键任务应用时的必备要求，确保所使用的黑盒子人工智能模型的透明度和可解释性。XAI的重要性涵盖了各个领域，从医疗保健到金融，在这些领域中，了解深度学习算法的决策过程是至关重要的。大多数基于人工智能的计算机视觉模型往往是黑盒子，因此在图像处理中提供深度神经网络的可解释性对于它们在医疗图像分析、自动驾驶和遥感应用中的广泛采用和部署至关重要。最近，已经提出了几种针对图像分类任务的XAI方法。相比之下，在可解释性方面，图像分割在计算机视觉应用中，特别是在遥感领域中，受到了相对较少的关注。只有少数研究提出了基于梯度的XAI算法来进行图像分割。

    eXplainable Artificial Intelligence (XAI) has emerged as an essential requirement when dealing with mission-critical applications, ensuring transparency and interpretability of the employed black box AI models. The significance of XAI spans various domains, from healthcare to finance, where understanding the decision-making process of deep learning algorithms is essential. Most AI-based computer vision models are often black boxes; hence, providing explainability of deep neural networks in image processing is crucial for their wide adoption and deployment in medical image analysis, autonomous driving, and remote sensing applications. Recently, several XAI methods for image classification tasks have been introduced. On the contrary, image segmentation has received comparatively less attention in the context of explainability, although it is a fundamental task in computer vision applications, especially in remote sensing. Only some research proposes gradient-based XAI algorithms for imag
    
[^44]: 冬小麦分割的PEFT技术的实证研究

    Empirical Study of PEFT techniques for Winter Wheat Segmentation. (arXiv:2310.01825v1 [cs.CV])

    [http://arxiv.org/abs/2310.01825](http://arxiv.org/abs/2310.01825)

    本研究通过使用PEFT技术，探索跨区域和跨年份的分布外推广性，以适应农作物监测的需求。

    

    参数高效微调（PEFT）技术最近经历了显著的增长，并被广泛用于将大规模视觉和语言模型适应于各种领域，以最小的计算需求实现令人满意的模型性能。尽管取得了这些进展，但在实际场景中，特别是在遥感和农作物监测的关键领域中，仍需要进一步研究潜在的PEFT应用。不同地区的气候多样性和对全面的大规模数据集的需求，给精确识别不同地理位置和不断变化的种植季节的作物类型造成了重大障碍。本研究旨在通过全面探索跨区域和跨年份的分布外推广性，使用国内领先的冬小麦作物监测模型，来填补这一差距。这项工作的目标是探索PEFT方法在作物监测中的应用。具体而言，我们专注于适应性地调整PEFT方法以适应农作物监测的需求。

    Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced significant growth and have been extensively employed to adapt large vision and language models to various domains, enabling satisfactory model performance with minimal computational needs. Despite these advances, more research has yet to delve into potential PEFT applications in real-life scenarios, particularly in the critical domains of remote sensing and crop monitoring. The diversity of climates across different regions and the need for comprehensive large-scale datasets have posed significant obstacles to accurately identify crop types across varying geographic locations and changing growing seasons. This study seeks to bridge this gap by comprehensively exploring the feasibility of cross-area and cross-year out-of-distribution generalization using the State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to explore PEFT approaches for crop monitoring. Specifically, we focus on adap
    
[^45]: Mini-BEHAVIOR：面向具身人工智能的长期决策制定的过程生成基准

    Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI. (arXiv:2310.01824v1 [cs.AI])

    [http://arxiv.org/abs/2310.01824](http://arxiv.org/abs/2310.01824)

    Mini-BEHAVIOR是一个面向具身人工智能的基准，旨在挑战智能体解决类似于日常挑战的复杂活动，并通过过程生成实现了无限的任务变化和对开放式学习的支持。

    

    我们提出了Mini-BEHAVIOR，一种新颖的面向具身人工智能的基准，挑战智能体利用推理和决策技能解决类似于日常人类挑战的复杂活动。Mini-BEHAVIOR环境是一个快速，现实的Gridworld环境，既具有快速原型设计和易用性的优点，同时也保留了复杂具身人工智能基准中符号级的物理现实感和复杂性。我们引入了关键特性，如过程生成，以实现无限的任务变化和对开放式学习的支持。Mini-BEHAVIOR提供了原始BEHAVIOR基准中各种家务任务的实现，以及用于数据收集和强化学习代理训练的入门代码。总之，Mini-BEHAVIOR为评估具身人工智能中的决策制定和规划解决方案提供了一个快速、开放式的基准。它作为研究的用户友好的入口点，促进了评估和发展过程。

    We present Mini-BEHAVIOR, a novel benchmark for embodied AI that challenges agents to use reasoning and decision-making skills to solve complex activities that resemble everyday human challenges. The Mini-BEHAVIOR environment is a fast, realistic Gridworld environment that offers the benefits of rapid prototyping and ease of use while preserving a symbolic level of physical realism and complexity found in complex embodied AI benchmarks. We introduce key features such as procedural generation, to enable the creation of countless task variations and support open-ended learning. Mini-BEHAVIOR provides implementations of various household tasks from the original BEHAVIOR benchmark, along with starter code for data collection and reinforcement learning agent training. In essence, Mini-BEHAVIOR offers a fast, open-ended benchmark for evaluating decision-making and planning solutions in embodied AI. It serves as a user-friendly entry point for research and facilitates the evaluation and devel
    
[^46]: MIMO-NeRF: 多输入多输出神经辐射场的快速神经渲染

    MIMO-NeRF: Fast Neural Rendering with Multi-input Multi-output Neural Radiance Fields. (arXiv:2310.01821v1 [cs.CV])

    [http://arxiv.org/abs/2310.01821](http://arxiv.org/abs/2310.01821)

    MIMO-NeRF是一种多输入多输出神经辐射场算法，通过使用MIMO MLP并进行分组映射，提高了渲染速度，并通过自监督学习方法减轻了部分模糊性。

    

    神经辐射场（NeRF）已经显示出在新视角合成方面的令人印象深刻的结果。然而，它们依赖于单输入单输出多层感知机（SISO MLP）的重复使用，将3D坐标和视角映射到颜色和体积密度，这会导致渲染速度变慢。我们提出了一种多输入多输出神经辐射场（MIMO-NeRF），通过将SISO MLP替换为MIMO MLP并进行分组映射来减少运行的MLP数量。其中一个显著的挑战是，每个点的颜色和体积密度可以根据组内输入坐标的选择有所不同，这可能会导致一些明显的模糊性。我们还提出了一种自监督学习方法，通过多个快速重构的MLP对MIMO MLP进行正则化，以减轻此模糊性而无需使用预训练模型。我们呈现了包括比较和消融研究在内的全面的实验评估结果。

    Neural radiance fields (NeRFs) have shown impressive results for novel view synthesis. However, they depend on the repetitive use of a single-input single-output multilayer perceptron (SISO MLP) that maps 3D coordinates and view direction to the color and volume density in a sample-wise manner, which slows the rendering. We propose a multi-input multi-output NeRF (MIMO-NeRF) that reduces the number of MLPs running by replacing the SISO MLP with a MIMO MLP and conducting mappings in a group-wise manner. One notable challenge with this approach is that the color and volume density of each point can differ according to a choice of input coordinates in a group, which can lead to some notable ambiguity. We also propose a self-supervised learning method that regularizes the MIMO MLP with multiple fast reformulated MLPs to alleviate this ambiguity without using pretrained models. The results of a comprehensive experimental evaluation including comparative and ablation studies are presented to
    
[^47]: 进向鲁棒度评估图神经网络解释性的方法

    Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks. (arXiv:2310.01820v1 [cs.LG])

    [http://arxiv.org/abs/2310.01820](http://arxiv.org/abs/2310.01820)

    本文研究了评估图神经网络解释性的鲁棒度的方法，并指出了现有度量方法的局限性。

    

    图神经网络（GNN）是一种利用图数据中的依赖结构通过节点之间的消息传递进行建模的神经网络模型。GNN已经成为分析图结构数据的关键架构，在敏感领域的广泛应用要求对其决策过程有全面的理解，这就需要一个GNN可解释性的框架。为了评估GNN解释函数的性能，需要提供可靠的保真度度量。本文研究了这一基础性挑战，重点在现有的保真度度量方法中揭示了潜在的局限性，包括Fid_+，Fid_-和Fid_Δ。具体而言，本文介绍了一个正式的信息论解释性定义，并证明了现有度量方法的限制

    Graph Neural Networks (GNNs) are neural models that leverage the dependency structure in graphical data via message passing among the graph nodes. GNNs have emerged as pivotal architectures in analyzing graph-structured data, and their expansive application in sensitive domains requires a comprehensive understanding of their decision-making processes -- necessitating a framework for GNN explainability. An explanation function for GNNs takes a pre-trained GNN along with a graph as input, to produce a `sufficient statistic' subgraph with respect to the graph label. A main challenge in studying GNN explainability is to provide fidelity measures that evaluate the performance of these explanation functions. This paper studies this foundational challenge, spotlighting the inherent limitations of prevailing fidelity metrics, including $Fid_+$, $Fid_-$, and $Fid_\Delta$. Specifically, a formal, information-theoretic definition of explainability is introduced and it is shown that existing metri
    
[^48]: AutoLoRa：参数免调自动鲁棒微调框架

    AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework. (arXiv:2310.01818v1 [cs.LG])

    [http://arxiv.org/abs/2310.01818](http://arxiv.org/abs/2310.01818)

    AutoLoRa是一个自动鲁棒微调框架，通过引入低秩分支和启发式策略，解决了现有鲁棒微调存在的梯度方向分歧和超参数敏感性问题。

    

    鲁棒微调（RFT）是一种低成本策略，用于在下游应用中获得对抗鲁棒性，无需大量计算资源和收集大量数据。本文揭示了现有RFT存在的问题，即通过特征提取器（FE）优化对抗性和自然目标会导致显著不同的梯度方向。这种分歧在优化过程中引入不稳定性，从而阻碍了对抗鲁棒性的实现，并使RFT对超参数高度敏感。为了解决这个问题，我们提出了一个低秩（LoRa）分支，将RFT分解为两个不同的组件：通过LoRa分支优化自然目标和通过FE优化对抗目标。此外，我们还引入了启发式策略来自动调整学习率和损失项的标量。大量实证评估表明，我们提出的自动化RFT通过LoRa分解得到了...

    Robust Fine-Tuning (RFT) is a low-cost strategy to obtain adversarial robustness in downstream applications, without requiring a lot of computational resources and collecting significant amounts of data. This paper uncovers an issue with the existing RFT, where optimizing both adversarial and natural objectives through the feature extractor (FE) yields significantly divergent gradient directions. This divergence introduces instability in the optimization process, thereby hindering the attainment of adversarial robustness and rendering RFT highly sensitive to hyperparameters. To mitigate this issue, we propose a low-rank (LoRa) branch that disentangles RFT into two distinct components: optimizing natural objectives via the LoRa branch and adversarial objectives via the FE. Besides, we introduce heuristic strategies for automating the scheduling of the learning rate and the scalars of loss terms. Extensive empirical evaluations demonstrate that our proposed automated RFT disentangled via
    
[^49]: NFT的价格是由什么决定的？

    What Determines the Price of NFTs?. (arXiv:2310.01815v1 [cs.CE])

    [http://arxiv.org/abs/2310.01815](http://arxiv.org/abs/2310.01815)

    该论文分析了在OpenSea交易的NFT收藏的链上和链下数据，探讨了NFT价格的影响因素。结果发现，尽管文本和图像数据可以解释收藏内部价格的变化，但这些特征不能推广到新的收藏品。

    

    在数字艺术的不断发展中，非同质化代币（NFT）已经成为一个突破性的平台，桥接了艺术和技术领域。NFT作为一种基础框架，彻底改变了数字艺术市场，使艺术家能够以前所未有的方式展示和变现自己的作品。NFT将存储在区块链上的元数据与离链数据（例如图像）相结合，创建了一种新颖的数字所有权形式。目前还不完全清楚这些因素如何共同决定NFT的价格。在本研究中，我们分析了在OpenSea交易的NFT收藏的链上和链下数据，以了解什么影响NFT的定价。我们的研究结果表明，尽管NFT的文本和图像数据可以用来解释收藏中价格的变化，但这些提取的特征不能推广到新的、未见过的收藏品。此外，我们发现NFT收藏的交易量往往与其在线存在（如社交媒体关注者数量）有关。

    In the evolving landscape of digital art, Non-Fungible Tokens (NFTs) have emerged as a groundbreaking platform, bridging the realms of art and technology. NFTs serve as the foundational framework that has revolutionized the market for digital art, enabling artists to showcase and monetize their creations in unprecedented ways. NFTs combine metadata stored on the blockchain with off-chain data, such as images, to create a novel form of digital ownership. It is not fully understood how these factors come together to determine NFT prices. In this study, we analyze both on-chain and off-chain data of NFT collections trading on OpenSea to understand what influences NFT pricing. Our results show that while text and image data of the NFTs can be used to explain price variations within collections, the extracted features do not generalize to new, unseen collections. Furthermore, we find that an NFT collection's trading volume often relates to its online presence, like social media followers an
    
[^50]: 基于广义Kullback-Leibler散度的仿真推断

    Simulation-based Inference with the Generalized Kullback-Leibler Divergence. (arXiv:2310.01808v1 [stat.ML])

    [http://arxiv.org/abs/2310.01808](http://arxiv.org/abs/2310.01808)

    本研究提出了一种基于广义Kullback-Leibler散度的仿真推断方法，通过考虑非归一化分布中的归一化常数，将神经后验估计与神经比值估计结合为一个目标，并研究了一种混合模型来实现最佳效果。

    

    在基于仿真的推断中，目标是在似然函数只隐式存在的情况下解决逆问题。神经后验估计通常使用归一化密度估计器作为后验的代理模型。由于优化的是Kullback-Leibler散度，这种形式很难适应非归一化代理模型。我们提出了一种优化广义Kullback-Leibler散度的方法，该方法考虑了非归一化分布中的归一化常数。当模型类被归一化时，该目标恢复了神经后验估计，并将其统一到了神经比值估计中，将两者结合为一个目标。我们研究了一种混合模型，通过学习归一化基础分布和学习比值来实现最佳效果。我们还提供了基准结果。

    In Simulation-based Inference, the goal is to solve the inverse problem when the likelihood is only known implicitly. Neural Posterior Estimation commonly fits a normalized density estimator as a surrogate model for the posterior. This formulation cannot easily fit unnormalized surrogates because it optimizes the Kullback-Leibler divergence. We propose to optimize a generalized Kullback-Leibler divergence that accounts for the normalization constant in unnormalized distributions. The objective recovers Neural Posterior Estimation when the model class is normalized and unifies it with Neural Ratio Estimation, combining both into a single objective. We investigate a hybrid model that offers the best of both worlds by learning a normalized base distribution and a learned ratio. We also present benchmark results.
    
[^51]: 通过吸引子动力学实现离散、组合和符号表示

    Discrete, compositional, and symbolic representations through attractor dynamics. (arXiv:2310.01807v1 [cs.AI])

    [http://arxiv.org/abs/2310.01807](http://arxiv.org/abs/2310.01807)

    这项工作探讨了如何通过模拟吸引子动力学来更加神经可行地实现离散化，从而将连续的表示空间划分为对应于符号序列的分区。通过引入符号空间结构，可以在丰富的感知输入的吸引子支持表示空间中实现组合性。

    

    组合性是离散符号系统（如语言和程序）的重要特征，它使得这些系统尽管使用有限的符号集合，但仍具有无限的容量。它在认知科学和人工智能领域的推理中都具有很好的抽象性。然而，连续和符号处理之间的界面通常是通过算法级别上的量化或softmax采样步骤来实现的。在本研究中，我们通过模拟吸引子动力学将离散化实现得更加神经可行，这种方法将连续的表示空间划分为对应于符号序列的分区。在吸引子网络的基础上，引入了新的训练方法，我们展示了在丰富的感知输入的吸引子支持表示空间中引入符号空间结构可以产生组合性。最后，我们认为我们的模型展示了一种信息增长的过程。

    Compositionality is an important feature of discrete symbolic systems, such as language and programs, as it enables them to have infinite capacity despite a finite symbol set. It serves as a useful abstraction for reasoning in both cognitive science and in AI, yet the interface between continuous and symbolic processing is often imposed by fiat at the algorithmic level, such as by means of quantization or a softmax sampling step. In this work, we explore how discretization could be implemented in a more neurally plausible manner through the modeling of attractor dynamics that partition the continuous representation space into basins that correspond to sequences of symbols. Building on established work in attractor networks and introducing novel training methods, we show that imposing structure in the symbolic space can produce compositionality in the attractor-supported representation space of rich sensory inputs. Lastly, we argue that our model exhibits the process of an information b
    
[^52]: GNNX-BENCH: 通过深度基准测试揭示基于扰动的GNN解释器的实用性

    GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking. (arXiv:2310.01794v1 [cs.LG])

    [http://arxiv.org/abs/2310.01794](http://arxiv.org/abs/2310.01794)

    本研究通过基准测试系统评估了基于扰动的GNN解释性方法，发现帕累托最优方法在噪声存在的情况下表现出卓越效力和稳定性。

    

    已经提出了许多解释性方法来揭示GNN的内部工作方式。尽管所有提出的算法都包含了实证评估，但这些评估的询问方面缺乏多样性。因此，关于GNN解释性的各个方面，如对事实求证推理器的比较分析、它们对不同GNN架构、噪声、非凸损失表面中的随机性、在领域约束条件下的可行性等等，尚未得到正式的研究。受此需求的激发，我们在基于扰动的GNN解释性方法上进行了基准测试研究，旨在系统评估和比较各种解释性技术。在我们的研究的关键发现中，我们确定了在噪声存在的情况下表现出卓越效力和稳定性的帕累托最优方法。然而，我们的研究揭示了所有算法都受到稳定性的影响。

    Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that all algorithms are affected by stabi
    
[^53]: 大型语言模型能够提供对研究论文有用的反馈吗？一项大规模实证分析。

    Can large language models provide useful feedback on research papers? A large-scale empirical analysis. (arXiv:2310.01783v1 [cs.LG])

    [http://arxiv.org/abs/2310.01783](http://arxiv.org/abs/2310.01783)

    这项研究通过大规模实证分析探讨了使用大型语言模型生成科学论文反馈的实用性。通过对GPT-4生成的反馈与人类同行评审的比较，发现大型语言模型在提供科学反馈方面具有潜力。

    

    专家的反馈是严谨研究的基础。然而，学术产出的快速增长和复杂的专业知识挑战了传统的科学反馈机制。越来越难获取高质量的同行评审意见。初级研究人员或来自资源匮乏的环境尤其难以及时获得反馈。随着GPT-4等大型语言模型的突破，使用大型语言模型生成对科学论文的反馈引起了广泛兴趣。然而，LLM生成的反馈的实用性还没有得到系统研究。为了填补这一空白，我们使用GPT-4创建了一个自动化流程，对科学论文的完整PDF提供评论。我们通过两个大规模研究评估了GPT-4反馈的质量。首先，我们在15本Nature类期刊（总共3096篇论文）和ICLR m 上定量比较了GPT-4生成的反馈与人类同行评审的反馈。

    Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR m
    
[^54]: 采用估计的注意力掩码的稀疏线性注意力（SEA）

    SEA: Sparse Linear Attention with Estimated Attention Mask. (arXiv:2310.01777v1 [cs.CL])

    [http://arxiv.org/abs/2310.01777](http://arxiv.org/abs/2310.01777)

    提出了SEA方法，可以通过估计注意力掩码实现线性复杂度的稀疏注意力，解决了transformer处理长序列时注意力操作复杂度高的问题，并保持了可解释性。

    

    近年来，transformer架构在需要对序列元素之间的成对关系建模的任务上取得了重大突破，如自然语言理解任务。然而，由于注意力操作的二次复杂度，transformer在处理长序列时存在困难，因此先前的研究旨在通过稀疏化或线性逼近注意力矩阵来降低复杂度。然而，这些方法无法直接从教师的注意力矩阵中提取知识，并且通常需要完全重新训练。此外，先前的稀疏和线性方法如果不能产生完全二次的注意力矩阵，还可能失去可解释性。为了解决这些挑战，我们提出了SEA：采用估计注意力掩码的稀疏线性注意力方法。SEA通过基于核的线性注意力方法估计注意力矩阵，并创建一个对完整注意力矩阵进行稀疏逼近的方法。

    The transformer architecture has made breakthroughs in recent years on tasks which require modeling pairwise relationships between sequential elements, as is the case in natural language understanding. However, transformers struggle with long sequences due to the quadratic complexity of the attention operation, and previous research has aimed to lower the complexity by sparsifying or linearly approximating the attention matrix. Yet, these approaches cannot straightforwardly distill knowledge from a teacher's attention matrix, and often require complete retraining from scratch. Furthermore, previous sparse and linear approaches may also lose interpretability if they do not produce full quadratic attention matrices. To address these challenges, we propose SEA: Sparse linear attention with an Estimated Attention mask. SEA estimates the attention matrix with linear complexity via kernel-based linear attention, then creates a sparse approximation to the full attention matrix with a top-k se
    
[^55]: 损失平坦性与神经网络中压缩表示的简单联系

    A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])

    [http://arxiv.org/abs/2310.01770](http://arxiv.org/abs/2310.01770)

    该论文研究了深度神经网络中损失平坦性和神经表示压缩之间的关系，通过简单的数学关系，证明了损失平坦性与神经表示的压缩相关。

    

    对深度神经网络的泛化能力进行研究的方法有很多种，包括至少两种不同的方法：一种基于参数空间中损失景观的形状，另一种基于特征空间中表示流形的结构（即单位活动的空间）。这两种方法相关但很少同时进行研究和明确关联。在这里，我们提出了一种简单的分析方法来建立这种联系。我们展示了在深度神经网络学习的最后阶段，神经表示流形的体积压缩与正在进行的参数优化所探索的最小值周围的损失平坦性相关。我们证明了这可以由一个相对简单的数学关系来预测：损失平坦性意味着神经表示的压缩。我们的结果与\citet{ma_linear_2021}的先前研究密切相关，该研究展示了平坦性（即小特征值）与表示流形的体积压缩之间的关系。

    Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of t
    
[^56]: 过参数化如何减缓矩阵感知中的梯度下降：对称性和初始化的问题。

    How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization. (arXiv:2310.01769v1 [cs.LG])

    [http://arxiv.org/abs/2310.01769](http://arxiv.org/abs/2310.01769)

    该论文研究了过参数化如何影响矩阵感知问题中梯度下降的收敛行为，在对称和非对称设置下给出了不同的收敛速度。

    

    本文详细阐述了过参数化如何改变梯度下降在矩阵感知问题中的收敛行为。在对称设置中，通过对称参数化学习未知的半正定矩阵，我们给出了过参数化情况下（$k>r$）随机初始化梯度下降的新型$\Omega (1/T^2)$下界，与精确参数化情况（$k=r$）的收敛速度$\exp (-\Omega (T))$形成鲜明对比。接下来，我们研究了不对称设置，其中$M^* \in \mathbb{R}^{n_1 \times n_2}$是未知矩阵，采用非对称参数化学习。

    This paper rigorously shows how over-parameterization changes the convergence behaviors of gradient descent (GD) for the matrix sensing problem, where the goal is to recover an unknown low-rank ground-truth matrix from near-isotropic linear measurements. First, we consider the symmetric setting with the symmetric parameterization where $M^* \in \mathbb{R}^{n \times n}$ is a positive semi-definite unknown matrix of rank $r \ll n$, and one uses a symmetric parameterization $XX^\top$ to learn $M^*$. Here $X \in \mathbb{R}^{n \times k}$ with $k > r$ is the factor matrix. We give a novel $\Omega (1/T^2)$ lower bound of randomly initialized GD for the over-parameterized case ($k >r$) where $T$ is the number of iterations. This is in stark contrast to the exact-parameterization scenario ($k=r$) where the convergence rate is $\exp (-\Omega (T))$. Next, we study asymmetric setting where $M^* \in \mathbb{R}^{n_1 \times n_2}$ is the unknown matrix of rank $r \ll \min\{n_1,n_2\}$, and one uses an 
    
[^57]: BackDiff：一种用于广义可传递蛋白质背映射的扩散模型

    Backdiff: a diffusion model for generalized transferable protein backmapping. (arXiv:2310.01768v1 [q-bio.QM])

    [http://arxiv.org/abs/2310.01768](http://arxiv.org/abs/2310.01768)

    BackDiff是一种用于蛋白质背映射的新的生成模型，它利用条件分数扩散模型和几何表示，旨在实现在各种粗粒化模型和蛋白质中的广义化和可靠化。

    

    粗粒化（CG）模型在研究蛋白质结构、热力学性质和构象动力学方面起着关键作用。由于粗粒化过程中的信息丢失，在许多蛋白质设计和药物发现应用中需要详细的原子表示进行深入研究时，从CG映射回原子级构型是必不可少的。尽管最近在数据驱动的背映射方法方面取得了进展，但设计一种能够普遍应用于不同CG模型和蛋白质的背映射方法仍然没有解决。在这项工作中，我们提出了BackDiff，一种旨在实现蛋白质背映射问题的泛化和可靠性的新的生成模型。BackDiff利用具有几何表示的条件分数扩散模型。由于不同的CG模型可以包含不同的粗粒化位点，其中包括选定的原子（CG原子）和原子坐标的简单CG辅助函数（CG辅助）

    Coarse-grained (CG) models play a crucial role in the study of protein structures, protein thermodynamic properties, and protein conformation dynamics. Due to the information loss in the coarse-graining process, backmapping from CG to all-atom configurations is essential in many protein design and drug discovery applications when detailed atomic representations are needed for in-depth studies. Despite recent progress in data-driven backmapping approaches, devising a backmapping method that can be universally applied across various CG models and proteins remains unresolved. In this work, we propose BackDiff, a new generative model designed to achieve generalization and reliability in the protein backmapping problem. BackDiff leverages the conditional score-based diffusion model with geometric representations. Since different CG models can contain different coarse-grained sites which include selected atoms (CG atoms) and simple CG auxiliary functions of atomistic coordinates (CG auxiliar
    
[^58]: 探索针对以人为中心的人工智能的反事实对齐损失

    Exploring Counterfactual Alignment Loss towards Human-centered AI. (arXiv:2310.01766v1 [cs.LG])

    [http://arxiv.org/abs/2310.01766](http://arxiv.org/abs/2310.01766)

    该论文提出了一个基于反事实生成的以人为中心的框架，并引入了一种新的损失函数，用于保证反事实生成归因的特征与人类专家对齐。

    

    深度神经网络在监督学习任务中具有令人印象深刻的准确性。然而，它们缺乏透明度，使得人们难以信任它们的结果，特别是在安全-批评领域如医疗保健中。为了解决这个问题，最近的解释引导学习方法提出了将基于梯度的注意力映射与人类专家标注的图像区域对齐的方法，从而获得一个本质上以人为中心的模型。然而，这些方法所基于的注意力映射可能无法因果地归因于模型预测，从而损害了对其对齐的有效性。为了解决这个问题，我们提出了一个基于反事实生成的新型以人为中心的框架。具体而言，我们利用反事实生成的因果归因能力引入了一种新的损失，称为反事实对齐损失（CF-Align）。这个损失保证了分类器由反事实生成归因的特征与人类专家对齐。

    Deep neural networks have demonstrated impressive accuracy in supervised learning tasks. However, their lack of transparency makes it hard for humans to trust their results, especially in safe-critic domains such as healthcare. To address this issue, recent explanation-guided learning approaches proposed to align the gradient-based attention map to image regions annotated by human experts, thereby obtaining an intrinsically human-centered model. However, the attention map these methods are based on may fail to causally attribute the model predictions, thus compromising their validity for alignment. To address this issue, we propose a novel human-centered framework based on counterfactual generation. In particular, we utilize the counterfactual generation's ability for causal attribution to introduce a novel loss called the CounterFactual Alignment (CF-Align) loss. This loss guarantees that the features attributed by the counterfactual generation for the classifier align with the human 
    
[^59]: 数据清洗与机器学习：系统性文献综述

    Data Cleaning and Machine Learning: A Systematic Literature Review. (arXiv:2310.01765v1 [cs.LG])

    [http://arxiv.org/abs/2310.01765](http://arxiv.org/abs/2310.01765)

    本文系统综述了数据清洗与机器学习的关系，并总结了最新的数据清洗方法和ML的应用领域。在这两个领域存在着互相促进的关系，研究人员提出了未来的研究方向和建议。

    

    背景：机器学习（ML）被整合到越来越多的系统中，用于各种应用。由于ML模型的性能高度依赖于其训练数据的质量，因此对于检测和修复数据错误（即数据清洗）的方法越来越受关注。研究人员还在探索如何使用ML进行数据清洗，从而在ML和数据清洗之间构建了双重关系。据我们所知，尚无对这种关系进行全面综述的研究。目标：本文的目标有两个。首先，总结了最新的数据清洗方法，包括ML用于数据清洗和数据清洗用于ML。其次，提出了未来的工作建议。方法：我们对2016年至2022年期间发表的论文进行了系统性文献综述。我们确定了不同类型的数据清洗活动，包括特征清洗、标签清洗、实体匹配、异常值检测、缺失数据填充等。

    Context: Machine Learning (ML) is integrated into a growing number of systems for various applications. Because the performance of an ML model is highly dependent on the quality of the data it has been trained on, there is a growing interest in approaches to detect and repair data errors (i.e., data cleaning). Researchers are also exploring how ML can be used for data cleaning; hence creating a dual relationship between ML and data cleaning. To the best of our knowledge, there is no study that comprehensively reviews this relationship. Objective: This paper's objectives are twofold. First, it aims to summarize the latest approaches for data cleaning for ML and ML for data cleaning. Second, it provides future work recommendations. Method: We conduct a systematic literature review of the papers published between 2016 and 2022 inclusively. We identify different types of data cleaning activities with and for ML: feature cleaning, label cleaning, entity matching, outlier detection, imputati
    
[^60]: 使用Vanilla Score对多模态分布进行采样：基于数据初始化的好处

    Sampling Multimodal Distributions with the Vanilla Score: Benefits of Data-Based Initialization. (arXiv:2310.01762v1 [cs.LG])

    [http://arxiv.org/abs/2310.01762](http://arxiv.org/abs/2310.01762)

    这项研究证明了通过在经验分布上初始化并运行使用早期停止的Langevin扩散的Vanilla Score方法，可以对多模态分布进行采样。

    

    在统计和生成模型方法中，基于得分函数（分布的对数似然的导数）的方法有着悠久的历史和最近的兴趣爆发。在开创性的工作中，Hyvärinen提出了Vanilla Score Matching作为一种通过计算底层真实数据的得分函数估计来学习分布的方法，并建立了该方法与对比分歧和伪似然估计等已有技术之间的联系。现在已经众所周知，Vanilla Score Matching在学习多模态分布时存在显著困难。虽然有各种方式可以克服这个困难，但以下问题仍然没有得到答案 - 是否有一种自然的方法可以仅使用Vanilla Score对多模态分布进行采样？受一系列相关实验性工作的启发，我们证明了使用早期停止的Langevin扩散，以经验分布为初始化，并在...

    There is a long history, as well as a recent explosion of interest, in statistical and generative modeling approaches based on score functions -derivatives of the log-likelihood of a distribution. In seminal works, Hyv\"arinen proposed vanilla score matching as a way to learn distributions from data by computing an estimate of the score function of the underlying ground truth, and established connections between this method and established techniques like Contrastive Divergence and Pseudolikelihood estimation. It is by now well-known that vanilla score matching has significant difficulties learning multimodal distributions. Although there are various ways to overcome this difficulty, the following question has remained unanswered -- is there a natural way to sample multimodal distributions using just the vanilla score? Inspired by a long line of related experimental works, we prove that the Langevin diffusion with early stopping, initialized at the empirical distribution, and run on 
    
[^61]: ReLU激活函数在神经网络嵌入优化中的线性化：最佳日前能量调度

    Linearization of ReLU Activation Function for Neural Network-Embedded Optimization:Optimal Day-Ahead Energy Scheduling. (arXiv:2310.01758v1 [cs.LG])

    [http://arxiv.org/abs/2310.01758](http://arxiv.org/abs/2310.01758)

    本文研究了将非线性激活函数线性化的方法，特别关注修正线性单元（ReLU）函数。并针对神经网络嵌入优化问题提出并比较了四种定制的ReLU激活函数的线性化方法。

    

    神经网络在电力系统领域中得到广泛应用。它们可以用于更好地预测输入信息，并以更高的准确性对系统性能进行建模。在一些应用中，如基于神经网络的微电网日前能量调度中，训练模型的输入特征是在强制限制同一学习模型的输出的优化模型中解决的变量。这将会产生一个嵌入神经网络优化问题；在神经网络中使用非线性激活函数将使这类问题异常困难，甚至无法解决。为了应对这一新兴挑战，本文研究了不同的线性化非线性激活函数的方法，特别关注广泛使用的修正线性单元（ReLU）函数。本文开发、分析和比较了四种适用于ReLU激活函数的线性化方法。

    Neural networks have been widely applied in the power system area. They can be used for better predicting input information and modeling system performance with increased accuracy. In some applications such as battery degradation neural network-based microgrid day-ahead energy scheduling, the input features of the trained learning model are variables to be solved in optimization models that enforce limits on the output of the same learning model. This will create a neural network-embedded optimization problem; the use of nonlinear activation functions in the neural network will make such problems extremely hard to solve if not unsolvable. To address this emerging challenge, this paper investigated different methods for linearizing the nonlinear activation functions with a particular focus on the widely used rectified linear unit (ReLU) function. Four linearization methods tailored for the ReLU activation function are developed, analyzed and compared in this paper. Each method employs a
    
[^62]: 改进的算法用于具有无界损失的对抗性多臂老虎机问题

    Improved Algorithms for Adversarial Bandits with Unbounded Losses. (arXiv:2310.01756v1 [stat.ML])

    [http://arxiv.org/abs/2310.01756](http://arxiv.org/abs/2310.01756)

    改进的算法用于解决对抗性多臂老虎机问题，无需先验知识，实现自适应且无需统一探索的遗憾界限，能够处理任意无界损失，并通过实验证明优于现有算法。

    

    我们考虑具有无界损失的对抗性多臂老虎机问题，其中算法对损失的大小没有先验知识。我们提出了UMAB-NN和UMAB-G两种算法，分别用于非负和一般的无界损失。对于非负无界损失，UMAB-NN实现了第一个自适应且无需统一探索的遗憾界限。在此基础上，我们进一步发展了UMAB-G，可以学习任意无界损失。我们的分析揭示了MAB问题中正负损失之间的不对称性，并提供了额外的见解。我们还通过大量实证评估来配合我们的理论发现，显示出我们的算法始终优于所有处理无界损失的现有算法。

    We consider the Adversarial Multi-Armed Bandits (MAB) problem with unbounded losses, where the algorithms have no prior knowledge on the sizes of the losses. We present UMAB-NN and UMAB-G, two algorithms for non-negative and general unbounded loss respectively. For non-negative unbounded loss, UMAB-NN achieves the first adaptive and scale free regret bound without uniform exploration. Built up on that, we further develop UMAB-G that can learn from arbitrary unbounded loss. Our analysis reveals the asymmetry between positive and negative losses in the MAB problem and provide additional insights. We also accompany our theoretical findings with extensive empirical evaluations, showing that our algorithms consistently out-performs all existing algorithms that handles unbounded losses.
    
[^63]: CausalTime：用于因果发现基准测试的逼真生成时间序列

    CausalTime: Realistically Generated Time-series for Benchmarking of Causal Discovery. (arXiv:2310.01753v1 [cs.LG])

    [http://arxiv.org/abs/2310.01753](http://arxiv.org/abs/2310.01753)

    CausalTime引入了一种生成逼真时间序列的流程，能够生成与真实数据极其相似且带有基准因果图的时间序列，用于定量性能评估。该流程利用深度神经网络和正态流捕捉逼真的动态，提取假设的因果图，并生成适合算法评估的多样化时间序列。

    

    时间序列因果发现（TSCD）是机器学习中的一个基本问题。然而，现有的合成数据集无法正确评估或预测算法在真实数据上的性能。本研究引入了CausalTime流程，用于生成高度类似真实数据且带有基准因果图的时间序列，以进行定量性能评估。该流程从特定场景的真实观测数据开始，生成相匹配的基准数据集。首先，我们利用深度神经网络和正态流来准确捕捉逼真的动态。其次，通过对神经网络进行重要性分析或利用先前知识，提取假设的因果图。第三，在因果模型中将因果项、残差项和噪声项拆分，得到基本真实的因果图。最后，利用拟合的网络和派生的因果图，我们生成适合算法评估的多样化时间序列。

    Time-series causal discovery (TSCD) is a fundamental problem of machine learning. However, existing synthetic datasets cannot properly evaluate or predict the algorithms' performance on real data. This study introduces the CausalTime pipeline to generate time-series that highly resemble the real data and with ground truth causal graphs for quantitative performance evaluation. The pipeline starts from real observations in a specific scenario and produces a matching benchmark dataset. Firstly, we harness deep neural networks along with normalizing flow to accurately capture realistic dynamics. Secondly, we extract hypothesized causal graphs by performing importance analysis on the neural network or leveraging prior knowledge. Thirdly, we derive the ground truth causal graphs by splitting the causal model into causal term, residual term, and noise term. Lastly, using the fitted network and the derived causal graph, we generate corresponding versatile time-series proper for algorithm asses
    
[^64]: 5G网络切片：多个机器学习分类器的分析

    5G Network Slicing: Analysis of Multiple Machine Learning Classifiers. (arXiv:2310.01747v1 [cs.CR])

    [http://arxiv.org/abs/2310.01747](http://arxiv.org/abs/2310.01747)

    本研究分析了多个机器学习分类器对于5G网络切片的准确性和精度。研究发现，逻辑回归模型、线性判别模型、k最近邻模型、决策树模型、随机森林模型、SVC BernoulliNB模型和GaussianNB模型都能够有效地检测网络切片，并为动态调整网络切片来满足不同服务需求提供了有效的方法。

    

    将一个物理5G通信基础设施分割成几个具有不同特征的虚拟网络切片，如带宽、延迟、可靠性、安全性和服务质量，这就是5G网络切片。每个切片是一个独立的逻辑网络，满足特定服务或用例的要求，如虚拟现实、游戏、自动驾驶车辆或工业自动化。网络切片可以动态调整以满足服务的变化需求，从而以更具成本效益和高效的方式在共享基础设施上提供多样化的服务和应用。本文评估了各种机器学习技术，包括逻辑回归模型、线性判别模型、k最近邻模型、决策树模型、随机森林模型、SVC BernoulliNB模型和GaussianNB模型，以研究每个模型在检测网络切片上的准确性和精度。该报告还提供了一个概述...

    The division of one physical 5G communications infrastructure into several virtual network slices with distinct characteristics such as bandwidth, latency, reliability, security, and service quality is known as 5G network slicing. Each slice is a separate logical network that meets the requirements of specific services or use cases, such as virtual reality, gaming, autonomous vehicles, or industrial automation. The network slice can be adjusted dynamically to meet the changing demands of the service, resulting in a more cost-effective and efficient approach to delivering diverse services and applications over a shared infrastructure. This paper assesses various machine learning techniques, including the logistic regression model, linear discriminant model, k-nearest neighbor's model, decision tree model, random forest model, SVC BernoulliNB model, and GaussianNB model, to investigate the accuracy and precision of each model on detecting network slices. The report also gives an overview
    
[^65]: 具有统计保证的随机维度降低

    Randomized Dimension Reduction with Statistical Guarantees. (arXiv:2310.01739v1 [cs.LG])

    [http://arxiv.org/abs/2310.01739](http://arxiv.org/abs/2310.01739)

    本论文提出了一种基于“矩阵抽样”的快速随机低秩分解算法，用于大型矩阵的维度降低，从而提高计算效率和数据利用率。

    

    大型模型和庞大数据是现代算法取得前所未有成功的关键驱动力，尤其是在科学计算和机器学习领域。然而，不断增长的维度和模型复杂性以及数据预处理的工作量也给计算和数据聚合带来了巨大的成本。随着摩尔定律放缓导致计算成本从硬件层面降低的减速，快速大型经典例程的启发式算法和利用有限数据的高效算法对于推动算法潜力的极限变得越来越不可或缺。本论文探索了一些快速执行和高效利用数据的算法。从计算效率的角度出发，我们设计和分析了基于“矩阵抽样”的大型矩阵的快速随机低秩分解算法，该算法可以被视为一种维度降低策略。

    Large models and enormous data are essential driving forces of the unprecedented successes achieved by modern algorithms, especially in scientific computing and machine learning. Nevertheless, the growing dimensionality and model complexity, as well as the non-negligible workload of data pre-processing, also bring formidable costs to such successes in both computation and data aggregation. As the deceleration of Moore's Law slackens the cost reduction of computation from the hardware level, fast heuristics for expensive classical routines and efficient algorithms for exploiting limited data are increasingly indispensable for pushing the limit of algorithm potency. This thesis explores some of such algorithms for fast execution and efficient data utilization.  From the computational efficiency perspective, we design and analyze fast randomized low-rank decomposition algorithms for large matrices based on "matrix sketching", which can be regarded as a dimension reduction strategy in the 
    
[^66]: 融合模仿学习和强化学习以实现鲁棒策略改进

    Blending Imitation and Reinforcement Learning for Robust Policy Improvement. (arXiv:2310.01737v1 [cs.LG])

    [http://arxiv.org/abs/2310.01737](http://arxiv.org/abs/2310.01737)

    本文提出了一种融合模仿学习和强化学习的方法，根据在线评估结果交替使用二者，以提高样本效率和学习效果。

    

    虽然强化学习在性能上表现出色，但其样本复杂度仍然是一个重大障碍，限制了其在各个领域的广泛应用。模仿学习利用神经网络优化样本效率，但通常受到所使用的专家示范的质量限制。本文介绍了一种融合模仿学习和强化学习的方法，该方法根据在线评估结果交替使用二者，有效地提高了学习效率。这种算法能够从多种黑盒专家示范中学习和改进。

    While reinforcement learning (RL) has shown promising performance, its sample complexity continues to be a substantial hurdle, restricting its broader application across a variety of domains. Imitation learning (IL) utilizes oracles to improve sample efficiency, yet it is often constrained by the quality of the oracles deployed. which actively interleaves between IL and RL based on an online estimate of their performance. RPI draws on the strengths of IL, using oracle queries to facilitate exploration, an aspect that is notably challenging in sparse-reward RL, particularly during the early stages of learning. As learning unfolds, RPI gradually transitions to RL, effectively treating the learned policy as an improved oracle. This algorithm is capable of learning from and improving upon a diverse set of black-box oracles. Integral to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient (RPG), both of which reason over whether to perform state-wise imitation from the o
    
[^67]: Nugget: 文本的神经聚合嵌入

    Nugget: Neural Agglomerative Embeddings of Text. (arXiv:2310.01732v1 [cs.CL])

    [http://arxiv.org/abs/2310.01732](http://arxiv.org/abs/2310.01732)

    Nugget是一种基于动态选择的输入令牌子集的文本嵌入方法，通过自编码和机器翻译等任务，将语言分割为有意义的单元，优于相关方法，在语义比较任务中表现出色，并且允许扩展语言模型的上下文窗口。

    

    在现代语言理解中，嵌入文本序列是一个广泛需求。现有方法主要侧重于恒定大小的表示。这是有问题的，因为文本中包含的信息量通常随输入的长度而变化。我们提出了一种称为Nugget的解决方案，它将语言编码为基于动态选择的输入令牌子集的表示。通过自编码和机器翻译等任务，学习这些nuggets，并直观地将语言分割成有意义的单元。我们展示了Nugget在涉及语义比较的任务中优于相关方法。最后，我们证明了这些紧凑单元允许扩展语言模型(LM)的上下文窗口，从而提出了新的LM可能会对更大量的内容进行条件处理。

    Embedding text sequences is a widespread requirement in modern language understanding. Existing approaches focus largely on constant-size representations. This is problematic, as the amount of information contained in text often varies with the length of the input. We propose a solution called Nugget, which encodes language into a representation based on a dynamically selected subset of input tokens. These nuggets are learned through tasks like autoencoding and machine translation, and intuitively segment language into meaningful units. We demonstrate Nugget outperforms related approaches in tasks involving semantic comparison. Finally, we illustrate these compact units allow for expanding the contextual window of a language model (LM), suggesting new future LMs that can condition on significantly larger amounts of content.
    
[^68]: Time-LLM: 通过重新编程大型语言模型进行时间序列预测

    Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. (arXiv:2310.01728v1 [cs.LG])

    [http://arxiv.org/abs/2310.01728](http://arxiv.org/abs/2310.01728)

    这项工作介绍了Time-LLM，一个重新编程的框架，通过重新利用大型语言模型，可以进行一般的时间序列预测。

    

    时间序列预测在许多实际动态系统中具有重要意义并得到了广泛研究。不同于自然语言处理（NLP）和计算机视觉（CV），在这些领域，一个单一的大型模型可以处理多个任务，而时间序列预测的模型通常是专门化的，需要为不同的任务和应用设计不同的模型。虽然在NLP和CV领域中，预训练的基础模型取得了令人瞩目的进展，但是在时间序列领域的发展受到数据稀疏性的限制。最近的研究表明，大型语言模型（LLMs）在复杂的序列标记中具有强大的模式识别和推理能力。然而，有效地将时间序列数据和自然语言的模态进行对齐以利用这些能力仍然具有挑战性。在这项工作中，我们提出了Time-LLM，这是一个重新编程的框架，可以重用LLMs来进行一般的时间序列预测，同时保持骨干语言模型的完整性。

    Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by 
    
[^69]: 用于无测试故障定位的大语言模型

    Large Language Models for Test-Free Fault Localization. (arXiv:2310.01726v1 [cs.SE])

    [http://arxiv.org/abs/2310.01726](http://arxiv.org/abs/2310.01726)

    该论文提出了一种基于大语言模型的故障定位方法，称为LLMAO，具有无需测试覆盖信息就能定位有问题的代码行的能力。

    

    故障定位（FL）旨在自动定位有问题的代码行，这是许多手动和自动调试任务的关键第一步。以前的FL技术假设提供输入测试，并且通常需要进行广泛的程序分析、程序插桩或数据预处理。以往的深度学习自动程序修复技术在小数据集上学习困难，并在实际程序上产生有限的结果。受到大语言模型（LLMs）在很少示例上适应新任务的能力的启发，我们研究了将LLMs应用于行级故障定位的可行性。具体地，我们建议通过在LLMs学习的表示之上微调一小组双向适配器层来克服LLMs的自左向右特性，从而产生LLMAO，这是第一个基于语言模型的故障定位方法，它可以在没有任何测试覆盖信息的情况下定位有问题的代码行。我们使用3.5亿、60亿和160亿段来微调LLMs。

    Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion para
    
[^70]: PrACTiS: Perceiver-Attentional Copulas for Time Series（时间序列的感知-注意力联合分布模型）

    PrACTiS: Perceiver-Attentional Copulas for Time Series. (arXiv:2310.01720v1 [cs.LG])

    [http://arxiv.org/abs/2310.01720](http://arxiv.org/abs/2310.01720)

    PrACTiS combines perceiver architecture with copula structure to enhance time-series forecasting and reduce computational demands. It incorporates midpoint inference and local attention mechanisms to effectively capture dependencies within imputed samples. The copula-based attention and output variance testing mechanism capture the joint distribution of missing data and mitigate error propagation during prediction.

    

    融合联合分布结构的Transformer模型在时间序列预测中表现出色。然而，它们过于依赖自注意力机制，需要大量计算资源，因此限制了它们在各种任务中的实际应用。本研究提出了一种将感知器架构与联合分布结构相结合的模型，以增强时间序列预测能力。通过利用感知器作为编码器，我们能够高效地将复杂的高维多模态数据转换为紧凑的潜空间，从而显著降低计算需求。为了进一步降低复杂度，我们引入了中点推断和局部注意力机制，使模型能够有效地捕捉插补样本中的依赖关系。随后，我们采用基于联合分布的注意力和输出方差测试机制来捕捉缺失数据的联合分布，同时减少预测过程中的误差传播。

    Transformers incorporating copula structures have demonstrated remarkable performance in time series prediction. However, their heavy reliance on self-attention mechanisms demands substantial computational resources, thus limiting their practical utility across a wide range of tasks. In this work, we present a model that combines the perceiver architecture with a copula structure to enhance time-series forecasting. By leveraging the perceiver as the encoder, we efficiently transform complex, high-dimensional, multimodal data into a compact latent space, thereby significantly reducing computational demands. To further reduce complexity, we introduce midpoint inference and local attention mechanisms, enabling the model to capture dependencies within imputed samples effectively. Subsequently, we deploy the copula-based attention and output variance testing mechanism to capture the joint distribution of missing data, while simultaneously mitigating error propagation during prediction. Our 
    
[^71]: 无监督句法分析的集成蒸馏

    Ensemble Distillation for Unsupervised Constituency Parsing. (arXiv:2310.01717v1 [cs.CL])

    [http://arxiv.org/abs/2310.01717](http://arxiv.org/abs/2310.01717)

    本论文提出了一种集成蒸馏的方法来提高无监督句法解析的性能，并且通过蒸馏将集成知识转移到一个学生模型中，解决了常见的多教师蒸馏方法中的过度平滑问题。

    

    我们研究了无监督句法分析任务，该任务将句子的词和短语组织成一个层次结构，而不使用语言学注释的数据。我们观察到现有的无监督解析器捕捉到了解析结构的不同方面，可以利用这些来提高无监督分析的性能。为此，我们提出了“树平均”的概念，基于此我们进一步提出了一种新的无监督解析的集成方法。为了提高推理效率，我们进一步将集成知识蒸馏到一个学生模型中；这种集成-蒸馏的过程是缓解常见的多教师蒸馏方法中存在的过度平滑问题的有效方法。实验证明我们的方法超过了所有先前的方法，始终表现出其在不同集成组件和领域转移条件下的有效性和稳健性。

    We investigate the unsupervised constituency parsing task, which organizes words and phrases of a sentence into a hierarchical structure without using linguistically annotated data. We observe that existing unsupervised parsers capture differing aspects of parsing structures, which can be leveraged to enhance unsupervised parsing performance. To this end, we propose a notion of "tree averaging," based on which we further propose a novel ensemble method for unsupervised parsing. To improve inference efficiency, we further distill the ensemble knowledge into a student model; such an ensemble-then-distill process is an effective approach to mitigate the over-smoothing problem existing in common multi-teacher distilling methods. Experiments show that our method surpasses all previous approaches, consistently demonstrating its effectiveness and robustness across various runs, with different ensemble components, and under domain-shift conditions.
    
[^72]: 大型语言模型作为类比推理器

    Large Language Models as Analogical Reasoners. (arXiv:2310.01714v1 [cs.LG])

    [http://arxiv.org/abs/2310.01714](http://arxiv.org/abs/2310.01714)

    本研究提出了一种新的提示方法，称为类比提示，用于自动引导大型语言模型的推理过程。通过在上下文中自动生成相关实例或知识，该方法在多种推理任务中表现出优异的性能。

    

    语言模型的思维链（CoT）提示在推理任务中展现出令人印象深刻的性能，但通常需要有标记的推理过程示例。在这项工作中，我们引入了一种新的提示方法，称为类比提示，旨在自动引导大型语言模型的推理过程。受类比推理的启发，类比推理是一种认知过程，人类从相关的过去经验中获取知识来解决新问题。我们的方法促使语言模型自动生成上下文中的相关实例或知识，然后解决给定的问题，具有以下几个优点：它省去了标记或检索实例的需求，提供了普适性和便利性；它还可以根据每个问题定制生成的示例和知识，提供了适应性。实验结果表明，我们的方法在各种推理任务中优于0-shot CoT和手动few-shot CoT，包括数学问题求解。

    Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, Analogical Prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving i
    
[^73]: 丢弃模式的生成自编码器

    Generative Autoencoding of Dropout Patterns. (arXiv:2310.01712v1 [cs.LG])

    [http://arxiv.org/abs/2310.01712](http://arxiv.org/abs/2310.01712)

    本论文提出了一种称为解读自编码器的生成模型，通过为训练数据集中的每个数据点分配独特的随机丢弃模式来进行训练，只依靠重构误差来提供更稳定的训练性能，并在CIFAR-10数据集上展示了与DCGAN相媲美的采样质量。

    

    我们提出了一种称为解读自编码器的生成模型。在这个模型中，我们为训练数据集中的每个数据点分配一个唯一的随机丢弃模式，然后使用这个模式作为被编码的信息来训练自编码器来重构相应的数据点。由于解读自编码器的训练仅依赖于重构误差，所以相比其他生成模型，它具有更稳定的训练性能。尽管它很简单，但解读自编码器在CIFAR-10数据集上展现出了与DCGAN相媲美的采样质量。

    We propose a generative model termed Deciphering Autoencoders. In this model, we assign a unique random dropout pattern to each data point in the training dataset and then train an autoencoder to reconstruct the corresponding data point using this pattern as information to be encoded. Since the training of Deciphering Autoencoders relies solely on reconstruction error, it offers more stable training than other generative models. Despite its simplicity, Deciphering Autoencoders show comparable sampling quality to DCGAN on the CIFAR-10 dataset.
    
[^74]: 关于基于模型和无模型强化学习的表示复杂性的研究

    On Representation Complexity of Model-based and Model-free Reinforcement Learning. (arXiv:2310.01706v1 [cs.LG])

    [http://arxiv.org/abs/2310.01706](http://arxiv.org/abs/2310.01706)

    本研究在电路复杂度的角度探讨了基于模型和无模型强化学习的表示复杂性。理论上证明了某些MDP可以用恒定深度电路表示转移和奖励函数，但最优$Q$-函数的电路复杂度指数级增加。我们的理论揭示了为什么基于模型的算法通常比无模型的算法具有更好的样本复杂性。

    

    我们在电路复杂度的背景下研究了基于模型和无模型的强化学习的表示复杂性。我们在理论上证明，存在一类广泛的马尔可夫决策过程（MDP），它们的转移和奖励函数可以用具有多项式大小的恒定深度电路表示，而最优的$Q$-函数在恒定深度电路中遭受指数级电路复杂度。通过关注逼近误差并建立到复杂性理论的联系，我们的理论从新的表示复杂性角度为什么基于模型的算法通常比无模型的算法具有更好的样本复杂性提供了独特的见解：在某些情况下，环境的真实规则（模型）易于表示，而其他数量，如$Q$-函数，似乎很复杂。我们通过比较转移核函数、奖励函数和最优$Q$-函数的逼近误差来经验性地验证我们的理论。

    We study the representation complexity of model-based and model-free reinforcement learning (RL) in the context of circuit complexity. We prove theoretically that there exists a broad class of MDPs such that their underlying transition and reward functions can be represented by constant depth circuits with polynomial size, while the optimal $Q$-function suffers an exponential circuit complexity in constant-depth circuits. By drawing attention to the approximation errors and building connections to complexity theory, our theory provides unique insights into why model-based algorithms usually enjoy better sample complexity than model-free algorithms from a novel representation complexity perspective: in some cases, the ground-truth rule (model) of the environment is simple to represent, while other quantities, such as $Q$-function, appear complex. We empirically corroborate our theory by comparing the approximation error of the transition kernel, reward function, and optimal $Q$-function
    
[^75]: Transformers是高效的分层化化学图学习模型

    Transformers are efficient hierarchical chemical graph learners. (arXiv:2310.01704v1 [cs.LG])

    [http://arxiv.org/abs/2310.01704](http://arxiv.org/abs/2310.01704)

    本论文介绍了SubFormer，这是一个图转换器，通过在子图上进行消息传递机制来降低标记数量并增强学习长程交互。在化学结构的分子属性预测基准上，SubFormer在计算成本的一小部分下与最先进的图转换器相竞争，并且能够以几分钟的时间进行训练。注意权重的解释显示SubFormer展现出有限的关注权重。

    

    Transformers，从自然语言处理中改编而来，正在成为图表示学习的主要方法。当代的图转换器通常将节点或边视为独立的标记。这种方法导致即使对于中等规模的图形也存在计算挑战，因为自我注意复杂度随标记数量的平方级增长。在本文中，我们引入了SubFormer，这是一个通过消息传递机制在子图上操作的图转换器。这种方法减少了标记数量，并增强了学习长程交互。我们在化学结构上的分子属性预测基准中展示了SubFormer，并展示了它在计算成本的一小部分下与最先进的图转换器相竞争，使用消费级显卡进行训练的时间约为几分钟。我们使用化学结构解释了注意权重。我们表明SubFormer表现出有限的关注权重。

    Transformers, adapted from natural language processing, are emerging as a leading approach for graph representation learning. Contemporary graph transformers often treat nodes or edges as separate tokens. This approach leads to computational challenges for even moderately-sized graphs due to the quadratic scaling of self-attention complexity with token count. In this paper, we introduce SubFormer, a graph transformer that operates on subgraphs that aggregate information by a message-passing mechanism. This approach reduces the number of tokens and enhances learning long-range interactions. We demonstrate SubFormer on benchmarks for predicting molecular properties from chemical structures and show that it is competitive with state-of-the-art graph transformers at a fraction of the computational cost, with training times on the order of minutes on a consumer-grade graphics card. We interpret the attention weights in terms of chemical structures. We show that SubFormer exhibits limited ov
    
[^76]: 通过近似对角化提高长序列状态空间模型的鲁棒性

    Robustifying State-space Models for Long Sequences via Approximate Diagonalization. (arXiv:2310.01698v1 [cs.LG])

    [http://arxiv.org/abs/2310.01698](http://arxiv.org/abs/2310.01698)

    本文提出了一种用于处理机器学习中不适定对角化问题的通用解决方案，通过引入基于拟谱理论的“扰动然后对角化”（PTD）方法，改善了长序列状态空间模型的鲁棒性。

    

    最近，状态空间模型（SSM）作为学习长期序列任务的框架已经出现。一个例子是结构化状态空间序列（S4）层，它使用HiPPO初始化框架的对角加低秩结构。然而，S4层的复杂结构带来了挑战；为了应对这些挑战，模型如S4D和S5考虑了纯对角结构。这个选择简化了实现，提高了计算效率，并允许信道通信。然而，对HiPPO框架进行对角化本身就是一个不适定问题。在本文中，我们提出了一个通用的解决方案，用于处理机器学习中的这类不适定对角化问题。我们介绍了一种通用的、后向稳定的“扰动然后对角化”（PTD）方法，它基于非正常算子的拟谱理论，并可以被解释为非正常矩阵的近似对角化。

    State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable "perturb-then-diagonalize" (PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defini
    
[^77]: DANI:快速的、具有保持拓扑结构属性的扩散感知网络推断

    DANI: Fast Diffusion Aware Network Inference with Preserving Topological Structure Property. (arXiv:2310.01696v1 [cs.SI])

    [http://arxiv.org/abs/2310.01696](http://arxiv.org/abs/2310.01696)

    DANI是一种快速的、具有保持拓扑结构属性的扩散感知网络推断方法。

    

    近年来，社交网络的快速增长和数据访问限制导致了获取完整拓扑结构的困难。然而，这些网络上的扩散信息是可用的，并且已经提出了许多算法来利用这些信息推断潜在的网络。先前提出的算法只关注推断更多的链接，忽视了保持潜在社交网络关键拓扑特征的重要性。在本文中，我们提出了一种名为DANI的新方法，通过保持结构特性来推断潜在网络。它基于从时间序列级联中派生的马尔可夫转移矩阵，以及可以从结构角度观察级联行为的节点之间的相似性。此外，所提出的方法具有线性时间复杂度（与节点数量、级联数量和级联平均长度的平方线性增长），

    The fast growth of social networks and their data access limitations in recent years has led to increasing difficulty in obtaining the complete topology of these networks. However, diffusion information over these networks is available, and many algorithms have been proposed to infer the underlying networks using this information. The previously proposed algorithms only focus on inferring more links and ignore preserving the critical topological characteristics of the underlying social networks. In this paper, we propose a novel method called DANI to infer the underlying network while preserving its structural properties. It is based on the Markov transition matrix derived from time series cascades, as well as the node-node similarity that can be observed in the cascade behavior from a structural point of view. In addition, the presented method has linear time complexity (increases linearly with the number of nodes, number of cascades, and square of the average length of cascades), and
    
[^78]: 使用级联扩散模型进行热带气旋预测

    Forecasting Tropical Cyclones with Cascaded Diffusion Models. (arXiv:2310.01690v1 [physics.ao-ph])

    [http://arxiv.org/abs/2310.01690](http://arxiv.org/abs/2310.01690)

    本研究利用级联扩散模型预测热带气旋轨迹和降水模式，通过整合多源数据实现准确的预测，对高度脆弱地区具有重要意义。

    

    随着气候变化，飓风变得更加强烈，基于人工智能模型的预测方法比基于数学模型的传统方法更加经济实惠和易于获取。本研究利用扩散模型通过整合卫星成像、遥感和大气数据，采用级联方法进行飓风轨迹和降水模式的预测，训练数据集包括来自六个主要盆地的51个飓风。实验证明，级联模型的最终预测在36小时内显示准确的预测结果，所有三项任务的结构相似性指数（SSIM）和峰值信噪比（PSNR）的值都超过了0.5和20dB。本研究还强调了扩散模型等人工智能方法在高性能需求（如飓风预测）方面的高效性和计算经济性，使其成为高度脆弱地区的理想选择。

    As cyclones become more intense due to climate change, the rise of AI-based modelling provides a more affordable and accessible approach compared to traditional methods based on mathematical models. This work leverages diffusion models to forecast cyclone trajectories and precipitation patterns by integrating satellite imaging, remote sensing, and atmospheric data, employing a cascaded approach that incorporates forecasting, super-resolution, and precipitation modelling, with training on a dataset of 51 cyclones from six major basins. Experiments demonstrate that the final forecasts from the cascaded models show accurate predictions up to a 36-hour rollout, with SSIM and PSNR values exceeding 0.5 and 20 dB, respectively, for all three tasks. This work also highlights the promising efficiency of AI methods such as diffusion models for high-performance needs, such as cyclone forecasting, while remaining computationally affordable, making them ideal for highly vulnerable regions with crit
    
[^79]: 从稳定到混沌：在二次回归中分析梯度下降动力学

    From Stability to Chaos: Analyzing Gradient Descent Dynamics in Quadratic Regression. (arXiv:2310.01687v1 [cs.LG])

    [http://arxiv.org/abs/2310.01687](http://arxiv.org/abs/2310.01687)

    本文通过对二次回归模型中梯度下降的动力学进行全面研究，发现动力学可以用一个特定的立方映射来概括，并详细划分了五个训练阶段。同时，通过实验也证明了这些阶段的推广性能。

    

    我们在二次回归模型的背景下，使用大阶恒定步长对梯度下降的动力学进行了全面的研究。在这个框架下，我们发现动力学可以被一个特定的立方映射所概括，自然地由步长参数化。通过对步长参数进行细粒度的分叉分析，我们描述了五个不同的训练阶段：（1）单调、（2）抛物线、（3）周期性、（4）混沌和（5）发散，精确地划定了每个阶段的边界。作为示例，我们提供了涉及相位恢复和使用二次激活函数和恒定外层的两层神经网络的例子，利用正交训练数据。我们的模拟表明，这五个阶段也在一般的非正交数据中显现。我们还在各个非单调（非发散）阶段进行了经验性的推广性能研究。特别地，我们研究了在不同阶段训练时的推广性能。

    We conduct a comprehensive investigation into the dynamics of gradient descent using large-order constant step-sizes in the context of quadratic regression models. Within this framework, we reveal that the dynamics can be encapsulated by a specific cubic map, naturally parameterized by the step-size. Through a fine-grained bifurcation analysis concerning the step-size parameter, we delineate five distinct training phases: (1) monotonic, (2) catapult, (3) periodic, (4) chaotic, and (5) divergent, precisely demarcating the boundaries of each phase. As illustrations, we provide examples involving phase retrieval and two-layer neural networks employing quadratic activation functions and constant outer-layers, utilizing orthogonal training data. Our simulations indicate that these five phases also manifest with generic non-orthogonal data. We also empirically investigate the generalization performance when training in the various non-monotonic (and non-divergent) phases. In particular, we o
    
[^80]: 机器学习在医学影像中的可解释性框架

    A Framework for Interpretability in Machine Learning for Medical Imaging. (arXiv:2310.01685v1 [cs.LG])

    [http://arxiv.org/abs/2310.01685](http://arxiv.org/abs/2310.01685)

    本文提出了一个机器学习在医学影像中的可解释性框架，明确了解释性的目标和要素，以指导方法设计并改进实际应用。

    

    机器学习在医学影像中的可解释性是一个重要的研究方向。然而，对于可解释性的定义存在一种普遍的模糊感。为什么需要在医学影像中的机器学习中解释性？当需要解释性时，实际上追求的目标是什么？为了回答这些问题，我们确定了在医学影像中的机器学习可解释性的目标和要素需要形式化。通过对医学图像分析和机器学习的交叉点中常见的实际任务和目标进行推理，我们确定了四个核心要素：定位、视觉可识别性、物理归因和透明度。总的来说，本文在医学影像的背景下系统化了可解释性的需求，我们的实践观点澄清了具体的医学影像机器学习可解释性目标和考虑因素，以指导方法设计并改进实际应用。我们的目标是为模型设计提供实用和教学信息。

    Interpretability for machine learning models in medical imaging (MLMI) is an important direction of research. However, there is a general sense of murkiness in what interpretability means. Why does the need for interpretability in MLMI arise? What goals does one actually seek to address when interpretability is needed? To answer these questions, we identify a need to formalize the goals and elements of interpretability in MLMI. By reasoning about real-world tasks and goals common in both medical image analysis and its intersection with machine learning, we identify four core elements of interpretability: localization, visual recognizability, physical attribution, and transparency. Overall, this paper formalizes interpretability needs in the context of medical imaging, and our applied perspective clarifies concrete MLMI-specific goals and considerations in order to guide method design and improve real-world usage. Our goal is to provide practical and didactic information for model desig
    
[^81]: 设计以用户为中心的行为干预来预防血糖异常，并提供新颖的反事实解释

    Designing User-Centric Behavioral Interventions to Prevent Dysglycemia with Novel Counterfactual Explanations. (arXiv:2310.01684v1 [cs.AI])

    [http://arxiv.org/abs/2310.01684](http://arxiv.org/abs/2310.01684)

    这项研究设计了一种以用户为中心的行为干预方法，通过提供新颖的反事实解释来预防血糖异常，有望对社会产生重要影响。

    

    通过生活方式行为维持正常血糖水平对于保持健康和预防疾病至关重要。频繁接触血糖异常（即高血糖和低血糖等异常事件）会导致慢性并发症，包括糖尿病、肾脏疾病及需透析治疗、心肌梗死、中风、截肢和死亡。因此，能够预测血糖异常并向用户提供行动反馈以改变饮食、运动和药物治疗来预防异常血糖事件的工具可能具有重要的社会影响。反事实解释可以通过生成类似于原始输入但导致不同预测结果的假设实例，提供模型为何对特定预测的见解。因此，反事实解释可以被视为设计AI驱动的健康干预来预防不良健康结果（如血糖异常）的一种手段。在本文中，我们设计了GlyCoa...

    Maintaining normal blood glucose levels through lifestyle behaviors is central to maintaining health and preventing disease. Frequent exposure to dysglycemia (i.e., abnormal glucose events such as hyperlycemia and hypoglycemia) leads to chronic complications including diabetes, kidney disease and need for dialysis, myocardial infarction, stroke, amputation, and death. Therefore, a tool capable of predicting dysglycemia and offering users actionable feedback about how to make changes in their diet, exercise, and medication to prevent abnormal glycemic events could have significant societal impacts. Counterfactual explanations can provide insights into why a model made a particular prediction by generating hypothetical instances that are similar to the original input but lead to a different prediction outcome. Therefore, counterfactuals can be viewed as a means to design AI-driven health interventions to prevent adverse health outcomes such as dysglycemia. In this paper, we design GlyCoa
    
[^82]: 深度神经网络中的可交换宽度和深度缩放

    Commutative Width and Depth Scaling in Deep Neural Networks. (arXiv:2310.01683v1 [stat.ML])

    [http://arxiv.org/abs/2310.01683](http://arxiv.org/abs/2310.01683)

    该论文研究了深度神经网络中宽度和深度的可交换缩放。通过分析神经函数的行为，并确定了宽度和深度趋近于无穷大时的可交换性条件，并研究了神经协方差核的可交换性。研究结果表明，在具有跳跃连接的深度神经网络中，当分支适当缩放以避免爆炸行为时，将宽度和深度趋近于无穷大会得到相同的协方差结构。

    

    该论文是关于神经网络中宽度和深度的可交换缩放的系列论文的第二篇。我们的目标是理解神经函数（依赖于神经网络模型的函数）在宽度和深度趋近于无穷大时的行为，并最终确定在哪些条件下可交换性成立，即无论如何取宽度和深度的极限，神经函数都趋向于相同的极限。在本文中，我们正式引入和定义了可交换性框架，并讨论了它对神经网络设计和缩放的影响。我们研究了神经协方差核的可交换性，该核反映了网络层对数据的分离情况。我们的研究结果扩展了之前在[55]中建立的结果，通过展示在具有跳跃连接的深度神经网络中，当分支适当缩放以避免爆炸行为时，将宽度和深度趋近于无穷大会得到相同的协方差结构。

    This paper is the second in the series Commutative Scaling of Width and Depth (WD) about commutativity of infinite width and depth limits in deep neural networks. Our aim is to understand the behaviour of neural functions (functions that depend on a neural network model) as width and depth go to infinity (in some sense), and eventually identify settings under which commutativity holds, i.e. the neural function tends to the same limit no matter how width and depth limits are taken. In this paper, we formally introduce and define the commutativity framework, and discuss its implications on neural network design and scaling. We study commutativity for the neural covariance kernel which reflects how network layers separate data. Our findings extend previous results established in [55] by showing that taking the width and depth to infinity in a deep neural network with skip connections, when branches are suitably scaled to avoid exploding behaviour, result in the same covariance structure n
    
[^83]: 用概率保护特征估计和实现传统公平度量方法

    Estimating and Implementing Conventional Fairness Metrics With Probabilistic Protected Features. (arXiv:2310.01679v1 [cs.LG])

    [http://arxiv.org/abs/2310.01679](http://arxiv.org/abs/2310.01679)

    本文提出了一种方法，用于在只有有限受保护属性标签访问的情况下估计和减少公平违规行为。该方法可以估计现有模型的公平度量范围，并通过解决优化问题训练模型以限制公平违规。与现有方法不同的是，该方法利用了上下文信息。

    

    大多数训练公平模型的技术需要访问受保护属性（例如种族、性别），无论是在训练时还是在生产中。然而，许多重要应用中这些保护属性大部分是不可获得的。在本文中，我们开发了一种方法，用于在有限受保护属性标签访问情况下度量和减少公平违规行为。具体而言，我们假设对于感兴趣的数据集只能访问一小部分受保护属性标签，但对于其余数据集，只能通过概率估计受保护属性标签（例如通过贝叶斯改进的姓氏地理编码）。基于这种设定，我们提出了一种方法来估计现有模型的常见公平度量的范围，并开发了一种通过解决约束非凸优化问题来限制公平违规的模型训练方法。与类似的现有方法不同，我们的方法利用了上下文信息，特别是关联关系。

    The vast majority of techniques to train fair models require access to the protected attribute (e.g., race, gender), either at train time or in production. However, in many important applications this protected attribute is largely unavailable. In this paper, we develop methods for measuring and reducing fairness violations in a setting with limited access to protected attribute labels. Specifically, we assume access to protected attribute labels on a small subset of the dataset of interest, but only probabilistic estimates of protected attribute labels (e.g., via Bayesian Improved Surname Geocoding) for the rest of the dataset. With this setting in mind, we propose a method to estimate bounds on common fairness metrics for an existing model, as well as a method for training a model to limit fairness violations by solving a constrained non-convex optimization problem. Unlike similar existing approaches, our methods take advantage of contextual information -specifically, the relations
    
[^84]: Score dynamics: 使用条件扩散模型通过皮秒时间步提高分子动力学的规模化

    Score dynamics: scaling molecular dynamics with picosecond timesteps via conditional diffusion model. (arXiv:2310.01678v1 [physics.comp-ph])

    [http://arxiv.org/abs/2310.01678](http://arxiv.org/abs/2310.01678)

    该论文提出了Score dynamics (SD) 方法，通过条件扩散模型，可以使用1 ps的时间步长进行分子动力学模拟。基于图神经网络的Score dynamics模型展示了在丙氨酸二肽和短链烷烃案例中的效果。

    

    我们提出了一种称为Score dynamics (SD) 的通用框架，用于从分子动力学模拟中学习有效的演化算子，用于原子级和粗粒化动力学。SD以分数为中心，即与动态自由度的转换对数概率导数相关的量。后者在分数时间步中起到与MD中力场相同的作用，但在去噪扩散概率模型中用于生成动态变量的离散转变。这种时间步长可以比典型的MD时间步长大几个数量级。在这项工作中，我们构建了基于图神经网络的Score dynamics模型，用于演化以1~ps时间步长的现实分子体系。我们通过丙氨酸二肽和水溶液中的短链烷烃的案例研究证明了Score dynamics的效能。通过从条件概率的平稳分布中推导出的平衡预测和对转换速率和转换的动力学预测进行演示。

    We propose score dynamics (SD), a general framework for learning effective evolution operators for atomistic as well as coarse-grained dynamics from molecular-dynamics (MD) simulations. SD is centered around scores, or derivatives of the transition log-probability with respect to the dynamical degrees of freedom. The latter play the same role as force fields in MD but are used in denoising diffusion probability models to generate discrete transitions of the dynamical variables in an SD timestep, which can be orders of magnitude larger than a typical MD timestep. In this work, we construct graph neural network based score dynamics models of realistic molecular systems that are evolved with 1~ps timesteps. We demonstrate the efficacy of score dynamics with case studies of alanine dipeptide and short alkanes in aqueous solution. Both equilibrium predictions derived from the stationary distributions of the conditional probability and kinetic predictions for the transition rates and transit
    
[^85]: GNN中的局部感知图重连

    Locality-Aware Graph-Rewiring in GNNs. (arXiv:2310.01668v1 [cs.LG])

    [http://arxiv.org/abs/2310.01668](http://arxiv.org/abs/2310.01668)

    本论文提出了一种在GNN中考虑局部性的图重连技术，以减少过度压缩、保持图的稀疏性，并改善长程交互的捕捉能力。

    

    图神经网络（GNNs）是一种用于图上机器学习的流行模型，通常遵循消息传递范式，在对邻居节点的信息进行聚合时，节点的特征会被递归地更新。虽然在输入图上交换消息赋予了GNNs强大的归纳偏置，但也使得GNNs容易过度压缩，从而无法捕捉给定图中的长程交互。为了解决这个问题，图重连技术被提出作为一种改善信息流的手段，通过改变图的连接性。在这项工作中，我们确定了图重连的三个期望：（i）减少过度压缩，（ii）尊重图的局部性，以及（iii）保持图的稀疏性。我们强调了空间和频谱重连技术之间存在的根本权衡；尽管前者通常满足（i）和（ii）但不满足（iii），后者通常在满足（i）和（iii）的同时牺牲了（ii）。

    Graph Neural Networks (GNNs) are popular models for machine learning on graphs that typically follow the message-passing paradigm, whereby the feature of a node is updated recursively upon aggregating information over its neighbors. While exchanging messages over the input graph endows GNNs with a strong inductive bias, it can also make GNNs susceptible to over-squashing, thereby preventing them from capturing long-range interactions in the given graph. To rectify this issue, graph rewiring techniques have been proposed as a means of improving information flow by altering the graph connectivity. In this work, we identify three desiderata for graph-rewiring: (i) reduce over-squashing, (ii) respect the locality of the graph, and (iii) preserve the sparsity of the graph. We highlight fundamental trade-offs that occur between spatial and spectral rewiring techniques; while the former often satisfy (i) and (ii) but not (iii), the latter generally satisfy (i) and (iii) at the expense of (ii)
    
[^86]: Artemis: 针对高效隐私保护机器学习的HE感知训练

    Artemis: HE-Aware Training for Efficient Privacy-Preserving Machine Learning. (arXiv:2310.01664v1 [cs.LG])

    [http://arxiv.org/abs/2310.01664](http://arxiv.org/abs/2310.01664)

    Artemis是一种针对基于同态加密的隐私保护机器学习的高效DNN修剪技术，通过HE感知修剪策略最大化降低计算成本，并取得了显著的改进。

    

    基于同态加密的隐私保护机器学习（PPML）是一项有前景的基础隐私技术。要使其更加实用，需要降低其计算成本，特别是在处理现代大型深度神经网络时。模型压缩通过修剪在传统明文机器学习中非常有效，但无法有效应用于HE-PPML。我们提出了Artemis，一种针对HE推理的高效DNN修剪技术。我们审慎研究了两种HE感知修剪策略（位置和对角线），以减少旋转操作的数量，在HE卷积中占主导地位的计算时间。我们发现，基于对角线修剪的 Pareto 最优解是完全可行的。Artemis的优势在于将DNN训练与修剪相结合，通过一种新的团体Lasso正则化目标驱动，以最大化HE特定的成本降低（由旋转操作主导）。我们展示了Artemis在先前的HE导向修剪的基础上取得改进，并能够实现1.2-6倍的改进。

    Privacy-Preserving ML (PPML) based on Homomorphic Encryption (HE) is a promising foundational privacy technology. Making it more practical requires lowering its computational cost, especially, in handling modern large deep neural networks. Model compression via pruning is highly effective in conventional plaintext ML but cannot be effectively applied to HE-PPML as is.  We propose Artemis, a highly effective DNN pruning technique for HE-based inference. We judiciously investigate two HE-aware pruning strategies (positional and diagonal) to reduce the number of Rotation operations, which dominate compute time in HE convolution. We find that Pareto-optimal solutions are based fully on diagonal pruning. Artemis' benefits come from coupling DNN training, driven by a novel group Lasso regularization objective, with pruning to maximize HE-specific cost reduction (dominated by the Rotation operations). We show that Artemis improves on prior HE-oriented pruning and can achieve a 1.2-6x improvem
    
[^87]: 家庭电力数据生成器 (HEDGE): 一款用于生成电动车、住宅用电需求和光伏发电曲线的开放接入工具

    Home Electricity Data Generator (HEDGE): An open-access tool for the generation of electric vehicle, residential demand, and PV generation profiles. (arXiv:2310.01661v1 [eess.SY])

    [http://arxiv.org/abs/2310.01661](http://arxiv.org/abs/2310.01661)

    本文介绍了家庭电力数据生成器(HEDGE)，它是一款用于生成真实住宅能源数据的开放接入工具。通过使用真实的英国数据，HEDGE能够生成每日的光伏发电曲线、家庭电力负荷以及电动车消耗和在家可用性等曲线。这一工具填补了当前研究中缺乏可用数据的问题。

    

    本文介绍了家庭电力数据生成器(HEDGE)，一款用于随机生成真实住宅能源数据的开放接入工具。HEDGE基于英国实际数据生成了真实的住宅光伏发电曲线、家庭电力负荷以及电动车消耗和在家可用性的每日曲线。缺乏可用数据是研究住宅分布式能源资源特征和协调的主要障碍，特别是在使用数据驱动方法如基于机器学习的预测和基于强化学习的控制时。一个关键问题是虽然大量数据可用，但格式不可用，并且同一住宅的连续几天数据也不可用。我们通过开放接入的HEDGE工具来填补这些空白，该工具生成了一致于单个住宅的能源数据序列，包括曲线幅度和行为聚类。

    In this paper, we present the Home Electricity Data Generator (HEDGE), an open-access tool for the random generation of realistic residential energy data. HEDGE generates realistic daily profiles of residential PV generation, household electric loads, and electric vehicle consumption and at-home availability, based on real-life UK datasets. The lack of usable data is a major hurdle for research on residential distributed energy resources characterisation and coordination, especially when using data-driven methods such as machine learning-based forecasting and reinforcement learning-based control. A key issue is that while large data banks are available, they are not in a usable format, and numerous subsequent days of data for a given single home are unavailable. We fill these gaps with the open-access HEDGE tool which generates data sequences of energy data for several days in a way that is consistent for single homes, both in terms of profile magnitude and behavioural clusters. From r
    
[^88]: PolySketchFormer:基于草图的多项式核变换器加速Transformer

    PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels. (arXiv:2310.01655v1 [cs.LG])

    [http://arxiv.org/abs/2310.01655](http://arxiv.org/abs/2310.01655)

    本文通过使用多项式函数和多项式草图，实现了一个快速注意力机制PolySketchFormer，以突破Transformer架构中注意力机制的二次复杂性难题，无需假设注意力矩阵具有稀疏结构，并提出了高效的基于块的算法。

    

    Transformer架构中注意力机制的二次复杂性一直是扩展大型基础模型进行长上下文任务的瓶颈。实际上，最近的理论结果表明，在假设强指数时间假设的情况下，近似softmax注意力机制的输出在次二次时间内是困难的。本文通过用多项式函数和多项式草图替代softmax来突破这个理论障碍。特别是，我们展示了从随机数值线性代数文献中的多项式核的草图可以用于近似多项式注意力，从而实现了显著更快的注意力机制，而不需要假设注意力矩阵具有稀疏结构，这在许多先前的工作中已经完成。此外，我们提出了一种高效的基于块的算法，该算法使我们能够将因果掩码应用于注意力矩阵，而无需显式地计算$n \times n$注意力矩阵并计算输出。

    The quadratic complexity of attention in transformer architectures remains a big bottleneck in scaling up large foundation models for long context. In fact, recent theoretical results show the hardness of approximating the output of softmax attention mechanism in sub-quadratic time assuming Strong Exponential Time Hypothesis. In this paper, we show how to break this theoretical barrier by replacing softmax with a polynomial function and polynomial sketching. In particular we show that sketches for Polynomial Kernel from the randomized numerical linear algebra literature can be used to approximate the polynomial attention which leads to a significantly faster attention mechanism without assuming any sparse structure for the attention matrix that has been done in many previous works.  In addition, we propose an efficient block-based algorithm that lets us apply the causal mask to the attention matrix without explicitly realizing the $n \times n$ attention matrix and compute the output of
    
[^89]: 用简单的排列欺骗（视觉和）语言模型

    Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations. (arXiv:2310.01651v1 [cs.LG])

    [http://arxiv.org/abs/2310.01651](http://arxiv.org/abs/2310.01651)

    本文揭示了大型语言和视觉-语言模型中的一个特定漏洞，即它们对于多项选择问答的排列敏感性，这对于模型可靠性分析非常重要。这些漏洞在各种模型规模和最新的模型中都存在。

    

    大型语言和视觉-语言模型因其令人印象深刻的指示遵循能力和上下文学习能力而迅速在实践中部署。这引发了一个迫切的需求，即仔细分析它们的鲁棒性，以便利益相关者能够了解这些模型在任何给定应用程序中是否足够可信。在本文中，我们重点介绍了流行模型中的一个特定漏洞，即在多项选择问答（MCQA）中的排列敏感性。具体来说，我们通过实证研究表明，流行模型易受多项选择提示中答案集的对抗性排列攻击，这令人惊讶，因为模型理想上应该和人类一样对提示排列具有不变性。这些漏洞在各种模型规模下持续存在，并存在于最新的语言和视觉-语言模型中。

    Large language and vision-language models are rapidly being deployed in practice thanks to their impressive capabilities in instruction following, in-context learning, and so on. This raises an urgent need to carefully analyse their robustness so that stakeholders can understand if and when such models are trustworthy enough to be relied upon in any given application. In this paper, we highlight a specific vulnerability in popular models, namely permutation sensitivity in multiple-choice question answering (MCQA). Specifically, we show empirically that popular models are vulnerable to adversarial permutation in answer sets for multiple-choice prompting, which is surprising as models should ideally be as invariant to prompt permutation as humans are. These vulnerabilities persist across various model sizes, and exist in very recent language and vision-language models. Code is available at \url{https://github.com/ys-zong/FoolyourVLLMs}.
    
[^90]: CoDBench: 对连续动力系统数据驱动模型的重要评估

    CoDBench: A Critical Evaluation of Data-driven Models for Continuous Dynamical Systems. (arXiv:2310.01650v1 [cs.LG])

    [http://arxiv.org/abs/2310.01650](http://arxiv.org/abs/2310.01650)

    CoDBench是一个对连续动力系统数据驱动模型进行全面评估的基准套件，涵盖了11种最先进的模型，并对4种不同类别的这些模型进行了全面评估。这有助于科学机器学习领域的决策制定。

    

    连续动力系统通过微分方程来建模，广泛用于模拟诸如等离子体动力学、多孔介质流动、天气预报和流行病动力学等重要问题。最近，数据驱动模型被成功应用于建模这些系统。然而，与计算机视觉等已建立的领域相比，对于不同类别的这些模型的优点和潜在应用的研究相对有限，这可能会影响科学机器学习中的决策制定。本文介绍了CoDBench，一个包含11种最先进的解微分方程数据驱动模型的全面基准套件。具体而言，我们对4种不同类别的模型进行了全面评估，包括前馈神经网络、深度操作回归模型、基于频率的神经算子和变压器架构，并与8个广泛适用的基准数据集进行了对比。

    Continuous dynamical systems, characterized by differential equations, are ubiquitously used to model several important problems: plasma dynamics, flow through porous media, weather forecasting, and epidemic dynamics. Recently, a wide range of data-driven models has been used successfully to model these systems. However, in contrast to established fields like computer vision, limited studies are available analyzing the strengths and potential applications of different classes of these models that could steer decision-making in scientific machine learning. Here, we introduce CodBench, an exhaustive benchmarking suite comprising 11 state-of-the-art data-driven models for solving differential equations. Specifically, we comprehensively evaluate 4 distinct categories of models, viz., feed forward neural networks, deep operator regression models, frequency-based neural operators, and transformer architectures against 8 widely applicable benchmark datasets encompassing challenges from fluid 
    
[^91]: 关于训练导数约束神经网络

    On Training Derivative-Constrained Neural Networks. (arXiv:2310.01649v1 [cs.LG])

    [http://arxiv.org/abs/2310.01649](http://arxiv.org/abs/2310.01649)

    该论文研究了导数约束神经网络（DC NN），提出了一种整合RELU激活函数（IReLU）来改进DC NN的训练，并探究了去归一化和标签缩放对于稳定DC训练的作用。研究还评估了这些方法在物理相关设置中的效果，证明了使用IReLU激活函数、去归一化和标签缩放的现有架构更好地融入了导数约束提供的训练信号。

    

    我们将神经网络对输入的预测相对于输入的（部分）导数作为额外的训练信号的情况称之为导数约束神经网络（DC NN）。这种情况在自然科学中的物理相关设置中很常见。我们提出了一种整合RELU (IReLU)激活函数，以改进DC NN的训练。我们还研究了去归一化和标签缩放以帮助稳定DC训练。我们在包括量子化学和科学机器学习（SciML）任务在内的物理相关设置上评估了我们的方法。我们证明了使用IReLU激活函数结合去归一化和标签缩放的现有架构更好地融入了导数约束提供的训练信号。

    We refer to the setting where the (partial) derivatives of a neural network's (NN's) predictions with respect to its inputs are used as additional training signal as a derivative-constrained (DC) NN. This situation is common in physics-informed settings in the natural sciences. We propose an integrated RELU (IReLU) activation function to improve training of DC NNs. We also investigate denormalization and label rescaling to help stabilize DC training. We evaluate our methods on physics-informed settings including quantum chemistry and Scientific Machine Learning (SciML) tasks. We demonstrate that existing architectures with IReLU activations combined with denormalization and label rescaling better incorporate training signal provided by derivative constraints.
    
[^92]: 大型预训练模型的等变适应

    Equivariant Adaptation of Large Pre-Trained Models. (arXiv:2310.01647v1 [cs.LG])

    [http://arxiv.org/abs/2310.01647](http://arxiv.org/abs/2310.01647)

    本论文提出了一种适用于大型预训练网络的等变适应方法，通过使用一个简单的规范化网络来使输入转换为规范形式。然而，我们观察到规范定向可能与训练分布的方向不一致，从而影响性能。

    

    等变网络被专门设计用于保证与一组输入变换的一致行为，从而提高样本效率并实现更准确和稳健的预测。然而，重新设计主流深度神经网络架构的每个组件以实现所选等变性是一个困难的问题，并且可能导致在训练和推理过程中计算开销很大的网络。最近提出的一种替代等变性的方法是使用一个简单的规范化网络，在将输入提供给不受约束的预测网络之前将其转换为规范形式。我们在这里展示了这种方法可以有效地使一个大型预训练网络等变。然而，我们观察到产生的规范定向可能与训练分布的方向不一致，从而影响性能。通过使用依赖于数据集的先验知识来指导规范化函数，我们正在…

    Equivariant networks are specifically designed to ensure consistent behavior with respect to a set of input transformations, leading to higher sample efficiency and more accurate and robust predictions. However, redesigning each component of prevalent deep neural network architectures to achieve chosen equivariance is a difficult problem and can result in a computationally expensive network during both training and inference. A recently proposed alternative towards equivariance that removes the architectural constraints is to use a simple canonicalization network that transforms the input to a canonical form before feeding it to an unconstrained prediction network. We show here that this approach can effectively be used to make a large pre-trained network equivariant. However, we observe that the produced canonical orientations can be misaligned with those of the training distribution, hindering performance. Using dataset-dependent priors to inform the canonicalization function, we are
    
[^93]: 图数据上嘈杂伪标签的深入洞察

    Deep Insights into Noisy Pseudo Labeling on Graph Data. (arXiv:2310.01634v1 [cs.LG])

    [http://arxiv.org/abs/2310.01634](http://arxiv.org/abs/2310.01634)

    该论文深入研究了图数据上嘈杂伪标签的影响，通过理论分析展示了伪标签对图学习模型性能的边界和收敛性的影响，并提出了一种谨慎的伪标签方法。

    

    伪标签（PL）是一种广泛应用的策略，通过在训练过程中对潜在样本进行自注释，扩大已标记数据集。一些研究表明，它可以改善图学习模型的性能。然而，我们注意到错误标签对图训练过程可能具有致命影响。不适当的PL可能导致性能下降，特别是在图数据上，噪音可以传播。令人惊讶的是，相关错误在文献中很少被理论分析。在本文中，我们旨在深入洞察PL对图学习模型的影响。我们首先通过展示错误被PL阈值的置信度和多视图预测的一致性所限制来分析PL策略的错误。然后，我们从理论上说明了PL对收敛性质的影响。基于这些分析，我们提出了一种谨慎的伪标签方法，在这种方法中，我们使用最高置信度和多视图进行伪标签。

    Pseudo labeling (PL) is a wide-applied strategy to enlarge the labeled dataset by self-annotating the potential samples during the training process. Several works have shown that it can improve the graph learning model performance in general. However, we notice that the incorrect labels can be fatal to the graph training process. Inappropriate PL may result in the performance degrading, especially on graph data where the noise can propagate. Surprisingly, the corresponding error is seldom theoretically analyzed in the literature. In this paper, we aim to give deep insights of PL on graph learning models. We first present the error analysis of PL strategy by showing that the error is bounded by the confidence of PL threshold and consistency of multi-view prediction. Then, we theoretically illustrate the effect of PL on convergence property. Based on the analysis, we propose a cautious pseudo labeling methodology in which we pseudo label the samples with highest confidence and multi-view
    
[^94]: 通过最优输运进行从观察中的模仿学习

    Imitation Learning from Observation through Optimal Transport. (arXiv:2310.01632v1 [cs.RO])

    [http://arxiv.org/abs/2310.01632](http://arxiv.org/abs/2310.01632)

    本文提出了一种通过最优输运进行从观察中的模仿学习的方法，该方法不需要学习模型或对抗学习，可以与任何强化学习算法集成，并在各种连续控制任务上超过了现有最先进方法，在ILfO设置下实现了专家级的性能。

    

    从观察中的模仿学习（ILfO）是一种学习者试图在没有直接指导的情况下，使用观测数据模仿专家行为的设置。在本文中，我们重新审视了最优输运在IL中的应用，其中根据学习者和专家的状态轨迹之间的Wasserstein距离生成奖励。我们证明了现有方法可以简化为生成无需学习模型或对抗学习的奖励函数。与许多其他最先进的方法不同，我们的方法可以与任何强化学习算法集成，并适用于ILfO。我们在各种连续控制任务上展示了这种简单方法的有效性，并发现即使只观察单个专家轨迹而没有动作，它在ILfO设置中超过了现有最先进方法，在一系列评估领域中实现了专家级的性能。

    Imitation Learning from Observation (ILfO) is a setting in which a learner tries to imitate the behavior of an expert, using only observational data and without the direct guidance of demonstrated actions. In this paper, we re-examine the use of optimal transport for IL, in which a reward is generated based on the Wasserstein distance between the state trajectories of the learner and expert. We show that existing methods can be simplified to generate a reward function without requiring learned models or adversarial learning. Unlike many other state-of-the-art methods, our approach can be integrated with any RL algorithm, and is amenable to ILfO. We demonstrate the effectiveness of this simple approach on a variety of continuous control tasks and find that it surpasses the state of the art in the IlfO setting, achieving expert-level performance across a range of evaluation domains even when observing only a single expert trajectory without actions.
    
[^95]: 操作学习与数值分析相遇：通过迭代方法改进神经网络

    Operator Learning Meets Numerical Analysis: Improving Neural Networks through Iterative Methods. (arXiv:2310.01618v1 [cs.LG])

    [http://arxiv.org/abs/2310.01618](http://arxiv.org/abs/2310.01618)

    本文通过将深度学习与经典数值分析进行比较，将神经网络视为具有固定点的算子，并通过迭代方法的收敛证明，改进了神经网络的性能。通过实证评估，我们证明了通过网络算子进行迭代可以提高性能，并通过引入迭代图神经网络PIGN进一步展示了迭代的好处。

    

    深度神经网络尽管在许多应用中取得成功，但往往缺乏确立的理论基础。本文通过将深度学习与经典数值分析进行对比，弥合了这一差距。我们将神经网络视为具有固定点的算子，这些固定点代表着期望的解，从而建立了基于算子方程的迭代方法的理论框架。在定义的条件下，我们提出了基于固定点理论的收敛证明。我们证明了流行的架构，如扩散模型和AlphaFold，本质上采用了迭代算子学习。经验证明，通过网络算子进行迭代可以提高性能。我们还介绍了一种迭代图神经网络，PIGN，进一步展示了迭代的好处。我们的工作旨在通过将数值分析的见解与深度学习相结合，增加对深度学习的理解，有可能指导未来网络设计。

    Deep neural networks, despite their success in numerous applications, often function without established theoretical foundations. In this paper, we bridge this gap by drawing parallels between deep learning and classical numerical analysis. By framing neural networks as operators with fixed points representing desired solutions, we develop a theoretical framework grounded in iterative methods for operator equations. Under defined conditions, we present convergence proofs based on fixed point theory. We demonstrate that popular architectures, such as diffusion models and AlphaFold, inherently employ iterative operator learning. Empirical assessments highlight that performing iterations through network operators improves performance. We also introduce an iterative graph neural network, PIGN, that further demonstrates benefits of iterations. Our work aims to enhance the understanding of deep learning by merging insights from numerical analysis, potentially guiding the design of future net
    
[^96]: 多批次强化学习中的样本效率：对于维度相关的适应性的需求

    Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity. (arXiv:2310.01616v1 [cs.LG])

    [http://arxiv.org/abs/2310.01616](http://arxiv.org/abs/2310.01616)

    本文理论上探讨了强化学习中样本效率和适应性之间的关系，发现样本效率算法需要的批次数K具有Ω(log log d)的下界，其中n = O(poly(d))。

    

    我们在理论上探讨了强化学习中样本效率和适应性之间的关系。如果算法在问题的维度d中使用的环境查询次数n是多项式的，那么它是样本效率的。适应性是指查询被发送和反馈被处理以更新查询策略的频率。为了研究这种相互作用，我们采用了一个学习框架，允许在K个批次中发送查询，在每个批次之后处理反馈并更新查询。这个模型包括整个适应性谱，从非自适应的“离线”（K=1）到完全自适应（K=n）的场景，以及中间的情况。对于策略评估和在d维线性函数逼近下的最佳策略识别问题，我们为样本有效算法所需要的批次数K建立了Ω(log log d)的下界，其中n = O(poly(d))。我们的结果表明，样本效率算法需要的批次数K具有 Ω(log log d) 的下界，其中n = O(poly(d))。

    We theoretically explore the relationship between sample-efficiency and adaptivity in reinforcement learning. An algorithm is sample-efficient if it uses a number of queries $n$ to the environment that is polynomial in the dimension $d$ of the problem. Adaptivity refers to the frequency at which queries are sent and feedback is processed to update the querying strategy. To investigate this interplay, we employ a learning framework that allows sending queries in $K$ batches, with feedback being processed and queries updated after each batch. This model encompasses the whole adaptivity spectrum, ranging from non-adaptive 'offline' ($K=1$) to fully adaptive ($K=n$) scenarios, and regimes in between. For the problems of policy evaluation and best-policy identification under $d$-dimensional linear function approximation, we establish $\Omega(\log \log d)$ lower bounds on the number of batches $K$ required for sample-efficient algorithms with $n = O(poly(d))$ queries. Our results show that j
    
[^97]: 使用基于梯度的方法学习离散对数的困难度

    Intractability of Learning the Discrete Logarithm with Gradient-Based Methods. (arXiv:2310.01611v1 [cs.LG])

    [http://arxiv.org/abs/2310.01611](http://arxiv.org/abs/2310.01611)

    本文研究了使用梯度法学习离散对数的困难度，发现损失函数梯度在一个固定点附近聚集，且无论网络架构复杂性如何，都会导致学习离散对数奇偶比特的能力受限。

    

    离散对数问题是数论中的一个基本挑战，对密码协议具有重要的影响。本文研究了在有限循环群中，使用基于梯度的方法学习离散对数的奇偶比特的限制。我们的主要结果通过理论分析和实证验证，揭示了损失函数的梯度在一个固定点附近的聚集性质，独立于使用的对数的底数。这种聚集性质导致了使用基于梯度的方法高效学习离散对数的奇偶比特的能力受限，无论训练的网络架构的复杂性如何。我们的证明依赖于内积空间中的Boas-Bellman不等式，并通过某些矩阵的谱范数建立了离散对数奇偶比特函数的近似正交性。使用基于神经网络的方法的实证实验证实了我们的理论结果。

    The discrete logarithm problem is a fundamental challenge in number theory with significant implications for cryptographic protocols. In this paper, we investigate the limitations of gradient-based methods for learning the parity bit of the discrete logarithm in finite cyclic groups of prime order. Our main result, supported by theoretical analysis and empirical verification, reveals the concentration of the gradient of the loss function around a fixed point, independent of the logarithm's base used. This concentration property leads to a restricted ability to learn the parity bit efficiently using gradient-based methods, irrespective of the complexity of the network architecture being trained.  Our proof relies on Boas-Bellman inequality in inner product spaces and it involves establishing approximate orthogonality of discrete logarithm's parity bit functions through the spectral norm of certain matrices. Empirical experiments using a neural network-based approach further verify the l
    
[^98]: 对抗性上下文多臂赌博进行内核化研究

    Adversarial Contextual Bandits Go Kernelized. (arXiv:2310.01609v1 [stat.ML])

    [http://arxiv.org/abs/2310.01609](http://arxiv.org/abs/2310.01609)

    本研究通过将损失函数纳入再生核希尔伯特空间，对对抗性线性上下文多臂赌博的在线学习进行了内核化研究，并提出了一种计算效率高的算法，能够在多种特征值衰减假设下实现接近最优的遗憾保证。在多项式特征值衰减和指数特征值衰减的情况下，遗憾上界分别为 $\widetilde{O}(KT^{\frac{1}{2}(1+\frac{1}{c})})$ 和 $\widetilde{O}(\sqrt{T})$。

    

    我们通过将损失函数纳入再生核希尔伯特空间，研究了对抗性线性上下文多臂赌博的在线学习问题的一般化。这允许对复杂的决策场景进行更灵活的建模。我们提出了一种计算效率高的算法，利用新的乐观偏好估计器对损失函数进行估计，并在基于底层核的多种特征值衰减假设下实现了接近最优的遗憾保证。具体而言，在多项式特征值衰减指数 $c>1$ 的假设下，遗憾为 $\widetilde{O}(KT^{\frac{1}{2}(1+\frac{1}{c})})$，其中 $T$ 表示轮数，$K$ 表示动作数量。此外，当特征值衰减遵循指数模式时，我们实现了一个更加紧密的遗憾界 $\widetilde{O}(\sqrt{T})$。这些速度与已知的所有特殊情况下的下界匹配，并与已知的最佳上界相匹配。

    We study a generalization of the problem of online learning in adversarial linear contextual bandits by incorporating loss functions that belong to a reproducing kernel Hilbert space, which allows for a more flexible modeling of complex decision-making scenarios. We propose a computationally efficient algorithm that makes use of a new optimistically biased estimator for the loss functions and achieves near-optimal regret guarantees under a variety of eigenvalue decay assumptions made on the underlying kernel. Specifically, under the assumption of polynomial eigendecay with exponent $c>1$, the regret is $\widetilde{O}(KT^{\frac{1}{2}(1+\frac{1}{c})})$, where $T$ denotes the number of rounds and $K$ the number of actions. Furthermore, when the eigendecay follows an exponential pattern, we achieve an even tighter regret bound of $\widetilde{O}(\sqrt{T})$. These rates match the lower bounds in all special cases where lower bounds are known at all, and match the best known upper bounds avai
    
[^99]: 使用深度强化学习解决二次分配问题

    Solving the Quadratic Assignment Problem using Deep Reinforcement Learning. (arXiv:2310.01604v1 [cs.LG])

    [http://arxiv.org/abs/2310.01604](http://arxiv.org/abs/2310.01604)

    本文使用深度强化学习方法解决二次分配问题，提出了一种新颖的双指针网络，可以在没有特定实例重新训练的情况下产生解决方案。

    

    二次分配问题 (QAP) 是一个 NP 困难问题，对其进行解决一直是一个挑战：与旅行推销员问题 (TSP) 等其他组合问题不同，使用先进的整数规划技术可以在包含数百甚至数千个位置的实例上精确解决，尚未找到解决大小超过30的 QAP 实例的方法。然而，解决 QAP 问题至关重要，因为它具有许多关键的应用，如电子布线设计和设备布局选择。我们提出了一种使用深度强化学习解决 QAP 问题的方法。我们的方法依赖于一种新颖的双指针网络，这个网络在选择下一个位置来放置设施和选择前一个位置来放置设施之间进行交替。我们使用合成实例的大型数据集在 A2C 上训练我们的模型，产生不需要特定实例重新训练的解决方案。

    The Quadratic Assignment Problem (QAP) is an NP-hard problem which has proven particularly challenging to solve: unlike other combinatorial problems like the traveling salesman problem (TSP), which can be solved to optimality for instances with hundreds or even thousands of locations using advanced integer programming techniques, no methods are known to exactly solve QAP instances of size greater than 30. Solving the QAP is nevertheless important because of its many critical applications, such as electronic wiring design and facility layout selection. We propose a method to solve the original Koopmans-Beckman formulation of the QAP using deep reinforcement learning. Our approach relies on a novel double pointer network, which alternates between selecting a location in which to place the next facility and a facility to place in the previous location. We train our model using A2C on a large dataset of synthetic instances, producing solutions with no instance-specific retraining necessary
    
[^100]: 基于正确拓扑区域的池式主动学习

    Pool-Based Active Learning with Proper Topological Regions. (arXiv:2310.01597v1 [cs.LG])

    [http://arxiv.org/abs/2310.01597](http://arxiv.org/abs/2310.01597)

    本文提出了一种基于正确拓扑区域的池式主动学习策略的元方法，可用于多类别分类任务。该方法在各种基准数据集上进行了实证研究，并与文献中的经典方法具有竞争力。

    

    机器学习方法通常依赖于大样本量以获得良好的性能，但在许多应用中很难提供标记集。池式主动学习方法可以从一组未标记的数据中检测出对训练最相关的数据。本文提出了一种基于正确拓扑区域的池式主动学习策略的元方法，用于多类别分类任务。PTR是基于拓扑数据分析(TDA)的相关区域，用于采样冷启动点或在主动学习方案中使用。所提出的方法在各种基准数据集上进行了实证研究，与文献中的经典方法具有竞争力。

    Machine learning methods usually rely on large sample size to have good performance, while it is difficult to provide labeled set in many applications. Pool-based active learning methods are there to detect, among a set of unlabeled data, the ones that are the most relevant for the training. We propose in this paper a meta-approach for pool-based active learning strategies in the context of multi-class classification tasks based on Proper Topological Regions. PTR, based on topological data analysis (TDA), are relevant regions used to sample cold-start points or within the active learning scheme. The proposed method is illustrated empirically on various benchmark datasets, being competitive to the classical methods from the literature.
    
[^101]: 使用知识引导的机器学习进行规定火模拟，用于土地管理

    Prescribed Fire Modeling using Knowledge-Guided Machine Learning for Land Management. (arXiv:2310.01593v1 [cs.LG])

    [http://arxiv.org/abs/2310.01593](http://arxiv.org/abs/2310.01593)

    本论文介绍了使用知识引导的机器学习框架来进行规定火模拟，解决了传统方法中的物理不一致性和预测偏差等问题。

    

    近年来，灾热发生大火的威胁不断加剧，因此需要有效的规定火管理。传统上，基于过程的计算机模拟被用于规划预防野火的规定火。然而，即使是简化的过程模型如QUIC-Fire也过于计算密集，无法用于实时决策，特别是当天气条件迅速变化时。传统的火灾建模的机器学习方法可以提供计算速度加快，但在物理上不一致的预测，由于类别不平衡而引起的有偏预测，火灾蔓延指标（如燃烧面积、蔓延速度）的有偏估计以及在分布外的风条件下的可推广性上存在挑战。本文提出了一种新颖的机器学习（ML）框架，旨在快速模拟规定火，并解决了这些问题。通过结合领域知识，所提出的方法有助于减少燃料密度估计中的物理不一致性。

    In recent years, the increasing threat of devastating wildfires has underscored the need for effective prescribed fire management. Process-based computer simulations have traditionally been employed to plan prescribed fires for wildfire prevention. However, even simplified process models like QUIC-Fire are too compute-intensive to be used for real-time decision-making, especially when weather conditions change rapidly. Traditional ML methods used for fire modeling offer computational speedup but struggle with physically inconsistent predictions, biased predictions due to class imbalance, biased estimates for fire spread metrics (e.g., burned area, rate of spread), and generalizability in out-of-distribution wind conditions. This paper introduces a novel machine learning (ML) framework that enables rapid emulation of prescribed fires while addressing these concerns. By incorporating domain knowledge, the proposed method helps reduce physical inconsistencies in fuel density estimates in 
    
[^102]: 对比学习中的表示和分配伤害的研究

    An Investigation of Representation and Allocation Harms in Contrastive Learning. (arXiv:2310.01583v1 [stat.ML])

    [http://arxiv.org/abs/2310.01583](http://arxiv.org/abs/2310.01583)

    该论文研究了对比学习中的表示和分配伤害问题，发现在自监督学习中，对比学习容易造成少数群体的表示和多数群体的表示合并，从而导致分配伤害。通过因果中介分析和随机块模型的解释，强调了研究和缓解这种表示伤害的重要性。

    

    在有监督学习环境中，少数群体的代表不足对其表现的影响被认为是一个严重问题，然而，在自监督学习（SSL）的背景下，对此问题的研究还不够充分。在本文中，我们展示了对比学习（CL）这一流行的SSL变体会倾向于将少数群体的表示与某些多数群体的表示合并。我们将这种现象称为表示伤害，并在图像和文本数据集上使用相应的流行CL方法进行了验证。此外，我们对下游分类任务的分配伤害进行了因果中介分析，结果表明表示伤害在其中起到了一部分责任，从而强调了研究和缓解表示伤害的重要性。最后，我们使用随机块模型对表示伤害提供了理论解释，该模型导致对比学习环境下的表示神经网络崩溃。

    The effect of underrepresentation on the performance of minority groups is known to be a serious problem in supervised learning settings; however, it has been underexplored so far in the context of self-supervised learning (SSL). In this paper, we demonstrate that contrastive learning (CL), a popular variant of SSL, tends to collapse representations of minority groups with certain majority groups. We refer to this phenomenon as representation harm and demonstrate it on image and text datasets using the corresponding popular CL methods. Furthermore, our causal mediation analysis of allocation harm on a downstream classification task reveals that representation harm is partly responsible for it, thus emphasizing the importance of studying and mitigating representation harm. Finally, we provide a theoretical explanation for representation harm using a stochastic block model that leads to a representational neural collapse in a contrastive learning setting.
    
[^103]: 开源大型语言模型的安全性：对齐是否真正防止它们被滥用？

    On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?. (arXiv:2310.01581v1 [cs.LG])

    [http://arxiv.org/abs/2310.01581](http://arxiv.org/abs/2310.01581)

    本论文研究了开源大型语言模型的安全性，指出对齐不能真正防止它们被滥用，可以通过简单的方式误导它们生成不期望的内容。

    

    大型语言模型（LLMs）在自然语言生成（NLG）任务中取得了前所未有的性能。然而，许多现有的研究表明，它们可能被滥用来生成不期望的内容。为了应对这个问题，在发布LLMs供公众访问之前，模型开发者通常通过监督微调（SFT）或强化学习与人类反馈（RLHF）来对齐这些语言模型。因此，这些对齐的大型语言模型在面对潜在有害或不道德的请求时会拒绝生成不期望的内容。一个自然的问题是：“对齐是否真的能够防止这些开源大型语言模型被滥用来生成不期望的内容？”在这项工作中，我们对这个问题给出了否定的答案。特别是，我们展示了这些开源、对齐的大型语言模型可以在不需要复杂计算或仔细设计提示的情况下被轻易误导生成不期望的内容。我们的关键思想是直接操纵生成过程。

    Large Language Models (LLMs) have achieved unprecedented performance in Natural Language Generation (NLG) tasks. However, many existing studies have shown that they could be misused to generate undesired content. In response, before releasing LLMs for public access, model developers usually align those language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning with Human Feedback (RLHF). Consequently, those aligned large language models refuse to generate undesired content when facing potentially harmful/unethical requests. A natural question is "could alignment really prevent those open-sourced large language models from being misused to generate undesired content?''. In this work, we provide a negative answer to this question. In particular, we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs. Our key idea is to directly manipulate the generation proc
    
[^104]: 全局工作空间基元的收缩性质

    Contraction Properties of the Global Workspace Primitive. (arXiv:2310.01571v1 [cs.LG])

    [http://arxiv.org/abs/2310.01571](http://arxiv.org/abs/2310.01571)

    本文扩展了关于多区域递归神经网络的稳定性研究，并在全局工作空间模块化结构上证明了松散稳定性条件。实证结果显示全局工作空间稀疏组合网络在测试表现和韧性方面表现出较好的性能，强调了稳定性对于实现模块化RNN的重要性。

    

    为了推动关于多区域递归神经网络(RNNs)的重要新兴研究领域，我们在Kozachkov等人的“递归构建稳定的递归神经网络组装品”的基础上在理论上和实证上进行了扩展。我们证明了该架构的显著特例的松散稳定性条件，特别是针对全局工作空间模块化结构。我们通过对具有少量可训练参数的全局工作空间稀疏组合网络进行实证成功，不仅在整体测试表现方面表现出强大的性能，还对单个子网络的移除具有更大的韧性。这些全局工作空间接触区拓扑的实证结果依赖于稳定性的保持，突出了我们的理论工作对于实现模块化RNN的成功的相关性。此外，通过更广泛地探索不同子网络模块之间的连接结构的稀疏性，

    To push forward the important emerging research field surrounding multi-area recurrent neural networks (RNNs), we expand theoretically and empirically on the provably stable RNNs of RNNs introduced by Kozachkov et al. in "RNNs of RNNs: Recursive Construction of Stable Assemblies of Recurrent Neural Networks". We prove relaxed stability conditions for salient special cases of this architecture, most notably for a global workspace modular structure. We then demonstrate empirical success for Global Workspace Sparse Combo Nets with a small number of trainable parameters, not only through strong overall test performance but also greater resilience to removal of individual subnetworks. These empirical results for the global workspace inter-area topology are contingent on stability preservation, highlighting the relevance of our theoretical work for enabling modular RNN success. Further, by exploring sparsity in the connectivity structure between different subnetwork modules more broadly, we 
    
[^105]: 迭代式规划中的选项发现

    Iterative Option Discovery for Planning, by Planning. (arXiv:2310.01569v1 [cs.AI])

    [http://arxiv.org/abs/2310.01569](http://arxiv.org/abs/2310.01569)

    本文提出了一种迭代式选项发现方法，通过学习一组局部强策略来指导搜索算法，从而在强化学习和规划中应用于复杂领域。

    

    发现有用的时间抽象，也就是选项，被广泛认为是将强化学习和规划应用于日益复杂的领域的关键。在AlphaZero中使用的Expert Iteration策略学习方法的经验成功基础上，提出了Option Iteration，一种类似的选项发现方法。Option Iteration不是学习一个单一的强策略，而是学习一组选项策略，对于遇到的每个状态，至少有一种策略在某个未来的时间点与搜索结果吻合。直观地说，这可能更容易，因为它允许算法根据情况灵活调整，而不是学习一个在当前状态的细节上具有复杂依赖性的全局策略。通过学习这样一组局部强策略，我们可以使用它们来指导搜索算法，从而形成良性循环，获得更好的结果。

    Discovering useful temporal abstractions, in the form of options, is widely thought to be key to applying reinforcement learning and planning to increasingly complex domains. Building on the empirical success of the Expert Iteration approach to policy learning used in AlphaZero, we propose Option Iteration, an analogous approach to option discovery. Rather than learning a single strong policy that is trained to match the search results everywhere, Option Iteration learns a set of option policies trained such that for each state encountered, at least one policy in the set matches the search results for some horizon into the future. Intuitively, this may be significantly easier as it allows the algorithm to hedge its bets compared to learning a single globally strong policy, which may have complex dependencies on the details of the current state. Having learned such a set of locally strong policies, we can use them to guide the search algorithm resulting in a virtuous cycle where better 
    
[^106]: 从InSAR图像中基于因果关系的大规模飓风后建筑损害快速检测

    Causality-informed Rapid Post-hurricane Building Damage Detection in Large Scale from InSAR Imagery. (arXiv:2310.01565v1 [cs.LG])

    [http://arxiv.org/abs/2310.01565](http://arxiv.org/abs/2310.01565)

    本文介绍了一种从InSAR图像中快速检测飓风后建筑损伤的方法，通过编码复杂的因果关系图来提高建筑损害信息的准确性。

    

    及时准确评估飓风引起的建筑物损害对于有效的飓风后应对和恢复工作至关重要。最近，遥感技术提供了大规模光学或干涉合成孔径雷达（InSAR）图像数据，可立即用于进行快速建筑损害评估。与光学卫星图像相比，合成孔径雷达可以穿透云层，并在各种天气条件下提供更完整的受损区域空间覆盖。然而，这些InSAR图像通常包含由共同发生或共同位置的建筑损坏、洪水、洪水/风引发的植被变化以及人为活动引起的高度噪声和混合信号，使得准确提取建筑损害信息具有挑战性。在本文中，我们介绍了一种从InSAR图像中快速检测飓风后建筑损伤的方法。该方法编码复杂的因果关系图来提高建筑损害信息的准确性。

    Timely and accurate assessment of hurricane-induced building damage is crucial for effective post-hurricane response and recovery efforts. Recently, remote sensing technologies provide large-scale optical or Interferometric Synthetic Aperture Radar (InSAR) imagery data immediately after a disastrous event, which can be readily used to conduct rapid building damage assessment. Compared to optical satellite imageries, the Synthetic Aperture Radar can penetrate cloud cover and provide more complete spatial coverage of damaged zones in various weather conditions. However, these InSAR imageries often contain highly noisy and mixed signals induced by co-occurring or co-located building damage, flood, flood/wind-induced vegetation changes, as well as anthropogenic activities, making it challenging to extract accurate building damage information. In this paper, we introduced an approach for rapid post-hurricane building damage detection from InSAR imagery. This approach encoded complex causal 
    
[^107]: SmartPlay: 一种用于评估LLMs作为智能Agent能力的基准

    SmartPlay : A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v1 [cs.LG])

    [http://arxiv.org/abs/2310.01557](http://arxiv.org/abs/2310.01557)

    SmartPlay是一个用于评估LLMs作为智能Agent能力的基准，包括6个具有不同挑战的游戏，并测试了智能LLM Agent的多种关键能力。这不仅是一个评估LLM Agent整体性能的严格测试场地，还可以分析每个能力的表现。

    

    最近的大型语言模型(LLMs)在智能Agent和下一代自动化方面展示了巨大的潜力，但目前缺乏一个系统化的基准来评估LLMs作为Agent的能力。我们介绍了SmartPlay：一个具有挑战性的基准和评估LLMs作为Agent的方法论。SmartPlay包括6个不同的游戏，包括剪刀石头布、汉诺塔、Minecraft等。每个游戏都具有独特的设置，提供最多20个评估设置和无限的环境变化。SmartPlay中的每个游戏都独特地挑战了智能LLM Agent的9个重要能力的子集，包括对对象依赖的推理、提前规划、空间推理、从历史中学习和理解随机性。每个游戏测试的能力集的区别使我们能够单独分析每个能力。SmartPlay不仅是评估LLM Agent整体性能的严格测试场地，而且也是评估Agent在不同能力方面的性能的一个重要工具。

    Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as
    
[^108]: 利用选择的能力优化决策树学习

    Harnessing the Power of Choices in Decision Tree Learning. (arXiv:2310.01551v1 [cs.LG])

    [http://arxiv.org/abs/2310.01551](http://arxiv.org/abs/2310.01551)

    该研究提出了一种利用选择能力的决策树学习算法Top-$k$，相比于传统贪婪算法和最优决策树算法，在准确率和性能上取得更好的结果。

    

    我们提出了一种简单的对标准和经验成功的决策树学习算法（如ID3、C4.5和CART）进行推广的方法。这些算法凭借贪婪的特性，在机器学习领域发挥了关键作用：它们通过迭代地基于最佳属性进行划分来构建决策树。我们的算法Top-$k$则考虑$k$个最佳属性作为可能的划分，而不仅仅是单个最佳属性。我们通过理论和实证研究展示了这个简单推广方法的优势。首先，我们证明了一个“贪婪层次定理”，对于每个$k \in \mathbb{N}$，Top-$(k+1)$比Top-$k$更加强大：在某些数据分布下，前者可以达到$1-\varepsilon$的准确率，而后者只能达到$\frac1{2}+\varepsilon$的准确率。然后，我们通过大量实验表明，Top-$k$算法在决策树学习中优于两种主要方法：经典贪婪算法和较新的“最优决策树”算法。

    We propose a simple generalization of standard and empirically successful decision tree learning algorithms such as ID3, C4.5, and CART. These algorithms, which have been central to machine learning for decades, are greedy in nature: they grow a decision tree by iteratively splitting on the best attribute. Our algorithm, Top-$k$, considers the $k$ best attributes as possible splits instead of just the single best attribute. We demonstrate, theoretically and empirically, the power of this simple generalization. We first prove a {\sl greediness hierarchy theorem} showing that for every $k \in \mathbb{N}$, Top-$(k+1)$ can be dramatically more powerful than Top-$k$: there are data distributions for which the former achieves accuracy $1-\varepsilon$, whereas the latter only achieves accuracy $\frac1{2}+\varepsilon$. We then show, through extensive experiments, that Top-$k$ outperforms the two main approaches to decision tree learning: classic greedy algorithms and more recent "optimal decis
    
[^109]: 关于有界均值的投注置信区间的近优性

    On the near-optimality of betting confidence sets for bounded means. (arXiv:2310.01547v1 [math.ST])

    [http://arxiv.org/abs/2310.01547](http://arxiv.org/abs/2310.01547)

    本文为投注置信区间的改进性能提供了理论解释，并比较了经典方法中的宽度，发现投注置信区间具有较小的极限宽度。

    

    在统计学中，从独立同分布（i.i.d.）观测中构建一元分布的非渐近置信区间（CI）是一项基本任务。对于有界观测值，经典的非参数方法通过反转标准浓度界限（如Hoeffding或Bernstein不等式）来进行。最近，一种替代的基于投注的方法被用于定义CI和其时间一致变体，称为置信序列（CS），已被证明在实证上优于经典方法。本文为这种投注CI和CS的改进经验性性能提供了理论上的解释。我们的主要贡献如下：（i）我们首先比较CI，使用它们的一阶渐近宽度的值（经过$\sqrt{n}$缩放），并且表明Waudby-Smith和Ramdas（2023）的投注CI比现有的经验Bernstein（EB）CI的极限宽度更小。（ii）接下来，我们建立了两个下界。

    Constructing nonasymptotic confidence intervals (CIs) for the mean of a univariate distribution from independent and identically distributed (i.i.d.) observations is a fundamental task in statistics. For bounded observations, a classical nonparametric approach proceeds by inverting standard concentration bounds, such as Hoeffding's or Bernstein's inequalities. Recently, an alternative betting-based approach for defining CIs and their time-uniform variants called confidence sequences (CSs), has been shown to be empirically superior to the classical methods. In this paper, we provide theoretical justification for this improved empirical performance of betting CIs and CSs.  Our main contributions are as follows: (i) We first compare CIs using the values of their first-order asymptotic widths (scaled by $\sqrt{n}$), and show that the betting CI of Waudby-Smith and Ramdas (2023) has a smaller limiting width than existing empirical Bernstein (EB)-CIs. (ii) Next, we establish two lower bounds
    
[^110]: 融合具有互补专长的模型

    Fusing Models with Complementary Expertise. (arXiv:2310.01542v1 [cs.LG])

    [http://arxiv.org/abs/2310.01542](http://arxiv.org/abs/2310.01542)

    该论文研究了融合具有互补专长的模型的问题，并将其应用于不同任务和领域中，通过监督学习的方式实现了显著的性能提升。

    

    训练能够在不同任务和领域中进行泛化的AI模型一直是推动AI研究的开放问题之一。基础模型的出现使得获得特定任务的专家模型变得更加容易，但是在测试时可能会遇到的数据的异质性意味着任何单个专家都不足够。我们考虑融合专家模型输出的Fusion of Experts（FoE）问题，这些专家模型具有对数据分布的互补知识，并将其形式化为监督学习的一个实例。我们的方法适用于判别任务和生成任务，并在图像和文本分类、文本摘要、多项选择问答和生成文本自动评估等方面取得了显著的性能提升。我们还将我们的方法扩展到“节俭”设置，即希望在测试时减少专家模型的评估次数。

    Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the "frugal" setting where it is desired to reduce the number of expert model evaluations at test time.
    
[^111]: 基于非参数子空间监测的对抗客户检测在联邦物联网中的应用

    Adversarial Client Detection via Non-parametric Subspace Monitoring in the Internet of Federated Things. (arXiv:2310.01537v1 [cs.LG])

    [http://arxiv.org/abs/2310.01537](http://arxiv.org/abs/2310.01537)

    本文提出了一种基于非参数子空间监测的方法FedRR，用于对抗客户检测和控制误报率，以解决联邦物联网中的安全问题。

    

    联邦物联网（IoFT）代表了一个由互联系统组成的网络，以联邦学习作为支撑，促进协作式知识获取，同时确保个体系统的数据隐私。然而，IoFT的广泛应用受到安全问题的阻碍，特别是联邦学习网络易受对抗性攻击的影响。本文提出了一种有效的非参数方法FedRR，它利用联邦学习生成的传输参数更新的低秩特征来解决对抗性攻击问题。此外，我们的方法能够准确检测对抗性客户，并在没有攻击发生的情况下控制误报率。基于MNIST数据集的数字识别实验验证了我们方法的优势。

    The Internet of Federated Things (IoFT) represents a network of interconnected systems with federated learning as the backbone, facilitating collaborative knowledge acquisition while ensuring data privacy for individual systems. The wide adoption of IoFT, however, is hindered by security concerns, particularly the susceptibility of federated learning networks to adversarial attacks. In this paper, we propose an effective non-parametric approach FedRR, which leverages the low-rank features of the transmitted parameter updates generated by federated learning to address the adversarial attack problem. Besides, our proposed method is capable of accurately detecting adversarial clients and controlling the false alarm rate under the scenario with no attack occurring. Experiments based on digit recognition using the MNIST datasets validated the advantages of our approach.
    
[^112]: 现场预测利用多头卷积神经网络和深度生成模型预测次日边际排放

    Nowcasting day-ahead marginal emissions using multi-headed CNNs and deep generative models. (arXiv:2310.01524v1 [cs.LG])

    [http://arxiv.org/abs/2310.01524](http://arxiv.org/abs/2310.01524)

    本论文提出了一种利用多头卷积神经网络和深度生成模型的方法，进行现场预测次日边际排放因素，以适应高灵活性和分布式能源资源渗透的电力系统。

    

    利用多头卷积神经网络和深度生成模型现场预测次日边际排放因素对于具有高灵活性和分布式能源资源渗透的电力系统变得越来越重要。在当前能源系统中，由天然气和燃煤发电厂提供的大部分的固定发电量，已经广泛研究了对次日排放的预测。相反，随着我们转向以灵活的电力市场、可分配资源和竞争性低成本发电（如大规模电池或氢储存）为特征的能源系统，系统操作员将能够从不同发电以及排放路径的组合中进行选择。为了充分开发给定调度计划的排放影响，我们需要一个具有两个层级的近实时工作流程。第一层是一个市场模型，不断解决一个安全约束经济调度模型。第二层根据市场模型的输出确定边际排放，这是本文的主题。

    Nowcasting day-ahead marginal emissions factors is increasingly important for power systems with high flexibility and penetration of distributed energy resources. With a significant share of firm generation from natural gas and coal power plants, forecasting day-ahead emissions in the current energy system has been widely studied. In contrast, as we shift to an energy system characterized by flexible power markets, dispatchable sources, and competing low-cost generation such as large-scale battery or hydrogen storage, system operators will be able to choose from a mix of different generation as well as emission pathways. To fully develop the emissions implications of a given dispatch schedule, we need a near real-time workflow with two layers. The first layer is a market model that continuously solves a security-constrained economic dispatch model. The second layer determines the marginal emissions based on the output of the market model, which is the subject of this paper. We propose 
    
[^113]: 噪声注入对于动态灰箱模型创建的益处

    The Benefit of Noise-Injection for Dynamic Gray-Box Model Creation. (arXiv:2310.01517v1 [cs.LG])

    [http://arxiv.org/abs/2310.01517](http://arxiv.org/abs/2310.01517)

    本文研究了噪声注入在动态灰箱模型创建中的益处。通过向训练数据集中注入噪声，可以解决模型的非线性、未建模的动态和局部极小值等挑战，在水-水换热器的动态模型测试中取得了显著的建模误差减小。

    

    灰箱模型相比于黑箱方法在设备仿真器开发中具有显著优势，因为其集成了物理学，在训练领域之外提供了更多的模型信心。然而，模型的非线性、未建模的动态和局部极小值等挑战引入了不确定性，这些挑战是现有方法无法克服的，导致其性能低于黑箱模型。本文旨在通过向训练数据集中注入噪声来解决这些不确定性。噪声注入丰富了数据集，并提供了对这些不确定性的鲁棒性衡量。以水-水换热器的动态模型作为演示案例，并使用一对实际设备和实时数据流测试。与未处理的信号数据相比，噪声注入的应用导致建模误差显著减小（均方根误差减少）。

    Gray-box models offer significant benefit over black-box approaches for equipment emulator development for equipment since their integration of physics provides more confidence in the model outside of the training domain. However, challenges such as model nonlinearity, unmodeled dynamics, and local minima introduce uncertainties into grey-box creation that contemporary approaches have failed to overcome, leading to their under-performance compared with black-box models. This paper seeks to address these uncertainties by injecting noise into the training dataset. This noise injection enriches the dataset and provides a measure of robustness against such uncertainties. A dynamic model for a water-to-water heat exchanger has been used as a demonstration case for this approach and tested using a pair of real devices with live data streaming. Compared to the unprocessed signal data, the application of noise injection resulted in a significant reduction in modeling error (root mean square er
    
[^114]: 张量环优化的增强量子张量神经网络

    Tensor Ring Optimized Quantum-Enhanced Tensor Neural Networks. (arXiv:2310.01515v1 [quant-ph])

    [http://arxiv.org/abs/2310.01515](http://arxiv.org/abs/2310.01515)

    张量环优化的增强量子张量神经网络（TR-QNet）将串联纠缠门应用于张量网络，通过量子比特测量和随机梯度下降算法对其参数进行优化，实现了在多个数据集上的二元分类精度提升。

    

    量子机器学习研究者通常将张量网络（TN）融入深度神经网络（DNN）和变分优化中。然而，用于训练模型每个层的可训练权重的标准优化技术在经典实现中受到模型参数之间的相关性和纠缠结构的影响。为了解决这个问题，提出了一种张量环优化的变分量子学习分类器（Quan-TR）的多层设计，该设计用串联纠缠门替代张量网络的全连接层，并称之为张量环优化的增强量子张量神经网络（TR-QNet）。通过量子比特测量，使用随机梯度下降算法对TR-QNet参数进行优化。在鸢尾花、MNIST和CIFAR-10三个不同的数据集上评估了所提出的TR-QNet，以展示二元分类所实现的精度提升。

    Quantum machine learning researchers often rely on incorporating Tensor Networks (TN) into Deep Neural Networks (DNN) and variational optimization. However, the standard optimization techniques used for training the contracted trainable weights of each model layer suffer from the correlations and entanglement structure between the model parameters on classical implementations. To address this issue, a multi-layer design of a Tensor Ring optimized variational Quantum learning classifier (Quan-TR) comprising cascading entangling gates replacing the fully connected (dense) layers of a TN is proposed, and it is referred to as Tensor Ring optimized Quantum-enhanced tensor neural Networks (TR-QNet). TR-QNet parameters are optimized through the stochastic gradient descent algorithm on qubit measurements. The proposed TR-QNet is assessed on three distinct datasets, namely Iris, MNIST, and CIFAR-10, to demonstrate the enhanced precision achieved for binary classification. On quantum simulations
    
[^115]: CODA: 通过概念漂移模拟器实现时间域泛化

    CODA: Temporal Domain Generalization via Concept Drift Simulator. (arXiv:2310.01508v1 [cs.LG])

    [http://arxiv.org/abs/2310.01508](http://arxiv.org/abs/2310.01508)

    该论文介绍了一个模型无关的时间域泛化方法，通过概念漂移模拟器来解决真实世界数据中的概念漂移问题。

    

    在真实世界的应用中，由于底层时间趋势引起的联合分布的变化，机器学习模型往往变得过时，这种现象被称为“概念漂移”。现有的研究提出了模型特定的策略，实现在近期领域的时间泛化。然而，真实世界数据集的多样性要求定制化的预测模型体系结构。为此，迫切需要一种模型无关的时间域泛化方法，能够在不同的数据模态和体系结构之间保持普遍性。在这项工作中，我们旨在从数据中心的角度解决概念漂移问题，绕过考虑数据和模型之间的交互。开发这样的框架面临着重要的挑战：(i)现有的生成模型难以生成超出分布之外的未来数据，(ii)准确捕捉沿时间顺序的联合分布的时间趋势。

    In real-world applications, machine learning models often become obsolete due to shifts in the joint distribution arising from underlying temporal trends, a phenomenon known as the "concept drift". Existing works propose model-specific strategies to achieve temporal generalization in the near-future domain. However, the diverse characteristics of real-world datasets necessitate customized prediction model architectures. To this end, there is an urgent demand for a model-agnostic temporal domain generalization approach that maintains generality across diverse data modalities and architectures. In this work, we aim to address the concept drift problem from a data-centric perspective to bypass considering the interaction between data and model. Developing such a framework presents non-trivial challenges: (i) existing generative models struggle to generate out-of-distribution future data, and (ii) precisely capturing the temporal trends of joint distribution along chronological source doma
    
[^116]: 基于量子的多分类问题中的复杂系统特征选择与边缘计算

    Quantum-Based Feature Selection for Multi-classification Problem in Complex Systems with Edge Computing. (arXiv:2310.01443v1 [cs.LG])

    [http://arxiv.org/abs/2310.01443](http://arxiv.org/abs/2310.01443)

    本文提出了一种基于量子的多分类问题特征选择算法QReliefF，通过量子编码和相似性计算，可以有效降低算法复杂性并提高计算效率。

    

    复杂系统的边缘计算需要大量的多特征数据来提取适当的见解以支持决策，因此寻找一种可行的特征选择方法以提高计算效率和节约资源消耗变得非常重要。本文提出了一种基于量子的多分类问题特征选择算法，名为QReliefF，它可以有效降低算法复杂性并提高计算效率。首先，通过执行CMP和R_y操作，将每个样本的所有特征编码为量子态，然后应用幅度估计来计算任意两个量子态（即两个样本）之间的相似性。根据相似性，利用Grover-Long方法找到最近的k个邻居样本，然后更新权重向量。通过上述过程的一定次数的迭代，可以选择所需的特征。

    The complex systems with edge computing require a huge amount of multi-feature data to extract appropriate insights for their decision making, so it is important to find a feasible feature selection method to improve the computational efficiency and save the resource consumption. In this paper, a quantum-based feature selection algorithm for the multi-classification problem, namely, QReliefF, is proposed, which can effectively reduce the complexity of algorithm and improve its computational efficiency. First, all features of each sample are encoded into a quantum state by performing operations CMP and R_y, and then the amplitude estimation is applied to calculate the similarity between any two quantum states (i.e., two samples). According to the similarities, the Grover-Long method is utilized to find the nearest k neighbor samples, and then the weight vector is updated. After a certain number of iterations through the above process, the desired features can be selected with regards to
    
[^117]: 构建灵活、可扩展且机器学习准备的多模态肿瘤学数据集

    Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets. (arXiv:2310.01438v1 [cs.LG])

    [http://arxiv.org/abs/2310.01438](http://arxiv.org/abs/2310.01438)

    本文提出了一种灵活、可扩展且机器学习准备的多模态肿瘤学数据集(MINDS)框架，用于融合来自不同来源的数据，并提供了探索关系和构建大规模多模态机器学习模型的界面。

    

    数据采集、存储和处理技术的进步导致了异质医学数据的快速增长。将放射学扫描、组织病理学图像和分子信息与临床数据整合是开发对疾病有全面理解和优化治疗至关重要的。在复杂疾病（如癌症）中，将来自多个来源的数据进行整合的需求更加突出，以实现精准医学和个性化治疗。本研究提出了多模态肿瘤数据系统（MINDS）-一种灵活、可扩展且经济高效的元数据框架，用于将来自公共来源（如癌症研究数据共享库）的异构数据有效地融合到一个相互连接且以患者为中心的框架中。MINDS提供了一个可以探索不同数据类型之间关系并构建大规模多模态机器学习模型的界面。通过协调多模态数据，MINDS旨在实现促进研究创新、精准医学和个性化治疗的目标。

    The advancements in data acquisition, storage, and processing techniques have resulted in the rapid growth of heterogeneous medical data. Integrating radiological scans, histopathology images, and molecular information with clinical data is essential for developing a holistic understanding of the disease and optimizing treatment. The need for integrating data from multiple sources is further pronounced in complex diseases such as cancer for enabling precision medicine and personalized treatments. This work proposes Multimodal Integration of Oncology Data System (MINDS) - a flexible, scalable, and cost-effective metadata framework for efficiently fusing disparate data from public sources such as the Cancer Research Data Commons (CRDC) into an interconnected, patient-centric framework. MINDS offers an interface for exploring relationships across data types and building cohorts for developing large-scale multimodal machine learning models. By harmonizing multimodal data, MINDS aims to pot
    
[^118]: 使用GPT-4的图神经网络架构搜索

    Graph Neural Architecture Search with GPT-4. (arXiv:2310.01436v1 [cs.LG])

    [http://arxiv.org/abs/2310.01436](http://arxiv.org/abs/2310.01436)

    本文将GPT-4集成到图神经网络架构搜索（GNAS）中，提出了一种新的GPT-4基于的GNAS方法（GPT4GNAS），通过设计新的提示来引导GPT-4生成更准确的图神经网络，实验证明嵌入GPT-4到GNAS中优于现有方法。

    

    图神经网络架构搜索（GNAS）在自动设计图神经网络方面取得了有希望的结果。然而，GNAS仍然需要大量的人工劳动和丰富的领域知识来设计搜索空间和搜索策略。本文将GPT-4集成到GNAS中，提出了一种基于GPT-4的图神经网络架构搜索方法（简称为GPT4GNAS）。我们的方法的基本思想是为GPT-4设计一类新的提示，以指导GPT-4进行图神经网络架构的生成任务。这些提示包括GNAS的搜索空间、搜索策略和搜索反馈的描述。通过迭代地运行具有提示的GPT-4，GPT4GNAS能够生成更准确的图神经网络，并快速收敛。实验结果表明，嵌入GPT-4到GNAS中优于现有最先进的GNAS方法。

    Graph Neural Architecture Search (GNAS) has shown promising results in automatically designing graph neural networks. However, GNAS still requires intensive human labor with rich domain knowledge to design the search space and search strategy. In this paper, we integrate GPT-4 into GNAS and propose a new GPT-4 based Graph Neural Architecture Search method (GPT4GNAS for short). The basic idea of our method is to design a new class of prompts for GPT-4 to guide GPT-4 toward the generative task of graph neural architectures. The prompts consist of descriptions of the search space, search strategy, and search feedback of GNAS. By iteratively running GPT-4 with the prompts, GPT4GNAS generates more accurate graph neural networks with fast convergence. Experimental results show that embedding GPT-4 into GNAS outperforms the state-of-the-art GNAS methods.
    
[^119]: 革新移动互动：在移动设备上实现30亿参数的GPT LLM

    Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile. (arXiv:2310.01434v1 [cs.CL])

    [http://arxiv.org/abs/2310.01434](http://arxiv.org/abs/2310.01434)

    本文介绍了一种在移动设备上执行30亿参数的GPT LLM的创新方法，通过集成本地代码和模型量化技术实现了在低内存设备上的平稳运行，并解决了网络依赖性的问题。这种应用不仅具有通用助手功能，还可以实现文本到操作的无缝移动互动。

    

    近年来，人工智能领域取得了显著进展，特别是基于变压器架构的强大大型语言模型（LLM）的出现。云端的LLM，例如OpenAI的ChatGPT，具有令人印象深刻的功能，但由于网络依赖性，延迟和隐私问题令人担忧。本文提出了一种创新的LLM推断方法，展望了未来在没有网络连接的情况下，拥有数十亿参数的LLM可以直接在移动设备上执行。本文展示了一种具有30亿参数的精调GPT LLM，可以在内存只有4GB的设备上平稳运行。通过整合本地代码和模型量化技术，该应用不仅可以作为通用助手，还可以通过文本到操作的功能实现无缝的移动互动。本文提供了有关训练流程、实现细节和测试结果的见解。

    The field of Artificial Intelligence has witnessed remarkable progress in recent years, especially with the emergence of powerful large language models (LLMs) based on the transformer architecture. Cloud-based LLMs, such as OpenAI's ChatGPT, offer impressive capabilities but come with concerns regarding latency and privacy due to network dependencies. This article presents an innovative approach to LLM inference, envisioning a future where LLMs with billions of parameters can be executed directly on mobile devices without network connectivity. The article showcases a fine-tuned GPT LLM with 3 billion parameters that can operate smoothly on devices with as low as 4GB of memory. Through the integration of native code and model quantization techniques, the application not only serves as a general-purpose assistant but also facilitates seamless mobile interactions with text-to-actions features. The article provides insights into the training pipeline, implementation details, test results, 
    
[^120]: AI-Aristotle: 一种物理启发的系统生物学灰盒识别框架

    AI-Aristotle: A Physics-Informed framework for Systems Biology Gray-Box Identification. (arXiv:2310.01433v1 [q-bio.QM])

    [http://arxiv.org/abs/2310.01433](http://arxiv.org/abs/2310.01433)

    AI-Aristotle是一种物理启发的框架，用于系统生物学中的参数估计和灰盒识别。它结合了X-TFC和PINNs技术，并使用符号回归技术进行参数发现和验证。通过在两个系统生物学基准问题上的测试，我们验证了AI-Aristotle的准确性、速度、灵活性和鲁棒性。

    

    从观测数据中发现控制物理和生物系统的数学方程是科学研究中的一个基本挑战。我们提出了一种新的物理启发框架，用于系统生物学领域参数估计和缺失物理识别（灰盒）。该框架名为AI-Aristotle，结合了X-TFC领域分解、物理启发神经网络（PINNs）和符号回归（SR）技术，用于参数发现和灰盒识别。我们基于系统生物学中的两个基准问题进行了AI-Aristotle的准确性、速度、灵活性和鲁棒性测试：药代动力学药物吸收模型和葡萄糖胰岛素相互作用的高频内分泌模型。我们比较了两种机器学习方法（X-TFC和PINNs），并采用了两种不同的符号回归技术来交叉验证我们的结果。尽管目前的工作重点是性能

    Discovering mathematical equations that govern physical and biological systems from observed data is a fundamental challenge in scientific research. We present a new physics-informed framework for parameter estimation and missing physics identification (gray-box) in the field of Systems Biology. The proposed framework -- named AI-Aristotle -- combines eXtreme Theory of Functional Connections (X-TFC) domain-decomposition and Physics-Informed Neural Networks (PINNs) with symbolic regression (SR) techniques for parameter discovery and gray-box identification. We test the accuracy, speed, flexibility and robustness of AI-Aristotle based on two benchmark problems in Systems Biology: a pharmacokinetics drug absorption model, and an ultradian endocrine model for glucose-insulin interactions. We compare the two machine learning methods (X-TFC and PINNs), and moreover, we employ two different symbolic regression techniques to cross-verify our results. While the current work focuses on the perfo
    
[^121]: REMEDI: 基于强化学习的自适应PSC疾病进展代谢建模

    REMEDI: REinforcement learning-driven adaptive MEtabolism modeling of primary sclerosing cholangitis DIsease progression. (arXiv:2310.01426v1 [q-bio.QM])

    [http://arxiv.org/abs/2310.01426](http://arxiv.org/abs/2310.01426)

    REMEDI是一个基于强化学习的框架，能够捕捉PSC疾病进展过程中胆汁酸动力学和机体自适应响应，有助于探索治疗方法。

    

    原发性硬化性胆管炎(PSC)是一种罕见的疾病，胆汁酸代谢的改变导致持续性肝脏损伤。本文引入了REMEDI，一个能够捕捉PSC进展过程中胆汁酸动力学和机体自适应响应的框架，可以帮助探索治疗方法。REMEDI将描述胆汁酸代谢的微分方程(Differential Equation, DE)机械模型与强化学习(RL)相结合，连续地模拟PSC中机体的自适应。自适应的目标是通过调节胆汁酸代谢相关的酶来维持稳态。这些酶对应于DE参数。REMEDI利用RL来近似PSC中的自适应，将稳态视为奖励信号，将DE参数的调整视为相应的动作。在真实世界数据上，REMEDI生成的胆汁酸动态和参数调整与已发表研究结果一致。此外，我们的结果支持文献中的讨论。

    Primary sclerosing cholangitis (PSC) is a rare disease wherein altered bile acid metabolism contributes to sustained liver injury. This paper introduces REMEDI, a framework that captures bile acid dynamics and the body's adaptive response during PSC progression that can assist in exploring treatments. REMEDI merges a differential equation (DE)-based mechanistic model that describes bile acid metabolism with reinforcement learning (RL) to emulate the body's adaptations to PSC continuously. An objective of adaptation is to maintain homeostasis by regulating enzymes involved in bile acid metabolism. These enzymes correspond to the parameters of the DEs. REMEDI leverages RL to approximate adaptations in PSC, treating homeostasis as a reward signal and the adjustment of the DE parameters as the corresponding actions. On real-world data, REMEDI generated bile acid dynamics and parameter adjustments consistent with published findings. Also, our results support discussions in the literature th
    
[^122]: Borges与AI

    Borges and AI. (arXiv:2310.01425v1 [cs.CL])

    [http://arxiv.org/abs/2310.01425](http://arxiv.org/abs/2310.01425)

    这篇论文主张通过探索乔治·路易斯·博尔赫斯的意象来理解大型语言模型和人工智能之间的关系。

    

    许多人认为大型语言模型（LLMs）开启了人工智能（AI）时代。一些人看到了机遇，而其他人则看到了危险。然而，支持者和反对者都通过科幻小说中流行的意象来理解AI。机器是否会变得有感知能力并反抗其创造者？我们是否会经历纸夹夹子启示？在回答这些问题之前，我们首先应该问一下，这种心理意象是否对手头的现象提供了良好的描述。仅通过神灵的情绪来理解天气模式的方法是有限的。相反，本文主张通过乔治·路易斯·博尔赫斯的意象来理解LLMs及其与AI的关系，博尔赫斯是20世纪文学大师，魔幻现实主义先驱和后现代文学的前奏。这种探索方式带来了新的视角，阐明了语言模型与人工智能之间的关系。

    Many believe that Large Language Models (LLMs) open the era of Artificial Intelligence (AI). Some see opportunities while others see dangers. Yet both proponents and opponents grasp AI through the imagery popularised by science fiction. Will the machine become sentient and rebel against its creators? Will we experience a paperclip apocalypse? Before answering such questions, we should first ask whether this mental imagery provides a good description of the phenomenon at hand. Understanding weather patterns through the moods of the gods only goes so far. The present paper instead advocates understanding LLMs and their connection to AI through the imagery of Jorge Luis Borges, a master of 20th century literature, forerunner of magical realism, and precursor to postmodern literature. This exercise leads to a new perspective that illuminates the relation between language modelling and artificial intelligence.
    
[^123]: AI生成文本检测工具的实证研究

    An Empirical Study of AI Generated Text Detection Tools. (arXiv:2310.01423v1 [cs.CL])

    [http://arxiv.org/abs/2310.01423](http://arxiv.org/abs/2310.01423)

    本研究实证研究了AI生成文本检测工具在多领域ChatGPT材料中的有效性，并创建了一个多领域数据集来测试最先进API和工具的能力。

    

    自从ChatGPT成为一种重要的AI生成文本模型以来，它在各种应用中（包括软件开发和维护）提供高质量的回应，吸引了许多人的兴趣。ChatGPT有很大的潜力，但其误用可能会带来严重问题，特别是在教育和公共安全领域。目前已经有几种AI生成文本检测工具可供使用，但它们都是在真实文本上进行测试的。然而，需要进一步研究它们对多领域ChatGPT材料的有效性。本研究旨在通过创建一个多领域数据集来测试用于检测大学和其他研究机构使用的最先进API和工具的能力。为此，创建了一个包含文章、摘要、故事、新闻和产品评论的大型数据集。第二步是使用新创建的数据集来测试六种工具的性能。

    Since ChatGPT has emerged as a major AIGC model, providing high-quality responses across a wide range of applications (including software development and maintenance), it has attracted much interest from many individuals. ChatGPT has great promise, but there are serious problems that might arise from its misuse, especially in the realms of education and public safety. Several AIGC detectors are available, and they have all been tested on genuine text. However, more study is needed to see how effective they are for multi-domain ChatGPT material. This study aims to fill this need by creating a multi-domain dataset for testing the state-of-the-art APIs and tools for detecting artificially generated information used by universities and other research institutions. A large dataset consisting of articles, abstracts, stories, news, and product reviews was created for this study. The second step is to use the newly created dataset to put six tools through their paces. Six different artificial 
    
[^124]: 视频推荐中鲁棒多臂赌博框架的设计原则

    Design Principles of Robust Multi-Armed Bandit Framework in Video Recommendations. (arXiv:2310.01419v1 [cs.IR])

    [http://arxiv.org/abs/2310.01419](http://arxiv.org/abs/2310.01419)

    本文提出了一种鲁棒多臂赌博框架的设计原则，以解决推荐系统中的利用挑战和分布变化问题，并通过实验证明了其相对收益提升。

    

    目前推荐系统中的多臂赌博方法更多关注有效的探索技术，而没有充分解决与分布变化和项目竞争相关的常见利用挑战。很少有工作指导设计能够解决推荐系统中这些频繁挑战的鲁棒赌博框架。在本文中，我们提出了一种新的设计原则，以使赌博模型对时变元数据信号鲁棒，对项目竞争少有影响，并防止由于数据稀疏性而导致权重波动。通过一系列实验，我们系统地研究了几个重要的赌博设计选择的影响。我们通过深入分析展示了我们提出的设计原则在使赌博模型对动态行为变化鲁棒方面的优势。值得注意的是，相对于不采用我们设计选择的基线赌博模型，我们展示了高达11.88％的改进相对收益。

    Current multi-armed bandit approaches in recommender systems (RS) have focused more on devising effective exploration techniques, while not adequately addressing common exploitation challenges related to distributional changes and item cannibalization. Little work exists to guide the design of robust bandit frameworks that can address these frequent challenges in RS. In this paper, we propose a new design principles to (i) make bandit models robust to time-variant metadata signals, (ii) less prone to item cannibalization, and (iii) prevent their weights fluctuating due to data sparsity. Through a series of experiments, we systematically examine the influence of several important bandit design choices. We demonstrate the advantage of our proposed design principles at making bandit models robust to dynamic behavioral changes through in-depth analyses. Noticeably, we show improved relative gain compared to a baseline bandit model not incorporating our design choices of up to $11.88\%$ and
    
[^125]: 利用预训练计算机视觉模型和异常扩散轨迹的Gramian Angular Fields

    Gramian Angular Fields for leveraging pretrained computer vision models with anomalous diffusion trajectories. (arXiv:2310.01416v1 [eess.IV])

    [http://arxiv.org/abs/2310.01416](http://arxiv.org/abs/2310.01416)

    利用Gramian Angular Fields方法，我们提出了一种新的数据驱动方法，用于处理异常扩散轨迹，并成功应用于多个领域，包括物理学、化学、生物学和生态学。

    

    异常扩散存在于从原子到大尺度的所有尺度上。一些典型的系统包括：超冷原子、细胞核中的端粒、水分在水泥基材料中的传输、节肢动物的自由运动和鸟类的迁徙模式。对扩散的表征提供了关于这些系统动力学的重要信息，并提供了一个跨学科的框架来研究扩散传输。因此，识别潜在的扩散机制并以高置信度推断异常扩散指数{$\alpha$}的问题对物理学、化学、生物学和生态学来说至关重要。在异常扩散挑战赛中，将机器学习技术和从轨迹中提取的统计信息相结合的原始轨迹的分类和分析已被广泛研究。我们在这里提出了一种新的数据驱动方法来处理扩散轨迹。该方法利用了Gramian Angular Fields (GAF)

    Anomalous diffusion is present at all scales, from atomic to large scales. Some exemplary systems are; ultra-cold atoms, telomeres in the nucleus of cells, moisture transport in cement-based materials, the free movement of arthropods, and the migration patterns of birds. The characterization of the diffusion gives critical information about the dynamics of these systems and provides an interdisciplinary framework with which to study diffusive transport. Thus, the problem of identifying underlying diffusive regimes and inferring the anomalous diffusion exponent {$\alpha$} with high confidence is critical to physics, chemistry, biology, and ecology. Classification and analysis of raw trajectories combining machine learning techniques with statistics extracted from them have widely been studied in the Anomalous Diffusion Challenge ge (Munoz-Gil et al., 2021). Here we present a new data-driven method for working with diffusive trajectories. This method utilizes Gramian Angular Fields (GAF)
    
[^126]: 表示工程化：AI透明化的自上而下方法

    Representation Engineering: A Top-Down Approach to AI Transparency. (arXiv:2310.01405v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.01405](http://arxiv.org/abs/2310.01405)

    这项研究介绍了一种名为表示工程化（RepE）的自上而下方法，通过借鉴认知神经科学的见解，提供了一种增强AI系统透明性的解决方案。该方法将集群级别的表示放在分析的核心，为监测和操纵深度神经网络中的高级认知现象提供了新的方法，并展示了在解决与安全相关的问题上的潜力。

    

    本文中，我们确定并描述了表示工程化（RepE）这一新兴领域，这是一种通过借鉴认知神经科学的见解来增强AI系统透明性的方法。RepE将集群级别的表示放在分析的核心，而不是神经元或电路，为我们提供了监测和操纵深度神经网络（DNNs）中高级认知现象的新方法。我们提供了RepE技术的基准和初步分析，显示它们提供了简单而有效的解决方案，用于改善我们对大型语言模型的理解和控制。我们展示了这些方法如何在包括诚实性、无害性、追求权力等一系列与安全相关的问题上发挥作用，展示了自上而下透明性研究的潜力。我们希望这项工作能够促进RepE的进一步探索，并推动AI系统的透明性和安全性的进步。

    In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.
    
[^127]: 关于基于训练的ChatGPT检测方法的泛化性研究

    On the Generalization of Training-based ChatGPT Detection Methods. (arXiv:2310.01307v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.01307](http://arxiv.org/abs/2310.01307)

    本研究综合调查了基于训练的ChatGPT检测方法在分布漂移下的泛化行为，包括提示、文本长度、主题和语言任务，在新数据集上揭示了有启示性的发现，为未来方法或数据的开发提供了指导。

    

    ChatGPT是最受欢迎的语言模型之一，在各种自然语言任务上表现出色。因此，有迫切的需求从人类编写的文本中检测出由ChatGPT生成的文本。一种广泛研究的方法是训练分类模型来区分二者。然而，现有研究也表明，在测试过程中，训练的模型可能会遭受分布漂移的影响，即它们对于预测未见过的语言任务或主题生成的文本是无效的。在这项工作中，我们旨在全面研究这些方法在由多种因素引起的分布漂移下的泛化行为，包括提示、文本长度、主题和语言任务。为实现这一目标，我们首先收集了一个包含人类和ChatGPT文本的新数据集，然后对收集的数据集进行了广泛的研究。我们的研究揭示了有启示性的发现，为未来方法或数据的开发提供了指导。

    ChatGPT is one of the most popular language models which achieve amazing performance on various natural language tasks. Consequently, there is also an urgent need to detect the texts generated ChatGPT from human written. One of the extensively studied methods trains classification models to distinguish both. However, existing studies also demonstrate that the trained models may suffer from distribution shifts (during test), i.e., they are ineffective to predict the generated texts from unseen language tasks or topics. In this work, we aim to have a comprehensive investigation on these methods' generalization behaviors under distribution shift caused by a wide range of factors, including prompts, text lengths, topics, and language tasks. To achieve this goal, we first collect a new dataset with human and ChatGPT texts, and then we conduct extensive studies on the collected dataset. Our studies unveil insightful findings which provide guidance for developing future methodologies or data 
    
[^128]: 使用语义推理实现更快更准确的神经网络

    Faster and Accurate Neural Networks with Semantic Inference. (arXiv:2310.01259v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.01259](http://arxiv.org/abs/2310.01259)

    本研究提出了一种名为语义推理（SINF）的新框架，在减少计算负载的同时，通过聚类语义相似的类来提取子图，从而减少深度神经网络的计算负担，并在性能上有限损失。

    

    深度神经网络通常具有显著的计算负担。虽然提出了结构化剪枝和专门用于移动设备的神经网络的方法，但它们会导致明显的准确率损失。在本文中，我们利用潜在表示中的内在冗余来减少计算负载，并在性能上有限损失。我们证明，语义上相似的输入共享许多滤波器，尤其是在较早的层次上。因此，可以对语义上相似的类进行聚类，以创建特定于聚类的子图。为此，我们提出了一个名为语义推理（SINF）的新框架。简而言之，SINF（i）使用一个小的附加分类器来识别对象属于的语义聚类，并（ii）执行与该语义聚类相关的基本DNN提取的子图进行推理。为了提取每个特定于聚类的子图，我们提出了一个名为区分能力得分（DCS）的新方法，用于找到具有区分能力的子图。

    Deep neural networks (DNN) usually come with a significant computational burden. While approaches such as structured pruning and mobile-specific DNNs have been proposed, they incur drastic accuracy loss. In this paper we leverage the intrinsic redundancy in latent representations to reduce the computational load with limited loss in performance. We show that semantically similar inputs share many filters, especially in the earlier layers. Thus, semantically similar classes can be clustered to create cluster-specific subgraphs. To this end, we propose a new framework called Semantic Inference (SINF). In short, SINF (i) identifies the semantic cluster the object belongs to using a small additional classifier and (ii) executes the subgraph extracted from the base DNN related to that semantic cluster for inference. To extract each cluster-specific subgraph, we propose a new approach named Discriminative Capability Score (DCS) that finds the subgraph with the capability to discriminate amon
    
[^129]: 用于评估中压电网可靠性的图同构网络

    Graph Isomorphic Networks for Assessing Reliability of the Medium-Voltage Grid. (arXiv:2310.01181v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.01181](http://arxiv.org/abs/2310.01181)

    本论文提出使用图同构网络（GINs）进行中压电网的n-1评估，相比传统数学优化方法，GIN方法展示了更快和更可靠的电网评估，将预测时间缩短约1000倍。

    

    随着向可再生能源的转变和传统容量的下降，确保电网的可靠性变得越来越具有挑战性。配电系统运营商（DSO）通过验证n-1原则来实现电网的可靠性，以确保组件故障时的连续运行。电力网络的复杂基于图的数据包含关键的n-1评估信息：图结构和有关节点/电缆的数据。与传统的机器学习方法不同，图神经网络（GNN）直接处理图结构化数据。本文提出在中压电网中使用图同构网络（GINs）进行n-1评估。GIN框架的设计是为了推广到未见过的电网，利用图结构和有关节点/电缆的数据。所提出的GIN方法比传统的数学优化方法展示了更快和更可靠的电网评估，将预测时间缩短约1000倍。

    Ensuring electricity grid reliability becomes increasingly challenging with the shift towards renewable energy and declining conventional capacities. Distribution System Operators (DSOs) aim to achieve grid reliability by verifying the n-1 principle, ensuring continuous operation in case of component failure. Electricity networks' complex graph-based data holds crucial information for n-1 assessment: graph structure and data about stations/cables. Unlike traditional machine learning methods, Graph Neural Networks (GNNs) directly handle graph-structured data. This paper proposes using Graph Isomorphic Networks (GINs) for n-1 assessments in medium voltage grids. The GIN framework is designed to generalise to unseen grids and utilise graph structure and data about stations/cables. The proposed GIN approach demonstrates faster and more reliable grid assessments than a traditional mathematical optimisation approach, reducing prediction times by approximately a factor of 1000. The findings o
    
[^130]: 有限秩核岭回归的测试误差的理论分析

    A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge Regression. (arXiv:2310.00987v1 [cs.LG])

    [http://arxiv.org/abs/2310.00987](http://arxiv.org/abs/2310.00987)

    本研究通过推导任意有限秩核岭回归模型的尖锐的非渐近性上界和下界，填补了有限秩核岭回归保证的空白。

    

    现有的对于一般核回归模型的统计学学习保证在使用有限秩核时往往会得到宽松的边界。然而，在几个机器学习问题中，如在执行迁移学习时，将预训练的深度神经网络的最后一层微调以适应新任务时，有限秩核会自然地出现。本文通过推导任意有限秩核岭回归模型的尖锐的非渐近性上界和下界，填补了有限秩核岭回归保证的空白。我们的边界比之前针对有限秩核岭回归模型推导的边界更紧，并且与类似结果不同的是，它们也适用于任何正则化参数。

    Existing statistical learning guarantees for general kernel regressors often yield loose bounds when used with finite-rank kernels. Yet, finite-rank kernels naturally appear in several machine learning problems, e.g.\ when fine-tuning a pre-trained deep neural network's last layer to adapt it to a novel task when performing transfer learning. We address this gap for finite-rank kernel ridge regression (KRR) by deriving sharp non-asymptotic upper and lower bounds for the KRR test error of any finite-rank KRR. Our bounds are tighter than previously derived bounds on finite-rank KRR, and unlike comparable results, they also remain valid for any regularization parameters.
    
[^131]: 硬件资源使用的概率学习中的路径结构多重边桥

    Path Structured Multimarginal Schr\"odinger Bridge for Probabilistic Learning of Hardware Resource Usage by Control Software. (arXiv:2310.00604v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2310.00604](http://arxiv.org/abs/2310.00604)

    该论文研究了硬件资源使用的概率学习，通过解决路径结构多重边桥问题，可以预测控制软件的硬件资源利用情况，并在任意时间实现线性收敛。

    

    路径结构多重边桥问题（MSBP）的解决方案是与观察到的概率测度序列或分布快照一致的最可能的测度值轨迹。我们利用最近在解决这种结构化的MSBP方面的算法进展，用于学习控制软件的随机硬件资源使用情况。该解决方案能够预测所需时间的硬件资源可用性的时变分布，并具有线性收敛性的保证。我们在模型预测控制软件执行案例研究中展示了我们的概率学习方法的效力。该方法迅速收敛到对控制器硬件资源利用的准确预测。该方法可广泛应用于任何软件，以预测任意时间的网络物理上下文依赖性性能。

    The solution of the path structured multimarginal Schr\"{o}dinger bridge problem (MSBP) is the most-likely measure-valued trajectory consistent with a sequence of observed probability measures or distributional snapshots. We leverage recent algorithmic advances in solving such structured MSBPs for learning stochastic hardware resource usage by control software. The solution enables predicting the time-varying distribution of hardware resource availability at a desired time with guaranteed linear convergence. We demonstrate the efficacy of our probabilistic learning approach in a model predictive control software execution case study. The method exhibits rapid convergence to an accurate prediction of hardware resource utilization of the controller. The method can be broadly applied to any software to predict cyber-physical context-dependent performance at arbitrary time.
    
[^132]: JoMA: 通过MLP和注意力的联合动力学来解密多层Transformer

    JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention. (arXiv:2310.00535v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00535](http://arxiv.org/abs/2310.00535)

    本文提出了联合MLP/注意力（JoMA）动态，用于解析多层Transformer架构的训练过程。通过预测非线性激活情况下注意力的行为，我们解释了多层Transformer中标记的层次组合方法。实验证实了我们的理论发现。

    

    我们提出了联合MLP/注意力（JoMA）动态，这是一种新颖的数学框架，用于理解多层Transformer架构的训练过程。通过在Transformer中去除自注意力层，我们得到仅包含MLP层的修改后动态。JoMA消除了先前分析中的不切实际的假设（例如缺乏残差连接），并预测注意力在非线性激活的情况下首先变得稀疏（为了学习重要的标记），然后变得密集（为了学习不那么重要的标记），而在线性情况下，它与现有研究一致，显示出注意力随时间变得稀疏。我们利用JoMA定性地解释了多层Transformer中如何将标记组合成层次结构，当输入标记是由潜在的层次生成模型生成时。在从现实世界数据集（Wikitext2/Wikitext103）训练的模型和各种预训练模型（OPT，Pythia）上进行的实验证实了我们的理论发现。

    We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings
    
[^133]: 关于生成模型在其自己的数据上迭代训练的稳定性研究

    On the Stability of Iterative Retraining of Generative Models on their own Data. (arXiv:2310.00429v1 [cs.LG])

    [http://arxiv.org/abs/2310.00429](http://arxiv.org/abs/2310.00429)

    本文研究了生成模型在混合数据集上训练对稳定性的影响，通过证明初始生成模型足够接近数据分布并且数据比例适当，迭代训练是稳定的。

    

    深度生成模型在建模复杂数据方面取得了巨大的进展，往往展现出超过典型人类能力的样本真实性辨别能力。这一成功的关键驱动力无疑是这些模型消耗海量网络规模数据的结果。由于这些模型惊人的性能和易得性，网络上将不可避免地出现越来越多的合成内容。这个事实直接意味着生成模型的未来迭代必须面对一个现实：它们的训练数据由清洁数据和先前模型生成的人工数据组成。在本文中，我们开发了一个框架来对混合数据集（包括真实数据和合成数据）上训练生成模型对稳定性的影响进行严格研究。我们首先证明了在初始生成模型足够好地近似数据分布并且真实数据与合成数据的比例适当的情况下，迭代训练的稳定性。

    Deep generative models have made tremendous progress in modeling complex data, often exhibiting generation quality that surpasses a typical human's ability to discern the authenticity of samples. Undeniably, a key driver of this success is enabled by the massive amounts of web-scale data consumed by these models. Due to these models' striking performance and ease of availability, the web will inevitably be increasingly populated with synthetic content. Such a fact directly implies that future iterations of generative models must contend with the reality that their training is curated from both clean data and artificially generated data from past models. In this paper, we develop a framework to rigorously study the impact of training generative models on mixed datasets (of real and synthetic data) on their stability. We first prove the stability of iterative training under the condition that the initial generative models approximate the data distribution well enough and the proportion o
    
[^134]: FedLPA: 使用分层后验聚合的个性化单次联邦学习

    FedLPA: Personalized One-shot Federated Learning with Layer-Wise Posterior Aggregation. (arXiv:2310.00339v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00339](http://arxiv.org/abs/2310.00339)

    本文提出了一种名为FedLPA的新颖方法，采用分层后验聚合的方式实现个性化单次联邦学习。FedLPA能够高效地将本地模型聚合到全局模型，解决了单次聚合在非相同训练数据分布下的性能问题。

    

    将本地客户端训练的神经网络高效地聚合到服务器上的全局模型是联邦学习中的一个广泛研究课题。最近，受到隐私问题减少、潜在攻击减弱和通信开销降低的推动，单次联邦学习（即将客户端与服务器间的通信限制为一轮）在研究者中越来越受欢迎。然而，单次聚合的性能容易受到非相同训练数据分布的影响，在一些实际场景中表现出高度的统计异质性。为了解决这个问题，我们提出了一种新颖的单次聚合方法——分层后验聚合（FedLPA）。FedLPA能够聚合本地模型，获得更准确的全局模型，而无需额外的辅助数据集或暴露任何机密的本地信息，比如标签分布。

    Efficiently aggregating trained neural networks from local clients into a global model on a server is a widely researched topic in federated learning. Recently, motivated by diminishing privacy concerns, mitigating potential attacks, and reducing the overhead of communication, one-shot federated learning (i.e., limiting client-server communication into a single round) has gained popularity among researchers. However, the one-shot aggregation performances are sensitively affected by the non-identical training data distribution, which exhibits high statistical heterogeneity in some real-world scenarios. To address this issue, we propose a novel one-shot aggregation method with Layer-wise Posterior Aggregation, named FedLPA. FedLPA aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any confidential local information, e.g., label distributions. To effectively capture the statistics maintained in the biased local datasets in
    
[^135]: CausalImages: 一个用于地球观测、生物医学和社会科学图像的因果推断的 R 包

    CausalImages: An R Package for Causal Inference with Earth Observation, Bio-medical, and Social Science Images. (arXiv:2310.00233v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00233](http://arxiv.org/abs/2310.00233)

    causalimages R 包提供了使用图像进行因果推断的新工具，并能够整合卫星和生物医学图像等新数据源，通过分解治疗效果的异质性和控制混淆变量来进行分析。

    

    causalimages R 包能够通过图像和图像序列数据进行因果推断，为将卫星和生物医学图像等新数据源整合到因果关系研究中提供新工具。一组函数可用于基于图像的因果推断分析。例如，一种关键函数利用可解释的贝叶斯框架通过图像来分解治疗效果的异质性。这样可以确定哪种类型的图像或图像序列对干预最敏感。第二个建模函数允许研究人员使用图像来控制混淆变量。该包还允许研究人员生成作为图像或视频内容向量摘要的嵌入。最后，还提供了基础结构函数，例如用于将大规模图像和图像序列数据写入序列化字节字符串以进行更快速的图像分析的工具。因此，causalimages 在 R 中开启了因果推断的新能力，让研究

    The causalimages R package enables causal inference with image and image sequence data, providing new tools for integrating novel data sources like satellite and bio-medical imagery into the study of cause and effect. One set of functions enables image-based causal inference analyses. For example, one key function decomposes treatment effect heterogeneity by images using an interpretable Bayesian framework. This allows for determining which types of images or image sequences are most responsive to interventions. A second modeling function allows researchers to control for confounding using images. The package also allows investigators to produce embeddings that serve as vector summaries of the image or video content. Finally, infrastructural functions are also provided, such as tools for writing large-scale image and image sequence data as sequentialized byte strings for more rapid image analysis. causalimages therefore opens new capabilities for causal inference in R, letting research
    
[^136]: 多分辨率主动学习的傅里叶神经算子

    Multi-Resolution Active Learning of Fourier Neural Operators. (arXiv:2309.16971v1 [cs.LG])

    [http://arxiv.org/abs/2309.16971](http://arxiv.org/abs/2309.16971)

    提出了一种多分辨率主动学习的傅里叶神经算子（MRA-FNO），通过动态选择输入函数和分辨率来降低数据成本并优化学习效率。

    

    傅里叶神经算子（FNO）是一种流行的算子学习框架，不仅在许多任务中实现了最先进的性能，而且在训练和预测方面高效。然而，在实践中，为FNO收集训练数据是一个昂贵的瓶颈，因为它经常需要进行昂贵的物理模拟。为了解决这个问题，我们提出了多分辨率主动学习的FNO（MRA-FNO），它能够动态选择输入函数和分辨率，尽量降低数据成本，同时优化学习效率。具体而言，我们提出了概率多分辨率FNO，并使用集成蒙特卡洛方法开发了一种有效的后验推理算法。为了进行主动学习，我们最大化效用成本比作为获取函数，在每一步获取新的样本和分辨率。我们使用矩匹配和矩阵行列式引理实现了可行，高效的效用计算。此外，我们还开发了一种方法来。

    Fourier Neural Operator (FNO) is a popular operator learning framework, which not only achieves the state-of-the-art performance in many tasks, but also is highly efficient in training and prediction. However, collecting training data for the FNO is a costly bottleneck in practice, because it often demands expensive physical simulations. To overcome this problem, we propose Multi-Resolution Active learning of FNO (MRA-FNO), which can dynamically select the input functions and resolutions to lower the data cost as much as possible while optimizing the learning efficiency. Specifically, we propose a probabilistic multi-resolution FNO and use ensemble Monte-Carlo to develop an effective posterior inference algorithm. To conduct active learning, we maximize a utility-cost ratio as the acquisition function to acquire new examples and resolutions at each step. We use moment matching and the matrix determinant lemma to enable tractable, efficient utility computation. Furthermore, we develop a
    
[^137]: ONNXExplainer:基于ONNX的通用框架，使用Shapley值解释神经网络

    ONNXExplainer: an ONNX Based Generic Framework to Explain Neural Networks Using Shapley Values. (arXiv:2309.16916v1 [cs.LG])

    [http://arxiv.org/abs/2309.16916](http://arxiv.org/abs/2309.16916)

    ONNXExplainer是一个基于ONNX的通用框架，使用Shapley值解释神经网络。它提供了自动微分和优化方法，实现了一次性部署和高效的解释计算。

    

    理解神经网络模型为什么会做出某些决策与推理性能一样重要。已经提出了各种方法来帮助解释神经网络模型的预测，其中Shapley值最受欢迎。SHAP包是解释使用TensorFlow或PyTorch实现的神经网络的Shapley值的领先实现，但缺乏跨平台支持、一次性部署且效率低下。为了解决这些问题，我们提出了ONNXExplainer，它是一个使用ONNX生态系统中的Shapley值来解释神经网络的通用框架。在ONNXExplainer中，我们开发了自己的自动微分和优化方法，不仅实现了神经网络推理和解释的一次性部署，还显著提高了解释的效率，并减少了内存消耗。为了公平比较目的，我们还在TensorFlow和PyTorch中实现了相同的优化。

    Understanding why a neural network model makes certain decisions can be as important as the inference performance. Various methods have been proposed to help practitioners explain the prediction of a neural network model, of which Shapley values are most popular. SHAP package is a leading implementation of Shapley values to explain neural networks implemented in TensorFlow or PyTorch but lacks cross-platform support, one-shot deployment and is highly inefficient. To address these problems, we present the ONNXExplainer, which is a generic framework to explain neural networks using Shapley values in the ONNX ecosystem. In ONNXExplainer, we develop its own automatic differentiation and optimization approach, which not only enables One-Shot Deployment of neural networks inference and explanations, but also significantly improves the efficiency to compute explanation with less memory consumption. For fair comparison purposes, we also implement the same optimization in TensorFlow and PyTorch
    
[^138]: 早期检测2型糖尿病患者白蛋白尿风险的监督学习模型

    Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients. (arXiv:2309.16742v1 [cs.LG])

    [http://arxiv.org/abs/2309.16742](http://arxiv.org/abs/2309.16742)

    该论文研究了2型糖尿病患者白蛋白尿的早期预测问题，并开发了一种监督学习模型。通过使用不同的监督学习算法对184条数据进行训练，得出了预测结果。

    

    糖尿病，尤其是2型糖尿病，仍然是一个重大的健康问题。与糖尿病相关的一个主要问题是其并发症的发展。糖尿病肾病是糖尿病的一种慢性并发症，不利地影响肾脏，导致肾脏损伤。诊断糖尿病肾病涉及考虑各种标准之一，其中之一是尿液中白蛋白的病理学病理学数量，称为白蛋白尿。因此，对糖尿病患者尿液中白蛋白尿的早期预测具有及时预防措施的潜力。本研究旨在开发一种监督学习模型，以预测2型糖尿病患者患有白蛋白尿的风险。所选的监督学习算法包括朴素贝叶斯，支持向量机（SVM），决策树，随机森林，AdaBoost，XGBoost和多层感知器（MLP）。我们的私有数据集包括184条糖尿病并发症风险因素的条目被用来训练算法

    Diabetes, especially T2DM, continues to be a significant health problem. One of the major concerns associated with diabetes is the development of its complications. Diabetic nephropathy, one of the chronic complication of diabetes, adversely affects the kidneys, leading to kidney damage. Diagnosing diabetic nephropathy involves considering various criteria, one of which is the presence of a pathologically significant quantity of albumin in urine, known as albuminuria. Thus, early prediction of albuminuria in diabetic patients holds the potential for timely preventive measures. This study aimed to develop a supervised learning model to predict the risk of developing albuminuria in T2DM patients. The selected supervised learning algorithms included Na\"ive Bayes, Support Vector Machine (SVM), decision tree, random forest, AdaBoost, XGBoost, and Multi-Layer Perceptron (MLP). Our private dataset, comprising 184 entries of diabetes complications risk factors, was used to train the algorithm
    
[^139]: 鲁棒的离线强化学习 - 认证置信区间

    Robust Offline Reinforcement Learning -- Certify the Confidence Interval. (arXiv:2309.16631v1 [cs.LG])

    [http://arxiv.org/abs/2309.16631](http://arxiv.org/abs/2309.16631)

    本文提出了一种使用随机平滑算法认证给定策略在离线环境中的鲁棒性的方法，证明了该算法的高效性，并得到实验证实。

    

    目前，强化学习（RL），特别是深度RL，在研究领域中得到了越来越多的关注。然而，由于攻击方式变得成熟，RL的安全性成为一个明显的问题。为了抵御此类对抗性攻击，已经开发了几种实用的方法，如对抗性训练、数据过滤等。然而，这些方法大多基于经验算法和实验，缺乏对算法鲁棒性的严格理论分析。本文提出了一种利用随机平滑认证给定策略的鲁棒性的算法，该算法的效率可以证明和进行，与没有随机平滑的算法一样高效。不同环境的实验证实了我们算法的正确性。

    Currently, reinforcement learning (RL), especially deep RL, has received more and more attention in the research area. However, the security of RL has been an obvious problem due to the attack manners becoming mature. In order to defend against such adversarial attacks, several practical approaches are developed, such as adversarial training, data filtering, etc. However, these methods are mostly based on empirical algorithms and experiments, without rigorous theoretical analysis of the robustness of the algorithms. In this paper, we develop an algorithm to certify the robustness of a given policy offline with random smoothing, which could be proven and conducted as efficiently as ones without random smoothing. Experiments on different environments confirm the correctness of our algorithm.
    
[^140]: 具有共享和个性化不确定表示的贝叶斯个性化联邦学习

    Bayesian Personalized Federated Learning with Shared and Personalized Uncertainty Representations. (arXiv:2309.15499v1 [cs.LG])

    [http://arxiv.org/abs/2309.15499](http://arxiv.org/abs/2309.15499)

    本文提出了一种贝叶斯个性化联邦学习（BPFL）的框架，用于处理联邦学习系统中的客户端不确定性和异质性。这个框架通过分解和共同学习统计异质性客户端数据上的共享和个性化不确定表示。

    

    贝叶斯个性化联邦学习（BPFL）解决了现有个性化联邦学习（PFL）面临的挑战。BPFL旨在通过处理客户端数据的统计异质性来量化客户端内部和跨客户端之间的不确定性和异质性。在PFL中，最近一些初步工作提出将隐藏的神经表示分解为共享和局部组件，并展示了有趣的结果。然而，大多数工作没有解决联邦学习系统中的客户端不确定性和异质性，同时适当地解耦神经表示是具有挑战性且常常是临时性的。在本文中，我们首次尝试引入一个通用的BPFL框架，通过分解和共同学习统计异质性客户端数据上的共享和个性化不确定表示。

    Bayesian personalized federated learning (BPFL) addresses challenges in existing personalized FL (PFL). BPFL aims to quantify the uncertainty and heterogeneity within and across clients towards uncertainty representations by addressing the statistical heterogeneity of client data. In PFL, some recent preliminary work proposes to decompose hidden neural representations into shared and local components and demonstrates interesting results. However, most of them do not address client uncertainty and heterogeneity in FL systems, while appropriately decoupling neural representations is challenging and often ad hoc. In this paper, we make the first attempt to introduce a general BPFL framework to decompose and jointly learn shared and personalized uncertainty representations on statistically heterogeneous client data over time. A Bayesian federated neural network BPFed instantiates BPFL by jointly learning cross-client shared uncertainty and client-specific personalized uncertainty over stat
    
[^141]: SEPT: 为运动预测的高效场景表示学习

    SEPT: Towards Efficient Scene Representation Learning for Motion Prediction. (arXiv:2309.15289v1 [cs.CV])

    [http://arxiv.org/abs/2309.15289](http://arxiv.org/abs/2309.15289)

    SEPT是一个利用自监督学习进行场景表示学习的建模框架，通过预训练的编码器捕捉轨迹的运动学特征、道路网络的空间结构以及道路和代理之间的交互作用，实现了在运动预测任务上的最先进性能。

    

    运动预测对于自动驾驶汽车在复杂交通环境中安全运行至关重要。提取交通元素之间的有效时空关系是准确预测的关键。本文受到预训练大型语言模型成功应用的启发，提出了SEPT，这是一个利用自监督学习来开发复杂交通场景中强大的时空理解能力的建模框架。具体而言，我们的方法涉及到在场景输入上进行三个掩码重构建模任务，包括代理路径和道路网络，预训练场景编码器以捕捉轨迹的运动学特征，道路网络的空间结构以及道路和代理之间的交互作用。预训练的编码器然后在下游预测任务上进行微调。大量实验证明，SEPT在Argoverse 1和Argoverse上无需精心设计的架构或手动特征工程，达到了最先进的性能水平。

    Motion prediction is crucial for autonomous vehicles to operate safely in complex traffic environments. Extracting effective spatiotemporal relationships among traffic elements is key to accurate forecasting. Inspired by the successful practice of pretrained large language models, this paper presents SEPT, a modeling framework that leverages self-supervised learning to develop powerful spatiotemporal understanding for complex traffic scenes. Specifically, our approach involves three masking-reconstruction modeling tasks on scene inputs including agents' trajectories and road network, pretraining the scene encoder to capture kinematics within trajectory, spatial structure of road network, and interactions among roads and agents. The pretrained encoder is then finetuned on the downstream forecasting task. Extensive experiments demonstrate that SEPT, without elaborate architectural design or manual feature engineering, achieves state-of-the-art performance on the Argoverse 1 and Argoverse
    
[^142]: 基于Transformer的医学咨询用户查询分类与专家特长相关的研究

    Tranformer-based classification of user queries for medical consultancy with respect to expert specialisation. (arXiv:2309.14662v1 [cs.LG])

    [http://arxiv.org/abs/2309.14662](http://arxiv.org/abs/2309.14662)

    本研究利用RuBERT模型和Transformer技术，提出了一种用于医学咨询的用户查询分类方法，重点关注专家特长，表现出超过92%的性能，具有良好的泛化性能和实际应用价值。

    

    在数字医疗时代，对于熟练的医疗支持的需求正在增长。本研究提出了一种创新策略，利用RuBERT模型，将医学咨询领域的用户查询进行分类，并着重关注专家的特长。通过利用Transformer模型的能力，我们在多样化的数据集上对预训练的RuBERT模型进行微调，实现了查询与特定医学专长之间的精确对应。通过使用全面的数据集，我们证明了我们的方法在交叉验证和传统的测试和训练集划分下均具有优秀的性能，F1得分超过92%。我们的方法在心脏病学、神经病学和皮肤科等医学领域的泛化性能也非常出色。这种方法提供了实际益处，可以将用户引导至适当的专家以获得及时而有针对性的医疗建议。它还提高了医疗系统的效率，减少了从业者的负担。

    The need for skilled medical support is growing in the era of digital healthcare. This research presents an innovative strategy, utilising the RuBERT model, for categorising user inquiries in the field of medical consultation with a focus on expert specialisation. By harnessing the capabilities of transformers, we fine-tuned the pre-trained RuBERT model on a varied dataset, which facilitates precise correspondence between queries and particular medical specialisms. Using a comprehensive dataset, we have demonstrated our approach's superior performance with an F1-score of over 92%, calculated through both cross-validation and the traditional split of test and train datasets. Our approach has shown excellent generalisation across medical domains such as cardiology, neurology and dermatology. This methodology provides practical benefits by directing users to appropriate specialists for prompt and targeted medical advice. It also enhances healthcare system efficiency, reduces practitioner 
    
[^143]: 无监督的图深度学习揭示了城市地区突发洪水风险概况

    Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas. (arXiv:2309.14610v1 [cs.LG])

    [http://arxiv.org/abs/2309.14610](http://arxiv.org/abs/2309.14610)

    本研究基于无监督图深度学习模型提出了集成城市洪水风险评级模型，能够捕捉区域之间的空间依赖关系和洪水危险与城市要素之间的复杂相互作用，揭示了城市地区的突发洪水风险概况

    

    城市洪水风险源于与洪水危险、洪水暴露以及社会和物理脆弱性相关的多个要素之间的复杂和非线性相互作用，以及复杂的空间洪水依赖关系。然而，现有的用于表征城市洪水风险的方法主要是基于洪水平原地图，侧重于有限数量的要素，主要是危险和暴露要素，没有考虑要素之间的相互作用或空间区域之间的依赖关系。为了填补这一空白，本研究提出了一种基于新颖的无监督图深度学习模型（称为FloodRisk-Net）的集成城市洪水风险评级模型。FloodRisk-Net能够捕捉区域之间的空间依赖关系以及洪水危险和城市要素之间的复杂和非线性相互作用，从而确定突发洪水风险。利用美国多个都市统计区（MSAs）的数据，该模型将它们的洪水风险特征化为

    Urban flood risk emerges from complex and nonlinear interactions among multiple features related to flood hazard, flood exposure, and social and physical vulnerabilities, along with the complex spatial flood dependence relationships. Existing approaches for characterizing urban flood risk, however, are primarily based on flood plain maps, focusing on a limited number of features, primarily hazard and exposure features, without consideration of feature interactions or the dependence relationships among spatial areas. To address this gap, this study presents an integrated urban flood-risk rating model based on a novel unsupervised graph deep learning model (called FloodRisk-Net). FloodRisk-Net is capable of capturing spatial dependence among areas and complex and nonlinear interactions among flood hazards and urban features for specifying emergent flood risk. Using data from multiple metropolitan statistical areas (MSAs) in the United States, the model characterizes their flood risk into
    
[^144]: 在序列长度上并行化非线性顺序模型

    Parallelizing non-linear sequential models over the sequence length. (arXiv:2309.12252v1 [cs.LG])

    [http://arxiv.org/abs/2309.12252](http://arxiv.org/abs/2309.12252)

    本论文提出了一种并行算法，能够加速GPU对于顺序模型的评估速度，提高了3个数量级，而不降低输出准确性。该算法适用于各种架构，并在长时间序列分类问题中发现了门控循环单元的有效性。

    

    顺序模型，例如循环神经网络和神经常微分方程，在训练过程中一直由于其本质上的顺序特性而存在训练缓慢的问题。多年来这个瓶颈一直存在，因为很多人认为顺序模型无法并行化。我们通过并行算法挑战了这个长期以来的信念，加速了GPU对于顺序模型的评估速度，速度提高了3个数量级，而不牺牲输出准确性。该算法不需要顺序模型架构中的任何特殊结构，适用于各种架构。使用我们的方法，训练顺序模型可以比常规的顺序方法快10倍以上，而训练结果没有明显差异。借助这种加速训练，我们在一个包含17k个时间样本的长时间序列分类问题中发现了门控循环单元的有效性。通过克服训练瓶颈，我们的工作使得顺序模型的训练更加高效。

    Sequential models, such as Recurrent Neural Networks and Neural Ordinary Differential Equations, have long suffered from slow training due to their inherent sequential nature. For many years this bottleneck has persisted, as many thought sequential models could not be parallelized. We challenge this long-held belief with our parallel algorithm that accelerates GPU evaluation of sequential models by up to 3 orders of magnitude faster without compromising output accuracy. The algorithm does not need any special structure in the sequential models' architecture, making it applicable to a wide range of architectures. Using our method, training sequential models can be more than 10 times faster than the common sequential method without any meaningful difference in the training results. Leveraging this accelerated training, we discovered the efficacy of the Gated Recurrent Unit in a long time series classification problem with 17k time samples. By overcoming the training bottleneck, our work 
    
[^145]: 通过差分神经计算，智能机器在无结构环境中工作

    Intelligent machines work in unstructured environments by differential neural computing. (arXiv:2309.08835v1 [eess.SP])

    [http://arxiv.org/abs/2309.08835](http://arxiv.org/abs/2309.08835)

    本研究提出了一种基于差分神经计算的智能机器方法，通过提取环境信息的主要特征并应用相应的编码刺激到记忆阻性器件，成功地实现了处理无结构环境信息的类人能力，并展现了良好的可扩展性和泛化性。该方法在物体抓取和自动驾驶等应用方面得到了验证。

    

    希望智能机器能够在现实世界中高效地工作，需要一种新的方法来准确地理解未知环境中的无结构信息，具有良好的准确性、可扩展性和泛化性，就像人类一样。本文介绍了一种基于记忆阻性神经计算的感知信号差分处理和学习方法，通过提取环境信息的主要特征并应用相关编码刺激到记忆阻性器件，我们成功地获得了处理无结构环境信息的类人能力，如机械刺激的放大（>720%）和适应（<50%）。该方法还展现了良好的可扩展性和泛化性，在智能机器的两个典型应用中得到了验证：物体抓取和自动驾驶。在物体抓取方面，通过在1毫秒内使用单个记忆阻性器件学习未知物体特征（例如尖锐的角和光滑的表面），一个机器手实现了安全稳定的抓取。

    Expecting intelligent machines to efficiently work in real world requires a new method to understand unstructured information in unknown environments with good accuracy, scalability and generalization, like human. Here, a memristive neural computing based perceptual signal differential processing and learning method for intelligent machines is presented, via extracting main features of environmental information and applying associated encoded stimuli to memristors, we successfully obtain human-like ability in processing unstructured environmental information, such as amplification (>720%) and adaptation (<50%) of mechanical stimuli. The method also exhibits good scalability and generalization, validated in two typical applications of intelligent machines: object grasping and autonomous driving. In the former, a robot hand experimentally realizes safe and stable grasping, through learning unknown object features (e.g., sharp corner and smooth surface) with a single memristor in 1 ms. In
    
[^146]: 探索神经网络坍塌现象：批归一化和权重衰减的影响

    Towards Understanding Neural Collapse: The Effects of Batch Normalization and Weight Decay. (arXiv:2309.04644v1 [cs.LG])

    [http://arxiv.org/abs/2309.04644](http://arxiv.org/abs/2309.04644)

    本文研究了批归一化和权重衰减对神经网络坍塌现象的影响，并提出了对坍塌现象进行度量的方法。通过理论和实验，证明了在接近最优时，批归一化和权重衰减能够促使神经网络坍塌的出现。

    

    神经网络坍塌是最近观察到的一种在神经网络分类器最后一层中出现的几何结构。具体来说，神经网络坍塌指的是在神经网络训练的终端阶段，1）最后一层特征的类内变异趋向于零，2）类特征均值构成等角紧框架（ETF），3）最后一层类特征和权重在缩放上相等，4）分类行为崩溃到最近的类中心（NCC）决策规则。本文研究了批归一化和权重衰减对神经网络坍塌现象的影响。我们提出了几何直观的类内和类间余弦相似度度量，捕捉了神经网络坍塌的多个核心方面。利用这个度量，我们在正则化交叉熵损失接近最优时，提供了关于批归一化和权重衰减下神经网络坍塌的理论保证。我们还进行了进一步的实验。

    Neural Collapse is a recently observed geometric structure that emerges in the final layer of neural network classifiers. Specifically, Neural Collapse states that at the terminal phase of neural networks training, 1) the intra-class variability of last-layer features tends to zero, 2) the class feature means form an Equiangular Tight Frame (ETF), 3) last-layer class features and weights becomes equal up the scaling, and 4) classification behavior collapses to the nearest class center (NCC) decision rule. This paper investigates the effect of batch normalization and weight decay on the emergence of Neural Collapse. We propose the geometrically intuitive intra-class and inter-class cosine similarity measure which captures multiple core aspects of Neural Collapse. With this measure, we provide theoretical guarantees of Neural Collapse emergence with last-layer batch normalization and weight decay when the regularized cross-entropy loss is near optimal. We also perform further experiments
    
[^147]: 非对称动量：对梯度下降的重新思考

    Asymmetric Momentum: A Rethinking of Gradient Descent. (arXiv:2309.02130v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.02130](http://arxiv.org/abs/2309.02130)

    Loss-Controlled Asymmetric Momentum (LCAM) is proposed as a simple and versatile optimization method that can adapt to all types of datasets by dividing the training process into different loss phases and using different momentum. Experimental results suggest that frequently-changing parameters should be accelerated in non-sparse gradients, challenging the conventional wisdom.

    

    通过理论和实验证明，与Adam等现有的自适应方法相比，我们提出了最简单的SGD增强方法——Loss-Controlled Asymmetric Momentum（LCAM）。通过对损失进行平均，我们将训练过程分为不同的损失阶段，并使用不同的动量。它不仅可以加速稀疏梯度下的缓慢变化参数，类似于自适应优化器，还可以选择在非稀疏梯度下加速频繁变化的参数，从而适应所有类型的数据集。我们通过权重耦合和权重牵引的概念对机器学习训练过程进行重新解释，并通过实验证明权重具有方向特异性，与数据集的特异性相关。因此，有趣的是，我们观察到在非稀疏梯度情况下，实际上应该加速频繁变化的参数。

    Through theoretical and experimental validation, unlike all existing adaptive methods like Adam which penalize frequently-changing parameters and are only applicable to sparse gradients, we propose the simplest SGD enhanced method, Loss-Controlled Asymmetric Momentum(LCAM). By averaging the loss, we divide training process into different loss phases and using different momentum. It not only can accelerates slow-changing parameters for sparse gradients, similar to adaptive optimizers, but also can choose to accelerates frequently-changing parameters for non-sparse gradients, thus being adaptable to all types of datasets. We reinterpret the machine learning training process through the concepts of weight coupling and weight traction, and experimentally validate that weights have directional specificity, which are correlated with the specificity of the dataset. Thus interestingly, we observe that in non-sparse gradients, frequently-changing parameters should actually be accelerated, which
    
[^148]: 选择性场景文本去除

    Selective Scene Text Removal. (arXiv:2309.00410v1 [cs.CV])

    [http://arxiv.org/abs/2309.00410](http://arxiv.org/abs/2309.00410)

    本论文提出了一种选择性场景文本去除（SSTR）任务，该任务可以根据用户指定的目标词汇来去除场景图像中的文本，实验结果表明所提出的方法有效地实现了目标词汇的去除。

    

    场景文本去除（Scene text removal，STR）是一种图像转换任务，用于去除场景图像中的文本区域。传统的STR方法会删除所有的场景文本。这意味着现有的方法无法选择要删除的文本。本文提出了一种名为选择性场景文本去除（Selective scene text removal，SSTR）的新任务设置，只删除用户指定的目标词汇。虽然SSTR比STR更复杂，但是我们提出的多模块结构使得SSTR的训练更加高效。实验结果表明，所提出的方法可以如预期地去除目标词汇。

    Scene text removal (STR) is the image transformation task to remove text regions in scene images. The conventional STR methods remove all scene text. This means that the existing methods cannot select text to be removed. In this paper, we propose a novel task setting named selective scene text removal (SSTR) that removes only target words specified by the user. Although SSTR is a more complex task than STR, the proposed multi-module structure enables efficient training for SSTR. Experimental results show that the proposed method can remove target words as expected.
    
[^149]: 统一的数据管理和综合性能评估用于城市时空预测[实验，分析和基准]的论文

    Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis & Benchmark]. (arXiv:2308.12899v1 [cs.LG])

    [http://arxiv.org/abs/2308.12899](http://arxiv.org/abs/2308.12899)

    该论文提出了针对城市时空预测的统一数据管理和综合性能评估方法。其贡献包括引入“原子文件”作为统一存储格式，提供了城市时空预测模型技术进展的全面概述，并建立了性能排行榜和鉴定了有潜力的模型和数据集。

    

    随着深度学习技术的发展和大规模数据集的可用性，城市时空预测领域正在迅速发展。然而，从不同来源和以不同格式存储的多样化城市时空数据的访问和利用仍然存在挑战，而在深度学习模型大量增加的情况下，确定有效的模型结构和组件也是一个挑战。本文解决了这些挑战，并提供了三个重要的贡献。首先，我们引入了“原子文件”，这是一种为城市时空大数据设计的统一存储格式，并在40个不同的数据集上验证了其有效性，简化了数据管理。其次，我们全面概述了城市时空预测模型的技术进展，指导了强大模型的发展。第三，我们使用不同的模型和数据集进行了大量实验，建立了性能排行榜并确定了有潜力的模型和数据集。

    The field of urban spatial-temporal prediction is advancing rapidly with the development of deep learning techniques and the availability of large-scale datasets. However, challenges persist in accessing and utilizing diverse urban spatial-temporal datasets from different sources and stored in different formats, as well as determining effective model structures and components with the proliferation of deep learning models. This work addresses these challenges and provides three significant contributions. Firstly, we introduce "atomic files", a unified storage format designed for urban spatial-temporal big data, and validate its effectiveness on 40 diverse datasets, simplifying data management. Secondly, we present a comprehensive overview of technological advances in urban spatial-temporal prediction models, guiding the development of robust models. Thirdly, we conduct extensive experiments using diverse models and datasets, establishing a performance leaderboard and identifying promis
    
[^150]: 通过再生性正则化维持可塑性

    Maintaining Plasticity via Regenerative Regularization. (arXiv:2308.11958v1 [cs.LG])

    [http://arxiv.org/abs/2308.11958](http://arxiv.org/abs/2308.11958)

    本文提出了一种名为L2 Init的简单方法，通过将L2正则化应用于初始参数，来维持神经网络在处理非平稳数据流时的可塑性且易于实施。该方法使得参数能够迅速适应新任务并减轻可塑性的丢失。

    

    在连续学习中，可塑性指的是代理快速适应新信息的能力。已知神经网络在处理非平稳数据流时会失去可塑性。本文提出了一种名为L2 Init的非常简单的方法，通过将L2正则化应用于初始参数，来维持可塑性。这与标准的L2正则化非常相似，唯一的区别在于L2 Init正则化朝向原点。L2 Init易于实施，只需要选择一个超参数。这个方法的动机与重置神经元或参数值的方法相同。直观上讲，当最近的损失对特定参数不敏感时，这些参数会向它们的初始值漂移。这使得参数能够迅速适应新任务。在代表连续学习中不同类型非平稳性的简单问题上，我们证明了L2 Init能够一致地减轻可塑性的丢失。

    In continual learning, plasticity refers to the ability of an agent to quickly adapt to new information. Neural networks are known to lose plasticity when processing non-stationary data streams. In this paper, we propose L2 Init, a very simple approach for maintaining plasticity by incorporating in the loss function L2 regularization toward initial parameters. This is very similar to standard L2 regularization (L2), the only difference being that L2 regularizes toward the origin. L2 Init is simple to implement and requires selecting only a single hyper-parameter. The motivation for this method is the same as that of methods that reset neurons or parameter values. Intuitively, when recent losses are insensitive to particular parameters, these parameters drift toward their initial values. This prepares parameters to adapt quickly to new tasks. On simple problems representative of different types of nonstationarity in continual learning, we demonstrate that L2 Init consistently mitigates 
    
[^151]: 利用可接受边界进行启发式学习

    Utilizing Admissible Bounds for Heuristic Learning. (arXiv:2308.11905v1 [cs.AI])

    [http://arxiv.org/abs/2308.11905](http://arxiv.org/abs/2308.11905)

    本文通过将可接受启发式作为截断高斯分布的参数，明确了在监督启发式学习中可接受启发式的作用，紧缩了假设空间。

    

    虽然利用现代机器学习技术学习前向搜索算法的启发式函数近年来受到了关注，但对于它们应该学习的内容、如何训练以及为什么这样做的理论认识还很少。这种理解的不足导致文献中进行数据集选择（次优成本对最优成本或可接受对不可接受启发式）和优化指标（例如平方误差和绝对误差）时进行了临时选择。此外，由于所得到的训练启发式函数缺乏可接受性，对于学习过程中可接受性的重要性也缺乏关注。本文通过将可接受启发式作为截断高斯分布的参数，明确了在监督启发式学习中可接受启发式的作用，相比普通高斯分布，紧缩了假设空间。我们认为这个数学模型忠实地遵循了最大熵原则。

    While learning a heuristic function for forward search algorithms with modern machine learning techniques has been gaining interest in recent years, there has been little theoretical understanding of \emph{what} they should learn, \emph{how} to train them, and \emph{why} we do so. This lack of understanding leads to various literature performing an ad-hoc selection of datasets (suboptimal vs optimal costs or admissible vs inadmissible heuristics) and optimization metrics (e.g., squared vs absolute errors). Moreover, due to the lack of admissibility of the resulting trained heuristics, little focus has been put on the role of admissibility \emph{during} learning. This paper articulates the role of admissible heuristics in supervised heuristic learning using them as parameters of Truncated Gaussian distributions, which tightens the hypothesis space compared to ordinary Gaussian distributions. We argue that this mathematical model faithfully follows the principle of maximum entropy and em
    
[^152]: 使用大型语言模型进行数据竞争检测

    Data Race Detection Using Large Language Models. (arXiv:2308.07505v1 [cs.LG])

    [http://arxiv.org/abs/2308.07505](http://arxiv.org/abs/2308.07505)

    本研究探索了一种基于大型语言模型的数据竞争检测方法，通过结合提示工程和微调技术，发现LLMs可以作为数据竞争检测的可行方法，但在需要详细信息时仍不如传统工具竞争。

    

    大型语言模型（LLMs）作为一种替代策略，展示了在分析和优化高性能计算程序方面的显著优势，避免了资源密集型手动工具的创建。本文中，我们探讨了一种新颖的基于LLM的数据竞争检测方法，结合了提示工程和微调技术。我们创建了一个名为DRB-ML的专用数据集，该数据集源自DataRaceBench，并具有精细的标签，显示了数据竞争对及其相关变量、行号和读/写信息的存在。然后，我们使用DRB-ML评估了代表性的LLMs并微调了开源模型。我们的实验证明，LLMs可以作为数据竞争检测的可行方法。然而，当我们需要有关引起数据竞争的变量对的详细信息时，它们仍无法与传统的数据竞争检测工具竞争。

    Large language models (LLMs) are demonstrating significant promise as an alternate strategy to facilitate analyses and optimizations of high-performance computing programs, circumventing the need for resource-intensive manual tool creation. In this paper, we explore a novel LLM-based data race detection approach combining prompting engineering and fine-tuning techniques. We create a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with fine-grain labels showing the presence of data race pairs and their associated variables, line numbers, and read/write information. DRB-ML is then used to evaluate representative LLMs and fine-tune open-source ones. Our experiment shows that LLMs can be a viable approach to data race detection. However, they still cannot compete with traditional data race detection tools when we need detailed information about variable pairs causing data races.
    
[^153]: 基于深度学习的图像水印技术：简要调查

    Deep Learning based Image Watermarking: A Brief Survey. (arXiv:2308.04603v1 [cs.MM])

    [http://arxiv.org/abs/2308.04603](http://arxiv.org/abs/2308.04603)

    该论文调查了基于深度学习的图像水印技术的最新研究进展，将其分为嵌入器-提取器联合训练、深度网络作为特征变换和混合方案。对每个类别的研究方向进行了分析和总结，并讨论了未来的研究方向。

    

    图像水印是指在一张封面图像中秘密嵌入和提取水印以保护图像的行为。近年来，基于深度学习的图像水印技术层出不穷。为了研究最新的技术，本调查将前沿的基于深度学习的图像水印技术分为嵌入器-提取器联合训练、深度网络作为特征变换和混合方案。还分析和总结了每个类别中的研究方向。此外，还讨论了潜在的未来研究方向，展望未来的研究。

    The act of secretly embedding and extracting a watermark on a cover image to protect it is known as image watermarking. In recent years, deep learning-based image watermarking techniques have been emerging one after another. To study the state-of-the-art, this survey categorizes cutting-edge deep learning-based image watermarking techniques into Embedder-Extractor Joint Training, Deep Networks as a Feature Transformation, and Hybrid schemes. Research directions in each category are also analyzed and summarized. Additionally, potential future research directions are discussed to envision future studies.
    
[^154]: ToolLLM: 促进大型语言模型掌握16000多个真实世界API

    ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. (arXiv:2307.16789v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2307.16789](http://arxiv.org/abs/2307.16789)

    ToolLLM是一个通用的工具使用框架，旨在促进大型语言模型掌握16000多个真实世界API。该框架包括数据构建、模型训练和评估。使用ChatGPT自动构建的ToolBench数据集用于指令调整，涵盖了各种工具使用情境和API。

    

    尽管开源的大型语言模型（LLM）如LLaMA的进展，但它们在工具使用能力方面仍然受到严重限制，即使用外部工具（API）来满足人类指令。原因是当前的指令调整主要集中在基本语言任务上，但忽略了工具使用领域。这与最先进的闭源LLM（如ChatGPT）的出色工具使用能力形成对比。为了弥补这一差距，我们介绍了ToolLLM，一个包括数据构建、模型训练和评估的通用工具使用框架。我们首先介绍了ToolBench，一个用于工具使用的指令调整数据集，该数据集是使用ChatGPT自动构建的。具体而言，构建可以分为三个阶段：（i）API收集：我们从RapidAPI Hub收集了16464个真实世界的RESTful API，涵盖了49个类别；（ii）指令生成：我们提示ChatGPT生成涉及这些API的多样化指令，涵盖了单工具和多工具使用场景。

    Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-too
    
[^155]: 深度学习与自适应滤波：Stein无偏风险估计方法的应用

    Deep Learning Meets Adaptive Filtering: A Stein's Unbiased Risk Estimator Approach. (arXiv:2307.16708v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2307.16708](http://arxiv.org/abs/2307.16708)

    本文通过算法展开的方法，将递归最小二乘法（RLS）和等变自适应源分离（EASI）两种算法转化为深度神经网络的层，通过利用训练过程有效地进行源信号估计。同时，通过使用基于Stein无偏风险估计（SURE）的损失函数训练，进一步提高了性能，实证评估证明了该方法的有效性。

    

    本文通过算法展开的视角重新审视了两种著名的自适应滤波算法，即递归最小二乘法（RLS）和等变自适应源分离（EASI），在源估计和分离的环境中。在展开方法的基础上，我们引入了新的基于任务的深度学习框架，称为Deep RLS和Deep EASI。这些架构将原始算法的迭代变换为深度神经网络的层，从而通过利用训练过程有效地进行源信号估计。为了进一步提高性能，我们提出使用基于Stein无偏风险估计（SURE）的损失函数对这些深度展开网络进行训练。我们的实证评估证明了这种基于SURE的方法对于增强源信号估计的有效性。

    This paper revisits two prominent adaptive filtering algorithms through the lens of algorithm unrolling, namely recursive least squares (RLS) and equivariant adaptive source separation (EASI), in the context of source estimation and separation. Building upon the unrolling methodology, we introduce novel task-based deep learning frameworks, denoted as Deep RLS and Deep EASI. These architectures transform the iterations of the original algorithms into layers of a deep neural network, thereby enabling efficient source signal estimation by taking advantage of a training process. To further enhance performance, we propose training these deep unrolled networks utilizing a loss function grounded on a Stein's unbiased risk estimator (SURE). Our empirical evaluations demonstrate the efficacy of this SURE-based approach for enhanced source signal estimation.
    
[^156]: 腐败鲁棒的Lipschitz上下文搜索

    Corruption-Robust Lipschitz Contextual Search. (arXiv:2307.13903v1 [cs.LG])

    [http://arxiv.org/abs/2307.13903](http://arxiv.org/abs/2307.13903)

    该论文研究了学习具有被篡改的二进制信号的Lipschitz函数的问题，提出了一种腐败鲁棒算法。该算法在不同损失函数下实现了不同程度的后悔。

    

    我研究了学习具有被篡改的二进制信号的Lipschitz函数的问题。学习者试图学习一个由对手选择的Lipschitz函数$f$。在每一轮中，对手在输入空间中选择一个上下文向量$x_t$，学习者对真实函数值$f(x_t)$进行猜测，并接收一个指示猜测是高还是低的二进制信号。在总共$C$轮中，信号可能被篡改，但学习者不知道$C$的值。学习者的目标是造成小的累积损失。我提出了一个自然而强大的技术验证，对设计腐败鲁棒算法非常有用。我设计了一些算法（将Lipschitz参数$L$视为常数）：对于对称损失，学习者在$d=1$时达到后悔$O(C\log T)$，在$d>1$时达到后悔$O_d(C\log T + T^{(d-1)/d})$；对于计价损失，学习者在$d/(d+1)$时达到后悔$\widetilde{O}(T^{d/(d+1)} + C\cdot T^{1/(d+1)})$。

    I study the problem of learning a Lipschitz function with corrupted binary signals. The learner tries to learn a Lipschitz function $f$ that the adversary chooses. In each round, the adversary selects a context vector $x_t$ in the input space, and the learner makes a guess to the true function value $f(x_t)$ and receives a binary signal indicating whether the guess was high or low. In a total of $C$ rounds, the signal may be corrupted, though the value of $C$ is unknown to the learner. The learner's goal is to incur a small cumulative loss. I present a natural yet powerful technique sanity check, which proves useful in designing corruption-robust algorithms. I design algorithms which (treating the Lipschitz parameter $L$ as constant): for the symmetric loss, the learner achieves regret $O(C\log T)$ with $d = 1$ and $O_d(C\log T + T^{(d-1)/d})$ with $d > 1$; for the pricing loss the learner achieves regret $\widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.
    
[^157]: 一种具有规划、长期上下文理解和程序合成能力的现实世界WebAgent

    A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. (arXiv:2307.12856v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12856](http://arxiv.org/abs/2307.12856)

    这篇论文介绍了一种名为WebAgent的LLM驱动代理，通过自我经验学习，在真实网站上完成任务。该方法通过规划、总结和生成代码来提高在真实网站上的成功率。

    

    最近，预训练的大型语言模型（LLMs）在自主Web自动化方面取得了更好的泛化性能和样本效率。然而，在真实世界的网站上，性能仍然受到三个方面的限制：开放领域性、有限的上下文长度和对HTML的归纳偏差的缺乏。我们介绍了一种名为WebAgent的LLM驱动代理，它通过自我经验学习，在遵循自然语言指令的前提下，在真实网站上完成任务。WebAgent通过将指令分解为规范的子指令，将长HTML文档总结为与任务相关的片段，并通过从中生成的Python程序对网站进行操作来提前进行规划。我们使用Flan-U-PaLM设计了WebAgent，用于生成有根代码，并使用HTML-T5进行预训练LLMs，利用局部和全局注意机制以及混合长跨度去噪目标来进行规划和总结。我们通过实验证明，我们的模块化方法提高了在真实网站上的成功率。

    Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by ov
    
[^158]: 大型语言模型中的上下文学习在学习标签关系上具有创新，但并非传统学习方法

    In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning. (arXiv:2307.12375v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.12375](http://arxiv.org/abs/2307.12375)

    大型语言模型（LLMs）在包含标签关系示例的上下文中的学习能力使其在下游任务中表现显著提高，但与传统学习方法不同。我们研究了上下文示例中的标签如何影响预测、预训练中学习到的标签关系如何与上下文示例相互作用以及上下文学习如何聚合标签信息。研究结果揭示了LLMs的工作机制及其对上下文信息的处理方式。

    

    在下游任务中，大型语言模型（LLMs）的性能在包含输入-标签关系示例的上下文中通常显著提高。然而，目前对LLMs的这种上下文学习（ICL）能力的工作机制尚无共识：例如，虽然Xie等人（2021年）将ICL比作一种通用学习算法，但Min等人（2022b年）认为ICL甚至不能从上下文示例中学习标签关系。在本文中，我们研究了以下三个问题：（1）上下文示例的标签如何影响预测结果，（2）预训练期间学习到的标签关系如何与上下文中提供的输入-标签示例相互作用，以及（3）ICL如何聚合来自上下文示例的标签信息。我们的研究发现，LLMs通常会整合上下文标签的信息，但预训练和上下文标签关系被区别对待，模型不会将所有上下文信息等同对待。我们的结果揭示了对LLMs的理解。

    The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into underst
    
[^159]: 图像和声音的滥用用于在多模态LLMs中进行间接指令注入

    (Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])

    [http://arxiv.org/abs/2307.10490](http://arxiv.org/abs/2307.10490)

    本论文展示了如何利用图像和声音在多模态LLMs中进行间接指令注入，攻击者通过生成对抗扰动并将其融入图像或音频录音中，以操纵模型输出特定文本和指导对话的行为。

    

    我们展示了如何利用图像和声音在多模态LLMs中进行间接提示和指令注入。攻击者生成与提示相对应的对抗扰动，并将其融入图像或音频录音中。当用户向（未修改的良性）模型询问被扰动的图像或音频时，扰动会引导模型输出攻击者选择的文本和/或使后续对话遵循攻击者的指令。我们用几个概念验证示例针对LLaVa和PandaGPT来说明这种攻击。

    We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
    
[^160]: 基于元学习的速率分割多址(Multiple Access)的预编码优化框架

    A Meta-Learning Based Precoder Optimization Framework for Rate-Splitting Multiple Access. (arXiv:2307.08822v1 [eess.SP])

    [http://arxiv.org/abs/2307.08822](http://arxiv.org/abs/2307.08822)

    本文提出了一种基于元学习的预编码优化框架，通过利用紧凑神经网络过拟合来优化Rate-Splitting Multiple Access (RSMA)预编码，结合部分信道状态信息，从而绕过了其他训练数据的需求，同时在中等规模情况下达到了类似传统方法的平均和速率性能，在大规模情况下显著优于低复杂度算法。

    

    在这篇论文中，我们提出了一种基于元学习的预编码优化框架，来直接优化具有部分发射机信道状态信息的Rate-Splitting Multiple Access (RSMA)预编码。通过利用紧凑神经网络过拟合来最大化明确的平均和速率表达式，我们有效地绕过了对任何其他训练数据的需求，同时最小化了总运行时间。数值结果表明，基于元学习的解决方案在中等规模情况下达到了类似传统预编码优化的平均和速率性能，并在大规模情况下显著优于次优的低复杂度预编码算法。

    In this letter, we propose the use of a meta-learning based precoder optimization framework to directly optimize the Rate-Splitting Multiple Access (RSMA) precoders with partial Channel State Information at the Transmitter (CSIT). By exploiting the overfitting of the compact neural network to maximize the explicit Average Sum-Rate (ASR) expression, we effectively bypass the need for any other training data while minimizing the total running time. Numerical results reveal that the meta-learning based solution achieves similar ASR performance to conventional precoder optimization in medium-scale scenarios, and significantly outperforms sub-optimal low complexity precoder algorithms in the large-scale regime.
    
[^161]: 在大型语言模型中的上下文压缩的上下文自编码器

    In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])

    [http://arxiv.org/abs/2307.06945](http://arxiv.org/abs/2307.06945)

    在大型语言模型中，我们提出了一种称为In-context Autoencoder (ICAE)的上下文自编码器，它通过将长上下文压缩为有限数量的内存槽，实现了$4\times$的上下文压缩，并能够根据内存槽进行条件处理以响应各种提示。

    

    我们提出了一种用于大型语言模型中上下文压缩的上下文自编码器（ICAE）。 ICAE有两个模块：一个可学习的编码器，通过从LLM中采用LoRA方式将长上下文压缩为有限数量的内存槽，以及一个固定的解码器，作为目标LLM，可以根据内存槽来进行各种目的的条件处理。我们首先使用自编码和语言建模目标在大规模文本数据上预训练ICAE，使其能够生成准确和全面表示原始上下文的内存槽。然后，我们使用少量指导数据对预训练的ICAE进行微调，以增强其与各种提示的交互，从而产生理想的响应。我们的实验结果表明，使用我们提出的预训练和微调范式学习的ICAE可以有效地产生$4\times$上下文压缩的内存槽，目标LLM可以很好地对其进行条件处理，以响应各种提示。

    We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
    
[^162]: 使用机器学习抑制动力系统中的未知干扰

    Suppressing unknown disturbances to dynamical systems using machine learning. (arXiv:2307.03690v1 [eess.SY])

    [http://arxiv.org/abs/2307.03690](http://arxiv.org/abs/2307.03690)

    本文提出了一种使用机器学习的无模型方法，可以仅通过系统在已知强迫函数影响下的观测，识别和抑制未知系统的未知干扰。这项方法对训练函数有非常温和的限制，能够稳健地识别和抑制大类别的未知干扰。

    

    识别和抑制动力系统中的未知干扰是一个在许多不同领域中应用的问题。在本文中，我们提出了一种无模型的方法，仅基于系统在已知强迫函数影响下的先前观测来识别和抑制未知系统的未知干扰。我们发现，在对训练函数有非常温和的限制下，我们的方法能够稳健地识别和抑制大类别的未知干扰。我们通过一个示例说明了我们的方案，其中识别和抑制了 Lorenz 系统的混沌干扰。

    Identifying and suppressing unknown disturbances to dynamical systems is a problem with applications in many different fields. In this Letter, we present a model-free method to identify and suppress an unknown disturbance to an unknown system based only on previous observations of the system under the influence of a known forcing function. We find that, under very mild restrictions on the training function, our method is able to robustly identify and suppress a large class of unknown disturbances. We illustrate our scheme with an example where a chaotic disturbance to the Lorenz system is identified and suppressed.
    
[^163]: 前列腺成像中分割基础模型的实证分析

    Empirical Analysis of a Segmentation Foundation Model in Prostate Imaging. (arXiv:2307.03266v1 [eess.IV])

    [http://arxiv.org/abs/2307.03266](http://arxiv.org/abs/2307.03266)

    本文通过对前列腺成像中的新型基础模型UniverSeg进行了实证评估研究，并将其与传统方法进行了比较。结果表明基础模型可能是医学成像领域未来的方向。

    

    大多数医学图像分割的最先进技术依赖于深度学习模型。然而，这些模型通常在狭义任务上以监督的方式进行训练，需要昂贵的标记数据集。最近，在自然语言生成等多个机器学习领域取得的进展已经证明了构建基础模型的可行性和实用性，这些模型可以在几乎没有标记数据的情况下为各种不同的下游任务定制。这可能代表了医学成像的范式转变，我们预计基础模型可能塑造该领域的未来。本文考虑了一个最近开发的应用于医学图像分割的基础模型UniverSeg。我们在前列腺成像的背景下进行了经验评估研究，并将其与传统的训练任务特定分割模型的方法进行了比较。我们的结果和讨论突出了几个重要因素，这些因素在基础模型开发中可能非常重要。

    Most state-of-the-art techniques for medical image segmentation rely on deep-learning models. These models, however, are often trained on narrowly-defined tasks in a supervised fashion, which requires expensive labeled datasets. Recent advances in several machine learning domains, such as natural language generation have demonstrated the feasibility and utility of building foundation models that can be customized for various downstream tasks with little to no labeled data. This likely represents a paradigm shift for medical imaging, where we expect that foundation models may shape the future of the field. In this paper, we consider a recently developed foundation model for medical image segmentation, UniverSeg. We conduct an empirical evaluation study in the context of prostate imaging and compare it against the conventional approach of training a task-specific segmentation model. Our results and discussion highlight several important factors that will likely be important in the develo
    
[^164]: GKD：自回归序列模型的广义知识蒸馏

    GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models. (arXiv:2306.13649v1 [cs.LG])

    [http://arxiv.org/abs/2306.13649](http://arxiv.org/abs/2306.13649)

    本文提出了广义知识蒸馏（GKD），通过从学生中采样输出序列来缓解分布不匹配，并在优化替代KL等离散度方面处理模型欠规范，达到了在摘要任务上最先进的性能。

    

    知识蒸馏通常用于压缩神经网络，以减少推理成本和内存占用。然而，当前针对自回归模型（如生成语言模型）的蒸馏方法存在两个关键问题：（1）训练期间输出序列和部署时由学生模型生成的序列之间分布不匹配，（2）模型欠规范，学生模型可能不够表达老师分布。为了解决这些问题，我们提出了广义知识蒸馏（GKD）。GKD通过在训练期间从学生中采样输出序列来缓解分布不匹配。此外，GKD通过优化替代KL等离散度来处理模型欠规范，这些离散度集中于生成可能符合老师分布的学生样本。我们证明，在摘要任务上，GKD优于常用的LLM蒸馏方法，在几个基准数据集上实现了最先进的性能。

    Knowledge distillation is commonly used for compressing neural networks to reduce their inference cost and memory footprint. However, current distillation methods for auto-regressive models, such as generative language models (LMs), suffer from two key issues: (1) distribution mismatch between output sequences during training and the sequences generated by the student during its deployment, and (2) model under-specification, where the student model may not be expressive enough to fit the teacher's distribution. To address these issues, we propose Generalized Knowledge Distillation (GKD). GKD mitigates distribution mismatch by sampling output sequences from the student during training. Furthermore, GKD handles model under-specification by optimizing alternative divergences, such as reverse KL, that focus on generating samples from the student that are likely under the teacher's distribution. We demonstrate that GKD outperforms commonly-used approaches for distilling LLMs on summarizatio
    
[^165]: MLP的规模化：归纳偏差的故事

    Scaling MLPs: A Tale of Inductive Bias. (arXiv:2306.13575v1 [cs.LG])

    [http://arxiv.org/abs/2306.13575](http://arxiv.org/abs/2306.13575)

    本文研究了多层感知器（MLP）在视觉任务中的性能极限，并探讨了MLP相较于其他深度学习模型的归纳偏差，旨在推进深度学习理论和实践的结合。

    

    在此工作中，我们重新审视了深度学习中最基本的构建块——多层感知器（MLP），并研究了它在视觉任务中的性能极限。MLP的实验性洞见在多个方面都非常重要。

    In this work we revisit the most fundamental building block in deep learning, the multi-layer perceptron (MLP), and study the limits of its performance on vision tasks. Empirical insights into MLPs are important for multiple reasons. (1) Given the recent narrative "less inductive bias is better", popularized due to transformers eclipsing convolutional models, it is natural to explore the limits of this hypothesis. To that end, MLPs offer an ideal test bed, being completely free of any inductive bias. (2) MLPs have almost exclusively been the main protagonist in the deep learning theory literature due to their mathematical simplicity, serving as a proxy to explain empirical phenomena observed for more complex architectures. Surprisingly, experimental datapoints for MLPs are very difficult to find in the literature, especially when coupled with large pre-training protocols. This discrepancy between practice and theory is worrying: Do MLPs reflect the empirical advances exhibited by pract
    
[^166]: OpenDataVal：一种数据价值评估的统一基准测试

    OpenDataVal: a Unified Benchmark for Data Valuation. (arXiv:2306.10577v1 [cs.LG])

    [http://arxiv.org/abs/2306.10577](http://arxiv.org/abs/2306.10577)

    本文介绍了一种名为OpenDataVal的基准测试框架，该框架整合了多种数据集和九种最先进的数据估值算法实现，并提供了四个下游机器学习任务来评估数据价值的质量。

    

    评估单个数据点的质量和影响对于提高模型性能和减轻训练数据集中不良偏差至关重要。尽管已经提出了几个数据估值算法来量化数据质量，但还缺乏一个系统化和标准化的数据估值基准测试系统。本文介绍了OpenDataVal，一种易于使用和统一的基准测试框架，使研究人员和从业者能够应用和比较各种数据估值算法。OpenDataVal提供了一个综合环境，包括（i）各种图像，自然语言和表格数据集，（ii）九种不同的最先进的数据估值算法的实现，以及（iii）可以导入任何scikit-learn模型的预测模型API。此外，我们提出了四个下游机器学习任务，用于评估数据值的质量。我们使用OpenDataVal进行基准测试分析，量化并比较不同数据估值算法在不同数据集上的表现。

    Assessing the quality and impact of individual data points is critical for improving model performance and mitigating undesirable biases within the training dataset. Several data valuation algorithms have been proposed to quantify data quality, however, there lacks a systemic and standardized benchmarking system for data valuation. In this paper, we introduce OpenDataVal, an easy-to-use and unified benchmark framework that empowers researchers and practitioners to apply and compare various data valuation algorithms. OpenDataVal provides an integrated environment that includes (i) a diverse collection of image, natural language, and tabular datasets, (ii) implementations of nine different state-of-the-art data valuation algorithms, and (iii) a prediction model API that can import any models in scikit-learn. Furthermore, we propose four downstream machine learning tasks for evaluating the quality of data values. We perform benchmarking analysis using OpenDataVal, quantifying and comparin
    
[^167]: Ada-NAV：用于机器人导航的自适应轨迹优化策略学习方法

    Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation. (arXiv:2306.06192v1 [cs.RO])

    [http://arxiv.org/abs/2306.06192](http://arxiv.org/abs/2306.06192)

    Ada-NAV是一种自适应轨迹优化策略学习方法，采用降低策略随机性的方法平衡探索与利用，提高机器人导航任务的采样效率。在真实世界的测试中表现优异，可以在更短的采样时间内取得更高的性能。

    

    强化学习方法在学习机器人导航策略方面十分有效，但其采样效率低的问题也十分明显。在策略优化中，这种效率低下部分来自于未能适当地平衡探索与利用的问题，特别是在面对非静态时。为了加入探索与利用的平衡以提高采样效率，我们提出了Ada-NAV，一种自适应轨迹长度方案，其中长度随着策略的随机性（用其Shannon或差分熵表示）的减小而增加。我们的自适应轨迹长度方案由于更频繁的梯度更新强调了训练开始时的探索，后来则更强调利用。在网格世界，仿真机器人环境和真实世界机器人实验中，我们证明了该方法的优点，表现在性能和采样效率上均优于常数和随机采样的轨迹长度。在固定的样本预算下，相对于现有的基准方法，Ada-NAV的性能提高了高达46％，采样数量减少了高达80％。

    Reinforcement learning methods, while effective for learning robotic navigation strategies, are known to be highly sample inefficient. This sample inefficiency comes in part from not suitably balancing the explore-exploit dilemma, especially in the presence of non-stationarity, during policy optimization. To incorporate a balance of exploration-exploitation for sample efficiency, we propose Ada-NAV, an adaptive trajectory length scheme where the length grows as a policy's randomness, represented by its Shannon or differential entropy, decreases. Our adaptive trajectory length scheme emphasizes exploration at the beginning of training due to more frequent gradient updates and emphasizes exploitation later on with longer trajectories. In gridworld, simulated robotic environments, and real-world robotic experiments, we demonstrate the merits of the approach over constant and randomly sampled trajectory lengths in terms of performance and sample efficiency. For a fixed sample budget, Ada-N
    
[^168]: 利用预训练模型的速率降低原则进行图像聚类

    Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models. (arXiv:2306.05272v1 [cs.CV])

    [http://arxiv.org/abs/2306.05272](http://arxiv.org/abs/2306.05272)

    本论文提出了一种新的图像聚类流程，利用大型预训练模型的强大特征表示，在规模上有效地对图像进行聚类，并通过优化速率降低目标和 CLIP 的图像-文本绑定，成功地提高了聚类的准确性和自标记算法的效果。

    

    大型预训练模型的出现已经在视觉表示学习和自然语言处理方面带来了范式转变，但是聚类未标记的图像作为一种基本和经典的机器学习问题，仍然缺乏有效的解决方案，特别是对于大规模数据集。在本文中，我们提出了一种新的图像聚类流程，利用 CLIP 等大型预训练模型的强大特征表示，在规模上有效地对图像进行聚类。我们展示了预训练特征通过进一步优化速率降低目标，更具有结构性。由此产生的特征可以显著提高聚类的准确性，例如从 ImageNet-1k 的 57％提高到 66％。此外，通过利用 CLIP 的图像-文本绑定，我们展示了新的聚类方法如何导致简单而有效的自标记算法，成功地应用于未标记的大型数据集，例如 MS-COCO 和 LAION-Aesthetics。

    The advent of large pre-trained models has brought about a paradigm shift in both visual representation learning and natural language processing. However, clustering unlabeled images, as a fundamental and classic machine learning problem, still lacks effective solution, particularly for large-scale datasets. In this paper, we propose a novel image clustering pipeline that leverages the powerful feature representation of large pre-trained models such as CLIP and cluster images effectively and efficiently at scale. We show that the pre-trained features are significantly more structured by further optimizing the rate reduction objective. The resulting features may significantly improve the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore, by leveraging CLIP's image-text binding, we show how the new clustering method leads to a simple yet effective self-labeling algorithm that successfully works on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We wi
    
[^169]: Vocos：消除时域和基于傅里叶变化的神经声码器在高质量音频合成中的差距

    Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis. (arXiv:2306.00814v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2306.00814](http://arxiv.org/abs/2306.00814)

    Vocos是一个新的模型，通过直接生成傅里叶谱系数，消除了时域和基于傅里叶变化的神经声码器在高质量音频合成中的差距，并实现了计算效率的大幅提升。

    

    近期神经声码器的发展主要由在时域中操作的生成对抗网络（GAN）推动。虽然有效，但这种方法忽视了时频表示提供的归纳偏差，从而导致冗余和计算密集的上采样操作。基于傅里叶变换的时频表示是一个有吸引力的替代方案，它与人的听觉感知更加准确，并且通过其计算得到了经过充分验证的快速算法的好处。然而，直接重建复值谱图在历史上一直存在问题，主要是由于相位恢复问题。本研究旨在通过提出Vocos，一个直接生成傅里叶谱系数的新模型，来消除这个差距。我们的评估结果表明，Vocos不仅与音频质量的最新技术水平相匹配，而且在计算效率上实现了数量级的提升，与传统方法相比速度提高了一个数量级。

    Recent advancements in neural vocoding are predominantly driven by Generative Adversarial Networks (GANs) operating in the time-domain. While effective, this approach neglects the inductive bias offered by time-frequency representations, resulting in reduntant and computionally-intensive upsampling operations. Fourier-based time-frequency representation is an appealing alternative, aligning more accurately with human auditory perception, and benefitting from well-established fast algorithms for its computation. Nevertheless, direct reconstruction of complex-valued spectrograms has been historically problematic, primarily due to phase recovery issues. This study seeks to close this gap by presenting Vocos, a new model that directly generates Fourier spectral coefficients. Vocos not only matches the state-of-the-art in audio quality, as demonstrated in our evaluations, but it also substantially improves computational efficiency, achieving an order of magnitude increase in speed compared 
    
[^170]: HiGen：层次图生成网络

    HiGen: Hierarchical Graph Generative Networks. (arXiv:2305.19337v1 [cs.LG])

    [http://arxiv.org/abs/2305.19337](http://arxiv.org/abs/2305.19337)

    HiGen是一种新颖的图形生成网络，能够以粗到细的方式捕捉图形的层次结构，并使用多项式分布来生成具有整数值的子图的边权。它能够有效地捕捉图形的局部和全局属性，并实现了最先进的性能。

    

    大多数真实世界的图表现出层次结构，这通常被现有的图形生成方法所忽视。为了解决这个限制，我们提出了一种新颖的图形生成网络，能够以粗到细的方式捕捉图形的层次结构并成功地生成图形子结构。在每个层次上，该模型并行生成社区，使用独立的模型预测社区之间的跨边。这种模块化方法使生成的图形网络高度可扩展。此外，我们用多项式分布建模层次图形的输出分布，并针对此分布推导了递归分解，使我们能够以自回归的方式生成具有整数值的子图的边权。实证研究证明，所提出的生成模型能够有效地捕捉图形的局部和全局属性，并实现了最先进的性能。

    Most real-world graphs exhibit a hierarchical structure, which is often overlooked by existing graph generation methods. To address this limitation, we propose a novel graph generative network that captures the hierarchical nature of graphs and successively generates the graph sub-structures in a coarse-to-fine fashion. At each level of hierarchy, this model generates communities in parallel, followed by the prediction of cross-edges between communities using a separate model. This modular approach results in a highly scalable graph generative network. Moreover, we model the output distribution of edges in the hierarchical graph with a multinomial distribution and derive a recursive factorization for this distribution, enabling us to generate sub-graphs with integer-valued edge weights in an autoregressive approach. Empirical studies demonstrate that the proposed generative model can effectively capture both local and global properties of graphs and achieves state-of-the-art performanc
    
[^171]: 关于扩散建模在异常检测中的应用

    On Diffusion Modeling for Anomaly Detection. (arXiv:2305.18593v1 [cs.LG])

    [http://arxiv.org/abs/2305.18593](http://arxiv.org/abs/2305.18593)

    本文研究了扩散建模在无监督和半监督异常检测中的应用，发现去噪扩散概率模型表现很好但计算成本高，因此提出了一种替代方法——扩散时间概率模型，该模型能够通过较大的时间步长上的高后验密度识别异常，并通过深度神经网络提高效率。

    

    扩散模型以其在生成建模中的优异性能而闻名，成为基于密度的异常检测的有吸引力的候选算法。本文研究了各种扩散建模方法在无监督和半监督异常检测中的应用。尤其是发现去噪扩散概率模型（DDPM）在异常检测方面具备很好的表现，但计算成本较高。通过简化DDPM在异常检测中的应用，我们自然地引出另一种称为扩散时间概率模型（DTPM）的替代方法。DTPM估计给定输入的扩散时间的后验分布，能够通过较大的时间步长上的高后验密度识别异常。我们导出了此后验分布的解析形式，并利用深度神经网络提高推理效率。通过在ADBenh基准测试中的实证评估，我们证明了所有基于扩散的异常检测方法的实用性。

    Known for their impressive performance in generative modeling, diffusion models are attractive candidates for density-based anomaly detection. This paper investigates different variations of diffusion modeling for unsupervised and semi-supervised anomaly detection. In particular, we find that Denoising Diffusion Probability Models (DDPM) are performant on anomaly detection benchmarks yet computationally expensive. By simplifying DDPM in application to anomaly detection, we are naturally led to an alternative approach called Diffusion Time Probabilistic Model (DTPM). DTPM estimates the posterior distribution over diffusion time for a given input, enabling the identification of anomalies due to their higher posterior density at larger timesteps. We derive an analytical form for this posterior density and leverage a deep neural network to improve inference efficiency. Through empirical evaluations on the ADBench benchmark, we demonstrate that all diffusion-based anomaly detection methods 
    
[^172]: 剪枝与低秩参数高效微调相遇

    Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning. (arXiv:2305.18403v1 [cs.LG])

    [http://arxiv.org/abs/2305.18403](http://arxiv.org/abs/2305.18403)

    本论文提出了一种名为LoRAPrune的框架，可以高效微调和部署大型预训练模型，通过利用低秩自适应的值和梯度来设计PEFT感知的剪枝准则，并提出了一个迭代剪枝过程来去除冗余参数，实验结果表明与最先进的方法相比，可以显著降低模型大小和推理时间，同时保持竞争性的准确性。

    

    大型预训练模型（LPM）在各种任务中表现出卓越的性能。虽然出现了参数高效微调（PEFT）来便宜地微调这些大型模型用于下游任务，但它们的部署仍然受到巨大的模型规模和计算成本的制约。神经网络剪枝通过删除冗余参数来提供模型压缩的解决方案，但大多数现有方法依赖于计算参数梯度。然而，对于LPM而言，获得梯度是计算上禁止的，这需要探索替代方法。为此，我们提出了一种用于LPM高效微调和部署的统一框架，称为LoRAPrune。我们首先设计了一个PEFT感知的剪枝准则，该准则利用了低秩自适应（LoRA）的值和梯度，而不是预训练参数的梯度进行重要性评估。然后，我们提出了一个迭代剪枝过程来基于剪枝准则去除冗余参数。各种基准数据集上的实验结果表明，与最先进的方法相比，我们的框架可以显著降低模型大小和推理时间，同时保持竞争性的准确性。

    Large pre-trained models (LPMs), such as LLaMA and ViT-G, have shown exceptional performance across various tasks. Although parameter-efficient fine-tuning (PEFT) has emerged to cheaply fine-tune these large models on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Neural network pruning offers a solution for model compression by removing redundant parameters, but most existing methods rely on computing parameter gradients. However, obtaining the gradients is computationally prohibitive for LPMs, which necessitates the exploration of alternative approaches. To this end, we propose a unified framework for efficient fine-tuning and deployment of LPMs, termed LoRAPrune. We first design a PEFT-aware pruning criterion, which utilizes the values and gradients of Low-Rank Adaption (LoRA), rather than the gradients of pre-trained parameters for importance estimation. We then propose an iterative pruning procedure to remove redundant paramet
    
[^173]: 自动化搜索空间生成的神经架构搜索

    Automated Search-Space Generation Neural Architecture Search. (arXiv:2305.18030v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18030](http://arxiv.org/abs/2305.18030)

    Automated Search-Space Generation Neural Architecture Search (ASGNAS) is an automated system that trains general DNNs covering all candidate connections and operations and produces high-performing sub-networks. It minimizes the need for human expertise and manual intervention by automatically generating the search space and utilizing a Hierarchical Half-Space Projected Gradient (H2SPG) to ensure network validity and performance.

    

    现有的神经架构搜索（NAS）方法通常依赖于事先手工创建搜索空间来搜索通用深度神经网络（DNN）中的最优子网络。这样的要求使得在没有显著的人工专业知识和手动干预的情况下将它们扩展到通用场景变得具有挑战性。为了克服这些限制，我们提出了Automated Search-Space Generation Neural Architecture Search（ASGNAS），可能是第一个自动化系统，以一次性的方式训练覆盖所有候选连接和操作的通用DNN，并产生高性能的子网络。技术上，ASGNAS具有三个显著的贡献以减少人力工作：（i）通用DNN的自动搜索空间生成；（ii）利用生成的搜索空间内的层次结构和依赖关系的Hierarchical Half-Space Projected Gradient（H2SPG），在优化过程中确保网络的有效性，并可靠地产生具有高性能和稀疏性的解决方案。

    To search an optimal sub-network within a general deep neural network (DNN), existing neural architecture search (NAS) methods typically rely on handcrafting a search space beforehand. Such requirements make it challenging to extend them onto general scenarios without significant human expertise and manual intervention. To overcome the limitations, we propose Automated Search-Space Generation Neural Architecture Search (ASGNAS), perhaps the first automated system to train general DNNs that cover all candidate connections and operations and produce high-performing sub-networks in the one shot manner. Technologically, ASGNAS delivers three noticeable contributions to minimize human efforts: (i) automated search space generation for general DNNs; (ii) a Hierarchical Half-Space Projected Gradient (H2SPG) that leverages the hierarchy and dependency within generated search space to ensure the network validity during optimization, and reliably produces a solution with both high performance an
    
[^174]: 基于表示的Jensen-Shannon散度

    The Representation Jensen-Shannon Divergence. (arXiv:2305.16446v1 [cs.LG])

    [http://arxiv.org/abs/2305.16446](http://arxiv.org/abs/2305.16446)

    本文提出了一种基于表示的新型散度——表示Jensen-Shannon散度，通过将数据分布嵌入到RKHS中，并利用表示的协方差算子的频谱，实现对数据分布的估计，并提供了具有灵活性，可扩展性，可微分性的经验协方差矩阵估计函数和基于核矩阵的估计函数。

    

    统计散度量化概率分布之间的差异，是机器学习中的一种重要方法。但是，由于数据的底层分布通常未知，从经验样本中估计散度是一个基本难题。本文提出了一种基于再生核希尔伯特空间(RKHS)中协方差算子的新型散度——表示Jensen-Shannon散度。我们的方法将数据分布嵌入到RKHS中，并利用表示的协方差算子的频谱。我们提供了一个从经验协方差矩阵估计的估计函数，它通过使用Fourier特征将数据映射到RKHS中。此估计函数是灵活、可扩展、可微分的，并且适用于小批量优化问题。此外，我们还提供了一种基于核矩阵的估计函数，而不需要对RKHS进行显式映射。我们证明这个量是Jensen-Shannon散度的一个下界。

    Statistical divergences quantify the difference between probability distributions finding multiple uses in machine-learning. However, a fundamental challenge is to estimate divergence from empirical samples since the underlying distributions of the data are usually unknown. In this work, we propose the representation Jensen-Shannon Divergence, a novel divergence based on covariance operators in reproducing kernel Hilbert spaces (RKHS). Our approach embeds the data distributions in an RKHS and exploits the spectrum of the covariance operators of the representations. We provide an estimator from empirical covariance matrices by explicitly mapping the data to an RKHS using Fourier features. This estimator is flexible, scalable, differentiable, and suitable for minibatch-based optimization problems. Additionally, we provide an estimator based on kernel matrices without having an explicit mapping to the RKHS. We show that this quantity is a lower bound on the Jensen-Shannon divergence, and 
    
[^175]: 差分隐私潜在扩散模型

    Differentially Private Latent Diffusion Models. (arXiv:2305.15759v1 [stat.ML])

    [http://arxiv.org/abs/2305.15759](http://arxiv.org/abs/2305.15759)

    本文提出使用差分隐私训练潜在扩散模型(LDMs)，通过预训练自编码器将高维像素空间转变为低维潜在空间实现更高效快速的DMs训练，并且通过只微调注意力模块减少了可训练参数的数量。

    

    扩散模型(DMs)被广泛用于生成高质量图像数据集。然而，由于它们直接在高维像素空间中运行，DMs的优化计算成本高，需要长时间的训练。这导致由于差分隐私的可组合性属性，大量噪音注入到差分隐私学习过程中。为了解决这个挑战，我们提出使用差分隐私训练潜在扩散模型(LDMs)。LDMs使用强大的预训练自编码器将高维像素空间减少到更低维的潜在空间，使训练DMs更加高效和快速。与[Ghalebikesabi等人，2023]预先用公共数据预训练DMs，然后再用隐私数据进行微调不同，我们仅微调LDMs中不同层的注意力模块以获得隐私敏感数据，相对于整个DM微调，可减少大约96%的可训练参数数量。

    Diffusion models (DMs) are widely used for generating high-quality image datasets. However, since they operate directly in the high-dimensional pixel space, optimization of DMs is computationally expensive, requiring long training times. This contributes to large amounts of noise being injected into the differentially private learning process, due to the composability property of differential privacy. To address this challenge, we propose training Latent Diffusion Models (LDMs) with differential privacy. LDMs use powerful pre-trained autoencoders to reduce the high-dimensional pixel space to a much lower-dimensional latent space, making training DMs more efficient and fast. Unlike [Ghalebikesabi et al., 2023] that pre-trains DMs with public data then fine-tunes them with private data, we fine-tune only the attention modules of LDMs at varying layers with privacy-sensitive data, reducing the number of trainable parameters by approximately 96% compared to fine-tuning the entire DM. We te
    
[^176]: 神经网络学习轨迹的转移

    Transferring Learning Trajectories of Neural Networks. (arXiv:2305.14122v1 [cs.LG])

    [http://arxiv.org/abs/2305.14122](http://arxiv.org/abs/2305.14122)

    本研究提出了转移学习轨迹的算法，可将之前训练过的神经网络的学习轨迹应用在新的训练中，并能在任何直接训练之前实现非平凡的准确性。

    

    训练深度神经网络（DNN）是计算密集型的，这在执行重复训练运行（例如模型集成或知识蒸馏）时尤其成问题。一旦我们在某个数据集上训练了一个DNN，我们就拥有了其学习轨迹（即训练期间的中间参数序列），其中可能包含学习数据集的有用信息。然而，尚未尝试利用给定学习轨迹的这种信息进行另一种训练。本文将问题形式化为“转移”给定学习轨迹从一个初始参数到另一个初始参数，称为学习转移问题，并通过匹配沿轨迹逐渐平移对称性的梯度导出了第一个算法，以近似解决它。我们经验证明，转移参数在任何直接训练之前就能达到非平凡的准确性。此外，我们分析了转移参数的损失景观属性。

    Training deep neural networks (DNNs) is computationally expensive, which is problematic especially when performing duplicated training runs, such as model ensemble or knowledge distillation. Once we have trained one DNN on some dataset, we have its learning trajectory (i.e., a sequence of intermediate parameters during training) which may potentially contain useful information for learning the dataset. However, there has been no attempt to utilize such information of a given learning trajectory for another training. In this paper, we formulate the problem of "transferring" a given learning trajectory from one initial parameter to another one, called learning transfer problem, and derive the first algorithm to approximately solve it by matching gradients successively along the trajectory via permutation symmetry. We empirically show that the transferred parameters achieve non-trivial accuracy before any direct training. Also, we analyze the loss landscape property of the transferred par
    
[^177]: 机器学习公平性的因果关联权衡分析

    Causality-Aided Trade-off Analysis for Machine Learning Fairness. (arXiv:2305.13057v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13057](http://arxiv.org/abs/2305.13057)

    本论文使用因果分析作为一种方法，通过分析机器学习流程中公平性参数与其他关键指标的权衡，提供了一种系统理解和决策基础，帮助开发者在提供公平机器学习服务时做出明智的决策。

    

    在增强机器学习公平性方面，越来越多的关注点涌现出来。尽管有越来越多的提升公平性的方法，但我们对应用这些方法时，在考虑因素之间的权衡缺乏系统性的理解。这种理解对于开发者做出关于提供公平机器学习服务的明智决策至关重要。然而，当存在多个公平性参数和其他关键指标，并且彼此之间存在耦合甚至冲突时，分析这些权衡是极其困难的。本文提出使用因果分析作为一种原则性方法，在机器学习流程中分析公平性参数与其他关键指标之间的权衡。为了实际有效地进行因果分析，我们提出了一组领域特定的优化方法，以促进准确的因果发现，并基于成熟的因果推断提出了一个统一的、新颖的权衡分析接口。

    There has been an increasing interest in enhancing the fairness of machine learning (ML). Despite the growing number of fairness-improving methods, we lack a systematic understanding of the trade-offs among factors considered in the ML pipeline when fairness-improving methods are applied. This understanding is essential for developers to make informed decisions regarding the provision of fair ML services. Nonetheless, it is extremely difficult to analyze the trade-offs when there are multiple fairness parameters and other crucial metrics involved, coupled, and even in conflict with one another.  This paper uses causality analysis as a principled method for analyzing trade-offs between fairness parameters and other crucial metrics in ML pipelines. To ractically and effectively conduct causality analysis, we propose a set of domain-specific optimizations to facilitate accurate causal discovery and a unified, novel interface for trade-off analysis based on well-established causal inferenc
    
[^178]: CNN 压缩的评估度量

    Evaluation Metrics for CNNs Compression. (arXiv:2305.10616v1 [cs.LG])

    [http://arxiv.org/abs/2305.10616](http://arxiv.org/abs/2305.10616)

    本文提供了神经网络压缩的评估度量综述，从而为标准化神经网络压缩贡献力量。

    

    研究人员致力于开发不同的神经网络压缩技术，但社区似乎缺乏标准化的评估和比较不同压缩技术的方法，这是识别不同应用程序的最合适的压缩技术的关键。本文通过提出评估度量的综述来为神经网络压缩的标准化贡献。这些度量已被实现到NetZIP，一个标准化的神经网络压缩基准之中。我们通过三个案例研究展示一些被审查的度量，分别聚焦于目标分类、目标检测和边缘设备。

    There is a lot of research effort devoted by researcher into developing different techniques for neural networks compression, yet the community seems to lack standardised ways of evaluating and comparing between different compression techniques, which is key to identifying the most suitable compression technique for different applications. In this paper we contribute towards standardisation of neural network compression by providing a review of evaluation metrics. These metrics have been implemented into NetZIP, a standardised neural network compression bench. We showcase some of the metrics reviewed using three case studies focusing on object classification, object detection, and edge devices.
    
[^179]: 无法学习的样本给出了一种虚假的安全感：通过可学习的例子穿透那些无法利用的数据

    Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples. (arXiv:2305.09241v1 [cs.LG])

    [http://arxiv.org/abs/2305.09241](http://arxiv.org/abs/2305.09241)

    “无法学习的样本”提出一种对数据进行保护的方法，但它无法阻止未经授权的用户对保护后的数据进行利用。通过提出“可学习的未经授权示例”和一种新的纯化过程，我们可以实现对数据的更好保护。

    

    在当下随处可见的安全漏洞中，保护数据免于未经授权的利用是至关重要的。最近，一种叫做“无法学习的样本”（UEs）的方法被提出，通过对数据进行微小的扰动，使得模型无法在原始的干净分布上准确地对其进行分类，从而提供了一种强大的保护措施。然而，我们发现 UEs 带来的安全威胁是虚假的，因为它们无法阻止未经授权的用户利用其他未受保护的数据来去除保护，将无法学习的数据重转为可学习。基于这一观察，我们正式定义了一种威胁，引入了“可学习的未经授权示例”（LEs），这些是已经去除保护的UEs。我们的方法的核心是通过一种新的纯化过程，将UEs投射到LEs的流形上。这是通过一种新的联合条件扩散模型来实现的，该模型对UEs进行去噪。

    Safeguarding data from unauthorized exploitation is vital for privacy and security, especially in recent rampant research in security breach such as adversarial/membership attacks. To this end, \textit{unlearnable examples} (UEs) have been recently proposed as a compelling protection, by adding imperceptible perturbation to data so that models trained on them cannot classify them accurately on original clean distribution. Unfortunately, we find UEs provide a false sense of security, because they cannot stop unauthorized users from utilizing other unprotected data to remove the protection, by turning unlearnable data into learnable again. Motivated by this observation, we formally define a new threat by introducing \textit{learnable unauthorized examples} (LEs) which are UEs with their protection removed. The core of this approach is a novel purification process that projects UEs onto the manifold of LEs. This is realized by a new joint-conditional diffusion model which denoises UEs con
    
[^180]: 使用物理信息可逆神经网络进行高效贝叶斯推断的研究

    Efficient Bayesian inference using physics-informed invertible neural networks for inverse problems. (arXiv:2304.12541v1 [math.NA])

    [http://arxiv.org/abs/2304.12541](http://arxiv.org/abs/2304.12541)

    本文提出了一种使用物理信息可逆神经网络(PI-INN)解决贝叶斯反问题的新方法，该方法可以高效地进行抽样和准确的密度评估。研究通过残差项和独立性损失项确保了INN输出的统计独立性，并在多项实验中证明了其有效性和准确性。

    

    本文提出了一种利用物理信息可逆神经网络(PI-INN)解决贝叶斯反问题的新方法。PI-INN的结构包括两个子网络：一个可逆神经网络(INN)和一个神经基础网络(NB-Net)。通过NB-Net帮助建立参数输入和INN输出之间的可逆映射，以提供可行的后验分布估计，从而实现高效的抽样和准确的密度评估。此外，PI-INN的损失函数包含两个部分：一部分是基于残差的物理信息损失项，另一部分是新的独立性损失项。提出的独立性损失项可以高斯化随机潜变量，并通过有效利用估计的密度函数，确保INN输出的两个部分之间的统计独立性。通过进行反向运动学和反向扩散等多项实验验证了所提出的PI-INN的有效性和准确性。

    In the paper, we propose a novel approach for solving Bayesian inverse problems with physics-informed invertible neural networks (PI-INN). The architecture of PI-INN consists of two sub-networks: an invertible neural network (INN) and a neural basis network (NB-Net). The invertible map between the parametric input and the INN output with the aid of NB-Net is constructed to provide a tractable estimation of the posterior distribution, which enables efficient sampling and accurate density evaluation. Furthermore, the loss function of PI-INN includes two components: a residual-based physics-informed loss term and a new independence loss term. The presented independence loss term can Gaussianize the random latent variables and ensure statistical independence between two parts of INN output by effectively utilizing the estimated density function. Several numerical experiments are presented to demonstrate the efficiency and accuracy of the proposed PI-INN, including inverse kinematics, inver
    
[^181]: 通过改进的积分逼近加速扩散采样过程

    On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation. (arXiv:2304.11328v1 [cs.LG])

    [http://arxiv.org/abs/2304.11328](http://arxiv.org/abs/2304.11328)

    本文提出了一种通过优化系数来加速流行的基于反向ODE解算的扩散采样过程的方法，优化方法为改进的积分逼近（IIA），在每个反向时间步长，我们建议针对某些选择的系数最小化MSE函数，给定预训练的扩散模型，只需要在一批样本上进行特定数量的神经功能评估（NFEs）一次IIA过程即可获得最佳解。

    

    一种流行的基于扩散的采样策略尝试有效地解决反向常微分方程（ODEs）。所得ODE求解器的系数由ODE公式，反向离散的时间步长和使用的ODE方法预先确定。本文考虑通过优化某些系数来加速几种流行的基于ODE的采样过程，优化方法为改进的积分逼近（IIA）。在每个反向时间步长，我们建议针对某些选择的系数最小化均方误差（MSE）函数。通过应用一组细粒度时间步长的原始ODE求解器构造MSE，从原理上提供了更精确的积分逼近，以预测下一个扩散隐藏状态，给定预训练的扩散模型，只需要在一批样本上进行特定数量的神经功能评估（NFEs）一次IIA过程即可获得最佳解

    One popular diffusion-based sampling strategy attempts to solve the reverse ordinary differential equations (ODEs) effectively. The coefficients of the obtained ODE solvers are pre-determined by the ODE formulation, the reverse discrete timesteps, and the employed ODE methods. In this paper, we consider accelerating several popular ODE-based sampling processes by optimizing certain coefficients via improved integration approximation (IIA). At each reverse timestep, we propose to minimize a mean squared error (MSE) function with respect to certain selected coefficients. The MSE is constructed by applying the original ODE solver for a set of fine-grained timesteps which in principle provides a more accurate integration approximation in predicting the next diffusion hidden state. Given a pre-trained diffusion model, the procedure for IIA for a particular number of neural functional evaluations (NFEs) only needs to be conducted once over a batch of samples. The obtained optimal solutions f
    
[^182]: 离散与反向传播的桥梁：直通法与其它方法

    Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])

    [http://arxiv.org/abs/2304.08612](http://arxiv.org/abs/2304.08612)

    本文提出了一种新方法来逼近生成离散潜变量的参数的梯度，其中包括了一些数值方法，实现了二阶精度，取得了实验上的持续改进。

    

    反向传播是深度学习中的基石，但其仅限于计算连续变量的梯度，限制了涉及离散潜变量的问题的研究。针对这个问题，我们提出了一种新的方法来近似生成离散潜变量的参数的梯度。我们首先考察了广泛使用的 Straight-Through（ST）启发式方法，并证明它作为梯度的一阶近似值。在此基础上，我们提出了一种新的方法，称为 ReinMax，它集成了 Heun's Method，一种解ODE的二阶数值方法，以近似梯度。我们的方法实现了二阶精度，而不需要 Hessian 或其他二阶导数。我们进行了结构化输出预测和无监督生成建模任务的实验。我们的结果显示，\ours 在现有技术中带来了持续的改进，包括 ST 和 Straight-Through Gum。

    Backpropagation, the cornerstone of deep learning, is limited to computing gradients solely for continuous variables. This limitation hinders various research on problems involving discrete latent variables. To address this issue, we propose a novel approach for approximating the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose a novel method called ReinMax, which integrates Heun's Method, a second-order numerical method for solving ODEs, to approximate the gradient. Our method achieves second-order accuracy without requiring Hessian or other second-order derivatives. We conduct experiments on structured output prediction and unsupervised generative modeling tasks. Our results show that \ours brings consistent improvements over the state of the art, including ST and Straight-Through Gum
    
[^183]: 关于AI生成文本检测的可能性的探讨

    On the Possibilities of AI-Generated Text Detection. (arXiv:2304.04736v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.04736](http://arxiv.org/abs/2304.04736)

    该研究探讨了AI生成文本检测的可能性，提出了精确的样本复杂度界限，并指出了设计更准确的检测方法和提高透明度的挑战。

    

    我们的工作着眼于检测由大型语言模型(LLM)生成的输出，以区分其与人类生成的输出。这项能力在许多应用中非常重要。然而，关于这种区分的可能性一直是该领域内的争议话题。因此，一个核心问题是我们是否能够检测到AI生成的文本，如果能，何时能检测到。在这项工作中，我们提供了证据表明，除非人类和机器生成的文本分布在整个支持中完全相同，否则几乎总是可以检测到AI生成的文本。这个观察结果来自于信息论中的标准结果，并依赖于机器生成的文本越像人类，我们就需要更多的样本来检测它。我们得出了AI生成文本检测的精确样本复杂度界限，告诉需要多少个样本才能检测到AI生成的文本。这引起了更多设计更准确的AI生成文本检测方法和提高LLM透明度的挑战。

    Our work focuses on the challenge of detecting outputs generated by Large Language Models (LLMs) to distinguish them from those generated by humans. This ability is of the utmost importance in numerous applications. However, the possibility of such discernment has been the subject of debate within the community. Therefore, a central question is whether we can detect AI-generated text and, if so, when. In this work, we provide evidence that it should almost always be possible to detect AI-generated text unless the distributions of human and machine-generated texts are exactly the same over the entire support. This observation follows from the standard results in information theory and relies on the fact that if the machine text becomes more human-like, we need more samples to detect it. We derive a precise sample complexity bound of AI-generated text detection, which tells how many samples are needed to detect AI-generated text. This gives rise to additional challenges of designing more
    
[^184]: 基于学习的静态恶意软件分类器的对抗性鲁棒性

    Adversarial Robustness of Learning-based Static Malware Classifiers. (arXiv:2303.13372v1 [cs.CR])

    [http://arxiv.org/abs/2303.13372](http://arxiv.org/abs/2303.13372)

    本文提出了一种通用的对抗性补丁（UAP）攻击方法，只需附加一个相对较小的字节级补丁即可绕过MalConv分类器的检测，可以将恶意软件文件的检测率降低80％。同时，作者提出了窗口消除处理作为应对此种攻击的一种方法。

    

    恶意软件检测一直是恶意软件作者和反病毒系统之间持续的军备竞赛阶段。随着这场竞赛规模的不断增加，利用机器学习（ML）的解决方案得到了关注。然而，这种趋势使得直接对ML进行攻击对于对手而言成为一种有吸引力的前景。本文研究了这场军备竞赛的两个方面，即从恶意软件文件的原始字节中操作的基于卷积神经网络的流行分类器MalConv的角度。首先，我们表明MalConv易受到对抗性补丁攻击的影响:将一个字节级的补丁附加到恶意软件文件中，使其绕过检测的概率高达94.3％。此外，我们开发了一种通用的对抗性补丁（UAP）攻击，在任何包含该补丁的恶意软件文件的恒定时间内，可以将其检测率降低80％。即使相对于原始文件大小而言，这些补丁的大小也相对较小-在2％-8％之间。为了抵御这种攻击，我们进行了窗口消除处理，允许识别恶意代码的部分不受对抗性补丁攻击。

    Malware detection has long been a stage for an ongoing arms race between malware authors and anti-virus systems. Solutions that utilize machine learning (ML) gain traction as the scale of this arms race increases. This trend, however, makes performing attacks directly on ML an attractive prospect for adversaries. We study this arms race from both perspectives in the context of MalConv, a popular convolutional neural network-based malware classifier that operates on raw bytes of files. First, we show that MalConv is vulnerable to adversarial patch attacks: appending a byte-level patch to malware files bypasses detection 94.3% of the time. Moreover, we develop a universal adversarial patch (UAP) attack where a single patch can drop the detection rate in constant time of any malware file that contains it by 80%. These patches are effective even being relatively small with respect to the original file size -between 2%-8%. As a countermeasure, we then perform window ablation that allows u
    
[^185]: 随机梯度下降的概率稳定性

    The Probabilistic Stability of Stochastic Gradient Descent. (arXiv:2303.13093v1 [cs.LG])

    [http://arxiv.org/abs/2303.13093](http://arxiv.org/abs/2303.13093)

    本文重新定义了随机梯度下降的稳定性，并使用概率稳定性来回答深度学习理论中的一个根本问题：SGD如何从数量庞大的可能严重过拟合的解中选择有意义的神经网络解。

    

    深度学习理论中的一个基本开放问题是如何定义和理解随机梯度下降(SGD)接近固定点的稳定性。传统文献依赖于参数统计矩，特别是参数方差的收敛来量化稳定性。本文重新定义了SGD的稳定性，并使用\textit{概率收敛}条件来定义SGD的\textit{概率稳定性}。提出的稳定性直接回答了深度学习理论中的一个根本问题：SGD如何从数量庞大的可能严重过拟合的解中选择有意义的神经网络解。为了实现这一点，我们表明只有在概率性稳定性的镜头下，SGD才表现出丰富而实际相关的学习阶段，如完全失去稳定性阶段、不正确学习阶段、收敛到低秩鞍点阶段和正确学习阶段。当应用于神经网络时，这些相图意味着具有实际意义的稳定和不稳定区域。

    A fundamental open problem in deep learning theory is how to define and understand the stability of stochastic gradient descent (SGD) close to a fixed point. Conventional literature relies on the convergence of statistical moments, esp., the variance, of the parameters to quantify the stability. We revisit the definition of stability for SGD and use the \textit{convergence in probability} condition to define the \textit{probabilistic stability} of SGD. The proposed stability directly answers a fundamental question in deep learning theory: how SGD selects a meaningful solution for a neural network from an enormous number of solutions that may overfit badly. To achieve this, we show that only under the lens of probabilistic stability does SGD exhibit rich and practically relevant phases of learning, such as the phases of the complete loss of stability, incorrect learning, convergence to low-rank saddles, and correct learning. When applied to a neural network, these phase diagrams imply t
    
[^186]: 用表示学习和组合变量构建多变量网络的视觉分析

    Visual Analytics of Multivariate Networks with Representation Learning and Composite Variable Construction. (arXiv:2303.09590v1 [cs.SI])

    [http://arxiv.org/abs/2303.09590](http://arxiv.org/abs/2303.09590)

    本文提出了一种用于研究多变量网络的视觉分析工作流程，其中包括神经网络学习阶段、降维和优化阶段以及用户交互式可视化接口进行解释。关键的组合变量构建步骤将非线性特征重塑为线性特征，以方便检查和理解。案例研究表明该工作流程具有有效性和可理解性。

    

    多变量网络在真实世界的数据驱动应用中经常被发现。发掘和理解多变量网络中的关系并不是一项简单的任务。本文提出了一种用于研究多变量网络以提取网络不同结构和语义特征之间关联的视觉分析工作流程（例如，什么是在社交网络密度方面与不同属性的组合关系）。该工作流程包括基于神经网络的学习阶段，根据所选输入和输出属性来对数据进行分类，降维和优化阶段以产生一个简化的结果集合以便检查，最后通过用户交互式可视化接口进行解释阶段的操作。我们设计的一个关键部分是组合变量构建步骤，该步骤将由神经网络获得的非线性特征重塑为直观解释的线性特征。我们通过对大型组织员工之间的电子邮件通信数据集进行案例研究，证明了工作流程的有效性和可理解性。

    Multivariate networks are commonly found in real-world data-driven applications. Uncovering and understanding the relations of interest in multivariate networks is not a trivial task. This paper presents a visual analytics workflow for studying multivariate networks to extract associations between different structural and semantic characteristics of the networks (e.g., what are the combinations of attributes largely relating to the density of a social network?). The workflow consists of a neural-network-based learning phase to classify the data based on the chosen input and output attributes, a dimensionality reduction and optimization phase to produce a simplified set of results for examination, and finally an interpreting phase conducted by the user through an interactive visualization interface. A key part of our design is a composite variable construction step that remodels nonlinear features obtained by neural networks into linear features that are intuitive to interpret. We demon
    
[^187]: 在视觉Transformer中学习成长人工海马，实现弹性终身学习

    Learning to Grow Artificial Hippocampi in Vision Transformers for Resilient Lifelong Learning. (arXiv:2303.08250v1 [cs.CV])

    [http://arxiv.org/abs/2303.08250](http://arxiv.org/abs/2303.08250)

    本文提出了一种在Vision Transformer中学习成长人工海马的方法，以实现弹性终身学习。通过神经架构搜索进行维护，选取多头自注意力块中的最终线性投影层进行ArtiHippo的实现和成长。

    

    终身学习需要拥有人类智能的韧性，即不存在灾难性遗忘，这种韧性与大脑中复杂的记忆机制，尤其是海马维护的长期记忆（LM）紧密相关。Transformer已经成为人工智能“大脑”的对应体，但LM组件在终身学习中尚未充分探索。本文提出了一种在Vision Transformer中学习成长人工海马（ArtiHippo）以实现弹性终身学习的方法。通过全面消融实验，选定多头自注意力（MHSA）块中的最终线性投影层来实现和成长ArtiHippo。ArtiHippo由专家混合（MoEs）表示。每个专家组件是线性投影层的现场变体，通过神经架构搜索（NAS）进行维护，搜索空间由四个基本成长操作（跳过、重用、适应和新）定义。

    Lifelong learning without catastrophic forgetting (i.e., resiliency) possessed by human intelligence is entangled with sophisticated memory mechanisms in the brain, especially the long-term memory (LM) maintained by Hippocampi. To a certain extent, Transformers have emerged as the counterpart ``Brain" of Artificial Intelligence (AI), and yet leave the LM component under-explored for lifelong learning settings. This paper presents a method of learning to grow Artificial Hippocampi (ArtiHippo) in Vision Transformers (ViTs) for resilient lifelong learning. With a comprehensive ablation study, the final linear projection layer in the multi-head self-attention (MHSA) block is selected in realizing and growing ArtiHippo. ArtiHippo is represented by a mixture of experts (MoEs). Each expert component is an on-site variant of the linear projection layer, maintained via neural architecture search (NAS) with the search space defined by four basic growing operations -- skip, reuse, adapt, and new 
    
[^188]: 关于D-分离的不太可能性

    On the Unlikelihood of D-Separation. (arXiv:2303.05628v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05628](http://arxiv.org/abs/2303.05628)

    本文提供了分析证据表明，在大型图中，d-分离是罕见的现象，除非图是极其稀疏的。对于因果发现的PC算法和UniformSGS算法进行了平均情况分析，并给出了上界，上界以指数速度衰减到0。

    

    因果发现旨在从由其生成的数据中恢复因果图；基于约束的方法通过使用一个oracle在图中搜索一个d-分离的节点集来实现。本文提供了分析证据，即在大型图中，即使保证存在，d-分离也是一个罕见的现象，除非图是极其稀疏的。我们还对因果发现的PC算法进行了平均情况分析，以及我们称之为UniformSGS的SGS算法的变体。我们考虑一个节点集V={v1，...，vn}，并生成一个随机DAG G=(V,E)，其中如果a<b，则(va，vb)∈E的独立概率为p1，如果a>b，则为0。我们给出了当x和y可d-分离时，集合V- {x，y} d-分离x和y的概率的上界；我们的上界在|V|→∞时以指数速度衰减到0。对于PC算法，虽然已知其最坏情况的保证在非稀疏图上失效。

    Causal discovery aims to recover a causal graph from data generated by it; constraint based methods do so by searching for a d-separating conditioning set of nodes in the graph via an oracle. In this paper, we provide analytic evidence that on large graphs, d-separation is a rare phenomenon, even when guaranteed to exist, unless the graph is extremely sparse. We then provide an analytic average case analysis of the PC Algorithm for causal discovery, as well as a variant of the SGS Algorithm we call UniformSGS. We consider a set $V=\{v_1,\ldots,v_n\}$ of nodes, and generate a random DAG $G=(V,E)$ where $(v_a, v_b) \in E$ with i.i.d. probability $p_1$ if $a<b$ and $0$ if $a > b$. We provide upper bounds on the probability that a subset of $V-\{x,y\}$ d-separates $x$ and $y$, conditional on $x$ and $y$ being d-separable; our upper bounds decay exponentially fast to $0$ as $|V| \rightarrow \infty$. For the PC Algorithm, while it is known that its worst-case guarantees fail on non-sparse gr
    
[^189]: Lumos: 针对分布式设备的异构感知联邦图学习

    Lumos: Heterogeneity-aware Federated Graph Learning over Decentralized Devices. (arXiv:2303.00492v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00492](http://arxiv.org/abs/2303.00492)

    Lumos是第一个支持节点级联邦图学习的框架，具有特征和度数保护功能。它采用了树构造器和去中心化节点聚合策略来提高图的表示能力。

    

    由于图神经网络(GNN)能够处理图结构数据，因此在现实世界的网络应用和系统中广泛使用。然而，对数据隐私的日益关注严重挑战了传统的集中式模型训练范式，其中服务器持有所有图信息。联邦学习是一种新兴的协作计算范式，可以在不集中数据的情况下进行模型训练。现有的联邦GNN研究主要集中在客户端持有不同的图或子图的系统上。然而，还没有对每个客户端仅知道其直接邻居的实际节点级联邦情况进行研究。在本文中，我们提出了第一个支持节点级联邦图的有监督和无监督学习的联邦GNN框架Lumos，它具有特征和度数保护功能。我们首先设计了一个树构造器，以提高在有限结构信息下的表示能力。我们还提出了一种基于邻居信息的去中心化节点聚合策略。

    Graph neural networks (GNN) have been widely deployed in real-world networked applications and systems due to their capability to handle graph-structured data. However, the growing awareness of data privacy severely challenges the traditional centralized model training paradigm, where a server holds all the graph information. Federated learning is an emerging collaborative computing paradigm that allows model training without data centralization. Existing federated GNN studies mainly focus on systems where clients hold distinctive graphs or sub-graphs. The practical node-level federated situation, where each client is only aware of its direct neighbors, has yet to be studied. In this paper, we propose the first federated GNN framework called Lumos that supports supervised and unsupervised learning with feature and degree protection on node-level federated graphs. We first design a tree constructor to improve the representation capability given the limited structural information. We fur
    
[^190]: 无限维扩散模型

    Infinite-Dimensional Diffusion Models. (arXiv:2302.10130v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.10130](http://arxiv.org/abs/2302.10130)

    该论文提出了一种在无限维度中直接制定扩散基于的生成模型的方法，相比于传统的先离散化再应用扩散模型的方法，这种方法能够避免参数细化导致算法性能下降，同时提供了维度无关的距离界限，为无限维扩散模型设计提供了准则。

    

    扩散模型对于许多应用领域都产生了深远的影响，包括那些数据本质上是无限维的领域，如图像或时间序列。标准方法是首先离散化数据，然后将扩散模型应用于离散的数据。然而，这种方法在细化离散化参数时通常会导致算法性能下降。在本文中，我们直接在无限维度中制定基于扩散的生成模型，并将其应用于函数的生成建模。我们证明了我们的公式在无限维度环境中是良好定义的，并提供了从样本到目标测度的维度无关的距离界限。利用我们的理论，我们还制定了无限维扩散模型设计的准则。对于图像分布，这些准则与当前用于扩散模型的经典选择一致。对于其他分布...

    Diffusion models have had a profound impact on many application areas, including those where data are intrinsically infinite-dimensional, such as images or time series. The standard approach is first to discretize and then to apply diffusion models to the discretized data. While such approaches are practically appealing, the performance of the resulting algorithms typically deteriorates as discretization parameters are refined. In this paper, we instead directly formulate diffusion-based generative models in infinite dimensions and apply them to the generative modeling of functions. We prove that our formulations are well posed in the infinite-dimensional setting and provide dimension-independent distance bounds from the sample to the target measure. Using our theory, we also develop guidelines for the design of infinite-dimensional diffusion models. For image distributions, these guidelines are in line with the canonical choices currently made for diffusion models. For other distribut
    
[^191]: 基于Koopman算子的全秩权重的泛化界限：新的观点

    Koopman-based generalization bound: New aspect for full-rank weights. (arXiv:2302.05825v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05825](http://arxiv.org/abs/2302.05825)

    我们提出了一种使用Koopman算子对全秩神经网络权重进行泛化的新界限，当权重矩阵的条件数较小时，该界限比现有基于范数的界限更紧。我们的界限不与现有界限相矛盾，而是对现有界限进行的补充。此外，我们的界限可以与现有界限结合以得到更紧的界限。该研究结果为理解全秩权重神经网络的泛化提供了新的视角，同时也为算子理论分析和神经网络泛化之间提供了连接。

    

    我们提出了一种使用Koopman算子对神经网络进行泛化的新界限。大部分现有研究都集中在低秩权重矩阵上，而我们专注于全秩权重矩阵。当权重矩阵的条件数较小时，我们的界限比现有基于范数的界限更紧。特别地，如果权重矩阵是正交的，我们的界限与网络的宽度完全无关。我们的界限不与现有界限相矛盾，而是对现有界限进行的补充。由几个已有实验证明，低秩性并不是泛化的唯一原因。此外，我们的界限可以与现有界限结合以得到更紧的界限。我们的结果为理解具有全秩权重矩阵的神经网络的泛化提供了新的视角，同时还为算子理论分析和神经网络泛化之间提供了连接。

    We propose a new bound for generalization of neural networks using Koopman operators. Whereas most of existing works focus on low-rank weight matrices, we focus on full-rank weight matrices. Our bound is tighter than existing norm-based bounds when the condition numbers of weight matrices are small. Especially, it is completely independent of the width of the network if the weight matrices are orthogonal. Our bound does not contradict to the existing bounds but is a complement to the existing bounds. As supported by several existing empirical results, low-rankness is not the only reason for generalization. Furthermore, our bound can be combined with the existing bounds to obtain a tighter bound. Our result sheds new light on understanding generalization of neural networks with full-rank weight matrices, and it provides a connection between operator-theoretic analysis and generalization of neural networks.
    
[^192]: 回顾链将语言模型与反馈对齐

    Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02676](http://arxiv.org/abs/2302.02676)

    该研究提出了一种新颖的技术，即回顾链，可以轻松优化，并可以从任何形式的反馈中学习，而不受其极性的影响。

    

    从人类偏好中学习对于语言模型具有重要意义，这样才能对人类有所帮助并符合人类和社会价值观。先前的研究通过从人类反馈中学习来理解和遵循指令取得了显著成功。然而，这些方法要么是基于被人类注释者喜欢的手动挑选的模型生成，使得它们在数据利用方面效果不佳且普遍应用具有挑战性，要么依赖于奖励函数和强化学习，这容易出现奖励函数不完美和极难优化的问题。在本文中，我们提出了一种新颖的技术，“回顾链”，它易于优化，并可以从任何形式的反馈中学习，而不受其极性的影响。我们的想法受到了人类如何从以语言形式呈现的广泛反馈中学习的启发。我们将所有类型的反馈转换成句子，然后用它们来微调模型，从而利用这种方法。

    Learning from human preferences is important for language models to be helpful and useful for humans, and to align with human and social values. Prior work have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them ineffective in terms of data utilization and challenging to apply in general, or they depend on reward functions and reinforcement learning, which are prone to imperfect reward function and extremely challenging to optimize. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sentences, which are then used to fine-tune the model, allowing us to take advantage
    
[^193]: 探索和利用辅助数据来改善小样本泛化问题

    Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data. (arXiv:2302.00674v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00674](http://arxiv.org/abs/2302.00674)

    本文提出了一种在少样本学习过程中假定有辅助数据的训练范式FLAD，并针对自动混合辅助和目标数据的方法局限，提出了两种计算复杂度独立于辅助数据集数量的算法，通过FLAD和这两种算法的比较，可以发现这两种算法的表现更好。

    

    小样本学习在许多实际应用中都有价值，但学习一个通用的模型且不过度拟合少数标记数据点是具有挑战性的。本文关注辅助数据的小样本学习（FLAD），一种在少样本学习过程中假定有辅助数据的训练范式，以期提高泛化性能。先前的工作已经提出了自动混合辅助和目标数据的方法，但这些方法通常随辅助数据集的数量呈线性（或更差）缩放，从而限制了它们的实用性。在本文中，我们将FLAD与在多臂老虎机设置中至关重要的探索与利用困境联系起来，并推导出算法，其计算复杂度独立于辅助数据集的数量，从而使我们能够扩展到比先前方法多100倍的辅助数据集。我们提出了两种算法——EXP3-FLAD和UCB1-FLAD，并将它们与先前只进行探索或利用的FLAD方法进行了比较，发现这些算法表现更好。

    Few-shot learning is valuable in many real-world applications, but learning a generalizable model without overfitting to the few labeled datapoints is challenging. In this work, we focus on Few-shot Learning with Auxiliary Data (FLAD), a training paradigm that assumes access to auxiliary data during few-shot learning in hopes of improving generalization. Previous works have proposed automated methods for mixing auxiliary and target data, but these methods typically scale linearly (or worse) with the number of auxiliary datasets, limiting their practicality. In this work we relate FLAD to the explore-exploit dilemma that is central to the multi-armed bandit setting and derive algorithms whose computational complexity is independent of the number of auxiliary datasets, allowing us to scale to 100x more auxiliary datasets than prior methods. We propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- and compare them with prior FLAD methods that either explore or exploit, finding that the com
    
[^194]: 深度算子学习减轻了PDE中维数灾难

    Deep Operator Learning Lessens the Curse of Dimensionality for PDEs. (arXiv:2301.12227v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12227](http://arxiv.org/abs/2301.12227)

    本文指出，利用DNN对Banach空间上的Lipschitz算子进行深度算子学习可以减轻PDE中的维数灾难，包括椭圆方程、抛物线方程和Burgers方程，并可用于提供算子学习中的离散化不变性的见解。

    

    深度神经网络在许多领域均已获得显著成功，并且它们在PDE相关问题中的应用正在迅速发展。本文利用DNNs估计了在Banach空间上学习Lipschitz算子的泛化误差，并应用于各种PDE解算子。我们的目标是指定DNN的宽度、深度和需要训练样本的数量，以保证一定的测试误差。在对数据分布或算子结构进行温和的假设下，我们的分析表明，深度算子学习可以放松对PDE离散化分辨率的依赖，从而减轻许多与PDE相关的问题中的维数灾难，包括椭圆方程、抛物线方程和Burgers方程。我们的结果也应用于提供算子学习中的离散化不变性的见解。

    Deep neural networks (DNNs) have achieved remarkable success in numerous domains, and their application to PDE-related problems has been rapidly advancing. This paper provides an estimate for the generalization error of learning Lipschitz operators over Banach spaces using DNNs with applications to various PDE solution operators. The goal is to specify DNN width, depth, and the number of training samples needed to guarantee a certain testing error. Under mild assumptions on data distributions or operator structures, our analysis shows that deep operator learning can have a relaxed dependence on the discretization resolution of PDEs and, hence, lessen the curse of dimensionality in many PDE-related problems including elliptic equations, parabolic equations, and Burgers equations. Our results are also applied to give insights about discretization-invariant in operator learning.
    
[^195]: 通过代理建模实现高效的激活函数优化

    Efficient Activation Function Optimization through Surrogate Modeling. (arXiv:2301.05785v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05785](http://arxiv.org/abs/2301.05785)

    本文提出了一种基于代理建模的方法，通过扩展的基准测试空间在较少的函数评估次数中发现了优化的高效激活函数架构，并在多个标准基准测试中实现了最先进的性能。

    

    精心设计的激活函数可以提高神经网络在许多机器学习任务中的性能。然而，人类很难构建最优激活函数，而当前的激活函数搜索算法过于昂贵。本文通过三个步骤旨在改进现有技术：首先，通过使用2,913个系统生成的激活函数从头训练卷积、残差和视觉变换器架构来创建 Act-Bench-CNN、Act-Bench-ResNet 和 Act-Bench-ViT 基准数据集。第二，开发了基于代理的方法用于优化基准空间，发现与模型预测分布和激活函数输出分布相关联的 Fisher 信息矩阵的频谱对性能的预测性很高。第三，使用代理在较少的函数评估次数中发现了改进的激活函数架构，同时在几个标准基准测试中实现了最先进的性能。

    Carefully designed activation functions can improve the performance of neural networks in many machine learning tasks. However, it is difficult for humans to construct optimal activation functions, and current activation function search algorithms are prohibitively expensive. This paper aims to improve the state of the art through three steps: First, the benchmark datasets Act-Bench-CNN, Act-Bench-ResNet, and Act-Bench-ViT were created by training convolutional, residual, and vision transformer architectures from scratch with 2,913 systematically generated activation functions. Second, a characterization of the benchmark space was developed, leading to a new surrogate-based method for optimization. More specifically, the spectrum of the Fisher information matrix associated with the model's predictive distribution at initialization and the activation function's output distribution were found to be highly predictive of performance. Third, the surrogate was used to discover improved activ
    
[^196]: 使用新的广义加权组稀疏包络正则化学习k级稀疏神经网络

    Learning k-Level Sparse Neural Networks Using a New Generalized Weighted Group Sparse Envelope Regularization. (arXiv:2212.12921v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12921](http://arxiv.org/abs/2212.12921)

    本论文提出了一种利用加权组稀疏包络正则化方法学习k级稀疏神经网络的高效方法，同时保证网络的硬件友好的结构化稀疏性，加快网络评估速度，而且能够在训练中预定义稀疏度水平，同时几乎不降低网络准确度甚至有可能提高。

    

    我们提出了一种高效的方法，在训练过程中学习无结构和有结构稀疏的神经网络，利用一种称为"加权组稀疏包络函数" (WGSEF) 的稀疏包络函数的新广义。WGSEF作为一个神经元组选择器，用于引导结构化稀疏性。该方法能够确保深度神经网络 (DNN) 的硬件友好的结构化稀疏性，以有效加速DNN的评估。值得注意的是，该方法是可适应的，允许任何硬件指定组定义，如滤波器、通道、滤波器形状、层深度、单个参数 (无结构)等。由于WGSEF的特性，所提出的方法可以在训练收敛时预定义稀疏度水平，同时保持网络准确度的极小降低甚至改善。我们引入了一种高效的技术来计算精确的...

    We propose an efficient method to learn both unstructured and structured sparse neural networks during training, utilizing a novel generalization of the sparse envelope function (SEF) used as a regularizer, termed {\itshape{weighted group sparse envelope function}} (WGSEF). The WGSEF acts as a neuron group selector, which is leveraged to induce structured sparsity. The method ensures a hardware-friendly structured sparsity of a deep neural network (DNN) to efficiently accelerate the DNN's evaluation. Notably, the method is adaptable, letting any hardware specify group definitions, such as filters, channels, filter shapes, layer depths, a single parameter (unstructured), etc. Owing to the WGSEF's properties, the proposed method allows to a pre-define sparsity level that would be achieved at the training convergence, while maintaining negligible network accuracy degradation or even improvement in the case of redundant parameters. We introduce an efficient technique to calculate the exact
    
[^197]: 用引导想象扩充小规模数据集

    Expanding Small-Scale Datasets with Guided Imagination. (arXiv:2211.13976v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.13976](http://arxiv.org/abs/2211.13976)

    本论文提出了一个引导想象框架(GIF)，通过利用DALL-E2和Stable Diffusion (SD)等生成模型，从种子数据中扩充小规模数据集。该框架通过在先验模型的语义空间中优化种子数据潜在特征来创建逼真的图像，并引入了类别保持和样本多样性的标准来指导想象过程。

    

    DNN的功效在很大程度上取决于训练数据的数量和质量。然而，大规模收集和标注数据通常费时费力。为了解决这个问题，我们探索了一项名为数据集扩充的新任务，旨在通过自动创建新的标记样本来扩充一个小规模的可用数据集。为此，我们提出了一个引导想象框架(GIF)，利用DALL-E2和Stable Diffusion (SD)等尖端生成模型的力量，从输入的种子数据中“想象”并创建信息丰富的新数据。具体而言，GIF通过在先验模型的语义有意义的空间中优化种子数据的潜在特征来进行数据的想象，从而创建具有新内容的照片般逼真的图像。为了引导想象朝着创建用于模型训练的信息丰富样本的方向发展，我们引入了两个关键标准，即类别保持信息提升和样本多样性促进。这些标准被证明有效地提高了生成图像的质量和多样性。

    The power of DNNs relies heavily on the quantity and quality of training data. However, collecting and annotating data on a large scale is often expensive and time-consuming. To address this issue, we explore a new task, termed dataset expansion, aimed at expanding a ready-to-use small dataset by automatically creating new labeled samples. To this end, we present a Guided Imagination Framework (GIF) that leverages cutting-edge generative models like DALL-E2 and Stable Diffusion (SD) to "imagine" and create informative new data from the input seed data. Specifically, GIF conducts data imagination by optimizing the latent features of the seed data in the semantically meaningful space of the prior model, resulting in the creation of photo-realistic images with new content. To guide the imagination towards creating informative samples for model training, we introduce two key criteria, i.e., class-maintained information boosting and sample diversity promotion. These criteria are verified to
    
[^198]: 探索物理潜空间用于高分辨率流恢复

    Exploring Physical Latent Spaces for High-Resolution Flow Restoration. (arXiv:2211.11298v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11298](http://arxiv.org/abs/2211.11298)

    本文探索了使用物理模拟的潜空间来训练深度神经网络模型，实现了对细致模拟的恢复。通过将模拟的自由度作为神经网络的工具，并使用物理降维潜空间进行学习，神经网络能够发现改进给定任务性能的替代动态。

    

    本文通过偏微分方程（PDEs）的物理模拟与训练深度神经网络模型相结合，探索了使用模拟自由度作为神经网络的潜空间。与先前的工作不同，本文将模拟空间的自由度纯粹视为神经网络要使用的工具。我们展示了这个概念用于学习降维表示的能力，因为使用传统的降维表示在长时间跨度上忠实地保留正确解决方案是非常具有挑战性的，特别是对于具有大量小尺度特征的解决方案。本文着重于使用这种物理降维潜空间来恢复精细模拟，通过训练模型，使其能够根据学习目标尽可能多地修改降维的物理状态的内容。这种自主性允许神经网络发现改进给定任务性能的替代动态。

    We explore training deep neural network models in conjunction with physics simulations via partial differential equations (PDEs), using the simulated degrees of freedom as latent space for a neural network. In contrast to previous work, this paper treats the degrees of freedom of the simulated space purely as tools to be used by the neural network. We demonstrate this concept for learning reduced representations, as it is extremely challenging to faithfully preserve correct solutions over long time-spans with traditional reduced representations, particularly for solutions with large amounts of small scale features. This work focuses on the use of such physical, reduced latent space for the restoration of fine simulations, by training models that can modify the content of the reduced physical states as much as needed to best satisfy the learning objective. This autonomy allows the neural networks to discover alternate dynamics that significantly improve the performance in the given task
    
[^199]: 连续时间情节马尔可夫决策过程的平方根遗憾界限

    Square-root regret bounds for continuous-time episodic Markov decision processes. (arXiv:2210.00832v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00832](http://arxiv.org/abs/2210.00832)

    本论文研究了连续时间情节马尔可夫决策过程的强化学习问题，提出了一种基于值迭代和上界置信区间的学习算法，并导出了该算法的最坏情况下预期遗憾的上界和下界，均为情节数的平方根阶。模拟实验结果证明了该算法的性能。

    

    我们研究了有限时间情节马尔可夫决策过程（MDPs）的强化学习。与离散时间MDPs不同，连续时间MDPs的转换时间在每次转换时会按指数分布，并且速率参数取决于每个状态-动作对。我们提出了一种基于值迭代和上界置信区间的学习算法。我们导出了所提算法的最坏情况下预期遗憾的上界，并建立了最坏情况下的下界，这两个界都与情节数的平方根有关。最后，我们进行了模拟实验来说明我们算法的性能。

    We study reinforcement learning for continuous-time Markov decision processes (MDPs) in the finite-horizon episodic setting. In contrast to discrete-time MDPs, the inter-transition times of a continuous-time MDP are exponentially distributed with rate parameters depending on the state--action pair at each transition. We present a learning algorithm based on the methods of value iteration and upper confidence bound. We derive an upper bound on the worst-case expected regret for the proposed algorithm, and establish a worst-case lower bound, both bounds are of the order of square-root on the number of episodes. Finally, we conduct simulation experiments to illustrate the performance of our algorithm.
    
[^200]: 用机器学习方法求解大规模双层和随机规划问题的新方法: 以自行车网络设计为例

    A Machine Learning Approach to Solving Large Bilevel and Stochastic Programs: Application to Cycling Network Design. (arXiv:2209.09404v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2209.09404](http://arxiv.org/abs/2209.09404)

    我们提出了一种基于机器学习的新方法，用于求解涉及大量独立从属者的双层规划问题。该方法通过从一个采样的子集中估计未采样从属者的目标值来优化模型，实现了对一般从属者特征的表征学习。

    

    我们提出了一种基于机器学习的新方法，用于求解涉及大量独立从属者的双层规划问题，其中特殊情况包括两阶段随机规划。我们提出了一个优化模型，明确考虑到从一个采样的子集的从属者，并利用机器学习模型估计未采样的从属者的目标值。与现有方法不同的是，我们将机器学习模型训练嵌入到优化问题中，这允许我们使用无法通过领导者决策表示的一般从属者特征。我们证明了生成的领导者决策在原始目标函数的最优间隙上的界限，该目标函数考虑到完整的从属者集合。然后，我们开发了从属者采样算法来缩小界限，并提出了一种表示学习方法来学习从属者特征，这些特征可以用作嵌入机器学习模型的输入。通过使用一个自行车网络设计问题的合成实例进行实验，

    We present a novel machine learning-based approach to solving bilevel programs that involve a large number of independent followers, which as a special case include two-stage stochastic programming. We propose an optimization model that explicitly considers a sampled subset of followers and exploits a machine learning model to estimate the objective values of unsampled followers. Unlike existing approaches, we embed machine learning model training into the optimization problem, which allows us to employ general follower features that can not be represented using leader decisions. We prove bounds on the optimality gap of the generated leader decision as measured by the original objective function that considers the full follower set. We then develop follower sampling algorithms to tighten the bounds and a representation learning approach to learn follower features, which can be used as inputs to the embedded machine learning model. Using synthetic instances of a cycling network design p
    
[^201]: 使用隐马尔可夫模型学习强化学习任务自动机

    Learning Task Automata for Reinforcement Learning using Hidden Markov Models. (arXiv:2208.11838v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.11838](http://arxiv.org/abs/2208.11838)

    本文提出了一种学习非马尔可夫任务规范的新方法，通过从代理经验中学习，将其表示为有限状态的任务自动机。利用隐马尔可夫模型和新的提炼方法，使得代理能够解决稀疏和非马尔可夫奖励的强化学习任务。

    

    在环境具有稀疏和非马尔可夫奖励的情况下，使用标量奖励信号训练强化学习代理通常是不可行的。此外，在训练之前手工创建这些奖励函数往往容易出错，特别是当环境的动力学只有部分已知时。本文提出了一种新颖的流程，通过对未知环境中代理经验的 episodes 进行学习，从而学习非马尔可夫任务规范的简洁有限状态“任务自动机”。我们利用了两个关键算法的见解。首先，我们通过将产品 MDP 视为部分可观测 MDP 并使用众所周知的 Baum-Welch 算法来学习隐马尔可夫模型，从而学习到了规范自动机和环境的 MDP（初始都未知）所组成的模型。其次，我们提出了一种新颖的方法，从学到的产品 MDP 中提炼任务自动机（假设为确定性有限自动机）。我们学到的任务自动机使得代理可以解决非马尔可夫任务规范。

    Training reinforcement learning (RL) agents using scalar reward signals is often infeasible when an environment has sparse and non-Markovian rewards. Moreover, handcrafting these reward functions before training is prone to misspecification, especially when the environment's dynamics are only partially known. This paper proposes a novel pipeline for learning non-Markovian task specifications as succinct finite-state `task automata' from episodes of agent experience within unknown environments. We leverage two key algorithmic insights. First, we learn a product MDP, a model composed of the specification's automaton and the environment's MDP (both initially unknown), by treating the product MDP as a partially observable MDP and using the well-known Baum-Welch algorithm for learning hidden Markov models. Second, we propose a novel method for distilling the task automaton (assumed to be a deterministic finite automaton) from the learnt product MDP. Our learnt task automaton enables the dec
    
[^202]: 学习ACOPF的分桶主动采样

    Bucketized Active Sampling for Learning ACOPF. (arXiv:2208.07497v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.07497](http://arxiv.org/abs/2208.07497)

    本文提出了一种新颖的主动学习框架——分桶主动采样（BAS），旨在在时间限制内训练最佳的OPF代理。BAS将输入分布分成桶，并使用收集函数确定下一次采样的位置。实验结果显示了BAS的好处。

    

    本文考虑最优潮流（OPF）的优化代理，即近似OPF的输入/输出关系的机器学习模型。最近的研究集中在证明这些代理可以具有较高的准确性。然而，它们的训练需要大量的数据，每个实例都需要对输入分布的样本进行OPF的（离线）求解。为了满足市场清算应用的要求，本文提出了一种新颖的主动学习框架——分桶主动采样（BAS），旨在在时间限制内训练最佳的OPF代理。BAS将输入分布分成桶，并使用收集函数确定下一次采样的位置。通过将相同的分桶应用于验证集，BAS利用标记的验证样本来选择未标记的样本。BAS还依赖于随时间增加和减少的自适应学习率。实验结果显示了BAS的好处。

    This paper considers optimization proxies for Optimal Power Flow (OPF), i.e., machine-learning models that approximate the input/output relationship of OPF. Recent work has focused on showing that such proxies can be of high fidelity. However, their training requires significant data, each instance necessitating the (offline) solving of an OPF for a sample of the input distribution. To meet the requirements of market-clearing applications, this paper proposes Bucketized Active Sampling (BAS), a novel active learning framework that aims at training the best possible OPF proxy within a time limit. BAS partitions the input distribution into buckets and uses an acquisition function to determine where to sample next. By applying the same partitioning to the validation set, BAS leverages labeled validation samples in the selection of unlabeled samples. BAS also relies on an adaptive learning rate that increases and decreases over time. Experimental results demonstrate the benefits of BAS.
    
[^203]: 物理约束深度学习用于气候降尺度

    Physics-Constrained Deep Learning for Climate Downscaling. (arXiv:2208.05424v6 [physics.ao-ph] UPDATED)

    [http://arxiv.org/abs/2208.05424](http://arxiv.org/abs/2208.05424)

    本文提出了一种物理约束深度学习降尺度模型的方法，以保证模型在预测物理变量时满足守恒定律，并提高其性能。

    This paper proposes a method for physics-constrained deep learning downscaling models to ensure that the models satisfy conservation laws when predicting physical variables, while improving their performance according to traditional metrics.

    可靠的高分辨率气候和天气数据的可用性对于指导气候适应和减缓的长期决策以及指导对极端事件的快速响应至关重要。预测模型受计算成本限制，因此通常生成粗分辨率预测。统计降尺度，包括深度学习的超分辨率方法，可以提供一种有效的方法来上采样低分辨率数据。然而，尽管在某些情况下取得了视觉上令人信服的结果，但这些模型在预测物理变量时经常违反守恒定律。为了保持物理量的守恒，我们开发了一种方法，保证深度学习降尺度模型满足物理约束条件，同时根据传统指标提高其性能。我们比较了不同的约束方法，并展示了它们在不同的神经架构以及各种气候和天气数据上的适用性。

    The availability of reliable, high-resolution climate and weather data is important to inform long-term decisions on climate adaptation and mitigation and to guide rapid responses to extreme events. Forecasting models are limited by computational costs and, therefore, often generate coarse-resolution predictions. Statistical downscaling, including super-resolution methods from deep learning, can provide an efficient method of upsampling low-resolution data. However, despite achieving visually compelling results in some cases, such models frequently violate conservation laws when predicting physical variables. In order to conserve physical quantities, we develop methods that guarantee physical constraints are satisfied by a deep learning downscaling model while also improving their performance according to traditional metrics. We compare different constraining approaches and demonstrate their applicability across different neural architectures as well as a variety of climate and weather
    
[^204]: 一种用于多步鲍型自适应异方差时间序列预测的通用框架

    A general framework for multi-step ahead adaptive conformal heteroscedastic time series forecasting. (arXiv:2207.14219v7 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.14219](http://arxiv.org/abs/2207.14219)

    本文介绍了一种名为AEnbMIMOCQR的新颖算法，通过自适应集成的方式，在不需要数据拆分的情况下，以分布无关的方式生成多步鲍型预测区间。该方法考虑了异方差性，并对分布转变具有鲁棒性，在实验中表现优于其他竞争方法。

    

    本文介绍了一种新颖的模型无关算法，名为自适应集成批量多输入多输出鲍型分位数回归（AEnbMIMOCQR），使得预测者能够以分布无关的方式生成固定预设失配率的多步鲍型预测区间。我们的方法基于鲍型预测原理，但不需要数据拆分，并且即使在数据不可互换的情况下也能提供接近精确的覆盖率。此外，所得到的预测区间在预测时间范围内经验证明有效，并且考虑了异方差性。AEnbMIMOCQR被设计成对分布转变具有鲁棒性，这意味着其预测区间在无限的时间范围内保持可靠，而无需重新训练或对数据生成过程进行不切实际的严格假设。通过系统实验，我们证明了我们的方法在鲍型预测中优于其他竞争方法。

    This paper introduces a novel model-agnostic algorithm called adaptive ensemble batch multi-input multi-output conformalized quantile regression (AEnbMIMOCQR} that enables forecasters to generate multi-step ahead prediction intervals for a fixed pre-specified miscoverage rate in a distribution-free manner. Our method is grounded on conformal prediction principles, however, it does not require data splitting and provides close to exact coverage even when the data is not exchangeable. Moreover, the resulting prediction intervals, besides being empirically valid along the forecast horizon, do not neglect heteroscedasticity. AEnbMIMOCQR is designed to be robust to distribution shifts, which means that its prediction intervals remain reliable over an unlimited period of time, without entailing retraining or imposing unrealistic strict assumptions on the data-generating process. Through methodically experimentation, we demonstrate that our approach outperforms other competitive methods on bo
    
[^205]: 通过异常值消除在连续学习中的内存填充问题

    Memory Population in Continual Learning via Outlier Elimination. (arXiv:2207.01145v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.01145](http://arxiv.org/abs/2207.01145)

    本文引入了一种名为内存异常值消除（MOE）的方法，通过选择来自标签同质子种群的样本来识别和消除内存缓冲区中的异常值，以解决连续学习中的遗忘问题。

    

    连续学习中的灾难性遗忘是发展连续学习算法的主要障碍。缓解遗忘的一种常见方法是使用内存缓冲区，在训练新任务时存储之前学习过的任务示例的子集。填充内存的默认方法是随机选择先前的示例，但这可能引入离群值或噪声样本，从而损害模型的泛化能力。本文介绍了一种名为内存异常值消除（MOE）的方法，通过选择来自标签同质子种群的样本来识别和消除内存缓冲区中的异常值。我们展示了高同质性的空间与更具代表性的特征空间相关联的关系。实际上，如果一个样本被不同标签的样本所包围，MOE会移除该样本。我们在CIFAR-10、CIFAR-100和CO数据集上证明了MOE的有效性。

    Catastrophic forgetting, the phenomenon of forgetting previously learned tasks when learning a new one, is a major hurdle in developing continual learning algorithms. A popular method to alleviate forgetting is to use a memory buffer, which stores a subset of previously learned task examples for use during training on new tasks. The de facto method of filling memory is by randomly selecting previous examples. However, this process could introduce outliers or noisy samples that could hurt the generalization of the model. This paper introduces Memory Outlier Elimination (MOE), a method for identifying and eliminating outliers in the memory buffer by choosing samples from label-homogeneous subpopulations. We show that a space with a high homogeneity is related to a feature space that is more representative of the class distribution. In practice, MOE removes a sample if it is surrounded by samples from different labels. We demonstrate the effectiveness of MOE on CIFAR-10, CIFAR-100, and CO
    
[^206]: 精馏决策树

    Distillation Decision Tree. (arXiv:2206.04661v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2206.04661](http://arxiv.org/abs/2206.04661)

    精馏决策树（DDT）是一种通过将黑盒模型中的知识精馏到决策树中来促进解释性的方法。该方法建立在知识精馏的理论基础上，并且在结构稳定性的条件下可以有效实现。

    

    机器学习模型，特别是黑盒模型，因其出色的预测能力而受到广泛青睐。然而，由于缺乏可解释性，它们经常面临批评和挑战。矛盾的是，它们强大的预测能力表明对底层数据有深入的理解，从而意味着重要的解释潜力。借助知识精馏的新概念，我们引入了精馏决策树（DDT）的方法。该方法将关于数据的知识从黑盒模型精馏到决策树中，从而促进了对黑盒模型的解释。通过知识精馏过程构建的DDT的可解释性在很大程度上依赖于其结构的稳定性。我们为DDT的结构稳定性建立了理论基础，证明其在一些假设下可以实现结构稳定性。此外，我们还开发了算法用于...

    Machine learning models, particularly the black-box models, are widely favored for their outstanding predictive capabilities. However, they often face scrutiny and criticism due to the lack of interpretability. Paradoxically, their strong predictive capabilities suggest a deep understanding about the underlying data, implying significant potential for interpretation. Leveraging the emerging concept of knowledge distillation, we introduced the method of distillation decision tree (DDT). This method enables the distillation of knowledge about the data from a black-box model into a decision tree, thereby facilitating the interpretation of the black-box model. Constructed through the knowledge distillation process, the interpretability of DDT relies significantly on the stability of its structure. We establish the theoretical foundations for the structural stability of DDT, demonstrating that its structure can achieve stability under mild assumptions. Furthermore, we develop algorithms for
    
[^207]: 局部不变性解释：通过局部不变性学习实现稳定且单向的解释

    Locally Invariant Explanations: Towards Stable and Unidirectional Explanations through Local Invariant Learning. (arXiv:2201.12143v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12143](http://arxiv.org/abs/2201.12143)

    本文提出了一种基于局部不变性学习的模型无关解释方法，通过借鉴不变风险最小化原理，可以生成高保真度、稳定且单向的解释。方法基于博弈论的形式化，能够在局部例子附近准确捕捉黑盒函数的梯度符号变化，选择恰当的特征归因。

    

    局部可解释的模型无关解释（LIME）方法是解释黑盒模型的最流行方法之一。尽管已经提出了许多变种，但很少有方法能够简单地生成既具有高保真度又稳定且直观的解释。在本文中，我们提出了一种新颖的观点，通过借鉴不变风险最小化（IRM）原理（最初用于全局的样本外泛化）来提供这种具有高保真度、稳定且单向的解释的模型无关的局部解释方法。我们的方法基于博弈论的形式化，在理论上证明了我们的方法在寻找解释的局部例子附近黑盒函数梯度突然改变符号的特征时具有很强的倾向性，而在其他情况下，它会选择更保守的（特征）归因。

    Locally interpretable model agnostic explanations (LIME) method is one of the most popular methods used to explain black-box models at a per example level. Although many variants have been proposed, few provide a simple way to produce high fidelity explanations that are also stable and intuitive. In this work, we provide a novel perspective by proposing a model agnostic local explanation method inspired by the invariant risk minimization (IRM) principle -originally proposed for (global) out-of-distribution generalization -- to provide such high fidelity explanations that are also stable and unidirectional across nearby examples. Our method is based on a game theoretic formulation where we theoretically show that our approach has a strong tendency to eliminate features where the gradient of the black-box function abruptly changes sign in the locality of the example we want to explain, while in other cases it is more careful and will choose a more conservative (feature) attribution, a 
    
[^208]: Sci-Net: 尺度不变模型用于从航空图像中分割建筑物

    Sci-Net: Scale Invariant Model for Buildings Segmentation from Aerial Imagery. (arXiv:2111.06812v5 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2111.06812](http://arxiv.org/abs/2111.06812)

    本文提出了一种尺度不变的神经网络架构Sci-Net，用于从广泛范围空间分辨率的航空图像中分割建筑物，并且在实验中表现出显著优越性能。

    

    建筑物的分割是地球观测和航空图像分析领域的基本任务。现有文献中大部分基于深度学习的方法只能应用于固定或狭窄范围的空间分辨率图像。在实际场景中，用户处理的是广泛的图像分辨率范围。因此，给定的航空图像通常需要进行重采样以匹配用于训练深度学习模型的空间分辨率数据集，这导致分割性能的下降。为了克服这一挑战，我们在本文中提出了尺度不变神经网络（Sci-Net）架构，用于从广泛范围的空间分辨率航空图像中分割建筑物。具体而言，我们的方法利用了UNet的层次表示和Dense Atrous Spatial Pyramid Pooling来提取细粒度的多尺度表示。Sci-Net在Open Cities AI和Multi-Scale Building数据集上明显优于现有模型。

    Buildings' segmentation is a fundamental task in the field of earth observation and aerial imagery analysis. Most existing deep learning-based methods in the literature can be applied to a fixed or narrow-range spatial resolution imagery. In practical scenarios, users deal with a broad spectrum of image resolutions. Thus, a given aerial image often needs to be re-sampled to match the spatial resolution of the dataset used to train the deep learning model, which results in a degradation in segmentation performance. To overcome this challenge, we propose, in this manuscript, Scale-invariant Neural Network (Sci-Net) architecture that segments buildings from wide-range spatial resolution aerial images. Specifically, our approach leverages UNet hierarchical representation and Dense Atrous Spatial Pyramid Pooling to extract fine-grained multi-scale representations. Sci-Net significantly outperforms state of the art models on the Open Cities AI and the Multi-Scale Building datasets with a ste
    
[^209]: 推荐系统中用于排名蒸馏的双重修正策略

    Dual Correction Strategy for Ranking Distillation in Top-N Recommender System. (arXiv:2109.03459v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2109.03459](http://arxiv.org/abs/2109.03459)

    本文提出了一种双重修正策略（DCD），用于在推荐系统中更有效地将教师模型的排名信息转移到学生模型。这种方法不仅充分利用了学生模型的预测误差，还提供了更全面的视角，解决了松弛排名蒸馏方法的限制。

    

    知识蒸馏是将训练充分的大模型（教师）的知识转移到小模型（学生）的重要研究领域，对于推荐系统的实际部署而言，它已成为一个重要的研究方向。最近，松弛排名蒸馏（RRD）表明，在推荐列表中蒸馏排名信息能够显著提高性能。然而，该方法仍然存在以下限制：1）它未充分利用学生模型的预测误差，使得训练效率不高；2）它只蒸馏用户侧的排名信息，在稀疏的隐式反馈下提供的视角不足。本文提出了一种更高效的蒸馏方法，即双重修正策略（DCD），通过教师模型和学生模型预测之间的差异来决定要蒸馏的知识。

    Knowledge Distillation (KD), which transfers the knowledge of a well-trained large model (teacher) to a small model (student), has become an important area of research for practical deployment of recommender systems. Recently, Relaxed Ranking Distillation (RRD) has shown that distilling the ranking information in the recommendation list significantly improves the performance. However, the method still has limitations in that 1) it does not fully utilize the prediction errors of the student model, which makes the training not fully efficient, and 2) it only distills the user-side ranking information, which provides an insufficient view under the sparse implicit feedback. This paper presents Dual Correction strategy for Distillation (DCD), which transfers the ranking information from the teacher model to the student model in a more efficient manner. Most importantly, DCD uses the discrepancy between the teacher model and the student model predictions to decide which knowledge to be disti
    
[^210]: Certifiers使神经网络容易受到可用性攻击。

    Certifiers Make Neural Networks Vulnerable to Availability Attacks. (arXiv:2108.11299v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.11299](http://arxiv.org/abs/2108.11299)

    对神经网络进行认证以确保预测的鲁棒性是实现可靠AI的关键概念，然而这种方法存在安全风险，对手可以故意触发后备策略，使系统容易受到可用性攻击。

    

    为了实现可靠、鲁棒和安全的人工智能系统，在不能信任AI预测时实施后备策略是至关重要的。对神经网络的认证是检查这些预测鲁棒性的可靠方法。他们保证对于一些预测，某些操纵或攻击方式不会改变结果。对于没有保证的其他预测，该方法会放弃预测，并需要调用后备策略，这通常会增加额外的成本，可能需要人工操作，甚至无法提供任何预测。虽然这是实现安全和可靠的人工智能的关键概念，但我们首次展示了这种方法自身存在安全风险，因为对手可以故意触发这些后备策略。除了某些输入和扰动自然发生时的放弃外，对手还可以在训练时使用攻击方式以很高的概率故意触发后备策略。

    To achieve reliable, robust, and safe AI systems, it is vital to implement fallback strategies when AI predictions cannot be trusted. Certifiers for neural networks are a reliable way to check the robustness of these predictions. They guarantee for some predictions that a certain class of manipulations or attacks could not have changed the outcome. For the remaining predictions without guarantees, the method abstains from making a prediction, and a fallback strategy needs to be invoked, which typically incurs additional costs, can require a human operator, or even fail to provide any prediction. While this is a key concept towards safe and secure AI, we show for the first time that this approach comes with its own security risks, as such fallback strategies can be deliberately triggered by an adversary. In addition to naturally occurring abstains for some inputs and perturbations, the adversary can use training-time attacks to deliberately trigger the fallback with high probability. Th
    
[^211]: 约束资源下神经模块专业化的动力学研究

    Dynamics of specialization in neural modules under resource constraints. (arXiv:2106.02626v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2106.02626](http://arxiv.org/abs/2106.02626)

    本研究使用人工神经网络模拟实验，发现结构模块化并不一定能够确保功能专业化，在特定环境和资源限制下，才能够出现专业化现象。

    

    长期以来，人们一直认为大脑在结构和功能上高度模块化，但最近的证据使一些人对两种模块化的程度产生了怀疑。我们使用人工神经网络来测试结构模块化是否足以保证功能专业化，并发现一般情况下，并不一定成立，除非在极端水平上。然后，我们系统地测试了环境和网络的哪些特征会导致专业化的出现。我们使用了一个简单的玩具环境、任务和网络，以精确控制条件，并表明在这个设置中，几个不同的专业化度量指标给出了类似的结果。我们进一步发现，（1）专业化只能在环境中那些可以明确分离的特征存在的情况下出现，（2）专业化更容易在网络资源受到强烈限制的情况下出现，（3）这些发现在 qualitatively 上相似。

    It has long been believed that the brain is highly modular both in terms of structure and function, although recent evidence has led some to question the extent of both types of modularity. We used artificial neural networks to test the hypothesis that structural modularity is sufficient to guarantee functional specialization, and find that in general, this doesn't necessarily hold except at extreme levels. We then systematically tested which features of the environment and network do lead to the emergence of specialization. We used a simple toy environment, task and network, allowing us precise control, and show that in this setup, several distinct measures of specialization give qualitatively similar results. We further find that (1) specialization can only emerge in environments where features of that environment are meaningfully separable, (2) specialization preferentially emerges when the network is strongly resource-constrained, and (3) these findings are qualitatively similar ac
    
[^212]: 深度对比基于补丁的子空间学习用于相机图像信号处理

    Deep Contrastive Patch-Based Subspace Learning for Camera Image Signal Processing. (arXiv:2104.00253v4 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2104.00253](http://arxiv.org/abs/2104.00253)

    本论文提出了一种称为补丁子空间学习自动编码器 (PSL-AE) 的深度神经网络模型，用于改进相机图像信号处理的鲁棒性，特别是在处理异质伪影（如图像降噪）时。

    

    相机图像信号处理 (ISP) 管道可以在不同的图像信号处理任务中获得令人满意的结果。然而，大多数这些方法，包括采用编码器-解码器深度架构的方法，通常会使用一致应用于整个图像的统一滤波器。然而，自然地将相机图像视为异质的，因为颜色强度和人工噪声分布在单个图像的二维域内都存在巨大的差异。各种各样的莫尔环，运动模糊，褪色或基于镜头的投影失真都有可能导致异质图像伪影滤波问题。在本文中，我们提出了一种特定的基于补丁的局部子空间深度神经网络，改进了相机ISP的鲁棒性，使其对异质伪影（特别是图像降噪）具有更好的效果。我们将我们的三重深度训练模型称为补丁子空间学习自动编码器 (PSL-AE)。

    Camera Image Signal Processing (ISP) pipelines can get appealing results in different image signal processing tasks. Nonetheless, the majority of these methods, including those employing an encoder-decoder deep architecture for the task, typically utilize a uniform filter applied consistently across the entire image. However, it is natural to view a camera image as heterogeneous, as the color intensity and the artificial noise are distributed vastly differently, even across the two-dimensional domain of a single image. Varied Moire ringing, motion blur, color-bleaching, or lens-based projection distortions can all potentially lead to a heterogeneous image artifact filtering problem. In this paper, we present a specific patch-based, local subspace deep neural network that improves Camera ISP to be robust to heterogeneous artifacts (especially image denoising). We call our three-fold deep-trained model the Patch Subspace Learning Autoencoder (PSL-AE). The PSL-AE model does not make assum
    
[^213]: 简单均匀抽样对具有异常值的基于中心的聚类是否有效：何时以及为什么？

    Is Simple Uniform Sampling Effective for Center-Based Clustering with Outliers: When and Why?. (arXiv:2103.00558v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.00558](http://arxiv.org/abs/2103.00558)

    本文提出了一个简单的均匀抽样框架用于解决带有异常值的中心聚类问题，并通过引入“显著性”度量来解释其有效性。在假设给定实例是“显著”的情况下，样本大小可以独立于输入数据的大小和维度。该方法具有简单性和显著优势。

    

    真实世界的数据集通常包含异常值，异常值的存在可以使聚类问题变得更具挑战性。在本文中，我们提出了一个简单的均匀抽样框架，用于解决具有异常值的三种代表性的基于中心的聚类问题：带有异常值的$k$中心/中位数/均值聚类。我们的分析与先前的（均匀和非均匀）抽样思想根本不同。为了理论上解释均匀抽样的有效性，我们引入了一个“显著性”度量，并证明我们的框架的性能取决于给定实例的显著性程度。特别地，如果我们假设给定实例是“显著”的话，样本大小可以独立于输入数据大小$n$和维度$d$，这在实践中是相当合理的假设。由于其简单性，与非均匀抽样方法相比，均匀抽样方法还具有几个显著的优势。

    Real-world datasets often contain outliers, and the presence of outliers can make the clustering problems to be much more challenging. In this paper, we propose a simple uniform sampling framework for solving three representative center-based clustering with outliers problems: $k$-center/median/means clustering with outliers. Our analysis is fundamentally different from the previous (uniform and non-uniform) sampling based ideas. To explain the effectiveness of uniform sampling in theory, we introduce a measure of "significance" and prove that the performance of our framework depends on the significance degree of the given instance. In particular, the sample size can be independent of the input data size $n$ and the dimensionality $d$, if we assume the given instance is "significant", which is in fact a fairly reasonable assumption in practice. Due to its simplicity, the uniform sampling approach also enjoys several significant advantages over the non-uniform sampling approaches in pra
    
[^214]: 表面上的极小估计距离和等距到凸处理中的极小流形学习

    Minimax Estimation of Distances on a Surface and Minimax Manifold Learning in the Isometric-to-Convex Setting. (arXiv:2011.12478v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2011.12478](http://arxiv.org/abs/2011.12478)

    研究通过表面重建来获得在光滑次流形上估计内在距离的极小最优性以及在等距问题中使用重建表面计算距离的Isomap变体的极小最优性。

    

    我们首先考虑了在光滑次流形上估计内禀距离的问题。我们展示了通过对表面进行重建可以获得极小最优性，并讨论了用于此目的的特定网格构造——切点Delaunay复合体的使用。然后我们转向流形学习，并认为在重建的表面上计算距离的Isomap变体对于等距问题的极小最优性是合适的。

    We start by considering the problem of estimating intrinsic distances on a smooth submanifold. We show that minimax optimality can be obtained via a reconstruction of the surface, and discuss the use of a particular mesh construction -- the tangential Delaunay complex -- for that purpose. We then turn to manifold learning and argue that a variant of Isomap where the distances are instead computed on a reconstructed surface is minimax optimal for the isometric variant of the problem.
    
[^215]: 用于组合优化的强化学习综述

    A Survey on Reinforcement Learning for Combinatorial Optimization. (arXiv:2008.12248v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.12248](http://arxiv.org/abs/2008.12248)

    该论文综述了强化学习在组合优化中的应用，特别关注了旅行推销员问题。通过比较和总结现代强化学习算法和传统方法的差异，论文展示了深度强化学习的优势，并强调了将深度学习机制与强化学习相结合的有效性。

    

    该论文详细回顾了强化学习在组合优化中的应用，介绍了从1950年代开始的组合优化历史，并将其与近年来的强化学习算法进行了比较。论文特别关注了著名的组合问题——旅行推销员问题（TSP）。它将现代强化学习算法在TSP上的方法与1970年代发表的一种方法进行了比较。通过比较这些方法之间的相似性和差异性，论文展示了由于机器学习技术和计算能力的进化，强化学习算法是如何进行优化的。论文随后简要介绍了TSP中的深度强化学习方法，即基于传统数学框架的深度强化学习。在深度强化学习中，引入了注意力和特征编码机制，以生成近似最优解。调查结果显示，将注意力等深度学习机制与强化学习相结合，可以有效地近似解决组合优化问题。

    This paper gives a detailed review of reinforcement learning (RL) in combinatorial optimization, introduces the history of combinatorial optimization starting in the 1950s, and compares it with the RL algorithms of recent years. This paper explicitly looks at a famous combinatorial problem-traveling salesperson problem (TSP). It compares the approach of modern RL algorithms for the TSP with an approach published in the 1970s. By comparing the similarities and variances between these methodologies, the paper demonstrates how RL algorithms are optimized due to the evolution of machine learning techniques and computing power. The paper then briefly introduces the deep learning approach to the TSP named deep RL, which is an extension of the traditional mathematical framework. In deep RL, attention and feature encoding mechanisms are introduced to generate near-optimal solutions. The survey shows that integrating the deep learning mechanism, such as attention with RL, can effectively approx
    
[^216]: 将监督学习和VAEs统一在基于正态流的神经网络模型中对天文粒子重建进行覆盖、系统性和拟合好坏的研究

    Unifying supervised learning and VAEs -- coverage, systematics and goodness-of-fit in normalizing-flow based neural network models for astro-particle reconstructions. (arXiv:2008.05825v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.05825](http://arxiv.org/abs/2008.05825)

    本研究将监督学习和VAEs统一于基于正态流的神经网络模型中，对天文粒子重建进行了覆盖、系统性和拟合好坏的研究，并通过KL散度目标实现了监督学习和VAEs的统一。利用条件正态化流的方法可以计算神经网络模型的拟合优度p值。

    

    在天文粒子物理学中，基于神经网络的事件属性预测变得越来越常见。然而，在许多情况下，结果只被用作点预测。统计不确定性和覆盖率(1)，系统不确定性(2)或拟合优度度量(3)经常没有被计算。在这里，我们描述了一种特定的训练和网络架构选择，可以将所有这些属性融入到一个单一的网络模型中。我们展示了数据和标签联合分布的KL散度目标使得在随机变分推理的一种统一下将监督学习和变分自编码器(VAEs)统一起来。这种统一性激发了一种扩展的监督学习方案，可以计算神经网络模型的拟合优度p值。在这种建设中，利用神经网络进行的条件正态化流至关重要。我们讨论了它们如何为已定义的后验分布严格定义覆盖率。

    Neural-network based predictions of event properties in astro-particle physics are getting more and more common. However, in many cases the result is just utilized as a point prediction. Statistical uncertainties and coverage (1), systematic uncertainties (2) or a goodness-of-fit measure (3) are often not calculated. Here we describe a certain choice of training and network architecture that allows to incorporate all these properties into a single network model. We show that a KL-divergence objective of the joint distribution of data and labels allows to unify supervised learning and variational autoencoders (VAEs) under one umbrella of stochastic variational inference. The unification motivates an extended supervised learning scheme which allows to calculate a goodness-of-fit p-value for the neural network model. Conditional normalizing flows amortized with a neural network are crucial in this construction. We discuss how they allow to rigorously define coverage for posteriors defined
    
[^217]: 带有假定结构的张量聚类：统计最优性和计算限制的研究

    Tensor Clustering with Planted Structures: Statistical Optimality and Computational Limits. (arXiv:2005.10743v4 [math.ST] UPDATED)

    [http://arxiv.org/abs/2005.10743](http://arxiv.org/abs/2005.10743)

    本文研究了带有假定结构的高阶聚类的统计和计算限制，确定了聚类存在性和聚类支持的临界值，并提出了相应的算法。在超图种植团问题和超图种植稠密子图恢复的计算困难猜想下，我们证明了在特定信噪比范围内无法使用多项式时间算法解决这些问题。

    

    本文研究了带有假定结构的高阶聚类的统计和计算限制。我们关注两种聚类模型，常数高阶聚类（CHC）和秩一高阶聚类（ROHC），并研究了测试聚类存在性（检测）和识别聚类支持（恢复）的方法和理论。具体而言，我们确定了当信噪比处于某些临界值时，CHC和ROHC的检测/恢复是统计上可能的。我们还发展了紧密的计算阈值：当信噪比低于这些阈值时，我们证明在超图种植团问题（HPC）检测和超图种植稠密子图（HPDS）恢复的计算困难猜想下，多项式时间算法无法解决这些问题。此外，我们提出了多项式时间的张量算法，在信噪比高于这些阈值时实现可靠的检测和恢复。同时，我们还研究了稀疏性和...

    This paper studies the statistical and computational limits of high-order clustering with planted structures. We focus on two clustering models, constant high-order clustering (CHC) and rank-one higher-order clustering (ROHC), and study the methods and theory for testing whether a cluster exists (detection) and identifying the support of cluster (recovery).  Specifically, we identify the sharp boundaries of signal-to-noise ratio for which CHC and ROHC detection/recovery are statistically possible. We also develop the tight computational thresholds: when the signal-to-noise ratio is below these thresholds, we prove that polynomial-time algorithms cannot solve these problems under the computational hardness conjectures of hypergraphic planted clique (HPC) detection and hypergraphic planted dense subgraph (HPDS) recovery. We also propose polynomial-time tensor algorithms that achieve reliable detection and recovery when the signal-to-noise ratio is above these thresholds. Both sparsity an
    

