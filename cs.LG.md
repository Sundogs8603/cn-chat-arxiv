# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Policy Gradient Methods in the Presence of Symmetries and State Abstractions.](http://arxiv.org/abs/2305.05666) | 本文研究了在连续控制环境中的抽象，提出了一种策略梯度定理，允许利用环境的近似对称性进行策略优化，并提出了一系列演员-评论家算法进行策略和MDP同态映射的学习，最后展示了算法在连续对称性环境和视觉控制任务中的有效性。 |
| [^2] | [ImageBind: One Embedding Space To Bind Them All.](http://arxiv.org/abs/2305.05665) | ImageBind是一种新的跨模态联合嵌入方法，只需要使用图像配对数据就可以将不同模态的数据绑定在一起，并实现跨模态检索、组合和生成等多种应用。 |
| [^3] | [ShapeCoder: Discovering Abstractions for Visual Programs from Unstructured Primitives.](http://arxiv.org/abs/2305.05661) | ShapeCoder是能够从非结构化的基元中发现视觉程序的抽象的第一个系统，用于重写程序以使其更紧凑，暴露自由度更少。在对合成和现实世界数据集的实验中，ShapeCoder优于现有基线，程序大小减少了3倍。 |
| [^4] | [TidyBot: Personalized Robot Assistance with Large Language Models.](http://arxiv.org/abs/2305.05658) | 本文研究了使用机器人进行家庭清扫的个性化问题。通过使用大型语言模型少样本摘要能力，机器人可以学习用户的偏好并将其推广到未来的场景中，从而实现快速适应。 |
| [^5] | [Predicting Cardiovascular Disease Risk using Photoplethysmography and Deep Learning.](http://arxiv.org/abs/2305.05648) | 利用光电容积描记术和深度学习预测心血管疾病风险，可在低成本下对低收入和中等收入国家进行大规模筛查，并超越了传统风险评分工具。 |
| [^6] | [A duality framework for generalization analysis of random feature models and two-layer neural networks.](http://arxiv.org/abs/2305.05642) | 本文提出了一个针对随机特征模型和双层神经网络的泛化分析的对偶性框架，并证明了学习不会受到维数灾难的影响，使 RFMs 可以在核范围之外发挥作用。 |
| [^7] | [Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare.](http://arxiv.org/abs/2305.05640) | 本研究提出了一种在医疗保健领域构建面向实体的知识图谱的端到端表示学习方法 HEER，通过将领域特定的约束和特征纳入到图嵌入算法中，有效地改善了下游预测任务。 |
| [^8] | [Deep Learning-based Estimation for Multitarget Radar Detection.](http://arxiv.org/abs/2305.05621) | 本文提出了基于卷积神经网络 (CNN) 的新方法直接从雷达信号的 range-Doppler 地图中估计移动目标的距离和速度。在不同信噪比 (SNR) 区间内，该方法比其他方法提供更好的估计精度和较短的预测时间。 |
| [^9] | [Metric Space Magnitude and Generalisation in Neural Networks.](http://arxiv.org/abs/2305.05611) | 本研究使用一种新的拓扑不变量——大小，来量化深度神经网络的学习过程，研究其内部表示并提出一个新方法来确定其泛化能力，实验证明此框架可作为泛化错误的指标。 |
| [^10] | [Can point cloud networks learn statistical shape models of anatomies?.](http://arxiv.org/abs/2305.05610) | 本文研究了基于点云的统计形态建模技术，摆脱了传统基于对应关系的方法的瓶颈，提出了一种新的学习方法，并证明了它的有效性和鲁棒性。 |
| [^11] | [The Role of Relevance in Fair Ranking.](http://arxiv.org/abs/2305.05608) | 本文结合社会学、信息检索和机器学习公平性的角度和工具，着眼于相关性在公平排序中的应用和作用，并推导出相关性评分应满足的一组期望标准以实现有意义地指导公平干预措施。 |
| [^12] | [Deep Learning and Geometric Deep Learning: an introduction for mathematicians and physicists.](http://arxiv.org/abs/2305.05601) | 本文重点介绍了图神经网络在深度学习和几何深度学习中的关键要素：得分和损失函数，并解释了模型训练的主要步骤。 |
| [^13] | [StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure.](http://arxiv.org/abs/2305.05588) | 本文开发了StrAE框架，该框架利用句子结构无监督地学习多级节点嵌入，并且发现使用显式结构可以提高嵌入表现，新的对比目标优于标准的交叉熵目标。同时，完全忠实于结构确实能够根据相应模型的性能消除结构类型之间的歧义。 |
| [^14] | [Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels.](http://arxiv.org/abs/2305.05580) | 该论文提出了一种使用合成数据和伪标签的无监督领域自适应技术，在保持分类准确性的同时，将合成域的图像转换为真实世界域，用于服装视觉模式分类。 |
| [^15] | [FAENet: Frame Averaging Equivariant GNN for Materials Modeling.](http://arxiv.org/abs/2305.05577) | 本文提出了 FAENet ，一种简单、快速、表现力强的 GNN，用于材料建模，其通过随机帧平均（SFA）使任何模型 E(3) 等变或不变。理论和实验证明了其在准确性和可扩展性方面的优越性。 |
| [^16] | [An Algorithm For Adversary Aware Decentralized Networked MARL.](http://arxiv.org/abs/2305.05573) | 本文提出了一种对抗感知的去中心化网络多智能体强化学习算法，该算法允许非对抗性智能体在对抗方存在的情况下达成共识。 |
| [^17] | [SMAClite: A Lightweight Environment for Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2305.05566) | 本论文介绍了SMAClite，一个轻量级的多智能体强化学习环境，解耦了原有的闭源游戏并提供了开源框架，SMAClite在运行时速度和内存方面超越了SMAC。 |
| [^18] | [SkelEx and BoundEx: Natural Visualization of ReLU Neural Networks.](http://arxiv.org/abs/2305.05562) | SkelEx和BoundEx是第一批自然可视化的方法，可以提取ReLU神经网络的关键点和决策边界。这两种方法为在低维数据上训练的ReLU NN引入了非常自然的可视化工具。 |
| [^19] | [CIT-EmotionNet: CNN Interactive Transformer Network for EEG Emotion Recognition.](http://arxiv.org/abs/2305.05548) | 本研究提出了一种新颖的CNN交互式Transformer网络，用于EEG情绪识别，集成了全局和局部特征，通过CNN交互式Transformer模块促进局部和全局特征的交互和融合，获得了最先进的性能。 |
| [^20] | [Walk4Me: Telehealth Community Mobility Assessment, An Automated System for Early Diagnosis and Disease Progression.](http://arxiv.org/abs/2305.05543) | Walk4Me是一个远程康复社区行动能力评估系统，采用基于人工智能的步态特征检测来识别疾病患者的步态障碍并追踪疾病的进展。 |
| [^21] | [Localization of Ultra-dense Emitters with Neural Networks.](http://arxiv.org/abs/2305.05542) | 本论文介绍了一种名为LUENN的深度卷积神经网络，可以在发射器图像重叠显著时，利用独特的架构平滑地容纳从完全孤立到共聚的发射器，从而将半径为1微米平方内可用的发射器密度的范围扩大了6倍以上，降低了定位精度的惩罚，并提高了时间分辨率。 |
| [^22] | [Efficient pattern-based anomaly detection in a network of multivariate devices.](http://arxiv.org/abs/2305.05538) | 该论文提出了一种高效的基于模式挖掘的异常检测方法，该方法不仅考虑多元时间序列，还考虑了实体间的通信关系，并提供可解释的解释。实验表明，该方法比现有方法更有效率和有效。 |
| [^23] | [Integrating Holistic and Local Information to Estimate Emotional Reaction Intensity.](http://arxiv.org/abs/2305.05534) | 该论文提出了一种多模态架构，结合视频和音频信息来估计情感反应强度，使用回归标记解决了可变视频长度的问题，实现了对情感反应强度的准确估计。 |
| [^24] | [An ensemble of convolution-based methods for fault detection using vibration signals.](http://arxiv.org/abs/2305.05532) | 本文提出了集成了三种基于卷积的方法，用于解决振动信号的故障检测问题。实验结果表明，该方法在准确率上优于其他方法，达到了超过98.8\%的准确率。 |
| [^25] | [Modelling Concurrency Bugs Using Machine Learning.](http://arxiv.org/abs/2305.05531) | 人工智能的机器学习已被广泛应用，本文着眼于利用机器学习检测并发错误的可行性，挑战是设计有效的模型，解决嘈杂数据和数据不平衡等挑战。 |
| [^26] | [Accelerate Langevin Sampling with Birth-Death process and Exploration Component.](http://arxiv.org/abs/2305.05529) | 该论文提出了一种新的采样方法，在探索新模式和传递有用信息的过程中利用了Birth-Death过程和探索组件，具有高效和指数渐近收敛等优点。 |
| [^27] | [Exploring a Gradient-based Explainable AI Technique for Time-Series Data: A Case Study of Assessing Stroke Rehabilitation Exercises.](http://arxiv.org/abs/2305.05525) | 本文提出了一种基于梯度的可解释AI技术的方法，用于识别时间序列数据中的显著帧，在中风康复锻炼领域具有应用潜力。 |
| [^28] | [Minimal Learning Machine for Multi-Label Learning.](http://arxiv.org/abs/2305.05518) | 本研究提出了一种最小学习机方法来适应多标签学习，与其他方法相比，它具有最先进的性能、确定性以及简单的超参数选择方式。 |
| [^29] | [Balancing Privacy and Security in Federated Learning with FedGT: A Group Testing Framework.](http://arxiv.org/abs/2305.05506) | 该论文提出了FedGT框架，通过群体测试的方法在联邦学习中识别并删除恶意客户，从而平衡了隐私和安全，保护数据隐私并提高了识别恶意客户的能力。 |
| [^30] | [Recursions Are All You Need: Towards Efficient Deep Unfolding Networks.](http://arxiv.org/abs/2305.05505) | 本文提出了一个基于递归的框架来提高深度展开网络的效率，在训练过程中随机递归数量，以减少整体训练时间，同时引入了一个可学习单元来调节模型特征，可以使网络削减高达75%的可学习参数。 |
| [^31] | [Effects of Real-Life Traffic Sign Alteration on YOLOv7- an Object Recognition Model.](http://arxiv.org/abs/2305.05499) | 本研究调查了改变后的交通标志对目标识别模型准确性和性能的影响，结果表明当暴露于不太可能的条件下修改后的交通标志时，物体检测模型的准确率显著降低。 |
| [^32] | [Self-Supervised Anomaly Detection of Rogue Soil Moisture Sensors.](http://arxiv.org/abs/2305.05495) | 本文提出了一种自我监督的异常传感器检测方法，使用了基于具有对比损失的神经网络和DBSCAN的技术，通过负采样中使用动态时间规整来区分良好和异常的传感器，并在实验中验证了该方法的可行性。 |
| [^33] | [Investigating the effect of sub-word segmentation on the performance of transformer language models.](http://arxiv.org/abs/2305.05480) | 本文研究使用单语词段算法StateMorph训练语言模型时，可以使模型更高效地收敛并获得更好的验证分数。 |
| [^34] | [Going beyond research datasets: Novel intent discovery in the industry setting.](http://arxiv.org/abs/2305.05474) | 本文提出了一种用于电子商务领域实时意图发现的新方法，通过对现实生活数据进行预训练和利用对话结构细化，性能提高了33pp。 |
| [^35] | [Graph Neural Networks for Airfoil Design.](http://arxiv.org/abs/2305.05469) | 本文研究基于图神经网络的飞翼设计，利用已知架构的改进解决了求解二维不可压缩Navier-Stokes方程的近似解问题。同时，本文还关注了模型性能表现和新形状的自动生成。 |
| [^36] | [The emergence of clusters in self-attention dynamics.](http://arxiv.org/abs/2305.05465) | 本文证实了当Transformer处理一系列token时，出现“领导者”的经验观察，即随着时间趋于无穷大，代表token的粒子会聚集在特定的极限对象附近，这取决于价值矩阵的谱。 |
| [^37] | [Optimization- and AI-based approaches to academic quality quantification for transparent academic recruitment: part 1-model development.](http://arxiv.org/abs/2305.05460) | 该论文开发了两种计算框架来量化高校教育质量，通过一个单一指标（AQI）来综合衡量学者的学术品质。 |
| [^38] | [A Cross-Frequency Protective Emblem: Protective Options for Medical Units and Wounded Soldiers in the Context of (fully) Autonomous Warfare.](http://arxiv.org/abs/2305.05459) | 本文提出了一种能够应对(完全)自主战争中保护非战斗人员的跨频保护徽章设计方案。 |
| [^39] | [Robust Implicit Regularization via Weight Normalization.](http://arxiv.org/abs/2305.05448) | 本文提出了使用权重规范化的梯度下降作为过度参数化模型的鲁棒隐式正则化方法，实现了对欧几里德范数较低的参数的隐式偏好，并建立了一个统一框架来解决线性模型和神经网络之间的隐式正则化隔阂。 |
| [^40] | [Attention-Based Transformer Networks for Quantum State Tomography.](http://arxiv.org/abs/2305.05433) | 本研究提出一种基于注意力机制和变压器网络的 QST 方法，可捕捉不同测量之间的相关性，并成功应用于检索量子态的密度矩阵，特别是对于受限测量数据的情况表现良好。 |
| [^41] | [Echo from noise: synthetic ultrasound image generation using diffusion models for real image segmentation.](http://arxiv.org/abs/2305.05424) | 该论文提出了一种基于DDPM的方法来生成合成超声图像，用于医学图像分析任务中的有效替代品。在左心室和左房分割任务中，使用仅合成图像训练的神经网络相对于之前最先进方法提高了9.09％、3.7％和15.0％的Dice分数。 |
| [^42] | [Consistent Text Categorization using Data Augmentation in e-Commerce.](http://arxiv.org/abs/2305.05402) | 本文提出了一种在电子商务中使用数据增强实现一致的文本分类的新框架，该框架旨在改进产品分类模型的一致性，同时保持其生产水平的性能。 |
| [^43] | [Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions.](http://arxiv.org/abs/2305.05400) | 本研究探讨了使用随机Lp范数失真对图像分类器的训练和测试数据进行增强，并评估模型对不可感知随机失真的稳健性，发现稳健性可能会提高模型在随机失真方面的性能，但也可能会损害L∞范数的稳健性。 |
| [^44] | [On the Relation between Sharpness-Aware Minimization and Adversarial Robustness.](http://arxiv.org/abs/2305.05392) | SAM和对抗性训练（AT）都可以视为特定的特征扰动，其改善了对抗性能。然而，SAM和AT在扰动强度方面是不同的，从而带来了不同的精度和鲁棒性权衡。SAM单独使用可以在不牺牲清晰度精度的情况下提高对抗鲁棒性。 |
| [^45] | [Two to Five Truths in Non-Negative Matrix Factorization.](http://arxiv.org/abs/2305.05389) | 本文提出了一种受规范化拉普拉斯图的启发的矩阵缩放方法，可以大大提高非负矩阵分解在文本主题模型中的质量。 |
| [^46] | [TASTY: A Transformer based Approach to Space and Time complexitY.](http://arxiv.org/abs/2305.05379) | 本文旨在通过创建一个跨多种语言的代码片段标记数据集，以填补从代码中分类时间和空间复杂性的空白，并提出了使用基于代码的多模型来实现这一目标。 |
| [^47] | [Professional Certification Benchmark Dataset: The First 500 Jobs For Large Language Models.](http://arxiv.org/abs/2305.05377) | 本研究创建了一个基准数据集，测试了大型语言模型的职业准备技能，比较了GPT-3和Turbo-GPT3.5在1149个专业认证领域的表现，Turbo-GPT3.5的通过率达到了100%。模型证明了在计算机、医疗保健和金融等各个领域中的技能和潜力。这个数据集可以用来进一步训练和评估语言模型的表现。 |
| [^48] | [HybridNet: Dual-Branch Fusion of Geometrical and Topological Views for VLSI Congestion Prediction.](http://arxiv.org/abs/2305.05374) | 本文提出了HybridNet，一种基于几何与拓扑视角的VLSI阻塞预测的双分支融合网络，通过在网络结构中做出几个关键设计，充分综合电路的拓扑与几何特征，相较于以往方法取得了10.9％的提高。 |
| [^49] | [GNNs,You can be Stronger,Deeper and Faster.](http://arxiv.org/abs/2305.05368) | 本文介绍了一种新的理解GNN表现能力的视角，并提出了一个新的采样节点级残差模块SDF，具有更好的表现能力。 |
| [^50] | [Large Language Model Programs.](http://arxiv.org/abs/2305.05364) | 本文提出了一种将大型预训练语言模型嵌入算法或程序中，扩展其能力的方法。这种方法可以在未经微调的情况下通过更具算法性的方法获得不错的性能提升。 |
| [^51] | [Turning Privacy-preserving Mechanisms against Federated Learning.](http://arxiv.org/abs/2305.05355) | 本文探讨了如何将隐私保护机制应用于联邦学习，并指出了该配置中的一个关键安全漏洞，并设计了对此漏洞的攻击方式。 |
| [^52] | [Towards the Characterization of Representations Learned via Capsule-based Network Architectures.](http://arxiv.org/abs/2305.05349) | 本研究旨在评估胶囊网络架构学习的表示方法及其可解释性，发现其编码的表示可能与部分-整体关系并不严格相关。 |
| [^53] | [Mediapipe and CNNs for Real-Time ASL Gesture Recognition.](http://arxiv.org/abs/2305.05296) | Mediapipe和CNN用于实时美国手语手势识别。测试结果表明准确率可达99.95％，有潜力用于听力障碍人士的通信设备，并可以应用于其他相似手语。这项研究对计算机视觉和机器学习领域做出了重要贡献。 |
| [^54] | [On the Limitations of Model Stealing with Uncertainty Quantification Models.](http://arxiv.org/abs/2305.05293) | 本文研究了使用不确定性量化模型进行模型盗窃的局限性，发现在实际盗窃过程中相互不确定是不可避免的。作者尝试使用多个可能的网络并将它们的预测组合以提高质量，但结果表明只有微弱的改善。作者发现网络多样性不足是导致这一结果的原因之一。 |
| [^55] | [Dialogue Planning via Brownian Bridge Stochastic Process for Goal-directed Proactive Dialogue.](http://arxiv.org/abs/2305.05290) | 该论文提出了一种利用随机过程对话规划的方法，该方法通过布朗桥过程建模对话路径的时间动态以实现目标导向型对话系统并取得了良好的效果。 |
| [^56] | [Causal Discovery with Unobserved Variables: A Proxy Variable Approach.](http://arxiv.org/abs/2305.05281) | 本文提出了一种基于代理变量的方法，以解决因未观察变量而在观测数据中导致错误识别的问题。该方法可适用于连续变量系统，通过提出正则条件控制离散化误差来识别因果关系。 |
| [^57] | [Causal Discovery from Subsampled Time Series with Proxy Variables.](http://arxiv.org/abs/2305.05276) | 本研究提出了一种使用代理变量方法的无模型算法，可以从子采样时间序列中无需参数约束地识别整个因果结构。 |
| [^58] | [DietCNN: Multiplication-free Inference for Quantized CNNs.](http://arxiv.org/abs/2305.05274) | 本文提出了一种用查表法代替CNN中乘法的新方法，其保留了主要CNN操作的语义，且可在FPGA实现中实现显著的能量降低，具有重要的实用价值。 |
| [^59] | [Robust Acoustic and Semantic Contextual Biasing in Neural Transducers for Speech Recognition.](http://arxiv.org/abs/2305.05271) | 本文提出了一种使用轻量级字符表示编码细粒度发音特征的方法，称为声学偏置，以提高针对声音相似性引导的上下文偏置，在神经变换器等自动语音识别系统性能中重要的改进。 |
| [^60] | [Towards Understanding Generalization of Macro-AUC in Multi-label Learning.](http://arxiv.org/abs/2305.05248) | 本研究探究了 multi-label 学习中常用的 Macro-AUC 的泛化性质，并发现数据集中标签不平衡对泛化界限有重要影响。未经变量处理的基于损失函数的算法可能由于对标签的不平衡更敏感而表现较差，这一结论在多个数据集上得到验证。 |
| [^61] | [Leveraging Generative AI Models for Synthetic Data Generation in Healthcare: Balancing Research and Privacy.](http://arxiv.org/abs/2305.05247) | 本文研究了在医疗保健领域使用生成式AI模型合成匿名化病人数据进行研究和培训的方法，旨在平衡数据访问和隐私保护，并探索其在医疗保健领域的应用，并讨论了其未来的挑战和研究方向。 |
| [^62] | [Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection.](http://arxiv.org/abs/2305.05239) | 本文提出了一个通用的Learnable Behavioral Control (LBC)框架，使得行为选择空间得到扩大，并通过基于赌博机的元控制器实现行为控制。在Atari游戏上，我们的代理已经达到10个游戏的人类水平，并在7个游戏中达到了目前的最高分。 |
| [^63] | [Architectural Vision for Quantum Computing in the Edge-Cloud Continuum.](http://arxiv.org/abs/2305.05238) | 本文提出了一种基于边缘-云连续体的架构，旨在探索利用移动QPUs的系统和分布式异构资源以使混合应用程序受益。该文讨论了如何整合QPUs和解决方法，并介绍了混合经典-量子神经网络的分布式推理引擎。 |
| [^64] | [Traffic Forecasting on New Roads Unseen in the Training Data Using Spatial Contrastive Pre-Training.](http://arxiv.org/abs/2305.05237) | 本文提出一种名为SCPT的框架，利用对比学习进行空间预训练，并引入一个空间编码器模块，用于从未见数据中提取特征。该方法可以用于进行新道路的交通预测，无需重新训练模型。 |
| [^65] | [FedNoRo: Towards Noise-Robust Federated Learning by Addressing Class Imbalance and Label Noise Heterogeneity.](http://arxiv.org/abs/2305.05230) | 本文提出了一个名为 FedNoRo 的两阶段框架，用于解决类别不平衡和标签噪声异质性的联邦学习问题，并在 ICH 和 ISIC2019 数据集上取得了更好的表现。 |
| [^66] | [BARA: Efficient Incentive Mechanism with Online Reward Budget Allocation in Cross-Silo Federated Learning.](http://arxiv.org/abs/2305.05221) | BARA是一种在线奖励预算分配算法，用于激励跨边缘联邦学习中的数据所有者为模型训练做出贡献，并解决了现有研究中被忽略的奖励预算分配问题。 |
| [^67] | [Graph Neural Network-based surrogate model for granular flows.](http://arxiv.org/abs/2305.05218) | 研究基于图神经网络的颗粒流替代模型，能够捕捉大规模系统的复杂行为，并且表现出比传统机器学习方法更高的性能。 |
| [^68] | [Dataset of a parameterized U-bend flow for Deep Learning Applications.](http://arxiv.org/abs/2305.05216) | 该数据集包含10,000个U形弯管内的流体流动和热传递模拟，提供全面的基准，可用于各种设计优化问题和方法的研究。该数据集的独特特征是三种不同的数据类型，包括设计参数和目标组合、几何结构的二维图像以及数值模拟的解变量的表示方法。 |
| [^69] | [FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance.](http://arxiv.org/abs/2305.05176) | 本论文介绍了如何在使用大型语言模型的同时降低成本和提高性能。作者提出了三种策略，包括提示适应，LLM近似和LLM级联，并且通过 FrugalGPT 这种方法，实现了大大降低成本或是提高准确率的效果。 |
| [^70] | [Logic for Explainable AI.](http://arxiv.org/abs/2305.05172) | 本文介绍了一种基于符号逻辑的综合、语义和计算理论，以探讨可解释性人工智能的三个维度，以深入理解分类器所做出的决策。 |
| [^71] | [Cooperating Graph Neural Networks with Deep Reinforcement Learning for Vaccine Prioritization.](http://arxiv.org/abs/2305.05163) | 本研究采用深度强化学习合作的图神经网络，结合疾病动力学模型，开发了一种疫苗优先考虑策略，旨在在供应有限的情况下降低疫情的总体负担。 |
| [^72] | [Effective Medical Code Prediction via Label Internal Alignment.](http://arxiv.org/abs/2305.05162) | 本文提出了一种通过多视角注意力机制的神经网络，以在临床文本中预测医疗代码，并在标签空间与临床文本之间进行对齐，实现了对先前技术水平的提升。 |
| [^73] | [Latent Interactive A2C for Improved RL in Open Many-Agent Systems.](http://arxiv.org/abs/2305.05159) | 本论文介绍了一种基于编码-解码架构的潜在交互式A2C方法，以在多智能体系统中实现强化学习的改进。该方法显著提高了样本效率，通过学习隐藏状态和其他智能体的行为，解决了在竞争或对抗环境中从其他智能体中获得各种信息可能不可行的问题。 |
| [^74] | [DeepTree: Modeling Trees with Situated Latents.](http://arxiv.org/abs/2305.05153) | 提出了一种名为DeepTree的新型建树方法，利用位于情境潜变量的深度神经网络模型，通过学习树木分支结构的发展规律而非手动定义来建模树木，并能够生成各种形状的树木，而不需要定义复杂的分支规则。 |
| [^75] | [Physics-informed neural network for seismic wave inversion in layered semi-infinite domain.](http://arxiv.org/abs/2305.05150) | 本文提出了一种基于物理学约束的神经网络框架，在半无限层状域地震波反演中进行地下物质分布的反演，通过将吸收边界条件作为软约束器纳入网络以避免过多计算。实验表明该方法有效性良好。 |
| [^76] | [Localisation of Mammographic masses by Greedy Backtracking of Activations in the Stacked Auto-Encoders.](http://arxiv.org/abs/2305.05136) | 本文提出了一种基于自编码器的乳腺肿块定位方法，利用最大类激活值定位异常区域，该方法在准确性和效率方面均超过了现有的基于深度卷积神经网络的技术。 |
| [^77] | [A Kriging-Random Forest Hybrid Model for Real-time Ground Property Prediction during Earth Pressure Balance Shield Tunneling.](http://arxiv.org/abs/2305.05128) | 该研究提出了一种Kriging-Random Forest混合模型，结合先前预测的地质信息和实时的运行参数信息，为地压平衡盾构机前方地质预测提供指导，从而缓解施工风险。 |
| [^78] | [Comparing Foundation Models using Data Kernels.](http://arxiv.org/abs/2305.05126) | 本文采用基于数据内核的方法比较基础模型，不受度量指标的约束，通过嵌入空间几何实现点对点和多模型比较，并成功诱导了一组与下游指标强相关的模型距离函数流形。 |
| [^79] | [Flexible Job Shop Scheduling via Dual Attention Network Based Reinforcement Learning.](http://arxiv.org/abs/2305.05119) | 本文提出了一种基于深度强化学习和自注意模型的端到端学习框架，通过双重注意力网络来准确而简洁地表示操作和机器之间错综复杂的关系，以协同地确定FJSP的优先级分配规则。 |
| [^80] | [Federated Learning Operations Made Simple with Flame.](http://arxiv.org/abs/2305.05118) | Flame 是一个工具，通过引入高级抽象——角色和通道，将联邦学习应用程序描述为拓扑抽象图，解耦 ML 应用程序逻辑与底层部署细节，使得可以专门定制部署，减少开发工作，改进自动化和调整。 |
| [^81] | [Communication-Robust Multi-Agent Learning by Adaptable Auxiliary Multi-Agent Adversary Generation.](http://arxiv.org/abs/2305.05116) | 本文提出了一种适应性的辅助对抗生成的方法，命名为MA3C，以在合作多代理强化学习环境下实现通信鲁棒性，能够通过共同的目标最小化协同能力，有效提高通信策略的鲁棒性，在通信攻击下取得更好的性能。 |
| [^82] | [When a CBR in Hand is Better than Twins in the Bush.](http://arxiv.org/abs/2305.05111) | 本文讨论了一个回归问题——预测航班起飞延误。一个由 XGB-CBR Twin 转换来的 CBR 模型提供了最准确的局部预测、最易解释的局部解释表示和全局重要性的维护。 |
| [^83] | [Semi-Supervised Federated Learning for Keyword Spotting.](http://arxiv.org/abs/2305.05110) | 本研究证明了半监督联邦学习和联邦学习在关键词检测中的有效性。在使用少量标记数据的情况下，使用SSFL可以显著提高模型性能。 |
| [^84] | [TinyML Design Contest for Life-Threatening Ventricular Arrhythmia Detection.](http://arxiv.org/abs/2305.05105) | TDC'22是第一届面向ICDs低功耗微控制器的人工智能/机器学习（AI/ML）算法创新竞赛。本次竞赛的挑战是开发一种基于AI/ML的新型实时检测算法，对危及生命的室性心律失常进行检测。 |
| [^85] | [Towards unraveling calibration biases in medical image analysis.](http://arxiv.org/abs/2305.05101) | 本研究旨在解决医学图像分析中AI系统的校准偏差问题，即模型预测与实际数据不符合的问题。目前，大部分关于医疗算法公平性的研究都侧重于歧视偏差的评估，而校准偏差的评估仍然不足。 |
| [^86] | [Adaptive Domain Generalization for Digital Pathology Images.](http://arxiv.org/abs/2305.05100) | 本研究旨在解决数字病理学图像中的域漂移问题，引入一种反应性域泛化技术，在测试时自适应域偏移，无需事先预测或提供示例。 |
| [^87] | [Who Needs Decoders? Efficient Estimation of Sequence-level Attributes.](http://arxiv.org/abs/2305.05098) | 研究提出了非自回归代理模型(NAP)，通过编码序列直接预测通用标量值序列级属性，可以高效地实现解码步骤的规避。在机器翻译和语音识别两个场景下，NAP分别可以优于深度集成和高精度地预测性能指标。 |
| [^88] | [Self-Repellent Random Walks on General Graphs - Achieving Minimal Sampling Variance via Nonlinear Markov Chains.](http://arxiv.org/abs/2305.05097) | 本文设计了一种自我排斥随机游走模型，可实现较小的渐近采样方差，适用于网络拓扑的采样和邻域探索。 |
| [^89] | [Performative Federated Learning: A Solution to Model-Dependent and Heterogeneous Distribution Shifts.](http://arxiv.org/abs/2305.05090) | 本文提出了一种表现性联邦学习框架，通过引入分布映射来应对数据分布偏移，解决了模型依赖和客户端数据的异构分布。 |
| [^90] | [Functional Equivalence and Path Connectivity of Reducible Hyperbolic Tangent Networks.](http://arxiv.org/abs/2305.05089) | 本文对于单隐藏层双曲正切结构给出了单元冗余和可约的功能等价类的算法刻画，并证明了这样的功能等价类是分段线性路径连通的集合。 |
| [^91] | [Large-Scale Study of Temporal Shift in Health Insurance Claims.](http://arxiv.org/abs/2305.05087) | 本文是对健康保险索赔数据中历史偏差进行的大规模研究，通过构建算法测试时间偏移，进行回顾性扫描以寻找时间偏移，并创建1010个任务来评估242项医疗保健结果。研究发现有9.7%的任务显示出人群水平的时间偏移，93%显示出已发现的子人群内的时间偏移。 |
| [^92] | [A Unifying Framework of Attention-based Neural Load Forecasting.](http://arxiv.org/abs/2305.05082) | 本文提出了一种基于注意力机制的神经负载预测统一框架，包括时变特征加权、分层时间注意力和特征增强误差修正等模块，取得了优秀的预测性能。 |
| [^93] | [Earth Movers in The Big Data Era: A Review of Optimal Transport in Machine Learning.](http://arxiv.org/abs/2305.05080) | 本文回顾了最优输运在机器学习中的应用，并探讨了如何将其扩展以适应大数据和高维数据的需求。 |
| [^94] | [Recommender Systems with Generative Retrieval.](http://arxiv.org/abs/2305.05065) | 本文提出了一种新型的生成式检索模型，将检索和生成组合在一起以产生推荐。 |
| [^95] | [Web Content Filtering through knowledge distillation of Large Language Models.](http://arxiv.org/abs/2305.05027) | 本文提出了一种基于大语言模型知识蒸馏的 URL 分类方法，可用于网络内容过滤，其学生模型在参数数量减少 175 倍的情况下，精度提升了 9%，超过了当前最先进方法。 |
| [^96] | [Domain Agnostic Image-to-image Translation using Low-Resolution Conditioning.](http://arxiv.org/abs/2305.05023) | 本文提出了一种低分辨率条件下的领域无关的图像翻译方法，实现了源图像的可视特征与低分辨率目标图像的信息相结合，解决了领域相关的细粒度问题。 |
| [^97] | [Domain independent post-processing with graph U-nets: Applications to Electrical Impedance Tomographic Imaging.](http://arxiv.org/abs/2305.05020) | 本文提出了一种基于图形U-Net的领域独立后处理方法，可应用于电阻抗层析成像（EIT）中来提高重建效果，实现了对灵活几何形状的处理。 |
| [^98] | [Do Not Blindly Imitate the Teacher: Using Perturbed Loss for Knowledge Distillation.](http://arxiv.org/abs/2305.05010) | 本文提出了一种新的知识蒸馏目标函数PTLoss，通过扰动老师的输出分布，使其更接近真实标签分布，从而提高学生的性能。 |
| [^99] | [Autoencoder-based prediction of ICU clinical codes.](http://arxiv.org/abs/2305.04992) | 本文研究了基于自编码器的 ICU 临床代码预测，针对不完整的临床代码清单，使用了各种自编码器方法以及两个强基准。结果表明，基于共现的方法表现略微更好，对抗自编码器实现了最佳性能。 |
| [^100] | [Explanation-based Finetuning Makes Models More Robust to Spurious Cues.](http://arxiv.org/abs/2305.04990) | 本文提出一种新型方法——解释性微调，通过让模型在给出答案的同时生成支持该答案的自由文本解释，来减轻LLMs依赖虚假关联，使得模型对虚假提示更加强韧，并具有广泛适用性。 |
| [^101] | [FedHB: Hierarchical Bayesian Federated Learning.](http://arxiv.org/abs/2305.04979) | 该论文提出了一种层次贝叶斯联邦学习方法，通过块坐标下降分布式算法实现对客户端私有数据不透露的学习，在收敛速度上与正则化相同。 |
| [^102] | [LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization.](http://arxiv.org/abs/2305.04971) | 本文提出了一种基于标签正则化的通用框架，其中包括传统的LS，但也可以建模实例特定的变体。我们提出了一种双层优化的方法（LABO），用于学习标签正则化，并得到了可解释的最优标签平滑解。 |
| [^103] | [UQ for Credit Risk Management: A deep evidence regression approach.](http://arxiv.org/abs/2305.04967) | 本文扩展了Deep Evidence Regression方法，将其应用于预测信用风险中的违约损失；我们提供了相关的学习框架，并在模拟和实际数据上进行了验证。 |
| [^104] | [From Relational Pooling to Subgraph GNNs: A Universal Framework for More Expressive Graph Neural Networks.](http://arxiv.org/abs/2305.04963) | 本研究提出了一种从关系池化到子图GNN的通用框架，可提高任何基本GNN模型的表现力，通过明确标记节点作为附加特征来实现此目的， 并可在许多syn上实现超越性能。 |
| [^105] | [Joint Moment Retrieval and Highlight Detection Via Natural Language Queries.](http://arxiv.org/abs/2305.04961) | 本文提出了一种基于自然语言查询的联合视频摘要和精华片段检测方法，利用视觉和音频线索匹配用户的查询，实现检索视频中最相关和有趣的时刻。 |
| [^106] | [The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification.](http://arxiv.org/abs/2305.04940) | 本文介绍了一种早期层组合的方法EarlyBIRD，该方法可以有效利用深度自然语言处理模型的资源和可用信息，从而提高代码分类的性能，在缺陷检测方面平均可提高2个点。 |
| [^107] | [Generative Pretrained Autoregressive Transformer Graph Neural Network applied to the Analysis and Discovery of Novel Proteins.](http://arxiv.org/abs/2305.04934) | 本研究使用基于语言模型的深度学习策略，在蛋白质建模中应用transformer和图卷积的结构预训练生成模型，进一步训练后能够设计具有特定性质的蛋白质，案例验证表明该方法可生成理想目标性质的蛋白质。 |
| [^108] | [Uncertainty Quantification in Machine Learning for Engineering Design and Health Prognostics: A Tutorial.](http://arxiv.org/abs/2305.04933) | 本文教程介绍机器学习中的不确定性量化方法，帮助提升其安全性和可靠性，进而促进其在高风险决策背景下的广泛应用，特别关注神经网络及其在工程设计和预测健康管理中的应用。 |
| [^109] | [Gaussian process deconvolution.](http://arxiv.org/abs/2305.04871) | 本文提出了一种基于高斯过程的贝叶斯非参数方法，可以解决连续时间信号的去卷积问题，适用于观测值中可能存在缺失数据且信号滤波器未知的情况。 |
| [^110] | [Leveraging Deep Learning and Digital Twins to Improve Energy Performance of Buildings.](http://arxiv.org/abs/2305.04498) | 本研究提出了Deep Energy Twin的解决方案，将深度学习和数字孪生相结合，以识别建筑物能源使用模式并提供优化能源效率的洞见。 |
| [^111] | [Bayesian Over-the-Air FedAvg via Channel Driven Stochastic Gradient Langevin Dynamics.](http://arxiv.org/abs/2305.04152) | 本文提出了无线 FALD 协议，可以在无噪声通信的情况下高效地在无线系统中实现分布式贝叶斯学习，实现了在通信回合之间多个本地更新以及由小批量计算的随机梯度，并进行了样本收敛分析。 |
| [^112] | [Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling.](http://arxiv.org/abs/2305.04111) | 本文提出了EDGE，一种新的离散扩散模型用于生成大型图，并通过删除边来鼓励图的稀疏性。EDGE在每个去噪步骤中仅关注图中一部分节点，并且可以明确地对图的节点度数进行建模。实验表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。 |
| [^113] | [Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion.](http://arxiv.org/abs/2305.03509) | Diffusion Explainer是第一个可交互的可视化工具，用于解释稳定扩散如何将文本提示转化为图像，用户可以通过动画和交互元素流畅地在多个抽象级别之间过渡，从而更好地理解提示对图像生成的影响。 |
| [^114] | [Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders.](http://arxiv.org/abs/2305.02640) | 本文提出了因果强度变分模型，解决了从不确定数据中恢复因果关系存在的低样本利用率和分布假设无能力的问题，同时考虑了潜在混淆因素。 |
| [^115] | [CSP: Self-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Representations.](http://arxiv.org/abs/2305.01118) | CSP提出了一个自监督对比学习框架，通过利用地理信息，学习地理位置的有效表示，进一步提高了模型在大规模地理标记图像上的表现。 |
| [^116] | [Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation.](http://arxiv.org/abs/2305.00909) | 提出一种基于语法引导的粗-细代码生成模型，支持从粗到细的多次迭代，实现了更加符合人脑思维方式的代码编写方式。 |
| [^117] | [Activation Functions Not To Active: A Plausible Theory on Interpreting Neural Networks.](http://arxiv.org/abs/2305.00663) | 本文提供了一个合理的理论来解释神经网络的高维空间，并且将激活函数的角色描述为放大函数，将低维线性空间映射为无限维超级空间。 |
| [^118] | [Calibration Error Estimation Using Fuzzy Binning.](http://arxiv.org/abs/2305.00543) | 本文提出了一种模糊校准误差度量（FCE），利用模糊分箱方法计算校准误差，从而缓解了概率偏斜的影响并提供了更紧密的估计值。与传统指标ECE相比，FCE在多类设置中表现更好，https://github.com/srdgFHE/FCE-paper。 |
| [^119] | [Optimizing Privacy, Utility and Efficiency in Constrained Multi-Objective Federated Learning.](http://arxiv.org/abs/2305.00312) | 该论文提供了一种在有限制的多目标联邦学习中优化隐私、效用和效率的方法，开发了两种改进的算法来解决隐私泄露、效用损失和训练成本等三个主要目标，并在两个真实世界的数据集上进行了实验验证，优于现有方法。 |
| [^120] | [On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective.](http://arxiv.org/abs/2304.13836) | 本论文评估了RemOve-And-Retrain（ROAR）协议的可靠性。研究结果表明，ROAR基准测试中的属性可能有更少的有关决策的重要信息，这种偏差称为毛糙度偏差，并提醒人们不要在ROAR指标上进行盲目的依赖。 |
| [^121] | [Improving Adversarial Transferability by Intermediate-level Perturbation Decay.](http://arxiv.org/abs/2304.13410) | 本文提出了一种名为中间层扰动衰减（ILPD）的新型中间层方法，可以通过单个优化阶段制作可转移的对抗性样本，并在过程中鼓励中间层扰动处于有效的敌对方向，并同时具有很大的幅度。 |
| [^122] | [Medical Image Deidentification, Cleaning and Compression Using Pylogik.](http://arxiv.org/abs/2304.12322) | 提出了一个Python框架下的库PyLogik来帮助超声图像去标识化和清洗压缩，为深度学习和数据共享应用提供图像数据支持。 |
| [^123] | [Backpropagation-free Training of Deep Physical Neural Networks.](http://arxiv.org/abs/2304.11042) | 该论文提出了一种新方法来训练深度学习模型，不需要使用反向传播算法。该方法可以有效地应用于基于物理系统的深度学习。 |
| [^124] | [PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces.](http://arxiv.org/abs/2304.10255) | PED-ANOVA 提出了一个新的 f-ANOVA 公式，能够在任意子空间中高效地计算超参数的重要性，有助于深度学习中好的超参数空间设计。 |
| [^125] | [BanditQ -- No-Regret Learning with Guaranteed Per-User Rewards in Adversarial Environments.](http://arxiv.org/abs/2304.05219) | 提出了一种名为BanditQ的新的在线预测策略，在敌对设置中以公平的方式达到目标速率约束，并实现了$O(T^{3/4})$的遗憾。 |
| [^126] | [Deep-learning based measurement of planetary radial velocities in the presence of stellar variability.](http://arxiv.org/abs/2304.04807) | 本文提出了一种基于深度学习的方法，使用神经网络来减少三年HARPS-N太阳-星形光谱中的恒星径向速度抖动。该方法能够以前无法想象的小行星径向速度检测精度，为缓解恒星径向速度变异提供了希望。 |
| [^127] | [BloombergGPT: A Large Language Model for Finance.](http://arxiv.org/abs/2303.17564) | 本文提出了BloombergGPT，一个500亿参数的金融领域的大型语言模型，其基于Bloomberg的广泛数据来源和通用数据集进行训练。通过混合数据集训练，该模型在金融任务上表现出色，并且不会牺牲在普通任务上的性能。 |
| [^128] | [Quantum Circuit Fidelity Improvement with Long Short-Term Memory Networks.](http://arxiv.org/abs/2303.17523) | 本文提出使用长短期记忆网络解决量子计算中的保真度问题，利用时间序列预测方法预测量子电路的保真度。 |
| [^129] | [Adaptive Riemannian Metrics on SPD Manifolds.](http://arxiv.org/abs/2303.15477) | 本文提出了自适应黎曼度量来改进SPD神经网络的次优性能，实验结果表明该度量能使网络表现更好。 |
| [^130] | [Towards Domain Generalization for ECG and EEG Classification: Algorithms and Benchmarks.](http://arxiv.org/abs/2303.11338) | 本论文提出了一个开源生物信号领域泛化评估基准，并引入一种专门解决生物信号中领域泛化问题的神经网络架构DGNet-Bio。通过实验证明，DGNet-Bio在ECG和EEG分类领域泛化上优于现有方法。 |
| [^131] | [Embedding Theory of Reservoir Computing and Reducing Reservoir Network Using Time Delays.](http://arxiv.org/abs/2303.09042) | 本文结合延迟嵌入理论和广义嵌入理论，严谨证明了RC本质上是原始输入非线性动态系统的高维嵌入。我们进一步发现了时间延迟和网络神经元数量之间的权衡关系，并显着减小了洪泛计算网络的大小，实现了比全尺寸RC更好的性能。 |
| [^132] | [Provably Convergent Plug-and-Play Quasi-Newton Methods.](http://arxiv.org/abs/2303.07271) | 本文提出了一种可证明收敛的PnP方法，使用拟牛顿步骤以加速收敛，相对于现有的PnP方法对去噪器或保真度函数施加了较轻的限制。 |
| [^133] | [Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks.](http://arxiv.org/abs/2302.07260) | 本文提出了一个基于带随机先验的神经网络的深度学习框架用于高维输出的贝叶斯优化，可有效地处理全局优化问题，即使在高维度向量空间或无限维函数空间中也能近似功能关系。 |
| [^134] | [Symbolic Discovery of Optimization Algorithms.](http://arxiv.org/abs/2302.06675) | 该论文提出了一种将算法发现视为程序搜索的方法，并用于发现深度神经网络训练的优化算法。他们的方法发现了一种简单而有效的优化算法Lion，它比Adam更节省内存并且在ImageNet上的准确率提高了2％，并且预训练的计算时间也减少了多达5倍。 |
| [^135] | [Federated contrastive learning models for prostate cancer diagnosis and Gleason grading.](http://arxiv.org/abs/2302.06089) | 该研究提出了一个面向大规模病理图像和异质性挑战的联邦对比学习模型（FCL），通过最大化本地客户端和服务器模型间的注意力一致性来增强模型的泛化能力。 在前列腺癌诊断和格里森分级任务中，FCL表现出优异的性能。 |
| [^136] | [Interpretations of Domain Adaptations via Layer Variational Analysis.](http://arxiv.org/abs/2302.01798) | 本研究通过层变分分析证明了转移学习的成功可以通过相应的数据条件得到保证，并提出了一种基于网络的转移学习的替代方法，该方法在领域适应方面显示出了效率和准确性的提高。 |
| [^137] | [Proximal Causal Learning of Conditional Average Treatment Effects.](http://arxiv.org/abs/2301.10913) | 提出了P-学习器，一种用于学习处理效果异质性的定制两阶段损失函数，能够依靠代理变量进行因果推断，具有较高的灵活性和效率。 |
| [^138] | [The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks.](http://arxiv.org/abs/2301.07068) | 本文提出了#DNN-Verification问题，即计算违反特定安全性质的DNN输入配置数量的问题。作者提出了一种新的方法和一种随机的近似方法，分别给出了确切的违规计数和可证明概率界，并在安全关键基准测试上进行了实验比较。 |
| [^139] | [Discovery of structure-property relations for molecules via hypothesis-driven active learning over the chemical space.](http://arxiv.org/abs/2301.02665) | 通过假设学习技术，将符号回归和主动学习元素结合起来，为机器学习算法在化学空间中快速探索和预测分子的结构-性质关系提供了一种新的框架。 |
| [^140] | [Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features.](http://arxiv.org/abs/2212.13881) | 本文确定并表征了深度全连接神经网络学习特征的机制，提出了深度神经特征假设并解释了深度学习现象，同时也引领了对递归学习特征的核方法中特征学习的更广泛的理解。 |
| [^141] | [Pretraining Without Attention.](http://arxiv.org/abs/2212.10544) | 本文通过使用基于状态空间模型的序列路由方法提出了一种不依赖注意力机制的预训练模型BiGS，可以达到与BERT预训练准确度相当的GLUE测试结果，并具有不同的归纳偏差。 |
| [^142] | [Policy Optimization over General State and Action Spaces.](http://arxiv.org/abs/2211.16715) | 本文提出了一种新方法并引入了函数近似来解决通用状态和动作空间上的强化学习问题，同时介绍了一种新的策略双平均法。 |
| [^143] | [Mirror descent of Hopfield model.](http://arxiv.org/abs/2211.15880) | 本研究提出利用镜像下降技术来初始化神经网络参数，通过Hopfield模型作为原型，成功训练模型，相较于传统随机参数初始化的梯度下降方法，有明显的性能提升，可以增强机器学习模型的优化能力。 |
| [^144] | [Validating Large Language Models with ReLM.](http://arxiv.org/abs/2211.15458) | ReLM是一种使用正则表达式验证和查询LLM的系统，可以解决LLM数据记忆、偏见、毒性和语言理解等问题，具有高效性和广泛性。 |
| [^145] | [Few-shot Image Generation via Adaptation-Aware Kernel Modulation.](http://arxiv.org/abs/2210.16559) | 本研究提出了一种自适应感知核调制下的少样本图像生成方法，改进了现有方法在选择源模型知识方面的局限性。 |
| [^146] | [Boosting the Cycle Counting Power of Graph Neural Networks with I$^2$-GNNs.](http://arxiv.org/abs/2210.13978) | 本文研究了一种名为Subgraph MPNNs的GNN模型。我们发现，Subgraph MPNNs不能在节点级别上计数超过4个的环，这对于生物学、化学和社交网络分析等应用至关重要。 |
| [^147] | [Designing Universal Causal Deep Learning Models: The Case of Infinite-Dimensional Dynamical Systems from Stochastic Analysis.](http://arxiv.org/abs/2210.13300) | 设计了一个DL模型框架，名为因果神经算子（CNO），以逼近因果算子（CO），并证明了CNO模型可以在紧致集上一致逼近Hölder或平滑迹类算子。 |
| [^148] | [Autoencoded sparse Bayesian in-IRT factorization, calibration, and amortized inference for the Work Disability Functional Assessment Battery.](http://arxiv.org/abs/2210.10952) | 本文开发了一种贝叶斯层级模型，利用基于稀疏性的收缩和自编码器，实现了工作残障功能评估电池的尺度分解、条目选择、参数识别和响应评分，并取得了显著的提高。 |
| [^149] | [Weakly Supervised Learning for Analyzing Political Campaigns on Facebook.](http://arxiv.org/abs/2210.10669) | 本研究提出了一种基于弱监督学习的方法，通过分析Facebook上政治广告的立场和议题，以及其使用人口统计学定位，来了解政治活动的特点和时间动态。 |
| [^150] | [ScionFL: Efficient and Robust Secure Quantized Aggregation.](http://arxiv.org/abs/2210.07376) | 本文介绍了ScionFL，这是第一个在联邦学习中高效运行在量化输入上并同时提供对恶意客户端强健性的安全聚合框架。 |
| [^151] | [Coresets for Wasserstein Distributionally Robust Optimization Problems.](http://arxiv.org/abs/2210.04260) | 本文提出了一种构建一般Wasserstein分布鲁棒优化问题核心集的统一框架。 |
| [^152] | [Deep Span Representations for Named Entity Recognition.](http://arxiv.org/abs/2210.04182) | 本研究提出了DSpERT模型，通过跨度Transformer逐层聚合标记表示作为键和值，产生了深层语义的跨度表示，从而解决了现有跨度基础NER系统中长跨度实体显着无效性和重叠跨度表示的耦合问题。实验结果表明，DSpERT在八个NER基准测试中取得了性能高于或与最新最先进系统竞争的成果。 |
| [^153] | [On the Impossible Safety of Large AI Models.](http://arxiv.org/abs/2209.15259) | 本文探讨了大型人工智能模型的安全问题，并指出了构建既精确又安全的模型的基本不可能性，并提供了统计学下限证明。 |
| [^154] | [Generating Formal Safety Assurances for High-Dimensional Reachability.](http://arxiv.org/abs/2209.12336) | 提出了一个称为Safe DeepReach的框架，用于为DeepReach方法生成正式安全保障。该框架将新颖的Lipschitz连续性分析与区间边界传播方法相结合，以有效和可伸缩的方式保证解决方案的安全性。 |
| [^155] | [Robust Information Bottleneck for Task-Oriented Communication with Digital Modulation.](http://arxiv.org/abs/2209.10382) | 本文提出了一种数字调制的面向任务的通信方案，DT-JSCC，首先探讨编码表示的信息量和在接收表示中的信息失真鲁棒性之间的固有权衡，解决了仅传输任务相关信息和JSCC在数字通信系统上兼容性的问题。 |
| [^156] | [Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach.](http://arxiv.org/abs/2209.06995) | 本文提出了一种名为PATRON的方法，使用基于提示信息的不确定性的数据选择策略来提高预训练语言模型微调的few-shot性能，在六个文本分类数据集上实验证实该方法的性能优于最先进的冷启动数据选择基线，且仅使用128标签的情况下，该方法可以达到91.0%和92.1%的完全监督性能。 |
| [^157] | [EDeNN: Event Decay Neural Networks for low latency vision.](http://arxiv.org/abs/2209.04362) | EDeNN是一种新型的神经网络，更接近原始事件数据流，避免了积累事件到图像帧的过程，展现了在角速度回归和竞争光流估计方面最先进的性能。 |
| [^158] | [Generative Adversarial Super-Resolution at the Edge with Knowledge Distillation.](http://arxiv.org/abs/2209.03355) | 该论文提出了一种高效的基于生成对抗网络的实时超分辨率模型EdgeSRGAN，使用模型量化提高了CPU和Edge TPU设备上的执行效率，并通过知识蒸馏技术进一步优化模型，保留了较好的图像质量。 |
| [^159] | [Back-to-Bones: Rediscovering the Role of Backbones in Domain Generalization.](http://arxiv.org/abs/2209.01121) | 本文重点研究骨干在深度学习模型中域泛化能力的影响，通过评估各种特征提取器发现其准确性与单域分类任务性能有显著线性相关性。 |
| [^160] | [On Reality and the Limits of Language Data: Aligning LLMs with Human Norms.](http://arxiv.org/abs/2208.11981) | 本文研究了大型语言模型（LLMs）在仅使用语言数据的情况下理解物理世界的能力，使用新颖且严密控制的推理测试（ART）与人类规范进行对比，研究发现了LLMs在某些常识关系模型中可以直接从数据中学习，但存在弱点，例如在部分和包含关系方面表现不足。 |
| [^161] | [Measuring incompatibility and clustering quantum observables with a quantum switch.](http://arxiv.org/abs/2208.06210) | 本研究提出了一种量子不相容性测量指标“共同本征空间干扰”，并进一步通过量子开关实现量化，方便量子机器学习任务的处理。同时，算法可以对测量结果进行聚类，确定共享相似测量环境的观察者组。 |
| [^162] | [Majorization-minimization for Sparse Nonnegative Matrix Factorization with the $\beta$-divergence.](http://arxiv.org/abs/2207.06316) | 本文提出了一种带 $\beta$-差异的稀疏非负矩阵分解的主导最小化算法，其能够适用于任何 $\beta$-差异和其他稀疏约束。 |
| [^163] | [Towards the Practical Utility of Federated Learning in the Medical Domain.](http://arxiv.org/abs/2207.03075) | 本研究提出了应用联邦学习于医学领域的实用指南，包括三个具有代表性的医学数据集的实验，旨在提高医保业的数据效率，并形成适用于全行业的标准。 |
| [^164] | [Measuring Forgetting of Memorized Training Examples.](http://arxiv.org/abs/2207.00099) | 本文提出了一种测量机器学习模型对训练样本遗忘程度的技术，发现标准的图像、语音和语言模型在时间上确实会遗忘示例，确定性训练的模型不会遗忘。 |
| [^165] | [Exploring Chemical Space with Score-based Out-of-distribution Generation.](http://arxiv.org/abs/2206.07632) | MOOD是一种基于分数的异分布扩散策略，它通过属性预测器的梯度进行条件生成，从而使得逆向时间扩散过程通过指导目标特性到高分数区域，从而允许我们搜索新颖且有意义的分子。 |
| [^166] | [Combating Client Dropout in Federated Learning via Friend Model Substitution.](http://arxiv.org/abs/2205.13222) | 本研究提出了一种新算法FL-FDMS，可以在客户端退役的情况下，即时找到数据分布相似的客户端，并使用这些客户端的本地模型更新来替代缺失的客户端的更新，实现更高的学习准确性和更低的通信成本。 |
| [^167] | [FedAdapter: Efficient Federated Learning for Modern NLP.](http://arxiv.org/abs/2205.10162) | FedAdapter是一个框架，可以自动化适配器配置以提高联邦学习的效率，有助于面向现代NLP的高效训练。 |
| [^168] | [Provably Safe Reinforcement Learning: A Theoretical and Experimental Comparison.](http://arxiv.org/abs/2205.06750) | 该论文介绍了现有可证明安全的RL方法的分类，并在倒立摆和四旋翼稳定任务上进行了实验，证明这些方法都是安全的且表现与不安全的方法相媲美。 |
| [^169] | [One-shot Federated Learning without Server-side Training.](http://arxiv.org/abs/2204.12493) | 本文提出了一种无需额外训练阶段和公共数据集的交叉场所的一次性联邦学习算法MA-Echo，该算法通过调和最优区域迭代更新所有本地模型的参数，从而将它们拉近到一个低损失区域，而不会影响模型在自身数据集上的性能。 |
| [^170] | [Reinforced MOOCs Concept Recommendation in Heterogeneous Information Networks.](http://arxiv.org/abs/2203.11011) | 本文提出了一种强化学习框架下基于异构信息网络的概念推荐方法，可以更好地向不同专业水平的用户精细推荐知识。 |
| [^171] | [A policy gradient approach for optimization of smooth risk measures.](http://arxiv.org/abs/2202.11046) | 本文介绍了一种应用于on-policy和off-policy RL情况下的策略梯度算法，用于最小化广义平滑风险度量，能够收敛到平滑风险度量的稳态点，并适用于均值-方差和畸变风险度量的优化。 |
| [^172] | [A Prospective Approach for Human-to-Human Interaction Recognition from Wi-Fi Channel Data using Attention Bidirectional Gated Recurrent Neural Network with GUI Application Implementation.](http://arxiv.org/abs/2202.08146) | 该论文提出了一种基于Wi-Fi信道数据的人与人互动识别的新方法，使用注意力双向门控循环神经网络实现高精度的实时处理，准确率达到98.22%，并提供了一个GUI应用程序方便实时应用。 |
| [^173] | [Proportional Fairness in Federated Learning.](http://arxiv.org/abs/2202.01666) | 本文提出了用于保证联邦学习中公平性的比例公平性 (PF) 概念，并提出了一种新颖易于实现的算法 PropFair，能够在所有客户端平均性能和最差 10% 客户端平均性能之间达到良好平衡。 |
| [^174] | [Lifelong Learning on Evolving Graphs Under the Constraints of Imbalanced Classes and New Classes.](http://arxiv.org/abs/2112.10558) | 本文提出了gDOC方法和gDOC+方法，以解决在不平衡类别分布和新类别约束下进行演化图终身学习的问题。在标准基准数据集上，我们的方法优于多个基线和最先进的方法。 |
| [^175] | [Off-Policy Evaluation in Partially Observed Markov Decision Processes under Sequential Ignorability.](http://arxiv.org/abs/2110.12343) | 本文提出基于部分历史的重要性权重的估算器，可一致地估计目标策略的稳态平均回报，在POMDP中进行偏差控制式评估比在（完全观察到的）马尔可夫决策过程中进行偏差控制式评估更具挑战性，但比强化学习中的模型自由偏差控制式评估更容易。 |
| [^176] | [The Optimization of the Constant Flow Parallel Micropump Using RBF Neural Network.](http://arxiv.org/abs/2109.08717) | 本文通过实施RBF神经网络，提出了重叠时间的概念来优化恒流并联机械位移微型泵，在将左右泵互换角色的往复运动期间最小化压力脉冲。 |
| [^177] | [Development of a Risk-Free COVID-19 Screening Algorithm from Routine Blood Tests Using Ensemble Machine Learning.](http://arxiv.org/abs/2108.05660) | 本研究提出了一种风险低且高精度的堆叠集成机器学习模型，可以从常规血液检查中识别COVID-19患者，具有很高的准确率、精确度和召回率。 |
| [^178] | [Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning.](http://arxiv.org/abs/2107.04050) | 本文针对均场控制问题提出了一种高效的基于模型的强化学习算法$M^3-UCRL$，在未知系统动力学的情况下，该算法可平衡探索和利用，并实现了可证明的问题求解，具有较好的表现。 |
| [^179] | [Online Learning in Budget-Constrained Dynamic Colonel Blotto Games.](http://arxiv.org/abs/2103.12833) | 本文研究了动态Blotto博弈中有限资源的战略分配问题，使用在线学习方法和带背包和组合的赌博机分析该问题。 |
| [^180] | [Complexity of Stochastic Dual Dynamic Programming.](http://arxiv.org/abs/1912.07702) | 本文分析了随机对偶动态规划算法的迭代复杂度，发现当阶段数增加时，某些确定性算法的复杂度略微增加，而随机对偶动态规划的复杂度则呈指数增长。 |
| [^181] | [A Kernel Stein Test for Comparing Latent Variable Models.](http://arxiv.org/abs/1907.00586) | 本研究提出了一种核Stein检验方法，可用于比较具有潜变量的模型，相比于当前方法，性能更好。 |

# 详细

[^1]: 存在对称性和状态抽象的政策梯度方法

    Policy Gradient Methods in the Presence of Symmetries and State Abstractions. (arXiv:2305.05666v1 [cs.LG])

    [http://arxiv.org/abs/2305.05666](http://arxiv.org/abs/2305.05666)

    本文研究了在连续控制环境中的抽象，提出了一种策略梯度定理，允许利用环境的近似对称性进行策略优化，并提出了一系列演员-评论家算法进行策略和MDP同态映射的学习，最后展示了算法在连续对称性环境和视觉控制任务中的有效性。

    

    针对高维度和复杂问题，强化学习依靠抽象来提高效率和泛化性能。本文研究了在连续控制环境中的抽象，并将MDP同态的定义扩展到连续状态和动作空间的情况。我们针对抽象MDP的随机和确定性策略导出了一种策略梯度定理。我们的策略梯度结果允许利用环境的近似对称性进行策略优化。基于这些定理，我们提出了一系列演员-评论家算法，这些算法能够同时学习策略和MDP同态映射，使用松散双仿射度量。最后，我们引入了一系列具有连续对称性的环境，以进一步展示我们的算法在存在这些对称性的情况下进行动作抽象的能力。我们在这些环境以及具有挑战性的视觉控制任务中展示了我们方法的有效性。

    Reinforcement learning on high-dimensional and complex problems relies on abstraction for improved efficiency and generalization. In this paper, we study abstraction in the continuous-control setting, and extend the definition of MDP homomorphisms to the setting of continuous state and action spaces. We derive a policy gradient theorem on the abstract MDP for both stochastic and deterministic policies. Our policy gradient results allow for leveraging approximate symmetries of the environment for policy optimization. Based on these theorems, we propose a family of actor-critic algorithms that are able to learn the policy and the MDP homomorphism map simultaneously, using the lax bisimulation metric. Finally, we introduce a series of environments with continuous symmetries to further demonstrate the ability of our algorithm for action abstraction in the presence of such symmetries. We demonstrate the effectiveness of our method on our environments, as well as on challenging visual contro
    
[^2]: ImageBind:一个共同嵌入空间绑定所有模态的方法

    ImageBind: One Embedding Space To Bind Them All. (arXiv:2305.05665v1 [cs.CV])

    [http://arxiv.org/abs/2305.05665](http://arxiv.org/abs/2305.05665)

    ImageBind是一种新的跨模态联合嵌入方法，只需要使用图像配对数据就可以将不同模态的数据绑定在一起，并实现跨模态检索、组合和生成等多种应用。

    

    我们提出了ImageBind，这是一种跨越图像、文本、音频、深度、热传感和IMU数据的六种不同模态的联合嵌入方法。我们展示，不需要训练所有配对数据，只需要图像配对数据就足以将这些模态绑定在一起。ImageBind可以利用最近的大规模视觉-语言模型，并通过使用它们与图像的自然配对，将它们的零样本能力扩展到新的模态。它可以实现“开箱即用”的新型应用程序，包括跨模态检索、用算术组合模态、跨模态检测和生成。新型应用随着图像编码器的强度而不断改进，我们在跨模态的零样本识别任务上取得了新的最优成绩，超过了专家监督模型。最后，我们还展示了强的几何识别结果，超过了以前的工作，ImageBind成为了评估视觉模态联合学习的一种新方法。

    We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate visio
    
[^3]: ShapeCoder：从非结构化的基元中发现视觉程序的抽象

    ShapeCoder: Discovering Abstractions for Visual Programs from Unstructured Primitives. (arXiv:2305.05661v1 [cs.GR])

    [http://arxiv.org/abs/2305.05661](http://arxiv.org/abs/2305.05661)

    ShapeCoder是能够从非结构化的基元中发现视觉程序的抽象的第一个系统，用于重写程序以使其更紧凑，暴露自由度更少。在对合成和现实世界数据集的实验中，ShapeCoder优于现有基线，程序大小减少了3倍。

    

    程序是一种越来越受欢迎的可视化数据表示方式，可以揭示紧凑、可解释的结构，支持操作。视觉程序通常以特定领域语言(DSLs)编写。找到“好”的程序，即仅暴露有意义的自由度，需要访问具有“好”函数库的DSLs，这些函数库通常由领域专家编写。我们提出了ShapeCoder，这是第一个能够接受形状数据集的系统，该数据集使用非结构化基元表示，并共同发现(i)有用的抽象函数和(ii)使用这些抽象来解释输入形状的程序。发现的抽象捕获数据集中的常见模式(结构和参数)，因此，使用这些抽象重写的程序更紧凑，暴露的自由度更少。ShapeCoder改进了以前的抽象发现方法，在更复杂的输入下，发现更好的抽象，在较少的先验知识下产生更好的结果。我们的实验表明，ShapeCoder在合成和现实世界数据集上优于现有基线，程序大小减少了3倍。

    Programs are an increasingly popular representation for visual data, exposing compact, interpretable structure that supports manipulation. Visual programs are usually written in domain-specific languages (DSLs). Finding "good" programs, that only expose meaningful degrees of freedom, requires access to a DSL with a "good" library of functions, both of which are typically authored by domain experts. We present ShapeCoder, the first system capable of taking a dataset of shapes, represented with unstructured primitives, and jointly discovering (i) useful abstraction functions and (ii) programs that use these abstractions to explain the input shapes. The discovered abstractions capture common patterns (both structural and parametric) across the dataset, so that programs rewritten with these abstractions are more compact, and expose fewer degrees of freedom. ShapeCoder improves upon previous abstraction discovery methods, finding better abstractions, for more complex inputs, under less stri
    
[^4]: TidyBot: 应用大语言模型的个性化机器人物理辅助

    TidyBot: Personalized Robot Assistance with Large Language Models. (arXiv:2305.05658v1 [cs.RO])

    [http://arxiv.org/abs/2305.05658](http://arxiv.org/abs/2305.05658)

    本文研究了使用机器人进行家庭清扫的个性化问题。通过使用大型语言模型少样本摘要能力，机器人可以学习用户的偏好并将其推广到未来的场景中，从而实现快速适应。

    

    为了使机器人能够有效个性化地提供物理辅助，它必须学习用户的个人喜好并将其应用于未来的场景中。本文研究了使用机器人进行家庭清扫的个性化问题，这些机器人能够通过捡起物品并将其放回原处来整理房间。一个关键的挑战是确定每个物品的正确位置，因为人们的喜好可以因个人品味或文化背景而大不相同。例如，一个人可能喜欢把衬衫放在抽屉里，而另一个人可能喜欢把衬衫放在架子上。我们旨在建立系统，这些系统可以通过与特定人的先前交互学习这样的喜好，而只需要几个示例。我们展示了机器人可以将基于语言的规划和感知与大型语言模型(LLMs)的少样本摘要能力相结合，从而推断出广泛适用于未来交互的用户偏好。这种方法实现了快速适应，并取得了91.2%的准确率。

    For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accurac
    
[^5]: 利用光电容积描记术和深度学习预测心血管疾病风险

    Predicting Cardiovascular Disease Risk using Photoplethysmography and Deep Learning. (arXiv:2305.05648v1 [cs.CV])

    [http://arxiv.org/abs/2305.05648](http://arxiv.org/abs/2305.05648)

    利用光电容积描记术和深度学习预测心血管疾病风险，可在低成本下对低收入和中等收入国家进行大规模筛查，并超越了传统风险评分工具。

    

    心血管疾病(CVDs)是低收入和中等收入国家早期死亡率的主要原因。在这些人群中，早期 CVD 检测和干预至关重要，然而许多现有的 CVD 风险评分需要身体检查或实验室检测，这在这样的健康系统中会面临挑战，因为受限的可及性。在这里，我们调查了使用光电容积描记术(PPG)的潜力，这是一种在大多数智能手机上都可用的传感技术，可以在低成本下实现大规模筛查，用于 CVD 风险预测。我们开发了一种基于深度学习的 PPG-CVD 风险评分(DLS)，仅使用年龄、性别、吸烟状态和 PPG 作为预测因子，预测未来 10 年发生重要的不良心血管事件(MACE：非致命性心肌梗塞，中风和心血管死亡) 的概率。我们与办公室制定的 refit-WHO 评分进行了比较，该评分采用了 WHO 和 Globorisk 评分的共享预测因子（年龄，性别，吸烟状态，收缩压和总胆固醇），并使用两个独立的前瞻性队列。我们的结果表明，在两个队列中，DLS 在预测 MACE 方面表现优于 refit-WHO 评分，分别为0.76和0.80，而办公室制定的 refit-WHO 评分为0.66和0.70。我们的研究结果提示，基于 PPG 的 DLS 具有潜力帮助在低收入和中等收入国家早期低成本识别 CVD 高风险个体。

    Cardiovascular diseases (CVDs) are responsible for a large proportion of premature deaths in low- and middle-income countries. Early CVD detection and intervention is critical in these populations, yet many existing CVD risk scores require a physical examination or lab measurements, which can be challenging in such health systems due to limited accessibility. Here we investigated the potential to use photoplethysmography (PPG), a sensing technology available on most smartphones that can potentially enable large-scale screening at low cost, for CVD risk prediction. We developed a deep learning PPG-based CVD risk score (DLS) to predict the probability of having major adverse cardiovascular events (MACE: non-fatal myocardial infarction, stroke, and cardiovascular death) within ten years, given only age, sex, smoking status and PPG as predictors. We compared the DLS with the office-based refit-WHO score, which adopts the shared predictors from WHO and Globorisk scores (age, sex, smoking st
    
[^6]: 随机特征模型和双层神经网络的泛化分析的对偶性框架

    A duality framework for generalization analysis of random feature models and two-layer neural networks. (arXiv:2305.05642v1 [stat.ML])

    [http://arxiv.org/abs/2305.05642](http://arxiv.org/abs/2305.05642)

    本文提出了一个针对随机特征模型和双层神经网络的泛化分析的对偶性框架，并证明了学习不会受到维数灾难的影响，使 RFMs 可以在核范围之外发挥作用。

    

    本文研究在高维分析中出现的自然函数空间 $\mathcal{F}_{p,\pi}$ 和 Barron 空间中学习函数的问题。通过对偶分析，我们揭示了这些空间的逼近和估计可以在某种意义下被视为等价的。这使得我们能够在研究这两种模型的泛化时更专注于更容易的逼近和估计问题。通过定义一种基于信息的复杂度来有效地控制估计误差，建立了对偶等价性。此外，我们通过对两个具体应用进行综合分析展示了我们的对偶性框架的灵活性。第一个应用是研究使用 RFMs 学习 $\mathcal{F}_{p,\pi}$ 中的函数。我们证明只要 $p>1$，学习不会受到维数灾难的影响，这意味着 RFMs 可以在核范围之外发挥作用。

    We consider the problem of learning functions in the $\mathcal{F}_{p,\pi}$ and Barron spaces, which are natural function spaces that arise in the high-dimensional analysis of random feature models (RFMs) and two-layer neural networks. Through a duality analysis, we reveal that the approximation and estimation of these spaces can be considered equivalent in a certain sense. This enables us to focus on the easier problem of approximation and estimation when studying the generalization of both models. The dual equivalence is established by defining an information-based complexity that can effectively control estimation errors. Additionally, we demonstrate the flexibility of our duality framework through comprehensive analyses of two concrete applications.  The first application is to study learning functions in $\mathcal{F}_{p,\pi}$ with RFMs. We prove that the learning does not suffer from the curse of dimensionality as long as $p>1$, implying RFMs can work beyond the kernel regime. Our 
    
[^7]: 面向个人或实体的知识图谱表示学习：在医疗保健领域应用的研究

    Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare. (arXiv:2305.05640v1 [cs.AI])

    [http://arxiv.org/abs/2305.05640](http://arxiv.org/abs/2305.05640)

    本研究提出了一种在医疗保健领域构建面向实体的知识图谱的端到端表示学习方法 HEER，通过将领域特定的约束和特征纳入到图嵌入算法中，有效地改善了下游预测任务。

    

    知识图谱是一种按本体或模式组织信息的流行方式，已经在从搜索到推荐的各种场景中得到了应用。尽管在知识图谱方面有了一些进展，但知识表示仍然是跨行业的一个非常棘手的任务，特别是在生物医学和医疗保健领域，由于实体之间的复杂相互关系、异质性、缺乏标准化和数据稀疏性等因素，这一任务尤其具有挑战性。本文提出了一种面向医疗保健领域构建面向实体的知识图谱的端到端表示学习方法，重点是捕捉生物医学领域的独特特征。所提出的框架名为HEER（Healthcare Entity-Entity Representation learning），将领域特定的约束和特征纳入到图嵌入算法中。对多个基准数据集的结果表明，与最先进的方法相比，HEER在改善下游预测任务方面具有有效性。

    Knowledge graphs (KGs) are a popular way to organise information based on ontologies or schemas and have been used across a variety of scenarios from search to recommendation. Despite advances in KGs, representing knowledge remains a non-trivial task across industries and it is especially challenging in the biomedical and healthcare domains due to complex interdependent relations between entities, heterogeneity, lack of standardization, and sparseness of data. KGs are used to discover diagnoses or prioritize genes relevant to disease, but they often rely on schemas that are not centred around a node or entity of interest, such as a person. Entity-centric KGs are relatively unexplored but hold promise in representing important facets connected to a central node and unlocking downstream tasks beyond graph traversal and reasoning, such as generating graph embeddings and training graph neural networks for a wide range of predictive tasks. This paper presents an end-to-end representation le
    
[^8]: 基于深度学习的多目标雷达检测估计

    Deep Learning-based Estimation for Multitarget Radar Detection. (arXiv:2305.05621v1 [eess.SP])

    [http://arxiv.org/abs/2305.05621](http://arxiv.org/abs/2305.05621)

    本文提出了基于卷积神经网络 (CNN) 的新方法直接从雷达信号的 range-Doppler 地图中估计移动目标的距离和速度。在不同信噪比 (SNR) 区间内，该方法比其他方法提供更好的估计精度和较短的预测时间。

    

    目标检测和识别是无线环境中一个非常具有挑战性的任务，其中存在大量物体，无论是有效地确定它们的位置，还是识别它们并预测它们的移动。在这项工作中，我们提出了一种基于卷积神经网络 (CNN) 的新方法，直接从检测信号的 range-Doppler 地图中估计移动目标的距离和速度。我们将所得结果与二维 (2D) 周期图，以及类似的 2DResFreq 和 VGG-19 网络进行比较，并展示我们的模型执行的估计过程在不同信噪比 (SNR) 区间内提供了更好的距离和速度指数估计精度，同时减少了预测时间。然后，我们使用峰值信噪比 (PSNR) 来评估我们提出的算法的性能，该指标是分析从压缩或降噪得到的输出图像质量的相关指标。

    Target detection and recognition is a very challenging task in a wireless environment where a multitude of objects are located, whether to effectively determine their positions or to identify them and predict their moves. In this work, we propose a new method based on a convolutional neural network (CNN) to estimate the range and velocity of moving targets directly from the range-Doppler map of the detected signals. We compare the obtained results to the two dimensional (2D) periodogram, and to the similar state of the art methods, 2DResFreq and VGG-19 network and show that the estimation process performed with our model provides better estimation accuracy of range and velocity index in different signal to noise ratio (SNR) regimes along with a reduced prediction time. Afterwards, we assess the performance of our proposed algorithm using the peak signal to noise ratio (PSNR) which is a relevant metric to analyse the quality of an output image obtained from compression or noise reductio
    
[^9]: 度量空间大小和神经网络中的泛化性能

    Metric Space Magnitude and Generalisation in Neural Networks. (arXiv:2305.05611v1 [cs.LG])

    [http://arxiv.org/abs/2305.05611](http://arxiv.org/abs/2305.05611)

    本研究使用一种新的拓扑不变量——大小，来量化深度神经网络的学习过程，研究其内部表示并提出一个新方法来确定其泛化能力，实验证明此框架可作为泛化错误的指标。

    

    深度学习模型在许多应用中取得了重大成功，但它们的内部工作过程仍然是难以捉摸的。本文的目的是通过一种称为“大小”的新拓扑不变量的视角来量化深度神经网络的学习过程。大小是一种等距不变量；它的属性是研究的一个活跃领域，因为它编码了度量空间中许多已知的不变量。我们使用大小来研究神经网络的内部表示，并提出了一种确定它们泛化能力的新方法。此外，我们在理论上将大小维度和泛化错误连接起来，并实验性地证明，所提出的框架可以成为泛化错误的一个良好指标。

    Deep learning models have seen significant successes in numerous applications, but their inner workings remain elusive. The purpose of this work is to quantify the learning process of deep neural networks through the lens of a novel topological invariant called magnitude. Magnitude is an isometry invariant; its properties are an active area of research as it encodes many known invariants of a metric space. We use magnitude to study the internal representations of neural networks and propose a new method for determining their generalisation capabilities. Moreover, we theoretically connect magnitude dimension and the generalisation error, and demonstrate experimentally that the proposed framework can be a good indicator of the latter.
    
[^10]: 点云网络能学习解剖结构的统计形态模型吗?

    Can point cloud networks learn statistical shape models of anatomies?. (arXiv:2305.05610v1 [cs.CV])

    [http://arxiv.org/abs/2305.05610](http://arxiv.org/abs/2305.05610)

    本文研究了基于点云的统计形态建模技术，摆脱了传统基于对应关系的方法的瓶颈，提出了一种新的学习方法，并证明了它的有效性和鲁棒性。

    

    统计形态建模(SSM)是研究和量化人群内解剖变化的有价值工具。然而，传统的基于对应关系的SSM生成方法每添加一个新对象就需要耗费时间的重新优化过程，使得推理过程在临床研究中变得不可行。此外，构建SSM需要输入完整的几何代理(例如，高分辨率二进制体积或表面网格)作为输入形状。而无序的3D点云表示更容易从各种医学成像实践中获取(例如，阈值图像和表面扫描)。最近，点云深度网络在学习不同点云任务的排列不变特征方面取得了显著的成功(例如，完成、语义分割、分类)。然而，它们在从点云中学习SSM方面的应用还未被探索。在本文中，我们展示了...

    Statistical Shape Modeling (SSM) is a valuable tool for investigating and quantifying anatomical variations within populations of anatomies. However, traditional correspondence-based SSM generation methods require a time-consuming re-optimization process each time a new subject is added to the cohort, making the inference process prohibitive for clinical research. Additionally, they require complete geometric proxies (e.g., high-resolution binary volumes or surface meshes) as input shapes to construct the SSM. Unordered 3D point cloud representations of shapes are more easily acquired from various medical imaging practices (e.g., thresholded images and surface scanning). Point cloud deep networks have recently achieved remarkable success in learning permutation-invariant features for different point cloud tasks (e.g., completion, semantic segmentation, classification). However, their application to learning SSM from point clouds is to-date unexplored. In this work, we demonstrate that 
    
[^11]: 相关性在公平排序中的作用

    The Role of Relevance in Fair Ranking. (arXiv:2305.05608v1 [cs.IR])

    [http://arxiv.org/abs/2305.05608](http://arxiv.org/abs/2305.05608)

    本文结合社会学、信息检索和机器学习公平性的角度和工具，着眼于相关性在公平排序中的应用和作用，并推导出相关性评分应满足的一组期望标准以实现有意义地指导公平干预措施。

    

    在线平台在机会获取中起着重要作用：基于相关性的排名通过在招聘平台的工作职位、求职者或在线市场的卖家中分配曝光机会来创建和限制选项。为了负责任地这样做，这些社会相关系统采用各种公平措施和干预措施，其中许多措施试图根据价值分配曝光机会。但是，因为这些构造通常不是直接可观察的，所以平台必须使用代理评分，如相关性，并从搜索者的行为信号中推断出它们。然而，关键问题仍然存在，即相关性在高风险的公平排序中是否履行其作为价值评分这样的作用。本文结合社会学、信息检索和机器学习公平性的角度和工具，推导出相关性评分应满足的一组期望标准，以便有意义地指导公平干预措施。

    Online platforms mediate access to opportunity: relevance-based rankings create and constrain options by allocating exposure to job openings and job candidates in hiring platforms, or sellers in a marketplace. In order to do so responsibly, these socially consequential systems employ various fairness measures and interventions, many of which seek to allocate exposure based on worthiness. Because these constructs are typically not directly observable, platforms must instead resort to using proxy scores such as relevance and infer them from behavioral signals such as searcher clicks. Yet, it remains an open question whether relevance fulfills its role as such a worthiness score in high-stakes fair rankings.  In this paper, we combine perspectives and tools from the social sciences, information retrieval, and fairness in machine learning to derive a set of desired criteria that relevance scores should satisfy in order to meaningfully guide fairness interventions. We then empirically show 
    
[^12]: 深度学习与几何深度学习：数学家和物理学家的介绍。

    Deep Learning and Geometric Deep Learning: an introduction for mathematicians and physicists. (arXiv:2305.05601v1 [cs.LG])

    [http://arxiv.org/abs/2305.05601](http://arxiv.org/abs/2305.05601)

    本文重点介绍了图神经网络在深度学习和几何深度学习中的关键要素：得分和损失函数，并解释了模型训练的主要步骤。

    

    在本文中，我们为了让读者进一步了解深度学习和几何深度学习，重点介绍了图神经网络。我们介绍了这些算法的关键要素：得分和损失函数，并解释了模型训练的主要步骤。我们不打算提供完整和详尽的论述，但我们会引入几个概念，以便快速介绍该主题。我们提供一些附录，讨论了KL散度，回归，多层感知机和通用逼近定理来补充我们的讨论。

    In this expository paper we want to give a brief introduction, with few key references for further reading, to the inner functioning of the new and successfull algorithms of Deep Learning and Geometric Deep Learning with a focus on Graph Neural Networks. We go over the key ingredients for these algorithms: the score and loss function and we explain the main steps for the training of a model. We do not aim to give a complete and exhaustive treatment, but we isolate few concepts to give a fast introduction to the subject. We provide some appendices to complement our treatment discussing Kullback-Leibler divergence, regression, Multi-layer Perceptrons and the Universal Approximation Theorem.
    
[^13]: StrAE：使用显式结构的自编码预训练嵌入的表示学习

    StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure. (arXiv:2305.05588v1 [cs.CL])

    [http://arxiv.org/abs/2305.05588](http://arxiv.org/abs/2305.05588)

    本文开发了StrAE框架，该框架利用句子结构无监督地学习多级节点嵌入，并且发现使用显式结构可以提高嵌入表现，新的对比目标优于标准的交叉熵目标。同时，完全忠实于结构确实能够根据相应模型的性能消除结构类型之间的歧义。

    

    本文通过开发StrAE这一自编码框架，探究了在NLP中使用显式结构进行表示学习的实用性，该框架利用句子结构无监督地学习多级节点嵌入。我们在包括一种新的对比损失在内的不同类型句子结构和目标下使用StrAE进行模型训练，并在一系列内在和外在任务上评估所学嵌入。实验结果表明，通过StrAE利用显式结构可以提高嵌入，新的对比目标优于标准的交叉熵目标。此外，与以往的做法相比，我们发现完全忠实于结构确实能够根据相应模型的性能消除结构类型之间的歧义。作为StrAE实用性的进一步证明，我们开发了一个简单的p

    This work explores the utility of explicit structure for representation learning in NLP by developing StrAE -- an autoencoding framework that faithfully leverages sentence structure to learn multi-level node embeddings in an unsupervised fashion. We use StrAE to train models across different types of sentential structure and objectives, including a novel contrastive loss over structure, and evaluate the learnt embeddings on a series of both intrinsic and extrinsic tasks. Our experiments indicate that leveraging explicit structure through StrAE leads to improved embeddings over prior work, and that our novel contrastive objective over structure outperforms the standard cross-entropy objective. Moreover, in contrast to findings from prior work that weakly leverages structure, we find that being completely faithful to structure does enable disambiguation between types of structure based on the corresponding model's performance. As further evidence of StrAE's utility, we develop a simple p
    
[^14]: Fashion CUT：使用合成数据和伪标签的无监督领域自适应，用于服装视觉模式分类

    Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels. (arXiv:2305.05580v1 [cs.CV])

    [http://arxiv.org/abs/2305.05580](http://arxiv.org/abs/2305.05580)

    该论文提出了一种使用合成数据和伪标签的无监督领域自适应技术，在保持分类准确性的同时，将合成域的图像转换为真实世界域，用于服装视觉模式分类。

    

    精确的产品信息对于电子商务商店非常重要，以允许客户浏览、筛选和搜索产品。由于缺失或不正确的信息会导致客户体验不佳，因此使用机器学习来纠正不准确或缺失的信息是关键。然而，在服装图像分类任务上获得高性能需要大量的注释数据，但由于标注成本而费用昂贵。一种解决方案是生成不需要手动标记的合成数据。但是，使用仅由合成图像组成的数据集来训练模型，可能会导致在实际数据上推理时性能下降，因为存在领域转移。本文介绍了一种新的无监督领域自适应技术，将合成域的图像转换为真实世界域。我们的方法结合了生成神经网络和分类器的联合训练，以产生逼真的图像同时保持分类准确性。

    Accurate product information is critical for e-commerce stores to allow customers to browse, filter, and search for products. Product data quality is affected by missing or incorrect information resulting in poor customer experience. While machine learning can be used to correct inaccurate or missing information, achieving high performance on fashion image classification tasks requires large amounts of annotated data, but it is expensive to generate due to labeling costs. One solution can be to generate synthetic data which requires no manual labeling. However, training a model with a dataset of solely synthetic images can lead to poor generalization when performing inference on real-world data because of the domain shift. We introduce a new unsupervised domain adaptation technique that converts images from the synthetic domain into the real-world domain. Our approach combines a generative neural network and a classifier that are jointly trained to produce realistic images while preser
    
[^15]: FAENet：用于材料建模的帧平均等变GNN

    FAENet: Frame Averaging Equivariant GNN for Materials Modeling. (arXiv:2305.05577v1 [cs.LG])

    [http://arxiv.org/abs/2305.05577](http://arxiv.org/abs/2305.05577)

    本文提出了 FAENet ，一种简单、快速、表现力强的 GNN，用于材料建模，其通过随机帧平均（SFA）使任何模型 E(3) 等变或不变。理论和实验证明了其在准确性和可扩展性方面的优越性。

    

    机器学习技术在材料建模中的应用通常涉及到已知对特定对称性具有等变或不变性的函数。虽然图神经网络在这些任务中已被证明是成功的，但它们通过模型架构来强制实现对称性，这通常会降低它们的表现力、可扩展性和可理解性。在本文中，我们引入了一个灵活的框架，通过随机帧平均（SFA）从而通过数据变换使任何模型 E(3) 等变或不变. 同时我们还引入了FAENet，它是一个简单、快速且表现力强的 GNN，通过优化 SFA，以没有任何对称性约束的方式处理几何信息。我们在OC20数据集（S2EF，IS2RE）以及常见的分子建模任务（QM9，QM7-X）上从理论和实验证明了我们方法的有效性、超强的准确性和可扩展性。具有示例实现的软件包可在 https://faenet.readthedocs.io获得。

    Applications of machine learning techniques for materials modeling typically involve functions known to be equivariant or invariant to specific symmetries. While graph neural networks (GNNs) have proven successful in such tasks, they enforce symmetries via the model architecture, which often reduces their expressivity, scalability and comprehensibility. In this paper, we introduce (1) a flexible framework relying on stochastic frame-averaging (SFA) to make any model E(3)-equivariant or invariant through data transformations. (2) FAENet: a simple, fast and expressive GNN, optimized for SFA, that processes geometric information without any symmetrypreserving design constraints. We prove the validity of our method theoretically and empirically demonstrate its superior accuracy and computational scalability in materials modeling on the OC20 dataset (S2EF, IS2RE) as well as common molecular modeling tasks (QM9, QM7-X). A package implementation is available at https://faenet.readthedocs.io.
    
[^16]: 一种对抗感知的去中心化网络多智能体强化学习算法

    An Algorithm For Adversary Aware Decentralized Networked MARL. (arXiv:2305.05573v1 [cs.LG])

    [http://arxiv.org/abs/2305.05573](http://arxiv.org/abs/2305.05573)

    本文提出了一种对抗感知的去中心化网络多智能体强化学习算法，该算法允许非对抗性智能体在对抗方存在的情况下达成共识。

    

    去中心化的多智能体强化学习算法已经在文献中变得流行，因为它允许异构体拥有自己的奖励函数，相对于假定所有智能体拥有共同奖励函数的经典多智能体马尔可夫决策过程(MDP)设置。在本文中，我们遵循现有合作MARL的工作，在一个连接的时变网络中，智能体可以相互交换信息以达成共识。我们在共识更新中引入漏洞，在现有的MARL算法中，智能体可以偏离其常规的共识更新，我们称之为对抗性智能体。然后，我们提供了一种算法，使非对抗性智能体可以在受到约束条件限制的情况下，在对抗性存在的情况下达成共识。

    Decentralized multi-agent reinforcement learning (MARL) algorithms have become popular in the literature since it allows heterogeneous agents to have their own reward functions as opposed to canonical multi-agent Markov Decision Process (MDP) settings which assume common reward functions over all agents. In this work, we follow the existing work on collaborative MARL where agents in a connected time varying network can exchange information among each other in order to reach a consensus. We introduce vulnerabilities in the consensus updates of existing MARL algorithms where agents can deviate from their usual consensus update, who we term as adversarial agents. We then proceed to provide an algorithm that allows non-adversarial agents to reach a consensus in the presence of adversaries under a constrained setting.
    
[^17]: SMAClite: 多智能体强化学习的轻量级环境

    SMAClite: A Lightweight Environment for Multi-Agent Reinforcement Learning. (arXiv:2305.05566v1 [cs.LG])

    [http://arxiv.org/abs/2305.05566](http://arxiv.org/abs/2305.05566)

    本论文介绍了SMAClite，一个轻量级的多智能体强化学习环境，解耦了原有的闭源游戏并提供了开源框架，SMAClite在运行时速度和内存方面超越了SMAC。

    

    目前缺少适用于多智能体强化学习算法的标准基准。Starcraft多智能体挑战（SMAC）已经在多智能体强化学习研究中广泛使用，但构建在重型的闭源计算机游戏StarCraft II之上。因此，SMAC的计算成本很高，需要具备关于游戏的特殊知识和使用专有工具才能对环境进行任何有意义的修改或贡献。我们介绍了SMAClite - 一个基于SMAC的挑战，不仅与Starcraft II解耦，而且是开源的，同时提供了一个框架，使得可以创建新的SMAClite内容而无需任何特殊知识。我们进行了实验，证明了通过在SMAClite上训练多智能体强化学习算法，可以复现SMAC的结果。然后我们证明，SMAClite在运行时速度和内存方面超越SMAC。

    There is a lack of standard benchmarks for Multi-Agent Reinforcement Learning (MARL) algorithms. The Starcraft Multi-Agent Challenge (SMAC) has been widely used in MARL research, but is built on top of a heavy, closed-source computer game, StarCraft II. Thus, SMAC is computationally expensive and requires knowledge and the use of proprietary tools specific to the game for any meaningful alteration or contribution to the environment. We introduce SMAClite -- a challenge based on SMAC that is both decoupled from Starcraft II and open-source, along with a framework which makes it possible to create new content for SMAClite without any special knowledge. We conduct experiments to show that SMAClite is equivalent to SMAC, by training MARL algorithms on SMAClite and reproducing SMAC results. We then show that SMAClite outperforms SMAC in both runtime speed and memory.
    
[^18]: SkelEx和BoundEx：ReLU神经网络的自然可视化

    SkelEx and BoundEx: Natural Visualization of ReLU Neural Networks. (arXiv:2305.05562v1 [cs.LG])

    [http://arxiv.org/abs/2305.05562](http://arxiv.org/abs/2305.05562)

    SkelEx和BoundEx是第一批自然可视化的方法，可以提取ReLU神经网络的关键点和决策边界。这两种方法为在低维数据上训练的ReLU NN引入了非常自然的可视化工具。

    

    尽管权重和偏差的可解释性有限，但仍然是编码ReLU神经网络学习函数最流行的方式。这就是为什么我们引入SkelEx算法，以提取ReLU NN学习的成员函数的骨架，使这些函数更易于解释和分析。据我们所知，这是第一个从关键点的角度考虑线性区域的工作。作为自然的后续工作，我们还介绍了BoundEx，这是我们所知道的第一个从ReLU NN的实现中提取决策边界的分析方法。这两种方法都为在低维数据上训练的ReLU NN引入了非常自然的可视化工具。

    Despite their limited interpretability, weights and biases are still the most popular encoding of the functions learned by ReLU Neural Networks (ReLU NNs). That is why we introduce SkelEx, an algorithm to extract a skeleton of the membership functions learned by ReLU NNs, making those functions easier to interpret and analyze. To the best of our knowledge, this is the first work that considers linear regions from the perspective of critical points. As a natural follow-up, we also introduce BoundEx, which is the first analytical method known to us to extract the decision boundary from the realization of a ReLU NN. Both of those methods introduce very natural visualization tool for ReLU NNs trained on low-dimensional data.
    
[^19]: CIT-EmotionNet：用于EEG情绪识别的CNN交互式Transformer网络

    CIT-EmotionNet: CNN Interactive Transformer Network for EEG Emotion Recognition. (arXiv:2305.05548v1 [eess.SP])

    [http://arxiv.org/abs/2305.05548](http://arxiv.org/abs/2305.05548)

    本研究提出了一种新颖的CNN交互式Transformer网络，用于EEG情绪识别，集成了全局和局部特征，通过CNN交互式Transformer模块促进局部和全局特征的交互和融合，获得了最先进的性能。

    

    利用脑电信号进行情绪识别是情感计算和智能交互中的一个重要研究挑战。然而，有效地将脑电信号的全局和局部特征相结合以提高情绪识别的性能仍然是一项困难任务。本研究提出了一种新颖的CNN交互式Transformer网络，用于EEG情绪识别，称为CIT-EmotionNet，它能够有效地集成EEG信号的全局和局部特征。我们首先将原始EEG信号转换为空间频率表示，作为输入。然后在单个框架内并行地集成了卷积神经网络（CNN）和Transformer。最后，我们设计了一个CNN交互式Transformer模块，促进局部和全局特征的交互和融合，从而增强了模型从EEG空间频率表示中提取两种类型特征的能力。提出的CIT-EmotionNet在公开数据集上优于传统EEG情绪识别方法，并达到了最先进的性能，证明了我们提出的方法的有效性。

    Emotion recognition using Electroencephalogram (EEG) signals has emerged as a significant research challenge in affective computing and intelligent interaction. However, effectively combining global and local features of EEG signals to improve performance in emotion recognition is still a difficult task. In this study, we propose a novel CNN Interactive Transformer Network for EEG Emotion Recognition, known as CIT-EmotionNet, which efficiently integrates global and local features of EEG signals. Initially, we convert raw EEG signals into spatial-frequency representations, which serve as inputs. Then, we integrate Convolutional Neural Network (CNN) and Transformer within a single framework in a parallel manner. Finally, we design a CNN interactive Transformer module, which facilitates the interaction and fusion of local and global features, thereby enhancing the model's ability to extract both types of features from EEG spatial-frequency representations. The proposed CIT-EmotionNet outp
    
[^20]: Walk4Me: 远程康复社区行动能力评估——一个早期诊断和疾病进展的自动化系统

    Walk4Me: Telehealth Community Mobility Assessment, An Automated System for Early Diagnosis and Disease Progression. (arXiv:2305.05543v1 [eess.SP])

    [http://arxiv.org/abs/2305.05543](http://arxiv.org/abs/2305.05543)

    Walk4Me是一个远程康复社区行动能力评估系统，采用基于人工智能的步态特征检测来识别疾病患者的步态障碍并追踪疾病的进展。

    

    我们介绍了Walk4Me，这是一个远程康复社区行动能力评估系统，旨在促进早期诊断、严重性和进展识别。我们通过以下三个方法实现了这一目标：1）促进早期诊断；2）识别临床严重性的早期指标；3）量化和跟踪疾病在步行期的进展。为此，我们采用基于人工智能（AI）的步态特征检测来检测患者和通常发育的同龄人的行走方式。我们的系统通过我们的新型Walk4Me API从设备传感器（例如移动设备的加速度等）远程实时收集数据。我们的Web应用程序提取时间/空间步态特征和原始数据信号特征，然后采用传统机器学习和深度学习技术来识别模式，以便实现：1）识别与疾病相关的步态障碍的患者；2）描述活动能力限制的程度；3）确定疾病的进展。

    We introduce Walk4Me, a telehealth community mobility assessment system designed to facilitate early diagnosis, severity, and progression identification. Our system achieves this by 1) enabling early diagnosis, 2) identifying early indicators of clinical severity, and 3) quantifying and tracking the progression of the disease across the ambulatory phase of the disease. To accomplish this, we employ an Artificial Intelligence (AI)-based detection of gait characteristics in patients and typically developing peers. Our system remotely and in real-time collects data from device sensors (e.g., acceleration from a mobile device, etc.) using our novel Walk4Me API. Our web application extracts temporal/spatial gait characteristics and raw data signal characteristics and then employs traditional machine learning and deep learning techniques to identify patterns that can 1) identify patients with gait disturbances associated with disease, 2) describe the degree of mobility limitation, and 3) ide
    
[^21]: 基于神经网络的超密度发射机定位

    Localization of Ultra-dense Emitters with Neural Networks. (arXiv:2305.05542v1 [eess.SP])

    [http://arxiv.org/abs/2305.05542](http://arxiv.org/abs/2305.05542)

    本论文介绍了一种名为LUENN的深度卷积神经网络，可以在发射器图像重叠显著时，利用独特的架构平滑地容纳从完全孤立到共聚的发射器，从而将半径为1微米平方内可用的发射器密度的范围扩大了6倍以上，降低了定位精度的惩罚，并提高了时间分辨率。

    

    单分子定位显微术（SMLM）扩展了我们观察亚细胞结构的能力，但在时间分辨率上受到限制。 增加发射器密度将改善时间分辨率，但当前的分析算法在发射器图像重叠显著时遇到困难。 我们提出了一种名为LUENN的深度卷积神经网络，它利用了一个独特的架构，拒绝了孤立发射器的假设； 它可以平滑地容纳从完全孤立到共聚的发射器。 除了提供不确定性估计外，这种架构还提高了实验室的可用性，缩短了成像时间并放宽了实验的要求。它将半径为1微米平方内可用的发射器密度的范围扩大了6倍以上，从而降低了定位精度的惩罚，并提高了时间分辨率。

    Single-Molecule Localization Microscopy (SMLM) has expanded our ability to visualize subcellular structures but is limited in its temporal resolution. Increasing emitter density will improve temporal resolution, but current analysis algorithms struggle as emitter images significantly overlap. Here we present a deep convolutional neural network called LUENN which utilizes a unique architecture that rejects the isolated emitter assumption; it can smoothly accommodate emitters that range from completely isolated to co-located. This architecture, alongside an accurate estimator of location uncertainty, extends the range of usable emitter densities by a factor of 6 to over 31 emitters per micrometer-squared with reduced penalty to localization precision and improved temporal resolution. Apart from providing uncertainty estimation, the algorithm improves usability in laboratories by reducing imaging times and easing requirements for successful experiments.
    
[^22]: 多元设备网络中高效基于模式的异常检测

    Efficient pattern-based anomaly detection in a network of multivariate devices. (arXiv:2305.05538v1 [cs.SI])

    [http://arxiv.org/abs/2305.05538](http://arxiv.org/abs/2305.05538)

    该论文提出了一种高效的基于模式挖掘的异常检测方法，该方法不仅考虑多元时间序列，还考虑了实体间的通信关系，并提供可解释的解释。实验表明，该方法比现有方法更有效率和有效。

    

    许多组织管理服务质量并监视大量设备和服务器，其中每个实体都与遥测或物理传感器数据序列相关联。最近，提出了各种方法来检测行为异常，然而现有方法关注多元时间序列，并忽略实体间的通信。此外，我们旨在支持最终用户不仅在定位在某个时期导致异常的实体和传感器方面，而且解释这个决定。我们提出了一种可扩展的方法来使用两步方法检测异常。首先，我们恢复网络中实体之间的关系，因为关系通常是动态的，由未知的基础过程引起。接下来，我们基于顺序模式的嵌入报告异常。模式挖掘是高效的并支持解释，即模式代表时间序列中频繁发生的行为。我们扩展了模式挖掘以基于频率过滤顺序模式，并引入模式模板以将模式报告为文本，从而提供可解释的解释。我们在几个大规模场景上评估了我们的方法，表明我们的方法比现有方法更有效率和有效。

    Many organisations manage service quality and monitor a large set devices and servers where each entity is associated with telemetry or physical sensor data series. Recently, various methods have been proposed to detect behavioural anomalies, however existing approaches focus on multivariate time series and ignore communication between entities. Moreover, we aim to support end-users in not only in locating entities and sensors causing an anomaly at a certain period, but also explain this decision. We propose a scalable approach to detect anomalies using a two-step approach. First, we recover relations between entities in the network, since relations are often dynamic in nature and caused by an unknown underlying process. Next, we report anomalies based on an embedding of sequential patterns. Pattern mining is efficient and supports interpretation, i.e. patterns represent frequent occurring behaviour in time series. We extend pattern mining to filter sequential patterns based on frequen
    
[^23]: 合并整体与局部信息估计情感反应强度

    Integrating Holistic and Local Information to Estimate Emotional Reaction Intensity. (arXiv:2305.05534v1 [cs.CV])

    [http://arxiv.org/abs/2305.05534](http://arxiv.org/abs/2305.05534)

    该论文提出了一种多模态架构，结合视频和音频信息来估计情感反应强度，使用回归标记解决了可变视频长度的问题，实现了对情感反应强度的准确估计。

    

    基于视频的情感反应强度（ERI）估计可以从被试观看刺激的视频中测量出他们对几种情感维度的反应强度。我们提出了一种多模态的视频ERI架构，结合了视频和音频信息。视频输入首先按帧进行空间编码，结合编码主体面部表情的整体特征和编码其表情局部特征的特征。然后通过门控循环单元（GRUs）从帧到帧进行时间上的组合，再通过变压器在全局范围内进行组合。我们通过一个回归标记解决可变视频长度的问题，该标记从所有帧中累积信息以产生独立于视频长度的固定维度向量。音频信息的处理类似：在每个帧内提取的频谱信息通过一系列GRUs和具有回归令牌的变压器的时间整合。视频和音频回归分别与情感反应强度相关联。

    Video-based Emotional Reaction Intensity (ERI) estimation measures the intensity of subjects' reactions to stimuli along several emotional dimensions from videos of the subject as they view the stimuli. We propose a multi-modal architecture for video-based ERI combining video and audio information. Video input is encoded spatially first, frame-by-frame, combining features encoding holistic aspects of the subjects' facial expressions and features encoding spatially localized aspects of their expressions. Input is then combined across time: from frame-to-frame using gated recurrent units (GRUs), then globally by a transformer. We handle variable video length with a regression token that accumulates information from all frames into a fixed-dimensional vector independent of video length. Audio information is handled similarly: spectral information extracted within each frame is integrated across time by a cascade of GRUs and a transformer with regression token. The video and audio regressi
    
[^24]: 基于卷积的方法集合用于振动信号的故障检测

    An ensemble of convolution-based methods for fault detection using vibration signals. (arXiv:2305.05532v1 [eess.SP])

    [http://arxiv.org/abs/2305.05532](http://arxiv.org/abs/2305.05532)

    本文提出了集成了三种基于卷积的方法，用于解决振动信号的故障检测问题。实验结果表明，该方法在准确率上优于其他方法，达到了超过98.8\%的准确率。

    

    本文研究了使用测试平台上行星齿轮箱振动信号的多元时间序列解决故障检测问题。对于多元时间序列分类问题，常见的机器学习和深度学习方法包括基于距离、基于功能数据、基于特征和基于卷积核的方法。最近的研究表明，使用ROCKET、ResNet和FCN等基于卷积核的方法对多元时间序列数据分类具有强大的性能。我们提出了三种基于卷积核的方法的集合，并通过优于其他方法并实现超过98.8\%准确率的实验结果，证明了其在解决故障检测问题中的有效性。

    This paper focuses on solving a fault detection problem using multivariate time series of vibration signals collected from planetary gearboxes in a test rig. Various traditional machine learning and deep learning methods have been proposed for multivariate time-series classification, including distance-based, functional data-oriented, feature-driven, and convolution kernel-based methods. Recent studies have shown using convolution kernel-based methods like ROCKET, and 1D convolutional neural networks with ResNet and FCN, have robust performance for multivariate time-series data classification. We propose an ensemble of three convolution kernel-based methods and show its efficacy on this fault detection problem by outperforming other approaches and achieving an accuracy of more than 98.8\%.
    
[^25]: 使用机器学习模拟并发错误

    Modelling Concurrency Bugs Using Machine Learning. (arXiv:2305.05531v1 [cs.SE])

    [http://arxiv.org/abs/2305.05531](http://arxiv.org/abs/2305.05531)

    人工智能的机器学习已被广泛应用，本文着眼于利用机器学习检测并发错误的可行性，挑战是设计有效的模型，解决嘈杂数据和数据不平衡等挑战。

    

    人工智能在近年来得到了很大的发展，机器学习在各个领域的应用也越来越广泛。我们特别关注的是在并行程序的安全性和稳定性方面，机器学习应用的一个具体例子。检测并发错误已经吸引程序员很长时间，因为增加了复杂度的并发程序更容易出现失败。通过开发这样的自动检测工具，可以极大地节省调试时间，同时减少意外错误的数量。我们相信机器学习可以通过提供超过当前方法的优势，从整体上提高工具的准确性和编程语言的灵活性来实现这一目标。然而，由于机器学习方法特有的诸多挑战（例如数据不平衡和嘈杂数据），设计有效的机器学习模型来检测并发错误并不是一件简单的任务。

    Artificial Intelligence has gained a lot of traction in the recent years, with machine learning notably starting to see more applications across a varied range of fields. One specific machine learning application that is of interest to us is that of software safety and security, especially in the context of parallel programs. The issue of being able to detect concurrency bugs automatically has intrigued programmers for a long time, as the added layer of complexity makes concurrent programs more prone to failure. The development of such automatic detection tools provides considerable benefits to programmers in terms of saving time while debugging, as well as reducing the number of unexpected bugs. We believe machine learning may help achieve this goal by providing additional advantages over current approaches, in terms of both overall tool accuracy as well as programming language flexibility. However, due to the presence of numerous challenges specific to the machine learning approach (
    
[^26]: 利用Birth-Death 过程和探索组件加速Langevin采样

    Accelerate Langevin Sampling with Birth-Death process and Exploration Component. (arXiv:2305.05529v1 [stat.CO])

    [http://arxiv.org/abs/2305.05529](http://arxiv.org/abs/2305.05529)

    该论文提出了一种新的采样方法，在探索新模式和传递有用信息的过程中利用了Birth-Death过程和探索组件，具有高效和指数渐近收敛等优点。

    

    在计算科学和工程中，采样已知概率分布是一项基本任务。针对多峰性，我们提出了一种新的采样方法，利用了Birth-Death过程和探索组件。该方法的主要思想是“三思而后行”。我们保留两组采样器，一组在较高温度下，一组在原始温度下。前者作为探索新模式和将有用信息传递给后者的先驱，后者在接收信息后对目标分布进行采样。我们推导了均场极限，并展示了探索过程如何决定采样效率。此外，在温和假设下，我们证明了指数渐近收敛。最后，我们对以前文献中的实验进行了测试，并将我们的方法与以前的方法进行了比较。

    Sampling a probability distribution with known likelihood is a fundamental task in computational science and engineering. Aiming at multimodality, we propose a new sampling method that takes advantage of both birth-death process and exploration component. The main idea of this method is \textit{look before you leap}. We keep two sets of samplers, one at warmer temperature and one at original temperature. The former one serves as pioneer in exploring new modes and passing useful information to the other, while the latter one samples the target distribution after receiving the information. We derive a mean-field limit and show how the exploration process determines sampling efficiency. Moreover, we prove exponential asymptotic convergence under mild assumption. Finally, we test on experiments from previous literature and compared our methodology to previous ones.
    
[^27]: 基于梯度的可解释AI技术在时间序列数据中的探索：以评估中风康复锻炼为例

    Exploring a Gradient-based Explainable AI Technique for Time-Series Data: A Case Study of Assessing Stroke Rehabilitation Exercises. (arXiv:2305.05525v1 [cs.LG])

    [http://arxiv.org/abs/2305.05525](http://arxiv.org/abs/2305.05525)

    本文提出了一种基于梯度的可解释AI技术的方法，用于识别时间序列数据中的显著帧，在中风康复锻炼领域具有应用潜力。

    

    可解释AI技术被广泛应用于各种应用中，然而在医疗保健领域尤其是时间序列数据上的探索却有限。本文提出了一种基于弱监督模型和基于梯度的可解释AI技术的方法，用于识别在中风康复锻炼中涉及补偿动作的时间序列数据中的显著帧，并取得了不错的评估结果。

    Explainable artificial intelligence (AI) techniques are increasingly being explored to provide insights into why AI and machine learning (ML) models provide a certain outcome in various applications. However, there has been limited exploration of explainable AI techniques on time-series data, especially in the healthcare context. In this paper, we describe a threshold-based method that utilizes a weakly supervised model and a gradient-based explainable AI technique (i.e. saliency map) and explore its feasibility to identify salient frames of time-series data. Using the dataset from 15 post-stroke survivors performing three upper-limb exercises and labels on whether a compensatory motion is observed or not, we implemented a feed-forward neural network model and utilized gradients of each input on model outcomes to identify salient frames that involve compensatory motions. According to the evaluation using frame-level annotations, our approach achieved a recall of 0.96 and an F2-score of
    
[^28]: 多标签学习的最小学习机。

    Minimal Learning Machine for Multi-Label Learning. (arXiv:2305.05518v1 [cs.LG])

    [http://arxiv.org/abs/2305.05518](http://arxiv.org/abs/2305.05518)

    本研究提出了一种最小学习机方法来适应多标签学习，与其他方法相比，它具有最先进的性能、确定性以及简单的超参数选择方式。

    

    基于距离的监督方法——最小学习机，通过学习输入和输出距离矩阵之间的映射，从数据中构建预测模型。本文提出了一种方法并评估了这种技术及其核心组件——距离映射如何适应多标签学习。所提出的方法基于将距离映射与逆距离加权相结合。虽然这种方法是多标签学习文献中最简单的方法之一，但它在小到中等规模的多标签学习问题上实现了最先进的性能。除了它的简单性，所提出的方法是完全确定性的，并且可以通过基于排名损失统计量的方法选择其超参数，这种方法具有封闭形式，因此避免了传统的基于交叉验证的超参数调整方法。此外，由于其简单的线性距离映射构造方式，我们展示了所提出的方法可以评估预测的不确定性。

    Distance-based supervised method, the minimal learning machine, constructs a predictive model from data by learning a mapping between input and output distance matrices. In this paper, we propose methods and evaluate how this technique and its core component, the distance mapping, can be adapted to multi-label learning. The proposed approach is based on combining the distance mapping with an inverse distance weighting. Although the proposal is one of the simplest methods in the multi-label learning literature, it achieves state-of-the-art performance for small to moderate-sized multi-label learning problems. Besides its simplicity, the proposed method is fully deterministic and its hyper-parameter can be selected via ranking loss-based statistic which has a closed form, thus avoiding conventional cross-validation-based hyper-parameter tuning. In addition, due to its simple linear distance mapping-based construction, we demonstrate that the proposed method can assess predictions' uncert
    
[^29]: 在联邦学习中平衡隐私与安全：FedGT的群体测试框架

    Balancing Privacy and Security in Federated Learning with FedGT: A Group Testing Framework. (arXiv:2305.05506v1 [cs.LG])

    [http://arxiv.org/abs/2305.05506](http://arxiv.org/abs/2305.05506)

    该论文提出了FedGT框架，通过群体测试的方法在联邦学习中识别并删除恶意客户，从而平衡了隐私和安全，保护数据隐私并提高了识别恶意客户的能力。

    

    我们提出FedGT，一个新颖的框架，用于在联邦学习中识别恶意客户并进行安全聚合。受到群体测试的启发，该框架利用重叠的客户组来检测恶意客户的存在，并通过译码操作识别它们。然后，将这些被识别的客户从模型的训练中删除，并在其余客户之间执行训练。FedGT在隐私和安全之间取得平衡，允许改进识别能力同时仍保护数据隐私。具体而言，服务器学习每个组中客户的聚合模型。通过对MNIST和CIFAR-10数据集进行大量实验，证明了FedGT的有效性，展示了其识别恶意客户的能力，具有低误检和虚警概率，产生高模型效用。

    We propose FedGT, a novel framework for identifying malicious clients in federated learning with secure aggregation. Inspired by group testing, the framework leverages overlapping groups of clients to detect the presence of malicious clients in the groups and to identify them via a decoding operation. The identified clients are then removed from the training of the model, which is performed over the remaining clients. FedGT strikes a balance between privacy and security, allowing for improved identification capabilities while still preserving data privacy. Specifically, the server learns the aggregated model of the clients in each group. The effectiveness of FedGT is demonstrated through extensive experiments on the MNIST and CIFAR-10 datasets, showing its ability to identify malicious clients with low misdetection and false alarm probabilities, resulting in high model utility.
    
[^30]: 递归是你需要的全部：朝着高效深度展开网络迈进

    Recursions Are All You Need: Towards Efficient Deep Unfolding Networks. (arXiv:2305.05505v1 [cs.CV])

    [http://arxiv.org/abs/2305.05505](http://arxiv.org/abs/2305.05505)

    本文提出了一个基于递归的框架来提高深度展开网络的效率，在训练过程中随机递归数量，以减少整体训练时间，同时引入了一个可学习单元来调节模型特征，可以使网络削减高达75%的可学习参数。

    

    深度展开网络在压缩感知方面的应用非常成功，因为它们提供了简单性和可解释性。然而，由于大多数深度展开网络是迭代的，这会在网络中产生较大的冗余。在这项工作中，我们提出了一个新的基于递归的框架，以增强深度展开模型的效率。首先，使用递归有效地消除深度展开网络中的冗余。其次，我们在训练过程中随机递归的数量，以减少整体训练时间。最后，为了有效地利用递归的能力，我们引入了一个可学习单元，根据迭代总数和当前迭代索引调节模型的特征。为评估所提出的框架，我们将其应用于ISTA-Net+和COAST。广泛的测试显示，我们的提出的框架允许网络削减高达75%的可学习参数，同时大多数维持其性能。

    The use of deep unfolding networks in compressive sensing (CS) has seen wide success as they provide both simplicity and interpretability. However, since most deep unfolding networks are iterative, this incurs significant redundancies in the network. In this work, we propose a novel recursion-based framework to enhance the efficiency of deep unfolding models. First, recursions are used to effectively eliminate the redundancies in deep unfolding networks. Secondly, we randomize the number of recursions during training to decrease the overall training time. Finally, to effectively utilize the power of recursions, we introduce a learnable unit to modulate the features of the model based on both the total number of iterations and the current iteration index. To evaluate the proposed framework, we apply it to both ISTA-Net+ and COAST. Extensive testing shows that our proposed framework allows the network to cut down as much as 75% of its learnable parameters while mostly maintaining its per
    
[^31]: 实际交通标志改变对YOLOv7-目标识别模型的影响

    Effects of Real-Life Traffic Sign Alteration on YOLOv7- an Object Recognition Model. (arXiv:2305.05499v1 [cs.CV])

    [http://arxiv.org/abs/2305.05499](http://arxiv.org/abs/2305.05499)

    本研究调查了改变后的交通标志对目标识别模型准确性和性能的影响，结果表明当暴露于不太可能的条件下修改后的交通标志时，物体检测模型的准确率显著降低。

    

    图像处理技术的进步导致了物体识别(OR)模型在各种应用中的广泛使用，如机场安全和邮件分拣。这些模型已成为AI能力的标志，并支持着国家邮政运营等重要服务。然而，OR模型的性能可能会受到现实场景的影响，例如交通标志改变。因此，本研究调查了改变后的交通标志对目标识别模型准确性和性能的影响。为此，使用公开数据集创建了不同类型的交通标志修改，包括大小、形状、颜色、可见性和角度的改变，并分析这些修改对YOLOv7 (You Only Look Once)模型的检测和分类能力的影响。结果表明，当暴露于不太可能的条件下修改后的交通标志时，物体检测模型的准确率显著降低。

    The advancement of Image Processing has led to the widespread use of Object Recognition (OR) models in various applications, such as airport security and mail sorting. These models have become essential in signifying the capabilities of AI and supporting vital services like national postal operations. However, the performance of OR models can be impeded by real-life scenarios, such as traffic sign alteration. Therefore, this research investigates the effects of altered traffic signs on the accuracy and performance of object recognition models. To this end, a publicly available dataset was used to create different types of traffic sign alterations, including changes to size, shape, color, visibility, and angles. The impact of these alterations on the YOLOv7 (You Only Look Once) model's detection and classification abilities were analyzed. It reveals that the accuracy of object detection models decreases significantly when exposed to modified traffic signs under unlikely conditions. This
    
[^32]: 自我监督的异常土壤湿度传感器检测

    Self-Supervised Anomaly Detection of Rogue Soil Moisture Sensors. (arXiv:2305.05495v1 [cs.LG])

    [http://arxiv.org/abs/2305.05495](http://arxiv.org/abs/2305.05495)

    本文提出了一种自我监督的异常传感器检测方法，使用了基于具有对比损失的神经网络和DBSCAN的技术，通过负采样中使用动态时间规整来区分良好和异常的传感器，并在实验中验证了该方法的可行性。

    

    物联网数据是成功的农业数字化转型的核心要素，但物联网数据也带来了很多挑战，比如数据污染风险。当传感器提供持续不正确的测量值时，称为异常数据。为确保正确的分析结果，与IoT数据一起工作时必需的预处理步骤是检测这些异常传感器。现有的方法假定已知良好的传感器或者大多数传感器都是良好的，但真实世界的数据往往是完全没有标记的并且数量庞大，需要能够在没有先前信息的情况下检测到异常传感器的自我监督方法。我们提出了一种基于具有对比损失的神经网络和DBSCAN的自我监督型异常传感器检测器。我们的论文的一个核心贡献是在三元组损失的负采样中使用动态时间规整。我们基于合成数据和真实世界的土壤湿度数据进行了实验，结果表明我们的方法可以准确地区分良好和异常的传感器，甚至在没有标记训练数据的情况下也能做到。

    IoT data is a central element in the successful digital transformation of agriculture. However, IoT data comes with its own set of challenges. E.g., the risk of data contamination due to rogue sensors. A sensor is considered rogue when it provides incorrect measurements over time. To ensure correct analytical results, an essential preprocessing step when working with IoT data is the detection of such rogue sensors. Existing methods assume that well-behaving sensors are known or that a large majority of the sensors is well-behaving. However, real-world data is often completely unlabeled and voluminous, calling for self-supervised methods that can detect rogue sensors without prior information. We present a self-supervised anomalous sensor detector based on a neural network with a contrastive loss, followed by DBSCAN. A core contribution of our paper is the use of Dynamic Time Warping in the negative sampling for the triplet loss. This novelty makes the use of triplet networks feasible f
    
[^33]: 探究子词分割对transformer语言模型性能的影响

    Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])

    [http://arxiv.org/abs/2305.05480](http://arxiv.org/abs/2305.05480)

    本文研究使用单语词段算法StateMorph训练语言模型时，可以使模型更高效地收敛并获得更好的验证分数。

    

    我们想研究词段如何影响语言模型的性能。我们使用了一种单语词段算法StateMorph，在芬兰语和俄语中训练了GPT-2和BERT模型。作为比较，我们还训练了一个使用BPE和Morfessor分割算法的模型。我们的初步结果表明，StateMorph可以帮助模型更有效地收敛并获得更好的验证分数。

    We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
    
[^34]: 超越研究数据集：工业场景中的新型意图发现

    Going beyond research datasets: Novel intent discovery in the industry setting. (arXiv:2305.05474v1 [cs.CL])

    [http://arxiv.org/abs/2305.05474](http://arxiv.org/abs/2305.05474)

    本文提出了一种用于电子商务领域实时意图发现的新方法，通过对现实生活数据进行预训练和利用对话结构细化，性能提高了33pp。

    

    新型意图发现自动化了将相似的信息（问题）分组以识别以前未知的意图的过程。然而，当前的研究集中在仅具有问题字段并且与现实生活数据集有显著差异的公共可用数据集上。本文提出了改进在大型电子商务平台中部署的意图发现流程的方法。我们展示了在领域内数据上进行预训练语言模型的收益：既有自监督的方式，也有弱监督的方式。我们还设计了一种最佳方法，用于在精调聚类任务期间利用现实生活数据集的对话结构（即问题和答案），我们称其为Conv。我们提出的所有方法综合利用了现实生活数据集，为只针对问题的Constrained Deep Adaptive Clustering (CDAC)模型提供了高达33pp的性能提升。相比之下，仅针对问题数据的CDAC模型只比基准线高达13pp的性能提升。

    Novel intent discovery automates the process of grouping similar messages (questions) to identify previously unknown intents. However, current research focuses on publicly available datasets which have only the question field and significantly differ from real-life datasets. This paper proposes methods to improve the intent discovery pipeline deployed in a large e-commerce platform. We show the benefit of pre-training language models on in-domain data: both self-supervised and with weak supervision. We also devise the best method to utilize the conversational structure (i.e., question and answer) of real-life datasets during fine-tuning for clustering tasks, which we call Conv. All our methods combined to fully utilize real-life datasets give up to 33pp performance boost over state-of-the-art Constrained Deep Adaptive Clustering (CDAC) model for question only. By comparison CDAC model for the question data only gives only up to 13pp performance boost over the naive baseline.
    
[^35]: 基于图神经网络的飞翼设计研究

    Graph Neural Networks for Airfoil Design. (arXiv:2305.05469v1 [cs.LG])

    [http://arxiv.org/abs/2305.05469](http://arxiv.org/abs/2305.05469)

    本文研究基于图神经网络的飞翼设计，利用已知架构的改进解决了求解二维不可压缩Navier-Stokes方程的近似解问题。同时，本文还关注了模型性能表现和新形状的自动生成。

    

    近年来，借助深度学习框架解决偏微分方程（PDE）的研究得到了广泛应用，Graph神经网络（GNN）在解决PDE数值计算方面使用广泛，尤其是对于非结构化数据。但是，复杂的PDE，如Navier-Stokes方程的数值解仍然是一项具有挑战性的任务，而目前大部分相关工作都是集中于简单几何结构的风流模拟或适用于设计目的的外观定性结果。本研究旨在利用针对PDE深度学习方法和GNN的工作，提出一种已知架构的改进，以解决在不同飞翼几何形状下求解二维定常不可压缩Navier-Stokes方程的近似解问题，并对模型的性能进行测试不仅要关注预测精度，还要能够自 主生成新的合理形状。

    The study of partial differential equations (PDE) through the framework of deep learning emerged a few years ago leading to the impressive approximations of simple dynamics. Graph neural networks (GNN) turned out to be very useful in those tasks by allowing the treatment of unstructured data often encountered in the field of numerical resolutions of PDE. However, the resolutions of harder PDE such as Navier-Stokes equations are still a challenging task and most of the work done on the latter concentrate either on simulating the flow around simple geometries or on qualitative results that looks physical for design purpose. In this study, we try to leverage the work done on deep learning for PDE and GNN by proposing an adaptation of a known architecture in order to tackle the task of approximating the solution of the two-dimensional steady-state incompressible Navier-Stokes equations over different airfoil geometries. In addition to that, we test our model not only on its performance ove
    
[^36]: 自注意力动态中的聚类现象

    The emergence of clusters in self-attention dynamics. (arXiv:2305.05465v1 [cs.LG])

    [http://arxiv.org/abs/2305.05465](http://arxiv.org/abs/2305.05465)

    本文证实了当Transformer处理一系列token时，出现“领导者”的经验观察，即随着时间趋于无穷大，代表token的粒子会聚集在特定的极限对象附近，这取决于价值矩阵的谱。

    

    将Transformer视为相互作用的粒子系统，当权重不随时间变化时，本文描述了学习表示的几何形状。我们展示了代表token的粒子随着时间趋于无穷大而趋向于特定的极限对象。出现的极限对象类型取决于价值矩阵的谱。此外，在一维情况下，我们证明了自我注意力矩阵收敛于低秩布尔矩阵。这些结果的组合在数学上证实了Vaswani等人的经验观察，即Transformer处理一系列token时会出现“领导者”。

    Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. The type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. \cite{vaswani2017attention} that \emph{leaders} appear in a sequence of tokens when processed by Transformers.
    
[^37]: 基于优化和人工智能的高校教育质量量化方法，用于透明招聘：第一部分模型开发

    Optimization- and AI-based approaches to academic quality quantification for transparent academic recruitment: part 1-model development. (arXiv:2305.05460v1 [cs.AI])

    [http://arxiv.org/abs/2305.05460](http://arxiv.org/abs/2305.05460)

    该论文开发了两种计算框架来量化高校教育质量，通过一个单一指标（AQI）来综合衡量学者的学术品质。

    

    为了公正地招聘高校和研究机构的教育人员，根据全球公认的学术品质特征确定正确的衡量标准是一个十分微妙、富有挑战性但非常重要的问题。在这一连串的两篇论文中，我们考虑了第一篇论文中的学术品质量化建模部分，而在第二篇论文中考虑了案例研究部分。针对学术品质量化建模，我们开发了两个可用于构建决策支持工具的计算框架：(i) 基于优化的框架，以及 (ii) 基于孪生网络的框架（一种人工神经网络类型）。两个模型的输出都是一个称为学术品质量指数(Academic Quality Index, AQI)的单一指标，它是总体学术品质的度量标准。我们使用全球第一和平均非第一类大学的学者数据，根据《泰晤士高等教育世界大学排名》和 QS 世界大学排名进行假设。

    For fair academic recruitment at universities and research institutions, determination of the right measure based on globally accepted academic quality features is a highly delicate, challenging, but quite important problem to be addressed. In a series of two papers, we consider the modeling part for academic quality quantification in the first paper, in this paper, and the case studies part in the second paper. For academic quality quantification modeling, we develop two computational frameworks which can be used to construct a decision-support tool: (i) an optimization-based framework and (ii) a Siamese network (a type of artificial neural network)-based framework. The output of both models is a single index called Academic Quality Index (AQI) which is a measure of the overall academic quality. The data of academics from first-class and average-class world universities, based on Times Higher Education World University Rankings and QS World University Rankings, are assumed as the refe
    
[^38]: 一种跨频保护徽章：(完全)自主战争中医疗单位和受伤士兵的保护选项

    A Cross-Frequency Protective Emblem: Protective Options for Medical Units and Wounded Soldiers in the Context of (fully) Autonomous Warfare. (arXiv:2305.05459v1 [cs.NI])

    [http://arxiv.org/abs/2305.05459](http://arxiv.org/abs/2305.05459)

    本文提出了一种能够应对(完全)自主战争中保护非战斗人员的跨频保护徽章设计方案。

    

    在(完全)自主战争中保护非战斗人员引发了国际保护标志的时效性问题。我们建议设计一种跨频保护徽章，以便武器系统能够检测到并相应作出措施。在技术部署方面，考虑采用雷达信标等形式，以及机器学习的方法进行解释和分析。

    The protection of non-combatants in times of (fully) autonomous warfare raises the question of the timeliness of the international protective emblem. Incidents in the recent past indicate that it is becoming necessary to transfer the protective emblem to other dimensions of transmission and representation. (Fully) Autonomous weapon systems are often launched from a great distance to the aiming point and there may be no possibility for the operators to notice protective emblems at the point of impact. In this case, the weapon system would have to detect such protective emblems and, if necessary, disintegrate autonomously or request an abort via human-in-the-loop. In our paper, we suggest ways in which a cross-frequency protective emblem can be designed. On the one hand, the technical deployment, e.g. in the form of RADAR beacons, is considered, as well as the interpretation by methods of machine learning. With regard to the technical deployment, possibilities are considered to address d
    
[^39]: 借助权重规范化的鲁棒性隐式正则化

    Robust Implicit Regularization via Weight Normalization. (arXiv:2305.05448v1 [cs.LG])

    [http://arxiv.org/abs/2305.05448](http://arxiv.org/abs/2305.05448)

    本文提出了使用权重规范化的梯度下降作为过度参数化模型的鲁棒隐式正则化方法，实现了对欧几里德范数较低的参数的隐式偏好，并建立了一个统一框架来解决线性模型和神经网络之间的隐式正则化隔阂。

    

    过度参数化的模型可能有许多插值解; 隐式正则化是指特定优化方法对众多插值解之一的隐含喜好。已经建立的工作表明，（随机）梯度下降在用于训练深度线性网络时倾向于具有低秩和/或稀疏解的隐式偏差，从某种程度上解释了为什么通过梯度下降训练的过度参数化神经网络模型在实践中具有良好的泛化性能。然而，现有的平方损失目标理论通常需要可训练权重的非常小的初始化，这与实践中为了更快的收敛和更好的泛化性能而初始化的更大规模的权重矛盾。在本文中，我们旨在通过纳入并分析采用权重规范化的梯度下降来弥合这一差距，其中权重向量以极坐标参数化，导致自然的权重归一化。我们表明，在过度参数化的线性回归模型的设置中，采用权重规范化的梯度下降对欧几里德范数较低的权重向量具有隐式正则化作用。此外，我们建立了一个新颖的统一框架，将权重规范化的隐式偏差与非线性模型的经验范数正则化联系起来，从而弥合了线性模型和神经网络之间的差距。

    Overparameterized models may have many interpolating solutions; implicit regularization refers to the hidden preference of a particular optimization method towards a certain interpolating solution among the many. A by now established line of work has shown that (stochastic) gradient descent tends to have an implicit bias towards low rank and/or sparse solutions when used to train deep linear networks, explaining to some extent why overparameterized neural network models trained by gradient descent tend to have good generalization performance in practice. However, existing theory for square-loss objectives often requires very small initialization of the trainable weights, which is at odds with the larger scale at which weights are initialized in practice for faster convergence and better generalization performance. In this paper, we aim to close this gap by incorporating and analyzing gradient descent with weight normalization, where the weight vector is reparamterized in terms of polar
    
[^40]: 基于注意力机制的变压器网络用于量子态重构

    Attention-Based Transformer Networks for Quantum State Tomography. (arXiv:2305.05433v1 [quant-ph])

    [http://arxiv.org/abs/2305.05433](http://arxiv.org/abs/2305.05433)

    本研究提出一种基于注意力机制和变压器网络的 QST 方法，可捕捉不同测量之间的相关性，并成功应用于检索量子态的密度矩阵，特别是对于受限测量数据的情况表现良好。

    

    由于其良好的表达能力，神经网络一直被用于量子态重构（QST）。为了进一步提高重构量子态的效率，本文探讨了语言建模与量子态重构之间的相似性，并提出了一种基于注意力机制和变压器网络的 QST 方法，用于捕捉不同测量之间的相关性。我们的方法直接从测量统计数据中检索量子态的密度矩阵，并辅助使用综合损失函数来帮助最小化实际态与检索态之间的差异。然后，我们系统地跟踪了涉及各种参数调整的常见训练策略对基于注意力机制的 QST 方法的不同影响。结合这些技术，我们建立了一个稳健的基准线，可以有效地重构纯态和混合态。此外，通过比较三种不同的神经网络方法的性能，我们证明了我们的基于注意力机制的方法表现优于其他方法，特别是对于受限测量数据的情况。

    Neural networks have been actively explored for quantum state tomography (QST) due to their favorable expressibility. To further enhance the efficiency of reconstructing quantum states, we explore the similarity between language modeling and quantum state tomography and propose an attention-based QST method that utilizes the Transformer network to capture the correlations between measured results from different measurements. Our method directly retrieves the density matrices of quantum states from measured statistics, with the assistance of an integrated loss function that helps minimize the difference between the actual states and the retrieved states. Then, we systematically trace different impacts within a bag of common training strategies involving various parameter adjustments on the attention-based QST method. Combining these techniques, we establish a robust baseline that can efficiently reconstruct pure and mixed quantum states. Furthermore, by comparing the performance of thre
    
[^41]: 来自噪声的回音：基于扩散模型的合成超声图像生成用于实际图像分割

    Echo from noise: synthetic ultrasound image generation using diffusion models for real image segmentation. (arXiv:2305.05424v1 [eess.IV])

    [http://arxiv.org/abs/2305.05424](http://arxiv.org/abs/2305.05424)

    该论文提出了一种基于DDPM的方法来生成合成超声图像，用于医学图像分析任务中的有效替代品。在左心室和左房分割任务中，使用仅合成图像训练的神经网络相对于之前最先进方法提高了9.09％、3.7％和15.0％的Dice分数。

    

    我们提出了一种新的方法，通过使用基于去噪扩散概率模型（DDPM）和心脏超声语义标签地图生成合成图像。我们展示出这些合成图像可以作为深度学习模型在医学图像分析任务中的有效替代品，如图像分割。为了证明这种方法的有效性，我们生成了合成的2D心脏超声图像并训练了一个神经网络对左心室和左房进行分割。在仅使用合成图像进行训练的情况下，所训练的神经网络在未见过的真实图像数据集上对左心室内膜、心外膜和左心房的分割分别产生了88.5±6.0％、92.3±3.9％和86.3±10.7％的平均Dice分数。这比先前最先进的方法相比提高了9.09％、3.7％和15.0％。该提议的方法在应用领域具有潜力。

    We propose a novel pipeline for the generation of synthetic images via Denoising Diffusion Probabilistic Models (DDPMs) guided by cardiac ultrasound semantic label maps. We show that these synthetic images can serve as a viable substitute for real data in the training of deep-learning models for medical image analysis tasks such as image segmentation. To demonstrate the effectiveness of this approach, we generated synthetic 2D echocardiography images and trained a neural network for segmentation of the left ventricle and left atrium. The performance of the network trained on exclusively synthetic images was evaluated on an unseen dataset of real images and yielded mean Dice scores of 88.5 $\pm 6.0$ , 92.3 $\pm 3.9$, 86.3 $\pm 10.7$ \% for left ventricular endocardial, epicardial and left atrial segmentation respectively. This represents an increase of $9.09$, $3.7$ and $15.0$ \% in Dice scores compared to the previous state-of-the-art. The proposed pipeline has the potential for applic
    
[^42]: 在电子商务中使用数据增强实现一致的文本分类

    Consistent Text Categorization using Data Augmentation in e-Commerce. (arXiv:2305.05402v1 [cs.LG])

    [http://arxiv.org/abs/2305.05402](http://arxiv.org/abs/2305.05402)

    本文提出了一种在电子商务中使用数据增强实现一致的文本分类的新框架，该框架旨在改进产品分类模型的一致性，同时保持其生产水平的性能。

    

    大规模电子商务数据分类是一项关键的、广泛应用于工业领域的任务。本文旨在改进一家主要网络公司已经在使用的产品分类模型，该模型用于多种应用。在该模型核心中，产品分类模型是一个文本分类模型，接受产品标题作为输入，并从数千个可用候选项中输出最合适的类别。经过进一步观察，我们发现了类似物品标签上的不一致性。例如，标题中关于颜色或尺寸的小变化，会对模型产生较大影响。这种现象可能会对下游的推荐或搜索应用造成负面影响，导致用户体验下降。为了解决这个问题，我们提出了一个新的框架，实现一致的文本分类。我们的目标是提高模型的一致性，并保持其生产水平的性能。

    The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications. At its core, the product categorization model is a text classification model that takes a product title as an input and outputs the most suitable category out of thousands of available candidates. Upon a closer inspection, we found inconsistencies in the labeling of similar items. For example, minor modifications of the product title pertaining to colors or measurements majorly impacted the model's output. This phenomenon can negatively affect downstream recommendation or search applications, leading to a sub-optimal user experience.  To address this issue, we propose a new framework for consistent text categorization. Our goal is to improve the model's consistency while maintaining its production-level performance. W
    
[^43]: 使用随机Lp范数失真探究图像分类器的腐败稳健性

    Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions. (arXiv:2305.05400v1 [cs.LG])

    [http://arxiv.org/abs/2305.05400](http://arxiv.org/abs/2305.05400)

    本研究探讨了使用随机Lp范数失真对图像分类器的训练和测试数据进行增强，并评估模型对不可感知随机失真的稳健性，发现稳健性可能会提高模型在随机失真方面的性能，但也可能会损害L∞范数的稳健性。

    

    稳健性是机器学习分类器实现安全和可靠的基本属性。在对图像分类模型的对抗稳健性和形式稳健性验证领域中，稳健性通常被定义为在Lp范数距离内对所有输入变化的稳定性。然而，对随机失真的稳健性通常通过在现实世界中观察到的变化来改进和评估，而很少考虑数学定义的Lp范数失真。本研究探讨了使用随机Lp范数失真来增强图像分类器的训练和测试数据。我们借鉴了对抗稳健性领域的方法来评估模型对不可感知随机失真的稳健性。我们实证和理论上研究了在不同Lp范数之间稳健性是否可转移，并得出结论，哪些Lp范数的失真应该用来训练和评估模型。我们发现训练数据增强可能会提高模型在随机失真方面的性能，但也可能会损害L∞范数的稳健性。

    Robustness is a fundamental property of machine learning classifiers to achieve safety and reliability. In the fields of adversarial robustness and formal robustness verification of image classification models, robustness is commonly defined as the stability to all input variations within an Lp-norm distance. However, robustness to random corruptions is usually improved and evaluated using variations observed in the real-world, while mathematically defined Lp-norm corruptions are rarely considered. This study investigates the use of random Lp-norm corruptions to augment the training and test data of image classifiers. We adapt an approach from the field of adversarial robustness to assess the model robustness to imperceptible random corruptions. We empirically and theoretically investigate whether robustness is transferable across different Lp-norms and derive conclusions on which Lp-norm corruptions a model should be trained and evaluated on. We find that training data augmentation wi
    
[^44]: 关于锐度感知优化与对抗鲁棒性之间的关系

    On the Relation between Sharpness-Aware Minimization and Adversarial Robustness. (arXiv:2305.05392v1 [cs.LG])

    [http://arxiv.org/abs/2305.05392](http://arxiv.org/abs/2305.05392)

    SAM和对抗性训练（AT）都可以视为特定的特征扰动，其改善了对抗性能。然而，SAM和AT在扰动强度方面是不同的，从而带来了不同的精度和鲁棒性权衡。SAM单独使用可以在不牺牲清晰度精度的情况下提高对抗鲁棒性。

    

    我们在对抗鲁棒性的背景下提出了对锐度感知优化（SAM）的新理解。本文指出，SAM和对抗性训练（AT）都可以视为特定的特征扰动，其改善了对抗鲁棒性。然而，SAM和AT在扰动强度方面是不同的，从而带来了不同的精度和鲁棒性权衡。在一个简化模型中，我们提供了这些声明的理论证据和严格的数学证明。此外，我们进行了实验证明，仅利用SAM可以实现比标准训练更好的对抗鲁棒性，这是意外的好处。由于对抗训练可能会导致清晰度精度的降低，我们展示了仅使用SAM可以在不牺牲清晰度精度的情况下提高鲁棒性。源代码可在https://github.com/weizeming/SAM_AT获取。

    We propose a novel understanding of Sharpness-Aware Minimization (SAM) in the context of adversarial robustness. In this paper, we point out that both SAM and adversarial training (AT) can be viewed as specific feature perturbations, which improve adversarial robustness. However, we note that SAM and AT are distinct in terms of perturbation strength, leading to different accuracy and robustness trade-offs. We provide theoretical evidence for these claims in a simplified model with rigorous mathematical proofs. Furthermore, we conduct experiment to demonstrate that only utilizing SAM can achieve superior adversarial robustness compared to standard training, which is an unexpected benefit. As adversarial training can suffer from a decrease in clean accuracy, we show that using SAM alone can improve robustness without sacrificing clean accuracy. Code is available at https://github.com/weizeming/SAM_AT.
    
[^45]: 非负矩阵分解中的两到五个真相

    Two to Five Truths in Non-Negative Matrix Factorization. (arXiv:2305.05389v1 [cs.LG])

    [http://arxiv.org/abs/2305.05389](http://arxiv.org/abs/2305.05389)

    本文提出了一种受规范化拉普拉斯图的启发的矩阵缩放方法，可以大大提高非负矩阵分解在文本主题模型中的质量。

    

    本文探讨了在使用非负矩阵分解构建主题模型时，矩阵缩放在计数矩阵上的作用。我们提出了一种受规范化拉普拉斯图（NL） 的启发的矩阵缩放方法，可以大大提高非负矩阵分解的质量。在文本分析中，非负矩阵分解 (NMF) 通常用于计数矩阵的共现“上下文”和“术语”。受 LSE 的启发，矩阵缩放对各种数据集中的文本主题模型都有显着的改进。我们在三个数据集上展示了矩阵缩放在 NMF 中的巨大影响，可以大大改善主题模型的质量。

    In this paper, we explore the role of matrix scaling on a matrix of counts when building a topic model using non-negative matrix factorization. We present a scaling inspired by the normalized Laplacian (NL) for graphs that can greatly improve the quality of a non-negative matrix factorization. The results parallel those in the spectral graph clustering work of \cite{Priebe:2019}, where the authors proved adjacency spectral embedding (ASE) spectral clustering was more likely to discover core-periphery partitions and Laplacian Spectral Embedding (LSE) was more likely to discover affinity partitions. In text analysis non-negative matrix factorization (NMF) is typically used on a matrix of co-occurrence ``contexts'' and ``terms" counts. The matrix scaling inspired by LSE gives significant improvement for text topic models in a variety of datasets. We illustrate the dramatic difference a matrix scalings in NMF can greatly improve the quality of a topic model on three datasets where human an
    
[^46]: TASTY：一种基于Transformer的时空复杂度分析方法

    TASTY: A Transformer based Approach to Space and Time complexitY. (arXiv:2305.05379v1 [cs.SE])

    [http://arxiv.org/abs/2305.05379](http://arxiv.org/abs/2305.05379)

    本文旨在通过创建一个跨多种语言的代码片段标记数据集，以填补从代码中分类时间和空间复杂性的空白，并提出了使用基于代码的多模型来实现这一目标。

    

    基于代码的语言模型在软件工程领域中表现出非常有前途的结果，如代码的完善、代码的补全和生成。然而，由于缺乏数据集，从代码中分类时间和空间复杂性的任务还未得到广泛探索，先前的努力仅限于Java。在这个项目中，我们旨在通过创建一个跨多种语言的代码片段标记数据集来填补这些空白（目前是Python和C ++数据集，不久将发布C，C＃和JavaScript数据集）。我们发现现有的时间复杂性计算库和工具仅适用于少数用例。缺乏明确定义的基于规则的系统促使运用最近提出的基于代码的多模型。我们展示了死代码消除和增加LM的最大序列长度的有效性。除了时间复杂性外，我们还建议使用LM来寻找空间复杂性。

    Code based Language Models (LMs) have shown very promising results in the field of software engineering with applications such as code refinement, code completion and generation. However, the task of time and space complexity classification from code has not been extensively explored due to a lack of datasets, with prior endeavors being limited to Java. In this project, we aim to address these gaps by creating a labelled dataset of code snippets spanning multiple languages (Python and C++ datasets currently, with C, C#, and JavaScript datasets being released shortly). We find that existing time complexity calculation libraries and tools only apply to a limited number of use-cases. The lack of a well-defined rule based system motivates the application of several recently proposed code-based LMs. We demonstrate the effectiveness of dead code elimination and increasing the maximum sequence length of LMs. In addition to time complexity, we propose to use LMs to find space complexities from
    
[^47]: 专业认证基准数据集：大语言模型的前500个职位

    Professional Certification Benchmark Dataset: The First 500 Jobs For Large Language Models. (arXiv:2305.05377v1 [cs.AI])

    [http://arxiv.org/abs/2305.05377](http://arxiv.org/abs/2305.05377)

    本研究创建了一个基准数据集，测试了大型语言模型的职业准备技能，比较了GPT-3和Turbo-GPT3.5在1149个专业认证领域的表现，Turbo-GPT3.5的通过率达到了100%。模型证明了在计算机、医疗保健和金融等各个领域中的技能和潜力。这个数据集可以用来进一步训练和评估语言模型的表现。

    

    本研究创建了一个专业认证调查，以测试大型语言模型并评估其就业技能。它比较了两个AI模型GPT-3和Turbo-GPT3.5在一个包括1149个专业认证的基准数据集上的表现，强调的是职业准备而不是学术表现。GPT-3在39%的专业认证中取得了通过分数（>70%正确率），而没有进行微调或考试准备。模型显示出在各种与计算机相关的领域中的资格，如云和虚拟化、商业分析、网络设置和维修以及数据分析等。Turbo-GPT3.5在颇具价值的Offensive Security Certified Professional（OSCP）考试中得分100%。模型还展现了在其他职业领域，包括护理、持牌咨询、药剂学和教学中的能力。Turbo-GPT3.5在没有准备的情况下获得了金融业监管局（FINRA）系列6考试的70%的成绩。感兴趣的读者可以使用这个500个工作职位的数据集来进一步训练和评估语言模型在实际的现实场景中的表现。

    The research creates a professional certification survey to test large language models and evaluate their employable skills. It compares the performance of two AI models, GPT-3 and Turbo-GPT3.5, on a benchmark dataset of 1149 professional certifications, emphasizing vocational readiness rather than academic performance. GPT-3 achieved a passing score (>70% correct) in 39% of the professional certifications without fine-tuning or exam preparation. The models demonstrated qualifications in various computer-related fields, such as cloud and virtualization, business analytics, cybersecurity, network setup and repair, and data analytics. Turbo-GPT3.5 scored 100% on the valuable Offensive Security Certified Professional (OSCP) exam. The models also displayed competence in other professional domains, including nursing, licensed counseling, pharmacy, and teaching. Turbo-GPT3.5 passed the Financial Industry Regulatory Authority (FINRA) Series 6 exam with a 70% grade without preparation. Interes
    
[^48]: HybridNet: 基于几何与拓扑视角的VLSI阻塞预测的双分支融合

    HybridNet: Dual-Branch Fusion of Geometrical and Topological Views for VLSI Congestion Prediction. (arXiv:2305.05374v1 [cs.LG])

    [http://arxiv.org/abs/2305.05374](http://arxiv.org/abs/2305.05374)

    本文提出了HybridNet，一种基于几何与拓扑视角的VLSI阻塞预测的双分支融合网络，通过在网络结构中做出几个关键设计，充分综合电路的拓扑与几何特征，相较于以往方法取得了10.9％的提高。

    

    准确的阻塞预测是帮助设计师在VLSI设计周期内更快迭代的重要环节，而本文提出了一种新的策略，通过在网络结构中做出几个关键设计，充分综合电路的拓扑与几何特征。具体来说，我们构建了两个独立的图（几何图、拓扑图），根据它们的唯一属性采用不同的边缘构建方案。然后，我们提出了一个双分支网络，每个路径中都有不同的编码器层，并通过精细的融合策略进行聚合表示。我们的网络名为HybridNet，不仅提供了一种简单而有效的方法来捕捉单元之间的几何交互，而且还保留了原始电路拓扑关系。在ISPD2015基准测试上的实验结果显示，相较于以往方法，我们取得了10.9％的提高。

    Accurate early congestion prediction can prevent unpleasant surprises at the routing stage, playing a crucial character in assisting designers to iterate faster in VLSI design cycles. In this paper, we introduce a novel strategy to fully incorporate topological and geometrical features of circuits by making several key designs in our network architecture. To be more specific, we construct two individual graphs (geometry-graph, topology-graph) with distinct edge construction schemes according to their unique properties. We then propose a dual-branch network with different encoder layers in each pathway and aggregate representations with a sophisticated fusion strategy. Our network, named HybridNet, not only provides a simple yet effective way to capture the geometric interactions of cells, but also preserves the original topological relationships in the netlist. Experimental results on the ISPD2015 benchmarks show that we achieve an improvement of 10.9% compared to previous methods.
    
[^49]: GNNs: 可以更强、更新、更快

    GNNs,You can be Stronger,Deeper and Faster. (arXiv:2305.05368v1 [cs.LG])

    [http://arxiv.org/abs/2305.05368](http://arxiv.org/abs/2305.05368)

    本文介绍了一种新的理解GNN表现能力的视角，并提出了一个新的采样节点级残差模块SDF，具有更好的表现能力。

    

    图神经网络（GNNs）是一类可以从图结构数据中学习并通过集成邻居节点的表示学习来表现出色的神经网络。然而，GNN的性能会随着层数增加而逐渐降低。本文引入了一个新的概念——k跳子图聚合，提出了一种新的理解GNN表现能力的视角，揭示了传统深层GNN表现逐渐退化的潜在原因，包括聚合子图的重叠以及基于残差的GNN实际上利用了1到k跳子图聚合结果来提高有效性。此外，我们提出了一种新的采样节点级残差模块SDF，通过理论推导证明其比之前的残差方法具有更优的表现能力，可以利用1到k跳跃子图的信息。

    Graph neural networks (GNNs), a type of neural network that can learn from graph-structured data and learn the representation of nodes by aggregating their neighbors, have shown excellent performance in downstream tasks.However, it is known that the performance of graph neural networks (GNNs) degrades gradually as the number of layers increases. Based on k-hop subgraph aggregation, which is a new concept, we propose a new perspective to understand the expressive power of GNN.From this perspective, we reveal the potential causes of the performance degradation of the deep traditional GNN - aggregated subgraph overlap, and the fact that the residual-based graph neural networks in fact exploit the aggregation results of 1 to k hop subgraphs to improve the effectiveness.Further, we propose a new sampling-based node-level residual module named SDF, which is shown by theoretical derivation to obtain a superior expressive power compared to previous residual methods by using information from 1 
    
[^50]: 大型语言模型程序

    Large Language Model Programs. (arXiv:2305.05364v1 [cs.LG])

    [http://arxiv.org/abs/2305.05364](http://arxiv.org/abs/2305.05364)

    本文提出了一种将大型预训练语言模型嵌入算法或程序中，扩展其能力的方法。这种方法可以在未经微调的情况下通过更具算法性的方法获得不错的性能提升。

    

    近年来，大型预训练语言模型(LLMs)已经证明了它们能够通过几个示例来执行指令并执行新的任务的能力。通过这种在上下文示例中参数化LLMs的可能性，可以以比微调低得多的成本拓展它们的能力。我们扩展了这一推理线路，并提出了一种方法，通过将LLM嵌入算法或程序中，进一步扩展LLM的能力。为了证明这种方法的优点，我们提供了一个证据支持的问答的说明性例子。我们通过更具算法性的方法而没有任何微调，在通过一系列思路基线的基础上获得了6.4%的改进。此外，我们从这个角度突出了最近的工作，并讨论了与标准方法相比的优点和缺点。

    In recent years, large pre-trained language models (LLMs) have demonstrated the ability to follow instructions and perform novel tasks from a few examples. The possibility to parameterise an LLM through such in-context examples widens their capability at a much lower cost than finetuning. We extend this line of reasoning and present a method which further expands the capabilities of an LLM by embedding it within an algorithm or program. To demonstrate the benefits of this approach, we present an illustrative example of evidence-supported question-answering. We obtain a 6.4\% improvement over the chain of thought baseline through a more algorithmic approach without any finetuning. Furthermore, we highlight recent work from this perspective and discuss the advantages and disadvantages in comparison to the standard approaches.
    
[^51]: 将隐私保护机制用于联邦学习的探讨

    Turning Privacy-preserving Mechanisms against Federated Learning. (arXiv:2305.05355v1 [cs.LG])

    [http://arxiv.org/abs/2305.05355](http://arxiv.org/abs/2305.05355)

    本文探讨了如何将隐私保护机制应用于联邦学习，并指出了该配置中的一个关键安全漏洞，并设计了对此漏洞的攻击方式。

    

    最近，研究人员成功地利用图形神经网络（GNN）构建了增强型推荐系统，因为它们能够从相关实体之间的交互中学习模式。此外，先前的研究已经调查了联合学习作为建立全局GNN模型的本地隐私保护机制的主要解决方案，而无需将敏感数据收集到单个计算单元中。然而，隐私问题可能会出现，因为分布式客户端生成的局部模型更新的分析可能会返回与敏感本地数据相关的信息。出于这个原因，专家们提出了将联合学习与差分隐私策略和社区驱动方法相结合的解决方案，这涉及将来自邻居客户端的数据组合起来，使个体局部更新不那么依赖于局部敏感数据。在本文中，我们确定了这种配置中一个关键的安全漏洞，并设计了一种攻击，能够欺骗。

    Recently, researchers have successfully employed Graph Neural Networks (GNNs) to build enhanced recommender systems due to their capability to learn patterns from the interaction between involved entities. In addition, previous studies have investigated federated learning as the main solution to enable a native privacy-preserving mechanism for the construction of global GNN models without collecting sensitive data into a single computation unit. Still, privacy issues may arise as the analysis of local model updates produced by the federated clients can return information related to sensitive local data. For this reason, experts proposed solutions that combine federated learning with Differential Privacy strategies and community-driven approaches, which involve combining data from neighbor clients to make the individual local updates less dependent on local sensitive data. In this paper, we identify a crucial security flaw in such a configuration, and we design an attack capable of dece
    
[^52]: 旨在表征基于胶囊网络架构学习的表示方法

    Towards the Characterization of Representations Learned via Capsule-based Network Architectures. (arXiv:2305.05349v1 [cs.LG])

    [http://arxiv.org/abs/2305.05349](http://arxiv.org/abs/2305.05349)

    本研究旨在评估胶囊网络架构学习的表示方法及其可解释性，发现其编码的表示可能与部分-整体关系并不严格相关。

    

    胶囊网络作为标准深度神经网络的一种更为紧凑和可解释的替代方法而重新引入。尽管最近的研究证明了其压缩能力，但至今尚未完全评估其可解释性质。在这里，我们进行了一项系统而原则性的研究，以评估这种类型网络的可解释性。此外，我们特别注意分析所学到的表示中是否确实编码了部分-整体关系的水平。在MNIST、SVHN、PASCAL-part和CelebA数据集中的分析表明，在CapsNets中编码的表示可能既不像文献中通常所述的那样分离，也不是严格与部分-整体关系相关的。

    Capsule Networks (CapsNets) have been re-introduced as a more compact and interpretable alternative to standard deep neural networks. While recent efforts have proved their compression capabilities, to date, their interpretability properties have not been fully assessed. Here, we conduct a systematic and principled study towards assessing the interpretability of these types of networks. Moreover, we pay special attention towards analyzing the level to which part-whole relationships are indeed encoded within the learned representation. Our analysis in the MNIST, SVHN, PASCAL-part and CelebA datasets suggest that the representations encoded in CapsNets might not be as disentangled nor strictly related to parts-whole relationships as is commonly stated in the literature.
    
[^53]: Mediapipe和CNN用于实时美国手语手势识别

    Mediapipe and CNNs for Real-Time ASL Gesture Recognition. (arXiv:2305.05296v1 [cs.CV])

    [http://arxiv.org/abs/2305.05296](http://arxiv.org/abs/2305.05296)

    Mediapipe和CNN用于实时美国手语手势识别。测试结果表明准确率可达99.95％，有潜力用于听力障碍人士的通信设备，并可以应用于其他相似手语。这项研究对计算机视觉和机器学习领域做出了重要贡献。

    

    本研究论文描述了一种使用现代计算机视觉和机器学习方法进行识别美国手语（ASL）运动的实时系统。所提出的方法利用Mediapipe库进行特征提取，使用卷积神经网络（CNN）进行ASL手势分类。测试结果表明，所提出的系统可以以99.95％的准确率检测所有ASL字母，表明它在为听力障碍人士设计的通信设备中有潜力。所提出的方法也可以应用于其他具有相似手部运动的手语，从而可能提高听力丧失人士的生活质量。总的来说，该研究证明了使用Mediapipe和CNN进行实时手语识别的有效性，对计算机视觉和机器学习领域做出了重要贡献。

    This research paper describes a realtime system for identifying American Sign Language (ASL) movements that employs modern computer vision and machine learning approaches. The suggested method makes use of the Mediapipe library for feature extraction and a Convolutional Neural Network (CNN) for ASL gesture classification. The testing results show that the suggested system can detect all ASL alphabets with an accuracy of 99.95%, indicating its potential for use in communication devices for people with hearing impairments. The proposed approach can also be applied to additional sign languages with similar hand motions, potentially increasing the quality of life for people with hearing loss. Overall, the study demonstrates the effectiveness of using Mediapipe and CNN for real-time sign language recognition, making a significant contribution to the field of computer vision and machine learning.
    
[^54]: 关于使用不确定性量化模型的模型盗窃限制

    On the Limitations of Model Stealing with Uncertainty Quantification Models. (arXiv:2305.05293v1 [cs.LG])

    [http://arxiv.org/abs/2305.05293](http://arxiv.org/abs/2305.05293)

    本文研究了使用不确定性量化模型进行模型盗窃的局限性，发现在实际盗窃过程中相互不确定是不可避免的。作者尝试使用多个可能的网络并将它们的预测组合以提高质量，但结果表明只有微弱的改善。作者发现网络多样性不足是导致这一结果的原因之一。

    

    模型盗窃旨在以原始训练成本的一小部分推断受害者模型的功能。然而，在实践中，模型的架构、权重尺寸和原始训练数据无法准确确定，导致在盗窃过程中相互不确定。在这项工作中，我们通过生成多个可能的网络，并将它们的预测组合起来来显式地处理这种不确定性，从而提高盗窃模型的质量。为此，我们比较了五个流行的不确定性量化模型在模型盗窃任务中的表现。令人惊讶的是，我们的结果表明，这些考虑的模型在标签一致性（即保真度）方面只能带来微小的改进。为了找到原因，我们通过查看预测方差作为训练迭代函数的方式来检查模型预测的多样性。我们意识到，在训练过程中，模型往往具有相似的预测，这表明我们想要利用的网络多样性不存在。

    Model stealing aims at inferring a victim model's functionality at a fraction of the original training cost. While the goal is clear, in practice the model's architecture, weight dimension, and original training data can not be determined exactly, leading to mutual uncertainty during stealing. In this work, we explicitly tackle this uncertainty by generating multiple possible networks and combining their predictions to improve the quality of the stolen model. For this, we compare five popular uncertainty quantification models in a model stealing task. Surprisingly, our results indicate that the considered models only lead to marginal improvements in terms of label agreement (i.e., fidelity) to the stolen model. To find the cause of this, we inspect the diversity of the model's prediction by looking at the prediction variance as a function of training iterations. We realize that during training, the models tend to have similar predictions, indicating that the network diversity we wanted
    
[^55]: 通过布朗桥随机过程进行目标导向主动对话的对话规划

    Dialogue Planning via Brownian Bridge Stochastic Process for Goal-directed Proactive Dialogue. (arXiv:2305.05290v1 [cs.CL])

    [http://arxiv.org/abs/2305.05290](http://arxiv.org/abs/2305.05290)

    该论文提出了一种利用随机过程对话规划的方法，该方法通过布朗桥过程建模对话路径的时间动态以实现目标导向型对话系统并取得了良好的效果。

    

    目标导向型对话系统旨在通过多轮对话主动地达到预先确定的目标。实现此任务的关键在于规划对话路径，使其平稳并连贯地指向目标。然而，这是一项具有挑战性并未被深入探究的任务。在本文中，我们提出了一种连贯对话规划方法，它使用随机过程来建模对话路径的时间动态。我们定义了一个潜在空间，通过布朗桥过程捕捉了目标导向行为的连贯性，从而允许我们灵活地将用户反馈纳入对话规划。基于导出的潜在轨迹，我们使用预先训练的语言模型显式地生成对话路径。最后，我们将这些路径作为自然语言提示来引导对话生成。我们的实验表明，我们的方法生成了更连贯的话语，并以更高的成功率实现了目标。

    Goal-directed dialogue systems aim to proactively reach a pre-determined target through multi-turn conversations. The key to achieving this task lies in planning dialogue paths that smoothly and coherently direct conversations towards the target. However, this is a challenging and under-explored task. In this work, we propose a coherent dialogue planning approach that uses a stochastic process to model the temporal dynamics of dialogue paths. We define a latent space that captures the coherence of goal-directed behavior using a Brownian bridge process, which allows us to incorporate user feedback flexibly in dialogue planning. Based on the derived latent trajectories, we generate dialogue paths explicitly using pre-trained language models. We finally employ these paths as natural language prompts to guide dialogue generation. Our experiments show that our approach generates more coherent utterances and achieves the goal with a higher success rate.
    
[^56]: 带有未观察变量的因果关系发现：一种代理变量方法

    Causal Discovery with Unobserved Variables: A Proxy Variable Approach. (arXiv:2305.05281v1 [stat.ME])

    [http://arxiv.org/abs/2305.05281](http://arxiv.org/abs/2305.05281)

    本文提出了一种基于代理变量的方法，以解决因未观察变量而在观测数据中导致错误识别的问题。该方法可适用于连续变量系统，通过提出正则条件控制离散化误差来识别因果关系。

    

    从观测数据中发现因果关系具有重要意义。未观察变量（例如潜在混杂或中介）的存在可能会导致错误的因果识别。为了克服这个问题，近端因果探索方法试图通过未观察变量的代理来调整偏差。特别地，基于假设检验的方法通过测试引发的线性违规来识别因果边缘。然而，这些方法只适用于有严格级别约束的离散数据，这限制了它们在现实世界中的应用。本文通过扩展近端假设检验来解决这个问题，以适用于由连续变量组成的系统。我们的策略是提出给定隐藏因子的观测变量的条件分布的正则条件，使得如果我们将其观察代理以足够的有限细格离散化，则涉及的离散化误差可以有效地受到控制。基于这个方法，我们提出并分析了一种具有一般先验限制的新近端因果搜索算法。

    Discovering causal relations from observational data is important. The existence of unobserved variables (e.g. latent confounding or mediation) can mislead the causal identification. To overcome this problem, proximal causal discovery methods attempted to adjust for the bias via the proxy of the unobserved variable. Particularly, hypothesis test-based methods proposed to identify the causal edge by testing the induced violation of linearity. However, these methods only apply to discrete data with strict level constraints, which limits their practice in the real world. In this paper, we fix this problem by extending the proximal hypothesis test to cases where the system consists of continuous variables. Our strategy is to present regularity conditions on the conditional distributions of the observed variables given the hidden factor, such that if we discretize its observed proxy with sufficiently fine, finite bins, the involved discretization error can be effectively controlled. Based o
    
[^57]: 从子采样时间序列中使用代理变量进行因果推断

    Causal Discovery from Subsampled Time Series with Proxy Variables. (arXiv:2305.05276v1 [cs.LG])

    [http://arxiv.org/abs/2305.05276](http://arxiv.org/abs/2305.05276)

    本研究提出了一种使用代理变量方法的无模型算法，可以从子采样时间序列中无需参数约束地识别整个因果结构。

    

    从时间序列数据推断因果结构是许多科学研究的核心兴趣。采样频率远低于因果影响频率是此类推断的主要障碍。为了克服这个问题，已经提出了许多基于模型和非模型的方法，但是要么局限于线性情况，要么无法建立可识别性。在本研究中，我们提出了一种无模型的算法，可以在没有任何参数约束的情况下从子采样时间序列识别整个因果结构。该方法的思想是，子采样的挑战主要来自于“未观察到”的时间步，因此应使用为未观察到变量设计的工具处理此问题。在这些工具中，我们发现代理变量方法特别适合，因为未观察到变量的代理变量自然是在观察到的时间步上本身。根据这种直觉，我们建立了全面的结构可识别性。

    Inferring causal structures from time series data is the central interest of many scientific inquiries. A major barrier to such inference is the problem of subsampling, i.e., the frequency of measurements is much lower than that of causal influence. To overcome this problem, numerous model-based and model-free methods have been proposed, yet either limited to the linear case or failed to establish identifiability. In this work, we propose a model-free algorithm that can identify the entire causal structure from subsampled time series, without any parametric constraint. The idea is that the challenge of subsampling arises mainly from \emph{unobserved} time steps and therefore should be handled with tools designed for unobserved variables. Among these tools, we find the proxy variable approach particularly fits, in the sense that the proxy of an unobserved variable is naturally itself at the observed time step. Following this intuition, we establish comprehensive structural identifiabili
    
[^58]: DietCNN:用于量化CNN的无乘积推理

    DietCNN: Multiplication-free Inference for Quantized CNNs. (arXiv:2305.05274v1 [cs.CV])

    [http://arxiv.org/abs/2305.05274](http://arxiv.org/abs/2305.05274)

    本文提出了一种用查表法代替CNN中乘法的新方法，其保留了主要CNN操作的语义，且可在FPGA实现中实现显著的能量降低，具有重要的实用价值。

    

    网络嵌入式系统与机器智能的不断增长的需求已经成为促进研究界在嵌入式资源有限的设备上实现基于卷积神经网络（CNN）的推断的催化剂。删除昂贵的乘法操作来重新设计CNN已经显示出在减少推断能量使用方面具有有前途的效果。本文提出了一种用查表法代替CNN中乘法的新方法。与完全修改CNN操作的现有方法不同，所提出的方法保留了主要CNN操作的语义。符合CNN层操作的现有机制确保了标准CNN的可靠性。实验证明，基于单个激活码本的无乘积CNN在MNIST-LeNet-5、CIFAR10-VGG-11和Tiny ImageNet-ResNet的FPGA实现中，每次推理能够实现4.7倍、5.6倍和3.5倍的能量降低。

    The rising demand for networked embedded systems with machine intelligence has been a catalyst for sustained attempts by the research community to implement Convolutional Neural Networks (CNN) based inferencing on embedded resource-limited devices. Redesigning a CNN by removing costly multiplication operations has already shown promising results in terms of reducing inference energy usage. This paper proposes a new method for replacing multiplications in a CNN by table look-ups. Unlike existing methods that completely modify the CNN operations, the proposed methodology preserves the semantics of the major CNN operations. Conforming to the existing mechanism of the CNN layer operations ensures that the reliability of a standard CNN is preserved. It is shown that the proposed multiplication-free CNN, based on a single activation codebook, can achieve 4.7x, 5.6x, and 3.5x reduction in energy per inference in an FPGA implementation of MNIST-LeNet-5, CIFAR10-VGG-11, and Tiny ImageNet-ResNet
    
[^59]: 神经变换器中的鲁棒性语音和语义上下文偏置

    Robust Acoustic and Semantic Contextual Biasing in Neural Transducers for Speech Recognition. (arXiv:2305.05271v1 [cs.CL])

    [http://arxiv.org/abs/2305.05271](http://arxiv.org/abs/2305.05271)

    本文提出了一种使用轻量级字符表示编码细粒度发音特征的方法，称为声学偏置，以提高针对声音相似性引导的上下文偏置，在神经变换器等自动语音识别系统性能中重要的改进。

    

    基于注意力机制的上下文偏置方法已经在端到端自动语音识别（E2E ASR）系统中，如神经变换器中，展现出重要的改进，特别是对于大众的或个性化的罕见词的识别更是如此。这些方法采用交叉注意力来将模型偏置于注入为偏置短语的特定上下文实体。先前的方法通常依赖于子单词编码器来编码偏置短语。然而，子单词标记粗糙，无法捕捉关键的发音信息，这对于基于声音相似性的偏置至关重要。在这项工作中，我们提议使用轻量级字符表示来编码细粒度的发音特征，以改善受声音相似性引导的上下文偏置（称为声学偏置）。我们进一步整合预训练的基于神经语言模型(NLM)的编码器，将话语的语义上下文与上下文实体一起编码以提高下文偏置性能。

    Attention-based contextual biasing approaches have shown significant improvements in the recognition of generic and/or personal rare-words in End-to-End Automatic Speech Recognition (E2E ASR) systems like neural transducers. These approaches employ cross-attention to bias the model towards specific contextual entities injected as bias-phrases to the model. Prior approaches typically relied on subword encoders for encoding the bias phrases. However, subword tokenizations are coarse and fail to capture granular pronunciation information which is crucial for biasing based on acoustic similarity. In this work, we propose to use lightweight character representations to encode fine-grained pronunciation features to improve contextual biasing guided by acoustic similarity between the audio and the contextual entities (termed acoustic biasing). We further integrate pretrained neural language model (NLM) based encoders to encode the utterance's semantic context along with contextual entities to
    
[^60]: 关于多标签学习中Macro-AUC的泛化理解探究

    Towards Understanding Generalization of Macro-AUC in Multi-label Learning. (arXiv:2305.05248v1 [cs.LG])

    [http://arxiv.org/abs/2305.05248](http://arxiv.org/abs/2305.05248)

    本研究探究了 multi-label 学习中常用的 Macro-AUC 的泛化性质，并发现数据集中标签不平衡对泛化界限有重要影响。未经变量处理的基于损失函数的算法可能由于对标签的不平衡更敏感而表现较差，这一结论在多个数据集上得到验证。

    

    在多标签学习中，Macro-AUC是类内AUC算术平均值，通常在实践中使用。然而，它的理论理解远远不足。为了解决这个问题，我们基于对应的代理损失函数表征各种学习算法的宏AUC的泛化属性。我们在理论上确定了影响泛化界限的数据集的关键因素：标签类别不平衡。我们对不平衡感知误差界限的结果表明，广泛使用的未经变量处理的基于损失函数的算法比提出的基于成对和重新加权的算法更敏感于标签类别的不平衡，这可能意味着它的性能较差。此外，各种数据集上的经验结果证实了我们的理论结果。就技术而言，我们提出了一种新的（更通用的）McDiarmid型集中不等式，这可能具有独立的兴趣。

    Macro-AUC is the arithmetic mean of the class-wise AUCs in multi-label learning and is commonly used in practice. However, its theoretical understanding is far lacking. Toward solving it, we characterize the generalization properties of various learning algorithms based on the corresponding surrogate losses w.r.t. Macro-AUC. We theoretically identify a critical factor of the dataset affecting the generalization bounds: \emph{the label-wise class imbalance}. Our results on the imbalance-aware error bounds show that the widely-used univariate loss-based algorithm is more sensitive to the label-wise class imbalance than the proposed pairwise and reweighted loss-based ones, which probably implies its worse performance. Moreover, empirical results on various datasets corroborate our theory findings. To establish it, technically, we propose a new (and more general) McDiarmid-type concentration inequality, which may be of independent interest.
    
[^61]: 在医疗保健领域利用生成式人工智能模型进行合成数据生成：平衡研究与隐私

    Leveraging Generative AI Models for Synthetic Data Generation in Healthcare: Balancing Research and Privacy. (arXiv:2305.05247v1 [cs.LG])

    [http://arxiv.org/abs/2305.05247](http://arxiv.org/abs/2305.05247)

    本文研究了在医疗保健领域使用生成式AI模型合成匿名化病人数据进行研究和培训的方法，旨在平衡数据访问和隐私保护，并探索其在医疗保健领域的应用，并讨论了其未来的挑战和研究方向。

    

    电子病历和数字化医疗数据的广泛应用，促使了使用数据驱动的洞见以增强病患结果，建立诊断和治疗方案。然而，使用真实的病人数据会导致隐私和监管挑战，包括HIPAA和GDPR的合规要求。生成式AI模型，如GAN和VAE用于合成数据生成，提供了一种有前途的解决方案，即平衡有价值的数据访问和病人隐私保护。本文研究使用生成式AI模型创建逼真的匿名化病人数据进行研究和培训，探索合成数据在医疗保健领域的应用，并讨论其益处，挑战和未来研究方向。合成数据有潜力通过提供匿名病人数据来革命化医疗保健，同时保护隐私并实现多种应用。

    The widespread adoption of electronic health records and digital healthcare data has created a demand for data-driven insights to enhance patient outcomes, diagnostics, and treatments. However, using real patient data presents privacy and regulatory challenges, including compliance with HIPAA and GDPR. Synthetic data generation, using generative AI models like GANs and VAEs offers a promising solution to balance valuable data access and patient privacy protection. In this paper, we examine generative AI models for creating realistic, anonymized patient data for research and training, explore synthetic data applications in healthcare, and discuss its benefits, challenges, and future research directions. Synthetic data has the potential to revolutionize healthcare by providing anonymized patient data while preserving privacy and enabling versatile applications.
    
[^62]: 可学习的行为控制：通过高效行为选择打破Atari人类世界记录

    Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection. (arXiv:2305.05239v1 [cs.LG])

    [http://arxiv.org/abs/2305.05239](http://arxiv.org/abs/2305.05239)

    本文提出了一个通用的Learnable Behavioral Control (LBC)框架，使得行为选择空间得到扩大，并通过基于赌博机的元控制器实现行为控制。在Atari游戏上，我们的代理已经达到10个游戏的人类水平，并在7个游戏中达到了目前的最高分。

    

    在深度强化学习中，探索问题是主要挑战之一。最近，一些有希望的工作尝试使用基于群体的方法来处理这个问题，通过从不同探索策略的人群中收集具有不同行为的样本。自适应策略选择已被用于行为控制。然而，行为选择空间在很大程度上受到预定义策略种群的限制，这进一步限制了行为多样性。在本文中，我们提出了一个通用框架称为可学习的行为控制（LBC）来解决这种限制。该框架a)通过从所有策略中制定混合行为映射，实现了显著扩大的行为选择空间；b)构建了一个统一的可学习的行为选择过程。我们将LBC引入分布式离线演员-评论家方法中，并通过基于赌博机的元控制器优化行为映射的选择来实现行为控制。我们的代理已经在10个Atari游戏中达到了人类水平，并在7个游戏中达到了目前的最高分。我们还展示了LBC框架的良好泛化能力，并在机器人控制任务上进行了测试。

    The exploration problem is one of the main challenges in deep reinforcement learning (RL). Recent promising works tried to handle the problem with population-based methods, which collect samples with diverse behaviors derived from a population of different exploratory policies. Adaptive policy selection has been adopted for behavior control. However, the behavior selection space is largely limited by the predefined policy population, which further limits behavior diversity. In this paper, we propose a general framework called Learnable Behavioral Control (LBC) to address the limitation, which a) enables a significantly enlarged behavior selection space via formulating a hybrid behavior mapping from all policies; b) constructs a unified learnable process for behavior selection. We introduce LBC into distributed off-policy actor-critic methods and achieve behavior control via optimizing the selection of the behavior mappings with bandit-based meta-controllers. Our agents have achieved 10
    
[^63]: 边缘-云连续体中量子计算的架构愿景

    Architectural Vision for Quantum Computing in the Edge-Cloud Continuum. (arXiv:2305.05238v1 [quant-ph])

    [http://arxiv.org/abs/2305.05238](http://arxiv.org/abs/2305.05238)

    本文提出了一种基于边缘-云连续体的架构，旨在探索利用移动QPUs的系统和分布式异构资源以使混合应用程序受益。该文讨论了如何整合QPUs和解决方法，并介绍了混合经典-量子神经网络的分布式推理引擎。

    

    目前，量子处理器（QPUs）仅由云供应商提供。然而，随着近期的进展，到处都可以很快地托管QPUs。现有工作尚未借鉴边缘计算研究，探索利用移动QPUs的系统，或者分布式异构资源如何使混合应用程序受益。因此，本文提出了边缘-云连续体中量子计算的架构。我们讨论了延伸现有经典边缘计算工作以整合QPUs的必要性、挑战和解决方法。我们描述了如何通过热启动来定义利用分布在连续体各处的分层资源的工作流程。然后，我们介绍了带有混合经典-量子神经网络（QNNs）的分布式推理引擎，以帮助系统设计人员适应具有复杂要求且产生最高异质度的应用程序。我们提出了解决方案，重点关注分布式环境中的经典层分区和QPU分配，考虑了系统的物理限制。

    Quantum processing units (QPUs) are currently exclusively available from cloud vendors. However, with recent advancements, hosting QPUs is soon possible everywhere. Existing work has yet to draw from research in edge computing to explore systems exploiting mobile QPUs, or how hybrid applications can benefit from distributed heterogeneous resources. Hence, this work presents an architecture for Quantum Computing in the edge-cloud continuum. We discuss the necessity, challenges, and solution approaches for extending existing work on classical edge computing to integrate QPUs. We describe how warm-starting allows defining workflows that exploit the hierarchical resources spread across the continuum. Then, we introduce a distributed inference engine with hybrid classical-quantum neural networks (QNNs) to aid system designers in accommodating applications with complex requirements that incur the highest degree of heterogeneity. We propose solutions focusing on classical layer partitioning a
    
[^64]: 利用空间对比预训练进行未见训练数据的新道路交通预测

    Traffic Forecasting on New Roads Unseen in the Training Data Using Spatial Contrastive Pre-Training. (arXiv:2305.05237v1 [cs.LG])

    [http://arxiv.org/abs/2305.05237](http://arxiv.org/abs/2305.05237)

    本文提出一种名为SCPT的框架，利用对比学习进行空间预训练，并引入一个空间编码器模块，用于从未见数据中提取特征。该方法可以用于进行新道路的交通预测，无需重新训练模型。

    

    随着时间推移会不断建设新的道路，但是之前的深度预测模型对于新道路（未见数据）的泛化能力很少被探索。本文引入了一个被称为时空（ST）分割的新设置，以评估模型对未见数据的泛化能力。在这个设置中，模型训练时使用一部分的道路数据，但测试时使用未见数据的道路。我们还提出了一种新的框架，称之为空间对比预训练（SCPT），其中引入了一个空间编码器模块来提取推理时未见道路的潜在特征。这个空间编码器是使用对比学习预训练的。在推理时，空间编码器仅需要新道路的两天交通数据，而不需要任何重新训练。我们还展示了空间编码器的输出可以有效地用于推断未见道路上的潜在节点嵌入。

    New roads are being constructed all the time. However, the capabilities of previous deep forecasting models to generalize to new roads not seen in the training data (unseen roads) are rarely explored. In this paper, we introduce a novel setup called a spatio-temporal (ST) split to evaluate the models' capabilities to generalize to unseen roads. In this setup, the models are trained on data from a sample of roads, but tested on roads not seen in the training data. Moreover, we also present a novel framework called Spatial Contrastive Pre-Training (SCPT) where we introduce a spatial encoder module to extract latent features from unseen roads during inference time. This spatial encoder is pre-trained using contrastive learning. During inference, the spatial encoder only requires two days of traffic data on the new roads and does not require any re-training. We also show that the output from the spatial encoder can be used effectively to infer latent node embeddings on unseen roads during 
    
[^65]: FedNoRo: 针对类别不平衡和标签噪声异质性的噪声-鲁棒联邦学习

    FedNoRo: Towards Noise-Robust Federated Learning by Addressing Class Imbalance and Label Noise Heterogeneity. (arXiv:2305.05230v1 [cs.LG])

    [http://arxiv.org/abs/2305.05230](http://arxiv.org/abs/2305.05230)

    本文提出了一个名为 FedNoRo 的两阶段框架，用于解决类别不平衡和标签噪声异质性的联邦学习问题，并在 ICH 和 ISIC2019 数据集上取得了更好的表现。

    

    联邦噪声标签学习(FNLL)正在成为一种有前途的隐私保护的多源分散学习工具。现有研究基于全局数据类别平衡的假设，可能无法建模复杂的标签噪声，特别是在医学场景中。本文首先提出了一个更为真实的联邦标签噪声问题，其中全局数据是类别不平衡的，并且标签噪声是异质的，然后提出了一个名为 FedNoRo 的两阶段框架，用于噪声-鲁棒联邦学习。具体而言，在 FedNoRo 的第一阶段，采用每类损失指标之后跟随高斯混合模型进行嘈杂客户端识别。在第二阶段，同时采用知识蒸馏和距离感知聚合函数进行噪声-鲁棒联邦模型更新。对广泛使用的 ICH 和 ISIC2019 数据集的实验结果表明，FedNoRo 相对于最先进的 FNLL 方法在解决联邦学习中的类别不平衡和标签噪声异质性方面具有卓越的性能。

    Federated noisy label learning (FNLL) is emerging as a promising tool for privacy-preserving multi-source decentralized learning. Existing research, relying on the assumption of class-balanced global data, might be incapable to model complicated label noise, especially in medical scenarios. In this paper, we first formulate a new and more realistic federated label noise problem where global data is class-imbalanced and label noise is heterogeneous, and then propose a two-stage framework named FedNoRo for noise-robust federated learning. Specifically, in the first stage of FedNoRo, per-class loss indicators followed by Gaussian Mixture Model are deployed for noisy client identification. In the second stage, knowledge distillation and a distance-aware aggregation function are jointly adopted for noise-robust federated model updating. Experimental results on the widely-used ICH and ISIC2019 datasets demonstrate the superiority of FedNoRo against the state-of-the-art FNLL methods for addre
    
[^66]: BARA: 高效的在线奖励预算分配跨边缘联邦学习激励机制

    BARA: Efficient Incentive Mechanism with Online Reward Budget Allocation in Cross-Silo Federated Learning. (arXiv:2305.05221v1 [cs.LG])

    [http://arxiv.org/abs/2305.05221](http://arxiv.org/abs/2305.05221)

    BARA是一种在线奖励预算分配算法，用于激励跨边缘联邦学习中的数据所有者为模型训练做出贡献，并解决了现有研究中被忽略的奖励预算分配问题。

    

    联邦学习（FL）是一种有前途的分布式机器学习框架，可以保护数据隐私。跨边缘联邦学习通过交换多个通信循环的模型参数，使不同组织的孤立数据岛协作一个参数服务器（PS）来完成模型训练。在跨边缘FL中，激励机制对于激励数据所有者为FL训练做出贡献是必不可少的。然而，如何在不同的循环之间分配奖励预算是一个重要但被现有研究大量忽略的复杂问题。解决这个问题的挑战在于奖励预算分配和FL模型效用改进之间的不透明反馈，使得最优奖励预算分配变得复杂。为了解决这个问题，我们设计了一种使用贝叶斯优化的在线奖励预算分配算法，名为BARA（\underline{B}udget \underline{A}llocation for \underline{R}everse \underline{A}uction）。

    Federated learning (FL) is a prospective distributed machine learning framework that can preserve data privacy. In particular, cross-silo FL can complete model training by making isolated data islands of different organizations collaborate with a parameter server (PS) via exchanging model parameters for multiple communication rounds. In cross-silo FL, an incentive mechanism is indispensable for motivating data owners to contribute their models to FL training. However, how to allocate the reward budget among different rounds is an essential but complicated problem largely overlooked by existing works. The challenge of this problem lies in the opaque feedback between reward budget allocation and model utility improvement of FL, making the optimal reward budget allocation complicated. To address this problem, we design an online reward budget allocation algorithm using Bayesian optimization named BARA (\underline{B}udget \underline{A}llocation for \underline{R}everse \underline{A}uction).
    
[^67]: 基于图神经网络的颗粒流替代模型

    Graph Neural Network-based surrogate model for granular flows. (arXiv:2305.05218v1 [physics.geo-ph])

    [http://arxiv.org/abs/2305.05218](http://arxiv.org/abs/2305.05218)

    研究基于图神经网络的颗粒流替代模型，能够捕捉大规模系统的复杂行为，并且表现出比传统机器学习方法更高的性能。

    

    准确模拟颗粒流动力学对于评估各种岩土工程风险至关重要，包括山体滑坡和泥石流。颗粒流涉及颗粒动态重组，表现出从固体样本到液体样本的复杂转变。传统的连续体和离散数值方法在模拟大规模系统时计算成本高。基于统计或机器学习的模型提供了一种替代方案，但它们通常是以有限的一组参数为基础的经验性模型。由于传统的机器学习模型需要大量的训练数据才能实现泛化，因此它们的学习会依赖于排列的顺序。为了解决这些问题，我们使用了图神经网络——一种学习局部相互作用的最先进的机器学习架构。图表示了动态变化的颗粒流的状态和相互作用定律，例如颗粒之间的能量和动量交换。我们开发了一种基于图神经网络的颗粒流模拟器，能够准确地捕捉高度动态的大规模系统的复杂行为。我们在一系列基准问题上展示了这种方法的有效性，并表明所提出的模型明显优于传统的机器学习方法。

    Accurate simulation of granular flow dynamics is crucial for assessing various geotechnical risks, including landslides and debris flows. Granular flows involve a dynamic rearrangement of particles exhibiting complex transitions from solid-like to fluid-like responses. Traditional continuum and discrete numerical methods are limited by their computational cost in simulating large-scale systems. Statistical or machine learning-based models offer an alternative. Still, they are largely empirical, based on a limited set of parameters. Due to their permutation-dependent learning, traditional machine learning-based models require huge training data to generalize. To resolve these problems, we use a graph neural network, a state-of-the-art machine learning architecture that learns local interactions. Graphs represent the state of dynamically changing granular flows and the interaction laws, such as energy and momentum exchange between grains. We develop a graph neural network-based simulator
    
[^68]: 用于深度学习应用的参数化U弯流数据集

    Dataset of a parameterized U-bend flow for Deep Learning Applications. (arXiv:2305.05216v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2305.05216](http://arxiv.org/abs/2305.05216)

    该数据集包含10,000个U形弯管内的流体流动和热传递模拟，提供全面的基准，可用于各种设计优化问题和方法的研究。该数据集的独特特征是三种不同的数据类型，包括设计参数和目标组合、几何结构的二维图像以及数值模拟的解变量的表示方法。

    

    该数据集包含10,000个U形弯管内的流体流动和热传递模拟。每个模拟都由28个设计参数描述，并使用计算流体力学方法进行处理。该数据集为研究各种设计优化问题和方法提供了全面的基准。为这些研究，可以采用监督、半监督和无监督的深度学习方法。该数据集的一个独特特征是，每个形状可以通过三个不同的数据类型进行表示，包括设计参数和目标组合、几何结构的五个不同分辨率的二维图像以及数值模拟的解变量的表示。第三个表示方法可以考虑用于深度学习方法的数值模拟的特定数据结构。生成数据的源代码和容器已发布在GitHub上。

    This dataset contains 10,000 fluid flow and heat transfer simulations in U-bend shapes. Each of them is described by 28 design parameters, which are processed with the help of Computational Fluid Dynamics methods. The dataset provides a comprehensive benchmark for investigating various problems and methods from the field of design optimization. For these investigations supervised, semi-supervised and unsupervised deep learning approaches can be employed. One unique feature of this dataset is that each shape can be represented by three distinct data types including design parameter and objective combinations, five different resolutions of 2D images from the geometry and the solution variables of the numerical simulation, as well as a representation using the cell values of the numerical mesh. This third representation enables considering the specific data structure of numerical simulations for deep learning approaches. The source code and the container used to generate the data are publ
    
[^69]: FrugalGPT: 如何在降低成本和提高性能的同时使用大型语言模型

    FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance. (arXiv:2305.05176v1 [cs.LG])

    [http://arxiv.org/abs/2305.05176](http://arxiv.org/abs/2305.05176)

    本论文介绍了如何在使用大型语言模型的同时降低成本和提高性能。作者提出了三种策略，包括提示适应，LLM近似和LLM级联，并且通过 FrugalGPT 这种方法，实现了大大降低成本或是提高准确率的效果。

    

    目前有越来越多的用户可以使用付费的大型语言模型（LLM）进行查询。我们回顾了查询流行的LLM API（例如GPT-4，ChatGPT，J1-Jumbo）涉及的成本，发现这些模型具有异构的价格结构，费用可能相差数个数量级。特别是在大量查询和文本的情况下使用LLM可能会很昂贵。因此，我们总结和讨论了三种策略，用户可以利用这些策略来减少使用LLM的汇编成本：1）提示适应，2）LLM近似和3）LLM级联。作为示例，我们提出了FrugalGPT，它是LLM级联的一个简单而灵活的实例，可以学习使用哪些LLM组合来处理不同查询，以降低成本、提高准确性。我们的实验表明，FrugalGPT可以在仅使用费用的98％或与GPT-4相同的成本下，达到最佳单个LLM的性能（例如GPT-4），或者以4％的准确率提高GPT-4的性能。

    There is a rapidly growing number of large language models (LLMs) that users can query for a fee. We review the cost associated with querying popular LLM APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have heterogeneous pricing structures, with fees that can differ by two orders of magnitude. In particular, using LLMs on large collections of queries and text can be expensive. Motivated by this, we outline and discuss three types of strategies that users can exploit to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM cascade which learns which combinations of LLMs to use for different queries in order to reduce cost and improve accuracy. Our experiments show that FrugalGPT can match the performance of the best individual LLM (e.g. GPT-4) with up to 98% cost reduction or improve the accuracy over GPT-4 by 4% with the same co
    
[^70]: 可解释性人工智能的逻辑

    Logic for Explainable AI. (arXiv:2305.05172v1 [cs.AI])

    [http://arxiv.org/abs/2305.05172](http://arxiv.org/abs/2305.05172)

    本文介绍了一种基于符号逻辑的综合、语义和计算理论，以探讨可解释性人工智能的三个维度，以深入理解分类器所做出的决策。

    

    可解释性人工智能的核心问题在于理解（学习）分类器所做出的决策。这种理解有三个方面，在近年来得到了显著关注。第一维与为判断决策所必要和充分的实例条件有关，从而提供了可视化的实例抽象，可视为“决策背后的原因”。下一维与描述足以作出决策的最小条件有关，从而确定了与决策无关的实例最大方面。最后一个维度将其移动到了决策，即标识对实例进行最小扰动以产生替代决策所必需的最小条件。我们在本教程中讨论了沿这些方面的可解释性的全面、语义和计算理论，这是基于符号逻辑的一些最新发展。

    A central quest in explainable AI relates to understanding the decisions made by (learned) classifiers. There are three dimensions of this understanding that have been receiving significant attention in recent years. The first dimension relates to characterizing conditions on instances that are necessary and sufficient for decisions, therefore providing abstractions of instances that can be viewed as the "reasons behind decisions." The next dimension relates to characterizing minimal conditions that are sufficient for a decision, therefore identifying maximal aspects of the instance that are irrelevant to the decision. The last dimension relates to characterizing minimal conditions that are necessary for a decision, therefore identifying minimal perturbations to the instance that yield alternate decisions. We discuss in this tutorial a comprehensive, semantical and computational theory of explainability along these dimensions which is based on some recent developments in symbolic logic
    
[^71]: 采用深度强化学习合作的图神经网络用于疫苗优先考虑

    Cooperating Graph Neural Networks with Deep Reinforcement Learning for Vaccine Prioritization. (arXiv:2305.05163v1 [q-bio.PE])

    [http://arxiv.org/abs/2305.05163](http://arxiv.org/abs/2305.05163)

    本研究采用深度强化学习合作的图神经网络，结合疾病动力学模型，开发了一种疫苗优先考虑策略，旨在在供应有限的情况下降低疫情的总体负担。

    

    本研究探讨了疫苗优先考虑策略，以在供应有限的情况下减少疫情的总体负担。现有方法通过假设亚组人口内的同质性行为和缺乏移动性动态集成，进行宏观或简化的微观疫苗分配。直接将这些模型应用于微观疫苗分配会导致次优解，因为缺乏与行为相关的细节。为了解决这个问题，我们首先将疾病动力学中的移动性异质性融入模型，并使用一个Trans-vaccine-SEIR模型模拟疾病的演变过程。然后我们开发了一种新的深度强化学习方法，来寻求高度空间-时间疾病演化系统的最优疫苗分配策略。图神经网络被用来有效捕捉移动接触网络的结构特性和提取动态疾病特征。在我们的评估中，所提出的框架r

    This study explores the vaccine prioritization strategy to reduce the overall burden of the pandemic when the supply is limited. Existing methods conduct macro-level or simplified micro-level vaccine distribution by assuming the homogeneous behavior within subgroup populations and lacking mobility dynamics integration. Directly applying these models for micro-level vaccine allocation leads to sub-optimal solutions due to the lack of behavioral-related details. To address the issue, we first incorporate the mobility heterogeneity in disease dynamics modeling and mimic the disease evolution process using a Trans-vaccine-SEIR model. Then we develop a novel deep reinforcement learning to seek the optimal vaccine allocation strategy for the high-degree spatial-temporal disease evolution system. The graph neural network is used to effectively capture the structural properties of the mobility contact network and extract the dynamic disease features. In our evaluation, the proposed framework r
    
[^72]: 通过标签内部对齐实现有效的医疗代码预测

    Effective Medical Code Prediction via Label Internal Alignment. (arXiv:2305.05162v1 [cs.LG])

    [http://arxiv.org/abs/2305.05162](http://arxiv.org/abs/2305.05162)

    本文提出了一种通过多视角注意力机制的神经网络，以在临床文本中预测医疗代码，并在标签空间与临床文本之间进行对齐，实现了对先前技术水平的提升。

    

    临床记录通常是由医生输入系统的。这些记录需要标记标准的医疗代码，而每个代码代表一种诊断或医疗治疗程序。对这些记录进行注释费时且容易出错。本文提出了一种多视角注意力机制的神经网络，以从临床文本中预测医疗代码。我们的方法结合了三个信息方面：临床文本的语义上下文，标签（医疗代码）空间之间的关系以及每个临床文本和医疗代码之间的对齐。实验结果表明，我们的方法在多个指标上实现了对先前的技术水平的提升。

    The clinical notes are usually typed into the system by physicians. They are typically required to be marked by standard medical codes, and each code represents a diagnosis or medical treatment procedure. Annotating these notes is time consuming and prone to error. In this paper, we proposed a multi-view attention based Neural network to predict medical codes from clinical texts. Our method incorporates three aspects of information, the semantic context of the clinical text, the relationship among the label (medical codes) space, and the alignment between each pair of a clinical text and medical code. Our method is verified to be effective on the open source dataset. The experimental result shows that our method achieves better performance against the prior state-of-art on multiple metrics.
    
[^73]: 潜在交互式A2C：在开放式多智能体系统中实现强化学习的改进

    Latent Interactive A2C for Improved RL in Open Many-Agent Systems. (arXiv:2305.05159v1 [cs.LG])

    [http://arxiv.org/abs/2305.05159](http://arxiv.org/abs/2305.05159)

    本论文介绍了一种基于编码-解码架构的潜在交互式A2C方法，以在多智能体系统中实现强化学习的改进。该方法显著提高了样本效率，通过学习隐藏状态和其他智能体的行为，解决了在竞争或对抗环境中从其他智能体中获得各种信息可能不可行的问题。

    

    目前广泛应用于多智能体强化学习(MARL)的方法是集中式训练，但该方法需要从其他智能体中获得各种信息，这在竞争或对抗环境中可能不可行。最近，交互式优势演员-评论家(IA2C)方法采用了分散的训练和执行，旨在从可能存在噪声的观察中预测其他智能体的行动。本文提出了一种利用编码-解码架构学习隐藏状态和其他智能体行动的潜在IA2C，我们在两个由众多智能体组成的领域中的实验结果表明，潜在IA2C通过降低方差和更快收敛显著提高了样本效率。此外，我们还介绍了这些领域的开放版本，智能体种群可能随时间变化，并对这些实例进行了评估。

    There is a prevalence of multiagent reinforcement learning (MARL) methods that engage in centralized training. But, these methods involve obtaining various types of information from the other agents, which may not be feasible in competitive or adversarial settings. A recent method, the interactive advantage actor critic (IA2C), engages in decentralized training coupled with decentralized execution, aiming to predict the other agents' actions from possibly noisy observations. In this paper, we present the latent IA2C that utilizes an encoder-decoder architecture to learn a latent representation of the hidden state and other agents' actions. Our experiments in two domains -each populated by many agents -- reveal that the latent IA2C significantly improves sample efficiency by reducing variance and converging faster. Additionally, we introduce open versions of these domains where the agent population may change over time, and evaluate on these instances as well.
    
[^74]: 深度树：利用位于情境潜变量的深度神经网络建模树形结构

    DeepTree: Modeling Trees with Situated Latents. (arXiv:2305.05153v1 [cs.LG])

    [http://arxiv.org/abs/2305.05153](http://arxiv.org/abs/2305.05153)

    提出了一种名为DeepTree的新型建树方法，利用位于情境潜变量的深度神经网络模型，通过学习树木分支结构的发展规律而非手动定义来建模树木，并能够生成各种形状的树木，而不需要定义复杂的分支规则。

    

    本文提出了一种新型建树方法——DeepTree，该方法通过学习树木分支结构的发展规律而非手动定义来建模树木。我们使用神经网络管道来训练“位于情境潜变量”的空间，以便实现在树模型中单个节点的基础上进行局部分支生长预测。我们成功地模拟了树木的生长过程，并能够生成各种形状的树木，而不需要定义复杂的分支规则。

    In this paper, we propose DeepTree, a novel method for modeling trees based on learning developmental rules for branching structures instead of manually defining them. We call our deep neural model situated latent because its behavior is determined by the intrinsic state -encoded as a latent space of a deep neural model- and by the extrinsic (environmental) data that is situated as the location in the 3D space and on the tree structure. We use a neural network pipeline to train a situated latent space that allows us to locally predict branch growth only based on a single node in the branch graph of a tree model. We use this representation to progressively develop new branch nodes, thereby mimicking the growth process of trees. Starting from a root node, a tree is generated by iteratively querying the neural network on the newly added nodes resulting in the branching structure of the whole tree. Our method enables generating a wide variety of tree shapes without the need to define intri
    
[^75]: 基于物理学约束的神经网络在半无限层状域地震波反演中的应用

    Physics-informed neural network for seismic wave inversion in layered semi-infinite domain. (arXiv:2305.05150v1 [physics.geo-ph])

    [http://arxiv.org/abs/2305.05150](http://arxiv.org/abs/2305.05150)

    本文提出了一种基于物理学约束的神经网络框架，在半无限层状域地震波反演中进行地下物质分布的反演，通过将吸收边界条件作为软约束器纳入网络以避免过多计算。实验表明该方法有效性良好。

    

    估算地下物质分布是地震学和地震工程中的难题。物理学约束的神经网络（PINN）的发展为地震反演带来了新的可能性。本文提出了一种PINN框架，用于半无限层状域地震波反演。通过将吸收边界条件作为软约束器纳入网络以避免过多计算。具体而言，我们设计了一个轻量级网络来学习未知的物质分布，以及一个深层神经网络来逼近解决方案变量。整个网络是端到端的，并受稀疏测量数据和基本物理定律（即控制方程和初边界条件）的约束。进行了各种实验来验证我们的反演方法在1D半无限域中地震波传播反演中的有效性。

    Estimating the material distribution of Earth's subsurface is a challenging task in seismology and earthquake engineering. The recent development of physics-informed neural network (PINN) has shed new light on seismic inversion. In this paper, we present a PINN framework for seismic wave inversion in layered (1D) semi-infinite domain. The absorbing boundary condition is incorporated into the network as a soft regularizer for avoiding excessive computation. In specific, we design a lightweight network to learn the unknown material distribution and a deep neural network to approximate solution variables. The entire network is end-to-end and constrained by both sparse measurement data and the underlying physical laws (i.e., governing equations and initial/boundary conditions). Various experiments have been conducted to validate the effectiveness of our proposed approach for inverse modeling of seismic wave propagation in 1D semi-infinite domain.
    
[^76]: 基于自编码器贪婪回溯的乳腺X线摄影质块定位

    Localisation of Mammographic masses by Greedy Backtracking of Activations in the Stacked Auto-Encoders. (arXiv:2305.05136v1 [cs.CV])

    [http://arxiv.org/abs/2305.05136](http://arxiv.org/abs/2305.05136)

    本文提出了一种基于自编码器的乳腺肿块定位方法，利用最大类激活值定位异常区域，该方法在准确性和效率方面均超过了现有的基于深度卷积神经网络的技术。

    

    乳腺X线摄影图像的分析需要准确的定位突出的乳腺肿块。在计算机辅助诊断领域，医师通常标记肿块或感兴趣区域(ROI)，并从中提取特征。本文提出一种新的乳腺肿块定位框架，基于堆叠自编码器的最大类激活值。我们假设在乳腺X线摄影图像中激活异常类的图像区域将是引起异常的乳腺肿块。实验使用IRMA乳腺X线摄影数据集中随机选择的200个图像(100个正常和100个异常)进行。由专业放射科医生标记的异常肿块区域用作标准答案。所提出的方法在突出区域检测准确性方面优于现有的基于深度卷积神经网络 (DCNN)的技术。所提出的贪婪回溯方法更为高效，不需要大量标记。

    Mammographic image analysis requires accurate localisation of salient mammographic masses. In mammographic computer-aided diagnosis, mass or Region of Interest (ROI) is often marked by physicians and features are extracted from the marked ROI. In this paper, we present a novel mammographic mass localisation framework, based on the maximal class activations of the stacked auto-encoders. We hypothesize that the image regions activating abnormal classes in mammographic images will be the breast masses which causes the anomaly. The experiment is conducted using randomly selected 200 mammographic images (100 normal and 100 abnormal) from IRMA mammographic dataset. Abnormal mass regions marked by an expert radiologist are used as the ground truth. The proposed method outperforms existing Deep Convolutional Neural Network (DCNN) based techniques in terms of salient region detection accuracy. The proposed greedy backtracking method is more efficient and does not require a vast number of labell
    
[^77]: 一种基于Kriging-Random Forest混合模型的地压平衡盾构隧道实时地质预测

    A Kriging-Random Forest Hybrid Model for Real-time Ground Property Prediction during Earth Pressure Balance Shield Tunneling. (arXiv:2305.05128v1 [cs.LG])

    [http://arxiv.org/abs/2305.05128](http://arxiv.org/abs/2305.05128)

    该研究提出了一种Kriging-Random Forest混合模型，结合先前预测的地质信息和实时的运行参数信息，为地压平衡盾构机前方地质预测提供指导，从而缓解施工风险。

    

    该文提出了一种Kriging-Random Forest混合模型，通过将Kriging外推算法和Random Forest相结合，为地压平衡盾构机前方地质预测提供指导，从而缓解施工风险。该算法同时利用了先前预测的地质信息和实时的运行参数信息进行预测，运用加权平均方法将预测结果结合，使得预测结果具有最小的不确定性。

    A kriging-random forest hybrid model is developed for real-time ground property prediction ahead of the earth pressure balanced shield by integrating Kriging extrapolation and random forest, which can guide shield operating parameter selection thereby mitigate construction risks. The proposed KRF algorithm synergizes two types of information: prior information and real-time information. The previously predicted ground properties with EPB operating parameters are extrapolated via the Kriging algorithm to provide prior information for the prediction of currently being excavated ground properties. The real-time information refers to the real-time operating parameters of the EPB shield, which are input into random forest to provide a real-time prediction of ground properties. The integration of these two predictions is achieved by assigning weights to each prediction according to their uncertainties, ensuring the prediction of KRF with minimum uncertainty. The performance of the KRF algori
    
[^78]: 使用数据内核比较基础模型

    Comparing Foundation Models using Data Kernels. (arXiv:2305.05126v1 [cs.LG])

    [http://arxiv.org/abs/2305.05126](http://arxiv.org/abs/2305.05126)

    本文采用基于数据内核的方法比较基础模型，不受度量指标的约束，通过嵌入空间几何实现点对点和多模型比较，并成功诱导了一组与下游指标强相关的模型距离函数流形。

    

    最近自主学习和神经网络扩展的进展使得可以创建大型基础模型，这些模型可以轻松地适应各种下游任务。目前比较基础模型的范式涉及在各种策划数据集上使用聚合指标进行基准测试。不幸的是，这种模型比较方法严重依赖于度量指标的选择，这使得它在理想度量不明显或不可用的情况下不适用。在这项工作中，我们提出了一种没有度量指标的基础模型比较方法，通过它们的嵌入空间几何来实现。我们的方法基于随机图理论，并促进点对点和多模型比较。此外，我们展示了如何使用我们的框架诱导一组配备有与一些下游指标强相关的距离函数的模型流形。

    Recent advances in self-supervised learning and neural network scaling have enabled the creation of large models -- known as foundation models -- which can be easily adapted to a wide range of downstream tasks. The current paradigm for comparing foundation models involves benchmarking them with aggregate metrics on various curated datasets. Unfortunately, this method of model comparison is heavily dependent on the choice of metric, which makes it unsuitable for situations where the ideal metric is either not obvious or unavailable. In this work, we present a metric-free methodology for comparing foundation models via their embedding space geometry. Our methodology is grounded in random graph theory, and facilitates both pointwise and multi-model comparison. Further, we demonstrate how our framework can be used to induce a manifold of models equipped with a distance function that correlates strongly with several downstream metrics.
    
[^79]: 基于双重注意力网络的弹性车间调度问题的深度强化学习

    Flexible Job Shop Scheduling via Dual Attention Network Based Reinforcement Learning. (arXiv:2305.05119v1 [cs.LG])

    [http://arxiv.org/abs/2305.05119](http://arxiv.org/abs/2305.05119)

    本文提出了一种基于深度强化学习和自注意模型的端到端学习框架，通过双重注意力网络来准确而简洁地表示操作和机器之间错综复杂的关系，以协同地确定FJSP的优先级分配规则。

    

    弹性制造催生了复杂的调度问题，如弹性车间调度问题（FJSP）。在FJSP中，操作可以在多台机器上进行处理，导致操作和机器之间存在错综复杂的关系。最近的研究利用深度强化学习（DRL）来学习优先级分配规则（PDRs）以解决FJSP。然而，相对于诸如OR-Tools等精确方法的解决方案，解决方案的质量仍有提高的空间。为了解决这个问题，本文提出了一种新的端到端学习框架，结合了自注意模型进行深度特征提取和DRL进行可扩展决策制定的优点。操作和机器之间的复杂关系被准确而简洁地表示出来，提出了一个由多个相互连接的操作信息注意块和机器信息注意块组成的双注意力网络（DAN）。DAN利用这些复杂的关系，以协同地确定FJSP的PDRs。

    Flexible manufacturing has given rise to complex scheduling problems such as the flexible job shop scheduling problem (FJSP). In FJSP, operations can be processed on multiple machines, leading to intricate relationships between operations and machines. Recent works have employed deep reinforcement learning (DRL) to learn priority dispatching rules (PDRs) for solving FJSP. However, the quality of solutions still has room for improvement relative to that by the exact methods such as OR-Tools. To address this issue, this paper presents a novel end-to-end learning framework that weds the merits of self-attention models for deep feature extraction and DRL for scalable decision-making. The complex relationships between operations and machines are represented precisely and concisely, for which a dual-attention network (DAN) comprising several interconnected operation message attention blocks and machine message attention blocks is proposed. The DAN exploits the complicated relationships to co
    
[^80]: Flame：简化联邦学习操作的工具

    Federated Learning Operations Made Simple with Flame. (arXiv:2305.05118v1 [cs.LG])

    [http://arxiv.org/abs/2305.05118](http://arxiv.org/abs/2305.05118)

    Flame 是一个工具，通过引入高级抽象——角色和通道，将联邦学习应用程序描述为拓扑抽象图，解耦 ML 应用程序逻辑与底层部署细节，使得可以专门定制部署，减少开发工作，改进自动化和调整。

    

    分布式机器学习方法，包括广泛的联邦学习技术，在广泛分布的基础架构上部署机器学习应用程序时带来了许多优点。然而，为了实现预期的效益，需要进行应用程序和配置级别的更改，这涉及部署特定的细节。通过引入更高级别的抽象——角色和通道，可以大大减少这些复杂性，并将联邦学习应用程序描述为拓扑抽象图（TAG）。TAG将ML应用程序逻辑与底层部署细节解耦，使得可以专门定制应用程序部署，从而降低开发工作量，并为改进自动化和调整铺平道路。我们推出了Flame，这是第一个支持这些抽象概念的系统，并演示了它对多个用例的好处。

    Distributed machine learning approaches, including a broad class of federated learning techniques, present a number of benefits when deploying machine learning applications over widely distributed infrastructures. To realize the expected benefits, however, introduces substantial operational challenges due to required application and configuration-level changes related to deployment-specific details. Such complexities can be greatly reduced by introducing higher-level abstractions -- role and channel -- using which federated learning applications are described as Topology Abstraction Graphs (TAGs). TAGs decouple the ML application logic from the underlying deployment details, making it possible to specialize the application deployment, thus reducing development effort and paving the way for improved automation and tuning. We present Flame, the first system that supports these abstractions, and demonstrate its benefits for several use cases.
    
[^81]: 通过可调节的辅助多代理对抗生成实现通信鲁棒的多代理学习

    Communication-Robust Multi-Agent Learning by Adaptable Auxiliary Multi-Agent Adversary Generation. (arXiv:2305.05116v1 [cs.LG])

    [http://arxiv.org/abs/2305.05116](http://arxiv.org/abs/2305.05116)

    本文提出了一种适应性的辅助对抗生成的方法，命名为MA3C，以在合作多代理强化学习环境下实现通信鲁棒性，能够通过共同的目标最小化协同能力，有效提高通信策略的鲁棒性，在通信攻击下取得更好的性能。

    

    在合作多代理强化学习环境下，通信可以促进代理的协作，然而现有研究主要集中在提高代理的通信效率，忽略了真实通信中的噪声或潜在攻击者可能导致的困难。因此，通信策略的稳健性成为一个急需探究的问题。本文提出使用辅助对抗方法训练的自我系统可以应对这种局限，并提出了一种适应性多代理辅助对抗生成的方法，命名为MA3C，以获得稳健的通信策略。具体而言，我们引入了一种新颖的信息攻击方法，将辅助对手的学习建模为一个协作问题，旨在通过一个共同的目标最小化自我系统的协同能力，从而使每个信息通道都可能受到不同的信息攻击。此外，我们将适应性MA3C框架集成到现有的MARL算法中，并将其应用于基准任务。实验结果表明，MA3C可以有效提高通信策略的鲁棒性，在通信攻击下取得更好的性能，验证了我们方法的有效性和泛化性。

    Communication can promote coordination in cooperative Multi-Agent Reinforcement Learning (MARL). Nowadays, existing works mainly focus on improving the communication efficiency of agents, neglecting that real-world communication is much more challenging as there may exist noise or potential attackers. Thus the robustness of the communication-based policies becomes an emergent and severe issue that needs more exploration. In this paper, we posit that the ego system trained with auxiliary adversaries may handle this limitation and propose an adaptable method of Multi-Agent Auxiliary Adversaries Generation for robust Communication, dubbed MA3C, to obtain a robust communication-based policy. In specific, we introduce a novel message-attacking approach that models the learning of the auxiliary attacker as a cooperative problem under a shared goal to minimize the coordination ability of the ego system, with which every information channel may suffer from distinct message attacks. Furthermore
    
[^82]: 当手上有一个CBR比丛林里的两个更好时

    When a CBR in Hand is Better than Twins in the Bush. (arXiv:2305.05111v1 [cs.LG])

    [http://arxiv.org/abs/2305.05111](http://arxiv.org/abs/2305.05111)

    本文讨论了一个回归问题——预测航班起飞延误。一个由 XGB-CBR Twin 转换来的 CBR 模型提供了最准确的局部预测、最易解释的局部解释表示和全局重要性的维护。

    

    被称为可解释的AI方法常被支持解释性与准确性之间存在权衡的人贬低为不准确。然而，在许多问题的情况下，这种权衡并不存在。本文讨论了一个回归问题——预测航班起飞延误，在这个问题中，使用梯度提升决策树的XGBoost实现训练了最精确的数据回归模型。而在构建XGB-CBR Twin并将XGBoost特征重要性转换为CBR模型中的全局权重时，结果单独使用的CBR模型提供了最准确的局部预测，保持了全局重要性，提供了最易解释的局部解释表示。这个结果的CBR模型成为了这个问题情境下准确性和解释性的基准，从而用于评估两种添加特征属性方法SHAP和LIME来解释XGBoost回归模型。

    AI methods referred to as interpretable are often discredited as inaccurate by supporters of the existence of a trade-off between interpretability and accuracy. In many problem contexts however this trade-off does not hold. This paper discusses a regression problem context to predict flight take-off delays where the most accurate data regression model was trained via the XGBoost implementation of gradient boosted decision trees. While building an XGB-CBR Twin and converting the XGBoost feature importance into global weights in the CBR model, the resultant CBR model alone provides the most accurate local prediction, maintains the global importance to provide a global explanation of the model, and offers the most interpretable representation for local explanations. This resultant CBR model becomes a benchmark of accuracy and interpretability for this problem context, and hence it is used to evaluate the two additive feature attribute methods SHAP and LIME to explain the XGBoost regressio
    
[^83]: 基于半监督的联邦学习用于关键词检测

    Semi-Supervised Federated Learning for Keyword Spotting. (arXiv:2305.05110v1 [cs.LG])

    [http://arxiv.org/abs/2305.05110](http://arxiv.org/abs/2305.05110)

    本研究证明了半监督联邦学习和联邦学习在关键词检测中的有效性。在使用少量标记数据的情况下，使用SSFL可以显著提高模型性能。

    

    关键词检测是移动设备和虚拟助手中音频应用的重要组成部分。联邦学习的最新发展显着扩展了利用多个分布式设备的计算和私有数据资源进行机器学习模型训练的能力。然而，现有的联邦学习方法通常要求设备具有准确的标签，而当涉及到本地音频数据时，这可能既昂贵又不实用。在本研究中，我们首先证明了半监督联邦学习和联邦学习在关键词检测中的有效性。然后，我们将调查扩展到半监督的联邦学习（SSFL）用于关键词检测，其中设备具有完全未标记的数据，而服务器可以访问少量标记数据。我们使用最先进的SSL、FL和SSFL技术进行数字分析，证明了利用丰富资源可以显著提高KWS模型的性能。

    Keyword Spotting (KWS) is a critical aspect of audio-based applications on mobile devices and virtual assistants. Recent developments in Federated Learning (FL) have significantly expanded the ability to train machine learning models by utilizing the computational and private data resources of numerous distributed devices. However, existing FL methods typically require that devices possess accurate ground-truth labels, which can be both expensive and impractical when dealing with local audio data. In this study, we first demonstrate the effectiveness of Semi-Supervised Federated Learning (SSL) and FL for KWS. We then extend our investigation to Semi-Supervised Federated Learning (SSFL) for KWS, where devices possess completely unlabeled data, while the server has access to a small amount of labeled data. We perform numerical analyses using state-of-the-art SSL, FL, and SSFL techniques to demonstrate that the performance of KWS models can be significantly improved by leveraging the abun
    
[^84]: 面向危及生命的室性心律失常检测的微小机器学习设计竞赛

    TinyML Design Contest for Life-Threatening Ventricular Arrhythmia Detection. (arXiv:2305.05105v1 [eess.SP])

    [http://arxiv.org/abs/2305.05105](http://arxiv.org/abs/2305.05105)

    TDC'22是第一届面向ICDs低功耗微控制器的人工智能/机器学习（AI/ML）算法创新竞赛。本次竞赛的挑战是开发一种基于AI/ML的新型实时检测算法，对危及生命的室性心律失常进行检测。

    

    第一届ACM/IEEE微小机器学习设计竞赛（TDC）于2022年在第41届计算机辅助设计国际会议（ICCAD）上举行，是一项具有挑战性的多月研发竞赛。TDC'22专注于需要在可植入设备上创新和实现人工智能/机器学习（AI/ML）算法的真实医疗问题。TDC'22的挑战问题是开发一种基于AI/ML的新型实时检测算法，用于心脏除颤器（ICDs）上使用的低功率微控制器对危及生命的室性心律失常进行检测。数据集包含来自90个受试者的8种不同心律类型的超过38,000个5秒心内电图（IEGM）片段。专用硬件平台是STMicroelectronics制造的NUCLEO-L432KC。TDC'22面向全球多人团队，吸引了来自50多个组织的150多支队伍参赛。本文首先介绍这一医疗问题，

    The first ACM/IEEE TinyML Design Contest (TDC) held at the 41st International Conference on Computer-Aided Design (ICCAD) in 2022 is a challenging, multi-month, research and development competition. TDC'22 focuses on real-world medical problems that require the innovation and implementation of artificial intelligence/machine learning (AI/ML) algorithms on implantable devices. The challenge problem of TDC'22 is to develop a novel AI/ML-based real-time detection algorithm for life-threatening ventricular arrhythmia over low-power microcontrollers utilized in Implantable Cardioverter-Defibrillators (ICDs). The dataset contains more than 38,000 5-second intracardiac electrograms (IEGMs) segments over 8 different types of rhythm from 90 subjects. The dedicated hardware platform is NUCLEO-L432KC manufactured by STMicroelectronics. TDC'22, which is open to multi-person teams world-wide, attracted more than 150 teams from over 50 organizations. This paper first presents the medical problem, da
    
[^85]: 旨在揭示医学图像分析中的校准偏差

    Towards unraveling calibration biases in medical image analysis. (arXiv:2305.05101v1 [eess.IV])

    [http://arxiv.org/abs/2305.05101](http://arxiv.org/abs/2305.05101)

    本研究旨在解决医学图像分析中AI系统的校准偏差问题，即模型预测与实际数据不符合的问题。目前，大部分关于医疗算法公平性的研究都侧重于歧视偏差的评估，而校准偏差的评估仍然不足。

    

    近年来，人工智能系统在医学图像分析领域的自动化应用取得了巨大发展。与此同时，大量研究表明，在各种应用场景中，人工智能系统可能存在系统性和不公平的对某些人群的歧视。这两个事实促使算法公平性研究在这个领域的出现。迄今为止，关于医疗算法公平性的大部分研究侧重于以经典的歧视指标例如AUC和准确度来评估偏差。然而，模型校准方面的潜在偏差只是最近开始得到评估。这在使用临床决策支持系统时尤为重要，因为预测不确定性是医疗专业人员优化评估和结合多个信息来源的关键。本工作研究了自动检测恶性皮肤病变的模型中的歧视和校准偏差。

    In recent years the development of artificial intelligence (AI) systems for automated medical image analysis has gained enormous momentum. At the same time, a large body of work has shown that AI systems can systematically and unfairly discriminate against certain populations in various application scenarios. These two facts have motivated the emergence of algorithmic fairness studies in this field. Most research on healthcare algorithmic fairness to date has focused on the assessment of biases in terms of classical discrimination metrics such as AUC and accuracy. Potential biases in terms of model calibration, however, have only recently begun to be evaluated. This is especially important when working with clinical decision support systems, as predictive uncertainty is key for health professionals to optimally evaluate and combine multiple sources of information. In this work we study discrimination and calibration biases in models trained for automatic detection of malignant dermatol
    
[^86]: 数字病理学图像的自适应域泛化

    Adaptive Domain Generalization for Digital Pathology Images. (arXiv:2305.05100v1 [eess.IV])

    [http://arxiv.org/abs/2305.05100](http://arxiv.org/abs/2305.05100)

    本研究旨在解决数字病理学图像中的域漂移问题，引入一种反应性域泛化技术，在测试时自适应域偏移，无需事先预测或提供示例。

    

    在基于人工智能的组织病理学中，域偏移是常见且经得起考验的。本研究专注于染色和扫描仪变化，然而，这种变化并没有显示整个图景——偏移可能是其他偏移的组合，或者是“隐形”偏移，它们虽然不明显，但仍然会影响机器学习模型的性能。此外，重要的是让模型在没有昂贵或稀缺标注的情况下对这些变化进行泛化，特别是在组织病理学领域，并且希望将模型部署在更大的范围内。因此，需要“反应性”域泛化技术：在测试时适应域偏移而不需要在训练时间预测或提供偏移的示例。我们进行了文献综述，并介绍了在测试时间适应模型参数的技术，通过对第二个

    In AI-based histopathology, domain shifts are common and well-studied. However, this research focuses on stain and scanner variations, which do not show the full picture-- shifts may be combinations of other shifts, or "invisible" shifts that are not obvious but still damage performance of machine learning models. Furthermore, it is important for models to generalize to these shifts without expensive or scarce annotations, especially in the histopathology space and if wanting to deploy models on a larger scale. Thus, there is a need for "reactive" domain generalization techniques: ones that adapt to domain shifts at test-time rather than requiring predictions of or examples of the shifts at training time. We conduct a literature review and introduce techniques that react to domain shifts rather than requiring a prediction of them in advance. We investigate test time training, a technique for domain generalization that adapts model parameters at test-time through optimization of a secon
    
[^87]: 谁需要解码器？高效预测序列级属性。（arXiv:2305.05098v1 [cs.LG]）

    Who Needs Decoders? Efficient Estimation of Sequence-level Attributes. (arXiv:2305.05098v1 [cs.LG])

    [http://arxiv.org/abs/2305.05098](http://arxiv.org/abs/2305.05098)

    研究提出了非自回归代理模型(NAP)，通过编码序列直接预测通用标量值序列级属性，可以高效地实现解码步骤的规避。在机器翻译和语音识别两个场景下，NAP分别可以优于深度集成和高精度地预测性能指标。

    

    现代化序列到序列的模型通常需要自回归解码，这往往非常消耗资源。然而，对于某些下游任务，例如越界检测和资源分配，实际解码输出并不需要，只需要一个序列的标量属性。在这些场景下，知道系统输出质量以预测性能较差比知道输出本身更为重要，那么是否可以绕过自回归解码？我们提出了非自回归代理（NAP）模型，可以高效地预测通用标量值序列级属性。重要的是，NAP直接从编码预测这些指标，避免了昂贵的自回归解码阶段。我们考虑了两个序列到序列任务：机器翻译（MT）和语音识别（ASR）。在MT的越界检测中，NAP表现优于深度集成，同时速度显著更快。NAP也被证明能够高准确度地预测ASR的性能指标，例如词错误率。我们的发现表明，在属性可以从编码中直接预测的任务中，NAP为传统基于解码的方法提供了高效的替代方案。

    State-of-the-art sequence-to-sequence models often require autoregressive decoding, which can be highly expensive. However, for some downstream tasks such as out-of-distribution (OOD) detection and resource allocation, the actual decoding output is not needed just a scalar attribute of this sequence. In these scenarios, where for example knowing the quality of a system's output to predict poor performance prevails over knowing the output itself, is it possible to bypass the autoregressive decoding? We propose Non-Autoregressive Proxy (NAP) models that can efficiently predict general scalar-valued sequence-level attributes. Importantly, NAPs predict these metrics directly from the encodings, avoiding the expensive autoregressive decoding stage. We consider two sequence-to-sequence task: Machine Translation (MT); and Automatic Speech Recognition (ASR). In OOD for MT, NAPs outperform a deep ensemble while being significantly faster. NAPs are also shown to be able to predict performance me
    
[^88]: 通用图上的自我排斥随机游走 - 通过非线性马尔可夫链实现最小采样方差

    Self-Repellent Random Walks on General Graphs - Achieving Minimal Sampling Variance via Nonlinear Markov Chains. (arXiv:2305.05097v1 [math.PR])

    [http://arxiv.org/abs/2305.05097](http://arxiv.org/abs/2305.05097)

    本文设计了一种自我排斥随机游走模型，可实现较小的渐近采样方差，适用于网络拓扑的采样和邻域探索。

    

    我们考虑在离散状态空间上的随机游走，例如一般的无向图，其中随机游走设计成通过采样和邻域探索来逼近网络拓扑上的目标量，以马尔可夫链蒙特卡罗 (MCMC) 程序的形式进行。对于任何相应于目标概率分布的马尔可夫链，我们设计了一种自我排斥随机游走 (SRRW)，它不太可能转移到过去被高度访问的节点，而更可能转移到很少被访问的节点。对于一类由正实数 {\alpha} 参数化的 SRRW，我们证明了该过程的经验分布几乎肯定收敛于底层马尔可夫链内核的目标 (平稳) 分布。然后我们提供了一个中心极限定理，并推导出所得到的渐近协方差矩阵的精确形式，这使我们能够表明，具有更强的排斥作用 (较大的 {\alpha}) 的 SRRW 一定比具有较弱的排斥作用 (较小的 {\alpha}) 的 SRRW 实现更小的渐近采样方差。

    We consider random walks on discrete state spaces, such as general undirected graphs, where the random walkers are designed to approximate a target quantity over the network topology via sampling and neighborhood exploration in the form of Markov chain Monte Carlo (MCMC) procedures. Given any Markov chain corresponding to a target probability distribution, we design a self-repellent random walk (SRRW) which is less likely to transition to nodes that were highly visited in the past, and more likely to transition to seldom visited nodes. For a class of SRRWs parameterized by a positive real {\alpha}, we prove that the empirical distribution of the process converges almost surely to the the target (stationary) distribution of the underlying Markov chain kernel. We then provide a central limit theorem and derive the exact form of the arising asymptotic co-variance matrix, which allows us to show that the SRRW with a stronger repellence (larger {\alpha}) always achieves a smaller asymptotic
    
[^89]: 表现性联邦学习：应对模型依赖和异构分布偏移的解决方案

    Performative Federated Learning: A Solution to Model-Dependent and Heterogeneous Distribution Shifts. (arXiv:2305.05090v1 [cs.LG])

    [http://arxiv.org/abs/2305.05090](http://arxiv.org/abs/2305.05090)

    本文提出了一种表现性联邦学习框架，通过引入分布映射来应对数据分布偏移，解决了模型依赖和客户端数据的异构分布。

    

    本文考虑了一个由多个客户端和一个服务器组成的联邦学习系统，其中客户端旨在从分布式数据中协作学习一个共同的决策模型。与传统的假设客户端数据静态的联邦学习框架不同，我们考虑了客户端的数据分布可能会被部署的决策模型重塑的情况。我们借鉴了表现性预测中的分布偏移映射的想法，将其形式化为模型依赖的数据分布偏移，并提出了一个表现性联邦学习框架。

    We consider a federated learning (FL) system consisting of multiple clients and a server, where the clients aim to collaboratively learn a common decision model from their distributed data. Unlike the conventional FL framework that assumes the client's data is static, we consider scenarios where the clients' data distributions may be reshaped by the deployed decision model. In this work, we leverage the idea of distribution shift mappings in performative prediction to formalize this model-dependent data distribution shift and propose a performative federated learning framework. We first introduce necessary and sufficient conditions for the existence of a unique performative stable solution and characterize its distance to the performative optimal solution. Then we propose the performative FedAvg algorithm and show that it converges to the performative stable solution at a rate of O(1/T) under both full and partial participation schemes. In particular, we use novel proof techniques and 
    
[^90]: 可约的双曲正切网络的功能等价和路径连通性

    Functional Equivalence and Path Connectivity of Reducible Hyperbolic Tangent Networks. (arXiv:2305.05089v1 [cs.NE])

    [http://arxiv.org/abs/2305.05089](http://arxiv.org/abs/2305.05089)

    本文对于单隐藏层双曲正切结构给出了单元冗余和可约的功能等价类的算法刻画，并证明了这样的功能等价类是分段线性路径连通的集合。

    

    理解人工神经网络的学习过程需要澄清学习发生的参数空间的结构。神经网络参数的功能等价类是实现相同输入输出函数的参数集合。然而，存在一小部分可约参数，其功能等价类由网络单元之间的冗余造成，因而具有更丰富的功能等价类。本文对于单隐藏层双曲正切结构给出了单元冗余和可约的功能等价类的算法刻画。我们证明了这样的功能等价类是分段线性路径连通的集合，并且对于具有多数冗余单元的参数，这些集合的直径最多为7线段。

    Understanding the learning process of artificial neural networks requires clarifying the structure of the parameter space within which learning takes place. A neural network parameter's functional equivalence class is the set of parameters implementing the same input--output function. For many architectures, almost all parameters have a simple and well-documented functional equivalence class. However, there is also a vanishing minority of reducible parameters, with richer functional equivalence classes caused by redundancies among the network's units.  In this paper, we give an algorithmic characterisation of unit redundancies and reducible functional equivalence classes for a single-hidden-layer hyperbolic tangent architecture. We show that such functional equivalence classes are piecewise-linear path-connected sets, and that for parameters with a majority of redundant units, the sets have a diameter of at most 7 linear segments.
    
[^91]: 健康保险索赔数据中历史偏差的大规模研究

    Large-Scale Study of Temporal Shift in Health Insurance Claims. (arXiv:2305.05087v1 [cs.LG])

    [http://arxiv.org/abs/2305.05087](http://arxiv.org/abs/2305.05087)

    本文是对健康保险索赔数据中历史偏差进行的大规模研究，通过构建算法测试时间偏移，进行回顾性扫描以寻找时间偏移，并创建1010个任务来评估242项医疗保健结果。研究发现有9.7%的任务显示出人群水平的时间偏移，93%显示出已发现的子人群内的时间偏移。

    

    多数用于预测临床结果的机器学习模型是通过历史数据开发的。然而，即使这些模型在不久的将来部署，数据集随时间发生偏移可能会导致不理想的表现。为了捕捉这种现象，我们认为在特定时间点预测的任务（即要预测的结果）是非平稳的，如果历史模型不再是预测该结果的最佳模型。我们构建了一个算法来测试人群水平或已发现的子人群内的时间偏移。然后，我们构建了一个元算法，在一组大量的任务中执行回顾性扫描以寻找时间偏移。根据我们所知，这是健康保健领域对历史偏差的首次全面评估。我们通过评估242项医疗保健结果从2015年到2020年的健康保险索赔数据集中的历史偏移来创建1010个任务。9.7%的任务显示出人群水平的时间偏移，93.0%显示出已发现的子人群内的时间偏移。

    Most machine learning models for predicting clinical outcomes are developed using historical data. Yet, even if these models are deployed in the near future, dataset shift over time may result in less than ideal performance. To capture this phenomenon, we consider a task--that is, an outcome to be predicted at a particular time point--to be non-stationary if a historical model is no longer optimal for predicting that outcome. We build an algorithm to test for temporal shift either at the population level or within a discovered sub-population. Then, we construct a meta-algorithm to perform a retrospective scan for temporal shift on a large collection of tasks. Our algorithms enable us to perform the first comprehensive evaluation of temporal shift in healthcare to our knowledge. We create 1,010 tasks by evaluating 242 healthcare outcomes for temporal shift from 2015 to 2020 on a health insurance claims dataset. 9.7% of the tasks show temporal shifts at the population level, and 93.0% ha
    
[^92]: 一种基于注意力机制的神经负载预测统一框架

    A Unifying Framework of Attention-based Neural Load Forecasting. (arXiv:2305.05082v1 [cs.LG])

    [http://arxiv.org/abs/2305.05082](http://arxiv.org/abs/2305.05082)

    本文提出了一种基于注意力机制的神经负载预测统一框架，包括时变特征加权、分层时间注意力和特征增强误差修正等模块，取得了优秀的预测性能。

    

    准确的负载预测对于电力网络的可靠和高效的规划和运营至关重要。本文提出了一种负载预测的统一深度学习框架，其中包括时变特征加权、分层时间注意力和特征增强误差修正。我们的框架采用模块化设计，具有良好的泛化能力。首先，特征加权机制将输入特征分配到不同的时间权重；其次，采用递归编码-解码结构和分层注意力进行负载预测，分层注意力能够进行相似日选择，以重新评估历史信息在每个时间步的重要性；第三，开发了一个误差修正模块，探索误差和学习特征隐藏信息以进一步提高模型的预测性能。实验结果表明，我们提出的框架在两个公共数据集上优于现有方法。

    Accurate load forecasting is critical for reliable and efficient planning and operation of electric power grids. In this paper, we propose a unifying deep learning framework for load forecasting, which includes time-varying feature weighting, hierarchical temporal attention, and feature-reinforced error correction. Our framework adopts a modular design with good generalization capability. First, the feature-weighting mechanism assigns input features with temporal weights. Second, a recurrent encoder-decoder structure with hierarchical attention is developed as a load predictor. The hierarchical attention enables a similar day selection, which re-evaluates the importance of historical information at each time step. Third, we develop an error correction module that explores the errors and learned feature hidden information to further improve the model's forecasting performance. Experimental results demonstrate that our proposed framework outperforms existing methods on two public dataset
    
[^93]: 大数据时代的地球移动者: 最优输运在机器学习中的回顾

    Earth Movers in The Big Data Era: A Review of Optimal Transport in Machine Learning. (arXiv:2305.05080v1 [cs.LG])

    [http://arxiv.org/abs/2305.05080](http://arxiv.org/abs/2305.05080)

    本文回顾了最优输运在机器学习中的应用，并探讨了如何将其扩展以适应大数据和高维数据的需求。

    

    最优输运(OT)是一个数学框架,首次出现于18世纪,并引发出大量方法来回答许多理论和应用问题。过去的十年见证了这个经典优化问题对机器学习的显着贡献。本文探讨了最优输运在机器学习中的使用方式及其扩展的问题。在专题与背景的允许下,我们提供了关于最优输运的全面调查,并确保其呈现具有可访问性。首先,我们解释了最优输运的背景,并介绍了不同的类型、特性和显著应用。然后,我们着重探讨了如何将最优输运扩展以应对当前大数据和高维数据的需求。我们对用于扩展OT的文献方法进行了系统分析,并以结构化的方式呈现结果以促进理解。最后,我们探讨了可扩展最优输运在机器学习中未来研究的一些最有前途的方向。

    Optimal Transport (OT) is a mathematical framework that first emerged in the eighteenth century and has led to a plethora of methods for answering many theoretical and applied questions. The last decade is a witness of the remarkable contributions of this classical optimization problem to machine learning. This paper is about where and how optimal transport is used in machine learning with a focus on the question of salable optimal transport. We provide a comprehensive survey of optimal transport while ensuring an accessible presentation as permitted by the nature of the topic and the context. First, we explain optimal transport background and introduce different flavors (i.e. mathematical formulations), properties, and notable applications. We then address the fundamental question of how to scale optimal transport to cope with the current demands of big and high dimensional data. We conduct a systematic analysis of the methods used in the literature for scaling OT and present the find
    
[^94]: 生成式检索推荐系统

    Recommender Systems with Generative Retrieval. (arXiv:2305.05065v1 [cs.IR])

    [http://arxiv.org/abs/2305.05065](http://arxiv.org/abs/2305.05065)

    本文提出了一种新型的生成式检索模型，将检索和生成组合在一起以产生推荐。

    

    现代推荐系统使用大规模检索模型进行推荐，包括两个阶段：训练双编码模型将查询和候选项嵌入到相同的空间中，然后使用近似最近邻搜索来选择给定查询嵌入的顶部候选项。本文提出了一种新的单阶段范例：生成式检索模型，该模型通过自回归方式在一个阶段中解码目标候选项的标识符。为此，我们不是为每个项目分配随机生成的原子ID，而是生成语义ID：每个项目的语义有意义的元组编码词，它作为其唯一标识符。我们使用称为RQ-VAE的分层方法生成这些编码词。一旦我们对所有项目都有了语义ID，就会训练基于Transformer的序列到序列模型来预测下一个项目的语义ID。由于这个模型以自回归的方式直接预测标识下一个项的编码词元组，因此它可以将检索和生成组合在一起以产生推荐。

    Modern recommender systems leverage large-scale retrieval models consisting of two stages: training a dual-encoder model to embed queries and candidates in the same space, followed by an Approximate Nearest Neighbor (ANN) search to select top candidates given a query's embedding. In this paper, we propose a new single-stage paradigm: a generative retrieval model which autoregressively decodes the identifiers for the target candidates in one phase. To do this, instead of assigning randomly generated atomic IDs to each item, we generate Semantic IDs: a semantically meaningful tuple of codewords for each item that serves as its unique identifier. We use a hierarchical method called RQ-VAE to generate these codewords. Once we have the Semantic IDs for all the items, a Transformer based sequence-to-sequence model is trained to predict the Semantic ID of the next item. Since this model predicts the tuple of codewords identifying the next item directly in an autoregressive manner, it can be c
    
[^95]: 基于大语言模型知识蒸馏的网络内容过滤方法

    Web Content Filtering through knowledge distillation of Large Language Models. (arXiv:2305.05027v1 [cs.LG])

    [http://arxiv.org/abs/2305.05027](http://arxiv.org/abs/2305.05027)

    本文提出了一种基于大语言模型知识蒸馏的 URL 分类方法，可用于网络内容过滤，其学生模型在参数数量减少 175 倍的情况下，精度提升了 9%，超过了当前最先进方法。

    

    本文提出了一种基于大语言模型的 URL 分类方法，旨在实现网络内容过滤的主要目标：保障组织免受法律和伦理风险，限制访问高风险或可疑网站，以及促进安全的专业工作环境。我们的方法利用大语言模型生成准确的分类，并利用已有的知识蒸馏技术创建更小、更专业的学生模型，以用于网络内容过滤。在将通过大型安全供应商收集的客户遥测数据的 30 个不同内容类别的网站进行分类的任务中，我们的学生模型通过蒸馏结果实现了 9% 的分类精度提升，超过了当前最先进方法。我们的学生模型在参数数量上与原始的大语言模型相比减少了 175 倍，从而达到了与老师模型相匹配的性能，可以用于大规模的在线扫描。

    We introduce a state-of-the-art approach for URL categorization that leverages the power of Large Language Models (LLMs) to address the primary objectives of web content filtering: safeguarding organizations from legal and ethical risks, limiting access to high-risk or suspicious websites, and fostering a secure and professional work environment. Our method utilizes LLMs to generate accurate classifications and then employs established knowledge distillation techniques to create smaller, more specialized student models tailored for web content filtering. Distillation results in a student model with a 9\% accuracy rate improvement in classifying websites, sourced from customer telemetry data collected by a large security vendor, into 30 distinct content categories based on their URLs, surpassing the current state-of-the-art approach. Our student model matches the performance of the teacher LLM with 175 times less parameters, allowing the model to be used for in-line scanning of large vo
    
[^96]: 低分辨率条件下的领域无关的图像翻译

    Domain Agnostic Image-to-image Translation using Low-Resolution Conditioning. (arXiv:2305.05023v1 [eess.IV])

    [http://arxiv.org/abs/2305.05023](http://arxiv.org/abs/2305.05023)

    本文提出了一种低分辨率条件下的领域无关的图像翻译方法，实现了源图像的可视特征与低分辨率目标图像的信息相结合，解决了领域相关的细粒度问题。

    

    图像翻译方法旨在学习跨领域的映射，假定用于翻译的图像共享内容，但具有自己的领域特定信息（即风格）。本文提出了一种领域无关的图像翻译方法，旨在解决细粒度问题，其中领域相关。具体而言，我们的领域无关方法旨在生成一幅图像，将源图像的可视特征与低频信息（例如姿势、颜色）相结合。我们提出了一种新的方法，通过训练生成模型来产生同时具有源图像的独特信息和低分辨率目标图像信息的图像。

    Generally, image-to-image translation (i2i) methods aim at learning mappings across domains with the assumption that the images used for translation share content (e.g., pose) but have their own domain-specific information (a.k.a. style). Conditioned on a target image, such methods extract the target style and combine it with the source image content, keeping coherence between the domains. In our proposal, we depart from this traditional view and instead consider the scenario where the target domain is represented by a very low-resolution (LR) image, proposing a domain-agnostic i2i method for fine-grained problems, where the domains are related. More specifically, our domain-agnostic approach aims at generating an image that combines visual features from the source image with low-frequency information (e.g. pose, color) of the LR target image. To do so, we present a novel approach that relies on training the generative model to produce images that both share distinctive information of 
    
[^97]: 基于图形U-Net的领域独立后处理：应用于电阻抗层析成像

    Domain independent post-processing with graph U-nets: Applications to Electrical Impedance Tomographic Imaging. (arXiv:2305.05020v1 [eess.IV])

    [http://arxiv.org/abs/2305.05020](http://arxiv.org/abs/2305.05020)

    本文提出了一种基于图形U-Net的领域独立后处理方法，可应用于电阻抗层析成像（EIT）中来提高重建效果，实现了对灵活几何形状的处理。

    

    从边界测量中重建层析图像需要灵活处理目标领域。例如，当系统方程由偏微分方程建模时，重建通常在有限元（FE）网格上完成，从而允许灵活的几何形状。因此，所得重建的任何处理最好也在FE网格上进行。为此，我们将非常成功的U-Net架构扩展到适用于灵活的FE网格。为了实现这一目的，将FE网格转换为图形，并在图形上制定具有新集群池和反池的图形U-Net，模拟基于最大池的经典邻域。我们展示了图形U-Net的有效性和灵活性，以改善从电阻抗层析（EIT）测量中得到的重建，这是一个非线性和高度病态的逆问题。性能进行了评估。

    Reconstruction of tomographic images from boundary measurements requires flexibility with respect to target domains. For instance, when the system equations are modeled by partial differential equations the reconstruction is usually done on finite element (FE) meshes, allowing for flexible geometries. Thus, any processing of the obtained reconstructions should be ideally done on the FE mesh as well. For this purpose, we extend the hugely successful U-Net architecture that is limited to rectangular pixel or voxel domains to an equivalent that works flexibly on FE meshes. To achieve this, the FE mesh is converted into a graph and we formulate a graph U-Net with a new cluster pooling and unpooling on the graph that mimics the classic neighborhood based max-pooling. We demonstrate effectiveness and flexibility of the graph U-Net for improving reconstructions from electrical impedance tomographic (EIT) measurements, a nonlinear and highly ill-posed inverse problem. The performance is evalua
    
[^98]: 不要盲目模仿老师：使用扰动损失进行知识蒸馏

    Do Not Blindly Imitate the Teacher: Using Perturbed Loss for Knowledge Distillation. (arXiv:2305.05010v1 [cs.LG])

    [http://arxiv.org/abs/2305.05010](http://arxiv.org/abs/2305.05010)

    本文提出了一种新的知识蒸馏目标函数PTLoss，通过扰动老师的输出分布，使其更接近真实标签分布，从而提高学生的性能。

    

    知识蒸馏是一种常用的技术，用于将大型教师模型的知识传输到小型学生模型中。通常，学生通过最小化其输出分布和教师的输出分布之间的KL散度来模仿教师。本文认为这种学习目标是次优的，因为教师的输出分布与地面真实标签分布存在差异。因此，强制学生盲目模仿不可靠的教师输出分布会导致性能下降。为此，我们提出了一种新的知识蒸馏目标函数PTLoss，首先通过Maclaurin级数表示香草KL蒸馏损失函数，然后扰动该级数中的主导项。这种扰动损失隐式地将原始老师转换为具有更接近地面真实分布的代理老师。我们建立了香草KL蒸馏和扰动KL蒸馏之间的理论联系。

    Knowledge distillation is a popular technique to transfer knowledge from large teacher models to a small student model. Typically, the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher's output distribution. In this work, we argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher's output distribution and the ground truth label distribution. Therefore, forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution. We establish the theoretical connection between
    
[^99]: 基于自编码器的 ICU 临床代码预测

    Autoencoder-based prediction of ICU clinical codes. (arXiv:2305.04992v1 [cs.LG])

    [http://arxiv.org/abs/2305.04992](http://arxiv.org/abs/2305.04992)

    本文研究了基于自编码器的 ICU 临床代码预测，针对不完整的临床代码清单，使用了各种自编码器方法以及两个强基准。结果表明，基于共现的方法表现略微更好，对抗自编码器实现了最佳性能。

    

    电子病历中诊断代码的可用性对于患者护理以及报销目的至关重要。然而，将其输入到电子病历中非常繁琐，而且一些临床代码可能会被忽略。针对不完整的临床代码清单，我们研究了机器学习方法在预测完整临床代码方面的性能，并评估了在这项任务中包含其他临床患者数据的增加预测价值。我们使用了 MIMIC-III 数据集，并将完整临床代码的任务框架定为推荐问题。我们考虑了各种自编码器方法以及两个强基准；项共现和奇异值分解（SVD）。输入包括 1）记录的已知临床代码，2）代码加变量。基于共现的方法略微表现更好（F1 分数=0.26，均值平均准确度[MAP]=0.19），而 SVD（F1=0.24，MAP=0.18）表现较差。然而，当代码加变量时，对抗自编码器实现了最佳性能。

    Availability of diagnostic codes in Electronic Health Records (EHRs) is crucial for patient care as well as reimbursement purposes. However, entering them in the EHR is tedious, and some clinical codes may be overlooked. Given an in-complete list of clinical codes, we investigate the performance of ML methods on predicting the complete ones, and assess the added predictive value of including other clinical patient data in this task. We used the MIMIC-III dataset and frame the task of completing the clinical codes as a recommendation problem. We con-sider various autoencoder approaches plus two strong baselines; item co-occurrence and Singular Value Decomposition (SVD). Inputs are 1) a record's known clinical codes, 2) the codes plus variables. The co-occurrence-based ap-proach performed slightly better (F1 score=0.26, Mean Average Precision [MAP]=0.19) than the SVD (F1=0.24, MAP=0.18). However, the adversarial autoencoder achieved the best performance when using the codes plus variable
    
[^100]: 解释性微调使模型对虚假提示更强韧

    Explanation-based Finetuning Makes Models More Robust to Spurious Cues. (arXiv:2305.04990v1 [cs.CL])

    [http://arxiv.org/abs/2305.04990](http://arxiv.org/abs/2305.04990)

    本文提出一种新型方法——解释性微调，通过让模型在给出答案的同时生成支持该答案的自由文本解释，来减轻LLMs依赖虚假关联，使得模型对虚假提示更加强韧，并具有广泛适用性。

    

    大型语言模型（LLMs）非常强大，有时会学习到标签和与任务无关的特征之间的相关性，导致在分布外数据上泛化能力差。我们提出解释性微调作为减轻LLMs依赖虚假关联的一种新的通用方法。与标准微调只在给定输入的情况下预测答案不同，我们微调模型以生成支持其答案的自由文本解释。为了评估我们的方法，我们在人工构建的训练集上微调模型，该训练集包含不同类型的虚假提示，并在没有这些提示的测试集上进行测试。与标准微调相比，我们的方法在四个分类任务的准确性下降方面使模型极其强韧：ComVE（+1.2），CREAK（+9.1），e-SNLI（+15.4）和SBIC（+6.5）。此外，我们的方法与模型生成的解释同样有效，这意味着我们的方法具有广泛的适用性。

    Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a novel and general approach to mitigate LLMs' reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer. To evaluate our method, we finetune the model on artificially constructed training sets containing different types of spurious cues, and test it on a test set without these cues. Compared to standard finetuning, our method makes models remarkably more robust against spurious cues in terms of accuracy drop across four classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). Moreover, our method works equally well with explanations generated by the model, implyin
    
[^101]: FedHB: 层次贝叶斯联邦学习

    FedHB: Hierarchical Bayesian Federated Learning. (arXiv:2305.04979v1 [cs.LG])

    [http://arxiv.org/abs/2305.04979](http://arxiv.org/abs/2305.04979)

    该论文提出了一种层次贝叶斯联邦学习方法，通过块坐标下降分布式算法实现对客户端私有数据不透露的学习，在收敛速度上与正则化相同。

    

    本文提出了一种新的层次贝叶斯联邦学习方法，通过层次贝叶斯建模合理地描述了客户端本地数据的生成过程：构成客户端本地模型的随机变量，由更高水平的全局变量进行控制。有趣的是，我们贝叶斯模型中的变分推断导致了一个优化问题，其块坐标下降求解成为一个可分客户端的分布式算法，这使得客户端完全不需要透露自己的私有数据，因此与联邦学习完全兼容。我们还强调，我们的块坐标算法具有特定形式，包括Fed-Avg和Fed-Prox在内的众所周知的FL算法都可以作为其特例进行子归。除了引入新的建模和导出之外，我们还提供了收敛性分析，表明我们的块坐标FL算法以$O(1/\sqrt{t})$的速度收敛到目标的（本地）最优解，这与正则化具有相同的速率。

    We propose a novel hierarchical Bayesian approach to Federated Learning (FL), where our model reasonably describes the generative process of clients' local data via hierarchical Bayesian modeling: constituting random variables of local models for clients that are governed by a higher-level global variate. Interestingly, the variational inference in our Bayesian model leads to an optimisation problem whose block-coordinate descent solution becomes a distributed algorithm that is separable over clients and allows them not to reveal their own private data at all, thus fully compatible with FL. We also highlight that our block-coordinate algorithm has particular forms that subsume the well-known FL algorithms including Fed-Avg and Fed-Prox as special cases. Beyond introducing novel modeling and derivations, we also offer convergence analysis showing that our block-coordinate FL algorithm converges to an (local) optimum of the objective at the rate of $O(1/\sqrt{t})$, the same rate as regul
    
[^102]: LABO: 通过双层优化实现最佳标签正则化学习

    LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization. (arXiv:2305.04971v1 [cs.LG])

    [http://arxiv.org/abs/2305.04971](http://arxiv.org/abs/2305.04971)

    本文提出了一种基于标签正则化的通用框架，其中包括传统的LS，但也可以建模实例特定的变体。我们提出了一种双层优化的方法（LABO），用于学习标签正则化，并得到了可解释的最优标签平滑解。

    

    正则化技术对于改善深度神经网络的泛化性能和训练效率至关重要。许多深度学习算法依赖于权重衰减、丢弃、批/层归一化等技术来更快地收敛和泛化。标签平滑（LS）是另一种简单、通用且高效的正则化方法，可用于各种监督分类任务。然而，传统的LS假设每个非目标类别出现的概率相等，不能根据实例对标签进行优化。本文提出了一种基于标签正则化的通用框架，包括传统的LS但也可以建模实例特定的变体。基于该框架，我们提出了一种通过设计双层优化（LABO）问题来学习标签正则化的高效方法。我们得出了内环节的确定性和可解释解，而无需存储经过训练模型的参数或输出。

    Regularization techniques are crucial to improving the generalization performance and training efficiency of deep neural networks. Many deep learning algorithms rely on weight decay, dropout, batch/layer normalization to converge faster and generalize. Label Smoothing (LS) is another simple, versatile and efficient regularization which can be applied to various supervised classification tasks. Conventional LS, however, regardless of the training instance assumes that each non-target class is equally likely. In this work, we present a general framework for training with label regularization, which includes conventional LS but can also model instance-specific variants. Based on this formulation, we propose an efficient way of learning LAbel regularization by devising a Bi-level Optimization (LABO) problem. We derive a deterministic and interpretable solution of the inner loop as the optimal label smoothing without the need to store the parameters or the output of a trained model. Finally
    
[^103]: 信用风险管理中的量化不确定性：一种深度证据回归方法

    UQ for Credit Risk Management: A deep evidence regression approach. (arXiv:2305.04967v1 [q-fin.RM])

    [http://arxiv.org/abs/2305.04967](http://arxiv.org/abs/2305.04967)

    本文扩展了Deep Evidence Regression方法，将其应用于预测信用风险中的违约损失；我们提供了相关的学习框架，并在模拟和实际数据上进行了验证。

    

    机器学习已经广泛应用于各种信用风险应用程序中。由于信用风险的固有性质，量化预测风险指标的不确定性是必要的，将考虑不确定性的深度学习模型应用于信用风险设置中非常有帮助。在本项工作中，我们探索了一种可扩展的UQ感知深度学习技术，Deep Evidence Regression，并将其应用于预测违约损失。我们通过将Deep Evidence Regression方法扩展到通过Weibull过程生成的目标变量的学习来为文献做出了贡献，并提供了相关的学习框架。我们展示了我们的方法在模拟和实际数据上的应用。

    Machine Learning has invariantly found its way into various Credit Risk applications. Due to the intrinsic nature of Credit Risk, quantifying the uncertainty of the predicted risk metrics is essential, and applying uncertainty-aware deep learning models to credit risk settings can be very helpful. In this work, we have explored the application of a scalable UQ-aware deep learning technique, Deep Evidence Regression and applied it to predicting Loss Given Default. We contribute to the literature by extending the Deep Evidence Regression methodology to learning target variables generated by a Weibull process and provide the relevant learning framework. We demonstrate the application of our approach to both simulated and real-world data.
    
[^104]: 从关系池化到子图图神经网络：更具表现力的图神经网络的通用框架

    From Relational Pooling to Subgraph GNNs: A Universal Framework for More Expressive Graph Neural Networks. (arXiv:2305.04963v1 [cs.LG])

    [http://arxiv.org/abs/2305.04963](http://arxiv.org/abs/2305.04963)

    本研究提出了一种从关系池化到子图GNN的通用框架，可提高任何基本GNN模型的表现力，通过明确标记节点作为附加特征来实现此目的， 并可在许多syn上实现超越性能。

    

    关系池化是用于构建更具表现力和置换不变的图神经网络的框架。然而，对于关系池化在表现力方面的确切增强及其与Weisfeiler-Lehman分层的联系的理解是有限的。从关系池化出发，我们提出了将节点明确标记为附加特征以提高消息传递神经网络的表现力的方法。然后将该方法扩展到高维WL，得到一种新的$k,l$-WL算法，比$k$-WL更通用的框架。我们在理论上分析了$k,l$-WL相对于$k$和$l$的表达能力，并将其与大量的子图GNN统一起来。我们还系统地讨论了复杂度降低方法，以构建强大而实用的$k,l$-GNN实例。我们在理论和实验中证明了我们的方法是通用兼容的，并能够提高任何基本GNN模型的表现力。我们的$k,l$-GNN在许多syn上实现了超越性能。

    Relational pooling is a framework for building more expressive and permutation-invariant graph neural networks. However, there is limited understanding of the exact enhancement in the expressivity of RP and its connection with the Weisfeiler Lehman hierarchy. Starting from RP, we propose to explicitly assign labels to nodes as additional features to improve expressive power of message passing neural networks. The method is then extended to higher dimensional WL, leading to a novel $k,l$-WL algorithm, a more general framework than $k$-WL. Theoretically, we analyze the expressivity of $k,l$-WL with respect to $k$ and $l$ and unifies it with a great number of subgraph GNNs. Complexity reduction methods are also systematically discussed to build powerful and practical $k,l$-GNN instances. We theoretically and experimentally prove that our method is universally compatible and capable of improving the expressivity of any base GNN model. Our $k,l$-GNNs achieve superior performance on many syn
    
[^105]: 基于自然语言查询的联合时刻检索和精华片段检测

    Joint Moment Retrieval and Highlight Detection Via Natural Language Queries. (arXiv:2305.04961v1 [cs.CV])

    [http://arxiv.org/abs/2305.04961](http://arxiv.org/abs/2305.04961)

    本文提出了一种基于自然语言查询的联合视频摘要和精华片段检测方法，利用视觉和音频线索匹配用户的查询，实现检索视频中最相关和有趣的时刻。

    

    随着互联网上视频内容的不断增多，视频摘要成为计算机视觉领域中日益重要的任务。在本项目中，我们提出了一种基于多模态转换器的自然语言查询联合视频摘要和精华片段检测新方法。该方法将使用视觉和音频线索来匹配用户的自然语言查询，以检索视频中最相关和有趣的时刻。我们的方法采用多个最近用于视觉转换器(ViTs)的技术，创建了一种类似于编码器-解码器的Transformer模型。我们还评估了我们的方法在多个数据集上的表现，例如YouTube Highlights和TVSum，以证明我们所提出的方法的灵活性。

    Video summarization has become an increasingly important task in the field of computer vision due to the vast amount of video content available on the internet. In this project, we propose a new method for natural language query based joint video summarization and highlight detection using multi-modal transformers. This approach will use both visual and audio cues to match a user's natural language query to retrieve the most relevant and interesting moments from a video. Our approach employs multiple recent techniques used in Vision Transformers (ViTs) to create a transformer-like encoder-decoder model. We evaluated our approach on multiple datasets such as YouTube Highlights and TVSum to demonstrate the flexibility of our proposed method.
    
[^106]: 早起的鸟儿捉到虫：利用编码器模型的早期层进行更有效的代码分类

    The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification. (arXiv:2305.04940v1 [cs.SE])

    [http://arxiv.org/abs/2305.04940](http://arxiv.org/abs/2305.04940)

    本文介绍了一种早期层组合的方法EarlyBIRD，该方法可以有效利用深度自然语言处理模型的资源和可用信息，从而提高代码分类的性能，在缺陷检测方面平均可提高2个点。

    

    现代自然语言处理技术在软件工程任务如漏洞检测和类型推理方面表现出了卓越的优势。然而，训练深度自然语言处理模型需要大量计算资源。本文探讨了一些技术，旨在实现这些模型中资源和可用信息的最佳利用。我们提出了一种通用的方法EarlyBIRD，从预训练的transformer模型的早期层构建代码的复合表示。我们通过比较12种创建复合表示的策略与仅使用最后一个编码器层的标准实践，在CodeBERT模型上实证研究了这种方法的可行性。我们在4个数据集上的评估表明，几个早期层的组合在缺陷检测方面产生更好的性能，而一些组合则改进了多类分类。具体而言，我们获得了平均检测增强2。

    The use of modern Natural Language Processing (NLP) techniques has shown to be beneficial for software engineering tasks, such as vulnerability detection and type inference. However, training deep NLP models requires significant computational resources. This paper explores techniques that aim at achieving the best usage of resources and available information in these models.  We propose a generic approach, EarlyBIRD, to build composite representations of code from the early layers of a pre-trained transformer model. We empirically investigate the viability of this approach on the CodeBERT model by comparing the performance of 12 strategies for creating composite representations with the standard practice of only using the last encoder layer.  Our evaluation on four datasets shows that several early layer combinations yield better performance on defect detection, and some combinations improve multi-class classification. More specifically, we obtain a +2 average improvement of detection 
    
[^107]: 应用基于生成式预训练自回归Transformer图神经网络的方法分析和发现新型蛋白质

    Generative Pretrained Autoregressive Transformer Graph Neural Network applied to the Analysis and Discovery of Novel Proteins. (arXiv:2305.04934v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.04934](http://arxiv.org/abs/2305.04934)

    本研究使用基于语言模型的深度学习策略，在蛋白质建模中应用transformer和图卷积的结构预训练生成模型，进一步训练后能够设计具有特定性质的蛋白质，案例验证表明该方法可生成理想目标性质的蛋白质。

    

    本文报道了一种灵活的基于语言模型的深度学习策略，应用于解决蛋白质建模中的正向和反向问题，使用一个整合了transformer和图卷积的注意力神经网络结构，在因果多头图机制中实现预训练生成模型。该模型被用于预测二级结构内容（每个残基的水平和总体内容）、蛋白质可溶性和测序任务。进一步在反向任务上训练，该模型能够设计具有这些性质作为目标特征的蛋白质。该模型被制定为一个通用的框架，完全基于提示，可以为各种下游任务进行适应。我们发现添加额外任务会产生相互协同作用，使模型在整体性能上得到提高，超过仅在每个数据集上训练模型的可能性。案例研究用于验证该方法，生成具有理想目标性质，包括稳定性和可溶性的蛋白质，并进行实验性研究。

    We report a flexible language-model based deep learning strategy, applied here to solve complex forward and inverse problems in protein modeling, based on an attention neural network that integrates transformer and graph convolutional architectures in a causal multi-headed graph mechanism, to realize a generative pretrained model. The model is applied to predict secondary structure content (per-residue level and overall content), protein solubility, and sequencing tasks. Further trained on inverse tasks, the model is rendered capable of designing proteins with these properties as target features. The model is formulated as a general framework, completely prompt-based, and can be adapted for a variety of downstream tasks. We find that adding additional tasks yields emergent synergies that the model exploits in improving overall performance, beyond what would be possible by training a model on each dataset alone. Case studies are presented to validate the method, yielding protein designs
    
[^108]: 机器学习中的不确定性量化在工程设计与健康预测中的应用：一篇教程

    Uncertainty Quantification in Machine Learning for Engineering Design and Health Prognostics: A Tutorial. (arXiv:2305.04933v1 [cs.LG])

    [http://arxiv.org/abs/2305.04933](http://arxiv.org/abs/2305.04933)

    本文教程介绍机器学习中的不确定性量化方法，帮助提升其安全性和可靠性，进而促进其在高风险决策背景下的广泛应用，特别关注神经网络及其在工程设计和预测健康管理中的应用。

    

    在机器学习模型中，不确定性量化（UQ）作为安全保障的基本层，可以通过启用合理的风险评估和管理来促进更加原则性的决策制定。UQ使得ML模型的安全性和可靠性得到改善，有潜力显著促进高风险决策背景下的ML解决方案的广泛采用，例如医疗保健，制造业和航空等。在本教程中，我们旨在全方位介绍机器学习模型中新兴UQ方法，特别关注神经网络，以及这些UQ方法在解决工程设计和预测健康管理问题方面的应用。为此，我们首先对涉及ML模型UQ的不确定性类型，来源和原因进行了全面分类。接下来，我们提供了几种最先进的UQ方法的教程式描述：高斯过程回归，贝叶斯神经网络。

    On top of machine learning models, uncertainty quantification (UQ) functions as an essential layer of safety assurance that could lead to more principled decision making by enabling sound risk assessment and management. The safety and reliability improvement of ML models empowered by UQ has the potential to significantly facilitate the broad adoption of ML solutions in high-stakes decision settings, such as healthcare, manufacturing, and aviation, to name a few. In this tutorial, we aim to provide a holistic lens on emerging UQ methods for ML models with a particular focus on neural networks and the applications of these UQ methods in tackling engineering design as well as prognostics and health management problems. Toward this goal, we start with a comprehensive classification of uncertainty types, sources, and causes pertaining to UQ of ML models. Next, we provide a tutorial-style description of several state-of-the-art UQ methods: Gaussian process regression, Bayesian neural network
    
[^109]: 高斯过程去卷积问题的贝叶斯非参数方法

    Gaussian process deconvolution. (arXiv:2305.04871v1 [stat.ML])

    [http://arxiv.org/abs/2305.04871](http://arxiv.org/abs/2305.04871)

    本文提出了一种基于高斯过程的贝叶斯非参数方法，可以解决连续时间信号的去卷积问题，适用于观测值中可能存在缺失数据且信号滤波器未知的情况。

    

    本文考虑去卷积问题，即从卷积处理的观测值 $\mathbf{y}$ 中恢复潜在信号 $x(\cdot)$，其中观测值 $\mathbf{y}$ 可能对应于 $y$ 的一部分缺失，滤波器 $h$ 可能未知且噪声 $\eta$ 可加性。当 $x$ 是连续时间信号时，我们采用高斯过程（GP）先验分布来解决这一问题，提出了一种闭合的贝叶斯非参数去卷积策略。我们首先分析了直接模型，以建立其良好定义的条件。然后，我们转向逆问题，研究了：（i）一些必要条件，使得贝叶斯去卷积计算有可能成立，以及（ii）在哪种程度上可以从数据中学习到滤波器 $h$，以及在盲去卷积情况下可以近似滤波器 $h$ 的程度。所提出的方法被称为高斯过程去卷积（GPDC），并与其他去卷积方法进行了比较。

    Let us consider the deconvolution problem, that is, to recover a latent source $x(\cdot)$ from the observations $\y = [y_1,\ldots,y_N]$ of a convolution process $y = x\star h + \eta$, where $\eta$ is an additive noise, the observations in $\y$ might have missing parts with respect to $y$, and the filter $h$ could be unknown. We propose a novel strategy to address this task when $x$ is a continuous-time signal: we adopt a Gaussian process (GP) prior on the source $x$, which allows for closed-form Bayesian nonparametric deconvolution. We first analyse the direct model to establish the conditions under which the model is well defined. Then, we turn to the inverse problem, where we study i) some necessary conditions under which Bayesian deconvolution is feasible, and ii) to which extent the filter $h$ can be learnt from data or approximated for the blind deconvolution case. The proposed approach, termed Gaussian process deconvolution (GPDC) is compared to other deconvolution methods concep
    
[^110]: 利用深度学习和数字孪生技术提高建筑物能源性能

    Leveraging Deep Learning and Digital Twins to Improve Energy Performance of Buildings. (arXiv:2305.04498v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04498](http://arxiv.org/abs/2305.04498)

    本研究提出了Deep Energy Twin的解决方案，将深度学习和数字孪生相结合，以识别建筑物能源使用模式并提供优化能源效率的洞见。

    

    建筑物数字化转型积累了大量运营数据，需要智能化解决方案来利用这些数据来提高能源性能。本研究提出了一种解决方案，即“Deep Energy Twin”，将深度学习和数字孪生相结合，以更好地理解建筑物能源使用情况，并识别提高能源效率的潜力。采用本体论创建参数数字孪生，以提供建筑物中不同系统之间数据格式的一致性。基于创建的数字孪生和收集的数据，使用深度学习方法进行数据分析，以识别模式并为能源优化提供洞见。作为示例，本研究在瑞典诺尔肯平的一座公共历史建筑中进行了案例研究，比较了最先进的深度学习架构在建筑物能源预测中的性能。

    Digital transformation in buildings accumulates massive operational data, which calls for smart solutions to utilize these data to improve energy performance. This study has proposed a solution, namely Deep Energy Twin, for integrating deep learning and digital twins to better understand building energy use and identify the potential for improving energy efficiency. Ontology was adopted to create parametric digital twins to provide consistency of data format across different systems in a building. Based on created digital twins and collected data, deep learning methods were used for performing data analytics to identify patterns and provide insights for energy optimization. As a demonstration, a case study was conducted in a public historic building in Norrk\"oping, Sweden, to compare the performance of state-of-the-art deep learning architectures in building energy forecasting.
    
[^111]: 基于无线通信的通道驱动随机梯度 Langevin 动力学贝叶斯联邦平均

    Bayesian Over-the-Air FedAvg via Channel Driven Stochastic Gradient Langevin Dynamics. (arXiv:2305.04152v1 [cs.LG])

    [http://arxiv.org/abs/2305.04152](http://arxiv.org/abs/2305.04152)

    本文提出了无线 FALD 协议，可以在无噪声通信的情况下高效地在无线系统中实现分布式贝叶斯学习，实现了在通信回合之间多个本地更新以及由小批量计算的随机梯度，并进行了样本收敛分析。

    

    可扩展贝叶斯推理方法的近期发展已经重新引起了对采用贝叶斯学习作为传统频率学习的替代方法的兴趣，其通过不确定性量化提供了改进的模型校准。最近，引入了联邦平均 Langevin 动力学(FALD)作为联邦平均的变体，可以在没有噪声的通信存在下有效地实现分布式贝叶斯学习。在本文中，我们提出了无线 FALD(WFALD)，这是一种新颖的协议，通过集成基于空中计算和基于通道驱动的 Monte Carlo 更新来实现无线系统中的 FALD。与先前的无线贝叶斯学习相比，WFALD 可以实现(i) 在通信回合之间多个本地更新；并且(ii) 由小批量计算的随机梯度。以 2-Wasserstein 距离为衡量标准，给出了样本收敛分析。

    The recent development of scalable Bayesian inference methods has renewed interest in the adoption of Bayesian learning as an alternative to conventional frequentist learning that offers improved model calibration via uncertainty quantification. Recently, federated averaging Langevin dynamics (FALD) was introduced as a variant of federated averaging that can efficiently implement distributed Bayesian learning in the presence of noiseless communications. In this paper, we propose wireless FALD (WFALD), a novel protocol that realizes FALD in wireless systems by integrating over-the-air computation and channel-driven sampling for Monte Carlo updates. Unlike prior work on wireless Bayesian learning, WFALD enables (\emph{i}) multiple local updates between communication rounds; and (\emph{ii}) stochastic gradients computed by mini-batch. A convergence analysis is presented in terms of the 2-Wasserstein distance between the samples produced by WFALD and the targeted global posterior distribut
    
[^112]: 离散扩散建模下的高效和度数引导图生成

    Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])

    [http://arxiv.org/abs/2305.04111](http://arxiv.org/abs/2305.04111)

    本文提出了EDGE，一种新的离散扩散模型用于生成大型图，并通过删除边来鼓励图的稀疏性。EDGE在每个去噪步骤中仅关注图中一部分节点，并且可以明确地对图的节点度数进行建模。实验表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。

    

    基于扩散的生成图模型已被证明在生成高质量小图方面非常有效。然而，它们需要更可扩展性，以生成包含数千个节点的大图并满足图统计。本文提出了EDGE，一种新的基于扩散的生成图模型，用于生成大型图的生成任务。为了提高计算效率，我们通过在每个时间步长随机删除边来鼓励图的稀疏性，并最终获得一张空白图。EDGE仅在每个去噪步骤中关注图中一部分节点。它比以前的基于扩散的模型更少地进行边预测。此外，EDGE明确地允许对图的节点度数进行建模，进一步提高了模型的性能。实证研究表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。它还在生成质量方面优于基准模型。

    Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation qual
    
[^113]: Diffusion Explainer：用于文本到图像稳定扩散的可视化解释工具

    Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])

    [http://arxiv.org/abs/2305.03509](http://arxiv.org/abs/2305.03509)

    Diffusion Explainer是第一个可交互的可视化工具，用于解释稳定扩散如何将文本提示转化为图像，用户可以通过动画和交互元素流畅地在多个抽象级别之间过渡，从而更好地理解提示对图像生成的影响。

    

    基于扩散的生成模型通过创造逼真的图像而获得了全球关注。然而，它们复杂的内部结构和操作往往使得非专业人员难以理解。我们提出了 Diffusion Explainer，这是第一个交互式可视化工具，用于解释稳定扩散如何将文本提示转化为图像。Diffusion Explainer紧密地将稳定扩散的复杂组件的视觉概述与其潜在操作的详细说明相结合，通过动画和交互元素使用户可以流畅地在多个抽象级别之间过渡。通过比较由两个相关文本提示引导的图像表示的演变来指导精细时间步长，用户可以发现提示对图像生成的影响。Diffusion Explainer在用户的Web浏览器中本地运行，无需安装或专门的硬件，扩大了公众对现代人工智能技术的教育获取。

    Diffusion-based generative models' impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern AI tec
    
[^114]: 学习在存在隐性混淆因素的情况下从不确定数据中恢复因果关系

    Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders. (arXiv:2305.02640v1 [cs.LG])

    [http://arxiv.org/abs/2305.02640](http://arxiv.org/abs/2305.02640)

    本文提出了因果强度变分模型，解决了从不确定数据中恢复因果关系存在的低样本利用率和分布假设无能力的问题，同时考虑了潜在混淆因素。

    

    在具有潜在变量的因果发现中，我们定义了两个数据范式：确定数据：具有观察节点单值的单个骨架结构，和不确定数据：具有观察节点多值的一组多骨架结构。多个骨架引入低样本利用率，多个值引入了分布假设的无能力，这两者导致从不确定数据中恢复因果关系至今仍然未被充分探索。我们设计了因果强度变分模型来解决这两个问题。具体地，我们利用因果强度而不是独立噪声作为潜变量来调节证据下界。通过这种设计思想，不同骨架的因果强度被看作是一个分布，并可以表示为单值因果图矩阵。此外，考虑到潜在混淆因素，我们将因果图G分解为两个相关子图O和C。O包含观察节点之间的纯关系，而C表示混淆因素。

    In Causal Discovery with latent variables, We define two data paradigms: definite data: a single-skeleton structure with observed nodes single-value, and indefinite data: a set of multi-skeleton structures with observed nodes multi-value. Multi,skeletons induce low sample utilization and multi values induce incapability of the distribution assumption, both leading that recovering causal relations from indefinite data is, as of yet, largely unexplored. We design the causal strength variational model to settle down these two problems. Specifically, we leverage the causal strength instead of independent noise as latent variable to mediate evidence lower bound. By this design ethos, The causal strength of different skeletons is regarded as a distribution and can be expressed as a single-valued causal graph matrix. Moreover, considering the latent confounders, we disentangle the causal graph G into two relatisubgraphs O and C. O contains pure relations between observed nodes, while C repres
    
[^115]: CSP：针对地理空间视觉表示的自监督对比空间预训练

    CSP: Self-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Representations. (arXiv:2305.01118v1 [cs.CV])

    [http://arxiv.org/abs/2305.01118](http://arxiv.org/abs/2305.01118)

    CSP提出了一个自监督对比学习框架，通过利用地理信息，学习地理位置的有效表示，进一步提高了模型在大规模地理标记图像上的表现。

    

    大量的地理标记图像公开可用，而对象类别等标签则相对稀缺且收集成本高昂。同时，对比学习在各种自然图像和语言任务中取得了巨大成功，仅需很少的带标签数据。然而，现有的方法未能充分利用地理空间信息，这可能是区分视觉上相似的对象的关键。为了在预训练、微调和推理阶段直接利用与图像相关的丰富地理空间信息，我们提出了针对地理标记图像的自监督学习框架 Contrastive Spatial Pre-Training（CSP）。我们使用双编码器分别对图像及其对应的地理位置进行编码，利用对比目标从图像中学习有效的位置表示，这些表示可以转移到下游监督任务，例如图像分类。实验证明，CSP可以提高模型在大规模地理标记图像上的性能。

    Geo-tagged images are publicly available in large quantities, whereas labels such as object classes are rather scarce and expensive to collect. Meanwhile, contrastive learning has achieved tremendous success in various natural image and language tasks with limited labeled data. However, existing methods fail to fully leverage geospatial information, which can be paramount to distinguishing objects that are visually similar. To directly leverage the abundant geospatial information associated with images in pre-training, fine-tuning, and inference stages, we present Contrastive Spatial Pre-Training (CSP), a self-supervised learning framework for geo-tagged im- ages. We use a dual-encoder to separately encode the images and their corresponding geo-locations, and use contrastive objectives to learn effective location representations from images, which can be transferred to downstream supervised tasks such as image classification. Experiments show that CSP can improve model performance on b
    
[^116]: 大纲先行，细节后至：基于语法引导的粗-细代码生成

    Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation. (arXiv:2305.00909v2 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2305.00909](http://arxiv.org/abs/2305.00909)

    提出一种基于语法引导的粗-细代码生成模型，支持从粗到细的多次迭代，实现了更加符合人脑思维方式的代码编写方式。

    

    对于一个复杂算法的实现，人类程序员的做法通常是先概述一下控制流程，然后迭代进行丰富，最终生成一些精心加工的语法结构和层次变量。然而，现有的大型语言模型一次性生成代码，没有中间环节，以反映"大纲先行，细节后至"的结构化思维过程。受到思维链提示的最新成功启发，我们提出了ChainCoder，这是一种程序综合语言模型，它逐步生成Python代码，即从粗到细进行多次迭代。我们首先通过抽象语法树解析将源代码分解为布局框架组件和附件组件，以构建层次表示。然后我们将预测目标重新启动，形成多次通过目标，每次生成一个子序列，这些子序列在层次结构中串联起来。最后，我们利用量身定制的Transformer体系结构来实现模型的优化。

    For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. However, state-of-the-art large language models generate codes in a single pass, without intermediate warm-ups to reflect the structured thought process of "outline-then-detail". Inspired by the recent success of chain-of-thought prompting, we propose ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes. We first decompose source code into layout frame components and accessory components via abstract syntax tree parsing to construct a hierarchical representation. We then reform our prediction target into a multi-pass objective, each pass generates a subsequence, which is concatenated in the hierarchy. Finally, a tailored transformer architecture is leveraged to joi
    
[^117]: 激活函数不再激活：神经网络解释的合理理论

    Activation Functions Not To Active: A Plausible Theory on Interpreting Neural Networks. (arXiv:2305.00663v1 [cs.LG])

    [http://arxiv.org/abs/2305.00663](http://arxiv.org/abs/2305.00663)

    本文提供了一个合理的理论来解释神经网络的高维空间，并且将激活函数的角色描述为放大函数，将低维线性空间映射为无限维超级空间。

    

    研究人员普遍认为神经网络模拟一个高维空间，但却不能清晰地定义这个空间。那么这个空间是什么？它的维数是多少？是否具有有限的维数？本文开发了一个关于激活函数在神经网络中作用的可行理论，定义了一个高维（更精确地，是一个无限维）的空间。我们认为激活函数充当了一个放大函数的作用，将低维线性空间映射成了一个无限维的空间。

    Researchers commonly believe that neural networks model a high-dimensional space but cannot give a clear definition of this space. What is this space? What is its dimension? And does it has finite dimensions? In this paper, we develop a plausible theory on interpreting neural networks in terms of the role of activation functions in neural networks and define a high-dimensional (more precisely, an infinite-dimensional) space. We conjunction that the activation function acts as a magnifying function that maps the low-dimensional linear space into an infinite-dimensional space. Given a dataset with each example of $d$ features $f_1$, $f_2$, $\cdots$, $f_d$, we believe that NNs model a special space with infinite dimensions, each of which is a monomial $$\prod_{i_1, i_2, \cdots, i_d} f_1^{i_1} f_2^{i_2} \cdots f_d^{i_d}$$ for some non-negative integers ${i_1, i_2, \cdots, i_d} \in \mathbb{Z}_{0}^{+}=\{0,1,2,3,\ldots\} $. We term such an infinite-dimensional space $\textit{ Super Space (SS)
    
[^118]: 使用模糊分箱进行校准误差估计

    Calibration Error Estimation Using Fuzzy Binning. (arXiv:2305.00543v1 [cs.LG])

    [http://arxiv.org/abs/2305.00543](http://arxiv.org/abs/2305.00543)

    本文提出了一种模糊校准误差度量（FCE），利用模糊分箱方法计算校准误差，从而缓解了概率偏斜的影响并提供了更紧密的估计值。与传统指标ECE相比，FCE在多类设置中表现更好，https://github.com/srdgFHE/FCE-paper。

    

    基于神经网络的决策往往会过于自信，其原始结果的概率并不符合真实的决策概率。神经网络的校准是实现更可靠的深度学习框架的关键步骤。先前的校准误差度量主要利用清晰的分箱成员资格度量。这加剧了模型概率的偏斜，并描绘了校准误差的不完整图像。在本文中，我们提出了一种利用模糊分箱方法计算校准误差的模糊校准误差度量（FCE）。这种方法缓解了概率偏斜的影响，并在测量校准误差时提供了更紧密的估计值。我们比较了我们的指标与ECE在不同的数据群体和类别成员身份中的表现。我们的结果显示，FCE在校准误差估计方面表现更好，特别是在多类设置中，缓解了模型置信度分数偏斜对校准误差估计的影响。我们提供了我们的代码https://github.com/srdgFHE/FCE-paper，以便未来的可重复性和使用FCE进行校准误差估计。

    Neural network-based decisions tend to be overconfident, where their raw outcome probabilities do not align with the true decision probabilities. Calibration of neural networks is an essential step towards more reliable deep learning frameworks. Prior metrics of calibration error primarily utilize crisp bin membership-based measures. This exacerbates skew in model probabilities and portrays an incomplete picture of calibration error. In this work, we propose a Fuzzy Calibration Error metric (FCE) that utilizes a fuzzy binning approach to calculate calibration error. This approach alleviates the impact of probability skew and provides a tighter estimate while measuring calibration error. We compare our metric with ECE across different data populations and class memberships. Our results show that FCE offers better calibration error estimation, especially in multi-class settings, alleviating the effects of skew in model confidence scores on calibration error estimation. We make our code a
    
[^119]: 在有限制的多目标联邦学习中优化隐私、效用和效率

    Optimizing Privacy, Utility and Efficiency in Constrained Multi-Objective Federated Learning. (arXiv:2305.00312v1 [cs.LG])

    [http://arxiv.org/abs/2305.00312](http://arxiv.org/abs/2305.00312)

    该论文提供了一种在有限制的多目标联邦学习中优化隐私、效用和效率的方法，开发了两种改进的算法来解决隐私泄露、效用损失和训练成本等三个主要目标，并在两个真实世界的数据集上进行了实验验证，优于现有方法。

    

    传统上，联邦学习旨在优化单个目标，通常是效用。然而，为了使联邦学习系统值得信赖，它需要同时满足多个/多个目标，例如最大化模型性能、最小化隐私泄露和训练成本，并对恶意攻击具有鲁棒性。多目标优化（MOO）旨在同时优化多个相互冲突的目标，非常适合解决值得信赖的联合学习（TFL）的优化问题。在本文中，我们将MOO和TFL统一起来，通过制定约束的多目标联合学习（CMOFL）问题来解决此问题。在这种制定下，现有的MOO算法可以直接适用于TFL。不同于现有的CMOFL作品专注于效用、效率、公平性和鲁棒性，我们考虑优化隐私泄露以及效用损失和训练成本，这是TFL系统的三个主要目标之一。我们开发了两种改进的CMOFL算法，它们返回一组平衡良好的模型，满足隐私、效用和效率。基于两个真实世界的数据集的实验表明，我们的方法在隐私、效用和效率之间的权衡方面优于现有方法。

    Conventionally, federated learning aims to optimize a single objective, typically the utility. However, for a federated learning system to be trustworthy, it needs to simultaneously satisfy multiple/many objectives, such as maximizing model performance, minimizing privacy leakage and training cost, and being robust to malicious attacks. Multi-Objective Optimization (MOO) aiming to optimize multiple conflicting objectives at the same time is quite suitable for solving the optimization problem of Trustworthy Federated Learning (TFL). In this paper, we unify MOO and TFL by formulating the problem of constrained multi-objective federated learning (CMOFL). Under this formulation, existing MOO algorithms can be adapted to TFL straightforwardly. Different from existing CMOFL works focusing on utility, efficiency, fairness, and robustness, we consider optimizing privacy leakage along with utility loss and training cost, the three primary objectives of a TFL system. We develop two improved CMOF
    
[^120]: 论RemOve-And-Retrain的陷阱：数据处理不等式的视角

    On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective. (arXiv:2304.13836v1 [cs.LG])

    [http://arxiv.org/abs/2304.13836](http://arxiv.org/abs/2304.13836)

    本论文评估了RemOve-And-Retrain（ROAR）协议的可靠性。研究结果表明，ROAR基准测试中的属性可能有更少的有关决策的重要信息，这种偏差称为毛糙度偏差，并提醒人们不要在ROAR指标上进行盲目的依赖。

    

    本文评估了RemOve-And-Retrain（ROAR）协议的可靠性，该协议用于测量特征重要性估计的性能。我们从理论背景和实证实验中发现，具有较少有关决策功能的信息的属性在ROAR基准测试中表现更好，与ROAR的原始目的相矛盾。这种现象也出现在最近提出的变体RemOve-And-Debias（ROAD）中，我们提出了ROAR归因度量中毛糙度偏差的一致趋势。我们的结果提醒人们不要盲目依赖ROAR的性能评估指标。

    This paper assesses the reliability of the RemOve-And-Retrain (ROAR) protocol, which is used to measure the performance of feature importance estimates. Our findings from the theoretical background and empirical experiments indicate that attributions that possess less information about the decision function can perform better in ROAR benchmarks, conflicting with the original purpose of ROAR. This phenomenon is also observed in the recently proposed variant RemOve-And-Debias (ROAD), and we propose a consistent trend of blurriness bias in ROAR attribution metrics. Our results caution against uncritical reliance on ROAR metrics.
    
[^121]: 使用中间层扰动衰减来提高敌对迁移能力

    Improving Adversarial Transferability by Intermediate-level Perturbation Decay. (arXiv:2304.13410v1 [cs.LG])

    [http://arxiv.org/abs/2304.13410](http://arxiv.org/abs/2304.13410)

    本文提出了一种名为中间层扰动衰减（ILPD）的新型中间层方法，可以通过单个优化阶段制作可转移的对抗性样本，并在过程中鼓励中间层扰动处于有效的敌对方向，并同时具有很大的幅度。

    

    中间层攻击是指通过遵循对抗方向，尝试扰动特征表示的攻击方法，在制作可转移的对抗样本中展现出良好的性能。现有的这类攻击方法通常由两个分离的阶段构成，首先需要确定一个方向向导，然后扩大中间层扰动对该方向向导的标量投影。然而，得到的扰动在特征空间中难免会偏离向导。本文发现，这种偏离可能导致次优的攻击效果。为了解决这个问题，我们开发了一种名为中间层扰动衰减（ILPD）的新型中间层方法，可以通过单个优化阶段制作可转移的对抗性样本。具体来说，该方法鼓励中间层扰动处于有效的敌对方向，并同时具有很大的幅度。

    Intermediate-level attacks that attempt to perturb feature representations following an adversarial direction drastically have shown favorable performance in crafting transferable adversarial examples. Existing methods in this category are normally formulated with two separate stages, where a directional guide is required to be determined at first and the scalar projection of the intermediate-level perturbation onto the directional guide is enlarged thereafter. The obtained perturbation deviates from the guide inevitably in the feature space, and it is revealed in this paper that such a deviation may lead to sub-optimal attack. To address this issue, we develop a novel intermediate-level method that crafts adversarial examples within a single stage of optimization. In particular, the proposed method, named intermediate-level perturbation decay (ILPD), encourages the intermediate-level perturbation to be in an effective adversarial direction and to possess a great magnitude simultaneous
    
[^122]: 使用Pylogik进行医学影像去标识化和清洗压缩

    Medical Image Deidentification, Cleaning and Compression Using Pylogik. (arXiv:2304.12322v1 [eess.IV])

    [http://arxiv.org/abs/2304.12322](http://arxiv.org/abs/2304.12322)

    提出了一个Python框架下的库PyLogik来帮助超声图像去标识化和清洗压缩，为深度学习和数据共享应用提供图像数据支持。

    

    应用大数据和机器学习在医疗记录信息方面须注意，必须清洗和去标识化数据。当受保护的健康信息嵌入在影像元数据中时，促进多中心合作中数据共享和协调变得尤其困难。我们提出了一个新的Python框架下的库，称为PyLogik，帮助解决超声图像特别具有挑战性的数据清洗问题，因为这些图像直接包含很多PHI。PyLogik通过一系列的文本检测/提取、过滤、阈值化、形态学和轮廓比较处理图像体积。这种方法去标识化图像，减小文件大小，并为深度学习和数据共享应用准备好了图像数据。为了评估PyLogik在兴趣区域（ROI）的识别有效性，随机抽取了50张心脏超声图像（超声心动图）进行处理。

    Leveraging medical record information in the era of big data and machine learning comes with the caveat that data must be cleaned and deidentified. Facilitating data sharing and harmonization for multi-center collaborations are particularly difficult when protected health information (PHI) is contained or embedded in image meta-data. We propose a novel library in the Python framework, called PyLogik, to help alleviate this issue for ultrasound images, which are particularly challenging because of the frequent inclusion of PHI directly on the images. PyLogik processes the image volumes through a series of text detection/extraction, filtering, thresholding, morphological and contour comparisons. This methodology deidentifies the images, reduces file sizes, and prepares image volumes for applications in deep learning and data sharing. To evaluate its effectiveness in the identification of regions of interest (ROI), a random sample of 50 cardiac ultrasounds (echocardiograms) were processed
    
[^123]: 无需反向传播的深度物理神经网络训练

    Backpropagation-free Training of Deep Physical Neural Networks. (arXiv:2304.11042v1 [cs.LG])

    [http://arxiv.org/abs/2304.11042](http://arxiv.org/abs/2304.11042)

    该论文提出了一种新方法来训练深度学习模型，不需要使用反向传播算法。该方法可以有效地应用于基于物理系统的深度学习。

    

    近年来，深度学习在诸如视觉和自然语言处理等各个领域取得了杰出的成功。这一成功很大程度上归功于深度学习模型的大规模，预计会不断增加。这种深度学习模型的增长伴随着与其可扩展性和训练、推理阶段中的能耗等问题相关的问题。虽然已经提出了一些基于非传统物理系统的工作来解决推理阶段的能效问题，但深度学习模型的有效训练仍未得到解决。迄今为止，数字深度学习模型的训练主要依赖于反向传播，但这种方法不适用于物理实现，因为它需要完全了解所谓前向传递的计算。在这里，我们通过提出一种简单的深度神经网络结构来解决这个问题。

    Recent years have witnessed the outstanding success of deep learning in various fields such as vision and natural language processing. This success is largely indebted to the massive size of deep learning models that is expected to increase unceasingly. This growth of the deep learning models is accompanied by issues related to their considerable energy consumption, both during the training and inference phases, as well as their scalability. Although a number of work based on unconventional physical systems have been proposed which addresses the issue of energy efficiency in the inference phase, efficient training of deep learning models has remained unaddressed. So far, training of digital deep learning models mainly relies on backpropagation, which is not suitable for physical implementation as it requires perfect knowledge of the computation performed in the so-called forward pass of the neural network. Here, we tackle this issue by proposing a simple deep neural network architectur
    
[^124]: PED-ANOVA: 在任意子空间中高效量化超参数重要性

    PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces. (arXiv:2304.10255v1 [cs.LG])

    [http://arxiv.org/abs/2304.10255](http://arxiv.org/abs/2304.10255)

    PED-ANOVA 提出了一个新的 f-ANOVA 公式，能够在任意子空间中高效地计算超参数的重要性，有助于深度学习中好的超参数空间设计。

    

    深度学习中超参数优化的流行使得好的超参数空间设计对于训练强模型至关重要，而好的超参数空间设计又严重依赖于了解不同超参数的作用。这激发了关于超参数重要性的研究，例如使用功能方差分析 (f-ANOVA) 的流行方法。然而，原始的 f-ANOVA 公式不适用于算法设计师最相关的子空间，例如由最佳性能定义的子空间。为了解决这个问题，我们推导了一个新的针对任意子空间的 f-ANOVA 公式，并提出了一个算法，使用 Pearson 散度 (PED) 实现超参数重要性的闭式计算。我们证明，这个新算法，称为 PED-ANOVA，能够成功地识别不同子空间中重要的超参数，同时计算效率极高。

    The recent rise in popularity of Hyperparameter Optimization (HPO) for deep learning has highlighted the role that good hyperparameter (HP) space design can play in training strong models. In turn, designing a good HP space is critically dependent on understanding the role of different HPs. This motivates research on HP Importance (HPI), e.g., with the popular method of functional ANOVA (f-ANOVA). However, the original f-ANOVA formulation is inapplicable to the subspaces most relevant to algorithm designers, such as those defined by top performance. To overcome this problem, we derive a novel formulation of f-ANOVA for arbitrary subspaces and propose an algorithm that uses Pearson divergence (PED) to enable a closed-form computation of HPI. We demonstrate that this new algorithm, dubbed PED-ANOVA, is able to successfully identify important HPs in different subspaces while also being extremely computationally efficient.
    
[^125]: BanditQ - 在敌对环境中保证用户每次奖励的无悔学习

    BanditQ -- No-Regret Learning with Guaranteed Per-User Rewards in Adversarial Environments. (arXiv:2304.05219v1 [cs.LG])

    [http://arxiv.org/abs/2304.05219](http://arxiv.org/abs/2304.05219)

    提出了一种名为BanditQ的新的在线预测策略，在敌对设置中以公平的方式达到目标速率约束，并实现了$O(T^{3/4})$的遗憾。

    

    经典的在线预测算法如Hedge在设计上具有不公平性，因为它们尝试尽可能多地玩最具回报的臂而忽略次优臂，以实现亚线性遗憾。本文考虑在具有对所有臂累积奖励速率下界的敌对设置中，以公平的在线预测问题。通过将基本排队论与在线学习相结合，我们提出了一种名为BanditQ的新的在线预测策略，它在全信息设置下实现了目标速率约束，并实现了$O(T^{3/4})$的遗憾。BanditQ的设计和分析涉及潜在函数方法的新颖应用，并具有独立的兴趣。

    Classic online prediction algorithms, such as Hedge, are inherently unfair by design, as they try to play the most rewarding arm as many times as possible while ignoring the sub-optimal arms to achieve sublinear regret. In this paper, we consider a fair online prediction problem in the adversarial setting with hard lower bounds on the rate of accrual of rewards for all arms. By combining elementary queueing theory with online learning, we propose a new online prediction policy, called BanditQ, that achieves the target rate constraints while achieving a regret of $O(T^{3/4})$ in the full-information setting. The design and analysis of BanditQ involve a novel use of the potential function method and are of independent interest.
    
[^126]: 在恒星变异存在下，基于深度学习的行星径向速度测量方法

    Deep-learning based measurement of planetary radial velocities in the presence of stellar variability. (arXiv:2304.04807v1 [astro-ph.EP])

    [http://arxiv.org/abs/2304.04807](http://arxiv.org/abs/2304.04807)

    本文提出了一种基于深度学习的方法，使用神经网络来减少三年HARPS-N太阳-星形光谱中的恒星径向速度抖动。该方法能够以前无法想象的小行星径向速度检测精度，为缓解恒星径向速度变异提供了希望。

    

    本文提出了一种基于深度学习的方法，用于在恒星变异存在下测量小行星的径向速度。我们使用神经网络来减少三年HARPS-N太阳-星形光谱中的恒星径向速度抖动。我们开发并比较了不同的降维和数据分割方法，以及各种神经网络体系结构，包括单线CNN、单线CNN集合和多线CNN。我们将类似于行星的径向速度注入光谱中并使用网络恢复它们。我们发现，多线CNN能够恢复0.2m/s半振幅、50天周期的行星，其振幅误差为8.8％，周期误差为0.7％。这种方法展示了在缓解恒星径向速度变异的同时，实现了以前无法想象的小行星径向速度检测精度的承诺。

    We present a deep-learning based approach for measuring small planetary radial velocities in the presence of stellar variability. We use neural networks to reduce stellar RV jitter in three years of HARPS-N sun-as-a-star spectra. We develop and compare dimensionality-reduction and data splitting methods, as well as various neural network architectures including single line CNNs, an ensemble of single line CNNs, and a multi-line CNN. We inject planet-like RVs into the spectra and use the network to recover them. We find that the multi-line CNN is able to recover planets with 0.2 m/s semi-amplitude, 50 day period, with 8.8% error in the amplitude and 0.7% in the period. This approach shows promise for mitigating stellar RV variability and enabling the detection of small planetary RVs with unprecedented precision.
    
[^127]: BloombergGPT：金融领域的大型语言模型

    BloombergGPT: A Large Language Model for Finance. (arXiv:2303.17564v1 [cs.LG])

    [http://arxiv.org/abs/2303.17564](http://arxiv.org/abs/2303.17564)

    本文提出了BloombergGPT，一个500亿参数的金融领域的大型语言模型，其基于Bloomberg的广泛数据来源和通用数据集进行训练。通过混合数据集训练，该模型在金融任务上表现出色，并且不会牺牲在普通任务上的性能。

    

    自然语言处理在金融技术领域有着广泛而复杂的应用，从情感分析和命名实体识别到问答。大型语言模型（LLM）已被证明在各种任务上非常有效；然而，专为金融领域设计的LLM尚未在文献中报告。在本文中，我们提出了BloombergGPT，一个拥有500亿个参数的语言模型，它是基于广泛的金融数据进行训练的。我们构建了一种3630亿个标记的数据集，该数据集基于彭博社的广泛数据来源，可能是迄今最大的领域特定数据集，同时又增加了来自通用数据集的3450亿个标记。我们在标准LLM基准、开放式金融基准和一套最能准确反映我们预期用途的内部基准上验证了BloombergGPT。我们的混合数据集训练产生了一个在金融任务上明显优于现有模型的模型，同时不会牺牲普通任务的性能。

    The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general 
    
[^128]: 利用长短期记忆网络提高量子电路保真度

    Quantum Circuit Fidelity Improvement with Long Short-Term Memory Networks. (arXiv:2303.17523v1 [quant-ph])

    [http://arxiv.org/abs/2303.17523](http://arxiv.org/abs/2303.17523)

    本文提出使用长短期记忆网络解决量子计算中的保真度问题，利用时间序列预测方法预测量子电路的保真度。

    

    量子计算已进入噪声中间规模量子（NISQ）时代，目前我们拥有的量子处理器对辐射和温度等环境变量敏感，因此会产生嘈杂的输出。虽然已经有许多算法和应用程序用于NISQ处理器，但我们仍面临着解释其嘈杂结果的不确定性。具体来说，我们对所选择的量子态有多少信心？这种信心很重要，因为NISQ计算机将输出其量子位测量的概率分布，有时很难区分分布是否表示有意义的计算或只是随机噪声。本文提出了一种新方法来解决这个问题，将量子电路保真度预测框架为时间序列预测问题，因此可以利用长短期记忆（LSTM）神经网络的强大能力。一个完整的工作流程来构建训练电路

    Quantum computing has entered the Noisy Intermediate-Scale Quantum (NISQ) era. Currently, the quantum processors we have are sensitive to environmental variables like radiation and temperature, thus producing noisy outputs. Although many proposed algorithms and applications exist for NISQ processors, we still face uncertainties when interpreting their noisy results. Specifically, how much confidence do we have in the quantum states we are picking as the output? This confidence is important since a NISQ computer will output a probability distribution of its qubit measurements, and it is sometimes hard to distinguish whether the distribution represents meaningful computation or just random noise. This paper presents a novel approach to attack this problem by framing quantum circuit fidelity prediction as a Time Series Forecasting problem, therefore making it possible to utilize the power of Long Short-Term Memory (LSTM) neural networks. A complete workflow to build the training circuit d
    
[^129]: 对称正定矩阵上的自适应黎曼度量

    Adaptive Riemannian Metrics on SPD Manifolds. (arXiv:2303.15477v1 [cs.LG])

    [http://arxiv.org/abs/2303.15477](http://arxiv.org/abs/2303.15477)

    本文提出了自适应黎曼度量来改进SPD神经网络的次优性能，实验结果表明该度量能使网络表现更好。

    

    由于其内在能够编码数据中的潜在结构相关性，对称正定（SPD）矩阵在机器学习中受到广泛关注。为了反映SPD流形的非欧几里得几何，已经提出了许多成功的黎曼度量。然而，现有的固定度量张量可能会导致SPD矩阵学习的次优性能，特别是对于SPD神经网络。为了解决这个限制，我们利用拉回的思想，提出了自适应SPD流形的黎曼度量。此外，我们还对我们的度量提出了全面的理论。三个数据集上的实验表明，配备了我们提出的度量的SPD网络可以展现出优越的性能。

    Symmetric Positive Definite (SPD) matrices have received wide attention in machine learning due to their intrinsic capacity of encoding underlying structural correlation in data. To reflect the non-Euclidean geometry of SPD manifolds, many successful Riemannian metrics have been proposed. However, existing fixed metric tensors might lead to sub-optimal performance for SPD matrices learning, especially for SPD neural networks. To remedy this limitation, we leverage the idea of pullback and propose adaptive Riemannian metrics for SPD manifolds. Moreover, we present comprehensive theories for our metrics. Experiments on three datasets demonstrate that equipped with the proposed metrics, SPD networks can exhibit superior performance.
    
[^130]: 面向心电图和脑电图分类的领域泛化：算法和基准

    Towards Domain Generalization for ECG and EEG Classification: Algorithms and Benchmarks. (arXiv:2303.11338v1 [eess.SP])

    [http://arxiv.org/abs/2303.11338](http://arxiv.org/abs/2303.11338)

    本论文提出了一个开源生物信号领域泛化评估基准，并引入一种专门解决生物信号中领域泛化问题的神经网络架构DGNet-Bio。通过实验证明，DGNet-Bio在ECG和EEG分类领域泛化上优于现有方法。

    

    尽管机器和深度学习系统在许多领域取得了巨大的成功，但它们尚未能够在医疗保健的关键任务中牢固地确立自己。其中一个主要原因在于，当模型面对之前未见过的分布之外的样本时，它们的性能会显著下降。这就是领域泛化（DG）问题。我们的目标是提出一个评估DG算法的基准，并引入一种新颖的架构来解决生物信号分类中的DG问题。在本文中，我们描述了生物信号的领域泛化问题，重点关注心电图（ECG）和脑电图（EEG），并提出并实现了一个开源生物信号领域泛化评估基准。此外，我们将计算机视觉领域的最先进DG算法改进为1D生物信号分类问题，并评估它们的有效性。最后，我们还介绍了一种新颖的神经网络架构，称为DGNet-Bio，专门设计用于解决生物信号中的DG问题。我们的实验表明，DGNet-Bio在新提出的ECG和EEG分类领域泛化基准上优于现有方法。

    Despite their immense success in numerous fields, machine and deep learning systems have not have not yet been able to firmly establish themselves in mission-critical applications in healthcare. One of the main reasons lies in the fact that when models are presented with previously unseen, Out-of-Distribution samples, their performance deteriorates significantly. This is known as the Domain Generalization (DG) problem. Our objective in this work is to propose a benchmark for evaluating DG algorithms, in addition to introducing a novel architecture for tackling DG in biosignal classification. In this paper, we describe the Domain Generalization problem for biosignals, focusing on electrocardiograms (ECG) and electroencephalograms (EEG) and propose and implement an open-source biosignal DG evaluation benchmark. Furthermore, we adapt state-of-the-art DG algorithms from computer vision to the problem of 1D biosignal classification and evaluate their effectiveness. Finally, we also introduc
    
[^131]: 嵌入式理论在洪泛计算中的应用及利用时间延迟减少洪泛网络规模

    Embedding Theory of Reservoir Computing and Reducing Reservoir Network Using Time Delays. (arXiv:2303.09042v1 [cs.LG])

    [http://arxiv.org/abs/2303.09042](http://arxiv.org/abs/2303.09042)

    本文结合延迟嵌入理论和广义嵌入理论，严谨证明了RC本质上是原始输入非线性动态系统的高维嵌入。我们进一步发现了时间延迟和网络神经元数量之间的权衡关系，并显着减小了洪泛计算网络的大小，实现了比全尺寸RC更好的性能。

    

    洪泛计算作为一种特殊的循环神经网络，由于在重构或/和预测复杂物理系统方面具有卓越的功效和高性能，因此正在爆炸性发展。然而，触发RC如此有效应用的机制仍不清楚，需要深入而系统的探索。本文结合延迟嵌入理论和广义嵌入理论，严谨证明了RC本质上是原始输入非线性动态系统的高维嵌入。因此，利用这种嵌入特性，我们将标准RC和时间延迟RC统一到一个通用框架中，并且我们对网络的输出层仅引入时间延迟，进一步发现了时间延迟和网络神经元数量之间的权衡关系。基于这一发现，我们显着减小了洪泛计算网络的大小，用于重构和预测一些代表性的物理系统，并且更让人惊讶的是，实现了比全尺寸RC更好的性能。

    Reservoir computing (RC), a particular form of recurrent neural network, is under explosive development due to its exceptional efficacy and high performance in reconstruction or/and prediction of complex physical systems. However, the mechanism triggering such effective applications of RC is still unclear, awaiting deep and systematic exploration. Here, combining the delayed embedding theory with the generalized embedding theory, we rigorously prove that RC is essentially a high dimensional embedding of the original input nonlinear dynamical system. Thus, using this embedding property, we unify into a universal framework the standard RC and the time-delayed RC where we novelly introduce time delays only into the network's output layer, and we further find a trade-off relation between the time delays and the number of neurons in RC. Based on this finding, we significantly reduce the network size of RC for reconstructing and predicting some representative physical systems, and, more surp
    
[^132]: 可证收敛的即插即用拟牛顿方法

    Provably Convergent Plug-and-Play Quasi-Newton Methods. (arXiv:2303.07271v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2303.07271](http://arxiv.org/abs/2303.07271)

    本文提出了一种可证明收敛的PnP方法，使用拟牛顿步骤以加速收敛，相对于现有的PnP方法对去噪器或保真度函数施加了较轻的限制。

    

    即插即用（PnP）方法是一类高效的迭代算法，旨在利用经典优化算法（如ISTA或ADMM），将数据保真度项和深度去噪器相结合。现有的可证明的PnP方法对去噪器或保真度函数施加了严格的限制，如非扩张性或严格凸性。本文提出了一种可证明的PnP方法，该方法基于近端去噪器施加相对较轻的条件，并引入了拟牛顿步骤以大大加速收敛。通过将深度去噪器特别参数化为梯度步骤，我们进一步将拟牛顿PnP算法的固定点表征为可能非凸函数的临界点。

    Plug-and-Play (PnP) methods are a class of efficient iterative methods that aim to combine data fidelity terms and deep denoisers using classical optimization algorithms, such as ISTA or ADMM. Existing provable PnP methods impose heavy restrictions on the denoiser or fidelity function, such as nonexpansiveness or strict convexity. In this work, we propose a provable PnP method that imposes relatively light conditions based on proximal denoisers, and introduce a quasi-Newton step to greatly accelerate convergence. By specially parameterizing the deep denoiser as a gradient step, we further characterize the fixed-points of the quasi-Newton PnP algorithm as critical points of a possibly non-convex function.
    
[^133]: 基于随机先验网络的高维输出可扩展贝叶斯优化

    Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07260](http://arxiv.org/abs/2302.07260)

    本文提出了一个基于带随机先验的神经网络的深度学习框架用于高维输出的贝叶斯优化，可有效地处理全局优化问题，即使在高维度向量空间或无限维函数空间中也能近似功能关系。

    

    科学和工程中的一些基本问题涉及到未知的高维度映射一组可控变量到昂贵实验结果的黑盒函数的全局优化任务。贝叶斯优化（BO）技术已被证明在使用相对较少的目标函数评估时处理全局优化问题时非常有效，但当处理高维输出时，其性能受到影响。为克服维度主要挑战，本文提出了一个基于带随机先验的神经网络的自举集成的BO和序贯决策制定的深度学习框架。使用适当的体系结构选择，我们证明了所提出的框架可以近似设计变量和感兴趣量之间的功能关系，即使在后者取值于高维向量空间或甚至无限维函数空间的情况下。在贝叶斯优化的背景下，该方法允许高效和可扩展的处理高维度黑盒函数的全局优化。

    Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of 
    
[^134]: 优化算法的符号式发现

    Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06675](http://arxiv.org/abs/2302.06675)

    该论文提出了一种将算法发现视为程序搜索的方法，并用于发现深度神经网络训练的优化算法。他们的方法发现了一种简单而有效的优化算法Lion，它比Adam更节省内存并且在ImageNet上的准确率提高了2％，并且预训练的计算时间也减少了多达5倍。

    

    我们提出了一种将算法发现视为程序搜索的方法，并应用于发现用于深度神经网络训练的优化算法。我们利用高效搜索技术来探索无限和稀疏的程序空间。为了填补代理任务和目标任务之间巨大的泛化差距，我们还引入了程序选择和简化策略。我们的方法发现了一种简单而有效的优化算法，$ \textbf {Lion} $（$ \textit {Evo $\textbf {L} $ved S $ \textbf {i} $ gn M $ \textbf {o} $ me $ \textbf {n} $ tum} $）。它的记忆效率比Adam更高，因为它只跟踪动量。与自适应优化器不同，通过符号运算计算的每个参数的更新具有相同的大小。我们将Lion与广泛使用的优化器（例如Adam和Adafactor）进行了比较，以在不同任务上训练各种模型。在图像分类中，Lion将在ImageNet上ViT的准确性提高了最多2％，并节省了多达5倍的预训练计算时间。

    We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\textbf{Lion}$ ($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compu
    
[^135]: 面向前列腺癌诊断和格里森分级的联邦对比学习模型

    Federated contrastive learning models for prostate cancer diagnosis and Gleason grading. (arXiv:2302.06089v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.06089](http://arxiv.org/abs/2302.06089)

    该研究提出了一个面向大规模病理图像和异质性挑战的联邦对比学习模型（FCL），通过最大化本地客户端和服务器模型间的注意力一致性来增强模型的泛化能力。 在前列腺癌诊断和格里森分级任务中，FCL表现出优异的性能。

    

    人工智能在医学影像领域的应用效果显著。然而，稳健的人工智能模型训练需要大规模的数据集，但数据收集面临沟通、伦理和隐私保护等限制。联邦学习可以通过协调多个客户端训练模型而不共享原始数据来解决上述问题。本研究设计了一个面向大规模病理图像和异质性挑战的联邦对比学习框架（FCL），通过最大化本地客户端和服务器模型间的注意力一致性来增强模型的泛化能力。为了缓解参数传输中的隐私泄露问题并验证FCL的稳健性，我们使用差分隐私通过添加噪音进一步保护模型。我们在19,635个来自多个客户端的前列腺癌WSI上评估了FCL在癌症诊断任务和格里森分级任务中的有效性。在诊断任务中，我们实现了区分癌性和非癌性WSI的0.99的曲线下面积（AUC）。在格里森分级任务中，我们的FCL模型实现了一个均方误差（MSE）为0.143，相比最先进的方法降低了21.7％。

    The application effect of artificial intelligence (AI) in the field of medical imaging is remarkable. Robust AI model training requires large datasets, but data collection faces communication, ethics, and privacy protection constraints. Fortunately, federated learning can solve the above problems by coordinating multiple clients to train the model without sharing the original data. In this study, we design a federated contrastive learning framework (FCL) for large-scale pathology images and the heterogeneity challenges. It enhances the model's generalization ability by maximizing the attention consistency between the local client and server models. To alleviate the privacy leakage problem when transferring parameters and verify the robustness of FCL, we use differential privacy to further protect the model by adding noise. We evaluate the effectiveness of FCL on the cancer diagnosis task and Gleason grading task on 19,635 prostate cancer WSIs from multiple clients. In the diagnosis tas
    
[^136]: 通过层变分分析解释领域适应

    Interpretations of Domain Adaptations via Layer Variational Analysis. (arXiv:2302.01798v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01798](http://arxiv.org/abs/2302.01798)

    本研究通过层变分分析证明了转移学习的成功可以通过相应的数据条件得到保证，并提出了一种基于网络的转移学习的替代方法，该方法在领域适应方面显示出了效率和准确性的提高。

    This study establishes the theory of transfer learning in deep learning through formal derivations and heuristic analysis, proving that the success of transfer learning can be guaranteed with corresponding data conditions. An alternative method for network-based transfer learning is proposed, which shows an increase in efficiency and accuracy for domain adaptation.

    转移学习在许多应用中表现出高效的性能，但有限的文献报道了其背后的机制。本研究建立了正式的推导和启发式分析，以制定深度学习中转移学习的理论。我们的框架利用层变分分析证明了转移学习的成功可以通过相应的数据条件得到保证。此外，我们的理论计算产生了对知识转移过程的直观解释。随后，我们推导出了一种基于网络的转移学习的替代方法。该方法在领域适应方面显示出了效率和准确性的提高。当适应期间的新领域数据足够稀疏时，它特别有优势。对各种任务的数值实验验证了我们的理论，并验证了我们的分析表达式在领域适应方面比梯度下降方法表现更好。

    Transfer learning is known to perform efficiently in many applications empirically, yet limited literature reports the mechanism behind the scene. This study establishes both formal derivations and heuristic analysis to formulate the theory of transfer learning in deep learning. Our framework utilizing layer variational analysis proves that the success of transfer learning can be guaranteed with corresponding data conditions. Moreover, our theoretical calculation yields intuitive interpretations towards the knowledge transfer process. Subsequently, an alternative method for network-based transfer learning is derived. The method shows an increase in efficiency and accuracy for domain adaptation. It is particularly advantageous when new domain data is sufficiently sparse during adaptation. Numerical experiments over diverse tasks validated our theory and verified that our analytic expression achieved better performance in domain adaptation than the gradient descent method.
    
[^137]: 条件平均处理效果的近因因果学习

    Proximal Causal Learning of Conditional Average Treatment Effects. (arXiv:2301.10913v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.10913](http://arxiv.org/abs/2301.10913)

    提出了P-学习器，一种用于学习处理效果异质性的定制两阶段损失函数，能够依靠代理变量进行因果推断，具有较高的灵活性和效率。

    

    高效灵活地估计处理效果的异质性是从医学到市场等各种领域中的一个重要任务，在此过程中有许多有前途的条件平均处理效果估计器。然而，这些估计器通常依赖于测量到的协变量足以证明条件交换性的假设。我们提出了P-学习器，受到R-学习器和DR-学习器的启发，这是一种专门为在观察到的协变量给定可交换性是不合理的情况下，学习异质性处理效果的定制两阶段损失函数，并希望依靠代理变量进行因果推断。我们提出的估计器可以通过现成的损失最小化机器学习方法实现，在核回归的情况下，只要妨碍组件得到合理的估计，就能满足估计误差的oracle限制。

    Efficiently and flexibly estimating treatment effect heterogeneity is an important task in a wide variety of settings ranging from medicine to marketing, and there are a considerable number of promising conditional average treatment effect estimators currently available. These, however, typically rely on the assumption that the measured covariates are enough to justify conditional exchangeability. We propose the P-learner, motivated by the R- and DR-learner, a tailored two-stage loss function for learning heterogeneous treatment effects in settings where exchangeability given observed covariates is an implausible assumption, and we wish to rely on proxy variables for causal inference. Our proposed estimator can be implemented by off-the-shelf loss-minimizing machine learning methods, which in the case of kernel regression satisfies an oracle bound on the estimated error as long as the nuisance components are estimated reasonably well.
    
[^138]: 深度神经网络不安全输入计数的#DNN-Verification问题

    The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks. (arXiv:2301.07068v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.07068](http://arxiv.org/abs/2301.07068)

    本文提出了#DNN-Verification问题，即计算违反特定安全性质的DNN输入配置数量的问题。作者提出了一种新的方法和一种随机的近似方法，分别给出了确切的违规计数和可证明概率界，并在安全关键基准测试上进行了实验比较。

    

    深度神经网络（DNN）在需要高度安全性的关键任务中，例如自动驾驶中越来越被采用。虽然最先进的验证器可以用来检查DNN是否不安全，即是否存在至少一种不安全的输入配置，但它们的是/否输出对于其他目的（如屏蔽、模型选择或培训改进）的信息不足够详细。在本文中，我们介绍了#DNN-Verification问题，它涉及计算导致DNN违反特定安全性质的输入配置数量。我们分析了这个问题的复杂性，并提出了一种新的方法，它返回确切的违规计数。由于该问题的#P完备性，我们还提出了一种随机的近似方法，该方法提供了正确计数的可证明概率界，同时显著降低了计算要求。我们在一组安全关键基准测试上呈现了实验结果，比较了我们的方法与最先进的验证器和基于计数的启发式算法。

    Deep Neural Networks are increasingly adopted in critical tasks that require a high level of safety, e.g., autonomous driving. While state-of-the-art verifiers can be employed to check whether a DNN is unsafe w.r.t. some given property (i.e., whether there is at least one unsafe input configuration), their yes/no output is not informative enough for other purposes, such as shielding, model selection, or training improvements. In this paper, we introduce the #DNN-Verification problem, which involves counting the number of input configurations of a DNN that result in a violation of a particular safety property. We analyze the complexity of this problem and propose a novel approach that returns the exact count of violations. Due to the #P-completeness of the problem, we also propose a randomized, approximate method that provides a provable probabilistic bound of the correct count while significantly reducing computational requirements. We present experimental results on a set of safety-cr
    
[^139]: 通过基于假设的主动学习，探索分子的结构-性质关系

    Discovery of structure-property relations for molecules via hypothesis-driven active learning over the chemical space. (arXiv:2301.02665v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.02665](http://arxiv.org/abs/2301.02665)

    通过假设学习技术，将符号回归和主动学习元素结合起来，为机器学习算法在化学空间中快速探索和预测分子的结构-性质关系提供了一种新的框架。

    

    发现应用于药物靶点、生物分子系统、催化剂、光伏、有机电子和电池的分子候选者，需要开发能够快速探索化学空间的机器学习算法，以针对所需的功能进行目标化。本文介绍了一种基于假设学习的化学空间主动学习的新方法。我们构建了关于感兴趣的结构和功能之间可能关系的假设，基于少量的数据，并将它们引入高斯过程中作为（概率）均值函数。这种方法将符号回归方法（如SISSO）和主动学习的元素结合到一个框架中。构建这个框架的主要重点是在主动学习环境中逼近物理定律，以实现更稳健的预测性能，因为机器学习中传统的保留集评估并不能完全解决这个问题。

    Discovery of the molecular candidates for applications in drug targets, biomolecular systems, catalysts, photovoltaics, organic electronics, and batteries, necessitates development of machine learning algorithms capable of rapid exploration of the chemical spaces targeting the desired functionalities. Here we introduce a novel approach for the active learning over the chemical spaces based on hypothesis learning. We construct the hypotheses on the possible relationships between structures and functionalities of interest based on a small subset of data and introduce them as (probabilistic) mean functions for the Gaussian process. This approach combines the elements from the symbolic regression methods such as SISSO and active learning into a single framework. The primary focus of constructing this framework is to approximate physical laws in an active learning regime toward a more robust predictive performance, as traditional evaluation on hold-out sets in machine learning doesn't accou
    
[^140]: 深度全连接网络和递归学习特征的核机器的特征学习机制

    Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features. (arXiv:2212.13881v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13881](http://arxiv.org/abs/2212.13881)

    本文确定并表征了深度全连接神经网络学习特征的机制，提出了深度神经特征假设并解释了深度学习现象，同时也引领了对递归学习特征的核方法中特征学习的更广泛的理解。

    

    近年来，神经网络在许多技术和科学任务中取得了令人瞩目的成果。然而，这些模型自动选择用于预测的特征或数据模式的机制仍不清楚。确定这样的机制是推动神经网络性能和可解释性以及促进这些模型在科学应用中可靠采用的关键。在本文中，我们确定并表征了深度全连接神经网络学习特征的机制。我们提出了深度神经特征假设，该假设表明神经特征学习是通过实现平均梯度外积来加强与模型输出密切相关的特征。我们的假设揭示了各种深度学习现象，包括假特征的出现和简单性偏差以及如何修剪网络可以提高性能，《彩票假设》。此外，我们的工作中确定的机制也引领了对递归学习特征的核方法中特征学习的更广泛的理解。

    In recent years neural networks have achieved impressive results on many technological and scientific tasks. Yet, the mechanism through which these models automatically select features, or patterns in data, for prediction remains unclear. Identifying such a mechanism is key to advancing performance and interpretability of neural networks and promoting reliable adoption of these models in scientific applications. In this paper, we identify and characterize the mechanism through which deep fully connected neural networks learn features. We posit the Deep Neural Feature Ansatz, which states that neural feature learning occurs by implementing the average gradient outer product to up-weight features strongly related to model output. Our ansatz sheds light on various deep learning phenomena including emergence of spurious features and simplicity biases and how pruning networks can increase performance, the "lottery ticket hypothesis." Moreover, the mechanism identified in our work leads to a
    
[^141]: 不依赖注意力机制的预训练

    Pretraining Without Attention. (arXiv:2212.10544v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10544](http://arxiv.org/abs/2212.10544)

    本文通过使用基于状态空间模型的序列路由方法提出了一种不依赖注意力机制的预训练模型BiGS，可以达到与BERT预训练准确度相当的GLUE测试结果，并具有不同的归纳偏差。

    

    在自然语言处理中，Transformer模型是预训练中取得成功的关键。虽然也有其他架构被用于预训练，但下游任务的准确率要么显著下降，要么需要注意力机制才能达到标准测试的基准（如GLUE）。本文探讨了一种不依赖注意力机制的预训练方法，采用最近在基于状态空间模型（SSM）的序列路由方面的进展。我们提出的模型Bidirectional Gated SSM（BiGS）结合了SSM层和乘性门控架构，这在简化序列建模架构中已经被证明是有效的。该模型学习不考虑成对交互的静态层。即使如此，BiGS能够达到与BERT预训练准确度相当的GLUE测试结果，并且可以在不进行近似的情况下扩展到4096个标记的长形式预训练。分析表明，尽管这些模型的平均准确率相似，但与BERT相比，这种方法在交互和句法表示方面具有不同的归纳偏差。本文所有模型可在 https://git 上获得。

    Transformers have been essential to pretraining success in NLP. While other architectures have been used, downstream accuracy is either significantly worse, or requires attention layers to match standard benchmarks such as GLUE. This work explores pretraining without attention by using recent advances in sequence routing based on state-space models (SSMs). Our proposed model, Bidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative gating architecture that has been effective in simplified sequence modeling architectures. The model learns static layers that do not consider pair-wise interactions. Even so, BiGS is able to match BERT pretraining accuracy on GLUE and can be extended to long-form pretraining of 4096 tokens without approximation. Analysis shows that while the models have similar average accuracy, the approach has different inductive biases than BERT in terms of interactions and syntactic representations. All models from this work are available at https://git
    
[^142]: 通用状态和动作空间上的策略优化

    Policy Optimization over General State and Action Spaces. (arXiv:2211.16715v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16715](http://arxiv.org/abs/2211.16715)

    本文提出了一种新方法并引入了函数近似来解决通用状态和动作空间上的强化学习问题，同时介绍了一种新的策略双平均法。

    

    通用状态和动作空间上的强化学习问题异常困难。本文提出了一种新方法，并引入了函数近似来解决这个问题。同时，还提出了一种新的策略双平均法。这些方法都可以应用于不同类型的RL问题。

    Reinforcement learning (RL) problems over general state and action spaces are notoriously challenging. In contrast to the tableau setting, one can not enumerate all the states and then iteratively update the policies for each state. This prevents the application of many well-studied RL methods especially those with provable convergence guarantees. In this paper, we first present a substantial generalization of the recently developed policy mirror descent method to deal with general state and action spaces. We introduce new approaches to incorporate function approximation into this method, so that we do not need to use explicit policy parameterization at all. Moreover, we present a novel policy dual averaging method for which possibly simpler function approximation techniques can be applied. We establish linear convergence rate to global optimality or sublinear convergence to stationarity for these methods applied to solve different classes of RL problems under exact policy evaluation. 
    
[^143]: Hopfield模型的镜像下降优化技术

    Mirror descent of Hopfield model. (arXiv:2211.15880v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15880](http://arxiv.org/abs/2211.15880)

    本研究提出利用镜像下降技术来初始化神经网络参数，通过Hopfield模型作为原型，成功训练模型，相较于传统随机参数初始化的梯度下降方法，有明显的性能提升，可以增强机器学习模型的优化能力。

    

    镜像下降是一种优雅的优化技术，它利用参数模型的对偶空间来进行梯度下降。虽然最初是为凸优化而开发的，但它越来越多地应用于机器学习领域。本研究提出了一种新方法，利用镜像下降来初始化神经网络的参数。具体来说，我们证明了通过使用Hopfield模型作为神经网络的原型，镜像下降可以有效地训练模型，并比依赖于随机参数初始化的传统梯度下降方法表现出更好的性能。我们的发现突显了镜像下降作为一种有前途的初始化技术，可以增强机器学习模型的优化能力。

    Mirror descent is an elegant optimization technique that leverages a dual space of parametric models to perform gradient descent. While originally developed for convex optimization, it has increasingly been applied in the field of machine learning. In this study, we propose a novel approach for utilizing mirror descent to initialize the parameters of neural networks. Specifically, we demonstrate that by using the Hopfield model as a prototype for neural networks, mirror descent can effectively train the model with significantly improved performance compared to traditional gradient descent methods that rely on random parameter initialization. Our findings highlight the potential of mirror descent as a promising initialization technique for enhancing the optimization of machine learning models.
    
[^144]: 使用ReLM验证大型语言模型

    Validating Large Language Models with ReLM. (arXiv:2211.15458v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15458](http://arxiv.org/abs/2211.15458)

    ReLM是一种使用正则表达式验证和查询LLM的系统，可以解决LLM数据记忆、偏见、毒性和语言理解等问题，具有高效性和广泛性。

    

    即使大型语言模型(LLM)因为可以生成自然的文本而备受推崇，但是越来越多人关注LLM可能带来的负面影响，如数据记忆、偏见和不恰当语言使用。不幸的是，LLM的复杂性和生成能力使得验证（和纠正）这些问题变得困难。在这项工作中，我们介绍了ReLM，这是一种使用标准正则表达式验证和查询LLM的系统。ReLM将广泛的语言模型评估形式化并启用，将复杂的评估规则简化为简单的正则表达式查询。我们的结果探索了关于记忆、性别偏见、毒性和语言理解的查询，显示ReLM相比最先进的特定查询技术达到了高达15倍的系统效率、2.5倍的数据效率以及更广泛的统计和提示调整覆盖范围。ReLM为越来越重要的LLM验证问题提供了竞争性和通用的基准。

    Although large language models (LLMs) have been touted for their ability to generate natural-sounding text, there are growing concerns around possible negative effects of LLMs such as data memorization, bias, and inappropriate language. Unfortunately, the complexity and generation capacities of LLMs make validating (and correcting) such concerns difficult. In this work, we introduce ReLM, a system for validating and querying LLMs using standard regular expressions. ReLM formalizes and enables a broad range of language model evaluations, reducing complex evaluation rules to simple regular expression queries. Our results exploring queries surrounding memorization, gender bias, toxicity, and language understanding show that ReLM achieves up to 15x higher system efficiency, 2.5x data efficiency, and increased statistical and prompt-tuning coverage compared to state-of-the-art ad-hoc queries. ReLM offers a competitive and general baseline for the increasingly important problem of LLM valida
    
[^145]: 自适应感知核调制下的少样本图像生成

    Few-shot Image Generation via Adaptation-Aware Kernel Modulation. (arXiv:2210.16559v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.16559](http://arxiv.org/abs/2210.16559)

    本研究提出了一种自适应感知核调制下的少样本图像生成方法，改进了现有方法在选择源模型知识方面的局限性。

    

    少样本图像生成旨在在给定极其有限的域内样本的前提下生成新的不同样本。最近的研究采用了迁移学习方法，利用已预训练在大规模来源域数据集上的GAN模型，并根据极少的目标域样本对该模型进行调整。最近的FSIG方法的核心是知识保留准则，它们旨在选择要保留到适应模型中的源模型知识子集。然而，现有方法的主要局限性在于它们的知识保留准则只考虑源域/源任务，没有考虑目标域/适应任务在选择源模型知识方面的影响，因此对于源域和目标域之间不同接近程度的设置的适用性存在疑问。本工作提出了两个贡献。第一个贡献是重新审视最近的FSIG作品及其实验。

    Few-shot image generation (FSIG) aims to learn to generate new and diverse samples given an extremely limited number of samples from a domain, e.g., 10 training samples. Recent work has addressed the problem using transfer learning approach, leveraging a GAN pretrained on a large-scale source domain dataset and adapting that model to the target domain based on very limited target domain samples. Central to recent FSIG methods are knowledge preserving criteria, which aim to select a subset of source model's knowledge to be preserved into the adapted model. However, a major limitation of existing methods is that their knowledge preserving criteria consider only source domain/source task, and they fail to consider target domain/adaptation task in selecting source model's knowledge, casting doubt on their suitability for setups of different proximity between source and target domain. Our work makes two contributions. As our first contribution, we re-visit recent FSIG works and their experi
    
[^146]: 提升I$^2$-GNN在循环计数方面的图神经网络性能

    Boosting the Cycle Counting Power of Graph Neural Networks with I$^2$-GNNs. (arXiv:2210.13978v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13978](http://arxiv.org/abs/2210.13978)

    本文研究了一种名为Subgraph MPNNs的GNN模型。我们发现，Subgraph MPNNs不能在节点级别上计数超过4个的环，这对于生物学、化学和社交网络分析等应用至关重要。

    

    消息传递神经网络(MPNNs)是一类被广泛应用的图神经网络(GNNs)。然而，MPNNs的表达能力有限，这启发我们研究可证明具有更强表达能力的GNN体系结构。本文提出研究子图MPNNs的计数能力，这是一类最新和常用的强大GNN模型，其从每个节点提取根据子图，在根节点分配唯一标识符并在其根据子图中编码根节点的表示。具体地，我们证明子图MPNNs不能在节点级别上计数超过4个的环，这意味着节点表示不能正确地编码周围的子结构。

    Message Passing Neural Networks (MPNNs) are a widely used class of Graph Neural Networks (GNNs). The limited representational power of MPNNs inspires the study of provably powerful GNN architectures. However, knowing one model is more powerful than another gives little insight about what functions they can or cannot express. It is still unclear whether these models are able to approximate specific functions such as counting certain graph substructures, which is essential for applications in biology, chemistry and social network analysis. Motivated by this, we propose to study the counting power of Subgraph MPNNs, a recent and popular class of powerful GNN models that extract rooted subgraphs for each node, assign the root node a unique identifier and encode the root node's representation within its rooted subgraph. Specifically, we prove that Subgraph MPNNs fail to count more-than-4-cycles at node level, implying that node representations cannot correctly encode the surrounding substru
    
[^147]: 设计通用因果深度学习模型：以随机分析中的无限维动态系统为例

    Designing Universal Causal Deep Learning Models: The Case of Infinite-Dimensional Dynamical Systems from Stochastic Analysis. (arXiv:2210.13300v2 [math.DS] UPDATED)

    [http://arxiv.org/abs/2210.13300](http://arxiv.org/abs/2210.13300)

    设计了一个DL模型框架，名为因果神经算子（CNO），以逼近因果算子（CO），并证明了CNO模型可以在紧致集上一致逼近Hölder或平滑迹类算子。

    

    因果算子（CO）在当代随机分析中扮演着重要角色，例如各种随机微分方程的解算子。然而，目前还没有一个能够逼近CO的深度学习（DL）模型的规范框架。本文通过引入一个DL模型设计框架来提出一个“几何感知”的解决方案，该框架以合适的无限维线性度量空间为输入，并返回适应这些线性几何的通用连续序列DL模型。我们称这些模型为因果神经算子（CNO）。我们的主要结果表明，我们的框架所产生的模型可以在紧致集上和跨任意有限时间视野上一致逼近Hölder或平滑迹类算子，这些算子因果地映射给定线性度量空间之间的序列。我们的分析揭示了关于CNO的潜在状态空间维度的新定量关系，甚至对于（经典的）有限维DL模型也有新的影响。

    Causal operators (CO), such as various solution operators to stochastic differential equations, play a central role in contemporary stochastic analysis; however, there is still no canonical framework for designing Deep Learning (DL) models capable of approximating COs. This paper proposes a "geometry-aware'" solution to this open problem by introducing a DL model-design framework that takes suitable infinite-dimensional linear metric spaces as inputs and returns a universal sequential DL model adapted to these linear geometries. We call these models Causal Neural Operators (CNOs). Our main result states that the models produced by our framework can uniformly approximate on compact sets and across arbitrarily finite-time horizons H\"older or smooth trace class operators, which causally map sequences between given linear metric spaces. Our analysis uncovers new quantitative relationships on the latent state-space dimension of CNOs which even have new implications for (classical) finite-d
    
[^148]: 自编码稀疏贝叶斯的IRT因子分解，校准，和摩尔滋互反推断：以工作残障功能评估电池 (WD-FAB) 为例

    Autoencoded sparse Bayesian in-IRT factorization, calibration, and amortized inference for the Work Disability Functional Assessment Battery. (arXiv:2210.10952v4 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2210.10952](http://arxiv.org/abs/2210.10952)

    本文开发了一种贝叶斯层级模型，利用基于稀疏性的收缩和自编码器，实现了工作残障功能评估电池的尺度分解、条目选择、参数识别和响应评分，并取得了显著的提高。

    

    工作残障功能评估电池 (WD-FAB) 是一种基于条目库的，用于评估职业相关的心理和身体功能的多维物品反应理论 (IRT) 工具。本文开发了一种贝叶斯层级模型，实现以下同时任务：尺度分解，条目选择，参数识别和响应评分。这种方法利用基于稀疏性的收缩，消除了线性因子化/IRT模型的高度参数化问题，而后用自动编码器进行摩尔滋互反推断计算。我们证明这一方法在WD-FAB中的应用，以及与传统方法的比较表明了其显著的提高。

    The Work Disability Functional Assessment Battery (WD-FAB) is a multidimensional item response theory (IRT) instrument designed for assessing work-related mental and physical function based on responses to an item bank. In prior iterations it was developed using traditional means -- linear factorization and null hypothesis statistical testing for item partitioning/selection, and finally, posthoc calibration of disjoint unidimensional IRT models. As a result, the WD-FAB, like many other IRT instruments, is a posthoc model. Its item partitioning, based on exploratory factor analysis, is blind to the final nonlinear IRT model and is not performed in a manner consistent with goodness of fit to the final model. In this manuscript, we develop a Bayesian hierarchical model for self-consistently performing the following simultaneous tasks: scale factorization, item selection, parameter identification, and response scoring. This method uses sparsity-based shrinkage to obviate the linear factori
    
[^149]: 针对Facebook上的政治活动的弱监督学习

    Weakly Supervised Learning for Analyzing Political Campaigns on Facebook. (arXiv:2210.10669v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.10669](http://arxiv.org/abs/2210.10669)

    本研究提出了一种基于弱监督学习的方法，通过分析Facebook上政治广告的立场和议题，以及其使用人口统计学定位，来了解政治活动的特点和时间动态。

    

    社交媒体平台目前是政治信息传播的主要渠道，政治家们能够通过这些平台针对特定人群进行宣传，并根据他们的反应进行调整。然而，使这种交流透明化是具有挑战性的，因为信息传播与目标受众紧密相连，并经常被多个利益攸关方共同传播。本文旨在第一步了解这些高度分散的政治活动。我们提出了一种弱监督的方法来识别Facebook上政治广告的立场和议题，并分析政治活动如何使用某种人口统计学定位，如位置、性别或年龄。此外，我们还分析了选举民意调查中政治广告的时间动态。

    Social media platforms are currently the main channel for political messaging, allowing politicians to target specific demographics and adapt based on their reactions. However, making this communication transparent is challenging, as the messaging is tightly coupled with its intended audience and often echoed by multiple stakeholders interested in advancing specific policies. Our goal in this paper is to take a first step towards understanding these highly decentralized settings. We propose a weakly supervised approach to identify the stance and issue of political ads on Facebook and analyze how political campaigns use some kind of demographic targeting by location, gender, or age. Furthermore, we analyze the temporal dynamics of the political ads on election polls.
    
[^150]: ScionFL: 高效且强健的安全量化聚合

    ScionFL: Efficient and Robust Secure Quantized Aggregation. (arXiv:2210.07376v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.07376](http://arxiv.org/abs/2210.07376)

    本文介绍了ScionFL，这是第一个在联邦学习中高效运行在量化输入上并同时提供对恶意客户端强健性的安全聚合框架。

    

    安全聚合在联邦学习中被广泛应用，以减轻与中央聚合器看到所有参数更新有关的隐私问题。不幸的是，大多数现有的安全聚合方案忽略了两个关键的正交研究方向，旨在（i）显着减少客户端-服务器通信和（ii）减轻恶意客户端的影响。然而，这两个额外的属性对于实现跨设备的联邦学习与成千上万甚至百万（移动）参与者至关重要。在本文中，我们通过引入ScionFL，联合了两个研究方向，这是FL的第一个安全聚合框架，可以在量化输入上有效运行，同时提供对恶意客户端的强健性。我们的框架利用（新颖的）多方计算（MPC）技术，并支持多个线性（1比特）量化方案，包括利用随机哈达玛变换和卡申表示的方案。

    Secure aggregation is commonly used in federated learning (FL) to alleviate privacy concerns related to the central aggregator seeing all parameter updates in the clear. Unfortunately, most existing secure aggregation schemes ignore two critical orthogonal research directions that aim to (i) significantly reduce client-server communication and (ii) mitigate the impact of malicious clients. However, both of these additional properties are essential to facilitate cross-device FL with thousands or even millions of (mobile) participants. In this paper, we unite both research directions by introducing ScionFL, the first secure aggregation framework for FL that operates efficiently on quantized inputs and simultaneously provides robustness against malicious clients. Our framework leverages (novel) multi-party computation (MPC) techniques and supports multiple linear (1-bit) quantization schemes, including ones that utilize the randomized Hadamard transform and Kashin's representation. Our th
    
[^151]: Wasserstein分布鲁棒优化问题的核心集

    Coresets for Wasserstein Distributionally Robust Optimization Problems. (arXiv:2210.04260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04260](http://arxiv.org/abs/2210.04260)

    本文提出了一种构建一般Wasserstein分布鲁棒优化问题核心集的统一框架。

    

    Wasserstein分布鲁棒优化（WDRO）是一种通过含糊数据增强机器学习鲁棒性的流行模型。然而，WDRO的复杂度在实践中可能是禁止性的，因为解决其“极小极大”表达式需要大量计算。最近，已经开发了一些针对特定机器学习任务（例如逻辑回归）的快速WDRO训练算法。然而，据我们所知，对于一般的大规模WDRO的设计高效算法的研究仍然非常有限。核心集是一种重要的工具，用于压缩大型数据集，因此已广泛应用于减少许多优化问题的计算复杂性。本文介绍了一种构建一般WDRO问题的$\epsilon$-coreset的统一框架。尽管由于不确定性而获取WDRO的传统核心集具有挑战性。

    Wasserstein distributionally robust optimization (\textsf{WDRO}) is a popular model to enhance the robustness of machine learning with ambiguous data. However, the complexity of \textsf{WDRO} can be prohibitive in practice since solving its ``minimax'' formulation requires a great amount of computation. Recently, several fast \textsf{WDRO} training algorithms for some specific machine learning tasks (e.g., logistic regression) have been developed. However, the research on designing efficient algorithms for general large-scale \textsf{WDRO}s is still quite limited, to the best of our knowledge. \textit{Coreset} is an important tool for compressing large dataset, and thus it has been widely applied to reduce the computational complexities for many optimization problems. In this paper, we introduce a unified framework to construct the $\epsilon$-coreset for the general \textsf{WDRO} problems. Though it is challenging to obtain a conventional coreset for \textsf{WDRO} due to the uncertaint
    
[^152]: 命名实体识别的深度跨度表示

    Deep Span Representations for Named Entity Recognition. (arXiv:2210.04182v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.04182](http://arxiv.org/abs/2210.04182)

    本研究提出了DSpERT模型，通过跨度Transformer逐层聚合标记表示作为键和值，产生了深层语义的跨度表示，从而解决了现有跨度基础NER系统中长跨度实体显着无效性和重叠跨度表示的耦合问题。实验结果表明，DSpERT在八个NER基准测试中取得了性能高于或与最新最先进系统竞争的成果。

    

    跨度基础模型是命名实体识别（NER）最简单直接的方法之一。 现有的跨度基础NER系统将标记表示浅层聚合到跨度表示中。 但是，这通常导致长跨度实体的显着无效性，重叠跨度表示的耦合，最终性能下降。 在本研究中，我们提出了DSpERT（来自Transformer的深度跨度编码器表示），它由标准Transformer和跨度Transformer组成。 后者使用低层次的跨度表示作为查询，并从底部到顶部逐层聚合标记表示作为键和值，因此，DSpERT产生了深层语义的跨度表示。 借助预训练语言模型的权重初始化，DSpERT在八个NER基准测试中取得了高于或与最新的最先进系统竞争的性能。 实验结果验证了深度对跨度基础NER系统的重要性。

    Span-based models are one of the most straightforward methods for named entity recognition (NER). Existing span-based NER systems shallowly aggregate the token representations to span representations. However, this typically results in significant ineffectiveness for long-span entities, a coupling between the representations of overlapping spans, and ultimately a performance degradation. In this study, we propose DSpERT (Deep Span Encoder Representations from Transformers), which comprises a standard Transformer and a span Transformer. The latter uses low-layered span representations as queries, and aggregates the token representations as keys and values, layer by layer from bottom to top. Thus, DSpERT produces span representations of deep semantics.  With weight initialization from pretrained language models, DSpERT achieves performance higher than or competitive with recent state-of-the-art systems on eight NER benchmarks. Experimental results verify the importance of the depth for s
    
[^153]: 关于大型人工智能模型不可能安全的论述

    On the Impossible Safety of Large AI Models. (arXiv:2209.15259v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15259](http://arxiv.org/abs/2209.15259)

    本文探讨了大型人工智能模型的安全问题，并指出了构建既精确又安全的模型的基本不可能性，并提供了统计学下限证明。

    

    大型人工智能模型(LAIMs)，其中大型语言模型是最突出的最近的例子，展示了令人印象深刻的性能。然而，实验证据表明，它们存在严重的安全问题。本文系统化地总结了有关构建任意准确和安全的机器学习模型的基本不可能性的知识。更确切地说，我们确定了今天许多机器学习设置的关键挑战特征。即，高精度似乎需要记住大型训练数据集，这些数据集通常是用户生成的，并且高度异构，包括敏感信息和假用户。然后，我们调查了统计下限，认为这构成了一个令人信服的理由，说明设计具有强安全保证的高精度LAIMs的可能性是不可能的。

    Large AI Models (LAIMs), of which large language models are the most prominent recent example, showcase some impressive performance. However they have been empirically found to pose serious security issues. This paper systematizes our knowledge about the fundamental impossibility of building arbitrarily accurate and secure machine learning models. More precisely, we identify key challenging features of many of today's machine learning settings. Namely, high accuracy seems to require memorizing large training datasets, which are often user-generated and highly heterogeneous, with both sensitive information and fake users. We then survey statistical lower bounds that, we argue, constitute a compelling case against the possibility of designing high-accuracy LAIMs with strong security guarantees.
    
[^154]: 面向高维可达性问题的形式化安全保障生成

    Generating Formal Safety Assurances for High-Dimensional Reachability. (arXiv:2209.12336v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.12336](http://arxiv.org/abs/2209.12336)

    提出了一个称为Safe DeepReach的框架，用于为DeepReach方法生成正式安全保障。该框架将新颖的Lipschitz连续性分析与区间边界传播方法相结合，以有效和可伸缩的方式保证解决方案的安全性。

    

    为自主系统提供正式的安全和性能保证变得日益重要。哈密顿-雅科比（HJ）可达性分析是一种流行的形式验证工具，用于提供这些保证，因为它可以处理一般非线性系统动态、有界对抗系统干扰以及状态和输入约束。但是，它涉及到求解PDE，其计算和内存复杂度随着状态维度的增加呈指数级增长，使其在大型系统上的直接使用变得不可行。最近提出的DeepReach方法通过利用正弦神经PDE求解器来克服了这一挑战，用于解决高维可达性问题，其计算要求随可达管复杂性而不是状态空间维度而变化。不幸的是，神经网络可能会出现错误，因此计算出的解决方案可能不安全，这没有达到我们提供正式安全保障的总体目标。为了解决这一挑战，我们提出了一个称为Safe DeepReach的框架，用于为DeepReach方法生成正式安全保障。我们的框架将新颖的Lipschitz连续性分析与区间边界传播方法相结合，以有效和可伸缩的方式保证解决方案的安全性。我们在几个基准示例上展示了我们提出的方法的有效性，包括基于感知的高维车道保持系统。

    Providing formal safety and performance guarantees for autonomous systems is becoming increasingly important. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for providing these guarantees, since it can handle general nonlinear system dynamics, bounded adversarial system disturbances, and state and input constraints. However, it involves solving a PDE, whose computational and memory complexity scales exponentially with respect to the state dimensionality, making its direct use on large-scale systems intractable. A recently proposed method called DeepReach overcomes this challenge by leveraging a sinusoidal neural PDE solver for high-dimensional reachability problems, whose computational requirements scale with the complexity of the underlying reachable tube rather than the state space dimension. Unfortunately, neural networks can make errors and thus the computed solution may not be safe, which falls short of achieving our overarching goal to provide fo
    
[^155]: 面向任务的通信中的鲁棒信息瓶颈与数字调制

    Robust Information Bottleneck for Task-Oriented Communication with Digital Modulation. (arXiv:2209.10382v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2209.10382](http://arxiv.org/abs/2209.10382)

    本文提出了一种数字调制的面向任务的通信方案，DT-JSCC，首先探讨编码表示的信息量和在接收表示中的信息失真鲁棒性之间的固有权衡，解决了仅传输任务相关信息和JSCC在数字通信系统上兼容性的问题。

    

    面向任务的通信通过使用基于学习的联合源—信道编码(JSCC)，旨在通过将任务相关的信息传输到接收方，设计一种通信高效的边缘推断系统。然而，仅传输任务相关信息而不引入任何冗余导致的信息变化可能会导致学习中的鲁棒性问题，而直接将源数据映射为连续的信道输入符号的JSCC在现有数字通信系统上存在兼容性问题。本文首先探讨编码表示的信息量和在接收表示中的信息失真鲁棒性之间的固有权衡，然后提出了一种数字调制的面向任务的通信方案，称为离散任务导向的JSCC (DT-JSCC)，其中发射机将特征编码为离散表示，并将其传输到接收方。

    Task-oriented communications, mostly using learning-based joint source-channel coding (JSCC), aim to design a communication-efficient edge inference system by transmitting task-relevant information to the receiver. However, only transmitting task-relevant information without introducing any redundancy may cause robustness issues in learning due to the channel variations, and the JSCC which directly maps the source data into continuous channel input symbols poses compatibility issues on existing digital communication systems. In this paper, we address these two issues by first investigating the inherent tradeoff between the informativeness of the encoded representations and the robustness to information distortion in the received representations, and then propose a task-oriented communication scheme with digital modulation, named discrete task-oriented JSCC (DT-JSCC), where the transmitter encodes the features into a discrete representation and transmits it to the receiver with the digi
    
[^156]: 冷启动情况下的数据选择策略：一种基于提示信息传递不确定性估计的few-shot语言模型微调方法

    Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach. (arXiv:2209.06995v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.06995](http://arxiv.org/abs/2209.06995)

    本文提出了一种名为PATRON的方法，使用基于提示信息的不确定性的数据选择策略来提高预训练语言模型微调的few-shot性能，在六个文本分类数据集上实验证实该方法的性能优于最先进的冷启动数据选择基线，且仅使用128标签的情况下，该方法可以达到91.0%和92.1%的完全监督性能。

    

    大型语言模型展现出了出色的few-shot性能，但性能对于few-shot实例的选择非常敏感。我们提出了一种名为PATRON的新方法，该方法使用基于提示信息的不确定性估计来选择预训练语言模型微调的数据，在冷启动情况下没有初始标记数据可用。在PATRON中，我们设计了（1）基于提示信息的不确定性传播方法来估计数据点的重要性和（2）一种分割-重写（PTR）策略，以在查询注释时促进样本的多样性。在六个文本分类数据集上的实验表明，PATRON的表现比最强的冷启动数据选择基线优越了6.9%。此外，仅使用128标签，PATRON基于普通微调和基于提示信息学习分别达到了91.0%和92.1%的完全监督性能。我们的PATRON实现可在\url{https://github.com/yueyu1030/Patron}上获得。

    Large Language Models have demonstrated remarkable few-shot performance, but the performance can be sensitive to the selection of few-shot instances. We propose PATRON, a new method that uses prompt-based uncertainty estimation for data selection for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON is available at \url{https://github.com/yueyu1030/Patron}.
    
[^157]: EDeNN: 事件衰减神经网络实现低延迟视觉

    EDeNN: Event Decay Neural Networks for low latency vision. (arXiv:2209.04362v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.04362](http://arxiv.org/abs/2209.04362)

    EDeNN是一种新型的神经网络，更接近原始事件数据流，避免了积累事件到图像帧的过程，展现了在角速度回归和竞争光流估计方面最先进的性能。

    

    尽管神经网络在计算机视觉任务上取得了成功，但数字“神经元”是对生物神经元的一种非常不精确的近似。当前的学习方法旨在在数字设备上进行，具有像图像帧这样的数字数据表示。相比之下，生物视觉系统通常比最先进的数字计算机视觉算法更具能力和效率。事件相机是一种新兴的传感器技术，它模仿生物视觉，使用异步触发的像素，放弃了图像帧的概念。为了利用现代学习技术，许多基于事件的算法不得不将事件积累回图像帧，从而浪费了事件相机的优势。我们采用相反的理念，开发了一种新型神经网络，该网络更接近原始事件数据流。我们在角速度回归和竞争光流估计方面展示了最先进的性能，同时避免了积累事件到图像帧的过程。

    Despite the success of neural networks in computer vision tasks, digital 'neurons' are a very loose approximation of biological neurons. Today's learning approaches are designed to function on digital devices with digital data representations such as image frames. In contrast, biological vision systems are generally much more capable and efficient than state-of-the-art digital computer vision algorithms. Event cameras are an emerging sensor technology which imitates biological vision with asynchronously firing pixels, eschewing the concept of the image frame. To leverage modern learning techniques, many event-based algorithms are forced to accumulate events back to image frames, somewhat squandering the advantages of event cameras.  We follow the opposite paradigm and develop a new type of neural network which operates closer to the original event data stream. We demonstrate state-of-the-art performance in angular velocity regression and competitive optical flow estimation, while avoid
    
[^158]: 带有知识蒸馏的边缘生成对抗超分辨率技术

    Generative Adversarial Super-Resolution at the Edge with Knowledge Distillation. (arXiv:2209.03355v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2209.03355](http://arxiv.org/abs/2209.03355)

    该论文提出了一种高效的基于生成对抗网络的实时超分辨率模型EdgeSRGAN，使用模型量化提高了CPU和Edge TPU设备上的执行效率，并通过知识蒸馏技术进一步优化模型，保留了较好的图像质量。

    

    单幅图像超分辨率可支持机器人任务，例如在需要可靠的视觉流以监视任务、处理远程操纵或研究相关视觉细节的环境中。本文提出一种高效的、基于生成对抗网络的实时超分辨率模型，称为EdgeSRGAN（代码可在https://github.com/PIC4SeR/EdgeSRGAN获得）。我们采用了原始SRGAN的定制化架构和模型量化，以提高CPU和Edge TPU设备上的执行效率，实现最高达200fps的推理计算。我们进一步通过将模型知识蒸馏到网络的较小版本中进行优化，并相比标准训练方法获得了显着的改进。我们的实验表明，与更重的最先进模型相比，我们的快速轻量级模型保留了相当令人满意的图像质量。最后，我们进行带有带宽降级的图像传输实验，以突显该系统优势。

    Single-Image Super-Resolution can support robotic tasks in environments where a reliable visual stream is required to monitor the mission, handle teleoperation or study relevant visual details. In this work, we propose an efficient Generative Adversarial Network model for real-time Super-Resolution, called EdgeSRGAN (code available at https://github.com/PIC4SeR/EdgeSRGAN). We adopt a tailored architecture of the original SRGAN and model quantization to boost the execution on CPU and Edge TPU devices, achieving up to 200 fps inference. We further optimize our model by distilling its knowledge to a smaller version of the network and obtain remarkable improvements compared to the standard training approach. Our experiments show that our fast and lightweight model preserves considerably satisfying image quality compared to heavier state-of-the-art models. Finally, we conduct experiments on image transmission with bandwidth degradation to highlight the advantages of the proposed system for 
    
[^159]: 重归骨干：重新发现骨干在域泛化中的作用

    Back-to-Bones: Rediscovering the Role of Backbones in Domain Generalization. (arXiv:2209.01121v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.01121](http://arxiv.org/abs/2209.01121)

    本文重点研究骨干在深度学习模型中域泛化能力的影响，通过评估各种特征提取器发现其准确性与单域分类任务性能有显著线性相关性。

    

    域泛化(DG)研究深度学习模型对于超出训练分布的泛化能力。在过去十年中，文献中充斥着声称能够获得更抽象和稳健的数据表示以应对域偏移的训练方法。最近的研究为DG提供了可重复的基准，指出了天真的经验风险最小化(ERM)算法的有效性。然而，研究人员仍然坚持使用相同过时的特征提取器，迄今为止尚未注意到不同骨干的影响。在本文中，我们重新回到骨干，提出了对它们内在泛化能力的全面分析，这在研究社区中被忽视。我们评估了各种特征提取器，从标准的残差解决方案到基于Transformer的架构，发现了一个明显的线性相关性，就是说，大规模单域分类任务的性能可以通过骨干的准确性凸显出来。

    Domain Generalization (DG) studies the capability of a deep learning model to generalize to out-of-training distributions. In the last decade, literature has been massively filled with training methodologies that claim to obtain more abstract and robust data representations to tackle domain shifts. Recent research has provided a reproducible benchmark for DG, pointing out the effectiveness of naive empirical risk minimization (ERM) over existing algorithms. Nevertheless, researchers persist in using the same outdated feature extractors, and no attention has been given to the effects of different backbones yet. In this paper, we start back to the backbones proposing a comprehensive analysis of their intrinsic generalization capabilities, which so far have been ignored by the research community. We evaluate a wide variety of feature extractors, from standard residual solutions to transformer-based architectures, finding an evident linear correlation between large-scale single-domain clas
    
[^160]: 论现实和语言数据限制：将LLMs与人类规范对齐

    On Reality and the Limits of Language Data: Aligning LLMs with Human Norms. (arXiv:2208.11981v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.11981](http://arxiv.org/abs/2208.11981)

    本文研究了大型语言模型（LLMs）在仅使用语言数据的情况下理解物理世界的能力，使用新颖且严密控制的推理测试（ART）与人类规范进行对比，研究发现了LLMs在某些常识关系模型中可以直接从数据中学习，但存在弱点，例如在部分和包含关系方面表现不足。

    

    最近，大型语言模型（LLMs）在利用大量自然语言数据中的语言关联进行实际应用方面取得了进展。然而，它们仅使用语言数据来理解物理世界的能力仍有疑问。在回顾现有协议之后，我们使用一种新颖且严密控制的推理测试（ART）来探讨这个问题，并比较人类规范与GPT-3版本之间的差异。我们的研究结果突出了通常可以直接从数据中学习的常识关系模型类别以及弱点所在。GPT-3为包括同义词、反义词和默认继承在内的几个关系方面提供了与人类主体相当的口头推理证据。没有来自人类判断的强化学习，GPT-3在具有部分和包含关系方面表现的区间下限处。在必要品质、大小顺序和强度顺序等方面也观察到了不足之处。把LLMs与象征性方法相结合，

    Recent advancements in Large Language Models (LLMs) harness linguistic associations in vast natural language data for practical applications. However, their ability to understand the physical world using only language data remains a question. After reviewing existing protocols, we explore this question using a novel and tightly controlled reasoning test (ART) and compare human norms against versions of GPT-3. Our findings highlight the categories of common-sense relations models that could learn directly from data and areas of weakness. GPT-3 offers evidence for verbal reasoning on a par with human subjects for several relations including Synonymy, Antonymy, and Default inheritance, Without reinforcement learning from human judgements, it appears GPT-3 performs at the lower end of the reference interval for Has-part and Contained-in. Weaknesses were observed also in affordance characteristics through Necessary-quality, Order-of-size and Order-of-intensity. Combining LLMs with symbolic 
    
[^161]: 用量子开关测量不相容性并对量子观测结果进行聚类

    Measuring incompatibility and clustering quantum observables with a quantum switch. (arXiv:2208.06210v3 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2208.06210](http://arxiv.org/abs/2208.06210)

    本研究提出了一种量子不相容性测量指标“共同本征空间干扰”，并进一步通过量子开关实现量化，方便量子机器学习任务的处理。同时，算法可以对测量结果进行聚类，确定共享相似测量环境的观察者组。

    

    不相容观测结果是量子力学的基石，并且是量子技术中的宝贵资源。本文提出了一种不相容性测量指标，即“共同本征空间干扰”，可以量化一个精确观测量对另一个本征空间的干扰量。该指标提供了 von Neumann 测量空间的度量，并可以通过使用称为“量子开关”的设置，让测量过程以不确定的顺序进行，从而高效地进行估计。由于这些特性，MED可以在量子机器学习任务中使用。通过提供一个无监督算法，对未知的von Neumann 测量结果进行聚类。我们的算法对噪声具有鲁棒性，并可用于确定共享近似相同测量环境的观察者组。

    The existence of incompatible observables is a cornerstone of quantum mechanics and a valuable resource in quantum technologies. Here we introduce a measure of incompatibility, called the mutual eigenspace disturbance (MED), which quantifies the amount of disturbance induced by the measurement of a sharp observable on the eigenspaces of another. The MED provides a metric on the space of von Neumann measurements, and can be efficiently estimated by letting the measurement processes act in an indefinite order, using a setup known as the quantum switch, which also allows one to quantify the noncommutativity of arbitrary quantum processes. Thanks to these features, the MED can be used in quantum machine learning tasks. We demonstrate this application by providing an unsupervised algorithm that clusters unknown von Neumann measurements. Our algorithm is robust to noise can be used to identify groups of observers that share approximately the same measurement context.
    
[^162]: 带 $\beta$-差异的稀疏非负矩阵分解的主导最小化算法

    Majorization-minimization for Sparse Nonnegative Matrix Factorization with the $\beta$-divergence. (arXiv:2207.06316v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.06316](http://arxiv.org/abs/2207.06316)

    本文提出了一种带 $\beta$-差异的稀疏非负矩阵分解的主导最小化算法，其能够适用于任何 $\beta$-差异和其他稀疏约束。

    

    本文提出了一种新的多元乘法更新方法，用于具有 $\beta$-差异和两个因子中的一个（比如说，激活矩阵）稀疏正则化的非负矩阵分解。标准的做法是限制字典的列具有单位范数，从而控制另一个因子（字典矩阵）的范数，以避免病态问题。我们的方法将原问题重新参数化为等价的标度不变的目标函数的优化问题。然后，我们导出块下降主导最小化算法，这些算法对于 $\ell_{1}$-正则化或更 "激进" 的对数正则化都可以产生简单的多元乘法更新。与其他最先进的方法相比，我们的算法在任何 $\beta$-差异（即任何 $\beta$ 的值）和其他稀疏约束上也具有通用性。

    This article introduces new multiplicative updates for nonnegative matrix factorization with the $\beta$-divergence and sparse regularization of one of the two factors (say, the activation matrix). It is well known that the norm of the other factor (the dictionary matrix) needs to be controlled in order to avoid an ill-posed formulation. Standard practice consists in constraining the columns of the dictionary to have unit norm, which leads to a nontrivial optimization problem. Our approach leverages a reparametrization of the original problem into the optimization of an equivalent scale-invariant objective function. From there, we derive block-descent majorization-minimization algorithms that result in simple multiplicative updates for either $\ell_{1}$-regularization or the more "aggressive" log-regularization. In contrast with other state-of-the-art methods, our algorithms are universal in the sense that they can be applied to any $\beta$-divergence (i.e., any value of $\beta$) and t
    
[^163]: 在医学领域中实用联邦学习的探索

    Towards the Practical Utility of Federated Learning in the Medical Domain. (arXiv:2207.03075v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.03075](http://arxiv.org/abs/2207.03075)

    本研究提出了应用联邦学习于医学领域的实用指南，包括三个具有代表性的医学数据集的实验，旨在提高医保业的数据效率，并形成适用于全行业的标准。

    

    联邦学习（FL）是一个活跃的研究领域。医学领域是采用FL的最适合领域之一，因为必须尊重患者隐私。然而，以往的研究并没有提供在医学领域中应用FL的实用指南。本文针对三个代表性的医学数据集，即长期的电子健康记录、皮肤癌图像和心电图信号，提出经验基准和实验设置。潜在的FL用户，如医疗机构和IT公司，可以将这些基准作为采用FL的指南，并尽可能减少试错。对于每个数据集，每个客户端数据来自不同的来源，以保留现实世界的异质性。我们评估了六种针对客户端数据异质性问题的FL算法，以及一种将两种典型FL算法的优点结合起来的混合算法。基于三种类型数据的实验结果，我们发现简单的FL算法可以达到与更复杂算法相当的性能。我们的工作为医疗机构和IT公司提供了在安全高效的方式下，应用FL从而改善医疗保健的实用指南。

    Federated learning (FL) is an active area of research. One of the most suitable areas for adopting FL is the medical domain, where patient privacy must be respected. Previous research, however, does not provide a practical guide to applying FL in the medical domain. We propose empirical benchmarks and experimental settings for three representative medical datasets with different modalities: longitudinal electronic health records, skin cancer images, and electrocardiogram signals. The likely users of FL such as medical institutions and IT companies can take these benchmarks as guides for adopting FL and minimize their trial and error. For each dataset, each client data is from a different source to preserve real-world heterogeneity. We evaluate six FL algorithms designed for addressing data heterogeneity among clients, and a hybrid algorithm combining the strengths of two representative FL algorithms. Based on experiment results from three modalities, we discover that simple FL algorith
    
[^164]: 记忆化训练样本的遗忘程度测量

    Measuring Forgetting of Memorized Training Examples. (arXiv:2207.00099v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.00099](http://arxiv.org/abs/2207.00099)

    本文提出了一种测量机器学习模型对训练样本遗忘程度的技术，发现标准的图像、语音和语言模型在时间上确实会遗忘示例，确定性训练的模型不会遗忘。

    

    机器学习模型表现出两种似乎矛盾的现象: 训练数据的记忆化和各种形式的遗忘。在记忆化中，模型过拟合特定的训练样本，并变得容易受到隐私攻击的影响。在遗忘中，出现在训练早期的样本最终会被遗忘。在本研究中，我们将这些现象联系起来。我们提出了一种技术来衡量模型“遗忘”训练样本的具体细节，从而使它们变得不太容易受到最近没有看到的样本的隐私攻击的影响。我们展示了，虽然非凸模型可以在最坏情况下永久记忆化数据，但标准的图像、语音和语言模型在时间上确实会遗忘示例。我们确定了非确定性作为可能的解释，表明确定性训练的模型不会遗忘。我们的结果表明，当使用极大数据集进行训练时，观察到的隐私可能会出现在训练早期看到的样本上，例如用于预先训练模型的样本。

    Machine learning models exhibit two seemingly contradictory phenomena: training data memorization, and various forms of forgetting. In memorization, models overfit specific training examples and become susceptible to privacy attacks. In forgetting, examples which appeared early in training are forgotten by the end. In this work, we connect these phenomena. We propose a technique to measure to what extent models "forget" the specifics of training examples, becoming less susceptible to privacy attacks on examples they have not seen recently. We show that, while non-convex models can memorize data forever in the worst-case, standard image, speech, and language models empirically do forget examples over time. We identify nondeterminism as a potential explanation, showing that deterministically trained models do not forget. Our results suggest that examples seen early when training with extremely large datasets - for instance those examples used to pre-train a model - may observe privacy be
    
[^165]: 基于分数的异分布生成探索化学空间

    Exploring Chemical Space with Score-based Out-of-distribution Generation. (arXiv:2206.07632v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2206.07632](http://arxiv.org/abs/2206.07632)

    MOOD是一种基于分数的异分布扩散策略，它通过属性预测器的梯度进行条件生成，从而使得逆向时间扩散过程通过指导目标特性到高分数区域，从而允许我们搜索新颖且有意义的分子。

    

    现有的分子生成模型的一个众所周知的局限性是生成的分子与训练集中的分子高度相似。为了生成全新的分子以寻找更好的新颖药物，需要更强大的化学空间探索技术。为此，我们提出了分数-based异分布扩散策略(MOOD)，该策略在随机微分方程(SDE)的生成中结合了异分布(OOD)控制，同时通过简单的超参数控制而无需额外的成本。由于一些新颖分子可能无法满足现实药物的基本要求，MOOD利用属性预测器的梯度进行条件生成，从而使得逆向时间扩散过程通过指导目标特性（如蛋白质-配体相互作用、药物样性和可合成性）到高分数区域，从而允许MOOD搜索新颖且有意义的分子。

    A well-known limitation of existing molecular generative models is that the generated molecules highly resemble those in the training set. To generate truly novel molecules that may have even better properties for de novo drug discovery, more powerful exploration in the chemical space is necessary. To this end, we propose Molecular Out-Of-distribution Diffusion(MOOD), a score-based diffusion scheme that incorporates out-of-distribution (OOD) control in the generative stochastic differential equation (SDE) with simple control of a hyperparameter, thus requires no additional costs. Since some novel molecules may not meet the basic requirements of real-world drugs, MOOD performs conditional generation by utilizing the gradients from a property predictor that guides the reverse-time diffusion process to high-scoring regions according to target properties such as protein-ligand interactions, drug-likeness, and synthesizability. This allows MOOD to search for novel and meaningful molecules r
    
[^166]: 通过友好模型替换解决联合学习中的客户端退役问题

    Combating Client Dropout in Federated Learning via Friend Model Substitution. (arXiv:2205.13222v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13222](http://arxiv.org/abs/2205.13222)

    本研究提出了一种新算法FL-FDMS，可以在客户端退役的情况下，即时找到数据分布相似的客户端，并使用这些客户端的本地模型更新来替代缺失的客户端的更新，实现更高的学习准确性和更低的通信成本。

    

    联合学习是一种新的分布式机器学习框架，以其数据隐私和通信效率的优势而闻名。由于许多情况下完全客户端参与不可行，研究了主动选择/采样一部分客户端的部分参与FL算法，旨在实现接近全参与情况的学习性能。本文研究了一种被动部分客户端参与情况，这种情况理解不足得多，部分参与是由于外部事件（即客户端退出）而不是FL算法的决定而导致的。我们将具有客户端退役的FL视为FL问题中的一类特殊情况，其中客户端可以提交替代（可能不准确）的本地模型更新。根据我们的收敛分析，我们开发了一种新算法FL-FDMS，可以即时发现客户端的朋友（即数据分布相似的客户端），并使用朋友的本地模型更新来替代放弃的客户端的缺失更新。我们在假设下证明FL-FDMS在理论上收敛到最优全局模型，该假设足以涵盖许多现有的FL算法。实验结果表明，与几个基线相比，当客户端退出率适中到高时，FL-FDMS实现了更高的学习准确性和更低的通信成本。

    Federated learning (FL) is a new distributed machine learning framework known for its benefits on data privacy and communication efficiency. Since full client participation in many cases is infeasible due to constrained resources, partial participation FL algorithms have been investigated that proactively select/sample a subset of clients, aiming to achieve learning performance close to the full participation case. This paper studies a passive partial client participation scenario that is much less well understood, where partial participation is a result of external events, namely client dropout, rather than a decision of the FL algorithm. We cast FL with client dropout as a special case of a larger class of FL problems where clients can submit substitute (possibly inaccurate) local model updates. Based on our convergence analysis, we develop a new algorithm FL-FDMS that discovers friends of clients (i.e., clients whose data distributions are similar) on-the-fly and uses friends' local
    
[^167]: FedAdapter: 面向现代 NLP 的高效联邦学习

    FedAdapter: Efficient Federated Learning for Modern NLP. (arXiv:2205.10162v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.10162](http://arxiv.org/abs/2205.10162)

    FedAdapter是一个框架，可以自动化适配器配置以提高联邦学习的效率，有助于面向现代NLP的高效训练。

    

    基于 Transformer 预训练模型的出现，为 NLP 带来了卓越的性能和通用性。但是，微调预训练模型对下游任务的需要私有数据，而联邦学习是解决这一问题的黄金方法（即FedNLP）。然而，我们的测量表明，由于大型模型的存在以及相应的高网络/计算成本，FedNLP无法进行。为了实现实用的FedNLP，我们确定了适配器作为关键构建块，这是一种插入各种模型层的小瓶颈模块。关键挑战是正确配置适配器的深度和宽度，这对训练速度和效率非常敏感。并不存在适用于所有情况的最佳配置：最佳选择因下游NLP任务、所需模型精度和移动资源而异。为了自动化适配器配置，我们提出了 FedAdapter，这是一个增强现有 FedNLP 的框架，具有两个新颖的设计。

    Transformer-based pre-trained models have revolutionized NLP for superior performance and generality. Fine-tuning pre-trained models for downstream tasks often requires private data, for which federated learning is the de-facto approach (i.e., FedNLP). However, our measurements show that FedNLP is prohibitively slow due to the large model sizes and the resultant high network/computation cost. Towards practical FedNLP, we identify as the key building blocks adapters, small bottleneck modules inserted at a variety of model layers. A key challenge is to properly configure the depth and width of adapters, to which the training speed and efficiency is highly sensitive. No silver-bullet configuration exists: the optimal choice varies across downstream NLP tasks, desired model accuracy, and mobile resources. To automate adapter configuration, we propose FedAdapter, a framework that enhances the existing FedNLP with two novel designs. First, FedAdapter progressively upgrades the adapter config
    
[^168]: 可证明安全的强化学习：理论和实验比较

    Provably Safe Reinforcement Learning: A Theoretical and Experimental Comparison. (arXiv:2205.06750v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.06750](http://arxiv.org/abs/2205.06750)

    该论文介绍了现有可证明安全的RL方法的分类，并在倒立摆和四旋翼稳定任务上进行了实验，证明这些方法都是安全的且表现与不安全的方法相媲美。

    

    确保强化学习（RL）算法的安全性对于开发其在许多实际任务中的潜力至关重要。然而，普通的RL并不保证安全。近年来，已经提出了几种方法来通过设计来提供RL的安全保证。然而，这些可证明安全的RL方法还没有进行全面比较。因此，我们介绍了现有可证明安全的RL方法的分类，介绍了连续和离散动作空间的理论基础，并在实验中对这些方法的性能进行了评估。这些方法根据安全方法如何适应动作进行分类：动作替换、动作投影和动作掩蔽。我们在倒立摆和四旋翼稳定任务上的实验表明，所有可证明安全的方法确实是安全的。此外，它们的训练表现可与不安全的基线相媲美。基准测试表明，应选择不同的可证明安全的RL方法。

    Ensuring safety of reinforcement learning (RL) algorithms is crucial to unlock their potential for many real-world tasks. However, vanilla RL does not guarantee safety. In recent years, several methods have been proposed to provide safety guarantees for RL by design. Yet, there is no comprehensive comparison of these provably safe RL methods. We therefore introduce a categorization of existing provably safe RL methods, present the theoretical foundations for both continuous and discrete action spaces, and benchmark the methods' performance empirically. The methods are categorized based on how the action is adapted by the safety method: action replacement, action projection, and action masking. Our experiments on an inverted pendulum and quadrotor stabilization task show that all provably safe methods are indeed always safe. Furthermore, their trained performance is comparable to unsafe baselines. The benchmarking suggests that different provably safe RL approaches should be selected de
    
[^169]: 无服务器端训练的一次性联邦学习

    One-shot Federated Learning without Server-side Training. (arXiv:2204.12493v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.12493](http://arxiv.org/abs/2204.12493)

    本文提出了一种无需额外训练阶段和公共数据集的交叉场所的一次性联邦学习算法MA-Echo，该算法通过调和最优区域迭代更新所有本地模型的参数，从而将它们拉近到一个低损失区域，而不会影响模型在自身数据集上的性能。

    

    联邦学习是一种新的机器学习范式，旨在保护隐私。最近，由于传统联邦学习的高通信代价，一次性联邦学习开始受到关注，以减少客户端和服务器之间的通信成本。本文提出了一种无服务器端训练的交叉场所的有效算法MA-Echo，通过探索公共调和最优区域，迭代更新所有本地模型的参数，将它们拉近到损失表面上的一个低损失区域，而不会影响它们在其自身数据集上的性能。

    Federated Learning (FL) has recently made significant progress as a new machine learning paradigm for privacy protection. Due to the high communication cost of traditional FL, one-shot federated learning is gaining popularity as a way to reduce communication cost between clients and the server. Most of the existing one-shot FL methods are based on Knowledge Distillation; however, {distillation based approach requires an extra training phase and depends on publicly available data sets or generated pseudo samples.} In this work, we consider a novel and challenging cross-silo setting: performing a single round of parameter aggregation on the local models without server-side training. In this setting, we propose an effective algorithm for Model Aggregation via Exploring Common Harmonized Optima (MA-Echo), which iteratively updates the parameters of all local models to bring them close to a common low-loss area on the loss surface, without harming performance on their own data sets at the s
    
[^170]: 强化型异构信息网络下MOOC概念推荐研究

    Reinforced MOOCs Concept Recommendation in Heterogeneous Information Networks. (arXiv:2203.11011v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2203.11011](http://arxiv.org/abs/2203.11011)

    本文提出了一种强化学习框架下基于异构信息网络的概念推荐方法，可以更好地向不同专业水平的用户精细推荐知识。

    

    大规模在线开放课程（MOOCs）通过互联网提供开放访问和广泛的互动参与，正在迅速成为在线和远程学习的首选方式。许多MOOC平台为用户提供课程推荐服务，以提高用户的学习体验。尽管这项服务很有用，但如果直接向用户推荐课程可能会忽视他们不同的专业水平，因此本文考虑了概念推荐这个问题，可以精细地向用户推荐知识。我们提出了一种新颖的方法——HinCRec-RL来解决MOOC中的概念推荐问题，该方法基于异构信息网络和强化学习。具体而言，我们提出将概念推荐问题塑造在强化学习框架内，以表征用户和知识概念之间的动态交互。

    Massive open online courses (MOOCs), which offer open access and widespread interactive participation through the internet, are quickly becoming the preferred method for online and remote learning. Several MOOC platforms offer the service of course recommendation to users, to improve the learning experience of users. Despite the usefulness of this service, we consider that recommending courses to users directly may neglect their varying degrees of expertise. To mitigate this gap, we examine an interesting problem of concept recommendation in this paper, which can be viewed as recommending knowledge to users in a fine-grained way. We put forward a novel approach, termed HinCRec-RL, for Concept Recommendation in MOOCs, which is based on Heterogeneous Information Networks and Reinforcement Learning. In particular, we propose to shape the problem of concept recommendation within a reinforcement learning framework to characterize the dynamic interaction between users and knowledge concepts 
    
[^171]: 平滑风险度量的策略梯度优化方法

    A policy gradient approach for optimization of smooth risk measures. (arXiv:2202.11046v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.11046](http://arxiv.org/abs/2202.11046)

    本文介绍了一种应用于on-policy和off-policy RL情况下的策略梯度算法，用于最小化广义平滑风险度量，能够收敛到平滑风险度量的稳态点，并适用于均值-方差和畸变风险度量的优化。

    

    我们提出了一种策略梯度算法，用于解决风险敏感的强化学习问题，包括on-policy和off-policy情况。我们考虑时间段马尔可夫决策过程，并利用累积折扣奖励的广义平滑风险度量来建模风险。我们提出了两个模板策略梯度算法，分别在on-policy和off-policy RL情况下优化平滑风险度量。我们推导出非渐进性界，量化了我们提出的算法收敛到平滑风险度量的稳态点的速率。作为特例，我们确定了我们的算法分别应用于均值-方差和畸变风险度量的优化。

    We propose policy gradient algorithms for solving a risk-sensitive reinforcement learning problem in on-policy as well as off-policy settings. We consider episodic Markov decision processes, and model the risk using the broad class of smooth risk measures of the cumulative discounted reward. We propose two template policy gradient algorithms that optimize a smooth risk measure in on-policy and off-policy RL settings, respectively. We derive non-asymptotic bounds that quantify the rate of convergence to our proposed algorithms to a stationary point of the smooth risk measure. As special cases, we establish that our algorithms apply to the optimization of mean-variance and distortion risk measures, respectively.
    
[^172]: 基于Wi-Fi信道数据的人与人互动识别的前瞻性方法——使用具有GUI应用程序实现的注意力双向门控循环神经网络(arXiv:2202.08146v4 [cs.LG] UPDATED)

    A Prospective Approach for Human-to-Human Interaction Recognition from Wi-Fi Channel Data using Attention Bidirectional Gated Recurrent Neural Network with GUI Application Implementation. (arXiv:2202.08146v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.08146](http://arxiv.org/abs/2202.08146)

    该论文提出了一种基于Wi-Fi信道数据的人与人互动识别的新方法，使用注意力双向门控循环神经网络实现高精度的实时处理，准确率达到98.22%，并提供了一个GUI应用程序方便实时应用。

    

    由于最近的技术进步、人工智能算法、智能城市和社会经济变革的需要，人类活动识别(HAR)研究已经获得了重要的动力。然而，现有的基于计算机视觉和传感器的HAR解决方案存在隐私问题、存储和功率消耗问题以及佩戴传感器的不适感，这促使研究人员观察到HAR研究的范式转变。作为回应，基于WiFi的HAR因其更粗粒度的信道状态信息的可用性而越来越受欢迎。然而，现有的基于WiFi的HAR方法仅限于对在相等时间内执行的独立和非并发人类活动进行分类。与最近的研究不同的是，我们的研究利用了多输入多输出通信链路，其中发射器是WiFi路由器，接收器是配备了Intel 5300 NIC的智能手机。我们提出了一种基于注意力双向门控循环神经网络(ABiGRNN)的WiFi信道数据的人与人互动(HHI)识别的新方法，该方法允许高精度的实时处理。我们使用HHI活动数据集评估我们的方法，实现了98.22%的准确率，优于现有的最先进方法。此外，我们提供了一个图形用户界面(GUI)应用程序，可以在实时场景中轻松实现我们的方法来识别HHI。

    Human Activity Recognition (HAR) research has gained significant momentum due to recent technological advancements, artificial intelligence algorithms, the need for smart cities, and socioeconomic transformation. However, existing computer vision and sensor-based HAR solutions have limitations such as privacy issues, memory and power consumption, and discomfort in wearing sensors for which researchers are observing a paradigm shift in HAR research. In response, WiFi-based HAR is gaining popularity due to the availability of more coarse-grained Channel State Information. However, existing WiFi-based HAR approaches are limited to classifying independent and non-concurrent human activities performed within equal time duration. Recent research commonly utilizes a Single Input Multiple Output communication link with a WiFi signal of 5 GHz channel frequency, using two WiFi routers or two Intel 5300 NICs as transmitter-receiver. Our study, on the other hand, utilizes a Multiple Input Multiple
    
[^173]: 联邦学习中的比例公平性

    Proportional Fairness in Federated Learning. (arXiv:2202.01666v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.01666](http://arxiv.org/abs/2202.01666)

    本文提出了用于保证联邦学习中公平性的比例公平性 (PF) 概念，并提出了一种新颖易于实现的算法 PropFair，能够在所有客户端平均性能和最差 10% 客户端平均性能之间达到良好平衡。

    

    随着联邦学习系统在现实世界中越来越广泛地部署，保证公平性变得至关重要但也面临着挑战，即需要为众多不同的客户端提供合理满意的表现。在本文中，我们引入并研究了联邦学习中一种新的公平性概念，即比例公平性 (PF)，它基于每个客户端性能的相对变化。通过与交易博弈的联系，我们提出了 PropFair，一种新颖且易于实现的算法，用于在联邦学习中寻找比例公平解，并研究了其收敛性质。通过对视觉和语言数据集的广泛实验，我们证明 PropFair 能够大致找到 PF 解，并在所有客户端的平均性能和最差 10% 客户端的平均性能之间实现良好的平衡。我们的代码可在 \url{https://github.com/huawei-noah/Federated-Learning/tree/main/FairFL} 找到。

    With the increasingly broad deployment of federated learning (FL) systems in the real world, it is critical but challenging to ensure fairness in FL, i.e. reasonably satisfactory performances for each of the numerous diverse clients. In this work, we introduce and study a new fairness notion in FL, called proportional fairness (PF), which is based on the relative change of each client's performance. From its connection with the bargaining games, we propose PropFair, a novel and easy-to-implement algorithm for finding proportionally fair solutions in FL and study its convergence properties. Through extensive experiments on vision and language datasets, we demonstrate that PropFair can approximately find PF solutions, and it achieves a good balance between the average performances of all clients and of the worst 10% clients. Our code is available at \url{https://github.com/huawei-noah/Federated-Learning/tree/main/FairFL}.
    
[^174]: 在不平衡类别和新类别的约束下对演化图进行终身学习

    Lifelong Learning on Evolving Graphs Under the Constraints of Imbalanced Classes and New Classes. (arXiv:2112.10558v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.10558](http://arxiv.org/abs/2112.10558)

    本文提出了gDOC方法和gDOC+方法，以解决在不平衡类别分布和新类别约束下进行演化图终身学习的问题。在标准基准数据集上，我们的方法优于多个基线和最先进的方法。

    

    终身图学习解决了不断适应演化图的图神经网络模型的问题。我们在本文中解决了终身图学习的两个关键挑战：处理新的类别和应对不平衡的类别分布。这两个挑战的结合尤其相关，因为新出现的类别通常只占数据的一小部分，加剧了原本就偏斜的类别分布。我们做出了几项贡献：首先，我们展示了未标记数据的数量对结果没有影响，这是一个终身学习的关键前提。其次，我们尝试了不同的标签率，并展示了我们的方法可以用极少量的标注节点就能表现良好。第三，我们提出了gDOC方法，在不平衡类别分布的约束下检测新类别。关键的部分是考虑到了类别不平衡的加权二元交叉熵损失函数。第四，我们提出了gDOC+方法，将gDOC与主动学习策略相结合，更有效地利用有限的标注数据检测新类别。我们的实验表明，我们的方法在演化图的终身学习标准基准数据集上优于多个基线和最先进的方法。

    Lifelong graph learning deals with the problem of continually adapting graph neural network (GNN) models to changes in evolving graphs. We address two critical challenges of lifelong graph learning in this work: dealing with new classes and tackling imbalanced class distributions. The combination of these two challenges is particularly relevant since newly emerging classes typically resemble only a tiny fraction of the data, adding to the already skewed class distribution. We make several contributions: First, we show that the amount of unlabeled data does not influence the results, which is an essential prerequisite for lifelong learning on a sequence of tasks. Second, we experiment with different label rates and show that our methods can perform well with only a tiny fraction of annotated nodes. Third, we propose the gDOC method to detect new classes under the constraint of having an imbalanced class distribution. The critical ingredient is a weighted binary cross-entropy loss functi
    
[^175]: 偏差控制式评估在部分可观察马尔可夫决策过程中的应用

    Off-Policy Evaluation in Partially Observed Markov Decision Processes under Sequential Ignorability. (arXiv:2110.12343v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.12343](http://arxiv.org/abs/2110.12343)

    本文提出基于部分历史的重要性权重的估算器，可一致地估计目标策略的稳态平均回报，在POMDP中进行偏差控制式评估比在（完全观察到的）马尔可夫决策过程中进行偏差控制式评估更具挑战性，但比强化学习中的模型自由偏差控制式评估更容易。

    

    本文着眼于动态治疗规则下可观察马尔可夫决策过程（POMDP）中的偏差控制式评估，基于假设系统为部分可观察马尔可夫决策过程，提出了一种估算器，即基于部分历史的重要性权重，并展示了当从行为策略中获得足够多的数据后，该估算器可以一致地估计目标策略的稳态平均回报。我们提供了估算器误差的上界，其多项式衰减于观测量的数量（即轨迹的数量乘以它们的长度），指数取决于目标策略和行为策略的重叠，以及基础系统的混合时间。此外，我们证明了该收敛速率在混合和重叠方面的假定下是极小化的。我们的结果表明，在POMDP中进行偏差控制式评估比在（完全观察到的）马尔可夫决策过程中进行偏差控制式评估更具挑战性，但比强化学习中的模型自由偏差控制式评估更容易。

    We consider off-policy evaluation of dynamic treatment rules under sequential ignorability, given an assumption that the underlying system can be modeled as a partially observed Markov decision process (POMDP). We propose an estimator, partial history importance weighting, and show that it can consistently estimate the stationary mean rewards of a target policy given long enough draws from the behavior policy. We provide an upper bound on its error that decays polynomially in the number of observations (i.e., the number of trajectories times their length), with an exponent that depends on the overlap of the target and behavior policies, and on the mixing time of the underlying system. Furthermore, we show that this rate of convergence is minimax given only our assumptions on mixing and overlap. Our results establish that off-policy evaluation in POMDPs is strictly harder than off-policy evaluation in (fully observed) Markov decision processes, but strictly easier than model-free off-po
    
[^176]: 使用RBF神经网络优化恒流并联微型泵

    The Optimization of the Constant Flow Parallel Micropump Using RBF Neural Network. (arXiv:2109.08717v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.08717](http://arxiv.org/abs/2109.08717)

    本文通过实施RBF神经网络，提出了重叠时间的概念来优化恒流并联机械位移微型泵，在将左右泵互换角色的往复运动期间最小化压力脉冲。

    

    本文旨在优化具有并联泵腔和被动止回阀的恒流并联机械位移微型泵的性能。关键任务是在左右泵互换吸入和输送角色时的往复运动期间，将由反流引起的压力脉冲最小化，这对稳定的恒流率产生负面影响。以往的工作尝试通过被动止回阀的机械设计来解决这个问题。本文提出了重叠时间的新概念，并通过实施RBF神经网络来实现从控制理论角度解决这个问题，同时使用无监督学习和监督学习对其进行了训练。实验结果表明，压力脉冲在0.15-0.25 MPa的范围内得到了优化，相比于最大泵工作压力40 MPa，这是一个重大的改进。

    The objective of this work is to optimize the performance of a constant flow parallel mechanical displacement micropump, which has parallel pump chambers and incorporates passive check valves. The critical task is to minimize the pressure pulse caused by regurgitation, which negatively impacts the constant flow rate, during the reciprocating motion when the left and right pumps interchange their role of aspiration and transfusion. Previous works attempt to solve this issue via the mechanical design of passive check valves. In this work, the novel concept of overlap time is proposed, and the issue is solved from the aspect of control theory by implementing a RBF neural network trained by both unsupervised and supervised learning. The experimental results indicate that the pressure pulse is optimized in the range of 0.15 - 0.25 MPa, which is a significant improvement compared to the maximum pump working pressure of 40 MPa.
    
[^177]: 利用集成机器学习从常规血液检查中开发无风险的COVID-19筛查算法

    Development of a Risk-Free COVID-19 Screening Algorithm from Routine Blood Tests Using Ensemble Machine Learning. (arXiv:2108.05660v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.05660](http://arxiv.org/abs/2108.05660)

    本研究提出了一种风险低且高精度的堆叠集成机器学习模型，可以从常规血液检查中识别COVID-19患者，具有很高的准确率、精确度和召回率。

    

    反转录聚合酶链反应（RT-PCR）是辨别COVID-19感染的银弹诊断检测。快速抗原检测是一种筛查检测，可在15分钟内识别COVID-19阳性患者，但其灵敏度低于PCR检测。本研究通过利用COVID-19患者免疫和血液学资料的参数偏差，提出了一种风险低且高精度的堆叠集成机器学习模型，从常规血液检查中识别COVID-19患者，具有很高的准确率、精确度和召回率。

    The Reverse Transcription Polymerase Chain Reaction (RTPCR)} test is the silver bullet diagnostic test to discern COVID infection. Rapid antigen detection is a screening test to identify COVID positive patients in little as 15 minutes, but has a lower sensitivity than the PCR tests. Besides having multiple standardized test kits, many people are getting infected and either recovering or dying even before the test due to the shortage and cost of kits, lack of indispensable specialists and labs, time-consuming result compared to bulk population especially in developing and underdeveloped countries. Intrigued by the parametric deviations in immunological and hematological profile of a COVID patient, this research work leveraged the concept of COVID-19 detection by proposing a risk-free and highly accurate Stacked Ensemble Machine Learning model to identify a COVID patient from communally available-widespread-cheap routine blood tests which gives a promising accuracy, precision, recall and
    
[^178]: 高效的基于模型的多智能体均场强化学习算法

    Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning. (arXiv:2107.04050v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2107.04050](http://arxiv.org/abs/2107.04050)

    本文针对均场控制问题提出了一种高效的基于模型的强化学习算法$M^3-UCRL$，在未知系统动力学的情况下，该算法可平衡探索和利用，并实现了可证明的问题求解，具有较好的表现。

    

    多智能体系统的学习充满挑战，包括智能体相互作用所引入的非稳态和状态和动作空间的组合性质等多个因素。本文关注的是均场控制问题，其假设存在无限数量的相同智能体，旨在共同最大化收益。针对未知系统动力学的情况，本文提出了一种高效的基于模型的强化学习算法$M^3-UCRL$，在策略学习期间平衡探索和利用，并实现了这一问题的可证明求解。该算法在多个基准问题上的表现优于多种最先进的方法。

    Learning in multi-agent systems is highly challenging due to several factors including the non-stationarity introduced by agents' interactions and the combinatorial nature of their state and action spaces. In particular, we consider the Mean-Field Control (MFC) problem which assumes an asymptotically infinite population of identical agents that aim to collaboratively maximize the collective reward. In many cases, solutions of an MFC problem are good approximations for large systems, hence, efficient learning for MFC is valuable for the analogous discrete agent setting with many agents. Specifically, we focus on the case of unknown system dynamics where the goal is to simultaneously optimize for the rewards and learn from experience. We propose an efficient model-based reinforcement learning algorithm, $M^3-UCRL$, that runs in episodes, balances between exploration and exploitation during policy learning, and provably solves this problem. Our main theoretical contributions are the first
    
[^179]: 预算约束下的动态Blotto博弈在线学习研究

    Online Learning in Budget-Constrained Dynamic Colonel Blotto Games. (arXiv:2103.12833v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.12833](http://arxiv.org/abs/2103.12833)

    本文研究了动态Blotto博弈中有限资源的战略分配问题，使用在线学习方法和带背包和组合的赌博机分析该问题。

    

    本文研究了在动态环境下使用Blotto博弈(CBG)对有限资源进行战略分配，并采用在线学习方法分析了该问题。在该模型中，一方是拥有有限军队的学习者，另一方是对手。在每轮中，学习者与对手进行单次Blotto博弈，并根据过去的观察结果战略性地确定在战场上的军队分配。对手随机选择其分配行动，其分布对学习者不可知。学习者的目标是在不违反预算约束的情况下，通过遵循学习算法实现的累积奖励与最佳混合策略的累积奖励之间的差异最小化。 在动态CBG中的学习是在组合赌博机和背包赌博机的框架下分析的。

    In this paper, we study the strategic allocation of limited resources using a Colonel Blotto game (CBG) under a dynamic setting and analyze the problem using an online learning approach. In this model, one of the players is a learner who has limited troops to allocate over a finite time horizon, and the other player is an adversary. In each round, the learner plays a one-shot Colonel Blotto game with the adversary and strategically determines the allocation of troops among battlefields based on past observations. The adversary chooses its allocation action randomly from some fixed distribution that is unknown to the learner. The learner's objective is to minimize its regret, which is the difference between the cumulative reward of the best mixed strategy and the realized cumulative reward by following a learning algorithm while not violating the budget constraint. The learning in dynamic CBG is analyzed under the framework of combinatorial bandits and bandits with knapsacks. We first c
    
[^180]: 随机对偶动态规划的复杂性分析

    Complexity of Stochastic Dual Dynamic Programming. (arXiv:1912.07702v9 [math.OC] UPDATED)

    [http://arxiv.org/abs/1912.07702](http://arxiv.org/abs/1912.07702)

    本文分析了随机对偶动态规划算法的迭代复杂度，发现当阶段数增加时，某些确定性算法的复杂度略微增加，而随机对偶动态规划的复杂度则呈指数增长。

    

    随机对偶动态规划是一种用于多阶段随机优化的割平面类型算法，源于30年前。尽管该方法在实践中很受欢迎，但目前尚不存在任何关于该方法收敛速度的分析。本文首先通过引入新颖的数学工具，包括搜索点的饱和性，为解决相对简单的多阶段优化问题的基本动态割平面方法建立了所需的迭代次数，即迭代复杂性。然后，我们进一步精化这些基本工具，针对标准阶段独立性假设下的更一般的多阶段随机优化问题，建立了确定性和随机对偶动态规划方法的迭代复杂性。我们的结果表明，某些确定性算法的复杂度随着阶段数$T$的增加而轻微增加，事实上对于折扣问题是$T$的线性函数。然而，对于随机对偶动态规划，我们发现其迭代复杂度对阶段数有指数依赖性。我们还提供了数值实验以验证我们的理论分析，并展示了确定性对偶动态规划的线性增长确实可能在实践中发生。

    Stochastic dual dynamic programming is a cutting plane type algorithm for multi-stage stochastic optimization originated about 30 years ago. In spite of its popularity in practice, there does not exist any analysis on the convergence rates of this method. In this paper, we first establish the number of iterations, i.e., iteration complexity, required by a basic dynamic cutting plane method for solving relatively simple multi-stage optimization problems, by introducing novel mathematical tools including the saturation of search points. We then refine these basic tools and establish the iteration complexity for both deterministic and stochastic dual dynamic programming methods for solving more general multi-stage stochastic optimization problems under the standard stage-wise independence assumption. Our results indicate that the complexity of some deterministic variants of these methods mildly increases with the number of stages $T$, in fact linearly dependent on $T$ for discounted probl
    
[^181]: 一种用于比较潜变量模型的核Stein检验

    A Kernel Stein Test for Comparing Latent Variable Models. (arXiv:1907.00586v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1907.00586](http://arxiv.org/abs/1907.00586)

    本研究提出了一种核Stein检验方法，可用于比较具有潜变量的模型，相比于当前方法，性能更好。

    

    我们提出了一种基于核的非参数拟合优度检验，旨在比较两个模型，这两个模型都可能具有未观测的潜变量，且观测变量的边缘分布不可观测。所提出的检验将最近提出的核Stein距离(KSD)检验(Liu et al., 2016, Chwialkowski et al., 2016, Yang et al., 2018)推广到潜变量模型的情况下，是一个比之前处理的全观测模型更加通用的类别。新的检验通过适当的校准阈值，得到良好控制的一类错误。对于某些具有低维潜在结构和高维观测的模型，在我们的测试中，显著优于基于模型样本和不利用潜在结构的相对最大平均距离检测。

    We propose a kernel-based nonparametric test of relative goodness of fit, where the goal is to compare two models, both of which may have unobserved latent variables, such that the marginal distribution of the observed variables is intractable. The proposed test generalizes the recently proposed kernel Stein discrepancy (KSD) tests (Liu et al., 2016, Chwialkowski et al., 2016, Yang et al., 2018) to the case of latent variable models, a much more general class than the fully observed models treated previously. The new test, with a properly calibrated threshold, has a well-controlled type-I error. In the case of certain models with low-dimensional latent structure and high-dimensional observations, our test significantly outperforms the relative Maximum Mean Discrepancy test, which is based on samples from the models and does not exploit the latent structure.
    

