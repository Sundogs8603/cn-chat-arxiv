# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Theoretically Principled Trade-off for Stateful Defenses against Query-Based Black-Box Attacks.](http://arxiv.org/abs/2307.16331) | 本文提供了一种针对有状态防御的检测率和误报率之间的权衡的理论描述，并给出了特征提取器检测率的上界。 |
| [^2] | [RoseNNa: A performant, portable library for neural network inference with application to computational fluid dynamics.](http://arxiv.org/abs/2307.16322) | RoseNNa是一种用于神经网络推断的高效、可移植的库，通过自动模型转换和集成，无需修改现有求解器代码即可将神经网络应用于计算流体动力学问题。 |
| [^3] | [Towards Practical Robustness Auditing for Linear Regression.](http://arxiv.org/abs/2307.16315) | 本论文研究了实用的算法，用于查找或证伪数据集中对普通最小二乘回归具有影响的小子集。通过实证研究发现，这些算法方法在鲁棒性检查中表现良好，并提供了对低维回归问题的有用检查。但对于高维回归问题，计算瓶颈仍然存在。通过使用新颖的谱算法，我们取得了一些进展。 |
| [^4] | [Mask-guided Data Augmentation for Multiparametric MRI Generation with a Rare Hepatocellular Carcinoma.](http://arxiv.org/abs/2307.16314) | 本文提出了一种基于掩模的数据增强架构，通过生成式深度学习方法合成具有巨大细小肝细胞癌的多参数MRI图像，并通过肿瘤掩模实现逼真的生成效果。 |
| [^5] | [You Shall not Pass: the Zero-Gradient Problem in Predict and Optimize for Convex Optimization.](http://arxiv.org/abs/2307.16304) | 这篇论文介绍了预测和优化中的一个问题——零梯度问题，并提出了解决方法。通过利用微分优化的数学特性和真实世界基准的验证，该方法解决了这个问题。 |
| [^6] | [Predicting delays in Indian lower courts using AutoML and Decision Forests.](http://arxiv.org/abs/2307.16285) | 本文使用AutoML和决策森林构建了一个分类模型，通过案件信息预测印度下级法院的延迟。最佳模型的准确率为81.4％，该研究证明了基于相关数据点的AI模型在预测印度法院延迟方面的可行性。 |
| [^7] | [zkDL: Efficient Zero-Knowledge Proofs of Deep Learning Training.](http://arxiv.org/abs/2307.16273) | zkDL是一种高效的深度学习训练零知识证明，通过zkReLU协议和新颖的算术电路构建方案，实现了对深度网络训练过程的有效验证。 |
| [^8] | [DRL4Route: A Deep Reinforcement Learning Framework for Pick-up and Delivery Route Prediction.](http://arxiv.org/abs/2307.16246) | DRL4Route是一种用于接送路线预测的深度强化学习框架，结合了深度学习模型的行为学习能力和强化学习的非可微分目标优化能力，解决了训练和测试标准不匹配的问题。 |
| [^9] | [Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey.](http://arxiv.org/abs/2307.16236) | 本综述调查了一系列生物启发的突触可塑性模型在深度学习中的应用，并与脉冲神经网络中的可塑性模型进行了联系。生物启发深度学习代表了一个令人兴奋的研究方向，旨在推动当前技术的发展以及对智能的理解。 |
| [^10] | [Spiking Neural Networks and Bio-Inspired Supervised Deep Learning: A Survey.](http://arxiv.org/abs/2307.16235) | 这篇综述文章回顾了基于生物学启发的人工智能方法，介绍了脉冲神经网络模型，并讨论了脉冲神经网络训练的挑战以及生物启发的深度学习方法。 |
| [^11] | [Robust Electric Vehicle Balancing of Autonomous Mobility-On-Demand System: A Multi-Agent Reinforcement Learning Approach.](http://arxiv.org/abs/2307.16228) | 本文提出了一种基于多智能体强化学习的方法来解决电动车在自主移动出行系统中的平衡问题，并考虑了车辆供应和需求的不确定性。 |
| [^12] | [Optimizing the Neural Network Training for OCR Error Correction of Historical Hebrew Texts.](http://arxiv.org/abs/2307.16220) | 这篇论文提出了一种使用较少的手动创建数据训练的轻量级神经网络方法，用于希伯来文OCR后校正。研究目标是开发自动生成训练数据的方法，改善OCR后校正结果，并研究最适用于历史文档的OCR后校正的数据集类型。 |
| [^13] | [Text Analysis Using Deep Neural Networks in Digital Humanities and Information Science.](http://arxiv.org/abs/2307.16217) | 本论文研究了在数字人文研究中使用深度神经网络进行文本分析的挑战，包括训练数据的可用性和领域适应的需求。 |
| [^14] | [Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs.](http://arxiv.org/abs/2307.16214) | 本研究提出一种用于家族谱问答的深度神经网络方法，该方法将家族谱数据表示为知识图并与非结构化文本结合，使用Transformer模型进行训练。这种方法解决了家族谱领域中模型无法处理图结构和缺乏训练数据集的问题。 |
| [^15] | [Toward a Period-Specific Optimized Neural Network for OCR Error Correction of Historical Hebrew Texts.](http://arxiv.org/abs/2307.16213) | 本研究针对历史希伯来文本OCR错误校正的问题，提出了一种特定时期优化的神经网络模型。然而，由于希伯来语是形态丰富语言，神经网络所需的训练数据集不够充足，且最佳的网络结构和超参数值尚不明确。另外，不同流派和时期的语言变化可能会影响OCR后校正模型的准确性。 |
| [^16] | [Robust Multi-Agent Reinforcement Learning with State Uncertainty.](http://arxiv.org/abs/2307.16212) | 本论文研究了在多智能体强化学习中状态不确定性的问题。通过引入状态扰动对手，将该问题建模为马尔可夫博弈，并提出了鲁棒均衡作为解决概念。 |
| [^17] | [Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment.](http://arxiv.org/abs/2307.16210) | 在多模态实体对齐中，现有的方法忽视了视觉图像的不完整性和模糊性，本文通过分析表明模型在面对不完整性时容易出现过拟合和性能下降的问题。 |
| [^18] | [Around the GLOBE: Numerical Aggregation Question-Answering on Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks.](http://arxiv.org/abs/2307.16208) | 本文介绍了一种在异构家谱知识图谱上进行数字聚合问答的方法，在基因谱领域提出了自然语言问题并获得准确答案的能力还未被充分研究。 |
| [^19] | [Deep Convolutional Neural Networks with Zero-Padding: Feature Extraction and Learning.](http://arxiv.org/abs/2307.16203) | 本文研究了在特征提取和学习中使用零填充的深度卷积神经网络的性能，并证明了相比于全连接网络，零填充的DCNNs具有更好的性能和平移不变性。 |
| [^20] | [Shuffled Differentially Private Federated Learning for Time Series Data Analytics.](http://arxiv.org/abs/2307.16196) | 该论文介绍了一种针对时间序列数据的隐私保护联邦学习算法，通过使用局部差分隐私和洗牌技术，有效实现了在保护隐私的同时提高模型准确性。实验结果表明该方法在时间序列数据分析中具有良好的应用价值。 |
| [^21] | [An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training.](http://arxiv.org/abs/2307.16189) | 这项研究探讨了16位计算中机器学习模型的数值不稳定性问题，并提出了一种基于Adam优化器的新方法来提高16位神经网络的学习过程的鲁棒性。 |
| [^22] | [ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2307.16186) | 本文提出了一种利用对称性先验知识的框架来解决多Agent强化学习中的数据效率问题，通过将数据增强和一致性损失集成到现有方法中，能够提高模型训练效率，并且泛化性能良好。 |
| [^23] | [Unified Model for Image, Video, Audio and Language Tasks.](http://arxiv.org/abs/2307.16184) | UnIVAL是一个统一的模型，可以同时支持图像、视频、音频和语言任务。 |
| [^24] | [Redundancy-aware unsupervised rankings for collections of gene sets.](http://arxiv.org/abs/2307.16182) | 该论文提出了一种冗余感知的无监督排名方法，用于对基因集合进行排序。该方法考虑了基因集合的重要性分数以及集合中的单个元素和大小分布，并通过一种技巧规避了计算复杂度的问题。 |
| [^25] | [Adaptive learning of density ratios in RKHS.](http://arxiv.org/abs/2307.16164) | 该论文研究在再生核希尔伯特空间中的一类密度比率估计方法，提出了一种自适应学习的参数选择原则，并在有限样本情况下推导出新的误差界。其方法在二次损失的情况下实现了极小化最优误差率。 |
| [^26] | [Variance Control for Distributional Reinforcement Learning.](http://arxiv.org/abs/2307.16152) | 本研究针对分布式强化学习中Q函数估计器的有效性进行了分析，提出了一种新的估计器QEM和DRL算法QEMRL，在Atari和Mujoco基准任务上取得了显著的样本效率和收敛性能的提升。 |
| [^27] | [An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid.](http://arxiv.org/abs/2307.16149) | 这篇论文提出了一种利用LSTM和DDPM相结合的方案来解决智能电网系统中的能量盗窃检测和预测问题。通过重构和预测误差，系统能够准确识别能量盗窃的实例，并在实验中表现出较好的性能。 |
| [^28] | [Pupil Learning Mechanism.](http://arxiv.org/abs/2307.16141) | 本研究提出了瞳孔学习机制（PLM），通过瞳孔学习过程来修改神经网络结构和权重，解决梯度消失和过拟合问题。实验证实了PLM模块设计的有效性，并证明了该模型相对于其他模型的优势。 |
| [^29] | [User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination.](http://arxiv.org/abs/2307.16139) | 本文提出了一种用户可控的机制，用于调节大型语言模型在生成回应时创造力和对外部知识的忠诚度之间的平衡。这种机制通过在训练过程中引入数值标记，并使用自动化过程计算标记的程度，从而实现用户对模型的依赖程度的控制。 |
| [^30] | [Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems.](http://arxiv.org/abs/2307.16120) | 本文提出了一种使用循环动量加速的深度展开网络，该网络能够有效应用于非线性逆向成像问题。通过利用长短期记忆循环神经网络学习和保留先前梯度的知识，该方法在两个非线性逆向问题上获得了良好的结果。 |
| [^31] | [TMPNN: High-Order Polynomial Regression Based on Taylor Map Factorization.](http://arxiv.org/abs/2307.16105) | 本文提出了一种基于Taylor映射因式分解的高阶多项式回归方法，可以实现多目标回归和捕捉目标之间的内在关系。通过在不同的数据集上进行基准测试，表明该方法在特定任务上优于其他回归方法。 |
| [^32] | [AI Increases Global Access to Reliable Flood Forecasts.](http://arxiv.org/abs/2307.16104) | 本研究开发了一个人工智能模型，可以准确预测未经测量流域的极端水文事件，从而提高了全球洪水预警的覆盖范围。 |
| [^33] | [On Neural Network approximation of ideal adversarial attack and convergence of adversarial training.](http://arxiv.org/abs/2307.16099) | 这项研究通过使用神经网络对理想的对抗攻击进行近似表示，并将对抗训练转化为进攻网络和防守网络之间的数学博弈，同时给出了对抗训练在样本大小$n$下的收敛速度。 |
| [^34] | [ADR-GNN: Advection-Diffusion-Reaction Graph Neural Networks.](http://arxiv.org/abs/2307.16092) | 本文提出了ADR-GNN，一种基于平流-扩散-反应系统的图神经网络架构。ADR-GNN在涉及平流的复杂现象建模方面具有优势，并在节点分类和时空数据集上显示出与最先进网络相比的改进或竞争性表现。 |
| [^35] | [Rapid Flood Inundation Forecast Using Fourier Neural Operator.](http://arxiv.org/abs/2307.16090) | 使用傅里叶神经算子进行淹没预测的混合方法，在洪水事件中表现出高的准确性和普适性。 |
| [^36] | [Using Implicit Behavior Cloning and Dynamic Movement Primitive to Facilitate Reinforcement Learning for Robot Motion Planning.](http://arxiv.org/abs/2307.16062) | 本文提出了一种利用隐式行为克隆和动态运动原语来促进机器人运动规划的强化学习方法。通过利用人类示范数据提高训练速度，以及将运动规划转化为更简单的规划空间，该方法在仿真和实际机器人实验中展示了更快的训练速度和更高的得分。 |
| [^37] | [Click-Conversion Multi-Task Model with Position Bias Mitigation for Sponsored Search in eCommerce.](http://arxiv.org/abs/2307.16060) | 该论文提出了两种无位置偏差的CTR和CVR预测模型：基于位置感知的点击转化（PACC）和基于位置嵌入的PACC（PACC-PE），通过实验证明这些模型在电子商务搜索中减轻了位置偏差，并具有更好的排名效果。 |
| [^38] | [Unveiling Exotic Magnetic Phases in Fibonacci Quasicrystalline Stacking of Ferromagnetic Layers through Machine Learning.](http://arxiv.org/abs/2307.16052) | 本研究利用机器学习揭示了费波那契准晶叠层的非共线准周期铁磁配置，该配置具有独特的磁相行为，其中磁化率随着叠层数量的增加呈对数减少。 |
| [^39] | [Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback.](http://arxiv.org/abs/2307.16039) | Okapi是一种使用强化学习从人类反馈中调优的多语言大型语言模型，它解决了目前开源语言模型只针对英语和少数流行语言进行指令调优的限制问题。 |
| [^40] | [Developing novel ligands with enhanced binding affinity for the sphingosine 1-phosphate receptor 1 using machine learning.](http://arxiv.org/abs/2307.16037) | 该研究利用机器学习开发了具有增强结合亲和力的新型配体对于多发性硬化症（MS）治疗。研究通过对基于siponimod的分子变体进行大规模生成和筛选，发现了六个有前景的具有良好药物样性和易于合成的候选物，并通过分析发现了几个有助于高结合亲和力的化学性质。 |
| [^41] | [MUSE: Multi-View Contrastive Learning for Heterophilic Graphs.](http://arxiv.org/abs/2307.16026) | MUSE是一种用于异质图的多视角对比学习模型，通过构建两个视角来捕捉自我节点和邻居节点的信息，并整合融合后的节点表示，以增强图神经网络的效果。 |
| [^42] | [Fuzzy Logic Visual Network (FLVN): A neuro-symbolic approach for visual features matching.](http://arxiv.org/abs/2307.16019) | 本研究提出了一种神经符号化的视觉特征匹配方法，将模糊逻辑视觉网络（FLVN）应用于零样本学习（ZSL）分类问题，通过神经符号化LTN框架学习视觉-语义嵌入空间，并引入类别层次和高级归纳偏好作为先验知识，以提升分类效果。 |
| [^43] | [UPFL: Unsupervised Personalized Federated Learning towards New Clients.](http://arxiv.org/abs/2307.15994) | 本文提出了一种无监督个性化联邦学习方法UPFL，解决了联邦学习中新客户加入时的个性化模型问题。 |
| [^44] | [RGB-D-Fusion: Image Conditioned Depth Diffusion of Humanoid Subjects.](http://arxiv.org/abs/2307.15988) | RGB-D-Fusion是一种多模态条件去噪扩散概率模型，用于生成高分辨率深度图。它通过两个去噪扩散概率模型实现，其中一个是图像条件的，另一个是基于低分辨率RGB-D图像的。此外，还引入了深度噪声增强技术来提高模型的鲁棒性。 |
| [^45] | [Vehicle Price Prediction By Aggregating decision tree model With Boosting Model.](http://arxiv.org/abs/2307.15982) | 本研究使用决策树模型和梯度提升模型结合来预测二手车价格，通过对数据的归一化、标准化和清洗，提高预测准确性。这一方法在评估中表现良好，可用于未来二手车价格的预测。 |
| [^46] | [Initial State Interventions for Deconfounded Imitation Learning.](http://arxiv.org/abs/2307.15980) | 本文介绍了一种针对模仿学习中因果混淆问题的初始化状态干预算法，该算法能够遮蔽观测中的混淆因素并提高性能表现。 |
| [^47] | [Blockchain-empowered Federated Learning for Healthcare Metaverses: User-centric Incentive Mechanism with Optimal Data Freshness.](http://arxiv.org/abs/2307.15975) | 这篇论文介绍了一种基于区块链的医疗元宇宙联邦学习方法，通过用户中心的激励机制提供最优数据新鲜度，并通过隐私保护框架和跨链技术增强数据安全性。 |
| [^48] | [Graph Condensation for Inductive Node Representation Learning.](http://arxiv.org/abs/2307.15967) | 本论文提出了一种映射感知的图形压缩方法（MCond），通过学习节点之间的映射关系，实现了在合成图中高效地处理未知数据的能力。 |
| [^49] | [Recommendation Unlearning via Matrix Correction.](http://arxiv.org/abs/2307.15960) | 本文提出了一种Interaction and Mapping Matrices Correction (IMCorrect)方法，可以平衡推荐系统遗忘中的完整性、效用和效率。通过将用户-物品交互矩阵与映射矩阵相乘，IMCorrect能够修正推荐结果，提高推荐系统的性能。 |
| [^50] | [The effect of network topologies on fully decentralized learning: a preliminary investigation.](http://arxiv.org/abs/2307.15947) | 本研究通过分析不同网络拓扑结构对分布式学习模型性能的影响，揭示了节点连接性和网络属性在知识传播过程中的不同作用。结果表明，高连接性可能导致更好的模型性能。 |
| [^51] | [PIMbot: Policy and Incentive Manipulation for Multi-Robot Reinforcement Learning in Social Dilemmas.](http://arxiv.org/abs/2307.15944) | PIMbot是一种新的多机器人协作中奖励函数操纵的方法，通过策略和激励操纵来影响多机器人通信以实现不同的结果。 |
| [^52] | [Continual Learning in Predictive Autoscaling.](http://arxiv.org/abs/2307.15941) | 本论文提出了一种基于重放的持续学习方法，使用少量历史数据，解决了预测自动缩放中的性能下降问题。 |
| [^53] | [A Theory for Emergence of Complex Skills in Language Models.](http://arxiv.org/abs/2307.15936) | 本文提出了一个统计框架，通过分析语言模型的交叉熵损失与基本语言任务的能力之间的关系，揭示了语言模型中复杂技能产生的机制。研究结果表明，通过扩展定律，预训练模型能够高效学习，并表现出违反通常泛化理论的能力。 |
| [^54] | [A Noisy-Label-Learning Formulation for Immune Repertoire Classification and Disease-Associated Immune Receptor Sequence Identification.](http://arxiv.org/abs/2307.15934) | 本文提出了一个用于免疫库分类和疾病相关免疫受体序列识别的噪声标签学习方法，通过设计强健的训练策略和同时训练多个模型来解决传统方法中的噪声标签和确认偏见问题。这对于发展新疫苗和免疫疗法具有重要意义。 |
| [^55] | [Dynamic deep-reinforcement-learning algorithm in Partially Observed Markov Decision Processes.](http://arxiv.org/abs/2307.15931) | 本研究研究了在部分可观测马尔可夫决策过程中解决的动作序列的好处，并提出了几种扩展深度强化学习算法的结构和方法。 |
| [^56] | [Opportunistic Air Quality Monitoring and Forecasting with Expandable Graph Neural Networks.](http://arxiv.org/abs/2307.15916) | 提出了一种可扩展的图神经网络模型，能够处理不同空间结构的已有和新增基础设施收集的环境空气质量数据，并嵌入到任何空气质量预测模型中应用于具有演变空间结构的场景。 |
| [^57] | [An Automata-Theoretic Approach to Synthesizing Binarized Neural Networks.](http://arxiv.org/abs/2307.15907) | 这篇论文提出了一种用自动机理论方法合成满足指定属性的二进制神经网络的方法，并定义了一种时态逻辑作为规范语言。 |
| [^58] | [Multi-view Sparse Laplacian Eigenmaps for nonlinear Spectral Feature Selection.](http://arxiv.org/abs/2307.15905) | 本研究提出了一种多视角稀疏拉普拉斯特征图方法，用于高维数据的特征选择，能够从多个数据视图中有效地捕捉数据的基本结构。使用稀疏约束和可扩展的优化算法，通过降维和迭代算法解决优化问题，得到了一个能够捕捉基本数据结构的特征子集。 |
| [^59] | [A new Gradient TD Algorithm with only One Step-size: Convergence Rate Analysis using $L$-$\lambda$ Smoothness.](http://arxiv.org/abs/2307.15892) | 本论文提出了一种新的梯度时序差分算法，只使用一个步长参数，并证明收敛速度至少为$O(1/t)$。 |
| [^60] | [First-order Policy Optimization for Robust Policy Evaluation.](http://arxiv.org/abs/2307.15890) | 该论文提出了一种名为FRPE的一阶策略评估方法，用于鲁棒马尔可夫决策过程的策略评估。该方法在确定性和随机设置下都能提供统一的框架，并具有较小的样本复杂度。 |
| [^61] | [Explaining Full-disk Deep Learning Model for Solar Flare Prediction using Attribution Methods.](http://arxiv.org/abs/2307.15878) | 本文研究了太阳耀斑预测的深度学习方法，并使用归因方法解释模型的预测结果。通过使用全盘线磁图像和类别加权技术，我们提出了一个预测模型，并评估了其性能。我们还应用了三种归因方法对模型的预测结果进行解释和验证。 |
| [^62] | [GraphDAC: A Graph-Analytic Approach to Dynamic Airspace Configuration.](http://arxiv.org/abs/2307.15876) | 本研究提出了一种图分析方法用于动态空域配置，通过构建约束嵌入的图形，应用自适应算法来生成合作机场群体并均匀分配工作负载，在各种交通条件下实现了50％的工作负载减少，并为优化空域配置的推荐系统奠定了基础。 |
| [^63] | [Cross-dimensional transfer learning in medical image segmentation with deep learning.](http://arxiv.org/abs/2307.15872) | 这篇论文介绍了一种有效的跨维度迁移学习方法，将在自然图像上训练的2D分类网络的效能迁移到医学图像分割中，克服了限量标注数据和获取约束的问题。 |
| [^64] | [Efficient Semi-Supervised Federated Learning for Heterogeneous Participants.](http://arxiv.org/abs/2307.15870) | 本论文提出了一种高效的半监督异构参与者联邦学习系统，通过引入聚类正则化来改进模型在数据非独立同分布情况下的性能，并对模型收敛性进行了理论和实验研究。 |
| [^65] | [Faster Stochastic Algorithms for Minimax Optimization under Polyak--{\L}ojasiewicz Conditions.](http://arxiv.org/abs/2307.15868) | 该论文提出了基于Polyak-Lojasiewicz条件的极小极大优化的快速随机算法SPIDER-GDA，并证明了该算法能够在较低的复杂度下找到接近最优解的结果。 |
| [^66] | [Catching Elusive Depression via Facial Micro-Expression Recognition.](http://arxiv.org/abs/2307.15862) | 本文通过使用面部微表情（FMEs）识别来诊断隐性抑郁症，从而捕捉难以捉摸的抑郁症。我们提出了一种基于面部标志点的关注区域（ROI）方法来解决FMEs极低强度和微妙的识别问题，并提供了一种低成本和保护隐私的自我诊断解决方案。 |
| [^67] | [Multi-output Headed Ensembles for Product Item Classification.](http://arxiv.org/abs/2307.15858) | 本论文提出了一个基于深度学习的产品项目分类模型框架，利用平均集成和融合分类器的简单性和稳健性，并通过元数据特征和低层特征工程提升分类质量。 |
| [^68] | [Improving Realistic Worst-Case Performance of NVCiM DNN Accelerators through Training with Right-Censored Gaussian Noise.](http://arxiv.org/abs/2307.15853) | 通过使用k-th百分位性能（KPP）来捕捉在CiM加速器上执行的DNN模型的真实最坏情况性能，从而改善NVCiM DNN加速器的性能。 |
| [^69] | [Comprehensive Algorithm Portfolio Evaluation using Item Response Theory.](http://arxiv.org/abs/2307.15850) | 本文提出了一种改进的基于IRT的框架，用于评估算法组合在数据集仓库中的性能，同时获取算法一致性和异常性等特征。该框架通过对传统IRT模型进行倒转和重新解释来实现，不需要额外的数据集特征计算。 |
| [^70] | [Quantum Kernel Estimation With Neutral Atoms For Supervised Classification: A Gate-Based Approach.](http://arxiv.org/abs/2307.15840) | 本文提出了一种用中性原子进行量子核估计的监督分类的基于门的方法，通过实现一种参数化序列来进行特征映射和核矩阵的计算。 |
| [^71] | [Holistic Survey of Privacy and Fairness in Machine Learning.](http://arxiv.org/abs/2307.15838) | 这项综合调查研究了机器学习中隐私和公平性的重要性，并强调了将这两个目标同时集成到模型中的挑战和可能的方法。 |
| [^72] | [Mean Estimation with User-level Privacy under Data Heterogeneity.](http://arxiv.org/abs/2307.15835) | 本文提出了一种在数据异质性中保持用户级隐私的均值估计方法，允许用户数据在分布和数量上的差异，并证明了估计器的渐近最优性和可达到的误差下界。 |
| [^73] | [A Distance Correlation-Based Approach to Characterize the Effectiveness of Recurrent Neural Networks for Time Series Forecasting.](http://arxiv.org/abs/2307.15830) | 本文通过距离相关性的方法来研究循环神经网络对于时间序列预测的有效性，发现激活层能够学习时间序列的滞后结构，但是在连续的几层中逐渐丧失这些信息，导致预测质量变差，同时激活层也不能很好地建模移动平均和异方差时间。 |
| [^74] | [RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.](http://arxiv.org/abs/2307.15818) | 本文研究了将互联网规模数据上训练的视觉-语言模型直接应用于机器人控制的方法，实现了泛化能力的提升和新兴的语义推理。通过在机器人轨迹数据和互联网规模的视觉-语言任务上共同微调最先进的视觉-语言模型，为单一的端到端训练模型提供了同时学习机器人观测到行为映射和利用语言和视觉-语言数据的益处的能力。 |
| [^75] | [Multi-growth stage plant recognition: a case study of Palmer amaranth (Amaranthus palmeri) in cotton (Gossypium hirsutum).](http://arxiv.org/abs/2307.15816) | 本文研究了在棉花中以Amaranthus palmeri为例的Palmer amaranth杂草的八个生长阶段识别，对You Only Look Once (YOLO)架构进行了比较分析。 |
| [^76] | [Anomaly Detection in Industrial Machinery using IoT Devices and Machine Learning: a Systematic Mapping.](http://arxiv.org/abs/2307.15807) | 这项研究通过使用物联网设备和机器学习方法，系统地探索了在工业机械中进行异常检测的挑战和机遇。该研究强调了机器学习算法在自动化检测工业机械异常中的重要性。 |
| [^77] | [On Single Index Models beyond Gaussian Data.](http://arxiv.org/abs/2307.15804) | 该论文研究了超越高斯数据的单指数模型，探索了对稳定性和对称性的违反情况下的样本复杂性控制。 |
| [^78] | [SAFE: Saliency-Aware Counterfactual Explanations for DNN-based Automated Driving Systems.](http://arxiv.org/abs/2307.15786) | 本文提出了一种基于显著性图的方法，来生成更具信息性的CF解释，用于解释DNN自动驾驶系统的决策过程。 |
| [^79] | [Spherical and Hyperbolic Toric Topology-Based Codes On Graph Embedding for Ising MRF Models: Classical and Quantum Topology Machine Learning.](http://arxiv.org/abs/2307.15778) | 本论文介绍了在图嵌入中应用信息几何来描述Ising模型的基态，通过利用球面和双曲面拓扑上的编码，建立了机器学习和纠错编码之间的联系，并通过优化纠错码和发展嵌入方法提出了一种新的编码方法。 |
| [^80] | [Seeking the Yield Barrier: High-Dimensional SRAM Evaluation Through Optimal Manifold.](http://arxiv.org/abs/2307.15773) | 本研究通过最优流形概念将替代模型和重要性采样方法联系起来，提出了一种新型的高维SRAM评估方法。该方法名为OPTIMIS，结合了神经耦合流和洋葱采样，在保持性能优势的同时具备鲁棒性和一致性。 |
| [^81] | [Weighted variation spaces and approximation by shallow ReLU networks.](http://arxiv.org/abs/2307.15772) | 本文研究了在有界域上通过单隐藏层ReLU网络逼近函数的问题，介绍了新的模型类定义加权变差空间，该定义与域本身相关。 |
| [^82] | [The Hydra Effect: Emergent Self-repair in Language Model Computations.](http://arxiv.org/abs/2307.15771) | 本研究通过因果分析探究了语言模型计算的内部结构，发现了Hydra效应和晚期MLP层的平衡功能，并分析了它们在语言模型中的影响。 |
| [^83] | [Resume Evaluation through Latent Dirichlet Allocation and Natural Language Processing for Effective Candidate Selection.](http://arxiv.org/abs/2307.15752) | 本文提出了一种使用LDA和自然语言处理的方法，通过提取简历中的实体并将其用于评分，从而实现有效的候选人选择。该方法在考虑所有属性的情况下达到了82%的准确率。 |
| [^84] | [How regularization affects the geometry of loss functions.](http://arxiv.org/abs/2307.15744) | 这篇论文研究了正则化方法对于损失函数几何形状的影响，特别是对于深度神经网络，讨论了一种基本的几何特性Morse函数，并探讨了几种正则化方法使得正则化后的函数成为Morse函数的条件。 |
| [^85] | [AI for Anticipatory Action: Moving Beyond Climate Forecasting.](http://arxiv.org/abs/2307.15727) | AI用于预测行动从气候预测向预测行动转变，利用机器学习模型的强大能力，在评估气候对特定人口的影响方面填补了方法论上的差距，以推进对气候变化最脆弱人口的灾害响应。 |
| [^86] | [Curiosity-Driven Reinforcement Learning based Low-Level Flight Control.](http://arxiv.org/abs/2307.15724) | 本文提出了一种基于好奇心驱动的算法，通过从里程数据生成适当的电机速度来实现自主学习以控制飞行。通过该算法，四旋翼飞行器可以在控制偏航方向朝向目标位置的同时通过障碍物，该算法基于预测误差的新的好奇心方法。 |
| [^87] | [Identifying acute illness phenotypes via deep temporal interpolation and clustering network on physiologic signatures.](http://arxiv.org/abs/2307.15719) | 通过深度时间插值和聚类网络分析入院6小时内的生命体征数据，在大规模的数据集上鉴别出不同的急性疾病表型，为早期临床决策提供支持。 |
| [^88] | [Synthetic pre-training for neural-network interatomic potentials.](http://arxiv.org/abs/2307.15714) | 这项研究通过利用合成原子数据进行预训练，提高了神经网络原子势模型在计算实践中的数值精度和稳定性。初步实验表明了该方法在碳的等变图神经网络势方法中的可行性。 |
| [^89] | [R-LPIPS: An Adversarially Robust Perceptual Similarity Metric.](http://arxiv.org/abs/2307.15157) | 该论文提出了一种针对对抗性示例具有鲁棒性的新型感知相似度度量方法R-LPIPS，用于解决在计算机视觉中广泛采用的LPIPS度量方法对对抗性示例的敏感性问题。 |
| [^90] | [Information Gained Subgroup Discovery in Datasets.](http://arxiv.org/abs/2307.15089) | 该论文研究了在数据集中通过信息提升的方法进行子群发现。具体针对肺癌治疗，在保持或提高治疗效果的同时减少副作用对于改善患者的生活质量非常重要，临床指南虽然提供了治疗建议，但仍未将治疗结果纳入考量。 |
| [^91] | [MATNilm: Multi-appliance-task Non-intrusive Load Monitoring with Limited Labeled Data.](http://arxiv.org/abs/2307.14778) | 本研究提出了MATNilm框架，能够通过样本增强和共享的分层拆分结构，在有限标记数据下提高非侵入式负载监测的性能。 |
| [^92] | [Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior.](http://arxiv.org/abs/2307.14619) | 本文提出了一个理论框架，研究了在非线性动态系统中模仿复杂专家演示的行为。通过稳定模仿策略并确保准确估计演示者分布，可以使模仿者与演示者的轨迹分布相近。 |
| [^93] | [Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models.](http://arxiv.org/abs/2307.14430) | 本论文提出了一个数据驱动的技能框架，用于理解和训练语言模型。通过研究人类获得技能的有序性，我们证明了语言模型学习技能时也有一定的顺序，并且这种顺序可以改善对语言模型的理解和数据高效训练。 |
| [^94] | [Simulation-based Inference for Cardiovascular Models.](http://arxiv.org/abs/2307.13918) | 本研究将心血管模型的逆问题作为统计推理进行解决，在体外进行了五个生物标记物的不确定性分析，展示了模拟推理的能力。 |
| [^95] | [Graph Neural Networks For Mapping Variables Between Programs -- Extended Version.](http://arxiv.org/abs/2307.13014) | 本文提出了使用图神经网络(GNNs)基于程序的抽象语法树(ASTs)来映射变量集，以解决程序比较、分析、修复和克隆检测等任务。在初学者编程作业中进行的实验证明了变量映射的有效性。 |
| [^96] | [Active Control of Flow over Rotating Cylinder by Multiple Jets using Deep Reinforcement Learning.](http://arxiv.org/abs/2307.12083) | 本研究使用深度强化学习算法结合旋转和多个可控喷口，通过优化喷口数量和位置，传感器位置以及每个动作可允许的最大流量和每个episode中允许的总喷口数的形式，实现对旋转圆柱体流动的主动控制，抑制涡流脱落和稳定卡门涡流。 |
| [^97] | [High-performance real-world optical computing trained by in situ model-free optimization.](http://arxiv.org/abs/2307.11957) | 本论文提出了一种无模型优化光学计算系统的方法，通过在原位进行轻量级优化，实现了高性能的真实光学计算。实验证明该方法在分类准确度上优于传统方法，并展示了在高速细胞分析方面的潜力。这种方法的固有简单性和低需求的计算资源促进了光学计算技术从实验室研究向真实世界应用的转变。 |
| [^98] | [A Holistic Assessment of the Reliability of Machine Learning Systems.](http://arxiv.org/abs/2307.10586) | 本文提出了一种用于评估机器学习系统可靠性的整体评估方法，通过评估分布内准确性、分布偏移鲁棒性、对抗鲁棒性、校准性和越界检测能力等五个关键属性，引入了可靠性得分来评估整个系统的可靠性。 |
| [^99] | [For One-Shot Decoding: Unsupervised Deep Learning-Based Polar Decoder.](http://arxiv.org/abs/2307.08004) | 本文提出了一种基于无监督深度学习的极化解码器的单次解码方法，通过自监督学习训练神经网络，消除了对预定义标签的依赖，实现了在通信系统实际数据上的直接训练，其性能接近最大后验概率解码器，并展示出更优越的泛化能力。 |
| [^100] | [DISPEL: Domain Generalization via Domain-Specific Liberating.](http://arxiv.org/abs/2307.07181) | DISPEL是一种通过后处理细粒度掩蔽方法，能够在嵌入空间中过滤掉未定义和无法区分的领域特定特征的领域泛化方法。 |
| [^101] | [Tackling Combinatorial Distribution Shift: A Matrix Completion Perspective.](http://arxiv.org/abs/2307.06457) | 该论文研究了组合分布偏移的问题，提出了基于矩阵补全的解决方法。通过在特殊情况下的双线性嵌入，实现对训练中未涵盖的测试分布进行外推。这个设置将缺失非随机数据的矩阵补全问题广义化。 |
| [^102] | [Interpreting deep embeddings for disease progression clustering.](http://arxiv.org/abs/2307.06060) | 本文提出了一种在疾病进展聚类中解读深度嵌入的新方法，并通过评估2型糖尿病参与者数据集展示了对疾病进展模式的临床意义性见解。 |
| [^103] | [Benchmarking Bayesian Causal Discovery Methods for Downstream Treatment Effect Estimation.](http://arxiv.org/abs/2307.04988) | 该研究评估了六种基准因果发现方法和一种新提出的基于 GFlowNets 的方法在治疗效果估计任务中的表现，并发现 GFlowNets 具有捕捉各种有用和多样的平均处理效应模式的能力。 |
| [^104] | [Comparison of Point Cloud and Image-based Models for Calorimeter Fast Simulation.](http://arxiv.org/abs/2307.04780) | 本文比较了使用点云和基于图像的模型来进行量热器快速模拟，发现点云更自然地表示了量热器淋浴，处理稀疏数据集更优秀，并且可以使用更紧凑的模型和数据文件进行实现。 |
| [^105] | [FreeDrag: Point Tracking is Not What You Need for Interactive Point-based Image Editing.](http://arxiv.org/abs/2307.04684) | FreeDrag提出了一种基于特征的方法来解决DragGAN在点追踪方面的困难，通过自适应模板特征、线性搜索和模糊定位技术，实现了稳定和高效的基于点的图像编辑。 |
| [^106] | [Solvent: A Framework for Protein Folding.](http://arxiv.org/abs/2307.04603) | Solvent是一个用于蛋白质折叠的统一研究框架，支持最新模型重要组件的实现和基准测试，并提供对蛋白质结构建模领域的有用见解。 |
| [^107] | [Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks.](http://arxiv.org/abs/2307.04228) | 本论文提出了一种高效的贝叶斯行程时间层析成像方法，利用敏感性信息的多项式混沌展开和深度生成网络，以处理地质复杂性先验的挑战。 |
| [^108] | [When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment.](http://arxiv.org/abs/2307.03864) | Transformer在强化学习中的作用是增强记忆能力而不是改进信用分配。 |
| [^109] | [Offline Reinforcement Learning with Imbalanced Datasets.](http://arxiv.org/abs/2307.02752) | 本文提出了一种在不平衡数据集中的新型离线强化学习方法，通过将CQL与回溯过程相结合来提取策略，从而有效地解决了不平衡数据集带来的挑战。 |
| [^110] | [How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model.](http://arxiv.org/abs/2307.02129) | 本文研究了深度神经网络学习组合性数据的问题，通过对随机层次模型进行分类任务，发现深度CNN学习这个任务所需的训练数据数量随着类别数、组合数和迭代次数的增加而渐进增加。 |
| [^111] | [Large-scale unsupervised audio pre-training for video-to-speech synthesis.](http://arxiv.org/abs/2306.15464) | 本文提出了一种利用大规模无监督音频预训练的方法，用于视频到语音合成。通过训练编码器-解码器模型，我们可以在不需要视频对应的情况下，使用丰富的仅音频数据集进行合成。 |
| [^112] | [G-NM: A Group of Numerical Time Series Prediction Models.](http://arxiv.org/abs/2306.11667) | G-NM是一组集合了传统和现代模型的数字时间序列预测模型，旨在提高对复杂自然现象中的模式和趋势的预测能力。 |
| [^113] | [Blocked Cross-Validation: A Precise and Efficient Method for Hyperparameter Tuning.](http://arxiv.org/abs/2306.06591) | 屏蔽交叉验证（BCV）是一种准确高效的超参数调整方法，相比于传统的重复交叉验证（RCV），BCV可以提供更准确的误差估计结果，且运行次数更少。使用真实数据集的实验结果表明，BCV在超参数调优任务中优于RCV，能够以更少的计算实现更高的精度。 |
| [^114] | [Estimating Koopman operators with sketching to provably learn large scale dynamical systems.](http://arxiv.org/abs/2306.04520) | 本文提出利用随机投影技术优化了Koopman算子的估计器，加快了计算速度，并给出了精确的误差界限，提高了算法的可靠性。 |
| [^115] | [A Self-Supervised Approach for Cluster Assessment of High-Dimensional Data.](http://arxiv.org/abs/2306.00011) | 本文提出了一种利用自监督深度神经网络生成代表性嵌入的方法，用于评估复杂图像数据中的聚类结构。 |
| [^116] | [Conditional Diffusion Models for Semantic 3D Medical Image Synthesis.](http://arxiv.org/abs/2305.18453) | 这篇论文提出了Med-DDPM，一种使用扩散模型进行语义化三维医学图像合成的创新解决方案，它通过控制像素级掩码标签的生成过程，能够生成高质量逼真的医学图像，并且在精度、稳定性和多样性等指标上优于GAN技术，也优于传统的增强技术和GAN合成图像。 |
| [^117] | [Language Models are Bounded Pragmatic Speakers.](http://arxiv.org/abs/2305.17760) | 本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。经过人类反馈的强化学习微调的大型语言模型具有概念上类似于 快与慢思考模型的思维模型，而这种思维模型被归因于人类。此研究凸显了采用认知概率建模方法对语言模型的理解、评估和推进的价值。 |
| [^118] | [The Representation Jensen-Shannon Divergence.](http://arxiv.org/abs/2305.16446) | 本文提出了一种基于表示的新型散度——表示Jensen-Shannon散度，通过将数据分布嵌入到RKHS中，并利用表示的协方差算子的频谱，实现对数据分布的估计，并提供了具有灵活性，可扩展性，可微分性的经验协方差矩阵估计函数和基于核矩阵的估计函数。 |
| [^119] | [Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy.](http://arxiv.org/abs/2305.07805) | Mesh2SSM是一种基于无监督排列不变表示学习的方法，可以将模板点云变形为特定主体的网格，形成基于对应关系的解剖学统计形态模型。 |
| [^120] | [Explainable Parallel RCNN with Novel Feature Representation for Time Series Forecasting.](http://arxiv.org/abs/2305.04876) | 本论文提出了一种新的特征表示策略——移位法，将过去数据和未来协变量融合起来进行时间序列预测，并开发了一个并行的深度学习框架，同时提出了一种可解释的方法来解释模型的特征重要性。 |
| [^121] | [Deep Transfer Learning for Automatic Speech Recognition: Towards Better Generalization.](http://arxiv.org/abs/2304.14535) | 本文Survey了基于DTL的ASR框架，并介绍了如何使用实际数据集进行深度迁移学习以达到更好的泛化性能。 |
| [^122] | [Data-driven Piecewise Affine Decision Rules for Stochastic Programming with Covariate Information.](http://arxiv.org/abs/2304.13646) | 本研究提出一种嵌入非凸分段仿射决策规则的经验风险最小化方法，用于学习特征与最优决策之间的直接映射。所提出的方法可用于广泛的非凸型SP问题，并且在数值研究中表现出优越的性能。 |
| [^123] | [Explaining, Analyzing, and Probing Representations of Self-Supervised Learning Models for Sensor-based Human Activity Recognition.](http://arxiv.org/abs/2304.07304) | 本文探究了基于传感器的人类活动识别中自监督学习框架的深度表示，通过解释这些表示与有监督表示的区别，比较它们的鲁棒性，并使用显著图将其应用于预测不同的活动。 |
| [^124] | [Measuring Re-identification Risk.](http://arxiv.org/abs/2304.07210) | 该论文提出了一个用于测量用户表示中再识别风险的理论框架，该框架基于假设检验，可以限制攻击者从用户的表示中获取其身份的概率。作者展示了该框架应对实际应用的普适性，并补充了攻击算法来衡量现实应用的风险。 |
| [^125] | [SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models.](http://arxiv.org/abs/2303.10464) | 本文提出了SPDF算法来实现大规模语言模型的高效训练。通过非结构化权重稀疏性来进行预训练，可以降低计算成本，而密集微调则可以保证高性能的表现。 |
| [^126] | [ICICLE: Interpretable Class Incremental Continual Learning.](http://arxiv.org/abs/2303.07811) | ICICLE提出了一种基于样本的可解释的类增量连续学习方法，通过采用原型部分化方法来解决解释性概念漂移的问题，实验结果表明其在不需要样本的情况下表现优于现有的方法。 |
| [^127] | [One-4-All: Neural Potential Fields for Embodied Navigation.](http://arxiv.org/abs/2303.04011) | 本文提出了一种新方法 One-4-All (O4A)，利用自我监督和流形学习，实现了一种无图形、端到端的导航程序。其通过贪婪最小化潜在空间内的潜力函数来进行导航，在真实世界的导航中具有重要应用价值。 |
| [^128] | [TopSpark: A Timestep Optimization Methodology for Energy-Efficient Spiking Neural Networks on Autonomous Mobile Agents.](http://arxiv.org/abs/2303.01826) | TopSpark提出了一种时间步长优化方法，用于提高自主移动机器人上脉冲神经网络的能量效率。该方法消除了固定时间步长的限制，使得SNNs能够在训练和推断阶段都实现更高的能量效率，并且能够在运行时进行高效的在线学习。 |
| [^129] | [Understanding the Diffusion Objective as a Weighted Integral of ELBOs.](http://arxiv.org/abs/2303.00848) | 本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。 |
| [^130] | [Unsupervised Pathology Detection: A Deep Dive Into the State of the Art.](http://arxiv.org/abs/2303.00609) | 本文深入研究了无监督病理检测技术的最新发展，通过评估和基准测试多种尖端方法，证明了工业和医疗文献中新开发的特征建模方法在各种模态和数据集上创立了新的技术水平。 |
| [^131] | [Referential communication in heterogeneous communities of pre-trained visual deep networks.](http://arxiv.org/abs/2302.08913) | 异构视觉深度网络社区中的预训练网络可以自我监督地开发出共享协议，以指代一组目标中的目标对象，并可用于沟通不同粒度的未知对象类别。 |
| [^132] | [CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image Classification.](http://arxiv.org/abs/2302.02314) | 本研究提出了一种新的分类网络CECT，利用可控的卷积神经网络和Transformer进行组合，能同时捕捉多个局部和全局尺度上的特征。在两个公共COVID-19数据集上评估后表现优于现有的最先进方法。这一新方法在医学图像分类领域中有广泛的应用前景。 |
| [^133] | [Continuous Spatiotemporal Transformers.](http://arxiv.org/abs/2301.13338) | 连续时空转换器（CST）是一种新的转换器架构，通过在Sobolev空间中进行优化，能够建模连续系统并保证连续平滑的输出。在多个任务上，包括学习脑动力学，CST表现出卓越的性能。 |
| [^134] | [Generating Multidimensional Clusters With Support Lines.](http://arxiv.org/abs/2301.10327) | 提出了一个名为Clugen的模块化合成数据生成过程，能够使用任意分布创建支持线段的多维聚类，适用于评估聚类算法并有潜力成为广泛使用的框架。 |
| [^135] | [Designing Data: Proactive Data Collection and Iteration for Machine Learning.](http://arxiv.org/abs/2301.10319) | 这篇论文介绍了一种设计数据的迭代方法，将人机交互概念与机器学习技术相结合，以解决机器学习应用中数据收集缺乏多样性导致的失败问题。通过预收集计划、收集监控和数据熟悉度等步骤，该方法可以提高模型的泛化能力并在跨组交叉群体上取得更好的结果。 |
| [^136] | [SpArX: Sparse Argumentative Explanations for Neural Networks.](http://arxiv.org/abs/2301.09559) | 该论文提出了一种稀疏的神经网络论证解释方法SpArX，通过利用多层感知器和定量论证框架之间的关系，可以为神经网络的决策过程提供更忠实和深入的解释。 |
| [^137] | [Uncertainty in Real-Time Semantic Segmentation on Embedded Systems.](http://arxiv.org/abs/2301.01201) | 本文提出了一种结合贝叶斯回归和动量传播的预测方法，能够实时在嵌入式硬件上产生有意义的不确定性，从而使语义分割模型在自动驾驶和人机交互等领域的实时应用成为可能。 |
| [^138] | [Clustering with Neural Network and Index.](http://arxiv.org/abs/2212.03853) | 介绍了一种新的带有神经网络和索引的聚类模型CNNI，该模型使用神经网络对数据点进行聚类，实现了第一个能够处理非凸形状数据的参数化聚类模型。 |
| [^139] | [The European AI Liability Directives -- Critique of a Half-Hearted Approach and Lessons for the Future.](http://arxiv.org/abs/2211.13960) | 这篇论文对欧洲AI责任指令的提案进行了详细研究，发现其代表了一个半心态的方法，并提出了三个创新贡献。 |
| [^140] | [Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning.](http://arxiv.org/abs/2211.09273) | 本研究通过对抗机器学习的方法，提出了一种名为DARE-GP的解决方案，可以在不影响智能音箱效用的情况下，逃避与智能音箱相连的语音情感识别分类器，从而保护隐私。 |
| [^141] | [Human-in-the-Loop Mixup.](http://arxiv.org/abs/2211.01202) | 本研究针对使用Mixup的合成数据进行人机协作研究，发现人类知觉与合成数据标签不一致，这些发现有助于提高下游模型的可靠性。 |
| [^142] | [Unsupervised Model Adaptation for Source-free Segmentation of Medical Images.](http://arxiv.org/abs/2211.00807) | 本文提出了一种无监督模型自适应方法，用于解决医学图像分割中分布转变的问题。该方法通过创建共享的源/目标潜在特征空间，使得源培训的分类器能够在目标域上保持性能。 |
| [^143] | [Latent Multimodal Functional Graphical Model Estimation.](http://arxiv.org/abs/2210.17237) | 本研究提出了一个潜在多模态功能图模型估计的新框架，通过同时估计转换算子和潜在图来填补当前科学方法在估计多模态功能数据图模型方面的空白 |
| [^144] | [Functional Central Limit Theorem and Strong Law of Large Numbers for Stochastic Gradient Langevin Dynamics.](http://arxiv.org/abs/2210.02092) | 这篇论文研究了具有固定步长的随机梯度 Langevin 动力学在数据流不独立的情况下的混合性质，并证明了其具有大数定律和函数中心极限定理。 |
| [^145] | [Predicting Mutual Funds' Performance using Deep Learning and Ensemble Techniques.](http://arxiv.org/abs/2209.09649) | 本研究通过测试深度学习模型和传统统计技术对基金表现的预测准确性，发现使用现代贝叶斯优化训练的LSTM和GRUs深度学习方法比传统方法更准确地预测基金的夏普比率。将LSTM和GRUs的预测结果进行集成可以实现最佳表现。 |
| [^146] | [Quantization for decentralized learning under subspace constraints.](http://arxiv.org/abs/2209.07821) | 本文提出一种基于子空间约束的分散学习中的量化方法，该方法通过使用微分随机量化器压缩估计值，在保持低误差的同时减少通信开销。 |
| [^147] | [Patching Weak Convolutional Neural Network Models through Modularization and Composition.](http://arxiv.org/abs/2209.06116) | 本文提出了一种通过压缩模块化和组合来修补卷积神经网络模型的弱点的方法，该方法无需重新训练整个模型，并在多个基准数据集上实现了与最先进方法相当甚至更好的结果。 |
| [^148] | [What does a platypus look like? Generating customized prompts for zero-shot image classification.](http://arxiv.org/abs/2209.03320) | 本文介绍了一种通过结合大型语言模型和开放词汇模型生成自定义提示的方法，以提高零样本图像分类的准确性和效率。通过利用语言模型生成描述性句子的方式，这种方法能够捕捉图像类别的重要区分特征。 |
| [^149] | [SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability.](http://arxiv.org/abs/2208.09418) | 本文提出了一种名为SAFARI的方法，用于评估深度学习的解释可靠性。该方法针对现有技术无法解决的几个挑战，通过引入两种黑盒评估方法，即最坏情况解释差异和一般情况下的鲁棒性的概率概念，来解决现有度量不全面、XAI技术异质性和误解罕见性等问题。使用遗传算法和子集模拟进行评估。 |
| [^150] | [See What the Robot Can't See: Learning Cooperative Perception for Visual Navigation.](http://arxiv.org/abs/2208.00759) | 使用图神经网络架构的邻域特征聚合模块实现了所有传感器间的通讯，解决了机器人视觉导航中缺乏全局定位信息的问题并实现了高效导航。 |
| [^151] | [Reducing Training Time in Cross-Silo Federated Learning using Multigraph Topology.](http://arxiv.org/abs/2207.09657) | 本文提出了一种新的多图拓扑结构，用于跨数据仓库联邦学习，通过孤立节点实现模型聚合，从而有效地减少了训练时间。 |
| [^152] | [Flexible Differentiable Optimization via Model Transformations.](http://arxiv.org/abs/2206.06135) | 本文介绍了DiffOpt.jl，一个基于MathOptInterface构建的Julia库，它可以对任何模型的参数进行微分求解，不仅限于凸锥规划和二次规划标准形式。使用该库, 可以实现灵活高效地求解一系列模型的优化问题，为基于优化的机器学习提供了新的机会。 |
| [^153] | [Removing the fat from your posterior samples with margarine.](http://arxiv.org/abs/2205.12841) | 本文总结了一种使用掩蔽自回归流和核密度估计器的方法，可以学习对应于核心科学参数的边际后验密度。该方法在计算边际库尔巴克-勒布勒散度、边际贝叶斯模型维度、似然函数模拟和先验模拟等方面具有广泛应用。 |
| [^154] | [Causal Discovery and Knowledge Injection for Contestable Neural Networks.](http://arxiv.org/abs/2205.09787) | 本研究提出了一种可以进行双向互动的方法，通过允许神经网络展示其所学因果图，并允许人类修改因果图后重新注入机器中，从而提供了一种调试神经网络的方式，实验结果显示该方法可以显著改善预测性能。 |
| [^155] | [A Comprehensive Survey on Model Quantization for Deep Neural Networks.](http://arxiv.org/abs/2205.07877) | 本文综述了深度神经网络中的模型量化，这是一种用低位宽存储全精度值以实现节约内存和操作成本的压缩方法。文章分类介绍了各种量化方法，并讨论了使用比例因子匹配数据范围和适当的训练方法的重要性。本文还回顾了模型量化的最新研究，并强调了其优缺点和当前的挑战和未来研究方向。 |
| [^156] | [Dynamic Operads, Dynamic Categories: From Deep Learning to Prediction Markets.](http://arxiv.org/abs/2205.03906) | 本文研究了动态操作符和动态类别的概念，并将其应用于预测市场和深度学习领域。 |
| [^157] | [Improved Group Robustness via Classifier Retraining on Independent Splits.](http://arxiv.org/abs/2204.09583) | 通过在独立的分割上重新训练分类器，该方法改善了群体鲁棒性，有助于提高模型在具有少见特征的子群上的性能。 |
| [^158] | [Explainability in Process Outcome Prediction: Guidelines to Obtain Interpretable and Faithful Models.](http://arxiv.org/abs/2203.16073) | 本文介绍了在过程结果预测中应用解释性模型的指南，并通过对事件、案例和控制流的分析，通过实验评估了不同模型的解释性能力。 |
| [^159] | [HiFi++: a Unified Framework for Bandwidth Extension and Speech Enhancement.](http://arxiv.org/abs/2203.13086) | 本文提出了一种统一框架HiFi++，用于带宽扩展和语音增强任务，通过改进的生成器架构，在这些任务中表现出与最先进方法相媲美甚至更好的性能，并且消耗更少的计算资源。 |
| [^160] | [Local neural operator for solving transient partial differential equations on varied domains.](http://arxiv.org/abs/2203.08145) | 本文提出了一种局部神经算子（LNO），用于解决在不同域上的瞬态偏微分方程，该模型可通过一种便捷的策略进行边界处理，并在未知域上预测解决方案。实验证明，LNO比传统的有限元方法计算速度快约1000倍，并且能够高效地解决各种流体流动问题。 |
| [^161] | [BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification.](http://arxiv.org/abs/2203.01937) | 本文提出了一种适用于多标签、嘈杂CXR学习的方法，使用基于袋的多标签描述符平滑地重新标记数据集中的样本，并进行训练以提高模型性能。 |
| [^162] | [Holdouts set for predictive model updating.](http://arxiv.org/abs/2202.06374) | 该论文研究了在复杂环境中如何更新预测风险评分来指导干预。作者提出使用留置集的方式进行更新，通过找到留置集的合适大小可以保证更新后的风险评分性能良好，同时减少留置样本数量。研究结果表明，该方法在总成本增长速度方面具有竞争优势。 |
| [^163] | [Learning Interpretable, High-Performing Policies for Autonomous Driving.](http://arxiv.org/abs/2202.02352) | 本研究提出了一种叫做可解释连续控制树（ICCTs）的树状模型，通过现代的强化学习方法进行优化，能够学习到高性能、可解释的策略。这种方法在自动驾驶领域表现出了比基准模型高33%的性能，并且达到300倍至600倍的减少。 |
| [^164] | [Identifying Pauli spin blockade using deep learning.](http://arxiv.org/abs/2202.00574) | 该论文介绍了一种使用深度学习算法自动识别Pauli自旋阻碍的方法，通过训练算法使用模拟数据和跨设备验证，克服了PSB数据的稀缺性，并在硅场效应晶体管器件上取得了96％的准确性。该方法具有鲁棒性，预计可在所有类型的量子点器件上使用。 |
| [^165] | [DoCoM: Compressed Decentralized Optimization with Near-Optimal Sample Complexity.](http://arxiv.org/abs/2202.00255) | 提出了一种名为DoCoM的分散式优化算法，通过压缩谣言传播一致性追踪同时跟踪平均迭代和随机梯度，并引入动量步骤和局部梯度估计，实现通信高效和近似最优样本复杂度。 |
| [^166] | [Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings.](http://arxiv.org/abs/2201.05575) | 本文提出了一种新的知识图嵌入方法kNN-KGE，它通过预训练语言模型和最近邻的线性插值，允许罕见或新出现的实体被明确地记忆，而不是隐藏在模型参数中。实验结果显示，该方法能够改善链接预测结果并在低资源环境中表现出更好的性能。 |
| [^167] | [Persistent Homological State-Space Estimation of Functional Human Brain Networks at Rest.](http://arxiv.org/abs/2201.00087) | 该论文提出了一种新的数据驱动的拓扑数据分析方法，用于估计静息状态下人类脑网络的状态空间。该方法通过惩罚网络之间的拓扑距离将动态变化的脑网络聚类为不同的状态，并通过考虑时间维度来提高准确性。研究还发现脑网络的整体拓扑具有遗传特征。 |
| [^168] | [Last-Iterate Convergence of Saddle-Point Optimizers via High-Resolution Differential Equations.](http://arxiv.org/abs/2112.13826) | 本研究利用高分辨率微分方程在鞍点优化中设计了不同的微分方程模型，这些模型在双线性博弈中的收敛性质与离散方法相匹配。 |
| [^169] | [Spectral learning of multivariate extremes.](http://arxiv.org/abs/2111.07799) | 我们提出了一种用于分析多元极值的谱聚类算法，并通过理论和数值实验展示了其在学习角度测度方面的性能。 |
| [^170] | [Physics-Informed Neural Operator for Learning Partial Differential Equations.](http://arxiv.org/abs/2111.03794) | 本文提出了一种名为PINO的方法，它能够通过同时利用数据和物理约束条件来学习偏微分方程的解算子，克服了纯数据驱动和基于物理的方法的局限性，并且可以在不同分辨率上合并数据和约束条件。 |
| [^171] | [Deep Exploration for Recommendation Systems.](http://arxiv.org/abs/2109.12509) | 本文提出了一种深度探索方法以解决推荐系统中奖励稀少时的问题，并在高保真度的工业级模拟器下进行了实验，证明了该算法相比现有算法有很大的提升。 |
| [^172] | [Offline Decentralized Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2108.01832) | 本文提出了一种离线分散式多智能体强化学习框架，通过利用价值偏差和转移规范化来修改转移概率，使智能体能够学习到高性能和协调的策略。 |
| [^173] | [Reverse Engineering of Generative Models: Inferring Model Hyperparameters from Generated Images.](http://arxiv.org/abs/2106.07873) | 本研究提出了一种逆向工程生成模型的方法，通过分析生成的图像来推断模型的超参数，以识别和理解生成模型的潜在滥用。通过指纹估计网络和解析网络，我们能够从生成的图像中预测生成模型的网络架构和训练损失函数。 |
| [^174] | [Nonasymptotic theory for two-layer neural networks: Beyond the bias-variance trade-off.](http://arxiv.org/abs/2106.04795) | 这项研究提出了针对两层神经网络的非渐近泛化理论，通过引入缩放变分正则化，并利用"岭-套索对偶性"获得了新的预测界限，解释了大型神经网络在超参数化情况下的表现以及双谷现象。 |
| [^175] | [Exploring Flip Flop memories and beyond: training recurrent neural networks with key insights.](http://arxiv.org/abs/2010.07858) | 本研究通过全面调查和描述时间处理任务的实现，特别是3位锁存器存储器，为循环神经网络的训练方法提供了关键见解和贡献。 |
| [^176] | [Modelling of functional profiles and explainable shape shifts detection: An approach combining the notion of the Fr\'echet mean with the shape invariant model}.](http://arxiv.org/abs/2010.02968) | 该论文提出了一种结合Fréchet均值和形状不变模型的方法，用于检测功能性轮廓中的形状变化，并构建了功能性数据的控制图，可解释性强且能识别潜在变化。 |
| [^177] | [Fair Algorithms for Hierarchical Agglomerative Clustering.](http://arxiv.org/abs/2005.03197) | 提出了一种公平的层次聚类算法，该算法不受距离链接准则的限制，能适应不同的公平度量标准，并可以处理多个受保护群体。 |
| [^178] | [Generalization Bounds and Representation Learning for Estimation of Potential Outcomes and Causal Effects.](http://arxiv.org/abs/2001.07426) | 本论文研究了从记录的背景、决策和结果中估计个体层面的因果效应的问题，给出了基于距离度量的广义化界限以及相应的样本重新加权方法，并设计了最小化界限的表示学习算法来实现估计的准确性。 |
| [^179] | [Recovery Guarantees for Quadratic Tensors with Sparse Observations.](http://arxiv.org/abs/1811.00148) | 本研究考察了稀疏观测下的二次张量恢复问题，发现非凸方法能够在线性样本数量下保证误差最小化问题的全局极小值，并改进了观测有限情况下的CP模型性能。 |
| [^180] | [Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network.](http://arxiv.org/abs/1808.03314) | 本文通过形式化推导来解释了递归神经网络（RNN）和长短期记忆（LSTM）网络的基本原理，并提出了一种将RNN转化为“Vanilla LSTM”网络的方法。 |

# 详细

[^1]: 针对基于查询的黑盒攻击的有状态防御的理论权衡

    Theoretically Principled Trade-off for Stateful Defenses against Query-Based Black-Box Attacks. (arXiv:2307.16331v1 [cs.LG])

    [http://arxiv.org/abs/2307.16331](http://arxiv.org/abs/2307.16331)

    本文提供了一种针对有状态防御的检测率和误报率之间的权衡的理论描述，并给出了特征提取器检测率的上界。

    

    对抗性样本即使在受限的黑盒条件下也对机器学习系统的完整性构成威胁，取得了惊人的成功率。有状态防御已经成为一种有效的对策，通过维护最近查询的缓冲区并检测相似度过高的新查询来检测潜在的攻击。然而，这些防御在攻击检测和误报率之间存在基本的权衡，通常通过手动选择特征提取器和相似性阈值来优化此权衡，这些方法在实践中表现良好，但对于这个权衡的形式限制以及影响它的特征提取器和基础问题域的确切属性缺乏充分的理解。本文旨在通过提供对有状态防御的检测率和误报率之间的权衡的理论描述来填补这一差距。我们给出了一般类特征提取器检测率的上界，并分析了特征提取器和基本问题域的属性。

    Adversarial examples threaten the integrity of machine learning systems with alarming success rates even under constrained black-box conditions. Stateful defenses have emerged as an effective countermeasure, detecting potential attacks by maintaining a buffer of recent queries and detecting new queries that are too similar. However, these defenses fundamentally pose a trade-off between attack detection and false positive rates, and this trade-off is typically optimized by hand-picking feature extractors and similarity thresholds that empirically work well. There is little current understanding as to the formal limits of this trade-off and the exact properties of the feature extractors/underlying problem domain that influence it. This work aims to address this gap by offering a theoretical characterization of the trade-off between detection and false positive rates for stateful defenses. We provide upper bounds for detection rates of a general class of feature extractors and analyze the
    
[^2]: RoseNNa: 一种用于神经网络推断的高效、可移植的库，应用于计算流体动力学

    RoseNNa: A performant, portable library for neural network inference with application to computational fluid dynamics. (arXiv:2307.16322v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2307.16322](http://arxiv.org/abs/2307.16322)

    RoseNNa是一种用于神经网络推断的高效、可移植的库，通过自动模型转换和集成，无需修改现有求解器代码即可将神经网络应用于计算流体动力学问题。

    

    神经网络机器学习的兴起引发了高级库的出现，包括TensorFlow和PyTorch，以支持它们的功能。计算流体动力学（CFD）研究人员受益于这一趋势，产生了可以缩短模拟时间的强大神经网络。例如，多层感知机（MLPs）和长短期记忆（LSTM）循环神经网络（RNN）体系结构可以表示诸如湍流等子网格物理效应。在CFD求解器中实现神经网络是具有挑战性的，因为用于机器学习和CFD的编程语言大多不重叠。我们介绍了roseNNa库，它弥合了神经网络推断和CFD之间的差距。RoseNNa是一种非侵入性、轻量级（1000行代码）且高效的神经网络推断工具，重点关注于增强PDE求解器（如CFD）中使用的较小网络，这些求解器通常是用C/C++或Fortran编写的。RoseNNa通过自动地进行模型转换和集成来实现这一目标，无需修改现有求解器代码即可将神经网络应用于CFD问题。

    The rise of neural network-based machine learning ushered in high-level libraries, including TensorFlow and PyTorch, to support their functionality. Computational fluid dynamics (CFD) researchers have benefited from this trend and produced powerful neural networks that promise shorter simulation times. For example, multilayer perceptrons (MLPs) and Long Short Term Memory (LSTM) recurrent-based (RNN) architectures can represent sub-grid physical effects, like turbulence. Implementing neural networks in CFD solvers is challenging because the programming languages used for machine learning and CFD are mostly non-overlapping, We present the roseNNa library, which bridges the gap between neural network inference and CFD. RoseNNa is a non-invasive, lightweight (1000 lines), and performant tool for neural network inference, with focus on the smaller networks used to augment PDE solvers, like those of CFD, which are typically written in C/C++ or Fortran. RoseNNa accomplishes this by automatica
    
[^3]: 面向线性回归的实用鲁棒性审计

    Towards Practical Robustness Auditing for Linear Regression. (arXiv:2307.16315v1 [stat.ME])

    [http://arxiv.org/abs/2307.16315](http://arxiv.org/abs/2307.16315)

    本论文研究了实用的算法，用于查找或证伪数据集中对普通最小二乘回归具有影响的小子集。通过实证研究发现，这些算法方法在鲁棒性检查中表现良好，并提供了对低维回归问题的有用检查。但对于高维回归问题，计算瓶颈仍然存在。通过使用新颖的谱算法，我们取得了一些进展。

    

    我们研究了实用的算法，用于找到或证伪一个数据集中的小子集，当移除这些子集时，会改变普通最小二乘回归中的系数的符号。我们通过实证研究了用于此任务的先进算法技术的性能 - 混合整数二次约束优化用于一般线性回归问题，以及对特殊情况的确切贪婪方法。我们证明这些方法在很大程度上胜过了现有技术，并且为低维回归问题提供了有用的鲁棒性检查。然而，对于维度为3或更高的回归问题，仍然存在重要的计算瓶颈，特别是对于证伪这种小而具影响力的样本集合的存在。通过使用最近算法鲁棒统计领域创新的思想，我们在这一挑战上取得了一些进展，利用谱算法。我们总结了已知技术的局限性。

    We investigate practical algorithms to find or disprove the existence of small subsets of a dataset which, when removed, reverse the sign of a coefficient in an ordinary least squares regression involving that dataset. We empirically study the performance of well-established algorithmic techniques for this task -- mixed integer quadratically constrained optimization for general linear regression problems and exact greedy methods for special cases. We show that these methods largely outperform the state of the art and provide a useful robustness check for regression problems in a few dimensions. However, significant computational bottlenecks remain, especially for the important task of disproving the existence of such small sets of influential samples for regression problems of dimension $3$ or greater. We make some headway on this challenge via a spectral algorithm using ideas drawn from recent innovations in algorithmic robust statistics. We summarize the limitations of known techniqu
    
[^4]: 基于掩模的数据增强用于罕见肝细胞癌的多参数MRI生成

    Mask-guided Data Augmentation for Multiparametric MRI Generation with a Rare Hepatocellular Carcinoma. (arXiv:2307.16314v1 [eess.IV])

    [http://arxiv.org/abs/2307.16314](http://arxiv.org/abs/2307.16314)

    本文提出了一种基于掩模的数据增强架构，通过生成式深度学习方法合成具有巨大细小肝细胞癌的多参数MRI图像，并通过肿瘤掩模实现逼真的生成效果。

    

    数据增强被经典地用于提高深度学习模型的整体性能。然而，在医学应用中，特别是对于多参数数据集而言，这是具有挑战性的。例如，在生成合成图像的几个应用中使用的传统几何变换可能会以非现实的方式修改患者的解剖结构。因此，在医学领域需要专门的图像生成技术，例如逼真地模拟给定的病理。本文介绍了一种新的数据增强架构，通过生成式深度学习方法生成巨大细小肝细胞癌的合成多参数（T1动脉、T1门静脉和T2）磁共振图像（MRI）及其对应的肿瘤掩模。所提出的架构通过生成肝脏肿瘤掩模和腹部边缘，在Pix2Pix网络的输入中用于合成数据的创建。该方法的效率已经得到证明。

    Data augmentation is classically used to improve the overall performance of deep learning models. It is, however, challenging in the case of medical applications, and in particular for multiparametric datasets. For example, traditional geometric transformations used in several applications to generate synthetic images can modify in a non-realistic manner the patients' anatomy. Therefore, dedicated image generation techniques are necessary in the medical field to, for example, mimic a given pathology realistically. This paper introduces a new data augmentation architecture that generates synthetic multiparametric (T1 arterial, T1 portal, and T2) magnetic resonance images (MRI) of massive macrotrabecular subtype hepatocellular carcinoma with their corresponding tumor masks through a generative deep learning approach. The proposed architecture creates liver tumor masks and abdominal edges used as input in a Pix2Pix network for synthetic data creation. The method's efficiency is demonstrat
    
[^5]: 《你不得通过：凸优化中的零梯度问题》

    You Shall not Pass: the Zero-Gradient Problem in Predict and Optimize for Convex Optimization. (arXiv:2307.16304v1 [cs.LG])

    [http://arxiv.org/abs/2307.16304](http://arxiv.org/abs/2307.16304)

    这篇论文介绍了预测和优化中的一个问题——零梯度问题，并提出了解决方法。通过利用微分优化的数学特性和真实世界基准的验证，该方法解决了这个问题。

    

    预测和优化是一种越来越受欢迎的决策模式，它利用机器学习来预测优化问题中的未知参数。与最小化参数预测误差不同，它使用任务性能作为损失函数来训练预测模型。在凸优化领域，预测和优化由于最近开发的方法能够对问题参数进行微分，取得了显著进展。本文发现了这种方法的一个迄今未被注意到的缺点——零梯度问题，并提出了一种解决方法。建议的方法基于微分优化的数学特性，并使用两个真实世界的基准进行验证。

    Predict and optimize is an increasingly popular decision-making paradigm that employs machine learning to predict unknown parameters of optimization problems. Instead of minimizing the prediction error of the parameters, it trains predictive models using task performance as a loss function. In the convex optimization domain, predict and optimize has seen significant progress due to recently developed methods for differentiating optimization problem solutions over the problem parameters. This paper identifies a yet unnoticed drawback of this approach -- the zero-gradient problem -- and introduces a method to solve it. The suggested method is based on the mathematical properties of differential optimization and is verified using two real-world benchmarks.
    
[^6]: 使用AutoML和决策森林预测印度下级法院的延迟

    Predicting delays in Indian lower courts using AutoML and Decision Forests. (arXiv:2307.16285v1 [cs.LG])

    [http://arxiv.org/abs/2307.16285](http://arxiv.org/abs/2307.16285)

    本文使用AutoML和决策森林构建了一个分类模型，通过案件信息预测印度下级法院的延迟。最佳模型的准确率为81.4％，该研究证明了基于相关数据点的AI模型在预测印度法院延迟方面的可行性。

    

    本文提出了一个分类模型，根据案件在提交时可得到的信息,来预测印度下级法院的延迟。该模型建立在2010年提交的420万个法院案件及其在10年期间的结果的数据集上。数据集来自印度的7000多个下级法院。作者采用AutoML开发了一个多类别分类模型，覆盖了所有等待期，并使用二进制决策森林分类器提高了延迟分类的预测准确性。最佳模型的准确率达到81.4％，精确度、召回率和F1分别为0.81。研究证明了基于相关数据点（如辖区、法院、法官、主题和相关方）的AI模型在预测印度法院延迟方面的可行性。本文还讨论了相关文献的结果，并提出了改进和未来研究的方向。作者已提供数据集和Python代码文件。

    This paper presents a classification model that predicts delays in Indian lower courts based on case information available at filing. The model is built on a dataset of 4.2 million court cases filed in 2010 and their outcomes over a 10-year period. The data set is drawn from 7000+ lower courts in India. The authors employed AutoML to develop a multi-class classification model over all periods of pendency and then used binary decision forest classifiers to improve predictive accuracy for the classification of delays. The best model achieved an accuracy of 81.4%, and the precision, recall, and F1 were found to be 0.81. The study demonstrates the feasibility of AI models for predicting delays in Indian courts, based on relevant data points such as jurisdiction, court, judge, subject, and the parties involved. The paper also discusses the results in light of relevant literature and suggests areas for improvement and future research. The authors have made the dataset and Python code files u
    
[^7]: zkDL: 高效的深度学习训练零知识证明

    zkDL: Efficient Zero-Knowledge Proofs of Deep Learning Training. (arXiv:2307.16273v1 [cs.LG])

    [http://arxiv.org/abs/2307.16273](http://arxiv.org/abs/2307.16273)

    zkDL是一种高效的深度学习训练零知识证明，通过zkReLU协议和新颖的算术电路构建方案，实现了对深度网络训练过程的有效验证。

    

    深度学习的最新进展给人们的生活带来了重大变化。与此同时，这些快速发展引发了人们对深度网络训练过程的合法性的担忧。然而，为了保护不受信任的人工智能开发者的知识产权，通常禁止验证者通过访问模型参数和训练数据来直接检查训练过程。为了应对这个挑战，我们提出了zkDL，一种高效的深度学习训练零知识证明。zkDL的核心是zkReLU，一种专门针对ReLU激活函数的零知识证明协议，具有优化的证明时间和证明大小，这是可验证训练中的一个主要障碍，因为它的非算术性质。为了将zkReLU整合到整个训练过程的证明系统中，我们设计了一种新颖的从神经网络中构建算术电路的方法。通过利用丰富的并行计算资源，这个构建方案能够有效减少证明时间。

    The recent advancements in deep learning have brought about significant changes in various aspects of people's lives. Meanwhile, these rapid developments have raised concerns about the legitimacy of the training process of deep networks. However, to protect the intellectual properties of untrusted AI developers, directly examining the training process by accessing the model parameters and training data by verifiers is often prohibited.  In response to this challenge, we present zkDL, an efficient zero-knowledge proof of deep learning training. At the core of zkDL is zkReLU, a specialized zero-knowledge proof protocol with optimized proving time and proof size for the ReLU activation function, a major obstacle in verifiable training due to its non-arithmetic nature. To integrate zkReLU into the proof system for the entire training process, we devise a novel construction of an arithmetic circuit from neural networks. By leveraging the abundant parallel computation resources, this constru
    
[^8]: DRL4Route:一种用于接送路线预测的深度强化学习框架

    DRL4Route: A Deep Reinforcement Learning Framework for Pick-up and Delivery Route Prediction. (arXiv:2307.16246v1 [cs.LG])

    [http://arxiv.org/abs/2307.16246](http://arxiv.org/abs/2307.16246)

    DRL4Route是一种用于接送路线预测的深度强化学习框架，结合了深度学习模型的行为学习能力和强化学习的非可微分目标优化能力，解决了训练和测试标准不匹配的问题。

    

    近年来，接送路线预测(PDRP)在预测工人的未来服务路线方面受到越来越多的关注。基于监督学习的深度神经网络由于能够从大量历史数据中捕捉工人行为模式的强大能力而成为该任务的主导模型。虽然有着很大的潜力，但它们未能将不可微分的测试标准引入到训练过程中，导致训练和测试标准不匹配。这在实际系统中使用时极大地削减了它们的性能。为了解决上述问题，我们首次尝试将强化学习(RL)推广到路线预测任务中，从而产生了一种名为DRL4Route的新型RL框架。它结合了先前深度学习模型的行为学习能力和强化学习的非可微分目标优化能力。DRL4Route可以作为插件使用。

    Pick-up and Delivery Route Prediction (PDRP), which aims to estimate the future service route of a worker given his current task pool, has received rising attention in recent years. Deep neural networks based on supervised learning have emerged as the dominant model for the task because of their powerful ability to capture workers' behavior patterns from massive historical data. Though promising, they fail to introduce the non-differentiable test criteria into the training process, leading to a mismatch in training and test criteria. Which considerably trims down their performance when applied in practical systems. To tackle the above issue, we present the first attempt to generalize Reinforcement Learning (RL) to the route prediction task, leading to a novel RL-based framework called DRL4Route. It combines the behavior-learning abilities of previous deep learning models with the non-differentiable objective optimization ability of reinforcement learning. DRL4Route can serve as a plug-
    
[^9]: 突触可塑性模型和生物启发的无监督深度学习：一项综述

    Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey. (arXiv:2307.16236v1 [cs.NE])

    [http://arxiv.org/abs/2307.16236](http://arxiv.org/abs/2307.16236)

    本综述调查了一系列生物启发的突触可塑性模型在深度学习中的应用，并与脉冲神经网络中的可塑性模型进行了联系。生物启发深度学习代表了一个令人兴奋的研究方向，旨在推动当前技术的发展以及对智能的理解。

    

    最近出现的基于深度学习的技术在人工智能领域的各种任务中取得了出色的结果。然而，这些技术面临着对抗输入的鲁棒性、生态影响以及需要大量训练数据的挑战。作为回应，研究人员越来越关注生物学基础机制，这是由于生物大脑所展示的令人印象深刻的能力所吸引。本综述探讨了一系列这些生物启发模型的突触可塑性模型，在深度学习场景中的应用，并与脉冲神经网络中的可塑性模型进行了联系。总的来说，生物启发深度学习代表了一个令人兴奋的研究方向，旨在推动不仅当前技术的发展，而且还有对智能的理解。

    Recently emerged technologies based on Deep Learning (DL) achieved outstanding results on a variety of tasks in the field of Artificial Intelligence (AI). However, these encounter several challenges related to robustness to adversarial inputs, ecological impact, and the necessity of huge amounts of training data. In response, researchers are focusing more and more interest on biologically grounded mechanisms, which are appealing due to the impressive capabilities exhibited by biological brains. This survey explores a range of these biologically inspired models of synaptic plasticity, their application in DL scenarios, and the connections with models of plasticity in Spiking Neural Networks (SNNs). Overall, Bio-Inspired Deep Learning (BIDL) represents an exciting research direction, aiming at advancing not only our current technologies but also our understanding of intelligence.
    
[^10]: 脉冲神经网络和仿生监督深度学习：综述

    Spiking Neural Networks and Bio-Inspired Supervised Deep Learning: A Survey. (arXiv:2307.16235v1 [cs.NE])

    [http://arxiv.org/abs/2307.16235](http://arxiv.org/abs/2307.16235)

    这篇综述文章回顾了基于生物学启发的人工智能方法，介绍了脉冲神经网络模型，并讨论了脉冲神经网络训练的挑战以及生物启发的深度学习方法。

    

    长期以来，生物学和神经科学领域一直是计算机科学家寻求发展人工智能技术的重要灵感来源。本综述旨在全面回顾最近用于人工智能的仿生方法。首先介绍了生物神经元的计算原理和突触可塑性，然后详细介绍了脉冲神经网络模型，并强调了与脉冲神经网络训练相关的主要挑战，传统的反向传播优化方法无法直接应用。因此，我们讨论了最近提出的与反向传播不同的仿生训练方法，适用于传统网络和脉冲网络。生物启发的深度学习方法旨在推动当前模型的计算能力和生物可行性的进步。

    For a long time, biology and neuroscience fields have been a great source of inspiration for computer scientists, towards the development of Artificial Intelligence (AI) technologies. This survey aims at providing a comprehensive review of recent biologically-inspired approaches for AI. After introducing the main principles of computation and synaptic plasticity in biological neurons, we provide a thorough presentation of Spiking Neural Network (SNN) models, and we highlight the main challenges related to SNN training, where traditional backprop-based optimization is not directly applicable. Therefore, we discuss recent bio-inspired training methods, which pose themselves as alternatives to backprop, both for traditional and spiking networks. Bio-Inspired Deep Learning (BIDL) approaches towards advancing the computational capabilities and biological plausibility of current models.
    
[^11]: 自主移动出行系统中电动车平衡的鲁棒性：一种多智能体强化学习方法

    Robust Electric Vehicle Balancing of Autonomous Mobility-On-Demand System: A Multi-Agent Reinforcement Learning Approach. (arXiv:2307.16228v1 [cs.MA])

    [http://arxiv.org/abs/2307.16228](http://arxiv.org/abs/2307.16228)

    本文提出了一种基于多智能体强化学习的方法来解决电动车在自主移动出行系统中的平衡问题，并考虑了车辆供应和需求的不确定性。

    

    由于其经济和社会效益，电动自动驾驶车辆（EAV）正在未来的自主移动出行（AMoD）系统中引起关注。然而，EAV的独特充电模式（长时间充电、高充电频率、无法预测的充电行为等）使得准确预测EAV供应在E-AMoD系统中变得具有挑战性。此外，移动需求的预测不确定性使得在供应和需求不确定性下设计一个集成的车辆平衡解决方案成为紧迫且具有挑战性的任务。尽管基于强化学习的E-AMoD平衡算法取得了成功，但在EV供应或移动需求下的状态不确定性仍未被探索。在这项工作中，我们设计了一个基于多智能体强化学习（MARL）的框架，用于E-AMoD系统中的EAV平衡，其中包含对抗性智能体，用于模拟可能破坏车辆平衡解决方案的EAV供应和移动需求的不确定性。然后，我们提出了一个鲁棒的E-AMoD解决方案。

    Electric autonomous vehicles (EAVs) are getting attention in future autonomous mobility-on-demand (AMoD) systems due to their economic and societal benefits. However, EAVs' unique charging patterns (long charging time, high charging frequency, unpredictable charging behaviors, etc.) make it challenging to accurately predict the EAVs supply in E-AMoD systems. Furthermore, the mobility demand's prediction uncertainty makes it an urgent and challenging task to design an integrated vehicle balancing solution under supply and demand uncertainties. Despite the success of reinforcement learning-based E-AMoD balancing algorithms, state uncertainties under the EV supply or mobility demand remain unexplored. In this work, we design a multi-agent reinforcement learning (MARL)-based framework for EAVs balancing in E-AMoD systems, with adversarial agents to model both the EAVs supply and mobility demand uncertainties that may undermine the vehicle balancing solutions. We then propose a robust E-AMo
    
[^12]: 优化神经网络训练以进行历史希伯来文OCR错误校正

    Optimizing the Neural Network Training for OCR Error Correction of Historical Hebrew Texts. (arXiv:2307.16220v1 [cs.CL])

    [http://arxiv.org/abs/2307.16220](http://arxiv.org/abs/2307.16220)

    这篇论文提出了一种使用较少的手动创建数据训练的轻量级神经网络方法，用于希伯来文OCR后校正。研究目标是开发自动生成训练数据的方法，改善OCR后校正结果，并研究最适用于历史文档的OCR后校正的数据集类型。

    

    在过去几十年中，大量纸质文档如书籍和报纸已经使用光学字符识别技术进行数字化。这种技术在处理历史文档时容易出现错误。为了纠正OCR错误，提出了基于自然语言分析和神经网络等机器学习技术的后处理算法。神经网络的缺点是需要大量手动标记的数据进行训练，而这种数据通常不易获取。本文提出了一种创新的方法，使用较少的手动创建数据来训练轻量级的希伯来文OCR后校正神经网络。主要研究目标是开发一种自动生成语言和任务特定训练数据的方法，以改善神经网络在OCR后校正中的结果，并研究哪种类型的数据集对于历史文档的OCR后校正最有效。为此，进行了一系列实验。

    Over the past few decades, large archives of paper-based documents such as books and newspapers have been digitized using Optical Character Recognition. This technology is error-prone, especially for historical documents. To correct OCR errors, post-processing algorithms have been proposed based on natural language analysis and machine learning techniques such as neural networks. Neural network's disadvantage is the vast amount of manually labeled data required for training, which is often unavailable. This paper proposes an innovative method for training a light-weight neural network for Hebrew OCR post-correction using significantly less manually created data. The main research goal is to develop a method for automatically generating language and task-specific training data to improve the neural network results for OCR post-correction, and to investigate which type of dataset is the most effective for OCR post-correction of historical documents. To this end, a series of experiments u
    
[^13]: 数字人文和信息科学中使用深度神经网络进行文本分析

    Text Analysis Using Deep Neural Networks in Digital Humanities and Information Science. (arXiv:2307.16217v1 [cs.LG])

    [http://arxiv.org/abs/2307.16217](http://arxiv.org/abs/2307.16217)

    本论文研究了在数字人文研究中使用深度神经网络进行文本分析的挑战，包括训练数据的可用性和领域适应的需求。

    

    结合计算技术和人文学科是一项持续进行的努力，旨在使文本、图像、音频、视频和其他艺术品等资源在数字化时代易于获得、可搜索和可分析。在近年来，深度神经网络（DNN）在自动文本分析和自然语言处理（NLP）领域占据主导地位，有时呈现出超人类的表现。DNN是解决数字人文研究中与NLP相关的许多任务（例如拼写检查、语言检测、实体提取、作者检测、问答等）的最先进的机器学习算法。这些有监督算法从大量的“正确”和“错误”示例中学习模式，并将其应用于新的示例。然而，在数字人文研究中使用DNN分析文本资源存在两个主要挑战：（不）可用的训练数据和领域适应的需求。本文通过分析多个使用案例来探讨这些挑战。

    Combining computational technologies and humanities is an ongoing effort aimed at making resources such as texts, images, audio, video, and other artifacts digitally available, searchable, and analyzable. In recent years, deep neural networks (DNN) dominate the field of automatic text analysis and natural language processing (NLP), in some cases presenting a super-human performance. DNNs are the state-of-the-art machine learning algorithms solving many NLP tasks that are relevant for Digital Humanities (DH) research, such as spell checking, language detection, entity extraction, author detection, question answering, and other tasks. These supervised algorithms learn patterns from a large number of "right" and "wrong" examples and apply them to new examples. However, using DNNs for analyzing the text resources in DH research presents two main challenges: (un)availability of training data and a need for domain adaptation. This paper explores these challenges by analyzing multiple use-cas
    
[^14]: 用于半结构异构家族谱知识图的深度神经网络问答

    Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs. (arXiv:2307.16214v1 [cs.CL])

    [http://arxiv.org/abs/2307.16214](http://arxiv.org/abs/2307.16214)

    本研究提出一种用于家族谱问答的深度神经网络方法，该方法将家族谱数据表示为知识图并与非结构化文本结合，使用Transformer模型进行训练。这种方法解决了家族谱领域中模型无法处理图结构和缺乏训练数据集的问题。

    

    随着用户生成的家族谱越来越流行，新的家族谱信息系统得到了开发。最先进的自然语言问答算法使用基于自注意力网络的深度神经网络（DNN）架构。然而，其中一些模型使用基于序列的输入，不适合处理基于图的结构，而基于图的DNN模型则依赖于在家族谱领域中不存在的高度全面的知识图。此外，这些有监督的DNN模型需要在家族谱领域中缺乏的训练数据集。本研究提出了一种用于家族谱问答的端到端方法：1）将家族谱数据表示为知识图，2）将其转换为文本，3）与非结构化文本结合，4）训练基于Transformer的问答模型。为了评估需要专门方法的必要性，对比了微调模式下的模型与使用我们提出的方法的模型。

    With the rising popularity of user-generated genealogical family trees, new genealogical information systems have been developed. State-of-the-art natural question answering algorithms use deep neural network (DNN) architecture based on self-attention networks. However, some of these models use sequence-based inputs and are not suitable to work with graph-based structure, while graph-based DNN models rely on high levels of comprehensiveness of knowledge graphs that is nonexistent in the genealogical domain. Moreover, these supervised DNN models require training datasets that are absent in the genealogical domain. This study proposes an end-to-end approach for question answering using genealogical family trees by: 1) representing genealogical data as knowledge graphs, 2) converting them to texts, 3) combining them with unstructured texts, and 4) training a trans-former-based question answering model. To evaluate the need for a dedicated approach, a comparison between the fine-tuned mode
    
[^15]: 朝着历史希伯来文本OCR错误校正的特定时期优化神经网络

    Toward a Period-Specific Optimized Neural Network for OCR Error Correction of Historical Hebrew Texts. (arXiv:2307.16213v1 [cs.CL])

    [http://arxiv.org/abs/2307.16213](http://arxiv.org/abs/2307.16213)

    本研究针对历史希伯来文本OCR错误校正的问题，提出了一种特定时期优化的神经网络模型。然而，由于希伯来语是形态丰富语言，神经网络所需的训练数据集不够充足，且最佳的网络结构和超参数值尚不明确。另外，不同流派和时期的语言变化可能会影响OCR后校正模型的准确性。

    

    在过去几十年中，使用光学字符识别（OCR）技术对大量纸质历史文档（如书籍和报纸）进行了数字化存档。不幸的是，这种广泛使用的技术在处理写于数百年前的OCR文档时容易出错。神经网络在解决各种文本处理任务，包括OCR后校正方面取得了巨大成功。然而，对于希伯来语这样的形态丰富语言，使用神经网络处理历史语料库的主要缺点是它们需要大量充足的训练数据集来学习。此外，由于希伯来语的独特特点，神经网络在OCR错误校正方面的最佳结构和超参数值（预定义参数）尚不清楚。此外，语言在不同的流派和时期也会发生变化。这些变化可能会影响OCR后校正神经网络模型的准确性。

    Over the past few decades, large archives of paper-based historical documents, such as books and newspapers, have been digitized using the Optical Character Recognition (OCR) technology. Unfortunately, this broadly used technology is error-prone, especially when an OCRed document was written hundreds of years ago. Neural networks have shown great success in solving various text processing tasks, including OCR post-correction. The main disadvantage of using neural networks for historical corpora is the lack of sufficiently large training datasets they require to learn from, especially for morphologically-rich languages like Hebrew. Moreover, it is not clear what are the optimal structure and values of hyperparameters (predefined parameters) of neural networks for OCR error correction in Hebrew due to its unique features. Furthermore, languages change across genres and periods. These changes may affect the accuracy of OCR post-correction neural network models. To overcome these challenge
    
[^16]: 具有状态不确定性的鲁棒多智能体强化学习

    Robust Multi-Agent Reinforcement Learning with State Uncertainty. (arXiv:2307.16212v1 [cs.LG])

    [http://arxiv.org/abs/2307.16212](http://arxiv.org/abs/2307.16212)

    本论文研究了在多智能体强化学习中状态不确定性的问题。通过引入状态扰动对手，将该问题建模为马尔可夫博弈，并提出了鲁棒均衡作为解决概念。

    

    在真实世界的多智能体强化学习（MARL）应用中，由于测量不准确或恶意攻击等原因，智能体可能无法获得完美的状态信息，这给智能体的策略的鲁棒性带来了挑战。虽然鲁棒性在MARL部署中变得越来越重要，但很少有先前的工作研究了MARL中的状态不确定性，无论是在问题的表述还是算法设计方面。受到这个鲁棒性问题及缺乏相应研究的启发，我们在本工作中研究了具有状态不确定性的MARL问题。我们首次将该问题建模为具有状态扰动对手的马尔可夫博弈（MG-SPA），通过引入一组状态扰动对手到马尔可夫博弈中。然后，我们引入鲁棒均衡（RE）作为MG-SPA的解决概念。我们进行了对MG-SPA的基本分析，例如给出了满足鲁棒均衡的条件。

    In real-world multi-agent reinforcement learning (MARL) applications, agents may not have perfect state information (e.g., due to inaccurate measurement or malicious attacks), which challenges the robustness of agents' policies. Though robustness is getting important in MARL deployment, little prior work has studied state uncertainties in MARL, neither in problem formulation nor algorithm design. Motivated by this robustness issue and the lack of corresponding studies, we study the problem of MARL with state uncertainty in this work. We provide the first attempt to the theoretical and empirical analysis of this challenging problem. We first model the problem as a Markov Game with state perturbation adversaries (MG-SPA) by introducing a set of state perturbation adversaries into a Markov Game. We then introduce robust equilibrium (RE) as the solution concept of an MG-SPA. We conduct a fundamental analysis regarding MG-SPA such as giving conditions under which such a robust equilibrium e
    
[^17]: 重新思考多模态实体对齐中的不确定性缺失和模糊的视觉模态

    Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment. (arXiv:2307.16210v1 [cs.AI])

    [http://arxiv.org/abs/2307.16210](http://arxiv.org/abs/2307.16210)

    在多模态实体对齐中，现有的方法忽视了视觉图像的不完整性和模糊性，本文通过分析表明模型在面对不完整性时容易出现过拟合和性能下降的问题。

    

    作为实体对齐（EA）的重要扩展，多模态实体对齐（MMEA）旨在通过利用相关的视觉信息来识别跨不同知识图谱（KGs）之间的相同实体。然而，现有的MMEA方法主要集中在多模态实体特征的融合范式上，而忽视了缺失和内在模糊性的视觉图像所带来的挑战。本文对视觉模态不完整性进行了进一步分析，在我们提出的MMEA-UMVM数据集上对最新的MMEA模型进行了基准测试，该数据集包含涵盖双语和单语对齐KGs的类型，并采用标准（非迭代）和迭代训练范式来评估模型性能。我们的研究表明，在面对模态不完整性时，模型很容易过拟合模态噪声，并在高缺失模态的情况下出现性能振荡或下降。这证明了增加视觉不确定性的问题。

    As a crucial extension of entity alignment (EA), multi-modal entity alignment (MMEA) aims to identify identical entities across disparate knowledge graphs (KGs) by exploiting associated visual information. However, existing MMEA approaches primarily concentrate on the fusion paradigm of multi-modal entity features, while neglecting the challenges presented by the pervasive phenomenon of missing and intrinsic ambiguity of visual images. In this paper, we present a further analysis of visual modality incompleteness, benchmarking latest MMEA models on our proposed dataset MMEA-UMVM, where the types of alignment KGs covering bilingual and monolingual, with standard (non-iterative) and iterative training paradigms to evaluate the model performance. Our research indicates that, in the face of modality incompleteness, models succumb to overfitting the modality noise, and exhibit performance oscillations or declines at high rates of missing modality. This proves that the inclusion of additiona
    
[^18]: 在异构家谱知识图谱上使用深度神经网络进行数字聚合问答的GLOBE方法

    Around the GLOBE: Numerical Aggregation Question-Answering on Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks. (arXiv:2307.16208v1 [cs.CL])

    [http://arxiv.org/abs/2307.16208](http://arxiv.org/abs/2307.16208)

    本文介绍了一种在异构家谱知识图谱上进行数字聚合问答的方法，在基因谱领域提出了自然语言问题并获得准确答案的能力还未被充分研究。

    

    文章介绍了一种在异构家谱知识图谱上进行数字聚合问答的方法，该方法利用深度神经网络实现自然语言问题与精确答案之间的转换。目前，在基因谱领域，提出自然语言问题并获得准确答案的能力还未被充分研究，而研究者在人文和社会科学等领域可以从这种能力中受益匪浅。

    One of the key AI tools for textual corpora exploration is natural language question-answering (QA). Unlike keyword-based search engines, QA algorithms receive and process natural language questions and produce precise answers to these questions, rather than long lists of documents that need to be manually scanned by the users. State-of-the-art QA algorithms based on DNNs were successfully employed in various domains. However, QA in the genealogical domain is still underexplored, while researchers in this field (and other fields in humanities and social sciences) can highly benefit from the ability to ask questions in natural language, receive concrete answers and gain insights hidden within large corpora. While some research has been recently conducted for factual QA in the genealogical domain, to the best of our knowledge, there is no previous research on the more challenging task of numerical aggregation QA (i.e., answering questions combining aggregation functions, e.g., count, ave
    
[^19]: 深度卷积神经网络中的零填充: 特征提取和学习

    Deep Convolutional Neural Networks with Zero-Padding: Feature Extraction and Learning. (arXiv:2307.16203v1 [cs.LG])

    [http://arxiv.org/abs/2307.16203](http://arxiv.org/abs/2307.16203)

    本文研究了在特征提取和学习中使用零填充的深度卷积神经网络的性能，并证明了相比于全连接网络，零填充的DCNNs具有更好的性能和平移不变性。

    

    本文研究了在特征提取和学习中使用零填充的深度卷积神经网络（DCNNs）的性能。在验证了零填充在实现平移等价性和池化在实现平移不变性的作用后，我们证明了使用相同数量的自由参数，任何深度全连接网络（DFCNs）都可以通过具有零填充的DCNNs来表示。这证明了相比于DFCNs在特征提取上，零填充的DCNNs具有更好的性能。因此，我们推导出了具有零填充的DCNNs的普遍一致性，并且展示了其学习过程中的平移不变性。我们的所有理论结果都通过数值实验进行了验证，包括玩具模拟和真实数据运行。

    This paper studies the performance of deep convolutional neural networks (DCNNs) with zero-padding in feature extraction and learning. After verifying the roles of zero-padding in enabling translation-equivalence, and pooling in its translation-invariance driven nature, we show that with similar number of free parameters, any deep fully connected networks (DFCNs) can be represented by DCNNs with zero-padding. This demonstrates that DCNNs with zero-padding is essentially better than DFCNs in feature extraction. Consequently, we derive universal consistency of DCNNs with zero-padding and show its translation-invariance in the learning process. All our theoretical results are verified by numerical experiments including both toy simulations and real-data running.
    
[^20]: 洗牌式差分隐私联邦学习在时间序列数据分析中的应用

    Shuffled Differentially Private Federated Learning for Time Series Data Analytics. (arXiv:2307.16196v1 [cs.LG])

    [http://arxiv.org/abs/2307.16196](http://arxiv.org/abs/2307.16196)

    该论文介绍了一种针对时间序列数据的隐私保护联邦学习算法，通过使用局部差分隐私和洗牌技术，有效实现了在保护隐私的同时提高模型准确性。实验结果表明该方法在时间序列数据分析中具有良好的应用价值。

    

    可信任的联邦学习旨在在确保客户隐私的同时实现最优性能。现有的隐私保护联邦学习方法主要针对图像数据，缺乏时间序列数据的应用，而时间序列数据有许多重要的应用，如机器健康监测、人体活动识别等。此外，对时间序列数据分析模型进行保护性噪音处理可能会严重干扰时态相关的学习，导致精度下降更大。为解决这些问题，我们开发了一种适用于时间序列数据的隐私保护联邦学习算法。具体而言，我们采用了局部差分隐私将隐私保护信任边界扩展到客户端。我们还结合了洗牌技术，实现了对隐私增强，在减小因局部差分隐私而导致的精度下降方面起到了缓解作用。我们在五个时间序列数据集上进行了大量实验。评估结果显示，我们的算法在时间序列数据上取得了良好效果。

    Trustworthy federated learning aims to achieve optimal performance while ensuring clients' privacy. Existing privacy-preserving federated learning approaches are mostly tailored for image data, lacking applications for time series data, which have many important applications, like machine health monitoring, human activity recognition, etc. Furthermore, protective noising on a time series data analytics model can significantly interfere with temporal-dependent learning, leading to a greater decline in accuracy. To address these issues, we develop a privacy-preserving federated learning algorithm for time series data. Specifically, we employ local differential privacy to extend the privacy protection trust boundary to the clients. We also incorporate shuffle techniques to achieve a privacy amplification, mitigating the accuracy decline caused by leveraging local differential privacy. Extensive experiments were conducted on five time series datasets. The evaluation results reveal that our
    
[^21]: 用于16位神经网络训练中数值不稳定性的高效方法

    An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training. (arXiv:2307.16189v1 [cs.LG])

    [http://arxiv.org/abs/2307.16189](http://arxiv.org/abs/2307.16189)

    这项研究探讨了16位计算中机器学习模型的数值不稳定性问题，并提出了一种基于Adam优化器的新方法来提高16位神经网络的学习过程的鲁棒性。

    

    在这项研究中，我们深入探讨了在16位计算中使用流行的优化算法（如RMSProp和Adam）时观察到的数值不稳定性的复杂性。这种不稳定性通常在深度神经网络的训练阶段中出现，导致学习过程受到干扰，从而妨碍了这些模型的有效部署。我们确定了单一超参数epsilon是这种数值不稳定性的主要原因。对16位计算中这些优化器中epsilon的作用进行了深入探索，发现微调其值可以恢复RMSProp和Adam的功能，从而实现有效利用16位神经网络。我们提出了一种新的方法来减轻被发现的数值不稳定性问题。该方法利用Adam优化器的更新，并显著改善了16位神经网络的学习过程的鲁棒性。

    In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit 
    
[^22]: ESP:利用对称性先验知识进行多Agent强化学习

    ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning. (arXiv:2307.16186v1 [cs.MA])

    [http://arxiv.org/abs/2307.16186](http://arxiv.org/abs/2307.16186)

    本文提出了一种利用对称性先验知识的框架来解决多Agent强化学习中的数据效率问题，通过将数据增强和一致性损失集成到现有方法中，能够提高模型训练效率，并且泛化性能良好。

    

    近年来，多Agent强化学习（MARL）取得了令人期待的成果。然而，大多数现有的强化学习方法需要大量的数据进行模型训练。此外，数据高效的强化学习要求构建强大的归纳偏差，在当前的MARL方法中被忽视了。受多Agent系统中对称现象的启发，本文提出了一种框架，通过将数据增强和精心设计的一致性损失集成到现有的MARL方法中，来利用先验知识。此外，所提出的框架是模型无关的，可以应用于大多数当前的MARL算法。对多个具有挑战性的任务进行了实验测试，证明了所提出的框架的有效性。此外，将所提出的框架应用于一个物理多机器人实验平台，展示了它的优越性。

    Multi-agent reinforcement learning (MARL) has achieved promising results in recent years. However, most existing reinforcement learning methods require a large amount of data for model training. In addition, data-efficient reinforcement learning requires the construction of strong inductive biases, which are ignored in the current MARL approaches. Inspired by the symmetry phenomenon in multi-agent systems, this paper proposes a framework for exploiting prior knowledge by integrating data augmentation and a well-designed consistency loss into the existing MARL methods. In addition, the proposed framework is model-agnostic and can be applied to most of the current MARL algorithms. Experimental tests on multiple challenging tasks demonstrate the effectiveness of the proposed framework. Moreover, the proposed framework is applied to a physical multi-robot testbed to show its superiority.
    
[^23]: 统一的图像、视频、音频和语言任务模型

    Unified Model for Image, Video, Audio and Language Tasks. (arXiv:2307.16184v1 [cs.CV])

    [http://arxiv.org/abs/2307.16184](http://arxiv.org/abs/2307.16184)

    UnIVAL是一个统一的模型，可以同时支持图像、视频、音频和语言任务。

    

    大型语言模型（LLMs）使得建立通用代理变得不再是幻想。构建这种通用模型的一个关键难题是任务和模态的多样性和异质性。统一解决方案是一个有希望的解决方案，可以在一个统一的框架内支持多样的任务和模态。虽然一些大型模型（例如Flameigno）经过大规模数据集训练，可以支持超过两个模态，但目前小到中型的统一模型仍然局限于两个模态，通常是图像-文本或视频-文本。我们要提出的问题是：是否可能构建一个高效支持所有模态的统一模型？为了回答这个问题，我们提出了UnIVAL，这是对这个雄心勃勃目标迈出的一步。UnIVAL模型拥有约0.25亿个参数，不依赖于复杂的数据集大小或数十亿参数的模型，将文本、图像、视频和音频统一到一个模型中。

    Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification, allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g., Flamingo (Alayrac et al., 2022), trained on massive datasets, can support more than two modalities, current small to mid-scale unified models are still limited to 2 modalities, usually image-text or video-text. The question that we ask is: is it possible to build efficiently a unified model that can support all modalities? To answer this, we propose UnIVAL, a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text, images, video, and audio into a single model. Our model 
    
[^24]: 基因集合的冗余感知无监督排名方法

    Redundancy-aware unsupervised rankings for collections of gene sets. (arXiv:2307.16182v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.16182](http://arxiv.org/abs/2307.16182)

    该论文提出了一种冗余感知的无监督排名方法，用于对基因集合进行排序。该方法考虑了基因集合的重要性分数以及集合中的单个元素和大小分布，并通过一种技巧规避了计算复杂度的问题。

    

    基因集合的生物学角色被用于将其分组成集合，并且这些集合通常是高维、重叠和冗余的。这导致了对其内容的解释和研究并不直观。为了降低维度或增加可解释性，生物信息学正在寻找解决方案。一种可能性是将重叠的基因集合聚合成更大的通路，但修改后的生物通路很难在生物学上得到证明。我们提出使用重要性分数对集合中的通路进行排名，并从集合覆盖的角度进行研究。所提出的基于Shapley值的分数考虑了单个元素和集合大小在家族中的分布；此外，一种技巧允许我们规避Shapley值计算的指数复杂度。最后，我们解决了在所得排名中考虑冗余感知的挑战。

    The biological roles of gene sets are used to group them into collections. These collections are often characterized by being high-dimensional, overlapping, and redundant families of sets, thus precluding a straightforward interpretation and study of their content. Bioinformatics looked for solutions to reduce their dimension or increase their intepretability. One possibility lies in aggregating overlapping gene sets to create larger pathways, but the modified biological pathways are hardly biologically justifiable. We propose to use importance scores to rank the pathways in the collections studying the context from a set covering perspective. The proposed Shapley values-based scores consider the distribution of the singletons and the size of the sets in the families; Furthermore, a trick allows us to circumvent the usual exponential complexity of Shapley values' computation. Finally, we address the challenge of including a redundancy awareness in the obtained rankings where, in our ca
    
[^25]: 在RKHS中自适应学习密度比率

    Adaptive learning of density ratios in RKHS. (arXiv:2307.16164v1 [cs.LG])

    [http://arxiv.org/abs/2307.16164](http://arxiv.org/abs/2307.16164)

    该论文研究在再生核希尔伯特空间中的一类密度比率估计方法，提出了一种自适应学习的参数选择原则，并在有限样本情况下推导出新的误差界。其方法在二次损失的情况下实现了极小化最优误差率。

    

    从有限数量的密度观测中估计两个概率密度的比率是机器学习和统计学中的一个核心问题，应用包括双样本检验、分歧估计、生成建模、协变量转移适应、条件密度估计和新颖性检测。本研究分析了一大类密度比率估计方法，它们通过在再生核希尔伯特空间（RKHS）中最小化真实密度比率与模型之间的正则Bregman距离。我们推导出新的有限样本误差界，并提出了一种Lepskii类型的参数选择原则，在不知道密度比率的正则性的情况下最小化误差界。在二次损失的特殊情况下，我们的方法自适应地实现了极小化最优误差率。提供了一个数值示例。

    Estimating the ratio of two probability densities from finitely many observations of the densities is a central problem in machine learning and statistics with applications in two-sample testing, divergence estimation, generative modeling, covariate shift adaptation, conditional density estimation, and novelty detection. In this work, we analyze a large class of density ratio estimation methods that minimize a regularized Bregman divergence between the true density ratio and a model in a reproducing kernel Hilbert space (RKHS). We derive new finite-sample error bounds, and we propose a Lepskii type parameter choice principle that minimizes the bounds without knowledge of the regularity of the density ratio. In the special case of quadratic loss, our method adaptively achieves a minimax optimal error rate. A numerical illustration is provided.
    
[^26]: 分布式强化学习的方差控制

    Variance Control for Distributional Reinforcement Learning. (arXiv:2307.16152v1 [cs.LG])

    [http://arxiv.org/abs/2307.16152](http://arxiv.org/abs/2307.16152)

    本研究针对分布式强化学习中Q函数估计器的有效性进行了分析，提出了一种新的估计器QEM和DRL算法QEMRL，在Atari和Mujoco基准任务上取得了显著的样本效率和收敛性能的提升。

    

    尽管在过去的几年中，分布式强化学习（DRL）得到了广泛的研究，但很少有研究探讨了在分布式环境下获得的Q函数估计器的有效性。为了充分了解Q函数的近似误差如何影响整个训练过程，我们进行了一些误差分析，并在理论上展示了如何同时减小误差项的偏差和方差。在这种新的理解下，我们构建了一个新的估计器“Quantiled Expansion Mean”（QEM），并从统计学的角度引入了一个新的DRL算法（QEMRL）。我们在各种Atari和Mujoco基准任务上广泛评估了我们的QEMRL算法，并证明QEMRL在样本效率和收敛性能方面明显优于基线算法。

    Although distributional reinforcement learning (DRL) has been widely examined in the past few years, very few studies investigate the validity of the obtained Q-function estimator in the distributional setting. To fully understand how the approximation errors of the Q-function affect the whole training process, we do some error analysis and theoretically show how to reduce both the bias and the variance of the error terms. With this new understanding, we construct a new estimator \emph{Quantiled Expansion Mean} (QEM) and introduce a new DRL algorithm (QEMRL) from the statistical perspective. We extensively evaluate our QEMRL algorithm on a variety of Atari and Mujoco benchmark tasks and demonstrate that QEMRL achieves significant improvement over baseline algorithms in terms of sample efficiency and convergence performance.
    
[^27]: 智能电网中一种有效的用于能量盗窃检测和预测的LSTM-DDPM方案

    An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid. (arXiv:2307.16149v1 [cs.LG])

    [http://arxiv.org/abs/2307.16149](http://arxiv.org/abs/2307.16149)

    这篇论文提出了一种利用LSTM和DDPM相结合的方案来解决智能电网系统中的能量盗窃检测和预测问题。通过重构和预测误差，系统能够准确识别能量盗窃的实例，并在实验中表现出较好的性能。

    

    能量盗窃检测（ETD）和能量消耗预测（ECF）是智能电网系统中两个相互关联的挑战。共同解决这些问题对于确保系统安全至关重要。本论文解决了智能电网系统中的ETD和ECF的相互关联挑战。所提出的解决方案结合了长短期记忆（LSTM）和去噪扩散概率模型（DDPM），用于生成输入重构和预测。通过利用重构和预测误差，系统能够识别能量盗窃的实例，基于重构误差和预测误差的方法相互补充，可以检测不同类型的攻击。通过在真实和合成数据集上进行大量实验，所提出的方案在ETD和ECF问题上表现优于基准方法。集成方法显著提升了ETD性能，能够准确检测到基准方法未能检测到的能量盗窃攻击。该研究提供了一种可行的解决方案来解决智能电网系统中ETD和ECF的挑战。

    Energy theft detection (ETD) and energy consumption forecasting (ECF) are two interconnected challenges in smart grid systems. Addressing these issues collectively is crucial for ensuring system security. This paper addresses the interconnected challenges of ETD and ECF in smart grid systems. The proposed solution combines long short-term memory (LSTM) and a denoising diffusion probabilistic model (DDPM) to generate input reconstruction and forecasting. By leveraging the reconstruction and forecasting errors, the system identifies instances of energy theft, with the methods based on reconstruction error and forecasting error complementing each other in detecting different types of attacks. Through extensive experiments on real-world and synthetic datasets, the proposed scheme outperforms baseline methods in ETD and ECF problems. The ensemble method significantly enhances ETD performance, accurately detecting energy theft attacks that baseline methods fail to detect. The research offers
    
[^28]: 瞳孔学习机制

    Pupil Learning Mechanism. (arXiv:2307.16141v1 [cs.LG])

    [http://arxiv.org/abs/2307.16141](http://arxiv.org/abs/2307.16141)

    本研究提出了瞳孔学习机制（PLM），通过瞳孔学习过程来修改神经网络结构和权重，解决梯度消失和过拟合问题。实验证实了PLM模块设计的有效性，并证明了该模型相对于其他模型的优势。

    

    在人工神经网络的研究中，很少涉及梯度消失和过拟合问题。在本研究中，我们遵循瞳孔学习过程，通过解释、选择、理解、背诵和整理等特征，推导出修改两层神经网络（2LNNs）的网络结构和权重的瞳孔学习机制（PLM）。PLM包括顺序学习、自适应学习、完美学习和少过拟合学习等模块。基于一个铜价预测数据集，我们进行了实验证明了PLM模块设计的有效性，并评估了PLM的性能。实证结果确实证实了PLM模块设计的优越性，并显示出所提出的PLM模型相对于线性回归模型和基于传统反向传播的2LNN模型的优势。

    Studies on artificial neural networks rarely address both vanishing gradients and overfitting issues. In this study, we follow the pupil learning procedure, which has the features of interpreting, picking, understanding, cramming, and organizing, to derive the pupil learning mechanism (PLM) by which to modify the network structure and weights of 2-layer neural networks (2LNNs). The PLM consists of modules for sequential learning, adaptive learning, perfect learning, and less-overfitted learning. Based upon a copper price forecasting dataset, we conduct an experiment to validate the PLM module design modules, and an experiment to evaluate the performance of PLM. The empirical results indeed approve the PLM module design and show the superiority of the proposed PLM model over the linear regression model and the conventional backpropagation-based 2LNN model.
    
[^29]: 用户可控的大型语言模型中的知识融合：平衡创造力和幻觉

    User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination. (arXiv:2307.16139v1 [cs.CL])

    [http://arxiv.org/abs/2307.16139](http://arxiv.org/abs/2307.16139)

    本文提出了一种用户可控的机制，用于调节大型语言模型在生成回应时创造力和对外部知识的忠诚度之间的平衡。这种机制通过在训练过程中引入数值标记，并使用自动化过程计算标记的程度，从而实现用户对模型的依赖程度的控制。

    

    在现代对话系统中，使用大型语言模型（LLMs）由于其生成多样、相关且有创造性的回应能力而呈指数增长。尽管LLMs具有这些优点，但在创造力和对外部知识的忠诚度之间取得平衡仍然是一个关键挑战。本文提出了一种创新的用户可控机制，用于调节LLM在想象能力和与事实信息的一致性之间的平衡。我们的方法在LLM的训练的微调阶段中引入一个表示生成回应中对参考知识忠诚度程度的数值标记。这个程度是通过自动化过程计算的，该过程使用ROUGE分数衡量词汇重叠，使用Sentence-BERT嵌入衡量语义相似性，以及LLM的自我评估分数。在模型推理过程中，用户可以操作这个数值标记，从而控制LLM对外部知识的依赖程度。

    In modern dialogue systems, the use of Large Language Models (LLMs) has grown exponentially due to their capacity to generate diverse, relevant, and creative responses. Despite their strengths, striking a balance between the LLMs' creativity and their faithfulness to external knowledge remains a key challenge. This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information. Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses. This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score. During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledg
    
[^30]: 深度展开网络与循环动量加速用于非线性反问题

    Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems. (arXiv:2307.16120v1 [cs.LG])

    [http://arxiv.org/abs/2307.16120](http://arxiv.org/abs/2307.16120)

    本文提出了一种使用循环动量加速的深度展开网络，该网络能够有效应用于非线性逆向成像问题。通过利用长短期记忆循环神经网络学习和保留先前梯度的知识，该方法在两个非线性逆向问题上获得了良好的结果。

    

    结合基于模型的迭代算法和数据驱动的深度学习解决方案，深度展开网络(DuNets)已成为解决逆向成像问题的流行工具。虽然DuNets已成功应用于许多线性逆向问题，但非线性问题往往会影响方法的性能。受优化算法中常用的动量加速技术启发，我们提出了一种循环动量加速(RMA)框架，该框架使用长短期记忆循环神经网络(LSTM-RNN)来模拟动量加速过程。RMA模块利用LSTM-RNN学习和保留先前梯度的知识能力。我们将RMA应用于两种流行的DuNets——学习的近端梯度下降(LPGD)和学习的原始-对偶(LPD)方法，分别得到LPGD-RMA和LPD-RMA。我们在两个非线性逆向问题上提供了实验结果：非线性去卷积问题、

    Combining the strengths of model-based iterative algorithms and data-driven deep learning solutions, deep unrolling networks (DuNets) have become a popular tool to solve inverse imaging problems. While DuNets have been successfully applied to many linear inverse problems, nonlinear problems tend to impair the performance of the method. Inspired by momentum acceleration techniques that are often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, an
    
[^31]: TMPNN: 基于Taylor映射因式分解的高阶多项式回归

    TMPNN: High-Order Polynomial Regression Based on Taylor Map Factorization. (arXiv:2307.16105v1 [cs.LG])

    [http://arxiv.org/abs/2307.16105](http://arxiv.org/abs/2307.16105)

    本文提出了一种基于Taylor映射因式分解的高阶多项式回归方法，可以实现多目标回归和捕捉目标之间的内在关系。通过在不同的数据集上进行基准测试，表明该方法在特定任务上优于其他回归方法。

    

    多项式回归被广泛应用于表达非线性模式。然而，考虑到很高的多项式阶数可能导致过拟合和对未见数据的拟合能力差。本文提出了一种基于Taylor映射因式分解构建高阶多项式回归的方法。该方法自然地实现了多目标回归，并能捕捉目标之间的内在关系。此外，我们介绍了一种模型解释方法，以微分方程系统的形式呈现。通过在UCI开放数据集，Feynman象征回归数据集和Friedman-1数据集上进行基准测试，我们证明了提出的方法与最先进的回归方法相当，并在特定任务上表现出色。

    Polynomial regression is widely used and can help to express nonlinear patterns. However, considering very high polynomial orders may lead to overfitting and poor extrapolation ability for unseen data. The paper presents a method for constructing a high-order polynomial regression based on the Taylor map factorization. This method naturally implements multi-target regression and can capture internal relationships between targets. Additionally, we introduce an approach for model interpretation in the form of systems of differential equations. By benchmarking on UCI open access datasets, Feynman symbolic regression datasets, and Friedman-1 datasets, we demonstrate that the proposed method performs comparable to the state-of-the-art regression methods and outperforms them on specific tasks.
    
[^32]: 人工智能提高了全球可靠洪水预警的覆盖范围

    AI Increases Global Access to Reliable Flood Forecasts. (arXiv:2307.16104v1 [cs.LG])

    [http://arxiv.org/abs/2307.16104](http://arxiv.org/abs/2307.16104)

    本研究开发了一个人工智能模型，可以准确预测未经测量流域的极端水文事件，从而提高了全球洪水预警的覆盖范围。

    

    洪水是最常见和影响最大的自然灾害之一，对发展中国家尤其具有不对称的影响，这些国家往往缺乏密集的水流监测网络。准确及时的预警对于减轻洪水风险至关重要，但准确的水文模拟模型通常需要根据每个应用的流域中的长时间数据记录进行校准。我们开发了一个人工智能（AI）模型，可以预测7天内的极端水文事件。该模型在所有大洲、前导时间和重现期中均明显优于当前最先进的全球水文模型（Copernicus应急管理服务全球洪水意识系统）。AI在未经测量的流域中的预测尤其有效，这很重要，因为全球只有百分之几的流域具有流量观测站，而发展中国家的未经测量的流域数量占比很高，对人类特别脆弱。

    Floods are one of the most common and impactful natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow monitoring networks. Accurate and timely warnings are critical for mitigating flood risks, but accurate hydrological simulation models typically must be calibrated to long data records in each watershed where they are applied. We developed an Artificial Intelligence (AI) model to predict extreme hydrological events at timescales up to 7 days in advance. This model significantly outperforms current state of the art global hydrology models (the Copernicus Emergency Management Service Global Flood Awareness System) across all continents, lead times, and return periods. AI is especially effective at forecasting in ungauged basins, which is important because only a few percent of the world's watersheds have stream gauges, with a disproportionate number of ungauged basins in developing countries that are especially vulnerable to the human 
    
[^33]: 关于神经网络近似理想对抗攻击和对抗训练收敛性的论文翻译

    On Neural Network approximation of ideal adversarial attack and convergence of adversarial training. (arXiv:2307.16099v1 [cs.LG])

    [http://arxiv.org/abs/2307.16099](http://arxiv.org/abs/2307.16099)

    这项研究通过使用神经网络对理想的对抗攻击进行近似表示，并将对抗训练转化为进攻网络和防守网络之间的数学博弈，同时给出了对抗训练在样本大小$n$下的收敛速度。

    

    对抗攻击通常是通过对输入数据和模型进行基于梯度的操作来实现的，这导致每次生成攻击时都需要进行大量的计算。在这项工作中，我们将对抗攻击表示为可训练的函数的思想更加巩固，而无需进一步计算梯度。我们首先激发出在适当条件下，理论上的最佳攻击可以表示为光滑的分段函数（分段H\"older函数）。然后我们通过神经网络得到了这些函数的近似结果。随后，我们通过神经网络模拟理想的攻击过程，并将对抗训练化简为进攻网络和防守模型（防守网络）之间的数学博弈。在这样的设置中，我们还得到了对抗训练的样本大小$n$对于对抗损失的收敛速度。

    Adversarial attacks are usually expressed in terms of a gradient-based operation on the input data and model, this results in heavy computations every time an attack is generated. In this work, we solidify the idea of representing adversarial attacks as a trainable function, without further gradient computation. We first motivate that the theoretical best attacks, under proper conditions, can be represented as smooth piece-wise functions (piece-wise H\"older functions). Then we obtain an approximation result of such functions by a neural network. Subsequently, we emulate the ideal attack process by a neural network and reduce the adversarial training to a mathematical game between an attack network and a training model (a defense network). We also obtain convergence rates of adversarial loss in terms of the sample size $n$ for adversarial training in such a setting.
    
[^34]: ADR-GNN：平流-扩散-反应图神经网络

    ADR-GNN: Advection-Diffusion-Reaction Graph Neural Networks. (arXiv:2307.16092v1 [cs.LG])

    [http://arxiv.org/abs/2307.16092](http://arxiv.org/abs/2307.16092)

    本文提出了ADR-GNN，一种基于平流-扩散-反应系统的图神经网络架构。ADR-GNN在涉及平流的复杂现象建模方面具有优势，并在节点分类和时空数据集上显示出与最先进网络相比的改进或竞争性表现。

    

    图神经网络在学习图结构数据表示方面取得了显著的成功。然而，GNN在建模涉及平流的复杂现象方面仍面临挑战。本文提出了一种基于平流-扩散-反应系统的新型GNN架构，称为ADR-GNN。平流模式化了信息的有向传输，扩散捕捉了信息的局部平滑，反应代表了通道中信息的非线性转换。我们对ADR-GNN的定性行为进行了分析，显示了将平流、扩散和反应结合的好处。为了证明其有效性，我们在真实世界的节点分类和时空数据集上评估了ADR-GNN，并展示了它相对于最先进网络的改进或竞争性表现。

    Graph neural networks (GNNs) have shown remarkable success in learning representations for graph-structured data. However, GNNs still face challenges in modeling complex phenomena that involve advection. In this paper, we propose a novel GNN architecture based on Advection-Diffusion-Reaction systems, called ADR-GNN. Advection models the directed transportation of information, diffusion captures the local smoothing of information, and reaction represents the non-linear transformation of information in channels. We provide an analysis of the qualitative behavior of ADR-GNN, that shows the benefit of combining advection, diffusion, and reaction. To demonstrate its efficacy, we evaluate ADR-GNN on real-world node classification and spatio-temporal datasets, and show that it improves or offers competitive performance compared to state-of-the-art networks.
    
[^35]: 使用傅里叶神经算子的快速洪水淹没预测

    Rapid Flood Inundation Forecast Using Fourier Neural Operator. (arXiv:2307.16090v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2307.16090](http://arxiv.org/abs/2307.16090)

    使用傅里叶神经算子进行淹没预测的混合方法，在洪水事件中表现出高的准确性和普适性。

    

    洪水淹没预测为洪水事件前和洪水事件期间的紧急规划提供了重要的信息。实时洪水淹没预测工具仍然缺乏。近年来，高分辨率的水动力模型已变得更加可行，但是实时预测街道和建筑物水深仍然需要大量计算资源。在这里，我们提出了一个基于过程和数据驱动机器学习（ML）的混合方法，用于洪水范围和淹没深度的预测。我们使用傅里叶神经算子（FNO），一种高效的ML方法，进行替代建模。通过使用六个历史风暴事件中的模拟水深（每15分钟）进行训练，在休斯顿（美国德克萨斯州）的城市区域中演示了FNO模型，然后在两个预留事件上进行了测试。结果显示，FNO优于基准的U-Net模型。它在所有测试的前导时间（最长为3小时）内保持高的预测能力，并在应用于新地点时表现良好，说明其具有良好的普适性。

    Flood inundation forecast provides critical information for emergency planning before and during flood events. Real time flood inundation forecast tools are still lacking. High-resolution hydrodynamic modeling has become more accessible in recent years, however, predicting flood extents at the street and building levels in real-time is still computationally demanding. Here we present a hybrid process-based and data-driven machine learning (ML) approach for flood extent and inundation depth prediction. We used the Fourier neural operator (FNO), a highly efficient ML method, for surrogate modeling. The FNO model is demonstrated over an urban area in Houston (Texas, U.S.) by training using simulated water depths (in 15-min intervals) from six historical storm events and then tested over two holdout events. Results show FNO outperforms the baseline U-Net model. It maintains high predictability at all lead times tested (up to 3 hrs) and performs well when applying to new sites, suggesting s
    
[^36]: 使用隐式行为克隆和动态运动原语，促进机器人运动规划的强化学习

    Using Implicit Behavior Cloning and Dynamic Movement Primitive to Facilitate Reinforcement Learning for Robot Motion Planning. (arXiv:2307.16062v1 [cs.RO])

    [http://arxiv.org/abs/2307.16062](http://arxiv.org/abs/2307.16062)

    本文提出了一种利用隐式行为克隆和动态运动原语来促进机器人运动规划的强化学习方法。通过利用人类示范数据提高训练速度，以及将运动规划转化为更简单的规划空间，该方法在仿真和实际机器人实验中展示了更快的训练速度和更高的得分。

    

    多自由度机器人的动作规划中，强化学习仍然面临训练速度慢和泛化能力差的问题。本文提出了一种新的基于强化学习的机器人运动规划框架，利用隐式行为克隆和动态运动原语来提高离线策略强化学习代理的训练速度和泛化能力。隐式行为克隆利用人类示范数据提高强化学习的训练速度，而动态运动原语作为一种启发式模型，将运动规划转化为更简单的规划空间。为了支持这一框架，我们还使用拾取-放置实验创建了人类示范数据集，供类似研究使用。在仿真比较实验中，我们发现该方法相比传统的强化学习代理具有更快的训练速度和更高的得分。在实际机器人实验中，该方法展示了在简单组装任务中的适用性。本文提供了一种新的方法，以提高机器人运动规划强化学习的训练速度和泛化能力。

    Reinforcement learning (RL) for motion planning of multi-degree-of-freedom robots still suffers from low efficiency in terms of slow training speed and poor generalizability. In this paper, we propose a novel RL-based robot motion planning framework that uses implicit behavior cloning (IBC) and dynamic movement primitive (DMP) to improve the training speed and generalizability of an off-policy RL agent. IBC utilizes human demonstration data to leverage the training speed of RL, and DMP serves as a heuristic model that transfers motion planning into a simpler planning space. To support this, we also create a human demonstration dataset using a pick-and-place experiment that can be used for similar studies. Comparison studies in simulation reveal the advantage of the proposed method over the conventional RL agents with faster training speed and higher scores. A real-robot experiment indicates the applicability of the proposed method to a simple assembly task. Our work provides a novel pe
    
[^37]: 在电子商务中减轻位置偏差的赞助搜索点击转化多任务模型

    Click-Conversion Multi-Task Model with Position Bias Mitigation for Sponsored Search in eCommerce. (arXiv:2307.16060v1 [cs.IR])

    [http://arxiv.org/abs/2307.16060](http://arxiv.org/abs/2307.16060)

    该论文提出了两种无位置偏差的CTR和CVR预测模型：基于位置感知的点击转化（PACC）和基于位置嵌入的PACC（PACC-PE），通过实验证明这些模型在电子商务搜索中减轻了位置偏差，并具有更好的排名效果。

    

    位置偏差是指用户倾向于关注搜索结果列表中排名较高的项目，而不考虑与查询的实际相关性的现象，这种现象在许多排序系统中普遍存在。训练数据中的位置偏差会使排序模型发生偏差，导致商品排序、点击率（CTR）和转化率（CVR）预测日益不公平。为了共同减轻项目CTR和CVR预测中的位置偏差，我们提出了两种无位置偏差的CTR和CVR预测模型：基于位置感知的点击转化（PACC）和基于位置嵌入的PACC（PACC-PE）。PACC基于概率分解，将位置信息建模为概率。PACC-PE利用神经网络将产品特定的位置信息建模为嵌入。对电子商务赞助产品搜索数据集进行的实验表明，我们提出的模型在排名效果上更好，并且能够大大减轻CTR和CVR预测中的位置偏差。

    Position bias, the phenomenon whereby users tend to focus on higher-ranked items of the search result list regardless of the actual relevance to queries, is prevailing in many ranking systems. Position bias in training data biases the ranking model, leading to increasingly unfair item rankings, click-through-rate (CTR), and conversion rate (CVR) predictions. To jointly mitigate position bias in both item CTR and CVR prediction, we propose two position-bias-free CTR and CVR prediction models: Position-Aware Click-Conversion (PACC) and PACC via Position Embedding (PACC-PE). PACC is built upon probability decomposition and models position information as a probability. PACC-PE utilizes neural networks to model product-specific position information as embedding. Experiments on the E-commerce sponsored product search dataset show that our proposed models have better ranking effectiveness and can greatly alleviate position bias in both CTR and CVR prediction.
    
[^38]: 通过机器学习揭示费波那契准晶叠层中的异域磁相

    Unveiling Exotic Magnetic Phases in Fibonacci Quasicrystalline Stacking of Ferromagnetic Layers through Machine Learning. (arXiv:2307.16052v1 [cond-mat.str-el])

    [http://arxiv.org/abs/2307.16052](http://arxiv.org/abs/2307.16052)

    本研究利用机器学习揭示了费波那契准晶叠层的非共线准周期铁磁配置，该配置具有独特的磁相行为，其中磁化率随着叠层数量的增加呈对数减少。

    

    在这项研究中，我们对费波那契准晶叠层的铁磁层进行了全面的理论分析，该结构可以利用范德华磁性材料实现。我们构建了这种磁性异质结构的模型，该模型包括最多二次邻层磁相互作用，在这个准晶体系中展现了几何挫折和磁序之间的复杂关系。为了遍历参数空间并识别不同的磁相，我们采用了机器学习方法，这证明了在揭示该系统复杂磁性行为方面的强大能力。我们提供了随着模型参数变化的磁相图的详尽描述。值得注意的是，除了其他共线和非共线相，我们发现了一种独特的铁磁交替螺旋相。在这个非共线准周期铁磁配置中，磁化率随着叠层高度的增加呈对数减少。

    In this study, we conduct a comprehensive theoretical analysis of a Fibonacci quasicrystalline stacking of ferromagnetic layers, potentially realizable using van der Waals magnetic materials. We construct a model of this magnetic heterostructure, which includes up to second neighbor interlayer magnetic interactions, that displays a complex relationship between geometric frustration and magnetic order in this quasicrystalline system. To navigate the parameter space and identify distinct magnetic phases, we employ a machine learning approach, which proves to be a powerful tool in revealing the complex magnetic behavior of this system. We offer a thorough description of the magnetic phase diagram as a function of the model parameters. Notably, we discover among other collinear and non-collinear phases, a unique ferromagnetic alternating helical phase. In this non-collinear quasiperiodic ferromagnetic configuration the magnetization decreases logarithmically with the stack height.
    
[^39]: Okapi: 使用强化学习从人类反馈中调优的多语言大型语言模型

    Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback. (arXiv:2307.16039v1 [cs.CL])

    [http://arxiv.org/abs/2307.16039](http://arxiv.org/abs/2307.16039)

    Okapi是一种使用强化学习从人类反馈中调优的多语言大型语言模型，它解决了目前开源语言模型只针对英语和少数流行语言进行指令调优的限制问题。

    

    开发大型语言模型的关键技术之一是指令调优，它有助于将模型的响应与人类预期对齐，实现令人印象深刻的学习能力。两种主要的指令调优方法是监督微调（SFT）和使用人类反馈的强化学习（RLHF），目前已应用于生产最佳的商业语言模型（例如ChatGPT）。为提高语言模型在研究和开发工作中的可访问性，最近还推出了各种经过指令调优的开源语言模型，例如Alpaca、Vicuna等。然而，现有的开源语言模型仅对英语和少数流行语言进行了指令调优，从而限制了它们在全球其他语言中的影响力和可访问性。最近有一些探索多语言大型语言模型指令调优的工作，但目前只使用了SFT作为指令调优的唯一方法。这已经存在了一些问题。

    A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has lef
    
[^40]: 利用机器学习开发具有增强结合亲和力的新型配体对于S1PR1

    Developing novel ligands with enhanced binding affinity for the sphingosine 1-phosphate receptor 1 using machine learning. (arXiv:2307.16037v1 [cs.LG])

    [http://arxiv.org/abs/2307.16037](http://arxiv.org/abs/2307.16037)

    该研究利用机器学习开发了具有增强结合亲和力的新型配体对于多发性硬化症（MS）治疗。研究通过对基于siponimod的分子变体进行大规模生成和筛选，发现了六个有前景的具有良好药物样性和易于合成的候选物，并通过分析发现了几个有助于高结合亲和力的化学性质。

    

    多发性硬化症（MS）是一种影响美国近一百万人的严重神经系统疾病。Sphingosine-1-phosphate receptor 1（S1PR1）是MS的一个蛋白靶点。Siponimod，S1PR1的一个配体，于2019年被FDA批准用于MS治疗，但仍然需要更好的疗法。为此，我们通过优化一个自编码器机器学习模型，将化学公式转化为数学向量，并生成了500多个基于siponimod的分子变体，其中25个化合物对S1PR1的预测结合亲和力更高。该模型能够在不到一小时的时间内生成这些配体。对这些化合物进行筛选，发现了六个具有良好药物样性和易于合成的有前景的候选物。此外，通过分析这些配体的结合相互作用，我们发现了几个有助于高结合亲和力的化学性质。该研究表明，机器学习可以加速药物研发过程，并发现具有增强结合亲和力的新型配体。

    Multiple sclerosis (MS) is a debilitating neurological disease affecting nearly one million people in the United States. Sphingosine-1-phosphate receptor 1, or S1PR1, is a protein target for MS. Siponimod, a ligand of S1PR1, was approved by the FDA in 2019 for MS treatment, but there is a demonstrated need for better therapies. To this end, we finetuned an autoencoder machine learning model that converts chemical formulas into mathematical vectors and generated over 500 molecular variants based on siponimod, out of which 25 compounds had higher predicted binding affinity to S1PR1. The model was able to generate these ligands in just under one hour. Filtering these compounds led to the discovery of six promising candidates with good drug-like properties and ease of synthesis. Furthermore, by analyzing the binding interactions for these ligands, we uncovered several chemical properties that contribute to high binding affinity to S1PR1. This study demonstrates that machine learning can ac
    
[^41]: MUSE: 多视角对比学习用于异质图

    MUSE: Multi-View Contrastive Learning for Heterophilic Graphs. (arXiv:2307.16026v1 [cs.LG])

    [http://arxiv.org/abs/2307.16026](http://arxiv.org/abs/2307.16026)

    MUSE是一种用于异质图的多视角对比学习模型，通过构建两个视角来捕捉自我节点和邻居节点的信息，并整合融合后的节点表示，以增强图神经网络的效果。

    

    近年来，自监督学习已成为解决传统图神经网络中标签依赖和泛化性能差的问题的一种有希望的方法。然而，现有的自监督方法在异质图上的效果有限，这是由于同质性假设导致相邻节点具有类似的节点表示。本文提出了一种多视角对比学习模型MUSE用于异质图。具体而言，我们通过增强对比学习的GNN分别构建了两个视角来捕捉自我节点和邻居节点的信息。然后我们整合这两个视角的信息来融合节点表示。融合对比用于增强融合后的节点表示的效果。此外，考虑到邻域上下文信息对于信息融合的影响可能因不同的自我节点而异，我们使用信息融合控制器对其进行建模。

    In recent years, self-supervised learning has emerged as a promising approach in addressing the issues of label dependency and poor generalization performance in traditional GNNs. However, existing self-supervised methods have limited effectiveness on heterophilic graphs, due to the homophily assumption that results in similar node representations for connected nodes. In this work, we propose a multi-view contrastive learning model for heterophilic graphs, namely, MUSE. Specifically, we construct two views to capture the information of the ego node and its neighborhood by GNNs enhanced with contrastive learning, respectively. Then we integrate the information from these two views to fuse the node representations. Fusion contrast is utilized to enhance the effectiveness of fused node representations. Further, considering that the influence of neighboring contextual information on information fusion may vary across different ego nodes, we employ an information fusion controller to model 
    
[^42]: 模糊逻辑视觉网络（FLVN）：一种神经符号化的视觉特征匹配方法

    Fuzzy Logic Visual Network (FLVN): A neuro-symbolic approach for visual features matching. (arXiv:2307.16019v1 [cs.CV])

    [http://arxiv.org/abs/2307.16019](http://arxiv.org/abs/2307.16019)

    本研究提出了一种神经符号化的视觉特征匹配方法，将模糊逻辑视觉网络（FLVN）应用于零样本学习（ZSL）分类问题，通过神经符号化LTN框架学习视觉-语义嵌入空间，并引入类别层次和高级归纳偏好作为先验知识，以提升分类效果。

    

    神经符号化整合了符号知识表示和深度神经网络的学习能力。通过将一阶逻辑语言具有不可微性质的操作打造成实数张量之间可微分的逻辑张量网络（LTNs），可以将背景知识以逻辑公理的形式纳入其中。然而，仅有少量研究探讨了这种方法在提升零样本学习（ZSL）分类方面的潜在益处。本研究介绍了模糊逻辑视觉网络（FLVN），该网络在神经符号化LTN框架中构建了学习视觉-语义嵌入空间的任务。FLVN在网络中引入了类别层次（类和宏类）的先验知识，以及强大的高级归纳偏好。后者使得网络能够处理类级属性中的异常情况，并确保相同类别的图像之间的相似性，防止过早过拟合已见类别。

    Neuro-symbolic integration aims at harnessing the power of symbolic knowledge representation combined with the learning capabilities of deep neural networks. In particular, Logic Tensor Networks (LTNs) allow to incorporate background knowledge in the form of logical axioms by grounding a first order logic language as differentiable operations between real tensors. Yet, few studies have investigated the potential benefits of this approach to improve zero-shot learning (ZSL) classification. In this study, we present the Fuzzy Logic Visual Network (FLVN) that formulates the task of learning a visual-semantic embedding space within a neuro-symbolic LTN framework. FLVN incorporates prior knowledge in the form of class hierarchies (classes and macro-classes) along with robust high-level inductive biases. The latter allow, for instance, to handle exceptions in class-level attributes, and to enforce similarity between images of the same class, preventing premature overfitting to seen classes a
    
[^43]: UPFL：面向新客户的无监督个性化联邦学习

    UPFL: Unsupervised Personalized Federated Learning towards New Clients. (arXiv:2307.15994v1 [cs.LG])

    [http://arxiv.org/abs/2307.15994](http://arxiv.org/abs/2307.15994)

    本文提出了一种无监督个性化联邦学习方法UPFL，解决了联邦学习中新客户加入时的个性化模型问题。

    

    个性化联邦学习作为解决数据异质性挑战的一种有效方法，已经引起了广泛关注。本文针对联邦学习中一个相对未被探索的问题进行研究。当联邦模型被训练和部署后，一个未标记的新客户加入时，为新客户提供个性化模型成为一项极具挑战性的任务。为了解决这个问题，我们将自适应风险最小化技术扩展到无监督个性化联邦学习的场景，并提出了我们的方法FedTTA。我们进一步通过两种简单且有效的优化策略改进了FedTTA：使用代理正则化增强自适应模型的训练，并通过熵提前停止自适应。此外，我们还提出了一种专为FedTTA设计的知识蒸馏损失，以解决设备异质性问题。对比11个基准方法在5个数据集上的广泛实验表明了我们提出方法的有效性。

    Personalized federated learning has gained significant attention as a promising approach to address the challenge of data heterogeneity. In this paper, we address a relatively unexplored problem in federated learning. When a federated model has been trained and deployed, and an unlabeled new client joins, providing a personalized model for the new client becomes a highly challenging task. To address this challenge, we extend the adaptive risk minimization technique into the unsupervised personalized federated learning setting and propose our method, FedTTA. We further improve FedTTA with two simple yet effective optimization strategies: enhancing the training of the adaptation model with proxy regularization and early-stopping the adaptation through entropy. Moreover, we propose a knowledge distillation loss specifically designed for FedTTA to address the device heterogeneity. Extensive experiments on five datasets against eleven baselines demonstrate the effectiveness of our proposed 
    
[^44]: RGB-D-Fusion: 基于图像条件的人形主体深度扩散

    RGB-D-Fusion: Image Conditioned Depth Diffusion of Humanoid Subjects. (arXiv:2307.15988v1 [cs.CV])

    [http://arxiv.org/abs/2307.15988](http://arxiv.org/abs/2307.15988)

    RGB-D-Fusion是一种多模态条件去噪扩散概率模型，用于生成高分辨率深度图。它通过两个去噪扩散概率模型实现，其中一个是图像条件的，另一个是基于低分辨率RGB-D图像的。此外，还引入了深度噪声增强技术来提高模型的鲁棒性。

    

    我们提出了RGB-D-Fusion，这是一种多模态条件去噪扩散概率模型，用于通过人形主体的低分辨率单目RGB图像生成高分辨率深度图。RGB-D-Fusion首先使用图像条件的去噪扩散概率模型生成低分辨率深度图，然后使用基于低分辨率RGB-D图像的第二个去噪扩散概率模型将深度图上采样。我们还引入了一种新颖的增强技术，深度噪声增强，以增加我们的超分辨率模型的鲁棒性。

    We present RGB-D-Fusion, a multi-modal conditional denoising diffusion probabilistic model to generate high resolution depth maps from low-resolution monocular RGB images of humanoid subjects. RGB-D-Fusion first generates a low-resolution depth map using an image conditioned denoising diffusion probabilistic model and then upsamples the depth map using a second denoising diffusion probabilistic model conditioned on a low-resolution RGB-D image. We further introduce a novel augmentation technique, depth noise augmentation, to increase the robustness of our super-resolution model.
    
[^45]: 通过聚合决策树模型与Boosting模型进行车辆价格预测

    Vehicle Price Prediction By Aggregating decision tree model With Boosting Model. (arXiv:2307.15982v1 [cs.LG])

    [http://arxiv.org/abs/2307.15982](http://arxiv.org/abs/2307.15982)

    本研究使用决策树模型和梯度提升模型结合来预测二手车价格，通过对数据的归一化、标准化和清洗，提高预测准确性。这一方法在评估中表现良好，可用于未来二手车价格的预测。

    

    预测二手车价格是一个有趣且需要的问题。由于需要考虑多个属性以进行准确的预测，车辆价格预测可能是一项具有挑战性的任务。在预测过程中的重要步骤是数据的收集和预处理。本项目使用Python脚本对数据进行归一化、标准化和清洗，以避免机器学习算法受到不必要的干扰。本项目使用的数据集在使用不同的预测技术进行类似研究时可能非常有价值。根据数据集做出了许多假设。提出的系统采用了决策树模型和梯度提升预测模型，通过将两者结合在一起来获得更接近准确预测的结果，该模型已经得到评估并且表现出了良好的性能。在使用相同数据集的情况下，对二手车未来价格的预测将结合多个模型。

    Predicting the price of used vehicles is a more interesting and needed problem by many users. Vehicle price prediction can be a challenging task due to the high number of attributes that should be considered for accurate prediction. The major step in the prediction process is the collection and pre-processing of the data. In this project, python scripts were built to normalize, standardize, and clean data to avoid unnecessary noise for machine learning algorithms. The data set used in this project can be very valuable in conducting similar research using different prediction techniques. Many assumptions were made on the basis of the data set. The proposed system uses a Decision tree model and Gradient boosting predictive model, which are combined in other to get closed to accurate prediction, the proposed model was evaluated and it gives a promising performance. The future price prediction of used vehicles with the help of the same data set will comprise different models.
    
[^46]: 初始化状态干预对解决去除混淆的模仿学习问题的影响

    Initial State Interventions for Deconfounded Imitation Learning. (arXiv:2307.15980v1 [cs.LG])

    [http://arxiv.org/abs/2307.15980](http://arxiv.org/abs/2307.15980)

    本文介绍了一种针对模仿学习中因果混淆问题的初始化状态干预算法，该算法能够遮蔽观测中的混淆因素并提高性能表现。

    

    模仿学习存在因果混淆问题，即学习策略关注的特征并不因果地影响专家的行为，而是表面上的相关性。因果混淆的智能体在训练中产生了低的开环监督损失，但在部署时表现出差的闭环性能。我们考虑在观测空间的分离表示中遮蔽已观测到的混淆因素的问题。我们提出了一种新的遮蔽算法，利用了对初始系统状态进行干预的能力，避免了对专家查询、专家奖励函数或因果图规范的任何要求。在一定的假设下，我们在理论上证明了该算法是保守的，即不会错误地遮蔽因果相关的观测；此外，对初始状态的干预能够严格减少过度保守性。该遮蔽算法被应用于两个示例控制系统的行为克隆中。

    Imitation learning suffers from causal confusion. This phenomenon occurs when learned policies attend to features that do not causally influence the expert actions but are instead spuriously correlated. Causally confused agents produce low open-loop supervised loss but poor closed-loop performance upon deployment. We consider the problem of masking observed confounders in a disentangled representation of the observation space. Our novel masking algorithm leverages the usual ability to intervene in the initial system state, avoiding any requirement involving expert querying, expert reward functions, or causal graph specification. Under certain assumptions, we theoretically prove that this algorithm is conservative in the sense that it does not incorrectly mask observations that causally influence the expert; furthermore, intervening on the initial state serves to strictly reduce excess conservatism. The masking algorithm is applied to behavior cloning for two illustrative control system
    
[^47]: 基于区块链的医疗元宇宙联邦学习：具有最优数据新鲜度的用户中心激励机制

    Blockchain-empowered Federated Learning for Healthcare Metaverses: User-centric Incentive Mechanism with Optimal Data Freshness. (arXiv:2307.15975v1 [cs.GT])

    [http://arxiv.org/abs/2307.15975](http://arxiv.org/abs/2307.15975)

    这篇论文介绍了一种基于区块链的医疗元宇宙联邦学习方法，通过用户中心的激励机制提供最优数据新鲜度，并通过隐私保护框架和跨链技术增强数据安全性。

    

    鉴于元宇宙的革命性作用，医疗元宇宙日益成为一个具有变革性力量的领域，创造了智能医疗系统，提供沉浸式和个性化的服务。医疗元宇宙使用户能够进行有效的决策和数据分析。然而，在构建医疗元宇宙方面仍存在重大挑战，例如敏感数据泄露的风险，感知数据安全和新鲜度问题以及数据共享激励的担忧。在本文中，我们首先设计了一个基于分散式联邦学习（FL）的面向用户的隐私保护框架，用于医疗元宇宙。为了进一步提高医疗元宇宙的隐私保护性能，采用了一个基于跨链的FL框架来增强感知数据的安全性。该框架利用了一个具有主链和多个子链的分层跨链结构，以在医疗元宇宙中进行分散化、隐私保护和安全的数据训练。

    Given the revolutionary role of metaverses, healthcare metaverses are emerging as a transformative force, creating intelligent healthcare systems that offer immersive and personalized services. The healthcare metaverses allow for effective decision-making and data analytics for users. However, there still exist critical challenges in building healthcare metaverses, such as the risk of sensitive data leakage and issues with sensing data security and freshness, as well as concerns around incentivizing data sharing. In this paper, we first design a user-centric privacy-preserving framework based on decentralized Federated Learning (FL) for healthcare metaverses. To further improve the privacy protection of healthcare metaverses, a cross-chain empowered FL framework is utilized to enhance sensing data security. This framework utilizes a hierarchical cross-chain architecture with a main chain and multiple subchains to perform decentralized, privacy-preserving, and secure data training in bo
    
[^48]: 图形压缩方法用于归纳节点表示学习

    Graph Condensation for Inductive Node Representation Learning. (arXiv:2307.15967v1 [cs.LG])

    [http://arxiv.org/abs/2307.15967](http://arxiv.org/abs/2307.15967)

    本论文提出了一种映射感知的图形压缩方法（MCond），通过学习节点之间的映射关系，实现了在合成图中高效地处理未知数据的能力。

    

    大规模图引导网络面临着计算挑战，限制了它们在不同应用中的有效性。为了解决这个问题，图形压缩作为一种有希望的技术出现了，它通过构建一个小的合成图来高效地训练图引导网络并保持性能。然而，由于节点之间的拓扑结构，图形压缩仅限于压缩观察到的训练节点及其对应的结构，因此缺乏有效处理未知数据的能力。因此，在推理阶段仍需要原始大图来对归纳节点进行消息传递，导致计算需求巨大。为了解决这个问题，我们提出了映射感知的图形压缩（MCond）方法，明确学习从原始节点到合成节点的一对多节点映射，以无缝地将新节点整合到合成图中。

    Graph neural networks (GNNs) encounter significant computational challenges when handling large-scale graphs, which severely restricts their efficacy across diverse applications. To address this limitation, graph condensation has emerged as a promising technique, which constructs a small synthetic graph for efficiently training GNNs while retaining performance. However, due to the topology structure among nodes, graph condensation is limited to condensing only the observed training nodes and their corresponding structure, thus lacking the ability to effectively handle the unseen data. Consequently, the original large graph is still required in the inference stage to perform message passing to inductive nodes, resulting in substantial computational demands. To overcome this issue, we propose mapping-aware graph condensation (MCond), explicitly learning the one-to-many node mapping from original nodes to synthetic nodes to seamlessly integrate new nodes into the synthetic graph for induc
    
[^49]: 通过矩阵修正进行推荐系统遗忘

    Recommendation Unlearning via Matrix Correction. (arXiv:2307.15960v1 [cs.IR])

    [http://arxiv.org/abs/2307.15960](http://arxiv.org/abs/2307.15960)

    本文提出了一种Interaction and Mapping Matrices Correction (IMCorrect)方法，可以平衡推荐系统遗忘中的完整性、效用和效率。通过将用户-物品交互矩阵与映射矩阵相乘，IMCorrect能够修正推荐结果，提高推荐系统的性能。

    

    推荐系统对于为用户提供个性化服务非常重要，但大量收集的用户数据引发了有关隐私（如敏感数据）、安全（如恶意数据）和效用（如有害数据）的担忧。为了解决这些挑战，推荐系统遗忘成为一种有前途的方法，允许特定数据和模型被遗忘，以减轻敏感/恶意/有害用户数据的风险。然而，现有的方法通常难以在完整性、效用和效率之间取得平衡，即在其中一个方面妥协，导致子优化的推荐系统遗忘。本文提出了一种Interaction and Mapping Matrices Correction (IMCorrect)方法来进行推荐系统遗忘。首先，我们揭示了许多协同过滤算法可以被制定为基于映射的方法，其中推荐结果可以通过将用户-物品交互矩阵与映射矩阵相乘来获得。然后，IMC

    Recommender systems are important for providing personalized services to users, but the vast amount of collected user data has raised concerns about privacy (e.g., sensitive data), security (e.g., malicious data) and utility (e.g., toxic data). To address these challenges, recommendation unlearning has emerged as a promising approach, which allows specific data and models to be forgotten, mitigating the risks of sensitive/malicious/toxic user data. However, existing methods often struggle to balance completeness, utility, and efficiency, i.e., compromising one for the other, leading to suboptimal recommendation unlearning. In this paper, we propose an Interaction and Mapping Matrices Correction (IMCorrect) method for recommendation unlearning. Firstly, we reveal that many collaborative filtering (CF) algorithms can be formulated as mapping-based approach, in which the recommendation results can be obtained by multiplying the user-item interaction matrix with a mapping matrix. Then, IMC
    
[^50]: 网络拓扑对完全分布式学习的影响：初步调查

    The effect of network topologies on fully decentralized learning: a preliminary investigation. (arXiv:2307.15947v1 [cs.LG])

    [http://arxiv.org/abs/2307.15947](http://arxiv.org/abs/2307.15947)

    本研究通过分析不同网络拓扑结构对分布式学习模型性能的影响，揭示了节点连接性和网络属性在知识传播过程中的不同作用。结果表明，高连接性可能导致更好的模型性能。

    

    在分布式机器学习系统中，数据通常被分割在多个设备或节点之间，每个节点使用自己的数据训练本地模型。然后，这些本地模型被共享和合并，以创建一个可以在新数据上进行准确预测的全局模型。本文开始探索连接节点的网络拓扑对通过节点之间的直接协作训练的机器学习模型性能的影响。我们研究不同类型的拓扑结构如何影响“知识传播”，即节点能够将其他节点的数据中学习到的模式知识融入到本地模型中的能力。具体而言，我们强调了更或者更少连接的节点（中心节点和叶节点）以及宏观网络属性（主要是度分布和模块化）在这个过程中的不同作用。我们展示了，虽然已知弱连接能够保证知识传播，但具有更高的连接性可能会产生更好的模型性能。

    In a decentralized machine learning system, data is typically partitioned among multiple devices or nodes, each of which trains a local model using its own data. These local models are then shared and combined to create a global model that can make accurate predictions on new data. In this paper, we start exploring the role of the network topology connecting nodes on the performance of a Machine Learning model trained through direct collaboration between nodes. We investigate how different types of topologies impact the "spreading of knowledge", i.e., the ability of nodes to incorporate in their local model the knowledge derived by learning patterns in data available in other nodes across the networks. Specifically, we highlight the different roles in this process of more or less connected nodes (hubs and leaves), as well as that of macroscopic network properties (primarily, degree distribution and modularity). Among others, we show that, while it is known that even weak connectivity a
    
[^51]: PIMbot: 多机器人强化学习中社会困境的策略和激励操纵

    PIMbot: Policy and Incentive Manipulation for Multi-Robot Reinforcement Learning in Social Dilemmas. (arXiv:2307.15944v1 [cs.RO])

    [http://arxiv.org/abs/2307.15944](http://arxiv.org/abs/2307.15944)

    PIMbot是一种新的多机器人协作中奖励函数操纵的方法，通过策略和激励操纵来影响多机器人通信以实现不同的结果。

    

    最近的研究证明了强化学习在实现多机器人协作，尤其是在面临自我利益与集体利益之间的社会困境时的潜力。然而，诸如信息传递不畅和对抗机器人之类的环境因素可能会影响合作，因此必须探索如何操纵多机器人通信以实现不同的结果。本文提出了一种新方法，即PIMbot，通过两种不同形式的操纵（策略和激励操纵）来操纵多机器人协作中的奖励函数。我们的工作为最近的多智能体强化学习社会困境引入了一种新的操纵角度，该角度利用了独特的奖励函数来进行激励。通过利用我们提出的PIMbot机制，机器人能够有效地操纵社会困境环境。PIMbot对任务结果可能产生积极和消极的影响。

    Recent research has demonstrated the potential of reinforcement learning (RL) in enabling effective multi-robot collaboration, particularly in social dilemmas where robots face a trade-off between self-interests and collective benefits. However, environmental factors such as miscommunication and adversarial robots can impact cooperation, making it crucial to explore how multi-robot communication can be manipulated to achieve different outcomes. This paper presents a novel approach, namely PIMbot, to manipulating the reward function in multi-robot collaboration through two distinct forms of manipulation: policy and incentive manipulation. Our work introduces a new angle for manipulation in recent multi-agent RL social dilemmas that utilize a unique reward function for incentivization. By utilizing our proposed PIMbot mechanisms, a robot is able to manipulate the social dilemma environment effectively. PIMbot has the potential for both positive and negative impacts on the task outcome, w
    
[^52]: 在预测自动缩放中的持续学习

    Continual Learning in Predictive Autoscaling. (arXiv:2307.15941v1 [cs.LG])

    [http://arxiv.org/abs/2307.15941](http://arxiv.org/abs/2307.15941)

    本论文提出了一种基于重放的持续学习方法，使用少量历史数据，解决了预测自动缩放中的性能下降问题。

    

    预测自动缩放被用于预测服务器的工作负载，并提前准备资源，以确保在动态云环境中的服务水平目标（SLOs）。然而，在实践中，其预测任务常常在外部事件（如促销活动和应用程序重新配置）引起的异常流量下性能下降，常见的解决方案是使用长时间历史数据重新训练模型，但代价是高计算和存储成本。为了更好地解决这个问题，我们提出了一种基于重放的持续学习方法，即基于密度的记忆选择和基于提示的网络学习模型（DMSHM），只使用一小部分历史日志数据来实现准确的预测。首先，我们发现了在预测任务中应用重放式持续学习时的样本重叠现象。为了克服这一挑战并有效地整合新的样本分布，我们提出了一种基于密度的记忆选择和基于提示的网络学习模型（DMSHM）。

    Predictive Autoscaling is used to forecast the workloads of servers and prepare the resources in advance to ensure service level objectives (SLOs) in dynamic cloud environments.However, in practice, its prediction task often suffers from performance degradation under abnormal traffics caused by external events (such as sales promotional activities and applications' re-configurations), for which a common solution is to re-train the model with data of a long historical period, but at the expense of high computational and storage costs.To better address this problem, we propose a replay-based continual learning method, i.e., Density-based Memory Selection and Hint-based Network Learning Model (DMSHM), using only a small part of the historical log to achieve accurate predictions.First, we discover the phenomenon of sample overlap when applying replay-based continual learning in prediction tasks. In order to surmount this challenge and effectively integrate new sample distribution, we propo
    
[^53]: 语言模型中复杂技能产生的理论

    A Theory for Emergence of Complex Skills in Language Models. (arXiv:2307.15936v1 [cs.LG])

    [http://arxiv.org/abs/2307.15936](http://arxiv.org/abs/2307.15936)

    本文提出了一个统计框架，通过分析语言模型的交叉熵损失与基本语言任务的能力之间的关系，揭示了语言模型中复杂技能产生的机制。研究结果表明，通过扩展定律，预训练模型能够高效学习，并表现出违反通常泛化理论的能力。

    

    当语言模型的参数集合和训练语料库扩大时，新的技能将在 AI 产品中出现的主要驱动因素。这种现象尚不为人所理解，并且通过对基于梯度训练的数学分析提供机械解释似乎很困难。本文采用不同的方法，使用著名的（和经验性的）LLM扩展定律和简单的统计框架来分析出现。贡献包括：（a）一个统计框架将LLM的交叉熵损失与语言任务基本技能的能力相关联。（b）数学分析表明，扩展定律意味着强烈的归纳偏见，使预训练模型能够学习得非常高效。我们非正式地称之为“弹弓泛化”，因为表面上看，它似乎提供了在技能水平上违反通常泛化理论的能力。（c）弹弓泛化的一个关键例子，即在执行任务时的能力。

    A major driver of AI products today is the fact that new skills emerge in language models when their parameter set and training corpora are scaled up. This phenomenon is poorly understood, and a mechanistic explanation via mathematical analysis of gradient-based training seems difficult. The current paper takes a different approach, analysing emergence using the famous (and empirical) Scaling Laws of LLMs and a simple statistical framework. Contributions include: (a) A statistical framework that relates cross-entropy loss of LLMs to competence on the basic skills that underlie language tasks. (b) Mathematical analysis showing that the Scaling Laws imply a strong form of inductive bias that allows the pre-trained model to learn very efficiently. We informally call this {\em slingshot generalization} since naively viewed it appears to give competence levels at skills that violate usual generalization theory. (c) A key example of slingshot generalization, that competence at executing task
    
[^54]: 一个用于免疫库分类和疾病相关免疫受体序列识别的噪声标签学习方法

    A Noisy-Label-Learning Formulation for Immune Repertoire Classification and Disease-Associated Immune Receptor Sequence Identification. (arXiv:2307.15934v1 [cs.LG])

    [http://arxiv.org/abs/2307.15934](http://arxiv.org/abs/2307.15934)

    本文提出了一个用于免疫库分类和疾病相关免疫受体序列识别的噪声标签学习方法，通过设计强健的训练策略和同时训练多个模型来解决传统方法中的噪声标签和确认偏见问题。这对于发展新疫苗和免疫疗法具有重要意义。

    

    免疫库分类是计算生物学中的一个前沿研究课题，对于新疫苗和免疫疗法的发展做出了重要贡献。然而，传统的实例空间多实例学习方法直接将包级别标签分配给实例，面临大量噪声标签和极低的证人率的问题。本文提出了一种噪声标签学习方法来解决免疫库分类任务。为了纠正序列级别分类器对免疫库级别标签的不准确监督，我们设计了一种强健的训练策略：初始标签经过平滑处理，逐步使用模型的预测进行修正。此外，我们同时训练了两个具有相同架构但参数初始化不同的模型，以纠正已知的自我训练模式中的“确认偏见”问题。

    Immune repertoire classification, a typical multiple instance learning (MIL) problem, is a frontier research topic in computational biology that makes transformative contributions to new vaccines and immune therapies. However, the traditional instance-space MIL, directly assigning bag-level labels to instances, suffers from the massive amount of noisy labels and extremely low witness rate. In this work, we propose a noisy-label-learning formulation to solve the immune repertoire classification task. To remedy the inaccurate supervision of repertoire-level labels for a sequence-level classifier, we design a robust training strategy: The initial labels are smoothed to be asymmetric and are progressively corrected using the model's predictions throughout the training process. Furthermore, two models with the same architecture but different parameter initialization are co-trained simultaneously to remedy the known "confirmation bias" problem in the self-training-like schema. As a result, w
    
[^55]: 部分可观测马尔可夫决策过程中的动态深度强化学习算法

    Dynamic deep-reinforcement-learning algorithm in Partially Observed Markov Decision Processes. (arXiv:2307.15931v1 [cs.LG])

    [http://arxiv.org/abs/2307.15931](http://arxiv.org/abs/2307.15931)

    本研究研究了在部分可观测马尔可夫决策过程中解决的动作序列的好处，并提出了几种扩展深度强化学习算法的结构和方法。

    

    在最近的研究中，强化学习取得了很大的进步，并且在实际应用中引起了越来越多的兴趣。在许多情况下，由于非静态干扰，使得智能体难以保持性能。这种干扰产生了被称为部分可观测马尔可夫决策过程的环境。在实践中，部分可观测马尔可夫决策过程通过引入额外的估计器或在强化学习的上下文中使用递归神经网络来处理。这两种情况都需要处理轨迹上的序列信息。然而，目前只有很少有研究探讨要考虑的信息的影响以及处理它们的网络结构。本研究展示了在解决部分可观测马尔可夫决策过程时包含动作序列的好处，并提出了几种结构和方法来扩展最新的深度强化学习算法。

    Reinforcement learning has been greatly improved in recent studies and an increased interest in real-world implementation has emerged in recent years. In many cases, due to the non-static disturbances, it becomes challenging for the agent to keep the performance. The disturbance results in the environment called Partially Observable Markov Decision Process. In common practice, Partially Observable Markov Decision Process is handled by introducing an additional estimator, or Recurrent Neural Network is utilized in the context of reinforcement learning. Both of the cases require to process sequential information on the trajectory. However, there are only a few studies investigating the effect of information to consider and the network structure to handle them. This study shows the benefit of action sequence inclusion in order to solve Partially Observable Markov Decision Process. Several structures and approaches are proposed to extend one of the latest deep reinforcement learning algori
    
[^56]: 机会主义环境空气质量监测和预测与可扩展的图神经网络

    Opportunistic Air Quality Monitoring and Forecasting with Expandable Graph Neural Networks. (arXiv:2307.15916v1 [cs.LG])

    [http://arxiv.org/abs/2307.15916](http://arxiv.org/abs/2307.15916)

    提出了一种可扩展的图神经网络模型，能够处理不同空间结构的已有和新增基础设施收集的环境空气质量数据，并嵌入到任何空气质量预测模型中应用于具有演变空间结构的场景。

    

    近年来，环境空气质量监测和预测已成为热门研究课题。由于城市地区已建立了健全的数据收集设施，基于数据驱动的空气质量预测方法引起了广泛关注。通常由国家研究机构或科技巨头部署的固定基础设施往往无法满足不同个性化场景的需求，例如在没有任何现有基础设施的区域进行预测。因此，规模较小的研究机构或有限预算的公司不得不寻求定制解决方案，通过引入更灵活的数据收集基础设施。在本文中，我们提出了一种可扩展的图注意力网络 (EGAT) 模型，它可以处理已有和新增基础设施收集的数据，同时适应不同的空间结构。此外，我们的提议可以嵌入任何空气质量预测模型，适用于具有演变空间结构的场景。

    Air Quality Monitoring and Forecasting has been a popular research topic in recent years. Recently, data-driven approaches for air quality forecasting have garnered significant attention, owing to the availability of well-established data collection facilities in urban areas. Fixed infrastructures, typically deployed by national institutes or tech giants, often fall short in meeting the requirements of diverse personalized scenarios, e.g., forecasting in areas without any existing infrastructure. Consequently, smaller institutes or companies with limited budgets are compelled to seek tailored solutions by introducing more flexible infrastructures for data collection. In this paper, we propose an expandable graph attention network (EGAT) model, which digests data collected from existing and newly-added infrastructures, with different spatial structures. Additionally, our proposal can be embedded into any air quality forecasting models, to apply to the scenarios with evolving spatial str
    
[^57]: 用自动机理论方法合成二进制神经网络

    An Automata-Theoretic Approach to Synthesizing Binarized Neural Networks. (arXiv:2307.15907v1 [cs.LG])

    [http://arxiv.org/abs/2307.15907](http://arxiv.org/abs/2307.15907)

    这篇论文提出了一种用自动机理论方法合成满足指定属性的二进制神经网络的方法，并定义了一种时态逻辑作为规范语言。

    

    深度神经网络(DNNs，也称为NNs)在各种任务中被广泛使用，并且已被证明是成功的。然而，伴随而来的昂贵的计算和存储成本使得在资源受限设备上部署成为一个重要问题。为了解决这个问题，量化已经成为一种有效的方法，通过将浮点数量化为低宽度的定点表示来减少DNNs的成本，同时几乎不会降低准确性。发展了量化神经网络(QNNs)，而将二进制神经网络(BNNs)限制为二进制值作为一个特殊情况。神经网络的另一个关注点是它们的脆弱性和缺乏可解释性。尽管在DNN的可信性方面有许多活跃的研究，但对于QNNs的方法却很少提出。为此，本文提出了一个用自动机理论方法合成满足指定属性的BNNs的方法。更具体地说，我们定义了一种称为BLTL的时态逻辑作为规范语言。

    Deep neural networks, (DNNs, a.k.a. NNs), have been widely used in various tasks and have been proven to be successful. However, the accompanied expensive computing and storage costs make the deployments in resource-constrained devices a significant concern. To solve this issue, quantization has emerged as an effective way to reduce the costs of DNNs with little accuracy degradation by quantizing floating-point numbers to low-width fixed-point representations. Quantized neural networks (QNNs) have been developed, with binarized neural networks (BNNs) restricted to binary values as a special case. Another concern about neural networks is their vulnerability and lack of interpretability. Despite the active research on trustworthy of DNNs, few approaches have been proposed to QNNs. To this end, this paper presents an automata-theoretic approach to synthesizing BNNs that meet designated properties. More specifically, we define a temporal logic, called BLTL, as the specification language. W
    
[^58]: 多视角稀疏拉普拉斯特征图用于非线性谱特征选择

    Multi-view Sparse Laplacian Eigenmaps for nonlinear Spectral Feature Selection. (arXiv:2307.15905v1 [cs.LG])

    [http://arxiv.org/abs/2307.15905](http://arxiv.org/abs/2307.15905)

    本研究提出了一种多视角稀疏拉普拉斯特征图方法，用于高维数据的特征选择，能够从多个数据视图中有效地捕捉数据的基本结构。使用稀疏约束和可扩展的优化算法，通过降维和迭代算法解决优化问题，得到了一个能够捕捉基本数据结构的特征子集。

    

    高维数据集的复杂性对机器学习模型提出了重大挑战，包括过拟合、计算复杂度和结果解释的困难。为了解决这些问题，必须确定一个能够捕捉数据基本结构的信息子集。本研究提出了用于特征选择的多视角稀疏拉普拉斯特征图（MSLE），它有效地结合了多个数据视图，强制施加了稀疏约束，并采用可扩展的优化算法来确定捕捉基本数据结构的特征子集。MSLE是一种基于图的方法，利用多个数据视角构建高维数据的更健壮和信息丰富的表示。该方法应用了稀疏特征分解来降低数据的维度，得到了一个降维后的特征集。通过迭代算法来解决优化问题，实现了子集的特征选择。

    The complexity of high-dimensional datasets presents significant challenges for machine learning models, including overfitting, computational complexity, and difficulties in interpreting results. To address these challenges, it is essential to identify an informative subset of features that captures the essential structure of the data. In this study, the authors propose Multi-view Sparse Laplacian Eigenmaps (MSLE) for feature selection, which effectively combines multiple views of the data, enforces sparsity constraints, and employs a scalable optimization algorithm to identify a subset of features that capture the fundamental data structure. MSLE is a graph-based approach that leverages multiple views of the data to construct a more robust and informative representation of high-dimensional data. The method applies sparse eigendecomposition to reduce the dimensionality of the data, yielding a reduced feature set. The optimization problem is solved using an iterative algorithm alternati
    
[^59]: 仅使用一个步长的新型梯度时序差分算法：通过$L$-$\lambda$平滑性进行收敛速率分析

    A new Gradient TD Algorithm with only One Step-size: Convergence Rate Analysis using $L$-$\lambda$ Smoothness. (arXiv:2307.15892v1 [cs.LG])

    [http://arxiv.org/abs/2307.15892](http://arxiv.org/abs/2307.15892)

    本论文提出了一种新的梯度时序差分算法，只使用一个步长参数，并证明收敛速度至少为$O(1/t)$。

    

    梯度时序差分（GTD）算法是第一个具有收敛保证的离策略学习线性函数逼近算法，其复杂度为$O(d)$（$d$是特征数量）。本文提出了一种名为Impression GTD的全新单时间尺度GTD算法，用于最小化期望td更新（NEU）目标，并只有一个步长参数。我们证明这种新算法的收敛速度至少与$O(1/t)$一样快。

    Gradient Temporal Difference (GTD) algorithms (Sutton et al., 2008, 2009) are the first $O(d)$ ($d$ is the number features) algorithms that have convergence guarantees for off-policy learning with linear function approximation. Liu et al. (2015) and Dalal et. al. (2018) proved the convergence rates of GTD, GTD2 and TDC are $O(t^{-\alpha/2})$ for some $\alpha \in (0,1)$. This bound is tight (Dalal et al., 2020), and slower than $O(1/\sqrt{t})$. GTD algorithms also have two step-size parameters, which are difficult to tune. In literature, there is a "single-time-scale" formulation of GTD. However, this formulation still has two step-size parameters.  This paper presents a truly single-time-scale GTD algorithm for minimizing the Norm of Expected td Update (NEU) objective, and it has only one step-size parameter. We prove that the new algorithm, called Impression GTD, converges at least as fast as $O(1/t)$. Furthermore, based on a generalization of the expected smoothness (Gower et al. 201
    
[^60]: 鲁棒策略评估的一阶策略优化

    First-order Policy Optimization for Robust Policy Evaluation. (arXiv:2307.15890v1 [math.OC])

    [http://arxiv.org/abs/2307.15890](http://arxiv.org/abs/2307.15890)

    该论文提出了一种名为FRPE的一阶策略评估方法，用于鲁棒马尔可夫决策过程的策略评估。该方法在确定性和随机设置下都能提供统一的框架，并具有较小的样本复杂度。

    

    我们采用一种策略优化的视角来进行具有s-rectangular模糊集的鲁棒马尔可夫决策过程的策略评估。所开发的方法，名为一阶策略评估（FRPE），提供了对确定性（离线）和随机（在线）设置下的鲁棒策略评估的第一个统一框架，可以使用表格表示或通用函数逼近。特别地，在确定性设置中建立了线性收敛，而在随机设置中具有近似O(1/ε^2)的样本复杂度。FRPE还可以自然地扩展到评估带有(s, a)-rectangular模糊集的鲁棒状态-动作值函数。我们讨论了将开发的结果应用于大规模鲁棒MDP的随机策略优化。

    We adopt a policy optimization viewpoint towards policy evaluation for robust Markov decision process with $\mathrm{s}$-rectangular ambiguity sets. The developed method, named first-order policy evaluation (FRPE), provides the first unified framework for robust policy evaluation in both deterministic (offline) and stochastic (online) settings, with either tabular representation or generic function approximation. In particular, we establish linear convergence in the deterministic setting, and $\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexity in the stochastic setting. FRPE also extends naturally to evaluating the robust state-action value function with $(\mathrm{s}, \mathrm{a})$-rectangular ambiguity sets. We discuss the application of the developed results for stochastic policy optimization of large-scale robust MDPs.
    
[^61]: 使用归因方法解释全盘深度学习模型在太阳耀斑预测中的应用

    Explaining Full-disk Deep Learning Model for Solar Flare Prediction using Attribution Methods. (arXiv:2307.15878v1 [cs.LG])

    [http://arxiv.org/abs/2307.15878](http://arxiv.org/abs/2307.15878)

    本文研究了太阳耀斑预测的深度学习方法，并使用归因方法解释模型的预测结果。通过使用全盘线磁图像和类别加权技术，我们提出了一个预测模型，并评估了其性能。我们还应用了三种归因方法对模型的预测结果进行解释和验证。

    

    本文对于太阳耀斑预测的深度学习方法进行了研究，主要关注被忽视的近边耀斑，并利用归因方法对模型预测结果进行了后期定性解释。我们提出了一个太阳耀斑预测模型，使用每小时的全盘线磁图像进行训练，并采用二进制预测模式来预测在接下来的24小时内可能发生的大于等于M级耀斑。为了解决类别不平衡问题，我们采用了数据增强和类别加权技术的融合，并使用真实技能统计量（TSS）和海德克技能得分（HSS）来评估模型的整体性能。此外，我们还应用了三种归因方法，分别是引导梯度加权类激活映射、综合梯度和深层Shapley可加解释，对模型的预测结果进行了解释和交叉验证。我们的分析揭示了...

    This paper contributes to the growing body of research on deep learning methods for solar flare prediction, primarily focusing on highly overlooked near-limb flares and utilizing the attribution methods to provide a post hoc qualitative explanation of the model's predictions. We present a solar flare prediction model, which is trained using hourly full-disk line-of-sight magnetogram images and employs a binary prediction mode to forecast $\geq$M-class flares that may occur within the following 24-hour period. To address the class imbalance, we employ a fusion of data augmentation and class weighting techniques; and evaluate the overall performance of our model using the true skill statistic (TSS) and Heidke skill score (HSS). Moreover, we applied three attribution methods, namely Guided Gradient-weighted Class Activation Mapping, Integrated Gradients, and Deep Shapley Additive Explanations, to interpret and cross-validate our model's predictions with the explanations. Our analysis reve
    
[^62]: GraphDAC：一种图分析方法用于动态空域配置

    GraphDAC: A Graph-Analytic Approach to Dynamic Airspace Configuration. (arXiv:2307.15876v1 [math.OC])

    [http://arxiv.org/abs/2307.15876](http://arxiv.org/abs/2307.15876)

    本研究提出了一种图分析方法用于动态空域配置，通过构建约束嵌入的图形，应用自适应算法来生成合作机场群体并均匀分配工作负载，在各种交通条件下实现了50％的工作负载减少，并为优化空域配置的推荐系统奠定了基础。

    

    当前的国家空中交通系统（NAS）由于空中交通量的增加达到了容量限制，并且基于过时的预战术计划。本研究提出了一种更动态的空域配置（DAC）方法，可以增加吞吐量并适应波动的空中交通，特别适用于紧急情况。所提出的方法将空域构建为一个带约束的图形，压缩其维度，并应用谱聚类启用的自适应算法生成合作机场群体并均匀分配工作负载。在各种交通条件下，我们的实验表明工作负载不平衡减少了50％。这项研究最终可以成为优化空域配置的推荐系统的基础。代码可在https://github.com/KeFenge2022/GraphDAC.git中找到。

    The current National Airspace System (NAS) is reaching capacity due to increased air traffic, and is based on outdated pre-tactical planning. This study proposes a more dynamic airspace configuration (DAC) approach that could increase throughput and accommodate fluctuating traffic, ideal for emergencies. The proposed approach constructs the airspace as a constraints-embedded graph, compresses its dimensions, and applies a spectral clustering-enabled adaptive algorithm to generate collaborative airport groups and evenly distribute workloads among them. Under various traffic conditions, our experiments demonstrate a 50\% reduction in workload imbalances. This research could ultimately form the basis for a recommendation system for optimized airspace configuration. Code available at https://github.com/KeFenge2022/GraphDAC.git
    
[^63]: 医学图像分割中的跨维度迁移学习与深度学习

    Cross-dimensional transfer learning in medical image segmentation with deep learning. (arXiv:2307.15872v1 [eess.IV])

    [http://arxiv.org/abs/2307.15872](http://arxiv.org/abs/2307.15872)

    这篇论文介绍了一种有效的跨维度迁移学习方法，将在自然图像上训练的2D分类网络的效能迁移到医学图像分割中，克服了限量标注数据和获取约束的问题。

    

    在过去的十年中，卷积神经网络已经在各种图像分析和计算机视觉应用中取得了突破性进展，2D图像分类网络的性能不断提升，并且在由数百万自然图片组成的数据库上进行训练。然而，由于医学图像分析中存在有限的标注数据和获取约束，进展受到了限制。考虑到医学成像数据的容积性，这些限制尤为明显。在本文中，我们引入了一种有效的方法，将在自然图像上训练的2D分类网络的效能迁移到2D、3D单模态和多模态的医学图像分割应用中。在这个方向上，我们设计了基于两个关键原理的新型架构：通过将2D预训练编码器嵌入到更高维度的U-Net中进行权重转移，以及通过将2D分割网络扩展到更高维度的网络进行维度转移。所提出的网络经过了测试...

    Over the last decade, convolutional neural networks have emerged and advanced the state-of-the-art in various image analysis and computer vision applications. The performance of 2D image classification networks is constantly improving and being trained on databases made of millions of natural images. However, progress in medical image analysis has been hindered by limited annotated data and acquisition constraints. These limitations are even more pronounced given the volumetry of medical imaging data. In this paper, we introduce an efficient way to transfer the efficiency of a 2D classification network trained on natural images to 2D, 3D uni- and multi-modal medical image segmentation applications. In this direction, we designed novel architectures based on two key principles: weight transfer by embedding a 2D pre-trained encoder into a higher dimensional U-Net, and dimensional transfer by expanding a 2D segmentation network into a higher dimension one. The proposed networks were teste
    
[^64]: 高效的半监督异构参与者联邦学习

    Efficient Semi-Supervised Federated Learning for Heterogeneous Participants. (arXiv:2307.15870v1 [cs.LG])

    [http://arxiv.org/abs/2307.15870](http://arxiv.org/abs/2307.15870)

    本论文提出了一种高效的半监督异构参与者联邦学习系统，通过引入聚类正则化来改进模型在数据非独立同分布情况下的性能，并对模型收敛性进行了理论和实验研究。

    

    联邦学习（FL）允许多个客户端在私有数据上协同训练机器学习模型，但在资源有限的环境中训练和部署大型模型用于广泛应用是具有挑战性的。幸运的是，分离式联邦学习（SFL）通过减轻客户端的计算和通信负担提供了优秀的解决方案。SFL通常假设客户端具有标记的数据进行本地训练，然而在实践中并非总是如此。以前的研究采用半监督技术来利用FL中的无标记数据，但数据的非独立同分布性提出了确保训练效率的另一个挑战。在这里，我们提出了一种新颖的系统Pseudo-Clustering Semi-SFL，用于在标记数据位于服务器上的情境下训练模型。通过引入聚类正则化，可以提高数据非独立同分布情况下的模型性能。此外，我们对模型收敛性进行了理论和实验研究，发现了...

    Federated Learning (FL) has emerged to allow multiple clients to collaboratively train machine learning models on their private data. However, training and deploying large models for broader applications is challenging in resource-constrained environments. Fortunately, Split Federated Learning (SFL) offers an excellent solution by alleviating the computation and communication burden on the clients SFL often assumes labeled data for local training on clients, however, it is not the case in practice.Prior works have adopted semi-supervised techniques for leveraging unlabeled data in FL, but data non-IIDness poses another challenge to ensure training efficiency. Herein, we propose Pseudo-Clustering Semi-SFL, a novel system for training models in scenarios where labeled data reside on the server. By introducing Clustering Regularization, model performance under data non-IIDness can be improved. Besides, our theoretical and experimental investigations into model convergence reveal that the 
    
[^65]: 基于Polyak-Lojasiewicz条件的极小极大优化的快速随机算法

    Faster Stochastic Algorithms for Minimax Optimization under Polyak--{\L}ojasiewicz Conditions. (arXiv:2307.15868v1 [math.OC])

    [http://arxiv.org/abs/2307.15868](http://arxiv.org/abs/2307.15868)

    该论文提出了基于Polyak-Lojasiewicz条件的极小极大优化的快速随机算法SPIDER-GDA，并证明了该算法能够在较低的复杂度下找到接近最优解的结果。

    

    本文考虑Polyak-Lojasiewicz (PL)条件下的随机一阶优化算法。我们提出了SPIDER-GDA算法来解决形式为$\min_x \max_y f(x,y)\triangleq \frac{1}{n} \sum_{i=1}^n f_i(x,y)$的有限和问题，其中目标函数$f(x,y)$在$x$方向上是$\mu_x$-PL的，在$y$方向上是$\mu_y$-PL的；每个$f_i(x,y)$都是$L$-平滑的。我们证明了SPIDER-GDA算法可以在${\mathcal O}\left((n + \sqrt{n}\,\kappa_x\kappa_y^2)\log (1/\epsilon)\right)$的随机一阶oracle (SFO)复杂度内找到一个$\epsilon$-最优解，这比现有方法的SFO上界${\mathcal O}\big((n + n^{2/3}\kappa_x\kappa_y^2)\log (1/\epsilon)\big)$更好，其中$\kappa_x\triangleq L/\mu_x$，$\kappa_y\triangleq L/\mu_y$。对于病态情况，我们提供了一种加速算法来进一步降低计算成本。它可以实现$\tilde{{\mathcal O}}\big((n+\sqrt{n}\,\kappa_x\kappa_y)\log^2 (1/\epsilon)\big)$的SFO上界。

    This paper considers stochastic first-order algorithms for minimax optimization under Polyak--{\L}ojasiewicz (PL) conditions. We propose SPIDER-GDA for solving the finite-sum problem of the form $\min_x \max_y f(x,y)\triangleq \frac{1}{n} \sum_{i=1}^n f_i(x,y)$, where the objective function $f(x,y)$ is $\mu_x$-PL in $x$ and $\mu_y$-PL in $y$; and each $f_i(x,y)$ is $L$-smooth. We prove SPIDER-GDA could find an $\epsilon$-optimal solution within ${\mathcal O}\left((n + \sqrt{n}\,\kappa_x\kappa_y^2)\log (1/\epsilon)\right)$ stochastic first-order oracle (SFO) complexity, which is better than the state-of-the-art method whose SFO upper bound is ${\mathcal O}\big((n + n^{2/3}\kappa_x\kappa_y^2)\log (1/\epsilon)\big)$, where $\kappa_x\triangleq L/\mu_x$ and $\kappa_y\triangleq L/\mu_y$. For the ill-conditioned case, we provide an accelerated algorithm to reduce the computational cost further. It achieves $\tilde{{\mathcal O}}\big((n+\sqrt{n}\,\kappa_x\kappa_y)\log^2 (1/\epsilon)\big)$ SFO u
    
[^66]: 通过面部微表情识别捕捉难以捉摸的抑郁症

    Catching Elusive Depression via Facial Micro-Expression Recognition. (arXiv:2307.15862v1 [cs.CV])

    [http://arxiv.org/abs/2307.15862](http://arxiv.org/abs/2307.15862)

    本文通过使用面部微表情（FMEs）识别来诊断隐性抑郁症，从而捕捉难以捉摸的抑郁症。我们提出了一种基于面部标志点的关注区域（ROI）方法来解决FMEs极低强度和微妙的识别问题，并提供了一种低成本和保护隐私的自我诊断解决方案。

    

    抑郁症是一种常见的精神健康障碍，会导致持续抑郁的情绪和情感困扰等后果。其中一类抑郁症是隐性抑郁症，患者故意或无意地通过外在乐观主义隐藏真实情感，从而使诊断和治疗变得复杂和延迟，并导致意外自杀。在本文中，我们提出使用面部微表情（FMEs）来检测和识别潜在的真实情感来诊断隐性抑郁症。然而，FMEs的极低强度和微妙的特性使它们的识别成为一项艰巨的任务。我们提出了一种基于面部标志点的关注区域（ROI）方法来解决这一挑战，并描述了一种低成本和保护隐私的解决方案，可以在个人环境（例如在家中）使用便携式移动设备进行自我诊断。我们呈现了验证我们方法的结果和发现，并讨论其他技术挑战和解决方案。

    Depression is a common mental health disorder that can cause consequential symptoms with continuously depressed mood that leads to emotional distress. One category of depression is Concealed Depression, where patients intentionally or unintentionally hide their genuine emotions through exterior optimism, thereby complicating and delaying diagnosis and treatment and leading to unexpected suicides. In this paper, we propose to diagnose concealed depression by using facial micro-expressions (FMEs) to detect and recognize underlying true emotions. However, the extremely low intensity and subtle nature of FMEs make their recognition a tough task. We propose a facial landmark-based Region-of-Interest (ROI) approach to address the challenge, and describe a low-cost and privacy-preserving solution that enables self-diagnosis using portable mobile devices in a personal setting (e.g., at home). We present results and findings that validate our method, and discuss other technical challenges and f
    
[^67]: 多输出头信息的产品项目分类集成

    Multi-output Headed Ensembles for Product Item Classification. (arXiv:2307.15858v1 [cs.LG])

    [http://arxiv.org/abs/2307.15858](http://arxiv.org/abs/2307.15858)

    本论文提出了一个基于深度学习的产品项目分类模型框架，利用平均集成和融合分类器的简单性和稳健性，并通过元数据特征和低层特征工程提升分类质量。

    

    本论文研究了大规模电子商务目录中产品项目分类的问题。电子商务目录的分类包含了数千种流派，商家可以随时上传商品进行分类。然而，商家的分类结果往往是错误的，但却被当作自动生成训练集时的标签，从而形成了一个反馈循环，导致模型质量越来越差。由于缺乏规模可观的筛选训练集，分类问题变得尤为显著。在这种情况下，通常使用多个分类器的组合来对抗单一分类器的泛化性能较差的问题。我们提出了一个基于深度学习的可扩展分类模型框架，可以受益于平均集成和融合分类器的简单性和稳健性。我们还能够利用元数据特征和低层特征工程来提升分类质量。

    In this paper, we revisit the problem of product item classification for large-scale e-commerce catalogs. The taxonomy of e-commerce catalogs consists of thousands of genres to which are assigned items that are uploaded by merchants on a continuous basis. The genre assignments by merchants are often wrong but treated as ground truth labels in automatically generated training sets, thus creating a feedback loop that leads to poorer model quality over time. This problem of taxonomy classification becomes highly pronounced due to the unavailability of sizable curated training sets.  Under such a scenario it is common to combine multiple classifiers to combat poor generalization performance from a single classifier. We propose an extensible deep learning based classification model framework that benefits from the simplicity and robustness of averaging ensembles and fusion based classifiers. We are also able to use metadata features and low-level feature engineering to boost classification 
    
[^68]: 通过使用正确截尾高斯噪声训练，改进NVCiM DNN加速器的真实最坏情况性能

    Improving Realistic Worst-Case Performance of NVCiM DNN Accelerators through Training with Right-Censored Gaussian Noise. (arXiv:2307.15853v1 [cs.LG])

    [http://arxiv.org/abs/2307.15853](http://arxiv.org/abs/2307.15853)

    通过使用k-th百分位性能（KPP）来捕捉在CiM加速器上执行的DNN模型的真实最坏情况性能，从而改善NVCiM DNN加速器的性能。

    

    基于非易失性存储器（NVM）设备构建的计算存储一体（CiM）技术，由于其原地数据处理能力和卓越的能源效率，对于加速深度神经网络（DNN）具有很大潜力。然而，将经过良好训练的模型参数映射到NVM设备后，往往会出现与预期值相差较大的偏差，导致CiM DNN加速器性能显著降低。目前已经提出了很多解决方案来解决此问题，但主要集中于改进CiM DNN加速器的平均性能。如何在设备变化的影响下保证最坏情况性能，对于许多安全关键的应用，如自动驾驶汽车，至关重要，但远未得到充分探索。

    Compute-in-Memory (CiM), built upon non-volatile memory (NVM) devices, is promising for accelerating deep neural networks (DNNs) owing to its in-situ data processing capability and superior energy efficiency. Unfortunately, the well-trained model parameters, after being mapped to NVM devices, can often exhibit large deviations from their intended values due to device variations, resulting in notable performance degradation in these CiM-based DNN accelerators. There exists a long list of solutions to address this issue. However, they mainly focus on improving the mean performance of CiM DNN accelerators. How to guarantee the worst-case performance under the impact of device variations, which is crucial for many safety-critical applications such as self-driving cars, has been far less explored. In this work, we propose to use the k-th percentile performance (KPP) to capture the realistic worst-case performance of DNN models executing on CiM accelerators. Through a formal analysis of the 
    
[^69]: 使用项目反应理论对综合算法组合进行评估

    Comprehensive Algorithm Portfolio Evaluation using Item Response Theory. (arXiv:2307.15850v1 [stat.ML])

    [http://arxiv.org/abs/2307.15850](http://arxiv.org/abs/2307.15850)

    本文提出了一种改进的基于IRT的框架，用于评估算法组合在数据集仓库中的性能，同时获取算法一致性和异常性等特征。该框架通过对传统IRT模型进行倒转和重新解释来实现，不需要额外的数据集特征计算。

    

    项目反应理论（IRT）被提出用于教育心理测量学领域，用于评估学生能力、测试题难度和区分度。最近，IRT已被应用于评估单个分类数据集上的机器学习算法性能，其中学生现在是一个算法，而测试题是算法要对观察结果进行分类。本文提出了一种改进的基于IRT的框架，用于评估一个算法组合在一个数据集仓库中的表现，同时获取算法一致性和异常性等更丰富的特征，这些特征描述了算法性能的重要方面。这些特征是通过对传统IRT模型进行新颖的倒转和重新解释而得到的，而不需要额外的数据集特征计算。我们对不同应用的算法组合在该框架上进行了测试，证明了其广泛适用性。

    Item Response Theory (IRT) has been proposed within the field of Educational Psychometrics to assess student ability as well as test question difficulty and discrimination power. More recently, IRT has been applied to evaluate machine learning algorithm performance on a single classification dataset, where the student is now an algorithm, and the test question is an observation to be classified by the algorithm. In this paper we present a modified IRT-based framework for evaluating a portfolio of algorithms across a repository of datasets, while simultaneously eliciting a richer suite of characteristics such as algorithm consistency and anomalousness - that describe important aspects of algorithm performance. These characteristics arise from a novel inversion and reinterpretation of the traditional IRT model without requiring additional dataset feature computations. We test this framework on algorithm portfolios for a wide range of applications, demonstrating the broad applicability 
    
[^70]: 用中性原子进行量子核估计的监督分类：一种基于门的方法

    Quantum Kernel Estimation With Neutral Atoms For Supervised Classification: A Gate-Based Approach. (arXiv:2307.15840v1 [quant-ph])

    [http://arxiv.org/abs/2307.15840](http://arxiv.org/abs/2307.15840)

    本文提出了一种用中性原子进行量子核估计的监督分类的基于门的方法，通过实现一种参数化序列来进行特征映射和核矩阵的计算。

    

    量子核估计（QKE）是一种基于利用量子计算机估计经典难以计算的核函数的技术，然后由经典计算机用于训练支持向量机（SVM）。由于实现难以经典模拟的特征映射所需的2-局部算子数量较高，需要较高的量子比特连接性，而目前超导设备上无法实现这一点。因此，可以利用中性原子量子计算机，因为它们允许更自由地安排原子。文献中可以找到基于中性原子的QKE示例，但它们侧重于图学习并使用类比方法。本文介绍了一种基于门模型的通用方法。在从激光脉冲开始推导1比特和2比特门之后，实现了一个用于3比特特征映射的参数化序列。然后，利用该序列从经验上计算核矩阵。

    Quantum Kernel Estimation (QKE) is a technique based on leveraging a quantum computer to estimate a kernel function that is classically difficult to calculate, which is then used by a classical computer for training a Support Vector Machine (SVM). Given the high number of 2-local operators necessary for realizing a feature mapping hard to simulate classically, a high qubit connectivity is needed, which is not currently possible on superconducting devices. For this reason, neutral atom quantum computers can be used, since they allow to arrange the atoms with more freedom. Examples of neutral-atom-based QKE can be found in the literature, but they are focused on graph learning and use the analogue approach. In this paper, a general method based on the gate model is presented. After deriving 1-qubit and 2-qubit gates starting from laser pulses, a parameterized sequence for feature mapping on 3 qubits is realized. This sequence is then used to empirically compute the kernel matrix starting
    
[^71]: 机器学习中隐私与公平性的整体调查

    Holistic Survey of Privacy and Fairness in Machine Learning. (arXiv:2307.15838v1 [cs.LG])

    [http://arxiv.org/abs/2307.15838](http://arxiv.org/abs/2307.15838)

    这项综合调查研究了机器学习中隐私和公平性的重要性，并强调了将这两个目标同时集成到模型中的挑战和可能的方法。

    

    隐私和公平性是负责任的人工智能(AI)和值得信赖的机器学习(ML)的两个重要支柱。每个目标在文献中都得到了独立研究，旨在减少实现它们所带来的效用损失。尽管学术界和工业界对此表现出了巨大兴趣，但仍迫切需要更深入的研究来揭示如何将这两个目标同时集成到ML模型中。与被广泛接受的权衡不同，即隐私-效用和公平-效用，隐私和公平性之间的相互关系尚未被很好地理解。虽然一些研究提出了两个目标函数之间的权衡，但也有其他研究表明在某些场景中这些函数是相互一致的。为了填补这一研究空白，我们对ML中的隐私和公平性进行了彻底的回顾，包括监督学习、无监督学习、半监督学习和强化学习。

    Privacy and fairness are two crucial pillars of responsible Artificial Intelligence (AI) and trustworthy Machine Learning (ML). Each objective has been independently studied in the literature with the aim of reducing utility loss in achieving them. Despite the significant interest attracted from both academia and industry, there remains an immediate demand for more in-depth research to unravel how these two objectives can be simultaneously integrated into ML models. As opposed to well-accepted trade-offs, i.e., privacy-utility and fairness-utility, the interrelation between privacy and fairness is not well-understood. While some works suggest a trade-off between the two objective functions, there are others that demonstrate the alignment of these functions in certain scenarios. To fill this research gap, we provide a thorough review of privacy and fairness in ML, including supervised, unsupervised, semi-supervised, and reinforcement learning. After examining and consolidating the liter
    
[^72]: 在数据异质性中保持用户级隐私的均值估计

    Mean Estimation with User-level Privacy under Data Heterogeneity. (arXiv:2307.15835v1 [cs.CR])

    [http://arxiv.org/abs/2307.15835](http://arxiv.org/abs/2307.15835)

    本文提出了一种在数据异质性中保持用户级隐私的均值估计方法，允许用户数据在分布和数量上的差异，并证明了估计器的渐近最优性和可达到的误差下界。

    

    当前许多现代数据分析任务面临的一个关键挑战是用户数据的异质性。不同的用户可能拥有截然不同数量的数据点。更重要的是，不能假设所有用户从相同的底层分布中进行采样。例如，在语言数据中，不同的语音风格导致了数据的异质性。在这项工作中，我们提出了一个简单的异质用户数据模型，允许用户数据在分布和数量上的差异，并提供了一种在保持用户级差分隐私的同时估计人口均值的方法。我们证明了我们的估计器的渐近最优性，并证明了在我们引入的设置中可以达到的误差的一般下界。

    A key challenge in many modern data analysis tasks is that user data are heterogeneous. Different users may possess vastly different numbers of data points. More importantly, it cannot be assumed that all users sample from the same underlying distribution. This is true, for example in language data, where different speech styles result in data heterogeneity. In this work we propose a simple model of heterogeneous user data that allows user data to differ in both distribution and quantity of data, and provide a method for estimating the population-level mean while preserving user-level differential privacy. We demonstrate asymptotic optimality of our estimator and also prove general lower bounds on the error achievable in the setting we introduce.
    
[^73]: 基于距离相关性的方法来刻画循环神经网络在时间序列预测中的有效性

    A Distance Correlation-Based Approach to Characterize the Effectiveness of Recurrent Neural Networks for Time Series Forecasting. (arXiv:2307.15830v1 [cs.LG])

    [http://arxiv.org/abs/2307.15830](http://arxiv.org/abs/2307.15830)

    本文通过距离相关性的方法来研究循环神经网络对于时间序列预测的有效性，发现激活层能够学习时间序列的滞后结构，但是在连续的几层中逐渐丧失这些信息，导致预测质量变差，同时激活层也不能很好地建模移动平均和异方差时间。

    

    时间序列预测受到了广泛关注，循环神经网络(RNNs)作为处理序列数据的一种常用模型之一，具有很强的能力。然而，之前关于RNNs在时间序列预测中的研究结果不一致，并且对于不同数据集的性能差异缺乏深入洞察。本文提出了一种通过距离相关性这一多功能指标来将时间序列的特征与RNNs的组成部分联系起来的方法。该指标允许我们通过RNN激活层的信息流来解释和说明其性能。我们实证表明，RNN的激活层能够很好地学习时间序列的滞后结构。然而，在连续的几层中，它们逐渐丧失了这些信息，从而使具有大滞后结构的序列的预测质量变差。我们还显示，激活层不能充分建模移动平均和异方差时间。

    Time series forecasting has received a lot of attention with recurrent neural networks (RNNs) being one of the widely used models due to their ability to handle sequential data. Prior studies of RNNs for time series forecasting yield inconsistent results with limited insights as to why the performance varies for different datasets. In this paper, we provide an approach to link the characteristics of time series with the components of RNNs via the versatile metric of distance correlation. This metric allows us to examine the information flow through the RNN activation layers to be able to interpret and explain their performance. We empirically show that the RNN activation layers learn the lag structures of time series well. However, they gradually lose this information over a span of a few consecutive layers, thereby worsening the forecast quality for series with large lag structures. We also show that the activation layers cannot adequately model moving average and heteroskedastic time
    
[^74]: RT-2：视觉-语言-行动模型将网络知识转化为机器人控制

    RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. (arXiv:2307.15818v1 [cs.RO])

    [http://arxiv.org/abs/2307.15818](http://arxiv.org/abs/2307.15818)

    本文研究了将互联网规模数据上训练的视觉-语言模型直接应用于机器人控制的方法，实现了泛化能力的提升和新兴的语义推理。通过在机器人轨迹数据和互联网规模的视觉-语言任务上共同微调最先进的视觉-语言模型，为单一的端到端训练模型提供了同时学习机器人观测到行为映射和利用语言和视觉-语言数据的益处的能力。

    

    本文研究了如何将在互联网规模数据上训练的视觉-语言模型直接应用于端到端的机器人控制，以提升泛化能力并实现新兴的语义推理。我们的目标是让单一的端到端训练模型既能学会将机器人观测映射到行为，又能享受来自网络的语言和视觉-语言数据的益处。为此，我们提出在机器人轨迹数据和互联网规模的视觉-语言任务（如视觉问答）上共同微调最先进的视觉-语言模型。与其他方法不同，我们提出了一个简单的通用方法来实现这个目标：为了使自然语言回答和机器人行为都能以相同的格式进行处理，我们将行为表示为文本标记，并将它们直接纳入模型的训练集中，与自然语言标记相同。我们将这类模型称为视觉-语言-行动模型（VLA）。

    We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and inst
    
[^75]: 多生长阶段植物识别：以棉花（Gossypium hirsutum）中的Palmer amaranth（Amaranthus palmeri）为例的案例研究

    Multi-growth stage plant recognition: a case study of Palmer amaranth (Amaranthus palmeri) in cotton (Gossypium hirsutum). (arXiv:2307.15816v1 [cs.CV])

    [http://arxiv.org/abs/2307.15816](http://arxiv.org/abs/2307.15816)

    本文研究了在棉花中以Amaranthus palmeri为例的Palmer amaranth杂草的八个生长阶段识别，对You Only Look Once (YOLO)架构进行了比较分析。

    

    许多先进的基于图像的精准农业技术，用于植物育种、田间作物研究和特定场地作物管理，都依赖于对具有高度可变形态生长阶段的植物的可靠检测和表型鉴定。卷积神经网络（CNNs）已经显示出在基于图像的植物表型鉴定和杂草识别方面具有潜力，但它们在识别生长阶段的能力，通常在外观上有明显差异的情况下，还不确定。Palmer amaranth（Amaranthus palmeri）是棉花（Gossypium hirsutum）生产中一种特别具有挑战性的杂草植物，它在整个生长季节的不同生长阶段和在同一生长阶段的不同植株中都表现出高度可变的植物形态，这是由于高遗传多样性引起的。本文以A. palmeri在棉花中的八个生长阶段识别作为一个具有挑战性的You Only Look Once (YOLO)架构模型进行研究。我们比较了来自YOLO v3、v5、v6、v6 3.0、v7和v8的26种不同架构变体。

    Many advanced, image-based precision agricultural technologies for plant breeding, field crop research, and site-specific crop management hinge on the reliable detection and phenotyping of plants across highly variable morphological growth stages. Convolutional neural networks (CNNs) have shown promise for image-based plant phenotyping and weed recognition, but their ability to recognize growth stages, often with stark differences in appearance, is uncertain. Amaranthus palmeri (Palmer amaranth) is a particularly challenging weed plant in cotton (Gossypium hirsutum) production, exhibiting highly variable plant morphology both across growth stages over a growing season, as well as between plants at a given growth stage due to high genetic diversity. In this paper, we investigate eight-class growth stage recognition of A. palmeri in cotton as a challenging model for You Only Look Once (YOLO) architectures. We compare 26 different architecture variants from YOLO v3, v5, v6, v6 3.0, v7, an
    
[^76]: 使用物联网设备和机器学习在工业机械中进行异常检测：一种系统映射

    Anomaly Detection in Industrial Machinery using IoT Devices and Machine Learning: a Systematic Mapping. (arXiv:2307.15807v1 [cs.LG])

    [http://arxiv.org/abs/2307.15807](http://arxiv.org/abs/2307.15807)

    这项研究通过使用物联网设备和机器学习方法，系统地探索了在工业机械中进行异常检测的挑战和机遇。该研究强调了机器学习算法在自动化检测工业机械异常中的重要性。

    

    在智能工业中，异常检测对于预防设备故障、减少停机时间和提高安全性至关重要。物联网（IoT）使得从工业机械中收集大量数据成为可能，为异常检测提供了丰富的信息来源。然而，物联网生态系统生成的数据量和复杂性使得人工手动检测异常变得困难。机器学习算法可以通过分析生成的数据来自动化工业机械中的异常检测。此外，每种技术基于数据的性质和相应的系统都有特定的优势和弱点。然而，目前关于异常检测的研究主要集中在解决网络和网络安全相关问题，对于工业部门的关注有限。此外，这些研究还未涵盖使用机器学习在工业机械中进行异常检测所涉及的挑战。

    Anomaly detection is critical in the smart industry for preventing equipment failure, reducing downtime, and improving safety. Internet of Things (IoT) has enabled the collection of large volumes of data from industrial machinery, providing a rich source of information for Anomaly Detection. However, the volume and complexity of data generated by the Internet of Things ecosystems make it difficult for humans to detect anomalies manually. Machine learning (ML) algorithms can automate anomaly detection in industrial machinery by analyzing generated data. Besides, each technique has specific strengths and weaknesses based on the data nature and its corresponding systems. However, the current systematic mapping studies on Anomaly Detection primarily focus on addressing network and cybersecurity-related problems, with limited attention given to the industrial sector. Additionally, these studies do not cover the challenges involved in using ML for Anomaly Detection in industrial machinery wi
    
[^77]: 关于高斯数据之外的单指数模型

    On Single Index Models beyond Gaussian Data. (arXiv:2307.15804v1 [cs.LG])

    [http://arxiv.org/abs/2307.15804](http://arxiv.org/abs/2307.15804)

    该论文研究了超越高斯数据的单指数模型，探索了对稳定性和对称性的违反情况下的样本复杂性控制。

    

    稀疏高维函数已成为研究使用浅层神经网络的梯度下降方法行为的丰富框架，展示了它们在线性模型之外进行特征学习的能力。其中最简单的是单指数模型 $f(x) = \phi( x \cdot \theta^*)$，其中标签由一个未知的一维投影 $\theta^*$ 应用于任意非线性标量连接函数 $\phi$ 产生。通过专注于高斯数据，最近几项研究工作建立了一个引人注目的图景，将信息指数（与连接函数的正则性相关）与所需的样本复杂性进行了控制。实质上，这些工具利用了高斯分布的稳定性和球对称性。在本研究中，我们从 \cite{arous2020online} 的框架出发，探索了超越高斯设定的这个图景的扩展，其中稳定性或对称性可能被违反。

    Sparse high-dimensional functions have arisen as a rich framework to study the behavior of gradient-descent methods using shallow neural networks, showcasing their ability to perform feature learning beyond linear models. Amongst those functions, the simplest are single-index models $f(x) = \phi( x \cdot \theta^*)$, where the labels are generated by an arbitrary non-linear scalar link function $\phi$ applied to an unknown one-dimensional projection $\theta^*$ of the input data. By focusing on Gaussian data, several recent works have built a remarkable picture, where the so-called information exponent (related to the regularity of the link function) controls the required sample complexity. In essence, these tools exploit the stability and spherical symmetry of Gaussian distributions. In this work, building from the framework of \cite{arous2020online}, we explore extensions of this picture beyond the Gaussian setting, where both stability or symmetry might be violated. Focusing on the pl
    
[^78]: SAFE: 基于显著性的DNN自动驾驶系统的对抗性解释

    SAFE: Saliency-Aware Counterfactual Explanations for DNN-based Automated Driving Systems. (arXiv:2307.15786v1 [cs.LG])

    [http://arxiv.org/abs/2307.15786](http://arxiv.org/abs/2307.15786)

    本文提出了一种基于显著性图的方法，来生成更具信息性的CF解释，用于解释DNN自动驾驶系统的决策过程。

    

    CF解释器识别出在输入中最少的修改将模型的输出改变为其补集。换句话说，CF解释器计算出越过模型决策边界所需的最小修改。目前的深度生成CF模型通常与用户选择的特征一起工作，而不是关注黑盒模型的判别特征。因此，这样的CF示例可能不一定位于决策边界附近，从而违背了CF的定义。为了解决这个问题，本文提出了一种新颖的方法，利用显著图生成更具信息性的CF解释。

    A CF explainer identifies the minimum modifications in the input that would alter the model's output to its complement. In other words, a CF explainer computes the minimum modifications required to cross the model's decision boundary. Current deep generative CF models often work with user-selected features rather than focusing on the discriminative features of the black-box model. Consequently, such CF examples may not necessarily lie near the decision boundary, thereby contradicting the definition of CFs. To address this issue, we propose in this paper a novel approach that leverages saliency maps to generate more informative CF explanations. Source codes are available at: https://github.com/Amir-Samadi//Saliency_Aware_CF.
    
[^79]: 在图嵌入中基于球面和双曲面拓扑的编码应用于Ising MRF模型：经典和量子拓扑机器学习

    Spherical and Hyperbolic Toric Topology-Based Codes On Graph Embedding for Ising MRF Models: Classical and Quantum Topology Machine Learning. (arXiv:2307.15778v1 [cs.IT])

    [http://arxiv.org/abs/2307.15778](http://arxiv.org/abs/2307.15778)

    本论文介绍了在图嵌入中应用信息几何来描述Ising模型的基态，通过利用球面和双曲面拓扑上的编码，建立了机器学习和纠错编码之间的联系，并通过优化纠错码和发展嵌入方法提出了一种新的编码方法。

    

    本文介绍了将信息几何应用于描述Ising模型的基态的方法。通过利用托里克和球面拓扑上的循环和准循环码的奇偶检验矩阵来实现。该方法建立了机器学习和纠错编码之间的联系，特别是在自同构和准循环码循环矩阵的尺寸方面。这种方法对基于捕获集的嵌入方法的发展具有影响。利用统计物理学和数字几何学来优化纠错码，从而导致这些嵌入和稀疏因子化方法的出现。本文通过演示长距离领域的最新DNN架构（ChordMixer，Mega，Mega-chunk，CDIL，...）与特定类型（Cage-graph，Repeat Accumulate）的区块和卷积LDPC码等价的方式，建立了DNN架构和纠错编码之间的直接联系。

    The paper introduces the application of information geometry to describe the ground states of Ising models. This is achieved by utilizing parity-check matrices of cyclic and quasi-cyclic codes on toric and spherical topologies. The approach establishes a connection between machine learning and error-correcting coding, specifically in terms of automorphism and the size of the circulant of the quasi-cyclic code. This proposed approach has implications for the development of new embedding methods based on trapping sets. Statistical physics and number geometry are utilized to optimize error-correcting codes, leading to these embedding and sparse factorization methods. The paper establishes a direct connection between DNN architecture and error-correcting coding by demonstrating how state-of-the-art DNN architectures (ChordMixer, Mega, Mega-chunk, CDIL, ...) from the long-range arena can be equivalent to specific types (Cage-graph, Repeat Accumulate) of block and convolutional LDPC codes. Q
    
[^80]: 寻求收益壁垒：通过最优流形进行高维SRAM评估

    Seeking the Yield Barrier: High-Dimensional SRAM Evaluation Through Optimal Manifold. (arXiv:2307.15773v1 [cs.LG])

    [http://arxiv.org/abs/2307.15773](http://arxiv.org/abs/2307.15773)

    本研究通过最优流形概念将替代模型和重要性采样方法联系起来，提出了一种新型的高维SRAM评估方法。该方法名为OPTIMIS，结合了神经耦合流和洋葱采样，在保持性能优势的同时具备鲁棒性和一致性。

    

    随着模型电路将规模缩小到亚微米级别的先进技术节点，有效获得SRAM组件故障概率的准确估计已成为一个核心问题。在本研究中，我们重新审视了经典的范数最小化方法，并推广了它以适用于无限组件，并提出了新颖的最优流形概念，将基于替代模型和重要性采样（IS）的产量估计方法联系起来。接着，我们推导出了一个次优流形，最优超球体，它引导了一种高效的采样方法，能够识别到故障边界，称为洋葱采样。最后，我们使用神经耦合流作为重要性采样的提议分布，该分布类似于替代模型从样本中学习。这些组合产生了一种名为"优化流形重要性采样"（OPTIMIS）的新型产量估计方法，它保持了替代模型和重要性采样方法的优点，具有强韧性和稳定性。

    Being able to efficiently obtain an accurate estimate of the failure probability of SRAM components has become a central issue as model circuits shrink their scale to submicrometer with advanced technology nodes. In this work, we revisit the classic norm minimization method. We then generalize it with infinite components and derive the novel optimal manifold concept, which bridges the surrogate-based and importance sampling (IS) yield estimation methods. We then derive a sub-optimal manifold, optimal hypersphere, which leads to an efficient sampling method being aware of the failure boundary called onion sampling. Finally, we use a neural coupling flow (which learns from samples like a surrogate model) as the IS proposal distribution. These combinations give rise to a novel yield estimation method, named Optimal Manifold Important Sampling (OPTIMIS), which keeps the advantages of the surrogate and IS methods to deliver state-of-the-art performance with robustness and consistency, with 
    
[^81]: 加权变差空间与浅层ReLU网络的逼近

    Weighted variation spaces and approximation by shallow ReLU networks. (arXiv:2307.15772v1 [stat.ML])

    [http://arxiv.org/abs/2307.15772](http://arxiv.org/abs/2307.15772)

    本文研究了在有界域上通过单隐藏层ReLU网络逼近函数的问题，介绍了新的模型类定义加权变差空间，该定义与域本身相关。

    

    本文研究了在有界域Ω⊂Rd上，通过宽度为n的单隐藏层ReLU神经网络的输出来逼近函数f的情况。这种非线性的n项字典逼近已经得到广泛研究，因为它是神经网络逼近(NNA)的最简单情况。对于这种NNA形式，有几个著名的逼近结果，引入了在Ω上的函数的新型模型类，其逼近速率避免了维数灾难。这些新型模型类包括Barron类和基于稀疏性或变差的类，例如Radon域BV类。本文关注于在域Ω上定义这些新型模型类。当前这些模型类的定义不依赖于域Ω。通过引入加权变差空间的概念，给出了关于域的更恰当的模型类定义。这些新型模型类与域本身相关。

    We investigate the approximation of functions $f$ on a bounded domain $\Omega\subset \mathbb{R}^d$ by the outputs of single-hidden-layer ReLU neural networks of width $n$. This form of nonlinear $n$-term dictionary approximation has been intensely studied since it is the simplest case of neural network approximation (NNA). There are several celebrated approximation results for this form of NNA that introduce novel model classes of functions on $\Omega$ whose approximation rates avoid the curse of dimensionality. These novel classes include Barron classes, and classes based on sparsity or variation such as the Radon-domain BV classes.  The present paper is concerned with the definition of these novel model classes on domains $\Omega$. The current definition of these model classes does not depend on the domain $\Omega$. A new and more proper definition of model classes on domains is given by introducing the concept of weighted variation spaces. These new model classes are intrinsic to th
    
[^82]: Hydra效应：语言模型计算中的自适应自修复机制

    The Hydra Effect: Emergent Self-repair in Language Model Computations. (arXiv:2307.15771v1 [cs.LG])

    [http://arxiv.org/abs/2307.15771](http://arxiv.org/abs/2307.15771)

    本研究通过因果分析探究了语言模型计算的内部结构，发现了Hydra效应和晚期MLP层的平衡功能，并分析了它们在语言模型中的影响。

    

    本研究使用因果分析探究语言模型计算的内部结构，并展示了两种模式：（1）一种自适应计算形式，即语言模型中的某一自注意层被删减后另一层进行补偿（我们称之为Hydra效应）；（2）在后期多层感知机层中存在的平衡功能，用于调节最大似然令牌。我们的删减研究表明，语言模型层之间通常相对松散耦合（对一层的删减只会影响一小部分下游层）。令人惊讶的是，即使在没有任何形式的随机失活的语言模型中，这些效应仍然存在。我们在事实回忆的背景下分析了这些效应，并考虑了它们对语言模型的电路层面归因的影响。

    We investigate the internal structure of language model computations using causal analysis and demonstrate two motifs: (1) a form of adaptive computation where ablations of one attention layer of a language model cause another layer to compensate (which we term the Hydra effect) and (2) a counterbalancing function of late MLP layers that act to downregulate the maximum-likelihood token. Our ablation studies demonstrate that language model layers are typically relatively loosely coupled (ablations to one layer only affect a small number of downstream layers). Surprisingly, these effects occur even in language models trained without any form of dropout. We analyse these effects in the context of factual recall and consider their implications for circuit-level attribution in language models.
    
[^83]: 通过潜在狄利克雷分配和自然语言处理对简历进行评估，以实现有效的候选人选择

    Resume Evaluation through Latent Dirichlet Allocation and Natural Language Processing for Effective Candidate Selection. (arXiv:2307.15752v1 [cs.CL])

    [http://arxiv.org/abs/2307.15752](http://arxiv.org/abs/2307.15752)

    本文提出了一种使用LDA和自然语言处理的方法，通过提取简历中的实体并将其用于评分，从而实现有效的候选人选择。该方法在考虑所有属性的情况下达到了82%的准确率。

    

    本文提出了一种使用潜在狄利克雷分配（LDA）和使用SpaCy进行实体检测的简历评分方法。所提出的方法首先使用SpaCy的命名实体识别（NER）从简历中提取相关实体，如教育背景、工作经历和技能。然后，LDA模型使用这些实体为简历评分，为每个实体分配主题概率。此外，我们使用SpaCy的NER进行实体检测的详细分析，并报告其评估指标。使用LDA，我们提出的系统将简历分解为潜在主题，并提取有意义的语义表示。我们的模型实现了77%的准确率，只考虑技能，在考虑所有属性（如学院名称、工作经历、学位和技能）的情况下，总体准确率达到82%。

    In this paper, we propose a method for resume rating using Latent Dirichlet Allocation (LDA) and entity detection with SpaCy. The proposed method first extracts relevant entities such as education, experience, and skills from the resume using SpaCy's Named Entity Recognition (NER). The LDA model then uses these entities to rate the resume by assigning topic probabilities to each entity. Furthermore, we conduct a detailed analysis of the entity detection using SpaCy's NER and report its evaluation metrics. Using LDA, our proposed system breaks down resumes into latent topics and extracts meaningful semantic representations. With a vision to define our resume score to be more content-driven rather than a structure and keyword match driven, our model has achieved 77% accuracy with respect to only skills in consideration and an overall 82% accuracy with all attributes in consideration. (like college name, work experience, degree and skills)
    
[^84]: 正则化对损失函数的几何形状的影响

    How regularization affects the geometry of loss functions. (arXiv:2307.15744v1 [cs.LG])

    [http://arxiv.org/abs/2307.15744](http://arxiv.org/abs/2307.15744)

    这篇论文研究了正则化方法对于损失函数几何形状的影响，特别是对于深度神经网络，讨论了一种基本的几何特性Morse函数，并探讨了几种正则化方法使得正则化后的函数成为Morse函数的条件。

    

    神经网络所学习的内容基本上取决于底层损失函数的几何形状。我们研究了不同的正则化方法对损失函数的几何形状的影响。对于光滑函数来说，其中一个最基本的几何特性是它是否为Morse函数。对于非线性深度神经网络来说，未经正则化的损失函数L通常不是Morse函数。我们考虑了几种不同的正则化方法，包括权重衰减，并研究了哪些正则化方法使得经过正则化后的函数L_ε成为Morse函数。

    What neural networks learn depends fundamentally on the geometry of the underlying loss function. We study how different regularizers affect the geometry of this function. One of the most basic geometric properties of a smooth function is whether it is Morse or not. For nonlinear deep neural networks, the unregularized loss function $L$ is typically not Morse. We consider several different regularizers, including weight decay, and study for which regularizers the regularized function $L_\epsilon$ becomes Morse.
    
[^85]: AI用于预测行动：超越气候预测

    AI for Anticipatory Action: Moving Beyond Climate Forecasting. (arXiv:2307.15727v1 [cs.LG])

    [http://arxiv.org/abs/2307.15727](http://arxiv.org/abs/2307.15727)

    AI用于预测行动从气候预测向预测行动转变，利用机器学习模型的强大能力，在评估气候对特定人口的影响方面填补了方法论上的差距，以推进对气候变化最脆弱人口的灾害响应。

    

    灾害应对机构正从气候预测范式转向预测行动范式：不仅评估气候将如何，而且评估气候对特定人口的影响，从而实现主动响应和资源分配。机器学习模型在气候预测方面变得异常强大，但在促进预测行动方面仍存在方法论上的差距。本文综述了预测行动的概述，回顾了机器学习的相关应用，识别了共同面临的挑战，并突出了机器学习在推进对气候变化最脆弱人口的灾害响应方面的独特贡献。

    Disaster response agencies have been shifting from a paradigm of climate forecasting towards one of anticipatory action: assessing not just what the climate will be, but how it will impact specific populations, thereby enabling proactive response and resource allocation. Machine learning models are becoming exceptionally powerful at climate forecasting, but methodological gaps remain in terms of facilitating anticipatory action. Here we provide an overview of anticipatory action, review relevant applications of machine learning, identify common challenges, and highlight areas where machine learning can uniquely contribute to advancing disaster response for populations most vulnerable to climate change.
    
[^86]: 基于好奇心驱动的强化学习低级飞行控制

    Curiosity-Driven Reinforcement Learning based Low-Level Flight Control. (arXiv:2307.15724v1 [cs.LG])

    [http://arxiv.org/abs/2307.15724](http://arxiv.org/abs/2307.15724)

    本文提出了一种基于好奇心驱动的算法，通过从里程数据生成适当的电机速度来实现自主学习以控制飞行。通过该算法，四旋翼飞行器可以在控制偏航方向朝向目标位置的同时通过障碍物，该算法基于预测误差的新的好奇心方法。

    

    好奇心是许多有智能的自然生物的主要动机之一，通过具有可衡量智能水平的探索来实现更有效的学习。它使人类和许多动物能够通过寻找让他们感到惊讶的状态来高效地探索，以便学习更多他们所不知道的内容。因此，在保持好奇心的同时，他们能够学得更好。在机器学习领域，好奇心通常被结合到基于强化学习的算法中，作为一种内在的奖励。本研究提出了一种基于好奇心驱动的算法，通过从里程数据生成适当的电机速度来实现自主学习以控制飞行。通过我们提出的算法，四旋翼飞行器可以在控制偏航方向朝向所需位置的同时通过障碍物。为了实现这一目标，我们还提出了一种基于预测误差的新的好奇心方法。我们通过使用在线策略、离线策略和在线策略加噪音的测试来验证我们的算法。

    Curiosity is one of the main motives in many of the natural creatures with measurable levels of intelligence for exploration and, as a result, more efficient learning. It makes it possible for humans and many animals to explore efficiently by searching for being in states that make them surprised with the goal of learning more about what they do not know. As a result, while being curious, they learn better. In the machine learning literature, curiosity is mostly combined with reinforcement learning-based algorithms as an intrinsic reward. This work proposes an algorithm based on the drive of curiosity for autonomous learning to control by generating proper motor speeds from odometry data. The quadcopter controlled by our proposed algorithm can pass through obstacles while controlling the Yaw direction of the quad-copter toward the desired location. To achieve that, we also propose a new curiosity approach based on prediction error. We ran tests using on-policy, off-policy, on-policy pl
    
[^87]: 通过深度时间插值和聚类网络对生理特征进行研究，以鉴别急性疾病表型

    Identifying acute illness phenotypes via deep temporal interpolation and clustering network on physiologic signatures. (arXiv:2307.15719v1 [cs.LG])

    [http://arxiv.org/abs/2307.15719](http://arxiv.org/abs/2307.15719)

    通过深度时间插值和聚类网络分析入院6小时内的生命体征数据，在大规模的数据集上鉴别出不同的急性疾病表型，为早期临床决策提供支持。

    

    在医院入院的最初几个小时，对临床轨迹的影响很大，但由于数据不足，早期临床决策往往受到困扰。通过对入院6小时内的生命体征进行聚类分析，具有明显病理生理特征和结果的病人表型可以支持早期临床决策。我们创建了一个单中心、长期的电子病历数据集，包括75,762名成年患者在三级护理中心入院超过6小时。我们提出了一种深度时间插值和聚类网络，从稀疏、不规则抽样的生命体征数据中提取潜在表示，并在训练队列（n=41,502）中得出不同的患者表型。根据验证队列（n=17,415）选择了模型和超参数。测试队列（n=16,845）用于分析可重复性和与生物标志物的相关性。训练、验证和测试队列的年龄分布（54-55岁）、性别（55%女性）、种族、合并症和疾病严重程度均相似。共有四个簇被鉴别出来。

    Initial hours of hospital admission impact clinical trajectory, but early clinical decisions often suffer due to data paucity. With clustering analysis for vital signs within six hours of admission, patient phenotypes with distinct pathophysiological signatures and outcomes may support early clinical decisions. We created a single-center, longitudinal EHR dataset for 75,762 adults admitted to a tertiary care center for 6+ hours. We proposed a deep temporal interpolation and clustering network to extract latent representations from sparse, irregularly sampled vital sign data and derived distinct patient phenotypes in a training cohort (n=41,502). Model and hyper-parameters were chosen based on a validation cohort (n=17,415). Test cohort (n=16,845) was used to analyze reproducibility and correlation with biomarkers. The training, validation, and testing cohorts had similar distributions of age (54-55 yrs), sex (55% female), race, comorbidities, and illness severity. Four clusters were id
    
[^88]: 用于神经网络原子间势的合成预训练

    Synthetic pre-training for neural-network interatomic potentials. (arXiv:2307.15714v1 [physics.comp-ph])

    [http://arxiv.org/abs/2307.15714](http://arxiv.org/abs/2307.15714)

    这项研究通过利用合成原子数据进行预训练，提高了神经网络原子势模型在计算实践中的数值精度和稳定性。初步实验表明了该方法在碳的等变图神经网络势方法中的可行性。

    

    基于机器学习的原子间势已经改变了原子材料建模领域。然而，机器学习势方法严重依赖于训练过程中的量子力学参考数据的质量和数量，因此开发数据集和训练流程是一个日益重要的挑战。在其他机器学习研究领域常见的“合成”（人工）数据的思想的基础上，我们通过展示自身使用现有机器学习势方法生成的大规模合成原子数据进行预训练，以提高神经网络原子势模型的数值精度和计算稳定性。我们展示了一系列关于碳的等变图神经网络势方法的可行性，并进行初步实验测试了该方法的局限性。

    Machine learning (ML) based interatomic potentials have transformed the field of atomistic materials modelling. However, ML potentials depend critically on the quality and quantity of quantum-mechanical reference data with which they are trained, and therefore developing datasets and training pipelines is becoming an increasingly central challenge. Leveraging the idea of "synthetic" (artificial) data that is common in other areas of ML research, we here show that synthetic atomistic data, themselves obtained at scale with an existing ML potential, constitute a useful pre-training task for neural-network interatomic potential models. Once pre-trained with a large synthetic dataset, these models can be fine-tuned on a much smaller, quantum-mechanical one, improving numerical accuracy and stability in computational practice. We demonstrate feasibility for a series of equivariant graph-neural-network potentials for carbon, and we carry out initial experiments to test the limits of the appr
    
[^89]: R-LPIPS: 一种具有对抗鲁棒性的感知相似度度量

    R-LPIPS: An Adversarially Robust Perceptual Similarity Metric. (arXiv:2307.15157v1 [cs.CV])

    [http://arxiv.org/abs/2307.15157](http://arxiv.org/abs/2307.15157)

    该论文提出了一种针对对抗性示例具有鲁棒性的新型感知相似度度量方法R-LPIPS，用于解决在计算机视觉中广泛采用的LPIPS度量方法对对抗性示例的敏感性问题。

    

    相似度度量在计算机视觉中扮演着重要角色，用于捕捉图像的基本语义。最近几年，出现了先进的相似度度量方法，例如学习的感知图像块相似度（LPIPS）。这些度量利用来自训练神经网络的深度特征，并在相对图像相似度评估中展现出与人类感知密切一致的能力。然而，现在已经众所周知，神经网络容易受到对抗性示例的影响，即对人类来说不可见的小扰动，被精心设计用来故意误导模型。因此，LPIPS度量方法也对这些对抗性示例敏感。这种敏感性引入了重大的安全问题，特别是考虑到LPIPS在大规模应用中的广泛采用。本文提出了鲁棒的学习感知图像块相似度（R-LPIPS）度量方法，一种利用对抗性训练的深度特征的新度量方法。

    Similarity metrics have played a significant role in computer vision to capture the underlying semantics of images. In recent years, advanced similarity metrics, such as the Learned Perceptual Image Patch Similarity (LPIPS), have emerged. These metrics leverage deep features extracted from trained neural networks and have demonstrated a remarkable ability to closely align with human perception when evaluating relative image similarity. However, it is now well-known that neural networks are susceptible to adversarial examples, i.e., small perturbations invisible to humans crafted to deliberately mislead the model. Consequently, the LPIPS metric is also sensitive to such adversarial examples. This susceptibility introduces significant security concerns, especially considering the widespread adoption of LPIPS in large-scale applications. In this paper, we propose the Robust Learned Perceptual Image Patch Similarity (R-LPIPS) metric, a new metric that leverages adversarially trained deep f
    
[^90]: 数据集中的信息提升子群发现

    Information Gained Subgroup Discovery in Datasets. (arXiv:2307.15089v1 [cs.LG])

    [http://arxiv.org/abs/2307.15089](http://arxiv.org/abs/2307.15089)

    该论文研究了在数据集中通过信息提升的方法进行子群发现。具体针对肺癌治疗，在保持或提高治疗效果的同时减少副作用对于改善患者的生活质量非常重要，临床指南虽然提供了治疗建议，但仍未将治疗结果纳入考量。

    

    肺癌是癌症死亡的主要原因。预计2023年将有超过238,340例新的肺癌患者，其中有超过127,070例死亡。选择正确的治疗方案是提高存活率和改善患者生活质量的重要因素。癌症治疗可能引发副作用，这些毒副反应会引起不同的健康问题，影响患者的生活质量。因此，在保持或提高治疗效果的同时减少治疗副作用是临床角度要追求的重要目标。另一方面，临床指南包括癌症治疗建议的一般知识，以协助临床医生。尽管他们根据癌症疾病方面和个体患者特征提供治疗建议，但并未提供基于治疗结果的统计分析。因此，需要对临床指南与治疗结果进行比较。

    Lung cancer is the leading cause of cancer death. More than 238,340 new cases of lung cancer patients are expected in 2023, with an estimation of more than 127,070 deaths. Choosing the correct treatment is an important element to enhance the probability of survival and to improve patient's quality of life. Cancer treatments might provoke secondary effects. These toxicities cause different health problems that impact the patient's quality of life. Hence, reducing treatments toxicities while maintaining or improving their effectivenes is an important goal that aims to be pursued from the clinical perspective. On the other hand, clinical guidelines include general knowledge about cancer treatment recommendations to assist clinicians. Although they provide treatment recommendations based on cancer disease aspects and individual patient features, a statistical analysis taking into account treatment outcomes is not provided here. Therefore, the comparison between clinical guidelines with tre
    
[^91]: MATNilm: 有限标注数据下多设备任务的无干扰负载监测

    MATNilm: Multi-appliance-task Non-intrusive Load Monitoring with Limited Labeled Data. (arXiv:2307.14778v1 [cs.LG])

    [http://arxiv.org/abs/2307.14778](http://arxiv.org/abs/2307.14778)

    本研究提出了MATNilm框架，能够通过样本增强和共享的分层拆分结构，在有限标记数据下提高非侵入式负载监测的性能。

    

    非侵入式负载监测 (NILM) 通过将整个房屋的总功耗信号分解来识别各种家用电器的状态和电能消耗。高效准确的负载监测有助于建立用户个人资料、智能家居能源管理和峰值负荷转移。这对于最终用户和公用事业都有益处，可以提高电力分配网络的整体效率。现有的方法主要集中在为每个电器开发单独的模型。这些方法通常依赖于大量难以收集的家庭标记数据。在本文中，我们提出了一种多设备任务框架，配合一个训练高效的样本增强 (SA) 方案，以有限的标记数据提高分解性能。对于每个电器，我们开发了一个共享的分层拆分结构，用于其回归和分类任务。此外，我们还提出了一个二维注意机制。

    Non-intrusive load monitoring (NILM) identifies the status and power consumption of various household appliances by disaggregating the total power usage signal of an entire house. Efficient and accurate load monitoring facilitates user profile establishment, intelligent household energy management, and peak load shifting. This is beneficial for both the end-users and utilities by improving the overall efficiency of a power distribution network. Existing approaches mainly focus on developing an individual model for each appliance. Those approaches typically rely on a large amount of household-labeled data which is hard to collect. In this paper, we propose a multi-appliance-task framework with a training-efficient sample augmentation (SA) scheme that boosts the disaggregation performance with limited labeled data. For each appliance, we develop a shared-hierarchical split structure for its regression and classification tasks. In addition, we also propose a two-dimensional attention mech
    
[^92]: 模仿复杂轨迹：桥接低层稳定性与高层行为

    Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior. (arXiv:2307.14619v1 [cs.LG])

    [http://arxiv.org/abs/2307.14619](http://arxiv.org/abs/2307.14619)

    本文提出了一个理论框架，研究了在非线性动态系统中模仿复杂专家演示的行为。通过稳定模仿策略并确保准确估计演示者分布，可以使模仿者与演示者的轨迹分布相近。

    

    我们提出了一个理论框架来研究在非线性动态系统中模仿随机、非马尔可夫、潜在多模态（即“复杂”）专家演示的行为。我们的框架使用低层控制器（无论是学习的还是隐含的）来稳定围绕专家演示的模仿策略。我们证明，在（a）合适的低层稳定性保证和（b）学习策略的随机连续性属性（我们称之为“总变差连续性”）（TVC）的情况下，一个精确估计演示者状态分布上的行动的模仿者会与演示者对整个轨迹的分布相近。然后，我们证明可以通过将流行的数据增强规则与一种新颖的算法技巧相结合（即在执行时添加增强噪声）来确保TVC并且最小程度上降低精度。我们将我们的保证实例化为由扩散模型参数化的策略，并证明如果学习者准确地估计了演示者的分布，则最终完成这种实例化。

    We propose a theoretical framework for studying the imitation of stochastic, non-Markovian, potentially multi-modal (i.e. "complex" ) expert demonstrations in nonlinear dynamical systems. Our framework invokes low-level controllers either learned or implicit in position-command control - to stabilize imitation policies around expert demonstrations. We show that with (a) a suitable low-level stability guarantee and (b) a stochastic continuity property of the learned policy we call "total variation continuity" (TVC), an imitator that accurately estimates actions on the demonstrator's state distribution closely matches the demonstrator's distribution over entire trajectories. We then show that TVC can be ensured with minimal degradation of accuracy by combining a popular data-augmentation regimen with a novel algorithmic trick: adding augmentation noise at execution time. We instantiate our guarantees for policies parameterized by diffusion models and prove that if the learner accuratel
    
[^93]: Skill-it! 一个数据驱动的技能框架，用于理解和训练语言模型

    Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models. (arXiv:2307.14430v1 [cs.CL])

    [http://arxiv.org/abs/2307.14430](http://arxiv.org/abs/2307.14430)

    本论文提出了一个数据驱动的技能框架，用于理解和训练语言模型。通过研究人类获得技能的有序性，我们证明了语言模型学习技能时也有一定的顺序，并且这种顺序可以改善对语言模型的理解和数据高效训练。

    

    训练数据的质量对于预训练的大型语言模型的性能有重要影响。在固定的token预算下，我们研究如何选择能够在各个任务中获得良好下游模型性能的数据。我们提出了一个新的框架，基于一个简单的假设：人类在有意义的顺序中获得相互依赖的技能，语言模型在学习一组技能时也会遵循这样的顺序。如果存在这样的顺序，可以用于改进对语言模型的理解和数据高效训练。利用这种直觉，我们的框架将技能的概念和有序的技能集合的概念形式化为相关数据。首先，使用合成数据和真实数据，我们证明了这些有序的技能集合的存在，并且它们的存在使得在训练其先决条件技能时可以使用更少的数据来学习更高级的技能。其次，使用我们提出的框架，我们介绍了一种在线数据采样算法。

    The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm
    
[^94]: 基于模拟的推理用于心血管模型

    Simulation-based Inference for Cardiovascular Models. (arXiv:2307.13918v1 [stat.ML])

    [http://arxiv.org/abs/2307.13918](http://arxiv.org/abs/2307.13918)

    本研究将心血管模型的逆问题作为统计推理进行解决，在体外进行了五个生物标记物的不确定性分析，展示了模拟推理的能力。

    

    在过去的几十年中，血流动力学模拟器不断发展，已成为研究体外心血管系统的首选工具。虽然这样的工具通常用于从生理参数模拟全身血流动力学，但解决将波形映射回合理的生理参数的逆问题仍然有很大的潜力和挑战。受模拟推理（SBI）的进展的启发，我们将这个逆问题作为统计推理来处理。与其他方法不同，SBI为感兴趣的参数提供了后验分布，提供了关于个体测量的不确定性的多维表示。我们通过对比几种测量模态来展示这种能力，进行了五个临床感兴趣的生物标志物的体外不确定性分析。除了确认已知事实，比如估计心率的可行性，我们的研究还突出了…

    Over the past decades, hemodynamics simulators have steadily evolved and have become tools of choice for studying cardiovascular systems in-silico. While such tools are routinely used to simulate whole-body hemodynamics from physiological parameters, solving the corresponding inverse problem of mapping waveforms back to plausible physiological parameters remains both promising and challenging. Motivated by advances in simulation-based inference (SBI), we cast this inverse problem as statistical inference. In contrast to alternative approaches, SBI provides \textit{posterior distributions} for the parameters of interest, providing a \textit{multi-dimensional} representation of uncertainty for \textit{individual} measurements. We showcase this ability by performing an in-silico uncertainty analysis of five biomarkers of clinical interest comparing several measurement modalities. Beyond the corroboration of known facts, such as the feasibility of estimating heart rate, our study highlight
    
[^95]: 用于程序之间变量映射的图神经网络——扩展版本

    Graph Neural Networks For Mapping Variables Between Programs -- Extended Version. (arXiv:2307.13014v1 [cs.SE])

    [http://arxiv.org/abs/2307.13014](http://arxiv.org/abs/2307.13014)

    本文提出了使用图神经网络(GNNs)基于程序的抽象语法树(ASTs)来映射变量集，以解决程序比较、分析、修复和克隆检测等任务。在初学者编程作业中进行的实验证明了变量映射的有效性。

    

    自动程序分析是计算机科学中许多领域的关键研究领域，特别是形式方法和人工智能。由于程序等价问题的不可判定性，比较两个程序非常具有挑战性。通常，为了比较两个程序，需要对两个程序的变量集之间建立关系。因此，在诸如程序等价性、程序分析、程序修复和克隆检测等任务上，映射两个程序之间的变量是非常有用的。在这项工作中，我们提出使用图神经网络(GNNs)基于两个程序的抽象语法树(ASTs)来映射变量集。为了展示变量映射的优势，我们在程序修复任务中提供了这些映射的三个用例，以修复初学者编程作业中常见的和经常发生的错误。实验结果基于一个包含4166对错误/修正程序的数据集。

    Automated program analysis is a pivotal research domain in many areas of Computer Science -- Formal Methods and Artificial Intelligence, in particular. Due to the undecidability of the problem of program equivalence, comparing two programs is highly challenging. Typically, in order to compare two programs, a relation between both programs' sets of variables is required. Thus, mapping variables between two programs is useful for a panoply of tasks such as program equivalence, program analysis, program repair, and clone detection. In this work, we propose using graph neural networks (GNNs) to map the set of variables between two programs based on both programs' abstract syntax trees (ASTs). To demonstrate the strength of variable mappings, we present three use-cases of these mappings on the task of program repair to fix well-studied and recurrent bugs among novice programmers in introductory programming assignments (IPAs). Experimental results on a dataset of 4166 pairs of incorrect/corr
    
[^96]: 使用深度强化学习的多喷口对旋转圆柱体流动进行主动控制

    Active Control of Flow over Rotating Cylinder by Multiple Jets using Deep Reinforcement Learning. (arXiv:2307.12083v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2307.12083](http://arxiv.org/abs/2307.12083)

    本研究使用深度强化学习算法结合旋转和多个可控喷口，通过优化喷口数量和位置，传感器位置以及每个动作可允许的最大流量和每个episode中允许的总喷口数的形式，实现对旋转圆柱体流动的主动控制，抑制涡流脱落和稳定卡门涡流。

    

    人工智能的真正威力体现在强化学习中，由于其动态性质，强化学习在计算和物理方面更为复杂。旋转和喷射已被证实是减少钝体所受阻力的一种有效的主动流动控制方式。本研究将使用深度强化学习算法(DRL)来添加旋转，并利用多个可控喷口以实现最大可能的阻力抑制。将介绍DRL代码的特点，包括控制参数、其限制以及用于旋转的DRL网络的优化。本研究将重点优化喷口的数量和位置、传感器位置以及每个动作可允许的最大流量和每个episode中允许的总喷口数的形式。研究结果表明，将旋转与DRL工具相结合是有希望的，因为它可以抑制涡流脱落，稳定卡门涡流。

    The real power of artificial intelligence appears in reinforcement learning, which is computationally and physically more sophisticated due to its dynamic nature. Rotation and injection have been a proven way of active flow control to reduce the drag force exerted on blunt bodies. Rotation will be added to the cylinder alongside the deep reinforcement learning (DRL) algorithm, which uses multiple controlled jets to reach maximum possible drag suppression. Characteristics of the DRL code, including controlling parameters, their limitations, and optimization of the DRL network for use with rotation will be presented. This work will focus on optimizing the number and positions of the jets, sensors location, and maximum allowed flow rate to jets in the form of maximum allowed flow rate of each actuation and the total number of them per episode. It is found that combining the rotation with the DRL tools is promising, since it suppresses the vortex shedding, stabilizes the Karman vortex stre
    
[^97]: 通过原位无模型优化实现高性能真实光学计算

    High-performance real-world optical computing trained by in situ model-free optimization. (arXiv:2307.11957v1 [physics.optics] CROSS LISTED)

    [http://arxiv.org/abs/2307.11957](http://arxiv.org/abs/2307.11957)

    本论文提出了一种无模型优化光学计算系统的方法，通过在原位进行轻量级优化，实现了高性能的真实光学计算。实验证明该方法在分类准确度上优于传统方法，并展示了在高速细胞分析方面的潜力。这种方法的固有简单性和低需求的计算资源促进了光学计算技术从实验室研究向真实世界应用的转变。

    

    光学计算系统可以提供高速和低能耗的数据处理，但在计算密集的训练和从模拟到现实的转换中存在不足。我们提出了一种基于得分梯度估计算法的轻量级原位优化光学计算系统的无模型解决方案。该方法将系统视为黑盒子，直接将损失反向传播到光学权重的概率分布，从而避免了对计算密集和有偏见的系统模拟的需求。通过在单层衍射光学计算系统上进行实验证明在MNIST和FMNIST数据集上具有优越的分类准确度。此外，我们展示了其在无图片和高速细胞分析方面的潜力。我们提出的方法的固有简单性，结合其对计算资源的低需求，加速了光学计算从实验室演示到真实世界应用的过渡。

    Optical computing systems can provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and simulation-to-reality gap. We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score gradient estimation algorithm. This approach treats the system as a black box and back-propagates loss directly to the optical weights' probabilistic distributions, hence circumventing the need for computation-heavy and biased system simulation. We demonstrate a superior classification accuracy on the MNIST and FMNIST datasets through experiments on a single-layer diffractive optical computing system. Furthermore, we show its potential for image-free and high-speed cell analysis. The inherent simplicity of our proposed method, combined with its low demand for computational resources, expedites the transition of optical computing from laboratory demonstrations to real-world applications.
    
[^98]: 机器学习系统可靠性的整体评估

    A Holistic Assessment of the Reliability of Machine Learning Systems. (arXiv:2307.10586v1 [cs.LG])

    [http://arxiv.org/abs/2307.10586](http://arxiv.org/abs/2307.10586)

    本文提出了一种用于评估机器学习系统可靠性的整体评估方法，通过评估分布内准确性、分布偏移鲁棒性、对抗鲁棒性、校准性和越界检测能力等五个关键属性，引入了可靠性得分来评估整个系统的可靠性。

    

    随着机器学习系统越来越多地渗透到医疗保健、交通运输、军事和国家安全等高风险领域，人们对其可靠性产生了担忧。尽管取得了显著进展，但由于对抗攻击或环境变化，这些系统的性能可能显著降低，导致预测过于自信、无法检测输入故障以及在意外场景中无法泛化。本文提出了一种用于评估机器学习系统可靠性的整体评估方法。我们的框架评估了五个关键属性：分布内准确性、分布偏移鲁棒性、对抗鲁棒性、校准性和越界检测能力。我们还引入了一个可靠性得分来评估整个系统的可靠性。为了提供不同算法方法性能的见解，我们识别和分类了最先进的技术，然后使用真实世界的任务对其中的一些进行了评估。

    As machine learning (ML) systems increasingly permeate high-stakes settings such as healthcare, transportation, military, and national security, concerns regarding their reliability have emerged. Despite notable progress, the performance of these systems can significantly diminish due to adversarial attacks or environmental changes, leading to overconfident predictions, failures to detect input faults, and an inability to generalize in unexpected scenarios. This paper proposes a holistic assessment methodology for the reliability of ML systems. Our framework evaluates five key properties: in-distribution accuracy, distribution-shift robustness, adversarial robustness, calibration, and out-of-distribution detection. A reliability score is also introduced and used to assess the overall system reliability. To provide insights into the performance of different algorithmic approaches, we identify and categorize state-of-the-art techniques, then evaluate a selection on real-world tasks using
    
[^99]: 一种基于无监督深度学习的极化解码器的单次解码方法

    For One-Shot Decoding: Unsupervised Deep Learning-Based Polar Decoder. (arXiv:2307.08004v1 [cs.IT])

    [http://arxiv.org/abs/2307.08004](http://arxiv.org/abs/2307.08004)

    本文提出了一种基于无监督深度学习的极化解码器的单次解码方法，通过自监督学习训练神经网络，消除了对预定义标签的依赖，实现了在通信系统实际数据上的直接训练，其性能接近最大后验概率解码器，并展示出更优越的泛化能力。

    

    我们提出了一种无监督深度学习的解码方案，可以实现极化码的单次解码。在这种方案中，我们不再使用信息位向量作为标签来训练神经网络（NN），而是通过自监督学习利用极化码的生成矩阵来训练NN以实现有界距离解码。这种方法消除了对预定义标签的依赖，增强了直接在通信系统的实际数据上进行训练的可能性，从而提高了可应用性。此外，计算机模拟表明，（i）所提出的方案的误比特率（BER）和块错误率（BLER）性能在非常短的数据包上可以接近最大后验概率（MAP）解码器的性能，（ii）与传统的解码器相比，所提出的NN解码器展示了更优越的泛化能力。

    We propose an unsupervised deep learning-based decoding scheme that enables one-shot decoding of polar codes. In the proposed scheme, rather than using the information bit vectors as labels for training the neural network (NN) through supervised learning as the conventional scheme did, the NN is trained to function as a bounded distance decoder by leveraging the generator matrix of polar codes through self-supervised learning. This approach eliminates the reliance on predefined labels, empowering the potential to train directly on the actual data within communication systems and thereby enhancing the applicability. Furthermore, computer simulations demonstrate that (i) the bit error rate (BER) and block error rate (BLER) performances of the proposed scheme can approach those of the maximum a posteriori (MAP) decoder for very short packets and (ii) the proposed NN decoder exhibits much superior generalization ability compared to the conventional one.
    
[^100]: DISPEL：通过领域特定解放进行域泛化

    DISPEL: Domain Generalization via Domain-Specific Liberating. (arXiv:2307.07181v1 [cs.CV])

    [http://arxiv.org/abs/2307.07181](http://arxiv.org/abs/2307.07181)

    DISPEL是一种通过后处理细粒度掩蔽方法，能够在嵌入空间中过滤掉未定义和无法区分的领域特定特征的领域泛化方法。

    

    领域泛化旨在通过仅在有限的源领域上进行训练，学习一个能够在未知测试领域上表现良好的泛化模型。然而，现有的领域泛化方法往往引入与预测无关的噪声或需要收集领域标签来解决。为了应对这些挑战，我们从不同的角度考虑了领域泛化问题，将底层特征组划分为领域共享和领域特定特征。然而，领域特定特征很难从输入数据中进行识别和区分。在这项工作中，我们提出了一种名为DISPEL（DomaIn-SPEcific Liberating）的后处理细粒度掩蔽方法，能够在嵌入空间中过滤掉未定义和无法区分的领域特定特征。具体而言，DISPEL利用一个掩蔽生成器为每个输入数据生成一个唯一的掩蔽来过滤领域特定特征。DISPEL框架非常灵活，可以应用于任何细粒度的掩蔽任务和领域泛化方法。

    Domain generalization aims to learn a generalization model that can perform well on unseen test domains by only training on limited source domains. However, existing domain generalization approaches often bring in prediction-irrelevant noise or require the collection of domain labels. To address these challenges, we consider the domain generalization problem from a different perspective by categorizing underlying feature groups into domain-shared and domain-specific features. Nevertheless, the domain-specific features are difficult to be identified and distinguished from the input data. In this work, we propose DomaIn-SPEcific Liberating (DISPEL), a post-processing fine-grained masking approach that can filter out undefined and indistinguishable domain-specific features in the embedding space. Specifically, DISPEL utilizes a mask generator that produces a unique mask for each input data to filter domain-specific features. The DISPEL framework is highly flexible to be applied to any fin
    
[^101]: 解决组合分布偏移问题：基于矩阵补全的观点

    Tackling Combinatorial Distribution Shift: A Matrix Completion Perspective. (arXiv:2307.06457v1 [cs.LG])

    [http://arxiv.org/abs/2307.06457](http://arxiv.org/abs/2307.06457)

    该论文研究了组合分布偏移的问题，提出了基于矩阵补全的解决方法。通过在特殊情况下的双线性嵌入，实现对训练中未涵盖的测试分布进行外推。这个设置将缺失非随机数据的矩阵补全问题广义化。

    

    在分布偏移下获得严格的统计保证仍然是一个开放且活跃的研究领域。我们研究了一种称为组合分布偏移的设置，其中(a)在测试和训练分布下，标签$z$由特征$(x,y)$的对决定，(b)训练分布涵盖了$x$和$y$分别的一定边缘分布，但是(c)测试分布涉及了一个在训练分布中未涵盖的$(x,y)$的产品分布的示例。我们专注于标签由双线性嵌入到Hilbert空间$H$中给出的特殊情况：$\mathbb{E}[z \mid x,y ]=\langle f_{\star}(x),g_{\star}(y)\rangle_{{H}}$，我们的目标是对在训练中未涵盖的测试分布域进行外推，即实现双线性组合外推。我们的设置将缺失非随机数据的矩阵补全的一个特殊情况广义化，对于该情况，所有现有结果都要求....

    Obtaining rigorous statistical guarantees for generalization under distribution shift remains an open and active research area. We study a setting we call combinatorial distribution shift, where (a) under the test- and training-distributions, the labels $z$ are determined by pairs of features $(x,y)$, (b) the training distribution has coverage of certain marginal distributions over $x$ and $y$ separately, but (c) the test distribution involves examples from a product distribution over $(x,y)$ that is {not} covered by the training distribution. Focusing on the special case where the labels are given by bilinear embeddings into a Hilbert space $H$: $\mathbb{E}[z \mid x,y ]=\langle f_{\star}(x),g_{\star}(y)\rangle_{{H}}$, we aim to extrapolate to a test distribution domain that is $not$ covered in training, i.e., achieving bilinear combinatorial extrapolation.  Our setting generalizes a special case of matrix completion from missing-not-at-random data, for which all existing results requi
    
[^102]: 解读疾病进展聚类中的深度嵌入

    Interpreting deep embeddings for disease progression clustering. (arXiv:2307.06060v1 [stat.ML])

    [http://arxiv.org/abs/2307.06060](http://arxiv.org/abs/2307.06060)

    本文提出了一种在疾病进展聚类中解读深度嵌入的新方法，并通过评估2型糖尿病参与者数据集展示了对疾病进展模式的临床意义性见解。

    

    我们提出了一种在患者聚类的背景下解读深度嵌入的新方法。我们在来自英国生物库的2型糖尿病参与者数据集上评估我们的方法，并展示出对疾病进展模式的临床意义性见解。

    We propose a novel approach for interpreting deep embeddings in the context of patient clustering. We evaluate our approach on a dataset of participants with type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful insights into disease progression patterns.
    
[^103]: 用于下游治疗效果估计的贝叶斯因果发现方法的基准测试

    Benchmarking Bayesian Causal Discovery Methods for Downstream Treatment Effect Estimation. (arXiv:2307.04988v1 [cs.LG])

    [http://arxiv.org/abs/2307.04988](http://arxiv.org/abs/2307.04988)

    该研究评估了六种基准因果发现方法和一种新提出的基于 GFlowNets 的方法在治疗效果估计任务中的表现，并发现 GFlowNets 具有捕捉各种有用和多样的平均处理效应模式的能力。

    

    决策中因果性的实际应用被广泛认可，因果发现和推理在本质上是相互交织的。然而，在因果发现方法的评估中存在明显的差距，对下游推理的重视程度不足。为了填补这一空白，我们评估了六种已建立的基准因果发现方法和一种基于 GFlowNets 的新方法在治疗效果估计的下游任务上的表现。通过实施一个稳健的评估过程，我们为治疗效果估计的这些因果发现方法的有效性提供了有价值的见解，考虑了合成和真实场景以及低数据场景。此外，我们研究的结果表明，GFlowNets 具有有效捕捉各种有用和多样的平均处理效应模式的能力。

    The practical utility of causality in decision-making is widely recognized, with causal discovery and inference being inherently intertwined. Nevertheless, a notable gap exists in the evaluation of causal discovery methods, where insufficient emphasis is placed on downstream inference. To address this gap, we evaluate six established baseline causal discovery methods and a newly proposed method based on GFlowNets, on the downstream task of treatment effect estimation. Through the implementation of a robust evaluation procedure, we offer valuable insights into the efficacy of these causal discovery methods for treatment effect estimation, considering both synthetic and real-world scenarios, as well as low-data scenarios. Furthermore, the results of our study demonstrate that GFlowNets possess the capability to effectively capture a wide range of useful and diverse ATE modes.
    
[^104]: 点云与基于图像的模型在量热器快速模拟中的比较

    Comparison of Point Cloud and Image-based Models for Calorimeter Fast Simulation. (arXiv:2307.04780v1 [cs.LG])

    [http://arxiv.org/abs/2307.04780](http://arxiv.org/abs/2307.04780)

    本文比较了使用点云和基于图像的模型来进行量热器快速模拟，发现点云更自然地表示了量热器淋浴，处理稀疏数据集更优秀，并且可以使用更紧凑的模型和数据文件进行实现。

    

    基于得分的生成模型是一种新的生成模型类别，已被证明能准确生成高维度的量热器数据集。生成模型的最新进展使用图像与3D体素表示和建模复杂的量热器淋浴。然而，点云可能是量热器淋浴更自然的表示方式，尤其是在具有高粒度的量热器中。点云保留了原始模拟的所有信息，更自然地处理稀疏数据集，并且可以使用更紧凑的模型和数据文件进行实现。在这项工作中，用同一组量热器模拟数据训练了两种最先进的基于得分的模型，并进行了直接比较。

    Score based generative models are a new class of generative models that have been shown to accurately generate high dimensional calorimeter datasets. Recent advances in generative models have used images with 3D voxels to represent and model complex calorimeter showers. Point clouds, however, are likely a more natural representation of calorimeter showers, particularly in calorimeters with high granularity. Point clouds preserve all of the information of the original simulation, more naturally deal with sparse datasets, and can be implemented with more compact models and data files. In this work, two state-of-the-art score based models are trained on the same set of calorimeter simulation and directly compared.
    
[^105]: FreeDrag: 点追踪并不适用于交互式的基于点的图像编辑

    FreeDrag: Point Tracking is Not What You Need for Interactive Point-based Image Editing. (arXiv:2307.04684v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.04684](http://arxiv.org/abs/2307.04684)

    FreeDrag提出了一种基于特征的方法来解决DragGAN在点追踪方面的困难，通过自适应模板特征、线性搜索和模糊定位技术，实现了稳定和高效的基于点的图像编辑。

    

    为了满足图像编辑的复杂和多样化需求，对图像内容的精确和灵活的操纵是不可或缺的。最近，DragGAN通过基于点的操纵实现了令人印象深刻的编辑结果。然而，我们观察到DragGAN在点的追踪上存在困难，包括错误追踪和模糊追踪。为了解决这些问题，我们提出了FreeDrag，它采用了基于特征的方法来减轻DragGAN中点追踪的负担。FreeDrag结合了自适应模板特征、线性搜索和模糊定位技术，实现了稳定和高效的基于点的图像编辑。大量实验表明，我们的方法优于DragGAN，并能在具有相似特征的困难情景下实现稳定的基于点的编辑。

    To serve the intricate and varied demands of image editing, precise and flexible manipulation of image content is indispensable. Recently, DragGAN has achieved impressive editing results through point-based manipulation. However, we have observed that DragGAN struggles with miss tracking, where DragGAN encounters difficulty in effectively tracking the desired handle points, and ambiguous tracking, where the tracked points are situated within other regions that bear resemblance to the handle points. To deal with the above issues, we propose FreeDrag, which adopts a feature-oriented approach to free the burden on point tracking within the point-oriented methodology of DragGAN. The FreeDrag incorporates adaptive template features, line search, and fuzzy localization techniques to perform stable and efficient point-based image editing. Extensive experiments demonstrate that our method is superior to the DragGAN and enables stable point-based editing in challenging scenarios with similar st
    
[^106]: Solvent: 一个用于蛋白质折叠的框架

    Solvent: A Framework for Protein Folding. (arXiv:2307.04603v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2307.04603](http://arxiv.org/abs/2307.04603)

    Solvent是一个用于蛋白质折叠的统一研究框架，支持最新模型重要组件的实现和基准测试，并提供对蛋白质结构建模领域的有用见解。

    

    一致性和可靠性对于进行人工智能研究至关重要。许多著名的研究领域，如目标检测，已经通过稳定的基准框架进行了比较和验证。在AlphaFold2之后，蛋白质折叠任务已经进入了一个新阶段，许多方法都是基于AlphaFold2的组件提出的。在蛋白质折叠中，一个统一的研究框架的重要性包括实现和基准，以一致且公平地比较各种方法。为了实现这一点，我们提出了Solvent，一个支持最新模型重要组件的蛋白质折叠框架。Solvent包含在一个统一的代码库中实现的不同模型，并支持在相同数据集上对定义的模型进行训练和评估。我们对著名算法及其组件进行了基准测试，并进行了实验以对蛋白质结构建模领域提供有用的见解。我们希望Solvent能提高蛋白质折叠研究的可靠性。

    Consistency and reliability are crucial for conducting AI research. Many famous research fields, such as object detection, have been compared and validated with solid benchmark frameworks. After AlphaFold2, the protein folding task has entered a new phase, and many methods are proposed based on the component of AlphaFold2. The importance of a unified research framework in protein folding contains implementations and benchmarks to consistently and fairly compare various approaches. To achieve this, we present Solvent, an protein folding framework that supports significant components of state-of-th-arts models in the manner of off-the-shelf interface Solvent contains different models implemented in a unified codebase and supports training and evaluation for defined models on the same dataset. We benchmark well-known algorithms and their components and provide experiments that give helpful insights into the protein structure modeling field. We hope that Solvent will increase the reliabili
    
[^107]: 高效的贝叶斯行程时间层析成像方法，利用基于敏感性的多项式混沌展开和深度生成网络的地质复杂先验

    Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks. (arXiv:2307.04228v1 [physics.geo-ph])

    [http://arxiv.org/abs/2307.04228](http://arxiv.org/abs/2307.04228)

    本论文提出了一种高效的贝叶斯行程时间层析成像方法，利用敏感性信息的多项式混沌展开和深度生成网络，以处理地质复杂性先验的挑战。

    

    蒙特卡洛马尔可夫链（MCMC）方法常常面临两个根本性挑战：先验分布的准确刻画和似然函数的高效评估。在层析成像的贝叶斯研究中，主成分分析（PCA）在某些情况下可以方便地定义先验分布，并同时借助基于多项式混沌展开（PCE）的准确代理模型来替代计算密集的全物理正向求解器。当PCA无法直接提供定义先验分布的方式时，可以采用深度生成模型（例如变分自编码器（VAEs））等替代方法。然而，准确产生一个能够捕捉VAE的潜在参数与正向建模输出之间复杂非线性关系的代理模型是一个显著的挑战。

    Monte Carlo Markov Chain (MCMC) methods commonly confront two fundamental challenges: the accurate characterization of the prior distribution and the efficient evaluation of the likelihood. In the context of Bayesian studies on tomography, principal component analysis (PCA) can in some cases facilitate the straightforward definition of the prior distribution, while simultaneously enabling the implementation of accurate surrogate models based on polynomial chaos expansion (PCE) to replace computationally intensive full-physics forward solvers. When faced with scenarios where PCA does not offer a direct means of easily defining the prior distribution alternative methods like deep generative models (e.g., variational autoencoders (VAEs)), can be employed as viable options. However, accurately producing a surrogate capable of capturing the intricate non-linear relationship between the latent parameters of a VAE and the outputs of forward modeling presents a notable challenge. Indeed, while
    
[^108]: 何时使用Transformer在强化学习中发光？从记忆和信用分配中解耦

    When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment. (arXiv:2307.03864v1 [cs.LG])

    [http://arxiv.org/abs/2307.03864](http://arxiv.org/abs/2307.03864)

    Transformer在强化学习中的作用是增强记忆能力而不是改进信用分配。

    

    强化学习算法面临两个不同的挑战：学习有效的过去和当前观测的表示，并确定行动如何影响未来的收益。这两个挑战都涉及到建模长期依赖关系。Transformer架构在解决涉及长期依赖关系的问题，包括在强化学习领域方面非常成功。然而，Transformer基于的强化学习方法表现强劲的原因尚不清楚：是因为它们学习了有效的记忆，还是因为它们执行了有效的信用分配？在引入记忆长度和信用分配长度的形式定义之后，我们设计了简单的可配置任务来测量这些不同的量。我们的实证结果表明，Transformer可以增强强化学习算法的记忆能力，扩展到需要记住1500步前观察的任务。然而，Transformer无法改进长期的信用分配。总之，我们的研究揭示了Transformer在强化学习中记忆和信用分配方面的不同作用。

    Reinforcement learning (RL) algorithms face two distinct challenges: learning effective representations of past and present observations, and determining how actions influence future returns. Both challenges involve modeling long-term dependencies. The transformer architecture has been very successful to solve problems that involve long-term dependencies, including in the RL domain. However, the underlying reason for the strong performance of Transformer-based RL methods remains unclear: is it because they learn effective memory, or because they perform effective credit assignment? After introducing formal definitions of memory length and credit assignment length, we design simple configurable tasks to measure these distinct quantities. Our empirical results reveal that Transformers can enhance the memory capacity of RL algorithms, scaling up to tasks that require memorizing observations $1500$ steps ago. However, Transformers do not improve long-term credit assignment. In summary, our
    
[^109]: 在不平衡数据集中的离线强化学习

    Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2307.02752v1 [cs.LG])

    [http://arxiv.org/abs/2307.02752](http://arxiv.org/abs/2307.02752)

    本文提出了一种在不平衡数据集中的新型离线强化学习方法，通过将CQL与回溯过程相结合来提取策略，从而有效地解决了不平衡数据集带来的挑战。

    

    当前离线强化学习（RL）研究中对基准的普遍使用导致了对实际数据集分布不平衡的忽视。由于探索或安全考虑的挑战，实际离线RL数据集在状态空间上通常是不平衡的。我们在本文中具体说明了离线RL中不平衡数据集的特性，其中状态覆盖率遵循一个由偏态策略所特征化的幂律分布。理论上和实证上，我们证明了基于分布约束的典型离线RL方法，如保守Q学习（CQL），在不平衡数据集下提取策略是无效的。受自然智能的启发，我们提出了一种新的离线RL方法，该方法利用CQL的增强与回溯过程相结合，以回忆以往相关经验，有效地缓解不平衡数据集带来的挑战。我们在多个任务上评估了我们的方法。

    The prevalent use of benchmarks in current offline reinforcement learning (RL) research has led to a neglect of the imbalance of real-world dataset distributions in the development of models. The real-world offline RL dataset is often imbalanced over the state space due to the challenge of exploration or safety considerations. In this paper, we specify properties of imbalanced datasets in offline RL, where the state coverage follows a power law distribution characterized by skewed policies. Theoretically and empirically, we show that typically offline RL methods based on distributional constraints, such as conservative Q-learning (CQL), are ineffective in extracting policies under the imbalanced dataset. Inspired by natural intelligence, we propose a novel offline RL method that utilizes the augmentation of CQL with a retrieval process to recall past related experiences, effectively alleviating the challenges posed by imbalanced datasets. We evaluate our method on several tasks in the 
    
[^110]: 深度神经网络如何学习组合性数据：随机层次模型

    How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model. (arXiv:2307.02129v1 [cs.LG])

    [http://arxiv.org/abs/2307.02129](http://arxiv.org/abs/2307.02129)

    本文研究了深度神经网络学习组合性数据的问题，通过对随机层次模型进行分类任务，发现深度CNN学习这个任务所需的训练数据数量随着类别数、组合数和迭代次数的增加而渐进增加。

    

    学习一般高维任务是非常困难的，因为它需要与维度成指数增长的训练数据数量。然而，深度卷积神经网络（CNN）在克服这一挑战方面显示出了卓越的成功。一种普遍的假设是可学习任务具有高度结构化，CNN利用这种结构建立了数据的低维表示。然而，我们对它们需要多少训练数据以及这个数字如何取决于数据结构知之甚少。本文回答了针对一个简单的分类任务的这个问题，该任务旨在捕捉真实数据的相关方面：随机层次模型。在这个模型中，$n_c$个类别中的每一个对应于$m$个同义组合的高层次特征，并且这些特征又通过一个重复$L$次的迭代过程由子特征组成。我们发现，需要深度CNN学习这个任务的训练数据数量$P^*$（i）随着$n_c m^L$的增长而渐进地增长，这只有...

    Learning generic high-dimensional tasks is notably hard, as it requires a number of training data exponential in the dimension. Yet, deep convolutional neural networks (CNNs) have shown remarkable success in overcoming this challenge. A popular hypothesis is that learnable tasks are highly structured and that CNNs leverage this structure to build a low-dimensional representation of the data. However, little is known about how much training data they require, and how this number depends on the data structure. This paper answers this question for a simple classification task that seeks to capture relevant aspects of real data: the Random Hierarchy Model. In this model, each of the $n_c$ classes corresponds to $m$ synonymic compositions of high-level features, which are in turn composed of sub-features through an iterative process repeated $L$ times. We find that the number of training data $P^*$ required by deep CNNs to learn this task (i) grows asymptotically as $n_c m^L$, which is only
    
[^111]: 大规模无监督音频预训练用于视频到语音合成

    Large-scale unsupervised audio pre-training for video-to-speech synthesis. (arXiv:2306.15464v1 [cs.SD])

    [http://arxiv.org/abs/2306.15464](http://arxiv.org/abs/2306.15464)

    本文提出了一种利用大规模无监督音频预训练的方法，用于视频到语音合成。通过训练编码器-解码器模型，我们可以在不需要视频对应的情况下，使用丰富的仅音频数据集进行合成。

    

    视频到语音合成是从无声视频中重建语音信号的任务。目前大多数已建立的方法都采用了一个两步法，首先从视频中提取中间表示，如谱图，然后传递给声码器生成原始音频。最近的一些工作专注于端到端合成，即同时生成原始音频和任何中间表示。所有这些方法都需要在几乎完全是音频-视觉数据集上进行训练，即每个音频样本都有对应的视频样本。这排除了使用丰富的仅音频数据集的可能性，这些数据集可能没有对应的视觉模态（例如有声读物、广播播客、语音识别数据集等），以及多年来由音频机器学习社区开发的仅音频架构。在本文中，我们建议在超过3500小时的数据上训练编码器-解码器模型。

    Video-to-speech synthesis is the task of reconstructing the speech signal from a silent video of a speaker. Most established approaches to date involve a two-step process, whereby an intermediate representation from the video, such as a spectrogram, is extracted first and then passed to a vocoder to produce the raw audio. Some recent work has focused on end-to-end synthesis, whereby the generation of raw audio and any intermediate representations is performed jointly. All such approaches involve training on data from almost exclusively audio-visual datasets, i.e. every audio sample has a corresponding video sample. This precludes the use of abundant audio-only datasets which may not have a corresponding visual modality (e.g. audiobooks, radio podcasts, speech recognition datasets etc.), as well as audio-only architectures that have been developed by the audio machine learning community over the years. In this paper we propose to train encoder-decoder models on more than 3,500 hours of 
    
[^112]: G-NM：一组数字时间序列预测模型

    G-NM: A Group of Numerical Time Series Prediction Models. (arXiv:2306.11667v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11667](http://arxiv.org/abs/2306.11667)

    G-NM是一组集合了传统和现代模型的数字时间序列预测模型，旨在提高对复杂自然现象中的模式和趋势的预测能力。

    

    本研究聚焦于开发和实施一个综合的数字时间序列预测模型集合，统称为数字时间序列预测模型组（G-NM）。该集合包括传统模型如自回归综合移动平均（ARIMA）、Holt-Winters方法和支持向量回归（SVR），以及现代神经网络模型，如循环神经网络（RNN）和长短期记忆（LSTM）。G-NM明确构建以增强我们对复杂自然现象中固有模式和趋势的预测能力。通过利用与这些事件相关的时间序列数据，G-NM便于对此类现象在延长时间段内进行预测。本研究的主要目标是推进我们对此类事件的理解，并大幅提高预测准确性。G-NM包括线性和非线性依赖关系，以及季节性趋势。

    In this study, we focus on the development and implementation of a comprehensive ensemble of numerical time series forecasting models, collectively referred to as the Group of Numerical Time Series Prediction Model (G-NM). This inclusive set comprises traditional models such as Autoregressive Integrated Moving Average (ARIMA), Holt-Winters' method, and Support Vector Regression (SVR), in addition to modern neural network models including Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM). G-NM is explicitly constructed to augment our predictive capabilities related to patterns and trends inherent in complex natural phenomena. By utilizing time series data relevant to these events, G-NM facilitates the prediction of such phenomena over extended periods. The primary objective of this research is to both advance our understanding of such occurrences and to significantly enhance the accuracy of our forecasts. G-NM encapsulates both linear and non-linear dependencies, seasonal
    
[^113]: 屏蔽交叉验证：一种准确高效的超参数调整方法

    Blocked Cross-Validation: A Precise and Efficient Method for Hyperparameter Tuning. (arXiv:2306.06591v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06591](http://arxiv.org/abs/2306.06591)

    屏蔽交叉验证（BCV）是一种准确高效的超参数调整方法，相比于传统的重复交叉验证（RCV），BCV可以提供更准确的误差估计结果，且运行次数更少。使用真实数据集的实验结果表明，BCV在超参数调优任务中优于RCV，能够以更少的计算实现更高的精度。

    

    超参数调整在优化预测学习器的性能中起着关键作用。交叉验证（CV）是一种广泛采用的技术，用于估计不同超参数设置的误差。重复交叉验证（RCV）常用于减少CV误差的变化性。在本文中，我们引入了一种名为屏蔽交叉验证（BCV）的新方法，其中重复次数相对于CV分区和学习器的随机行为进行了阻塞。理论分析和实证实验证明，与RCV相比，BCV可以提供更准确的误差估计结果，即使运行次数显著减少。我们使用真实数据集提供了大量示例，展示了BCV在超参数调优中的有效性和效率。我们的结果表明，BCV在超参数调优中优于RCV，可以以更少的计算实现更高的精度。

    Hyperparameter tuning plays a crucial role in optimizing the performance of predictive learners. Cross--validation (CV) is a widely adopted technique for estimating the error of different hyperparameter settings. Repeated cross-validation (RCV) has been commonly employed to reduce the variability of CV errors. In this paper, we introduce a novel approach called blocked cross-validation (BCV), where the repetitions are blocked with respect to both CV partition and the random behavior of the learner. Theoretical analysis and empirical experiments demonstrate that BCV provides more precise error estimates compared to RCV, even with a significantly reduced number of runs. We present extensive examples using real--world data sets to showcase the effectiveness and efficiency of BCV in hyperparameter tuning. Our results indicate that BCV outperforms RCV in hyperparameter tuning, achieving greater precision with fewer computations.
    
[^114]: 利用草图技术估计Koopman算子并可靠地学习大规模动态系统

    Estimating Koopman operators with sketching to provably learn large scale dynamical systems. (arXiv:2306.04520v1 [stat.ML])

    [http://arxiv.org/abs/2306.04520](http://arxiv.org/abs/2306.04520)

    本文提出利用随机投影技术优化了Koopman算子的估计器，加快了计算速度，并给出了精确的误差界限，提高了算法的可靠性。

    

    Koopman算子理论允许使用非参数机器学习算法来预测和分析复杂的动态系统。本文提出利用随机投影（草图技术）提高基于核的Koopman算子估计器的计算效率。我们在合成和大规模分子动力学数据集上进行了广泛实验，并建立了非渐进误差界，给出了统计学习速率和计算效率之间的权衡的精确刻画。我们的经验和理论分析表明，经过改进的估计器在保证准确性的同时大大提高了计算效率。

    The theory of Koopman operators allows to deploy non-parametric machine learning algorithms to predict and analyze complex dynamical systems. Estimators such as principal component regression (PCR) or reduced rank regression (RRR) in kernel spaces can be shown to provably learn Koopman operators from finite empirical observations of the system's time evolution. Scaling these approaches to very long trajectories is a challenge and requires introducing suitable approximations to make computations feasible. In this paper, we boost the efficiency of different kernel-based Koopman operator estimators using random projections (sketching). We derive, implement and test the new "sketched" estimators with extensive experiments on synthetic and large-scale molecular dynamics datasets. Further, we establish non asymptotic error bounds giving a sharp characterization of the trade-offs between statistical learning rates and computational efficiency. Our empirical and theoretical analysis shows that
    
[^115]: 自监督方法用于高维数据聚类评估

    A Self-Supervised Approach for Cluster Assessment of High-Dimensional Data. (arXiv:2306.00011v1 [cs.LG])

    [http://arxiv.org/abs/2306.00011](http://arxiv.org/abs/2306.00011)

    本文提出了一种利用自监督深度神经网络生成代表性嵌入的方法，用于评估复杂图像数据中的聚类结构。

    

    估计数据集中聚类数量和基础聚类结构是一个至关重要的任务。现实世界中的数据通常是无标签、复杂且高维的，这使得传统聚类算法难以表现良好。为解决这个问题，我们提出了一个基于深度学习的框架，用于评估复杂图像数据中的聚类结构。首先，我们的框架使用自监督深度神经网络生成复杂数据的代表性嵌入，然后将这些低维嵌入馈送到VAT/iVAT中。

    Estimating the number of clusters and underlying cluster structure in a dataset is a crucial task. Real-world data are often unlabeled, complex and high-dimensional, which makes it difficult for traditional clustering algorithms to perform well. In recent years, a matrix reordering based algorithm, called "visual assessment of tendency" (VAT), and its variants have attracted many researchers from various domains to estimate the number of clusters and inherent cluster structure present in the data. However, these algorithms fail when applied to high-dimensional data due to the curse of dimensionality, as they rely heavily on the notions of closeness and farness between data points. To address this issue, we propose a deep-learning based framework for cluster structure assessment in complex, image datasets. First, our framework generates representative embeddings for complex data using a self-supervised deep neural network, and then, these low-dimensional embeddings are fed to VAT/iVAT a
    
[^116]: 基于条件扩散模型的语义化三维医学图像合成

    Conditional Diffusion Models for Semantic 3D Medical Image Synthesis. (arXiv:2305.18453v1 [eess.IV])

    [http://arxiv.org/abs/2305.18453](http://arxiv.org/abs/2305.18453)

    这篇论文提出了Med-DDPM，一种使用扩散模型进行语义化三维医学图像合成的创新解决方案，它通过控制像素级掩码标签的生成过程，能够生成高质量逼真的医学图像，并且在精度、稳定性和多样性等指标上优于GAN技术，也优于传统的增强技术和GAN合成图像。

    

    本文提出了Med-DDPM，它是一种创新的解决方案，使用扩散模型进行语义化的三维医学图像合成，解决了医学成像中数据稀缺、采集方法不一致和隐私问题等普遍存在的问题。实验结果表明，扩散模型在稳定性和性能方面都超过了生成对抗网络（GAN），能够生成高质量、逼真的三维医学图像。Med-DDPM的独特特点在于使用语义条件进行三维图像合成的扩散模型。通过控制像素级掩码标签的生成过程，它便于创建逼真的医学图像。经验证明，Med-DDPM在精度、稳定性和多样性等指标上优于GAN技术。此外，Med-DDPM在增强分割模型的准确性方面也优于传统的增强技术和GAN合成图像。它解决了医学图像合成中的难点。

    This paper introduces Med-DDPM, an innovative solution using diffusion models for semantic 3D medical image synthesis, addressing the prevalent issues in medical imaging such as data scarcity, inconsistent acquisition methods, and privacy concerns. Experimental evidence illustrates that diffusion models surpass Generative Adversarial Networks (GANs) in stability and performance, generating high-quality, realistic 3D medical images. The distinct feature of Med-DDPM is its use of semantic conditioning for the diffusion model in 3D image synthesis. By controlling the generation process through pixel-level mask labels, it facilitates the creation of realistic medical images. Empirical evaluations underscore the superior performance of Med-DDPM over GAN techniques in metrics such as accuracy, stability, and versatility. Furthermore, Med-DDPM outperforms traditional augmentation techniques and synthetic GAN images in enhancing the accuracy of segmentation models. It addresses challenges such
    
[^117]: 语言模型是有限实用说话者

    Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17760](http://arxiv.org/abs/2305.17760)

    本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。经过人类反馈的强化学习微调的大型语言模型具有概念上类似于 快与慢思考模型的思维模型，而这种思维模型被归因于人类。此研究凸显了采用认知概率建模方法对语言模型的理解、评估和推进的价值。

    

    本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。特别地，我们展示了经过人类反馈的强化学习微调的大型语言模型（Ouyang等人，2022）具有概念上类似于 快与慢思考模型（Kahneman，2011）的思维模型，而这种思维模型被心理学家们归因于人类。我们讨论了从人类反馈中的强化学习作为快与慢思考模型的局限性，并提出了扩展这个框架的途径。本研究实质上凸显了采用认知概率建模方法来获得对语言模型的理解、评估和推进方面的深刻见解的价值。

    How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
    
[^118]: 基于表示的Jensen-Shannon散度

    The Representation Jensen-Shannon Divergence. (arXiv:2305.16446v1 [cs.LG])

    [http://arxiv.org/abs/2305.16446](http://arxiv.org/abs/2305.16446)

    本文提出了一种基于表示的新型散度——表示Jensen-Shannon散度，通过将数据分布嵌入到RKHS中，并利用表示的协方差算子的频谱，实现对数据分布的估计，并提供了具有灵活性，可扩展性，可微分性的经验协方差矩阵估计函数和基于核矩阵的估计函数。

    

    统计散度量化概率分布之间的差异，是机器学习中的一种重要方法。但是，由于数据的底层分布通常未知，从经验样本中估计散度是一个基本难题。本文提出了一种基于再生核希尔伯特空间(RKHS)中协方差算子的新型散度——表示Jensen-Shannon散度。我们的方法将数据分布嵌入到RKHS中，并利用表示的协方差算子的频谱。我们提供了一个从经验协方差矩阵估计的估计函数，它通过使用Fourier特征将数据映射到RKHS中。此估计函数是灵活、可扩展、可微分的，并且适用于小批量优化问题。此外，我们还提供了一种基于核矩阵的估计函数，而不需要对RKHS进行显式映射。我们证明这个量是Jensen-Shannon散度的一个下界。

    Statistical divergences quantify the difference between probability distributions finding multiple uses in machine-learning. However, a fundamental challenge is to estimate divergence from empirical samples since the underlying distributions of the data are usually unknown. In this work, we propose the representation Jensen-Shannon Divergence, a novel divergence based on covariance operators in reproducing kernel Hilbert spaces (RKHS). Our approach embeds the data distributions in an RKHS and exploits the spectrum of the covariance operators of the representations. We provide an estimator from empirical covariance matrices by explicitly mapping the data to an RKHS using Fourier features. This estimator is flexible, scalable, differentiable, and suitable for minibatch-based optimization problems. Additionally, we provide an estimator based on kernel matrices without having an explicit mapping to the RKHS. We show that this quantity is a lower bound on the Jensen-Shannon divergence, and 
    
[^119]: Mesh2SSM: 从表面网格到解剖学统计形态模型

    Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy. (arXiv:2305.07805v1 [cs.CV])

    [http://arxiv.org/abs/2305.07805](http://arxiv.org/abs/2305.07805)

    Mesh2SSM是一种基于无监督排列不变表示学习的方法，可以将模板点云变形为特定主体的网格，形成基于对应关系的解剖学统计形态模型。

    

    统计形态建模是从医学图像（如MRI和CT扫描）中捕获的分割解剖结构中发现显著形态参数的计算过程，可全面描述种群中特定主体的解剖学。由于人体解剖结构中存在大量的非线性变异，传统的形态建模过程常常面临挑战。深度学习技术可以学习到复杂的非线性形态表示，并生成更忠实于基础种群变异的统计形态模型。然而，现有的深度学习模型仍然存在局限性，需要已建立/优化的形态模型进行训练。我们提出了Mesh2SSM，一种新的方法，利用无监督、排列不变的表示学习来估计如何将模板点云变形为特定主体的网格，形成基于对应关系的形态模型。Mesh2SSM还可以学习种群特定的模板，从而减少了数据对齐的需要。

    Statistical shape modeling is the computational process of discovering significant shape parameters from segmented anatomies captured by medical images (such as MRI and CT scans), which can fully describe subject-specific anatomy in the context of a population. The presence of substantial non-linear variability in human anatomy often makes the traditional shape modeling process challenging. Deep learning techniques can learn complex non-linear representations of shapes and generate statistical shape models that are more faithful to the underlying population-level variability. However, existing deep learning models still have limitations and require established/optimized shape models for training. We propose Mesh2SSM, a new approach that leverages unsupervised, permutation-invariant representation learning to estimate how to deform a template point cloud to subject-specific meshes, forming a correspondence-based shape model. Mesh2SSM can also learn a population-specific template, reduci
    
[^120]: 可解释的平行RCNN与新颖特征表示法用于时间序列预测

    Explainable Parallel RCNN with Novel Feature Representation for Time Series Forecasting. (arXiv:2305.04876v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04876](http://arxiv.org/abs/2305.04876)

    本论文提出了一种新的特征表示策略——移位法，将过去数据和未来协变量融合起来进行时间序列预测，并开发了一个并行的深度学习框架，同时提出了一种可解释的方法来解释模型的特征重要性。

    

    准确的时间序列预测是数据科学中的基本挑战。往往会受到外部协变量，例如天气或人类干预的影响，这些影响可以在许多应用中被合理地预测。我们将它们称为预测的未来协变量。然而，现有的尝试使用自回归模型迭代预测时间序列的方法最终会导致指数级的误差累积。其他考虑用编码器和解码器分别处理历史和未来数据的策略会对自己产生限制。为了解决这些限制，我们提出了一种新的特征表示策略——移位法，将过去的数据和未来协变量融合起来以考虑它们的相互作用。为了提取时间序列中的复杂动力学，我们开发了一个并行的深度学习框架，由RNN和CNN组成，二者都被层级地使用。同时，我们还运用了跳跃连接技术来提高模型的训练效率和精度。最后，我们提出了一种可解释的方法来解释模型的特征重要性，为时间序列的底层过程提供了洞见。

    Accurate time series forecasting is a fundamental challenge in data science. It is often affected by external covariates such as weather or human intervention, which in many applications, may be predicted with reasonable accuracy. We refer to them as predicted future covariates. However, existing methods that attempt to predict time series in an iterative manner with autoregressive models end up with exponential error accumulations. Other strategies hat consider the past and future in the encoder and decoder respectively limit themselves by dealing with the historical and future data separately. To address these limitations, a novel feature representation strategy -- shifting -- is proposed to fuse the past data and future covariates such that their interactions can be considered. To extract complex dynamics in time series, we develop a parallel deep learning framework composed of RNN and CNN, both of which are used hierarchically. We also utilize the skip connection technique to impro
    
[^121]: 基于深度迁移学习的自动语音识别：迈向更好的泛化

    Deep Transfer Learning for Automatic Speech Recognition: Towards Better Generalization. (arXiv:2304.14535v1 [cs.SD])

    [http://arxiv.org/abs/2304.14535](http://arxiv.org/abs/2304.14535)

    本文Survey了基于DTL的ASR框架，并介绍了如何使用实际数据集进行深度迁移学习以达到更好的泛化性能。

    

    最近，深度学习在自动语音识别（ASR）方面面临着一个重要的挑战，这需要大规模的训练数据集和高计算和存储资源。此外，机器学习（ML）方法和深度学习技术通常假设训练和测试数据来自相同的域，具有相同的输入特征空间和数据分布特性。然而，在一些现实世界的人工智能（AI）应用中，这种假设是无法适用的。DTL被引入来克服这些问题，它有助于使用实际数据集开发高性能的模型，这些实际数据集即使很小或稍有不同，但与训练数据相关。本文提出了基于DTL的ASR框架的全面调查，以阐明最新的发展。

    Automatic speech recognition (ASR) has recently become an important challenge when using deep learning (DL). It requires large-scale training datasets and high computational and storage resources. Moreover, DL techniques and machine learning (ML) approaches in general, hypothesize that training and testing data come from the same domain, with the same input feature space and data distribution characteristics. This assumption, however, is not applicable in some real-world artificial intelligence (AI) applications. Moreover, there are situations where gathering real data is challenging, expensive, or rarely occurring, which can not meet the data requirements of DL models. deep transfer learning (DTL) has been introduced to overcome these issues, which helps develop high-performing models using real datasets that are small or slightly different but related to the training data. This paper presents a comprehensive survey of DTL-based ASR frameworks to shed light on the latest developments 
    
[^122]: 基于数据驱动的分段仿射决策规则用于带协变信息的随机规划

    Data-driven Piecewise Affine Decision Rules for Stochastic Programming with Covariate Information. (arXiv:2304.13646v1 [math.OC])

    [http://arxiv.org/abs/2304.13646](http://arxiv.org/abs/2304.13646)

    本研究提出一种嵌入非凸分段仿射决策规则的经验风险最小化方法，用于学习特征与最优决策之间的直接映射。所提出的方法可用于广泛的非凸型SP问题，并且在数值研究中表现出优越的性能。

    

    本文针对带协变信息的随机规划，提出了一种嵌入非凸分段仿射决策规则(PADR)的经验风险最小化(ERM)方法，旨在学习特征与最优决策之间的直接映射。我们建立了基于PADR的ERM模型的非渐近一致性结果，可用于无约束问题，以及约束问题的渐近一致性结果。为了解决非凸和非可微的ERM问题，我们开发了一个增强的随机主导下降算法，并建立了沿（复合强）方向稳定性的渐近收敛以及复杂性分析。我们表明，所提出的PADR-based ERM方法适用于广泛的非凸型SP问题，并具有理论一致性保证和计算可处理性。数值研究表明，在各种设置下，PADR-based ERM方法相对于最先进的方法具有优越的性能。

    Focusing on stochastic programming (SP) with covariate information, this paper proposes an empirical risk minimization (ERM) method embedded within a nonconvex piecewise affine decision rule (PADR), which aims to learn the direct mapping from features to optimal decisions. We establish the nonasymptotic consistency result of our PADR-based ERM model for unconstrained problems and asymptotic consistency result for constrained ones. To solve the nonconvex and nondifferentiable ERM problem, we develop an enhanced stochastic majorization-minimization algorithm and establish the asymptotic convergence to (composite strong) directional stationarity along with complexity analysis. We show that the proposed PADR-based ERM method applies to a broad class of nonconvex SP problems with theoretical consistency guarantees and computational tractability. Our numerical study demonstrates the superior performance of PADR-based ERM methods compared to state-of-the-art approaches under various settings,
    
[^123]: 自监督学习模型在基于传感器的人类活动识别中的表达解释、分析和探究

    Explaining, Analyzing, and Probing Representations of Self-Supervised Learning Models for Sensor-based Human Activity Recognition. (arXiv:2304.07304v1 [cs.LG])

    [http://arxiv.org/abs/2304.07304](http://arxiv.org/abs/2304.07304)

    本文探究了基于传感器的人类活动识别中自监督学习框架的深度表示，通过解释这些表示与有监督表示的区别，比较它们的鲁棒性，并使用显著图将其应用于预测不同的活动。

    

    近年来，自监督学习（SSL）框架被广泛应用于基于传感器的人类活动识别（HAR），以便学习深度表示而不需要数据注释。虽然SSL框架的性能几乎与监督模型相当，但对SSL模型学习的表示进行解释的研究却很有限。然而，现代解释方法可以帮助揭示SSL表示与监督表示之间的差异：它们是如何被学习的、它们保留了哪些输入数据属性，以及何时可以选择SSL进行训练。本文旨在分析两个最近的SSL框架SimCLR和VICReg的深度表示。具体而言，重点放在以下几个方面：(i) 比较监督和SSL模型对输入数据中的破坏的鲁棒性；(ii) 使用显著图解释深度学习模型的预测，并突出显示用于预测各种活动的主要输入通道。

    In recent years, self-supervised learning (SSL) frameworks have been extensively applied to sensor-based Human Activity Recognition (HAR) in order to learn deep representations without data annotations. While SSL frameworks reach performance almost comparable to supervised models, studies on interpreting representations learnt by SSL models are limited. Nevertheless, modern explainability methods could help to unravel the differences between SSL and supervised representations: how they are being learnt, what properties of input data they preserve, and when SSL can be chosen over supervised training. In this paper, we aim to analyze deep representations of two recent SSL frameworks, namely SimCLR and VICReg. Specifically, the emphasis is made on (i) comparing the robustness of supervised and SSL models to corruptions in input data; (ii) explaining predictions of deep learning models using saliency maps and highlighting what input channels are mostly used for predicting various activitie
    
[^124]: 计量再识别风险

    Measuring Re-identification Risk. (arXiv:2304.07210v1 [cs.CR])

    [http://arxiv.org/abs/2304.07210](http://arxiv.org/abs/2304.07210)

    该论文提出了一个用于测量用户表示中再识别风险的理论框架，该框架基于假设检验，可以限制攻击者从用户的表示中获取其身份的概率。作者展示了该框架应对实际应用的普适性，并补充了攻击算法来衡量现实应用的风险。

    

    紧凑的用户表示（例如嵌入）是个性化服务的支柱。在这项工作中，我们提出了一个新的理论框架，用于测量这种用户表示中的再识别风险。我们的框架基于假设检验，正式限制了攻击者能够从用户的表示中获取其身份的概率。作为应用，我们展示了我们的框架足够普遍以模拟重要的现实世界应用，例如 Chrome 的基于兴趣的广告的 Topics API。我们通过展示再识别攻击的合理好的算法来补充我们的理论界限，我们用这些算法来估计 Topics API 中的再识别风险。我们相信这项工作提供了一个严谨且可解释的再识别风险概念和测量它的框架，可以用来指导现实世界的应用。

    Compact user representations (such as embeddings) form the backbone of personalization services. In this work, we present a new theoretical framework to measure re-identification risk in such user representations. Our framework, based on hypothesis testing, formally bounds the probability that an attacker may be able to obtain the identity of a user from their representation. As an application, we show how our framework is general enough to model important real-world applications such as the Chrome's Topics API for interest-based advertising. We complement our theoretical bounds by showing provably good attack algorithms for re-identification that we use to estimate the re-identification risk in the Topics API. We believe this work provides a rigorous and interpretable notion of re-identification risk and a framework to measure it that can be used to inform real-world applications.
    
[^125]: SPDF：大规模语言模型的稀疏预训练和密集微调

    SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models. (arXiv:2303.10464v1 [cs.LG])

    [http://arxiv.org/abs/2303.10464](http://arxiv.org/abs/2303.10464)

    本文提出了SPDF算法来实现大规模语言模型的高效训练。通过非结构化权重稀疏性来进行预训练，可以降低计算成本，而密集微调则可以保证高性能的表现。

    

    预训练和微调范式为自然语言处理（NLP）的多项突破做出了贡献。语言模型首先在大型数据集上进行跨域知识的预训练（例如，Pile、MassiveText等），然后在特定任务的数据上进行微调（例如，自然语言生成、文本摘要等）。虽然扩大模型和数据集大小有助于提高LLM性能，但这也带来了极为禁止性的计算成本。预训练LLMs通常需要比微调演习更多的FLOPs，两个阶段之间的模型容量通常保持不变。为了实现相对于训练FLOPs的训练效率，我们建议在两个阶段之间解耦模型容量，并引入稀疏预训练和密集微调（SPDF）。在这项工作中，我们展示了使用非结构化权重稀疏性来仅训练子集权重的好处。

    The pre-training and fine-tuning paradigm has contributed to a number of breakthroughs in Natural Language Processing (NLP). Instead of directly training on a downstream task, language models are first pre-trained on large datasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then fine-tuned on task-specific data (e.g., natural language generation, text summarization, etc.). Scaling the model and dataset size has helped improve the performance of LLMs, but unfortunately, this also leads to highly prohibitive computational costs. Pre-training LLMs often require orders of magnitude more FLOPs than fine-tuning and the model capacity often remains the same between the two phases. To achieve training efficiency w.r.t training FLOPs, we propose to decouple the model capacity between the two phases and introduce Sparse Pre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits of using unstructured weight sparsity to train only a subset of weights during 
    
[^126]: ICICLE: 可解释的类增量连续学习方法

    ICICLE: Interpretable Class Incremental Continual Learning. (arXiv:2303.07811v1 [cs.LG])

    [http://arxiv.org/abs/2303.07811](http://arxiv.org/abs/2303.07811)

    ICICLE提出了一种基于样本的可解释的类增量连续学习方法，通过采用原型部分化方法来解决解释性概念漂移的问题，实验结果表明其在不需要样本的情况下表现优于现有的方法。

    

    连续学习能够增量学习新任务而不忘记之前学习的内容，从而促进新旧任务之间的正向知识转移。然而，连续学习对解释性提出了新的挑战，因为模型预测背后的原理可能会随着时间而改变，导致解释性概念漂移。本文通过提出基于样本的 Interpretable Class-InCremental LEarning (ICICLE) 方法，采用原型部分化方法，解决了这个问题。它包括三个关键的创新点：解释性正则化、以微粒粒度为基础的原型初始化策略以及针对原型部分的任务时效偏差补偿。我们的实验结果表明，ICICLE减少了解释性概念漂移，并且在不需要样本的情况下表现优于现有的方法。

    Continual learning enables incremental learning of new tasks without forgetting those previously learned, resulting in positive knowledge transfer that can enhance performance on both new and old tasks. However, continual learning poses new challenges for interpretability, as the rationale behind model predictions may change over time, leading to interpretability concept drift. We address this problem by proposing Interpretable Class-InCremental LEarning (ICICLE), an exemplar-free approach that adopts a prototypical part-based approach. It consists of three crucial novelties: interpretability regularization that distills previously learned concepts while preserving user-friendly positive reasoning; proximity-based prototype initialization strategy dedicated to the fine-grained setting; and task-recency bias compensation devoted to prototypical parts. Our experimental results demonstrate that ICICLE reduces the interpretability concept drift and outperforms the existing exemplar-free me
    
[^127]: 一招鲜：神经潜力场用于实体导航

    One-4-All: Neural Potential Fields for Embodied Navigation. (arXiv:2303.04011v2 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2303.04011](http://arxiv.org/abs/2303.04011)

    本文提出了一种新方法 One-4-All (O4A)，利用自我监督和流形学习，实现了一种无图形、端到端的导航程序。其通过贪婪最小化潜在空间内的潜力函数来进行导航，在真实世界的导航中具有重要应用价值。

    

    机器人的基本任务之一是在两个位置之间导航。特别地，真实世界的导航可能需要使用高维RGB图像进行长期规划，这对于端到端的学习方法构成了重大挑战。目前的半参数方法通过将学习模块与环境的拓扑记忆相结合来实现长期规划，通常使用以先前收集的图像为基础的图形表示。然而，实际上使用这些图表通常需要调整大量的修剪启发式方法，以避免虚假的边缘，限制运行时内存使用并允许相当快速的图形查询。在这项工作中，我们提出了One-4-All（O4A），一种利用自我监督和流形学习来获得无图形、端到端导航管道的方法，其中目标被指定为一幅图像。导航通过贪婪地最小化连续定义在O4A潜在空间上的潜力函数来实现。我们的系统是离线训练的。

    A fundamental task in robotics is to navigate between two locations. In particular, real-world navigation can require long-horizon planning using high-dimensional RGB images, which poses a substantial challenge for end-to-end learning-based approaches. Current semi-parametric methods instead achieve long-horizon navigation by combining learned modules with a topological memory of the environment, often represented as a graph over previously collected images. However, using these graphs in practice typically involves tuning a number of pruning heuristics to avoid spurious edges, limit runtime memory usage and allow reasonably fast graph queries. In this work, we present One-4-All (O4A), a method leveraging self-supervised and manifold learning to obtain a graph-free, end-to-end navigation pipeline in which the goal is specified as an image. Navigation is achieved by greedily minimizing a potential function defined continuously over the O4A latent space. Our system is trained offline on 
    
[^128]: TopSpark:一种用于自主移动机器人上的节能脉冲神经网络的时间步长优化方法

    TopSpark: A Timestep Optimization Methodology for Energy-Efficient Spiking Neural Networks on Autonomous Mobile Agents. (arXiv:2303.01826v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2303.01826](http://arxiv.org/abs/2303.01826)

    TopSpark提出了一种时间步长优化方法，用于提高自主移动机器人上脉冲神经网络的能量效率。该方法消除了固定时间步长的限制，使得SNNs能够在训练和推断阶段都实现更高的能量效率，并且能够在运行时进行高效的在线学习。

    

    自主移动机器人需要低功耗/高效能的机器学习算法来完成基于机器学习的任务，并能够适应多样化的环境，因为移动机器人通常由电池供电。脉冲神经网络（SNNs）可以实现这些要求，因为它们通过稀疏计算和具有生物启发式学习机制的高效在线学习，提供低功耗/高效能的处理。最近的研究表明，通过减少每个神经元处理一个脉冲序列（时间步长）的计算时间，可以优化SNNs的能量消耗。然而，现有技术依赖于大量的设计搜索来确定只能进行推断的固定时间步长设置，从而阻碍了SNNs在训练和推断阶段实现进一步的能量效率提高。这些技术还限制了SNNs在运行时进行高效的在线学习。为了解决这个问题，我们提出了TopSpark方法

    Autonomous mobile agents require low-power/energy-efficient machine learning (ML) algorithms to complete their ML-based tasks while adapting to diverse environments, as mobile agents are usually powered by batteries. These requirements can be fulfilled by Spiking Neural Networks (SNNs) as they offer low power/energy processing due to their sparse computations and efficient online learning with bio-inspired learning mechanisms for adapting to different environments. Recent works studied that the energy consumption of SNNs can be optimized by reducing the computation time of each neuron for processing a sequence of spikes (timestep). However, state-of-the-art techniques rely on intensive design searches to determine fixed timestep settings for only inference, thereby hindering the SNNs from achieving further energy efficiency gains in both training and inference. These techniques also restrict the SNNs from performing efficient online learning at run time. Toward this, we propose TopSpar
    
[^129]: 以ELBOs的加权积分理解扩散目标

    Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00848](http://arxiv.org/abs/2303.00848)

    本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。

    

    文献中的扩散模型采用不同的目标进行优化，并且这些目标都是加权损失的特例，其中加权函数指定每个噪声级别的权重。均匀加权对应于最大似然的原则性近似ELBO的最大化。但是实际上，由于更好的样本质量，目前的扩散模型使用非均匀加权。本文揭示了加权损失（带有任何加权）和ELBO目标之间的直接关系。我们展示了加权损失可以被写成一种ELBOs的加权积分形式，其中每个噪声级别都有一个ELBO。如果权重函数是单调的，那么加权损失是一种基于似然的目标：它在简单的数据增强下（即高斯噪声扰动）下最大化ELBO。我们的主要贡献是更深入地理解了扩散目标，但我们还进行了一些比较单调和非单调权重的实验。

    Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
    
[^130]: 无监督病理检测：深入探究现有技术的最新进展

    Unsupervised Pathology Detection: A Deep Dive Into the State of the Art. (arXiv:2303.00609v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.00609](http://arxiv.org/abs/2303.00609)

    本文深入研究了无监督病理检测技术的最新发展，通过评估和基准测试多种尖端方法，证明了工业和医疗文献中新开发的特征建模方法在各种模态和数据集上创立了新的技术水平。

    

    深度无监督方法正在越来越多地被用于医学图像中的病理检测和分割应用，因为它们承诺可以减轻对大规模标注数据集的需求，并且在检测任何一种罕见病理方面比有监督方法更具普适性。随着无监督异常检测 (UAD) 文献的不断增长和新的范式的出现，重要的是要在一个公共框架中不断地评估和基准测试新的方法，以重新评估最新技术进展，并确定有前途的研究方向。为此，我们在多个医学数据集上评估了多种尖端 UAD 方法，并将其与已在脑 MRI 的 UAD 中确立的最新技术进展进行比较。我们的实验表明，工业和医疗文献中新开发的特征建模方法相对于之前的工作实现了更高的性能，并在各种模态和数据集上创立了新的技术水平。

    Deep unsupervised approaches are gathering increased attention for applications such as pathology detection and segmentation in medical images since they promise to alleviate the need for large labeled datasets and are more generalizable than their supervised counterparts in detecting any kind of rare pathology. As the Unsupervised Anomaly Detection (UAD) literature continuously grows and new paradigms emerge, it is vital to continuously evaluate and benchmark new methods in a common framework, in order to reassess the state-of-the-art (SOTA) and identify promising research directions. To this end, we evaluate a diverse selection of cutting-edge UAD methods on multiple medical datasets, comparing them against the established SOTA in UAD for brain MRI. Our experiments demonstrate that newly developed feature-modeling methods from the industrial and medical literature achieve increased performance compared to previous work and set the new SOTA in a variety of modalities and datasets. Add
    
[^131]: 异构视觉深度网络社区中的指代性沟通

    Referential communication in heterogeneous communities of pre-trained visual deep networks. (arXiv:2302.08913v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.08913](http://arxiv.org/abs/2302.08913)

    异构视觉深度网络社区中的预训练网络可以自我监督地开发出共享协议，以指代一组目标中的目标对象，并可用于沟通不同粒度的未知对象类别。

    

    随着大型预训练图像处理神经网络被嵌入自动驾驶汽车或机器人等自主代理中，一个问题出现了：在它们具有不同架构和训练方式的情况下，这些系统如何相互之间进行沟通以了解周围的世界。作为朝着这个方向的第一步，我们系统地探索了在一组异构最先进的预训练视觉网络社区中进行"指代性沟通"的任务，结果表明它们可以自我监督地发展一种共享协议来指代一组候选目标中的目标对象。在某种程度上，这种共享协议也可以用来沟通不同粒度的先前未见过的对象类别。此外，一个最初不属于现有社区的视觉网络可以轻松地学习到社区的协议。最后，我们定性和定量地研究了这种新产生的协议的属性，提供了一些证据。

    As large pre-trained image-processing neural networks are being embedded in autonomous agents such as self-driving cars or robots, the question arises of how such systems can communicate with each other about the surrounding world, despite their different architectures and training regimes. As a first step in this direction, we systematically explore the task of \textit{referential communication} in a community of heterogeneous state-of-the-art pre-trained visual networks, showing that they can develop, in a self-supervised way, a shared protocol to refer to a target object among a set of candidates. This shared protocol can also be used, to some extent, to communicate about previously unseen object categories of different granularity. Moreover, a visual network that was not initially part of an existing community can learn the community's protocol with remarkable ease. Finally, we study, both qualitatively and quantitatively, the properties of the emergent protocol, providing some evi
    
[^132]: CECT：可控的卷积神经网络和Transformer用于COVID-19图像分类

    CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image Classification. (arXiv:2302.02314v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.02314](http://arxiv.org/abs/2302.02314)

    本研究提出了一种新的分类网络CECT，利用可控的卷积神经网络和Transformer进行组合，能同时捕捉多个局部和全局尺度上的特征。在两个公共COVID-19数据集上评估后表现优于现有的最先进方法。这一新方法在医学图像分类领域中有广泛的应用前景。

    

    大多数计算机视觉模型基于卷积神经网络（CNN）或Transformer开发，前者（后者）可以捕捉局部（全局）特征。为了减轻模型性能受局部（全局）特征缺乏的限制，我们开发了一种新的分类网络CECT，通过可控的卷积神经网络和Transformer进行组合。CECT由卷积编码块、转置卷积解码块和Transformer分类块组成。不同于传统的基于CNN或Transformer的方法，我们的CECT可以同时捕捉多个局部和全局尺度上的特征。此外，本文提出的集合系数可以控制不同尺度局部特征的贡献。我们在两个公共COVID-19数据集上评估了CECT，并且在所有评估指标上均优于现有的最先进方法。由于其卓越的特征捕捉能力，我们相信CECT可以作为一种通用而有效的工具扩展到其他医学图像分类场景中。

    Most computer vision models are developed based on either convolutional neural network (CNN) or transformer, while the former (latter) method captures local (global) features. To relieve model performance limitations due to the lack of global (local) features, we develop a novel classification network CECT by controllable ensemble CNN and transformer. CECT is composed of a convolutional encoder block, a transposed-convolutional decoder block, and a transformer classification block. Different from conventional CNN- or transformer-based methods, our CECT can capture features at both multi-local and global scales. Besides, the contribution of local features at different scales can be controlled with the proposed ensemble coefficients. We evaluate CECT on two public COVID-19 datasets and it outperforms existing state-of-the-art methods on all evaluation metrics. With remarkable feature capture ability, we believe CECT can be extended to other medical image classification scenarios as a dia
    
[^133]: 连续时空转换器

    Continuous Spatiotemporal Transformers. (arXiv:2301.13338v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13338](http://arxiv.org/abs/2301.13338)

    连续时空转换器（CST）是一种新的转换器架构，通过在Sobolev空间中进行优化，能够建模连续系统并保证连续平滑的输出。在多个任务上，包括学习脑动力学，CST表现出卓越的性能。

    

    在机器学习中，建模时空动态系统是一个重要的挑战。Transformer模型在自然语言处理和计算机视觉领域取得了很大的成功，能够提供可解释的数据表示。然而，在建模连续动态系统时，传统的Transformer模型存在一个限制，即它们是基于离散时间和空间的模型，因此无法保证连续采样。为了解决这个挑战，我们提出了一种新的连续时空转换器（CST）架构，专门用于建模连续系统。这种新的框架通过在Sobolev空间中进行优化，保证了连续平滑的输出。我们将CST与传统的Transformer模型以及其他时空动态建模方法进行了对比实验，结果表明在多个合成和真实系统的任务中，包括从钙成像数据中学习脑动力学，CST取得了卓越的性能。

    Modeling spatiotemporal dynamical systems is a fundamental challenge in machine learning. Transformer models have been very successful in NLP and computer vision where they provide interpretable representations of data. However, a limitation of transformers in modeling continuous dynamical systems is that they are fundamentally discrete time and space models and thus have no guarantees regarding continuous sampling. To address this challenge, we present the Continuous Spatiotemporal Transformer (CST), a new transformer architecture that is designed for the modeling of continuous systems. This new framework guarantees a continuous and smooth output via optimization in Sobolev space. We benchmark CST against traditional transformers as well as other spatiotemporal dynamics modeling methods and achieve superior performance in a number of tasks on synthetic and real systems, including learning brain dynamics from calcium imaging data.
    
[^134]: 生成支持线的多维聚类

    Generating Multidimensional Clusters With Support Lines. (arXiv:2301.10327v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10327](http://arxiv.org/abs/2301.10327)

    提出了一个名为Clugen的模块化合成数据生成过程，能够使用任意分布创建支持线段的多维聚类，适用于评估聚类算法并有潜力成为广泛使用的框架。

    

    合成数据对于评估聚类技术、补充和扩展真实数据以及允许更完整地覆盖给定问题空间至关重要。而合成数据生成器具有创建大量数据的潜力，这在真实世界数据稀缺时至关重要，同时提供了一个被充分理解的生成过程和一个可解释的工具，以系统地研究聚类分析算法。在这里，我们提出了一个名为Clugen的模块化合成数据生成过程，能够使用任意分布创建支持线段的多维聚类。Clugen是开源的，有全面的单元测试和文档，并且适用于Python、R、Julia和MATLAB/Octave生态系统。我们证明了我们的方法可以在各个维度上产生丰富多样的结果，适合用于评估聚类算法，并有潜力成为各个领域中广泛使用的框架。

    Synthetic data is essential for assessing clustering techniques, complementing and extending real data, and allowing for more complete coverage of a given problem's space. In turn, synthetic data generators have the potential of creating vast amounts of data -- a crucial activity when real-world data is at premium -- while providing a well-understood generation procedure and an interpretable instrument for methodically investigating cluster analysis algorithms. Here, we present Clugen, a modular procedure for synthetic data generation, capable of creating multidimensional clusters supported by line segments using arbitrary distributions. Clugen is open source, comprehensively unit tested and documented, and is available for the Python, R, Julia, and MATLAB/Octave ecosystems. We demonstrate that our proposal can produce rich and varied results in various dimensions, is fit for use in the assessment of clustering algorithms, and has the potential to be a widely used framework in diverse 
    
[^135]: 设计数据：针对机器学习的主动数据收集和迭代

    Designing Data: Proactive Data Collection and Iteration for Machine Learning. (arXiv:2301.10319v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2301.10319](http://arxiv.org/abs/2301.10319)

    这篇论文介绍了一种设计数据的迭代方法，将人机交互概念与机器学习技术相结合，以解决机器学习应用中数据收集缺乏多样性导致的失败问题。通过预收集计划、收集监控和数据熟悉度等步骤，该方法可以提高模型的泛化能力并在跨组交叉群体上取得更好的结果。

    

    数据收集的缺乏多样性导致机器学习应用中的严重失败。虽然机器学习开发人员可以进行数据后处理，但这需要耗费大量时间并很少全面。因此，需要新的方法来跟踪和管理数据收集、迭代和模型训练，以评估数据集是否反映了真实世界的变异。我们提出了设计数据的迭代方法，将人机交互概念与机器学习技术相结合。我们的过程包括（1）预收集计划，主动促使并记录预期的数据分布；（2）收集监控，系统性地鼓励采样多样性；（3）数据熟悉度，使用密度估计方法识别模型对其不熟悉的样本。我们将设计数据应用于数据收集和建模任务。我们发现在跨组交叉群体训练的模型上，“设计”数据集的泛化能力优于大小相似但针对性较低的数据集的模型。

    Lack of diversity in data collection has caused significant failures in machine learning (ML) applications. While ML developers perform post-collection interventions, these are time intensive and rarely comprehensive. Thus, new methods to track & manage data collection, iteration, and model training are necessary for evaluating whether datasets reflect real world variability. We present designing data, an iterative approach to data collection connecting HCI concepts with ML techniques. Our process includes (1) Pre-Collection Planning, to reflexively prompt and document expected data distributions; (2) Collection Monitoring, to systematically encourage sampling diversity; and (3) Data Familiarity, to identify samples that are unfamiliar to a model using density estimation. We apply designing data to a data collection and modeling task. We find models trained on ''designed'' datasets generalize better across intersectional groups than those trained on similarly sized but less targeted da
    
[^136]: SpArX: 稀疏的神经网络论证解释

    SpArX: Sparse Argumentative Explanations for Neural Networks. (arXiv:2301.09559v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.09559](http://arxiv.org/abs/2301.09559)

    该论文提出了一种稀疏的神经网络论证解释方法SpArX，通过利用多层感知器和定量论证框架之间的关系，可以为神经网络的决策过程提供更忠实和深入的解释。

    

    神经网络在人工智能中有各种应用，但解释它们的决策仍然具有挑战性。现有方法通常关注解释改变单个输入如何影响神经网络的输出。然而，一个与神经网络的输入输出行为一致的解释未必忠实于其实际机制。在本文中，我们利用多层感知器和定量论证框架之间的关系，为多层感知器的机制创建了论证性解释。我们的SpArX方法首先将多层感知器稀疏化，同时保持尽可能多的原始结构。然后将稀疏的多层感知器转化为等效的定量论证框架，以揭示多层感知器的潜在决策过程，产生全局和/或局部解释。我们通过实验证明，SpArX比现有方法可以给出更忠实的解释，同时提供更深入的洞察实际推理过程。

    Neural networks (NNs) have various applications in AI, but explaining their decisions remains challenging. Existing approaches often focus on explaining how changing individual inputs affects NNs' outputs. However, an explanation that is consistent with the input-output behaviour of an NN is not necessarily faithful to the actual mechanics thereof. In this paper, we exploit relationships between multi-layer perceptrons (MLPs) and quantitative argumentation frameworks (QAFs) to create argumentative explanations for the mechanics of MLPs. Our SpArX method first sparsifies the MLP while maintaining as much of the original structure as possible. It then translates the sparse MLP into an equivalent QAF to shed light on the underlying decision process of the MLP, producing global and/or local explanations. We demonstrate experimentally that SpArX can give more faithful explanations than existing approaches, while simultaneously providing deeper insights into the actual reasoning process of M
    
[^137]: 嵌入式系统实时语义分割中的不确定性

    Uncertainty in Real-Time Semantic Segmentation on Embedded Systems. (arXiv:2301.01201v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.01201](http://arxiv.org/abs/2301.01201)

    本文提出了一种结合贝叶斯回归和动量传播的预测方法，能够实时在嵌入式硬件上产生有意义的不确定性，从而使语义分割模型在自动驾驶和人机交互等领域的实时应用成为可能。

    

    语义分割模型在自动驾驶和人机交互等领域的应用需要实时预测能力。实时应用的挑战被资源受限的硬件所加剧。虽然这些平台上实时方法的开发得到了增加，但这些模型无法足够地考虑到存在的不确定性。本文通过将预先训练模型的深层特征提取与贝叶斯回归和动量传播相结合，提出了一种关注不确定性的预测方法。我们演示了该方法如何在嵌入式硬件上实时产生有意义的不确定性，同时保持预测性能。

    Application for semantic segmentation models in areas such as autonomous vehicles and human computer interaction require real-time predictive capabilities. The challenges of addressing real-time application is amplified by the need to operate on resource constrained hardware. Whilst development of real-time methods for these platforms has increased, these models are unable to sufficiently reason about uncertainty present. This paper addresses this by combining deep feature extraction from pre-trained models with Bayesian regression and moment propagation for uncertainty aware predictions. We demonstrate how the proposed method can yield meaningful uncertainty on embedded hardware in real-time whilst maintaining predictive performance.
    
[^138]: 带有神经网络和索引的聚类模型

    Clustering with Neural Network and Index. (arXiv:2212.03853v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03853](http://arxiv.org/abs/2212.03853)

    介绍了一种新的带有神经网络和索引的聚类模型CNNI，该模型使用神经网络对数据点进行聚类，实现了第一个能够处理非凸形状数据的参数化聚类模型。

    

    介绍一种新的聚类模型，称为带有神经网络和索引的聚类模型（CNNI）。CNNI使用神经网络对数据点进行聚类。神经网络的训练模仿监督学习，使用内部聚类评估指标作为损失函数。进行了一项实验来测试新模型的可行性，并与K均值和高斯混合模型（GMM）等其他聚类模型的结果进行了比较。结果表明CNNI可以正确地对数据进行聚类；CNNI配备了MMJ-SC，成为第一个能够处理非凸形状（非平面几何）数据的参数化（归纳式）聚类模型。

    A new model called Clustering with Neural Network and Index (CNNI) is introduced. CNNI uses a Neural Network to cluster data points. Training of the Neural Network mimics supervised learning, with an internal clustering evaluation index acting as the loss function. An experiment is conducted to test the feasibility of the new model, and compared with results of other clustering models like K-means and Gaussian Mixture Model (GMM). The result shows CNNI can work properly for clustering data; CNNI equipped with MMJ-SC, achieves the first parametric (inductive) clustering model that can deal with non-convex shaped (non-flat geometry) data.
    
[^139]: 欧洲人工智能责任指令——对半心态方法的批评和未来经验教训的研究

    The European AI Liability Directives -- Critique of a Half-Hearted Approach and Lessons for the Future. (arXiv:2211.13960v6 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2211.13960](http://arxiv.org/abs/2211.13960)

    这篇论文对欧洲AI责任指令的提案进行了详细研究，发现其代表了一个半心态的方法，并提出了三个创新贡献。

    

    随着ChatGPT等人工智能系统征服世界，全球AI系统的最佳责任框架仍然是一个未解决的问题。2022年9月，欧洲委员会提出了两项关于AI责任的提案，概述了欧洲的方法：一项新的AI责任指令和对产品责任指令的修订。它们构成了欧盟AI监管的最终基石。关键是，责任提案和欧盟AI法案是相互联系的：后者不包含任何受影响人员的个人权利，前者缺乏关于AI开发和部署的具体、实质性规则。综合起来，这些法案可能会在AI监管领域引发布鲁塞尔效应，对美国和其他国家产生重大影响。本文提出了三个新的贡献。首先，它详细研究了委员会的提案，并表明，虽然朝着正确方向迈出了步伐，但最终代表着半心态的方法：如果通过改正方案，

    As ChatGPT et al. conquer the world, the optimal liability framework for AI systems remains an unsolved problem across the globe. In a much-anticipated move, the European Commission advanced two proposals outlining the European approach to AI liability in September 2022: a novel AI Liability Directive and a revision of the Product Liability Directive. They constitute the final cornerstone of EU AI regulation. Crucially, the liability proposals and the EU AI Act are inherently intertwined: the latter does not contain any individual rights of affected persons, and the former lack specific, substantive rules on AI development and deployment. Taken together, these acts may well trigger a Brussels Effect in AI regulation, with significant consequences for the US and beyond.  This paper makes three novel contributions. First, it examines in detail the Commission proposals and shows that, while making steps in the right direction, they ultimately represent a half-hearted approach: if enacted 
    
[^140]: 针对声学对抗性机器学习逃避的实时语音情感检测的隐私保护

    Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning. (arXiv:2211.09273v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09273](http://arxiv.org/abs/2211.09273)

    本研究通过对抗机器学习的方法，提出了一种名为DARE-GP的解决方案，可以在不影响智能音箱效用的情况下，逃避与智能音箱相连的语音情感识别分类器，从而保护隐私。

    

    情感监视是一个新兴领域，涉及到广泛的隐私问题。这些问题在普遍存在的物联网设备和多传感器的支持下，变得更加严重，而这些设备可以支持这些监视用例。本文考虑了一个这样的应用案例：使用与智能音箱相连的语音情感识别（SER）分类器。本文展示了如何在不影响智能音箱的效用的情况下，逃避与智能音箱相连的黑盒SER分类器。通过对机器学习的对抗逃避的视角来考虑这个隐私问题。我们的解决方案名为“通过遗传规划击败声学情感识别的DARE-GP”，它使用遗传规划生成非侵入性的添加音频扰动（AAPs）。通过约束这些AAP的进化，可以保护转录准确性，同时降低SER分类器的性能。这些AAP的加性特性，以及为特定目标生成这些AAPs的方法，使DARE-GP成为一个有效的隐私保护方法。

    Emotional Surveillance is an emerging area with wide-reaching privacy concerns. These concerns are exacerbated by ubiquitous IoT devices with multiple sensors that can support these surveillance use cases. The work presented here considers one such use case: the use of a speech emotion recognition (SER) classifier tied to a smart speaker. This work demonstrates the ability to evade black-box SER classifiers tied to a smart speaker without compromising the utility of the smart speaker. This privacy concern is considered through the lens of adversarial evasion of machine learning. Our solution, Defeating Acoustic Recognition of Emotion via Genetic Programming (DARE-GP), uses genetic programming to generate non-invasive additive audio perturbations (AAPs). By constraining the evolution of these AAPs, transcription accuracy can be protected while simultaneously degrading SER classifier performance. The additive nature of these AAPs, along with an approach that generates these AAPs for a fi
    
[^141]: 人机协作Mixup

    Human-in-the-Loop Mixup. (arXiv:2211.01202v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01202](http://arxiv.org/abs/2211.01202)

    本研究针对使用Mixup的合成数据进行人机协作研究，发现人类知觉与合成数据标签不一致，这些发现有助于提高下游模型的可靠性。

    

    将模型表示与人类对齐已被发现能提高鲁棒性和泛化性。然而，这类方法通常专注于标准的观测数据。合成数据正在蓬勃发展并推动机器学习的许多进展，但往往不清楚合成标签是否与人类的知觉保持一致，这可能导致模型表示不与人类对齐。我们专注于Mixup中使用的合成数据：一种强大的正则化方法，已被证明可以提高模型的鲁棒性、泛化性和校准性。我们设计了一系列全面的启发式接口，并将其发布为HILL MixE Suite，并招募了159名参与者对Mixup示例进行了感知判断及其不确定性。我们发现人类知觉不一致地与传统上用于合成数据的标签对齐，并开始展示这些发现的适用性，以潜在地提高下游模型的可靠性。

    Aligning model representations to humans has been found to improve robustness and generalization. However, such methods often focus on standard observational data. Synthetic data is proliferating and powering many advances in machine learning; yet, it is not always clear whether synthetic labels are perceptually aligned to humans -- rendering it likely model representations are not human aligned. We focus on the synthetic data used in mixup: a powerful regularizer shown to improve model robustness, generalization, and calibration. We design a comprehensive series of elicitation interfaces, which we release as HILL MixE Suite, and recruit 159 participants to provide perceptual judgments along with their uncertainties, over mixup examples. We find that human perceptions do not consistently align with the labels traditionally used for synthetic points, and begin to demonstrate the applicability of these findings to potentially increase the reliability of downstream models, particularly wh
    
[^142]: 无监督模型自适应用于无源医学图像分割

    Unsupervised Model Adaptation for Source-free Segmentation of Medical Images. (arXiv:2211.00807v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.00807](http://arxiv.org/abs/2211.00807)

    本文提出了一种无监督模型自适应方法，用于解决医学图像分割中分布转变的问题。该方法通过创建共享的源/目标潜在特征空间，使得源培训的分类器能够在目标域上保持性能。

    

    最近深度神经网络的普及使得在提供足够的训练数据的情况下，语义分割网络在医学领域实现了人类水平的性能。然而，当面临预测分布不同的图像的语义映射时，这些网络在泛化方面存在困难，需要在新的分布上对模型重新训练。这个昂贵的过程需要专业知识来生成训练标签。在医学领域中，通过选择成像设备（如MRI或CT扫描仪）可能会自然产生分布转变。为了克服在在成功地在一个完全注释的“源域”中训练模型之后，在目标域中标记图像的需要，可以使用无监督领域自适应（UDA）。大多数UDA方法通过创建共享的源/目标潜在特征空间来确保目标泛化。这使得源培训的分类器能够在目标域上保持性能。然而，大多数UDA方法要么选择性地偏向源特征空间，使得目标域表现不佳，要么嵌入了具体领域知识，导致无法泛化到不同的目标领域。

    The recent prevalence of deep neural networks has lead semantic segmentation networks to achieve human-level performance in the medical field when sufficient training data is provided. Such networks however fail to generalize when tasked with predicting semantic maps for out-of-distribution images, requiring model re-training on the new distributions. This expensive process necessitates expert knowledge in order to generate training labels. Distribution shifts can arise naturally in the medical field via the choice of imaging device, i.e. MRI or CT scanners. To combat the need for labeling images in a target domain after a model is successfully trained in a fully annotated \textit{source domain} with a different data distribution, unsupervised domain adaptation (UDA) can be used. Most UDA approaches ensure target generalization by creating a shared source/target latent feature space. This allows a source trained classifier to maintain performance on the target domain. However most UDA 
    
[^143]: 潜在多模态功能图模型估计

    Latent Multimodal Functional Graphical Model Estimation. (arXiv:2210.17237v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2210.17237](http://arxiv.org/abs/2210.17237)

    本研究提出了一个潜在多模态功能图模型估计的新框架，通过同时估计转换算子和潜在图来填补当前科学方法在估计多模态功能数据图模型方面的空白

    

    共同多模态功能数据采集是一种现代的方法，通过最近在神经学和生物科学中的工程突破，可以同时从同一主体中测量来自多种模式的功能数据。获取这样的数据的一个重要动机是通过结合多模态信号来发现潜在的连接性。尽管存在科学兴趣，但在估计多模态功能数据下的图模型方面仍存在差距。为此，我们提出了一个新的综合框架，对数据生成过程进行建模，并识别从观测空间到潜在空间的算子映射。然后，我们开发了一个估计器，可以同时估计转换算子和潜在图。这个估计器基于偏相关算子，我们从多元到功能设置中严格推广了它。我们的程序是pr封闭的

    Joint multimodal functional data acquisition, where functional data from multiple modes are measured simultaneously from the same subject, has emerged as an exciting modern approach enabled by recent engineering breakthroughs in the neurological and biological sciences. One prominent motivation to acquire such data is to enable new discoveries of the underlying connectivity by combining multimodal signals. Despite the scientific interest, there remains a gap in principled statistical methods for estimating the graph underlying multimodal functional data. To this end, we propose a new integrative framework that models the data generation process and identifies operators mapping from the observation space to the latent space. We then develop an estimator that simultaneously estimates the transformation operators and the latent graph. This estimator is based on the partial correlation operator, which we rigorously extend from the multivariate to the functional setting. Our procedure is pr
    
[^144]: 随机梯度 Langevin 动力学的函数中心极限定理和大数定律研究

    Functional Central Limit Theorem and Strong Law of Large Numbers for Stochastic Gradient Langevin Dynamics. (arXiv:2210.02092v2 [math.PR] UPDATED)

    [http://arxiv.org/abs/2210.02092](http://arxiv.org/abs/2210.02092)

    这篇论文研究了具有固定步长的随机梯度 Langevin 动力学在数据流不独立的情况下的混合性质，并证明了其具有大数定律和函数中心极限定理。

    

    我们研究了一个重要的机器学习优化算法的混合性质：具有固定步长的随机梯度 Langevin 动力学（SGLD）。数据流不被假设为独立，因此 SGLD 不是一个马尔可夫链，而仅仅是一个随机环境中的马尔可夫链，这极大地复杂了数学处理。我们推导了 SGLD 的大数定律和函数中心极限定理。

    We study the mixing properties of an important optimization algorithm of machine learning: the stochastic gradient Langevin dynamics (SGLD) with a fixed step size. The data stream is not assumed to be independent hence the SGLD is not a Markov chain, merely a \emph{Markov chain in a random environment}, which complicates the mathematical treatment considerably. We derive a strong law of large numbers and a functional central limit theorem for SGLD.
    
[^145]: 使用深度学习和集成技术预测互惠基金的表现

    Predicting Mutual Funds' Performance using Deep Learning and Ensemble Techniques. (arXiv:2209.09649v3 [q-fin.ST] UPDATED)

    [http://arxiv.org/abs/2209.09649](http://arxiv.org/abs/2209.09649)

    本研究通过测试深度学习模型和传统统计技术对基金表现的预测准确性，发现使用现代贝叶斯优化训练的LSTM和GRUs深度学习方法比传统方法更准确地预测基金的夏普比率。将LSTM和GRUs的预测结果进行集成可以实现最佳表现。

    

    预测基金表现对投资者和基金经理都有益处，但也是一项具有挑战性的任务。本文测试了深度学习模型是否比传统统计技术更准确地预测基金表现。基金表现通常通过夏普比率来评估，夏普比率表示风险调整后的表现，以确保在基金之间有可比性。我们根据美国投资于上市大盘股的600多只开放式股票型互惠基金的月度回报时间序列数据计算了年化夏普比率。我们发现，使用现代贝叶斯优化训练的长短期记忆（LSTM）和门控循环单元（GRUs）深度学习方法，比传统统计方法更准确地预测基金的夏普比率。将LSTM和GRUs的预测结果组合的集成方法，实现了所有模型中最好的表现。

    Predicting fund performance is beneficial to both investors and fund managers, and yet is a challenging task. In this paper, we have tested whether deep learning models can predict fund performance more accurately than traditional statistical techniques. Fund performance is typically evaluated by the Sharpe ratio, which represents the risk-adjusted performance to ensure meaningful comparability across funds. We calculated the annualised Sharpe ratios based on the monthly returns time series data for more than 600 open-end mutual funds investing in listed large-cap equities in the United States. We find that long short-term memory (LSTM) and gated recurrent units (GRUs) deep learning methods, both trained with modern Bayesian optimization, provide higher accuracy in forecasting funds' Sharpe ratios than traditional statistical ones. An ensemble method, which combines forecasts from LSTM and GRUs, achieves the best performance of all models. There is evidence to say that deep learning an
    
[^146]: 基于子空间约束的分散学习中的量化方法

    Quantization for decentralized learning under subspace constraints. (arXiv:2209.07821v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2209.07821](http://arxiv.org/abs/2209.07821)

    本文提出一种基于子空间约束的分散学习中的量化方法，该方法通过使用微分随机量化器压缩估计值，在保持低误差的同时减少通信开销。

    

    本文考虑了分散优化问题，其中每个代理都有自己的代价函数需要最小化，同时还要满足子空间约束，要求网络中的最小化者位于低维子空间中。这种约束形式包括共识或单任务优化作为特殊情况，并允许更一般的任务相关性模型，如多任务平滑性和耦合优化。为了应对通信约束，我们提出并研究了一种自适应的分散策略，其中代理在与邻居通信之前使用微分随机量化器压缩其估计值。分析表明，在量化噪声的一些一般条件和足够小的步长$\mu$下，该策略在均方误差和平均比特率方面都是稳定的：通过减小$\mu$，可以保持估计误差很小（与$\mu$数量级相当），而不增加无限的比特率。

    In this paper, we consider decentralized optimization problems where agents have individual cost functions to minimize subject to subspace constraints that require the minimizers across the network to lie in low-dimensional subspaces. This constrained formulation includes consensus or single-task optimization as special cases, and allows for more general task relatedness models such as multitask smoothness and coupled optimization. In order to cope with communication constraints, we propose and study an adaptive decentralized strategy where the agents employ differential randomized quantizers to compress their estimates before communicating with their neighbors. The analysis shows that, under some general conditions on the quantization noise, and for sufficiently small step-sizes $\mu$, the strategy is stable both in terms of mean-square error and average bit rate: by reducing $\mu$, it is possible to keep the estimation errors small (on the order of $\mu$) without increasing indefinit
    
[^147]: 通过模块化和组合来修补卷积神经网络模型的弱点

    Patching Weak Convolutional Neural Network Models through Modularization and Composition. (arXiv:2209.06116v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06116](http://arxiv.org/abs/2209.06116)

    本文提出了一种通过压缩模块化和组合来修补卷积神经网络模型的弱点的方法，该方法无需重新训练整个模型，并在多个基准数据集上实现了与最先进方法相当甚至更好的结果。

    

    尽管深度神经网络在许多应用程序中取得了巨大成功，但在实践中并不总是具有鲁棒性。本文关注的是修补卷积神经网络模型的弱点，而不是通过昂贵的重新训练整个模型来改进它。我们提出了一种压缩模块化方法CNNSplitter，它将具有$N$类分类任务的强CNN模型分解为$N$个较小的CNN模块。每个模块是一个子模型，包含强模型的部分卷积核。为了修补在目标类别（TC）上表现不佳的弱CNN模型，我们将其与从强CNN模型中获得的相应模块相结合。这样，弱CNN模型识别TC的能力可以大大提高，而无需重新训练整个模型。我们在几个基准数据集上展示了我们提出的方法的有效性，并表明它实现了与最先进方法相当甚至更好的结果。

    Despite great success in many applications, deep neural networks are not always robust in practice. For instance, a convolutional neuron network (CNN) model for classification tasks often performs unsatisfactorily in classifying some particular classes of objects. In this work, we are concerned with patching the weak part of a CNN model instead of improving it through the costly retraining of the entire model. Inspired by the fundamental concepts of modularization and composition in software engineering, we propose a compressed modularization approach, CNNSplitter, which decomposes a strong CNN model for $N$-class classification into $N$ smaller CNN modules. Each module is a sub-model containing a part of the convolution kernels of the strong model. To patch a weak CNN model that performs unsatisfactorily on a target class (TC), we compose the weak CNN model with the corresponding module obtained from a strong CNN model. The ability of the weak CNN model to recognize the TC can thus be
    
[^148]: 长什么样的鸭嘴兽？生成自定义提示以进行零样本图像分类

    What does a platypus look like? Generating customized prompts for zero-shot image classification. (arXiv:2209.03320v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.03320](http://arxiv.org/abs/2209.03320)

    本文介绍了一种通过结合大型语言模型和开放词汇模型生成自定义提示的方法，以提高零样本图像分类的准确性和效率。通过利用语言模型生成描述性句子的方式，这种方法能够捕捉图像类别的重要区分特征。

    

    开放词汇模型是图像分类的一种有前景的新范式。与传统的分类模型不同，开放词汇模型在推断时对任意指定的类别集合进行分类，这些类别由自然语言表示。这种自然语言通常包含一组手写模板（例如，“一张{}的照片”），每个类别名称都会填充进去。本文介绍了一种简单的方法来生成更高准确度的提示，而不依赖于任务领域的显式知识，并且需要更少的手工构建句子。为了实现这一目标，我们将开放词汇模型与大型语言模型（LLMs）结合起来，创建出通过语言模型生成的自定义提示（CuPL，读作“couple”）。具体而言，我们利用LLMs中包含的知识来生成包含图像类别的重要区分特征的描述性句子。这使得模型更加重视这些特征。

    Open-vocabulary models are a promising new paradigm for image classification. Unlike traditional classification models, open-vocabulary models classify among any arbitrary set of categories specified with natural language during inference. This natural language, called "prompts", typically consists of a set of hand-written templates (e.g., "a photo of a {}") which are completed with each of the category names. This work introduces a simple method to generate higher accuracy prompts, without relying on any explicit knowledge of the task domain and with far fewer hand-constructed sentences. To achieve this, we combine open-vocabulary models with large language models (LLMs) to create Customized Prompts via Language models (CuPL, pronounced "couple"). In particular, we leverage the knowledge contained in LLMs in order to generate many descriptive sentences that contain important discriminating characteristics of the image categories. This allows the model to place a greater importance on 
    
[^149]: SAFARI：鲁棒性可解释性评估的多功能高效方法

    SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability. (arXiv:2208.09418v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.09418](http://arxiv.org/abs/2208.09418)

    本文提出了一种名为SAFARI的方法，用于评估深度学习的解释可靠性。该方法针对现有技术无法解决的几个挑战，通过引入两种黑盒评估方法，即最坏情况解释差异和一般情况下的鲁棒性的概率概念，来解决现有度量不全面、XAI技术异质性和误解罕见性等问题。使用遗传算法和子集模拟进行评估。

    

    深度学习的可解释性是建立可信赖的人工智能的一道障碍。尽管可解释人工智能（XAI）社区做出了巨大的努力，但解释缺乏鲁棒性——无法区分的输入扰动可能会导致不同的解释结果。因此，针对给定的XAI方法评估深度学习可解释性的鲁棒性至关重要。本文识别了现有技术无法共同应对的几个挑战：i)现有指标不全面；ii)XAI技术高度异质；iii)误解通常是罕见事件。为了解决这些挑战，我们引入了两种黑盒评估方法，分别涉及最坏情况解释差异和一般情况下的鲁棒性的概率概念。使用具有定制适应度函数的遗传算法（GA）来解决约束优化，以实现高效的最坏情况评估。使用专门用于估计罕见事件概率的子集模拟（SS）来进行整体评估。

    Interpretability of Deep Learning (DL) is a barrier to trustworthy AI. Despite great efforts made by the Explainable AI (XAI) community, explanations lack robustness -- indistinguishable input perturbations may lead to different XAI results. Thus, it is vital to assess how robust DL interpretability is, given an XAI method. In this paper, we identify several challenges that the state-of-the-art is unable to cope with collectively: i) existing metrics are not comprehensive; ii) XAI techniques are highly heterogeneous; iii) misinterpretations are normally rare events. To tackle these challenges, we introduce two black-box evaluation methods, concerning the worst-case interpretation discrepancy and a probabilistic notion of how robust in general, respectively. Genetic Algorithm (GA) with bespoke fitness function is used to solve constrained optimisation for efficient worst-case evaluation. Subset Simulation (SS), dedicated to estimate rare event probabilities, is used for evaluating overa
    
[^150]: 看见机器人看不到的东西：学习协作感知进行视觉导航

    See What the Robot Can't See: Learning Cooperative Perception for Visual Navigation. (arXiv:2208.00759v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2208.00759](http://arxiv.org/abs/2208.00759)

    使用图神经网络架构的邻域特征聚合模块实现了所有传感器间的通讯，解决了机器人视觉导航中缺乏全局定位信息的问题并实现了高效导航。

    

    本文讨论了如何在一个未知环境中利用视觉传感器引导机器人到达目的地的问题。在缺乏全局定位信息的条件下，我们通过训练传感器编码和传递相关的视角信息给机器人，让机器人能够在使用第一视角图像的情况下尽可能高效地导航到目标。通过实现一个基于邻域的特征聚合模块，我们使用图神经网络（GNN）架构克服了让所有传感器甚至无法直接看到目标的传感器预测通向目标最短路方向的挑战。

    We consider the problem of navigating a mobile robot towards a target in an unknown environment that is endowed with visual sensors, where neither the robot nor the sensors have access to global positioning information and only use first-person-view images. In order to overcome the need for positioning, we train the sensors to encode and communicate relevant viewpoint information to the mobile robot, whose objective it is to use this information to navigate as efficiently as possible to the target. We overcome the challenge of enabling all the sensors (even those that cannot directly see the target) to predict the direction along the shortest path to the target by implementing a neighborhood-based feature aggregation module using a Graph Neural Network (GNN) architecture. In our experiments, we first demonstrate generalizability to previously unseen environments with various sensor layouts. Our results show that by using communication between the sensors and the robot, we achieve up to
    
[^151]: 使用多图拓扑在跨数据仓库联邦学习中减少训练时间

    Reducing Training Time in Cross-Silo Federated Learning using Multigraph Topology. (arXiv:2207.09657v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.09657](http://arxiv.org/abs/2207.09657)

    本文提出了一种新的多图拓扑结构，用于跨数据仓库联邦学习，通过孤立节点实现模型聚合，从而有效地减少了训练时间。

    

    联邦学习是一个活跃的研究课题，因为它使得多个参与者能够共同训练一个模型而无需共享本地数据。目前，跨数据仓库联邦学习是一种常见的训练设置，它利用几百个可靠的数据仓库和高速访问链路来训练模型。虽然这种方法在实际场景中被广泛应用，但设计一个稳健的拓扑结构以减少训练时间仍然是一个未解决的问题。本文提出了一种新的用于跨数据仓库联邦学习的多图拓扑。我们首先使用覆盖图构建多图，然后将这个多图解析成具有孤立节点的不同简单图。孤立节点的存在使得我们可以在等待其他节点的情况下进行模型聚合，从而有效地减少训练时间。对三个公共数据集进行的大量实验表明，我们提出的方法与最新的拓扑结构相比，显著减少了训练时间。

    Federated learning is an active research topic since it enables several participants to jointly train a model without sharing local data. Currently, cross-silo federated learning is a popular training setting that utilizes a few hundred reliable data silos with high-speed access links to training a model. While this approach has been widely applied in real-world scenarios, designing a robust topology to reduce the training time remains an open problem. In this paper, we present a new multigraph topology for cross-silo federated learning. We first construct the multigraph using the overlay graph. We then parse this multigraph into different simple graphs with isolated nodes. The existence of isolated nodes allows us to perform model aggregation without waiting for other nodes, hence effectively reducing the training time. Intensive experiments on three public datasets show that our proposed method significantly reduces the training time compared with recent state-of-the-art topologies w
    
[^152]: 通过模型变换实现灵活可导优化

    Flexible Differentiable Optimization via Model Transformations. (arXiv:2206.06135v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06135](http://arxiv.org/abs/2206.06135)

    本文介绍了DiffOpt.jl，一个基于MathOptInterface构建的Julia库，它可以对任何模型的参数进行微分求解，不仅限于凸锥规划和二次规划标准形式。使用该库, 可以实现灵活高效地求解一系列模型的优化问题，为基于优化的机器学习提供了新的机会。

    

    本文介绍了DiffOpt.jl，这是一个Julia库，它可以对包含目标值和/或约束条件的任意参数进行优化求解，并进行微分。这个库基于MathOptInterface构建，因此可以利用丰富的求解器生态系统，并与像JuMP这样的建模语言组合得很好。DiffOpt提供前向和反向微分模式，使得可以应用于超参数优化、反向传播和敏感性分析等多种用途，通过将约束优化与端到端可导性编程相结合。DiffOpt是基于两个已知的规则来求解的，分别是凸锥规划和二次规划标准形式的微分。然而，由于可以通过模型转换进行微分，因此用户不仅限于这些形式，还可以针对可以转换为这些标准形式的任何模型的参数进行微分。这特别包括混合仿射锥约束和凸四次约束的程序，这些程序无法直接建模为标准锥形程序。因此，DiffOpt使得可以灵活高效地求解一系列模型的优化问题，为基于优化的机器学习提供了新的机会。

    We introduce DiffOpt.jl, a Julia library to differentiate through the solution of optimization problems with respect to arbitrary parameters present in the objective and/or constraints. The library builds upon MathOptInterface, thus leveraging the rich ecosystem of solvers and composing well with modeling languages like JuMP. DiffOpt offers both forward and reverse differentiation modes, enabling multiple use cases from hyperparameter optimization to backpropagation and sensitivity analysis, bridging constrained optimization with end-to-end differentiable programming. DiffOpt is built on two known rules for differentiating quadratic programming and conic programming standard forms. However, thanks ability to differentiate through model transformation, the user is not limited to these forms and can differentiate with respect to the parameters of any model that can be reformulated into these standard forms. This notably includes programs mixing affine conic constraints and convex quadrat
    
[^153]: 用人造黄油从后验样本中去除脂肪

    Removing the fat from your posterior samples with margarine. (arXiv:2205.12841v3 [astro-ph.IM] UPDATED)

    [http://arxiv.org/abs/2205.12841](http://arxiv.org/abs/2205.12841)

    本文总结了一种使用掩蔽自回归流和核密度估计器的方法，可以学习对应于核心科学参数的边际后验密度。该方法在计算边际库尔巴克-勒布勒散度、边际贝叶斯模型维度、似然函数模拟和先验模拟等方面具有广泛应用。

    

    贝叶斯分析已经成为许多宇宙学领域的不可或缺工具，包括引力波研究、宇宙微波背景和宇宙黎明时期的21厘米信号等现象。该方法提供了一种将复杂模型与描述关键宇宙学和天体物理信号以及各种污染信号和仪器效应的'干扰参数'拟合到数据的方式。在本文中，我们总结了一种使用掩蔽自回归流和核密度估计器来学习对应于核心科学参数的边际后验密度的方法。我们发现，边际或“无干扰”的后验分布及其相关的似然函数具有许多应用，包括计算以前难以处理的边际库尔巴克-勒布勒散度和边际贝叶斯模型维度，似然函数模拟和先验模拟。我们使用玩具例子和实际案例分别展示了每个应用。

    Bayesian analysis has become an indispensable tool across many different cosmological fields including the study of gravitational waves, the Cosmic Microwave Background and the 21-cm signal from the Cosmic Dawn among other phenomena. The method provides a way to fit complex models to data describing key cosmological and astrophysical signals and a whole host of contaminating signals and instrumental effects modelled with 'nuisance parameters'. In this paper, we summarise a method that uses Masked Autoregressive Flows and Kernel Density Estimators to learn marginal posterior densities corresponding to core science parameters. We find that the marginal or 'nuisance-free' posteriors and the associated likelihoods have an abundance of applications including; the calculation of previously intractable marginal Kullback-Leibler divergences and marginal Bayesian Model Dimensionalities, likelihood emulation and prior emulation. We demonstrate each application using toy examples, examples from t
    
[^154]: 可争议神经网络的因果发现与知识注入

    Causal Discovery and Knowledge Injection for Contestable Neural Networks. (arXiv:2205.09787v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09787](http://arxiv.org/abs/2205.09787)

    本研究提出了一种可以进行双向互动的方法，通过允许神经网络展示其所学因果图，并允许人类修改因果图后重新注入机器中，从而提供了一种调试神经网络的方式，实验结果显示该方法可以显著改善预测性能。

    

    神经网络在解决机器学习任务方面表现出色，但它们是否学习到了相关的因果关系尚不清楚，而它们的黑箱特性使得模型构建者难以理解和调试。我们提出了一种新颖的方法来解决这些问题，通过允许神经网络驱动的机器展示其所学因果图，并允许人类修改因果图后重新注入机器中，实现双向互动。所学模型保证符合因果图并遵循专家知识，其中部分知识也可以事先给定。通过对模型行为进行可视化并实现知识注入，我们的方法允许从数据中发现因果结构并支撑预测的从业者进行调试。在真实和合成表格数据上的实验表明，我们的方法可以改进预测性能高达2.4倍。

    Neural networks have proven to be effective at solving machine learning tasks but it is unclear whether they learn any relevant causal relationships, while their black-box nature makes it difficult for modellers to understand and debug them. We propose a novel method overcoming these issues by allowing a two-way interaction whereby neural-network-empowered machines can expose the underpinning learnt causal graphs and humans can contest the machines by modifying the causal graphs before re-injecting them into the machines. The learnt models are guaranteed to conform to the graphs and adhere to expert knowledge, some of which can also be given up-front. By building a window into the model behaviour and enabling knowledge injection, our method allows practitioners to debug networks based on the causal structure discovered from the data and underpinning the predictions. Experiments with real and synthetic tabular data show that our method improves predictive performance up to 2.4x while pr
    
[^155]: 模型量化在深度神经网络中的应用：综述与分析

    A Comprehensive Survey on Model Quantization for Deep Neural Networks. (arXiv:2205.07877v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.07877](http://arxiv.org/abs/2205.07877)

    本文综述了深度神经网络中的模型量化，这是一种用低位宽存储全精度值以实现节约内存和操作成本的压缩方法。文章分类介绍了各种量化方法，并讨论了使用比例因子匹配数据范围和适当的训练方法的重要性。本文还回顾了模型量化的最新研究，并强调了其优缺点和当前的挑战和未来研究方向。

    

    深度神经网络在机器学习领域中的应用取得了重大进展，但是需要大量的参数存储和运算会带来硬件成本的增加和挑战。对此，提出了压缩方法以设计高效的加速器，其中最重要的方法是把全精度的值存储在低位宽中，这就可以节约内存同时用低成本的简单运算代替原本的操作。由于模型量化的灵活性和对设计高效硬件的影响，最近几年提出了许多深度神经网络量化方法，因此需要进行综合性的调查以更好地理解、分析和比较。本文提供了一份全面的综述，介绍了量化概念并从不同角度分类方法，讨论了使用比例因子匹配数据范围的重要性以及使用适当的训练方法避免精度损失的方法。我们还回顾了近年来对模型量化的研究，并强调其优点和缺点。最后，我们讨论了当前的挑战和未来的研究方向。

    Recent advances in machine learning by deep neural networks are significant. But using these networks has been accompanied by a huge number of parameters for storage and computations that leads to an increase in the hardware cost and posing challenges. Therefore, compression approaches have been proposed to design efficient accelerators. One important approach for deep neural network compression is quantization that full-precision values are stored in low bit-width. In this way, in addition to memory saving, the operations will be replaced by simple ones with low cost. Many methods are suggested for DNNs Quantization in recent years, because of flexibility and influence in designing efficient hardware. Therefore, an integrated report is essential for better understanding, analysis, and comparison. In this paper, we provide a comprehensive survey. We describe the quantization concepts and categorize the methods from different perspectives. We discuss using the scale factor to match the 
    
[^156]: 动态操作符，动态类别：从深度学习到预测市场

    Dynamic Operads, Dynamic Categories: From Deep Learning to Prediction Markets. (arXiv:2205.03906v4 [math.CT] UPDATED)

    [http://arxiv.org/abs/2205.03906](http://arxiv.org/abs/2205.03906)

    本文研究了动态操作符和动态类别的概念，并将其应用于预测市场和深度学习领域。

    

    自然组织系统适应内外压力，这发生在抽象层次的所有级别上。我们的论文旨在清晰地思考这个想法，并在前言中广泛阐述这个想法，使其对具有哲学兴趣的读者有普遍的可理解性。在剩下的章节中，我们转向更紧凑的范畴论。我们定义了动态组织的幺半双范畴Org，我们提供了Org丰富的，或称为动态的，范畴结构的定义，例如动态范畴、操作符和幺半范畴，并展示它们如何实现激励哲学思想。我们给出了两个动态范畴结构的例子：预测市场作为动态操作符，深度学习作为动态幺半范畴。

    Natural organized systems adapt to internal and external pressures and this happens at all levels of the abstraction hierarchy. Wanting to think clearly about this idea motivates our paper, and so the idea is elaborated extensively in the introduction, which should be broadly accessible to a philosophically-interested audience. In the remaining sections, we turn to more compressed category theory. We define the monoidal double category Org of dynamic organizations, we provide definitions of Org-enriched, or dynamic, categorical structures -- e.g. dynamic categories, operads, and monoidal categories -- and we show how they instantiate the motivating philosophical ideas. We give two examples of dynamic categorical structures: prediction markets as a dynamic operad and deep learning as a dynamic monoidal category.
    
[^157]: 在独立分割上重新训练分类器以提高群体鲁棒性

    Improved Group Robustness via Classifier Retraining on Independent Splits. (arXiv:2204.09583v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.09583](http://arxiv.org/abs/2204.09583)

    通过在独立的分割上重新训练分类器，该方法改善了群体鲁棒性，有助于提高模型在具有少见特征的子群上的性能。

    

    通过最小化平均风险进行训练的深度神经网络可以达到强大的平均性能。然而，如果某个子群在整体数据中代表性不足，它们的性能可能会下降。群体分布鲁棒优化（Sagawa等人，2020a），简称群体DRO，是学习具有强大最差群体性能模型的基准方法。我们注意到该方法在训练时需要为每个示例提供群体标签，并且可能对小群体过拟合，需要强正则化。在训练时只有有限数量的群体标签时，Just Train Twice（Liu等人，2021），简称JTT，是一种两阶段方法，首先为每个无标签示例推断出伪群体标签，然后根据推断的群体标签应用群体DRO。推断过程对过拟合也很敏感，有时涉及额外的超参数。本文设计了一种简单的方法，基于独立分割上的分类器重新训练的思想。

    Deep neural networks trained by minimizing the average risk can achieve strong average performance. Still, their performance for a subgroup may degrade if the subgroup is underrepresented in the overall data population. Group distributionally robust optimization (Sagawa et al., 2020a), or group DRO in short, is a widely used baseline for learning models with strong worst-group performance. We note that this method requires group labels for every example at training time and can overfit to small groups, requiring strong regularization. Given a limited amount of group labels at training time, Just Train Twice (Liu et al., 2021), or JTT in short, is a two-stage method that infers a pseudo group label for every unlabeled example first, then applies group DRO based on the inferred group labels. The inference process is also sensitive to overfitting, sometimes involving additional hyperparameters. This paper designs a simple method based on the idea of classifier retraining on independent sp
    
[^158]: 解释性在过程结果预测中的应用：获得可解释和可信模型的指南

    Explainability in Process Outcome Prediction: Guidelines to Obtain Interpretable and Faithful Models. (arXiv:2203.16073v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.16073](http://arxiv.org/abs/2203.16073)

    本文介绍了在过程结果预测中应用解释性模型的指南，并通过对事件、案例和控制流的分析，通过实验评估了不同模型的解释性能力。

    

    尽管在预测性过程监控领域已经开始采用可解释人工智能领域的模型，但评估仍主要基于性能指标，并未考虑解释的可操作性和影响。本文通过解释的可解释性和可信度来定义过程结果预测领域的可解释性。我们通过事件、案例和控制流的角度进行分析这些特性，这是典型的基于过程的分析方法。我们在十三个真实事件日志上对七个分类器进行了基准测试，其中包含一系列透明和非透明的机器学习和深度学习模型，并补充了解释性技术。接下来，本文提供了一套名为X-MOP的指南，使用户能够根据个人需求选择最合适的解释性模型。

    Although a recent shift has been made in the field of predictive process monitoring to use models from the explainable artificial intelligence field, the evaluation still occurs mainly through performance-based metrics, thus not accounting for the actionability and implications of the explanations. In this paper, we define explainability through the interpretability of the explanations and the faithfulness of the explainability model in the field of process outcome prediction. The introduced properties are analysed along the event, case, and control flow perspective which are typical for a process-based analysis. This allows comparing inherently created explanations with post-hoc explanations. We benchmark seven classifiers on thirteen real-life events logs, and these cover a range of transparent and non-transparent machine learning and deep learning models, further complemented with explainability techniques. Next, this paper contributes a set of guidelines named X-MOP which allows se
    
[^159]: HiFi++：用于带宽扩展和语音增强的统一框架

    HiFi++: a Unified Framework for Bandwidth Extension and Speech Enhancement. (arXiv:2203.13086v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2203.13086](http://arxiv.org/abs/2203.13086)

    本文提出了一种统一框架HiFi++，用于带宽扩展和语音增强任务，通过改进的生成器架构，在这些任务中表现出与最先进方法相媲美甚至更好的性能，并且消耗更少的计算资源。

    

    近期对抗生成网络在神经声码器中展现出优异的性能，超过了最佳的自回归和流模型。本文中，我们展示了这一成功可以扩展到其他有条件音频生成任务。特别是，在 HiFi 声码器的基础上，我们提出了一种新颖的 HiFi++ 通用框架，用于带宽扩展和语音增强。我们展示了通过改进的生成器架构，HiFi++ 在这些任务中表现出与最先进方法相媲美甚至更好的性能，同时消耗更少的计算资源。我们通过一系列广泛的实验验证了我们方法的有效性。

    Generative adversarial networks have recently demonstrated outstanding performance in neural vocoding outperforming best autoregressive and flow-based models. In this paper, we show that this success can be extended to other tasks of conditional audio generation. In particular, building upon HiFi vocoders, we propose a novel HiFi++ general framework for bandwidth extension and speech enhancement. We show that with the improved generator architecture, HiFi++ performs better or comparably with the state-of-the-art in these tasks while spending significantly less computational resources. The effectiveness of our approach is validated through a series of extensive experiments.
    
[^160]: 在不同域上解决瞬态偏微分方程的局部神经算子

    Local neural operator for solving transient partial differential equations on varied domains. (arXiv:2203.08145v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.08145](http://arxiv.org/abs/2203.08145)

    本文提出了一种局部神经算子（LNO），用于解决在不同域上的瞬态偏微分方程，该模型可通过一种便捷的策略进行边界处理，并在未知域上预测解决方案。实验证明，LNO比传统的有限元方法计算速度快约1000倍，并且能够高效地解决各种流体流动问题。

    

    人工智能显示出降低解决偏微分方程的巨大成本的巨大潜力。然而，在实践中尚未完全实现，因为神经网络在固定的域和边界上定义和训练。在本文中，我们提出了一种用于在不同域上解决瞬态偏微分方程的局部神经算子（LNO）。它还配有一种便捷的策略，包括边界处理，使得预训练的LNO能够预测不同域上的解决方案。为了演示，LNO从随机生成的数据样本中学习Navier-Stokes方程，然后预训练的LNO被用作显式数值时间步进方案，以解决未曾见过的域中的流体流动，例如，盖板驱动腔内的流动和气动级联中的流动。与传统的有限元方法相比，计算气动级联中的流动速度约快1000倍。使用预训练的LNO的求解过程实现了很高的效率，具有显著的

    Artificial intelligence (AI) shows great potential to reduce the huge cost of solving partial differential equations (PDEs). However, it is not fully realized in practice as neural networks are defined and trained on fixed domains and boundaries. Herein, we propose local neural operator (LNO) for solving transient PDEs on varied domains. It comes together with a handy strategy including boundary treatments, enabling one pre-trained LNO to predict solutions on different domains. For demonstration, LNO learns Navier-Stokes equations from randomly generated data samples, and then the pre-trained LNO is used as an explicit numerical time-marching scheme to solve the flow of fluid on unseen domains, e.g., the flow in a lid-driven cavity and the flow across the cascade of airfoils. It is about 1000$\times$ faster than the conventional finite element method to calculate the flow across the cascade of airfoils. The solving process with pre-trained LNO achieves great efficiency, with significan
    
[^161]: BoMD：适用于嘈杂X光分类的多标签描述符包

    BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification. (arXiv:2203.01937v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2203.01937](http://arxiv.org/abs/2203.01937)

    本文提出了一种适用于多标签、嘈杂CXR学习的方法，使用基于袋的多标签描述符平滑地重新标记数据集中的样本，并进行训练以提高模型性能。

    

    深度学习方法在医学图像问题的分类精度方面表现出色，这在很大程度上归功于具有清洁标签的大规模数据集的可用性。然而，考虑到这种手动注释的高成本，新的医学图像分类问题可能需要依赖于从放射学报告中提取的机器生成的嘈杂标签。事实上，许多胸部X光分类器已经从带有嘈杂标签的数据集中建模，但它们的训练过程通常不具有噪声标签样本的鲁棒性，导致次优模型。此外，CXR数据集大多是多标记的，因此当前设计用于多类问题的嘈杂标签学习方法不能轻松地进行调整。本文提出了一种新方法，用于嘈杂多标签CXR学习，其中检测并平滑地重新标记数据集中的样本，然后用于训练常见的多标签分类器。该方法优化了一个基于袋的多标签表示方法，以便有效地使用从放射学报告中提取的信息。

    Deep learning methods have shown outstanding classification accuracy in medical imaging problems, which is largely attributed to the availability of large-scale datasets manually annotated with clean labels. However, given the high cost of such manual annotation, new medical imaging classification problems may need to rely on machine-generated noisy labels extracted from radiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been modelled from datasets with noisy labels, but their training procedure is in general not robust to noisy-label samples, leading to sub-optimal models. Furthermore, CXR datasets are mostly multi-label, so current noisy-label learning methods designed for multi-class problems cannot be easily adapted. In this paper, we propose a new method designed for the noisy multi-label CXR learning, which detects and smoothly re-labels samples from the dataset, which is then used to train common multi-label classifiers. The proposed method optimises a ba
    
[^162]: 针对预测模型更新的留置集

    Holdouts set for predictive model updating. (arXiv:2202.06374v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.06374](http://arxiv.org/abs/2202.06374)

    该论文研究了在复杂环境中如何更新预测风险评分来指导干预。作者提出使用留置集的方式进行更新，通过找到留置集的合适大小可以保证更新后的风险评分性能良好，同时减少留置样本数量。研究结果表明，该方法在总成本增长速度方面具有竞争优势。

    

    在复杂的环境中，如医疗保健领域，预测风险评分在指导干预方面起着越来越重要的作用。然而，直接更新用于指导干预的风险评分可能导致偏差风险估计。为了解决这个问题，我们提出使用“留置集”来进行更新-留置集是一个不接受风险评分指导干预的人群的子集。在留置集的大小上取得平衡是关键，以确保更新后的风险评分性能良好，同时最大限度地减少留置样本的数量。我们证明了这种方法使得总成本可以以$O\left(N^{2/3}\right)$的速度增长，其中$N$是人口规模，并且认为在一般情况下没有竞争性的替代方法。通过定义适当的损失函数，我们描述了一些条件，可以很容易地确定最佳留置集大小（OHS），并引入参数化和半参数化算法来估计OHS，并展示了其在最新风险评分中的应用。

    In complex settings, such as healthcare, predictive risk scores play an increasingly crucial role in guiding interventions. However, directly updating risk scores used to guide intervention can lead to biased risk estimates. To address this, we propose updating using a `holdout set' - a subset of the population that does not receive interventions guided by the risk score. Striking a balance in the size of the holdout set is essential, to ensure good performance of the updated risk score whilst minimising the number of held out samples. We prove that this approach enables total costs to grow at a rate $O\left(N^{2/3}\right)$ for a population of size $N$, and argue that in general circumstances there is no competitive alternative. By defining an appropriate loss function, we describe conditions under which an optimal holdout size (OHS) can be readily identified, and introduce parametric and semi-parametric algorithms for OHS estimation, demonstrating their use on a recent risk score for 
    
[^163]: 学习自动驾驶的可解释、高性能策略

    Learning Interpretable, High-Performing Policies for Autonomous Driving. (arXiv:2202.02352v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.02352](http://arxiv.org/abs/2202.02352)

    本研究提出了一种叫做可解释连续控制树（ICCTs）的树状模型，通过现代的强化学习方法进行优化，能够学习到高性能、可解释的策略。这种方法在自动驾驶领域表现出了比基准模型高33%的性能，并且达到300倍至600倍的减少。

    

    强化学习中的基于梯度的方法在学习自动驾驶车辆的策略方面取得了巨大的成功。虽然这些方法的性能值得在现实世界中采用，但是这些策略缺乏可解释性，限制了在安全关键和法律监管领域中的部署能力。自动驾驶要求可解释和可验证的控制策略，以保持高性能。我们提出了可解释的连续控制树（ICCTs），这是一种基于树状模型，可以通过现代的基于梯度的强化学习方法进行优化，从而产生高性能、可解释的策略。我们方法的关键是允许在稀疏的类似决策树的表示中进行直接优化的过程。我们在六个领域对ICCTs进行验证，并显示ICCTs能够学习到比基准模型在自动驾驶场景中更可解释的策略表示，并且在性能方面超过基准模型最高达33%，同时实现了300倍至600倍的减少。

    Gradient-based approaches in reinforcement learning (RL) have achieved tremendous success in learning policies for autonomous vehicles. While the performance of these approaches warrants real-world adoption, these policies lack interpretability, limiting deployability in the safety-critical and legally-regulated domain of autonomous driving (AD). AD requires interpretable and verifiable control policies that maintain high performance. We propose Interpretable Continuous Control Trees (ICCTs), a tree-based model that can be optimized via modern, gradient-based, RL approaches to produce high-performing, interpretable policies. The key to our approach is a procedure for allowing direct optimization in a sparse decision-tree-like representation. We validate ICCTs against baselines across six domains, showing that ICCTs are capable of learning interpretable policy representations that parity or outperform baselines by up to 33% in AD scenarios while achieving a 300x-600x reduction in the nu
    
[^164]: 使用深度学习识别Pauli自旋阻碍

    Identifying Pauli spin blockade using deep learning. (arXiv:2202.00574v3 [cond-mat.mes-hall] UPDATED)

    [http://arxiv.org/abs/2202.00574](http://arxiv.org/abs/2202.00574)

    该论文介绍了一种使用深度学习算法自动识别Pauli自旋阻碍的方法，通过训练算法使用模拟数据和跨设备验证，克服了PSB数据的稀缺性，并在硅场效应晶体管器件上取得了96％的准确性。该方法具有鲁棒性，预计可在所有类型的量子点器件上使用。

    

    Pauli自旋阻碍（PSB）可用于自旋量子位的初始化和读取，即使在高温下也能发挥作用，但很难识别。我们提出了一种利用电荷传输测量自动识别PSB的机器学习算法。通过训练算法使用模拟数据和跨设备验证，克服了PSB数据的稀缺性。我们在硅场效应晶体管器件上演示了我们的方法，并在不同的测试器件上报告了96％的准确性，这表明该方法对器件变异性具有鲁棒性。预计该方法可在所有类型的量子点器件上使用。

    Pauli spin blockade (PSB) can be employed as a great resource for spin qubit initialisation and readout even at elevated temperatures but it can be difficult to identify. We present a machine learning algorithm capable of automatically identifying PSB using charge transport measurements. The scarcity of PSB data is circumvented by training the algorithm with simulated data and by using cross-device validation. We demonstrate our approach on a silicon field-effect transistor device and report an accuracy of 96% on different test devices, giving evidence that the approach is robust to device variability. The approach is expected to be employable across all types of quantum dot devices.
    
[^165]: DoCoM：具有近似最优样本复杂度的压缩分散式优化

    DoCoM: Compressed Decentralized Optimization with Near-Optimal Sample Complexity. (arXiv:2202.00255v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.00255](http://arxiv.org/abs/2202.00255)

    提出了一种名为DoCoM的分散式优化算法，通过压缩谣言传播一致性追踪同时跟踪平均迭代和随机梯度，并引入动量步骤和局部梯度估计，实现通信高效和近似最优样本复杂度。

    

    本文提出了一种名为DoCoM的算法，用于实现通信高效的分散式优化。算法通过压缩式谣言传播一致性追踪来同时跟踪平均迭代和随机梯度，以实现近似最优样本复杂度。其次，通过引入动量步骤和局部梯度估计，实现自适应方差归约。我们证明，在T次迭代中，DoCoM可以在所有参与代理中找到一个近似稳定的解，满足$\mathbb{E}[ \| \nabla f( \theta ) \|^2 ] = \mathcal{O}( 1 / T^{2/3} )$，其中$f(\theta)$是一个平滑的（可能非凸）目标函数。需要注意的是，我们通过分析设计了一个新的潜力函数，紧密跟踪了DoCoM的一次迭代进展来证明这一点。

    This paper proposes the Doubly Compressed Momentum-assisted stochastic gradient tracking algorithm $\texttt{DoCoM}$ for communication-efficient decentralized optimization. The algorithm features two main ingredients to achieve a near-optimal sample complexity while allowing for communication compression. First, the algorithm tracks both the averaged iterate and stochastic gradient using compressed gossiping consensus. Second, a momentum step is incorporated for adaptive variance reduction with the local gradient estimates. We show that $\texttt{DoCoM}$ finds a near-stationary solution at all participating agents satisfying $\mathbb{E}[ \| \nabla f( \theta ) \|^2 ] = \mathcal{O}( 1 / T^{2/3} )$ in $T$ iterations, where $f(\theta)$ is a smooth (possibly non-convex) objective function. Notice that the proof is achieved via analytically designing a new potential function that tightly tracks the one-iteration progress of $\texttt{DoCoM}$. As a corollary, our analysis also established the li
    
[^166]: 通过记忆推理：最近邻知识图嵌入

    Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings. (arXiv:2201.05575v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2201.05575](http://arxiv.org/abs/2201.05575)

    本文提出了一种新的知识图嵌入方法kNN-KGE，它通过预训练语言模型和最近邻的线性插值，允许罕见或新出现的实体被明确地记忆，而不是隐藏在模型参数中。实验结果显示，该方法能够改善链接预测结果并在低资源环境中表现出更好的性能。

    

    以往的知识图嵌入方法通常将实体映射到表示，并利用评分函数预测目标实体，但它们通常难以推理出罕见或新出现的未知实体。在本文中，我们提出了一种新的知识图嵌入方法kNN-KGE，它利用预训练语言模型，并通过线性插值将其实体分布与k个最近邻相结合。我们根据知识存储中实体嵌入空间中的距离计算最近邻。我们的方法可以明确地记忆罕见或新出现的实体，而不是隐藏在模型参数中。实验结果表明，我们的方法可以改善归纳和传递式链接预测结果，并在只有少量三元组的低资源环境中表现出更好的性能，而这可能更容易通过明确的记忆进行推理。

    Previous knowledge graph embedding approaches usually map entities to representations and utilize score functions to predict the target entities, yet they typically struggle to reason rare or emerging unseen entities. In this paper, we propose kNN-KGE, a new knowledge graph embedding approach with pre-trained language models, by linearly interpolating its entity distribution with k-nearest neighbors. We compute the nearest neighbors based on the distance in the entity embedding space from the knowledge store. Our approach can allow rare or emerging entities to be memorized explicitly rather than implicitly in model parameters. Experimental results demonstrate that our approach can improve inductive and transductive link prediction results and yield better performance for low-resource settings with only a few triples, which might be easier to reason via explicit memory. Code is available at https://github.com/zjunlp/KNN-KG.
    
[^167]: 静息时人类功能性脑网络的持续同调状态空间估计

    Persistent Homological State-Space Estimation of Functional Human Brain Networks at Rest. (arXiv:2201.00087v3 [math.AT] UPDATED)

    [http://arxiv.org/abs/2201.00087](http://arxiv.org/abs/2201.00087)

    该论文提出了一种新的数据驱动的拓扑数据分析方法，用于估计静息状态下人类脑网络的状态空间。该方法通过惩罚网络之间的拓扑距离将动态变化的脑网络聚类为不同的状态，并通过考虑时间维度来提高准确性。研究还发现脑网络的整体拓扑具有遗传特征。

    

    我们提出了一种新的数据驱动的拓扑数据分析（TDA）方法，用于估计动态变化的人类功能性脑网络的状态空间。我们的方法通过对网络之间的拓扑距离进行惩罚，将动态变化的脑网络聚类为拓扑上不同的状态。我们的方法通过计算网络之间的Wasserstein距离考虑了数据的时间维度。我们的方法在估计脑网络的状态空间方面表现优于广泛使用的k-means聚类方法。该方法被应用于更准确地确定动态变化的功能性脑网络的状态空间。随后，我们采用双生子研究设计探讨了脑网络的整体拓扑是否具有遗传特征。

    We present a new data driven topological data analysis (TDA) approach for estimating state spaces in dynamically changing human functional brain networks of human. Our approach penalizes the topological distance between networks and clusters dynamically changing brain networks into topologically distinct states. Our method takes into account the temporal dimension of the data through the Wasserstein distance between networks. Our method is shown to outperform the widely used k-means clustering often used in estimating the state space in brain networks. The method is applied to more accurately determine the state spaces of dynamically changing functional brain networks. Subsequently, we address the question of whether the overall topology of brain networks is a heritable feature using the twin study design.
    
[^168]: 高分辨率微分方程在鞍点优化器的最后迭代收敛中的应用

    Last-Iterate Convergence of Saddle-Point Optimizers via High-Resolution Differential Equations. (arXiv:2112.13826v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2112.13826](http://arxiv.org/abs/2112.13826)

    本研究利用高分辨率微分方程在鞍点优化中设计了不同的微分方程模型，这些模型在双线性博弈中的收敛性质与离散方法相匹配。

    

    几种广泛使用的一阶鞍点优化方法在衍生时可以得到与梯度下降上升 (GDA) 方法相同的连续时间常微分方程 (ODE)，但是这些方法的收敛性质在简单的双线性博弈中是有差异的。因此，ODE 视角在分析单目标优化方法方面已经发挥了强大的作用，但在鞍点优化中的应用尚不明晰。我们采用在流体动力学中研究的高分辨率微分方程 (HRDEs) 框架来设计几种鞍点优化方法的微分方程模型。尤其需要指出的是，这些 HRDEs 对应于不同的鞍点优化方法是不同的。此外，在双线性博弈中，HRDEs 的收敛性质与相应的离散方法的性质相匹配。

    Several widely-used first-order saddle-point optimization methods yield an identical continuous-time ordinary differential equation (ODE) that is identical to that of the Gradient Descent Ascent (GDA) method when derived naively. However, the convergence properties of these methods are qualitatively different, even on simple bilinear games. Thus the ODE perspective, which has proved powerful in analyzing single-objective optimization methods, has not played a similar role in saddle-point optimization.  We adopt a framework studied in fluid dynamics -- known as High-Resolution Differential Equations (HRDEs) -- to design differential equation models for several saddle-point optimization methods. Critically, these HRDEs are distinct for various saddle-point optimization methods. Moreover, in bilinear games, the convergence properties of the HRDEs match the qualitative features of the corresponding discrete methods. Additionally, we show that the HRDE of Optimistic Gradient Descent Ascent 
    
[^169]: 多元极值的谱学习

    Spectral learning of multivariate extremes. (arXiv:2111.07799v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2111.07799](http://arxiv.org/abs/2111.07799)

    我们提出了一种用于分析多元极值的谱聚类算法，并通过理论和数值实验展示了其在学习角度测度方面的性能。

    

    我们提出了一种用于分析多元极值的谱聚类算法。具体而言，我们关注极值理论中由角度或谱测度表征的多元极值的渐近依赖性。我们的工作研究了谱聚类的理论性能，该聚类基于从极值样本中构建的随机k最近邻图，即对于半径超过一个较大阈值的随机向量的角度部分。具体而言，我们推导出线性因子模型产生的极值的渐近分布，并证明，在某些条件下，谱聚类可以一致地识别出在该模型中产生的极值的聚类。基于这个结果，我们提出了一种简单的一致性估计策略来学习角度测度。我们的理论结果与数值实验相结合，展示了我们方法在有限样本情况下的性能。

    We propose a spectral clustering algorithm for analyzing the dependence structure of multivariate extremes. More specifically, we focus on the asymptotic dependence of multivariate extremes characterized by the angular or spectral measure in extreme value theory. Our work studies the theoretical performance of spectral clustering based on a random $k$-nearest neighbor graph constructed from an extremal sample, i.e., the angular part of random vectors for which the radius exceeds a large threshold. In particular, we derive the asymptotic distribution of extremes arising from a linear factor model and prove that, under certain conditions, spectral clustering can consistently identify the clusters of extremes arising in this model. Leveraging this result we propose a simple consistent estimation strategy for learning the angular measure. Our theoretical findings are complemented with numerical experiments illustrating the finite sample performance of our methods.
    
[^170]: 物理启发式神经算子用于学习偏微分方程

    Physics-Informed Neural Operator for Learning Partial Differential Equations. (arXiv:2111.03794v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.03794](http://arxiv.org/abs/2111.03794)

    本文提出了一种名为PINO的方法，它能够通过同时利用数据和物理约束条件来学习偏微分方程的解算子，克服了纯数据驱动和基于物理的方法的局限性，并且可以在不同分辨率上合并数据和约束条件。

    

    本文提出了一种名为“物理启发式神经算子”（PINO）的方法，它使用现有数据和/或物理约束条件来学习一类参数化的偏微分方程的解算子。该混合方法允许PINO克服纯数据驱动和基于物理的方法所面临的局限性。将数据和PDE约束相结合，PINO能够同时适应数据量有限和/或质量较差的情况以及困难PDE约束的优化。此外，与其他混合学习方法相比，PINO具有一个独特的特性，即能够在不同分辨率上合并数据和PDE约束。这允许我们将来自数值求解器的低分辨率数据与高分辨率PDE约束相结合，而所得到的PINO即使在高分辨率的测试实例上也没有精度下降。

    In this paper, we propose physics-informed neural operators (PINO) that uses available data and/or physics constraints to learn the solution operator of a family of parametric Partial Differential Equation (PDE). This hybrid approach allows PINO to overcome the limitations of purely data-driven and physics-based methods. For instance, data-driven methods fail to learn when data is of limited quantity and/or quality, and physics-based approaches fail to optimize on challenging PDE constraints. By combining both data and PDE constraints, PINO overcomes all these challenges. Additionally, a unique property that PINO enjoys over other hybrid learning methods is its ability to incorporate data and PDE constraints at different resolutions. This allows us to combine coarse-resolution data, which is inexpensive to obtain from numerical solvers, with higher resolution PDE constraints, and the resulting PINO has no degradation in accuracy even on high-resolution test instances. This discretizati
    
[^171]: 推荐系统的深度探索

    Deep Exploration for Recommendation Systems. (arXiv:2109.12509v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2109.12509](http://arxiv.org/abs/2109.12509)

    本文提出了一种深度探索方法以解决推荐系统中奖励稀少时的问题，并在高保真度的工业级模拟器下进行了实验，证明了该算法相比现有算法有很大的提升。

    

    现代推荐系统应从延迟反馈中探索和学习。过去的研究往往侧重于从用户对单个推荐的响应中学习。这些工作利用了监督学习和强化学习的方法，但放弃了学习用户之后的行为。在过去的工作中，虽然致力于从随后的行为中学习，但缺乏有效的方法来引导并获取有意义的延迟反馈。当奖励较少时，通过引导探索有意义的延迟反馈变得特别具有挑战性。为了解决这个问题，我们为推荐系统开发了深度探索方法。具体而言，我们将推荐系统形式化为一个序列决策问题，并证明了深度探索方法在单步探索方面的优势。我们的实验是在高保真度的工业级模拟器下进行的，并且证明了该算法相比现有算法有很大的提升。

    Modern recommendation systems ought to benefit by probing for and learning from delayed feedback. Research has tended to focus on learning from a user's response to a single recommendation. Such work, which leverages methods of supervised and bandit learning, forgoes learning from the user's subsequent behavior. Where past work has aimed to learn from subsequent behavior, there has been a lack of effective methods for probing to elicit informative delayed feedback. Effective exploration through probing for delayed feedback becomes particularly challenging when rewards are sparse. To address this, we develop deep exploration methods for recommendation systems. In particular, we formulate recommendation as a sequential decision problem and demonstrate benefits of deep exploration over single-step exploration. Our experiments are carried out with high-fidelity industrial-grade simulators and establish large improvements over existing algorithms.
    
[^172]: 离线分散式多智能体强化学习

    Offline Decentralized Multi-Agent Reinforcement Learning. (arXiv:2108.01832v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.01832](http://arxiv.org/abs/2108.01832)

    本文提出了一种离线分散式多智能体强化学习框架，通过利用价值偏差和转移规范化来修改转移概率，使智能体能够学习到高性能和协调的策略。

    

    在许多真实世界的多智能体合作任务中，由于成本高昂和风险，智能体无法在学习过程中持续与环境进行交互并收集经验，而必须从离线数据集中进行学习。然而，每个智能体数据集中的状态转移动态可能与其他智能体在执行中学习策略引发的转移动态相差很大，从而导致价值估计出现较大的误差。因此，智能体学习到的是不协调且性能较低的策略。本文提出了一种离线分散式多智能体强化学习框架，利用价值偏差和转移规范化来有意地修改转移概率。价值偏差乐观地增加了高价值下一状态的转移概率，而转移规范化对下一状态的转移概率进行了规范化。它们共同使智能体能够学习到高性能和协调的策略。在理论上，我们证明了该框架的收敛性。

    In many real-world multi-agent cooperative tasks, due to high cost and risk, agents cannot continuously interact with the environment and collect experiences during learning, but have to learn from offline datasets. However, the transition dynamics in the dataset of each agent can be much different from the ones induced by the learned policies of other agents in execution, creating large errors in value estimates. Consequently, agents learn uncoordinated low-performing policies. In this paper, we propose a framework for offline decentralized multi-agent reinforcement learning, which exploits value deviation and transition normalization to deliberately modify the transition probabilities. Value deviation optimistically increases the transition probabilities of high-value next states, and transition normalization normalizes the transition probabilities of next states. They together enable agents to learn high-performing and coordinated policies. Theoretically, we prove the convergence of
    
[^173]: 从生成的图像中推断生成模型的超参数：生成模型的逆向工程

    Reverse Engineering of Generative Models: Inferring Model Hyperparameters from Generated Images. (arXiv:2106.07873v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2106.07873](http://arxiv.org/abs/2106.07873)

    本研究提出了一种逆向工程生成模型的方法，通过分析生成的图像来推断模型的超参数，以识别和理解生成模型的潜在滥用。通过指纹估计网络和解析网络，我们能够从生成的图像中预测生成模型的网络架构和训练损失函数。

    

    最先进的生成模型能够合成逼真的图像，令人难以区分是真实照片还是生成的图像。识别和理解篡改媒体对于缓解生成模型潜在滥用的社会关切至关重要。我们提出了对生成模型进行逆向工程的方法，从这些模型生成的图像中推断模型的超参数。我们定义了一个新颖的问题，即“模型解析”，通过分析生成的图像来估计生成模型的网络架构和训练损失函数，这对人类而言似乎是不可能的任务。为了解决这个问题，我们提出了一个框架，包括两个组成部分：指纹估计网络（FEN），通过训练带有四个约束的指纹估计网络来估计生成模型的指纹，以鼓励指纹具有期望的特性；解析网络（PN），从估计的指纹中预测网络架构和损失函数。为了评估我们的方法，我们收集了一些生成模型的图像，并对其进行了推断。

    State-of-the-art (SOTA) Generative Models (GMs) can synthesize photo-realistic images that are hard for humans to distinguish from genuine photos. Identifying and understanding manipulated media are crucial to mitigate the social concerns on the potential misuse of GMs. We propose to perform reverse engineering of GMs to infer model hyperparameters from the images generated by these models. We define a novel problem, ``model parsing", as estimating GM network architectures and training loss functions by examining their generated images -- a task seemingly impossible for human beings. To tackle this problem, we propose a framework with two components: a Fingerprint Estimation Network (FEN), which estimates a GM fingerprint from a generated image by training with four constraints to encourage the fingerprint to have desired properties, and a Parsing Network (PN), which predicts network architecture and loss functions from the estimated fingerprints. To evaluate our approach, we collect a
    
[^174]: 两层神经网络的非渐近理论：超越偏差-方差折衷

    Nonasymptotic theory for two-layer neural networks: Beyond the bias-variance trade-off. (arXiv:2106.04795v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.04795](http://arxiv.org/abs/2106.04795)

    这项研究提出了针对两层神经网络的非渐近泛化理论，通过引入缩放变分正则化，并利用"岭-套索对偶性"获得了新的预测界限，解释了大型神经网络在超参数化情况下的表现以及双谷现象。

    

    大型神经网络在现代深度学习实践中表现出惊人的效果，即使在超参数化的情况下，即活跃参数数量相对于样本大小很大。这与传统观点相矛盾，传统观点认为机器学习模型必须在偏差和方差之间进行权衡以实现最佳泛化。为了解决这个冲突，我们通过引入缩放变分正则化，给出了针对具有ReLU激活函数的两层神经网络的非渐近泛化理论。有趣的是，这个正则化器从梯度优化的角度来看等价于岭回归，但在控制模型复杂性方面起到了类似于分组套索的作用。通过利用这种"岭-套索对偶性"，我们得到了适用于所有网络宽度的新的预测界限，从而重现了双谷现象。此外，在信号强的情况下，超参数化的最小风险低于其欠参数化的对应值，并且几乎是最小最大方法。

    Large neural networks have proved remarkably effective in modern deep learning practice, even in the overparametrized regime where the number of active parameters is large relative to the sample size. This contradicts the classical perspective that a machine learning model must trade off bias and variance for optimal generalization. To resolve this conflict, we present a nonasymptotic generalization theory for two-layer neural networks with ReLU activation function by incorporating scaled variation regularization. Interestingly, the regularizer is equivalent to ridge regression from the angle of gradient-based optimization, but plays a similar role to the group lasso in controlling the model complexity. By exploiting this "ridge-lasso duality," we obtain new prediction bounds for all network widths, which reproduce the double descent phenomenon. Moreover, the overparametrized minimum risk is lower than its underparametrized counterpart when the signal is strong, and is nearly minimax o
    
[^175]: 探索锁存器存储器及其他：利用关键见解训练循环神经网络

    Exploring Flip Flop memories and beyond: training recurrent neural networks with key insights. (arXiv:2010.07858v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.07858](http://arxiv.org/abs/2010.07858)

    本研究通过全面调查和描述时间处理任务的实现，特别是3位锁存器存储器，为循环神经网络的训练方法提供了关键见解和贡献。

    

    在不同领域中，训练神经网络以执行不同任务是非常重要的。具体而言，循环神经网络(RNNs)在计算神经科学中非常有趣。机器学习的开源框架，如Tensorflow和Keras，对我们目前使用的技术的发展产生了重大影响。本研究旨在通过全面调查和描述时间处理任务（特别是3位锁存器存储器）的实现，做出重大贡献。我们深入探讨了整个建模过程，包括方程、任务参数化和软件开发。通过一系列的可视化和分析工具，精心分析了所获得的网络以阐明其动态特性。此外，所提供的代码具有足够的灵活性，可以促进对不同任务和系统的建模。此外，我们还介绍了如何将存储器状态有效地存储在一个立方体的顶点中。

    Training neural networks to perform different tasks is relevant across various disciplines. In particular, Recurrent Neural Networks (RNNs) are of great interest in Computational Neuroscience. Open-source frameworks dedicated to Machine Learning, such as Tensorflow and Keras have produced significant changes in the development of technologies that we currently use. This work aims to make a significant contribution by comprehensively investigating and describing the implementation of a temporal processing task, specifically a 3-bit Flip Flop memory. We delve into the entire modelling process, encompassing equations, task parametrization, and software development. The obtained networks are meticulously analyzed to elucidate dynamics, aided by an array of visualization and analysis tools. Moreover, the provided code is versatile enough to facilitate the modelling of diverse tasks and systems. Furthermore, we present how memory states can be efficiently stored in the vertices of a cube in 
    
[^176]: 功能性轮廓建模和可解释形状变化检测：结合Fréchet均值与形状不变模型的方法

    Modelling of functional profiles and explainable shape shifts detection: An approach combining the notion of the Fr\'echet mean with the shape invariant model}. (arXiv:2010.02968v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2010.02968](http://arxiv.org/abs/2010.02968)

    该论文提出了一种结合Fréchet均值和形状不变模型的方法，用于检测功能性轮廓中的形状变化，并构建了功能性数据的控制图，可解释性强且能识别潜在变化。

    

    提出了一种适用于检测功能性轮廓中形状变化的建模框架，结合了Fréchet均值概念和变形模型的概念。利用Fréchet均值提供的广义均值感知能够捕捉研究对象轮廓的典型模式，而变形模型的概念，特别是形状不变模型，允许对轮廓与典型形状之间的偏差进行可解释的参数化。构建和提出了与数据的功能性特性和所采用的变形模型相兼容的EWMA类型控制图，利用研究对象的轮廓在广义均值感知下的某些形状特征，实现对形状和/或变形过程潜在变化的识别。进一步将形状变形过程的潜在变化区分为与幅度和/或相位相关的显著变化。

    A modelling framework suitable for detecting shape shifts in functional profiles combining the notion of Fr\'echet mean and the concept of deformation models is developed and proposed. The generalized mean sense offerred by the Fr\'echet mean notion is employed to capture the typical pattern of the profiles under study, while the concept of deformation models, and in particular of the shape invariant model, allows for interpretable parameterizations of profile's deviations from the typical shape. EWMA-type control charts compatible with the functional nature of data and the employed deformation model are built and proposed, exploiting certain shape characteristics of the profiles under study with respect to the generalised mean sense, allowing for the identification of potential shifts concerning the shape and/or the deformation process. Potential shifts in the shape deformation process, are further distingu\-ished to significant shifts with respect to amplitude and/or the phase of the
    
[^177]: 公平的层次聚类算法

    Fair Algorithms for Hierarchical Agglomerative Clustering. (arXiv:2005.03197v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2005.03197](http://arxiv.org/abs/2005.03197)

    提出了一种公平的层次聚类算法，该算法不受距离链接准则的限制，能适应不同的公平度量标准，并可以处理多个受保护群体。

    

    层次聚类算法在现代数据科学中被广泛使用，旨在将数据集分割为聚类，并生成数据样本之间的层次关系。层次聚类算法被应用于生物学、自然语言处理和推荐系统等许多应用中。因此，确保这些算法是公平的至关重要 -- 即使数据集对某些受保护群体存在偏差，生成的聚类输出也不应歧视来自任何这些群体的样本。然而，最近针对公平聚类的研究主要集中在基于中心的聚类算法，如k-中值和k-均值聚类。在本文中，我们提出了一种公平的层次聚类算法，它能在使用任何距离链接准则的情况下强制执行公平约束，并能推广到层次聚类的任何自然公平度量，适用于多个受保护群体。

    Hierarchical Agglomerative Clustering (HAC) algorithms are extensively utilized in modern data science, and seek to partition the dataset into clusters while generating a hierarchical relationship between the data samples. HAC algorithms are employed in many applications, such as biology, natural language processing, and recommender systems. Thus, it is imperative to ensure that these algorithms are fair -- even if the dataset contains biases against certain protected groups, the cluster outputs generated should not discriminate against samples from any of these groups. However, recent work in clustering fairness has mostly focused on center-based clustering algorithms, such as k-median and k-means clustering. In this paper, we propose fair algorithms for performing HAC that enforce fairness constraints 1) irrespective of the distance linkage criteria used, 2) generalize to any natural measures of clustering fairness for HAC, 3) work for multiple protected groups, and 4) have competiti
    
[^178]: 广义化界限和表示学习用于估计潜在结果和因果效应

    Generalization Bounds and Representation Learning for Estimation of Potential Outcomes and Causal Effects. (arXiv:2001.07426v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2001.07426](http://arxiv.org/abs/2001.07426)

    本论文研究了从记录的背景、决策和结果中估计个体层面的因果效应的问题，给出了基于距离度量的广义化界限以及相应的样本重新加权方法，并设计了最小化界限的表示学习算法来实现估计的准确性。

    

    医疗、经济和教育等各领域的从业者都渴望应用机器学习来改善决策。由于实验的成本和不切实际性，以及电子记录保留的巨大增长，非实验观测数据评估决策的问题引起了关注。本文即是在这个背景下展开研究。我们特别研究了从记录的背景、决策和结果中估计个体层面的因果效应，例如单个患者对不同药物的反应。我们给出了基于接受不同治疗组之间距离度量的估计效果误差的广义化界限，允许样本重新加权。我们给出了我们界限紧密的条件，并展示了它与无监督领域适应结果的关系。在我们的理论结果的指导下，我们设计了最小化界限的表示学习算法，通过正则化表示向量。

    Practitioners in diverse fields such as healthcare, economics and education are eager to apply machine learning to improve decision making. The cost and impracticality of performing experiments and a recent monumental increase in electronic record keeping has brought attention to the problem of evaluating decisions based on non-experimental observational data. This is the setting of this work. In particular, we study estimation of individual-level causal effects, such as a single patient's response to alternative medication, from recorded contexts, decisions and outcomes. We give generalization bounds on the error in estimated effects based on distance measures between groups receiving different treatments, allowing for sample re-weighting. We provide conditions under which our bound is tight and show how it relates to results for unsupervised domain adaptation. Led by our theoretical results, we devise representation learning algorithms that minimize our bound, by regularizing the rep
    
[^179]: 对稀疏观测下的二次张量进行恢复的保证

    Recovery Guarantees for Quadratic Tensors with Sparse Observations. (arXiv:1811.00148v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1811.00148](http://arxiv.org/abs/1811.00148)

    本研究考察了稀疏观测下的二次张量恢复问题，发现非凸方法能够在线性样本数量下保证误差最小化问题的全局极小值，并改进了观测有限情况下的CP模型性能。

    

    我们考虑张量完整问题，即预测张量中缺失的条目。常用的CP模型具有三次乘积形式，但一种替代的二次模型家族已经从推荐系统等应用中出现，其是对成对乘积求和而不是三次乘积。非凸方法是学习二次模型的首选方法，本研究考察了它们的样本复杂度和误差保证。我们的主要结果是，只需线性数量的样本，平均均方误差目标函数的所有局部极小值都是全局极小值，并可以恢复原始张量。我们通过对合成和真实世界数据的实验证实了我们的理论结果，表明在观测数量有限的情况下，二次模型比CP模型具有更好的性能。

    We consider the tensor completion problem of predicting the missing entries of a tensor. The commonly used CP model has a triple product form, but an alternate family of quadratic models, which are the sum of pairwise products instead of a triple product, have emerged from applications such as recommendation systems. Non-convex methods are the method of choice for learning quadratic models, and this work examines their sample complexity and error guarantee. Our main result is that with the number of samples being only linear in the dimension, all local minima of the mean squared error objective are global minima and recover the original tensor. We substantiate our theoretical results with experiments on synthetic and real-world data, showing that quadratic models have better performance than CP models where there are a limited amount of observations available.
    
[^180]: 递归神经网络（RNN）和长短期记忆（LSTM）网络的基础

    Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network. (arXiv:1808.03314v10 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1808.03314](http://arxiv.org/abs/1808.03314)

    本文通过形式化推导来解释了递归神经网络（RNN）和长短期记忆（LSTM）网络的基本原理，并提出了一种将RNN转化为“Vanilla LSTM”网络的方法。

    

    长短期记忆（LSTM）网络在广泛的实际应用中表现出了高效的效果，因此在科学期刊、技术博客和实现指南中得到了广泛的关注。然而，在大多数文章中，LSTM网络及其父类RNN的推推理公式被以公理的方式陈述，而训练公式则完全被省略。此外，关于“展开”RNN的技术在文献中通常被描述，但缺乏解释。本文旨在在一篇文章中解释RNN和LSTM的基本原理。我们从信号处理的概念中形式化地推导出了RNN的基本公式。然后，我们提出并证明了一个精确的陈述，得到了RNN的展开技术。我们还审查了训练标准RNN的困难，并通过一系列逻辑论证将RNN转化为“Vanilla LSTM”网络。我们提供了与训练过程相关的所有方程。

    Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of "unrolling" an RNN is routinely presented without justification throughout the literature. The goal of this paper is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in signal processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the "Vanilla LSTM" network through a series of logical arguments. We provide all equations pertaining to the 
    

