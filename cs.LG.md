# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Human-compatible driving partners through data-regularized self-play reinforcement learning](https://arxiv.org/abs/2403.19648) | 提出了Human-Regularized PPO (HR-PPO)算法，通过自我博弈训练代理，实现在封闭环境中逼真且有效的驾驶伙伴 |
| [^2] | [Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models](https://arxiv.org/abs/2403.19647) | 该论文介绍了一种新方法，即稀疏特征电路，可以在语言模型中发现和编辑可解释的因果图，为我们提供了对未预料机制的详细理解和包含了用于提高分类器泛化能力的SHIFT方法。 |
| [^3] | [Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models](https://arxiv.org/abs/2403.19631) | 提出了用于多跳问题回答的检索增强模型编辑（RAE）框架，利用互信息最大化的检索方法和修剪策略，实现了对语言模型的有效优化。 |
| [^4] | [Metric Learning from Limited Pairwise Preference Comparisons](https://arxiv.org/abs/2403.19629) | 在有限成对偏好比较下研究度量学习，表明虽然无法学习单个理想项目，但当比较对象表现出低维结构时，每个用户可以帮助学习限制在低维子空间中的度量。 |
| [^5] | [Top-$k$ Classification and Cardinality-Aware Prediction](https://arxiv.org/abs/2403.19625) | 该论文研究了Top-$k$分类任务，并提出了基于基数感知的损失函数，通过实例相关的成本敏感学习，引入了新的基数感知算法 |
| [^6] | [Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative Models](https://arxiv.org/abs/2403.19620) | 本研究采用创意对抗网络和演化方法在深度生成模型的潜在空间中进行协作互动演化艺术，通过自动美学和协作交互式人类评估指标评估生成的图像质量。 |
| [^7] | [ILPO-NET: Network for the invariant recognition of arbitrary volumetric patterns in 3D](https://arxiv.org/abs/2403.19612) | ILPO-Net是一种处理任意形状模式的新方法，通过卷积运算对局部空间模式方向具有不变性，在各种体积数据集上展现出优越性能并显著减少参数数量。 |
| [^8] | [Data-Adaptive Tradeoffs among Multiple Risks in Distribution-Free Prediction](https://arxiv.org/abs/2403.19605) | 开发了一种能够在数据自适应选择阈值和权衡参数时有效控制风险的方法，支持单调和几乎单调风险，无需分布假设。 |
| [^9] | [Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers](https://arxiv.org/abs/2403.19591) | 本文提出的GQA-LUT算法在变压器中的非线性操作中具有量化感知性，并实现了对INT8-based LUT逼近的应用，节约了大量硬件和功耗 |
| [^10] | [DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs](https://arxiv.org/abs/2403.19588) | 本文重新探讨了密集连接卷积网络（DenseNets），揭示了其相对于ResNet风格架构的被低估有效性，通过改进架构、块设计和训练方法，使得DenseNets可以与现代架构竞争，并在ImageNet-1K上接近最新技术水平。 |
| [^11] | [Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics](https://arxiv.org/abs/2403.19578) | 使用关键动作令牌（KAT）框架，研究展示了文本预训练的变形器（GPT-4 Turbo）在机器人领域可实现视觉模仿学习，将视觉观测映射为模拟示范者行为的动作序列，表现优越于现有的模仿学习方法。 |
| [^12] | [Swarm Characteristics Classification Using Neural Networks](https://arxiv.org/abs/2403.19572) | 本文研究了使用监督神经网络时间序列分类（NN TSC）预测军事背景下群体自主体的关键属性和战术，以及展示了NN TSC在快速推断攻击群体情报方面的有效性。 |
| [^13] | [GrINd: Grid Interpolation Network for Scattered Observations](https://arxiv.org/abs/2403.19570) | GrINd是一种新颖的网络架构，通过傅立叶插值层将离散观测数据映射到高分辨率网格，实现了从稀疏和分散的观测数据中预测时空物理系统演变的能力。 |
| [^14] | [Self-Improved Learning for Scalable Neural Combinatorial Optimization](https://arxiv.org/abs/2403.19561) | 提出一种新颖的自我改进学习(SIL)方法，实现神经组合优化的更好可扩展性，通过自身生成解决方案作为伪标签，设计线性复杂度的注意机制来处理大规模组合优化问题实例。 |
| [^15] | [Croissant: A Metadata Format for ML-Ready Datasets](https://arxiv.org/abs/2403.19546) | Croissant是一种面向机器学习数据集的元数据格式，使数据集更易发现、可移植和互操作，有助于解决ML数据管理和负责任AI中的重要挑战。 |
| [^16] | [Detecting Financial Bots on the Ethereum Blockchain](https://arxiv.org/abs/2403.19530) | 本研究提出了一种利用机器学习检测金融机器人在以太坊平台上的新方法，并建立了金融机器人的分类法，对机器人进行了检测并创建了地面实况数据集。 |
| [^17] | [Model Stock: All we need is just a few fine-tuned models](https://arxiv.org/abs/2403.19522) | 本文提出了一种高效的微调方法，只使用少量模型就能获得优越的性能，通过权重空间和层次加权平均技术超越了现有的模型方法。 |
| [^18] | [Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models](https://arxiv.org/abs/2403.19521) | 通过深入研究Transformer-based语言模型在事实回忆任务中的机制，我们发现了零/少次样本情况下的特定任务头、MLP层和残差流的功能，以及抗过度自信机制。 |
| [^19] | [Maximum Likelihood Estimation on Stochastic Blockmodels for Directed Graph Clustering](https://arxiv.org/abs/2403.19516) | 本文研究了有向图聚类问题，提出了一种基于有向随机块模型的最大似然估计方法，并引入了两种高效且可解释的有向聚类算法。 |
| [^20] | [CDIMC-net: Cognitive Deep Incomplete Multi-view Clustering Network](https://arxiv.org/abs/2403.19514) | 提出了CDIMC-net，一个认知式深度不完整多视图聚类网络，通过结合视图特定深度编码器和图嵌入策略，在框架中捕获每个视图的高级特征和局部结构，从而解决了现有方法中的模型扁平、对噪声或异常值敏感等问题。 |
| [^21] | [Debiasing Cardiac Imaging with Controlled Latent Diffusion Models](https://arxiv.org/abs/2403.19508) | 通过生成合成数据，使用受控潜在扩散模型来消除心脏成像中的偏差和不平衡问题。 |
| [^22] | [SineNet: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations](https://arxiv.org/abs/2403.19507) | SineNet提出了一种通过多个依次连接的U-shaped网络块（波）在解决时间依赖偏微分方程时减少错位特征的方法 |
| [^23] | [Tensor Network-Constrained Kernel Machines as Gaussian Processes](https://arxiv.org/abs/2403.19500) | 本文证明了Canonical Polyadic Decomposition和Tensor Train约束的核机器的输出会在对参数进行i.i.d.先验分布的情况下恢复为高斯过程，并且通过实验证明了Tensor Train模型相对于Canonical Polyadic Decomposition模型具有更多高斯过程行为。 |
| [^24] | [Client-supervised Federated Learning: Towards One-model-for-all Personalization](https://arxiv.org/abs/2403.19499) | 提出了一种新的客户监督联邦学习框架，在联邦学习系统中学习一个强大的全局模型，以实现与个性化模型相媲美的性能。 |
| [^25] | [Regression with Multi-Expert Deferral](https://arxiv.org/abs/2403.19494) | 这是一个回归问题的新框架，涉及将预测推迟给多个专家，提出了单阶段和双阶段情景的全面分析，并引入了新的代理损失函数及其支持的一致性界限。 |
| [^26] | [$H$-Consistency Guarantees for Regression](https://arxiv.org/abs/2403.19480) | 提出了通用的$H$一致性界工具来分析回归问题，并针对平方损失推导出一系列新颖的$H$一致性界；基于对$H$一致性的分析，为对抗性回归提供了有原则的代理损失。 |
| [^27] | [Deep decomposition method for the limited aperture inverse obstacle scattering problem](https://arxiv.org/abs/2403.19470) | 提出了一种用于有限孔径逆障碍散射问题的深度分解方法，通过向神经网络架构提供与散射模型相关联的物理运算符，实现深度学习在逆问题上工作，并避免扭曲解决方案。 |
| [^28] | [Offline Imitation Learning from Multiple Baselines with Applications to Compiler Optimization](https://arxiv.org/abs/2403.19462) | 通过离线多基线模仿学习算法，在编译器优化领域实现了超越标准RL的策略学习效果 |
| [^29] | [Fisher-Rao Gradient Flows of Linear Programs and State-Action Natural Policy Gradients](https://arxiv.org/abs/2403.19448) | 研究了基于Fisher信息矩阵的自然梯度方法在线性规划中的应用，展示了线性收敛性，提出了改进现有结果的熵正则化误差估计，并对扰动的Fisher-Rao梯度流和自然梯度流的次线性收敛性进行了研究。 |
| [^30] | [Transparent and Clinically Interpretable AI for Lung Cancer Detection in Chest X-Rays](https://arxiv.org/abs/2403.19444) | 使用概念瓶颈模型的ante-hoc方法将临床概念引入到分类管道中，提供了肺癌检测决策过程中的有价值见解，相较于基线深度学习模型实现了更好的分类性能（F1 > 0.9）。 |
| [^31] | [Exploiting Individual Graph Structures to Enhance Ecological Momentary Assessment (EMA) Forecasting](https://arxiv.org/abs/2403.19442) | 利用循环和时间图神经网络（GNN）结合图内部关系信息，显著减少了均方误差（MSE）并提升了生态瞬时评估（EMA）数据预测的准确性。 |
| [^32] | [A Novel Stochastic Transformer-based Approach for Post-Traumatic Stress Disorder Detection using Audio Recording of Clinical Interviews](https://arxiv.org/abs/2403.19441) | 提出了一种基于深度学习和随机Transformer的方法，利用临床面谈的音频记录实现了最新水平的创伤后应激障碍检测。 |
| [^33] | [Scaling up ridge regression for brain encoding in a massive individual fMRI dataset](https://arxiv.org/abs/2403.19421) | 该论文评估了不同的并行化技术，以缩短岭回归脑编码模型训练时间，并在大规模深度fMRI数据集上取得了成功。 |
| [^34] | [Fairness in Ranking: Robustness through Randomization without the Protected Attribute](https://arxiv.org/abs/2403.19419) | 提出了一种针对排名后处理的随机化方法，无需受保护属性，通过数值研究显示了方法相对于基线排名的P-公平性和相对于归一化折扣累计增益(NDCG)的有效性的稳健性。 |
| [^35] | [Constants of Motion for Conserved and Non-conserved Dynamics](https://arxiv.org/abs/2403.19418) | 该论文通过机器学习技术得到的动态模型，结合李对称技术分析得到了守恒和非守恒情况下1D和2D谐振子的运动积分，展示了非守恒模型中存在的能量守恒常数，以及在各种频率比例情况下推广了角动量。 |
| [^36] | [Tabular Learning: Encoding for Entity and Context Embeddings](https://arxiv.org/abs/2403.19405) | 挑战常用的序数编码，提出基于字符串相似性编码的表格学习方法，取得了更好的分类效果和性能提升。 |
| [^37] | [Hardness of Learning Boolean Functions from Label Proportions](https://arxiv.org/abs/2403.19401) | 本研究针对从标签比例学习布尔函数的难题性展开研究，发现在特定情况下寻找满足子集合中常数比例的布尔函数的子句是NP难的。 |
| [^38] | [On Uncertainty Quantification for Near-Bayes Optimal Algorithms](https://arxiv.org/abs/2403.19381) | 该论文提出了一种基于常用机器学习算法的近似贝叶斯最优方法，可以恢复由未知任务分布定义的贝叶斯后验，并提出了一种通用的不确定性量化方法。 |
| [^39] | [Artificial Intelligence (AI) Based Prediction of Mortality, for COVID-19 Patients](https://arxiv.org/abs/2403.19355) | 通过研究九种机器学习和深度学习算法，以及两种特征选择方法的结合，本研究发现预测新冠肺炎患者死亡率、ICU需求和通气天数的最后状态中，仅有10个特征对预测具有用处，其中急性肾损伤特征最为重要。 |
| [^40] | [An Interactive Human-Machine Learning Interface for Collecting and Learning from Complex Annotations](https://arxiv.org/abs/2403.19339) | 提出了一种用于二元分类任务的交互式人机学习界面，允许人类注释者利用反事实例来补充标准二元标签，并讨论了未来延伸工作的挑战。 |
| [^41] | [MedBN: Robust Test-Time Adaptation against Malicious Test Samples](https://arxiv.org/abs/2403.19326) | MedBN是一种针对恶意测试样本的新方法，利用中值批次归一化（MedBN）在测试时间推断期间保护模型免受攻击，并成功集成到现有的TTA框架中。 |
| [^42] | [Hypergraph-based Multi-View Action Recognition using Event Cameras](https://arxiv.org/abs/2403.19316) | 提出了一个名为HyperMV的多视角事件动作识别框架，实现了将离散事件数据转换成帧状表示，并利用共享卷积网络提取视角相关特征。 |
| [^43] | [FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth Estimation](https://arxiv.org/abs/2403.19294) | FlowDepth提出了通过Dynamic Motion Flow Module（DMFM）解耦光流，通过运用基于机制的方法解决移动物体引起的不匹配问题，同时使用Depth-Cue-Aware Blur（DCABlur）和Cost-Volume稀疏损失来解决高频和低纹理区域的光度误差不公平问题。 |
| [^44] | [Graph Neural Networks for Treatment Effect Prediction](https://arxiv.org/abs/2403.19289) | 提出了一种图神经网络来减少治疗效果预测所需的训练集大小，有效利用电子商务数据的图结构，为治疗效果预测带来新的可能性 |
| [^45] | [Fine-Tuning Language Models with Reward Learning on Policy](https://arxiv.org/abs/2403.19279) | 提出了在策略上的奖励学习框架，使用策略样本优化奖励模型以保持其分布上的一致性 |
| [^46] | [A Machine Learning Approach for Crop Yield and Disease Prediction Integrating Soil Nutrition and Weather Factors](https://arxiv.org/abs/2403.19273) | 本文提出了一种整合土壤营养和气象因素的机器学习方法，用于作物产量和病害预测，致力于解决孟加拉国农业中作物选择和病害预测中的挑战。 |
| [^47] | [Removing the need for ground truth UWB data collection: self-supervised ranging error correction using deep reinforcement learning](https://arxiv.org/abs/2403.19262) | 提出了一种使用深度强化学习进行自监督测距误差校正的方法，无需收集地面真实数据集，实验表明性能与有监督方法相媲美 |
| [^48] | [Inferring Latent Temporal Sparse Coordination Graph for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2403.19253) | 提出了一种用于多智能体强化学习的潜在时间稀疏协调图，能够有效处理智能体之间的协作关系并利用历史观测来进行知识交换 |
| [^49] | [MPXGAT: An Attention based Deep Learning Model for Multiplex Graphs Embedding](https://arxiv.org/abs/2403.19246) | MPXGAT是一种基于注意力机制的深度学习模型，用于多重图嵌入，通过结合GATs在层内和层间连接的强大性能，实现了准确预测多重网络中各层内和层间的链接。 |
| [^50] | [Sine Activated Low-Rank Matrices for Parameter Efficient Learning](https://arxiv.org/abs/2403.19243) | 整合正弦函数到低秩分解过程中，提高模型准确性的同时保持参数高效性。 |
| [^51] | [AZ-NAS: Assembling Zero-Cost Proxies for Network Architecture Search](https://arxiv.org/abs/2403.19232) | AZ-NAS提出了一种新方法，通过集合各种零成本代理来增强网络排名与性能之间的相关性，在表达性、进步性、可训练性和复杂性等方面进行了分析。 |
| [^52] | [Dual-Personalizing Adapter for Federated Foundation Models](https://arxiv.org/abs/2403.19211) | 提出了一种新的设置，称为测试时间个性化，不仅关注目标本地任务，还延伸到其他展示测试时间个性化的任务 |
| [^53] | [From Activation to Initialization: Scaling Insights for Optimizing Neural Fields](https://arxiv.org/abs/2403.19205) | 本文研究了神经场的初始化和激活之间的关系，强调了网络初始化、架构选择和优化过程之间的深层联系，为神经场的优化提供了理论基础。 |
| [^54] | [Make Large Language Model a Better Ranker](https://arxiv.org/abs/2403.19181) | 本文介绍了一种具有对齐列表排名目标的语言模型框架（ALRO），旨在弥合大型语言模型的能力与推荐系统排名任务的要求之间的差距。 |
| [^55] | [Enhancing Trust and Privacy in Distributed Networks: A Comprehensive Survey on Blockchain-based Federated Learning](https://arxiv.org/abs/2403.19178) | 本文深入研究了基于区块链的联邦学习（BCFL），突出了区块链的安全功能与联邦学习的隐私保护模式之间的协同作用。 |
| [^56] | [Evaluating Fair Feature Selection in Machine Learning for Healthcare](https://arxiv.org/abs/2403.19165) | 通过考虑对所有人口统计群体均等重要性的公平特征选择方法，在医疗保健领域评估算法公平性，确保在减少偏见和全局分类错误之间实现平衡。 |
| [^57] | [D'OH: Decoder-Only random Hypernetworks for Implicit Neural Representations](https://arxiv.org/abs/2403.19163) | 本文提出使用仅运行时解码器的超网络，不依赖离线数据训练，以更好地模拟跨层参数冗余。 |
| [^58] | [Disentangling Length from Quality in Direct Preference Optimization](https://arxiv.org/abs/2403.19159) | 针对直接偏好优化中的长度问题展开研究，揭示了DPO中显著的利用情况，并将其与分布外引导联系起来。 |
| [^59] | [Towards Understanding Dual BN In Hybrid Adversarial Training](https://arxiv.org/abs/2403.19150) | 在混合对抗训练中，分离仿射参数比分离统计数据在模型训练中发挥更重要的作用。 |
| [^60] | [Topological Cycle Graph Attention Network for Brain Functional Connectivity](https://arxiv.org/abs/2403.19149) | 提出了一种新型的拓扑循环图注意力网络（CycGAT），通过循环关联矩阵建立独立循环基础，结合循环图卷积和注意力机制，增强了对大脑功能连接中关键路径的定位能力。 |
| [^61] | [Tiny Graph Neural Networks for Radio Resource Management](https://arxiv.org/abs/2403.19143) | LR-MPGNN模型采用低秩近似技术，显著减少了模型大小和参数数量，取得了巨大的性能提升。 |
| [^62] | [Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM](https://arxiv.org/abs/2403.19114) | EvoEval通过将现有基准演化为不同的目标领域，创建了一个新的程序合成基准套件，以充分评估LLM编码能力。 |
| [^63] | [Synthetic Medical Imaging Generation with Generative Adversarial Networks For Plain Radiographs](https://arxiv.org/abs/2403.19107) | 开发了一种开源合成图像生成工具（GIST），利用生成对抗网络生成高质量合成图像数据，改进了数字健康AI算法，有助于提高诊断准确性、患者护理，并减少医疗纠纷索赔。 |
| [^64] | [Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation](https://arxiv.org/abs/2403.19103) | PRISM是一种算法，可以自动识别人类可解释且易传递的提示，从而有效生成所需概念，仅使用黑盒访问T2I模型。 |
| [^65] | [Optimizing Quantum Convolutional Neural Network Architectures for Arbitrary Data Dimension](https://arxiv.org/abs/2403.19099) | 优化量子卷积神经网络架构，使其能处理任意输入数据维度，并同时优化了辅助量子资源分配 |
| [^66] | [Improving Cancer Imaging Diagnosis with Bayesian Networks and Deep Learning: A Bayesian Deep Learning Approach](https://arxiv.org/abs/2403.19083) | 本文研究了贝叶斯深度学习模型，结合深度学习和贝叶斯网络的优势，最小化各自的劣势，并分析了其在健康产业中图像分类方面的应用。 |
| [^67] | [Enhancing Conformal Prediction Using E-Test Statistics](https://arxiv.org/abs/2403.19082) | 该论文利用E-检验统计引入BB-predictor，增强符合性预测效果。 |
| [^68] | [Tiny Machine Learning: Progress and Futures](https://arxiv.org/abs/2403.19076) | TinyML是一种将深度学习模型压缩到物联网设备和微控制器中实现无处不在智能的新方法，需要共同设计算法和系统堆栈以克服硬件限制。 |
| [^69] | [Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers](https://arxiv.org/abs/2403.19060) | 本文提出了一种人类中心的建筑机器人方法，通过强化学习驱动的助手机器人为木工劳动者提供环境上下文协助，推进了机器人在建筑中的应用。 |
| [^70] | [Equity in Healthcare: Analyzing Disparities in Machine Learning Predictions of Diabetic Patient Readmissions](https://arxiv.org/abs/2403.19057) | 该研究使用机器学习模型在预测糖尿病患者再入院方面实现了公平和准确，其中GBM模型表现出色，在跨不同人口统计因素下实现了平衡的预测结果。 |
| [^71] | [Detecting Generative Parroting through Overfitting Masked Autoencoders](https://arxiv.org/abs/2403.19050) | 本研究提出利用过拟合的遮蔽自编码器(MAE)来检测生成模型中的生成性模仿，建立了基于训练数据集损失的检测阈值，为了确保生成模型的合法使用和提升其法律合规性提供了一种新方法。 |
| [^72] | [Visualizing High-Dimensional Temporal Data Using Direction-Aware t-SNE](https://arxiv.org/abs/2403.19040) | 提出了两个互补的方向感知损失项，强调数据的时间方面，引导优化和结果嵌入以展示可能被忽略的时间模式。 |
| [^73] | [Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data](https://arxiv.org/abs/2403.19031) | 综合实验表明，利用LLMs进行数据增强可以... |
| [^74] | [Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning with Asymmetric Rewards](https://arxiv.org/abs/2403.19024) | 本文扩展了强化学习和控制理论中对称技术的应用范围，通过利用动态对称性学习动力学模型，而不要求奖励具有相同的对称性。 |
| [^75] | [Towards LLM-RecSys Alignment with Textual ID Learning](https://arxiv.org/abs/2403.19021) | 通过提出IDGen，将每个推荐项目表示为独特、简洁、语义丰富的文本ID，从而使得基于大型语言模型的推荐更好地与自然语言生成对齐。 |
| [^76] | [Thelxino\"e: Recognizing Human Emotions Using Pupillometry and Machine Learning](https://arxiv.org/abs/2403.19014) | 该研究提出了一种使用瞳孔测量来识别 VR 中情绪的方法，通过特征工程和梯度提升模型，在 Thelxino\"e 框架中取得了98.8%的高准确率，为发展更具沉浸感和互动性的 VR 环境开辟了新的途径。 |
| [^77] | [Sequential Inference of Hospitalization ElectronicHealth Records Using Probabilistic Models](https://arxiv.org/abs/2403.19011) | 这项工作设计了一个概率无监督模型，用于推断医院电子健康记录中的多个任意长度序列，可以捕捉复杂的关系，并且可以在原始数据上训练。 |
| [^78] | [Towards Sustainable SecureML: Quantifying Carbon Footprint of Adversarial Machine Learning](https://arxiv.org/abs/2403.19009) | 本文首次探究了对抗机器学习的碳足迹，并引入了Robustness Carbon Trade-off Index（RCTI），该指标捕捉了碳排放对抗性稳健性变化的敏感性。 |
| [^79] | [Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems](https://arxiv.org/abs/2403.18998) | 提出了针对微服务系统的少样本异常跟踪分类的新框架，利用多头注意力自编码器构建系统特定的跟踪表示，并应用基于Transformer编码器的模型无关元学习进行高效分类。 |
| [^80] | [Causal-StoNet: Causal Inference for High-Dimensional Complex Data](https://arxiv.org/abs/2403.18994) | 本文提出了一种基于深度学习技术的新颖因果推断方法，用于处理高维复杂数据。 |
| [^81] | [Robustness and Visual Explanation for Black Box Image, Video, and ECG Signal Classification with Reinforcement Learning](https://arxiv.org/abs/2403.18985) | 提出了一个通用的强化学习框架，用于从心电图信号分析到图像和视频分类的不同模型类型的对抗攻击，通过识别敏感区域并在最小程度扭曲下诱导错误分类，生成优越的定位掩模，并在鲁棒性和透明度方面取得了显著进展。 |
| [^82] | [TextCraftor: Your Text Encoder Can be Image Quality Controller](https://arxiv.org/abs/2403.18978) | 通过微调文本编码器来改进文本到图像扩散模型的性能。 |
| [^83] | [A Survey on Large Language Models from Concept to Implementation](https://arxiv.org/abs/2403.18969) | Transformer模型在改革传统任务和推进跨行业研究和开发中产生革命性影响。 |
| [^84] | [LORD: Large Models based Opposite Reward Design for Autonomous Driving](https://arxiv.org/abs/2403.18965) | LORD通过不期望的语言目标，提出了一种基于大模型的相反奖励设计，以便有效利用大型预训练模型作为零-shot奖励模型。 |
| [^85] | [Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models](https://arxiv.org/abs/2403.18957) | 该研究旨在调查不安全用户生成内容游戏中的违法推广威胁，收集了一组包含性暴力和暴力内容的真实图像数据集。 |
| [^86] | [Structurally Prune Anything: Any Architecture, Any Framework, Any Time](https://arxiv.org/abs/2403.18955) | SPA是一个多功能的结构剪枝框架，可以对任何架构、任何框架的神经网络进行剪枝，而且可以在任何训练阶段进行剪枝。 |
| [^87] | [Hybridizing Traditional and Next-Generation Reservoir Computing to Accurately and Efficiently Forecast Dynamical Systems](https://arxiv.org/abs/2403.18953) | 介绍了一种将传统水库计算和下一代水库计算相结合的方法，用于精确预测复杂和混沌动力系统的时间序列，当单独使用RC和NGRC组件不足以满足时，混合方法具有显著优势。 |
| [^88] | [Self-Supervised Interpretable Sensorimotor Learning via Latent Functional Modularity](https://arxiv.org/abs/2403.18947) | MoNet是一种结合端到端学习和模块化网络架构的方法，通过认知引导的对比损失函数，在潜在空间中高效学习任务特定的决策过程，而无需任务级别的监督，同时提高端到端推断的可解释性，实现了在实际环境中的有效视觉自主导航。 |
| [^89] | [Random Aggregate Beamforming for Over-the-Air Federated Learning in Large-Scale Networks](https://arxiv.org/abs/2403.18946) | 本文提出了一种随机聚合波束成形的方案，通过随机抽样生成聚合器波束形成矢量，以解决大规模网络中的聚合误差最小化和设备选择最大化问题。 |
| [^90] | [Optimizing Wireless Networks with Deep Unfolding: Comparative Study on Two Deep Unfolding Mechanisms](https://arxiv.org/abs/2403.18930) | 本论文进行了关于两种深度展开机制的比较研究，提出了一个半展开深度学习模型和一个完全展开的深度学习模型，以高效进行无线网络中的功率控制。 |
| [^91] | [A Review of Neuroscience-Inspired Machine Learning](https://arxiv.org/abs/2403.18929) | 该论文综述了神经科学启发的机器学习领域，重点讨论了通过生物可信的信用指派方案进行学习的优势，包括兼容性广泛、能效高、适用于各种学习条件和硬件、以及实时自适应神经形态处理系统的发展。 |
| [^92] | [Enhancing Efficiency in Sparse Models with Sparser Selection](https://arxiv.org/abs/2403.18926) | 提出了一种新颖的MoE模型\tool，通过利用小型专家和基于阈值的路由器，使标记能够选择性地仅涉及到必要的参数，从而在减少MoE层计算负载50%以上的同时提高模型性能。 |
| [^93] | [Nature-Guided Cognitive Evolution for Predicting Dissolved Oxygen Concentrations in North Temperate Lakes](https://arxiv.org/abs/2403.18923) | 提出了一种自然引导的认知进化策略，通过多层融合自适应学习和自然过程，有效预测北温带湖泊中的溶解氧浓度 |
| [^94] | [SMOF: Streaming Modern CNNs on FPGAs with Smart Off-Chip Eviction](https://arxiv.org/abs/2403.18921) | 本文介绍了一种在FPGA上使用智能离片驱逐流模式CNN的方法，通过引入权重和激活的驱逐机制来解决现代拓扑结构在芯片内存使用方面的性能限制。 |
| [^95] | [PLOT-TAL -- Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization](https://arxiv.org/abs/2403.18915) | 提出了使用最优传输进行少样本时序动作定位的提示学习方法，通过多提示学习框架和最优传输理论的结合，有效地捕捉通用特征和减轻过拟合风险 |
| [^96] | [A Geometric Explanation of the Likelihood OOD Detection Paradox](https://arxiv.org/abs/2403.18910) | 高似然区域将不会被生成如果它们包含最小概率质量，基于此观察提出了一种通过本地固有维度估计进行离群检测的方法 |
| [^97] | [Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning](https://arxiv.org/abs/2403.18886) | 提出了一种名为SEMA的新型微调方法，旨在通过自我扩展预训练模型与模块化适配，实现持续学习过程中的最小遗忘，解决先前针对静态模型架构情况下存在的过多参数分配或适应性不足等问题。 |
| [^98] | [AIC-UNet: Anatomy-informed Cascaded UNet for Robust Multi-Organ Segmentation](https://arxiv.org/abs/2403.18878) | 引入了一种新方法，在任何现有编码-解码分割模型上，通过条件化模型预测与可学习的解剖先验来施加解剖约束。 |
| [^99] | [Predicting risk of cardiovascular disease using retinal OCT imaging](https://arxiv.org/abs/2403.18873) | 这项研究探讨了使用光学相干断层扫描（OCT）作为额外成像技术来预测心血管疾病的潜力，并通过自监督深度学习和随机森林分类器结合的方法成功区分了心血管疾病风险和非风险患者。 |
| [^100] | [Targeted Visualization of the Backbone of Encoder LLMs](https://arxiv.org/abs/2403.18872) | 本文研究了将DeepView方法应用于自然语言处理领域，以减少编码器模型存在的风险并解释模型决策过程。 |
| [^101] | [Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification](https://arxiv.org/abs/2403.18871) | 提出了一种模板引导方法，将气胸的临床知识融入XAI方法，以过滤掉落在模板之外的不相关解释。 |
| [^102] | [SugarcaneNet2024: An Optimized Weighted Average Ensemble Approach of LASSO Regularized Pre-trained Models for Sugarcane Disease Classification](https://arxiv.org/abs/2403.18870) | SugarcaneNet2024是通过优化加权平均集成LASSO正则化的预训练模型，在甘蔗病害分类中表现出色，具有快速准确的检测能力。 |
| [^103] | [Graph Bayesian Optimization for Multiplex Influence Maximization](https://arxiv.org/abs/2403.18866) | 本文提出了使用多层扩散模型和信息关联机制解决多重影响最大化问题的方法 |
| [^104] | [Interpretable Machine Learning for Weather and Climate Prediction: A Survey](https://arxiv.org/abs/2403.18864) | 可解释的机器学习技术对于增强天气和气候建模的可信度和实用性至关重要，包括后验可解释性技术和从头设计的固有可解释模型。 |
| [^105] | [Directed Criteria Citation Recommendation and Ranking Through Link Prediction](https://arxiv.org/abs/2403.18855) | 该研究提出使用链接预测作为自动展示相关文献的方法，并通过模型生成的语义表示在推荐和排名任务上取得优越性能。 |
| [^106] | [Spatio-seasonal risk assessment of upward lightning at tall objects using meteorological reanalysis data](https://arxiv.org/abs/2403.18853) | 该研究利用随机森林分析了在高耸物体处测得的上升闪电与35个较大尺度气象变量之间的关系，发现较大尺度的向上速度、10米高度的风速和风向以及云物理变量对UL风险评估贡献最大，进而预测了研究区域UL的风险。 |
| [^107] | [The Blind Normalized Stein Variational Gradient Descent-Based Detection for Intelligent Massive Random Access](https://arxiv.org/abs/2403.18846) | 提出了一种基于盲归一化斯坦变分梯度下降的检测器，用于解决智能大规模随机接入中的前导碰撞问题，并通过开发改进Hadamard变换和设计块MHT层来提高检测性能。 |
| [^108] | [JEP-KD: Joint-Embedding Predictive Architecture Based Knowledge Distillation for Visual Speech Recognition](https://arxiv.org/abs/2403.18843) | 该论文提出了一种基于联合嵌入预测架构的知识蒸馏方法JEP-KD，旨在通过引入生成网络在嵌入层内增强视频编码器的语义特征提取能力，从而更有效地利用音频特征，逐步减少视觉语音识别与自动语音识别之间的性能差距。 |
| [^109] | [Feynman Diagrams as Computational Graphs](https://arxiv.org/abs/2403.18840) | 该论文提出了一种用于量子场论中高阶费曼图的计算图表示方法，通过组织成张量操作的分形结构显著减少计算冗余，集成了Taylor-mode自动微分技术，开发了费曼图编译器以优化计算图。 |
| [^110] | [Long Short-Term Memory Pattern Recognition in Currency Trading](https://arxiv.org/abs/2403.18839) | 本研究通过分析金融市场的Wyckoff阶段框架，探讨了累积模式和交易范围等阶段的重要性，揭示了如何利用LSTM模型分析市场数据以预测价格走势和做出明智决策。 |
| [^111] | [A New Method for Sensorless Estimation of the Speed and Position in Brushed DC Motors Using Support Vector Machines](https://arxiv.org/abs/2403.18833) | 使用支持向量机在无刷直流电机中开发出一种新方法，基于电流纹波进行速度和位置估计，通过检测脉冲来估计速度并计数来估计位置，能够检测鬼脉冲和丢弃虚假脉冲，具有较高的准确性和可靠性。 |
| [^112] | [Bridging Generative Networks with the Common Model of Cognition](https://arxiv.org/abs/2403.18827) | 通过将模块重构为影子生成系统，实现了认知架构与生成神经网络的无缝连接 |
| [^113] | [Enhancing Financial Data Visualization for Investment Decision-Making](https://arxiv.org/abs/2403.18822) | LSTM网络在金融数据预测中的应用，利用多个特征提升模型捕捉复杂模式，通过可视化关键属性揭示微妙差异，采用25天时间步长内的输入结构捕捉时间上的复杂性 |
| [^114] | [ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation](https://arxiv.org/abs/2403.18807) | 通过使用预训练的ViT模型生成的全局图像先验，为单图深度估计模型提供更详细的上下文信息，并提出了一种新的使用扩散骨干且受ViT嵌入条件约束的深度估计模型。 |
| [^115] | [Fpga-Based Neural Thrust Controller for UAVs](https://arxiv.org/abs/2403.18703) | 本研究探索了将 FPGA 作为一种解决方案，通过实现神经网络控制器以提高无人机适应性和性能，尤其在未知环境中。 |
| [^116] | [Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models](https://arxiv.org/abs/2403.18159) | 通过信号传播分析，提出了一种改进大型语言模型的量化知识蒸馏方法，并提供了ov-freeze稳定KD-QAT过程的洞察。 |
| [^117] | [Predicting species occurrence patterns from partial observations](https://arxiv.org/abs/2403.18028) | 提出了一个问题，即采用卫星图像和其他物种出现信息来预测物种出现模式，并提出了一个通用模型R-Tran，可以利用部分观测数据进行预测，优于其他方法。 |
| [^118] | [Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER](https://arxiv.org/abs/2403.18025) | 提出了Mask Specific Language Modeling（MSLM）方法来改善LM在微调过程中对目标领域知识的敏感性，通过加权领域特定术语的重要性进行学习。 |
| [^119] | [DORE: A Dataset For Portuguese Definition Generation](https://arxiv.org/abs/2403.18018) | DORE是第一个用于葡萄牙语的定义生成数据集，填补了这一领域的空白，包含超过10万个定义，并评估了多种基于深度学习的模型。 |
| [^120] | [LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning](https://arxiv.org/abs/2403.17919) | 逐层重要性采样的新方法LISA在微调任务中表现出色，记忆成本低且优于传统方法。 |
| [^121] | [Fake or JPEG? Revealing Common Biases in Generated Image Detection Datasets](https://arxiv.org/abs/2403.17608) | 许多AI生成图像检测数据集存在与JPEG压缩和图像大小相关的偏见，去除这些偏见可以显著提高对JPEG压缩的稳健性并显著改变检测器的跨生成器性能。 |
| [^122] | [Expectations Versus Reality: Evaluating Intrusion Detection Systems in Practice](https://arxiv.org/abs/2403.17458) | 论文通过实证比较不同入侵检测系统，发现最佳解决方案取决于外部变量，如攻击类型、复杂性和网络环境，深度神经网络在某些数据集上表现最佳，但并非始终是最佳选择。 |
| [^123] | [CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions](https://arxiv.org/abs/2403.17210) | 通过CADGL框架，利用上下文感知深度图学习来预测药物-药物相互作用，解决了现有DDI预测模型在泛化、特征提取和现实应用方面的挑战 |
| [^124] | [Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy](https://arxiv.org/abs/2403.16591) | 论文探讨了本地差分隐私、贝叶斯隐私及其之间的相互关系，揭示了关于效用-隐私权衡的新见解，并提出了一个框架来突出攻击和防御策略的相互作用和效果。 |
| [^125] | [DeepMachining: Online Prediction of Machining Errors of Lathe Machines](https://arxiv.org/abs/2403.16451) | DeepMachining是一种基于深度学习的AI系统，可以在线预测车床机床加工操作的误差，通过预训练和微调模型，实现了高准确性预测，是首批使用预训练深度学习模型预测车床机床加工误差的工厂实验之一。 |
| [^126] | [Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices](https://arxiv.org/abs/2403.15905) | 提出了面向资源受限设备的低能耗自适应个性化框架目标块微调，根据数据漂移类型微调不同模块以实现最佳性能和降低能源消耗。 |
| [^127] | [Insights into the Lottery Ticket Hypothesis and the Iterative Magnitude Pruning](https://arxiv.org/abs/2403.15022) | 通过对迭代幅度剪枝过程中不同阶段获得的解决方案的体积/几何和损失景观特征进行经验研究，我们试图洞察走势彩票假设和迭代幅度剪枝中的现象。 |
| [^128] | [Detoxifying Large Language Models via Knowledge Editing](https://arxiv.org/abs/2403.14472) | 本文研究了使用知识编辑技术对大型语言模型进行去毒化，在构建了SafeEdit基准的基础上，提出了一种简单而有效的方法 DINM，可以通过少量调整步骤减少模型的毒性，同时对各种去毒方法的内部机制进行了深入分析。 |
| [^129] | [SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks](https://arxiv.org/abs/2403.14302) | 提出了一种新型脉冲自注意机制DSSA以及结合ResNet的多阶段架构的SpikingResformer架构，旨在改善性能和能效，并减少参数。 |
| [^130] | [When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings](https://arxiv.org/abs/2403.12984) | 将药物SMILES字符串视为句子并利用文本分类方法进行药物分类，证实了通过简单的自然语言处理方法解决复杂问题的可能性 |
| [^131] | [Policy Bifurcation in Safe Reinforcement Learning](https://arxiv.org/abs/2403.12847) | 我们的研究发现在安全强化学习中可能存在策略分叉现象，提出了拓扑分析以证明在一些情景下，策略需要具有不连续性或多值性，这对应于障碍物自由状态空间为非单连通时需要策略分叉的情况。 |
| [^132] | [Zeolite Adsorption Property Prediction using Deep Learning](https://arxiv.org/abs/2403.12659) | 通过深度学习模型，本研究提出了一个比传统分子模拟快4到5个数量级的方法来预测沸石的吸附性能，并可以用于识别吸附位点。 |
| [^133] | [ROUTERBENCH: A Benchmark for Multi-LLM Routing System](https://arxiv.org/abs/2403.12031) | 提出了ROUTERBENCH，一个用于评估LLM路由系统性能的基准测试框架，包括超过405k推理结果的数据集，以支持路由策略的开发。 |
| [^134] | [Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates](https://arxiv.org/abs/2403.11687) | 在非光滑设置下，提出了用于计算具有内映射的外映射固定点的隐式导数的新方法NSID，并提供了确定性情况下迭代微分（ITD）和近似隐式微分（AID）的改进线性收敛速率。 |
| [^135] | [Dual-Channel Multiplex Graph Neural Networks for Recommendation](https://arxiv.org/abs/2403.11624) | 该研究提出了一种名为双通道多重图神经网络（DCMGNN）的新型推荐框架，能够有效解决现有推荐方法中存在的多通路关系行为模式建模和对目标关系影响忽略的问题。 |
| [^136] | [Covid-19 detection from CT scans using EfficientNet and Attention mechanism](https://arxiv.org/abs/2403.11505) | 开发了一个基于深度学习模型的管道，结合EfficientNet和注意力机制，用于从肺部CT扫描图像中检测COVID-19，并在竞赛数据集验证集上表现优异 |
| [^137] | [FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation](https://arxiv.org/abs/2403.08059) | FluoroSAM是用于X光图像的分割的语言对齐基础模型，提供了一种在X光成像领域具有广泛适用性的自动图像分析工具。 |
| [^138] | [CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control](https://arxiv.org/abs/2403.07728) | CAS框架允许在在线选择性预测中控制FCR，通过自适应选择和校准集构造输出符合预测区间 |
| [^139] | [Can Small Language Models be Good Reasoners for Sequential Recommendation?](https://arxiv.org/abs/2403.04260) | 提出了逐步知识提取框架（SLIM），为顺序推荐系统解决了大型语言模型（LLMs）高资源需求的难题，使其能以资源高效的方式享受LLMs的出色推理能力。 |
| [^140] | [OpenGraph: Towards Open Graph Foundation Models](https://arxiv.org/abs/2403.01121) | 该论文旨在通过开发一个通用图基础模型，以解决现有图神经网络在泛化到与训练数据显著不同的未见图数据时遇到的困难。 |
| [^141] | [Deep Reinforcement Learning: A Convex Optimization Approach](https://arxiv.org/abs/2402.19212) | 本文提出了一种深度强化学习的凸优化方法，通过每集使用凸优化来训练神经网络近似最优$Q$-函数，确保收敛参数可以无限接近最优参数。 |
| [^142] | [Informed Meta-Learning](https://arxiv.org/abs/2402.16105) | 该研究提出了通知元学习这一新范式，旨在通过人类和机器之间的跨任务知识共享，提高数据效率和抵御观测噪声。 |
| [^143] | [Imbalanced Data Clustering using Equilibrium K-Means](https://arxiv.org/abs/2402.14490) | Equilibrium K-Means（EKM）是一种新颖且简单的K均值类型算法，通过减少聚类中心在大类簇中心聚集的倾向，显著改善了不平衡数据的聚类结果。 |
| [^144] | [HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?](https://arxiv.org/abs/2402.11815) | 提出了一种基于对比学习的单一模型，用较少的参数实现与基线相当的机器生成文本检测性能 |
| [^145] | [Brant-2: Foundation Model for Brain Signals](https://arxiv.org/abs/2402.10251) | Brant-2是脑信号领域最大的基础模型，相比于Brant，它不仅对数据变化和建模尺度具有稳健性，还能适用于更广泛范围的脑神经数据。 |
| [^146] | [PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models](https://arxiv.org/abs/2402.08714) | 本研究提出了PRDP方法，通过近端奖励差异预测实现了稳定的黑盒奖励微调扩散模型，能够在大规模提示数据集上进行训练，并且具有更好的训练稳定性。 |
| [^147] | [Re-Envisioning Command and Control](https://arxiv.org/abs/2402.07946) | 重新构想的论文提出了未来指挥与控制（C2）决策需要面对更复杂和挑战性的环境，因此提出了基于人工智能系统与人类强有力伙伴关系的未来C2的愿景。这个愿景的核心是优化C2操作流程，保持协同努力，发展自适应的集体知识系统。 |
| [^148] | [Scalable Interactive Machine Learning for Future Command and Control](https://arxiv.org/abs/2402.06501) | 未来战争将需要指挥与控制（C2）人员在复杂且潜在模糊的情况下以更短的时间内做出决策。本论文通过利用互动式机器学习方法，结合人工智能和人类智能，以提高C2运作的适应性和效率。 |
| [^149] | [TopoNav: Topological Navigation for Efficient Exploration in Sparse Reward Environments](https://arxiv.org/abs/2402.04061) | TopoNav是一种拓扑导航框架，它通过主动拓扑映射、内部奖励机制和层次化目标优先级的组合来实现在稀疏奖励环境中高效探索。 |
| [^150] | [COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations](https://arxiv.org/abs/2402.01786) | COA-GPT是一种利用大型语言模型快速高效生成有效行动方案的算法，它融合了军事学说和领域专业知识，并在军事游戏中的实验中展示了其快速生成战略合理COAs的优势。 |
| [^151] | [HQ-VAE: Hierarchical Discrete Representation Learning with Variational Bayes](https://arxiv.org/abs/2401.00365) | HQ-VAE提出了一种统一框架，利用变分贝叶斯在分层结构中随机学习离散表示，解决了传统VQ-VAE中的码书/层坍塌问题。 |
| [^152] | [SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution](https://arxiv.org/abs/2312.11598) | SkillDiffuser通过将可解释的技能学习与条件扩散规划相结合，实现了在高层指令下生成连贯轨迹的分层规划。 |
| [^153] | [Do Similar Entities have Similar Embeddings?](https://arxiv.org/abs/2312.10370) | 本文挑战了实体相似性在图中在嵌入空间中自然反映的主流假设，通过进行广泛的实验来衡量这种关系。 |
| [^154] | [Open Datasheets: Machine-readable Documentation for Open Datasets and Responsible AI Assessments](https://arxiv.org/abs/2312.06153) | 本文介绍了一种面向开放数据集和负责任人工智能评估的机器可读文档框架，旨在提高数据集的可理解性和可用性，简化数据集评估过程，促进更可靠的数据应用，从而培育更负责任和可信赖的人工智能系统。 |
| [^155] | [MMM: Generative Masked Motion Model](https://arxiv.org/abs/2312.03596) | MMM 提出了一种基于遮蔽运动模型的新颖运动生成范式，通过运动标记器和条件遮蔽运动变换器，在实时性能、高保真度和运动可编辑性之间取得平衡。 |
| [^156] | [HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces](https://arxiv.org/abs/2312.03160) | HybridNeRF方法将大多数对象呈现为表面，仅对少部分具有挑战性的区域进行体积建模，实现了神经渲染的高效率。 |
| [^157] | [Solution-Set Geometry and Regularization Path of a Nonconvexly Regularized Convex Sparse Model](https://arxiv.org/abs/2311.18438) | sGMC模型作为LASSO的非凸正则化替代品，在保留LASSO模型优势的同时，其解集几何、解唯一性和稀疏性与LASSO模型具有相似且优雅的特性。 |
| [^158] | [Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models](https://arxiv.org/abs/2311.13628) | 提示风险控制是一个轻量级框架，通过严格的信息风险度量族的上限选取提示，帮助减轻大型语言模型负责部署过程中产生意外糟糕响应的风险。 |
| [^159] | [PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF](https://arxiv.org/abs/2311.13099) | 该研究展示了物理学模拟与 NeRF 结合，无需中间形态代理，通过 Q-GMLS 捕捉非线性动力学和大形变，实现了高质量弹性动力学生成，并适应 NeRF 密度场调整最小二乘核，从而高效合成各种高弹性材料的物理逼真动画。 |
| [^160] | [Continual Learning: Applications and the Road Forward](https://arxiv.org/abs/2311.11908) | 连续学习是机器学习的子领域，致力于让机器学习模型在新数据上不断学习，而不忘记过去学到的知识。研究揭示了内存限制场景的主导地位，并讨论了连续学习在解决模型编辑、个性化、专业化、设备端学习、快速（重新）训练和强化学习等问题中的作用。 |
| [^161] | [Towards probabilistic Weather Forecasting with Conditioned Spatio-Temporal Normalizing Flows](https://arxiv.org/abs/2311.06958) | 该论文提出了一种基于条件化归一化流的概率天气预测方法，通过实验证明其能够捕捉和良好外推空间-时间相关性。 |
| [^162] | [Real-Time Recurrent Reinforcement Learning](https://arxiv.org/abs/2311.04830) | 本文提出了实时递归强化学习（RTRRL）方法，通过结合元-强化学习RNN架构、外部强化学习算法和RFLO局部在线学习，成功解决部分可观察马尔可夫决策过程中的离散和连续控制任务。实验结果表明，在计算复杂性相当的情况下，使用BPTT或RTRL替代RTRRL中的优化算法并不能提高回报。 |
| [^163] | [What's in a Prior? Learned Proximal Networks for Inverse Problems](https://arxiv.org/abs/2310.14344) | 提供了一个框架来发展学习的近端网络，证明它们提供了确切的proximal操作符。 |
| [^164] | [Integration of Graph Neural Network and Neural-ODEs for Tumor Dynamic Prediction](https://arxiv.org/abs/2310.00926) | 提出了使用图神经网络与神经-ODE整合的方法用于肿瘤动态预测，有效整合了多种数据信息，有助于增强个性化肿瘤预测。 |
| [^165] | [To Whom are You Talking? A Deep Learning Model to Endow Social Robots with Addressee Estimation Skills](https://arxiv.org/abs/2308.10757) | 通过深度学习模型，这项研究解决了社交机器人在生态场景中理解说话者对象的问题。 |
| [^166] | [Machine Learning-Powered Combinatorial Clock Auction](https://arxiv.org/abs/2308.10226) | 本文提出了一种机器学习驱动的组合时钟拍卖，通过仅使用需求查询而不是价值查询来获取投标人的偏好信息。 |
| [^167] | [A stability theorem for bigraded persistence barcodes](https://arxiv.org/abs/2303.14694) | 使用矩角复合体的同调概念定义了双分级持久性条码，并证明了它们的稳定性定理 |
| [^168] | [Toward a Theory of Causation for Interpreting Neural Code Models](https://arxiv.org/abs/2302.03788) | 该论文介绍了一种名为$do_{code}$的后验解释方法，用于解释神经代码模型的预测，基于因果推断，旨在实现面向编程语言的解释。 |
| [^169] | [Towards Enhancing Time Series Contrastive Learning: A Dynamic Bad Pair Mining Approach](https://arxiv.org/abs/2302.03357) | 提出了一种动态坏样本挖掘（DBPM）算法，可靠地识别和抑制时间序列对比学习中的噪声正样本对和错误正样本对。 |
| [^170] | [Optimal Transport Perturbations for Safe Reinforcement Learning with Robustness Guarantees](https://arxiv.org/abs/2301.13375) | 引入了基于最优输运扰动的安全强化学习框架，通过构建最坏情况的虚拟状态转换以提升鲁棒性能和安全性。 |
| [^171] | [SOLD: Sinhala Offensive Language Dataset](https://arxiv.org/abs/2212.00851) | 本文介绍了一种新的低资源语言——僧伽罗语攻击性语言识别数据集(SOLD)，填补了目前攻击性语言识别研究局限于高资源语言的空白。 |
| [^172] | [Bi-objective Ranking and Selection Using Stochastic Kriging](https://arxiv.org/abs/2209.03919) | 提出了一种使用随机克里金的贝叶斯双目标排序和选择方法，以减少在识别具有最佳期望性能解时的错误分类 |
| [^173] | [Efficient Deep Learning-based Estimation of the Vital Signs on Smartphones](https://arxiv.org/abs/2204.08989) | 提出了一种通过深度学习的端到端解决方案，用于在智能手机上高效估计生命体征，消除了繁琐的预处理步骤。 |
| [^174] | [Channel Estimation via Successive Denoising in MIMO OFDM Systems: A Reinforcement Learning Approach](https://arxiv.org/abs/2101.10300) | 提出了一种基于强化学习框架的频域信道估计去噪方法，通过逐步信道去噪过程和信道曲率计算来识别不可靠信道估计。 |
| [^175] | [Random Vector Functional Link Networks for Function Approximation on Manifolds](https://arxiv.org/abs/2007.15776) | 本文提供了对于连续函数在紧致域上的通用逼近器的严格证明，填补了单层神经网络随机向量功能链接网络在实践中成功的理论缺口 |
| [^176] | [A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding.](http://arxiv.org/abs/2401.10746) | 本研究系统评估了使用深度学习和欧几里得对齐对脑电解码的影响。结果表明，欧几里得对齐能够显著提高解码率，并且减少了收敛时间。 |
| [^177] | [A Comprehensive Study of Knowledge Editing for Large Language Models.](http://arxiv.org/abs/2401.01286) | 本研究全面研究了大型语言模型的知识编辑，旨在有效修改模型的行为，同时保持整体性能。 |
| [^178] | [Partial Label Learning with a Partner.](http://arxiv.org/abs/2312.11034) | 本文介绍了一种在部分标签学习中识别和纠正错误标记样本的方法，通过引入合作伙伴分类器和互相监督范式，以及使用模糊机制实现分类器之间的协作。 |
| [^179] | [A Study on the Calibration of In-context Learning.](http://arxiv.org/abs/2312.04021) | 本研究关注上下文学习（ICL），通过定制提示来调整静态语言模型（LMs），研究了在各种自然语言理解和推理任务中性能和校准之间的平衡。研究发现随着ICL示例数量的增加，模型的校准会先增加而后得到改善，而校准误差主要出现在低样本场景下。此外，微调和CoT提示等方法可能导致校准误差和不可靠的自然语言解释，提示需要针对可靠性场景开发新的方法。 |
| [^180] | [Conditions on Preference Relations that Guarantee the Existence of Optimal Policies.](http://arxiv.org/abs/2311.01990) | 我们引入了直接偏好过程框架，通过对偏好的序结构进行分析，我们提出了保证最优策略存在的条件。这个研究缩小了学习偏好反馈算法在理论和实践之间的差距，并提供了最优策略存在的证明。 |
| [^181] | [ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer's Disease.](http://arxiv.org/abs/2310.15301) | ADMarker是一个多模式联邦学习系统，用于在自然生活环境中监测阿尔茨海默病的数字生物标志物。它具有新颖的联邦学习架构，能够准确检测出数字生物标志物，并在临床试验中展示出高准确率和早期AD识别能力。 |
| [^182] | [Learning to Solve Climate Sensor Placement Problems with a Transformer.](http://arxiv.org/abs/2310.12387) | 本文介绍了一种使用深度强化学习方法学习改进传感器布放策略的新方法，通过与其他方法的对比实验证明了该方法在产生高质量解决方案方面的有效性和优越性。 |
| [^183] | [Few-Shot Learning Patterns in Financial Time-Series for Trend-Following Strategies.](http://arxiv.org/abs/2310.10500) | 这项研究提出了一种在金融时间序列中使用少样本学习的趋势跟踪预测器，能够快速适应新的市场条件，并获得较高的夏普比率。 |
| [^184] | [Non-Redundant Graph Neural Networks with Improved Expressiveness.](http://arxiv.org/abs/2310.04190) | 本文提出了一种非冗余图神经网络的改进表达能力方法，通过基于邻域树的聚合方案减少冗余，提高了表达能力，实验证明了它对于减轻过度压缩的有效性。 |
| [^185] | [Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection.](http://arxiv.org/abs/2310.02861) | 《Rayleigh Quotient Graph Neural Networks用于图级异常检测的研究》提出使用Rayleigh Quotient作为驱动因素，通过探索图的固有光谱特征来实现图级异常检测。 |
| [^186] | [SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping.](http://arxiv.org/abs/2310.01201) | SWoTTeD是一种扩展的张量分解方法，用于发现复杂时间模式下的隐藏表征。在实验中，SWoTTeD不仅能与最新的基于张量分解的方法一样准确地重建数据，还能提取出对临床医生有意义的时间表征。 |
| [^187] | [On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters.](http://arxiv.org/abs/2309.17053) | 这项研究探讨了图神经网络（GNN）表达能力和$k$维Weisfeiler-Leman ($k$WL)测试之间的关系，研究发现了$k$WL测试可以有效区分具有不同出现次数的模式图$P$的图形，并研究了模式图计数问题的最小维度$k$。 |
| [^188] | [GPSINDy: Data-Driven Discovery of Equations of Motion.](http://arxiv.org/abs/2309.11076) | GPSINDy是一种数据驱动的方法，通过将高斯过程回归与SINDy相结合，能够从噪声数据中发现非线性动力学系统模型，并在实验证明了其在系统动态和预测未来轨迹方面的改进性能。 |
| [^189] | [Differentiable Turbulence.](http://arxiv.org/abs/2307.03683) | 深度学习与物理启发式选择的深度学习架构相结合，实现了可微分湍流模型，在大涡模拟中有效地建模子网格尺度湍流，并能推广到多种流动配置。 |
| [^190] | [Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion.](http://arxiv.org/abs/2307.02496) | 本研究利用可逆神经网络和误差扩散方法，通过测量气泡引起的磁场波动，重建电解过程中的气泡分布和电导率图，并实现了比传统方法更优异的性能。 |
| [^191] | [Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting.](http://arxiv.org/abs/2306.17563) | 本论文提出了一种名为PRP的新技术，通过使用两两排名提示来显著减轻大型语言模型（LLM）的负担，并首次在标准基准测试中实现了最先进的排名性能。 |
| [^192] | [Differentially Private Distributed Estimation and Learning.](http://arxiv.org/abs/2306.15865) | 本文研究了在网络环境中的分布式估计和学习问题，通过交换私有观测信息，代理可以集体估计未知数量，而保护隐私。通过线性聚合方案和差分隐私（DP）调整的随机化方案，本研究提出了一种能够在保证隐私的同时高效组合观测数据的算法。 |
| [^193] | [Targeted collapse regularized autoencoder for anomaly detection: black hole at the center.](http://arxiv.org/abs/2306.12627) | 本文提出一种在自编码器的损失函数中添加一个轻量级的约束项，用于解决传统自编码器在异常检测中的不足，并取得了良好的表现。 |
| [^194] | [Quantum machine learning for image classification.](http://arxiv.org/abs/2304.09224) | 本论文提出了两种混合量子-经典的神经网络模型用于图像分类，其中包括一个具有并行量子层的神经网络和一个具有量子卷积层的神经网络。其中一个混合量子方法在MNIST数据集上展现了超过99%的惊人准确率。 |
| [^195] | [Evaluating gesture-generation in a large-scale open challenge: The GENEA Challenge 2022.](http://arxiv.org/abs/2303.08737) | 本文介绍了GENEA Challenge 2022的研究结果，该比赛旨在基准测试基于数据驱动的自动共同语言手势生成。使用具有相同语音和动作的数据集，众多参赛团队的手势生成系统在几个大型用户研究中得到了评估，因此能够进行直接比较。 |
| [^196] | [Feature Unlearning for Pre-trained GANs and VAEs.](http://arxiv.org/abs/2303.05699) | 本文提出了一种从预训练的GAN和VAE模型中消除特定特征的方法，并通过实验证明了方法的有效性。 |
| [^197] | [Data-free Defense of Black Box Models Against Adversarial Attacks.](http://arxiv.org/abs/2211.01579) | 本研究提出了一种无数据情况下对黑盒模型进行防御的方法，通过生成模型构建合成数据，并使用模型窃取技术训练替代模型网络，同时采用小波噪声去除器（WNR）减少对抗性污染。 |
| [^198] | [Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System.](http://arxiv.org/abs/2204.11970) | 本研究提供了一种使用机器学习技术开发预测模型的多阶段系统，可高精度预测三种眼疾患者的视力变化，并辅助眼科医生进行临床决策和患者咨询。 |

# 详细

[^1]: 通过数据正则化的自我博弈强化学习实现与人类兼容的驾驶伙伴

    Human-compatible driving partners through data-regularized self-play reinforcement learning

    [https://arxiv.org/abs/2403.19648](https://arxiv.org/abs/2403.19648)

    提出了Human-Regularized PPO (HR-PPO)算法，通过自我博弈训练代理，实现在封闭环境中逼真且有效的驾驶伙伴

    

    自主驾驶汽车面临的一个核心挑战是与人类进行协调。因此，在模拟环境中，将逼真的人类代理纳入自动驾驶系统的可扩展训练和评估是至关重要的。我们提出了一种名为Human-Regularized PPO (HR-PPO)的多智能体算法，其中代理通过自我博弈进行训练，对偏离人类参考策略的行为进行小幅惩罚，以构建在封闭环境中既逼真又有效的代理。

    arXiv:2403.19648v1 Announce Type: cross  Abstract: A central challenge for autonomous vehicles is coordinating with humans. Therefore, incorporating realistic human agents is essential for scalable training and evaluation of autonomous driving systems in simulation. Simulation agents are typically developed by imitating large-scale, high-quality datasets of human driving. However, pure imitation learning agents empirically have high collision rates when executed in a multi-agent closed-loop setting. To build agents that are realistic and effective in closed-loop settings, we propose Human-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are trained through self-play with a small penalty for deviating from a human reference policy. In contrast to prior work, our approach is RL-first and only uses 30 minutes of imperfect human demonstrations. We evaluate agents in a large set of multi-agent traffic scenes. Results show our HR-PPO agents are highly effective in achieving goa
    
[^2]: 稀疏特征电路：在语言模型中发现和编辑可解释的因果图

    Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models

    [https://arxiv.org/abs/2403.19647](https://arxiv.org/abs/2403.19647)

    该论文介绍了一种新方法，即稀疏特征电路，可以在语言模型中发现和编辑可解释的因果图，为我们提供了对未预料机制的详细理解和包含了用于提高分类器泛化能力的SHIFT方法。

    

    我们介绍了用于发现和应用稀疏特征电路的方法。这些电路是人类可解释特征的因果相关子网络，用于解释语言模型行为。 在先前的工作中确定的电路由多义且难以解释的单元组成，例如注意力头或神经元，使它们不适用于许多下游应用。 相比之下，稀疏特征电路实现了对未预料机制的详细理解。 由于它们基于细粒度单元，稀疏特征电路对下游任务非常有用：我们 introduc了SHIFT，通过切除人类判断为任务不相关的特征，从而提高分类器的泛化能力。 最后，我们通过发现成千上万个稀疏特征电路来展示一个完全无监督且可扩展的可解释性管线，用于自动发现的模型行为。

    arXiv:2403.19647v1 Announce Type: cross  Abstract: We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.
    
[^3]: 多跳问题回答中的检索增强知识编辑在语言模型中的应用

    Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models

    [https://arxiv.org/abs/2403.19631](https://arxiv.org/abs/2403.19631)

    提出了用于多跳问题回答的检索增强模型编辑（RAE）框架，利用互信息最大化的检索方法和修剪策略，实现了对语言模型的有效优化。

    

    大型语言模型（LLMs）在问题回答任务中显示出高效能，但往往难以整合实时知识更新，导致可能过时或不准确的响应。当处理多跳问题时，这个问题变得更具挑战性，因为它们要求LLMs更新和整合与问题相关的多个知识片段。为了解决这个问题，我们提出了针对多跳问题回答定制的检索增强模型编辑（RAE）框架。RAE首先检索编辑后的事实，然后通过上下文学习来完善语言模型。具体而言，我们的检索方法基于互信息最大化，利用LLMs的推理能力来识别链式事实，而天真的基于相似性的搜索可能会忽略这些事实。此外，我们的框架还采用了修剪策略，从检索到的事实中消除冗余信息，这增强了编辑

    arXiv:2403.19631v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions. To tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework tailored for multi-hop question answering. RAE first retrieves edited facts and then refines the language model through in-context learning. Specifically, our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that na\"ive similarity-based searches might miss. Additionally, our framework incorporates a pruning strategy to eliminate redundant information from the retrieved facts, which enhances the edi
    
[^4]: 有限成对偏好比较下的度量学习

    Metric Learning from Limited Pairwise Preference Comparisons

    [https://arxiv.org/abs/2403.19629](https://arxiv.org/abs/2403.19629)

    在有限成对偏好比较下研究度量学习，表明虽然无法学习单个理想项目，但当比较对象表现出低维结构时，每个用户可以帮助学习限制在低维子空间中的度量。

    

    我们研究了在理想点模型下的偏好比较中的度量学习，其中用户如果一个项目比其潜在理想项目更接近，则更喜欢该项目。这些项目嵌入到具有未知马氏距离的$\mathbb{R}^d$中，该距离在用户间共享。尽管最近的工作表明，通过每个用户$\mathcal{O}(d)$个成对比较可以同时恢复度量和理想项目，但在实践中，我们经常有$o(d)$的有限比较预算。我们研究了即使已知学习单个理想项目现在不再可能，度量是否仍然可以恢复。我们发现一般来说，$o(d)$比较不会揭示有关度量的信息，即使用户数量无限。然而，当比较的项目表现出低维结构时，每个用户都可以有助于学习限制在低维子空间中的度量，这样度量就可以被恢复。

    arXiv:2403.19629v1 Announce Type: new  Abstract: We study metric learning from preference comparisons under the ideal point model, in which a user prefers an item over another if it is closer to their latent ideal item. These items are embedded into $\mathbb{R}^d$ equipped with an unknown Mahalanobis distance shared across users. While recent work shows that it is possible to simultaneously recover the metric and ideal items given $\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have a limited budget of $o(d)$ comparisons. We study whether the metric can still be recovered, even though it is known that learning individual ideal items is now no longer possible. We show that in general, $o(d)$ comparisons reveals no information about the metric, even with infinitely many users. However, when comparisons are made over items that exhibit low-dimensional structure, each user can contribute to learning the metric restricted to a low-dimensional subspace so that the metric
    
[^5]: 基于Top-$k$分类和基数感知预测

    Top-$k$ Classification and Cardinality-Aware Prediction

    [https://arxiv.org/abs/2403.19625](https://arxiv.org/abs/2403.19625)

    该论文研究了Top-$k$分类任务，并提出了基于基数感知的损失函数，通过实例相关的成本敏感学习，引入了新的基数感知算法

    

    我们详细研究了Top-$k$分类，即预测输入的$k$个最有可能的类别，超越了单类别预测。我们证明了多类别分类中几种流行的替代损失函数，如comp-sum和约束损失，对于Top-$k$损失具有关于$h$-一致性边界的支持。这些边界保证了关于假设集$H$的一致性，由于它们的非渐近和特定假设集性质，提供比贝叶斯一致性更强的保证。为了解决准确性和基数$k$之间的折衷，我们进一步通过实例相关的成本敏感学习引入了基数感知损失函数。对于这些函数，我们推导了成本敏感comp-sum和约束替代损失，建立了它们的$H$-一致性边界和贝叶斯一致性。最小化这些损失导致了新的基数感知算法

    arXiv:2403.19625v1 Announce Type: new  Abstract: We present a detailed study of top-$k$ classification, the task of predicting the $k$ most probable classes for an input, extending beyond single-class prediction. We demonstrate that several prevalent surrogate loss functions in multi-class classification, such as comp-sum and constrained losses, are supported by $H$-consistency bounds with respect to the top-$k$ loss. These bounds guarantee consistency in relation to the hypothesis set $H$, providing stronger guarantees than Bayes-consistency due to their non-asymptotic and hypothesis-set specific nature. To address the trade-off between accuracy and cardinality $k$, we further introduce cardinality-aware loss functions through instance-dependent cost-sensitive learning. For these functions, we derive cost-sensitive comp-sum and constrained surrogate losses, establishing their $H$-consistency bounds and Bayes-consistency. Minimizing these losses leads to new cardinality-aware algorithm
    
[^6]: 在深度生成模型的潜在空间中协作互动演化艺术

    Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative Models

    [https://arxiv.org/abs/2403.19620](https://arxiv.org/abs/2403.19620)

    本研究采用创意对抗网络和演化方法在深度生成模型的潜在空间中进行协作互动演化艺术，通过自动美学和协作交互式人类评估指标评估生成的图像质量。

    

    生成对抗网络（GANs）在生成高质量图像方面取得了巨大成功，因此被用作生成艺术图像的主要方法之一。然而，通常图像生成过程涉及从学习的艺术表征的潜在空间中进行采样，对输出的控制很少。在这项工作中，我们首先使用已知为创意对抗网络（CANs）的架构训练GANs生成创意图像，然后，我们采用一个演化方法在模型的潜在空间内导航以发现图像。我们使用自动美学和协作交互式人类评估指标来评估生成的图像。在人类互动评估案例中，我们提出了基于多位参与者评估的协作评估。此外，我们还尝试了一种旨在提高图像质量的智能突变运算符。

    arXiv:2403.19620v1 Announce Type: cross  Abstract: Generative Adversarial Networks (GANs) have shown great success in generating high quality images and are thus used as one of the main approaches to generate art images. However, usually the image generation process involves sampling from the latent space of the learned art representations, allowing little control over the output. In this work, we first employ GANs that are trained to produce creative images using an architecture known as Creative Adversarial Networks (CANs), then, we employ an evolutionary approach to navigate within the latent space of the models to discover images. We use automatic aesthetic and collaborative interactive human evaluation metrics to assess the generated images. In the human interactive evaluation case, we propose a collaborative evaluation based on the assessments of several participants. Furthermore, we also experiment with an intelligent mutation operator that aims to improve the quality of the ima
    
[^7]: ILPO-NET：用于三维中任意体积模式不变识别的网络

    ILPO-NET: Network for the invariant recognition of arbitrary volumetric patterns in 3D

    [https://arxiv.org/abs/2403.19612](https://arxiv.org/abs/2403.19612)

    ILPO-Net是一种处理任意形状模式的新方法，通过卷积运算对局部空间模式方向具有不变性，在各种体积数据集上展现出优越性能并显著减少参数数量。

    

    现代空间数据分析中，有效识别空间模式并学习其层次结构至关重要。体积数据应用寻求确保对位移和模式旋转均具有不变性的技术。ILPO-Net（Invariant to Local Patterns Orientation Network）是一种新颖方法，通过Wigner矩阵展开，在卷积操作中处理任意形状的模式，从而本质上对局部空间模式方向具有不变性。我们的架构无缝集成了新的卷积运算符，在各种体积数据集（如MedMNIST和CATH）上进行基准测试，表现出比基准线更卓越的性能，并且参数数量显著减少 - 在MedMNIST的情况下减少了高达1000倍。

    arXiv:2403.19612v1 Announce Type: cross  Abstract: Effective recognition of spatial patterns and learning their hierarchy is crucial in modern spatial data analysis. Volumetric data applications seek techniques ensuring invariance not only to shifts but also to pattern rotations. While traditional methods can readily achieve translational invariance, rotational invariance possesses multiple challenges and remains an active area of research. Here, we present ILPO-Net (Invariant to Local Patterns Orientation Network), a novel approach that handles arbitrarily shaped patterns with the convolutional operation inherently invariant to local spatial pattern orientations using the Wigner matrix expansions. Our architecture seamlessly integrates the new convolution operator and, when benchmarked on diverse volumetric datasets such as MedMNIST and CATH, demonstrates superior performance over the baselines with significantly reduced parameter counts - up to 1000 times fewer in the case of MedMNIS
    
[^8]: 数据自适应预测中多重风险的权衡

    Data-Adaptive Tradeoffs among Multiple Risks in Distribution-Free Prediction

    [https://arxiv.org/abs/2403.19605](https://arxiv.org/abs/2403.19605)

    开发了一种能够在数据自适应选择阈值和权衡参数时有效控制风险的方法，支持单调和几乎单调风险，无需分布假设。

    

    决策管道通常由各种风险函数之间的权衡特征。通常希望以数据自适应的方式管理这些权衡。我们证明，如果这样做得太幼稚，最先进的不确定性量化方法会导致对假定风险保证的重大违反。为了解决这个问题，我们开发了能够在阈值和权衡参数自适应选择时允许有效控制风险的方法。我们的方法支持单调和几乎单调的风险，但不做任何分布假设。为了说明我们方法的优点，我们在合成数据和大规模视觉数据集MS-COCO上进行了数值实验。

    arXiv:2403.19605v1 Announce Type: cross  Abstract: Decision-making pipelines are generally characterized by tradeoffs among various risk functions. It is often desirable to manage such tradeoffs in a data-adaptive manner. As we demonstrate, if this is done naively, state-of-the art uncertainty quantification methods can lead to significant violations of putative risk guarantees.   To address this issue, we develop methods that permit valid control of risk when threshold and tradeoff parameters are chosen adaptively. Our methodology supports monotone and nearly-monotone risks, but otherwise makes no distributional assumptions.   To illustrate the benefits of our approach, we carry out numerical experiments on synthetic data and the large-scale vision dataset MS-COCO.
    
[^9]: 基因量化感知逼近用于变压器中的非线性操作

    Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers

    [https://arxiv.org/abs/2403.19591](https://arxiv.org/abs/2403.19591)

    本文提出的GQA-LUT算法在变压器中的非线性操作中具有量化感知性，并实现了对INT8-based LUT逼近的应用，节约了大量硬件和功耗

    

    在变压器及其轻量级变体中，非线性函数普遍存在，导致硬件成本显著且经常被低估。先前的最先进作品通过分段线性逼近来优化这些操作，并将参数存储在查找表（LUT）中，但大多数需要不友好的高精度算术，如FP/INT 32，并且缺乏对纯整数INT量化的考虑。本文提出了一种遗传LUT-逼近算法，即GQA-LUT，它可以自动确定具有量化意识的参数。结果表明，对于普通和线性Transformer模型的挑战性语义分割任务，GQA-LUT实现了可忽略的性能降级。此外，提出的GQA-LUT使得能够使用基于INT8的LUT逼近，相比高精度FP/INT 32，可以实现81.3~81.7%的面积节约和79.3~80.2%的功耗降低。

    arXiv:2403.19591v1 Announce Type: new  Abstract: Non-linear functions are prevalent in Transformers and their lightweight variants, incurring substantial and frequently underestimated hardware costs. Previous state-of-the-art works optimize these operations by piece-wise linear approximation and store the parameters in look-up tables (LUT), but most of them require unfriendly high-precision arithmetics such as FP/INT 32 and lack consideration of integer-only INT quantization. This paper proposed a genetic LUT-Approximation algorithm namely GQA-LUT that can automatically determine the parameters with quantization awareness. The results demonstrate that GQA-LUT achieves negligible degradation on the challenging semantic segmentation task for both vanilla and linear Transformer models. Besides, proposed GQA-LUT enables the employment of INT8-based LUT-Approximation that achieves an area savings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to the high-precision FP/INT 32 alte
    
[^10]: DenseNets重生：超越ResNets和ViTs的范式转变

    DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs

    [https://arxiv.org/abs/2403.19588](https://arxiv.org/abs/2403.19588)

    本文重新探讨了密集连接卷积网络（DenseNets），揭示了其相对于ResNet风格架构的被低估有效性，通过改进架构、块设计和训练方法，使得DenseNets可以与现代架构竞争，并在ImageNet-1K上接近最新技术水平。

    

    本文复苏了密集连接卷积网络（DenseNets），揭示了它们相对于主导的ResNet风格架构被低估的有效性。我们认为DenseNets的潜力被忽视，是因为未曾触及的训练方法和传统设计元素未能完全展现其能力。我们的初步研究表明，通过连接的密集连接是强大的，表明DenseNets可以被重新激活以与现代架构竞争。我们系统地改进了次优组件 - 架构调整、块重新设计和改进的训练配方，以扩展DenseNets并提高内存效率，同时保持连接快捷方式。我们的模型采用简单的架构元素，最终超越了Swin Transformer、ConvNeXt和DeiT-III - 残差学习谱系中的关键架构。此外，我们的模型在ImageNet-1K上展现出接近最新技术水平的性能，竞争wi

    arXiv:2403.19588v1 Announce Type: cross  Abstract: This paper revives Densely Connected Convolutional Networks (DenseNets) and reveals the underrated effectiveness over predominant ResNet-style architectures. We believe DenseNets' potential was overlooked due to untouched training methods and traditional design elements not fully revealing their capabilities. Our pilot study shows dense connections through concatenation are strong, demonstrating that DenseNets can be revitalized to compete with modern architectures. We methodically refine suboptimal components - architectural adjustments, block redesign, and improved training recipes towards widening DenseNets and boosting memory efficiency while keeping concatenation shortcuts. Our models, employing simple architectural elements, ultimately surpass Swin Transformer, ConvNeXt, and DeiT-III - key architectures in the residual learning lineage. Furthermore, our models exhibit near state-of-the-art performance on ImageNet-1K, competing wi
    
[^11]: 关键动作令牌在机器人学中实现上下文模仿学习

    Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics

    [https://arxiv.org/abs/2403.19578](https://arxiv.org/abs/2403.19578)

    使用关键动作令牌（KAT）框架，研究展示了文本预训练的变形器（GPT-4 Turbo）在机器人领域可实现视觉模仿学习，将视觉观测映射为模拟示范者行为的动作序列，表现优越于现有的模仿学习方法。

    

    我们展示了现成的基于文本的变形器，无需额外训练，就可以执行少样本上下文内视觉模仿学习，将视觉观测映射为模拟示范者行为的动作序列。我们通过将视觉观测（输入）和动作轨迹（输出）转换为一系列令牌，这些令牌可以被文本预训练的变形器（GPT-4 Turbo）接收和生成，通过我们称之为关键动作令牌（KAT）的框架来实现这一点。尽管仅在语言上训练，我们展示这些变形器擅长将标记化的视觉关键点观察翻译为行为轨迹，在真实世界的日常任务套件中，在低数据情况下表现与优于最先进的模仿学习（扩散策略）。KAT不同于通常在语言领域操作，它利用基于文本的变形器在视觉和动作领域中学习。

    arXiv:2403.19578v1 Announce Type: cross  Abstract: We show that off-the-shelf text-based Transformers, with no additional training, can perform few-shot in-context visual imitation learning, mapping visual observations to action sequences that emulate the demonstrator's behaviour. We achieve this by transforming visual observations (inputs) and trajectories of actions (outputs) into sequences of tokens that a text-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a framework we call Keypoint Action Tokens (KAT). Despite being trained only on language, we show that these Transformers excel at translating tokenised visual keypoint observations into action trajectories, performing on par or better than state-of-the-art imitation learning (diffusion policies) in the low-data regime on a suite of real-world, everyday tasks. Rather than operating in the language domain as is typical, KAT leverages text-based Transformers to operate in the vision and action domains to learn ge
    
[^12]: 使用神经网络对群体特性进行分类

    Swarm Characteristics Classification Using Neural Networks

    [https://arxiv.org/abs/2403.19572](https://arxiv.org/abs/2403.19572)

    本文研究了使用监督神经网络时间序列分类（NN TSC）预测军事背景下群体自主体的关键属性和战术，以及展示了NN TSC在快速推断攻击群体情报方面的有效性。

    

    理解群体自主体的特性对于国防和安全应用至关重要。本文介绍了使用监督神经网络时间序列分类（NN TSC）来预测军事环境中群体自主体的关键属性和战术的研究。具体地，NN TSC被应用于推断两个二进制属性 - 通信和比例导航 - 这两者结合定义了四种互斥的群体战术。我们发现文献中对于使用神经网络进行群体分类存在一定的空白，并展示了NN TSC在快速推断有关攻击群体情报以指导反制动作方面的有效性。通过模拟的群体对战，我们评估了NN TSC在观察窗口要求、噪声鲁棒性和对群体规模的可扩展性方面的性能。关键发现显示NN能够使用较短的观察窗口以97%的准确率预测群体行为。

    arXiv:2403.19572v1 Announce Type: new  Abstract: Understanding the characteristics of swarming autonomous agents is critical for defense and security applications. This article presents a study on using supervised neural network time series classification (NN TSC) to predict key attributes and tactics of swarming autonomous agents for military contexts. Specifically, NN TSC is applied to infer two binary attributes - communication and proportional navigation - which combine to define four mutually exclusive swarm tactics. We identify a gap in literature on using NNs for swarm classification and demonstrate the effectiveness of NN TSC in rapidly deducing intelligence about attacking swarms to inform counter-maneuvers. Through simulated swarm-vs-swarm engagements, we evaluate NN TSC performance in terms of observation window requirements, noise robustness, and scalability to swarm size. Key findings show NNs can predict swarm behaviors with 97% accuracy using short observation windows of
    
[^13]: GrINd：网格插值网络用于离散观测

    GrINd: Grid Interpolation Network for Scattered Observations

    [https://arxiv.org/abs/2403.19570](https://arxiv.org/abs/2403.19570)

    GrINd是一种新颖的网络架构，通过傅立叶插值层将离散观测数据映射到高分辨率网格，实现了从稀疏和分散的观测数据中预测时空物理系统演变的能力。

    

    从稀疏和分散的观测数据中预测时空物理系统的演变在各个科学领域中都是一个重要挑战。传统方法依赖于密集的网格结构数据，限制了它们在稀疏观测场景中的适用性。为了解决这一挑战，我们引入了GrINd（用于离散观测的网格插值网络），这是一种新颖的网络架构，通过使用傅立叶插值层将离散观测映射到高分辨率网格，发挥了基于网格的模型的高性能。在高分辨率空间中，一个NeuralPDE模型通过可微分的ODE求解器和完全卷积神经网络来参数化系统的动态，预测未来时间点的系统状态。我们在DynaBench基准数据集上对GrINd进行了实证评估，包括六种在分散位置观测到的不同物理系统，展示了其s。

    arXiv:2403.19570v1 Announce Type: new  Abstract: Predicting the evolution of spatiotemporal physical systems from sparse and scattered observational data poses a significant challenge in various scientific domains. Traditional methods rely on dense grid-structured data, limiting their applicability in scenarios with sparse observations. To address this challenge, we introduce GrINd (Grid Interpolation Network for Scattered Observations), a novel network architecture that leverages the high-performance of grid-based models by mapping scattered observations onto a high-resolution grid using a Fourier Interpolation Layer. In the high-resolution space, a NeuralPDE-class model predicts the system's state at future timepoints using differentiable ODE solvers and fully convolutional neural networks parametrizing the system's dynamics. We empirically evaluate GrINd on the DynaBench benchmark dataset, comprising six different physical systems observed at scattered locations, demonstrating its s
    
[^14]: 自我改进学习用于可扩展神经组合优化

    Self-Improved Learning for Scalable Neural Combinatorial Optimization

    [https://arxiv.org/abs/2403.19561](https://arxiv.org/abs/2403.19561)

    提出一种新颖的自我改进学习(SIL)方法，实现神经组合优化的更好可扩展性，通过自身生成解决方案作为伪标签，设计线性复杂度的注意机制来处理大规模组合优化问题实例。

    

    end-to-end神经组合优化(NCO)方法在解决复杂组合优化问题方面表现出有希望的性能，而不需要专家设计。然而，现有方法在处理大规模问题时存在困难，限制了它们的实际适用性。为了克服这一限制，本研究提出了一种新颖的自我改进学习(SIL)方法，以实现神经组合优化的更好可扩展性。具体来说，我们开发了一种高效的自我改进机制，使模型能够在没有标记数据的情况下直接在大规模问题实例上进行训练。通过一种创新的局部重构方法，该方法可以通过自身迭代生成更好的解决方案作为伪标签，以指导有效的模型训练。此外，我们设计了一种线性复杂度的注意机制，使模型能够有效处理低计算开销的大规模组合优化问题实例。

    arXiv:2403.19561v1 Announce Type: cross  Abstract: The end-to-end neural combinatorial optimization (NCO) method shows promising performance in solving complex combinatorial optimization problems without the need for expert design. However, existing methods struggle with large-scale problems, hindering their practical applicability. To overcome this limitation, this work proposes a novel Self-Improved Learning (SIL) method for better scalability of neural combinatorial optimization. Specifically, we develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without any labeled data. Powered by an innovative local reconstruction approach, this method can iteratively generate better solutions by itself as pseudo-labels to guide efficient model training. In addition, we design a linear complexity attention mechanism for the model to efficiently handle large-scale combinatorial problem instances with low computation overhead. Comprehens
    
[^15]: Croissant：一种面向机器学习数据集的元数据格式

    Croissant: A Metadata Format for ML-Ready Datasets

    [https://arxiv.org/abs/2403.19546](https://arxiv.org/abs/2403.19546)

    Croissant是一种面向机器学习数据集的元数据格式，使数据集更易发现、可移植和互操作，有助于解决ML数据管理和负责任AI中的重要挑战。

    

    数据是机器学习（ML）的关键资源，但处理数据仍然是一个主要的摩擦点。本文介绍了Croissant，一种用于数据集的元数据格式，简化了数据被ML工具和框架使用的方式。Croissant使数据集更易发现、可移植和互操作，从而解决了ML数据管理和负责任AI中的重要挑战。Croissant已得到几个流行数据集库的支持，涵盖数十万个数据集，可以加载到最流行的ML框架中。

    arXiv:2403.19546v1 Announce Type: cross  Abstract: Data is a critical resource for Machine Learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that simplifies how data is used by ML tools and frameworks. Croissant makes datasets more discoverable, portable and interoperable, thereby addressing significant challenges in ML data management and responsible AI. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, ready to be loaded into the most popular ML frameworks.
    
[^16]: 在以太坊区块链上检测金融机器人

    Detecting Financial Bots on the Ethereum Blockchain

    [https://arxiv.org/abs/2403.19530](https://arxiv.org/abs/2403.19530)

    本研究提出了一种利用机器学习检测金融机器人在以太坊平台上的新方法，并建立了金融机器人的分类法，对机器人进行了检测并创建了地面实况数据集。

    

    集成机器人在分布式账本技术（DLTs）中促进了效率和自动化。然而，它们的使用也与掠夺性交易和市场操纵相关，并可能对系统完整性构成威胁。因此，了解DLTs中机器人的部署程度至关重要；尽管如此，目前的检测系统主要基于规则，并且缺乏灵活性。在这项研究中，我们提出了一种利用机器学习检测以太坊平台上金融机器人的新方法。首先，我们系统化现有的科学文献并收集轶事证据，以建立金融机器人的分类法，包括7个类别和24个子类别。接下来，我们创建一个包含133个人类和137个机器人地址的地面实况数据集。第三，我们使用无监督和有监督的机器学习算法来检测部署在以太坊上的机器人。

    arXiv:2403.19530v1 Announce Type: cross  Abstract: The integration of bots in Distributed Ledger Technologies (DLTs) fosters efficiency and automation. However, their use is also associated with predatory trading and market manipulation, and can pose threats to system integrity. It is therefore essential to understand the extent of bot deployment in DLTs; despite this, current detection systems are predominantly rule-based and lack flexibility. In this study, we present a novel approach that utilizes machine learning for the detection of financial bots on the Ethereum platform. First, we systematize existing scientific literature and collect anecdotal evidence to establish a taxonomy for financial bots, comprising 7 categories and 24 subcategories. Next, we create a ground-truth dataset consisting of 133 human and 137 bot addresses. Third, we employ both unsupervised and supervised machine learning algorithms to detect bots deployed on Ethereum. The highest-performing clustering algori
    
[^17]: 模型库：我们只需要几个经过良好调整的模型

    Model Stock: All we need is just a few fine-tuned models

    [https://arxiv.org/abs/2403.19522](https://arxiv.org/abs/2403.19522)

    本文提出了一种高效的微调方法，只使用少量模型就能获得优越的性能，通过权重空间和层次加权平均技术超越了现有的模型方法。

    

    本文介绍了一种高效的大型预训练模型微调方法，提供强大的内分布（ID）和外分布（OOD）性能。与需要大量微调模型进行平均的传统做法不同，我们的方法使用更少的模型来获得最终权重，同时产生更高的准确性。从微调权重的权重空间中汲取关键见解，我们揭示了性能和接近权重空间中心的强连接。基于此，我们引入一种方法，通过仅使用两个微调模型来近似中心接近的权重，可在训练期间或之后应用。我们的创新的逐层权重平均技术超越了Model Soup等最先进的模型方法，仅利用两个微调模型。这种策略可以被称为模型库，突出了它依赖于选择少量模型来进行综合的特点。

    arXiv:2403.19522v1 Announce Type: new  Abstract: This paper introduces an efficient fine-tuning method for large pre-trained models, offering strong in-distribution (ID) and out-of-distribution (OOD) performance. Breaking away from traditional practices that need a multitude of fine-tuned models for averaging, our approach employs significantly fewer models to achieve final weights yet yield superior accuracy. Drawing from key insights in the weight space of fine-tuned weights, we uncover a strong link between the performance and proximity to the center of weight space. Based on this, we introduce a method that approximates a center-close weight using only two fine-tuned models, applicable during or after training. Our innovative layer-wise weight averaging technique surpasses state-of-the-art model methods such as Model Soup, utilizing only two fine-tuned models. This strategy can be aptly coined Model Stock, highlighting its reliance on selecting a minimal number of models to draw a 
    
[^18]: 解释基于Transformer模型的语言模型在事实回忆中的关键机制

    Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models

    [https://arxiv.org/abs/2403.19521](https://arxiv.org/abs/2403.19521)

    通过深入研究Transformer-based语言模型在事实回忆任务中的机制，我们发现了零/少次样本情况下的特定任务头、MLP层和残差流的功能，以及抗过度自信机制。

    

    本文深入探讨了Transformer-based语言模型在事实回忆任务中所采用的机制。在零次样本情况下，给定类似“法国的首都是”的提示，特定任务的注意力头会从上下文中提取主题实体，如“法国”，并将其传递给后续的MLP以回忆所需的答案，如“巴黎”。我们引入了一种新颖的分析方法，旨在将MLP的输出分解为人类可理解的组件。通过这种方法，我们量化了跟随这些特定任务头的MLP层的功能。在残差流中，它会擦除或放大来自各个头的信息。此外，它会生成一个组件，将残差流重新定向到预期答案的方向。这些零次机制也适用于少次样本情况。此外，我们观察到一种广泛存在的抗过度自信机制。

    arXiv:2403.19521v1 Announce Type: cross  Abstract: In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like "The capital of France is," task-specific attention heads extract the topic entity, such as "France," from the context and pass it to subsequent MLPs to recall the required answer such as "Paris." We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These zero-shot mechanisms are also employed in few-shot scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in 
    
[^19]: 针对有向图聚类问题的随机块模型最大似然估计

    Maximum Likelihood Estimation on Stochastic Blockmodels for Directed Graph Clustering

    [https://arxiv.org/abs/2403.19516](https://arxiv.org/abs/2403.19516)

    本文研究了有向图聚类问题，提出了一种基于有向随机块模型的最大似然估计方法，并引入了两种高效且可解释的有向聚类算法。

    

    本文通过统计学的视角研究了有向图聚类问题，将聚类问题建模为有向随机块模型（DSBM）中潜在社区的估计。我们对DSBM进行最大似然估计（MLE），从而确定给定观察到的图结构时最可能的社区分配。除了统计观点外，我们进一步建立了这种MLE公式与一种新颖的流优化启发式之间的等价性，该启发式同时考虑了两个重要的有向图统计量：边密度和边方向。基于这种有向聚类的新公式，我们引入了两种高效且可解释的有向聚类算法，分别是谱聚类算法和基于半定规划的聚类算法。我们为谱聚类算法的错误聚类顶点数提供了一个理论上界。

    arXiv:2403.19516v1 Announce Type: cross  Abstract: This paper studies the directed graph clustering problem through the lens of statistics, where we formulate clustering as estimating underlying communities in the directed stochastic block model (DSBM). We conduct the maximum likelihood estimation (MLE) on the DSBM and thereby ascertain the most probable community assignment given the observed graph structure. In addition to the statistical point of view, we further establish the equivalence between this MLE formulation and a novel flow optimization heuristic, which jointly considers two important directed graph statistics: edge density and edge orientation. Building on this new formulation of directed clustering, we introduce two efficient and interpretable directed clustering algorithms, a spectral clustering algorithm and a semidefinite programming based clustering algorithm. We provide a theoretical upper bound on the number of misclustered vertices of the spectral clustering algor
    
[^20]: CDIMC-net: 认知式深度不完整多视图聚类网络

    CDIMC-net: Cognitive Deep Incomplete Multi-view Clustering Network

    [https://arxiv.org/abs/2403.19514](https://arxiv.org/abs/2403.19514)

    提出了CDIMC-net，一个认知式深度不完整多视图聚类网络，通过结合视图特定深度编码器和图嵌入策略，在框架中捕获每个视图的高级特征和局部结构，从而解决了现有方法中的模型扁平、对噪声或异常值敏感等问题。

    

    近年来，关于不完整多视图聚类的研究正逐渐受到关注，该研究探讨了在缺失视图上的具有挑战性的多视图聚类问题。尽管已经提出了一系列方法来解决这一问题，但以下问题仍然存在：1）几乎所有现有方法都是基于浅层模型，很难获得有区分力的共同表示。2）这些方法通常对噪声或异常值敏感，因为负样本被视为与重要样本同等重要。在本文中，我们提出了一种新颖的不完整多视图聚类网络，称为认知式深度不完整多视图聚类网络（CDIMC-net），以解决这些问题。具体而言，它通过将视图特定的深度编码器和图嵌入策略纳入框架来捕获每个视图的高级特征和局部结构。此外，根据人类认知，即

    arXiv:2403.19514v1 Announce Type: cross  Abstract: In recent years, incomplete multi-view clustering, which studies the challenging multi-view clustering problem on missing views, has received growing research interests. Although a series of methods have been proposed to address this issue, the following problems still exist: 1) Almost all of the existing methods are based on shallow models, which is difficult to obtain discriminative common representations. 2) These methods are generally sensitive to noise or outliers since the negative samples are treated equally as the important samples. In this paper, we propose a novel incomplete multi-view clustering network, called Cognitive Deep Incomplete Multi-view Clustering Network (CDIMC-net), to address these issues. Specifically, it captures the high-level features and local structure of each view by incorporating the view-specific deep encoders and graph embedding strategy into a framework. Moreover, based on the human cognition, i.e., 
    
[^21]: 用受控潜在扩散模型消除心脏成像的偏差

    Debiasing Cardiac Imaging with Controlled Latent Diffusion Models

    [https://arxiv.org/abs/2403.19508](https://arxiv.org/abs/2403.19508)

    通过生成合成数据，使用受控潜在扩散模型来消除心脏成像中的偏差和不平衡问题。

    

    深度学习对基于心脏磁共振成像进行疾病诊断和预后的解决方案的进展受到训练数据高度不平衡和偏差的阻碍。为了解决这个问题，我们提出一种方法，通过基于敏感属性（如性别、年龄、体重指数和健康状况）生成合成数据来减轻数据集中固有的不平衡性。我们采用基于去噪扩散概率模型的ControlNet来以患者元数据和使用大型队列研究（具体来说是UK Biobank）中分割掩模导出的心脏几何形状为条件生成文本。我们通过使用已建立的定量指标评估生成图像的逼真程度来评估我们的方法。此外，我们进行了一个下游分类任务，旨在通过合成生成的样本纠正代表性不足群体内的不平衡来改正分类器的偏差。我们的实验示范

    arXiv:2403.19508v1 Announce Type: cross  Abstract: The progress in deep learning solutions for disease diagnosis and prognosis based on cardiac magnetic resonance imaging is hindered by highly imbalanced and biased training data. To address this issue, we propose a method to alleviate imbalances inherent in datasets through the generation of synthetic data based on sensitive attributes such as sex, age, body mass index, and health condition. We adopt ControlNet based on a denoising diffusion probabilistic model to condition on text assembled from patient metadata and cardiac geometry derived from segmentation masks using a large-cohort study, specifically, the UK Biobank. We assess our method by evaluating the realism of the generated images using established quantitative metrics. Furthermore, we conduct a downstream classification task aimed at debiasing a classifier by rectifying imbalances within underrepresented groups through synthetically generated samples. Our experiments demons
    
[^22]: SineNet：学习时间依赖偏微分方程的时间动力学

    SineNet: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations

    [https://arxiv.org/abs/2403.19507](https://arxiv.org/abs/2403.19507)

    SineNet提出了一种通过多个依次连接的U-shaped网络块（波）在解决时间依赖偏微分方程时减少错位特征的方法

    

    我们考虑使用深度神经网络来解决时间依赖偏微分方程（PDE），其中多尺度处理对于建模复杂的时间演变动态至关重要。虽然先前的研究通常使用具有跳跃连接的U-Net架构来实现多尺度处理，但我们的分析显示，特征需要跨层演变导致跳跃连接中存在时间错位的特征，从而限制了模型的性能。为了解决这一局限，我们提出了SineNet，由多个依次连接的U形网络块组成，称为波。在SineNet中，高分辨率特征通过多个阶段逐渐演变，从而减少每个阶段内的错位量。我们进一步分析了跳跃连接在实现多尺度信息的并行和连续处理中的作用。我们的方法在多个PDE上经过严格测试。

    arXiv:2403.19507v1 Announce Type: new  Abstract: We consider using deep neural networks to solve time-dependent partial differential equations (PDEs), where multi-scale processing is crucial for modeling complex, time-evolving dynamics. While the U-Net architecture with skip connections is commonly used by prior studies to enable multi-scale processing, our analysis shows that the need for features to evolve across layers results in temporally misaligned features in skip connections, which limits the model's performance. To address this limitation, we propose SineNet, consisting of multiple sequentially connected U-shaped network blocks, referred to as waves. In SineNet, high-resolution features are evolved progressively through multiple stages, thereby reducing the amount of misalignment within each stage. We furthermore analyze the role of skip connections in enabling both parallel and sequential processing of multi-scale information. Our method is rigorously tested on multiple PDE d
    
[^23]: 张量网络约束的核机器作为高斯过程

    Tensor Network-Constrained Kernel Machines as Gaussian Processes

    [https://arxiv.org/abs/2403.19500](https://arxiv.org/abs/2403.19500)

    本文证明了Canonical Polyadic Decomposition和Tensor Train约束的核机器的输出会在对参数进行i.i.d.先验分布的情况下恢复为高斯过程，并且通过实验证明了Tensor Train模型相对于Canonical Polyadic Decomposition模型具有更多高斯过程行为。

    

    张量网络（TNs）最近被用来通过约束模型权重加快核机器的速度，产生了指数级的计算和存储节约。在本文中，我们证明Canonical Polyadic Decomposition（CPD）和Tensor Train（TT）约束的核机器的输出会在对参数进行i.i.d.先验分布的情况下恢复为高斯过程（GP），我们完全表征了这一过程。我们分析了CPD和TT约束模型的收敛性，并展示了TT相对于CPD具有更多GP行为的模型，而模型参数的数量相同。我们通过两个数值实验在两个方面实证观察了这一行为，分别是分析到GP的收敛性和预测性能。因此，我们建立了张量网络约束的核机器和高斯过程之间的联系。

    arXiv:2403.19500v1 Announce Type: new  Abstract: Tensor Networks (TNs) have recently been used to speed up kernel machines by constraining the model weights, yielding exponential computational and storage savings. In this paper we prove that the outputs of Canonical Polyadic Decomposition (CPD) and Tensor Train (TT)-constrained kernel machines recover a Gaussian Process (GP), which we fully characterize, when placing i.i.d. priors over their parameters. We analyze the convergence of both CPD and TT-constrained models, and show how TT yields models exhibiting more GP behavior compared to CPD, for the same number of model parameters. We empirically observe this behavior in two numerical experiments where we respectively analyze the convergence to the GP and the performance at prediction. We thereby establish a connection between TN-constrained kernel machines and GPs.
    
[^24]: 客户监督的联邦学习：迈向一模型适用于所有个性化

    Client-supervised Federated Learning: Towards One-model-for-all Personalization

    [https://arxiv.org/abs/2403.19499](https://arxiv.org/abs/2403.19499)

    提出了一种新的客户监督联邦学习框架，在联邦学习系统中学习一个强大的全局模型，以实现与个性化模型相媲美的性能。

    

    个性化联邦学习（PerFL）是一种新的机器学习范式，它在联邦学习环境下为不同客户交付个性化模型。大多数PerFL方法需要在客户端上进行额外的学习过程，以使用其自己的本地数据调整全球共享模型以适应客户特定的个性化模型。然而，在PerFL中的模型适应过程在模型部署和测试时间仍然是一个挑战。本工作通过提出一种新颖的联邦学习框架来解决这一挑战，学习只一个强大全球模型，以在FL系统中实现与那些个性化模型在未知/测试客户上具有竞争性能力。具体来说，我们设计了一种新的客户监督联邦学习（FedCS）来揭示客户对实例潜在表示的偏见，以便全球模型可以学习客户特定和客户不可知的知识。实验研究表明，FedCS可以学习

    arXiv:2403.19499v1 Announce Type: new  Abstract: Personalized Federated Learning (PerFL) is a new machine learning paradigm that delivers personalized models for diverse clients under federated learning settings. Most PerFL methods require extra learning processes on a client to adapt a globally shared model to the client-specific personalized model using its own local data. However, the model adaptation process in PerFL is still an open challenge in the stage of model deployment and test time. This work tackles the challenge by proposing a novel federated learning framework to learn only one robust global model to achieve competitive performance to those personalized models on unseen/test clients in the FL system. Specifically, we design a new Client-Supervised Federated Learning (FedCS) to unravel clients' bias on instances' latent representations so that the global model can learn both client-specific and client-agnostic knowledge. Experimental study shows that the FedCS can learn a
    
[^25]: 具有多专家推迟的回归

    Regression with Multi-Expert Deferral

    [https://arxiv.org/abs/2403.19494](https://arxiv.org/abs/2403.19494)

    这是一个回归问题的新框架，涉及将预测推迟给多个专家，提出了单阶段和双阶段情景的全面分析，并引入了新的代理损失函数及其支持的一致性界限。

    

    学习与多个专家推迟是一个框架，其中学习者可以选择将预测推迟给多个专家。虽然在分类情境中该问题得到了重视，但由于标签空间的无限和连续特性，它在回归中面临独特挑战。在这项工作中，我们引入了一个具有推迟的回归新框架，其中涉及将预测推迟给多个专家。我们针对单阶段情景和双阶段情景进行了全面分析，前者涉及预测器和推迟函数的同时学习，后者涉及具有已训练预测器和学习推迟函数。我们为两种情景引入了新的代理损失函数，并证明它们受到$H$-一致性界限的支持。这些界限提供了比贝叶斯一致性更强的一致性保证，因为它们是非渐近的，且假设集

    arXiv:2403.19494v1 Announce Type: new  Abstract: Learning to defer with multiple experts is a framework where the learner can choose to defer the prediction to several experts. While this problem has received significant attention in classification contexts, it presents unique challenges in regression due to the infinite and continuous nature of the label space. In this work, we introduce a novel framework of regression with deferral, which involves deferring the prediction to multiple experts. We present a comprehensive analysis for both the single-stage scenario, where there is simultaneous learning of predictor and deferral functions, and the two-stage scenario, which involves a pre-trained predictor with a learned deferral function. We introduce new surrogate loss functions for both scenarios and prove that they are supported by $H$-consistency bounds. These bounds provide consistency guarantees that are stronger than Bayes consistency, as they are non-asymptotic and hypothesis set
    
[^26]: 回归问题中的$H$一致性保证

    $H$-Consistency Guarantees for Regression

    [https://arxiv.org/abs/2403.19480](https://arxiv.org/abs/2403.19480)

    提出了通用的$H$一致性界工具来分析回归问题，并针对平方损失推导出一系列新颖的$H$一致性界；基于对$H$一致性的分析，为对抗性回归提供了有原则的代理损失。

    

    我们对回归问题中的$H$一致性界进行了详细研究。我们首先提出了一些新的定理，这些定理推广了先前用于建立$H$一致性界的工具。这种概括对于分析针对回归问题特定的$H$一致性界至关重要。接下来，我们在对称分布和有界假设集的条件下，证明了一系列关于平方损失的新颖$H$一致性界，包括Huber损失、所有$\ell_p$损失（$p \geq 1$）、平方$\epsilon$-不敏感损失的正结果，以及对于在平方支持向量回归（SVR）中使用的$\epsilon$-不敏感损失的负结果。我们进一步利用对回归问题中$H$一致性的分析，并提出了针对对抗回归的有原则的代理损失（第5节）。这为对抗性回归建立了新颖的算法，我们报告了有利的实验结果。

    arXiv:2403.19480v1 Announce Type: new  Abstract: We present a detailed study of $H$-consistency bounds for regression. We first present new theorems that generalize the tools previously given to establish $H$-consistency bounds. This generalization proves essential for analyzing $H$-consistency bounds specific to regression. Next, we prove a series of novel $H$-consistency bounds for surrogate loss functions of the squared loss, under the assumption of a symmetric distribution and a bounded hypothesis set. This includes positive results for the Huber loss, all $\ell_p$ losses, $p \geq 1$, the squared $\epsilon$-insensitive loss, as well as a negative result for the $\epsilon$-insensitive loss used in squared Support Vector Regression (SVR). We further leverage our analysis of $H$-consistency for regression and derive principled surrogate losses for adversarial regression (Section 5). This readily establishes novel algorithms for adversarial regression, for which we report favorable exp
    
[^27]: 有限孔径逆障碍散射问题的深度分解方法

    Deep decomposition method for the limited aperture inverse obstacle scattering problem

    [https://arxiv.org/abs/2403.19470](https://arxiv.org/abs/2403.19470)

    提出了一种用于有限孔径逆障碍散射问题的深度分解方法，通过向神经网络架构提供与散射模型相关联的物理运算符，实现深度学习在逆问题上工作，并避免扭曲解决方案。

    

    在本文中，我们考虑了一种针对有限孔径逆障碍散射问题的深度学习方法。传统深度学习仅依赖数据是众所周知的，当只有间接观测数据和一个物理模型可用时，这可能限制其在逆问题上的性能。在面对这些局限性时，一个基本问题出现了：是否可能使深度学习能够在没有标记数据的情况下处理逆问题，并且了解它正在学习的内容？本文提出了一个用于这些目的的深度分解方法（DDM），它不需要地面真实标签。它通过向神经网络架构提供与散射模型相关联的物理运算符来实现这一点。此外，DDM中还实现了一种基于深度学习的数据完整性方案，以防止扭曲有限孔径数据的逆问题的解决方案。此外，除了解决i

    arXiv:2403.19470v1 Announce Type: cross  Abstract: In this paper, we consider a deep learning approach to the limited aperture inverse obstacle scattering problem. It is well known that traditional deep learning relies solely on data, which may limit its performance for the inverse problem when only indirect observation data and a physical model are available. A fundamental question arises in light of these limitations: is it possible to enable deep learning to work on inverse problems without labeled data and to be aware of what it is learning? This work proposes a deep decomposition method (DDM) for such purposes, which does not require ground truth labels. It accomplishes this by providing physical operators associated with the scattering model to the neural network architecture. Additionally, a deep learning based data completion scheme is implemented in DDM to prevent distorting the solution of the inverse problem for limited aperture data. Furthermore, apart from addressing the i
    
[^28]: 离线多基线模仿学习及其在编译器优化中的应用

    Offline Imitation Learning from Multiple Baselines with Applications to Compiler Optimization

    [https://arxiv.org/abs/2403.19462](https://arxiv.org/abs/2403.19462)

    通过离线多基线模仿学习算法，在编译器优化领域实现了超越标准RL的策略学习效果

    

    这项工作研究了一个强化学习（RL）问题，我们在其中给定了使用K个基线策略收集的一组轨迹。这些策略中的每一个在单独的情况下可能相当次优，并且在状态空间的互补部分表现出较强的性能。旨在学习一个策略，使其在整个状态空间上的表现与最佳组合基线相当。我们提出了一种基于简单模仿学习的算法，证明了其准确性的样本复杂度界限，并通过展示一种匹配的下界证明了算法的极小-极大最优性。此外，在机器学习引导编译器优化的设置中应用该算法，学习用于内联程序的策略，目标是创建一个小的二进制文件。我们证明可以通过我们的方法的几次迭代学习到一个优于通过标准RL学到的初始策略的策略。

    arXiv:2403.19462v1 Announce Type: new  Abstract: This work studies a Reinforcement Learning (RL) problem in which we are given a set of trajectories collected with K baseline policies. Each of these policies can be quite suboptimal in isolation, and have strong performance in complementary parts of the state space. The goal is to learn a policy which performs as well as the best combination of baselines on the entire state space. We propose a simple imitation learning based algorithm, show a sample complexity bound on its accuracy and prove that the the algorithm is minimax optimal by showing a matching lower bound. Further, we apply the algorithm in the setting of machine learning guided compiler optimization to learn policies for inlining programs with the objective of creating a small binary. We demonstrate that we can learn a policy that outperforms an initial policy learned via standard RL through a few iterations of our approach.
    
[^29]: Fisher-Rao线性规划和状态-动作自然策略梯度的梯度流

    Fisher-Rao Gradient Flows of Linear Programs and State-Action Natural Policy Gradients

    [https://arxiv.org/abs/2403.19448](https://arxiv.org/abs/2403.19448)

    研究了基于Fisher信息矩阵的自然梯度方法在线性规划中的应用，展示了线性收敛性，提出了改进现有结果的熵正则化误差估计，并对扰动的Fisher-Rao梯度流和自然梯度流的次线性收敛性进行了研究。

    

    Kakade的自然策略梯度方法近年来得到广泛研究，表明在有或无正则化的情况下具有线性收敛性。我们研究了另一种基于状态-动作分布的Fisher信息矩阵的自然梯度方法，但在理论方面接受度较低。在这里，状态-动作分布在状态-动作多面体内遵循Fisher-Rao梯度流，相对于线性势。因此，我们更全面地研究线性规划的Fisher-Rao梯度流，并显示了线性收敛性，其速率取决于线性规划的几何特性。换句话说，这提供了线性规划的熵正则化引起的误差估计，这改进了现有结果。我们拓展了这些结果，并展示了对扰动的Fisher-Rao梯度流和自然梯度流的次线性收敛性，直到逼近误差。

    arXiv:2403.19448v1 Announce Type: cross  Abstract: Kakade's natural policy gradient method has been studied extensively in the last years showing linear convergence with and without regularization. We study another natural gradient method which is based on the Fisher information matrix of the state-action distributions and has received little attention from the theoretical side. Here, the state-action distributions follow the Fisher-Rao gradient flow inside the state-action polytope with respect to a linear potential. Therefore, we study Fisher-Rao gradient flows of linear programs more generally and show linear convergence with a rate that depends on the geometry of the linear program. Equivalently, this yields an estimate on the error induced by entropic regularization of the linear program which improves existing results. We extend these results and show sublinear convergence for perturbed Fisher-Rao gradient flows and natural gradient flows up to an approximation error. In particul
    
[^30]: 透明且临床可解释的人工智能用于胸部X射线肺癌检测

    Transparent and Clinically Interpretable AI for Lung Cancer Detection in Chest X-Rays

    [https://arxiv.org/abs/2403.19444](https://arxiv.org/abs/2403.19444)

    使用概念瓶颈模型的ante-hoc方法将临床概念引入到分类管道中，提供了肺癌检测决策过程中的有价值见解，相较于基线深度学习模型实现了更好的分类性能（F1 > 0.9）。

    

    arXiv:2403.19444v1 公告类型：新 简要摘要：透明人工智能（XAI）领域正在迅速发展，旨在解决复杂黑匣子深度学习模型在现实应用中的信任问题。现有的事后XAI技术最近已被证明在医疗数据上表现不佳，产生不可靠的解释，不适合临床使用。为解决这一问题，我们提出了一种基于概念瓶颈模型的ante-hoc方法，首次将临床概念引入分类管道，使用户可以深入了解决策过程。在一个大型公共数据集上，我们聚焦于胸部X射线和相关医疗报告的二元分类任务，即肺癌的检测。与基准深度学习模型相比，我们的方法在肺癌检测中获得了更好的分类性能（F1 > 0.9），同时生成了临床相关且更可靠的解释。

    arXiv:2403.19444v1 Announce Type: new  Abstract: The rapidly advancing field of Explainable Artificial Intelligence (XAI) aims to tackle the issue of trust regarding the use of complex black-box deep learning models in real-world applications. Existing post-hoc XAI techniques have recently been shown to have poor performance on medical data, producing unreliable explanations which are infeasible for clinical use. To address this, we propose an ante-hoc approach based on concept bottleneck models which introduces for the first time clinical concepts into the classification pipeline, allowing the user valuable insight into the decision-making process. On a large public dataset of chest X-rays and associated medical reports, we focus on the binary classification task of lung cancer detection. Our approach yields improved classification performance in lung cancer detection when compared to baseline deep learning models (F1 > 0.9), while also generating clinically relevant and more reliable
    
[^31]: 利用个体图结构提升生态瞬时评估（EMA）的预测能力

    Exploiting Individual Graph Structures to Enhance Ecological Momentary Assessment (EMA) Forecasting

    [https://arxiv.org/abs/2403.19442](https://arxiv.org/abs/2403.19442)

    利用循环和时间图神经网络（GNN）结合图内部关系信息，显著减少了均方误差（MSE）并提升了生态瞬时评估（EMA）数据预测的准确性。

    

    在不断发展的心理病理学领域中，准确评估和预测从生态瞬时评估（EMA）获取的数据至关重要。EMA提供随时间丰富的情境性心理病理测量，实际上会导致多元时间序列（MTS）数据。因此，分析中会出现许多挑战，源于情绪、行为和情境EMA数据固有的时间复杂性以及它们之间的相互依赖性。为了解决这两个方面，本研究调查了循环和时间图神经网络（GNN）的表现。总体而言，通过合并反映变量之间内部关系的图中的额外信息，GNN显着提高了结果，将均方误差（MSE）从1.02降低到0.84，与基线LSTM模型相比。因此，还探讨了利用不同特征构建图对GNN性能的影响。

    arXiv:2403.19442v1 Announce Type: new  Abstract: In the evolving field of psychopathology, the accurate assessment and forecasting of data derived from Ecological Momentary Assessment (EMA) is crucial. EMA offers contextually-rich psychopathological measurements over time, that practically lead to Multivariate Time Series (MTS) data. Thus, many challenges arise in analysis from the temporal complexities inherent in emotional, behavioral, and contextual EMA data as well as their inter-dependencies. To address both of these aspects, this research investigates the performance of Recurrent and Temporal Graph Neural Networks (GNNs). Overall, GNNs, by incorporating additional information from graphs reflecting the inner relationships between the variables, notably enhance the results by decreasing the Mean Squared Error (MSE) to 0.84 compared to the baseline LSTM model at 1.02. Therefore, the effect of constructing graphs with different characteristics on GNN performance is also explored. Ad
    
[^32]: 基于新型随机Transformer的方法，利用临床面谈的音频记录检测创伤后应激障碍

    A Novel Stochastic Transformer-based Approach for Post-Traumatic Stress Disorder Detection using Audio Recording of Clinical Interviews

    [https://arxiv.org/abs/2403.19441](https://arxiv.org/abs/2403.19441)

    提出了一种基于深度学习和随机Transformer的方法，利用临床面谈的音频记录实现了最新水平的创伤后应激障碍检测。

    

    创伤后应激障碍（PTSD）是一种在目睹或体验极端创伤事件后可能产生的精神障碍。PTSD可以影响任何人，无论种族或文化背景如何。被估计每十一人中就有一人在他们的一生中会经历PTSD。临床施用的PTSD量表（CAPS）和民用PTSD量表（PCL-C）的面谈是PTSD诊断的黄金标准。这些问卷可以被受试者的回答所欺骗。这项工作提出了一个基于深度学习的方法，利用临床面谈的音频记录在PTSD检测中取得了最新水平表现。我们的方法基于从临床面谈的音频记录中提取的MFCC低级特征，接着通过使用随机Transformer进行深度高级学习。我们提出的方法在eDAIC数据集上以2.92的RMSE实现了最新水平表现。

    arXiv:2403.19441v1 Announce Type: cross  Abstract: Post-traumatic stress disorder (PTSD) is a mental disorder that can be developed after witnessing or experiencing extremely traumatic events. PTSD can affect anyone, regardless of ethnicity, or culture. An estimated one in every eleven people will experience PTSD during their lifetime. The Clinician-Administered PTSD Scale (CAPS) and the PTSD Check List for Civilians (PCL-C) interviews are gold standards in the diagnosis of PTSD. These questionnaires can be fooled by the subject's responses. This work proposes a deep learning-based approach that achieves state-of-the-art performances for PTSD detection using audio recordings during clinical interviews. Our approach is based on MFCC low-level features extracted from audio recordings of clinical interviews, followed by deep high-level learning using a Stochastic Transformer. Our proposed approach achieves state-of-the-art performances with an RMSE of 2.92 on the eDAIC dataset thanks to t
    
[^33]: 在大规模个体fMRI数据集中扩展岭回归进行脑编码

    Scaling up ridge regression for brain encoding in a massive individual fMRI dataset

    [https://arxiv.org/abs/2403.19421](https://arxiv.org/abs/2403.19421)

    该论文评估了不同的并行化技术，以缩短岭回归脑编码模型训练时间，并在大规模深度fMRI数据集上取得了成功。

    

    使用神经影像数据进行大脑编码是一种旨在直接从复杂刺激特征（如电影帧）预测人类大脑活动的分析方法。岭回归是一种流行的脑编码预测模型，因为它具有良好的样本外泛化性能。然而，在处理包含许多大规模深度功能磁共振成像（fMRI）数据集时，训练岭回归模型可能非常耗时，这些数据集包括许多脑活动的空间-时间样本。本文评估了不同的并行化技术，以减少在CNeuroMod Friends数据集上使用岭回归进行脑编码的训练时间，该数据集是目前可用的最大的深度fMRI资源之一。通过多线程，我们的结果表明，Intel Math Kernel库（

    arXiv:2403.19421v1 Announce Type: cross  Abstract: Brain encoding with neuroimaging data is an established analysis aimed at predicting human brain activity directly from complex stimuli features such as movie frames. Typically, these features are the latent space representation from an artificial neural network, and the stimuli are image, audio, or text inputs. Ridge regression is a popular prediction model for brain encoding due to its good out-of-sample generalization performance. However, training a ridge regression model can be highly time-consuming when dealing with large-scale deep functional magnetic resonance imaging (fMRI) datasets that include many space-time samples of brain activity. This paper evaluates different parallelization techniques to reduce the training time of brain encoding with ridge regression on the CNeuroMod Friends dataset, one of the largest deep fMRI resource currently available. With multi-threading, our results show that the Intel Math Kernel Library (
    
[^34]: 排名中的公平性：通过随机化实现抗干扰而无需受保护属性

    Fairness in Ranking: Robustness through Randomization without the Protected Attribute

    [https://arxiv.org/abs/2403.19419](https://arxiv.org/abs/2403.19419)

    提出了一种针对排名后处理的随机化方法，无需受保护属性，通过数值研究显示了方法相对于基线排名的P-公平性和相对于归一化折扣累计增益(NDCG)的有效性的稳健性。

    

    关于机器学习中的公平性，尤其是与分类问题有关的公平性，引起了极大兴趣。在涉及排名的问题中，如在线广告、推荐系统和人力资源自动化中，仍然需要进行大量公平性方面的工作。两个复杂之处在于：首先，在许多应用程序中可能无法获取受保护属性。其次，排名的公平性存在多个衡量标准，基于单个衡量标准的优化方法可能会产生相对其他衡量标准不公平的排名。在这项工作中，我们提出了一种针对排名后处理的随机化方法，不需要受保护属性的可用性。通过广泛的数值研究，我们展示了我们的方法相对于基线排名的P-公平性和相对于归一化折扣累计增益(NDCG)的效果的稳健性，改进了先前提出的方法。

    arXiv:2403.19419v1 Announce Type: cross  Abstract: There has been great interest in fairness in machine learning, especially in relation to classification problems. In ranking-related problems, such as in online advertising, recommender systems, and HR automation, much work on fairness remains to be done. Two complications arise: first, the protected attribute may not be available in many applications. Second, there are multiple measures of fairness of rankings, and optimization-based methods utilizing a single measure of fairness of rankings may produce rankings that are unfair with respect to other measures. In this work, we propose a randomized method for post-processing rankings, which do not require the availability of the protected attribute. In an extensive numerical study, we show the robustness of our methods with respect to P-Fairness and effectiveness with respect to Normalized Discounted Cumulative Gain (NDCG) from the baseline ranking, improving on previously proposed meth
    
[^35]: 运动的守恒和非守恒动力学的运动积分

    Constants of Motion for Conserved and Non-conserved Dynamics

    [https://arxiv.org/abs/2403.19418](https://arxiv.org/abs/2403.19418)

    该论文通过机器学习技术得到的动态模型，结合李对称技术分析得到了守恒和非守恒情况下1D和2D谐振子的运动积分，展示了非守恒模型中存在的能量守恒常数，以及在各种频率比例情况下推广了角动量。

    

    这篇论文首先介绍了一个通过应用机器学习技术（FJet）到时间序列数据获得的动力学模型；然后利用李对称技术对该动力学模型进行分析以获得运动积分。该分析针对1D和2D谐振子的守恒和非守恒情况进行。对于1D谐振子，在欠阻尼、过阻尼和临界阻尼情况下找到了运动积分。对于非守恒模型的存在这样的常数是对整个系统（即振荡器加耗散环境）能量守恒的一种表现的新颖解释。对于2D谐振子，在等向和非等向情况下找到了运动积分，包括频率不可共轭的情况；还推广到任意维度。此外，还确定了一个常数，它将角动量推广到所有频率比例的情况。

    arXiv:2403.19418v1 Announce Type: new  Abstract: This paper begins with a dynamical model that was obtained by applying a machine learning technique (FJet) to time-series data; this dynamical model is then analyzed with Lie symmetry techniques to obtain constants of motion. This analysis is performed on both the conserved and non-conserved cases of the 1D and 2D harmonic oscillators. For the 1D oscillator, constants are found in the cases where the system is underdamped, overdamped, and critically damped. The novel existence of such a constant for a non-conserved model is interpreted as a manifestation of the conservation of energy of the {\em total} system (i.e., oscillator plus dissipative environment). For the 2D oscillator, constants are found for the isotropic and anisotropic cases, including when the frequencies are incommensurate; it is also generalized to arbitrary dimensions. In addition, a constant is identified which generalizes angular momentum for all ratios of the frequen
    
[^36]: 表格学习：实体和上下文嵌入的编码

    Tabular Learning: Encoding for Entity and Context Embeddings

    [https://arxiv.org/abs/2403.19405](https://arxiv.org/abs/2403.19405)

    挑战常用的序数编码，提出基于字符串相似性编码的表格学习方法，取得了更好的分类效果和性能提升。

    

    本文研究了不同编码技术对实体和上下文嵌入的影响，旨在挑战常用的序数编码在表格学习中的应用。通过在多个数据集上应用不同的预处理方法和网络架构，对编码器如何影响网络学习结果进行了评估。通过保持测试、验证和训练数据的一致性，结果表明，对于分类数据，序数编码并不是最合适的编码器，无法正确预处理数据并分类目标变量。通过基于字符串相似性对特征进行编码，计算相似性矩阵作为网络输入，取得了更好的结果。这适用于实体和上下文嵌入，在多标签分类中，变换器架构在序数编码和相似性编码方面表现出更好的性能。

    arXiv:2403.19405v1 Announce Type: cross  Abstract: Examining the effect of different encoding techniques on entity and context embeddings, the goal of this work is to challenge commonly used Ordinal encoding for tabular learning. Applying different preprocessing methods and network architectures over several datasets resulted in a benchmark on how the encoders influence the learning outcome of the networks. By keeping the test, validation and training data consistent, results have shown that ordinal encoding is not the most suited encoder for categorical data in terms of preprocessing the data and thereafter, classifying the target variable correctly. A better outcome was achieved, encoding the features based on string similarities by computing a similarity matrix as input for the network. This is the case for both, entity and context embeddings, where the transformer architecture showed improved performance for Ordinal and Similarity encoding with regard to multi-label classification 
    
[^37]: 从标签比例学习布尔函数的困难性

    Hardness of Learning Boolean Functions from Label Proportions

    [https://arxiv.org/abs/2403.19401](https://arxiv.org/abs/2403.19401)

    本研究针对从标签比例学习布尔函数的难题性展开研究，发现在特定情况下寻找满足子集合中常数比例的布尔函数的子句是NP难的。

    

    近年来，从标签比例进行学习(LLP)的框架在机器学习中变得越来越重要。在这种情况下，训练示例被聚合到子集或袋中，只有每个袋的平均标签可用于学习一个基于示例的预测器。这是对传统的PAC学习的推广，后者是单位大小袋的特例。最近的研究对LLP的计算学习方面进行了研究(Saket, NeurIPS'21; Saket, NeurIPS'22)，展示了在LLP设置中学习半空间的算法和困难。在这项工作中，我们专注于LLP学习布尔函数的难题。我们的第一个结果表明，考虑到大小不超过$2$的袋子集合，这些袋子集合与OR函数一致时，要找到一个满足任何常数部分袋子的常数多子句CNF是NP难的。这与(Saket, NeurIPS'21)的工作形成对比，后者提供了一个$(2/5)$-approx。

    arXiv:2403.19401v1 Announce Type: cross  Abstract: In recent years the framework of learning from label proportions (LLP) has been gaining importance in machine learning. In this setting, the training examples are aggregated into subsets or bags and only the average label per bag is available for learning an example-level predictor. This generalizes traditional PAC learning which is the special case of unit-sized bags. The computational learning aspects of LLP were studied in recent works (Saket, NeurIPS'21; Saket, NeurIPS'22) which showed algorithms and hardness for learning halfspaces in the LLP setting. In this work we focus on the intractability of LLP learning Boolean functions. Our first result shows that given a collection of bags of size at most $2$ which are consistent with an OR function, it is NP-hard to find a CNF of constantly many clauses which satisfies any constant-fraction of the bags. This is in contrast with the work of (Saket, NeurIPS'21) which gave a $(2/5)$-approx
    
[^38]: 关于近贝叶斯最优算法的不确定性量化

    On Uncertainty Quantification for Near-Bayes Optimal Algorithms

    [https://arxiv.org/abs/2403.19381](https://arxiv.org/abs/2403.19381)

    该论文提出了一种基于常用机器学习算法的近似贝叶斯最优方法，可以恢复由未知任务分布定义的贝叶斯后验，并提出了一种通用的不确定性量化方法。

    

    贝叶斯建模允许对预测不确定性进行量化，在安全关键应用中至关重要。然而，对于许多机器学习（ML）算法，构建或实现它们的贝叶斯对应是困难的。 在这项工作中，我们提出了一种解决这一挑战的有前途的方法，该方法基于常用的ML算法在各种任务中高效，并且可能在未知任务分布下接近贝叶斯最优。我们证明了通过使用该算法构建一个鞅后验，可以恢复由任务分布定义的贝叶斯后验，在这种设置中是未知但最优的。我们进一步提出了一种适用于通用ML算法的实用不确定性量化方法。基于各种非NN和NN算法的实验表明了我们方法的效果。

    arXiv:2403.19381v1 Announce Type: cross  Abstract: Bayesian modelling allows for the quantification of predictive uncertainty which is crucial in safety-critical applications. Yet for many machine learning (ML) algorithms, it is difficult to construct or implement their Bayesian counterpart. In this work we present a promising approach to address this challenge, based on the hypothesis that commonly used ML algorithms are efficient across a wide variety of tasks and may thus be near Bayes-optimal w.r.t. an unknown task distribution. We prove that it is possible to recover the Bayesian posterior defined by the task distribution, which is unknown but optimal in this setting, by building a martingale posterior using the algorithm. We further propose a practical uncertainty quantification method that apply to general ML algorithms. Experiments based on a variety of non-NN and NN algorithms demonstrate the efficacy of our method.
    
[^39]: 基于人工智能（AI）的新冠肺炎患者死亡率预测

    Artificial Intelligence (AI) Based Prediction of Mortality, for COVID-19 Patients

    [https://arxiv.org/abs/2403.19355](https://arxiv.org/abs/2403.19355)

    通过研究九种机器学习和深度学习算法，以及两种特征选择方法的结合，本研究发现预测新冠肺炎患者死亡率、ICU需求和通气天数的最后状态中，仅有10个特征对预测具有用处，其中急性肾损伤特征最为重要。

    

    对于严重感染的新冠肺炎患者，识别高风险患者并预测生存和是否需要重症监护（ICU）至关重要。本研究调查了九种机器学习和深度学习算法与两种广泛使用的特征选择方法结合的性能，以预测代表死亡、ICU需求和通气天数的最后状态。采用五折交叉验证进行训练和验证。为减少偏差，训练和测试集根据相似分布拆分。预测建模中发现仅有10个特征在预测中有用，其中在住院期间发生急性肾损伤的特征最为重要。算法的性能取决于特征数量和数据预处理。

    arXiv:2403.19355v1 Announce Type: new  Abstract: For severely affected COVID-19 patients, it is crucial to identify high-risk patients and predict survival and need for intensive care (ICU). Most of the proposed models are not well reported making them less reproducible and prone to high risk of bias particularly in presence of imbalance data/class. In this study, the performances of nine machine and deep learning algorithms in combination with two widely used feature selection methods were investigated to predict last status representing mortality, ICU requirement, and ventilation days. Fivefold cross-validation was used for training and validation purposes. To minimize bias, the training and testing sets were split maintaining similar distributions. Only 10 out of 122 features were found to be useful in prediction modelling with Acute kidney injury during hospitalization feature being the most important one. The algorithms performances depend on feature numbers and data pre-processin
    
[^40]: 一种用于收集和学习复杂注释的交互式人机学习界面

    An Interactive Human-Machine Learning Interface for Collecting and Learning from Complex Annotations

    [https://arxiv.org/abs/2403.19339](https://arxiv.org/abs/2403.19339)

    提出了一种用于二元分类任务的交互式人机学习界面，允许人类注释者利用反事实例来补充标准二元标签，并讨论了未来延伸工作的挑战。

    

    人机交互已被证明可以通过提升模型性能、加速学习并增强用户信心来改善机器学习系统。本研究旨在减轻人类注释者需适应传统标签所施加的限制的期望，为此，我们提出了一种用于二元分类任务的人机学习界面，使人类注释者可以利用反事实例来补充标准的二元标签作为数据集的注释。最后，我们讨论了将来延伸此工作所面临的挑战。

    arXiv:2403.19339v1 Announce Type: new  Abstract: Human-Computer Interaction has been shown to lead to improvements in machine learning systems by boosting model performance, accelerating learning and building user confidence. In this work, we aim to alleviate the expectation that human annotators adapt to the constraints imposed by traditional labels by allowing for extra flexibility in the form that supervision information is collected. For this, we propose a human-machine learning interface for binary classification tasks which enables human annotators to utilise counterfactual examples to complement standard binary labels as annotations for a dataset. Finally we discuss the challenges in future extensions of this work.
    
[^41]: MedBN: 针对恶意测试样本的鲁棒测试时间适应

    MedBN: Robust Test-Time Adaptation against Malicious Test Samples

    [https://arxiv.org/abs/2403.19326](https://arxiv.org/abs/2403.19326)

    MedBN是一种针对恶意测试样本的新方法，利用中值批次归一化（MedBN）在测试时间推断期间保护模型免受攻击，并成功集成到现有的TTA框架中。

    

    Test-time adaptation (TTA)已经成为解决由于训练和测试数据之间未预料到的分布转移而导致性能下降的一个有希望的解决方案。虽然最近的TTA方法在适应于测试数据的变化方面表现出色，但这种适应性使模型暴露于恶意示例的攻击中，这一方面受到了有限的关注。之前的研究已经发现，即使测试批次的一小部分受到恶意篡改，也会暴露出TTA中的安全漏洞。作为对新兴威胁的回应，我们提出了中值批次归一化（MedBN），利用了中值对在测试时间推断期间批次归一化层内的统计估计的稳健性。我们的方法是算法不可知的，因此可以与现有的TTA框架无缝集成。我们在基准数据集，包括CIFAR10-C、CIFAR100-C和ImageNet-C上的实验结果一致表明，MedBN性能优越

    arXiv:2403.19326v1 Announce Type: new  Abstract: Test-time adaptation (TTA) has emerged as a promising solution to address performance decay due to unforeseen distribution shifts between training and test data. While recent TTA methods excel in adapting to test data variations, such adaptability exposes a model to vulnerability against malicious examples, an aspect that has received limited attention. Previous studies have uncovered security vulnerabilities within TTA even when a small proportion of the test batch is maliciously manipulated. In response to the emerging threat, we propose median batch normalization (MedBN), leveraging the robustness of the median for statistics estimation within the batch normalization layer during test-time inference. Our method is algorithm-agnostic, thus allowing seamless integration with existing TTA frameworks. Our experimental results on benchmark datasets, including CIFAR10-C, CIFAR100-C and ImageNet-C, consistently demonstrate that MedBN outperf
    
[^42]: 基于超图的多视角事件相机动作识别

    Hypergraph-based Multi-View Action Recognition using Event Cameras

    [https://arxiv.org/abs/2403.19316](https://arxiv.org/abs/2403.19316)

    提出了一个名为HyperMV的多视角事件动作识别框架，实现了将离散事件数据转换成帧状表示，并利用共享卷积网络提取视角相关特征。

    

    视频数据的动作识别是具有广泛应用的基石。单视角动作识别由于依赖单一视角而面临限制。相比之下，多视角方法从不同视角捕获互补信息以提高准确性。最近，事件相机作为创新的仿生传感器崭露头角，为基于事件的动作识别带来了进展。然而，现有作品主要关注单一视角场景，在多视角事件数据利用方面存在空白，特别是在信息不足和语义错配等挑战方面。为了弥补这一空白，我们引入了HyperMV，这是一个多视角基于事件的动作识别框架。HyperMV将离散事件数据转换成类似帧的表示，并使用共享的卷积网络提取与视角相关的特征。通过将段视为顶点并使用基于规则的方法构建超边

    arXiv:2403.19316v1 Announce Type: cross  Abstract: Action recognition from video data forms a cornerstone with wide-ranging applications. Single-view action recognition faces limitations due to its reliance on a single viewpoint. In contrast, multi-view approaches capture complementary information from various viewpoints for improved accuracy. Recently, event cameras have emerged as innovative bio-inspired sensors, leading to advancements in event-based action recognition. However, existing works predominantly focus on single-view scenarios, leaving a gap in multi-view event data exploitation, particularly in challenges like information deficit and semantic misalignment. To bridge this gap, we introduce HyperMV, a multi-view event-based action recognition framework. HyperMV converts discrete event data into frame-like representations and extracts view-related features using a shared convolutional network. By treating segments as vertices and constructing hyperedges using rule-based and
    
[^43]: FlowDepth: 解耦光流用于自监督单目深度估计

    FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth Estimation

    [https://arxiv.org/abs/2403.19294](https://arxiv.org/abs/2403.19294)

    FlowDepth提出了通过Dynamic Motion Flow Module（DMFM）解耦光流，通过运用基于机制的方法解决移动物体引起的不匹配问题，同时使用Depth-Cue-Aware Blur（DCABlur）和Cost-Volume稀疏损失来解决高频和低纹理区域的光度误差不公平问题。

    

    自监督多帧方法目前在深度估计方面取得了令人期待的结果。然而，这些方法经常因为移动物体而遭受不匹配问题，这打破了静态假设。此外，在计算图像的高频或低纹理区域的光度误差时可能会出现不公平现象。为了解决这些问题，现有方法使用额外的语义先验黑盒网络来分离移动物体，并仅在损失水平上改进模型。因此，我们提出了FlowDepth，其中动态运动光流模块（DMFM）通过基于机制的方法解耦光流，并对动态区域进行变形，从而解决了不匹配问题。为解决由高频和低纹理区域引起的光度误差的不公平问题，我们分别在输入和损失水平上使用Depth-Cue-Aware Blur（DCABlur）和Cost-Volume稀疏损失来解决这个问题。

    arXiv:2403.19294v1 Announce Type: cross  Abstract: Self-supervised multi-frame methods have currently achieved promising results in depth estimation. However, these methods often suffer from mismatch problems due to the moving objects, which break the static assumption. Additionally, unfairness can occur when calculating photometric errors in high-freq or low-texture regions of the images. To address these issues, existing approaches use additional semantic priori black-box networks to separate moving objects and improve the model only at the loss level. Therefore, we propose FlowDepth, where a Dynamic Motion Flow Module (DMFM) decouples the optical flow by a mechanism-based approach and warps the dynamic regions thus solving the mismatch problem. For the unfairness of photometric errors caused by high-freq and low-texture regions, we use Depth-Cue-Aware Blur (DCABlur) and Cost-Volume sparsity loss respectively at the input and the loss level to solve the problem. Experimental results 
    
[^44]: 用于治疗效果预测的图神经网络

    Graph Neural Networks for Treatment Effect Prediction

    [https://arxiv.org/abs/2403.19289](https://arxiv.org/abs/2403.19289)

    提出了一种图神经网络来减少治疗效果预测所需的训练集大小，有效利用电子商务数据的图结构，为治疗效果预测带来新的可能性

    

    在电子商务中估计因果效应往往涉及昂贵的治疗分配，这在大规模设置中可能是不切实际的。利用机器学习来预测这种治疗效果而无需实际干预是减少风险的一种标准做法。然而，现有的治疗效果预测方法往往依赖于大规模实验构建的训练集，因此从根本上存在风险。在这项工作中，我们提出了一种图神经网络，以减少所需的训练集大小，依赖于电子商务数据中常见的图。具体地，我们将问题视为具有有限数量标记实例的节点回归，开发了一个类似于先前因果效应估计器的双模型神经架构，并测试了不同的消息传递层进行编码。此外，作为额外步骤，我们将模型与获取函数相结合，以引导信息传递。

    arXiv:2403.19289v1 Announce Type: cross  Abstract: Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of th
    
[^45]: 使用奖励学习在策略上微调语言模型

    Fine-Tuning Language Models with Reward Learning on Policy

    [https://arxiv.org/abs/2403.19279](https://arxiv.org/abs/2403.19279)

    提出了在策略上的奖励学习框架，使用策略样本优化奖励模型以保持其分布上的一致性

    

    强化学习从人类反馈（RLHF）作为一种有效的方法出现，用于使大型语言模型（LLMs）与人类偏好保持一致。RLHF包含三个步骤，即收集人类偏好、奖励学习和策略优化，通常是串行执行的。然而，（固定的）奖励模型可能会因为策略优化不断改变LLMs的数据分布而遭受不准确的离分布情况。从最新的LLMs重复收集新的偏好数据可能会缓解这个问题，但不幸的是，这会使得结果系统更加复杂和难以优化。在本文中，我们提出了在策略上的奖励学习（RLP），这是一个无监督的框架，使用策略样本来优化奖励模型以保持其分布上的一致性。具体而言，引入了一种无监督的多视图学习方法来学习策略样本的稳健表示。

    arXiv:2403.19279v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences. RLHF contains three steps, i.e., human preference collecting, reward learning, and policy optimization, which are usually performed serially. Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs' data distribution. Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize. In this paper, we propose reward learning on policy (RLP), an unsupervised framework that refines a reward model using policy samples to keep it on-distribution. Specifically, an unsupervised multi-view learning method is introduced to learn robust representations of policy samples. Meanwhile, a synthetic
    
[^46]: 一种整合土壤营养和气象因素的作物产量和病害预测的机器学习方法

    A Machine Learning Approach for Crop Yield and Disease Prediction Integrating Soil Nutrition and Weather Factors

    [https://arxiv.org/abs/2403.19273](https://arxiv.org/abs/2403.19273)

    本文提出了一种整合土壤营养和气象因素的机器学习方法，用于作物产量和病害预测，致力于解决孟加拉国农业中作物选择和病害预测中的挑战。

    

    本文的主要目标是开发一个智能的农业决策支持系统，用于孟加拉国的作物选择和病害预测。该国家的经济在很大程度上依赖于农业。然而，选择生产率更高的作物以及有效控制作物病害是农民必须面对的障碍。本研究通过利用机器学习方法和实际数据集来解决这些问题。推荐的方法使用了多种数据集，涵盖作物生产、土壤条件、农业气象区域、作物病害和气象因素。这些数据集提供了关于病害趋势、作物对土壤营养需求以及农业生产历史的有见地信息。通过整合这些知识，该模型首先根据特定用户位置的土壤营养推荐主要选定作物的列表。然后，通过将来自不同数据源的信息进行整合，模型可以预测不同作物的产量和病害情况。

    arXiv:2403.19273v1 Announce Type: cross  Abstract: The development of an intelligent agricultural decision-supporting system for crop selection and disease forecasting in Bangladesh is the main objective of this work. The economy of the nation depends heavily on agriculture. However, choosing crops with better production rates and efficiently controlling crop disease are obstacles that farmers have to face. These issues are addressed in this research by utilizing machine learning methods and real-world datasets. The recommended approach uses a variety of datasets on the production of crops, soil conditions, agro-meteorological regions, crop disease, and meteorological factors. These datasets offer insightful information on disease trends, soil nutrition demand of crops, and agricultural production history. By incorporating this knowledge, the model first recommends the list of primarily selected crops based on the soil nutrition of a particular user location. Then the predictions of me
    
[^47]: 不需要地面真实数据收集：使用深度强化学习进行自监督测距误差校正

    Removing the need for ground truth UWB data collection: self-supervised ranging error correction using deep reinforcement learning

    [https://arxiv.org/abs/2403.19262](https://arxiv.org/abs/2403.19262)

    提出了一种使用深度强化学习进行自监督测距误差校正的方法，无需收集地面真实数据集，实验表明性能与有监督方法相媲美

    

    室内定位利用UWB技术因其厘米级准确度潜力而备受关注。然而，多径效应和非直射条件导致了基站与标签之间的测距误差。现有的缓解这些测距误差的方法依赖于收集大量标记数据集，这使它们在实际部署中不切实际。本文提出了一种新颖的自监督深度强化学习方法，不需要标记的地面真实数据。一个强化学习代理将通道脉冲响应作为状态，并预测校正以减小校正和估计测距之间的误差。该代理通过结合轨迹的可预测性与过滤和平滑处理生成的校正，自监督地学习和迭代改进。对真实世界UWB测量的实验表明，其性能与最先进的监督方法相媲美。

    arXiv:2403.19262v1 Announce Type: cross  Abstract: Indoor positioning using UWB technology has gained interest due to its centimeter-level accuracy potential. However, multipath effects and non-line-of-sight conditions cause ranging errors between anchors and tags. Existing approaches for mitigating these ranging errors rely on collecting large labeled datasets, making them impractical for real-world deployments. This paper proposes a novel self-supervised deep reinforcement learning approach that does not require labeled ground truth data. A reinforcement learning agent uses the channel impulse response as a state and predicts corrections to minimize the error between corrected and estimated ranges. The agent learns, self-supervised, by iteratively improving corrections that are generated by combining the predictability of trajectories with filtering and smoothening. Experiments on real-world UWB measurements demonstrate comparable performance to state-of-the-art supervised methods, o
    
[^48]: 推断多智能体强化学习的潜在时间稀疏协调图

    Inferring Latent Temporal Sparse Coordination Graph for Multi-Agent Reinforcement Learning

    [https://arxiv.org/abs/2403.19253](https://arxiv.org/abs/2403.19253)

    提出了一种用于多智能体强化学习的潜在时间稀疏协调图，能够有效处理智能体之间的协作关系并利用历史观测来进行知识交换

    

    有效的智能体协调对于合作式多智能体强化学习(MARL)至关重要。当前MARL中的图学习方法局限性较大，仅仅依赖一步观察，忽略了重要的历史经验，导致生成的图存在缺陷，促进了冗余或有害信息交换。为了解决这些挑战，我们提出了推断用于MARL的潜在时间稀疏协调图（LTS-CG）。LTS-CG利用智能体的历史观测来计算智能体对概率矩阵，从中抽取稀疏图并用于智能体之间的知识交换，从而同时捕捉智能体的依赖关系和关系不确定性。该过程的计算复杂性仅与智能

    arXiv:2403.19253v1 Announce Type: new  Abstract: Effective agent coordination is crucial in cooperative Multi-Agent Reinforcement Learning (MARL). While agent cooperation can be represented by graph structures, prevailing graph learning methods in MARL are limited. They rely solely on one-step observations, neglecting crucial historical experiences, leading to deficient graphs that foster redundant or detrimental information exchanges. Additionally, high computational demands for action-pair calculations in dense graphs impede scalability. To address these challenges, we propose inferring a Latent Temporal Sparse Coordination Graph (LTS-CG) for MARL. The LTS-CG leverages agents' historical observations to calculate an agent-pair probability matrix, where a sparse graph is sampled from and used for knowledge exchange between agents, thereby simultaneously capturing agent dependencies and relation uncertainty. The computational complexity of this procedure is only related to the number o
    
[^49]: MPXGAT：一种用于多重图嵌入的基于注意力机制的深度学习模型

    MPXGAT: An Attention based Deep Learning Model for Multiplex Graphs Embedding

    [https://arxiv.org/abs/2403.19246](https://arxiv.org/abs/2403.19246)

    MPXGAT是一种基于注意力机制的深度学习模型，用于多重图嵌入，通过结合GATs在层内和层间连接的强大性能，实现了准确预测多重网络中各层内和层间的链接。

    

    图表示学习已迅速成为一个关键的研究领域。尽管其日益普及，但大部分研究仍仅限于嵌入单层图，这在表示具有多方面关系的复杂系统时不足以胜任。为了填补这一空白，我们引入了MPXGAT，这是一种创新的基于注意力机制的深度学习模型，专门用于多重图嵌入。通过利用图注意力网络（GATs）的稳健性，MPXGAT通过利用层内和层间连接捕捉多重网络的结构。这种利用有助于准确预测网络多个层内和层间的链接。我们在各种基准数据集上进行的全面实验证实，MPXGAT始终优于当前最先进的竞争算法。

    arXiv:2403.19246v1 Announce Type: new  Abstract: Graph representation learning has rapidly emerged as a pivotal field of study. Despite its growing popularity, the majority of research has been confined to embedding single-layer graphs, which fall short in representing complex systems with multifaceted relationships. To bridge this gap, we introduce MPXGAT, an innovative attention-based deep learning model tailored to multiplex graph embedding. Leveraging the robustness of Graph Attention Networks (GATs), MPXGAT captures the structure of multiplex networks by harnessing both intra-layer and inter-layer connections. This exploitation facilitates accurate link prediction within and across the network's multiple layers. Our comprehensive experimental evaluation, conducted on various benchmark datasets, confirms that MPXGAT consistently outperforms state-of-the-art competing algorithms.
    
[^50]: 用正弦激活的低秩矩阵实现参数高效学习

    Sine Activated Low-Rank Matrices for Parameter Efficient Learning

    [https://arxiv.org/abs/2403.19243](https://arxiv.org/abs/2403.19243)

    整合正弦函数到低秩分解过程中，提高模型准确性的同时保持参数高效性。

    

    低秩分解已经成为在神经网络架构中增强参数效率的重要工具，在机器学习的各种应用中越来越受到关注。这些技术显著降低了参数数量，取得了简洁性和性能之间的平衡。然而，一个常见的挑战是在参数效率和模型准确性之间做出妥协，参数减少往往导致准确性不及完整秩对应模型。在这项工作中，我们提出了一个创新的理论框架，在低秩分解过程中整合了一个正弦函数。这种方法不仅保留了低秩方法的参数效率特性的好处，还增加了分解的秩，从而提高了模型的准确性。我们的方法被证明是现有低秩模型的一种适应性增强，正如其成功证实的那样。

    arXiv:2403.19243v1 Announce Type: new  Abstract: Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model accuracy. Our method proves to be an adaptable enhancement for existing low-rank models, as evidenced by its successful 
    
[^51]: AZ-NAS: 针对网络架构搜索的零成本代理组装

    AZ-NAS: Assembling Zero-Cost Proxies for Network Architecture Search

    [https://arxiv.org/abs/2403.19232](https://arxiv.org/abs/2403.19232)

    AZ-NAS提出了一种新方法，通过集合各种零成本代理来增强网络排名与性能之间的相关性，在表达性、进步性、可训练性和复杂性等方面进行了分析。

    

    训练免费的网络架构搜索（NAS）旨在发现具有零成本代理的表现优异的网络，捕获与最终性能相关的网络特征。然而，先前训练免费的NAS方法估计的网络排名与性能之间的相关性较弱。为解决此问题，我们提出了AZ-NAS，一种新方法，利用各种零成本代理的集合来显著增强预测的网络排名与性能地面实况之间的相关性。为了实现这一点，我们引入了四种新的零成本代理，这些代理在表达性、进步性、可训练性和复杂性等方面相辅相成，分析了架构的不同特征。代理评分可以在单次前向和后向传递中同时获得，使整个NAS过程极其高效。

    arXiv:2403.19232v1 Announce Type: cross  Abstract: Training-free network architecture search (NAS) aims to discover high-performing networks with zero-cost proxies, capturing network characteristics related to the final performance. However, network rankings estimated by previous training-free NAS methods have shown weak correlations with the performance. To address this issue, we propose AZ-NAS, a novel approach that leverages the ensemble of various zero-cost proxies to enhance the correlation between a predicted ranking of networks and the ground truth substantially in terms of the performance. To achieve this, we introduce four novel zero-cost proxies that are complementary to each other, analyzing distinct traits of architectures in the views of expressivity, progressivity, trainability, and complexity. The proxy scores can be obtained simultaneously within a single forward and backward pass, making an overall NAS process highly efficient. In order to integrate the rankings predic
    
[^52]: 为联邦基金会模型提供双重个性化适配器

    Dual-Personalizing Adapter for Federated Foundation Models

    [https://arxiv.org/abs/2403.19211](https://arxiv.org/abs/2403.19211)

    提出了一种新的设置，称为测试时间个性化，不仅关注目标本地任务，还延伸到其他展示测试时间个性化的任务

    

    最近，基础模型，尤其是大型语言模型（LLMs），通过微调大量的指令数据，展现出了适应各种任务的令人印象深刻的能力。值得注意的是，联邦基金会模型作为一种隐私保护方法，在分布式学习（FL）环境下通过利用许多分布式数据集进行协作微调模型，这些数据集具有非IID数据。为了减轻通信和计算开销，引入了参数高效方法以提高效率，并且一些研究将个性化方法调整为联邦基金会模型，以获得更好的用户偏好对齐。然而，现有研究中存在的一个关键缺口是在真实应用中忽略了测试时间分布转移。因此，为了弥合这一差距，我们提出了一个新的设置，称为测试时间个性化，它不仅专注于目标本地任务，还延伸到其他展示测试时间个性化的任务。

    arXiv:2403.19211v1 Announce Type: cross  Abstract: Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning large amounts of instruction data. Notably, federated foundation models emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to federated foundation models for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications. Therefore, to bridge this gap, we propose a new setting, termed test-time personalization, which not only concentrates on the targeted local task but also extends to other tasks that exhibit test-t
    
[^53]: 从激活到初始化：优化神经场的扩展见解

    From Activation to Initialization: Scaling Insights for Optimizing Neural Fields

    [https://arxiv.org/abs/2403.19205](https://arxiv.org/abs/2403.19205)

    本文研究了神经场的初始化和激活之间的关系，强调了网络初始化、架构选择和优化过程之间的深层联系，为神经场的优化提供了理论基础。

    

    在计算机视觉领域，神经场作为一种当代工具，利用神经网络进行信号表示而备受重视。尽管已经在使这些网络适应解决各种问题方面取得了显著进展，但该领域仍然缺乏一个全面的理论框架。本文旨在填补这一空白，深入探讨初始化和激活之间错综复杂的相互作用，为神经场的强化优化提供基础。我们的理论见解揭示了网络初始化、架构选择和优化过程之间的深层联系，强调在设计尖端神经场时需要采用全面的方法。

    arXiv:2403.19205v1 Announce Type: cross  Abstract: In the realm of computer vision, Neural Fields have gained prominence as a contemporary tool harnessing neural networks for signal representation. Despite the remarkable progress in adapting these networks to solve a variety of problems, the field still lacks a comprehensive theoretical framework. This article aims to address this gap by delving into the intricate interplay between initialization and activation, providing a foundational basis for the robust optimization of Neural Fields. Our theoretical insights reveal a deep-seated connection among network initialization, architectural choices, and the optimization process, emphasizing the need for a holistic approach when designing cutting-edge Neural Fields.
    
[^54]: 让大型语言模型成为更好的排名器

    Make Large Language Model a Better Ranker

    [https://arxiv.org/abs/2403.19181](https://arxiv.org/abs/2403.19181)

    本文介绍了一种具有对齐列表排名目标的语言模型框架（ALRO），旨在弥合大型语言模型的能力与推荐系统排名任务的要求之间的差距。

    

    大型语言模型（LLMs）的发展显著增强了各个领域的能力，导致推荐系统（RSs）概念和开发方式发生了转变。然而，现有研究主要集中在点对点和成对推荐范式上。这些方法在基于LLM的推荐器中效率低下，因为利用大型语言模型的计算成本很高。一些研究虽然深入研究了列表型方法，但在排名任务中表现不佳。这一不足归因于排名和语言生成目标之间的不匹配。为此，本文介绍了具有对齐列表排名目标的语言模型框架（ALRO）。ALRO旨在弥合LLMs的能力与推荐系统排名任务的微妙要求之间的差距。ALRO的一个关键特性是引入了软lambda值lo

    arXiv:2403.19181v1 Announce Type: cross  Abstract: The evolution of Large Language Models (LLMs) has significantly enhanced capabilities across various fields, leading to a paradigm shift in how Recommender Systems (RSs) are conceptualized and developed. However, existing research primarily focuses on point-wise and pair-wise recommendation paradigms. These approaches prove inefficient in LLM-based recommenders due to the high computational cost of utilizing Large Language Models. While some studies have delved into list-wise approaches, they fall short in ranking tasks. This shortfall is attributed to the misalignment between the objectives of ranking and language generation. To this end, this paper introduces the Language Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks within recommender systems. A key feature of ALRO is the introduction of soft lambda lo
    
[^55]: 在分布式网络中增强信任和隐私：基于区块链的联邦学习综述

    Enhancing Trust and Privacy in Distributed Networks: A Comprehensive Survey on Blockchain-based Federated Learning

    [https://arxiv.org/abs/2403.19178](https://arxiv.org/abs/2403.19178)

    本文深入研究了基于区块链的联邦学习（BCFL），突出了区块链的安全功能与联邦学习的隐私保护模式之间的协同作用。

    

    当集中式服务器存在单点故障风险时，像区块链这样的去中心化方法通过在多个实体之间实施共识机制提供了一个引人注目的解决方案。将分布式计算与加密技术相结合，去中心化技术引入了一种新颖的计算范式。区块链通过在网络节点之间经过共识验证和记录交易来确保安全、透明和防篡改的数据管理。联邦学习（FL）作为一种分布式机器学习框架，使参与者能够在协同训练模型的同时通过避免直接原始数据交换来保护数据隐私。尽管对去中心化方法越来越感兴趣，但它们在FL中的应用仍然尚未得到充分探讨。本文深入调查了基于区块链的FL（BCFL），重点关注区块链的安全特性与FL的隐私保护模式之间的协同作用。

    arXiv:2403.19178v1 Announce Type: cross  Abstract: While centralized servers pose a risk of being a single point of failure, decentralized approaches like blockchain offer a compelling solution by implementing a consensus mechanism among multiple entities. Merging distributed computing with cryptographic techniques, decentralized technologies introduce a novel computing paradigm. Blockchain ensures secure, transparent, and tamper-proof data management by validating and recording transactions via consensus across network nodes. Federated Learning (FL), as a distributed machine learning framework, enables participants to collaboratively train models while safeguarding data privacy by avoiding direct raw data exchange. Despite the growing interest in decentralized methods, their application in FL remains underexplored. This paper presents a thorough investigation into Blockchain-based FL (BCFL), spotlighting the synergy between blockchain's security features and FL's privacy-preserving mo
    
[^56]: 在医疗保健领域评估机器学习中的公平特征选择

    Evaluating Fair Feature Selection in Machine Learning for Healthcare

    [https://arxiv.org/abs/2403.19165](https://arxiv.org/abs/2403.19165)

    通过考虑对所有人口统计群体均等重要性的公平特征选择方法，在医疗保健领域评估算法公平性，确保在减少偏见和全局分类错误之间实现平衡。

    

    随着机器学习在医疗保健领域的普及，自动化社会偏见进一步加剧健康差距的潜力构成了重大风险。我们从特征选择的角度探讨算法公平性。传统的特征选择方法通过去除资源密集、相关或不相关的特征来识别用于更好决策的特征，但忽略了这些因素在不同子群体中可能存在的差异。为了应对这些问题，我们评估了一种考虑对所有人口统计群体均等重要性的公平特征选择方法。我们在特征选择过程中同时考虑了公平性度量和错误度量，以确保在最大程度减少偏见和全局分类错误之间实现平衡。我们在三个公开可用的医疗保健数据集上测试了我们的方法。在所有三个数据集上，我们观察到公平性指标得到改善，同时分类错误仅有轻微下降。

    arXiv:2403.19165v1 Announce Type: new  Abstract: With the universal adoption of machine learning in healthcare, the potential for the automation of societal biases to further exacerbate health disparities poses a significant risk. We explore algorithmic fairness from the perspective of feature selection. Traditional feature selection methods identify features for better decision making by removing resource-intensive, correlated, or non-relevant features but overlook how these factors may differ across subgroups. To counter these issues, we evaluate a fair feature selection method that considers equal importance to all demographic groups. We jointly considered a fairness metric and an error metric within the feature selection process to ensure a balance between minimizing both bias and global classification error. We tested our approach on three publicly available healthcare datasets. On all three datasets, we observed improvements in fairness metrics coupled with a minimal degradation 
    
[^57]: D'OH: 仅解码器随机超网络用于隐式神经表示

    D'OH: Decoder-Only random Hypernetworks for Implicit Neural Representations

    [https://arxiv.org/abs/2403.19163](https://arxiv.org/abs/2403.19163)

    本文提出使用仅运行时解码器的超网络，不依赖离线数据训练，以更好地模拟跨层参数冗余。

    

    深度隐式函数被发现是一种有效的工具，可以高效地编码各种自然信号。它们的吸引力在于能够紧凑地表示信号，几乎不需要离线训练数据。相反，它们利用深度网络的隐式偏差来解耦信号中的隐藏冗余。在本文中，我们探讨了这样一个假设：通过利用层之间存在的冗余可以实现更好的压缩。我们提出使用一种新颖的仅运行时解码器的超网络 - 它不使用离线训练数据 - 来更好地建模跨层参数冗余。先前在深度隐式函数中应用超网络的应用都采用了依赖大量离线数据集的前馈编码器/解码器框架，这些数据集无法泛化到训练信号之外。相反，我们提出一种用于初始化运行时深度隐式函数的策略

    arXiv:2403.19163v1 Announce Type: new  Abstract: Deep implicit functions have been found to be an effective tool for efficiently encoding all manner of natural signals. Their attractiveness stems from their ability to compactly represent signals with little to no off-line training data. Instead, they leverage the implicit bias of deep networks to decouple hidden redundancies within the signal. In this paper, we explore the hypothesis that additional compression can be achieved by leveraging the redundancies that exist between layers. We propose to use a novel run-time decoder-only hypernetwork - that uses no offline training data - to better model this cross-layer parameter redundancy. Previous applications of hyper-networks with deep implicit functions have applied feed-forward encoder/decoder frameworks that rely on large offline datasets that do not generalize beyond the signals they were trained on. We instead present a strategy for the initialization of run-time deep implicit func
    
[^58]: 在直接偏好优化中将长度与质量分离

    Disentangling Length from Quality in Direct Preference Optimization

    [https://arxiv.org/abs/2403.19159](https://arxiv.org/abs/2403.19159)

    针对直接偏好优化中的长度问题展开研究，揭示了DPO中显著的利用情况，并将其与分布外引导联系起来。

    

    Reinforcement Learning from Human Feedback (RLHF)是最近大型语言模型成功的关键组成部分。然而，RLHF被认为利用了人类偏好中的偏见，比如冗长性。精心格式化和雄辩的答案通常会被用户更高评价，即使它们在帮助性和客观性上较低。一些方法已经被开发来控制这些偏见，在古典RLHF文献中这个问题已有所探讨，但对于直接对齐算法如直接偏好优化（DPO）这个问题相对较少探索。与古典RLHF不同，DPO不训练单独的奖励模型或直接使用强化学习，因此之前用来控制冗长性的方法无法直接应用于这种情况。我们的工作做出了几点贡献。首次在DPO环境中研究长度问题，显示DPO中存在显著的利用，并将其与分布外引导相关联。

    arXiv:2403.19159v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has been a crucial component in the recent success of Large Language Models. However, RLHF is know to exploit biases in human preferences, such as verbosity. A well-formatted and eloquent answer is often more highly rated by users, even when it is less helpful and objective. A number of approaches have been developed to control those biases in the classical RLHF literature, but the problem remains relatively under-explored for Direct Alignment Algorithms such as Direct Preference Optimization (DPO). Unlike classical RLHF, DPO does not train a separate reward model or use reinforcement learning directly, so previous approaches developed to control verbosity cannot be directly applied to this setting. Our work makes several contributions. For the first time, we study the length problem in the DPO setting, showing significant exploitation in DPO and linking it to out-of-distribution bootstra
    
[^59]: 探索混合对抗训练中的双重批量归一化模型

    Towards Understanding Dual BN In Hybrid Adversarial Training

    [https://arxiv.org/abs/2403.19150](https://arxiv.org/abs/2403.19150)

    在混合对抗训练中，分离仿射参数比分离统计数据在模型训练中发挥更重要的作用。

    

    有关在对抗训练（AT）中应用批量归一化（BN）的关注日益增长，尤其是当模型同时在对抗样本和干净样本上进行训练（称为混合-AT）时。一个先前研究常见的做法是假设对抗样本和干净样本来自两个不同的领域，采用双重BN，分别用于对抗分支和干净分支。激励双重BN的一种流行观念是，估计这种混合分布的规范化统计数据是具有挑战性的，因此为规范化而将其分开可以实现更强的鲁棒性。与这一观念相反，我们发现，在模型训练中，分离统计数据的作用比分离仿射参数的作用较小。这一发现与先前的研究（Rebuffi等人，2023）一致，我们在其研究的基础上进行进一步的探讨。

    arXiv:2403.19150v1 Announce Type: cross  Abstract: There is a growing concern about applying batch normalization (BN) in adversarial training (AT), especially when the model is trained on both adversarial samples and clean samples (termed Hybrid-AT). With the assumption that adversarial and clean samples are from two different domains, a common practice in prior works is to adopt Dual BN, where BN and BN are used for adversarial and clean branches, respectively. A popular belief for motivating Dual BN is that estimating normalization statistics of this mixture distribution is challenging and thus disentangling it for normalization achieves stronger robustness. In contrast to this belief, we reveal that disentangling statistics plays a less role than disentangling affine parameters in model training. This finding aligns with prior work (Rebuffi et al., 2023), and we build upon their research for further investigations. We demonstrate that the domain gap between adversarial and clean sam
    
[^60]: 基于拓扑循环图注意力网络的大脑功能连接研究

    Topological Cycle Graph Attention Network for Brain Functional Connectivity

    [https://arxiv.org/abs/2403.19149](https://arxiv.org/abs/2403.19149)

    提出了一种新型的拓扑循环图注意力网络（CycGAT），通过循环关联矩阵建立独立循环基础，结合循环图卷积和注意力机制，增强了对大脑功能连接中关键路径的定位能力。

    

    在这项研究中，我们介绍了一种新颖的拓扑循环图注意力网络（CycGAT），旨在描绘大脑功能图中的功能骨干——信号传输的关键路径——与无关紧要的冗余连接（围绕该核心结构形成循环）之间的区别。我们首先引入了一个循环关联矩阵，建立了图中的独立循环基础，将其与边的关系进行映射。我们提出了一个循环图卷积，利用从循环关联矩阵导出的循环邻接矩阵，以特定地过滤循环域中的边信号。此外，我们通过引入边位置编码增强循环中的拓扑意识，进一步提高了循环图卷积的表示能力。我们通过模拟证明了CycGAT的定位能力，并在ABCD数据集上展示了其有效性。

    arXiv:2403.19149v1 Announce Type: new  Abstract: This study, we introduce a novel Topological Cycle Graph Attention Network (CycGAT), designed to delineate a functional backbone within brain functional graph--key pathways essential for signal transmissio--from non-essential, redundant connections that form cycles around this core structure. We first introduce a cycle incidence matrix that establishes an independent cycle basis within a graph, mapping its relationship with edges. We propose a cycle graph convolution that leverages a cycle adjacency matrix, derived from the cycle incidence matrix, to specifically filter edge signals in a domain of cycles. Additionally, we strengthen the representation power of the cycle graph convolution by adding an attention mechanism, which is further augmented by the introduction of edge positional encodings in cycles, to enhance the topological awareness of CycGAT. We demonstrate CycGAT's localization through simulation and its efficacy on an ABCD s
    
[^61]: 用于无线资源管理的微型图神经网络

    Tiny Graph Neural Networks for Radio Resource Management

    [https://arxiv.org/abs/2403.19143](https://arxiv.org/abs/2403.19143)

    LR-MPGNN模型采用低秩近似技术，显著减少了模型大小和参数数量，取得了巨大的性能提升。

    

    高效的无线资源管理需求激增，迫使我们开发复杂而紧凑的神经网络架构。本文通过介绍一种针对无线资源管理定制的图神经网络（GNN）——低秩消息传递图神经网络（LR-MPGNN）来提出一种新方法。LR-MPGNN的核心是采用低秩近似技术，将传统的线性层替换为其低秩对应层。这种创新设计显著减少了模型大小和参数数量。我们通过几个关键指标对LR-MPGNN模型的性能进行评估：模型大小、参数数量、通信系统的加权总速率以及权重矩阵的特征值分布。我们的广泛评估表明，LR-MPGNN模型的模型大小减少了60倍

    arXiv:2403.19143v1 Announce Type: new  Abstract: The surge in demand for efficient radio resource management has necessitated the development of sophisticated yet compact neural network architectures. In this paper, we introduce a novel approach to Graph Neural Networks (GNNs) tailored for radio resource management by presenting a new architecture: the Low Rank Message Passing Graph Neural Network (LR-MPGNN). The cornerstone of LR-MPGNN is the implementation of a low-rank approximation technique that substitutes the conventional linear layers with their low-rank counterparts. This innovative design significantly reduces the model size and the number of parameters. We evaluate the performance of the proposed LR-MPGNN model based on several key metrics: model size, number of parameters, weighted sum rate of the communication system, and the distribution of eigenvalues of weight matrices. Our extensive evaluations demonstrate that the LR-MPGNN model achieves a sixtyfold decrease in model 
    
[^62]: 顶级排行榜 = 顶级编程能力，永远吗？EvoEval: 通过LLM演化编码基准

    Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM

    [https://arxiv.org/abs/2403.19114](https://arxiv.org/abs/2403.19114)

    EvoEval通过将现有基准演化为不同的目标领域，创建了一个新的程序合成基准套件，以充分评估LLM编码能力。

    

    LLM已成为生成代码任务的首选，LLM的训练、开发和使用随着专门用于生成代码的LLM的指数增长而增加。为了评估LLM在编码上的能力，学术界和行业从业者依赖于流行的人工制定的基准。然而，之前的基准只包含了数量和种类非常有限的问题。此外，由于流行度和年龄，许多基准容易发生数据泄漏，其中示例解决方案可以很容易地在网络上找到，因此可能出现在训练数据中。这些限制不可避免地导致我们要探讨：现有基准的排行榜表现是否可靠且全面足以衡量LLM的程序合成能力？为了解决这个问题，我们引入EvoEval--一个通过将现有基准演化为不同的目标领域而创建的程序合成基准套件，用于全面评估LLM编码。

    arXiv:2403.19114v1 Announce Type: cross  Abstract: LLMs have become the go-to choice for code generation tasks, with an exponential increase in the training, development, and usage of LLMs specifically for code generation. To evaluate the ability of LLMs on code, both academic and industry practitioners rely on popular handcrafted benchmarks. However, prior benchmarks contain only a very limited set of problems, both in quantity and variety. Further, due to popularity and age, many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data. Such limitations inevitably lead us to inquire: Is the leaderboard performance on existing benchmarks reliable and comprehensive enough to measure the program synthesis ability of LLMs? To address this, we introduce EvoEval -- a program synthesis benchmark suite created by evolving existing benchmarks into different targeted domains for a comprehensive evaluation of LLM coding a
    
[^63]: 使用生成对抗网络生成用于普通X射线的合成医学图像

    Synthetic Medical Imaging Generation with Generative Adversarial Networks For Plain Radiographs

    [https://arxiv.org/abs/2403.19107](https://arxiv.org/abs/2403.19107)

    开发了一种开源合成图像生成工具（GIST），利用生成对抗网络生成高质量合成图像数据，改进了数字健康AI算法，有助于提高诊断准确性、患者护理，并减少医疗纠纷索赔。

    

    在医学影像学中，由于患者隐私限制以及在罕见疾病情况下获取足够数据可能会困难，通常会限制对数据的访问。该研究的目的是开发一种可重复使用的开源合成图像生成工具（GAN Image Synthesis Tool，GIST），易于使用且易于部署。该工具通过生成高质量的与特定患者无关的合成图像数据，有助于改进和标准化数字健康领域的人工智能算法。其图像生成能力包括能够生成发病率较低的病变或受伤的影像。改善数字健康人工智能算法可以提高诊断准确性，帮助患者护理，减少医疗纠纷索赔，并最终降低整体医疗保健成本。该工具在现有的生成对抗网络（GANs）算法基础上构建。

    arXiv:2403.19107v1 Announce Type: cross  Abstract: In medical imaging, access to data is commonly limited due to patient privacy restrictions and the issue that it can be difficult to acquire enough data in the case of rare diseases.[1] The purpose of this investigation was to develop a reusable open-source synthetic image generation pipeline, the GAN Image Synthesis Tool (GIST), that is easy to use as well as easy to deploy. The pipeline helps to improve and standardize AI algorithms in the digital health space by generating high quality synthetic image data that is not linked to specific patients. Its image generation capabilities include the ability to generate imaging of pathologies or injuries with low incidence rates. This improvement of digital health AI algorithms could improve diagnostic accuracy, aid in patient care, decrease medicolegal claims, and ultimately decrease the overall cost of healthcare. The pipeline builds on existing Generative Adversarial Networks (GANs) algor
    
[^64]: 用于个性化文本到图像生成的自动化黑盒提示工程

    Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation

    [https://arxiv.org/abs/2403.19103](https://arxiv.org/abs/2403.19103)

    PRISM是一种算法，可以自动识别人类可解释且易传递的提示，从而有效生成所需概念，仅使用黑盒访问T2I模型。

    

    提示工程对于控制文本到图像（T2I）生成模型的输出是有效的，但由于需要手动制作提示而导致工作繁重。这一挑战促使了自动提示生成算法的发展。然而，这些方法通常在T2I模型之间的可传递性方面遇到困难，需要对基础模型进行白盒访问，并产生非直观的提示。在这项工作中，我们介绍了PRISM，这是一种算法，可以仅使用黑盒访问T2I模型就自动识别人类可解释且易传递的提示，从而有效生成所需概念。受大型语言模型（LLM）越狱的启发，PRISM利用LLM的上下文学习能力来迭代地改进给定参考图像的候选提示分布。我们的实验展示了PRISM在为对象、样式等生成准确提示方面的多样性和有效性。

    arXiv:2403.19103v1 Announce Type: cross  Abstract: Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, sty
    
[^65]: 为任意数据维度优化量子卷积神经网络架构

    Optimizing Quantum Convolutional Neural Network Architectures for Arbitrary Data Dimension

    [https://arxiv.org/abs/2403.19099](https://arxiv.org/abs/2403.19099)

    优化量子卷积神经网络架构，使其能处理任意输入数据维度，并同时优化了辅助量子资源分配

    

    Quantum Convolutional Neural Networks (QCNNs)代表了在量子机器学习中的一种有前途的方法，为量子和经典数据分析开辟了新的方向。然而，将QCNNs应用于经典数据时会出现限制。为解决这一问题，我们提出了一种能够处理任意输入数据维度的QCNN架构，同时优化了诸如辅助量子资源分配的功能。

    arXiv:2403.19099v1 Announce Type: cross  Abstract: Quantum convolutional neural networks (QCNNs) represent a promising approach in quantum machine learning, paving new directions for both quantum and classical data analysis. This approach is particularly attractive due to the absence of the barren plateau problem, a fundamental challenge in training quantum neural networks (QNNs), and its feasibility. However, a limitation arises when applying QCNNs to classical data. The network architecture is most natural when the number of input qubits is a power of two, as this number is reduced by a factor of two in each pooling layer. The number of input qubits determines the dimensions (i.e. the number of features) of the input data that can be processed, restricting the applicability of QCNN algorithms to real-world data. To address this issue, we propose a QCNN architecture capable of handling arbitrary input data dimensions while optimizing the allocation of quantum resources such as ancilla
    
[^66]: 使用贝叶斯网络和深度学习改进癌症成像诊断：一种贝叶斯深度学习方法

    Improving Cancer Imaging Diagnosis with Bayesian Networks and Deep Learning: A Bayesian Deep Learning Approach

    [https://arxiv.org/abs/2403.19083](https://arxiv.org/abs/2403.19083)

    本文研究了贝叶斯深度学习模型，结合深度学习和贝叶斯网络的优势，最小化各自的劣势，并分析了其在健康产业中图像分类方面的应用。

    

    随着人工智能应用的最新进展，利用机器学习中的理论和算法可以创建许多精确的模型来训练和预测给定的数据集。本文旨在研究深度学习和贝叶斯网络预测模型背后的理论，根据每个模型的优势和劣势，采用不同的方法构建贝叶斯深度学习模型，结合优势同时最小化劣势。最终，将分析结果贝叶斯深度学习方法在健康产业中分类图像的应用和准确性。

    arXiv:2403.19083v1 Announce Type: cross  Abstract: With recent advancements in the development of artificial intelligence applications using theories and algorithms in machine learning, many accurate models can be created to train and predict on given datasets. With the realization of the importance of imaging interpretation in cancer diagnosis, this article aims to investigate the theory behind Deep Learning and Bayesian Network prediction models. Based on the advantages and drawbacks of each model, different approaches will be used to construct a Bayesian Deep Learning Model, combining the strengths while minimizing the weaknesses. Finally, the applications and accuracy of the resulting Bayesian Deep Learning approach in the health industry in classifying images will be analyzed.
    
[^67]: 利用E-检验统计增强符合性预测

    Enhancing Conformal Prediction Using E-Test Statistics

    [https://arxiv.org/abs/2403.19082](https://arxiv.org/abs/2403.19082)

    该论文利用E-检验统计引入BB-predictor，增强符合性预测效果。

    

    符合性预测（CP）作为一种稳健的框架，能够量化机器学习（ML）模型所做预测的不确定性。与传统的点预测器不同，CP基于数据可交换性的假设生成统计上有效的预测区域，也称为预测区间。通常，构建符合性预测依赖于p值。然而，本文走上了另一条路径，利用E-检验统计的力量通过引入一个下界预测器（BB-predictor）来增强符合性预测的效力。

    arXiv:2403.19082v1 Announce Type: cross  Abstract: Conformal Prediction (CP) serves as a robust framework that quantifies uncertainty in predictions made by Machine Learning (ML) models. Unlike traditional point predictors, CP generates statistically valid prediction regions, also known as prediction intervals, based on the assumption of data exchangeability. Typically, the construction of conformal predictions hinges on p-values. This paper, however, ventures down an alternative path, harnessing the power of e-test statistics to augment the efficacy of conformal predictions by introducing a BB-predictor (bounded from the below predictor).
    
[^68]: 小型机器学习：进展与未来展望

    Tiny Machine Learning: Progress and Futures

    [https://arxiv.org/abs/2403.19076](https://arxiv.org/abs/2403.19076)

    TinyML是一种将深度学习模型压缩到物联网设备和微控制器中实现无处不在智能的新方法，需要共同设计算法和系统堆栈以克服硬件限制。

    

    Tiny Machine Learning（TinyML）是机器学习的一个新领域。通过将深度学习模型压缩到数十亿个物联网设备和微控制器（MCUs）中，我们扩展了人工智能应用的范围，实现了无处不在的智能。然而，由于硬件限制，TinyML具有挑战性：有限的内存资源使得难以容纳为云和移动平台设计的深度学习模型。对于裸机设备，编译器和推断引擎支持也有限。因此，我们需要共同设计算法和系统堆栈以实现TinyML。

    arXiv:2403.19076v1 Announce Type: cross  Abstract: Tiny Machine Learning (TinyML) is a new frontier of machine learning. By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of AI applications and enable ubiquitous intelligence. However, TinyML is challenging due to hardware constraints: the tiny memory resource makes it difficult to hold deep learning models designed for cloud and mobile platforms. There is also limited compiler and inference engine support for bare-metal devices. Therefore, we need to co-design the algorithm and system stack to enable TinyML. In this review, we will first discuss the definition, challenges, and applications of TinyML. We then survey the recent progress in TinyML and deep learning on MCUs. Next, we will introduce MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT devices with system-algorithm co-design. We will further extend the solution from inference to training and in
    
[^69]: 人类中心施工机器人：基于强化学习的助手机器人为木工劳动者提供环境上下文协助

    Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers

    [https://arxiv.org/abs/2403.19060](https://arxiv.org/abs/2403.19060)

    本文提出了一种人类中心的建筑机器人方法，通过强化学习驱动的助手机器人为木工劳动者提供环境上下文协助，推进了机器人在建筑中的应用。

    

    在这个充满活力的建筑行业中，传统的机器人集成主要集中在自动化特定任务，通常忽略了建筑工作流程中人类因素的复杂性和变化性。本文提出了一种以人为本的方法，设计了一个“工作伴侣漫游器”，旨在协助建筑工人完成其现有实践，旨在增强安全性和工作流程的流畅性，同时尊重建筑劳动的技术性质。我们对在木工模板工程中部署机器人系统进行了深入研究，展示了一个原型，通过环境相关的强化学习（RL）驱动模块化框架，重点强调了在动态环境中的机动性、安全性和舒适的工人-机器人协作。我们的研究推进了机器人在建筑中的应用，倡导协作模型，其中自适应机器人支持而不是取代人类，强调了交互式的潜力。

    arXiv:2403.19060v1 Announce Type: cross  Abstract: In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows. This paper introduces a human-centered approach with a ``work companion rover" designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor's skilled nature. We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual Reinforcement Learning (RL)-driven modular framework. Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans, underscoring the potential for an interactive a
    
[^70]: 在卫生保健中的公平性：分析机器学习对糖尿病患者再入院的预测中的不平等

    Equity in Healthcare: Analyzing Disparities in Machine Learning Predictions of Diabetic Patient Readmissions

    [https://arxiv.org/abs/2403.19057](https://arxiv.org/abs/2403.19057)

    该研究使用机器学习模型在预测糖尿病患者再入院方面实现了公平和准确，其中GBM模型表现出色，在跨不同人口统计因素下实现了平衡的预测结果。

    

    本研究旨在调查机器学习（ML）模型如何能够公平、准确地预测糖尿病患者的再入院情况，跨不同人口统计学因素（年龄、性别、种族）。我们比较了 Deep Learning、广义线性模型、梯度上升机（GBM）和朴素贝叶斯等模型。GBM表现出色，具有 84.3% 的 F1 分数和 82.2% 的准确度，准确地预测了跨不同人口统计学因素的再入院情况。对所有模型进行了公平性分析。GBM最小化了预测中的差异，在性别和种族之间获得了平衡的结果。对于两性来说，它显示出较低的假阴性率（FDR）（6-7%）和假阳性率（FPR）（5%）。此外，对于种族群体，如非裔美国人（8%）和亚裔（7%），FDR保持较低水平。类似地，对于年龄群体（40 岁以下和40 岁以上患者），FPR在 4% 的水平上保持一致，表明其精确性和减少偏见的能力。这些发现强调了

    arXiv:2403.19057v1 Announce Type: new  Abstract: This study investigates how machine learning (ML) models can predict hospital readmissions for diabetic patients fairly and accurately across different demographics (age, gender, race). We compared models like Deep Learning, Generalized Linear Models, Gradient Boosting Machines (GBM), and Naive Bayes. GBM stood out with an F1-score of 84.3% and accuracy of 82.2%, accurately predicting readmissions across demographics. A fairness analysis was conducted across all the models. GBM minimized disparities in predictions, achieving balanced results across genders and races. It showed low False Discovery Rates (FDR) (6-7%) and False Positive Rates (FPR) (5%) for both genders. Additionally, FDRs remained low for racial groups, such as African Americans (8%) and Asians (7%). Similarly, FPRs were consistent across age groups (4%) for both patients under 40 and those above 40, indicating its precision and ability to reduce bias. These findings empha
    
[^71]: 通过过拟合的遮蔽自编码器检测生成性模仿

    Detecting Generative Parroting through Overfitting Masked Autoencoders

    [https://arxiv.org/abs/2403.19050](https://arxiv.org/abs/2403.19050)

    本研究提出利用过拟合的遮蔽自编码器(MAE)来检测生成模型中的生成性模仿，建立了基于训练数据集损失的检测阈值，为了确保生成模型的合法使用和提升其法律合规性提供了一种新方法。

    

    生成式人工智能模型的出现彻底改变了数字内容创建的方式，然而由于生成性模仿问题，模型过于模仿其训练数据而给版权完整性带来挑战。本研究提出了一种新方法来解决这个问题，即利用一个过拟合的遮蔽自编码器(MAE)来有效地检测这种模仿样本。我们基于训练数据集上的平均损失建立一个检测阈值，从而精确定位修改后数据集中的模仿内容。初步评估表明了有希望的结果，显示了我们方法确保生成模型的合法使用并加强法律合规性方面的潜力。

    arXiv:2403.19050v1 Announce Type: cross  Abstract: The advent of generative AI models has revolutionized digital content creation, yet it introduces challenges in maintaining copyright integrity due to generative parroting, where models mimic their training data too closely. Our research presents a novel approach to tackle this issue by employing an overfitted Masked Autoencoder (MAE) to detect such parroted samples effectively. We establish a detection threshold based on the mean loss across the training dataset, allowing for the precise identification of parroted content in modified datasets. Preliminary evaluations demonstrate promising results, suggesting our method's potential to ensure ethical use and enhance the legal compliance of generative models.
    
[^72]: 使用方向感知t-SNE可视化高维时间数据

    Visualizing High-Dimensional Temporal Data Using Direction-Aware t-SNE

    [https://arxiv.org/abs/2403.19040](https://arxiv.org/abs/2403.19040)

    提出了两个互补的方向感知损失项，强调数据的时间方面，引导优化和结果嵌入以展示可能被忽略的时间模式。

    

    许多现实世界数据集包含时间组件或涉及从一个状态到另一个状态的转变。为了进行探索性数据分析，我们可以将这些高维数据集表示为二维地图，使用数据对象的嵌入进行探索，并用有向边表示它们的时间关系。大多数现有的降维技术，如t-SNE和UMAP，在构建嵌入时未考虑数据的时间性或关系性，导致时间上杂乱的可视化，使潜在有趣的模式变得模糊。为了解决这个问题，我们在t-SNE的优化函数中提出了两个互补的方向感知损失项，强调数据的时间方面，引导优化和结果嵌入以展示可能被忽略的时间模式。定向一致损失（DCL）鼓励附近的箭头

    arXiv:2403.19040v1 Announce Type: new  Abstract: Many real-world data sets contain a temporal component or involve transitions from state to state. For exploratory data analysis, we can represent these high-dimensional data sets in two-dimensional maps, using embeddings of the data objects under exploration and representing their temporal relationships with directed edges. Most existing dimensionality reduction techniques, such as t-SNE and UMAP, do not take into account the temporal or relational nature of the data when constructing the embeddings, resulting in temporally cluttered visualizations that obscure potentially interesting patterns. To address this problem, we propose two complementary, direction-aware loss terms in the optimization function of t-SNE that emphasize the temporal aspects of the data, guiding the optimization and the resulting embedding to reveal temporal patterns that might otherwise go unnoticed. The Directional Coherence Loss (DCL) encourages nearby arrows c
    
[^73]: 使用公共社交媒体数据评估大型语言模型在健康相关文本分类任务中的表现

    Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data

    [https://arxiv.org/abs/2403.19031](https://arxiv.org/abs/2403.19031)

    综合实验表明，利用LLMs进行数据增强可以...

    

    大型语言模型（LLMs）在NLP任务中展现出卓越的成功。然而，几乎没有研究试图评估它们在基于社交媒体的健康相关自然语言处理任务中的表现，这些任务传统上很难获得高分。我们在6个文本分类任务上对一个基于支持向量机（SVMs）的监督经典机器学习模型，三个基于RoBERTa、BERTweet和SocBERT的监督预训练语言模型（PLMs），以及两个基于GPT3.5和GPT4的LLM分类器进行了基准测试。我们开发了三种利用LLMs进行文本分类的方法：将LLMs用作零次分类器，将LLMs用作注释器为监督分类器注释训练数据，以及利用LLMs进行少量示例来增加手动注释数据。我们全面的实验表明，利用LLMs进行数据增强可以...

    arXiv:2403.19031v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable success in NLP tasks. However, there is a paucity of studies that attempt to evaluate their performances on social media-based health-related natural language processing tasks, which have traditionally been difficult to achieve high scores in. We benchmarked one supervised classic machine learning model based on Support Vector Machines (SVMs), three supervised pretrained language models (PLMs) based on RoBERTa, BERTweet, and SocBERT, and two LLM based classifiers (GPT3.5 and GPT4), across 6 text classification tasks. We developed three approaches for leveraging LLMs for text classification: employing LLMs as zero-shot classifiers, us-ing LLMs as annotators to annotate training data for supervised classifiers, and utilizing LLMs with few-shot examples for augmentation of manually annotated data. Our comprehensive experiments demonstrate that employ-ing data augmentation using LLM
    
[^74]: 利用动态对称性进行基于模型的非对称奖励强化学习

    Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning with Asymmetric Rewards

    [https://arxiv.org/abs/2403.19024](https://arxiv.org/abs/2403.19024)

    本文扩展了强化学习和控制理论中对称技术的应用范围，通过利用动态对称性学习动力学模型，而不要求奖励具有相同的对称性。

    

    强化学习中最近的工作利用模型中的对称性来提高策略训练的采样效率。一个常用的简化假设是动力学和奖励都表现出相同的对称性。然而，在许多真实环境中，动力学模型表现出与奖励模型独立的对称性：奖励可能不满足与动力学相同的对称性。本文探讨了只假定动力学表现出对称性的情况，扩展了强化学习和控制理论学习中可应用对称技术的问题范围。我们利用卡塔恩移动框架方法引入一种学习动力学的技术，通过构造，这种动力学表现出指定的对称性。我们通过数值实验展示了所提出的方法学到了更准确的动态模型。

    arXiv:2403.19024v1 Announce Type: cross  Abstract: Recent work in reinforcement learning has leveraged symmetries in the model to improve sample efficiency in training a policy. A commonly used simplifying assumption is that the dynamics and reward both exhibit the same symmetry. However, in many real-world environments, the dynamical model exhibits symmetry independent of the reward model: the reward may not satisfy the same symmetries as the dynamics. In this paper, we investigate scenarios where only the dynamics are assumed to exhibit symmetry, extending the scope of problems in reinforcement learning and learning in control theory where symmetry techniques can be applied. We use Cartan's moving frame method to introduce a technique for learning dynamics which, by construction, exhibit specified symmetries. We demonstrate through numerical experiments that the proposed method learns a more accurate dynamical model.
    
[^75]: 朝向LLM-RecSys对齐与文本ID学习的方向

    Towards LLM-RecSys Alignment with Textual ID Learning

    [https://arxiv.org/abs/2403.19021](https://arxiv.org/abs/2403.19021)

    通过提出IDGen，将每个推荐项目表示为独特、简洁、语义丰富的文本ID，从而使得基于大型语言模型的推荐更好地与自然语言生成对齐。

    

    基于大型语言模型(LLMs)的生成式推荐已经将传统的基于排名的推荐方式转变为文本生成范例。然而，与固有操作人类词汇的标准NLP任务相反，目前生成式推荐领域的研究在如何在文本生成范式中以简洁而有意义的ID表示有效编码推荐项目方面存在困难。为了更好地对齐LLMs与推荐需求，我们提出了IDGen，使用人类语言标记将每个项目表示为独特、简洁、语义丰富、与平台无关的文本ID。这通过在基于LLM的推荐系统旁训练文本ID生成器来实现，使个性化推荐能够无缝集成到自然语言生成中。值得注意的是，由于用户历史记录以自然语言表达并与原始数据集解耦，我们的方法提出了潜在的

    arXiv:2403.19021v1 Announce Type: cross  Abstract: Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potenti
    
[^76]: Thelxino\"e:使用瞳孔测量和机器学习识别人类情绪

    Thelxino\"e: Recognizing Human Emotions Using Pupillometry and Machine Learning

    [https://arxiv.org/abs/2403.19014](https://arxiv.org/abs/2403.19014)

    该研究提出了一种使用瞳孔测量来识别 VR 中情绪的方法，通过特征工程和梯度提升模型，在 Thelxino\"e 框架中取得了98.8%的高准确率，为发展更具沉浸感和互动性的 VR 环境开辟了新的途径。

    

    在这项研究中，我们提出了一种使用瞳孔测量来识别虚拟现实（VR）中情绪的方法。我们通过VR头显分析对视觉和听觉刺激的瞳孔直径响应，并专注于从VR生成的数据中提取时域、频域和时频域的关键特征。我们的方法利用特征选择，通过最大相关性最小冗余性（mRMR）识别最具影响力的特征。通过应用梯度提升模型，一种使用叠加决策树的集成学习技术，我们在进行特征工程时实现了98.8%的准确率，而未进行特征工程时为84.9%。这项研究对Thelxino\"e框架做出了重大贡献，旨在通过整合多传感器数据实现更加逼真和情感共鸣的触觉交互，为开发更具沉浸感和互动性的VR环境打开了新的途径，为未来铺平了道路。

    arXiv:2403.19014v1 Announce Type: new  Abstract: In this study, we present a method for emotion recognition in Virtual Reality (VR) using pupillometry. We analyze pupil diameter responses to both visual and auditory stimuli via a VR headset and focus on extracting key features in the time-domain, frequency-domain, and time-frequency domain from VR generated data. Our approach utilizes feature selection to identify the most impactful features using Maximum Relevance Minimum Redundancy (mRMR). By applying a Gradient Boosting model, an ensemble learning technique using stacked decision trees, we achieve an accuracy of 98.8% with feature engineering, compared to 84.9% without it. This research contributes significantly to the Thelxino\"e framework, aiming to enhance VR experiences by integrating multiple sensor data for realistic and emotionally resonant touch interactions. Our findings open new avenues for developing more immersive and interactive VR environments, paving the way for futur
    
[^77]: 使用概率模型顺序推断医院电子健康记录

    Sequential Inference of Hospitalization ElectronicHealth Records Using Probabilistic Models

    [https://arxiv.org/abs/2403.19011](https://arxiv.org/abs/2403.19011)

    这项工作设计了一个概率无监督模型，用于推断医院电子健康记录中的多个任意长度序列，可以捕捉复杂的关系，并且可以在原始数据上训练。

    

    在动态的医院环境中，决策支持可以成为改善患者结果的有价值工具。在这种动态环境中，基于数据的推断未来结果具有挑战性，因为长序列（如实验室检测和药物）经常更新。我们设计了一个适用于医院电子健康记录（EHR）数据中的多个任意长度序列的概率无监督模型。该模型使用潜在变量结构，捕捉了药物、诊断、实验室检测、神经评估和药物之间的复杂关系。它可以在原始数据上训练，而无需任何损失转换或时间分箱。

    arXiv:2403.19011v1 Announce Type: cross  Abstract: In the dynamic hospital setting, decision support can be a valuable tool for improving patient outcomes. Data-driven inference of future outcomes is challenging in this dynamic setting, where long sequences such as laboratory tests and medications are updated frequently. This is due in part to heterogeneity of data types and mixed-sequence types contained in variable length sequences. In this work we design a probabilistic unsupervised model for multiple arbitrary-length sequences contained in hospitalization Electronic Health Record (EHR) data. The model uses a latent variable structure and captures complex relationships between medications, diagnoses, laboratory tests, neurological assessments, and medications. It can be trained on original data, without requiring any lossy transformations or time binning. Inference algorithms are derived that use partial data to infer properties of the complete sequences, including their length and 
    
[^78]: 迈向可持续的SecureML: 量化对抗机器学习的碳足迹

    Towards Sustainable SecureML: Quantifying Carbon Footprint of Adversarial Machine Learning

    [https://arxiv.org/abs/2403.19009](https://arxiv.org/abs/2403.19009)

    本文首次探究了对抗机器学习的碳足迹，并引入了Robustness Carbon Trade-off Index（RCTI），该指标捕捉了碳排放对抗性稳健性变化的敏感性。

    

    机器学习(ML)在各行各业的广泛应用引起了可持续性担忧，因为其巨大的能源消耗和碳排放。在对抗性ML中，这一问题变得更加紧迫，因为它着重于增强模型对抗不同基于网络的攻击的安全性。在ML系统中实施防御通常需要额外的计算资源和网络安全措施，加剧了它们的环境影响。本文探讨了对抗性ML的碳足迹，提供实证证据将更大的模型稳健性与更高的排放联系起来。为了解决量化这种权衡的迫切需求，我们引入了Robustness Carbon Trade-off Index (RCTI)。这一新颖的度量指标受经济弹性原理启发，捕捉了碳排放对抗性稳健性变化的敏感性。

    arXiv:2403.19009v1 Announce Type: new  Abstract: The widespread adoption of machine learning (ML) across various industries has raised sustainability concerns due to its substantial energy usage and carbon emissions. This issue becomes more pressing in adversarial ML, which focuses on enhancing model security against different network-based attacks. Implementing defenses in ML systems often necessitates additional computational resources and network security measures, exacerbating their environmental impacts. In this paper, we pioneer the first investigation into adversarial ML's carbon footprint, providing empirical evidence connecting greater model robustness to higher emissions. Addressing the critical need to quantify this trade-off, we introduce the Robustness Carbon Trade-off Index (RCTI). This novel metric, inspired by economic elasticity principles, captures the sensitivity of carbon emissions to changes in adversarial robustness. We demonstrate the RCTI through an experiment i
    
[^79]: 微服务系统的少样本跨系统异常跟踪分类

    Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems

    [https://arxiv.org/abs/2403.18998](https://arxiv.org/abs/2403.18998)

    提出了针对微服务系统的少样本异常跟踪分类的新框架，利用多头注意力自编码器构建系统特定的跟踪表示，并应用基于Transformer编码器的模型无关元学习进行高效分类。

    

    微服务系统（MSS）由于其复杂和动态的特性可能在各种故障类别中出现故障。为了有效处理故障，AIOps工具利用基于跟踪的异常检测和根本原因分析。本文提出了一个新颖的框架，用于微服务系统的少样本异常跟踪分类。我们的框架包括两个主要组成部分：（1）多头注意力自编码器用于构建系统特定的跟踪表示，从而实现（2）基于Transformer编码器的模型无关元学习，以进行有效和高效的少样本异常跟踪分类。该框架在两个代表性的MSS，Trainticket和OnlineBoutique上进行了评估，使用开放数据集。结果表明，我们的框架能够调整学到的知识，以对新的、未见的新颖故障类别的异常跟踪进行分类，无论是在最初训练的同一系统内，还是在其他系统中。

    arXiv:2403.18998v1 Announce Type: cross  Abstract: Microservice-based systems (MSS) may experience failures in various fault categories due to their complex and dynamic nature. To effectively handle failures, AIOps tools utilize trace-based anomaly detection and root cause analysis. In this paper, we propose a novel framework for few-shot abnormal trace classification for MSS. Our framework comprises two main components: (1) Multi-Head Attention Autoencoder for constructing system-specific trace representations, which enables (2) Transformer Encoder-based Model-Agnostic Meta-Learning to perform effective and efficient few-shot learning for abnormal trace classification. The proposed framework is evaluated on two representative MSS, Trainticket and OnlineBoutique, with open datasets. The results show that our framework can adapt the learned knowledge to classify new, unseen abnormal traces of novel fault categories both within the same system it was initially trained on and even in the 
    
[^80]: Causal-StoNet: 高维复杂数据的因果推断

    Causal-StoNet: Causal Inference for High-Dimensional Complex Data

    [https://arxiv.org/abs/2403.18994](https://arxiv.org/abs/2403.18994)

    本文提出了一种基于深度学习技术的新颖因果推断方法，用于处理高维复杂数据。

    

    随着数据科学的发展，收集越来越复杂的数据集已经变得司空见惯。在这些数据集中，数据维度可能非常高，并且潜在的数据生成过程可能是未知的，高度非线性的。因此，在许多领域，如医学、计量经济学和社会科学，对高维复杂数据进行因果推断的任务已经成为一个基本问题。然而，现有的因果推断方法通常是在假设数据维度较低或潜在数据生成过程为线性或近似线性的情况下开发的。为了解决这些挑战，本文提出了一种新颖的因果推断方法，用于处理高维复杂数据。所提出的方法基于深度学习技术，包括最近发展的稀疏深度学习理论和随机神经网络。

    arXiv:2403.18994v1 Announce Type: cross  Abstract: With the advancement of data science, the collection of increasingly complex datasets has become commonplace. In such datasets, the data dimension can be extremely high, and the underlying data generation process can be unknown and highly nonlinear. As a result, the task of making causal inference with high-dimensional complex data has become a fundamental problem in many disciplines, such as medicine, econometrics, and social science. However, the existing methods for causal inference are frequently developed under the assumption that the data dimension is low or that the underlying data generation process is linear or approximately linear. To address these challenges, this paper proposes a novel causal inference approach for dealing with high-dimensional complex data. The proposed approach is based on deep learning techniques, including sparse deep learning theory and stochastic neural networks, that have been developed in recent lit
    
[^81]: 强化学习用于黑盒图像、视频和心电图信号分类的鲁棒性和视觉解释

    Robustness and Visual Explanation for Black Box Image, Video, and ECG Signal Classification with Reinforcement Learning

    [https://arxiv.org/abs/2403.18985](https://arxiv.org/abs/2403.18985)

    提出了一个通用的强化学习框架，用于从心电图信号分析到图像和视频分类的不同模型类型的对抗攻击，通过识别敏感区域并在最小程度扭曲下诱导错误分类，生成优越的定位掩模，并在鲁棒性和透明度方面取得了显著进展。

    

    我们提出了一个通用的强化学习（RL）框架，针对从心电图信号分析（1D）、图像分类（2D）到视频分类（3D）等不同模型类型进行对抗攻击的优化。该框架专注于识别敏感区域并在最小程度扭曲和各种扭曲类型下诱导错误分类。新颖的RL方法在所有三个应用中优于最先进的方法，证明了其效率。我们的RL方法生成了优越的定位掩模，增强了图像分类和心电图分析模型的可解释性。对于心电图分析等应用，我们的平台突出了临床医生关注的关键心电图片段，同时确保对流行扭曲的韧性。这一全面的工具旨在通过对抗性训练和透明度提高各种应用和数据类型的韧性。

    arXiv:2403.18985v1 Announce Type: cross  Abstract: We present a generic Reinforcement Learning (RL) framework optimized for crafting adversarial attacks on different model types spanning from ECG signal analysis (1D), image classification (2D), and video classification (3D). The framework focuses on identifying sensitive regions and inducing misclassifications with minimal distortions and various distortion types. The novel RL method outperforms state-of-the-art methods for all three applications, proving its efficiency. Our RL approach produces superior localization masks, enhancing interpretability for image classification and ECG analysis models. For applications such as ECG analysis, our platform highlights critical ECG segments for clinicians while ensuring resilience against prevalent distortions. This comprehensive tool aims to bolster both resilience with adversarial training and transparency across varied applications and data types.
    
[^82]: TextCraftor：您的文本编码器可以成为图像质量控制器

    TextCraftor: Your Text Encoder Can be Image Quality Controller

    [https://arxiv.org/abs/2403.18978](https://arxiv.org/abs/2403.18978)

    通过微调文本编码器来改进文本到图像扩散模型的性能。

    

    基于扩散的文本到图像生成模型，如稳定扩散，已经彻底改变了内容生成领域，使得在诸如图像编辑和视频合成等领域取得了重大进展。尽管这些模型具有强大的功能，但它们并非没有局限性。合成与输入文本相契合的图像仍然具有挑战性，并且需要多次运行以精心设计的提示才能实现令人满意的结果。为了减轻这些局限性，许多研究努力对预训练的扩散模型，即UNet，进行微调，利用各种技术。然而，在这些努力中，一个关键问题，即对文本到图像扩散模型训练进行微调以改善文本到图像扩散模型性能是否可能和可行，仍然大多未被探讨。我们的研究结果显示，与其替换CLIP文本编码器，更好的方法是微调文本编码器以提升文本到图像扩散模型的性能。

    arXiv:2403.18978v1 Announce Type: cross  Abstract: Diffusion-based text-to-image generative models, e.g., Stable Diffusion, have revolutionized the field of content generation, enabling significant advancements in areas like image editing and video synthesis. Despite their formidable capabilities, these models are not without their limitations. It is still challenging to synthesize an image that aligns well with the input text, and multiple runs with carefully crafted prompts are required to achieve satisfactory results. To mitigate these limitations, numerous studies have endeavored to fine-tune the pre-trained diffusion models, i.e., UNet, utilizing various technologies. Yet, amidst these efforts, a pivotal question of text-to-image diffusion model training has remained largely unexplored: Is it possible and feasible to fine-tune the text encoder to improve the performance of text-to-image diffusion models? Our findings reveal that, instead of replacing the CLIP text encoder used in 
    
[^83]: 从概念到实现的大型语言模型综述

    A Survey on Large Language Models from Concept to Implementation

    [https://arxiv.org/abs/2403.18969](https://arxiv.org/abs/2403.18969)

    Transformer模型在改革传统任务和推进跨行业研究和开发中产生革命性影响。

    

    最近基于Transformer架构构建的大型语言模型(LLMs)的发展极大拓宽了自然语言处理(NLP)应用的范围，超越了最初在聊天机器人技术中的应用。本文研究了这些模型的多方面应用，着重介绍了GPT系列。这项探索聚焦于人工智能(AI)驱动工具在改革传统编码和问题解决等任务上的革命性影响，同时在跨越不同行业的研究和开发中开辟新路径。从代码解释和图像描述到促进交互式系统的搭建和推进计算领域，Transformer模型体现了深度学习、数据分析和神经网络设计的协同作用。本综述深入探讨了Transformer模型的最新研究，突出了

    arXiv:2403.18969v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs), particularly those built on Transformer architectures, have significantly broadened the scope of natural language processing (NLP) applications, transcending their initial use in chatbot technology. This paper investigates the multifaceted applications of these models, with an emphasis on the GPT series. This exploration focuses on the transformative impact of artificial intelligence (AI) driven tools in revolutionizing traditional tasks like coding and problem-solving, while also paving new paths in research and development across diverse industries. From code interpretation and image captioning to facilitating the construction of interactive systems and advancing computational domains, Transformer models exemplify a synergy of deep learning, data analysis, and neural network design. This survey provides an in-depth look at the latest research in Transformer models, highlighting the
    
[^84]: LORD：基于大模型的相反奖励设计用于自动驾驶

    LORD: Large Models based Opposite Reward Design for Autonomous Driving

    [https://arxiv.org/abs/2403.18965](https://arxiv.org/abs/2403.18965)

    LORD通过不期望的语言目标，提出了一种基于大模型的相反奖励设计，以便有效利用大型预训练模型作为零-shot奖励模型。

    

    强化学习（RL）驱动的自动驾驶已经成为一种有希望的替代数据驱动模仿学习方法的选择。然而，为RL制定有效的奖励函数面临挑战，因为要在不同场景中定义和量化良好的驾驶行为的复杂性。最近，大型预训练模型作为零-shot奖励模型，为指定具有期望语言目标的任务引起了重要关注。然而，对于自动驾驶的期望语言目标，如“安全驾驶”，对于预训练模型来说是模糊且难以理解的。另一方面，不期望的语言目标，比如“碰撞”，更加具体且可跟踪。在这项工作中，我们引入了LORD，这是一种新颖的基于大模型的相反奖励设计，通过不期望的语言目标来实现对大型预训练模型的有效使用，作为零-shot奖励模型。通过大量实验，我们提出的框架显示

    arXiv:2403.18965v1 Announce Type: cross  Abstract: Reinforcement learning (RL) based autonomous driving has emerged as a promising alternative to data-driven imitation learning approaches. However, crafting effective reward functions for RL poses challenges due to the complexity of defining and quantifying good driving behaviors across diverse scenarios. Recently, large pretrained models have gained significant attention as zero-shot reward models for tasks specified with desired linguistic goals. However, the desired linguistic goals for autonomous driving such as "drive safely" are ambiguous and incomprehensible by pretrained models. On the other hand, undesired linguistic goals like "collision" are more concrete and tractable. In this work, we introduce LORD, a novel large models based opposite reward design through undesired linguistic goals to enable the efficient use of large pretrained models as zero-shot reward models. Through extensive experiments, our proposed framework shows
    
[^85]: 利用大规模视觉语言模型调节不安全用户生成内容游戏中的违法在线图片推广

    Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models

    [https://arxiv.org/abs/2403.18957](https://arxiv.org/abs/2403.18957)

    该研究旨在调查不安全用户生成内容游戏中的违法推广威胁，收集了一组包含性暴力和暴力内容的真实图像数据集。

    

    在线用户生成内容游戏（UGCGs）在儿童和青少年中越来越受欢迎，用于社交互动和更有创意的在线娱乐。然而，它们存在着更高的暴露不良内容的风险，引发了人们对儿童和青少年在线安全的日益关注。我们采取了第一步研究对不安全UGCGs的违法推广进行威胁性分析。我们收集了一组现实世界数据集，包括2,924张展示不同性暴力和暴力内容的图像，这些内容被游戏创建者用于推广UGCGs。

    arXiv:2403.18957v1 Announce Type: cross  Abstract: Online user-generated content games (UGCGs) are increasingly popular among children and adolescents for social interaction and more creative online entertainment. However, they pose a heightened risk of exposure to explicit content, raising growing concerns for the online safety of children and adolescents. Despite these concerns, few studies have addressed the issue of illicit image-based promotions of unsafe UGCGs on social media, which can inadvertently attract young users. This challenge arises from the difficulty of obtaining comprehensive training data for UGCG images and the unique nature of these images, which differ from traditional unsafe content. In this work, we take the first step towards studying the threat of illicit promotions of unsafe UGCGs. We collect a real-world dataset comprising 2,924 images that display diverse sexually explicit and violent content used to promote UGCGs by their game creators. Our in-depth studi
    
[^86]: 任何结构裁剪：任何架构，任何框架，任何时候

    Structurally Prune Anything: Any Architecture, Any Framework, Any Time

    [https://arxiv.org/abs/2403.18955](https://arxiv.org/abs/2403.18955)

    SPA是一个多功能的结构剪枝框架，可以对任何架构、任何框架的神经网络进行剪枝，而且可以在任何训练阶段进行剪枝。

    

    arXiv:2403.18955v1 公告类型：新  摘要：神经网络剪枝是增强深度学习模型效率的关键技术。与无结构剪枝不同，后者仅将特定参数设置为零，结构剪枝消除了整个通道，从而产生直接的计算和存储优势。然而，不同的耦合参数模式，如残差连接和组卷积，不同的深度学习框架，以及可以执行剪枝的各种时间阶段使得现有的剪枝方法对不同架构、框架和剪枝准则 less adaptable。为了解决这个问题，我们引入了Structurally Prune Anything（SPA），这是一个多功能的结构剪枝框架，可以对任何架构，任何框架的神经网络进行剪枝，而且可以在训练的任何阶段进行剪枝。SPA利用标准化的计算图和ONNX表示来对不同的神经网络架构进行剪枝，而无需

    arXiv:2403.18955v1 Announce Type: new  Abstract: Neural network pruning serves as a critical technique for enhancing the efficiency of deep learning models. Unlike unstructured pruning, which only sets specific parameters to zero, structured pruning eliminates entire channels, thus yielding direct computational and storage benefits. However, the diverse patterns for coupling parameters, such as residual connections and group convolutions, the diverse deep learning frameworks, and the various time stages at which pruning can be performed make existing pruning methods less adaptable to different architectures, frameworks, and pruning criteria. To address this, we introduce Structurally Prune Anything (SPA), a versatile structured pruning framework that can prune neural networks with any architecture, from any framework, and at any stage of training. SPA leverages a standardized computational graph and ONNX representation to prune diverse neural network architectures without the need for 
    
[^87]: 将传统和下一代水库计算混合以精确高效地预测动力系统

    Hybridizing Traditional and Next-Generation Reservoir Computing to Accurately and Efficiently Forecast Dynamical Systems

    [https://arxiv.org/abs/2403.18953](https://arxiv.org/abs/2403.18953)

    介绍了一种将传统水库计算和下一代水库计算相结合的方法，用于精确预测复杂和混沌动力系统的时间序列，当单独使用RC和NGRC组件不足以满足时，混合方法具有显著优势。

    

    水库计算（RCs）是强大的用于时间序列预测的机器学习架构。最近，引入了下一代水库计算（NGRCs），相比RCs，它们具有显著的优势，如降低的计算开销和较低的数据要求。然而，NGRCs与RCs存在着自身独特的实际困难，包括对数据中的采样时间和非线性类型的敏感性。在这里，我们介绍了一种用于复杂和混沌动力系统时间序列预测的混合RC-NGRC方法。我们展示了我们的混合方法可以产生准确的短期预测，并在RC和NGRC各自独立时能够捕捉动力系统的长期统计情况。当两个组件的预测能力都受限时，例如对于小的RC和大的采样时间时，混合RC-NGRC方法的优势最为显著。

    arXiv:2403.18953v1 Announce Type: new  Abstract: Reservoir computers (RCs) are powerful machine learning architectures for time series prediction. Recently, next generation reservoir computers (NGRCs) have been introduced, offering distinct advantages over RCs, such as reduced computational expense and lower data requirements. However, NGRCs have their own practical difficulties distinct from those of RCs, including sensitivity to sampling time and type of nonlinearities in the data. Here, we introduce a hybrid RC-NGRC approach for time series forecasting of complex and chaotic dynamical systems. We show that our hybrid approach can produce accurate short term predictions and capture the long term statistics of dynamical systems in situations where the RC and NGRC components alone are insufficient. The advantage of the hybrid RC-NGRC approach is most pronounced when both components are limited in their prediction capabilities, e.g. for a small RC and a large sampling time in the traini
    
[^88]: 自监督可解释的感知动作学习通过潜在功能模块性

    Self-Supervised Interpretable Sensorimotor Learning via Latent Functional Modularity

    [https://arxiv.org/abs/2403.18947](https://arxiv.org/abs/2403.18947)

    MoNet是一种结合端到端学习和模块化网络架构的方法，通过认知引导的对比损失函数，在潜在空间中高效学习任务特定的决策过程，而无需任务级别的监督，同时提高端到端推断的可解释性，实现了在实际环境中的有效视觉自主导航。

    

    我们介绍了MoNet，这是一种将端到端学习与模块化网络架构相结合的新方法，用于自监督和可解释的感知动作学习。MoNet由三个功能上不同的神经模块组成：感知、规划和控制。通过认知引导的对比损失函数，MoNet有效地在潜在空间中学习特定任务的决策过程，而不需要任务级别的监督。此外，我们的方法还融入了一种在线事后解释方法，增强了端到端推断的可解释性，而不影响感知动作性能。在现实世界的室内环境中，MoNet展示了有效的视觉自主导航，在任务特异性分析中超越了基线模型11%至47%。我们进一步通过感知显著性地图的事后分析探讨了我们网络的可解释性。

    arXiv:2403.18947v1 Announce Type: new  Abstract: We introduce MoNet, a novel method that combines end-to-end learning with modular network architectures for self-supervised and interpretable sensorimotor learning. MoNet is composed of three functionally distinct neural modules: Perception, Planning, and Control. Leveraging its inherent modularity through a cognition-guided contrastive loss function, MoNet efficiently learns task-specific decision-making processes in latent space, without requiring task-level supervision. Moreover, our method incorporates an online post-hoc explainability approach, which enhances the interpretability of the end-to-end inferences without a trade-off in sensorimotor performance. In real-world indoor environments, MoNet demonstrates effective visual autonomous navigation, surpassing baseline models by 11% to 47% in task specificity analysis. We further delve into the interpretability of our network through the post-hoc analysis of perceptual saliency maps 
    
[^89]: 面向大规模网络的随机聚合波束成形用于空中联邦学习

    Random Aggregate Beamforming for Over-the-Air Federated Learning in Large-Scale Networks

    [https://arxiv.org/abs/2403.18946](https://arxiv.org/abs/2403.18946)

    本文提出了一种随机聚合波束成形的方案，通过随机抽样生成聚合器波束形成矢量，以解决大规模网络中的聚合误差最小化和设备选择最大化问题。

    

    目前，部署普遍人工智能（AI）应用于网络边缘的趋势日益明显。作为一种有望实现安全边缘智能的框架，联邦学习（FL）受到了广泛关注，空中计算（AirComp）已经被集成以进一步提高通信效率。本文考虑了联合设备选择和聚合波束成形设计，其目标是最小化聚合误差并最大化选定设备的数量。这导致了一个组合问题，在大规模网络中尤其难以解决。为了以一种经济有效的方式解决问题，我们提出了一种基于随机聚合波束成形的方案，该方案通过随机抽样生成聚合波束形成矢量，而非优化。该方案的实施不需要进行信道估计。

    arXiv:2403.18946v1 Announce Type: cross  Abstract: At present, there is a trend to deploy ubiquitous artificial intelligence (AI) applications at the edge of the network. As a promising framework that enables secure edge intelligence, federated learning (FL) has received widespread attention, and over-the-air computing (AirComp) has been integrated to further improve the communication efficiency. In this paper, we consider a joint device selection and aggregate beamforming design with the objectives of minimizing the aggregate error and maximizing the number of selected devices. This yields a combinatorial problem, which is difficult to solve especially in large-scale networks. To tackle the problems in a cost-effective manner, we propose a random aggregate beamforming-based scheme, which generates the aggregator beamforming vector via random sampling rather than optimization. The implementation of the proposed scheme does not require the channel estimation. We additionally use asympto
    
[^90]: 优化无线网络与深度展开：两种深度展开机制的比较研究

    Optimizing Wireless Networks with Deep Unfolding: Comparative Study on Two Deep Unfolding Mechanisms

    [https://arxiv.org/abs/2403.18930](https://arxiv.org/abs/2403.18930)

    本论文进行了关于两种深度展开机制的比较研究，提出了一个半展开深度学习模型和一个完全展开的深度学习模型，以高效进行无线网络中的功率控制。

    

    在这项工作中，我们对两种深度展开机制进行比较研究，以高效地进行下一代无线网络中的功率控制。功率控制问题被建模为多个干扰链接上的能量效率问题。该问题是非凸的。我们采用分数规划转换为该问题设计了两个解决方案。第一个解决方案是数值解，而第二个解决方案是闭式解。基于第一个解决方案，我们设计了一个半展开深度学习模型，其中我们结合了无线通信领域知识和数据驱动深度学习的最新进展。此外，基于闭式解的亮点，我们设计了一个完全展开的深度学习模型，在这个模型中充分利用了表达力强的闭式功率控制解和深度学习的进步。在仿真结果中，我们比较了所提出方法的性能。

    arXiv:2403.18930v1 Announce Type: cross  Abstract: In this work, we conduct a comparative study on two deep unfolding mechanisms to efficiently perform power control in the next generation wireless networks. The power control problem is formulated as energy efficiency over multiple interference links. The problem is nonconvex. We employ fractional programming transformation to design two solutions for the problem. The first solution is a numerical solution while the second solution is a closed-form solution. Based on the first solution, we design a semi-unfolding deep learning model where we combine the domain knowledge of the wireless communications and the recent advances in the data-driven deep learning. Moreover, on the highlights of the closed-form solution, fully deep unfolded deep learning model is designed in which we fully leveraged the expressive closed-form power control solution and deep learning advances. In the simulation results, we compare the performance of the propose
    
[^91]: 神经科学启发的机器学习综述

    A Review of Neuroscience-Inspired Machine Learning

    [https://arxiv.org/abs/2403.18929](https://arxiv.org/abs/2403.18929)

    该论文综述了神经科学启发的机器学习领域，重点讨论了通过生物可信的信用指派方案进行学习的优势，包括兼容性广泛、能效高、适用于各种学习条件和硬件、以及实时自适应神经形态处理系统的发展。

    

    深度学习的一大批评在于用于学习的信用指派方案的生物不可信性 - 误差反向传播。这种不可信性转化为实际限制，涵盖科学领域，包括与硬件不兼容和非可微实现，从而导致昂贵的能源需求。相比之下，具有生物学可行性的信用指派与实际上的任何学习条件兼容，并且节能。因此，它可适应硬件和科学建模，例如学习物理系统和非可微行为。此外，它可以导致实时的自适应神经形态处理系统的发展。为了解决这一问题，出现了人工智能研究的一个跨学科分支，位于神经科学、认知科学和机器学习的交集处。

    arXiv:2403.18929v1 Announce Type: cross  Abstract: One major criticism of deep learning centers around the biological implausibility of the credit assignment schema used for learning -- backpropagation of errors. This implausibility translates into practical limitations, spanning scientific fields, including incompatibility with hardware and non-differentiable implementations, thus leading to expensive energy requirements. In contrast, biologically plausible credit assignment is compatible with practically any learning condition and is energy-efficient. As a result, it accommodates hardware and scientific modeling, e.g. learning with physical systems and non-differentiable behavior. Furthermore, it can lead to the development of real-time, adaptive neuromorphic processing systems. In addressing this problem, an interdisciplinary branch of artificial intelligence research that lies at the intersection of neuroscience, cognitive science, and machine learning has emerged. In this paper, w
    
[^92]: 用更稀疏的选择提高稀疏模型的效率

    Enhancing Efficiency in Sparse Models with Sparser Selection

    [https://arxiv.org/abs/2403.18926](https://arxiv.org/abs/2403.18926)

    提出了一种新颖的MoE模型\tool，通过利用小型专家和基于阈值的路由器，使标记能够选择性地仅涉及到必要的参数，从而在减少MoE层计算负载50%以上的同时提高模型性能。

    

    稀疏模型，包括稀疏的专家混合（MoE）模型，已经成为缩放Transformer模型的有效方法。然而，它们通常存在计算效率低的问题，因为大量参数通过将值乘以零或低激活值无谓参与计算。为了解决这一问题，我们提出了一种名为\tool 的新颖MoE模型，旨在提升稀疏MoE模型的功效和效率。 \tool 利用小型专家和基于阈值的路由器，使标记能够选择性地仅涉及到必要的参数。我们在语言建模和机器翻译任务上进行了大量实验，结果表明\tool 可以在不牺牲性能的情况下，将MoE层的计算负载减少50\%以上，同时提高模型性能。此外，我们展示了\tool 的通用性，通过将其应用于密集模型，在推断期间实现稀疏计算。

    arXiv:2403.18926v1 Announce Type: cross  Abstract: Sparse models, including sparse Mixture-of-Experts (MoE) models, have emerged as an effective approach for scaling Transformer models. However, they often suffer from computational inefficiency since a significant number of parameters are unnecessarily involved in computations via multiplying values by zero or low activation values. To address this issue, we present \tool, a novel MoE designed to enhance both the efficacy and efficiency of sparse MoE models. \tool leverages small experts and a threshold-based router to enable tokens to selectively engage only essential parameters. Our extensive experiments on language modeling and machine translation tasks demonstrate that \tool can enhance model performance while decreasing the computation load at MoE layers by over 50\% without sacrificing performance. Furthermore, we present the versatility of \tool by applying it to dense models, enabling sparse computation during inference. We pro
    
[^93]: 自然引导的认知进化用于预测北温带湖泊中的溶解氧浓度

    Nature-Guided Cognitive Evolution for Predicting Dissolved Oxygen Concentrations in North Temperate Lakes

    [https://arxiv.org/abs/2403.18923](https://arxiv.org/abs/2403.18923)

    提出了一种自然引导的认知进化策略，通过多层融合自适应学习和自然过程，有效预测北温带湖泊中的溶解氧浓度

    

    预测北温带湖泊中的溶解氧（DO）浓度需要对不同生态系统中的物候模式进行全面研究，这凸显了选择物候特征和特征交互的重要性。基于过程的模型受部分过程知识限制或特征表示过于简化，而机器学习模型在有效选择不同湖泊类型和任务的相关特征交互方面面临挑战，尤其是在DO数据收集不频繁的情况下。在本文中，我们提出了一种自然引导的认知进化（NGCE）策略，这代表了自适应学习与自然过程多层融合。具体来说，我们利用代谢过程为基础的模型生成模拟DO标签。利用这些模拟标签，我们实施了一个多种群认知进化搜索，模型反映自然有机体，适应性地

    arXiv:2403.18923v1 Announce Type: cross  Abstract: Predicting dissolved oxygen (DO) concentrations in north temperate lakes requires a comprehensive study of phenological patterns across various ecosystems, which highlights the significance of selecting phenological features and feature interactions. Process-based models are limited by partial process knowledge or oversimplified feature representations, while machine learning models face challenges in efficiently selecting relevant feature interactions for different lake types and tasks, especially under the infrequent nature of DO data collection. In this paper, we propose a Nature-Guided Cognitive Evolution (NGCE) strategy, which represents a multi-level fusion of adaptive learning with natural processes. Specifically, we utilize metabolic process-based models to generate simulated DO labels. Using these simulated labels, we implement a multi-population cognitive evolutionary search, where models, mirroring natural organisms, adaptiv
    
[^94]: SMOF：在FPGA上使用智能离片驱逐流模式CNN

    SMOF: Streaming Modern CNNs on FPGAs with Smart Off-Chip Eviction

    [https://arxiv.org/abs/2403.18921](https://arxiv.org/abs/2403.18921)

    本文介绍了一种在FPGA上使用智能离片驱逐流模式CNN的方法，通过引入权重和激活的驱逐机制来解决现代拓扑结构在芯片内存使用方面的性能限制。

    

    卷积神经网络（CNNs）在众多视觉任务中展现出了其有效性。然而，它们高的处理要求需要有效的硬件加速以满足应用的性能目标。在FPGAs领域，用户通常采用基于流式数据流架构，通过分层流水线和在芯片上保留数据以降低离片内存访问来获得显著的性能增益。然而，现代拓扑结构，如UNet、YOLO和X3D模型，利用长跳跃连接，需要大量的芯片内存空间，从而限制了这种系统架构所达到的性能。本文通过在计算流程中沿着权重和激活驱逐机制至离片内存，考虑到可用的计算和内存资源，来解决上述限制。

    arXiv:2403.18921v1 Announce Type: cross  Abstract: Convolutional Neural Networks (CNNs) have demonstrated their effectiveness in numerous vision tasks. However, their high processing requirements necessitate efficient hardware acceleration to meet the application's performance targets. In the space of FPGAs, streaming-based dataflow architectures are often adopted by users, as significant performance gains can be achieved through layer-wise pipelining and reduced off-chip memory access by retaining data on-chip. However, modern topologies, such as the UNet, YOLO, and X3D models, utilise long skip connections, requiring significant on-chip storage and thus limiting the performance achieved by such system architectures. The paper addresses the above limitation by introducing weight and activation eviction mechanisms to off-chip memory along the computational pipeline, taking into account the available compute and memory resources. The proposed mechanism is incorporated into an existing t
    
[^95]: 使用最优传输进行少样本时序动作定位的提示学习

    PLOT-TAL -- Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization

    [https://arxiv.org/abs/2403.18915](https://arxiv.org/abs/2403.18915)

    提出了使用最优传输进行少样本时序动作定位的提示学习方法，通过多提示学习框架和最优传输理论的结合，有效地捕捉通用特征和减轻过拟合风险

    

    本文介绍了一种新的方法来处理少样本学习中的时序动作定位（TAL）。我们的工作解决了传统单样本学习方法的固有局限性，这些方法往往由于无法在真实世界的视频中跨不同上下文进行泛化而导致过拟合。鉴于视频中摄像机视角、背景和物体的多样性，我们提出了一个增强了最优传输的多提示学习框架。这个设计允许模型为每个动作学习一组多样的提示，更有效地捕捉通用特征并分布表示以减轻过拟合的风险。此外，通过采用最优传输理论，我们可以有效地将这些提示与动作特征进行对齐，优化以获得适应视频数据多面性的综合表示。我们的实验证明了动作定位方面的显著改进。

    arXiv:2403.18915v1 Announce Type: cross  Abstract: This paper introduces a novel approach to temporal action localization (TAL) in few-shot learning. Our work addresses the inherent limitations of conventional single-prompt learning methods that often lead to overfitting due to the inability to generalize across varying contexts in real-world videos. Recognizing the diversity of camera views, backgrounds, and objects in videos, we propose a multi-prompt learning framework enhanced with optimal transport. This design allows the model to learn a set of diverse prompts for each action, capturing general characteristics more effectively and distributing the representation to mitigate the risk of overfitting. Furthermore, by employing optimal transport theory, we efficiently align these prompts with action features, optimizing for a comprehensive representation that adapts to the multifaceted nature of video data. Our experiments demonstrate significant improvements in action localization a
    
[^96]: 对离群数据检测悖论的似然几何解释

    A Geometric Explanation of the Likelihood OOD Detection Paradox

    [https://arxiv.org/abs/2403.18910](https://arxiv.org/abs/2403.18910)

    高似然区域将不会被生成如果它们包含最小概率质量，基于此观察提出了一种通过本地固有维度估计进行离群检测的方法

    

    基于似然的深度生成模型(DGMs)通常表现出令人困惑的行为：当在相对复杂的数据集上训练时，它们会给来自更简单来源的离群数据赋予更高的似然值。更使人感到神秘的是，尽管具有更高的似然值，但这些DGMs从未生成过离群样本。这个双管齐下的悖论尚未得到最终解释，使得基于似然的离群检测不可靠。我们的主要观察是，如果高似然区域中包含了最小概率质量，那么这些区域将不会被生成。我们演示了在围绕低维流形数据的地方可能出现大密度但低概率质量的看似矛盾情况。我们还展示了通过本地固有维度(LID)估计可以识别这种场景，并提出了一种通过预训练的DGM获得的似然和LID估计相配对的离群检测方法。

    arXiv:2403.18910v1 Announce Type: cross  Abstract: Likelihood-based deep generative models (DGMs) commonly exhibit a puzzling behaviour: when trained on a relatively complex dataset, they assign higher likelihood values to out-of-distribution (OOD) data from simpler sources. Adding to the mystery, OOD samples are never generated by these DGMs despite having higher likelihoods. This two-pronged paradox has yet to be conclusively explained, making likelihood-based OOD detection unreliable. Our primary observation is that high-likelihood regions will not be generated if they contain minimal probability mass. We demonstrate how this seeming contradiction of large densities yet low probability mass can occur around data confined to low-dimensional manifolds. We also show that this scenario can be identified through local intrinsic dimension (LID) estimation, and propose a method for OOD detection which pairs the likelihoods and LID estimates obtained from a pre-trained DGM. Our method can b
    
[^97]: 使用混合适配器进行预训练模型的自我扩展以实现持续学习

    Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning

    [https://arxiv.org/abs/2403.18886](https://arxiv.org/abs/2403.18886)

    提出了一种名为SEMA的新型微调方法，旨在通过自我扩展预训练模型与模块化适配，实现持续学习过程中的最小遗忘，解决先前针对静态模型架构情况下存在的过多参数分配或适应性不足等问题。

    

    持续学习旨在从连续到达的数据流中学习，最大限度地减少先前学到的知识的遗忘。本文提出了一种名为SEMA的新型微调方法，称为自我扩展预训练模型与模块化适配，自动决定...（摘要未完整）

    arXiv:2403.18886v1 Announce Type: new  Abstract: Continual learning aims to learn from a stream of continuously arriving data with minimum forgetting of previously learned knowledge. While previous works have explored the effectiveness of leveraging the generalizable knowledge from pre-trained models in continual learning, existing parameter-efficient fine-tuning approaches focus on the use of a predetermined or task-wise set of adapters or prompts. However, these approaches still suffer from forgetting due to task interference on jointly used parameters or restricted flexibility. The reliance on a static model architecture may lead to the allocation of excessive parameters that are not essential or, conversely, inadequate adaptation for downstream tasks, given that the scale and distribution of incoming data are unpredictable in continual learning. We propose Self-Expansion of pre-trained models with Modularized Adaptation (SEMA), a novel fine-tuning approach which automatically decid
    
[^98]: AIC-UNet: 用于健壮多器官分割的解剖信息驱动级联UNet

    AIC-UNet: Anatomy-informed Cascaded UNet for Robust Multi-Organ Segmentation

    [https://arxiv.org/abs/2403.18878](https://arxiv.org/abs/2403.18878)

    引入了一种新方法，在任何现有编码-解码分割模型上，通过条件化模型预测与可学习的解剖先验来施加解剖约束。

    

    强加关键解剖特征，例如器官数量、形状、大小和相对位置，对于构建健壮的多器官分割模型至关重要。我们通过在现有的编码-解码分割模型上引入可学习的解剖先验，来实施解剖约束的新方法。具体来说，给定腹部扫描时，编码器的一部分通过薄板样条（TPS）网格插值将可学习的先验空间对准给定的输入扫描。然后在解码阶段整合变形的先验以指导模型。

    arXiv:2403.18878v1 Announce Type: cross  Abstract: Imposing key anatomical features, such as the number of organs, their shapes, sizes, and relative positions, is crucial for building a robust multi-organ segmentation model. Current attempts to incorporate anatomical features include broadening effective receptive fields (ERF) size with resource- and data-intensive modules such as self-attention or introducing organ-specific topology regularizers, which may not scale to multi-organ segmentation problems where inter-organ relation also plays a huge role. We introduce a new approach to impose anatomical constraints on any existing encoder-decoder segmentation model by conditioning model prediction with learnable anatomy prior. More specifically, given an abdominal scan, a part of the encoder spatially warps a learnable prior to align with the given input scan using thin plate spline (TPS) grid interpolation. The warped prior is then integrated during the decoding phase to guide the model
    
[^99]: 使用视网膜OCT成像预测心血管疾病风险

    Predicting risk of cardiovascular disease using retinal OCT imaging

    [https://arxiv.org/abs/2403.18873](https://arxiv.org/abs/2403.18873)

    这项研究探讨了使用光学相干断层扫描（OCT）作为额外成像技术来预测心血管疾病的潜力，并通过自监督深度学习和随机森林分类器结合的方法成功区分了心血管疾病风险和非风险患者。

    

    我们调查了光学相干断层扫描（OCT）作为一种额外成像技术来预测未来心血管疾病（CVD）的潜力。我们利用基于变分自动编码器（VAE）的自监督深度学习方法学习了高维3D OCT图像的低维表示，并捕捉了OCT图像中不同视网膜层的独特特征。随后使用学习到的潜在特征和参与者的人口统计数据以及临床数据训练了一个随机森林（RF）分类器，以区分处于CVD事件风险（心梗或中风）和非CVD病例的患者。我们的预测模型基于对多模态数据的训练，评估其能力来正确识别在图像获取后的5年内可能患有CVD事件（心梗或中风）的个体。我们的自监督VAE特征选择和多模态随机森林分类器区分

    arXiv:2403.18873v1 Announce Type: cross  Abstract: We investigated the potential of optical coherence tomography (OCT) as an additional imaging technique to predict future cardiovascular disease (CVD). We utilised a self-supervised deep learning approach based on Variational Autoencoders (VAE) to learn low-dimensional representations of high-dimensional 3D OCT images and to capture distinct characteristics of different retinal layers within the OCT image. A Random Forest (RF) classifier was subsequently trained using the learned latent features and participant demographic and clinical data, to differentiate between patients at risk of CVD events (MI or stroke) and non-CVD cases. Our predictive model, trained on multimodal data, was assessed based on its ability to correctly identify individuals likely to suffer from a CVD event(MI or stroke), within a 5-year interval after image acquisition. Our self-supervised VAE feature selection and multimodal Random Forest classifier differentiate
    
[^100]: 编码器LLMs骨干的定向可视化

    Targeted Visualization of the Backbone of Encoder LLMs

    [https://arxiv.org/abs/2403.18872](https://arxiv.org/abs/2403.18872)

    本文研究了将DeepView方法应用于自然语言处理领域，以减少编码器模型存在的风险并解释模型决策过程。

    

    基于注意力机制的大型语言模型(LLMs)是自然语言处理(NLP)中的最新技术。其中最常见的两种架构是编码器，如BERT，和解码器，如GPT模型。尽管编码器模型取得了成功，但它们也存在一些风险，包括偏见问题或易受对抗性攻击的影响，这表明了需要可解释的AI来检测这些问题。虽然目前存在各种关注预测单个输入的局部可解释性方法，但基于降维的用于分类检查的全局方法，并且在其他领域出现并超越仅在嵌入空间中使用t-SNE的方法，在NLP中并不十分广泛传播。为了缩小这一差距，我们研究了DeepView方法在NLP领域的应用，该方法用于在二维中可视化决策函数的一部分以及数据集。

    arXiv:2403.18872v1 Announce Type: cross  Abstract: Attention based Large Language Models (LLMs) are the state-of-the-art in natural language processing (NLP). The two most common architectures are encoders such as BERT, and decoders like the GPT models. Despite the success of encoder models, on which we focus in this work, they also bear several risks, including issues with bias or their susceptibility for adversarial attacks, signifying the necessity for explainable AI to detect such issues. While there does exist various local explainability methods focusing on the prediction of single inputs, global methods based on dimensionality reduction for classification inspection, which have emerged in other domains and that go further than just using t-SNE in the embedding space, are not widely spread in NLP.   To reduce this gap, we investigate the application of DeepView, a method for visualizing a part of the decision function together with a data set in two dimensions, to the NLP domain.
    
[^101]: 通过临床领域知识衍生的模板提高了后续AI解释在气胸分类中的应用

    Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification

    [https://arxiv.org/abs/2403.18871](https://arxiv.org/abs/2403.18871)

    提出了一种模板引导方法，将气胸的临床知识融入XAI方法，以过滤掉落在模板之外的不相关解释。

    

    气胸是一种急性胸部疾病，由于肺部和胸壁之间异常积聚气体引起。为了解决深度学习模型常见的不透明问题，引入了可解释人工智能（XAI）方法来勾画深度学习模型进行气胸诊断相关区域。然而，这些解释有时会偏离实际病变区域，突显了进一步改进的必要性。我们提出了一种模板引导方法，将气胸的临床知识融入XAI方法生成的模型解释中，从而提高这些解释的质量。利用放射科医生创建的一种病变划分，我们的方法首先生成代表可能发生气胸区域的模板。然后将此模板叠加到模型解释上，以过滤掉落在模板之外的不相关解释。

    arXiv:2403.18871v1 Announce Type: cross  Abstract: Background: Pneumothorax is an acute thoracic disease caused by abnormal air collection between the lungs and chest wall. To address the opaqueness often associated with deep learning (DL) models, explainable artificial intelligence (XAI) methods have been introduced to outline regions related to pneumothorax diagnoses made by DL models. However, these explanations sometimes diverge from actual lesion areas, highlighting the need for further improvement. Method: We propose a template-guided approach to incorporate the clinical knowledge of pneumothorax into model explanations generated by XAI methods, thereby enhancing the quality of these explanations. Utilizing one lesion delineation created by radiologists, our approach first generates a template that represents potential areas of pneumothorax occurrence. This template is then superimposed on model explanations to filter out extraneous explanations that fall outside the template's b
    
[^102]: SugarcaneNet2024: LASSO正则化的预训练模型的优化加权平均集成方法用于甘蔗病害分类

    SugarcaneNet2024: An Optimized Weighted Average Ensemble Approach of LASSO Regularized Pre-trained Models for Sugarcane Disease Classification

    [https://arxiv.org/abs/2403.18870](https://arxiv.org/abs/2403.18870)

    SugarcaneNet2024是通过优化加权平均集成LASSO正则化的预训练模型，在甘蔗病害分类中表现出色，具有快速准确的检测能力。

    

    甘蔗作为世界糖业的关键作物，容易受多种病害侵害，这些病害对其产量和质量都有重大负面影响。为了有效管理和实施预防措施，必须及时准确地检测病害。本研究提出了一种名为SugarcaneNet2024的独特模型，通过叶片图像处理，能够优于先前方法自动快速检测甘蔗病害。我们提出的模型汇总了七个定制的、经过LASSO正则化的预训练模型的优化加权平均集成，特别是InceptionV3、InceptionResNetV2、DenseNet201、DenseNet169、Xception和ResNet152V2。最初，我们在这些预训练模型底部添加了三层更密集层，具有0.0001的LASSO正则化，三个30%的dropout层和三个启用renorm的批量归一化，以提高性能。

    arXiv:2403.18870v1 Announce Type: cross  Abstract: Sugarcane, a key crop for the world's sugar industry, is prone to several diseases that have a substantial negative influence on both its yield and quality. To effectively manage and implement preventative initiatives, diseases must be detected promptly and accurately. In this study, we present a unique model called sugarcaneNet2024 that outperforms previous methods for automatically and quickly detecting sugarcane disease through leaf image processing. Our proposed model consolidates an optimized weighted average ensemble of seven customized and LASSO-regularized pre-trained models, particularly InceptionV3, InceptionResNetV2, DenseNet201, DenseNet169, Xception, and ResNet152V2. Initially, we added three more dense layers with 0.0001 LASSO regularization, three 30% dropout layers, and three batch normalizations with renorm enabled at the bottom of these pre-trained models to improve the performance. The accuracy of sugarcane leaf dise
    
[^103]: 多层次图贝叶斯优化用于多重影响最大化

    Graph Bayesian Optimization for Multiplex Influence Maximization

    [https://arxiv.org/abs/2403.18866](https://arxiv.org/abs/2403.18866)

    本文提出了使用多层扩散模型和信息关联机制解决多重影响最大化问题的方法

    

    影响最大化(IM)是在社交网络中识别有限数量的初始有影响力的用户以最大化受影响用户数量的问题。然而，先前的研究主要集中在个体信息传播上，忽略了多个信息项的同时和互动传播。本文首先使用多层扩散模型和信息关联机制对多重影响最大化(Multi-IM)问题进行了规划。在这个问题中，种子集是具有影响力的用户和信息关联的组合。

    arXiv:2403.18866v1 Announce Type: cross  Abstract: Influence maximization (IM) is the problem of identifying a limited number of initial influential users within a social network to maximize the number of influenced users. However, previous research has mostly focused on individual information propagation, neglecting the simultaneous and interactive dissemination of multiple information items. In reality, when users encounter a piece of information, such as a smartphone product, they often associate it with related products in their minds, such as earphones or computers from the same brand. Additionally, information platforms frequently recommend related content to users, amplifying this cascading effect and leading to multiplex influence diffusion.   This paper first formulates the Multiplex Influence Maximization (Multi-IM) problem using multiplex diffusion models with an information association mechanism. In this problem, the seed set is a combination of influential users and inform
    
[^104]: 可解释的机器学习用于天气和气候预测：一项调查

    Interpretable Machine Learning for Weather and Climate Prediction: A Survey

    [https://arxiv.org/abs/2403.18864](https://arxiv.org/abs/2403.18864)

    可解释的机器学习技术对于增强天气和气候建模的可信度和实用性至关重要，包括后验可解释性技术和从头设计的固有可解释模型。

    

    最近，先进的机器学习模型在天气和气候预测方面取得了高预测准确性。然而，这些复杂模型通常缺乏固有的透明度和可解释性，表现为“黑匣子”，阻碍了用户信任，也限制了进一步的模型改进。因此，可解释的机器学习技术在增强天气和气候建模的可信度和实用性方面变得至关重要。在本调查中，我们审查了应用于气象预测的当前可解释的机器学习方法。我们将方法分类为两个主要范例：1）后验可解释性技术，解释预训练模型的方法，如基于扰动、基于博弈论和基于梯度的归因方法。2）从头开始设计固有可解释模型的方法，使用树集成和可解释神经网络等架构。我们总结了每种技术如何提供对预测模型内部的见解。

    arXiv:2403.18864v1 Announce Type: cross  Abstract: Advanced machine learning models have recently achieved high predictive accuracy for weather and climate prediction. However, these complex models often lack inherent transparency and interpretability, acting as "black boxes" that impede user trust and hinder further model improvements. As such, interpretable machine learning techniques have become crucial in enhancing the credibility and utility of weather and climate modeling. In this survey, we review current interpretable machine learning approaches applied to meteorological predictions. We categorize methods into two major paradigms: 1) Post-hoc interpretability techniques that explain pre-trained models, such as perturbation-based, game theory based, and gradient-based attribution methods. 2) Designing inherently interpretable models from scratch using architectures like tree ensembles and explainable neural networks. We summarize how each technique provides insights into the pre
    
[^105]: 通过链接预测进行定向标准引文推荐和排名

    Directed Criteria Citation Recommendation and Ranking Through Link Prediction

    [https://arxiv.org/abs/2403.18855](https://arxiv.org/abs/2403.18855)

    该研究提出使用链接预测作为自动展示相关文献的方法，并通过模型生成的语义表示在推荐和排名任务上取得优越性能。

    

    我们探讨使用链接预测作为自动展示与新文档在主题或上下文上可能相关的现有文献的代理。我们的模型使用基于Transformer的图嵌入来编码每个文档的含义，呈现为引文网络中的节点。我们展示了我们的模型生成的语义表示能够在推荐和排名任务中胜过其他基于内容的方法。这为探索引文图提供了一种整体方法，特别是在那些这些文档正确互相引用至关重要的领域，以便最小化任何不一致性的可能性。

    arXiv:2403.18855v1 Announce Type: cross  Abstract: We explore link prediction as a proxy for automatically surfacing documents from existing literature that might be topically or contextually relevant to a new document. Our model uses transformer-based graph embeddings to encode the meaning of each document, presented as a node within a citation network. We show that the semantic representations that our model generates can outperform other content-based methods in recommendation and ranking tasks. This provides a holistic approach to exploring citation graphs in domains where it is critical that these documents properly cite each other, so as to minimize the possibility of any inconsistencies
    
[^106]: 利用气象再分析数据对高耸物体上升闪电的时空风险进行评估

    Spatio-seasonal risk assessment of upward lightning at tall objects using meteorological reanalysis data

    [https://arxiv.org/abs/2403.18853](https://arxiv.org/abs/2403.18853)

    该研究利用随机森林分析了在高耸物体处测得的上升闪电与35个较大尺度气象变量之间的关系，发现较大尺度的向上速度、10米高度的风速和风向以及云物理变量对UL风险评估贡献最大，进而预测了研究区域UL的风险。

    

    本研究调查了高耸物体处的闪电情况，并评估了东阿尔卑斯山及其周边地区上升闪电（UL）的风险。虽然不常见，但UL对风力涡轮机尤其构成威胁，因为其长时间电流可能会造成重大破坏。当前的风险评估方法忽视了气象条件的影响，可能低估了UL的风险。因此，本研究采用随机森林，一种机器学习技术，分析了在奥地利Gaisberg Tower测量的UL与35个较大尺度气象变量之间的关系。其中，较大尺度的向上速度、10米高度的风速和风向以及云物理变量提供了最相关的信息。随机森林以1 km^2的分辨率预测了研究区域的UL风险。强烈的地表临近风与受高地形上升的偏转相结合会增加UL风险。UL的日变化周期

    arXiv:2403.18853v1 Announce Type: cross  Abstract: This study investigates lightning at tall objects and evaluates the risk of upward lightning (UL) over the eastern Alps and its surrounding areas. While uncommon, UL poses a threat, especially to wind turbines, as the long-duration current of UL can cause significant damage. Current risk assessment methods overlook the impact of meteorological conditions, potentially underestimating UL risks. Therefore, this study employs random forests, a machine learning technique, to analyze the relationship between UL measured at Gaisberg Tower (Austria) and $35$ larger-scale meteorological variables. Of these, the larger-scale upward velocity, wind speed and direction at 10 meters and cloud physics variables contribute most information. The random forests predict the risk of UL across the study area at a 1 km$^2$ resolution. Strong near-surface winds combined with upward deflection by elevated terrain increase UL risk. The diurnal cycle of the UL 
    
[^107]: 基于盲归一化斯坦变分梯度下降的智能大规模随机接入检测

    The Blind Normalized Stein Variational Gradient Descent-Based Detection for Intelligent Massive Random Access

    [https://arxiv.org/abs/2403.18846](https://arxiv.org/abs/2403.18846)

    提出了一种基于盲归一化斯坦变分梯度下降的检测器，用于解决智能大规模随机接入中的前导碰撞问题，并通过开发改进Hadamard变换和设计块MHT层来提高检测性能。

    

    缺乏高效的前导检测算法仍然是解决实际通信场景中智能大规模随机接入(RA)中前导碰撞问题的挑战。为解决这一问题，我们提出了一种新颖的基于最大似然估计(MLE)模型的早期前导检测方案，在授予式RA流程的第一步。提出了一种新颖的基于盲归一化斯坦变分梯度下降(SVGD)的检测器，以获得MLE模型的近似解。首先，通过探索Hadamard变换和小波变换之间的关系，开发了一种新的改进Hadamard变换(MHT)，使用二阶导数滤波器将高频分离出重要部分。接下来，为了消除SVGD检测器中的噪声并减轻梯度消失问题，设计了基于MHT的块MHT层，该层基于MHT、缩放层、软阈值层构建。

    arXiv:2403.18846v1 Announce Type: cross  Abstract: The lack of an efficient preamble detection algorithm remains a challenge for solving preamble collision problems in intelligent massive random access (RA) in practical communication scenarios. To solve this problem, we present a novel early preamble detection scheme based on a maximum likelihood estimation (MLE) model at the first step of the grant-based RA procedure. A novel blind normalized Stein variational gradient descent (SVGD)-based detector is proposed to obtain an approximate solution to the MLE model. First, by exploring the relationship between the Hadamard transform and wavelet transform, a new modified Hadamard transform (MHT) is developed to separate high-frequencies from important components using the second-order derivative filter. Next, to eliminate noise and mitigate the vanishing gradients problem in the SVGD-based detectors, the block MHT layer is designed based on the MHT, scaling layer, soft-thresholding layer, i
    
[^108]: JEP-KD：基于联合嵌入预测架构的视觉语音识别知识蒸馏

    JEP-KD: Joint-Embedding Predictive Architecture Based Knowledge Distillation for Visual Speech Recognition

    [https://arxiv.org/abs/2403.18843](https://arxiv.org/abs/2403.18843)

    该论文提出了一种基于联合嵌入预测架构的知识蒸馏方法JEP-KD，旨在通过引入生成网络在嵌入层内增强视频编码器的语义特征提取能力，从而更有效地利用音频特征，逐步减少视觉语音识别与自动语音识别之间的性能差距。

    

    视觉语音识别（VSR）任务通常被认为具有比自动语音识别（ASR）更低的理论性能上限，这是由于通过视觉方式传达语义信息的固有限制。为了减轻这一挑战，本文介绍了一种先进的知识蒸馏方法，使用一个名为JEP-KD的Joint-Embedding Predictive Architecture（JEPA），旨在更有效地利用音频特征进行模型训练。JEP-KD的核心在于在嵌入层内包含一个生成网络，增强了视频编码器对语义特征提取的能力，并使其与预先训练的ASR模型编码器的音频特征更加接近。该方法旨在逐渐减少VSR和ASR之间的性能差距。此外，还建立了一套全面的多模态、多阶段训练方案，以增强JEP-KD框架的稳健性。

    arXiv:2403.18843v1 Announce Type: cross  Abstract: Visual Speech Recognition (VSR) tasks are generally recognized to have a lower theoretical performance ceiling than Automatic Speech Recognition (ASR), owing to the inherent limitations of conveying semantic information visually. To mitigate this challenge, this paper introduces an advanced knowledge distillation approach using a Joint-Embedding Predictive Architecture (JEPA), named JEP-KD, designed to more effectively utilize audio features during model training. Central to JEP-KD is the inclusion of a generative network within the embedding layer, which enhances the video encoder's capacity for semantic feature extraction and brings it into closer alignment with the audio features from a pre-trained ASR model's encoder. This approach aims to progressively reduce the performance gap between VSR and ASR. Moreover, a comprehensive multimodal, multistage training regimen for the JEP-KD framework is established, bolstering the robustness 
    
[^109]: 费曼图作为计算图的表示

    Feynman Diagrams as Computational Graphs

    [https://arxiv.org/abs/2403.18840](https://arxiv.org/abs/2403.18840)

    该论文提出了一种用于量子场论中高阶费曼图的计算图表示方法，通过组织成张量操作的分形结构显著减少计算冗余，集成了Taylor-mode自动微分技术，开发了费曼图编译器以优化计算图。

    

    我们提出了一种适用于空间、时间、动量和频率领域的高阶费曼图的计算图表示，适用于量子场论（QFT）。利用戴森-施温格方程和树图方程，我们的方法有效地将这些图组织成张量操作的分形结构，显著减少了计算冗余。这种方法不仅简化了复杂图的评估，还促进了场论重整化方案的高效实施，对增强微扰QFT计算至关重要。这一进展的关键在于集成了Taylor-mode自动微分，这是机器学习包中用于在计算图上高效计算高阶导数的关键技术。为了操作化这些概念，我们开发了一个费曼图编译器，优化了各种计算图。

    arXiv:2403.18840v1 Announce Type: cross  Abstract: We propose a computational graph representation of high-order Feynman diagrams in Quantum Field Theory (QFT), applicable to any combination of spatial, temporal, momentum, and frequency domains. Utilizing the Dyson-Schwinger and parquet equations, our approach effectively organizes these diagrams into a fractal structure of tensor operations, significantly reducing computational redundancy. This approach not only streamlines the evaluation of complex diagrams but also facilitates an efficient implementation of the field-theoretic renormalization scheme, crucial for enhancing perturbative QFT calculations. Key to this advancement is the integration of Taylor-mode automatic differentiation, a key technique employed in machine learning packages to compute higher-order derivatives efficiently on computational graphs. To operationalize these concepts, we develop a Feynman diagram compiler that optimizes diagrams for various computational pl
    
[^110]: 货币交易中的长短期记忆模式识别

    Long Short-Term Memory Pattern Recognition in Currency Trading

    [https://arxiv.org/abs/2403.18839](https://arxiv.org/abs/2403.18839)

    本研究通过分析金融市场的Wyckoff阶段框架，探讨了累积模式和交易范围等阶段的重要性，揭示了如何利用LSTM模型分析市场数据以预测价格走势和做出明智决策。

    

    本研究深入分析了通过Richard D. Wyckoff在20世纪早期设计的Wyckoff阶段框架的金融市场。重点关注Wyckoff框架中的累积模式，研究探讨了交易范围和次级测试阶段，阐明了它们在理解市场动态和识别潜在交易机会中的重要性。通过剖析这些阶段的复杂性，本研究揭示了市场结构通过创造流动性，为交易者如何利用这些知识来预测价格走势和做出明智决策提供了见解。有效检测和分析Wyckoff模式需要能够处理复杂市场数据的强大计算模型，其中空间数据最好通过卷积神经网络(CNN)进行分析，而时间数据则通过长短期记忆（LSTM）模型进行分析。

    arXiv:2403.18839v1 Announce Type: cross  Abstract: This study delves into the analysis of financial markets through the lens of Wyckoff Phases, a framework devised by Richard D. Wyckoff in the early 20th century. Focusing on the accumulation pattern within the Wyckoff framework, the research explores the phases of trading range and secondary test, elucidating their significance in understanding market dynamics and identifying potential trading opportunities. By dissecting the intricacies of these phases, the study sheds light on the creation of liquidity through market structure, offering insights into how traders can leverage this knowledge to anticipate price movements and make informed decisions. The effective detection and analysis of Wyckoff patterns necessitate robust computational models capable of processing complex market data, with spatial data best analyzed using Convolutional Neural Networks (CNNs) and temporal data through Long Short-Term Memory (LSTM) models. The creation
    
[^111]: 使用支持向量机在无刷直流电机中无传感器估计速度和位置的新方法

    A New Method for Sensorless Estimation of the Speed and Position in Brushed DC Motors Using Support Vector Machines

    [https://arxiv.org/abs/2403.18833](https://arxiv.org/abs/2403.18833)

    使用支持向量机在无刷直流电机中开发出一种新方法，基于电流纹波进行速度和位置估计，通过检测脉冲来估计速度并计数来估计位置，能够检测鬼脉冲和丢弃虚假脉冲，具有较高的准确性和可靠性。

    

    目前，对于许多应用来说，了解电机的速度和位置是必要的。这可以通过将机械传感器耦合到电机轴上或使用无传感器技术来实现。无刷直流电机中的无传感器技术可以分为两种类型：1）基于动态无刷直流电机模型的技术和2）基于电流纹波分量的技术。本文提出了一种基于电流纹波的新方法，用于使用支持向量机在无刷直流电机中进行速度和位置估计。所提出的方法仅测量电流并检测该信号中的脉冲。通过使用检测到的脉冲之间的反距离来估计电机速度，并通过计算所有检测到的脉冲来估计位置。与其他无传感器方法相比，该方法的主要优点是能够检测鬼脉冲并丢弃虚假脉冲。在两个fra进行了测试

    arXiv:2403.18833v1 Announce Type: cross  Abstract: Currently, for many applications, it is necessary to know the speed and position of motors. This can be achieved using mechanical sensors coupled to the motor shaft or using sensorless techniques. The sensorless techniques in brushed dc motors can be classified into two types: 1) techniques based on the dynamic brushed dc motor model and 2) techniques based on the ripple component of the current. This paper presents a new method, based on the ripple component, for speed and position estimation in brushed dc motors, using support vector machines. The proposed method only measures the current and detects the pulses in this signal. The motor speed is estimated by using the inverse distance between the detected pulses, and the position is estimated by counting all detected pulses. The ability to detect ghost pulses and to discard false pulses is the main advantage of this method over other sensorless methods. The performed tests on two fra
    
[^112]: 将生成网络与认知共同模型相结合

    Bridging Generative Networks with the Common Model of Cognition

    [https://arxiv.org/abs/2403.18827](https://arxiv.org/abs/2403.18827)

    通过将模块重构为影子生成系统，实现了认知架构与生成神经网络的无缝连接

    

    本文提出了一个理论框架，将认知共同模型调整为人工智能领域中的大型生成网络模型。通过将共同模型中的模块重构为周边的影子生成系统，这些影子生成系统辅助处理高层推理，从而实现认知架构与生成神经网络的无缝连接。

    arXiv:2403.18827v1 Announce Type: new  Abstract: This article presents a theoretical framework for adapting the Common Model of Cognition to large generative network models within the field of artificial intelligence. This can be accomplished by restructuring modules within the Common Model into shadow production systems that are peripheral to a central production system, which handles higher-level reasoning based on the shadow productions' output. Implementing this novel structure within the Common Model allows for a seamless connection between cognitive architectures and generative neural networks.
    
[^113]: 提升金融数据可视化以辅助投资决策

    Enhancing Financial Data Visualization for Investment Decision-Making

    [https://arxiv.org/abs/2403.18822](https://arxiv.org/abs/2403.18822)

    LSTM网络在金融数据预测中的应用，利用多个特征提升模型捕捉复杂模式，通过可视化关键属性揭示微妙差异，采用25天时间步长内的输入结构捕捉时间上的复杂性

    

    本文探讨了利用长短期记忆（LSTM）网络预测股票动态的潜力，着重于识别微妙的涨跌模式。利用纽约证券交易所（NYSE）的数据集，研究融合了多种特征，以增强LSTM在捕捉复杂模式方面的能力。通过可视化关键属性（如开盘价、收盘价、最低价和最高价），有助于揭示对全面市场理解至关重要的微妙差异。该研究精心设计的LSTM输入结构，灵感来自于已建立的指导方针，结合了25天时间步长内的价格和交易量属性，使得模型能够捕捉时间上的复杂性。

    arXiv:2403.18822v1 Announce Type: cross  Abstract: Navigating the intricate landscape of financial markets requires adept forecasting of stock price movements. This paper delves into the potential of Long Short-Term Memory (LSTM) networks for predicting stock dynamics, with a focus on discerning nuanced rise and fall patterns. Leveraging a dataset from the New York Stock Exchange (NYSE), the study incorporates multiple features to enhance LSTM's capacity in capturing complex patterns. Visualization of key attributes, such as opening, closing, low, and high prices, aids in unraveling subtle distinctions crucial for comprehensive market understanding. The meticulously crafted LSTM input structure, inspired by established guidelines, incorporates both price and volume attributes over a 25-day time step, enabling the model to capture temporal intricacies. A comprehensive methodology, including hyperparameter tuning with Grid Search, Early Stopping, and Callback mechanisms, leads to a remar
    
[^114]: ECoDepth: 有效调整扩散模型以用于单目深度估计

    ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation

    [https://arxiv.org/abs/2403.18807](https://arxiv.org/abs/2403.18807)

    通过使用预训练的ViT模型生成的全局图像先验，为单图深度估计模型提供更详细的上下文信息，并提出了一种新的使用扩散骨干且受ViT嵌入条件约束的深度估计模型。

    

    在缺乏视差线索的情况下，基于学习的单图深度估计（SIDE）模型严重依赖图像中的阴影和上下文线索。我们从已有研究的启发中探讨使用从预训练的ViT模型生成的全局图像先验，以提供更详细的上下文信息。基于这一想法，我们提出了一种新的使用扩散骨干的SIDE模型，其受到ViT嵌入的条件约束。

    arXiv:2403.18807v1 Announce Type: cross  Abstract: In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embedding
    
[^115]: 基于 FPGA 的无人机神经推力控制器

    Fpga-Based Neural Thrust Controller for UAVs

    [https://arxiv.org/abs/2403.18703](https://arxiv.org/abs/2403.18703)

    本研究探索了将 FPGA 作为一种解决方案，通过实现神经网络控制器以提高无人机适应性和性能，尤其在未知环境中。

    

    无人机（UAVs）的出现通过提供多功能、成本效益和可访问平台，改善了各种领域实施最先进算法的可能性。为了完成更广泛的任务，越来越需要增强机载计算性能以应对不断增加的复杂性和动态环境条件。最近的进展看到深度神经网络（DNNs）的应用，特别是与强化学习一起，以提高无人机的适应性和性能，尤其是在未知环境中。然而，DNNs 的计算要求对许多 UAVs 上有限的计算资源构成挑战。本文探讨了将现场可编程门阵列（FPGAs）作为应对这一挑战的可行解决方案，提供了灵活性、高性能、能量和时间效率。我们提出了一种配备 Artix-7 FPGA 的新型硬件板。

    arXiv:2403.18703v1 Announce Type: cross  Abstract: The advent of unmanned aerial vehicles (UAVs) has improved a variety of fields by providing a versatile, cost-effective and accessible platform for implementing state-of-the-art algorithms. To accomplish a broader range of tasks, there is a growing need for enhanced on-board computing to cope with increasing complexity and dynamic environmental conditions. Recent advances have seen the application of Deep Neural Networks (DNNs), particularly in combination with Reinforcement Learning (RL), to improve the adaptability and performance of UAVs, especially in unknown environments. However, the computational requirements of DNNs pose a challenge to the limited computing resources available on many UAVs. This work explores the use of Field Programmable Gate Arrays (FPGAs) as a viable solution to this challenge, offering flexibility, high performance, energy and time efficiency. We propose a novel hardware board equipped with an Artix-7 FPGA 
    
[^116]: 噢！我们冷冻：通过信号传播分析改进大型语言模型的量化知识蒸馏

    Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models

    [https://arxiv.org/abs/2403.18159](https://arxiv.org/abs/2403.18159)

    通过信号传播分析，提出了一种改进大型语言模型的量化知识蒸馏方法，并提供了ov-freeze稳定KD-QAT过程的洞察。

    

    大型生成模型，如大型语言模型（LLMs）和扩散模型分别在NLP和计算机视觉领域引起了革命。然而，它们的推理速度慢，计算和内存需求高，这使得在边缘设备上部署它们变得具有挑战性。在这项研究中，我们提出了一种轻量级的量化感知微调技术，使用知识蒸馏（KD-QAT）来改善使用常用数据集改进4位重量量化的LLMs的性能，以实现流行的语言使用案例，在设备聊天应用中。为了改进这种微调范式，作为主要贡献，我们通过经验研究训练过程中的梯度传播，提供对KD-QAT稳定性的洞察，以更好地理解基于KD-QAT的方法对低位量化误差的脆弱性。根据我们的见解，我们提出了ov-freeze，一种稳定KD-QAT过程的简单技术。最后，我们进行了实验

    arXiv:2403.18159v1 Announce Type: cross  Abstract: Large generative models, such as large language models (LLMs) and diffusion models have as revolutionized the fields of NLP and computer vision respectively. However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices. In this study, we propose a light-weight quantization aware fine tuning technique using knowledge distillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs using commonly available datasets to realize a popular language use case, on device chat applications. To improve this paradigm of finetuning, as main contributions, we provide insights into stability of KD-QAT by empirically studying the gradient propagation during training to better understand the vulnerabilities of KD-QAT based approaches to low-bit quantization errors. Based on our insights, we propose ov-freeze, a simple technique to stabilize the KD-QAT process. Finally, we expe
    
[^117]: 从部分观测预测物种出现模式

    Predicting species occurrence patterns from partial observations

    [https://arxiv.org/abs/2403.18028](https://arxiv.org/abs/2403.18028)

    提出了一个问题，即采用卫星图像和其他物种出现信息来预测物种出现模式，并提出了一个通用模型R-Tran，可以利用部分观测数据进行预测，优于其他方法。

    

    为了应对生物多样性和气候危机，我们需要了解物种分布的位置以及这些模式如何变化。然而，大多数物种的观测数据仍然非常有限，可用数据的量在不同分类群之间差异很大。我们提出了一个问题，即在给定卫星图像和其他物种出现信息的情况下预测物种出现模式。为了在此任务上评估算法，我们介绍了SatButterfly数据集，其中包含了蝴蝶的卫星图像、环境数据和观测数据，旨在与现有的鸟类观测数据集SatBird配对。为了解决这一任务，我们提出了一个通用模型R-Tran，用于预测物种出现模式，可以在任何地方使用部分观测数据。我们发现，R-Tran在预测物种遭遇率方面优于其他方法。

    arXiv:2403.18028v1 Announce Type: cross  Abstract: To address the interlinked biodiversity and climate crises, we need an understanding of where species occur and how these patterns are changing. However, observational data on most species remains very limited, and the amount of data available varies greatly between taxonomic groups. We introduce the problem of predicting species occurrence patterns given (a) satellite imagery, and (b) known information on the occurrence of other species. To evaluate algorithms on this task, we introduce SatButterfly, a dataset of satellite images, environmental data and observational data for butterflies, which is designed to pair with the existing SatBird dataset of bird observational data. To address this task, we propose a general model, R-Tran, for predicting species occurrence patterns that enables the use of partial observational data wherever found. We find that R-Tran outperforms other methods in predicting species encounter rates with partial
    
[^118]: 通过特定掩码损失改善预训练语言模型的敏感性：以生物医学实体识别为例

    Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER

    [https://arxiv.org/abs/2403.18025](https://arxiv.org/abs/2403.18025)

    提出了Mask Specific Language Modeling（MSLM）方法来改善LM在微调过程中对目标领域知识的敏感性，通过加权领域特定术语的重要性进行学习。

    

    将语言模型（LMs）调整到新领域通常通过在特定领域数据上微调预训练LM（PLM）来实现。微调将新知识引入LM，使它能够理解和有效执行目标域任务。然而，微调可能会无意中变得不够敏感，如果它忽视了源域和目标域之间的广泛差异（例如在词义上）。为了解决微调不敏感的问题，我们提出了Mask Specific Language Modeling（MSLM），一种通过在微调过程中适当加权领域特定术语（DS-terms）的重要性来有效获取目标领域知识的方法。MSLM同时屏蔽DS术语和通用词，然后通过确保LM受到更大惩罚来学习特定于掩码的损失。

    arXiv:2403.18025v1 Announce Type: cross  Abstract: Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for in
    
[^119]: DORE：一份用于葡萄牙语定义生成的数据集

    DORE: A Dataset For Portuguese Definition Generation

    [https://arxiv.org/abs/2403.18018](https://arxiv.org/abs/2403.18018)

    DORE是第一个用于葡萄牙语的定义生成数据集，填补了这一领域的空白，包含超过10万个定义，并评估了多种基于深度学习的模型。

    

    arXiv:2403.18018v1 公告类型：新的 摘要：定义建模（DM）是自动为特定单词生成词典定义的任务。具有DM能力的计算系统可以在多个受众中受益，因为DM被视为监督自然语言生成问题，这些系统需要大量带注释的数据集来训练机器学习（ML）模型。已经发布了一些用于英语和其他高资源语言的DM数据集。尽管葡萄牙语在大多数自然语言处理任务中被认为是一种中/高资源语言，且被2亿多母语人口使用，但目前尚无葡萄牙语的DM数据集。在这项研究中，我们通过引入DORE填补了这一空白；这是第一个用于葡萄牙语的定义建模数据集，包含超过10万个定义。我们还在DORE上评估了几种基于深度学习的DM模型，并报告了结果。

    arXiv:2403.18018v1 Announce Type: new  Abstract: Definition modelling (DM) is the task of automatically generating a dictionary definition for a specific word. Computational systems that are capable of DM can have numerous applications benefiting a wide range of audiences. As DM is considered a supervised natural language generation problem, these systems require large annotated datasets to train the machine learning (ML) models. Several DM datasets have been released for English and other high-resource languages. While Portuguese is considered a mid/high-resource language in most natural language processing tasks and is spoken by more than 200 million native speakers, there is no DM dataset available for Portuguese. In this research, we fill this gap by introducing DORE; the first dataset for Definition MOdelling for PoRtuguEse containing more than 100,000 definitions. We also evaluate several deep learning based DM models on DORE and report the results. The dataset and the findings o
    
[^120]: LISA：用于高效内存大型语言模型微调的逐层重要性采样

    LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning

    [https://arxiv.org/abs/2403.17919](https://arxiv.org/abs/2403.17919)

    逐层重要性采样的新方法LISA在微调任务中表现出色，记忆成本低且优于传统方法。

    

    机器学习领域自大型语言模型（LLMs）首次出现以来取得了令人瞩目的进展，然而它们巨大的内存消耗已成为大规模训练的主要障碍。虽然已经提出了诸如低秩调整（LoRA）之类的参数高效微调技术来缓解这一问题，但在大多数大规模微调设置中，它们的性能仍无法与完整参数训练相匹配。为弥补这一不足，我们研究了LoRA在微调任务中的逐层特性，并观察到不同层之间权重范数的异常偏斜。利用这一关键观察，我们发现了一个令人惊讶简单的训练策略，在记忆成本低于LoRA的情况下，在广泛的设置中优于LoRA和完整参数训练。我们将其命名为Layerwise Importance Sampled AdamW（LISA），这是LoRA的一个有希望的替代方案，应用了

    arXiv:2403.17919v1 Announce Type: cross  Abstract: The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of
    
[^121]: 伪造还是JPEG？揭示生成图像检测数据集中的常见偏见

    Fake or JPEG? Revealing Common Biases in Generated Image Detection Datasets

    [https://arxiv.org/abs/2403.17608](https://arxiv.org/abs/2403.17608)

    许多AI生成图像检测数据集存在与JPEG压缩和图像大小相关的偏见，去除这些偏见可以显著提高对JPEG压缩的稳健性并显著改变检测器的跨生成器性能。

    

    生成图像模型的广泛应用凸显了检测人造内容的迫切需求，这是打击广泛操纵和误导的关键一步。因此，许多检测器和相关数据集已经出现。然而，许多这些数据集不经意地引入了不良偏见，从而影响了检测器的效果和评估。本文强调了许多用于AI生成图像检测的数据集包含与JPEG压缩和图像大小有关的偏见。使用GenImage数据集，我们证明检测器确实从这些不受欢迎的因素中学习。此外，我们展示去除这些命名偏见会显著增加针对JPEG压缩的鲁棒性，并显著改变评估检测器的跨生成器性能。具体来说，对于ResNet50和S

    arXiv:2403.17608v1 Announce Type: cross  Abstract: The widespread adoption of generative image models has highlighted the urgent need to detect artificial content, which is a crucial step in combating widespread manipulation and misinformation. Consequently, numerous detectors and associated datasets have emerged. However, many of these datasets inadvertently introduce undesirable biases, thereby impacting the effectiveness and evaluation of detectors. In this paper, we emphasize that many datasets for AI-generated image detection contain biases related to JPEG compression and image size. Using the GenImage dataset, we demonstrate that detectors indeed learn from these undesired factors. Furthermore, we show that removing the named biases substantially increases robustness to JPEG compression and significantly alters the cross-generator performance of evaluated detectors. Specifically, it leads to more than 11 percentage points increase in cross-generator performance for ResNet50 and S
    
[^122]: 期望与现实：实践中评估入侵检测系统

    Expectations Versus Reality: Evaluating Intrusion Detection Systems in Practice

    [https://arxiv.org/abs/2403.17458](https://arxiv.org/abs/2403.17458)

    论文通过实证比较不同入侵检测系统，发现最佳解决方案取决于外部变量，如攻击类型、复杂性和网络环境，深度神经网络在某些数据集上表现最佳，但并非始终是最佳选择。

    

    我们的论文通过实证比较最近的入侵检测系统，为用户提供客观比较，以帮助用户根据其需求选择最适合的解决方案。我们的结果显示，没有一种解决方案是最好的，而是取决于外部变量，如攻击类型、复杂性和数据集中的网络环境。例如，BoT_IoT和Stratosphere IoT数据集都捕获了与物联网相关的攻击，但深度神经网络在使用BoT_IoT数据集进行测试时表现最佳，而在使用Stratosphere IoT数据集进行测试时HELAD表现最佳。因此，尽管我们发现深度神经网络解决方案在测试数据集上具有最高的平均F1分数，但并不总是表现最好的。我们进一步讨论了使用文献和项目存储库中的IDS的困难，这使得就IDS选择得出明确结论变得复杂。

    arXiv:2403.17458v1 Announce Type: cross  Abstract: Our paper provides empirical comparisons between recent IDSs to provide an objective comparison between them to help users choose the most appropriate solution based on their requirements. Our results show that no one solution is the best, but is dependent on external variables such as the types of attacks, complexity, and network environment in the dataset. For example, BoT_IoT and Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural network performed the best when tested using the BoT_IoT dataset while HELAD performed the best when tested using the Stratosphere IoT dataset. So although we found that a deep neural network solution had the highest average F1 scores on tested datasets, it is not always the best-performing one. We further discuss difficulties in using IDS from literature and project repositories, which complicated drawing definitive conclusions regarding IDS selection.
    
[^123]: CADGL: 上下文感知深度图学习用于预测药物-药物相互作用

    CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions

    [https://arxiv.org/abs/2403.17210](https://arxiv.org/abs/2403.17210)

    通过CADGL框架，利用上下文感知深度图学习来预测药物-药物相互作用，解决了现有DDI预测模型在泛化、特征提取和现实应用方面的挑战

    

    药物-药物相互作用（DDIs）的研究是药物开发过程中的一个关键元素。DDIs发生在一个药物的性质受其他药物包含的影响时。检测有利的DDIs有可能为在实际设置中应用的创新药物的创造和推进铺平道路。然而，现有的DDI预测模型在极端情况下的泛化、稳健特征提取以及现实应用可能性方面持续面临挑战。我们旨在通过利用上下文感知深度图学习的有效性，引入一种名为CADGL的新颖框架来应对这些挑战。基于定制的变分图自编码器（VGAE），我们利用两个上下文预处理器从两个不同视角：局部邻域和分子上下文，在异质图结构中提取特征，捕获关键的结构和生理化学信息。

    arXiv:2403.17210v1 Announce Type: cross  Abstract: Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process of drug development. DDIs occur when one drug's properties are affected by the inclusion of other drugs. Detecting favorable DDIs has the potential to pave the way for creating and advancing innovative medications applicable in practical settings. However, existing DDI prediction models continue to face challenges related to generalization in extreme cases, robust feature extraction, and real-life application possibilities. We aim to address these challenges by leveraging the effectiveness of context-aware deep graph learning by introducing a novel framework named CADGL. Based on a customized variational graph autoencoder (VGAE), we capture critical structural and physio-chemical information using two context preprocessors for feature extraction from two different perspectives: local neighborhood and molecular context, in a heterogeneous graphical structure. Ou
    
[^124]: 揭示本地差分隐私、平均贝叶斯隐私和最大贝叶斯隐私之间的相互作用

    Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy

    [https://arxiv.org/abs/2403.16591](https://arxiv.org/abs/2403.16591)

    论文探讨了本地差分隐私、贝叶斯隐私及其之间的相互关系，揭示了关于效用-隐私权衡的新见解，并提出了一个框架来突出攻击和防御策略的相互作用和效果。

    

    机器学习的迅速发展导致了隐私定义的多样化，由于对隐私构成的威胁，包括本地差分隐私（LDP）的概念。虽然被广泛接受并在许多领域中被利用，但这种传统的隐私测量方法仍然存在一定限制，从无法防止推断披露到缺乏对对手背景知识的考虑。在这项全面研究中，我们引入贝叶斯隐私并深入探讨本地差分隐私和其贝叶斯对应物之间错综复杂的关系，揭示了关于效用-隐私权衡的新见解。我们引入了一个框架，概括了攻击和防御策略，突出它们之间的相互作用和效果。我们的理论贡献基于平均贝叶斯隐私（ABP）和最大贝叶斯隐私之间的严格定义和关系。

    arXiv:2403.16591v1 Announce Type: cross  Abstract: The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Max
    
[^125]: DeepMachining: 铣床机床加工误差在线预测

    DeepMachining: Online Prediction of Machining Errors of Lathe Machines

    [https://arxiv.org/abs/2403.16451](https://arxiv.org/abs/2403.16451)

    DeepMachining是一种基于深度学习的AI系统，可以在线预测车床机床加工操作的误差，通过预训练和微调模型，实现了高准确性预测，是首批使用预训练深度学习模型预测车床机床加工误差的工厂实验之一。

    

    我们描述了DeepMachining，这是一种基于深度学习的人工智能系统，用于在线预测车床加工操作的加工误差。我们基于工厂的制造数据构建并评估了DeepMachining。具体来说，我们首先对特定车床机床操作预训练深度学习模型，以学习加工状态的显著特征。然后，我们微调预训练模型以适应特定加工任务。我们展示了DeepMachining在涉及不同工件和刀具的多个任务中实现了高预测准确性。据我们所知，这项工作是使用预训练深度学习模型预测车床机床加工误差的首批工厂实验之一。

    arXiv:2403.16451v1 Announce Type: cross  Abstract: We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states. Then, we fine-tune the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.
    
[^126]: 面向资源受限设备的低能量自适应个性化研究

    Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices

    [https://arxiv.org/abs/2403.15905](https://arxiv.org/abs/2403.15905)

    提出了面向资源受限设备的低能耗自适应个性化框架目标块微调，根据数据漂移类型微调不同模块以实现最佳性能和降低能源消耗。

    

    机器学习（ML）模型个性化以解决数据漂移问题在物联网（IoT）应用中是一个重要挑战。目前，大多数方法侧重于微调完整基础模型或其最后几层以适应新数据，但往往忽视能源成本。我们提出了一种面向资源受限设备的低能耗自适应个性化框架——目标块微调（TBFT）。我们将数据漂移和个性化分为三种类型：输入级别、特征级别和输出级别。针对每种类型，我们微调不同模型块以实现在降低能源成本的情况下达到最佳性能。具体而言，输入级、特征级和输出级对应于微调模型的前端、中段和后端。

    arXiv:2403.15905v1 Announce Type: new  Abstract: The personalization of machine learning (ML) models to address data drift is a significant challenge in the context of Internet of Things (IoT) applications. Presently, most approaches focus on fine-tuning either the full base model or its last few layers to adapt to new data, while often neglecting energy costs. However, various types of data drift exist, and fine-tuning the full base model or the last few layers may not result in optimal performance in certain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy adaptive personalization framework designed for resource-constrained devices. We categorize data drift and personalization into three types: input-level, feature-level, and output-level. For each type, we fine-tune different blocks of the model to achieve optimal performance with reduced energy costs. Specifically, input-, feature-, and output-level correspond to fine-tuning the front, middle, and rear blocks of 
    
[^127]: 走势彩票假设和迭代幅度剪枝的洞见

    Insights into the Lottery Ticket Hypothesis and the Iterative Magnitude Pruning

    [https://arxiv.org/abs/2403.15022](https://arxiv.org/abs/2403.15022)

    通过对迭代幅度剪枝过程中不同阶段获得的解决方案的体积/几何和损失景观特征进行经验研究，我们试图洞察走势彩票假设和迭代幅度剪枝中的现象。

    

    arXiv:2403.15022v1 公告类型：新摘要：深度神经网络的走势彩票假设强调了重新训练利用迭代幅度剪枝过程获得的更稀疏网络时所使用的初始化的重要性。至今尚缺乏关于走势彩票假设中提出的特定初始化为何更有利于泛化（和训练）性能的解释。此外，迭代幅度剪枝中的基本原理，如剪枝较小幅度权重和迭代过程的作用，尚缺乏完全理解和解释。在本研究中，我们尝试通过对在迭代幅度剪枝过程的各个阶段获得的解决方案的体积/几何和损失景观特征进行经验研究，以洞察这些现象。

    arXiv:2403.15022v1 Announce Type: new  Abstract: Lottery ticket hypothesis for deep neural networks emphasizes the importance of initialization used to re-train the sparser networks obtained using the iterative magnitude pruning process. An explanation for why the specific initialization proposed by the lottery ticket hypothesis tends to work better in terms of generalization (and training) performance has been lacking. Moreover, the underlying principles in iterative magnitude pruning, like the pruning of smaller magnitude weights and the role of the iterative process, lack full understanding and explanation. In this work, we attempt to provide insights into these phenomena by empirically studying the volume/geometry and loss landscape characteristics of the solutions obtained at various stages of the iterative magnitude pruning process.
    
[^128]: 通过知识编辑实现对大型语言模型的去毒化

    Detoxifying Large Language Models via Knowledge Editing

    [https://arxiv.org/abs/2403.14472](https://arxiv.org/abs/2403.14472)

    本文研究了使用知识编辑技术对大型语言模型进行去毒化，在构建了SafeEdit基准的基础上，提出了一种简单而有效的方法 DINM，可以通过少量调整步骤减少模型的毒性，同时对各种去毒方法的内部机制进行了深入分析。

    

    本文研究了使用知识编辑技术来对大型语言模型（LLMs）进行去毒化。我们构建了一个名为SafeEdit的基准，涵盖了九种不安全类别，具有各种强大的攻击提示，并配备了全面的度量标准进行系统评估。我们进行了实验，比较了知识编辑方法与之前的基准线，结果表明知识编辑有潜力在对LLMs进行去毒化时，在对一般性能的影响相对有限。然后，我们提出了一个简单但有效的基准线，称为通过术中神经监测去毒化（DINM），通过仅一次实例的少量调整步骤减少LLMs的毒性。我们进一步对各种去毒方法的内部机制进行了深入分析，表明先前的方法如SFT和DPO可能仅抑制有毒参数的激活，而DINM则减轻有毒参数的毒性。

    arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
    
[^129]: SpikingResformer: 将ResNet和Vision Transformer在脉冲神经网络中进行桥接

    SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks

    [https://arxiv.org/abs/2403.14302](https://arxiv.org/abs/2403.14302)

    提出了一种新型脉冲自注意机制DSSA以及结合ResNet的多阶段架构的SpikingResformer架构，旨在改善性能和能效，并减少参数。

    

    Vision Transformer在人工神经网络中取得了显著成功，这导致了在脉冲神经网络(SNNs)中结合自注意机制和基于Transformer的结构引起越来越多的兴趣。为了解决这些挑战，我们提出了一种名为Dual Spike Self-Attention (DSSA)的新型脉冲自注意机制，带有合理的扩展方法。基于DSSA，我们提出了一种名为SpikingResformer的新型脉冲Vision Transformer架构，将基于ResNet的多阶段架构与我们提出的DSSA相结合，以提高性能和能效，同时减少参数。

    arXiv:2403.14302v1 Announce Type: cross  Abstract: The remarkable success of Vision Transformers in Artificial Neural Networks (ANNs) has led to a growing interest in incorporating the self-attention mechanism and transformer-based architecture into Spiking Neural Networks (SNNs). While existing methods propose spiking self-attention mechanisms that are compatible with SNNs, they lack reasonable scaling methods, and the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting local features. To address these challenges, we propose a novel spiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a reasonable scaling method. Based on DSSA, we propose a novel spiking Vision Transformer architecture called SpikingResformer, which combines the ResNet-based multi-stage architecture with our proposed DSSA to improve both performance and energy efficiency while reducing parameters. Experimental results show that SpikingResformer ach
    
[^130]: 当SMILES拥有语言：使用文本分类方法对药物SMILES字符串进行药物分类

    When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings

    [https://arxiv.org/abs/2403.12984](https://arxiv.org/abs/2403.12984)

    将药物SMILES字符串视为句子并利用文本分类方法进行药物分类，证实了通过简单的自然语言处理方法解决复杂问题的可能性

    

    复杂的化学结构，如药物，通常由SMILES字符串来定义，作为分子和键的序列。这些SMILES字符串在不同的基于机器学习的药物相关研究和表示工作中使用。在这项工作中，我们摆脱复杂的表示法，提出了一个问题：如果我们将药物SMILES视为常规句子，并进行文本分类以进行药物分类会怎样？我们的实验证实了这种可能性，获得了非常有竞争力的分数。该研究探讨了将每个原子和键视为句子组件的概念，利用基本的自然语言处理方法对药物类型进行分类，表明复杂的问题也可以用更简单的视角来解决。数据和代码可在此处找到：https://github.com/azminewasi/Drug-Classification-NLP。

    arXiv:2403.12984v1 Announce Type: cross  Abstract: Complex chemical structures, like drugs, are usually defined by SMILES strings as a sequence of molecules and bonds. These SMILES strings are used in different complex machine learning-based drug-related research and representation works. Escaping from complex representation, in this work, we pose a single question: What if we treat drug SMILES as conventional sentences and engage in text classification for drug classification? Our experiments affirm the possibility with very competitive scores. The study explores the notion of viewing each atom and bond as sentence components, employing basic NLP methods to categorize drug types, proving that complex problems can also be solved with simpler perspectives. The data and code are available here: https://github.com/azminewasi/Drug-Classification-NLP.
    
[^131]: 安全强化学习中的策略分叉

    Policy Bifurcation in Safe Reinforcement Learning

    [https://arxiv.org/abs/2403.12847](https://arxiv.org/abs/2403.12847)

    我们的研究发现在安全强化学习中可能存在策略分叉现象，提出了拓扑分析以证明在一些情景下，策略需要具有不连续性或多值性，这对应于障碍物自由状态空间为非单连通时需要策略分叉的情况。

    

    安全强化学习为受限最优控制问题提供了先进的解决方案。现有的安全强化学习研究隐含地假设策略函数具有连续性，即策略以平稳、连续的方式将状态映射到动作；然而，我们的研究发现在某些情况下，可行策略应该是不连续或多值的，而在不连续的局部极小值之间插值可能会不可避免地导致约束违规。我们是第一个识别出这种现象生成机制的研究，并采用拓扑分析严谨地证明了安全强化学习中策略分叉的存在，这对应于可达元组的可收缩性。我们的定理揭示了在障碍物自由状态空间为非单连通的情景中，需要策略分叉，意味着其输出动作需要迅速响应状态的变化。

    arXiv:2403.12847v1 Announce Type: new  Abstract: Safe reinforcement learning (RL) offers advanced solutions to constrained optimal control problems. Existing studies in safe RL implicitly assume continuity in policy functions, where policies map states to actions in a smooth, uninterrupted manner; however, our research finds that in some scenarios, the feasible policy should be discontinuous or multi-valued, interpolating between discontinuous local optima can inevitably lead to constraint violations. We are the first to identify the generating mechanism of such a phenomenon, and employ topological analysis to rigorously prove the existence of policy bifurcation in safe RL, which corresponds to the contractibility of the reachable tuple. Our theorem reveals that in scenarios where the obstacle-free state space is non-simply connected, a feasible policy is required to be bifurcated, meaning its output action needs to change abruptly in response to the varying state. To train such a bifu
    
[^132]: 使用深度学习预测沸石的吸附性能

    Zeolite Adsorption Property Prediction using Deep Learning

    [https://arxiv.org/abs/2403.12659](https://arxiv.org/abs/2403.12659)

    通过深度学习模型，本研究提出了一个比传统分子模拟快4到5个数量级的方法来预测沸石的吸附性能，并可以用于识别吸附位点。

    

    能够有效预测沸石的吸附性能对于加速新材料设计过程有很大的益处。本工作中，我们提出了一个比分子模拟快4到5个数量级的模型，用于吸附性能的预测。通过机器学习模型预测得到的结果与蒙特卡洛模拟得到的数值一致，验证了该模型可用于性能预测，同时还展示了该模型可以用于识别吸附位点。

    arXiv:2403.12659v1 Announce Type: cross  Abstract: The ability to efficiently predict adsorption properties of zeolites can be of large benefit in accelerating the design process of novel materials. The existing configuration space for these materials is wide, while existing molecular simulation methods are computationally expensive. In this work, we propose a model which is 4 to 5 orders of magnitude faster at adsorption properties compared to molecular simulations. To validate the model, we generated datasets containing various aluminium configurations for the MOR, MFI, RHO and ITW zeolites along with their heat of adsorptions and Henry coefficients for CO$_2$, obtained from Monte Carlo simulations. The predictions obtained from the Machine Learning model are in agreement with the values obtained from the Monte Carlo simulations, confirming that the model can be used for property prediction. Furthermore, we show that the model can be used for identifying adsorption sites. Finally, we
    
[^133]: ROUTERBENCH：用于多LLM路由系统的基准测试

    ROUTERBENCH: A Benchmark for Multi-LLM Routing System

    [https://arxiv.org/abs/2403.12031](https://arxiv.org/abs/2403.12031)

    提出了ROUTERBENCH，一个用于评估LLM路由系统性能的基准测试框架，包括超过405k推理结果的数据集，以支持路由策略的开发。

    

    随着大型语言模型（LLMs）的应用范围不断扩大，对有效的服务解决方案的需求变得日益关键。尽管LLMs具有多样性，但没有单一模型可以最优地解决所有任务和应用，特别是在平衡性能和成本之间。为了弥补这一限制，发展了LLM路由系统，这些系统结合了各种模型的优势，以克服单个LLMs的约束。然而，缺乏用于评估LLM路由器性能的标准化基准测试，阻碍了这一领域的进展。为弥合这一差距，我们提出了ROUTERBENCH，这是一个新颖的评估框架，旨在系统评估LLM路由系统的功效，以及一个包括来自代表性LLMs的超过405k推理结果的全面数据集，以支持路由策略的开发。我们进一步提出了一个LLM路由的理论框架，以及...

    arXiv:2403.12031v1 Announce Type: cross  Abstract: As the range of applications for Large Language Models (LLMs) continues to grow, the demand for effective serving solutions becomes increasingly critical. Despite the versatility of LLMs, no single model can optimally address all tasks and applications, particularly when balancing performance with cost. This limitation has led to the development of LLM routing systems, which combine the strengths of various models to overcome the constraints of individual LLMs. Yet, the absence of a standardized benchmark for evaluating the performance of LLM routers hinders progress in this area. To bridge this gap, we present ROUTERBENCH, a novel evaluation framework designed to systematically assess the efficacy of LLM routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative LLMs to support the development of routing strategies. We further propose a theoretical framework for LLM routing, and del
    
[^134]: 非光滑隐式微分：确定性和随机收敛速率

    Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates

    [https://arxiv.org/abs/2403.11687](https://arxiv.org/abs/2403.11687)

    在非光滑设置下，提出了用于计算具有内映射的外映射固定点的隐式导数的新方法NSID，并提供了确定性情况下迭代微分（ITD）和近似隐式微分（AID）的改进线性收敛速率。

    

    我们研究了有效计算参数化不可微收缩映射固定点导数的问题。这个问题在机器学习中有广泛的应用，包括超参数优化、元学习和数据污染攻击。我们分析了两种流行的方法：迭代微分（ITD）和近似隐式微分（AID）。在非光滑设置中的一个关键挑战是链规则不再成立。在Bolte等人（2022）最近的工作基础上，他们证明了不可微分ITD的线性收敛，我们提供了确定性情况下ITD和AID的改进线性收敛速率。我们进一步介绍了NSID，一种新的方法，用于在固定点被定义为只通过随机无偏估计器访问的外映射和内映射的组合时计算隐式导数。我们建立了该方法的收敛速率。

    arXiv:2403.11687v1 Announce Type: cross  Abstract: We study the problem of efficiently computing the derivative of the fixed-point of a parametric non-differentiable contraction map. This problem has wide applications in machine learning, including hyperparameter optimization, meta-learning and data poisoning attacks. We analyze two popular approaches: iterative differentiation (ITD) and approximate implicit differentiation (AID). A key challenge behind the nonsmooth setting is that the chain rule does not hold anymore. Building upon the recent work by Bolte et al. (2022), who proved the linear convergence of non-differentiable ITD, we provide refined linear convergence rates for both ITD and AID in the deterministic case. We further introduce NSID, a new method to compute the implicit derivative when the fixed point is defined as the composition of an outer map and an inner map which is accessible only through a stochastic unbiased estimator. We establish rates for the convergence of 
    
[^135]: 双通道多重图神经网络用于推荐

    Dual-Channel Multiplex Graph Neural Networks for Recommendation

    [https://arxiv.org/abs/2403.11624](https://arxiv.org/abs/2403.11624)

    该研究提出了一种名为双通道多重图神经网络（DCMGNN）的新型推荐框架，能够有效解决现有推荐方法中存在的多通路关系行为模式建模和对目标关系影响忽略的问题。

    

    高效的推荐系统在准确捕捉反映个人偏好的用户和项目属性方面发挥着至关重要的作用。一些现有的推荐技术已经开始将重点转向在真实世界的推荐场景中对用户和项目之间的各种类型交互关系进行建模，例如在线购物平台上的点击、标记收藏和购买。然而，这些方法仍然面临两个重要的缺点：(1) 不足的建模和利用用户和项目之间多通路关系形成的各种行为模式对表示学习的影响，以及(2) 忽略了行为模式中不同关系对推荐系统场景中目标关系的影响。在本研究中，我们介绍了一种新颖的推荐框架，即双通道多重图神经网络（DCMGNN），该框架解决了上述挑战。

    arXiv:2403.11624v1 Announce Type: cross  Abstract: Efficient recommender systems play a crucial role in accurately capturing user and item attributes that mirror individual preferences. Some existing recommendation techniques have started to shift their focus towards modeling various types of interaction relations between users and items in real-world recommendation scenarios, such as clicks, marking favorites, and purchases on online shopping platforms. Nevertheless, these approaches still grapple with two significant shortcomings: (1) Insufficient modeling and exploitation of the impact of various behavior patterns formed by multiplex relations between users and items on representation learning, and (2) ignoring the effect of different relations in the behavior patterns on the target relation in recommender system scenarios. In this study, we introduce a novel recommendation framework, Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the aforementioned challenges
    
[^136]: 使用EfficientNet和注意力机制从CT扫描中检测Covid-19

    Covid-19 detection from CT scans using EfficientNet and Attention mechanism

    [https://arxiv.org/abs/2403.11505](https://arxiv.org/abs/2403.11505)

    开发了一个基于深度学习模型的管道，结合EfficientNet和注意力机制，用于从肺部CT扫描图像中检测COVID-19，并在竞赛数据集验证集上表现优异

    

    通过深度学习模型，我们开发了一个基于管道的COVID-19检测方法，用于从肺部CT扫描图像中检测COVID-19。我们的方法结合了EfficientNet和注意力机制，并取得了比去年竞赛数据集验证集上的其他团队更好的结果。

    arXiv:2403.11505v1 Announce Type: cross  Abstract: Manual diagnosis and analysis of COVID-19 through the examination of lung Computed Tomography (CT) scan images by physicians tends to result in inefficiency, especially with high patient volumes and numerous images per patient. We address the need for automation by developing a deep learning model-based pipeline for COVID-19 detection from CT scan images of the lungs. The Domain adaptation, Explainability, and Fairness in AI for Medical Image Analysis Workshop and COVID-19 Diagnosis Competition (DEF-AI-MIA COV19D) provides an opportunity to assess our designed pipeline for COVID-19 detection from CT scan images. The proposed pipeline incorporates EfficientNet with an Attention mechanism with a pre-processing step. Our pipeline outperforms last year's teams on the validation set of the competition dataset.
    
[^137]: FluoroSAM: 用于X光图像分割的语言对齐基础模型

    FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation

    [https://arxiv.org/abs/2403.08059](https://arxiv.org/abs/2403.08059)

    FluoroSAM是用于X光图像的分割的语言对齐基础模型，提供了一种在X光成像领域具有广泛适用性的自动图像分析工具。

    

    自动X光图像分割将加速诊断和介入精准医学领域的研究和发展。先前的研究已经提出了适用于解决特定图像分析问题的特定任务模型，但这些模型的效用受限于特定任务领域，要拓展到更广泛的应用则需要额外的数据、标签和重新训练工作。最近，基础模型（FMs） - 训练在大量高度变化数据上的机器学习模型因此使得广泛适用性成为可能 - 已经成为自动图像分析的有希望的工具。现有的用于医学图像分析的FMs聚焦于对象被明显可见边界清晰定义的场景和模式，如内窥镜手术工具分割。相比之下，X光成像通常没有提供这种清晰的边界或结构先验。在X光图像形成期间，复杂的三维

    arXiv:2403.08059v1 Announce Type: cross  Abstract: Automated X-ray image segmentation would accelerate research and development in diagnostic and interventional precision medicine. Prior efforts have contributed task-specific models capable of solving specific image analysis problems, but the utility of these models is restricted to their particular task domain, and expanding to broader use requires additional data, labels, and retraining efforts. Recently, foundation models (FMs) -- machine learning models trained on large amounts of highly variable data thus enabling broad applicability -- have emerged as promising tools for automated image analysis. Existing FMs for medical image analysis focus on scenarios and modalities where objects are clearly defined by visually apparent boundaries, such as surgical tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally offer such clearly delineated boundaries or structure priors. During X-ray image formation, complex 3D
    
[^138]: CAS: 一种具有FCR控制的在线选择性符合预测的通用算法

    CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control

    [https://arxiv.org/abs/2403.07728](https://arxiv.org/abs/2403.07728)

    CAS框架允许在在线选择性预测中控制FCR，通过自适应选择和校准集构造输出符合预测区间

    

    我们研究了在线方式下后选择预测推断的问题。为了避免将资源耗费在不重要的单位上，在报告其预测区间之前对当前个体进行初步选择在在线预测任务中是常见且有意义的。由于在线选择导致所选预测区间中存在时间多重性，因此控制实时误覆盖陈述率（FCR）来测量平均误覆盖误差是重要的。我们开发了一个名为CAS（适应性选择后校准）的通用框架，可以包裹任何预测模型和在线选择规则，以输出后选择的预测区间。如果选择了当前个体，我们首先对历史数据进行自适应选择来构建校准集，然后为未观察到的标签输出符合预测区间。我们为校准集提供了可行的构造方式

    arXiv:2403.07728v1 Announce Type: cross  Abstract: We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) to measure the averaged miscoverage error. We develop a general framework named CAS (Calibration after Adaptive Selection) that can wrap around any prediction model and online selection rule to output post-selection prediction intervals. If the current individual is selected, we first perform an adaptive selection on historical data to construct a calibration set, then output a conformal prediction interval for the unobserved label. We provide tractable constructions for the calibration set for 
    
[^139]: 小型语言模型能成为顺序推荐系统的良好推理者吗？

    Can Small Language Models be Good Reasoners for Sequential Recommendation?

    [https://arxiv.org/abs/2403.04260](https://arxiv.org/abs/2403.04260)

    提出了逐步知识提取框架（SLIM），为顺序推荐系统解决了大型语言模型（LLMs）高资源需求的难题，使其能以资源高效的方式享受LLMs的出色推理能力。

    

    大型语言模型（LLMs）由于其出色的语言理解和生成能力，为顺序推荐开拓了新的领域。然而，要成功实现由LLMs赋能的顺序推荐还有许多挑战需要解决。首先，用户行为模式通常复杂，仅仅依靠LLMs的一步推理可能会导致错误或与任务无关的响应。其次，LLMs（例如ChatGPT-175B）极高的资源需求是难以承受且在实际顺序推荐系统中不切实际的。本文提出了一个新颖的逐步知识提取框架用于推荐（SLIM），为顺序推荐器以“瘦”（即资源高效）的方式享受LLMs出色的推理能力铺平了一条有前途的道路。我们引入基于用户行为序列的CoT提示来实现更好的推荐。

    arXiv:2403.04260v1 Announce Type: cross  Abstract: Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larg
    
[^140]: OpenGraph: 迈向开放图基础模型

    OpenGraph: Towards Open Graph Foundation Models

    [https://arxiv.org/abs/2403.01121](https://arxiv.org/abs/2403.01121)

    该论文旨在通过开发一个通用图基础模型，以解决现有图神经网络在泛化到与训练数据显著不同的未见图数据时遇到的困难。

    

    arXiv:2403.01121v1 公告类型: 跨交互   摘要: 图学习已成为解释和利用各领域的关系数据的不可或缺部分，从推荐系统到社交网络分析。在这种背景下，各种GNN已经成为编码图的结构信息的有希望的方法论，通过有效地捕捉图的潜在结构，这些GNN已经展示出在增强图学习任务性能方面的巨大潜力，例如链接预测和节点分类。然而，尽管取得了成功，一个显著的挑战仍然存在: 这些先进方法通常在将显著不同于训练实例的未见图数据泛化时遇到困难。在这项工作中，我们的目标是通过开发一个通用图基础模型来推进图学习范式。该模型旨在理解多样图数据中存在的复杂拓扑模式，使其在零-shot情况下表现出色。

    arXiv:2403.01121v1 Announce Type: cross  Abstract: Graph learning has become indispensable for interpreting and harnessing relational data in diverse fields, ranging from recommendation systems to social network analysis. In this context, a variety of GNNs have emerged as promising methodologies for encoding the structural information of graphs. By effectively capturing the graph's underlying structure, these GNNs have shown great potential in enhancing performance in graph learning tasks, such as link prediction and node classification. However, despite their successes, a significant challenge persists: these advanced methods often face difficulties in generalizing to unseen graph data that significantly differs from the training instances. In this work, our aim is to advance the graph learning paradigm by developing a general graph foundation model. This model is designed to understand the complex topological patterns present in diverse graph data, enabling it to excel in zero-shot g
    
[^141]: 深度强化学习：一个凸优化方法

    Deep Reinforcement Learning: A Convex Optimization Approach

    [https://arxiv.org/abs/2402.19212](https://arxiv.org/abs/2402.19212)

    本文提出了一种深度强化学习的凸优化方法，通过每集使用凸优化来训练神经网络近似最优$Q$-函数，确保收敛参数可以无限接近最优参数。

    

    在本文中，我们考虑了具有连续状态和动作空间的非线性系统的强化学习。我们提出了一种分集学习算法，其中我们在每个集中使用凸优化来找到最优$Q$-函数的两层神经网络近似。凸优化方法确保每个集合中计算的权重是最优的，关于当前集合的采样状态和动作。对于稳定的非线性系统，我们证明了算法收敛，并且经过训练的神经网络的收敛参数可以与最优神经网络参数无限接近。特别是，如果正则化参数为$\rho$，时间长度为$T$，那么经过训练的神经网络的参数收敛到$w$，其中$w$与最优参数$w^\star$之间的距离受到$\mathcal{O}(\rho T^{-1})$的限制。

    arXiv:2402.19212v1 Announce Type: cross  Abstract: In this paper, we consider reinforcement learning of nonlinear systems with continuous state and action spaces. We present an episodic learning algorithm, where we for each episode use convex optimization to find a two-layer neural network approximation of the optimal $Q$-function. The convex optimization approach guarantees that the weights calculated at each episode are optimal, with respect to the given sampled states and actions of the current episode. For stable nonlinear systems, we show that the algorithm converges and that the converging parameters of the trained neural network can be made arbitrarily close to the optimal neural network parameters. In particular, if the regularization parameter is $\rho$ and the time horizon is $T$, then the parameters of the trained neural network converge to $w$, where the distance between $w$ from the optimal parameters $w^\star$ is bounded by $\mathcal{O}(\rho T^{-1})$. That is, when the nu
    
[^142]: 通知元学习

    Informed Meta-Learning

    [https://arxiv.org/abs/2402.16105](https://arxiv.org/abs/2402.16105)

    该研究提出了通知元学习这一新范式，旨在通过人类和机器之间的跨任务知识共享，提高数据效率和抵御观测噪声。

    

    在真实应用中盛行的嘈杂和低数据情况下，机器学习中一个突出的挑战在于有效地融合促进数据效率和稳健性的归纳偏差。元学习和通知机器学习是两种将先验知识纳入机器学习流程的方法。前者依赖于一种纯数据驱动的先验来源，而后者受专家知识的形式化表示引导。本文介绍了一种新颖的混合范式，通知元学习，旨在实现人类和机器之间跨任务知识共享的互补性。我们建立了通知元学习的基本组成部分，并提出了这一框架的具体实例--通知神经过程。通过一系列说明性和更大规模的实验，我们展示了通知元学习在提高数据效率和抵御观测噪声方面的潜在优势。

    arXiv:2402.16105v1 Announce Type: new  Abstract: In noisy and low-data regimes prevalent in real-world applications, an outstanding challenge of machine learning lies in effectively incorporating inductive biases that promote data efficiency and robustness. Meta-learning and informed ML stand out as two approaches for incorporating prior knowledge into the ML pipeline. While the former relies on a purely data-driven source of priors, the latter is guided by a formal representation of expert knowledge. This paper introduces a novel hybrid paradigm, informed meta-learning, seeking complementarity in cross-task knowledge sharing of humans and machines. We establish the foundational components of informed meta-learning and present a concrete instantiation of this framework--the Informed Neural Process. Through a series of illustrative and larger-scale experiments, we demonstrate the potential benefits of informed meta-learning in improving data efficiency and robustness to observational no
    
[^143]: 使用Equilibrium K-Means进行不平衡数据聚类

    Imbalanced Data Clustering using Equilibrium K-Means

    [https://arxiv.org/abs/2402.14490](https://arxiv.org/abs/2402.14490)

    Equilibrium K-Means（EKM）是一种新颖且简单的K均值类型算法，通过减少聚类中心在大类簇中心聚集的倾向，显著改善了不平衡数据的聚类结果。

    

    不平衡数据指的是数据点在不同类别之间分布不均衡，这给传统的硬聚类算法和模糊聚类算法（如硬K均值（HKM，或者Lloyd算法）和模糊K均值（FKM，或者Bezdek算法））带来了挑战。本文介绍了一种新颖且简单的K均值类型算法——Equilibrium K-Means（EKM），它在两个步骤之间交替进行，显著改善了不平衡数据的聚类结果，减少了聚类中心向大类簇中心聚集的倾向。我们还提出了对HKM、FKM和EKM的统一视角，表明它们本质上是具有明确关系的牛顿方法的梯度下降算法。EKM具有与FKM相同的时间和空间复杂度，但对其成员定义提供了更清晰的物理意义。我们在两个合成数据集和十个真实数据集上展示了EKM的性能，并将其与各种聚类算法进行了比较。

    arXiv:2402.14490v1 Announce Type: new  Abstract: Imbalanced data, characterized by an unequal distribution of data points across different clusters, poses a challenge for traditional hard and fuzzy clustering algorithms, such as hard K-means (HKM, or Lloyd's algorithm) and fuzzy K-means (FKM, or Bezdek's algorithm). This paper introduces equilibrium K-means (EKM), a novel and simple K-means-type algorithm that alternates between just two steps, yielding significantly improved clustering results for imbalanced data by reducing the tendency of centroids to crowd together in the center of large clusters. We also present a unifying perspective for HKM, FKM, and EKM, showing they are essentially gradient descent algorithms with an explicit relationship to Newton's method. EKM has the same time and space complexity as FKM but offers a clearer physical meaning for its membership definition. We illustrate the performance of EKM on two synthetic and ten real datasets, comparing it to various cl
    
[^144]: HU在SemEval-2024任务8A中的表现：对比学习能否学习嵌入以检测机器生成的文本？

    HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?

    [https://arxiv.org/abs/2402.11815](https://arxiv.org/abs/2402.11815)

    提出了一种基于对比学习的单一模型，用较少的参数实现与基线相当的机器生成文本检测性能

    

    这篇论文描述了我们为SemEval-2024任务8“多生成器、多领域和多语言黑匣子机器生成文本检测”开发的系统。由于大型语言模型（LLM）在虚假文本生成、网络钓鱼、考试作弊甚至抄袭版权材料中的使用，机器生成文本一直是主要关注的问题之一。许多系统已经被开发用于检测机器生成的文本。然而，这些系统中的大部分依赖于文本生成模型，这是一个在实际场景中不切实际的限制，因为通常不可能知道用户用于文本生成的具体模型。在这项工作中，我们提出了基于对比学习的单一模型，其使用基线参数的大约40%（149M比355M），但在测试数据集上表现出了可比的性能（在137个参与者中排名第21）。我们的关键发现是，即使没有多个模型的集成，

    arXiv:2402.11815v1 Announce Type: cross  Abstract: This paper describes our system developed for SemEval-2024 Task 8, "Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection." Machine-generated texts have been one of the main concerns due to the use of large language models (LLM) in fake text generation, phishing, cheating in exams, or even plagiarizing copyright materials. A lot of systems have been developed to detect machine-generated text. Nonetheless, the majority of these systems rely on the text-generating model, a limitation that is impractical in real-world scenarios, as it's often impossible to know which specific model the user has used for text generation. In this work, we propose a single model based on contrastive learning, which uses ~40% of the baseline's parameters (149M vs. 355M) but shows a comparable performance on the test dataset (21st out of 137 participants). Our key finding is that even without an ensemble of multiple models, a
    
[^145]: Brant-2：脑信号基础模型

    Brant-2: Foundation Model for Brain Signals

    [https://arxiv.org/abs/2402.10251](https://arxiv.org/abs/2402.10251)

    Brant-2是脑信号领域最大的基础模型，相比于Brant，它不仅对数据变化和建模尺度具有稳健性，还能适用于更广泛范围的脑神经数据。

    

    基础模型受益于在大量未标记数据上进行预训练，并且在少量标记数据的情况下能够在各种应用中表现出色。这种模型在分析脑信号方面特别有效，因为这一领域涵盖了众多应用场景，并且进行大规模注释是成本高昂的。在这项工作中，我们提出了脑信号领域最大的基础模型，Brant-2。与用于颅内神经信号的基础模型Brant相比，Brant-2不仅对数据变化和建模尺度表现出稳健性，而且可以应用于更广泛范围的脑神经数据。通过在大量任务上进行实验，我们展示了Brant-2对脑信号中各种应用场景的适应性。进一步分析揭示了Brant-2的可扩展性，验证了每个组件的有效性，并展示了我们模型保持的能力。

    arXiv:2402.10251v1 Announce Type: cross  Abstract: Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintai
    
[^146]: PRDP：大规模扩散模型的近端奖励差异预测用于奖励微调

    PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models

    [https://arxiv.org/abs/2402.08714](https://arxiv.org/abs/2402.08714)

    本研究提出了PRDP方法，通过近端奖励差异预测实现了稳定的黑盒奖励微调扩散模型，能够在大规模提示数据集上进行训练，并且具有更好的训练稳定性。

    

    奖励微调已成为将基础模型与下游目标对齐的一种有前途的方法。在语言领域，使用强化学习（RL）来最大化反映人类偏好的奖励已取得了显著的成功。然而，在视觉领域，现有的基于RL的奖励微调方法在大规模训练中存在不稳定性，使它们无法推广到复杂的、未知的提示。在本文中，我们提出了近端奖励差异预测（PRDP），首次在超过100K个提示的大规模提示数据集上实现了稳定的黑盒奖励微调扩散模型。我们的主要创新是奖励差异预测（RDP）目标，该目标与RL目标具有相同的最优解，同时享受更好的训练稳定性。

    arXiv:2402.08714v1 Announce Type: cross Abstract: Reward finetuning has emerged as a promising approach to aligning foundation models with downstream objectives. Remarkable success has been achieved in the language domain by using reinforcement learning (RL) to maximize rewards that reflect human preference. However, in the vision domain, existing RL-based reward finetuning methods are limited by their instability in large-scale training, rendering them incapable of generalizing to complex, unseen prompts. In this paper, we propose Proximal Reward Difference Prediction (PRDP), enabling stable black-box reward finetuning for diffusion models for the first time on large-scale prompt datasets with over 100K prompts. Our key innovation is the Reward Difference Prediction (RDP) objective that has the same optimal solution as the RL objective while enjoying better training stability. Specifically, the RDP objective is a supervised regression objective that tasks the diffusion model with pred
    
[^147]: 重新构想指挥与控制

    Re-Envisioning Command and Control

    [https://arxiv.org/abs/2402.07946](https://arxiv.org/abs/2402.07946)

    重新构想的论文提出了未来指挥与控制（C2）决策需要面对更复杂和挑战性的环境，因此提出了基于人工智能系统与人类强有力伙伴关系的未来C2的愿景。这个愿景的核心是优化C2操作流程，保持协同努力，发展自适应的集体知识系统。

    

    未来的战争将要求在更复杂、快节奏、不结构化和极具挑战性的环境中进行指挥与控制（C2）决策。C2将因被拒绝、退化、间歇和有限的通信以及需要考虑到多个作战领域中的许多数据流而变得更加复杂。然而，当前的C2实践——源自工业时代而非新兴的智能时代——是线性的且耗时。而且，这些方法可能无法在未来战场上与对手保持优势。为了应对这些挑战，我们提出了一种基于人工智能（AI）系统与人类之间强有力伙伴关系的未来C2愿景。这个未来愿景体现在三个运营影响上：优化C2操作流程，保持协同努力，以及发展自适应的集体知识系统。本文阐述了所设想的未来指挥与控制的愿景。

    Future warfare will require Command and Control (C2) decision-making to occur in more complex, fast-paced, ill-structured, and demanding conditions. C2 will be further complicated by operational challenges such as Denied, Degraded, Intermittent, and Limited (DDIL) communications and the need to account for many data streams, potentially across multiple domains of operation. Yet, current C2 practices -- which stem from the industrial era rather than the emerging intelligence era -- are linear and time-consuming. Critically, these approaches may fail to maintain overmatch against adversaries on the future battlefield. To address these challenges, we propose a vision for future C2 based on robust partnerships between humans and artificial intelligence (AI) systems. This future vision is encapsulated in three operational impacts: streamlining the C2 operations process, maintaining unity of effort, and developing adaptive collective knowledge systems. This paper illustrates the envisaged fu
    
[^148]: 可扩展互动式机器学习用于未来指挥与控制

    Scalable Interactive Machine Learning for Future Command and Control

    [https://arxiv.org/abs/2402.06501](https://arxiv.org/abs/2402.06501)

    未来战争将需要指挥与控制（C2）人员在复杂且潜在模糊的情况下以更短的时间内做出决策。本论文通过利用互动式机器学习方法，结合人工智能和人类智能，以提高C2运作的适应性和效率。

    

    未来战争将需要指挥与控制（C2）人员在复杂且潜在模糊的情况下以更短的时间内做出决策。鉴于需要强大的决策过程和决策支持工具，人工智能和人类智能的集成具有革命性地改变C2运作流程的潜力，以确保在快速变化的操作环境中的适应性和效率。我们提议利用最近在互动式机器学习方面取得的突破，人类可以与机器学习算法合作以指导机器学习算法的行为。本文确定了目前科技发展中存在的几个差距，未来的工作应该解决这些差距，以扩展这些方法在复杂的C2环境中发挥作用。特别是，我们描述了三个研究重点领域，共同旨在实现可扩展的互动式机器学习（SIML）：1）开发人工智能与人类交互算法以实现协同规划。

    Future warfare will require Command and Control (C2) personnel to make decisions at shrinking timescales in complex and potentially ill-defined situations. Given the need for robust decision-making processes and decision-support tools, integration of artificial and human intelligence holds the potential to revolutionize the C2 operations process to ensure adaptability and efficiency in rapidly changing operational environments. We propose to leverage recent promising breakthroughs in interactive machine learning, in which humans can cooperate with machine learning algorithms to guide machine learning algorithm behavior. This paper identifies several gaps in state-of-the-art science and technology that future work should address to extend these approaches to function in complex C2 contexts. In particular, we describe three research focus areas that together, aim to enable scalable interactive machine learning (SIML): 1) developing human-AI interaction algorithms to enable planning in co
    
[^149]: TopoNav：节约奖励环境中高效探索的拓扑导航

    TopoNav: Topological Navigation for Efficient Exploration in Sparse Reward Environments

    [https://arxiv.org/abs/2402.04061](https://arxiv.org/abs/2402.04061)

    TopoNav是一种拓扑导航框架，它通过主动拓扑映射、内部奖励机制和层次化目标优先级的组合来实现在稀疏奖励环境中高效探索。

    

    自动化机器人在未知区域的探索面临着一个重大挑战——在没有先前地图和有限外部反馈的情况下有效导航。在稀疏奖励环境中，这个挑战更加严峻，传统的探索技术往往失败。本文介绍了TopoNav，一种全新的框架，使机器人能够克服这些限制，实现高效、适应性强且目标导向的探索。TopoNav的基本构建模块是主动拓扑映射、内部奖励机制和层次化目标优先级。在探索过程中，TopoNav构建了动态拓扑地图，捕获关键位置和路径。它利用内部奖励来指导机器人朝着地图中指定的子目标前进，促进在稀疏奖励环境中的结构化探索。为了确保高效导航，TopoNav采用了分层目标驱动的主动拓扑框架，使机器人能够优先考虑最紧急的目标。

    Autonomous robots exploring unknown areas face a significant challenge -- navigating effectively without prior maps and with limited external feedback. This challenge intensifies in sparse reward environments, where traditional exploration techniques often fail. In this paper, we introduce TopoNav, a novel framework that empowers robots to overcome these constraints and achieve efficient, adaptable, and goal-oriented exploration. TopoNav's fundamental building blocks are active topological mapping, intrinsic reward mechanisms, and hierarchical objective prioritization. Throughout its exploration, TopoNav constructs a dynamic topological map that captures key locations and pathways. It utilizes intrinsic rewards to guide the robot towards designated sub-goals within this map, fostering structured exploration even in sparse reward settings. To ensure efficient navigation, TopoNav employs the Hierarchical Objective-Driven Active Topologies framework, enabling the robot to prioritize immed
    
[^150]: COA-GPT：用于军事行动中加速行动方案开发的生成式预训练变压器

    COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations

    [https://arxiv.org/abs/2402.01786](https://arxiv.org/abs/2402.01786)

    COA-GPT是一种利用大型语言模型快速高效生成有效行动方案的算法，它融合了军事学说和领域专业知识，并在军事游戏中的实验中展示了其快速生成战略合理COAs的优势。

    

    军事行动中行动方案（COAs）的开发传统上是一个耗时且复杂的过程。针对这一挑战，本研究介绍了COA-GPT，一种利用大型语言模型（LLMs）快速高效生成有效COAs的新算法。COA-GPT通过上下文学习将军事学说和领域专业知识融入到LLMs中，允许指挥官输入任务信息（包括文本和图像格式），并获得与战略对齐的COAs以供审查和批准。独特的是，COA-GPT不仅加速了COA的开发，在几秒钟内生成初始COAs，还能根据指挥官的反馈实时精细化改进。本研究在《星际争霸II》游戏的军事相关场景中评估了COA-GPT，将其性能与最先进的强化学习算法进行了比较。我们的结果表明COA-GPT在更快生成战略合理的COAs方面具有优势。

    The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, 
    
[^151]: HQ-VAE：具有变分贝叶斯的分层离散表示学习

    HQ-VAE: Hierarchical Discrete Representation Learning with Variational Bayes

    [https://arxiv.org/abs/2401.00365](https://arxiv.org/abs/2401.00365)

    HQ-VAE提出了一种统一框架，利用变分贝叶斯在分层结构中随机学习离散表示，解决了传统VQ-VAE中的码书/层坍塌问题。

    

    向量量化（VQ）是一种确定性学习具有离散码书表示的特征的技术。通常通过变分自动编码模型 VQ-VAE 来执行，可以进一步扩展到分层结构以进行高保真重建。然而，VQ-VAE 的这种分层扩展经常受到码书/层坍塌问题的困扰，其中码书未被有效地用来表达数据，从而降低重建精度。为了缓解这个问题，我们提出了一个新颖的统一框架，在变分贝叶斯框架的基础上随机学习分层离散表示，称为分层量化变分自动编码器（HQ-VAE）。

    arXiv:2401.00365v2 Announce Type: replace-cross  Abstract: Vector quantization (VQ) is a technique to deterministically learn features with discrete codebook representations. It is commonly performed with a variational autoencoding model, VQ-VAE, which can be further extended to hierarchical structures for making high-fidelity reconstructions. However, such hierarchical extensions of VQ-VAE often suffer from the codebook/layer collapse issue, where the codebook is not efficiently used to express the data, and hence degrades reconstruction accuracy. To mitigate this problem, we propose a novel unified framework to stochastically learn hierarchical discrete representation on the basis of the variational Bayes framework, called hierarchically quantized variational autoencoder (HQ-VAE). HQ-VAE naturally generalizes the hierarchical variants of VQ-VAE, such as VQ-VAE-2 and residual-quantized VAE (RQ-VAE), and provides them with a Bayesian training scheme. Our comprehensive experiments on im
    
[^152]: SkillDiffuser: 通过技能抽象在基于扩散的任务执行中实现可解释的分层规划

    SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution

    [https://arxiv.org/abs/2312.11598](https://arxiv.org/abs/2312.11598)

    SkillDiffuser通过将可解释的技能学习与条件扩散规划相结合，实现了在高层指令下生成连贯轨迹的分层规划。

    

    扩散模型展示了在机器人轨迹规划方面的强大潜力。然而，从高层指令生成连贯的轨迹仍然具有挑战性，特别是对于需要多个顺序技能的长距离组合任务。我们提出了SkillDiffuser，这是一个端到端的分层规划框架，将可解释的技能学习与条件扩散规划相结合，以解决这一问题。在较高层次，技能抽象模块从视觉观察和语言指令中学习离散的、人类可理解的技能表示。然后，这些学习到的技能嵌入被用来条件化扩散模型，生成与技能对齐的定制潜在轨迹。这允许生成符合可学习技能的多样状态轨迹。通过将技能学习与条件轨迹生成相结合，SkillDiffuser产生连贯的行为

    arXiv:2312.11598v2 Announce Type: replace-cross  Abstract: Diffusion models have demonstrated strong potential for robotic trajectory planning. However, generating coherent trajectories from high-level instructions remains challenging, especially for long-range composition tasks requiring multiple sequential skills. We propose SkillDiffuser, an end-to-end hierarchical planning framework integrating interpretable skill learning with conditional diffusion planning to address this problem. At the higher level, the skill abstraction module learns discrete, human-understandable skill representations from visual observations and language instructions. These learned skill embeddings are then used to condition the diffusion model to generate customized latent trajectories aligned with the skills. This allows generating diverse state trajectories that adhere to the learnable skills. By integrating skill learning with conditional trajectory generation, SkillDiffuser produces coherent behavior fo
    
[^153]: 相似的实体是否具有相似的嵌入？

    Do Similar Entities have Similar Embeddings?

    [https://arxiv.org/abs/2312.10370](https://arxiv.org/abs/2312.10370)

    本文挑战了实体相似性在图中在嵌入空间中自然反映的主流假设，通过进行广泛的实验来衡量这种关系。

    

    为了进行链接预测而开发的知识图嵌入模型（KGEMs）学习知识图中实体的向量表示，即嵌入。一个普遍的默认假设是KGE实体相似性假设，即这些KGEMs在它们的嵌入空间中保留图的结构，即将相似的实体放在图中彼此靠近。这种理想的性质使得KGEMs在推荐系统或药物再利用等下游任务中被广泛使用。然而，实体的相似性与嵌入空间中的相似性之间的关系很少被正式评估。通常，KGEMs是基于其唯一的链接预测能力进行评估的，使用类似Hits@K或Mean Rank的排名指标。本文质疑了图中的实体相似性在嵌入空间中天然反映这一流行假设。因此，我们进行了大量实验来衡量

    arXiv:2312.10370v2 Announce Type: replace  Abstract: Knowledge graph embedding models (KGEMs) developed for link prediction learn vector representations for entities in a knowledge graph, known as embeddings. A common tacit assumption is the KGE entity similarity assumption, which states that these KGEMs retain the graph's structure within their embedding space, \textit{i.e.}, position similar entities within the graph close to one another. This desirable property make KGEMs widely used in downstream tasks such as recommender systems or drug repurposing. Yet, the relation of entity similarity and similarity in the embedding space has rarely been formally evaluated. Typically, KGEMs are assessed based on their sole link prediction capabilities, using ranked-based metrics such as Hits@K or Mean Rank. This paper challenges the prevailing assumption that entity similarity in the graph is inherently mirrored in the embedding space. Therefore, we conduct extensive experiments to measure the 
    
[^154]: 开放数据表：面向开放数据集和负责任人工智能评估的机器可读文档

    Open Datasheets: Machine-readable Documentation for Open Datasets and Responsible AI Assessments

    [https://arxiv.org/abs/2312.06153](https://arxiv.org/abs/2312.06153)

    本文介绍了一种面向开放数据集和负责任人工智能评估的机器可读文档框架，旨在提高数据集的可理解性和可用性，简化数据集评估过程，促进更可靠的数据应用，从而培育更负责任和可信赖的人工智能系统。

    

    本文介绍了一种无代码的、面向开放数据集的机器可读文档框架，重点关注负责任人工智能（RAI）考虑因素。该框架旨在提高开放数据集的可理解性和可用性，促进更容易地发现和使用数据集，更好地理解内容和上下文，以及评估数据集的质量和准确性。所提出的框架旨在简化数据集的评估，帮助研究人员、数据科学家和其他开放数据用户快速识别符合其需求和组织政策或法规的数据集。论文还讨论了框架的实施，并提出了最大化其潜力的建议。预期该框架将增强在研究和决策中使用的数据的质量和可靠性，促进更负责任、可信赖的人工智能系统的发展。

    arXiv:2312.06153v2 Announce Type: replace-cross  Abstract: This paper introduces a no-code, machine-readable documentation framework for open datasets, with a focus on responsible AI (RAI) considerations. The framework aims to improve comprehensibility, and usability of open datasets, facilitating easier discovery and use, better understanding of content and context, and evaluation of dataset quality and accuracy. The proposed framework is designed to streamline the evaluation of datasets, helping researchers, data scientists, and other open data users quickly identify datasets that meet their needs and organizational policies or regulations. The paper also discusses the implementation of the framework and provides recommendations to maximize its potential. The framework is expected to enhance the quality and reliability of data used in research and decision-making, fostering the development of more responsible and trustworthy AI systems.
    
[^155]: MMM：生成式遮蔽运动模型

    MMM: Generative Masked Motion Model

    [https://arxiv.org/abs/2312.03596](https://arxiv.org/abs/2312.03596)

    MMM 提出了一种基于遮蔽运动模型的新颖运动生成范式，通过运动标记器和条件遮蔽运动变换器，在实时性能、高保真度和运动可编辑性之间取得平衡。

    

    最近在使用扩散和自回归模型进行文本到运动生成方面取得了一些进展，显示出了良好的结果。然而，这些模型往往在实时性能、高保真度和运动可编辑性之间存在权衡。为了解决这一问题，我们引入了MMM，一种基于遮蔽运动模型的新颖而简单的运动生成范式。MMM由两个关键组件组成：（1）运动标记器，将3D人体运动转化为潜在空间中的一系列离散标记，以及（2）条件遮蔽运动变换器，学习预测在预先计算的文本标记的条件下随机遮蔽的运动标记。通过在所有方向上关注运动和文本标记，MMM明确地捕获了运动标记之间的固有依赖关系以及运动和文本标记之间的语义映射。在推断过程中，这允许对与fi高度一致的多个运动标记进行并行和迭代解码。

    arXiv:2312.03596v2 Announce Type: replace-cross  Abstract: Recent advances in text-to-motion generation using diffusion and autoregressive models have shown promising results. However, these models often suffer from a trade-off between real-time performance, high fidelity, and motion editability. To address this gap, we introduce MMM, a novel yet simple motion generation paradigm based on Masked Motion Model. MMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into a sequence of discrete tokens in latent space, and (2) a conditional masked motion transformer that learns to predict randomly masked motion tokens, conditioned on the pre-computed text tokens. By attending to motion and text tokens in all directions, MMM explicitly captures inherent dependency among motion tokens and semantic mapping between motion and text tokens. During inference, this allows parallel and iterative decoding of multiple motion tokens that are highly consistent with fi
    
[^156]: HybridNeRF：通过自适应体积表面实现高效神经渲染

    HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces

    [https://arxiv.org/abs/2312.03160](https://arxiv.org/abs/2312.03160)

    HybridNeRF方法将大多数对象呈现为表面，仅对少部分具有挑战性的区域进行体积建模，实现了神经渲染的高效率。

    

    神经辐射场提供了最先进的视图合成质量，但渲染速度较慢。一个原因是它们利用体素渲染，在渲染时需要每个光线进行许多采样（和模型查询）。尽管这种表示灵活且易于优化，但大多数现实世界的对象可以更有效地用表面而不是体积建模，从而每个光线需要更少的采样。这一观察促成了对表面表示的实质性进展，如符号距离函数，但这些可能难以建模半透明和薄结构。我们提出了一种方法，HybridNeRF，通过将大多数对象呈现为表面，同时对（通常）小部分具有挑战性的区域进行体积建模，从而利用这两种表示的优势。我们评估了HybridNeRF在具有挑战性的Eyeful Tower数据集以及其他常用视图合成数据集上的性能。

    arXiv:2312.03160v2 Announce Type: replace-cross  Abstract: Neural radiance fields provide state-of-the-art view synthesis quality but tend to be slow to render. One reason is that they make use of volume rendering, thus requiring many samples (and model queries) per ray at render time. Although this representation is flexible and easy to optimize, most real-world objects can be modeled more efficiently with surfaces instead of volumes, requiring far fewer samples per ray. This observation has spurred considerable progress in surface representations such as signed distance functions, but these may struggle to model semi-opaque and thin structures. We propose a method, HybridNeRF, that leverages the strengths of both representations by rendering most objects as surfaces while modeling the (typically) small fraction of challenging regions volumetrically. We evaluate HybridNeRF against the challenging Eyeful Tower dataset along with other commonly used view synthesis datasets. When compari
    
[^157]: 一个非凸正则化凸稀疏模型的解集几何与正则化路径

    Solution-Set Geometry and Regularization Path of a Nonconvexly Regularized Convex Sparse Model

    [https://arxiv.org/abs/2311.18438](https://arxiv.org/abs/2311.18438)

    sGMC模型作为LASSO的非凸正则化替代品，在保留LASSO模型优势的同时，其解集几何、解唯一性和稀疏性与LASSO模型具有相似且优雅的特性。

    

    广义极小极大凹（GMC）惩罚是一种非凸稀疏正则化器，可以保持正则化最小二乘问题的整体凸性。本文关注GMC模型的一个重要实例，称为缩放GMC（sGMC），并就其解集几何和正则化路径提出各种显著发现。我们的研究表明，虽然sGMC惩罚是LASSO惩罚的非凸扩展（即$\ell_1$范数），但sGMC模型保留了LASSO模型的许多著名特性，因此可以作为LASSO的一个偏差较小的替代品而不会失去其优势。具体而言，对于固定的正则化参数$\lambda$，我们展示了sGMC模型的解集几何、解唯一性和稀疏性可以以一种类似优雅的方式刻画为LASSO模型（参见，例如，Osborne等人2000年，R. J. Tibshirani 2013年）。对于变化的$\lambda$，我们证明了...

    arXiv:2311.18438v2 Announce Type: cross  Abstract: The generalized minimax concave (GMC) penalty is a nonconvex sparse regularizer which can preserve the overall-convexity of the regularized least-squares problem. In this paper, we focus on a significant instance of the GMC model termed scaled GMC (sGMC), and present various notable findings on its solution-set geometry and regularization path. Our investigation indicates that while the sGMC penalty is a nonconvex extension of the LASSO penalty (i.e., the $\ell_1$-norm), the sGMC model preserves many celebrated properties of the LASSO model, hence can serve as a less biased surrogate of LASSO without losing its advantages. Specifically, for a fixed regularization parameter $\lambda$, we show that the solution-set geometry, solution uniqueness and sparseness of the sGMC model can be characterized in a similar elegant way to the LASSO model (see, e.g., Osborne et al. 2000, R. J. Tibshirani 2013). For a varying $\lambda$, we prove that th
    
[^158]: 提示风险控制：大型语言模型负责部署的严格框架

    Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models

    [https://arxiv.org/abs/2311.13628](https://arxiv.org/abs/2311.13628)

    提示风险控制是一个轻量级框架，通过严格的信息风险度量族的上限选取提示，帮助减轻大型语言模型负责部署过程中产生意外糟糕响应的风险。

    

    大型语言模型能力的爆炸式增长引发了对如何最好地提示模型执行特定任务的兴趣浪潮。选择一个基于验证集上平均性能的提示可能很诱人，但这可能导致生成出乎意料的糟糕响应，尤其是对于处境最困难的用户。为了减轻这一可能性，我们提出提示风险控制，这是一个轻量级框架，根据信息风险度量族的严格上限选择提示。我们提供了用于产生多种度量上限的方法，包括衡量最坏情况响应和用户群体生成质量不均衡的量，此外，我们扩展了基础统计界定技术，以适应部署中分布变化可能性的情况。在开放式聊天、医学问题等应用上的实验表明了我们方法的有效性。

    arXiv:2311.13628v2 Announce Type: replace-cross  Abstract: The recent explosion in the capabilities of large language models has led to a wave of interest in how best to prompt a model to perform a given task. While it may be tempting to simply choose a prompt based on average performance on a validation set, this can lead to a deployment where unexpectedly poor responses are generated, especially for the worst-off users. To mitigate this prospect, we propose Prompt Risk Control, a lightweight framework for selecting a prompt based on rigorous upper bounds on families of informative risk measures. We offer methods for producing bounds on a diverse set of metrics, including quantities that measure worst-case responses and disparities in generation quality across the population of users. In addition, we extend the underlying statistical bounding techniques to accommodate the possibility of distribution shifts in deployment. Experiments on applications such as open-ended chat, medical que
    
[^159]: PIE-NeRF: 使用 NeRF 进行基于物理的交互弹性动力学

    PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF

    [https://arxiv.org/abs/2311.13099](https://arxiv.org/abs/2311.13099)

    该研究展示了物理学模拟与 NeRF 结合，无需中间形态代理，通过 Q-GMLS 捕捉非线性动力学和大形变，实现了高质量弹性动力学生成，并适应 NeRF 密度场调整最小二乘核，从而高效合成各种高弹性材料的物理逼真动画。

    

    我们展示了物理学模拟与 NeRF 可以无缝集成，用于生成真实物体的高质量弹性动力学。与现有方法不同，我们以无网格的方式离散化非线性超弹性，避免了中间辅助形态代理物如四面体网格或体素网格的必要性。采用二次广义移动最小二乘（Q-GMLS）来捕捉隐式模型上的非线性动态和大形变。这种无网格集成使复杂和共维度形状的多功能模拟成为可能。我们根据 NeRF 密度场自适应地放置最小二乘核，显著降低非线性模拟的复杂性。因此，可以方便地使用我们的方法以交互速率合成各种高弹性材料的物理逼真动画。有关更多信息，请访问我们的项目页面https://fytalo

    arXiv:2311.13099v2 Announce Type: replace-cross  Abstract: We show that physics-based simulations can be seamlessly integrated with NeRF to generate high-quality elastodynamics of real-world objects. Unlike existing methods, we discretize nonlinear hyperelasticity in a meshless way, obviating the necessity for intermediate auxiliary shape proxies like a tetrahedral mesh or voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed to capture nonlinear dynamics and large deformation on the implicit model. Such meshless integration enables versatile simulations of complex and codimensional shapes. We adaptively place the least-square kernels according to the NeRF density field to significantly reduce the complexity of the nonlinear simulation. As a result, physically realistic animations can be conveniently synthesized using our method for a wide range of hyperelastic materials at an interactive rate. For more information, please visit our project page at https://fytalo
    
[^160]: 连续学习：应用与未来路径

    Continual Learning: Applications and the Road Forward

    [https://arxiv.org/abs/2311.11908](https://arxiv.org/abs/2311.11908)

    连续学习是机器学习的子领域，致力于让机器学习模型在新数据上不断学习，而不忘记过去学到的知识。研究揭示了内存限制场景的主导地位，并讨论了连续学习在解决模型编辑、个性化、专业化、设备端学习、快速（重新）训练和强化学习等问题中的作用。

    

    连续学习是机器学习的一个子领域，旨在使机器学习模型能够在新数据上不断学习，通过积累知识而不遗忘过去所学。本研究退一步思考，并提出问题：“为什么首先要关注连续学习？”。我们通过审视近期在四个主要机器学习会议上发表的连续学习论文来铺垫，展示了受内存限制的场景主导了该领域。然后，我们讨论了机器学习中的五个未解问题，尽管乍看起来可能与连续学习无关，但我们展示了连续学习将必然成为它们解决方案的一部分。这些问题包括模型编辑、个性化和专业化、设备端学习、更快的（重新）训练和强化学习。最后，通过比较这些未解问题的期望和当前的假设

    arXiv:2311.11908v3 Announce Type: replace-cross  Abstract: Continual learning is a subfield of machine learning, which aims to allow machine learning models to continuously learn on new data, by accumulating knowledge without forgetting what was learned in the past. In this work, we take a step back, and ask: "Why should one care about continual learning in the first place?". We set the stage by examining recent continual learning papers published at four major machine learning conferences, and show that memory-constrained settings dominate the field. Then, we discuss five open problems in machine learning, and even though they might seem unrelated to continual learning at first sight, we show that continual learning will inevitably be part of their solution. These problems are model editing, personalization and specialization, on-device learning, faster (re-)training and reinforcement learning. Finally, by comparing the desiderata from these unsolved problems and the current assumptio
    
[^161]: 基于条件化空间-时间归一化流的概率天气预测

    Towards probabilistic Weather Forecasting with Conditioned Spatio-Temporal Normalizing Flows

    [https://arxiv.org/abs/2311.06958](https://arxiv.org/abs/2311.06958)

    该论文提出了一种基于条件化归一化流的概率天气预测方法，通过实验证明其能够捕捉和良好外推空间-时间相关性。

    

    生成式归一化流能够建模多模态空间分布，已经成功地模拟了时间相关性。由于其训练稳定性、可逆性以及在采样和推断方面的高效性，这些模型比其他类型的生成模型提供了几项好处。这使它们成为随机空间-时间预测问题的合适候选者，在许多科学领域中都普遍存在，如地球科学、天体物理学或分子科学。本文介绍了用于随机空间-时间建模的条件化归一化流。该方法在从ERA5数据集进行的日温度和小时等压图预测任务上进行了评估。实验表明，我们的方法能够捕捉空间-时间相关性，并能够在训练期间使用的时间范围之外进行良好的外推。

    arXiv:2311.06958v2 Announce Type: replace-cross  Abstract: Generative normalizing flows are able to model multimodal spatial distributions, and they have been shown to model temporal correlations successfully as well. These models provide several benefits over other types of generative models due to their training stability, invertibility and efficiency in sampling and inference. This makes them a suitable candidate for stochastic spatio-temporal prediction problems, which are omnipresent in many fields of sciences, such as earth sciences, astrophysics or molecular sciences. In this paper, we present conditional normalizing flows for stochastic spatio-temporal modelling. The method is evaluated on the task of daily temperature and hourly geopotential map prediction from ERA5 datasets. Experiments show that our method is able to capture spatio-temporal correlations and extrapolates well beyond the time horizon used during training.
    
[^162]: 实时递归强化学习

    Real-Time Recurrent Reinforcement Learning

    [https://arxiv.org/abs/2311.04830](https://arxiv.org/abs/2311.04830)

    本文提出了实时递归强化学习（RTRRL）方法，通过结合元-强化学习RNN架构、外部强化学习算法和RFLO局部在线学习，成功解决部分可观察马尔可夫决策过程中的离散和连续控制任务。实验结果表明，在计算复杂性相当的情况下，使用BPTT或RTRL替代RTRRL中的优化算法并不能提高回报。

    

    在本文中，我们提出了实时递归强化学习（RTRRL），这是一种对部分可观察马尔可夫决策过程（POMDPs）中的离散和连续控制任务进行求解的生物学合理方法。RTRRL由三部分组成：（1）一个元-强化学习循环神经网络（RNN）架构，独立实现了一个演员-评论家算法；（2）一个外部强化学习算法，利用时序差分学习和荷兰资格追踪来训练元-强化学习网络；和（3）随机反馈局部在线（RFLO）学习，一种用于计算网络参数梯度的在线自动微分算法。我们的实验结果表明，通过将RTRRL中的优化算法替换为生物不合理的时延反向传播（BPTT）或实时递归学习（RTRL），并不能改善回报，同时在匹配BPTT的计算复杂性的情况下，甚至会增加返回。

    arXiv:2311.04830v2 Announce Type: replace  Abstract: In this paper we propose real-time recurrent reinforcement learning (RTRRL), a biologically plausible approach to solving discrete and continuous control tasks in partially-observable markov decision processes (POMDPs). RTRRL consists of three parts: (1) a Meta-RL RNN architecture, implementing on its own an actor-critic algorithm; (2) an outer reinforcement learning algorithm, exploiting temporal difference learning and dutch eligibility traces to train the Meta-RL network; and (3) random-feedback local-online (RFLO) learning, an online automatic differentiation algorithm for computing the gradients with respect to parameters of the network.Our experimental results show that by replacing the optimization algorithm in RTRRL with the biologically implausible back propagation through time (BPTT), or real-time recurrent learning (RTRL), one does not improve returns, while matching the computational complexity for BPTT, and even increasi
    
[^163]: 先验中的内容是什么？用于逆问题的学习近端网络

    What's in a Prior? Learned Proximal Networks for Inverse Problems

    [https://arxiv.org/abs/2310.14344](https://arxiv.org/abs/2310.14344)

    提供了一个框架来发展学习的近端网络，证明它们提供了确切的proximal操作符。

    

    Proximal operators are ubiquitous in inverse problems, commonly appearing as part of algorithmic strategies to regularize problems that are otherwise ill-posed. Modern deep learning models have been brought to bear for these tasks too, as in the framework of plug-and-play or deep unrolling, where they loosely resemble proximal operators. Yet, something essential is lost in employing these purely data-driven approaches: there is no guarantee that a general deep network represents the proximal operator of any function, nor is there any characterization of the function for which the network might provide some approximate proximal. This not only makes guaranteeing convergence of iterative schemes challenging but, more fundamentally, complicates the analysis of what has been learned by these networks about their training data.

    arXiv:2310.14344v2 Announce Type: replace-cross  Abstract: Proximal operators are ubiquitous in inverse problems, commonly appearing as part of algorithmic strategies to regularize problems that are otherwise ill-posed. Modern deep learning models have been brought to bear for these tasks too, as in the framework of plug-and-play or deep unrolling, where they loosely resemble proximal operators. Yet, something essential is lost in employing these purely data-driven approaches: there is no guarantee that a general deep network represents the proximal operator of any function, nor is there any characterization of the function for which the network might provide some approximate proximal. This not only makes guaranteeing convergence of iterative schemes challenging but, more fundamentally, complicates the analysis of what has been learned by these networks about their training data. Herein we provide a framework to develop learned proximal networks (LPN), prove that they provide exact pro
    
[^164]: 图神经网络与神经-ODE的整合用于肿瘤动态预测

    Integration of Graph Neural Network and Neural-ODEs for Tumor Dynamic Prediction

    [https://arxiv.org/abs/2310.00926](https://arxiv.org/abs/2310.00926)

    提出了使用图神经网络与神经-ODE整合的方法用于肿瘤动态预测，有效整合了多种数据信息，有助于增强个性化肿瘤预测。

    

    在抗癌药物开发中，一个主要的科学挑战是解开患者肿瘤样本的高维基因组数据、相应肿瘤的器官来源、与给定治疗相关的药物靶点以及 resulting treatment response 之间的复杂关系。此外，为了实现精准医学在根据治疗反应识别和调整患者治疗方案的愿景，有必要构建能够整合纵向肿瘤大小和多模态、高内容数据的肿瘤动态模型。在这项工作中，我们通过提出一个异构图编码器，结合了一个双分图图卷积神经网络（GCN）和神经常微分方程（神经-ODEs），迈向增强个性化肿瘤动态预测的一步。我们将该方法应用于大量来源于患者的异种移植模型（PDX）数据。

    arXiv:2310.00926v2 Announce Type: replace  Abstract: In anti-cancer drug development, a major scientific challenge is disentangling the complex relationships between high-dimensional genomics data from patient tumor samples, the corresponding tumor's organ of origin, the drug targets associated with given treatments and the resulting treatment response. Furthermore, to realize the aspirations of precision medicine in identifying and adjusting treatments for patients depending on the therapeutic response, there is a need for building tumor dynamic models that can integrate both longitudinal tumor size as well as multimodal, high-content data. In this work, we take a step towards enhancing personalized tumor dynamic predictions by proposing a heterogeneous graph encoder that utilizes a bipartite Graph Convolutional Neural network (GCN) combined with Neural Ordinary Differential Equations (Neural-ODEs). We applied the methodology to a large collection of patient-derived xenograft (PDX) da
    
[^165]: 谁在与你交谈？一种赋予社交机器人定位能力的深度学习模型

    To Whom are You Talking? A Deep Learning Model to Endow Social Robots with Addressee Estimation Skills

    [https://arxiv.org/abs/2308.10757](https://arxiv.org/abs/2308.10757)

    通过深度学习模型，这项研究解决了社交机器人在生态场景中理解说话者对象的问题。

    

    交流塑造了我们的社交世界。为了让机器人被视为社交的，并被纳入我们的社交环境中，理解驱使人际交流的一些动态是至关重要的。在这项工作中，我们解决了定位对象的问题，即理解话语的对象，通过解释和利用说话者的非语言身体信号。我们通过实现一个混合深度学习模型来做到这一点，该模型由卷积层和LSTM单元组成，以描绘说话者脸部的图像和说话者身体姿势的2D向量作为输入。我们的实现选择是为了开发一个在社交机器人上可以部署并能在生态场景中高效的模型。我们展示了我们的模型能够从机器人的自我中心观点解决定位对象的问题。

    arXiv:2308.10757v2 Announce Type: replace-cross  Abstract: Communicating shapes our social word. For a robot to be considered social and being consequently integrated in our social environment it is fundamental to understand some of the dynamics that rule human-human communication. In this work, we tackle the problem of Addressee Estimation, the ability to understand an utterance's addressee, by interpreting and exploiting non-verbal bodily cues from the speaker. We do so by implementing an hybrid deep learning model composed of convolutional layers and LSTM cells taking as input images portraying the face of the speaker and 2D vectors of the speaker's body posture. Our implementation choices were guided by the aim to develop a model that could be deployed on social robots and be efficient in ecological scenarios. We demonstrate that our model is able to solve the Addressee Estimation problem in terms of addressee localisation in space, from a robot ego-centric point of view.
    
[^166]: 机器学习驱动的组合时钟拍卖

    Machine Learning-Powered Combinatorial Clock Auction

    [https://arxiv.org/abs/2308.10226](https://arxiv.org/abs/2308.10226)

    本文提出了一种机器学习驱动的组合时钟拍卖，通过仅使用需求查询而不是价值查询来获取投标人的偏好信息。

    

    我们研究了迭代组合拍卖（ICA）的设计。在这个领域中的主要挑战在于束空间随着物品数量呈指数增长。为了解决这个问题，最近有几篇论文提出了基于机器学习（ML）的偏好调查算法，旨在仅从投标人那里获取最重要的信息。然而，从实际角度看，这些先前工作的主要缺点是通过价值查询（即，“对于捆绑包$\{A,B\}$，您的价值是多少？”）引出投标人的偏好。在大多数实际ICA领域中，价值查询被认为是不切实际的，因为它们给投标人带来了不切实际的高认知负担，这就是为什么它们在实践中不被使用的原因。在本文中，我们通过设计一个机器学习驱动的组合时钟拍卖来解决这一缺点，该拍卖只通过需求查询（即，“在价格$p$下，您对捆绑包$\{A,B\}$的需求有多少？”）从投标人那里引起信息。

    arXiv:2308.10226v2 Announce Type: replace-cross  Abstract: We study the design of iterative combinatorial auctions (ICAs). The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, several papers have recently proposed machine learning (ML)-based preference elicitation algorithms that aim to elicit only the most important information from bidders. However, from a practical point of view, the main shortcoming of this prior work is that those designs elicit bidders' preferences via value queries (i.e., ``What is your value for the bundle $\{A,B\}$?''). In most real-world ICA domains, value queries are considered impractical, since they impose an unrealistically high cognitive burden on bidders, which is why they are not used in practice. In this paper, we address this shortcoming by designing an ML-powered combinatorial clock auction that elicits information from the bidders only via demand queries (i.e., ``At prices $p$, what
    
[^167]: 一个关于双分级持久性条码的稳定性定理

    A stability theorem for bigraded persistence barcodes

    [https://arxiv.org/abs/2303.14694](https://arxiv.org/abs/2303.14694)

    使用矩角复合体的同调概念定义了双分级持久性条码，并证明了它们的稳定性定理

    

    我们使用越托里斯-里普斯滤波的矩角复合体的普通和双重同调定义了有限伪度量空间X的双分级持久同调模和双分级条码。我们证明了关于双分级持久性双同调模和条码的稳定性定理。

    arXiv:2303.14694v2 Announce Type: replace-cross  Abstract: We define bigraded persistent homology modules and bigraded barcodes of a finite pseudo-metric space X using the ordinary and double homology of the moment-angle complex associated with the Vietoris-Rips filtration of X. We prove a stability theorem for the bigraded persistent double homology modules and barcodes.
    
[^168]: 面向解释神经代码模型的因果论理论

    Toward a Theory of Causation for Interpreting Neural Code Models

    [https://arxiv.org/abs/2302.03788](https://arxiv.org/abs/2302.03788)

    该论文介绍了一种名为$do_{code}$的后验解释方法，用于解释神经代码模型的预测，基于因果推断，旨在实现面向编程语言的解释。

    

    Neural Language Models of Code，或者称为神经代码模型（NCMs），正在迅速从研究原型发展为商业开发者工具。因此，理解这些模型的能力和局限性变得至关重要。然而，这些模型的能力通常是使用自动化指标来衡量的，这些指标通常只能揭示它们真实性能的一部分。一般来说，NCMs的性能似乎很有前途，但目前关于这些模型如何做出决策仍有很多未知。因此，本文介绍了一种名为$do_{code}$的后验解释方法，该方法专门针对NCMs，能够解释模型的预测。$do_{code}$基于因果推断，以实现面向编程语言的解释。虽然$do_{code}$的理论基础可扩展到探索不同的模型属性，但我们提供了一个具体的实例，旨在减少影响...

    arXiv:2302.03788v2 Announce Type: replace-cross  Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact o
    
[^169]: 优化时间序列对比学习：一种动态坏样本挖掘方法

    Towards Enhancing Time Series Contrastive Learning: A Dynamic Bad Pair Mining Approach

    [https://arxiv.org/abs/2302.03357](https://arxiv.org/abs/2302.03357)

    提出了一种动态坏样本挖掘（DBPM）算法，可靠地识别和抑制时间序列对比学习中的噪声正样本对和错误正样本对。

    

    不是所有的正样本对对时间序列对比学习都有益处。本文研究了两种可能影响对比学习学习到的时间序列表示质量的坏正样本对：噪声正样本对和错误正样本对。我们观察到，当存在噪声正样本对时，模型往往只学习噪声的模式（噪声对齐）。与此同时，当出现错误正样本对时，模型会浪费大量精力来对齐非代表性模式（错误对齐）。为了解决这个问题，我们提出了一种动态坏样本挖掘（DBPM）算法，可可靠地识别和抑制时间序列对比学习中的坏正样本对。具体来说，DBPM利用内存模块动态跟踪每个正样本对在训练过程中的训练行为，从而使我们能够基于每个时期识别潜在的坏正样本对。

    arXiv:2302.03357v2 Announce Type: replace  Abstract: Not all positive pairs are beneficial to time series contrastive learning. In this paper, we study two types of bad positive pairs that can impair the quality of time series representation learned through contrastive learning: the noisy positive pair and the faulty positive pair. We observe that, with the presence of noisy positive pairs, the model tends to simply learn the pattern of noise (Noisy Alignment). Meanwhile, when faulty positive pairs arise, the model wastes considerable amount of effort aligning non-representative patterns (Faulty Alignment). To address this problem, we propose a Dynamic Bad Pair Mining (DBPM) algorithm, which reliably identifies and suppresses bad positive pairs in time series contrastive learning. Specifically, DBPM utilizes a memory module to dynamically track the training behavior of each positive pair along training process. This allows us to identify potential bad positive pairs at each epoch based
    
[^170]: 使用带有鲁棒性保证的最优输运扰动进行安全强化学习

    Optimal Transport Perturbations for Safe Reinforcement Learning with Robustness Guarantees

    [https://arxiv.org/abs/2301.13375](https://arxiv.org/abs/2301.13375)

    引入了基于最优输运扰动的安全强化学习框架，通过构建最坏情况的虚拟状态转换以提升鲁棒性能和安全性。

    

    基于最优输运成本不确定性集，引入了一个安全强化学习框架，通过应用最优输运扰动来构建最坏情况的虚拟状态转换，提供了一种有效的实现方法。在连续控制任务的实验中，我们的方法展示了鲁棒性能，并显著提高了部署时的安全性。

    arXiv:2301.13375v2 Announce Type: replace-cross  Abstract: Robustness and safety are critical for the trustworthy deployment of deep reinforcement learning. Real-world decision making applications require algorithms that can guarantee robust performance and safety in the presence of general environment disturbances, while making limited assumptions on the data collection process during training. In order to accomplish this goal, we introduce a safe reinforcement learning framework that incorporates robustness through the use of an optimal transport cost uncertainty set. We provide an efficient implementation based on applying Optimal Transport Perturbations to construct worst-case virtual state transitions, which does not impact data collection during training and does not require detailed simulator access. In experiments on continuous control tasks with safety constraints, our approach demonstrates robust performance while significantly improving safety at deployment time compared to 
    
[^171]: SOLD：僧伽罗语攻击性语言数据集

    SOLD: Sinhala Offensive Language Dataset

    [https://arxiv.org/abs/2212.00851](https://arxiv.org/abs/2212.00851)

    本文介绍了一种新的低资源语言——僧伽罗语攻击性语言识别数据集(SOLD)，填补了目前攻击性语言识别研究局限于高资源语言的空白。

    

    在线攻击性内容的普遍存在，比如仇恨言论和网络欺凌，已成为全球性现象。这引起了人工智能（AI）和自然语言处理（NLP）社区的兴趣，促使开发各种系统，能够自动检测潜在有害内容。然而，除了少数几个例外情况外，大多数关于这一主题的数据集都处理英语和少数其他高资源语言。因此，攻击性语言识别研究一直局限于这些语言。本文通过处理僧伽罗语攻击性语言识别来填补这一空白，僧伽罗语是斯里兰卡有超过1700万人口使用的低资源印欧语言。我们介绍了僧伽罗语攻击性语言数据集（SOLD），并在该数据集上展示了多个实验。

    arXiv:2212.00851v2 Announce Type: replace-cross  Abstract: The widespread of offensive content online, such as hate speech and cyber-bullying, is a global phenomenon. This has sparked interest in the artificial intelligence (AI) and natural language processing (NLP) communities, motivating the development of various systems trained to detect potentially harmful content automatically. These systems require annotated datasets to train the machine learning (ML) models. However, with a few notable exceptions, most datasets on this topic have dealt with English and a few other high-resource languages. As a result, the research in offensive language identification has been limited to these languages. This paper addresses this gap by tackling offensive language identification in Sinhala, a low-resource Indo-Aryan language spoken by over 17 million people in Sri Lanka. We introduce the Sinhala Offensive Language Dataset (SOLD) and present multiple experiments on this dataset. SOLD is a manuall
    
[^172]: 使用随机克里金的双目标排序和选择

    Bi-objective Ranking and Selection Using Stochastic Kriging

    [https://arxiv.org/abs/2209.03919](https://arxiv.org/abs/2209.03919)

    提出了一种使用随机克里金的贝叶斯双目标排序和选择方法，以减少在识别具有最佳期望性能解时的错误分类

    

    我们考虑双目标排序和选择问题，目标是在观察到两个目标结果具有不确定性的候选集中正确识别帕累托最优解，例如，在运行多目标随机模拟优化过程之后。在识别这些解时，观测性能的噪声扰动可能导致两种错误：真正帕累托最优的解可能被错误地认为是被支配的，而真正被支配的解可能被错误地认为是帕累托最优的。我们提出了一种新颖的贝叶斯双目标排序和选择方法，通过顺序分配额外样本给竞争解，以减少在识别具有最佳期望性能的解时的错误分类。该方法使用随机克里金构建客观预测分布。

    arXiv:2209.03919v3 Announce Type: replace-cross  Abstract: We consider bi-objective ranking and selection problems, where the goal is to correctly identify the Pareto optimal solutions among a finite set of candidates for which the two objective outcomes have been observed with uncertainty (e.g., after running a multiobjective stochastic simulation optimization procedure). When identifying these solutions, the noise perturbing the observed performance may lead to two types of errors: solutions that are truly Pareto-optimal can be wrongly considered dominated, and solutions that are truly dominated can be wrongly considered Pareto-optimal. We propose a novel Bayesian bi-objective ranking and selection method that sequentially allocates extra samples to competitive solutions, in view of reducing the misclassification errors when identifying the solutions with the best expected performance. The approach uses stochastic kriging to build reliable predictive distributions of the objective ou
    
[^173]: 在智能手机上高效基于深度学习的生命体征估计

    Efficient Deep Learning-based Estimation of the Vital Signs on Smartphones

    [https://arxiv.org/abs/2204.08989](https://arxiv.org/abs/2204.08989)

    提出了一种通过深度学习的端到端解决方案，用于在智能手机上高效估计生命体征，消除了繁琐的预处理步骤。

    

    随着智能手机在我们的日常生活中的日益普及，这些设备已经能够执行许多复杂的任务。针对对生命体征的持续监测需求，特别是针对老年人或患有某些类型疾病的人群，开发能够使用智能手机估计生命体征的算法引起了全球研究人员的关注。研究人员已经在探索使用可以在智能手机上运行的算法来估计生命体征，例如心率、血氧饱和度水平和呼吸率。然而，许多这些算法需要多个预处理步骤，这可能会引入一些实现开销或需要设计几个手工阶段才能获得最佳结果。为了解决这个问题，本研究提出了一种新颖的基于深度学习的移动设备生命体征估计的端到端解决方案，消除了对预处理的需求。

    arXiv:2204.08989v3 Announce Type: replace-cross  Abstract: With the increasing use of smartphones in our daily lives, these devices have become capable of performing many complex tasks. Concerning the need for continuous monitoring of vital signs, especially for the elderly or those with certain types of diseases, the development of algorithms that can estimate vital signs using smartphones has attracted researchers worldwide. In particular, researchers have been exploring ways to estimate vital signs, such as heart rate, oxygen saturation levels, and respiratory rate, using algorithms that can be run on smartphones. However, many of these algorithms require multiple pre-processing steps that might introduce some implementation overheads or require the design of a couple of hand-crafted stages to obtain an optimal result. To address this issue, this research proposes a novel end-to-end solution to mobile-based vital sign estimation using deep learning that eliminates the need for pre-p
    
[^174]: 基于强化学习的MIMO OFDM系统中的逐步去噪信道估计

    Channel Estimation via Successive Denoising in MIMO OFDM Systems: A Reinforcement Learning Approach

    [https://arxiv.org/abs/2101.10300](https://arxiv.org/abs/2101.10300)

    提出了一种基于强化学习框架的频域信道估计去噪方法，通过逐步信道去噪过程和信道曲率计算来识别不可靠信道估计。

    

    通常，在多输入多输出（MIMO）正交频分复用（OFDM）系统中实现可靠通信需要接收端进行准确的信道估计。现有文献主要集中在依赖于时间域信道分析或需要大量预先标记数据集进行训练的信道估计去噪方法上。为解决这些限制，我们提出了一种基于强化学习框架的频域去噪方法，无需先验信道知识和预标记数据。我们的方法包括基于信道曲率计算的新型逐步信道去噪过程，其中我们获得一个信道曲率幅度阈值来识别不可靠的信道估计。基于这一过程，我们将去噪机制建模为马尔可夫决策过程。

    arXiv:2101.10300v5 Announce Type: replace-cross  Abstract: In general, reliable communication via multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM) requires accurate channel estimation at the receiver. The existing literature largely focuses on denoising methods for channel estimation that depend on either (i) channel analysis in the time-domain with prior channel knowledge or (ii) supervised learning techniques which require large pre-labeled datasets for training. To address these limitations, we present a frequency-domain denoising method based on a reinforcement learning framework that does not need a priori channel knowledge and pre-labeled data. Our methodology includes a new successive channel denoising process based on channel curvature computation, for which we obtain a channel curvature magnitude threshold to identify unreliable channel estimates. Based on this process, we formulate the denoising mechanism as a Markov decision process, wh
    
[^175]: 用于流形上函数逼近的随机向量功能链接网络

    Random Vector Functional Link Networks for Function Approximation on Manifolds

    [https://arxiv.org/abs/2007.15776](https://arxiv.org/abs/2007.15776)

    本文提供了对于连续函数在紧致域上的通用逼近器的严格证明，填补了单层神经网络随机向量功能链接网络在实践中成功的理论缺口

    

    feed-forward神经网络的学习速度因慢而著名，在深度学习应用中已经成为瓶颈数十年。为了应对这一问题，研究人员和实践者尝试引入随机性来减少学习需求。基于Igelnik和Pao的原始构造，具有随机输入到隐藏层权重和偏置的单层神经网络在实践中取得成功，但缺乏必要的理论证明。本文填补了这一理论空白。我们提供了一个（更正的）严格证明，证明Igelnik和Pao的构造是一个连续函数在紧致域上的通用逼近器，逼近误差像渐近衰减。

    arXiv:2007.15776v3 Announce Type: replace-cross  Abstract: The learning speed of feed-forward neural networks is notoriously slow and has presented a bottleneck in deep learning applications for several decades. For instance, gradient-based learning algorithms, which are used extensively to train neural networks, tend to work slowly when all of the network parameters must be iteratively tuned. To counter this, both researchers and practitioners have tried introducing randomness to reduce the learning requirement. Based on the original construction of Igelnik and Pao, single layer neural-networks with random input-to-hidden layer weights and biases have seen success in practice, but the necessary theoretical justification is lacking. In this paper, we begin to fill this theoretical gap. We provide a (corrected) rigorous proof that the Igelnik and Pao construction is a universal approximator for continuous functions on compact domains, with approximation error decaying asymptotically lik
    
[^176]: 利用深度学习对脑电解码中的欧几里得对齐进行系统评估

    A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding. (arXiv:2401.10746v1 [eess.SP])

    [http://arxiv.org/abs/2401.10746](http://arxiv.org/abs/2401.10746)

    本研究系统评估了使用深度学习和欧几里得对齐对脑电解码的影响。结果表明，欧几里得对齐能够显著提高解码率，并且减少了收敛时间。

    

    脑电图（EEG）信号经常用于各种脑机接口（BCI）任务。尽管深度学习（DL）技术显示出有希望的结果，但它们受到大量数据要求的限制。通过利用来自多个受试者的数据，迁移学习能够更有效地训练DL模型。一种越来越受欢迎的技术是欧几里得对齐（EA），因为它易于使用、计算复杂度低并且与深度学习模型兼容。然而，很少有研究评估其对共享和个体DL模型的训练效果的影响。在这项工作中，我们系统地评估了EA与DL相结合在解码BCI信号中的效果。我们使用EA来训练来自多个受试者的共享模型，并评估其对新受试者的可迁移性。我们的实验结果表明，它将目标受试者的解码率提高了4.33％，并且收敛时间缩短了超过70％。我们还为个体模型进行了训练。

    Electroencephalography (EEG) signals are frequently used for various Brain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have shown promising results, they are hindered by the substantial data requirements. By leveraging data from multiple subjects, transfer learning enables more effective training of DL models. A technique that is gaining popularity is Euclidean Alignment (EA) due to its ease of use, low computational complexity, and compatibility with Deep Learning models. However, few studies evaluate its impact on the training performance of shared and individual DL models. In this work, we systematically evaluate the effect of EA combined with DL for decoding BCI signals. We used EA to train shared models with data from multiple subjects and evaluated its transferability to new subjects. Our experimental results show that it improves decoding in the target subject by 4.33% and decreases convergence time by more than 70%. We also trained individual models for 
    
[^177]: 大型语言模型的知识编辑全面研究

    A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])

    [http://arxiv.org/abs/2401.01286](http://arxiv.org/abs/2401.01286)

    本研究全面研究了大型语言模型的知识编辑，旨在有效修改模型的行为，同时保持整体性能。

    

    大型语言模型(LLM)在理解和生成与人类交流紧密相似的文本方面展现出了非凡的能力。然而，其主要限制在于训练过程中的显著计算需求，这是由于其广泛的参数化造成的。这一挑战在于世界的动态性，需要频繁更新LLM以修正过时的信息或集成新知识，从而确保其持续的相关性。许多应用需要在训练后进行持续的模型调整，以解决缺陷或不良行为。近年来，对于LLM的知识编辑技术的兴趣越来越高，在特定领域内有效地修改LLM的行为，同时保持整体性能在各种输入中的表现。本文首先定义了知识编辑的目标和挑战，然后综述了现有的知识编辑方法和技术，并讨论了其应用和未来发展的方向。

    Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
    
[^178]: 部分标签学习中的合作伙伴

    Partial Label Learning with a Partner. (arXiv:2312.11034v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11034](http://arxiv.org/abs/2312.11034)

    本文介绍了一种在部分标签学习中识别和纠正错误标记样本的方法，通过引入合作伙伴分类器和互相监督范式，以及使用模糊机制实现分类器之间的协作。

    

    在部分标签学习中，每个实例与一组候选标签相关联，其中只有一个是真实标签。现有的工作大多关注构建强大的分类器来估计候选标签的标签置信度，以识别正确的标签。然而，这些方法通常很难纠正错误标记的样本。为了帮助现有的部分标签学习方法识别和纠正错误标记的样本，在本文中，我们引入了一种新的合作伙伴分类器，并提出了一种新的“互相监督”范式。具体来说，我们根据隐含的事实实例化了合作伙伴分类器，即一个样本的非候选标签不应该被分配给它，这在部分标签学习中具有内在的准确性并且尚未得到充分研究。此外，我们还提出了一个新的协作项来将基础分类器和合作伙伴分类器链接起来。在每个互相监督的阶段中，两个分类器将通过一个模糊机制互相模糊彼此的预测。

    In partial label learning (PLL), each instance is associated with a set of candidate labels among which only one is ground-truth. The majority of the existing works focuses on constructing robust classifiers to estimate the labeling confidence of candidate labels in order to identify the correct one. However, these methods usually struggle to rectify mislabeled samples. To help existing PLL methods identify and rectify mislabeled samples, in this paper, we introduce a novel partner classifier and propose a novel ``mutual supervision'' paradigm. Specifically, we instantiate the partner classifier predicated on the implicit fact that non-candidate labels of a sample should not be assigned to it, which is inherently accurate and has not been fully investigated in PLL. Furthermore, a novel collaborative term is formulated to link the base classifier and the partner one. During each stage of mutual supervision, both classifiers will blur each other's predictions through a blurring mechanism
    
[^179]: 关于上下文学习的校准研究

    A Study on the Calibration of In-context Learning. (arXiv:2312.04021v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.04021](http://arxiv.org/abs/2312.04021)

    本研究关注上下文学习（ICL），通过定制提示来调整静态语言模型（LMs），研究了在各种自然语言理解和推理任务中性能和校准之间的平衡。研究发现随着ICL示例数量的增加，模型的校准会先增加而后得到改善，而校准误差主要出现在低样本场景下。此外，微调和CoT提示等方法可能导致校准误差和不可靠的自然语言解释，提示需要针对可靠性场景开发新的方法。

    

    准确的不确定性量化对于语言模型（LMs）的安全部署至关重要，以前的研究已经证明了现代LMs校准性的改进。我们的研究重点是上下文学习（ICL），一种通过定制提示来调整静态LMs的常见方法，并研究在广泛的自然语言理解和推理任务中性能和校准之间的平衡。通过全面的实验，我们观察到，随着ICL示例数量的增加，模型最初会出现增加的校准误差，然后才能实现更好的校准，而校准误差往往在低样本场景下出现。此外，我们发现以提高可用性为目标的方法，如微调和CoT提示，可能导致校准误差和不可靠的自然语言解释，这表明在期望模型可靠性的场景中可能需要新的方法。

    Accurate uncertainty quantification is crucial for the safe deployment of language models (LMs), and prior research has demonstrated improvements in the calibration of modern LMs. Our study focuses on in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examines the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations, suggesting that new methods may be required for scenarios where models are expected to be reliable.
    
[^180]: 条件对偏好关系进行限制以保证最优策略的存在

    Conditions on Preference Relations that Guarantee the Existence of Optimal Policies. (arXiv:2311.01990v1 [cs.LG])

    [http://arxiv.org/abs/2311.01990](http://arxiv.org/abs/2311.01990)

    我们引入了直接偏好过程框架，通过对偏好的序结构进行分析，我们提出了保证最优策略存在的条件。这个研究缩小了学习偏好反馈算法在理论和实践之间的差距，并提供了最优策略存在的证明。

    

    学习偏好反馈 (LfPF) 在训练大型语言模型和某些类型的交互式学习代理中起着至关重要的作用。然而，LfPF算法的理论和应用之间存在着实质性的差距。目前保证在LfPF问题中存在最优策略的结果假设偏好和转移动态都由马尔可夫决策过程确定。我们引入了直接偏好过程，这是一种在部分可观察、非马尔可夫环境中分析LfPF问题的新框架。在这个框架中，我们通过考虑偏好的序结构建立了保证最优策略存在的条件。利用冯·诺依曼-摩根斯特恩期望效用定理，我们表明直接偏好过程推广了标准强化学习问题。我们的研究结果缩小了LfPF算法在经验成功和理论理解之间的差距，并提供了最优策略存在的证明。

    Learning from Preferential Feedback (LfPF) plays an essential role in training Large Language Models, as well as certain types of interactive learning agents. However, a substantial gap exists between the theory and application of LfPF algorithms. Current results guaranteeing the existence of optimal policies in LfPF problems assume that both the preferences and transition dynamics are determined by a Markov Decision Process. We introduce the Direct Preference Process, a new framework for analyzing LfPF problems in partially-observable, non-Markovian environments. Within this framework, we establish conditions that guarantee the existence of optimal policies by considering the ordinal structure of the preferences. Using the von Neumann-Morgenstern Expected Utility Theorem, we show that the Direct Preference Process generalizes the standard reinforcement learning problem. Our findings narrow the gap between the empirical success and theoretical understanding of LfPF algorithms and provi
    
[^181]: ADMarker: 一种多模式联邦学习系统，用于监测阿尔茨海默病的数字生物标志物

    ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer's Disease. (arXiv:2310.15301v1 [cs.LG])

    [http://arxiv.org/abs/2310.15301](http://arxiv.org/abs/2310.15301)

    ADMarker是一个多模式联邦学习系统，用于在自然生活环境中监测阿尔茨海默病的数字生物标志物。它具有新颖的联邦学习架构，能够准确检测出数字生物标志物，并在临床试验中展示出高准确率和早期AD识别能力。

    

    阿尔茨海默病（AD）及相关痴呆症由于人口老龄化而成为全球日益严重的健康挑战。本文介绍了ADMarker，这是第一个将多模式传感器和新的联邦学习算法整合起来，以在自然生活环境中检测多维AD数字生物标志物的端到端系统。ADMarker具有一种新颖的三阶段多模式联邦学习架构，能够以保护隐私的方式准确检测数字生物标志物。我们的方法共同解决了数据标签有限、数据异质性和有限的计算资源等几个主要的现实世界挑战。我们建立了一个紧凑的多模式硬件系统，并在一个为期四周的临床试验中将其部署在91名老年参与者身上。结果表明，ADMarker能够准确检测出全面的数字生物标志物，准确率高达93.8％，并以平均88.9％的准确率识别出早期AD。ADMarker提供了一个新的平台，可以在监测AD患者时提供准确和高效的数据分析。

    Alzheimer's Disease (AD) and related dementia are a growing global health challenge due to the aging population. In this paper, we present ADMarker, the first end-to-end system that integrates multi-modal sensors and new federated learning algorithms for detecting multidimensional AD digital biomarkers in natural living environments. ADMarker features a novel three-stage multi-modal federated learning architecture that can accurately detect digital biomarkers in a privacy-preserving manner. Our approach collectively addresses several major real-world challenges, such as limited data labels, data heterogeneity, and limited computing resources. We built a compact multi-modality hardware system and deployed it in a four-week clinical trial involving 91 elderly participants. The results indicate that ADMarker can accurately detect a comprehensive set of digital biomarkers with up to 93.8% accuracy and identify early AD with an average of 88.9% accuracy. ADMarker offers a new platform that 
    
[^182]: 使用Transformer学习解决气候传感器布放问题

    Learning to Solve Climate Sensor Placement Problems with a Transformer. (arXiv:2310.12387v1 [cs.LG])

    [http://arxiv.org/abs/2310.12387](http://arxiv.org/abs/2310.12387)

    本文介绍了一种使用深度强化学习方法学习改进传感器布放策略的新方法，通过与其他方法的对比实验证明了该方法在产生高质量解决方案方面的有效性和优越性。

    

    由于其NP难性质，环境监测和灾害管理中的传感器布放优化是一个具有挑战性的问题。传统的传感器布放方法包括精确、近似或启发式方法，其中启发式方法是最常用的。然而，启发式方法受到专家直觉和经验的限制。深度学习（DL）已经成为自动生成启发式算法的一种有前景的方法。本文介绍了一种新颖的传感器布放方法，重点是使用深度强化学习（RL）方法学习改进的启发式算法。我们的方法利用了一个强化学习公式来学习改进的启发式算法，通过演员-评论家算法训练策略网络。我们通过进行全面的实验将我们的方法与几种最先进的方法进行比较，证明了我们提出的方法在产生高质量解决方案方面的有效性和优越性。我们的工作提出了一种有前景的方法，用于解决气候传感器布放问题。

    The optimal placement of sensors for environmental monitoring and disaster management is a challenging problem due to its NP-hard nature. Traditional methods for sensor placement involve exact, approximation, or heuristic approaches, with the latter being the most widely used. However, heuristic methods are limited by expert intuition and experience. Deep learning (DL) has emerged as a promising approach for generating heuristic algorithms automatically. In this paper, we introduce a novel sensor placement approach focused on learning improvement heuristics using deep reinforcement learning (RL) methods. Our approach leverages an RL formulation for learning improvement heuristics, driven by an actor-critic algorithm for training the policy network. We compare our method with several state-of-the-art approaches by conducting comprehensive experiments, demonstrating the effectiveness and superiority of our proposed approach in producing high-quality solutions. Our work presents a promisi
    
[^183]: 在金融时间序列中的少样本学习模式用于趋势跟踪策略

    Few-Shot Learning Patterns in Financial Time-Series for Trend-Following Strategies. (arXiv:2310.10500v1 [q-fin.TR])

    [http://arxiv.org/abs/2310.10500](http://arxiv.org/abs/2310.10500)

    这项研究提出了一种在金融时间序列中使用少样本学习的趋势跟踪预测器，能够快速适应新的市场条件，并获得较高的夏普比率。

    

    针对系统化交易策略的预测模型在金融市场条件发生变化时无法迅速适应的问题，我们提出了一种新颖的时间序列趋势跟踪预测器，能够快速适应新的市场条件。我们利用深度学习领域的最新进展，采用少样本学习的方法。我们提出了交叉注意力时间序列趋势网络 - X-Trend，通过对金融时间序列进行关注，从上下文样本集中转移相似模式的趋势，对新的目标情境进行预测和定位。X-Trend能够快速适应新的金融情境，其夏普比率相对于神经预测器提高了18.9%，相对于传统时间序列方法提高了10倍。

    Forecasting models for systematic trading strategies do not adapt quickly when financial market conditions change, as was seen in the advent of the COVID-19 pandemic in 2020, when market conditions changed dramatically causing many forecasting models to take loss-making positions. To deal with such situations, we propose a novel time-series trend-following forecaster that is able to quickly adapt to new market conditions, referred to as regimes. We leverage recent developments from the deep learning community and use few-shot learning. We propose the Cross Attentive Time-Series Trend Network - X-Trend which takes positions attending over a context set of financial time-series regimes. X-Trend transfers trends from similar patterns in the context set to make predictions and take positions for a new distinct target regime. X-Trend is able to quickly adapt to new financial regimes with a Sharpe ratio increase of 18.9% over a neural forecaster and 10-fold over a conventional Time-series 
    
[^184]: 非冗余图神经网络的改进表达能力

    Non-Redundant Graph Neural Networks with Improved Expressiveness. (arXiv:2310.04190v1 [cs.LG])

    [http://arxiv.org/abs/2310.04190](http://arxiv.org/abs/2310.04190)

    本文提出了一种非冗余图神经网络的改进表达能力方法，通过基于邻域树的聚合方案减少冗余，提高了表达能力，实验证明了它对于减轻过度压缩的有效性。

    

    消息传递的图神经网络通过聚合来自所有邻居节点的消息来迭代计算节点嵌入。这个过程可以被视为Weisfeiler-Leman方法的神经变体，这限制了它们的表达能力。此外，过度平滑和过度压缩限制了这些网络能够有效利用的层数。消息传递中相同信息的重复交换和编码会放大过度压缩。我们提出了一种基于邻域树的新的聚合方案，允许通过修剪标准消息传递中的展开树的分支来控制冗余。我们证明减少冗余可以提高表达能力，并通过实验证明它减轻了过度压缩。我们研究了消息传递中的冗余与计算中的冗余之间的相互作用，并提出了邻域树的紧凑表示，通过神经树规范化技术计算节点和图的嵌入。

    Message passing graph neural networks iteratively compute node embeddings by aggregating messages from all neighbors. This procedure can be viewed as a neural variant of the Weisfeiler-Leman method, which limits their expressive power. Moreover, oversmoothing and oversquashing restrict the number of layers these networks can effectively utilize. The repeated exchange and encoding of identical information in message passing amplifies oversquashing. We propose a novel aggregation scheme based on neighborhood trees, which allows for controlling the redundancy by pruning branches of the unfolding trees underlying standard message passing. We prove that reducing redundancy improves expressivity and experimentally show that it alleviates oversquashing. We investigate the interaction between redundancy in message passing and redundancy in computation and propose a compact representation of neighborhood trees, from which we compute node and graph embeddings via a neural tree canonization techn
    
[^185]: Rayleigh Quotient Graph Neural Networks用于图级异常检测的研究

    Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection. (arXiv:2310.02861v1 [cs.LG])

    [http://arxiv.org/abs/2310.02861](http://arxiv.org/abs/2310.02861)

    《Rayleigh Quotient Graph Neural Networks用于图级异常检测的研究》提出使用Rayleigh Quotient作为驱动因素，通过探索图的固有光谱特征来实现图级异常检测。

    

    图级异常检测在癌症诊断和酶预测等领域中广泛应用。然而，现有方法无法捕捉到图异常的潜在属性，导致框架设计不可解释和性能不令人满意。在本文中，我们退一步重新研究了异常和正常图之间的光谱差异。我们的主要观察表明，这两个类之间的累计光谱能量存在显著差异。此外，我们证明了图信号的累计光谱能量可以用其瑞利商表示，这表明瑞利商是图异常属性的一个驱动因素。受此启发，我们提出了Rayleigh Quotient Graph Neural Network（RQGNN），这是第一个用于图级异常检测的光谱GNN，为探索异常图的固有光谱特征提供了新的视角。

    Graph-level anomaly detection has gained significant attention as it finds many applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the underlying properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. In this paper, we take a step back and re-investigate the spectral differences between anomalous and normal graphs. Our main observation shows a significant disparity in the accumulated spectral energy between these two classes. Moreover, we prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network (RQGNN), the first spectral GNN for graph-level anomaly detection, providing a new perspective on exploring the inherent spectral features of anomalous graph
    
[^186]: SWoTTeD:张量分解在时间表征中的扩展

    SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping. (arXiv:2310.01201v1 [cs.LG])

    [http://arxiv.org/abs/2310.01201](http://arxiv.org/abs/2310.01201)

    SWoTTeD是一种扩展的张量分解方法，用于发现复杂时间模式下的隐藏表征。在实验中，SWoTTeD不仅能与最新的基于张量分解的方法一样准确地重建数据，还能提取出对临床医生有意义的时间表征。

    

    张量分解最近在机器学习领域对于个体追踪数据的分析，如电子健康记录(EHR)，引起了人们的关注。然而，当数据遵循复杂的时间模式时，这个任务变得更加困难。本文引入了时间表征的概念，即一组随时间变化的特征，并提出了SWoTTeD (Sliding Window for Temporal Tensor Decomposition)方法，一种发现隐藏时间模式的新方法。SWoTTeD集成了多种约束和正则化方法，以增强提取到的表征的可解释性。我们使用合成数据集和真实世界数据集进行验证，并提供了使用巴黎大学医院的数据的原始用例。结果表明，SWoTTeD能够至少与最新的基于张量分解的模型一样准确地重建数据，并提取到对临床医生有意义的时间表征。

    Tensor decomposition has recently been gaining attention in the machine learning community for the analysis of individual traces, such as Electronic Health Records (EHR). However, this task becomes significantly more difficult when the data follows complex temporal patterns. This paper introduces the notion of a temporal phenotype as an arrangement of features over time and it proposes SWoTTeD (Sliding Window for Temporal Tensor Decomposition), a novel method to discover hidden temporal patterns. SWoTTeD integrates several constraints and regularizations to enhance the interpretability of the extracted phenotypes. We validate our proposal using both synthetic and real-world datasets, and we present an original usecase using data from the Greater Paris University Hospital. The results show that SWoTTeD achieves at least as accurate reconstruction as recent state-of-the-art tensor decomposition models, and extracts temporal phenotypes that are meaningful for clinicians.
    
[^187]: 关于Weisfeiler-Leman测试在图形模式参数中的能力

    On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters. (arXiv:2309.17053v1 [cs.LG])

    [http://arxiv.org/abs/2309.17053](http://arxiv.org/abs/2309.17053)

    这项研究探讨了图神经网络（GNN）表达能力和$k$维Weisfeiler-Leman ($k$WL)测试之间的关系，研究发现了$k$WL测试可以有效区分具有不同出现次数的模式图$P$的图形，并研究了模式图计数问题的最小维度$k$。

    

    图神经网络（GNN）领域的重要研究揭示了图神经网络的表达能力与$k$维Weisfeiler-Leman（$k$WL）测试之间的直接对应关系，$k$WL测试是一种广为认可的用于验证图同构的方法。这个连接重新引发了人们对$k$WL测试能够有效区分的特定图属性的兴趣。这个领域的研究的中心是确定最小维度$k$，使得$k$WL可以区分具有不同出现次数的模式图$P$的图形。我们将这个最小$k$称为这个模式计数问题的WL维度。这个调查传统上探讨与图案相关的两个不同的计数问题：子图计数和诱导子图计数。有趣的是，尽管它们最初看起来是具有看似不同方法的独立挑战，但这两个问题都是一个更全面问题的相互关联部分。

    Seminal research in the field of graph neural networks (GNNs) has revealed a direct correspondence between the expressive capabilities of GNNs and the $k$-dimensional Weisfeiler-Leman ($k$WL) test, a widely-recognized method for verifying graph isomorphism. This connection has reignited interest in comprehending the specific graph properties effectively distinguishable by the $k$WL test. A central focus of research in this field revolves around determining the least dimensionality $k$, for which $k$WL can discern graphs with different number of occurrences of a pattern graph $P$. We refer to such a least $k$ as the WL-dimension of this pattern counting problem. This inquiry traditionally delves into two distinct counting problems related to patterns: subgraph counting and induced subgraph counting. Intriguingly, despite their initial appearance as separate challenges with seemingly divergent approaches, both of these problems are interconnected components of a more comprehensive proble
    
[^188]: GPSINDy: 数据驱动的动力学系统方程发现

    GPSINDy: Data-Driven Discovery of Equations of Motion. (arXiv:2309.11076v1 [cs.LG])

    [http://arxiv.org/abs/2309.11076](http://arxiv.org/abs/2309.11076)

    GPSINDy是一种数据驱动的方法，通过将高斯过程回归与SINDy相结合，能够从噪声数据中发现非线性动力学系统模型，并在实验证明了其在系统动态和预测未来轨迹方面的改进性能。

    

    本论文考虑从有噪声数据中发现动力学系统模型的问题。已知噪声存在对符号回归算法来说是一个重要问题。我们将高斯过程回归（一种非参数学习方法）与SINDy（一种参数学习方法）相结合，从数据中识别非线性动力学系统。我们的方法具有简单性和与SINDy相比在有噪声数据上表现出更好的鲁棒性的优点。我们在Lotka-Volterra模型和仿真中的单轮车动态模型上以及在使用硬件数据的NVIDIA JetRacer系统上展示了我们的方法。我们展示了相较于SINDy，我们的方法在发现系统动态和预测未来轨迹方面的改进性能。

    In this paper, we consider the problem of discovering dynamical system models from noisy data. The presence of noise is known to be a significant problem for symbolic regression algorithms. We combine Gaussian process regression, a nonparametric learning method, with SINDy, a parametric learning approach, to identify nonlinear dynamical systems from data. The key advantages of our proposed approach are its simplicity coupled with the fact that it demonstrates improved robustness properties with noisy data over SINDy. We demonstrate our proposed approach on a Lotka-Volterra model and a unicycle dynamic model in simulation and on an NVIDIA JetRacer system using hardware data. We demonstrate improved performance over SINDy for discovering the system dynamics and predicting future trajectories.
    
[^189]: 可微分湍流

    Differentiable Turbulence. (arXiv:2307.03683v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2307.03683](http://arxiv.org/abs/2307.03683)

    深度学习与物理启发式选择的深度学习架构相结合，实现了可微分湍流模型，在大涡模拟中有效地建模子网格尺度湍流，并能推广到多种流动配置。

    

    深度学习在改进大涡模拟(SGS)中子网格尺度湍流闭合模型的准确性方面越来越具有潜力。我们利用可微分湍流的概念，在使用物理启发式选择的深度学习架构与端到端可微分求解器相结合的情况下，学习了适用于二维湍流流动的高效且多功能的SGS模型。我们对所选择的架构中的归纳偏差进行了深入分析，发现包含小尺度的非局部特征对于有效的SGS建模最为关键，而大尺度特征可以提高事后解场的点精确度。滤波速度梯度张量可以通过将输入和输出分解为各向同性、偏离同性和反对称分量来直接映射到SGS应力。我们发现该模型可以推广到多种流动配置，包括较高和较低的速度 Reynolds 数。

    Deep learning is increasingly becoming a promising pathway to improving the accuracy of sub-grid scale (SGS) turbulence closure models for large eddy simulations (LES). We leverage the concept of differentiable turbulence, whereby an end-to-end differentiable solver is used in combination with physics-inspired choices of deep learning architectures to learn highly effective and versatile SGS models for two-dimensional turbulent flow. We perform an in-depth analysis of the inductive biases in the chosen architectures, finding that the inclusion of small-scale non-local features is most critical to effective SGS modeling, while large-scale features can improve pointwise accuracy of the a-posteriori solution field. The filtered velocity gradient tensor can be mapped directly to the SGS stress via decomposition of the inputs and outputs into isotropic, deviatoric, and anti-symmetric components. We see that the model can generalize to a variety of flow configurations, including higher and l
    
[^190]: 使用可逆神经网络和误差扩散学习利用导电图重建气泡分布

    Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion. (arXiv:2307.02496v1 [eess.IV])

    [http://arxiv.org/abs/2307.02496](http://arxiv.org/abs/2307.02496)

    本研究利用可逆神经网络和误差扩散方法，通过测量气泡引起的磁场波动，重建电解过程中的气泡分布和电导率图，并实现了比传统方法更优异的性能。

    

    电解是环保的氢气生产过程中的关键，但是过程中产生的气泡会阻碍反应，降低电池效率，增加能量消耗。此外，这些气泡会导致电池内部的电导率发生变化，从而导致周围产生感应磁场的变化。因此，利用外部磁传感器测量这些气泡引起的磁场波动，并求解Biot-Savart定律的反问题，可以估计电池内的电导率，从而得到气泡的大小和位置。然而，仅凭几个感应磁场测量值确定高分辨率电导率图是一个病态反问题。为了克服这个问题，我们利用可逆神经网络（INN）重建电导率场。我们的定性结果和使用随机误差扩散的定量评估表明，与Tikhonov正则化相比，INN具有更优异的性能。

    Electrolysis is crucial for eco-friendly hydrogen production, but gas bubbles generated during the process hinder reactions, reduce cell efficiency, and increase energy consumption. Additionally, these gas bubbles cause changes in the conductivity inside the cell, resulting in corresponding variations in the induced magnetic field around the cell. Therefore, measuring these gas bubble-induced magnetic field fluctuations using external magnetic sensors and solving the inverse problem of Biot-Savart Law allows for estimating the conductivity in the cell and, thus, bubble size and location. However, determining high-resolution conductivity maps from only a few induced magnetic field measurements is an ill-posed inverse problem. To overcome this, we exploit Invertible Neural Networks (INNs) to reconstruct the conductivity field. Our qualitative results and quantitative evaluation using random error diffusion show that INN achieves far superior performance compared to Tikhonov regularizatio
    
[^191]: 大型语言模型是有效的文本排序器，具有两两排名提示

    Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting. (arXiv:2306.17563v1 [cs.IR])

    [http://arxiv.org/abs/2306.17563](http://arxiv.org/abs/2306.17563)

    本论文提出了一种名为PRP的新技术，通过使用两两排名提示来显著减轻大型语言模型（LLM）的负担，并首次在标准基准测试中实现了最先进的排名性能。

    

    使用大型语言模型（LLM）通过直接将查询和候选文档输入提示进行文档排序是一个有趣且实用的问题。然而，迄今为止取得了有限的成功，研究人员发现很难在基准数据集上超越精调基准排序器。我们分析了现有方法使用的点对点和列表排序提示，并认为现成的LLM没有完全理解这些排序公式，可能是由于LLM的训练方式的特性。在本文中，我们提出了一种名为两两排名提示（PRP）的新技术，大大减轻了LLM的负担。我们的结果是文献中首次使用中等规模的开源LLM在标准基准测试中实现了最先进的排名性能。在TREC-DL2020上，基于20B参数的Flan-UL2模型的PRP超过了文献中基于商业黑盒GPT-4的最佳方法。

    Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, there has been limited success so far, as researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these ranking formulations, possibly due to the nature of how LLMs are trained. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL2020, PRP based on the Flan-UL2 model with 20B parameters outperforms the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that ha
    
[^192]: 差分隐私分布式估计和学习

    Differentially Private Distributed Estimation and Learning. (arXiv:2306.15865v1 [cs.LG])

    [http://arxiv.org/abs/2306.15865](http://arxiv.org/abs/2306.15865)

    本文研究了在网络环境中的分布式估计和学习问题，通过交换私有观测信息，代理可以集体估计未知数量，而保护隐私。通过线性聚合方案和差分隐私（DP）调整的随机化方案，本研究提出了一种能够在保证隐私的同时高效组合观测数据的算法。

    

    我们研究了在网络环境中的分布式估计和学习问题，其中代理通过交换信息来估计从其私下观察的样本中未知的统计属性。通过交换私有观测信息，代理可以集体估计未知数量，但他们也面临隐私风险。我们的聚合方案的目标是在时间和网络中高效地组合观测数据，同时满足代理的隐私需求，而不需要任何超越他们本地附近的协调。我们的算法使参与的代理能够从离线或随时间在线获取的私有信号中估计完整的充分统计量，并保护其信号和网络附近的隐私。这是通过线性聚合方案和调整的随机化方案实现的，将噪声添加到交换的估计数据中以满足差分隐私（DP）。

    We study distributed estimation and learning problems in a networked environment in which agents exchange information to estimate unknown statistical properties of random variables from their privately observed samples. By exchanging information about their private observations, the agents can collectively estimate the unknown quantities, but they also face privacy risks. The goal of our aggregation schemes is to combine the observed data efficiently over time and across the network, while accommodating the privacy needs of the agents and without any coordination beyond their local neighborhoods. Our algorithms enable the participating agents to estimate a complete sufficient statistic from private signals that are acquired offline or online over time, and to preserve the privacy of their signals and network neighborhoods. This is achieved through linear aggregation schemes with adjusted randomization schemes that add noise to the exchanged estimates subject to differential privacy (DP
    
[^193]: 针对异常检测的目标塌缩正则化自编码器：中心的黑洞

    Targeted collapse regularized autoencoder for anomaly detection: black hole at the center. (arXiv:2306.12627v1 [cs.LG])

    [http://arxiv.org/abs/2306.12627](http://arxiv.org/abs/2306.12627)

    本文提出一种在自编码器的损失函数中添加一个轻量级的约束项，用于解决传统自编码器在异常检测中的不足，并取得了良好的表现。

    

    自编码器已广泛用于最近的异常检测技术开发中。它们的应用前提是在正常训练数据上训练自编码器后，异常输入将表现出显著的重构误差。因此，这使得正常和异常样本之间有了明显的区别。然而，在实践中观察到，自编码器可以一定程度上泛化到正常类之外，并在一些异常样本上实现较小的重构误差。为了改善性能，各种技术提出了其他组件和更复杂的训练程序。在这项工作中，我们提出了一个非常简单的替代方法：不是添加神经网络组件、涉及计算和繁琐的训练，而是通过在潜在空间中调节表示的范数，用一个计算简单的项来补充重构损失。我们方法的简单性最小化了复杂性。

    Autoencoders have been extensively used in the development of recent anomaly detection techniques. The premise of their application is based on the notion that after training the autoencoder on normal training data, anomalous inputs will exhibit a significant reconstruction error. Consequently, this enables a clear differentiation between normal and anomalous samples. In practice, however, it is observed that autoencoders can generalize beyond the normal class and achieve a small reconstruction error on some of the anomalous samples. To improve the performance, various techniques propose additional components and more sophisticated training procedures. In this work, we propose a remarkably straightforward alternative: instead of adding neural network components, involved computations, and cumbersome training, we complement the reconstruction loss with a computationally light term that regulates the norm of representations in the latent space. The simplicity of our approach minimizes th
    
[^194]: 图像分类的量子机器学习方法

    Quantum machine learning for image classification. (arXiv:2304.09224v1 [quant-ph])

    [http://arxiv.org/abs/2304.09224](http://arxiv.org/abs/2304.09224)

    本论文提出了两种混合量子-经典的神经网络模型用于图像分类，其中包括一个具有并行量子层的神经网络和一个具有量子卷积层的神经网络。其中一个混合量子方法在MNIST数据集上展现了超过99%的惊人准确率。

    

    图像识别和分类是各行各业中多种实际应用的基本任务，是现代世界中至关重要的领域。近年来，尤其是神经网络，机器学习模型已成为解决这些问题的强大工具。然而，通过混合量子-经典方法利用量子效应可以进一步增强传统经典模型的能力。在这里，我们提出两种混合量子-经典模型：一个具有并行量子层的神经网络和一个具有量子卷积层的神经网络，来解决图像分类问题。我们其中一个混合量子方法在MNIST数据集上展现了超过99%的惊人准确率。值得注意的是，在我们提出的量子电路中，所有可变参数都是可训练的，并且我们将量子部分分成多个并行可变量子电路以提高神经网络学习的效率。

    Image recognition and classification are fundamental tasks with diverse practical applications across various industries, making them critical in the modern world. Recently, machine learning models, particularly neural networks, have emerged as powerful tools for solving these problems. However, the utilization of quantum effects through hybrid quantum-classical approaches can further enhance the capabilities of traditional classical models. Here, we propose two hybrid quantum-classical models: a neural network with parallel quantum layers and a neural network with a quanvolutional layer, which address image classification problems. One of our hybrid quantum approaches demonstrates remarkable accuracy of more than 99% on the MNIST dataset. Notably, in the proposed quantum circuits all variational parameters are trainable, and we divide the quantum part into multiple parallel variational quantum circuits for efficient neural network learning. In summary, our study contributes to the ong
    
[^195]: 在大规模开放挑战中评估手势生成：GENEA Challenge 2022的研究报告(arXiv:2303.08737v1 [cs.HC])

    Evaluating gesture-generation in a large-scale open challenge: The GENEA Challenge 2022. (arXiv:2303.08737v1 [cs.HC])

    [http://arxiv.org/abs/2303.08737](http://arxiv.org/abs/2303.08737)

    本文介绍了GENEA Challenge 2022的研究结果，该比赛旨在基准测试基于数据驱动的自动共同语言手势生成。使用具有相同语音和动作的数据集，众多参赛团队的手势生成系统在几个大型用户研究中得到了评估，因此能够进行直接比较。

    

    本文报道了第二届GENEA Challenge，对基于数据驱动的自动共同语言手势生成进行了基准测试。参赛团队使用相同的语音和运动数据集构建手势生成系统。这些系统生成的动作使用标准化的可视化管道渲染为视频，并在几个大型众包用户研究中进行评估。与比较不同研究论文时不同的是，这里的结果差异仅由于方法之间的差异，从而实现了系统之间的直接比较。该数据集基于18小时的全身动作捕捉，包括手指，并记录了不同人物参与双人 对话。十个团队参加了分为全身和上半身肢体表达的两个层次的挑战。对于每个层次，我们评估了手势运动的人类相似度和其对特定语音信号的适用性。我们的评估将人类相似度与手势适用性解藕开，这一点一直是困难的。

    This paper reports on the second GENEA Challenge to benchmark data-driven automatic co-speech gesture generation. Participating teams used the same speech and motion dataset to build gesture-generation systems. Motion generated by all these systems was rendered to video using a standardised visualisation pipeline and evaluated in several large, crowdsourced user studies. Unlike when comparing different research papers, differences in results are here only due to differences between methods, enabling direct comparison between systems. The dataset was based on 18 hours of full-body motion capture, including fingers, of different persons engaging in a dyadic conversation. Ten teams participated in the challenge across two tiers: full-body and upper-body gesticulation. For each tier, we evaluated both the human-likeness of the gesture motion and its appropriateness for the specific speech signal. Our evaluations decouple human-likeness from gesture appropriateness, which has been a difficu
    
[^196]: 预训练GAN和VAE的特征消除

    Feature Unlearning for Pre-trained GANs and VAEs. (arXiv:2303.05699v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05699](http://arxiv.org/abs/2303.05699)

    本文提出了一种从预训练的GAN和VAE模型中消除特定特征的方法，并通过实验证明了方法的有效性。

    

    我们解决了从预训练的图像生成模型（GAN和VAE）中消除特征的问题。与常见的消除任务不同，我们的目标是从预训练的生成模型中消除特定的特征，例如面部图像中的发型。由于目标特征仅出现在图像的局部区域中，从预训练模型中消除整个图像可能导致失去图像剩余区域中的其他细节。为了指定要消除的特征，我们收集包含目标特征的随机生成图像。然后，我们识别与目标特征对应的潜在表示，并使用表示来微调预训练模型。通过对MNIST和CelebA数据集进行实验，我们展示了成功删除目标特征同时保持原始模型的可信度。进一步的对抗性攻击实验证明了消除后的模型的鲁棒性。

    We tackle the problem of feature unlearning from a pre-trained image generative model: GANs and VAEs. Unlike a common unlearning task where an unlearning target is a subset of the training set, we aim to unlearn a specific feature, such as hairstyle from facial images, from the pre-trained generative models. As the target feature is only presented in a local region of an image, unlearning the entire image from the pre-trained model may result in losing other details in the remaining region of the image. To specify which features to unlearn, we collect randomly generated images that contain the target features. We then identify a latent representation corresponding to the target feature and then use the representation to fine-tune the pre-trained model. Through experiments on MNIST and CelebA datasets, we show that target features are successfully removed while keeping the fidelity of the original models. Further experiments with an adversarial attack show that the unlearned model is mo
    
[^197]: 无数据情况下对黑盒模型进行防御的方法

    Data-free Defense of Black Box Models Against Adversarial Attacks. (arXiv:2211.01579v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01579](http://arxiv.org/abs/2211.01579)

    本研究提出了一种无数据情况下对黑盒模型进行防御的方法，通过生成模型构建合成数据，并使用模型窃取技术训练替代模型网络，同时采用小波噪声去除器（WNR）减少对抗性污染。

    

    许多公司通过API仅将训练好的深度模型作为黑盒暴露给第三方用户，以保护模型的细节（如架构、学习权重、训练细节等）。本研究提出了一种针对黑盒模型在无数据情况下进行对抗攻击的新型防御机制。我们通过生成模型构建合成数据，并使用模型窃取技术训练替代模型网络。为了最小化扰动样本上的对抗性污染，我们提出了“小波噪声去除器”(WNR)，它在输入图像上执行离散小波分解，并仅选择我们的“小波系数选择模块”(WCSM)确定的少数重要系数。为了在通过WNR去除噪声后恢复图像的高频内容，我们进一步训练了一个“再生器”网络，目标是恢复系数。

    Several companies often safeguard their trained deep models (i.e., details of architecture, learnt weights, training details etc.) from third-party users by exposing them only as black boxes through APIs. Moreover, they may not even provide access to the training data due to proprietary reasons or sensitivity concerns. In this work, we propose a novel defense mechanism for black box models against adversarial attacks in a data-free set up. We construct synthetic data via generative model and train surrogate network using model stealing techniques. To minimize adversarial contamination on perturbed samples, we propose 'wavelet noise remover' (WNR) that performs discrete wavelet decomposition on input images and carefully select only a few important coefficients determined by our 'wavelet coefficient selection module' (WCSM). To recover the high-frequency content of the image after noise removal via WNR, we further train a 'regenerator' network with the objective of retrieving the coeffi
    
[^198]: 基于机器学习的多阶段系统对真实患者数据进行视力预测

    Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System. (arXiv:2204.11970v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2204.11970](http://arxiv.org/abs/2204.11970)

    本研究提供了一种使用机器学习技术开发预测模型的多阶段系统，可高精度预测三种眼疾患者的视力变化，并辅助眼科医生进行临床决策和患者咨询。

    

    现实生活中，眼科学中的玻璃体手术药物治疗是治疗年龄相关性黄斑变性（AMD）、糖尿病性黄斑水肿（DME）和视网膜静脉阻塞（RVO）相关疾病的一种普遍治疗方法。然而，在真实世界的情况下，由于数据的异质性和不完整性，患者往往会在多年时间内失去视力，尽管接受治疗。本文采用多种IT系统，提出了一种用于研究的数据集成流程，该流程融合了德国一家最佳医疗保健医院的眼科部门的不同IT系统。经过使用机器学习技术开发预测模型，我们实现了对患者视力的预测。我们的结果表明，我们的系统可以为三种疾病的预测提供高准确性。此外，我们还展示了我们的系统可以作为工具，辅助眼科医生进行临床决策和患者咨询。

    In ophthalmology, intravitreal operative medication therapy (IVOM) is a widespread treatment for diseases related to the age-related macular degeneration (AMD), the diabetic macular edema (DME), as well as the retinal vein occlusion (RVO). However, in real-world settings, patients often suffer from loss of vision on time scales of years despite therapy, whereas the prediction of the visual acuity (VA) and the earliest possible detection of deterioration under real-life conditions is challenging due to heterogeneous and incomplete data. In this contribution, we present a workflow for the development of a research-compatible data corpus fusing different IT systems of the department of ophthalmology of a German maximum care hospital. The extensive data corpus allows predictive statements of the expected progression of a patient and his or her VA in each of the three diseases. We found out for the disease AMD a significant deterioration of the visual acuity over time. Within our proposed m
    

