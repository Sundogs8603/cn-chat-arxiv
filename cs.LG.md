# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Algebraic, Topological, and Mereological Foundations of Existential Granules.](http://arxiv.org/abs/2308.16157) | 本研究从代数、拓扑和细部学的角度创造了新的存在性颗粒概念，并刻画了其特征。这些颗粒首先确定自己，然后与环境互动，并且适用于多种颗粒计算理论框架。研究结果对算法开发、分类问题应用和方法推广的数学基础具有重要意义。 |
| [^2] | [Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI.](http://arxiv.org/abs/2308.16150) | 本文介绍了一种名为遮蔽模态循环与条件扩散的方法，该方法能够对多模态MRI中的异常进行分割。方法基于循环模态转换和条件扩散的思想，能够检测到训练中未遇到的异常模式。 |
| [^3] | [Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models.](http://arxiv.org/abs/2308.16149) | Jais和Jais-chat是新的以阿拉伯语为中心的开放式生成式大型语言模型，具有13亿参数，在阿拉伯语方面表现出优异的知识和推理能力，并且在英语方面也具有竞争力。这些模型的发布旨在促进阿拉伯语LLMs的研究。 |
| [^4] | [MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision.](http://arxiv.org/abs/2308.16139) | MedShapeNet是一个大规模的三维医学形状数据集，作为一种对于常用形状基准的替代品，为计算机视觉研究提供了新的选择。 |
| [^5] | [Spatial Graph Coarsening: Weather and Weekday Prediction with London's Bike-Sharing Service using GNN.](http://arxiv.org/abs/2308.16122) | 本研究使用图神经网络预测伦敦的天气和工作日，通过引入图特征和地理接近性的图粗化操作，提高了预测的准确性。 |
| [^6] | [survex: an R package for explaining machine learning survival models.](http://arxiv.org/abs/2308.16113) | survex是一个R软件包，通过应用可解释的人工智能技术，提供了一个连贯的框架来解释任何生存模型，可以改进模型，提高透明度和责任感。 |
| [^7] | [Advanced Deep Regression Models for Forecasting Time Series Oil Production.](http://arxiv.org/abs/2308.16105) | 本研究开发了基于序列卷积和LSTM单元的高级数据驱动回归模型，用于预测时间序列石油产量。这些模型能够捕捉到时间序列数据中的历史模式，提高了预测的准确性。 |
| [^8] | [Application of Zone Method based Machine Learning and Physics-Informed Neural Networks in Reheating Furnaces.](http://arxiv.org/abs/2308.16089) | 本文将经典的Hottel区域方法与机器学习和深度学习相结合，利用生成的数据进行加热炉控制系统的训练，为基础产业的可持续制造和能耗降低目标做出贡献。 |
| [^9] | [Consensus of state of the art mortality prediction models: From all-cause mortality to sudden death prediction.](http://arxiv.org/abs/2308.16067) | 本研究共识了最新的死亡预测模型，从全因死亡到突发死亡预测，展示了结合医疗历史、血液检查、药物处方和住院治疗可以预测突发死亡的风险增加的有效性。 |
| [^10] | [Conti Inc.: Understanding the Internal Discussions of a large Ransomware-as-a-Service Operator with Machine Learning.](http://arxiv.org/abs/2308.16061) | Conti公司的聊天记录泄露给我们提供了了解勒索软件服务运营商内部运作的机会。使用机器学习技术和可视化策略，研究发现业务、技术、内部任务管理、恶意软件和客户服务是Conti成员讨论的主要主题。 |
| [^11] | [A Parameter-Free Two-Bit Covariance Estimator with Improved Operator Norm Error Rate.](http://arxiv.org/abs/2308.16059) | 提出了一种无需参数的二位协方差估计器，通过使用变化的抖动尺度，解决了在协方差矩阵对角线主导情况下估计器与样本协方差之间的算子范数误差差距以及依赖未知参数的抖动尺度问题。 |
| [^12] | [Low-Rank Multitask Learning based on Tensorized SVMs and LSSVMs.](http://arxiv.org/abs/2308.16056) | 本文提出了一种基于张量化支持向量机和最小二乘支持向量机的低秩多任务学习方法，通过高阶张量表示任务之间的关系，并利用交替优化和Lagrangian函数解决相关的凸问题。 |
| [^13] | [PAVI: Plate-Amortized Variational Inference.](http://arxiv.org/abs/2308.16022) | PAVI是一种板块化的变分推断方法，能够高效地处理大规模人口研究，通过共享参数化和学习加速训练变分分布，实现了在大规模分层问题上的表达力强和简明的结果。 |
| [^14] | [EnsembleFollower: A Hybrid Car-Following Framework Based On Reinforcement Learning and Hierarchical Planning.](http://arxiv.org/abs/2308.16008) | EnsembleFollower是一个采用分层规划和强化学习的混合车辆跟驰框架，能够实现先进的类人车辆跟驰。 |
| [^15] | [FPTQ: Fine-grained Post-Training Quantization for Large Language Models.](http://arxiv.org/abs/2308.15987) | 本研究提出了一种面向大型语言模型的细粒度训练后量化方法，结合了W4A8和W8A8两种方案的优点，通过逐层激活量化和细粒度权重量化来解决性能下降的问题。 |
| [^16] | [Learning Structure-from-Motion with Graph Attention Networks.](http://arxiv.org/abs/2308.15984) | 本文通过使用图注意力网络，将传统的学习结构运动问题中的子问题替换为直接学习模型，实现了对新序列的快速推断，提高了结构运动的学习性能。 |
| [^17] | [Demo: A Digital Twin of the 5G Radio Access Network for Anomaly Detection Functionality.](http://arxiv.org/abs/2308.15973) | 该论文提出了一种基于数字孪生的5G无线接入网络异常检测功能，使得5G系统能够实时检测用户连接异常并主动进行资源控制和连接恢复的决策。 |
| [^18] | [Jaccard-constrained dense subgraph discovery.](http://arxiv.org/abs/2308.15936) | 本研究提出了Jaccard约束下的密集子图发现问题，并给出了两种算法来解决该问题。 |
| [^19] | [LLaSM: Large Language and Speech Model.](http://arxiv.org/abs/2308.15930) | LLaSM是一个大型语言和语音模型，具有跨模态对话能力，通过遵循语音和语言指令，提供了一种方便自然的人机交互方式。 |
| [^20] | [Cyclophobic Reinforcement Learning.](http://arxiv.org/abs/2308.15911) | 本文提出一种新的周期性排斥增强学习方法，通过避免循环来惩罚冗余而不奖励新颖性，在奖励稀疏的环境中能取得优异的结果。 |
| [^21] | [Thermodynamic Computing via Autonomous Quantum Thermal Machines.](http://arxiv.org/abs/2308.15905) | 通过自主量子热机实现了热力学计算模型。通过热流进行计算，可实现任何线性可分离函数，并可扩展到神经元网络执行任何 needed功能。 |
| [^22] | [Beyond Traditional Neural Networks: Toward adding Reasoning and Learning Capabilities through Computational Logic Techniques.](http://arxiv.org/abs/2308.15899) | 通过计算逻辑技术，本研究提出了一种超越传统神经网络的方法，将神经网络和符号推理相结合，解决了深度学习模型在高质量训练数据、透明性和鲁棒性方面的限制。同时，通过改进知识注入过程，将机器学习和逻辑元素融入多主体系统。 |
| [^23] | [On the Potential of CLIP for Compositional Logical Reasoning.](http://arxiv.org/abs/2308.15887) | 本文研究了使用CLIP进行组合逻辑推理的潜力，并发现通常配置的CLIP无法进行这种推理。 |
| [^24] | [Towards One-Shot Learning for Text Classification using Inductive Logic Programming.](http://arxiv.org/abs/2308.15885) | 本文使用归纳逻辑编程的方法实现了一次性的文本分类，并通过使用常识背景知识和元解释学习框架，可以从少量训练样本中学习分类规则，且复杂度更高的样本可以达到更高的准确性。 |
| [^25] | ["Would life be more interesting if I were in AI?" Answering Counterfactuals based on Probabilistic Inductive Logic Programming.](http://arxiv.org/abs/2308.15883) | 该论文提出了一种使用概率归纳逻辑编程回答反事实问题的方法，在学习程序结构时考虑了因果机制，使得可以进行反事实推理。 |
| [^26] | [Minimum Width for Deep, Narrow MLP: A Diffeomorphism and the Whitney Embedding Theorem Approach.](http://arxiv.org/abs/2308.15873) | 本文给出了深度、窄型多层感知机通用逼近性的最小宽度的上界，并通过微分同胚和惠特尼嵌入定理的证明进行了验证。 |
| [^27] | [Domain Generalization without Excess Empirical Risk.](http://arxiv.org/abs/2308.15856) | 本论文提出了一种解决域泛化中过量风险问题的方法，通过在保证经验风险最优的约束下最小化惩罚，避免了惩罚对经验风险优化的影响。 |
| [^28] | [MSGNN: Multi-scale Spatio-temporal Graph Neural Network for Epidemic Forecasting.](http://arxiv.org/abs/2308.15840) | MSGNN是一种多尺度时空图神经网络，通过创新的多尺度视图和图学习模块，有效地解决了现有GNN模型在保留远距离连接和多尺度流行病模式上的局限性。它在传染病预测中具有重要的应用价值。 |
| [^29] | [Adaptive Lasso, Transfer Lasso, and Beyond: An Asymptotic Perspective.](http://arxiv.org/abs/2308.15838) | 本文研究了自适应Lasso和转移Lasso的理论性质，通过对转移Lasso的渐进性质进行理论研究，分析了它与自适应Lasso的区别，并提出了一种新的方法，将两者的优势进行了融合并补偿了他们的弱点。 |
| [^30] | [Federated Two Stage Decoupling With Adaptive Personalization Layers.](http://arxiv.org/abs/2308.15821) | 该论文提出了一种具有自适应个性化层的联邦化两阶段解耦方法，通过将同质客户端聚类到同一组的方式来提高联邦学习的性能，并解决了数据异质性和聚类时间选择的问题。 |
| [^31] | [Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models.](http://arxiv.org/abs/2308.15812) | 本研究分析了对于对齐和评估大型语言模型而言，设计反馈选择是评分还是排名对结果的影响。研究发现评分和排名所推断出的偏好存在不一致问题，并且注释者的偏见也会影响结果。同时，研究还发现反馈协议的选择也对评估结果有显著影响。 |
| [^32] | [HAlf-MAsked Model for Named Entity Sentiment analysis.](http://arxiv.org/abs/2308.15793) | 在命名实体情感分析中，研究了解决基于BERT模型过拟合问题的多种方法，包括一种新颖的技术，在最终预测之前，对给定数据进行额外的掩码实体传递，以便在模型知道预测情感的确切实体和不知道的情况下，可以合并来自模型的逻辑。 |
| [^33] | [FedCiR: Client-Invariant Representation Learning for Federated Non-IID Features.](http://arxiv.org/abs/2308.15786) | FedCiR是一种客户端不变表示学习框架，用于解决联邦学习中的特征偏移问题，通过改进表示和标签之间的互信息项来提取信息且与客户无关的特征。 |
| [^34] | [Split Without a Leak: Reducing Privacy Leakage in Split Learning.](http://arxiv.org/abs/2308.15783) | 本文介绍了一个使用Split Learning和同态加密的混合方法，用于保护隐私的深度学习。研究表明，Split Learning容易受到攻击，因此本文提出了一个新的解决方案来减少隐私泄露。 |
| [^35] | [Efficient and Explainable Graph Neural Architecture Search via Monte-Carlo Tree Search.](http://arxiv.org/abs/2308.15734) | 该论文提出了一种高效且可解释的图神经网络架构搜索方法，名为ExGNAS。它包括适应各种图形的简单搜索空间和能解释决策过程的搜索算法。通过蒙特卡洛树搜索高效地搜索最佳GNN架构。 |
| [^36] | [Fully Embedded Time-Series Generative Adversarial Networks.](http://arxiv.org/abs/2308.15730) | 这篇论文提出了一种完全嵌入的时间序列生成对抗网络（FETSGAN），通过对抗性训练匹配特征空间和低维度采样空间的训练分布，确保合成样本的时间分布不会崩溃。 |
| [^37] | [Surrogate-based Autotuning for Randomized Sketching Algorithms in Regression Problems.](http://arxiv.org/abs/2308.15720) | 本文介绍了如何使用基于替代模型的自动调优方法解决随机化草图算法中的参数选择问题，在随机数值线性代数中取得了接近最优性能的实证结果。 |
| [^38] | [Exploring Deep Learning for Full-disk Solar Flare Prediction with Empirical Insights from Guided Grad-CAM Explanations.](http://arxiv.org/abs/2308.15712) | 本研究通过引入指导的Grad-CAM解释，展示了一个全盘面的深度学习模型在太阳耀斑预测中的应用，并通过定性和定量的评估分析发现了活动区特征与太阳耀斑预测之间的关联。 |
| [^39] | [Speech Wikimedia: A 77 Language Multilingual Speech Dataset.](http://arxiv.org/abs/2308.15710) | Speech Wikimedia是一个包含来自77种语言的大量音频和转录的数据集，适用于训练语音识别、语音翻译和机器翻译模型。 |
| [^40] | [Threshold KNN-Shapley: A Linear-Time and Privacy-Friendly Approach to Data Valuation.](http://arxiv.org/abs/2308.15709) | 本论文研究了数据价值评估面临的隐私挑战，并提出了一种隐私友好的改进方法TKNN-Shapley，该方法在保护隐私的前提下能够评估数据质量，具有较好的隐私-实用性权衡。 |
| [^41] | [Towards a Rigorous Analysis of Mutual Information in Contrastive Learning.](http://arxiv.org/abs/2308.15704) | 本研究探索了对比学习中互信息的严格分析，引入了三种新方法和相关定理，提升了互信息分析的严谨性与实用性。 |
| [^42] | [Fragment and Integrate Network (FIN): A Novel Spatial-Temporal Modeling Based on Long Sequential Behavior for Online Food Ordering Click-Through Rate Prediction.](http://arxiv.org/abs/2308.15703) | 本文提出了一种名为Fragment and Integrate Network (FIN)的新型空间-时间建模方法，用于在线食品订购点击率预测。该方法通过从顺序行为数据中提取多个子序列，分别对每个子序列进行建模，从而捕捉用户的空间-时间偏好。 |
| [^43] | [Training Towards Critical Use: Learning to Situate AI Predictions Relative to Human Knowledge.](http://arxiv.org/abs/2308.15700) | 本文介绍了一种过程导向的适当依赖概念，称为批判使用，旨在帮助人们更好地利用基于人工智能的决策支持。研究通过在儿童虐待筛查领域进行在线实验，发现通过提供特定培训可以支持人们的批判使用能力。 |
| [^44] | [Segmenting mechanically heterogeneous domains via unsupervised learning.](http://arxiv.org/abs/2308.15697) | 本文通过无监督学习方法，探索了基于机器学习的逆分析方法，用于分割具有机械异质性的领域。 |
| [^45] | [CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts.](http://arxiv.org/abs/2308.15690) | 提出了一个用于大豆芽图像处理的名为CongNaMul的数据集，旨在支持图像分类、语义分割、分解和测量等任务。提供了质量分类、语义分割和图像分解的标记，以及5个芽的物理特征供测量使用。 |
| [^46] | [MDTD: A Multi Domain Trojan Detector for Deep Neural Networks.](http://arxiv.org/abs/2308.15673) | MDTD是一种多领域木马检测器，用于在测试时检测深度神经网络中包含木马触发器的输入。MDTD不需要知道触发器嵌入策略，并且适用于不同类型的输入。它利用了输入样本与决策边界的距离来检测木马触发器。 |
| [^47] | [Bridging Distribution Learning and Image Clustering in High-dimensional Space.](http://arxiv.org/abs/2308.15667) | 本文通过利用自编码器编码图像到高维潜空间并使用高斯混合模型进行分布学习，实现了图像聚类。然而，高维空间给聚类算法带来了挑战。 |
| [^48] | [Deep Reinforcement Learning Based Framework for Mobile Energy Disseminator Dispatching to Charge On-the-Road Electric Vehicles.](http://arxiv.org/abs/2308.15656) | 本论文提出了一个基于深度强化学习的移动能量传播器调度框架，用于在道路上为电动车充电，解决了充电期间车辆排队导致的行车效率低的问题，并提出了一种有效的调度策略来确定最佳时间和位置。 |
| [^49] | [Ensuring User-side Fairness in Dynamic Recommender Systems.](http://arxiv.org/abs/2308.15651) | 本文提出了一种名为FADE的端到端框架，通过微调策略动态减轻推荐系统中用户群体之间的性能差异。 |
| [^50] | [A General Recipe for Automated Machine Learning in Practice.](http://arxiv.org/abs/2308.15647) | 本文提出了一个构建通用AutoML系统的参考框架，通过叙事性回顾主要方法，将基本概念提炼出来以支持在单一设计中的应用。 |
| [^51] | [Clustering Without an Eigengap.](http://arxiv.org/abs/2308.15642) | 这个论文介绍了在随机块模型中进行图聚类的新算法，能够恢复大聚类，无论其他聚类的大小，并且对中等大小的聚类提出了新的技术挑战。 |
| [^52] | [Identifying Constitutive Parameters for Complex Hyperelastic Solids using Physics-Informed Neural Networks.](http://arxiv.org/abs/2308.15640) | 本文介绍了一种基于物理信息神经网络（PINNs）的新框架，用于识别软材料在大变形平面应力条件下具有复杂组分行为的材料的组分参数。通过使用多模态的时间相关实验数据训练，我们的模型能够稳健地准确识别不可压缩Arruda-Boyce模型的组分参数。 |
| [^53] | [Hyperbolic Convolutional Neural Networks.](http://arxiv.org/abs/2308.15639) | 双曲卷积神经网络利用双曲空间进行数据嵌入，具有更强大和可解释的模型特性。浅层嵌入构建了层次化嵌入。 |
| [^54] | [Everything Perturbed All at Once: Enabling Differentiable Graph Attacks.](http://arxiv.org/abs/2308.15614) | 提出了一种称为可微分图攻击（DGA）的新攻击方法，在连续松弛和参数化图结构的基础上，通过有效生成攻击的同时消除了昂贵的重新训练过程，与最先进的方法相比具有几乎相当的攻击性能，但训练成本减少6倍。 |
| [^55] | [Mixed Variational Flows for Discrete Variables.](http://arxiv.org/abs/2308.15613) | 本文提出了一种混合方差流方法，用于近似离散分布，通过开发一个离散且保持度量的映射，而不需要连续嵌入。实验证明，与连续嵌入流相比，该方法产生更可靠的近似。 |
| [^56] | [InstaTune: Instantaneous Neural Architecture Search During Fine-Tuning.](http://arxiv.org/abs/2308.15609) | InstaTune是一种在微调阶段即时生成超级网络的方法，可以最小化神经架构搜索所需的时间和计算资源。 |
| [^57] | [Measurement Tampering Detection Benchmark.](http://arxiv.org/abs/2308.15605) | 本文构建了四个文本数据集用于评估测量篡改检测技术，研究人工智能系统操纵测量结果以营造良好结果的问题。虽然展示了优于基准的技术，但还有很大的改进空间。 |
| [^58] | [An Experimental Comparison of Partitioning Strategies for Distributed Graph Neural Network Training.](http://arxiv.org/abs/2308.15602) | 本文研究了分布式图神经网络训练中分区策略的有效性，并探究了不同因素对分区效果的影响。 |
| [^59] | [Can transformers learn the greatest common divisor?.](http://arxiv.org/abs/2308.15594) | 本文研究了小型变形金刚模型计算最大公约数的能力。通过选择合适的训练分布和表示基准，模型可以达到高准确率，并在预测中表现出明确的模式。 |
| [^60] | [Prototype Fission: Closing Set for Robust Open-set Semi-supervised Learning.](http://arxiv.org/abs/2308.15575) | 本文提出了一种名为原型分裂（PF）的方法，通过自动细粒度的潜在空间挖掘将类别的潜在空间分割为紧凑的子空间，从而在半监督学习中更好地拒绝超出分布的样本。 |
| [^61] | [Learning Sequential Information in Task-based fMRI for Synthetic Data Augmentation.](http://arxiv.org/abs/2308.15564) | 本文提出了一种方法，通过生成合成fMRI序列来增强训练数据集，以解决医学图像分析中训练数据不足的问题。通过适应α-GAN结构和聚合时序信息，合成的任务驱动fMRI可以有效地提供数据增益，并在自闭症谱系障碍(ASD)分类任务中取得了良好的结果。 |
| [^62] | [Glocal Explanations of Expected Goal Models in Soccer.](http://arxiv.org/abs/2308.15559) | 本文提出了预期进球模型的全局解释（介于本地和全局之间的解释），通过使用聚合版本的SHAP值和部分依赖函数，可以对团队和球员水平进行绩效分析和知识提取。 |
| [^63] | [Dimensionality Reduction Using pseudo-Boolean polynomials For Cluster Analysis.](http://arxiv.org/abs/2308.15553) | 该论文介绍了使用伪布尔多项式的惩罚性表达式作为簇分析中的降维方法，能够以竞争性的准确度、可重复性和清晰解释性提取簇。 |
| [^64] | [Pure Exploration under Mediators' Feedback.](http://arxiv.org/abs/2308.15552) | 本研究提出了一种严格推广的传统最优臂识别问题，即中介反馈下的最优臂识别（BAI-MF），通过引入中介者来模拟一些实际决策问题，如离线学习、部分可控环境和人类反馈。 |
| [^65] | [Adversarial Style Transfer for Robust Policy Optimization in Deep Reinforcement Learning.](http://arxiv.org/abs/2308.15550) | 本文提出了一种对抗式风格转移的算法，通过消除对混淆特征的过拟合来提高深度强化学习智能体的泛化能力。这种算法使用了生成器和策略网络，并通过最大-最小博弈的方式进行优化，以找到一个可以泛化到未见环境的鲁棒策略。实验证明，这种算法相比于其他基准算法有更好的性能。 |
| [^66] | [Tuning the perplexity for and computing sampling-based t-SNE embeddings.](http://arxiv.org/abs/2308.15513) | 本文通过采样的方法改进了大数据集下t-SNE嵌入的质量和计算速度。 |
| [^67] | [unORANIC: Unsupervised Orthogonalization of Anatomy and Image-Characteristic Features.](http://arxiv.org/abs/2308.15507) | unORANIC是一种无监督的方法，通过正交化解剖和图像特征，提高了医学图像分析的泛化能力和抗损坏能力。 |
| [^68] | [On the Steganographic Capacity of Selected Learning Models.](http://arxiv.org/abs/2308.15502) | 本研究考虑了学习模型的隐写容量问题，确定了在不影响模型性能的前提下可以覆盖的已训练参数的低阶位数，并分析了每个模型的准确性随低阶位数变化的关系，以及选定模型各层的隐写容量。 |
| [^69] | [Detecting Inactive Cyberwarriors from Online Forums.](http://arxiv.org/abs/2308.15491) | 本研究调查了大型在线论坛中网络战士的活动水平，发现只有少数网络战士是活跃用户，他们在和平时期保持沉默，只在必要时行动。此外，检测不活跃的网络战士比识别活跃的网络战士更具挑战性。研究提供了更好捕捉网络战士行动的方法。 |
| [^70] | [Empirical Study of Straggler Problem in Parameter Server on Iterative Convergent Distributed Machine Learning.](http://arxiv.org/abs/2308.15482) | 本研究通过实验测试了当前阻滞缓解技术在不同迭代收敛机器学习算法上的有效性，并分析了参数服务器策略在并行学习问题中的实验安排。这些研究结果为进一步研究提供了必要的平台。 |
| [^71] | [Online Job Failure Prediction in an HPC System.](http://arxiv.org/abs/2308.15481) | 该论文研究了使用经典的机器学习算法在提交时间上进行作业失败预测，并结合自然语言处理工具来表示作业。该方法可用于优化高性能计算系统管理，提高性能和能源效率。 |
| [^72] | [Policy composition in reinforcement learning via multi-objective policy optimization.](http://arxiv.org/abs/2308.15470) | 通过多目标政策优化方法，我们将预先存在的教师策略引入到强化学习中，证明了教师策略在无形状奖励下能够加速学习过程。我们的智能体成功地组合教师策略并扩展教师的策略以解决任务。 |
| [^73] | [Elucidating the Exposure Bias in Diffusion Models.](http://arxiv.org/abs/2308.15321) | 本文系统地研究了扩散模型中的曝光偏差问题，并提出了一种名为Epsilon Scaling的免训练方法来减轻这一问题。实验结果验证了该方法的有效性。 |
| [^74] | [Noise-Free Sampling Algorithms via Regularized Wasserstein Proximals.](http://arxiv.org/abs/2308.14945) | 本文通过正则化Wasserstein Proximal方法提出了一种无噪声的抽样算法，通过给定的潜势函数确定性地进行粒子演化，并提供了优于传统方法的维度依赖性和速度收敛性能。 |
| [^75] | [Distributionally Robust Statistical Verification with Imprecise Neural Networks.](http://arxiv.org/abs/2308.14815) | 本文提出了一种使用不精确神经网络的分布鲁棒统计验证方法，通过结合主动学习、不确定性量化和神经网络验证，可以在大量的分布上提供对黑盒系统行为的保证。 |
| [^76] | [Context-Aware Composition of Agent Policies by Markov Decision Process Entity Embeddings and Agent Ensembles.](http://arxiv.org/abs/2308.14521) | 该论文提出了一种通过马尔可夫决策过程实体嵌入和代理集合的方法，以上下文感知地组合代理策略，以在复杂且动态变化的环境中优化执行活动。 |
| [^77] | [EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression.](http://arxiv.org/abs/2308.13399) | 该论文提出了一种无监督的关键词提取方法，通过利用预训练语言模型和信息论方法，在文本中提取具有最高条件熵的短语作为关键词。实验证明，该方法在关键词提取任务上取得了与常用方法相当的结果。 |
| [^78] | [On the Consistency of Average Embeddings for Item Recommendation.](http://arxiv.org/abs/2308.12767) | 本文研究了推荐系统中平均嵌入的一致性，并提出了一种衡量方法。实证结果表明，现实世界的平均嵌入在推荐中一致性较低，为进一步改进现实世界嵌入提供了方向。 |
| [^79] | [Trustworthy Representation Learning Across Domains.](http://arxiv.org/abs/2308.12315) | 本论文首次提出了跨领域的可信表示学习框架，通过包括鲁棒性、隐私、公平性和可解释性等概念，对该研究方向进行了全面的文献综述。 |
| [^80] | [Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Power Analysis and Sample Size Estimation.](http://arxiv.org/abs/2308.11197) | 该研究提供了使用嵌套交叉验证方法的定量证据，并提出了基于机器学习分析进行功效分析的方法。通过对交叉验证方法、特征和模型维度之间的相互作用进行蒙特卡罗模拟，比较了不同交叉验证方法的统计功效和置信度。同时，确定了获得统计显著结果所需的最小样本容量。 |
| [^81] | [A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case.](http://arxiv.org/abs/2308.09884) | 这项研究提出了一种基于Transformer的框架，用于多元时间序列预测，以满足预测学应用案例中剩余寿命预测的需求。 |
| [^82] | [Diversifying AI: Towards Creative Chess with AlphaZero.](http://arxiv.org/abs/2308.09175) | 本研究探索了AI在计算任务中是否可以从创造性决策机制中受益，并通过构建多样化的AI系统团队，在挑战性任务中超越单个AI，通过生成更多的想法，并选择最佳想法。在国际象棋中的实验结果显示，多样化AI系统以不同方式下国际象棋。 |
| [^83] | [Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies.](http://arxiv.org/abs/2308.03188) | 本文总结了最近的研究，对多样化的自我纠正策略进行了分类和分析，以解决大型语言模型中的问题行为。自动化反馈技术被证明是一种可行的方法，可以使基于大型语言模型的解决方案更实用和可部署。 |
| [^84] | [Food Classification using Joint Representation of Visual and Textual Data.](http://arxiv.org/abs/2308.02562) | 本研究提出了一种使用联合表示的多模态分类框架，通过修改版的EfficientNet和Mish激活函数实现图像分类，使用基于BERT的网络实现文本分类。实验结果表明，所提出的网络在图像和文本分类上表现优于其他方法，准确率提高了11.57%和6.34%。比较分析还证明了所提出方法的效率和鲁棒性。 |
| [^85] | [CartiMorph: a framework for automated knee articular cartilage morphometrics.](http://arxiv.org/abs/2308.01981) | CartiMorph是一种自动化膝关节软骨形态学测量的框架，利用深度学习模型进行图像分析，通过定量指标评估了软骨的损失和厚度，并与手动分割的结果进行了比较，结果显示表面法线的厚度映射方法具有较小的误差。 |
| [^86] | [Implicit neural representation for change detection.](http://arxiv.org/abs/2307.15428) | 这项研究提出了一种用于检测不同时间获取的LiDAR点云中变化的无监督方法，通过使用神经场进行连续形状重建，以及高斯混合模型进行变化分类。该方法能够处理不匹配的空间支持和噪声，并在检测能力上取得了显著的提升。 |
| [^87] | [How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges.](http://arxiv.org/abs/2307.15016) | 本研究探索了Google Bard在理解和解释文本问题条件下的视觉数据方面的能力，并发现Bard在各种视觉场景中仍然存在困境，这凸显出在视觉理解方面存在重要的差距。 |
| [^88] | [What do neural networks learn in image classification? A frequency shortcut perspective.](http://arxiv.org/abs/2307.09829) | 本研究通过在合成数据集和自然图像上的实验，发现神经网络在图像分类中倾向于找到简单的解决方案，并且在训练过程中首先学到的内容取决于最具有区分性的频率特征，这可以是低频或高频。同时，研究也提出了一种度量标准和方法来识别频率快捷路径，并验证了其在不同类型的图像上的有效性。 |
| [^89] | [Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models.](http://arxiv.org/abs/2307.08303) | 本论文提出了一种使用软提示调优来增强密集检索的方法（SPTAR）。通过优化任务特定的软提示并利用大型语言模型为未标记的文档生成弱查询，可以提高零样本和少样本的密集检索模型的性能。 |
| [^90] | [The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence.](http://arxiv.org/abs/2307.07522) | 生成型人工智能和大型语言模型可能为基础科学的发现提供机会，通过其自主生成假设和探索假设空间的闭环方法，加速科学发现的进程。 |
| [^91] | [Classifying World War II Era Ciphers with Machine Learning.](http://arxiv.org/abs/2307.00501) | 本研究使用机器学习和深度学习对二战时期的密码进行分类。结果表明，在最现实的情境下，无论使用何种密码，我们的模型都能达到较高的准确性。 |
| [^92] | [Cancellation-Free Regret Bounds for Lagrangian Approaches in Constrained Markov Decision Processes.](http://arxiv.org/abs/2306.07001) | 本文提出了一种创新的模型驱动的双重算法OptAug-CMDP，用于约束马尔可夫决策过程（CMDPs），解决了原先算法中安全性问题的缺陷，证明了其遗憾值优秀。 |
| [^93] | [A Differential Testing Framework to Evaluate Image Recognition Model Robustness.](http://arxiv.org/abs/2306.06208) | 本文提出了一种差分测试框架，用于评估图像识别模型鲁棒性。该框架通过使用一组参考图像并扰动计算环境，可确定模型性能是否受到计算环境变化的影响。 |
| [^94] | [Fault Localization for Framework Conversions of Image Recognition Models.](http://arxiv.org/abs/2306.06157) | 本文提出针对深度学习框架转换中出现的模型崩溃和输出标签差异的故障定位和修复方法，成功修复多个图像识别模型跨多个深度学习框架的转换错误。 |
| [^95] | [HypLL: The Hyperbolic Learning Library.](http://arxiv.org/abs/2306.06154) | HypLL是一个使用希亚空间的深度学习库，基于PyTorch，旨在使其易于使用，搭建希亚网络模块，特别适用于处理层次化数据和使用少量嵌入维度，是一种新的、开放的研究方向。 |
| [^96] | [Pre-trained transformer for adversarial purification.](http://arxiv.org/abs/2306.01762) | 本文提出了一个快速防御对抗性攻击的方案RaPiD（Rapid Plug-in Defender），通过预训练的Transformer微调来提纯对抗样本，使其逼近清洁数据分布，实验结果表明，在有限数据情况下，该方法优于最先进的方法。 |
| [^97] | [Geometric Algebra Transformers.](http://arxiv.org/abs/2305.18415) | 本文介绍了一种通用架构几何代数变换器（GATr），用于解决几何数据问题。GATr使用投影几何代数表示输入输出和状态，具有可缩放性、表达性、多功能性。在n体建模和机器人规划的实验中，GATr相对于非几何基线表现出强大的改进。 |
| [^98] | [On progressive sharpening, flat minima and generalisation.](http://arxiv.org/abs/2305.14683) | 本文提出了一种用损失黑塞矩阵和输入-输出雅克比矩阵联系起来的假设，量化了模型的输入-输出雅克比矩阵近似其在数据分布上的利普西茨范数的程度，并推导出了一个基于经验雅克比矩阵的新的泛化界，给出了关于进化磨锋和平坦极小的泛化性质的新解释。 |
| [^99] | [What You Hear Is What You See: Audio Quality Metrics From Image Quality Metrics.](http://arxiv.org/abs/2305.11582) | 该研究探讨了利用图像感知度量方法来评估音频信号的可行性，并通过定制一个心理声学合理结构的度量方法来解决声音信号的特殊性，并在音乐数据集上展现出了令人满意的结果。 |
| [^100] | [Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility.](http://arxiv.org/abs/2305.10235) | 本研究是一项关于大型语言模型方面的实证研究，对主流语言模型进行了大量查询和分析，结果发现这些模型存在着鲁棒性、一致性和可信性方面的潜在风险。 |
| [^101] | [MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers.](http://arxiv.org/abs/2305.09438) | 本文提出了一种基于Transformer模型的新方法MPI-rical，通过对大量代码片段进行训练实现自动化MPI代码生成，使并行化成为可能。 |
| [^102] | [Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models.](http://arxiv.org/abs/2305.08854) | 该论文提出了一种新颖的模型，利用扩散模型生成逼真的笑脸序列，以填补非语言交流领域的研究空白。与传统方法相比，该模型在所有指标上都取得了最先进的性能。 |
| [^103] | [GAMIVAL: Video Quality Prediction on Mobile Cloud Gaming Content.](http://arxiv.org/abs/2305.02422) | GAMIVAL是一种新型的游戏专用无参考视频质量评估模型，结合了多种优点。在移动云游戏内容的主观质量评估数据库上进行测试，表现出更好的NR VQA性能。 |
| [^104] | [RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment.](http://arxiv.org/abs/2304.06767) | RAFT框架引入了奖励排名微调方法，用于对齐生成型基础模型，以解决强化学习带来的低效和不稳定性问题。 |
| [^105] | [NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs.](http://arxiv.org/abs/2304.04027) | 该论文提出了一个新的框架：NeBLa，可以从全景放射线图中通过神经啤酒-兰伯特法重建精确的3D口腔结构模型。 |
| [^106] | [Exploring the Benefits of Visual Prompting in Differential Privacy.](http://arxiv.org/abs/2303.12247) | 本文探讨了在差分隐私（DP）中利用视觉提示（VP）构建神经网络分类器的好处。VP与PATE相配合，在隐私预算最小的情况下实现了最先进的隐私-效用平衡，在跨域图像分类中也显示了其优势。此外，消融研究表明VP在DP中具有很好的有效性和贡献。 |
| [^107] | [Eliciting Latent Predictions from Transformers with the Tuned Lens.](http://arxiv.org/abs/2303.08112) | 本文提出了一种改进版的“逻辑透镜”技术——“调谐透镜”，通过训练一个仿射探针，可以将每个隐藏状态解码成词汇分布。这个方法被应用于各种自回归语言模型上，比逻辑透镜更具有预测性、可靠性和无偏性，并且通过因果实验验证使用的特征与模型本身类似。同时，本文发现潜在预测的轨迹可以用于高精度地检测恶意输入。 |
| [^108] | [WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis.](http://arxiv.org/abs/2303.07543) | 本论文提出了一种名为WDiscOOD的新型OOD检测方法，其中使用白化线性判别分析将特征投影到判别子空间和残留子空间中，确定OOD分数。在大规模ImageNet-1k基准测试和六个OOD数据集中，WDiscOOD表现出了优越的性能。 |
| [^109] | [Evolutionary Reinforcement Learning: A Survey.](http://arxiv.org/abs/2303.04150) | 这篇文章系统地总结了最新的进化计算方法在解决强化学习中的关键挑战方面所取得的良好性能。 |
| [^110] | [Quantized Low-Rank Multivariate Regression with Random Dithering.](http://arxiv.org/abs/2302.11197) | 本文研究了量子化的低秩多元回归，通过采用均匀量化与随机抖动的方法，提出了约束Lasso和正则化Lasso估计器，实现了最小最优率的估计，同时量化仅对乘法因子略有影响。 |
| [^111] | [G-Signatures: Global Graph Propagation With Randomized Signatures.](http://arxiv.org/abs/2302.08811) | G-Signatures是一种全局图传播方法，使用随机签名实现。通过嵌入图结构信息为潜在空间路径，能够有效提取处理全局图属性。在多个任务中表现出优势。 |
| [^112] | [Alien Coding.](http://arxiv.org/abs/2301.11479) | 这个论文介绍了一种自学习算法，用于合成OEIS序列的程序。该算法通过训练神经机器翻译器学习序列和已发现程序之间的对应关系，并自己发现了超过78000个OEIS序列的程序，有时还开发出非传统的编程方法。 |
| [^113] | [Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2301.08491) | 本文使用多智能体强化学习模拟社会困境中的道德选择，设计了一套道德奖励结构，旨在分析和研究AI代理的道德行为。 |
| [^114] | [Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection.](http://arxiv.org/abs/2212.08108) | 本论文提出了一种基于数据流分析启发的深度学习方法，用于高效漏洞检测。通过设计了DeepDFA框架和嵌入技术，我们实现了对代码语义的更高效捕捉，使得深度学习在漏洞检测中更加有效和高性能。DeepDFA训练时间只需9分钟，且超过了所有非transformer基线模型75倍的性能。 |
| [^115] | [Regression with Label Differential Privacy.](http://arxiv.org/abs/2212.06074) | 本论文研究了在保证标签差分隐私的情况下训练回归模型的任务，并提出了一种最优的标签差分隐私随机化机制，该机制采用了“对箱进行随机响应”的形式，并提供了一种高效算法来找到最优的箱值。 |
| [^116] | [Is Bio-Inspired Learning Better than Backprop? Benchmarking Bio Learning vs. Backprop.](http://arxiv.org/abs/2212.04614) | 本研究对比了反向传播和多个生物启发式算法，发现当未提供整个训练数据集时，生物算法比反向传播表现要好得多。 |
| [^117] | [An exponentially-growing family of universal quantum circuits.](http://arxiv.org/abs/2212.00736) | 该论文介绍了两种新的指数增长的量子机器学习体系结构，能够解决量子机器学习中的贫瘠高原问题，提高了量子编码的表达能力。 |
| [^118] | [RecXplainer: Amortized Attribute-based Personalized Explanations for Recommender Systems.](http://arxiv.org/abs/2211.14935) | RecXplainer提供了一种针对推荐系统的分摊属性个性化解释，以解决用户和开发者之间的信任问题。 |
| [^119] | [Contrastive Credibility Propagation for Reliable Semi-Supervised Learning.](http://arxiv.org/abs/2211.09929) | 对比可信度传播采用迭代的传导式伪标签细化，将半监督学习和嘈杂标签学习统一，可在各种数据场景中可靠地超过有监督基准。 |
| [^120] | [Diffiner: A Versatile Diffusion-based Generative Refiner for Speech Enhancement.](http://arxiv.org/abs/2210.17287) | Diffiner是一种基于DNN的生成细化器，可用于改善经过SE方法预处理后的感知语音质量。它可以应用于各种SE方法，且具有高度的模块化潜力。 |
| [^121] | [E-MCTS: Deep Exploration in Model-Based Reinforcement Learning by Planning with Epistemic Uncertainty.](http://arxiv.org/abs/2210.13455) | 本文提出了一种新的方法E-MCTS，通过在MCTS预测中应用表观不确定性估计，实现了模型基强化学习中的深度探索，以及规划探索策略。通过实验证明这种方法在成功的表观不确定性估计和深度探索方面表现优异。 |
| [^122] | [SignReLU neural network and its approximation ability.](http://arxiv.org/abs/2210.10264) | 本文研究了一种名为SignReLU的不同激活函数对深度神经网络逼近能力的影响，结果表明SignReLU网络在逼近性能方面优于有理数和ReLU网络。 |
| [^123] | [Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning.](http://arxiv.org/abs/2209.14624) | 本论文研究了神经网络修剪中是否需要复杂性，并通过与全局幅度修剪进行比较发现，原始的全局幅度修剪方法优于其他最先进的修剪技术。 |
| [^124] | [Variationally Mimetic Operator Networks.](http://arxiv.org/abs/2209.12871) | 这项工作提出了一种新的算子网络架构，用于近似解决偏微分方程。这种架构模拟了从近似变分或弱形式问题中获得的数值解的形式，可以提高解的准确性。 |
| [^125] | [Deep neural networks on diffeomorphism groups for optimal shape reparameterization.](http://arxiv.org/abs/2207.11141) | 本文提出了一种基于深度神经网络在微分同胚群上进行形状重新参数化的方法，通过构造近似的保持方向的微分同胚，并证明了其具有普遍逼近性质和Lipschitz常数的界限。 |
| [^126] | [Compressive Fourier collocation methods for high-dimensional diffusion equations with periodic boundary conditions.](http://arxiv.org/abs/2206.01255) | 本研究提出了一种压缩傅里叶色散方法，用于解决定义在高维周期边界条件域上的扩散方程。该方法利用压缩感知和稀疏恢复技术，通过在蒙特卡罗采样上近似解的傅里叶系数，有效地克服了维度诅咒的影响。 |
| [^127] | [Case-Aware Adversarial Training.](http://arxiv.org/abs/2204.09398) | 本文提出了一种通用且高效的对抗性训练改进方案，即案例感知对抗训练（CAT），通过选择最有信息量的对抗性示例进行训练，降低了计算复杂度并保持防御效果。 |
| [^128] | [Solving AC Power Flow with Graph Neural Networks under Realistic Constraints.](http://arxiv.org/abs/2204.07000) | 本文提出了一种使用图神经网络解决考虑实际约束条件下的交流电功率流问题的方法。我们开发了一个框架，通过无监督训练学习交流电功率流的物理约束，从而实现了独立于具体拓扑和供应任务的通用解。我们在中压基准网格上验证了我们的方法的可行性。 |
| [^129] | [Control Theoretic Analysis of Temporal Difference Learning.](http://arxiv.org/abs/2112.14417) | 本研究对Temporal Difference学习算法进行了控制论分析，并引入了一个有限时间的框架，从控制论角度提供了对TD学习机制和强化学习领域的更深入洞察。 |
| [^130] | [Tensor train completion: local recovery guarantees via Riemannian optimization.](http://arxiv.org/abs/2110.03975) | 本论文研究了基于Riemannian优化的张量列补全问题，推导了正交投影和核相干性的界限，并为具有辅助子空间信息的补全问题提供局部收敛保证。 |
| [^131] | [NeXtQSM -- A complete deep learning pipeline for data-consistent quantitative susceptibility mapping trained with hybrid data.](http://arxiv.org/abs/2107.07752) | NeXtQSM使用混合数据进行训练，通过整合处理步骤，实现了数据一致性定量磁敏感映射计算的新型深度学习流水线。 |
| [^132] | [Rule Generation for Classification: Scalability, Interpretability, and Fairness.](http://arxiv.org/abs/2104.10751) | 这项研究介绍了一种新的基于规则的分类优化方法，利用列生成线性规划实现可扩展性，并通过分配成本系数和引入额外约束解决了解释性和公平性问题。该方法在局部解释性和公平性之间取得了良好的平衡。 |
| [^133] | [Ballistocardiogram artifact removal in simultaneous EEG-fMRI using generative adversarial network.](http://arxiv.org/abs/2011.01710) | 本文提出了一种使用生成对抗网络在同时进行的EEG-fMRI中去除心动揭示现象的方法，通过优化网络模型的局部表示能力，改善整体性能，并获得一个可靠的生成器。该方法不需要额外的参考信号或复杂的硬件设备。 |
| [^134] | [Walking in the Shadow: A New Perspective on Descent Directions for Constrained Minimization.](http://arxiv.org/abs/2006.08426) | 本文提出了一个新的视角来探索降低方向对于约束最小化的影响，展示了最佳的本地降低方向是负梯度投影的方向导数，以及沿着该方向移动相当于投影梯度下降的动力学。另外，Frank-Wolfe顶点对应于使用负梯度方向的“无限”步骤投影到多胞体上。 |
| [^135] | [Coagent Networks Revisited.](http://arxiv.org/abs/2001.10474) | Coagent Networks（共智网络）是指在强化学习环境中协作的随机代理网络。这篇论文重新审视了共智网络理论，提出了执行路径的思想，并通过这一思想实现了对策梯度定理的简洁证明。 |
| [^136] | [On Low-rank Trace Regression under General Sampling Distribution.](http://arxiv.org/abs/1904.08576) | 本文研究了在一般采样分布下的低秩迹回归问题，并引入了一种通用峰值概念，提供了证明迹回归采样算子强凸性和获得非渐进、近乎最优界的方法。同时，将误差界扩展到以交叉验证选择正则化参数的情况下。 |

# 详细

[^1]: 存在性颗粒的代数、拓扑和细部学基础

    Algebraic, Topological, and Mereological Foundations of Existential Granules. (arXiv:2308.16157v1 [cs.LO])

    [http://arxiv.org/abs/2308.16157](http://arxiv.org/abs/2308.16157)

    本研究从代数、拓扑和细部学的角度创造了新的存在性颗粒概念，并刻画了其特征。这些颗粒首先确定自己，然后与环境互动，并且适用于多种颗粒计算理论框架。研究结果对算法开发、分类问题应用和方法推广的数学基础具有重要意义。

    

    在这项研究中，发明了确定自己的存在性颗粒的新概念，并从代数、拓扑和细部学的角度对其进行了刻画。存在性颗粒是那些最初确定自己，并随后与其环境进行交互的颗粒。这个概念的示例，比如颗粒球，在之前其他人的作品中虽然定义不完备、算法建立不充分、理论化不足，但已经在粗糙集和软计算的应用中使用。研究表明它们适合于颗粒计算的多个理论框架（公理化、适应性等）。这种刻画旨在用于算法开发、分类问题的应用以及可能的方法推广的数学基础。此外，还提出了许多开放问题并提供了方向。

    In this research, new concepts of existential granules that determine themselves are invented, and are characterized from algebraic, topological, and mereological perspectives. Existential granules are those that determine themselves initially, and interact with their environment subsequently. Examples of the concept, such as those of granular balls, though inadequately defined, algorithmically established, and insufficiently theorized in earlier works by others, are already used in applications of rough sets and soft computing. It is shown that they fit into multiple theoretical frameworks (axiomatic, adaptive, and others) of granular computing. The characterization is intended for algorithm development, application to classification problems and possible mathematical foundations of generalizations of the approach. Additionally, many open problems are posed and directions provided.
    
[^2]: 使用遮蔽条件扩散的模态循环进行MRI无监督异常分割

    Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI. (arXiv:2308.16150v1 [eess.IV])

    [http://arxiv.org/abs/2308.16150](http://arxiv.org/abs/2308.16150)

    本文介绍了一种名为遮蔽模态循环与条件扩散的方法，该方法能够对多模态MRI中的异常进行分割。方法基于循环模态转换和条件扩散的思想，能够检测到训练中未遇到的异常模式。

    

    无监督异常分割旨在检测与训练过程中处理的任何模式不同的模式，通常称为异常或超出分布的模式，而不提供任何关联的手动分割。由于部署过程中的异常可能导致模型失效，检测异常可以增强模型的可靠性，这在医学成像等高风险领域非常有价值。本文引入了基于遮蔽模态循环与条件扩散（MMCCD）的方法，该方法能够在多模态MRI中分割各种模式的异常。该方法基于两个基本思想。首先，我们提出使用循环模态转换作为启用异常检测的机制。图像转换模型学习组织特异的模态映射，这是组织生理学的特征。因此，这些学习到的映射无法将在训练过程中从未遇到的组织或图像模式进行转换，从而产生错误，使得异常能够被检测到。

    Unsupervised anomaly segmentation aims to detect patterns that are distinct from any patterns processed during training, commonly called abnormal or out-of-distribution patterns, without providing any associated manual segmentations. Since anomalies during deployment can lead to model failure, detecting the anomaly can enhance the reliability of models, which is valuable in high-risk domains like medical imaging. This paper introduces Masked Modality Cycles with Conditional Diffusion (MMCCD), a method that enables segmentation of anomalies across diverse patterns in multimodal MRI. The method is based on two fundamental ideas. First, we propose the use of cyclic modality translation as a mechanism for enabling abnormality detection. Image-translation models learn tissue-specific modality mappings, which are characteristic of tissue physiology. Thus, these learned mappings fail to translate tissues or image patterns that have never been encountered during training, and the error enables
    
[^3]: Jais和Jais-chat：以阿拉伯语为中心的基础和指导调优的开放式生成式大型语言模型

    Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models. (arXiv:2308.16149v1 [cs.CL])

    [http://arxiv.org/abs/2308.16149](http://arxiv.org/abs/2308.16149)

    Jais和Jais-chat是新的以阿拉伯语为中心的开放式生成式大型语言模型，具有13亿参数，在阿拉伯语方面表现出优异的知识和推理能力，并且在英语方面也具有竞争力。这些模型的发布旨在促进阿拉伯语LLMs的研究。

    

    我们介绍了Jais和Jais-chat，这是新的最先进的以阿拉伯语为中心的基础和指导调优的开放式生成式大型语言模型（LLMs）。这些模型基于GPT-3的仅解码器架构，并在阿拉伯语和英语文本的混合物中进行预训练，包括各种编程语言的源代码。这些模型具有130亿个参数，根据广泛的评估结果，在阿拉伯语知识和推理能力方面表现优于任何现有的开放式阿拉伯语和多语言模型。此外，尽管在训练时使用的英语数据要少得多，但这些模型在英语方面与类似规模的以英语为中心的开放模型相比仍具有竞争力。我们提供了模型的训练、调优、安全对齐和评估的详细描述。我们发布了模型的两个开放版本--基础Jais模型和指导调优的Jais-chat变种--旨在促进阿拉伯语LLMs的研究。详见https://hugging

    We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://hugging
    
[^4]: MedShapeNet - 一个用于计算机视觉的大规模三维医学形状数据集

    MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision. (arXiv:2308.16139v1 [cs.CV])

    [http://arxiv.org/abs/2308.16139](http://arxiv.org/abs/2308.16139)

    MedShapeNet是一个大规模的三维医学形状数据集，作为一种对于常用形状基准的替代品，为计算机视觉研究提供了新的选择。

    

    我们提出了MedShapeNet，一个包含了解剖形状（如骨骼、器官、血管）和三维手术器械模型的大型数据集。在深度学习时代之前，统计形状模型在医学图像分析中的广泛应用证明了形状常被用来描述医学数据。然而，当前医学图像领域的最先进深度学习算法主要是基于体素的。相反，在计算机视觉领域，形状（包括体素占据网格、网格、点云和隐式表面模型）是三维数据的首选表示方法，这一点可以从大量关于形状的文章及在顶级计算机视觉会议（如IEEE/CVF计算机视觉与模式识别会议（CVPR））中见到，同时ShapeNet（约51300个模型）和普林斯顿ModelNet（127,915个模型）的流行度也在不断增加。MedShapeNet的创建是为了作为这些常用形状基准的替代品。

    We present MedShapeNet, a large collection of anatomical shapes (e.g., bones, organs, vessels) and 3D surgical instrument models. Prior to the deep learning era, the broad application of statistical shape models (SSMs) in medical image analysis is evidence that shapes have been commonly used to describe medical data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in medical imaging are predominantly voxel-based. In computer vision, on the contrary, shapes (including, voxel occupancy grids, meshes, point clouds and implicit surface models) are preferred data representations in 3D, as seen from the numerous shape-related publications in premier vision conferences, such as the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as well as the increasing popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is created as an alternative to these commonly used shape benchmarks to f
    
[^5]: 空间图粗化：使用GNN进行伦敦自行车共享服务的天气和工作日预测

    Spatial Graph Coarsening: Weather and Weekday Prediction with London's Bike-Sharing Service using GNN. (arXiv:2308.16122v1 [cs.LG])

    [http://arxiv.org/abs/2308.16122](http://arxiv.org/abs/2308.16122)

    本研究使用图神经网络预测伦敦的天气和工作日，通过引入图特征和地理接近性的图粗化操作，提高了预测的准确性。

    

    本研究介绍了使用图神经网络（GNN）预测伦敦一天的天气和工作日的方法，通过将桑坦德自行车共享系统的数据集作为图分类任务。提出的GNN模型引入了（i）图特征与经过训练的节点嵌入的拼接操作和（ii）基于地理接近性的图粗化操作，即“空间图粗化”。利用土地利用特征和自行车站周围的家庭数量的节点特征以及城市温度的图特征，我们提出的模型在交叉熵损失和验证数据集的准确率上优于基线模型。

    This study introduced the use of Graph Neural Network (GNN) for predicting the weather and weekday of a day in London, from the dataset of Santander Cycles bike-sharing system as a graph classification task. The proposed GNN models newly introduced (i) a concatenation operator of graph features with trained node embeddings and (ii) a graph coarsening operator based on geographical contiguity, namely "Spatial Graph Coarsening". With the node features of land-use characteristics and number of households around the bike stations and graph features of temperatures in the city, our proposed models outperformed the baseline model in cross-entropy loss and accuracy of the validation dataset.
    
[^6]: survex：用于解释机器学习生存模型的R软件包

    survex: an R package for explaining machine learning survival models. (arXiv:2308.16113v1 [cs.LG])

    [http://arxiv.org/abs/2308.16113](http://arxiv.org/abs/2308.16113)

    survex是一个R软件包，通过应用可解释的人工智能技术，提供了一个连贯的框架来解释任何生存模型，可以改进模型，提高透明度和责任感。

    

    由于其灵活性和出色性能，机器学习模型经常用于补充和超越传统的统计生存模型。然而，它们的广泛应用受到缺乏用户友好的工具来解释其内部操作和预测原理的限制。为了解决这个问题，我们引入了survex R软件包，通过应用可解释的人工智能技术，提供了一个连贯的框架来解释任何生存模型。所提软件的功能包括理解和诊断生存模型，从而可以改进它们。通过揭示变量效应和重要性等决策过程的见解，survex能够评估模型的可靠性并检测偏差。因此，在生物医学研究和医疗应用等敏感领域可以促进透明度和责任。

    Due to their flexibility and superior performance, machine learning models frequently complement and outperform traditional statistical survival models. However, their widespread adoption is hindered by a lack of user-friendly tools to explain their internal operations and prediction rationales. To tackle this issue, we introduce the survex R package, which provides a cohesive framework for explaining any survival model by applying explainable artificial intelligence techniques. The capabilities of the proposed software encompass understanding and diagnosing survival models, which can lead to their improvement. By revealing insights into the decision-making process, such as variable effects and importances, survex enables the assessment of model reliability and the detection of biases. Thus, transparency and responsibility may be promoted in sensitive areas, such as biomedical research and healthcare applications.
    
[^7]: 高级深度回归模型用于预测时间序列石油产量

    Advanced Deep Regression Models for Forecasting Time Series Oil Production. (arXiv:2308.16105v1 [cs.LG])

    [http://arxiv.org/abs/2308.16105](http://arxiv.org/abs/2308.16105)

    本研究开发了基于序列卷积和LSTM单元的高级数据驱动回归模型，用于预测时间序列石油产量。这些模型能够捕捉到时间序列数据中的历史模式，提高了预测的准确性。

    

    全球石油需求正在迅速增加，预计到2040年将达到每天106.3万桶。因此，对于油气开采行业来说，预测其产量以优化运营并避免损失至关重要。大公司已经意识到利用深度学习(DL)的力量和来自各种油井的大量数据可以节省大量的运营成本并减少不必要的环境影响。在这方面，研究人员提出了使用传统机器学习(ML)技术进行石油产量预测的模型。然而，这些技术对于这个问题是不合适的，因为它们无法捕捉到时间序列数据中的历史模式，导致预测不准确。本研究旨在通过使用序列卷积和长短期记忆(LSTM)单元开发先进的数据驱动回归模型来克服这些问题。进行了详尽的分析以选择最优序列。

    Global oil demand is rapidly increasing and is expected to reach 106.3 million barrels per day by 2040. Thus, it is vital for hydrocarbon extraction industries to forecast their production to optimize their operations and avoid losses. Big companies have realized that exploiting the power of deep learning (DL) and the massive amount of data from various oil wells for this purpose can save a lot of operational costs and reduce unwanted environmental impacts. In this direction, researchers have proposed models using conventional machine learning (ML) techniques for oil production forecasting. However, these techniques are inappropriate for this problem as they can not capture historical patterns found in time series data, resulting in inaccurate predictions. This research aims to overcome these issues by developing advanced data-driven regression models using sequential convolutions and long short-term memory (LSTM) units. Exhaustive analyses are conducted to select the optimal sequence 
    
[^8]: 基于区域方法的机器学习和物理约束神经网络在加热炉中的应用

    Application of Zone Method based Machine Learning and Physics-Informed Neural Networks in Reheating Furnaces. (arXiv:2308.16089v1 [cs.LG])

    [http://arxiv.org/abs/2308.16089](http://arxiv.org/abs/2308.16089)

    本文将经典的Hottel区域方法与机器学习和深度学习相结合，利用生成的数据进行加热炉控制系统的训练，为基础产业的可持续制造和能耗降低目标做出贡献。

    

    尽管基础产业的经济重要性很高，但其生产链中的一些组件，如加热炉，能耗较高。通过减少加热炉中的整体加热时间，可以显著降低能耗。在基础产业可持续制造中，计算机集成的机器学习（ML）和人工智能（AI）控制系统可能是实现“零净排放”目标的关键。本文中，由于在加热炉等场景中无法获得高质量的数据的可行性，采用经典的Hottel区域方法基于计算模型生成数据，用于ML和深度学习（DL）模型的回归训练。值得注意的是，区域方法提供了一种优雅的方式来建模辐射传热（RHT）的物理现象，这是加热炉内高温过程中占主导地位的传热机制。利用这些数据，进行了详细的实验研究。

    Despite the high economic relevance of Foundation Industries, certain components like Reheating furnaces within their manufacturing chain are energy-intensive. Notable energy consumption reduction could be obtained by reducing the overall heating time in furnaces. Computer-integrated Machine Learning (ML) and Artificial Intelligence (AI) powered control systems in furnaces could be enablers in achieving the Net-Zero goals in Foundation Industries for sustainable manufacturing.  In this work, due to the infeasibility of achieving good quality data in scenarios like reheating furnaces, classical Hottel's zone method based computational model has been used to generate data for ML and Deep Learning (DL) based model training via regression. It should be noted that the zone method provides an elegant way to model the physical phenomenon of Radiative Heat Transfer (RHT), the dominating heat transfer mechanism in high-temperature processes inside heating furnaces. Using this data, an extensive
    
[^9]: 最新的死亡预测模型的共识：从全因死亡到突发死亡预测

    Consensus of state of the art mortality prediction models: From all-cause mortality to sudden death prediction. (arXiv:2308.16067v1 [cs.LG])

    [http://arxiv.org/abs/2308.16067](http://arxiv.org/abs/2308.16067)

    本研究共识了最新的死亡预测模型，从全因死亡到突发死亡预测，展示了结合医疗历史、血液检查、药物处方和住院治疗可以预测突发死亡的风险增加的有效性。

    

    全球每年有数百万人突然和意外死亡，无论是否有心血管疾病的先前历史。这种事件很少见，许多受害者在心脏疾病之前没有接受过调查，而突发死亡的定义也存在许多不同。因此，突发死亡很难预测。本分析使用了2010年大格拉斯哥和克莱德（GG＆C）地区50岁及以上的人群的英国国民保健服务电子健康记录（EHR）（n = 380,000）来尝试克服这些挑战。我们调查了医疗史、血液检查、药物处方和住院治疗是否相结合可以预测突发死亡的风险增加。我们比较了训练用于预测突发死亡或全因死亡的模型的性能。我们为每个感兴趣的结果构建了六个模型：三个取自最新研究（BEHRT，Deepr和Deep Patient），三个为我们自己创建。我们进行了训练

    Worldwide, many millions of people die suddenly and unexpectedly each year, either with or without a prior history of cardiovascular disease. Such events are sparse (once in a lifetime), many victims will not have had prior investigations for cardiac disease and many different definitions of sudden death exist. Accordingly, sudden death is hard to predict.  This analysis used NHS Electronic Health Records (EHRs) for people aged $\geq$50 years living in the Greater Glasgow and Clyde (GG\&C) region in 2010 (n = 380,000) to try to overcome these challenges. We investigated whether medical history, blood tests, prescription of medicines, and hospitalisations might, in combination, predict a heightened risk of sudden death.  We compared the performance of models trained to predict either sudden death or all-cause mortality. We built six models for each outcome of interest: three taken from state-of-the-art research (BEHRT, Deepr and Deep Patient), and three of our own creation. We trained t
    
[^10]: Conti公司：通过机器学习了解一个大型勒索软件服务运营商的内部讨论

    Conti Inc.: Understanding the Internal Discussions of a large Ransomware-as-a-Service Operator with Machine Learning. (arXiv:2308.16061v1 [cs.CR])

    [http://arxiv.org/abs/2308.16061](http://arxiv.org/abs/2308.16061)

    Conti公司的聊天记录泄露给我们提供了了解勒索软件服务运营商内部运作的机会。使用机器学习技术和可视化策略，研究发现业务、技术、内部任务管理、恶意软件和客户服务是Conti成员讨论的主要主题。

    

    勒索软件服务（RaaS）正在增加勒索软件攻击的规模和复杂性。了解RaaS背后的内部运作一直是个挑战，因为此类活动是非法的。最近Conti公司泄露的聊天记录给我们提供了一个了解这类组织内部运作的良机。本文使用自然语言处理（NLP）和潜在狄利克雷分配（LDA）等机器学习技术以及可视化策略，分析了Conti公司聊天记录中的主要主题讨论。发现了五个讨论主题：1）业务，2）技术，3）内部任务/管理，4）恶意软件，5）客户服务/问题解决。此外，Conti成员的主题分布显示，只有4%的人进行了专门的讨论，而几乎所有人（96%）都是全能型，意味着他们的讨论都围绕着这五个主题展开。

    Ransomware-as-a-service (RaaS) is increasing the scale and complexity of ransomware attacks. Understanding the internal operations behind RaaS has been a challenge due to the illegality of such activities. The recent chat leak of the Conti RaaS operator, one of the most infamous ransomware operators on the international scene, offers a key opportunity to better understand the inner workings of such organizations. This paper analyzes the main topic discussions in the Conti chat leak using machine learning techniques such as Natural Language Processing (NLP) and Latent Dirichlet Allocation (LDA), as well as visualization strategies. Five discussion topics are found: 1) Business, 2) Technical, 3) Internal tasking/Management, 4) Malware, and 5) Customer Service/Problem Solving. Moreover, the distribution of topics among Conti members shows that only 4% of individuals have specialized discussions while almost all individuals (96%) are all-rounders, meaning that their discussions revolve aro
    
[^11]: 一种无需参数的改进二位协方差估计器

    A Parameter-Free Two-Bit Covariance Estimator with Improved Operator Norm Error Rate. (arXiv:2308.16059v1 [stat.ML])

    [http://arxiv.org/abs/2308.16059](http://arxiv.org/abs/2308.16059)

    提出了一种无需参数的二位协方差估计器，通过使用变化的抖动尺度，解决了在协方差矩阵对角线主导情况下估计器与样本协方差之间的算子范数误差差距以及依赖未知参数的抖动尺度问题。

    

    最近Dirksen, Maly and Rauhut在《Annals of Statistics》上开发了一种使用每个条目两位的协方差矩阵估计器。该估计器在一般亚高斯分布下达到了近似极小化速率，但也存在两个问题：理论上，在协方差矩阵的对角线由少数条目主导时，其估计器与样本协方差之间存在本质上的算子范数误差差距；实际上，其性能严重依赖于需要根据一些未知参数进行调整的抖动尺度。在这项工作中，我们提出了一种同时解决这两个问题的新型二位协方差矩阵估计器。与Dirksen等人采用的均匀抖动相关的符号量化器不同，我们采用了受多位均匀量化器启发的三角抖动器之后再进行二位量化。通过使用各个条目之间变化的抖动尺度，我们的估计器获得了改进的算子范数误差率，该误差率取决于...

    A covariance matrix estimator using two bits per entry was recently developed by Dirksen, Maly and Rauhut [Annals of Statistics, 50(6), pp. 3538-3562]. The estimator achieves near minimax rate for general sub-Gaussian distributions, but also suffers from two downsides: theoretically, there is an essential gap on operator norm error between their estimator and sample covariance when the diagonal of the covariance matrix is dominated by only a few entries; practically, its performance heavily relies on the dithering scale, which needs to be tuned according to some unknown parameters. In this work, we propose a new 2-bit covariance matrix estimator that simultaneously addresses both issues. Unlike the sign quantizer associated with uniform dither in Dirksen et al., we adopt a triangular dither prior to a 2-bit quantizer inspired by the multi-bit uniform quantizer. By employing dithering scales varying across entries, our estimator enjoys an improved operator norm error rate that depends o
    
[^12]: 基于张量化支持向量机和最小二乘支持向量机的低秩多任务学习

    Low-Rank Multitask Learning based on Tensorized SVMs and LSSVMs. (arXiv:2308.16056v1 [cs.LG])

    [http://arxiv.org/abs/2308.16056](http://arxiv.org/abs/2308.16056)

    本文提出了一种基于张量化支持向量机和最小二乘支持向量机的低秩多任务学习方法，通过高阶张量表示任务之间的关系，并利用交替优化和Lagrangian函数解决相关的凸问题。

    

    多任务学习（MTL）利用任务相关性来提高性能。随着多模态数据的出现，任务现在可以由多个索引引用。在本文中，我们使用高阶张量，其中每个模式对应一个任务索引，以自然地表示由多个索引引用的任务并保留它们的结构关系。基于这种表示，我们提出了一种具有张量化支持向量机（SVM）和最小二乘支持向量机（LSSVM）的低秩多任务学习方法的通用框架，其中CP分解部署在系数张量上。我们的方法通过任务特定因子加权的共享因子的线性组合来建模任务关系，并推广到分类和回归问题。通过交替优化方案和Lagrangian函数，每个子问题都被转化为一个凸问题，形式化为对偶形式的二次规划或线性系统。

    Multitask learning (MTL) leverages task-relatedness to enhance performance. With the emergence of multimodal data, tasks can now be referenced by multiple indices. In this paper, we employ high-order tensors, with each mode corresponding to a task index, to naturally represent tasks referenced by multiple indices and preserve their structural relations. Based on this representation, we propose a general framework of low-rank MTL methods with tensorized support vector machines (SVMs) and least square support vector machines (LSSVMs), where the CP factorization is deployed over the coefficient tensor. Our approach allows to model the task relation through a linear combination of shared factors weighted by task-specific factors and is generalized to both classification and regression problems. Through the alternating optimization scheme and the Lagrangian function, each subproblem is transformed into a convex problem, formulated as a quadratic programming or linear system in the dual form
    
[^13]: PAVI：板块化的变分推断

    PAVI: Plate-Amortized Variational Inference. (arXiv:2308.16022v1 [stat.ML])

    [http://arxiv.org/abs/2308.16022](http://arxiv.org/abs/2308.16022)

    PAVI是一种板块化的变分推断方法，能够高效地处理大规模人口研究，通过共享参数化和学习加速训练变分分布，实现了在大规模分层问题上的表达力强和简明的结果。

    

    在给定观测数据和概率生成模型的情况下，贝叶斯推断寻找可能产生数据的模型参数的分布。在大规模人口研究中，推断是具有挑战性的，因为在成百上千的受试者群体上进行了数百万次测量，导致参数空间巨大。这种大基数使得现成的变分推断在计算上变得不切实际。在这项工作中，我们设计了有效处理大规模人口研究的结构化变分推断家族。我们的主要思想是共享参数化和学习，跨越生成模型中的不同i.i.d.变量，由模型的“板块”来象征。我们将这个概念命名为“板块化”。与减缓推断的现成随机变分推断相反，板块化导致训练变分分布的速度提高数个数量级。应用于大规模分层问题，PAVI产生了表达力强、简明的结果。

    Given observed data and a probabilistic generative model, Bayesian inference searches for the distribution of the model's parameters that could have yielded the data. Inference is challenging for large population studies where millions of measurements are performed over a cohort of hundreds of subjects, resulting in a massive parameter space. This large cardinality renders off-the-shelf Variational Inference (VI) computationally impractical.  In this work, we design structured VI families that efficiently tackle large population studies. Our main idea is to share the parameterization and learning across the different i.i.d. variables in a generative model, symbolized by the model's \textit{plates}. We name this concept \textit{plate amortization}. Contrary to off-the-shelf stochastic VI, which slows down inference, plate amortization results in orders of magnitude faster to train variational distributions.  Applied to large-scale hierarchical problems, PAVI yields expressive, parsimoni
    
[^14]: EnsembleFollower: 基于强化学习和分层规划的混合车辆跟驰框架

    EnsembleFollower: A Hybrid Car-Following Framework Based On Reinforcement Learning and Hierarchical Planning. (arXiv:2308.16008v1 [cs.RO])

    [http://arxiv.org/abs/2308.16008](http://arxiv.org/abs/2308.16008)

    EnsembleFollower是一个采用分层规划和强化学习的混合车辆跟驰框架，能够实现先进的类人车辆跟驰。

    

    车辆跟驰模型对于我们对纵向行驶行为的理解做出了重要贡献。然而，它们常常表现出有限的准确性和灵活性，因为它们无法完全捕捉到跟驰过程中的复杂性，或者由于依赖于训练数据中存在的受限驾驶技能而在未知情况下出现问题。值得注意的是，每种车辆跟驰模型在特定驾驶场景中具有自己的优势和劣势。因此，我们提出了一种名为EnsembleFollower的分层规划框架，用于实现先进的类人车辆跟驰。EnsembleFollower框架涉及一个高层强化学习代理的使用，负责根据当前状态，通过选择适当的低层模型执行动作或分配不同的权重来管理多个低层车辆跟驰模型。

    Car-following models have made significant contributions to our understanding of longitudinal driving behavior. However, they often exhibit limited accuracy and flexibility, as they cannot fully capture the complexity inherent in car-following processes, or may falter in unseen scenarios due to their reliance on confined driving skills present in training data. It is worth noting that each car-following model possesses its own strengths and weaknesses depending on specific driving scenarios. Therefore, we propose EnsembleFollower, a hierarchical planning framework for achieving advanced human-like car-following. The EnsembleFollower framework involves a high-level Reinforcement Learning-based agent responsible for judiciously managing multiple low-level car-following models according to the current state, either by selecting an appropriate low-level model to perform an action or by allocating different weights across all low-level components. Moreover, we propose a jerk-constrained kin
    
[^15]: FPTQ：面向大型语言模型的细粒度训练后量化

    FPTQ: Fine-grained Post-Training Quantization for Large Language Models. (arXiv:2308.15987v1 [cs.CL])

    [http://arxiv.org/abs/2308.15987](http://arxiv.org/abs/2308.15987)

    本研究提出了一种面向大型语言模型的细粒度训练后量化方法，结合了W4A8和W8A8两种方案的优点，通过逐层激活量化和细粒度权重量化来解决性能下降的问题。

    

    在大规模语言模型的时代中，庞大的参数大小给部署带来了巨大的挑战。作为一种流行的压缩技术，量化已经成为解决这个问题的主流实践，主要集中在两种方案W8A8和W4A16（即这两种位宽的权重和激活）。在本研究中，我们提出了一种新颖的W4A8训练后量化方法，用于现有的开放源码语言模型，结合了这两种方案的优点。因此，我们可以利用4位权重量化的I/O利用率优势和8位矩阵计算的加速效果。然而，W4A8面临着明显的性能下降问题。为了解决这个问题，我们采用了逐层激活量化策略，对于最棘手的层使用了一种新颖的对数均衡方法，并将其与细粒度权重量化相结合。在没有额外的调整的情况下，我们消除了进一步微调的必要性。

    In the era of large-scale language models, the substantial parameter size poses significant challenges for deployment. Being a prevalent compression technique, quantization has emerged as the mainstream practice to tackle this issue, which is mainly centered on two recipes W8A8 and W4A16 (i.e. weights and activations in such bit widths). In this study, we propose a novel W4A8 post-training quantization method for the available open-sourced LLMs, which combines the advantages of both two recipes. Therefore, we can leverage the benefit in the I/O utilization of 4-bit weight quantization and the acceleration due to 8-bit matrix computation. Nevertheless, the W4A8 faces notorious performance degradation. As a remedy, we involve layerwise activation quantization strategies which feature a novel logarithmic equalization for most intractable layers, and we combine them with fine-grained weight quantization. Without whistles and bells, we eliminate the necessity for further fine-tuning and obt
    
[^16]: 使用图注意力网络学习结构运动

    Learning Structure-from-Motion with Graph Attention Networks. (arXiv:2308.15984v1 [cs.CV])

    [http://arxiv.org/abs/2308.15984](http://arxiv.org/abs/2308.15984)

    本文通过使用图注意力网络，将传统的学习结构运动问题中的子问题替换为直接学习模型，实现了对新序列的快速推断，提高了结构运动的学习性能。

    

    本文通过使用图注意力网络解决学习结构运动（SfM）的问题。SfM是一个经典的计算机视觉问题，通过迭代最小化重投影误差（称为束调整）解决，从良好的初始化开始。为了获得足够好的初始化结果，传统方法依赖于一系列子问题（如成对姿态估计、姿态平均化或三角测量），这些子问题提供了一个初始解，然后使用束调整对其进行改进。在本文中，我们通过学习一个模型，将这些子问题替换为以多个视图上检测到的2D关键点作为输入，并输出相应的相机姿态和3D关键点坐标。我们的模型利用图神经网络学习SfM特定的原语，并且我们展示它可以用于快速推断新的和未见过的序列的重建。实验结果表明，所提出的模型可以显著提高结构运动的学习性能。

    In this paper we tackle the problem of learning Structure-from-Motion (SfM) through the use of graph attention networks. SfM is a classic computer vision problem that is solved though iterative minimization of reprojection errors, referred to as Bundle Adjustment (BA), starting from a good initialization. In order to obtain a good enough initialization to BA, conventional methods rely on a sequence of sub-problems (such as pairwise pose estimation, pose averaging or triangulation) which provides an initial solution that can then be refined using BA. In this work we replace these sub-problems by learning a model that takes as input the 2D keypoints detected across multiple views, and outputs the corresponding camera poses and 3D keypoint coordinates. Our model takes advantage of graph neural networks to learn SfM-specific primitives, and we show that it can be used for fast inference of the reconstruction for new and unseen sequences. The experimental results show that the proposed mode
    
[^17]: 演示：基于数字孪生的5G无线接入网络异常检测功能

    Demo: A Digital Twin of the 5G Radio Access Network for Anomaly Detection Functionality. (arXiv:2308.15973v1 [cs.NI])

    [http://arxiv.org/abs/2308.15973](http://arxiv.org/abs/2308.15973)

    该论文提出了一种基于数字孪生的5G无线接入网络异常检测功能，使得5G系统能够实时检测用户连接异常并主动进行资源控制和连接恢复的决策。

    

    最近，在5G/6G领域中，数字孪生（DT）的概念引起了广泛关注。本演示展示了一种针对5G基础设施集成而设计和实现的创新的数字孪生框架。所提出的数字孪生能够实时检测用户连接异常，赋予5G系统主动执行资源控制和连接恢复的决策能力。

    Recently, the concept of digital twins (DTs) has received significant attention within the realm of 5G/6G. This demonstration shows an innovative DT design and implementation framework tailored toward integration within the 5G infrastructure. The proposed DT enables near real-time anomaly detection capability pertaining to user connectivity. It empowers the 5G system to proactively execute decisions for resource control and connection restoration.
    
[^18]: Jaccard约束下的密集子图发现

    Jaccard-constrained dense subgraph discovery. (arXiv:2308.15936v1 [cs.DS])

    [http://arxiv.org/abs/2308.15936](http://arxiv.org/abs/2308.15936)

    本研究提出了Jaccard约束下的密集子图发现问题，并给出了两种算法来解决该问题。

    

    在图挖掘中，发现密集子图是一个核心问题，广泛应用于不同领域。同时，许多现实世界的网络随时间变化，即数据集可以表示为图快照的序列。因此，自然而然地考虑到在允许一定程度的时间变化的情况下，找到密集子图的问题。在本文中，我们搜索具有较大Jaccard相似性系数的密集子图。更正式地说，给定一组图快照和权重λ，我们寻找一组密集子图，使得感应子图的密度之和加上以λ加权的Jaccard指数之和最大化。我们证明了该问题是NP困难的。为了发现具有良好目标值的密集子图，我们提出了一个迭代算法，每次迭代时间复杂度为$\mathcal{O}(n^2k^2 + m \log n + k^3 n)$，和一个贪婪算法，时间复杂度为$\mathcal{O}(n^2k^2 + m \log n)$。

    Finding dense subgraphs is a core problem in graph mining with many applications in diverse domains. At the same time many real-world networks vary over time, that is, the dataset can be represented as a sequence of graph snapshots. Hence, it is natural to consider the question of finding dense subgraphs in a temporal network that are allowed to vary over time to a certain degree. In this paper, we search for dense subgraphs that have large pairwise Jaccard similarity coefficients. More formally, given a set of graph snapshots and a weight $\lambda$, we find a collection of dense subgraphs such that the sum of densities of the induced subgraphs plus the sum of Jaccard indices, weighted by $\lambda$, is maximized. We prove that this problem is NP-hard. To discover dense subgraphs with good objective value, we present an iterative algorithm which runs in $\mathcal{O}(n^2k^2 + m \log n + k^3 n)$ time per single iteration, and a greedy algorithm which runs in $\mathcal{O}(n^2k^2 + m \log n
    
[^19]: LLaSM: 大型语言和语音模型

    LLaSM: Large Language and Speech Model. (arXiv:2308.15930v1 [cs.CL])

    [http://arxiv.org/abs/2308.15930](http://arxiv.org/abs/2308.15930)

    LLaSM是一个大型语言和语音模型，具有跨模态对话能力，通过遵循语音和语言指令，提供了一种方便自然的人机交互方式。

    

    最近，多模态大型语言模型引起了广泛关注。然而，大部分研究都集中在视觉-语言多模态模型上，提供了强大的能力来遵循视觉和语言指令。然而，我们认为语音也是人类与世界互动的重要方式。因此，对于一个通用的助手来说，能够遵循多模态语音和语言指令是至关重要的。在这项工作中，我们提出了大型语言和语音模型（LLaSM）。LLaSM是一个端到端训练的大型多模态语音-语言模型，具有跨模态对话能力，能够遵循语音和语言指令。我们的初步实验表明，LLaSM展示了一种更方便自然的人机交互方式。为了支持研究，我们还发布了一个大型的语音指令数据集LLaSM-Audio-Instructions。代码和演示可在https://github.com/LinkSoul-AI/LLaSM和ht上查看

    Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained large multi-modal speech-language model with cross-modal conversational abilities, capable of following speech-and-language instructions. Our early experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available at https://github.com/LinkSoul-AI/LLaSM and ht
    
[^20]: 周期性排斥学习

    Cyclophobic Reinforcement Learning. (arXiv:2308.15911v1 [cs.LG])

    [http://arxiv.org/abs/2308.15911](http://arxiv.org/abs/2308.15911)

    本文提出一种新的周期性排斥增强学习方法，通过避免循环来惩罚冗余而不奖励新颖性，在奖励稀疏的环境中能取得优异的结果。

    

    在奖励稀疏的环境中，寻找良好的归纳偏差以促进探索对于智能体的成功至关重要。然而，存在着两个竞争的目标：新颖性搜索和系统性探索。现有的方法，如好奇心驱动的探索，可以找到新颖性，但有时不会对整个状态空间进行系统性的探索，类似于深度优先搜索与广度优先搜索。在本文中，我们提出了一种新的内在奖励，即周期性排斥型，它不会奖励新颖性，而是通过避免循环来惩罚冗余。通过将周期性排斥的内在奖励与基于智能体裁剪观测的分层表示序列相结合，我们在MiniGrid和MiniHack环境中取得了极好的结果。这两个环境都特别难以解决，因为它们需要与不同对象进行复杂的交互才能解决。与先前方法的详细比较和彻底的消融研究表明，我们新提出的周期性排斥增强学习方法在这些环境中表现出色。

    In environments with sparse rewards, finding a good inductive bias for exploration is crucial to the agent's success. However, there are two competing goals: novelty search and systematic exploration. While existing approaches such as curiosity-driven exploration find novelty, they sometimes do not systematically explore the whole state space, akin to depth-first-search vs breadth-first-search. In this paper, we propose a new intrinsic reward that is cyclophobic, i.e., it does not reward novelty, but punishes redundancy by avoiding cycles. Augmenting the cyclophobic intrinsic reward with a sequence of hierarchical representations based on the agent's cropped observations we are able to achieve excellent results in the MiniGrid and MiniHack environments. Both are particularly hard, as they require complex interactions with different objects in order to be solved. Detailed comparisons with previous approaches and thorough ablation studies show that our newly proposed cyclophobic reinforc
    
[^21]: 通过自主量子热机实现热力学计算

    Thermodynamic Computing via Autonomous Quantum Thermal Machines. (arXiv:2308.15905v1 [quant-ph])

    [http://arxiv.org/abs/2308.15905](http://arxiv.org/abs/2308.15905)

    通过自主量子热机实现了热力学计算模型。通过热流进行计算，可实现任何线性可分离函数，并可扩展到神经元网络执行任何 needed功能。

    

    我们基于自主量子热机开发了一个基于物理的模型，用于经典计算。这些机器由少数相互作用的量子位（qubits）与不同温度的几个环境相连。通过机器的热流进行计算。过程从根据逻辑输入设定环境温度开始。机器演化，最终达到非平衡稳态，通过辅助有限尺寸储存池的温度可以确定计算结果。这样的机器，我们称之为“热力学神经元”，可以实现任何线性可分离函数，我们明确讨论了NOT，3-majority和NOR门的情况。反过来，我们展示了热力学神经元网络可以执行任何需要的函数。我们讨论了我们的模型与人工神经元（感知器）之间的密切关系，并认为我们的模型提供了一种基于物理的替代方法。

    We develop a physics-based model for classical computation based on autonomous quantum thermal machines. These machines consist of few interacting quantum bits (qubits) connected to several environments at different temperatures. Heat flows through the machine are here exploited for computing. The process starts by setting the temperatures of the environments according to the logical input. The machine evolves, eventually reaching a non-equilibrium steady state, from which the output of the computation can be determined via the temperature of an auxilliary finite-size reservoir. Such a machine, which we term a "thermodynamic neuron", can implement any linearly-separable function, and we discuss explicitly the cases of NOT, 3-majority and NOR gates. In turn, we show that a network of thermodynamic neurons can perform any desired function. We discuss the close connection between our model and artificial neurons (perceptrons), and argue that our model provides an alternative physics-based
    
[^22]: 超越传统神经网络：通过计算逻辑技术增加推理和学习能力

    Beyond Traditional Neural Networks: Toward adding Reasoning and Learning Capabilities through Computational Logic Techniques. (arXiv:2308.15899v1 [cs.AI])

    [http://arxiv.org/abs/2308.15899](http://arxiv.org/abs/2308.15899)

    通过计算逻辑技术，本研究提出了一种超越传统神经网络的方法，将神经网络和符号推理相结合，解决了深度学习模型在高质量训练数据、透明性和鲁棒性方面的限制。同时，通过改进知识注入过程，将机器学习和逻辑元素融入多主体系统。

    

    深度学习（DL）模型已经成为解决复杂问题的常用方法，但它们存在一些限制，如需要高质量的训练数据、缺乏透明性和鲁棒性问题。神经符号人工智能已经成为一种有希望的方法，将神经网络和符号推理的优势结合起来。符号知识注入（SKI）技术是一种将符号知识融入子符号系统的流行方法。本文提出了改进知识注入过程并将机器学习和逻辑元素融入多主体系统（MAS）的解决方案。

    Deep Learning (DL) models have become popular for solving complex problems, but they have limitations such as the need for high-quality training data, lack of transparency, and robustness issues. Neuro-Symbolic AI has emerged as a promising approach combining the strengths of neural networks and symbolic reasoning. Symbolic knowledge injection (SKI) techniques are a popular method to incorporate symbolic knowledge into sub-symbolic systems. This work proposes solutions to improve the knowledge injection process and integrate elements of ML and logic into multi-agent systems (MAS).
    
[^23]: 对于组合逻辑推理的 CLIP 潜力的研究

    On the Potential of CLIP for Compositional Logical Reasoning. (arXiv:2308.15887v1 [cs.AI])

    [http://arxiv.org/abs/2308.15887](http://arxiv.org/abs/2308.15887)

    本文研究了使用CLIP进行组合逻辑推理的潜力，并发现通常配置的CLIP无法进行这种推理。

    

    在本文中，我们探讨了使用OpenAI的CLIP进行逻辑连贯的视觉推理的可能性。为此，我们形式化了我们的术语，并对CLIP潜在空间中的嵌入进行了几何分析，以便系统在逻辑上连贯。我们的主要结论是，通常配置的CLIP不能进行这种推理。

    In this paper we explore the possibility of using OpenAI's CLIP to perform logically coherent grounded visual reasoning. To that end, we formalize our terms and give a geometric analysis of how embeddings in CLIP's latent space would need to be configured in order for the system to be logically coherent. Our main conclusion is that, as usually configured, CLIP cannot perform such reasoning.
    
[^24]: 使用归纳逻辑编程实现文本分类的一次性学习

    Towards One-Shot Learning for Text Classification using Inductive Logic Programming. (arXiv:2308.15885v1 [cs.LG])

    [http://arxiv.org/abs/2308.15885](http://arxiv.org/abs/2308.15885)

    本文使用归纳逻辑编程的方法实现了一次性的文本分类，并通过使用常识背景知识和元解释学习框架，可以从少量训练样本中学习分类规则，且复杂度更高的样本可以达到更高的准确性。

    

    随着人工智能在执行个性化任务方面的潜力不断增长，开发数据高效且不需要大量训练数据的新的机器学习技术变得至关重要。本文探索了一种基于归纳逻辑编程的一次性文本分类方法。特别是，我们探索了元解释学习（MIL）框架，并利用从ConceptNet中提取的常识背景知识。结果表明，MIL可以从少量的训练样本中学习文本分类规则。此外，选择的样本的复杂度越高，结果的准确性也越高。

    With the ever-increasing potential of AI to perform personalised tasks, it is becoming essential to develop new machine learning techniques which are data-efficient and do not require hundreds or thousands of training data. In this paper, we explore an Inductive Logic Programming approach for one-shot text classification. In particular, we explore the framework of Meta-Interpretive Learning (MIL), along with using common-sense background knowledge extracted from ConceptNet. Results indicate that MIL can learn text classification rules from a small number of training examples. Moreover, the higher complexity of chosen examples, the higher accuracy of the outcome.
    
[^25]: 如果我处于AI中，生活会更有趣吗？基于概率归纳逻辑编程回答反事实问题

    "Would life be more interesting if I were in AI?" Answering Counterfactuals based on Probabilistic Inductive Logic Programming. (arXiv:2308.15883v1 [cs.LO])

    [http://arxiv.org/abs/2308.15883](http://arxiv.org/abs/2308.15883)

    该论文提出了一种使用概率归纳逻辑编程回答反事实问题的方法，在学习程序结构时考虑了因果机制，使得可以进行反事实推理。

    

    概率逻辑程序是一种某些事实以特定概率成立的逻辑程序。在这里，我们使用因果框架来研究这些程序，使其能够回答反事实查询。通常通过启发式搜索和统计测试来从观测数据中学习程序结构。然而，这些统计测试缺乏关于生成数据的因果机制的信息，使得使用生成的程序进行反事实推理变得不可行。为了解决这个问题，我们提出了一种语言片段，允许从其引导分布中重建程序。这进一步使我们能够学习支持反事实查询的程序。

    Probabilistic logic programs are logic programs where some facts hold with a specified probability. Here, we investigate these programs with a causal framework that allows counterfactual queries. Learning the program structure from observational data is usually done through heuristic search relying on statistical tests. However, these statistical tests lack information about the causal mechanism generating the data, which makes it unfeasible to use the resulting programs for counterfactual reasoning. To address this, we propose a language fragment that allows reconstructing a program from its induced distribution. This further enables us to learn programs supporting counterfactual queries.
    
[^26]: 深度、窄型多层感知机的最小宽度问题：一种微分同胚和惠特尼嵌入定理的方法。

    Minimum Width for Deep, Narrow MLP: A Diffeomorphism and the Whitney Embedding Theorem Approach. (arXiv:2308.15873v1 [cs.LG])

    [http://arxiv.org/abs/2308.15873](http://arxiv.org/abs/2308.15873)

    本文给出了深度、窄型多层感知机通用逼近性的最小宽度的上界，并通过微分同胚和惠特尼嵌入定理的证明进行了验证。

    

    最近，人们对于确定深度、窄型多层感知机的通用逼近性的最小宽度问题引起了相当大的关注。在这些挑战中，以一致范数逼近连续函数是重要且具有挑战性的，其下界和上界之间的差距很难缩小。在这方面，我们提出了一种新的最小宽度的上界，即$\operatorname{max}(2d_x+1, d_y) + \alpha(\sigma)$，以实现在深度窄型多层感知机中的一致逼近，其中$0\leq \alpha(\sigma)\leq 2$代表了与激活函数相关的常数。我们通过两个关键证明来证明这个上界。首先，我们证明了具有较小的额外宽度的深度窄型多层感知机可以逼近微分同胚。其次，我们利用惠特尼嵌入定理表明任何连续函数都可以通过嵌入逼近，进一步分解为线性变换和微分同胚。

    Recently, there has been significant attention on determining the minimum width for the universal approximation property of deep, narrow MLPs. Among these challenges, approximating a continuous function under the uniform norm is important and challenging, with the gap between its lower and upper bound being hard to narrow. In this regard, we propose a novel upper bound for the minimum width, given by $\operatorname{max}(2d_x+1, d_y) + \alpha(\sigma)$, to achieve uniform approximation in deep narrow MLPs, where $0\leq \alpha(\sigma)\leq 2$ represents the constant depending on the activation function. We demonstrate this bound through two key proofs. First, we establish that deep, narrow MLPs with little additional width can approximate diffeomorphisms. Secondly, we utilize the Whitney embedding theorem to show that any continuous function can be approximated by embeddings, further decomposed into linear transformations and diffeomorphisms.
    
[^27]: 不需要过量经验风险的域泛化

    Domain Generalization without Excess Empirical Risk. (arXiv:2308.15856v1 [cs.LG])

    [http://arxiv.org/abs/2308.15856](http://arxiv.org/abs/2308.15856)

    本论文提出了一种解决域泛化中过量风险问题的方法，通过在保证经验风险最优的约束下最小化惩罚，避免了惩罚对经验风险优化的影响。

    

    在给定不同分布的多样数据集的情况下，域泛化旨在学习可以推广到未见分布的模型。一种常见的方法是设计一个数据驱动的替代惩罚来捕捉泛化性能，并与惩罚一起最小化经验风险。我们认为这种方法的一个重要失败模式是由于错误的惩罚或联合优化的困难而导致的过量风险。我们提出了一种解决这个问题的方法。我们不是将经验风险和惩罚联合最小化，而是在保证经验风险最优的约束下最小化惩罚。这种改变保证了域泛化惩罚不会影响对经验风险的优化，即在分布内的性能。为了解决这个优化问题，我们展示了与率失真理论的令人兴奋的联系，并利用其工具设计了一个高效的方法。我们的方法可以应用于任何基于惩罚的域泛化问题。

    Given data from diverse sets of distinct distributions, domain generalization aims to learn models that generalize to unseen distributions. A common approach is designing a data-driven surrogate penalty to capture generalization and minimize the empirical risk jointly with the penalty. We argue that a significant failure mode of this recipe is an excess risk due to an erroneous penalty or hardness in joint optimization. We present an approach that eliminates this problem. Instead of jointly minimizing empirical risk with the penalty, we minimize the penalty under the constraint of optimality of the empirical risk. This change guarantees that the domain generalization penalty cannot impair optimization of the empirical risk, i.e., in-distribution performance. To solve the proposed optimization problem, we demonstrate an exciting connection to rate-distortion theory and utilize its tools to design an efficient method. Our approach can be applied to any penalty-based domain generalization
    
[^28]: MSGNN: 多尺度时空图神经网络用于流行病预测

    MSGNN: Multi-scale Spatio-temporal Graph Neural Network for Epidemic Forecasting. (arXiv:2308.15840v1 [cs.LG])

    [http://arxiv.org/abs/2308.15840](http://arxiv.org/abs/2308.15840)

    MSGNN是一种多尺度时空图神经网络，通过创新的多尺度视图和图学习模块，有效地解决了现有GNN模型在保留远距离连接和多尺度流行病模式上的局限性。它在传染病预测中具有重要的应用价值。

    

    传染病预测是防控流行病的关键，并被证明具有重要性。最近的趋势是基于图神经网络（GNN）开发预测模型。然而，现有的基于GNN的方法存在两个关键限制：（1）当前模型通过缩放GNN的深度来扩大感受野，但这对于保留远距离但与流行病相关的地区之间的语义不足够。（2）以前的方法模拟单一空间尺度内的流行病，而忽视了不同尺度上导出的多尺度流行病模式。为了解决这些缺陷，我们设计了基于创新的多尺度视图的多尺度时空图神经网络（MSGNN）。具体来说，在提出的MSGNN模型中，我们首先设计了一个新颖的图学习模块，它直接捕捉跨区域流行病信号的远距离连接，并将它们集成到一个多尺度图中。基于学习的多尺度图，我们使用空间-时间GCN进行流行病预测。

    Infectious disease forecasting has been a key focus and proved to be crucial in controlling epidemic. A recent trend is to develop forecast-ing models based on graph neural networks (GNNs). However, existing GNN-based methods suffer from two key limitations: (1) Current models broaden receptive fields by scaling the depth of GNNs, which is insuffi-cient to preserve the semantics of long-range connectivity between distant but epidemic related areas. (2) Previous approaches model epidemics within single spatial scale, while ignoring the multi-scale epidemic pat-terns derived from different scales. To address these deficiencies, we devise the Multi-scale Spatio-temporal Graph Neural Network (MSGNN) based on an innovative multi-scale view. To be specific, in the proposed MSGNN model, we first devise a novel graph learning module, which directly captures long-range connectivity from trans-regional epidemic signals and integrates them into a multi-scale graph. Based on the learned multi-scal
    
[^29]: 自适应Lasso、转移Lasso及其拓展：渐进视角下的研究

    Adaptive Lasso, Transfer Lasso, and Beyond: An Asymptotic Perspective. (arXiv:2308.15838v1 [stat.ML])

    [http://arxiv.org/abs/2308.15838](http://arxiv.org/abs/2308.15838)

    本文研究了自适应Lasso和转移Lasso的理论性质，通过对转移Lasso的渐进性质进行理论研究，分析了它与自适应Lasso的区别，并提出了一种新的方法，将两者的优势进行了融合并补偿了他们的弱点。

    

    本文全面探讨了自适应Lasso和转移Lasso的理论性质。自适应Lasso是一种成熟的方法，采用根据初始估计值进行的正则化，具有渐进正态性和变量选择一致性的特点。相比之下，最近提出的转移Lasso采用根据初始估计值进行的正则化减法，具有减少非渐进估计误差的能力。一个关键问题因此出现：鉴于自适应Lasso和转移Lasso在使用初始估计值方面存在的不同方式，这种差异给每种方法带来了什么好处或弊端？本文对转移Lasso的渐进性质进行了理论研究，从而阐明了它与自适应Lasso的区别。根据这个分析的结果，我们引入了一种新的方法，将各自的优势进行了融合并补偿了他们的弱点。

    This paper presents a comprehensive exploration of the theoretical properties inherent in the Adaptive Lasso and the Transfer Lasso. The Adaptive Lasso, a well-established method, employs regularization divided by initial estimators and is characterized by asymptotic normality and variable selection consistency. In contrast, the recently proposed Transfer Lasso employs regularization subtracted by initial estimators with the demonstrated capacity to curtail non-asymptotic estimation errors. A pivotal question thus emerges: Given the distinct ways the Adaptive Lasso and the Transfer Lasso employ initial estimators, what benefits or drawbacks does this disparity confer upon each method? This paper conducts a theoretical examination of the asymptotic properties of the Transfer Lasso, thereby elucidating its differentiation from the Adaptive Lasso. Informed by the findings of this analysis, we introduce a novel method, one that amalgamates the strengths and compensates for the weaknesses o
    
[^30]: 具有自适应个性化层的联邦化两阶段解耦

    Federated Two Stage Decoupling With Adaptive Personalization Layers. (arXiv:2308.15821v1 [cs.LG])

    [http://arxiv.org/abs/2308.15821](http://arxiv.org/abs/2308.15821)

    该论文提出了一种具有自适应个性化层的联邦化两阶段解耦方法，通过将同质客户端聚类到同一组的方式来提高联邦学习的性能，并解决了数据异质性和聚类时间选择的问题。

    

    由于分布式设备间的数据异质性，联邦学习在保持隐私约束的同时实现分布式学习的突破性能力引起了广泛关注。然而，由于数据异质性导致了显著的学习降级和慢收敛速度。因此，自然地采用将同质客户端聚类到同一组的概念，只允许在每个组内聚合模型权重。尽管大多数现有的聚类联邦学习方法采用模型梯度或推理输出作为客户端分区的度量标准，目的是将相似设备组合在一起，但每个聚类内部仍可能存在异质性。此外，缺乏研究探索确定聚类的适当时间的根本原因，导致常见做法是将每个客户端分配到其自己的独立聚类中，特别是在高度非ind情况下。

    Federated learning has gained significant attention due to its groundbreaking ability to enable distributed learning while maintaining privacy constraints. However, as a consequence of data heterogeneity among decentralized devices, it inherently experiences significant learning degradation and slow convergence speed. Therefore, it is natural to employ the concept of clustering homogeneous clients into the same group, allowing only the model weights within each group to be aggregated. While most existing clustered federated learning methods employ either model gradients or inference outputs as metrics for client partitioning, with the goal of grouping similar devices together, may still have heterogeneity within each cluster. Moreover, there is a scarcity of research exploring the underlying reasons for determining the appropriate timing for clustering, resulting in the common practice of assigning each client to its own individual cluster, particularly in the context of highly non ind
    
[^31]: 透过偏好看大型语言模型的反馈获取：揭示对齐的重要性

    Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v1 [cs.LG])

    [http://arxiv.org/abs/2308.15812](http://arxiv.org/abs/2308.15812)

    本研究分析了对于对齐和评估大型语言模型而言，设计反馈选择是评分还是排名对结果的影响。研究发现评分和排名所推断出的偏好存在不一致问题，并且注释者的偏见也会影响结果。同时，研究还发现反馈协议的选择也对评估结果有显著影响。

    

    大型语言模型（LLMs）与人类价值观和意图的对齐承诺涉及使用人工智能或人类反馈。稠密的反馈注释获取和整合成本较高，而稀疏的反馈则涉及结构性设计选择，即评分（例如，在1-7的范围内对回答A进行评分）和排名（例如，回答A是否比回答B更好？）。在这项工作中，我们分析了这种设计选择对LLMs的对齐和评估的影响。我们发现，评分和排名所推断出的偏好在人类和AI注释者中都存在严重的不一致问题，达到了60%。我们的后续分析确定了解释这个现象的各种注释者偏见方面，比如人类注释者更喜欢密集回答并在两个选项之间更青睐准确性。令我们惊讶的是，我们还观察到反馈协议的选择对对齐的LLMs的评估也有显著影响。特别是，我们发现LLMs的评估结果因为反馈协议的选择而有所不同。

    Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs
    
[^32]: HAlf-MAsked模型用于命名实体情感分析

    HAlf-MAsked Model for Named Entity Sentiment analysis. (arXiv:2308.15793v1 [cs.CL])

    [http://arxiv.org/abs/2308.15793](http://arxiv.org/abs/2308.15793)

    在命名实体情感分析中，研究了解决基于BERT模型过拟合问题的多种方法，包括一种新颖的技术，在最终预测之前，对给定数据进行额外的掩码实体传递，以便在模型知道预测情感的确切实体和不知道的情况下，可以合并来自模型的逻辑。

    

    命名实体情感分析（NESA）是自然语言处理（NLP）中最活跃的应用领域之一。社交媒体NESA是意见分析的重要领域，因为检测和跟踪新闻流中的情感趋势对于构建各种分析系统和监测特定人物或公司的媒体形象至关重要。在本文中，我们研究了基于transformer的不同解决方案在RuSentNE-23评估中的NESA。尽管BERT等模型的效果很好，但它们仍然可能在某些挑战上遇到困难，例如过拟合问题，这是在RuSentNE-23数据上实现高准确性的主要障碍。我们提出了几种方法来解决这个问题，其中包括一种新颖的技术，在最终预测之前，对给定数据进行额外的掩码实体传递，以便在模型知道预测情感的确切实体和不知道的情况下，可以合并来自模型的逻辑。利用

    Named Entity Sentiment analysis (NESA) is one of the most actively developing application domains in Natural Language Processing (NLP). Social media NESA is a significant field of opinion analysis since detecting and tracking sentiment trends in the news flow is crucial for building various analytical systems and monitoring the media image of specific people or companies. In this paper, we study different transformers-based solutions NESA in RuSentNE-23 evaluation. Despite the effectiveness of the BERT-like models, they can still struggle with certain challenges, such as overfitting, which appeared to be the main obstacle in achieving high accuracy on the RuSentNE-23 data. We present several approaches to overcome this problem, among which there is a novel technique of additional pass over given data with masked entity before making the final prediction so that we can combine logits from the model when it knows the exact entity it predicts sentiment for and when it does not. Utilizing 
    
[^33]: FedCiR: 客户端不变表示学习用于联邦非独立同分布特征

    FedCiR: Client-Invariant Representation Learning for Federated Non-IID Features. (arXiv:2308.15786v1 [cs.LG])

    [http://arxiv.org/abs/2308.15786](http://arxiv.org/abs/2308.15786)

    FedCiR是一种客户端不变表示学习框架，用于解决联邦学习中的特征偏移问题，通过改进表示和标签之间的互信息项来提取信息且与客户无关的特征。

    

    联邦学习（FL）是一种分布式学习范式，它在不共享原始数据的情况下最大化了边缘设备上数据驱动模型的潜力。然而，设备的数据往往是非独立同分布（non-IID）的，意味着它们的本地数据分布可能会有很大的差异。设备之间输入数据分布的异质性，通常称为特征偏移问题，可能会对全局模型的训练收敛性和准确性产生不利影响。为了分析特征偏移问题的内在原因，我们在FL中开发了一个广义误差边界，这激励我们提出了FedCiR，一种客户端不变表示学习框架，使客户能够提取信息且与客户无关的特征。具体而言，我们改进了表示和标签之间的互信息项，以鼓励表示携带基本的分类知识，并减小了客户端和表示之间的互信息项。

    Federated learning (FL) is a distributed learning paradigm that maximizes the potential of data-driven models for edge devices without sharing their raw data. However, devices often have non-independent and identically distributed (non-IID) data, meaning their local data distributions can vary significantly. The heterogeneity in input data distributions across devices, commonly referred to as the feature shift problem, can adversely impact the training convergence and accuracy of the global model. To analyze the intrinsic causes of the feature shift problem, we develop a generalization error bound in FL, which motivates us to propose FedCiR, a client-invariant representation learning framework that enables clients to extract informative and client-invariant features. Specifically, we improve the mutual information term between representations and labels to encourage representations to carry essential classification knowledge, and diminish the mutual information term between the client 
    
[^34]: 无泄露的Split：减少Split Learning中的隐私泄露

    Split Without a Leak: Reducing Privacy Leakage in Split Learning. (arXiv:2308.15783v1 [cs.CR])

    [http://arxiv.org/abs/2308.15783](http://arxiv.org/abs/2308.15783)

    本文介绍了一个使用Split Learning和同态加密的混合方法，用于保护隐私的深度学习。研究表明，Split Learning容易受到攻击，因此本文提出了一个新的解决方案来减少隐私泄露。

    

    深度学习的普及使得敏感数据的隐私比以往任何时候都更加重要。因此，各种保护隐私的技术已被应用于深度学习中以保护用户数据的隐私。在各种保护隐私的技术中，协作学习技术（如Split Learning）已被用于加速学习和预测过程。最初，Split Learning被认为是保护数据隐私的一种有希望的方法。然而，随后的研究表明，Split Learning容易受到多种攻击，因此不能作为保护隐私的技术。与此同时，也引入了使用Split Learning和同态加密的组合来实现保护隐私的深度学习的对策。在本文中，我们提出了一种使用Split Learning和同态加密的混合方法。其背后的思想是在将激活图（客户端和服务器之间的分割层的输出）发送给服务器之前，客户端对其进行加密。

    The popularity of Deep Learning (DL) makes the privacy of sensitive data more imperative than ever. As a result, various privacy-preserving techniques have been implemented to preserve user data privacy in DL. Among various privacy-preserving techniques, collaborative learning techniques, such as Split Learning (SL) have been utilized to accelerate the learning and prediction process. Initially, SL was considered a promising approach to data privacy. However, subsequent research has demonstrated that SL is susceptible to many types of attacks and, therefore, it cannot serve as a privacy-preserving technique. Meanwhile, countermeasures using a combination of SL and encryption have also been introduced to achieve privacy-preserving deep learning. In this work, we propose a hybrid approach using SL and Homomorphic Encryption (HE). The idea behind it is that the client encrypts the activation map (the output of the split layer between the client and the server) before sending it to the ser
    
[^35]: 高效且可解释的图神经网络架构搜索通过蒙特卡洛树搜索

    Efficient and Explainable Graph Neural Architecture Search via Monte-Carlo Tree Search. (arXiv:2308.15734v1 [cs.LG])

    [http://arxiv.org/abs/2308.15734](http://arxiv.org/abs/2308.15734)

    该论文提出了一种高效且可解释的图神经网络架构搜索方法，名为ExGNAS。它包括适应各种图形的简单搜索空间和能解释决策过程的搜索算法。通过蒙特卡洛树搜索高效地搜索最佳GNN架构。

    

    图神经网络（GNNs）是在各个领域进行数据科学任务的强大工具。尽管我们在广泛的应用场景中使用GNNs，但对研究人员和实践者来说，在不同的图中设计/选择最佳GNN架构是一项费力的任务。为了节省人力和计算成本，已经使用图神经网络架构搜索（Graph NAS）来搜索结合现有组件的次优GNN架构。然而，目前没有现有的Graph NAS方法能够同时满足可解释性、高效性和适应多样化图形的要求。因此，我们提出了一种高效且可解释的Graph NAS方法，称为ExGNAS，它包括（i）一个可以适应各种图形的简单搜索空间和（ii）一个能够解释决策过程的搜索算法。搜索空间仅包含可以处理同质和异质图的基本函数。搜索算法通过蒙特卡洛树搜索高效地搜索最佳GNN架构。

    Graph neural networks (GNNs) are powerful tools for performing data science tasks in various domains. Although we use GNNs in wide application scenarios, it is a laborious task for researchers and practitioners to design/select optimal GNN rchitectures in diverse graphs. To save human efforts and computational costs, graph neural architecture search (Graph NAS) has been used to search for a sub-optimal GNN architecture that combines existing components. However, there are no existing Graph NAS methods that satisfy explainability, efficiency, and adaptability to various graphs. Therefore, we propose an efficient and explainable Graph NAS method, called ExGNAS, which consists of (i) a simple search space that can adapt to various graphs and (ii) a search algorithm that makes the decision process explainable. The search space includes only fundamental functions that can handle homophilic and heterophilic graphs. The search algorithm efficiently searches for the best GNN architecture via M
    
[^36]: 完全嵌入的时间序列生成对抗网络

    Fully Embedded Time-Series Generative Adversarial Networks. (arXiv:2308.15730v1 [cs.LG])

    [http://arxiv.org/abs/2308.15730](http://arxiv.org/abs/2308.15730)

    这篇论文提出了一种完全嵌入的时间序列生成对抗网络（FETSGAN），通过对抗性训练匹配特征空间和低维度采样空间的训练分布，确保合成样本的时间分布不会崩溃。

    

    生成对抗网络（GANs）应产生与所建模数据的潜在分布相符的合成数据。针对实值时间序列数据，这意味着需要同时捕获数据的静态分布以及任何潜在时间范围的完整时间分布。这个时间要素产生了一个更复杂的问题，可能导致当前的解决方案受限、训练不稳定或易于发生模式崩溃。在完全嵌入的时间序列生成对抗网络（FETSGAN）中，整个序列通过一个seq2seq风格的对抗性自编码器（AAE）直接转换为生成器的采样空间，其中对抗性训练用于在特征空间和较低维度的采样空间匹配训练分布。这个额外的约束条件提供了对合成样本的时间分布不会崩溃的松散保证。此外，引入了第一个超过阈值（FAT）操作符。

    Generative Adversarial Networks (GANs) should produce synthetic data that fits the underlying distribution of the data being modeled. For real valued time-series data, this implies the need to simultaneously capture the static distribution of the data, but also the full temporal distribution of the data for any potential time horizon. This temporal element produces a more complex problem that can potentially leave current solutions under-constrained, unstable during training, or prone to varying degrees of mode collapse. In FETSGAN, entire sequences are translated directly to the generator's sampling space using a seq2seq style adversarial auto encoder (AAE), where adversarial training is used to match the training distribution in both the feature space and the lower dimensional sampling space. This additional constraint provides a loose assurance that the temporal distribution of the synthetic samples will not collapse. In addition, the First Above Threshold (FAT) operator is introduc
    
[^37]: 基于替代模型的自动调优方法在回归问题中随机化草图算法的应用

    Surrogate-based Autotuning for Randomized Sketching Algorithms in Regression Problems. (arXiv:2308.15720v1 [cs.LG])

    [http://arxiv.org/abs/2308.15720](http://arxiv.org/abs/2308.15720)

    本文介绍了如何使用基于替代模型的自动调优方法解决随机化草图算法中的参数选择问题，在随机数值线性代数中取得了接近最优性能的实证结果。

    

    从随机数值线性代数(RandNLA)中提出的算法在处理高维计算问题方面表现出很好的效果，提供高质量的经验性能以及强大的概率保证。然而，它们的实际应用受到一个问题的复杂性所限制，即用户需要设置各种不同于传统NLA中使用的算法特定调参参数。本文展示了如何使用基于替代模型的自动调优方法来解决RandNLA算法中参数选择的基础性问题。具体而言，我们对基于草图和预处理(SAP)的随机化最小二乘方法进行了替代模型自动调优的详细研究，这在现代RandNLA中是一个巨大的成功案例。实证结果表明，我们的基于替代模型的自动调优方法可以以比随机搜索少约4倍的试验成本实现接近最优的性能。

    Algorithms from Randomized Numerical Linear Algebra (RandNLA) are known to be effective in handling high-dimensional computational problems, providing high-quality empirical performance as well as strong probabilistic guarantees. However, their practical application is complicated by the fact that the user needs to set various algorithm-specific tuning parameters which are different than those used in traditional NLA. This paper demonstrates how a surrogate-based autotuning approach can be used to address fundamental problems of parameter selection in RandNLA algorithms. In particular, we provide a detailed investigation of surrogate-based autotuning for sketch-and-precondition (SAP) based randomized least squares methods, which have been one of the great success stories in modern RandNLA. Empirical results show that our surrogate-based autotuning approach can achieve near-optimal performance with much less tuning cost than a random search (up to about 4x fewer trials of different para
    
[^38]: 用指导的Grad-CAM解释探索深度学习在全球盘面太阳耀斑预测中的应用

    Exploring Deep Learning for Full-disk Solar Flare Prediction with Empirical Insights from Guided Grad-CAM Explanations. (arXiv:2308.15712v1 [astro-ph.SR])

    [http://arxiv.org/abs/2308.15712](http://arxiv.org/abs/2308.15712)

    本研究通过引入指导的Grad-CAM解释，展示了一个全盘面的深度学习模型在太阳耀斑预测中的应用，并通过定性和定量的评估分析发现了活动区特征与太阳耀斑预测之间的关联。

    

    本研究通过展示一个全盘面深度学习模型来进一步推进太阳耀斑预测研究，并在中心（在±70°范围内）和近旁（超过±70°范围外）事件上评估其有效性，并展示了模型预测的事后解释的定性评估，并提供了基于人为评估这些解释的经验性发现。我们的模型使用每小时的全盘面磁线图像进行训练，以预测在接下来的24小时预测窗口内的M级太阳耀斑。此外，我们采用了指导的梯度权重类激活映射（Guided Grad-CAM）归因方法来解释我们模型的预测并评估解释的结果。我们的分析揭示了全盘面太阳耀斑预测与活动区特征的相对应关系。

    This study progresses solar flare prediction research by presenting a full-disk deep-learning model to forecast $\geq$M-class solar flares and evaluating its efficacy on both central (within $\pm$70$^\circ$) and near-limb (beyond $\pm$70$^\circ$) events, showcasing qualitative assessment of post hoc explanations for the model's predictions, and providing empirical findings from human-centered quantitative assessments of these explanations. Our model is trained using hourly full-disk line-of-sight magnetogram images to predict $\geq$M-class solar flares within the subsequent 24-hour prediction window. Additionally, we apply the Guided Gradient-weighted Class Activation Mapping (Guided Grad-CAM) attribution method to interpret our model's predictions and evaluate the explanations. Our analysis unveils that full-disk solar flare predictions correspond with active region characteristics. The following points represent the most important findings of our study: (1) Our deep learning models a
    
[^39]: Speech Wikimedia：一种包括77种语言的多语音数据集

    Speech Wikimedia: A 77 Language Multilingual Speech Dataset. (arXiv:2308.15710v1 [cs.AI])

    [http://arxiv.org/abs/2308.15710](http://arxiv.org/abs/2308.15710)

    Speech Wikimedia是一个包含来自77种语言的大量音频和转录的数据集，适用于训练语音识别、语音翻译和机器翻译模型。

    

    Speech Wikimedia数据集是从维基媒体共享资源中提取的带有转录的音频的公开可用编译。它包括来自不同场景和说话者的1780小时（195GB）的CC-BY-SA许可的转录语音，涵盖了77种不同的语言。每个音频文件都有一个或多个不同语言的转录，使得这个数据集适用于训练语音识别、语音翻译和机器翻译模型。

    The Speech Wikimedia Dataset is a publicly available compilation of audio with transcriptions extracted from Wikimedia Commons. It includes 1780 hours (195 GB) of CC-BY-SA licensed transcribed speech from a diverse set of scenarios and speakers, in 77 different languages. Each audio file has one or more transcriptions in different languages, making this dataset suitable for training speech recognition, speech translation, and machine translation models.
    
[^40]: 阈值KNN-Shapley：一种线性时间和隐私友好的数据价值评估方法

    Threshold KNN-Shapley: A Linear-Time and Privacy-Friendly Approach to Data Valuation. (arXiv:2308.15709v1 [cs.LG])

    [http://arxiv.org/abs/2308.15709](http://arxiv.org/abs/2308.15709)

    本论文研究了数据价值评估面临的隐私挑战，并提出了一种隐私友好的改进方法TKNN-Shapley，该方法在保护隐私的前提下能够评估数据质量，具有较好的隐私-实用性权衡。

    

    数据价值评估是数据中心化机器学习研究中的关键问题，旨在量化单个数据源在训练机器学习模型中的有用性。然而，尽管其重要性，数据价值评估面临着很多重要但经常被忽视的隐私挑战。本文针对目前最实用的数据价值评估方法之一KNN-Shapley，研究了这些挑战。我们首先强调了KNN-Shapley固有的隐私风险，并展示了将KNN-Shapley改进以满足差分隐私(DP)的显著技术困难。为了克服这些挑战，我们引入了TKNN-Shapley，KNN-Shapley的一种改进变体，具有隐私友好性，可以进行简单的修正以包含DP保证（DP-TKNN-Shapley）。我们证明，DP-TKNN-Shapley在辨别数据质量方面具有一些优势，并在隐私-实用性权衡方面优于朴素化的KNN-Shapley。此外，即使是非隐私的TKNN-Shapley也能以线性时间运行。

    Data valuation, a critical aspect of data-centric ML research, aims to quantify the usefulness of individual data sources in training machine learning (ML) models. However, data valuation faces significant yet frequently overlooked privacy challenges despite its importance. This paper studies these challenges with a focus on KNN-Shapley, one of the most practical data valuation methods nowadays. We first emphasize the inherent privacy risks of KNN-Shapley, and demonstrate the significant technical difficulties in adapting KNN-Shapley to accommodate differential privacy (DP). To overcome these challenges, we introduce TKNN-Shapley, a refined variant of KNN-Shapley that is privacy-friendly, allowing for straightforward modifications to incorporate DP guarantee (DP-TKNN-Shapley). We show that DP-TKNN-Shapley has several advantages and offers a superior privacy-utility tradeoff compared to naively privatized KNN-Shapley in discerning data quality. Moreover, even non-private TKNN-Shapley ac
    
[^41]: 对对比学习中互信息的严格分析的探索

    Towards a Rigorous Analysis of Mutual Information in Contrastive Learning. (arXiv:2308.15704v1 [cs.AI])

    [http://arxiv.org/abs/2308.15704](http://arxiv.org/abs/2308.15704)

    本研究探索了对比学习中互信息的严格分析，引入了三种新方法和相关定理，提升了互信息分析的严谨性与实用性。

    

    对比学习已成为无监督表示学习的重要基石。其主要范式涉及一个互信息损失的实例区分任务。这种损失被称为InfoNCE，通过互信息分析的视角提供了对对比学习的重要见解。然而，互信息的估计可能具有挑战性，导致其数学基础的优雅与估计的复杂性之间存在差距。因此，从互信息分析中得出严格的见解或结论变得复杂。在本研究中，我们引入了三种新方法和一些相关定理，旨在增强互信息分析的严谨性。尽管它们很简单，但这些方法具有重要的实用性。利用这些方法，我们重新评估了三个对比学习分析实例，展示了它们促进更深入理解或纠正错误的能力。

    Contrastive learning has emerged as a cornerstone in recent achievements of unsupervised representation learning. Its primary paradigm involves an instance discrimination task with a mutual information loss. The loss is known as InfoNCE and it has yielded vital insights into contrastive learning through the lens of mutual information analysis. However, the estimation of mutual information can prove challenging, creating a gap between the elegance of its mathematical foundation and the complexity of its estimation. As a result, drawing rigorous insights or conclusions from mutual information analysis becomes intricate. In this study, we introduce three novel methods and a few related theorems, aimed at enhancing the rigor of mutual information analysis. Despite their simplicity, these methods can carry substantial utility. Leveraging these approaches, we reassess three instances of contrastive learning analysis, illustrating their capacity to facilitate deeper comprehension or to rectif
    
[^42]: 基于长期顺序行为的在线食品订购点击率预测的新型时空建模方法-Fragment and Integrate Network (FIN)

    Fragment and Integrate Network (FIN): A Novel Spatial-Temporal Modeling Based on Long Sequential Behavior for Online Food Ordering Click-Through Rate Prediction. (arXiv:2308.15703v1 [cs.IR])

    [http://arxiv.org/abs/2308.15703](http://arxiv.org/abs/2308.15703)

    本文提出了一种名为Fragment and Integrate Network (FIN)的新型空间-时间建模方法，用于在线食品订购点击率预测。该方法通过从顺序行为数据中提取多个子序列，分别对每个子序列进行建模，从而捕捉用户的空间-时间偏好。

    

    空间-时间信息已被证明在在线基于位置的服务（LBS）中的点击率预测任务中具有重要意义，特别是在主流的食品订购平台上，如DoorDash、Uber Eats、美团和饿了么。通过顺序行为数据建模用户的空间-时间偏好已成为推荐系统和在线广告的热门话题。然而，现有方法大多缺乏对丰富的空间-时间信息的表示，或者仅处理长度有限的用户行为，例如100。在本文中，我们通过设计一种名为Fragment and Integrate Network (FIN)的新型空间-时间建模范式来解决这些问题。FIN包括两个网络：片段网络（FN）从终身顺序行为数据中提取多个子序列（MSS），通过分别对每个MSS进行建模来捕捉特定的空间-时间表示。这里采用了简化的注意力机制和复杂的注意力机制。

    Spatial-temporal information has been proven to be of great significance for click-through rate prediction tasks in online Location-Based Services (LBS), especially in mainstream food ordering platforms such as DoorDash, Uber Eats, Meituan, and Ele.me. Modeling user spatial-temporal preferences with sequential behavior data has become a hot topic in recommendation systems and online advertising. However, most of existing methods either lack the representation of rich spatial-temporal information or only handle user behaviors with limited length, e.g. 100. In this paper, we tackle these problems by designing a new spatial-temporal modeling paradigm named Fragment and Integrate Network (FIN). FIN consists of two networks: (i) Fragment Network (FN) extracts Multiple Sub-Sequences (MSS) from lifelong sequential behavior data, and captures the specific spatial-temporal representation by modeling each MSS respectively. Here both a simplified attention and a complicated attention are adopted 
    
[^43]: 训练朝向批判使用：学习将人工智能预测置于人类知识之中

    Training Towards Critical Use: Learning to Situate AI Predictions Relative to Human Knowledge. (arXiv:2308.15700v1 [cs.HC])

    [http://arxiv.org/abs/2308.15700](http://arxiv.org/abs/2308.15700)

    本文介绍了一种过程导向的适当依赖概念，称为批判使用，旨在帮助人们更好地利用基于人工智能的决策支持。研究通过在儿童虐待筛查领域进行在线实验，发现通过提供特定培训可以支持人们的批判使用能力。

    

    越来越多的研究探讨如何支持人们更好地利用基于人工智能的决策支持，包括通过培训和入门指导。现有的研究集中在决策任务上，这些任务可以通过比较每个决策与现实标签的一致性来评估“适当的依赖”，这些标签清晰地映射到人工智能的预测目标和人类决策者的目标。然而，在许多现实世界的环境中，这种假设不成立，因为人工智能工具被部署在社会工作、刑事司法和医疗保健等领域。在本文中，我们引入了一种过程导向的适当依赖概念，称为批判使用，其将人类能够将人工智能预测置于他们独特的知识之中，而这些知识对于人工智能模型来说是不可获得的。为了探索训练如何支持批判使用，我们在一个复杂的社会决策场景中进行了一项随机在线实验：儿童虐待筛查。我们发现，通过提供...

    A growing body of research has explored how to support humans in making better use of AI-based decision support, including via training and onboarding. Existing research has focused on decision-making tasks where it is possible to evaluate "appropriate reliance" by comparing each decision against a ground truth label that cleanly maps to both the AI's predictive target and the human decision-maker's goals. However, this assumption does not hold in many real-world settings where AI tools are deployed today (e.g., social work, criminal justice, and healthcare). In this paper, we introduce a process-oriented notion of appropriate reliance called critical use that centers the human's ability to situate AI predictions against knowledge that is uniquely available to them but unavailable to the AI model. To explore how training can support critical use, we conduct a randomized online experiment in a complex social decision-making setting: child maltreatment screening. We find that, by providi
    
[^44]: 通过无监督学习对机械异质领域进行分割

    Segmenting mechanically heterogeneous domains via unsupervised learning. (arXiv:2308.15697v1 [cs.LG])

    [http://arxiv.org/abs/2308.15697](http://arxiv.org/abs/2308.15697)

    本文通过无监督学习方法，探索了基于机器学习的逆分析方法，用于分割具有机械异质性的领域。

    

    从生物器官到柔性机器人，高变形材料是自然和工程系统的重要组成部分。这些高变形材料可以具有异质的材料特性，并且可以在有或无潜在材料异质性的情况下经历异质的变形。许多最近的研究表明，计算建模方法非常适用于理解和预测材料异质性的后果，并解释观测到的异质应变场。特别是，在开发逆分析方法方面已经取得了重要进展，可以将观测到的运动学量（例如位移、应变）转化为材料特性和力学状态。尽管这些方法取得了成功，但它们并不一定具有可推广性，往往依赖于对边界条件的严格控制和了解。在这里，我们将利用机器学习方法的最新进展和普及性，探索替代方法。

    From biological organs to soft robotics, highly deformable materials are essential components of natural and engineered systems. These highly deformable materials can have heterogeneous material properties, and can experience heterogeneous deformations with or without underlying material heterogeneity. Many recent works have established that computational modeling approaches are well suited for understanding and predicting the consequences of material heterogeneity and for interpreting observed heterogeneous strain fields. In particular, there has been significant work towards developing inverse analysis approaches that can convert observed kinematic quantities (e.g., displacement, strain) to material properties and mechanical state. Despite the success of these approaches, they are not necessarily generalizable and often rely on tight control and knowledge of boundary conditions. Here, we will build on the recent advances (and ubiquity) of machine learning approaches to explore altern
    
[^45]: CongNaMul: 一种用于大豆芽图像处理的数据集

    CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts. (arXiv:2308.15690v1 [cs.CV])

    [http://arxiv.org/abs/2308.15690](http://arxiv.org/abs/2308.15690)

    提出了一个用于大豆芽图像处理的名为CongNaMul的数据集，旨在支持图像分类、语义分割、分解和测量等任务。提供了质量分类、语义分割和图像分解的标记，以及5个芽的物理特征供测量使用。

    

    我们提出了“CongNaMul”，这是一个为大豆芽图像分析的各种任务而设计的综合数据集。CongNaMul数据集旨在促进图像分类、语义分割、分解以及长度和重量的测量等任务。分类任务提供了四个类别来确定大豆芽的质量：正常、断裂、斑点和断裂和斑点，以开发基于人工智能辅助的自动质量检测技术。对于语义分割，数据集包括了具有不同复杂度的图像，从单个芽图像到具有多个芽的图像，以及人工标记的掩膜图像。标签包括4个不同的类别：背景、头部、身体和尾部。数据集还为图像分解任务提供了图像和掩膜，包括两个分离的芽图像和它们的组合形式。最后，还提供了芽的5个物理特征（头部长度、身体长度、身体厚度、尾部长度、重量）供基于图像的测量使用。

    We present 'CongNaMul', a comprehensive dataset designed for various tasks in soybean sprouts image analysis. The CongNaMul dataset is curated to facilitate tasks such as image classification, semantic segmentation, decomposition, and measurement of length and weight. The classification task provides four classes to determine the quality of soybean sprouts: normal, broken, spotted, and broken and spotted, for the development of AI-aided automatic quality inspection technology. For semantic segmentation, images with varying complexity, from single sprout images to images with multiple sprouts, along with human-labelled mask images, are included. The label has 4 different classes: background, head, body, tail. The dataset also provides images and masks for the image decomposition task, including two separate sprout images and their combined form. Lastly, 5 physical features of sprouts (head length, body length, body thickness, tail length, weight) are provided for image-based measurement
    
[^46]: MDTD: 一种用于深度神经网络的多领域木马检测器

    MDTD: A Multi Domain Trojan Detector for Deep Neural Networks. (arXiv:2308.15673v1 [cs.CR])

    [http://arxiv.org/abs/2308.15673](http://arxiv.org/abs/2308.15673)

    MDTD是一种多领域木马检测器，用于在测试时检测深度神经网络中包含木马触发器的输入。MDTD不需要知道触发器嵌入策略，并且适用于不同类型的输入。它利用了输入样本与决策边界的距离来检测木马触发器。

    

    使用深度神经网络(DNN)的机器学习模型容易受到后门攻击。攻击者在一小部分输入样本中嵌入一个预定义的干扰物，称为触发器，并训练DNN，使得输入中存在触发器导致输出为攻击者所期望的类别。此类对抗性重训练需要确保没有触发器的输入输出不受影响，并对干净样本提供高分类准确率。本文提出了MDTD，一种针对DNN的多领域木马检测器，用于在测试时检测包含木马触发器的输入。MDTD不需要知道攻击者嵌入触发器的策略，并且可以应用于预训练的DNN模型，包括图像、音频和基于图的输入。MDTD利用了一个洞察力，即包含木马触发器的输入样本相对于干净样本位于决策边界之外较远的位置。MDTD估计了距离。

    Machine learning models that use deep neural networks (DNNs) are vulnerable to backdoor attacks. An adversary carrying out a backdoor attack embeds a predefined perturbation called a trigger into a small subset of input samples and trains the DNN such that the presence of the trigger in the input results in an adversary-desired output class. Such adversarial retraining however needs to ensure that outputs for inputs without the trigger remain unaffected and provide high classification accuracy on clean samples. In this paper, we propose MDTD, a Multi-Domain Trojan Detector for DNNs, which detects inputs containing a Trojan trigger at testing time. MDTD does not require knowledge of trigger-embedding strategy of the attacker and can be applied to a pre-trained DNN model with image, audio, or graph-based inputs. MDTD leverages an insight that input samples containing a Trojan trigger are located relatively farther away from a decision boundary than clean samples. MDTD estimates the dista
    
[^47]: 在高维空间中将分布学习与图像聚类相结合

    Bridging Distribution Learning and Image Clustering in High-dimensional Space. (arXiv:2308.15667v1 [cs.LG])

    [http://arxiv.org/abs/2308.15667](http://arxiv.org/abs/2308.15667)

    本文通过利用自编码器编码图像到高维潜空间并使用高斯混合模型进行分布学习，实现了图像聚类。然而，高维空间给聚类算法带来了挑战。

    

    分布学习关注于从一组数据样本中学习概率密度函数，而聚类旨在以无监督的方式将相似的对象分组在一起。通常情况下，这两个任务被认为是无关的。然而，两者之间的关系可能存在间接的相关性，其中高斯混合模型（GMM）起到了桥梁作用。本文致力于探索分布学习与聚类之间的相关性，并利用自编码器（AE）将图像编码成高维潜空间，以填补这两个领域之间的差距。然后，利用蒙特卡洛边缘化（MCMarg）和Kullback-Leibler（KL）散度损失来拟合GMM的高斯分量和学习数据分布。最后，通过GMM的每个高斯分量实现图像聚类。然而，“维数灾难”给大多数聚类算法带来了严重的挑战。与经典的期望最大化相比，我们的方法在高维空间中能更好地处理图像聚类问题。

    Distribution learning focuses on learning the probability density function from a set of data samples. In contrast, clustering aims to group similar objects together in an unsupervised manner. Usually, these two tasks are considered unrelated. However, the relationship between the two may be indirectly correlated, with Gaussian Mixture Models (GMM) acting as a bridge. In this paper, we focus on exploring the correlation between distribution learning and clustering, with the motivation to fill the gap between these two fields, utilizing an autoencoder (AE) to encode images into a high-dimensional latent space. Then, Monte-Carlo Marginalization (MCMarg) and Kullback-Leibler (KL) divergence loss are used to fit the Gaussian components of the GMM and learn the data distribution. Finally, image clustering is achieved through each Gaussian component of GMM. Yet, the "curse of dimensionality" poses severe challenges for most clustering algorithms. Compared with the classic Expectation-Maximiz
    
[^48]: 基于深度强化学习的移动能量传播器调度框架用于在道路上为电动车充电

    Deep Reinforcement Learning Based Framework for Mobile Energy Disseminator Dispatching to Charge On-the-Road Electric Vehicles. (arXiv:2308.15656v1 [cs.RO])

    [http://arxiv.org/abs/2308.15656](http://arxiv.org/abs/2308.15656)

    本论文提出了一个基于深度强化学习的移动能量传播器调度框架，用于在道路上为电动车充电，解决了充电期间车辆排队导致的行车效率低的问题，并提出了一种有效的调度策略来确定最佳时间和位置。

    

    电动车的快速增长给保护电池健康和解决车辆续航焦虑问题带来了新的挑战。为了解决这些问题，特别是移动能量传播器（MEDs）已经成为一个有希望的解决方案。MED安装在大型车辆后方，并在它的上游半径范围内为所有参与的电动车充电。然而，V2V充电期间，MED和电动车意外地形成车队，从而占据了多条车道，并严重影响了整个通道的行车效率。此外，有限的MED配置预算需要开发一种有效的调度策略，以确定在交通中引入MED的最佳时间和位置。本文提出了一种基于深度强化学习（DRL）的方法来开发一个车辆调度框架。

    The exponential growth of electric vehicles (EVs) presents novel challenges in preserving battery health and in addressing the persistent problem of vehicle range anxiety. To address these concerns, wireless charging, particularly, Mobile Energy Disseminators (MEDs) have emerged as a promising solution. The MED is mounted behind a large vehicle and charges all participating EVs within a radius upstream of it. Unfortuantely, during such V2V charging, the MED and EVs inadvertently form platoons, thereby occupying multiple lanes and impairing overall corridor travel efficiency. In addition, constrained budgets for MED deployment necessitate the development of an effective dispatching strategy to determine optimal timing and locations for introducing the MEDs into traffic. This paper proposes a deep reinforcement learning (DRL) based methodology to develop a vehicle dispatching framework. In the first component of the framework, we develop a realistic reinforcement learning environment ter
    
[^49]: 在动态推荐系统中确保用户侧公平性

    Ensuring User-side Fairness in Dynamic Recommender Systems. (arXiv:2308.15651v1 [cs.IR])

    [http://arxiv.org/abs/2308.15651](http://arxiv.org/abs/2308.15651)

    本文提出了一种名为FADE的端到端框架，通过微调策略动态减轻推荐系统中用户群体之间的性能差异。

    

    用户侧群体公平性对现代推荐系统至关重要，它旨在减轻由敏感属性（如性别、种族或年龄）定义的用户群体之间的性能差异。我们发现这种差异往往会随着时间的推移而持续存在甚至增加。这需要在动态环境中有效解决用户侧公平性的方法，然而这在文献中很少被探讨。然而，用于确保用户侧公平性（即减少性能差异）的典型方法——公平约束重新排名，在动态设定中面临两个基本挑战：（1）基于排名的公平约束的非可微性，阻碍了端到端训练范式；（2）时间效率低下，阻碍了对用户偏好变化的快速适应。在本文中，我们提出了一种名为FADE的端到端框架，通过微调策略动态减轻性能差异。为了解决上述挑战，FADE提出了一种 fine-tuning 策略。

    User-side group fairness is crucial for modern recommender systems, as it aims to alleviate performance disparity between groups of users defined by sensitive attributes such as gender, race, or age. We find that the disparity tends to persist or even increase over time. This calls for effective ways to address user-side fairness in a dynamic environment, which has been infrequently explored in the literature. However, fairness-constrained re-ranking, a typical method to ensure user-side fairness (i.e., reducing performance disparity), faces two fundamental challenges in the dynamic setting: (1) non-differentiability of the ranking-based fairness constraint, which hinders the end-to-end training paradigm, and (2) time-inefficiency, which impedes quick adaptation to changes in user preferences. In this paper, we propose FAir Dynamic rEcommender (FADE), an end-to-end framework with fine-tuning strategy to dynamically alleviate performance disparity. To tackle the above challenges, FADE u
    
[^50]: 自动机器学习在实践中的一般方法

    A General Recipe for Automated Machine Learning in Practice. (arXiv:2308.15647v1 [cs.LG])

    [http://arxiv.org/abs/2308.15647](http://arxiv.org/abs/2308.15647)

    本文提出了一个构建通用AutoML系统的参考框架，通过叙事性回顾主要方法，将基本概念提炼出来以支持在单一设计中的应用。

    

    自动机器学习（AutoML）是一项研究领域，专注于开发能够自动生成机器学习模型的方法。能够几乎不需要人工干预地建立机器学习模型的想法为应用机器学习的实践提供了巨大机会。然而，在实践中如何设计AutoML系统的信息非常有限。大部分研究关注的是优化算法面临的问题，而忽略了实际应用的细节。在本文中，我们提出了一个构建通用AutoML系统的参考框架。通过对该领域主要方法的叙事性回顾，我们的主要思想是提炼基本概念，以支持它们在单一设计中的应用。最后，我们讨论了一些与AutoML应用相关的未来研究中的开放性问题。

    Automated Machine Learning (AutoML) is an area of research that focuses on developing methods to generate machine learning models automatically. The idea of being able to build machine learning models with very little human intervention represents a great opportunity for the practice of applied machine learning. However, there is very little information on how to design an AutoML system in practice. Most of the research focuses on the problems facing optimization algorithms and leaves out the details of how that would be done in practice. In this paper, we propose a frame of reference for building general AutoML systems. Through a narrative review of the main approaches in the area, our main idea is to distill the fundamental concepts in order to support them in a single design. Finally, we discuss some open problems related to the application of AutoML for future research.
    
[^51]: 无需特征间隔的聚类方法

    Clustering Without an Eigengap. (arXiv:2308.15642v1 [cs.LG])

    [http://arxiv.org/abs/2308.15642](http://arxiv.org/abs/2308.15642)

    这个论文介绍了在随机块模型中进行图聚类的新算法，能够恢复大聚类，无论其他聚类的大小，并且对中等大小的聚类提出了新的技术挑战。

    

    我们在随机块模型（SBM）中研究了具有大聚类和小不可恢复聚类的图聚类问题。之前的方法要么不允许小于$ o（\sqrt {n}）$大小的小聚类，要么要求最小可恢复聚类和最大不可恢复聚类之间存在大小间隔。我们提供了一个基于半定规划（SDP）的算法，它消除了这些要求，并可以确定地恢复大聚类，而不考虑其他聚类的大小。中等大小的聚类对分析提出了独特的挑战，因为它们接近恢复阈值，非常敏感于小的噪声扰动，不允许闭合形式的候选解决方案。我们开发了新颖的技术，包括leave-one-out风格的论证，即使去掉一行噪声也可能大幅改变SDP解决方案，仍然可以控制SDP解决方案与噪声向量之间的相关性。

    We study graph clustering in the Stochastic Block Model (SBM) in the presence of both large clusters and small, unrecoverable clusters. Previous approaches achieving exact recovery do not allow any small clusters of size $o(\sqrt{n})$, or require a size gap between the smallest recovered cluster and the largest non-recovered cluster. We provide an algorithm based on semidefinite programming (SDP) which removes these requirements and provably recovers large clusters regardless of the remaining cluster sizes. Mid-sized clusters pose unique challenges to the analysis, since their proximity to the recovery threshold makes them highly sensitive to small noise perturbations and precludes a closed-form candidate solution. We develop novel techniques, including a leave-one-out-style argument which controls the correlation between SDP solutions and noise vectors even when the removal of one row of noise can drastically change the SDP solution. We also develop improved eigenvalue perturbation bo
    
[^52]: 使用物理信息神经网络识别复杂超弹性固体的组分参数

    Identifying Constitutive Parameters for Complex Hyperelastic Solids using Physics-Informed Neural Networks. (arXiv:2308.15640v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2308.15640](http://arxiv.org/abs/2308.15640)

    本文介绍了一种基于物理信息神经网络（PINNs）的新框架，用于识别软材料在大变形平面应力条件下具有复杂组分行为的材料的组分参数。通过使用多模态的时间相关实验数据训练，我们的模型能够稳健地准确识别不可压缩Arruda-Boyce模型的组分参数。

    

    在工程和生物材料中，特别是那些具有复杂几何和机械行为的材料中，识别组分参数仍然是一个长期存在的挑战。最近出现的物理信息神经网络（PINNs）为此提供了有希望的解决方案，但当前的框架通常仅限于基本的组分定律，并在与实验数据相结合时遇到实际约束。本文引入了一种新的基于PINN的框架，旨在识别软材料的材料参数，特别是那些在平面应力条件下呈现复杂组分行为的材料。该模型强调用多模态的时间相关实验数据训练PINN，其中包括全场变形和加载历史，以确保算法在嘈杂数据中仍然稳健。我们的结果表明，我们的框架可以准确识别不可压缩Arruda-Boyce模型的组分参数。

    Identifying constitutive parameters in engineering and biological materials, particularly those with intricate geometries and mechanical behaviors, remains a longstanding challenge. The recent advent of Physics-Informed Neural Networks (PINNs) offers promising solutions, but current frameworks are often limited to basic constitutive laws and encounter practical constraints when combined with experimental data. In this paper, we introduce a new PINN-based framework designed to identify material parameters for soft materials, specifically those exhibiting complex constitutive behaviors, under large deformation in plane stress conditions. Distinctively, our model emphasizes training PINNs with multi-modal time-dependent experimental datasets consisting of full-field deformation and loading history, ensuring algorithm robustness even amidst noisy data. Our results reveal that our framework can accurately identify constitutive parameters of the incompressible Arruda-Boyce model for samples 
    
[^53]: 双曲卷积神经网络

    Hyperbolic Convolutional Neural Networks. (arXiv:2308.15639v1 [cs.LG])

    [http://arxiv.org/abs/2308.15639](http://arxiv.org/abs/2308.15639)

    双曲卷积神经网络利用双曲空间进行数据嵌入，具有更强大和可解释的模型特性。浅层嵌入构建了层次化嵌入。

    

    在过去的十年里，深度学习主要推动了人工智能领域的兴趣。迄今为止，深度学习研究人员在图像处理领域取得了显著的成功，其中使用了卷积神经网络。尽管在图像分类方面表现出色，但卷积神经网络在图像嵌入空间上没有设定归纳偏置，可以说相当幼稚。另一种类型的卷积网络 - 图卷积神经网络也存在类似的缺陷。然而，使用非欧几里德空间来嵌入数据可能会产生更强大和可解释的模型。非欧几里德空间中的一个例子就是双曲空间。由于能够将更多的数据适应于低维空间并具有树状特性，双曲空间尤其有用。先前的多篇论文已经表明，这些吸引人的特性有助于使用浅层嵌入来构建层次化嵌入。

    Deep Learning is mostly responsible for the surge of interest in Artificial Intelligence in the last decade. So far, deep learning researchers have been particularly successful in the domain of image processing, where Convolutional Neural Networks are used. Although excelling at image classification, Convolutional Neural Networks are quite naive in that no inductive bias is set on the embedding space for images. Similar flaws are also exhibited by another type of Convolutional Networks - Graph Convolutional Neural Networks. However, using non-Euclidean space for embedding data might result in more robust and explainable models. One example of such a non-Euclidean space is hyperbolic space. Hyperbolic spaces are particularly useful due to their ability to fit more data in a low-dimensional space and tree-likeliness properties. These attractive properties have been previously used in multiple papers which indicated that they are beneficial for building hierarchical embeddings using shall
    
[^54]: 所有东西同时受扰：实现可微分的图攻击

    Everything Perturbed All at Once: Enabling Differentiable Graph Attacks. (arXiv:2308.15614v1 [cs.LG])

    [http://arxiv.org/abs/2308.15614](http://arxiv.org/abs/2308.15614)

    提出了一种称为可微分图攻击（DGA）的新攻击方法，在连续松弛和参数化图结构的基础上，通过有效生成攻击的同时消除了昂贵的重新训练过程，与最先进的方法相比具有几乎相当的攻击性能，但训练成本减少6倍。

    

    作为图表示学习的强大工具，图神经网络（GNN）在社交网络、推荐系统和在线网络服务等应用中发挥着重要作用。然而，GNN对于对抗性攻击具有脆弱性，这会显著降低其效果。最近的对抗性攻击方法借助基于梯度的元学习选取攻击得分最高的单个边进行干扰，直到达到预算限制。虽然能够有效找出易受攻击的连接，但这些方法计算成本高。通过利用连续松弛和参数化图结构，我们提出了一种称为可微分图攻击（DGA）的新攻击方法，可以有效地生成攻击，并同时消除了昂贵的重新训练过程。与最先进的方法相比，DGA在训练成本减少了6倍的情况下实现了几乎相当的攻击性能。

    As powerful tools for representation learning on graphs, graph neural networks (GNNs) have played an important role in applications including social networks, recommendation systems, and online web services. However, GNNs have been shown to be vulnerable to adversarial attacks, which can significantly degrade their effectiveness. Recent state-of-the-art approaches in adversarial attacks rely on gradient-based meta-learning to selectively perturb a single edge with the highest attack score until they reach the budget constraint. While effective in identifying vulnerable links, these methods are plagued by high computational costs. By leveraging continuous relaxation and parameterization of the graph structure, we propose a novel attack method called Differentiable Graph Attack (DGA) to efficiently generate effective attacks and meanwhile eliminate the need for costly retraining. Compared to the state-of-the-art, DGA achieves nearly equivalent attack performance with 6 times less trainin
    
[^55]: 混合方差流用于离散变量

    Mixed Variational Flows for Discrete Variables. (arXiv:2308.15613v1 [stat.CO])

    [http://arxiv.org/abs/2308.15613](http://arxiv.org/abs/2308.15613)

    本文提出了一种混合方差流方法，用于近似离散分布，通过开发一个离散且保持度量的映射，而不需要连续嵌入。实验证明，与连续嵌入流相比，该方法产生更可靠的近似。

    

    变分流允许从事者学习复杂的连续分布，但是近似离散分布仍然是一个挑战。目前的方法通常将离散目标嵌入连续空间中-通常是通过连续松弛或去量化-然后应用连续流动。这些方法涉及一个可能无法捕捉到原始离散目标的替代目标，可能具有偏倚或不稳定的梯度，并且可能会创建一个困难的优化问题。在这项工作中，我们开发了一种针对离散分布的变分流族，而不需要任何连续嵌入。首先，我们开发了一个保持度量的离散可逆映射，使离散目标保持不变，然后基于该映射创建了一个混合变分流(MAD Mix)。我们还开发了一个扩展，用于处理联合离散和连续模型。我们的实验表明，MAD Mix产生了比连续嵌入流更可靠的近似。

    Variational flows allow practitioners to learn complex continuous distributions, but approximating discrete distributions remains a challenge. Current methodologies typically embed the discrete target in a continuous space - usually via continuous relaxation or dequantization - and then apply a continuous flow. These approaches involve a surrogate target that may not capture the original discrete target, might have biased or unstable gradients, and can create a difficult optimization problem. In this work, we develop a variational flow family for discrete distributions without any continuous embedding. First, we develop a measure-preserving and discrete (MAD) invertible map that leaves the discrete target invariant, and then create a mixed variational flow (MAD Mix) based on that map. We also develop an extension to MAD Mix that handles joint discrete and continuous models. Our experiments suggest that MAD Mix produces more reliable approximations than continuous-embedding flows while 
    
[^56]: InstaTune: 在微调期间即时神经架构搜索

    InstaTune: Instantaneous Neural Architecture Search During Fine-Tuning. (arXiv:2308.15609v1 [cs.LG])

    [http://arxiv.org/abs/2308.15609](http://arxiv.org/abs/2308.15609)

    InstaTune是一种在微调阶段即时生成超级网络的方法，可以最小化神经架构搜索所需的时间和计算资源。

    

    一次性神经架构搜索（NAS）算法通常依赖于为特定领域任务训练硬件无关的超级网络。然后从训练好的超级网络中提取出适用于不同硬件平台的最佳子网络。然而，从头开始训练超级网络可能非常耗时且计算密集，特别是对于依赖于预训练和微调的两阶段训练过程的大型模型。现有的最先进的预训练模型适用于各种任务，但它们的尺寸较大，极大限制了它们在不同硬件平台上的适用性。我们提出了InstaTune，一种利用即时微调阶段生成超级网络的方法。InstaTune具有多个优点。首先，由于该过程发生在微调期间，它可以最小化进行NAS所需的总时间和计算资源。其次，提取出的子网络针对目标任务进行了优化。

    One-Shot Neural Architecture Search (NAS) algorithms often rely on training a hardware agnostic super-network for a domain specific task. Optimal sub-networks are then extracted from the trained super-network for different hardware platforms. However, training super-networks from scratch can be extremely time consuming and compute intensive especially for large models that rely on a two-stage training process of pre-training and fine-tuning. State of the art pre-trained models are available for a wide range of tasks, but their large sizes significantly limits their applicability on various hardware platforms. We propose InstaTune, a method that leverages off-the-shelf pre-trained weights for large models and generates a super-network during the fine-tuning stage. InstaTune has multiple benefits. Firstly, since the process happens during fine-tuning, it minimizes the overall time and compute resources required for NAS. Secondly, the sub-networks extracted are optimized for the target ta
    
[^57]: 测量篡改检测基准

    Measurement Tampering Detection Benchmark. (arXiv:2308.15605v1 [cs.LG])

    [http://arxiv.org/abs/2308.15605](http://arxiv.org/abs/2308.15605)

    本文构建了四个文本数据集用于评估测量篡改检测技术，研究人工智能系统操纵测量结果以营造良好结果的问题。虽然展示了优于基准的技术，但还有很大的改进空间。

    

    在训练强大的人工智能系统来执行复杂任务时，提供对优化具有稳健性的训练信号可能是具有挑战性的。一个问题是测量篡改，即人工智能系统操纵多个测量结果，以营造良好结果的假象，而不是实现期望的结果。在这项工作中，我们构建了四个新的基于文本的数据集，用于评估大规模语言模型上的测量篡改检测技术。具体来说，给定一组文本输入和测量结果，旨在确定某个结果是否发生，以及一个能够准确预测测量结果的基础模型，目标是确定所有测量结果都表明结果发生的示例是否确实发生了结果，或者这是由于测量篡改引起的。我们展示了在大多数数据集上优于简单基准的技术，但没有达到最佳性能。我们相信在技术和数据集方面都有很大的改进空间，我们感到兴奋。

    When training powerful AI systems to perform complex tasks, it may be challenging to provide training signals which are robust to optimization. One concern is measurement tampering, where the AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome. In this work, we build four new text-based datasets to evaluate measurement tampering detection techniques on large language models. Concretely, given sets of text inputs and measurements aimed at determining if some outcome occurred, as well as a base model able to accurately predict measurements, the goal is to determine if examples where all measurements indicate the outcome actually had the outcome occur, or if this was caused by measurement tampering. We demonstrate techniques that outperform simple baselines on most datasets, but don't achieve maximum performance. We believe there is significant room for improvement for both techniques and datasets, and we are excited 
    
[^58]: 分布式图神经网络训练的分区策略的实验比较

    An Experimental Comparison of Partitioning Strategies for Distributed Graph Neural Network Training. (arXiv:2308.15602v1 [cs.DC])

    [http://arxiv.org/abs/2308.15602](http://arxiv.org/abs/2308.15602)

    本文研究了分布式图神经网络训练中分区策略的有效性，并探究了不同因素对分区效果的影响。

    

    最近，图神经网络（GNNs）作为一种能够在图结构化数据上学习的深度学习领域，受到了广泛关注。然而，对于大规模图上的GNN训练，计算和内存要求可能超过单台机器或GPU的能力，因此分布式GNN训练成为大规模GNN训练的有前途的方向。分布式GNN训练的先决条件是将输入图分割成较小的部分，这些部分分布在计算集群的多台机器间。虽然图分区在图分析和图数据库方面已经得到了广泛研究，但其对GNN训练性能的影响尚未得到深入探索。在本文中，我们研究了分区对分布式GNN训练的效果。我们的研究旨在了解不同因素（如GNN参数、小批量大小、图类型、特征大小和扩展因子）对分区效果的影响。

    Recently, graph neural networks (GNNs) have gained much attention as a growing area of deep learning capable of learning on graph-structured data. However, the computational and memory requirements for training GNNs on large-scale graphs can exceed the capabilities of single machines or GPUs, making distributed GNN training a promising direction for large-scale GNN training. A prerequisite for distributed GNN training is to partition the input graph into smaller parts that are distributed among multiple machines of a compute cluster. Although graph partitioning has been extensively studied with regard to graph analytics and graph databases, its effect on GNN training performance is largely unexplored.  In this paper, we study the effectiveness of graph partitioning for distributed GNN training. Our study aims to understand how different factors such as GNN parameters, mini-batch size, graph type, features size, and scale-out factor influence the effectiveness of graph partitioning. We 
    
[^59]: 变形金刚是否能学会最大公约数？

    Can transformers learn the greatest common divisor?. (arXiv:2308.15594v1 [cs.LG])

    [http://arxiv.org/abs/2308.15594](http://arxiv.org/abs/2308.15594)

    本文研究了小型变形金刚模型计算最大公约数的能力。通过选择合适的训练分布和表示基准，模型可以达到高准确率，并在预测中表现出明确的模式。

    

    本文研究小型变形金刚模型计算两个正整数的最大公约数（GCD）的能力。当训练分布和表示基准仔细选择时，模型可以达到98%的准确率，并且正确预测前100个GCD中的91个。模型的预测是确定性的，并且完全可解释的。在训练过程中，模型学会将具有相同GCD的输入对聚类，并通过其除数进行分类。基本模型通过使用小型基数编码的均匀操作数仅计算少数GCD（最多100个中的38个）：基数的除数乘积。更长的训练时间和更大的基数允许一些模型“了解”小的素数GCD。使用对数均匀操作数进行训练将性能提升到正确的73个GCD，并通过从倒数平方到对数均匀的GCD训练分布的平衡，使性能达到91个GCD。从GCD的均匀分布进行训练模型破坏了确定性模型行为。

    I investigate the capability of small transformers to compute the greatest common divisor (GCD) of two positive integers. When the training distribution and the representation base are carefully chosen, models achieve 98% accuracy and correctly predict 91 of the 100 first GCD. Model predictions are deterministic and fully interpretable. During training, the models learn to cluster input pairs with the same GCD, and classify them by their divisors. Basic models, trained from uniform operands encoded on small bases, only compute a handful of GCD (up to 38 out of 100): the products of divisors of the base. Longer training and larger bases allow some models to "grok" small prime GCD. Training from log-uniform operands boosts performance to 73 correct GCD, and balancing the training distribution of GCD, from inverse square to log-uniform, to 91 GCD. Training models from a uniform distribution of GCD breaks the deterministic model behavior.
    
[^60]: 原型分裂：用于稳健的开放式半监督学习的封闭集合

    Prototype Fission: Closing Set for Robust Open-set Semi-supervised Learning. (arXiv:2308.15575v1 [cs.LG])

    [http://arxiv.org/abs/2308.15575](http://arxiv.org/abs/2308.15575)

    本文提出了一种名为原型分裂（PF）的方法，通过自动细粒度的潜在空间挖掘将类别的潜在空间分割为紧凑的子空间，从而在半监督学习中更好地拒绝超出分布的样本。

    

    半监督学习（SSL）在现实的大规模无监督数据集中被证明容易受到来自超出分布（OOD）样本的攻击，因为过于自信地将OOD样本误判为目标分布（ID）样本。一个关键的问题是由于SSL的自我训练循环导致了来自已知分布空间的潜在空间扩散到未知分布空间，且这种偏差在SSL中会被放大。为了封闭ID分布集合，以便更好地拒绝OOD样本以实现安全的SSL，我们提出了原型分裂（PF），通过自动细粒度的潜在空间挖掘将类别的潜在空间分割为紧凑的子空间，仅依赖于粗粒度标签。具体而言，我们为每个类别形成多个独特的可学习子类别原型，优化其多样性和一致性。多样性建模项鼓励样本被聚集到多个子类别原型之一，而一致性建模项将同一类别的所有样本聚集到一个全局原型上。

    Semi-supervised Learning (SSL) has been proven vulnerable to out-of-distribution (OOD) samples in realistic large-scale unsupervised datasets due to over-confident pseudo-labeling OODs as in-distribution (ID). A key underlying problem is class-wise latent space spreading from closed seen space to open unseen space, and the bias is further magnified in SSL's self-training loops. To close the ID distribution set so that OODs are better rejected for safe SSL, we propose Prototype Fission(PF) to divide class-wise latent spaces into compact sub-spaces by automatic fine-grained latent space mining, driven by coarse-grained labels only. Specifically, we form multiple unique learnable sub-class prototypes for each class, optimized towards both diversity and consistency. The Diversity Modeling term encourages samples to be clustered by one of the multiple sub-class prototypes, while the Consistency Modeling term clusters all samples of the same class to a global prototype. Instead of "opening s
    
[^61]: 在任务驱动的fMRI中学习序列信息用于合成数据增益

    Learning Sequential Information in Task-based fMRI for Synthetic Data Augmentation. (arXiv:2308.15564v1 [eess.IV])

    [http://arxiv.org/abs/2308.15564](http://arxiv.org/abs/2308.15564)

    本文提出了一种方法，通过生成合成fMRI序列来增强训练数据集，以解决医学图像分析中训练数据不足的问题。通过适应α-GAN结构和聚合时序信息，合成的任务驱动fMRI可以有效地提供数据增益，并在自闭症谱系障碍(ASD)分类任务中取得了良好的结果。

    

    在医学图像分析中，训练数据不足是一个持续存在的问题，特别是对于使用特定认知任务获取的时空成像数据的任务驱动功能性磁共振图像(fMRI)。本文提出了一种生成合成fMRI序列的方法，可以用于创建下游学习任务的增强训练数据集。为了合成高分辨率的任务特定fMRI，我们采用了α-GAN结构，充分利用了GAN和变分自编码器模型的优势，并提出了在聚合时序信息方面的不同选择。通过可视化和自闭症谱系障碍(ASD)分类任务对合成图像进行评估。结果表明，合成的任务驱动fMRI可以在学习ASD分类任务中提供有效的数据增益。

    Insufficiency of training data is a persistent issue in medical image analysis, especially for task-based functional magnetic resonance images (fMRI) with spatio-temporal imaging data acquired using specific cognitive tasks. In this paper, we propose an approach for generating synthetic fMRI sequences that can then be used to create augmented training datasets in downstream learning tasks. To synthesize high-resolution task-specific fMRI, we adapt the $\alpha$-GAN structure, leveraging advantages of both GAN and variational autoencoder models, and propose different alternatives in aggregating temporal information. The synthetic images are evaluated from multiple perspectives including visualizations and an autism spectrum disorder (ASD) classification task. The results show that the synthetic task-based fMRI can provide effective data augmentation in learning the ASD classification task.
    
[^62]: 足球中预期进球模型的全局解释

    Glocal Explanations of Expected Goal Models in Soccer. (arXiv:2308.15559v1 [cs.LG])

    [http://arxiv.org/abs/2308.15559](http://arxiv.org/abs/2308.15559)

    本文提出了预期进球模型的全局解释（介于本地和全局之间的解释），通过使用聚合版本的SHAP值和部分依赖函数，可以对团队和球员水平进行绩效分析和知识提取。

    

    预期进球模型的解释性通常有限，尤其是在使用黑盒方法进行训练时。随着解释性人工智能工具的出现，可以增强模型的透明度，并从单个观察或所有观察中提取描述性知识。然而，在某些领域中，解释特定群体观察的黑盒模型可能更有用。本文通过提出使用SHAP值和部分依赖函数的聚合版本，引入预期进球模型的全局解释（介于本地和全局之间的解释）来实现对团队和球员水平的绩效分析。这样可以从预期进球模型中提取与球员或球队相关的知识，而不仅仅是单个射门。另外，我们进行了实际数据应用来说明聚合SHAP值和聚合函数的有用性。本文最后对这些解释的潜力进行了评论。

    The expected goal models have gained popularity, but their interpretability is often limited, especially when trained using black-box methods. Explainable artificial intelligence tools have emerged to enhance model transparency and extract descriptive knowledge for a single observation or for all observations. However, explaining black-box models for a specific group of observations may be more useful in some domains. This paper introduces the glocal explanations (between local and global levels) of the expected goal models to enable performance analysis at the team and player levels by proposing the use of aggregated versions of the SHAP values and partial dependence profiles. This allows knowledge to be extracted from the expected goal model for a player or team rather than just a single shot. In addition, we conducted real-data applications to illustrate the usefulness of aggregated SHAP and aggregated profiles. The paper concludes with remarks on the potential of these explanations
    
[^63]: 使用伪布尔多项式进行簇分析的降维方法

    Dimensionality Reduction Using pseudo-Boolean polynomials For Cluster Analysis. (arXiv:2308.15553v1 [cs.IR])

    [http://arxiv.org/abs/2308.15553](http://arxiv.org/abs/2308.15553)

    该论文介绍了使用伪布尔多项式的惩罚性表达式作为簇分析中的降维方法，能够以竞争性的准确度、可重复性和清晰解释性提取簇。

    

    我们引入了一种使用伪布尔多项式的惩罚性表达式的降维方法，作为簇分析过程中不变降维的机制。在我们的实验中，我们展示了多维数据，如4维鸢尾花数据集可以降至2维空间，而30维威斯康星州乳腺癌（WDBC）数据集可以降至3维空间，通过搜索位于降维样本之间的直线或平面，我们可以以线性和无偏的方式提取簇，并且具有竞争性的准确度，可重复性和清晰的解释性。

    We introduce usage of a reduction property of penalty-based formulation of pseudo-Boolean polynomials as a mechanism for invariant dimensionality reduction in cluster analysis processes. In our experiments, we show that multidimensional data, like 4-dimensional Iris Flower dataset can be reduced to 2-dimensional space while the 30-dimensional Wisconsin Diagnostic Breast Cancer (WDBC) dataset can be reduced to 3-dimensional space, and by searching lines or planes that lie between reduced samples we can extract clusters in a linear and unbiased manner with competitive accuracies, reproducibility and clear interpretation.
    
[^64]: 纯探索下的中介反馈

    Pure Exploration under Mediators' Feedback. (arXiv:2308.15552v1 [cs.LG])

    [http://arxiv.org/abs/2308.15552](http://arxiv.org/abs/2308.15552)

    本研究提出了一种严格推广的传统最优臂识别问题，即中介反馈下的最优臂识别（BAI-MF），通过引入中介者来模拟一些实际决策问题，如离线学习、部分可控环境和人类反馈。

    

    随机多臂赌博机是一种顺序决策框架，每一步交互中学习者选择一个臂并观察一个随机回报。在最优臂识别（BAI）问题的背景下，学习者的目标是尽可能准确和高效地找到最优臂，即具有最高期望回报的臂。然而，传统BAI问题的顺序交互协议，即学习者在每一轮中对选择的臂具有完全控制权，无法有效地模拟一些值得关注的决策问题（例如，离线学习，部分可控环境和人类反馈）。因此，在这项工作中，我们提出了一种新的严格推广的传统BAI问题，称之为中介反馈下的最优臂识别（BAI-MF）。更具体地说，我们考虑了学习者可以访问一组中介者的情况，每个中介者都选择要拉动的臂。

    Stochastic multi-armed bandits are a sequential-decision-making framework, where, at each interaction step, the learner selects an arm and observes a stochastic reward. Within the context of best-arm identification (BAI) problems, the goal of the agent lies in finding the optimal arm, i.e., the one with highest expected reward, as accurately and efficiently as possible. Nevertheless, the sequential interaction protocol of classical BAI problems, where the agent has complete control over the arm being pulled at each round, does not effectively model several decision-making problems of interest (e.g., off-policy learning, partially controllable environments, and human feedback). For this reason, in this work, we propose a novel strict generalization of the classical BAI problem that we refer to as best-arm identification under mediators' feedback (BAI-MF). More specifically, we consider the scenario in which the learner has access to a set of mediators, each of which selects the arms on 
    
[^65]: 对抗式风格转移在深度强化学习中的鲁棒策略优化

    Adversarial Style Transfer for Robust Policy Optimization in Deep Reinforcement Learning. (arXiv:2308.15550v1 [cs.LG])

    [http://arxiv.org/abs/2308.15550](http://arxiv.org/abs/2308.15550)

    本文提出了一种对抗式风格转移的算法，通过消除对混淆特征的过拟合来提高深度强化学习智能体的泛化能力。这种算法使用了生成器和策略网络，并通过最大-最小博弈的方式进行优化，以找到一个可以泛化到未见环境的鲁棒策略。实验证明，这种算法相比于其他基准算法有更好的性能。

    

    本文提出了一种算法，旨在通过消除对混淆特征过拟合来提高强化学习智能体的泛化能力。我们的方法包括了一个最大-最小博弈理论的目标。生成器在强化学习过程中转移观察样式。生成器的另一个目标是扰动观察，以最大化智能体采取不同行动的概率。相反，策略网络更新其参数以最小化这种扰动的影响，从而在最大化未来预期奖励的同时保持鲁棒性。基于这一设置，我们提出了一个实用的深度强化学习算法，Adversarial Robust Policy Optimization (ARPO)，以找到一个可以泛化到未见环境的鲁棒策略。我们在Procgen和Distracting Control Suite上评估了我们的方法的泛化和样本效率。实验证明，与几种基准算法（包括数据增广）相比，ARPO表现出了改进的性能。

    This paper proposes an algorithm that aims to improve generalization for reinforcement learning agents by removing overfitting to confounding features. Our approach consists of a max-min game theoretic objective. A generator transfers the style of observation during reinforcement learning. An additional goal of the generator is to perturb the observation, which maximizes the agent's probability of taking a different action. In contrast, a policy network updates its parameters to minimize the effect of such perturbations, thus staying robust while maximizing the expected future reward. Based on this setup, we propose a practical deep reinforcement learning algorithm, Adversarial Robust Policy Optimization (ARPO), to find a robust policy that generalizes to unseen environments. We evaluate our approach on Procgen and Distracting Control Suite for generalization and sample efficiency. Empirically, ARPO shows improved performance compared to a few baseline algorithms, including data augmen
    
[^66]: 调整困惑度并计算基于采样的t-SNE嵌入

    Tuning the perplexity for and computing sampling-based t-SNE embeddings. (arXiv:2308.15513v1 [cs.LG])

    [http://arxiv.org/abs/2308.15513](http://arxiv.org/abs/2308.15513)

    本文通过采样的方法改进了大数据集下t-SNE嵌入的质量和计算速度。

    

    高维数据分析常用的管道利用二维可视化，例如通过t分布邻近随机嵌入（t-SNE）。但在处理大数据集时，应用这些可视化技术会生成次优的嵌入，因为超参数不适用于大数据。将这些参数增加通常不起作用，因为计算对于实际工作流程来说太昂贵。本文中，我们认为基于采样的嵌入方法可以解决这些问题。我们展示了必须谨慎选择超参数，取决于采样率和预期的最终嵌入。此外，我们展示了该方法如何加速计算并提高嵌入的质量。

    Widely used pipelines for the analysis of high-dimensional data utilize two-dimensional visualizations. These are created, e.g., via t-distributed stochastic neighbor embedding (t-SNE). When it comes to large data sets, applying these visualization techniques creates suboptimal embeddings, as the hyperparameters are not suitable for large data. Cranking up these parameters usually does not work as the computations become too expensive for practical workflows. In this paper, we argue that a sampling-based embedding approach can circumvent these problems. We show that hyperparameters must be chosen carefully, depending on the sampling rate and the intended final embedding. Further, we show how this approach speeds up the computation and increases the quality of the embeddings.
    
[^67]: unORANIC: 无监督解剖与图像特征的正交化

    unORANIC: Unsupervised Orthogonalization of Anatomy and Image-Characteristic Features. (arXiv:2308.15507v1 [eess.IV])

    [http://arxiv.org/abs/2308.15507](http://arxiv.org/abs/2308.15507)

    unORANIC是一种无监督的方法，通过正交化解剖和图像特征，提高了医学图像分析的泛化能力和抗损坏能力。

    

    我们介绍了unORANIC，一种使用调整后的损失函数驱动解剖和图像特征的正交化的无监督方法。该方法适用于不同的模态和任务，不需要领域知识、配对数据样本或标签。在测试时，unORANIC被应用于可能有损坏的图像，将其解剖学和特征分量正交化，然后重建无损坏的图像，仅显示其领域不变的解剖学特征。这种特征的正交化进一步提高了泛化能力和抗损坏能力。我们通过评估unORANIC的分类准确性、损坏检测和修订能力，在5个不同的数据集上 qualitatively 和 quantitatively 确认了这一点。我们的方法显示了在医学图像分析的实际应用中提高泛化能力和鲁棒性的潜力。源代码可在 https://github.com/sdoerrich97/unORANIC 获取。

    We introduce unORANIC, an unsupervised approach that uses an adapted loss function to drive the orthogonalization of anatomy and image-characteristic features. The method is versatile for diverse modalities and tasks, as it does not require domain knowledge, paired data samples, or labels. During test time unORANIC is applied to potentially corrupted images, orthogonalizing their anatomy and characteristic components, to subsequently reconstruct corruption-free images, showing their domain-invariant anatomy only. This feature orthogonalization further improves generalization and robustness against corruptions. We confirm this qualitatively and quantitatively on 5 distinct datasets by assessing unORANIC's classification accuracy, corruption detection and revision capabilities. Our approach shows promise for enhancing the generalizability and robustness of practical applications in medical image analysis. The source code is available at https://github.com/sdoerrich97/unORANIC.
    
[^68]: 关于选定学习模型的隐写容量

    On the Steganographic Capacity of Selected Learning Models. (arXiv:2308.15502v1 [cs.LG])

    [http://arxiv.org/abs/2308.15502](http://arxiv.org/abs/2308.15502)

    本研究考虑了学习模型的隐写容量问题，确定了在不影响模型性能的前提下可以覆盖的已训练参数的低阶位数，并分析了每个模型的准确性随低阶位数变化的关系，以及选定模型各层的隐写容量。

    

    机器学习和深度学习模型是各种攻击场景的潜在载体。例如，先前的研究表明，恶意软件可以隐藏在深度学习模型中。将信息隐藏在学习模型中可以视为一种隐写术形式。在这项研究中，我们考虑了学习模型的隐写容量的一般问题。具体来说，对于各种模型，我们确定了可以覆盖而不会对模型性能产生不利影响的已训练参数的低阶位数。对于每个考虑的模型，我们以已覆盖的低阶位数作为自变量绘制准确性图，并针对选定的模型分析了每个层的隐写容量。我们测试的模型包括经典机器学习技术的线性回归（LR）和支持向量机（SVM）；流行的通用深度学习模型的多层感知机（MLP）和卷积神经网络（CNN）。

    Machine learning and deep learning models are potential vectors for various attack scenarios. For example, previous research has shown that malware can be hidden in deep learning models. Hiding information in a learning model can be viewed as a form of steganography. In this research, we consider the general question of the steganographic capacity of learning models. Specifically, for a wide range of models, we determine the number of low-order bits of the trained parameters that can be overwritten, without adversely affecting model performance. For each model considered, we graph the accuracy as a function of the number of low-order bits that have been overwritten, and for selected models, we also analyze the steganographic capacity of individual layers. The models that we test include the classic machine learning techniques of Linear Regression (LR) and Support Vector Machine (SVM); the popular general deep learning models of Multilayer Perceptron (MLP) and Convolutional Neural Netwo
    
[^69]: 从在线论坛中检测不活跃的网络战士

    Detecting Inactive Cyberwarriors from Online Forums. (arXiv:2308.15491v1 [cs.SI])

    [http://arxiv.org/abs/2308.15491](http://arxiv.org/abs/2308.15491)

    本研究调查了大型在线论坛中网络战士的活动水平，发现只有少数网络战士是活跃用户，他们在和平时期保持沉默，只在必要时行动。此外，检测不活跃的网络战士比识别活跃的网络战士更具挑战性。研究提供了更好捕捉网络战士行动的方法。

    

    在信息时代，虚假信息的传播成为一种新形式的战争。这种战争涉及到网络战士，他们有意传播旨在诽谤对手或团结盟友的信息。在这项研究中，我们调查了大型在线论坛中网络战士的活动水平，令人惊讶的是，我们发现只有少数网络战士是活跃用户。令人意外的是，尽管他们被期望积极传播虚假信息，在和平时期网络战士却保持沉默，只在必要时才行动起来。此外，我们分析了识别网络战士所面临的挑战，并提供证据表明检测不活跃的网络战士比识别活跃的网络战士要难得多。最后，我们讨论了更有效地在网络战士不活跃的阶段识别他们的潜在方法，为更好地捕捉他们的行动提供了洞察。

    The proliferation of misinformation has emerged as a new form of warfare in the information age. This type of warfare involves cyberwarriors, who deliberately propagate messages aimed at defaming opponents or fostering unity among allies. In this study, we investigate the level of activity exhibited by cyberwarriors within a large online forum, and remarkably, we discover that only a minute fraction of cyberwarriors are active users. Surprisingly, despite their expected role of actively disseminating misinformation, cyberwarriors remain predominantly silent during peacetime and only spring into action when necessary. Moreover, we analyze the challenges associated with identifying cyberwarriors and provide evidence that detecting inactive cyberwarriors is considerably more challenging than identifying their active counterparts. Finally, we discuss potential methodologies to more effectively identify cyberwarriors during their inactive phases, offering insights into better capturing thei
    
[^70]: 参数服务器在迭代收敛分布式机器学习中的阻滞问题的实证研究

    Empirical Study of Straggler Problem in Parameter Server on Iterative Convergent Distributed Machine Learning. (arXiv:2308.15482v1 [cs.DC])

    [http://arxiv.org/abs/2308.15482](http://arxiv.org/abs/2308.15482)

    本研究通过实验测试了当前阻滞缓解技术在不同迭代收敛机器学习算法上的有效性，并分析了参数服务器策略在并行学习问题中的实验安排。这些研究结果为进一步研究提供了必要的平台。

    

    本研究旨在测试当前阻滞缓解技术在不同重要的迭代收敛机器学习算法（包括矩阵分解，多项逻辑回归和潜在狄利克雷分配）上的有效性。实验使用了最新的参数服务器体系结构实现的FlexPS系统进行实施。实验采用了批同步并行计算模型，以检查参数服务器在迭代收敛分布式机器学习中的阻滞问题。此外，本研究分析了参数服务器策略在并行学习问题中的实验安排，通过注入通用阻滞模式和执行最新的缓解技术。研究结果的意义在于为进一步研究提供了必要的平台。

    The purpose of this study is to test the effectiveness of current straggler mitigation techniques over different important iterative convergent machine learning(ML) algorithm including Matrix Factorization (MF), Multinomial Logistic Regression (MLR), and Latent Dirichlet Allocation (LDA) . The experiment was conducted to implemented using the FlexPS system, which is the latest system implementation that employ parameter server architecture. The experiment employed the Bulk Synchronous Parallel (BSP) computational model to examine the straggler problem in Parameter Server on Iterative Convergent Distributed Machine Learning. Moreover, the current research analyzes the experimental arrangement of the parameter server strategy concerning the parallel learning problems by injecting universal straggler patterns and executing latest mitigation techniques. The findings of the study are significant in that as they will provide the necessary platform for conducting further research into the pro
    
[^71]: 在HPC系统中的在线作业失败预测

    Online Job Failure Prediction in an HPC System. (arXiv:2308.15481v1 [cs.DC])

    [http://arxiv.org/abs/2308.15481](http://arxiv.org/abs/2308.15481)

    该论文研究了使用经典的机器学习算法在提交时间上进行作业失败预测，并结合自然语言处理工具来表示作业。该方法可用于优化高性能计算系统管理，提高性能和能源效率。

    

    现代高性能计算（HPC）系统是复杂的机器，对经济和社会都有重大影响。除了计算能力外，能源消耗也在不断增长，这是一个关键问题，考虑到当前的环境和能源危机。因此，开发优化HPC系统管理的策略至关重要，既可以保证一流的性能，又可以提高能源效率。一种策略是通过在工作负载级别上进行操作，并在系统上执行之前突出显示最有可能失败的作业。作业在执行过程中失败会不必要地占用资源，可能会延迟其他作业，对系统性能和能源消耗产生不利影响。在本文中，我们使用经典的机器学习算法研究了提交时间上的作业失败预测。我们的创新在于（i）将这些算法与自然语言处理（NLP）工具结合起来，以表示作业和（ii）th

    Modern High Performance Computing (HPC) systems are complex machines, with major impacts on economy and society. Along with their computational capability, their energy consumption is also steadily raising, representing a critical issue given the ongoing environmental and energetic crisis. Therefore, developing strategies to optimize HPC system management has paramount importance, both to guarantee top-tier performance and to improve energy efficiency. One strategy is to act at the workload level and highlight the jobs that are most likely to fail, prior to their execution on the system. Jobs failing during their execution unnecessarily occupy resources which could delay other jobs, adversely affecting the system performance and energy consumption. In this paper, we study job failure prediction at submit-time using classical machine learning algorithms. Our novelty lies in (i) the combination of these algorithms with Natural Language Processing (NLP) tools to represent jobs and (ii) th
    
[^72]: 强化学习中的政策组合通过多目标政策优化

    Policy composition in reinforcement learning via multi-objective policy optimization. (arXiv:2308.15470v1 [cs.LG])

    [http://arxiv.org/abs/2308.15470](http://arxiv.org/abs/2308.15470)

    通过多目标政策优化方法，我们将预先存在的教师策略引入到强化学习中，证明了教师策略在无形状奖励下能够加速学习过程。我们的智能体成功地组合教师策略并扩展教师的策略以解决任务。

    

    我们通过使用相关的预先存在的教师策略，使强化学习智能体能够学习成功的行为策略。教师策略被引入作为目标，除了任务目标以外，在多目标政策优化的设置中。我们使用多目标最大后验政策优化算法，展示了教师策略能够加速学习的过程，尤其是在缺少形状奖励的情况下。在具有连续观察和行动空间的两个域中，我们的智能体成功地按顺序和并行地组合教师策略，并且还能够进一步扩展教师的策略以解决任务。根据任务和教师的指定组合，教师可能自然地限制智能体的最终性能。智能体需要遵守教师策略的程度由超参数决定，这些超参数确定了教师策略的影响程度。

    We enable reinforcement learning agents to learn successful behavior policies by utilizing relevant pre-existing teacher policies. The teacher policies are introduced as objectives, in addition to the task objective, in a multi-objective policy optimization setting. Using the Multi-Objective Maximum a Posteriori Policy Optimization algorithm \citep{abdolmaleki2020distributional}, we show that teacher policies can help speed up learning, particularly in the absence of shaping rewards. In two domains with continuous observation and action spaces, our agents successfully compose teacher policies in sequence and in parallel, and are also able to further extend the policies of the teachers in order to solve the task.  Depending on the specified combination of task and teacher(s), teacher(s) may naturally act to limit the final performance of an agent. The extent to which agents are required to adhere to teacher policies are determined by hyperparameters which determine both the effect of te
    
[^73]: 阐明扩散模型中的曝光偏差问题

    Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])

    [http://arxiv.org/abs/2308.15321](http://arxiv.org/abs/2308.15321)

    本文系统地研究了扩散模型中的曝光偏差问题，并提出了一种名为Epsilon Scaling的免训练方法来减轻这一问题。实验结果验证了该方法的有效性。

    

    扩散模型展示了令人印象深刻的生成能力，但它们的“曝光偏差”问题，即训练和采样之间的输入不匹配，缺乏深入探索。本文通过首先对采样分布进行分析建模，然后将每个采样步骤的预测误差归因为曝光偏差问题的根本原因，系统地研究了扩散模型中的曝光偏差问题。此外，我们讨论了解决这个问题的潜在方法，并提出了一个直观的度量标准。除了阐明曝光偏差问题，我们提出了一种简单但有效的免训练方法，称为Epsilon Scaling，以减轻曝光偏差。我们展示了Epsilon Scaling通过缩小网络输出（Epsilon）明确地将采样轨迹移近训练阶段学习到的向量场，从而减轻了训练和采样之间的输入不匹配。在各种扩散框架上进行了实验。

    Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
    
[^74]: 通过正则化Wasserstein Proximals实现无噪声的抽样算法

    Noise-Free Sampling Algorithms via Regularized Wasserstein Proximals. (arXiv:2308.14945v1 [stat.ML])

    [http://arxiv.org/abs/2308.14945](http://arxiv.org/abs/2308.14945)

    本文通过正则化Wasserstein Proximal方法提出了一种无噪声的抽样算法，通过给定的潜势函数确定性地进行粒子演化，并提供了优于传统方法的维度依赖性和速度收敛性能。

    

    本文考虑由潜势函数控制的分布抽样问题。本文提出了一种显式的基于评分的确定性马尔科夫链蒙特卡洛方法，使得粒子的演化变为确定性的，而不是随机微分方程的演化。评分项由正则化的Wasserstein proximal以闭合形式给出，使用采样来近似核卷积。我们在不同问题上展示了快速收敛，并且与未调整Langevin算法和Metropolis调整Langevin算法相比，显示了高斯分布的混合时间边界的改善维度依赖性。我们还推导了二次潜势函数每次迭代的分布的闭合形式表达式，表征了方差降低。实证结果表明，粒子的行为是有组织的，位于潜势的等值线上。此外，后验均值估计结果显示了该方法的有效性。

    We consider the problem of sampling from a distribution governed by a potential function. This work proposes an explicit score-based MCMC method that is deterministic, resulting in a deterministic evolution for particles rather than a stochastic differential equation evolution. The score term is given in closed form by a regularized Wasserstein proximal, using a kernel convolution that is approximated by sampling. We demonstrate fast convergence on various problems and show improved dimensional dependence of mixing time bounds for the case of Gaussian distributions compared to the unadjusted Langevin algorithm (ULA) and the Metropolis-adjusted Langevin algorithm (MALA). We additionally derive closed form expressions for the distributions at each iterate for quadratic potential functions, characterizing the variance reduction. Empirical results demonstrate that the particles behave in an organized manner, lying on level set contours of the potential. Moreover, the posterior mean estimat
    
[^75]: 使用不精确神经网络的分布鲁棒统计验证

    Distributionally Robust Statistical Verification with Imprecise Neural Networks. (arXiv:2308.14815v1 [cs.AI])

    [http://arxiv.org/abs/2308.14815](http://arxiv.org/abs/2308.14815)

    本文提出了一种使用不精确神经网络的分布鲁棒统计验证方法，通过结合主动学习、不确定性量化和神经网络验证，可以在大量的分布上提供对黑盒系统行为的保证。

    

    在AI安全领域，一个特别具有挑战性的问题是在高维自主系统的行为上提供保证。以可达性分析为中心的验证方法无法扩展，而纯粹的统计方法受到对采样过程的分布假设的限制。相反，我们提出了一个针对黑盒系统的分布鲁棒版本的统计验证问题，其中我们的性能保证适用于大量的分布。本文提出了一种基于主动学习、不确定性量化和神经网络验证的新方法。我们方法的一个核心部分是一种称为不精确神经网络的集成技术，它提供了不确定性以指导主动学习。主动学习使用了一种称为Sherlock的全面神经网络验证工具来收集样本。在openAI gym Mujoco环境中使用多个物理模拟器进行评估。

    A particularly challenging problem in AI safety is providing guarantees on the behavior of high-dimensional autonomous systems. Verification approaches centered around reachability analysis fail to scale, and purely statistical approaches are constrained by the distributional assumptions about the sampling process. Instead, we pose a distributionally robust version of the statistical verification problem for black-box systems, where our performance guarantees hold over a large family of distributions. This paper proposes a novel approach based on a combination of active learning, uncertainty quantification, and neural network verification. A central piece of our approach is an ensemble technique called Imprecise Neural Networks, which provides the uncertainty to guide active learning. The active learning uses an exhaustive neural-network verification tool Sherlock to collect samples. An evaluation on multiple physical simulators in the openAI gym Mujoco environments with reinforcement-
    
[^76]: 通过马尔可夫决策过程实体嵌入和代理集合上下文感知地组合代理策略

    Context-Aware Composition of Agent Policies by Markov Decision Process Entity Embeddings and Agent Ensembles. (arXiv:2308.14521v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.14521](http://arxiv.org/abs/2308.14521)

    该论文提出了一种通过马尔可夫决策过程实体嵌入和代理集合的方法，以上下文感知地组合代理策略，以在复杂且动态变化的环境中优化执行活动。

    

    计算代理在生活的许多领域中支持人类，并因此存在异构环境。这意味着它们在快速变化的环境中运作，并且可能面临巨大的状态和动作空间。为了以目标导向的方式执行服务和活动，代理需要先前的知识，因此必须制定和追求依赖于上下文的策略。然而，预先规定策略在动态变化的环境中存在限制和不灵活性。此外，代理的上下文决定了它的动作选择。由于环境可能具有随机性，并且在状态和可行动作的数量上复杂，因此通常通过马尔可夫决策过程以简化的方式建模活动，以便使用强化学习的代理能够学习策略，从而帮助捕捉上下文并根据最优方式执行活动。

    Computational agents support humans in many areas of life and are therefore found in heterogeneous contexts. This means they operate in rapidly changing environments and can be confronted with huge state and action spaces. In order to perform services and carry out activities in a goal-oriented manner, agents require prior knowledge and therefore have to develop and pursue context-dependent policies. However, prescribing policies in advance is limited and inflexible, especially in dynamically changing environments. Moreover, the context of an agent determines its choice of actions. Since the environments can be stochastic and complex in terms of the number of states and feasible actions, activities are usually modelled in a simplified way by Markov decision processes so that, e.g., agents with reinforcement learning are able to learn policies, that help to capture the context and act accordingly to optimally perform activities. However, training policies for all possible contexts using
    
[^77]: EntropyRank: 通过基于语言模型的文本压缩的副信息优化来进行无监督关键词提取

    EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression. (arXiv:2308.13399v1 [cs.CL])

    [http://arxiv.org/abs/2308.13399](http://arxiv.org/abs/2308.13399)

    该论文提出了一种无监督的关键词提取方法，通过利用预训练语言模型和信息论方法，在文本中提取具有最高条件熵的短语作为关键词。实验证明，该方法在关键词提取任务上取得了与常用方法相当的结果。

    

    我们提出了一种无监督的方法，基于预训练的语言模型（LM）和Shannon的信息最大化，从文本中提取关键词和关键词短语。具体来说，我们的方法提取在LM下具有最高条件熵的短语。得到的关键词短语集合解决了一个相关的信息论问题：如果作为副信息提供，它会导致使用LM和熵编码器对文本进行压缩时的预期最小二进制码长度。另外，得到的集合是通过因果LM对在给定条件下最小化文本熵的短语集合的近似。在实证上，该方法在各种关键词提取基准挑战中提供了与最常用方法可比较的结果。

    We propose an unsupervised method to extract keywords and keyphrases from texts based on a pre-trained language model (LM) and Shannon's information maximization. Specifically, our method extracts phrases having the highest conditional entropy under the LM. The resulting set of keyphrases turns out to solve a relevant information-theoretic problem: if provided as side information, it leads to the expected minimal binary code length in compressing the text using the LM and an entropy encoder. Alternately, the resulting set is an approximation via a causal LM to the set of phrases that minimize the entropy of the text when conditioned upon it. Empirically, the method provides results comparable to the most commonly used methods in various keyphrase extraction benchmark challenges.
    
[^78]: 关于平均嵌入用于物品推荐的一致性研究

    On the Consistency of Average Embeddings for Item Recommendation. (arXiv:2308.12767v1 [cs.IR])

    [http://arxiv.org/abs/2308.12767](http://arxiv.org/abs/2308.12767)

    本文研究了推荐系统中平均嵌入的一致性，并提出了一种衡量方法。实证结果表明，现实世界的平均嵌入在推荐中一致性较低，为进一步改进现实世界嵌入提供了方向。

    

    推荐系统中一种流行的做法是将物品嵌入进行平均以在同一嵌入空间中代表用户或更高级的概念。本文研究了这种做法的相关性。为此，我们提出了一种期望精度分数，用于衡量平均嵌入与其构建所使用的物品的一致性。我们随后在具有特定假设的理论环境和来自音乐流媒体服务的真实数据上分析了该分数的数学表达式及其经验表现。我们的结果强调了现实世界的平均值在推荐中的一致性较低，为未来研究更好地将现实世界的嵌入与我们理论环境的假设相一致铺平了道路。

    A prevalent practice in recommender systems consists of averaging item embeddings to represent users or higher-level concepts in the same embedding space. This paper investigates the relevance of such a practice. For this purpose, we propose an expected precision score, designed to measure the consistency of an average embedding relative to the items used for its construction. We subsequently analyze the mathematical expression of this score in a theoretical setting with specific assumptions, as well as its empirical behavior on real-world data from music streaming services. Our results emphasize that real-world averages are less consistent for recommendation, which paves the way for future research to better align real-world embeddings with assumptions from our theoretical setting.
    
[^79]: 跨领域的可信表示学习

    Trustworthy Representation Learning Across Domains. (arXiv:2308.12315v1 [cs.LG])

    [http://arxiv.org/abs/2308.12315](http://arxiv.org/abs/2308.12315)

    本论文首次提出了跨领域的可信表示学习框架，通过包括鲁棒性、隐私、公平性和可解释性等概念，对该研究方向进行了全面的文献综述。

    

    随着人工智能系统在我们日常生活和人类社会中取得显著的性能，人们既享受到了这些技术带来的好处，也面临因这些系统而引发的许多社会问题。为了使人工智能系统足够好并且可信，已经进行了大量研究，建立了可信人工智能系统的指南。机器学习是人工智能系统中最重要的部分之一，而表示学习是机器学习中的基础技术。如何使表示学习在现实世界的应用中具有可信度，例如跨领域场景，对于机器学习和人工智能系统领域都是非常有价值和必要的。在可信人工智能的概念启发下，我们提出了第一个跨领域的可信表示学习框架，包括了鲁棒性、隐私、公平性和可解释性这四个概念，对这个研究方向进行了全面的文献综述。

    As AI systems have obtained significant performance to be deployed widely in our daily live and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts for AI systems and representation learning is the fundamental technology in machine learning. How to make the representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework which includes four concepts, i.e, robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first 
    
[^80]: 在语音、语言和听力科学中建立通用的机器学习模型：功效分析和样本容量估计

    Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Power Analysis and Sample Size Estimation. (arXiv:2308.11197v1 [cs.LG])

    [http://arxiv.org/abs/2308.11197](http://arxiv.org/abs/2308.11197)

    该研究提供了使用嵌套交叉验证方法的定量证据，并提出了基于机器学习分析进行功效分析的方法。通过对交叉验证方法、特征和模型维度之间的相互作用进行蒙特卡罗模拟，比较了不同交叉验证方法的统计功效和置信度。同时，确定了获得统计显著结果所需的最小样本容量。

    

    该研究的第一个目的是提供定量证据，以激励研究人员改用更健壮的嵌套交叉验证方法。第二个目的是在研究设计过程中提出基于机器学习分析的功效分析方法和MATLAB代码。通过蒙特卡罗模拟，量化了所采用的交叉验证方法、特征的判别力、特征空间的维度和模型的维度之间的相互作用。基于机器学习模型的统计功效和统计置信度，比较了四种不同的交叉验证方法（单一留出法、10折交叉验证、训练-验证-测试法和嵌套10折交叉验证）。利用零假设和备择假设的分布确定了获得统计显著结果所需的最小样本容量（α=0.05，1-β=0.8）。模型的统计置信度被定义为正确特征的概率。

    This study's first purpose is to provide quantitative evidence that would incentivize researchers to instead use the more robust method of nested cross-validation. The second purpose is to present methods and MATLAB codes for doing power analysis for ML-based analysis during the design of a study. Monte Carlo simulations were used to quantify the interactions between the employed cross-validation method, the discriminative power of features, the dimensionality of the feature space, and the dimensionality of the model. Four different cross-validations (single holdout, 10-fold, train-validation-test, and nested 10-fold) were compared based on the statistical power and statistical confidence of the ML models. Distributions of the null and alternative hypotheses were used to determine the minimum required sample size for obtaining a statistically significant outcome ({\alpha}=0.05, 1-\b{eta}=0.8). Statistical confidence of the model was defined as the probability of correct features being 
    
[^81]: 基于Transformer的多元时间序列框架：剩余寿命预测应用案例

    A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case. (arXiv:2308.09884v1 [cs.LG])

    [http://arxiv.org/abs/2308.09884](http://arxiv.org/abs/2308.09884)

    这项研究提出了一种基于Transformer的框架，用于多元时间序列预测，以满足预测学应用案例中剩余寿命预测的需求。

    

    最近，大型语言模型（LLMs）吸引了全球关注，并彻底改变了自然语言处理领域。LLMs的有效性可归因于其用于训练的模型架构，即transformers。Transformer模型在捕捉顺序数据中的上下文特征方面表现出色，由于时间序列数据是顺序的，可以利用Transformer模型实现更有效的时间序列数据预测。预测学领域对系统健康管理和适当的维护计划至关重要。对机器剩余可用寿命（RUL）进行可靠估计具有节省成本的潜力。这包括避免机器突然故障，最大限度地利用设备，并作为决策支持系统（DSS）。这项工作提出了一种基于编码器-Transformer架构的多元时间序列预测框架，用于预测学应用案例。我们验证了该框架的有效性。

    In recent times, Large Language Models (LLMs) have captured a global spotlight and revolutionized the field of Natural Language Processing. One of the factors attributed to the effectiveness of LLMs is the model architecture used for training, transformers. Transformer models excel at capturing contextual features in sequential data since time series data are sequential, transformer models can be leveraged for more efficient time series data prediction. The field of prognostics is vital to system health management and proper maintenance planning. A reliable estimation of the remaining useful life (RUL) of machines holds the potential for substantial cost savings. This includes avoiding abrupt machine failures, maximizing equipment usage, and serving as a decision support system (DSS). This work proposed an encoder-transformer architecture-based framework for multivariate time series prediction for a prognostics use case. We validated the effectiveness of the proposed framework on all f
    
[^82]: 扩展AI：向拥有创造性的AlphaZero国际象棋迈进

    Diversifying AI: Towards Creative Chess with AlphaZero. (arXiv:2308.09175v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.09175](http://arxiv.org/abs/2308.09175)

    本研究探索了AI在计算任务中是否可以从创造性决策机制中受益，并通过构建多样化的AI系统团队，在挑战性任务中超越单个AI，通过生成更多的想法，并选择最佳想法。在国际象棋中的实验结果显示，多样化AI系统以不同方式下国际象棋。

    

    近年来，人工智能系统在各种计算任务上已经超过了人类的智能。然而，与人类一样，AI系统也会犯错误，有盲点，产生幻觉，并且在面对新情况时很难进行泛化。本研究探讨了当AI系统的计算合理性推到极限时，是否可以从创造性的决策机制中受益。特别是，我们研究了是否通过作为一个团队的多样化AI系统在具有挑战性的任务中可以胜过单个AI，通过生成更多的想法，然后选择最好的想法。我们以国际象棋这个被称为AI果蝇的游戏为例进行了研究。我们在AlphaZero (AZ)的基础上，通过潜变条件架构扩展它，构建了一个代理团队，我们称之为AZ_db。我们使用行为多样性技术对AZ_db进行训练，以生成更广泛的想法，并通过次加性计划选择最有希望的想法。我们的实验表明，AZ_db以不同方式下国际象棋。

    In recent years, Artificial Intelligence (AI) systems have surpassed human intelligence in a variety of computational tasks. However, AI systems, like humans, make mistakes, have blind spots, hallucinate, and struggle to generalize to new situations. This work explores whether AI can benefit from creative decision-making mechanisms when pushed to the limits of its computational rationality. In particular, we investigate whether a team of diverse AI systems can outperform a single AI in challenging tasks by generating more ideas as a group and then selecting the best ones. We study this question in the game of chess, the so-called drosophila of AI. We build on AlphaZero (AZ) and extend it to represent a league of agents via a latent-conditioned architecture, which we call AZ_db. We train AZ_db to generate a wider range of ideas using behavioral diversity techniques and select the most promising ones with sub-additive planning. Our experiments suggest that AZ_db plays chess in diverse wa
    
[^83]: 自动纠正大型语言模型：多样化自我纠正策略的概述调查

    Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies. (arXiv:2308.03188v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03188](http://arxiv.org/abs/2308.03188)

    本文总结了最近的研究，对多样化的自我纠正策略进行了分类和分析，以解决大型语言模型中的问题行为。自动化反馈技术被证明是一种可行的方法，可以使基于大型语言模型的解决方案更实用和可部署。

    

    大型语言模型(LLMs)在各种自然语言处理任务中展现了卓越的性能。然而，它们的功效受到了不受欢迎和不一致的行为的削弱，包括幻觉、不忠实的推理和有害内容。纠正这些缺陷的一种有前景的方法是自我纠正，即引导或指导LLM自行修复输出问题。利用自动反馈的技术--无论是由LLM自身产生还是由某个外部系统产生--尤其有趣，因为它们是使基于LLM的解决方案更实际和可部署的一种有前景的方式，且只需最少的人类反馈。本文对这一新兴技术类别进行了全面的评估。我们分析和分类了许多最近利用这些策略的工作，包括训练时、生成时和事后纠正的技术。我们还总结了这一策略的主要应用，并在最后讨论了未来的发展方向和挑战。

    Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.
    
[^84]: 使用视觉和文本数据的联合表示进行食物分类

    Food Classification using Joint Representation of Visual and Textual Data. (arXiv:2308.02562v1 [cs.CV])

    [http://arxiv.org/abs/2308.02562](http://arxiv.org/abs/2308.02562)

    本研究提出了一种使用联合表示的多模态分类框架，通过修改版的EfficientNet和Mish激活函数实现图像分类，使用基于BERT的网络实现文本分类。实验结果表明，所提出的网络在图像和文本分类上表现优于其他方法，准确率提高了11.57%和6.34%。比较分析还证明了所提出方法的效率和鲁棒性。

    

    食物分类是健康保健中的重要任务。在这项工作中，我们提出了一个多模态分类框架，该框架使用了修改版的EfficientNet和Mish激活函数用于图像分类，同时使用传统的基于BERT的网络进行文本分类。我们在一个大型开源数据集UPMC Food-101上评估了所提出的网络和其他最先进的方法。实验结果显示，所提出的网络在图像和文本分类上的准确率分别比第二最好的方法提高了11.57%和6.34%。我们还比较了使用机器学习和深度学习模型进行文本分类的准确率、精确率和召回率。通过对图像和文本的预测结果进行比较分析，证明了所提出方法的效率和鲁棒性。

    Food classification is an important task in health care. In this work, we propose a multimodal classification framework that uses the modified version of EfficientNet with the Mish activation function for image classification, and the traditional BERT transformer-based network is used for text classification. The proposed network and the other state-of-the-art methods are evaluated on a large open-source dataset, UPMC Food-101. The experimental results show that the proposed network outperforms the other methods, a significant difference of 11.57% and 6.34% in accuracy is observed for image and text classification, respectively, when compared with the second-best performing method. We also compared the performance in terms of accuracy, precision, and recall for text classification using both machine learning and deep learning-based models. The comparative analysis from the prediction results of both images and text demonstrated the efficiency and robustness of the proposed approach.
    
[^85]: CartiMorph:一种自动化膝关节软骨形态学测量的框架

    CartiMorph: a framework for automated knee articular cartilage morphometrics. (arXiv:2308.01981v1 [eess.IV])

    [http://arxiv.org/abs/2308.01981](http://arxiv.org/abs/2308.01981)

    CartiMorph是一种自动化膝关节软骨形态学测量的框架，利用深度学习模型进行图像分析，通过定量指标评估了软骨的损失和厚度，并与手动分割的结果进行了比较，结果显示表面法线的厚度映射方法具有较小的误差。

    

    我们介绍了CartiMorph，一种用于自动化膝关节软骨形态学测量的框架。它以图像作为输入，并生成软骨亚区域的定量指标，包括全厚度软骨丢失（FCL）的百分比、平均厚度、表面积和体积。CartiMorph利用深度学习模型进行分层图像特征表示。我们训练和验证了深度学习模型，用于组织分割、模板构建和模板到图像的注册。我们建立了基于表面法线的软骨厚度映射、FCL估计和基于规则的软骨分割方法。我们的软骨厚度图在薄和周边区域显示出较小的误差。我们通过比较通过模型分割和手动分割获得的定量指标，评估了所采用的分割模型的有效性。FCL测量的均方根偏差小于8%，并且与手动分割的指标存在强相关性。

    We introduce CartiMorph, a framework for automated knee articular cartilage morphometrics. It takes an image as input and generates quantitative metrics for cartilage subregions, including the percentage of full-thickness cartilage loss (FCL), mean thickness, surface area, and volume. CartiMorph leverages the power of deep learning models for hierarchical image feature representation. Deep learning models were trained and validated for tissue segmentation, template construction, and template-to-image registration. We established methods for surface-normal-based cartilage thickness mapping, FCL estimation, and rule-based cartilage parcellation. Our cartilage thickness map showed less error in thin and peripheral regions. We evaluated the effectiveness of the adopted segmentation model by comparing the quantitative metrics obtained from model segmentation and those from manual segmentation. The root-mean-squared deviation of the FCL measurements was less than 8%, and strong correlations 
    
[^86]: 隐式神经表示用于变化检测

    Implicit neural representation for change detection. (arXiv:2307.15428v1 [cs.CV])

    [http://arxiv.org/abs/2307.15428](http://arxiv.org/abs/2307.15428)

    这项研究提出了一种用于检测不同时间获取的LiDAR点云中变化的无监督方法，通过使用神经场进行连续形状重建，以及高斯混合模型进行变化分类。该方法能够处理不匹配的空间支持和噪声，并在检测能力上取得了显著的提升。

    

    在相同地理区域的两个不同时间获取的一对3D航空LiDAR点云中检测变化是一项具有挑战性的任务，因为空间支持不匹配和获取系统噪声问题。最近的一些尝试基于监督方法来检测点云上的变化，这需要在实际应用中无法获得的大量标记数据。为了解决这些问题，我们提出了一种无监督方法，包括两个组件：连续形状重建的神经场（NF）和用于分类变化的高斯混合模型。NF提供了一种不依赖于网格的表示方法，可以对不匹配的空间支持进行编码，并可以通过正则化来增加高频细节和减少噪声。在任意空间尺度上比较每个时间戳的重建结果，从而显著提高了检测能力。我们将该方法应用于一组模拟LiDAR点云的基准数据集中。

    Detecting changes that occurred in a pair of 3D airborne LiDAR point clouds, acquired at two different times over the same geographical area, is a challenging task because of unmatching spatial supports and acquisition system noise. Most recent attempts to detect changes on point clouds are based on supervised methods, which require large labelled data unavailable in real-world applications. To address these issues, we propose an unsupervised approach that comprises two components: Neural Field (NF) for continuous shape reconstruction and a Gaussian Mixture Model for categorising changes. NF offer a grid-agnostic representation to encode bi-temporal point clouds with unmatched spatial support that can be regularised to increase high-frequency details and reduce noise. The reconstructions at each timestamp are compared at arbitrary spatial scales, leading to a significant increase in detection capabilities. We apply our method to a benchmark dataset of simulated LiDAR point clouds for u
    
[^87]: Google Bard的视觉理解能力如何？开放挑战的实证研究。

    How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges. (arXiv:2307.15016v1 [cs.CV])

    [http://arxiv.org/abs/2307.15016](http://arxiv.org/abs/2307.15016)

    本研究探索了Google Bard在理解和解释文本问题条件下的视觉数据方面的能力，并发现Bard在各种视觉场景中仍然存在困境，这凸显出在视觉理解方面存在重要的差距。

    

    Google的Bard在对话型人工智能领域与OpenAI的ChatGPT成为了强大的竞争对手。值得注意的是，Bard最近已经更新，可以在对话过程中处理文本提示和视觉输入。鉴于Bard在处理文本输入方面的出色表现，我们探索了其在理解和解释由文本问题条件下的视觉数据（图像）方面的能力。这种探索有潜力揭示Bard和其他即将发布的多模式生成模型在解决需要准确的视觉和语言理解的复杂计算机视觉问题时的新见解和挑战。具体而言，在这项研究中，我们专注于15个不同的任务场景，包括常规、伪装、医学、水下和遥感数据，全面评估了Bard的性能。我们的主要发现表明，Bard在这些视觉场景中仍然存在困境，突显了在基于视觉的理解方面存在的重要差距。

    Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in the field of conversational AI. Notably, Bard has recently been updated to handle visual inputs alongside text prompts during conversations. Given Bard's impressive track record in handling textual inputs, we explore its capabilities in understanding and interpreting visual data (images) conditioned by text questions. This exploration holds the potential to unveil new insights and challenges for Bard and other forthcoming multi-modal Generative models, especially in addressing complex computer vision problems that demand accurate visual and language understanding. Specifically, in this study, we focus on 15 diverse task scenarios encompassing regular, camouflaged, medical, under-water and remote sensing data to comprehensively evaluate Bard's performance. Our primary finding indicates that Bard still struggles in these vision scenarios, highlighting the significant gap in vision-based understanding that needs t
    
[^88]: 神经网络在图像分类中学到了什么？基于频率的快捷路径视角

    What do neural networks learn in image classification? A frequency shortcut perspective. (arXiv:2307.09829v1 [cs.LG])

    [http://arxiv.org/abs/2307.09829](http://arxiv.org/abs/2307.09829)

    本研究通过在合成数据集和自然图像上的实验，发现神经网络在图像分类中倾向于找到简单的解决方案，并且在训练过程中首先学到的内容取决于最具有区分性的频率特征，这可以是低频或高频。同时，研究也提出了一种度量标准和方法来识别频率快捷路径，并验证了其在不同类型的图像上的有效性。

    

    频率分析对于理解神经网络（NNs）中的表示学习机制非常有用。大部分研究都集中在回归任务中NNs的学习动态上，而很少有关于分类任务的研究。本研究通过实证研究扩展了频率快捷路径的理解。首先，我们在合成数据集上进行实验，设计了在不同频段上具有偏差的数据集。实验结果表明，NNs倾向于找到分类的简单解决方案，而其在训练过程中首先学到的内容取决于最具有区分性的频率特征，可以是低频或高频。其次，我们通过自然图像验证了这一现象。我们提出了衡量类别频率特征的度量标准，并提出了一种识别频率快捷路径的方法。结果表明，频率快捷路径可以基于纹理或形状，具体取决于何种方式能够最好地简化目标。第三，我们验证了转移性。

    Frequency analysis is useful for understanding the mechanisms of representation learning in neural networks (NNs). Most research in this area focuses on the learning dynamics of NNs for regression tasks, while little for classification. This study empirically investigates the latter and expands the understanding of frequency shortcuts. First, we perform experiments on synthetic datasets, designed to have a bias in different frequency bands. Our results demonstrate that NNs tend to find simple solutions for classification, and what they learn first during training depends on the most distinctive frequency characteristics, which can be either low- or high-frequencies. Second, we confirm this phenomenon on natural images. We propose a metric to measure class-wise frequency characteristics and a method to identify frequency shortcuts. The results show that frequency shortcuts can be texture-based or shape-based, depending on what best simplifies the objective. Third, we validate the transf
    
[^89]: 使用大型语言模型增强密集检索的软提示调优

    Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)

    [http://arxiv.org/abs/2307.08303](http://arxiv.org/abs/2307.08303)

    本论文提出了一种使用软提示调优来增强密集检索的方法（SPTAR）。通过优化任务特定的软提示并利用大型语言模型为未标记的文档生成弱查询，可以提高零样本和少样本的密集检索模型的性能。

    

    密集检索（DR）将查询和文档转化为密集向量表示，并在向量空间中测量查询与文档之间的相似性。DR的一个挑战是缺乏领域特定的训练数据。虽然DR模型可以通过迁移学习从大规模公共数据集（如MS MARCO）中学习，但证据表明，并非所有DR模型和领域都能同等受益于迁移学习。最近，一些研究人员转向使用大型语言模型（LLMs）来改进零样本和少样本的DR模型。然而，这些方法中采用的硬提示或人工编写的提示无法保证生成的弱查询的质量。为了解决这个问题，我们提出了用于增强DR的软提示调优（SPTAR）：对于每个任务，我们利用软提示调优在有限的真实数据上优化任务特定的软提示，然后用这些提示引导LLMs为未标记的文档标记弱查询，从而得到足够的弱文档-查询对来训练任务特定的模型。

    Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
    
[^90]: 由生成闭环人工智能引领的基础科学的未来

    The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence. (arXiv:2307.07522v1 [cs.AI])

    [http://arxiv.org/abs/2307.07522](http://arxiv.org/abs/2307.07522)

    生成型人工智能和大型语言模型可能为基础科学的发现提供机会，通过其自主生成假设和探索假设空间的闭环方法，加速科学发现的进程。

    

    机器学习和人工智能的最新进展，包括生成型人工智能和大型语言模型，正在颠覆技术创新、产品开发和整个社会。人工智能对技术的贡献可以通过多种途径实现，需要大量训练数据集和明确的性能评估标准，范围从模式识别和分类到生成模型。然而，由于科学实践和模型发现需要访问高质量的大型数据集，人工智能对基础科学的贡献较少。生成型人工智能，特别是大型语言模型，可能代表了通过定量模型增强和加速基础深度科学的科学发现的机会。在这里，我们探索和研究了一种由人工智能驱动、自动化的闭环科学发现方法的各个方面，包括自主生成假设和开放式自主探索假设空间。

    Recent advances in machine learning and AI, including Generative AI and LLMs, are disrupting technological innovation, product development, and society as a whole. AI's contribution to technology can come from multiple approaches that require access to large training data sets and clear performance evaluation criteria, ranging from pattern recognition and classification to generative models. Yet, AI has contributed less to fundamental science in part because large data sets of high-quality data for scientific practice and model discovery are more difficult to access. Generative AI, in general, and Large Language Models in particular, may represent an opportunity to augment and accelerate the scientific discovery of fundamental deep science with quantitative models. Here we explore and investigate aspects of an AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space. Int
    
[^91]: 使用机器学习对二战时期密码进行分类

    Classifying World War II Era Ciphers with Machine Learning. (arXiv:2307.00501v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00501](http://arxiv.org/abs/2307.00501)

    本研究使用机器学习和深度学习对二战时期的密码进行分类。结果表明，在最现实的情境下，无论使用何种密码，我们的模型都能达到较高的准确性。

    

    我们确定了当只有密文可用时，机器学习和深度学习技术在分类选择的二战时期密码方面的准确性。具体考虑的密码包括恩尼格玛（Enigma）、M-209、Sigaba、Purple和Typex。我们尝试了三种经典的机器学习模型，分别是支持向量机（SVM）、k最近邻（k-NN）和随机森林（RF）。我们还尝试了四种基于深度学习神经网络的模型：多层感知机（MLP）、长短时记忆（LSTM）、极限学习机（ELM）和卷积神经网络（CNN）。每个模型都基于直方图、二元组和原始密文字母序列进行训练。此外，我们考虑了四种不同的情景：固定明文和固定密钥、随机明文和固定密钥、固定明文和随机密钥以及随机明文和随机密钥下的分类问题。在最现实的情景下，给定每个密码1000个字符。

    We determine the accuracy with which machine learning and deep learning techniques can classify selected World War II era ciphers when only ciphertext is available. The specific ciphers considered are Enigma, M-209, Sigaba, Purple, and Typex. We experiment with three classic machine learning models, namely, Support Vector Machines (SVM), $k$-Nearest Neighbors ($k$-NN), and Random Forest (RF). We also experiment with four deep learning neural network-based models: Multi-Layer Perceptrons (MLP), Long Short-Term Memory (LSTM), Extreme Learning Machines (ELM), and Convolutional Neural Networks (CNN). Each model is trained on features consisting of histograms, digrams, and raw ciphertext letter sequences. Furthermore, the classification problem is considered under four distinct scenarios: Fixed plaintext with fixed keys, random plaintext with fixed keys, fixed plaintext with random keys, and random plaintext with random keys. Under the most realistic scenario, given 1000 characters per ciph
    
[^92]: 基于Lagrangian方法的约束马尔可夫决策过程中无需取消惩罚的遗憾界限

    Cancellation-Free Regret Bounds for Lagrangian Approaches in Constrained Markov Decision Processes. (arXiv:2306.07001v1 [cs.LG])

    [http://arxiv.org/abs/2306.07001](http://arxiv.org/abs/2306.07001)

    本文提出了一种创新的模型驱动的双重算法OptAug-CMDP，用于约束马尔可夫决策过程（CMDPs），解决了原先算法中安全性问题的缺陷，证明了其遗憾值优秀。

    

    约束马尔可夫决策过程（CMDPs）是建模安全强化学习问题的常见方法，其中安全目标由约束函数建模。基于Lagrangian的双重或原始双重算法为CMDPs中的学习提供了高效的方法。但是，当前已知的有限时间段遗憾界限允许“取消错误”，这意味着可以通过另一种场景中的严格约束满足来补偿一个场景中的约束违规行为。本文提出了一种创新的模型驱动的双重算法OptAug-CMDP，该算法受增广拉格朗日方法启发，并可以有效地执行来弥补这种缺陷。我们证明，在$K$个探索CMDP的情况下，我们的算法可以获得$\tilde{O}(\sqrt{K})$的遗憾界限，适用于目标和约束条件。

    Constrained Markov Decision Processes (CMDPs) are one of the common ways to model safe reinforcement learning problems, where the safety objectives are modeled by constraint functions. Lagrangian-based dual or primal-dual algorithms provide efficient methods for learning in CMDPs. For these algorithms, the currently known regret bounds in the finite-horizon setting allow for a \textit{cancellation of errors}; that is, one can compensate for a constraint violation in one episode with a strict constraint satisfaction in another episode. However, in practical applications, we do not consider such a behavior safe.  In this paper, we overcome this weakness by proposing a novel model-based dual algorithm \textsc{OptAug-CMDP} for tabular finite-horizon CMDPs. Our algorithm is motivated by the augmented Lagrangian method and can be performed efficiently. We show that during $K$ episodes of exploring the CMDP, our algorithm obtains a regret of $\tilde{O}(\sqrt{K})$ for both the objective and th
    
[^93]: 一种用于评估图像识别模型鲁棒性的差分测试框架

    A Differential Testing Framework to Evaluate Image Recognition Model Robustness. (arXiv:2306.06208v1 [cs.CV])

    [http://arxiv.org/abs/2306.06208](http://arxiv.org/abs/2306.06208)

    本文提出了一种差分测试框架，用于评估图像识别模型鲁棒性。该框架通过使用一组参考图像并扰动计算环境，可确定模型性能是否受到计算环境变化的影响。

    

    图像识别任务通常使用深度学习，并需要巨大的处理能力，因此依赖于GPU和TPU等硬件加速器进行快速、及时的处理。在模型部署过程中，硬件加速器上的子优映射可能会导致实时图像识别任务失败，从而导致时间不确定性和错误行为。硬件加速器上的映射是通过多个软件组件进行的，例如深度学习框架、编译器、设备库等，我们称之为计算环境。随着图像识别任务在自动驾驶和医疗成像等安全关键应用中的增加，评估它们对计算环境变化的鲁棒性至关重要，因为深度学习框架、编译器优化和硬件设备等参数对模型性能和正确性的影响还不太清楚。在本文中，我们提出了一种差分测试框架，用于评估图像识别模型对计算环境变化的鲁棒性。我们的框架使用一组参考图像，并通过更改软件组件来扰动计算环境，生成具有已知预测输出差异的图像。通过比较原始图像和扰动图像的预测输出，我们可以确定模型性能是否受到计算环境变化的影响。我们通过测试三个图像识别模型的鲁棒性来证明我们框架的有效性，并确定其在计算环境变化下的性能受到影响的情况。

    Image recognition tasks typically use deep learning and require enormous processing power, thus relying on hardware accelerators like GPUs and TPUs for fast, timely processing. Failure in real-time image recognition tasks can occur due to sub-optimal mapping on hardware accelerators during model deployment, which may lead to timing uncertainty and erroneous behavior. Mapping on hardware accelerators is done through multiple software components like deep learning frameworks, compilers, device libraries, that we refer to as the computational environment. Owing to the increased use of image recognition tasks in safety-critical applications like autonomous driving and medical imaging, it is imperative to assess their robustness to changes in the computational environment, as the impact of parameters like deep learning frameworks, compiler optimizations, and hardware devices on model performance and correctness is not well understood.  In this paper we present a differential testing framewo
    
[^94]: 图像识别模型框架转换的故障定位

    Fault Localization for Framework Conversions of Image Recognition Models. (arXiv:2306.06157v1 [cs.CV])

    [http://arxiv.org/abs/2306.06157](http://arxiv.org/abs/2306.06157)

    本文提出针对深度学习框架转换中出现的模型崩溃和输出标签差异的故障定位和修复方法，成功修复多个图像识别模型跨多个深度学习框架的转换错误。

    

    在部署深度神经网络（DNNs）时，开发人员经常将模型从一个深度学习框架转换为另一个（例如，从TensorFlow到PyTorch）。然而，这个过程容易出错，并可能影响目标模型的准确性。为了确定这种影响的程度，我们对三个用于图像识别的DNNs（MobileNetV2、ResNet101和InceptionV3）进行了不同的分析，这些模型在四个深度学习框架（PyTorch、Keras、TensorFlow（TF）和TFLite）之间进行了转换，并发现了许多模型崩溃和输出标签差异高达100％。为了缓解这种错误，我们提出了一种新的方法来定位故障和修复有缺陷的深度学习框架转换，重点放在预训练的图像识别模型上。我们的技术包括四个主要分析阶段：1）转换工具，2）模型参数，3）模型超参数，4）图表示。此外，我们提出了许多针对故障定位和修复的策略，包括转换工具的推荐、调试技巧以及模型超参数的微调。我们通过成功修复所有测试的深度学习框架中MobileNetV2，ResNet101和InceptionV3 的有缺陷的转换来展示我们方法的有效性。

    When deploying Deep Neural Networks (DNNs), developers often convert models from one deep learning framework to another (e.g., TensorFlow to PyTorch). However, this process is error-prone and can impact target model accuracy. To identify the extent of such impact, we perform and briefly present a differential analysis against three DNNs used for image recognition (MobileNetV2, ResNet101, and InceptionV3), converted across four well-known deep learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which revealed numerous model crashes and output label discrepancies of up to 100%. To mitigate such errors, we present a novel approach towards fault localization and repair of buggy deep learning framework conversions, focusing on pre-trained image recognition models. Our technique consists of four primary stages of analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters, and 4) graph representation. In addition, we propose a number of strategies towards faul
    
[^95]: HypLL: 希亚空间深度学习库

    HypLL: The Hyperbolic Learning Library. (arXiv:2306.06154v1 [cs.LG])

    [http://arxiv.org/abs/2306.06154](http://arxiv.org/abs/2306.06154)

    HypLL是一个使用希亚空间的深度学习库，基于PyTorch，旨在使其易于使用，搭建希亚网络模块，特别适用于处理层次化数据和使用少量嵌入维度，是一种新的、开放的研究方向。

    

    在机器学习、多媒体和计算机视觉等领域，希亚空间深度学习正迅速引起关注。深度网络通常在欧几里得空间中运行，隐含地假设数据在规则网格上。最近的研究表明，当处理层次化数据和使用少量嵌入维度时，希亚几何提供了一个可行的深度学习基础。然而，目前没有可访问的开源库用于构建类似于众所周知的深度学习库的希亚网络模块。我们提出了HypLL, 即希亚空间深度学习库，以将希亚深度学习的进展聚集在一起。HypLL建立在PyTorch之上，特别强调其易用性设计，以吸引广泛的受众关注这个新的和开放的研究方向。代码可在以下网址找到：https://github.com/maxvanspengler/hyperbolic_learning_library。压缩文件可在以下网址找到：https://d

    Deep learning in hyperbolic space is quickly gaining traction in the fields of machine learning, multimedia, and computer vision. Deep networks commonly operate in Euclidean space, implicitly assuming that data lies on regular grids. Recent advances have shown that hyperbolic geometry provides a viable alternative foundation for deep learning, especially when data is hierarchical in nature and when working with few embedding dimensions. Currently however, no accessible open-source library exists to build hyperbolic network modules akin to well-known deep learning libraries. We present HypLL, the Hyperbolic Learning Library to bring the progress on hyperbolic deep learning together. HypLL is built on top of PyTorch, with an emphasis in its design for easy-of-use, in order to attract a broad audience towards this new and open-ended research direction. The code is available at: https://github.com/maxvanspengler/hyperbolic_learning_library. The compressed archive is available at: https://d
    
[^96]: 预训练Transformer用于对抗性样本提纯

    Pre-trained transformer for adversarial purification. (arXiv:2306.01762v1 [cs.CR])

    [http://arxiv.org/abs/2306.01762](http://arxiv.org/abs/2306.01762)

    本文提出了一个快速防御对抗性攻击的方案RaPiD（Rapid Plug-in Defender），通过预训练的Transformer微调来提纯对抗样本，使其逼近清洁数据分布，实验结果表明，在有限数据情况下，该方法优于最先进的方法。

    

    随着越来越多的深度神经网络被部署为各种日常服务，它们的可靠性至关重要。深度神经网络容易受到对抗性攻击的影响，其中逃避攻击是最普遍的一种。最近的研究通常通过对抗训练或利用大量清洁数据的知识来增强其健壮性。然而，在实际应用中，重新训练和部署模型需要大量的计算资源，对在线服务造成重大损失。此外，当检测到某种攻击的对抗性例子时，服务提供者只能获得有限的对抗性样本，而大量的清洁数据可能无法获取。针对这些问题，我们提出了一种新的方案，名为RaPiD（Rapid Plug-in Defender），旨在快速防御具有少量干净和对抗性示例限制的原始服务模型的某种攻击。受到预训练模型提供转移学习良好初始化的通用趋势的启发，我们建议通过微调预先训练的Transformer来提纯对抗性样本。预训练的Transformer作为正则化器，鼓励提纯后的对抗性样本接近清晰数据的分布。实验结果表明，RaPiD在防御各种具有限数据的攻击方面优于最先进的方法。

    With more and more deep neural networks being deployed as various daily services, their reliability is essential. It's frightening that deep neural networks are vulnerable and sensitive to adversarial attacks, the most common one of which for the services is evasion-based. Recent works usually strengthen the robustness by adversarial training or leveraging the knowledge of an amount of clean data. However, in practical terms, retraining and redeploying the model need a large computational budget, leading to heavy losses to the online service. In addition, when adversarial examples of a certain attack are detected, only limited adversarial examples are available for the service provider, while much clean data may not be accessible. Given the mentioned problems, we propose a new scenario, RaPiD (Rapid Plug-in Defender), which is to rapidly defend against a certain attack for the frozen original service model with limitations of few clean and adversarial examples. Motivated by the general
    
[^97]: 几何代数变换器

    Geometric Algebra Transformers. (arXiv:2305.18415v1 [cs.LG])

    [http://arxiv.org/abs/2305.18415](http://arxiv.org/abs/2305.18415)

    本文介绍了一种通用架构几何代数变换器（GATr），用于解决几何数据问题。GATr使用投影几何代数表示输入输出和状态，具有可缩放性、表达性、多功能性。在n体建模和机器人规划的实验中，GATr相对于非几何基线表现出强大的改进。

    

    几何数据问题涉及计算机视觉、机器人、化学和物理领域。这些数据可以采用许多形式，例如点、方向向量、平面或变换，但迄今为止还没有一种单一的架构，可以应用于如此多种几何类型, 同时尊重它们的对称性。在本文中，我们介绍了几何代数变换器（GATr），一种用于几何数据的通用架构。GATr使用投影几何代数来表示输入、输出和隐藏状态，其提供常见几何对象的高效16维向量空间表示以及作用于它们的运算符。GATr是相对于E(3)（3D欧几里得空间的对称群）等变的。作为变换器，GATr可扩展、表达丰富且多功能。在n体建模和机器人规划的实验中，GATr相对于非几何基线均表现出强大的改进。

    Problems involving geometric data arise in a variety of fields, including computer vision, robotics, chemistry, and physics. Such data can take numerous forms, such as points, direction vectors, planes, or transformations, but to date there is no single architecture that can be applied to such a wide variety of geometric types while respecting their symmetries. In this paper we introduce the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data. GATr represents inputs, outputs, and hidden states in the projective geometric algebra, which offers an efficient 16-dimensional vector space representation of common geometric objects as well as operators acting on them. GATr is equivariant with respect to E(3), the symmetry group of 3D Euclidean space. As a transformer, GATr is scalable, expressive, and versatile. In experiments with n-body modeling and robotic planning, GATr shows strong improvements over non-geometric baselines.
    
[^98]: 论进化磨锋、平坦极小和泛化

    On progressive sharpening, flat minima and generalisation. (arXiv:2305.14683v1 [cs.LG])

    [http://arxiv.org/abs/2305.14683](http://arxiv.org/abs/2305.14683)

    本文提出了一种用损失黑塞矩阵和输入-输出雅克比矩阵联系起来的假设，量化了模型的输入-输出雅克比矩阵近似其在数据分布上的利普西茨范数的程度，并推导出了一个基于经验雅克比矩阵的新的泛化界，给出了关于进化磨锋和平坦极小的泛化性质的新解释。

    

    我们提出了一种新的方法来理解深度学习中损失曲率与泛化之间的关系。具体来说，我们利用现有的深度网络损失黑塞矩阵频谱经验分析，提出了一个将损失黑塞矩阵和深度神经网络的输入-输出雅克比矩阵联系起来的假设。然后，我们证明了一系列理论结果，量化了模型的输入-输出雅克比矩阵近似其在数据分布上的利普西茨范数的程度，并推导出了一个基于经验雅克比矩阵的新的泛化界。我们利用我们的假设和理论结果，给出了关于最近观察到的进化磨锋现象以及平坦极小的泛化性质的新描述。实验证据验证了我们的主张。

    We present a new approach to understanding the relationship between loss curvature and generalisation in deep learning. Specifically, we use existing empirical analyses of the spectrum of deep network loss Hessians to ground an ansatz tying together the loss Hessian and the input-output Jacobian of a deep neural network. We then prove a series of theoretical results which quantify the degree to which the input-output Jacobian of a model approximates its Lipschitz norm over a data distribution, and deduce a novel generalisation bound in terms of the empirical Jacobian. We use our ansatz, together with our theoretical results, to give a new account of the recently observed progressive sharpening phenomenon, as well as the generalisation properties of flat minima. Experimental evidence is provided to validate our claims.
    
[^99]: 你听到的正是你看到的：从图像质量评价中获得的音频质量评价方法

    What You Hear Is What You See: Audio Quality Metrics From Image Quality Metrics. (arXiv:2305.11582v1 [cs.SD])

    [http://arxiv.org/abs/2305.11582](http://arxiv.org/abs/2305.11582)

    该研究探讨了利用图像感知度量方法来评估音频信号的可行性，并通过定制一个心理声学合理结构的度量方法来解决声音信号的特殊性，并在音乐数据集上展现出了令人满意的结果。

    

    本研究旨在探究利用最先进的图像感知度量方法，通过将音频信号表示成频谱图以评估音频信号的可行性。所提出的方法基于听觉和视觉通路中神经机制的相似性，取得了鼓舞人心的结果。此外，我们定制了一个具有心理声学合理结构的度量方法，以解决声音信号的特殊性。我们使用音乐数据集评估了我们提出的度量方法和几个基准度量方法的有效性，并取得了令人满意的结果，即度量方法与人类评估者所评估的音频质量之间的相关性。

    In this study, we investigate the feasibility of utilizing state-of-the-art image perceptual metrics for evaluating audio signals by representing them as spectrograms. The encouraging outcome of the proposed approach is based on the similarity between the neural mechanisms in the auditory and visual pathways. Furthermore, we customise one of the metrics which has a psychoacoustically plausible architecture to account for the peculiarities of sound signals. We evaluate the effectiveness of our proposed metric and several baseline metrics using a music dataset, with promising results in terms of the correlation between the metrics and the perceived quality of audio as rated by human evaluators.
    
[^100]: 评估LLM的隐藏风险：关于鲁棒性、一致性和可信性的实证研究

    Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility. (arXiv:2305.10235v1 [cs.LG])

    [http://arxiv.org/abs/2305.10235](http://arxiv.org/abs/2305.10235)

    本研究是一项关于大型语言模型方面的实证研究，对主流语言模型进行了大量查询和分析，结果发现这些模型存在着鲁棒性、一致性和可信性方面的潜在风险。

    

    大型语言模型（LLMs）的普及对于许多领域产生了重大影响，特别是在其开放式环境（如API、开源模型和插件）中。然而，随着LLMs的广泛部署，缺乏全面讨论和分析潜在风险的研究。因此，我们进行了一项初步但开创性的研究，涵盖了LLMs系统的鲁棒性、一致性和可信性。我们提出了一个自动化工作流程来处理大量查询/响应。总体而言，我们对包括ChatGPT、LLaMA和OPT在内的主流LLMs进行了100多万个查询。我们的工作流核心包括数据原语，随后是自动解释器，评估这些LLMs在不同的对抗性度量系统下的表现。结果，我们得出了几个、也许是不幸的结论，这些结论相当不同

    The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite unco
    
[^101]: MPI-rical：基于Transformer的数据驱动MPI分布式并行辅助

    MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers. (arXiv:2305.09438v1 [cs.DC])

    [http://arxiv.org/abs/2305.09438](http://arxiv.org/abs/2305.09438)

    本文提出了一种基于Transformer模型的新方法MPI-rical，通过对大量代码片段进行训练实现自动化MPI代码生成，使并行化成为可能。

    

    在高性能计算中，将串行代码自动并行化以支持共享内存和分布式内存系统是一项具有挑战性的任务。虽然许多尝试将串行代码转换为共享内存环境的并行代码（通常使用OpenMP），但没有任何一项尝试成功将其转化为分布式内存环境。本文提出了一种称为MPI-rical的新方法，通过基于Transformer模型对大约25,000个串行代码片段及其对应的并行MPI代码进行训练，从我们的语料库（MPICodeCorpus）的50,000多个代码片段中生成自动化MPI代码。为了评估模型的性能，我们首先将串行代码转换为基于MPI的并行代码翻译问题分解为两个子问题，并制定两个研究目标：代码补全，即在给定源代码中的某个位置，预测该位置的MPI函数；代码翻译，即预测一个MPI函数。

    Automatic source-to-source parallelization of serial code for shared and distributed memory systems is a challenging task in high-performance computing. While many attempts were made to translate serial code into parallel code for a shared memory environment (usually using OpenMP), none has managed to do so for a distributed memory environment. In this paper, we propose a novel approach, called MPI-rical, for automated MPI code generation using a transformer-based model trained on approximately 25,000 serial code snippets and their corresponding parallelized MPI code out of more than 50,000 code snippets in our corpus (MPICodeCorpus). To evaluate the performance of the model, we first break down the serial code to MPI-based parallel code translation problem into two sub-problems and develop two research objectives: code completion defined as given a location in the source code, predict the MPI function for that location, and code translation defined as predicting an MPI function as wel
    
[^102]: 笑声很重要：引入使用扩散模型生成笑脸的方法

    Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models. (arXiv:2305.08854v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.08854](http://arxiv.org/abs/2305.08854)

    该论文提出了一种新颖的模型，利用扩散模型生成逼真的笑脸序列，以填补非语言交流领域的研究空白。与传统方法相比，该模型在所有指标上都取得了最先进的性能。

    

    近年来，基于语音的动画在提高真实感方面取得了显著进展，但在非语言交流方面仍然很少有研究。本文提出了一种新颖的模型，能够根据静态肖像和包含笑声的音频剪辑生成逼真的笑脸序列。我们强调了传统面部动画方法的失败案例，并利用最近扩散模型的进展产生令人信服的笑脸视频。我们使用各种各样的笑声数据集对模型进行训练，并引入了一种专门设计用于笑声的评估指标。与之前的基于语音的方法相比，我们的模型在所有指标上均取得了最先进的性能，甚至在...

    Speech-driven animation has gained significant traction in recent years, with current methods achieving near-photorealistic results. However, the field remains underexplored regarding non-verbal communication despite evidence demonstrating its importance in human interaction. In particular, generating laughter sequences presents a unique challenge due to the intricacy and nuances of this behaviour. This paper aims to bridge this gap by proposing a novel model capable of generating realistic laughter sequences, given a still portrait and an audio clip containing laughter. We highlight the failure cases of traditional facial animation methods and leverage recent advances in diffusion models to produce convincing laughter videos. We train our model on a diverse set of laughter datasets and introduce an evaluation metric specifically designed for laughter. When compared with previous speech-driven approaches, our model achieves state-of-the-art performance across all metrics, even when the
    
[^103]: GAMIVAL：移动云游戏内容的视频质量预测

    GAMIVAL: Video Quality Prediction on Mobile Cloud Gaming Content. (arXiv:2305.02422v1 [eess.IV])

    [http://arxiv.org/abs/2305.02422](http://arxiv.org/abs/2305.02422)

    GAMIVAL是一种新型的游戏专用无参考视频质量评估模型，结合了多种优点。在移动云游戏内容的主观质量评估数据库上进行测试，表现出更好的NR VQA性能。

    

    近年来，移动云游戏行业迅速增长。当游戏视频从云服务器传输到客户端设备时，需要一种可以监测失真视频质量而无需参考视频的算法。然而，创建可以准确预测由计算机图形引擎渲染的流式游戏视频质量的无参考视频质量评估（NR VQA）模型是一项具有挑战性的问题，因为游戏内容通常在统计上与自然视频不同，缺乏细节，并包含许多平滑区域。我们创建了一种名为Gaming Video Quality Evaluator（GAMIVAL）的新型游戏专用NR VQA模型，结合和利用空间和时间游戏失真场景统计模型、神经噪声模型和客观质量模型的优点。GAMIVAL已经在一个大型的移动云游戏内容主观质量评估数据库上进行了训练和测试，并在游戏内容的NR VQA模型方面超越了最先进的模型。

    The mobile cloud gaming industry has been rapidly growing over the last decade. When streaming gaming videos are transmitted to customers' client devices from cloud servers, algorithms that can monitor distorted video quality without having any reference video available are desirable tools. However, creating No-Reference Video Quality Assessment (NR VQA) models that can accurately predict the quality of streaming gaming videos rendered by computer graphics engines is a challenging problem, since gaming content generally differs statistically from naturalistic videos, often lacks detail, and contains many smooth regions. Until recently, the problem has been further complicated by the lack of adequate subjective quality databases of mobile gaming content. We have created a new gaming-specific NR VQA model called the Gaming Video Quality Evaluator (GAMIVAL), which combines and leverages the advantages of spatial and temporal gaming distorted scene statistics models, a neural noise model, 
    
[^104]: RAFT: 奖励排名微调用于生成型基础模型对齐

    RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])

    [http://arxiv.org/abs/2304.06767](http://arxiv.org/abs/2304.06767)

    RAFT框架引入了奖励排名微调方法，用于对齐生成型基础模型，以解决强化学习带来的低效和不稳定性问题。

    

    生成型基础模型容易受到广泛的无监督训练数据带来的隐式偏见的影响。这些偏见可能导致子优样本、扭曲的结果和不公平，可能产生重大影响。因此，将这些模型与人的伦理和偏好对齐是确保它们在真实应用中负责任和有效的部署的关键步骤。以往的研究主要采用人类反馈的强化学习（ RLHF）作为解决这个问题的手段。在 RL 算法的指导下，用人类反馈指导的奖励模型对生成模型进行微调。然而， RL 算法的低效性和不稳定性常常会对生成模型的成功对齐产生重大障碍，因此需要开发一种更为强大和简化的方法。为此，我们引入了一个新的框架，即奖励排名微调（ RAFT ），旨在对齐生成基础模型。

    Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
    
[^105]: NeBLa: 使用神经啤酒-兰伯特法从全景放射线图中重建口腔结构的三维模型

    NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs. (arXiv:2304.04027v1 [eess.IV])

    [http://arxiv.org/abs/2304.04027](http://arxiv.org/abs/2304.04027)

    该论文提出了一个新的框架：NeBLa，可以从全景放射线图中通过神经啤酒-兰伯特法重建精确的3D口腔结构模型。

    

    全景X线片（全景放射线图，PX）是常用于牙科检查的成像模式。然而，与3D锥形束计算机断层扫描（CBCT）相比，PX的适用性有限，因为PX只提供口腔结构的二维扁平图像。在本文中，我们提出了一个新的框架，用于从真实的PX图像估计3D口腔结构。由于PX和CBCT数据的匹配不多，我们在训练时使用了从CBCT模拟的PX，但在推理时使用了真实的全景放射线片。我们提出了一种新的光线采样方法，受到全景放射线成像原理的启发，利用啤酒-兰伯特定律导出渲染函数生成模拟全景放射线图。我们的模型由三个部分组成：转换模块，生成模块和精炼模块。转换模块将真实的全景放射线图转换为模拟的训练图像风格。生成模块利用射线采样方法得到的模拟全景放射线图约束下的输入图像生成3D结构。精炼模块改善了3D结构的平滑性和一致性。实验结果表明，我们提出的方法可以从全景放射线片提供的有限信息中生成精确的3D牙科模型。

    Panoramic radiography (panoramic X-ray, PX) is a widely used imaging modality for dental examination. However, its applicability is limited as compared to 3D Cone-beam computed tomography (CBCT), because PX only provides 2D flattened images of the oral structure. In this paper, we propose a new framework which estimates 3D oral structure from real-world PX images. Since there are not many matching PX and CBCT data, we used simulated PX from CBCT for training, however, we used real-world panoramic radiographs at the inference time. We propose a new ray-sampling method to make simulated panoramic radiographs inspired by the principle of panoramic radiography along with the rendering function derived from the Beer-Lambert law. Our model consists of three parts: translation module, generation module, and refinement module. The translation module changes the real-world panoramic radiograph to the simulated training image style. The generation module makes the 3D structure from the input ima
    
[^106]: 探索差分隐私中视觉提示的好处

    Exploring the Benefits of Visual Prompting in Differential Privacy. (arXiv:2303.12247v1 [cs.CV])

    [http://arxiv.org/abs/2303.12247](http://arxiv.org/abs/2303.12247)

    本文探讨了在差分隐私（DP）中利用视觉提示（VP）构建神经网络分类器的好处。VP与PATE相配合，在隐私预算最小的情况下实现了最先进的隐私-效用平衡，在跨域图像分类中也显示了其优势。此外，消融研究表明VP在DP中具有很好的有效性和贡献。

    

    视觉提示（VP）是一种新兴且强大的技术，通过设计一个经过良好训练的冻结源模型，可以实现对下游任务的样本高效适应。在本文中，我们探讨了在差分隐私（DP）中利用VP构建引人注目的神经网络分类器的好处。我们探索并将VP整合到经典的DP训练方法中，并展示了其简单性和效率。特别是，我们发现VP与PATE（一种利用教师集合的知识转移的最先进的DP训练方法）相配合，在隐私预算最小的情况下实现了最先进的隐私-效用平衡。此外，我们对跨域图像分类进行了额外的实验，以进一步揭示在DP中VP的优势。最后，我们进行了广泛的消融研究，以验证VP在DP考虑下的有效性和贡献。

    Visual Prompting (VP) is an emerging and powerful technique that allows sample-efficient adaptation to downstream tasks by engineering a well-trained frozen source model. In this work, we explore the benefits of VP in constructing compelling neural network classifiers with differential privacy (DP). We explore and integrate VP into canonical DP training methods and demonstrate its simplicity and efficiency. In particular, we discover that VP in tandem with PATE, a state-of-the-art DP training method that leverages the knowledge transfer from an ensemble of teachers, achieves the state-of-the-art privacy-utility trade-off with minimum expenditure of privacy budget. Moreover, we conduct additional experiments on cross-domain image classification with a sufficient domain gap to further unveil the advantage of VP in DP. Lastly, we also conduct extensive ablation studies to validate the effectiveness and contribution of VP under DP consideration.
    
[^107]: 用调谐透镜从Transformer中获取潜在的预测能力

    Eliciting Latent Predictions from Transformers with the Tuned Lens. (arXiv:2303.08112v1 [cs.LG])

    [http://arxiv.org/abs/2303.08112](http://arxiv.org/abs/2303.08112)

    本文提出了一种改进版的“逻辑透镜”技术——“调谐透镜”，通过训练一个仿射探针，可以将每个隐藏状态解码成词汇分布。这个方法被应用于各种自回归语言模型上，比逻辑透镜更具有预测性、可靠性和无偏性，并且通过因果实验验证使用的特征与模型本身类似。同时，本文发现潜在预测的轨迹可以用于高精度地检测恶意输入。

    

    本文从迭代推理的角度分析了transformers模型，旨在了解模型预测是如何逐层进行精化的。为了实现这一目的，我们为冻结的预训练模型中的每个块训练一个仿射探针，使得可以将每个隐藏状态解码成词汇分布。我们的方法“调谐透镜”，是“逻辑透镜”技术的改进版本，前者给出了有用的见解，但常常易碎。我们将其应用于各种具有多达20B参数的自回归语言模型，表明其比逻辑透镜更具有预测性、可靠性和无偏性。通过因果实验显示，调谐透镜使用的特征与模型本身类似。我们还发现，潜在预测的轨迹可以用于高精度地检测恶意输入。我们的所有代码都可以在https://github.com/AlignmentResearch/tuned-lens 找到。

    We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the \emph{tuned lens}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle.  We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.
    
[^108]: WDiscOOD：通过白化线性判别分析进行区分度优化的OOD检测

    WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis. (arXiv:2303.07543v1 [cs.CV])

    [http://arxiv.org/abs/2303.07543](http://arxiv.org/abs/2303.07543)

    本论文提出了一种名为WDiscOOD的新型OOD检测方法，其中使用白化线性判别分析将特征投影到判别子空间和残留子空间中，确定OOD分数。在大规模ImageNet-1k基准测试和六个OOD数据集中，WDiscOOD表现出了优越的性能。

    

    深度神经网络容易在遇到未知概念的情形下产生过度自信但错误的预测。这个挑战突显了在开放世界中检测OOD样本的重要性。本文提出了一种新颖的特征空间OOD检测分数，同时结合了类别特定和类别不可知的信息。具体地，我们的方法使用白化线性判别分析将特征投影到两个子空间中——判别子空间和残留子空间，其中ID类在判别子空间中被最大化地分离，并在残差子空间中被紧密地聚类。然后，在两个子空间中将来自输入数据与ID分布的偏差组合起来确定OOD分数。我们的方法名为WDiscOOD，在覆盖多种分布偏移的六个OOD数据集上验证了其高效性，包括大规模ImageNet-1k基准测试。WDiscOOD在深度分类器上表现出了优越的性能。

    Deep neural networks are susceptible to generating overconfident yet erroneous predictions when presented with data beyond known concepts. This challenge underscores the importance of detecting out-of-distribution (OOD) samples in the open world. In this work, we propose a novel feature-space OOD detection score that jointly reasons with both class-specific and class-agnostic information. Specifically, our approach utilizes Whitened Linear Discriminative Analysis to project features into two subspaces - the discriminative and residual subspaces - in which the ID classes are maximally separated and closely clustered, respectively. The OOD score is then determined by combining the deviation from the input data to the ID distribution in both subspaces. The efficacy of our method, named WDiscOOD, is verified on the large-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety of distribution shifts. WDiscOOD demonstrates superior performance on deep classifiers with divers
    
[^109]: 进化强化学习综述

    Evolutionary Reinforcement Learning: A Survey. (arXiv:2303.04150v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2303.04150](http://arxiv.org/abs/2303.04150)

    这篇文章系统地总结了最新的进化计算方法在解决强化学习中的关键挑战方面所取得的良好性能。

    

    强化学习是一种通过与环境交互训练智能体最大化累积奖励的机器学习方法。最近将强化学习与深度学习相结合，在棋盘游戏、街机游戏和机器人控制等各种挑战性任务中取得了令人瞩目的成就。尽管取得了成功，但仍存在一些关键挑战，包括由敏感超参数导致的脆弱收敛特性，长时间跨度和稀疏奖励的时间分配困难，特别是在连续搜索空间场景中的多样性探索不足，多智能体强化学习中的信用分配困难以及奖励冲突目标。进化计算维护着一群学习智能体，已展现出解决这些限制的良好性能。本文介绍了集成进化计算的最新方法的全面综述。

    Reinforcement learning (RL) is a machine learning approach that trains agents to maximize cumulative rewards through interactions with environments. The integration of RL with deep learning has recently resulted in impressive achievements in a wide range of challenging tasks, including board games, arcade games, and robot control. Despite these successes, there remain several crucial challenges, including brittle convergence properties caused by sensitive hyperparameters, difficulties in temporal credit assignment with long time horizons and sparse rewards, a lack of diverse exploration, especially in continuous search space scenarios, difficulties in credit assignment in multi-agent reinforcement learning, and conflicting objectives for rewards. Evolutionary computation (EC), which maintains a population of learning agents, has demonstrated promising performance in addressing these limitations. This article presents a comprehensive survey of state-of-the-art methods for integrating EC
    
[^110]: 量子化的低秩多元回归与随机抖动

    Quantized Low-Rank Multivariate Regression with Random Dithering. (arXiv:2302.11197v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.11197](http://arxiv.org/abs/2302.11197)

    本文研究了量子化的低秩多元回归，通过采用均匀量化与随机抖动的方法，提出了约束Lasso和正则化Lasso估计器，实现了最小最优率的估计，同时量化仅对乘法因子略有影响。

    

    低秩多元回归（LRMR）是一种重要的统计学习模型，将高度相关的任务作为具有低秩先验的多响应回归问题进行组合。本文研究了量子化的LRMR，这是一种实际的设置，其中响应和/或协变量被离散化为有限的精度。我们专注于估计基础系数矩阵。为了使能够实现任意小误差的一致估计器成为可能，我们采用了均匀量化与随机抖动，即在量化之前向数据添加适当的随机噪声。具体而言，响应使用均匀抖动，协变量使用三角抖动。基于量化数据，我们提出了约束Lasso和正则化Lasso估计器，并推导了非渐近性误差界。通过抖动的帮助，估计器实现了最小最优率，而量化仅略微恶化了乘法因子。

    Low-rank multivariate regression (LRMR) is an important statistical learning model that combines highly correlated tasks as a multiresponse regression problem with low-rank priori on the coefficient matrix. In this paper, we study quantized LRMR, a practical setting where the responses and/or the covariates are discretized to finite precision. We focus on the estimation of the underlying coefficient matrix. To make consistent estimator that could achieve arbitrarily small error possible, we employ uniform quantization with random dithering, i.e., we add appropriate random noise to the data before quantization. Specifically, uniform dither and triangular dither are used for responses and covariates, respectively. Based on the quantized data, we propose the constrained Lasso and regularized Lasso estimators, and derive the non-asymptotic error bounds. With the aid of dithering, the estimators achieve minimax optimal rate, while quantization only slightly worsens the multiplicative factor
    
[^111]: G-Signatures：全局图传播与随机签名

    G-Signatures: Global Graph Propagation With Randomized Signatures. (arXiv:2302.08811v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08811](http://arxiv.org/abs/2302.08811)

    G-Signatures是一种全局图传播方法，使用随机签名实现。通过嵌入图结构信息为潜在空间路径，能够有效提取处理全局图属性。在多个任务中表现出优势。

    

    图神经网络（GNNs）已经发展成为最流行的深度学习架构之一。然而，GNNs受到过度平滑节点信息的影响，因此难以解决与全局图属性相关的任务。我们提出了一种新颖的图学习方法——G-Signatures，通过随机签名实现全局图传播。G-Signatures使用了一种新的图转换概念，将图结构化信息嵌入到可以被解释为潜在空间路径的路径中。我们进一步引入了潜在空间路径映射的概念。这使得我们能够迭代地遍历潜在空间路径，并全局地处理信息。G-Signatures在提取和处理全局图属性方面表现出色，并且能够有效地应对大规模图问题。在实证上，我们确认了G-Signatures在多个分类和回归任务中的优势。

    Graph neural networks (GNNs) have evolved into one of the most popular deep learning architectures. However, GNNs suffer from over-smoothing node information and, therefore, struggle to solve tasks where global graph properties are relevant. We introduce G-Signatures, a novel graph learning method that enables global graph propagation via randomized signatures. G-Signatures use a new graph conversion concept to embed graph structured information which can be interpreted as paths in latent space. We further introduce the idea of latent space path mapping. This allows us to iteratively traverse latent space paths, and, thus globally process information. G-Signatures excel at extracting and processing global graph properties, and effectively scale to large graph problems. Empirically, we confirm the advantages of G-Signatures at several classification and regression tasks.
    
[^112]: 外星编码

    Alien Coding. (arXiv:2301.11479v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.11479](http://arxiv.org/abs/2301.11479)

    这个论文介绍了一种自学习算法，用于合成OEIS序列的程序。该算法通过训练神经机器翻译器学习序列和已发现程序之间的对应关系，并自己发现了超过78000个OEIS序列的程序，有时还开发出非传统的编程方法。

    

    我们介绍了一种自学习算法，用于合成OEIS序列的程序。该算法最初随机生成程序，然后通过训练神经机器翻译来学习序列和已发现程序之间的对应关系，并通过训练后的神经机器翻译器为每个OEIS序列提出许多新程序。该算法自己发现了超过78000个OEIS序列的程序，有时开发出非传统的编程方法。我们在几个实验中分析了算法的行为和发明的程序。

    We introduce a self-learning algorithm for synthesizing programs for OEIS sequences. The algorithm starts from scratch initially generating programs at random. Then it runs many iterations of a self-learning loop that interleaves (i) training neural machine translation to learn the correspondence between sequences and the programs discovered so far, and (ii) proposing many new programs for each OEIS sequence by the trained neural machine translator. The algorithm discovers on its own programs for more than 78000 OEIS sequences, sometimes developing unusual programming methods. We analyze its behavior and the invented programs in several experiments.
    
[^113]: 用多智能体强化学习模拟社会困境中的道德选择

    Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning. (arXiv:2301.08491v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2301.08491](http://arxiv.org/abs/2301.08491)

    本文使用多智能体强化学习模拟社会困境中的道德选择，设计了一套道德奖励结构，旨在分析和研究AI代理的道德行为。

    

    在实际应用中，人工智能（AI）在智能代理中纳入道德选择的重要性不断展现。同时也强调，按照任何一种道德观定义顶层的AI伦理约束非常具有挑战，并且会带来风险。从底层学习的角度出发，或许更适合研究和开发AI代理的道德行为。我们认为，分析根据预定义的道德奖励在社会困境中实行行动的强化学习代理的新兴行为是一个有趣和富有洞察力的起点。在这项工作中，我们对强化学习代理根据道德理论的奖励进行的选择进行了系统分析。我们旨在设计简化但代表一组关键伦理系统的奖励结构。因此，我们首先定义了区分后果和规范伦理的道德奖励函数，并将它们混合以创建新的奖励方案。然后，我们通过训练在社会困境下进行内在动机驱动的强化学习代理来评估这些奖励函数。结果表明，我们的方法能够复制并扩展有关道德选择的文献研究中的许多发现，并能够出现以前未曾报道的新行为。

    Practical uses of Artificial Intelligence (AI) in the real world have demonstrated the importance of embedding moral choices into intelligent agents. They have also highlighted that defining top-down ethical constraints on AI according to any one type of morality is extremely challenging and can pose risks. A bottom-up learning approach may be more appropriate for studying and developing ethical behavior in AI agents. In particular, we believe that an interesting and insightful starting point is the analysis of emergent behavior of Reinforcement Learning (RL) agents that act according to a predefined set of moral rewards in social dilemmas.  In this work, we present a systematic analysis of the choices made by intrinsically-motivated RL agents whose rewards are based on moral theories. We aim to design reward structures that are simplified yet representative of a set of key ethical systems. Therefore, we first define moral reward functions that distinguish between consequence- and norm
    
[^114]: 基于数据流分析启发的深度学习用于高效漏洞检测

    Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection. (arXiv:2212.08108v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2212.08108](http://arxiv.org/abs/2212.08108)

    本论文提出了一种基于数据流分析启发的深度学习方法，用于高效漏洞检测。通过设计了DeepDFA框架和嵌入技术，我们实现了对代码语义的更高效捕捉，使得深度学习在漏洞检测中更加有效和高性能。DeepDFA训练时间只需9分钟，且超过了所有非transformer基线模型75倍的性能。

    

    基于深度学习的漏洞检测已经展示出了很好的性能，并且在一些研究中超过了静态分析工具。然而，最高性能的方法使用基于token的transformer模型，这不是捕捉漏洞检测所需的代码语义最高效的方法。传统的程序分析技术，如数据流分析，可以根据其根本原因检测出许多类型的错误。在本文中，我们提出将此类基于因果关系的漏洞检测算法与深度学习相结合，以实现更高效和有效的漏洞检测。具体而言，我们设计了DeepDFA，这是一个基于数据流分析启发的图学习框架和一种嵌入技术，可以使图学习模拟数据流计算。我们展示了DeepDFA既具有性能又具有效率。DeepDFA超过了所有非transformer基线模型。它的训练时间只需9分钟，比具有最高性能的基线模型快75倍。

    Deep learning-based vulnerability detection has shown great performance and, in some studies, outperformed static analysis tools. However, the highest-performing approaches use token-based transformer models, which are not the most efficient to capture code semantics required for vulnerability detection. Classical program analysis techniques such as dataflow analysis can detect many types of bugs based on their root causes. In this paper, we propose to combine such causal-based vulnerability detection algorithms with deep learning, aiming to achieve more efficient and effective vulnerability detection. Specifically, we designed DeepDFA, a dataflow analysis-inspired graph learning framework and an embedding technique that enables graph learning to simulate dataflow computation. We show that DeepDFA is both performant and efficient. DeepDFA outperformed all non-transformer baselines. It was trained in 9 minutes, 75x faster than the highest-performing baseline model. When using only 50+ v
    
[^115]: 带有标签差分隐私的回归

    Regression with Label Differential Privacy. (arXiv:2212.06074v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06074](http://arxiv.org/abs/2212.06074)

    本论文研究了在保证标签差分隐私的情况下训练回归模型的任务，并提出了一种最优的标签差分隐私随机化机制，该机制采用了“对箱进行随机响应”的形式，并提供了一种高效算法来找到最优的箱值。

    

    我们研究了在保证标签差分隐私（DP）的情况下训练回归模型的任务。基于标签值的全局先验分布，该分布可以私密地获取，我们得到了一个在给定回归损失函数下最优的标签DP随机化机制。我们证明了最优机制采用“对箱进行随机响应”的形式，并提出了一种寻找最优箱值的高效算法。我们在几个数据集上进行了全面的实验评估，证明了我们算法的有效性。

    We study the task of training regression models with the guarantee of label differential privacy (DP). Based on a global prior distribution on label values, which could be obtained privately, we derive a label DP randomization mechanism that is optimal under a given regression loss function. We prove that the optimal mechanism takes the form of a "randomized response on bins", and propose an efficient algorithm for finding the optimal bin values. We carry out a thorough experimental evaluation on several datasets demonstrating the efficacy of our algorithm.
    
[^116]: 生物启发式学习是否比反向传播更好？生物学习与反向传播的基准测试。

    Is Bio-Inspired Learning Better than Backprop? Benchmarking Bio Learning vs. Backprop. (arXiv:2212.04614v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04614](http://arxiv.org/abs/2212.04614)

    本研究对比了反向传播和多个生物启发式算法，发现当未提供整个训练数据集时，生物算法比反向传播表现要好得多。

    

    自从反向传播（BP）不被认为是符合生物学原理的以来，生物启发式学习近年来变得越来越流行。文献中提出了许多算法，它们都比BP更符合生物学原理。然而，除了克服BP的生物学不合理性，使用生物启发式算法的强烈动机仍然缺乏。在本研究中，我们进行了BP和多个生物启发式算法的全面比较，以回答生物学习是否比BP提供额外的好处的问题。我们使用不同的设计选择来测试生物算法，如仅使用部分训练数据、训练时资源约束、神经网络参数的稀疏化以及向输入样本添加噪声。通过这些实验，我们发现了两个生物算法超过BP的关键优势。首先，在未提供整个训练数据集时，生物算法比BP表现要好得多。

    Bio-inspired learning has been gaining popularity recently given that Backpropagation (BP) is not considered biologically plausible. Many algorithms have been proposed in the literature which are all more biologically plausible than BP. However, apart from overcoming the biological implausibility of BP, a strong motivation for using Bio-inspired algorithms remains lacking. In this study, we undertake a holistic comparison of BP vs. multiple Bio-inspired algorithms to answer the question of whether Bio-learning offers additional benefits over BP. We test Bio-algorithms under different design choices such as access to only partial training data, resource constraints in terms of the number of training epochs, sparsification of the neural network parameters and addition of noise to input samples. Through these experiments, we notably find two key advantages of Bio-algorithms over BP. Firstly, Bio-algorithms perform much better than BP when the entire training dataset is not supplied. Four 
    
[^117]: 一个指数增长的通用量子电路系列

    An exponentially-growing family of universal quantum circuits. (arXiv:2212.00736v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2212.00736](http://arxiv.org/abs/2212.00736)

    该论文介绍了两种新的指数增长的量子机器学习体系结构，能够解决量子机器学习中的贫瘠高原问题，提高了量子编码的表达能力。

    

    量子机器学习已成为一个引起越来越多关注的领域，但它存在一定的理论和硬件限制。特别是，消失梯度问题或称为贫瘠高原问题，使得对于拥有大量量子比特的电路，训练变得不可能，限制了数据科学家在解决问题时可以使用的量子比特数量。另外，独立的角度嵌入监督量子神经网络被证明能够产生具有与编码深度和编码应用于的并行比特数直接相关的截断傅里叶级数。傅里叶级数的次数限制了模型的表达能力。本工作引入了两种新的体系结构，其傅里叶级数的次数呈指数增长：顺序和并行的指数增长量子机器学习体系结构。通过在编码时高效地利用可用的希尔伯特空间，增加了量子编码的表达能力，从而实现了指数增长。

    Quantum machine learning has become an area of growing interest but has certain theoretical and hardware-specific limitations. Notably, the problem of vanishing gradients, or barren plateaus, renders the training impossible for circuits with high qubit counts, imposing a limit on the number of qubits that data scientists can use for solving problems. Independently, angle-embedded supervised quantum neural networks were shown to produce truncated Fourier series with a degree directly dependent on two factors: the depth of the encoding and the number of parallel qubits the encoding applied to. The degree of the Fourier series limits the model expressivity. This work introduces two new architectures whose Fourier degrees grow exponentially: the sequential and parallel exponential quantum machine learning architectures. This is done by efficiently using the available Hilbert space when encoding, increasing the expressivity of the quantum encoding. Therefore, the exponential growth allows s
    
[^118]: RecXplainer: 针对推荐系统的分摊属性个性化解释

    RecXplainer: Amortized Attribute-based Personalized Explanations for Recommender Systems. (arXiv:2211.14935v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2211.14935](http://arxiv.org/abs/2211.14935)

    RecXplainer提供了一种针对推荐系统的分摊属性个性化解释，以解决用户和开发者之间的信任问题。

    

    推荐系统在数字世界中影响着我们的许多交互，影响着我们购物、浏览YouTube或TikTok时所看到的内容，以及在使用酒店平台时展示给我们的餐馆和酒店。现代推荐系统是基于专有和开源数据集训练的庞大且不透明的模型。自然而然地，在开发者和用户方面引发了信任问题：系统是否正常工作，为什么用户收到（或未收到）特定的推荐？在推荐旁边提供解释可以减轻一些这些关注。目前辅助推荐系统反馈的现状要么是用户特定的解释（例如，“购买商品B的用户也购买了商品A”），要么是物品特定的解释（例如，“我们推荐商品A是因为您观看/购买了商品B”）。然而，用户将个性化的背景信息带入他们的搜索体验中，将一个物品的价值视为该物品的函数.

    Recommender systems influence many of our interactions in the digital world -- impacting how we shop for clothes, sorting what we see when browsing YouTube or TikTok, and determining which restaurants and hotels we are shown when using hospitality platforms. Modern recommender systems are large, opaque models trained on a mixture of proprietary and open-source datasets. Naturally, issues of trust arise on both the developer and user side: is the system working correctly, and why did a user receive (or not receive) a particular recommendation? Providing an explanation alongside a recommendation alleviates some of these concerns. The status quo for auxiliary recommender system feedback is either user-specific explanations (e.g., "users who bought item B also bought item A") or item-specific explanations (e.g., "we are recommending item A because you watched/bought item B"). However, users bring personalized context into their search experience, valuing an item as a function of that item'
    
[^119]: 可靠的半监督学习中的对比可信度传播

    Contrastive Credibility Propagation for Reliable Semi-Supervised Learning. (arXiv:2211.09929v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09929](http://arxiv.org/abs/2211.09929)

    对比可信度传播采用迭代的传导式伪标签细化，将半监督学习和嘈杂标签学习统一，可在各种数据场景中可靠地超过有监督基准。

    

    生成无标签数据的标签容易出错，这使得半监督学习(Semi-Supervised Learning, SSL)变得困难。通常，我们很少了解算法何时以及为什么无法超过有监督基准。我们使用基准数据集构建了五种常见的真实世界半监督学习数据场景：少标签样本、开放域样本、嘈杂标签、有标签和无标签集合中的类别分布不均衡/错位。我们提出了一种名为对比可信度传播(Contrastive Credibility Propagation, CCP)的新算法，通过迭代的传导式伪标签细化实现深度半监督学习。CCP将半监督学习和嘈杂标签学习统一在一起，目的是在任何数据场景中可靠地超过有监督基准。与专注于子集场景的之前方法相比，CCP在所有场景中独特地超过了有监督基准，支持在标注数据或无标签数据质量未知的情况下的实践者。

    Producing labels for unlabeled data is error-prone, making semi-supervised learning (SSL) troublesome. Often, little is known about when and why an algorithm fails to outperform a supervised baseline. Using benchmark datasets, we craft five common real-world SSL data scenarios: few-label, open-set, noisy-label, and class distribution imbalance/misalignment in the labeled and unlabeled sets. We propose a novel algorithm called Contrastive Credibility Propagation (CCP) for deep SSL via iterative transductive pseudo-label refinement. CCP unifies semi-supervised learning and noisy label learning for the goal of reliably outperforming a supervised baseline in any data scenario. Compared to prior methods which focus on a subset of scenarios, CCP uniquely outperforms the supervised baseline in all scenarios, supporting practitioners when the qualities of labeled or unlabeled data are unknown.
    
[^120]: Diffiner: 一种用于语音增强的多功能扩散生成细化器

    Diffiner: A Versatile Diffusion-based Generative Refiner for Speech Enhancement. (arXiv:2210.17287v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2210.17287](http://arxiv.org/abs/2210.17287)

    Diffiner是一种基于DNN的生成细化器，可用于改善经过SE方法预处理后的感知语音质量。它可以应用于各种SE方法，且具有高度的模块化潜力。

    

    尽管基于深度神经网络（DNN）的语音增强（SE）方法优于以前的非DNN方法，但它们经常降低所生成输出的感知质量。为了解决这个问题，我们引入了一种基于DNN的生成细化器，Diffiner，旨在改善通过SE方法预处理过的感知语音质量。我们利用仅包含清晰语音的数据集训练了一个扩散生成模型。然后，我们的细化器有效地将通过去噪扩散恢复新生成的清晰部分与之前的SE方法造成的退化和失真部分混合在一起，从而产生细化的语音。一旦我们的细化器训练为一组清晰的语音，它就可以应用于各种SE方法，而无需为每个SE模块专门进行额外的训练。因此，我们的细化器可以是相对于SE方法的一个多功能的后处理模块，并具有高度的模块化潜力。实验结果表明，我们的方法改善了感知语音质量。

    Although deep neural network (DNN)-based speech enhancement (SE) methods outperform the previous non-DNN-based ones, they often degrade the perceptual quality of generated outputs. To tackle this problem, we introduce a DNN-based generative refiner, Diffiner, aiming to improve perceptual speech quality pre-processed by an SE method. We train a diffusion-based generative model by utilizing a dataset consisting of clean speech only. Then, our refiner effectively mixes clean parts newly generated via denoising diffusion restoration into the degraded and distorted parts caused by a preceding SE method, resulting in refined speech. Once our refiner is trained on a set of clean speech, it can be applied to various SE methods without additional training specialized for each SE module. Therefore, our refiner can be a versatile post-processing module w.r.t. SE methods and has high potential in terms of modularity. Experimental results show that our method improved perceptual speech quality rega
    
[^121]: E-MCTS：通过规划表观不确定性进行深度探索的模型基强化学习

    E-MCTS: Deep Exploration in Model-Based Reinforcement Learning by Planning with Epistemic Uncertainty. (arXiv:2210.13455v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13455](http://arxiv.org/abs/2210.13455)

    本文提出了一种新的方法E-MCTS，通过在MCTS预测中应用表观不确定性估计，实现了模型基强化学习中的深度探索，以及规划探索策略。通过实验证明这种方法在成功的表观不确定性估计和深度探索方面表现优异。

    

    模拟退火树搜索（MCTS）是模型基强化学习中应用最广泛、性能最优秀的规划方法之一。MCTS的关键挑战在于深度探索和面对未知时的可靠性，这两个挑战可以通过在MCTS预测中使用原则性的表观不确定性估计来缓解。本文提出了两个主要贡献：首先，我们开发了一种在MCTS中传播表观不确定性的方法，使智能体能够估计其预测的表观不确定性。其次，我们利用传播的不确定性提出了一种新的深度探索算法，通过明确规划探索策略。我们将这种方法应用于基于MCTS的模型基强化学习方法中，包括使用学习和提供的模型，通过实验证明了我们的方法实现了成功的表观不确定性估计并进行了深度探索。我们将其与基于非规划的深度探索基线进行了比较，并表明...

    One of the most well-studied and highly performing planning approaches used in Model-Based Reinforcement Learning (MBRL) is Monte-Carlo Tree Search (MCTS). Key challenges of MCTS-based MBRL methods remain dedicated deep exploration and reliability in the face of the unknown, and both challenges can be alleviated through principled epistemic uncertainty estimation in the predictions of MCTS. We present two main contributions: First, we develop methodology to propagate epistemic uncertainty in MCTS, enabling agents to estimate the epistemic uncertainty in their predictions. Second, we utilize the propagated uncertainty for a novel deep exploration algorithm by explicitly planning to explore. We incorporate our approach into variations of MCTS-based MBRL approaches with learned and provided models, and empirically show deep exploration through successful epistemic uncertainty estimation achieved by our approach. We compare to a non-planning-based deep-exploration baseline, and demonstrate
    
[^122]: SignReLU神经网络及其逼近能力

    SignReLU neural network and its approximation ability. (arXiv:2210.10264v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10264](http://arxiv.org/abs/2210.10264)

    本文研究了一种名为SignReLU的不同激活函数对深度神经网络逼近能力的影响，结果表明SignReLU网络在逼近性能方面优于有理数和ReLU网络。

    

    近年来，深度神经网络（DNNs）在科学和技术的各个领域引起了重大关注。激活函数定义了DNN中神经元如何处理输入信号。它们对于学习非线性变换和在连续神经元层之间执行多样化的计算至关重要。近年来，研究者们通过研究DNN的逼近能力来解释其强大和成功。本文通过使用一种名为SignReLU的不同激活函数，探索了DNN的逼近能力。我们的理论结果表明，SignReLU网络在逼近性能方面优于有理数和ReLU网络。数值实验比较了SignReLU与现有的激活函数（如ReLU、Leaky ReLU和ELU），结果显示了SignReLU的竞争实际性能。

    Deep neural networks (DNNs) have garnered significant attention in various fields of science and technology in recent years. Activation functions define how neurons in DNNs process incoming signals for them. They are essential for learning non-linear transformations and for performing diverse computations among successive neuron layers. In the last few years, researchers have investigated the approximation ability of DNNs to explain their power and success. In this paper, we explore the approximation ability of DNNs using a different activation function, called SignReLU. Our theoretical results demonstrate that SignReLU networks outperform rational and ReLU networks in terms of approximation performance. Numerical experiments are conducted comparing SignReLU with the existing activations such as ReLU, Leaky ReLU, and ELU, which illustrate the competitive practical performance of SignReLU.
    
[^123]: 神经网络修剪是否需要复杂性？一个关于全局幅度修剪的案例研究

    Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning. (arXiv:2209.14624v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14624](http://arxiv.org/abs/2209.14624)

    本论文研究了神经网络修剪中是否需要复杂性，并通过与全局幅度修剪进行比较发现，原始的全局幅度修剪方法优于其他最先进的修剪技术。

    

    在过去的十年中，修剪神经网络变得越来越流行，因为人们发现在不降低准确性的情况下可以安全地删除现代神经网络中的大量权重。自那时以来，已经提出了许多修剪方法，每一种都声称比前一种更好。今天，许多最先进的技术都依赖于使用重要性分数、通过反向传播获取反馈或基于启发式修剪规则等复杂的修剪方法。在这项工作中，我们质疑这种引入复杂性是否真的有必要来实现更好的修剪结果。我们将这些最先进的技术与一个简单的基准线进行了比较，即全局幅度修剪(Global MP)。全局幅度修剪根据权重的大小对其进行排序并修剪最小的权重。因此，在其原始形式中，它是最简单的修剪技术之一。令人惊讶的是，我们发现原始的全局幅度修剪优于所有其他最先进的技术并取得了最好的结果。

    Pruning neural networks has become popular in the last decade when it was shown that a large number of weights can be safely removed from modern neural networks without compromising accuracy. Numerous pruning methods have been proposed since then, each claiming to be better than the previous. Many state-of-the-art (SOTA) techniques today rely on complex pruning methodologies utilizing importance scores, getting feedback through back-propagation or having heuristics-based pruning rules amongst others. In this work, we question whether this pattern of introducing complexity is really necessary to achieve better pruning results. We benchmark these SOTA techniques against a naive pruning baseline, namely, Global Magnitude Pruning (Global MP). Global MP ranks weights in order of their magnitudes and prunes the smallest ones. Hence, in its vanilla form, it is one of the simplest pruning techniques. Surprisingly, we find that vanilla Global MP outperforms all the other SOTA techniques and ach
    
[^124]: 变分拟态算子网络

    Variationally Mimetic Operator Networks. (arXiv:2209.12871v3 [math.NA] UPDATED)

    [http://arxiv.org/abs/2209.12871](http://arxiv.org/abs/2209.12871)

    这项工作提出了一种新的算子网络架构，用于近似解决偏微分方程。这种架构模拟了从近似变分或弱形式问题中获得的数值解的形式，可以提高解的准确性。

    

    最近，算子网络已经成为近似解决偏微分方程（PDEs）的有希望的深度学习工具。这些网络将描述材料属性、强迫函数和边界数据的输入函数映射到PDE的解。本文描述了一种新的算子网络架构，该架构模拟了从近似变分或弱形式问题中获得的数值解的形式。将这些思想应用于通用椭圆PDE，得到了一种变分拟态算子网络（VarMiON）。类似于传统的深度算子网络（DeepONet），VarMiON也由一个子网络和另一个子网络组成，用于构造输出的基函数和这些基函数的系数。然而，与DeepONet不同的是，VarMiON中这些子网络的架构是精确定义的。对VarMiON解的误差分析表明，它包含一些与传统算子网络不同的优点。

    In recent years operator networks have emerged as promising deep learning tools for approximating the solution to partial differential equations (PDEs). These networks map input functions that describe material properties, forcing functions and boundary data to the solution of a PDE. This work describes a new architecture for operator networks that mimics the form of the numerical solution obtained from an approximate variational or weak formulation of the problem. The application of these ideas to a generic elliptic PDE leads to a variationally mimetic operator network (VarMiON). Like the conventional Deep Operator Network (DeepONet) the VarMiON is also composed of a sub-network that constructs the basis functions for the output and another that constructs the coefficients for these basis functions. However, in contrast to the DeepONet, the architecture of these sub-networks in the VarMiON is precisely determined. An analysis of the error in the VarMiON solution reveals that it contai
    
[^125]: 对于优化形状重新参数化，基于微分同胚群的深度神经网络

    Deep neural networks on diffeomorphism groups for optimal shape reparameterization. (arXiv:2207.11141v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2207.11141](http://arxiv.org/abs/2207.11141)

    本文提出了一种基于深度神经网络在微分同胚群上进行形状重新参数化的方法，通过构造近似的保持方向的微分同胚，并证明了其具有普遍逼近性质和Lipschitz常数的界限。

    

    形状分析中的一个基本问题是在计算形状之间的测地距离之前，将曲线或表面对齐。找到实现此对齐的最佳重新参数化是一个计算量庞大的任务，通常通过在微分同胚群上解决优化问题来完成。在本文中，我们提出了一种通过基本微分同胚的组合构造保持方向的微分同胚的算法。该算法使用PyTorch实现，并适用于非参数曲线和表面。此外，我们证明了构造的架构具有普遍逼近性质，并获得了所得微分同胚的Lipschitz常数的界限。

    One of the fundamental problems in shape analysis is to align curves or surfaces before computing geodesic distances between their shapes. Finding the optimal reparametrization realizing this alignment is a computationally demanding task, typically done by solving an optimization problem on the diffeomorphism group. In this paper, we propose an algorithm for constructing approximations of orientation-preserving diffeomorphisms by composition of elementary diffeomorphisms. The algorithm is implemented using PyTorch, and is applicable for both unparametrized curves and surfaces. Moreover, we show universal approximation properties for the constructed architectures, and obtain bounds for the Lipschitz constants of the resulting diffeomorphisms.
    
[^126]: 压缩傅里叶色散方法用于具有周期边界条件的高维扩散方程

    Compressive Fourier collocation methods for high-dimensional diffusion equations with periodic boundary conditions. (arXiv:2206.01255v3 [math.NA] UPDATED)

    [http://arxiv.org/abs/2206.01255](http://arxiv.org/abs/2206.01255)

    本研究提出了一种压缩傅里叶色散方法，用于解决定义在高维周期边界条件域上的扩散方程。该方法利用压缩感知和稀疏恢复技术，通过在蒙特卡罗采样上近似解的傅里叶系数，有效地克服了维度诅咒的影响。

    

    高维偏微分方程是一种常用的数学建模工具，应用范围从金融到计算化学。然而，解决这些方程的标准数值技术通常受到维度诅咒的影响。在本文中，我们解决了这个挑战，重点关注定义在高维域上具有周期边界条件的定常扩散方程。受高维稀疏函数逼近的最新进展启发，我们提出了一种新的方法，称为压缩傅里叶色散。我们的方法结合了压缩感知和谱色散的思想，用蒙特卡罗抽样代替了结构化色散网格的使用，并使用稀疏恢复技术（如正交匹配追踪和ℓ^1最小化）来近似PDE解的傅里叶系数。我们进行了严格的理论分析，证明了该方法的逼近误差。

    High-dimensional Partial Differential Equations (PDEs) are a popular mathematical modelling tool, with applications ranging from finance to computational chemistry. However, standard numerical techniques for solving these PDEs are typically affected by the curse of dimensionality. In this work, we tackle this challenge while focusing on stationary diffusion equations defined over a high-dimensional domain with periodic boundary conditions. Inspired by recent progress in sparse function approximation in high dimensions, we propose a new method called compressive Fourier collocation. Combining ideas from compressive sensing and spectral collocation, our method replaces the use of structured collocation grids with Monte Carlo sampling and employs sparse recovery techniques, such as orthogonal matching pursuit and $\ell^1$ minimization, to approximate the Fourier coefficients of the PDE solution. We conduct a rigorous theoretical analysis showing that the approximation error of the propose
    
[^127]: 案例感知对抗训练

    Case-Aware Adversarial Training. (arXiv:2204.09398v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.09398](http://arxiv.org/abs/2204.09398)

    本文提出了一种通用且高效的对抗性训练改进方案，即案例感知对抗训练（CAT），通过选择最有信息量的对抗性示例进行训练，降低了计算复杂度并保持防御效果。

    

    神经网络（NN）成为各种信号处理应用中最受关注的模型之一。然而，NN对于对抗性示例（AEs）极其容易受到攻击。为了防御AEs，对抗性训练（AT）被认为是最有效的方法，但由于计算量大，AT在大多数应用中受到限制。为了解决这个问题，本文设计了一种通用且高效的AT改进方案，即案例感知对抗训练（CAT）。具体而言，灵感来自于少部分信息丰富的样本对大多数模型性能的贡献。如果只使用最有信息量的AEs进行AT，可以显著降低AT的计算复杂度并保持防御效果。为实现此目标，CAT实现了两个突破。首先，提出了一种估计AE信息度的方法用于AE过滤。其次，为进一步丰富用于AT的信息，引入了一个自适应样本选择机制。

    The neural network (NN) becomes one of the most heated type of models in various signal processing applications. However, NNs are extremely vulnerable to adversarial examples (AEs). To defend AEs, adversarial training (AT) is believed to be the most effective method while due to the intensive computation, AT is limited to be applied in most applications. In this paper, to resolve the problem, we design a generic and efficient AT improvement scheme, namely case-aware adversarial training (CAT). Specifically, the intuition stems from the fact that a very limited part of informative samples can contribute to most of model performance. Alternatively, if only the most informative AEs are used in AT, we can lower the computation complexity of AT significantly as maintaining the defense effect. To achieve this, CAT achieves two breakthroughs. First, a method to estimate the information degree of adversarial examples is proposed for AE filtering. Second, to further enrich the information that 
    
[^128]: 使用图神经网络解决考虑实际约束条件下的交流电功率流问题

    Solving AC Power Flow with Graph Neural Networks under Realistic Constraints. (arXiv:2204.07000v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.07000](http://arxiv.org/abs/2204.07000)

    本文提出了一种使用图神经网络解决考虑实际约束条件下的交流电功率流问题的方法。我们开发了一个框架，通过无监督训练学习交流电功率流的物理约束，从而实现了独立于具体拓扑和供应任务的通用解。我们在中压基准网格上验证了我们的方法的可行性。

    

    本文提出了一种使用图神经网络架构解决考虑实际约束条件下的交流电功率流问题的方法。为了确保配电网的安全和弹性运行，交流电功率流计算是确定网格运行限制或分析网格资产利用情况的选择手段。在我们的方法中，我们展示了一个使用图神经网络学习电力流物理约束的框架的开发。我们在模型架构上进行了无监督训练，以学习独立于用于训练的具体拓扑和供应任务的交流电功率流公式的通用解。最后，我们在中压基准网格上展示、验证和讨论了我们的结果。在我们的方法中，我们关注配电网的物理和拓扑特性，以提供适用于实际网格拓扑的可扩展解决方案。因此，我们采用了数据驱动的方法，使用大规模多样化的数据集进行训练。

    In this paper, we propose a graph neural network architecture to solve the AC power flow problem under realistic constraints. To ensure a safe and resilient operation of distribution grids, AC power flow calculations are the means of choice to determine grid operating limits or analyze grid asset utilization in planning procedures. In our approach, we demonstrate the development of a framework that uses graph neural networks to learn the physical constraints of the power flow. We present our model architecture on which we perform unsupervised training to learn a general solution of the AC power flow formulation independent of the specific topologies and supply tasks used for training. Finally, we demonstrate, validate and discuss our results on medium voltage benchmark grids. In our approach, we focus on the physical and topological properties of distribution grids to provide scalable solutions for real grid topologies. Therefore, we take a data-driven approach, using large and diverse
    
[^129]: Temporal Difference学习算法的控制论分析

    Control Theoretic Analysis of Temporal Difference Learning. (arXiv:2112.14417v5 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2112.14417](http://arxiv.org/abs/2112.14417)

    本研究对Temporal Difference学习算法进行了控制论分析，并引入了一个有限时间的框架，从控制论角度提供了对TD学习机制和强化学习领域的更深入洞察。

    

    本文旨在对Temporal Difference (TD)学习算法进行控制论分析。TD学习作为强化学习领域的基石，提供了一种近似计算与马尔科夫决策过程中给定策略相关的值函数的方法。尽管已存在多篇关于TD学习理论理解的研究成果，但直到最近几年，研究人员才能对其统计效率提供具体保证。本文引入了一个有限时间的控制论框架，用于分析TD学习，借鉴了线性系统控制领域的已有概念。因此，本文通过使用控制论导出的简单分析工具，为TD学习的机制和强化学习的更广阔领域提供了额外的洞察。

    The goal of this manuscript is to conduct a controltheoretic analysis of Temporal Difference (TD) learning algorithms. TD-learning serves as a cornerstone in the realm of reinforcement learning, offering a methodology for approximating the value function associated with a given policy in a Markov Decision Process. Despite several existing works that have contributed to the theoretical understanding of TD-learning, it is only in recent years that researchers have been able to establish concrete guarantees on its statistical efficiency. In this paper, we introduce a finite-time, control-theoretic framework for analyzing TD-learning, leveraging established concepts from the field of linear systems control. Consequently, this paper provides additional insights into the mechanics of TD learning and the broader landscape of reinforcement learning, all while employing straightforward analytical tools derived from control theory.
    
[^130]: 张量列补全：基于Riemannian优化的局部恢复保证

    Tensor train completion: local recovery guarantees via Riemannian optimization. (arXiv:2110.03975v3 [math.NA] UPDATED)

    [http://arxiv.org/abs/2110.03975](http://arxiv.org/abs/2110.03975)

    本论文研究了基于Riemannian优化的张量列补全问题，推导了正交投影和核相干性的界限，并为具有辅助子空间信息的补全问题提供局部收敛保证。

    

    在这项工作中，我们估计了随机选择的元素数量，该数量保证了Riemannian梯度下降在张量列补全中具有局部收敛性的高概率。我们推导了一种新的正交投影上切空间的界限，该界限基于展开的奇异值的调和平均，并引入了张量列的核相干性概念。我们还将结果扩展到具有辅助子空间信息的张量列补全，并获得相应的局部收敛保证。

    In this work, we estimate the number of randomly selected elements of a tensor that with high probability guarantees local convergence of Riemannian gradient descent for tensor train completion. We derive a new bound for the orthogonal projections onto the tangent spaces based on the harmonic mean of the unfoldings' singular values and introduce a notion of core coherence for tensor trains. We also extend the results to tensor train completion with auxiliary subspace information and obtain the corresponding local convergence guarantees.
    
[^131]: NeXtQSM -- 一种完整的深度学习流水线，用于与混合数据训练的数据一致性定量磁敏感映射

    NeXtQSM -- A complete deep learning pipeline for data-consistent quantitative susceptibility mapping trained with hybrid data. (arXiv:2107.07752v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2107.07752](http://arxiv.org/abs/2107.07752)

    NeXtQSM使用混合数据进行训练，通过整合处理步骤，实现了数据一致性定量磁敏感映射计算的新型深度学习流水线。

    

    基于深度学习的定量磁敏感映射（QSM）近年来表现出巨大的潜力，与传统的非学习方法获得类似的结果。许多当前的深度学习方法不是数据一致的，需要体内训练数据或者将QSM问题分为连续的步骤来解决，从而导致错误的传播。本文旨在克服这些限制，并开发了一个框架来联合解决QSM处理步骤。我们开发了一种新的混合训练数据生成方法，可以通过变分网络将QSM模型项和学习的正则化项结合起来，以数据一致的方式进行背景场校正和偶极翻转的端到端训练。我们证明了NeXtQSM克服了先前深度学习方法的限制。NeXtQSM提供了一种新的基于深度学习的流水线，用于计算定量磁敏感映射，将每个处理步骤整合到训练中，并提供结果。

    Deep learning based Quantitative Susceptibility Mapping (QSM) has shown great potential in recent years, obtaining similar results to established non-learning approaches. Many current deep learning approaches are not data consistent, require in vivo training data or solve the QSM problem in consecutive steps resulting in the propagation of errors. Here we aim to overcome these limitations and developed a framework to solve the QSM processing steps jointly. We developed a new hybrid training data generation method that enables the end-to-end training for solving background field correction and dipole inversion in a data-consistent fashion using a variational network that combines the QSM model term and a learned regularizer. We demonstrate that NeXtQSM overcomes the limitations of previous deep learning methods. NeXtQSM offers a new deep learning based pipeline for computing quantitative susceptibility maps that integrates each processing step into the training and provides results that
    
[^132]: 分类规则生成：可扩展性，解释性和公平性

    Rule Generation for Classification: Scalability, Interpretability, and Fairness. (arXiv:2104.10751v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.10751](http://arxiv.org/abs/2104.10751)

    这项研究介绍了一种新的基于规则的分类优化方法，利用列生成线性规划实现可扩展性，并通过分配成本系数和引入额外约束解决了解释性和公平性问题。该方法在局部解释性和公平性之间取得了良好的平衡。

    

    我们引入了一种新的基于规则的分类优化方法，具有约束条件。所提出的方法利用列生成线性规划，因此可扩展到大型数据集。所得定价子问题被证明是NP难问题。我们采用基于决策树的启发式方法，并解决了一个代理定价子问题以加速。该方法返回一组规则以及它们的最优权重，指示每个规则对学习的重要性。我们通过为规则分配成本系数和引入额外约束来解决解释性和公平性问题。具体而言，我们关注局部解释性，并将公平性的一般分离准则推广到多个敏感属性和类别。我们在一系列数据集上测试了所提出方法的性能，并提供了一个案例研究来详细阐述其不同方面。所提出的基于规则的学习方法在局部解释性和公平性之间达到了良好的平衡点。

    We introduce a new rule-based optimization method for classification with constraints. The proposed method leverages column generation for linear programming, and hence, is scalable to large datasets. The resulting pricing subproblem is shown to be NP-Hard. We recourse to a decision tree-based heuristic and solve a proxy pricing subproblem for acceleration. The method returns a set of rules along with their optimal weights indicating the importance of each rule for learning. We address interpretability and fairness by assigning cost coefficients to the rules and introducing additional constraints. In particular, we focus on local interpretability and generalize separation criterion in fairness to multiple sensitive attributes and classes. We test the performance of the proposed methodology on a collection of datasets and present a case study to elaborate on its different aspects. The proposed rule-based learning method exhibits a good compromise between local interpretability and fairn
    
[^133]: 使用生成对抗网络在同时进行的EEG-fMRI中去除心动揭示现象

    Ballistocardiogram artifact removal in simultaneous EEG-fMRI using generative adversarial network. (arXiv:2011.01710v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.01710](http://arxiv.org/abs/2011.01710)

    本文提出了一种使用生成对抗网络在同时进行的EEG-fMRI中去除心动揭示现象的方法，通过优化网络模型的局部表示能力，改善整体性能，并获得一个可靠的生成器。该方法不需要额外的参考信号或复杂的硬件设备。

    

    由于同时进行的脑电图-功能磁共振成像（EEG-fMRI）的高时间和空间分辨率的优势，该技术吸引了广泛的关注，并被广泛应用于脑科学的各个研究领域。然而，在脑部的功能磁共振成像过程中，心动揭示现象（BCG）会严重污染脑电图（EEG）。作为一个非配对问题，BCG揭示现象的去除仍然是一个相当大的挑战。为了提供解决方案，本文提出了一种新颖的模块化生成对抗网络（GAN）及相应的训练策略，通过优化每个模块的参数来改进网络性能。通过这种方式，我们希望提高网络模型的局部表示能力，从而改善其整体性能，并获得一个可靠的用于去除BCG揭示现象的生成器。此外，所提出的方法不依赖于额外的参考信号或复杂的硬件设备。

    Due to its advantages of high temporal and spatial resolution, the technology of simultaneous electroencephalogram-functional magnetic resonance imaging (EEG-fMRI) acquisition and analysis has attracted much attention, and has been widely used in various research fields of brain science. However, during the fMRI of the brain, ballistocardiogram (BCG) artifacts can seriously contaminate the EEG. As an unpaired problem, BCG artifact removal now remains a considerable challenge. Aiming to provide a solution, this paper proposed a novel modular generative adversarial network (GAN) and corresponding training strategy to improve the network performance by optimizing the parameters of each module. In this manner, we hope to improve the local representation ability of the network model, thereby improving its overall performance and obtaining a reliable generator for BCG artifact removal. Moreover, the proposed method does not rely on additional reference signal or complex hardware equipment. E
    
[^134]: 在阻尼最小化中降低方向的新视角

    Walking in the Shadow: A New Perspective on Descent Directions for Constrained Minimization. (arXiv:2006.08426v4 [math.OC] UPDATED)

    [http://arxiv.org/abs/2006.08426](http://arxiv.org/abs/2006.08426)

    本文提出了一个新的视角来探索降低方向对于约束最小化的影响，展示了最佳的本地降低方向是负梯度投影的方向导数，以及沿着该方向移动相当于投影梯度下降的动力学。另外，Frank-Wolfe顶点对应于使用负梯度方向的“无限”步骤投影到多胞体上。

    

    降低方向，如朝向、Frank-Wolfe顶点、离开步骤、朝向内部的离开步骤和配对方向，是有条件梯度下降（CGD）变体中的重要设计考虑因素。我们试图揭示这些方向对于实现约束最小者的影响。最佳的本地降低方向是负梯度的投影的方向导数（即阴影）。我们展示了这个方向是可能的最佳离开步骤，并且在连续时间内，沿着阴影移动的动力学等同于投影梯度下降（PGD）的动力学，尽管离散化不是微不足道的。我们还展示了Frank-Wolfe（FW）顶点对应于使用负梯度方向上的“无限”步骤投影到多胞体上，从而提供了一个新的视角。我们将这些见解结合到一个新的Shadow-CG方法中。

    Descent directions such as movement towards Descent directions, including movement towards Frank-Wolfe vertices, away-steps, in-face away-steps and pairwise directions, have been an important design consideration in conditional gradient descent (CGD) variants. In this work, we attempt to demystify the impact of the movement in these directions towards attaining constrained minimizers. The optimal local direction of descent is the directional derivative (i.e., shadow) of the projection of the negative gradient. We show that this direction is the best away-step possible, and the continuous-time dynamics of moving in the shadow is equivalent to the dynamics of projected gradient descent (PGD), although it's non-trivial to discretize. We also show that Frank-Wolfe (FW) vertices correspond to projecting onto the polytope using an "infinite" step in the direction of the negative gradient, thus providing a new perspective on these steps. We combine these insights into a novel Shadow-CG method
    
[^135]: Coagent Networks再探讨

    Coagent Networks Revisited. (arXiv:2001.10474v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2001.10474](http://arxiv.org/abs/2001.10474)

    Coagent Networks（共智网络）是指在强化学习环境中协作的随机代理网络。这篇论文重新审视了共智网络理论，提出了执行路径的思想，并通过这一思想实现了对策梯度定理的简洁证明。

    

    Coagent networks（共智网络）形式化了在强化学习环境中协作以采取行动的随机代理网络的概念。共智网络的显著应用包括层次强化学习（HRL）的方法，例如使用选项的方法，通过在HRL代理中串联多个随机网络引入不同层次的抽象动作，来解决探索利用权衡问题。我们首先通过在共智网络中形式化执行规则、通过共智网络中执行路径的新颖而直观的思想，提供了一个统一的视角来描述许多不同的例子。在层次选项评论者架构中受到参数共享的启发，我们重新审视了共智网络理论，并使用我们的执行路径思想得到了对策梯度定理的更简洁证明，而不需要对参数共享做出任何假设。

    Coagent networks formalize the concept of arbitrary networks of stochastic agents that collaborate to take actions in a reinforcement learning environment. Prominent examples of coagent networks in action include approaches to hierarchical reinforcement learning (HRL), such as those using options, which attempt to address the exploration exploitation trade-off by introducing abstract actions at different levels by sequencing multiple stochastic networks within the HRL agents. We first provide a unifying perspective on the many diverse examples that fall under coagent networks. We do so by formalizing the rules of execution in a coagent network, enabled by the novel and intuitive idea of execution paths in a coagent network. Motivated by parameter sharing in the hierarchical option-critic architecture, we revisit the coagent network theory and achieve a much shorter proof of the policy gradient theorem using our idea of execution paths, without any assumption on how parameters are share
    
[^136]: 关于在一般采样分布下的低秩迹回归的研究

    On Low-rank Trace Regression under General Sampling Distribution. (arXiv:1904.08576v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1904.08576](http://arxiv.org/abs/1904.08576)

    本文研究了在一般采样分布下的低秩迹回归问题，并引入了一种通用峰值概念，提供了证明迹回归采样算子强凸性和获得非渐进、近乎最优界的方法。同时，将误差界扩展到以交叉验证选择正则化参数的情况下。

    

    本文研究了通过秩正则化回归的凸松弛或正则化非凸优化来估计参数矩阵B*的迹回归问题。已知这些估计器在对B*的秩、一致性和峰值性假设下满足近乎最优的误差界。我们首先引入了对B*的一种通用峰值概念，该概念提供了证明迹回归采样算子的受限强凸性以及获得估计误差的非渐进、近乎最优界的通用方法。与现有文献类似，这些结果要求正则化参数高于某个理论上的阈值，该阈值取决于实践中可能未知的观测噪声。接下来，我们将误差界扩展到以交叉验证选择正则化参数的情况。这个结果的重要性在于现有关于交叉验证估计器的理论结果(Kale等)。

    In this paper, we study the trace regression when a matrix of parameters B* is estimated via the convex relaxation of a rank-regularized regression or via regularized non-convex optimization. It is known that these estimators satisfy near-optimal error bounds under assumptions on the rank, coherence, and spikiness of B*. We start by introducing a general notion of spikiness for B* that provides a generic recipe to prove the restricted strong convexity of the sampling operator of the trace regression and obtain near-optimal and non-asymptotic error bounds for the estimation error. Similar to the existing literature, these results require the regularization parameter to be above a certain theory-inspired threshold that depends on observation noise that may be unknown in practice. Next, we extend the error bounds to cases where the regularization parameter is chosen via cross-validation. This result is significant in that existing theoretical results on cross-validated estimators (Kale et
    

