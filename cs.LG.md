# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [On Practical Aspects of Aggregation Defenses against Data Poisoning Attacks.](http://arxiv.org/abs/2306.16415) | 本文研究了数据中毒攻击的聚合防御策略的实践方面，并针对Deep Partition Aggregation进行了评估，包括效率、性能和鲁棒性。实验结果显示，基于缩放基础模型的方法能够提高聚合防御的训练效率。 |
| [^2] | [MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning.](http://arxiv.org/abs/2306.16413) | MultiZoo和MultiBench是用于多模态深度学习的标准化工具包，提供了多模态算法的实现和大规模基准测试，以促进对多模态模型能力和限制的理解，并确保易用性和可重复性。 |
| [^3] | [Sharper Model-free Reinforcement Learning for Average-reward Markov Decision Processes.](http://arxiv.org/abs/2306.16394) | 本论文提出了针对无限周期平均奖励马尔可夫决策过程（MDPs）的多个高效无模型强化学习（RL）算法，包括在线设置和仿真器设置，实现了最佳的遗憾和样本使用依赖性。 |
| [^4] | [Accelerating Sampling and Aggregation Operations in GNN Frameworks with GPU Initiated Direct Storage Accesses.](http://arxiv.org/abs/2306.16384) | 本论文提出了一种通过利用GPU发起直接存储访问来加速GNN框架中的采样和聚合操作的方法，解决了在训练大规模图上时CPU无法充分利用GPU资源的问题。 |
| [^5] | [Multi-Site Clinical Federated Learning using Recursive and Attentive Models and NVFlare.](http://arxiv.org/abs/2306.16367) | 本论文提出了一种多站点临床联邦学习的方法，使用递归和注意力模型以及NVFlare框架。研究引入了两种示例性的自然语言处理模型，LSTM模型和BERT模型，这些模型在理解上下文和语义方面表现出了优异的性能。 |
| [^6] | [Beyond NTK with Vanilla Gradient Descent: A Mean-Field Analysis of Neural Networks with Polynomial Width, Samples, and Time.](http://arxiv.org/abs/2306.16361) | 本文提供了对多项式宽度两层神经网络上的投影梯度流的均场分析，证明了原始梯度下降与NTK之间的明显差异，即在样本复杂度方面原始梯度下降可以比核方法实现更高的性能。 |
| [^7] | [cuSLINK: Single-linkage Agglomerative Clustering on the GPU.](http://arxiv.org/abs/2306.16354) | cuSLINK是一种在GPU上实现的单链接聚类算法，具有较低的空间复杂度和可重复使用的构建模块，适用于广泛的数据挖掘和机器学习应用。 |
| [^8] | [Information-Computation Tradeoffs for Learning Margin Halfspaces with Random Classification Noise.](http://arxiv.org/abs/2306.16352) | 本文研究了学习具有随机分类噪声的边界半空间的信息-计算权衡问题，发现了样本复杂性和计算效率算法之间的固有差距，并给出了相应的样本复杂性下界。 |
| [^9] | [Emulating the dynamics of complex systems using autoregressive models on manifolds (mNARX).](http://arxiv.org/abs/2306.16335) | 本研究提出了一种名为mNARX的方法，通过构建流形和自回归模型，以高效准确地近似复杂系统的动力学响应。这种方法能够将整个问题分解成较小的子问题，具有良好的可扩展性，并且与传统的降维技术相结合，适用于建模复杂系统。 |
| [^10] | [Identifiability of Discretized Latent Coordinate Systems via Density Landmarks Detection.](http://arxiv.org/abs/2306.16334) | 本文提出了一种新颖的可识别性形式，称为量化坐标可识别性。在无监督的情况下，我们展示了在高度通用的非线性映射下，可以恢复离散化的潜在坐标，而无需额外的归纳偏差。这一发现对解缠研究具有重要意义。 |
| [^11] | [Representation Learning via Variational Bayesian Networks.](http://arxiv.org/abs/2306.16326) | VBN是一种利用层次和关系信息的新颖的贝叶斯实体表示学习模型，特别适用于数据稀缺的“长尾”实体建模。通过使用层次先验和明确关系约束，VBN提供了更好的建模效果，并通过密度表示实体，对数据稀缺情况下的不确定性进行建模。我们还提出了一种可扩展的变分贝叶斯优化算法来实现快速的近似贝叶斯推断。 |
| [^12] | [Gaussian random field approximation via Stein's method with applications to wide random neural networks.](http://arxiv.org/abs/2306.16308) | 本研究利用Stein方法推导出Wasserstein距离的上界，通过高斯平滑技术将平滑度量转化为Wasserstein距离。通过特殊化结果，我们获得了广义随机神经网络中对高斯随机场逼近的首个上界。 |
| [^13] | [Relevant Entity Selection: Knowledge Graph Bootstrapping via Zero-Shot Analogical Pruning.](http://arxiv.org/abs/2306.16296) | 本文提出了一种基于类比的方法，通过选择和修剪相关实体，从而引导知识图谱的构建。实证结果显示，这种方法在两个领域和异质种子实体的数据集上优于其他机器学习方法，并具有较低的参数数量。这些结果支持在相关任务中进一步应用类比推理。 |
| [^14] | [Clustering-based Identification of Precursors of Extreme Events in Chaotic Systems.](http://arxiv.org/abs/2306.16291) | 本文研究了基于聚类的方法在混沌系统中识别极端事件前兆的应用，通过聚类系统状态、概率转移矩阵和状态空间划分识别了两个不同混沌系统中的极端事件前兆。 |
| [^15] | [Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting.](http://arxiv.org/abs/2306.16275) | 本研究提出了一种通过迭代提示与ChatGPT或GPT-4进行多轮交互的方法来改进食物影响摘要的准确性。这种方法在产品特定指导(PSG)开发中具有潜力应用的价值。 |
| [^16] | [S2SNet: A Pretrained Neural Network for Superconductivity Discovery.](http://arxiv.org/abs/2306.16270) | S2SNet是一个预训练的神经网络，用于预测超导性。它利用了注意机制，并基于包含晶体结构和超导临界温度的新数据集S2S进行训练。 |
| [^17] | [Deep Unfolded Simulated Bifurcation for Massive MIMO Signal Detection.](http://arxiv.org/abs/2306.16264) | 本文提出了一种基于量子启发式算法的模拟双分岔用于大规模MIMO信号检测的方法，并通过修改算法和使用深度展开技术，显著改善了检测性能。 |
| [^18] | [Latent SDEs on Homogeneous Spaces.](http://arxiv.org/abs/2306.16248) | 这篇论文研究了在齐次空间上的潜在SDE，通过使用单位球上的SDE进行变分推断，提出了一种简单且直观的表达式来计算近似后验和先验过程之间的KL散度。 |
| [^19] | [Continuous-Time q-learning for McKean-Vlasov Control Problems.](http://arxiv.org/abs/2306.16208) | 本文研究了连续时间q-learning在熵正则化强化学习框架下用于McKean-Vlasov控制问题，并揭示了两种不同的q函数的存在及其积分表示。 |
| [^20] | [Defining data science: a new field of inquiry.](http://arxiv.org/abs/2306.16177) | 数据科学是一种新的研究范式，具有潜力和应用广泛性，在40多个学科、数百个研究领域和成千上万个应用中出现。然而，由于其起步阶段，目前存在许多定义的冗余和不一致性的问题。 |
| [^21] | [Mitigating the Accuracy-Robustness Trade-off via Multi-Teacher Adversarial Distillation.](http://arxiv.org/abs/2306.16170) | 本文介绍了一种名为多教师对抗鲁棒性蒸馏的方法，它通过使用强大的干净样本教师和鲁棒性教师来改进深度神经网络的对抗训练过程，以减轻准确性和鲁棒性之间的权衡。 |
| [^22] | [Communication Resources Constrained Hierarchical Federated Learning for End-to-End Autonomous Driving.](http://arxiv.org/abs/2306.16169) | 本文提出了一种优化的通信资源受限的分层联邦学习框架，用于端到端自动驾驶模型，通过混合数据和模型聚合来降低泛化误差，并在CARLA仿真平台上评估了其有效性。 |
| [^23] | [Recent Advances in Optimal Transport for Machine Learning.](http://arxiv.org/abs/2306.16156) | 最优输运在机器学习中的最新进展包括生成建模和迁移学习等领域，并且计算最优输运的发展也与机器学习实践相互影响。 |
| [^24] | [MLSMM: Machine Learning Security Maturity Model.](http://arxiv.org/abs/2306.16127) | 这篇论文提出了一个初步的机器学习安全成熟度模型（MLSMM），通过将安全实践沿着ML开发生命周期进行组织和建立成熟度水平，以评估安全实践的成熟度。这将有助于促进产业界和学术界的更紧密合作。 |
| [^25] | [More efficient manual review of automatically transcribed tabular data.](http://arxiv.org/abs/2306.16126) | 本文提出了一种更高效的自动转录表格数据的人工审核方法，通过使用机器学习方法转录大量的手写职业代码并进行手动审核，达到了准确性的提升和工作流程的改善。 |
| [^26] | [Semantic Positive Pairs for Enhancing Contrastive Instance Discrimination.](http://arxiv.org/abs/2306.16122) | 提出了一种名为语义正向对集合（SPPS）的方法，可以在表示学习过程中识别具有相似语义内容的图像，并将它们视为正向实例，从而减少丢弃重要特征的风险。在多个实验数据集上的实验证明了该方法的有效性。 |
| [^27] | [Time Regularization in Optimal Time Variable Learning.](http://arxiv.org/abs/2306.16111) | 本文研究了时间正则化在最优时间变量学习中的应用，通过引入一个与离散动力系统中的时间范围直接相关的正则化项，并提出了一种适应性剪枝方法用于减少网络复杂性和训练时间，同时保持表达能力。在MNIST和Fashion MNIST数据集上的分类任务上验证了方法的有效性。 |
| [^28] | [Sparse Representations, Inference and Learning.](http://arxiv.org/abs/2306.16097) | 这篇论文介绍了一种通用框架，可以用于解决涉及弱长程相互作用的各种推理问题，包括压缩感知和感知器学习。利用统计物理学的方法，我们可以研究这些问题的基本限制，并提出相应的算法。 |
| [^29] | [Empirical Loss Landscape Analysis of Neural Network Activation Functions.](http://arxiv.org/abs/2306.16090) | 本研究通过经验分析了双曲正切、修正线性单元和指数线性单元激活函数相关的神经网络损失曲面，发现修正线性单元呈现最凸型曲面，指数线性单元呈现最平坦曲面并具有更优的泛化性能。所有激活函数的损失曲面中存在宽阔和狭窄的山谷，而狭窄的山谷与饱和神经元和隐含的正则化网络结构相关。 |
| [^30] | [Mass Spectra Prediction with Structural Motif-based Graph Neural Networks.](http://arxiv.org/abs/2306.16085) | 提出了一种基于结构模式和图神经网络的质谱预测网络（MoMS-Net），可以通过考虑子结构在图级别的方式实现质谱预测，从而更好地扩展质谱库并提高预测准确性。 |
| [^31] | [Secure and Fast Asynchronous Vertical Federated Learning via Cascaded Hybrid Optimization.](http://arxiv.org/abs/2306.16077) | 本论文提出了一种在垂直联邦学习中使用级联混合优化的方法，通过在下游使用零阶优化保护隐私并在上游使用一阶优化提高收敛速度，从而解决了ZOO-based VFL收敛速度较慢的问题。 |
| [^32] | [Federated Generative Learning with Foundation Models.](http://arxiv.org/abs/2306.16064) | 本文提出了一种基于基础生成模型的联邦生成学习框架，通过传输提示与分布式训练数据，可以远程合成有信息量的训练数据，从而改善了通信效率、适应分布转移、提升性能、加强隐私保护。 |
| [^33] | [DUET: 2D Structured and Approximately Equivariant Representations.](http://arxiv.org/abs/2306.16058) | DUET是一种2D结构化且近似等变表示方法，相比于其他方法，可以在保留输入变换信息的同时具有更好的可控性和更高的准确性。 |
| [^34] | [Improving Primate Sounds Classification using Binary Presorting for Deep Learning.](http://arxiv.org/abs/2306.16054) | 本研究提出了一种改进的方法，通过使用二进制预排序和卷积神经网络对灵长类声音进行分类，以提高多类分类任务的性能。在“ComparE 2021”数据集上进行的实验结果显示，相比于基准模型，该方法的准确性和UAR分数显著提高。 |
| [^35] | [Evaluating Similitude and Robustness of Deep Image Denoising Models via Adversarial Attack.](http://arxiv.org/abs/2306.16050) | 通过对抗攻击评估了深度图像去噪模型的相似性和鲁棒性，发现现有模型容易被攻击。还研究了去噪模型的迁移性在图像去噪任务中的特性。 |
| [^36] | [OpenNDD: Open Set Recognition for Neurodevelopmental Disorders Detection.](http://arxiv.org/abs/2306.16045) | OpenNDD是一个用于神经发育障碍检测的开放性识别框架，结合了自动编码器和对抗循环点开放性识别技术，能准确识别已知类别并识别未遇到的类别。 |
| [^37] | [Lightweight Modeling of User Context Combining Physical and Virtual Sensor Data.](http://arxiv.org/abs/2306.16029) | 本文提出了一个轻量级的用户上下文建模方法，通过结合物理和虚拟传感器数据，利用机器学习技术实现上下文感知服务，并通过采集大规模数据集进行推断过程优化。 |
| [^38] | [Exponential separations between classical and quantum learners.](http://arxiv.org/abs/2306.16028) | 本文研究了经典学习者和量子学习者之间的指数区别，并提出了两个新的学习分离问题，其中经典困难主要在于识别生成数据的函数。 |
| [^39] | [A Distributed Computation Model Based on Federated Learning Integrates Heterogeneous models and Consortium Blockchain for Solving Time-Varying Problems.](http://arxiv.org/abs/2306.16023) | 本研究提出了一种基于联邦学习和联盟区块链的分布式计算模型，解决了时变问题中异构模型和模型间协作的难题。 |
| [^40] | [Structure in Reinforcement Learning: A Survey and Open Problems.](http://arxiv.org/abs/2306.16021) | 这项调查研究了强化学习中结构的角色和重要性，并介绍了各个子领域在提高强化学习的性能方面所做的工作。 |
| [^41] | [BayesFlow: Amortized Bayesian Workflows With Neural Networks.](http://arxiv.org/abs/2306.16015) | BayesFlow是一个Python库，提供了使用神经网络进行摊还贝叶斯推断的功能，用户可以在模型仿真上训练定制的神经网络，并将其用于任何后续应用。这种摊还贝叶斯推断能够快速准确地进行推断，并实现了对不可计算后验分布的近似。 |
| [^42] | [Systematic analysis of the impact of label noise correction on ML Fairness.](http://arxiv.org/abs/2306.15994) | 本论文对标签噪声修正技术在保证机器学习模型公平性方面的有效性进行了系统分析，通过开发经验方法来评估这些修正方法对修正有偏数据集上的模型的影响。 |
| [^43] | [MyDigitalFootprint: an extensive context dataset for pervasive computing applications at the edge.](http://arxiv.org/abs/2306.15990) | "MyDigitalFootprint" 是一个以边缘计算为应用场景的全面背景数据集，支持多模态上下文识别和社交关系建模。该数据集包含了智能手机传感器数据、物理接近信息和在线社交网络互动。通过这个数据集，研究人员可以在用户的自然环境中进行大规模数据分析和上下文适应研究。 |
| [^44] | [Reconstructing the Hemodynamic Response Function via a Bimodal Transformer.](http://arxiv.org/abs/2306.15971) | 本研究首次引入了一种双模态变换器预测模型，通过历史血流和神经活动来推断当前血流，增强了模型的预测能力，并提出了关于血液动力学响应神经活动的假设。 |
| [^45] | [Separable Physics-Informed Neural Networks.](http://arxiv.org/abs/2306.15969) | 这项研究提出了一种可分离的物理信息神经网络（SPINN），通过逐个处理轴来显著减少了多维 PDE 中的网络传播数量，并使用正向模式自动微分降低了计算成本，使得可以在单个普通 GPU 上使用大量的配点。 |
| [^46] | [Action and Trajectory Planning for Urban Autonomous Driving with Hierarchical Reinforcement Learning.](http://arxiv.org/abs/2306.15968) | 本文提出了一种使用分层强化学习方法的城市自动驾驶行动和轨迹规划器，通过感知信息和分层模型来学习和规划自动驾驶车辆行为，并解决复杂城市场景下的行驶任务和动态环境变化的挑战。 |
| [^47] | [Graph Interpolation via Fast Fused-Gromovization.](http://arxiv.org/abs/2306.15963) | 本文提出了一种通过快速融合Gromov化的方法，用于图插值和图数据增强。通过考虑图结构和信号之间的相互作用，我们提出了一种匹配节点之间的最优策略来解决这一问题。为了提高可扩展性，我们引入了一种放松的FGW求解器来加速算法的收敛速度。 |
| [^48] | [Reduce Computational Complexity for Convolutional Layers by Skipping Zeros.](http://arxiv.org/abs/2306.15951) | 本文提出了C-K-S算法，通过修剪滤波器和转换稀疏张量为稠密张量的方式，跳过卷积层中的0元素，从而降低了计算复杂度。实验证明，C-K-S相对于PyTorch具有优势。 |
| [^49] | [Pb-Hash: Partitioned b-bit Hashing.](http://arxiv.org/abs/2306.15944) | Pb-Hash提出了一种分区b位哈希的方法，通过将B位哈希分成m个块来重复使用已有的哈希，能够显著减小模型的大小。 |
| [^50] | [Interpretable Anomaly Detection in Cellular Networks by Learning Concepts in Variational Autoencoders.](http://arxiv.org/abs/2306.15938) | 本文提出了一种利用变分自编码器学习概念在手机网络中解释异常检测的方法，通过重构损失和Z分数来检测异常，并通过K-means算法增强表示学习，实现了异常的可解释性。该框架为手机网络中的异常检测提供了更快且自主的解决方案，并展示了深度学习算法处理大数据的潜力。 |
| [^51] | [Curious Replay for Model-based Adaptation.](http://arxiv.org/abs/2306.15934) | 好奇回放是一种针对模型为基础的代理的优先经验回放方法，通过使用好奇度基础的优先信号，它提高了探索性能，并在Crafter基准测试中取得了更好的成绩。 |
| [^52] | [You Can Generate It Again: Data-to-text Generation with Verification and Correction Prompting.](http://arxiv.org/abs/2306.15933) | 本文提出了一种多步骤生成、验证和纠正的数据生成文本方法，通过专门的错误指示提示来改善输出质量。 |
| [^53] | [NIPD: A Federated Learning Person Detection Benchmark Based on Real-World Non-IID Data.](http://arxiv.org/abs/2306.15932) | NIPD是一个基于真实世界非独立与同分布数据的联邦学习人体检测基准，开源了一个非独立和同分布的物联网人体检测数据集。 |
| [^54] | [Learning Dynamic Graphs from All Contextual Information for Accurate Point-of-Interest Visit Forecasting.](http://arxiv.org/abs/2306.15927) | 提出了一种名为忙碌图神经网络（BysGNN）的临时图神经网络，利用所有背景信息和时间序列数据来学习兴趣点之间的多背景相关性，以实现准确的访问量预测。 |
| [^55] | [Most Language Models can be Poets too: An AI Writing Assistant and Constrained Text Generation Studio.](http://arxiv.org/abs/2306.15926) | 这项研究展示了如何通过在语言模型中应用过滤函数来生成有限约束文本，并提出了一个AI写作助手工具，可以根据用户的需求生成带有各种约束条件的文本。 |
| [^56] | [The curse of dimensionality in operator learning.](http://arxiv.org/abs/2306.15924) | 算子学习中存在维度诅咒，但对于由Hamilton-Jacobi方程定义的解算子可以克服维度诅咒。 |
| [^57] | [On information captured by neural networks: connections with memorization and generalization.](http://arxiv.org/abs/2306.15918) | 研究了神经网络在训练过程中捕获的信息，探讨了神经网络在特殊、模糊或少数子群体示例上的行为，在泛化中提供了重要的信息和理解。 |
| [^58] | [Transfer Learning with Random Coefficient Ridge Regression.](http://arxiv.org/abs/2306.15915) | 本文提出了在迁移学习中使用随机系数岭回归的方法，通过最小化估计风险或预测风险来确定目标模型和源模型的回归系数的最优权重，并使用随机矩阵理论得出了最优权重的极限值。 |
| [^59] | [DCT: Dual Channel Training of Action Embeddings for Reinforcement Learning with Large Discrete Action Spaces.](http://arxiv.org/abs/2306.15913) | 本文提出了一个双通道动作嵌入训练的框架，能够在大离散动作空间中学习稳健策略，并成功应用在2D迷宫环境和真实世界电子商务任务中。 |
| [^60] | [RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$.](http://arxiv.org/abs/2306.15909) | RL$^3$是一种原则性混合方法，通过将传统强化学习学到的任务特定动作值作为元强化学习神经网络的输入，提高了元强化学习的性能。 |
| [^61] | [Deep Learning Models for Water Stage Predictions in South Florida.](http://arxiv.org/abs/2306.15907) | 本论文利用深度学习模型训练代理模型，快速预测南佛罗里达州迈阿密河下游的水位，并与基于物理的模型进行比较。 |
| [^62] | [Dimension Independent Mixup for Hard Negative Sample in Collaborative Filtering.](http://arxiv.org/abs/2306.15905) | 本文提出了一种协同过滤训练中维度无关的困难负样本混合方法（DINS），通过对采样区域的新视角进行重新审视来改进现有的采样方法。实验证明，DINS优于其他负采样方法，证实了其有效性和优越性。 |
| [^63] | [Individual and Structural Graph Information Bottlenecks for Out-of-Distribution Generalization.](http://arxiv.org/abs/2306.15902) | 这项工作提出了一种统一框架，个别和结构化图信息瓶颈（IS-GIB），用于解决域外图像通用化中的问题，通过丢弃虚假特征和利用结构关联来提高性能。 |
| [^64] | [Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias.](http://arxiv.org/abs/2306.15895) | 本论文研究了大型语言模型作为属性化训练数据生成器的应用。通过使用具有多样性属性的提示，我们能够生成多样化且归因的数据。研究表明，在高基数和多样领域的数据集中，使用属性化提示对生成模型性能有积极影响。此外，论文还展示了关于偏差、多样性和效率的全面实证研究结果，并得出了三个关键观察：系统性偏差存在于生成数据中，多样性和效率之间存在权衡，属性化训练数据生成可以改善模型性能。 |
| [^65] | [Asymptotic-Preserving Convolutional DeepONets Capture the Diffusive Behavior of the Multiscale Linear Transport Equations.](http://arxiv.org/abs/2306.15891) | 本文介绍了两种新型的渐近保持的卷积Deep Operator网络（APCONs），用于解决多尺度时变线性输运问题。该方法通过采用多个局部卷积操作、池化和激活操作来捕捉线性输运问题的扩散行为，并验证了方法的有效性。 |
| [^66] | [A Unified View of Deep Learning for Reaction and Retrosynthesis Prediction: Current Status and Future Challenges.](http://arxiv.org/abs/2306.15890) | 本论文提供了深度学习在反应和逆合成预测中的统一视角。通过调查现有的深度学习方法和模型，总结了它们的设计机制、优势和不足之处。同时，探讨了当前解决方案的局限性和问题本身的挑战，并提出了未来研究的有希望的方向。 |
| [^67] | [Sequential Attention Source Identification Based on Feature Representation.](http://arxiv.org/abs/2306.15886) | 本文提出了一种基于序列到序列的本地化框架，通过估计用户之间的影响概率生成多个特征，并通过时序注意机制区分不同时刻预测源的重要性。该方法在解决时变感染情景下用户交互问题方面具有较高的准确性，对新场景的检测也具有良好的可扩展性。 |
| [^68] | [Blockwise Feature Interaction in Recommendation Systems.](http://arxiv.org/abs/2306.15881) | 该论文提出了一种称为块状特征交互 (BFI) 的方法，通过将特征交互过程分成较小的块，以显著减少内存占用和计算负担。实验证明，BFI算法在准确性上接近标准DCNv2，同时大大减少了计算开销和参数数量，为高效推荐系统的发展做出了贡献。 |
| [^69] | [Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion.](http://arxiv.org/abs/2306.15875) | 本文描述了一种使用声音转换生成样本特定触发器的后门攻击方法，有效地绕过了深度语音分类中存在的安全防护措施，并且不会引入额外的可听噪音。 |
| [^70] | [Discovering stochastic partial differential equations from limited data using variational Bayes inference.](http://arxiv.org/abs/2306.15873) | 本文提出了一种新的框架，结合了随机微积分、变分贝叶斯理论和稀疏学习的概念，用于从有限数据中准确地发现随机偏微分方程（SPDEs）。作者应用该方法成功地识别了随机热方程、随机Allen-Cahn方程和随机Nagumo方程，并证明了该方法在各种科学应用中的重要性。 |
| [^71] | [GraSS: Contrastive Learning with Gradient Guided Sampling Strategy for Remote Sensing Image Semantic Segmentation.](http://arxiv.org/abs/2306.15868) | 提出了一种基于对比学习的带有梯度引导采样策略（GraSS）用于遥感图像语义分割任务，解决了正样本混淆和特征适应偏差的问题。 |
| [^72] | [Differentially Private Distributed Estimation and Learning.](http://arxiv.org/abs/2306.15865) | 本文研究了在网络环境中的分布式估计和学习问题，通过交换私有观测信息，代理可以集体估计未知数量，而保护隐私。通过线性聚合方案和差分隐私（DP）调整的随机化方案，本研究提出了一种能够在保证隐私的同时高效组合观测数据的算法。 |
| [^73] | [Hierarchical Graph Neural Networks for Proprioceptive 6D Pose Estimation of In-hand Objects.](http://arxiv.org/abs/2306.15858) | 本文提出了一种分层图神经网络架构，用于结合多模态数据，实现几何信息有根据的6D物体姿态估计。 |
| [^74] | [Pure exploration in multi-armed bandits with low rank structure using oblivious sampler.](http://arxiv.org/abs/2306.15856) | 本文研究了纯探索问题中具有低秩结构的多臂赌博机，提出了一种盲目采样器应用以解决分离设置下的纯探索问题，并给出了具有低秩序列的多臂赌博机纯探索问题的上界和下界。 |
| [^75] | [GoalieNet: A Multi-Stage Network for Joint Goalie, Equipment, and Net Pose Estimation in Ice Hockey.](http://arxiv.org/abs/2306.15853) | GoalieNet是一个多阶段深度神经网络，用于联合推测冰球中守门员、装备和球网的姿势。实验结果表明，GoalieNet在大量关键点的准确率超过80%的情况下，整体平均准确率达到84%，表明这种联合姿势推测方法具有潜力。 |
| [^76] | [Ordering for Non-Replacement SGD.](http://arxiv.org/abs/2306.15848) | 这项研究提出了一种优化非替代性SGD算法收敛速度的排序策略，并通过实验证实了其有效性。 |
| [^77] | [Easing Color Shifts in Score-Based Diffusion Models.](http://arxiv.org/abs/2306.15832) | 本文提出了在基于得分的扩散模型中缓解颜色偏移的计算廉价解决方案，并引入了一个简单的非线性绕过连接来改善生成图像的空间均值。 |
| [^78] | [Confidence-based Ensembles of End-to-End Speech Recognition Models.](http://arxiv.org/abs/2306.15824) | 本文提出了一种基于置信度的端到端语音识别模型的集成方法，通过只使用最有信心的模型的输出来提高性能，并通过两个应用程序的实验证明了方法的有效性。 |
| [^79] | [Learning normal asymmetry representations for homologous brain structures.](http://arxiv.org/abs/2306.15811) | 这篇论文介绍了一种学习同源脑结构正常不对称表示的新方法，该方法利用异常检测和表示学习，通过Siamese架构将脑结构的左右半球映射到一个正常不对称嵌入空间。该方法可以量化与正常不对称性偏离的程度。 |
| [^80] | [FLuRKA: Fast fused Low-Rank & Kernel Attention.](http://arxiv.org/abs/2306.15799) | FLuRKA是一种融合低秩和核注意力的新型transformer类别，相较于传统的近似技术，在运行时间性能和质量方面都表现出显著的提升。 |
| [^81] | [HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.](http://arxiv.org/abs/2306.15794) | HyenaDNA是一种基于隐式卷积的基因组序列建模方法，可以在单核苷酸分辨率下对长范围相互作用进行建模。 |
| [^82] | [A Population-Level Analysis of Neural Dynamics in Robust Legged Robots.](http://arxiv.org/abs/2306.15793) | 本研究通过分析神经动力学在稳健步行机器人中的人群层活动，揭示了控制器的拓扑结构对平衡能力的影响；通过应用神经干扰探究系统的强迫响应，发现循环状态动力学具有结构化和低维特征，并提出了一种新的控制机制的存在证据。 |
| [^83] | [Probing the Transition to Dataset-Level Privacy in ML Models Using an Output-Specific and Data-Resolved Privacy Profile.](http://arxiv.org/abs/2306.15790) | 本论文针对差分隐私（DP）框架的不足，研究了使用DP机制训练的模型在相邻数据集上的“覆盖程度”。通过连接覆盖程度指标和已有研究，我们排名了训练集中个别样本的隐私，并形成了一个隐私配置文件。此外，我们展示了隐私配置文件可以用来探测观察到的隐私转换。 |
| [^84] | [Structured State Space Models for Multiple Instance Learning in Digital Pathology.](http://arxiv.org/abs/2306.15789) | 本文提出使用结构化状态空间模型作为多实例学习器，用于高效建模和分类整个数字病理学切片图像中的组织斑块序列。 |
| [^85] | [An Empirical Evaluation of the Rashomon Effect in Explainable Machine Learning.](http://arxiv.org/abs/2306.15786) | 通过对不同数据集、模型和指标进行定量评估，我们发现罗生门效应对可解释机器学习具有影响，这为之前的轶事证据提供了实证支持，并展示了科学家和实践者面临的挑战。 |
| [^86] | [UTRNet: High-Resolution Urdu Text Recognition In Printed Documents.](http://arxiv.org/abs/2306.15782) | 本文提出了一种解决印刷乌尔都文本识别挑战的新方法，并引入了大规模实际标记数据集和合成数据集，提供了乌尔都文本行检测的基准数据集，同时开发了一个在线工具，实现了印刷文档中乌尔都OCR的端到端识别。 |
| [^87] | [Next Steps for Human-Centered Generative AI: A Technical Perspective.](http://arxiv.org/abs/2306.15774) | 这项研究从技术角度定义和提出了人类中心生成式人工智能(HGAI)的下一步工作，包括与人类价值观对齐、适应人类的意图表达和增强人类在协作工作流中的能力。这个工作的目标是吸引跨学科研究团队对HGAI的新兴想法进行讨论，并保持未来工作景观的整体连贯性。 |
| [^88] | [What Makes ImageNet Look Unlike LAION.](http://arxiv.org/abs/2306.15769) | 本研究通过重新搜索大规模的LAION数据集，尝试重新创建图像网，并发现与原始图像网相比，新建的LAIONet具有明显不同之处。这种差异的原因是，在基于图像描述进行搜索时，存在信息瓶颈，从而减轻了选择偏差。 |
| [^89] | [Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost.](http://arxiv.org/abs/2306.15766) | 本研究提出了一种利用大型语言模型进行标注的方法，以最小成本提升NLP模型的泛化能力。通过差异性采样策略，我们证明了这种方法在分类和排序任务上能够显著改善模型效果。 |
| [^90] | [High Fidelity Image Counterfactuals with Probabilistic Causal Models.](http://arxiv.org/abs/2306.15764) | 提出了一个通用的因果生成建模框架，用于准确估计具有高保真度的图像反事实推理，通过利用因果中介分析和生成建模的思想，设计了新的深度因果机制，实验证明了该方法的准确性。 |
| [^91] | [Predicting the Impact of Batch Refactoring Code Smells on Application Resource Consumption.](http://arxiv.org/abs/2306.15763) | 本论文研究了批量重构代码异味对应用资源消耗的影响，并设计了算法来预测代码重构对资源消耗的影响。通过对31个开源应用程序的16种代码异味类型的研究，得出了独立和批量重构特定代码异味后的CPU和内存利用率的变化。 |
| [^92] | [To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration.](http://arxiv.org/abs/2306.15749) | 神经形态计算旨在通过仿真脑部操作来提高深度学习模型的效率，但是在SNNs的高效硬件后端设计上仍需进一步研究。 |
| [^93] | [Ticketed Learning-Unlearning Schemes.](http://arxiv.org/abs/2306.15744) | 提出了一种新的票据化学习-遗忘模型，其中学习算法通过向每个参与训练示例发送额外信息并保留一部分中央信息，实现了学习和遗忘。我们提供了一种高效的票据化学习-遗忘方案，用于广泛的场景。 |
| [^94] | [Stochastic Gradient Bayesian Optimal Experimental Designs for Simulation-based Inference.](http://arxiv.org/abs/2306.15731) | 该研究通过建立比率模拟推断算法与随机梯度变分推断的重要连接，将贝叶斯最优实验设计 (BOED) 扩展到模拟推断应用中，实现了同时优化实验设计和摊还推断方法。 |
| [^95] | [Rethinking Closed-loop Training for Autonomous Driving.](http://arxiv.org/abs/2306.15713) | 这项研究提出了闭环训练对于自动驾驶的重要性，并分析了不同训练基准设计和流行RL算法的不足。为了解决这些问题，提出了一种新的强化学习驾驶代理TRAVL，通过多步预测和利用虚拟数据进行高效学习。 |
| [^96] | [Privacy-Preserving Community Detection for Locally Distributed Multiple Networks.](http://arxiv.org/abs/2306.15709) | 本文提出了一种保护隐私的本地分布多网络社区检测方法，利用隐私保护来进行共识社区检测和估计。采用随机响应机制对网络边进行扰动，通过隐私保护分布式谱聚类算法在扰动邻接矩阵上执行，以防止社区之间的抵消。同时，开发了两步偏差调整过程来消除扰动和网络矩阵带来的偏差。 |
| [^97] | [Quantum Federated Learning: Analysis, Design and Implementation Challenges.](http://arxiv.org/abs/2306.15708) | 该论文提供了关于量子联邦学习（QFL）的综述，包括QFL框架的设计思路、应用案例和关键因素的考虑，同时分析了各种QFL研究项目的技术贡献和局限性，并提出了未来的研究方向和待解决的问题。 |
| [^98] | [On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection.](http://arxiv.org/abs/2306.15705) | 本论文提出了一个用于高效的无数据对抗检测的通用对抗扰动方法，通过发现对抗样本与高维输入中的特定向量的关系，计算出通用对抗扰动（UAPs）。基于此，提出了一个无数据对抗检测框架，通过对UAPs对正常样本和对抗样本的反应产生不同的结果，从而实现了在各种文本分类任务上具有竞争力的检测性能。具体实验结果显示，该方法维持了与正常推断相等的时间消耗。 |
| [^99] | [Imitation with Spatial-Temporal Heatmap: 2nd Place Solution for NuPlan Challenge.](http://arxiv.org/abs/2306.15700) | 本文介绍了我们在NuPlan挑战赛中的第二名解决方案。我们的方法通过使用空间-时间热力图预测未来多模态状态，并使用轨迹优化技术实现安全的自主驾驶规划。 |
| [^100] | [Procedural content generation of puzzle games using conditional generative adversarial networks.](http://arxiv.org/abs/2306.15696) | 本文介绍了一种实验方法，使用参数化生成对抗网络为益智游戏Lily's Garden生成关卡。虽然GAN在逼近地图形状方面表现良好，但在逼近方块分布方面存在困难。可能通过尝试替代GAN的架构来改进这一情况。 |
| [^101] | [Joint Learning of Network Topology and Opinion Dynamics Based on Bandit Algorithms.](http://arxiv.org/abs/2306.15695) | 本文提出了一种基于多臂赌博算法的学习算法，用于联合学习网络拓扑和混合舆论动力学。通过改进网络和更新规则的初始估计，减少预测误差，并在数值实验中表现出了优于其他方法的性能。 |
| [^102] | [Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale.](http://arxiv.org/abs/2306.15687) | Voicebox是一种大规模的多语言通用语音生成模型，通过使用非自回归的流匹配模型，在文本和音频上下文条件下进行训练，可以实现零样本跨语言文本到语音合成，噪声去除，内容编辑，风格转换和多样化的样本生成等多种任务。 |
| [^103] | [The Challenge of Quickly Determining the Quality of a Single-Photon Source.](http://arxiv.org/abs/2306.15683) | 该研究通过使用数据增强技术，结合实验数据和自举样本，提出了一种快速评估单光子源质量的方法，并揭示了多光子发射事件概率的不确定性对质量评估的重要影响。 |
| [^104] | [ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram.](http://arxiv.org/abs/2306.15681) | ECG-QA是第一个专为心电图分析设计的问答数据集，包括涵盖广泛临床相关ECG主题的问题模板和多样化的ECG解读问题。这一资源将为未来的医疗保健问答研究提供有价值的见解。 |
| [^105] | [Generating Parametric BRDFs from Natural Language Descriptions.](http://arxiv.org/abs/2306.15679) | 这项研究开发了一个模型，可以根据描述性的文本提示生成参数化的BRDFs，为艺术性地创作3D环境提供了一种新的方法。 |
| [^106] | [KAPLA: Pragmatic Representation and Fast Solving of Scalable NN Accelerator Dataflow.](http://arxiv.org/abs/2306.15676) | 本文提出了KAPLA，一个用于可扩展NN加速器数据流优化的快速求解器。通过实用的指令和全面的数据流表示，KAPLA能够有效地进行设计空间的探索，并快速确定方案的有效性和效率。 |
| [^107] | [Asynchronous Algorithmic Alignment with Cocycles.](http://arxiv.org/abs/2306.15632) | 该论文提出了一种将节点状态更新和消息函数调用分离的数学框架，以实现异步计算，并以此作为基础，进行了异步算法和神经网络的对齐。 |
| [^108] | [Extending Context Window of Large Language Models via Positional Interpolation.](http://arxiv.org/abs/2306.15595) | 通过位置插值方法，我们可以在最小微调的情况下将RoPE-based预训练语言模型的上下文窗口扩展到最多32768，并在多个任务上获得强有力的实证结果。通过线性降低输入位置索引的大小，我们保持了扩展模型在原始上下文窗口内任务的质量。 |
| [^109] | [Geometric Ultrasound Localization Microscopy.](http://arxiv.org/abs/2306.15548) | 这项研究提出了一种基于几何框架的超声定位显微镜方法，通过仅依赖到达时间差信息实现微气泡的定位，并在精度和可靠性方面超越了现有的方法。 |
| [^110] | [MIMIC: Masked Image Modeling with Image Correspondences.](http://arxiv.org/abs/2306.15128) | MIMIC是一种基于图像对应关系的遮蔽图像建模方法，通过挖掘不需要任何注释的数据集，使用多个自监督模型进行训练，达到了在多个下游任务上优于使用注释挖掘的表示的效果。 |
| [^111] | [PhD Thesis: Exploring the role of (self-)attention in cognitive and computer vision architecture.](http://arxiv.org/abs/2306.14650) | 该论文研究了注意力和记忆在复杂推理任务中的作用，通过以Transformer为基础模型并结合记忆，扩展了自我注意力模型。研究结果表明，在视觉推理任务中，使用基于特征和空间注意力的自我注意力与ResNet50相结合可以高效解决具有挑战性的任务。此外，该论文提出了基于注意力和记忆的认知架构GAMR，它在样本效率、鲁棒性和组合性方面优于其他架构，并具有对新的推理任务的零样本泛化能力。 |
| [^112] | [The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs.](http://arxiv.org/abs/2306.14325) | 本文提出了一个神经符号模型，用于从语言输入中进行目标推断，并通过人类实验验证了该模型的准确性和优势。 |
| [^113] | [An Efficient Virtual Data Generation Method for Reducing Communication in Federated Learning.](http://arxiv.org/abs/2306.12088) | 本论文提出FedINIBoost，一种新的联邦学习方法，通过梯度匹配构建有效的提取模块，在减少通信开销的同时提高了模型准确率。 |
| [^114] | [G-NM: A Group of Numerical Time Series Prediction Models.](http://arxiv.org/abs/2306.11667) | G-NM是一组集合了传统和现代模型的数字时间序列预测模型，旨在提高对复杂自然现象中的模式和趋势的预测能力。 |
| [^115] | [Regularized Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy Gradient, and Sample Complexity.](http://arxiv.org/abs/2306.11626) | 本论文研究了正则化鲁棒MDP问题和风险敏感MDP问题的相关性，并提出了有效的学习算法和样本复杂度分析。 |
| [^116] | [A Universal Unbiased Method for Classification from Aggregate Observations.](http://arxiv.org/abs/2306.11343) | 这项研究提出了一种新的基于聚合观测的分类方法，通过为每个实例的每个标签权衡重要性，为分类器提供纯化的监督来学习，从而实现了任意损失的分类风险的无偏估计。 |
| [^117] | [Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network.](http://arxiv.org/abs/2306.10946) | 本文提出了一种基于注意力知识图卷积网络的旅游景点推荐模型，通过自动语义发掘目标景点的相邻实体，根据旅客的喜好选择，预测类似景点的概率，实验中取得良好效果。 |
| [^118] | [The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement.](http://arxiv.org/abs/2306.09633) | 谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。 |
| [^119] | [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting.](http://arxiv.org/abs/2306.09364) | TSMixer是一种用于多元时间序列预测的轻量级MLP-Mixer模型，可以有效地捕捉时间序列属性并在准确性方面超越了Transformers的方法。 |
| [^120] | [Self-supervised Equality Embedded Deep Lagrange Dual for Approximate Constrained Optimization.](http://arxiv.org/abs/2306.06674) | 该论文提出了一种自监督等式嵌入深度Lagrange对偶算法，用于解决不带标签的逼近限制优化问题。此方法通过在神经网络中嵌入等式约束来确保可行解，并使用原始-对偶方法进行训练，同时DeepLDE取得了最好的优化结果。 |
| [^121] | [Understanding the Effect of the Long Tail on Neural Network Compression.](http://arxiv.org/abs/2306.06238) | 本文研究了在神经网络压缩中如何保持与原始网络的语义相同，在长尾现象中探讨了压缩提高推广性能的记忆要素。 |
| [^122] | [Genomic Interpreter: A Hierarchical Genomic Deep Neural Network with 1D Shifted Window Transformer.](http://arxiv.org/abs/2306.05143) | 本文提出了一种名为“基因组解释器”的模型，可以对基因组数据进行预测并发现基因调控的层次依赖关系，性能优于现有模型。 |
| [^123] | [Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal.](http://arxiv.org/abs/2306.04502) | 本文提出了一种名为AGRA的自适应梯度异常值去除方法，能够在模型训练过程中动态调整数据集从而有效提高模型学习效果。 |
| [^124] | [Maximum Likelihood Training of Autoencoders.](http://arxiv.org/abs/2306.01843) | 本文介绍了一种成功的最大似然训练方法，用于非约束自编码器，将生成建模的优异性质与高效自编码器相结合。作者克服了两个挑战：设计了消除迭代的估计器并提出了稳定的最大似然训练目标。实验证明这种方法可以成功训练一系列非约束性自编码器，并取得了有竞争力的性能。 |
| [^125] | [Human-Aligned Calibration for AI-Assisted Decision Making.](http://arxiv.org/abs/2306.00074) | 本文通过引入一种基于主动询问决策者个人偏好的置信度构造方法，解决了现有置信度对于决策者信任决策的不准确问题，从而提高决策的准确性和效率。 |
| [^126] | [Low-rank extended Kalman filtering for online learning of neural networks from streaming data.](http://arxiv.org/abs/2305.19535) | 本文提出一种基于低秩扩展卡尔曼滤波的高效在线学习算法，其能够估计非线性函数的参数，具有更快的适应性和更快的奖励积累。 |
| [^127] | [Geometric Graph Filters and Neural Networks: Limit Properties and Discriminability Trade-offs.](http://arxiv.org/abs/2305.18467) | 本文研究了从流形采样点构造的图与流形神经网络的关系，并证明了这些图形上的卷积滤波器和神经网络收敛于连续流形上的卷积滤波器和神经网络。研究发现，图滤波器的可分性和近似流形滤波器所需行为的能力之间存在重要权衡，在神经网络中由于非线性的频率混合属性而得到改善。 |
| [^128] | [Just a Glimpse: Rethinking Temporal Information for Video Continual Learning.](http://arxiv.org/abs/2305.18418) | 本文提出了一种基于单个帧的新型重播机制SMILE，用于有效的视频连续学习。实验表明，在内存受到极端限制时，视频的多样性比时间信息更重要。 |
| [^129] | [Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective.](http://arxiv.org/abs/2305.15408) | 本文从理论层面探究了带有“思维链”提示的大型语言模型在解决基本数学和决策问题中的能力，发现自回归Transformer大小恒定即可解决任务，揭示了“思维链”提示的背后机制。 |
| [^130] | [Relabel Minimal Training Subset to Flip a Prediction.](http://arxiv.org/abs/2305.12809) | 本文利用扩展影响函数提出了一种有效的识别和重新标记最小训练子集的方法，并证明其始终能够成功翻转测试结果，同时还提供了挑战模型预测、评估模型鲁棒性和洞察训练集偏差等多重作用。 |
| [^131] | [Multi-task Hierarchical Adversarial Inverse Reinforcement Learning.](http://arxiv.org/abs/2305.12633) | 多任务分层对抗逆强化学习（MH-AIRL）通过分层结构和基本技能的可重复使用来提高多任务模仿学习的效率，适用于没有任务或技能注释的演示。 |
| [^132] | [Generalizing to new calorimeter geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation.](http://arxiv.org/abs/2305.11531) | 基于几何感知的自回归模型能够学习电磁量计响应如何随几何形状变化，能够快速有效地模拟非环形的电磁量计。 |
| [^133] | [Segmentation of fundus vascular images based on a dual-attention mechanism.](http://arxiv.org/abs/2305.03617) | 本文提出了基于双重注意机制的眼底血管图像分割方法，可以从空间和通道维度提取图像信息，并通过引入空间注意机制和Dropout层解决光照变化和不均匀对比度等问题，实验结果表明，该方法可以产生令人满意的结果。 |
| [^134] | [Statistical Optimality of Deep Wide Neural Networks.](http://arxiv.org/abs/2305.02657) | 本文研究了深度宽松弛ReLU神经网络的泛化能力，证明适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中，但过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。 |
| [^135] | [Deep Learning assisted microwave-plasma interaction based technique for plasma density estimation.](http://arxiv.org/abs/2304.14807) | 本文提出了一种基于深度学习辅助微波等离子体相互作用技术的等离子体密度估计方法，通过测量微波散射引起的电场模式来估计密度剖面。 |
| [^136] | [A Cookbook of Self-Supervised Learning.](http://arxiv.org/abs/2304.12210) | 这篇论文提出了一本自监督学习的食谱，旨在降低自监督学习研究的门槛，并使研究人员能够了解各种选择和参数在自监督学习中的作用。 |
| [^137] | [Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification.](http://arxiv.org/abs/2304.02836) | 本文提出了一种新的肺部结节分类方法，使用变压器模型整合了EHR中的成像和临床特征。 |
| [^138] | [Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models.](http://arxiv.org/abs/2304.01046) | 本文研究了一种训练阅读理解和逻辑推理模型的反向方法，利用相对准确性的策略来训练模型，通过Polytuplet Loss函数来确保优先学习答案选择的相对正确性，获得了不错的成果，提出了具有一般性的训练方法和模型架构。 |
| [^139] | [Pgx: Hardware-accelerated parallel game simulation for reinforcement learning.](http://arxiv.org/abs/2303.17503) | Pgx是一个用JAX编写的游戏模拟器集合，具有强化学习硬件加速能力，支持并行执行，速度比现有的强化学习库快10倍。 它实现了Backgammon，Shogi和Go等基准测试游戏。 |
| [^140] | [PeakNet: An Autonomous Bragg Peak Finder with Deep Neural Networks.](http://arxiv.org/abs/2303.15301) | PeakNet是一个利用深度神经网络的自动Bragg峰点寻找器，它通过实时调整来适应逐发强背景散射的波动，消除了手动调整算法参数的需求，减少了误报峰点的数量。 |
| [^141] | [Preserving Linear Separability in Continual Learning by Backward Feature Projection.](http://arxiv.org/abs/2303.14595) | 提出了一种名为BFP的连续学习方法，它通过将新特征变换为旧特征的线性变换来维护线性可分性，从而允许新特征方向的出现以适应新任务，同时保留旧任务的信息。 |
| [^142] | [Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis.](http://arxiv.org/abs/2303.13391) | Xplainer是一个透明且可解释的零样本诊断新框架，通过对存在的描述性观察进行分类来提高自动诊断集成到临床工作流程中的效率，同时避免需要大量注释数据的问题。 |
| [^143] | [Linking generative semi-supervised learning and generative open-set recognition.](http://arxiv.org/abs/2303.11702) | 本研究旨在探究生成半监督学习和生成开放集识别之间的关系。SSL-GANs和OSR-GANs方法的相似性在于都要求生成器在互补空间中产生样本，并通过正则化来推广开放空间。研究结果表明SSL优化边缘-GAN在结合SSL-OSR任务方面树立新的标准，但在某些OSR任务中OSR优化的ARP-GAN仍然略优于SSL-GAN。 |
| [^144] | [PyVBMC: Efficient Bayesian inference in Python.](http://arxiv.org/abs/2303.09519) | PyVBMC是一种高效的Python工具，用于黑盒计算模型的贝叶斯推断和模型选择，可以处理连续参数不超过约10-15个的计算或统计模型。 |
| [^145] | [A DeepONet multi-fidelity approach for residual learning in reduced order modeling.](http://arxiv.org/abs/2302.12682) | 本文提出了一种利用DeepONets的多信度方法来提高降阶模型的精度。通过将模型降阶与机器学习的残差学习相结合，可以学习并推断新预测的误差。该框架最大化利用高信度信息，用于构建降阶模型和学习残差。实验结果证明了该方法的有效性。 |
| [^146] | [Extensible Motion-based Identification of XR Users using Non-Specific Motion Data.](http://arxiv.org/abs/2302.07517) | 提出了一种可扩展XR用户基于运动的识别方法。与现有基线方法相比，该方法通过仅使用少量的注册数据来识别新用户，可以在几秒钟内注册新用户，而且在仅有少量注册数据可用时也更可靠。 |
| [^147] | [Performance Bounds for Policy-Based Average Reward Reinforcement Learning Algorithms.](http://arxiv.org/abs/2302.01450) | 本文通过获得平均回报MDPs的有限时间误差界限解决了近似策略迭代和强化学习算法在平均回报设置中的性能界限问题 |
| [^148] | [Towards fully covariant machine learning.](http://arxiv.org/abs/2301.13724) | 本文探讨了机器学习中多个被动对称性的影响，并提出了关于机器学习实践中尊重被动对称性的 dos and don'ts。此外，还讨论了被动对称性与因果建模的关系，并指出在学习问题的目标是样本外推广时，实现被动对称性尤其有价值。 |
| [^149] | [Generalization on the Unseen, Logic Reasoning and Degree Curriculum.](http://arxiv.org/abs/2301.13105) | 本文研究了在逻辑推理任务中对未知数据的泛化能力，提供了网络架构在该设置下的表现证据，发现了一类网络模型在未知数据上学习了最小度插值器，并对长度普通化现象提供了解释。 |
| [^150] | [Reef-insight: A framework for reef habitat mapping with clustering methods via remote sensing.](http://arxiv.org/abs/2301.10876) | Reef-Insight是一种利用聚类方法和遥感技术进行珊瑚礁栖息地映射的无监督机器学习框架，通过比较不同的聚类方法，我们发现遥感数据可以有效地用于珊瑚礁栖息地的映射。 |
| [^151] | [Solar Coronal Hole Analysis and Prediction using Computer Vision and LSTM Neural Network.](http://arxiv.org/abs/2301.06732) | 本研究使用计算机视觉和LSTM神经网络分析和预测太阳冕空洞的大小和趋势，并据此为地球对太阳冕空洞的影响做准备。 |
| [^152] | [Shorter Latency of Real-time Epileptic Seizure Detection via Probabilistic Prediction.](http://arxiv.org/abs/2301.03465) | 通过引入概率预测，将癫痫发作检测任务转化为交叉期样本的标注规则，并使用深度学习框架进行特征提取和预测概率增强，并应用累积决策规则，成功缩短了癫痫发作检测的延迟。 |
| [^153] | [Deep R Programming.](http://arxiv.org/abs/2301.01188) | 该课程介绍了流行的数据科学语言R，并旨在培养学生、从业者和研究者成为独立的R语言用户。 |
| [^154] | [Localising In-Domain Adaptation of Transformer-Based Biomedical Language Models.](http://arxiv.org/abs/2212.10422) | 本研究针对生物医学领域内自适应问题，探讨了两种途径来在非英语语言中产生生物医学语言模型。一种是通过神经机器翻译将英文资源翻译为目标语言，注重数量；另一种是直接基于高质量、狭谱的语料库进行本地化。这些方法有助于解决资源较少语言如意大利语的领域内适应问题。 |
| [^155] | [Support Vector Regression: Risk Quadrangle Framework.](http://arxiv.org/abs/2212.09178) | 本文结合风险四方理论，研究了支持向量回归（SVR）。研究结果发现，SVR的两种形式对应于等效误差度量的最小化，同时加上正则化惩罚项。通过构造基本风险四方框，我们证明了SVR是对两个对称条件分位数的平均数的渐近无偏估计量。此外，我们证明了$\varepsilon$-SVR和$\nu$-SVR在一般随机环境下的等价性。 |
| [^156] | [Differentiable User Models.](http://arxiv.org/abs/2211.16277) | 该研究提出了可微分的用户模型，通过引入可广泛应用的可微分替代品解决了现代先进用户模型与机器学习流程的不兼容性和计算代价过高的问题。实验证明，在线应用中可以实现与现有无似然推理方法相当的建模能力，并展示了在菜单搜索任务中如何利用认知模型进行在线交互。 |
| [^157] | [Sequential Gradient Coding For Straggler Mitigation.](http://arxiv.org/abs/2211.13802) | 本文介绍了两种方案，结合了梯度编码和选择性重复任务，实现了更好的拖车减轻性能。 |
| [^158] | [QueryForm: A Simple Zero-shot Form Entity Query Framework.](http://arxiv.org/abs/2211.07730) | QueryForm是一种简单的零样本表单实体查询框架，通过使用双重提示机制和利用大规模查询-实体对进行预训练，能够从结构化文档中提取实体值，无需目标特定的训练数据，达到了新的最先进技术水平。 |
| [^159] | [Generalization of generative model for neuronal ensemble inference method.](http://arxiv.org/abs/2211.05634) | 提出了基于泛化的神经元集合推理方法，解决了贝叶斯推断模型中神经元活动非平稳性的问题。 |
| [^160] | [Near-optimal multiple testing in Bayesian linear models with finite-sample FDR control.](http://arxiv.org/abs/2211.02778) | 本文针对高维贝叶斯线性模型的多重检验问题，开发了近似最优的多重检验程序，能够在有限样本下控制频率FDR，并能达到近似最优功率。 |
| [^161] | [The Numerical Stability of Hyperbolic Representation Learning.](http://arxiv.org/abs/2211.00181) | 本文研究了超几何表征学习中的数值不稳定性问题，比较了两种流行的超几何模型Poincar\'e球和Lorentz模型，发现Lorentz模型具有更好的数值稳定性和优化性能，同时提出一种新的欧几里得优化方案作为超几何学习的另一个选择。 |
| [^162] | [Cross-client Label Propagation for Transductive and Semi-Supervised Federated Learning.](http://arxiv.org/abs/2210.06434) | 跨客户标签传播（XCLP）是一种用于跨设备和半监督联邦学习的新方法，在联邦学习中展示了更高的分类准确率。 |
| [^163] | [Incorporating Prior Knowledge into Neural Networks through an Implicit Composite Kernel.](http://arxiv.org/abs/2205.07384) | 本论文提出了一种通过深度学习和高斯过程的复合核来将先验知识融入神经网络的方法。通过隐式定义的神经网络核函数和选择的第二个核函数，可以模拟已知特性，并提高深度学习应用的性能。 |
| [^164] | [Probabilistic AutoRegressive Neural Networks for Accurate Long-range Forecasting.](http://arxiv.org/abs/2204.09640) | PARNN是一种概率自回归神经网络模型，能够准确预测具有非平稳性、非线性、非周期性、长期依赖和混沌模式的复杂时间序列数据，并通过预测区间提供不确定性量化。 |
| [^165] | [PyDTS: A Python Package for Discrete-Time Survival (Regularized) Regression with Competing Risks.](http://arxiv.org/abs/2204.05731) | PyDTS是一个用于离散时间生存数据半参数竞争风险模型的Python包，支持包括LASSO和弹性网等正则化回归方法。 |
| [^166] | [Test-time Adaptation with Slot-Centric Models.](http://arxiv.org/abs/2203.11194) | 本论文提出了一种以插槽为中心的模型，用于解析分布外的场景，并通过测试时自适应来提高模型性能。通过结合自监督损失和建模偏差，该模型在场景分解任务上取得了良好的效果。 |
| [^167] | [SUPERNOVA: Automating Test Selection and Defect Prevention in AAA Video Games Using Risk Based Testing and Machine Learning.](http://arxiv.org/abs/2203.05566) | SUPERNOVA是一个负责自动化测试选择和缺陷预防的系统，它使用基于风险的测试和机器学习方法来解决视频游戏测试中的挑战。 |
| [^168] | [CoCoFL: Communication- and Computation-Aware Federated Learning via Partial NN Freezing and Quantization.](http://arxiv.org/abs/2203.05468) | CoCoFL是一种基于神经网络冻结和量化的通信和计算感知的联邦学习技术，可以适应设备的异构资源，并提高准确度。 |
| [^169] | [High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning.](http://arxiv.org/abs/2203.01311) | 本文研究了高模态场景下的高效表示学习，提出了两种新的信息论度量方法来量化模态和交互的异质性，以加速对多样化和少被研究的模态的推广。 (arXiv:2203.01311v4 [cs.LG] UPDATED) |
| [^170] | [Leveraging Trust for Joint Multi-Objective and Multi-Fidelity Optimization.](http://arxiv.org/abs/2112.13901) | 这篇论文研究了一种利用信任度量来支持多目标和多数据来源优化的方法。通过将信任增益作为目标之一，将多保真度考虑在内，实现了同时进行多目标和多数据来源优化的目标。比较了两种优化方法：同时选择输入参数和保真度标准的整体方法以及基于帕累托优化的方法。 |
| [^171] | [Improving Differentially Private SGD via Randomly Sparsified Gradients.](http://arxiv.org/abs/2112.00845) | 通过在差分隐私随机梯度下降法中对梯度进行随机稀疏化，我们找到了一个调整收敛界的方法，从而在噪声占主导地位时获得更小的上界。这个观察表明，差分隐私随机梯度下降法有特殊的梯度压缩潜力。 |
| [^172] | [Exponential Lower Bounds for Threshold Circuits of Sub-Linear Depth and Energy.](http://arxiv.org/abs/2107.00223) | 本文研究阈值电路和其他神经网络理论模型的计算能力。主要结果证明，任何阈值电路$C$的大小、深度、能量和权重满足$\log(rk(M_C)) \le ed (\log s + \log w + \log n)$。这表明，即使是子线性深度的阈值电路的大小也存在指数下界。 |
| [^173] | [GeoT: A Geometry-aware Transformer for Reliable Molecular Property Prediction and Chemically Interpretable Representation Learning.](http://arxiv.org/abs/2106.15516) | GeoT是一种几何感知的Transformer模型，用于可靠的分子属性预测和化学可解释表示学习。它通过注意力机制学习分子图结构，并生成与训练目标相关联的注意力图。与基于MPNN模型相比，GeoT具有可比的性能，同时减少了计算复杂性。 |
| [^174] | [Open-set learning with augmented category by exploiting unlabeled data (Open-LACU).](http://arxiv.org/abs/2002.01368) | Open-LACU是一种新的开放式学习策略，它可以将分类器推广到观察到的和未观察到的新颖类别之间，并通过定义不同的背景和未知类别来提高训练成本效益性，确保在存在未观察到的新颖类别时进行安全分类。 |
| [^175] | [Understanding Graph Neural Networks with Asymmetric Geometric Scattering Transforms.](http://arxiv.org/abs/1911.06253) | 这项工作介绍了一种具有非对称几何散射变换的图神经网络，通过引入一类非对称小波，它统一和扩展了现有图形散射架构的理论结果，并为未来的深度学习架构为图形提供了基础。 |
| [^176] | [Understanding a Version of Multivariate Symmetric Uncertainty to assist in Feature Selection.](http://arxiv.org/abs/1709.08730) | 通过分析多变量对称不确定性度量在特征选择中的作用，我们发现了一种条件，能够在属性数量、基数和样本大小不同组合下保持这一度量的良好质量，为降维过程提供了有用的准则。 |

# 详细

[^1]: 关于数据中毒攻击的聚合防御的实践方面

    On Practical Aspects of Aggregation Defenses against Data Poisoning Attacks. (arXiv:2306.16415v1 [cs.LG])

    [http://arxiv.org/abs/2306.16415](http://arxiv.org/abs/2306.16415)

    本文研究了数据中毒攻击的聚合防御策略的实践方面，并针对Deep Partition Aggregation进行了评估，包括效率、性能和鲁棒性。实验结果显示，基于缩放基础模型的方法能够提高聚合防御的训练效率。

    

    对于深度学习来说，数据的增加不仅带来机会，也带来风险，因为恶意训练样本可以操纵深度学习模型的行为。这种攻击被称为数据中毒。近期对抗数据中毒的防御策略的进展突出了聚合方案在实现认证中毒鲁棒性方面的有效性。然而，这些方法的实践影响仍不清楚。本文重点研究了Deep Partition Aggregation，一种代表性的聚合防御，并评估了其实际方面，包括效率、性能和鲁棒性。为了评估，我们使用了被调整到64×64分辨率的ImageNet数据集，以便在比以前更大的规模上进行评估。首先，我们展示了一种简单且实用的基于缩放基础模型的方法，它改善了聚合防御的训练和推理效率。其次，我们提供了支持数据剖分的实证证据。

    The increasing access to data poses both opportunities and risks in deep learning, as one can manipulate the behaviors of deep learning models with malicious training samples. Such attacks are known as data poisoning. Recent advances in defense strategies against data poisoning have highlighted the effectiveness of aggregation schemes in achieving state-of-the-art results in certified poisoning robustness. However, the practical implications of these approaches remain unclear. Here we focus on Deep Partition Aggregation, a representative aggregation defense, and assess its practical aspects, including efficiency, performance, and robustness. For evaluations, we use ImageNet resized to a resolution of 64 by 64 to enable evaluations at a larger scale than previous ones. Firstly, we demonstrate a simple yet practical approach to scaling base models, which improves the efficiency of training and inference for aggregation defenses. Secondly, we provide empirical evidence supporting the data
    
[^2]: MultiZoo & MultiBench: 用于多模态深度学习的标准化工具包

    MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning. (arXiv:2306.16413v1 [cs.LG])

    [http://arxiv.org/abs/2306.16413](http://arxiv.org/abs/2306.16413)

    MultiZoo和MultiBench是用于多模态深度学习的标准化工具包，提供了多模态算法的实现和大规模基准测试，以促进对多模态模型能力和限制的理解，并确保易用性和可重复性。

    

    学习多模态表示涉及整合来自多种异构数据源的信息。为了加快对少研究的模态和任务的进展，同时确保现实世界的稳健性，我们发布了MultiZoo，一个公共工具包，其中包含> 20个核心多模态算法的标准化实现，以及MultiBench，一个涵盖15个数据集，10个模态，20个预测任务和6个研究领域的大规模基准测试。这些提供了一个自动化的端到端机器学习流水线，简化和标准化数据加载、实验设置和模型评估。为了实现全面评估，我们提供了一套全面的方法来评估（1）泛化能力，（2）时间和空间复杂度，和（3）模态鲁棒性。MultiBench为更好地了解多模态模型的功能和限制铺平了道路，同时确保易于使用、可访问性和可重复性。我们的工具包是公开可用的。

    Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of > 20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, wi
    
[^3]: 平均奖励马尔可夫决策过程的更精确的无模型强化学习

    Sharper Model-free Reinforcement Learning for Average-reward Markov Decision Processes. (arXiv:2306.16394v1 [cs.LG])

    [http://arxiv.org/abs/2306.16394](http://arxiv.org/abs/2306.16394)

    本论文提出了针对无限周期平均奖励马尔可夫决策过程（MDPs）的多个高效无模型强化学习（RL）算法，包括在线设置和仿真器设置，实现了最佳的遗憾和样本使用依赖性。

    

    我们提出了几种经过验证高效的无模型强化学习算法，用于无限周期平均奖励马尔可夫决策过程（MDPs）。我们考虑在线设置和拥有仿真器的设置。在在线设置中，我们提出了基于参考优势分解的无模型强化学习算法。我们的算法在T步之后达到 $\widetilde{O}(S^5A^2\mathrm{sp}(h^*)\sqrt{T})$ 的遗憾，其中 $S\times A$ 是状态-动作空间的大小， $\mathrm{sp}(h^*)$ 是最优偏差函数的跨度。我们的结果是第一个在弱通信MDPs中达到T的最佳依赖性的。在仿真器设置中，我们提出了一个无模型强化学习算法，使用 $\widetilde{O} \left(\frac{SA\mathrm{sp}^2(h^*)}{\epsilon^2}+\frac{S^2A\mathrm{sp}(h^*)}{\epsilon} \right)$ 个样本找到一个 $\epsilon$-最优策略，而极小化下界是 $\Omega\left(\frac{SA\mathrm{sp}(h^*)}{\epsilon^2}\right)$。我们的结果基于两个新的技术

    We develop several provably efficient model-free reinforcement learning (RL) algorithms for infinite-horizon average-reward Markov Decision Processes (MDPs). We consider both online setting and the setting with access to a simulator. In the online setting, we propose model-free RL algorithms based on reference-advantage decomposition. Our algorithm achieves $\widetilde{O}(S^5A^2\mathrm{sp}(h^*)\sqrt{T})$ regret after $T$ steps, where $S\times A$ is the size of state-action space, and  $\mathrm{sp}(h^*)$ the span of the optimal bias function. Our results are the first to achieve optimal dependence in $T$ for weakly communicating MDPs.  In the simulator setting, we propose a model-free RL algorithm that finds an $\epsilon$-optimal policy using $\widetilde{O} \left(\frac{SA\mathrm{sp}^2(h^*)}{\epsilon^2}+\frac{S^2A\mathrm{sp}(h^*)}{\epsilon} \right)$ samples, whereas the minimax lower bound is $\Omega\left(\frac{SA\mathrm{sp}(h^*)}{\epsilon^2}\right)$.  Our results are based on two new te
    
[^4]: 加速GNN框架中的采样和聚合操作：利用GPU发起直接存储访问

    Accelerating Sampling and Aggregation Operations in GNN Frameworks with GPU Initiated Direct Storage Accesses. (arXiv:2306.16384v1 [cs.DC])

    [http://arxiv.org/abs/2306.16384](http://arxiv.org/abs/2306.16384)

    本论文提出了一种通过利用GPU发起直接存储访问来加速GNN框架中的采样和聚合操作的方法，解决了在训练大规模图上时CPU无法充分利用GPU资源的问题。

    

    图神经网络（GNNs）正在成为学习图结构数据和进行复杂推理任务的一个强大工具，适用于各个应用领域。尽管已经证明GNNs在中等规模的图上具有有效性，但在大规模图上训练仍然面临着数据访问和数据移动方法的不足。现有的GNN训练框架使用CPU进行图采样和特征聚合，而模型权重的训练和更新则由GPU执行。然而，我们深入分析发现CPU无法实现所需的吞吐量以充分利用昂贵的GPU资源。此外，当图和其嵌入不能适应CPU内存时，操作系统引入的开销，如处理页面错误，会成为关键路径的瓶颈。为了解决这些问题，我们提出了GPU发起的直接存储访问方法。

    Graph Neural Networks (GNNs) are emerging as a powerful tool for learning from graph-structured data and performing sophisticated inference tasks in various application domains. Although GNNs have been shown to be effective on modest-sized graphs, training them on large-scale graphs remains a significant challenge due to lack of efficient data access and data movement methods. Existing frameworks for training GNNs use CPUs for graph sampling and feature aggregation, while the training and updating of model weights are executed on GPUs. However, our in-depth profiling shows the CPUs cannot achieve the throughput required to saturate GNN model training throughput, causing gross under-utilization of expensive GPU resources. Furthermore, when the graph and its embeddings do not fit in the CPU memory, the overhead introduced by the operating system, say for handling page-faults, comes in the critical path of execution.  To address these issues, we propose the GPU Initiated Direct Storage Ac
    
[^5]: 多站点临床联邦学习使用递归和注意力模型以及NVFlare

    Multi-Site Clinical Federated Learning using Recursive and Attentive Models and NVFlare. (arXiv:2306.16367v1 [cs.LG])

    [http://arxiv.org/abs/2306.16367](http://arxiv.org/abs/2306.16367)

    本论文提出了一种多站点临床联邦学习的方法，使用递归和注意力模型以及NVFlare框架。研究引入了两种示例性的自然语言处理模型，LSTM模型和BERT模型，这些模型在理解上下文和语义方面表现出了优异的性能。

    

    数字健康数据的快速增长引发了对利用机器学习方法（如自然语言处理）来审查医疗记录、临床笔记和其他基于文本的健康信息的兴趣。虽然自然语言处理技术在增强患者护理和决策制定方面展示出了巨大的潜力，但数据隐私和遵守法规仍然是关键问题。联邦学习成为一种可行的解决方案，使多个组织能够在不传播原始数据的情况下共同训练机器学习模型。本文通过将联邦学习、自然语言处理模型和由NVIDIA开发的NVFlare框架相结合，提出了一种实用的医疗自然语言处理方法。我们介绍了两个示例性的自然语言处理模型：基于长短期记忆（LSTM）的模型和双向编码器表示转换（BERT），它们在理解上下文和语义方面表现出 exceptional 性能。

    The prodigious growth of digital health data has precipitated a mounting interest in harnessing machine learning methodologies, such as natural language processing (NLP), to scrutinize medical records, clinical notes, and other text-based health information. Although NLP techniques have exhibited substantial potential in augmenting patient care and informing clinical decision-making, data privacy and adherence to regulations persist as critical concerns. Federated learning (FL) emerges as a viable solution, empowering multiple organizations to train machine learning models collaboratively without disseminating raw data. This paper proffers a pragmatic approach to medical NLP by amalgamating FL, NLP models, and the NVFlare framework, developed by NVIDIA. We introduce two exemplary NLP models, the Long-Short Term Memory (LSTM)-based model and Bidirectional Encoder Representations from Transformers (BERT), which have demonstrated exceptional performance in comprehending context and semant
    
[^6]: 超越NTK与范式梯度下降：对具有多项式宽度、样本和时间的神经网络的均场分析

    Beyond NTK with Vanilla Gradient Descent: A Mean-Field Analysis of Neural Networks with Polynomial Width, Samples, and Time. (arXiv:2306.16361v1 [cs.LG])

    [http://arxiv.org/abs/2306.16361](http://arxiv.org/abs/2306.16361)

    本文提供了对多项式宽度两层神经网络上的投影梯度流的均场分析，证明了原始梯度下降与NTK之间的明显差异，即在样本复杂度方面原始梯度下降可以比核方法实现更高的性能。

    

    尽管近年来在非凸优化的两层神经网络上取得了理论上的进展，但关于神经网络上的梯度下降是否可以比核方法实现更高的样本复杂度仍然是一个未解决的问题。本文提供了对多项式宽度两层神经网络上的投影梯度流的清晰均场分析。与之前的工作不同，我们的分析不需要对优化算法进行不自然的修改。我们证明，当样本大小$n = O(d^{3.1})$，其中$d$是输入的维度，网络在多项式次迭代中收敛到一个非平凡的错误，这个错误无法通过使用$n \ll d^4$个样本的核方法实现，因此清楚地证明了原始梯度下降和NTK之间的明显差异。

    Despite recent theoretical progress on the non-convex optimization of two-layer neural networks, it is still an open question whether gradient descent on neural networks without unnatural modifications can achieve better sample complexity than kernel methods. This paper provides a clean mean-field analysis of projected gradient flow on polynomial-width two-layer neural networks. Different from prior works, our analysis does not require unnatural modifications of the optimization algorithm. We prove that with sample size $n = O(d^{3.1})$ where $d$ is the dimension of the inputs, the network converges in polynomially many iterations to a non-trivial error that is not achievable by kernel methods using $n \ll d^4$ samples, hence demonstrating a clear separation between unmodified gradient descent and NTK.
    
[^7]: cuSLINK: GPU上的单链接聚类算法

    cuSLINK: Single-linkage Agglomerative Clustering on the GPU. (arXiv:2306.16354v1 [cs.LG])

    [http://arxiv.org/abs/2306.16354](http://arxiv.org/abs/2306.16354)

    cuSLINK是一种在GPU上实现的单链接聚类算法，具有较低的空间复杂度和可重复使用的构建模块，适用于广泛的数据挖掘和机器学习应用。

    

    在本文中，我们提出了cuSLINK，该算法是对GPU上的SLINK算法的一种新型和最先进的改进，它只需要$O(Nk)$的空间，并使用参数$k$来权衡空间和时间。我们还提出了一组新颖且可重复使用的构建模块，这些构建模块包括针对$k$-NN图构建、生成树和树状图聚类的高度优化的计算模式。我们展示了如何使用这些基本模块在GPU上全面实现cuSLINK，进一步使得原本难以处理的广泛的现实世界的数据挖掘和机器学习应用成为可能。除了在流行的HDBSCAN算法中是主要的计算瓶颈之外，我们的cuSLINK算法的全面影响还涵盖了一系列重要的应用，包括社交网络和计算机网络中的聚类分析、自然语言处理和计算机视觉。用户可以在https://docs.rapids.ai/api/cuml/latest/api/#agg获取cuSLINK。

    In this paper, we propose cuSLINK, a novel and state-of-the-art reformulation of the SLINK algorithm on the GPU which requires only $O(Nk)$ space and uses a parameter $k$ to trade off space and time. We also propose a set of novel and reusable building blocks that compose cuSLINK. These building blocks include highly optimized computational patterns for $k$-NN graph construction, spanning trees, and dendrogram cluster extraction. We show how we used our primitives to implement cuSLINK end-to-end on the GPU, further enabling a wide range of real-world data mining and machine learning applications that were once intractable. In addition to being a primary computational bottleneck in the popular HDBSCAN algorithm, the impact of our end-to-end cuSLINK algorithm spans a large range of important applications, including cluster analysis in social and computer networks, natural language processing, and computer vision. Users can obtain cuSLINK at https://docs.rapids.ai/api/cuml/latest/api/#agg
    
[^8]: 学习具有随机分类噪声的边界半空间的信息-计算权衡

    Information-Computation Tradeoffs for Learning Margin Halfspaces with Random Classification Noise. (arXiv:2306.16352v1 [cs.LG])

    [http://arxiv.org/abs/2306.16352](http://arxiv.org/abs/2306.16352)

    本文研究了学习具有随机分类噪声的边界半空间的信息-计算权衡问题，发现了样本复杂性和计算效率算法之间的固有差距，并给出了相应的样本复杂性下界。

    

    我们研究了使用随机分类噪声学习γ-边界半空间的问题。我们建立了一个信息-计算权衡，表明了问题的样本复杂性与计算效率算法的样本复杂性之间存在固有差距。具体而言，问题的样本复杂性为Θ(1/ (γ^2 ε))。我们首先给出了一个简单高效的算法，其样本复杂性为O(1/ (γ^2 ε^2))。我们的主要结果是统计查询（SQ）算法和低次多项式测试的下界，这表明在计算效率算法中，样本复杂性对1/ε的二次依赖是固有的。具体而言，我们的结果暗示了任何有效的SQ学习器或低次测试的样本复杂性的下界为Ω(1/ (γ^(1/2) ε^2))。

    We study the problem of PAC learning $\gamma$-margin halfspaces with Random Classification Noise. We establish an information-computation tradeoff suggesting an inherent gap between the sample complexity of the problem and the sample complexity of computationally efficient algorithms. Concretely, the sample complexity of the problem is $\widetilde{\Theta}(1/(\gamma^2 \epsilon))$. We start by giving a simple efficient algorithm with sample complexity $\widetilde{O}(1/(\gamma^2 \epsilon^2))$. Our main result is a lower bound for Statistical Query (SQ) algorithms and low-degree polynomial tests suggesting that the quadratic dependence on $1/\epsilon$ in the sample complexity is inherent for computationally efficient algorithms. Specifically, our results imply a lower bound of $\widetilde{\Omega}(1/(\gamma^{1/2} \epsilon^2))$ on the sample complexity of any efficient SQ learner or low-degree test.
    
[^9]: 使用流形上的自回归模型（mNARX）模拟复杂系统的动力学

    Emulating the dynamics of complex systems using autoregressive models on manifolds (mNARX). (arXiv:2306.16335v1 [stat.CO])

    [http://arxiv.org/abs/2306.16335](http://arxiv.org/abs/2306.16335)

    本研究提出了一种名为mNARX的方法，通过构建流形和自回归模型，以高效准确地近似复杂系统的动力学响应。这种方法能够将整个问题分解成较小的子问题，具有良好的可扩展性，并且与传统的降维技术相结合，适用于建模复杂系统。

    

    在这项研究中，我们提出了一种新颖的替代建模方法，可以高效准确地近似复杂动态系统对随时间变化的外部刺激的响应，并且可以延长时间段。我们的方法被称为“带外部输入的流形非线性自回归建模”（mNARX），它涉及构建一个问题特定的外部输入流形，以便构造自回归替代模型。这个流形是mNARX的核心，通过结合系统的物理性质以及先前的专家和领域知识来逐步构建。因为mNARX将整个问题分解成一系列更小的子问题，每个子问题的复杂度比原来的低，所以它在问题的复杂性方面具有良好的可扩展性，无论是最终替代模型的训练成本还是评估成本。此外，mNARX与传统的降维技术非常契合，使其非常适合建模。

    In this study, we propose a novel surrogate modelling approach to efficiently and accurately approximate the response of complex dynamical systems driven by time-varying exogenous excitations over extended time periods. Our approach, that we name \emph{manifold nonlinear autoregressive modelling with exogenous input} (mNARX), involves constructing a problem-specific exogenous input manifold that is optimal for constructing autoregressive surrogates. The manifold, which forms the core of mNARX, is constructed incrementally by incorporating the physics of the system, as well as prior expert- and domainknowledge. Because mNARX decomposes the full problem into a series of smaller sub-problems, each with a lower complexity than the original, it scales well with the complexity of the problem, both in terms of training and evaluation costs of the final surrogate. Furthermore, mNARX synergizes well with traditional dimensionality reduction techniques, making it highly suitable for modelling 
    
[^10]: 通过密度标志检测来识别离散化潜在坐标系统的可识别性

    Identifiability of Discretized Latent Coordinate Systems via Density Landmarks Detection. (arXiv:2306.16334v1 [cs.LG])

    [http://arxiv.org/abs/2306.16334](http://arxiv.org/abs/2306.16334)

    本文提出了一种新颖的可识别性形式，称为量化坐标可识别性。在无监督的情况下，我们展示了在高度通用的非线性映射下，可以恢复离散化的潜在坐标，而无需额外的归纳偏差。这一发现对解缠研究具有重要意义。

    

    解缠旨在仅从观察到的分布中恢复有意义的潜在真实因素。 可识别性为解缠提供了理论基础。 不幸的是，在自适应独立潜变量因子的情况下，在一般的非线性光滑因子到观测的映射下，无监督的可识别性在i.i.d.设置下是理论上不可能的。 在这项工作中，我们展示了非常惊人的是，在高度通用的非线性光滑映射（一个微分同胚）下，可以恢复离散化的潜在坐标，而不需要对映射进行任何额外的归纳偏差。 这是在假设潜在密度具有轴对齐的不连续标志的情况下，但不做因素的统计独立的不现实的假设。 我们引入了这种新颖的可识别性形式，称为量化坐标可识别性，并对恢复离散坐标进行了全面的证明。

    Disentanglement aims to recover meaningful latent ground-truth factors from only the observed distribution. Identifiability provides the theoretical grounding for disentanglement to be well-founded. Unfortunately, unsupervised identifiability of independent latent factors is a theoretically proven impossibility in the i.i.d. setting under a general nonlinear smooth map from factors to observations. In this work, we show that, remarkably, it is possible to recover discretized latent coordinates under a highly generic nonlinear smooth mapping (a diffeomorphism) without any additional inductive bias on the mapping. This is, assuming that latent density has axis-aligned discontinuity landmarks, but without making the unrealistic assumption of statistical independence of the factors. We introduce this novel form of identifiability, termed quantized coordinate identifiability, and provide a comprehensive proof of the recovery of discretized coordinates.
    
[^11]: 通过变分贝叶斯网络实现表示学习

    Representation Learning via Variational Bayesian Networks. (arXiv:2306.16326v1 [cs.LG])

    [http://arxiv.org/abs/2306.16326](http://arxiv.org/abs/2306.16326)

    VBN是一种利用层次和关系信息的新颖的贝叶斯实体表示学习模型，特别适用于数据稀缺的“长尾”实体建模。通过使用层次先验和明确关系约束，VBN提供了更好的建模效果，并通过密度表示实体，对数据稀缺情况下的不确定性进行建模。我们还提出了一种可扩展的变分贝叶斯优化算法来实现快速的近似贝叶斯推断。

    

    我们提出了一种新颖的贝叶斯实体表示学习模型-变分贝叶斯网络 (VBN)，该模型利用层次和关系信息，并对“长尾”中的实体建模特别有用，因为这种情况下数据稀缺。VBN通过两种互补机制提供了更好的长尾实体建模：首先，VBN采用了信息丰富的层次先验，使共享共同祖先的实体之间的信息传播成为可能。此外，VBN建模了实体之间的明确关系，强制实体之间的互补结构和一致性，引导学习的表示向更有意义的空间布局。其次，VBN通过密度表示实体（而不是向量），从而对数据稀缺情况下起到补充作用的不确定性进行建模。最后，我们提出了一种可扩展的变分贝叶斯优化算法，实现了快速的近似贝叶斯推断。我们评估了VBN在语言学任务上的有效性。

    We present Variational Bayesian Network (VBN) - a novel Bayesian entity representation learning model that utilizes hierarchical and relational side information and is particularly useful for modeling entities in the ``long-tail'', where the data is scarce. VBN provides better modeling for long-tail entities via two complementary mechanisms: First, VBN employs informative hierarchical priors that enable information propagation between entities sharing common ancestors. Additionally, VBN models explicit relations between entities that enforce complementary structure and consistency, guiding the learned representations towards a more meaningful arrangement in space. Second, VBN represents entities by densities (rather than vectors), hence modeling uncertainty that plays a complementary role in coping with data scarcity. Finally, we propose a scalable Variational Bayes optimization algorithm that enables fast approximate Bayesian inference. We evaluate the effectiveness of VBN on linguist
    
[^12]: 通过Stein方法对高斯随机场进行逼近及其在广义随机神经网络中的应用

    Gaussian random field approximation via Stein's method with applications to wide random neural networks. (arXiv:2306.16308v1 [math.PR])

    [http://arxiv.org/abs/2306.16308](http://arxiv.org/abs/2306.16308)

    本研究利用Stein方法推导出Wasserstein距离的上界，通过高斯平滑技术将平滑度量转化为Wasserstein距离。通过特殊化结果，我们获得了广义随机神经网络中对高斯随机场逼近的首个上界。

    

    我们利用Stein方法推导出了基于Wasserstein距离（$W_1$）的上界，该距离是连续随机场与高斯分布之间的距离。我们开发了一种新颖的高斯平滑技术，使我们能够将平滑度量中的上界转化为$W_1$距离。平滑性是基于使用Laplacian算子的幂构建的协方差函数，设计成与Cameron-Martin或Reproducing Kernel Hilbert Space相关联的高斯过程具有易操作的特征。这个特征使我们能够超越之前文献中考虑的一维区间型指标集。通过特化我们的一般结果，我们获得了在任意深度和Lipschitz激活函数的广义随机神经网络中对高斯随机场逼近的首个上界。我们的上界明确地用网络宽度和随机权重的矩来表示。

    We derive upper bounds on the Wasserstein distance ($W_1$), with respect to $\sup$-norm, between any continuous $\mathbb{R}^d$ valued random field indexed by the $n$-sphere and the Gaussian, based on Stein's method. We develop a novel Gaussian smoothing technique that allows us to transfer a bound in a smoother metric to the $W_1$ distance. The smoothing is based on covariance functions constructed using powers of Laplacian operators, designed so that the associated Gaussian process has a tractable Cameron-Martin or Reproducing Kernel Hilbert Space. This feature enables us to move beyond one dimensional interval-based index sets that were previously considered in the literature. Specializing our general result, we obtain the first bounds on the Gaussian random field approximation of wide random neural networks of any depth and Lipschitz activation functions at the random field level. Our bounds are explicitly expressed in terms of the widths of the network and moments of the random wei
    
[^13]: 相关实体选择：通过零样本类比修剪进行知识图谱引导

    Relevant Entity Selection: Knowledge Graph Bootstrapping via Zero-Shot Analogical Pruning. (arXiv:2306.16296v1 [cs.AI])

    [http://arxiv.org/abs/2306.16296](http://arxiv.org/abs/2306.16296)

    本文提出了一种基于类比的方法，通过选择和修剪相关实体，从而引导知识图谱的构建。实证结果显示，这种方法在两个领域和异质种子实体的数据集上优于其他机器学习方法，并具有较低的参数数量。这些结果支持在相关任务中进一步应用类比推理。

    

    知识图谱构建可以被视为一个迭代过程，从高质量的核心开始，通过知识提取方法不断改进。这样的核心可以从像Wikidata这样的开放式知识图谱中获得。然而，由于这种通用知识图谱的规模，将其作为整体集成可能会包含无关内容和可扩展性问题。我们提出了一种基于类比的方法，从通用知识图谱中的感兴趣种子实体开始，并保留或修剪其相邻实体。我们通过两个手动标记的数据集 在Wikidata上评估了我们的方法，这些数据集包含领域同质或异质的种子实体。我们从实证上证明了我们的基于类比的方法优于LSTM，随机森林，支持向量机和多层感知器，且参数数量大大减少。我们还在迁移学习环境中评估了其泛化能力。这些结果对于进一步将基于类比的推理集成到相关任务中提供了支持。

    Knowledge Graph Construction (KGC) can be seen as an iterative process starting from a high quality nucleus that is refined by knowledge extraction approaches in a virtuous loop. Such a nucleus can be obtained from knowledge existing in an open KG like Wikidata. However, due to the size of such generic KGs, integrating them as a whole may entail irrelevant content and scalability issues. We propose an analogy-based approach that starts from seed entities of interest in a generic KG, and keeps or prunes their neighboring entities. We evaluate our approach on Wikidata through two manually labeled datasets that contain either domain-homogeneous or -heterogeneous seed entities. We empirically show that our analogy-based approach outperforms LSTM, Random Forest, SVM, and MLP, with a drastically lower number of parameters. We also evaluate its generalization potential in a transfer learning setting. These results advocate for the further integration of analogy-based inference in tasks relate
    
[^14]: 基于聚类的混沌系统极端事件前兆的识别

    Clustering-based Identification of Precursors of Extreme Events in Chaotic Systems. (arXiv:2306.16291v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2306.16291](http://arxiv.org/abs/2306.16291)

    本文研究了基于聚类的方法在混沌系统中识别极端事件前兆的应用，通过聚类系统状态、概率转移矩阵和状态空间划分识别了两个不同混沌系统中的极端事件前兆。

    

    在自然界中，如剧烈的气候模式、罕见巨浪或雪崩等过程中，动力系统状态的突然而迅速的高振幅变化被称为极端事件。这些事件往往具有灾难性的影响，因此它们的描述和预测具有重要意义。然而，由于其混沌特性，它们的建模至今仍面临巨大挑战。本文探索了基于数据驱动的基于模块化的聚类技术在混沌系统中识别罕见和极端事件前兆的应用性。提出了一种基于聚类系统状态、概率转移矩阵和状态空间划分的识别框架，并在两个展示极端事件的不同混沌系统上进行了测试：Moehliss-Faisst-Eckhardt自持湍流模型和二维科尔莫戈洛夫流动模型。两者都展示了能量和耗散中的突发极端事件。结果表明，该方法能够有效识别极端事件的前兆。

    Abrupt and rapid high-amplitude changes in a dynamical system's states known as extreme event appear in many processes occurring in nature, such as drastic climate patterns, rogue waves, or avalanches. These events often entail catastrophic effects, therefore their description and prediction is of great importance. However, because of their chaotic nature, their modelling represents a great challenge up to this day. The applicability of a data-driven modularity-based clustering technique to identify precursors of rare and extreme events in chaotic systems is here explored. The proposed identification framework based on clustering of system states, probability transition matrices and state space tessellation was developed and tested on two different chaotic systems that exhibit extreme events: the Moehliss-Faisst-Eckhardt model of self-sustained turbulence and the 2D Kolmogorov flow. Both exhibit extreme events in the form of bursts in kinetic energy and dissipation. It is shown that th
    
[^15]: 利用GPT-4进行食物影响摘要以通过迭代提示增强产品特定指导开发

    Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting. (arXiv:2306.16275v1 [cs.CL])

    [http://arxiv.org/abs/2306.16275](http://arxiv.org/abs/2306.16275)

    本研究提出了一种通过迭代提示与ChatGPT或GPT-4进行多轮交互的方法来改进食物影响摘要的准确性。这种方法在产品特定指导(PSG)开发中具有潜力应用的价值。

    

    食物影响从新药申请(NDA)中的摘要是产品特定指导(PSG)开发和评估的重要组成部分。然而，从大量药物申请审查文件中手动摘要食物影响是耗时的，这引发了开发自动化方法的需求。最近大型语言模型(LLMs)如ChatGPT和GPT-4的进展已经展示了在改善自动文本摘要效果方面的巨大潜力，但其在PSG评估中准确概括食物影响的能力尚不清楚。在这项研究中，我们引入了一种简单而有效的方法，即迭代提示，通过多轮交互更有效和高效地与ChatGPT或GPT-4进行互动。具体而言，我们提出了一个三轮迭代提示的方法来进行食物影响摘要，其中在连续的轮次中分别提供了关键字聚焦和长度控制的提示以细化。

    Food effect summarization from New Drug Application (NDA) is an essential component of product-specific guidance (PSG) development and assessment. However, manual summarization of food effect from extensive drug application review documents is time-consuming, which arouses a need to develop automated methods. Recent advances in large language models (LLMs) such as ChatGPT and GPT-4, have demonstrated great potential in improving the effectiveness of automated text summarization, but its ability regarding the accuracy in summarizing food effect for PSG assessment remains unclear. In this study, we introduce a simple yet effective approach, iterative prompting, which allows one to interact with ChatGPT or GPT-4 more effectively and efficiently through multi-turn interaction. Specifically, we propose a three-turn iterative prompting approach to food effect summarization in which the keyword-focused and length-controlled prompts are respectively provided in consecutive turns to refine the 
    
[^16]: S2SNet：一个预训练的神经网络用于超导性发现

    S2SNet: A Pretrained Neural Network for Superconductivity Discovery. (arXiv:2306.16270v1 [cond-mat.supr-con])

    [http://arxiv.org/abs/2306.16270](http://arxiv.org/abs/2306.16270)

    S2SNet是一个预训练的神经网络，用于预测超导性。它利用了注意机制，并基于包含晶体结构和超导临界温度的新数据集S2S进行训练。

    

    超导性能使得电流能够在没有能量损失的情况下流动，因此使固体材料具有超导性是物理学、材料科学和电气工程的一个重要目标。已经有16位诺贝尔奖得主因为其对超导性研究的贡献而获得了奖项。超导体对可持续发展目标（SDGs）具有重要价值，如应对气候变化、可负担得起的清洁能源、工业、创新和基础设施等方面。然而，目前仍没有一个统一的物理理论能够解释所有超导性机制。人们认为，超导性的微观机制不仅与分子组成有关，还与晶体结构有关。因此，在SuperCon和Material Project的基础上建立了一个新的数据集S2S，其中包含晶体结构和超导临界温度的信息。基于这个新数据集，我们提出了一种新颖的模型S2SNet，该模型利用了注意机制来进行超导性预测。

    Superconductivity allows electrical current to flow without any energy loss, and thus making solids superconducting is a grand goal of physics, material science, and electrical engineering. More than 16 Nobel Laureates have been awarded for their contribution to superconductivity research. Superconductors are valuable for sustainable development goals (SDGs), such as climate change mitigation, affordable and clean energy, industry, innovation and infrastructure, and so on. However, a unified physics theory explaining all superconductivity mechanism is still unknown. It is believed that superconductivity is microscopically due to not only molecular compositions but also the geometric crystal structure. Hence a new dataset, S2S, containing both crystal structures and superconducting critical temperature, is built upon SuperCon and Material Project. Based on this new dataset, we propose a novel model, S2SNet, which utilizes the attention mechanism for superconductivity prediction. To over
    
[^17]: 深度展开模拟的双分岔用于大规模MIMO信号检测

    Deep Unfolded Simulated Bifurcation for Massive MIMO Signal Detection. (arXiv:2306.16264v1 [cs.IT])

    [http://arxiv.org/abs/2306.16264](http://arxiv.org/abs/2306.16264)

    本文提出了一种基于量子启发式算法的模拟双分岔用于大规模MIMO信号检测的方法，并通过修改算法和使用深度展开技术，显著改善了检测性能。

    

    多输入多输出（MIMO）是下一代无线通信的关键组成部分。最近，基于深度学习技术和量子（启发式）算法的各种MIMO信号检测器已被提出，以改善与传统检测器相比的检测性能。本文关注量子启发式算法中的模拟双分岔（SB）算法，并提出了两种技术来改善其检测性能。第一种是修改受Levenberg-Marquardt算法启发的算法，以消除最大似然检测的局部最小值。第二种是使用深度展开，一种深度学习技术，来训练迭代算法的内部参数。我们提出了一种深度展开的SB，通过使SB的更新规则可微分。数值结果表明，这些提出的检测器显著改善了大规模MIMO系统的信号检测性能。

    Multiple-input multiple-output (MIMO) is a key ingredient of next-generation wireless communications. Recently, various MIMO signal detectors based on deep learning techniques and quantum(-inspired) algorithms have been proposed to improve the detection performance compared with conventional detectors. This paper focuses on the simulated bifurcation (SB) algorithm, a quantum-inspired algorithm. This paper proposes two techniques to improve its detection performance. The first is modifying the algorithm inspired by the Levenberg-Marquardt algorithm to eliminate local minima of maximum likelihood detection. The second is the use of deep unfolding, a deep learning technique to train the internal parameters of an iterative algorithm. We propose a deep-unfolded SB by making the update rule of SB differentiable. The numerical results show that these proposed detectors significantly improve the signal detection performance in massive MIMO systems.
    
[^18]: 齐次空间上的潜在SDE

    Latent SDEs on Homogeneous Spaces. (arXiv:2306.16248v1 [cs.LG])

    [http://arxiv.org/abs/2306.16248](http://arxiv.org/abs/2306.16248)

    这篇论文研究了在齐次空间上的潜在SDE，通过使用单位球上的SDE进行变分推断，提出了一种简单且直观的表达式来计算近似后验和先验过程之间的KL散度。

    

    我们考虑在潜在变量模型中的变分贝叶斯推断问题，其中一个（可能是复杂的）观测随机过程由潜在随机微分方程（SDE）的解决方案所驱动。受到学习大规模数据中（几乎任意）潜在神经SDE时所面临的挑战的启发，例如效率梯度计算，我们退一步并研究了一个特定的子类。在我们的情况下，SDE在一个齐次潜在空间上演变，并由相应（矩阵）Lie群的随机动力学所诱导。在学习问题中，单位$n$-球上的SDE可以说是这一设置中最相关的。值得注意的是，在变分推断中，单位球不仅有助于使用真正无信息的先验SDE，而且我们还获得了关于近似后验和先验过程之间的Kullback-Leibler散度的特别简单和直观的表达式，这在证据下界中至关重要。实验证明了我们的方法的性能优势。

    We consider the problem of variational Bayesian inference in a latent variable model where a (possibly complex) observed stochastic process is governed by the solution of a latent stochastic differential equation (SDE). Motivated by the challenges that arise when trying to learn an (almost arbitrary) latent neural SDE from large-scale data, such as efficient gradient computation, we take a step back and study a specific subclass instead. In our case, the SDE evolves on a homogeneous latent space and is induced by stochastic dynamics of the corresponding (matrix) Lie group. In learning problems, SDEs on the unit $n$-sphere are arguably the most relevant incarnation of this setup. Notably, for variational inference, the sphere not only facilitates using a truly uninformative prior SDE, but we also obtain a particularly simple and intuitive expression for the Kullback-Leibler divergence between the approximate posterior and prior process in the evidence lower bound. Experiments demonstrat
    
[^19]: 连续时间q-learning用于McKean-Vlasov控制问题

    Continuous-Time q-learning for McKean-Vlasov Control Problems. (arXiv:2306.16208v1 [cs.LG])

    [http://arxiv.org/abs/2306.16208](http://arxiv.org/abs/2306.16208)

    本文研究了连续时间q-learning在熵正则化强化学习框架下用于McKean-Vlasov控制问题，并揭示了两种不同的q函数的存在及其积分表示。

    

    本文研究了q-learning，在熵正则化强化学习框架下，用于连续时间的McKean-Vlasov控制问题。与Jia和Zhou（2022c）的单个代理控制问题不同，代理之间的均场相互作用使得q函数的定义更加复杂，我们揭示了自然产生两种不同q函数的情况：（i）被称为集成q函数（用$q$表示），作为Gu、Guo、Wei和Xu（2023）引入的集成Q函数的一阶近似，可以通过涉及测试策略的弱鞅条件进行学习；（ii）作为策略改进迭代中所使用的实质q函数（用$q_e$表示）。我们证明了这两个q函数在所有测试策略下通过积分表示相关联。基于集成q函数的弱鞅条件和我们提出的搜索方法，我们设计了算法来学习两个q函数以解决Mckean-Vlasov控制问题。

    This paper studies the q-learning, recently coined as the continuous-time counterpart of Q-learning by Jia and Zhou (2022c), for continuous time Mckean-Vlasov control problems in the setting of entropy-regularized reinforcement learning. In contrast to the single agent's control problem in Jia and Zhou (2022c), the mean-field interaction of agents render the definition of q-function more subtle, for which we reveal that two distinct q-functions naturally arise: (i) the integrated q-function (denoted by $q$) as the first-order approximation of the integrated Q-function introduced in Gu, Guo, Wei and Xu (2023) that can be learnt by a weak martingale condition involving test policies; and (ii) the essential q-function (denoted by $q_e$) that is employed in the policy improvement iterations. We show that two q-functions are related via an integral representation under all test policies. Based on the weak martingale condition of the integrated q-function and our proposed searching method of
    
[^20]: 定义数据科学：一种新的研究范式

    Defining data science: a new field of inquiry. (arXiv:2306.16177v1 [cs.LG])

    [http://arxiv.org/abs/2306.16177](http://arxiv.org/abs/2306.16177)

    数据科学是一种新的研究范式，具有潜力和应用广泛性，在40多个学科、数百个研究领域和成千上万个应用中出现。然而，由于其起步阶段，目前存在许多定义的冗余和不一致性的问题。

    

    数据科学不是一门科学，而是一种研究范式。它的力量、范围和规模将超越科学，成为促使知识发现并改变世界的重要手段。我们尚未理解和定义它，这对于实现其潜力和管理其风险至关重要。现代数据科学处于起步阶段。自1962年以来缓慢发展，并且自2000年以来发展迅速，它是一种根本性的新的研究领域，是21世纪最活跃、最强大和发展最快的创新之一。由于其价值、力量和适用性，它正在40多个学科、数百个研究领域和成千上万个应用中出现。数以百万计的数据科学出版物中包含了无数关于数据科学和数据科学问题解决的定义。由于其起步阶段，许多定义是独立的、应用特定的、相互不完整的、冗余的或不一致的，因此数据科学也是如此。本研究通过提出解决数据科学多重定义挑战的方法来解决这个问题。

    Data science is not a science. It is a research paradigm. Its power, scope, and scale will surpass science, our most powerful research paradigm, to enable knowledge discovery and change our world. We have yet to understand and define it, vital to realizing its potential and managing its risks. Modern data science is in its infancy. Emerging slowly since 1962 and rapidly since 2000, it is a fundamentally new field of inquiry, one of the most active, powerful, and rapidly evolving 21st century innovations. Due to its value, power, and applicability, it is emerging in 40+ disciplines, hundreds of research areas, and thousands of applications. Millions of data science publications contain myriad definitions of data science and data science problem solving. Due to its infancy, many definitions are independent, application-specific, mutually incomplete, redundant, or inconsistent, hence so is data science. This research addresses this data science multiple definitions challenge by proposing 
    
[^21]: 通过多教师对抗蒸馏减轻准确性与鲁棒性之间的权衡

    Mitigating the Accuracy-Robustness Trade-off via Multi-Teacher Adversarial Distillation. (arXiv:2306.16170v1 [cs.LG])

    [http://arxiv.org/abs/2306.16170](http://arxiv.org/abs/2306.16170)

    本文介绍了一种名为多教师对抗鲁棒性蒸馏的方法，它通过使用强大的干净样本教师和鲁棒性教师来改进深度神经网络的对抗训练过程，以减轻准确性和鲁棒性之间的权衡。

    

    对抗训练是一种改善深度神经网络对抗攻击鲁棒性的实用方法。虽然有效提高了鲁棒性，但对干净样本的性能却有所下降，这意味着准确性和鲁棒性之间存在一种权衡。最近的一些研究尝试在对抗训练中使用知识蒸馏方法，取得了提高鲁棒性的竞争性性能，但并没有显著改善对干净样本的准确性。为了减轻准确性和鲁棒性之间的权衡，本文引入了多教师对抗鲁棒性蒸馏（MTARD），通过应用强大的干净样本教师和强大的鲁棒样本教师来指导模型的对抗训练过程。在优化过程中，为了确保不同教师显示相似的知识水平，我们设计了基于熵的平衡算法进行调整。

    Adversarial training is a practical approach for improving the robustness of deep neural networks against adversarial attacks. Although bringing reliable robustness, the performance toward clean examples is negatively affected after adversarial training, which means a trade-off exists between accuracy and robustness. Recently, some studies have tried to use knowledge distillation methods in adversarial training, achieving competitive performance in improving the robustness but the accuracy for clean samples is still limited. In this paper, to mitigate the accuracy-robustness trade-off, we introduce the Multi-Teacher Adversarial Robustness Distillation (MTARD) to guide the model's adversarial training process by applying a strong clean teacher and a strong robust teacher to handle the clean examples and adversarial examples, respectively. During the optimization process, to ensure that different teachers show similar knowledge scales, we design the Entropy-Based Balance algorithm to adj
    
[^22]: 通信资源受限的分层联邦学习用于端到端自动驾驶

    Communication Resources Constrained Hierarchical Federated Learning for End-to-End Autonomous Driving. (arXiv:2306.16169v1 [cs.RO])

    [http://arxiv.org/abs/2306.16169](http://arxiv.org/abs/2306.16169)

    本文提出了一种优化的通信资源受限的分层联邦学习框架，用于端到端自动驾驶模型，通过混合数据和模型聚合来降低泛化误差，并在CARLA仿真平台上评估了其有效性。

    

    尽管联邦学习（FL）通过模型聚合提高了端到端自动驾驶的泛化能力，但传统的单跳联邦学习（SFL）由于车辆和云服务器之间的远程通信而收敛速度较慢。分层联邦学习（HFL）通过引入中间边缘服务器克服了这些缺点。然而，受限的通信资源和HFL性能之间的协调成为一个紧迫的问题。本文提出了一种基于优化的通信资源受限的分层联邦学习（CRCHFL）框架，通过使用混合数据和模型聚合来最小化自动驾驶模型的泛化误差。通过在CARLA仿真平台上评估了所提出的CRCHFL的有效性。结果表明，所提出的CRCHFL既加快了收敛速度，又增强了联邦学习自动驾驶模型的泛化能力。

    While federated learning (FL) improves the generalization of end-to-end autonomous driving by model aggregation, the conventional single-hop FL (SFL) suffers from slow convergence rate due to long-range communications among vehicles and cloud server. Hierarchical federated learning (HFL) overcomes such drawbacks via introduction of mid-point edge servers. However, the orchestration between constrained communication resources and HFL performance becomes an urgent problem. This paper proposes an optimization-based Communication Resource Constrained Hierarchical Federated Learning (CRCHFL) framework to minimize the generalization error of the autonomous driving model using hybrid data and model aggregation. The effectiveness of the proposed CRCHFL is evaluated in the Car Learning to Act (CARLA) simulation platform. Results show that the proposed CRCHFL both accelerates the convergence rate and enhances the generalization of federated learning autonomous driving model. Moreover, under the 
    
[^23]: 机器学习中最优输运的最新进展

    Recent Advances in Optimal Transport for Machine Learning. (arXiv:2306.16156v1 [cs.LG])

    [http://arxiv.org/abs/2306.16156](http://arxiv.org/abs/2306.16156)

    最优输运在机器学习中的最新进展包括生成建模和迁移学习等领域，并且计算最优输运的发展也与机器学习实践相互影响。

    

    最近，最优输运被提出作为机器学习中比较和操作概率分布的概率框架。这个框架源于其丰富的历史和理论，并提供了新的解决方案，如生成建模和迁移学习。在这项调查中，我们探讨了最优输运在2012年至2022年期间对机器学习的贡献，重点关注机器学习的四个子领域：有监督学习、无监督学习、迁移学习和强化学习。我们还突出了计算最优输运的最新发展，并与机器学习实践相互影响。

    Recently, Optimal Transport has been proposed as a probabilistic framework in Machine Learning for comparing and manipulating probability distributions. This is rooted in its rich history and theory, and has offered new solutions to different problems in machine learning, such as generative modeling and transfer learning. In this survey we explore contributions of Optimal Transport for Machine Learning over the period 2012 -- 2022, focusing on four sub-fields of Machine Learning: supervised, unsupervised, transfer and reinforcement learning. We further highlight the recent development in computational Optimal Transport, and its interplay with Machine Learning practice.
    
[^24]: MLSMM：机器学习安全成熟度模型

    MLSMM: Machine Learning Security Maturity Model. (arXiv:2306.16127v1 [cs.SE])

    [http://arxiv.org/abs/2306.16127](http://arxiv.org/abs/2306.16127)

    这篇论文提出了一个初步的机器学习安全成熟度模型（MLSMM），通过将安全实践沿着ML开发生命周期进行组织和建立成熟度水平，以评估安全实践的成熟度。这将有助于促进产业界和学术界的更紧密合作。

    

    在机器学习（ML）基于软件组件开发过程中，评估安全实践的成熟度没有得到像传统软件开发那样的关注。在这篇蓝天想法的论文中，我们提出了一个初步的机器学习安全成熟度模型（MLSMM），该模型将安全实践沿着ML开发生命周期进行组织，并为每个实践建立了三个成熟度水平。我们将MLSMM视为产业界和学术界更紧密合作的一步。

    Assessing the maturity of security practices during the development of Machine Learning (ML) based software components has not gotten as much attention as traditional software development. In this Blue Sky idea paper, we propose an initial Machine Learning Security Maturity Model (MLSMM) which organizes security practices along the ML-development lifecycle and, for each, establishes three levels of maturity. We envision MLSMM as a step towards closer collaboration between industry and academia.
    
[^25]: 更高效的自动转录表格数据的人工审核方法

    More efficient manual review of automatically transcribed tabular data. (arXiv:2306.16126v1 [cs.LG])

    [http://arxiv.org/abs/2306.16126](http://arxiv.org/abs/2306.16126)

    本文提出了一种更高效的自动转录表格数据的人工审核方法，通过使用机器学习方法转录大量的手写职业代码并进行手动审核，达到了准确性的提升和工作流程的改善。

    

    机器学习方法在转录历史数据方面已经证明其有效性。然而，即使是高精度的方法的结果也需要手动验证和纠正。这样的人工审核可能耗时且昂贵，因此本文的目标是提高其效率。我们先前使用机器学习方法的高精度转录了挪威 1950 年人口普查中的 230 万份手写职业代码，然后手动审核了 3%（9 万份）置信度最低的代码。我们将这 9 万份代码分配给人工审核员，并使用我们的注释工具进行审核。为了评估审核员之间的一致性，部分代码被分配给多位审核员。然后，我们分析了审核结果以了解准确性改进与工作量之间的关系。此外，我们还采访了审核员以改善工作流程。审核员更正了 62.8% 的标签，并在 31.9% 的案例中与模型的标签达成一致。约 0.2% 的图片需要进一步的人工审核。

    Machine learning methods have proven useful in transcribing historical data. However, results from even highly accurate methods require manual verification and correction. Such manual review can be time-consuming and expensive, therefore the objective of this paper was to make it more efficient. Previously, we used machine learning to transcribe 2.3 million handwritten occupation codes from the Norwegian 1950 census with high accuracy (97%). We manually reviewed the 90,000 (3%) codes with the lowest model confidence. We allocated those 90,000 codes to human reviewers, who used our annotation tool to review the codes. To assess reviewer agreement, some codes were assigned to multiple reviewers. We then analyzed the review results to understand the relationship between accuracy improvements and effort. Additionally, we interviewed the reviewers to improve the workflow. The reviewers corrected 62.8% of the labels and agreed with the model label in 31.9% of cases. About 0.2% of the images 
    
[^26]: 增强对比实例区分的语义正向对

    Semantic Positive Pairs for Enhancing Contrastive Instance Discrimination. (arXiv:2306.16122v1 [cs.CV])

    [http://arxiv.org/abs/2306.16122](http://arxiv.org/abs/2306.16122)

    提出了一种名为语义正向对集合（SPPS）的方法，可以在表示学习过程中识别具有相似语义内容的图像，并将它们视为正向实例，从而减少丢弃重要特征的风险。在多个实验数据集上的实验证明了该方法的有效性。

    

    基于实例区分的自监督学习算法有效地防止表示坍缩，并在表示学习中产生了有希望的结果。然而，在嵌入空间中吸引正向对（即相同实例的两个视图）并排斥所有其他实例（即负向对），无论它们的类别，可能导致丢弃重要的特征。为了解决这个问题，我们提出了一种方法来识别具有相似语义内容的图像，并将它们视为正向实例，命名为语义正向对集合（SPPS），从而在表示学习中减少了丢弃重要特征的风险。我们的方法可以与任何对比实例区分框架（如SimCLR或MOCO）一起使用。我们在三个数据集（ImageNet、STL-10和CIFAR-10）上进行实验证明了我们的方法的有效性。实验结果表明，我们的方法始终优于基线方法vanilla。

    Self-supervised learning algorithms based on instance discrimination effectively prevent representation collapse and produce promising results in representation learning. However, the process of attracting positive pairs (i.e., two views of the same instance) in the embedding space and repelling all other instances (i.e., negative pairs) irrespective of their categories could result in discarding important features. To address this issue, we propose an approach to identifying those images with similar semantic content and treating them as positive instances, named semantic positive pairs set (SPPS), thereby reducing the risk of discarding important features during representation learning. Our approach could work with any contrastive instance discrimination framework such as SimCLR or MOCO. We conduct experiments on three datasets: ImageNet, STL-10 and CIFAR-10 to evaluate our approach. The experimental results show that our approach consistently outperforms the baseline method vanilla 
    
[^27]: 时间正则化在最优时间变量学习中的应用

    Time Regularization in Optimal Time Variable Learning. (arXiv:2306.16111v1 [cs.LG])

    [http://arxiv.org/abs/2306.16111](http://arxiv.org/abs/2306.16111)

    本文研究了时间正则化在最优时间变量学习中的应用，通过引入一个与离散动力系统中的时间范围直接相关的正则化项，并提出了一种适应性剪枝方法用于减少网络复杂性和训练时间，同时保持表达能力。在MNIST和Fashion MNIST数据集上的分类任务上验证了方法的有效性。

    

    最近，在arXiv:2204.08528中引入了深度神经网络（DNNs）中的最优时间变量学习。在本文中，我们通过引入一个与离散动力系统中的时间范围直接相关的正则化项，扩展了这个概念。此外，我们提出了一种适应性剪枝方法，用于Residual Neural Networks (ResNets)，可以减少网络复杂性，同时不损害表达能力，同时减少训练时间。通过将这些概念应用于著名的MNIST和Fashion MNIST数据集上的分类任务，我们展示了结果。我们的PyTorch代码可在https://github.com/frederikkoehne/time_variable_learning上找到。

    Recently, optimal time variable learning in deep neural networks (DNNs) was introduced in arXiv:2204.08528. In this manuscript we extend the concept by introducing a regularization term that directly relates to the time horizon in discrete dynamical systems. Furthermore, we propose an adaptive pruning approach for Residual Neural Networks (ResNets), which reduces network complexity without compromising expressiveness, while simultaneously decreasing training time. The results are illustrated by applying the proposed concepts to classification tasks on the well known MNIST and Fashion MNIST data sets. Our PyTorch code is available on https://github.com/frederikkoehne/time_variable_learning.
    
[^28]: 稀疏表示、推理和学习

    Sparse Representations, Inference and Learning. (arXiv:2306.16097v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2306.16097](http://arxiv.org/abs/2306.16097)

    这篇论文介绍了一种通用框架，可以用于解决涉及弱长程相互作用的各种推理问题，包括压缩感知和感知器学习。利用统计物理学的方法，我们可以研究这些问题的基本限制，并提出相应的算法。

    

    近年来，统计物理学已经证明是探究机器学习中出现的大维推理问题的一个有价值的工具。统计物理学提供了分析工具来研究其解决方案中的基本限制，并提出算法来解决个别实例。在这些笔记中，我们将基于2022年Les Houches夏季学校中Marc M\'ezard的讲座，介绍一种可以在弱长程相互作用的各种问题中使用的通用框架，包括压缩感知问题或感知器的学习问题。我们将看到这些问题如何在复制对称级别上进行研究，使用中腔方法的发展，既作为理论工具，也作为算法。

    In recent years statistical physics has proven to be a valuable tool to probe into large dimensional inference problems such as the ones occurring in machine learning. Statistical physics provides analytical tools to study fundamental limitations in their solutions and proposes algorithms to solve individual instances. In these notes, based on the lectures by Marc M\'ezard in 2022 at the summer school in Les Houches, we will present a general framework that can be used in a large variety of problems with weak long-range interactions, including the compressed sensing problem, or the problem of learning in a perceptron. We shall see how these problems can be studied at the replica symmetric level, using developments of the cavity methods, both as a theoretical tool and as an algorithm.
    
[^29]: 神经网络激活函数的经验损失曲面分析

    Empirical Loss Landscape Analysis of Neural Network Activation Functions. (arXiv:2306.16090v1 [cs.LG])

    [http://arxiv.org/abs/2306.16090](http://arxiv.org/abs/2306.16090)

    本研究通过经验分析了双曲正切、修正线性单元和指数线性单元激活函数相关的神经网络损失曲面，发现修正线性单元呈现最凸型曲面，指数线性单元呈现最平坦曲面并具有更优的泛化性能。所有激活函数的损失曲面中存在宽阔和狭窄的山谷，而狭窄的山谷与饱和神经元和隐含的正则化网络结构相关。

    

    激活函数通过引入非线性在神经网络设计中起着重要作用。先前的研究表明激活函数的选择会影响损失曲面的性质。了解激活函数与损失曲面性质的关系对神经网络架构和训练算法设计是重要的。本研究从经验上分析了与双曲正切、修正线性单元和指数线性单元激活函数相关的神经网络损失曲面。实验证明修正线性单元产生最凸型的损失曲面，指数线性单元产生最平坦的损失曲面，并且展现出更优的泛化性能。对于所有激活函数，损失曲面中存在宽阔和狭窄的山谷，并且狭窄的山谷与饱和神经元和隐含的正则化网络结构相关。

    Activation functions play a significant role in neural network design by enabling non-linearity. The choice of activation function was previously shown to influence the properties of the resulting loss landscape. Understanding the relationship between activation functions and loss landscape properties is important for neural architecture and training algorithm design. This study empirically investigates neural network loss landscapes associated with hyperbolic tangent, rectified linear unit, and exponential linear unit activation functions. Rectified linear unit is shown to yield the most convex loss landscape, and exponential linear unit is shown to yield the least flat loss landscape, and to exhibit superior generalisation performance. The presence of wide and narrow valleys in the loss landscape is established for all activation functions, and the narrow valleys are shown to correlate with saturated neurons and implicitly regularised network configurations.
    
[^30]: 基于结构模式的图神经网络预测质谱

    Mass Spectra Prediction with Structural Motif-based Graph Neural Networks. (arXiv:2306.16085v1 [cs.LG])

    [http://arxiv.org/abs/2306.16085](http://arxiv.org/abs/2306.16085)

    提出了一种基于结构模式和图神经网络的质谱预测网络（MoMS-Net），可以通过考虑子结构在图级别的方式实现质谱预测，从而更好地扩展质谱库并提高预测准确性。

    

    质谱是来自目标分子的离子化碎片的团块，对于分子结构的鉴定在各个领域起着至关重要的作用。一种常见的分析方法是使用光谱库搜索，将未知的质谱与数据库进行交叉引用。然而，基于搜索的方法的有效性受到现有质谱数据库范围的限制，强调了通过质谱预测扩展数据库的需要。在这项研究中，我们提出了基于结构模式和图神经网络（GNNs）实现的质谱预测网络（MoMS-Net）。我们已经在多样的质谱中测试了我们的模型，并观察到它在其他现有模型上的优越性。MoMS-Net在图级别考虑了子结构，这有助于融入长程依赖性，同时使用更少的内存。

    Mass spectra, which are agglomerations of ionized fragments from targeted molecules, play a crucial role across various fields for the identification of molecular structures. A prevalent analysis method involves spectral library searches,where unknown spectra are cross-referenced with a database. The effectiveness of such search-based approaches, however, is restricted by the scope of the existing mass spectra database, underscoring the need to expand the database via mass spectra prediction. In this research, we propose the Motif-based Mass Spectrum Prediction Network (MoMS-Net), a system that predicts mass spectra using the information derived from structural motifs and the implementation of Graph Neural Networks (GNNs). We have tested our model across diverse mass spectra and have observed its superiority over other existing models. MoMS-Net considers substructure at the graph level, which facilitates the incorporation of long-range dependencies while using less memory compared to t
    
[^31]: 安全高效的异步垂直联邦学习:基于级联混合优化方法

    Secure and Fast Asynchronous Vertical Federated Learning via Cascaded Hybrid Optimization. (arXiv:2306.16077v1 [cs.LG])

    [http://arxiv.org/abs/2306.16077](http://arxiv.org/abs/2306.16077)

    本论文提出了一种在垂直联邦学习中使用级联混合优化的方法，通过在下游使用零阶优化保护隐私并在上游使用一阶优化提高收敛速度，从而解决了ZOO-based VFL收敛速度较慢的问题。

    

    垂直联邦学习(VFL)因能够在垂直分割的数据上联合训练隐私保护模型而引起越来越多的关注。最近的研究表明，应用零阶优化(ZOO)在构建实用的VFL算法方面具有许多优势。然而，基于ZOO的VFL存在一个关键问题，即其收敛速度较慢，限制了其在处理现代大型模型时的应用。为了解决这个问题，我们提出了一种在VFL中使用级联混合优化方法。该方法中，下游模型（客户端）使用ZOO进行训练以保护隐私并确保不共享内部信息。同时，上游模型（服务器）在本地使用一阶优化(FOO)进行更新，这显著提高了收敛速度，使得能够在不损害隐私和安全性的前提下训练大型模型。我们在理论上证明了我们的VFL框架比基于ZOO的VFL更快地收敛。

    Vertical Federated Learning (VFL) attracts increasing attention because it empowers multiple parties to jointly train a privacy-preserving model over vertically partitioned data. Recent research has shown that applying zeroth-order optimization (ZOO) has many advantages in building a practical VFL algorithm. However, a vital problem with the ZOO-based VFL is its slow convergence rate, which limits its application in handling modern large models. To address this problem, we propose a cascaded hybrid optimization method in VFL. In this method, the downstream models (clients) are trained with ZOO to protect privacy and ensure that no internal information is shared. Meanwhile, the upstream model (server) is updated with first-order optimization (FOO) locally, which significantly improves the convergence rate, making it feasible to train the large models without compromising privacy and security. We theoretically prove that our VFL framework converges faster than the ZOO-based VFL, as the c
    
[^32]: 基于基础生成模型的联邦生成学习

    Federated Generative Learning with Foundation Models. (arXiv:2306.16064v1 [cs.LG])

    [http://arxiv.org/abs/2306.16064](http://arxiv.org/abs/2306.16064)

    本文提出了一种基于基础生成模型的联邦生成学习框架，通过传输提示与分布式训练数据，可以远程合成有信息量的训练数据，从而改善了通信效率、适应分布转移、提升性能、加强隐私保护。

    

    现有的联邦学习解决方案主要集中在在客户端和服务器之间传输特征、参数或梯度，这导致了严重的低效和隐私泄露问题。借助新兴的基础生成模型，我们提出了一种新颖的联邦学习框架，称为联邦生成学习，它在客户端和服务器之间传输与分布式训练数据相关的提示。通过接收到的包含较少隐私信息的提示以及基础生成模型，可以远程合成有信息量的训练数据。这个新框架具有多个优势，包括改善了通信效率、更好的适应分布转移、实现了显著的性能提升、加强了隐私保护。在ImageNet和DomainNet数据集上进行的广泛实验证明了这些优势。

    Existing federated learning solutions focus on transmitting features, parameters or gadients between clients and server, which suffer from serious low-efficiency and privacy-leakage problems. Thanks to the emerging foundation generative models, we propose a novel federated learning framework, namely Federated Generative Learning, that transmits prompts associated with distributed training data between clients and server. The informative training data can be synthesized remotely based on received prompts containing little privacy and the foundation generative models. The new framework possesses multiple advantages, including improved communication efficiency, better resilience to distribution shift, substantial performance gains, and enhanced privacy protection, which are verified in extensive experiments on ImageNet and DomainNet datasets.
    
[^33]: DUET: 2D结构化且近似等变表示

    DUET: 2D Structured and Approximately Equivariant Representations. (arXiv:2306.16058v1 [cs.LG])

    [http://arxiv.org/abs/2306.16058](http://arxiv.org/abs/2306.16058)

    DUET是一种2D结构化且近似等变表示方法，相比于其他方法，可以在保留输入变换信息的同时具有更好的可控性和更高的准确性。

    

    多视图自监督学习(MSSL)基于学习相对于一组输入变换的不变性。然而，不变性从表示中部分或完全移除与变换相关的信息，这可能对需要这些信息的特定下游任务的性能造成损害。我们提出了2D结构化和等变表示，称为DUET，它们是以矩阵结构组织的2D表示，并且对作用于输入数据的变换具有等变性。DUET表示保留有关输入变换的信息，同时保持语义表达能力。与SimCLR（Chen等，2020）（无结构和不变性）和ESSL（Dangovski等，2022）（无结构和等变性）相比，DUET表示的结构化和等变性使得生成具有更低的重建误差的可控性成为可能，而SimCLR或ESSL则无法实现可控性。DUET还实现了更高的准确性。

    Multiview Self-Supervised Learning (MSSL) is based on learning invariances with respect to a set of input transformations. However, invariance partially or totally removes transformation-related information from the representations, which might harm performance for specific downstream tasks that require such information. We propose 2D strUctured and EquivarianT representations (coined DUET), which are 2d representations organized in a matrix structure, and equivariant with respect to transformations acting on the input data. DUET representations maintain information about an input transformation, while remaining semantically expressive. Compared to SimCLR (Chen et al., 2020) (unstructured and invariant) and ESSL (Dangovski et al., 2022) (unstructured and equivariant), the structured and equivariant nature of DUET representations enables controlled generation with lower reconstruction error, while controllability is not possible with SimCLR or ESSL. DUET also achieves higher accuracy fo
    
[^34]: 通过使用二进制预排序来改进深度学习中的灵长类声音分类

    Improving Primate Sounds Classification using Binary Presorting for Deep Learning. (arXiv:2306.16054v1 [cs.SD])

    [http://arxiv.org/abs/2306.16054](http://arxiv.org/abs/2306.16054)

    本研究提出了一种改进的方法，通过使用二进制预排序和卷积神经网络对灵长类声音进行分类，以提高多类分类任务的性能。在“ComparE 2021”数据集上进行的实验结果显示，相比于基准模型，该方法的准确性和UAR分数显著提高。

    

    在野生动物观察和保护领域，使用音频记录的机器学习方法越来越受欢迎。不幸的是，来自这个研究领域的可用数据集通常不是最佳的学习材料；样本可能被弱标记，长度不同或信噪比较差。在本研究中，我们引入了一种广义方法，首先重新标记MEL谱图表示的子段，以在实际的多类分类任务中获得更高的性能。对于二进制预排序和分类，我们使用了卷积神经网络（CNN）和各种数据增强技术。我们展示了这种方法在具有挑战性的“ComparE 2021”数据集上的结果，在分类不同灵长类物种声音的任务中，报告了与相比较的基准模型相比显著更高的准确性和UAR分数。

    In the field of wildlife observation and conservation, approaches involving machine learning on audio recordings are becoming increasingly popular. Unfortunately, available datasets from this field of research are often not optimal learning material; Samples can be weakly labeled, of different lengths or come with a poor signal-to-noise ratio. In this work, we introduce a generalized approach that first relabels subsegments of MEL spectrogram representations, to achieve higher performances on the actual multi-class classification tasks. For both the binary pre-sorting and the classification, we make use of convolutional neural networks (CNN) and various data-augmentation techniques. We showcase the results of this approach on the challenging \textit{ComparE 2021} dataset, with the task of classifying between different primate species sounds, and report significantly higher Accuracy and UAR scores in contrast to comparatively equipped model baselines.
    
[^35]: 通过对抗攻击评估深度图像去噪模型的相似性和鲁棒性

    Evaluating Similitude and Robustness of Deep Image Denoising Models via Adversarial Attack. (arXiv:2306.16050v1 [cs.CV])

    [http://arxiv.org/abs/2306.16050](http://arxiv.org/abs/2306.16050)

    通过对抗攻击评估了深度图像去噪模型的相似性和鲁棒性，发现现有模型容易被攻击。还研究了去噪模型的迁移性在图像去噪任务中的特性。

    

    深度神经网络在图像去噪领域有着广泛的应用，并且比传统的图像去噪方法更优越。然而，深度神经网络不可避免地显示出弱鲁棒性，在面对对抗攻击时易受损。本文发现了现有深度图像去噪方法之间的相似之处，它们都容易被对抗攻击欺骗。首先，我们提出了一种去噪-PGD方法，它是一种全对抗的去噪模型。当前主流的非盲去噪模型（DnCNN，FFDNet，ECNDNet，BRDNet），盲去噪模型（DnCNN-B，Noise2Noise，RDDCNN-B，FAN），即插即用（DPIR，CurvPnP）和展开去噪模型（DeamNet）应用于灰度和彩色图像都可以被同一组方法攻击。其次，由于去噪-PGD的迁移性在图像去噪任务中很突出，我们设计了实验来探索迁移性下的潜在特性。

    Deep neural networks (DNNs) have a wide range of applications in the field of image denoising, and they are superior to traditional image denoising. However, DNNs inevitably show vulnerability, which is the weak robustness in the face of adversarial attacks. In this paper, we find some similitudes between existing deep image denoising methods, as they are consistently fooled by adversarial attacks. First, denoising-PGD is proposed which is a denoising model full adversarial method. The current mainstream non-blind denoising models (DnCNN, FFDNet, ECNDNet, BRDNet), blind denoising models (DnCNN-B, Noise2Noise, RDDCNN-B, FAN), and plug-and-play (DPIR, CurvPnP) and unfolding denoising models (DeamNet) applied to grayscale and color images can be attacked by the same set of methods. Second, since the transferability of denoising-PGD is prominent in the image denoising task, we design experiments to explore the characteristic of the latent under the transferability. We correlate transferabi
    
[^36]: OpenNDD:用于神经发育障碍检测的开放性识别

    OpenNDD: Open Set Recognition for Neurodevelopmental Disorders Detection. (arXiv:2306.16045v1 [cs.CV])

    [http://arxiv.org/abs/2306.16045](http://arxiv.org/abs/2306.16045)

    OpenNDD是一个用于神经发育障碍检测的开放性识别框架，结合了自动编码器和对抗循环点开放性识别技术，能准确识别已知类别并识别未遇到的类别。

    

    神经发育障碍(NDDs)是一组高患病率的障碍，表现出临床行为的相似性，使得精确识别不同的NDDs（如自闭症谱系障碍（ASD）和注意力缺陷多动障碍（ADHD））非常具有挑战性。此外，对于NDDs诊断并没有可靠的生理标志物，而仅依赖于心理评估标准。然而，通过智能辅助诊断来防止误诊和漏诊是至关重要的，这与随后的相应治疗密切相关。为了缓解这些问题，我们提出了一种新颖的用于NDDs筛查和检测的开放性识别框架，这是在该领域中首次应用开放性识别。它结合了自动编码器和对抗循环点开放性识别，能够准确识别已知类别，并能够识别过去未遇到的类别。

    Neurodevelopmental disorders (NDDs) are a highly prevalent group of disorders and represent strong clinical behavioral similarities, and that make it very challenging for accurate identification of different NDDs such as autism spectrum disorder (ASD) and attention-deficit hyperactivity disorder (ADHD). Moreover, there is no reliable physiological markers for NDDs diagnosis and it solely relies on psychological evaluation criteria. However, it is crucial to prevent misdiagnosis and underdiagnosis by intelligent assisted diagnosis, which is closely related to the follow-up corresponding treatment. In order to relieve these issues, we propose a novel open set recognition framework for NDDs screening and detection, which is the first application of open set recognition in this field. It combines auto encoder and adversarial reciprocal points open set recognition to accurately identify known classes as well as recognize classes never encountered. And considering the strong similarities bet
    
[^37]: 用户上下文的轻量级建模：结合物理和虚拟传感器数据

    Lightweight Modeling of User Context Combining Physical and Virtual Sensor Data. (arXiv:2306.16029v1 [cs.LG])

    [http://arxiv.org/abs/2306.16029](http://arxiv.org/abs/2306.16029)

    本文提出了一个轻量级的用户上下文建模方法，通过结合物理和虚拟传感器数据，利用机器学习技术实现上下文感知服务，并通过采集大规模数据集进行推断过程优化。

    

    移动设备上传感器产生的大量数据，结合机器学习技术的进步，支持上下文感知服务，识别用户的当前情况（即物理上下文），并优化系统的个性化功能。然而，上下文感知性能主要取决于上下文推断过程的准确性，这与大规模和标记数据集的可用性紧密相关。在这项工作中，我们介绍了一个开发的框架，用于收集包含个人移动设备导出的多样化传感数据的数据集。这个框架已经被3个志愿者用户使用了两个星期，生成了一个包含超过36K个样本和1331个特征的数据集。我们还提出了一种轻量级的方法来建模用户上下文，能够在用户移动设备上高效地执行整个推理过程。为了实现这个目标，我们使用了六种维度减少技术，以优化上下文分类过程。

    The multitude of data generated by sensors available on users' mobile devices, combined with advances in machine learning techniques, support context-aware services in recognizing the current situation of a user (i.e., physical context) and optimizing the system's personalization features. However, context-awareness performances mainly depend on the accuracy of the context inference process, which is strictly tied to the availability of large-scale and labeled datasets. In this work, we present a framework developed to collect datasets containing heterogeneous sensing data derived from personal mobile devices. The framework has been used by 3 voluntary users for two weeks, generating a dataset with more than 36K samples and 1331 features. We also propose a lightweight approach to model the user context able to efficiently perform the entire reasoning process on the user mobile device. To this aim, we used six dimensionality reduction techniques in order to optimize the context classifi
    
[^38]: 经典学习者与量子学习者之间的指数区别

    Exponential separations between classical and quantum learners. (arXiv:2306.16028v1 [quant-ph])

    [http://arxiv.org/abs/2306.16028](http://arxiv.org/abs/2306.16028)

    本文研究了经典学习者和量子学习者之间的指数区别，并提出了两个新的学习分离问题，其中经典困难主要在于识别生成数据的函数。

    

    尽管量子机器学习社区在处理经典数据时已经展示了量子学习优势，但在寻找量子学习算法可以在经典学习算法上实现可证明指数加速的学习问题方面，仍然存在挑战。本文讨论与此问题相关的计算学习理论概念，并讨论定义上的微妙差异可能导致学习者需要满足和解决显著不同的要求和任务。我们研究了已有的具有可证明量子加速性质的学习问题，并发现它们主要依赖于计算生成数据的函数的经典难度，而不是识别这个函数。为了解决这个问题，我们提出了两个新的学习分离问题，其中经典困难主要在于识别生成数据的函数。

    Despite significant effort, the quantum machine learning community has only demonstrated quantum learning advantages for artificial cryptography-inspired datasets when dealing with classical data. In this paper we address the challenge of finding learning problems where quantum learning algorithms can achieve a provable exponential speedup over classical learning algorithms. We reflect on computational learning theory concepts related to this question and discuss how subtle differences in definitions can result in significantly different requirements and tasks for the learner to meet and solve. We examine existing learning problems with provable quantum speedups and find that they largely rely on the classical hardness of evaluating the function that generates the data, rather than identifying it. To address this, we present two new learning separations where the classical difficulty primarily lies in identifying the function generating the data. Furthermore, we explore computational h
    
[^39]: 基于联邦学习的分布式计算模型集成异构模型和联盟区块链解决时变问题

    A Distributed Computation Model Based on Federated Learning Integrates Heterogeneous models and Consortium Blockchain for Solving Time-Varying Problems. (arXiv:2306.16023v1 [cs.AI])

    [http://arxiv.org/abs/2306.16023](http://arxiv.org/abs/2306.16023)

    本研究提出了一种基于联邦学习和联盟区块链的分布式计算模型，解决了时变问题中异构模型和模型间协作的难题。

    

    针对复杂环境中的时变问题，递归神经网络已经取得了很大的发展，有效地解决了这些问题。然而，由于集中式处理的方式限制，模型性能受到现实中模型和数据的孤立问题等因素的极大影响。因此，分布式人工智能（例如联邦学习）的出现使得模型间的动态聚合成为可能。然而，现有的联邦学习集成过程仍然依赖于服务器，可能给整体模型带来很大的风险。此外，它只允许同质模型之间的协作，并且没有很好的解决方案来处理异构模型之间的交互。因此，我们提出了一种基于联盟区块链网络的分布式计算模型（DCM），以提高整体模型的可信度和异构模型之间的有效协调。此外，我们还提出了一种分布式层级集成（DHI）算法。

    The recurrent neural network has been greatly developed for effectively solving time-varying problems corresponding to complex environments. However, limited by the way of centralized processing, the model performance is greatly affected by factors like the silos problems of the models and data in reality. Therefore, the emergence of distributed artificial intelligence such as federated learning (FL) makes it possible for the dynamic aggregation among models. However, the integration process of FL is still server-dependent, which may cause a great risk to the overall model. Also, it only allows collaboration between homogeneous models, and does not have a good solution for the interaction between heterogeneous models. Therefore, we propose a Distributed Computation Model (DCM) based on the consortium blockchain network to improve the credibility of the overall model and effective coordination among heterogeneous models. In addition, a Distributed Hierarchical Integration (DHI) algorith
    
[^40]: 强化学习中的结构：调查与开放问题

    Structure in Reinforcement Learning: A Survey and Open Problems. (arXiv:2306.16021v1 [cs.LG])

    [http://arxiv.org/abs/2306.16021](http://arxiv.org/abs/2306.16021)

    这项调查研究了强化学习中结构的角色和重要性，并介绍了各个子领域在提高强化学习的性能方面所做的工作。

    

    强化学习（RL）借助深度神经网络（DNN）在函数逼近方面的表达能力，已经在许多应用中取得了相当大的成功。然而，在应对多样且不可预测的动态、嘈杂信号以及庞大的状态和动作空间等各种真实场景时，其实用性仍然有限。这个限制源于诸如数据效率低、泛化能力有限、缺少安全保证和不可解释性等问题。为了克服这些挑战并在这些关键指标上提高性能，一个有前途的途径是将问题的附加结构信息纳入强化学习的学习过程中。强化学习的各个子领域已经提出了许多方法来纳入这样的归纳偏差。我们将这些多样化的方法统一到一个框架下，揭示结构在学习问题中的作用。

    Reinforcement Learning (RL), bolstered by the expressive capabilities of Deep Neural Networks (DNNs) for function approximation, has demonstrated considerable success in numerous applications. However, its practicality in addressing a wide range of real-world scenarios, characterized by diverse and unpredictable dynamics, noisy signals, and large state and action spaces, remains limited. This limitation stems from issues such as poor data efficiency, limited generalization capabilities, a lack of safety guarantees, and the absence of interpretability, among other factors. To overcome these challenges and improve performance across these crucial metrics, one promising avenue is to incorporate additional structural information about the problem into the RL learning process. Various sub-fields of RL have proposed methods for incorporating such inductive biases. We amalgamate these diverse methodologies under a unified framework, shedding light on the role of structure in the learning prob
    
[^41]: BayesFlow: 使用神经网络的摊还贝叶斯工作流

    BayesFlow: Amortized Bayesian Workflows With Neural Networks. (arXiv:2306.16015v1 [cs.LG])

    [http://arxiv.org/abs/2306.16015](http://arxiv.org/abs/2306.16015)

    BayesFlow是一个Python库，提供了使用神经网络进行摊还贝叶斯推断的功能，用户可以在模型仿真上训练定制的神经网络，并将其用于任何后续应用。这种摊还贝叶斯推断能够快速准确地进行推断，并实现了对不可计算后验分布的近似。

    

    现代贝叶斯推断涉及一系列计算技术，用于估计、验证和从概率模型中得出结论，作为数据分析中有原则的工作流的一部分。贝叶斯工作流中的典型问题包括近似不可计算后验分布以适应不同的模型类型，以及通过复杂性和预测性能比较同一过程的竞争模型。本文介绍了Python库BayesFlow，用于基于仿真训练已建立的神经网络架构，用于摊还数据压缩和推断。在BayesFlow中实现的摊还贝叶斯推断使用户能够在模型仿真上训练定制的神经网络，并将这些网络重用于模型的任何后续应用。由于训练好的网络可以几乎即时地执行推断，因此前期的神经网络训练很快就能够摊还。

    Modern Bayesian inference involves a mixture of computational techniques for estimating, validating, and drawing conclusions from probabilistic models as part of principled workflows for data analysis. Typical problems in Bayesian workflows are the approximation of intractable posterior distributions for diverse model types and the comparison of competing models of the same process in terms of their complexity and predictive performance. This manuscript introduces the Python library BayesFlow for simulation-based training of established neural network architectures for amortized data compression and inference. Amortized Bayesian inference, as implemented in BayesFlow, enables users to train custom neural networks on model simulations and re-use these networks for any subsequent application of the models. Since the trained networks can perform inference almost instantaneously, the upfront neural network training is quickly amortized.
    
[^42]: 标签噪声修正对机器学习公平性影响的系统分析

    Systematic analysis of the impact of label noise correction on ML Fairness. (arXiv:2306.15994v1 [cs.LG])

    [http://arxiv.org/abs/2306.15994](http://arxiv.org/abs/2306.15994)

    本论文对标签噪声修正技术在保证机器学习模型公平性方面的有效性进行了系统分析，通过开发经验方法来评估这些修正方法对修正有偏数据集上的模型的影响。

    

    任意、不一致或错误的决策引发了严重关切，防止不公平模型是机器学习中一个日益重要的挑战。数据经常反映了过去的歧视行为，训练在此类数据上的模型可能反映了对敏感属性（如性别、种族、年龄）的偏见。开发公平模型的一种方法是对训练数据进行预处理，去除潜在的偏见并保留相关信息，例如通过纠正有偏见的标签。虽然有多种标签噪声修正方法可用，但对于它们在识别歧视方面的行为的信息非常有限。在本工作中，我们开发了一种经验方法来系统评估标签噪声修正技术在确保训练于有偏数据集上的模型公平性方面的有效性。我们的方法涉及操纵标签噪声的程度，可用于公平性基准测试以及标准机器学习数据集。我们应用了该方法来评估几种常见的标签噪声修正方法，并通过公平性评估指标评估这些修正方法的性能。

    Arbitrary, inconsistent, or faulty decision-making raises serious concerns, and preventing unfair models is an increasingly important challenge in Machine Learning. Data often reflect past discriminatory behavior, and models trained on such data may reflect bias on sensitive attributes, such as gender, race, or age. One approach to developing fair models is to preprocess the training data to remove the underlying biases while preserving the relevant information, for example, by correcting biased labels. While multiple label noise correction methods are available, the information about their behavior in identifying discrimination is very limited. In this work, we develop an empirical methodology to systematically evaluate the effectiveness of label noise correction techniques in ensuring the fairness of models trained on biased datasets. Our methodology involves manipulating the amount of label noise and can be used with fairness benchmarks but also with standard ML datasets. We apply t
    
[^43]: "MyDigitalFootprint: 用于边缘普适计算应用的全面背景数据集"

    MyDigitalFootprint: an extensive context dataset for pervasive computing applications at the edge. (arXiv:2306.15990v1 [cs.LG])

    [http://arxiv.org/abs/2306.15990](http://arxiv.org/abs/2306.15990)

    "MyDigitalFootprint" 是一个以边缘计算为应用场景的全面背景数据集，支持多模态上下文识别和社交关系建模。该数据集包含了智能手机传感器数据、物理接近信息和在线社交网络互动。通过这个数据集，研究人员可以在用户的自然环境中进行大规模数据分析和上下文适应研究。

    

    连接智能设备的广泛传播促进了互联网在边缘的快速扩展和演变。个人移动设备与周围的其他智能对象进行交互，根据快速变化的用户上下文调整行为。移动设备本地处理这些数据的能力对于快速适应至关重要。这可以通过用户应用中集成的单个详细处理过程或用于上下文处理的中间件平台实现。然而，在移动环境中缺乏考虑用户上下文复杂性的公共数据集阻碍了研究进展。我们介绍了MyDigitalFootprint，这是一个大规模数据集，包括智能手机传感器数据、物理接近信息和在线社交网络互动。该数据集支持多模态上下文识别和社交关系建模。它囊括了31位志愿者用户在其自然环境中的两个月的测量数据，允许无限制的使用。

    The widespread diffusion of connected smart devices has contributed to the rapid expansion and evolution of the Internet at its edge. Personal mobile devices interact with other smart objects in their surroundings, adapting behavior based on rapidly changing user context. The ability of mobile devices to process this data locally is crucial for quick adaptation. This can be achieved through a single elaboration process integrated into user applications or a middleware platform for context processing. However, the lack of public datasets considering user context complexity in the mobile environment hinders research progress. We introduce MyDigitalFootprint, a large-scale dataset comprising smartphone sensor data, physical proximity information, and Online Social Networks interactions. This dataset supports multimodal context recognition and social relationship modeling. It spans two months of measurements from 31 volunteer users in their natural environment, allowing for unrestricted be
    
[^44]: 通过双模态变换器重建血液动力学响应函数

    Reconstructing the Hemodynamic Response Function via a Bimodal Transformer. (arXiv:2306.15971v1 [q-bio.NC])

    [http://arxiv.org/abs/2306.15971](http://arxiv.org/abs/2306.15971)

    本研究首次引入了一种双模态变换器预测模型，通过历史血流和神经活动来推断当前血流，增强了模型的预测能力，并提出了关于血液动力学响应神经活动的假设。

    

    血流与神经活动之间的关系被广泛认可，在fMRI研究中，血流经常被用作神经活动的替代指标。在微观水平上，已经显示神经活动会影响附近血管的血流。本研究首次引入了一种预测模型，直接在明确的神经元群体水平上解决了这个问题。使用清醒小鼠的体内记录，我们使用一种新颖的时空双模态变换器架构，根据历史血流和持续自发神经活动来推断当前的血流。我们的发现表明，结合神经活动明显改善了模型预测血流值的能力。通过分析模型的行为，我们提出了关于血液动力学响应神经活动的尚未深入研究的性质的假设。

    The relationship between blood flow and neuronal activity is widely recognized, with blood flow frequently serving as a surrogate for neuronal activity in fMRI studies. At the microscopic level, neuronal activity has been shown to influence blood flow in nearby blood vessels. This study introduces the first predictive model that addresses this issue directly at the explicit neuronal population level. Using in vivo recordings in awake mice, we employ a novel spatiotemporal bimodal transformer architecture to infer current blood flow based on both historical blood flow and ongoing spontaneous neuronal activity. Our findings indicate that incorporating neuronal activity significantly enhances the model's ability to predict blood flow values. Through analysis of the model's behavior, we propose hypotheses regarding the largely unexplored nature of the hemodynamic response to neuronal activity.
    
[^45]: 可分离的物理信息神经网络

    Separable Physics-Informed Neural Networks. (arXiv:2306.15969v1 [cs.LG])

    [http://arxiv.org/abs/2306.15969](http://arxiv.org/abs/2306.15969)

    这项研究提出了一种可分离的物理信息神经网络（SPINN），通过逐个处理轴来显著减少了多维 PDE 中的网络传播数量，并使用正向模式自动微分降低了计算成本，使得可以在单个普通 GPU 上使用大量的配点。

    

    物理信息神经网络(PINNs)最近已经成为有希望的基于数据的PDE求解器，在各种PDE上显示出令人鼓舞的结果。然而，训练PINNs来解决多维PDE和逼近高度复杂解函数存在根本限制。在这些具有挑战性的PDE上所需的训练点数量(配点)大大增加，但由于昂贵的计算成本和庞大的内存开销，其受到严重限制。为了解决这个问题，我们提出了一种用于PINNs的网络架构和训练算法。所提出的方法，可分离的PINN (SPINN)，在多维PDE中按轴逐个处理，从而显著减少了网络传播的数量，不同于传统PINNs中的逐点处理。我们还提出使用正向模式自动微分来降低计算PDE残差的计算成本，从而在单个普通GPU上可以使用大量的配点(>10^7)。

    Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. However, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate highly complex solution functions. The number of training points (collocation points) required on these challenging PDEs grows substantially, but it is severely limited due to the expensive computational costs and heavy memory overhead. To overcome this issue, we propose a network architecture and training algorithm for PINNs. The proposed method, separable PINN (SPINN), operates on a per-axis basis to significantly reduce the number of network propagations in multi-dimensional PDEs unlike point-wise processing in conventional PINNs. We also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points (>10^7) on a single commodity GPU. The
    
[^46]: 基于分层强化学习的城市自动驾驶行动和轨迹规划

    Action and Trajectory Planning for Urban Autonomous Driving with Hierarchical Reinforcement Learning. (arXiv:2306.15968v1 [cs.RO])

    [http://arxiv.org/abs/2306.15968](http://arxiv.org/abs/2306.15968)

    本文提出了一种使用分层强化学习方法的城市自动驾驶行动和轨迹规划器，通过感知信息和分层模型来学习和规划自动驾驶车辆行为，并解决复杂城市场景下的行驶任务和动态环境变化的挑战。

    

    强化学习在简单驾驶场景中为自动驾驶车辆的规划和决策取得了可观的进展。然而，现有的强化学习算法在复杂的城市场景中无法学习关键的驾驶技能。首先，城市驾驶场景要求自动驾驶车辆处理多个驾驶任务，而传统的强化学习算法无法胜任。其次，城市场景中其他车辆的存在导致环境不断变化，这对强化学习算法来规划自动驾驶车辆的行动和轨迹构成了挑战。在这项工作中，我们提出了一种使用分层强化学习方法的行动和轨迹规划器(atHRL)，该方法通过使用激光雷达和鸟瞰图的感知来建模代理行为。所提出的atHRL方法通过使用分层DDPG算法，在连续情况下学习做出关于代理未来轨迹的决策，并基于此计算目标路径点。

    Reinforcement Learning (RL) has made promising progress in planning and decision-making for Autonomous Vehicles (AVs) in simple driving scenarios. However, existing RL algorithms for AVs fail to learn critical driving skills in complex urban scenarios. First, urban driving scenarios require AVs to handle multiple driving tasks of which conventional RL algorithms are incapable. Second, the presence of other vehicles in urban scenarios results in a dynamically changing environment, which challenges RL algorithms to plan the action and trajectory of the AV. In this work, we propose an action and trajectory planner using Hierarchical Reinforcement Learning (atHRL) method, which models the agent behavior in a hierarchical model by using the perception of the lidar and birdeye view. The proposed atHRL method learns to make decisions about the agent's future trajectory and computes target waypoints under continuous settings based on a hierarchical DDPG algorithm. The waypoints planned by the 
    
[^47]: 通过快速融合Gromov化实现图插值

    Graph Interpolation via Fast Fused-Gromovization. (arXiv:2306.15963v1 [cs.LG])

    [http://arxiv.org/abs/2306.15963](http://arxiv.org/abs/2306.15963)

    本文提出了一种通过快速融合Gromov化的方法，用于图插值和图数据增强。通过考虑图结构和信号之间的相互作用，我们提出了一种匹配节点之间的最优策略来解决这一问题。为了提高可扩展性，我们引入了一种放松的FGW求解器来加速算法的收敛速度。

    

    图数据增强已被证明对于增强图神经网络（GNN）的泛化能力和鲁棒性在图级分类方面是有效的。然而，现有的方法主要集中在独立地增强图信号空间和图结构空间，忽视了它们的共同作用。本文通过将问题形式化为一个最优传输问题，旨在找到一种匹配图之间节点的最优策略，考虑图结构和信号之间的相互作用，以解决这个限制。为了解决这个问题，我们提出了一种新颖的图mixup算法，称为FGWMixup，它利用融合Gromov-Wasserstein（FGW）度量空间来识别源图的“中点”。为了提高我们方法的可扩展性，我们引入了一个放松的FGW求解器，通过将收敛速度从O(t^-1)加速到O(t^-2)，提高了FGWMixup的收敛速度。在五个数据集上进行了大量实验证明了我们的方法的有效性。

    Graph data augmentation has proven to be effective in enhancing the generalizability and robustness of graph neural networks (GNNs) for graph-level classifications. However, existing methods mainly focus on augmenting the graph signal space and the graph structure space independently, overlooking their joint interaction. This paper addresses this limitation by formulating the problem as an optimal transport problem that aims to find an optimal strategy for matching nodes between graphs considering the interactions between graph structures and signals. To tackle this problem, we propose a novel graph mixup algorithm dubbed FGWMixup, which leverages the Fused Gromov-Wasserstein (FGW) metric space to identify a "midpoint" of the source graphs. To improve the scalability of our approach, we introduce a relaxed FGW solver that accelerates FGWMixup by enhancing the convergence rate from $\mathcal{O}(t^{-1})$ to $\mathcal{O}(t^{-2})$. Extensive experiments conducted on five datasets, utilizin
    
[^48]: 通过跳过零元素降低卷积层的计算复杂度

    Reduce Computational Complexity for Convolutional Layers by Skipping Zeros. (arXiv:2306.15951v1 [cs.LG])

    [http://arxiv.org/abs/2306.15951](http://arxiv.org/abs/2306.15951)

    本文提出了C-K-S算法，通过修剪滤波器和转换稀疏张量为稠密张量的方式，跳过卷积层中的0元素，从而降低了计算复杂度。实验证明，C-K-S相对于PyTorch具有优势。

    

    深度神经网络依赖并行处理器进行加速。为了为其设计运算符，需要不仅有优化算法以降低复杂度，还需要充分利用硬件资源。卷积层主要包含三种运算符：前向传播的卷积，反向传播的反卷积和膨胀卷积。当执行这些运算时，始终会向张量中添加0元素，导致冗余计算。本文提出了C-K-S算法（ConvV2, KS-deconv, Sk-dilated），以两种方式跳过这些0元素：修剪滤波器以排除填充的0元素；将稀疏张量转换为稠密张量，避免在反卷积和膨胀卷积中插入0元素。与普通卷积相比，反卷积由于其复杂性而难以加速。本文提供了C-K-S的高性能GPU实现，并通过与PyTorch的比较验证了其有效性。根据实验结果，在某些情况下，C-K-S相对于PyTorch具有优势。

    Deep neural networks rely on parallel processors for acceleration. To design operators for them, it requires not only good algorithm to reduce complexity, but also sufficient utilization of hardwares. Convolutional layers mainly contain 3 kinds of operators: convolution in forward propagation, deconvolution and dilated-convolution in backward propagation. When executing these operators, 0s are always added to tensors, causing redundant calculations. This paper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these 0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors to dense tensors, to avoid inserted 0s in deconvolution and dilated-convolution. In contrast to regular convolution, deconvolution is hard to accelerate due to its complicacy. This paper provides high-performance GPU implementations of C-K-S, and verifies their effectiveness with comparison to PyTorch. According to the experiments, C-K-S has advantages over PyTorch in certain cas
    
[^49]: Pb-Hash: 分区b位哈希

    Pb-Hash: Partitioned b-bit Hashing. (arXiv:2306.15944v1 [cs.LG])

    [http://arxiv.org/abs/2306.15944](http://arxiv.org/abs/2306.15944)

    Pb-Hash提出了一种分区b位哈希的方法，通过将B位哈希分成m个块来重复使用已有的哈希，能够显著减小模型的大小。

    

    许多哈希算法，包括minwise哈希（MinHash），一次置换哈希（OPH）和一致加权采样（CWS），生成B位整数。对于每个数据向量的k个哈希，存储空间将是B×k位；当用于大规模学习时，模型大小将是2^B×k，这可能很昂贵。一种标准策略是仅使用B位中的最低b位，并略微增加哈希的数量k。在这项研究中，我们提出通过将B位分成m个块，例如b×m=B，来重复使用哈希。对应地，模型大小变为m×2^b×k，这可能比原来的2^B×k要小得多。我们的理论分析显示，通过将哈希值分成m个块，准确性会下降。换句话说，使用B/m位的m个块将不如直接使用B位精确。这是由于通过重新使用相同的哈希值引起的相关性。另一方面，

    Many hashing algorithms including minwise hashing (MinHash), one permutation hashing (OPH), and consistent weighted sampling (CWS) generate integers of $B$ bits. With $k$ hashes for each data vector, the storage would be $B\times k$ bits; and when used for large-scale learning, the model size would be $2^B\times k$, which can be expensive. A standard strategy is to use only the lowest $b$ bits out of the $B$ bits and somewhat increase $k$, the number of hashes. In this study, we propose to re-use the hashes by partitioning the $B$ bits into $m$ chunks, e.g., $b\times m =B$. Correspondingly, the model size becomes $m\times 2^b \times k$, which can be substantially smaller than the original $2^B\times k$.  Our theoretical analysis reveals that by partitioning the hash values into $m$ chunks, the accuracy would drop. In other words, using $m$ chunks of $B/m$ bits would not be as accurate as directly using $B$ bits. This is due to the correlation from re-using the same hash. On the other h
    
[^50]: 可解释的变分自编码器学习概念在手机网络中的异常检测

    Interpretable Anomaly Detection in Cellular Networks by Learning Concepts in Variational Autoencoders. (arXiv:2306.15938v1 [cs.LG])

    [http://arxiv.org/abs/2306.15938](http://arxiv.org/abs/2306.15938)

    本文提出了一种利用变分自编码器学习概念在手机网络中解释异常检测的方法，通过重构损失和Z分数来检测异常，并通过K-means算法增强表示学习，实现了异常的可解释性。该框架为手机网络中的异常检测提供了更快且自主的解决方案，并展示了深度学习算法处理大数据的潜力。

    

    本文以可解释的方式解决了手机网络中的异常检测挑战，并提出了一种利用变分自编码器(VAEs)学习每个关键性能指标(KPI)的潜在空间的可解释表示的新方法。这使得可以基于重构损失和Z分数来检测异常。我们通过使用K-means算法增强表示学习，通过附加信息中心点(c)确保异常的可解释性。通过分析特定KPI的潜在维度中的模式，我们评估了模型的性能，并展示了其可解释性和异常。该提议的框架为在手机网络中检测异常提供了更快且自主的解决方案，并展示了基于深度学习的算法处理大数据的潜力。

    This paper addresses the challenges of detecting anomalies in cellular networks in an interpretable way and proposes a new approach using variational autoencoders (VAEs) that learn interpretable representations of the latent space for each Key Performance Indicator (KPI) in the dataset. This enables the detection of anomalies based on reconstruction loss and Z-scores. We ensure the interpretability of the anomalies via additional information centroids (c) using the K-means algorithm to enhance representation learning. We evaluate the performance of the model by analyzing patterns in the latent dimension for specific KPIs and thereby demonstrate the interpretability and anomalies. The proposed framework offers a faster and autonomous solution for detecting anomalies in cellular networks and showcases the potential of deep learning-based algorithms in handling big data.
    
[^51]: 对于模型为基础的适应性的好奇回放

    Curious Replay for Model-based Adaptation. (arXiv:2306.15934v1 [cs.LG])

    [http://arxiv.org/abs/2306.15934](http://arxiv.org/abs/2306.15934)

    好奇回放是一种针对模型为基础的代理的优先经验回放方法，通过使用好奇度基础的优先信号，它提高了探索性能，并在Crafter基准测试中取得了更好的成绩。

    

    代理必须能够在环境改变时快速适应。我们发现现有的基于模型的强化学习代理在这方面做得不好，部分原因是它们如何利用过去的经验来训练其世界模型。在这里，我们提出了一种称为好奇回放的方法，它是针对基于模型的代理的一种优先经验回放方法，通过使用好奇度基础的优先信号。使用好奇回放的代理在受到动物行为启发的探索范式和Crafter基准测试中表现出改进的性能。带有好奇回放的DreamerV3在Crafter上超越了最先进的性能，实现了19.4的平均分数，大大改善了之前DreamerV3使用均匀回放时的最高分数14.5，并且在Deepmind Control Suite上的性能也相似。好奇回放的代码可以在https://github.com/AutonomousAgentsLab/curiousreplay上找到。

    Agents must be able to adapt quickly as an environment changes. We find that existing model-based reinforcement learning agents are unable to do this well, in part because of how they use past experiences to train their world model. Here, we present Curious Replay -- a form of prioritized experience replay tailored to model-based agents through use of a curiosity-based priority signal. Agents using Curious Replay exhibit improved performance in an exploration paradigm inspired by animal behavior and on the Crafter benchmark. DreamerV3 with Curious Replay surpasses state-of-the-art performance on Crafter, achieving a mean score of 19.4 that substantially improves on the previous high score of 14.5 by DreamerV3 with uniform replay, while also maintaining similar performance on the Deepmind Control Suite. Code for Curious Replay is available at https://github.com/AutonomousAgentsLab/curiousreplay
    
[^52]: 通过验证和纠正提示进行数据生成文本生成

    You Can Generate It Again: Data-to-text Generation with Verification and Correction Prompting. (arXiv:2306.15933v1 [cs.CL])

    [http://arxiv.org/abs/2306.15933](http://arxiv.org/abs/2306.15933)

    本文提出了一种多步骤生成、验证和纠正的数据生成文本方法，通过专门的错误指示提示来改善输出质量。

    

    尽管现有模型取得了显著进展，从结构化数据输入生成文本描述（称为数据生成文本）仍然是一个具有挑战性的任务。在本文中，我们提出了一种新的方法，通过引入包括生成、验证和纠正阶段的多步骤过程，超越了传统的一次性生成方法。我们的方法，VCP（验证和纠正提示），从模型生成初始输出开始。然后，我们继续验证所生成文本的不同方面的正确性。验证步骤的观察结果被转化为专门的错误指示提示，该提示指示模型在重新生成输出时考虑已识别的错误。为了增强模型的纠正能力，我们开发了一个经过精心设计的培训过程。该过程使模型能够融入错误指示提示的反馈，从而改善输出生成。

    Despite significant advancements in existing models, generating text descriptions from structured data input, known as data-to-text generation, remains a challenging task. In this paper, we propose a novel approach that goes beyond traditional one-shot generation methods by introducing a multi-step process consisting of generation, verification, and correction stages. Our approach, VCP(Verification and Correction Prompting), begins with the model generating an initial output. We then proceed to verify the correctness of different aspects of the generated text. The observations from the verification step are converted into a specialized error-indication prompt, which instructs the model to regenerate the output while considering the identified errors. To enhance the model's correction ability, we have developed a carefully designed training procedure. This procedure enables the model to incorporate feedback from the error-indication prompt, resulting in improved output generation. Throu
    
[^53]: NIPD（一种基于真实世界非独立与同分布数据的联邦学习人体检测基准）

    NIPD: A Federated Learning Person Detection Benchmark Based on Real-World Non-IID Data. (arXiv:2306.15932v1 [cs.CV])

    [http://arxiv.org/abs/2306.15932](http://arxiv.org/abs/2306.15932)

    NIPD是一个基于真实世界非独立与同分布数据的联邦学习人体检测基准，开源了一个非独立和同分布的物联网人体检测数据集。

    

    联邦学习（FL）是一种保护隐私的分布式机器学习方法，已被广泛应用于无线通信网络。FL使得物联网客户端可以在防止隐私泄漏的情况下获得良好训练的模型。如果将FL与边缘设备结合使用，可以在边缘直接处理视频数据，从而将人员检测部署在计算能力有限的边缘设备上。然而，由于不同相机的硬件和部署场景不同，相机所收集的数据呈现非独立和同分布（non-IID）的特性，FL聚合得到的全局模型的效果较差。同时，现有研究缺乏用于研究物联网相机上非独立和同分布问题的公共数据集。因此，我们开源了一个非独立和同分布的物联网人体检测（NIPD）数据集，该数据集由五个不同的相机收集而来。据我们所知，这是第一个真正基于设备的非独立和同分布的人体检测数据集。

    Federated learning (FL), a privacy-preserving distributed machine learning, has been rapidly applied in wireless communication networks. FL enables Internet of Things (IoT) clients to obtain well-trained models while preventing privacy leakage. Person detection can be deployed on edge devices with limited computing power if combined with FL to process the video data directly at the edge. However, due to the different hardware and deployment scenarios of different cameras, the data collected by the camera present non-independent and identically distributed (non-IID), and the global model derived from FL aggregation is less effective. Meanwhile, existing research lacks public data set for real-world FL object detection, which is not conducive to studying the non-IID problem on IoT cameras. Therefore, we open source a non-IID IoT person detection (NIPD) data set, which is collected from five different cameras. To our knowledge, this is the first true device-based non-IID person detection 
    
[^54]: 从所有背景信息中学习动态图以准确预测兴趣点的访问量

    Learning Dynamic Graphs from All Contextual Information for Accurate Point-of-Interest Visit Forecasting. (arXiv:2306.15927v1 [cs.LG])

    [http://arxiv.org/abs/2306.15927](http://arxiv.org/abs/2306.15927)

    提出了一种名为忙碌图神经网络（BysGNN）的临时图神经网络，利用所有背景信息和时间序列数据来学习兴趣点之间的多背景相关性，以实现准确的访问量预测。

    

    在城市地区预测兴趣点（POI）的访问量对于规划和决策是至关重要的，涉及领域包括城市规划、交通管理、公共卫生和社会研究。尽管这个问题可以被看作是多元时间序列预测任务，但目前的方法不能充分利用POIs之间不断变化的多背景相关性。因此，我们提出了一种称为忙碌图神经网络（BysGNN）的临时图神经网络，旨在学习和揭示POIs之间的潜在多背景相关性，以准确预测访问量。与其他仅使用时间序列数据来学习动态图的方法不同，BysGNN利用所有背景信息和时间序列数据来学习准确的动态图表示。通过结合所有背景、时间和空间信号，我们观察到预测准确性显著提高，超越了当前最先进的方法。

    Forecasting the number of visits to Points-of-Interest (POI) in an urban area is critical for planning and decision-making for various application domains, from urban planning and transportation management to public health and social studies. Although this forecasting problem can be formulated as a multivariate time-series forecasting task, the current approaches cannot fully exploit the ever-changing multi-context correlations among POIs. Therefore, we propose Busyness Graph Neural Network (BysGNN), a temporal graph neural network designed to learn and uncover the underlying multi-context correlations between POIs for accurate visit forecasting. Unlike other approaches where only time-series data is used to learn a dynamic graph, BysGNN utilizes all contextual information and time-series data to learn an accurate dynamic graph representation. By incorporating all contextual, temporal, and spatial signals, we observe a significant improvement in our forecasting accuracy over state-of-t
    
[^55]: 大多数语言模型也可以成为诗人：一个AI写作助手和有限文本生成工具

    Most Language Models can be Poets too: An AI Writing Assistant and Constrained Text Generation Studio. (arXiv:2306.15926v1 [cs.CL])

    [http://arxiv.org/abs/2306.15926](http://arxiv.org/abs/2306.15926)

    这项研究展示了如何通过在语言模型中应用过滤函数来生成有限约束文本，并提出了一个AI写作助手工具，可以根据用户的需求生成带有各种约束条件的文本。

    

    尽管有关有限自然语言生成领域的快速进展，但对已被词汇、语义或音韵约束的语言模型的潜力研究时间很少。我们发现，大多数语言模型即使在显著约束下也能生成引人入胜的文本。我们提出了一种简单而普适的技术，通过在生成文本单元之前组合应用过滤函数到语言模型的词汇，来修改语言模型的输出。这种方法是即插即用的，不需要对模型进行修改。为展示这种技术的价值，我们介绍了一个易于使用的AI写作助手，名为有限文本生成工具（CTGS）。CTGS允许用户生成或选择具有各种约束条件的文本，例如禁止某个字母，强制生成的单词具有一定的音节数，或强制生成与给定上下文相关的单词等。

    Despite rapid advancement in the field of Constrained Natural Language Generation, little time has been spent on exploring the potential of language models which have had their vocabularies lexically, semantically, and/or phonetically constrained. We find that most language models generate compelling text even under significant constraints. We present a simple and universally applicable technique for modifying the output of a language model by compositionally applying filter functions to the language models vocabulary before a unit of text is generated. This approach is plug-and-play and requires no modification to the model. To showcase the value of this technique, we present an easy to use AI writing assistant called Constrained Text Generation Studio (CTGS). CTGS allows users to generate or choose from text with any combination of a wide variety of constraints, such as banning a particular letter, forcing the generated words to have a certain number of syllables, and/or forcing the 
    
[^56]: 运算学习中的维度诅咒

    The curse of dimensionality in operator learning. (arXiv:2306.15924v1 [cs.LG])

    [http://arxiv.org/abs/2306.15924](http://arxiv.org/abs/2306.15924)

    算子学习中存在维度诅咒，但对于由Hamilton-Jacobi方程定义的解算子可以克服维度诅咒。

    

    神经算子架构利用神经网络来近似映射函数空间之间的算子，可以用于通过模拟加速模型评估，或者从数据中发现模型。因此，这一方法在近年来受到越来越多的关注，引发了算子学习领域的快速发展。本文的第一项贡献是证明了对于一般的只由其 $C^r$ 或 Lipschitz 正则性特征化的算子类，算子学习遭受了维度诅咒，这里通过无穷维输入和输出函数空间的表征来精确定义维度诅咒。该结果适用于包括 PCA-Net、DeepONet 和 FNO 在内的多种现有神经算子。本文的第二项贡献是证明了对于由Hamilton-Jacobi方程定义的解算子，可以克服一般的维度诅咒；这是通过引入新的表示方法来实现的。

    Neural operator architectures employ neural networks to approximate operators mapping between Banach spaces of functions; they may be used to accelerate model evaluations via emulation, or to discover models from data. Consequently, the methodology has received increasing attention over recent years, giving rise to the rapidly growing field of operator learning. The first contribution of this paper is to prove that for general classes of operators which are characterized only by their $C^r$- or Lipschitz-regularity, operator learning suffers from a curse of dimensionality, defined precisely here in terms of representations of the infinite-dimensional input and output function spaces. The result is applicable to a wide variety of existing neural operators, including PCA-Net, DeepONet and the FNO. The second contribution of the paper is to prove that the general curse of dimensionality can be overcome for solution operators defined by the Hamilton-Jacobi equation; this is achieved by lev
    
[^57]: 关于神经网络所捕获的信息：与记忆和泛化的联系

    On information captured by neural networks: connections with memorization and generalization. (arXiv:2306.15918v1 [cs.LG])

    [http://arxiv.org/abs/2306.15918](http://arxiv.org/abs/2306.15918)

    研究了神经网络在训练过程中捕获的信息，探讨了神经网络在特殊、模糊或少数子群体示例上的行为，在泛化中提供了重要的信息和理解。

    

    尽管深度学习很受欢迎并取得了成功，但对于神经网络何时、如何以及为什么能够泛化到未见示例的情况下，人们的理解仍然有限。因为学习可以被看作是从数据中提取信息，所以我们正式研究了神经网络在训练过程中捕获的信息。具体而言，我们从信息论的角度出发，从噪声标签的存在下观察学习，并导出了将标签噪声信息限制在权重中的学习算法。然后，我们定义了一个个体样本对深度网络训练提供的唯一信息的概念，为神经网络在非典型、模糊或属于少数子群体的示例上的行为提供了一些启示。我们通过导出非平凡的泛化差距界限，将示例信息性与泛化联系起来。最后，通过研究知识提取，我们强调了数据和标签复杂性在泛化中的重要作用。总的来说，我们的发现有助于理解神经网络的创新和贡献。

    Despite the popularity and success of deep learning, there is limited understanding of when, how, and why neural networks generalize to unseen examples. Since learning can be seen as extracting information from data, we formally study information captured by neural networks during training. Specifically, we start with viewing learning in presence of noisy labels from an information-theoretic perspective and derive a learning algorithm that limits label noise information in weights. We then define a notion of unique information that an individual sample provides to the training of a deep network, shedding some light on the behavior of neural networks on examples that are atypical, ambiguous, or belong to underrepresented subpopulations. We relate example informativeness to generalization by deriving nonvacuous generalization gap bounds. Finally, by studying knowledge distillation, we highlight the important role of data and label complexity in generalization. Overall, our findings contr
    
[^58]: 用随机系数岭回归进行迁移学习

    Transfer Learning with Random Coefficient Ridge Regression. (arXiv:2306.15915v1 [stat.ML])

    [http://arxiv.org/abs/2306.15915](http://arxiv.org/abs/2306.15915)

    本文提出了在迁移学习中使用随机系数岭回归的方法，通过最小化估计风险或预测风险来确定目标模型和源模型的回归系数的最优权重，并使用随机矩阵理论得出了最优权重的极限值。

    

    在高维环境中，随机系数岭回归提供了固定系数回归的一个重要替代方案，当效果被期望为小但不为零时。本文考虑在迁移学习的情况下估计和预测随机系数岭回归，在目标模型的观测值之外，还提供了来自不同但可能相关的回归模型的源样本。源模型对目标模型的信息量可以通过回归系数之间的相关性来量化。本文提出了两个目标模型回归系数的估计器，它们是目标模型和源模型岭估计的加权和，其中权重可以通过最小化经验估计风险或预测风险来确定。利用随机矩阵理论，在$p/n \rightarrow \gamma$的条件下，得出了最优权重的极限值，其中$p$是...

    Ridge regression with random coefficients provides an important alternative to fixed coefficients regression in high dimensional setting when the effects are expected to be small but not zeros. This paper considers estimation and prediction of random coefficient ridge regression in the setting of transfer learning, where in addition to observations from the target model, source samples from different but possibly related regression models are available. The informativeness of the source model to the target model can be quantified by the correlation between the regression coefficients. This paper proposes two estimators of regression coefficients of the target model as the weighted sum of the ridge estimates of both target and source models, where the weights can be determined by minimizing the empirical estimation risk or prediction risk. Using random matrix theory, the limiting values of the optimal weights are derived under the setting when $p/n \rightarrow \gamma$, where $p$ is the 
    
[^59]: DCT: 大离散动作空间强化学习的双通道动作嵌入训练

    DCT: Dual Channel Training of Action Embeddings for Reinforcement Learning with Large Discrete Action Spaces. (arXiv:2306.15913v1 [cs.LG])

    [http://arxiv.org/abs/2306.15913](http://arxiv.org/abs/2306.15913)

    本文提出了一个双通道动作嵌入训练的框架，能够在大离散动作空间中学习稳健策略，并成功应用在2D迷宫环境和真实世界电子商务任务中。

    

    在面临维度灾难的嘈杂环境中，学习稳健策略并广义化大离散动作空间是智能系统面临的一个挑战。本文提出了一种新颖的框架，能够高效地学习动作嵌入，同时实现对原始动作的重构以及对未来状态的预测。我们使用编码器-解码器架构进行动作嵌入，并通过双通道损失来平衡动作重构和状态预测精度。我们将训练好的解码器与标准强化学习算法结合使用，以在嵌入空间中生成动作。实验结果表明，我们的架构在一个具有4000多个离散噪声动作的2D迷宫环境和使用真实世界电子商务交易数据的产品推荐任务中能够胜过两个竞争基线模型。

    The ability to learn robust policies while generalizing over large discrete action spaces is an open challenge for intelligent systems, especially in noisy environments that face the curse of dimensionality. In this paper, we present a novel framework to efficiently learn action embeddings that simultaneously allow us to reconstruct the original action as well as to predict the expected future state. We describe an encoder-decoder architecture for action embeddings with a dual channel loss that balances between action reconstruction and state prediction accuracy. We use the trained decoder in conjunction with a standard reinforcement learning algorithm that produces actions in the embedding space. Our architecture is able to outperform two competitive baselines in two diverse environments: a 2D maze environment with more than 4000 discrete noisy actions, and a product recommendation task that uses real-world e-commerce transaction data. Empirical results show that the model results in 
    
[^60]: RL$^3$:通过RL内部的RL$^2$提升元强化学习方法

    RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])

    [http://arxiv.org/abs/2306.15909](http://arxiv.org/abs/2306.15909)

    RL$^3$是一种原则性混合方法，通过将传统强化学习学到的任务特定动作值作为元强化学习神经网络的输入，提高了元强化学习的性能。

    

    元强化学习（meta-RL）方法，如RL$^2$，已经成为学习针对给定任务分布的数据高效的强化学习算法的有希望的方法。然而，这些强化学习算法在长期任务和超出分布任务方面存在困难，因为它们依赖于递归神经网络来处理经验序列，而不是将它们总结为一般的强化学习组件，例如价值函数。此外，即使是transformers在训练和推理成本变得禁止之前也对它们可以有效推理的历史长度有实际限制。相比之下，传统的强化学习算法在数据效率方面不足，因为它们没有利用领域知识，但随着更多数据的可用性，它们会收敛到最优策略。在本文中，我们提出了RL$^3$，一种组合了传统强化学习和元强化学习的原则性混合方法，通过将通过传统强化学习学习到的特定任务动作值作为元强化学习神经网络的一个输入。

    Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
    
[^61]: 南佛罗里达州水位预测的深度学习模型

    Deep Learning Models for Water Stage Predictions in South Florida. (arXiv:2306.15907v1 [cs.LG])

    [http://arxiv.org/abs/2306.15907](http://arxiv.org/abs/2306.15907)

    本论文利用深度学习模型训练代理模型，快速预测南佛罗里达州迈阿密河下游的水位，并与基于物理的模型进行比较。

    

    模拟和预测河流系统的水位对于洪水警报、水力操作和洪水减轻至关重要。在工程领域中，使用HEC-RAS、MIKE和SWMM等工具建立详细的基于物理的水文和水力计算模型来模拟整个流域，从而预测系统中任意点的水位。然而，这些基于物理的模型计算量大，尤其对于大流域和长时间模拟来说。为了解决这个问题，我们训练了几个深度学习（DL）模型作为代理模型，快速预测水位。本文以南佛罗里达州迈阿密河的下游水位为案例研究。数据集来自南佛罗里达水管理区（SFWMD）的DBHYDRO数据库，时间跨度为2010年1月1日至2020年12月31日。广泛的实验表明，DL模型的性能与基于物理的模型相当。

    Simulating and predicting water levels in river systems is essential for flood warnings, hydraulic operations, and flood mitigations. In the engineering field, tools such as HEC-RAS, MIKE, and SWMM are used to build detailed physics-based hydrological and hydraulic computational models to simulate the entire watershed, thereby predicting the water stage at any point in the system. However, these physics-based models are computationally intensive, especially for large watersheds and for longer simulations. To overcome this problem, we train several deep learning (DL) models for use as surrogate models to rapidly predict the water stage. The downstream stage of the Miami River in South Florida is chosen as a case study for this paper. The dataset is from January 1, 2010, to December 31, 2020, downloaded from the DBHYDRO database of the South Florida Water Management District (SFWMD). Extensive experiments show that the performance of the DL models is comparable to that of the physics-bas
    
[^62]: 协同过滤中维度无关的困难负样本混合方法

    Dimension Independent Mixup for Hard Negative Sample in Collaborative Filtering. (arXiv:2306.15905v1 [cs.IR])

    [http://arxiv.org/abs/2306.15905](http://arxiv.org/abs/2306.15905)

    本文提出了一种协同过滤训练中维度无关的困难负样本混合方法（DINS），通过对采样区域的新视角进行重新审视来改进现有的采样方法。实验证明，DINS优于其他负采样方法，证实了其有效性和优越性。

    

    协同过滤（CF）是一种广泛应用的技术，可以基于过去的互动预测用户的偏好。负采样在使用隐式反馈训练基于CF的模型时起到至关重要的作用。本文提出了一种基于采样区域的新视角来重新审视现有的采样方法。我们指出，目前的采样方法主要集中在点采样或线采样上，缺乏灵活性，并且有相当大一部分困难采样区域未被探索。为了解决这个限制，我们提出了一种维度无关的困难负样本混合方法（DINS），它是第一个针对训练基于CF的模型的区域采样方法。DINS包括三个模块：困难边界定义、维度无关混合和多跳池化。在真实世界的数据集上进行的实验证明，DINS优于其他负采样方法，证明了它的有效性和优越性。

    Collaborative filtering (CF) is a widely employed technique that predicts user preferences based on past interactions. Negative sampling plays a vital role in training CF-based models with implicit feedback. In this paper, we propose a novel perspective based on the sampling area to revisit existing sampling methods. We point out that current sampling methods mainly focus on Point-wise or Line-wise sampling, lacking flexibility and leaving a significant portion of the hard sampling area un-explored. To address this limitation, we propose Dimension Independent Mixup for Hard Negative Sampling (DINS), which is the first Area-wise sampling method for training CF-based models. DINS comprises three modules: Hard Boundary Definition, Dimension Independent Mixup, and Multi-hop Pooling. Experiments with real-world datasets on both matrix factorization and graph-based models demonstrate that DINS outperforms other negative sampling methods, establishing its effectiveness and superiority. Our wo
    
[^63]: 个别和结构化图信息瓶颈对于域外通用化的重要性

    Individual and Structural Graph Information Bottlenecks for Out-of-Distribution Generalization. (arXiv:2306.15902v1 [cs.LG])

    [http://arxiv.org/abs/2306.15902](http://arxiv.org/abs/2306.15902)

    这项工作提出了一种统一框架，个别和结构化图信息瓶颈（IS-GIB），用于解决域外图像通用化中的问题，通过丢弃虚假特征和利用结构关联来提高性能。

    

    域外图像通用化对于许多实际应用来说都是至关重要的。现有方法忽视了丢弃与标签无关的输入中的虚假或噪声特征。此外，它们主要进行实例级别的类不变图学习，并未充分利用图实例之间的结构化类别关系。在本研究中，我们致力于在一个统一的框架中解决这些问题，称为个别和结构化图信息瓶颈（IS-GIB）。为了消除由分布偏移引起的类别虚假特征，我们提出个别图信息瓶颈（I-GIB），通过最小化输入图与其嵌入之间的互信息来丢弃不相关信息。为了利用结构内部和跨域之间的关联，我们提出了结构化图信息瓶颈（S-GIB）。特别是对于一批具有多个域的图，S-GIB首先计算成对的输入-输入、嵌入-嵌入等。

    Out-of-distribution (OOD) graph generalization are critical for many real-world applications. Existing methods neglect to discard spurious or noisy features of inputs, which are irrelevant to the label. Besides, they mainly conduct instance-level class-invariant graph learning and fail to utilize the structural class relationships between graph instances. In this work, we endeavor to address these issues in a unified framework, dubbed Individual and Structural Graph Information Bottlenecks (IS-GIB). To remove class spurious feature caused by distribution shifts, we propose Individual Graph Information Bottleneck (I-GIB) which discards irrelevant information by minimizing the mutual information between the input graph and its embeddings. To leverage the structural intra- and inter-domain correlations, we propose Structural Graph Information Bottleneck (S-GIB). Specifically for a batch of graphs with multiple domains, S-GIB first computes the pair-wise input-input, embedding-embedding, a
    
[^64]: 大型语言模型作为属性化训练数据生成器：多样性和偏差的故事

    Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias. (arXiv:2306.15895v1 [cs.CL])

    [http://arxiv.org/abs/2306.15895](http://arxiv.org/abs/2306.15895)

    本论文研究了大型语言模型作为属性化训练数据生成器的应用。通过使用具有多样性属性的提示，我们能够生成多样化且归因的数据。研究表明，在高基数和多样领域的数据集中，使用属性化提示对生成模型性能有积极影响。此外，论文还展示了关于偏差、多样性和效率的全面实证研究结果，并得出了三个关键观察：系统性偏差存在于生成数据中，多样性和效率之间存在权衡，属性化训练数据生成可以改善模型性能。

    

    近期大型语言模型(LLMs)被广泛应用于各种自然语言处理(NLP)任务的训练数据生成。尽管之前的研究探索了使用生成数据进行模型训练的不同方法，但它们通常依赖于简单的类别条件提示，这可能限制了生成数据的多样性，并且继承了LLM的系统性偏差。因此，我们研究了使用具有多样属性的提示(例如指定长度和风格等属性)进行训练数据生成，这有潜力产生多样和归因的生成数据。我们的研究关注具有高基数和多样领域的数据集，在这方面，我们证明了属性化提示在生成模型性能方面优于简单的类别条件提示。此外，我们还展示了一项包括偏差、多样性和效率等关键方面的全面实证研究，并强调了三个关键观察：首先，系统性偏差在生成数据中存在；其次，多样性和效率之间存在权衡；最后，进行属性化训练数据生成可以改善模型性能。

    Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, sy
    
[^65]: 渐近保持的卷积Deep Operator网络捕捉多尺度线性输运方程的扩散行为

    Asymptotic-Preserving Convolutional DeepONets Capture the Diffusive Behavior of the Multiscale Linear Transport Equations. (arXiv:2306.15891v1 [cs.LG])

    [http://arxiv.org/abs/2306.15891](http://arxiv.org/abs/2306.15891)

    本文介绍了两种新型的渐近保持的卷积Deep Operator网络（APCONs），用于解决多尺度时变线性输运问题。该方法通过采用多个局部卷积操作、池化和激活操作来捕捉线性输运问题的扩散行为，并验证了方法的有效性。

    

    本文介绍了两种新型的渐近保持的卷积Deep Operator网络（APCONs），旨在解决多尺度时变线性输运问题。我们观察到，使用修改过的多层感知器（MLP）的基本物理约束DeepONets可能在保持期望的宏观行为上表现出不稳定性。因此，需要使用渐近保持的损失函数。受扩散方程中的热核的启发，我们提出了一种新的架构，称为卷积Deep Operator网络，它在每个滤波器层中采用多个局部卷积操作而不是全局热核，并结合池化和激活操作。我们的APCON方法的参数数量与网格大小无关，并能够捕捉线性输运问题的扩散行为。最后，通过几个数值实例验证了我们方法的有效性。

    In this paper, we introduce two types of novel Asymptotic-Preserving Convolutional Deep Operator Networks (APCONs) designed to address the multiscale time-dependent linear transport problem. We observe that the vanilla physics-informed DeepONets with modified MLP may exhibit instability in maintaining the desired limiting macroscopic behavior. Therefore, this necessitates the utilization of an asymptotic-preserving loss function. Drawing inspiration from the heat kernel in the diffusion equation, we propose a new architecture called Convolutional Deep Operator Networks, which employ multiple local convolution operations instead of a global heat kernel, along with pooling and activation operations in each filter layer. Our APCON methods possess a parameter count that is independent of the grid size and are capable of capturing the diffusive behavior of the linear transport problem. Finally, we validate the effectiveness of our methods through several numerical examples.
    
[^66]: 深度学习在反应和逆合成预测中的统一视角：现状与未来挑战

    A Unified View of Deep Learning for Reaction and Retrosynthesis Prediction: Current Status and Future Challenges. (arXiv:2306.15890v1 [cs.LG])

    [http://arxiv.org/abs/2306.15890](http://arxiv.org/abs/2306.15890)

    本论文提供了深度学习在反应和逆合成预测中的统一视角。通过调查现有的深度学习方法和模型，总结了它们的设计机制、优势和不足之处。同时，探讨了当前解决方案的局限性和问题本身的挑战，并提出了未来研究的有希望的方向。

    

    反应和逆合成预测是计算化学中的基本任务，最近引起了机器学习和药物发现领域的关注。各种深度学习方法已被提出来解决这些问题，并且一些方法已经取得了初步的成功。在本综述中，我们对基于先进深度学习模型的反应和逆合成预测进行了全面的调查。我们总结了现有方法的设计机制、优势和不足之处。然后，我们讨论了当前解决方案的局限性以及问题本身的挑战。最后，我们提出了有利于未来研究的方向。据我们所知，这是第一份全面和系统的综述，旨在提供反应和逆合成预测的统一理解。

    Reaction and retrosynthesis prediction are fundamental tasks in computational chemistry that have recently garnered attention from both the machine learning and drug discovery communities. Various deep learning approaches have been proposed to tackle these problems, and some have achieved initial success. In this survey, we conduct a comprehensive investigation of advanced deep learning-based models for reaction and retrosynthesis prediction. We summarize the design mechanisms, strengths, and weaknesses of state-of-the-art approaches. Then, we discuss the limitations of current solutions and open challenges in the problem itself. Finally, we present promising directions to facilitate future research. To our knowledge, this paper is the first comprehensive and systematic survey that seeks to provide a unified understanding of reaction and retrosynthesis prediction.
    
[^67]: 基于特征表示的时序关注源识别

    Sequential Attention Source Identification Based on Feature Representation. (arXiv:2306.15886v1 [cs.SI])

    [http://arxiv.org/abs/2306.15886](http://arxiv.org/abs/2306.15886)

    本文提出了一种基于序列到序列的本地化框架，通过估计用户之间的影响概率生成多个特征，并通过时序注意机制区分不同时刻预测源的重要性。该方法在解决时变感染情景下用户交互问题方面具有较高的准确性，对新场景的检测也具有良好的可扩展性。

    

    基于快照观察的源定位已被广泛研究，因其易得和低成本的特点。然而，现有方法未能及时解决时变感染情景下用户之间的交互问题。因此，这些方法在异质交互情景中的准确性下降。为解决这一关键问题，本文提出了一种基于序列到序列的本地化框架，称为基于时序图注意力的源识别（TGASI），基于归纳学习思想。具体来说，编码器通过估计两个用户之间的影响概率，集中生成多个特征，解码器通过设计的时序注意机制，在不同时刻区分预测源的重要性。值得一提的是，归纳学习思想确保了TGASI在不知道其他先验知识的情况下能在新场景中检测源，证明了TGASI的可扩展性。综合实验结果表明，TGASI在多个数据集上的表现优于其他方法。

    Snapshot observation based source localization has been widely studied due to its accessibility and low cost. However, the interaction of users in existing methods does not be addressed in time-varying infection scenarios. So these methods have a decreased accuracy in heterogeneous interaction scenarios. To solve this critical issue, this paper proposes a sequence-to-sequence based localization framework called Temporal-sequence based Graph Attention Source Identification (TGASI) based on an inductive learning idea. More specifically, the encoder focuses on generating multiple features by estimating the influence probability between two users, and the decoder distinguishes the importance of prediction sources in different timestamps by a designed temporal attention mechanism. It's worth mentioning that the inductive learning idea ensures that TGASI can detect the sources in new scenarios without knowing other prior knowledge, which proves the scalability of TGASI. Comprehensive experim
    
[^68]: 推荐系统中的块状特征交互

    Blockwise Feature Interaction in Recommendation Systems. (arXiv:2306.15881v1 [cs.IR])

    [http://arxiv.org/abs/2306.15881](http://arxiv.org/abs/2306.15881)

    该论文提出了一种称为块状特征交互 (BFI) 的方法，通过将特征交互过程分成较小的块，以显著减少内存占用和计算负担。实验证明，BFI算法在准确性上接近标准DCNv2，同时大大减少了计算开销和参数数量，为高效推荐系统的发展做出了贡献。

    

    特征交互在推荐系统中起着至关重要的作用，因为它们捕捉了用户偏好和物品特征之间的复杂关系。现有方法（如深度和交叉网络 DCNv2）可能由于其跨层操作而面临高计算需求的问题。本文提出了一种新颖的方法，称为块状特征交互 (BFI)，以帮助缓解这个问题。通过将特征交互过程分成较小的块，我们可以显著减少内存占用和计算负担。我们开发了四个变体（分别为 P、Q、T、S）的 BFI，并进行了实证比较。我们的实验结果表明，所提出的算法在与标准 DCNv2 相比时能够实现接近的准确性，同时大大减少了计算开销和参数数量。本文通过提供一种改进推荐系统的实际解决方案，为高效推荐系统的发展做出了贡献。

    Feature interactions can play a crucial role in recommendation systems as they capture complex relationships between user preferences and item characteristics. Existing methods such as Deep & Cross Network (DCNv2) may suffer from high computational requirements due to their cross-layer operations. In this paper, we propose a novel approach called blockwise feature interaction (BFI) to help alleviate this issue. By partitioning the feature interaction process into smaller blocks, we can significantly reduce both the memory footprint and the computational burden. Four variants (denoted by P, Q, T, S, respectively) of BFI have been developed and empirically compared. Our experimental results demonstrate that the proposed algorithms achieves close accuracy compared to the standard DCNv2, while greatly reducing the computational overhead and the number of parameters. This paper contributes to the development of efficient recommendation systems by providing a practical solution for improving
    
[^69]: 通过声音转换对深度语音分类进行后门攻击，真实与虚假之间的较量

    Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion. (arXiv:2306.15875v1 [cs.SD])

    [http://arxiv.org/abs/2306.15875](http://arxiv.org/abs/2306.15875)

    本文描述了一种使用声音转换生成样本特定触发器的后门攻击方法，有效地绕过了深度语音分类中存在的安全防护措施，并且不会引入额外的可听噪音。

    

    深度语音分类取得了巨大的成功，并在很大程度上推动了许多现实世界应用的出现。然而，后门攻击对其造成了新的安全威胁，特别是在不可信任的第三方平台上，攻击者预定义的触发器可以激活后门。现有语音后门攻击中的大多数触发器都是与样本无关的，即使这些触发器设计得不可察觉，它们仍然可以听到。本文探索了一种利用基于声音转换的样本特定触发器的后门攻击。具体来说，我们采用预训练的声音转换模型生成触发器，确保毒化样本不引入任何额外的可听噪音。两个语音分类任务上的大量实验证明了我们攻击的有效性。此外，我们分析了激活提出的后门攻击的具体场景，并验证了其对微调的抵抗能力。

    Deep speech classification has achieved tremendous success and greatly promoted the emergence of many real-world applications. However, backdoor attacks present a new security threat to it, particularly with untrustworthy third-party platforms, as pre-defined triggers set by the attacker can activate the backdoor. Most of the triggers in existing speech backdoor attacks are sample-agnostic, and even if the triggers are designed to be unnoticeable, they can still be audible. This work explores a backdoor attack that utilizes sample-specific triggers based on voice conversion. Specifically, we adopt a pre-trained voice conversion model to generate the trigger, ensuring that the poisoned samples does not introduce any additional audible noise. Extensive experiments on two speech classification tasks demonstrate the effectiveness of our attack. Furthermore, we analyzed the specific scenarios that activated the proposed backdoor and verified its resistance against fine-tuning.
    
[^70]: 用变分贝叶斯推断从有限数据中发现随机偏微分方程

    Discovering stochastic partial differential equations from limited data using variational Bayes inference. (arXiv:2306.15873v1 [stat.ML])

    [http://arxiv.org/abs/2306.15873](http://arxiv.org/abs/2306.15873)

    本文提出了一种新的框架，结合了随机微积分、变分贝叶斯理论和稀疏学习的概念，用于从有限数据中准确地发现随机偏微分方程（SPDEs）。作者应用该方法成功地识别了随机热方程、随机Allen-Cahn方程和随机Nagumo方程，并证明了该方法在各种科学应用中的重要性。

    

    我们提出了一个新的框架，用于从数据中发现随机偏微分方程（SPDEs）。所提出的方法将随机微积分、变分贝叶斯理论和稀疏学习的概念相结合。我们提出了扩展的Kramers-Moyal展开形式，以响应状态来表示SPDE的漂移和扩散项，并使用带有稀疏学习技术的Spike-and-Slab先验来高效准确地发现潜在的SPDEs。所提出的方法已经应用于三个典型的SPDEs，分别是随机热方程、随机Allen-Cahn方程和随机Nagumo方程。我们的结果表明，所提出的方法可以准确地识别有限数据中的潜在SPDEs。这是首次尝试从数据中发现SPDEs，对于各种科学应用，如气候建模、金融预测和化学动力学，具有重要意义。

    We propose a novel framework for discovering Stochastic Partial Differential Equations (SPDEs) from data. The proposed approach combines the concepts of stochastic calculus, variational Bayes theory, and sparse learning. We propose the extended Kramers-Moyal expansion to express the drift and diffusion terms of an SPDE in terms of state responses and use Spike-and-Slab priors with sparse learning techniques to efficiently and accurately discover the underlying SPDEs. The proposed approach has been applied to three canonical SPDEs, (a) stochastic heat equation, (b) stochastic Allen-Cahn equation, and (c) stochastic Nagumo equation. Our results demonstrate that the proposed approach can accurately identify the underlying SPDEs with limited data. This is the first attempt at discovering SPDEs from data, and it has significant implications for various scientific applications, such as climate modeling, financial forecasting, and chemical kinetics.
    
[^71]: GraSS:带有梯度引导采样策略的对比学习用于遥感图像语义分割

    GraSS: Contrastive Learning with Gradient Guided Sampling Strategy for Remote Sensing Image Semantic Segmentation. (arXiv:2306.15868v1 [cs.LG])

    [http://arxiv.org/abs/2306.15868](http://arxiv.org/abs/2306.15868)

    提出了一种基于对比学习的带有梯度引导采样策略（GraSS）用于遥感图像语义分割任务，解决了正样本混淆和特征适应偏差的问题。

    

    自监督对比学习（SSCL）在遥感图像（RSI）理解方面取得了重大的里程碑。其核心在于设计一种无监督实例区分预训练任务，从大量无标签图像中提取有利于下游任务的图像特征。然而，现有的基于实例区分的SSCL在应用于RSI语义分割任务时存在两个限制：1）正样本混淆问题；2）特征适应偏差。在需要像素级或目标级特征的语义分割任务中，它引入了特征适应偏差。在本研究中，我们观察到鉴别信息可以通过无监督对比损失的梯度映射到RSI的特定区域，这些特定区域往往包含特殊的地面对象。基于此，我们提出了一种带有梯度引导采样策略的对比学习（GraSS）用于RSI语义分割。

    Self-supervised contrastive learning (SSCL) has achieved significant milestones in remote sensing image (RSI) understanding. Its essence lies in designing an unsupervised instance discrimination pretext task to extract image features from a large number of unlabeled images that are beneficial for downstream tasks. However, existing instance discrimination based SSCL suffer from two limitations when applied to the RSI semantic segmentation task: 1) Positive sample confounding issue; 2) Feature adaptation bias. It introduces a feature adaptation bias when applied to semantic segmentation tasks that require pixel-level or object-level features. In this study, We observed that the discrimination information can be mapped to specific regions in RSI through the gradient of unsupervised contrastive loss, these specific regions tend to contain singular ground objects. Based on this, we propose contrastive learning with Gradient guided Sampling Strategy (GraSS) for RSI semantic segmentation. Gr
    
[^72]: 差分隐私分布式估计和学习

    Differentially Private Distributed Estimation and Learning. (arXiv:2306.15865v1 [cs.LG])

    [http://arxiv.org/abs/2306.15865](http://arxiv.org/abs/2306.15865)

    本文研究了在网络环境中的分布式估计和学习问题，通过交换私有观测信息，代理可以集体估计未知数量，而保护隐私。通过线性聚合方案和差分隐私（DP）调整的随机化方案，本研究提出了一种能够在保证隐私的同时高效组合观测数据的算法。

    

    我们研究了在网络环境中的分布式估计和学习问题，其中代理通过交换信息来估计从其私下观察的样本中未知的统计属性。通过交换私有观测信息，代理可以集体估计未知数量，但他们也面临隐私风险。我们的聚合方案的目标是在时间和网络中高效地组合观测数据，同时满足代理的隐私需求，而不需要任何超越他们本地附近的协调。我们的算法使参与的代理能够从离线或随时间在线获取的私有信号中估计完整的充分统计量，并保护其信号和网络附近的隐私。这是通过线性聚合方案和调整的随机化方案实现的，将噪声添加到交换的估计数据中以满足差分隐私（DP）。

    We study distributed estimation and learning problems in a networked environment in which agents exchange information to estimate unknown statistical properties of random variables from their privately observed samples. By exchanging information about their private observations, the agents can collectively estimate the unknown quantities, but they also face privacy risks. The goal of our aggregation schemes is to combine the observed data efficiently over time and across the network, while accommodating the privacy needs of the agents and without any coordination beyond their local neighborhoods. Our algorithms enable the participating agents to estimate a complete sufficient statistic from private signals that are acquired offline or online over time, and to preserve the privacy of their signals and network neighborhoods. This is achieved through linear aggregation schemes with adjusted randomization schemes that add noise to the exchanged estimates subject to differential privacy (DP
    
[^73]: 用于手中物体自感知6D姿态估计的分层图神经网络

    Hierarchical Graph Neural Networks for Proprioceptive 6D Pose Estimation of In-hand Objects. (arXiv:2306.15858v1 [cs.RO])

    [http://arxiv.org/abs/2306.15858](http://arxiv.org/abs/2306.15858)

    本文提出了一种分层图神经网络架构，用于结合多模态数据，实现几何信息有根据的6D物体姿态估计。

    

    机器人操作，特别是手中物体的操作，通常需要准确估计物体的6D姿态。为了提高估计姿态的准确性，目前6D物体姿态估计的最先进方法使用来自一个或多个模态的观测数据，例如RGB图像、深度和触觉读数。然而，现有方法对这些模态捕获的物体的基本几何结构的利用有限，从而增加了对视觉特征的依赖性。这导致当面对缺乏这种视觉特征的物体或者视觉特征被遮挡时，性能较差。此外，现有方法也没有充分利用手指位置中嵌入的感觉信息。为了解决这些限制，本文介绍了一种用于结合多模态（视觉和触觉）数据的分层图神经网络架构，实现几何信息有根据的6D物体姿态估计。

    Robotic manipulation, in particular in-hand object manipulation, often requires an accurate estimate of the object's 6D pose. To improve the accuracy of the estimated pose, state-of-the-art approaches in 6D object pose estimation use observational data from one or more modalities, e.g., RGB images, depth, and tactile readings. However, existing approaches make limited use of the underlying geometric structure of the object captured by these modalities, thereby, increasing their reliance on visual features. This results in poor performance when presented with objects that lack such visual features or when visual features are simply occluded. Furthermore, current approaches do not take advantage of the proprioceptive information embedded in the position of the fingers. To address these limitations, in this paper: (1) we introduce a hierarchical graph neural network architecture for combining multimodal (vision and touch) data that allows for a geometrically informed 6D object pose estima
    
[^74]: 纯探索问题中具有低秩结构的多臂赌博机的盲目采样器应用

    Pure exploration in multi-armed bandits with low rank structure using oblivious sampler. (arXiv:2306.15856v1 [cs.LG])

    [http://arxiv.org/abs/2306.15856](http://arxiv.org/abs/2306.15856)

    本文研究了纯探索问题中具有低秩结构的多臂赌博机，提出了一种盲目采样器应用以解决分离设置下的纯探索问题，并给出了具有低秩序列的多臂赌博机纯探索问题的上界和下界。

    

    本文考虑纯探索问题中奖励序列的低秩结构。首先，我们提出了纯探索问题中的分离设置，其中探索策略无法接收其探索的反馈。由于这种分离设置，探索策略需要盲目地采样臂。通过引入奖励向量的核信息，我们提供了对于时间变化和固定情况的高效算法，其遗憾界为$O(d\sqrt{(\ln N)/n})$。然后，我们证明了低秩序列的多臂赌博机的纯探索问题的下界。我们的上界与下界之间存在$O(\sqrt{\ln N})$的差距。

    In this paper, we consider the low rank structure of the reward sequence of the pure exploration problems. Firstly, we propose the separated setting in pure exploration problem, where the exploration strategy cannot receive the feedback of its explorations. Due to this separation, it requires that the exploration strategy to sample the arms obliviously. By involving the kernel information of the reward vectors, we provide efficient algorithms for both time-varying and fixed cases with regret bound $O(d\sqrt{(\ln N)/n})$. Then, we show the lower bound to the pure exploration in multi-armed bandits with low rank sequence. There is an $O(\sqrt{\ln N})$ gap between our upper bound and the lower bound.
    
[^75]: GoalieNet: 冰球中用于联合推测守门员、装备和球网姿势的多阶段网络

    GoalieNet: A Multi-Stage Network for Joint Goalie, Equipment, and Net Pose Estimation in Ice Hockey. (arXiv:2306.15853v1 [cs.CV])

    [http://arxiv.org/abs/2306.15853](http://arxiv.org/abs/2306.15853)

    GoalieNet是一个多阶段深度神经网络，用于联合推测冰球中守门员、装备和球网的姿势。实验结果表明，GoalieNet在大量关键点的准确率超过80%的情况下，整体平均准确率达到84%，表明这种联合姿势推测方法具有潜力。

    

    在计算机视觉驱动的冰球分析领域中，最具挑战性且研究最少的任务之一是守门员姿势推测。与一般的人体姿势推测不同，守门员姿势推测更加复杂，它不仅涉及检测隐藏在厚厚内衬和面具下的守门员关键点，还涉及大量与守门员身穿的厚重护腿和手套、球棒以及冰球网相关的非人类关键点。为了解决这个挑战，我们引入了GoalieNet，这是一个用于联合推测守门员姿势、守门员装备和球网的多阶段深度神经网络。通过使用NHL基准数据进行实验，结果表明，所提出的GoalieNet可以在所有关键点上实现84%的平均准确率，其中有22个关键点的准确率超过80%。这表明，这种联合姿势推测方法可能是一个有前途的研究方向。

    In the field of computer vision-driven ice hockey analytics, one of the most challenging and least studied tasks is goalie pose estimation. Unlike general human pose estimation, goalie pose estimation is much more complex as it involves not only the detection of keypoints corresponding to the joints of the goalie concealed under thick padding and mask, but also a large number of non-human keypoints corresponding to the large leg pads and gloves worn, the stick, as well as the hockey net. To tackle this challenge, we introduce GoalieNet, a multi-stage deep neural network for jointly estimating the pose of the goalie, their equipment, and the net. Experimental results using NHL benchmark data demonstrate that the proposed GoalieNet can achieve an average of 84\% accuracy across all keypoints, where 22 out of 29 keypoints are detected with more than 80\% accuracy. This indicates that such a joint pose estimation approach can be a promising research direction.
    
[^76]: 非替代性SGD的排序

    Ordering for Non-Replacement SGD. (arXiv:2306.15848v1 [cs.LG])

    [http://arxiv.org/abs/2306.15848](http://arxiv.org/abs/2306.15848)

    这项研究提出了一种优化非替代性SGD算法收敛速度的排序策略，并通过实验证实了其有效性。

    

    降低机器学习的运行时间和提高效率的一种方法是减少所使用的优化算法的收敛速度。洗牌是一种广泛应用于机器学习的算法技术，但在理论上却只在最近几年才开始受到关注。针对随机洗牌和增量梯度下降的不同收敛速度，我们寻求找到一种排序方式，以改善非替代形式算法的收敛速度。基于已有的最优与当前迭代之间的距离界限，我们得到了一个上界，该上界取决于迭代开始时的梯度。通过对该界限进行分析，我们能够为强凸和凸函数开发出常数和递减步长的最优排序。我们还通过对合成数据集和真实数据集进行实验证实了我们的结果。另外，我们还能将这种排序方式与小批量和增大批次梯度结合起来。

    One approach for reducing run time and improving efficiency of machine learning is to reduce the convergence rate of the optimization algorithm used. Shuffling is an algorithm technique that is widely used in machine learning, but it only started to gain attention theoretically in recent years. With different convergence rates developed for random shuffling and incremental gradient descent, we seek to find an ordering that can improve the convergence rates for the non-replacement form of the algorithm. Based on existing bounds of the distance between the optimal and current iterate, we derive an upper bound that is dependent on the gradients at the beginning of the epoch. Through analysis of the bound, we are able to develop optimal orderings for constant and decreasing step sizes for strongly convex and convex functions. We further test and verify our results through experiments on synthesis and real data sets. In addition, we are able to combine the ordering with mini-batch and furth
    
[^77]: 在基于得分的扩散模型中缓解颜色偏移

    Easing Color Shifts in Score-Based Diffusion Models. (arXiv:2306.15832v1 [cs.LG])

    [http://arxiv.org/abs/2306.15832](http://arxiv.org/abs/2306.15832)

    本文提出了在基于得分的扩散模型中缓解颜色偏移的计算廉价解决方案，并引入了一个简单的非线性绕过连接来改善生成图像的空间均值。

    

    得分模型生成的图像可能会因空间均值的错误而出现颜色偏移，这种效应在较大的图像中会越来越明显。本文提出了一种计算廉价的解决方案，以减轻基于得分的扩散模型中的颜色偏移。我们在得分网络中引入了一个简单的非线性绕过连接，用于处理输入的空间均值，并预测得分函数的均值。这种网络架构显著改善了生成图像的空间均值，我们证明了改进与生成图像大小的关系近似独立。因此，我们的解决方案为跨图像尺寸的颜色偏移问题提供了相对廉价的解决方案。最后，我们讨论了在理想化情况下颜色偏移的起源，以推动我们的方法的提出。

    Generated images of score-based models can suffer from errors in their spatial means, an effect, referred to as a color shift, which grows for larger images. This paper introduces a computationally inexpensive solution to mitigate color shifts in score-based diffusion models. We propose a simple nonlinear bypass connection in the score network, designed to process the spatial mean of the input and to predict the mean of the score function. This network architecture substantially improves the resulting spatial means of the generated images, and we show that the improvement is approximately independent of the size of the generated images. As a result, our solution offers a comparatively inexpensive solution for the color shift problem across image sizes. Lastly, we discuss the origin of color shifts in an idealized setting in order to motivate our approach.
    
[^78]: 基于置信度的端到端语音识别模型的集成

    Confidence-based Ensembles of End-to-End Speech Recognition Models. (arXiv:2306.15824v1 [eess.AS])

    [http://arxiv.org/abs/2306.15824](http://arxiv.org/abs/2306.15824)

    本文提出了一种基于置信度的端到端语音识别模型的集成方法，通过只使用最有信心的模型的输出来提高性能，并通过两个应用程序的实验证明了方法的有效性。

    

    每年端到端语音识别模型的数量不断增长。这些模型经常被调整到新领域或语言，导致专家系统大量涌现，在目标数据上取得了很好的结果，但在自己专业领域之外的性能却相对较差。我们通过基于置信度的集成来探索这些专家模型的组合：在模型中只使用最有信心的模型的输出。我们假设模型的目标数据除了一个小的验证集外是不可用的。我们通过两个应用程序展示了我们方法的有效性。首先，我们展示了一个由5个单语模型组成的基于置信度的集成优于通过专用语言识别模块进行模型选择的系统。其次，我们证明了可以将基础模型和调整模型相结合，在原始和目标数据上取得强大的结果。我们在多个数据集和模型架构上验证了所有结果。

    The number of end-to-end speech recognition models grows every year. These models are often adapted to new domains or languages resulting in a proliferation of expert systems that achieve great results on target data, while generally showing inferior performance outside of their domain of expertise. We explore combination of such experts via confidence-based ensembles: ensembles of models where only the output of the most-confident model is used. We assume that models' target data is not available except for a small validation set. We demonstrate effectiveness of our approach with two applications. First, we show that a confidence-based ensemble of 5 monolingual models outperforms a system where model selection is performed via a dedicated language identification block. Second, we demonstrate that it is possible to combine base and adapted models to achieve strong results on both original and target data. We validate all our results on multiple datasets and model architectures.
    
[^79]: 学习同源脑结构的正常不对称表示

    Learning normal asymmetry representations for homologous brain structures. (arXiv:2306.15811v1 [q-bio.NC])

    [http://arxiv.org/abs/2306.15811](http://arxiv.org/abs/2306.15811)

    这篇论文介绍了一种学习同源脑结构正常不对称表示的新方法，该方法利用异常检测和表示学习，通过Siamese架构将脑结构的左右半球映射到一个正常不对称嵌入空间。该方法可以量化与正常不对称性偏离的程度。

    

    尽管正常的同源脑结构在定义上是近似对称的，但它们也会因自然老化等原因而存在形状差异。另一方面，神经退行性疾病会导致不对称程度增加或改变其位置。目前，确定这些变化是否由病理性恶化引起仍然具有挑战性。目前的临床工具要么依赖于主观评估、基本体积测量，要么依赖于特定疾病的深度学习模型。本文介绍了一种基于异常检测和表示学习的方法，用于学习同源脑结构中正常的不对称模式。我们的框架使用Siamese架构，将脑结构左右半球的3D分割映射到一个基于支持向量数据描述目标学习的正常不对称嵌入空间。由于只使用健康样本进行训练，它可以量化与正常不对称性偏离的程度。

    Although normal homologous brain structures are approximately symmetrical by definition, they also have shape differences due to e.g. natural ageing. On the other hand, neurodegenerative conditions induce their own changes in this asymmetry, making them more pronounced or altering their location. Identifying when these alterations are due to a pathological deterioration is still challenging. Current clinical tools rely either on subjective evaluations, basic volume measurements or disease-specific deep learning models. This paper introduces a novel method to learn normal asymmetry patterns in homologous brain structures based on anomaly detection and representation learning. Our framework uses a Siamese architecture to map 3D segmentations of left and right hemispherical sides of a brain structure to a normal asymmetry embedding space, learned using a support vector data description objective. Being trained using healthy samples only, it can quantify deviations-from-normal-asymmetry pa
    
[^80]: FLuRKA: 快速融合低秩和核注意力

    FLuRKA: Fast fused Low-Rank & Kernel Attention. (arXiv:2306.15799v1 [cs.LG])

    [http://arxiv.org/abs/2306.15799](http://arxiv.org/abs/2306.15799)

    FLuRKA是一种融合低秩和核注意力的新型transformer类别，相较于传统的近似技术，在运行时间性能和质量方面都表现出显著的提升。

    

    自从transformer结构的提出以来，许多高效的近似自注意力技术已经变得流行起来。其中两种流行的技术类别是低秩和核方法。我们观察到这两种方法的优势相互补充，利用这些协同效应来融合低秩和核方法，产生了一种新的transformer类别：FLuRKA（快速低秩和核注意力）。FLuRKA相对于这些近似技术提供了可观的性能提升，并且具有高质量。我们在理论和实证方面评估了FLuRKA的运行时间性能和质量。我们的运行时间分析提供了多种参数配置，在这些配置下，FLuRKA具有加速效果；我们的准确性分析限定了FLuRKA相对于全注意力的误差。我们实例化了三种FLuRKA变体，相对于低秩和核方法分别实现了高达3.3倍和1.7倍的经验加速。这意味着更快的运行时间，而且质量仍然保持不错。

    Many efficient approximate self-attention techniques have become prevalent since the inception of the transformer architecture. Two popular classes of these techniques are low-rank and kernel methods. Each of these methods has its own strengths. We observe these strengths synergistically complement each other and exploit these synergies to fuse low-rank and kernel methods, producing a new class of transformers: FLuRKA (Fast Low-Rank and Kernel Attention). FLuRKA provide sizable performance gains over these approximate techniques and are of high quality. We theoretically and empirically evaluate both the runtime performance and quality of FLuRKA. Our runtime analysis posits a variety of parameter configurations where FLuRKA exhibit speedups and our accuracy analysis bounds the error of FLuRKA with respect to full-attention. We instantiate three FLuRKA variants which experience empirical speedups of up to 3.3x and 1.7x over low-rank and kernel methods respectively. This translates to spe
    
[^81]: HyenaDNA：单核苷酸分辨率下的长范围基因组序列建模

    HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution. (arXiv:2306.15794v1 [cs.LG])

    [http://arxiv.org/abs/2306.15794](http://arxiv.org/abs/2306.15794)

    HyenaDNA是一种基于隐式卷积的基因组序列建模方法，可以在单核苷酸分辨率下对长范围相互作用进行建模。

    

    基因组（DNA）序列编码了大量关于基因调控和蛋白质合成的信息。类似自然语言模型，研究人员提出了基因组的基础模型，从非标记的基因组数据中学习可推广的特征，然后进行下游任务的微调，如识别调控元件。由于注意力的二次扩展，先前基于Transformer的基因组模型仅使用512到4k个标记作为上下文（<0.001%的人类基因组），严重限制了DNA的长范围相互作用建模。此外，这些方法依赖于标记器来聚合有意义的DNA单元，丢失了单核苷酸分辨率，其中微小的遗传变异可以通过单核苷酸多态性（SNP）完全改变蛋白质功能。最近，基于隐式卷积的大型语言模型Hyena显示出能够与注意力质量相匹配，同时允许更长的上下文长度和更低的时间复杂性。利用Hyenas n

    Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyenas n
    
[^82]: 一个对稳健态步行机器人神经动力学的人群层面分析

    A Population-Level Analysis of Neural Dynamics in Robust Legged Robots. (arXiv:2306.15793v1 [cs.RO])

    [http://arxiv.org/abs/2306.15793](http://arxiv.org/abs/2306.15793)

    本研究通过分析神经动力学在稳健步行机器人中的人群层活动，揭示了控制器的拓扑结构对平衡能力的影响；通过应用神经干扰探究系统的强迫响应，发现循环状态动力学具有结构化和低维特征，并提出了一种新的控制机制的存在证据。

    

    基于循环神经网络的增强学习系统能够完成复杂的运动控制任务，如步态和操作，然而，它们的基本机制仍然难以解释。我们的目标是利用计算神经科学方法来理解稳健机器人步行控制器的人群层活动。我们的研究从分析拓扑结构开始，发现脆弱的控制器具有更多的固定点和不稳定方向，导致在指导下保持平衡时更差。接下来，我们通过在主导人群层活动方向上应用有针对性的神经干扰来分析系统的强迫响应。我们发现循环状态动力学在行走过程中具有结构化和低维特征，与灵长类动物的研究结果相符。此外，当循环状态扰动为零时，脆弱的控制器仍能够行走，这表明一种新的控制机制的存在。

    Recurrent neural network-based reinforcement learning systems are capable of complex motor control tasks such as locomotion and manipulation, however, much of their underlying mechanisms still remain difficult to interpret. Our aim is to leverage computational neuroscience methodologies to understanding the population-level activity of robust robot locomotion controllers. Our investigation begins by analyzing topological structure, discovering that fragile controllers have a higher number of fixed points with unstable directions, resulting in poorer balance when instructed to stand in place. Next, we analyze the forced response of the system by applying targeted neural perturbations along directions of dominant population-level activity. We find evidence that recurrent state dynamics are structured and low-dimensional during walking, which aligns with primate studies. Additionally, when recurrent states are perturbed to zero, fragile agents continue to walk, which is indicative of a st
    
[^83]: 利用输出特定和数据解析的隐私配置文件研究机器学习模型中的数据集级隐私转换

    Probing the Transition to Dataset-Level Privacy in ML Models Using an Output-Specific and Data-Resolved Privacy Profile. (arXiv:2306.15790v1 [cs.LG])

    [http://arxiv.org/abs/2306.15790](http://arxiv.org/abs/2306.15790)

    本论文针对差分隐私（DP）框架的不足，研究了使用DP机制训练的模型在相邻数据集上的“覆盖程度”。通过连接覆盖程度指标和已有研究，我们排名了训练集中个别样本的隐私，并形成了一个隐私配置文件。此外，我们展示了隐私配置文件可以用来探测观察到的隐私转换。

    

    差分隐私（DP）是保护机器学习模型中用户数据的主要技术。然而，这种框架存在选择隐私预算ε的不清晰以及特定训练模型对特定数据行的隐私泄露量的缺乏量化等问题。我们通过研究一个将使用DP机制在数据集上训练的模型与训练相邻数据集产生的分布的“覆盖程度”进行量化的隐私指标，解决了这些限制，并提出了一种新的视角来可视化DP结果。我们将这种覆盖程度指标与文献中的已有研究相连接，并用它来对训练集中个别样本的隐私进行排名，形成了隐私配置文件。我们还展示了隐私配置文件可以用来探测观察到的邻近分布中发生的不可区分性转换随着ε的递减

    Differential privacy (DP) is the prevailing technique for protecting user data in machine learning models. However, deficits to this framework include a lack of clarity for selecting the privacy budget $\epsilon$ and a lack of quantification for the privacy leakage for a particular data row by a particular trained model. We make progress toward these limitations and a new perspective by which to visualize DP results by studying a privacy metric that quantifies the extent to which a model trained on a dataset using a DP mechanism is ``covered" by each of the distributions resulting from training on neighboring datasets. We connect this coverage metric to what has been established in the literature and use it to rank the privacy of individual samples from the training set in what we call a privacy profile. We additionally show that the privacy profile can be used to probe an observed transition to indistinguishability that takes place in the neighboring distributions as $\epsilon$ decrea
    
[^84]: 结构化状态空间模型在数字病理学中的多实例学习

    Structured State Space Models for Multiple Instance Learning in Digital Pathology. (arXiv:2306.15789v1 [cs.CV])

    [http://arxiv.org/abs/2306.15789](http://arxiv.org/abs/2306.15789)

    本文提出使用结构化状态空间模型作为多实例学习器，用于高效建模和分类整个数字病理学切片图像中的组织斑块序列。

    

    多实例学习是一种理想的组织病理学数据分析模式，其中大量的全切片图像通常用单个全局标签进行注释。在这种情况下，整个切片图像被建模为一组要进行聚合和分类的组织斑块。用于进行此分类的常见模型包括循环神经网络和transformer。尽管使用了强大的压缩算法，如深度预训练神经网络，来降低每个斑块的维度，但来自整个切片图像的序列仍然过长，通常包含数万个斑块。结构化状态空间模型是一种新兴的序列建模替代方案，专门设计用于高效建模长序列。这些模型通过将输入序列最优投影到压缩整个序列的内存单元中来实现。在本文中，我们提出将状态空间模型用作多实例学习器，以实现可变性的分析。

    Multiple instance learning is an ideal mode of analysis for histopathology data, where vast whole slide images are typically annotated with a single global label. In such cases, a whole slide image is modelled as a collection of tissue patches to be aggregated and classified. Common models for performing this classification include recurrent neural networks and transformers. Although powerful compression algorithms, such as deep pre-trained neural networks, are used to reduce the dimensionality of each patch, the sequences arising from whole slide images remain excessively long, routinely containing tens of thousands of patches. Structured state space models are an emerging alternative for sequence modelling, specifically designed for the efficient modelling of long sequences. These models invoke an optimal projection of an input sequence into memory units that compress the entire sequence. In this paper, we propose the use of state space models as a multiple instance learner to a vari
    
[^85]: 可解释机器学习中罗生门效应的实证评估

    An Empirical Evaluation of the Rashomon Effect in Explainable Machine Learning. (arXiv:2306.15786v1 [cs.LG])

    [http://arxiv.org/abs/2306.15786](http://arxiv.org/abs/2306.15786)

    通过对不同数据集、模型和指标进行定量评估，我们发现罗生门效应对可解释机器学习具有影响，这为之前的轶事证据提供了实证支持，并展示了科学家和实践者面临的挑战。

    

    罗生门效应描述了以下现象：对于给定的数据集，可能存在许多具有相同良好性能但采用不同解决策略的模型。罗生门效应对可解释机器学习具有影响，特别是对解释的可比性。我们对三种不同比较场景提供了统一视角，并在不同数据集、模型、归因方法和指标上进行了定量评估。我们发现超参数调整起到了一定作用，指标选择也很重要。我们的结果为先前的轶事证据提供了实证支持，并展示了科学家和实践者面临的挑战。

    The Rashomon Effect describes the following phenomenon: for a given dataset there may exist many models with equally good performance but with different solution strategies. The Rashomon Effect has implications for Explainable Machine Learning, especially for the comparability of explanations. We provide a unified view on three different comparison scenarios and conduct a quantitative evaluation across different datasets, models, attribution methods, and metrics. We find that hyperparameter-tuning plays a role and that metric selection matters. Our results provide empirical support for previously anecdotal evidence and exhibit challenges for both scientists and practitioners.
    
[^86]: UTRNet: 印刷文档中高分辨率乌尔都文本识别

    UTRNet: High-Resolution Urdu Text Recognition In Printed Documents. (arXiv:2306.15782v1 [cs.CV])

    [http://arxiv.org/abs/2306.15782](http://arxiv.org/abs/2306.15782)

    本文提出了一种解决印刷乌尔都文本识别挑战的新方法，并引入了大规模实际标记数据集和合成数据集，提供了乌尔都文本行检测的基准数据集，同时开发了一个在线工具，实现了印刷文档中乌尔都OCR的端到端识别。

    

    本文提出了一种新颖方法来解决印刷乌尔都文本识别的挑战，使用高分辨率、多尺度的语义特征提取。我们提出的UTRNet架构，一个混合CNN-RNN模型，在基准数据集上展示了最先进的性能。为了解决以前工作的局限性，这些工作很难推广到乌尔都文本的复杂性和缺乏足够的实际标记数据，我们引入了UTRSet-Real，一个包含超过11,000行的大规模实际标记数据集和UTRSet-Synth，一个与实际世界非常相似的含有20,000行的合成数据集，并对现有的IIITH数据集的基准真实性进行了修正，使其成为未来研究的更可靠的资源。我们还提供了UrduDoc，一种用于扫描文档中乌尔都文本行检测的基准数据集。此外，我们还开发了一种在线工具，通过将UTRNet与文本的端到端乌尔都OCR集成在印刷文档中。

    In this paper, we propose a novel approach to address the challenges of printed Urdu text recognition using high-resolution, multi-scale semantic feature extraction. Our proposed UTRNet architecture, a hybrid CNN-RNN model, demonstrates state-of-the-art performance on benchmark datasets. To address the limitations of previous works, which struggle to generalize to the intricacies of the Urdu script and the lack of sufficient annotated real-world data, we have introduced the UTRSet-Real, a large-scale annotated real-world dataset comprising over 11,000 lines and UTRSet-Synth, a synthetic dataset with 20,000 lines closely resembling real-world and made corrections to the ground truth of the existing IIITH dataset, making it a more reliable resource for future research. We also provide UrduDoc, a benchmark dataset for Urdu text line detection in scanned documents. Additionally, we have developed an online tool for end-to-end Urdu OCR from printed documents by integrating UTRNet with a tex
    
[^87]: 人类中心生成式人工智能的下一步：技术视角

    Next Steps for Human-Centered Generative AI: A Technical Perspective. (arXiv:2306.15774v1 [cs.HC])

    [http://arxiv.org/abs/2306.15774](http://arxiv.org/abs/2306.15774)

    这项研究从技术角度定义和提出了人类中心生成式人工智能(HGAI)的下一步工作，包括与人类价值观对齐、适应人类的意图表达和增强人类在协作工作流中的能力。这个工作的目标是吸引跨学科研究团队对HGAI的新兴想法进行讨论，并保持未来工作景观的整体连贯性。

    

    通过反复跨学科讨论，我们从技术角度为人类中心生成式人工智能(HGAI)定义和提出了下一步的工作。我们贡献了一个路线图，概述了生成式人工智能在三个层面上的未来方向：与人类价值观对齐；适应人类的意图表达；增强人类在协作工作流中的能力。该路线图旨在吸引跨学科研究团队对HGAI的新兴想法进行全面的讨论，同时保持未来工作景观的整体连贯性。

    Through iterative, cross-disciplinary discussions, we define and propose next-steps for Human-centered Generative AI (HGAI) from a technical perspective. We contribute a roadmap that lays out future directions of Generative AI spanning three levels: Aligning with human values; Accommodating humans' expression of intents; and Augmenting humans' abilities in a collaborative workflow. This roadmap intends to draw interdisciplinary research teams to a comprehensive list of emergent ideas in HGAI, identifying their interested topics while maintaining a coherent big picture of the future work landscape.
    
[^88]: 图像网为何与LAION网络截然不同

    What Makes ImageNet Look Unlike LAION. (arXiv:2306.15769v1 [cs.LG])

    [http://arxiv.org/abs/2306.15769](http://arxiv.org/abs/2306.15769)

    本研究通过重新搜索大规模的LAION数据集，尝试重新创建图像网，并发现与原始图像网相比，新建的LAIONet具有明显不同之处。这种差异的原因是，在基于图像描述进行搜索时，存在信息瓶颈，从而减轻了选择偏差。

    

    图像网是通过Flickr图像搜索结果创建的。如果我们仅根据图像描述重新创建图像网，搜索大规模的LAION数据集会发生什么呢？本研究进行了这个反事实的调查。我们发现重新创建的图像网，我们称之为LAIONet，与原始图像网有明显不同之处。具体而言，原始图像网中图像的类内相似性远高于LAIONet。因此，在图像网上训练的模型在LAIONet上表现明显较差。我们提出了一个严格解释这种差异的观点，并通过系统性的实验予以支持。简而言之，仅基于图像描述进行搜索会产生信息瓶颈，从而减轻了基于图像过滤时存在的选择偏差。我们的解释形式化了一个长期的直觉。

    ImageNet was famously created from Flickr image search results. What if we recreated ImageNet instead by searching the massive LAION dataset based on image captions alone? In this work, we carry out this counterfactual investigation. We find that the resulting ImageNet recreation, which we call LAIONet, looks distinctly unlike the original. Specifically, the intra-class similarity of images in the original ImageNet is dramatically higher than it is for LAIONet. Consequently, models trained on ImageNet perform significantly worse on LAIONet. We propose a rigorous explanation for the discrepancy in terms of a subtle, yet important, difference in two plausible causal data-generating processes for the respective datasets, that we support with systematic experimentation. In a nutshell, searching based on an image caption alone creates an information bottleneck that mitigates the selection bias otherwise present in image-based filtering. Our explanation formalizes a long-held intuition in th
    
[^89]: 大型语言模型作为标注者：以最小成本增强NLP模型的泛化能力

    Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost. (arXiv:2306.15766v1 [cs.CL])

    [http://arxiv.org/abs/2306.15766](http://arxiv.org/abs/2306.15766)

    本研究提出了一种利用大型语言模型进行标注的方法，以最小成本提升NLP模型的泛化能力。通过差异性采样策略，我们证明了这种方法在分类和排序任务上能够显著改善模型效果。

    

    最先进的监督式NLP模型能够达到很高的准确度，但对于低数据领域的输入也容易出现失败，例如训练数据中未包含的领域。作为收集特定领域的真实标签的近似方法，我们研究了使用大型语言模型(LLM)对输入进行标注并提升NLP模型的泛化能力。具体而言，给定LLM标注的预算，我们提出了一种算法来选择最具信息量的输入进行标注和重新训练NLP模型。我们发现流行的基于不确定性采样的主动学习策略效果不佳。相反，我们提出了一种基于基础模型和微调NLP模型之间预测分数差异的采样策略，利用了大多数NLP模型都是从基础模型微调而来的事实。分类(语义相似度)和排序(语义搜索)任务的实验证明我们的采样策略带来了显著的收益。

    State-of-the-art supervised NLP models achieve high accuracy but are also susceptible to failures on inputs from low-data regimes, such as domains that are not represented in training data. As an approximation to collecting ground-truth labels for the specific domain, we study the use of large language models (LLMs) for annotating inputs and improving the generalization of NLP models. Specifically, given a budget for LLM annotations, we present an algorithm for sampling the most informative inputs to annotate and retrain the NLP model. We find that popular active learning strategies such as uncertainty-based sampling do not work well. Instead, we propose a sampling strategy based on the difference in prediction scores between the base model and the finetuned NLP model, utilizing the fact that most NLP models are finetuned from a base model. Experiments with classification (semantic similarity) and ranking (semantic search) tasks show that our sampling strategy leads to significant gain
    
[^90]: 使用概率因果模型生成高保真度图像的反事实推理

    High Fidelity Image Counterfactuals with Probabilistic Causal Models. (arXiv:2306.15764v1 [cs.LG])

    [http://arxiv.org/abs/2306.15764](http://arxiv.org/abs/2306.15764)

    提出了一个通用的因果生成建模框架，用于准确估计具有高保真度的图像反事实推理，通过利用因果中介分析和生成建模的思想，设计了新的深度因果机制，实验证明了该方法的准确性。

    

    我们提出了一个通用的因果生成建模框架，用于准确估计具有深度结构因果模型的高保真度图像的反事实推理。对于高维结构变量（如图像）的干预和反事实查询的估计仍然是一项具有挑战性的任务。我们利用因果中介分析的思想和生成建模的进展，为因果模型中的结构变量设计了新的深度因果机制。我们的实验证明，我们提出的机制能够准确地推断和估计直接、间接和总效应，这可以通过反事实的公理严密性来衡量。

    We present a general causal generative modelling framework for accurate estimation of high fidelity image counterfactuals with deep structural causal models. Estimation of interventional and counterfactual queries for high-dimensional structured variables, such as images, remains a challenging task. We leverage ideas from causal mediation analysis and advances in generative modelling to design new deep causal mechanisms for structured variables in causal models. Our experiments demonstrate that our proposed mechanisms are capable of accurate abduction and estimation of direct, indirect and total effects as measured by axiomatic soundness of counterfactuals.
    
[^91]: 预测批量重构代码异味对应用资源消耗的影响

    Predicting the Impact of Batch Refactoring Code Smells on Application Resource Consumption. (arXiv:2306.15763v1 [cs.SE])

    [http://arxiv.org/abs/2306.15763](http://arxiv.org/abs/2306.15763)

    本论文研究了批量重构代码异味对应用资源消耗的影响，并设计了算法来预测代码重构对资源消耗的影响。通过对31个开源应用程序的16种代码异味类型的研究，得出了独立和批量重构特定代码异味后的CPU和内存利用率的变化。

    

    自动化批量重构已经成为一种重新架构软件的事实上的机制，可能会对代码质量和可维护性产生重大设计缺陷的负面影响。尽管已知自动化批量重构技术能够显著提高软件的质量和可维护性，但其对资源利用的影响尚未得到良好的研究。本文旨在弥合批量重构代码异味和资源消耗之间的差距。它确定了软件代码异味批量重构与资源消耗之间的关系。接下来，它旨在设计算法来预测代码异味重构对资源消耗的影响。本文对31个开源应用程序的16种代码异味类型及其对资源利用的联合效果进行了研究。它详细地分析了在独立和批量重构特定代码异味后应用的CPU和内存利用率的变化。然后利用这个分析

    Automated batch refactoring has become a de-facto mechanism to restructure software that may have significant design flaws negatively impacting the code quality and maintainability. Although automated batch refactoring techniques are known to significantly improve overall software quality and maintainability, their impact on resource utilization is not well studied. This paper aims to bridge the gap between batch refactoring code smells and consumption of resources. It determines the relationship between software code smell batch refactoring, and resource consumption. Next, it aims to design algorithms to predict the impact of code smell refactoring on resource consumption. This paper investigates 16 code smell types and their joint effect on resource utilization for 31 open source applications. It provides a detailed empirical analysis of the change in application CPU and memory utilization after refactoring specific code smells in isolation and in batches. This analysis is then used 
    
[^92]: 何去何从：深度学习加速的数字硬件视角

    To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v1 [cs.NE])

    [http://arxiv.org/abs/2306.15749](http://arxiv.org/abs/2306.15749)

    神经形态计算旨在通过仿真脑部操作来提高深度学习模型的效率，但是在SNNs的高效硬件后端设计上仍需进一步研究。

    

    随着深度学习模型规模的增加，它们在涵盖计算机视觉到自然语言处理等领域变得越来越有竞争力；然而，这是以效率为代价的，因为它们需要越来越多的内存和计算能力。生物脑的功耗效率超过任何大规模深度学习（DL）模型；因此，神经形态计算试图模仿脑部操作，例如基于脉冲的信息处理，以提高DL模型的效率。尽管脑部有诸如高效的信息传输、密集的神经元连接和计算与存储的共同位置等优势，但可用的生物基底严重限制了生物大脑的进化。电子硬件没有相同的约束；因此，虽然建模脉冲神经网络（SNNs）可能揭示了一个谜题的一部分，但对于SNNs的高效硬件后端设计需要进一步研究。

    As deep learning models scale, they become increasingly competitive from domains spanning computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms the one of any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, po
    
[^93]: 票据化学习-遗忘方案

    Ticketed Learning-Unlearning Schemes. (arXiv:2306.15744v1 [cs.LG])

    [http://arxiv.org/abs/2306.15744](http://arxiv.org/abs/2306.15744)

    提出了一种新的票据化学习-遗忘模型，其中学习算法通过向每个参与训练示例发送额外信息并保留一部分中央信息，实现了学习和遗忘。我们提供了一种高效的票据化学习-遗忘方案，用于广泛的场景。

    

    我们考虑所定义的学习-遗忘范式。首先，给定一个数据集，目标是学习一个好的预测器，例如最小化某个损失函数的预测器。随后，给定任何希望遗忘的示例子集，目标是在不知道原始训练数据集的情况下，学习一个与在幸存示例上从头开始学习时所生成的预测器完全相同的好的预测器。我们提出了一种新的票据化学习-遗忘模型，其中学习算法可以通过一个小型（加密的）“票据”向每个参与训练示例发送额外的信息，并保留一小部分“中央”信息以供以后使用。随后，希望遗忘的示例将其票据提交给遗忘算法，该算法还使用中央信息返回一个新的预测器。我们提供了一种高效的票据化学习-遗忘方案，用于广泛的场景。

    We consider the learning--unlearning paradigm defined as follows. First given a dataset, the goal is to learn a good predictor, such as one minimizing a certain loss. Subsequently, given any subset of examples that wish to be unlearnt, the goal is to learn, without the knowledge of the original training dataset, a good predictor that is identical to the predictor that would have been produced when learning from scratch on the surviving examples.  We propose a new ticketed model for learning--unlearning wherein the learning algorithm can send back additional information in the form of a small-sized (encrypted) ``ticket'' to each participating training example, in addition to retaining a small amount of ``central'' information for later. Subsequently, the examples that wish to be unlearnt present their tickets to the unlearning algorithm, which additionally uses the central information to return a new predictor. We provide space-efficient ticketed learning--unlearning schemes for a broad
    
[^94]: 基于随机梯度贝叶斯优化实验设计的模拟推断研究

    Stochastic Gradient Bayesian Optimal Experimental Designs for Simulation-based Inference. (arXiv:2306.15731v1 [cs.LG])

    [http://arxiv.org/abs/2306.15731](http://arxiv.org/abs/2306.15731)

    该研究通过建立比率模拟推断算法与随机梯度变分推断的重要连接，将贝叶斯最优实验设计 (BOED) 扩展到模拟推断应用中，实现了同时优化实验设计和摊还推断方法。

    

    模拟推断方法用于解决具有挑战性的逆问题的复杂科学模型。然而，由于其非可微性质，模拟推断模型常常面临重大困难，这阻碍了梯度优化技术的应用。贝叶斯最优实验设计 (BOED) 是一种强大的方法，旨在最有效地利用实验资源，以改进推断结果。尽管随机梯度 BOED 方法在高维设计问题上取得了有希望的结果，但由于许多模拟推断模型的难以处理的非可微性质，BOED 与模拟推断的整合大多被忽视。在本研究中，我们通过利用互信息界限，建立了基于比率的模拟推断推理算法与基于随机梯度的变分推断之间的重要连接。这种连接使我们能够将BOED扩展到模拟推断应用领域，实现实验设计和摊还推断方法的同时优化。

    Simulation-based inference (SBI) methods tackle complex scientific models with challenging inverse problems. However, SBI models often face a significant hurdle due to their non-differentiable nature, which hampers the use of gradient-based optimization techniques. Bayesian Optimal Experimental Design (BOED) is a powerful approach that aims to make the most efficient use of experimental resources for improved inferences. While stochastic gradient BOED methods have shown promising results in high-dimensional design problems, they have mostly neglected the integration of BOED with SBI due to the difficult non-differentiable property of many SBI simulators. In this work, we establish a crucial connection between ratio-based SBI inference algorithms and stochastic gradient-based variational inference by leveraging mutual information bounds. This connection allows us to extend BOED to SBI applications, enabling the simultaneous optimization of experimental designs and amortized inference fu
    
[^95]: 重新思考闭环训练对于自动驾驶的作用

    Rethinking Closed-loop Training for Autonomous Driving. (arXiv:2306.15713v1 [cs.RO])

    [http://arxiv.org/abs/2306.15713](http://arxiv.org/abs/2306.15713)

    这项研究提出了闭环训练对于自动驾驶的重要性，并分析了不同训练基准设计和流行RL算法的不足。为了解决这些问题，提出了一种新的强化学习驾驶代理TRAVL，通过多步预测和利用虚拟数据进行高效学习。

    

    最近高保真度模拟器的进展使得自动驾驶代理的闭环训练成为可能，从而潜在地解决了训练与部署之间的分布偏移问题，并允许安全且廉价地进行大规模训练。然而，对于如何构建闭环训练的有效训练基准缺乏了解。在这项工作中，我们提出了第一项经验研究，分析了不同训练基准设计对于学习代理成功的影响，例如如何设计交通场景和规模化训练环境。此外，我们发现许多流行的强化学习算法在自动驾驶的背景下无法达到令人满意的性能，因为它们缺乏长期规划，并且训练时间非常长。为了解决这些问题，我们提出了轨迹价值学习（TRAVL），一种基于强化学习的驾驶代理，通过多步预测进行规划，并利用廉价生成的虚拟数据进行高效学习。

    Recent advances in high-fidelity simulators have enabled closed-loop training of autonomous driving agents, potentially solving the distribution shift in training v.s. deployment and allowing training to be scaled both safely and cheaply. However, there is a lack of understanding of how to build effective training benchmarks for closed-loop training. In this work, we present the first empirical study which analyzes the effects of different training benchmark designs on the success of learning agents, such as how to design traffic scenarios and scale training environments. Furthermore, we show that many popular RL algorithms cannot achieve satisfactory performance in the context of autonomous driving, as they lack long-term planning and take an extremely long time to train. To address these issues, we propose trajectory value learning (TRAVL), an RL-based driving agent that performs planning with multistep look-ahead and exploits cheaply generated imagined data for efficient learning. O
    
[^96]: 保护隐私的本地分布多网络社区检测

    Privacy-Preserving Community Detection for Locally Distributed Multiple Networks. (arXiv:2306.15709v1 [cs.SI])

    [http://arxiv.org/abs/2306.15709](http://arxiv.org/abs/2306.15709)

    本文提出了一种保护隐私的本地分布多网络社区检测方法，利用隐私保护来进行共识社区检测和估计。采用随机响应机制对网络边进行扰动，通过隐私保护分布式谱聚类算法在扰动邻接矩阵上执行，以防止社区之间的抵消。同时，开发了两步偏差调整过程来消除扰动和网络矩阵带来的偏差。

    

    现代多层网络由于隐私、所有权和通信成本的原因，常常以本地和分布式的方式存储和分析。关于基于这些数据的模型化统计方法用于社区检测的文献仍然有限。本文提出了一种新的方法，用于基于本地存储和计算的网络数据的多层随机块模型中的共识社区检测和估计，并采用隐私保护。开发了一种名为隐私保护分布式谱聚类（ppDSC）的新算法。为了保护边的隐私，我们采用了随机响应（RR）机制来扰动网络边，该机制满足差分隐私的强概念。ppDSC算法在平方的RR扰动邻接矩阵上执行，以防止不同层之间的社区相互抵消。为了消除RR和平方网络矩阵所带来的偏差，我们开发了一个两步偏差调整过程。

    Modern multi-layer networks are commonly stored and analyzed in a local and distributed fashion because of the privacy, ownership, and communication costs. The literature on the model-based statistical methods for community detection based on these data is still limited. This paper proposes a new method for consensus community detection and estimation in a multi-layer stochastic block model using locally stored and computed network data with privacy protection. A novel algorithm named privacy-preserving Distributed Spectral Clustering (ppDSC) is developed. To preserve the edges' privacy, we adopt the randomized response (RR) mechanism to perturb the network edges, which satisfies the strong notion of differential privacy. The ppDSC algorithm is performed on the squared RR-perturbed adjacency matrices to prevent possible cancellation of communities among different layers. To remove the bias incurred by RR and the squared network matrices, we develop a two-step bias-adjustment procedure.
    
[^97]: 量子联邦学习：分析、设计和实现挑战

    Quantum Federated Learning: Analysis, Design and Implementation Challenges. (arXiv:2306.15708v1 [quant-ph])

    [http://arxiv.org/abs/2306.15708](http://arxiv.org/abs/2306.15708)

    该论文提供了关于量子联邦学习（QFL）的综述，包括QFL框架的设计思路、应用案例和关键因素的考虑，同时分析了各种QFL研究项目的技术贡献和局限性，并提出了未来的研究方向和待解决的问题。

    

    由于量子计算和机器学习的进展，量子联邦学习（QFL）引起了广泛关注。随着对QFL的需求不断增加，迫切需要了解其在分布式环境中的复杂性。本文旨在全面介绍当前QFL的研究现状，填补现有文献中的重要知识空白。我们提出了新的QFL框架的思路，探讨了各种应用案例，并考虑了影响其设计的关键因素。同时，对各种QFL研究项目的技术贡献和局限性进行了考察，并提出了未来的研究方向和待解决的问题。

    Quantum Federated Learning (QFL) has gained significant attention due to quantum computing and machine learning advancements. As the demand for QFL continues to surge, there is a pressing need to comprehend its intricacies in distributed environments. This paper aims to provide a comprehensive overview of the current state of QFL, addressing a crucial knowledge gap in the existing literature. We develop ideas for new QFL frameworks, explore diverse use cases of applications, and consider the critical factors influencing their design. The technical contributions and limitations of various QFL research projects are examined while presenting future research directions and open questions for further exploration.
    
[^98]: 关于用于高效的无数据对抗检测的通用对抗扰动

    On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection. (arXiv:2306.15705v1 [cs.CL])

    [http://arxiv.org/abs/2306.15705](http://arxiv.org/abs/2306.15705)

    本论文提出了一个用于高效的无数据对抗检测的通用对抗扰动方法，通过发现对抗样本与高维输入中的特定向量的关系，计算出通用对抗扰动（UAPs）。基于此，提出了一个无数据对抗检测框架，通过对UAPs对正常样本和对抗样本的反应产生不同的结果，从而实现了在各种文本分类任务上具有竞争力的检测性能。具体实验结果显示，该方法维持了与正常推断相等的时间消耗。

    

    检测经过精心设计的对抗样本以欺骗模型是确保社交安全应用的关键步骤。然而，现有的对抗检测方法需要访问足够的训练数据，这引发了与隐私泄露和通用性相关的明显关注。在这项工作中，我们验证了攻击算法产生的对抗样本与高维输入中的特定向量密切相关。这些向量称为通用对抗扰动（UAPs），可以在没有原始训练数据的情况下计算得出。基于这一发现，我们提出了一个无数据对抗检测框架，该框架通过对通用对抗扰动对正常样本和对抗样本产生不同的反应。实验结果显示，我们的方法在各种文本分类任务上具有竞争力的检测性能，并且维持了与正常推断相等的时间消耗。

    Detecting adversarial samples that are carefully crafted to fool the model is a critical step to socially-secure applications. However, existing adversarial detection methods require access to sufficient training data, which brings noteworthy concerns regarding privacy leakage and generalizability. In this work, we validate that the adversarial sample generated by attack algorithms is strongly related to a specific vector in the high-dimensional inputs. Such vectors, namely UAPs (Universal Adversarial Perturbations), can be calculated without original training data. Based on this discovery, we propose a data-agnostic adversarial detection framework, which induces different responses between normal and adversarial samples to UAPs. Experimental results show that our method achieves competitive detection performance on various text classification tasks, and maintains an equivalent time consumption to normal inference.
    
[^99]: Imitation with Spatial-Temporal Heatmap: NuPlan挑战第二名解决方案

    Imitation with Spatial-Temporal Heatmap: 2nd Place Solution for NuPlan Challenge. (arXiv:2306.15700v1 [cs.RO])

    [http://arxiv.org/abs/2306.15700](http://arxiv.org/abs/2306.15700)

    本文介绍了我们在NuPlan挑战赛中的第二名解决方案。我们的方法通过使用空间-时间热力图预测未来多模态状态，并使用轨迹优化技术实现安全的自主驾驶规划。

    

    本文介绍了我们在NuPlan挑战赛2023中的第二名解决方案。在现实世界中进行自主驾驶是非常复杂和不确定的。在复杂的多模态场景中实现安全规划是一项非常具有挑战性的任务。我们的方法，基于空间-时间热力图的模仿学习，创新性地使用热力图表示预测未来的多模态状态，并使用轨迹优化技术来确保最终的安全性。实验证明，我们的方法有效地平衡了车辆的进展和安全性，生成了安全又舒适的轨迹。在NuPlan比赛中，我们取得了第二高的总分，同时在自我进展和舒适度指标中获得了最佳得分。

    This paper presents our 2nd place solution for the NuPlan Challenge 2023. Autonomous driving in real-world scenarios is highly complex and uncertain. Achieving safe planning in the complex multimodal scenarios is a highly challenging task. Our approach, Imitation with Spatial-Temporal Heatmap, adopts the learning form of behavior cloning, innovatively predicts the future multimodal states with a heatmap representation, and uses trajectory refinement techniques to ensure final safety. The experiment shows that our method effectively balances the vehicle's progress and safety, generating safe and comfortable trajectories. In the NuPlan competition, we achieved the second highest overall score, while obtained the best scores in the ego progress and comfort metrics.
    
[^100]: 使用条件生成对抗网络进行益智游戏的过程化内容生成

    Procedural content generation of puzzle games using conditional generative adversarial networks. (arXiv:2306.15696v1 [cs.AI])

    [http://arxiv.org/abs/2306.15696](http://arxiv.org/abs/2306.15696)

    本文介绍了一种实验方法，使用参数化生成对抗网络为益智游戏Lily's Garden生成关卡。虽然GAN在逼近地图形状方面表现良好，但在逼近方块分布方面存在困难。可能通过尝试替代GAN的架构来改进这一情况。

    

    本文提出了一种使用参数化生成对抗网络（GAN）为益智游戏Lily's Garden生成关卡的实验方法。我们从真实关卡中提取两个条件向量，以控制GAN输出的细节。虽然GAN在逼近第一个条件（地图形状）方面表现良好，但在逼近第二个条件（方块分布）方面却有困难。我们假设通过尝试替代GAN的生成器和判别器的架构可能会改进这一情况。

    In this article, we present an experimental approach to using parameterized Generative Adversarial Networks (GANs) to produce levels for the puzzle game Lily's Garden. We extract two condition vectors from the real levels in an effort to control the details of the GAN's outputs. While the GANs perform well in approximating the first condition (map shape), they struggle to approximate the second condition (piece distribution). We hypothesize that this might be improved by trying out alternative architectures for both the Generator and Discriminator of the GANs.
    
[^101]: 基于赌博算法的网络拓扑和舆论动力学的联合学习研究

    Joint Learning of Network Topology and Opinion Dynamics Based on Bandit Algorithms. (arXiv:2306.15695v1 [cs.SI])

    [http://arxiv.org/abs/2306.15695](http://arxiv.org/abs/2306.15695)

    本文提出了一种基于多臂赌博算法的学习算法，用于联合学习网络拓扑和混合舆论动力学。通过改进网络和更新规则的初始估计，减少预测误差，并在数值实验中表现出了优于其他方法的性能。

    

    本文研究了网络拓扑和混合舆论动力学的联合学习，其中代理人可能具有不同的更新规则。这种模型捕捉了现实个体互动的多样性。我们提出了一种基于多臂赌博算法的学习算法来解决这个问题。算法的目标是从几个候选规则中找到每个代理人的更新规则，并学习底层网络。在每次迭代中，算法假设每个代理人都有一个更新的规则，然后修改网络估计以减少验证误差。数值实验表明，所提出的算法改进了网络和更新规则的初始估计，减少了预测误差，并且比其他方法（如稀疏线性回归和高斯过程回归）表现更好。

    We study joint learning of network topology and a mixed opinion dynamics, in which agents may have different update rules. Such a model captures the diversity of real individual interactions. We propose a learning algorithm based on multi-armed bandit algorithms to address the problem. The goal of the algorithm is to find each agent's update rule from several candidate rules and to learn the underlying network. At each iteration, the algorithm assumes that each agent has one of the updated rules and then modifies network estimates to reduce validation error. Numerical experiments show that the proposed algorithm improves initial estimates of the network and update rules, decreases prediction error, and performs better than other methods such as sparse linear regression and Gaussian process regression.
    
[^102]: Voicebox：大规模的多语言通用语音生成模型

    Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale. (arXiv:2306.15687v1 [eess.AS])

    [http://arxiv.org/abs/2306.15687](http://arxiv.org/abs/2306.15687)

    Voicebox是一种大规模的多语言通用语音生成模型，通过使用非自回归的流匹配模型，在文本和音频上下文条件下进行训练，可以实现零样本跨语言文本到语音合成，噪声去除，内容编辑，风格转换和多样化的样本生成等多种任务。

    

    大规模生成模型，如GPT和DALL-E已经改变了自然语言处理和计算机视觉研究的方式。这些模型不仅可以生成高质量的文本或图像输出，而且还是通用的，可以解决未被明确教授的任务。相比之下，语音生成模型在规模和任务通用化方面仍然比较原始。在本文中，我们介绍了Voicebox，这是最多功能的面向规模的文本引导生成模型。Voicebox是一个非自回归的流匹配模型，通过在音频上下文和文本条件下进行训练，用50,000小时的未经过滤或增强的语音进行填充。与GPT类似，Voicebox可以通过上下文学习执行多种不同的任务，但更加灵活，因为它还可以对未来的上下文进行条件约束。Voicebox可以用于单语或跨语言零样本的文本到语音合成，噪声去除，内容编辑，风格转换和多样化的样本生成。特别是，Voicebox

    Large-scale generative models such as GPT and DALL-E have revolutionized natural language processing and computer vision research. These models not only generate high fidelity text or image outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are neither filtered nor enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox 
    
[^103]: 快速确定单光子源质量的挑战

    The Challenge of Quickly Determining the Quality of a Single-Photon Source. (arXiv:2306.15683v1 [physics.optics])

    [http://arxiv.org/abs/2306.15683](http://arxiv.org/abs/2306.15683)

    该研究通过使用数据增强技术，结合实验数据和自举样本，提出了一种快速评估单光子源质量的方法，并揭示了多光子发射事件概率的不确定性对质量评估的重要影响。

    

    近期文献提出了新颖方法来快速评估单光子源（SPS）的质量，例如量子点，以解决实验验证通过光强干涉仪验证效率低和耗时长的问题。然而，由于缺乏不确定性讨论和可重复性细节，这些方法的可靠性引发了关切。本研究对从发射波长为1.3μm，由80MHz激光器激发的InGaAs/GaAs外延量子点获得的八组数据进行了研究。该研究通过采用数据增强的机器学习技术，将实验数据与自举样本相结合，引入了一种新颖的贡献。对合成样本的高效直方图拟合导出的SPS质量指标，即多光子发射事件的概率进行分析，揭示了由描述探测率的泊松过程中随机变异性贡献的显著不确定性。忽视这个因素可能导致对SPS质量的错误评估。

    Novel methods for rapidly estimating single-photon source (SPS) quality, e.g. of quantum dots, have been promoted in recent literature to address the expensive and time-consuming nature of experimental validation via intensity interferometry. However, the frequent lack of uncertainty discussions and reproducible details raises concerns about their reliability. This study investigates one such proposal on eight datasets obtained from an InGaAs/GaAs epitaxial quantum dot that emits at 1.3 {\mu}m and is excited by an 80 MHz laser. The study introduces a novel contribution by employing data augmentation, a machine learning technique, to supplement experimental data with bootstrapped samples. Analysis of the SPS quality metric, i.e. the probability of multi-photon emission events, as derived from efficient histogram fitting of the synthetic samples, reveals significant uncertainty contributed by stochastic variability in the Poisson processes that describe detection rates. Ignoring this sou
    
[^104]: ECG-QA：结合心电图的综合问答数据集

    ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram. (arXiv:2306.15681v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.15681](http://arxiv.org/abs/2306.15681)

    ECG-QA是第一个专为心电图分析设计的问答数据集，包括涵盖广泛临床相关ECG主题的问题模板和多样化的ECG解读问题。这一资源将为未来的医疗保健问答研究提供有价值的见解。

    

    由于自然语言处理的显著进展，医疗保健领域中的问答问题（QA）引起了广泛关注。然而，现有的医疗保健QA数据集主要集中在医学影像、临床记录或结构化的电子健康记录表上。这使得将心电图（ECG）数据与这些系统相结合的巨大潜力几乎未被利用。为填补这一空白，我们提出了ECG-QA，这是专门针对ECG分析设计的第一个QA数据集。该数据集包括共70个涵盖了广泛临床相关ECG主题的问题模板，每个问题都经过一名ECG专家的验证，以确保其临床效用。因此，我们的数据集包含了多样化的ECG解读问题，包括需要对两个不同的ECG进行比较分析的问题。此外，我们还进行了许多实验，为未来的研究方向提供了有价值的见解。我们相信ECG-QA将成为一个宝贵的资源，供研究者们探索和应用。

    Question answering (QA) in the field of healthcare has received much attention due to significant advancements in natural language processing. However, existing healthcare QA datasets primarily focus on medical images, clinical notes, or structured electronic health record tables. This leaves the vast potential of combining electrocardiogram (ECG) data with these systems largely untapped. To address this gap, we present ECG-QA, the first QA dataset specifically designed for ECG analysis. The dataset comprises a total of 70 question templates that cover a wide range of clinically relevant ECG topics, each validated by an ECG expert to ensure their clinical utility. As a result, our dataset includes diverse ECG interpretation questions, including those that require a comparative analysis of two different ECGs. In addition, we have conducted numerous experiments to provide valuable insights for future research directions. We believe that ECG-QA will serve as a valuable resource for the de
    
[^105]: 从自然语言描述生成参数化BRDFs

    Generating Parametric BRDFs from Natural Language Descriptions. (arXiv:2306.15679v1 [cs.GR])

    [http://arxiv.org/abs/2306.15679](http://arxiv.org/abs/2306.15679)

    这项研究开发了一个模型，可以根据描述性的文本提示生成参数化的BRDFs，为艺术性地创作3D环境提供了一种新的方法。

    

    艺术性地创作3D环境是一项艰巨的工作，同时也需要熟练的内容创作者。使用机器学习来解决生成3D内容的不同方面，如生成网格、排列几何、合成纹理等方面已取得了令人印象深刻的改进。在本文中，我们开发了一个模型，可以根据描述性的文本提示生成双向反射分布函数（BRDFs）。 BRDFs是表征光与表面材料相互作用的四维概率分布。它们可以被表示为参数化的形式，也可以通过列举每对入射和出射角度的概率密度来表示。前者适用于艺术编辑，而后者则用于测量实际材料的外观。许多工作都集中在从材料图像假设BRDF模型。我们学习了从材料的文本描述到参数化BRDF的映射。我们的模型是一个...

    Artistic authoring of 3D environments is a laborious enterprise that also requires skilled content creators. There have been impressive improvements in using machine learning to address different aspects of generating 3D content, such as generating meshes, arranging geometry, synthesizing textures, etc. In this paper we develop a model to generate Bidirectional Reflectance Distribution Functions (BRDFs) from descriptive textual prompts. BRDFs are four dimensional probability distributions that characterize the interaction of light with surface materials. They are either represented parametrically, or by tabulating the probability density associated with every pair of incident and outgoing angles. The former lends itself to artistic editing while the latter is used when measuring the appearance of real materials. Numerous works have focused on hypothesizing BRDF models from images of materials. We learn a mapping from textual descriptions of materials to parametric BRDFs. Our model is f
    
[^106]: KAPLA：可扩展NN加速器数据流的实用化表示和快速求解

    KAPLA: Pragmatic Representation and Fast Solving of Scalable NN Accelerator Dataflow. (arXiv:2306.15676v1 [cs.AR])

    [http://arxiv.org/abs/2306.15676](http://arxiv.org/abs/2306.15676)

    本文提出了KAPLA，一个用于可扩展NN加速器数据流优化的快速求解器。通过实用的指令和全面的数据流表示，KAPLA能够有效地进行设计空间的探索，并快速确定方案的有效性和效率。

    

    数据流调度决策对神经网络（NN）加速器至关重要。最近可扩展的NN加速器支持一组丰富的先进数据流技术。因此，全面表示和快速找到优化的数据流方案的问题变得更加复杂和具有挑战性。在这项工作中，我们首先提出了可扩展多节点NN架构上时间和空间调度的全面实用化数据流表示。一份非正式的分层分类表明，数据流空间不同层次之间的紧密耦合是快速设计探索的主要难点。一组形式化的张量中心指令准确地表示各种层间和层内方案，并允许快速确定其有效性和效率。然后，我们构建了一个通用的、优化的和快速的数据流求解器KAPLA，利用实用的指令来进行设计空间的有效有效性检查和优化方案的探索。

    Dataflow scheduling decisions are of vital importance to neural network (NN) accelerators. Recent scalable NN accelerators support a rich set of advanced dataflow techniques. The problems of comprehensively representing and quickly finding optimized dataflow schemes thus become significantly more complicated and challenging. In this work, we first propose comprehensive and pragmatic dataflow representations for temporal and spatial scheduling on scalable multi-node NN architectures. An informal hierarchical taxonomy highlights the tight coupling across different levels of the dataflow space as the major difficulty for fast design exploration. A set of formal tensor-centric directives accurately express various inter-layer and intra-layer schemes, and allow for quickly determining their validity and efficiency. We then build a generic, optimized, and fast dataflow solver, KAPLA, which makes use of the pragmatic directives to explore the design space with effective validity check and eff
    
[^107]: 异步算法与Cocycles的对齐

    Asynchronous Algorithmic Alignment with Cocycles. (arXiv:2306.15632v1 [cs.LG])

    [http://arxiv.org/abs/2306.15632](http://arxiv.org/abs/2306.15632)

    该论文提出了一种将节点状态更新和消息函数调用分离的数学框架，以实现异步计算，并以此作为基础，进行了异步算法和神经网络的对齐。

    

    最先进的神经算法推理器使用图神经网络（GNN）中的消息传递。但是，典型的GNN在定义和调用消息函数之间模糊了区别，迫使节点在每一层都向其邻居发送消息，同步地进行。然而，当将GNN应用于学习执行动态规划算法时，大多数步骤只有少数几个节点会有有意义的更新要发送。因此，通过在图中发送太多无关的数据，可能导致低效率，而许多中间的GNN步骤必须学习身份函数。在这项工作中，我们明确地分离了节点状态更新和消息函数调用的概念。通过这种分离，我们得到了一个数学表达，可以让我们思考算法和神经网络中的异步计算。

    State-of-the-art neural algorithmic reasoners make use of message passing in graph neural networks (GNNs). But typical GNNs blur the distinction between the definition and invocation of the message function, forcing a node to send messages to its neighbours at every layer, synchronously. When applying GNNs to learn to execute dynamic programming algorithms, however, on most steps only a handful of the nodes would have meaningful updates to send. One, hence, runs the risk of inefficiencies by sending too much irrelevant data across the graph -- with many intermediate GNN steps having to learn identity functions. In this work, we explicitly separate the concepts of node state update and message function invocation. With this separation, we obtain a mathematical formulation that allows us to reason about asynchronous computation in both algorithms and neural networks.
    
[^108]: 通过位置插值扩展大型语言模型的上下文窗口

    Extending Context Window of Large Language Models via Positional Interpolation. (arXiv:2306.15595v1 [cs.CL])

    [http://arxiv.org/abs/2306.15595](http://arxiv.org/abs/2306.15595)

    通过位置插值方法，我们可以在最小微调的情况下将RoPE-based预训练语言模型的上下文窗口扩展到最多32768，并在多个任务上获得强有力的实证结果。通过线性降低输入位置索引的大小，我们保持了扩展模型在原始上下文窗口内任务的质量。

    

    我们提出了一种位置插值（PI）方法，可以在最小微调的情况下将RoPE-based预训练语言模型（如LLaMA模型）的上下文窗口大小扩展到最多32768，并且在需要长上下文的各种任务（包括密钥检索、语言建模和长篇文档摘要等）上展现出良好的实证结果。同时，通过位置插值扩展的模型在原始上下文窗口内的任务中相对保持良好的质量。为了实现这一目标，位置插值线性地降低输入位置索引的大小，以匹配原始的上下文窗口大小，而不是超过训练时上下文长度，这可能会导致严重的高注意力分数，完全破坏自注意机制。我们的理论研究表明，插值的上界至少是推断的上界的$\sim 600 \times$要小，进一步证明了其稳定性。

    We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least $\sim 600 \times$ smaller than that of extrapolation, further demonstrating its stability. Models extend
    
[^109]: 几何超声定位显微镜

    Geometric Ultrasound Localization Microscopy. (arXiv:2306.15548v1 [cs.CV])

    [http://arxiv.org/abs/2306.15548](http://arxiv.org/abs/2306.15548)

    这项研究提出了一种基于几何框架的超声定位显微镜方法，通过仅依赖到达时间差信息实现微气泡的定位，并在精度和可靠性方面超越了现有的方法。

    

    对比增强超声（CEUS）已成为无创动态可视化医学诊断方法，然而超声定位显微镜（ULM）通过提供十倍更高的分辨率实现了突破性进展。到目前为止，延迟和求和（DAS）波束形成器被用于渲染ULM帧，最终确定图像的分辨率能力。为了充分利用ULM，本研究质疑波束形成是否是ULM最有效的处理步骤，提出了一种仅依赖到达时间差（TDoA）信息的替代方法。为此，提出了一种新颖的通过椭圆交点定位微气泡的几何框架，以克服现有波束形成的局限性。我们基于一个公共数据集进行了基准比较，结果表明我们的几何ULM在精度和可靠性方面优于现有的基准方法，仅利用了部分可用的换能器数据。

    Contrast-Enhanced Ultra-Sound (CEUS) has become a viable method for non-invasive, dynamic visualization in medical diagnostics, yet Ultrasound Localization Microscopy (ULM) has enabled a revolutionary breakthrough by offering ten times higher resolution. To date, Delay-And-Sum (DAS) beamformers are used to render ULM frames, ultimately determining the image resolution capability. To take full advantage of ULM, this study questions whether beamforming is the most effective processing step for ULM, suggesting an alternative approach that relies solely on Time-Difference-of-Arrival (TDoA) information. To this end, a novel geometric framework for micro bubble localization via ellipse intersections is proposed to overcome existing beamforming limitations. We present a benchmark comparison based on a public dataset for which our geometric ULM outperforms existing baseline methods in terms of accuracy and reliability while only utilizing a portion of the available transducer data.
    
[^110]: MIMIC: 基于图像对应关系的遮蔽图像建模

    MIMIC: Masked Image Modeling with Image Correspondences. (arXiv:2306.15128v1 [cs.CV])

    [http://arxiv.org/abs/2306.15128](http://arxiv.org/abs/2306.15128)

    MIMIC是一种基于图像对应关系的遮蔽图像建模方法，通过挖掘不需要任何注释的数据集，使用多个自监督模型进行训练，达到了在多个下游任务上优于使用注释挖掘的表示的效果。

    

    许多像素级的密集预测任务——如计算机视觉中的深度估计和语义分割——如今依赖于预训练的图像表示。因此，筛选有效的预训练数据集至关重要。不幸的是，有效的预训练数据集仅通过模拟环境中的带有注释的3D网格、点云和相机参数筛选而来，并不具备多视角场景。我们提出了一种不需要任何注释的数据集筛选机制。我们从开源视频数据集和合成的3D环境中挖掘了两个数据集：MIMIC-1M(包含1.3M个多视角图像对)和MIMIC-3M(包含3.1M个多视角图像对)。我们使用多个自监督模型进行训练，采用不同的遮蔽图像建模目标，展示了以下发现：在多个下游任务中，基于MIMIC-3M训练的表示优于使用注释挖掘的表示，包括深度估计、语义分割、表面法线和姿态估计等。

    Many pixelwise dense prediction tasks-depth estimation and semantic segmentation in computer vision today rely on pretrained image representations. Therefore, curating effective pretraining datasets is vital. Unfortunately, the effective pretraining datasets are those with multi-view scenes and have only been curated using annotated 3D meshes, point clouds, and camera parameters from simulated environments. We propose a dataset-curation mechanism that does not require any annotations. We mine two datasets: MIMIC-1M with 1.3M and MIMIC-3M with 3.1M multi-view image pairs from open-sourced video datasets and from synthetic 3D environments. We train multiple self-supervised models with different masked image modeling objectives to showcase the following findings: Representations trained on MIMIC-3M outperform those mined using annotations on multiple downstream tasks, including depth estimation, semantic segmentation, surface normals, and pose estimation. They also outperform representati
    
[^111]: 博士论文：探索认知和计算机视觉架构中的(自我)注意力的作用

    PhD Thesis: Exploring the role of (self-)attention in cognitive and computer vision architecture. (arXiv:2306.14650v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.14650](http://arxiv.org/abs/2306.14650)

    该论文研究了注意力和记忆在复杂推理任务中的作用，通过以Transformer为基础模型并结合记忆，扩展了自我注意力模型。研究结果表明，在视觉推理任务中，使用基于特征和空间注意力的自我注意力与ResNet50相结合可以高效解决具有挑战性的任务。此外，该论文提出了基于注意力和记忆的认知架构GAMR，它在样本效率、鲁棒性和组合性方面优于其他架构，并具有对新的推理任务的零样本泛化能力。

    

    我们研究了注意力和记忆在复杂推理任务中的作用。我们通过分析基于Transformer的自我注意力模型并将其与记忆相结合来扩展它。通过研究合成视觉推理测试，我们完善了推理任务的分类法。通过将自我注意力与ResNet50结合，我们使用基于特征和空间注意力增强特征图，从而实现了对具有挑战性的视觉推理任务的高效解决。我们的研究结果有助于理解SVRT任务对注意力的需求。此外，我们提出了GAMR，一种结合了注意力和记忆的认知架构，灵感来自主动视觉理论。GAMR在样本效率、鲁棒性和组合性方面优于其他架构，并在新的推理任务上表现出零样本泛化能力。

    We investigate the role of attention and memory in complex reasoning tasks. We analyze Transformer-based self-attention as a model and extend it with memory. By studying a synthetic visual reasoning test, we refine the taxonomy of reasoning tasks. Incorporating self-attention with ResNet50, we enhance feature maps using feature-based and spatial attention, achieving efficient solving of challenging visual reasoning tasks. Our findings contribute to understanding the attentional needs of SVRT tasks. Additionally, we propose GAMR, a cognitive architecture combining attention and memory, inspired by active vision theory. GAMR outperforms other architectures in sample efficiency, robustness, and compositionality, and shows zero-shot generalization on new reasoning tasks.
    
[^112]: 神经符号反向规划引擎（NIPE）：基于语言输入的概率社交推理建模

    The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs. (arXiv:2306.14325v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.14325](http://arxiv.org/abs/2306.14325)

    本文提出了一个神经符号模型，用于从语言输入中进行目标推断，并通过人类实验验证了该模型的准确性和优势。

    

    人类是社交性的生物。我们经常推理其他智能体，而这种社交推理的关键部分是在了解他们的行为时推断他们的目标。在许多情况下，我们可以从语言描述的智能体、动作和背景环境中进行直观但可靠的目标推断。在本文中，我们研究了在一个概率目标推断领域中，语言驱动和影响社交推理的过程。我们提出了一个神经符号模型，该模型从智能体场景的语言输入中进行目标推断。其中的“神经”部分是一个大型语言模型（LLM），将语言描述转化为代码表示，而“符号”部分则是一个贝叶斯反向规划引擎。为了测试我们的模型，我们设计和进行了一个关于语言目标推断任务的人类实验。我们的模型与人类的反应模式非常相似，并且比单独使用LLM更好地预测了人类的判断。

    Human beings are social creatures. We routinely reason about other agents, and a crucial component of this social reasoning is inferring people's goals as we learn about their actions. In many settings, we can perform intuitive but reliable goal inference from language descriptions of agents, actions, and the background environments. In this paper, we study this process of language driving and influencing social reasoning in a probabilistic goal inference domain. We propose a neuro-symbolic model that carries out goal inference from linguistic inputs of agent scenarios. The "neuro" part is a large language model (LLM) that translates language descriptions to code representations, and the "symbolic" part is a Bayesian inverse planning engine. To test our model, we design and run a human experiment on a linguistic goal inference task. Our model closely matches human response patterns and better predicts human judgements than using an LLM alone.
    
[^113]: 一种减少联邦学习通信的高效虚拟数据生成方法

    An Efficient Virtual Data Generation Method for Reducing Communication in Federated Learning. (arXiv:2306.12088v1 [cs.LG])

    [http://arxiv.org/abs/2306.12088](http://arxiv.org/abs/2306.12088)

    本论文提出FedINIBoost，一种新的联邦学习方法，通过梯度匹配构建有效的提取模块，在减少通信开销的同时提高了模型准确率。

    

    通信开销是联邦学习中的主要挑战之一。一些经典的方案假设服务器可以从本地模型中提取参与者训练数据的辅助信息来构建中央虚拟数据集。服务器使用虚拟数据集来微调聚合的全局模型，以在较少的通信轮次内达到目标测试精度。本文将上述解决方案概括为基于数据的通信高效联邦学习框架。提出框架的关键是设计一个有效的提取模块（EM），它确保虚拟数据集对微调聚合的全局模型产生积极影响。与现有方法使用生成器来设计EM不同，我们提出的方法FedINIBoost借鉴了梯度匹配的思想来构建EM。具体而言，FedINIBoost在每个通信轮次的每个参与者中使用两个步骤构建真实数据集的代理数据集。然后服务器聚合所有的代理数据集来构建中央虚拟数据集。

    Communication overhead is one of the major challenges in Federated Learning(FL). A few classical schemes assume the server can extract the auxiliary information about training data of the participants from the local models to construct a central dummy dataset. The server uses the dummy dataset to finetune aggregated global model to achieve the target test accuracy in fewer communication rounds. In this paper, we summarize the above solutions into a data-based communication-efficient FL framework. The key of the proposed framework is to design an efficient extraction module(EM) which ensures the dummy dataset has a positive effect on finetuning aggregated global model. Different from the existing methods that use generator to design EM, our proposed method, FedINIBoost borrows the idea of gradient match to construct EM. Specifically, FedINIBoost builds a proxy dataset of the real dataset in two steps for each participant at each communication round. Then the server aggregates all the pr
    
[^114]: G-NM：一组数字时间序列预测模型

    G-NM: A Group of Numerical Time Series Prediction Models. (arXiv:2306.11667v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11667](http://arxiv.org/abs/2306.11667)

    G-NM是一组集合了传统和现代模型的数字时间序列预测模型，旨在提高对复杂自然现象中的模式和趋势的预测能力。

    

    本研究聚焦于开发和实施一个综合的数字时间序列预测模型集合，统称为数字时间序列预测模型组（G-NM）。该集合包括传统模型如自回归综合移动平均（ARIMA）、Holt-Winters方法和支持向量回归（SVR），以及现代神经网络模型，如循环神经网络（RNN）和长短期记忆（LSTM）。G-NM明确构建以增强我们对复杂自然现象中固有模式和趋势的预测能力。通过利用与这些事件相关的时间序列数据，G-NM便于对此类现象在延长时间段内进行预测。本研究的主要目标是推进我们对此类事件的理解，并大幅提高预测准确性。G-NM包括线性和非线性依赖关系，以及季节性趋势。

    In this study, we focus on the development and implementation of a comprehensive ensemble of numerical time series forecasting models, collectively referred to as the Group of Numerical Time Series Prediction Model (G-NM). This inclusive set comprises traditional models such as Autoregressive Integrated Moving Average (ARIMA), Holt-Winters' method, and Support Vector Regression (SVR), in addition to modern neural network models including Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM). G-NM is explicitly constructed to augment our predictive capabilities related to patterns and trends inherent in complex natural phenomena. By utilizing time series data relevant to these events, G-NM facilitates the prediction of such phenomena over extended periods. The primary objective of this research is to both advance our understanding of such occurrences and to significantly enhance the accuracy of our forecasts. G-NM encapsulates both linear and non-linear dependencies, seasonal
    
[^115]: 正则化鲁棒的MDPs和风险敏感的MDPs：等价性、策略梯度和样本复杂度

    Regularized Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy Gradient, and Sample Complexity. (arXiv:2306.11626v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2306.11626](http://arxiv.org/abs/2306.11626)

    本论文研究了正则化鲁棒MDP问题和风险敏感MDP问题的相关性，并提出了有效的学习算法和样本复杂度分析。

    

    本论文关注于正则化鲁棒马尔可夫决策过程（MDP）问题的强化学习，它是鲁棒MDP框架的一个扩展。我们首先介绍了风险敏感MDP，并建立了风险敏感MDP和正则化鲁棒MDP之间的等价性。这种等价性为解决正则化RMDP提供了另一种视角，并且使得设计高效的学习算法成为可能。在这种等价性的基础上，我们进一步推导了正则化鲁棒MDP问题的策略梯度定理，并在具有直接参数化的表格设置下证明了精确策略梯度方法的全局收敛性。我们还提出了一种基于样本的离线学习算法，即鲁棒的FZI迭代，用于具有KL散度正则化项的特定正则化鲁棒MDP问题，并分析了算法的样本复杂度。我们的结果也得到了数值模拟的支持。

    This paper focuses on reinforcement learning for the regularized robust Markov decision process (MDP) problem, an extension of the robust MDP framework. We first introduce the risk-sensitive MDP and establish the equivalence between risk-sensitive MDP and regularized robust MDP. This equivalence offers an alternative perspective for addressing the regularized RMDP and enables the design of efficient learning algorithms. Given this equivalence, we further derive the policy gradient theorem for the regularized robust MDP problem and prove the global convergence of the exact policy gradient method under the tabular setting with direct parameterization. We also propose a sample-based offline learning algorithm, namely the robust fitted-Z iteration (RFZI), for a specific regularized robust MDP problem with a KL-divergence regularization term and analyze the sample complexity of the algorithm. Our results are also supported by numerical simulations.
    
[^116]: 一种用于基于聚合观测的分类的通用无偏方法。

    A Universal Unbiased Method for Classification from Aggregate Observations. (arXiv:2306.11343v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11343](http://arxiv.org/abs/2306.11343)

    这项研究提出了一种新的基于聚合观测的分类方法，通过为每个实例的每个标签权衡重要性，为分类器提供纯化的监督来学习，从而实现了任意损失的分类风险的无偏估计。

    

    在传统的监督分类中，需要对个体实例进行真实标签的收集。然而，出于隐私和昂贵的注释成本等原因，为个体实例收集真实标签可能是困难的。这激发了对基于聚合观测的分类（CFAO）的研究，其中监督是以组的形式提供的，而不是个体实例。CFAO是一个广义的学习框架，包含了各种学习问题，如多实例学习和从标签比例学习。本文的目标是提出一个新的CFAO通用方法，其具有任意损失的分类风险的无偏估计器--以前的研究未能实现这一目标。在实践中，我们的方法通过权衡群组中每个实例的每个标签的重要性，为分类器提供纯化的监督来学习。从理论上讲，我们提出的方法不仅保证了风险的控制，而且还提供了确定风险上界的有效方法。

    In conventional supervised classification, true labels are required for individual instances. However, it could be prohibitive to collect the true labels for individual instances, due to privacy concerns or unaffordable annotation costs. This motivates the study on classification from aggregate observations (CFAO), where the supervision is provided to groups of instances, instead of individual instances. CFAO is a generalized learning framework that contains various learning problems, such as multiple-instance learning and learning from label proportions. The goal of this paper is to present a novel universal method of CFAO, which holds an unbiased estimator of the classification risk for arbitrary losses -- previous research failed to achieve this goal. Practically, our method works by weighing the importance of each label for each instance in the group, which provides purified supervision for the classifier to learn. Theoretically, our proposed method not only guarantees the risk con
    
[^117]: 基于注意力知识图卷积网络的旅游景点推荐

    Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network. (arXiv:2306.10946v1 [cs.IR] CROSS LISTED)

    [http://arxiv.org/abs/2306.10946](http://arxiv.org/abs/2306.10946)

    本文提出了一种基于注意力知识图卷积网络的旅游景点推荐模型，通过自动语义发掘目标景点的相邻实体，根据旅客的喜好选择，预测类似景点的概率，实验中取得良好效果。

    

    基于知识图谱的推荐算法在相对成熟阶段，但在特定领域的推荐仍存在问题。例如在旅游领域，选择适合的旅游景点属性流程作为推荐基础较为复杂。本文提出改进的注意力知识图卷积网络模型(Att-KGCN)，自动语义地发掘目标景点的相邻实体，利用注意力层将相对相似的位置进行聚合，并通过推理旅客喜好选择，预测类似景点的概率作为推荐系统。实验中，采用索科特拉岛-也门的旅游数据，证明了注意力知识图卷积网络在旅游领域的景点推荐效果良好。

    The recommendation algorithm based on knowledge graphs is at a relatively mature stage. However, there are still some problems in the recommendation of specific areas. For example, in the tourism field, selecting suitable tourist attraction attributes process is complicated as the recommendation basis for tourist attractions. In this paper, we propose the improved Attention Knowledge Graph Convolution Network model, named (Att-KGCN), which automatically discovers the neighboring entities of the target scenic spot semantically. The attention layer aggregates relatively similar locations and represents them with an adjacent vector. Then, according to the tourist's preferred choices, the model predicts the probability of similar spots as a recommendation system. A knowledge graph dataset of tourist attractions used based on tourism data on Socotra Island-Yemen. Through experiments, it is verified that the Attention Knowledge Graph Convolution Network has a good effect on the recommendatio
    
[^118]: 虚假黎明：重新评估谷歌强化学习在芯片宏观布局中的应用

    The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])

    [http://arxiv.org/abs/2306.09633](http://arxiv.org/abs/2306.09633)

    谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。

    

    谷歌2021年在《自然》杂志上发表的有关使用强化学习设计芯片的论文，因为所声称的结果缺乏充分的文件记录和关键步骤的说明，引发争议并受到媒体的批评报道。 而两项独立的评估填补了空白，证明谷歌强化学习落后于人类设计师、落后于一种众所周知的算法（模拟退火），并且还落后于普遍可用的商业软件。交叉检查的数据表明，由于行为、分析和报告中的错误，该《自然》文章的完整性受到了严重的损害。

    Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
    
[^119]: TSMixer: 用于多元时间序列预测的轻量级MLP-Mixer模型

    TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting. (arXiv:2306.09364v1 [cs.LG])

    [http://arxiv.org/abs/2306.09364](http://arxiv.org/abs/2306.09364)

    TSMixer是一种用于多元时间序列预测的轻量级MLP-Mixer模型，可以有效地捕捉时间序列属性并在准确性方面超越了Transformers的方法。

    

    Transformers因其能够捕捉长序列交互而在时间序列预测中备受青睐。然而，其内存和计算要求高的问题对长期预测构成了严重瓶颈。为了解决这一问题，我们提出了TSMixer，这是一种轻量级神经架构，专为多元预测和补丁时间序列表示学习而设计，是Transformers的有效替代。我们的模型借鉴了MLP-Mixer模型在计算机视觉中的成功经验。我们展示了将视觉MLP-Mixer适应于时间序列的挑战，并引入了经过实验证实的组件以提高准确性。这包括一种新的设计范式，即将在线协调头附加到MLP-Mixer骨干上，以显式地建模时间序列的属性，如层次结构和通道相关性。我们还提出了一种混合通道建模方法，平衡了编码多个时间序列通道和保留单个通道信息之间的权衡。我们的实验表明，TSMixer在一元和多元时间序列预测任务中均实现了最先进的性能，同时需要比基于Transformers的方法少得多的参数。

    Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approa
    
[^120]: 自监督等式嵌入深度Lagrange对偶算法优化逼近限制优化问题

    Self-supervised Equality Embedded Deep Lagrange Dual for Approximate Constrained Optimization. (arXiv:2306.06674v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2306.06674](http://arxiv.org/abs/2306.06674)

    该论文提出了一种自监督等式嵌入深度Lagrange对偶算法，用于解决不带标签的逼近限制优化问题。此方法通过在神经网络中嵌入等式约束来确保可行解，并使用原始-对偶方法进行训练，同时DeepLDE取得了最好的优化结果。

    

    在限制优化问题中，传统求解方法通常计算量较大，特别是在规模较大、时间敏感的问题上更是如此。因此，使用神经网络作为快速最优解逼近器引起了人们的越来越大兴趣，但是将约束条件与神经网络结合起来是具有挑战性的。为此，我们提出了一种称为DeepLDE的深度Lagrange对偶算法，该框架学习在不使用标签的情况下寻找最优解，通过将等式约束嵌入神经网络来确保可行解，并使用原始-对偶方法对不等式约束进行训练。此外，我们证明了DeepLDE的收敛性，并表明仅靠原始-对偶学习方法无法确保等式约束，需要等式嵌入的帮助。在凸、非凸和交流最优潮流（AC-OPF）问题的模拟结果中，我们展示了DeepLDE的最优性能而且始终保证可行解。

    Conventional solvers are often computationally expensive for constrained optimization, particularly in large-scale and time-critical problems. While this leads to a growing interest in using neural networks (NNs) as fast optimal solution approximators, incorporating the constraints with NNs is challenging. In this regard, we propose deep Lagrange dual with equality embedding (DeepLDE), a framework that learns to find an optimal solution without using labels. To ensure feasible solutions, we embed equality constraints into the NNs and train the NNs using the primal-dual method to impose inequality constraints. Furthermore, we prove the convergence of DeepLDE and show that the primal-dual learning method alone cannot ensure equality constraints without the help of equality embedding. Simulation results on convex, non-convex, and AC optimal power flow (AC-OPF) problems show that the proposed DeepLDE achieves the smallest optimality gap among all the NN-based approaches while always ensuri
    
[^121]: 理解长尾效应对神经网络压缩的影响

    Understanding the Effect of the Long Tail on Neural Network Compression. (arXiv:2306.06238v1 [cs.LG])

    [http://arxiv.org/abs/2306.06238](http://arxiv.org/abs/2306.06238)

    本文研究了在神经网络压缩中如何保持与原始网络的语义相同，在长尾现象中探讨了压缩提高推广性能的记忆要素。

    

    网络压缩现在是神经网络研究的一个成熟的子领域，过去的十年中，取得了显著的进展，以减小模型尺寸和加速推断为目标，同时保持分类准确性。然而，许多研究观察到，仅关注总体准确性可能是误导的。例如，已经证明全模型和压缩模型之间的差异可能会偏向于在数据集中低频的类。这引出了一个重要的研究问题，即“我们能否在保持与原始网络语义等同的情况下实现网络压缩？”在本文中，我们研究了这个问题，重点关注计算机视觉数据集中Feldman等人观察到的“长尾”现象。他们认为，某些输入（适当定义）的记忆对于实现良好的泛化是必要的。由于压缩限制了网络的容量（因此也限制了其记忆能力），所以我们研究了这个问题：

    Network compression is now a mature sub-field of neural network research: over the last decade, significant progress has been made towards reducing the size of models and speeding up inference, while maintaining the classification accuracy. However, many works have observed that focusing on just the overall accuracy can be misguided. E.g., it has been shown that mismatches between the full and compressed models can be biased towards under-represented classes. This raises the important research question, \emph{can we achieve network compression while maintaining ``semantic equivalence'' with the original network?} In this work, we study this question in the context of the ``long tail'' phenomenon in computer vision datasets observed by Feldman, et al. They argue that \emph{memorization} of certain inputs (appropriately defined) is essential to achieving good generalization. As compression limits the capacity of a network (and hence also its ability to memorize), we study the question: a
    
[^122]: 基因组解释器：一种带有1D移动窗口变换器的层次基因组深度神经网络

    Genomic Interpreter: A Hierarchical Genomic Deep Neural Network with 1D Shifted Window Transformer. (arXiv:2306.05143v1 [cs.LG])

    [http://arxiv.org/abs/2306.05143](http://arxiv.org/abs/2306.05143)

    本文提出了一种名为“基因组解释器”的模型，可以对基因组数据进行预测并发现基因调控的层次依赖关系，性能优于现有模型。

    

    随着基因组数据量和质量的增加，提取新的洞见需要可解释的机器学习模型。本文提出了一种新的基因组测定预测结构：基因组解释器。该模型在基因组测定预测任务中表现优于现有模型。我们的模型可以识别基因组位点的层次依赖关系。这是通过1D-Swin进行实现的，这是我们设计的一种用于建模长范围层次数据的新型变换器块。在一个包含38,171个17K碱基对的DNA片段的数据集上进行评估，基因组解释器在染色质可达性和基因表达预测方面表现出卓越的性能，并揭示了基因调控的潜在“语法”。

    Given the increasing volume and quality of genomics data, extracting new insights requires interpretable machine-learning models. This work presents Genomic Interpreter: a novel architecture for genomic assay prediction. This model outperforms the state-of-the-art models for genomic assay prediction tasks. Our model can identify hierarchical dependencies in genomic sites. This is achieved through the integration of 1D-Swin, a novel Transformer-based block designed by us for modelling long-range hierarchical data. Evaluated on a dataset containing 38,171 DNA segments of 17K base pairs, Genomic Interpreter demonstrates superior performance in chromatin accessibility and gene expression prediction and unmasks the underlying `syntax' of gene regulation.
    
[^123]: 自适应基于梯度的异常值去除的嘈杂标签学习方法

    Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal. (arXiv:2306.04502v1 [cs.LG])

    [http://arxiv.org/abs/2306.04502](http://arxiv.org/abs/2306.04502)

    本文提出了一种名为AGRA的自适应梯度异常值去除方法，能够在模型训练过程中动态调整数据集从而有效提高模型学习效果。

    

    训练可靠和高性能模型需要准确和丰富的数据集，但即便是人工标注的数据集也会包含错误，更不用说自动标注的数据集了。现有的一些数据去噪方法主要集中于检测异常值并进行永久性去除，但这种方法很容易过度或者欠度过滤数据集。在本论文中，我们提出了一种新的自适应梯度异常值去除方法（AGRA），不同于在模型训练之前清洗数据集，我们的方法在训练过程中动态调整数据集。通过比较一组样本的累积梯度和单个样本的梯度，我们的方法可以决定是否在当前更新时保留对应的样本，以此来确定它是否有助于模型的学习效果。在多个数据集上进行的广泛评估表明，AGRA方法的有效性，并且全面的结果分析证实了我们方法的理论和实践收益。

    An accurate and substantial dataset is necessary to train a reliable and well-performing model. However, even manually labeled datasets contain errors, not to mention automatically labeled ones. The problem of data denoising was addressed in different existing research, most of which focuses on the detection of outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates the AGRA effectiveness, while comprehensive results analysis sup
    
[^124]: 自编码器的最大似然训练

    Maximum Likelihood Training of Autoencoders. (arXiv:2306.01843v1 [cs.LG])

    [http://arxiv.org/abs/2306.01843](http://arxiv.org/abs/2306.01843)

    本文介绍了一种成功的最大似然训练方法，用于非约束自编码器，将生成建模的优异性质与高效自编码器相结合。作者克服了两个挑战：设计了消除迭代的估计器并提出了稳定的最大似然训练目标。实验证明这种方法可以成功训练一系列非约束性自编码器，并取得了有竞争力的性能。

    

    最大似然训练在生成建模中具有优异的统计性质，尤其是在归一化流模型中非常流行。另一方面，由于流形假设，生成自编码器有望比归一化流更高效。本文首次引入了非约束自编码器的成功最大似然训练，将这两种范式融合在一起。为此，我们识别并克服了两个挑战：首先，现有的自由格式网络的最大似然估计器过于缓慢，依赖于迭代方案，其成本随潜在维度呈线性增长。我们引入了一个改进的估计器，消除了迭代，从而使成本保持不变（每个批次的运行时间大约是普通自编码器的两倍）。其次，我们证明朴素地将最大似然应用于自编码器可能导致发散解决方案，并利用这个想法来推动稳定的最大似然训练目标。我们进行了实验，表明所提出的训练方法可以成功训练一系列非约束性自编码器，并在生成图像、插值和变换等任务中取得了有竞争力的性能。

    Maximum likelihood training has favorable statistical properties and is popular for generative modeling, especially with normalizing flows. On the other hand, generative autoencoders promise to be more efficient than normalizing flows due to the manifold hypothesis. In this work, we introduce successful maximum likelihood training of unconstrained autoencoders for the first time, bringing the two paradigms together. To do so, we identify and overcome two challenges: Firstly, existing maximum likelihood estimators for free-form networks are unacceptably slow, relying on iteration schemes whose cost scales linearly with latent dimension. We introduce an improved estimator which eliminates iteration, resulting in constant cost (roughly double the runtime per batch of a vanilla autoencoder). Secondly, we demonstrate that naively applying maximum likelihood to autoencoders can lead to divergent solutions and use this insight to motivate a stable maximum likelihood training objective. We per
    
[^125]: 人类对齐校准用于AI辅助决策制定

    Human-Aligned Calibration for AI-Assisted Decision Making. (arXiv:2306.00074v1 [cs.LG])

    [http://arxiv.org/abs/2306.00074](http://arxiv.org/abs/2306.00074)

    本文通过引入一种基于主动询问决策者个人偏好的置信度构造方法，解决了现有置信度对于决策者信任决策的不准确问题，从而提高决策的准确性和效率。

    

    当使用二元分类器提供决策支持时，它通常提供标签预测和置信度值。然后，决策者应使用置信度值来校准对预测的信任程度。在这种情况下，人们经常认为置信度值应对预测标签与实际标签匹配的概率进行良好校准的估计。然而，多条实证证据表明，决策者难以使用这些置信度值很好地确定何时信任预测。本文的目标首先是理解为什么，然后研究如何构建更有用的置信度值。我们首先认为，在广泛类的效用函数中，存在数据分布，对于这些分布，理性决策者通常难以使用以上置信度值发现最佳决策政策——最佳的决策者需要人类对齐。然后，我们引入了一种基于主动询问决策者他们在所面临的二元分类任务的决策上的个人偏好的新方法来构造置信度值。我们表明，该方法产生的置信度值比使用标准置信度度量导致更好的决策。

    Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. In this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. We first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values -- an optimal decision maker wou
    
[^126]: 基于流数据的神经网络在线学习的低秩扩展卡尔曼滤波算法

    Low-rank extended Kalman filtering for online learning of neural networks from streaming data. (arXiv:2305.19535v1 [stat.ML])

    [http://arxiv.org/abs/2305.19535](http://arxiv.org/abs/2305.19535)

    本文提出一种基于低秩扩展卡尔曼滤波的高效在线学习算法，其能够估计非线性函数的参数，具有更快的适应性和更快的奖励积累。

    

    本文提出了一种高效的在线近似贝叶斯推理算法，用于从可能非平稳的数据流中估计非线性函数的参数。该方法基于扩展卡尔曼滤波器（EKF），但使用了一种新颖的低秩加对角线的后验精度矩阵分解，其每步的成本与模型参数数量成线性关系。与基于随机变分推理的方法不同，我们的方法是完全确定的，并且不需要步长调整。我们通过实验证明，这导致更快（更高效）的学习，从而在用作上下文赌博算法的一部分时实现更快速的适应性和更快的奖励积累。

    We propose an efficient online approximate Bayesian inference algorithm for estimating the parameters of a nonlinear function from a potentially non-stationary data stream. The method is based on the extended Kalman filter (EKF), but uses a novel low-rank plus diagonal decomposition of the posterior precision matrix, which gives a cost per step which is linear in the number of model parameters. In contrast to methods based on stochastic variational inference, our method is fully deterministic, and does not require step-size tuning. We show experimentally that this results in much faster (more sample efficient) learning, which results in more rapid adaptation to changing distributions, and faster accumulation of reward when used as part of a contextual bandit algorithm.
    
[^127]: 几何图滤波器和神经网络：极限性质和判别度的权衡

    Geometric Graph Filters and Neural Networks: Limit Properties and Discriminability Trade-offs. (arXiv:2305.18467v1 [cs.LG])

    [http://arxiv.org/abs/2305.18467](http://arxiv.org/abs/2305.18467)

    本文研究了从流形采样点构造的图与流形神经网络的关系，并证明了这些图形上的卷积滤波器和神经网络收敛于连续流形上的卷积滤波器和神经网络。研究发现，图滤波器的可分性和近似流形滤波器所需行为的能力之间存在重要权衡，在神经网络中由于非线性的频率混合属性而得到改善。

    

    本文研究了图神经网络（GNN）和流形神经网络（MNN）之间的关系，当图是从流形采样点构造而成时，从而编码几何信息。我们考虑了卷积MNN和GNN，其中流形和图卷积分别以拉普拉斯-贝尔特拉米算子和图Laplacian为定义。使用适当的核，我们分析了密集和中等稀疏图。我们证明了非渐近误差界，表明这些图形上的卷积滤波器和神经网络收敛于连续流形上的卷积滤波器和神经网络。通过这个分析的副产品，我们观察到了图滤波器的可分性和近似流形滤波器所需行为的能力之间的重要权衡。然后，我们讨论了这种权衡如何在神经网络中由于非线性的频率混合属性而改善。

    This paper studies the relationship between a graph neural network (GNN) and a manifold neural network (MNN) when the graph is constructed from a set of points sampled from the manifold, thus encoding geometric information. We consider convolutional MNNs and GNNs where the manifold and the graph convolutions are respectively defined in terms of the Laplace-Beltrami operator and the graph Laplacian. Using the appropriate kernels, we analyze both dense and moderately sparse graphs. We prove non-asymptotic error bounds showing that convolutional filters and neural networks on these graphs converge to convolutional filters and neural networks on the continuous manifold. As a byproduct of this analysis, we observe an important trade-off between the discriminability of graph filters and their ability to approximate the desired behavior of manifold filters. We then discuss how this trade-off is ameliorated in neural networks due to the frequency mixing property of nonlinearities. We further d
    
[^128]: 一瞥：重新思考视频不断学习中的时间信息

    Just a Glimpse: Rethinking Temporal Information for Video Continual Learning. (arXiv:2305.18418v1 [cs.CV])

    [http://arxiv.org/abs/2305.18418](http://arxiv.org/abs/2305.18418)

    本文提出了一种基于单个帧的新型重播机制SMILE，用于有效的视频连续学习。实验表明，在内存受到极端限制时，视频的多样性比时间信息更重要。

    

    增量学习是连续学习研究中最重要的设置之一，因为它与现实世界的应用场景密切相关。随着类别/任务数量的增加，由于受到内存大小的限制，灾难性遗忘会出现。在视频领域研究持续学习面临更大的挑战，因为视频数据包含大量帧，这会使回放记忆负担更重。目前的常见做法是从视频流中对帧进行子采样，并将其存储在回放记忆中。在本文中，我们提出了一种基于单个帧的新型重播机制SMILE，用于有效的视频连续学习。通过大量实验，我们表明在极端内存限制下，视频的多样性比时间信息更重要。因此，我们的方法侧重于从代表大量独特视频的少量帧中学习。我们在三个代表性视频数据集Kin上进行了实验。

    Class-incremental learning is one of the most important settings for the study of Continual Learning, as it closely resembles real-world application scenarios. With constrained memory sizes, catastrophic forgetting arises as the number of classes/tasks increases. Studying continual learning in the video domain poses even more challenges, as video data contains a large number of frames, which places a higher burden on the replay memory. The current common practice is to sub-sample frames from the video stream and store them in the replay memory. In this paper, we propose SMILE a novel replay mechanism for effective video continual learning based on individual/single frames. Through extensive experimentation, we show that under extreme memory constraints, video diversity plays a more significant role than temporal information. Therefore, our method focuses on learning from a small number of frames that represent a large number of unique videos. On three representative video datasets, Kin
    
[^129]: 从理论角度揭示“思维链”背后的奥秘

    Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective. (arXiv:2305.15408v1 [cs.LG])

    [http://arxiv.org/abs/2305.15408](http://arxiv.org/abs/2305.15408)

    本文从理论层面探究了带有“思维链”提示的大型语言模型在解决基本数学和决策问题中的能力，发现自回归Transformer大小恒定即可解决任务，揭示了“思维链”提示的背后机制。

    

    最近的研究发现，"思维链"提示能够显著提高大型语言模型（LLMs）的性能，特别是在涉及数学或推理的复杂任务中。尽管获得了巨大的实证成功，但“思维链”背后的机制以及它如何释放LLMs的潜力仍然是神秘的。本文首次从理论上回答了这些问题。具体而言，我们研究了LLMs带有“思维链”在解决基本数学和决策问题中的能力。我们首先给出一个不可能的结果，表明任何有限深度的Transformer都不能直接输出正确的基本算术/方程任务的答案，除非模型大小随着输入长度的增加呈超多项式增长。相反，我们通过构造证明，大小恒定的自回归Transformer足以通过使用常用的数学语言形式生成“思维链”推导来解决这两个任务。

    Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the capacity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that any bounded-depth Transformer cannot directly output correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of a constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language forma
    
[^130]: 通过重新标记最小训练子集来翻转预测

    Relabel Minimal Training Subset to Flip a Prediction. (arXiv:2305.12809v1 [cs.LG])

    [http://arxiv.org/abs/2305.12809](http://arxiv.org/abs/2305.12809)

    本文利用扩展影响函数提出了一种有效的识别和重新标记最小训练子集的方法，并证明其始终能够成功翻转测试结果，同时还提供了挑战模型预测、评估模型鲁棒性和洞察训练集偏差等多重作用。

    

    Yang等人发现，仅删除1%的训练数据就可能导致预测结果翻转。鉴于机器学习模型中存在噪声数据的普遍性，本文提出了一个问题：在模型训练之前通过重新标记一个小的训练数据子集可否导致测试结果翻转？本文利用扩展影响函数提出了一种有效的识别和重新标记这种子集的方法，并证明了其始终能够产生成功的结果。这种机制有多重作用：（1）提供了一种补充方法，可以通过恢复可能错误标记的训练数据来挑战模型预测；（2）评估模型的鲁棒性，因为本文发现子集的大小与训练集中噪声数据的比例之间存在显著关系；（3）提供了洞察训练集偏差的见解。据我们所知，这项工作代表了对识别最小训练子集问题的第一次研究。

    Yang et al. (2023) discovered that removing a mere 1% of training points can often lead to the flipping of a prediction. Given the prevalence of noisy data in machine learning models, we pose the question: can we also result in the flipping of a test prediction by relabeling a small subset of the training data before the model is trained? In this paper, utilizing the extended influence function, we propose an efficient procedure for identifying and relabeling such a subset, demonstrating consistent success. This mechanism serves multiple purposes: (1) providing a complementary approach to challenge model predictions by recovering potentially mislabeled training points; (2) evaluating model resilience, as our research uncovers a significant relationship between the subset's size and the ratio of noisy data in the training set; and (3) offering insights into bias within the training set. To the best of our knowledge, this work represents the first investigation into the problem of identi
    
[^131]: 多任务分层对抗逆强化学习

    Multi-task Hierarchical Adversarial Inverse Reinforcement Learning. (arXiv:2305.12633v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12633](http://arxiv.org/abs/2305.12633)

    多任务分层对抗逆强化学习（MH-AIRL）通过分层结构和基本技能的可重复使用来提高多任务模仿学习的效率，适用于没有任务或技能注释的演示。

    

    多任务模仿学习旨在通过多任务专家演示训练出能够执行一系列任务的策略，这对于通用目的的机器人至关重要。现有的多任务模仿学习算法在数据效率和复杂长时任务的性能上存在问题。我们开发了多任务分层对抗逆强化学习（MH-AIRL），用于学习分层结构的多任务策略，这对于具有长时间范围的组合任务更加有益，并且通过识别和传递可重复使用的基本技能来提高专家数据的效率。为实现这一目标，MH-AIRL有效地合成了基于上下文的多任务学习、AIRL（一种模仿学习方法）和分层策略学习。此外，MH-AIRL还可以适应没有任务或技能注释的演示（即仅包含状态-动作对），这在实践中更易于获取。对于MH-AIRL的每个模块都提供了理论上的证明。

    Multi-task Imitation Learning (MIL) aims to train a policy capable of performing a distribution of tasks based on multi-task expert demonstrations, which is essential for general-purpose robots. Existing MIL algorithms suffer from low data efficiency and poor performance on complex long-horizontal tasks. We develop Multi-task Hierarchical Adversarial Inverse Reinforcement Learning (MH-AIRL) to learn hierarchically-structured multi-task policies, which is more beneficial for compositional tasks with long horizons and has higher expert data efficiency through identifying and transferring reusable basic skills across tasks. To realize this, MH-AIRL effectively synthesizes context-based multi-task learning, AIRL (an IL approach), and hierarchical policy learning. Further, MH-AIRL can be adopted to demonstrations without the task or skill annotations (i.e., state-action pairs only) which are more accessible in practice. Theoretical justifications are provided for each module of MH-AIRL, and
    
[^132]: 基于几何感知的自回归模型用于新型电磁量计几何模拟的泛化研究

    Generalizing to new calorimeter geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation. (arXiv:2305.11531v1 [physics.ins-det])

    [http://arxiv.org/abs/2305.11531](http://arxiv.org/abs/2305.11531)

    基于几何感知的自回归模型能够学习电磁量计响应如何随几何形状变化，能够快速有效地模拟非环形的电磁量计。

    

    在粒子物理数据分析中，生成对撞产物的模拟探测器响应至关重要，但计算非常昂贵。其中一个子探测器，电磁量计由于其单元格的高粒度和复杂的相互作用而占据了计算时间的主导地位。生成模型可以提供更快的样本生成，但目前需要大量努力来优化特定探测器几何形状的性能，通常需要许多网络来描述不同的单元格大小和排列方式，这些模型不能推广到其他几何形状。我们开发了一种“几何感知”自回归模型，学习电磁量计响应如何随几何形状变化，能够生成看不见的几何形状的模拟响应而无需其他训练。该几何感知模型在涉及关键响应的生成和真实分布之间的Wasserstein距离等指标上比基线模型优越50％。我们通过在二维空间中模拟具有前所未有的细粒度，并扩展到非平面几何形状，展示了该方法的可行性和速度。

    Generation of simulated detector response to collision products is crucial to data analysis in particle physics, but computationally very expensive. One subdetector, the calorimeter, dominates the computational time due to the high granularity of its cells and complexity of the interaction. Generative models can provide more rapid sample production, but currently require significant effort to optimize performance for specific detector geometries, often requiring many networks to describe the varying cell sizes and arrangements, which do not generalize to other geometries. We develop a {\it geometry-aware} autoregressive model, which learns how the calorimeter response varies with geometry, and is capable of generating simulated responses to unseen geometries without additional training. The geometry-aware model outperforms a baseline, unaware model by 50\% in metrics such as the Wasserstein distance between generated and true distributions of key quantities which summarize the simulate
    
[^133]: 基于双重注意机制的眼底血管图像分割

    Segmentation of fundus vascular images based on a dual-attention mechanism. (arXiv:2305.03617v1 [eess.IV])

    [http://arxiv.org/abs/2305.03617](http://arxiv.org/abs/2305.03617)

    本文提出了基于双重注意机制的眼底血管图像分割方法，可以从空间和通道维度提取图像信息，并通过引入空间注意机制和Dropout层解决光照变化和不均匀对比度等问题，实验结果表明，该方法可以产生令人满意的结果。

    

    精确地分割视网膜眼底图像中的血管对于早期筛查、诊断和评估某些眼部疾病至关重要。然而，这些图像中存在明显的光照变化和不均匀对比度，这使得分割变得非常具挑战性。因此，本文采用了一种注意融合机制，该机制结合了Transformer构建的通道注意和空间注意机制，从空间和通道维度提取视网膜眼底图像的信息。为了消除编码器图像中的噪声，引入了一个空间注意机制在跳跃连接中。此外，使用Dropout层随机舍弃一些神经元，以防止神经网络过度拟合并提高其泛化性能。在公共数据集DERIVE、STARE和CHASEDB1上进行了实验。结果显示，与一些最近的视网膜眼底图像分割算法相比，我们的方法产生了令人满意的结果。

    Accurately segmenting blood vessels in retinal fundus images is crucial in the early screening, diagnosing, and evaluating some ocular diseases. However, significant light variations and non-uniform contrast in these images make segmentation quite challenging. Thus, this paper employ an attention fusion mechanism that combines the channel attention and spatial attention mechanisms constructed by Transformer to extract information from retinal fundus images in both spatial and channel dimensions. To eliminate noise from the encoder image, a spatial attention mechanism is introduced in the skip connection. Moreover, a Dropout layer is employed to randomly discard some neurons, which can prevent overfitting of the neural network and improve its generalization performance. Experiments were conducted on publicly available datasets DERIVE, STARE, and CHASEDB1. The results demonstrate that our method produces satisfactory results compared to some recent retinal fundus image segmentation algor
    
[^134]: 深度宽松弛神经网络的统计优化性

    Statistical Optimality of Deep Wide Neural Networks. (arXiv:2305.02657v1 [stat.ML])

    [http://arxiv.org/abs/2305.02657](http://arxiv.org/abs/2305.02657)

    本文研究了深度宽松弛ReLU神经网络的泛化能力，证明适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中，但过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。

    

    本文研究了定义在有界域$\mathcal X \subset \mathbb R^{d}$上的深度宽松弛ReLU神经网络的泛化能力。首先证明了神经网络的泛化能力可以被相应的深度神经切向核回归所完全描绘。然后，我们研究了深度神经切向核的谱特性，并证明了深度神经切向核在$\mathcal{X}$上为正定，其特征值衰减率为$(d+1)/d$。由于核回归中已经建立的理论，我们得出结论，适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中。最后，我们证明过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。

    In this paper, we consider the generalization ability of deep wide feedforward ReLU neural networks defined on a bounded domain $\mathcal X \subset \mathbb R^{d}$. We first demonstrate that the generalization ability of the neural network can be fully characterized by that of the corresponding deep neural tangent kernel (NTK) regression. We then investigate on the spectral properties of the deep NTK and show that the deep NTK is positive definite on $\mathcal{X}$ and its eigenvalue decay rate is $(d+1)/d$. Thanks to the well established theories in kernel regression, we then conclude that multilayer wide neural networks trained by gradient descent with proper early stopping achieve the minimax rate, provided that the regression function lies in the reproducing kernel Hilbert space (RKHS) associated with the corresponding NTK. Finally, we illustrate that the overfitted multilayer wide neural networks can not generalize well on $\mathbb S^{d}$.
    
[^135]: 一种基于深度学习辅助微波等离子体相互作用技术的等离子体密度估计方法

    Deep Learning assisted microwave-plasma interaction based technique for plasma density estimation. (arXiv:2304.14807v1 [physics.plasm-ph])

    [http://arxiv.org/abs/2304.14807](http://arxiv.org/abs/2304.14807)

    本文提出了一种基于深度学习辅助微波等离子体相互作用技术的等离子体密度估计方法，通过测量微波散射引起的电场模式来估计密度剖面。

    

    电子密度是表征任何等离子体的关键参数。低温等离子体领域的大部分应用和研究都基于等离子体密度和等离子体温度。传统的电子密度测量方法针对给定线性低温等离子体设备提供轴向和径向剖面。这些方法存在操作范围较小、仪器沉重以及数据分析过程复杂等主要缺点。为了应对这些实际问题，本文提出了一种新颖的机器学习（ML）辅助微波等离子体相互作用的策略，该策略能够确定等离子体内电子密度剖面。通过测量微波散射引起的电场模式来估计密度剖面。该策略针对一个模拟的训练数据集进行了概念验证，其中包括低温、非磁化和碰撞性等离子体的不同类型的高斯形状密度剖面范围内。

    The electron density is a key parameter to characterize any plasma. Most of the plasma applications and research in the area of low-temperature plasmas (LTPs) is based on plasma density and plasma temperature. The conventional methods for electron density measurements offer axial and radial profiles for any given linear LTP device. These methods have major disadvantages of operational range (not very wide), cumbersome instrumentation, and complicated data analysis procedures. To address such practical concerns, the article proposes a novel machine learning (ML) assisted microwave-plasma interaction based strategy which is capable enough to determine the electron density profile within the plasma. The electric field pattern due to microwave scattering is measured to estimate the density profile. The proof of concept is tested for a simulated training data set comprising a low-temperature, unmagnetized, collisional plasma. Different types of Gaussian-shaped density profiles, in the range
    
[^136]: 自监督学习食谱

    A Cookbook of Self-Supervised Learning. (arXiv:2304.12210v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.12210](http://arxiv.org/abs/2304.12210)

    这篇论文提出了一本自监督学习的食谱，旨在降低自监督学习研究的门槛，并使研究人员能够了解各种选择和参数在自监督学习中的作用。

    

    自监督学习被称为智能的暗物质，是推进机器学习的一条有前途的道路。然而，像烹饪一样，训练自监督学习方法是一门需要高门槛的精细艺术。虽然许多组件是熟悉的，但成功训练自监督学习方法涉及到从前趋任务到训练超参数的令人眼花缭乱的选择。我们的目标是通过以食谱的形式奠定基础和介绍最新的自监督学习方法，降低进入自监督学习研究的门槛。我们希望赋予好奇的研究人员在方法领域中航行，理解各种调节钮的作用，并获得探索自监督学习美味之处所需的技能。

    Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.
    
[^137]: 长期的多模式变压器整合EHR中成像和潜在临床特征，用于肺部结节分类。

    Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification. (arXiv:2304.02836v1 [eess.IV])

    [http://arxiv.org/abs/2304.02836](http://arxiv.org/abs/2304.02836)

    本文提出了一种新的肺部结节分类方法，使用变压器模型整合了EHR中的成像和临床特征。

    

    将重复成像和医疗背景（如电子健康记录）纳入预测性孤立性肺部结节（SPN）诊断模型可以极大增加准确性。然而，像成像和诊断代码这样的临床常规模式可能是异步的，并且在不同时间尺度上进行不规则采样，这是长期多模态学习的障碍。我们提出了一种基于变压器的多模态策略，将重复成像与日常收集的EHR中的长期临床特征相整合，以进行SPN分类。我们对潜在临床特征进行无监督的解缠缚，并利用时间距离缩放自注意力来联合学习临床特征表达和胸部计算机断层扫描（CT）。我们的分类器是在一个公共数据集的2,668个扫描和1,149名志愿者的长期胸部CT、账单代码、药物和实验室检查记录中进行预训练的。

    The accuracy of predictive models for solitary pulmonary nodule (SPN) diagnosis can be greatly increased by incorporating repeat imaging and medical context, such as electronic health records (EHRs). However, clinically routine modalities such as imaging and diagnostic codes can be asynchronous and irregularly sampled over different time scales which are obstacles to longitudinal multimodal learning. In this work, we propose a transformer-based multimodal strategy to integrate repeat imaging with longitudinal clinical signatures from routinely collected EHRs for SPN classification. We perform unsupervised disentanglement of latent clinical signatures and leverage time-distance scaled self-attention to jointly learn from clinical signatures expressions and chest computed tomography (CT) scans. Our classifier is pretrained on 2,668 scans from a public dataset and 1,149 subjects with longitudinal chest CTs, billing codes, medications, and laboratory tests from EHRs of our home institution
    
[^138]: “Polytuplet Loss: 训练阅读理解和逻辑推理模型的反向方法”

    Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2304.01046](http://arxiv.org/abs/2304.01046)

    本文研究了一种训练阅读理解和逻辑推理模型的反向方法，利用相对准确性的策略来训练模型，通过Polytuplet Loss函数来确保优先学习答案选择的相对正确性，获得了不错的成果，提出了具有一般性的训练方法和模型架构。

    

    在整个学校教育过程中，学生们将受到阅读理解和逻辑推理的考验。学生们已经开发了各种策略来完成此类考试，其中有些被认为是通常表现优于其他策略的。这样一种策略涉及强调相对准确性而非绝对准确性，理论上可以在不完全掌握解题所需信息的情况下得出正确答案。本文研究了应用这种策略来训练迁移学习模型以解决阅读理解和逻辑推理问题的有效性。这些模型在具有挑战性的阅读理解和逻辑推理基准数据集ReClor上进行了评估。尽管以前的研究集中于逻辑推理技能，但我们专注于一种通用的训练方法和模型架构。我们提出了Polytuplet Loss函数，是三元组损失函数的扩展，以确保优先学习答案选择的相对正确性而非学习绝对正确性。

    Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
    
[^139]: Pgx:强化学习硬件加速的并行游戏模拟器

    Pgx: Hardware-accelerated parallel game simulation for reinforcement learning. (arXiv:2303.17503v1 [cs.AI])

    [http://arxiv.org/abs/2303.17503](http://arxiv.org/abs/2303.17503)

    Pgx是一个用JAX编写的游戏模拟器集合，具有强化学习硬件加速能力，支持并行执行，速度比现有的强化学习库快10倍。 它实现了Backgammon，Shogi和Go等基准测试游戏。

    

    我们提出了Pgx，这是一个用JAX编写的棋盘游戏模拟器集合。由于JAX的自动向量化和即时编译功能，Pgx易于在GPU/TPU加速器上进行大规模并行执行。我们发现，在单个A100 GPU上的Pgx模拟比现有的强化学习库快10倍。Pgx实现了被认为是人工智能研究中至关重要的基准测试的游戏，如Backgammon，Shogi和Go。 Pgx可在https://github.com/sotetsuk/pgx获得。

    We propose Pgx, a collection of board game simulators written in JAX. Thanks to auto-vectorization and Just-In-Time compilation of JAX, Pgx scales easily to thousands of parallel execution on GPU/TPU accelerators. We found that the simulation of Pgx on a single A100 GPU is 10x faster than that of existing reinforcement learning libraries. Pgx implements games considered vital benchmarks in artificial intelligence research, such as Backgammon, Shogi, and Go. Pgx is available at https://github.com/sotetsuk/pgx.
    
[^140]: PeakNet：一种利用深度神经网络的自动Bragg峰点寻找器

    PeakNet: An Autonomous Bragg Peak Finder with Deep Neural Networks. (arXiv:2303.15301v2 [physics.ins-det] UPDATED)

    [http://arxiv.org/abs/2303.15301](http://arxiv.org/abs/2303.15301)

    PeakNet是一个利用深度神经网络的自动Bragg峰点寻找器，它通过实时调整来适应逐发强背景散射的波动，消除了手动调整算法参数的需求，减少了误报峰点的数量。

    

    近年来，在X射线自由电子激光器（XFEL）和同步辐射设施中的串行晶体学取得了巨大的进步，使得可以对大分子结构和分子过程进行新颖的科学研究。然而，这些实验产生了大量的数据，给数据减少和实时反馈带来了计算挑战。Bragg峰点寻找算法用于识别有用的图像，并提供关于命中率和分辨率的实时反馈。来自缓冲溶液、注射喷嘴和其他屏蔽材料的逐发强度波动和强背景散射使得这成为一个耗时的优化问题。在这里，我们提出了PeakNet，一种利用深度神经网络的自动Bragg峰点寻找器。

    Serial crystallography at X-ray free electron laser (XFEL) and synchrotron facilities has experienced tremendous progress in recent times enabling novel scientific investigations into macromolecular structures and molecular processes. However, these experiments generate a significant amount of data posing computational challenges in data reduction and real-time feedback. Bragg peak finding algorithm is used to identify useful images and also provide real-time feedback about hit-rate and resolution. Shot-to-shot intensity fluctuations and strong background scattering from buffer solution, injection nozzle and other shielding materials make this a time-consuming optimization problem. Here, we present PeakNet, an autonomous Bragg peak finder that utilizes deep neural networks. The development of this system 1) eliminates the need for manual algorithm parameter tuning, 2) reduces false-positive peaks by adjusting to shot-to-shot variations in strong background scattering in real-time, 3) e
    
[^141]: 通过反向特征投影在不断学习中维护线性可分性

    Preserving Linear Separability in Continual Learning by Backward Feature Projection. (arXiv:2303.14595v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.14595](http://arxiv.org/abs/2303.14595)

    提出了一种名为BFP的连续学习方法，它通过将新特征变换为旧特征的线性变换来维护线性可分性，从而允许新特征方向的出现以适应新任务，同时保留旧任务的信息。

    

    在不断学习中，灾难性遗忘一直是一个重大挑战，因为模型需要在有限或没有以前查看任务的数据情况下学习新任务。为了解决这个挑战，基于特征空间知识蒸馏的方法已被提出并证明可以减少遗忘。然而，大多数特征蒸馏方法直接约束新特征以匹配旧特征，忽视了可塑性的需求。为了实现更好的稳定性-可塑性平衡，我们提出了Backward Feature Projection（BFP），这是一种连续学习方法，允许新特征在旧特征的可学习线性变换中发生变化。BFP保留旧类别的线性可分性，同时允许新的特征方向出现以适应新的类别。BFP可以与现有的经验重播方法集成，并显著提高性能。我们还证明，BFP有助于学习更好的表示空间。

    Catastrophic forgetting has been a major challenge in continual learning, where the model needs to learn new tasks with limited or no access to data from previously seen tasks. To tackle this challenge, methods based on knowledge distillation in feature space have been proposed and shown to reduce forgetting. However, most feature distillation methods directly constrain the new features to match the old ones, overlooking the need for plasticity. To achieve a better stability-plasticity trade-off, we propose Backward Feature Projection (BFP), a method for continual learning that allows the new features to change up to a learnable linear transformation of the old features. BFP preserves the linear separability of the old classes while allowing the emergence of new feature directions to accommodate new classes. BFP can be integrated with existing experience replay methods and boost performance by a significant margin. We also demonstrate that BFP helps learn a better representation space,
    
[^142]: Xplainer：从X射线观察到可解释的零样本诊断

    Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis. (arXiv:2303.13391v1 [cs.CV])

    [http://arxiv.org/abs/2303.13391](http://arxiv.org/abs/2303.13391)

    Xplainer是一个透明且可解释的零样本诊断新框架，通过对存在的描述性观察进行分类来提高自动诊断集成到临床工作流程中的效率，同时避免需要大量注释数据的问题。

    

    通过医学图像进行自动诊断预测，是支持临床决策的宝贵资源。然而，这样的系统通常需要在大量注释数据上进行训练，而医学领域的注释数据往往很少。零样本方法通过允许在不依赖标记数据的情况下灵活适应具有不同临床结果的新设置来解决这一挑战。此外，为了将自动诊断集成到临床工作流程中，方法应该是透明且可解释的，增加医疗专业人员的信任并促进正确性验证。在这项工作中，我们引入了Xplainer，这是一个在临床设置中进行可解释的零样本诊断的新框架。Xplainer将对比视觉语言模型的分类即描述方法适应于多标签医学诊断任务。具体而言，我们提示模型对存在的描述性观察进行分类，而不是直接预测诊断。

    Automated diagnosis prediction from medical images is a valuable resource to support clinical decision-making. However, such systems usually need to be trained on large amounts of annotated data, which often is scarce in the medical domain. Zero-shot methods address this challenge by allowing a flexible adaption to new settings with different clinical findings without relying on labeled data. Further, to integrate automated diagnosis in the clinical workflow, methods should be transparent and explainable, increasing medical professionals' trust and facilitating correctness verification. In this work, we introduce Xplainer, a novel framework for explainable zero-shot diagnosis in the clinical setting. Xplainer adapts the classification-by-description approach of contrastive vision-language models to the multi-label medical diagnosis task. Specifically, instead of directly predicting a diagnosis, we prompt the model to classify the existence of descriptive observations, which a radiologi
    
[^143]: 连接生成半监督学习和生成开放集识别

    Linking generative semi-supervised learning and generative open-set recognition. (arXiv:2303.11702v1 [cs.CV])

    [http://arxiv.org/abs/2303.11702](http://arxiv.org/abs/2303.11702)

    本研究旨在探究生成半监督学习和生成开放集识别之间的关系。SSL-GANs和OSR-GANs方法的相似性在于都要求生成器在互补空间中产生样本，并通过正则化来推广开放空间。研究结果表明SSL优化边缘-GAN在结合SSL-OSR任务方面树立新的标准，但在某些OSR任务中OSR优化的ARP-GAN仍然略优于SSL-GAN。

    

    本研究在生成对抗网络（GANs）的背景下，探究了半监督学习（SSL）和开放集识别（OSR）之间的关系。尽管以前没有正式将SSL和OSR联系起来的研究，但它们各自的方法有惊人的相似之处。具体而言，SSL-GAN和OSR-GAN要求生成器在互补空间中产生样本。随后，通过对生成样本进行正则化，SSL和OSR分类器都可以完全识别开放空间。为了证明SSL和OSR之间的关联，我们在理论上和实验上比较了最先进的SSL-GAN方法和最先进的OSR-GAN方法。结果表明，文献基础更加牢固的SSL优化边缘-GAN在结合SSL-OSR任务方面树立新的标准，并在某些一般的OSR实验中取得了新的最先进的结果。然而，OSR优化的对抗性互惠点（ARP）-GAN在一些OSR任务中仍然略优于SSL-GAN。

    This study investigates the relationship between semi-supervised learning (SSL) and open-set recognition (OSR) in the context of generative adversarial networks (GANs). Although no previous study has formally linked SSL and OSR, their respective methods share striking similarities. Specifically, SSL-GANs and OSR-GANs require generator to produce samples in the complementary space. Subsequently, by regularising networks with generated samples, both SSL and OSR classifiers generalize the open space. To demonstrate the connection between SSL and OSR, we theoretically and experimentally compare state-of-the-art SSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate that the SSL optimised margin-GANs, which have a stronger foundation in literature, set the new standard for the combined SSL-OSR task and achieves new state-of-other art results in certain general OSR experiments. However, the OSR optimised adversarial reciprocal point (ARP)-GANs still slightly out-performe
    
[^144]: PyVBMC：Python中高效的贝叶斯推断

    PyVBMC: Efficient Bayesian inference in Python. (arXiv:2303.09519v1 [stat.ML])

    [http://arxiv.org/abs/2303.09519](http://arxiv.org/abs/2303.09519)

    PyVBMC是一种高效的Python工具，用于黑盒计算模型的贝叶斯推断和模型选择，可以处理连续参数不超过约10-15个的计算或统计模型。

    

    PyVBMC是Variational Bayesian Monte Carlo（VBMC）算法的Python实现，用于黑盒计算模型的后验和模型推断。VBMC是一种用于高效参数估计和模型评估的近似推断方法，当模型评估是有点到非常昂贵（例如第二次或更多次）和/或嘈杂时。具体而言，VBMC计算：

    PyVBMC is a Python implementation of the Variational Bayesian Monte Carlo (VBMC) algorithm for posterior and model inference for black-box computational models (Acerbi, 2018, 2020). VBMC is an approximate inference method designed for efficient parameter estimation and model assessment when model evaluations are mildly-to-very expensive (e.g., a second or more) and/or noisy. Specifically, VBMC computes:  - a flexible (non-Gaussian) approximate posterior distribution of the model parameters, from which statistics and posterior samples can be easily extracted;  - an approximation of the model evidence or marginal likelihood, a metric used for Bayesian model selection.  PyVBMC can be applied to any computational or statistical model with up to roughly 10-15 continuous parameters, with the only requirement that the user can provide a Python function that computes the target log likelihood of the model, or an approximation thereof (e.g., an estimate of the likelihood obtained via simulation
    
[^145]: 一种用于降阶建模中残差学习的多信度DeepONet方法

    A DeepONet multi-fidelity approach for residual learning in reduced order modeling. (arXiv:2302.12682v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2302.12682](http://arxiv.org/abs/2302.12682)

    本文提出了一种利用DeepONets的多信度方法来提高降阶模型的精度。通过将模型降阶与机器学习的残差学习相结合，可以学习并推断新预测的误差。该框架最大化利用高信度信息，用于构建降阶模型和学习残差。实验结果证明了该方法的有效性。

    

    本文提出了一种利用多信度视角和DeepONets来提高降阶模型精度的新方法。降阶模型通过简化原始模型提供实时数值近似。通常忽略这种操作引入的误差以实现快速计算。我们提出将模型降阶与机器学习的残差学习相耦合，以便神经网络可以学习并推断新预测的误差。我们强调该框架最大化利用高信度信息，用于构建降阶模型和学习残差。我们在本文中探索了适用于传感器数据的主元正交分解（POD）和缺失POD与最新的DeepONet架构的集成。对一个参数基准函数和一个非线性参数Navier-Stokes问题进行了数值研究。

    In the present work, we introduce a novel approach to enhance the precision of reduced order models by exploiting a multi-fidelity perspective and DeepONets. Reduced models provide a real-time numerical approximation by simplifying the original model. The error introduced by the such operation is usually neglected and sacrificed in order to reach a fast computation. We propose to couple the model reduction to a machine learning residual learning, such that the above-mentioned error can be learned by a neural network and inferred for new predictions. We emphasize that the framework maximizes the exploitation of high-fidelity information, using it for building the reduced order model and for learning the residual. In this work, we explore the integration of proper orthogonal decomposition (POD), and gappy POD for sensors data, with the recent DeepONet architecture. Numerical investigations for a parametric benchmark function and a nonlinear parametric Navier-Stokes problem are presented.
    
[^146]: 使用非特定运动数据的可扩展XR用户基于运动的识别

    Extensible Motion-based Identification of XR Users using Non-Specific Motion Data. (arXiv:2302.07517v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2302.07517](http://arxiv.org/abs/2302.07517)

    提出了一种可扩展XR用户基于运动的识别方法。与现有基线方法相比，该方法通过仅使用少量的注册数据来识别新用户，可以在几秒钟内注册新用户，而且在仅有少量注册数据可用时也更可靠。

    

    本文提出了一种基于嵌入式和深度度量学习结合的方法，将距离和分类两种方法的优势相结合，用于通过用户的运动来识别扩展现实用户。我们在“半衰期：Alyx”VR游戏的用户数据集上进行了模型训练，并使用现有的基线分类模型作为对比。研究结果表明，基于嵌入式的方法可以通过只使用几分钟的注册数据，识别新用户的非特定运动，可以在几秒钟内注册新用户，而重新训练基线方法需要花费将近一天的时间，当只有很少的注册数据可用时，比基线方法更可靠，可以用于识别使用不同VR设备记录的新用户数据集。综上所述，我们的解决方案为易于扩展的XR用户识别系统奠定基础，可应用于广泛场景。

    In this paper, we combine the strengths of distance-based and classification-based approaches for the task of identifying extended reality users by their movements. For this we present an embedding-based approach that leverages deep metric learning. We train the model on a dataset of users playing the VR game ``Half-Life: Alyx'' and conduct multiple experiments and analyses using a state of the art classification-based model as baseline. The results show that the embedding-based method 1) is able to identify new users from non-specific movements using only a few minutes of enrollment data, 2) can enroll new users within seconds, while retraining the baseline approach takes almost a day, 3) is more reliable than the baseline approach when only little enrollment data is available, 4) can be used to identify new users from another dataset recorded with different VR devices.  Altogether, our solution is a foundation for easily extensible XR user identification systems, applicable to a wide
    
[^147]: 基于策略的平均回报强化学习算法的性能界限

    Performance Bounds for Policy-Based Average Reward Reinforcement Learning Algorithms. (arXiv:2302.01450v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01450](http://arxiv.org/abs/2302.01450)

    本文通过获得平均回报MDPs的有限时间误差界限解决了近似策略迭代和强化学习算法在平均回报设置中的性能界限问题

    

    许多基于策略的强化学习算法可以看作是近似策略迭代的实例，即策略改进和策略评估都是近似进行的。在平均回报目标是有意义的性能度量的应用中，通常使用折扣回报公式，并使折扣因子接近1，这相当于使预期的时间长的无限大。然而，对于误差性能，相应的理论界限与时间长的平方成比例。因此，即使将总回报除以时间长，平均回报问题的相应性能界限仍趋于无穷大。因此，获得近似策略迭代和强化学习算法在平均回报设置中的有意义的性能界限一直是一个未解决的问题。在本文中，我们通过获得平均回报MDPs的有限时间误差界限来解决这个未解决的问题，并证明了渐近情况下，

    Many policy-based reinforcement learning (RL) algorithms can be viewed as instantiations of approximate policy iteration (PI), i.e., where policy improvement and policy evaluation are both performed approximately. In applications where the average reward objective is the meaningful performance metric, discounted reward formulations are often used with the discount factor being close to $1,$ which is equivalent to making the expected horizon very large. However, the corresponding theoretical bounds for error performance scale with the square of the horizon. Thus, even after dividing the total reward by the length of the horizon, the corresponding performance bounds for average reward problems go to infinity. Therefore, an open problem has been to obtain meaningful performance bounds for approximate PI and RL algorithms for the average-reward setting. In this paper, we solve this open problem by obtaining the first finite-time error bounds for average-reward MDPs, and show that the asymp
    
[^148]: 迈向完全协变的机器学习

    Towards fully covariant machine learning. (arXiv:2301.13724v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.13724](http://arxiv.org/abs/2301.13724)

    本文探讨了机器学习中多个被动对称性的影响，并提出了关于机器学习实践中尊重被动对称性的 dos and don'ts。此外，还讨论了被动对称性与因果建模的关系，并指出在学习问题的目标是样本外推广时，实现被动对称性尤其有价值。

    

    任何对数据的表示都涉及到任意的研究者选择。由于这些选择是外部于数据生成过程的，每个选择都导致了一个确切的对称性，对应于将一个可能的表示转化为另一个表示的变换群。这些被动对称性包括坐标自由度、规范对称性和单位协变性，在物理学中都产生了重要的结果。在机器学习中，最明显的被动对称性是图的重新标记或置换对称性。我们的目标是理解这些被动对称性对机器学习的影响。如果要尊重被动对称性，我们讨论了机器学习实践中的应该和不应该。我们还讨论了与因果建模的联系，并认为在学习问题的目标是样本外推广时，实现被动对称性特别有价值。这篇论文是概念性的：它在物理学的语言之间进行翻译。

    Any representation of data involves arbitrary investigator choices. Because those choices are external to the data-generating process, each choice leads to an exact symmetry, corresponding to the group of transformations that takes one possible representation to another. These are the passive symmetries; they include coordinate freedom, gauge symmetry, and units covariance, all of which have led to important results in physics. In machine learning, the most visible passive symmetry is the relabeling or permutation symmetry of graphs. Our goal is to understand the implications for machine learning of the many passive symmetries in play. We discuss dos and don'ts for machine learning practice if passive symmetries are to be respected. We discuss links to causal modeling, and argue that the implementation of passive symmetries is particularly valuable when the goal of the learning problem is to generalize out of sample. This paper is conceptual: It translates among the languages of physic
    
[^149]: 对未知数据的泛化、逻辑推理和学位课程的概述

    Generalization on the Unseen, Logic Reasoning and Degree Curriculum. (arXiv:2301.13105v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13105](http://arxiv.org/abs/2301.13105)

    本文研究了在逻辑推理任务中对未知数据的泛化能力，提供了网络架构在该设置下的表现证据，发现了一类网络模型在未知数据上学习了最小度插值器，并对长度普通化现象提供了解释。

    

    本文考虑了逻辑（布尔）函数的学习，重点在于对未知数据的泛化（GOTU）设定，这是一种强大的分布外泛化的案例。这是由于某些推理任务（例如算术/逻辑）中数据的丰富组合性质使得代表性数据采样具有挑战性，并且在GOTU下成功学习为第一个“推理”学习者展示了一个小插图。然后，我们研究了通过(S)GD训练的不同网络架构在GOTU下的表现，并提供了理论和实验证据，证明了一个类别的网络模型（包括Transformer的实例、随机特征模型和对角线线性网络）在未知数据上学习了最小度插值器。我们还提供了证据表明，其他具有更大学习速率或均场网络的实例达到了渗漏最小度解。这些发现带来了两个影响：（1）我们提供了对长度普通化的解释

    This paper considers the learning of logical (Boolean) functions with focus on the generalization on the unseen (GOTU) setting, a strong case of out-of-distribution generalization. This is motivated by the fact that the rich combinatorial nature of data in certain reasoning tasks (e.g., arithmetic/logic) makes representative data sampling challenging, and learning successfully under GOTU gives a first vignette of an 'extrapolating' or 'reasoning' learner. We then study how different network architectures trained by (S)GD perform under GOTU and provide both theoretical and experimental evidence that for a class of network models including instances of Transformers, random features models, and diagonal linear networks, a min-degree-interpolator is learned on the unseen. We also provide evidence that other instances with larger learning rates or mean-field networks reach leaky min-degree solutions. These findings lead to two implications: (1) we provide an explanation to the length genera
    
[^150]: Reef-insight:一种通过遥感进行水域栖息地映射的聚类方法框架

    Reef-insight: A framework for reef habitat mapping with clustering methods via remote sensing. (arXiv:2301.10876v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10876](http://arxiv.org/abs/2301.10876)

    Reef-Insight是一种利用聚类方法和遥感技术进行珊瑚礁栖息地映射的无监督机器学习框架，通过比较不同的聚类方法，我们发现遥感数据可以有效地用于珊瑚礁栖息地的映射。

    

    环境损害一直是一个重大关注点，特别是在海岸区域和海洋中，考虑到气候变化和污染及极端气候事件的严重影响。我们目前的分析能力以及遥感等信息获取技术的进步，可以用于管理和研究珊瑚礁生态系统。在本文中，我们提出了一种名为Reef-Insight的无监督机器学习框架，它具有先进的聚类方法和遥感技术，用于珊瑚礁栖息地映射。我们的框架通过使用遥感数据比较不同的聚类方法进行珊瑚礁栖息地映射。我们评估了基于定性和视觉评估的四种主要聚类方法，包括k-means、层次聚类、高斯混合模型和密度聚类。我们利用了澳大利亚南大堡礁的One Tree Island珊瑚礁的遥感数据进行试验。我们的研究结果表明，通过遥感数据进行聚类方法可以有效进行珊瑚礁栖息地映射。

    Environmental damage has been of much concern, particularly in coastal areas and the oceans, given climate change and the drastic effects of pollution and extreme climate events. Our present-day analytical capabilities, along with advancements in information acquisition techniques such as remote sensing, can be utilised for the management and study of coral reef ecosystems. In this paper, we present Reef-Insight, an unsupervised machine learning framework that features advanced clustering methods and remote sensing for reef habitat mapping. Our framework compares different clustering methods for reef habitat mapping using remote sensing data. We evaluate four major clustering approaches based on qualitative and visual assessments which include k-means, hierarchical clustering, Gaussian mixture model, and density-based clustering. We utilise remote sensing data featuring the One Tree Island reef in Australia's Southern Great Barrier Reef. Our results indicate that clustering methods usi
    
[^151]: 使用计算机视觉和LSTM神经网络分析和预测太阳冕空洞

    Solar Coronal Hole Analysis and Prediction using Computer Vision and LSTM Neural Network. (arXiv:2301.06732v4 [astro-ph.SR] UPDATED)

    [http://arxiv.org/abs/2301.06732](http://arxiv.org/abs/2301.06732)

    本研究使用计算机视觉和LSTM神经网络分析和预测太阳冕空洞的大小和趋势，并据此为地球对太阳冕空洞的影响做准备。

    

    随着人类开始探索太空，太空天气的重要性变得明显。已经确立了太阳冕空洞这一种太空天气现象会对飞机和卫星的运作产生影响。太阳冕空洞是太阳上的一片区域，其特点是磁场线开放而温度相对较低，导致太阳风以高于平均速率进行发射。在本研究中，为了准备好应对太阳冕空洞对地球的影响，我们使用计算机视觉检测太阳动力学观测卫星（SDO）图像中的太阳冕空洞区域并计算其大小。我们对太阳每个区域的太阳冕空洞进行比较和相关性分析。然后，我们实施深度学习技术，特别是使用长短期记忆（LSTM）方法分析太阳冕空洞面积数据的趋势，并预测不同太阳区域未来7天的太阳冕空洞大小。通过分析太阳冕空洞面积的时间序列数据，本研究旨在辨识太阳冕空洞的变化趋势和预测其大小。

    As humanity has begun to explore space, the significance of space weather has become apparent. It has been established that coronal holes, a type of space weather phenomenon, can impact the operation of aircraft and satellites. The coronal hole is an area on the sun characterized by open magnetic field lines and relatively low temperatures, which result in the emission of the solar wind at higher than average rates. In this study, To prepare for the impact of coronal holes on the Earth, we use computer vision to detect the coronal hole region and calculate its size based on images from the Solar Dynamics Observatory (SDO). We compare the coronal holes for each region of the Sun and analyze the correlation. We then implement deep learning techniques, specifically the Long Short-Term Memory (LSTM) method, to analyze trends in the coronal hole area data and predict its size for different sun regions over 7 days. By analyzing time series data on the coronal hole area, this study aims to id
    
[^152]: 通过概率预测缩短实时癫痫发作检测的延迟

    Shorter Latency of Real-time Epileptic Seizure Detection via Probabilistic Prediction. (arXiv:2301.03465v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2301.03465](http://arxiv.org/abs/2301.03465)

    通过引入概率预测，将癫痫发作检测任务转化为交叉期样本的标注规则，并使用深度学习框架进行特征提取和预测概率增强，并应用累积决策规则，成功缩短了癫痫发作检测的延迟。

    

    虽然最近的研究提出了具有良好灵敏度性能的癫痫发作检测算法，但在实时场景中实现显著缩短的检测延迟仍然是一个挑战。在本文中，我们提出了一个新的深度学习框架，通过概率预测来缩短癫痫发作检测的延迟。我们首次将癫痫发作检测任务从传统的二分类转化为概率预测，通过引入癫痫定向脑电图记录的交叉期并提出使用软标签对交叉期样本进行标注的规则。同时，我们还提出了一种基于多尺度STFT的特征提取方法，结合3D-CNN架构，可以准确地捕捉样本的预测概率。此外，我们还提出了修正加权策略以增强预测概率，并提出了累积决策规则以实现显著缩短的检测延迟。

    Although recent studies have proposed seizure detection algorithms with good sensitivity performance, there is a remained challenge that they were hard to achieve significantly short detection latency in real-time scenarios. In this manuscript, we propose a novel deep learning framework intended for shortening epileptic seizure detection latency via probabilistic prediction. We are the first to convert the seizure detection task from traditional binary classification to probabilistic prediction by introducing a crossing period from seizure-oriented EEG recording and proposing a labeling rule using soft-label for crossing period samples. And, a novel multiscale STFT-based feature extraction method combined with 3D-CNN architecture is proposed to accurately capture predictive probabilities of samples. Furthermore, we also propose rectified weighting strategy to enhance predictive probabilities, and accumulative decision-making rule to achieve significantly shorter detection latency. We i
    
[^153]: 深度R编程

    Deep R Programming. (arXiv:2301.01188v2 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2301.01188](http://arxiv.org/abs/2301.01188)

    该课程介绍了流行的数据科学语言R，并旨在培养学生、从业者和研究者成为独立的R语言用户。

    

    深度R编程是一门全面的课程，重点介绍数据科学中最流行的语言之一——R语言（统计计算、数据可视化、机器学习、数据清洗和分析）。它深入介绍了R语言的基础知识，旨在培养有抱负的学生、从业者和研究者，让他们成为独立使用这个强大环境的用户。这个教材是一个非盈利项目，它的在线和PDF版本可以在 <https://deepr.gagolewski.com/> 免费获取。希望这个早期草案发放出来后能对读者有所帮助。

    Deep R Programming is a comprehensive course on one of the most popular languages in data science (statistical computing, graphics, machine learning, data wrangling and analytics). It introduces the base language in-depth and is aimed at ambitious students, practitioners, and researchers who would like to become independent users of this powerful environment. This textbook is a non-profit project. Its online and PDF versions are freely available at <https://deepr.gagolewski.com/>. This early draft is distributed in the hope that it will be useful.
    
[^154]: 基于转换器的生物医学语言模型的领域内自适应的本地化

    Localising In-Domain Adaptation of Transformer-Based Biomedical Language Models. (arXiv:2212.10422v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10422](http://arxiv.org/abs/2212.10422)

    本研究针对生物医学领域内自适应问题，探讨了两种途径来在非英语语言中产生生物医学语言模型。一种是通过神经机器翻译将英文资源翻译为目标语言，注重数量；另一种是直接基于高质量、狭谱的语料库进行本地化。这些方法有助于解决资源较少语言如意大利语的领域内适应问题。

    

    在数字医疗时代，医院每天产生的大量文本信息构成了一个重要但未充分利用的资产，可以通过特定任务、精细调整的生物医学语言表示模型来利用，从而改善患者护理和管理。对于这些专门领域，先前的研究表明，来自广覆盖点检的微调模型在大规模领域内资源的额外训练轮次上可以获益很大。然而，这些资源通常对于像意大利这样资源较少的语言是不可及的，使得当地医疗机构无法进行领域内适应。为了缩小这个差距，我们的工作探讨了两种可行的方法来在非英语语言中生成生物医学语言模型，以意大利语为具体案例：一种基于英文资源的神经机器翻译，追求数量而不是质量；另一种基于高质量、狭谱的语料库的方法，直接进行本地化。

    In the era of digital healthcare, the huge volumes of textual information generated every day in hospitals constitute an essential but underused asset that could be exploited with task-specific, fine-tuned biomedical language representation models, improving patient care and management. For such specialized domains, previous research has shown that fine-tuning models stemming from broad-coverage checkpoints can largely benefit additional training rounds over large-scale in-domain resources. However, these resources are often unreachable for less-resourced languages like Italian, preventing local medical institutions to employ in-domain adaptation. In order to reduce this gap, our work investigates two accessible approaches to derive biomedical language models in languages other than English, taking Italian as a concrete use-case: one based on neural machine translation of English resources, favoring quantity over quality; the other based on a high-grade, narrow-scoped corpus natively w
    
[^155]: 支持向量回归: 风险四方框架

    Support Vector Regression: Risk Quadrangle Framework. (arXiv:2212.09178v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.09178](http://arxiv.org/abs/2212.09178)

    本文结合风险四方理论，研究了支持向量回归（SVR）。研究结果发现，SVR的两种形式对应于等效误差度量的最小化，同时加上正则化惩罚项。通过构造基本风险四方框，我们证明了SVR是对两个对称条件分位数的平均数的渐近无偏估计量。此外，我们证明了$\varepsilon$-SVR和$\nu$-SVR在一般随机环境下的等价性。

    

    本文在基本的风险四方理论的背景下研究了支持向量回归（SVR），该理论将优化、风险管理和统计估计联系起来。研究结果表明，SVR的两种形式，$\varepsilon$-SVR和$\nu$-SVR，都对应于等效误差度量（分别为Vapnik误差和CVaR范数）的最小化，同时加上正则化惩罚项。这些误差度量又定义了相应的风险四方框。通过构造与SVR对应的基本风险四方框，我们证明了SVR是两个对称条件分位数的平均数的渐近无偏估计量。此外，我们在一般随机环境中证明了$\varepsilon$-SVR和$\nu$-SVR的等价性。此外，SVR被表述为带有正则化惩罚项的正则偏离最小化问题。最后，推导了在风险四方框架中的SVR的对偶形式。

    This paper investigates Support Vector Regression (SVR) in the context of the fundamental risk quadrangle theory, which links optimization, risk management, and statistical estimation. It is shown that both formulations of SVR, $\varepsilon$-SVR and $\nu$-SVR, correspond to the minimization of equivalent error measures (Vapnik error and CVaR norm, respectively) with a regularization penalty. These error measures, in turn, define the corresponding risk quadrangles. By constructing the fundamental risk quadrangle, which corresponds to SVR, we show that SVR is the asymptotically unbiased estimator of the average of two symmetric conditional quantiles. Further, we prove the equivalence of the $\varepsilon$-SVR and $\nu$-SVR in a general stochastic setting. Additionally, SVR is formulated as a regular deviation minimization problem with a regularization penalty. Finally, the dual formulation of SVR in the risk quadrangle framework is derived.
    
[^156]: 可微分的用户模型

    Differentiable User Models. (arXiv:2211.16277v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16277](http://arxiv.org/abs/2211.16277)

    该研究提出了可微分的用户模型，通过引入可广泛应用的可微分替代品解决了现代先进用户模型与机器学习流程的不兼容性和计算代价过高的问题。实验证明，在线应用中可以实现与现有无似然推理方法相当的建模能力，并展示了在菜单搜索任务中如何利用认知模型进行在线交互。

    

    概率用户建模对于在人机交互中构建机器学习系统至关重要。然而，现代先进的用户模型通常被设计为认知行为模拟器，与现代机器学习流程不兼容，对于大多数实际应用来说计算代价过高。为了解决这个问题，我们引入了可广泛应用的可微分替代品，绕过计算瓶颈，使现代认知模型的推理更高效。通过实验证明，我们可以以适用于在线应用的计算成本实现与现有的无似然推理方法相当的建模能力。最后，我们展示了人工智能助手如何在菜单搜索任务中使用认知模型进行在线交互，而在交互过程中通常需要数小时的计算时间。

    Probabilistic user modeling is essential for building machine learning systems in the ubiquitous cases with humans in the loop. However, modern advanced user models, often designed as cognitive behavior simulators, are incompatible with modern machine learning pipelines and computationally prohibitive for most practical applications. We address this problem by introducing widely-applicable differentiable surrogates for bypassing this computational bottleneck; the surrogates enable computationally efficient inference with modern cognitive models. We show experimentally that modeling capabilities comparable to the only available solution, existing likelihood-free inference methods, are achievable with a computational cost suitable for online applications. Finally, we demonstrate how AI-assistants can now use cognitive models for online interaction in a menu-search task, which has so far required hours of computation during interaction.
    
[^157]: 用于减轻慢节点影响力的顺序梯度编码

    Sequential Gradient Coding For Straggler Mitigation. (arXiv:2211.13802v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.13802](http://arxiv.org/abs/2211.13802)

    本文介绍了两种方案，结合了梯度编码和选择性重复任务，实现了更好的拖车减轻性能。

    

    在分布式计算中，慢节点（即拖拉机）通常成为瓶颈。Tandon等人提出的梯度编码（GC）是一种利用纠错码原理在拖车存在的情况下分布梯度计算的有效技术。本文考虑计算一个梯度序列$\{g(1),g(2),\ldots,g(J)\}$的分布式计算，其中每个梯度$g(t)$的处理从第$t$轮开始，到第$(t+T)$轮结束。这里$T\geq 0$表示一个延迟参数。对于GC方案，编码仅在计算节点之间进行，导致$T=0$的解。然而，$T>0$允许设计利用时间维度的方案。在本文中，我们提出了两种方案，相比于GC，性能得到了改进。我们的第一个方案将GC与选择性重复之前未完成的任务相结合，实现了改进的拖车减轻。在我们的第二个方案中，构成了我们的主要创新。

    In distributed computing, slower nodes (stragglers) usually become a bottleneck. Gradient Coding (GC), introduced by Tandon et al., is an efficient technique that uses principles of error-correcting codes to distribute gradient computation in the presence of stragglers. In this paper, we consider the distributed computation of a sequence of gradients $\{g(1),g(2),\ldots,g(J)\}$, where processing of each gradient $g(t)$ starts in round-$t$ and finishes by round-$(t+T)$. Here $T\geq 0$ denotes a delay parameter. For the GC scheme, coding is only across computing nodes and this results in a solution where $T=0$. On the other hand, having $T>0$ allows for designing schemes which exploit the temporal dimension as well. In this work, we propose two schemes that demonstrate improved performance compared to GC. Our first scheme combines GC with selective repetition of previously unfinished tasks and achieves improved straggler mitigation. In our second scheme, which constitutes our main contri
    
[^158]: QueryForm: 一种简单的零样本表单实体查询框架

    QueryForm: A Simple Zero-shot Form Entity Query Framework. (arXiv:2211.07730v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07730](http://arxiv.org/abs/2211.07730)

    QueryForm是一种简单的零样本表单实体查询框架，通过使用双重提示机制和利用大规模查询-实体对进行预训练，能够从结构化文档中提取实体值，无需目标特定的训练数据，达到了新的最先进技术水平。

    

    零样本迁移学习对于文档理解是一个至关重要但未被充分研究的场景，有助于减少标注文档实体所需的高成本。我们提出了一种新颖的基于查询的框架QueryForm，该框架以零样本的方式从类似表单的文档中提取实体值。QueryForm包含一个双重提示机制，将文档模式和特定实体类型组合成一个查询，用于提示Transformer模型执行单个实体提取任务。此外，我们提议利用从类似表单的网页生成的大规模查询-实体对进行QueryForm的预训练，这些网页带有弱HTML注释。通过将预训练和微调统一到相同的基于查询的框架中，QueryForm使模型能够从包含各种实体和布局的结构化文档中学习，从而更好地推广到目标文档类型，无需目标特定的训练数据。QueryForm在平均水平上建立了新的最先进技术水平。

    Zero-shot transfer learning for document understanding is a crucial yet under-investigated scenario to help reduce the high cost involved in annotating document entities. We present a novel query-based framework, QueryForm, that extracts entity values from form-like documents in a zero-shot fashion. QueryForm contains a dual prompting mechanism that composes both the document schema and a specific entity type into a query, which is used to prompt a Transformer model to perform a single entity extraction task. Furthermore, we propose to leverage large-scale query-entity pairs generated from form-like webpages with weak HTML annotations to pre-train QueryForm. By unifying pre-training and fine-tuning into the same query-based framework, QueryForm enables models to learn from structured documents containing various entities and layouts, leading to better generalization to target document types without the need for target-specific training data. QueryForm sets new state-of-the-art average 
    
[^159]: 神经元集合推理方法生成模型的泛化

    Generalization of generative model for neuronal ensemble inference method. (arXiv:2211.05634v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2211.05634](http://arxiv.org/abs/2211.05634)

    提出了基于泛化的神经元集合推理方法，解决了贝叶斯推断模型中神经元活动非平稳性的问题。

    

    无数神经元的相互作用构成了维持生命活动所必需的各种脑功能，因此，分析功能神经网络的结构非常重要。为了阐明大脑功能的机制，包括神经科学各个领域在内的许多研究正在积极开展功能神经元集合和中心结构的研究。此外，最近的研究表明，功能神经元集合和中心的存在有助于提高信息处理效率。因此，需要从神经元活动数据中推断功能神经元集合的方法，基于贝叶斯推断的方法已经被提出。然而，在贝叶斯推断中建立活动模型存在问题。由于每个神经元的活动特征取决于生理实验条件，因此其具有非平稳性。结果，在贝叶斯推断模型中假设的平稳性会妨碍推断。

    Various brain functions that are necessary to maintain life activities materialize through the interaction of countless neurons. Therefore, it is important to analyze the structure of functional neuronal network. To elucidate the mechanism of brain function, many studies are being actively conducted on the structure of functional neuronal ensemble and hub, including all areas of neuroscience. In addition, recent study suggests that the existence of functional neuronal ensembles and hubs contributes to the efficiency of information processing. For these reasons, there is a demand for methods to infer functional neuronal ensembles from neuronal activity data, and methods based on Bayesian inference have been proposed. However, there is a problem in modeling the activity in Bayesian inference. The features of each neuron's activity have non-stationarity depending on physiological experimental conditions. As a result, the assumption of stationarity in Bayesian inference model impedes infer
    
[^160]: 贝叶斯线性模型中具有有限样本FDR控制的近似最优多重检验

    Near-optimal multiple testing in Bayesian linear models with finite-sample FDR control. (arXiv:2211.02778v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2211.02778](http://arxiv.org/abs/2211.02778)

    本文针对高维贝叶斯线性模型的多重检验问题，开发了近似最优的多重检验程序，能够在有限样本下控制频率FDR，并能达到近似最优功率。

    

    在高维变量选择问题中，统计学家通常致力于设计能控制虚警发现率（FDR）的多重检验过程，同时识别更多相关变量。模型-X方法（如Knockoffs和条件随机化检验）在假设已知协变量分布的情况下，实现了有限样本FDR控制的主要目标。然而，这些方法是否还能实现最大化发现的次要目标仍不确定。事实上，设计具有有限样本FDR控制并发现更多相关变量的程序，在可能是最简单的线性模型中，仍然是一个基本开放问题。本文针对高维贝叶斯线性模型与各向同性协变量，开发了接近最优的多重检验程序。我们引入了能在有限样本下保证频率FDR控制的模型-X程序，即使模型被错误指定，也能达到近似最优功率。

    In high dimensional variable selection problems, statisticians often seek to design multiple testing procedures that control the False Discovery Rate (FDR), while concurrently identifying a greater number of relevant variables. Model-X methods, such as Knockoffs and conditional randomization tests, achieve the primary goal of finite-sample FDR control, assuming a known distribution of covariates. However, whether these methods can also achieve the secondary goal of maximizing discoveries remains uncertain. In fact, designing procedures to discover more relevant variables with finite-sample FDR control is a largely open question, even within the arguably simplest linear models.  In this paper, we develop near-optimal multiple testing procedures for high dimensional Bayesian linear models with isotropic covariates. We introduce Model-X procedures that provably control the frequentist FDR from finite samples, even when the model is misspecified, and conjecturally achieve near-optimal powe
    
[^161]: 超几何表征学习的数值稳定性

    The Numerical Stability of Hyperbolic Representation Learning. (arXiv:2211.00181v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00181](http://arxiv.org/abs/2211.00181)

    本文研究了超几何表征学习中的数值不稳定性问题，比较了两种流行的超几何模型Poincar\'e球和Lorentz模型，发现Lorentz模型具有更好的数值稳定性和优化性能，同时提出一种新的欧几里得优化方案作为超几何学习的另一个选择。

    

    由于超球的容量随半径的指数增长，超几何空间能够将具有层次结构的数据集嵌入其中而不失真。然而，这种指数增长的性质常常导致数值不稳定性，使得训练超几何学习模型有时会导致灾难性的NaN问题和浮点算术中遇到无法表示的值。在本文中，我们对两种广泛使用的超几何模型——Poincar\'e球和Lorentz模型的局限性进行了仔细的分析。我们首先展示了，在64位算术系统下，Poincar\'e球相对于Lorentz模型具有更大的能力来正确表示点。然后，我们从优化的角度理论上验证了Lorentz模型优于Poincar\'e球的优越性。鉴于两种模型的数值限制，我们确定一种欧几里得优化方案，在Poincar\'e球和Lorentz模型之外为超几何学习提供了一种新的方案。

    Given the exponential growth of the volume of the ball w.r.t. its radius, the hyperbolic space is capable of embedding trees with arbitrarily small distortion and hence has received wide attention for representing hierarchical datasets. However, this exponential growth property comes at a price of numerical instability such that training hyperbolic learning models will sometimes lead to catastrophic NaN problems, encountering unrepresentable values in floating point arithmetic. In this work, we carefully analyze the limitation of two popular models for the hyperbolic space, namely, the Poincar\'e ball and the Lorentz model. We first show that, under the 64 bit arithmetic system, the Poincar\'e ball has a relatively larger capacity than the Lorentz model for correctly representing points. Then, we theoretically validate the superiority of the Lorentz model over the Poincar\'e ball from the perspective of optimization. Given the numerical limitations of both models, we identify one Eucli
    
[^162]: 跨客户标签传播用于跨设备和半监督联邦学习

    Cross-client Label Propagation for Transductive and Semi-Supervised Federated Learning. (arXiv:2210.06434v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06434](http://arxiv.org/abs/2210.06434)

    跨客户标签传播（XCLP）是一种用于跨设备和半监督联邦学习的新方法，在联邦学习中展示了更高的分类准确率。

    

    我们提出了一种新的跨设备联邦学习方法：跨客户标签传播（XCLP）。XCLP通过多个客户端的数据共同估计数据图，并通过在图上传播标签信息来计算无标签数据的标签。为了避免客户端需要与他人共享数据，XCLP采用了两个密码学安全协议：安全的汉明距离计算和安全求和。我们展示了XCLP在联邦学习中的两个不同应用。在第一个应用中，我们使用它以一次性的方式预测未见测试点的标签。在第二个应用中，我们将其用于在联邦半监督设置中重复伪标记无标签训练数据。在真实的联邦和标准基准数据集上的实验证明，XCLP在这两个应用中比替代方法实现了更高的分类准确率。

    We present Cross-Client Label Propagation(XCLP), a new method for transductive federated learning. XCLP estimates a data graph jointly from the data of multiple clients and computes labels for the unlabeled data by propagating label information across the graph. To avoid clients having to share their data with anyone, XCLP employs two cryptographically secure protocols: secure Hamming distance computation and secure summation. We demonstrate two distinct applications of XCLP within federated learning. In the first, we use it in a one-shot way to predict labels for unseen test points. In the second, we use it to repeatedly pseudo-label unlabeled training data in a federated semi-supervised setting. Experiments on both real federated and standard benchmark datasets show that in both applications XCLP achieves higher classification accuracy than alternative approaches.
    
[^163]: 通过隐式复合核将先验知识融入神经网络

    Incorporating Prior Knowledge into Neural Networks through an Implicit Composite Kernel. (arXiv:2205.07384v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.07384](http://arxiv.org/abs/2205.07384)

    本论文提出了一种通过深度学习和高斯过程的复合核来将先验知识融入神经网络的方法。通过隐式定义的神经网络核函数和选择的第二个核函数，可以模拟已知特性，并提高深度学习应用的性能。

    

    引导神经网络（NN）学习以先验知识是具有挑战性的。相比之下，许多已知特性，如空间平滑性或季节性，在高斯过程（GP）中通过选择适当的核函数来建模是直接的。许多深度学习应用可以通过建模这些已知特性来改进。例如，卷积神经网络（CNNs）广泛用于遥感，这受到强烈的季节效应影响。我们提出通过使用由神经网络隐式定义的核函数与选择用于建模已知特性的第二个核函数（例如季节性）相结合的复合核来结合深度学习和GP的建模能力。我们通过将深度网络和基于Nystrom近似的高效映射相结合来实现这一想法，将其称为隐式复合核（ICK）。然后，我们采用样本优化的方法来近似完整的GP后验分布。我们证明了ICK的有效性，并在遥感和时间序列的任务上进行了实验。

    It is challenging to guide neural network (NN) learning with prior knowledge. In contrast, many known properties, such as spatial smoothness or seasonality, are straightforward to model by choosing an appropriate kernel in a Gaussian process (GP). Many deep learning applications could be enhanced by modeling such known properties. For example, convolutional neural networks (CNNs) are frequently used in remote sensing, which is subject to strong seasonal effects. We propose to blend the strengths of deep learning and the clear modeling capabilities of GPs by using a composite kernel that combines a kernel implicitly defined by a neural network with a second kernel function chosen to model known properties (e.g., seasonality). We implement this idea by combining a deep network and an efficient mapping based on the Nystrom approximation, which we call Implicit Composite Kernel (ICK). We then adopt a sample-then-optimize approach to approximate the full GP posterior distribution. We demons
    
[^164]: 准确的长期预测的概率自回归神经网络

    Probabilistic AutoRegressive Neural Networks for Accurate Long-range Forecasting. (arXiv:2204.09640v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2204.09640](http://arxiv.org/abs/2204.09640)

    PARNN是一种概率自回归神经网络模型，能够准确预测具有非平稳性、非线性、非周期性、长期依赖和混沌模式的复杂时间序列数据，并通过预测区间提供不确定性量化。

    

    预测时间序列数据是一个严重的研究领域，应用范围从股票价格到早期传染病预测。虽然已经提出了许多统计和机器学习方法，但实际预测问题通常需要将传统预测方法和现代神经网络模型相结合的混合解决方案。在这项研究中，我们介绍了概率自回归神经网络（PARNN），能够处理表现出非平稳性、非线性、非周期性、长期依赖和混沌模式的复杂时间序列数据。PARNN是通过改进自回归神经网络（ARNN）使用自回归积分移动平均（ARIMA）反馈误差进行构建的，结合了两种模型的可解释性、可扩展性和“白盒子般”的预测行为。值得注意的是，PARNN模型通过预测区间提供不确定性量化，使其与先进的深度学习工具区别开来。通过全面的计算。

    Forecasting time series data is a critical area of research with applications spanning from stock prices to early epidemic prediction. While numerous statistical and machine learning methods have been proposed, real-life prediction problems often require hybrid solutions that bridge classical forecasting approaches and modern neural network models. In this study, we introduce the Probabilistic AutoRegressive Neural Networks (PARNN), capable of handling complex time series data exhibiting non-stationarity, nonlinearity, non-seasonality, long-range dependence, and chaotic patterns. PARNN is constructed by improving autoregressive neural networks (ARNN) using autoregressive integrated moving average (ARIMA) feedback error, combining the explainability, scalability, and "white-box-like" prediction behavior of both models. Notably, the PARNN model provides uncertainty quantification through prediction intervals, setting it apart from advanced deep learning tools. Through comprehensive compu
    
[^165]: PyDTS：用于离散时间竞争风险（正则化）回归的 Python 包

    PyDTS: A Python Package for Discrete-Time Survival (Regularized) Regression with Competing Risks. (arXiv:2204.05731v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2204.05731](http://arxiv.org/abs/2204.05731)

    PyDTS是一个用于离散时间生存数据半参数竞争风险模型的Python包，支持包括LASSO和弹性网等正则化回归方法。

    

    时间至事件分析（生存分析）用于响应时间是指预定事件发生的时间。由于时间本身是离散的或由于将失败时间分组为间隔或舍入测量，因此时间至事件数据有时是离散的。此外，个体的失败可能是几种不同的失败类型之一，称为竞争风险（事件）。大多数生存回归分析的方法和软件包假定时间是在连续尺度上测量的。众所周知，将标准的连续时间模型应用于离散时间数据可能导致离散时间模型的估计器存在偏差。介绍了 Python 包 PyDTS，用于模拟，估计和评估离散时间生存数据的半参数竞争风险模型。该包实现了快速过程，使有效地包括正则化回归方法，如 LASSO 和弹性网络等。一个模拟

    Time-to-event analysis (survival analysis) is used when the response of interest is the time until a pre-specified event occurs. Time-to-event data are sometimes discrete either because time itself is discrete or due to grouping of failure times into intervals or rounding off measurements. In addition, the failure of an individual could be one of several distinct failure types, known as competing risks (events). Most methods and software packages for survival regression analysis assume that time is measured on a continuous scale. It is well-known that naively applying standard continuous-time models with discrete-time data may result in biased estimators of the discrete-time models. The Python package PyDTS, for simulating, estimating and evaluating semi-parametric competing-risks models for discrete-time survival data, is introduced. The package implements a fast procedure that enables including regularized regression methods, such as LASSO and elastic net, among others. A simulation 
    
[^166]: 以插槽为中心的模型在测试时的适应性

    Test-time Adaptation with Slot-Centric Models. (arXiv:2203.11194v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.11194](http://arxiv.org/abs/2203.11194)

    本论文提出了一种以插槽为中心的模型，用于解析分布外的场景，并通过测试时自适应来提高模型性能。通过结合自监督损失和建模偏差，该模型在场景分解任务上取得了良好的效果。

    

    当前的视觉检测器在训练分布内表现出色，但通常无法将分布外的场景解析为其组成实体。最近的测试时适应方法使用辅助的自监督损失来独立地调整网络参数，已经在图像分类任务中显示出在训练分布之外的泛化结果。在我们的研究中，我们发现这些损失对于场景分解任务来说是不足的，因为它们没有考虑到建模偏差。最近的以插槽为中心的生成模型尝试以自监督的方式将场景分解为实体，通过重建像素来实现。结合这两个工作线路，我们提出了一种半监督的插槽为中心的场景分解模型，即Slot-TTA，在测试时通过重建或交叉视图综合目标在每个场景上进行梯度下降适应。我们对Slot-TTA在多种输入模式上进行评估。

    Current visual detectors, though impressive within their training distribution, often fail to parse out-of-distribution scenes into their constituent entities. Recent test-time adaptation methods use auxiliary self-supervised losses to adapt the network parameters to each test example independently and have shown promising results towards generalization outside the training distribution for the task of image classification. In our work, we find evidence that these losses are insufficient for the task of scene decomposition, without also considering architectural inductive biases. Recent slot-centric generative models attempt to decompose scenes into entities in a self-supervised manner by reconstructing pixels. Drawing upon these two lines of work, we propose Slot-TTA, a semi-supervised slot-centric scene decomposition model that at test time is adapted per scene through gradient descent on reconstruction or cross-view synthesis objectives. We evaluate Slot-TTA across multiple input mo
    
[^167]: SUPERNOVA: 使用基于风险的测试和机器学习在AAA游戏中自动化测试选择和缺陷预防

    SUPERNOVA: Automating Test Selection and Defect Prevention in AAA Video Games Using Risk Based Testing and Machine Learning. (arXiv:2203.05566v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2203.05566](http://arxiv.org/abs/2203.05566)

    SUPERNOVA是一个负责自动化测试选择和缺陷预防的系统，它使用基于风险的测试和机器学习方法来解决视频游戏测试中的挑战。

    

    随着软件系统的不断增长，测试视频游戏变得越来越困难，传统方法难以扩展。手动测试是一个非常耗时的过程，因此很快就变得成本不可行。使用脚本进行自动化测试是可行的，但在非确定性环境中脚本是无效的，而且知道何时运行每个测试是另一个问题。现代游戏的复杂性、范围和玩家期望正在迅速增加，质量控制占据了生产成本和交付风险的很大一部分。减少这种风险并实现生产对于该行业来说是一个巨大挑战。为了在发布前后保持生产成本的实际性，我们将重点放在预防性质量保证策略以及测试和数据分析自动化上。我们提出了SUPERNOVA（用于新颖软件异常的测试选择和通用缺陷预防在外部存储库中的系统），这是一个负责

    Testing video games is an increasingly difficult task as traditional methods fail to scale with growing software systems. Manual testing is a very labor-intensive process, and therefore quickly becomes cost prohibitive. Using scripts for automated testing is affordable, however scripts are ineffective in non-deterministic environments, and knowing when to run each test is another problem altogether. The modern game's complexity, scope, and player expectations are rapidly increasing where quality control is a big portion of the production cost and delivery risk. Reducing this risk and making production happen is a big challenge for the industry currently. To keep production costs realistic up-to and after release, we are focusing on preventive quality assurance tactics alongside testing and data analysis automation. We present SUPERNOVA (Selection of tests and Universal defect Prevention in External Repositories for Novel Objective Verification of software Anomalies), a system responsib
    
[^168]: CoCoFL: 基于部分神经网络冻结和量化的通信和计算感知联邦学习

    CoCoFL: Communication- and Computation-Aware Federated Learning via Partial NN Freezing and Quantization. (arXiv:2203.05468v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.05468](http://arxiv.org/abs/2203.05468)

    CoCoFL是一种基于神经网络冻结和量化的通信和计算感知的联邦学习技术，可以适应设备的异构资源，并提高准确度。

    

    参与联邦学习的设备通常具有异构的通信、计算和内存资源。然而，在同步联邦学习中，所有设备需要在由服务器规定的相同截止日期之前完成训练。我们的研究结果表明，在受限设备上训练较小的神经网络子集（即通过删除神经元/滤波器）是低效的，阻止了这些设备对模型的有效贡献。这导致了针对受限设备的可达准确度的不公平性，特别是在设备之间存在类标签分布不均匀的情况下。我们提出了一种新颖的联邦学习技术CoCoFL，该技术在所有设备上保持完整的神经网络结构。为了适应设备的异构资源，CoCoFL冻结和量化选择的层，降低通信、计算和内存需求，而其他层仍然以全精度进行训练，实现高准确度。

    Devices participating in federated learning (FL) typically have heterogeneous communication, computation, and memory resources. However, in synchronous FL, all devices need to finish training by the same deadline dictated by the server. Our results show that training a smaller subset of the neural network (NN) at constrained devices, i.e., dropping neurons/filters as proposed by state of the art, is inefficient, preventing these devices to make an effective contribution to the model. This causes unfairness w.r.t the achievable accuracies of constrained devices, especially in cases with a skewed distribution of class labels across devices. We present a novel FL technique, CoCoFL, which maintains the full NN structure on all devices. To adapt to the devices' heterogeneous resources, CoCoFL freezes and quantizes selected layers, reducing communication, computation, and memory requirements, whereas other layers are still trained in full precision, enabling to reach a high accuracy. Thereby
    
[^169]: 高模态多模态Transformer：量化模态与交互异质性以进行高模态表示学习

    High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning. (arXiv:2203.01311v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01311](http://arxiv.org/abs/2203.01311)

    本文研究了高模态场景下的高效表示学习，提出了两种新的信息论度量方法来量化模态和交互的异质性，以加速对多样化和少被研究的模态的推广。 (arXiv:2203.01311v4 [cs.LG] UPDATED)

    

    许多现实世界的问题本质上是多模态的，例如人类用于交流的口语、手势和语用学，以及机器人上的力、本体感和视觉传感器。虽然多模态学习引起了广泛的兴趣，但这些方法主要关注一小组模态，主要是语言、视觉和音频。为了加速向多样化和少被研究的模态推广，本文研究了高模态场景下的高效表示学习，涉及一个大量的不同模态。由于为每个新模态添加新模型变得代价过高，关键的技术挑战是异质性量化：我们如何衡量哪些模态编码了类似的信息和交互，以便允许与先前的模态共享参数？本文提出了两种新的信息论度量方法来量化异质性：(1)模态异质性研究了两个模态之间的相似性。

    Many real-world problems are inherently multimodal, from spoken language, gestures, and paralinguistics humans use to communicate, to force, proprioception, and visual sensors on robots. While there has been an explosion of interest in multimodal learning, these methods are focused on a small set of modalities primarily in language, vision, and audio. In order to accelerate generalization towards diverse and understudied modalities, this paper studies efficient representation learning for high-modality scenarios involving a large set of diverse modalities. Since adding new models for every new modality becomes prohibitively expensive, a critical technical challenge is heterogeneity quantification: how can we measure which modalities encode similar information and interactions in order to permit parameter sharing with previous modalities? This paper proposes two new information theoretic metrics for heterogeneity quantification: (1) modality heterogeneity studies how similar 2 modalitie
    
[^170]: 利用信任进行多目标和多保真度优化的研究

    Leveraging Trust for Joint Multi-Objective and Multi-Fidelity Optimization. (arXiv:2112.13901v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.13901](http://arxiv.org/abs/2112.13901)

    这篇论文研究了一种利用信任度量来支持多目标和多数据来源优化的方法。通过将信任增益作为目标之一，将多保真度考虑在内，实现了同时进行多目标和多数据来源优化的目标。比较了两种优化方法：同时选择输入参数和保真度标准的整体方法以及基于帕累托优化的方法。

    

    在追求高效评估代价昂贵系统的优化过程中，本文研究了一种新颖的贝叶斯多目标和多保真度（MOMF）优化方法。传统的优化方法在多维度优化一个或多个目标时常常遇到极高的成本。多保真度方法通过利用多个成本较低的信息来源（如低分辨率模拟）提供潜在的解决方案。然而，将这两种策略整合起来却面临着巨大挑战。我们提出了创新性的信任度量方法，以支持同时优化多个目标和数据来源。我们的方法修改了多目标优化策略，将每次评估成本的信任增益作为一个目标纳入帕累托优化问题中，实现了同时进行MOMF优化并降低成本。我们提出并比较了两种MOMF优化方法：一种整体方法既选择输入参数又选择保真度标准。

    In the pursuit of efficient optimization of expensive-to-evaluate systems, this paper investigates a novel approach to Bayesian multi-objective and multi-fidelity (MOMF) optimization. Traditional optimization methods, while effective, often encounter prohibitively high costs in multi-dimensional optimizations of one or more objectives. Multi-fidelity approaches offer potential remedies by utilizing multiple, less costly information sources, such as low-resolution simulations. However, integrating these two strategies presents a significant challenge. We suggest the innovative use of a trust metric to support simultaneous optimization of multiple objectives and data sources. Our method modifies a multi-objective optimization policy to incorporate the trust gain per evaluation cost as one objective in a Pareto optimization problem, enabling simultaneous MOMF at lower costs. We present and compare two MOMF optimization methods: a holistic approach selecting both the input parameters and t
    
[^171]: 通过随机稀疏化梯度改进差分隐私随机梯度下降法

    Improving Differentially Private SGD via Randomly Sparsified Gradients. (arXiv:2112.00845v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.00845](http://arxiv.org/abs/2112.00845)

    通过在差分隐私随机梯度下降法中对梯度进行随机稀疏化，我们找到了一个调整收敛界的方法，从而在噪声占主导地位时获得更小的上界。这个观察表明，差分隐私随机梯度下降法有特殊的梯度压缩潜力。

    

    差分隐私随机梯度下降法（DP-SGD）已被广泛应用于深度学习，以提供严格定义的隐私保护，该方法要求对梯度进行剪切以限制个体梯度的最大范数，并添加各向同性高斯噪声。通过在非凸环境中分析DP-SGD的收敛速度，我们发现，在剪切和添加噪声之前对梯度进行随机稀疏化可以调整收敛界的内部成分之间的权衡，并在噪声占主导地位时导致更小的上界。此外，我们的理论分析和实证评估表明，这种权衡并不是微不足道的，而可能是DP-SGD的一种独特属性，因为取消噪声化或梯度剪切都会消除界限中的权衡。这一观察是有指示性的，因为它意味着DP-SGD具有特殊的内在空间，可以进行（甚至仅仅是随机的）梯度压缩。为了验证这一观察并利用它，我们提出了一种高效且轻量级的方法

    Differentially private stochastic gradient descent (DP-SGD) has been widely adopted in deep learning to provide rigorously defined privacy, which requires gradient clipping to bound the maximum norm of individual gradients and additive isotropic Gaussian noise. With analysis of the convergence rate of DP-SGD in a non-convex setting, we identify that randomly sparsifying gradients before clipping and noisification adjusts a trade-off between internal components of the convergence bound and leads to a smaller upper bound when the noise is dominant. Additionally, our theoretical analysis and empirical evaluations show that the trade-off is not trivial but possibly a unique property of DP-SGD, as either canceling noisification or gradient clipping eliminates the trade-off in the bound. This observation is indicative, as it implies DP-SGD has special inherent room for (even simply random) gradient compression. To verify the observation and utilize it, we propose an efficient and lightweight
    
[^172]: 阈值电路和子线性深度与能量的指数下界

    Exponential Lower Bounds for Threshold Circuits of Sub-Linear Depth and Energy. (arXiv:2107.00223v2 [cs.CC] UPDATED)

    [http://arxiv.org/abs/2107.00223](http://arxiv.org/abs/2107.00223)

    本文研究阈值电路和其他神经网络理论模型的计算能力。主要结果证明，任何阈值电路$C$的大小、深度、能量和权重满足$\log(rk(M_C)) \le ed (\log s + \log w + \log n)$。这表明，即使是子线性深度的阈值电路的大小也存在指数下界。

    

    本文研究了阈值电路和其他神经网络理论模型的计算能力，以以下四个复杂度度量为基础：大小（门数量）、深度、权重和能量。其中电路的能量复杂度衡量了它们计算的稀疏性，并被定义为在所有输入分配中产生非零值的门的最大数量。作为我们的主要结果，我们证明了任何大小为$s$，深度为$d$，能量为$e$，权重为$w$的阈值电路$C$满足$\log(rk(M_C)) \le ed (\log s + \log w + \log n)$，其中$rk(M_C)$是$C$计算的一个$2n$变量布尔函数的通信矩阵$M_C$的秩。因此，这样的阈值电路$C$只能计算具有通信矩阵的秩受到$w$和$d,e$的对数因子和$s$的线性因子的限制的布尔函数。这意味着甚至子线性深度的阈值电路的大小存在指数下界。

    In this paper, we investigate computational power of threshold circuits and other theoretical models of neural networks in terms of the following four complexity measures: size (the number of gates), depth, weight and energy. Here the energy complexity of a circuit measures sparsity of their computation, and is defined as the maximum number of gates outputting non-zero values taken over all the input assignments. As our main result, we prove that any threshold circuit $C$ of size $s$, depth $d$, energy $e$ and weight $w$ satisfies $\log (rk(M_C)) \le ed (\log s + \log w + \log n)$, where $rk(M_C)$ is the rank of the communication matrix $M_C$ of a $2n$-variable Boolean function that $C$ computes. Thus, such a threshold circuit $C$ is able to compute only a Boolean function of which communication matrix has rank bounded by a product of logarithmic factors of $s,w$ and linear factors of $d,e$. This implies an exponential lower bound on the size of even sublinear-depth threshold circuit i
    
[^173]: GeoT:一种几何感知的Transformer模型用于可靠的分子属性预测和化学可解释表示学习

    GeoT: A Geometry-aware Transformer for Reliable Molecular Property Prediction and Chemically Interpretable Representation Learning. (arXiv:2106.15516v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.15516](http://arxiv.org/abs/2106.15516)

    GeoT是一种几何感知的Transformer模型，用于可靠的分子属性预测和化学可解释表示学习。它通过注意力机制学习分子图结构，并生成与训练目标相关联的注意力图。与基于MPNN模型相比，GeoT具有可比的性能，同时减少了计算复杂性。

    

    近年来，分子表示学习已成为各种化学任务中的关键领域。然而，许多现有模型未能充分考虑分子结构的几何信息，导致表示不够直观。此外，广泛使用的消息传递机制局限于从化学角度解释实验结果。为了解决这些挑战，我们引入了一种新的基于Transformer的分子表示学习框架，称为几何感知Transformer（GeoT）。GeoT通过专门设计的基于注意力机制学习分子图结构，以提供可靠的解释性以及分子属性预测。因此，GeoT能够生成与训练目标相关联的原子间关系的注意力图。此外，GeoT展示了与基于MPNN模型相当的性能，同时实现了降低的计算复杂性。

    In recent years, molecular representation learning has emerged as a key area of focus in various chemical tasks. However, many existing models fail to fully consider the geometric information of molecular structures, resulting in less intuitive representations. Moreover, the widely used message-passing mechanism is limited to provide the interpretation of experimental results from a chemical perspective. To address these challenges, we introduce a novel Transformer-based framework for molecular representation learning, named the Geometry-aware Transformer (GeoT). GeoT learns molecular graph structures through attention-based mechanisms specifically designed to offer reliable interpretability, as well as molecular property prediction. Consequently, GeoT can generate attention maps of interatomic relationships associated with training objectives. In addition, GeoT demonstrates comparable performance to MPNN-based models while achieving reduced computational complexity. Our comprehensive 
    
[^174]: 利用未标记数据扩展类别的开放集学习（Open-LACU）

    Open-set learning with augmented category by exploiting unlabeled data (Open-LACU). (arXiv:2002.01368v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2002.01368](http://arxiv.org/abs/2002.01368)

    Open-LACU是一种新的开放式学习策略，它可以将分类器推广到观察到的和未观察到的新颖类别之间，并通过定义不同的背景和未知类别来提高训练成本效益性，确保在存在未观察到的新颖类别时进行安全分类。

    

    对于半监督学习（SSL）和开放式识别（OSR），已经进行了许多尝试以合成单个训练策略。然而，每次尝试都违反了开放集定义，因为这些方法在未标记的训练集中包含新颖的类别。本研究提出了一种新的学习策略，其中分类器能够在观察到的和未观察到的新颖类别之间进行推广，从而定义了观察到新颖类别的背景类别和未观察到新颖类别的未知类别。通过分类这两种新颖类别的方式，Open-LACU能够提高训练的成本效益性，并确保在存在未观察到的新颖类别时进行安全分类。

    Several efforts have been made to synthesize semi-supervised learning (SSL) and open set recognition (OSR) within a single training policy. However, each attempt violated the definition of an open set by incorporating novel categories within the unlabeled training set. Although such \textit{observed} novel categories are undoubtedly prevalent in application-grade datasets, they should not be conflated with the OSR-defined \textit{unobserved} novel categories, which only emerge during testing. This study proposes a new learning policy wherein classifiers generalize between observed and unobserved novel categories. Specifically, our open-set learning with augmented category by exploiting unlabeled data (Open-LACU) policy defines a background category for observed novel categories and an unknown category for unobserved novel categories. By separating these novel category types, Open-LACU promotes cost-efficient training by eliminating the need to label every category and ensures safe clas
    
[^175]: 了解具有非对称几何散射变换的图神经网络

    Understanding Graph Neural Networks with Asymmetric Geometric Scattering Transforms. (arXiv:1911.06253v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1911.06253](http://arxiv.org/abs/1911.06253)

    这项工作介绍了一种具有非对称几何散射变换的图神经网络，通过引入一类非对称小波，它统一和扩展了现有图形散射架构的理论结果，并为未来的深度学习架构为图形提供了基础。

    

    散射变换是一种基于小波的深度学习架构，作为卷积神经网络的模型。最近，有几篇工作引入了散射变换在非欧几里德设置（如图形）中的推广。我们的工作基于这些构造，引入了基于非常一般的非对称小波类的图形窗口化和非窗口化几何散射变换。我们证明了这些非对称图形散射变换与对称散射变换有许多相同的理论保证。因此，提出的构造统一和扩展了现有图形散射架构的已知理论结果。通过这样做，这项工作通过引入大量带有可证明稳定性和不变性保证的网络，有助于弥合几何散射和其他图神经网络之间的差距。这些结果为未来的深度学习架构为图形提供了基础。

    The scattering transform is a multilayered wavelet-based deep learning architecture that acts as a model of convolutional neural networks. Recently, several works have introduced generalizations of the scattering transform for non-Euclidean settings such as graphs. Our work builds upon these constructions by introducing windowed and non-windowed geometric scattering transforms for graphs based upon a very general class of asymmetric wavelets. We show that these asymmetric graph scattering transforms have many of the same theoretical guarantees as their symmetric counterparts. As a result, the proposed construction unifies and extends known theoretical results for many of the existing graph scattering architectures. In doing so, this work helps bridge the gap between geometric scattering and other graph neural networks by introducing a large family of networks with provable stability and invariance guarantees. These results lay the groundwork for future deep learning architectures for g
    
[^176]: 分析多变量对称不确定性度量在特征选择中的作用

    Understanding a Version of Multivariate Symmetric Uncertainty to assist in Feature Selection. (arXiv:1709.08730v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/1709.08730](http://arxiv.org/abs/1709.08730)

    通过分析多变量对称不确定性度量在特征选择中的作用，我们发现了一种条件，能够在属性数量、基数和样本大小不同组合下保持这一度量的良好质量，为降维过程提供了有用的准则。

    

    本文通过使用统计模拟技术在信息和非信息随机生成的特征混合中分析了多变量对称不确定性（MSU）度量的行为。实验结果显示属性数量、基数和样本大小对MSU的影响。我们发现了一种条件，能够在这三个因素的不同组合下保持MSU的良好质量，为驱动降维过程提供了一个有用的新准则。

    In this paper, we analyze the behavior of the multivariate symmetric uncertainty (MSU) measure through the use of statistical simulation techniques under various mixes of informative and non-informative randomly generated features. Experiments show how the number of attributes, their cardinalities, and the sample size affect the MSU. We discovered a condition that preserves good quality in the MSU under different combinations of these three factors, providing a new useful criterion to help drive the process of dimension reduction.
    

