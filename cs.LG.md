# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Machine Learning and Consumer Data.](http://arxiv.org/abs/2306.14118) | 数字革命导致了消费者行为的数字化，新兴现象使消费者行为更加复杂，传统的分析方法面临挑战。为了应对这个挑战，机器学习成为了有效解析和处理消费者数据的方法。 |
| [^2] | [Towards Trustworthy Explanation: On Causal Rationalization.](http://arxiv.org/abs/2306.14115) | 该论文介绍了一种新的因果关系解释方法，通过在解释中引入非虚假性和效率，从因果推断的角度定义了因果概率，从而建立了必要和充分解释的主要组成部分，相比现有的基于关联的解释方法，这种方法有更加优越的性能表现。 |
| [^3] | [TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning Granger Causal Structure from Event Sequences.](http://arxiv.org/abs/2306.14114) | 该论文提出了一种基于拓扑神经泊松自回归模型的方法，同时考虑先验拓扑网络和潜在的Granger因果结构来学习事件序列中的Granger因果关系，该方法通过推断因果关系来解决序列数据中的非 i.i.d. 问题 |
| [^4] | [Is RLHF More Difficult than Standard RL?.](http://arxiv.org/abs/2306.14111) | 本文证明了对于广泛的偏好模型，我们可以使用现有的算法和技术直接解决基于偏好的RL问题，而几乎不需要额外的成本。 |
| [^5] | [Language models are weak learners.](http://arxiv.org/abs/2306.14101) | 本文证明了，基于提示的大型语言模型可以作为弱学习器应用于表格数据的boosting算法中，并且在某些情况下可以优于传统的树形模型。 |
| [^6] | [Locally Differentially Private Distributed Online Learning with Guaranteed Optimality.](http://arxiv.org/abs/2306.14094) | 本文提出了一种具有保证最优性的方法，可以在分布式在线学习中同时保证差分隐私和学习准确性。 |
| [^7] | [Private Aggregation in Wireless Federated Learning with Heterogeneous Clusters.](http://arxiv.org/abs/2306.14088) | 本文探讨了在一个无线系统中，考虑到信息论隐私的条件下，通过基站连接到联合器的客户端，如何解决联邦学习中的隐私数据聚合问题。 |
| [^8] | [A Circuit Complexity Formulation of Algorithmic Information Theory.](http://arxiv.org/abs/2306.14087) | 该论文提出了一种基于电路复杂度的先验知识，旨在解决学习布尔函数的问题，重点是对简单解释具有归纳偏见。 |
| [^9] | [Leveraging Brain Modularity Prior for Interpretable Representation Learning of fMRI.](http://arxiv.org/abs/2306.14080) | 该论文提出了一种基于大脑模块化先验的动态表示学习框架，具有优秀的预测性能和可解释特征表示，为静息状态下的功能性核磁共振成像(rs-fMRI)的临床应用提供了一种新的工具和思路。 |
| [^10] | [Fighting Uncertainty with Gradients: Offline Reinforcement Learning via Diffusion Score Matching.](http://arxiv.org/abs/2306.14079) | 本文提出了平滑的距离数据度量标准，并将其与离线强化学习相结合，以对抗不确定性和分布偏移的挑战。该方法不仅在最小化梯度不确定性时稳定收敛到数据，而且不易低估真实不确定性，是一种有前途的策略搜索方法。 |
| [^11] | [Intensity-free Convolutional Temporal Point Process: Incorporating Local and Global Event Contexts.](http://arxiv.org/abs/2306.14072) | 本文提出了一种TPP建模方法，将连续时间卷积事件编码器与RNN集成，以融合局部和全局上下文。实验结果表明，该模型具有较好的性能表现。 |
| [^12] | [Waypoint Transformer: Reinforcement Learning via Supervised Learning with Intermediate Targets.](http://arxiv.org/abs/2306.14069) | Waypoint Transformer提出了一种改进RL的新方法，通过整合中间目标来实现，极大地提高了性能和稳定性，尤其在最具挑战性的环境和数据配置中表现得更加优秀。 |
| [^13] | [SEEDS: Emulation of Weather Forecast Ensembles with Diffusion Models.](http://arxiv.org/abs/2306.14066) | 本文提出了利用生成人工智能技术在规模上生成集合预测的方法，并产生了与完整的GEFS 31成员集合相似的预测能力，并且很好地模拟了大规模集合的统计数据。 |
| [^14] | [Modeling Graphs Beyond Hyperbolic: Graph Neural Networks in Symmetric Positive Definite Matrices.](http://arxiv.org/abs/2306.14064) | 本文构建了一个基于对称正定矩阵的图神经网络，可以很好地处理复杂图形数据。 |
| [^15] | [Offline Policy Evaluation for Reinforcement Learning with Adaptively Collected Data.](http://arxiv.org/abs/2306.14063) | 本论文提出了一种自适应采集数据的离线强化学习策略评估方法，为表格MDPs推导出高概率、实例相关的误差边界，并实现了自适应设置下的极小值最优离线学习。 |
| [^16] | [DesCo: Learning Object Recognition with Rich Language Descriptions.](http://arxiv.org/abs/2306.14060) | DesCo是一种新的物体识别模型学习范式，旨在通过详尽的语言描述提高对新对象和域的适应性。 |
| [^17] | [Towards Understanding Gradient Approximation in Equality Constrained Deep Declarative Networks.](http://arxiv.org/abs/2306.14054) | 本文探索了在等式约束下深度声明式网络中的梯度逼近策略，为训练深度学习模型提供了更为高效的计算方式。 |
| [^18] | [A Survey on Graph Neural Network Acceleration: Algorithms, Systems, and Customized Hardware.](http://arxiv.org/abs/2306.14052) | 本文综述了如何加速图神经网络处理图结构数据的方法，包括智能算法、高效系统和自定义硬件等，提出了GNN加速的分类，并为未来的研究提供了方向。 |
| [^19] | [Decision-Dependent Distributionally Robust Markov Decision Process Method in Dynamic Epidemic Control.](http://arxiv.org/abs/2306.14051) | 本文提出了一种新的分布鲁棒的马尔可夫决策过程框架，该框架允许考虑与决策相关的动态转移的不确定分布，从而更好地解决了动态流行病控制问题。 |
| [^20] | [H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models.](http://arxiv.org/abs/2306.14048) | 本文提出了一种通过预测文本中的热门元素来减少GPU内存消耗的方法，在实验中表现良好。 |
| [^21] | [Towards Optimal Pricing of Demand Response -- A Nonparametric Constrained Policy Optimization Approach.](http://arxiv.org/abs/2306.14047) | 本研究提出了一种非参数约束策略优化方法，通过消除对策略表示的限制性假设，提高优化度并确保策略更新的稳定性，从而实现电力负荷高峰和低峰之间的合理转移。 |
| [^22] | [Machine Learning needs its own Randomness Standard: Randomised Smoothing and PRNG-based attacks.](http://arxiv.org/abs/2306.14043) | 本文研究了机器学习中随机性可被攻击者利用的问题，主要关注流行的随机平滑方法。该方法用于训练可证明鲁棒性的模型和量化不确定性，但其随机性可能导致系统被攻击。 |
| [^23] | [Smoothed $f$-Divergence Distributionally Robust Optimization: Exponential Rate Efficiency and Complexity-Free Calibration.](http://arxiv.org/abs/2306.14041) | 分布鲁棒优化在实现统计保证界限时存在限制和保守性问题，但平滑的$f$-散度分布鲁棒优化可在指数衰减率方面实现最紧密的统计保证。 |
| [^24] | [Weighted Automata Extraction and Explanation of Recurrent Neural Networks for Natural Language Tasks.](http://arxiv.org/abs/2306.14040) | 本文提出了一种新的加权有限状态自动机（WFA）提取和解释框架来处理自然语言任务中的局限性，从而解决了精度和可扩展性方面的限制。 |
| [^25] | [Partitioning-Guided K-Means: Extreme Empty Cluster Resolution for Extreme Model Compression.](http://arxiv.org/abs/2306.14031) | 本研究提出了一种名为PG k-means的算法，基于划分来解决空簇，提高了iPQ与量化噪声的精确性。 |
| [^26] | [My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks.](http://arxiv.org/abs/2306.14030) | 本研究针对资源匮乏的印度语言马拉地语，提出了一个大型混合马拉地语-英语语料库，以及预训练的混合BERT模型和针对混合语言下游任务的评估数据集，该语料库训练的模型显著优于现有的BERT模型。 |
| [^27] | [Information criteria for structured parameter selection in high dimensional tree and graph models.](http://arxiv.org/abs/2306.14026) | 本文提出了适用于两种结构模型的信息准则，用于高维数据的参数选择，并平衡了虚假阳性和虚假阴性。 |
| [^28] | [Individualized Dosing Dynamics via Neural Eigen Decomposition.](http://arxiv.org/abs/2306.14020) | 本文提出了一个基于神经特征值分解的随机微分方程算法，可以用于个性化医疗给药模型，具有噪声水平可调、快速、连续、闭合的预测等特点，并在合成和真实医疗问题中展示了其鲁棒性和优越性能。 |
| [^29] | [Interpreting Forecasted Vital Signs Using N-BEATS in Sepsis Patients.](http://arxiv.org/abs/2306.14016) | 本文通过使用可解释的深度学习预测模型N-BEATS，预测并解读ICU感染患者的生命体征趋势，结果显示其性能优于基准模型。 |
| [^30] | [Boosting Multitask Learning on Graphs through Higher-Order Task Affinities.](http://arxiv.org/abs/2306.14009) | 本文从多任务学习的角度重新审视在给定图上预测节点标签的问题，提出通过更高级任务相似性来加强多任务学习，并开发了一种算法来将任务分组以应对负迁移问题。 |
| [^31] | [A clustering and graph deep learning-based framework for COVID-19 drug repurposing.](http://arxiv.org/abs/2306.13995) | 本文提出了一种基于聚类和图深度学习的方法，在 COVID-19 药物再利用中发现了具有见地的药物相互作用，并确定了潜在有效的药物再利用候选物。 |
| [^32] | [Kernel Support Vector Machine Classifiers with the $\ell_0$-Norm Hinge Loss.](http://arxiv.org/abs/2306.13991) | 本论文研究了一种基于核的支持向量机分类器，采用$\ell_0$-范数hinge loss处理标签噪声问题并提供了一种ADMM算法。 |
| [^33] | [Cross-Validation Is All You Need: A Statistical Approach To Label Noise Estimation.](http://arxiv.org/abs/2306.13990) | 本论文提出了一种重复交叉验证(Repeated Cross-Validation)方法，通过构建噪声直方图并提出三种基于该直方图的方法来检测标签噪声并清理数据，解决了结果预测分析中的数据清洗问题。 |
| [^34] | [SAM++: Enhancing Anatomic Matching using Semantic Information and Structural Inference.](http://arxiv.org/abs/2306.13988) | SAM++是一个医学影像解剖匹配框架，用于学习外观和语义嵌入，并且通过固定点匹配机制同时解决外观相似但语义不同或语义相似但外观不同的结构匹配问题。 |
| [^35] | [Robust Classification of High-Dimensional Data using Data-Adaptive Energy Distance.](http://arxiv.org/abs/2306.13985) | 该论文提出了一种用于高维低样本量数据分类的稳健的数据自适应能量距离分类器，该分类器无需调参且在一定条件下可以实现完美分类，已在模拟研究和实际数据分析中得到证明比其他方法表现更优。 |
| [^36] | [Mobile-Cloud Inference for Collaborative Intelligence.](http://arxiv.org/abs/2306.13982) | 移动设备上部分推断，生成紧凑的特征张量，并在服务器上进行进一步的推断，以降低推断时延和降低隐私风险。 |
| [^37] | [Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis.](http://arxiv.org/abs/2306.13960) | 本文提出了基于正则化SE(3)群卷积的体积医学图像分析方法，通过分解连续SO(3)核和空间核以实现旋转平移等变性，并在医学分类任务中获得了显著性能提升。 |
| [^38] | [DiffDTM: A conditional structure-free framework for bioactive molecules generation targeted for dual proteins.](http://arxiv.org/abs/2306.13957) | DiffDTM是一个去除条件结构限制的深度生成模型，用于针对双重蛋白质靶点的生物活性分子生成。该模型能够在一次性有条件生成的情况下生成具有高结合亲和力、可合成和新颖的药物样分子，并胜过最先进技术。 |
| [^39] | [Unleashing Realistic Air Quality Forecasting: Introducing the Ready-to-Use PurpleAirSF Dataset.](http://arxiv.org/abs/2306.13948) | 该论文介绍了PurpleAirSF数据集，这是一个易于获取的数据集，具有高时间分辨率、多种空气质量测量指标和广泛的地理范围，可用于研究人员的空气质量预测建模和空气污染模式研究，同时此数据集可用于未来开发新型的应用模型。 |
| [^40] | [Comparison of Pre-trained Language Models for Turkish Address Parsing.](http://arxiv.org/abs/2306.13947) | 本研究比较、评估了多语言和针对土耳其的BERT、DistilBERT、ELECTRA和RoBERTa模型在土耳其地址解析上的性能，结果发现针对土耳其的模型表现更佳。 |
| [^41] | [Large Sequence Models for Sequential Decision-Making: A Survey.](http://arxiv.org/abs/2306.13945) | 本综述全面概述了使用Transformer等序列模型解决顺序决策问题的最近研究进展，并按照处理样本效率、信用分配和部分可观察性的方式对其进行分类。 |
| [^42] | [Safe Reinforcement Learning with Dead-Ends Avoidance and Recovery.](http://arxiv.org/abs/2306.13944) | 本文提出了一种利用死局的边界来辨别安全和不安全状态，以确保安全性同时减少对探索的限制的方法。采用了分离的强化学习框架，训练了两个策略：一个任务策略，专注于任务表现，以及一个恢复策略，最大化安全性。 |
| [^43] | [Tuning structure learning algorithms with out-of-sample and resampling strategies.](http://arxiv.org/abs/2306.13932) | 本文提出了一种新的超参数调整方法 OTSL，它采用外样本和重抽样策略来估算给定输入数据集和结构学习算法的最佳超参数配置。实验表明，该方法优于现有技术，可提高结构学习算法的图形准确性。 |
| [^44] | [Comparative Study of Predicting Stock Index Using Deep Learning Models.](http://arxiv.org/abs/2306.13931) | 本文通过对传统预测方法和深度学习模型进行比较研究，证明Deep AR在股票指数预测方面表现最佳，具有较高的鲁棒性。 |
| [^45] | [Evaluating the Utility of GAN Generated Synthetic Tabular Data for Class Balancing and Low Resource Settings.](http://arxiv.org/abs/2306.13929) | 本研究评估了GAN生成的合成数据在解决分类任务中不平衡数据和提高模型在低资源环境中的性能方面的实用性，并发现在GAN合成数据上进行训练的模型性能表现更佳。 |
| [^46] | [On Convex Data-Driven Inverse Optimal Control for Nonlinear, Non-stationary and Stochastic Systems.](http://arxiv.org/abs/2306.13928) | 本文提出了一个凸数据驱动逆最优控制方案，能够有效解决非线性、非平稳和随机系统下的成本估计问题。 |
| [^47] | [Graph Neural Networks Provably Benefit from Structural Information: A Feature Learning Perspective.](http://arxiv.org/abs/2306.13926) | 本论文研究了GNN在神经网络特征学习理论中的作用。 发现图卷积网络显著增强了良性过拟合区域，在这个区域内信号学习超越了噪声记忆。 |
| [^48] | [Structuring Representation Geometry with Rotationally Equivariant Contrastive Learning.](http://arxiv.org/abs/2306.13924) | 本文在对比学习中引入了等变性目标并理论证明了最优解会将输入空间的增强变换对应于球形嵌入空间上的旋转变换，而我们的方法CARE获得更好的嵌入表示能力。 |
| [^49] | [Active Data Acquisition in Autonomous Driving Simulation.](http://arxiv.org/abs/2306.13923) | 本文提出了一种主动数据采集策略来解决自动驾驶模拟中大量冗余数据集的问题，并通过实验验证，该策略可以显著降低标注成本和数据集大小，同时提高数据集的整体质量，从而提高自动驾驶系统的性能。 |
| [^50] | [Multi-task multi-station earthquake monitoring: An all-in-one seismic Phase picking, Location, and Association Network (PLAN).](http://arxiv.org/abs/2306.13918) | 这项研究提出了一种使用图神经网络直接处理多站地震数据，实现同时进行相位拾取、关联和定位的方法，结合了站之间和任务之间的物理关系，在对比数据后证明具有较高的性能表现。 |
| [^51] | [G-TRACER: Expected Sharpness Optimization.](http://arxiv.org/abs/2306.13914) | G-TRACER是一种正则化深度学习结构优化方案，重点解决低信噪比问题，能有效促进泛化并获得了竞争性能。 |
| [^52] | [Spatio-temporal Storytelling? Leveraging Generative Models for Semantic Trajectory Analysis.](http://arxiv.org/abs/2306.13905) | 本文通过生成语言模型对语义轨迹进行分析和生成合成语义轨迹数据，为从城市规划到个性化推荐引擎和商业策略等一系列应用做出贡献。 |
| [^53] | [Differentially Private Decentralized Deep Learning with Consensus Algorithms.](http://arxiv.org/abs/2306.13892) | 本论文提出了一种具有差分隐私保护的分散学习算法，可用于合作分散深度学习，防止共享模型参数时泄露私有数据集的信息。 |
| [^54] | [L3Cube-MahaSent-MD: A Multi-domain Marathi Sentiment Analysis Dataset and Transformer Models.](http://arxiv.org/abs/2306.13888) | 本论文介绍了 L3Cube-MahaSent-MD，这是一个多领域的 Marathi 情感分析数据集。在其中，针对每个领域，我们构建了包含 1.5 万个样本的子数据集。我们微调了不同的单语和多语 BERT 模型，并报告了 MahaBERT 模型的最佳准确性。我们的研究提出了利用低资源多领域数据集进行情感分析的需求。 |
| [^55] | [Current density impedance imaging with PINNs.](http://arxiv.org/abs/2306.13881) | 本文提出了一种使用PINNs求解CDII的计算高效方法，该方法通过构造物理启发式损失函数实现正则化的最小二乘输出函数与基本微分方程的耦合。数值模拟结果表明其具有高效、准确和鲁棒性。 |
| [^56] | [Action Q-Transformer: Visual Explanation in Deep Reinforcement Learning with Encoder-Decoder Model using Action Query.](http://arxiv.org/abs/2306.13879) | 该论文提出了一个基于行为查询的Transformer编码器-解码器结构方法（AQT），用于生成深度强化学习模型，该模型更具可解释性，可以具体描述智能体的决策过程。 |
| [^57] | [Physics-Informed Machine Learning for Modeling and Control of Dynamical Systems.](http://arxiv.org/abs/2306.13867) | 物理信息机器学习是将机器学习算法与物理约束和抽象数学模型相结合，通过额外信息进行训练以获取更有效、物理上一致、数据效率更高的模型的方法。本文概述了物理信息机器学习在动态系统建模和控制中的最新进展，并讨论了领域面临的重要挑战和未来方向。 |
| [^58] | [Similarity Preserving Adversarial Graph Contrastive Learning.](http://arxiv.org/abs/2306.13854) | 本文提出了一种相似性保持的对抗图对比学习（SP-AGCL）框架，可以实现对抗攻击的对抗鲁棒性，同时保持节点特征相似性。 |
| [^59] | [A Unified Approach to Controlling Implicit Regularization via Mirror Descent.](http://arxiv.org/abs/2306.13853) | 本文提出了一种使用镜面下降方法来统一控制回归和分类问题中的隐式正则化的方法，在所有标准几何下都可以实现$\ell_p$（$p\in[1,\infty]$）范式的隐式正则化，并且可以实现学习理论中许多特殊类的控制的泛化保证。 |
| [^60] | [Is Pre-training Truly Better Than Meta-Learning?.](http://arxiv.org/abs/2306.13841) | 在少样本学习中，当数据集的正式多样性较低时，预训练模型（PT）胜过模型无关元学习（MAML）。当正式多样性较高时，MAML更好。 |
| [^61] | [Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data.](http://arxiv.org/abs/2306.13840) | 本论文提出使用多样性系数作为LLM预训练数据质量的指标，研究表明公开可用的LLM数据集的多样性系数很高。 |
| [^62] | [Computron: Serving Distributed Deep Learning Models with Model Parallel Swapping.](http://arxiv.org/abs/2306.13835) | 该论文介绍了Computron系统，该系统使用内存交换实现模型并行交换设计来服务于多个分布式深度学习模型，在处理多个大型模型任务时可提高资源利用率和交换速度。 |
| [^63] | [Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks.](http://arxiv.org/abs/2306.13831) | Minigrid & Miniworld能够提供一系列面向目标的2D和3D环境，通过采用极简主义的设计范式，使其易于定制和开发，这已经被RL社区广泛采用。另外，统一的API还能够实现在不同观测空间之间进行转移学习。 |
| [^64] | [Aircraft Environmental Impact Segmentation via Metric Learning.](http://arxiv.org/abs/2306.13830) | 本文介绍了机载环境影响建模中的度量学习方法，在弱监督度量学习任务下取得了显著性能提升，有望实现对机载环境影响更高效、更精准的建模。 |
| [^65] | [Generalised $f$-Mean Aggregation for Graph Neural Networks.](http://arxiv.org/abs/2306.13826) | 本文提出了一个广义聚合算子，GenAgg，它包括所有标准聚合器的函数空间。实验结果表明，GenAgg能够表示标准聚合器。 |
| [^66] | [Upscaling Global Hourly GPP with Temporal Fusion Transformer (TFT).](http://arxiv.org/abs/2306.13815) | 本研究使用时间融合变换器（TFT）解决了过去 GPP 时间序列不足的缺陷，实现了非植被特征在升尺度 GPP 估计中的应用，并取得了良好的模型性能。 |
| [^67] | [BatchGNN: Efficient CPU-Based Distributed GNN Training on Very Large Graphs.](http://arxiv.org/abs/2306.13814) | BatchGNN是一个高效的分布式CPU系统，用于在超大规模图上训练GNN模型，通过宏批处理、集成的图分区和原生的GNN层实现以及缓存聚合的输入特征等技术，实现了高效的训练，并在多个指标上超越了其他分布式GPU系统。 |
| [^68] | [Maintaining Plasticity in Deep Continual Learning.](http://arxiv.org/abs/2306.13812) | 持续性学习中，深度学习系统可能会失去适应新数据的能力，我们提出了一种名为对比可塑性的方法来解决这个问题。 |
| [^69] | [Elephants and Algorithms: A Review of the Current and Future Role of AI in Elephant Monitoring.](http://arxiv.org/abs/2306.13803) | 本文探讨了AI和ML在保护非洲保护区中至关重要的大象方面的作用。新的AI和ML技术提供了解决方案以处理并提取重要信息。在AI专家和生态研究人员之间的协作下，这些创新技术可以帮助增强野生动物保护，为其他物种设定先例。 |
| [^70] | [Tensor Dirichlet Process Multinomial Mixture Model for Passenger Trajectory Clustering.](http://arxiv.org/abs/2306.13794) | 提出了一种基于张量的狄利克雷过程多项式混合模型（Tensor-DPMM），通过张量保留了多维行程信息的多模式和分层结构，并以统一的一步方式进行乘客轨迹聚类，在自动确定聚类数方面具有优越性。 |
| [^71] | [QNNRepair: Quantized Neural Network Repair.](http://arxiv.org/abs/2306.13793) | QNNRepair 是一种用于修复量化神经网络的方法，通过解决神经元权重参数以修复在神经网络量化过程中导致性能下降的神经元，从而在不影响通过测试的性能的同时，提高在失败测试上的性能。 |
| [^72] | [Deconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models.](http://arxiv.org/abs/2306.13789) | 本文提出了一种针对文本分类模型的数据重构攻击，称为Mix And Match攻击，该攻击利用了分类模型基于LLM的特点，该攻击已被证明在随机和有机测试数据集上是有效的。 |
| [^73] | [A new approach to generalisation error of machine learning algorithms: Estimates and convergence.](http://arxiv.org/abs/2306.13784) | 本文提出了一种新的机器学习算法泛化误差的估计方法和收敛性分析，可以在不需要神经网络的任何假设下对误差进行估计，并只要求神经网络具有适当的逼近能力就可以将近似转化为目标函数f。 |
| [^74] | [Swin-Free: Achieving Better Cross-Window Attention and Efficiency with Size-varying Window.](http://arxiv.org/abs/2306.13776) | Swin-Free是一个Transformer模型的变体，通过变化大小的窗口实现了更好的跨窗口注意力和效率。与原始模型相比，在推理时速度更快、准确率更高。 |
| [^75] | [Nearest Neighbour with Bandit Feedback.](http://arxiv.org/abs/2306.13773) | 本论文提出一种新的最近邻算法，能够应用于上下文Bandit问题并处理完全对抗的设置，具有高效运行、快速搜索和准线性空间的优点。 |
| [^76] | [Meta-Path-based Probabilistic Soft Logic for Drug-Target Interaction Prediction.](http://arxiv.org/abs/2306.13770) | 本文提出了一种基于元路径和概率软逻辑的药物-靶点相互作用预测方法，可以更好地利用各类相似性丰富信息和拓扑信息，同时实现较快的预测速度。 |
| [^77] | [Functional-Group-Based Diffusion for Pocket-Specific Molecule Generation and Elaboration.](http://arxiv.org/abs/2306.13769) | 提出了一种基于功能团的扩散模型D3FG，用于口袋特异性分子生成和扩展。基于刚性体的功能团和质点的连接器可以共同形成增强配体-蛋白质相互作用的复杂片段，能够生成高质量的分子。 |
| [^78] | [CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation.](http://arxiv.org/abs/2306.13761) | 本文提出了一个称为CeBed的测试平台，用于评估和比较不同的数据驱动OFDM信道估计方法，解决了领域内实验条件不一致和缺乏可重复性的问题。 |
| [^79] | [Incremental Profit per Conversion: a Response Transformation for Uplift Modeling in E-Commerce Promotions.](http://arxiv.org/abs/2306.13759) | 本文提出了一种新的促销活动单位经济效率的增值度量方法IPC，通过响应转化方法解决响应相关成本增值模型中训练多个模型或计算复杂的问题。该方法只需转换过的数据、其倾向性和一个估计模型，可以提供准确的促销活动获利估计，是提高电子商务平台促销效率的实用工具。 |
| [^80] | [Four Axiomatic Characterizations of the Integrated Gradients Attribution Method.](http://arxiv.org/abs/2306.13753) | 本文提出了四个公理化表征，确定了集成梯度方法在符合不同公理集的归因方法类别中是唯一的。 |
| [^81] | [Analyzing scRNA-seq data by CCP-assisted UMAP and t-SNE.](http://arxiv.org/abs/2306.13750) | 本研究利用CCP辅助UMAP和t-SNE方法可视化scRNA-seq数据，解决了细胞类型聚类准确性和计算效率问题，是一种适用于众多scRNA-seq应用的预处理工具。 |
| [^82] | [Valid inference after prediction.](http://arxiv.org/abs/2306.13746) | 最近的研究聚焦于基于预测的推断，并提出了修正步骤以实现对未观测到响应和协变量之间关系的有效推断，Angelopoulos等人（2023）的方法成功控制了第一类错误率，并提供了正确命名覆盖的置信区间，但在某些情况下，其存在低功率问题。 |
| [^83] | [Multi-Target Multiplicity: Flexibility and Fairness in Target Specification under Resource Constraints.](http://arxiv.org/abs/2306.13738) | 该研究提出了一个新的概念和计算框架，称之为多目标多样性，用于评估目标选择对过程中灵活性和公平性的影响。研究还开发了一种算法，旨在在资源约束下平衡灵活性和公正性，并将其应用于两个真实世界的数据集中。 |
| [^84] | [Combining Public Human Activity Recognition Datasets to Mitigate Labeled Data Scarcity.](http://arxiv.org/abs/2306.13735) | 本文研究如何组合公共数据集以应对HAR领域的标注数据稀缺问题，并探讨用预训练模型来提高HAR模型泛化性能的可行性，结果表明采用仔细选择和组合的公共数据集可达到与使用领域特定数据集相当的结果，这是一种有前途的解决标注数据稀缺问题的方法。 |
| [^85] | [Review of compressed embedding layers and their applications for recommender systems.](http://arxiv.org/abs/2306.13724) | 论文综述了可训练的、压缩的嵌入层在压缩大型神经网络推荐系统中的应用，并提供了相关实验结果。 |
| [^86] | [Exploring the Potential of AI-Generated Synthetic Datasets: A Case Study on Telematics Data with ChatGPT.](http://arxiv.org/abs/2306.13700) | 本研究探讨了使用OpenAI的ChatGPT构建合成数据集的方法，并通过实践案例展示了其实现过程。结果表明，AI生成的合成数据集在解决数据隐私和稀缺性方面具有巨大的潜力。 |
| [^87] | [Curvature-enhanced Graph Convolutional Network for Biomolecular Interaction Prediction.](http://arxiv.org/abs/2306.13699) | 本文提出了一种基于曲率增强的图卷积神经网络，用于生物分子相互作用的预测，并在14个真实的生物相互作用网络上进行了广泛的验证。通过对局部结构引入几何信息，并在消息传递过程中使用Ollivier-Ricci曲率（ORC）作为权重，该模型在13个真实数据集中取得了最好的结果。 |
| [^88] | [Phase Unwrapping of Color Doppler Echocardiography using Deep Learning.](http://arxiv.org/abs/2306.13695) | 本文提出了一种基于展开型原始-对偶网络的新方法，用于纠正彩色多普勒心脏超声图像中的相位包裹伪影，与其他最新分割技术相比，该方法在性能上表现突出。 |
| [^89] | [Prediction of Deep Ice Layer Thickness Using Adaptive Recurrent Graph Neural Networks.](http://arxiv.org/abs/2306.13690) | 本文提出了一种使用自适应循环图神经网络模型来预测深层冰层厚度的方法，该模型表现更好且更加一致。 |
| [^90] | [Broadening the perspective for sustainable AI: Comprehensive sustainability criteria and indicators for AI systems.](http://arxiv.org/abs/2306.13686) | 本文提出了SCAIS框架，包含一组19个可持续性标准和67个指标，旨在促进和结构化关于可持续人工智能的讨论。这种跨学科方法为实现人工智能系统的可持续发展提供了基础。 |
| [^91] | [Evaluating the overall sensitivity of saliency-based explanation methods.](http://arxiv.org/abs/2306.13682) | 本文研究了如何生成对“黑匣子”深度学习模型的忠实解释，提出了一个扩展的测试方法来确定解释方法的总体敏感性，并通过例子展示了如何使用这个方法比较卷积神经网络的多个解释方法。 |
| [^92] | [Estimating the Value of Evidence-Based Decision Making.](http://arxiv.org/abs/2306.13681) | 本文提出了一个实证框架，用于估算证据决策的价值和统计精度投资回报。 |
| [^93] | [GPT-Based Models Meet Simulation: How to Efficiently Use Large-Scale Pre-Trained Language Models Across Simulation Tasks.](http://arxiv.org/abs/2306.13679) | 本文是关于大规模预训练语言模型在科学仿真中的研究。研究关注四个建模和仿真任务的LLMs的预期益处和限制，并提供实际指导。 |
| [^94] | [Intersectionality and Testimonial Injustice in Medical Records.](http://arxiv.org/abs/2306.13675) | 本研究探讨了使用交叉性来检测医疗记录中的证言不公正，进一步提醒人们单一人口统计因素无法充分涵盖病人经历的微妙身份，而忽略这些不公正可能导致医疗质量差或危及生命。 |
| [^95] | [MeciFace: Mechanomyography and Inertial Fusion based Glasses for Edge Real-Time Recognition of Facial and Eating Activities.](http://arxiv.org/abs/2306.13674) | MeciFace是一款注重隐私且低功耗的可穿戴设备，它采用轻量级卷积神经网络来监测面部表情和进食活动，面部表情案例的F1分数达到了86％，饮食监测则达到了90％的F1分数。 |
| [^96] | [Taming the Exponential Action Set: Sublinear Regret and Fast Convergence to Nash Equilibrium in Online Congestion Games.](http://arxiv.org/abs/2306.13673) | 本文提出了一种名为CongestEXP的分散算法，可以线性缩放设施数量，实现在线拥塞博弈的次线性遗憾上界，并以与已知最佳下界相匹配的速度快速收敛到纳什均衡。 |
| [^97] | [Best Practices for Machine Learning Systems: An Industrial Framework for Analysis and Optimization.](http://arxiv.org/abs/2306.13662) | 该论文提出了一个用于分析和优化机器学习系统最佳实践的工业框架。该框架包含五个步骤，能够提高机器学习系统的质量。 |
| [^98] | [FPGA Implementation of Convolutional Neural Network for Real-Time Handwriting Recognition.](http://arxiv.org/abs/2306.13557) | 该论文提出了一种在FPGA器件上实现卷积神经网络的实时手写体识别系统，能提高对手写字母和数字的识别精度。 |
| [^99] | [Pruning for Better Domain Generalizability.](http://arxiv.org/abs/2306.13237) | 本文研究了基于剪枝的域泛化方法，提出了一种新的剪枝评分方法DSS，该方法不是为了保持源准确性，而是直接增强模型的鲁棒性。实验证明该方法可以与最先进的泛化方法结合使用，即便只引入少量稀疏也能显著提高模型性能。 |
| [^100] | [ovla: Neural Network Ownership Verification using Latent Watermarks.](http://arxiv.org/abs/2306.13215) | ovla是一种使用隐式水印进行神经网络所有权验证的新方法，通过训练网络使水印保持休眠状态，只有在所有者的秘密密钥被应用时才会被激活，该方法在神经网络所有权验证准确率、误报率和抵抗对抗攻击方面优于现有的最先进方法。 |
| [^101] | [Can Differentiable Decision Trees Learn Interpretable Reward Functions?.](http://arxiv.org/abs/2306.13004) | 本文提出了使用可微分决策树从人类偏好中学习具有表达能力和可解释性的奖励函数，通过在多个环境上的评估，发现该方法能够学习到可解释的奖励函数，但在强化学习测试时表现受到树的离散性的影响。 |
| [^102] | [Communication-Efficient Federated Learning through Importance Sampling.](http://arxiv.org/abs/2306.12625) | 本文提出了一种通过重要性抽样实现有效通信的联邦学习方法，大大降低了发送模型更新的高通信成本，利用服务器端客户端分布和附加信息的接近关系，只需要较少的通信量即可实现。 |
| [^103] | [Neural Multigrid Memory For Computational Fluid Dynamics.](http://arxiv.org/abs/2306.12545) | 本文提出了一种新的数据驱动湍流流动模拟方法MGxTransformer，结合了VPTR和多重网格架构的优点，使得模拟结果更为准确和高效。 |
| [^104] | [Split Learning in 6G Edge Networks.](http://arxiv.org/abs/2306.12194) | Split learning (SL) enables servers to handle the major training workload while still enhancing data privacy, which is an important approach in 6G edge networks. This article provides an overview of the tailored 6G architecture to support edge SL, the critical design issues for edge SL, and presents future research direction and exciting applications of SL in 6G edge networks. |
| [^105] | [An Overview of Catastrophic AI Risks.](http://arxiv.org/abs/2306.12001) | 本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。 |
| [^106] | [Structure-Aware Robustness Certificates for Graph Classification.](http://arxiv.org/abs/2306.11915) | 该论文提出了一种新的随机平滑方法，可以根据图的不同预定义结构生成对应的鲁棒性证书，从而在多个基准图分类任务中取得领先的对抗性攻击下鲁棒性结果。 |
| [^107] | [Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations.](http://arxiv.org/abs/2306.11839) | 本文提出了针对于异质种群有害实验的早期停止方法CLASH，使用因果机器学习可以有效提前停止临床试验和A/B测试。 |
| [^108] | [Informed POMDP: Leveraging Additional Information in Model-Based RL.](http://arxiv.org/abs/2306.11488) | 本文提出了Informed POMDP，这是一种新的学习范式，通过学习环境模型来利用训练时可用的额外信息，该模型可以提高Dreamer算法策略的收敛速度。 |
| [^109] | [LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation.](http://arxiv.org/abs/2306.11222) | LoSparse 是一种新的语言模型压缩技术，通过低秩矩阵和稀疏矩阵逼近权重矩阵，结合了低秩逼近和裁剪的优点，可以显著降低语言模型大小和复杂度，并在多个自然语言任务中表现优异。 |
| [^110] | [Learn to Accumulate Evidence from All Training Samples: Theory and Practice.](http://arxiv.org/abs/2306.11113) | 本文提出了一种新的激活函数，All-Positive (AP)激活，避免了现有证据激活函数中的零证据区域，同时能够更好地表达负证据量。实验证明，该方法在多个基准数据集上优于现有方法。 |
| [^111] | [Neural Priming for Sample-Efficient Adaptation.](http://arxiv.org/abs/2306.10191) | 本文提出神经启动技术，用于使大型预训练模型适应于分布变化和下游任务，无需过多标记样本。通过回忆数据进行轻量更新可以显著提高准确性。 |
| [^112] | [Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models.](http://arxiv.org/abs/2306.09869) | 本论文提出了能量交叉注意力的EBM框架，通过更新和转移上下文向量，隐式最小化能量函数的嵌套层次，优化文本到图像扩散模型的语义对齐问题，实现零样本组合生成。 |
| [^113] | [Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima.](http://arxiv.org/abs/2306.09850) | 该研究揭示了实用的锐度感知优化算法在某些情况下不能够全程向最优点收敛。 |
| [^114] | [Sound Demixing Challenge 2023 -- Music Demixing Track Technical Report.](http://arxiv.org/abs/2306.09382) | 本文介绍了2023年声音分离挑战赛音乐分离赛道的两个有效方法，分别是时效高的源分离网络和适用于噪声鲁棒性源分离的损失掩模方法。 |
| [^115] | [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting.](http://arxiv.org/abs/2306.09364) | TSMixer是一种用于多元时间序列预测的轻量级MLP-Mixer模型，可以有效地捕捉时间序列属性并在准确性方面超越了Transformers的方法。 |
| [^116] | [DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data.](http://arxiv.org/abs/2306.09344) | 本文提出了一种全面评估图像的感知度量DreamSim，该度量使用合成数据学习人类视觉相似性的新维度，表现出优越性能。 |
| [^117] | [Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models.](http://arxiv.org/abs/2306.08997) | 通过使用大型语言模型，翻译了MIT数学和EECS课程中的4550个题目，开发出一个可以自动评分的模型，并探索了课程、问题和答案之间的关系。 |
| [^118] | [MOFI: Learning Image Representations from Noisy Entity Annotated Images.](http://arxiv.org/abs/2306.07952) | MOFI 提出了一种新的方法，自动从含噪图像文本对中为图像指定实体标签，创建了一个新的大规模数据集 I2E，通过研究不同的训练配方，学习到了能够有效学习图像表示的模型。 |
| [^119] | [Learning under Selective Labels with Heterogeneous Decision-makers: An Instrumental Variable Approach.](http://arxiv.org/abs/2306.07566) | 本文提出了一种处理选择性标记数据的学习问题的方法。通过利用历史决策由一组异质决策者做出的事实，我们建立了一种有原理的工具变量框架，并提出了一种加权学习方法，用于学习预测规则。 |
| [^120] | [Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural Surface Rendering.](http://arxiv.org/abs/2306.07392) | 通过神经表面渲染，NeuGraspNet能够在混乱场景中有效地从任意视角预测6DoF抓取质量，并能够在遮挡的场景中采样抓取候选项。 |
| [^121] | [A Differential Testing Framework to Evaluate Image Recognition Model Robustness.](http://arxiv.org/abs/2306.06208) | 本文提出了一种差分测试框架，用于评估图像识别模型鲁棒性。该框架通过使用一组参考图像并扰动计算环境，可确定模型性能是否受到计算环境变化的影响。 |
| [^122] | [NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics.](http://arxiv.org/abs/2306.06202) | 本文介绍了神经成像领域的图机器学习基准测试NeuroGraph，并探讨了数据集生成的搜索空间。 |
| [^123] | [Unsupervised Cross-Domain Soft Sensor Modelling via A Deep Bayesian Particle Flow Framework.](http://arxiv.org/abs/2306.04919) | 该论文提出了一种基于深度粒子流贝叶斯框架的跨领域软测量无监督建模方法，能够解决标签缺失、领域适应性和数据时间一致性等问题，提高软测量建模的准确性。 |
| [^124] | [ChatGPT Informed Graph Neural Network for Stock Movement Prediction.](http://arxiv.org/abs/2306.03763) | 该研究介绍了一种新的框架，利用ChatGPT技术增强图神经网络，能够从财经新闻中提取出不断变化的网络结构，并用于股票价格预测，获得了超过基于深度学习的最新基准的表现，提示了ChatGPT在文本推断和金融预测方面的潜力。 |
| [^125] | [Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning.](http://arxiv.org/abs/2306.03013) | 该研究提出了一个新的恶意服务器攻击框架SEER，可以在联邦学习中窃取用户数据，而且不易被客户端侦测出来。 |
| [^126] | [Hyperparameter Learning under Data Poisoning: Analysis of the Influence of Regularization via Multiobjective Bilevel Optimization.](http://arxiv.org/abs/2306.01613) | 本文提出了一种考虑攻击对超参数影响的最优攻击公式，将攻击建模为多目标双层优化问题，可以更准确地评估算法鲁棒性和学习超参数，在多个数据集上的评估证明了这种方法的优势。 |
| [^127] | [Balanced Training of Energy-Based Models with Adaptive Flow Sampling.](http://arxiv.org/abs/2306.00684) | 本文研究了能量基模型的训练算法，使用归一化流进行采样，提高了模型的统计精度和生成性能。 |
| [^128] | [Machine Learning Approach for Cancer Entities Association and Classification.](http://arxiv.org/abs/2306.00013) | 本研究利用命名实体识别和文本分类的机器学习方法自动从大量生物医学文献中提取癌症相关实体和实体间关系，以帮助推进癌症研究进展。 |
| [^129] | [Low-rank extended Kalman filtering for online learning of neural networks from streaming data.](http://arxiv.org/abs/2305.19535) | 本文提出一种基于低秩扩展卡尔曼滤波的高效在线学习算法，其能够估计非线性函数的参数，具有更快的适应性和更快的奖励积累。 |
| [^130] | [Compression with Bayesian Implicit Neural Representations.](http://arxiv.org/abs/2305.19185) | 该论文提出了一种用Bayesian隐式神经表示来压缩数据的方法，通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。 |
| [^131] | [Generalization Bounds for Magnitude-Based Pruning via Sparse Matrix Sketching.](http://arxiv.org/abs/2305.18789) | 本文提出了基于稀疏矩阵草图的幅值剪枝的新颖泛化误差界限方法，其具有强的泛化界限，可以在剪枝和超参数模型泛化误差边界的算法开发方面开辟新的道路。 |
| [^132] | [Task-Equivariant Graph Few-shot Learning.](http://arxiv.org/abs/2305.18758) | 本文提出了一种任务等变图Few-shot学习（TEG）框架，利用图神经网络的等变性质来使模型学习可转移的任务适应策略。该方法在各种Few-shot分类基准上展示了最先进的性能。 |
| [^133] | [Generating Behaviorally Diverse Policies with Latent Diffusion Models.](http://arxiv.org/abs/2305.18738) | 本文使用扩散模型将档案压缩为单个对于策略参数的生成模型，实现了13倍的压缩比率，同时保留了98%的原始回报和89%的原始覆盖率，并允许灵活选择和排序行为。 |
| [^134] | [Networked Time Series Imputation via Position-aware Graph Enhanced Variational Autoencoders.](http://arxiv.org/abs/2305.18612) | 本文提出了一种新颖的基于位置感知图增强变分自编码器方法来处理网络时间序列插补问题。该方法能够处理NTS数据中的图动态和缺失边，从而在合成和真实数据集上实现了优异的表现。 |
| [^135] | [MemeGraphs: Linking Memes to Knowledge Graphs.](http://arxiv.org/abs/2305.18391) | 该论文提出了一种使用场景图和知识图谱结构化表达网络文化表情包的方法，并将其用于分类。结果显示该方法相比使用学习表达式的模型有所改善。 |
| [^136] | [Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach.](http://arxiv.org/abs/2305.18088) | 本研究利用分子对接和机器学习回归方法，筛选出针对 SARS-CoV-2的主要蛋白酶3CL潜在治疗药物。其中，决策树回归（DTR）模型具有改进的统计措施R2和RMSE，有助于识别具有高结合亲和力和有利的结合能的药物。 |
| [^137] | [Language Models are Bounded Pragmatic Speakers.](http://arxiv.org/abs/2305.17760) | 本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。经过人类反馈的强化学习微调的大型语言模型具有概念上类似于 快与慢思考模型的思维模型，而这种思维模型被归因于人类。此研究凸显了采用认知概率建模方法对语言模型的理解、评估和推进的价值。 |
| [^138] | [PFNs Are Flexible Models for Real-World Bayesian Optimization.](http://arxiv.org/abs/2305.17535) | 本文使用灵活的PFN作为BO代理建模，该模型能够允许进一步信息纳入以进行非远视BO。在三种不同的问题上得到了很好的结果。 |
| [^139] | [Representation Transfer Learning via Multiple Pre-trained models for Linear Regression.](http://arxiv.org/abs/2305.16440) | 本文提出了一种基于表示迁移的学习方法，在给定很少数样本的情况下，通过提供一组在可能不同的数据领域上训练的预训练回归模型，来构建目标模型，使用这种方法可以提高模型的样本复杂度。 |
| [^140] | [Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipshitz Restraint.](http://arxiv.org/abs/2305.13599) | 本文提出了一种名为DR的灵活的方法，它通过不对称的学习率来解决由合作博弈引发的退化问题，该方法能够在两个基准测试中显著改善表现。 |
| [^141] | [The defender's perspective on automatic speaker verification: An overview.](http://arxiv.org/abs/2305.12804) | 本文综述了针对欺诈攻击和部分伪造语音等ASV攻击类型的防御方法，填补了现有综述中的空白。 |
| [^142] | [Spatio-temporal Diffusion Point Processes.](http://arxiv.org/abs/2305.12403) | 本文提出了一种新的参数化框架，利用扩散模型来学习复杂的时空联合分布，提出了一个精心设计的时空共同注意力模块，以自适应捕捉事件时间和空间之间的相互依赖性，在时空点过程的可能性和事件预测方面均优于现有方法。 |
| [^143] | [Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation.](http://arxiv.org/abs/2305.06446) | 该论文探讨了多智能体强化学习在情节式马尔可夫决策过程中的协作问题，提出了一种基于值迭代的算法，可以实现异步通信，在保证合作优势的同时降低通信开销。通过提供和证明的算法和复杂度界限，为多智能体强化学习在实际应用中提供理论依据。 |
| [^144] | [Explainable Parallel RCNN with Novel Feature Representation for Time Series Forecasting.](http://arxiv.org/abs/2305.04876) | 本论文提出了一种新的特征表示策略——移位法，将过去数据和未来协变量融合起来进行时间序列预测，并开发了一个并行的深度学习框架，同时提出了一种可解释的方法来解释模型的特征重要性。 |
| [^145] | [Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models.](http://arxiv.org/abs/2305.02279) | 本文提出了一种机器学习范式 Learngene，将积累的知识压缩成更为紧凑的信息片段并继承给后代模型，以便于适应新的环境 |
| [^146] | [Online Platt Scaling with Calibeating.](http://arxiv.org/abs/2305.00070) | 本文提出了一种在线Platt缩放及其校准方法，其理论基础强大，可以处理分布漂移和对抗性结果序列，无需超参数调整，在一系列合成和真实数据集上表现出卓越的性能。 |
| [^147] | [MUDiff: Unified Diffusion for Complete Molecule Generation.](http://arxiv.org/abs/2304.14621) | MUDiff 是一种新的分子数据生成模型，它通过组合离散和连续扩散过程来生成全面的分子表示。使用扩散过程可以捕捉分子过程的概率本质，并探索不同因素对分子结构和性质的影响。我们的模型还包括一个新颖的图形转换器架构，用于去噪扩散过程。 |
| [^148] | [TorchBench: Benchmarking PyTorch with High API Surface Coverage.](http://arxiv.org/abs/2304.14226) | TorchBench是一款新型基准测试套件，可全面表征PyTorch软件栈的性能，指导模型、PyTorch框架和GPU库的性能优化。 |
| [^149] | [Local Energy Distribution Based Hyperparameter Determination for Stochastic Simulated Annealing.](http://arxiv.org/abs/2304.11839) | 本文提出了一种基于局部能量分布的随机模拟退火超参数确定方法，该方法通过中心极限定理估计局部能量的分布，将超参数搜索的时间复杂度从O(n^3)降低到O(1)，在解决最大割问题中的实验中表现良好。 |
| [^150] | [Brain tumor multi classification and segmentation in MRO images using deep learning.](http://arxiv.org/abs/2304.10039) | 本研究提出了基于深度学习的脑肿瘤分类和分割模型，能够准确地将MRI图像中的肿瘤分为四类，并将肿瘤从图像中精确地分割出来，具有潜在的临床应用价值。 |
| [^151] | [Canonical and Noncanonical Hamiltonian Operator Inference.](http://arxiv.org/abs/2304.06262) | 介绍了一种结构保留的模型简化方法，可以用于规范和非规范哈密顿系统，具有收敛性和良好的保守性质。 |
| [^152] | [Model sparsification can simplify machine unlearning.](http://arxiv.org/abs/2304.04934) | 本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。 |
| [^153] | [From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding.](http://arxiv.org/abs/2303.12816) | 本文提出了一个用于实现参数高效的知识图谱嵌入的深度网络，通过增加深度克服因采用低维实体表示而导致的模型精度下降和模型参数减少有限的问题。 |
| [^154] | [Matryoshka Policy Gradient for Entropy-Regularized RL: Convergence and Global Optimality.](http://arxiv.org/abs/2303.12785) | 本文介绍了一种新的策略梯度算法——莫特里卡多策略梯度(MPG)，该算法尤其在最大熵强化学习中表现突出，能够实现一系列策略的训练和学习以达到任务的最优化，具有极高的收敛性和全局最优性。 |
| [^155] | [Make Landscape Flatter in Differentially Private Federated Learning.](http://arxiv.org/abs/2303.11242) | 提出了一个新的差分隐私联邦学习算法DP-FedSAM，它利用梯度扰动生成本地平坦模型，从而提高性能和隐私保证。 |
| [^156] | [Fault Detection via Occupation Kernel Principal Component Analysis.](http://arxiv.org/abs/2303.11138) | 本文提出了一种使用占据核PCA方法进行故障检测的新方法，并且通过数值模拟验证了其有效性。 |
| [^157] | [Finite-Sample Analysis of Learning High-Dimensional Single ReLU Neuron.](http://arxiv.org/abs/2303.02255) | 本文研究了高维单个ReLU神经元的有限样本学习问题，并提供了感知器算法GLM-tron的风险上下界，其中包括特殊情况，为高维ReLU回归问题提供了清晰的刻画。此外，对于对称伯努利数据的ReLU回归，随机梯度下降的过多风险不如GLM-tron。 |
| [^158] | [Heuristic Modularity Maximization Algorithms for Community Detection Rarely Return an Optimal Partition or Anything Similar.](http://arxiv.org/abs/2302.14698) | 通过使用80个真实和随机网络，研究对比了当前8种基于模块化启发式算法和一种精确整数规划方法在社区检测中的优化表现，结果显示平均只有16.9%的启发式算法返回最优划分或相似结果。 |
| [^159] | [SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks.](http://arxiv.org/abs/2302.13939) | 本论文提出了一种称之为SpikeGPT的生成语言模型，使用二进制、事件驱动脉冲激活单元进行训练，克服了SNN训练中的挑战性。该模型可以用于大规模语言生成任务。 |
| [^160] | [normflows: A PyTorch Package for Normalizing Flows.](http://arxiv.org/abs/2302.12014) | normflows是一个用于正则化流的PyTorch包，它允许使用基本分布、流层和神经网络构建正则化流模型，以建模概率分布。它在机器学习领域中应用广泛。 |
| [^161] | [K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs.](http://arxiv.org/abs/2302.11996) | 本文提出了一种名为K-SHAP的算法，来解决多个智能体保持匿名且仅有状态-动作对的情况下学习智能体决策的问题。 |
| [^162] | [Stochastic Generative Flow Networks.](http://arxiv.org/abs/2302.09465) | Stochastic GFlowNets通过学习动态模型来扩展了GFlowNets的适用性，解决了原模型只适用于确定性环境的问题，并在具有随机动态的各种标准基准测试中表现良好，具有显著优势。 |
| [^163] | [Online Instrumental Variable Regression: Regret Analysis and Bandit Feedback.](http://arxiv.org/abs/2302.09357) | 该论文研究了在线学习中内生性问题的解决方法，提出了使用Two-Stage Least Squares方法的在线变体O2SLS来处理内生性，取得了较好的识别率和预测遗憾率。 |
| [^164] | [Approximately Bayes-Optimal Pseudo Label Selection.](http://arxiv.org/abs/2302.08883) | 本文介绍了BPLS，一种用于PLS的贝叶斯框架，通过解析逼近选择标签实例的标准，以避免由过度自信但错误预测的实例选择而导致的确认偏差问题。 |
| [^165] | [In Search for a Generalizable Method for Source Free Domain Adaptation.](http://arxiv.org/abs/2302.06658) | 本研究针对生物声学中的分布变化问题，对现有的无源域自适应（SFDA）方法进行了探究和对比发现，现有方法的通用性不如之前想象的那样好，作者提出的新方法优于现有方法且具有较好的通用性。 |
| [^166] | [Ten Lessons We Have Learned in the New "Sparseland": A Short Handbook for Sparse Neural Network Researchers.](http://arxiv.org/abs/2302.02596) | 稀疏神经网络研究群体需要解决的十个核心问题与解决方案被总结在本文中。 |
| [^167] | [Learning Control-Oriented Dynamical Structure from Data.](http://arxiv.org/abs/2302.02529) | 本文提出从数据中学习控制导向的动态结构的方案，并成功应用于一些非线性动力学系统的稳定轨迹追踪，同时评估了所得到的闭环系统的鲁棒性、可解释性和可扩展性。 |
| [^168] | [Learning Prototype Classifiers for Long-Tailed Recognition.](http://arxiv.org/abs/2302.00491) | 本文介绍了针对长尾识别的原型分类器学习，通过联合学习原型以最小化基于概率分数与原型之间距离的平均交叉熵损失，并通过引入一种新的方法，自适应平衡来自不同类别的损失的重要性，进一步增强了原型分类器，从而实现在几个基准数据集上的竞争性性能。 |
| [^169] | [Bandit Convex Optimisation Revisited: FTRL Achieves $\tilde{O}(t^{1/2})$ Regret.](http://arxiv.org/abs/2302.00358) | 研究发现，在标准FTRL算法中插入使用多个函数评估的内核估计器可以获得一个赌徒凸优化算法，并且能够在对抗性时变凸损失函数下实现$\tilde{O}(t^{1/2})$遗憾。 |
| [^170] | [Identifying Adversarially Attackable and Robust Samples.](http://arxiv.org/abs/2301.12896) | 本文提出了一种深度学习方法，用于检测哪些样本最容易受到对抗性攻击，从而确定哪些样本最不容易受到攻击。实验结果表明，这种检测器在不同的模型结构中具有较好的可移植性和检测性能。 |
| [^171] | [On Heterogeneous Treatment Effects in Heterogeneous Causal Graphs.](http://arxiv.org/abs/2301.12383) | 本文通过推广因果图模型，描述了异质性因果图，提出了一种方法来研究不同调节因素对治疗效果和潜在中介变量的影响，解决了现实生活中高维度场景的挑战，并在真实数据应用中发现了新的异质性治疗效应。 |
| [^172] | [Investigating Labeler Bias in Face Annotation for Machine Learning.](http://arxiv.org/abs/2301.09902) | 面部注释标注者的刻板印象和个人特征会影响其数据标注的公正性，强调需要对整个人工智能培训过程保持高度透明以尽早识别和纠正偏见。 |
| [^173] | [Backdoor Attacks in Peer-to-Peer Federated Learning.](http://arxiv.org/abs/2301.09732) | 本文提出了一种基于点对点联邦学习（P2PFL）的新型后门攻击，利用结构图属性选择恶意节点，实现高攻击成功率，同时保持隐蔽性。同时还评估了这些攻击在多种现实条件下的鲁棒性，并设计了新的防御措施。 |
| [^174] | [Uncertainty Quantification for Local Model Explanations Without Model Access.](http://arxiv.org/abs/2301.05761) | 本论文提出了一种无需访问模型本身，通过自举方法量化不确定性的算法，可用于生成后处理解释和不确定性区间，其具有广泛的应用前景和优越的性能。 |
| [^175] | [Perceive and predict: self-supervised speech representation based loss functions for speech enhancement.](http://arxiv.org/abs/2301.04388) | 本文证明了干净和带噪声语音的特征编码之间的距离对语音质量和可懂度的评估有显著作用，使用此距离作为损失函数在语音增强方面效果优于传统方法。 |
| [^176] | [Deep Statistical Solver for Distribution System State Estimation.](http://arxiv.org/abs/2301.01835) | 该论文提出了基于图神经网络的深度统计求解器（DSS$^2$），应用于配电系统状态估计。DSS$^2$利用超图和节点消息传递方案更新潜在表示，并通过弱监督学习方法进行训练。实验结果证明了DSS$^2$在实际数据集上优于其他方法。 |
| [^177] | [DMOps: Data Management Operation and Recipes.](http://arxiv.org/abs/2301.01228) | DMOps旨在指导工业界优化构建NLP产品数据集，提供基线来简化数据操作。 |
| [^178] | [NISQ-ready community detection based on separation-node identification.](http://arxiv.org/abs/2212.14717) | 本文提出了一种基于 QUBO 的新方法，只需要与节点数量相同的量子比特，并以与输入图的邻接矩阵一样稀疏的 QUBO 矩阵表示，通过分离节点的概念实现了 QUBO 矩阵的显著改进，该方法只需要识别分离节点即可高效识别社区。 |
| [^179] | [Real-Time Neural Light Field on Mobile Devices.](http://arxiv.org/abs/2212.08057) | 本文提出了一种在移动设备上实时运行的高效网络，用于神经渲染。 |
| [^180] | [Deep conv-attention model for diagnosing left bundle branch block from 12-lead electrocardiograms.](http://arxiv.org/abs/2212.04936) | 本研究提出了一种使用深度学习模型的注意力机制，能够从12导联ECG数据中准确检测出LBBB心律不齐，准确率高达99.8%，优于其他最先进的模型。 |
| [^181] | [Variable Decision-Frequency Option Critic.](http://arxiv.org/abs/2212.04407) | 这篇论文提出了一个名为CTCO的框架，其中代理选择选项作为可变持续时间的子策略。这个框架可以以任何所需频率与系统交互，从而提供平滑的动作变化，相比传统RL和时间抽象RL方法，其性能更好。 |
| [^182] | [Clustering with Neural Network and Index.](http://arxiv.org/abs/2212.03853) | 介绍了一种新的带有神经网络和索引的聚类模型CNNI，该模型使用神经网络对数据点进行聚类，实现了第一个能够处理非凸形状数据的参数化聚类模型。 |
| [^183] | [On Large-Scale Multiple Testing Over Networks: An Asymptotic Approach.](http://arxiv.org/abs/2211.16059) | 本文提出了两种针对分布式环境的多重检验方法：比例匹配和贪婪聚合。贪心聚合方法有效地近似了每个节点的最优拒绝区域，且具有计算效率性质。 |
| [^184] | [Inverse Solvability and Security with Applications to Federated Learning.](http://arxiv.org/abs/2211.14115) | 介绍了逆可解性和安全性的概念，以及其在联邦学习中的应用。论文提供了模型示例，展示了如何通过增加用户数量来增加可解性和安全性。 |
| [^185] | [Hyperbolic Sliced-Wasserstein via Geodesic and Horospherical Projections.](http://arxiv.org/abs/2211.10066) | 本文提出了一种基于测地线和水平面投影的双曲切片Wasserstein距离，可用于比较具有基础分层结构的数据中定义的概率分布。 |
| [^186] | [A mixed-categorical correlation kernel for Gaussian process.](http://arxiv.org/abs/2211.08262) | 提出一种新的混合类别相关核的高斯过程代理，相较于其他现有模型在分析和工程问题上表现更好。 |
| [^187] | [CarbonTag: A Browser-Based Method for Approximating Energy Consumption of Online Ads.](http://arxiv.org/abs/2211.00071) | 本研究提出了一种基于浏览器的方法用于计算和减少在线广告的能源消耗，并介绍了不同级别的计算方法和一种内联方法。 |
| [^188] | [GFlowOut: Dropout with Generative Flow Networks.](http://arxiv.org/abs/2210.12928) | GFlowOut是一种使用生成流网络的Dropout方法，可以更好地估计复杂的后验分布和样本相关性，并在几个基准数据集上实现了最先进的性能和良好的校准不确定性估计。 |
| [^189] | [Diffusion Models for Causal Discovery via Topological Ordering.](http://arxiv.org/abs/2210.06201) | 本文提出基于扩散模型的DiffAN拓扑排序算法，用于解决因果发现中的搜索空间优化问题。 |
| [^190] | [Training Debiased Subnetworks with Contrastive Weight Pruning.](http://arxiv.org/abs/2210.05247) | 本文探讨了在存在强假相关的偏置网络中提取最优无偏子网络的问题，并提出了使用对比剪枝权重训练实现去偏置子网络的算法 DCWP，在多个应用中都有良好的效果。 |
| [^191] | [Towards Out-of-Distribution Adversarial Robustness.](http://arxiv.org/abs/2210.03150) | 该论文介绍了一种面向OOD的对抗鲁棒性方法，通过将每个攻击类型视为一个领域，应用风险外推方法实现对各攻击的相似鲁棒性水平，实现了在训练和测试时的更高性能，是对抗鲁棒性研究中的创新贡献。 |
| [^192] | [PathProx: A Proximal Gradient Algorithm for Weight Decay Regularized Deep Neural Networks.](http://arxiv.org/abs/2210.03069) | 本文提出一种用于权值衰减正则化深度神经网络的近端梯度算法 PathProx，它可以更快地收敛到标准权值衰减训练所共享的稀疏解。 |
| [^193] | [RankMe: Assessing the downstream performance of pretrained self-supervised representations by their rank.](http://arxiv.org/abs/2210.02885) | 本文提出了一个简单的无监督准则 RankMe，通过评估有效排名，可以指示学习JE-SSL表示的质量，而无需任何标签。 |
| [^194] | [LL-GNN: Low Latency Graph Neural Networks on FPGAs for High Energy Physics.](http://arxiv.org/abs/2209.14065) | 本文提出了一种新型的基于FPGA的低延迟图神经网络(LL-GNN)架构，针对粒子探测器领域的特殊需求，通过外积矩阵乘法方法、结构化邻接矩阵和列主数据布局等优化措施，实现了亚微秒级别的网络部署，并提供了一种GNN特定的算法-硬件协同设计方法。 |
| [^195] | [Taking a Respite from Representation Learning for Molecular Property Prediction.](http://arxiv.org/abs/2209.13492) | 本研究对一系列分子表征模型进行了系统评估，发现基于固定表征的模型在分子属性预测中具有一定优势，同时也揭示了活性断崖问题。 |
| [^196] | [Simplifying Model-based RL: Learning Representations, Latent-space Models, and Policies with One Objective.](http://arxiv.org/abs/2209.08466) | 提出了一个单一的、具有自洽性的目标，它共同优化了隐空间模型和策略，以实现高回报，从而简化模型为基础的强化学习方法。 |
| [^197] | [R\'{e}nyi Divergence Deep Mutual Learning.](http://arxiv.org/abs/2209.05732) | 本文提出在深度互相学习中使用R\'{e}nyi散度，它能够在不引入大量复杂度的情况下持续提高性能，获得了广泛的实证结果的支持。 |
| [^198] | [Quantifying Aleatoric and Epistemic Uncertainty in Machine Learning: Are Conditional Entropy and Mutual Information Appropriate Measures?.](http://arxiv.org/abs/2209.03302) | 该论文提出了对于机器学习中量化Aleatoric和Epistemic不确定性的条件熵和互信息作为度量方法的批评和质疑，提出了这些度量存在的各种不一致性，并对将总不确定性分解为其Aleatoric和Epistemic成分的加法分解进行了讨论。实验结果证明了这些理论发现的合理性，并提出了当前有关不确定性量化的实践存在的问题。 |
| [^199] | [Nonstationary Continuum-Armed Bandit Strategies for Automated Trading in a Simulated Financial Market.](http://arxiv.org/abs/2208.02901) | 本文提出了一种名为PRBO的新型交易算法，通过使用贝叶斯优化和“赌臂机中的赌臂机”框架实现了自动化交易策略在模拟金融市场中的适应性调整，与参考交易策略PRSH相比，PRBO产生了更高的收益。 |
| [^200] | [A Gradient Smoothed Functional Algorithm with Truncated Cauchy Random Perturbations for Stochastic Optimization.](http://arxiv.org/abs/2208.00290) | 本文提出了一种具有截断柯西随机扰动的随机梯度算法用于非凸目标函数的优化，算法具有稳定性与快速收敛性。 |
| [^201] | [Live in the Moment: Learning Dynamics Model Adapted to Evolving Policy.](http://arxiv.org/abs/2207.12141) | 本文提出了一种名为PDML的学习动力学模型的方法，该方法动态调整历史策略混合分布以适应训练过程中的进化策略，实验结果表明PDML优于最先进的方法，实现了更快的收敛和更好的最终性能。 |
| [^202] | [Error Analysis of Tensor-Train Cross Approximation.](http://arxiv.org/abs/2207.04327) | 本文提供了张量列车交叉逼近的误差分析，给出了精确和有噪声测量的整个张量的准确性保证，并提供了如何在计算成本和逼近准确性之间取得平衡的见解。 |
| [^203] | [Learning to correct spectral methods for simulating turbulent flows.](http://arxiv.org/abs/2207.00556) | 通过机器学习的谱求解器在物理先验知识的指导下可以比传统求解器更准确地模拟流体动力学问题。 |
| [^204] | [Language Models as Knowledge Embeddings.](http://arxiv.org/abs/2206.12617) | 该论文提出了一种使用语言模型来推导知识嵌入的方法LMKE，它旨在提高对丰富的长尾实体的表示能力并解决基于描述的先前方法的问题，实验结果表明该方法在多个基准数据集上实现了最先进的性能。 |
| [^205] | [Exceedance Probability Forecasting via Regression for Significant Wave Height Prediction.](http://arxiv.org/abs/2206.09821) | 本论文提出了一种基于回归的超标概率预测方法，用于预测显著波高，通过利用预测来估计超标概率，取得了更好的效果。 |
| [^206] | [Bounding and Approximating Intersectional Fairness through Marginal Fairness.](http://arxiv.org/abs/2206.05828) | 本文旨在通过统计分析了解边际公平性和交集公平性之间的关系，在一定条件下取得精确关系。在高概率下,通过边际公平性和其他有意义的统计量可以计算出交集公平性的界限。 |
| [^207] | [Intrinsic dimensionality and generalization properties of the $\mathcal{R}$-norm inductive bias.](http://arxiv.org/abs/2206.05317) | 本研究研究了$\mathcal{R}$-范归纳偏差的结构和统计特性，发现即使存在岭函数拟合数据，插值函数也是内在的多变量函数；并且，$\mathcal{R}$-范归纳偏差对于某些学习问题的统计最优泛化不足够。 |
| [^208] | [Predictive Multiplicity in Probabilistic Classification.](http://arxiv.org/abs/2206.01131) | 本文提出了一种概率分类的预测多样性度量框架，用于衡量在一组相似模型上风险评估的变化。 |
| [^209] | [Semi-Supervised Clustering of Sparse Graphs: Crossing the Information-Theoretic Threshold.](http://arxiv.org/abs/2205.11677) | 该论文提出了两种有效的算法来将标签信息与稀疏图结构相结合，解决了基于网络拓扑的聚类在稀疏图上的问题。 |
| [^210] | [Gradient-Based Trajectory Optimization With Learned Dynamics.](http://arxiv.org/abs/2204.04558) | 利用机器学习从数据中学习可微动力学模型，实现机器人进行高度动态和复杂任务的轨迹优化，硬件实验表明学习到的模型可成功执行这些任务。 |
| [^211] | [Neural Q-learning for solving PDEs.](http://arxiv.org/abs/2203.17128) | 本文提出了一种新的利用神经Q-学习算法解决椭圆型PDE数值方法，该算法无网格且具有克服维度灾难的潜力。本文证明了这种方法的有效性，并在单调PDE的情况下得到了极限神经网络收敛于PDE解的证明。 |
| [^212] | [Optimal Learning.](http://arxiv.org/abs/2203.15994) | 本文证明，通过解决一个带有惩罚项的离散过度参数化优化问题，可以找到近乎最优的 $\hat f$。 |
| [^213] | [Private Non-Convex Federated Learning Without a Trusted Server.](http://arxiv.org/abs/2203.06735) | 本文提出了一种用于非凸联邦学习的新型互中心隐私保护方法，并考虑了多种复杂的损失函数，包括满足PPL条件的损失函数和强凸损失函数。 |
| [^214] | [Interpretable Off-Policy Learning via Hyperbox Search.](http://arxiv.org/abs/2203.02473) | 本文提出了一个基于超立方体搜索的可解释离线策略学习算法，可以用合取范式表示，可以灵活逼近任何可测函数。在临床实践中具有重要意义。 |
| [^215] | [Practitioner Motives to Select Hyperparameter Optimization Methods.](http://arxiv.org/abs/2203.01717) | 研究探讨了机器学习从业者选择超参数优化方法的动机，结果表明这基于个人目标和背景因素，调查还给出了优化模型的六个主要目标。 |
| [^216] | [Generative modeling via tensor train sketching.](http://arxiv.org/abs/2202.11788) | 本文提出了一种基于张量列缩影的生成建模方法，可以用少量样本避免高维度带来的计算和样本复杂度困扰。 |
| [^217] | [Accelerating Non-Negative and Bounded-Variable Linear Regression Algorithms with Safe Screening.](http://arxiv.org/abs/2202.07258) | 本文提出了一种通过识别饱和坐标加速解决非负和有界变量线性回归问题的技术，实验结果表明其具有令人信服的加速效果。 |
| [^218] | [Towards Ignoring Backgrounds and Improving Generalization: a Costless DNN Visual Attention Mechanism.](http://arxiv.org/abs/2202.00232) | 本文提出了一种无需额外计算成本的DNN视觉注意机制，名为ISNet，能够忽略图像背景并在COVID-19和结核病检测任务中比多个最先进神经网络具有更好的最小化分类器决策偏差影响的能力。 |
| [^219] | [Unifying Pairwise Interactions in Complex Dynamics.](http://arxiv.org/abs/2201.11941) | 该论文提出了一个关于统一复杂系统中成对交互的统计库，并在多个实际案例中展示了如何同时利用不同的方法来揭示最适合解决一个问题的方法。 |
| [^220] | [DegreEmbed: incorporating entity embedding into logic rule learning for knowledge graph reasoning.](http://arxiv.org/abs/2112.09933) | 本文介绍了一种将实体嵌入和逻辑规则挖掘相结合的新模型DegreEmbed，用于知识图谱推理。实验结果表明，DegreEmbed在链接预测和规则提取方面优于现有方法。 |
| [^221] | [A Survey on Neural-symbolic Learning Systems.](http://arxiv.org/abs/2111.08164) | 本论文调查了神经符号学习系统的进展，挑战、方法、应用和未来方向四个方面。神经符号学习系统将神经系统的学习能力和符号系统的认知能力相结合，具有强大的感知和认知能力。 |
| [^222] | [OpenFWI: Large-Scale Multi-Structural Benchmark Datasets for Seismic Full Waveform Inversion.](http://arxiv.org/abs/2111.02926) | OpenFWI是一个用于地震全波形反演的大规模多结构基准数据集，包含多个领域、不同地质地下结构和各种数据样本数量。使用OpenFWI进行了四种深度学习方法的基准测试和物理驱动的实验。 |
| [^223] | [On the approximation capability of GNNs in node classification/regression tasks.](http://arxiv.org/abs/2106.08992) | 本文研究GNN在节点分类/回归任务中的逼近能力，发现在概率上GNN对于任何有限域上的可测函数都是通用逼近器。我们的研究还揭示了GNN的表达能力与其深度和宽度的变化有关，并且结果可扩展用于大型图。 |
| [^224] | [Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation.](http://arxiv.org/abs/2103.03102) | 本文提出了一种新的基准测试方法和工具，通过双因素扰动来评估深度学习分类器的鲁棒性。使用该方法和工具，作者比较了不同的两因素扰动条件下DL分类器的鲁棒性，并为开发更鲁棒的DL分类器提供了见解。 |
| [^225] | [Augmentation Inside the Network.](http://arxiv.org/abs/2012.10769) | 本文提出了一种将数据增强方法引入卷积神经网络中间层的方法，既使速度-准确性的平衡更平滑，同时在ImageNet和CIFAR-100数据集分类任务上实现更好的结果，特别是在CIFAR-100数据集上的结果可以比标准测试时间增强技术更快30％。 |
| [^226] | [No Need to Know Physics: Resilience of Process-based Model-free Anomaly Detection for Industrial Control Systems.](http://arxiv.org/abs/2012.03586) | 本文对基于过程的工业控制系统异常检测方案进行了分析，并介绍了验证这些检测系统的属性分类法。该研究还提出了一种新的通用框架来生成违反系统物理属性的对抗欺骗信号。在已发表的四个检测器中，只有一个具有鲁棒性，其鲁棒性来自于属性的引入。通过攻击，我们成功降低了攻击方案的召回率。 |
| [^227] | [Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity.](http://arxiv.org/abs/2004.12908) | 本文提出了一种名为RELL的方法，利用知识蒸馏和知识保持正则化方法，以协同集成在不同任务上独立学习的表示，在准线性复杂度下实现了前向和后向传递。实验结果表明，在各种基准数据集上，RELL的表现优于现有的最先进方法，尤其是在存在灾难性遗忘的情况下，能够显着改善反向传递。 |
| [^228] | [Speaker-change Aware CRF for Dialogue Act Classification.](http://arxiv.org/abs/2004.02913) | 本文提出了一种新的CRF模型，称为Speaker-change Aware CRF，以考虑对话行为分类中的说话人变化，实验结果表明其优于先前的方法。 |

# 详细

[^1]: 机器学习和消费者数据

    Machine Learning and Consumer Data. (arXiv:2306.14118v1 [cs.LG])

    [http://arxiv.org/abs/2306.14118](http://arxiv.org/abs/2306.14118)

    数字革命导致了消费者行为的数字化，新兴现象使消费者行为更加复杂，传统的分析方法面临挑战。为了应对这个挑战，机器学习成为了有效解析和处理消费者数据的方法。

    

    数字革命导致人类行为的数字化，创造了前所未有的机会，以了解以前未曾观察到的行动。新兴现象，例如群筹和众包，进一步阐明了消费者行为，同时也引入了新的行为模式。然而，这些数据的数量和复杂性给营销研究人员和从业者带来了重大挑战。传统的分析消费者数据的方法在处理新兴数据来源的广度、精度和规模方面表现不佳。为了解决这个问题，研究人员开发了计算方法来处理与消费者行为相关的“大数据”，这通常包括结构化数据、文本数据、音频数据和视觉数据。这些方法，尤其是机器学习，可以有效地解析和处理多方面的数据。鉴于这些最近的发展，本综述旨在让研究人员和从业者熟悉最新的计算方法，以应对消费数据的挑战。

    The digital revolution has led to the digitization of human behavior, creating unprecedented opportunities to understand observable actions on an unmatched scale. Emerging phenomena such as crowdfunding and crowdsourcing have further illuminated consumer behavior while also introducing new behavioral patterns. However, the sheer volume and complexity of this data present significant challenges for marketing researchers and practitioners. Traditional methods used to analyze consumer data fall short in handling the breadth, precision, and scale of emerging data sources. To address this, computational methods have been developed to manage the "big data" associated with consumer behavior, which typically includes structured data, textual data, audial data, and visual data. These methods, particularly machine learning, allow for effective parsing and processing of multi-faceted data. Given these recent developments, this review article seeks to familiarize researchers and practitioners with
    
[^2]: 朝着可信的解释：因果关系解释论文研究

    Towards Trustworthy Explanation: On Causal Rationalization. (arXiv:2306.14115v1 [cs.LG])

    [http://arxiv.org/abs/2306.14115](http://arxiv.org/abs/2306.14115)

    该论文介绍了一种新的因果关系解释方法，通过在解释中引入非虚假性和效率，从因果推断的角度定义了因果概率，从而建立了必要和充分解释的主要组成部分，相比现有的基于关联的解释方法，这种方法有更加优越的性能表现。

    

    随着自然语言处理的最新进展，解释成为了通过选择输入文本的子集来解释黑盒模型中主要变化的一个基本的自我解释图。然而，现有的基于关联的解释方法在两个或多个片段高度互相关联时无法识别真正的解释，因此对预测准确性提供类似的贡献，所谓的虚假性。为了解决这一限制，我们从因果推断的角度新颖地将两个因果期望值（非虚假性和效率）引入了解释中。我们根据一种新提出的解释结构因果模型定义了一系列的因果概率，通过其理论鉴定，建立了必要和充分解释的主要组成部分。我们在真实世界的评论和医疗数据集上证明了所提出的因果关系解释的优越性能。

    With recent advances in natural language processing, rationalization becomes an essential self-explaining diagram to disentangle the black box by selecting a subset of input texts to account for the major variation in prediction. Yet, existing association-based approaches on rationalization cannot identify true rationales when two or more snippets are highly inter-correlated and thus provide a similar contribution to prediction accuracy, so-called spuriousness. To address this limitation, we novelly leverage two causal desiderata, non-spuriousness and efficiency, into rationalization from the causal inference perspective. We formally define a series of probabilities of causation based on a newly proposed structural causal model of rationalization, with its theoretical identification established as the main component of learning necessary and sufficient rationales. The superior performance of the proposed causal rationalization is demonstrated on real-world review and medical datasets w
    
[^3]: TNPAR: 基于拓扑神经泊松自回归模型的事件序列Granger因果结构学习

    TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning Granger Causal Structure from Event Sequences. (arXiv:2306.14114v1 [cs.LG])

    [http://arxiv.org/abs/2306.14114](http://arxiv.org/abs/2306.14114)

    该论文提出了一种基于拓扑神经泊松自回归模型的方法，同时考虑先验拓扑网络和潜在的Granger因果结构来学习事件序列中的Granger因果关系，该方法通过推断因果关系来解决序列数据中的非 i.i.d. 问题

    

    从事件序列中学习Granger因果关系是各种应用中具有挑战性但又至关重要的任务。大多数现有方法都依赖于事件序列独立同分布 (i.i.d.) 的假设。然而，由于事件序列之间的固有依赖关系，这一 i.i.d. 假设经常被违反。幸运的是，在实践中，我们发现这些依赖关系可以被建模成一个拓扑网络，因此可以通过将先验拓扑网络引入Granger因果发现来解决非 i.i.d. 问题。这一发现促使我们解决两个问题：1) 如何在模型事件序列时同时考虑先验拓扑网络和潜在的Granger因果结构；2) 如何学习Granger因果结构。为此，我们设计了一个两阶段的统一拓扑神经泊松自回归模型。在生成阶段，我们采用神经泊松过程的一种变体来建模事件发生的时刻，并通过拓扑关系和现有事件序列推断因果关系。

    Learning Granger causality from event sequences is a challenging but essential task across various applications. Most existing methods rely on the assumption that event sequences are independent and identically distributed (i.i.d.). However, this i.i.d. assumption is often violated due to the inherent dependencies among the event sequences. Fortunately, in practice, we find these dependencies can be modeled by a topological network, suggesting a potential solution to the non-i.i.d. problem by introducing the prior topological network into Granger causal discovery. This observation prompts us to tackle two ensuing challenges: 1) how to model the event sequences while incorporating both the prior topological network and the latent Granger causal structure, and 2) how to learn the Granger causal structure. To this end, we devise a two-stage unified topological neural Poisson auto-regressive model. During the generation stage, we employ a variant of the neural Poisson process to model the 
    
[^4]: RLHF是否比标准RL更困难？

    Is RLHF More Difficult than Standard RL?. (arXiv:2306.14111v1 [cs.LG])

    [http://arxiv.org/abs/2306.14111](http://arxiv.org/abs/2306.14111)

    本文证明了对于广泛的偏好模型，我们可以使用现有的算法和技术直接解决基于偏好的RL问题，而几乎不需要额外的成本。

    

    从人类反馈学习的强化学习（RLHF）是从偏好信号学习，而标准强化学习（RL）则直接从奖励信号学习。偏好信号可能包含的信息比奖励信号少，这使得基于偏好的RL似乎更加困难。本文理论上证明，对于广泛的偏好模型，我们可以使用现有的算法和技术直接解决基于偏好的RL问题，而几乎不需要额外的成本。具体而言，我们将问题分为两类：（1）基于奖励概率模型的偏好，此时可以将问题简化为容忍奖励小误差的鲁棒奖励RL问题；（2）对于一般的任意偏好且目标是找到von Neumann获胜者的情况，我们将问题简化为多智能体奖励RL问题，该问题可以在一组受限制的策略下找到马尔可夫博弈的因子纳什平衡解。后一种情况可以进一步降低成对关系的MDP。

    Reinforcement learning from Human Feedback (RLHF) learns from preference signals, while standard Reinforcement Learning (RL) directly learns from reward signals. Preferences arguably contain less information than rewards, which makes preference-based RL seemingly more difficult. This paper theoretically proves that, for a wide range of preference models, we can solve preference-based RL directly using existing algorithms and techniques for reward-based RL, with small or no extra costs. Specifically, (1) for preferences that are drawn from reward-based probabilistic models, we reduce the problem to robust reward-based RL that can tolerate small errors in rewards; (2) for general arbitrary preferences where the objective is to find the von Neumann winner, we reduce the problem to multiagent reward-based RL which finds Nash equilibria for factored Markov games under a restricted set of policies. The latter case can be further reduce to adversarial MDP when preferences only depend on the f
    
[^5]: 语言模型是弱学习器

    Language models are weak learners. (arXiv:2306.14101v1 [cs.LG])

    [http://arxiv.org/abs/2306.14101](http://arxiv.org/abs/2306.14101)

    本文证明了，基于提示的大型语言模型可以作为弱学习器应用于表格数据的boosting算法中，并且在某些情况下可以优于传统的树形模型。

    

    实践和理论机器学习中的一个中心概念是弱学习器，即在任何给定的数据分布上都能取得比随机更好的性能的分类器，即使只是略微好一点。这样的弱学习器构成了经典机器学习方法（如boosting）的实用基础。在这项工作中，我们展示了基于提示的大型语言模型可以有效地作为上述弱学习器进行操作。具体而言，我们演示了在表格数据中使用大型语言模型（LLM）作为boosting算法中的弱学习器。我们展示了通过提供（根据感兴趣的分布进行适当采样的）表格数据样本的文本描述，LLM可以产生样本的汇总，作为分类的模板，并在这个任务中实现作为弱学习器的目的。我们将这些模型纳入boosting方法中，在某些环境中，可以利用LLM中的知识，优于传统的树形模型。

    A central notion in practical and theoretical machine learning is that of a $\textit{weak learner}$, classifiers that achieve better-than-random performance (on any given distribution over data), even by a small margin. Such weak learners form the practical basis for canonical machine learning methods such as boosting. In this work, we illustrate that prompt-based large language models can operate effectively as said weak learners. Specifically, we illustrate the use of a large language model (LLM) as a weak learner in a boosting algorithm applied to tabular data. We show that by providing (properly sampled according to the distribution of interest) text descriptions of tabular data samples, LLMs can produce a summary of the samples that serves as a template for classification and achieves the aim of acting as a weak learner on this task. We incorporate these models into a boosting approach, which in some settings can leverage the knowledge within the LLM to outperform traditional tree
    
[^6]: 具有保证最优性的局部差分隐私分布式在线学习

    Locally Differentially Private Distributed Online Learning with Guaranteed Optimality. (arXiv:2306.14094v1 [cs.LG])

    [http://arxiv.org/abs/2306.14094](http://arxiv.org/abs/2306.14094)

    本文提出了一种具有保证最优性的方法，可以在分布式在线学习中同时保证差分隐私和学习准确性。

    

    分布式在线学习由于其处理大规模数据集和流数据的能力而受到越来越多的关注。为了解决隐私保护问题，已经提出了许多个人私密分布式在线学习算法，大多数基于差分隐私，差分隐私已成为隐私保护的“黄金标准”。然而，这些算法常常面临为了隐私保护而牺牲学习准确性的困境。本文利用在线学习的独特特征，提出了一种方法来解决这一困境，并确保分布式在线学习中的差分隐私和学习准确性。具体而言，该方法在确保预期瞬时遗憾程度逐渐减小的同时，还能保证有限的累积隐私预算，即使在无限时间范围内。为了应对完全分布式环境，我们采用本地差分隐私框架，避免了对全局数据的依赖。

    Distributed online learning is gaining increased traction due to its unique ability to process large-scale datasets and streaming data. To address the growing public awareness and concern on privacy protection, plenty of private distributed online learning algorithms have been proposed, mostly based on differential privacy which has emerged as the ``gold standard" for privacy protection. However, these algorithms often face the dilemma of trading learning accuracy for privacy. By exploiting the unique characteristics of online learning, this paper proposes an approach that tackles the dilemma and ensures both differential privacy and learning accuracy in distributed online learning. More specifically, while ensuring a diminishing expected instantaneous regret, the approach can simultaneously ensure a finite cumulative privacy budget, even on the infinite time horizon. To cater for the fully distributed setting, we adopt the local differential-privacy framework which avoids the reliance
    
[^7]: 非同质化集群下的无线联邦学习中的私有数据聚合

    Private Aggregation in Wireless Federated Learning with Heterogeneous Clusters. (arXiv:2306.14088v1 [cs.LG])

    [http://arxiv.org/abs/2306.14088](http://arxiv.org/abs/2306.14088)

    本文探讨了在一个无线系统中，考虑到信息论隐私的条件下，通过基站连接到联合器的客户端，如何解决联邦学习中的隐私数据聚合问题。

    

    联邦学习是通过多个参与客户端私有数据的协同训练神经网络的方法。在训练神经网络的过程中，使用一种著名并广泛使用的迭代优化算法——梯度下降算法。每个客户端使用本地数据计算局部梯度并将其发送给联合器以进行聚合。客户端数据的隐私是一个主要问题。实际上，观察到局部梯度就足以泄露客户端的数据。已研究了用于应对联邦学习中隐私问题的私有聚合方案，其中所有用户都彼此连接并与联合器连接。本文考虑了一个无线系统架构，其中客户端仅通过基站连接到联合器。当需要信息论隐私时，我们推导出通信成本的基本极限，并引入和分析了一种针对这种情况量身定制的私有聚合方案。

    Federated learning collaboratively trains a neural network on privately owned data held by several participating clients. The gradient descent algorithm, a well-known and popular iterative optimization procedure, is run to train the neural network. Every client uses its local data to compute partial gradients and sends it to the federator which aggregates the results. Privacy of the clients' data is a major concern. In fact, observing the partial gradients can be enough to reveal the clients' data. Private aggregation schemes have been investigated to tackle the privacy problem in federated learning where all the users are connected to each other and to the federator. In this paper, we consider a wireless system architecture where clients are only connected to the federator via base stations. We derive fundamental limits on the communication cost when information-theoretic privacy is required, and introduce and analyze a private aggregation scheme tailored for this setting.
    
[^8]: 一种基于电路复杂度的算法信息论复杂性表述

    A Circuit Complexity Formulation of Algorithmic Information Theory. (arXiv:2306.14087v1 [cs.LG])

    [http://arxiv.org/abs/2306.14087](http://arxiv.org/abs/2306.14087)

    该论文提出了一种基于电路复杂度的先验知识，旨在解决学习布尔函数的问题，重点是对简单解释具有归纳偏见。

    

    在Solomonoffs归纳推理理论的启发下，我们提出了一种基于电路复杂度的先验知识。该方法有几个优点。首先，它依赖于一个复杂度衡量方式，不依赖于UTM的选择。真值表现为元件之间的运算而不是程序。其次，不存在停机问题。电路的输出值可以通过递归计算机来计算，时间与电路内门数成正比。我们的先验假设一个布尔函数，或者等效的编码为固定长度的布尔串，是由某个混合模型生成的。这个模型适用于从部分信息中学习布尔函数，通常在机器学习中遇到的“二元分类”问题。主张对简单解释具有归纳偏见。

    Inspired by Solomonoffs theory of inductive inference, we propose a prior based on circuit complexity. There are several advantages to this approach. First, it relies on a complexity measure that does not depend on the choice of UTM. There is one universal definition for Boolean circuits involving an universal operation such as nand with simple conversions to alternative definitions such as and, or, and not. Second, there is no analogue of the halting problem. The output value of a circuit can be calculated recursively by computer in time proportional to the number of gates, while a short program may run for a very long time. Our prior assumes that a Boolean function, or equivalently, Boolean string of fixed length, is generated by some Bayesian mixture of circuits. This model is appropriate for learning Boolean functions from partial information, a problem often encountered within machine learning as "binary classification." We argue that an inductive bias towards simple explanations 
    
[^9]: 利用大脑模块化先验进行fMRI可解释表示学习

    Leveraging Brain Modularity Prior for Interpretable Representation Learning of fMRI. (arXiv:2306.14080v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.14080](http://arxiv.org/abs/2306.14080)

    该论文提出了一种基于大脑模块化先验的动态表示学习框架，具有优秀的预测性能和可解释特征表示，为静息状态下的功能性核磁共振成像(rs-fMRI)的临床应用提供了一种新的工具和思路。

    

    静息状态下的功能性核磁共振成像(rs-fMRI)可以反映大脑的自发神经活动，并被广泛用于大脑疾病分析。以图论为视角，大脑在自发脑功能网络中表现出显著的模块化结构，每个模块由功能相关的感兴趣脑区域(ROIs)组成。然而，大多数现有的fMRI分析方法未能充分利用这种大脑模块化先验。我们提出了一种基于大脑模块化约束的动态表示学习(BMR)框架，其中包括三个主要组成部分：(1)动态图构建，(2)通过新颖的模块化约束图自编码器进行动态图学习和(3)可解释的特征识别。公开的 rs-fMRI 数据集上的实验结果表明，我们的 BMR 框架可以同时实现优秀的预测性能和可解释特征表示，展示了其作为 rs-fMRI 分析的有用工具的潜力。

    Resting-state functional magnetic resonance imaging (rs-fMRI) can reflect spontaneous neural activities in brain and is widely used for brain disorder analysis.Previous studies propose to extract fMRI representations through diverse machine/deep learning methods for subsequent analysis. But the learned features typically lack biological interpretability, which limits their clinical utility. From the view of graph theory, the brain exhibits a remarkable modular structure in spontaneous brain functional networks, with each module comprised of functionally interconnected brain regions-of-interest (ROIs). However, most existing learning-based methods for fMRI analysis fail to adequately utilize such brain modularity prior. In this paper, we propose a Brain Modularity-constrained dynamic Representation learning (BMR) framework for interpretable fMRI analysis, consisting of three major components: (1) dynamic graph construction, (2) dynamic graph learning via a novel modularity-constrained g
    
[^10]: 利用梯度对抗不确定性：通过扩散分数匹配实现离线强化学习

    Fighting Uncertainty with Gradients: Offline Reinforcement Learning via Diffusion Score Matching. (arXiv:2306.14079v1 [cs.LG])

    [http://arxiv.org/abs/2306.14079](http://arxiv.org/abs/2306.14079)

    本文提出了平滑的距离数据度量标准，并将其与离线强化学习相结合，以对抗不确定性和分布偏移的挑战。该方法不仅在最小化梯度不确定性时稳定收敛到数据，而且不易低估真实不确定性，是一种有前途的策略搜索方法。

    

    离线优化范式，例如离线强化学习（RL）或模仿学习（IL），允许策略搜索算法利用离线数据，但需要仔细处理不确定性以避免分布偏移的挑战。由于其在高维度中的有效性，基于梯度的策略搜索方法是一种有前途的方向；然而，我们需要更仔细地考虑这些方法如何与不确定性估计相互影响。我们声称，为了让不确定性度量适用于基于梯度的优化，它必须在最小化梯度不确定性时稳定地收敛到数据，并且不易低估真实不确定性。我们研究了平滑的数据距离作为度量标准，并展示了它不仅稳定地收敛到数据，而且还允许我们通过Lipschitz常数来分析模型偏差。此外，我们建立了平滑的数据距离和数据似然之间的等价性。

    Offline optimization paradigms such as offline Reinforcement Learning (RL) or Imitation Learning (IL) allow policy search algorithms to make use of offline data, but require careful incorporation of uncertainty in order to circumvent the challenges of distribution shift. Gradient-based policy search methods are a promising direction due to their effectiveness in high dimensions; however, we require a more careful consideration of how these methods interplay with uncertainty estimation. We claim that in order for an uncertainty metric to be amenable for gradient-based optimization, it must be (i) stably convergent to data when uncertainty is minimized with gradients, and (ii) not prone to underestimation of true uncertainty. We investigate smoothed distance to data as a metric, and show that it not only stably converges to data, but also allows us to analyze model bias with Lipschitz constants. Moreover, we establish an equivalence between smoothed distance to data and data likelihood, 
    
[^11]: 无强度卷积时空点过程: 融合局部与全局事件语境

    Intensity-free Convolutional Temporal Point Process: Incorporating Local and Global Event Contexts. (arXiv:2306.14072v1 [cs.LG])

    [http://arxiv.org/abs/2306.14072](http://arxiv.org/abs/2306.14072)

    本文提出了一种TPP建模方法，将连续时间卷积事件编码器与RNN集成，以融合局部和全局上下文。实验结果表明，该模型具有较好的性能表现。

    

    连续时间领域的事件预测是一项至关重要但相当困难的任务。时间点过程(TPP)学习模型在这个领域中表现出了巨大的优势。现有的模型主要集中于使用像循环神经网络(RNN)或自我注意机制之类的技术来编码事件的全局上下文。但是，局部事件上下文对事件的发生也起着重要作用，但这方面却很少被关注。流行的卷积神经网络专为捕获局部上下文而设计，但由于无法在连续时间模型化，因此从未应用于TPP建模。本研究提出了一种将局部和全局上下文相结合的新型TPP建模方法，即将连续时间卷积事件编码器与RNN集成。所提出的框架具有灵活性和可扩展性，可以处理具有长序列和复杂潜在模式的大型数据集。实验结果表明，所提出的模型改善了性能。

    Event prediction in the continuous-time domain is a crucial but rather difficult task. Temporal point process (TPP) learning models have shown great advantages in this area. Existing models mainly focus on encoding global contexts of events using techniques like recurrent neural networks (RNNs) or self-attention mechanisms. However, local event contexts also play an important role in the occurrences of events, which has been largely ignored. Popular convolutional neural networks, which are designated for local context capturing, have never been applied to TPP modelling due to their incapability of modelling in continuous time. In this work, we propose a novel TPP modelling approach that combines local and global contexts by integrating a continuous-time convolutional event encoder with an RNN. The presented framework is flexible and scalable to handle large datasets with long sequences and complex latent patterns. The experimental result shows that the proposed model improves the perfo
    
[^12]: Waypoint Transformer: 通过中间目标的监督学习进行强化学习

    Waypoint Transformer: Reinforcement Learning via Supervised Learning with Intermediate Targets. (arXiv:2306.14069v1 [cs.LG])

    [http://arxiv.org/abs/2306.14069](http://arxiv.org/abs/2306.14069)

    Waypoint Transformer提出了一种改进RL的新方法，通过整合中间目标来实现，极大地提高了性能和稳定性，尤其在最具挑战性的环境和数据配置中表现得更加优秀。

    

    尽管最近通过监督学习进行的离线强化学习取得了很多进展，以决策转换器（DT）架构在各个领域的成功而言，但在几个具有挑战性的基准测试中，DT还是表现不佳。这一低性能的根本原因在于它们无法无缝连接亚优化轨迹的片段。为了克服这个限制，我们提出了一种增强RvS方法的新方法，即通过整合中间目标来实现。我们引入Waypoint Transformer（WT），使用一种基于DT框架的架构，并且是通过自动生成的路径点进行条件化的。结果表明，与现有的RvS方法相比，最终回报显著增加，并且在性能上与现有的基于时间差分学习的最新方法相当或更优。此外，性能和稳定性的改进最大的是在最具挑战性的环境和数据配置中，包括AntMaze Large Play/Diverse。

    Despite the recent advancements in offline reinforcement learning via supervised learning (RvS) and the success of the decision transformer (DT) architecture in various domains, DTs have fallen short in several challenging benchmarks. The root cause of this underperformance lies in their inability to seamlessly connect segments of suboptimal trajectories. To overcome this limitation, we present a novel approach to enhance RvS methods by integrating intermediate targets. We introduce the Waypoint Transformer (WT), using an architecture that builds upon the DT framework and conditioned on automatically-generated waypoints. The results show a significant increase in the final return compared to existing RvS methods, with performance on par or greater than existing state-of-the-art temporal difference learning-based methods. Additionally, the performance and stability improvements are largest in the most challenging environments and data configurations, including AntMaze Large Play/Diverse
    
[^13]: SEEDS：利用扩散模型仿真天气预测集合

    SEEDS: Emulation of Weather Forecast Ensembles with Diffusion Models. (arXiv:2306.14066v1 [cs.LG])

    [http://arxiv.org/abs/2306.14066](http://arxiv.org/abs/2306.14066)

    本文提出了利用生成人工智能技术在规模上生成集合预测的方法，并产生了与完整的GEFS 31成员集合相似的预测能力，并且很好地模拟了大规模集合的统计数据。

    

    在不确定未来天气时，概率预测对决策非常重要。主要的方法是使用预测集合来表示和量化数值天气预报的不确定性。然而，产生集合的计算成本很高。本文提出利用最近的生成人工智能技术在规模上生成集合预测的方法。我们的方法从5成员集合GEFS重新预报数据集中学习基于数据驱动的概率扩散模型。该模型可以有效地进行采样，以产生联合情况下真实的天气预测，这些情况可以基于操作GEFS预测系统的少数成员条件化。根据ERA5分析评估，生成的集合与完整的GEFS 31成员集合具有相似的预测能力，并且很好地模拟了大规模集合的统计数据。我们还将相同的方法应用于开发扩散模型，进行生成后处理。模型可以基于少数预测成员条件化地生成类似于物理大模型集合的预测集合。

    Probabilistic forecasting is crucial to decision-making under uncertainty about future weather. The dominant approach is to use an ensemble of forecasts to represent and quantify uncertainty in operational numerical weather prediction. However, generating ensembles is computationally costly. In this paper, we propose to generate ensemble forecasts at scale by leveraging recent advances in generative artificial intelligence. Our approach learns a data-driven probabilistic diffusion model from the 5-member ensemble GEFS reforecast dataset. The model can then be sampled efficiently to produce realistic weather forecasts, conditioned on a few members of the operational GEFS forecasting system. The generated ensembles have similar predictive skill as the full GEFS 31-member ensemble, evaluated against ERA5 reanalysis, and emulate well the statistics of large physics-based ensembles. We also apply the same methodology to developing a diffusion model for generative post-processing: the model 
    
[^14]: 超越双曲模型：在对称正定矩阵中建模图神经网络

    Modeling Graphs Beyond Hyperbolic: Graph Neural Networks in Symmetric Positive Definite Matrices. (arXiv:2306.14064v1 [cs.LG])

    [http://arxiv.org/abs/2306.14064](http://arxiv.org/abs/2306.14064)

    本文构建了一个基于对称正定矩阵的图神经网络，可以很好地处理复杂图形数据。

    

    最近的研究表明，图数据的结构与嵌入空间的几何结构之间的对齐对于学习高质量的数据表示至关重要。欧氏空间和双曲空间的均匀几何允许使用最小的扭曲来表示具有均匀几何和拓扑特征，如网格和层次结构的图形。然而，现实世界的图形数据具有多种类型的几何和拓扑特征，需要更复杂的几何嵌入空间。在这项工作中，我们利用对称正定矩阵（SPD）的黎曼对称空间来构建图神经网络，以可靠地处理复杂的图形。为此，我们开发了一个创新的库，利用SPD gyrospace计算工具\cite{lopez2021gyroSPD}实现了在SPD中五个流行图神经网络的构建模块。实验结果表明，我们在SPD中的图神经网络显著优于其他方法。

    Recent research has shown that alignment between the structure of graph data and the geometry of an embedding space is crucial for learning high-quality representations of the data. The uniform geometry of Euclidean and hyperbolic spaces allows for representing graphs with uniform geometric and topological features, such as grids and hierarchies, with minimal distortion. However, real-world graph data is characterized by multiple types of geometric and topological features, necessitating more sophisticated geometric embedding spaces. In this work, we utilize the Riemannian symmetric space of symmetric positive definite matrices (SPD) to construct graph neural networks that can robustly handle complex graphs. To do this, we develop an innovative library that leverages the SPD gyrocalculus tools \cite{lopez2021gyroSPD} to implement the building blocks of five popular graph neural networks in SPD. Experimental results demonstrate that our graph neural networks in SPD substantially outperf
    
[^15]: 自适应采集数据的离线强化学习策略评估

    Offline Policy Evaluation for Reinforcement Learning with Adaptively Collected Data. (arXiv:2306.14063v1 [cs.LG])

    [http://arxiv.org/abs/2306.14063](http://arxiv.org/abs/2306.14063)

    本论文提出了一种自适应采集数据的离线强化学习策略评估方法，为表格MDPs推导出高概率、实例相关的误差边界，并实现了自适应设置下的极小值最优离线学习。

    

    发展离线RL方法样本复杂度的理论保证是实现数据需求量较大的RL算法实际可行的重要步骤。目前，大多数结果依赖于关于数据分布的不现实的假设，即包括一个由单一记录策略收集的i.i.d.轨迹集。我们考虑一个更一般的设置，即数据集可以是自适应收集的。我们为表格MDPs中的TMIS离线策略评估（OPE）估计器在这个广义设置中开发理论，推导其估计误差的高概率、实例相关边界。我们还回收了自适应设置下的极小值最优离线学习。最后，我们进行模拟，以经验分析这些估计器在自适应和非自适应模式下的行为。

    Developing theoretical guarantees on the sample complexity of offline RL methods is an important step towards making data-hungry RL algorithms practically viable. Currently, most results hinge on unrealistic assumptions about the data distribution -- namely that it comprises a set of i.i.d. trajectories collected by a single logging policy. We consider a more general setting where the dataset may have been gathered adaptively. We develop theory for the TMIS Offline Policy Evaluation (OPE) estimator in this generalized setting for tabular MDPs, deriving high-probability, instance-dependent bounds on its estimation error. We also recover minimax-optimal offline learning in the adaptive setting. Finally, we conduct simulations to empirically analyze the behavior of these estimators under adaptive and non-adaptive regimes.
    
[^16]: DesCo: 利用详尽的语言描述学习物体识别

    DesCo: Learning Object Recognition with Rich Language Descriptions. (arXiv:2306.14060v1 [cs.CV])

    [http://arxiv.org/abs/2306.14060](http://arxiv.org/abs/2306.14060)

    DesCo是一种新的物体识别模型学习范式，旨在通过详尽的语言描述提高对新对象和域的适应性。

    

    最近视觉语言方法的发展引起了学习视觉识别模型从语言监督的范式转变。这些方法将对象与语言查询（例如，“一张猫的照片”）对齐，并提高了模型识别新对象和域的适应性。最近，几项研究尝试使用包括属性，形状，纹理和关系等细粒度语义细节规范的复杂语言表达式查询这些模型。然而，仅仅将语言描述作为查询加入并不能保证模型对其进行精确解释。事实上，我们的实验表明，用于物体检测的视觉语言模型GLIP常常忽略语言描述中的上下文信息，而是过于依赖仅凭名称检测物体。为了解决这些挑战，我们提出了一种新的“描述条件（DesCo）”物体识别模型学习范式。

    Recent development in vision-language approaches has instigated a paradigm shift in learning visual recognition models from language supervision. These approaches align objects with language queries (e.g. "a photo of a cat") and improve the models' adaptability to identify novel objects and domains. Recently, several studies have attempted to query these models with complex language expressions that include specifications of fine-grained semantic details, such as attributes, shapes, textures, and relations. However, simply incorporating language descriptions as queries does not guarantee accurate interpretation by the models. In fact, our experiments show that GLIP, the state-of-the-art vision-language model for object detection, often disregards contextual information in the language descriptions and instead relies heavily on detecting objects solely by their names. To tackle the challenges, we propose a new description-conditioned (DesCo) paradigm of learning object recognition model
    
[^17]: 探索等式约束下深度声明式网络中的梯度逼近问题

    Towards Understanding Gradient Approximation in Equality Constrained Deep Declarative Networks. (arXiv:2306.14054v1 [cs.LG])

    [http://arxiv.org/abs/2306.14054](http://arxiv.org/abs/2306.14054)

    本文探索了在等式约束下深度声明式网络中的梯度逼近策略，为训练深度学习模型提供了更为高效的计算方式。

    

    本文研究了在忽略约束项时，深度声明式节点的梯度是否可以近似，从而导致全局损失函数的下降方向。这对于训练深度学习模型具有重要的实际应用，因为逼近方法通常比真实梯度计算更节省时间。我们提供了针对具有线性等式约束和归一化约束问题的理论分析，并展示了一些逼近方法在实践中的良好效果以及注意事项。

    We explore conditions for when the gradient of a deep declarative node can be approximated by ignoring constraint terms and still result in a descent direction for the global loss function. This has important practical application when training deep learning models since the approximation is often computationally much more efficient than the true gradient calculation. We provide theoretical analysis for problems with linear equality constraints and normalization constraints, and show examples where the approximation works well in practice as well as some cautionary tales for when it fails.
    
[^18]: 一项图神经网络加速的综述：算法、系统和自定义硬件

    A Survey on Graph Neural Network Acceleration: Algorithms, Systems, and Customized Hardware. (arXiv:2306.14052v1 [cs.LG])

    [http://arxiv.org/abs/2306.14052](http://arxiv.org/abs/2306.14052)

    本文综述了如何加速图神经网络处理图结构数据的方法，包括智能算法、高效系统和自定义硬件等，提出了GNN加速的分类，并为未来的研究提供了方向。

    

    图神经网络（GNN）正在成为处理图结构数据的机器学习研究中的新兴领域。尽管GNN在许多任务上取得了最先进的性能，但在涉及许多数据和严格延迟要求的实际应用中，它们面临着可扩展性挑战。为了解决这些挑战，许多研究已经在如何加速GNN方面进行了。这些加速技术涉及GNN流程的各个方面，从智能训练和推理算法到高效的系统和自定义硬件。随着对GNN加速的研究量快速增长，缺乏一个系统化的处理来提供一个统一的视图，并解决相关工作的复杂性问题。在这篇综述中，我们提供了GNN加速的分类，审查了现有方法，并提出了未来的研究方向。我们对GNN加速的分类处理连接了现有的工作，并为这一领域的进一步发展铺平了道路。

    Graph neural networks (GNNs) are emerging for machine learning research on graph-structured data. GNNs achieve state-of-the-art performance on many tasks, but they face scalability challenges when it comes to real-world applications that have numerous data and strict latency requirements. Many studies have been conducted on how to accelerate GNNs in an effort to address these challenges. These acceleration techniques touch on various aspects of the GNN pipeline, from smart training and inference algorithms to efficient systems and customized hardware. As the amount of research on GNN acceleration has grown rapidly, there lacks a systematic treatment to provide a unified view and address the complexity of relevant works. In this survey, we provide a taxonomy of GNN acceleration, review the existing approaches, and suggest future research directions. Our taxonomic treatment of GNN acceleration connects the existing works and sets the stage for further development in this area.
    
[^19]: 决策依赖分布鲁棒的马尔可夫决策过程在动态流行病控制中的应用

    Decision-Dependent Distributionally Robust Markov Decision Process Method in Dynamic Epidemic Control. (arXiv:2306.14051v1 [math.OC])

    [http://arxiv.org/abs/2306.14051](http://arxiv.org/abs/2306.14051)

    本文提出了一种新的分布鲁棒的马尔可夫决策过程框架，该框架允许考虑与决策相关的动态转移的不确定分布，从而更好地解决了动态流行病控制问题。

    

    本文提出了一种分布鲁棒的马尔可夫决策过程（DRMDP）方法，用于解决动态流行病控制问题。 Susceptible-Exposed-Infectious-Recovered（SEIR）模型广泛用于表示传染性疾病（如COVID-19）的随机传播。而马尔可夫决策过程为识别根据SEIR模型对抗疾病传播的最优措施（如疫苗接种和降低传播干预）提供了数学框架。 然而，这些情况中的不确定性需要一种更强大、不那么依赖于错误假设的方法。我们研究的主要目标是引入一种新的DRMDP框架，允许在一个与决策相关的模糊集合中考虑转移动态的不确定分布。具体而言，我们考虑在决策相关的模糊集合内的转移概率的最坏情况分布。为了克服与策略确定相关的计算复杂性，我们提出了一种可行的解决方案

    In this paper, we present a Distributionally Robust Markov Decision Process (DRMDP) approach for addressing the dynamic epidemic control problem. The Susceptible-Exposed-Infectious-Recovered (SEIR) model is widely used to represent the stochastic spread of infectious diseases, such as COVID-19. While Markov Decision Processes (MDP) offers a mathematical framework for identifying optimal actions, such as vaccination and transmission-reducing intervention, to combat disease spreading according to the SEIR model. However, uncertainties in these scenarios demand a more robust approach that is less reliant on error-prone assumptions. The primary objective of our study is to introduce a new DRMDP framework that allows for an ambiguous distribution of transition dynamics. Specifically, we consider the worst-case distribution of these transition probabilities within a decision-dependent ambiguity set. To overcome the computational complexities associated with policy determination, we propose a
    
[^20]: H$_2$O: 高效生成大型语言模型的热门元素预测器

    H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models. (arXiv:2306.14048v1 [cs.LG])

    [http://arxiv.org/abs/2306.14048](http://arxiv.org/abs/2306.14048)

    本文提出了一种通过预测文本中的热门元素来减少GPU内存消耗的方法，在实验中表现良好。

    

    大型语言模型(LLM)在最近取得了令人瞩目的成就, 但是由于成本过高，它们特别难以用于对话系统和故事创作等需要生成长内容的应用。除了模型参数外，通常还需要在GPU内存中存储大量临时状态信息，称为KV cache，它与序列长度和批量大小呈线性关系。在本文中, 我们提出了一种新颖的实现KV cache的方法，它显著地减少了其内存占用量。我们的方法基于一个引人注目的发现，即在计算注意力分数时，小部分标记贡献最大价值。我们称这些标记为热门元素(H$_2$)。通过全面的研究，我们发现(i) H$_2$的出现是自然而然的，并且与文本中标记的频繁共现强相关；(ii)去除它们会导致明显的性能下降。基于这些见解，我们提出了H$_2$O，一种用于高效生成LLM的热门元素预测器。H$_2$O可以准确地预测给定序列中的热门元素，并因此保持一个更小的KV cache,将GPU内存消耗减少了50%。我们在几个基准数据集上的实验表明,H$_2$O可以在显著减少内存消耗的同时，实现与完整KV cache相当的性能。

    Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insi
    
[^21]: 面向需求响应的最优定价--一种非参数约束策略优化方法

    Towards Optimal Pricing of Demand Response -- A Nonparametric Constrained Policy Optimization Approach. (arXiv:2306.14047v1 [cs.LG])

    [http://arxiv.org/abs/2306.14047](http://arxiv.org/abs/2306.14047)

    本研究提出了一种非参数约束策略优化方法，通过消除对策略表示的限制性假设，提高优化度并确保策略更新的稳定性，从而实现电力负荷高峰和低峰之间的合理转移。

    

    需求响应（DR）已被证明是降低电力市场供需两侧不确定性和峰值负荷的有效方法。然而，对于DR研究而言，一个关键问题是如何适当地调整电力价格，以将电力负荷从高峰转移到低峰时段。本文提出了一种创新的非参数约束策略优化方法，旨在提高优化度并确保策略更新的稳定性。

    Demand response (DR) has been demonstrated to be an effective method for reducing peak load and mitigating uncertainties on both the supply and demand sides of the electricity market. One critical question for DR research is how to appropriately adjust electricity prices in order to shift electrical load from peak to off-peak hours. In recent years, reinforcement learning (RL) has been used to address the price-based DR problem because it is a model-free technique that does not necessitate the identification of models for end-use customers. However, the majority of RL methods cannot guarantee the stability and optimality of the learned pricing policy, which is undesirable in safety-critical power systems and may result in high customer bills. In this paper, we propose an innovative nonparametric constrained policy optimization approach that improves optimality while ensuring stability of the policy update, by removing the restrictive assumption on policy representation that the majorit
    
[^22]: 机器学习需要自己的随机标准：随机平滑和基于PRNG的攻击。

    Machine Learning needs its own Randomness Standard: Randomised Smoothing and PRNG-based attacks. (arXiv:2306.14043v1 [cs.LG])

    [http://arxiv.org/abs/2306.14043](http://arxiv.org/abs/2306.14043)

    本文研究了机器学习中随机性可被攻击者利用的问题，主要关注流行的随机平滑方法。该方法用于训练可证明鲁棒性的模型和量化不确定性，但其随机性可能导致系统被攻击。

    

    随机性支持机器学习中的许多关键功能，包括优化、数据选择、隐私和安全。机器学习系统将生成或收集随机性的任务外包给了编译器、云服务提供商或工具链中的其他地方。但是，攻击者利用不良随机性甚至创建随机性的历史悠久，就像NSA放置后门在随机数生成器中以破解加密一样。本文考虑是否能够仅利用攻击者通常依赖的随机性来危害机器学习系统。我们将重点放在随机平滑上，这是一种流行的方法，用于训练可证明鲁棒性的模型，并为任意模型的特定输入数据点提供认证。我们选择随机平滑是因为它用于安全和安全（用于对抗对抗性示例和量化不确定性，分别）。在幕后，它依赖于采样高斯噪声来探索围绕数据点的体积。

    Randomness supports many critical functions in the field of machine learning (ML) including optimisation, data selection, privacy, and security. ML systems outsource the task of generating or harvesting randomness to the compiler, the cloud service provider or elsewhere in the toolchain. Yet there is a long history of attackers exploiting poor randomness, or even creating it -- as when the NSA put backdoors in random number generators to break cryptography. In this paper we consider whether attackers can compromise an ML system using only the randomness on which they commonly rely. We focus our effort on Randomised Smoothing, a popular approach to train certifiably robust models, and to certify specific input datapoints of an arbitrary model. We choose Randomised Smoothing since it is used for both security and safety -- to counteract adversarial examples and quantify uncertainty respectively. Under the hood, it relies on sampling Gaussian noise to explore the volume around a data poin
    
[^23]: 平滑的$f$-散度分布鲁棒优化：指数率效率和不带复杂性的校准。

    Smoothed $f$-Divergence Distributionally Robust Optimization: Exponential Rate Efficiency and Complexity-Free Calibration. (arXiv:2306.14041v1 [math.OC])

    [http://arxiv.org/abs/2306.14041](http://arxiv.org/abs/2306.14041)

    分布鲁棒优化在实现统计保证界限时存在限制和保守性问题，但平滑的$f$-散度分布鲁棒优化可在指数衰减率方面实现最紧密的统计保证。

    

    在数据驱动的优化中，样本平均逼近已知存在一个所谓的优化者诅咒，会导致在评估解决方案性能时产生乐观偏差。可以通过在估计的目标值中增加“保证空间”或通过分布鲁棒优化（DRO）来解决这个问题，后者是一种快速增长的方法，基于最坏情况分析，为获得的目标价值提供了保护界限。然而，在所有这些现有方法中，对真实解决方案性能的统计保证界限要么需要对目标函数复杂性有限制性条件和知识，要么会表现出取决于分布维度的过于保守的速率。我们认为，在这些挑战方面，一种特殊类型的DRO在理论上提供了强大的优势：对于一大类目标函数，它获得了对真实解的解决方案性能的统计界限，这在指数衰减率方面是可能的，就其紧缩程度而言，要紧密得多。

    In data-driven optimization, sample average approximation is known to suffer from the so-called optimizer's curse that causes optimistic bias in evaluating the solution performance. This can be tackled by adding a "margin" to the estimated objective value, or via distributionally robust optimization (DRO), a fast-growing approach based on worst-case analysis, which gives a protective bound on the attained objective value. However, in all these existing approaches, a statistically guaranteed bound on the true solution performance either requires restrictive conditions and knowledge on the objective function complexity, or otherwise exhibits an over-conservative rate that depends on the distribution dimension. We argue that a special type of DRO offers strong theoretical advantages in regard to these challenges: It attains a statistical bound on the true solution performance that is the tightest possible in terms of exponential decay rate, for a wide class of objective functions that not
    
[^24]: 自然语言任务中的加权自动机提取与递归神经网络解释

    Weighted Automata Extraction and Explanation of Recurrent Neural Networks for Natural Language Tasks. (arXiv:2306.14040v1 [cs.CL])

    [http://arxiv.org/abs/2306.14040](http://arxiv.org/abs/2306.14040)

    本文提出了一种新的加权有限状态自动机（WFA）提取和解释框架来处理自然语言任务中的局限性，从而解决了精度和可扩展性方面的限制。

    

    循环神经网络（RNN）在处理序列数据方面取得了巨大成功，但理解和分析它们的行为仍然是一个重大挑战。为此，许多人致力于从RNN中提取有限自动机，这对于分析和解释更方便。然而，现有的方法如精确学习和组合方法在可扩展性或精度方面存在局限性。因此，我们提出了一种新的加权有限状态自动机（WFA）提取和解释框架来解决自然语言任务中的局限性。

    Recurrent Neural Networks (RNNs) have achieved tremendous success in processing sequential data, yet understanding and analyzing their behaviours remains a significant challenge. To this end, many efforts have been made to extract finite automata from RNNs, which are more amenable for analysis and explanation. However, existing approaches like exact learning and compositional approaches for model extraction have limitations in either scalability or precision. In this paper, we propose a novel framework of Weighted Finite Automata (WFA) extraction and explanation to tackle the limitations for natural language tasks. First, to address the transition sparsity and context loss problems we identified in WFA extraction for natural language tasks, we propose an empirical method to complement missing rules in the transition diagram, and adjust transition matrices to enhance the context-awareness of the WFA. We also propose two data augmentation tactics to track more dynamic behaviours of RNN, 
    
[^25]: 基于划分指导的k-means算法：极端模型压缩下的极端空簇解决方法

    Partitioning-Guided K-Means: Extreme Empty Cluster Resolution for Extreme Model Compression. (arXiv:2306.14031v1 [cs.LG])

    [http://arxiv.org/abs/2306.14031](http://arxiv.org/abs/2306.14031)

    本研究提出了一种名为PG k-means的算法，基于划分来解决空簇，提高了iPQ与量化噪声的精确性。

    

    在深度学习中，紧凑性对于模型在低资源应用中的可用性至关重要，极端模型压缩的常见方法是量化。本文考虑迭代乘积量化（iPQ）与量化噪声是这一领域中的最新技术，但是这种量化框架由于存在空簇而导致推理质量下降，我们提出了几种新的增强方法，旨在通过解决空簇来提高iPQ与量化噪声的精确性。我们的贡献被称为基于划分指导的k-means算法（PG k-means），是由三个主要组成部分组成的高度增强的k-means实现。首先，我们提出了一种基于划分的预分配策略，确保没有初始空簇并鼓励均匀的权重分布。其次，我们提出了一种经验优越的空簇解决启发式算法，通过谨慎地分割大簇来执行。最后，我们限制了PG k-means的迭代总次数。

    Compactness in deep learning can be critical to a model's viability in low-resource applications, and a common approach to extreme model compression is quantization. We consider Iterative Product Quantization (iPQ) with Quant-Noise to be state-of-the-art in this area, but this quantization framework suffers from preventable inference quality degradation due to prevalent empty clusters. In this paper, we propose several novel enhancements aiming to improve the accuracy of iPQ with Quant-Noise by focusing on resolving empty clusters. Our contribution, which we call Partitioning-Guided k-means (PG k-means), is a heavily augmented k-means implementation composed of three main components. First, we propose a partitioning-based pre-assignment strategy that ensures no initial empty clusters and encourages an even weight-to-cluster distribution. Second, we propose an empirically superior empty cluster resolution heuristic executed via cautious partitioning of large clusters. Finally, we constr
    
[^26]: My Boli：混合马拉地语-英语的语料库、预训练语言模型和评估基准

    My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks. (arXiv:2306.14030v1 [cs.CL])

    [http://arxiv.org/abs/2306.14030](http://arxiv.org/abs/2306.14030)

    本研究针对资源匮乏的印度语言马拉地语，提出了一个大型混合马拉地语-英语语料库，以及预训练的混合BERT模型和针对混合语言下游任务的评估数据集，该语料库训练的模型显著优于现有的BERT模型。

    

    由于缺乏专门的混合语料库和预训练语言模型，混合语言数据的研究受到了限制。在这项工作中，我们关注资源匮乏的印度语言马拉地语，这个语言之前没有任何混合语言的研究。我们提出了L3Cube-MeCorpus，一个包含500万条推特的大型混合马拉地语-英语(Mr-En)语料库，用于预训练。我们还发布了L3Cube-MeBERT和MeRoBERTa，基于BERT的混合模型，在MeCorpus上预训练。此外，为了基准测试，我们提供了三个有监督的数据集MeHate、MeSent和MeLID，用于混合Mr-En仇恨言论检测、情感分析和语言识别等下游任务。这些评估数据集分别包含手动注释的\url{~}12,000条马拉地语-英语混合推特。削减实验表明，这个新语料库训练的模型显著优于现有的BERT模型。这是第一个提供混合马拉地语的代码的工作。

    The research on code-mixed data is limited due to the unavailability of dedicated code-mixed datasets and pre-trained language models. In this work, we focus on the low-resource Indian language Marathi which lacks any prior work in code-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English (Mr-En) corpus with 5 million tweets for pretraining. We also release L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models pre-trained on MeCorpus. Furthermore, for benchmarking, we present three supervised datasets MeHate, MeSent, and MeLID for downstream tasks like code-mixed Mr-En hate speech detection, sentiment analysis, and language identification respectively. These evaluation datasets individually consist of manually annotated \url{~}12,000 Marathi-English code-mixed tweets. Ablations show that the models trained on this novel corpus significantly outperform the existing state-of-the-art BERT models. This is the first work that presents artifacts for code-mix
    
[^27]: 高维树和图模型中的结构参数选择信息准则

    Information criteria for structured parameter selection in high dimensional tree and graph models. (arXiv:2306.14026v1 [cs.LG])

    [http://arxiv.org/abs/2306.14026](http://arxiv.org/abs/2306.14026)

    本文提出了适用于两种结构模型的信息准则，用于高维数据的参数选择，并平衡了虚假阳性和虚假阴性。

    

    在高维模型中进行参数选择通常会通过控制虚假阳性（相对）数量的方式进行微调。这是因为否则，少数的真实阳性可能会被众多的虚假阳性所主导。例如，当选择遵循于信息准则的朴素优化（例如AIC或Mallows的Cp）时，就会发生这种情况。可以认为，选择的过度估计来自于优化过程自身改变所选变量的统计特性，这样信息准则就不再反映选择与数据生成过程之间的真实差异。在lasso中，过度估计也可以与收缩估计器联系起来，这使得选择对错误的阳性选择过于宽容。因此，本文研究了精细的信息准则，仔细平衡虚假阳性和虚假阴性，用于没有收缩的估计器。具体而言，本文提出了一种树结构模型和一种图结构模型，并针对这两种模型调整了两个信息准则（阿卡贝克信息准则和贝叶斯信息准则），考虑了结构的因素。

    Parameter selection in high-dimensional models is typically finetuned in a way that keeps the (relative) number of false positives under control. This is because otherwise the few true positives may be dominated by the many possible false positives. This happens, for instance, when the selection follows from a naive optimisation of an information criterion, such as AIC or Mallows's Cp. It can be argued that the overestimation of the selection comes from the optimisation process itself changing the statistics of the selected variables, in a way that the information criterion no longer reflects the true divergence between the selection and the data generating process. In lasso, the overestimation can also be linked to the shrinkage estimator, which makes the selection too tolerant of false positive selections. For these reasons, this paper works on refined information criteria, carefully balancing false positives and false negatives, for use with estimators without shrinkage. In particul
    
[^28]: 基于神经特征值分解的个性化给药动力学模型

    Individualized Dosing Dynamics via Neural Eigen Decomposition. (arXiv:2306.14020v1 [cs.LG])

    [http://arxiv.org/abs/2306.14020](http://arxiv.org/abs/2306.14020)

    本文提出了一个基于神经特征值分解的随机微分方程算法，可以用于个性化医疗给药模型，具有噪声水平可调、快速、连续、闭合的预测等特点，并在合成和真实医疗问题中展示了其鲁棒性和优越性能。

    

    给药模型通常使用微分方程来建模生物动力学。特别是神经微分方程可以学习预测过程的导数，从而在不规则的时间点进行预测。然而，这种时间上的灵活性常常伴随着对噪声的高敏感性，而医疗问题往往存在高噪声和有限数据。此外，医疗给药模型必须可靠地泛化到个体患者和不断变化的治疗政策上。为了解决这些挑战，本文引入了基于神经特征值分解的随机微分方程算法（NESDE）。NESDE提供个性化建模（使用超网络对患者水平参数）；对新治疗政策的泛化（使用解耦控制）；根据噪声水平可调的表现（使用分段线性）；以及快速、连续、闭合的预测（使用谱表示）。本文在合成和真实医疗问题中展示了NESDE的鲁棒性，表明它在预测准确性和稳健性方面优于现有方法。

    Dosing models often use differential equations to model biological dynamics. Neural differential equations in particular can learn to predict the derivative of a process, which permits predictions at irregular points of time. However, this temporal flexibility often comes with a high sensitivity to noise, whereas medical problems often present high noise and limited data. Moreover, medical dosing models must generalize reliably over individual patients and changing treatment policies. To address these challenges, we introduce the Neural Eigen Stochastic Differential Equation algorithm (NESDE). NESDE provides individualized modeling (using a hypernetwork over patient-level parameters); generalization to new treatment policies (using decoupled control); tunable expressiveness according to the noise level (using piecewise linearity); and fast, continuous, closed-form prediction (using spectral representation). We demonstrate the robustness of NESDE in both synthetic and real medical probl
    
[^29]: 使用N-BEATS模型解读感染患者预测的生命体征趋势

    Interpreting Forecasted Vital Signs Using N-BEATS in Sepsis Patients. (arXiv:2306.14016v1 [cs.LG])

    [http://arxiv.org/abs/2306.14016](http://arxiv.org/abs/2306.14016)

    本文通过使用可解释的深度学习预测模型N-BEATS，预测并解读ICU感染患者的生命体征趋势，结果显示其性能优于基准模型。

    

    检测和预测感染性休克对患者的最佳处理结果极为重要。准确地预测感染患者的生命体征可为临床医生提供有价值的见解，以进行及时的干预，如给予稳定药物或优化输液策略。本研究使用N-BEATS可解释的深度学习预测模型，能够预测重症监护病房（ICU）中感染患者的3小时生命体征。我们采用N-BEATS可解释的配置预测生命体征趋势，并与实际趋势进行比较，以更好地了解患者的病情变化和药物对其生命体征的影响。我们使用公开可用的eICU Collaborative Research Database数据集评估我们的方法，并采用样外评估标准对生命体征预测进行了严格评估。我们使用误差度量来展示模型的性能，包括均方误差（MSE）、平均绝对误差（MAE）和平均绝对百分比误差（MAPE）。我们的结果表明，N-BEATS在预测感染患者的生命体征方面优于几种基准模型。

    Detecting and predicting septic shock early is crucial for the best possible outcome for patients. Accurately forecasting the vital signs of patients with sepsis provides valuable insights to clinicians for timely interventions, such as administering stabilizing drugs or optimizing infusion strategies. Our research examines N-BEATS, an interpretable deep-learning forecasting model that can forecast 3 hours of vital signs for sepsis patients in intensive care units (ICUs). In this work, we use the N-BEATS interpretable configuration to forecast the vital sign trends and compare them with the actual trend to understand better the patient's changing condition and the effects of infused drugs on their vital signs. We evaluate our approach using the publicly available eICU Collaborative Research Database dataset and rigorously evaluate the vital sign forecasts using out-of-sample evaluation criteria. We present the performance of our model using error metrics, including mean squared error (
    
[^30]: 通过更高级任务相似性加强图上的多任务学习

    Boosting Multitask Learning on Graphs through Higher-Order Task Affinities. (arXiv:2306.14009v1 [cs.LG])

    [http://arxiv.org/abs/2306.14009](http://arxiv.org/abs/2306.14009)

    本文从多任务学习的角度重新审视在给定图上预测节点标签的问题，提出通过更高级任务相似性来加强多任务学习，并开发了一种算法来将任务分组以应对负迁移问题。

    

    在给定图上预测节点标签是一个被广泛研究的问题，有许多应用，包括社区检测和分子图预测。本文从多任务学习的角度重新审视此问题，考虑同时在图上预测多个节点标签函数。为了具体说明，考虑重叠社区检测：每个社区成员身份是一个二进制节点分类任务。由于复杂的重叠模式，当我们将多个社区检测应用到naive多任务学习时，我们发现负迁移很普遍，因为不同的节点标签之间的任务关系高度非线性。为了解决这个挑战，我们开发了一种算法，基于更高级的任务相似性度量来将任务分组。然后我们在每个任务组上拟合多任务模型，产生在基线模型上的增强过程。我们将两个任务之间的更高级的任务相似性度量估计为预测损失。

    Predicting node labels on a given graph is a widely studied problem with many applications, including community detection and molecular graph prediction. This paper considers predicting multiple node labeling functions on graphs simultaneously and revisits this problem from a multitask learning perspective. For a concrete example, consider overlapping community detection: each community membership is a binary node classification task. Due to complex overlapping patterns, we find that negative transfer is prevalent when we apply naive multitask learning to multiple community detection, as task relationships are highly nonlinear across different node labeling. To address the challenge, we develop an algorithm to cluster tasks into groups based on a higher-order task affinity measure. We then fit a multitask model on each task group, resulting in a boosting procedure on top of the baseline model. We estimate the higher-order task affinity measure between two tasks as the prediction loss o
    
[^31]: 基于聚类和图深度学习的 COVID-19 药物再利用框架

    A clustering and graph deep learning-based framework for COVID-19 drug repurposing. (arXiv:2306.13995v1 [cs.AI])

    [http://arxiv.org/abs/2306.13995](http://arxiv.org/abs/2306.13995)

    本文提出了一种基于聚类和图深度学习的方法，在 COVID-19 药物再利用中发现了具有见地的药物相互作用，并确定了潜在有效的药物再利用候选物。

    

    药物再利用 (或重新定位) 是寻找已经获得药物监管机构 (例如美国食品药品监督管理局和治疗商品管理局) 批准用于其他疾病的药物的新治疗用途的过程。这包括分析不同生物实体之间的相互作用，例如药物靶点 (基因/蛋白质和生物通路) 和药物属性，以发现新的药物靶点或药物-疾病关系。机器学习和深度学习等人工智能方法已经成功地分析了生物医学领域中复杂的异质数据，并且已被用于药物再利用。本研究提出了一种新的无监督机器学习框架，其利用基于图的自编码器在异质药物数据上进行多特征类型聚类。数据集包括 438 种药物，其中 224 种正在接受 COVID-19 临床试验 (类别 A)。其余药物经过系统过滤以确保再利用潜在药物候选物的安全性 (类别 B)。所提出的框架在 COVID-19 特定药物再利用任务上进行评估，结果表明它能够揭示有见地的药物相互作用并确定潜在有效的药物再利用候选物。

    Drug repurposing (or repositioning) is the process of finding new therapeutic uses for drugs already approved by drug regulatory authorities (e.g., the Food and Drug Administration (FDA) and Therapeutic Goods Administration (TGA)) for other diseases. This involves analyzing the interactions between different biological entities, such as drug targets (genes/proteins and biological pathways) and drug properties, to discover novel drug-target or drug-disease relations. Artificial intelligence methods such as machine learning and deep learning have successfully analyzed complex heterogeneous data in the biomedical domain and have also been used for drug repurposing. This study presents a novel unsupervised machine learning framework that utilizes a graph-based autoencoder for multi-feature type clustering on heterogeneous drug data. The dataset consists of 438 drugs, of which 224 are under clinical trials for COVID-19 (category A). The rest are systematically filtered to ensure the safety 
    
[^32]: 带有$\ell_0$-范数hinge loss的核支持向量机分类器

    Kernel Support Vector Machine Classifiers with the $\ell_0$-Norm Hinge Loss. (arXiv:2306.13991v1 [cs.LG])

    [http://arxiv.org/abs/2306.13991](http://arxiv.org/abs/2306.13991)

    本论文研究了一种基于核的支持向量机分类器，采用$\ell_0$-范数hinge loss处理标签噪声问题并提供了一种ADMM算法。

    

    支持向量机(SVM)已成为最成功的二分类问题的机器学习技术之一。其关键思想是在保证训练样本正确分类的条件下，最大化数据到超平面的间隔。常用的hinge loss及其变体对标签噪声敏感，并且由于其无界性而对重采样不稳定。本文重点研究带有$\ell_0$-norm hinge loss（称为$\ell_0$-KSVM）的核SVM，该方法是hinge loss 和 $\ell_0$-norm 的复合函数，可以克服上述困难。

    Support Vector Machine (SVM) has been one of the most successful machine learning techniques for binary classification problems. The key idea is to maximize the margin from the data to the hyperplane subject to correct classification on training samples. The commonly used hinge loss and its variations are sensitive to label noise, and unstable for resampling due to its unboundedness. This paper is concentrated on the kernel SVM with the $\ell_0$-norm hinge loss (referred as $\ell_0$-KSVM), which is a composite function of hinge loss and $\ell_0$-norm and then could overcome the difficulties mentioned above. In consideration of the nonconvexity and nonsmoothness of $\ell_0$-norm hinge loss, we first characterize the limiting subdifferential of the $\ell_0$-norm hinge loss and then derive the equivalent relationship among the proximal stationary point, the Karush-Kuhn-Tucker point, and the local optimal solution of $\ell_0$-KSVM. Secondly, we develop an ADMM algorithm for $\ell_0$-KSVM, 
    
[^33]: 交叉验证就是你所需的：一种统计方法来估计标签噪声。

    Cross-Validation Is All You Need: A Statistical Approach To Label Noise Estimation. (arXiv:2306.13990v1 [cs.LG])

    [http://arxiv.org/abs/2306.13990](http://arxiv.org/abs/2306.13990)

    本论文提出了一种重复交叉验证(Repeated Cross-Validation)方法，通过构建噪声直方图并提出三种基于该直方图的方法来检测标签噪声并清理数据，解决了结果预测分析中的数据清洗问题。

    

    标签噪声在机器学习数据集中普遍存在。鉴定和消除标签噪声至关重要，因为在噪声数据上训练的模型会大幅降低准确性和泛化性。大多数现有的标签噪声检测方法都是为分类任务设计的，而基于结果预测分析的数据清理相对未被探索。受到交叉验证中不同折的性能波动的启发，我们提出了用于标签噪声估计的重复交叉验证（ReCoV）来填补这一空白。ReCoV通过记录每个最差表现折中的样本ID来构建一个噪声直方图，以此来排名样本的噪声水平。我们进一步提出了三种基于噪声直方图来鉴别嘈杂样本的方法，以解决越来越复杂的噪声分布。我们展示了ReCoV在分类任务基准中的优越表现，优于现有最先进标签清理算法。更重要的是，

    Label noise is prevalent in machine learning datasets. It is crucial to identify and remove label noise because models trained on noisy data can have substantially reduced accuracy and generalizability. Most existing label noise detection approaches are designed for classification tasks, and data cleaning for outcome prediction analysis is relatively unexplored. Inspired by the fluctuations in performance across different folds in cross-validation, we propose Repeated Cross-Validations for label noise estimation (ReCoV) to address this gap. ReCoV constructs a noise histogram that ranks the noise level of samples based on a large number of cross-validations by recording sample IDs in each worst-performing fold. We further propose three approaches for identifying noisy samples based on noise histograms to address increasingly complex noise distributions. We show that ReCoV outperforms state-of-the-art algorithms for label cleaning in a classification task benchmark. More importantly, we 
    
[^34]: SAM++: 利用语义信息和结构推理增强解剖匹配

    SAM++: Enhancing Anatomic Matching using Semantic Information and Structural Inference. (arXiv:2306.13988v1 [cs.CV])

    [http://arxiv.org/abs/2306.13988](http://arxiv.org/abs/2306.13988)

    SAM++是一个医学影像解剖匹配框架，用于学习外观和语义嵌入，并且通过固定点匹配机制同时解决外观相似但语义不同或语义相似但外观不同的结构匹配问题。

    

    医学影像例如 CT 和 MRI 提供了有关身体内部结构的详细信息，从这些图像中识别关键解剖结构在临床工作流程中起着至关重要的作用。目前的方法将此视为配准或关键点回归任务，具有准确匹配的局限性，并且只能处理预定义的地标。最近，一些方法已被引入以解决这些局限性，其中之一称为 SAM，提出使用密集的自我监督方法学习 CT 图像上每个点的独特嵌入，并取得了良好的结果。然而，SAM 在处理外观相似但语义不同或语义相似但外观不同的结构时仍可能面临困难。为了解决这些限制，我们提出了 SAM++，一个框架，通过一种新颖的固定点匹配机制同时学习外观和语义嵌入。我们测试了 SAM++ 框架。

    Medical images like CT and MRI provide detailed information about the internal structure of the body, and identifying key anatomical structures from these images plays a crucial role in clinical workflows. Current methods treat it as a registration or key-point regression task, which has limitations in accurate matching and can only handle predefined landmarks. Recently, some methods have been introduced to address these limitations. One such method, called SAM, proposes using a dense self-supervised approach to learn a distinct embedding for each point on the CT image and achieving promising results. Nonetheless, SAM may still face difficulties when dealing with structures that have similar appearances but different semantic meanings or similar semantic meanings but different appearances. To overcome these limitations, we propose SAM++, a framework that simultaneously learns appearance and semantic embeddings with a novel fixed-points matching mechanism. We tested the SAM++ framework 
    
[^35]: 使用数据自适应能量距离的高维数据稳健分类

    Robust Classification of High-Dimensional Data using Data-Adaptive Energy Distance. (arXiv:2306.13985v1 [stat.ML])

    [http://arxiv.org/abs/2306.13985](http://arxiv.org/abs/2306.13985)

    该论文提出了一种用于高维低样本量数据分类的稳健的数据自适应能量距离分类器，该分类器无需调参且在一定条件下可以实现完美分类，已在模拟研究和实际数据分析中得到证明比其他方法表现更优。

    

    在真实世界中，高维低样本量（HDLSS）数据的分类面临挑战，例如基因表达研究、癌症研究和医学成像等领域。本文提出了一些专门为HDLSS数据设计的分类器的开发和分析。这些分类器没有调节参数，并且是稳健的，因为它们不受底层数据分布的任何矩条件的影响。研究表明，在一些相当普遍的条件下，它们在HDLSS渐近区域内可以实现完美分类。还比较了所提出分类器的性能。我们的理论结果得到了广泛的模拟研究和实际数据分析的支持，证明了所提出分类技术优于几种广泛认可的方法的有希望优势。

    Classification of high-dimensional low sample size (HDLSS) data poses a challenge in a variety of real-world situations, such as gene expression studies, cancer research, and medical imaging. This article presents the development and analysis of some classifiers that are specifically designed for HDLSS data. These classifiers are free of tuning parameters and are robust, in the sense that they are devoid of any moment conditions of the underlying data distributions. It is shown that they yield perfect classification in the HDLSS asymptotic regime, under some fairly general conditions. The comparative performance of the proposed classifiers is also investigated. Our theoretical results are supported by extensive simulation studies and real data analysis, which demonstrate promising advantages of the proposed classification techniques over several widely recognized methods.
    
[^36]: 移动云协同智能推断

    Mobile-Cloud Inference for Collaborative Intelligence. (arXiv:2306.13982v1 [cs.LG])

    [http://arxiv.org/abs/2306.13982](http://arxiv.org/abs/2306.13982)

    移动设备上部分推断，生成紧凑的特征张量，并在服务器上进行进一步的推断，以降低推断时延和降低隐私风险。

    

    随着移动设备的人工智能应用越来越普及，对于深度学习模型推断的更快执行和更低的能量消耗需求也日益增加。历史上，移动设备上运行的模型较大型最新研究模型较小且简单，只能在云端运行。然而，只在云上进行推断存在带宽消耗增加和更高延迟的缺陷。此外，只在云上进行推断需要将输入数据（图像、音频）全部传输到云上，存在隐私泄露的风险。另有一种选择：共享移动云推断。在移动设备上执行部分推断以缩减输入数据的维度并生成一个紧凑的特征张量，这是输入信号的潜在空间表示。然后将特征张量传输到服务器进行进一步推断。这种策略可以降低推断时延。

    As AI applications for mobile devices become more prevalent, there is an increasing need for faster execution and lower energy consumption for deep learning model inference. Historically, the models run on mobile devices have been smaller and simpler in comparison to large state-of-the-art research models, which can only run on the cloud. However, cloud-only inference has drawbacks such as increased network bandwidth consumption and higher latency. In addition, cloud-only inference requires the input data (images, audio) to be fully transferred to the cloud, creating concerns about potential privacy breaches.  There is an alternative approach: shared mobile-cloud inference. Partial inference is performed on the mobile in order to reduce the dimensionality of the input data and arrive at a compact feature tensor, which is a latent space representation of the input signal. The feature tensor is then transmitted to the server for further inference. This strategy can reduce inference laten
    
[^37]: 基于正则化SE(3)群卷积的体积医学图像分析

    Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis. (arXiv:2306.13960v1 [cs.CV])

    [http://arxiv.org/abs/2306.13960](http://arxiv.org/abs/2306.13960)

    本文提出了基于正则化SE(3)群卷积的体积医学图像分析方法，通过分解连续SO(3)核和空间核以实现旋转平移等变性，并在医学分类任务中获得了显著性能提升。

    

    研究表明，正则组卷积神经网络(G-CNN)可以提高模型性能并提高对不同几何对称性的等变性。本文解决了SE(3)问题，即旋转平移等变性在体积数据上的问题。体积图像数据在许多医疗设置中普遍存在。受可分离组卷积的最新工作的启发，我们设计了一个SE(3)群卷积核，将其分解为连续的SO(3)（旋转）核和空间核。我们通过采样均匀的SO(3)网格来近似连续设定下的对称性。我们的连续SO(3)核是通过类似均匀网格的RBF插值参数化的。我们展示了我们的方法在体积医学图像分析中的优势。我们的SE(3)等变模型在具有挑战性的医学分类任务上始终优于CNN和常规离散G-CNN，并显示出显着改进的泛化能力。我们的方法在噪声数据下的性能提高了达到16.5%。

    Regular group convolutional neural networks (G-CNNs) have been shown to increase model performance and improve equivariance to different geometrical symmetries. This work addresses the problem of SE(3), i.e., roto-translation equivariance, on volumetric data. Volumetric image data is prevalent in many medical settings. Motivated by the recent work on separable group convolutions, we devise a SE(3) group convolution kernel separated into a continuous SO(3) (rotation) kernel and a spatial kernel. We approximate equivariance to the continuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernel is parameterized via RBF interpolation on similarly uniform grids. We demonstrate the advantages of our approach in volumetric medical image analysis. Our SE(3) equivariant models consistently outperform CNNs and regular discrete G-CNNs on challenging medical classification tasks and show significantly improved generalization capabilities. Our approach achieves up to a 16.5% gain in
    
[^38]: DiffDTM: 针对双重蛋白质靶点的生物活性分子生成的无条件结构框架

    DiffDTM: A conditional structure-free framework for bioactive molecules generation targeted for dual proteins. (arXiv:2306.13957v1 [cs.LG])

    [http://arxiv.org/abs/2306.13957](http://arxiv.org/abs/2306.13957)

    DiffDTM是一个去除条件结构限制的深度生成模型，用于针对双重蛋白质靶点的生物活性分子生成。该模型能够在一次性有条件生成的情况下生成具有高结合亲和力、可合成和新颖的药物样分子，并胜过最先进技术。

    

    深度生成模型的进展为带有所需性质的de novo分子生成提供了启示。然而，针对双重蛋白质靶点的分子生成仍面临着巨大的挑战，包括蛋白质3D结构数据的获取，自回归采样以及模型对未见过目标的泛化能力。我们提出了基于扩散模型的DiffDTM，这是一种新颖的无条件结构免深度生成模型，用于双重靶点分子生成，以解决上述问题。具体而言，DiffDTM接收蛋白质序列和分子图作为输入，而非蛋白质和分子构象，并引入信息融合模块以实现一次性有条件生成。我们进行了全面的多视图实验，证明了DiffDTM能够生成针对特定双重蛋白质的药物样，可合成，新颖且具有高结合亲和力的分子，并优于最先进技术（SOTA）。

    Advances in deep generative models shed light on de novo molecule generation with desired properties. However, molecule generation targeted for dual protein targets still faces formidable challenges including protein 3D structure data requisition for model training, auto-regressive sampling, and model generalization for unseen targets. Here, we proposed DiffDTM, a novel conditional structure-free deep generative model based on a diffusion model for dual targets based molecule generation to address the above issues. Specifically, DiffDTM receives protein sequences and molecular graphs as inputs instead of protein and molecular conformations and incorporates an information fusion module to achieve conditional generation in a one-shot manner. We have conducted comprehensive multi-view experiments to demonstrate that DiffDTM can generate drug-like, synthesis-accessible, novel, and high-binding affinity molecules targeting specific dual proteins, outperforming the state-of-the-art (SOTA) mo
    
[^39]: 实现真实空气质量预测：介绍易于使用的PurpleAirSF数据集

    Unleashing Realistic Air Quality Forecasting: Introducing the Ready-to-Use PurpleAirSF Dataset. (arXiv:2306.13948v1 [cs.LG])

    [http://arxiv.org/abs/2306.13948](http://arxiv.org/abs/2306.13948)

    该论文介绍了PurpleAirSF数据集，这是一个易于获取的数据集，具有高时间分辨率、多种空气质量测量指标和广泛的地理范围，可用于研究人员的空气质量预测建模和空气污染模式研究，同时此数据集可用于未来开发新型的应用模型。

    

    空气质量预测由于机器学习和深度学习模型的进步引起了人们的极大关注。然而，复杂的数据采集和开放数据集的缺乏给研究人员造成了挑战，从而阻碍了有效的模型验证。本文介绍了PurpleAirSF，这是一个综合全面且易于获取的数据集，从PurpleAir网络中收集而来。该数据集具有高时间分辨率、各种空气质量测量指标和广泛的地理覆盖范围，可作为研究人员开发新型预测模型、研究空气污染模式以及调查其对健康和环境的影响的有用工具。我们介绍了构建PurpleAirSF所采用的数据采集和处理方法。此外，我们还使用经典和现代时空预测模型进行了初步实验，从而为未来空气质量预测模型的制定建立了基准。

    Air quality forecasting has garnered significant attention recently, with data-driven models taking center stage due to advancements in machine learning and deep learning models. However, researchers face challenges with complex data acquisition and the lack of open-sourced datasets, hindering efficient model validation. This paper introduces PurpleAirSF, a comprehensive and easily accessible dataset collected from the PurpleAir network. With its high temporal resolution, various air quality measures, and diverse geographical coverage, this dataset serves as a useful tool for researchers aiming to develop novel forecasting models, study air pollution patterns, and investigate their impacts on health and the environment. We present a detailed account of the data collection and processing methods employed to build PurpleAirSF. Furthermore, we conduct preliminary experiments using both classic and modern spatio-temporal forecasting models, thereby establishing a benchmark for future air q
    
[^40]: 面向土耳其地址解析的预训练语言模型比较研究

    Comparison of Pre-trained Language Models for Turkish Address Parsing. (arXiv:2306.13947v1 [cs.CL])

    [http://arxiv.org/abs/2306.13947](http://arxiv.org/abs/2306.13947)

    本研究比较、评估了多语言和针对土耳其的BERT、DistilBERT、ELECTRA和RoBERTa模型在土耳其地址解析上的性能，结果发现针对土耳其的模型表现更佳。

    

    基于Transformer的预训练模型，如BERT及其变种，通过在大型语料库上的训练，在自然语言处理（NLP）任务中取得了巨大的成功。大多数学术研究都是基于英语进行的;然而，多语言和特定语言的研究数量正在稳步增加。此外，一些研究声称，针对特定语言的模型在各种任务中优于多语言模型。因此，社区倾向于针对其案例研究的语言来训练或微调模型。本文针对土耳其地图数据，全面评估了多语言和土耳其BERT、DistilBERT、ELECTRA和RoBERTa。此外，我们还提出了一个多层感知器（MLP）用于微调BERT，以及标准的一层微调方法。对于数据集，我们构建了一个质量相对较高的中等规模地址解析语料库。在这个数据集上进行的实验表明，

    Transformer based pre-trained models such as BERT and its variants, which are trained on large corpora, have demonstrated tremendous success for natural language processing (NLP) tasks. Most of academic works are based on the English language; however, the number of multilingual and language specific studies increase steadily. Furthermore, several studies claimed that language specific models outperform multilingual models in various tasks. Therefore, the community tends to train or fine-tune the models for the language of their case study, specifically. In this paper, we focus on Turkish maps data and thoroughly evaluate both multilingual and Turkish based BERT, DistilBERT, ELECTRA and RoBERTa. Besides, we also propose a MultiLayer Perceptron (MLP) for fine-tuning BERT in addition to the standard approach of one-layer fine-tuning. For the dataset, a mid-sized Address Parsing corpus taken with a relatively high quality is constructed. Conducted experiments on this dataset indicate that
    
[^41]: 大型序列模型用于顺序决策：综述

    Large Sequence Models for Sequential Decision-Making: A Survey. (arXiv:2306.13945v1 [cs.LG])

    [http://arxiv.org/abs/2306.13945](http://arxiv.org/abs/2306.13945)

    本综述全面概述了使用Transformer等序列模型解决顺序决策问题的最近研究进展，并按照处理样本效率、信用分配和部分可观察性的方式对其进行分类。

    

    Transformer结构促进了自然语言处理和计算机视觉中预测任务的大规模通用序列模型的发展，例如GPT-3和Swin Transformer。虽然最初设计用于预测问题，但自然而然地会询问它们是否适用于通常存在样本效率、信用分配和部分可观察性问题的顺序决策和强化学习问题。近年来，序列模型，特别是Transformer，吸引了RL社区越来越多的关注，产生了许多具有显着有效性和通用性的方法。该综述全面概述了最近的工作，旨在通过讨论顺序决策和序列建模之间的联系，并基于它们处理前述问题的方式对它们进行分类，解决使用序列模型（例如Transformer）解决顺序决策任务的问题。

    Transformer architectures have facilitated the development of large-scale and general-purpose sequence models for prediction tasks in natural language processing and computer vision, e.g., GPT-3 and Swin Transformer. Although originally designed for prediction problems, it is natural to inquire about their suitability for sequential decision-making and reinforcement learning problems, which are typically beset by long-standing issues involving sample efficiency, credit assignment, and partial observability. In recent years, sequence models, especially the Transformer, have attracted increasing interest in the RL communities, spawning numerous approaches with notable effectiveness and generalizability. This survey presents a comprehensive overview of recent works aimed at solving sequential decision-making tasks with sequence models such as the Transformer, by discussing the connection between sequential decision-making and sequence modeling, and categorizing them based on the way they 
    
[^42]: 具有避开死局和恢复能力的安全强化学习

    Safe Reinforcement Learning with Dead-Ends Avoidance and Recovery. (arXiv:2306.13944v1 [cs.LG])

    [http://arxiv.org/abs/2306.13944](http://arxiv.org/abs/2306.13944)

    本文提出了一种利用死局的边界来辨别安全和不安全状态，以确保安全性同时减少对探索的限制的方法。采用了分离的强化学习框架，训练了两个策略：一个任务策略，专注于任务表现，以及一个恢复策略，最大化安全性。

    

    安全性是将强化学习应用于现实环境任务时面临的主要挑战之一。为了确保在训练过程中和之后的安全性，现有的方法往往采用过于保守的策略以避免不安全的情况。但是，过于保守的策略严重阻碍了探索，使算法的回报大大降低。在本文中，我们提出了一种方法来构建一个边界，区分安全和不安全的状态。我们构建的边界等价于区分死局状态，表明安全探索的最大程度，因此在探索方面的限制最小。类似于恢复强化学习，我们利用一个分离的强化学习框架来学习两个策略，(1) 只考虑改善任务表现的任务策略，以及 (2) 最大化安全性的恢复策略。恢复策略和相应的安全性批判家在离线数据集上进行预训练，其中安全批判家会区分安全和不安全的状态，而恢复策略会采取措施以从不安全状态中恢复。

    Safety is one of the main challenges in applying reinforcement learning to realistic environmental tasks. To ensure safety during and after training process, existing methods tend to adopt overly conservative policy to avoid unsafe situations. However, overly conservative policy severely hinders the exploration, and makes the algorithms substantially less rewarding. In this paper, we propose a method to construct a boundary that discriminates safe and unsafe states. The boundary we construct is equivalent to distinguishing dead-end states, indicating the maximum extent to which safe exploration is guaranteed, and thus has minimum limitation on exploration. Similar to Recovery Reinforcement Learning, we utilize a decoupled RL framework to learn two policies, (1) a task policy that only considers improving the task performance, and (2) a recovery policy that maximizes safety. The recovery policy and a corresponding safety critic are pretrained on an offline dataset, in which the safety c
    
[^43]: 利用外样本和重抽样策略优化结构学习算法的超参数调整方法

    Tuning structure learning algorithms with out-of-sample and resampling strategies. (arXiv:2306.13932v1 [cs.LG])

    [http://arxiv.org/abs/2306.13932](http://arxiv.org/abs/2306.13932)

    本文提出了一种新的超参数调整方法 OTSL，它采用外样本和重抽样策略来估算给定输入数据集和结构学习算法的最佳超参数配置。实验表明，该方法优于现有技术，可提高结构学习算法的图形准确性。

    

    当实践者将结构学习算法应用于其数据时，面临的挑战之一是确定一组超参数；否则，假定一组超参数默认值。最佳超参数配置常常取决于多种因素，包括通常未知的真实底层图的大小和密度、输入数据的样本大小和结构学习算法等。我们提出了一种新的超参数调整方法，名为Out-of-sample Tuning for Structure Learning（OTSL），它采用外样本和重抽样策略来估算给定输入数据集和结构学习算法的最佳超参数配置。合成实验表明，使用OTSL作为混合和基于分数的结构学习算法的超参数调整手段，相对于现有技术，能够提高图形准确性。我们还演示了该方法在几个真实数据集中的适用性。

    One of the challenges practitioners face when applying structure learning algorithms to their data involves determining a set of hyperparameters; otherwise, a set of hyperparameter defaults is assumed. The optimal hyperparameter configuration often depends on multiple factors, including the size and density of the usually unknown underlying true graph, the sample size of the input data, and the structure learning algorithm. We propose a novel hyperparameter tuning method, called the Out-of-sample Tuning for Structure Learning (OTSL), that employs out-of-sample and resampling strategies to estimate the optimal hyperparameter configuration for structure learning, given the input data set and structure learning algorithm. Synthetic experiments show that employing OTSL as a means to tune the hyperparameters of hybrid and score-based structure learning algorithms leads to improvements in graphical accuracy compared to the state-of-the-art. We also illustrate the applicability of this approa
    
[^44]: 深度学习模型预测股票指数的比较研究

    Comparative Study of Predicting Stock Index Using Deep Learning Models. (arXiv:2306.13931v1 [cs.LG])

    [http://arxiv.org/abs/2306.13931](http://arxiv.org/abs/2306.13931)

    本文通过对传统预测方法和深度学习模型进行比较研究，证明Deep AR在股票指数预测方面表现最佳，具有较高的鲁棒性。

    

    过去几十年中现已尝试了许多时间序列预测方法，包括传统的技术分析、算法统计模型和最近的机器学习和人工智能方法。近年来，神经网络已经被纳入到预测场景中，如LSTM和常规RNN方法，利用了短期和长期的依赖关系。本研究评估了传统的预测方法，如ARIMA、SARIMA和SARIMAX，以及使用RNN构建的新型神经网络方法，如DF-RNN、DSSM和Deep AR。使用来自Kaggle的标准NIFTY-50数据集使用MSE、RMSE、MAPE、POCID和Theil's U等指标来评估这些模型。结果表明，Deep AR在所有其他常规深度学习和传统方法中表现最佳，其MAPE为0.01，RMSE为189。此外，当减少训练数据量时，Deep AR和GRU的性能不会降低，这表明它们的鲁棒性很高。

    Time series forecasting has seen many methods attempted over the past few decades, including traditional technical analysis, algorithmic statistical models, and more recent machine learning and artificial intelligence approaches. Recently, neural networks have been incorporated into the forecasting scenario, such as the LSTM and conventional RNN approaches, which utilize short-term and long-term dependencies. This study evaluates traditional forecasting methods, such as ARIMA, SARIMA, and SARIMAX, and newer neural network approaches, such as DF-RNN, DSSM, and Deep AR, built using RNNs. The standard NIFTY-50 dataset from Kaggle is used to assess these models using metrics such as MSE, RMSE, MAPE, POCID, and Theil's U. Results show that Deep AR outperformed all other conventional deep learning and traditional approaches, with the lowest MAPE of 0.01 and RMSE of 189. Additionally, the performance of Deep AR and GRU did not degrade when the amount of training data was reduced, suggesting t
    
[^45]: 评估GAN生成的合成表格数据用于类别平衡和低资源环境中的实用性

    Evaluating the Utility of GAN Generated Synthetic Tabular Data for Class Balancing and Low Resource Settings. (arXiv:2306.13929v1 [cs.LG])

    [http://arxiv.org/abs/2306.13929](http://arxiv.org/abs/2306.13929)

    本研究评估了GAN生成的合成数据在解决分类任务中不平衡数据和提高模型在低资源环境中的性能方面的实用性，并发现在GAN合成数据上进行训练的模型性能表现更佳。

    

    本研究旨在解决分类任务中不平衡数据的问题，并评估了SMOTE、ADASYN和GAN技术在生成合成数据以解决类别不平衡并改善在低资源环境中分类模型性能方面的适用性。该研究采用广义线性模型算法进行类别平衡实验，采用随机森林算法进行低资源设置实验，以评估在不同训练数据下的模型性能。所有分类模型的主要评估指标是召回率。类别平衡实验的结果表明，在GAN平衡数据上训练的GLM模型实现了最高的召回率。同样在低资源实验中，训练在GAN合成数据上的模型展现出比原始数据更好的召回率。这些发现展示了GAN生成的合成数据在解决分类任务中不平衡数据的挑战和改善低资源环境中模型性能的潜力。

    The present study aimed to address the issue of imbalanced data in classification tasks and evaluated the suitability of SMOTE, ADASYN, and GAN techniques in generating synthetic data to address the class imbalance and improve the performance of classification models in low-resource settings. The study employed the Generalised Linear Model (GLM) algorithm for class balancing experiments and the Random Forest (RF) algorithm for low-resource setting experiments to assess model performance under varying training data. The recall metric was the primary evaluation metric for all classification models. The results of the class balancing experiments showed that the GLM model trained on GAN-balanced data achieved the highest recall value. Similarly, in low-resource experiments, models trained on data enhanced with GAN-synthesized data exhibited better recall values than original data. These findings demonstrate the potential of GAN-generated synthetic data for addressing the challenge of imbal
    
[^46]: 针对非线性、非平稳和随机系统的凸数据驱动逆最优控制研究

    On Convex Data-Driven Inverse Optimal Control for Nonlinear, Non-stationary and Stochastic Systems. (arXiv:2306.13928v1 [math.OC])

    [http://arxiv.org/abs/2306.13928](http://arxiv.org/abs/2306.13928)

    本文提出了一个凸数据驱动逆最优控制方案，能够有效解决非线性、非平稳和随机系统下的成本估计问题。

    

    本文主要论述了一个有限时域的逆控制问题，其目的是从观测值中推断出驱动智能体行动的成本，即使这个成本是非凸和非平稳的，同时受到非线性、非平稳和随机因素的影响。在这种情况下，我们提出了一个解决方案，通过解决一个优化问题来实现成本估计，即使代理成本不是凸的，本文也能够生成凸问题。为了得出这个结果，我们还研究了一个以随机策略为决策变量的有限时域前向控制问题，并给出了最优解的显式表达式。此外，我们将我们的发现转化为算法流程，并通过虚拟实验和真实硬件实验验证了我们的方法的有效性。所有的实验结果都证实了我们方法的有效性。

    This paper is concerned with a finite-horizon inverse control problem, which has the goal of inferring, from observations, the possibly non-convex and non-stationary cost driving the actions of an agent. In this context, we present a result that enables cost estimation by solving an optimization problem that is convex even when the agent cost is not and when the underlying dynamics is nonlinear, non-stationary and stochastic. To obtain this result, we also study a finite-horizon forward control problem that has randomized policies as decision variables. For this problem, we give an explicit expression for the optimal solution. Moreover, we turn our findings into algorithmic procedures and we show the effectiveness of our approach via both in-silico and experimental validations with real hardware. All the experiments confirm the effectiveness of our approach.
    
[^47]: 图神经网络从结构信息中获益的证明：一个特征学习的视角

    Graph Neural Networks Provably Benefit from Structural Information: A Feature Learning Perspective. (arXiv:2306.13926v1 [cs.LG])

    [http://arxiv.org/abs/2306.13926](http://arxiv.org/abs/2306.13926)

    本论文研究了GNN在神经网络特征学习理论中的作用。 发现图卷积网络显著增强了良性过拟合区域，在这个区域内信号学习超越了噪声记忆。

    

    图神经网络(GNNs)在图表示学习方面取得了先驱性进展，在处理图输入时表现出比多层感知器(MLPs)更优越的特征学习和性能。然而，理解GNN的特征学习方面仍处于初始阶段。本研究旨在通过使用梯度下降训练研究图卷积在神经网络特征学习理论中的作用来弥补这一差距。我们提供了对两层图卷积网络(GCNs)中信号学习和噪声记忆的不同刻画，并将它们与两层卷积神经网络(CNNs)进行对比。我们的研究结果表明，与对应的CNNs相比，图卷积网络显著增强了良性过拟合区域，在这个区域内信号学习超越了噪声记忆，并且近似于因子$\sqrt{D}^{q-2}$，其中$D$表示节点的期望度数，$q$表示ReLU激活功能的幂次。

    Graph neural networks (GNNs) have pioneered advancements in graph representation learning, exhibiting superior feature learning and performance over multilayer perceptrons (MLPs) when handling graph inputs. However, understanding the feature learning aspect of GNNs is still in its initial stage. This study aims to bridge this gap by investigating the role of graph convolution within the context of feature learning theory in neural networks using gradient descent training. We provide a distinct characterization of signal learning and noise memorization in two-layer graph convolutional networks (GCNs), contrasting them with two-layer convolutional neural networks (CNNs). Our findings reveal that graph convolution significantly augments the benign overfitting regime over the counterpart CNNs, where signal learning surpasses noise memorization, by approximately factor $\sqrt{D}^{q-2}$, with $D$ denoting a node's expected degree and $q$ being the power of the ReLU activation function where 
    
[^48]: 使用旋转等变对比学习构建表示几何

    Structuring Representation Geometry with Rotationally Equivariant Contrastive Learning. (arXiv:2306.13924v1 [cs.LG])

    [http://arxiv.org/abs/2306.13924](http://arxiv.org/abs/2306.13924)

    本文在对比学习中引入了等变性目标并理论证明了最优解会将输入空间的增强变换对应于球形嵌入空间上的旋转变换，而我们的方法CARE获得更好的嵌入表示能力。

    

    自我监督学习将原始感知数据（如图像）转换为一个紧凑的空间，在这个空间中，简单的欧几里得距离可以衡量数据的有意义变化。本文通过将输入空间的变换对应于嵌入空间的简单（即线性）变换，来增加嵌入空间的额外几何结构。特别地，在对比学习场景下，我们引入等变性目标，并在理论上证明了最小值强制输入空间的增强变换对应于球形嵌入空间上的旋转变换。我们展示了仅将等变损失与非折叠项相结合可以导致非平凡的表示，而无需对数据增强具有不变性。通过鼓励近似不变性，即输入增强对应于小的旋转，可以实现最优性能。我们的方法CARE：通过对比增强诱导旋转等变性，提供更好的嵌入表示能力。

    Self-supervised learning converts raw perceptual data such as images to a compact space where simple Euclidean distances measure meaningful variations in data. In this paper, we extend this formulation by adding additional geometric structure to the embedding space by enforcing transformations of input space to correspond to simple (i.e., linear) transformations of embedding space. Specifically, in the contrastive learning setting, we introduce an equivariance objective and theoretically prove that its minima forces augmentations on input space to correspond to rotations on the spherical embedding space. We show that merely combining our equivariant loss with a non-collapse term results in non-trivial representations, without requiring invariance to data augmentations. Optimal performance is achieved by also encouraging approximate invariance, where input augmentations correspond to small rotations. Our method, CARE: Contrastive Augmentation-induced Rotational Equivariance, leads to im
    
[^49]: 自动驾驶模拟中的主动数据采集

    Active Data Acquisition in Autonomous Driving Simulation. (arXiv:2306.13923v1 [cs.LG])

    [http://arxiv.org/abs/2306.13923](http://arxiv.org/abs/2306.13923)

    本文提出了一种主动数据采集策略来解决自动驾驶模拟中大量冗余数据集的问题，并通过实验验证，该策略可以显著降低标注成本和数据集大小，同时提高数据集的整体质量，从而提高自动驾驶系统的性能。

    

    自动驾驶算法高度依赖基于学习的模型，这些模型需要大量的数据集进行训练。然而，这些数据集通常包含大量冗余信息，而且收集和处理这些数据集可能非常耗时和昂贵。为了解决这个问题，本文提出了主动数据采集策略的概念。对于高质量的数据，增加采集密度可以提高数据集的整体质量，最终实现与原始数据集类似甚至更好的结果，同时降低标注成本和数据集大小。本文设计了实验来验证收集的数据集的质量，并演示该策略可以显著降低标注成本和数据集大小，同时提高数据集的整体质量，从而提高自动驾驶系统的性能。实现所提出方法的源代码可在https://github.com/Th1nkMore公开获取。

    Autonomous driving algorithms rely heavily on learning-based models, which require large datasets for training. However, there is often a large amount of redundant information in these datasets, while collecting and processing these datasets can be time-consuming and expensive. To address this issue, this paper proposes the concept of an active data-collecting strategy. For high-quality data, increasing the collection density can improve the overall quality of the dataset, ultimately achieving similar or even better results than the original dataset with lower labeling costs and smaller dataset sizes. In this paper, we design experiments to verify the quality of the collected dataset and to demonstrate this strategy can significantly reduce labeling costs and dataset size while improving the overall quality of the dataset, leading to better performance of autonomous driving systems. The source code implementing the proposed approach is publicly available on https://github.com/Th1nkMore
    
[^50]: 多任务多站地震监测：一种全能的地震相位拾取、定位和关联网络（PLAN）

    Multi-task multi-station earthquake monitoring: An all-in-one seismic Phase picking, Location, and Association Network (PLAN). (arXiv:2306.13918v1 [physics.geo-ph])

    [http://arxiv.org/abs/2306.13918](http://arxiv.org/abs/2306.13918)

    这项研究提出了一种使用图神经网络直接处理多站地震数据，实现同时进行相位拾取、关联和定位的方法，结合了站之间和任务之间的物理关系，在对比数据后证明具有较高的性能表现。

    

    地震监测对于理解地震物理和评估地震危害至关重要。标准的监测工作流包括相位拾取、关联和定位这些相互关联、相互依赖的任务。虽然深度学习方法已成功应用于地震监测，但它们大多分别解决不同的任务，并忽略了站之间的地理关系。在这里，我们提出了一种直接在多站地震数据上操作的图神经网络，实现了同时进行相位拾取、关联和定位。特别地，这个网络结构中考虑了站点之间和任务之间的物理关系，以促进交叉站点和交叉任务预测的准确性、可解释性和物理一致性。在对Ridgecrest区域和日本地区的数据应用时，该方法表现出比先前基于深度学习的相位拾取和定位方法更优秀的性能。总的来说，我们的研究提供了一种通用的地震监测解决方案。

    Earthquake monitoring is vital for understanding the physics of earthquakes and assessing seismic hazards. A standard monitoring workflow includes the interrelated and interdependent tasks of phase picking, association, and location. Although deep learning methods have been successfully applied to earthquake monitoring, they mostly address the tasks separately and ignore the geographic relationships among stations. Here, we propose a graph neural network that operates directly on multi-station seismic data and achieves simultaneous phase picking, association, and location. Particularly, the inter-station and inter-task physical relationships are informed in the network architecture to promote accuracy, interpretability, and physical consistency among cross-station and cross-task predictions. When applied to data from the Ridgecrest region and Japan regions, this method showed superior performance over previous deep learning-based phase-picking and localization methods. Overall, our stu
    
[^51]: G-TRACER: 预期清晰度优化

    G-TRACER: Expected Sharpness Optimization. (arXiv:2306.13914v1 [stat.ML])

    [http://arxiv.org/abs/2306.13914](http://arxiv.org/abs/2306.13914)

    G-TRACER是一种正则化深度学习结构优化方案，重点解决低信噪比问题，能有效促进泛化并获得了竞争性能。

    

    我们提出了一种新的深度学习结构优化正则化方案，G-TRACER（"Geometric TRACE Ratio"），通过寻求平坦最小值促进泛化，并具有以广义Bayes目标的自然梯度下降为基础的理论基础。通过使用TRACER增加损失函数，曲率正则化优化器（例如SGD-TRACER和Adam-TRACER）可以作为现有优化器的修改简单地实现，不需要进行广泛的调整。我们展示了该方法收敛于未正则化目标的局部最小值附近（取决于正则化强度的邻域范围），并在许多基准计算机视觉和NLP数据集上展示了竞争性能，特别关注挑战性的低信噪比问题。

    We propose a new regularization scheme for the optimization of deep learning architectures, G-TRACER ("Geometric TRACE Ratio"), which promotes generalization by seeking flat minima, and has a sound theoretical basis as an approximation to a natural-gradient descent based optimization of a generalized Bayes objective. By augmenting the loss function with a TRACER, curvature-regularized optimizers (eg SGD-TRACER and Adam-TRACER) are simple to implement as modifications to existing optimizers and don't require extensive tuning. We show that the method converges to a neighborhood (depending on the regularization strength) of a local minimum of the unregularized objective, and demonstrate competitive performance on a number of benchmark computer vision and NLP datasets, with a particular focus on challenging low signal-to-noise ratio problems.
    
[^52]: 利用生成模型进行语义轨迹分析的时空叙事？

    Spatio-temporal Storytelling? Leveraging Generative Models for Semantic Trajectory Analysis. (arXiv:2306.13905v1 [cs.CL])

    [http://arxiv.org/abs/2306.13905](http://arxiv.org/abs/2306.13905)

    本文通过生成语言模型对语义轨迹进行分析和生成合成语义轨迹数据，为从城市规划到个性化推荐引擎和商业策略等一系列应用做出贡献。

    

    本文提出了一种使用生成语言模型对语义轨迹跟踪进行分析和生成合成语义轨迹数据（SST）的愿景。利用深度学习的进步，如自然语言处理（NLP）、计算机视觉等领域的进展，我们旨在创建智能模型，可以研究不同上下文中的语义轨迹，预测未来趋势，增强机器对动物、人类、货物等移动情况的理解，提高人机交互，并为从城市规划到个性化推荐引擎和商业策略等一系列应用做出贡献。

    In this paper, we lay out a vision for analysing semantic trajectory traces and generating synthetic semantic trajectory data (SSTs) using generative language model. Leveraging the advancements in deep learning, as evident by progress in the field of natural language processing (NLP), computer vision, etc. we intend to create intelligent models that can study the semantic trajectories in various contexts, predicting future trends, increasing machine understanding of the movement of animals, humans, goods, etc. enhancing human-computer interactions, and contributing to an array of applications ranging from urban-planning to personalized recommendation engines and business strategy.
    
[^53]: 具有共识算法的差分隐私分散深度学习

    Differentially Private Decentralized Deep Learning with Consensus Algorithms. (arXiv:2306.13892v1 [cs.LG])

    [http://arxiv.org/abs/2306.13892](http://arxiv.org/abs/2306.13892)

    本论文提出了一种具有差分隐私保护的分散学习算法，可用于合作分散深度学习，防止共享模型参数时泄露私有数据集的信息。

    

    合作分散深度学习依赖于通信代理之间的直接信息交换，每个代理都可以访问应该保持私有的本地数据集。目标是在训练后使得所有代理在模型参数上达成共识。然而，与不可信的邻居代理共享参数可能会泄露有关本地数据集的可利用信息。为了解决这个问题，我们介绍了一种差分隐私分散学习方法，以在合作训练期间和之后保护每个代理的本地数据集。在我们的方法中，我们将常用于集中式深度学习的差分隐私随机梯度下降（DP-SGD）泛化到实用的基于子梯度和ADMM的分散学习方法中。我们的算法的差分隐私保证适用于任意深度学习目标函数，并分析了强凸目标函数的收敛性质。我们将我们的算法与其他差分隐私算法进行比较。

    Cooperative decentralized deep learning relies on direct information exchange between communicating agents, each with access to a local dataset which should be kept private. The goal is for all agents to achieve consensus on model parameters after training. However, sharing parameters with untrustworthy neighboring agents could leak exploitable information about local datasets. To combat this, we introduce differentially private decentralized learning that secures each agent's local dataset during and after cooperative training. In our approach, we generalize Differentially Private Stochastic Gradient Descent (DP-SGD) -- a popular differentially private training method for centralized deep learning -- to practical subgradient- and ADMM-based decentralized learning methods. Our algorithms' differential privacy guarantee holds for arbitrary deep learning objective functions, and we analyze the convergence properties for strongly convex objective functions. We compare our algorithms again
    
[^54]: L3Cube-MahaSent-MD：一种多域 Marathi 情感分析数据集和 Transformer 模型

    L3Cube-MahaSent-MD: A Multi-domain Marathi Sentiment Analysis Dataset and Transformer Models. (arXiv:2306.13888v1 [cs.CL])

    [http://arxiv.org/abs/2306.13888](http://arxiv.org/abs/2306.13888)

    本论文介绍了 L3Cube-MahaSent-MD，这是一个多领域的 Marathi 情感分析数据集。在其中，针对每个领域，我们构建了包含 1.5 万个样本的子数据集。我们微调了不同的单语和多语 BERT 模型，并报告了 MahaBERT 模型的最佳准确性。我们的研究提出了利用低资源多领域数据集进行情感分析的需求。

    

    在低资源语言（如 Marathi）中进行情感分析一直受到相应数据集的限制。本文提出了 L3Cube-MahaSent-MD，它是一个多领域 Marathi 情感分析数据集，包括电影评论、普通推文、电视节目字幕和政治推文等四个不同领域。该数据集包含约 6 万个手动标记的样本，覆盖了三种不同情感 - 正面、负面和中性。我们针对每个领域创建了一个子数据集，每个子数据集包含 1.5 万个样本。这是 Indic 情感领域内的第一个全面的多领域情感分析数据集。我们针对这些数据集微调了不同的单语和多语 BERT 模型，并报告了 MahaBERT 模型的最佳准确性。我们还进行了广泛的领域内和领域间分析，从而凸显了低资源多域数据集的需求。数据集和模型均可在 https://github.com/l3cube-pune/MarathiN 获取。

    The exploration of sentiment analysis in low-resource languages, such as Marathi, has been limited due to the availability of suitable datasets. In this work, we present L3Cube-MahaSent-MD, a multi-domain Marathi sentiment analysis dataset, with four different domains - movie reviews, general tweets, TV show subtitles, and political tweets. The dataset consists of around 60,000 manually tagged samples covering 3 distinct sentiments - positive, negative, and neutral. We create a sub-dataset for each domain comprising 15k samples. The MahaSent-MD is the first comprehensive multi-domain sentiment analysis dataset within the Indic sentiment landscape. We fine-tune different monolingual and multilingual BERT models on these datasets and report the best accuracy with the MahaBERT model. We also present an extensive in-domain and cross-domain analysis thus highlighting the need for low-resource multi-domain datasets. The data and models are available at https://github.com/l3cube-pune/MarathiN
    
[^55]: 使用PINNs进行电流密度阻抗成像

    Current density impedance imaging with PINNs. (arXiv:2306.13881v1 [math.NA])

    [http://arxiv.org/abs/2306.13881](http://arxiv.org/abs/2306.13881)

    本文提出了一种使用PINNs求解CDII的计算高效方法，该方法通过构造物理启发式损失函数实现正则化的最小二乘输出函数与基本微分方程的耦合。数值模拟结果表明其具有高效、准确和鲁棒性。

    

    本文介绍了CDII-PINNs方法，它是一种在Tikhonov正则化框架下使用PINNs求解CDII的计算高效方法。该方法通过将正则化的最小二乘输出函数与描述电导和电压之间关系的基本微分方程相结合，构造了一个物理启发式损失函数。表示电导和电压的一对神经网络通过该损失函数进行耦合。最小化损失函数提供了重建结果。我们提供了严格的理论保证。我们通过对网络参数的先验选择，给出了CDII-PINNs的误差分析和收敛速率，这是依据样本数量而定的。数值模拟表明，CDII-PINNs具有高效、准确和鲁棒性，可以适用于$1\%$到$20\%$的噪声水平。

    In this paper, we introduce CDII-PINNs, a computationally efficient method for solving CDII using PINNs in the framework of Tikhonov regularization. This method constructs a physics-informed loss function by merging the regularized least-squares output functional with an underlying differential equation, which describes the relationship between the conductivity and voltage. A pair of neural networks representing the conductivity and voltage, respectively, are coupled by this loss function. Then, minimizing the loss function provides a reconstruction. A rigorous theoretical guarantee is provided. We give an error analysis for CDII-PINNs and establish a convergence rate, based on prior selected neural network parameters in terms of the number of samples. The numerical simulations demonstrate that CDII-PINNs are efficient, accurate and robust to noise levels ranging from $1\%$ to $20\%$.
    
[^56]: 基于行为查询的Transformer在深度强化学习中的可视化解释

    Action Q-Transformer: Visual Explanation in Deep Reinforcement Learning with Encoder-Decoder Model using Action Query. (arXiv:2306.13879v1 [cs.LG])

    [http://arxiv.org/abs/2306.13879](http://arxiv.org/abs/2306.13879)

    该论文提出了一个基于行为查询的Transformer编码器-解码器结构方法（AQT），用于生成深度强化学习模型，该模型更具可解释性，可以具体描述智能体的决策过程。

    

    Transformer在监督学习中的出色表现引起了人们对其在深度强化学习(DRL)中应用的日益关注，以在各种问题上实现高性能。然而，DRL智能体的决策过程是一个黑盒子，极大地阻碍了应用智能体到实际问题的发展。为了解决这个问题，我们提出了Action Q-Transformer (AQT)，它引入了一个Transformer编码器-解码器结构到基于Q-学习的DRL方法中。在AQT中，编码器计算状态值函数，解码器计算优势函数，以促进学习特定行为的不同注意力的获取。AQT中的解码器使用行为查询作为查询，代表每个行为的信息。这使我们能够获得表示状态值和每个行为的注意力。通过获取和可视化这些注意力，我们实现了更解释性和透明的DRL模型，并详细描述了智能体的决策过程。

    The excellent performance of Transformer in supervised learning has led to growing interest in its potential application to deep reinforcement learning (DRL) to achieve high performance on a wide variety of problems. However, the decision making of a DRL agent is a black box, which greatly hinders the application of the agent to real-world problems. To address this problem, we propose the Action Q-Transformer (AQT), which introduces a transformer encoder-decoder structure to Q-learning based DRL methods. In AQT, the encoder calculates the state value function and the decoder calculates the advantage function to promote the acquisition of different attentions indicating the agent's decision-making. The decoder in AQT utilizes action queries, which represent the information of each action, as queries. This enables us to obtain the attentions for the state value and for each action. By acquiring and visualizing these attentions that detail the agent's decision-making, we achieve a DRL mod
    
[^57]: 物理信息机器学习用于动态系统建模与控制

    Physics-Informed Machine Learning for Modeling and Control of Dynamical Systems. (arXiv:2306.13867v1 [eess.SY])

    [http://arxiv.org/abs/2306.13867](http://arxiv.org/abs/2306.13867)

    物理信息机器学习是将机器学习算法与物理约束和抽象数学模型相结合，通过额外信息进行训练以获取更有效、物理上一致、数据效率更高的模型的方法。本文概述了物理信息机器学习在动态系统建模和控制中的最新进展，并讨论了领域面临的重要挑战和未来方向。

    

    物理信息机器学习(PIML)是一组方法和工具，系统地将机器学习(ML)算法与物理约束和科学工程领域中开发的抽象数学模型相结合。与纯数据驱动方法相反，PIML模型可以通过强制执行能量和质量守恒等物理定律获得额外的信息进行训练。广义上讲，PIML模型可以包括稳定性、凸性或不变性等抽象属性和条件。PIML的基本前提是将ML与物理相结合可以产生更有效、物理上一致且数据效率更高的模型。本文旨在为动态系统建模和控制中最新的PIML进展提供类似于教程式的概述。具体而言，本文涵盖以下主题的理论、基本概念和方法、工具和应用的概述：1）用于系统识别的物理信息学习；2）用于预测和控制任务的物理信息学习；3）ML算法的物理引导设计。本文还讨论了PIML领域的重要挑战和未来方向。

    Physics-informed machine learning (PIML) is a set of methods and tools that systematically integrate machine learning (ML) algorithms with physical constraints and abstract mathematical models developed in scientific and engineering domains. As opposed to purely data-driven methods, PIML models can be trained from additional information obtained by enforcing physical laws such as energy and mass conservation. More broadly, PIML models can include abstract properties and conditions such as stability, convexity, or invariance. The basic premise of PIML is that the integration of ML and physics can yield more effective, physically consistent, and data-efficient models. This paper aims to provide a tutorial-like overview of the recent advances in PIML for dynamical system modeling and control. Specifically, the paper covers an overview of the theory, fundamental concepts and methods, tools, and applications on topics of: 1) physics-informed learning for system identification; 2) physics-in
    
[^58]: 相似性保持对抗图对比学习

    Similarity Preserving Adversarial Graph Contrastive Learning. (arXiv:2306.13854v1 [cs.LG])

    [http://arxiv.org/abs/2306.13854](http://arxiv.org/abs/2306.13854)

    本文提出了一种相似性保持的对抗图对比学习（SP-AGCL）框架，可以实现对抗攻击的对抗鲁棒性，同时保持节点特征相似性。

    

    最近的研究表明，图神经网络模型易受到对抗攻击，即对图结构和节点特征进行微小扰动。在各种图神经网络模型中，基于图对比学习（GCL）的方法特别容易受到对抗攻击，因为它们的固有设计高度依赖于从原始图派生出的自监督信号，然而当图受到攻击时，原始图中已经包含了噪声。为了实现对这种攻击的对抗鲁棒性，现有方法将对抗训练（AT）应用于GCL框架，将攻击的图作为GCL框架下的增强。然而，我们发现现有的经过对抗训练的GCL方法在保持节点特征相似性方面付出了代价。在本文中，我们提出了一种相似性保持的对抗图对比学习（SP-AGCL）框架，将干净的图与两个不同视图的辅助图进行对比。

    Recent works demonstrate that GNN models are vulnerable to adversarial attacks, which refer to imperceptible perturbation on the graph structure and node features. Among various GNN models, graph contrastive learning (GCL) based methods specifically suffer from adversarial attacks due to their inherent design that highly depends on the self-supervision signals derived from the original graph, which however already contains noise when the graph is attacked. To achieve adversarial robustness against such attacks, existing methods adopt adversarial training (AT) to the GCL framework, which considers the attacked graph as an augmentation under the GCL framework. However, we find that existing adversarially trained GCL methods achieve robustness at the expense of not being able to preserve the node feature similarity. In this paper, we propose a similarity-preserving adversarial graph contrastive learning (SP-AGCL) framework that contrasts the clean graph with two auxiliary views of differe
    
[^59]: 一种通过镜面下降控制隐式正则化的统一方法

    A Unified Approach to Controlling Implicit Regularization via Mirror Descent. (arXiv:2306.13853v1 [cs.LG])

    [http://arxiv.org/abs/2306.13853](http://arxiv.org/abs/2306.13853)

    本文提出了一种使用镜面下降方法来统一控制回归和分类问题中的隐式正则化的方法，在所有标准几何下都可以实现$\ell_p$（$p\in[1,\infty]$）范式的隐式正则化，并且可以实现学习理论中许多特殊类的控制的泛化保证。

    

    受深度神经网络的显著成功启发，人们对超参数模型的泛化性能产生了极大的兴趣。人们花费了大量精力来确定优化算法通过其“首选”解如何影响泛化，这种现象通常被称为隐式正则化。特别地，已经有人论证梯度下降（GD）在回归和分类问题中会引起隐式的$\ell_2$ -范数正则化。然而，不同算法的隐式正则化受限于特定的几何或特定类的学习问题，表明需要一个通用的方法来控制隐式正则化。为此，我们提出了一个统一的方法，使用镜面下降（MD）来控制回归和分类设置中的隐式正则化。具体而言，我们表明，MD与通用的下降方向一起使用时，在所有标准几何下都可以实现$\ell_p$（$p\in[1,\infty]$）范式的隐式正则化，并且在学习理论中使用的许多特殊类中也可以实现控制的泛化保证。

    Inspired by the remarkable success of deep neural networks, there has been significant interest in understanding the generalization performance of overparameterized models. Substantial efforts have been invested in characterizing how optimization algorithms impact generalization through their "preferred" solutions, a phenomenon commonly referred to as implicit regularization. In particular, it has been argued that gradient descent (GD) induces an implicit $\ell_2$-norm regularization in regression and classification problems. However, the implicit regularization of different algorithms are confined to either a specific geometry or a particular class of learning problems, indicating a gap in a general approach for controlling the implicit regularization. To address this, we present a unified approach using mirror descent (MD), a notable generalization of GD, to control implicit regularization in both regression and classification settings. More specifically, we show that MD with the gen
    
[^60]: 预训练真的比元学习更好吗？

    Is Pre-training Truly Better Than Meta-Learning?. (arXiv:2306.13841v1 [cs.LG])

    [http://arxiv.org/abs/2306.13841](http://arxiv.org/abs/2306.13841)

    在少样本学习中，当数据集的正式多样性较低时，预训练模型（PT）胜过模型无关元学习（MAML）。当正式多样性较高时，MAML更好。

    

    在少样本学习的背景下，目前普遍认为固定的预训练模型（PT）加上在评价时微调最后一层，胜过标准的元学习算法。我们通过深入的实证研究和广泛的数据集比较PT和模型无关元学习（MAML）这些说法。与以前的工作不同，我们强调使用相同的体系结构、相同的优化器，以及所有模型都训练到收敛。关键地，我们使用一个更严格的统计工具——效应量（Cohen's d）——来确定使用PT与使用MAML之间的模型差异的实际意义。然后使用一个预先提出的度量——多样性系数——来计算数据集的平均正式多样性。使用这种分析，我们证明了以下事实：1. 当数据集的正式多样性较低时，PT在平均意义上胜过MAML；2. 当正式多样性较高时，MAML胜过PT。

    In the context of few-shot learning, it is currently believed that a fixed pre-trained (PT) model, along with fine-tuning the final layer during evaluation, outperforms standard meta-learning algorithms. We re-evaluate these claims under an in-depth empirical examination of an extensive set of formally diverse datasets and compare PT to Model Agnostic Meta-Learning (MAML). Unlike previous work, we emphasize a fair comparison by using: the same architecture, the same optimizer, and all models trained to convergence. Crucially, we use a more rigorous statistical tool -- the effect size (Cohen's d) -- to determine the practical significance of the difference between a model trained with PT vs. a MAML. We then use a previously proposed metric -- the diversity coefficient -- to compute the average formal diversity of a dataset. Using this analysis, we demonstrate the following: 1. when the formal diversity of a data set is low, PT beats MAML on average and 2. when the formal diversity is hi
    
[^61]: 超越规模：多样性系数作为数据质量指标证明了LLMs是在形式多样的数据上预先训练的

    Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])

    [http://arxiv.org/abs/2306.13840](http://arxiv.org/abs/2306.13840)

    本论文提出使用多样性系数作为LLM预训练数据质量的指标，研究表明公开可用的LLM数据集的多样性系数很高。

    

    当前，预先训练强大的大语言模型(LLMs)的趋势主要集中在模型和数据集规模的扩大。然而，预先训练数据的质量对于训练强大的LLMs来说是一个重要因素，但它是一个模糊的概念，尚未完全表征。因此，我们使用最近提出的Task2Vec多样性系数来基于数据质量的形式方面，超越规模本身。具体而言，我们测量公开可用的预先训练数据集的多样性系数，以证明它们的形式多样性高于理论的下限和上限。此外，为了建立对多样性系数的信心，我们进行可解释性实验，并发现该系数与多样性的直观属性相吻合，例如，随着潜在概念数量的增加，它增加。我们得出结论，多样性系数是可靠的，表明公开可用的LLM数据集的多样性系数很高，并推测它可以作为预训练LLMs模型的数据质量指标。

    Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be
    
[^62]: Computron: 利用模型并行交换服务于分布式深度学习模型

    Computron: Serving Distributed Deep Learning Models with Model Parallel Swapping. (arXiv:2306.13835v1 [cs.DC])

    [http://arxiv.org/abs/2306.13835](http://arxiv.org/abs/2306.13835)

    该论文介绍了Computron系统，该系统使用内存交换实现模型并行交换设计来服务于多个分布式深度学习模型，在处理多个大型模型任务时可提高资源利用率和交换速度。

    

    当今许多以语言和图像为代表的表现最佳的深度学习模型都是包含数十亿参数的调整型模型。为了应对需要将这些大型模型服务于处理不同任务的工作负载，我们开发了Computron，这个系统使用内存交换在一个共享的GPU集群上服务于多个分布式模型。Computron 实现了一个模型并行交换设计，利用集群的总CPU-GPU链接带宽加快模型参数传输速度。该设计使交换大型模型成为可能，并可以提高资源利用率。我们展示了Computron成功地在多个GPU上并行化了模型交换，并对随机负载进行了测试，展示了如何容忍现实世界中的变化因素，例如突发性和偏斜的请求速率。Computron的源代码可在https://github.com/dlzou/computron中获得。

    Many of the most performant deep learning models today in fields like language and image understanding are fine-tuned models that contain billions of parameters. In anticipation of workloads that involve serving many of such large models to handle different tasks, we develop Computron, a system that uses memory swapping to serve multiple distributed models on a shared GPU cluster. Computron implements a model parallel swapping design that takes advantage of the aggregate CPU-GPU link bandwidth of a cluster to speed up model parameter transfers. This design makes swapping large models feasible and can improve resource utilization. We demonstrate that Computron successfully parallelizes model swapping on multiple GPUs, and we test it on randomized workloads to show how it can tolerate real world variability factors like burstiness and skewed request rates. Computron's source code is available at https://github.com/dlzou/computron.
    
[^63]: Minigrid & Miniworld: 模块化和可定制的强化学习环境，用于目标导向型任务

    Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks. (arXiv:2306.13831v1 [cs.LG])

    [http://arxiv.org/abs/2306.13831](http://arxiv.org/abs/2306.13831)

    Minigrid & Miniworld能够提供一系列面向目标的2D和3D环境，通过采用极简主义的设计范式，使其易于定制和开发，这已经被RL社区广泛采用。另外，统一的API还能够实现在不同观测空间之间进行转移学习。

    

    我们提出了Minigrid和Miniworld库，这两个库提供了一套面向目标的2D和3D环境。这些库是明确采用极简主义设计范式而创建的，以允许用户快速为各种研究特定需求开发新环境，因此它们已被RL社区广泛采用，促进了各种领域的研究。在本文中，我们概述了设计理念、环境细节以及它们的世界生成API。我们还通过案例研究展示了Minigrid和Miniworld之间统一API带来的附加功能，包括在不同观测空间之间进行转移学习（针对RL代理和人类）。Minigrid和Miniworld的源代码可在https://github.com/Farama-Foundation/{Minigrid, Miniworld}找到，文档可以在https://{minigrid, miniworld}.farama.org/找到。

    We present the Minigrid and Miniworld libraries which provide a suite of goal-oriented 2D and 3D environments. The libraries were explicitly created with a minimalistic design paradigm to allow users to rapidly develop new environments for a wide range of research-specific needs. As a result, both have received widescale adoption by the RL community, facilitating research in a wide range of areas. In this paper, we outline the design philosophy, environment details, and their world generation API. We also showcase the additional capabilities brought by the unified API between Minigrid and Miniworld through case studies on transfer learning (for both RL agents and humans) between the different observation spaces. The source code of Minigrid and Miniworld can be found at https://github.com/Farama-Foundation/{Minigrid, Miniworld} along with their documentation at https://{minigrid, miniworld}.farama.org/.
    
[^64]: 机载环境影响的度量学习分析

    Aircraft Environmental Impact Segmentation via Metric Learning. (arXiv:2306.13830v1 [cs.LG])

    [http://arxiv.org/abs/2306.13830](http://arxiv.org/abs/2306.13830)

    本文介绍了机载环境影响建模中的度量学习方法，在弱监督度量学习任务下取得了显著性能提升，有望实现对机载环境影响更高效、更精准的建模。

    

    度量学习是指为特定任务学习定制距离度量的过程。这一高级机器学习子领域对于依靠计算对象之间距离或相似度进行机器学习或数据挖掘任务的任何应用都是有用的。近年来，机器学习技术已被广泛应用于航空航天工程中，用于预测、提取模式、发现知识等。然而，度量学习作为一个可以提高复杂机器学习任务性能的元素，迄今在相关文献中很少被使用。本研究将经典的度量学习公式与新颖的元素应用于机载环境影响建模中，并通过弱监督度量学习任务，在新出现的机载环境影响描述和划分问题上实现了显著提高。这一结果将实现对机载环境影响更高效、更精准的建模，对于航空产业的可持续发展具有重要意义。

    Metric learning is the process of learning a tailored distance metric for a particular task. This advanced subfield of machine learning is useful to any machine learning or data mining task that relies on the computation of distances or similarities over objects. In recently years, machine learning techniques have been extensively used in aviation and aerospace engineering to make predictions, extract patterns, discover knowledge, etc. Nevertheless, metric learning, an element that can advance the performance of complex machine learning tasks, has so far been hardly utilized in relevant literature. In this study, we apply classic metric learning formulations with novel components on aviation environmental impact modeling. Through a weakly-supervised metric learning task, we achieve significant improvement in the newly emerged problem of aircraft characterization and segmentation for environmental impacts. The result will enable the more efficient and accurate modeling of aircraft envir
    
[^65]: 图神经网络的广义$f$-均值聚合

    Generalised $f$-Mean Aggregation for Graph Neural Networks. (arXiv:2306.13826v1 [cs.LG])

    [http://arxiv.org/abs/2306.13826](http://arxiv.org/abs/2306.13826)

    本文提出了一个广义聚合算子，GenAgg，它包括所有标准聚合器的函数空间。实验结果表明，GenAgg能够表示标准聚合器。

    

    图神经网络(GNN)架构由其更新和聚合模块的实现方式定义。许多工作都集中在新型参数化更新模块的方法上，而聚合模块相对较少受到关注。由于聚合函数很难参数化，目前大多数方法选择“标准聚合器”，如$\mathrm{mean}$、$\mathrm{sum}$或$\mathrm{max}$。尽管这种选择通常没有任何理由，但已经表明聚合器的选择对性能有重大影响，最佳聚合器的选择取决于问题。由于聚合是一种有损操作，选择最适合的聚合器以最小化信息丢失至关重要。本文提出了GenAgg，一种广义聚合运算符，它参数化了一个包括所有标准聚合器的函数空间。在我们的实验中，我们展示了GenAgg能够表示标准聚合器。

    Graph Neural Network (GNN) architectures are defined by their implementations of update and aggregation modules. While many works focus on new ways to parametrise the update modules, the aggregation modules receive comparatively little attention. Because it is difficult to parametrise aggregation functions, currently most methods select a "standard aggregator" such as $\mathrm{mean}$, $\mathrm{sum}$, or $\mathrm{max}$. While this selection is often made without any reasoning, it has been shown that the choice in aggregator has a significant impact on performance, and the best choice in aggregator is problem-dependent. Since aggregation is a lossy operation, it is crucial to select the most appropriate aggregator in order to minimise information loss. In this paper, we present GenAgg, a generalised aggregation operator, which parametrises a function space that includes all standard aggregators. In our experiments, we show that GenAgg is able to represent the standard aggregators with mu
    
[^66]: 使用时间融合变换器（TFT）升尺度全球小时级GPP估计

    Upscaling Global Hourly GPP with Temporal Fusion Transformer (TFT). (arXiv:2306.13815v1 [cs.LG])

    [http://arxiv.org/abs/2306.13815](http://arxiv.org/abs/2306.13815)

    本研究使用时间融合变换器（TFT）解决了过去 GPP 时间序列不足的缺陷，实现了非植被特征在升尺度 GPP 估计中的应用，并取得了良好的模型性能。

    

    可靠的总初级生产力（GPP）估计对于评估气候变化倡议至关重要，但目前仅来自分布稀疏的涡度协方差塔站点。这种限制阻碍了在区域和全球尺度上获得可靠的GPP量化。本研究通过Temporal Fusion Transformer（TFT）探索了一种新颖的升尺度解决方案，不依赖于过去的GPP时间序列。模型开发辅助使用了Random Forest Regressor （RFR）和XGBoost，随后是TFT和树算法的混合模型。最佳表现模型产生了0.704 NSE和3.54 RMSE的模型性能。该研究的另一个贡献是根据时间和通量塔站点对编码器特征重要性进行的分解分析。这种分析揭示了非植被相关特征（包括气象变量）在升尺度GPP估计中的重要性。

    Reliable estimates of Gross Primary Productivity (GPP), crucial for evaluating climate change initiatives, are currently only available from sparsely distributed eddy covariance tower sites. This limitation hampers access to reliable GPP quantification at regional to global scales. Prior machine learning studies on upscaling \textit{in situ} GPP to global wall-to-wall maps at sub-daily time steps faced limitations such as lack of input features at higher temporal resolutions and significant missing values. This research explored a novel upscaling solution using Temporal Fusion Transformer (TFT) without relying on past GPP time series. Model development was supplemented by Random Forest Regressor (RFR) and XGBoost, followed by the hybrid model of TFT and tree algorithms. The best preforming model yielded to model performance of 0.704 NSE and 3.54 RMSE. Another contribution of the study was the breakdown analysis of encoder feature importance based on time and flux tower sites. Such anal
    
[^67]: BatchGNN：高效地在超大规模图上进行基于CPU的分布式GNN训练

    BatchGNN: Efficient CPU-Based Distributed GNN Training on Very Large Graphs. (arXiv:2306.13814v1 [cs.LG])

    [http://arxiv.org/abs/2306.13814](http://arxiv.org/abs/2306.13814)

    BatchGNN是一个高效的分布式CPU系统，用于在超大规模图上训练GNN模型，通过宏批处理、集成的图分区和原生的GNN层实现以及缓存聚合的输入特征等技术，实现了高效的训练，并在多个指标上超越了其他分布式GPU系统。

    

    本文介绍了BatchGNN，一个分布式CPU系统，展示了一些技术，可以用于在大规模图上高效地训练GNN。通过宏批处理（macrobatching），将多个小批量的子图采样和特征提取批处理成一个通信中继，从而减少了输入特征静态时的冗余特征提取和通信开销。BatchGNN提供集成的图分区和原生的GNN层实现，以提高运行效率，并可以缓存聚合的输入特征以进一步减少采样开销。BatchGNN在OGBN图上训练的三个GNN模型中，平均速度比DistDGL快了$3\times$，超过了分布式GPU系统$P^3$和DistDGLv2报告的运行时，并可扩展到超大规模图。

    We present BatchGNN, a distributed CPU system that showcases techniques that can be used to efficiently train GNNs on terabyte-sized graphs. It reduces communication overhead with macrobatching in which multiple minibatches' subgraph sampling and feature fetching are batched into one communication relay to reduce redundant feature fetches when input features are static. BatchGNN provides integrated graph partitioning and native GNN layer implementations to improve runtime, and it can cache aggregated input features to further reduce sampling overhead. BatchGNN achieves an average $3\times$ speedup over DistDGL on three GNN models trained on OGBN graphs, outperforms the runtimes reported by distributed GPU systems $P^3$ and DistDGLv2, and scales to a terabyte-sized graph.
    
[^68]: 持续性深度学习中的可塑性维护

    Maintaining Plasticity in Deep Continual Learning. (arXiv:2306.13812v1 [cs.LG])

    [http://arxiv.org/abs/2306.13812](http://arxiv.org/abs/2306.13812)

    持续性学习中，深度学习系统可能会失去适应新数据的能力，我们提出了一种名为对比可塑性的方法来解决这个问题。

    

    现代深度学习系统专门用于一次性训练，而不是持续性学习，如果将深度学习系统应用于持续性学习中，则众所周知它们可能在记住早期的例子方面遭遇失败。更为基本但不为人知的是，它们也可能失去适应新数据的能力，这种现象被称为“可塑性丧失”。我们展示了使用MNIST和ImageNet数据集重构为一系列任务的持续学习中的可塑性丧失。在ImageNet中，二元分类的性能从一个早期任务的89％正确下降到77％，或者大约等于线性网络的水平。这种可塑性的丧失发生在各种深层网络架构，优化器和激活函数范围内，并且不会因批量归一化或放弃而得到缓解。在我们的实验中，通过我们提出的方法Contrastive Plasticity，可以缓解可塑性的丧失，该方法学习适应新的数据同时保留记住旧数据的能力。Contrastive Plasticity可以添加到任何神经网络中，而无需修改网络的架构，并带来非常少的计算开销。

    Modern deep-learning systems are specialized to problem settings in which training occurs once and then never again, as opposed to continual-learning settings in which training occurs continually. If deep-learning systems are applied in a continual learning setting, then it is well known that they may fail catastrophically to remember earlier examples. More fundamental, but less well known, is that they may also lose their ability to adapt to new data, a phenomenon called \textit{loss of plasticity}. We show loss of plasticity using the MNIST and ImageNet datasets repurposed for continual learning as sequences of tasks. In ImageNet, binary classification performance dropped from 89% correct on an early task down to 77%, or to about the level of a linear network, on the 2000th task. Such loss of plasticity occurred with a wide range of deep network architectures, optimizers, and activation functions, and was not eased by batch normalization or dropout. In our experiments, loss of plasti
    
[^69]: 大象与算法：人工智能在大象监测中的当前和未来作用综述

    Elephants and Algorithms: A Review of the Current and Future Role of AI in Elephant Monitoring. (arXiv:2306.13803v1 [cs.AI])

    [http://arxiv.org/abs/2306.13803](http://arxiv.org/abs/2306.13803)

    本文探讨了AI和ML在保护非洲保护区中至关重要的大象方面的作用。新的AI和ML技术提供了解决方案以处理并提取重要信息。在AI专家和生态研究人员之间的协作下，这些创新技术可以帮助增强野生动物保护，为其他物种设定先例。

    

    人工智能（AI）和机器学习（ML）为增进对动物行为和保护策略的理解提供了革命性机会。以非洲保护区中至关重要的大象为焦点，本文探讨了AI和ML在它们保护中的作用。给定从各种传感器（如摄像头、麦克风、地震仪、无人机和卫星）收集到的越来越多的数据，挑战在于管理和解读这些庞大的数据。新的AI和ML技术提供了简化这一过程的解决方案，帮助我们提取重要信息，否则可能会被忽视。本文重点介绍了不同的AI驱动监测方法及其在改善大象保护方面的潜力。AI专家和生态研究人员之间的协作是利用这些创新技术以增强野生动物保护的关键所在，为许多其他物种设定了先例。

    Artificial intelligence (AI) and machine learning (ML) present revolutionary opportunities to enhance our understanding of animal behavior and conservation strategies. Using elephants, a crucial species in Africa's protected areas, as our focal point, we delve into the role of AI and ML in their conservation. Given the increasing amounts of data gathered from a variety of sensors like cameras, microphones, geophones, drones, and satellites, the challenge lies in managing and interpreting this vast data. New AI and ML techniques offer solutions to streamline this process, helping us extract vital information that might otherwise be overlooked. This paper focuses on the different AI-driven monitoring methods and their potential for improving elephant conservation. Collaborative efforts between AI experts and ecological researchers are essential in leveraging these innovative technologies for enhanced wildlife conservation, setting a precedent for numerous other species.
    
[^70]: 乘客轨迹聚类的张量狄利克雷过程多项式混合模型

    Tensor Dirichlet Process Multinomial Mixture Model for Passenger Trajectory Clustering. (arXiv:2306.13794v1 [stat.ML])

    [http://arxiv.org/abs/2306.13794](http://arxiv.org/abs/2306.13794)

    提出了一种基于张量的狄利克雷过程多项式混合模型（Tensor-DPMM），通过张量保留了多维行程信息的多模式和分层结构，并以统一的一步方式进行乘客轨迹聚类，在自动确定聚类数方面具有优越性。

    

    基于出行记录的乘客聚类对于运输运营商至关重要。然而，现有方法由于乘客行程信息的分层结构而难以轻松进行乘客聚类，即：每个乘客有多次出行，每次出行包含多维多模式信息。此外，现有方法依赖于精确指定聚类数量开始，而每天有数百万通勤者使用交通系统时这是困难的。本文提出了一种新颖的基于张量的狄利克雷过程多项式混合模型（Tensor-DPMM），通过张量保留了多维行程信息的多模式和分层结构，并以统一的一步方式对它们进行聚类。该模型还通过使用狄利克雷过程决定乘客分配至现有聚类还是形成新聚类的概率，自动确定聚类数的能力。在真实的交通运输数据集上的实验表明，我们提出的Tensor-DPMM模型在乘客轨迹聚类方面具有优越性。

    Passenger clustering based on travel records is essential for transportation operators. However, existing methods cannot easily cluster the passengers due to the hierarchical structure of the passenger trip information, namely: each passenger has multiple trips, and each trip contains multi-dimensional multi-mode information. Furthermore, existing approaches rely on an accurate specification of the clustering number to start, which is difficult when millions of commuters are using the transport systems on a daily basis. In this paper, we propose a novel Tensor Dirichlet Process Multinomial Mixture model (Tensor-DPMM), which is designed to preserve the multi-mode and hierarchical structure of the multi-dimensional trip information via tensor, and cluster them in a unified one-step manner. The model also has the ability to determine the number of clusters automatically by using the Dirichlet Process to decide the probabilities for a passenger to be either assigned in an existing cluster 
    
[^71]: QNNRepair：量化神经网络修复的方法

    QNNRepair: Quantized Neural Network Repair. (arXiv:2306.13793v1 [cs.LG])

    [http://arxiv.org/abs/2306.13793](http://arxiv.org/abs/2306.13793)

    QNNRepair 是一种用于修复量化神经网络的方法，通过解决神经元权重参数以修复在神经网络量化过程中导致性能下降的神经元，从而在不影响通过测试的性能的同时，提高在失败测试上的性能。

    

    本文提出了QNNRepair，这是文献中第一个修正量化神经网络（QNNs）的方法。其旨在提高在量化之后的神经网络模型的准确性，其接受全精度和权重量化的神经网络以及一个修复数据集。QNNRepair将修复问题转化为线性规划，通过解决神经元权重参数以修复在神经网络量化过程中导致性能下降的神经元，从而在不影响通过测试的性能的同时，提高在失败测试上的性能。

    We present QNNRepair, the first method in the literature for repairing quantized neural networks (QNNs). QNNRepair aims to improve the accuracy of a neural network model after quantization. It accepts the full-precision and weight-quantized neural networks and a repair dataset of passing and failing tests. At first, QNNRepair applies a software fault localization method to identify the neurons that cause performance degradation during neural network quantization. Then, it formulates the repair problem into a linear programming problem of solving neuron weights parameters, which corrects the QNN's performance on failing tests while not compromising its performance on passing tests. We evaluate QNNRepair with widely used neural network architectures such as MobileNetV2, ResNet, and VGGNet on popular datasets, including high-resolution images. We also compare QNNRepair with the state-of-the-art data-free quantization method SQuant. According to the experiment results, we conclude that QNN
    
[^72]: 解构分类器：针对文本分类模型的数据重构攻击

    Deconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models. (arXiv:2306.13789v1 [cs.CL])

    [http://arxiv.org/abs/2306.13789](http://arxiv.org/abs/2306.13789)

    本文提出了一种针对文本分类模型的数据重构攻击，称为Mix And Match攻击，该攻击利用了分类模型基于LLM的特点，该攻击已被证明在随机和有机测试数据集上是有效的。

    

    自然语言处理（NLP）模型越来越受到现实世界应用的青睐，如文本分类。然而，它们对隐私攻击是脆弱的，包括旨在提取用于训练模型的数据的数据重构攻击。大多数以前关于数据重构攻击的研究都集中在LLM上，而分类模型被认为更安全。在这项工作中，我们提出了一种新的有针对性的数据重构攻击称为Mix And Match攻击，它利用了大多数分类模型基于LLM的事实。Mix And Match攻击使用目标模型的基础模型生成候选令牌，然后使用分类头修剪它们。我们广泛展示了攻击的有效性，使用了随机与有机的金丝雀。这项工作突出了在分类模型中考虑与数据重构攻击相关的隐私风险的重要性，并提供了有关如何提高模型的隐私保护的见解。

    Natural language processing (NLP) models have become increasingly popular in real-world applications, such as text classification. However, they are vulnerable to privacy attacks, including data reconstruction attacks that aim to extract the data used to train the model. Most previous studies on data reconstruction attacks have focused on LLM, while classification models were assumed to be more secure. In this work, we propose a new targeted data reconstruction attack called the Mix And Match attack, which takes advantage of the fact that most classification models are based on LLM. The Mix And Match attack uses the base model of the target model to generate candidate tokens and then prunes them using the classification head. We extensively demonstrate the effectiveness of the attack using both random and organic canaries. This work highlights the importance of considering the privacy risks associated with data reconstruction attacks in classification models and offers insights into po
    
[^73]: 机器学习算法泛化误差的新方法：估计和收敛性

    A new approach to generalisation error of machine learning algorithms: Estimates and convergence. (arXiv:2306.13784v1 [stat.ML])

    [http://arxiv.org/abs/2306.13784](http://arxiv.org/abs/2306.13784)

    本文提出了一种新的机器学习算法泛化误差的估计方法和收敛性分析，可以在不需要神经网络的任何假设下对误差进行估计，并只要求神经网络具有适当的逼近能力就可以将近似转化为目标函数f。

    

    本文考虑了深度神经学习的一个模型问题，即在有限的点集上已知一个函数的点值，学习该函数。通过典型的机器学习算法，包括给定的DNN体系结构和一个被认为可以完全解决的优化步骤，得到了深度神经网络插值器，该插值器是f的近似。在本文中，我们引入了一种估计（泛化）误差和收敛性的新方法。我们的结果包括：（i）在神经网络没有任何结构性假设的情况下，并在学习函数f的温和正则性假设下估计误差，（ii）只要神经网络空间具有适当的逼近能力，就可以将近似转化为目标函数f。

    In this work we consider a model problem of deep neural learning, namely the learning of a given function when it is assumed that we have access to its point values on a finite set of points. The deep neural network interpolant is the the resulting approximation of f, which is obtained by a typical machine learning algorithm involving a given DNN architecture and an optimisation step, which is assumed to be solved exactly. These are among the simplest regression algorithms based on neural networks. In this work we introduce a new approach to the estimation of the (generalisation) error and to convergence. Our results include (i) estimates of the error without any structural assumption on the neural networks and under mild regularity assumptions on the learning function f (ii) convergence of the approximations to the target function f by only requiring that the neural network spaces have appropriate approximation capability.
    
[^74]: Swin-Free: 通过变化大小的窗口实现更好的跨窗口注意力和效率

    Swin-Free: Achieving Better Cross-Window Attention and Efficiency with Size-varying Window. (arXiv:2306.13776v1 [cs.CV])

    [http://arxiv.org/abs/2306.13776](http://arxiv.org/abs/2306.13776)

    Swin-Free是一个Transformer模型的变体，通过变化大小的窗口实现了更好的跨窗口注意力和效率。与原始模型相比，在推理时速度更快、准确率更高。

    

    Transformer模型在语言任务中的成功后，已经展现出在计算机视觉领域的巨大潜力。其中Swin Transformer在准确度上超越了基于卷积的架构，在与Vision Transformer及其变体相比的效率方面也有所改进，因为后者相对于输入大小具有二次复杂度。Swin Transformer采用了移动窗口的功能，允许跨窗口连接，同时将自我注意计算限制在不重叠的局部窗口中。然而，移动窗口引入了内存复制操作，这占据了其运行时间的重要部分。为了解决这个问题，我们提出了Swin-Free，其中我们在各个阶段应用变化大小的窗口，而不是移动窗口，以实现局部窗口之间的交叉连接。通过这个简单的设计改变，Swin-Free在推理时比Swin Transformer运行更快，准确度更高。此外，我们还提出了几个比Swin-Free更快的变体。

    Transformer models have shown great potential in computer vision, following their success in language tasks. Swin Transformer is one of them that outperforms convolution-based architectures in terms of accuracy, while improving efficiency when compared to Vision Transformer (ViT) and its variants, which have quadratic complexity with respect to the input size. Swin Transformer features shifting windows that allows cross-window connection while limiting self-attention computation to non-overlapping local windows. However, shifting windows introduces memory copy operations, which account for a significant portion of its runtime. To mitigate this issue, we propose Swin-Free in which we apply size-varying windows across stages, instead of shifting windows, to achieve cross-connection among local windows. With this simple design change, Swin-Free runs faster than the Swin Transformer at inference with better accuracy. Furthermore, we also propose a few of Swin-Free variants that are faster 
    
[^75]: 带有Bandit反馈的最近邻算法

    Nearest Neighbour with Bandit Feedback. (arXiv:2306.13773v1 [cs.LG])

    [http://arxiv.org/abs/2306.13773](http://arxiv.org/abs/2306.13773)

    本论文提出一种新的最近邻算法，能够应用于上下文Bandit问题并处理完全对抗的设置，具有高效运行、快速搜索和准线性空间的优点。

    

    本文将最近邻算法应用于上下文Bandit问题中。我们的算法处理了完全对抗的设置，即不对数据生成过程做任何假设。当与快速数据结构（可能是近似的自适应最近邻搜索，如导航网络）相结合时，我们的算法非常高效-每次试验的运行时间对动作数和试验数呈对数多项式增长，并且只需要准线性空间。

    In this paper we adapt the nearest neighbour rule to the contextual bandit problem. Our algorithm handles the fully adversarial setting in which no assumptions at all are made about the data-generation process. When combined with a sufficiently fast data-structure for (perhaps approximate) adaptive nearest neighbour search, such as a navigating net, our algorithm is extremely efficient - having a per trial running time polylogarithmic in both the number of trials and actions, and taking only quasi-linear space.
    
[^76]: 基于元路径的概率软逻辑用于药物靶点相互作用预测

    Meta-Path-based Probabilistic Soft Logic for Drug-Target Interaction Prediction. (arXiv:2306.13770v1 [q-bio.BM])

    [http://arxiv.org/abs/2306.13770](http://arxiv.org/abs/2306.13770)

    本文提出了一种基于元路径和概率软逻辑的药物-靶点相互作用预测方法，可以更好地利用各类相似性丰富信息和拓扑信息，同时实现较快的预测速度。

    

    药物-靶点相互作用（DTI）预测近年来受到广泛关注，旨在预测药物是否与靶点结合，以自动化和加速药物设计的成本。近期提出的大多数方法使用单个药物-药物相似性和靶点-靶点相似性信息进行DTI预测，无法利用各种相似性的丰富信息。最近提出了一些方法来利用多相似性信息，但它们仍缺乏考虑药物和靶点所驻留的各种知识库的丰富拓扑信息的能力。更重要的是，这些方法的时间消耗非常高，阻止了大规模网络信息的使用。因此，我们提出了一种基于网络的药物-靶点相互作用预测方法，该方法将概率软逻辑（PSL）应用于元路径上。

    Drug-target interaction (DTI) prediction, which aims at predicting whether a drug will be bounded to a target, have received wide attention recently, with the goal to automate and accelerate the costly process of drug design. Most of the recently proposed methods use single drug-drug similarity and target-target similarity information for DTI prediction, which are unable to take advantage of the abundant information regarding various types of similarities between them. Very recently, some methods are proposed to leverage multi-similarity information, however, they still lack the ability to take into consideration the rich topological information of all sorts of knowledge bases where the drugs and targets reside in. More importantly, the time consumption of these approaches is very high, which prevents the usage of large-scale network information. We thus propose a network-based drug-target interaction prediction approach, which applies probabilistic soft logic (PSL) to meta-paths on a 
    
[^77]: 基于功能团的扩散用于口袋特异性分子生成和扩展

    Functional-Group-Based Diffusion for Pocket-Specific Molecule Generation and Elaboration. (arXiv:2306.13769v1 [q-bio.BM])

    [http://arxiv.org/abs/2306.13769](http://arxiv.org/abs/2306.13769)

    提出了一种基于功能团的扩散模型D3FG，用于口袋特异性分子生成和扩展。基于刚性体的功能团和质点的连接器可以共同形成增强配体-蛋白质相互作用的复杂片段，能够生成高质量的分子。

    

    近年来，提出了AI辅助的药物设计方法，以生成给定目标蛋白质的口袋结构的分子。大部分方法都是基于原子级别的方法，将原子视为基本组件，并生成原子位置和类型。然而，这种方法很难生成具有复杂结构的现实片段。为解决这个问题，我们提出了D3FG，一种基于功能团的扩散模型，用于口袋特异性分子生成和扩展。D3FG将分子分解为两类组件：定义为刚性体的功能团和质点的连接器。两种类型的组件可以共同形成增强配体-蛋白质相互作用的复杂片段。具体而言，在扩散过程中，D3FG将组件的位置、方向和类型的数据分布扩散到一个先验分布；在生成过程中，神经网络参数化去噪器逐渐去除三个变量的噪声，以获得现实分子。我们在三个任务上评估了我们的方法：针对八个目标进行库生成，针对五种现有药物进行片段完善，以及针对两个未知参考分子的目标进行去新设计，显示出我们的方法可以生成多样化和高品质的分子，并具有出色的性能。

    In recent years, AI-assisted drug design methods have been proposed to generate molecules given the pockets' structures of target proteins. Most of them are atom-level-based methods, which consider atoms as basic components and generate atom positions and types. In this way, however, it is hard to generate realistic fragments with complicated structures. To solve this, we propose D3FG, a functional-group-based diffusion model for pocket-specific molecule generation and elaboration. D3FG decomposes molecules into two categories of components: functional groups defined as rigid bodies and linkers as mass points. And the two kinds of components can together form complicated fragments that enhance ligand-protein interactions.  To be specific, in the diffusion process, D3FG diffuses the data distribution of the positions, orientations, and types of the components into a prior distribution; In the generative process, the noise is gradually removed from the three variables by denoisers parame
    
[^78]: CeBed: 一个基于深度数据驱动的OFDM信道估计的基准测试

    CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation. (arXiv:2306.13761v1 [cs.AI])

    [http://arxiv.org/abs/2306.13761](http://arxiv.org/abs/2306.13761)

    本文提出了一个称为CeBed的测试平台，用于评估和比较不同的数据驱动OFDM信道估计方法，解决了领域内实验条件不一致和缺乏可重复性的问题。

    

    深度学习广泛应用于无线通信问题中，包括信道估计。尽管存在许多数据驱动方法，但由于实验条件不一致和缺乏标准化的实验设计，对它们进行公正和现实的比较是困难的。此外，数据驱动方法的性能通常基于经验分析进行比较。缺乏可重复性和标准化评估工具（例如数据集、代码库）阻碍了数据驱动方法在信道估计和无线通信等领域的发展和进步。在这项工作中，我们介绍了一个建立基准测试的倡议，统一了几种数据驱动的OFDM信道估计方法。具体而言，我们提出了CeBed（信道估计测试平台），包括涵盖各种系统模型和传播条件的不同数据集，以及十个深度和传统的基线实现。本文旨在为评估和比较不同的数据驱动OFDM信道估计方法提供标准化的基准，解决领域内实验条件不一致和缺乏可重复性的问题。

    Deep learning has been extensively used in wireless communication problems, including channel estimation. Although several data-driven approaches exist, a fair and realistic comparison between them is difficult due to inconsistencies in the experimental conditions and the lack of a standardized experimental design. In addition, the performance of data-driven approaches is often compared based on empirical analysis. The lack of reproducibility and availability of standardized evaluation tools (e.g., datasets, codebases) hinder the development and progress of data-driven methods for channel estimation and wireless communication in general. In this work, we introduce an initiative to build benchmarks that unify several data-driven OFDM channel estimation approaches. Specifically, we present CeBed (a testbed for channel estimation) including different datasets covering various systems models and propagation conditions along with the implementation of ten deep and traditional baselines. Thi
    
[^79]: 增量转化收益：一种用于电子商务促销增值模型的响应转化方法

    Incremental Profit per Conversion: a Response Transformation for Uplift Modeling in E-Commerce Promotions. (arXiv:2306.13759v1 [cs.LG])

    [http://arxiv.org/abs/2306.13759](http://arxiv.org/abs/2306.13759)

    本文提出了一种新的促销活动单位经济效率的增值度量方法IPC，通过响应转化方法解决响应相关成本增值模型中训练多个模型或计算复杂的问题。该方法只需转换过的数据、其倾向性和一个估计模型，可以提供准确的促销活动获利估计，是提高电子商务平台促销效率的实用工具。

    

    促销在电子商务平台中发挥着关键作用，采用各种成本结构来推动用户参与。本文专注于具有响应相关成本的促销，只有当购买发生时才会产生费用。这些促销包括折扣和优惠券。虽然现有的增值模型方法旨在解决这一挑战，但这些方法通常需要训练多个模型，如元学习器，或者由于非转换个体的零成本和利润引起的零膨胀值而估算利润时遇到复杂情况。为了解决这些挑战，我们引入了增量转化收益（IPC），这是一种新的促销活动单位经济效率的增值度量。通过提出的响应转化方法，我们证明IPC仅需要转换后的数据、其倾向性和一个要估计的模型即可。结果，IPC解决了上述问题，同时减轻了通常与响应相关成本有关的噪声。对模拟数据和真实世界数据集的实证研究表明，IPC提供了对促销活动获利能力更准确、更稳定的估计，是提高电子商务平台促销定向的实用工具。

    Promotions play a crucial role in e-commerce platforms, and various cost structures are employed to drive user engagement. This paper focuses on promotions with response-dependent costs, where expenses are incurred only when a purchase is made. Such promotions include discounts and coupons. While existing uplift model approaches aim to address this challenge, these approaches often necessitate training multiple models, like meta-learners, or encounter complications when estimating profit due to zero-inflated values stemming from non-converted individuals with zero cost and profit.  To address these challenges, we introduce Incremental Profit per Conversion (IPC), a novel uplift measure of promotional campaigns' efficiency in unit economics. Through a proposed response transformation, we demonstrate that IPC requires only converted data, its propensity, and a single model to be estimated. As a result, IPC resolves the issues mentioned above while mitigating the noise typically associate
    
[^80]: 集成梯度归因方法的四个公理化表征

    Four Axiomatic Characterizations of the Integrated Gradients Attribution Method. (arXiv:2306.13753v1 [cs.LG])

    [http://arxiv.org/abs/2306.13753](http://arxiv.org/abs/2306.13753)

    本文提出了四个公理化表征，确定了集成梯度方法在符合不同公理集的归因方法类别中是唯一的。

    

    深度神经网络在精度和功能上取得了重大进展，但其内部工作仍然很大程度上未知。归因方法旨在通过指示每个输入对模型输出的贡献来揭示这些“黑匣子”模型的工作原理。集成梯度（IG）方法是一种基于公理的最先进的基线归因方法，这意味着它被设计为符合归因的特定原则。我们提出了IG的四个公理化表征，将IG确立为满足一类归因方法不同公理集的唯一方法。

    Deep neural networks have produced significant progress among machine learning models in terms of accuracy and functionality, but their inner workings are still largely unknown. Attribution methods seek to shine a light on these "black box" models by indicating how much each input contributed to a model's outputs. The Integrated Gradients (IG) method is a state of the art baseline attribution method in the axiomatic vein, meaning it is designed to conform to particular principles of attributions. We present four axiomatic characterizations of IG, establishing IG as the unique method to satisfy different sets of axioms among a class of attribution methods.
    
[^81]: 利用CCP辅助UMAP和t-SNE分析单细胞RNA测序数据

    Analyzing scRNA-seq data by CCP-assisted UMAP and t-SNE. (arXiv:2306.13750v1 [cs.LG])

    [http://arxiv.org/abs/2306.13750](http://arxiv.org/abs/2306.13750)

    本研究利用CCP辅助UMAP和t-SNE方法可视化scRNA-seq数据，解决了细胞类型聚类准确性和计算效率问题，是一种适用于众多scRNA-seq应用的预处理工具。

    

    单细胞RNA测序（scRNA-seq）被广泛用于揭示细胞异质性，这为我们提供了有关细胞间通信，细胞分化和差异基因表达的见解。然而，由于数据稀疏性和涉及的基因数量庞大，分析scRNA-seq数据是一项挑战。因此，降维和特征选择对于消除假信号并增强下游分析非常重要。相关聚类和投影（CCP）是最近引入的一种有效的预处理scRNA-seq数据的方法。CCP利用基因之间的相关性来对基因进行分组，并根据此分组使用细胞之间的相互作用来获取超级基因。由于CCP是一种不需要矩阵对角化的数据域方法，因此它可以用于许多下游机器学习任务。在这项工作中，我们利用CCP作为均匀流形逼近和投影（UMAP）和t分布随机邻域嵌入（t-SNE）的初始化工具来可视化scRNA-seq数据。我们证明，与其他最先进的方法相比，CCP辅助UMAP和t-SNE在细胞类型聚类方面显示出更高的准确性。我们的方法计算效率高，可以处理具有数百万个细胞和数千个基因的数据集。我们的CCP辅助UMAP和t-SNE可以作为许多scRNA-seq应用的预处理工具。

    Single-cell RNA sequencing (scRNA-seq) is widely used to reveal heterogeneity in cells, which has given us insights into cell-cell communication, cell differentiation, and differential gene expression. However, analyzing scRNA-seq data is a challenge due to sparsity and the large number of genes involved. Therefore, dimensionality reduction and feature selection are important for removing spurious signals and enhancing downstream analysis. Correlated clustering and projection (CCP) was recently introduced as an effective method for preprocessing scRNA-seq data. CCP utilizes gene-gene correlations to partition the genes and, based on the partition, employs cell-cell interactions to obtain super-genes. Because CCP is a data-domain approach that does not require matrix diagonalization, it can be used in many downstream machine learning tasks. In this work, we utilize CCP as an initialization tool for uniform manifold approximation and projection (UMAP) and t-distributed stochastic neighbo
    
[^82]: 在预测之后的有效推断

    Valid inference after prediction. (arXiv:2306.13746v1 [stat.ML])

    [http://arxiv.org/abs/2306.13746](http://arxiv.org/abs/2306.13746)

    最近的研究聚焦于基于预测的推断，并提出了修正步骤以实现对未观测到响应和协变量之间关系的有效推断，Angelopoulos等人（2023）的方法成功控制了第一类错误率，并提供了正确命名覆盖的置信区间，但在某些情况下，其存在低功率问题。

    

    近期的研究聚焦于基于预测的推断，即使用预先训练好的机器学习模型预测未观测到的响应变量，然后对该预测响应与某些协变量之间的关系进行推断。然而，将标准推断方法应用于该过程并不能准确量化未观测到（而非预测到）响应与协变量之间的关系。最近，Wang等人（2020）和Angelopoulos等人（2023）提出了修正（ii）步骤的方法，以实现对未观测到响应和协变量之间关系的有效推断。本文表明，Angelopoulos等人（2023）提出的方法成功地控制了第一类错误率，并提供了具有正确命名覆盖的置信区间，无论预先训练的机器学习模型用于预测未观测到的响应的质量如何。然而，我们也发现在某些情况下，所提出的方法具有低功率。

    Recent work has focused on the very common practice of prediction-based inference: that is, (i) using a pre-trained machine learning model to predict an unobserved response variable, and then (ii) conducting inference on the association between that predicted response and some covariates. As pointed out by Wang et al. [2020], applying a standard inferential approach in (ii) does not accurately quantify the association between the unobserved (as opposed to the predicted) response and the covariates. In recent work, Wang et al. [2020] and Angelopoulos et al. [2023] propose corrections to step (ii) in order to enable valid inference on the association between the unobserved response and the covariates. Here, we show that the method proposed by Angelopoulos et al. [2023] successfully controls the type 1 error rate and provides confidence intervals with correct nominal coverage, regardless of the quality of the pre-trained machine learning model used to predict the unobserved response. Howe
    
[^83]: 多目标多样性：在资源约束下灵活性和公平性的目标规定

    Multi-Target Multiplicity: Flexibility and Fairness in Target Specification under Resource Constraints. (arXiv:2306.13738v1 [cs.LG])

    [http://arxiv.org/abs/2306.13738](http://arxiv.org/abs/2306.13738)

    该研究提出了一个新的概念和计算框架，称之为多目标多样性，用于评估目标选择对过程中灵活性和公平性的影响。研究还开发了一种算法，旨在在资源约束下平衡灵活性和公正性，并将其应用于两个真实世界的数据集中。

    

    预测模型已被广泛地应用于决策制定的各个领域，如就业、教育、贷款和健康。然而，少有真正的实际问题可以被精确定义为预测任务。特别是，通常有许多合理的目标变量选项。以前的工作认为，这是一个重要的而且有时被低估的选择，也表明了目标选择对结果模型的公正性有重大影响。然而，现有的文献并没有提供一个形式化的框架来描述目标选择在特定任务中的重要程度。我们的工作通过在目标选择问题和最近关于预测多样性的工作之间建立联系来填补这一空白。具体来说，我们介绍了一个概念性和计算框架，评估了目标选择在多样性和群体选择率不平等方面影响个人结果的程度。我们称之为多目标多样性。我们还开发了一种算法，用于在资源约束下识别平衡灵活性和公正性的最优目标集。我们通过将其应用于两个真实世界的数据集来展示我们方法的实际实用性。我们的工作凸显出在建模过程中明确考虑目标变量选择的重要性，并提供了一种原则性的方法去做到这一点。

    Prediction models have been widely adopted as the basis for decision-making in domains as diverse as employment, education, lending, and health. Yet, few real world problems readily present themselves as precisely formulated prediction tasks. In particular, there are often many reasonable target variable options. Prior work has argued that this is an important and sometimes underappreciated choice, and has also shown that target choice can have a significant impact on the fairness of the resulting model. However, the existing literature does not offer a formal framework for characterizing the extent to which target choice matters in a particular task. Our work fills this gap by drawing connections between the problem of target choice and recent work on predictive multiplicity. Specifically, we introduce a conceptual and computational framework for assessing how the choice of target affects individuals' outcomes and selection rate disparities across groups. We call this multi-target mul
    
[^84]: 组合公共的人类活动识别数据集以减轻标注数据稀缺问题

    Combining Public Human Activity Recognition Datasets to Mitigate Labeled Data Scarcity. (arXiv:2306.13735v1 [cs.CV])

    [http://arxiv.org/abs/2306.13735](http://arxiv.org/abs/2306.13735)

    本文研究如何组合公共数据集以应对HAR领域的标注数据稀缺问题，并探讨用预训练模型来提高HAR模型泛化性能的可行性，结果表明采用仔细选择和组合的公共数据集可达到与使用领域特定数据集相当的结果，这是一种有前途的解决标注数据稀缺问题的方法。

    

    在移动设备上使用监督学习进行人类活动识别 (HAR) 可以带来强大的分类性能。然而，这种方法需要大量已标注的数据，不仅用于模型的初始训练，还用于在特定客户端上对其进行定制（客户端的数据通常与训练数据有很大区别）。由于数据注释的成本、干扰性和耗时的本质，这实际上是不切实际的。此外，即使有大量已标注的数据的帮助，模型在异构客户端上的部署也面临着概括未见数据的困难。计算机视觉和自然语言处理等其他领域已经提出了预训练模型的概念，利用大规模语料库减少标注数据需求并更好地管理异构性。到目前为止，由于没有足够大的公共数据集，这种有前途的方法还没有在HAR领域中实现。在本文中，我们提供了组合公共数据集以减轻HAR标注数据稀缺问题的好处的实证评估。我们分析了数据集大小和多样性对预训练模型性能的影响，并表明，通过仔细选择和组合数据集，我们可以获得与使用更大的领域特定数据集相当的结果。我们的结果表明，预训练多样化的公共数据集是改善HAR模型概括性能的有前途的方法，同时减轻标注数据稀缺问题。

    The use of supervised learning for Human Activity Recognition (HAR) on mobile devices leads to strong classification performances. Such an approach, however, requires large amounts of labeled data, both for the initial training of the models and for their customization on specific clients (whose data often differ greatly from the training data). This is actually impractical to obtain due to the costs, intrusiveness, and time-consuming nature of data annotation. Moreover, even with the help of a significant amount of labeled data, model deployment on heterogeneous clients faces difficulties in generalizing well on unseen data. Other domains, like Computer Vision or Natural Language Processing, have proposed the notion of pre-trained models, leveraging large corpora, to reduce the need for annotated data and better manage heterogeneity. This promising approach has not been implemented in the HAR domain so far because of the lack of public datasets of sufficient size. In this paper, we pr
    
[^85]: 可训练的压缩嵌入层及其在推荐系统上的应用综述

    Review of compressed embedding layers and their applications for recommender systems. (arXiv:2306.13724v1 [cs.LG])

    [http://arxiv.org/abs/2306.13724](http://arxiv.org/abs/2306.13724)

    论文综述了可训练的、压缩的嵌入层在压缩大型神经网络推荐系统中的应用，并提供了相关实验结果。

    

    我们回顾了可训练的、压缩的嵌入层的文献，并讨论了它们在压缩巨型神经推荐系统方面的适用性。我们还报告了使用我们的压缩嵌入层所测得的结果。

    We review the literature on trainable, compressed embedding layers and discuss their applicability for compressing gigantic neural recommender systems. We also report the results we measured with our compressed embedding layers.
    
[^86]: 探索AI生成的合成数据集的潜力：以ChatGPT为例的远程测控数据案例研究

    Exploring the Potential of AI-Generated Synthetic Datasets: A Case Study on Telematics Data with ChatGPT. (arXiv:2306.13700v1 [cs.CY])

    [http://arxiv.org/abs/2306.13700](http://arxiv.org/abs/2306.13700)

    本研究探讨了使用OpenAI的ChatGPT构建合成数据集的方法，并通过实践案例展示了其实现过程。结果表明，AI生成的合成数据集在解决数据隐私和稀缺性方面具有巨大的潜力。

    

    本研究探讨了在遥感技术领域内构建和利用合成数据集的方法，利用OpenAI强大的语言模型ChatGPT。 合成数据集为解决有关数据隐私、稀缺性和对变量的控制等挑战提供了有效的解决方案，这些特性使它们在研究中具有特别的价值。 然而，这些数据集的实用性在很大程度上取决于它们的质量，这是通过多样性、相关性和连贯性等方面来衡量的。 为了说明这一数据创建过程，展开了一项实践性案例研究，重点是生成合成的遥感数据集。 实验涉及对ChatGPT进行迭代指导，逐步提炼提示，并最终创建了哥伦布市区域规划方案的全面数据集。生成后，该合成数据集接受了评估，重点关注先前确定的质量参数。最终，结果表明，AI生成的合成数据集在各种研究应用中具有巨大的潜力，特别是在传统数据来源面临越来越多局限和挑战的情况下。

    This research delves into the construction and utilization of synthetic datasets, specifically within the telematics sphere, leveraging OpenAI's powerful language model, ChatGPT. Synthetic datasets present an effective solution to challenges pertaining to data privacy, scarcity, and control over variables - characteristics that make them particularly valuable for research pursuits. The utility of these datasets, however, largely depends on their quality, measured through the lenses of diversity, relevance, and coherence. To illustrate this data creation process, a hands-on case study is conducted, focusing on the generation of a synthetic telematics dataset. The experiment involved an iterative guidance of ChatGPT, progressively refining prompts and culminating in the creation of a comprehensive dataset for a hypothetical urban planning scenario in Columbus, Ohio. Upon generation, the synthetic dataset was subjected to an evaluation, focusing on the previously identified quality parame
    
[^87]: 基于曲率增强的图卷积神经网络在生物分子相互作用预测中的应用

    Curvature-enhanced Graph Convolutional Network for Biomolecular Interaction Prediction. (arXiv:2306.13699v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.13699](http://arxiv.org/abs/2306.13699)

    本文提出了一种基于曲率增强的图卷积神经网络，用于生物分子相互作用的预测，并在14个真实的生物相互作用网络上进行了广泛的验证。通过对局部结构引入几何信息，并在消息传递过程中使用Ollivier-Ricci曲率（ORC）作为权重，该模型在13个真实数据集中取得了最好的结果。

    

    几何深度学习已经证明在不规则数据分析中具有巨大的潜力。将几何信息纳入学习架构对其成功至关重要。本文首次提出了一种基于曲率增强的图卷积神经网络（CGCN）用于生物分子相互作用的预测。我们的CGCN采用了Ollivier-Ricci曲率（ORC）来表征网络局部结构并增强GCN的学习能力。更具体地，ORCs是根据节点邻域的局部拓扑进行评估的，并且在消息传递过程中用作特征聚合的权重。我们的CGCN模型在14个实际的生物分子相互作用网络和一系列模拟数据上进行了广泛的验证。结果表明，我们的CGCN可以达到最先进的结果。据我们所知，在14个真实数据集中，它比所有现有的模型表现都要好，在其中13个数据集中排名第一。该论文是一篇arXiv预印本。

    Geometric deep learning has demonstrated a great potential in non-Euclidean data analysis. The incorporation of geometric insights into learning architecture is vital to its success. Here we propose a curvature-enhanced graph convolutional network (CGCN) for biomolecular interaction prediction, for the first time. Our CGCN employs Ollivier-Ricci curvature (ORC) to characterize network local structures and to enhance the learning capability of GCNs. More specifically, ORCs are evaluated based on the local topology from node neighborhoods, and further used as weights for the feature aggregation in message-passing procedure. Our CGCN model is extensively validated on fourteen real-world bimolecular interaction networks and a series of simulated data. It has been found that our CGCN can achieve the state-of-the-art results. It outperforms all existing models, as far as we know, in thirteen out of the fourteen real-world datasets and ranks as the second in the rest one. The results from the
    
[^88]: 基于深度学习的彩色多普勒心脏超声相位展开技术研究

    Phase Unwrapping of Color Doppler Echocardiography using Deep Learning. (arXiv:2306.13695v1 [eess.IV])

    [http://arxiv.org/abs/2306.13695](http://arxiv.org/abs/2306.13695)

    本文提出了一种基于展开型原始-对偶网络的新方法，用于纠正彩色多普勒心脏超声图像中的相位包裹伪影，与其他最新分割技术相比，该方法在性能上表现突出。

    

    彩色多普勒心脏超声是一种广泛使用的非侵入性成像技术，可以提供关于心脏血流的实时信息。在左心室长轴视图中，彩色多普勒容易出现相位包裹现象，特别是在心脏收缩和舒张期。当基于彩色多普勒的定量方法时，必须纠正这种包裹伪影。我们开发了一个基于展开型原始-对偶网络的方法来解包(去伪影)彩色多普勒心脏超声图像，将其有效性与基于nnU-Net和Transformer模型的两种最新分割方法进行了比较。我们在自有数据集上对每种方法进行了训练和评估，并发现nnU-Net方法提供了最佳的去伪影结果，其次是展开型原始-对偶方法和基于Transformer的技术。值得注意的是，展开型原始-对偶网络拥有显著更少的可训练参数，但性能仍能与其他方法相媲美。

    Color Doppler echocardiography is a widely used non-invasive imaging modality that provides real-time information about the intracardiac blood flow. In an apical long-axis view of the left ventricle, color Doppler is subject to phase wrapping, or aliasing, especially during cardiac filling and ejection. When setting up quantitative methods based on color Doppler, it is necessary to correct this wrapping artifact. We developed an unfolded primal-dual network to unwrap (dealias) color Doppler echocardiographic images and compared its effectiveness against two state-of-the-art segmentation approaches based on nnU-Net and transformer models. We trained and evaluated the performance of each method on an in-house dataset and found that the nnU-Net-based method provided the best dealiased results, followed by the primal-dual approach and the transformer-based technique. Noteworthy, the primal-dual network, which had significantly fewer trainable parameters, performed competitively with respec
    
[^89]: 基于自适应循环图神经网络预测深层冰层厚度

    Prediction of Deep Ice Layer Thickness Using Adaptive Recurrent Graph Neural Networks. (arXiv:2306.13690v1 [cs.LG])

    [http://arxiv.org/abs/2306.13690](http://arxiv.org/abs/2306.13690)

    本文提出了一种使用自适应循环图神经网络模型来预测深层冰层厚度的方法，该模型表现更好且更加一致。

    

    随着应对气候变化和全球大气温度的增加，准确跟踪和预测极地冰层中的冰层厚度变得越来越重要。研究这些冰层揭示了气候趋势，降雪如何随时间变化以及未来气候和降水的轨迹。本文提出了一种机器学习模型，使用自适应、循环的图卷积网络，通过飞行雷达数据收集的近年积雪量，预测历史积雪量，进而预测深层冰层的厚度。我们发现，我们的模型表现更好，且比以前的模型和等效的非时间、非几何和非自适应模型更加一致。

    As we deal with the effects of climate change and the increase of global atmospheric temperatures, the accurate tracking and prediction of ice layers within polar ice sheets grows in importance. Studying these ice layers reveals climate trends, how snowfall has changed over time, and the trajectory of future climate and precipitation. In this paper, we propose a machine learning model that uses adaptive, recurrent graph convolutional networks to, when given the amount of snow accumulation in recent years gathered through airborne radar data, predict historic snow accumulation by way of the thickness of deep ice layers. We found that our model performs better and with greater consistency than our previous model as well as equivalent non-temporal, non-geometric, and non-adaptive models.
    
[^90]: 拓展可持续人工智能的视角： 人工智能系统的综合可持续性标准和指标

    Broadening the perspective for sustainable AI: Comprehensive sustainability criteria and indicators for AI systems. (arXiv:2306.13686v1 [cs.CY])

    [http://arxiv.org/abs/2306.13686](http://arxiv.org/abs/2306.13686)

    本文提出了SCAIS框架，包含一组19个可持续性标准和67个指标，旨在促进和结构化关于可持续人工智能的讨论。这种跨学科方法为实现人工智能系统的可持续发展提供了基础。

    

    人工智能系统的增加使用导致了多方面的社会、环境和经济后果，包括非透明的决策过程、歧视、不平等加剧、人工智能模型的能量消耗和温室气体排放，以及经济实力的集中。本文通过考虑可持续发展的多方面性，为“可持续人工智能”的理念提供了实质性的支持。提出了SCAIS框架（人工智能系统的可持续性标准和指标），其中包含一组19个可持续性标准和67个指标，这些标准和指标基于批判性审查和专家研讨的结果。这种跨学科方法为促进和结构化关于可持续人工智能的讨论提供了独特的整体性视角。此外，它提供了一个具体框架，为AI系统的后续发展和评估打下了基础。

    The increased use of AI systems is associated with multi-faceted societal, environmental, and economic consequences. These include non-transparent decision-making processes, discrimination, increasing inequalities, rising energy consumption and greenhouse gas emissions in AI model development and application, and an increasing concentration of economic power. By considering the multi-dimensionality of sustainability, this paper takes steps towards substantiating the call for an overarching perspective on "sustainable AI". It presents the SCAIS Framework (Sustainability Criteria and Indicators for Artificial Intelligence Systems) which contains a set 19 sustainability criteria for sustainable AI and 67 indicators that is based on the results of a critical review and expert workshops. This interdisciplinary approach contributes a unique holistic perspective to facilitate and structure the discourse on sustainable AI. Further, it provides a concrete framework that lays the foundation for 
    
[^91]: 评估基于显著性的解释方法的总体敏感性

    Evaluating the overall sensitivity of saliency-based explanation methods. (arXiv:2306.13682v1 [cs.LG])

    [http://arxiv.org/abs/2306.13682](http://arxiv.org/abs/2306.13682)

    本文研究了如何生成对“黑匣子”深度学习模型的忠实解释，提出了一个扩展的测试方法来确定解释方法的总体敏感性，并通过例子展示了如何使用这个方法比较卷积神经网络的多个解释方法。

    

    我们着眼于生成对"黑匣子"深度学习模型的忠实解释的需求。已经提出了几种测试来确定解释方法的忠实程度，但它们缺乏跨领域的适用性和严谨的方法论。因此，我们选择了一个现有的不依赖特定模型，并且非常适合比较多个解释方法的忠实程度的测试，并通过指定正式的阈值和建立标准来扩展它，以确定解释方法的总体敏感性。我们通过例子展示了如何使用这个扩展方法来比较卷积神经网络的多个解释方法。最后，我们讨论了敏感性和忠实程度之间的关系，并考虑如何调整测试来评估其他领域的不同解释方法。

    We address the need to generate faithful explanations of "black box" Deep Learning models. Several tests have been proposed to determine aspects of faithfulness of explanation methods, but they lack cross-domain applicability and a rigorous methodology. Hence, we select an existing test that is model agnostic and is well-suited for comparing one aspect of faithfulness (i.e., sensitivity) of multiple explanation methods, and extend it by specifying formal thresh-olds and building criteria to determine the over-all sensitivity of the explanation method. We present examples of how multiple explanation methods for Convolutional Neural Networks can be compared using this extended methodology. Finally, we discuss the relationship between sensitivity and faithfulness and consider how the test can be adapted to assess different explanation methods in other domains.
    
[^92]: 估算基于证据决策的价值

    Estimating the Value of Evidence-Based Decision Making. (arXiv:2306.13681v1 [stat.ME])

    [http://arxiv.org/abs/2306.13681](http://arxiv.org/abs/2306.13681)

    本文提出了一个实证框架，用于估算证据决策的价值和统计精度投资回报。

    

    商业/政策决策通常基于随机实验和观察性研究的证据。本文提出了一个实证框架来估算基于证据的决策（EBDM）的价值和统计精度投资回报。

    Business/policy decisions are often based on evidence from randomized experiments and observational studies. In this article we propose an empirical framework to estimate the value of evidence-based decision making (EBDM) and the return on the investment in statistical precision.
    
[^93]: 基于GPT的模型遇见仿真：如何有效地应用大规模预训练语言模型于仿真任务

    GPT-Based Models Meet Simulation: How to Efficiently Use Large-Scale Pre-Trained Language Models Across Simulation Tasks. (arXiv:2306.13679v1 [cs.HC])

    [http://arxiv.org/abs/2306.13679](http://arxiv.org/abs/2306.13679)

    本文是关于大规模预训练语言模型在科学仿真中的研究。研究关注四个建模和仿真任务的LLMs的预期益处和限制，并提供实际指导。

    

    大规模预训练语言模型（LLMs），如ChatGPT或GPT-4所提供的颠覆性技术，在多个应用领域引起了广泛关注，通常强调高水平的机会和担忧。本文是关于LLMs在科学仿真中应用的第一篇研究。我们关注四个建模和仿真任务，每次评估LLMs的预期益处和限制，同时为模型构建者提供实际指导。第一个任务旨在解释概念模型的结构，以促进参与者在建模过程中的参与。第二个任务专注于汇总仿真输出，以便模型用户能够识别出优选场景。第三个任务旨在通过文本传达对仿真可视化的见解，以扩大仿真平台的可访问性。最后，最后一个任务引出了使用LLMs解释仿真错误和问题的可能性，以便模型开发者更有效地解决这些问题。

    The disruptive technology provided by large-scale pre-trained language models (LLMs) such as ChatGPT or GPT-4 has received significant attention in several application domains, often with an emphasis on high-level opportunities and concerns. This paper is the first examination regarding the use of LLMs for scientific simulations. We focus on four modeling and simulation tasks, each time assessing the expected benefits and limitations of LLMs while providing practical guidance for modelers regarding the steps involved. The first task is devoted to explaining the structure of a conceptual model to promote the engagement of participants in the modeling process. The second task focuses on summarizing simulation outputs, so that model users can identify a preferred scenario. The third task seeks to broaden accessibility to simulation platforms by conveying the insights of simulation visualizations via text. Finally, the last task evokes the possibility of explaining simulation errors and pr
    
[^94]: 交叉性和医疗记录中的证言不公正

    Intersectionality and Testimonial Injustice in Medical Records. (arXiv:2306.13675v1 [cs.CY])

    [http://arxiv.org/abs/2306.13675](http://arxiv.org/abs/2306.13675)

    本研究探讨了使用交叉性来检测医疗记录中的证言不公正，进一步提醒人们单一人口统计因素无法充分涵盖病人经历的微妙身份，而忽略这些不公正可能导致医疗质量差或危及生命。

    

    检测证言不公正是解决不公平和促进包容性医疗实践的基本元素之一，其中许多是与生命密切相关的。然而，只使用单个人口统计因素来检测证言不公正并不能充分涵盖为患者体验做出贡献的微妙身份。此外，有些不公正只有在通过交叉性镜头检查产生的细微差别时才能显现。忽略这些不公正可能导致医疗质量差或危及生命。因此，在考虑交叉性的情况下可能会产生更准确的分类和公正的决策。为了说明这一点，我们使用真实的医疗数据来确定医疗记录中是否存在可能导致证言不公正的单词，使用公平度量（例如人口统计学平衡、差异交叉公平性和子组公平性）来评估不同子组体验证言不公正的严重程度，并分析患者的交叉身份如何导致这些不公正。

    Detecting testimonial injustice is an essential element of addressing inequities and promoting inclusive healthcare practices, many of which are life-critical. However, using a single demographic factor to detect testimonial injustice does not fully encompass the nuanced identities that contribute to a patient's experience. Further, some injustices may only be evident when examining the nuances that arise through the lens of intersectionality. Ignoring such injustices can result in poor quality of care or life-endangering events. Thus, considering intersectionality could result in more accurate classifications and just decisions. To illustrate this, we use real-world medical data to determine whether medical records exhibit words that could lead to testimonial injustice, employ fairness metrics (e.g. demographic parity, differential intersectional fairness, and subgroup fairness) to assess the severity to which subgroups are experiencing testimonial injustice, and analyze how the inter
    
[^95]: MeciFace：基于肌肉电和惯性融合的边缘实时识别面部表情和进食活动眼镜

    MeciFace: Mechanomyography and Inertial Fusion based Glasses for Edge Real-Time Recognition of Facial and Eating Activities. (arXiv:2306.13674v1 [cs.CV])

    [http://arxiv.org/abs/2306.13674](http://arxiv.org/abs/2306.13674)

    MeciFace是一款注重隐私且低功耗的可穿戴设备，它采用轻量级卷积神经网络来监测面部表情和进食活动，面部表情案例的F1分数达到了86％，饮食监测则达到了90％的F1分数。

    

    我们提出了MeciFace，这是一个低功耗（0.55瓦），注重隐私，实时边缘监测（RTE）的可穿戴解决方案，具有微小的内存占用（11-19 KB），旨在监测面部表情和进食活动。我们采用轻量级卷积神经网络作为面部和进食场景的主干模型。该系统在面部表情案例的RTE评估中产生了86％的F1分数。此外，我们对未知用户的RTE进行饮食监测，得到了90％的F1分数。

    We present MeciFace, a low-power (0.55 Watts), privacy-conscious, real-time on-the-edge (RTE) wearable solution with a tiny memory footprint (11-19 KB), designed to monitor facial expressions and eating activities. We employ lightweight convolutional neural networks as the backbone models for both facial and eating scenarios. The system yielded an F1-score of 86% for the RTE evaluation in the facial expression case. In addition, we obtained an F1-score of 90% for eating/drinking monitoring for the RTE of an unseen user.
    
[^96]: 驾驭指数级的动作集：在线拥塞博弈中次线性遗憾和快速收敛到纳什均衡

    Taming the Exponential Action Set: Sublinear Regret and Fast Convergence to Nash Equilibrium in Online Congestion Games. (arXiv:2306.13673v1 [cs.GT])

    [http://arxiv.org/abs/2306.13673](http://arxiv.org/abs/2306.13673)

    本文提出了一种名为CongestEXP的分散算法，可以线性缩放设施数量，实现在线拥塞博弈的次线性遗憾上界，并以与已知最佳下界相匹配的速度快速收敛到纳什均衡。

    

    拥塞博弈是一种强大的模型，涵盖了交通网络和资源分配等一系列工程系统。本文研究拥塞博弈的在线形式，其中代理重复参与博弈，并观察到带有随机性的反馈。我们提出了一种名为CongestEXP的分散算法，应用于经典的指数权重方法，通过在设施级别上保持权重，避免了传统算法对可能设施集大小的指数依赖，即$\binom{F}{k}\approx F^k$，并且仅与$F$线性缩放。具体地，我们证明了对于每个单独的玩家，CongestEXP可以实现$O(kF\sqrt{T})$的遗憾上界，其中$T$是时间周期。另一方面，利用权重的指数增长使CongestEXP能够实现快速收敛到纳什均衡，其收敛速度为$O(\ln F/\sqrt{T})$，这与已知的最佳下界相符，仅相差对数因子。我们的分析也适用于更广泛的游戏类别，包括那些具有连续分布权重和那些具有任意玩家行动的游戏。最后，我们在合成数据集和实际数据集上证明了CongestEXP的有效性。

    The congestion game is a powerful model that encompasses a range of engineering systems such as traffic networks and resource allocation. It describes the behavior of a group of agents who share a common set of $F$ facilities and take actions as subsets with $k$ facilities. In this work, we study the online formulation of congestion games, where agents participate in the game repeatedly and observe feedback with randomness. We propose CongestEXP, a decentralized algorithm that applies the classic exponential weights method. By maintaining weights on the facility level, the regret bound of CongestEXP avoids the exponential dependence on the size of possible facility sets, i.e., $\binom{F}{k} \approx F^k$, and scales only linearly with $F$. Specifically, we show that CongestEXP attains a regret upper bound of $O(kF\sqrt{T})$ for every individual player, where $T$ is the time horizon. On the other hand, exploiting the exponential growth of weights enables CongestEXP to achieve a fast conv
    
[^97]: 机器学习系统最佳实践：用于分析和优化的工业框架

    Best Practices for Machine Learning Systems: An Industrial Framework for Analysis and Optimization. (arXiv:2306.13662v1 [cs.SE])

    [http://arxiv.org/abs/2306.13662](http://arxiv.org/abs/2306.13662)

    该论文提出了一个用于分析和优化机器学习系统最佳实践的工业框架。该框架包含五个步骤，能够提高机器学习系统的质量。

    

    近年来，机器学习（ML）和人工智能社区开始对ML系统的软件工程（SE）产生越来越多的兴趣，这导致了大量旨在改善ML系统软件质量的最佳实践、规则和指南的出现。然而，对它们对整体质量的影响的理解却受到了较少的关注。这些实践通常以指导性方式呈现，没有明确地与它们对软件质量的整体贡献建立联系。基于对不同实践影响软件质量不同的观察以及单一质量方面可能由多个实践来解决的观察，我们提出了一个框架，重点关注实现的质量影响和实现优先顺序方面的最佳实践集合的分析。我们首先引入了一个专门针对ML系统量身定制的分层软件质量模型（SQM）。借助专家知识，建立了个体实践与SQM类别之间的联系。然后，我们定义了一个用于分析和优化一组最佳实践的ML系统最佳实践（MSBP）框架。该框架由五个步骤组成：（i）标识最佳实践，（ii）最佳实践的优先级排序，（iii）评估最佳实践的质量影响，（iv）最佳实践的操作化和实施，以及（v）评估。我们通过将其应用于工业ML系统的真实案例来说明该框架。我们的结果表明，该MSBP框架可用于改善ML系统的质量。

    In the last few years, the Machine Learning (ML) and Artificial Intelligence community has developed an increasing interest in Software Engineering (SE) for ML Systems leading to a proliferation of best practices, rules, and guidelines aiming at improving the quality of the software of ML Systems. However, understanding their impact on the overall quality has received less attention. Practices are usually presented in a prescriptive manner, without an explicit connection to their overall contribution to software quality. Based on the observation that different practices influence different aspects of software-quality and that one single quality aspect might be addressed by several practices we propose a framework to analyse sets of best practices with focus on quality impact and prioritization of their implementation. We first introduce a hierarchical Software Quality Model (SQM) specifically tailored for ML Systems. Relying on expert knowledge, the connection between individual practi
    
[^98]: FPGA实现的卷积神经网络用于实时手写体识别

    FPGA Implementation of Convolutional Neural Network for Real-Time Handwriting Recognition. (arXiv:2306.13557v1 [cs.AR])

    [http://arxiv.org/abs/2306.13557](http://arxiv.org/abs/2306.13557)

    该论文提出了一种在FPGA器件上实现卷积神经网络的实时手写体识别系统，能提高对手写字母和数字的识别精度。

    

    机器学习是计算机科学中最近蓬勃发展的领域。作为计算机硬件工程师，我们对将流行的软件机器学习结构进行硬件实现以优化其性能、可靠性和资源使用率充满热情。在本项目中，我们使用Altera DE1 FPGA套件设计了一个高度可配置的实时设备，用于识别手写字母和数字。我们遵循了IEEE-754 32位浮点标准、视频图形阵列（VGA）显示协议、通用异步收发器（UART）协议和Inter-Integrated Circuit（I2C）协议等各种工程标准，以实现项目目标。遵循这些标准，我们设计了一个32位浮点（FP）指令集架构（ISA）。我们使用System Verilog开发了一个5级RISC处理器，以管理图像处理、矩阵乘法和卷积运算等操作。该实现使用了卷积神经网络（CNN），以提高对手写字母和数字的识别精度，并在FPGA器件上实现了实时手写体识别系统。

    Machine Learning (ML) has recently been a skyrocketing field in Computer Science. As computer hardware engineers, we are enthusiastic about hardware implementations of popular software ML architectures to optimize their performance, reliability, and resource usage. In this project, we designed a highly-configurable, real-time device for recognizing handwritten letters and digits using an Altera DE1 FPGA Kit. We followed various engineering standards, including IEEE-754 32-bit Floating-Point Standard, Video Graphics Array (VGA) display protocol, Universal Asynchronous Receiver-Transmitter (UART) protocol, and Inter-Integrated Circuit (I2C) protocols to achieve the project goals. These significantly improved our design in compatibility, reusability, and simplicity in verifications. Following these standards, we designed a 32-bit floating-point (FP) instruction set architecture (ISA). We developed a 5-stage RISC processor in System Verilog to manage image processing, matrix multiplication
    
[^99]: 基于剪枝的域泛化方法研究

    Pruning for Better Domain Generalizability. (arXiv:2306.13237v1 [cs.LG])

    [http://arxiv.org/abs/2306.13237](http://arxiv.org/abs/2306.13237)

    本文研究了基于剪枝的域泛化方法，提出了一种新的剪枝评分方法DSS，该方法不是为了保持源准确性，而是直接增强模型的鲁棒性。实验证明该方法可以与最先进的泛化方法结合使用，即便只引入少量稀疏也能显著提高模型性能。

    

    本文探讨了使用剪枝作为一种可靠的方法来提高模型的泛化能力。我们发现现有的剪枝方法，如L2已经可以在目标域性能上提供小幅度的改善。我们进一步提出了一种新的剪枝评分方法，称为DSS，设计不是为了保持源准确性而是直接增强模型的鲁棒性。我们进行了实证实验来验证我们的方法，并证明它甚至可以与MIRO(Cha等人，2022年)等最先进的泛化方法结合使用，进一步提高性能。在MNIST到MNIST-M上，通过将60%通道稀疏引入模型，我们可以将基线性能提高5个百分点以上。在DomainBed基准和最先进的MIRO上，仅通过将10%稀疏引入模型，我们就可以进一步提高其性能。代码可在以下链接找到: https://github.com/AlexSunNik/Pruning-for-Better-Domain-Generaliza

    In this paper, we investigate whether we could use pruning as a reliable method to boost the generalization ability of the model. We found that existing pruning method like L2 can already offer small improvement on the target domain performance. We further propose a novel pruning scoring method, called DSS, designed not to maintain source accuracy as typical pruning work, but to directly enhance the robustness of the model. We conduct empirical experiments to validate our method and demonstrate that it can be even combined with state-of-the-art generalization work like MIRO(Cha et al., 2022) to further boost the performance. On MNIST to MNIST-M, we could improve the baseline performance by over 5 points by introducing 60% channel sparsity into the model. On DomainBed benchmark and state-of-the-art MIRO, we can further boost its performance by 1 point only by introducing 10% sparsity into the model. Code can be found at: https://github.com/AlexSunNik/Pruning-for-Better-Domain-Generaliza
    
[^100]: ovla：使用隐式水印进行神经网络所有权验证

    ovla: Neural Network Ownership Verification using Latent Watermarks. (arXiv:2306.13215v1 [cs.CR])

    [http://arxiv.org/abs/2306.13215](http://arxiv.org/abs/2306.13215)

    ovla是一种使用隐式水印进行神经网络所有权验证的新方法，通过训练网络使水印保持休眠状态，只有在所有者的秘密密钥被应用时才会被激活，该方法在神经网络所有权验证准确率、误报率和抵抗对抗攻击方面优于现有的最先进方法。

    

    神经网络的所有权验证对于保护这些模型免受非法复制、免费骑车、重新分配和其他知识产权的滥用非常重要。我们提出了一种基于隐式水印的神经网络所有权验证新方法。该方法通过解耦网络的正常操作和对带水印输入的响应来进行验证。其关键思想是训练网络，使得水印保持休眠状态，除非应用所有者的秘密密钥来激活它。我们展示了该方法在所有权验证准确率、误报率和抵抗对抗攻击方面优于现有的最先进方法。

    Ownership verification for neural networks is important for protecting these models from illegal copying, free-riding, re-distribution and other intellectual property misuse. We present a novel methodology for neural network ownership verification based on the notion of latent watermarks. Existing ownership verification methods either modify or introduce constraints to the neural network parameters, which are accessible to an attacker in a white-box attack and can be harmful to the network's normal operation, or train the network to respond to specific watermarks in the inputs similar to data poisoning-based backdoor attacks, which are susceptible to backdoor removal techniques. In this paper, we address these problems by decoupling a network's normal operation from its responses to watermarked inputs during ownership verification. The key idea is to train the network such that the watermarks remain dormant unless the owner's secret key is applied to activate it. The secret key is real
    
[^101]: 可微分决策树是否能够学习可解释的奖励函数?

    Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v1 [cs.LG])

    [http://arxiv.org/abs/2306.13004](http://arxiv.org/abs/2306.13004)

    本文提出了使用可微分决策树从人类偏好中学习具有表达能力和可解释性的奖励函数，通过在多个环境上的评估，发现该方法能够学习到可解释的奖励函数，但在强化学习测试时表现受到树的离散性的影响。

    

    学习人的意图和偏好的奖励函数越来越受到关注，但许多框架使用黑盒学习方法，难以解释。本文提出并评估了一种新颖方法，使用可微分决策树（DDT）从偏好中学习具有表达能力和可解释性的奖励函数，适用于低维和高维状态输入。我们在Cartpole、视觉网格世界环境和Atari游戏上评估了我们的算法，探讨了使用DDT学习可解释奖励函数的可行性。我们提供证据表明，学习到的奖励函数的树形结构有助于确定奖励函数与人类偏好的一致程度。我们可视化了学习到的奖励DDT，发现它们能够学习可解释的奖励函数，但树的离散性会影响强化学习在测试时的表现。

    There is an increasing interest in learning reward functions that model human intent and human preferences. However, many frameworks use blackbox learning methods that, while expressive, are difficult to interpret. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs) for both low- and high-dimensional state inputs. We explore and discuss the viability of learning interpretable reward functions using DDTs by evaluating our algorithm on Cartpole, Visual Gridworld environments, and Atari games. We provide evidence that that the tree structure of our learned reward function is useful in determining the extent to which a reward function is aligned with human preferences. We visualize the learned reward DDTs and find that they are capable of learning interpretable reward functions but that the discrete nature of the trees hurts the performance of reinforcement learning at test time. How
    
[^102]: 通过重要性抽样实现有效通信的联邦学习

    Communication-Efficient Federated Learning through Importance Sampling. (arXiv:2306.12625v1 [cs.LG])

    [http://arxiv.org/abs/2306.12625](http://arxiv.org/abs/2306.12625)

    本文提出了一种通过重要性抽样实现有效通信的联邦学习方法，大大降低了发送模型更新的高通信成本，利用服务器端客户端分布和附加信息的接近关系，只需要较少的通信量即可实现。

    

    客户端向服务器发送模型更新的高通信成本是可扩展联邦学习（FL）的重要瓶颈。现有方法中，使用随机压缩方法实现了最先进的比特率-准确性折衷——其中客户端n发送来自仅为该客户端的概率分布qφ（n）的样本，服务器使用这些样本估计客户端分布的平均值。然而，这种方法没有充分利用FL的设置，其中服务器在整个训练过程中具有预数据分布pθ的附加信息，该分布与客户端分布qφ（n）在Kullback-Leibler（KL）发散方面接近。在本文中，我们利用服务器端客户端分布qφ（n)与附加信息pθ之间的这种接近关系，并提出了一种框架，该框架需要大约Dkl（qφ（n）|| pθ）位的通信量。

    The high communication cost of sending model updates from the clients to the server is a significant bottleneck for scalable federated learning (FL). Among existing approaches, state-of-the-art bitrate-accuracy tradeoffs have been achieved using stochastic compression methods -- in which the client $n$ sends a sample from a client-only probability distribution $q_{\phi^{(n)}}$, and the server estimates the mean of the clients' distributions using these samples. However, such methods do not take full advantage of the FL setup where the server, throughout the training process, has side information in the form of a pre-data distribution $p_{\theta}$ that is close to the client's distribution $q_{\phi^{(n)}}$ in Kullback-Leibler (KL) divergence. In this work, we exploit this closeness between the clients' distributions $q_{\phi^{(n)}}$'s and the side information $p_{\theta}$ at the server, and propose a framework that requires approximately $D_{KL}(q_{\phi^{(n)}}|| p_{\theta})$ bits of com
    
[^103]: 神经多重网格内存用于计算流体力学

    Neural Multigrid Memory For Computational Fluid Dynamics. (arXiv:2306.12545v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2306.12545](http://arxiv.org/abs/2306.12545)

    本文提出了一种新的数据驱动湍流流动模拟方法MGxTransformer，结合了VPTR和多重网格架构的优点，使得模拟结果更为准确和高效。

    

    湍流流动模拟在多个应用中都扮演着至关重要的角色，包括飞机和船舶设计、工业流程优化和天气预报等。本文提出了一种先进的数据驱动湍流流动模拟方法，相较于现有方法有了显著的改进。我们的方法结合了视频预测变换器（VPTR）（Ye & Bilodeau, 2022）和多重网格架构（MgConv，MgResnet）（Ke等，2017）的优点。VPTR擅长捕捉复杂的时空依赖关系和处理大型输入数据，是湍流流动预测的一个有前途的选择。与此同时，多重网格架构利用多个具有不同分辨率的网格来捕捉湍流流动的多尺度特性，从而实现更精确和高效的模拟。通过我们的实验，我们展示了我们提出的方法MGxTransformer在准确预测速度、温度和湍流方面的有效性。

    Turbulent flow simulation plays a crucial role in various applications, including aircraft and ship design, industrial process optimization, and weather prediction. In this paper, we propose an advanced data-driven method for simulating turbulent flow, representing a significant improvement over existing approaches.  Our methodology combines the strengths of Video Prediction Transformer (VPTR) (Ye & Bilodeau, 2022) and Multigrid Architecture (MgConv, MgResnet) (Ke et al., 2017). VPTR excels in capturing complex spatiotemporal dependencies and handling large input data, making it a promising choice for turbulent flow prediction. Meanwhile, Multigrid Architecture utilizes multiple grids with different resolutions to capture the multiscale nature of turbulent flows, resulting in more accurate and efficient simulations.  Through our experiments, we demonstrate the effectiveness of our proposed approach, named MGxTransformer, in accurately predicting velocity, temperature, and turbulence in
    
[^104]: 6G边缘网络中的Split Learning

    Split Learning in 6G Edge Networks. (arXiv:2306.12194v1 [cs.LG])

    [http://arxiv.org/abs/2306.12194](http://arxiv.org/abs/2306.12194)

    Split learning (SL) enables servers to handle the major training workload while still enhancing data privacy, which is an important approach in 6G edge networks. This article provides an overview of the tailored 6G architecture to support edge SL, the critical design issues for edge SL, and presents future research direction and exciting applications of SL in 6G edge networks.

    

    随着分布式边缘计算资源的普及，6G移动网络将发展成为一个连接智能的网络。在这条线路上，将联邦学习纳入移动边缘的提议近年来引起了相当大的兴趣。然而，联邦学习的部署面临着重大的挑战，因为庞大的资源受限的物联网设备几乎无法支持设备上的模型训练。这导致了Split Learning (SL)的出现，它使服务器处理主要的训练工作负载，同时增强数据隐私。在本文中，我们简要概述了SL的关键发展，并阐述了其与无线边缘网络的无缝集成。我们首先说明了定制的6G体系结构，以支持边缘SL。然后，我们研究了边缘SL的关键设计问题，包括创新的资源高效学习框架和在单个边缘服务器下的资源管理策略。此外，我们扩展了Split Learning模型的多边缘服务器方案。最后，我们提出了全面的未来研究方向和一些刺激人心的SL在6G边缘网络中的应用。

    With the proliferation of distributed edge computing resources, the 6G mobile network will evolve into a network for connected intelligence. Along this line, the proposal to incorporate federated learning into the mobile edge has gained considerable interest in recent years. However, the deployment of federated learning faces substantial challenges as massive resource-limited IoT devices can hardly support on-device model training. This leads to the emergence of split learning (SL) which enables servers to handle the major training workload while still enhancing data privacy. In this article, we offer a brief overview of key advancements in SL and articulate its seamless integration with wireless edge networks. We begin by illustrating the tailored 6G architecture to support edge SL. Then, we examine the critical design issues for edge SL, including innovative resource-efficient learning frameworks and resource management strategies under a single edge server. Additionally, we expand t
    
[^105]: 人工智能灾难性风险综述

    An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])

    [http://arxiv.org/abs/2306.12001](http://arxiv.org/abs/2306.12001)

    本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。

    

    人工智能的快速发展引起了专家、政策制定者和世界各国领导人对越来越先进的人工智能系统可能带来灾难性风险的担忧。虽然已经有很多风险被单独详细介绍过，但迫切需要系统地讨论和说明潜在危险，以更好地支持减轻这些风险的努力。本文概述了人工智能灾难性风险的主要来源，我们将其分为四个类别：恶意使用，即个人或团体有意使用人工智能造成伤害；人工智能竞赛，即竞争环境促使行动者部署不安全的人工智能或放弃控制权交给人工智能；组织风险，突出人为和复杂系统如何增加灾难性事故发生的可能性；以及流氓人工智能，描述了控制比人类智能更高的代理程序困难的固有难题。对于每个风险类别，我们描述了具体的危害。

    Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
    
[^106]: 图分类问题中结构感知的鲁棒性认证

    Structure-Aware Robustness Certificates for Graph Classification. (arXiv:2306.11915v1 [cs.LG])

    [http://arxiv.org/abs/2306.11915](http://arxiv.org/abs/2306.11915)

    该论文提出了一种新的随机平滑方法，可以根据图的不同预定义结构生成对应的鲁棒性证书，从而在多个基准图分类任务中取得领先的对抗性攻击下鲁棒性结果。

    

    对于基于图的机器学习模型进行鲁棒性认证是保证安全性的一个至关重要的挑战。目前用于图分类器的鲁棒性证明保证与节点对翻转（添加或删除边缘）的总数有关，这相当于以邻接矩阵为中心的l0球。尽管从理论上看很有吸引力，但这种各向同性的结构噪声在实际场景中可能过于严格，因为有些节点对于确定分类器的输出更为关键。在这种情况下，证书给出了对图模型鲁棒性的悲观描述。为了解决这个问题，我们开发了一种基于随机平滑的方法，将非各向同性的噪声分布添加到输入图结构中。我们展示了我们的过程为分类器生成了结构感知的证书，因此鲁棒性证书的大小可以在图的不同预定义结构之间变化。我们在几个基准图分类任务上展示了我们方法的优势，在对抗性攻击的鲁棒性方面取得了最先进的结果。

    Certifying the robustness of a graph-based machine learning model poses a critical challenge for safety. Current robustness certificates for graph classifiers guarantee output invariance with respect to the total number of node pair flips (edge addition or edge deletion), which amounts to an $l_{0}$ ball centred on the adjacency matrix. Although theoretically attractive, this type of isotropic structural noise can be too restrictive in practical scenarios where some node pairs are more critical than others in determining the classifier's output. The certificate, in this case, gives a pessimistic depiction of the robustness of the graph model. To tackle this issue, we develop a randomised smoothing method based on adding an anisotropic noise distribution to the input graph structure. We show that our process generates structural-aware certificates for our classifiers, whereby the magnitude of robustness certificates can vary across different pre-defined structures of the graph. We demon
    
[^107]: 是否应该停止：具有异质种群的早期停止方法

    Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations. (arXiv:2306.11839v1 [stat.ME])

    [http://arxiv.org/abs/2306.11839](http://arxiv.org/abs/2306.11839)

    本文提出了针对于异质种群有害实验的早期停止方法CLASH，使用因果机器学习可以有效提前停止临床试验和A/B测试。

    

    随机实验由于治疗造成意外的有害影响，因此往往需要提前停止。目前确定何时提前终止实验的现有方法通常适用于总体数据，不考虑治疗效应的异质性。本文研究了针对异质种群有害实验的早期停止方法。我们首先确定现有方法在治疗对少数参与者造成伤害时往往无法停止实验。然后使用因果机器学习开发了CLASH，这是首个广泛适用于异质早期停止的方法。我们在模拟和实际数据上展示了CLASH的表现，并证明它在临床试验和A/B测试中都能有效提前停止。

    Randomized experiments often need to be stopped prematurely due to the treatment having an unintended harmful effect. Existing methods that determine when to stop an experiment early are typically applied to the data in aggregate and do not account for treatment effect heterogeneity. In this paper, we study the early stopping of experiments for harm on heterogeneous populations. We first establish that current methods often fail to stop experiments when the treatment harms a minority group of participants. We then use causal machine learning to develop CLASH, the first broadly-applicable method for heterogeneous early stopping. We demonstrate CLASH's performance on simulated and real data and show that it yields effective early stopping for both clinical trials and A/B tests.
    
[^108]: 透视额外信息的Informed POMDP: 模型驱动强化学习中的利用

    Informed POMDP: Leveraging Additional Information in Model-Based RL. (arXiv:2306.11488v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11488](http://arxiv.org/abs/2306.11488)

    本文提出了Informed POMDP，这是一种新的学习范式，通过学习环境模型来利用训练时可用的额外信息，该模型可以提高Dreamer算法策略的收敛速度。

    

    本文通过考虑训练时可用的额外信息，推广了POMDP中的交互学习问题。首先，我们引入了Informed POMDP，这是一种新的学习范式，清晰区分了训练信息和执行观察。接下来，我们提出了一个目标，用于从历史记录中学习出充分统计信息，以实现最优控制并利用这些信息。然后，我们展示了这个Informed目标由学习一个环境模型组成，从其中我们可以采样隐式轨迹。最后，我们证明在几个环境中，使用这个Informed环境模型可以大大提高Dreamer算法策略的收敛速度。这些结果以及提议的简单适应性，倡导在使用模型驱动强化学习学习POMDP时，要系统地考虑可用的额外信息。

    In this work, we generalize the problem of learning through interaction in a POMDP by accounting for eventual additional information available at training time. First, we introduce the informed POMDP, a new learning paradigm offering a clear distinction between the training information and the execution observation. Next, we propose an objective for learning a sufficient statistic from the history for the optimal control that leverages this information. We then show that this informed objective consists of learning an environment model from which we can sample latent trajectories. Finally, we show for the Dreamer algorithm that the convergence speed of the policies is sometimes greatly improved on several environments by using this informed environment model. Those results and the simplicity of the proposed adaptation advocate for a systematic consideration of eventual additional information when learning in a POMDP using model-based RL.
    
[^109]: 基于低秩和稀疏逼近的大型语言模型结构化压缩 LoSparse

    LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation. (arXiv:2306.11222v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11222](http://arxiv.org/abs/2306.11222)

    LoSparse 是一种新的语言模型压缩技术，通过低秩矩阵和稀疏矩阵逼近权重矩阵，结合了低秩逼近和裁剪的优点，可以显著降低语言模型大小和复杂度，并在多个自然语言任务中表现优异。

    

    Transformer 模型在多种自然语言任务中取得了显著的成果，但它们通常过于庞大，需要大量的内存和计算资源。为了降低这些模型的大小和复杂性，我们提出了 LoSparse（低秩和稀疏逼近）一种新的模型压缩技术，通过低秩矩阵和稀疏矩阵之和逼近权重矩阵。我们的方法结合了低秩逼近和裁剪的优点，同时避免了它们的局限性。低秩逼近压缩了神经元中的一致和表达力强的部分，而裁剪则消除了神经元中的不一致和表达力不强的部分。裁剪增强了低秩逼近的多样性，低秩逼近防止了裁剪丢失表达力强的神经元过多。我们在自然语言理解、问答和自然语言生成任务上评估了我们的方法。实验结果表明，我们的方法明显优于现有的压缩方法。

    Transformer models have achieved remarkable results in various natural language tasks, but they are often prohibitively large, requiring massive memories and computational resources. To reduce the size and complexity of these models, we propose LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix. Our method combines the advantages of both low-rank approximations and pruning, while avoiding their limitations. Low-rank approximation compresses the coherent and expressive parts in neurons, while pruning removes the incoherent and non-expressive parts in neurons. Pruning enhances the diversity of low-rank approximations, and low-rank approximation prevents pruning from losing too many expressive neurons. We evaluate our method on natural language understanding, question answering, and natural language generation tasks. We show that it significantly outperforms existing compre
    
[^110]: 学习从所有训练样本中累积证据：理论与实践

    Learn to Accumulate Evidence from All Training Samples: Theory and Practice. (arXiv:2306.11113v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11113](http://arxiv.org/abs/2306.11113)

    本文提出了一种新的激活函数，All-Positive (AP)激活，避免了现有证据激活函数中的零证据区域，同时能够更好地表达负证据量。实验证明，该方法在多个基准数据集上优于现有方法。

    

    基于置信度理论和主观逻辑的证据深度学习提供了一种原则性和计算效率高的方法，用于将确定性神经网络变为不确定性感知的模型。结果产生的证据模型可以使用学习到的证据量化细粒度不确定性。为了确保理论上合理的证据模型，证据需要是非负的，这需要使用特殊的激活函数进行模型训练和推断。这个限制通常导致预测性能不如标准的softmax模型，使得将它们扩展到许多大规模数据集具有挑战性。为了揭示这种不良行为的真正原因，我们在理论上研究证据模型，并确定了一个根本限制，它解释了不良性能：现有的证明激活函数创建了零证据区域，这阻止了模型从落入这些区域的训练样本中学习。对证据激活函数的深入分析揭示了它们可以被视为将原始输入转换为证据量，然后使用信念更新规则进行聚合的转换器。受这一观察的启发，我们提出了一种新的激活函数，称为全正激活（All-Positive，AP），它通过构造避免零证据区域，优于现有的证据激活函数在许多基准数据集上。此外，我们表明AP激活减少了ReLU激活，从而恢复了ReLU的良好正定性质，并且它的泛化允许负证据量，因此比现有的证据激活更具表现力。实验证明，我们提出的方法在几个具有挑战性的基准数据集上实现了有竞争力的性能，并优于现有方法。

    Evidential deep learning, built upon belief theory and subjective logic, offers a principled and computationally efficient way to turn a deterministic neural network uncertainty-aware. The resultant evidential models can quantify fine-grained uncertainty using the learned evidence. To ensure theoretically sound evidential models, the evidence needs to be non-negative, which requires special activation functions for model training and inference. This constraint often leads to inferior predictive performance compared to standard softmax models, making it challenging to extend them to many large-scale datasets. To unveil the real cause of this undesired behavior, we theoretically investigate evidential models and identify a fundamental limitation that explains the inferior performance: existing evidential activation functions create zero evidence regions, which prevent the model to learn from training samples falling into such regions. A deeper analysis of evidential activation functions 
    
[^111]: 神经启动技术用于小样本自适应

    Neural Priming for Sample-Efficient Adaptation. (arXiv:2306.10191v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10191](http://arxiv.org/abs/2306.10191)

    本文提出神经启动技术，用于使大型预训练模型适应于分布变化和下游任务，无需过多标记样本。通过回忆数据进行轻量更新可以显著提高准确性。

    

    本文提出神经启动技术，用于在未经过大量标记样本的情况下，使大型预训练模型适应于分布变化和下游任务。在给定类名或无标签测试样本时，神经启动可以使模型回忆起预训练期间看到的相关数据并以此为基础条件化其参数，从而使其针对测试分布做好准备。神经启动还可以在测试时进行，即使是针对如LAION-2B这样大型预训练数据集。在各种分布变化和迁移学习基准测试中，对回忆数据进行轻量更新可以显著提高准确性。具体而言，在零样本设置下，我们看到ImageNet的准确性提高了2.45％，在标准的迁移学习基准测试中平均准确性提高了3.81％。此外，在推理时使用神经启动来适应分布变化，我们看到ImageNetV2的准确性提高了1.41％。这些结果证明了神经启动在处理小样本自适应挑战中的有效性。

    We propose Neural Priming, a technique for adapting large pretrained models to distribution shifts and downstream tasks given few or no labeled examples. Presented with class names or unlabeled test samples, Neural Priming enables the model to recall and conditions its parameters on relevant data seen throughout pretraining, thereby priming it for the test distribution. Neural Priming can be performed at test time, even for pretraining datasets as large as LAION-2B. Performing lightweight updates on the recalled data significantly improves accuracy across a variety of distribution shift and transfer learning benchmarks. Concretely, in the zero-shot setting, we see a 2.45% improvement in accuracy on ImageNet and 3.81% accuracy improvement on average across standard transfer learning benchmarks. Further, using Neural Priming at inference to adapt to distribution shift, we see a 1.41% accuracy improvement on ImageNetV2. These results demonstrate the effectiveness of Neural Priming in addr
    
[^112]: 文本到图像扩散模型中的能量交叉注意力用于贝叶斯上下文更新

    Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])

    [http://arxiv.org/abs/2306.09869](http://arxiv.org/abs/2306.09869)

    本论文提出了能量交叉注意力的EBM框架，通过更新和转移上下文向量，隐式最小化能量函数的嵌套层次，优化文本到图像扩散模型的语义对齐问题，实现零样本组合生成。

    

    尽管文本到图像扩散模型在图像生成任务中表现出色，但最近的研究提出了一个问题，即生成的图像有时无法捕捉到文本提示的预期语义内容，这种现象通常被称为语义错位。为了解决这个问题，我们提出了一种新颖的基于能量的模型（EBM）框架。具体而言，我们首先在去噪自编码器的每个交叉注意力层中制定潜在图像表示和文本嵌入的EBM。然后，我们获得上下文向量的对数后验梯度，可以更新和转移到后续的交叉注意力层，从而隐式地最小化嵌套层次的能量函数。我们的潜在EBMs还允许零样本组合生成，即通过不同上下文的交叉注意力输出的线性组合。通过大量实验，我们证明了所提出的方法在处理各种图像生成任务方面非常有效，并可以显著降低文本提示和生成图像之间的语义错位现象。

    Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation 
    
[^113]: 实用的锐度感知优化算法不能全程向最优点收敛

    Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima. (arXiv:2306.09850v1 [cs.LG])

    [http://arxiv.org/abs/2306.09850](http://arxiv.org/abs/2306.09850)

    该研究揭示了实用的锐度感知优化算法在某些情况下不能够全程向最优点收敛。

    

    锐度感知优化(SAM)是一种优化器，它基于当前点$x_t$的梯度，在扰动$y_t=x_t+\rho\frac{\nabla f(x_t)}{\lVert\nabla f(x_t)\rVert}$处进行下降。现有研究证明了SAM对于平滑函数的收敛性，但是它们假设扰动的大小$\rho$逐渐衰减和/或在$y_t$中没有梯度归一化，这与实践不符。为了弥补这一差距，我们研究了具有实用配置（即常数$\rho$和$y_t$中的梯度归一化）的确定性/随机版本的SAM，并探讨了它们在具有（非）凸性假设的平滑函数上的收敛性质。令人惊讶的是，在许多情况下，我们发现SAM在收敛到全局最小值或稳定点方面具有有限的能力。对于平滑强凸函数，我们展示了确定性SAM具有严格的全局收敛率为$\tilde\Theta(\frac{1}{T^2})$，而随机SAM的收敛界则受到噪声水平降低的影响，这表明了平面目标表面的尖锐度和平缓性之间平衡的挑战。

    Sharpness-Aware Minimization (SAM) is an optimizer that takes a descent step based on the gradient at a perturbation $y_t = x_t + \rho \frac{\nabla f(x_t)}{\lVert \nabla f(x_t) \rVert}$ of the current point $x_t$. Existing studies prove convergence of SAM for smooth functions, but they do so by assuming decaying perturbation size $\rho$ and/or no gradient normalization in $y_t$, which is detached from practice. To address this gap, we study deterministic/stochastic versions of SAM with practical configurations (i.e., constant $\rho$ and gradient normalization in $y_t$) and explore their convergence properties on smooth functions with (non)convexity assumptions. Perhaps surprisingly, in many scenarios, we find out that SAM has limited capability to converge to global minima or stationary points. For smooth strongly convex functions, we show that while deterministic SAM enjoys tight global convergence rates of $\tilde \Theta(\frac{1}{T^2})$, the convergence bound of stochastic SAM suffer
    
[^114]: 2023年声音分离挑战赛——音乐分离赛道技术报告

    Sound Demixing Challenge 2023 -- Music Demixing Track Technical Report. (arXiv:2306.09382v1 [cs.SD])

    [http://arxiv.org/abs/2306.09382](http://arxiv.org/abs/2306.09382)

    本文介绍了2023年声音分离挑战赛音乐分离赛道的两个有效方法，分别是时效高的源分离网络和适用于噪声鲁棒性源分离的损失掩模方法。

    

    本文介绍了我们在2023年声音分离挑战赛音乐分离赛道中获奖的解决方案。我们专注于两种在此挑战中设计的方法：一种时效高的源分离网络，在MUSDB基准测试上实现了最先进的结果；一种适用于噪声鲁棒性源分离的损失掩模方法。在github.com/kuielab/sdx23上提供了模型训练和最终提交的代码。

    In this report, we present our award-winning solutions for the Music Demixing Track of Sound Demixing Challenge 2023. We focus on two methods designed for this challenge: a time-efficient source separation network that achieves state-of-the-art results on the MUSDB benchmark and a loss masking method for noise-robust source separation. Code for reproducing model training and final submissions is available at github.com/kuielab/sdx23.
    
[^115]: TSMixer: 用于多元时间序列预测的轻量级MLP-Mixer模型

    TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting. (arXiv:2306.09364v1 [cs.LG])

    [http://arxiv.org/abs/2306.09364](http://arxiv.org/abs/2306.09364)

    TSMixer是一种用于多元时间序列预测的轻量级MLP-Mixer模型，可以有效地捕捉时间序列属性并在准确性方面超越了Transformers的方法。

    

    Transformers因其能够捕捉长序列交互而在时间序列预测中备受青睐。然而，其内存和计算要求高的问题对长期预测构成了严重瓶颈。为了解决这一问题，我们提出了TSMixer，这是一种轻量级神经架构，专为多元预测和补丁时间序列表示学习而设计，是Transformers的有效替代。我们的模型借鉴了MLP-Mixer模型在计算机视觉中的成功经验。我们展示了将视觉MLP-Mixer适应于时间序列的挑战，并引入了经过实验证实的组件以提高准确性。这包括一种新的设计范式，即将在线协调头附加到MLP-Mixer骨干上，以显式地建模时间序列的属性，如层次结构和通道相关性。我们还提出了一种混合通道建模方法，平衡了编码多个时间序列通道和保留单个通道信息之间的权衡。我们的实验表明，TSMixer在一元和多元时间序列预测任务中均实现了最先进的性能，同时需要比基于Transformers的方法少得多的参数。

    Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approa
    
[^116]: DreamSim：使用合成数据学习人类视觉相似性的新维度

    DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data. (arXiv:2306.09344v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.09344](http://arxiv.org/abs/2306.09344)

    本文提出了一种全面评估图像的感知度量DreamSim，该度量使用合成数据学习人类视觉相似性的新维度，表现出优越性能。

    

    当前的感知相似性度量是在像素和图像块的层面操作的。这些度量使用低层次的颜色和纹理来比较图像，但未能捕捉图像布局、对象姿态和语义内容的中层次相似性和差异。本文提出了一种可以全面评估图像的感知度量。我们的第一步是收集一个包含多个相似维度图像对的人类相似性判断的新数据集。这个数据集的关键是评判是几乎自动的，并且由所有观察者共享。为了实现这一点，我们使用最近的文本到图像模型创建了一些沿不同维度扰动的合成图像对。我们观察到，现有的流行的感知度量无法解释我们的新数据，因此引入了一种新的度量DreamSim，以更好地与人类感知相匹配。我们分析了不同视觉属性如何影响度量结果，并发现它严重关注前景物体和语义内容。DreamSim表现出优越性能，在广泛的任务中比现有的度量更优，包括预测行为实验结果、预测对抗鲁棒性和与人类相似性判断相关。

    Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic conte
    
[^117]: 使用大型语言模型探索MIT数学和EECS课程

    Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models. (arXiv:2306.08997v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.08997](http://arxiv.org/abs/2306.08997)

    通过使用大型语言模型，翻译了MIT数学和EECS课程中的4550个题目，开发出一个可以自动评分的模型，并探索了课程、问题和答案之间的关系。

    

    我们整理了一个综合性数据集，包括了获取学位所需的所有MIT数学和电气工程及计算机科学（EECS）课程的题目集、期中考试和期末考试中的4550个问题和解决方案。我们评估了大型语言模型实现任何MIT数学和EECS专业毕业要求的能力。我们的结果表明，GPT-3.5成功解决了整个MIT课程的三分之一，而GPT-4在题目中不包含图像的测试集上经过提示工程后达到了完美的解决率。我们在此数据集上对开源大型语言模型进行了微调。我们采用GPT-4自动评分，提供了课程、问题和答案类型的详细性能分析。通过将问题嵌入低维空间，我们探索了问题、主题和课程之间的关系，并发现哪些问题和课程是解决其他问题和课程所必需的。

    We curate a comprehensive dataset of 4,550 questions and solutions from problem sets, midterm exams, and final exams across all MIT Mathematics and Electrical Engineering and Computer Science (EECS) courses required for obtaining a degree. We evaluate the ability of large language models to fulfill the graduation requirements for any MIT major in Mathematics and EECS. Our results demonstrate that GPT-3.5 successfully solves a third of the entire MIT curriculum, while GPT-4, with prompt engineering, achieves a perfect solve rate on a test set excluding questions based on images. We fine-tune an open-source large language model on this dataset. We employ GPT-4 to automatically grade model responses, providing a detailed performance breakdown by course, question, and answer type. By embedding questions in a low-dimensional space, we explore the relationships between questions, topics, and classes and discover which questions and classes are required for solving other questions and classes
    
[^118]: MOFI: 从含噪实体标注的图像中学习图像表示

    MOFI: Learning Image Representations from Noisy Entity Annotated Images. (arXiv:2306.07952v1 [cs.CV])

    [http://arxiv.org/abs/2306.07952](http://arxiv.org/abs/2306.07952)

    MOFI 提出了一种新的方法，自动从含噪图像文本对中为图像指定实体标签，创建了一个新的大规模数据集 I2E，通过研究不同的训练配方，学习到了能够有效学习图像表示的模型。

    

    本文提出了一种新的视觉基础模型 MOFI，旨在从含噪实体标注的图像中学习图像表示。MOFI 与以往的工作有两点不同：（i）预训练数据，（ii）训练配方。在数据方面，我们引入了一种新方法，自动从含噪图像文本对中为图像指定实体标签。我们使用命名实体识别模型从 alt-text 中提取实体，然后使用 CLIP 模型选择正确的实体作为图像的标签。这种方法简单易行，不需要昂贵的人工注释，并且可以轻松扩展到从 web 上挖掘的数十亿个图像文本对。通过这种方法，我们创建了 Image-to-Entities（I2E）这一新的大规模数据集，其中包含 10 亿张图像和 200 万个不同的实体，涵盖了野外丰富的视觉概念。基于 I2E 数据集，我们研究了不同的训练配方，包括有监督的预训练、对比度预训练。

    We present MOFI, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. The approach is simple, does not require costly human annotation, and can be readily scaled up to billions of image-text pairs mined from the web. Through this method, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes, including supervised pre-training, contrastive pre-train
    
[^119]: 学习选择标签下的异质决策者：一种工具变量方法

    Learning under Selective Labels with Heterogeneous Decision-makers: An Instrumental Variable Approach. (arXiv:2306.07566v1 [stat.ML])

    [http://arxiv.org/abs/2306.07566](http://arxiv.org/abs/2306.07566)

    本文提出了一种处理选择性标记数据的学习问题的方法。通过利用历史决策由一组异质决策者做出的事实，我们建立了一种有原理的工具变量框架，并提出了一种加权学习方法，用于学习预测规则。

    

    我们研究了在选择性标记数据下的学习问题。这种问题在历史决策导致结果仅部分标记时出现。标记数据分布可能与整体人群有显著差异，特别是当历史决策和目标结果可以同时受某些未观察到的因素影响时。因此，仅基于标记数据进行学习可能会导致在整体人群中的严重偏差。我们的论文通过利用许多应用中历史决策由一组异质决策者做出的事实来解决此挑战。具体而言，我们在一个有原理的工具变量框架下分析了这种设置。我们建立了满足观察到的数据时任何给定预测规则的全体风险的点识别条件，并在点识别失败时提供了尖锐的风险界限。我们进一步提出了一种加权学习方法，用于学习预测规则。

    We study the problem of learning with selectively labeled data, which arises when outcomes are only partially labeled due to historical decision-making. The labeled data distribution may substantially differ from the full population, especially when the historical decisions and the target outcome can be simultaneously affected by some unobserved factors. Consequently, learning with only the labeled data may lead to severely biased results when deployed to the full population. Our paper tackles this challenge by exploiting the fact that in many applications the historical decisions were made by a set of heterogeneous decision-makers. In particular, we analyze this setup in a principled instrumental variable (IV) framework. We establish conditions for the full-population risk of any given prediction rule to be point-identified from the observed data and provide sharp risk bounds when the point identification fails. We further propose a weighted learning approach that learns prediction ru
    
[^120]: 通过神经表面渲染，在混乱场景中学习任意视角的6DoF机器人抓取

    Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural Surface Rendering. (arXiv:2306.07392v1 [cs.RO])

    [http://arxiv.org/abs/2306.07392](http://arxiv.org/abs/2306.07392)

    通过神经表面渲染，NeuGraspNet能够在混乱场景中有效地从任意视角预测6DoF抓取质量，并能够在遮挡的场景中采样抓取候选项。

    

    机器人操作在智能辅助等各种应用领域中至关重要。其中一个主要挑战是在杂乱的环境中从任何视角有效地抓取对象，而不需要额外的场景探索。我们引入了NeuGraspNet，一种新颖的6DoF抓取检测方法，利用了神经体积表示和表面渲染的最新进展。我们的方法学习了全局（场景级别）和局部（抓取级别）神经表面表示，使得即使在场景的未见部分，也能有效地预测6DoF抓取质量。此外，我们将抓取重新解释为一个局部的神经表面渲染问题，使得模型能够编码机器人末端执行器和对象表面几何之间的交互。NeuGraspNet在单个视角上运行，并且可以在遮挡的场景中采样抓取候选项，表现出优于现有隐式和半隐式基线模型的性能。

    Robotic manipulation is critical for admitting robotic agents to various application domains, like intelligent assistance. A major challenge therein is the effective 6DoF grasping of objects in cluttered environments from any viewpoint without requiring additional scene exploration. We introduce $\textit{NeuGraspNet}$, a novel method for 6DoF grasp detection that leverages recent advances in neural volumetric representations and surface rendering. Our approach learns both global (scene-level) and local (grasp-level) neural surface representations, enabling effective and fully implicit 6DoF grasp quality prediction, even in unseen parts of the scene. Further, we reinterpret grasping as a local neural surface rendering problem, allowing the model to encode the interaction between the robot's end-effector and the object's surface geometry. NeuGraspNet operates on single viewpoints and can sample grasp candidates in occluded scenes, outperforming existing implicit and semi-implicit baselin
    
[^121]: 一种用于评估图像识别模型鲁棒性的差分测试框架

    A Differential Testing Framework to Evaluate Image Recognition Model Robustness. (arXiv:2306.06208v1 [cs.CV])

    [http://arxiv.org/abs/2306.06208](http://arxiv.org/abs/2306.06208)

    本文提出了一种差分测试框架，用于评估图像识别模型鲁棒性。该框架通过使用一组参考图像并扰动计算环境，可确定模型性能是否受到计算环境变化的影响。

    

    图像识别任务通常使用深度学习，并需要巨大的处理能力，因此依赖于GPU和TPU等硬件加速器进行快速、及时的处理。在模型部署过程中，硬件加速器上的子优映射可能会导致实时图像识别任务失败，从而导致时间不确定性和错误行为。硬件加速器上的映射是通过多个软件组件进行的，例如深度学习框架、编译器、设备库等，我们称之为计算环境。随着图像识别任务在自动驾驶和医疗成像等安全关键应用中的增加，评估它们对计算环境变化的鲁棒性至关重要，因为深度学习框架、编译器优化和硬件设备等参数对模型性能和正确性的影响还不太清楚。在本文中，我们提出了一种差分测试框架，用于评估图像识别模型对计算环境变化的鲁棒性。我们的框架使用一组参考图像，并通过更改软件组件来扰动计算环境，生成具有已知预测输出差异的图像。通过比较原始图像和扰动图像的预测输出，我们可以确定模型性能是否受到计算环境变化的影响。我们通过测试三个图像识别模型的鲁棒性来证明我们框架的有效性，并确定其在计算环境变化下的性能受到影响的情况。

    Image recognition tasks typically use deep learning and require enormous processing power, thus relying on hardware accelerators like GPUs and TPUs for fast, timely processing. Failure in real-time image recognition tasks can occur due to sub-optimal mapping on hardware accelerators during model deployment, which may lead to timing uncertainty and erroneous behavior. Mapping on hardware accelerators is done through multiple software components like deep learning frameworks, compilers, device libraries, that we refer to as the computational environment. Owing to the increased use of image recognition tasks in safety-critical applications like autonomous driving and medical imaging, it is imperative to assess their robustness to changes in the computational environment, as the impact of parameters like deep learning frameworks, compiler optimizations, and hardware devices on model performance and correctness is not well understood.  In this paper we present a differential testing framewo
    
[^122]: NeuroGraph:面向脑连接组学的图机器学习基准测试

    NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics. (arXiv:2306.06202v1 [cs.LG])

    [http://arxiv.org/abs/2306.06202](http://arxiv.org/abs/2306.06202)

    本文介绍了神经成像领域的图机器学习基准测试NeuroGraph，并探讨了数据集生成的搜索空间。

    

    机器学习为分析高维功能性神经成像数据提供了有价值的工具，已被证明对预测各种神经疾病、精神障碍和认知模式有效。在功能磁共振成像研究中，大脑区域之间的相互作用通常使用基于图的表示进行建模。图机器学习方法的有效性已在多个领域得到证实，标志着数据解释和预测建模中的一个转变步骤。然而，尽管有前景，但由于图形数据集构建的广泛预处理流水线和大参数搜索空间，在神经成像领域中应用这些技术的转换仍然受到意外的限制。本文介绍了NeuroGraph(一个基于图的神经成像数据集)，它涵盖了多个行为和认知特征类别。我们深入探讨了数据集生成搜索空间

    Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional Magnetic Resonance Imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain remains surprisingly under-explored due to the expansive preprocessing pipeline and large parameter search space for graph-based datasets construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets that span multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by c
    
[^123]: 一种基于深度贝叶斯粒子流框架的跨领域软测量无监督建模方法

    Unsupervised Cross-Domain Soft Sensor Modelling via A Deep Bayesian Particle Flow Framework. (arXiv:2306.04919v1 [cs.LG])

    [http://arxiv.org/abs/2306.04919](http://arxiv.org/abs/2306.04919)

    该论文提出了一种基于深度粒子流贝叶斯框架的跨领域软测量无监督建模方法，能够解决标签缺失、领域适应性和数据时间一致性等问题，提高软测量建模的准确性。

    

    数据驱动的软测量对于通过可靠的状态推断实现精确感知至关重要。然而，由于存在标签缺失、领域适应性和数据时间一致性等问题，开发具有代表性的软测量模型具有挑战性。为了解决这些问题，我们提出了一种基于深度粒子流贝叶斯 (DPFB) 框架，用于在无目标状态标签情况下进行跨领域软测量建模。具体来说，首先制定了一个顺序贝叶斯目标，以执行潜在的跨领域软感知问题的最大似然估计。在框架核心，我们结合物理学启发的粒子流，通过优化顺序贝叶斯目标来执行模型提取的潜在和隐藏特征的精确贝叶斯更新。由此，这些贡献使得该框架能够学习一个有机的近似后验特征表示，能够表征复杂的跨领域系统动力学并实现有效的软测量建模。

    Data-driven soft sensors are essential for achieving accurate perception through reliable state inference. However, developing representative soft sensor models is challenged by issues such as missing labels, domain adaptability, and temporal coherence in data. To address these challenges, we propose a deep Particle Flow Bayes (DPFB) framework for cross-domain soft sensor modeling in the absence of target state labels. In particular, a sequential Bayes objective is first formulated to perform the maximum likelihood estimation underlying the cross-domain soft sensing problem. At the core of the framework, we incorporate a physics-inspired particle flow that optimizes the sequential Bayes objective to perform an exact Bayes update of the model extracted latent and hidden features. As a result, these contributions enable the proposed framework to learn a cohesive approximate posterior feature representation capable of characterizing complex cross-domain system dynamics and performing effe
    
[^124]: ChatGPT信息的图神经网络用于股票价格预测

    ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.03763](http://arxiv.org/abs/2306.03763)

    该研究介绍了一种新的框架，利用ChatGPT技术增强图神经网络，能够从财经新闻中提取出不断变化的网络结构，并用于股票价格预测，获得了超过基于深度学习的最新基准的表现，提示了ChatGPT在文本推断和金融预测方面的潜力。

    

    ChatGPT已在各种自然语言处理（NLP）任务中展示了出色的能力。然而，它从时间文本数据（尤其是财经新闻）推断动态网络结构的潜力仍是一个未开发的领域。在这项研究中，我们介绍了一个新的框架，利用ChatGPT的图推断能力来增强图神经网络（GNN）。我们的框架巧妙地从文本数据中提取出不断变化的网络结构，并将这些网络结构融合到图神经网络中，进行后续的预测任务。股票价格预测的实验结果表明，我们的模型始终优于基于深度学习的最新基准。此外，基于我们模型的产出构建的组合展示出更高的年化累计回报、更低的波动性和最大回撤。这种卓越表现突显了ChatGPT用于基于文本的网络推断和金融预测应用的潜力。

    ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and 
    
[^125]: 明藏暗窃：伪装数据窃取攻击在联邦学习中的应用

    Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning. (arXiv:2306.03013v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2306.03013](http://arxiv.org/abs/2306.03013)

    该研究提出了一个新的恶意服务器攻击框架SEER，可以在联邦学习中窃取用户数据，而且不易被客户端侦测出来。

    

    恶意服务器攻击已经使得联邦学习中的数据窃取在大批量和安全聚合等之前被视为私密的设置中变得可行。然而，许多关于恶意服务器攻击客户端侦测性的疑虑被提出，这使得它们在被公开知晓后的实用性受到质疑。在本研究中，我们首次全面研究了客户端侦测的问题。我们证明了许多基于两个关键原则的恶意服务器攻击都可以通过合理的客户端检查来检测出来。此外，我们制定了实用恶意服务器攻击的理想要求，并提出了SEER攻击框架，它满足所有理想要求，可以从现实网络的梯度中窃取用户数据，即使是在大批量(我们的实验中最大可达512)和安全聚合的情况下。SEER攻击的关键是使用秘密解码器，在共享模型上进行联合训练。我们的工作是迈向实用恶意服务器攻击的有前途的第一步。

    Malicious server (MS) attacks have enabled the scaling of data stealing in federated learning to large batch sizes and secure aggregation, settings previously considered private. However, many concerns regarding client-side detectability of MS attacks were raised, questioning their practicality once they are publicly known. In this work, for the first time, we thoroughly study the problem of client-side detectability.We demonstrate that most prior MS attacks, which fundamentally rely on one of two key principles, are detectable by principled client-side checks. Further, we formulate desiderata for practical MS attacks and propose SEER, a novel attack framework that satisfies all desiderata, while stealing user data from gradients of realistic networks, even for large batch sizes (up to 512 in our experiments) and under secure aggregation. The key insight of SEER is the use of a secret decoder, which is jointly trained with the shared model. Our work represents a promising first step to
    
[^126]: 数据污染下的超参数学习：基于多目标二层优化的正则化影响分析

    Hyperparameter Learning under Data Poisoning: Analysis of the Influence of Regularization via Multiobjective Bilevel Optimization. (arXiv:2306.01613v1 [cs.LG])

    [http://arxiv.org/abs/2306.01613](http://arxiv.org/abs/2306.01613)

    本文提出了一种考虑攻击对超参数影响的最优攻击公式，将攻击建模为多目标双层优化问题，可以更准确地评估算法鲁棒性和学习超参数，在多个数据集上的评估证明了这种方法的优势。

    

    机器学习算法容易遭受数据污染攻击，即通过操纵部分训练数据来有意破坏算法的性能。最优攻击可以被制定为双层优化问题，并有助于在最坏情况下评估算法的强健性。我们发现当前的方法通常假定超参数保持不变，这导致了对算法鲁棒性和正则化影响的过于悲观的观点。因此我们提出了一种新的最优攻击公式，考虑攻击对超参数的影响，并将攻击建模为多目标双层优化问题。这允许制定最优攻击、学习超参数并在最坏情况下评估鲁棒性。我们将此攻击公式应用于使用$L_2$和$L_1$正则化的多个机器学习分类器上。我们对多个数据集的评估确认了先前策略的限制，并证明了我们提出的方法具有更精确的鲁棒性评估和在存在数据污染攻击时更有效地学习超参数的优点。

    Machine Learning (ML) algorithms are vulnerable to poisoning attacks, where a fraction of the training data is manipulated to deliberately degrade the algorithms' performance. Optimal attacks can be formulated as bilevel optimization problems and help to assess their robustness in worst-case scenarios. We show that current approaches, which typically assume that hyperparameters remain constant, lead to an overly pessimistic view of the algorithms' robustness and of the impact of regularization. We propose a novel optimal attack formulation that considers the effect of the attack on the hyperparameters and models the attack as a multiobjective bilevel optimization problem. This allows to formulate optimal attacks, learn hyperparameters and evaluate robustness under worst-case conditions. We apply this attack formulation to several ML classifiers using $L_2$ and $L_1$ regularization. Our evaluation on multiple datasets confirms the limitations of previous strategies and evidences the ben
    
[^127]: 使用自适应流采样平衡训练能量基模型

    Balanced Training of Energy-Based Models with Adaptive Flow Sampling. (arXiv:2306.00684v1 [cs.LG])

    [http://arxiv.org/abs/2306.00684](http://arxiv.org/abs/2306.00684)

    本文研究了能量基模型的训练算法，使用归一化流进行采样，提高了模型的统计精度和生成性能。

    

    能量基模型 (EBM) 是一种直接参数化未标准化对数密度的多功能密度估计模型。EBM 非常灵活，但缺乏模型的规范化常量，使模型的似然函数计算不可行。近年来，已经提出了许多近似采样器和变分推理技术来估计似然函数梯度进行训练。这些技术在生成样本方面表现出色，但对于估计密度的统计精度，例如确定数据集中不同类的相对重要性，却付出了很少的关注。在本文中，我们提出了一种新的最大似然训练算法，使用一种不同类型的生成模型，归一化流 (NF)，这种模型最近被提出以便于采样。我们的方法在训练过程中将 NF 拟合到 EBM 上，以便 NF 辅助下的采样方案能够始终为 EBM 提供准确的梯度，最终提高模型的统计精度。实验结果表明，与传统 EBM 训练技术相比，我们的方法产生了更高质量的样本和更好的生成性能。

    Energy-based models (EBMs) are versatile density estimation models that directly parameterize an unnormalized log density. Although very flexible, EBMs lack a specified normalization constant of the model, making the likelihood of the model computationally intractable. Several approximate samplers and variational inference techniques have been proposed to estimate the likelihood gradients for training. These techniques have shown promising results in generating samples, but little attention has been paid to the statistical accuracy of the estimated density, such as determining the relative importance of different classes in a dataset. In this work, we propose a new maximum likelihood training algorithm for EBMs that uses a different type of generative model, normalizing flows (NF), which have recently been proposed to facilitate sampling. Our method fits an NF to an EBM during training so that an NF-assisted sampling scheme provides an accurate gradient for the EBMs at all times, ultim
    
[^128]: 癌症实体的关联和分类的机器学习方法

    Machine Learning Approach for Cancer Entities Association and Classification. (arXiv:2306.00013v1 [cs.CL])

    [http://arxiv.org/abs/2306.00013](http://arxiv.org/abs/2306.00013)

    本研究利用命名实体识别和文本分类的机器学习方法自动从大量生物医学文献中提取癌症相关实体和实体间关系，以帮助推进癌症研究进展。

    

    根据世界卫生组织（WHO）的数据，癌症是全球第二大死因。不同类型癌症的科学研究以每年发布大量的研究文章的速度不断增长。与基因相关的药物、诊断、风险、症状、治疗等的信息和知识是帮助探索和推进癌症研究进展的重要因素。手动筛选这么大量的文章非常费时费力，很难制定任何假设。本研究使用两种最为重要的自然语言处理（NLP）功能，实体识别和文本分类，从生物医学文献中发现知识。命名实体识别（NER）借助用户友好的界面和内置字典识别并提取与癌症相关的预定义实体。文本分类采用机器学习方法，帮助探究癌症实体之间的关系。

    According to the World Health Organization (WHO), cancer is the second leading cause of death globally. Scientific research on different types of cancers grows at an ever-increasing rate, publishing large volumes of research articles every year. The insight information and the knowledge of the drug, diagnostics, risk, symptoms, treatments, etc., related to genes are significant factors that help explore and advance the cancer research progression. Manual screening of such a large volume of articles is very laborious and time-consuming to formulate any hypothesis. The study uses the two most non-trivial NLP, Natural Language Processing functions, Entity Recognition, and text classification to discover knowledge from biomedical literature. Named Entity Recognition (NER) recognizes and extracts the predefined entities related to cancer from unstructured text with the support of a user-friendly interface and built-in dictionaries. Text classification helps to explore the insights into the 
    
[^129]: 基于流数据的神经网络在线学习的低秩扩展卡尔曼滤波算法

    Low-rank extended Kalman filtering for online learning of neural networks from streaming data. (arXiv:2305.19535v1 [stat.ML])

    [http://arxiv.org/abs/2305.19535](http://arxiv.org/abs/2305.19535)

    本文提出一种基于低秩扩展卡尔曼滤波的高效在线学习算法，其能够估计非线性函数的参数，具有更快的适应性和更快的奖励积累。

    

    本文提出了一种高效的在线近似贝叶斯推理算法，用于从可能非平稳的数据流中估计非线性函数的参数。该方法基于扩展卡尔曼滤波器（EKF），但使用了一种新颖的低秩加对角线的后验精度矩阵分解，其每步的成本与模型参数数量成线性关系。与基于随机变分推理的方法不同，我们的方法是完全确定的，并且不需要步长调整。我们通过实验证明，这导致更快（更高效）的学习，从而在用作上下文赌博算法的一部分时实现更快速的适应性和更快的奖励积累。

    We propose an efficient online approximate Bayesian inference algorithm for estimating the parameters of a nonlinear function from a potentially non-stationary data stream. The method is based on the extended Kalman filter (EKF), but uses a novel low-rank plus diagonal decomposition of the posterior precision matrix, which gives a cost per step which is linear in the number of model parameters. In contrast to methods based on stochastic variational inference, our method is fully deterministic, and does not require step-size tuning. We show experimentally that this results in much faster (more sample efficient) learning, which results in more rapid adaptation to changing distributions, and faster accumulation of reward when used as part of a contextual bandit algorithm.
    
[^130]: Bayesian隐式神经表示下的压缩

    Compression with Bayesian Implicit Neural Representations. (arXiv:2305.19185v1 [cs.LG])

    [http://arxiv.org/abs/2305.19185](http://arxiv.org/abs/2305.19185)

    该论文提出了一种用Bayesian隐式神经表示来压缩数据的方法，通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。

    

    许多常见类型的数据可以表示为将坐标映射到信号值的函数，例如图像中的像素位置到RGB值。基于这个观点，可以通过对数据的功能表示进行超拟合，然后编码网络权重来压缩数据。然而，大多数当前的解决方案都效率低下，因为将精度量化到低比特会大幅降低重构质量。为解决这个问题，我们提出了过度拟合变分贝叶斯神经网络来压缩近似后验权重样本，而不是量化和熵编码它。该策略通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。此外，我们引入了一种学习先验权重分布的迭代算法，并采用主动尺寸调整来进一步提高效率。

    Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $\beta$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $\beta$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a pro
    
[^131]: 基于稀疏矩阵草图的幅值剪枝的泛化界限

    Generalization Bounds for Magnitude-Based Pruning via Sparse Matrix Sketching. (arXiv:2305.18789v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18789](http://arxiv.org/abs/2305.18789)

    本文提出了基于稀疏矩阵草图的幅值剪枝的新颖泛化误差界限方法，其具有强的泛化界限，可以在剪枝和超参数模型泛化误差边界的算法开发方面开辟新的道路。

    

    本文针对超参数神经网络的幅值剪枝，提出了一种新颖的泛化误差界限。本工作基于Arora等人（2018）的边界，其中误差取决于剪枝引起的逼近和剪枝模型中参数的数量，并改进了标准基于范数的泛化边界。使用我们新的基于幅值的压缩算法得到的剪枝估计可以高概率地接近未剪枝函数，从而改善了第一个标准。通过稀疏矩阵草图，剪枝矩阵的空间可以在远小于其维度的密集矩阵空间中高效地表示，从而降低了第二个标准。这导致了比许多最先进方法更强的泛化界限，从而在剪枝和超参数模型泛化误差边界的算法开发方面开辟了新的道路。除此之外，我们扩展了我们的结果，获得了结构剪枝的泛化界限，其中还涉及到块状权重的剪枝。我们的理论发现得到了对不同网络结构和数据集的广泛实验支持。

    In this paper, we derive a novel bound on the generalization error of Magnitude-Based pruning of overparameterized neural networks. Our work builds on the bounds in Arora et al. [2018] where the error depends on one, the approximation induced by pruning, and two, the number of parameters in the pruned model, and improves upon standard norm-based generalization bounds. The pruned estimates obtained using our new Magnitude-Based compression algorithm are close to the unpruned functions with high probability, which improves the first criteria. Using Sparse Matrix Sketching, the space of the pruned matrices can be efficiently represented in the space of dense matrices of much smaller dimensions, thereby lowering the second criterion. This leads to stronger generalization bound than many state-of-the-art methods, thereby breaking new ground in the algorithm development for pruning and bounding generalization error of overparameterized models. Beyond this, we extend our results to obtain gen
    
[^132]: 任务等变图Few-shot学习

    Task-Equivariant Graph Few-shot Learning. (arXiv:2305.18758v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18758](http://arxiv.org/abs/2305.18758)

    本文提出了一种任务等变图Few-shot学习（TEG）框架，利用图神经网络的等变性质来使模型学习可转移的任务适应策略。该方法在各种Few-shot分类基准上展示了最先进的性能。

    

    虽然图神经网络（GNN）在节点分类任务中取得了成功，但其性能严重依赖于每类具有足够标记节点的可用性。在现实情况下，不是所有类都有许多标记节点，模型可能需要分类新类别，使得手动标记变得困难。为了解决这个问题，GNN需要能够用有限数量的标记节点对节点进行分类，称为Few-shot节点分类。先前的基于剧集元学习的方法在Few-shot节点分类中取得了成功，但我们的发现表明仅有多样的训练元任务才能实现最佳性能。为了应对基于元学习的Few-shot学习的挑战，我们提出了一种新的方法，即任务等变图Few-shot学习（TEG）框架。我们的TEG框架通过利用图神经网络的等变性质来使模型学习可转移的任务适应策略。我们在各种Few-shot分类基准上展示了我们提出的方法的有效性，实现了最先进的性能。

    Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strate
    
[^133]: 使用潜在扩散模型生成行为多样化的策略

    Generating Behaviorally Diverse Policies with Latent Diffusion Models. (arXiv:2305.18738v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18738](http://arxiv.org/abs/2305.18738)

    本文使用扩散模型将档案压缩为单个对于策略参数的生成模型，实现了13倍的压缩比率，同时保留了98%的原始回报和89%的原始覆盖率，并允许灵活选择和排序行为。

    

    最近在品质多样化强化学习（QD-RL）领域取得了进展，使得学习行为多样化、高性能的策略成为可能。然而，这些方法通常涉及存储数千个策略，导致空间复杂度高且难以适应更多行为的扩展。将档案压缩为单个模型，同时保留原始策略集的性能和覆盖率，已被证明是具有挑战性的。在本文中，我们提出使用扩散模型将归档压缩为单个对于策略参数的生成模型。我们展示了我们的方法实现了13倍的压缩比率，同时恢复了98%的原始回报和89%的原始覆盖率。扩散模型的调节机制还允许灵活选择和排序行为，包括使用语言。

    Recent progress in Quality Diversity Reinforcement Learning (QD-RL) has enabled learning a collection of behaviorally diverse, high performing policies. However, these methods typically involve storing thousands of policies, which results in high space-complexity and poor scaling to additional behaviors. Condensing the archive into a single model while retaining the performance and coverage of the original collection of policies has proved challenging. In this work, we propose using diffusion models to distill the archive into a single generative model over policy parameters. We show that our method achieves a compression ratio of 13x while recovering 98% of the original rewards and 89% of the original coverage. Further, the conditioning mechanism of diffusion models allows for flexibly selecting and sequencing behaviors, including using language. Project website: https://sites.google.com/view/policydiffusion/home
    
[^134]: 基于位置感知图增强变分自编码器的网络时间序列插补

    Networked Time Series Imputation via Position-aware Graph Enhanced Variational Autoencoders. (arXiv:2305.18612v1 [cs.LG])

    [http://arxiv.org/abs/2305.18612](http://arxiv.org/abs/2305.18612)

    本文提出了一种新颖的基于位置感知图增强变分自编码器方法来处理网络时间序列插补问题。该方法能够处理NTS数据中的图动态和缺失边，从而在合成和真实数据集上实现了优异的表现。

    

    多元时间序列插补是近年来广泛研究的问题。现有方法可以分为两大类，包括（1）主要关注时间序列特征的深度递归或生成模型，以及（2）基于图神经网络（GNN）的模型，利用MTS固有图结构的拓扑信息作为插补的关系归纳偏差。然而，这些方法要么忽略了拓扑信息，要么假定图结构固定且准确已知。因此，在更具挑战性的网络时间序列（NTS）数据中，它们无法充分利用图动态进行精确的插补，其中底层图不断变化并可能存在缺失边。本文提出了一种新方法来克服这些限制。首先，我们定义了包含节点时间序列特征和图结构中缺失值的NTS插补问题。然后，我们设计了一种名为PGE-VAE的新模型，它利用定位编码技术将时间序列信息合并到图神经网络中。具体而言，我们建议使用自我注意机制来捕捉图中不同时间步骤和不同节点之间的依赖关系。此外，我们引入了一个动态图生成网络来学习图结构的演化，可以处理缺失的边并适应图动态。在合成和真实数据集上的广泛实验表明，我们提出的方法优于现有最先进的方法。

    Multivariate time series (MTS) imputation is a widely studied problem in recent years. Existing methods can be divided into two main groups, including (1) deep recurrent or generative models that primarily focus on time series features, and (2) graph neural networks (GNNs) based models that utilize the topological information from the inherent graph structure of MTS as relational inductive bias for imputation. Nevertheless, these methods either neglect topological information or assume the graph structure is fixed and accurately known. Thus, they fail to fully utilize the graph dynamics for precise imputation in more challenging MTS data such as networked time series (NTS), where the underlying graph is constantly changing and might have missing edges. In this paper, we propose a novel approach to overcome these limitations. First, we define the problem of imputation over NTS which contains missing values in both node time series features and graph structures. Then, we design a new mod
    
[^135]: MemeGraphs: 将网络文化表情包与知识图谱相连

    MemeGraphs: Linking Memes to Knowledge Graphs. (arXiv:2305.18391v1 [cs.LG])

    [http://arxiv.org/abs/2305.18391](http://arxiv.org/abs/2305.18391)

    该论文提出了一种使用场景图和知识图谱结构化表达网络文化表情包的方法，并将其用于分类。结果显示该方法相比使用学习表达式的模型有所改善。

    

    网络文化表情包是一种在社交媒体和互联网上流行的传播趋势和观点的形式，结合了图像和文本的模式。它们可以表达幽默和讽刺，但也可能含有冒犯性的内容。自动分析和分类网络文化表情包具有挑战性，因为其解释依赖于对视觉元素、语言和背景知识的理解。因此，重要的是有意义地表示这些来源以及它们之间的交互，以便将表情包作为整体分类。在这项工作中，我们提出使用场景图作为表示图像中物体及其视觉关系的结构化表达方式，并将知识图谱作为网络文化表情包分类的结构化表示，使用基于Transformer的架构。我们将我们的方法与ImgBERT进行比较，后者使用仅学习（而不是结构化）的表达式进行多模式建模，我们观察到始终有所改善。我们还提供了一个具有人工图注释的数据集，供比较。

    Memes are a popular form of communicating trends and ideas in social media and on the internet in general, combining the modalities of images and text. They can express humor and sarcasm but can also have offensive content. Analyzing and classifying memes automatically is challenging since their interpretation relies on the understanding of visual elements, language, and background knowledge. Thus, it is important to meaningfully represent these sources and the interaction between them in order to classify a meme as a whole. In this work, we propose to use scene graphs, that express images in terms of objects and their visual relations, and knowledge graphs as structured representations for meme classification with a Transformer-based architecture. We compare our approach with ImgBERT, a multimodal model that uses only learned (instead of structured) representations of the meme, and observe consistent improvements. We further provide a dataset with human graph annotations that we compa
    
[^136]: 利用分子对接和机器学习回归方法的药物重用以靶向COVID-19 3CL Protease

    Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach. (arXiv:2305.18088v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2305.18088](http://arxiv.org/abs/2305.18088)

    本研究利用分子对接和机器学习回归方法，筛选出针对 SARS-CoV-2的主要蛋白酶3CL潜在治疗药物。其中，决策树回归（DTR）模型具有改进的统计措施R2和RMSE，有助于识别具有高结合亲和力和有利的结合能的药物。

    

    COVID-19疫情已经成为全球健康危机，迫切需要快速鉴定潜在的治疗药物。为了应对这一挑战，药物重用是省时省力的唯一解决方案。本研究使用Zinc数据库对全球已批准（包括FDA批准）的5903种药物进行筛选，作为潜在的COVID-19治疗药物，以靶向SARS-CoV-2的主要蛋白酶3CL。我们使用Autodock-Vina进行分子对接，检查药物分子的功效。为了提高药物重用的效率，我们采用决策树、额外树、MLP、KNN、XGBoost和梯度提升等多个机器学习回归方法建模结合药物的结合亲和力。计算结果表明，决策树回归（DTR）模型具有改进的统计措施R2和RMSE。这些模拟结果有助于识别具有高结合亲和力和有利的结合能的药物。

    The COVID-19 pandemic has created a global health crisis, driving the need for the rapid identification of potential therapeutics. To meet this challenge, drug repurposing is the only solution with saving cost and time. In this study, we used the Zinc database to screen the world-approved including FDA-approved 5903 drugs for repurposing as potential COVID-19 treatments targeting the main protease 3CL of SARS-CoV-2. We performed molecular docking using Autodock-Vina to check the efficacy of drug molecules. To enhance the efficiency of drug repurposing approach, we modeled the binding affinities using several machine learning regression approaches for QSAR modeling such as decision tree, extra trees, MLP, KNN, XGBoost, and gradient boosting. The computational results demonstrated that Decision Tree Regression (DTR) model has improved statistical measures of R2 and RMSE. These simulated results helped to identify drugs with high binding affinity and favorable binding energies. From the s
    
[^137]: 语言模型是有限实用说话者

    Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17760](http://arxiv.org/abs/2305.17760)

    本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。经过人类反馈的强化学习微调的大型语言模型具有概念上类似于 快与慢思考模型的思维模型，而这种思维模型被归因于人类。此研究凸显了采用认知概率建模方法对语言模型的理解、评估和推进的价值。

    

    本文提出了一个概率认知模型，称为有限实用说话者，用于表征不同变体的语言模型的操作方式。特别地，我们展示了经过人类反馈的强化学习微调的大型语言模型（Ouyang等人，2022）具有概念上类似于 快与慢思考模型（Kahneman，2011）的思维模型，而这种思维模型被心理学家们归因于人类。我们讨论了从人类反馈中的强化学习作为快与慢思考模型的局限性，并提出了扩展这个框架的途径。本研究实质上凸显了采用认知概率建模方法来获得对语言模型的理解、评估和推进方面的深刻见解的价值。

    How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
    
[^138]: PFN是适用于实际贝叶斯优化的灵活模型。

    PFNs Are Flexible Models for Real-World Bayesian Optimization. (arXiv:2305.17535v1 [cs.LG])

    [http://arxiv.org/abs/2305.17535](http://arxiv.org/abs/2305.17535)

    本文使用灵活的PFN作为BO代理建模，该模型能够允许进一步信息纳入以进行非远视BO。在三种不同的问题上得到了很好的结果。

    

    本文使用先验数据拟合网络(PFNs)作为贝叶斯优化(BO)的灵活代理。PFN是一种神经过程，被训练用于近似后验预测分布(PPD)，适用于任何可有效采样的先验分布。我们描述了如何利用这种灵活性来进行BO的代理建模。我们使用PFN来模拟一个朴素高斯过程(GP)，一个先进的GP和一个贝叶斯神经网络(BNN)。此外，我们展示了如何将进一步的信息纳入先验，例如允许有关最优位置的提示(用户先验)，忽略不相关的维度，并通过学习获取函数来执行非远视BO。这些扩展的灵活性为使用PFN进行BO开辟了广阔的可能性。我们在人工高斯过程样本和三个不同的超参数优化测试平台上展示了PFN对BO的有用性：HPO-B、Bayesmark和PD1。

    In this paper, we use Prior-data Fitted Networks (PFNs) as a flexible surrogate for Bayesian Optimization (BO). PFNs are neural processes that are trained to approximate the posterior predictive distribution (PPD) for any prior distribution that can be efficiently sampled from. We describe how this flexibility can be exploited for surrogate modeling in BO. We use PFNs to mimic a naive Gaussian process (GP), an advanced GP, and a Bayesian Neural Network (BNN). In addition, we show how to incorporate further information into the prior, such as allowing hints about the position of optima (user priors), ignoring irrelevant dimensions, and performing non-myopic BO by learning the acquisition function. The flexibility underlying these extensions opens up vast possibilities for using PFNs for BO. We demonstrate the usefulness of PFNs for BO in a large-scale evaluation on artificial GP samples and three different hyperparameter optimization testbeds: HPO-B, Bayesmark, and PD1. We publish code 
    
[^139]: 多个预训练模型的表示迁移学习在线性回归中的研究

    Representation Transfer Learning via Multiple Pre-trained models for Linear Regression. (arXiv:2305.16440v1 [cs.LG])

    [http://arxiv.org/abs/2305.16440](http://arxiv.org/abs/2305.16440)

    本文提出了一种基于表示迁移的学习方法，在给定很少数样本的情况下，通过提供一组在可能不同的数据领域上训练的预训练回归模型，来构建目标模型，使用这种方法可以提高模型的样本复杂度。

    

    本文研究了在给定很少数样本的情况下，如何在感兴趣的数据领域（目标）上学习线性回归模型。我们提出了一种基于表示迁移的学习方法，通过提供一组在可能不同的数据领域（来源）上训练的预训练回归模型，来构建目标模型。该方法由两个阶段组成：（i）利用不同的源表示来构造适应目标数据的表示，（ii）将所得到的模型作为初始值，通过微调程序，在目标数据上重新训练整个（超参数）回归模型。对于训练方法的每个阶段，我们提供了学习模型与真实数据生成目标模型之间的超额风险限制。导出的限制显示了样本复杂度的提高。

    In this paper, we consider the problem of learning a linear regression model on a data domain of interest (target) given few samples. To aid learning, we are provided with a set of pre-trained regression models that are trained on potentially different data domains (sources). Assuming a representation structure for the data generating linear models at the sources and the target domains, we propose a representation transfer based learning method for constructing the target model. The proposed scheme is comprised of two phases: (i) utilizing the different source representations to construct a representation that is adapted to the target data, and (ii) using the obtained model as an initialization to a fine-tuning procedure that re-trains the entire (over-parameterized) regression model on the target data. For each phase of the training method, we provide excess risk bounds for the learned model compared to the true data generating target model. The derived bounds show a gain in sample co
    
[^140]: 不对称学习率的分离式理性化: 一种灵活的Lipschitz限制

    Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipshitz Restraint. (arXiv:2305.13599v1 [cs.LG])

    [http://arxiv.org/abs/2305.13599](http://arxiv.org/abs/2305.13599)

    本文提出了一种名为DR的灵活的方法，它通过不对称的学习率来解决由合作博弈引发的退化问题，该方法能够在两个基准测试中显著改善表现。

    

    通常情况下，自说明理性化模型通过合作博弈构建，其中生成器从输入文本中选择最易理解的部分作为原理，接着预测器基于所选择的原理进行预测。然而，这种合作博弈可能会引发退化问题，预测器过度拟合于由尚未训练好的生成器生成的信息不足的部分，反过来导致生成器收敛于趋向于选择无意义的部分的次优模型。本文从理论上将退化问题与预测器的Lipschitz连续性联系起来。随后，我们实验性地提出了一种名为DR的简单而有效的方法，可以自然、灵活地约束预测器的Lipschitz常数，并解决了退化问题。DR方法的主要思想是将生成器和预测器分离，为它们分配不对称的学习率。在两个广泛使用的基准测试中进行的一系列实验表明，我们的DR方法能够显著改善现有方法的表现。

    A self-explaining rationalization model is generally constructed by a cooperative game where a generator selects the most human-intelligible pieces from the input text as rationales, followed by a predictor that makes predictions based on the selected rationales. However, such a cooperative game may incur the degeneration problem where the predictor overfits to the uninformative pieces generated by a not yet well-trained generator and in turn, leads the generator to converge to a sub-optimal model that tends to select senseless pieces. In this paper, we theoretically bridge degeneration with the predictor's Lipschitz continuity. Then, we empirically propose a simple but effective method named DR, which can naturally and flexibly restrain the Lipschitz constant of the predictor, to address the problem of degeneration. The main idea of DR is to decouple the generator and predictor to allocate them with asymmetric learning rates. A series of experiments conducted on two widely used benchm
    
[^141]: 自动说话人验证中的防御者角度：综述

    The defender's perspective on automatic speaker verification: An overview. (arXiv:2305.12804v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2305.12804](http://arxiv.org/abs/2305.12804)

    本文综述了针对欺诈攻击和部分伪造语音等ASV攻击类型的防御方法，填补了现有综述中的空白。

    

    自动说话人验证(ASV)在安全敏感的环境中发挥着至关重要的作用。然而，ASV的可靠性已经受到欺诈攻击(如重放和合成语音)、对抗攻击和相对较新的部分伪造语音的影响。虽然有几篇综述论文涵盖了重放和合成语音、对抗攻击，但缺乏一篇全面评估针对这些攻击类型进行防御方法的论文。因此，本文的目的是提供对这些攻击类型使用的防御方法的全面和系统的概述。

    Automatic speaker verification (ASV) plays a critical role in security-sensitive environments. Regrettably, the reliability of ASV has been undermined by the emergence of spoofing attacks, such as replay and synthetic speech, as well as adversarial attacks and the relatively new partially fake speech. While there are several review papers that cover replay and synthetic speech, and adversarial attacks, there is a notable gap in a comprehensive review that addresses defense against adversarial attacks and the recently emerged partially fake speech. Thus, the aim of this paper is to provide a thorough and systematic overview of the defense methods used against these types of attacks.
    
[^142]: 时空扩散点过程

    Spatio-temporal Diffusion Point Processes. (arXiv:2305.12403v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12403](http://arxiv.org/abs/2305.12403)

    本文提出了一种新的参数化框架，利用扩散模型来学习复杂的时空联合分布，提出了一个精心设计的时空共同注意力模块，以自适应捕捉事件时间和空间之间的相互依赖性，在时空点过程的可能性和事件预测方面均优于现有方法。

    

    时空点过程是带有时间和空间的随机事件集合。由于计算复杂度的限制，现有的时空点过程解决方案在考虑时间和空间分布时，会妥协于条件独立性，未能很好地建模联合分布。这导致在描述与过去事件相关的时空相互作用时能力受到限制。本文提出一种新的参数化框架，利用扩散模型来学习复杂的时空联合分布。我们将目标联合分布的学习分解为多个步骤，其中每个步骤可以被一个高斯分布很好地描述。为了增强每个步骤的学习, 我们提出了一个精心设计的时空共同注意力模块，以自适应地捕捉事件时间和空间之间的相互依赖性。我们第一次打破了对时空相互作用限制的规定，提出了一种完全灵活的时空点过程框架。实验结果表明，我们提出的方法在时空点过程的可能性和事件预测方面均优于现有方法。

    Spatio-temporal point process (STPP) is a stochastic collection of events accompanied with time and space. Due to computational complexities, existing solutions for STPPs compromise with conditional independence between time and space, which consider the temporal and spatial distributions separately. The failure to model the joint distribution leads to limited capacities in characterizing the spatio-temporal entangled interactions given past events. In this work, we propose a novel parameterization framework for STPPs, which leverages diffusion models to learn complex spatio-temporal joint distributions. We decompose the learning of the target joint distribution into multiple steps, where each step can be faithfully described by a Gaussian distribution. To enhance the learning of each step, an elaborated spatio-temporal co-attention module is proposed to capture the interdependence between the event time and space adaptively. For the first time, we break the restrictions on spatio-temp
    
[^143]: 多智能体强化学习: 异步通信和线性函数逼近

    Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation. (arXiv:2305.06446v1 [cs.LG])

    [http://arxiv.org/abs/2305.06446](http://arxiv.org/abs/2305.06446)

    该论文探讨了多智能体强化学习在情节式马尔可夫决策过程中的协作问题，提出了一种基于值迭代的算法，可以实现异步通信，在保证合作优势的同时降低通信开销。通过提供和证明的算法和复杂度界限，为多智能体强化学习在实际应用中提供理论依据。

    

    我们研究了多智能体强化学习在情节式马尔科夫决策过程中的设置，多个智能体通过中央服务器进行通信以合作。我们提出了一种基于值迭代的可证明有效的算法，可以实现异步通信，同时确保合作优势且通信开销低。我们证明了在使用线性函数逼近的情况下，我们的算法具有 $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ 的遗憾值和 $\tilde{\mathcal{O}}(dHM^2)$ 的通信复杂度，其中 $d$ 是特征维数，$H$ 是时间跨度，$M$ 是智能体总数，$K$ 是总情节数。我们还提供了一个下限证明，表明通过协作至少需要 $\Omega(dM)$ 的通信复杂度才能改善性能。

    We study multi-agent reinforcement learning in the setting of episodic Markov decision processes, where multiple agents cooperate via communication through a central server. We propose a provably efficient algorithm based on value iteration that enable asynchronous communication while ensuring the advantage of cooperation with low communication overhead. With linear function approximation, we prove that our algorithm enjoys an $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ regret with $\tilde{\mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the total number of agents, and $K$ is the total number of episodes. We also provide a lower bound showing that a minimal $\Omega(dM)$ communication complexity is required to improve the performance through collaboration.
    
[^144]: 可解释的平行RCNN与新颖特征表示法用于时间序列预测

    Explainable Parallel RCNN with Novel Feature Representation for Time Series Forecasting. (arXiv:2305.04876v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04876](http://arxiv.org/abs/2305.04876)

    本论文提出了一种新的特征表示策略——移位法，将过去数据和未来协变量融合起来进行时间序列预测，并开发了一个并行的深度学习框架，同时提出了一种可解释的方法来解释模型的特征重要性。

    

    准确的时间序列预测是数据科学中的基本挑战。往往会受到外部协变量，例如天气或人类干预的影响，这些影响可以在许多应用中被合理地预测。我们将它们称为预测的未来协变量。然而，现有的尝试使用自回归模型迭代预测时间序列的方法最终会导致指数级的误差累积。其他考虑用编码器和解码器分别处理历史和未来数据的策略会对自己产生限制。为了解决这些限制，我们提出了一种新的特征表示策略——移位法，将过去的数据和未来协变量融合起来以考虑它们的相互作用。为了提取时间序列中的复杂动力学，我们开发了一个并行的深度学习框架，由RNN和CNN组成，二者都被层级地使用。同时，我们还运用了跳跃连接技术来提高模型的训练效率和精度。最后，我们提出了一种可解释的方法来解释模型的特征重要性，为时间序列的底层过程提供了洞见。

    Accurate time series forecasting is a fundamental challenge in data science. It is often affected by external covariates such as weather or human intervention, which in many applications, may be predicted with reasonable accuracy. We refer to them as predicted future covariates. However, existing methods that attempt to predict time series in an iterative manner with autoregressive models end up with exponential error accumulations. Other strategies hat consider the past and future in the encoder and decoder respectively limit themselves by dealing with the historical and future data separately. To address these limitations, a novel feature representation strategy -- shifting -- is proposed to fuse the past data and future covariates such that their interactions can be considered. To extract complex dynamics in time series, we develop a parallel deep learning framework composed of RNN and CNN, both of which are used hierarchically. We also utilize the skip connection technique to impro
    
[^145]: Learngene: 从祖先模型中继承压缩知识到后代模型

    Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models. (arXiv:2305.02279v1 [cs.LG])

    [http://arxiv.org/abs/2305.02279](http://arxiv.org/abs/2305.02279)

    本文提出了一种机器学习范式 Learngene，将积累的知识压缩成更为紧凑的信息片段并继承给后代模型，以便于适应新的环境

    

    在一个生物的连续进化过程中，它的基因积累了广泛的经验和知识，使新生后代能够快速适应其特定环境。受到这一观察的启发，我们提出了一种新的机器学习范 paradigm，即 Learngene，使学习模型能够融合基因的三个关键特征。 (i) 积累：知识在祖先模型的连续学习过程中积累。 (ii) 压缩：将积累的详尽知识压缩成更为紧凑的信息片段，即 Learngene。 (iii) 继承：将压缩的 Learngene 继承给后代模型，以便于适应新的环境。由于积累已在一些成熟的范式中得到研究，如大规模预训练和终身学习，因此我们专注于压缩和继承，这引发了三个关键问题，并为这些问题提供了初步的解决方案。

    During the continuous evolution of one organism's ancestry, its genes accumulate extensive experiences and knowledge, enabling newborn descendants to rapidly adapt to their specific environments. Motivated by this observation, we propose a novel machine learning paradigm \textit{Learngene} to enable learning models to incorporate three key characteristics of genes. (i) Accumulating: the knowledge is accumulated during the continuous learning of an \textbf{ancestry model}. (ii) Condensing: the exhaustive accumulated knowledge is condensed into a much more compact information piece, \ie \textbf{learngene}. (iii): Inheriting: the condensed \textbf{learngene} is inherited to make it easier for \textbf{descendant models} to adapt to new environments. Since accumulating has been studied in some well-developed paradigms like large-scale pre-training and lifelong learning, we focus on condensing and inheriting, which induces three key issues and we provide the preliminary solutions to these is
    
[^146]: 在线Platt缩放及其校准方法

    Online Platt Scaling with Calibeating. (arXiv:2305.00070v1 [cs.LG])

    [http://arxiv.org/abs/2305.00070](http://arxiv.org/abs/2305.00070)

    本文提出了一种在线Platt缩放及其校准方法，其理论基础强大，可以处理分布漂移和对抗性结果序列，无需超参数调整，在一系列合成和真实数据集上表现出卓越的性能。

    

    我们提出了一种在线后校准方法，称为在线Platt缩放(OPS)，它将Platt缩放技术与在线逻辑回归相结合。我们展示了OPS如何在分布漂移的i.i.d.和非i.i.d.情况下平稳适应。此外，当最佳的Platt缩放模型本身被错误校准时，我们使用一种最近开发的称为calibeating的技术来增强OPS，使其更加鲁棒。理论上，我们得到的OPS+calibeating方法对于对抗性结果序列是保证校准的。在实验上，它在一系列合成和真实数据集上均表现出卓越的性能，无需超参数调整。最后，我们将所有OPS思想扩展到beta缩放方法。

    We present an online post-hoc calibration method, called Online Platt Scaling (OPS), which combines the Platt scaling technique with online logistic regression. We demonstrate that OPS smoothly adapts between i.i.d. and non-i.i.d. settings with distribution drift. Further, in scenarios where the best Platt scaling model is itself miscalibrated, we enhance OPS by incorporating a recently developed technique called calibeating to make it more robust. Theoretically, our resulting OPS+calibeating method is guaranteed to be calibrated for adversarial outcome sequences. Empirically, it is effective on a range of synthetic and real-world datasets, with and without distribution drifts, achieving superior performance without hyperparameter tuning. Finally, we extend all OPS ideas to the beta scaling method.
    
[^147]: MUDiff: 统一扩散生成完整分子

    MUDiff: Unified Diffusion for Complete Molecule Generation. (arXiv:2304.14621v1 [cs.LG])

    [http://arxiv.org/abs/2304.14621](http://arxiv.org/abs/2304.14621)

    MUDiff 是一种新的分子数据生成模型，它通过组合离散和连续扩散过程来生成全面的分子表示。使用扩散过程可以捕捉分子过程的概率本质，并探索不同因素对分子结构和性质的影响。我们的模型还包括一个新颖的图形转换器架构，用于去噪扩散过程。

    

    我们提出了一种新的分子数据生成模型，通过组合离散和连续扩散过程来生成分子数据。我们的模型生成了分子的全面表示，包括原子特征、二维离散分子结构和三维连续分子坐标。使用扩散过程可以捕捉分子过程的概率本质，并探索不同因素对分子结构和性质的影响。此外，我们提出了一种新颖的图形转换器架构，用于去噪扩散过程。转换器对欧几里得变换是等变的，使其能够学习不变的原子和边界表示，同时保持原子坐标的等变性。这种转换器可以用于学习对几何变换具有鲁棒性的分子表示。我们通过实验和与现有方法的比较评估了我们模型的性能，展示了其生成更稳定和有效的分子结构的能力。

    We present a new model for generating molecular data by combining discrete and continuous diffusion processes. Our model generates a comprehensive representation of molecules, including atom features, 2D discrete molecule structures, and 3D continuous molecule coordinates. The use of diffusion processes allows for capturing the probabilistic nature of molecular processes and the ability to explore the effect of different factors on molecular structures and properties. Additionally, we propose a novel graph transformer architecture to denoise the diffusion process. The transformer is equivariant to Euclidean transformations, allowing it to learn invariant atom and edge representations while preserving the equivariance of atom coordinates. This transformer can be used to learn molecular representations robust to geometric transformations. We evaluate the performance of our model through experiments and comparisons with existing methods, showing its ability to generate more stable and val
    
[^148]: TorchBench: 用高API表面覆盖率评估PyTorch性能的基准套件

    TorchBench: Benchmarking PyTorch with High API Surface Coverage. (arXiv:2304.14226v1 [cs.LG])

    [http://arxiv.org/abs/2304.14226](http://arxiv.org/abs/2304.14226)

    TorchBench是一款新型基准测试套件，可全面表征PyTorch软件栈的性能，指导模型、PyTorch框架和GPU库的性能优化。

    

    深度学习是多个领域中的革命性技术。为了方便模型的开发和部署，提出了许多深度学习框架，其中PyTorch是最流行的解决方案之一。PyTorch软件栈的生态性能至关重要，可节省模型训练成本并减少模型推理的响应时间。本文提出了TorchBench，一款新型基准测试套件，用于研究PyTorch软件栈的性能。与现有基准测试套件不同，TorchBench包含了许多代表性模型，覆盖了大量PyTorch API表面。TorchBench能够全面地表征PyTorch软件栈的性能，指导模型、PyTorch框架和GPU库的性能优化。我们展示了TorchBench的两个实际用例。第一，我们对TorchBench进行性能剖析，以识别PyTorch的GPU性能效率问题。我们能够优化许多性能故障并向上游提交贡献。

    Deep learning (DL) has been a revolutionary technique in various domains. To facilitate the model development and deployment, many deep learning frameworks are proposed, among which PyTorch is one of the most popular solutions. The performance of ecosystem around PyTorch is critically important, which saves the costs of training models and reduces the response time of model inferences. In this paper, we propose TorchBench, a novel benchmark suite to study the performance of PyTorch software stack. Unlike existing benchmark suites, TorchBench encloses many representative models, covering a large PyTorch API surface. TorchBench is able to comprehensively characterize the performance of the PyTorch software stack, guiding the performance optimization across models, PyTorch framework, and GPU libraries. We show two practical use cases of TorchBench. (1) We profile TorchBench to identify GPU performance inefficiencies in PyTorch. We are able to optimize many performance bugs and upstream pa
    
[^149]: 基于局部能量分布的随机模拟退火超参数确定

    Local Energy Distribution Based Hyperparameter Determination for Stochastic Simulated Annealing. (arXiv:2304.11839v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.11839](http://arxiv.org/abs/2304.11839)

    本文提出了一种基于局部能量分布的随机模拟退火超参数确定方法，该方法通过中心极限定理估计局部能量的分布，将超参数搜索的时间复杂度从O(n^3)降低到O(1)，在解决最大割问题中的实验中表现良好。

    

    本文提出了一种基于局部能量分布的随机模拟退火（SSA）超参数确定方法。 SSA能够比典型的模拟退火（SA）更快地解决组合优化问题，但需要耗费时间进行超参数搜索。所提出的方法基于自旋（概率比特）的局部能量分布来确定超参数。自旋是SSA的基本计算元素，并通过权重与其他自旋进行图形连接。局部能量的分布可以基于中心极限定理（CLT）进行估计。基于CLT的正态分布用于确定超参数，其将超参数搜索的时间复杂度从传统方法的O(n^3)降低到O(1)。使用确定的超参数评估了SSA在Gset和K2000基准上的性能，用于最大割问题。结果表明，所提出的方法实现了平均割值的近似值。

    This paper presents a local energy distribution based hyperparameter determination for stochastic simulated annealing (SSA). SSA is capable of solving combinatorial optimization problems faster than typical simulated annealing (SA), but requires a time-consuming hyperparameter search. The proposed method determines hyperparameters based on the local energy distributions of spins (probabilistic bits). The spin is a basic computing element of SSA and is graphically connected to other spins with its weights. The distribution of the local energy can be estimated based on the central limit theorem (CLT). The CLT-based normal distribution is used to determine the hyperparameters, which reduces the time complexity for hyperparameter search from O(n^3) of the conventional method to O(1). The performance of SSA with the determined hyperparameters is evaluated on the Gset and K2000 benchmarks for maximum-cut problems. The results show that the proposed method achieves mean cut values of approxim
    
[^150]: 基于深度学习的脑肿瘤多分类和分割在MRO图像中的应用

    Brain tumor multi classification and segmentation in MRO images using deep learning. (arXiv:2304.10039v1 [eess.IV])

    [http://arxiv.org/abs/2304.10039](http://arxiv.org/abs/2304.10039)

    本研究提出了基于深度学习的脑肿瘤分类和分割模型，能够准确地将MRI图像中的肿瘤分为四类，并将肿瘤从图像中精确地分割出来，具有潜在的临床应用价值。

    

    本研究提出了一种基于深度学习的模型，用于从磁共振成像（MRI）扫描中对脑肿瘤进行分类和分割。分类模型基于EfficientNetB1架构，经过训练，将图像分为四类：脑膜瘤，胶质瘤，垂体腺瘤和无肿瘤。分割模型基于U-Net架构，经过训练，能够准确地从MRI图像中分割出肿瘤。该模型在一个公开可用的数据集上进行评估，并实现了高精度和分割指标，表明其在脑肿瘤的诊断和治疗中具有潜在的临床应用价值。

    This study proposes a deep learning model for the classification and segmentation of brain tumors from magnetic resonance imaging (MRI) scans. The classification model is based on the EfficientNetB1 architecture and is trained to classify images into four classes: meningioma, glioma, pituitary adenoma, and no tumor. The segmentation model is based on the U-Net architecture and is trained to accurately segment the tumor from the MRI images. The models are evaluated on a publicly available dataset and achieve high accuracy and segmentation metrics, indicating their potential for clinical use in the diagnosis and treatment of brain tumors.
    
[^151]: 规范和非规范哈密顿算子推理

    Canonical and Noncanonical Hamiltonian Operator Inference. (arXiv:2304.06262v1 [cs.LG])

    [http://arxiv.org/abs/2304.06262](http://arxiv.org/abs/2304.06262)

    介绍了一种结构保留的模型简化方法，可以用于规范和非规范哈密顿系统，具有收敛性和良好的保守性质。

    

    本文介绍了一种用于规范和非规范哈密顿系统的非侵入式和结构保留的模型简化方法。基于算子推理的思想，这种技术是可证明收敛的，并且在给定快照数据和系统哈密顿的灰盒知识的情况下，可以简化为一个直接的线性求解问题。通过几个涉及多个双曲型偏微分方程的例子，结果表明所提出的方法产生的简化模型除了准确且稳定地处理基础模式的添加外，还可以很好地保留其训练数据范围之外的守恒量。

    A method for the nonintrusive and structure-preserving model reduction of canonical and noncanonical Hamiltonian systems is presented. Based on the idea of operator inference, this technique is provably convergent and reduces to a straightforward linear solve given snapshot data and gray-box knowledge of the system Hamiltonian. Examples involving several hyperbolic partial differential equations show that the proposed method yields reduced models which, in addition to being accurate and stable with respect to the addition of basis modes, preserve conserved quantities well outside the range of their training data.
    
[^152]: 模型稀疏化可以简化机器反学习

    Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])

    [http://arxiv.org/abs/2304.04934](http://arxiv.org/abs/2304.04934)

    本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。

    

    最近的数据管制要求机器反学习（MU）：从模型中移除指定样例的影响。虽然可以通过使用剩余数据从头开始进行模型重新训练来进行精确反学习，但是其计算成本导致了近似但高效的反学习方案的开发。除了数据中心的MU解决方案，我们通过一种新颖的基于模型的视角推进MU：通过权值修剪进行稀疏化。我们的理论和实践结果表明，模型稀疏性可以提高近似反学习器的多标准反学习性能，缩小近似间隙，同时保持高效。有了这个认识，我们制定了两个新的稀疏感知反学习元方案，称为“先修剪，然后反学习”和“稀疏感知反学习”。广泛的实验表明，我们的发现和提议在各种场景下始终有益于MU，包括按类数据擦除、随机数据擦除和后门数据伪造等。

    Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
    
[^153]: 从宽到深：维度提升网络用于参数高效的知识图谱嵌入

    From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])

    [http://arxiv.org/abs/2303.12816](http://arxiv.org/abs/2303.12816)

    本文提出了一个用于实现参数高效的知识图谱嵌入的深度网络，通过增加深度克服因采用低维实体表示而导致的模型精度下降和模型参数减少有限的问题。

    

    知识图谱嵌入（KGE）将实体和关系映射到向量表示对于下游任务非常重要。传统的KGE方法需要相对高维的实体表示来保留知识图谱的结构信息，但会导致庞大的模型参数。最近的方法通过采用低维实体表示来降低模型参数，同时开发技术（例如知识蒸馏）来补偿降维。然而，这样的操作会导致模型精度下降和模型参数减少有限。具体来说，我们将所有实体表示的级联视为嵌入层，那么采用高维实体表示的传统KGE方法等同于扩展嵌入层的宽度以获得表现力。为了在不牺牲准确度的情况下实现参数效率，我们相反地增加深度，并提出一个更深的实体嵌入网络。

    Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
    
[^154]: 熵正则化强化学习的莫特里卡多策略梯度：收敛性与全局最优性

    Matryoshka Policy Gradient for Entropy-Regularized RL: Convergence and Global Optimality. (arXiv:2303.12785v1 [cs.LG])

    [http://arxiv.org/abs/2303.12785](http://arxiv.org/abs/2303.12785)

    本文介绍了一种新的策略梯度算法——莫特里卡多策略梯度(MPG)，该算法尤其在最大熵强化学习中表现突出，能够实现一系列策略的训练和学习以达到任务的最优化，具有极高的收敛性和全局最优性。

    

    本文介绍并研究了一种新的策略梯度算法——莫特里卡多策略梯度(MPG)，在最大熵强化学习的背景下，代理目标是最大化除了累计奖励外的熵奖励。MPG与标准PG的不同之处在于它训练一系列策略同时学习有限的任务，而不是针对单一的标准目标训练一个单一的策略。对于softmax策略，我们证明了MPG的收敛性和极限的全局最优性，通过证明MPG目标的唯一临界点是最优策略；即使在连续紧致状态空间的情况下，这些结果仍然成立。MPG直观、理论上Sound，我们进一步展示了标准最大熵目标的最优策略可以通过MPG框架的最优策略进行任意精度的逼近。最后，我们证明了在策略用神经网络参数化的情况下，MPG非常适合。

    A novel Policy Gradient (PG) algorithm, called Matryoshka Policy Gradient (MPG), is introduced and studied, in the context of max-entropy reinforcement learning, where an agent aims at maximising entropy bonuses additional to its cumulative rewards. MPG differs from standard PG in that it trains a sequence of policies to learn finite horizon tasks simultaneously, instead of a single policy for the single standard objective. For softmax policies, we prove convergence of MPG and global optimality of the limit by showing that the only critical point of the MPG objective is the optimal policy; these results hold true even in the case of continuous compact state space. MPG is intuitive, theoretically sound and we furthermore show that the optimal policy of the standard max-entropy objective can be approximated arbitrarily well by the optimal policy of the MPG framework. Finally, we justify that MPG is well suited when the policies are parametrized with neural networks and we provide an simp
    
[^155]: 在差分隐私联邦学习中降低梯度平面化

    Make Landscape Flatter in Differentially Private Federated Learning. (arXiv:2303.11242v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.11242](http://arxiv.org/abs/2303.11242)

    提出了一个新的差分隐私联邦学习算法DP-FedSAM，它利用梯度扰动生成本地平坦模型，从而提高性能和隐私保证。

    

    为了防御推理攻击并减轻联邦学习中的敏感信息泄漏，客户端级别的差分隐私联邦学习（DPFL）通过裁剪本地更新和添加随机噪声成为隐私保护的事实标准。然而，现有的DPFL方法往往会导致更尖锐的损失平面，并具有较差的权重扰动鲁棒性，从而导致严重的性能下降。为了缓解这些问题，我们提出了一种新的DPFL算法DP-FedSAM，它利用梯度扰动来减轻DP的负面影响。具体来说，DP-FedSAM将Sharpness Aware Minimization（SAM）优化器集成到其中，生成更稳定和具有更好的权重扰动鲁棒性的本地平坦模型，这导致本地更新的范数小且对DP噪声有鲁棒性，从而提高性能。从理论上来说，我们详细分析了DP-FedSAM如何减轻DP导致的性能下降。与此同时，我们基于Rényi差分隐私框架给出了严格的隐私分析，以保证我们提出的算法具有强隐私保证。在多个基准数据集上的广泛实验证明，在各种敌对设置下，DP-FedSAM相对于最先进的DPFL方法具有卓越的性能。

    To defend the inference attacks and mitigate the sensitive information leakages in Federated Learning (FL), client-level Differentially Private FL (DPFL) is the de-facto standard for privacy protection by clipping local updates and adding random noise. However, existing DPFL methods tend to make a sharper loss landscape and have poorer weight perturbation robustness, resulting in severe performance degradation. To alleviate these issues, we propose a novel DPFL algorithm named DP-FedSAM, which leverages gradient perturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM integrates Sharpness Aware Minimization (SAM) optimizer to generate local flatness models with better stability and weight perturbation robustness, which results in the small norm of local updates and robustness to DP noise, thereby improving the performance. From the theoretical perspective, we analyze in detail how DP-FedSAM mitigates the performance degradation induced by DP. Meanwhile, we give rigor
    
[^156]: 基于占据核主成分分析的故障检测

    Fault Detection via Occupation Kernel Principal Component Analysis. (arXiv:2303.11138v1 [stat.ML])

    [http://arxiv.org/abs/2303.11138](http://arxiv.org/abs/2303.11138)

    本文提出了一种使用占据核PCA方法进行故障检测的新方法，并且通过数值模拟验证了其有效性。

    

    自动系统的可靠操作很大程度上依赖于检测基础动态系统中的故障。虽然传统的基于模型的方法已被广泛用于故障检测，但基于数据的方法因其易于部署和对专家知识需求最小的特点而受到越来越多的关注。本文提出了一种使用占据核进行主成分分析（PCA）的新方法。占据核产生的特征映射适用于测量数据，由于使用积分具有内在的噪声鲁棒性，并且可以利用长度可变的不规则采样系统轨迹进行PCA。占据核PCA方法被用于开发一种重构误差方法进行故障检测，并且通过数值模拟验证了其有效性。

    The reliable operation of automatic systems is heavily dependent on the ability to detect faults in the underlying dynamical system. While traditional model-based methods have been widely used for fault detection, data-driven approaches have garnered increasing attention due to their ease of deployment and minimal need for expert knowledge. In this paper, we present a novel principal component analysis (PCA) method that uses occupation kernels. Occupation kernels result in feature maps that are tailored to the measured data, have inherent noise-robustness due to the use of integration, and can utilize irregularly sampled system trajectories of variable lengths for PCA. The occupation kernel PCA method is used to develop a reconstruction error approach to fault detection and its efficacy is validated using numerical simulations.
    
[^157]: 高维单个ReLU神经元的有限样本学习分析

    Finite-Sample Analysis of Learning High-Dimensional Single ReLU Neuron. (arXiv:2303.02255v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02255](http://arxiv.org/abs/2303.02255)

    本文研究了高维单个ReLU神经元的有限样本学习问题，并提供了感知器算法GLM-tron的风险上下界，其中包括特殊情况，为高维ReLU回归问题提供了清晰的刻画。此外，对于对称伯努利数据的ReLU回归，随机梯度下降的过多风险不如GLM-tron。

    

    本文研究了在过参数化的情况下（即输入维度可能超出样本数），学习具有平方损失的单个ReLU神经元的问题。我们分析了称为GLM-tron（Kakade等人，2011）的感知器算法，并提供了其维度无关的风险上界，用于高维ReLU回归的良好规定和规定错误设置。我们的风险上界恢复了几个现有结果作为特例。此外，在良好规定的情况下，我们为GLM-tron提供了一个实例匹配风险下界。我们的上下风险界提供了对可以通过GLM-tron学习的高维ReLU回归问题的清晰刻画。另一方面，我们针对对称伯努利数据的ReLU回归提供了一些随机梯度下降（SGD）的负面结果：如果模型规定良好，则SGD的过多风险可证明不比无视常数因素的GLM-tron的过多风险好。

    This paper considers the problem of learning a single ReLU neuron with squared loss (a.k.a., ReLU regression) in the overparameterized regime, where the input dimension can exceed the number of samples. We analyze a Perceptron-type algorithm called GLM-tron (Kakade et al., 2011) and provide its dimension-free risk upper bounds for high-dimensional ReLU regression in both well-specified and misspecified settings. Our risk bounds recover several existing results as special cases. Moreover, in the well-specified setting, we provide an instance-wise matching risk lower bound for GLM-tron. Our upper and lower risk bounds provide a sharp characterization of the high-dimensional ReLU regression problems that can be learned via GLM-tron. On the other hand, we provide some negative results for stochastic gradient descent (SGD) for ReLU regression with symmetric Bernoulli data: if the model is well-specified, the excess risk of SGD is provably no better than that of GLM-tron ignoring constant fa
    
[^158]: 启发式模块化最大化算法很难返回最优划分或相似结果的社区检测

    Heuristic Modularity Maximization Algorithms for Community Detection Rarely Return an Optimal Partition or Anything Similar. (arXiv:2302.14698v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2302.14698](http://arxiv.org/abs/2302.14698)

    通过使用80个真实和随机网络，研究对比了当前8种基于模块化启发式算法和一种精确整数规划方法在社区检测中的优化表现，结果显示平均只有16.9%的启发式算法返回最优划分或相似结果。

    

    社区检测是计算科学中的一项基本问题，在各个领域都有广泛应用。最常用的方法是设计算法来在网络节点的不同划分之间最大化模块化。通过使用来自各种背景的80个真实和随机网络，我们调查了当前启发式模块化最大化算法在返回最大模块化（最优）划分方面的成功程度。我们评估了（1）算法输出模块化与每个输入图的最大模块化之比，以及（2）它们的输出划分与该图的任何最优划分之间的最大相似度。我们将八种现有的启发式算法与全局最大化模块化的精确整数规划方法进行比较。平均基于模块化的启发式算法只为考虑的80个图的16.9%返回最优划分。此外，根据调整后的互信息的结果显示出实质性差异。

    Community detection is a fundamental problem in computational sciences with extensive applications in various fields. The most commonly used methods are the algorithms designed to maximize modularity over different partitions of the network nodes. Using 80 real and random networks from a wide range of contexts, we investigate the extent to which current heuristic modularity maximization algorithms succeed in returning maximum-modularity (optimal) partitions. We evaluate (1) the ratio of the algorithms' output modularity to the maximum modularity for each input graph, and (2) the maximum similarity between their output partition and any optimal partition of that graph. We compare eight existing heuristic algorithms against an exact integer programming method that globally maximizes modularity. The average modularity-based heuristic algorithm returns optimal partitions for only 16.9% of the 80 graphs considered. Additionally, results on adjusted mutual information reveal substantial diss
    
[^159]: SpikeGPT：带有脉冲神经网络的生成预训练语言模型

    SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. (arXiv:2302.13939v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.13939](http://arxiv.org/abs/2302.13939)

    本论文提出了一种称之为SpikeGPT的生成语言模型，使用二进制、事件驱动脉冲激活单元进行训练，克服了SNN训练中的挑战性。该模型可以用于大规模语言生成任务。

    

    随着大型语言模型的规模不断扩大，所需的计算资源也随之增加。脉冲神经网络（SNN）已成为一种能够利用稀疏和事件驱动激活减少模型推理计算开销的节能深度学习方法。虽然它们在许多计算机视觉任务上已经具有竞争力，但SNN的训练也被证明更具挑战性。因此，它们的性能落后于现代深度学习，我们尚未看到SNN在语言生成方面的有效性。在本文中，我们受到Receptance Weighted Key Value（RWKV）语言模型的启发，成功实现了“SpikeGPT”，它是一种具有二进制、事件驱动脉冲激活单元的生成语言模型。我们在两种模型变体上训练了所提出的模型：45M和216M参数。据我们所知，SpikeGPT是迄今最大的反向传播训练SNN模型，使其适用于非脉冲模型通常解决的大规模语言生成任务。

    As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suita
    
[^160]: normflows：一种用于正则化流的PyTorch包

    normflows: A PyTorch Package for Normalizing Flows. (arXiv:2302.12014v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12014](http://arxiv.org/abs/2302.12014)

    normflows是一个用于正则化流的PyTorch包，它允许使用基本分布、流层和神经网络构建正则化流模型，以建模概率分布。它在机器学习领域中应用广泛。

    

    正则化流通过一个可表达的可解密度来建模概率分布。它们通过一系列可逆函数（称为图层）将简单的基础分布（如高斯分布）转换。这些图层通常使用神经网络进行实现，使它们具有很高的表达能力。正则化流在机器学习中无处不在，并已应用于图像生成、文本建模、变分推断、逼近玻尔兹曼分布等众多问题上。在本文中，我们提出了normflows，这是一个用于正则化流的Python包。它允许从一系列基本分布、流层和神经网络构建正则化流模型。该包采用了广受欢迎的深度学习框架PyTorch实现，这使得将流集成到更大的机器学习模型或流水线中变得更简单。它支持大部分常见的正则化流架构，如Real NVP、Glow、Masked Autoregressive Flows、Neural Spline Flows、Residual Flows等。

    Normalizing flows model probability distributions through an expressive tractable density. They transform a simple base distribution, such as a Gaussian, through a sequence of invertible functions, which are referred to as layers. These layers typically use neural networks to become very expressive. Flows are ubiquitous in machine learning and have been applied to image generation, text modeling, variational inference, approximating Boltzmann distributions, and many other problems. Here, we present normflows, a Python package for normalizing flows. It allows to build normalizing flow models from a suite of base distributions, flow layers, and neural networks. The package is implemented in the popular deep learning framework PyTorch, which simplifies the integration of flows in larger machine learning models or pipelines. It supports most of the common normalizing flow architectures, such as Real NVP, Glow, Masked Autoregressive Flows, Neural Spline Flows, Residual Flows, and many more.
    
[^161]: K-SHAP: 一种用于匿名状态-动作对的策略聚类算法

    K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs. (arXiv:2302.11996v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11996](http://arxiv.org/abs/2302.11996)

    本文提出了一种名为K-SHAP的算法，来解决多个智能体保持匿名且仅有状态-动作对的情况下学习智能体决策的问题。

    

    从观测数据中学习智能体行为已经被证明可以提高我们对它们决策过程的理解，从而增强我们解释它们与环境和其他智能体之间交互的能力。尽管文献中已经提出了多种学习技术，但还有一种特定的情况尚未被探索，那就是智能体身份保持匿名的多智能体系统。例如，在金融市场中，标记数据通常是专有的，仅公开多个市场参与者交互而产生的匿名状态-动作对。因此，智能体行动序列不可观测，限制了现有工作的适用性。本文提出了一种策略聚类算法K-SHAP，它学习根据智能体策略对匿名状态-动作对进行分组。我们将该问题作为模仿学习(IL)任务，学习一个w...

    Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. In this paper, we propose a Policy Clustering algorithm, called K-SHAP, that learns to group anonymous state-action pairs according to the agent policies. We frame the problem as an Imitation Learning (IL) task, and we learn a w
    
[^162]: 随机生成流网络

    Stochastic Generative Flow Networks. (arXiv:2302.09465v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09465](http://arxiv.org/abs/2302.09465)

    Stochastic GFlowNets通过学习动态模型来扩展了GFlowNets的适用性，解决了原模型只适用于确定性环境的问题，并在具有随机动态的各种标准基准测试中表现良好，具有显著优势。

    

    生成流网络（或简称GFlowNets）是一类通过“推理即控制”学习采样复杂组合结构的概率代理。它们在从给定的能量景观中生成高质量、多样化的候选方面表现出巨大的潜力。然而，现有的GFlowNets仅适用于确定性环境，并在具有随机动态的更一般任务中失败，这可能限制了它们的适用性。为了克服这一挑战，本文介绍了一种新的算法，称为随机GFlowNets，它将GFlowNets扩展到随机环境。通过将状态转移分解为两个步骤，随机GFlowNets分离了环境随机性并学习了一个动态模型来捕捉它。广泛的实验结果表明，随机GFlowNets在具有随机动态的各种标准基准测试中，相较于标准GFlowNets以及MCMC和基于RL的方法，具有显著的优势。

    Generative Flow Networks (or GFlowNets for short) are a family of probabilistic agents that learn to sample complex combinatorial structures through the lens of "inference as control". They have shown great potential in generating high-quality and diverse candidates from a given energy landscape. However, existing GFlowNets can be applied only to deterministic environments, and fail in more general tasks with stochastic dynamics, which can limit their applicability. To overcome this challenge, this paper introduces Stochastic GFlowNets, a new algorithm that extends GFlowNets to stochastic environments. By decomposing state transitions into two steps, Stochastic GFlowNets isolate environmental stochasticity and learn a dynamics model to capture it. Extensive experimental results demonstrate that Stochastic GFlowNets offer significant advantages over standard GFlowNets as well as MCMC- and RL-based approaches, on a variety of standard benchmarks with stochastic dynamics.
    
[^163]: 在线工具变量回归: 遗憾分析和Bandit反馈

    Online Instrumental Variable Regression: Regret Analysis and Bandit Feedback. (arXiv:2302.09357v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09357](http://arxiv.org/abs/2302.09357)

    该论文研究了在线学习中内生性问题的解决方法，提出了使用Two-Stage Least Squares方法的在线变体O2SLS来处理内生性，取得了较好的识别率和预测遗憾率。

    

    内生性是实际数据中常见的现象，因为遗漏变量、战略行为、测量误差等原因导致噪声和协变量之间的依赖性。与之相反，现有的无界噪声和线性Bandit随机在线线性回归分析严重依赖外生性，即噪声和协变量之间的独立性。鉴于这一差距，我们研究了工具变量（IV）回归在随机在线学习中的超识别和恰好识别情况。我们提出使用Two-Stage Least Squares方法的在线变体（即O2SLS）来处理内生性。我们的分析表明，O2SLS实现了$ \mathcal{O} \left(d_x d_z \log ^ 2 T \right)$的识别率和$ \tilde {\mathcal {O}} \left(\gamma \sqrt {d_x T} \right)$的预测遗憾率。

    Endogeneity, i.e. the dependence between noise and covariates, is a common phenomenon in real data due to omitted variables, strategic behaviours, measurement errors etc. In contrast, the existing analyses of stochastic online linear regression with unbounded noise and linear bandits depend heavily on exogeneity, i.e. the independence between noise and covariates. Motivated by this gap, we study the over-and just-identified Instrumental Variable (IV) regression for stochastic online learning. IV regression and the Two-Stage Least Squares approach to it are widely deployed in economics and causal inference to identify the underlying model from an endogenous dataset. Thus, we propose to use an online variant of Two-Stage Least Squares approach, namely O2SLS, to tackle endogeneity in stochastic online learning. Our analysis shows that O2SLS achieves $\mathcal{O}\left(d_x d_z \log ^2 T\right)$ identification and $\tilde{\mathcal{O}}\left(\gamma \sqrt{d_x T}\right)$ oracle regret after $T$ 
    
[^164]: 近乎贝叶斯最优的伪标签选择

    Approximately Bayes-Optimal Pseudo Label Selection. (arXiv:2302.08883v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08883](http://arxiv.org/abs/2302.08883)

    本文介绍了BPLS，一种用于PLS的贝叶斯框架，通过解析逼近选择标签实例的标准，以避免由过度自信但错误预测的实例选择而导致的确认偏差问题。

    

    自训练的半监督学习严重依赖于伪标签选择（PLS）。选择通常取决于初始模型拟合标记数据的程度。过早的过拟合可能通过选择具有过度自信但错误的预测的实例（通常称为确认偏差）而传播到最终模型。本文介绍了BPLS，这是一种用于PLS的贝叶斯框架，旨在减轻这个问题。其核心是选择标签实例的标准：伪样本的后验预测的分析近似。我们通过证明伪样本的后验预测的贝叶斯最优性获得了这种选择标准。我们进一步通过解析逼近克服计算难题。它与边际似然的关系使我们能够提出基于拉普拉斯方法和高斯积分的逼近。我们针对参数广义线性和非参数广义加性模型对BPLS进行了实证评估。

    Semi-supervised learning by self-training heavily relies on pseudo-label selection (PLS). The selection often depends on the initial model fit on labeled data. Early overfitting might thus be propagated to the final model by selecting instances with overconfident but erroneous predictions, often referred to as confirmation bias. This paper introduces BPLS, a Bayesian framework for PLS that aims to mitigate this issue. At its core lies a criterion for selecting instances to label: an analytical approximation of the posterior predictive of pseudo-samples. We derive this selection criterion by proving Bayes optimality of the posterior predictive of pseudo-samples. We further overcome computational hurdles by approximating the criterion analytically. Its relation to the marginal likelihood allows us to come up with an approximation based on Laplace's method and the Gaussian integral. We empirically assess BPLS for parametric generalized linear and non-parametric generalized additive models
    
[^165]: 寻找一个适用于无源域自适应的通用方法

    In Search for a Generalizable Method for Source Free Domain Adaptation. (arXiv:2302.06658v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06658](http://arxiv.org/abs/2302.06658)

    本研究针对生物声学中的分布变化问题，对现有的无源域自适应（SFDA）方法进行了探究和对比发现，现有方法的通用性不如之前想象的那样好，作者提出的新方法优于现有方法且具有较好的通用性。

    

    无源域自适应（SFDA）很有吸引力，因为它允许使用未标记的数据将现成的模型适应到新领域。在这项研究中，我们将现有的SFDA技术应用于生物声学中一组具有挑战性的自然分布偏移，这些偏移与计算机视觉中通常研究的偏移非常不同。我们发现，现有的方法在相对排名上的表现与视觉基准中观察到的不同，并且有时比完全没有自适应表现更差。我们提出了一种新的简单方法，在我们的新偏移中表现优于现有方法，同时在一系列视觉数据集上表现出强大的性能。我们的发现表明，现有的SFDA方法并不像之前想象的那样具有通用性，并且考虑到不同的模态可以成为设计更强大模型的有用途径。

    Source-free domain adaptation (SFDA) is compelling because it allows adapting an off-the-shelf model to a new domain using only unlabelled data. In this work, we apply existing SFDA techniques to a challenging set of naturally-occurring distribution shifts in bioacoustics, which are very different from the ones commonly studied in computer vision. We find existing methods perform differently relative to each other than observed in vision benchmarks, and sometimes perform worse than no adaptation at all. We propose a new simple method which outperforms the existing methods on our new shifts while exhibiting strong performance on a range of vision datasets. Our findings suggest that existing SFDA methods are not as generalizable as previously thought and that considering diverse modalities can be a useful avenue for designing more robust models.
    
[^166]: 新“Sparseland”中我们学到的十个教训: 稀疏神经网络研究者的简短手册

    Ten Lessons We Have Learned in the New "Sparseland": A Short Handbook for Sparse Neural Network Researchers. (arXiv:2302.02596v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02596](http://arxiv.org/abs/2302.02596)

    稀疏神经网络研究群体需要解决的十个核心问题与解决方案被总结在本文中。

    

    本文旨在为不断发展的稀疏神经网络(SNN)研究群体服务。我们尝试总结了SNN中最常见的一些困惑，并从许多关键方面总结了十个问题和答案，包括密集与稀疏，无结构稀疏与结构稀疏，修剪与稀疏训练，稠密到稀疏训练与稀疏到稀疏训练，静态稀疏和动态稀疏等。

    This article does not propose any novel algorithm or new hardware for sparsity. Instead, it aims to serve the "common good" for the increasingly prosperous Sparse Neural Network (SNN) research community. We attempt to summarize some most common confusions in SNNs, that one may come across in various scenarios such as paper review/rebuttal and talks - many drawn from the authors' own bittersweet experiences! We feel that doing so is meaningful and timely, since the focus of SNN research is notably shifting from traditional pruning to more diverse and profound forms of sparsity before, during, and after training. The intricate relationships between their scopes, assumptions, and approaches lead to misunderstandings, for non-experts or even experts in SNNs. In response, we summarize ten Q\&As of SNNs from many key aspects, including dense vs. sparse, unstructured sparse vs. structured sparse, pruning vs. sparse training, dense-to-sparse training vs. sparse-to-sparse training, static spars
    
[^167]: 从数据中学习控制导向的动态结构

    Learning Control-Oriented Dynamical Structure from Data. (arXiv:2302.02529v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2302.02529](http://arxiv.org/abs/2302.02529)

    本文提出从数据中学习控制导向的动态结构的方案，并成功应用于一些非线性动力学系统的稳定轨迹追踪，同时评估了所得到的闭环系统的鲁棒性、可解释性和可扩展性。

    

    即使对于已知的非线性动力学系统，反馈控制器的综合也是一个困难的问题，通常需要利用动力学的特定结构以诱导稳定的闭环系统。对于一般的非线性模型，包括那些适用于数据的模型，可能没有足够的已知结构来可靠地综合一个稳定的反馈控制器。本文讨论了一个基于状态依赖的非线性跟踪控制器制定方案，基于一种用于一般非线性控制仿射系统的状态依赖型瑞卡蒂方程。该公式依赖于定义控制仿射动力学的向量场系统的非线性因子分解，该分解总是在温和的光滑性假设下存在。我们提出了一种从有限的数据集中学习这种分解的方法。在各种模拟非线性动力学系统上，我们通过实证来证明所学习的控制器版本在稳定的轨迹跟踪方面的功效。在我们的学习方法旁边，我们评估了所得到的闭环系统的鲁棒性、可解释性和可扩展性。

    Even for known nonlinear dynamical systems, feedback controller synthesis is a difficult problem that often requires leveraging the particular structure of the dynamics to induce a stable closed-loop system. For general nonlinear models, including those fit to data, there may not be enough known structure to reliably synthesize a stabilizing feedback controller. In this paper, we discuss a state-dependent nonlinear tracking controller formulation based on a state-dependent Riccati equation for general nonlinear control-affine systems. This formulation depends on a nonlinear factorization of the system of vector fields defining the control-affine dynamics, which always exists under mild smoothness assumptions. We propose a method for learning this factorization from a finite set of data. On a variety of simulated nonlinear dynamical systems, we empirically demonstrate the efficacy of learned versions of this controller in stable trajectory tracking. Alongside our learning method, we eva
    
[^168]: 针对长尾识别的原型分类器学习

    Learning Prototype Classifiers for Long-Tailed Recognition. (arXiv:2302.00491v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.00491](http://arxiv.org/abs/2302.00491)

    本文介绍了针对长尾识别的原型分类器学习，通过联合学习原型以最小化基于概率分数与原型之间距离的平均交叉熵损失，并通过引入一种新的方法，自适应平衡来自不同类别的损失的重要性，进一步增强了原型分类器，从而实现在几个基准数据集上的竞争性性能。

    

    近年来，由于现实世界中物体的幂律分布，长尾识别(LTR)问题受到了关注。LTR中大多数最新的工作使用softmax分类器，其具有将分类器范数与给定类别的训练数据量相关联的倾向。另一方面，原型分类器不受这种缺点的困扰，并且只使用最近类平均值（NCM）即可交付有前途的结果，其中原型是经验质心。然而，在LTR中，原型分类器作为softmax的替代方法的潜力相对较少被探索。在这项工作中，我们提出了原型分类器，该分类器联合学习原型，以最小化基于概率分数与原型之间距离的平均交叉熵损失。我们从理论上分析了基于欧几里德距离的原型分类器的性质，这导致了稳定的基于梯度的优化，对异常值具有鲁棒性。我们通过引入一种新的方法，自适应平衡来自不同类别的损失的重要性，进一步增强了原型分类器。对几个基准数据集的实验证明，我们提出的原型分类器在LTR上实现了有竞争力的性能，在几种最新方法中表现出色。

    The problem of long-tailed recognition (LTR) has received attention in recent years due to the fundamental power-law distribution of objects in the real-world. Most recent works in LTR use softmax classifiers that have a tendency to correlate classifier norm with the amount of training data for a given class. On the other hand, Prototype classifiers do not suffer from this shortcoming and can deliver promising results simply using Nearest-Class-Mean (NCM), a special case where prototypes are empirical centroids. However, the potential of Prototype classifiers as an alternative to softmax in LTR is relatively underexplored. In this work, we propose Prototype classifiers, which jointly learn prototypes that minimize average cross-entropy loss based on probability scores from distances to prototypes. We theoretically analyze the properties of Euclidean distance based prototype classifiers that leads to stable gradient-based optimization which is robust to outliers. We further enhance Prot
    
[^169]: 赌徒凸优化的再探讨：FTRL实现了$\tilde{O}(t^{1/2})$遗憾。

    Bandit Convex Optimisation Revisited: FTRL Achieves $\tilde{O}(t^{1/2})$ Regret. (arXiv:2302.00358v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00358](http://arxiv.org/abs/2302.00358)

    研究发现，在标准FTRL算法中插入使用多个函数评估的内核估计器可以获得一个赌徒凸优化算法，并且能够在对抗性时变凸损失函数下实现$\tilde{O}(t^{1/2})$遗憾。

    

    我们展示了使用多个函数评估的内核估计器可以轻松转换为采样基于的赌徒估计器，其期望值等于原始内核估计值。将此类赌徒估计器插入标准FTRL算法中，可获得赌徒凸优化算法，其针对敌对的时变凸损失函数实现了$\tilde{O}(t^{1/2})$遗憾。

    We show that a kernel estimator using multiple function evaluations can be easily converted into a sampling-based bandit estimator with expectation equal to the original kernel estimate. Plugging such a bandit estimator into the standard FTRL algorithm yields a bandit convex optimisation algorithm that achieves $\tilde{O}(t^{1/2})$ regret against adversarial time-varying convex loss functions.
    
[^170]: 鉴定容易受到对抗性攻击的样本和强韧样本

    Identifying Adversarially Attackable and Robust Samples. (arXiv:2301.12896v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12896](http://arxiv.org/abs/2301.12896)

    本文提出了一种深度学习方法，用于检测哪些样本最容易受到对抗性攻击，从而确定哪些样本最不容易受到攻击。实验结果表明，这种检测器在不同的模型结构中具有较好的可移植性和检测性能。

    

    对抗性攻击将微小的，难以感知的扰动插入输入样本，导致深度学习模型的输出发生大量不期望的变化。虽然对抗性攻击的生成和防御已经得到广泛研究，但对从输入数据角度理解对抗性攻击的研究仍然很有限。本文引入了样本攻击性的概念，旨在确定最容易受到对抗性攻击的样本（攻击性样本），从而反过来确定最不容易受到攻击的样本（强韧样本）。我们提出了一种基于深度学习的方法，用于检测针对未知目标模型的未见数据集中，容易受到对抗性攻击和强韧性样本。标准图像分类数据集上的实验证实了深度攻击性检测器在不同体系结构中的可移植性。我们发现，与基于简单模型不确定性的措施相比，深度攻击性检测器表现更好。

    Adversarial attacks insert small, imperceptible perturbations to input samples that cause large, undesired changes to the output of deep learning models. Despite extensive research on generating adversarial attacks and building defense systems, there has been limited research on understanding adversarial attacks from an input-data perspective. This work introduces the notion of sample attackability, where we aim to identify samples that are most susceptible to adversarial attacks (attackable samples) and conversely also identify the least susceptible samples (robust samples). We propose a deep-learning-based method to detect the adversarially attackable and robust samples in an unseen dataset for an unseen target model. Experiments on standard image classification datasets enables us to assess the portability of the deep attackability detector across a range of architectures. We find that the deep attackability detector performs better than simple model uncertainty-based measures for i
    
[^171]: 关于异质性因果图中的异质性治疗效应

    On Heterogeneous Treatment Effects in Heterogeneous Causal Graphs. (arXiv:2301.12383v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2301.12383](http://arxiv.org/abs/2301.12383)

    本文通过推广因果图模型，描述了异质性因果图，提出了一种方法来研究不同调节因素对治疗效果和潜在中介变量的影响，解决了现实生活中高维度场景的挑战，并在真实数据应用中发现了新的异质性治疗效应。

    

    异质性和共病是许多医疗困境的两大挑战，阻碍了针对有效治疗的研究以及对潜在神经生物机制的理解。本文首先通过推广因果图模型与混杂变量交互和多个中介变量的概念，把异质性因果图（HCGs）进行了概括描述，这些与治疗交互作用的混杂变量被称为调节因素，这使我们能够灵活地产生HCGs，给出不同的调节因素，明确描述治疗或潜在中介变量对结果的HCEs。我们在线性和非线性模型中建立了HCE的理论形式，并在个体层面上推导了其特性。为了处理高维度场景，我们开发了一种交互式结构学习，通过将双重机器学习策略纳入估计调节因素中，来解决问题。我们通过模拟和对HIV阳性患者戒烟干预的真实数据应用示例来演示我们提出的方法和发现出先前未记录的新的HCE。该方法在各个医疗领域中为理解异质性治疗效应和改善个性化医学提供了潜在应用。

    Heterogeneity and comorbidity are two interwoven challenges associated with various healthcare problems that greatly hampered research on developing effective treatment and understanding of the underlying neurobiological mechanism. Very few studies have been conducted to investigate heterogeneous causal effects (HCEs) in graphical contexts due to the lack of statistical methods. To characterize this heterogeneity, we first conceptualize heterogeneous causal graphs (HCGs) by generalizing the causal graphical model with confounder-based interactions and multiple mediators. Such confounders with an interaction with the treatment are known as moderators. This allows us to flexibly produce HCGs given different moderators and explicitly characterize HCEs from the treatment or potential mediators on the outcome. We establish the theoretical forms of HCEs and derive their properties at the individual level in both linear and nonlinear models. An interactive structural learning is developed to 
    
[^172]: 探究面部注释在机器学习中的标注者偏见

    Investigating Labeler Bias in Face Annotation for Machine Learning. (arXiv:2301.09902v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09902](http://arxiv.org/abs/2301.09902)

    面部注释标注者的刻板印象和个人特征会影响其数据标注的公正性，强调需要对整个人工智能培训过程保持高度透明以尽早识别和纠正偏见。

    

    在一个越来越依赖人工智能的世界中，考虑人工智能对人类的伦理影响变得比以往任何时候都更加重要。一个尚未充分探讨的关键挑战是标注者偏见，这可能会为训练创建本质上带有偏见的数据集，并随后导致在医疗保健、就业、教育和执法等领域中出现不准确或不公平的决策。因此，我们进行了一项研究，使用来自不同种族和性别的人的图像进行标记任务，以调查和衡量标注者偏见的存在。我们的结果表明，参与者拥有影响其决策过程的刻板印象，并且标注者人口统计数据对所分配的注释标签产生影响。我们还讨论了标注者偏见如何影响数据集，随后影响所训练的模型。总体而言，在整个人工智能培训过程中必须保持高度透明，尽早识别和纠正数据中的偏见。

    In a world increasingly reliant on artificial intelligence, it is more important than ever to consider the ethical implications of artificial intelligence on humanity. One key under-explored challenge is labeler bias, which can create inherently biased datasets for training and subsequently lead to inaccurate or unfair decisions in healthcare, employment, education, and law enforcement. Hence, we conducted a study to investigate and measure the existence of labeler bias using images of people from different ethnicities and sexes in a labeling task. Our results show that participants possess stereotypes that influence their decision-making process and that labeler demographics impact assigned labels. We also discuss how labeler bias influences datasets and, subsequently, the models trained on them. Overall, a high degree of transparency must be maintained throughout the entire artificial intelligence training process to identify and correct biases in the data as early as possible.
    
[^173]: 点对点联邦学习中的后门攻击

    Backdoor Attacks in Peer-to-Peer Federated Learning. (arXiv:2301.09732v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09732](http://arxiv.org/abs/2301.09732)

    本文提出了一种基于点对点联邦学习（P2PFL）的新型后门攻击，利用结构图属性选择恶意节点，实现高攻击成功率，同时保持隐蔽性。同时还评估了这些攻击在多种现实条件下的鲁棒性，并设计了新的防御措施。

    

    大多数机器学习应用程序依赖于集中式学习过程，这开放了曝光其训练数据集的风险。尽管联邦学习（FL）在某种程度上缓解了这些隐私风险，但它仍依赖于可信的聚合服务器来训练共享全局模型。最近，基于点对点联邦学习（P2PFL）的新分布式学习架构在隐私和可靠性方面都提供了优势。然而，在训练期间对毒化攻击的鲁棒性尚未得到研究。在本文中，我们提出了一种新的P2PFL后门攻击，利用结构图属性选择恶意节点，实现高攻击成功率，同时保持隐蔽性。我们在各种实际条件下评估我们的攻击，包括多个图形拓扑、网络中有限的敌对能见度以及具有非独立同分布数据的客户端。最后，我们展示了从FL中适应的现有防御措施的局限性，并设计了一种新的防御措施。

    Most machine learning applications rely on centralized learning processes, opening up the risk of exposure of their training datasets. While federated learning (FL) mitigates to some extent these privacy risks, it relies on a trusted aggregation server for training a shared global model. Recently, new distributed learning architectures based on Peer-to-Peer Federated Learning (P2PFL) offer advantages in terms of both privacy and reliability. Still, their resilience to poisoning attacks during training has not been investigated. In this paper, we propose new backdoor attacks for P2PFL that leverage structural graph properties to select the malicious nodes, and achieve high attack success, while remaining stealthy. We evaluate our attacks under various realistic conditions, including multiple graph topologies, limited adversarial visibility of the network, and clients with non-IID data. Finally, we show the limitations of existing defenses adapted from FL and design a new defense that su
    
[^174]: 无模型访问的本地模型解释的不确定性量化

    Uncertainty Quantification for Local Model Explanations Without Model Access. (arXiv:2301.05761v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05761](http://arxiv.org/abs/2301.05761)

    本论文提出了一种无需访问模型本身，通过自举方法量化不确定性的算法，可用于生成后处理解释和不确定性区间，其具有广泛的应用前景和优越的性能。

    

    我们提出了一种与模型无关的算法，用于生成后处理解释和不确定性区间，当仅具有模型的静态输入和输出样本时，而不是直接访问模型本身。这种情况可能会在模型评估昂贵、强制实行隐私、安全和带宽限制、或需要实时，设备上的实现时出现。我们的算法使用自举方法来量化不确定性，这种不确定性不可避免地出现在利用有限模型询问的样本生成解释时。通过模拟研究，我们展示了我们算法生成的不确定性区间与经典回归分析的简单置信区间以及当前贝叶斯方法相比，具有有利的区间宽度和覆盖概率的权衡。此外，我们通过应用本方法进一步展示了我们方法的能力。

    We present a model-agnostic algorithm for generating post-hoc explanations and uncertainty intervals for a machine learning model when only a static sample of inputs and outputs from the model is available, rather than direct access to the model itself. This situation may arise when model evaluations are expensive; when privacy, security and bandwidth constraints are imposed; or when there is a need for real-time, on-device explanations. Our algorithm uses a bootstrapping approach to quantify the uncertainty that inevitably arises when generating explanations from a finite sample of model queries. Through a simulation study, we show that the uncertainty intervals generated by our algorithm exhibit a favorable trade-off between interval width and coverage probability compared to the naive confidence intervals from classical regression analysis as well as current Bayesian approaches for quantifying explanation uncertainty. We further demonstrate the capabilities of our method by applying
    
[^175]: 感知和预测：基于自监督语音表示的语音增强损失函数

    Perceive and predict: self-supervised speech representation based loss functions for speech enhancement. (arXiv:2301.04388v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2301.04388](http://arxiv.org/abs/2301.04388)

    本文证明了干净和带噪声语音的特征编码之间的距离对语音质量和可懂度的评估有显著作用，使用此距离作为损失函数在语音增强方面效果优于传统方法。

    

    近期有关语音增强的研究探讨了利用自监督语音表示来辅助神经语音增强模型的训练。然而，很多相关研究侧重于使用自监督语音表示模型的最深或最终输出，而不是较早的特征编码。这种方式使用自监督表示经常缺乏充分的动机。本文证明了干净和带噪声语音的特征编码之间的距离与心理声学衡量标准以及人类平均意见得分显著相关。实验使用此距离作为损失函数，并使用感知语音质量评估(PESQ)和短时语音客观质量评估(STOI)等客观度量证明了其优于基于STFT频谱距离和其他常见语音增强文献中的损失函数的性能。

    Recent work in the domain of speech enhancement has explored the use of self-supervised speech representations to aid in the training of neural speech enhancement models. However, much of this work focuses on using the deepest or final outputs of self supervised speech representation models, rather than the earlier feature encodings. The use of self supervised representations in such a way is often not fully motivated. In this work it is shown that the distance between the feature encodings of clean and noisy speech correlate strongly with psychoacoustically motivated measures of speech quality and intelligibility, as well as with human Mean Opinion Score (MOS) ratings. Experiments using this distance as a loss function are performed and improved performance over the use of STFT spectrogram distance based loss as well as other common loss functions from speech enhancement literature is demonstrated using objective measures such as perceptual evaluation of speech quality (PESQ) and shor
    
[^176]: 深度统计求解器用于配电系统状态估计

    Deep Statistical Solver for Distribution System State Estimation. (arXiv:2301.01835v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01835](http://arxiv.org/abs/2301.01835)

    该论文提出了基于图神经网络的深度统计求解器（DSS$^2$），应用于配电系统状态估计。DSS$^2$利用超图和节点消息传递方案更新潜在表示，并通过弱监督学习方法进行训练。实验结果证明了DSS$^2$在实际数据集上优于其他方法。

    

    实现准确的配电系统状态估计（DSSE）面临着许多挑战，其中包括可观测性不足和配电系统密度高。虽然基于机器学习模型的数据驱动替代方案可能是一种选择，但由于缺乏标注数据，它们在DSSE中受到影响。实际上，配电系统中的测量往往是嘈杂、损坏和不可用的。为解决这些问题，我们提出了基于图神经网络（GNN）的深度学习模型——用于配电系统状态估计的深度统计求解器（DSS$^2$），它考虑到了配电系统的网络结构和物理控制功率流方程。DSS$^2$利用超图来表示配电系统的异构组件，并通过以节点为中心的消息传递方案更新其潜在表示。提出了一种弱监督学习方法，在不需要标记数据的情况下以学习优化的方式训练DSS$^2$。实验结果表明，DSS$^2$在实际数据集上提高了估计精度，优于现有方法。

    Implementing accurate Distribution System State Estimation (DSSE) faces several challenges, among which the lack of observability and the high density of the distribution system. While data-driven alternatives based on Machine Learning models could be a choice, they suffer in DSSE because of the lack of labeled data. In fact, measurements in the distribution system are often noisy, corrupted, and unavailable. To address these issues, we propose the Deep Statistical Solver for Distribution System State Estimation (DSS$^2$), a deep learning model based on graph neural networks (GNNs) that accounts for the network structure of the distribution system and for the physical governing power flow equations. DSS$^2$ leverages hypergraphs to represent the heterogeneous components of the distribution systems and updates their latent representations via a node-centric message-passing scheme. A weakly supervised learning approach is put forth to train the DSS$^2$ in a learning-to-optimize fashion w
    
[^177]: DMOps：数据管理操作和配方

    DMOps: Data Management Operation and Recipes. (arXiv:2301.01228v3 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2301.01228](http://arxiv.org/abs/2301.01228)

    DMOps旨在指导工业界优化构建NLP产品数据集，提供基线来简化数据操作。

    

    数据为中心的AI揭示了机器学习（ML）流程中数据的重要性。 学术界、工业界和政府部门已提出各种NLP数据研究计划。 虽然利用现有数据的能力至关重要，但在工业界，构建数据集的能力比以往任何时候都更为关键。鉴于这一趋势，我们提出了“数据管理操作和配方”，旨在指导工业界优化NLP产品数据集的构建。本文介绍了DMOps的概念，它来自于处理实际NLP数据管理的经验，并旨在通过提供基线来简化数据操作。

    Data-centric AI has shed light on the significance of data within the machine learning (ML) pipeline. Recognizing its significance, academia, industry, and government departments have suggested various NLP data research initiatives. While the ability to utilize existing data is essential, the ability to build a dataset has become more critical than ever, especially in the industry. In consideration of this trend, we propose a "Data Management Operations and Recipes" to guide the industry in optimizing the building of datasets for NLP products. This paper presents the concept of DMOps which is derived from real-world experiences with NLP data management and aims to streamline data operations by offering a baseline.
    
[^178]: 基于分离节点识别的 NISQ-ready 社区检测

    NISQ-ready community detection based on separation-node identification. (arXiv:2212.14717v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2212.14717](http://arxiv.org/abs/2212.14717)

    本文提出了一种基于 QUBO 的新方法，只需要与节点数量相同的量子比特，并以与输入图的邻接矩阵一样稀疏的 QUBO 矩阵表示，通过分离节点的概念实现了 QUBO 矩阵的显著改进，该方法只需要识别分离节点即可高效识别社区。

    

    网络结构分析在许多科学领域中都是必不可少的，从生物学到社会学。将这些网络聚类成分区的计算任务，即解决社区检测问题，通常是 NP-hard 的，因此启发式解决方案是不可或缺的。在新兴的量子计算技术中，对有效启发式的探索已经导致了特别有前途的方法的发展。由于所有已建立的量子社区检测方法都对硬件存在重大要求，我们引入了一种新的基于 QUBO 的方法，仅需要与节点数量相同的量子比特，并以与输入图的邻接矩阵一样稀疏的 QUBO 矩阵表示。通过分离节点的新概念，实现了 QUBO 矩阵的显著改进，这在相关工作中通常非常密集。这种方法不是直接将每个节点分配到一个社区中，而是依赖于分离节点的识别，从而能够高效地识别社区。我们在基准图上展示了我们方法的有效性，并在状态-of-the-art 的经典和量子社区检测算法中达到了竞争力的结果。

    The analysis of network structure is essential to many scientific areas, ranging from biology to sociology. As the computational task of clustering these networks into partitions, i.e., solving the community detection problem, is generally NP-hard, heuristic solutions are indispensable. The exploration of expedient heuristics has led to the development of particularly promising approaches in the emerging technology of quantum computing. Motivated by the substantial hardware demands for all established quantum community detection approaches, we introduce a novel QUBO based approach that only needs number-of-nodes many qubits and is represented by a QUBO-matrix as sparse as the input graph's adjacency matrix. The substantial improvement on the sparsity of the QUBO-matrix, which is typically very dense in related work, is achieved through the novel concept of separation-nodes. Instead of assigning every node to a community directly, this approach relies on the identification of a separati
    
[^179]: 移动设备上的实时神经光场渲染

    Real-Time Neural Light Field on Mobile Devices. (arXiv:2212.08057v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.08057](http://arxiv.org/abs/2212.08057)

    本文提出了一种在移动设备上实时运行的高效网络，用于神经渲染。

    

    近期，神经渲染场（NeRF）在利用隐式神经表示法来表示3D场景，并实现新视角合成方面取得了令人印象深刻的成果。由于体积渲染的过程，NeRF的推理速度非常缓慢，限制了在移动设备等资源受限的硬件上利用NeRF的应用场景。有许多工作致力于减少运行NeRF模型的延迟。但是，他们大多仍需要高端GPU进行加速或额外的存储内存，这在移动设备上都不可用。另一个新出现的方向则利用神经光场（NeLF）来进行加速，因为一条射线上只需进行一次向前传递即可预测像素颜色。然而，为了达到与NeRF类似的渲染质量，NeLF中的网络设计需要大量的计算，这对移动设备并不友好。在本文中，我们提出了一种在移动设备上实时运行的高效网络，用于神经渲染。

    Recent efforts in Neural Rendering Fields (NeRF) have shown impressive results on novel view synthesis by utilizing implicit neural representation to represent 3D scenes. Due to the process of volumetric rendering, the inference speed for NeRF is extremely slow, limiting the application scenarios of utilizing NeRF on resource-constrained hardware, such as mobile devices. Many works have been conducted to reduce the latency of running NeRF models. However, most of them still require high-end GPU for acceleration or extra storage memory, which is all unavailable on mobile devices. Another emerging direction utilizes the neural light field (NeLF) for speedup, as only one forward pass is performed on a ray to predict the pixel color. Nevertheless, to reach a similar rendering quality as NeRF, the network in NeLF is designed with intensive computation, which is not mobile-friendly. In this work, we propose an efficient network that runs in real-time on mobile devices for neural rendering. W
    
[^180]: 基于深度卷积-关注模型的12导联心电图左束支传导阻滞诊断

    Deep conv-attention model for diagnosing left bundle branch block from 12-lead electrocardiograms. (arXiv:2212.04936v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2212.04936](http://arxiv.org/abs/2212.04936)

    本研究提出了一种使用深度学习模型的注意力机制，能够从12导联ECG数据中准确检测出LBBB心律不齐，准确率高达99.8%，优于其他最先进的模型。

    

    心脏重新同步治疗（CRT）是一种用于补偿心跳不规则的治疗方法。研究表明，这种治疗在患有左束支传导阻滞（LBBB）心律不齐的心脏患者中更有效。因此，确定该心律不齐是确定是否使用CRT的重要初始步骤。然而，传统的心电图（ECG）检测LBBB的方法通常存在误差。因此，需要一种准确的方法从ECG数据中诊断这种心律不齐。本研究提出了一种深度学习模型，用于从12导联ECG数据检测LBBB心律不齐。该模型由1D稀疏卷积层构成，同时采用注意力机制来识别重要输入数据。该模型在PTB诊断ECG数据库上进行了训练和验证，实现了高达99.8%的准确率，优于其他最先进的模型。

    Cardiac resynchronization therapy (CRT) is a treatment that is used to compensate for irregularities in the heartbeat. Studies have shown that this treatment is more effective in heart patients with left bundle branch block (LBBB) arrhythmia. Therefore, identifying this arrhythmia is an important initial step in determining whether or not to use CRT. On the other hand, traditional methods for detecting LBBB on electrocardiograms (ECG) are often associated with errors. Thus, there is a need for an accurate method to diagnose this arrhythmia from ECG data. Machine learning, as a new field of study, has helped to increase human systems' performance. Deep learning, as a newer subfield of machine learning, has more power to analyze data and increase systems accuracy. This study presents a deep learning model for the detection of LBBB arrhythmia from 12-lead ECG data. This model consists of 1D dilated convolutional layers. Attention mechanism has also been used to identify important input da
    
[^181]: 可变化决策频率的选项评论者

    Variable Decision-Frequency Option Critic. (arXiv:2212.04407v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04407](http://arxiv.org/abs/2212.04407)

    这篇论文提出了一个名为CTCO的框架，其中代理选择选项作为可变持续时间的子策略。这个框架可以以任何所需频率与系统交互，从而提供平滑的动作变化，相比传统RL和时间抽象RL方法，其性能更好。

    

    在传统的强化学习算法中，代理在离散和固定的时间间隔内做出决策。决策之间的持续时间变成了一个关键的超参数，因为设置得太短可能会增加问题的难度，需要代理进行多次决策才能实现其目标，而设置得太长会导致代理失去对系统的控制。然而，物理系统不一定需要恒定的控制频率，对于学习代理来说，一般情况下，当需要时以高频率运行，而在可能时以低频率运行更好。我们提出了一个名为连续时间连续选项 (CTCO) 的框架，其中代理选择选项作为可变持续时间的子策略。这些选项是时间连续的，可以以任何所需频率与系统交互，从而提供平滑的动作变化。我们通过将其性能与传统 RL 和时间抽象 RL 方法进行比较，展示了 CTCO 的有效性。

    In classic reinforcement learning algorithms, agents make decisions at discrete and fixed time intervals. The duration between decisions becomes a crucial hyperparameter, as setting it too short may increase the difficulty of the problem by requiring the agent to make numerous decisions to achieve its goal, while setting it too long can result in the agent losing control over the system. However, physical systems do not necessarily require a constant control frequency, and for learning agents, it is often preferable to operate with a low frequency when possible and a high frequency when necessary. We propose a framework called Continuous-Time Continuous-Options (CTCO), where the agent chooses options as sub-policies of variable durations. These options are time-continuous and can interact with the system at any desired frequency providing a smooth change of actions. We demonstrate the effectiveness of CTCO by comparing its performance to classical RL and temporal-abstraction RL methods
    
[^182]: 带有神经网络和索引的聚类模型

    Clustering with Neural Network and Index. (arXiv:2212.03853v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03853](http://arxiv.org/abs/2212.03853)

    介绍了一种新的带有神经网络和索引的聚类模型CNNI，该模型使用神经网络对数据点进行聚类，实现了第一个能够处理非凸形状数据的参数化聚类模型。

    

    介绍一种新的聚类模型，称为带有神经网络和索引的聚类模型（CNNI）。CNNI使用神经网络对数据点进行聚类。神经网络的训练模仿监督学习，使用内部聚类评估指标作为损失函数。进行了一项实验来测试新模型的可行性，并与K均值和高斯混合模型（GMM）等其他聚类模型的结果进行了比较。结果表明CNNI可以正确地对数据进行聚类；CNNI配备了MMJ-SC，成为第一个能够处理非凸形状（非平面几何）数据的参数化（归纳式）聚类模型。

    A new model called Clustering with Neural Network and Index (CNNI) is introduced. CNNI uses a Neural Network to cluster data points. Training of the Neural Network mimics supervised learning, with an internal clustering evaluation index acting as the loss function. An experiment is conducted to test the feasibility of the new model, and compared with results of other clustering models like K-means and Gaussian Mixture Model (GMM). The result shows CNNI can work properly for clustering data; CNNI equipped with MMJ-SC, achieves the first parametric (inductive) clustering model that can deal with non-convex shaped (non-flat geometry) data.
    
[^183]: 关于网络规模下的大规模多重检验：一种渐进方法

    On Large-Scale Multiple Testing Over Networks: An Asymptotic Approach. (arXiv:2211.16059v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2211.16059](http://arxiv.org/abs/2211.16059)

    本文提出了两种针对分布式环境的多重检验方法：比例匹配和贪婪聚合。贪心聚合方法有效地近似了每个节点的最优拒绝区域，且具有计算效率性质。

    

    本文致力于开发计算和通信效率高的多重检验网络方法，对许多实际应用具有重要意义。我们采用一种渐进的方法，并提出了两种针对分布式环境的方法：比例匹配和贪婪聚合。比例匹配方法实现了全局 BH（Benjamini–Hochberg）性能，仅需要通信一次（估计）真零假设比例以及每个节点的 p 值数量。通过关注渐进最优功率，我们超越了 BH 过程，并提供了渐近最优解的显式特征。这导致了贪心聚合方法，该方法有效地近似了每个节点的最优拒绝区域，而计算效率自然来自贪心类型方法。此外，对于这两种方法，我们提供了 FDR 和功率的收敛速度。在各种情况下进行的广泛数值结果表明了方法的实用性。

    This work concerns developing communication- and computation-efficient methods for large-scale multiple testing over networks, which is of interest to many practical applications. We take an asymptotic approach and propose two methods, proportion-matching and greedy aggregation, tailored to distributed settings. The proportion-matching method achieves the global BH performance yet only requires a one-shot communication of the (estimated) proportion of true null hypotheses as well as the number of p-values at each node. By focusing on the asymptotic optimal power, we go beyond the BH procedure by providing an explicit characterization of the asymptotic optimal solution. This leads to the greedy aggregation method that effectively approximates the optimal rejection regions at each node, while computation efficiency comes from the greedy-type approach naturally. Moreover, for both methods, we provide the rate of convergence for both the FDR and power. Extensive numerical results over a va
    
[^184]: 逆可解性和安全性及其在联邦学习中的应用

    Inverse Solvability and Security with Applications to Federated Learning. (arXiv:2211.14115v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.14115](http://arxiv.org/abs/2211.14115)

    介绍了逆可解性和安全性的概念，以及其在联邦学习中的应用。论文提供了模型示例，展示了如何通过增加用户数量来增加可解性和安全性。

    

    我们介绍了逆可解性和安全性的概念，适用于一般线性前向模型，并展示了如何将其应用于联邦学习中使用的模型。我们提供了这样的模型的示例，其逆可解性和安全性在本文中得到定义。我们还展示了如何利用参与给定迭代的大量用户来增加可解性和安全性。最后，我们讨论了所提出概念的可能扩展，包括非线性情况。

    We introduce the concepts of inverse solvability and security for a generic linear forward model and demonstrate how they can be applied to models used in federated learning. We provide examples of such models which differ in the resulting inverse solvability and security as defined in this paper. We also show how the large number of users participating in a given iteration of federated learning can be leveraged to increase both solvability and security. Finally, we discuss possible extensions of the presented concepts including the nonlinear case.
    
[^185]: 基于测地线和水平面投影的双曲切片Wasserstein

    Hyperbolic Sliced-Wasserstein via Geodesic and Horospherical Projections. (arXiv:2211.10066v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10066](http://arxiv.org/abs/2211.10066)

    本文提出了一种基于测地线和水平面投影的双曲切片Wasserstein距离，可用于比较具有基础分层结构的数据中定义的概率分布。

    

    研究表明，对于许多具有基础分层结构的数据，将其嵌入双曲空间是有益的。因此，许多机器学习工具被扩展到这些空间，但只有很少对于这些空间中定义的概率分布进行比较的不一致性存在。在双曲空间上，最优输运距离在这样的黎曼流形上是明确定义的，并具有强大的理论性质，但计算成本较高。在欧几里得空间上，切片Wasserstein距离是更具计算效率的方法，它利用一维Wasserstein距离的闭合形式，但在双曲空间上不易获得。在本文中，我们提出了新的双曲切片Wasserstein不一致性构造。这些构造使用基本测地线上的水平面或测地线的投影。我们在不同的任务中研究和比较它们，其中双曲表示是相关的。

    It has been shown beneficial for many types of data which present an underlying hierarchical structure to be embedded in hyperbolic spaces. Consequently, many tools of machine learning were extended to such spaces, but only few discrepancies to compare probability distributions defined over those spaces exist. Among the possible candidates, optimal transport distances are well defined on such Riemannian manifolds and enjoy strong theoretical properties, but suffer from high computational cost. On Euclidean spaces, sliced-Wasserstein distances, which leverage a closed-form of the Wasserstein distance in one dimension, are more computationally efficient, but are not readily available on hyperbolic spaces. In this work, we propose to derive novel hyperbolic sliced-Wasserstein discrepancies. These constructions use projections on the underlying geodesics either along horospheres or geodesics. We study and compare them on different tasks where hyperbolic representations are relevant, such a
    
[^186]: 一种混合类别相关核的高斯过程

    A mixed-categorical correlation kernel for Gaussian process. (arXiv:2211.08262v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2211.08262](http://arxiv.org/abs/2211.08262)

    提出一种新的混合类别相关核的高斯过程代理，相较于其他现有模型在分析和工程问题上表现更好。

    

    近年来，基于高斯过程代理的混合类别元模型引起了越来越多的关注。在这种情况下，一些现有的方法使用不同的策略，通过使用连续核（例如，连续松弛和Gower距离基于高斯过程）或通过直接估计相关矩阵。在本文中，我们提出了一种基于核的方法，将连续指数核扩展为处理混合类别变量。所提出的核引导到了一个新的高斯代理，它概括了连续松弛和Gower距离基于高斯过程模型。我们在分析和工程问题上证明了，我们的提出的高斯过程模型比其他基于核的现有模型具有更高的可能性和更小的残差误差。我们的方法可使用开源软件SMT。

    Recently, there has been a growing interest for mixed-categorical meta-models based on Gaussian process (GP) surrogates. In this setting, several existing approaches use different strategies either by using continuous kernels (e.g., continuous relaxation and Gower distance based GP) or by using a direct estimation of the correlation matrix. In this paper, we present a kernel-based approach that extends continuous exponential kernels to handle mixed-categorical variables. The proposed kernel leads to a new GP surrogate that generalizes both the continuous relaxation and the Gower distance based GP models. We demonstrate, on both analytical and engineering problems, that our proposed GP model gives a higher likelihood and a smaller residual error than the other kernel-based state-of-the-art models. Our method is available in the open-source software SMT.
    
[^187]: CarbonTag: 一种基于浏览器的近似计算在线广告能耗方法

    CarbonTag: A Browser-Based Method for Approximating Energy Consumption of Online Ads. (arXiv:2211.00071v3 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2211.00071](http://arxiv.org/abs/2211.00071)

    本研究提出了一种基于浏览器的方法用于计算和减少在线广告的能源消耗，并介绍了不同级别的计算方法和一种内联方法。

    

    能源是当前最严重的环境挑战。碳排放对气候变化的贡献在很大程度上受到能源的生产和消费的影响。测量和减少服务的能源消耗是减少碳排放对环境产生不利影响的关键步骤。本研究调查了在线广告在渲染过程中的能源消耗，并提出了一种预测能源消耗的方法。据我们所知，这是第一项计算单个广告渲染过程中能源消耗的研究。本研究进一步介绍了不同级别的计算方法，以及一种基于 JavaScript 的内联方法，用于在页面上附加 CarbonTag，以计算和减少在线广告对能源消耗的贡献。

    Energy is today the most critical environmental challenge. The amount of carbon emissions contributing to climate change is significantly influenced by both the production and consumption of energy. Measuring and reducing the energy consumption of services is a crucial step toward reducing adverse environmental effects caused by carbon emissions. Millions of websites rely on online advertisements to generate revenue, with most websites earning most or all of their revenues from ads. As a result, hundreds of billions of online ads are delivered daily to internet users to be rendered in their browsers. Both the delivery and rendering of each ad consume energy. This study investigates how much energy online ads use in the rendering process and offers a way for predicting it as part of rendering the ad. To the best of the authors' knowledge, this is the first study to calculate the energy usage of single advertisements in the rendering process. Our research further introduces different lev
    
[^188]: GFlowOut：使用生成流网络的Dropout

    GFlowOut: Dropout with Generative Flow Networks. (arXiv:2210.12928v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12928](http://arxiv.org/abs/2210.12928)

    GFlowOut是一种使用生成流网络的Dropout方法，可以更好地估计复杂的后验分布和样本相关性，并在几个基准数据集上实现了最先进的性能和良好的校准不确定性估计。

    

    贝叶斯推理为解决现代神经网络的许多关键问题（例如不良校准和泛化，以及数据效率）提供了原则性的工具。然而，将贝叶斯推理扩展到大型架构是具有挑战性的并且需要严格的近似。Monte Carlo Dropout已被广泛用作近似推理的相对便宜的方法，并使用深度神经网络估计不确定性。传统上，dropout掩码是从固定分布中独立采样的。最近的研究表明，dropout掩码可以被视为潜在变量，并且可以使用变分推理进行推断。这些方法面临两个重要的挑战：（a）掩码的后验分布可能高度多模态，很难用标准变分推理进行近似；（b）充分利用dropout掩码之间的样本相关信息和相关性以改善后验估计并不是微不足道的。在本研究中，我们提出了GFlowOut，一种使用生成流网络模拟dropout掩码分布的新方法。我们的方法提供了一种灵活且可处理的方法来模拟复杂的掩码后验分布，并可以更好地捕获掩码之间的样本相关性。我们展示了GFlowOut在几个基准数据集上实现了最先进的性能，并提供了良好校准的不确定性估计。

    Bayesian Inference offers principled tools to tackle many critical problems with modern neural networks such as poor calibration and generalization, and data inefficiency. However, scaling Bayesian inference to large architectures is challenging and requires restrictive approximations. Monte Carlo Dropout has been widely used as a relatively cheap way for approximate Inference and to estimate uncertainty with deep neural networks. Traditionally, the dropout mask is sampled independently from a fixed distribution. Recent works show that the dropout mask can be viewed as a latent variable, which can be inferred with variational inference. These methods face two important challenges: (a) the posterior distribution over masks can be highly multi-modal which can be difficult to approximate with standard variational inference and (b) it is not trivial to fully utilize sample-dependent information and correlation among dropout masks to improve posterior estimation. In this work, we propose GF
    
[^189]: 基于拓扑排序的因果发现扩散模型

    Diffusion Models for Causal Discovery via Topological Ordering. (arXiv:2210.06201v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06201](http://arxiv.org/abs/2210.06201)

    本文提出基于扩散模型的DiffAN拓扑排序算法，用于解决因果发现中的搜索空间优化问题。

    

    在考虑到将功能关系约束为非线性带有加性噪声（ANM）的情况下，从观测数据中发现因果关系成为可能。即使带有强大的假设，因果发现也涉及到在有向无环图（DAGs）空间中进行昂贵的搜索问题。拓扑排序方法通过在排列空间中搜索而不是图形空间中搜索，从而减少了因果发现优化空间。对于ANMs，可以使用数据对数似然的Hessian来找到因果图中的叶节点，从而允许它的拓扑排序。然而，现有的用于获取Hessian的计算方法仍然无法扩展为变量数量和样本数量增加的情况。因此，受到扩散概率模型（DPMs）最近创新的启发，我们提出了一种名为DiffAN的拓扑排序算法。

    Discovering causal relations from observational data becomes possible with additional assumptions such as considering the functional relations to be constrained as nonlinear with additive noise (ANM). Even with strong assumptions, causal discovery involves an expensive search problem over the space of directed acyclic graphs (DAGs). \emph{Topological ordering} approaches reduce the optimisation space of causal discovery by searching over a permutation rather than graph space. For ANMs, the \emph{Hessian} of the data log-likelihood can be used for finding leaf nodes in a causal graph, allowing its topological ordering. However, existing computational methods for obtaining the Hessian still do not scale as the number of variables and the number of samples increase. Therefore, inspired by recent innovations in diffusion probabilistic models (DPMs), we propose \emph{DiffAN}\footnote{Implementation is available at \url{https://github.com/vios-s/DiffAN} .}, a topological ordering algorithm t
    
[^190]: 使用对比剪枝权重训练去偏置子网络

    Training Debiased Subnetworks with Contrastive Weight Pruning. (arXiv:2210.05247v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05247](http://arxiv.org/abs/2210.05247)

    本文探讨了在存在强假相关的偏置网络中提取最优无偏子网络的问题，并提出了使用对比剪枝权重训练实现去偏置子网络的算法 DCWP，在多个应用中都有良好的效果。

    

    神经网络通常存在偏置性，导致提供具有误导性的统计证据，不能很好地推广。因此，提出了在偏置网络中提取最优无偏功能子网络的问题。本文首先提出了现有算法在探索具有强假相关性的无偏子网络存在限制的理论洞见，然后进一步阐明了偏差冲突样本对结构学习的重要性，并基于学习的（伪）无偏样本和选择性偏差冲突样本，提出了去偏置对比剪枝（DCWP）算法。在图像分类、语言模型和强化学习等各种应用中验证了 DCWP 的有效性。

    Neural networks are often biased to spuriously correlated features that provide misleading statistical evidence that does not generalize. This raises an interesting question: ``Does an optimal unbiased functional subnetwork exist in a severely biased network? If so, how to extract such subnetwork?" While empirical evidence has been accumulated about the existence of such unbiased subnetworks, these observations are mainly based on the guidance of ground-truth unbiased samples. Thus, it is unexplored how to discover the optimal subnetworks with biased training datasets in practice. To address this, here we first present our theoretical insight that alerts potential limitations of existing algorithms in exploring unbiased subnetworks in the presence of strong spurious correlations. We then further elucidate the importance of bias-conflicting samples on structure learning. Motivated by these observations, we propose a Debiased Contrastive Weight Pruning (DCWP) algorithm, which probes unbi
    
[^191]: 迈向面向OOD的对抗鲁棒性

    Towards Out-of-Distribution Adversarial Robustness. (arXiv:2210.03150v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03150](http://arxiv.org/abs/2210.03150)

    该论文介绍了一种面向OOD的对抗鲁棒性方法，通过将每个攻击类型视为一个领域，应用风险外推方法实现对各攻击的相似鲁棒性水平，实现了在训练和测试时的更高性能，是对抗鲁棒性研究中的创新贡献。

    

    对抗鲁棒性仍然是深度学习的一个主要挑战。一个核心问题是对一种攻击的鲁棒性往往不能转移到其他攻击。我们展示了通过采用领域泛化方法，可以在许多常用攻击中改善鲁棒性。具体来说，我们将每种攻击视为一个领域，并应用风险外推方法（REx），促进对所有训练攻击的相似鲁棒水平。与现有方法相比，在训练期间看到的攻击上，我们获得类似或更高级别的最坏情况下的对抗鲁棒性。此外，在家族或测试时只遇到的攻击的调整中，我们实现了更高的性能。在攻击集合上，我们的方法将MNIST的最佳现有基线的准确性从3.4%提高到25.9％，在CIFAR10上从16.9％提高到23.5％。

    Adversarial robustness continues to be a major challenge for deep learning. A core issue is that robustness to one type of attack often fails to transfer to other attacks. While prior work establishes a theoretical trade-off in robustness against different $L_p$ norms, we show that there is potential for improvement against many commonly used attacks by adopting a domain generalisation approach. Concretely, we treat each type of attack as a domain, and apply the Risk Extrapolation method (REx), which promotes similar levels of robustness against all training attacks. Compared to existing methods, we obtain similar or superior worst-case adversarial robustness on attacks seen during training. Moreover, we achieve superior performance on families or tunings of attacks only encountered at test time. On ensembles of attacks, our approach improves the accuracy from 3.4% with the best existing baseline to 25.9% on MNIST, and from 16.9% to 23.5% on CIFAR10.
    
[^192]: PathProx: 一种用于权值衰减正则化深度神经网络的近端梯度算法

    PathProx: A Proximal Gradient Algorithm for Weight Decay Regularized Deep Neural Networks. (arXiv:2210.03069v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03069](http://arxiv.org/abs/2210.03069)

    本文提出一种用于权值衰减正则化深度神经网络的近端梯度算法 PathProx，它可以更快地收敛到标准权值衰减训练所共享的稀疏解。

    

    权值衰减是深度学习中最广泛使用的正则化方法之一，已被证明可以提高泛化能力和鲁棒性。驱动权值衰减的优化目标是损失之和加上与权值平方和成比例的项。本文认为，随机梯度下降（SGD）可能是这个目标的一种低效算法。对于带有ReLU激活函数的神经网络，权重衰减目标的解与另一个目标的解是等价的，其中正则化项改为与每个ReLU神经元关联的输入和输出权重的$\ell_2$（不是平方）范数乘积之和。这种替代（并且有效等价）的正则化方法提出了一种用于网络训练的新近端梯度算法。理论和实验证实了这种新的训练方法，显示它可以更快地收敛到标准权值衰减训练所共享的稀疏解。

    Weight decay is one of the most widely used forms of regularization in deep learning, and has been shown to improve generalization and robustness. The optimization objective driving weight decay is a sum of losses plus a term proportional to the sum of squared weights. This paper argues that stochastic gradient descent (SGD) may be an inefficient algorithm for this objective. For neural networks with ReLU activations, solutions to the weight decay objective are equivalent to those of a different objective in which the regularization term is instead a sum of products of $\ell_2$ (not squared) norms of the input and output weights associated with each ReLU neuron. This alternative (and effectively equivalent) regularization suggests a novel proximal gradient algorithm for network training. Theory and experiments support the new training approach, showing that it can converge much faster to the sparse solutions it shares with standard weight decay training.
    
[^193]: 通过排名评估预训练自监督表示的下游性能：RankMe

    RankMe: Assessing the downstream performance of pretrained self-supervised representations by their rank. (arXiv:2210.02885v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02885](http://arxiv.org/abs/2210.02885)

    本文提出了一个简单的无监督准则 RankMe，通过评估有效排名，可以指示学习JE-SSL表示的质量，而无需任何标签。

    

    无监督自监督学习（JE-SSL）的快速发展，使得出现了许多方法变化，但只有少数原则性指导方针，能够帮助从业人员成功地部署它们。本文开发了一个简单的无监督准则，即效果排名，可以指示学习JE-SSL表示的质量。这种方法简单而且计算友好，甚至可以在不需要任何标签的情况下评估JE-SSL表示的性能，即使在不同的下游数据集上也是如此。

    Joint-Embedding Self Supervised Learning (JE-SSL) has seen a rapid development, with the emergence of many method variations but only few principled guidelines that would help practitioners to successfully deploy them. The main reason for that pitfall comes from JE-SSL's core principle of not employing any input reconstruction therefore lacking visual cues of unsuccessful training. Adding non informative loss values to that, it becomes difficult to deploy SSL on a new dataset for which no labels can help to judge the quality of the learned representation. In this study, we develop a simple unsupervised criterion that is indicative of the quality of the learned JE-SSL representations: their effective rank. Albeit simple and computationally friendly, this method -- coined RankMe -- allows one to assess the performance of JE-SSL representations, even on different downstream datasets, without requiring any labels. A further benefit of RankMe is that it does not have any training or hyper-p
    
[^194]: LL-GNN: 基于FPGA的低延迟图神经网络在高能物理领域的应用

    LL-GNN: Low Latency Graph Neural Networks on FPGAs for High Energy Physics. (arXiv:2209.14065v4 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2209.14065](http://arxiv.org/abs/2209.14065)

    本文提出了一种新型的基于FPGA的低延迟图神经网络(LL-GNN)架构，针对粒子探测器领域的特殊需求，通过外积矩阵乘法方法、结构化邻接矩阵和列主数据布局等优化措施，实现了亚微秒级别的网络部署，并提供了一种GNN特定的算法-硬件协同设计方法。

    

    本文提出了一种新型的可重构架构，用于实现基于FPGA的低延迟图神经网络(LL-GNN)，以支持具有前所未有的低延迟性能的粒子探测器。由于在欧洲核子研究中心大型强子对撞机实验的一级触发器中需要以每秒数百太字节的数据速率进行在线事件选择，因此将基于FPGA的GNNs应用于粒子探测器面临着独特的挑战，需要在亚微秒级别内部署网络。本文提出了一种基于外积的矩阵乘法方法，并利用结构化邻接矩阵和列主数据布局进行了优化。此外，还引入了一种融合步骤，通过消除不必要的边界进一步降低了端到端设计的延迟。此外，本文还提出了一种GNN特定的算法-硬件协同设计方法，在给定延迟限制下寻找更好延迟和更高精度的设计。

    This work presents a novel reconfigurable architecture for Low Latency Graph Neural Network (LL-GNN) designs for particle detectors, delivering unprecedented low latency performance. Incorporating FPGA-based GNNs into particle detectors presents a unique challenge since it requires sub-microsecond latency to deploy the networks for online event selection with a data rate of hundreds of terabytes per second in the Level-1 triggers at the CERN Large Hadron Collider experiments. This paper proposes a novel outer-product based matrix multiplication approach, which is enhanced by exploiting the structured adjacency matrix and a column-major data layout. Moreover, a fusion step is introduced to further reduce the end-to-end design latency by eliminating unnecessary boundaries. Furthermore, a GNN-specific algorithm-hardware co-design approach is presented which not only finds a design with a much better latency but also finds a high accuracy design under given latency constraints. To facilita
    
[^195]: 暂停分子表征学习：用于分子属性预测的新方法

    Taking a Respite from Representation Learning for Molecular Property Prediction. (arXiv:2209.13492v3 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2209.13492](http://arxiv.org/abs/2209.13492)

    本研究对一系列分子表征模型进行了系统评估，发现基于固定表征的模型在分子属性预测中具有一定优势，同时也揭示了活性断崖问题。

    

    人工智能在药物发现中的应用越来越广泛，其中重要的任务之一就是分子属性预测。虽然分子表征学习的技术如此发达，但其背后的基础问题却未被认真探究。在本研究中，我们使用多种分子表征对一系列代表性模型进行了系统评估。除了常用的MoleculeNet基准数据集外，我们还从ChEMBL数据库和文献中收集了一套与阿片类物质相关的数据集以及两个额外的活性数据集。同时，我们也组装了一系列具有不同规模的描述符数据集来评估模型的性能。总共，我们训练了62,820个模型，其中包括50,220个使用固定表征的模型、4,200个使用SMILES序列的模型和8,400个使用分子图的模型。我们首先进行了数据集分析，并强调了阿片类物质中的活性断崖问题。

    Artificial intelligence (AI) has been widely applied in drug discovery with a major task as molecular property prediction. Despite booming techniques in molecular representation learning, fundamentals underlying molecular property prediction haven't been carefully examined yet. In this study, we conducted a systematic evaluation on a collection of representative models using various molecular representations. In addition to the commonly used MoleculeNet benchmark datasets, we also assembled a suite of opioids-related datasets from ChEMBL and two additional activity datasets from literature. To interrogate the basic predictive power, we also assembled a series of descriptors datasets with varying sizes to evaluate the models' performance. In total, we trained 62,820 models, including 50,220 models on fixed representations, 4,200 models on SMILES sequences and 8,400 models on molecular graphs. We first conducted dataset profiling and highlighted the activity-cliffs issue in the opioids-r
    
[^196]: 简化模型为基础的强化学习：通过一个目标学习表示、隐空间模型和策略

    Simplifying Model-based RL: Learning Representations, Latent-space Models, and Policies with One Objective. (arXiv:2209.08466v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.08466](http://arxiv.org/abs/2209.08466)

    提出了一个单一的、具有自洽性的目标，它共同优化了隐空间模型和策略，以实现高回报，从而简化模型为基础的强化学习方法。

    

    虽然学习环境内部模型的强化学习（RL）方法可能比其模型无关的对手更具样本效率，但学习从高维传感器中模拟原始观察结果的模型可能具有挑战性。

    While reinforcement learning (RL) methods that learn an internal model of the environment have the potential to be more sample efficient than their model-free counterparts, learning to model raw observations from high dimensional sensors can be challenging. Prior work has addressed this challenge by learning low-dimensional representation of observations through auxiliary objectives, such as reconstruction or value prediction. However, the alignment between these auxiliary objectives and the RL objective is often unclear. In this work, we propose a single objective which jointly optimizes a latent-space model and policy to achieve high returns while remaining self-consistent. This objective is a lower bound on expected returns. Unlike prior bounds for model-based RL on policy exploration or model guarantees, our bound is directly on the overall RL objective. We demonstrate that the resulting algorithm matches or improves the sample-efficiency of the best prior model-based and model-fre
    
[^197]: R\'{e}nyi散度深度互相学习

    R\'{e}nyi Divergence Deep Mutual Learning. (arXiv:2209.05732v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05732](http://arxiv.org/abs/2209.05732)

    本文提出在深度互相学习中使用R\'{e}nyi散度，它能够在不引入大量复杂度的情况下持续提高性能，获得了广泛的实证结果的支持。

    

    本文重审了一种简单而有效的计算范式——深度互相学习（DML）。我们提出使用R\'{e}nyi散度而不是KL散度，这种做法更加灵活、可调，以改善vanilla DML。这种修改能够在有限的附加复杂性下不断提高性能。该范例的收敛性进行了理论分析，并且表明具有恒定学习率的随机梯度下降在非凸优化任务的最坏情况下收敛的偏差为$\mathcal{O}(1)$。

    This paper revisits Deep Mutual Learning (DML), a simple yet effective computing paradigm. We propose using R\'{e}nyi divergence instead of the KL divergence, which is more flexible and tunable, to improve vanilla DML. This modification is able to consistently improve performance over vanilla DML with limited additional complexity. The convergence properties of the proposed paradigm are analyzed theoretically, and Stochastic Gradient Descent with a constant learning rate is shown to converge with $\mathcal{O}(1)$-bias in the worst case scenario for nonconvex optimization tasks. That is, learning will reach nearby local optima but continue searching within a bounded scope, which may help mitigate overfitting. Finally, our extensive empirical results demonstrate the advantage of combining DML and R\'{e}nyi divergence, which further improves generalized models.
    
[^198]: 机器学习中量化Aleatoric和Epistemic不确定性：条件熵和互信息是否适当的度量？

    Quantifying Aleatoric and Epistemic Uncertainty in Machine Learning: Are Conditional Entropy and Mutual Information Appropriate Measures?. (arXiv:2209.03302v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.03302](http://arxiv.org/abs/2209.03302)

    该论文提出了对于机器学习中量化Aleatoric和Epistemic不确定性的条件熵和互信息作为度量方法的批评和质疑，提出了这些度量存在的各种不一致性，并对将总不确定性分解为其Aleatoric和Epistemic成分的加法分解进行了讨论。实验结果证明了这些理论发现的合理性，并提出了当前有关不确定性量化的实践存在的问题。

    

    近来在机器学习中，以条件熵和互信息的形式量化Aleatoric和Epistemic这两种不确定性的趋势日益增长。这些度量的信息理论根基看起来很有吸引力，但我们发现了各种不一致性，这使得它们的适用性成为问题。除了这些度量，我们还批判性地讨论了将总不确定性分解为其Aleatoric和Epistemic成分的加法分解的想法。不同的计算机视觉任务的实验支持我们的理论发现，并提出了关于不确定性量化的当前实践的担忧。

    The quantification of aleatoric and epistemic uncertainty in terms of conditional entropy and mutual information, respectively, has recently become quite common in machine learning. While the properties of these measures, which are rooted in information theory, seem appealing at first glance, we identify various incoherencies that call their appropriateness into question. In addition to the measures themselves, we critically discuss the idea of an additive decomposition of total uncertainty into its aleatoric and epistemic constituents. Experiments across different computer vision tasks support our theoretical findings and raise concerns about current practice in uncertainty quantification.
    
[^199]: 非稳态连续赌臂策略在模拟金融市场中的自动化交易

    Nonstationary Continuum-Armed Bandit Strategies for Automated Trading in a Simulated Financial Market. (arXiv:2208.02901v3 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2208.02901](http://arxiv.org/abs/2208.02901)

    本文提出了一种名为PRBO的新型交易算法，通过使用贝叶斯优化和“赌臂机中的赌臂机”框架实现了自动化交易策略在模拟金融市场中的适应性调整，与参考交易策略PRSH相比，PRBO产生了更高的收益。

    

    本文探讨了设计一种自动化交易策略来适应不断变化着的市场状况的挑战。这个问题可以视为一个非稳态连续赌臂问题。为了解决这个问题，我们提出了一种名为PRBO的新型交易算法，它使用贝叶斯优化和“赌臂机中的赌臂机”框架来根据市场状况动态调整策略参数。我们使用Bristol股票交易所（BSE）来模拟包含异质自动交易代理人群体的金融市场，并将PRBO与通过随机爬山法调整策略参数的参考交易策略PRSH进行比较。结果表明，尽管要调整的超参数更少，但PRBO产生的利润显著高于PRSH。 PRBO的代码和实验数据已在GitHub上开源提供（https://github.com/HarmoniaLeo/PRZI-Bayesian-Optimisation）。

    We approach the problem of designing an automated trading strategy that can consistently profit by adapting to changing market conditions. This challenge can be framed as a Nonstationary Continuum-Armed Bandit (NCAB) problem. To solve the NCAB problem, we propose PRBO, a novel trading algorithm that uses Bayesian optimization and a ``bandit-over-bandit'' framework to dynamically adjust strategy parameters in response to market conditions. We use Bristol Stock Exchange (BSE) to simulate financial markets containing heterogeneous populations of automated trading agents and compare PRBO with PRSH, a reference trading strategy that adapts strategy parameters through stochastic hill-climbing. Results show that PRBO generates significantly more profit than PRSH, despite having fewer hyperparameters to tune. The code for PRBO and performing experiments is available online open-source (https://github.com/HarmoniaLeo/PRZI-Bayesian-Optimisation).
    
[^200]: 一种具有截断柯西随机扰动的渐进平滑函数算法用于随机优化

    A Gradient Smoothed Functional Algorithm with Truncated Cauchy Random Perturbations for Stochastic Optimization. (arXiv:2208.00290v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2208.00290](http://arxiv.org/abs/2208.00290)

    本文提出了一种具有截断柯西随机扰动的随机梯度算法用于非凸目标函数的优化，算法具有稳定性与快速收敛性。

    

    本文提出了一种随机梯度算法，用于最小化一个光滑的目标函数，该函数是噪声成本样本的期望，而只有后者对任何给定的参数进行观测。我们的算法采用带有随机扰动的梯度估计方案，这些扰动使用从delta球中得到的截断柯西分布形成。我们分析了所提出的梯度估计器的偏差和方差。我们发现，当目标函数是非凸的，而参数维数很高时，我们的算法非常有用。从渐近收敛分析中，我们建立了我们的算法几乎确定地收敛于目标函数的稳定点集合，并获得了收敛的渐近速率。我们还表明，我们的算法避免了不稳定的平衡点，意味着收敛到局部最小值。此外，我们对我们的算法进行了非渐近收敛性分析。特别地，我们在这里建立了一个非渐近保证收敛率的收敛性结果，该结果是实数值的和给出了精确的界限。

    In this paper, we present a stochastic gradient algorithm for minimizing a smooth objective function that is an expectation over noisy cost samples, and only the latter are observed for any given parameter. Our algorithm employs a gradient estimation scheme with random perturbations, which are formed using the truncated Cauchy distribution from the delta sphere. We analyze the bias and variance of the proposed gradient estimator. Our algorithm is found to be particularly useful in the case when the objective function is non-convex, and the parameter dimension is high. From an asymptotic convergence analysis, we establish that our algorithm converges almost surely to the set of stationary points of the objective function and obtains the asymptotic convergence rate. We also show that our algorithm avoids unstable equilibria, implying convergence to local minima. Further, we perform a non-asymptotic convergence analysis of our algorithm. In particular, we establish here a non-asymptotic b
    
[^201]: 活在当下：适应性进化策略的学习动力学模型

    Live in the Moment: Learning Dynamics Model Adapted to Evolving Policy. (arXiv:2207.12141v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.12141](http://arxiv.org/abs/2207.12141)

    本文提出了一种名为PDML的学习动力学模型的方法，该方法动态调整历史策略混合分布以适应训练过程中的进化策略，实验结果表明PDML优于最先进的方法，实现了更快的收敛和更好的最终性能。

    

    基于模型的强化学习通常比模型无关的强化学习在实践中具有更高的样本效率，因为它学习一个动力学模型来生成策略学习的样本。以前的研究学习使动力学模型适应所有历史策略下的经验状态-动作访问分布，即样本回放缓冲区。然而，在本文中，我们观察到，在“所有历史策略”下使动力学模型适应分布不一定有益于模型预测“当前策略”，因为正在使用的策略在时间上不断演变。训练过程中的进化策略会导致状态-动作访问分布的转移。我们在理论上分析了这种历史策略分布的转移如何影响模型学习和模型回滚。因此，我们提出了一种新颖的动力学模型学习方法，称为策略适应动力学模型学习(PDML)。PDML动态调整用于动力学模型学习的历史策略混合分布，以适应训练过程中的进化策略。实验结果表明，PDML优于最先进的方法，实现了更快的收敛和更好的最终性能。

    Model-based reinforcement learning (RL) often achieves higher sample efficiency in practice than model-free RL by learning a dynamics model to generate samples for policy learning. Previous works learn a dynamics model that fits under the empirical state-action visitation distribution for all historical policies, i.e., the sample replay buffer. However, in this paper, we observe that fitting the dynamics model under the distribution for \emph{all historical policies} does not necessarily benefit model prediction for the \emph{current policy} since the policy in use is constantly evolving over time. The evolving policy during training will cause state-action visitation distribution shifts. We theoretically analyze how this distribution shift over historical policies affects the model learning and model rollouts. We then propose a novel dynamics model learning method, named \textit{Policy-adapted Dynamics Model Learning (PDML)}. PDML dynamically adjusts the historical policy mixture dist
    
[^202]: 张量列车交叉逼近的误差分析

    Error Analysis of Tensor-Train Cross Approximation. (arXiv:2207.04327v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.04327](http://arxiv.org/abs/2207.04327)

    本文提供了张量列车交叉逼近的误差分析，给出了精确和有噪声测量的整个张量的准确性保证，并提供了如何在计算成本和逼近准确性之间取得平衡的见解。

    

    张量列车分解由于其对高维张量的简洁表示克服了维度灾难，因此在机器学习和量子物理中被广泛使用。交叉逼近最初是针对从一组选定的行和列中表示矩阵的有效方法，并且是从其少量元素中构建张量列车分解的高效方法。尽管张量列车交叉逼近在实际应用中取得了显著的性能，但其理论分析，特别是关于逼近误差的分析，迄今仍然缺乏。据我们所知，现有的结果仅提供逐元素逼近准确性保证，当延伸到整个张量时会导致非常松散的界限。在本文中，我们通过提供精确和有噪声测量的整个张量的准确性保证来弥合这一差距。我们的结果阐明了所选子张量的选择如何影响交叉逼近的质量，并提供了如何平衡计算成本和逼近准确性的见解。

    Tensor train decomposition is widely used in machine learning and quantum physics due to its concise representation of high-dimensional tensors, overcoming the curse of dimensionality. Cross approximation-originally developed for representing a matrix from a set of selected rows and columns-is an efficient method for constructing a tensor train decomposition of a tensor from few of its entries. While tensor train cross approximation has achieved remarkable performance in practical applications, its theoretical analysis, in particular regarding the error of the approximation, is so far lacking. To our knowledge, existing results only provide element-wise approximation accuracy guarantees, which lead to a very loose bound when extended to the entire tensor. In this paper, we bridge this gap by providing accuracy guarantees in terms of the entire tensor for both exact and noisy measurements. Our results illustrate how the choice of selected subtensors affects the quality of the cross appr
    
[^203]: 学习纠正用于模拟湍流的谱方法

    Learning to correct spectral methods for simulating turbulent flows. (arXiv:2207.00556v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.00556](http://arxiv.org/abs/2207.00556)

    通过机器学习的谱求解器在物理先验知识的指导下可以比传统求解器更准确地模拟流体动力学问题。

    

    尽管部分微分方程（PDE）在科学和工程中无处不在，但其中只有少数具有解析或闭合形式的解。这激发了大量经典的 PDE 数值模拟工作以及更近期的利用机器学习（ML）的数据驱动技术研究。最近的研究表明，经典数值技术和机器学习的混合可以比单独使用任一方法带来显着的改进。在本文中，我们展示了当加入基于物理学先验知识时，数值方案的选择至关重要。我们基于 Fourier 谱方法进行了改进，这种谱方法已知比其他数值方案更适合模拟具有平滑和周期性解的 PDE。具体而言，我们开发了用于三个流体力学常见 PDE 的 ML增强的谱求解器。在相同的解析度下，我们的模型比标准的谱求解器更准确（2-4倍），但整体运行时间更长。

    Despite their ubiquity throughout science and engineering, only a handful of partial differential equations (PDEs) have analytical, or closed-form solutions. This motivates a vast amount of classical work on numerical simulation of PDEs and more recently, a whirlwind of research into data-driven techniques leveraging machine learning (ML). A recent line of work indicates that a hybrid of classical numerical techniques and machine learning can offer significant improvements over either approach alone. In this work, we show that the choice of the numerical scheme is crucial when incorporating physics-based priors. We build upon Fourier-based spectral methods, which are known to be more efficient than other numerical schemes for simulating PDEs with smooth and periodic solutions. Specifically, we develop ML-augmented spectral solvers for three common PDEs of fluid dynamics. Our models are more accurate (2-4x) than standard spectral solvers at the same resolution but have longer overall ru
    
[^204]: 语言模型作为知识嵌入

    Language Models as Knowledge Embeddings. (arXiv:2206.12617v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.12617](http://arxiv.org/abs/2206.12617)

    该论文提出了一种使用语言模型来推导知识嵌入的方法LMKE，它旨在提高对丰富的长尾实体的表示能力并解决基于描述的先前方法的问题，实验结果表明该方法在多个基准数据集上实现了最先进的性能。

    

    知识嵌入是通过将实体和关系嵌入到连续向量空间中来表示知识图谱的一种方法。现有的方法主要是基于结构或基于描述。基于结构的方法学习表示，以保留知识图谱的内在结构。它们不能很好地表示现实世界知识图谱中有限结构信息下丰富的长尾实体。基于描述的方法利用文本信息和语言模型。在这个方向上的先前方法几乎无法超越基于结构的方法，并且存在昂贵的负采样和限制性描述需求等问题。在本文中，我们提出了LMKE，采用语言模型来推导知识嵌入，旨在丰富长尾实体的表示并解决基于描述的先前方法的问题。我们用对比学习框架来表述基于描述的知识嵌入学习，以提高训练和评价的效率。实验结果表明，LMKE在多个基准数据集上实现了最先进的性能，超越了基于结构和基于先前描述的方法。

    Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding entities and relations into continuous vector spaces. Existing methods are mainly structure-based or description-based. Structure-based methods learn representations that preserve the inherent structure of KGs. They cannot well represent abundant long-tail entities in real-world KGs with limited structural information. Description-based methods leverage textual information and language models. Prior approaches in this direction barely outperform structure-based ones, and suffer from problems like expensive negative sampling and restrictive description demand. In this paper, we propose LMKE, which adopts Language Models to derive Knowledge Embeddings, aiming at both enriching representations of long-tail entities and solving problems of prior description-based methods. We formulate description-based KE learning with a contrastive learning framework to improve efficiency in training and evaluation. Experimental resul
    
[^205]: 基于回归的超标概率预测方法用于显著波高预测

    Exceedance Probability Forecasting via Regression for Significant Wave Height Prediction. (arXiv:2206.09821v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.09821](http://arxiv.org/abs/2206.09821)

    本论文提出了一种基于回归的超标概率预测方法，用于预测显著波高，通过利用预测来估计超标概率，取得了更好的效果。

    

    显著波高预测是海洋数据分析中的一个关键问题。预测显著波高对于估计波能产生是至关重要的。此外，及时预测大浪的到来对于确保航海作业的安全很重要。我们将预测显著波高的极端值作为超标概率预测问题。因此，我们旨在估计显著波高将超过预定义阈值的概率。通常使用概率二分类模型来解决这个任务。相反，我们提出了一种基于预测模型的新方法。该方法利用未来观测的预测来根据累积分布函数估计超标概率。我们使用来自加拿大哈利法克斯海岸的浮标数据进行了实验。结果表明，所提出的方法更好。

    Significant wave height forecasting is a key problem in ocean data analytics. Predicting the significant wave height is crucial for estimating the energy production from waves. Moreover, the timely prediction of large waves is important to ensure the safety of maritime operations, e.g. passage of vessels. We frame the task of predicting extreme values of significant wave height as an exceedance probability forecasting problem. Accordingly, we aim at estimating the probability that the significant wave height will exceed a predefined threshold. This task is usually solved using a probabilistic binary classification model. Instead, we propose a novel approach based on a forecasting model. The method leverages the forecasts for the upcoming observations to estimate the exceedance probability according to the cumulative distribution function. We carried out experiments using data from a buoy placed in the coast of Halifax, Canada. The results suggest that the proposed methodology is better
    
[^206]: 通过边际公平性来界定和逼近交集公平性

    Bounding and Approximating Intersectional Fairness through Marginal Fairness. (arXiv:2206.05828v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.05828](http://arxiv.org/abs/2206.05828)

    本文旨在通过统计分析了解边际公平性和交集公平性之间的关系，在一定条件下取得精确关系。在高概率下,通过边际公平性和其他有意义的统计量可以计算出交集公平性的界限。

    

    机器学习中的歧视通常涉及多个维度（即保护属性）；因此，确保“交集公平性”，即不歧视任何子组，是理想的。众所周知，仅独立地保证每个维度的“边际公平性”通常是不足够的。然而，由于子组的指数数量，直接从数据中度量交集公平性是不可能的。在本文中，我们的主要目标是通过统计分析详细了解边际公平性和交集公平性之间的关系。我们首先确定了一组足够的条件，可以在其中获得精确关系。然后，我们证明了在一般情况下，在高概率下通过边际公平性和其他有意义的统计量可以计算出交集公平性的界限。除了它们的描述价值外，我们还展示了这些理论界限可以利用到一种启发式的提高方法中。

    Discrimination in machine learning often arises along multiple dimensions (a.k.a. protected attributes); it is then desirable to ensure \emph{intersectional fairness} -- i.e., that no subgroup is discriminated against. It is known that ensuring \emph{marginal fairness} for every dimension independently is not sufficient in general. Due to the exponential number of subgroups, however, directly measuring intersectional fairness from data is impossible. In this paper, our primary goal is to understand in detail the relationship between marginal and intersectional fairness through statistical analysis. We first identify a set of sufficient conditions under which an exact relationship can be obtained. Then, we prove bounds (easily computable through marginal fairness and other meaningful statistical quantities) in high-probability on intersectional fairness in the general case. Beyond their descriptive value, we show that these theoretical bounds can be leveraged to derive a heuristic impro
    
[^207]: $\mathcal{R}$-范归纳偏差的内在维度和泛化性质研究

    Intrinsic dimensionality and generalization properties of the $\mathcal{R}$-norm inductive bias. (arXiv:2206.05317v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05317](http://arxiv.org/abs/2206.05317)

    本研究研究了$\mathcal{R}$-范归纳偏差的结构和统计特性，发现即使存在岭函数拟合数据，插值函数也是内在的多变量函数；并且，$\mathcal{R}$-范归纳偏差对于某些学习问题的统计最优泛化不足够。

    

    我们研究了$\mathcal{R}$-范最小化插值函数的结构和统计特性，它们对特定目标函数标记的数据集进行标记。$\mathcal{R}$-范是一个归纳偏差，用于两层神经网络，最近引入了它来捕捉控制网络权重大小的功能效应，不受网络宽度的影响。我们发现，即使存在拟合数据的岭函数，这些插值函数也是内在的多变量函数；同时，$\mathcal{R}$-范归纳偏差对于某些学习问题的统计最优泛化不足够。总的来说，这些结果揭示了一个与实际神经网络训练相关的归纳偏差的新见解。

    We study the structural and statistical properties of $\mathcal{R}$-norm minimizing interpolants of datasets labeled by specific target functions. The $\mathcal{R}$-norm is the basis of an inductive bias for two-layer neural networks, recently introduced to capture the functional effect of controlling the size of network weights, independently of the network width. We find that these interpolants are intrinsically multivariate functions, even when there are ridge functions that fit the data, and also that the $\mathcal{R}$-norm inductive bias is not sufficient for achieving statistically optimal generalization for certain learning problems. Altogether, these results shed new light on an inductive bias that is connected to practical neural network training.
    
[^208]: 概率分类中的预测多样性

    Predictive Multiplicity in Probabilistic Classification. (arXiv:2206.01131v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.01131](http://arxiv.org/abs/2206.01131)

    本文提出了一种概率分类的预测多样性度量框架，用于衡量在一组相似模型上风险评估的变化。

    

    机器学习模型经常用于风险评估任务，比如预测消费者违约风险、预测一个人是否患有严重疾病或预测一个人在法庭上出现的风险。但如果有多个模型在一个预测任务中表现几乎同样好，那么这些模型的预测结果会有多大的变化？如果相似模型的预测结果相对一致，则可以选择优化惩罚损失的模型。但如果相似模型的预测结果有显著差异，这在机器学习中被称为预测多样性，即通过优化判定标准而获得的相似分类器的决策差异。本文提出了一种概率分类的预测多样性度量框架，引入了一些衡量在一组相似模型上风险评估变化的量

    Machine learning models are often used to inform real world risk assessment tasks: predicting consumer default risk, predicting whether a person suffers from a serious illness, or predicting a person's risk to appear in court. Given multiple models that perform almost equally well for a prediction task, to what extent do predictions vary across these models? If predictions are relatively consistent for similar models, then the standard approach of choosing the model that optimizes a penalized loss suffices. But what if predictions vary significantly for similar models? In machine learning, this is referred to as predictive multiplicity i.e. the prevalence of conflicting predictions assigned by near-optimal competing models. In this paper, we present a framework for measuring predictive multiplicity in probabilistic classification (predicting the probability of a positive outcome). We introduce measures that capture the variation in risk estimates over the set of competing models, and d
    
[^209]: 稀疏图的半监督聚类：跨越了信息理论门槛

    Semi-Supervised Clustering of Sparse Graphs: Crossing the Information-Theoretic Threshold. (arXiv:2205.11677v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.11677](http://arxiv.org/abs/2205.11677)

    该论文提出了两种有效的算法来将标签信息与稀疏图结构相结合，解决了基于网络拓扑的聚类在稀疏图上的问题。

    

    随机块模型是一种用于网络结构数据聚类和社区检测的基本随机图模型。数十年来对该问题的广泛研究已经建立了许多深刻的结果，其中Kesten-Stigum门槛处的相变现象特别有趣，从数学和应用角度都具有重要意义。它表明，如果模型参数在某个门槛以下，基于网络拓扑的任何估计器在稀疏图上都不能比随机猜测更好。然而，如果我们稍微扩展视野到普遍存在的半监督设置，这样的基本限制将完全消失。我们证明，通过揭示出任意一部分标记，可以在整个参数域内对检测问题进行处理。此外，我们引入了两种有效的算法，一种是基于组合的，一种是基于优化的，用于将标签信息与图结构相结合。我们的工作为随机块模型和半监督学习带来了全新的视角，标志着稀疏图聚类领域的重大突破。

    The stochastic block model is a canonical random graph model for clustering and community detection on network-structured data. Decades of extensive study on the problem have established many profound results, among which the phase transition at the Kesten-Stigum threshold is particularly interesting both from a mathematical and an applied standpoint. It states that no estimator based on the network topology can perform substantially better than chance on sparse graphs if the model parameter is below certain threshold. Nevertheless, if we slightly extend the horizon to the ubiquitous semi-supervised setting, such a fundamental limitation will disappear completely. We prove that with arbitrary fraction of the labels revealed, the detection problem is feasible throughout the parameter domain. Moreover, we introduce two efficient algorithms, one combinatorial and one based on optimization, to integrate label information with graph structures. Our work brings a new perspective to stochasti
    
[^210]: 基于学习动力学的梯度轨迹优化

    Gradient-Based Trajectory Optimization With Learned Dynamics. (arXiv:2204.04558v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2204.04558](http://arxiv.org/abs/2204.04558)

    利用机器学习从数据中学习可微动力学模型，实现机器人进行高度动态和复杂任务的轨迹优化，硬件实验表明学习到的模型可成功执行这些任务。

    

    近年来，轨迹优化方法在实际机器人上已经达到了非常高的性能水平。但这些方法严重依赖准确的分析动力学模型，然而，物理世界的一些方面仅能以有限的方式被捕捉。一种替代方法是利用机器学习技术从数据中学习系统的可微动力学模型。本文利用轨迹优化和模型学习，实现了在缺乏精确分析动力学模型的情况下使用机器人进行高动态和复杂任务。我们展示了神经网络可以从仅采集25分钟的交互数据中，准确地模拟大时间范围内高度非线性的行为，这些数据来自两种不同的机器人：（i）波士顿动力公司的机器狗，（ii）遥控汽车。此外，我们使用神经网络的梯度来执行基于梯度的轨迹优化。在硬件实验中，我们证明了我们学习到的模型在现实机器人上能够成功地执行高度动态和复杂的任务。

    Trajectory optimization methods have achieved an exceptional level of performance on real-world robots in recent years. These methods heavily rely on accurate analytical models of the dynamics, yet some aspects of the physical world can only be captured to a limited extent. An alternative approach is to leverage machine learning techniques to learn a differentiable dynamics model of the system from data. In this work, we use trajectory optimization and model learning for performing highly dynamic and complex tasks with robotic systems in absence of accurate analytical models of the dynamics. We show that a neural network can model highly nonlinear behaviors accurately for large time horizons, from data collected in only 25 minutes of interactions on two distinct robots: (i) the Boston Dynamics Spot and an (ii) RC car. Furthermore, we use the gradients of the neural network to perform gradient-based trajectory optimization. In our hardware experiments, we demonstrate that our learned mo
    
[^211]: 利用神经Q-学习解决偏微分方程

    Neural Q-learning for solving PDEs. (arXiv:2203.17128v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2203.17128](http://arxiv.org/abs/2203.17128)

    本文提出了一种新的利用神经Q-学习算法解决椭圆型PDE数值方法，该算法无网格且具有克服维度灾难的潜力。本文证明了这种方法的有效性，并在单调PDE的情况下得到了极限神经网络收敛于PDE解的证明。

    

    解决高维偏微分方程是科学计算中的一个主要挑战。本文通过改进强化学习中的Q-学习算法开发了一种新的解决椭圆型PDE的数值方法。我们的“Q-PDE”算法是无网格的，因此具有克服维度灾难的潜力。我们使用神经切向核（NTK）方法证明，使用Q-PDE算法训练的PDE解的神经网络逼近器，随着隐藏层单元数的增加，收敛于无穷维常微分方程（ODE）的轨迹。对于单调PDE（即由单调算符给出的可能是非线性的PDE），尽管NTK中缺乏谱间隙，我们证明了满足无穷维ODE的极限神经网络会随着训练时间的增加收敛于$L^2$中的PDE解。更一般地说，我们可以证明wi的任何不动点都是该无穷维ODE的解。

    Solving high-dimensional partial differential equations (PDEs) is a major challenge in scientific computing. We develop a new numerical method for solving elliptic-type PDEs by adapting the Q-learning algorithm in reinforcement learning. Our "Q-PDE" algorithm is mesh-free and therefore has the potential to overcome the curse of dimensionality. Using a neural tangent kernel (NTK) approach, we prove that the neural network approximator for the PDE solution, trained with the Q-PDE algorithm, converges to the trajectory of an infinite-dimensional ordinary differential equation (ODE) as the number of hidden units $\rightarrow \infty$. For monotone PDE (i.e. those given by monotone operators, which may be nonlinear), despite the lack of a spectral gap in the NTK, we then prove that the limit neural network, which satisfies the infinite-dimensional ODE, converges in $L^2$ to the PDE solution as the training time $\rightarrow \infty$. More generally, we can prove that any fixed point of the wi
    
[^212]: 最优学习

    Optimal Learning. (arXiv:2203.15994v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.15994](http://arxiv.org/abs/2203.15994)

    本文证明，通过解决一个带有惩罚项的离散过度参数化优化问题，可以找到近乎最优的 $\hat f$。

    

    本文研究了从关于 $f$ 的给定数据学习未知函数 $f$ 的问题。学习问题是给出一个近似值 $ \hat f $，用于预测数据外的 $f$ 值。这个学习问题的准确性取决于：（i）我们对 $f$ 有什么额外的信息（称为模型类假设），（ii）我们如何度量 $\hat f$ 的预测准确性，（iii）数据和数据站点的情况，（iv）数据观察是否受到噪声污染。在存在模型类假设的情况下，已知最优恢复性能的数学描述。在标准模型类假设下，本文证明，通过解决一个带有惩罚项的离散过度参数化优化问题，可以找到近乎最优的 $\hat f$。最优指的是误差受到固定倍数的限制。

    This paper studies the problem of learning an unknown function $f$ from given data about $f$. The learning problem is to give an approximation $\hat f$ to $f$ that predicts the values of $f$ away from the data. There are numerous settings for this learning problem depending on (i) what additional information we have about $f$ (known as a model class assumption), (ii) how we measure the accuracy of how well $\hat f$ predicts $f$, (iii) what is known about the data and data sites, (iv) whether the data observations are polluted by noise. A mathematical description of the optimal performance possible (the smallest possible error of recovery) is known in the presence of a model class assumption. Under standard model class assumptions, it is shown in this paper that a near optimal $\hat f$ can be found by solving a certain discrete over-parameterized optimization problem with a penalty term. Here, near optimal means that the error is bounded by a fixed constant times the optimal error. This
    
[^213]: 无需可信任服务器的私有非凸联邦学习

    Private Non-Convex Federated Learning Without a Trusted Server. (arXiv:2203.06735v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.06735](http://arxiv.org/abs/2203.06735)

    本文提出了一种用于非凸联邦学习的新型互中心隐私保护方法，并考虑了多种复杂的损失函数，包括满足PPL条件的损失函数和强凸损失函数。

    

    本文研究了在医院等多个数据中心间进行非凸损失函数的联邦学习，需要保证每个数据点的隐私，同时在没有可信赖的服务器的情况下进行计算。我们提出了一种互中心隐私保护方法，并使用它来解决异构数据和多种损失函数的问题。其中，我们研究了满足PPL条件的损失函数和强凸损失函数的情况。

    We study federated learning (FL) -- especially cross-silo FL -- with non-convex loss functions and data from people who do not trust the server or other silos. In this setting, each silo (e.g. hospital) must protect the privacy of each person's data (e.g. patient's medical record), even if the server or other silos act as adversarial eavesdroppers. To that end, we consider inter-silo record-level (ISRL) differential privacy (DP), which requires silo~$i$'s communications to satisfy record/item-level DP. We propose novel ISRL-DP algorithms for FL with heterogeneous (non-i.i.d.) silo data and two classes of Lipschitz continuous loss functions: First, we consider losses satisfying the Proximal Polyak-Lojasiewicz (PL) inequality, which is an extension of the classical PL condition to the constrained setting. In contrast to our result, prior works only considered unconstrained private optimization with Lipschitz PL loss, which rules out most interesting PL losses such as strongly convex prob
    
[^214]: 可解释的离线策略学习：基于超立方体搜索的方法

    Interpretable Off-Policy Learning via Hyperbox Search. (arXiv:2203.02473v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2203.02473](http://arxiv.org/abs/2203.02473)

    本文提出了一个基于超立方体搜索的可解释离线策略学习算法，可以用合取范式表示，可以灵活逼近任何可测函数。在临床实践中具有重要意义。

    

    个性化治疗决策已成为现代医学的重要组成部分。因此，目标是根据个体患者的特征进行治疗决策。已经开发了许多方法从观测数据中学习这样的策略，以实现在特定策略类别下获得最佳结果。但是，这些方法很少具有可解释性。然而，可解释性通常是临床实践中策略学习的前提条件。在本文中，我们提出了一种基于超立方体搜索的可解释离线策略学习算法。特别地，我们的策略可以用合取范式表示（即AND的OR），因此是容易理解的。我们证明了一个通用逼近定理，证明了我们的策略类可以灵活地逼近任何可测函数。为了优化，我们在分支定界框架内开发了一个定制的列生成过程。通过模拟研究，我们证明了我们的算法优于其他方法。

    Personalized treatment decisions have become an integral part of modern medicine. Thereby, the aim is to make treatment decisions based on individual patient characteristics. Numerous methods have been developed for learning such policies from observational data that achieve the best outcome across a certain policy class. Yet these methods are rarely interpretable. However, interpretability is often a prerequisite for policy learning in clinical practice. In this paper, we propose an algorithm for interpretable off-policy learning via hyperbox search. In particular, our policies can be represented in disjunctive normal form (i.e., OR-of-ANDs) and are thus intelligible. We prove a universal approximation theorem that shows that our policy class is flexible enough to approximate any measurable function arbitrarily well. For optimization, we develop a tailored column generation procedure within a branch-and-bound framework. Using a simulation study, we demonstrate that our algorithm outpe
    
[^215]: 选择超参数优化方法的从业者动机

    Practitioner Motives to Select Hyperparameter Optimization Methods. (arXiv:2203.01717v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01717](http://arxiv.org/abs/2203.01717)

    研究探讨了机器学习从业者选择超参数优化方法的动机，结果表明这基于个人目标和背景因素，调查还给出了优化模型的六个主要目标。

    

    先进的编程超参数优化方法，如贝叶斯优化，具有高样本效率，能够可靠地找到机器学习模型的最佳超参数值。然而，机器学习从业者经常应用样本效率较低的HPO方法，如网格搜索，这通常导致机器学习模型未经优化。我们怀疑，从业者选择HPO方法的原因基于个人动机，包括背景因素和个人目标。然而，从业者的动机仍然需要澄清，这妨碍了评估HPO方法以实现特定目标和以用户为中心的HPO工具的开发。为了了解从业者使用特定HPO方法的动机，我们采用混合方法，包括20个半结构化访谈和一项调查研究，共有71名机器学习专家参与，以收集访谈结果的外部有效性的证据。通过设置六个主要目标（例如，改进模型理解），

    Advanced programmatic hyperparameter optimization (HPO) methods, such as Bayesian optimization, have high sample efficiency in reproducibly finding optimal hyperparameter values of machine learning (ML) models. Yet, ML practitioners often apply less sample-efficient HPO methods, such as grid search, which often results in under-optimized ML models. As a reason for this behavior, we suspect practitioners choose HPO methods based on individual motives, consisting of contextual factors and individual goals. However, practitioners' motives still need to be clarified, hindering the evaluation of HPO methods for achieving specific goals and the user-centered development of HPO tools. To understand practitioners' motives for using specific HPO methods, we used a mixed-methods approach involving 20 semi-structured interviews and a survey study with 71 ML experts to gather evidence of the external validity of the interview results. By presenting six main goals (e.g., improving model understandi
    
[^216]: 基于张量列缩影的生成建模方法

    Generative modeling via tensor train sketching. (arXiv:2202.11788v6 [math.NA] UPDATED)

    [http://arxiv.org/abs/2202.11788](http://arxiv.org/abs/2202.11788)

    本文提出了一种基于张量列缩影的生成建模方法，可以用少量样本避免高维度带来的计算和样本复杂度困扰。

    

    本文提出了一种用于构建概率密度的张量列缩影表示的草图算法。与传统的递归奇异值分解（SVD）方法不同，我们提出并解决了一系列针对单个张量列缩影的小型线性系统。这种方法可以避免威胁到恢复问题的算法和样本复杂性的维度诅咒。针对马尔可夫模型，我们证明张量核心可以在样本复杂度以对数形式随着维度而缩放的自然条件下被恢复。最后，我们通过几个数字实验展示了该方法的性能。

    In this paper, we introduce a sketching algorithm for constructing a tensor train representation of a probability density from its samples. Our method deviates from the standard recursive SVD-based procedure for constructing a tensor train. Instead, we formulate and solve a sequence of small linear systems for the individual tensor train cores. This approach can avoid the curse of dimensionality that threatens both the algorithmic and sample complexities of the recovery problem. Specifically, for Markov models under natural conditions, we prove that the tensor cores can be recovered with a sample complexity that scales logarithmically in the dimensionality. Finally, we illustrate the performance of the method with several numerical experiments.
    
[^217]: 使用安全筛选加速非负和有界变量线性回归算法

    Accelerating Non-Negative and Bounded-Variable Linear Regression Algorithms with Safe Screening. (arXiv:2202.07258v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.07258](http://arxiv.org/abs/2202.07258)

    本文提出了一种通过识别饱和坐标加速解决非负和有界变量线性回归问题的技术，实验结果表明其具有令人信服的加速效果。

    

    非负和有界变量线性回归问题在机器学习和信号处理的各种应用中都有所涉及。本文提出了一种技术，通过在迭代过程中识别饱和坐标，加速现有解决器来解决这些问题。这类似于先前针对稀疏正则化回归问题提出的安全筛选技术。所提出的策略是经过证明是安全的，因为它提供了理论保证，表明所识别的坐标确实在最优解中是饱和的。对合成数据和实际数据的实验结果都表明，对于非负和有界变量问题都具有令人信服的加速效果。

    Non-negative and bounded-variable linear regression problems arise in a variety of applications in machine learning and signal processing. In this paper, we propose a technique to accelerate existing solvers for these problems by identifying saturated coordinates in the course of iterations. This is akin to safe screening techniques previously proposed for sparsity-regularized regression problems. The proposed strategy is provably safe as it provides theoretical guarantees that the identified coordinates are indeed saturated in the optimal solution. Experimental results on synthetic and real data show compelling accelerations for both non-negative and bounded-variable problems.
    
[^218]: 无需成本的DNN视觉注意机制：忽略背景，提高泛化

    Towards Ignoring Backgrounds and Improving Generalization: a Costless DNN Visual Attention Mechanism. (arXiv:2202.00232v6 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2202.00232](http://arxiv.org/abs/2202.00232)

    本文提出了一种无需额外计算成本的DNN视觉注意机制，名为ISNet，能够忽略图像背景并在COVID-19和结核病检测任务中比多个最先进神经网络具有更好的最小化分类器决策偏差影响的能力。

    

    本文介绍了一种用于图像分类器的注意机制，以及相应的深度神经网络（DNN）体系结构，称为ISNet。在训练期间，ISNet使用分割目标来学习如何找到图像的感兴趣区域，并将注意力集中在该区域上。该提议基于一种新颖的概念，即在LRP解释热图中最小化背景相关性。它可以应用于几乎任何分类神经网络架构，而不会在运行时增加任何额外的计算成本。由于能够忽略背景，因此结果单个DNN可以代替常见的分割器后跟分类器的流水线，速度更快，更轻。在注入图像背景的合成偏差之后（在各种应用中），我们将ISNet与多个最先进的神经网络进行比较，并定量证明其在最小化分类器决策中偏差影响方面具有优越的能力。 COVID-19和结核病检测任务分别作为使用案例来使用。

    This work introduces an attention mechanism for image classifiers and the corresponding deep neural network (DNN) architecture, dubbed ISNet. During training, the ISNet uses segmentation targets to learn how to find the image's region of interest and concentrate its attention on it. The proposal is based on a novel concept, background relevance minimization in LRP explanation heatmaps. It can be applied to virtually any classification neural network architecture, without any extra computational cost at run-time. Capable of ignoring the background, the resulting single DNN can substitute the common pipeline of a segmenter followed by a classifier, being faster and lighter. After injecting synthetic bias in images' backgrounds (in diverse applications), we compare the ISNet to multiple state-of-the-art neural networks, and quantitatively demonstrate its superior capacity of minimizing the bias influence over the classifier decisions. The tasks of COVID-19 and tuberculosis detection in ch
    
[^219]: 对复杂动态系统中的成对交互进行统一

    Unifying Pairwise Interactions in Complex Dynamics. (arXiv:2201.11941v2 [physics.data-an] UPDATED)

    [http://arxiv.org/abs/2201.11941](http://arxiv.org/abs/2201.11941)

    该论文提出了一个关于统一复杂系统中成对交互的统计库，并在多个实际案例中展示了如何同时利用不同的方法来揭示最适合解决一个问题的方法。

    

    科学家们已开发出数百种技术来衡量复杂系统中进程对之间的交互。但这些计算方法，从相关系数到因果推断，依赖于不同的定量理论，这些理论仍然大部分没有联系。在这里，我们介绍了237种成对交互统计量库，并评估它们在来自广泛实世界和模型生成系统的1053个多元时间序列上的行为。我们的分析突出不同数学公式之间的新共性，提供了一个丰富的跨学科文献的统一图景。接着，我们利用三个实际案例研究展示同时利用来自科学的多种方法可以揭示出最适合解决给定问题的方法，从而产生解释性的理解关系对的概念规定，推动成功表现。我们的框架提供了可扩展开放软件，使共同研究变得容易。

    Scientists have developed hundreds of techniques to measure the interactions between pairs of processes in complex systems. But these computational methods, from correlation coefficients to causal inference, rely on distinct quantitative theories that remain largely disconnected. Here we introduce a library of 237 statistics of pairwise interactions and assess their behavior on 1053 multivariate time series from a wide range of real-world and model-generated systems. Our analysis highlights new commonalities between different mathematical formulations, providing a unified picture of a rich interdisciplinary literature. Using three real-world case studies, we then show that simultaneously leveraging diverse methods from across science can uncover those most suitable for addressing a given problem, yielding interpretable understanding of the conceptual formulations of pairwise dependence that drive successful performance. Our framework is provided in extendable open software, enabling co
    
[^220]: DegreEmbed: 将实体嵌入融入逻辑规则学习，用于知识图谱推理

    DegreEmbed: incorporating entity embedding into logic rule learning for knowledge graph reasoning. (arXiv:2112.09933v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.09933](http://arxiv.org/abs/2112.09933)

    本文介绍了一种将实体嵌入和逻辑规则挖掘相结合的新模型DegreEmbed，用于知识图谱推理。实验结果表明，DegreEmbed在链接预测和规则提取方面优于现有方法。

    

    知识图谱是一种结构化的现实世界事实表示，它们是智能数据库，将人类知识融入其中，帮助机器模仿人类的问题解决方式。然而，实际的知识图谱通常非常庞大，而且不可避免地存在缺失的事实，从而损害了基于知识图谱推理的问答和推荐系统等应用。知识图谱中的链接预测是通过基于现有知识推理来完成缺失事实的任务。主要有两种研究流派：一种学习实体和关系的低维度嵌入，可以探索潜在模式；另一种通过挖掘逻辑规则获得良好的可解释性。不幸的是，涉及各种类型的实体和关系的现代知识图谱的异质性在先前的研究中没有得到很好的考虑。本文提出了DegreEmbed，一种模型，它将基于嵌入的学习和逻辑规则挖掘相结合，用于知识图谱推理。DegreEmbed将实体嵌入融入逻辑规则学习中，使模型能够同时处理实体属性和关系，并解决知识图谱的异质性问题。基准数据集上的实验结果表明，DegreEmbed在链接预测和规则提取方面优于现有方法。

    Knowledge graphs (KGs), as structured representations of real world facts, are intelligent databases incorporating human knowledge that can help machine imitate the way of human problem solving. However, KGs are usually huge and there are inevitably missing facts in KGs, thus undermining applications such as question answering and recommender systems that are based on knowledge graph reasoning. Link prediction for knowledge graphs is the task aiming to complete missing facts by reasoning based on the existing knowledge. Two main streams of research are widely studied: one learns low-dimensional embeddings for entities and relations that can explore latent patterns, and the other gains good interpretability by mining logical rules. Unfortunately, the heterogeneity of modern KGs that involve entities and relations of various types is not well considered in the previous studies. In this paper, we propose DegreEmbed, a model that combines embedding-based learning and logic rule mining for 
    
[^221]: 神经符号学习系统综述

    A Survey on Neural-symbolic Learning Systems. (arXiv:2111.08164v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08164](http://arxiv.org/abs/2111.08164)

    本论文调查了神经符号学习系统的进展，挑战、方法、应用和未来方向四个方面。神经符号学习系统将神经系统的学习能力和符号系统的认知能力相结合，具有强大的感知和认知能力。

    

    近年来，神经系统展现出了高度有效的学习能力和优越的知觉智能，然而它们缺乏有效的推理和认知能力。另一方面，符号系统展现出了卓越的认知智能，但与神经系统相比，它们在学习能力方面表现较差。鉴于两种方法的优缺点，理想的解决方案是将神经系统和符号系统相结合，创建神经符号学习系统，具有强大的感知和认知能力。本文旨在从四个不同的角度（挑战、方法、应用和未来方向）调查神经符号学习系统的进展。通过这样做，本研究旨在推动这一新兴领域向前发展，为研究人员提供全面和整体的概述。这个概述不仅将突出目前的最新技术，还将确定有前途的方向。

    In recent years, neural systems have demonstrated highly effective learning ability and superior perception intelligence. However, they have been found to lack effective reasoning and cognitive ability. On the other hand, symbolic systems exhibit exceptional cognitive intelligence but suffer from poor learning capabilities when compared to neural systems. Recognizing the advantages and disadvantages of both methodologies, an ideal solution emerges: combining neural systems and symbolic systems to create neural-symbolic learning systems that possess powerful perception and cognition. The purpose of this paper is to survey the advancements in neural-symbolic learning systems from four distinct perspectives: challenges, methods, applications, and future directions. By doing so, this research aims to propel this emerging field forward, offering researchers a comprehensive and holistic overview. This overview will not only highlight the current state-of-the-art but also identify promising a
    
[^222]: OpenFWI：用于地震全波形反演的大规模多结构基准数据集

    OpenFWI: Large-Scale Multi-Structural Benchmark Datasets for Seismic Full Waveform Inversion. (arXiv:2111.02926v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.02926](http://arxiv.org/abs/2111.02926)

    OpenFWI是一个用于地震全波形反演的大规模多结构基准数据集，包含多个领域、不同地质地下结构和各种数据样本数量。使用OpenFWI进行了四种深度学习方法的基准测试和物理驱动的实验。

    

    全波形反演 （FWI）是地球物理学中广泛用于从地震数据中重建高分辨率速度图的方法。数据驱动的FWI方法的成功导致了对开放数据集在地球物理学界的需求快速增长。我们提出了OpenFWI，一个大规模多结构基准数据集的集合，以促进FWI的多样化、严格和可重现的研究。OpenFWI由12个数据集（总共2.1TB）组成，这些数据集是从多个来源合成的，涵盖地球物理学中不同的领域（界面、断层、CO2储层等），覆盖不同的地质地下结构（平坦的、曲线的等），包含各种数量的数据样本（2K-67K）。它还包括一个用于3D FWI的数据集。此外，我们使用OpenFWI对四种深度学习方法进行基准测试，涵盖有监督和无监督的学习方式。除了基准测试外，我们还进行了物理化驱动的实验。

    Full waveform inversion (FWI) is widely used in geophysics to reconstruct high-resolution velocity maps from seismic data. The recent success of data-driven FWI methods results in a rapidly increasing demand for open datasets to serve the geophysics community. We present OpenFWI, a collection of large-scale multi-structural benchmark datasets, to facilitate diversified, rigorous, and reproducible research on FWI. In particular, OpenFWI consists of 12 datasets (2.1TB in total) synthesized from multiple sources. It encompasses diverse domains in geophysics (interface, fault, CO2 reservoir, etc.), covers different geological subsurface structures (flat, curve, etc.), and contains various amounts of data samples (2K - 67K). It also includes a dataset for 3D FWI. Moreover, we use OpenFWI to perform benchmarking over four deep learning methods, covering both supervised and unsupervised learning regimes. Along with the benchmarks, we implement additional experiments, including physics-driven 
    
[^223]: GNN在节点分类/回归任务中的逼近能力研究

    On the approximation capability of GNNs in node classification/regression tasks. (arXiv:2106.08992v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.08992](http://arxiv.org/abs/2106.08992)

    本文研究GNN在节点分类/回归任务中的逼近能力，发现在概率上GNN对于任何有限域上的可测函数都是通用逼近器。我们的研究还揭示了GNN的表达能力与其深度和宽度的变化有关，并且结果可扩展用于大型图。

    

    图神经网络(GNNs)是一种广泛的用于图处理的连接式模型。最近的研究表明，GNN可以逐步逼近关于图的任意函数，这取决于由Weisfeiler--Lehman(WL)测试定义的图等价关系。然而，这些结果存在一些局限性，一方面因为它们是用Stone-Weierstrass定理来推导得出的，这种方法本质上只是一种存在性证明，另一方面因为它们假设目标函数是连续的。此外，所有当前的结果都是专门针对图分类/回归任务的，而节点分类/回归问题也非常常见。在本文中，我们提出了一种新的方法来证明GNN的逼近能力，克服了这些局限性。我们发现，对于节点分类/回归任务，无论目标函数是连续的还是离散的，GNN在概率上都是通用逼近器。具体而言，我们发现，在一些温和条件下，任何有限域上的可测函数都可以通过一个只有一层隐藏层、ReLU激活函数和共享所有节点的边权值的GNN进行逼近。我们的分析也揭示了GNN的表达能力随其深度和宽度的变化及其结果针对大型图的可扩展性。

    Graph Neural Networks (GNNs) are a broad class of connectionist models for graph processing. Recent studies have shown that GNNs can approximate any function on graphs, modulo the equivalence relation on graphs defined by the Weisfeiler--Lehman (WL) test. However, these results suffer from some limitations, both because they were derived using the Stone--Weierstrass theorem -- which is existential in nature, -- and because they assume that the target function to be approximated must be continuous. Furthermore, all current results are dedicated to graph classification/regression tasks, where the GNN must produce a single output for the whole graph, while also node classification/regression problems, in which an output is returned for each node, are very common. In this paper, we propose an alternative way to demonstrate the approximation capability of GNNs that overcomes these limitations. Indeed, we show that GNNs are universal approximators in probability for node classification/regre
    
[^224]: 使用双因素扰动评估深度学习分类器的鲁棒性

    Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.03102](http://arxiv.org/abs/2103.03102)

    本文提出了一种新的基准测试方法和工具，通过双因素扰动来评估深度学习分类器的鲁棒性。使用该方法和工具，作者比较了不同的两因素扰动条件下DL分类器的鲁棒性，并为开发更鲁棒的DL分类器提供了见解。

    

    本文旨在对深度学习（DL）分类器的鲁棒性进行基准测试。我们创新地提出了一种新的基准测试方法来评估DL分类器的鲁棒性，并引入了一种新的四象限统计可视化工具，包括最小准确性、最大准确性、平均准确性和变异系数，用于评估DL分类器的鲁棒性。我们创建了一个全面的69个基准测试图像集，包括一个干净的集合、单因素扰动的集合和双因素扰动条件的集合，以衡量鲁棒的DL分类器。通过收集实验结果，我们首先报告使用双因素扰动图像可以提高DL分类器的鲁棒性和准确性。双因素扰动包括（1）在两个序列中都应用的两个数字扰动（椒盐噪声和高斯噪声），以及（2）一个数字扰动（椒盐噪声）和一个几何扰动（旋转）。除此之外，通过我们的基准测试工具，我们发现不同的DL模型对两因素扰动有不同的鲁棒性行为，暴露了分类器行为的不均匀性和基准测试的必要性。使用我们的基准测试方法和工具，我们比较了不同的两因素扰动条件下DL分类器的鲁棒性，并为开发更鲁棒的DL分类器提供了见解。

    This paper adds to the fundamental body of work on benchmarking the robustness of deep learning (DL) classifiers. We innovate a new benchmarking methodology to evaluate robustness of DL classifiers. Also, we introduce a new four-quadrant statistical visualization tool, including minimum accuracy, maximum accuracy, mean accuracy, and coefficient of variation, for benchmarking robustness of DL classifiers. To measure robust DL classifiers, we created a comprehensive 69 benchmarking image set, including a clean set, sets with single factor perturbations, and sets with two-factor perturbation conditions. After collecting experimental results, we first report that using two-factor perturbed images improves both robustness and accuracy of DL classifiers. The two-factor perturbation includes (1) two digital perturbations (salt & pepper noise and Gaussian noise) applied in both sequences, and (2) one digital perturbation (salt & pepper noise) and a geometric perturbation (rotation) applied in 
    
[^225]: 网络内数据增强方法

    Augmentation Inside the Network. (arXiv:2012.10769v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2012.10769](http://arxiv.org/abs/2012.10769)

    本文提出了一种将数据增强方法引入卷积神经网络中间层的方法，既使速度-准确性的平衡更平滑，同时在ImageNet和CIFAR-100数据集分类任务上实现更好的结果，特别是在CIFAR-100数据集上的结果可以比标准测试时间增强技术更快30％。

    

    本文提出了一种“网络内数据增强”方法，它可以在卷积神经网络的中间层模拟数据增强技术，通过变换数据流，并在可能时共享计算。该方法允许我们获得更平滑的速度-准确率折衷调整，比使用标准测试时间增强（TTA）技术更好地实现更好的结果。此外，当与测试时间增强相结合时，我们的方法可以进一步提高模型性能。我们在ImageNet-2012和CIFAR-100数据集上进行了验证和实验，结果表明我们的方法比翻转测试时间增强更快30％，在CIFAR-100上达到了相同的结果。

    In this paper, we present augmentation inside the network, a method that simulates data augmentation techniques for computer vision problems on intermediate features of a convolutional neural network. We perform these transformations, changing the data flow through the network, and sharing common computations when it is possible. Our method allows us to obtain smoother speed-accuracy trade-off adjustment and achieves better results than using standard test-time augmentation (TTA) techniques. Additionally, our approach can improve model performance even further when coupled with test-time augmentation. We validate our method on the ImageNet-2012 and CIFAR-100 datasets for image classification. We propose a modification that is 30% faster than the flip test-time augmentation and achieves the same results for CIFAR-100.
    
[^226]: 无需物理知识：基于过程的无模型异常检测在工业控制系统中的鲁棒性

    No Need to Know Physics: Resilience of Process-based Model-free Anomaly Detection for Industrial Control Systems. (arXiv:2012.03586v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2012.03586](http://arxiv.org/abs/2012.03586)

    本文对基于过程的工业控制系统异常检测方案进行了分析，并介绍了验证这些检测系统的属性分类法。该研究还提出了一种新的通用框架来生成违反系统物理属性的对抗欺骗信号。在已发表的四个检测器中，只有一个具有鲁棒性，其鲁棒性来自于属性的引入。通过攻击，我们成功降低了攻击方案的召回率。

    

    近年来，已经提出了许多基于过程的工业控制系统异常检测方案。本研究对此类方案进行了系统分析，并介绍了验证这些检测系统的属性分类法。然后，我们提出了一个新的通用框架来生成违反系统物理属性的对抗欺骗信号，并使用该框架来分析在顶级安全会议上发表的四个异常检测器。我们发现其中三个检测器容易受到许多对抗性操作（例如，使用预先计算的模式进行欺骗），我们称之为合成传感器欺骗，只有一个检测器对我们的攻击具有鲁棒性。我们研究了其鲁棒性的根源，并证明了来自我们引入的属性。我们的攻击降低了攻击方案的召回率（真阳性率），使它们无法正确检测异常。因此，我们发现的漏洞

    In recent years, a number of process-based anomaly detection schemes for Industrial Control Systems were proposed. In this work, we provide the first systematic analysis of such schemes, and introduce a taxonomy of properties that are verified by those detection systems. We then present a novel general framework to generate adversarial spoofing signals that violate physical properties of the system, and use the framework to analyze four anomaly detectors published at top security conferences. We find that three of those detectors are susceptible to a number of adversarial manipulations (e.g., spoofing with precomputed patterns), which we call Synthetic Sensor Spoofing and one is resilient against our attacks. We investigate the root of its resilience and demonstrate that it comes from the properties that we introduced. Our attacks reduce the Recall (True Positive Rate) of the attacked schemes making them not able to correctly detect anomalies. Thus, the vulnerabilities we discovered in
    
[^227]: 代表性集成在准线性复杂度下实现协同生命周期学习

    Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity. (arXiv:2004.12908v16 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2004.12908](http://arxiv.org/abs/2004.12908)

    本文提出了一种名为RELL的方法，利用知识蒸馏和知识保持正则化方法，以协同集成在不同任务上独立学习的表示，在准线性复杂度下实现了前向和后向传递。实验结果表明，在各种基准数据集上，RELL的表现优于现有的最先进方法，尤其是在存在灾难性遗忘的情况下，能够显着改善反向传递。

    

    在终身学习中，数据不仅可以用于改进当前任务的性能，还可以用于之前和尚未遇到的任务。传统的机器学习则从空白状态开始，仅针对单个任务使用数据。虽然传统迁移学习算法可以提高未来任务的性能，但在学习新任务后对旧任务的性能下降（称为遗忘）。近期针对连续或终身学习的许多方法都试图在给定新任务的情况下保持对旧任务的性能。但是，仅努力避免忘记将目标定得过低。终身学习的目标不仅应该是提高未来任务（前向传递）的性能，而且还应该是用任何新数据提高过去任务（反向传递）的性能。我们的关键见解是，我们可以协同集成分别在不同任务上独立学习的表示，以实现准线性复杂度下的前向和后向传递。本文提出了一种新方法，称为“终身学习中的表示集成（RELL）”，它集成了知识蒸馏和知识保持正则化方法，以利用不同表示中包含的互补信息。我们的实验表明，RELL在各种基准数据集上都优于现有最先进方法，尤其是在存在灾难性遗忘的情况下实现了显着更好的反向传递。

    In lifelong learning, data are used to improve performance not only on the current task, but also on previously encountered, and as yet unencountered tasks. In contrast, classical machine learning, which we define as, starts from a blank slate, or tabula rasa and uses data only for the single task at hand. While typical transfer learning algorithms can improve performance on future tasks, their performance on prior tasks degrades upon learning new tasks (called forgetting). Many recent approaches for continual or lifelong learning have attempted to maintain performance on old tasks given new tasks. But striving to avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning should be not only to improve performance on future tasks (forward transfer) but also on past tasks (backward transfer) with any new data. Our key insight is that we can synergistically ensemble representations -- that were learned independently on disparate tasks -- to enable both forward and bac
    
[^228]: 基于说话人感知的CRF用于对话行为分类

    Speaker-change Aware CRF for Dialogue Act Classification. (arXiv:2004.02913v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2004.02913](http://arxiv.org/abs/2004.02913)

    本文提出了一种新的CRF模型，称为Speaker-change Aware CRF，以考虑对话行为分类中的说话人变化，实验结果表明其优于先前的方法。

    

    最近在对话行为（DA）分类方面的研究将该任务作为一个序列标注问题，使用神经网络模型以及作为最后一层的条件随机场（CRF）。CRF模型根据输入话语序列来建模目标DA标签序列的条件概率。然而，该任务还涉及到另一个重要的输入序列，即说话人序列，而先前的工作在这方面没有做出考虑。为了解决这个限制，本文提出了一种简单的CRF层的修改，该层考虑到说话人变化。对SwDA语料库的实验表明，我们修改后的CRF层优于原始层，在某些DA标签上的差距非常大。此外，可视化展示了我们的CRF层可以以端到端的方式学习在说话人交替条件下DA标签对之间的有意义的、复杂的转移模式。代码已公开发布。

    Recent work in Dialogue Act (DA) classification approaches the task as a sequence labeling problem, using neural network models coupled with a Conditional Random Field (CRF) as the last layer. CRF models the conditional probability of the target DA label sequence given the input utterance sequence. However, the task involves another important input sequence, that of speakers, which is ignored by previous work. To address this limitation, this paper proposes a simple modification of the CRF layer that takes speaker-change into account. Experiments on the SwDA corpus show that our modified CRF layer outperforms the original one, with very wide margins for some DA labels. Further, visualizations demonstrate that our CRF layer can learn meaningful, sophisticated transition patterns between DA label pairs conditioned on speaker-change in an end-to-end way. Code is publicly available.
    

