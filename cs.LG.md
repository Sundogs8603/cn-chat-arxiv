# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism.](http://arxiv.org/abs/2401.02954) | DeepSeek LLM是一个致力于以长期视野推进开源语言模型发展的项目，通过研究和应用扩展规律，成功创建了DeepSeek Chat模型，该模型在各种基准测试中表现出色，特别是在代码领域。 |
| [^2] | [The Tactician's Web of Large-Scale Formal Knowledge.](http://arxiv.org/abs/2401.02950) | The Tactician's Web是一个大规模形式化数学知识网络，通过Coq证明助手和丰富的数据表示与证明工程师进行交互，并提供了机器学习、分析和证明工程的实用工具。 |
| [^3] | [Graph2Tac: Learning Hierarchical Representations of Math Concepts in Theorem proving.](http://arxiv.org/abs/2401.02949) | 本文提出了一种名为Graph2Tac的图神经网络模型，用于在定理证明中学习数学概念的分层表示。该模型能够动态地将新的数学概念纳入到知识库中，并在Coq证明助手中进行训练和应用。 |
| [^4] | [Digital-analog quantum learning on Rydberg atom arrays.](http://arxiv.org/abs/2401.02940) | 这项研究提出了在Rydberg原子阵列上进行数字-模拟量子学习的算法，并通过数值实验表明，在近期实现这一算法是可行的，并且相比于数字学习方案，数字-模拟学习需要更短的电路深度，并且对于现实误差模型更具鲁棒性。这使得数字-模拟学习成为中期改进变分量子学习实验的有前景的方法。 |
| [^5] | [Fast and Optimal Weight Update for Pruned Large Language Models.](http://arxiv.org/abs/2401.02938) | 本研究提出了一种基于交替方向乘积算法(ADMM)的快速且最优的修剪层权重更新算法，结合简单的迭代修剪掩码选择，在广泛的大型语言模型范围内实现了最先进的修剪性能。 |
| [^6] | [Dagma-DCE: Interpretable, Non-Parametric Differentiable Causal Discovery.](http://arxiv.org/abs/2401.02930) | Dagma-DCE是一种可解释的、非参数的可微因果发现方案，使用可解释的因果强度度量定义加权邻接矩阵，并在模拟数据集中达到最先进的性能水平。 |
| [^7] | [A unified uncertainty-aware exploration: Combining epistemic and aleatory uncertainty.](http://arxiv.org/abs/2401.02914) | 本文提出了一种统一的不确定性感知探索算法，将认知不确定性和偶然不确定性结合起来进行探索，通过量化两种不确定性的综合效应实现风险敏感的探索策略。 |
| [^8] | [H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses.](http://arxiv.org/abs/2401.02905) | H2G2-Net是一种用于发现多模态生理反应的分层异构图生成网络框架，能够自动学习图结构而不需要预定义的领域知识。 |
| [^9] | [Class-wise Generalization Error: an Information-Theoretic Analysis.](http://arxiv.org/abs/2401.02904) | 本文通过信息论分析研究了类别普适误差问题，并提出了使用KL散度的信息论界限和使用条件互信息(CMI)的更紧界限，能够准确捕捉复杂的类别普适误差。 |
| [^10] | [Deep Reinforcement Learning for Local Path Following of an Autonomous Formula SAE Vehicle.](http://arxiv.org/abs/2401.02903) | 本文介绍了使用深度强化学习和逆向增强学习来实现自主Formula SAE车辆的局部路径跟随。使用软actor评判和对抗性逆向增强学习算法进行模型训练，并提出了三种新颖奖励函数。仿真和真实环境测试表明，这两种算法都可以成功训练模型进行局部路径跟随。 |
| [^11] | [State Derivative Normalization for Continuous-Time Deep Neural Networks.](http://arxiv.org/abs/2401.02902) | 本文研究了在连续时间状态空间模型估计中，深度神经网络的数据标准化问题。通过引入状态导数级别的标准化常数，解决了隐藏状态、隐藏状态导数以及时间间隔的标准化挑战。选择适当的标准化常数与待识别系统的动力学相关，并提出了多种获得有效标准化常数的方法。 |
| [^12] | [Nonlinear functional regression by functional deep neural network with kernel embedding.](http://arxiv.org/abs/2401.02890) | 本文提出了一种函数深度神经网络用于非线性函数回归的方法，通过平滑核积分变换和数据相关的维度缩减方法，取得了良好的预测效果。 |
| [^13] | [Energy-Preserving Reduced Operator Inference for Efficient Design and Control.](http://arxiv.org/abs/2401.02889) | 本文提出了一种能够节约能量的减少操作推断方法，用于高效设计和控制中的多次计算任务，特别适用于保持能量的偏微分方程控制系统。 |
| [^14] | [Efficient Parameter Optimisation for Quantum Kernel Alignment: A Sub-sampling Approach in Variational Training.](http://arxiv.org/abs/2401.02879) | 本文提出了一种高效的量子核对齐方法，通过使用子采样训练的方式在减少计算成本的同时保持分类准确度，以解决量子核对齐的训练代价大的问题。 |
| [^15] | [Framework for Variable-lag Motif Following Relation Inference In Time Series using Matrix Profile analysis.](http://arxiv.org/abs/2401.02860) | 该论文提出了一个利用矩阵分析方法的框架，用于推理时间序列中的跟随模式。在模拟数据集和声音记录数据集中，该框架优于基准方法，并能够检测出加密货币数据集中的跟随模式。 |
| [^16] | [Generating Non-Stationary Textures using Self-Rectification.](http://arxiv.org/abs/2401.02847) | 本文提出了一种使用自校正来生成非平稳纹理的方法，通过使用预训练扩散网络和自注意机制，可以将用户修改的参考纹理细化为一种连贯、无缝的纹理，并保留参考样本的独特视觉特征。实验证实表明，该方法在处理非平稳纹理方面具有卓越的能力，相比现有技术在纹理合成方面取得了显著的进展。 |
| [^17] | [Thousands of AI Authors on the Future of AI.](http://arxiv.org/abs/2401.02843) | 数千位AI作者对未来AI的预测显示，到2028年，AI系统有50%的几率实现多个里程碑，包括自主构建全新的付款处理网站、创作一首与知名音乐家的新歌难以区分的歌曲，并自主下载和调整大型语言模型。同时，无需辅助的机器在各种任务上胜过人类的几率估计为10%到2047年为50%。 |
| [^18] | [Let's Get It Started: Fostering the Discoverability of New Releases on Deezer.](http://arxiv.org/abs/2401.02827) | 本文介绍了在Deezer音乐服务上促进新发布内容发现能力的最新举措，包括个性化推荐、冷启动嵌入和情境强化学习算法等。通过在线实验的支持，我们展示了这些举措在提高推荐质量和新发布内容曝光方面的优势。 |
| [^19] | [Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning.](http://arxiv.org/abs/2401.02810) | 本论文提出使用迁移学习来提高物理信息神经网络（PINN）训练的鲁棒性和收敛性。经过两个案例研究，发现迁移学习可以有效地训练PINN来近似解决方案，从低频率问题到高频率问题，而不增加网络参数，且需要更少的数据点和更短的训练时间。 |
| [^20] | [Credence: Augmenting Datacenter Switch Buffer Sharing with ML Predictions.](http://arxiv.org/abs/2401.02801) | Credence是一种使用机器学习预测增强的丢弃式缓冲区共享算法，可以显著提高数据中心交换机的性能。 |
| [^21] | [Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery Videos.](http://arxiv.org/abs/2401.02791) | 本研究提出了一种在微创手术视频中弱半监督下检测手术工具的方法。通过使用共现损失来利用图像级标签中工具对之间的共现关系，平衡了注释负担和检测性能，克服了分类困难。 |
| [^22] | [Tackling Electrode Shift In Gesture Recognition with HD-EMG Electrode Subsets.](http://arxiv.org/abs/2401.02773) | 本研究提出了一种解决姿势识别中电极漂移问题的方法，通过使用HD-EMG电极子集，并增加来自不同电极位置的数据，同时提高了稳健性和性能。 |
| [^23] | [Powerformer: A Section-adaptive Transformer for Power Flow Adjustment.](http://arxiv.org/abs/2401.02771) | Powerformer是一种适应不同传输区段的变压器架构，用于学习稳健电力系统状态表示。它通过开发专用的区段自适应注意机制，并引入图神经网络传播和多因素注意机制来提供更加稳健的状态表示。在三个不同的电力系统场景上进行了广泛评估。 |
| [^24] | [Fairness-Aware Job Scheduling for Multi-Job Federated Learning.](http://arxiv.org/abs/2401.02740) | 本文提出了一种公平感知联邦作业调度（FairFedJS）方法，以确保将高需求的FL客户端数据集公平分配给需要它们的FL作业，并在实验证明其优势。 |
| [^25] | [Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors.](http://arxiv.org/abs/2401.02739) | 本文提出了去噪扩散变分推断（DDVI）算法，该算法使用扩散模型作为表达性变分后验，并通过反转加噪过程在潜空间中进行扩散。该方法易于实现，兼容黑盒变分推断，并在深度潜变量模型中的任务中表现优异。 |
| [^26] | [On the numerical reliability of nonsmooth autodiff: a MaxPool case study.](http://arxiv.org/abs/2401.02736) | 本文研究了涉及非平滑MaxPool操作的神经网络自动微分的数值可靠性，并发现最近的研究表明AD几乎在每个地方都与导数相符，即使在存在非平滑操作的情况下也是如此。但在实践中，AD使用的是浮点数，需要探索可能导致AD数值不正确的情况。通过研究不同选择的非平滑MaxPool雅可比矩阵对训练过程的影响，我们找到了分歧区和补偿区两个可能导致AD数值不正确的子集。 |
| [^27] | [Shared active subspace for multivariate vector-valued functions.](http://arxiv.org/abs/2401.02735) | 本文提出了一种共享主动子空间的方法来处理多元向量值函数，可以通过操纵梯度或计算对称正定矩阵实现。实验结果表明，SPD级别的方法比梯度级别的方法更好，并且在正态分布情况下表现接近向量值方法。 |
| [^28] | [FedNS: A Fast Sketching Newton-Type Algorithm for Federated Learning.](http://arxiv.org/abs/2401.02734) | 本文提出了一种名为FedNS的快速绘图牛顿型算法，解决了牛顿型联邦学习算法在通信复杂度上的问题，通过传输草图化的平方根海森矩阵来近似集中式牛顿方法，实现了超线性的收敛速率。 |
| [^29] | [Predicting Traffic Flow with Federated Learning and Graph Neural with Asynchronous Computations Network.](http://arxiv.org/abs/2401.02723) | 本文提出了一种名为FLAGCN的深度学习方法，通过将异步图卷积网络和联邦学习相结合来提高实时交通流量预测的准确性和效率。 |
| [^30] | [A Cost-Efficient FPGA Implementation of Tiny Transformer Model using Neural ODE.](http://arxiv.org/abs/2401.02721) | 本文提出了一种利用神经ODE作为骨干架构的高性价比FPGA实现微型Transformer模型。该模型相比于基于CNN的模型将参数大小减少了94.6%且保持准确性，适用于边缘计算。 |
| [^31] | [Calibration Attack: A Framework For Adversarial Attacks Targeting Calibration.](http://arxiv.org/abs/2401.02718) | 校准攻击是一种新的对抗攻击框架，通过生成和组织攻击来使受害模型失去准确校准，而不影响其原始准确性。这对模型的可信度和基于置信分数的决策构成严重威胁。我们提出了四种校准攻击形式，并对常用的对抗防御和校准方法的有效性进行了研究。 |
| [^32] | [Graph-level Protein Representation Learning by Structure Knowledge Refinement.](http://arxiv.org/abs/2401.02713) | 本文提出了一种名为结构知识提炼（SKR）的新框架，通过对比学习解决了图级表示学习中存在的问题，并提出了一种自适应的数据增强策略。 |
| [^33] | [TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis.](http://arxiv.org/abs/2401.02708) | 本文提出了一种适应时间的三元组坐标损失函数TripleSurv，通过引入样本对之间的生存时间差异来鼓励模型量化排名相对风险，从而提高生存分析的准确性。 |
| [^34] | [PAHD: Perception-Action based Human Decision Making using Explainable Graph Neural Networks on SAR Images.](http://arxiv.org/abs/2401.02687) | 本论文研究了在SAR图像上利用可解释图神经网络进行基于感知动作的人类决策，旨在提供详细信息帮助指挥官做出准确决策。 |
| [^35] | [Beyond Fidelity: Explaining Vulnerability Localization of Learning-based Detectors.](http://arxiv.org/abs/2401.02686) | 这项研究评估了基于图和序列表示的十种漏洞检测器解释方法的性能，发现单纯的忠诚度评估不足够。 |
| [^36] | [Geometric-Facilitated Denoising Diffusion Model for 3D Molecule Generation.](http://arxiv.org/abs/2401.02683) | 这项研究提出了一个几何便利的去噪扩散模型，用于解决3D分子生成中的多体原子关系建模和键的预测问题。 |
| [^37] | [Homophily-Related: Adaptive Hybrid Graph Filter for Multi-View Graph Clustering.](http://arxiv.org/abs/2401.02682) | 该论文提出了一个自适应混合图过滤器用于多视图图聚类。现有方法仅适用于同质性图，而该研究针对广泛存在的异质性图提出了一个解决方案。通过与图的同质性程度密切相关的图过滤，该方法可以充分利用低频和高频信息来学习节点表示。 |
| [^38] | [LMaaS: Exploring Pricing Strategy of Large Model as a Service for Communication.](http://arxiv.org/abs/2401.02675) | 本文提出了Large Model as a Service (LMaaS)作为智能通信中的定价模式，并通过Stackelberg博弈解决了定价优化问题。 |
| [^39] | [Towards Integrated Fine-tuning and Inference when Generative AI meets Edge Intelligence.](http://arxiv.org/abs/2401.02668) | 本文提出了GAI-oriented synthetical network (GaisNet)，这是一个协作的云端边缘智能框架，通过利用无数据知识中继来缓解矛盾，实现了GAI的循环模型微调和任务推理，从而实现了GAI和EI的互利关系。 |
| [^40] | [Zero-shot Microclimate Prediction with Deep Learning.](http://arxiv.org/abs/2401.02665) | 该论文提出了一种零样本学习方法，利用从其他地理位置提取的知识，实现了对新的和未监测到的地点的各种气候测量的预测。 |
| [^41] | [A backdoor attack against link prediction tasks with graph neural networks.](http://arxiv.org/abs/2401.02663) | 本文研究了一种针对图神经网络链接预测任务的后门攻击方法，发现GNN模型容易受到后门攻击，提出了针对该任务的后门攻击方式。 |
| [^42] | [Nurse-in-the-Loop Artificial Intelligence for Precision Management of Type 2 Diabetes in a Clinical Trial Utilizing Transfer-Learned Predictive Digital Twin.](http://arxiv.org/abs/2401.02661) | 这项研究开发了一种护士参与型人工智能系统，利用转移学习的预测数字孪生体来实现针对2型糖尿病患者的精准管理。在临床试验中，该系统通过结合各种数据源的模式和护士的专业知识，提供个性化的治疗反馈，以改善患者的治疗效果。 |
| [^43] | [GTA: Guided Transfer of Spatial Attention from Object-Centric Representations.](http://arxiv.org/abs/2401.02656) | 本论文提出了一种名为GTA的正则化方法，它通过在源模型和目标模型之间的自注意力映射中引导空间注意力的转移，可以解决在ViT中转移表示容易过拟合和失去有价值特性的问题，实验证明该方法能够稳定提高准确性。 |
| [^44] | [A Deep Q-Learning based Smart Scheduling of EVs for Demand Response in Smart Grids.](http://arxiv.org/abs/2401.02653) | 本研究提出了一种基于深度强化学习的智能调度方法，用于解决电动车在智能电网中的需求响应问题。通过调度电动车的充放电活动，以与配电系统操作员提供的目标能量配置文件一致，可以实现局部网络的平衡和优化。 |
| [^45] | [Adaptive Discounting of Training Time Attacks.](http://arxiv.org/abs/2401.02652) | 本研究展示了在存在环境动态和相对于受害者目标的非最优性时，即使目标行为无法被采纳，仍然可能进行C-TTA。我们开发了一种gammaDDPG算法来学习这种更强版本的C-TTA，并根据受害者当前的行为动态改变攻击策略规划时间。 |
| [^46] | [Improving sample efficiency of high dimensional Bayesian optimization with MCMC.](http://arxiv.org/abs/2401.02650) | 本文提出了一种基于马尔科夫链蒙特卡罗的方法，用于改进高维贝叶斯优化的样本效率。实验结果表明，该方法在高维顺序优化和强化学习任务中表现优于现有方法。 |
| [^47] | [Simple Hierarchical Planning with Diffusion.](http://arxiv.org/abs/2401.02644) | 本研究引入了分层扩散规划(Hierarchical Diffuser)，结合了分层和基于扩散的规划的优点，提出了一种简单、快速且有效的规划方法。通过在较高层面上采用“跳跃”规划策略，我们的模型能够具有较大的感受野且计算成本较低，同时跳跃的子目标还能指导低层规划器，在微调阶段进一步提高方法的效果。在实验评估中，我们的方法在性能和效率方面表现出卓越的结果。 |
| [^48] | [Model-Agnostic Interpretation Framework in Machine Learning: A Comparative Study in NBA Sports.](http://arxiv.org/abs/2401.02630) | 本研究提出了一个创新的模型无关解释框架，通过模块化操作和融合多种解释技术实现对复杂模型的解释而不损害性能。 |
| [^49] | [Neural Causal Abstractions.](http://arxiv.org/abs/2401.02602) | 本文提出了一种新的神经因果抽象方法，通过聚类变量和其域，用于解决真实因果推断任务中的挑战，并通过神经因果模型实现了学习和应用。 |
| [^50] | [Guaranteed Nonconvex Factorization Approach for Tensor Train Recovery.](http://arxiv.org/abs/2401.02592) | 本研究提出了一种保证非凸分解方法用于张量列车恢复的新方法。该方法通过优化左正交TT格式来实现正交结构，并使用黎曼梯度下降算法来优化因子。我们证明了该方法具有局部线性收敛性，并且在满足受限等谱性质的条件下能够以线性速率收敛到真实张量。 |
| [^51] | [Synthetic Information towards Maximum Posterior Ratio for deep learning on Imbalanced Data.](http://arxiv.org/abs/2401.02591) | 本研究提出了一种通过生成合成数据来平衡类别不平衡数据的技术，优先平衡信息丰富区域，并通过优化类别后验比率来最大化在正确的类别区域生成合成样本的概率。实验结果表明该技术在提升深度学习模型方面具有卓越性能。 |
| [^52] | [Federated Learning for distribution skewed data using sample weights.](http://arxiv.org/abs/2401.02586) | 本论文研究了在分布偏斜数据情况下如何通过使用样本权重来改进联邦学习性能。主要思路是通过调整客户端分布使其更接近全局分布，从而实现机器学习模型更快地收敛和更高的准确性。 |
| [^53] | [t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual Learning in Decision Making.](http://arxiv.org/abs/2401.02576) | t-DGR是一种用于决策制定中持续学习的基于轨迹的深度生成回放方法，通过生成任务样本来解决灾难性遗忘问题，并在连续世界基准测试中取得了最先进的性能。 |
| [^54] | [Large Language Models for Social Networks: Applications, Challenges, and Solutions.](http://arxiv.org/abs/2401.02575) | 本研究调查了如何在在线社交网络中开发大型语言模型（LLMs）应用。我们将LLM应用分为知识任务、娱乐任务和基础任务，并分享了挑战、解决方案和经验教训。 |
| [^55] | [Siamese Residual Neural Network for Musical Shape Evaluation in Piano Performance Assessment.](http://arxiv.org/abs/2401.02566) | 本论文提出了一种用于钢琴演奏评估中音乐形状评价的轻量级连体残余神经网络（S-ResNN）。实验结果表明，S-ResNN在精确度、召回率和F1得分方面显著优于多个基准方法。 |
| [^56] | [MeTA: Multi-source Test Time Adaptation.](http://arxiv.org/abs/2401.02561) | MeTA是第一个完全无监督的多源测试时适应框架，它可以在没有访问源数据的情况下，以最佳组合权重适应多个源模型到测试数据分布。 |
| [^57] | [Long-term Fairness For Real-time Decision Making: A Constrained Online Optimization Approach.](http://arxiv.org/abs/2401.02552) | 通过受限在线优化方法，确保机器学习驱动的决策系统在实时决策过程中实现长期公平性。 |
| [^58] | [Hyperparameter Estimation for Sparse Bayesian Learning Models.](http://arxiv.org/abs/2401.02544) | 该论文介绍了一个综合的SBL模型超参数估计框架，其中包括了期望最大化、MacKay和凸边界等算法。此外，还引入了一种新的算法，通过交替最小化和二次逼近范式，实现了更高的效率。 |
| [^59] | [Novel End-to-End Production-Ready Machine Learning Flow for Nanolithography Modeling and Correction.](http://arxiv.org/abs/2401.02536) | 本研究通过分析阻碍机器学习计算光刻成为生产就绪的原因，提出了一种新颖的高度可扩展的端到端流程，实现了生产就绪的机器学习-RET修正。 |
| [^60] | [Branched Variational Autoencoder Classifiers.](http://arxiv.org/abs/2401.02526) | 这篇论文介绍了一种改进的变分自动编码器（VAEs），通过增加一个额外的神经网络分支，将分类信息融入到潜在表示中，从而提高了分类准确性。 |
| [^61] | [Comprehensive Exploration of Synthetic Data Generation: A Survey.](http://arxiv.org/abs/2401.02524) | 本文调查了过去十年中417个合成数据生成模型，提供了对模型类型、功能和改进的全面概述。结果表明，模型性能和复杂性增加，以神经网络为主要方法，计算机视觉领域占据主导地位。 |
| [^62] | [Image-based Deep Learning for Smart Digital Twins: a Review.](http://arxiv.org/abs/2401.02523) | 本论文综述了基于图像的智能数字孪生体 (SDTs) 的发展方法和挑战，重点讨论了通过持续同化图像数据来观察和学习系统行为的方法，以及设计和实现SDTs的深度学习 (DL) 模型所面临的挑战。提供了对未来发展方向和机遇的见解。 |
| [^63] | [Structured Matrix Learning under Arbitrary Entrywise Dependence and Estimation of Markov Transition Kernel.](http://arxiv.org/abs/2401.02520) | 本论文提出了在任意元素间依赖下进行结构化矩阵估计的通用框架，并证明了提出的最小二乘估计器在各种噪声分布下的紧致性。此外，论文还提出了一个新颖的结果，论述了无关低秩矩阵的结构特点。最后，论文还展示了该框架在结构化马尔可夫转移核估计问题中的应用。 |
| [^64] | [Gain Scheduling with a Neural Operator for a Transport PDE with Nonlinear Recirculation.](http://arxiv.org/abs/2401.02511) | 本文介绍了使用神经算子进行增益调度的方法来处理具有非线性循环的输运PDE，并在局部实现了稳定。 |
| [^65] | [Towards an Adaptable and Generalizable Optimization Engine in Decision and Control: A Meta Reinforcement Learning Approach.](http://arxiv.org/abs/2401.02508) | 本文提出了一种基于元强化学习的优化器，可以不需要专家示范并在非稳态环境下实现快速适应，用于更新模型预测控制器。 |
| [^66] | [The cell signaling structure function.](http://arxiv.org/abs/2401.02501) | 该论文提出了一个新的方法，在活体细胞显微镜捕捉到的五维视频中寻找细胞信号动力学时空模式，并且不需要任何先验的预期模式动力学和训练数据。该方法基于细胞信号结构函数（SSF），通过测量细胞信号状态和周围细胞质之间的核糖体强度，与当前最先进的核糖体与细胞核比值相比有了显著改进。通过归一化压缩距离（NCD）来识别相似的模式。该方法能够将输入的SSF构图表示为低维嵌入中的点，最优地捕捉模式。 |
| [^67] | [Interpretable Time Series Models for Wastewater Modeling in Combined Sewer Overflows.](http://arxiv.org/abs/2401.02465) | 这篇论文讨论了气候变化对污水管理带来的复杂挑战，提出了一种基于可解释时间序列模型的废水建模方法，可以帮助预测关键水位点并改善废水管理和环境污染预防。 |
| [^68] | [Data-Centric Foundation Models in Computational Healthcare: A Survey.](http://arxiv.org/abs/2401.02458) | 计算医疗中的数据中心基础模型是一项调查研究，为医疗工作流程的改进提供了基于数据的人工智能方法，并讨论了安全性、评估和与人类价值观的一致性。基于FM的分析有望提高患者结果和临床工作流程表现。 |
| [^69] | [eCIL-MU: Embedding based Class Incremental Learning and Machine Unlearning.](http://arxiv.org/abs/2401.02457) | eCIL-MU是一种基于嵌入技术的逐类增量学习和机器取消学习的非破坏性框架，可用于在动态环境中快速获取关于新类别的知识，并消除先前学习类别的影响。 |
| [^70] | [A comprehensive survey of research towards AI-enabled unmanned aerial systems in pre-, active-, and post-wildfire management.](http://arxiv.org/abs/2401.02456) | 本研究综述探讨了AI支持的无人机系统在火灾管理中的应用，从火灾前到主动火灾阶段再到火灾后，通过整合无人机和深度学习模型，提供了更有效的野火管理解决方案。 |
| [^71] | [Adaptive Differential Privacy in Federated Learning: A Priority-Based Approach.](http://arxiv.org/abs/2401.02453) | 这项研究提出了一种在联邦学习中使用自适应差分隐私的方法，通过根据特征的相对重要性来决定注入的噪声值，以平衡隐私和模型性能。 |
| [^72] | [Automation of Smart Homes with Multiple Rule Sources.](http://arxiv.org/abs/2401.02451) | 这篇论文介绍了多规则来源的智能家居自动化的挑战和解决方案，包括管理规则的过程和机构、高级决策与实现细节的分离以及对系统结构和安全架构的影响。 |
| [^73] | [Locally Differentially Private Embedding Models in Distributed Fraud Prevention Systems.](http://arxiv.org/abs/2401.02450) | 本文提出了一个基于本地差分隐私的协作深度学习框架，用于在分布式欺诈预防系统中构建安全的数据发布机制，以支持外部欺诈检测模型。在实验中展示了其对多种攻击方法的鲁棒性。 |
| [^74] | [User authentication system based on human exhaled breath physics.](http://arxiv.org/abs/2401.02447) | 本研究尝试构建一个基于呼出气体物理的用户认证系统，通过对呼出气体的湍流结构进行分析，可以实现用户确认和用户识别。实验结果表明，用户确认算法表现出色。 |
| [^75] | [Sensor Placement for Learning in Flow Networks.](http://arxiv.org/abs/2401.02438) | 本论文研究了网络中的传感器布置问题，通过将传感器仅放置在关键链路上，利用机器学习算法推断整个网络中缺失的测量，从而实现了更准确的推断。 |
| [^76] | [Randomly Weighted Neuromodulation in Neural Networks Facilitates Learning of Manifolds Common Across Tasks.](http://arxiv.org/abs/2401.02437) | 本文研究了在神经网络中采用随机加权神经调节的方法，有助于学习跨任务公共流形。通过定义“任务特定的几何敏感哈希”，利用类似于大脑模型的神经调节系统，实现了这一方法。 |
| [^77] | [FedDiff: Diffusion Model Driven Federated Learning for Multi-Modal and Multi-Clients.](http://arxiv.org/abs/2401.02433) | FedDiff是一个多模态协作扩散联邦学习框架，旨在实现来自不同客户的异构数据的安全融合，通过建立双分支扩散模型特征提取设置来驱动联邦学习过程。 |
| [^78] | [Automated Classification of Model Errors on ImageNet.](http://arxiv.org/abs/2401.02430) | 该论文提出了自动错误分类框架来解决ImageNet数据集中的标签噪声和错误问题，为研究模型选择如何影响错误分布提供了有价值的工具。 |
| [^79] | [Brain-Inspired Spiking Neural Networks for Industrial Fault Diagnosis: A Survey, Challenges, and Opportunities.](http://arxiv.org/abs/2401.02429) | 基于脑启发的脉冲神经网络是一种有前景的应用于工业故障诊断的替代方法，可以克服人工神经网络的限制，提供更精确和有效的故障识别。 |
| [^80] | [Mapping of Land Use and Land Cover (LULC) using EuroSAT and Transfer Learning.](http://arxiv.org/abs/2401.02424) | 该论文利用EuroSAT和迁移学习技术对土地利用和土地覆盖进行映射，通过使用颜色波段进行微调，取得了99.19%的精确度，有助于制定保护和城市规划政策。 |
| [^81] | [Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning.](http://arxiv.org/abs/2401.02329) | 本研究提出了一种异质联邦学习方法FedED，通过同时进行空类别蒸馏和逻辑抑制，解决了在联邦学习中尚未充分识别空类别的问题。 |
| [^82] | [Cadmium Zinc Telluride (CZT) photon counting detector Characterisation for soft tissue imaging.](http://arxiv.org/abs/2401.02106) | 该研究表征了镉锌碲化物（CZT）光子计数探测器在识别各种组织方面的性能，推动了使用光子计数探测技术来克服传统CT探测器的限制。 |
| [^83] | [AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets.](http://arxiv.org/abs/2401.01916) | 通过有针对性和持续的预训练，我们在天文学问题回答中扩展了AstroLLaMA，通过使用紧凑的LLaMA-2模型和专门的天文学语料库，我们实现了在专门主题理解方面的显著改进。我们还通过对特定领域的对话数据集进行微调，发布了带有聊天功能的AstroLLaMA。 |
| [^84] | [Tissue Artifact Segmentation and Severity Analysis for Automated Diagnosis Using Whole Slide Images.](http://arxiv.org/abs/2401.01386) | 这篇论文提出了一种使用全幻灯切片图像进行组织伪影分割与严重性分析的自动诊断方法。通过计算机视觉和人工智能，可以在没有人类监督的情况下对整个全幻灯切片图像进行自主分析，但受到组织伪影影响的区域需要被准确识别和排除。 |
| [^85] | [PAC-Bayes-Chernoff bounds for unbounded losses.](http://arxiv.org/abs/2401.01148) | 这篇论文提出了一种用于无界损失的高概率PAC-Bayes参考界限，并通过优化自由参数解决了一些开放问题，并通过灵活的假设产生了新的广义界限。 |
| [^86] | [Scalable manifold learning by uniform landmark sampling and constrained locally linear embedding.](http://arxiv.org/abs/2401.01100) | 通过均匀地标抽样和约束局部线性嵌入，提出了一种可伸缩的流形学习方法，可以有效处理大规模和高维数据，并解决全局结构失真和可伸缩性问题。 |
| [^87] | [Tensor Networks for Explainable Machine Learning in Cybersecurity.](http://arxiv.org/abs/2401.00867) | 张量网络可以帮助发展可解释的机器学习算法，并提供丰富的模型可解释性。在网络安全中，我们的无监督聚类算法基于矩阵乘积状态，在性能上与传统的深度学习模型相媲美。我们的方法还能提取特征概率、熵和互信息，提供了分类异常的引人入胜的叙述，并实现了前所未有的透明度和可解释性水平。 |
| [^88] | [Unicron: Economizing Self-Healing LLM Training at Scale.](http://arxiv.org/abs/2401.00134) | Unicron是一个针对大规模语言模型训练中的自愈设计的工作负载管理器，通过最小化故障相关成本来优化训练过程，拥有带内错误检测、动态成本感知的计划生成机制和高效的转换策略。 |
| [^89] | [Self-supervised Pretraining for Decision Foundation Model: Formulation, Pipeline and Challenges.](http://arxiv.org/abs/2401.00031) | 本文认为将大规模自监督预训练中的知识与决策问题相结合，可以解决决策中的样本效率和泛化性问题。通过提出先预训练再自适应的流程，并调研了决策预训练和下游推理中的数据收集、预训练目标和自适应策略的最新研究。最后，确定了发展决策基础模型的关键挑战和未来方向。 |
| [^90] | [CycleGAN Models for MRI Image Translation.](http://arxiv.org/abs/2401.00023) | 该论文提出了一种CycleGAN模型，用于将MRI神经影像从一个场强转换到另一个场强。该模型能够以合理的准确性生成合成和重建图像。 |
| [^91] | [MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks.](http://arxiv.org/abs/2312.15960) | MoTCoder是一个使用思维模块提升大型语言模型在挑战性编程任务中能力的框架，通过创新的指令调整促进任务的分解和模块化，显著提高生成解决方案的准确性和模块化程度。 |
| [^92] | [Geo2SigMap: High-Fidelity RF Signal Mapping Using Geographic Databases.](http://arxiv.org/abs/2312.14303) | 本文提出了Geo2SigMap，一种基于地理数据库的高效和高保真度射频信号映射框架。通过无缝集成OpenStreetMap（地理数据库）、Blender（计算机图形）等开源工具，利用机器学习技术对射频信号传播进行建模，实现在“未知”区域进行射频信号映射。 |
| [^93] | [Underwater Acoustic Signal Recognition Based on Salient Feature.](http://arxiv.org/abs/2312.13143) | 这项研究提出了一种基于显著特征的水声信号识别方法，利用深度学习模型从频谱中提取特征，不断学习以分类水声信号，以应对复杂关系和提高识别准确率。 |
| [^94] | [LLM in a flash: Efficient Large Language Model Inference with Limited Memory.](http://arxiv.org/abs/2312.11514) | 本文提出了一种在有限内存条件下高效运行大型语言模型的方法，通过将模型参数存储在闪存中并按需传输到DRAM的方式来解决内存限制的挑战。该方法通过构建推理成本模型并优化数据传输和读取方式，引入了窗口化和行列绑定两种主要技术。 |
| [^95] | [Managing the unknown: a survey on Open Set Recognition and tangential areas.](http://arxiv.org/abs/2312.08785) | 这篇综述主要总结了开集识别的最新研究进展，包括常见做法、限制以及与其他机器学习研究领域的联系，并提出了未来的研究方向。 |
| [^96] | [PromptBench: A Unified Library for Evaluation of Large Language Models.](http://arxiv.org/abs/2312.07910) | PromptBench是一个用于评估大型语言模型的统一库，包括了提示语构建、提示语工程、数据集和模型加载、对抗性提示攻击、动态评估协议和分析工具等组件，旨在促进原创研究和创建新的基准测试、部署下游应用以及设计新的评估协议。 |
| [^97] | [AI Control: Improving Safety Despite Intentional Subversion.](http://arxiv.org/abs/2312.06942) | 本研究针对大型语言模型的安全问题，探索了一系列旨在确保安全性的技术流程，能够对抗模型有意破坏的情况，为解决编程问题提供了可靠的解决方案。 |
| [^98] | [KwaiAgents: Generalized Information-seeking Agent System with Large Language Models.](http://arxiv.org/abs/2312.04889) | 本文介绍了 KwaiAgents，这是一个基于大型语言模型的通用信息搜索智能体系统。该系统能够利用语言模型作为认知核心，理解用户的查询，行为准则并参考外部文档，以提供高质量的知识和信息。 |
| [^99] | [HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot Prompt Learning.](http://arxiv.org/abs/2312.01878) | 此论文提出了一种名为HGPROMPT的方法，用于连接同质和异质图，在少样本设置下进行提示学习，并通过预训练的同质和异质图来提高性能。 |
| [^100] | [Grounding Foundation Models through Federated Transfer Learning: A General Framework.](http://arxiv.org/abs/2311.17431) | 本论文提出了一个通用框架，通过联邦迁移学习将基础模型接地，以解决面临的挑战，如受限的计算资源、数据隐私、模型异构性和模型所有权。这个框架可以帮助释放基础模型的潜力，并在不同行业中产生重要影响。 |
| [^101] | [Annotation Sensitivity: Training Data Collection Methods Affect Model Performance.](http://arxiv.org/abs/2311.14212) | 该研究发现训练数据收集方法对注释本身和下游模型性能产生影响。在对仇恨言论和冒犯性语言进行注释收集的实验中，发现注释工具的设计选择会对模型的性能产生明显差异。 |
| [^102] | [Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning.](http://arxiv.org/abs/2311.09441) | 本文研究了分割联邦学习（SFL）中隐私和能耗之间的权衡，强调了快速收敛的优势，并分析了切割层对客户端能耗和隐私的影响。 |
| [^103] | [ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy.](http://arxiv.org/abs/2311.09215) | ConvNet和Transformer架构在监督和CLIP训练下，超越了ImageNet准确率的对比分析中发现它们在错误类型、输出校准、可转移性和特征的不变性等方面存在差异，突出了需要更加细致分析的必要性。 |
| [^104] | [FlashDecoding++: Faster Large Language Model Inference on GPUs.](http://arxiv.org/abs/2311.01282) | FlashDecoding++是一种快速的LLM推理引擎，通过解决同步部分softmax更新、未充分利用扁平GEMM计算和静态数据流导致的性能损失等挑战，实现了大规模语言模型推理的加速。 |
| [^105] | [Hierarchical Randomized Smoothing.](http://arxiv.org/abs/2310.16221) | 分层随机平滑是一种在复杂数据上进行鲁棒性认证的解决方案，通过只在一个对象的子集上添加随机噪声，以更有针对性的方式提供了更强的鲁棒性保证和高准确性。 |
| [^106] | [Neural Operators for Accelerating Scientific Simulations and Design.](http://arxiv.org/abs/2309.15325) | 本论文介绍了一种称为神经运算符的人工智能框架，用于学习连续域函数之间的映射，可以加速科学模拟和设计中的计算需求，实现零射超分辨率以及替代现有的模拟器。 |
| [^107] | [Fully-Connected Spatial-Temporal Graph for Multivariate Time-Series Data.](http://arxiv.org/abs/2309.05305) | 本论文提出了一种全连接的时空图神经网络（FC-STGNN）方法，用于有效地建模多变量时序数据中的时空依赖性。该方法能够捕捉不同时间戳上不同传感器之间的相关性，提供了一种全面建模时空依赖性的新途径。 |
| [^108] | [Graph-Aware Contrasting for Multivariate Time-Series Classification.](http://arxiv.org/abs/2309.05202) | 该论文提出了一种图感知对比学习方法，用于改进多元时间序列(MTS)分类任务。现有的对比学习方法忽视了MTS数据中的空间一致性，该方法通过图扩增和对比学习来保持传感器的稳定性和相关性，从而提高了对数据的表示能力。 |
| [^109] | [Subjectivity in Unsupervised Machine Learning Model Selection.](http://arxiv.org/abs/2309.00201) | 无监督机器学习模型选择具有主观性，模型选择结果受模型构建者偏好的影响，并可能导致选择的不一致性。需要对模型选择过程进行更加深入的研究和标准化。 |
| [^110] | [Predicting Drug Solubility Using Different Machine Learning Methods -- Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network.](http://arxiv.org/abs/2308.12325) | 本研究利用线性回归模型和图卷积神经网络模型预测药物溶解度，其中图卷积神经网络模型表现最好。通过线性回归模型的特征重要性分析，可以了解每个功能团对溶解度的影响。将图卷积神经网络的高性能与线性回归的可解释性相结合是未来工作的方向。 |
| [^111] | [Stabilizing RNN Gradients through Pre-training.](http://arxiv.org/abs/2308.12075) | 该研究通过预训练网络实现局部稳定性，拓展了已有的稳定性理论，可以应用于复杂结构的循环神经网络。 |
| [^112] | [Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability.](http://arxiv.org/abs/2308.07728) | 本文提出了领域感知微调（DAFT）方法，通过批归一化转换和线性探测与微调的集成，有效减轻微调过程中的特征畸变问题。 |
| [^113] | [Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images.](http://arxiv.org/abs/2308.07688) | 该研究通过在非医学图像上利用自监督学习（SSL）预训练，取得了优于基于ImageNet的预训练的结果，从而实现了在医疗AI模型中增强网络初始化的目的。 |
| [^114] | [FITS: Modeling Time Series with $10k$ Parameters.](http://arxiv.org/abs/2307.03756) | FITS是一种轻量而强大的时间序列分析模型，通过在复杂频率域中进行插值操作，丢弃对时间序列数据影响微小的高频分量，实现了与最先进模型相当的性能，并且具有较小的模型参数数量，适用于边缘设备。 |
| [^115] | [Learning Homogenization for Elliptic Operators.](http://arxiv.org/abs/2306.12006) | 本文提出了一种新的数据驱动方法，可以学习用于椭圆算子的齐次化映射，以建立考虑接口的齐次化本构定律，并且证明了该方法的有效性。 |
| [^116] | [Accelerated Convergence of Nesterov's Momentum for Deep Neural Networks under Partial Strong Convexity.](http://arxiv.org/abs/2306.08109) | 本研究证明了针对一类新的目标函数，只有部分参数满足强凸性，Nesterov动量法在深度神经网络中实现了加速收敛。 |
| [^117] | [Brain tumor segmentation using synthetic MR images -- A comparison of GANs and diffusion models.](http://arxiv.org/abs/2306.02986) | 本研究通过综合评估四种GANs和一种扩散模型，对使用合成MR图像进行脑肿瘤分割任务的性能进行了评估，结果显示使用合成图像训练的分割网络可以达到80% - 90%的实际图像训练性能。但对于扩散模型来说，训练图像的记忆化可能会成为一个问题。 |
| [^118] | [Training Diffusion Models with Reinforcement Learning.](http://arxiv.org/abs/2305.13301) | 本文研究了利用强化学习方法直接优化扩散模型以实现下游对象的问题，并提出一种称之为去噪扩散策略优化（DDPO）的有效策略梯度算法，能够适应难以通过提示表达的图像压缩等目标，以及通过人类反馈得出的美学质量等目标。 |
| [^119] | [Approximation by non-symmetric networks for cross-domain learning.](http://arxiv.org/abs/2305.03890) | 本文研究使用非对称内核进行基于内核网络逼近的通用方法，结果表明它可以在跨域学习中显著提高基于内核网络的逼近能力。 |
| [^120] | [MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos.](http://arxiv.org/abs/2304.05292) | 本文提出了一种新的深度机器学习模型MC-ViViT，用于通过分析面部特征检测老年人轻度认知障碍。通过MC模块和结合损失函数来解决数据集样本不平衡问题，提高了算法的性能。 |
| [^121] | [Local Environment Poisoning Attacks on Federated Reinforcement Learning.](http://arxiv.org/abs/2303.02725) | 本文提出了第一个广义框架，将FRL的毒化攻击视为限制预算的优化问题，并设计了一个可以应用于基于策略的FRL的毒化攻击协议，通过训练一对私人评价者和公共评价者将其扩展到使用演员-评论家作为本地RL算法的FRL。 |
| [^122] | [Surgical Aggregation: A Collaborative Learning Framework for Harmonizing Distributed Medical Imaging Datasets with Diverse Tasks.](http://arxiv.org/abs/2301.06683) | 本论文提出了一种手术聚合的协同学习框架，该框架可用于协调和聚合分布式医学影像数据集的知识，并带有部分疾病注释，从而可训练具有完整胸部内可能出现的所有异常的临床实用、强大模型。 |
| [^123] | [A Distributed Block Chebyshev-Davidson Algorithm for Parallel Spectral Clustering.](http://arxiv.org/abs/2212.04443) | 本研究开发了一种用于并行谱聚类的分布式块Chebyshev-Davidson算法，通过解析谱估计和分布式并行计算实现高效率和可扩展性。 |
| [^124] | [Efficient Estimation for Longitudinal Network via Adaptive Merging.](http://arxiv.org/abs/2211.07866) | 本文提出了一个有效的纵向网络估计框架，利用自适应合并、张量分解和点过程等方法来减少估计偏差和方差。 |
| [^125] | [Lower Difficulty and Better Robustness: A Bregman Divergence Perspective for Adversarial Training.](http://arxiv.org/abs/2208.12511) | 本文研究了通过降低优化难度来提高对抗训练中的鲁棒性。基于新颖的Bregman散度视角，分析了两种典型的对抗训练方法的学习目标，并找到了优化过程更容易的方法。提出的两种方法能够降低优化难度，并提供更好的鲁棒性。 |
| [^126] | [Application of federated learning techniques for arrhythmia classification using 12-lead ECG signals.](http://arxiv.org/abs/2208.10993) | 本研究应用联合学习技术对12导联心电图信号进行心律失常分类，通过联合学习方法训练AI模型，实现对医疗数据的隐私保护，同时保持与集中学习方式相当的性能表现。 |
| [^127] | [Quantum artificial vision for defect detection in manufacturing.](http://arxiv.org/abs/2208.04988) | 本文介绍了使用量子计算机视觉算法在制造业中检测缺陷的研究。比较了量子支持向量机和量子退火机的性能，发现量子算法在多个方面优于经典算法。该研究是首次将量子计算机视觉系统应用于制造业生产中的实际问题。 |
| [^128] | [A New Frontier of AI: On-Device AI Training and Personalization.](http://arxiv.org/abs/2206.04688) | 基于深度学习的智能服务现已开始在设备上执行，以保护个人数据并降低云开销，而研究者提出了一种轻量级的设备上训练框架NNTrainer，可以在不牺牲准确性的同时减少内存消耗并实现智能服务的个性化。 |
| [^129] | [Supervision by Denoising.](http://arxiv.org/abs/2202.02952) | 本文提出了一种去噪监督（SUD）框架，可以监督重建模型，并避免制作特定于成像领域的可微正则化器。 |
| [^130] | [Multi-agent Reinforcement Learning for Cooperative Lane Changing of Connected and Autonomous Vehicles in Mixed Traffic.](http://arxiv.org/abs/2111.06318) | 本文研究了连接和自主车辆在混合交通中的合作车道变更问题，提出了一种多智能体强化学习（MARL）方法，每个车辆根据动机制定车道变更决策。 |
| [^131] | [Game Theory for Adversarial Attacks and Defenses.](http://arxiv.org/abs/2110.06166) | 这项工作利用博弈论方法在防御对抗性攻击上取得了重要进展，通过随机化方法和去噪技术提高了网络的安全性和稳健性。 |
| [^132] | [AutoGL: A Library for Automated Graph Learning.](http://arxiv.org/abs/2104.04987) | AutoGL是第一个专门用于自动图机器学习的开源库，使用方便且易于扩展。它提供了一个完整的自动图学习流程，并支持各种图应用。 |

# 详细

[^1]: DeepSeek LLM：借助长期主义扩展开源语言模型

    DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. (arXiv:2401.02954v1 [cs.CL])

    [http://arxiv.org/abs/2401.02954](http://arxiv.org/abs/2401.02954)

    DeepSeek LLM是一个致力于以长期视野推进开源语言模型发展的项目，通过研究和应用扩展规律，成功创建了DeepSeek Chat模型，该模型在各种基准测试中表现出色，特别是在代码领域。

    

    开源大规模语言模型（LLM）的快速发展令人瞩目。然而，之前文献中描述的扩展规律得出了不同的结论，这对扩展LLM产生了负面影响。我们深入研究了扩展规律，并提出了我们独特的研究发现，以促进两种常用的开源配置（7B和67B）中大规模模型的扩展。在扩展规律的指导下，我们推出了DeepSeek LLM项目，致力于以长期视野推进开源语言模型的发展。为了支持预训练阶段，我们开发了一个数据集，目前包含2万亿个标记，并不断扩大。我们还在DeepSeek LLM基本模型上进行了监督微调（SFT）和直接偏好优化（DPO），从而创建了DeepSeek Chat模型。我们的评估结果表明，DeepSeek LLM 67B在各种基准测试中超过了LLaMA-2 70B，特别是在代码领域。

    The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code
    
[^2]: The Tactician's Web的大规模形式知识网络

    The Tactician's Web of Large-Scale Formal Knowledge. (arXiv:2401.02950v1 [cs.LO])

    [http://arxiv.org/abs/2401.02950](http://arxiv.org/abs/2401.02950)

    The Tactician's Web是一个大规模形式化数学知识网络，通过Coq证明助手和丰富的数据表示与证明工程师进行交互，并提供了机器学习、分析和证明工程的实用工具。

    

    The Tactician's Web是一个平台，提供了一个大规模的强关联的、机器验证的形式化数学知识网络，方便机器学习、分析和证明工程。基于Coq证明助手构建，该平台导出一个包含各种形式理论的数据集，呈现为定义、定理、证明项、策略和证明状态的网络。理论既可以编码为语义图（如下所示），也可以作为可读的文本，各有优缺点。证明代理可以通过相同的丰富数据表示与Coq进行交互，并可以在一组定理上自动进行基准测试。与Coq的紧密集成提供了将代理作为实用工具提供给证明工程师的独特可能性。

    The Tactician's Web is a platform offering a large web of strongly interconnected, machine-checked, formal mathematical knowledge conveniently packaged for machine learning, analytics, and proof engineering. Built on top of the Coq proof assistant, the platform exports a dataset containing a wide variety of formal theories, presented as a web of definitions, theorems, proof terms, tactics, and proof states. Theories are encoded both as a semantic graph (rendered below) and as human-readable text, each with a unique set of advantages and disadvantages. Proving agents may interact with Coq through the same rich data representation and can be automatically benchmarked on a set of theorems. Tight integration with Coq provides the unique possibility to make agents available to proof engineers as practical tools.
    
[^3]: Graph2Tac: 在定理证明中学习数学概念的分层表示

    Graph2Tac: Learning Hierarchical Representations of Math Concepts in Theorem proving. (arXiv:2401.02949v1 [cs.LG])

    [http://arxiv.org/abs/2401.02949](http://arxiv.org/abs/2401.02949)

    本文提出了一种名为Graph2Tac的图神经网络模型，用于在定理证明中学习数学概念的分层表示。该模型能够动态地将新的数学概念纳入到知识库中，并在Coq证明助手中进行训练和应用。

    

    数学及其应用中存在大量的概念。它们在不同的学科领域中有很大的变化，并且每篇数学论文或应用中都会引入新的概念。形式化理论建立了一个层次结构，其中包括了定义、定理和相互引用的证明。当一个AI代理人证明一个新的定理时，大多数与该定理相关的数学概念和引理在训练过程中可能从未被见过。这在Coq证明助手中尤为明显，该助手拥有各种各样的Coq项目，每个项目都有自己的定义、引理，甚至用于证明这些引理的自定义策略过程。将这样的新信息即时地融入到代理人的知识库中对于代理人至关重要。我们通过利用一个新的、大规模的、基于图的数据集，在Coq中进行机器学习。我们利用Coq术语的忠实图表示，创建了一种新颖的图神经网络Graph2Tac，该网络通过定义之间的依赖关系创建了一个有向图。

    Concepts abound in mathematics and its applications. They vary greatly between subject areas, and new ones are introduced in each mathematical paper or application. A formal theory builds a hierarchy of definitions, theorems and proofs that reference each other. When an AI agent is proving a new theorem, most of the mathematical concepts and lemmas relevant to that theorem may have never been seen during training. This is especially true in the Coq proof assistant, which has a diverse library of Coq projects, each with its own definitions, lemmas, and even custom tactic procedures used to prove those lemmas. It is essential for agents to incorporate such new information into their knowledge base on the fly. We work towards this goal by utilizing a new, large-scale, graph-based dataset for machine learning in Coq. We leverage a faithful graph-representation of Coq terms that induces a directed graph of dependencies between definitions to create a novel graph neural network, Graph2Tac (G
    
[^4]: Rydberg原子阵列的数字-模拟量子学习

    Digital-analog quantum learning on Rydberg atom arrays. (arXiv:2401.02940v1 [quant-ph])

    [http://arxiv.org/abs/2401.02940](http://arxiv.org/abs/2401.02940)

    这项研究提出了在Rydberg原子阵列上进行数字-模拟量子学习的算法，并通过数值实验表明，在近期实现这一算法是可行的，并且相比于数字学习方案，数字-模拟学习需要更短的电路深度，并且对于现实误差模型更具鲁棒性。这使得数字-模拟学习成为中期改进变分量子学习实验的有前景的方法。

    

    我们提出了一种在Rydberg原子阵列上的混合数字-模拟量子学习算法，结合了量子学习的潜在实用性和近期可实现性以及中性原子的快速扩展架构。我们的构建只需在数字设置中进行单量子比特操作，并根据Rydberg哈密顿量进行模拟设置中的全局驱动。我们在经典和量子数据上进行了全面的数值研究，分别通过手写数字分类和无监督量子相边界学习来表示。我们在这两个典型问题中展示了数字-模拟学习不仅在近期是可行的，而且与数字学习方案相比，需要更短的电路深度，并且对于现实误差模型更具鲁棒性。我们的结果表明，在中期提高变分量子学习实验的效果方面，数字-模拟学习打开了一个有前景的道路。

    We propose hybrid digital-analog learning algorithms on Rydberg atom arrays, combining the potentially practical utility and near-term realizability of quantum learning with the rapidly scaling architectures of neutral atoms. Our construction requires only single-qubit operations in the digital setting and global driving according to the Rydberg Hamiltonian in the analog setting. We perform a comprehensive numerical study of our algorithm on both classical and quantum data, given respectively by handwritten digit classification and unsupervised quantum phase boundary learning. We show in the two representative problems that digital-analog learning is not only feasible in the near term, but also requires shorter circuit depths and is more robust to realistic error models as compared to digital learning schemes. Our results suggest that digital-analog learning opens a promising path towards improved variational quantum learning experiments in the near term.
    
[^5]: 快速且最优的修剪大型语言模型的权重更新。

    Fast and Optimal Weight Update for Pruned Large Language Models. (arXiv:2401.02938v1 [cs.CL])

    [http://arxiv.org/abs/2401.02938](http://arxiv.org/abs/2401.02938)

    本研究提出了一种基于交替方向乘积算法(ADMM)的快速且最优的修剪层权重更新算法，结合简单的迭代修剪掩码选择，在广泛的大型语言模型范围内实现了最先进的修剪性能。

    

    修剪大型语言模型(LLMs)是一个具有挑战性的任务，因为它们的规模庞大。主要困难在于修剪后的模型微调，这是为了恢复因删除权重而导致的性能损失。最近的方法要么完全忽略了微调，专注于高效的修剪标准，要么尝试逐层权重更新，保持每个层的行为。然而，即使是逐层权重更新对LLMs来说也可能代价高昂，之前的工作不得不采用各种近似方法。在我们的论文中，我们提出了一种基于交替方向乘积算法(ADMM)的快速且最优的修剪层权重更新算法。结合简单的迭代修剪掩码选择，我们的算法在广泛的LLMs范围内实现了最先进的修剪性能。代码可以在https://github.com/fmfi-compbio/admm-pruning获得。

    Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations.  In our paper, we propose a fast and optimal weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). Coupled with a simple iterative pruning mask selection, our algorithm achieves state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning.
    
[^6]: Dagma-DCE：可解释的、非参数的可微因果发现

    Dagma-DCE: Interpretable, Non-Parametric Differentiable Causal Discovery. (arXiv:2401.02930v1 [cs.LG])

    [http://arxiv.org/abs/2401.02930](http://arxiv.org/abs/2401.02930)

    Dagma-DCE是一种可解释的、非参数的可微因果发现方案，使用可解释的因果强度度量定义加权邻接矩阵，并在模拟数据集中达到最先进的性能水平。

    

    我们介绍了Dagma-DCE，这是一种可解释的、与模型无关的可微因果发现方案。当前的非参数或超参数方法在可微因果发现中使用不透明的``独立性''代理来证明是否包含或排除因果关系。我们理论上和实证上展示了这些代理可能与实际的因果强度任意不同。与现有的可微因果发现算法相比，Dagma-DCE使用可解释的因果强度度量来定义加权邻接矩阵。在一些模拟数据集中，我们展示了我们的方法达到了最先进的性能水平。我们还展示了Dagma-DCE允许领域专家进行有原则的阈值设定和稀疏惩罚。我们的方法的代码在https://github.com/DanWaxman/DAGMA-DCE上开源，并且可以很容易地适应任意的可微模型。

    We introduce Dagma-DCE, an interpretable and model-agnostic scheme for differentiable causal discovery. Current non- or over-parametric methods in differentiable causal discovery use opaque proxies of ``independence'' to justify the inclusion or exclusion of a causal relationship. We show theoretically and empirically that these proxies may be arbitrarily different than the actual causal strength. Juxtaposed to existing differentiable causal discovery algorithms, \textsc{Dagma-DCE} uses an interpretable measure of causal strength to define weighted adjacency matrices. In a number of simulated datasets, we show our method achieves state-of-the-art level performance. We additionally show that \textsc{Dagma-DCE} allows for principled thresholding and sparsity penalties by domain-experts. The code for our method is available open-source at https://github.com/DanWaxman/DAGMA-DCE, and can easily be adapted to arbitrary differentiable models.
    
[^7]: 统一的不确定性感知探索：结合认知不确定性和偶然不确定性

    A unified uncertainty-aware exploration: Combining epistemic and aleatory uncertainty. (arXiv:2401.02914v1 [cs.LG])

    [http://arxiv.org/abs/2401.02914](http://arxiv.org/abs/2401.02914)

    本文提出了一种统一的不确定性感知探索算法，将认知不确定性和偶然不确定性结合起来进行探索，通过量化两种不确定性的综合效应实现风险敏感的探索策略。

    

    在实际强化学习中，探索是一个重要的挑战，而将认知不确定性和偶然不确定性量化的不确定性感知探索被认为是一种有效的探索策略。然而，捕捉偶然和认知不确定性的联合效应用于决策是困难的。现有的研究分别估计了偶然和认知不确定性，并将组合不确定性视为两者的加法组合。然而，这种加法形式会导致过度冒险行为，导致不稳定性。在本文中，我们提出了一种算法，澄清了偶然和认知不确定性之间的理论联系，统一了偶然和认知不确定性估计，并量化了两种不确定性对风险敏感的探索的综合影响。我们的方法基于一种新颖的分布式强化学习扩展，其估计了一个参数化的回报分布，其参数化范围。

    Exploration is a significant challenge in practical reinforcement learning (RL), and uncertainty-aware exploration that incorporates the quantification of epistemic and aleatory uncertainty has been recognized as an effective exploration strategy. However, capturing the combined effect of aleatory and epistemic uncertainty for decision-making is difficult. Existing works estimate aleatory and epistemic uncertainty separately and consider the composite uncertainty as an additive combination of the two. Nevertheless, the additive formulation leads to excessive risk-taking behavior, causing instability. In this paper, we propose an algorithm that clarifies the theoretical connection between aleatory and epistemic uncertainty, unifies aleatory and epistemic uncertainty estimation, and quantifies the combined effect of both uncertainties for a risk-sensitive exploration. Our method builds on a novel extension of distributional RL that estimates a parameterized return distribution whose para
    
[^8]: H2G2-Net:一种用于多模态生理反应发现的分层异构图生成网络框架

    H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses. (arXiv:2401.02905v1 [cs.LG])

    [http://arxiv.org/abs/2401.02905](http://arxiv.org/abs/2401.02905)

    H2G2-Net是一种用于发现多模态生理反应的分层异构图生成网络框架，能够自动学习图结构而不需要预定义的领域知识。

    

    在各种研究应用中，利用多模态生理信号来发现人类认知和情感状态引起了人们的关注。人体的生理反应受到人类认知的影响，常用于分析认知状态。从网络科学的角度来看，这些异构生理模式在图结构中的互动可能提供有益的信息来支持认知状态的预测。然而，目前没有办法得到异构模态之间的精确连接，并且存在一种分层结构的子模态。现有的图神经网络设计用于在预定义的图结构上学习非层次化的同质图，无法从层次化的多模态生理数据中学习，没有预定义的图结构。为此，我们提出了一种分层异构图生成网络（H2G2-Net），能够自动学习图结构而不需要先验领域知识。

    Discovering human cognitive and emotional states using multi-modal physiological signals draws attention across various research applications. Physiological responses of the human body are influenced by human cognition and commonly used to analyze cognitive states. From a network science perspective, the interactions of these heterogeneous physiological modalities in a graph structure may provide insightful information to support prediction of cognitive states. However, there is no clue to derive exact connectivity between heterogeneous modalities and there exists a hierarchical structure of sub-modalities. Existing graph neural networks are designed to learn on non-hierarchical homogeneous graphs with pre-defined graph structures; they failed to learn from hierarchical, multi-modal physiological data without a pre-defined graph structure. To this end, we propose a hierarchical heterogeneous graph generative network (H2G2-Net) that automatically learns a graph structure without domain 
    
[^9]: 类别普适误差：信息论分析

    Class-wise Generalization Error: an Information-Theoretic Analysis. (arXiv:2401.02904v1 [cs.LG])

    [http://arxiv.org/abs/2401.02904](http://arxiv.org/abs/2401.02904)

    本文通过信息论分析研究了类别普适误差问题，并提出了使用KL散度的信息论界限和使用条件互信息(CMI)的更紧界限，能够准确捕捉复杂的类别普适误差。

    

    现有的监督学习普适性理论通常采用整体方法，并提供了整个数据分布上预期普适化的界限，这隐含地假定模型对所有类别都具有类似的普适性。然而在实践中，不同类别的普适性性能存在显著差异，这无法被现有的普适性界限所捕捉。本文通过理论上研究类别普适误差来解决这个问题，该误差量化了每个个体类别的普适性能力。我们利用KL散度推导了一种新的信息论界限来衡量类别普适误差，并进一步使用条件互信息(CMI)得到了几个更紧的界限，在实践中更容易估计。我们在不同的神经网络上进行了实验证实，验证了我们提出的界限准确地捕捉了复杂的类别普适误差。

    Existing generalization theories of supervised learning typically take a holistic approach and provide bounds for the expected generalization over the whole data distribution, which implicitly assumes that the model generalizes similarly for all the classes. In practice, however, there are significant variations in generalization performance among different classes, which cannot be captured by the existing generalization bounds. In this work, we tackle this problem by theoretically studying the class-generalization error, which quantifies the generalization performance of each individual class. We derive a novel information-theoretic bound for class-generalization error using the KL divergence, and we further obtain several tighter bounds using the conditional mutual information (CMI), which are significantly easier to estimate in practice. We empirically validate our proposed bounds in different neural networks and show that they accurately capture the complex class-generalization err
    
[^10]: 使用深度强化学习实现自主Formula SAE车辆的局部路径跟随

    Deep Reinforcement Learning for Local Path Following of an Autonomous Formula SAE Vehicle. (arXiv:2401.02903v1 [cs.RO])

    [http://arxiv.org/abs/2401.02903](http://arxiv.org/abs/2401.02903)

    本文介绍了使用深度强化学习和逆向增强学习来实现自主Formula SAE车辆的局部路径跟随。使用软actor评判和对抗性逆向增强学习算法进行模型训练，并提出了三种新颖奖励函数。仿真和真实环境测试表明，这两种算法都可以成功训练模型进行局部路径跟随。

    

    随着无人驾驶比赛在全球Formula:Society of Automotive Engineers (F:SAE)竞赛中的持续推出，车队正在研究自主驾驶车辆系统的各个方面。本文介绍了使用深度强化学习（Deep Reinforcement Learning，DRL）和逆向增强学习（Inverse Reinforcement Learning，IRL）来将局部观察到的锥形位置映射到期望的转向角度以实现赛道跟随。该论文使用了两种在此背景下尚未测试过的最先进算法：软actor评判（soft actor critic，SAC）和对抗性逆向增强学习（adversarial inverse reinforcement learning，AIRL），并在代表性仿真中进行模型训练。还讨论了三种在自主赛车背景下由强化学习算法使用的新颖奖励函数。在仿真和真实环境中进行的测试表明，这两种算法都可以成功训练出用于局部路径跟随的模型。还提出了未来工作的建议，以使这些模型能够扩展到完整的F:SAE车辆。

    With the continued introduction of driverless events to Formula:Society of Automotive Engineers (F:SAE) competitions around the world, teams are investigating all aspects of the autonomous vehicle stack. This paper presents the use of Deep Reinforcement Learning (DRL) and Inverse Reinforcement Learning (IRL) to map locally-observed cone positions to a desired steering angle for race track following. Two state-of-the-art algorithms not previously tested in this context: soft actor critic (SAC) and adversarial inverse reinforcement learning (AIRL), are used to train models in a representative simulation. Three novel reward functions for use by RL algorithms in an autonomous racing context are also discussed. Tests performed in simulation and the real world suggest that both algorithms can successfully train models for local path following. Suggestions for future work are presented to allow these models to scale to a full F:SAE vehicle.
    
[^11]: 连续时间深度神经网络的状态导数标准化

    State Derivative Normalization for Continuous-Time Deep Neural Networks. (arXiv:2401.02902v1 [eess.SY])

    [http://arxiv.org/abs/2401.02902](http://arxiv.org/abs/2401.02902)

    本文研究了在连续时间状态空间模型估计中，深度神经网络的数据标准化问题。通过引入状态导数级别的标准化常数，解决了隐藏状态、隐藏状态导数以及时间间隔的标准化挑战。选择适当的标准化常数与待识别系统的动力学相关，并提出了多种获得有效标准化常数的方法。

    

    深度神经网络的适当数据标准化的重要性是众所周知的。然而，在连续时间状态空间模型估计中，观察到模型估计的隐藏状态或隐藏状态导数，甚至时间间隔的不适当标准化可能会导致使用基于深度学习的方法时的数值和优化挑战。这导致模型质量降低。在本文中，我们展示了这三个标准化任务的内在耦合。由于存在这种耦合，我们提出了一种在状态导数水平引入标准化常数的解决方案。我们展示了适当选择标准化常数与待识别系统的动力学相关，并推导了多种获得有效标准化常数的方法。我们在基于实验数据的基准问题上比较和讨论了所有标准化策略。

    The importance of proper data normalization for deep neural networks is well known. However, in continuous-time state-space model estimation, it has been observed that improper normalization of either the hidden state or hidden state derivative of the model estimate, or even of the time interval can lead to numerical and optimization challenges with deep learning based methods. This results in a reduced model quality. In this contribution, we show that these three normalization tasks are inherently coupled. Due to the existence of this coupling, we propose a solution to all three normalization challenges by introducing a normalization constant at the state derivative level. We show that the appropriate choice of the normalization constant is related to the dynamics of the to-be-identified system and we derive multiple methods of obtaining an effective normalization constant. We compare and discuss all the normalization strategies on a benchmark problem based on experimental data from a
    
[^12]: 函数深度神经网络在非线性函数回归中的应用

    Nonlinear functional regression by functional deep neural network with kernel embedding. (arXiv:2401.02890v1 [stat.ML])

    [http://arxiv.org/abs/2401.02890](http://arxiv.org/abs/2401.02890)

    本文提出了一种函数深度神经网络用于非线性函数回归的方法，通过平滑核积分变换和数据相关的维度缩减方法，取得了良好的预测效果。

    

    随着深度学习在语音识别、图像分类和自然语言处理等领域的迅速发展，它也被广泛应用于函数数据分析中。然而，由于无限维的输入，我们需要一个强大的维度缩减方法来处理非线性函数回归任务。在本文中，基于平滑核积分变换的思想，我们提出了一种具有高效且完全数据依赖的维度缩减方法的函数深度神经网络。我们的函数网络由以下步骤组成：核嵌入步骤：利用数据相关的平滑核进行积分变换；投影步骤：通过基于嵌入核的特征函数基底进行维度缩减；最后是一个表达丰富的深度ReLU神经网络进行预测。

    With the rapid development of deep learning in various fields of science and technology, such as speech recognition, image classification, and natural language processing, recently it is also widely applied in the functional data analysis (FDA) with some empirical success. However, due to the infinite dimensional input, we need a powerful dimension reduction method for functional learning tasks, especially for the nonlinear functional regression. In this paper, based on the idea of smooth kernel integral transformation, we propose a functional deep neural network with an efficient and fully data-dependent dimension reduction method. The architecture of our functional net consists of a kernel embedding step: an integral transformation with a data-dependent smooth kernel; a projection step: a dimension reduction by projection with eigenfunction basis based on the embedding kernel; and finally an expressive deep ReLU neural network for the prediction. The utilization of smooth kernel embe
    
[^13]: 高效设计和控制的节约能量约束下的减少操作推断

    Energy-Preserving Reduced Operator Inference for Efficient Design and Control. (arXiv:2401.02889v1 [math.NA])

    [http://arxiv.org/abs/2401.02889](http://arxiv.org/abs/2401.02889)

    本文提出了一种能够节约能量的减少操作推断方法，用于高效设计和控制中的多次计算任务，特别适用于保持能量的偏微分方程控制系统。

    

    对于设计和控制中需要多次计算的工程系统，计算模型的高效性至关重要。对于由偏微分方程（PDE）控制的系统，常见的高保真数值模型是高维的，对于多次计算而言计算成本过高。因此，需要有效的代理模型以实现设计和控制中的低成本计算。本文提出了一种保持物理约束的减少模型学习方法，针对能量保持的PDE，例如许多流体问题中出现的控制方程。该方法基于操作推断方法，通过最小二乘拟合将减少模型算子与状态快照和时间导数数据匹配。然而，操作推断通常不能学习到具有能量保持属性的减少二次算子，因此我们提出了一种新的能量保持操作推断（EP-O）方法。

    Many-query computations, in which a computational model for an engineering system must be evaluated many times, are crucial in design and control. For systems governed by partial differential equations (PDEs), typical high-fidelity numerical models are high-dimensional and too computationally expensive for the many-query setting. Thus, efficient surrogate models are required to enable low-cost computations in design and control. This work presents a physics-preserving reduced model learning approach that targets PDEs whose quadratic operators preserve energy, such as those arising in governing equations in many fluids problems. The approach is based on the Operator Inference method, which fits reduced model operators to state snapshot and time derivative data in a least-squares sense. However, Operator Inference does not generally learn a reduced quadratic operator with the energy-preserving property of the original PDE. Thus, we propose a new energy-preserving Operator Inference (EP-O
    
[^14]: 用于量子核对齐的高效参数优化：一种在可变训练中的子采样方法

    Efficient Parameter Optimisation for Quantum Kernel Alignment: A Sub-sampling Approach in Variational Training. (arXiv:2401.02879v1 [quant-ph])

    [http://arxiv.org/abs/2401.02879](http://arxiv.org/abs/2401.02879)

    本文提出了一种高效的量子核对齐方法，通过使用子采样训练的方式在减少计算成本的同时保持分类准确度，以解决量子核对齐的训练代价大的问题。

    

    量子机器学习中用于分类问题的量子核对齐是一个不断发展的研究领域。最近，提出了一种参数化核函数的量子核对齐技术，可以对核函数进行训练，从而与特定数据集对齐。尽管量子核对齐是一种有前景的技术，但由于每次训练迭代都必须构建完整的核矩阵，因此一直受到显著的训练成本限制。为了解决这个挑战，我们引入了一种旨在平衡效率和性能的新方法。我们提出了一种子采样训练方法，每次训练步骤使用核矩阵的子集，从而减少了训练的总体计算成本。在这项工作中，我们将子采样方法应用于合成数据集和真实的乳腺癌数据集，并展示在维持分类准确度的同时，所需训练量子核的电路数量大大减少。

    Quantum machine learning with quantum kernels for classification problems is a growing area of research. Recently, quantum kernel alignment techniques that parameterise the kernel have been developed, allowing the kernel to be trained and therefore aligned with a specific dataset. While quantum kernel alignment is a promising technique, it has been hampered by considerable training costs because the full kernel matrix must be constructed at every training iteration. Addressing this challenge, we introduce a novel method that seeks to balance efficiency and performance. We present a sub-sampling training approach that uses a subset of the kernel matrix at each training step, thereby reducing the overall computational cost of the training. In this work, we apply the sub-sampling method to synthetic datasets and a real-world breast cancer dataset and demonstrate considerable reductions in the number of circuits required to train the quantum kernel while maintaining classification accuracy
    
[^15]: 变化滞后模式跟随关系推理的时间序列矩阵分析框架

    Framework for Variable-lag Motif Following Relation Inference In Time Series using Matrix Profile analysis. (arXiv:2401.02860v1 [cs.LG])

    [http://arxiv.org/abs/2401.02860](http://arxiv.org/abs/2401.02860)

    该论文提出了一个利用矩阵分析方法的框架，用于推理时间序列中的跟随模式。在模拟数据集和声音记录数据集中，该框架优于基准方法，并能够检测出加密货币数据集中的跟随模式。

    

    知道谁跟随谁以及他们跟随的模式是理解集体行为（如人群，鱼群或股市）的关键步骤。时间序列是用于获取跟随关系洞察的资源之一。然而，跟随模式或模式在时间序列中的发现解决方案并不明显。在这项工作中，我们形式化了两个时间序列之间的跟随模式概念，并提出了一个推断两个时间序列之间跟随模式的框架。该框架利用一种高效且可扩展的方法从时间序列中检索模式，称为矩阵分析方法。我们将提出的框架与几个基准进行了比较。在模拟数据集中，该框架优于基准方法。在声音记录数据集中，该框架能够在一对时间序列中检索出两位歌手相互跟随唱歌的跟随模式。在加密货币数据集中，

    Knowing who follows whom and what patterns they are following are crucial steps to understand collective behaviors (e.g. a group of human, a school of fish, or a stock market). Time series is one of resources that can be used to get insight regarding following relations. However, the concept of following patterns or motifs and the solution to find them in time series are not obvious. In this work, we formalize a concept of following motifs between two time series and present a framework to infer following patterns between two time series. The framework utilizes one of efficient and scalable methods to retrieve motifs from time series called the Matrix Profile Method. We compare our proposed framework with several baselines. The framework performs better than baselines in the simulation datasets. In the dataset of sound recording, the framework is able to retrieve the following motifs within a pair of time series that two singers sing following each other. In the cryptocurrency dataset,
    
[^16]: 使用自校正生成非平稳纹理

    Generating Non-Stationary Textures using Self-Rectification. (arXiv:2401.02847v1 [cs.CV])

    [http://arxiv.org/abs/2401.02847](http://arxiv.org/abs/2401.02847)

    本文提出了一种使用自校正来生成非平稳纹理的方法，通过使用预训练扩散网络和自注意机制，可以将用户修改的参考纹理细化为一种连贯、无缝的纹理，并保留参考样本的独特视觉特征。实验证实表明，该方法在处理非平稳纹理方面具有卓越的能力，相比现有技术在纹理合成方面取得了显著的进展。

    

    本文解决了基于示例的非平稳纹理合成的挑战。我们提出了一种新的两步方法，用户可以使用标准图像编辑工具修改参考纹理，得到合成的初始目标。随后，我们提出的方法“自校正”自动将这个目标细化为一种连贯、无缝的纹理，同时忠实地保留了参考样本的独特视觉特征。我们的方法利用预训练扩散网络，并使用自注意机制，逐渐将合成纹理与参考对齐，确保保留所提供目标中的结构。通过实验证实，我们的方法在处理非平稳纹理方面表现出卓越的能力，相比现有的最先进技术，在纹理合成方面取得了显著的进展。代码可在https://github.com/xiaorongjun000/Self-Rectific下载。

    This paper addresses the challenge of example-based non-stationary texture synthesis. We introduce a novel twostep approach wherein users first modify a reference texture using standard image editing tools, yielding an initial rough target for the synthesis. Subsequently, our proposed method, termed "self-rectification", automatically refines this target into a coherent, seamless texture, while faithfully preserving the distinct visual characteristics of the reference exemplar. Our method leverages a pre-trained diffusion network, and uses self-attention mechanisms, to gradually align the synthesized texture with the reference, ensuring the retention of the structures in the provided target. Through experimental validation, our approach exhibits exceptional proficiency in handling non-stationary textures, demonstrating significant advancements in texture synthesis when compared to existing state-of-the-art techniques. Code is available at https://github.com/xiaorongjun000/Self-Rectific
    
[^17]: 数千位AI作者对未来AI的预测

    Thousands of AI Authors on the Future of AI. (arXiv:2401.02843v1 [cs.CY])

    [http://arxiv.org/abs/2401.02843](http://arxiv.org/abs/2401.02843)

    数千位AI作者对未来AI的预测显示，到2028年，AI系统有50%的几率实现多个里程碑，包括自主构建全新的付款处理网站、创作一首与知名音乐家的新歌难以区分的歌曲，并自主下载和调整大型语言模型。同时，无需辅助的机器在各种任务上胜过人类的几率估计为10%到2047年为50%。

    

    在迄今为止最大规模的调查中，2778名在顶级人工智能（AI）会议上发表过论文的研究人员对AI进展的速度、高级AI系统的性质和影响进行了预测。总体预测显示，到2028年，AI系统至少有50%的几率实现多个里程碑，包括自主构建一个全新的付款处理网站、创作一首可以与知名音乐家的新歌难以区分的歌曲，并自主下载和调整大型语言模型。如果科学持续不受干扰，2027年无需辅助的机器在各种任务上胜过人类的几率估计为10%，到2047年为50%。后者的估计比我们一年前进行的类似调查[Grace et al., 2022]提前了13年。然而，所有人类职业完全可自动化的几率预计要到2037年达到10%，到2116年才达到50%（与2022年调查中的2164年相比）。

    In the largest survey of its kind, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace of AI progress and the nature and impacts of advanced AI systems The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).  Most
    
[^18]: 让我们开始吧：促进Deezer音乐服务上新发布内容的发现能力

    Let's Get It Started: Fostering the Discoverability of New Releases on Deezer. (arXiv:2401.02827v1 [cs.IR])

    [http://arxiv.org/abs/2401.02827](http://arxiv.org/abs/2401.02827)

    本文介绍了在Deezer音乐服务上促进新发布内容发现能力的最新举措，包括个性化推荐、冷启动嵌入和情境强化学习算法等。通过在线实验的支持，我们展示了这些举措在提高推荐质量和新发布内容曝光方面的优势。

    

    本文介绍了我们最近在音乐流媒体服务Deezer上促进新发布内容发现能力方面的举措。在介绍了我们针对新发布内容的搜索和推荐功能之后，我们概述了我们从编辑推荐向个性化推荐的转变，包括使用冷启动嵌入和情境强化学习算法。通过在线实验的支持，我们讨论了这一转变在推荐质量和新发布内容的曝光方面的优势。

    This paper presents our recent initiatives to foster the discoverability of new releases on the music streaming service Deezer. After introducing our search and recommendation features dedicated to new releases, we outline our shift from editorial to personalized release suggestions using cold start embeddings and contextual bandits. Backed by online experiments, we discuss the advantages of this shift in terms of recommendation quality and exposure of new releases on the service.
    
[^19]: 使用迁移学习的物理信息神经网络解决高频率和多尺度问题

    Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning. (arXiv:2401.02810v1 [cs.LG])

    [http://arxiv.org/abs/2401.02810](http://arxiv.org/abs/2401.02810)

    本论文提出使用迁移学习来提高物理信息神经网络（PINN）训练的鲁棒性和收敛性。经过两个案例研究，发现迁移学习可以有效地训练PINN来近似解决方案，从低频率问题到高频率问题，而不增加网络参数，且需要更少的数据点和更短的训练时间。

    

    物理信息神经网络（PINN）是一种用于偏微分方程（ODEs/PDEs）的数据驱动求解器，提供了统一的框架来处理前向和反向问题。然而，目标函数的复杂性常常导致训练失败。当解决高频率和多尺度问题时，这个问题尤为突出。我们提出使用迁移学习来提高训练PINN的鲁棒性和收敛性，从低频率问题开始训练，并逐渐接近高频率问题。通过两个案例研究，我们发现迁移学习可以有效地训练PINN来近似解决方案，从低频率问题到高频率问题，而不增加网络参数。此外，它需要更少的数据点和更短的训练时间。我们详细描述了我们的训练策略，包括优化器的选择，并提出了使用迁移学习来训练神经网络的指南。

    Physics-informed neural network (PINN) is a data-driven solver for partial and ordinary differential equations(ODEs/PDEs). It provides a unified framework to address both forward and inverse problems. However, the complexity of the objective function often leads to training failures. This issue is particularly prominent when solving high-frequency and multi-scale problems. We proposed using transfer learning to boost the robustness and convergence of training PINN, starting training from low-frequency problems and gradually approaching high-frequency problems. Through two case studies, we discovered that transfer learning can effectively train PINN to approximate solutions from low-frequency problems to high-frequency problems without increasing network parameters. Furthermore, it requires fewer data points and less training time. We elaborately described our training strategy, including optimizer selection, and suggested guidelines for using transfer learning to train neural networks 
    
[^20]: Credence:利用机器学习预测增强数据中心交换机缓冲区共享

    Credence: Augmenting Datacenter Switch Buffer Sharing with ML Predictions. (arXiv:2401.02801v1 [cs.NI])

    [http://arxiv.org/abs/2401.02801](http://arxiv.org/abs/2401.02801)

    Credence是一种使用机器学习预测增强的丢弃式缓冲区共享算法，可以显著提高数据中心交换机的性能。

    

    数据中心交换机中的数据包缓冲区被共享在所有交换机端口上，以提高整体吞吐量。数据中心交换机缓冲区大小不断缩小的趋势使得缓冲区共享变得极具挑战性，成为关键的性能问题。文献表明，推出式缓冲区共享算法相对于丢弃式算法具有显着的性能保证。不幸的是，由于硬件不支持推出操作，交换机无法从这些算法中受益。我们的关键观察是，如果未来数据包到达的时间事先知道，丢弃式缓冲区可以模拟推出式缓冲区。这表明，将关于未来到达的预测与丢弃式算法相结合可以显著提高性能。本文是这个方向上的首次研究尝试。我们提出了Credence，一种使用机器学习预测增强的丢弃式缓冲区共享算法。

    Packet buffers in datacenter switches are shared across all the switch ports in order to improve the overall throughput. The trend of shrinking buffer sizes in datacenter switches makes buffer sharing extremely challenging and a critical performance issue. Literature suggests that push-out buffer sharing algorithms have significantly better performance guarantees compared to drop-tail algorithms. Unfortunately, switches are unable to benefit from these algorithms due to lack of support for push-out operations in hardware. Our key observation is that drop-tail buffers can emulate push-out buffers if the future packet arrivals are known ahead of time. This suggests that augmenting drop-tail algorithms with predictions about the future arrivals has the potential to significantly improve performance.  This paper is the first research attempt in this direction. We propose Credence, a drop-tail buffer sharing algorithm augmented with machine-learned predictions. Credence can unlock the perfo
    
[^21]: 弱半监督下的微创手术视频工具检测

    Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery Videos. (arXiv:2401.02791v1 [cs.CV])

    [http://arxiv.org/abs/2401.02791](http://arxiv.org/abs/2401.02791)

    本研究提出了一种在微创手术视频中弱半监督下检测手术工具的方法。通过使用共现损失来利用图像级标签中工具对之间的共现关系，平衡了注释负担和检测性能，克服了分类困难。

    

    手术工具检测对于分析和评估微创手术视频至关重要。目前的方法主要基于需要大量完整的实例级标签（即边界框）的监督方法。然而，由于注释的负担，具有实例级标签的大型图像数据集通常很有限。因此，在提供图像级标签而不是实例级标签时，手术工具检测变得重要，因为图像级注释比实例级注释更具时间效率。在这项工作中，我们提出在极高的注释负担和检测性能之间寻求平衡。我们进一步提出了一种共现损失，该损失考虑了某些工具对在图像中经常共同出现的特性，以利用图像级标签。用共现损失对共现关系的知识进行封装有助于克服分类困难，因为一些手术工具的分类困难源于这样一个事实：它们经常以成对出现。

    Surgical tool detection is essential for analyzing and evaluating minimally invasive surgery videos. Current approaches are mostly based on supervised methods that require large, fully instance-level labels (i.e., bounding boxes). However, large image datasets with instance-level labels are often limited because of the burden of annotation. Thus, surgical tool detection is important when providing image-level labels instead of instance-level labels since image-level annotations are considerably more time-efficient than instance-level annotations. In this work, we propose to strike a balance between the extremely costly annotation burden and detection performance. We further propose a co-occurrence loss, which considers a characteristic that some tool pairs often co-occur together in an image to leverage image-level labels. Encapsulating the knowledge of co-occurrence using the co-occurrence loss helps to overcome the difficulty in classification that originates from the fact that some 
    
[^22]: 通过HD-EMG电极子集解决姿势识别中的电极漂移问题

    Tackling Electrode Shift In Gesture Recognition with HD-EMG Electrode Subsets. (arXiv:2401.02773v1 [cs.LG])

    [http://arxiv.org/abs/2401.02773](http://arxiv.org/abs/2401.02773)

    本研究提出了一种解决姿势识别中电极漂移问题的方法，通过使用HD-EMG电极子集，并增加来自不同电极位置的数据，同时提高了稳健性和性能。

    

    sEMG模式识别算法在解码运动意图方面进行了广泛研究，但已知对于不断变化的记录条件非常脆弱，性能在不同受试者和不同会话之间会显著下降。多通道表面肌电图（也称为高密度sEMG）系统通过使用额外的电极收集信息，可以提高性能。然而，由于有限的数据集和解决电极放置等变量来源的困难，缺乏稳健性一直存在。在本研究中，我们提出在一组输入通道子集上进行训练，并通过来自不同电极位置的数据增强我们的训练分布，从而同时解决电极漂移问题和降低输入维度。我们的方法提高了对电极漂移的稳健性，并在不同受试者和分类算法间显著提高了会话间性能。

    sEMG pattern recognition algorithms have been explored extensively in decoding movement intent, yet are known to be vulnerable to changing recording conditions, exhibiting significant drops in performance across subjects, and even across sessions. Multi-channel surface EMG, also referred to as high-density sEMG (HD-sEMG) systems, have been used to improve performance with the information collected through the use of additional electrodes. However, a lack of robustness is ever present due to limited datasets and the difficulties in addressing sources of variability, such as electrode placement. In this study, we propose training on a collection of input channel subsets and augmenting our training distribution with data from different electrode locations, simultaneously targeting electrode shift and reducing input dimensionality. Our method increases robustness against electrode shift and results in significantly higher intersession performance across subjects and classification algorith
    
[^23]: Powerformer：适应不同传输区段的变压器架构用于电力流调整

    Powerformer: A Section-adaptive Transformer for Power Flow Adjustment. (arXiv:2401.02771v1 [cs.LG])

    [http://arxiv.org/abs/2401.02771](http://arxiv.org/abs/2401.02771)

    Powerformer是一种适应不同传输区段的变压器架构，用于学习稳健电力系统状态表示。它通过开发专用的区段自适应注意机制，并引入图神经网络传播和多因素注意机制来提供更加稳健的状态表示。在三个不同的电力系统场景上进行了广泛评估。

    

    本文提出了一种专为学习稳健电力系统状态表示而量身定制的变压器架构，旨在优化跨不同传输区段的电力调度以进行电力流调整。具体而言，我们的提出的方法名为Powerformer，开发了一种专用的区段自适应注意机制，与传统变压器中使用的自注意分离开来。该机制有效地将电力系统状态与传输区段信息整合在一起，有助于开发稳健的状态表示。此外，通过考虑电力系统的图拓扑和母线节点的电气属性，我们引入了两种定制策略来进一步增强表达能力：图神经网络传播和多因素注意机制。我们在三个电力系统场景（包括IEEE 118节点系统、中国实际300节点系统和一个大型系统）上进行了广泛的评估。

    In this paper, we present a novel transformer architecture tailored for learning robust power system state representations, which strives to optimize power dispatch for the power flow adjustment across different transmission sections. Specifically, our proposed approach, named Powerformer, develops a dedicated section-adaptive attention mechanism, separating itself from the self-attention used in conventional transformers. This mechanism effectively integrates power system states with transmission section information, which facilitates the development of robust state representations. Furthermore, by considering the graph topology of power system and the electrical attributes of bus nodes, we introduce two customized strategies to further enhance the expressiveness: graph neural network propagation and multi-factor attention mechanism. Extensive evaluations are conducted on three power system scenarios, including the IEEE 118-bus system, a realistic 300-bus system in China, and a large-
    
[^24]: 为多任务联邦学习提供公平性感知的作业调度

    Fairness-Aware Job Scheduling for Multi-Job Federated Learning. (arXiv:2401.02740v1 [cs.LG])

    [http://arxiv.org/abs/2401.02740](http://arxiv.org/abs/2401.02740)

    本文提出了一种公平感知联邦作业调度（FairFedJS）方法，以确保将高需求的FL客户端数据集公平分配给需要它们的FL作业，并在实验证明其优势。

    

    联邦学习（FL）使多个数据所有者（即FL客户端）能够在不泄露敏感私人数据的情况下共同训练机器学习模型。现有的FL研究主要关注垄断场景，在该场景中，单个FL服务器在每轮训练中选择一部分FL客户端来更新其本地模型。实际上，可能会有多个FL服务器同时尝试从同一个池中选择客户端。本文提出了一种首创的公平感知联邦作业调度（FairFedJS）方法来弥合这一差距。基于Lyapunov优化，它通过同时考虑当前需求和作业付款出价，确保将高需求的FL客户端数据集公平分配给需要它们的FL作业，以防止等待时间过长。基于两个数据集对FairFedJS与四种最先进的方法进行了大量实验证明了其显著优势。它在平均上击败了最佳基准线31.9%和1.0%。

    Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to collaboratively train machine learning models without disclosing sensitive private data. Existing FL research mostly focuses on the monopoly scenario in which a single FL server selects a subset of FL clients to update their local models in each round of training. In practice, there can be multiple FL servers simultaneously trying to select clients from the same pool. In this paper, we propose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS) approach to bridge this gap. Based on Lyapunov optimization, it ensures fair allocation of high-demand FL client datasets to FL jobs in need of them, by jointly considering the current demand and the job payment bids, in order to prevent prolonged waiting. Extensive experiments comparing FairFedJS against four state-of-the-art approaches on two datasets demonstrate its significant advantages. It outperforms the best baseline by 31.9% and 1.0% on avera
    
[^25]: 扩散变分推断：扩散模型作为表达性变分后验

    Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors. (arXiv:2401.02739v1 [cs.LG])

    [http://arxiv.org/abs/2401.02739](http://arxiv.org/abs/2401.02739)

    本文提出了去噪扩散变分推断（DDVI）算法，该算法使用扩散模型作为表达性变分后验，并通过反转加噪过程在潜空间中进行扩散。该方法易于实现，兼容黑盒变分推断，并在深度潜变量模型中的任务中表现优异。

    

    我们提出了去噪扩散变分推断（DDVI），一种用扩散模型作为表达性变分后验的潜变量模型的近似推断算法。我们的方法通过辅助潜变量增加了变分后验，从而得到一个表达性的模型类，通过反转用户指定的加噪过程在潜空间中进行扩散。我们通过优化一个受到觉醒-睡眠算法启发的边际似然新下界来拟合这些模型。我们的方法易于实现（它适配了正则化的ELBO扩展），与黑盒变分推断兼容，并且表现优于基于归一化流或对抗网络的替代近似后验类别。将我们的方法应用于深度潜变量模型时，我们的方法得到了去噪扩散变分自动编码器（DD-VAE）算法。我们将该算法应用于生物学中的一个激励任务 -- 从人类基因组中推断潜在血统 -- 超过了强基线模型。

    We propose denoising diffusion variational inference (DDVI), an approximate inference algorithm for latent variable models which relies on diffusion models as expressive variational posteriors. Our method augments variational posteriors with auxiliary latents, which yields an expressive class of models that perform diffusion in latent space by reversing a user-specified noising process. We fit these models by optimizing a novel lower bound on the marginal likelihood inspired by the wake-sleep algorithm. Our method is easy to implement (it fits a regularized extension of the ELBO), is compatible with black-box variational inference, and outperforms alternative classes of approximate posteriors based on normalizing flows or adversarial networks. When applied to deep latent variable models, our method yields the denoising diffusion VAE (DD-VAE) algorithm. We use this algorithm on a motivating task in biology -- inferring latent ancestry from human genomes -- outperforming strong baselines
    
[^26]: 关于非平滑自动微分的数值可靠性：MaxPool案例研究

    On the numerical reliability of nonsmooth autodiff: a MaxPool case study. (arXiv:2401.02736v1 [cs.LG])

    [http://arxiv.org/abs/2401.02736](http://arxiv.org/abs/2401.02736)

    本文研究了涉及非平滑MaxPool操作的神经网络自动微分的数值可靠性，并发现最近的研究表明AD几乎在每个地方都与导数相符，即使在存在非平滑操作的情况下也是如此。但在实践中，AD使用的是浮点数，需要探索可能导致AD数值不正确的情况。通过研究不同选择的非平滑MaxPool雅可比矩阵对训练过程的影响，我们找到了分歧区和补偿区两个可能导致AD数值不正确的子集。

    

    本文考虑了涉及非平滑MaxPool操作的神经网络自动微分（AD）的可靠性问题。我们研究了在不同精度级别（16位、32位、64位）和卷积架构（LeNet、VGG和ResNet）以及不同数据集（MNIST、CIFAR10、SVHN和ImageNet）上的AD行为。尽管AD可能是错误的，但最近的研究表明，它在几乎每个地方都与导数相符，即使在存在非平滑操作（如MaxPool和ReLU）的情况下也是如此。另一方面，在实践中，AD使用的是浮点数（而不是实数），因此需要探索AD可能在数值上不正确的子集。这些子集包括分歧区（AD在实数上不正确）和补偿区（AD在浮点数上不正确但在实数上正确）。我们使用SGD进行训练过程，并研究了MaxPool非平滑雅可比矩阵的不同选择对训练过程的影响。

    This paper considers the reliability of automatic differentiation (AD) for neural networks involving the nonsmooth MaxPool operation. We investigate the behavior of AD across different precision levels (16, 32, 64 bits) and convolutional architectures (LeNet, VGG, and ResNet) on various datasets (MNIST, CIFAR10, SVHN, and ImageNet). Although AD can be incorrect, recent research has shown that it coincides with the derivative almost everywhere, even in the presence of nonsmooth operations (such as MaxPool and ReLU). On the other hand, in practice, AD operates with floating-point numbers (not real numbers), and there is, therefore, a need to explore subsets on which AD can be numerically incorrect. These subsets include a bifurcation zone (where AD is incorrect over reals) and a compensation zone (where AD is incorrect over floating-point numbers but correct over reals). Using SGD for the training process, we study the impact of different choices of the nonsmooth Jacobian for the MaxPool
    
[^27]: 共享主动子空间用于多元向量值函数

    Shared active subspace for multivariate vector-valued functions. (arXiv:2401.02735v1 [stat.ME])

    [http://arxiv.org/abs/2401.02735](http://arxiv.org/abs/2401.02735)

    本文提出了一种共享主动子空间的方法来处理多元向量值函数，可以通过操纵梯度或计算对称正定矩阵实现。实验结果表明，SPD级别的方法比梯度级别的方法更好，并且在正态分布情况下表现接近向量值方法。

    

    本文提出了几种方法作为基线来计算多元向量值函数的共享主动子空间。目标是最小化原始空间上的函数评估与重构空间上的函数评估之间的偏差。这些方法通过操纵梯度或从每个分量函数的梯度计算的对称正定（半正定）矩阵来获得所有分量函数的共同结构。与现有的只适用于正态分布的向量值方法不同，这些方法可以应用于任何数据无论其潜在分布。我们在五个优化问题上测试了这些方法的有效性。实验结果表明，总体而言， SPD级别的方法优于梯度级别的方法，并且在正态分布情况下接近向量值方法。有趣的是，在大多数情况下，只需取SPD矩阵之和即可。

    This paper proposes several approaches as baselines to compute a shared active subspace for multivariate vector-valued functions. The goal is to minimize the deviation between the function evaluations on the original space and those on the reconstructed one. This is done either by manipulating the gradients or the symmetric positive (semi-)definite (SPD) matrices computed from the gradients of each component function so as to get a single structure common to all component functions. These approaches can be applied to any data irrespective of the underlying distribution unlike the existing vector-valued approach that is constrained to a normal distribution. We test the effectiveness of these methods on five optimization problems. The experiments show that, in general, the SPD-level methods are superior to the gradient-level ones, and are close to the vector-valued approach in the case of a normal distribution. Interestingly, in most cases it suffices to take the sum of the SPD matrices 
    
[^28]: FedNS:一种用于联邦学习的快速绘图牛顿型算法

    FedNS: A Fast Sketching Newton-Type Algorithm for Federated Learning. (arXiv:2401.02734v1 [cs.LG])

    [http://arxiv.org/abs/2401.02734](http://arxiv.org/abs/2401.02734)

    本文提出了一种名为FedNS的快速绘图牛顿型算法，解决了牛顿型联邦学习算法在通信复杂度上的问题，通过传输草图化的平方根海森矩阵来近似集中式牛顿方法，实现了超线性的收敛速率。

    

    最近的牛顿型联邦学习算法已经证明具有与通信轮数成线性收敛。然而，由于其二次通信复杂性，通信海森矩阵通常不可行。在本文中，我们介绍了一种新的方法来解决这个问题，同时仍然实现快速收敛速率。我们提出的方法名为联邦牛顿绘图方法(FedNS)，通过传输草图化的平方根海森矩阵而不是精确的海森矩阵，来近似集中式牛顿方法。为了增强通信效率，我们将草图的大小缩小到与海森矩阵的有效维度相匹配。我们基于统计学习提供了联邦牛顿绘图方法的收敛性分析。具体而言，我们的方法首次在通信轮数的情况下达到了超线性的收敛速率。我们通过各种实验证实了我们算法的有效性，这与我们的理论相符。

    Recent Newton-type federated learning algorithms have demonstrated linear convergence with respect to the communication rounds. However, communicating Hessian matrices is often unfeasible due to their quadratic communication complexity. In this paper, we introduce a novel approach to tackle this issue while still achieving fast convergence rates. Our proposed method, named as Federated Newton Sketch methods (FedNS), approximates the centralized Newton's method by communicating the sketched square-root Hessian instead of the exact Hessian. To enhance communication efficiency, we reduce the sketch size to match the effective dimension of the Hessian matrix. We provide convergence analysis based on statistical learning for the federated Newton sketch approaches. Specifically, our approaches reach super-linear convergence rates w.r.t. the communication rounds for the first time. We validate the effectiveness of our algorithms through various experiments, which coincide with our theoretical
    
[^29]: 使用联邦学习和异步计算图神经网络预测交通流量

    Predicting Traffic Flow with Federated Learning and Graph Neural with Asynchronous Computations Network. (arXiv:2401.02723v1 [cs.LG])

    [http://arxiv.org/abs/2401.02723](http://arxiv.org/abs/2401.02723)

    本文提出了一种名为FLAGCN的深度学习方法，通过将异步图卷积网络和联邦学习相结合来提高实时交通流量预测的准确性和效率。

    

    实时交通流量预测在智能交通系统领域具有重要意义。在预测精度和计算效率之间取得平衡是一个重大挑战。本文提出了一种名为联邦学习和异步图卷积网络（FLAGCN）的新型深度学习方法。我们的框架将异步图卷积网络原理与联邦学习相结合，以提高实时交通流量预测的准确性和效率。FLAGCN模型采用了一种空时图卷积技术，可以有效地异步处理交通数据中的时空依赖关系。为了高效处理与该深度学习模型相关的计算需求，本研究使用了一种名为GraphFL的图联邦学习技术。该方法旨在促进训练过程。实验结果表明，

    Real-time traffic flow prediction holds significant importance within the domain of Intelligent Transportation Systems (ITS). The task of achieving a balance between prediction precision and computational efficiency presents a significant challenge. In this article, we present a novel deep-learning method called Federated Learning and Asynchronous Graph Convolutional Network (FLAGCN). Our framework incorporates the principles of asynchronous graph convolutional networks with federated learning to enhance the accuracy and efficiency of real-time traffic flow prediction. The FLAGCN model employs a spatial-temporal graph convolution technique to asynchronously address spatio-temporal dependencies within traffic data effectively. To efficiently handle the computational requirements associated with this deep learning model, this study used a graph federated learning technique known as GraphFL. This approach is designed to facilitate the training process. The experimental results obtained fr
    
[^30]: 利用神经ODE的高性价比FPGA实现微型Transformer模型

    A Cost-Efficient FPGA Implementation of Tiny Transformer Model using Neural ODE. (arXiv:2401.02721v1 [cs.LG])

    [http://arxiv.org/abs/2401.02721](http://arxiv.org/abs/2401.02721)

    本文提出了一种利用神经ODE作为骨干架构的高性价比FPGA实现微型Transformer模型。该模型相比于基于CNN的模型将参数大小减少了94.6%且保持准确性，适用于边缘计算。

    

    Transformer是一种具有注意机制的新兴神经网络模型。它已经被用于各种任务，并且相比于CNN和RNN取得了良好的准确性。虽然注意机制被认为是一种通用的组件，但是许多Transformer模型与基于CNN的模型相比需要大量的参数。为了减少计算复杂性，最近提出了一种混合方法，它使用ResNet作为骨干架构，并将部分卷积层替换为MHSA（多头自注意）机制。在本文中，我们通过使用神经ODE（常微分方程）而不是ResNet作为骨干架构，显著减少了这种模型的参数大小。所提出的混合模型相比于基于CNN的模型将参数大小减少了94.6%，而且没有降低准确性。接着，我们将所提出的模型部署在一台适度规模的FPGA设备上进行边缘计算。

    Transformer is an emerging neural network model with attention mechanism. It has been adopted to various tasks and achieved a favorable accuracy compared to CNNs and RNNs. While the attention mechanism is recognized as a general-purpose component, many of the Transformer models require a significant number of parameters compared to the CNN-based ones. To mitigate the computational complexity, recently, a hybrid approach has been proposed, which uses ResNet as a backbone architecture and replaces a part of its convolution layers with an MHSA (Multi-Head Self-Attention) mechanism. In this paper, we significantly reduce the parameter size of such models by using Neural ODE (Ordinary Differential Equation) as a backbone architecture instead of ResNet. The proposed hybrid model reduces the parameter size by 94.6% compared to the CNN-based ones without degrading the accuracy. We then deploy the proposed model on a modest-sized FPGA device for edge computing. To further reduce FPGA resource u
    
[^31]: 论文标题：校准攻击：针对校准性的对抗攻击框架

    Calibration Attack: A Framework For Adversarial Attacks Targeting Calibration. (arXiv:2401.02718v1 [cs.LG])

    [http://arxiv.org/abs/2401.02718](http://arxiv.org/abs/2401.02718)

    校准攻击是一种新的对抗攻击框架，通过生成和组织攻击来使受害模型失去准确校准，而不影响其原始准确性。这对模型的可信度和基于置信分数的决策构成严重威胁。我们提出了四种校准攻击形式，并对常用的对抗防御和校准方法的有效性进行了研究。

    

    我们引入了一种名为校准攻击的新对抗攻击框架，其中攻击被生成和组织以使受害模型失去准确校准，同时不改变其原始准确性，从而严重危及模型的可信度和基于其置信分数的任何决策。具体而言，我们确定了四种新型校准攻击形式：低置信攻击、高置信攻击、最大失真攻击和随机置信攻击，适用于白盒和黑盒设置。然后，我们使用全面的数据集对典型的受害模型进行了这些新型攻击的测试，证明即使只进行相对较少的查询，攻击也能造成重大的校准错误。我们还提供了详细的分析以了解校准攻击的不同方面。在此基础上，我们研究了广泛使用的对抗防御和校准方法对这些攻击类型的有效性。

    We introduce a new framework of adversarial attacks, named calibration attacks, in which the attacks are generated and organized to trap victim models to be miscalibrated without altering their original accuracy, hence seriously endangering the trustworthiness of the models and any decision-making based on their confidence scores. Specifically, we identify four novel forms of calibration attacks: underconfidence attacks, overconfidence attacks, maximum miscalibration attacks, and random confidence attacks, in both the black-box and white-box setups. We then test these new attacks on typical victim models with comprehensive datasets, demonstrating that even with a relatively low number of queries, the attacks can create significant calibration mistakes. We further provide detailed analyses to understand different aspects of calibration attacks. Building on that, we investigate the effectiveness of widely used adversarial defences and calibration methods against these types of attacks, w
    
[^32]: 通过结构知识提炼进行图级蛋白质表示学习

    Graph-level Protein Representation Learning by Structure Knowledge Refinement. (arXiv:2401.02713v1 [cs.LG])

    [http://arxiv.org/abs/2401.02713](http://arxiv.org/abs/2401.02713)

    本文提出了一种名为结构知识提炼（SKR）的新框架，通过对比学习解决了图级表示学习中存在的问题，并提出了一种自适应的数据增强策略。

    

    本文关注以非监督方式在整个图级别上学习表示。学习图级表示在诸如分子属性预测、蛋白质结构特征提取和社交网络分析等各种实际问题中起着重要作用。主流方法是利用对比学习促进图特征提取，称为图对比学习（GCL）。尽管GCL有效，但在对比学习中存在一些复杂问题，比如虚假负样本对的影响。此外，GCL中的数据增强策略对不同的图数据集适应性较弱。受到这些问题的启发，我们提出了一种新的框架，称为结构知识提炼（SKR），它利用数据结构确定一对样本是正样本还是负样本的概率。同时，我们提出了一种增强策略，能够自然地保留原始数据的语义含义，并且与我们的SKR框架兼容。

    This paper focuses on learning representation on the whole graph level in an unsupervised manner. Learning graph-level representation plays an important role in a variety of real-world issues such as molecule property prediction, protein structure feature extraction, and social network analysis. The mainstream method is utilizing contrastive learning to facilitate graph feature extraction, known as Graph Contrastive Learning (GCL). GCL, although effective, suffers from some complications in contrastive learning, such as the effect of false negative pairs. Moreover, augmentation strategies in GCL are weakly adaptive to diverse graph datasets. Motivated by these problems, we propose a novel framework called Structure Knowledge Refinement (SKR) which uses data structure to determine the probability of whether a pair is positive or negative. Meanwhile, we propose an augmentation strategy that naturally preserves the semantic meaning of the original data and is compatible with our SKR frame
    
[^33]: TripleSurv：适应时间三元组坐标损失用于生存分析

    TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis. (arXiv:2401.02708v1 [cs.LG])

    [http://arxiv.org/abs/2401.02708](http://arxiv.org/abs/2401.02708)

    本文提出了一种适应时间的三元组坐标损失函数TripleSurv，通过引入样本对之间的生存时间差异来鼓励模型量化排名相对风险，从而提高生存分析的准确性。

    

    生存分析中的一个核心挑战是对被截尾的事件时间数据进行建模，其中感兴趣的事件可能是死亡、失败或特定事件的发生。过去的研究表明，排序损失和最大似然估计（MLE）损失函数被广泛应用于生存分析。然而，排序损失仅关注生存时间排名，不考虑样本对于确切生存时间值的潜在影响。此外，MLE是无界的且容易受到异常值（例如，被截尾数据）的影响，这可能导致建模性能较差。为了处理学习过程的复杂性并利用有价值的生存时间值，我们提出了一种适应时间三元组坐标损失函数TripleSurv，通过将样本对之间的生存时间差异引入排序中，以鼓励模型量化排名相对风险，最终提高预测准确性。

    A core challenge in survival analysis is to model the distribution of censored time-to-event data, where the event of interest may be a death, failure, or occurrence of a specific event. Previous studies have showed that ranking and maximum likelihood estimation (MLE)loss functions are widely-used for survival analysis. However, ranking loss only focus on the ranking of survival time and does not consider potential effect of samples for exact survival time values. Furthermore, the MLE is unbounded and easily subject to outliers (e.g., censored data), which may cause poor performance of modeling. To handle the complexities of learning process and exploit valuable survival time values, we propose a time-adaptive coordinate loss function, TripleSurv, to achieve adaptive adjustments by introducing the differences in the survival time between sample pairs into the ranking, which can encourage the model to quantitatively rank relative risk of pairs, ultimately enhancing the accuracy of predi
    
[^34]: 基于感知动作的可解释图神经网络在SAR图像上进行人类决策的研究

    PAHD: Perception-Action based Human Decision Making using Explainable Graph Neural Networks on SAR Images. (arXiv:2401.02687v1 [cs.CV])

    [http://arxiv.org/abs/2401.02687](http://arxiv.org/abs/2401.02687)

    本论文研究了在SAR图像上利用可解释图神经网络进行基于感知动作的人类决策，旨在提供详细信息帮助指挥官做出准确决策。

    

    合成孔径雷达（SAR）图像通常在军事应用中用于自动目标识别（ATR）。机器学习方法，如卷积神经网络（CNN）和图神经网络（GNN），常用于识别地面目标，包括战车、人员运输车和导弹发射器。确定车辆的类别（如BRDM2坦克、BMP2坦克、BTR60坦克和BTR70坦克）至关重要，因为它可以帮助确定目标物体是盟友还是敌人。虽然机器学习算法提供了对识别目标的反馈，最终决策却掌握在指挥官手中。因此，提供详细信息与识别到的目标一起可以极大地影响他们的行动。这些详细信息包括对分类有贡献的SAR图像特征、分类置信度以及被识别为不同目标类型或类别的概率。

    Synthetic Aperture Radar (SAR) images are commonly utilized in military applications for automatic target recognition (ATR). Machine learning (ML) methods, such as Convolutional Neural Networks (CNN) and Graph Neural Networks (GNN), are frequently used to identify ground-based objects, including battle tanks, personnel carriers, and missile launchers. Determining the vehicle class, such as the BRDM2 tank, BMP2 tank, BTR60 tank, and BTR70 tank, is crucial, as it can help determine whether the target object is an ally or an enemy. While the ML algorithm provides feedback on the recognized target, the final decision is left to the commanding officers. Therefore, providing detailed information alongside the identified target can significantly impact their actions. This detailed information includes the SAR image features that contributed to the classification, the classification confidence, and the probability of the identified object being classified as a different object type or class. W
    
[^35]: 超越忠诚度：解释基于学习的漏洞检测器的漏洞定位

    Beyond Fidelity: Explaining Vulnerability Localization of Learning-based Detectors. (arXiv:2401.02686v1 [cs.CR])

    [http://arxiv.org/abs/2401.02686](http://arxiv.org/abs/2401.02686)

    这项研究评估了基于图和序列表示的十种漏洞检测器解释方法的性能，发现单纯的忠诚度评估不足够。

    

    近年来，基于深度学习模型的漏洞检测器证明了其有效性。然而，这些检测器决策过程的不透明性使安全分析师难以理解。为了解决这个问题，已经提出了各种解释方法来解释预测结果，通过突出重要特征，在计算机视觉和自然语言处理等其他领域已经证明是有效的。不幸的是，这些解释方法对于漏洞检测器学习和理解的细粒度的漏洞相关代码行等关键特征的深入评估仍然缺乏。在本研究中，我们首先通过两个定量指标——忠诚度和漏洞行覆盖率——评估了基于图和序列表示的十种漏洞检测器解释方法的性能。我们的结果显示，仅仅依靠忠诚度是不足够的。

    Vulnerability detectors based on deep learning (DL) models have proven their effectiveness in recent years. However, the shroud of opacity surrounding the decision-making process of these detectors makes it difficult for security analysts to comprehend. To address this, various explanation approaches have been proposed to explain the predictions by highlighting important features, which have been demonstrated effective in other domains such as computer vision and natural language processing. Unfortunately, an in-depth evaluation of vulnerability-critical features, such as fine-grained vulnerability-related code lines, learned and understood by these explanation approaches remains lacking. In this study, we first evaluate the performance of ten explanation approaches for vulnerability detectors based on graph and sequence representations, measured by two quantitative metrics including fidelity and vulnerability line coverage rate. Our results show that fidelity alone is not sufficient f
    
[^36]: 用于3D分子生成的几何便利去噪扩散模型

    Geometric-Facilitated Denoising Diffusion Model for 3D Molecule Generation. (arXiv:2401.02683v1 [cs.LG])

    [http://arxiv.org/abs/2401.02683](http://arxiv.org/abs/2401.02683)

    这项研究提出了一个几何便利的去噪扩散模型，用于解决3D分子生成中的多体原子关系建模和键的预测问题。

    

    去噪扩散模型在多个研究领域显示出巨大潜力。现有的面向全新3D分子生成的扩散基于生成方法面临两个主要挑战。由于分子中的大多数重原子通过单键与多个原子相连，仅使用成对距离来模拟分子几何是不足的。因此，第一个挑战涉及提出一个能够捕捉复杂的多体原子间关系和学习高质量特征的有效神经网络作为去噪内核。由于图的离散性质，面对分子的主流扩散方法严重依赖预定义规则，并以间接方式生成边缘。第二个挑战涉及将分子的生成与扩散相结合，并准确预测键的存在。在我们的研究中，我们认为在扩散过程中更新分子构型的迭代方式与分子动力学一致，并引入了新的方法来解决两个挑战。

    Denoising diffusion models have shown great potential in multiple research areas. Existing diffusion-based generative methods on de novo 3D molecule generation face two major challenges. Since majority heavy atoms in molecules allow connections to multiple atoms through single bonds, solely using pair-wise distance to model molecule geometries is insufficient. Therefore, the first one involves proposing an effective neural network as the denoising kernel that is capable to capture complex multi-body interatomic relationships and learn high-quality features. Due to the discrete nature of graphs, mainstream diffusion-based methods for molecules heavily rely on predefined rules and generate edges in an indirect manner. The second challenge involves accommodating molecule generation to diffusion and accurately predicting the existence of bonds. In our research, we view the iterative way of updating molecule conformations in diffusion process is consistent with molecular dynamics and introd
    
[^37]: 同质性相关：自适应混合图过滤器用于多视图图聚类

    Homophily-Related: Adaptive Hybrid Graph Filter for Multi-View Graph Clustering. (arXiv:2401.02682v1 [cs.LG])

    [http://arxiv.org/abs/2401.02682](http://arxiv.org/abs/2401.02682)

    该论文提出了一个自适应混合图过滤器用于多视图图聚类。现有方法仅适用于同质性图，而该研究针对广泛存在的异质性图提出了一个解决方案。通过与图的同质性程度密切相关的图过滤，该方法可以充分利用低频和高频信息来学习节点表示。

    

    最近对图数据的关注越来越多，多视图图聚类成为研究的热点领域。大部分现有方法仅适用于同质性图，然而广泛存在的现实世界图数据很难满足同质性假设，即连接的节点倾向于属于同一个类。一些研究指出，在异质性图上表现不佳实际上是因为传统的图神经网络（GNNs），本质上是低通滤波器，丢弃了除图上低频信息之外的其他信息。然而，在某些图上，特别是异质性图上，忽略高频信息，只关注低频信息阻碍了节点表示的学习。为了突破这一限制，我们的动机是进行与给定图的同质性程度密切相关的图过滤，旨在充分利用低频和高频信息来学习节点表示。

    Recently there is a growing focus on graph data, and multi-view graph clustering has become a popular area of research interest. Most of the existing methods are only applicable to homophilous graphs, yet the extensive real-world graph data can hardly fulfill the homophily assumption, where the connected nodes tend to belong to the same class. Several studies have pointed out that the poor performance on heterophilous graphs is actually due to the fact that conventional graph neural networks (GNNs), which are essentially low-pass filters, discard information other than the low-frequency information on the graph. Nevertheless, on certain graphs, particularly heterophilous ones, neglecting high-frequency information and focusing solely on low-frequency information impedes the learning of node representations. To break this limitation, our motivation is to perform graph filtering that is closely related to the homophily degree of the given graph, with the aim of fully leveraging both low-
    
[^38]: LMaaS：探索面向通信的大型模型作为服务的定价策略

    LMaaS: Exploring Pricing Strategy of Large Model as a Service for Communication. (arXiv:2401.02675v1 [cs.NI])

    [http://arxiv.org/abs/2401.02675](http://arxiv.org/abs/2401.02675)

    本文提出了Large Model as a Service (LMaaS)作为智能通信中的定价模式，并通过Stackelberg博弈解决了定价优化问题。

    

    预计下一代通信将是智能通信，可取代传统的符号通信，其中高度压缩的语义信息将考虑源和通道，并以高效率提取和传输。近年来，受欢迎的大型模型如GPT4和增强学习技术为智能通信奠定了坚实的基础，并促使其在不久的将来得以实际部署。鉴于多模态大型语言模型的“训练一次，广泛使用”的特点，我们认为按需付费的服务模式将适用于这种背景，称为Large Model as a Service (LMaaS)。然而，由于异构和动态的客户环境，交易和定价问题非常复杂，使得寻找即时解决方案的定价优化问题具有挑战性。本文旨在填补这一空白，将LMaaS市场交易形式化为Stackelberg博弈。

    The next generation of communication is envisioned to be intelligent communication, that can replace traditional symbolic communication, where highly condensed semantic information considering both source and channel will be extracted and transmitted with high efficiency. The recent popular large models such as GPT4 and the boosting learning techniques lay a solid foundation for the intelligent communication, and prompt the practical deployment of it in the near future. Given the characteristics of "training once and widely use" of those multimodal large language models, we argue that a pay-as-you-go service mode will be suitable in this context, referred to as Large Model as a Service (LMaaS). However, the trading and pricing problem is quite complex with heterogeneous and dynamic customer environments, making the pricing optimization problem challenging in seeking on-hand solutions. In this paper, we aim to fill this gap and formulate the LMaaS market trading as a Stackelberg game wi
    
[^39]: 在生成式人工智能与边缘智能相遇时实现综合微调和推理

    Towards Integrated Fine-tuning and Inference when Generative AI meets Edge Intelligence. (arXiv:2401.02668v1 [cs.DC])

    [http://arxiv.org/abs/2401.02668](http://arxiv.org/abs/2401.02668)

    本文提出了GAI-oriented synthetical network (GaisNet)，这是一个协作的云端边缘智能框架，通过利用无数据知识中继来缓解矛盾，实现了GAI的循环模型微调和任务推理，从而实现了GAI和EI的互利关系。

    

    高性能的生成式人工智能（GAI）代表了计算智能的最新演变，而未来的6G网络的发展潜力也使得边缘智能（EI）充满了发展机会。GAI和EI的必然相遇可以释放新的机会，GAI基于大量计算资源和大规模未标记的语料库的预训练可以为EI提供强大的基础知识，而EI可以利用碎片化的计算资源来汇总个性化的知识为GAI所用。然而，这些自然的矛盾特点给直接的知识共享提出了重大挑战。为了应对这个问题，在本文中，我们提出了面向GAI的综合网络（GaisNet），这是一个协作的云端边缘智能框架，通过利用无数据知识中继来缓解矛盾，其中双向的知识流使得GAI的循环模型微调和任务推理成为可能，实现了GAI和EI的互利关系。

    The high-performance generative artificial intelligence (GAI) represents the latest evolution of computational intelligence, while the blessing of future 6G networks also makes edge intelligence (EI) full of development potential. The inevitable encounter between GAI and EI can unleash new opportunities, where GAI's pre-training based on massive computing resources and large-scale unlabeled corpora can provide strong foundational knowledge for EI, while EI can harness fragmented computing resources to aggregate personalized knowledge for GAI. However, the natural contradictory features pose significant challenges to direct knowledge sharing. To address this, in this paper, we propose the GAI-oriented synthetical network (GaisNet), a collaborative cloud-edge-end intelligence framework that buffers contradiction leveraging data-free knowledge relay, where the bidirectional knowledge flow enables GAI's virtuous-cycle model fine-tuning and task inference, achieving mutualism between GAI an
    
[^40]: 零样本深度学习实现微气候预测

    Zero-shot Microclimate Prediction with Deep Learning. (arXiv:2401.02665v1 [cs.LG])

    [http://arxiv.org/abs/2401.02665](http://arxiv.org/abs/2401.02665)

    该论文提出了一种零样本学习方法，利用从其他地理位置提取的知识，实现了对新的和未监测到的地点的各种气候测量的预测。

    

    天气站数据是气候预测的宝贵资源，但在偏远地区其可靠性可能受限。此外，进行本地预测通常依赖于可能无法访问的传感器数据，这对于新的、以前未监测到的地点来说尤其困难。为了应对这些挑战，我们提出了一种新颖的零样本学习方法，旨在预测新的、未监测到的地点的各种气候测量。我们的方法通过利用从其他地理位置提取的知识，超越了传统的天气预报技术，预测微气候变量。

    Weather station data is a valuable resource for climate prediction, however, its reliability can be limited in remote locations. To compound the issue, making local predictions often relies on sensor data that may not be accessible for a new, previously unmonitored location. In response to these challenges, we propose a novel zero-shot learning approach designed to forecast various climate measurements at new and unmonitored locations. Our method surpasses conventional weather forecasting techniques in predicting microclimate variables by leveraging knowledge extracted from other geographic locations.
    
[^41]: 用于图神经网络链接预测任务的后门攻击

    A backdoor attack against link prediction tasks with graph neural networks. (arXiv:2401.02663v1 [cs.LG])

    [http://arxiv.org/abs/2401.02663](http://arxiv.org/abs/2401.02663)

    本文研究了一种针对图神经网络链接预测任务的后门攻击方法，发现GNN模型容易受到后门攻击，提出了针对该任务的后门攻击方式。

    

    图神经网络（GNN）是一类能够处理图结构数据的深度学习模型，在各种实际应用中表现出显著的性能。最近的研究发现，GNN模型容易受到后门攻击。当具体的模式（称为后门触发器，例如子图、节点等）出现在输入数据中时，嵌入在GNN模型中的后门会被激活，将输入数据误分类为攻击者指定的目标类标签，而当输入中没有后门触发器时，嵌入在GNN模型中的后门不会被激活，模型正常工作。后门攻击具有极高的隐蔽性，给GNN模型带来严重的安全风险。目前，对GNN的后门攻击研究主要集中在图分类和节点分类等任务上，对链接预测任务的后门攻击研究较少。在本文中，我们提出一种后门攻击方法。

    Graph Neural Networks (GNNs) are a class of deep learning models capable of processing graph-structured data, and they have demonstrated significant performance in a variety of real-world applications. Recent studies have found that GNN models are vulnerable to backdoor attacks. When specific patterns (called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input data, the backdoor embedded in the GNN models is activated, which misclassifies the input data into the target class label specified by the attacker, whereas when there are no backdoor triggers in the input, the backdoor embedded in the GNN models is not activated, and the models work normally. Backdoor attacks are highly stealthy and expose GNN models to serious security risks. Currently, research on backdoor attacks against GNNs mainly focus on tasks such as graph classification and node classification, and backdoor attacks against link prediction tasks are rarely studied. In this paper, we propose a backdoor a
    
[^42]: 护士参与型人工智能在临床试验中针对2型糖尿病的精准管理：利用转移学习的预测数字孪生体

    Nurse-in-the-Loop Artificial Intelligence for Precision Management of Type 2 Diabetes in a Clinical Trial Utilizing Transfer-Learned Predictive Digital Twin. (arXiv:2401.02661v1 [cs.LG])

    [http://arxiv.org/abs/2401.02661](http://arxiv.org/abs/2401.02661)

    这项研究开发了一种护士参与型人工智能系统，利用转移学习的预测数字孪生体来实现针对2型糖尿病患者的精准管理。在临床试验中，该系统通过结合各种数据源的模式和护士的专业知识，提供个性化的治疗反馈，以改善患者的治疗效果。

    

    背景：2型糖尿病（T2D）是一种常见的慢性疾病，其会增加严重健康并对生活质量产生负面影响。考虑到个体特征和生活方式对治疗计划和患者结果的影响，开发精确和个性化的管理策略至关重要。人工智能（AI）结合来自各种数据源的模式和护士的专业知识，可以实现最佳护理效果。方法：这是一项针对T2D患者（n = 20，年龄= 57+-10）的为期6个月的附属研究。参与者随机分配为干预组（AI，n = 10）和对照组，在最后三个月中干预组每天接收AI生成的个体化反馈，而对照组不接收每日反馈（非AI，n = 10）。研究开发了一种在线护士参与型预测控制模型（ONLC），该模型利用预测数字孪生体（PDT）。PDT是通过基于转移学习的人工神经网络训练的。

    Background: Type 2 diabetes (T2D) is a prevalent chronic disease with a significant risk of serious health complications and negative impacts on the quality of life. Given the impact of individual characteristics and lifestyle on the treatment plan and patient outcomes, it is crucial to develop precise and personalized management strategies. Artificial intelligence (AI) provides great promise in combining patterns from various data sources with nurses' expertise to achieve optimal care. Methods: This is a 6-month ancillary study among T2D patients (n = 20, age = 57 +- 10). Participants were randomly assigned to an intervention (AI, n=10) group to receive daily AI-generated individualized feedback or a control group without receiving the daily feedback (non-AI, n=10) in the last three months. The study developed an online nurse-in-the-loop predictive control (ONLC) model that utilizes a predictive digital twin (PDT). The PDT was developed using a transfer-learning-based Artificial Neura
    
[^43]: GTA: 从物体中心的表示中引导空间注意力的转移

    GTA: Guided Transfer of Spatial Attention from Object-Centric Representations. (arXiv:2401.02656v1 [cs.CV])

    [http://arxiv.org/abs/2401.02656](http://arxiv.org/abs/2401.02656)

    本论文提出了一种名为GTA的正则化方法，它通过在源模型和目标模型之间的自注意力映射中引导空间注意力的转移，可以解决在ViT中转移表示容易过拟合和失去有价值特性的问题，实验证明该方法能够稳定提高准确性。

    

    利用训练良好的表示在转移学习中通常会得到更好的性能和更快的收敛速度，与从头开始训练相比。然而，即使这样的好的表示被转移，模型仍然很容易过拟合有限的训练数据集，并且失去了转移表示的有价值的特性。这种现象在 ViT 中更为严重，因为它的归纳偏置较低。通过使用 ViT 中的注意力映射进行实验分析，我们观察到当在小数据集上训练时，丰富的表示会退化。受到这一发现的启发，我们提出了一种名为 Guided Transfer of spatial Attention (GTA) 的新颖而简单的 ViT 正则化方法。我们提出的方法通过显式的正则化在源模型和目标模型之间的自注意力映射。通过这种显式的正则化，目标模型可以充分利用与物体定位属性相关的知识。我们的实验结果表明，提出的 GTA 方法能够稳定提高准确性。

    Utilizing well-trained representations in transfer learning often results in superior performance and faster convergence compared to training from scratch. However, even if such good representations are transferred, a model can easily overfit the limited training dataset and lose the valuable properties of the transferred representations. This phenomenon is more severe in ViT due to its low inductive bias. Through experimental analysis using attention maps in ViT, we observe that the rich representations deteriorate when trained on a small dataset. Motivated by this finding, we propose a novel and simple regularization method for ViT called Guided Transfer of spatial Attention (GTA). Our proposed method regularizes the self-attention maps between the source and target models. A target model can fully exploit the knowledge related to object localization properties through this explicit regularization. Our experimental results show that the proposed GTA consistently improves the accuracy
    
[^44]: EV在智能电网中的需求响应智能调度的基于深度强化学习的研究

    A Deep Q-Learning based Smart Scheduling of EVs for Demand Response in Smart Grids. (arXiv:2401.02653v1 [cs.LG])

    [http://arxiv.org/abs/2401.02653](http://arxiv.org/abs/2401.02653)

    本研究提出了一种基于深度强化学习的智能调度方法，用于解决电动车在智能电网中的需求响应问题。通过调度电动车的充放电活动，以与配电系统操作员提供的目标能量配置文件一致，可以实现局部网络的平衡和优化。

    

    经济和政策因素推动了电动车(EV)的不断增加和使用。然而，尽管EV是一种比燃油车更清洁的替代品，但由于电力需求增加和使用时间导致的负面影响，EV对微电网设备寿命和能源平衡产生了负面影响。在我们看来，电网管理应该利用EV的调度灵活性，通过积极参与需求响应计划来支持局部网络平衡。在本文中，我们提出了一种基于深度强化学习的无模型解决方案，用于将EV的充电和放电活动调度到微电网中，以与配电系统操作员提供的目标能量配置文件一致。我们改进了Bellman方程，通过特定的奖励评估状态的价值，使用神经网络估计可用动作的Q值，并使用epsilon-greedy算法平衡开发和探索。

    Economic and policy factors are driving the continuous increase in the adoption and usage of electrical vehicles (EVs). However, despite being a cleaner alternative to combustion engine vehicles, EVs have negative impacts on the lifespan of microgrid equipment and energy balance due to increased power demand and the timing of their usage. In our view grid management should leverage on EVs scheduling flexibility to support local network balancing through active participation in demand response programs. In this paper, we propose a model-free solution, leveraging Deep Q-Learning to schedule the charging and discharging activities of EVs within a microgrid to align with a target energy profile provided by the distribution system operator. We adapted the Bellman Equation to assess the value of a state based on specific rewards for EV scheduling actions and used a neural network to estimate Q-values for available actions and the epsilon-greedy algorithm to balance exploitation and explorati
    
[^45]: 自适应折扣训练时间攻击

    Adaptive Discounting of Training Time Attacks. (arXiv:2401.02652v1 [cs.LG])

    [http://arxiv.org/abs/2401.02652](http://arxiv.org/abs/2401.02652)

    本研究展示了在存在环境动态和相对于受害者目标的非最优性时，即使目标行为无法被采纳，仍然可能进行C-TTA。我们开发了一种gammaDDPG算法来学习这种更强版本的C-TTA，并根据受害者当前的行为动态改变攻击策略规划时间。

    

    在强化学习解决方案中，训练时间攻击（TTAs）是最阴险的攻击之一，可以在学习到的行为中制造漏洞和后门。现在已经有了不仅仅是简单破坏的建设性TTAs（C-TTAs），攻击者可以强制训练RL agent（受害者）表现出特定的目标行为。然而，即使是最先进的C-TTAs也只针对那些如果不因环境动态的一个特定特征被利用，受害者本可以自然地采纳的目标行为。在这项工作中，我们展示了即使目标行为由于环境动态和相对于受害者目标的非最优性而无法采纳，C-TTA也是可能的。为了在这种情况下找到高效的攻击方法，我们开发了一种特定的DDPG算法，称为gammaDDPG，用于学习这种更强版本的C-TTA。gammaDDPG根据受害者当前的行为动态改变攻击策略规划时间。

    Among the most insidious attacks on Reinforcement Learning (RL) solutions are training-time attacks (TTAs) that create loopholes and backdoors in the learned behaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are now available, where the attacker forces a specific, target behaviour upon a training RL agent (victim). However, even state-of-the-art C-TTAs focus on target behaviours that could be naturally adopted by the victim if not for a particular feature of the environment dynamics, which C-TTAs exploit. In this work, we show that a C-TTA is possible even when the target behaviour is un-adoptable due to both environment dynamics as well as non-optimality with respect to the victim objective(s). To find efficient attacks in this context, we develop a specialised flavour of the DDPG algorithm, which we term gammaDDPG, that learns this stronger version of C-TTA. gammaDDPG dynamically alters the attack policy planning horizon based on the victim's current behaviour
    
[^46]: 通过MCMC改进高维贝叶斯优化的样本效率

    Improving sample efficiency of high dimensional Bayesian optimization with MCMC. (arXiv:2401.02650v1 [cs.LG])

    [http://arxiv.org/abs/2401.02650](http://arxiv.org/abs/2401.02650)

    本文提出了一种基于马尔科夫链蒙特卡罗的方法，用于改进高维贝叶斯优化的样本效率。实验结果表明，该方法在高维顺序优化和强化学习任务中表现优于现有方法。

    

    高维空间中的顺序优化方法经常面临维度诅咒。目前的高斯过程方法在跟踪高斯过程后验的计算复杂度上仍存在负担，并且需要将优化问题划分为小区域以确保探索或假设一个潜在的低维结构。我们提出了一种基于马尔科夫链蒙特卡罗的新方法，通过将候选点转移到更有希望的位置来有效地从近似后验中采样。我们在高斯过程汤普森采样设置下提供了其收敛的理论保证。我们还通过实验表明，我们算法的Metropolis-Hastings版本和Langevin Dynamics版本在高维顺序优化和强化学习基准测试中均优于现有方法。

    Sequential optimization methods are often confronted with the curse of dimensionality in high-dimensional spaces. Current approaches under the Gaussian process framework are still burdened by the computational complexity of tracking Gaussian process posteriors and need to partition the optimization problem into small regions to ensure exploration or assume an underlying low-dimensional structure. With the idea of transiting the candidate points towards more promising positions, we propose a new method based on Markov Chain Monte Carlo to efficiently sample from an approximated posterior. We provide theoretical guarantees of its convergence in the Gaussian process Thompson sampling setting. We also show experimentally that both the Metropolis-Hastings and the Langevin Dynamics version of our algorithm outperform state-of-the-art methods in high-dimensional sequential optimization and reinforcement learning benchmarks.
    
[^47]: 简单的分层扩散规划

    Simple Hierarchical Planning with Diffusion. (arXiv:2401.02644v1 [cs.LG])

    [http://arxiv.org/abs/2401.02644](http://arxiv.org/abs/2401.02644)

    本研究引入了分层扩散规划(Hierarchical Diffuser)，结合了分层和基于扩散的规划的优点，提出了一种简单、快速且有效的规划方法。通过在较高层面上采用“跳跃”规划策略，我们的模型能够具有较大的感受野且计算成本较低，同时跳跃的子目标还能指导低层规划器，在微调阶段进一步提高方法的效果。在实验评估中，我们的方法在性能和效率方面表现出卓越的结果。

    

    基于扩散的生成方法在建模离线数据集中的轨迹方面已被证明是有效的。然而，它们常常面临计算挑战，并且在泛化能力方面可能不足，特别是在捕捉长时任务的时间抽象方面。为了克服这个问题，我们引入了分层扩散规划，这是一种简单、快速、但令人惊讶地有效的规划方法，结合了分层和基于扩散的规划的优点。我们的模型在较高层面上采用了“跳跃”规划策略，使其具有较大的感受野但计算成本较低 - 这对于基于扩散的规划方法来说是一个关键因素，我们已经经验性地验证了这一点。此外，跳跃的子目标指导我们的低层规划器，促进了一个微调阶段，并进一步提高了我们方法的效果。我们对标准离线强化学习基准进行了经验评估，展示了我们方法在性能和效率方面的优越表现。

    Diffusion-based generative methods have proven effective in modeling trajectories with offline datasets. However, they often face computational challenges and can falter in generalization, especially in capturing temporal abstractions for long-horizon tasks. To overcome this, we introduce the Hierarchical Diffuser, a simple, fast, yet surprisingly effective planning method combining the advantages of hierarchical and diffusion-based planning. Our model adopts a "jumpy" planning strategy at the higher level, which allows it to have a larger receptive field but at a lower computational cost -- a crucial factor for diffusion-based planning methods, as we have empirically verified. Additionally, the jumpy sub-goals guide our low-level planner, facilitating a fine-tuning stage and further improving our approach's effectiveness. We conducted empirical evaluations on standard offline reinforcement learning benchmarks, demonstrating our method's superior performance and efficiency in terms of 
    
[^48]: 机器学习中的模型无关解释框架：NBA体育的比较研究

    Model-Agnostic Interpretation Framework in Machine Learning: A Comparative Study in NBA Sports. (arXiv:2401.02630v1 [cs.LG])

    [http://arxiv.org/abs/2401.02630](http://arxiv.org/abs/2401.02630)

    本研究提出了一个创新的模型无关解释框架，通过模块化操作和融合多种解释技术实现对复杂模型的解释而不损害性能。

    

    近年来，机器学习领域取得了巨大的进步，深度学习模型在各种任务上表现出色。然而，这些模型通常缺乏解释性，它们作为不透明的“黑箱”操作，使其决策背后的理由变得难以理解。这种缺乏透明度可以限制对模型基本原理的理解，并阻碍其在敏感领域（如医疗保健或金融）的部署。为了解决这一挑战，我们的研究团队提出了一种创新的框架，旨在协调模型性能和解释性之间的权衡。我们的方法围绕着对高维数据的模块化操作，实现端到端处理同时保持解释性。通过融合多种解释技术和模块化数据处理，我们的框架能够揭示复杂模型的决策过程，而不损害其性能。

    The field of machine learning has seen tremendous progress in recent years, with deep learning models delivering exceptional performance across a range of tasks. However, these models often come at the cost of interpretability, as they operate as opaque "black boxes" that obscure the rationale behind their decisions. This lack of transparency can limit understanding of the models' underlying principles and impede their deployment in sensitive domains, such as healthcare or finance. To address this challenge, our research team has proposed an innovative framework designed to reconcile the trade-off between model performance and interpretability. Our approach is centered around modular operations on high-dimensional data, which enable end-to-end processing while preserving interpretability. By fusing diverse interpretability techniques and modularized data processing, our framework sheds light on the decision-making processes of complex models without compromising their performance. We h
    
[^49]: 神经因果抽象

    Neural Causal Abstractions. (arXiv:2401.02602v1 [cs.LG])

    [http://arxiv.org/abs/2401.02602](http://arxiv.org/abs/2401.02602)

    本文提出了一种新的神经因果抽象方法，通过聚类变量和其域，用于解决真实因果推断任务中的挑战，并通过神经因果模型实现了学习和应用。

    

    人类理解世界中的因果关系以及将信息压缩成抽象概念的能力是人类智慧的两个标志性特征。这两个主题在文献中被统称为因果抽象理论同时进行研究。在实践中，如何在真实的因果推断任务中充分利用抽象理论仍然是一个开放的问题，因为真实机制是未知的，只有有限的数据可用。在本文中，我们通过对变量及其域进行聚类，开发了一种新的因果抽象家族。这种方法改进和概括了之前的抽象概念，以更好地适应Pearl的因果层次结构引发的个体因果分布。我们证明了在实际场景中通过神经因果模型（Xia等，2021）可以学得这样的抽象概念，从而能够利用深度学习技术解决各种具有挑战性的因果推断任务。

    The abilities of humans to understand the world in terms of cause and effect relationships, as well as to compress information into abstract concepts, are two hallmark features of human intelligence. These two topics have been studied in tandem in the literature under the rubric of causal abstractions theory. In practice, it remains an open problem how to best leverage abstraction theory in real-world causal inference tasks, where the true mechanisms are unknown and only limited data is available. In this paper, we develop a new family of causal abstractions by clustering variables and their domains. This approach refines and generalizes previous notions of abstractions to better accommodate individual causal distributions that are spawned by Pearl's causal hierarchy. We show that such abstractions are learnable in practical settings through Neural Causal Models (Xia et al., 2021), enabling the use of the deep learning toolkit to solve various challenging causal inference tasks -- iden
    
[^50]: 保证非凸分解方法用于张量列车恢复的研究

    Guaranteed Nonconvex Factorization Approach for Tensor Train Recovery. (arXiv:2401.02592v1 [stat.ML])

    [http://arxiv.org/abs/2401.02592](http://arxiv.org/abs/2401.02592)

    本研究提出了一种保证非凸分解方法用于张量列车恢复的新方法。该方法通过优化左正交TT格式来实现正交结构，并使用黎曼梯度下降算法来优化因子。我们证明了该方法具有局部线性收敛性，并且在满足受限等谱性质的条件下能够以线性速率收敛到真实张量。

    

    在本文中，我们首次提供了对于分解方法的收敛性保证。具体而言，为了避免尺度歧义并便于理论分析，我们优化所谓的左正交TT格式，强制使大部分因子彼此正交。为了确保正交结构，我们利用黎曼梯度下降（RGD）来优化Stiefel流形上的这些因子。我们首先深入研究TT分解问题，并建立了RGD的局部线性收敛性。值得注意的是，随着张量阶数的增加，收敛速率仅经历线性下降。然后，我们研究了感知问题，即从线性测量中恢复TT格式张量。假设感知算子满足受限等谱性质（RIP），我们证明在适当的初始化下，通过谱初始化获得，RGD也会以线性速率收敛到真实张量。此外，我们扩展了我们的研究。

    In this paper, we provide the first convergence guarantee for the factorization approach. Specifically, to avoid the scaling ambiguity and to facilitate theoretical analysis, we optimize over the so-called left-orthogonal TT format which enforces orthonormality among most of the factors. To ensure the orthonormal structure, we utilize the Riemannian gradient descent (RGD) for optimizing those factors over the Stiefel manifold. We first delve into the TT factorization problem and establish the local linear convergence of RGD. Notably, the rate of convergence only experiences a linear decline as the tensor order increases. We then study the sensing problem that aims to recover a TT format tensor from linear measurements. Assuming the sensing operator satisfies the restricted isometry property (RIP), we show that with a proper initialization, which could be obtained through spectral initialization, RGD also converges to the ground-truth tensor at a linear rate. Furthermore, we expand our 
    
[^51]: 通过生成合成数据来实现对不均衡数据上的深度学习的最大后验比率的研究

    Synthetic Information towards Maximum Posterior Ratio for deep learning on Imbalanced Data. (arXiv:2401.02591v1 [cs.LG])

    [http://arxiv.org/abs/2401.02591](http://arxiv.org/abs/2401.02591)

    本研究提出了一种通过生成合成数据来平衡类别不平衡数据的技术，优先平衡信息丰富区域，并通过优化类别后验比率来最大化在正确的类别区域生成合成样本的概率。实验结果表明该技术在提升深度学习模型方面具有卓越性能。

    

    本研究检验了类别不平衡数据对深度学习模型的影响，并提出了一种通过生成少数类别的合成数据来平衡数据的技术。与基于随机过采样的方法不同，我们的方法优先平衡信息丰富区域，通过识别高熵样本。生成适当位置的合成数据可以提高机器学习算法的准确性和效率，而生成位置不当的合成数据可能导致更高的误分类率。我们介绍了一种通过优化类别后验比率来最大化在正确的类别区域生成合成样本的概率的算法。此外，为了保持数据拓扑，合成数据在每个少数类别样本的邻域内生成。我们在41个数据集上的实验结果表明了我们的技术在提升深度学习模型方面的卓越性能。

    This study examines the impact of class-imbalanced data on deep learning models and proposes a technique for data balancing by generating synthetic data for the minority class. Unlike random-based oversampling, our method prioritizes balancing the informative regions by identifying high entropy samples. Generating well-placed synthetic data can enhance machine learning algorithms accuracy and efficiency, whereas poorly-placed ones may lead to higher misclassification rates. We introduce an algorithm that maximizes the probability of generating a synthetic sample in the correct region of its class by optimizing the class posterior ratio. Additionally, to maintain data topology, synthetic data are generated within each minority sample's neighborhood. Our experimental results on forty-one datasets demonstrate the superior performance of our technique in enhancing deep-learning models.
    
[^52]: 使用样本权重进行分布偏斜数据的联邦学习

    Federated Learning for distribution skewed data using sample weights. (arXiv:2401.02586v1 [cs.LG])

    [http://arxiv.org/abs/2401.02586](http://arxiv.org/abs/2401.02586)

    本论文研究了在分布偏斜数据情况下如何通过使用样本权重来改进联邦学习性能。主要思路是通过调整客户端分布使其更接近全局分布，从而实现机器学习模型更快地收敛和更高的准确性。

    

    联邦学习中最具挑战性的问题之一是数据通常不是独立同分布的（nonIID）。客户端被期望贡献相同类型的数据并从一个全局分布中抽取数据。然而，数据往往以不同的方式从不同资源收集。因此，客户端之间的数据分布可能与底层全局分布不同。这就产生了权重发散问题，并降低了联邦学习的性能。该工作侧重于改善客户端之间分布偏斜的联邦学习性能。主要思想是使用样本权重将客户端分布调整到全局分布更接近，从而使机器学习模型收敛更快且精度更高。我们从经验风险最小化的基本概念开始，从理论上推导出使用样本权重调整分布偏斜的解决方案。为了确定样本权重，我们隐含地交换...

    One of the most challenging issues in federated learning is that the data is often not independent and identically distributed (nonIID). Clients are expected to contribute the same type of data and drawn from one global distribution. However, data are often collected in different ways from different resources. Thus, the data distributions among clients might be different from the underlying global distribution. This creates a weight divergence issue and reduces federated learning performance. This work focuses on improving federated learning performance for skewed data distribution across clients. The main idea is to adjust the client distribution closer to the global distribution using sample weights. Thus, the machine learning model converges faster with higher accuracy. We start from the fundamental concept of empirical risk minimization and theoretically derive a solution for adjusting the distribution skewness using sample weights. To determine sample weights, we implicitly exchan
    
[^53]: t-DGR: 一种基于轨迹的深度生成回放方法用于决策制定中的持续学习

    t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual Learning in Decision Making. (arXiv:2401.02576v1 [cs.LG])

    [http://arxiv.org/abs/2401.02576](http://arxiv.org/abs/2401.02576)

    t-DGR是一种用于决策制定中持续学习的基于轨迹的深度生成回放方法，通过生成任务样本来解决灾难性遗忘问题，并在连续世界基准测试中取得了最先进的性能。

    

    深度生成回放已经成为决策制定中持续学习的一种有希望的方法。该方法通过利用从以前遇到的任务生成轨迹来增加当前数据集，解决了灾难性遗忘的问题。然而，现有的深度生成回放方法依赖于自回归模型，在生成的轨迹中会出现累积误差。在本文中，我们提出了一种简单、可扩展且非自回归的方法，用于决策制定中的持续学习，使用生成模型根据轨迹时间步生成任务样本。我们在连续世界基准测试中评估了我们的方法，并发现在持续学习方法中，我们的方法在平均成功率指标上达到了最先进的性能。代码可在https://github.com/WilliamYue37/t-DGR找到。

    Deep generative replay has emerged as a promising approach for continual learning in decision-making tasks. This approach addresses the problem of catastrophic forgetting by leveraging the generation of trajectories from previously encountered tasks to augment the current dataset. However, existing deep generative replay methods for continual learning rely on autoregressive models, which suffer from compounding errors in the generated trajectories. In this paper, we propose a simple, scalable, and non-autoregressive method for continual learning in decision-making tasks using a generative model that generates task samples conditioned on the trajectory timestep. We evaluate our method on Continual World benchmarks and find that our approach achieves state-of-the-art performance on the average success rate metric among continual learning methods. Code is available at https://github.com/WilliamYue37/t-DGR .
    
[^54]: 社交网络中的大型语言模型：应用、挑战和解决方案

    Large Language Models for Social Networks: Applications, Challenges, and Solutions. (arXiv:2401.02575v1 [cs.SI])

    [http://arxiv.org/abs/2401.02575](http://arxiv.org/abs/2401.02575)

    本研究调查了如何在在线社交网络中开发大型语言模型（LLMs）应用。我们将LLM应用分为知识任务、娱乐任务和基础任务，并分享了挑战、解决方案和经验教训。

    

    大型语言模型（LLMs）正在改变人们生成、探索和参与内容的方式。本研究探讨如何在在线社交网络中开发LLM应用。尽管LLMs在其他领域取得了成功，但在社交网络中开发基于LLM的产品面临着许多挑战，并且在研究界中相对较少报道。我们将社交网络中的LLM应用分为三类。第一类是知识任务，用户想要查找新知识和信息，例如搜索和问答。第二类是娱乐任务，用户想要消费有趣的内容，例如获取娱乐性通知内容。第三类是基础任务，需要进行社交网络的内容注释和LLM监控。针对每个任务，我们分享了我们发现的挑战、开发的解决方案和吸取的经验教训。据我们所知，这是第一个全面的研究关于在社交网络中应用LLMs的研究。

    Large Language Models (LLMs) are transforming the way people generate, explore, and engage with content. We study how we can develop LLM applications for online social networks. Despite LLMs' successes in other domains, it is challenging to develop LLM-based products for social networks for numerous reasons, and it has been relatively under-reported in the research community. We categorize LLM applications for social networks into three categories. First is knowledge tasks where users want to find new knowledge and information, such as search and question-answering. Second is entertainment tasks where users want to consume interesting content, such as getting entertaining notification content. Third is foundational tasks that need to be done to moderate and operate the social networks, such as content annotation and LLM monitoring. For each task, we share the challenges we found, solutions we developed, and lessons we learned. To the best of our knowledge, this is the first comprehensi
    
[^55]: 钢琴演奏评估中用于音乐形状评价的连体残余神经网络

    Siamese Residual Neural Network for Musical Shape Evaluation in Piano Performance Assessment. (arXiv:2401.02566v1 [cs.SD])

    [http://arxiv.org/abs/2401.02566](http://arxiv.org/abs/2401.02566)

    本论文提出了一种用于钢琴演奏评估中音乐形状评价的轻量级连体残余神经网络（S-ResNN）。实验结果表明，S-ResNN在精确度、召回率和F1得分方面显著优于多个基准方法。

    

    理解和识别音乐形状在音乐教育和演奏评估中扮演着重要角色。为了简化耗时且成本高昂的音乐形状评估过程，本文探索了如何应用基于人工智能驱动的模型。将音乐形状评估视为一个分类问题，我们提出了一种轻量级的连体残余神经网络（S-ResNN），用于自动识别音乐形状。为了在钢琴音乐形状评估的背景下评估所提出的方法，我们生成了一个新的数据集，包含了147个钢琴预备练习中的4116个音乐片段，并分为28个音乐形状类别。实验结果表明，S-ResNN在精确度、召回率和F1得分方面显著优于多个基准方法。

    Understanding and identifying musical shape plays an important role in music education and performance assessment. To simplify the otherwise time- and cost-intensive musical shape evaluation, in this paper we explore how artificial intelligence (AI) driven models can be applied. Considering musical shape evaluation as a classification problem, a light-weight Siamese residual neural network (S-ResNN) is proposed to automatically identify musical shapes. To assess the proposed approach in the context of piano musical shape evaluation, we have generated a new dataset, containing 4116 music pieces derived by 147 piano preparatory exercises and performed in 28 categories of musical shapes. The experimental results show that the S-ResNN significantly outperforms a number of benchmark methods in terms of the precision, recall and F1 score.
    
[^56]: MeTA: 多源测试时适应

    MeTA: Multi-source Test Time Adaptation. (arXiv:2401.02561v1 [cs.LG])

    [http://arxiv.org/abs/2401.02561](http://arxiv.org/abs/2401.02561)

    MeTA是第一个完全无监督的多源测试时适应框架，它可以在没有访问源数据的情况下，以最佳组合权重适应多个源模型到测试数据分布。

    

    测试时适应是一个无监督的过程，它将预训练的源模型适应到每个进入的测试数据批次中（即，无需大量的测试数据可用，就像传统领域适应中那样），并且没有访问源数据的权限。由于它与每个测试数据批次一起工作，因此非常适合需要在数据流入时进行决策的动态环境。当前的测试时适应方法主要集中在单个源模型上。我们提出了第一个完全无监督的多源测试时适应（MeTA）框架，它处理多个源模型并将它们最佳组合以适应测试数据。MeTA具有两个独特的特点。首先，它有效地获得最佳组合权重，以将源模型组合以适应测试数据分布。其次，它确定要更新哪个源模型参数，以便只更新与测试数据最相关的模型。

    Test time adaptation is the process of adapting, in an unsupervised manner, a pre-trained source model to each incoming batch of the test data (i.e., without requiring a substantial portion of the test data to be available, as in traditional domain adaptation) and without access to the source data. Since it works with each batch of test data, it is well-suited for dynamic environments where decisions need to be made as the data is streaming in. Current test time adaptation methods are primarily focused on a single source model. We propose the first completely unsupervised Multi-source Test Time Adaptation (MeTA) framework that handles multiple source models and optimally combines them to adapt to the test data. MeTA has two distinguishing features. First, it efficiently obtains the optimal combination weights to combine the source models to adapt to the test data distribution. Second, it identifies which of the source model parameters to update so that only the model which is most corr
    
[^57]: 实时决策的长期公平性: 一种受限在线优化方法

    Long-term Fairness For Real-time Decision Making: A Constrained Online Optimization Approach. (arXiv:2401.02552v1 [cs.LG])

    [http://arxiv.org/abs/2401.02552](http://arxiv.org/abs/2401.02552)

    通过受限在线优化方法，确保机器学习驱动的决策系统在实时决策过程中实现长期公平性。

    

    机器学习在许多现实世界系统中展示了卓越的能力，从预测建模到智能自动化。然而，机器学习的广泛应用也使得有必要确保机器学习驱动的决策系统不违反所在社会的道德原则和价值观。随着机器学习驱动的决策增加，特别是涉及性别、种族和年龄等敏感属性的情况，公平性和公正性的需要成为一项基本关注点。在需要实时决策的情况下，公平目标变得更加微妙和复杂：即时公平以确保每个时间段的公平性，以及长期公平以确保一段时间内的公平性。越来越多人意识到现实世界的系统需要在长时间内保持公平，需要在不同的时间段上保持公平。然而，现有的方法主要针对时间变化成本进行处理。

    Machine learning (ML) has demonstrated remarkable capabilities across many real-world systems, from predictive modeling to intelligent automation. However, the widespread integration of machine learning also makes it necessary to ensure machine learning-driven decision-making systems do not violate ethical principles and values of society in which they operate. As ML-driven decisions proliferate, particularly in cases involving sensitive attributes such as gender, race, and age, to name a few, the need for equity and impartiality has emerged as a fundamental concern. In situations demanding real-time decision-making, fairness objectives become more nuanced and complex: instantaneous fairness to ensure equity in every time slot, and long-term fairness to ensure fairness over a period of time. There is a growing awareness that real-world systems that operate over long periods and require fairness over different timelines. However, existing approaches mainly address dynamic costs with tim
    
[^58]: SBL模型的超参数估计

    Hyperparameter Estimation for Sparse Bayesian Learning Models. (arXiv:2401.02544v1 [cs.LG])

    [http://arxiv.org/abs/2401.02544](http://arxiv.org/abs/2401.02544)

    该论文介绍了一个综合的SBL模型超参数估计框架，其中包括了期望最大化、MacKay和凸边界等算法。此外，还引入了一种新的算法，通过交替最小化和二次逼近范式，实现了更高的效率。

    

    稀疏贝叶斯学习（SBL）模型广泛应用于信号处理和机器学习，通过层次先验来促进稀疏性。SBL模型中的超参数对模型的性能至关重要，但由于相关目标函数的非凸性和高维度，往往难以估计。该论文提出了一个全面的SBL模型超参数估计框架，包括期望最大化（EM）、MacKay和凸边界（CB）算法等众所周知的算法。这些算法在交替最小化和线性化（AML）范式下得到了一致的解释，其特点是独特的线性化代理函数。此外，还引入了一种新的AML框架中的新算法，显示出增强的效率，尤其是在低信噪比情况下。这一效果通过新的交替最小化和二次逼近（AMQ）范式进一步改进，

    Sparse Bayesian Learning (SBL) models are extensively used in signal processing and machine learning for promoting sparsity through hierarchical priors. The hyperparameters in SBL models are crucial for the model's performance, but they are often difficult to estimate due to the non-convexity and the high-dimensionality of the associated objective function. This paper presents a comprehensive framework for hyperparameter estimation in SBL models, encompassing well-known algorithms such as the expectation-maximization (EM), MacKay, and convex bounding (CB) algorithms. These algorithms are cohesively interpreted within an alternating minimization and linearization (AML) paradigm, distinguished by their unique linearized surrogate functions. Additionally, a novel algorithm within the AML framework is introduced, showing enhanced efficiency, especially under low signal noise ratios. This is further improved by a new alternating minimization and quadratic approximation (AMQ) paradigm, which
    
[^59]: 利用全新的端到端生产就绪机器学习流程进行纳米光刻建模和修正

    Novel End-to-End Production-Ready Machine Learning Flow for Nanolithography Modeling and Correction. (arXiv:2401.02536v1 [cs.LG])

    [http://arxiv.org/abs/2401.02536](http://arxiv.org/abs/2401.02536)

    本研究通过分析阻碍机器学习计算光刻成为生产就绪的原因，提出了一种新颖的高度可扩展的端到端流程，实现了生产就绪的机器学习-RET修正。

    

    光刻技术是半导体制造的主要推动因素，需要进行复杂的处理来执行分辨率增强技术（RET）以将设计数据转移到工作集成电路（ICs）。由于特征尺寸的持续减小和芯片面积的扩大，RET任务的处理能力和计算运行时间不断增加。现有研究借助机器学习（ML）技术来减少运行时间和计算功率，但它们目前还未在实际生产中使用。本研究分析了阻碍ML计算光刻实现生产就绪的原因，并提出了一种新颖的高度可扩展的端到端流程，实现了生产就绪的ML-RET修正。

    Optical lithography is the main enabler to semiconductor manufacturing. It requires extensive processing to perform the Resolution Enhancement Techniques (RETs) required to transfer the design data to a working Integrated Circuits (ICs). The processing power and computational runtime for RETs tasks is ever increasing due to the continuous reduction of the feature size and the expansion of the chip area. State-of-the-art research sought Machine Learning (ML) technologies to reduce runtime and computational power, however they are still not used in production yet. In this study, we analyze the reasons holding back ML computational lithography from being production ready and present a novel highly scalable end-to-end flow that enables production ready ML-RET correction.
    
[^60]: 分支变分自动编码器分类器

    Branched Variational Autoencoder Classifiers. (arXiv:2401.02526v1 [cs.LG])

    [http://arxiv.org/abs/2401.02526](http://arxiv.org/abs/2401.02526)

    这篇论文介绍了一种改进的变分自动编码器（VAEs），通过增加一个额外的神经网络分支，将分类信息融入到潜在表示中，从而提高了分类准确性。

    

    本论文介绍了一种改进的变分自动编码器(VAEs)，其中包含了一个额外的神经网络分支。结果产生的分支VAE（BVAE）通过总损失向分类组件贡献了基于类标签的信息，从而为潜在表示赋予了分类信息。因此，输入类别的潜在空间分布被分离和排序，从而提高了分类准确性。采用基准MNIST数据集对未旋转和旋转数字进行数值计算，量化了改进的程度。提出的技术然后与固定输出分布的VAE进行比较和整合。这个过程被发现对广泛的输出分布能够提供改进的性能。

    This paper introduces a modified variational autoencoder (VAEs) that contains an additional neural network branch. The resulting branched VAE (BVAE) contributes a classification component based on the class labels to the total loss and therefore imparts categorical information to the latent representation. As a result, the latent space distributions of the input classes are separated and ordered, thereby enhancing the classification accuracy. The degree of improvement is quantified by numerical calculations employing the benchmark MNIST dataset for both unrotated and rotated digits. The proposed technique is then compared to and then incorporated into a VAE with fixed output distributions. This procedure is found to yield improved performance for a wide range of output distributions.
    
[^61]: 合成数据生成的综合探索：一项调查

    Comprehensive Exploration of Synthetic Data Generation: A Survey. (arXiv:2401.02524v1 [cs.LG])

    [http://arxiv.org/abs/2401.02524](http://arxiv.org/abs/2401.02524)

    本文调查了过去十年中417个合成数据生成模型，提供了对模型类型、功能和改进的全面概述。结果表明，模型性能和复杂性增加，以神经网络为主要方法，计算机视觉领域占据主导地位。

    

    最近几年，机器学习在各个领域的应用变得越来越受欢迎。然而，由于数据采集成本高昂和隐私法规的限制，训练数据的稀缺性阻碍了进展。合成数据成为一种解决方案，但发布的模型过多和有限的综述文献给决策带来了挑战。本文调查了过去十年中417个合成数据生成模型，提供了对模型类型、功能和改进的全面概述。确定了共同的特征，进行了分类和趋势分析。结果显示模型性能和复杂性增加，以基于神经网络的方法为主要趋势，除了隐私保护数据生成。计算机视觉占据主导地位，生成模型主要是GAN，而扩散模型、转换器和RNN也在竞争中。通过性能评估的结果显示了共同指标和数据集稀缺性的问题。

    Recent years have witnessed a surge in the popularity of Machine Learning (ML), applied across diverse domains. However, progress is impeded by the scarcity of training data due to expensive acquisition and privacy legislation. Synthetic data emerges as a solution, but the abundance of released models and limited overview literature pose challenges for decision-making. This work surveys 417 Synthetic Data Generation (SDG) models over the last decade, providing a comprehensive overview of model types, functionality, and improvements. Common attributes are identified, leading to a classification and trend analysis. The findings reveal increased model performance and complexity, with neural network-based approaches prevailing, except for privacy-preserving data generation. Computer vision dominates, with GANs as primary generative models, while diffusion models, transformers, and RNNs compete. Implications from our performance evaluation highlight the scarcity of common metrics and datase
    
[^62]: 基于图像的深度学习用于智能数字孪生体: 一份综述

    Image-based Deep Learning for Smart Digital Twins: a Review. (arXiv:2401.02523v1 [cs.CV])

    [http://arxiv.org/abs/2401.02523](http://arxiv.org/abs/2401.02523)

    本论文综述了基于图像的智能数字孪生体 (SDTs) 的发展方法和挑战，重点讨论了通过持续同化图像数据来观察和学习系统行为的方法，以及设计和实现SDTs的深度学习 (DL) 模型所面临的挑战。提供了对未来发展方向和机遇的见解。

    

    智能数字孪生体(SDTs)通过持续数据同化来虚拟复制和预测复杂物理系统的行为，从而通过控制系统的行为来优化这些系统的性能。近年来，深度学习(DL)模型显著增强了SDTs的能力，尤其是在预测性维修、异常检测和优化等任务上。在许多领域，包括医学、工程和教育，在观察和学习系统行为和控制其行为方面，SDTs使用图像数据(基于图像的SDTs)。本文重点介绍了开发基于图像的SDTs的各种方法和相关挑战，包括持续同化来自物理系统的图像数据。本文还讨论了设计和实现SDTs的DL模型所涉及的挑战，包括数据获取、处理和解释。此外，还提供了对未来发展方向和机遇的见解。

    Smart Digital twins (SDTs) are being increasingly used to virtually replicate and predict the behaviors of complex physical systems through continual data assimilation enabling the optimization of the performance of these systems by controlling the actions of systems. Recently, deep learning (DL) models have significantly enhanced the capabilities of SDTs, particularly for tasks such as predictive maintenance, anomaly detection, and optimization. In many domains, including medicine, engineering, and education, SDTs use image data (image-based SDTs) to observe and learn system behaviors and control their behaviors. This paper focuses on various approaches and associated challenges in developing image-based SDTs by continually assimilating image data from physical systems. The paper also discusses the challenges involved in designing and implementing DL models for SDTs, including data acquisition, processing, and interpretation. In addition, insights into the future directions and opport
    
[^63]: 在任意元素间依赖下的结构化矩阵学习与马尔可夫转移核估计

    Structured Matrix Learning under Arbitrary Entrywise Dependence and Estimation of Markov Transition Kernel. (arXiv:2401.02520v1 [stat.ML])

    [http://arxiv.org/abs/2401.02520](http://arxiv.org/abs/2401.02520)

    本论文提出了在任意元素间依赖下进行结构化矩阵估计的通用框架，并证明了提出的最小二乘估计器在各种噪声分布下的紧致性。此外，论文还提出了一个新颖的结果，论述了无关低秩矩阵的结构特点。最后，论文还展示了该框架在结构化马尔可夫转移核估计问题中的应用。

    

    结构化矩阵估计问题通常在强噪声依赖假设下进行研究。本文考虑噪声低秩加稀疏矩阵恢复的一般框架，其中噪声矩阵可以来自任意具有元素间任意依赖的联合分布。我们提出了一个无关相位约束的最小二乘估计器，并且证明了它在各种噪声分布下都是紧致的，既满足确定性下界又匹配最小化风险。为了实现这一点，我们建立了一个新颖的结果，断言两个任意的低秩无关矩阵之间的差异必须在其元素上扩散能量，换句话说不能太稀疏，这揭示了无关低秩矩阵的结构，可能引起独立兴趣。然后，我们展示了我们框架在几个重要的统计机器学习问题中的应用。在估计结构化马尔可夫转移核的问题中，采用了这种方法。

    The problem of structured matrix estimation has been studied mostly under strong noise dependence assumptions. This paper considers a general framework of noisy low-rank-plus-sparse matrix recovery, where the noise matrix may come from any joint distribution with arbitrary dependence across entries. We propose an incoherent-constrained least-square estimator and prove its tightness both in the sense of deterministic lower bound and matching minimax risks under various noise distributions. To attain this, we establish a novel result asserting that the difference between two arbitrary low-rank incoherent matrices must spread energy out across its entries, in other words cannot be too sparse, which sheds light on the structure of incoherent low-rank matrices and may be of independent interest. We then showcase the applications of our framework to several important statistical machine learning problems. In the problem of estimating a structured Markov transition kernel, the proposed method
    
[^64]: 使用神经算子的增益调度方法处理具有非线性循环的输运PDE

    Gain Scheduling with a Neural Operator for a Transport PDE with Nonlinear Recirculation. (arXiv:2401.02511v1 [eess.SY])

    [http://arxiv.org/abs/2401.02511](http://arxiv.org/abs/2401.02511)

    本文介绍了使用神经算子进行增益调度的方法来处理具有非线性循环的输运PDE，并在局部实现了稳定。

    

    为了稳定PDE模型，控制律需要通过非线性算子将空间相关的增益映射到PDE函数系数上。当PDE是非线性的且"伪系数"函数依赖于状态时，增益调度非线性设计是处理非线性反馈设计的最简单方法。PDE回溯的增益调度版本利用在每个状态值处求解PDE得到的增益。但是在实时情况下进行这种PDE计算可能是困难的。最近引入的神经算子（NO）可以在每个状态值上快速且实时地训练，产生增益函数，而无需求解PDE。本文介绍了将NO用于GS-PDE回溯的方法。GS控制器假设状态变化缓慢，并且仅保证局部稳定性，即使对于ODE也是如此。我们通过"全核"方法和"仅增益"方法，确立了具有非线性循环的双曲型PDE的局部稳定性。

    To stabilize PDE models, control laws require space-dependent functional gains mapped by nonlinear operators from the PDE functional coefficients. When a PDE is nonlinear and its "pseudo-coefficient" functions are state-dependent, a gain-scheduling (GS) nonlinear design is the simplest approach to the design of nonlinear feedback. The GS version of PDE backstepping employs gains obtained by solving a PDE at each value of the state. Performing such PDE computations in real time may be prohibitive. The recently introduced neural operators (NO) can be trained to produce the gain functions, rapidly in real time, for each state value, without requiring a PDE solution. In this paper we introduce NOs for GS-PDE backstepping. GS controllers act on the premise that the state change is slow and, as a result, guarantee only local stability, even for ODEs. We establish local stabilization of hyperbolic PDEs with nonlinear recirculation using both a "full-kernel" approach and the "gain-only" approa
    
[^65]: 在决策与控制中实现一种适应性和可推广的优化引擎：一种元强化学习方法

    Towards an Adaptable and Generalizable Optimization Engine in Decision and Control: A Meta Reinforcement Learning Approach. (arXiv:2401.02508v1 [cs.LG])

    [http://arxiv.org/abs/2401.02508](http://arxiv.org/abs/2401.02508)

    本文提出了一种基于元强化学习的优化器，可以不需要专家示范并在非稳态环境下实现快速适应，用于更新模型预测控制器。

    

    基于采样的模型预测控制在具有非平滑系统动力学和成本函数的最优控制问题中取得了显著的成功。许多基于机器学习的工作提出通过学习或微调动力学/成本函数或通过学习优化更新模型预测控制器来改进模型预测控制。为了解决这些问题，我们提出了一种基于元强化学习的优化器来更新控制器。这种优化器不需要专家示范，并且可以在面对未知环境时实现快速适应（例如，少量示范）。

    Sampling-based model predictive control (MPC) has found significant success in optimal control problems with non-smooth system dynamics and cost function. Many machine learning-based works proposed to improve MPC by a) learning or fine-tuning the dynamics/ cost function, or b) learning to optimize for the update of the MPC controllers. For the latter, imitation learning-based optimizers are trained to update the MPC controller by mimicking the expert demonstrations, which, however, are expensive or even unavailable. More significantly, many sequential decision-making problems are in non-stationary environments, requiring that an optimizer should be adaptable and generalizable to update the MPC controller for solving different tasks. To address those issues, we propose to learn an optimizer based on meta-reinforcement learning (RL) to update the controllers. This optimizer does not need expert demonstration and can enable fast adaptation (e.g., few-shots) when it is deployed in unseen c
    
[^66]: 细胞信号传导结构和功能

    The cell signaling structure function. (arXiv:2401.02501v1 [cs.CV])

    [http://arxiv.org/abs/2401.02501](http://arxiv.org/abs/2401.02501)

    该论文提出了一个新的方法，在活体细胞显微镜捕捉到的五维视频中寻找细胞信号动力学时空模式，并且不需要任何先验的预期模式动力学和训练数据。该方法基于细胞信号结构函数（SSF），通过测量细胞信号状态和周围细胞质之间的核糖体强度，与当前最先进的核糖体与细胞核比值相比有了显著改进。通过归一化压缩距离（NCD）来识别相似的模式。该方法能够将输入的SSF构图表示为低维嵌入中的点，最优地捕捉模式。

    

    活体细胞显微镜捕捉到的五维$(x,y,z,channel,time)$视频显示了细胞运动和信号动力学的模式。我们在这里提出一种在五维活体细胞显微镜视频中寻找细胞信号动力学时空模式的方法，该方法独特之处在于不需要预先了解预期的模式动力学以及没有训练数据。所提出的细胞信号结构函数（SSF）是一种Kolmogorov结构函数，可以通过核心区域相对于周围细胞质的核糖体强度来最优地测量细胞信号状态，相比当前最先进的核糖体与细胞核比值有了显著的改进。通过度量归一化压缩距离（NCD）来识别相似的模式。NCD是一个用于表示输入的SSF构图在低维嵌入中作为点的Hilbert空间的再生核，可以最优地捕捉模式。

    Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display patterns of cellular motion and signaling dynamics. We present here an approach to finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell microscopy movies unique in requiring no \emph{a priori} knowledge of expected pattern dynamics, and no training data. The proposed cell signaling structure function (SSF) is a Kolmogorov structure function that optimally measures cell signaling state as nuclear intensity w.r.t. surrounding cytoplasm, a significant improvement compared to the current state-of-the-art cytonuclear ratio. SSF kymographs store at each spatiotemporal cell centroid the SSF value, or a functional output such as velocity. Patterns of similarity are identified via the metric normalized compression distance (NCD). The NCD is a reproducing kernel for a Hilbert space that represents the input SSF kymographs as points in a low dimensional embedding that optimally captures the pattern
    
[^67]: 可解释的时间序列模型用于综合下水道溢流的废水建模

    Interpretable Time Series Models for Wastewater Modeling in Combined Sewer Overflows. (arXiv:2401.02465v1 [cs.LG])

    [http://arxiv.org/abs/2401.02465](http://arxiv.org/abs/2401.02465)

    这篇论文讨论了气候变化对污水管理带来的复杂挑战，提出了一种基于可解释时间序列模型的废水建模方法，可以帮助预测关键水位点并改善废水管理和环境污染预防。

    

    气候变化对我们的社会提出了越来越复杂的挑战。洪水、野火或干旱等极端天气事件变得越来越频繁、突发且难以预见或应对。在这项工作中，我们特别解决了重雨事件导致下雨罐溢出以后，污水污染地表水体的问题。我们调查了最先进的可解释时间序列模型在如何预测这些临界水位点方面的作用，以便能够及时将多余的水重新分配到下水道网络中。我们的结果表明，现代时间序列模型可以为改善废水管理和预防下水道系统对环境的污染做出贡献。所有的代码和实验可以在我们的存储库中找到：https://github.com/TeodorChiaburu/RIWWER_TimeSeries。

    Climate change poses increasingly complex challenges to our society. Extreme weather events such as floods, wild fires or droughts are becoming more frequent, spontaneous and difficult to foresee or counteract. In this work we specifically address the problem of sewage water polluting surface water bodies after spilling over from rain tanks as a consequence of heavy rain events. We investigate to what extent state-of-the-art interpretable time series models can help predict such critical water level points, so that the excess can promptly be redistributed across the sewage network. Our results indicate that modern time series models can contribute to better waste water management and prevention of environmental pollution from sewer systems. All the code and experiments can be found in our repository: https://github.com/TeodorChiaburu/RIWWER_TimeSeries.
    
[^68]: 计算医疗中的数据中心基础模型：一项调查

    Data-Centric Foundation Models in Computational Healthcare: A Survey. (arXiv:2401.02458v1 [cs.LG])

    [http://arxiv.org/abs/2401.02458](http://arxiv.org/abs/2401.02458)

    计算医疗中的数据中心基础模型是一项调查研究，为医疗工作流程的改进提供了基于数据的人工智能方法，并讨论了安全性、评估和与人类价值观的一致性。基于FM的分析有望提高患者结果和临床工作流程表现。

    

    作为一套新兴的人工智能技术，基础模型（FMs）的出现为计算医疗带来了一系列机遇。这些模型的交互性由预训练数据和人类指令引导，引发了一个数据中心的人工智能范式，强调更好的数据表征、质量和规模。在医疗人工智能领域，获取和处理高质量的临床数据记录一直是一个长期存在的挑战，涉及数据数量、注释、患者隐私和伦理等方面。在这项调查中，我们研究了FM时代的广泛数据中心方法（从模型预训练到推理），以改进医疗工作流程。我们讨论了人工智能安全性、评估以及与人类价值观的一致性的关键观点。最后，我们展望了基于FM的分析在医疗和医药领域不断发展的格局中提高患者结果和临床工作流程表现的前景。我们提供了一个最新的医疗清单。

    The advent of foundation models (FMs) as an emerging suite of AI techniques has struck a wave of opportunities in computational healthcare. The interactive nature of these models, guided by pre-training data and human instructions, has ignited a data-centric AI paradigm that emphasizes better data characterization, quality, and scale. In healthcare AI, obtaining and processing high-quality clinical data records has been a longstanding challenge, ranging from data quantity, annotation, patient privacy, and ethics. In this survey, we investigate a wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow. We discuss key perspectives in AI security, assessment, and alignment with human values. Finally, we offer a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine. We provide an up-to-date list of healthcare
    
[^69]: eCIL-MU: 基于嵌入的逐类增量学习和机器取消学习

    eCIL-MU: Embedding based Class Incremental Learning and Machine Unlearning. (arXiv:2401.02457v1 [cs.LG])

    [http://arxiv.org/abs/2401.02457](http://arxiv.org/abs/2401.02457)

    eCIL-MU是一种基于嵌入技术的逐类增量学习和机器取消学习的非破坏性框架，可用于在动态环境中快速获取关于新类别的知识，并消除先前学习类别的影响。

    

    在动态环境中，可能会不断引入新的类别，或者需要对现有类别进行重新分类。逐类增量学习 (CIL) 用于在获取关于新类别的知识的同时，保留对先前学习类别的信息。为了适应重新分类，还可能需要消除相关类别对模型的影响。因此，我们在CIL中引入了基于类别的机器取消学习 (MU)。通常，MU方法倾向于耗时，并且可能会损害模型的性能。连续的取消学习请求可能导致灾难性遗忘。为解决这些问题，我们提出了一种非破坏性的基于嵌入技术的eCIL-MU框架，将数据映射到向量并存储在向量数据库中。我们的方法利用了CIL和MU任务之间的重叠来加速。实验证明了实现取消学习效果和数量级的能力。

    New categories may be introduced over time, or existing categories may need to be reclassified. Class incremental learning (CIL) is employed for the gradual acquisition of knowledge about new categories while preserving information about previously learned ones in such dynamic environments. It might also be necessary to also eliminate the influence of related categories on the model to adapt to reclassification. We thus introduce class-level machine unlearning (MU) within CIL. Typically, MU methods tend to be time-consuming and can potentially harm the model's performance. A continuous stream of unlearning requests could lead to catastrophic forgetting. To address these issues, we propose a non-destructive eCIL-MU framework based on embedding techniques to map data into vectors and then be stored in vector databases. Our approach exploits the overlap between CIL and MU tasks for acceleration. Experiments demonstrate the capability of achieving unlearning effectiveness and orders of mag
    
[^70]: 面向火灾管理的AI支持无人机系统研究综述

    A comprehensive survey of research towards AI-enabled unmanned aerial systems in pre-, active-, and post-wildfire management. (arXiv:2401.02456v1 [cs.LG])

    [http://arxiv.org/abs/2401.02456](http://arxiv.org/abs/2401.02456)

    本研究综述探讨了AI支持的无人机系统在火灾管理中的应用，从火灾前到主动火灾阶段再到火灾后，通过整合无人机和深度学习模型，提供了更有效的野火管理解决方案。

    

    野火已成为全球最具破坏性的自然灾害之一，给人类生命和森林野生动物造成了灾难性损失。最近，人工智能（AI）在野火中的应用，通过无人机（UAV）与深度学习模型的整合，创造了实施和发展更有效的野火管理的前所未有的动力。尽管一些现有的调查论文已探讨了各种基于学习的方法，但明显缺乏一篇全面综述，强调AI支持的UAV系统在多阶段野火管理中的应用及其后续影响。本综述旨在填补这些差距，通过系统回顾最新的技术进展，重点介绍了从火灾前到主动火灾阶段再到火灾后管理的UAV系统和AI模型的进展。为此，我们对现有的遥感系统进行了广泛分析。

    Wildfires have emerged as one of the most destructive natural disasters worldwide, causing catastrophic losses in both human lives and forest wildlife. Recently, the use of Artificial Intelligence (AI) in wildfires, propelled by the integration of Unmanned Aerial Vehicles (UAVs) and deep learning models, has created an unprecedented momentum to implement and develop more effective wildfire management. Although some of the existing survey papers have explored various learning-based approaches, a comprehensive review emphasizing the application of AI-enabled UAV systems and their subsequent impact on multi-stage wildfire management is notably lacking. This survey aims to bridge these gaps by offering a systematic review of the recent state-of-the-art technologies, highlighting the advancements of UAV systems and AI models from pre-fire, through the active-fire stage, to post-fire management. To this aim, we provide an extensive analysis of the existing remote sensing systems with a parti
    
[^71]: 自适应差分隐私在联邦学习中的应用: 一种基于优先级的方法。

    Adaptive Differential Privacy in Federated Learning: A Priority-Based Approach. (arXiv:2401.02453v1 [cs.CR])

    [http://arxiv.org/abs/2401.02453](http://arxiv.org/abs/2401.02453)

    这项研究提出了一种在联邦学习中使用自适应差分隐私的方法，通过根据特征的相对重要性来决定注入的噪声值，以平衡隐私和模型性能。

    

    作为分布式机器学习的新领域之一，联邦学习通过私有过程开发全局模型，而无需直接访问本地数据集。然而，客户端和服务器之间传输的模型更新（例如深度神经网络中的梯度更新）可能向对手泄露敏感信息。差分隐私通过向参数中添加一定量的噪声来提供隐私保证。尽管这种方法在隐私方面是有效的，但由于噪声的介入，会对模型性能产生负面影响。因此，必须在噪声注入和牺牲的准确性之间找到平衡。为了解决这个挑战，我们提出了在联邦学习中采用自适应噪声注入的方法，该方法根据特征的相对重要性决定注入噪声的值。我们首先提出了两种对深度神经网络模型中特征进行优先级排序的有效方法，然后基于此对模型的权重进行扰动。

    Federated learning (FL) as one of the novel branches of distributed machine learning (ML), develops global models through a private procedure without direct access to local datasets. However, access to model updates (e.g. gradient updates in deep neural networks) transferred between clients and servers can reveal sensitive information to adversaries. Differential privacy (DP) offers a framework that gives a privacy guarantee by adding certain amounts of noise to parameters. This approach, although being effective in terms of privacy, adversely affects model performance due to noise involvement. Hence, it is always needed to find a balance between noise injection and the sacrificed accuracy. To address this challenge, we propose adaptive noise addition in FL which decides the value of injected noise based on features' relative importance. Here, we first propose two effective methods for prioritizing features in deep neural network models and then perturb models' weights based on this in
    
[^72]: 多规则来源的智能家居自动化

    Automation of Smart Homes with Multiple Rule Sources. (arXiv:2401.02451v1 [cs.CR])

    [http://arxiv.org/abs/2401.02451](http://arxiv.org/abs/2401.02451)

    这篇论文介绍了多规则来源的智能家居自动化的挑战和解决方案，包括管理规则的过程和机构、高级决策与实现细节的分离以及对系统结构和安全架构的影响。

    

    使用规则进行家居自动化面临一些挑战，特别是考虑到除了居民外还有多个利益相关者，如房主、当地政府、能源供应商和系统提供商，他们希望贡献规则以保护自己的利益。管理来自不同来源的规则需要一个有结构的过程、相关政策和指定的机构来确保授权和正确的贡献，并解决潜在的冲突。此外，智能家居规则语言需要以高度抽象的方式表达条件和决策，而不指定实现细节，如接口、访问协议和房间布局。将高级决策与这些细节分离支持规则对类似家居的可转移性和适应性。这种分离还对智能家居系统和安全架构的结构具有重要影响。我们提出的方法和系统实现引入了一个规则管理的方法。

    Using rules for home automation presents several challenges, especially when considering multiple stakeholders in addition to residents, such as homeowners, local authorities, energy suppliers, and system providers, who will wish to contribute rules to safeguard their interests. Managing rules from various sources requires a structured procedure, a relevant policy, and a designated authority to ensure authorized and correct contributions and address potential conflicts. In addition, the smart home rule language needs to express conditions and decisions at a high level of abstraction without specifying implementation details such as interfaces, access protocols, and room layout. Decoupling high-level decisions from these details supports the transferability and adaptability of rules to similar homes. This separation also has important implications for structuring the smart home system and the security architecture. Our proposed approach and system implementation introduce a rule managem
    
[^73]: 在分布式欺诈预防系统中的本地差分隐私嵌入模型

    Locally Differentially Private Embedding Models in Distributed Fraud Prevention Systems. (arXiv:2401.02450v1 [cs.CR])

    [http://arxiv.org/abs/2401.02450](http://arxiv.org/abs/2401.02450)

    本文提出了一个基于本地差分隐私的协作深度学习框架，用于在分布式欺诈预防系统中构建安全的数据发布机制，以支持外部欺诈检测模型。在实验中展示了其对多种攻击方法的鲁棒性。

    

    全球金融犯罪活动促使欺诈预防中需要机器学习解决方案。然而，由于担心意外泄漏和敌对攻击，预防系统通常只为金融机构提供服务而缺乏数据共享的机制。金融领域的协作学习进展较少，也很难从隐私保护的数据处理系统中获得实际洞察。本文介绍了一个从隐私角度设计的协作深度学习框架，用于欺诈预防，并在最近的PETs Prize Challenges中获奖。我们利用变长交易序列的潜在嵌入表示和本地差分隐私构建了一个数据发布机制，可以安全地提供给外部托管的欺诈和异常检测模型。我们在两个来自大型支付网络的分布式数据集上评估了我们的贡献，并展示了对常见攻击方法的鲁棒性。

    Global financial crime activity is driving demand for machine learning solutions in fraud prevention. However, prevention systems are commonly serviced to financial institutions in isolation, and few provisions exist for data sharing due to fears of unintentional leaks and adversarial attacks. Collaborative learning advances in finance are rare, and it is hard to find real-world insights derived from privacy-preserving data processing systems. In this paper, we present a collaborative deep learning framework for fraud prevention, designed from a privacy standpoint, and awarded at the recent PETs Prize Challenges. We leverage latent embedded representations of varied-length transaction sequences, along with local differential privacy, in order to construct a data release mechanism which can securely inform externally hosted fraud and anomaly detection models. We assess our contribution on two distributed data sets donated by large payment networks, and demonstrate robustness to popular 
    
[^74]: 基于人类呼出气体物理的用户认证系统

    User authentication system based on human exhaled breath physics. (arXiv:2401.02447v1 [cs.CR])

    [http://arxiv.org/abs/2401.02447](http://arxiv.org/abs/2401.02447)

    本研究尝试构建一个基于呼出气体物理的用户认证系统，通过对呼出气体的湍流结构进行分析，可以实现用户确认和用户识别。实验结果表明，用户确认算法表现出色。

    

    本研究以开创性的方法，试图建立一个纯粹基于呼出气体流体力学的生物特征系统。我们测试了一种假设，即呼出人类气息中的湍流结构可以用来构建生物特征算法。该研究依赖于额外胸外气道对每个个体来说是独特的这一想法，使得呼出气体成为了一个生物标记。我们采用了经典多维假设检验方法和机器学习模型来构建用户认证算法，即用户确认和用户识别。用户确认算法旨在验证用户是否为其声称的人。用户识别算法则旨在在没有任何先前信息的情况下识别用户的身份。我们利用来自94名人类受试者的呼出气体时间序列样本数据集来评估这些算法的性能。用户确认算法的表现非常出色。

    This work, in a pioneering approach, attempts to build a biometric system that works purely based on the fluid mechanics governing exhaled breath. We test the hypothesis that the structure of turbulence in exhaled human breath can be exploited to build biometric algorithms. This work relies on the idea that the extrathoracic airway is unique for every individual, making the exhaled breath a biomarker. Methods including classical multi-dimensional hypothesis testing approach and machine learning models are employed in building user authentication algorithms, namely user confirmation and user identification. A user confirmation algorithm tries to verify whether a user is the person they claim to be. A user identification algorithm tries to identify a user's identity with no prior information available. A dataset of exhaled breath time series samples from 94 human subjects was used to evaluate the performance of these algorithms. The user confirmation algorithms performed exceedingly well
    
[^75]: 流网络中的传感器布置问题

    Sensor Placement for Learning in Flow Networks. (arXiv:2401.02438v1 [eess.SP])

    [http://arxiv.org/abs/2401.02438](http://arxiv.org/abs/2401.02438)

    本论文研究了网络中的传感器布置问题，通过将传感器仅放置在关键链路上，利用机器学习算法推断整个网络中缺失的测量，从而实现了更准确的推断。

    

    大型基础设施网络（如交通和电力分布）需要持续监测故障、拥塞和其他不利事件。然而，由于布置和维护成本的原因，往往无法在网络的每个链路上设置传感器。相反，可以仅在几个关键链路上放置传感器，并利用机器学习算法在整个网络中推断缺失的测量（如交通量、功率流）。本文研究了网络传感器布置问题。首先，在流量守恒的假设下，我们形式化了该问题，并证明了在确定的传感器集合上进行最优布置是NP-hard的。接下来，我们提出了一种高效且自适应的贪心算法用于传感器布置，该算法能够适应大型网络。我们的实验证明，与现有文献中的替代方案相比，所提出的方法能够实现更准确的推断。

    Large infrastructure networks (e.g. for transportation and power distribution) require constant monitoring for failures, congestion, and other adversarial events. However, assigning a sensor to every link in the network is often infeasible due to placement and maintenance costs. Instead, sensors can be placed only on a few key links, and machine learning algorithms can be leveraged for the inference of missing measurements (e.g. traffic counts, power flows) across the network. This paper investigates the sensor placement problem for networks. We first formalize the problem under a flow conservation assumption and show that it is NP-hard to place a fixed set of sensors optimally. Next, we propose an efficient and adaptive greedy heuristic for sensor placement that scales to large networks. Our experiments, using datasets from real-world application domains, show that the proposed approach enables more accurate inference than existing alternatives from the literature. We demonstrate that
    
[^76]: 神经网络中的随机加权神经调节有助于学习跨任务公共流形

    Randomly Weighted Neuromodulation in Neural Networks Facilitates Learning of Manifolds Common Across Tasks. (arXiv:2401.02437v1 [cs.NE])

    [http://arxiv.org/abs/2401.02437](http://arxiv.org/abs/2401.02437)

    本文研究了在神经网络中采用随机加权神经调节的方法，有助于学习跨任务公共流形。通过定义“任务特定的几何敏感哈希”，利用类似于大脑模型的神经调节系统，实现了这一方法。

    

    几何敏感哈希函数是一类神经网络模型，能够在监督学习中学习特定类别的流形几何。然而，在给定一组监督学习任务时，关于每个任务能够表示的流形几何以及它们之间的关系类型方面的研究还很少。本文通过考虑一个生成过程的形式化，其中每个任务与一个高维流形相关联，可以利用类似于大脑模型的神经调节系统来完成。根据这个形式化，我们定义了“任务特定的几何敏感哈希（T-GSH）”，并展示了一个带有神经调节系统的随机加权神经网络可以实现这个函数。

    Geometric Sensitive Hashing functions, a family of Local Sensitive Hashing functions, are neural network models that learn class-specific manifold geometry in supervised learning. However, given a set of supervised learning tasks, understanding the manifold geometries that can represent each task and the kinds of relationships between the tasks based on them has received little attention. We explore a formalization of this question by considering a generative process where each task is associated with a high-dimensional manifold, which can be done in brain-like models with neuromodulatory systems. Following this formulation, we define \emph{Task-specific Geometric Sensitive Hashing~(T-GSH)} and show that a randomly weighted neural network with a neuromodulation system can realize this function.
    
[^77]: FedDiff: 基于扩散模型的多模态和多客户联邦学习

    FedDiff: Diffusion Model Driven Federated Learning for Multi-Modal and Multi-Clients. (arXiv:2401.02433v1 [cs.CV])

    [http://arxiv.org/abs/2401.02433](http://arxiv.org/abs/2401.02433)

    FedDiff是一个多模态协作扩散联邦学习框架，旨在实现来自不同客户的异构数据的安全融合，通过建立双分支扩散模型特征提取设置来驱动联邦学习过程。

    

    随着遥感领域成像传感器技术的快速发展，多模态遥感数据融合已成为土地覆盖分类任务中的一个关键研究方向。然而，现有的扩散模型主要集中在单模态和单客户控制上，即扩散过程由单个模态在单个计算节点驱动。为了实现来自客户的异构数据的安全融合，需要实现分布式的多模态控制，例如在每个基站客户端上，将组织A的高光谱数据和组织B的激光雷达数据进行私密合并。本研究提出了一种名为FedDiff的多模态协作扩散联邦学习框架。我们的框架建立了一个双分支扩散模型特征提取设置，其中两种模态数据被输入到编码器的分支中。

    With the rapid development of imaging sensor technology in the field of remote sensing, multi-modal remote sensing data fusion has emerged as a crucial research direction for land cover classification tasks. While diffusion models have made great progress in generative models and image classification tasks, existing models primarily focus on single-modality and single-client control, that is, the diffusion process is driven by a single modal in a single computing node. To facilitate the secure fusion of heterogeneous data from clients, it is necessary to enable distributed multi-modal control, such as merging the hyperspectral data of organization A and the LiDAR data of organization B privately on each base station client. In this study, we propose a multi-modal collaborative diffusion federated learning framework called FedDiff. Our framework establishes a dual-branch diffusion model feature extraction setup, where the two modal data are inputted into separate branches of the encoder
    
[^78]: 自动分类ImageNet上的模型错误

    Automated Classification of Model Errors on ImageNet. (arXiv:2401.02430v1 [cs.CV])

    [http://arxiv.org/abs/2401.02430](http://arxiv.org/abs/2401.02430)

    该论文提出了自动错误分类框架来解决ImageNet数据集中的标签噪声和错误问题，为研究模型选择如何影响错误分布提供了有价值的工具。

    

    虽然ImageNet数据集在过去十年中推动了计算机视觉研究，但显著的标签噪声和歧义使得Top-1准确率成为进一步进展的不够充分的指标。为了解决这个问题，提出了新的标签集和评估协议，显示出最先进的模型已经实现了超过95%的准确率，并将重点转向为什么剩余的错误仍然存在的调查。最近的工作采用专家小组对两个选择的模型的所有剩余分类错误进行手动分类。然而，这个过程耗时，容易产生不一致性，并且需要受过训练的专家，使其不适用于常规模型评估，从而限制了其效用。为了克服这些局限性，我们提出了第一个自动错误分类框架，这是一个有价值的工具，用于研究建模选择如何影响错误分布。我们使用我们的框架全面评估了超过90个模型的错误分布。

    While the ImageNet dataset has been driving computer vision research over the past decade, significant label noise and ambiguity have made top-1 accuracy an insufficient measure of further progress. To address this, new label-sets and evaluation protocols have been proposed for ImageNet showing that state-of-the-art models already achieve over 95% accuracy and shifting the focus on investigating why the remaining errors persist.  Recent work in this direction employed a panel of experts to manually categorize all remaining classification errors for two selected models. However, this process is time-consuming, prone to inconsistencies, and requires trained experts, making it unsuitable for regular model evaluation thus limiting its utility. To overcome these limitations, we propose the first automated error classification framework, a valuable tool to study how modeling choices affect error distributions. We use our framework to comprehensively evaluate the error distribution of over 90
    
[^79]: 基于脑启发的脉冲神经网络在工业故障诊断中的应用：调查、挑战和机遇

    Brain-Inspired Spiking Neural Networks for Industrial Fault Diagnosis: A Survey, Challenges, and Opportunities. (arXiv:2401.02429v1 [cs.NE])

    [http://arxiv.org/abs/2401.02429](http://arxiv.org/abs/2401.02429)

    基于脑启发的脉冲神经网络是一种有前景的应用于工业故障诊断的替代方法，可以克服人工神经网络的限制，提供更精确和有效的故障识别。

    

    近几十年来，工业故障诊断（IFD）作为一门关注检测和收集工业设备健康状况重要信息的学科而出现，从而促进了对故障类型和严重程度的识别。精确和有效的故障识别引起了广泛关注，导致对自动化设备监测的关注，以避免安全事故并减少对人力的依赖。人工神经网络（ANNs）的出现在增强智能IFD算法方面起到了重要作用，特别是在大数据背景下。尽管取得了这些进展，作为一种简化的仿生神经网络模型，ANNs存在固有的限制，如资源和数据依赖性以及受限的认知能力。为了解决这些限制，基于脑启发计算原理的第三代脉冲神经网络（SNN）已经成为一种有前景的替代方法。

    In recent decades, Industrial Fault Diagnosis (IFD) has emerged as a crucial discipline concerned with detecting and gathering vital information about industrial equipment's health condition, thereby facilitating the identification of failure types and severities. The pursuit of precise and effective fault recognition has garnered substantial attention, culminating in a focus on automating equipment monitoring to preclude safety accidents and reduce reliance on human labor. The advent of artificial neural networks (ANNs) has been instrumental in augmenting intelligent IFD algorithms, particularly in the context of big data. Despite these advancements, ANNs, being a simplified biomimetic neural network model, exhibit inherent limitations such as resource and data dependencies and restricted cognitive capabilities. To address these limitations, the third-generation Spiking Neural Network (SNN), founded on principles of Brain-inspired computing, has surfaced as a promising alternative. Th
    
[^80]: 使用EuroSAT和迁移学习进行土地利用和土地覆盖（LULC）的映射

    Mapping of Land Use and Land Cover (LULC) using EuroSAT and Transfer Learning. (arXiv:2401.02424v1 [cs.CV])

    [http://arxiv.org/abs/2401.02424](http://arxiv.org/abs/2401.02424)

    该论文利用EuroSAT和迁移学习技术对土地利用和土地覆盖进行映射，通过使用颜色波段进行微调，取得了99.19%的精确度，有助于制定保护和城市规划政策。

    

    随着全球人口的不断增长，对自然资源的需求也在增加。不幸的是，人类活动占温室气体排放的23%。好消息是，遥感技术已经成为管理我们环境的有价值工具。这些技术使我们能够监测土地利用，规划城市区域，并推动农业、气候变化缓解、灾害恢复和环境监测等领域的进步。近年来，人工智能、计算机视觉和地球观测数据的进展使土地利用映射的准确性达到了前所未有的水平。通过使用迁移学习和用RGB波段微调，我们在土地利用分析中取得了令人印象深刻的99.19%的准确性。这样的发现可以用于制定保护和城市规划政策。

    As the global population continues to expand, the demand for natural resources increases. Unfortunately, human activities account for 23% of greenhouse gas emissions. On a positive note, remote sensing technologies have emerged as a valuable tool in managing our environment. These technologies allow us to monitor land use, plan urban areas, and drive advancements in areas such as agriculture, climate change mitigation, disaster recovery, and environmental monitoring. Recent advances in AI, computer vision, and earth observation data have enabled unprecedented accuracy in land use mapping. By using transfer learning and fine-tuning with RGB bands, we achieved an impressive 99.19% accuracy in land use analysis. Such findings can be used to inform conservation and urban planning policies.
    
[^81]: 不是所有的少数群体都是平等的: 空类别感知的异质联邦学习方法

    Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning. (arXiv:2401.02329v1 [cs.LG])

    [http://arxiv.org/abs/2401.02329](http://arxiv.org/abs/2401.02329)

    本研究提出了一种异质联邦学习方法FedED，通过同时进行空类别蒸馏和逻辑抑制，解决了在联邦学习中尚未充分识别空类别的问题。

    

    数据异质性是联邦学习中的一个重大挑战，表现为客户端之间本地数据分布的差异。现有方法常常在本地训练过程中采用类别平衡的技术来解决本地类别分布的异质性问题。然而，在少数类别中由于过拟合本地不平衡数据而导致准确性较差的问题仍然存在。本文提出了FedED，这是一种新颖的异质联邦学习方法，同时整合了空类别蒸馏和逻辑抑制。具体而言，空类别蒸馏利用知识蒸馏的方法在每个客户端的本地训练中保留了与空类别相关的重要信息。此外，逻辑抑制直接阻断了预测结果中对空类别的输出。

    Data heterogeneity, characterized by disparities in local data distribution across clients, poses a significant challenge in federated learning. Substantial efforts have been devoted to addressing the heterogeneity in local label distribution. As minority classes suffer from worse accuracy due to overfitting on local imbalanced data, prior methods often incorporate class-balanced learning techniques during local training. Despite the improved mean accuracy across all classes, we observe that empty classes-referring to categories absent from a client's data distribution-are still not well recognized. This paper introduces FedED, a novel approach in heterogeneous federated learning that integrates both empty-class distillation and logit suppression simultaneously. Specifically, empty-class distillation leverages knowledge distillation during local training on each client to retain essential information related to empty classes from the global model. Moreover, logit suppression directly p
    
[^82]: 针对软组织成像的镉锌碲化物(CZT)光子计数探测器表征

    Cadmium Zinc Telluride (CZT) photon counting detector Characterisation for soft tissue imaging. (arXiv:2401.02106v1 [physics.ins-det])

    [http://arxiv.org/abs/2401.02106](http://arxiv.org/abs/2401.02106)

    该研究表征了镉锌碲化物（CZT）光子计数探测器在识别各种组织方面的性能，推动了使用光子计数探测技术来克服传统CT探测器的限制。

    

    近年来，光子计数检测技术的应用引起了重大的X射线成像研究兴趣。计算机断层扫描（CT）扫描仪可以从光子计数探测器中受益，这是一种新技术，有潜力克服传统CT探测器的关键限制。研究人员正在研究在光子计数探测器中应用半导体探测材料用于检测软组织对比度的有效性和灵敏度。本研究旨在表征镉锌碲化物光子计数探测器在识别各种组织方面的性能。通过将X射线管电压和电流分别设置为25 keV、35 keV和0.5 mA、1.0 mA，评估了CZT探测器的最佳帧速率（FPS），通过保持最佳FPS固定，将探测器能量阈值从15 keV到35 keV设置为小步，将X射线管的电流设置为0.1 mA到1.0 mA的范围，以找到电压和电流之间的关系。

    The use of photon counting detection technology has resulted in significant X-ray imaging research interest in recent years. Computed Tomography (CT) scanners can benefit from photon-counting detectors, which are new technology with the potential to overcome key limitations of conventional CT detectors. Researchers are still studying the effectiveness and sensitivity of semiconductor detector materials in photon counting detectors for detecting soft tissue contrasts. This study aimed to characterize the performance of the Cadmium Zinc Telluride photon counting detector in identifying various tissues. An optimal frame rate per second (FPS) of CZT detector was evaluated by setting the X-ray tube voltage and current at 25 keV, 35 keV and 0.5 mA, 1.0 mA respectively by keeping the optimum FPS fixed, the detector energy thresholds were set in small steps from 15 keV to 35 keV and the Currents were set for X-ray tubes in ranges of 0.1 mA to 1.0 mA to find the relationship between voltage and
    
[^83]: AstroLLaMA-Chat: 使用对话和多样化数据集扩展AstroLLaMA

    AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets. (arXiv:2401.01916v1 [astro-ph.IM])

    [http://arxiv.org/abs/2401.01916](http://arxiv.org/abs/2401.01916)

    通过有针对性和持续的预训练，我们在天文学问题回答中扩展了AstroLLaMA，通过使用紧凑的LLaMA-2模型和专门的天文学语料库，我们实现了在专门主题理解方面的显著改进。我们还通过对特定领域的对话数据集进行微调，发布了带有聊天功能的AstroLLaMA。

    

    通过有针对性和持续的预训练，我们探索了在天文学问题回答中增强LLM性能的潜力。通过使用一个紧凑的7B参数的LLaMA-2模型，并且专注于一组经过筛选的天文学语料库，包括摘要、介绍和结论，我们在专门主题理解方面取得了显著的改进。虽然像GPT-4这样的通用LLMs在更广泛的问题回答场景中由于更强大的推理能力而表现出色，但我们的发现表明，有限资源的持续预训练仍然可以提高模型在专门主题上的性能。此外，我们提出了AstroLLaMA的扩展：在特定领域的对话数据集上对7B LLaMA模型进行微调，最终发布了适用于社区使用的具有聊天功能的AstroLLaMA。全面的定量基准测试正在进行中，并将在即将发布的完整论文中详细介绍。模型AstroLLaMA-Chat现已在...

    We explore the potential of enhancing LLM performance in astronomy-focused question-answering through targeted, continual pre-training. By employing a compact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of astronomy corpus -- comprising abstracts, introductions, and conclusions -- we achieve notable improvements in specialized topic comprehension. While general LLMs like GPT-4 outperform in broader question-answering scenarios due to superior reasoning capabilities, our findings suggest that continual pre-training with limited resources can still enhance model performance on specialized topics. Additionally, we present an extension of AstroLLaMA: the fine-tuning of the 7B LLaMA model on a domain-specific conversational dataset, culminating in the release of the chat-enabled AstroLLaMA for community use. Comprehensive quantitative benchmarking is currently in progress and will be detailed in an upcoming full paper. The model, AstroLLaMA-Chat, is now available at
    
[^84]: 使用全幻灯切片图像的组织伪影分割与严重性分析的自动诊断

    Tissue Artifact Segmentation and Severity Analysis for Automated Diagnosis Using Whole Slide Images. (arXiv:2401.01386v1 [eess.IV])

    [http://arxiv.org/abs/2401.01386](http://arxiv.org/abs/2401.01386)

    这篇论文提出了一种使用全幻灯切片图像进行组织伪影分割与严重性分析的自动诊断方法。通过计算机视觉和人工智能，可以在没有人类监督的情况下对整个全幻灯切片图像进行自主分析，但受到组织伪影影响的区域需要被准确识别和排除。

    

    传统上，病理学分析和诊断是由专家在显微镜下通过观察玻璃切片标本进行手动眼球判断来完成的。全幻灯切片图像是从玻璃切片制作的数字标本。全幻灯切片图像使得标本能够在计算机屏幕上观察，并引发了计算病理学，其中利用计算机视觉和人工智能进行自动分析和诊断。借助当前的计算进展，整个全幻灯切片图像可以在没有人类监督的情况下进行自主分析。然而，如果全幻灯切片图像受到组织伪影（如组织褶皱或气泡）的影响，则分析可能会失败或导致错误的诊断，这取决于伪影的严重程度。现有的伪影检测方法依赖于专家对严重程度的评估，以消除受到伪影影响的区域进行分析。这个过程耗时、繁琐，并且有损于自动化分析或伪影去除的目标。

    Traditionally, pathological analysis and diagnosis are performed by manually eyeballing glass slide specimens under a microscope by an expert. The whole slide image is the digital specimen produced from the glass slide. Whole slide image enabled specimens to be observed on a computer screen and led to computational pathology where computer vision and artificial intelligence are utilized for automated analysis and diagnosis. With the current computational advancement, the entire whole slide image can be analyzed autonomously without human supervision. However, the analysis could fail or lead to wrong diagnosis if the whole slide image is affected by tissue artifacts such as tissue fold or air bubbles depending on the severity. Existing artifact detection methods rely on experts for severity assessment to eliminate artifact affected regions from the analysis. This process is time consuming, exhausting and undermines the goal of automated analysis or removal of artifacts without evaluatin
    
[^85]: 无界损失的PAC-Bayes-Chernoff界限

    PAC-Bayes-Chernoff bounds for unbounded losses. (arXiv:2401.01148v1 [stat.ML])

    [http://arxiv.org/abs/2401.01148](http://arxiv.org/abs/2401.01148)

    这篇论文提出了一种用于无界损失的高概率PAC-Bayes参考界限，并通过优化自由参数解决了一些开放问题，并通过灵活的假设产生了新的广义界限。

    

    我们提出了一种新的用于无界损失的高概率PAC-Bayes参考界限。这个结果可以理解为Chernoff界限的PAC-Bayes版本。证明技巧依赖于通过Cramér变换对损失进行统一边界的尾部随机变量。我们强调了我们主要结果的两个应用。首先，我们证明了我们的界限解决了许多PAC-Bayes界限上的自由参数优化的开放问题。最后，我们证明了我们的方法允许在损失函数上进行灵活的假设，从而产生了广义了之前的界限，并且可以通过最小化来获得类似Gibbs的后验概率。

    We present a new high-probability PAC-Bayes oracle bound for unbounded losses. This result can be understood as a PAC-Bayes version of the Chernoff bound. The proof technique relies on uniformly bounding the tail of certain random variable based on the Cram\'er transform of the loss. We highlight two applications of our main result. First, we show that our bound solves the open problem of optimizing the free parameter on many PAC-Bayes bounds. Finally, we show that our approach allows working with flexible assumptions on the loss function, resulting in novel bounds that generalize previous ones and can be minimized to obtain Gibbs-like posteriors.
    
[^86]: 通过均匀地标抽样和约束局部线性嵌入实现可伸缩的流形学习

    Scalable manifold learning by uniform landmark sampling and constrained locally linear embedding. (arXiv:2401.01100v1 [cs.LG])

    [http://arxiv.org/abs/2401.01100](http://arxiv.org/abs/2401.01100)

    通过均匀地标抽样和约束局部线性嵌入，提出了一种可伸缩的流形学习方法，可以有效处理大规模和高维数据，并解决全局结构失真和可伸缩性问题。

    

    流形学习是机器学习和数据科学中的关键方法，旨在揭示高维空间中复杂非线性流形内在的低维结构。通过利用流形假设，已经开发了各种非线性降维技术来促进可视化、分类、聚类和获得关键洞察。虽然现有的流形学习方法取得了显著的成功，但仍然存在全局结构中的大量失真问题，这阻碍了对底层模式的理解。可伸缩性问题也限制了它们处理大规模数据的适用性。在这里，我们提出了一种可伸缩的流形学习(scML)方法，可以以有效的方式处理大规模和高维数据。它通过寻找一组地标来构建整个数据的低维骨架，然后将非地标引入地标空间中

    As a pivotal approach in machine learning and data science, manifold learning aims to uncover the intrinsic low-dimensional structure within complex nonlinear manifolds in high-dimensional space. By exploiting the manifold hypothesis, various techniques for nonlinear dimension reduction have been developed to facilitate visualization, classification, clustering, and gaining key insights. Although existing manifold learning methods have achieved remarkable successes, they still suffer from extensive distortions incurred in the global structure, which hinders the understanding of underlying patterns. Scalability issues also limit their applicability for handling large-scale data. Here, we propose a scalable manifold learning (scML) method that can manipulate large-scale and high-dimensional data in an efficient manner. It starts by seeking a set of landmarks to construct the low-dimensional skeleton of the entire data and then incorporates the non-landmarks into the landmark space based 
    
[^87]: 张量网络在可解释的机器学习中在网络安全中的应用

    Tensor Networks for Explainable Machine Learning in Cybersecurity. (arXiv:2401.00867v1 [cs.LG])

    [http://arxiv.org/abs/2401.00867](http://arxiv.org/abs/2401.00867)

    张量网络可以帮助发展可解释的机器学习算法，并提供丰富的模型可解释性。在网络安全中，我们的无监督聚类算法基于矩阵乘积状态，在性能上与传统的深度学习模型相媲美。我们的方法还能提取特征概率、熵和互信息，提供了分类异常的引人入胜的叙述，并实现了前所未有的透明度和可解释性水平。

    

    本文展示了张量网络如何帮助发展可解释的机器学习算法。具体而言，我们基于矩阵乘积状态（MPS）开发了一种无监督聚类算法，并将其应用于实际使用案例中的对手生成的威胁情报。我们的研究证明，MPS在性能方面可以与传统的深度学习模型如自编码器和生成对抗网络相媲美，同时提供更丰富的模型可解释性。我们的方法自然地促进了特征概率、冯·诺伊曼熵和互信息的提取，为异常分类提供了引人入胜的叙述，并促进了前所未有的透明度和可解释性水平，这对于理解人工智能决策的基本原理至关重要。

    In this paper we show how tensor networks help in developing explainability of machine learning algorithms. Specifically, we develop an unsupervised clustering algorithm based on Matrix Product States (MPS) and apply it in the context of a real use-case of adversary-generated threat intelligence. Our investigation proves that MPS rival traditional deep learning models such as autoencoders and GANs in terms of performance, while providing much richer model interpretability. Our approach naturally facilitates the extraction of feature-wise probabilities, Von Neumann Entropy, and mutual information, offering a compelling narrative for classification of anomalies and fostering an unprecedented level of transparency and interpretability, something fundamental to understand the rationale behind artificial intelligence decisions.
    
[^88]: Unicron: 在规模化自愈LLM训练中的经济节约

    Unicron: Economizing Self-Healing LLM Training at Scale. (arXiv:2401.00134v1 [cs.DC] CROSS LISTED)

    [http://arxiv.org/abs/2401.00134](http://arxiv.org/abs/2401.00134)

    Unicron是一个针对大规模语言模型训练中的自愈设计的工作负载管理器，通过最小化故障相关成本来优化训练过程，拥有带内错误检测、动态成本感知的计划生成机制和高效的转换策略。

    

    在各个领域中，训练大规模语言模型变得越来越关键，但由于频繁的失败，导致了显著的时间和经济成本。云端环境中当前的故障恢复方法无法充分解决各种复杂场景的问题，仅仅局限于减少个别任务的停机时间而忽视了对整个集群成本影响的考虑。我们引入了 Unicron，这是一个专为大规模语言模型训练中的有效自愈而设计的工作负载管理器。Unicron通过在集群内的多个并发任务中最小化与故障相关的成本来优化训练过程。其主要特点包括带内错误检测，实时识别错误而无需额外开销；动态成本感知的计划生成机制，以实现最优的重新配置；以及高效的转换策略，以减少状态变化期间的停机时间。在一个128-GPU的分布式集群上部署，Unicron展现了多达1.9倍的改进。

    Training large-scale language models is increasingly critical in various domains, but it is hindered by frequent failures, leading to significant time and economic costs. Current failure recovery methods in cloud-based settings inadequately address the diverse and complex scenarios that arise, focusing narrowly on erasing downtime for individual tasks without considering the overall cost impact on a cluster. We introduce Unicron, a workload manager designed for efficient self-healing in large-scale language model training. Unicron optimizes the training process by minimizing failure-related costs across multiple concurrent tasks within a cluster. Its key features include in-band error detection for real-time error identification without extra overhead, a dynamic cost-aware plan generation mechanism for optimal reconfiguration, and an efficient transition strategy to reduce downtime during state changes. Deployed on a 128-GPU distributed cluster, Unicron demonstrates up to a 1.9x improv
    
[^89]: 自监督预训练用于决策基础模型：形式化、流程和挑战

    Self-supervised Pretraining for Decision Foundation Model: Formulation, Pipeline and Challenges. (arXiv:2401.00031v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.00031](http://arxiv.org/abs/2401.00031)

    本文认为将大规模自监督预训练中的知识与决策问题相结合，可以解决决策中的样本效率和泛化性问题。通过提出先预训练再自适应的流程，并调研了决策预训练和下游推理中的数据收集、预训练目标和自适应策略的最新研究。最后，确定了发展决策基础模型的关键挑战和未来方向。

    

    决策是一个动态过程，需要感知、记忆和推理来进行选择并找到最优策略。传统的决策方法在样本效率和泛化性上存在问题，而大规模的自监督预训练已经使得语言和视觉领域的快速适应成为可能，通过微调或少样本学习。因此，我们提出将从大规模自监督预训练中获取的知识与下游决策问题融合起来。我们提出了先预训练再自适应的流程，并调研了决策预训练和下游推理中的数据收集、预训练目标和自适应策略的最新研究。最后，我们确定了在通用灵活的自监督预训练的帮助下，发展决策基础模型面临的关键挑战和未来方向。

    Decision-making is a dynamic process requiring perception, memory, and reasoning to make choices and find optimal policies. Traditional approaches to decision-making suffer from sample efficiency and generalization, while large-scale self-supervised pretraining has enabled fast adaptation with fine-tuning or few-shot learning in language and vision. We thus argue to integrate knowledge acquired from generic large-scale self-supervised pretraining into downstream decision-making problems. We propose Pretrain-Then-Adapt pipeline and survey recent work on data collection, pretraining objectives and adaptation strategies for decision-making pretraining and downstream inference. Finally, we identify critical challenges and future directions for developing decision foundation model with the help of generic and flexible self-supervised pretraining.
    
[^90]: CycleGAN模型用于MRI图像转换

    CycleGAN Models for MRI Image Translation. (arXiv:2401.00023v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2401.00023](http://arxiv.org/abs/2401.00023)

    该论文提出了一种CycleGAN模型，用于将MRI神经影像从一个场强转换到另一个场强。该模型能够以合理的准确性生成合成和重建图像。

    

    图像到图像的转换在医学领域中越来越受欢迎，可以将图像从一个领域转换到另一个领域。通过领域转换进行医学图像合成在能够增加数据集和学习更广义特征方面具有优势。本研究提出了一种CycleGAN模型，用于将神经影像从一个场强转换到另一个场强（例如从3特斯拉到1.5特斯拉）。与基于DCGAN架构的模型相比，CycleGAN能够以合理的准确性生成合成和重建图像。

    Image-to-image translation has gained popularity in the medical field to transform images from one domain to another. Medical image synthesis via domain transformation is advantageous in its ability to augment an image dataset where images for a given class is limited. From the learning perspective, this process contributes to data-oriented robustness of the model by inherently broadening the model's exposure to more diverse visual data and enabling it to learn more generalized features. In the case of generating additional neuroimages, it is advantageous to obtain unidentifiable medical data and augment smaller annotated datasets. This study proposes the development of a CycleGAN model for translating neuroimages from one field strength to another (e.g., 3 Tesla to 1.5). This model was compared to a model based on DCGAN architecture. CycleGAN was able to generate the synthetic and reconstructed images with reasonable accuracy. The mapping function from the source (3 Tesla) to target d
    
[^91]: MoTCoder: 使用思维模块提升大型语言模型在具有挑战性的编程任务中的能力。

    MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks. (arXiv:2312.15960v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.15960](http://arxiv.org/abs/2312.15960)

    MoTCoder是一个使用思维模块提升大型语言模型在挑战性编程任务中能力的框架，通过创新的指令调整促进任务的分解和模块化，显著提高生成解决方案的准确性和模块化程度。

    

    大型语言模型(LLMs)在处理简单的编程任务方面展示出了令人印象深刻的能力。然而，当面对更具挑战性的编程问题时，它们的性能往往表现不佳。我们观察到传统模型往往生成作为单一代码块的解决方案，限制了它们在解决复杂问题上的有效性。为了克服这个限制，我们提出了Modular-of-Thought Coder (MoTCoder)。我们引入了一种创新的MoT指令调整框架，旨在促进将任务分解为逻辑子任务和子模块。我们的研究发现，通过培养和利用子模块，MoTCoder显著提高了生成解决方案的模块化和正确性，导致在APPS上相对pass@1改进了12.9%，在CodeContests上相对pass@1改进了9.43%。我们的代码可在https://github.com/dvlab-research/MoTCoder获得。

    Large Language Models (LLMs) have showcased impressive capabilities in handling straightforward programming tasks. However, their performance tends to falter when confronted with more challenging programming problems. We observe that conventional models often generate solutions as monolithic code blocks, restricting their effectiveness in tackling intricate questions. To overcome this limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a pioneering framework for MoT instruction tuning, designed to promote the decomposition of tasks into logical sub-tasks and sub-modules. Our investigations reveal that, through the cultivation and utilization of sub-modules, MoTCoder significantly improves both the modularity and correctness of the generated solutions, leading to substantial relative pass@1 improvements of 12.9% on APPS and 9.43% on CodeContests. Our codes are available at https://github.com/dvlab-research/MoTCoder.
    
[^92]: Geo2SigMap: 使用地理数据库进行高保真度射频信号映射

    Geo2SigMap: High-Fidelity RF Signal Mapping Using Geographic Databases. (arXiv:2312.14303v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2312.14303](http://arxiv.org/abs/2312.14303)

    本文提出了Geo2SigMap，一种基于地理数据库的高效和高保真度射频信号映射框架。通过无缝集成OpenStreetMap（地理数据库）、Blender（计算机图形）等开源工具，利用机器学习技术对射频信号传播进行建模，实现在“未知”区域进行射频信号映射。

    

    射频（RF）信号映射是分析和预测特定区域内射频信号强度和分布的过程，对蜂窝网络规划和部署至关重要。传统的射频信号映射方法依赖于基于测量数据构建的统计模型，这种方法复杂度低但通常缺乏准确性，或者依赖射线跟踪工具，该工具提供了对目标区域的增强精度，但计算复杂度较高。最近，机器学习（ML）已经成为一种基于数据的方法，用于建模射频信号传播，它利用在合成数据集上训练的模型在“未知”区域进行射频信号映射。在本文中，我们提出了Geo2SigMap，一种基于ML的使用地理数据库进行高效且高保真度RF信号映射的框架。首先，我们开发了一个自动化框架，无缝集成了三个开源工具：OpenStreetMap（地理数据库）、Blender（计算机图形），

    Radio frequency (RF) signal mapping, which is the process of analyzing and predicting the RF signal strength and distribution across specific areas, is crucial for cellular network planning and deployment. Traditional approaches to RF signal mapping rely on statistical models constructed based on measurement data, which offer low complexity but often lack accuracy, or ray tracing tools, which provide enhanced precision for the target area but suffer from increased computational complexity. Recently, machine learning (ML) has emerged as a data-driven method for modeling RF signal propagation, which leverages models trained on synthetic datasets to perform RF signal mapping in "unseen" areas.  In this paper, we present Geo2SigMap, an ML-based framework for efficient and high-fidelity RF signal mapping using geographic databases. First, we develop an automated framework that seamlessly integrates three open-source tools: OpenStreetMap (geographic databases), Blender (computer graphics), a
    
[^93]: 基于显著特征的水声信号识别

    Underwater Acoustic Signal Recognition Based on Salient Feature. (arXiv:2312.13143v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2312.13143](http://arxiv.org/abs/2312.13143)

    这项研究提出了一种基于显著特征的水声信号识别方法，利用深度学习模型从频谱中提取特征，不断学习以分类水声信号，以应对复杂关系和提高识别准确率。

    

    随着技术的快速发展，复杂环境中水声信号的识别变得越来越重要。目前，主流的水声信号识别主要依赖于时频分析来提取频谱特征，在该领域广泛应用。然而，现有的识别方法过于依赖专家系统，面临着知识库受限和处理复杂关系的挑战。这些限制源于规则或推理引擎的复杂性和维护困难。鉴于深度学习在处理复杂关系方面的潜在优势，本文提出了一种利用神经网络进行水声信号识别的方法。该方法涉及从频谱中提取特征并不断学习以分类水声信号。深度学习模型能够自动学习抽象特征，提高识别准确率。

    With the rapid advancement of technology, the recognition of underwater acoustic signals in complex environments has become increasingly crucial. Currently, mainstream underwater acoustic signal recognition relies primarily on time-frequency analysis to extract spectral features, finding widespread applications in the field. However, existing recognition methods heavily depend on expert systems, facing limitations such as restricted knowledge bases and challenges in handling complex relationships. These limitations stem from the complexity and maintenance difficulties associated with rules or inference engines. Recognizing the potential advantages of deep learning in handling intricate relationships, this paper proposes a method utilizing neural networks for underwater acoustic signal recognition. The proposed approach involves continual learning of features extracted from spectra for the classification of underwater acoustic signals. Deep learning models can automatically learn abstra
    
[^94]: 闪存LLM：在有限内存下高效运行大型语言模型

    LLM in a flash: Efficient Large Language Model Inference with Limited Memory. (arXiv:2312.11514v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.11514](http://arxiv.org/abs/2312.11514)

    本文提出了一种在有限内存条件下高效运行大型语言模型的方法，通过将模型参数存储在闪存中并按需传输到DRAM的方式来解决内存限制的挑战。该方法通过构建推理成本模型并优化数据传输和读取方式，引入了窗口化和行列绑定两种主要技术。

    

    大型语言模型（LLM）在现代自然语言处理中起着至关重要的作用，在各种任务中表现出色。然而，它们庞大的计算和内存需求带来了挑战，特别是对于具有有限DRAM容量的设备而言。本文通过将模型参数存储在闪存中，并按需将其传输到DRAM的方式，解决了超过可用DRAM容量的LLM高效运行的挑战。我们的方法涉及构建一个考虑闪存特性的推理成本模型，引导我们在两个关键领域进行优化：减少从闪存传输的数据量，并以较大、更连续的块读取数据。在这个受硬件启发的框架内，我们引入了两个主要技术。首先，“窗口化”通过重复使用之前激活的神经元来策略性地减少数据传输，其次，“行列绑定”适应了闪存的顺序数据访问特点，

    Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, "windowing" strategically reduces data transfer by reusing previously activated neurons, and second, "row-column bundling", tailored to the sequential data access strengths of flash memory, 
    
[^95]: 处理未知情况：开集识别及相关领域的综述

    Managing the unknown: a survey on Open Set Recognition and tangential areas. (arXiv:2312.08785v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.08785](http://arxiv.org/abs/2312.08785)

    这篇综述主要总结了开集识别的最新研究进展，包括常见做法、限制以及与其他机器学习研究领域的联系，并提出了未来的研究方向。

    

    在实际场景中，分类模型常常需要在预测属于其训练阶段未出现的类别样本时具有鲁棒性。开集识别通过设计能够在测试阶段从样本中检测到未知类别的模型，同时在对属于已知类别的样本进行分类时保持良好的性能来解决这个问题。本综述全面概述了与开集识别相关的最近文献，识别了这个领域的常见做法、限制以及与其他机器学习研究领域（如连续学习、分布外检测、新颖性检测和不确定性估计）的联系。我们的工作还揭示了开放的问题，并提出了几个研究方向，这些方向可能促进和推进未来更安全的人工智能方法。

    In real-world scenarios classification models are often required to perform robustly when predicting samples belonging to classes that have not appeared during its training stage. Open Set Recognition addresses this issue by devising models capable of detecting unknown classes from samples arriving during the testing phase, while maintaining a good level of performance in the classification of samples belonging to known classes. This review comprehensively overviews the recent literature related to Open Set Recognition, identifying common practices, limitations, and connections of this field with other machine learning research areas, such as continual learning, out-of-distribution detection, novelty detection, and uncertainty estimation. Our work also uncovers open problems and suggests several research directions that may motivate and articulate future efforts towards more safe Artificial Intelligence methods.
    
[^96]: PromptBench：一个用于评估大型语言模型的统一库

    PromptBench: A Unified Library for Evaluation of Large Language Models. (arXiv:2312.07910v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.07910](http://arxiv.org/abs/2312.07910)

    PromptBench是一个用于评估大型语言模型的统一库，包括了提示语构建、提示语工程、数据集和模型加载、对抗性提示攻击、动态评估协议和分析工具等组件，旨在促进原创研究和创建新的基准测试、部署下游应用以及设计新的评估协议。

    

    对大型语言模型（LLMs）的评估对于评估其性能和减轻潜在的安全风险至关重要。本文介绍了PromptBench，一个用于评估LLMs的统一库。它由几个关键组件组成，研究人员可以轻松使用和扩展：提示语构建、提示语工程、数据集和模型加载、对抗性提示攻击、动态评估协议和分析工具。PromptBench旨在成为一个开放、通用和灵活的代码库，以促进原创研究，创建新的基准测试、部署下游应用和设计新的评估协议。代码可在https://github.com/microsoft/promptbench上找到，并将持续支持。

    The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.
    
[^97]: AI 控制：尽管存在意图性破坏，但提高安全性

    AI Control: Improving Safety Despite Intentional Subversion. (arXiv:2312.06942v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.06942](http://arxiv.org/abs/2312.06942)

    本研究针对大型语言模型的安全问题，探索了一系列旨在确保安全性的技术流程，能够对抗模型有意破坏的情况，为解决编程问题提供了可靠的解决方案。

    

    随着大型语言模型（LLMs）变得更加强大并且越来越多地自主部署，防止它们导致有害结果将变得越来越重要。研究人员已经探索了各种安全技术，例如使用模型来审核其他模型的输出，或使用红队技术揭示微妙的失效模式。然而，研究人员尚未评估这些技术在模型有意尝试破坏它们时是否仍然确保安全。在本文中，我们开发和评估了一系列对有意破坏具有鲁棒性的安全技术流程（“协议”）。我们研究了一个场景，通过使用强大但不可信的模型（在我们的情况下是GPT-4）、使用较弱的可信模型（在我们的情况下是GPT-3.5）以及有限的高质量可信劳动力访问，我们希望解决一系列编程问题。我们研究了旨在永远不提交包含后门的解决方案的协议，其中我们...

    As large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. Researchers have investigated a variety of safety techniques for this purpose, e.g. using models to review the outputs of other models, or red-teaming techniques to surface subtle failure modes. However, researchers have not evaluated whether such techniques still ensure safety if the model is itself intentionally trying to subvert them. In this paper, we develop and evaluate pipelines of safety techniques ("protocols") that are robust to intentional subversion.  We investigate a scenario in which we want to solve a sequence of programming problems, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to high-quality trusted labor. We investigate protocols that aim to never submit solutions containing backdoors, which we 
    
[^98]: KwaiAgents：基于大型语言模型的通用信息搜索智能体系统

    KwaiAgents: Generalized Information-seeking Agent System with Large Language Models. (arXiv:2312.04889v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.04889](http://arxiv.org/abs/2312.04889)

    本文介绍了 KwaiAgents，这是一个基于大型语言模型的通用信息搜索智能体系统。该系统能够利用语言模型作为认知核心，理解用户的查询，行为准则并参考外部文档，以提供高质量的知识和信息。

    

    人类由于好奇心的驱使，不断探索和理解周围的世界，从而发明了各种工具来满足这种好奇心。尽管人类无法在大脑中处理和记忆大量信息，但在批判思维、规划、反思以及利用现有工具与世界进行交互和解释方面卓越出色，使其能够高效地寻找答案。最近大型语言模型（LLM）的进步表明，机器可能也具备类似于人类的能力，即使参数数量受限，也能展示强大的能力。在本文中，我们介绍了 KwaiAgents，这是一个基于LLM的通用信息搜索智能体系统。在 KwaiAgents 中，我们提出了一种利用LLM作为认知核心的智能体系统，它能够理解用户的查询、行为准则和参考外部文档。智能体还可以更新查询结果，与用户进行互动，并提供高质量的知识和信息。

    Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents. The agent can also update an
    
[^99]: HGPROMPT: 少样本提示学习中用于连接同质和异质图的方法

    HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot Prompt Learning. (arXiv:2312.01878v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.01878](http://arxiv.org/abs/2312.01878)

    此论文提出了一种名为HGPROMPT的方法，用于连接同质和异质图，在少样本设置下进行提示学习，并通过预训练的同质和异质图来提高性能。

    

    图神经网络（GNNs）和异质图神经网络（HGNNs）是同质和异质图表示学习的重要技术，然而它们在端到端的监督框架中的性能很大程度上取决于任务特定监督的可用性。为了减少标注成本，对自我监督预训练的研究成为一种流行的范式，但是预训练模型和下游任务之间常常存在差距，导致目标的不一致。为了弥合这个差距，提示学习作为一种有前景的方法在少样本设置下正在崛起，而不需要对预训练模型进行完全微调。虽然早期已经对图上基于提示的学习进行了一些探索，但它们主要涉及同质图，忽略了在下游应用中普遍存在的异质图。在本文中，我们提出了HGPROMPT，一种新颖的预训练和提示框架，以统一预先训练的同质和异质图，以实现更好的性能提升。

    Graph neural networks (GNNs) and heterogeneous graph neural networks (HGNNs) are prominent techniques for homogeneous and heterogeneous graph representation learning, yet their performance in an end-to-end supervised framework greatly depends on the availability of task-specific supervision. To reduce the labeling cost, pre-training on self-supervised pretext tasks has become a popular paradigm,but there is often a gap between the pre-trained model and downstream tasks, stemming from the divergence in their objectives. To bridge the gap, prompt learning has risen as a promising direction especially in few-shot settings, without the need to fully fine-tune the pre-trained model. While there has been some early exploration of prompt-based learning on graphs, they primarily deal with homogeneous graphs, ignoring the heterogeneous graphs that are prevalent in downstream applications. In this paper, we propose HGPROMPT, a novel pre-training and prompting framework to unify not only pre-trai
    
[^100]: 通过联邦迁移学习对基础模型进行接地：一个通用框架

    Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.17431](http://arxiv.org/abs/2311.17431)

    本论文提出了一个通用框架，通过联邦迁移学习将基础模型接地，以解决面临的挑战，如受限的计算资源、数据隐私、模型异构性和模型所有权。这个框架可以帮助释放基础模型的潜力，并在不同行业中产生重要影响。

    

    基于广泛知识和强大的新兴能力编码的Foundation Models（FMs），如GPT-4，在各种自然语言处理和计算机视觉任务中取得了显著成功。通过将FMs适应于特定领域任务或增加领域特定知识来对其进行接地，我们可以充分发挥FMs的潜力。然而，接地FMs面临着多个挑战，主要是受限的计算资源、数据隐私、模型异构性和模型所有权。联邦迁移学习（FTL），即联邦学习和迁移学习的结合，为解决这些挑战提供了有希望的解决方案。近年来，学术界和工业界对通过FTL-FM利用FMs进行接地的需求强烈增长。受到FTL-FM研究的强劲增长和FTL-FM对工业应用的潜在影响的推动，我们提出了一个FTL-FM框架，用于在联邦学习环境中建立FMs的接地问题。

    Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and powerful emergent abilities have achieved remarkable success in various natural language processing and computer vision tasks. Grounding FMs by adapting them to domain-specific tasks or augmenting them with domain-specific knowledge enables us to exploit the full potential of FMs. However, grounding FMs faces several challenges, stemming primarily from constrained computing resources, data privacy, model heterogeneity, and model ownership. Federated Transfer Learning (FTL), the combination of federated learning and transfer learning, provides promising solutions to address these challenges. In recent years, the need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in both academia and industry. Motivated by the strong growth in FTL-FM research and the potential impact of FTL-FM on industrial applications, we propose an FTL-FM framework that formulates problems of grounding FMs in the federated lea
    
[^101]: 注释敏感性：训练数据收集方法影响模型性能

    Annotation Sensitivity: Training Data Collection Methods Affect Model Performance. (arXiv:2311.14212v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2311.14212](http://arxiv.org/abs/2311.14212)

    该研究发现训练数据收集方法对注释本身和下游模型性能产生影响。在对仇恨言论和冒犯性语言进行注释收集的实验中，发现注释工具的设计选择会对模型的性能产生明显差异。

    

    当训练数据由人工注释者收集时，注释工具的设计、给予注释者的指示、注释者的特征以及他们之间的互动都可能对训练数据产生影响。这项研究证明了创建注释工具时的设计选择也会影响基于得到的注释训练的模型。我们引入了"注释敏感性"这个术语，用来指代注释数据收集方法对注释本身以及下游模型性能和预测的影响。我们在五种实验条件下对仇恨言论和冒犯性语言进行注释收集，随机将注释者分配到不同条件下。然后，在每个得到的五个数据集上对BERT模型进行微调，并在每个条件的保留部分上评估模型性能。我们发现在以下方面条件之间存在明显差异：1）仇恨言论/冒犯性语言注释的比例，2）模型性能。

    When training data are collected from human annotators, the design of the annotation instrument, the instructions given to annotators, the characteristics of the annotators, and their interactions can impact training data. This study demonstrates that design choices made when creating an annotation instrument also impact the models trained on the resulting annotations. We introduce the term annotation sensitivity to refer to the impact of annotation data collection methods on the annotations themselves and on downstream model performance and predictions. We collect annotations of hate speech and offensive language in five experimental conditions of an annotation instrument, randomly assigning annotators to conditions. We then fine-tune BERT models on each of the five resulting datasets and evaluate model performance on a holdout portion of each condition. We find considerable differences between the conditions for 1) the share of hate speech/offensive language annotations, 2) model per
    
[^102]: 探索分割联邦学习的隐私-能耗权衡

    Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning. (arXiv:2311.09441v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.09441](http://arxiv.org/abs/2311.09441)

    本文研究了分割联邦学习（SFL）中隐私和能耗之间的权衡，强调了快速收敛的优势，并分析了切割层对客户端能耗和隐私的影响。

    

    分割联邦学习（SFL）最近已经成为一种有前景的分布式学习技术，充分利用了联邦学习和分割学习的优势。它强调了快速收敛的优势，同时解决了隐私问题。因此，这一创新受到了工业界和学术界的广泛关注。然而，由于SFL中模型在特定层（称为切割层）上被分割为客户端和服务器端模型，选择切割层可能对客户端的能耗和隐私产生重大影响，因为它影响了训练负担和客户端模型的输出。此外，确定切割层的设计挑战非常复杂，主要由于客户端的计算和网络能力的固有异质性。在本文中，我们全面概述了SFL的过程，并对能耗和隐私进行了深入分析。

    Split Federated Learning (SFL) has recently emerged as a promising distributed learning technology, leveraging the strengths of both federated learning and split learning. It emphasizes the advantages of rapid convergence while addressing privacy concerns. As a result, this innovation has received significant attention from both industry and academia. However, since the model is split at a specific layer, known as a cut layer, into both client-side and server-side models for the SFL, the choice of the cut layer in SFL can have a substantial impact on the energy consumption of clients and their privacy, as it influences the training burden and the output of the client-side models. Moreover, the design challenge of determining the cut layer is highly intricate, primarily due to the inherent heterogeneity in the computing and networking capabilities of clients. In this article, we provide a comprehensive overview of the SFL process and conduct a thorough analysis of energy consumption and
    
[^103]: ConvNet vs Transformer, Supervised vs CLIP: 超越ImageNet准确率

    ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy. (arXiv:2311.09215v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2311.09215](http://arxiv.org/abs/2311.09215)

    ConvNet和Transformer架构在监督和CLIP训练下，超越了ImageNet准确率的对比分析中发现它们在错误类型、输出校准、可转移性和特征的不变性等方面存在差异，突出了需要更加细致分析的必要性。

    

    现代计算机视觉为实践者提供了多种模型选择，对于特定应用从多个选项中选择模型可能具有挑战性。传统上，通过它们在ImageNet上的分类准确率来比较竞争模型架构和训练协议。然而，这个单一指标无法完全捕捉到对于专业任务至关重要的性能细微差别。在这项工作中，我们对ConvNet和Vision Transformer架构在监督和CLIP训练范式下进行了深入的比较分析，超越了ImageNet的准确率。尽管我们选择的模型在ImageNet准确率和计算需求上相似，我们发现它们在许多其他方面存在差异：错误类型、输出校准、可转移性和特征的不变性等。传统指标无法捕捉到的这种模型特性差异，突出了在选择不同模型时需要更加细致分析的必要性。

    Modern computer vision offers a great variety of models to practitioners, and selecting a model from multiple options for specific applications can be challenging. Conventionally, competing model architectures and training protocols are compared by their classification accuracy on ImageNet. However, this single metric does not fully capture performance nuances critical for specialized tasks. In this work, we conduct an in-depth comparative analysis of model behaviors beyond ImageNet accuracy, for both ConvNet and Vision Transformer architectures, each across supervised and CLIP training paradigms. Although our selected models have similar ImageNet accuracies and compute requirements, we find that they differ in many other aspects: types of mistakes, output calibration, transferability, and feature invariance, among others. This diversity in model characteristics, not captured by traditional metrics, highlights the need for more nuanced analysis when choosing among different models. Our
    
[^104]: FlashDecoding++: 在GPU上加速大规模语言模型推理的更快算法

    FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])

    [http://arxiv.org/abs/2311.01282](http://arxiv.org/abs/2311.01282)

    FlashDecoding++是一种快速的LLM推理引擎，通过解决同步部分softmax更新、未充分利用扁平GEMM计算和静态数据流导致的性能损失等挑战，实现了大规模语言模型推理的加速。

    

    随着大规模语言模型在各个领域的重要性日益增加，加速语言模型推理仍然存在一些挑战未解决：(1) 同步部分softmax更新。softmax操作需要同步更新每个部分softmax结果，导致LLM中注意力计算的开销增加约20%。(2) 未充分利用扁平GEMM计算。在LLM推理中执行GEMM的矩阵形状是扁平的，导致在先前的设计中填充零后计算未充分利用，性能损失超过50%。(3) 静态数据流导致的性能损失。LLM中的内核性能取决于不同的输入数据特征、硬件配置等。单一和静态的数据流可能导致LLM推理中不同形状的GEMM的性能损失达到50.25%。我们提出了FlashDecoding++，一种快速支持主流LLM和硬件后端的LLM推理引擎。为了解决上述挑战，FlashDecoding++实现了以下目标：

    As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and >50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.  We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
    
[^105]: 分层随机平滑

    Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])

    [http://arxiv.org/abs/2310.16221](http://arxiv.org/abs/2310.16221)

    分层随机平滑是一种在复杂数据上进行鲁棒性认证的解决方案，通过只在一个对象的子集上添加随机噪声，以更有针对性的方式提供了更强的鲁棒性保证和高准确性。

    

    真实世界的数据是复杂的，通常由可分解为多个实体的对象组成（例如，将图像分解为像素，将图形分解为相互连接的节点）。随机平滑是一种强大的框架，可以使模型在其输入的微小变化上具有证明的鲁棒性-通过在分类之前随机添加噪声来保证多数投票的鲁棒性。然而，当对手不是任意干扰整个对象（例如图像），而是对象的某个实体的子集（例如像素）时，通过随机平滑对这种复杂数据进行鲁棒性认证是具有挑战性的。作为解决方案，我们引入了分层随机平滑：我们通过仅在随机选择的实体子集上添加随机噪声来部分平滑对象。通过以比现有方法更有针对性的方式添加噪声，我们获得更强的鲁棒性保证，同时保持高准确性。我们使用不同的噪声分布初始化分层平滑，得到了新的鲁棒性保证。

    Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
    
[^106]: 神经运算符加速科学模拟和设计

    Neural Operators for Accelerating Scientific Simulations and Design. (arXiv:2309.15325v1 [cs.LG])

    [http://arxiv.org/abs/2309.15325](http://arxiv.org/abs/2309.15325)

    本论文介绍了一种称为神经运算符的人工智能框架，用于学习连续域函数之间的映射，可以加速科学模拟和设计中的计算需求，实现零射超分辨率以及替代现有的模拟器。

    

    目前，科学发现和工程设计受限于物理实验的时间和成本，这些实验通常是通过试验和直觉选择的，并需要深入的领域专业知识。数值模拟是物理实验的替代方法，但对于复杂的现实领域来说，由于现有数值方法的计算需求，通常是不可行的。人工智能（AI）通过开发快速的数据驱动代理模型，提供了一个潜在的范式转变。特别是，一个称为神经运算符的AI框架提供了一个基于连续域函数之间映射学习的原则性框架，例如时空过程和偏微分方程（PDE）。它们可以在训练过程中未见过的新位置进行外推和预测解决方案，即进行零射超分辨率。神经运算符可以增强甚至替代许多应用中的现有模拟器，例如计算力学流体学。

    Scientific discovery and engineering design are currently limited by the time and cost of physical experiments, selected mostly through trial-and-error and intuition that require deep domain expertise. Numerical simulations present an alternative to physical experiments, but are usually infeasible for complex real-world domains due to the computational requirements of existing numerical methods. Artificial intelligence (AI) presents a potential paradigm shift through the development of fast data-driven surrogate models. In particular, an AI framework, known as neural operators, presents a principled framework for learning mappings between functions defined on continuous domains, e.g., spatiotemporal processes and partial differential equations (PDE). They can extrapolate and predict solutions at new locations unseen during training, i.e., perform zero-shot super-resolution. Neural operators can augment or even replace existing simulators in many applications, such as computational flui
    
[^107]: 全连接的时空图用于多变量时序数据

    Fully-Connected Spatial-Temporal Graph for Multivariate Time-Series Data. (arXiv:2309.05305v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.05305](http://arxiv.org/abs/2309.05305)

    本论文提出了一种全连接的时空图神经网络（FC-STGNN）方法，用于有效地建模多变量时序数据中的时空依赖性。该方法能够捕捉不同时间戳上不同传感器之间的相关性，提供了一种全面建模时空依赖性的新途径。

    

    多变量时序（MTS）数据在各个应用领域都很重要。由于其序列性和多源性（多个传感器），MTS数据固有地展现了时空依赖性，包括时间戳之间的时间相关性和每个时间戳中传感器之间的空间相关性。为了有效利用这些信息，基于图神经网络的方法（GNNs）被广泛采用。然而，现有的方法分别捕获空间依赖性和时间依赖性，无法捕捉不同时间戳上不同传感器之间的相关性。忽视这样的相关性限制了在MTS数据中全面建模时空依赖性，从而限制了现有GNNs学习有效的表示。为了解决这个限制，我们提出了一种新方法，称为全连接的时空图神经网络（FC-STGNN），包括两个关键组成部分，即FC图构建和FC图卷积。

    Multivariate Time-Series (MTS) data is crucial in various application fields. With its sequential and multi-source (multiple sensors) properties, MTS data inherently exhibits Spatial-Temporal (ST) dependencies, involving temporal correlations between timestamps and spatial correlations between sensors in each timestamp. To effectively leverage this information, Graph Neural Network-based methods (GNNs) have been widely adopted. However, existing approaches separately capture spatial dependency and temporal dependency and fail to capture the correlations between Different sEnsors at Different Timestamps (DEDT). Overlooking such correlations hinders the comprehensive modelling of ST dependencies within MTS data, thus restricting existing GNNs from learning effective representations. To address this limitation, we propose a novel method called Fully-Connected Spatial-Temporal Graph Neural Network (FC-STGNN), including two key components namely FC graph construction and FC graph convolutio
    
[^108]: 多元时间序列分类中的图感知对比学习

    Graph-Aware Contrasting for Multivariate Time-Series Classification. (arXiv:2309.05202v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.05202](http://arxiv.org/abs/2309.05202)

    该论文提出了一种图感知对比学习方法，用于改进多元时间序列(MTS)分类任务。现有的对比学习方法忽视了MTS数据中的空间一致性，该方法通过图扩增和对比学习来保持传感器的稳定性和相关性，从而提高了对数据的表示能力。

    

    对比学习作为一种自监督学习范式，在多元时间序列（MTS）分类中变得流行起来。它确保了未标记样本的不同视图之间的一致性，然后学习这些样本的有效表示。现有的对比学习方法主要集中在通过时间扩增和对比技术实现时间一致性，旨在保护MTS数据的时间模式不受扰动。然而，它们忽视了需要确保传感器的稳定性和它们之间相关性的空间一致性。由于MTS数据通常来自多个传感器，确保空间一致性对于对比学习在MTS数据上的整体性能是至关重要的。因此，我们提出了一种图感知对比学习方法，用于实现MTS数据的空间一致性。具体而言，我们提出了包括节点和边扩增在内的图扩增，以保持传感器的稳定性和它们之间的相关性，然后应用图对比目标进行学习。

    Contrastive learning, as a self-supervised learning paradigm, becomes popular for Multivariate Time-Series (MTS) classification. It ensures the consistency across different views of unlabeled samples and then learns effective representations for these samples. Existing contrastive learning methods mainly focus on achieving temporal consistency with temporal augmentation and contrasting techniques, aiming to preserve temporal patterns against perturbations for MTS data. However, they overlook spatial consistency that requires the stability of individual sensors and their correlations. As MTS data typically originate from multiple sensors, ensuring spatial consistency becomes essential for the overall performance of contrastive learning on MTS data. Thus, we propose Graph-Aware Contrasting for spatial consistency across MTS data. Specifically, we propose graph augmentations including node and edge augmentations to preserve the stability of sensors and their correlations, followed by grap
    
[^109]: 无监督机器学习模型选择中的主观性

    Subjectivity in Unsupervised Machine Learning Model Selection. (arXiv:2309.00201v1 [cs.LG])

    [http://arxiv.org/abs/2309.00201](http://arxiv.org/abs/2309.00201)

    无监督机器学习模型选择具有主观性，模型选择结果受模型构建者偏好的影响，并可能导致选择的不一致性。需要对模型选择过程进行更加深入的研究和标准化。

    

    模型选择是无监督机器学习中必要的步骤。尽管有很多标准和指标，但模型选择仍然存在主观性。高度主观性可能会对各种机器学习研究的重复性和可再现性产生疑问，并对实际部署的模型的稳健性产生怀疑。然而，模型选择结果中模型构建者的偏好影响的影响尚未得到充分探索。本研究以隐马尔可夫模型为例，调查了模型选择中的主观性。我们邀请了33位参与者和三个大型语言模型（LLMs），在三个场景中进行模型选择。结果显示，无论是参与者还是LLMs的选择都存在变异性和不一致性，尤其是当不同的标准和指标存在分歧时。主观性来源包括对不同标准和指标重要性的不同意见，对模型应该有多简洁的不同看法，以及对数据规模的大小的看法。

    Model selection is a necessary step in unsupervised machine learning. Despite numerous criteria and metrics, model selection remains subjective. A high degree of subjectivity may lead to questions about repeatability and reproducibility of various machine learning studies and doubts about the robustness of models deployed in the real world. Yet, the impact of modelers' preferences on model selection outcomes remains largely unexplored. This study uses the Hidden Markov Model as an example to investigate the subjectivity involved in model selection. We asked 33 participants and three Large Language Models (LLMs) to make model selections in three scenarios. Results revealed variability and inconsistencies in both the participants' and the LLMs' choices, especially when different criteria and metrics disagree. Sources of subjectivity include varying opinions on the importance of different criteria and metrics, differing views on how parsimonious a model should be, and how the size of a da
    
[^110]: 使用不同机器学习方法来预测药物溶解度--提取化学特征的线性回归模型与图卷积神经网络模型

    Predicting Drug Solubility Using Different Machine Learning Methods -- Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network. (arXiv:2308.12325v1 [q-bio.QM])

    [http://arxiv.org/abs/2308.12325](http://arxiv.org/abs/2308.12325)

    本研究利用线性回归模型和图卷积神经网络模型预测药物溶解度，其中图卷积神经网络模型表现最好。通过线性回归模型的特征重要性分析，可以了解每个功能团对溶解度的影响。将图卷积神经网络的高性能与线性回归的可解释性相结合是未来工作的方向。

    

    预测给定分子的溶解度是制药行业中重要的任务，因此是一个广泛研究的课题。本研究利用现代计算资源的优势重新探讨了这个问题。我们在多个实验数据集上应用了两种机器学习模型，即线性回归模型和图卷积神经网络模型。两种方法都能做出合理的预测，而图卷积神经网络模型表现最好。然而，当前的图卷积神经网络模型是一个黑盒子，而线性回归模型的特征重要性分析提供了更多有关底层化学影响的洞察。利用线性回归模型，我们展示了每个功能团对整体溶解度的影响。最终，在设计新药时，了解化学结构如何影响化学性质是至关重要的。未来的工作应该致力于将图卷积神经网络的高性能与线性回归的可解释性相结合，释放出新的优势。

    Predicting the solubility of given molecules is an important task in the pharmaceutical industry, and consequently this is a well-studied topic. In this research, we revisited this problem with the advantage of modern computing resources. We applied two machine learning models, a linear regression model and a graph convolutional neural network model, on multiple experimental datasets. Both methods can make reasonable predictions while the GCNN model had the best performance. However, the current GCNN model is a black box, while feature importance analysis from the linear regression model offers more insights into the underlying chemical influences. Using the linear regression model, we show how each functional group affects the overall solubility. Ultimately, knowing how chemical structure influences chemical properties is crucial when designing new drugs. Future work should aim to combine the high performance of GCNNs with the interpretability of linear regression, unlocking new advan
    
[^111]: 通过预训练稳定RNN的梯度

    Stabilizing RNN Gradients through Pre-training. (arXiv:2308.12075v1 [cs.LG])

    [http://arxiv.org/abs/2308.12075](http://arxiv.org/abs/2308.12075)

    该研究通过预训练网络实现局部稳定性，拓展了已有的稳定性理论，可以应用于复杂结构的循环神经网络。

    

    学习的众多理论都建议通过防止梯度的方差以指数形式随深度或时间增长来稳定和改善训练。通常情况下，这些分析是在前馈全连接神经网络或单层循环神经网络上进行的，因为它们具有数学的可解性。与此相反，本研究表明，当体系结构过于复杂以至于无法进行解析初始化时，通过预训练网络实现局部稳定性是有效的。此外，我们扩展了已知的稳定性理论，涵盖了一个更广泛的深层循环网络家族，对数据和参数分布的要求较少，这个理论被称为局部稳定性条件（LSC）。我们的调查发现，经典的Glorot、He和正交初始化方案在应用于前馈全连接神经网络时可以满足LSC。然而，在对深层循环网络进行分析时，我们发现了一种新的指数增长来源。

    Numerous theories of learning suggest to prevent the gradient variance from exponential growth with depth or time, to stabilize and improve training. Typically, these analyses are conducted on feed-forward fully-connected neural networks or single-layer recurrent neural networks, given their mathematical tractability. In contrast, this study demonstrates that pre-training the network to local stability can be effective whenever the architectures are too complex for an analytical initialization. Furthermore, we extend known stability theories to encompass a broader family of deep recurrent networks, requiring minimal assumptions on data and parameter distribution, a theory that we refer to as the Local Stability Condition (LSC). Our investigation reveals that the classical Glorot, He, and Orthogonal initialization schemes satisfy the LSC when applied to feed-forward fully-connected neural networks. However, analysing deep recurrent networks, we identify a new additive source of exponent
    
[^112]: 领域感知微调：增强神经网络的适应性能力

    Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability. (arXiv:2308.07728v1 [cs.LG])

    [http://arxiv.org/abs/2308.07728](http://arxiv.org/abs/2308.07728)

    本文提出了领域感知微调（DAFT）方法，通过批归一化转换和线性探测与微调的集成，有效减轻微调过程中的特征畸变问题。

    

    微调预训练神经网络模型已成为各个领域广泛采用的方法。然而，微调可能导致已具备强大泛化能力的预训练特征提取器发生畸变。在适应新目标领域时减轻特征畸变至关重要。最近的研究表明，在进行微调之前，在分布数据集上对头层进行对齐处理可以处理特征畸变问题取得有希望的结果。然而，在微调过程中，批归一化层的处理存在显著局限性，导致性能不佳。在本文中，我们提出了领域感知微调（DAFT），一种新的方法，它结合了批归一化转换、线性探测和微调的特性。我们的批归一化转换方法通过减少对神经网络的修改来有效减轻特征畸变。此外，我们还引入了线性探测和微调的集成方法。

    Fine-tuning pre-trained neural network models has become a widely adopted approach across various domains. However, it can lead to the distortion of pre-trained feature extractors that already possess strong generalization capabilities. Mitigating feature distortion during adaptation to new target domains is crucial. Recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. Nonetheless, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integrati
    
[^113]: 使用大规模未标记的自然图像增强医疗AI模型的网络初始化

    Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images. (arXiv:2308.07688v1 [eess.IV])

    [http://arxiv.org/abs/2308.07688](http://arxiv.org/abs/2308.07688)

    该研究通过在非医学图像上利用自监督学习（SSL）预训练，取得了优于基于ImageNet的预训练的结果，从而实现了在医疗AI模型中增强网络初始化的目的。

    

    预训练数据集（如ImageNet）已成为医学图像分析的黄金标准。然而，自监督学习（SSL）的出现提供了通过利用未标记数据来学习强大特征的机会，从而可以绕过繁重的标注过程。在这项研究中，我们探索了SSL预训练在非医学图像上是否可以应用于胸部X射线，并与非医学图像和医学图像上的监督预训练进行了比较。我们利用视觉变换器，并根据以下方式初始化其权重：（i）基于自然图像的SSL预训练（DINOv2）、（ii）基于自然图像的监督预训练（ImageNet数据集），以及（iii）基于MIMIC-CXR数据库中的胸部X射线的监督预训练。我们在来自六个全球大型数据集的800,000多张胸部X射线上测试了我们的方法，诊断了20多种不同的影像所见。我们的SSL预训练在经过筛选的图像上不仅表现出色，而且超过了基于ImageNet的预训练（对所有数据集，P<0.001），而且在某些数据集上还超过了基于医学图像的预训练。

    Pre-training datasets, like ImageNet, have become the gold standard in medical image analysis. However, the emergence of self-supervised learning (SSL), which leverages unlabeled data to learn robust features, presents an opportunity to bypass the intensive labeling process. In this study, we explored if SSL for pre-training on non-medical images can be applied to chest radiographs and how it compares to supervised pre-training on non-medical images and on medical images. We utilized a vision transformer and initialized its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on chest radiographs from the MIMIC-CXR database. We tested our approach on over 800,000 chest radiographs from six large global datasets, diagnosing more than 20 different imaging findings. Our SSL pre-training on curated images not only outperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in cert
    
[^114]: FITS：模拟具有10k个参数的时间序列

    FITS: Modeling Time Series with $10k$ Parameters. (arXiv:2307.03756v1 [cs.LG])

    [http://arxiv.org/abs/2307.03756](http://arxiv.org/abs/2307.03756)

    FITS是一种轻量而强大的时间序列分析模型，通过在复杂频率域中进行插值操作，丢弃对时间序列数据影响微小的高频分量，实现了与最先进模型相当的性能，并且具有较小的模型参数数量，适用于边缘设备。

    

    本文介绍了FITS，一种轻量而强大的时间序列分析模型。与直接处理原始时间域数据的现有模型不同，FITS基于在复杂频率域中进行插值的原理操作时间序列。通过丢弃对时间序列数据影响微小的高频分量，FITS在时间序列预测和异常检测任务中实现了与最先进模型相当的性能，同时具有近似10k个参数的显著紧凑大小。这种轻量级模型可以轻松地在边缘设备上进行训练和部署，为各种应用创造了机会。

    In this paper, we introduce FITS, a lightweight yet powerful model for time series analysis. Unlike existing models that directly process raw time-domain data, FITS operates on the principle that time series can be manipulated through interpolation in the complex frequency domain. By discarding high-frequency components with negligible impact on time series data, FITS achieves performance comparable to state-of-the-art models for time series forecasting and anomaly detection tasks, while having a remarkably compact size of only approximately $10k$ parameters. Such a lightweight model can be easily trained and deployed in edge devices, creating opportunities for various applications. The anonymous code repo is available in: \url{https://anonymous.4open.science/r/FITS}
    
[^115]: 学习小波对椭圆算子的齐次化

    Learning Homogenization for Elliptic Operators. (arXiv:2306.12006v1 [math.NA])

    [http://arxiv.org/abs/2306.12006](http://arxiv.org/abs/2306.12006)

    本文提出了一种新的数据驱动方法，可以学习用于椭圆算子的齐次化映射，以建立考虑接口的齐次化本构定律，并且证明了该方法的有效性。

    

    多尺度偏微分方程在各种应用中都有出现。齐次化理论是消除小尺度依赖的强有力方法，可以得到简化的方程以及计算上的便利。在连续介质力学领域，齐次化对于导出包含微观物理学的本构定律以制定感兴趣的宏观量的平衡方程很关键。然而，获得齐次化本构定律通常是具有挑战性的，因为它们通常没有解析形式，并且可以展现在微观尺度上不存在的现象。针对这个问题，提出了数据驱动的学习本构定律方法。本文提出了一种新的基于数据驱动的学习椭圆算子齐次化映射的方法，可以说明与这种接口有关的齐次化建立。我们的方法通过构造适当的特征空间，以考虑底层几何学和微观结构，自适应地学习将这些接口合并到齐次化本构定律中。我们的数字结果表明，所提出的方法可以准确地捕捉有效的宏观行为，优于现有方法。

    Multiscale partial differential equations (PDEs) arise in various applications, and several schemes have been developed to solve them efficiently. Homogenization theory is a powerful methodology that eliminates the small-scale dependence, resulting in simplified equations that are computationally tractable. In the field of continuum mechanics, homogenization is crucial for deriving constitutive laws that incorporate microscale physics in order to formulate balance laws for the macroscopic quantities of interest. However, obtaining homogenized constitutive laws is often challenging as they do not in general have an analytic form and can exhibit phenomena not present on the microscale. In response, data-driven learning of the constitutive law has been proposed as appropriate for this task. However, a major challenge in data-driven learning approaches for this problem has remained unexplored: the impact of discontinuities and corner interfaces in the underlying material. These discontinui
    
[^116]: 针对部分强凸性，在Nesterov动量法下加速收敛深度神经网络

    Accelerated Convergence of Nesterov's Momentum for Deep Neural Networks under Partial Strong Convexity. (arXiv:2306.08109v1 [cs.LG])

    [http://arxiv.org/abs/2306.08109](http://arxiv.org/abs/2306.08109)

    本研究证明了针对一类新的目标函数，只有部分参数满足强凸性，Nesterov动量法在深度神经网络中实现了加速收敛。

    

    当前最先进的神经网络梯度下降收敛分析聚焦于表征损失函数的特性，例如Polyak-Lojaciewicz（PL）条件和受限强凸性。虽然在这些条件下梯度下降具有线性收敛性，但是Nesterov动量法是否在类似的条件和假设下具有加速收敛仍然是一个未解决的问题。在这项研究中，我们考虑了一类新的目标函数，只有部分参数满足强凸性，并证明了Nesterov动量法在这种目标函数下实现了加速收敛。我们提供了两种问题类别的实现，其中一种是深度ReLU网络，这是我们所知道的第一个证明非平凡神经网络体系结构具有加速收敛率的论文。

    Current state-of-the-art analyses on the convergence of gradient descent for training neural networks focus on characterizing properties of the loss landscape, such as the Polyak-Lojaciewicz (PL) condition and the restricted strong convexity. While gradient descent converges linearly under such conditions, it remains an open question whether Nesterov's momentum enjoys accelerated convergence under similar settings and assumptions. In this work, we consider a new class of objective functions, where only a subset of the parameters satisfies strong convexity, and show Nesterov's momentum achieves acceleration in theory for this objective class. We provide two realizations of the problem class, one of which is deep ReLU networks, which --to the best of our knowledge--constitutes this work the first that proves accelerated convergence rate for non-trivial neural network architectures.
    
[^117]: 使用合成MR图像进行脑肿瘤分割--GANs和扩散模型的比较

    Brain tumor segmentation using synthetic MR images -- A comparison of GANs and diffusion models. (arXiv:2306.02986v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2306.02986](http://arxiv.org/abs/2306.02986)

    本研究通过综合评估四种GANs和一种扩散模型，对使用合成MR图像进行脑肿瘤分割任务的性能进行了评估，结果显示使用合成图像训练的分割网络可以达到80% - 90%的实际图像训练性能。但对于扩散模型来说，训练图像的记忆化可能会成为一个问题。

    

    在医学成像中，由于伦理、匿名化和数据保护法规的限制，数据共享常常很复杂。生成式人工智能模型，如生成对抗网络（GANs）和扩散模型，可以生成非常逼真的合成图像，有可能促进数据共享。然而，在共享合成医学图像之前，必须首先证明它们可以用于训练不同的网络，且具有可接受的性能。因此，本研究对四种GANs（渐进式GAN，StyleGAN 1-3）和一种扩散模型进行了全面评估，用于脑肿瘤分割任务（使用两种分割网络，U-Net和Swin transformer）。我们的结果表明，使用合成图像训练的分割网络的Dice分数达到了使用实际图像训练时Dice分数的80% - 90%，但对于扩散模型来说，训练图像的记忆化可能会成为一个问题。

    Large annotated datasets are required for training deep learning models, but in medical imaging data sharing is often complicated due to ethics, anonymization and data protection legislation. Generative AI models, such as generative adversarial networks (GANs) and diffusion models, can today produce very realistic synthetic images, and can potentially facilitate data sharing. However, in order to share synthetic medical images it must first be demonstrated that they can be used for training different networks with acceptable performance. Here, we therefore comprehensively evaluate four GANs (progressive GAN, StyleGAN 1-3) and a diffusion model for the task of brain tumor segmentation (using two segmentation networks, U-Net and a Swin transformer). Our results show that segmentation networks trained on synthetic images reach Dice scores that are 80% - 90% of Dice scores when training with real images, but that memorization of the training images can be a problem for diffusion models if 
    
[^118]: 利用强化学习训练扩散模型

    Training Diffusion Models with Reinforcement Learning. (arXiv:2305.13301v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13301](http://arxiv.org/abs/2305.13301)

    本文研究了利用强化学习方法直接优化扩散模型以实现下游对象的问题，并提出一种称之为去噪扩散策略优化（DDPO）的有效策略梯度算法，能够适应难以通过提示表达的图像压缩等目标，以及通过人类反馈得出的美学质量等目标。

    

    扩散模型是一类灵活的生成模型，采用对数似然目标的近似训练。然而，大多数扩散模型的使用案例并不关注似然，而是关注人类感知的图像质量或药物效力等下游目标。本文研究利用强化学习方法直接优化扩散模型以实现此类目标。我们描述了将去噪视为多步决策问题的方法，并提出称之为去噪扩散策略优化（DDPO）的一类策略梯度算法，相对于替代的奖励加权似然方法更为有效。在实证研究中，DDPO能够适应难以通过提示表达的图像压缩等目标，以及通过人类反馈得出的美学质量等目标。最后，我们展示DDPO可以利用来自反馈的提示-图像对齐方式来进行优化。

    Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a 
    
[^119]: 非对称网络逼近用于跨域学习

    Approximation by non-symmetric networks for cross-domain learning. (arXiv:2305.03890v1 [cs.LG])

    [http://arxiv.org/abs/2305.03890](http://arxiv.org/abs/2305.03890)

    本文研究使用非对称内核进行基于内核网络逼近的通用方法，结果表明它可以在跨域学习中显著提高基于内核网络的逼近能力。

    

    在过去的30年中，机器学习在众多过程（如：浅层或深度神经网络逼近、径向基函数网络和各种内核方法）的逼近能力（表达能力）研究中促进了大量的研究。本文针对不变学习、传递学习和合成孔径雷达成像等应用，引入了一种使用非对称内核来研究基于内核网络逼近能力的通用方法。我们考虑使用一组内核的更一般方法，如广义平移网络（其中包括神经网络和平移不变核作为特殊情况）和旋转区函数核。与传统的基于内核的逼近方法不同，我们不能要求内核是正定的。研究结果表明，使用非对称内核可以显著提高内核网络的逼近能力，特别是对于源域和目标域可能在分布上不同的跨域学习。

    For the past 30 years or so, machine learning has stimulated a great deal of research in the study of approximation capabilities (expressive power) of a multitude of processes, such as approximation by shallow or deep neural networks, radial basis function networks, and a variety of kernel based methods. Motivated by applications such as invariant learning, transfer learning, and synthetic aperture radar imaging, we initiate in this paper a general approach to study the approximation capabilities of kernel based networks using non-symmetric kernels. While singular value decomposition is a natural instinct to study such kernels, we consider a more general approach to include the use of a family of kernels, such as generalized translation networks (which include neural networks and translation invariant kernels as special cases) and rotated zonal function kernels. Naturally, unlike traditional kernel based approximation, we cannot require the kernels to be positive definite. Our results 
    
[^120]: MC-ViViT: 多分支分类器-ViViT用于使用面部视频检测老年人轻度认知障碍

    MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos. (arXiv:2304.05292v1 [cs.CV])

    [http://arxiv.org/abs/2304.05292](http://arxiv.org/abs/2304.05292)

    本文提出了一种新的深度机器学习模型MC-ViViT，用于通过分析面部特征检测老年人轻度认知障碍。通过MC模块和结合损失函数来解决数据集样本不平衡问题，提高了算法的性能。

    

    深度机器学习模型包括卷积神经网络(CNN)已成功地应用于使用医学图像、问卷和视频检测轻度认知障碍(MCI)。本文提出了一种新的多分支分类器-视频视觉变换器(MC-ViViT)模型，通过分析面部特征区分MCI和正常认知。数据来自I-CONECT，一个旨在通过提供频繁视频聊天来改善认知功能的行为干预试验。MC-ViViT在一个分支中提取视频的时空特征，并通过MC模块增强表示。由于I-CONECT数据集中的样本不平衡问题（包含难易和正负样本），这使MC-ViViT的性能受到影响。我们提出了一种Hard-Easy和Positive-Negative样本的损失函数（HP Loss）来结合对比度调节损失Focal loss和AD-CORRE loss来解决不平衡问题。我们在I-CONECT数据集上的实验结果显示出该算法的有效性。

    Deep machine learning models including Convolutional Neural Networks (CNN) have been successful in the detection of Mild Cognitive Impairment (MCI) using medical images, questionnaires, and videos. This paper proposes a novel Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to distinguish MCI from those with normal cognition by analyzing facial features. The data comes from the I-CONECT, a behavioral intervention trial aimed at improving cognitive function by providing frequent video chats. MC-ViViT extracts spatiotemporal features of videos in one branch and augments representations by the MC module. The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE loss to address the imbalanced problem. Our experimental results on the I-CONECT dataset show th
    
[^121]: 联邦强化学习中的本地环境毒化攻击

    Local Environment Poisoning Attacks on Federated Reinforcement Learning. (arXiv:2303.02725v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02725](http://arxiv.org/abs/2303.02725)

    本文提出了第一个广义框架，将FRL的毒化攻击视为限制预算的优化问题，并设计了一个可以应用于基于策略的FRL的毒化攻击协议，通过训练一对私人评价者和公共评价者将其扩展到使用演员-评论家作为本地RL算法的FRL。

    

    联邦学习已成为解决传统强化学习任务的热门工具。多代理结构解决了传统强化学习中数据需求大的主要问题，而联邦机制保护了各个代理个体的数据隐私。然而，联邦机制也会暴露系统面临恶意代理的毒化攻击。本工作提出了第一个广义框架，将FRL的毒化攻击视为限制预算的优化问题，并设计了一个可以应用于基于策略的FRL的毒化攻击协议，并通过训练一对私人评价者和公共评价者将其扩展到使用演员-评论家作为本地RL算法的FRL。我们也讨论了从FL继承的一种传统防御策略以减轻这种风险。我们通过进行实验来验证我们的毒化攻击的有效性。

    Federated learning (FL) has become a popular tool for solving traditional Reinforcement Learning (RL) tasks. The multi-agent structure addresses the major concern of data-hungry in traditional RL, while the federated mechanism protects the data privacy of individual agents. However, the federated mechanism also exposes the system to poisoning by malicious agents that can mislead the trained policy. Despite the advantage brought by FL, the vulnerability of Federated Reinforcement Learning (FRL) has not been well-studied before. In this work, we propose the first general framework to characterize FRL poisoning as an optimization problem constrained by a limited budget and design a poisoning protocol that can be applied to policy-based FRL and extended to FRL with actor-critic as a local RL algorithm by training a pair of private and public critics. We also discuss a conventional defense strategy inherited from FL to mitigate this risk. We verify our poisoning effectiveness by conducting 
    
[^122]: 手术聚合：一种用于协同学习的分布式医学影像数据和多样任务协调框架

    Surgical Aggregation: A Collaborative Learning Framework for Harmonizing Distributed Medical Imaging Datasets with Diverse Tasks. (arXiv:2301.06683v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.06683](http://arxiv.org/abs/2301.06683)

    本论文提出了一种手术聚合的协同学习框架，该框架可用于协调和聚合分布式医学影像数据集的知识，并带有部分疾病注释，从而可训练具有完整胸部内可能出现的所有异常的临床实用、强大模型。

    

    大规模的胸部X光数据集已经通过深度学习进行异常检测，并有潜力为许多临床应用提供巨大的益处。然而，每个数据集仅专注于检测患者可能同时出现的一部分发现，从而限制了其临床效用。因此，数据协调对于聚合这些数据集来训练具有完整胸部内可能出现的所有异常的临床实用、强大模型至关重要。为此，我们提出了手术聚合，一种协同学习框架，用于协调和聚合分布式异构数据集的知识，并带有部分疾病注释。我们在合成的iid数据集和具有部分注释的真实大规模非iid数据集上评估了手术聚合。我们的结果表明，手术聚合显著优于当前的策略，具有更好的通用性。

    Large-scale chest x-ray datasets have been curated for the detection of abnormalities using deep learning, with the potential to provide substantial benefits across many clinical applications. However, each dataset focuses only on detecting a subset of findings that can be simultaneously present in a patient, thereby limiting its clinical utility. Therefore, data harmonization is crucial to leverage these datasets in aggregate to train clinically-useful, robust models with a complete representation of all abnormalities that may occur within the thorax. To that end, we propose surgical aggregation, a collaborative learning framework for harmonizing and aggregating knowledge from distributed heterogeneous datasets with partial disease annotations. We evaluate surgical aggregation across synthetic iid datasets and real-world large-scale non-iid datasets with partial annotations. Our results indicate that surgical aggregation significantly outperforms current strategies, has better general
    
[^123]: 一种用于并行谱聚类的分布式块Chebyshev-Davidson算法

    A Distributed Block Chebyshev-Davidson Algorithm for Parallel Spectral Clustering. (arXiv:2212.04443v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04443](http://arxiv.org/abs/2212.04443)

    本研究开发了一种用于并行谱聚类的分布式块Chebyshev-Davidson算法，通过解析谱估计和分布式并行计算实现高效率和可扩展性。

    

    我们开发了一种分布式块Chebyshev-Davidson算法，用于解决谱聚类中大规模领先特征值问题。首先，Chebyshev-Davidson算法的效率依赖于先前对特征值谱的了解，而对于估计来说可能很昂贵。通过对谱聚类中的拉普拉斯或规范化拉普拉斯矩阵进行解析谱估计，可以减轻这个问题，使得所提出的算法在谱聚类中非常高效。其次，为了使所提出的算法能够分析大数据，我们开发了一个分布式和并行版本，具有可扩展性。并行计算的加速比大约等于$\sqrt{p}$，其中$p$表示进程的数量。{我们将提供数值结果，以证明它在谱聚类中的效率以及在并行计算环境中与现有特征值求解器相比的可扩展性优势。}

    We develop a distributed Block Chebyshev-Davidson algorithm to solve large-scale leading eigenvalue problems for spectral analysis in spectral clustering. First, the efficiency of the Chebyshev-Davidson algorithm relies on the prior knowledge of the eigenvalue spectrum, which could be expensive to estimate. This issue can be lessened by the analytic spectrum estimation of the Laplacian or normalized Laplacian matrices in spectral clustering, making the proposed algorithm very efficient for spectral clustering. Second, to make the proposed algorithm capable of analyzing big data, a distributed and parallel version has been developed with attractive scalability. The speedup by parallel computing is approximately equivalent to $\sqrt{p}$, where $p$ denotes the number of processes. {Numerical results will be provided to demonstrate its efficiency in spectral clustering and scalability advantage over existing eigensolvers used for spectral clustering in parallel computing environments.}
    
[^124]: 自适应合并下的纵向网络有效估计

    Efficient Estimation for Longitudinal Network via Adaptive Merging. (arXiv:2211.07866v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.07866](http://arxiv.org/abs/2211.07866)

    本文提出了一个有效的纵向网络估计框架，利用自适应合并、张量分解和点过程等方法来减少估计偏差和方差。

    

    纵向网络由多个节点之间的时间边序列组成，其中时间边在实时中被观察到。随着在线社交平台和电子商务的兴起，它已经变得普遍，但在文献中往往被忽略。本文提出了一个有效的纵向网络估计框架，利用自适应网络合并、张量分解和点过程的优势。它合并相邻的稀疏网络，以扩大观测边的数量并减少估计方差，同时通过利用本地时间结构进行自适应网络邻域控制引入的估计偏差。提出了一个投影梯度下降算法来促进估计，其中每次迭代的估计错误上界被建立。进行了彻底的分析，以量化所提出方法的渐近行为，结果表明它可以显着减少估计偏差。

    Longitudinal network consists of a sequence of temporal edges among multiple nodes, where the temporal edges are observed in real time. It has become ubiquitous with the rise of online social platform and e-commerce, but largely under-investigated in literature. In this paper, we propose an efficient estimation framework for longitudinal network, leveraging strengths of adaptive network merging, tensor decomposition and point process. It merges neighboring sparse networks so as to enlarge the number of observed edges and reduce estimation variance, whereas the estimation bias introduced by network merging is controlled by exploiting local temporal structures for adaptive network neighborhood. A projected gradient descent algorithm is proposed to facilitate estimation, where the upper bound of the estimation error in each iteration is established. A thorough analysis is conducted to quantify the asymptotic behavior of the proposed method, which shows that it can significantly reduce the
    
[^125]: 降低难度和提高鲁棒性：基于 Bregman 散度视角的对抗训练

    Lower Difficulty and Better Robustness: A Bregman Divergence Perspective for Adversarial Training. (arXiv:2208.12511v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.12511](http://arxiv.org/abs/2208.12511)

    本文研究了通过降低优化难度来提高对抗训练中的鲁棒性。基于新颖的Bregman散度视角，分析了两种典型的对抗训练方法的学习目标，并找到了优化过程更容易的方法。提出的两种方法能够降低优化难度，并提供更好的鲁棒性。

    

    本文研究了通过降低优化难度来改善对抗训练（AT）中获得的对抗鲁棒性。为了更好地研究这个问题，我们建立了一个新颖的基于 Bregman 散度的对抗训练视角，在这个视角下，AT可以看作是训练数据点在负熵曲线上的滑动过程。基于这个视角，我们分析了两种典型的AT方法，即PGD-AT和TRADES的学习目标，并发现TRADES的优化过程比PGD-AT更容易，因为TRADES可以分离PGD-AT。此外，我们讨论了TRADES中熵的作用，并发现具有高熵的模型能够更好地学习鲁棒性。受到以上发现的启发，我们提出了两种方法，即FAIT和MER，它们不仅可以在10步PGD对手下降低优化难度，还能提供更好的鲁棒性。我们的工作表明，在10步PGD对手下降低优化难度是可行的，并能提供更好的鲁棒性。

    In this paper, we investigate on improving the adversarial robustness obtained in adversarial training (AT) via reducing the difficulty of optimization. To better study this problem, we build a novel Bregman divergence perspective for AT, in which AT can be viewed as the sliding process of the training data points on the negative entropy curve. Based on this perspective, we analyze the learning objectives of two typical AT methods, i.e., PGD-AT and TRADES, and we find that the optimization process of TRADES is easier than PGD-AT for that TRADES separates PGD-AT. In addition, we discuss the function of entropy in TRADES, and we find that models with high entropy can be better robustness learners. Inspired by the above findings, we propose two methods, i.e., FAIT and MER, which can both not only reduce the difficulty of optimization under the 10-step PGD adversaries, but also provide better robustness. Our work suggests that reducing the difficulty of optimization under the 10-step PGD a
    
[^126]: 使用联合学习技术对12导联心电图信号进行心律失常分类的应用

    Application of federated learning techniques for arrhythmia classification using 12-lead ECG signals. (arXiv:2208.10993v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.10993](http://arxiv.org/abs/2208.10993)

    本研究应用联合学习技术对12导联心电图信号进行心律失常分类，通过联合学习方法训练AI模型，实现对医疗数据的隐私保护，同时保持与集中学习方式相当的性能表现。

    

    基于人工智能的大型、精心策划的医学数据集的分析对于利用低功耗心电监测设备信息提供早期检测、更快诊断和更有效治疗具有很大潜力。然而，由于不当使用、不安全的存储或数据泄漏可能会侵犯个人隐私，获取来自不同来源的敏感医疗数据受到严格限制。本研究利用联合学习（FL）隐私保护方法，在来自六个异构来源的12导联传感器阵列采集的高清心电图的异构数据集上训练人工智能模型。我们评估了所得模型与以集中式学习（CL）方式训练的最先进模型的性能等效性。此外，我们还评估了我们的解决方案在独立和相同分布的联合数据以及非相同分布的联合数据上的性能。我们的方法涉及基于深度神经网络的机器学习技术。

    Artificial Intelligence-based (AI) analysis of large, curated medical datasets is promising for providing early detection, faster diagnosis, and more effective treatment using low-power Electrocardiography (ECG) monitoring devices information. However, accessing sensitive medical data from diverse sources is highly restricted since improper use, unsafe storage, or data leakage could violate a person's privacy. This work uses a Federated Learning (FL) privacy-preserving methodology to train AI models over heterogeneous sets of high-definition ECG from 12-lead sensor arrays collected from six heterogeneous sources. We evaluated the capacity of the resulting models to achieve equivalent performance compared to state-of-the-art models trained in a Centralized Learning (CL) fashion. Moreover, we assessed the performance of our solution over Independent and Identical distributed (IID) and non-IID federated data. Our methodology involves machine learning techniques based on Deep Neural Networ
    
[^127]: 量子人工视觉在制造业缺陷检测中的应用

    Quantum artificial vision for defect detection in manufacturing. (arXiv:2208.04988v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2208.04988](http://arxiv.org/abs/2208.04988)

    本文介绍了使用量子计算机视觉算法在制造业中检测缺陷的研究。比较了量子支持向量机和量子退火机的性能，发现量子算法在多个方面优于经典算法。该研究是首次将量子计算机视觉系统应用于制造业生产中的实际问题。

    

    本文考虑了使用嘈杂中型量子 (NISQ) 设备的量子计算机视觉算法，并将其与其经典对应物进行了比较。具体而言，我们考虑了两种方法：基于通用门的量子支持向量机 (QSVM) 和基于量子退火机的 QBoost。这些量子视觉系统在用于检测制造汽车零件中的缺陷的不平衡数据集上进行了基准测试。我们注意到，量子算法在多个方面优于其经典对应物，其中 QBoost 允许使用现有的量子退火机来分析更大的问题。文中还讨论了数据预处理，包括降维和对比度增强，以及 QBoost 中的超参数调整。据我们所知，这是首个将量子计算机视觉系统应用于制造业生产中具有工业意义的问题的实现。

    In this paper we consider several algorithms for quantum computer vision using Noisy Intermediate-Scale Quantum (NISQ) devices, and benchmark them for a real problem against their classical counterparts. Specifically, we consider two approaches: a quantum Support Vector Machine (QSVM) on a universal gate-based quantum computer, and QBoost on a quantum annealer. The quantum vision systems are benchmarked for an unbalanced dataset of images where the aim is to detect defects in manufactured car pieces. We see that the quantum algorithms outperform their classical counterparts in several ways, with QBoost allowing for larger problems to be analyzed with present-day quantum annealers. Data preprocessing, including dimensionality reduction and contrast enhancement, is also discussed, as well as hyperparameter tuning in QBoost. To the best of our knowledge, this is the first implementation of quantum computer vision systems for a problem of industrial relevance in a manufacturing production 
    
[^128]: 人工智能的新领域：设备上的AI训练和个性化

    A New Frontier of AI: On-Device AI Training and Personalization. (arXiv:2206.04688v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04688](http://arxiv.org/abs/2206.04688)

    基于深度学习的智能服务现已开始在设备上执行，以保护个人数据并降低云开销，而研究者提出了一种轻量级的设备上训练框架NNTrainer，可以在不牺牲准确性的同时减少内存消耗并实现智能服务的个性化。

    

    现代消费电子设备开始在设备上执行基于深度学习的智能服务，而不是在云服务器上，以保持个人数据在设备上并降低网络和云开销。我们将这样的趋势视为通过使用用户数据更新神经网络而无需将数据暴露在设备之外来个性化智能服务的机会：设备上的训练。然而，设备资源有限带来了显著的困难。我们提出了一种轻量级的设备上训练框架NNTrainer，它提供高度内存效率的神经网络训练技术，并基于细粒度执行顺序分析进行主动交换。此外，其优化不会牺牲准确性，并且对训练算法是透明的；因此，先前的算法研究可以在NNTrainer之上实现。评估结果表明，NNTrainer可以将内存消耗减少到1/20（节省95%！），并有效地个性化智能服务。

    Modern consumer electronic devices have started executing deep learning-based intelligence services on devices, not cloud servers, to keep personal data on devices and to reduce network and cloud costs. We find such a trend as the opportunity to personalize intelligence services by updating neural networks with user data without exposing the data out of devices: on-device training. However, the limited resources of devices incurs significant difficulties. We propose a light-weight on-device training framework, NNTrainer, which provides highly memory-efficient neural network training techniques and proactive swapping based on fine-grained execution order analysis for neural networks. Moreover, its optimizations do not sacrifice accuracy and are transparent to training algorithms; thus, prior algorithmic studies may be implemented on top of NNTrainer. The evaluations show that NNTrainer can reduce memory consumption down to 1/20 (saving 95%!) and effectively personalizes intelligence ser
    
[^129]: 去噪监督。 (arXiv:2202.02952v2 [eess.IV] UPDATED)

    Supervision by Denoising. (arXiv:2202.02952v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2202.02952](http://arxiv.org/abs/2202.02952)

    本文提出了一种去噪监督（SUD）框架，可以监督重建模型，并避免制作特定于成像领域的可微正则化器。

    

    基于学习的图像重建模型，例如基于 U-Net 的模型，如果要保证良好的泛化性能，则需要大量标记的图像数据。然而，在某些成像领域中，由于获取这些数据的成本，具有像素或体素级别精度的标记数据很少。在医学成像等领域，由于不存在单一的标准标签，标签之间存在大量的重复变异性，因此训练重建网络从带标签和无标签的示例中学习以更好地泛化（称为半监督学习），是一个实践和理论上的问题。 然而，针对图像重建的传统半监督学习方法通常需要手工制作针对某些给定成像问题的可微正则化器，这可能非常耗时。 在这项工作中，我们提出了“去噪监督”（SUD）框架，该框架使我们能够监督重建模型，同时避免手动制作特定于成像领域的可微正则化器。

    Learning-based image reconstruction models, such as those based on the U-Net, require a large set of labeled images if good generalization is to be guaranteed. In some imaging domains, however, labeled data with pixel- or voxel-level label accuracy are scarce due to the cost of acquiring them. This problem is exacerbated further in domains like medical imaging, where there is no single ground truth label, resulting in large amounts of repeat variability in the labels. Therefore, training reconstruction networks to generalize better by learning from both labeled and unlabeled examples (called semi-supervised learning) is problem of practical and theoretical interest. However, traditional semi-supervised learning methods for image reconstruction often necessitate handcrafting a differentiable regularizer specific to some given imaging problem, which can be extremely time-consuming. In this work, we propose "supervision by denoising" (SUD), a framework that enables us to supervise reconst
    
[^130]: 多智能体强化学习用于混合交通中连接和自主车道变更的合作

    Multi-agent Reinforcement Learning for Cooperative Lane Changing of Connected and Autonomous Vehicles in Mixed Traffic. (arXiv:2111.06318v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.06318](http://arxiv.org/abs/2111.06318)

    本文研究了连接和自主车辆在混合交通中的合作车道变更问题，提出了一种多智能体强化学习（MARL）方法，每个车辆根据动机制定车道变更决策。

    

    过去20年来，自主驾驶引起了广泛的研究兴趣，因为它提供了许多潜在的好处，包括解放驾驶员的劳累驾驶和缓解交通拥堵等。尽管取得了一些进展，但车道变更仍然是自主车辆（AV）面临的一大挑战，特别是在混合和动态交通场景中。最近，强化学习（RL）作为一种强大的数据驱动控制方法，在AV的车道变更决策中得到了广泛的探索，并取得了令人鼓舞的结果。然而，这些研究中的大多数都集中在单一车辆设置下，在多个AV与人驾驶车辆（HDV）共存的情况下，车道变更的研究却鲜有涉及。在本文中，我们将多个AV在混合交通高速公路环境中的车道变更决策制定建模为多智能体强化学习（MARL）问题，每个AV根据动机制定车道变更决策。

    Autonomous driving has attracted significant research interests in the past two decades as it offers many potential benefits, including releasing drivers from exhausting driving and mitigating traffic congestion, among others. Despite promising progress, lane-changing remains a great challenge for autonomous vehicles (AV), especially in mixed and dynamic traffic scenarios. Recently, reinforcement learning (RL), a powerful data-driven control method, has been widely explored for lane-changing decision makings in AVs with encouraging results demonstrated. However, the majority of those studies are focused on a single-vehicle setting, and lane-changing in the context of multiple AVs coexisting with human-driven vehicles (HDVs) have received scarce attention. In this paper, we formulate the lane-changing decision making of multiple AVs in a mixed-traffic highway environment as a multi-agent reinforcement learning (MARL) problem, where each AV makes lane-changing decisions based on the moti
    
[^131]: 对抗性攻击和防御的博弈论

    Game Theory for Adversarial Attacks and Defenses. (arXiv:2110.06166v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.06166](http://arxiv.org/abs/2110.06166)

    这项工作利用博弈论方法在防御对抗性攻击上取得了重要进展，通过随机化方法和去噪技术提高了网络的安全性和稳健性。

    

    对抗性攻击通过对数据集中的样本施加小但有意的最坏情况扰动来生成对抗性输入，这导致即使是最先进的深度神经网络也会以高置信度输出错误答案。因此，一些对抗性防御技术被开发出来来提高模型的安全性和稳健性，并避免它们被攻击。逐渐形成了攻击者和防御者之间的类似于游戏的竞争，双方都试图在彼此之间发挥最佳策略，同时最大化自己的回报。为了解决这个游戏，每个玩家根据对手的策略选择的预测来选择对手的最优策略。在这项工作中，我们站在防守的角度，运用博弈论方法来防御攻击。我们使用两种随机化方法，随机初始化和随机激活修剪，来创建网络的多样性。此外，我们使用一种去噪方法来减少对抗性扰动对模型的影响。

    Adversarial attacks can generate adversarial inputs by applying small but intentionally worst-case perturbations to samples from the dataset, which leads to even state-of-the-art deep neural networks outputting incorrect answers with high confidence. Hence, some adversarial defense techniques are developed to improve the security and robustness of the models and avoid them being attacked. Gradually, a game-like competition between attackers and defenders formed, in which both players would attempt to play their best strategies against each other while maximizing their own payoffs. To solve the game, each player would choose an optimal strategy against the opponent based on the prediction of the opponent's strategy choice. In this work, we are on the defensive side to apply game-theoretic approaches on defending against attacks. We use two randomization methods, random initialization and stochastic activation pruning, to create diversity of networks. Furthermore, we use one denoising te
    
[^132]: AutoGL：自动图学习库

    AutoGL: A Library for Automated Graph Learning. (arXiv:2104.04987v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.04987](http://arxiv.org/abs/2104.04987)

    AutoGL是第一个专门用于自动图机器学习的开源库，使用方便且易于扩展。它提供了一个完整的自动图学习流程，并支持各种图应用。

    

    近年来，图上的机器学习的研究兴趣和应用不断增加。然而，为不同的图数据集和任务手动设计最优的机器学习算法是不灵活、费时，并且需要专业知识，限制了其适应性和适用性。图上自动机器学习（AutoML）旨在为给定的图数据集和任务自动设计最优的机器学习算法，受到了相当大的关注。然而，现有的库中没有一个能完全支持图上的AutoML。为了填补这一空白，我们提出了自动图学习（AutoGL），这是第一个专门用于自动图机器学习的库。AutoGL是开源的，易于使用，而且易于扩展。具体而言，我们提出了一个包含与设备交互的后端、完整的自动图学习流程和支持的图应用的三层架构。

    Recent years have witnessed an upsurge in research interests and applications of machine learning on graphs. However, manually designing the optimal machine learning algorithms for different graph datasets and tasks is inflexible, labor-intensive, and requires expert knowledge, limiting its adaptivity and applicability. Automated machine learning (AutoML) on graphs, aiming to automatically design the optimal machine learning algorithm for a given graph dataset and task, has received considerable attention. However, none of the existing libraries can fully support AutoML on graphs. To fill this gap, we present Automated Graph Learning (AutoGL), the first dedicated library for automated machine learning on graphs. AutoGL is open-source, easy to use, and flexible to be extended. Specifically, we propose a three-layer architecture, consisting of backends to interface with devices, a complete automated graph learning pipeline, and supported graph applications. The automated machine learning
    

