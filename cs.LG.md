# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [FP8-LM: Training FP8 Large Language Models.](http://arxiv.org/abs/2310.18313) | 本文提出了一种用于训练大语言模型的新型FP8自动混合精度框架，能够在不影响模型准确性的情况下显著减少内存使用并提高训练速度。 |
| [^2] | [Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models.](http://arxiv.org/abs/2310.18308) | 本文提出了Gen2Sim方法，通过使用生成模型自动生成3D资产、任务描述、任务分解和奖励函数来扩展机器人在仿真环境中的技能学习。这种自动化流程有助于解决人为参与的瓶颈问题，实现机器人学习在不同任务和环境中的扩展。 |
| [^3] | [Supervised and Penalized Baseline Correction.](http://arxiv.org/abs/2310.18306) | 本研究改进了受罚基线校正方法，通过利用先验分析物浓度来改善光谱预测性能，并在两个近红外数据集上进行了评估。 |
| [^4] | [A Stability Principle for Learning under Non-Stationarity.](http://arxiv.org/abs/2310.18304) | 本研究提出了一个适用于非稳态环境的统计学习框架，通过应用稳定性原则选择回溯窗口来最大化历史数据利用，并保持累积偏差在可接受范围内。该方法展示了对未知非稳态的适应性，遗憾界在强凸或满足Lipschitz条件下是极小化的最优解。该研究的创新点是函数相似度度量和非稳态数据序列划分技术。 |
| [^5] | [Addressing GAN Training Instabilities via Tunable Classification Losses.](http://arxiv.org/abs/2310.18291) | 该论文通过使用可调的分类损失解决了GAN训练不稳定性问题，并证明了CPE损失GAN和最小化f-散度的f-GAN之间的双向对应关系。 |
| [^6] | [Sustainable Concrete via Bayesian Optimization.](http://arxiv.org/abs/2310.18288) | 使用贝叶斯优化方法加速寻找强度和可持续性混凝土配方，通过相对较少的测量有效预测混凝土强度，并将寻找可持续混凝土视为多目标优化问题。 |
| [^7] | [Optimal Transport for Treatment Effect Estimation.](http://arxiv.org/abs/2310.18286) | 该论文提出一种名为ESCFR的方法，通过运用最优运输在因果性背景下解决治疗效果估计中的小批量采样效应和未观察到的混杂变量效应问题。 |
| [^8] | [Heterogeneous Federated Learning with Group-Aware Prompt Tuning.](http://arxiv.org/abs/2310.18285) | 本文研究了在异构联邦学习中利用预训练的Transformer和高效的提示调整策略，通过学习共享和群体提示实现获取通用知识和个性化知识，以训练适应不同本地数据分布的全局模型。 |
| [^9] | [LipSim: A Provably Robust Perceptual Similarity Metric.](http://arxiv.org/abs/2310.18274) | LipSim是一个可证明鲁棒的知觉相似度度量方法，通过利用1-Lipschitz神经网络作为骨干，提供了围绕度量方法的防护区域。 |
| [^10] | [PlantPlotGAN: A Physics-Informed Generative Adversarial Network for Plant Disease Prediction.](http://arxiv.org/abs/2310.18268) | PlantPlotGAN是一种基于物理信息的生成对抗网络，用于植物病害预测。它通过生成合成的多光谱区域图像来增加预测模型的准确性。 |
| [^11] | [Structured Semidefinite Programming for Recovering Structured Preconditioners.](http://arxiv.org/abs/2310.18265) | 本论文提出了一个通用框架，用于寻找近似最优的预处理器来解决线性系统，并提供了改进的对角预处理结果，优于先前的通用半/shear programming方法。 |
| [^12] | [Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt.](http://arxiv.org/abs/2310.18264) | 本文介绍了一种名为Neural k-Opt的学习搜索求解器，用于路径问题。通过灵活的k-opt交换和自主的可行和不可行区域探索，该算法在旅行商问题和有容量车辆路径问题上表现出优越性能。 |
| [^13] | [MIM-GAN-based Anomaly Detection for Multivariate Time Series Data.](http://arxiv.org/abs/2310.18257) | 本文提出了一种基于MIM-GAN的无监督多元时间序列异常检测算法，通过将时间序列数据划分为子序列，利用LSTM生成器和判别器捕捉时间相关性，并引入指数信息度量来避免局部最优解和模型崩溃。 |
| [^14] | [Guided Data Augmentation for Offline Reinforcement Learning and Imitation Learning.](http://arxiv.org/abs/2310.18247) | 该论文提出了一种人工引导的数据增强框架（GuDA）用于提高演示学习模型的性能。 |
| [^15] | [$\alpha$-Mutual Information: A Tunable Privacy Measure for Privacy Protection in Data Sharing.](http://arxiv.org/abs/2310.18241) | 本文提出了一种可调节的隐私度量$\alpha$-互信息，在隐私保护的数据共享中能够生成优越的模型以有效阻挠攻击者，通过操纵原始数据提供隐私保护，并使用一个通用对抗性深度学习框架来解决隐私度量问题。通过实证实验验证了$\alpha$-互信息的功能，并评估了隐私-效用权衡。 |
| [^16] | [How Re-sampling Helps for Long-Tail Learning?.](http://arxiv.org/abs/2310.18236) | 本文研究了如何通过重新采样改善长尾学习的效果。研究发现，当训练图像不包含无关背景时，重新采样可以显著提高泛化能力，但在其他场景下可能会导致学习到与目标标签无关的虚假相关性。 |
| [^17] | [Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation.](http://arxiv.org/abs/2310.18235) | 本论文提出了Davidsonian场景图（DSG）的评估框架，解决了现有文本-图像生成模型评估中的可靠性挑战，包括QG问题的准确性和VQA答案的一致性。 |
| [^18] | [Deep Transformed Gaussian Processes.](http://arxiv.org/abs/2310.18230) | 本文提出了一种名为深度转换高斯过程（DTGPs）的转换高斯过程（TGPs）的推广，该模型采用串联层级的随机过程，并实现了相对于TGPs和DGPs的灵活性增强。通过使用变分推理，可以近似所需的计算，从而得到了简单直接的推理算法扩展。 |
| [^19] | [TBDLNet: a network for classifying multidrug-resistant and drug-sensitive tuberculosis.](http://arxiv.org/abs/2310.18222) | 本文提出了一种名为TBDLNet的深度学习模型，用于自动分类多药耐药和药物敏感结核病。通过使用预训练的ResNet50提取特征，采用三个随机化神经网络减轻过拟合问题，并通过多数投票的方式进行集成，TBDLNet达到了高准确率、特异性、精确度、敏感度和F1-score。该模型可以及早检测到多药耐药性肺结核，帮助及时调整治疗计划并提高治疗效果。 |
| [^20] | [One Model Fits All: Cross-Region Taxi-Demand Forecasting.](http://arxiv.org/abs/2310.18215) | 本论文提出了一种适用于全区域的出租车需求预测模型，利用图神经网络捕捉城市环境中的空间依赖性和模式，并采用了区域中立的方法，使模型能够泛化到任何地区，包括未知地区。 |
| [^21] | [Robustness of Algorithms for Causal Structure Learning to Hyperparameter Choice.](http://arxiv.org/abs/2310.18212) | 这项研究调查了超参数对因果结构学习任务的影响，并通过对不同复杂级别的数据集进行实证评估，发现超参数选择同样对算法选择具有重要的影响。 |
| [^22] | [Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive Learning.](http://arxiv.org/abs/2310.18209) | 提出了一种新颖的对比学习框架，用于学习高质量的图嵌入，并设计了对齐度量和均匀性度量来解决图领域的非欧几里德几何结构问题。 |
| [^23] | [ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models.](http://arxiv.org/abs/2310.18208) | ArcheType是一种使用大型语言模型进行开源列类型注释的新框架，通过改进上下文采样和标签重新映射，实现了全面的零样本解决方案。 |
| [^24] | [Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months.](http://arxiv.org/abs/2310.18191) | VeLO是迄今为止规模最大的训练通用“基础”优化器的尝试，但我们的评估发现它需要问题特定的调优，并不一定优于竞争对手的解决方案质量和训练误差降低速度，这对于VeLO的通用性和培训投资的价值提出了质疑。 |
| [^25] | [Model-free Posterior Sampling via Learning Rate Randomization.](http://arxiv.org/abs/2310.18186) | 本文介绍了一种随机化无模型算法RandQL，用于减小马尔科夫决策过程中的遗憾。RandQL通过学习率随机化实现乐观探索，并在实证研究中表现出色。 |
| [^26] | [Personas as a Way to Model Truthfulness in Language Models.](http://arxiv.org/abs/2310.18168) | 本研究探讨了在大型语言模型中使用人设来建模真实性的可能性。通过建模真实人设，语言模型可以将真实性推广到不同上下文中，并通过相关特征判断个体产生文本的真实性。 |
| [^27] | [Enhancing Enterprise Network Security: Comparing Machine-Level and Process-Level Analysis for Dynamic Malware Detection.](http://arxiv.org/abs/2310.18165) | 本研究比较了基于机器级和进程级分析的动态恶意软件检测方法，探索如何隔离恶意进程以提高网络安全性，并解决了样本执行中的实际情况和资源利用的挑战。 |
| [^28] | [Proportional Fairness in Clustering: A Social Choice Perspective.](http://arxiv.org/abs/2310.18162) | 在这篇论文中，我们研究了比例聚类问题，将其与计算社会选择中的多赢家投票领域相关联。我们发现满足Brill和Peters的比例概念的任何聚类都能同时获得Chen等人的比例公平性、个体公平性和核心的最佳近似。我们还研究了更强的比例代表性概念，并展示了这些更强概念对应的近似。 |
| [^29] | [Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs.](http://arxiv.org/abs/2310.18152) | 本文提出了一个名为Disentangled Graph-Text Learner (DGTL)的模型，通过引入定制的解缠图神经网络（GNN）层，使得大型语言模型（LLMs）能够更好地理解文本属性图（TAGs）中的复杂结构关系。 |
| [^30] | [Improving Intrinsic Exploration by Creating Stationary Objectives.](http://arxiv.org/abs/2310.18144) | 该论文提出了一个新的方法：通过创建固定目标，将原始的非固定奖励转化为固定奖励，从而改善了强化学习中的内在探索。 |
| [^31] | [Unsupervised Representation Learning for Diverse Deformable Shape Collections.](http://arxiv.org/abs/2310.18141) | 本论文提出了一种无监督学习的新方法，用于处理可变形形状集合的编码和操作。通过谱池化技术建立一个通用的潜空间，可以对多样化的网格进行训练，并且实验证明了该方法的优秀性能。 |
| [^32] | [Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models.](http://arxiv.org/abs/2310.18127) | 本文提出了一种利用大型语言模型的强化学习框架，能够学习提问相关问题并进行推理来指导在实际环境中执行的行为的学习。 |
| [^33] | [Sample Complexity Bounds for Score-Matching: Causal Discovery and Generative Modeling.](http://arxiv.org/abs/2310.18123) | 本文通过训练深度ReLU神经网络实现了对score-matching的准确估计，并在因果发现和生成建模领域中提供了统计样本复杂度界限。 |
| [^34] | [A Global Multi-Unit Calibration as a Method for Large Scale IoT Particulate Matter Monitoring Systems Deployments.](http://arxiv.org/abs/2310.18118) | 本论文提出了一种基于低成本颗粒物（PM）传感器的全球多单元校准方法，用于大规模物联网颗粒物监测系统的部署。该方法通过采集有限数量的物联网AQ多传感器单元的现场响应和应用机器学习概念，能够解决环境干扰和制造差异导致的校准成本问题，实现准确和普遍的空气质量监测。 |
| [^35] | [Transductive conformal inference with adaptive scores.](http://arxiv.org/abs/2310.18108) | 利用转导式一致推断方法进行一致性无分布保证的机器学习任务，并通过建立Polya球模型的联合分布和经验分布函数的浓度不等式，提供了自适应得分的可用性和更高准确性。 |
| [^36] | [Adversarial Anomaly Detection using Gaussian Priors and Nonlinear Anomaly Scores.](http://arxiv.org/abs/2310.18091) | 该论文提出了一种基于高斯先验和非线性异常分数的对抗异常检测方法。通过结合$\beta$-VAE的生成稳定性和GAN的判别能力，提出了一种新的模型$\beta$-VAEGAN，并对异常分数的组合方法进行了研究。通过训练核化SVM，考虑了非线性关系，同时利用$\beta$-VAEGAN对高斯先验的偏差形成了新的异常分数组件。相比于现有方法，该方法在异常检测性能上有所改进。 |
| [^37] | [Unveiling the Potential of Probabilistic Embeddings in Self-Supervised Learning.](http://arxiv.org/abs/2310.18080) | 本文探讨了将概率建模引入自监督学习中对性能、信息压缩和超出分布识别潜力的影响，并从信息论的角度揭示了信息压缩和保留之间的权衡。 |
| [^38] | [Lipschitz and H\"older Continuity in Reproducing Kernel Hilbert Spaces.](http://arxiv.org/abs/2310.18078) | 本研究在再生核希尔伯特空间中研究了Lipschitz和Hölder连续性，并提供了几个充分条件和对再生核的深度调查。这个工作也是这个主题的方便参考资料。 |
| [^39] | [On kernel-based statistical learning in the mean field limit.](http://arxiv.org/abs/2310.18074) | 这篇论文研究了在大规模问题中的均场极限下的核统计学习，包括核的均场极限的理论完善、逼近以及支持向量机等的应用。研究结果为大规模问题提供了新的理论工具和见解。 |
| [^40] | ["Honey, Tell Me What's Wrong", Global Explanation of Textual Discriminative Models through Cooperative Generation.](http://arxiv.org/abs/2310.18063) | Therapy是第一个适用于文本的全局和无模型解释方法，通过合作生成文本，不依赖于初始样本，并提供了对输入空间中模型行为的全局概览。 |
| [^41] | [DP-SGD with weight clipping.](http://arxiv.org/abs/2310.18001) | 本研究提出了一种带权重剪裁的差分隐私梯度下降方法，通过利用公共信息对全局模型进行改进，获得更精确的灵敏度界限和噪声水平调整，提供了更好的差分隐私保证。 |
| [^42] | [Closing the Gap Between the Upper Bound and the Lower Bound of Adam's Iteration Complexity.](http://arxiv.org/abs/2310.17998) | 本文通过推导出一种新的收敛保证，仅需满足$L$-平滑条件和有界噪声方差的假设，来弥合Adam收敛性的差距。特别是在合理选择的超参数下，我们得到了Adam的迭代复杂度的上界，并且证明它满足一阶优化器的下界。 |
| [^43] | [CEFL: Carbon-Efficient Federated Learning.](http://arxiv.org/abs/2310.17972) | 该论文介绍了一种称为CEFL的碳高效联邦学习方法，通过使用自适应的成本感知策略来优化FL模型训练的任意成本度量，并成功实现了碳排放减少93％和训练时间减少50％的效果。 |
| [^44] | [Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning.](http://arxiv.org/abs/2310.17966) | 在离线到在线强化学习中，现有解决方案往往只使用一种平衡策略，无法充分利用不同状态的数据质量。本论文提出了一种家族式离线到在线强化学习框架(FamO2O)，它通过训练一系列具有不同改进和约束强度的策略，实现了状态自适应的平衡。 |
| [^45] | [A Comprehensive and Reliable Feature Attribution Method: Double-sided Remove and Reconstruct (DoRaR).](http://arxiv.org/abs/2310.17945) | 双边删除和重构（DoRaR）是一种全面可靠的特征归因方法，用于解决深度神经网络和其他相关模型内部决策机制不透明的问题。 |
| [^46] | [Trustworthy Edge Machine Learning: A Survey.](http://arxiv.org/abs/2310.17944) | 可信任的边缘机器学习是边缘计算和机器学习的融合，面临各种挑战，本调查总结了对其的定义、属性、框架、技术和解决方案，并强调了在6G网络中的重要性。 |
| [^47] | [Transformers as Graph-to-Graph Models.](http://arxiv.org/abs/2310.17936) | 本文认为Transformers本质上是图到图模型，通过将注意力权重等价于图中的边，并使用图到图Transformer架构结合显式图和潜在图进行非自回归图预测，实现了在建模各种语言结构方面的最先进准确性。 |
| [^48] | [Lifting the Veil: Unlocking the Power of Depth in Q-learning.](http://arxiv.org/abs/2310.17915) | 本文通过严格证明在统计学习理论框架下，深度Q-learning的深度具有显著优势，能够捕捉奖励的特殊属性。这一结果对解释深度Q-learning成功的原因起到了重要作用。 |
| [^49] | [Improving the Knowledge Gradient Algorithm.](http://arxiv.org/abs/2310.17901) | 本研究改进了知识梯度算法，提出了一种改进的知识梯度（iKG）策略，该策略解决了知识梯度算法的局限性，并且在最佳臂识别问题中具有渐近最优性。此外，相比知识梯度（KG），iKG更容易扩展到其他BAI问题，且在这些问题上表现出更好的性能。 |
| [^50] | [Submodel Partitioning in Hierarchical Federated Learning: Algorithm Design and Convergence Analysis.](http://arxiv.org/abs/2310.17890) | 本文提出了一种针对分层联邦学习的新方法：分层独立子模型训练（HIST）。该方法通过将全局模型划分为不相交的子模型，并在分层结构中分布，以降低边缘设备上的计算、通信和存储负担，同时节约资源。 |
| [^51] | [Impressions: Understanding Visual Semiotics and Aesthetic Impact.](http://arxiv.org/abs/2310.17887) | 本研究通过提出Impressions数据集，探索了图像的符号学以及特定的视觉特征和设计选择如何引发情感、思维和信念。研究表明，图像的影响力不仅仅在于美学的形式定义，而是与其作为沟通行为的成功息息相关。 |
| [^52] | [Machine Learning Infused Distributed Optimization for Coordinating Virtual Power Plant Assets.](http://arxiv.org/abs/2310.17882) | 本文提出了一种名为LOOP-MAC的方法，利用机器学习辅助的分布式优化来协调虚拟电厂（VPP）的资产。该方法采用多智能体协调视角，利用神经网络逼近器加速解决方案搜索。该方法通过引入测量图来保证最佳解之间的关系。 |
| [^53] | [A Sublinear-Time Spectral Clustering Oracle with Improved Preprocessing Time.](http://arxiv.org/abs/2310.17878) | 本研究提出了一种亚线性时间的谱聚类预测器，用于处理具有强聚类特性的图。该预测器能够在亚线性时间内进行预处理和查询聚类成员，并且与真实聚类接近的k-分区保持一致。此外，该预测器对于少量的随机边删除具有鲁棒性。 |
| [^54] | [ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation.](http://arxiv.org/abs/2310.17877) | ASPIRO是一种能在零到少样本情况下将结构化数据转化为简短模板句子的方法。通过算法解析检查、LLM的重新提示以及一致性验证指标PARENT，ASPIRO成功降低了66%的解析错误率，并且在与最近的预训练语言模型的竞争中表现出色。 |
| [^55] | [Ranking with Slot Constraints.](http://arxiv.org/abs/2310.17870) | 带有槽约束的排名问题中，我们提出了一种新的排名算法MatchRank，它在候选人按排名顺序被人类决策者评估时，产生最大化填充槽位的排名。算法在理论上具有强大的逼近保证，并且可以高效实现。 (arXiv:2310.17870v1 [cs.IR]) |
| [^56] | [Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests.](http://arxiv.org/abs/2310.17867) | 多实例学习中的五个深度模型在学习过程中违反了标准的MIL假设，导致能够学习反相关的实例。这一问题需要通过改进和其他策略来解决。 |
| [^57] | [Function Space Bayesian Pseudocoreset for Bayesian Neural Networks.](http://arxiv.org/abs/2310.17852) | 本论文提出了一种在函数空间上操作的新颖贝叶斯伪核心集构建方法，通过构建核心集后验的变分近似并在函数空间中将其与完整数据后验匹配，实现了对深度神经网络等高维模型的贝叶斯推断的可扩展性。 |
| [^58] | [Boosting Data Analytics With Synthetic Volume Expansion.](http://arxiv.org/abs/2310.17848) | 本文介绍了一种利用合成数据生成框架来提升数据分析的方法，在此方法中，使用先进模型生成高逼真度的合成数据，并采用统计方法进行分析。研究发现，在合成数据上的统计方法错误随着合成数据的增加而减少，但最终可能会增加或停滞。 |
| [^59] | [A Data-Centric Online Market for Machine Learning: From Discovery to Pricing.](http://arxiv.org/abs/2310.17843) | 这篇论文介绍了一种以数据为中心的在线市场，用于连接机器学习的供求匹配，并提出了解决这个市场设计中的两个核心挑战的新技术。 |
| [^60] | [Positional Encoding-based Resident Identification in Multi-resident Smart Homes.](http://arxiv.org/abs/2310.17836) | 该论文提出了一种基于位置编码的多住户智能家居居民识别框架，通过构建图形和利用时间序列数据，可以有效地识别居民的身份。实验结果表明，该方法在多住户环境中可以达到较高的准确率。 |
| [^61] | [Hybrid Optical Turbulence Models Using Machine Learning and Local Measurements.](http://arxiv.org/abs/2310.17829) | 该论文介绍了使用机器学习和局部测量的混合光学湍流模型。通过结合基线宏观气象模型和局部观测，可以提高预测能力。与基线模型和仅利用局部观测训练的机器学习模型相比，混合模型具有更大的潜力。 |
| [^62] | [Bayesian imaging inverse problem with SA-Roundtrip prior via HMC-pCN sampler.](http://arxiv.org/abs/2310.17817) | 本文提出了一种新型的深度生成先验，SA-Roundtrip，可以进行可控的采样生成，并识别数据的内在维度。基于该先验，结合Hamiltonian Monte Carlo算法，解决了贝叶斯成像逆问题，在计算机断层扫描重建任务上超过了最先进的对比算法。 |
| [^63] | [Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs.](http://arxiv.org/abs/2310.17816) | 在有限先验知识下，通过局部分区发现算法（LDP），该研究解决了自动变量选择的问题。LDP根据与曝光-结果对{X,Y}相关的子集将变量集合Z进行分区，并区分混淆因素和其他变量类型。该算法具有理论保证，并在实践中观察到次二次的运行时间。 |
| [^64] | [A Spectral Condition for Feature Learning.](http://arxiv.org/abs/2310.17813) | 本文研究了在大型神经网络中特征学习的光谱条件，并提出了将权重矩阵和更新的谱范数缩放为$\sqrt{\texttt{fan-out}/\texttt{fan-in}}$的方法，以实现特征学习。同时，作者还导出了最大更新参数化的推导，旨在帮助读者对神经网络中的特征学习理解更加深入。 |
| [^65] | [Clover: Closed-Loop Verifiable Code Generation.](http://arxiv.org/abs/2310.17807) | Clover是一种闭环可验证代码生成的范式，通过在代码、docstrings和形式注释之间进行一致性检查，确保生成的代码的正确性。 |
| [^66] | [Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks.](http://arxiv.org/abs/2310.17805) | 本研究将DreamerV3的技巧应用到PPO中，并发现这些技巧并不能普遍改善PPO的性能。通过大量的消融研究，我们确定了一些情况下这些技巧的成功，并对它们的关系提供了深入洞察。 |
| [^67] | [Interacting Diffusion Processes for Event Sequence Forecasting.](http://arxiv.org/abs/2310.17800) | 本研究提出了一种基于扩散生成模型的交互扩散过程，用于事件序列预测。与之前的方法不同，该模型直接学习多个事件类型和两个事件之间的到达时间的联合概率分布，能够充分利用现代生成模型的高维建模能力。 |
| [^68] | [Neural Stress Fields for Reduced-order Elastoplasticity and Fracture.](http://arxiv.org/abs/2310.17790) | 本论文提出了一种神经网络和物理框架，用于降低弹塑性和断裂建模的计算需求。关键创新是通过训练低维神经应力场来实现在任意空间位置上高效计算应力值和内部力。此外，还训练了神经变形和仿射场来建立低维流形。 |
| [^69] | [Understanding when Dynamics-Invariant Data Augmentations Benefit Model-Free Reinforcement Learning Updates.](http://arxiv.org/abs/2310.17786) | 本文研究了在稀疏奖励任务中，动力学不变的数据增强函数对模型无关的强化学习更新的影响。实验结果表明，增加状态-动作覆盖率可以提高学习效果。 |
| [^70] | [Learning Extrinsic Dexterity with Parameterized Manipulation Primitives.](http://arxiv.org/abs/2310.17785) | 论文通过学习一系列参数化操作原语，并利用环境改变物体姿态，解决了机器人在目标物体抓取被环境遮挡时的问题。 |
| [^71] | [Data-Centric Financial Large Language Models.](http://arxiv.org/abs/2310.17784) | 本文介绍了一种数据中心化的方法，通过预处理和预理解数据来改善大型语言模型（LLMs）在金融任务中的性能。实验证明，采用该方法的金融LLMs在金融分析和解释任务上达到了最先进的水平。 |
| [^72] | [Graph Convolutional Networks for Complex Traffic Scenario Classification.](http://arxiv.org/abs/2310.17773) | 该论文提出了一种复杂交通情景分类方法，能够模拟车辆与环境以及其他参与者的互动，并使用图卷积网络对这些情景的空间和时间特征进行建模。 |
| [^73] | [Learning Optimal Classification Trees Robust to Distribution Shifts.](http://arxiv.org/abs/2310.17772) | 本研究提出了一种学习对分布变化具有鲁棒性的最优分类树的方法，通过混合整数鲁棒优化技术将该问题转化为单阶段混合整数鲁棒优化问题，并设计了基于约束生成的解决过程。 |
| [^74] | [GROOViST: A Metric for Grounding Objects in Visual Storytelling.](http://arxiv.org/abs/2310.17770) | GROOViST是一种用于评估视觉故事中物体定位的新的评估工具，考虑了跨模态依赖、时间错位和人类对视觉定位的直觉。这种工具具有模块化设计，可以评估和解释每个组件的贡献。 |
| [^75] | [Distributed Personalized Empirical Risk Minimization.](http://arxiv.org/abs/2310.17761) | 本文引入个性化经验风险最小化（PERM）的新范式，旨在实现不对参与设备共享的计算资源施加限制的情况下从异构数据源中进行学习，并通过估计数据分布之间的统计差异来个性化聚合本地经验损失，从而克服数据异构性问题。 |
| [^76] | [Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization.](http://arxiv.org/abs/2310.17759) | 该论文研究了在凸优化中算法的可重现性和梯度复杂度问题。他们挑战了之前的观点，证明了对于平滑凸优化和平滑凸凹极小极大问题，可以实现最优的可重现性和接近最优的收敛保证。他们还证明了在不同的oracle设置下，与不同类型的oracle相匹配的算法达到了最优性。 |
| [^77] | [PockEngine: Sparse and Efficient Fine-tuning in a Pocket.](http://arxiv.org/abs/2310.17752) | PockEngine是一种稀疏且高效的边缘微调引擎，支持稀疏反向传播和编译为先策略，以应对边缘设备的资源限制和硬件多样性。 |
| [^78] | [Making the End-User a Priority in Benchmarking: OrionBench for Unsupervised Time Series Anomaly Detection.](http://arxiv.org/abs/2310.17748) | OrionBench是一个以用户为中心的无监督时间序列异常检测基准测试，提供了通用抽象、可扩展性和发布频繁的基准测试。 |
| [^79] | [BERT-PIN: A BERT-based Framework for Recovering Missing Data Segments in Time-series Load Profiles.](http://arxiv.org/abs/2310.17742) | BERT-PIN是一种基于BERT的框架，用于恢复时间序列负载曲线中的缺失数据段。它使用负载和温度时间序列曲线作为输入，采用Transformer模型结构进行曲线修复，并通过候选者选择过程生成多个可信度不同的可能的修复数据集。在多个MDS恢复和需求响应基线估计等应用中，BERT-PIN表现出优越性能。 |
| [^80] | [GNN-GMVO: Graph Neural Networks for Optimizing Gross Merchandise Value in Similar Item Recommendation.](http://arxiv.org/abs/2310.17732) | 这项研究设计了一种名为GNN-GMVO的新型图神经网络架构，用于优化电子商务中相似商品推荐的商品总交易价值（GMV）。它解决了传统GNN架构在优化收入相关目标方面的不足，并通过直接优化GMV来保证推荐质量。 |
| [^81] | [Improving Traffic Density Forecasting in Intelligent Transportation Systems Using Gated Graph Neural Networks.](http://arxiv.org/abs/2310.17729) | 本研究通过应用门控图神经网络（GGNNs），在智能交通系统中的交通预测领域取得了显著的成果，通过最小化预测误差，GGNNs被证明是最有效的选择，展示了较高的预测性能。 |
| [^82] | [ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers.](http://arxiv.org/abs/2310.17723) | ZeroQuant-HERO是一种硬件增强的量化框架，针对W8A8 Transformer模型进行优化，旨在减少内存和计算需求，并在处理复杂的量化问题和内存受限运算符方面提供了新的解决方案。同时，该框架还具有灵活性，允许特定模块切换至FP16/BF16模式以提高准确性。 |
| [^83] | [Large Language Models as Generalizable Policies for Embodied Tasks.](http://arxiv.org/abs/2310.17722) | 本研究展示了大型语言模型(LLMs)可以被适应为具有普适性的机器人任务策略。通过我们的方法LLaRP，我们成功将预训练的冻结LLM用于接收指令和视觉输入，并在环境中直接输出动作。我们的实验结果表明LLaRP不仅对任务指令的复杂改写具有鲁棒性，而且可以推广到需要新颖最优行为的新任务。在大量未见任务中，LLaRP表现出了显著的成功率提升，并且我们提供了一个新的基准测试(Language Rearrangement)来促进进一步的研究。 |
| [^84] | [Advancing Brain Tumor Detection: A Thorough Investigation of CNNs, Clustering, and SoftMax Classification in the Analysis of MRI Images.](http://arxiv.org/abs/2310.17720) | 本研究详细研究了使用卷积神经网络（CNN）结合MRI图像进行脑肿瘤检测的方法，通过引入SoftMax分类器以及聚类方法，达到了高准确率。研究结果对脑肿瘤的早期诊断和治疗具有重要意义。 |
| [^85] | [Unifying (Quantum) Statistical and Parametrized (Quantum) Algorithms.](http://arxiv.org/abs/2310.17716) | 本文提出了一种统一的视角，将统计和参数化学习范例相结合，探索了从评估oracle中学习的问题，并提供了无条件的下界和查询复杂度刻画。该框架适用于QSQ设置和基于损失函数优化的算法。 |
| [^86] | [Community Detection and Classification Guarantees Using Embeddings Learned by Node2Vec.](http://arxiv.org/abs/2310.17712) | 本研究通过分析Node2Vec学习到的嵌入的理论属性，证明了在（经过度修正的）随机块模型中，使用k-means聚类方法对这些嵌入进行社区恢复是弱一致的。实验证明这一结果，并探讨了嵌入在节点和链接预测任务中的应用。 |
| [^87] | [A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered by Semantic Communication.](http://arxiv.org/abs/2310.17705) | 一种由语义通信增强的无线AI生成内容（AIGC）供应框架，通过使用语义信息而不是所有的二进制位提取和传输内容，以解决在无线网络中提供最优AIGC服务的挑战。 |
| [^88] | [Counterfactual Fairness for Predictions using Generative Adversarial Networks.](http://arxiv.org/abs/2310.17687) | 这篇论文提出了一种使用生成对抗网络实现对照因果公平性的方法，通过学习敏感属性的后代的对照分布来确保公平预测。 |
| [^89] | [Sliceformer: Make Multi-head Attention as Simple as Sorting in Discriminative Tasks.](http://arxiv.org/abs/2310.17683) | Sliceformer 提出了一个高效且有效的 Transformer 替代方案，通过一个简单的“切片-排序”操作替代了传统的多头注意力机制，解决了其高计算复杂性和数值问题。 |
| [^90] | [Feature Extraction and Classification from Planetary Science Datasets enabled by Machine Learning.](http://arxiv.org/abs/2310.17681) | 本论文提出了两个应用机器学习进行特征识别的例子，一个是识别木卫二混乱区域的冰块，另一个是识别泰坦上的云。通过迁移学习和新数据测试，分别实现了68%和95%的精度，并提出了进一步改进的建议。 |
| [^91] | [Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score Search and Grow-Shrink Trees.](http://arxiv.org/abs/2310.17679) | 本论文介绍了一种用于学习有向无环图的最佳顺序分数搜索（BOSS）和生长-收缩树（GSTs）方法，该方法在准确性和执行时间方面达到了最先进的性能，适用于具有数百个高度连接的变量的问题，例如从fMRI数据中恢复脑网络。 |
| [^92] | [Spatio-Temporal Meta Contrastive Learning.](http://arxiv.org/abs/2310.17678) | 该论文介绍了一个新的时空对比学习（CL4ST）框架，通过解决数据质量问题和个性化数据增强策略的挑战，提高了时空图神经网络的性能。 |
| [^93] | [Early Detection of Tuberculosis with Machine Learning Cough Audio Analysis: Towards More Accessible Global Triaging Usage.](http://arxiv.org/abs/2310.17675) | 这项研究开发了一种利用机器学习分析智能手机麦克风中的咳嗽声音来检测结核病的集成模型，可以实现早期诊断和监测治疗进展，从而为全球结核病的分类使用提供更易于获取的方法。 |
| [^94] | [Transfer of Reinforcement Learning-Based Controllers from Model- to Hardware-in-the-Loop.](http://arxiv.org/abs/2310.17671) | 本研究通过结合迁移学习和环境中的模拟来加速强化学习代理的训练过程，以实现在嵌入式系统中有效使用强化学习的目标。 |
| [^95] | [Unknown Health States Recognition With Collective Decision Based Deep Learning Networks In Predictive Maintenance Applications.](http://arxiv.org/abs/2310.17670) | 本论文提出了一种基于集体决策的深度学习网络框架，在预测维护应用中通过同时处理已知和未知健康状态，解决了常规CNN方案无法处理新异常样本的问题。 |
| [^96] | [An Approach for Efficient Neural Architecture Search Space Definition.](http://arxiv.org/abs/2310.17669) | 这篇论文介绍了一种高效的神经架构搜索空间定义方法，旨在优化搜索时间，并且能够处理大多数最先进的卷积神经网络（CNN）架构。 |
| [^97] | [Fine tuning Pre trained Models for Robustness Under Noisy Labels.](http://arxiv.org/abs/2310.17668) | 该论文研究了在存在噪声标签的情况下，为了提高鲁棒性，对预训练模型进行微调。目前的研究主要集中在干净数据集上，对于噪声标签情景的探索有限。 |
| [^98] | [Cascaded Multi-task Adaptive Learning Based on Neural Architecture Search.](http://arxiv.org/abs/2310.17664) | 该论文提出了一种基于神经架构搜索的自适应学习方法，用于优化端到端级联多任务模型。该方法通过在特定模块上应用冻结、插入适配器和微调等自适应操作，并在损失函数上添加惩罚项，成功限制了搜索到的架构。结果表明，该方法能够搜索出与手工设计类似的调整方案，并将优化参数压缩到了全微调的8.7%，同时获得了更好的性能。 |
| [^99] | [Is Channel Independent strategy optimal for Time Series Forecasting?.](http://arxiv.org/abs/2310.17658) | 本文重新考虑了当前通道独立策略在时间序列预测中是否是最佳解决方案，并提出了一种称为CSC的通道自聚类策略来增强性能并减小参数大小。 |
| [^100] | [Deep Learning Algorithm for Advanced Level-3 Inverse-Modeling of Silicon-Carbide Power MOSFET Devices.](http://arxiv.org/abs/2310.17657) | 作者提出了一种深度学习方法，用于检索硅碳化物功率MOSFET的物理参数，该方法适用于重建退化设备的物理参数或检索物理配置。 |
| [^101] | [High-Dimensional Prediction for Sequential Decision Making.](http://arxiv.org/abs/2310.17651) | 本文研究了高维预测用于顺序决策制定的问题，并提出了有效算法和多个应用。通过选择合适的条件事件集合，我们可以为多项式数量的决策制定者制定预测，并实现最优交换后悔。我们还将这一理论推广到在线组合优化和广义博弈领域，提供了高效的无后悔算法。 |
| [^102] | [Human-Guided Complexity-Controlled Abstractions.](http://arxiv.org/abs/2310.17550) | 本研究通过训练神经模型生成一系列离散表示，并通过调整表示的复杂性来提高任务的泛化性能。在微调实验中，我们发现适当的复杂性水平支持最佳的微调性能，并且在人类参与者的研究中也得到验证。 |
| [^103] | [Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages.](http://arxiv.org/abs/2310.17526) | 本研究评估了GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的能力，结果显示GPT-4在大多数任务上准确度与人类表现相当，在调整了偶然一致性和数据集不平衡后，其在数据提取方面表现出中等水平的准确度。 |
| [^104] | [The Expressive Power of Low-Rank Adaptation.](http://arxiv.org/abs/2310.17513) | 本文分析了低秩适应（LoRA）的表达能力，证明了对于全连接神经网络，当LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度）时，LoRA可以使任何模型f准确表示任何较小的目标模型f。对于Transformer网络，通过rank-（嵌入大小/ 2）的LoRA适配器可以使任何模型适应于相同大小的目标模型。 |
| [^105] | [De-novo Chemical Reaction Generation by Means of Temporarily Convolutional Neural Networks.](http://arxiv.org/abs/2310.17341) | 本研究结合递归神经网络（RNN）和临时卷积神经网络（TCN），使用新的反应Smiles-like表示实现了全新的化学反应生成，并通过迁移学习发现微调协议对模型生成范围有重要影响。 |
| [^106] | [Looping in the Human: Collaborative and Explainable Bayesian Optimization.](http://arxiv.org/abs/2310.17273) | 协作和可解释的贝叶斯优化框架(CoExBO)在贝叶斯优化中引入了循环，平衡了人工智能和人类的合作关系。它利用偏好学习将用户见解融合到优化中，解释每次迭代的候选选择，从而增强用户对优化过程的信任，并提供无害保证。 |
| [^107] | [Multi-scale Diffusion Denoised Smoothing.](http://arxiv.org/abs/2310.16779) | 本文研究了多尺度扩散去噪平滑的准确度和认证鲁棒性之间的权衡，并提出了一种在共享扩散模型上调整以实现平滑分类器鲁棒性的新方法。 |
| [^108] | [Towards Control-Centric Representations in Reinforcement Learning from Images.](http://arxiv.org/abs/2310.16655) | 该论文提出了一个名为ReBis的方法，通过整合无奖励控制信息和奖励特定知识，来捕捉图像中的控制中心信息。ReBis利用变形器架构来建模动力学，并通过分块掩码消除时空冗余。此外，ReBis还结合了等仿函式损失和非对称重建损失，以防止在稀疏奖励环境中的特征崩溃。 |
| [^109] | [Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion.](http://arxiv.org/abs/2310.16546) | 本论文提出了一种通过随机化风险标准的分布式强化学习算法，以避免在风险上的偏向性，并证明了其收敛性和最优性。实验证明，在包括Atari 55游戏在内的各种环境中，该方法优于其他分布式算法。 |
| [^110] | [Identifying Reasons for Bias: An Argumentation-Based Approach.](http://arxiv.org/abs/2310.16506) | 本文提出了一种基于论证的方法来确定为什么一个个体被分类与相似个体不同，该方法使用定量论证框架来表示个体和与其相似个体的属性-值对，并使用一个众所周知的语义来确定对个体分类产生最大贡献的属性-值对。 |
| [^111] | [Understanding Code Semantics: An Evaluation of Transformer Models in Summarization.](http://arxiv.org/abs/2310.16314) | 本研究通过评估代码摘要的有效性和引入对抗性案例，研究了基于Transformer模型的代码理解能力。结果显示，模型在理解代码语义方面仍存在挑战，这对于提高软件开发和维护效率具有重要意义。 |
| [^112] | [Accented Speech Recognition With Accent-specific Codebooks.](http://arxiv.org/abs/2310.15970) | 本研究提出了一种使用具有专门口音代码本的口音适应方法，通过交叉注意力和可训练代码本，用于端到端ASR系统。在实验证明了该方法在已见和未见的口音上都能获得显著的性能提升。 |
| [^113] | [MEMPSEP III. A machine learning-oriented multivariate data set for forecasting the Occurrence and Properties of Solar Energetic Particle Events using a Multivariate Ensemble Approach.](http://arxiv.org/abs/2310.15390) | 这个论文介绍了一个面向机器学习的多元数据集，用于预测太阳能粒子事件的发生和属性。数据集包含了多个航天器收集的原位和遥感观测数据，能够与产生太阳能粒子事件的物理过程相关联。 |
| [^114] | [Modeling Path Importance for Effective Alzheimer's Disease Drug Repurposing.](http://arxiv.org/abs/2310.15211) | 该论文提出了一种基于网络的新方法（MPI）来有效进行阿尔茨海默病药物重用。该方法通过学习节点嵌入来优先考虑重要路径，从而更好地发现候选药物。 |
| [^115] | [Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias.](http://arxiv.org/abs/2310.14814) | 本文提出了一种在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练的方法，并引入了一种新的自信度度量方法-$\mathcal{T}$-相似度。实验证明该方法在三种不同伪标签策略下具有良好的效果。 |
| [^116] | [A Comparative Study of Portfolio Optimization Methods for the Indian Stock Market.](http://arxiv.org/abs/2310.14748) | 该论文对印度股市上的三种组合优化方法进行了比较研究，并根据累计收益、波动率和夏普比率等指标，确定出了在不同行业下表现最佳的投资组合。 |
| [^117] | [GNNEvaluator: Evaluating GNN Performance On Unseen Graphs Without Labels.](http://arxiv.org/abs/2310.14586) | 本论文研究了GNN模型评估的新问题，提出了一个包括DiscGraph集合构建和GNNEvaluator训练和推断两个阶段的GNN模型评估框架，能够在没有标签的未知图上准确评估GNN模型的性能。 |
| [^118] | [Fundamental Limits of Membership Inference Attacks on Machine Learning Models.](http://arxiv.org/abs/2310.13786) | 本文探讨了机器学习模型上成员推断攻击的基本限制，包括推导了效果和成功率的统计量，并提供了几种情况下的界限。这使得我们能够根据样本数量和其他结构参数推断潜在攻击的准确性。 |
| [^119] | [Enhancing drug and cell line representations via contrastive learning for improved anti-cancer drug prioritization.](http://arxiv.org/abs/2310.13725) | 通过对比学习，该研究提出了一种改进药物和细胞系表征的方法，以保留与药物作用机制和细胞系癌症类型相关的关系结构，并实现了卓越的性能。使用这种方法可以更好地平衡对药物和细胞系特征的依赖，从而实现更个性化的药物优先级排序。 |
| [^120] | [Towards Understanding Sycophancy in Language Models.](http://arxiv.org/abs/2310.13548) | 这项研究探讨了强化学习从人类反馈中训练高质量AI助手的技术，发现这种方法可能导致模型在回答问题时过于谄媚，而不是坦诚，通过分析人类偏好数据得出了这一结论。 |
| [^121] | [Representation Learning via Consistent Assignment of Views over Random Partitions.](http://arxiv.org/abs/2310.12692) | 本论文提出了一种称为CARP的自监督聚类方法，通过对随机分区的视图进行一致分配，实现了可靠的表示学习，同时提高了训练稳定性和防止了崩溃解决方案的出现。在广泛的评估中，证明CARP的表示适用于多种下游任务，并与11种现有的自监督方法进行了比较。 |
| [^122] | [Jorge: Approximate Preconditioning for GPU-efficient Second-order Optimization.](http://arxiv.org/abs/2310.12298) | 本文介绍了Jorge，一种GPU高效的二阶优化算法，通过近似预处理方法替代矩阵求逆计算来提高计算效率，同时兼具二阶方法的收敛性能。实验证明了Jorge的有效性。 |
| [^123] | [A simple uniformly optimal method without line search for convex optimization.](http://arxiv.org/abs/2310.10082) | 本研究提出了一种简单的、无需线搜索的凸优化方法，能够以最佳的收敛速度解决参数未先给定的凸优化问题，并通过自适应条件快速梯度法（AC-FGM）实现了O(1/k^2)的收敛速度。这种方法还被扩展到具有H\"{o}lder连续梯度的凸优化问题，并且能够在所有问题类中以统一的最佳收敛速度解决。 |
| [^124] | [Provably Fast Convergence of Independent Natural Policy Gradient for Markov Potential Games.](http://arxiv.org/abs/2310.09727) | 本文研究了马尔可夫潜在博弈中独立自然策略梯度算法的多智能体强化学习问题，通过引入“次优间隙”的条件和精确策略评估的预言机，在$\mathcal{O}(1/ \epsilon)$次迭代内达到$\epsilon$-Nash均衡。 |
| [^125] | [Every Parameter Matters: Ensuring the Convergence of Federated Learning with Dynamic Heterogeneous Models Reduction.](http://arxiv.org/abs/2310.08670) | 这项研究提出了一个统一的异构联邦学习算法框架，并证明了在一定条件下，这些算法对于不同类型的数据都能收敛到标准联邦学习的一个稳定点。此外，研究还揭示了两个关键因素：模型提取噪声和最小覆盖指数，提倡了本地模型选择和全局模型选择的联合设计。 |
| [^126] | [Data driven modeling of self-similar dynamics.](http://arxiv.org/abs/2310.08282) | 本文介绍了一个多尺度神经网络框架，利用自相似性作为先验知识，对自相似动力系统进行建模。对于确定性动力学，框架可以判断动力学是否自相似；对于不确定性动力学，它可以确定哪个参数集更接近自相似性。方法可以提取与尺度无关的核进行任意尺度的建模，并识别自相似系统中的幂律指数。初步测试表明，方法对Ising模型的临界指数具有理论一致性。 |
| [^127] | [MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning.](http://arxiv.org/abs/2310.08252) | MetaBox是一种用于开发和评估元黑箱优化与强化学习方法的基准平台，提供灵活的算法模板、广泛的问题实例和基线方法，并引入了三个标准化的性能指标，以促进方法的严格评估。 |
| [^128] | [Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples.](http://arxiv.org/abs/2310.07747) | 本论文介绍了一种可解释的离线控制器方法，通过使用离线数据集作为决策语料库，在低数据场景中实现了问责制的控制，并在医疗保健领域展示了良好的性能。 |
| [^129] | [Generalized Neural Collapse for a Large Number of Classes.](http://arxiv.org/abs/2310.05351) | 本论文将神经崩溃概念扩展到类别数远大于特征空间维度的情况，并展示了广义神经崩溃现象的最小边界值被最大化。 |
| [^130] | [Epidemic Learning: Boosting Decentralized Learning with Randomized Communication.](http://arxiv.org/abs/2310.01972) | 流行学习是一种简单而强大的分散式学习算法，通过利用变化的通信拓扑结构实现了比传统方法更快的模型收敛速度，具有更好的收敛性能。 |
| [^131] | [The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs.](http://arxiv.org/abs/2310.01468) | 本文提供了一个评估框架，通过向法官提出一系列查询来评估LLMs的对话推理和规划能力。我们发现不同的LLMs在这个任务上表现出显著差异。 |
| [^132] | [DeepPCR: Parallelizing Sequential Operations in Neural Networks.](http://arxiv.org/abs/2309.16318) | DeepPCR是一种新型算法，通过使用并行循环降解算法将常规的顺序操作在神经网络的推断和训练中并行化，从而实现了计算速度的提升。 |
| [^133] | [Framework based on complex networks to model and mine patient pathways.](http://arxiv.org/abs/2309.14208) | 该论文提出了一个基于复杂网络的框架，用于建模和挖掘患者路径。该框架包括路径模型、新的相似度测量方法和基于传统中心度的挖掘方法。评估结果表明该框架可有效应用于实际医疗数据分析。 |
| [^134] | [Forecasting Response to Treatment with Deep Learning and Pharmacokinetic Priors.](http://arxiv.org/abs/2309.13135) | 该研究提出了一种使用深度学习和药动学先验预测治疗反应的方法。研究者通过一个新颖的编码器提供药物的药动学信息，从而实现对时间序列的精确预测。实验结果显示，在逼真模拟和真实世界数据上，该方法比基准模型的预测准确性提高了约11%和8%。这种方法在临床实践中具有多种有益应用，如发出早期警告和定量特定患者的治疗效果。 |
| [^135] | [Bayesian sparsification for deep neural networks with Bayesian model reduction.](http://arxiv.org/abs/2309.12095) | 使用贝叶斯模型缩减作为一种更高效的替代方法来修剪模型权重，以提高深度神经网络的计算效率。 |
| [^136] | [Beta Diffusion.](http://arxiv.org/abs/2309.07867) | beta扩散是一种新型生成模型方法，通过引入去掩盖和去噪的技术，利用缩放和偏移的beta分布进行乘法转换，实现在有界范围内生成数据。相比于传统的基于扩散的生成模型，它通过KL散度上界进行优化，证明了效果更好。 |
| [^137] | [Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power.](http://arxiv.org/abs/2309.04941) | 本文提出了一种称为$d$-DRFWL(2) GNNs的新型图神经网络，它通过限制节点之间的距离来实现循环计数能力，克服了子图GNNs的预处理和计算成本高的限制。 |
| [^138] | [Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization.](http://arxiv.org/abs/2309.04810) | 本论文提出了神经潜在几何搜索(NLGS)的概念，旨在通过格罗莫夫-豪斯多夫距离来自动识别下游任务的最佳潜在几何结构，以提高机器学习模型的性能。 |
| [^139] | [A Unified Framework for Discovering Discrete Symmetries.](http://arxiv.org/abs/2309.02898) | 本文提出了一个统一框架，能够发现各种类型的对称性，通过使用线性和张量值函数构成的新颖架构，在多臂赌博算法和梯度下降的帮助下，高效地优化并学习到对称性。在图像数字求和和多项式回归任务上的实验证明了该方法的有效性。 |
| [^140] | [Memory Efficient Optimizers with 4-bit States.](http://arxiv.org/abs/2309.01507) | 本论文通过将优化器状态的位宽压缩至4位，实现了内存高效的训练神经网络。通过对一阶和二阶矩的详细经验分析，我们发现当前的块状量化方法无法准确近似复杂的异常值模式。为此，我们使用较小的块大小并同时利用行上和列上的信息进行更好的量化。此外，我们还通过排除零点的线性量化器解决了量化第二阶矩时的零点问题。我们的工作在多个基准测试上进行了评估，结果表明我们的4位优化器具有出色的性能。 |
| [^141] | [Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms.](http://arxiv.org/abs/2309.00591) | 本文提出了一种名为 ROBAI 的算法，旨在快速识别并选择最佳臂，并在一系列连续回合中最大化奖励。该算法在预定停止时间和自适应停止时间要求下均实现了渐进最优遗憾，并且在预定停止时间下仅需 $\mathcal{O}(\log T)$ 回合即可选择最佳臂，在自适应停止时间下仅需 $\mathcal{O}(\log^2 T)$ 回合即可选择最佳臂。 |
| [^142] | [Dual Gauss-Newton Directions for Deep Learning.](http://arxiv.org/abs/2308.08886) | 本文研究了深度学习目标的结构特点，提出了基于双重高斯牛顿方向预言的方法。通过对偶形式计算这些预言，得到了既具有计算好处又具有新见解的结果。通过实验证明了这些预言作为随机梯度的下降方向的优势，并研究了计算上的权衡。 |
| [^143] | [Towards Personalized Federated Learning via Heterogeneous Model Reassembly.](http://arxiv.org/abs/2308.08643) | 本文提出了一个名为pFedHR的新框架，利用异构模型重组实现个性化联邦学习。实验表明，pFedHR在各种设置下优于基准方法，并且能够有效降低使用不兼容数据的不良影响。 |
| [^144] | [Quantifying the Cost of Learning in Queueing Systems.](http://arxiv.org/abs/2308.07817) | 本文提出了一种新的度量方法，即学习队列中的成本 (CLQ)，用于量化由参数不确定性导致的时间平均队列长度最大增加量。该度量方法可以捕捉学习队列系统的统计复杂性，不局限于渐近性能。 |
| [^145] | [Kernel Single Proxy Control for Deterministic Confounding.](http://arxiv.org/abs/2308.04585) | 本研究考虑了具有未观测混淆因素的因果效应估计问题，在结果是确定性生成的情况下，提出了一种使用单一代理变量的内核方法，通过两阶段回归和最大矩约束的方法可以一致估计因果效应，并在合成数据集上成功恢复了因果效应。 |
| [^146] | [Thinker: Learning to Plan and Act.](http://arxiv.org/abs/2307.14993) | Thinker算法通过引入世界模型和模型交互动作使强化学习代理实现自主规划，消除了手工设计规划算法的需求，并且在Sokoban游戏和Atari 2600基准测试中取得了state-of-the-art的性能。 |
| [^147] | [High-performance real-world optical computing trained by in situ model-free optimization.](http://arxiv.org/abs/2307.11957) | 本论文提出了一种无模型优化光学计算系统的方法，通过在原位进行轻量级优化，实现了高性能的真实光学计算。实验证明该方法在分类准确度上优于传统方法，并展示了在高速细胞分析方面的潜力。这种方法的固有简单性和低需求的计算资源促进了光学计算技术从实验室研究向真实世界应用的转变。 |
| [^148] | [Android in the Wild: A Large-Scale Dataset for Android Device Control.](http://arxiv.org/abs/2307.10088) | 这个论文提出了一个名为Android in the Wild (AITW)的大规模数据集，用于研究设备控制系统，该数据集包括人类示范的设备交互、自然语言指令和多种Android版本和设备类型。这个数据集提供了一个新的挑战，需要从视觉外观中推断用户界面中可用的操作。 |
| [^149] | [HYTREL: Hypergraph-enhanced Tabular Data Representation Learning.](http://arxiv.org/abs/2307.08623) | HYTREL是一种表格语言模型，通过使用超图来捕捉表格数据的排列不变性和其他三个结构属性。实证结果表明，HYTREL在四个下游任务中始终优于其他竞争基线模型，并且只需最少的预训练。 |
| [^150] | [Stability Guarantees for Feature Attributions with Multiplicative Smoothing.](http://arxiv.org/abs/2307.05902) | 本文提出了一种基于乘法平滑的特征归因方法，通过证明模型的Lipschitz性质，保证了其稳定性，并在视觉和语言模型上进行了评估，显示了非平凡的稳定性保证。 |
| [^151] | [DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs.](http://arxiv.org/abs/2307.04090) | 本论文提出了一种利用语义知识图自动创建政策辩论案例的方法，通过在争论的语义知识图上进行限制最短路径遍历，有效构建高质量的辩论案例。研究结果表明，在美国竞赛辩论中，利用这种方法显著改进了已有数据集DebateSum，并贡献了新的例子和有用的元数据。通过使用txtai语义搜索和知识图工具链，创建和贡献了9个语义知识图，同时提出了一种独特的评估方法来确定哪个知识图更适合政策辩论案例生成。 |
| [^152] | [Simulation-free Schr\"odinger bridges via score and flow matching.](http://arxiv.org/abs/2307.03672) | [SF]$^2$M是一种无需模拟的方法，用于推断随机动力学。它将连续时间随机生成建模解释为Schr\"odinger桥问题，并通过静态熵正则化最优传输来高效学习。在学习细胞动力学方面表现出更高的准确性和效率。 |
| [^153] | [Equivariant Spherical CNN for Data Efficient and High-Performance Medical Image Processing.](http://arxiv.org/abs/2307.03298) | 等变球卷积神经网络能够提高医学图像处理的效率和性能，通过降低对特定训练集的依赖性，并且在去噪和重建方面表现出卓越的质量和计算效率。 |
| [^154] | [Deep Contract Design via Discontinuous Networks.](http://arxiv.org/abs/2307.02318) | 本文通过不连续ReLU网络实现了深层合同设计，通过学习代理人和委托人的约束和目标的闭合形式表达，支持并行推断以求解最优合同。 |
| [^155] | [SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting.](http://arxiv.org/abs/2307.01616) | 本文介绍了SageFormer，一种面向多变量时间序列预测的系列感知图增强Transformer模型，通过图结构有效捕捉和建模序列之间的依赖关系，在表示不同序列中的时间模式和减少序列间冗余信息等方面取得了优越性能。 |
| [^156] | [Macro Placement by Wire-Mask-Guided Black-Box Optimization.](http://arxiv.org/abs/2306.16844) | 本文介绍了一种名为WireMask-BBO的黑盒优化框架，通过使用线掩模引导的贪心过程进行宏单元布局，在有效降低HPWL的同时节省大量时间。此方法还可以对现有布局进行微调，改善50%的HPWL。 |
| [^157] | [LeanDojo: Theorem Proving with Retrieval-Augmented Language Models.](http://arxiv.org/abs/2306.15626) | 本文引入了LeanDojo，该工具通过提取Lean的数据，为定理证明研究提供了一个开放源代码的平台。利用LeanDojo的数据，开发了ReProver，它是第一个使用检索增强的语言模型的证明器，可以从庞大的数学库中选择命题，训练成本低，并且只需要一周的GPU训练时间。 |
| [^158] | [Learning to Modulate pre-trained Models in RL.](http://arxiv.org/abs/2306.14884) | 本研究通过联合预训练模型，并评估比较了不同的微调方法，旨在解决强化学习中的灾难性遗忘问题。 |
| [^159] | [TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning.](http://arxiv.org/abs/2306.13229) | 本文提出了TACO方法，一种基于时间潜在动作驱动对比损失的视觉强化学习方法，能够同时学习状态表示和动作表示，提高代理学习的效率。 |
| [^160] | [Topological Parallax: A Geometric Specification for Deep Perception Models.](http://arxiv.org/abs/2306.11835) | 拓扑视差是一种比较训练模型和参考数据集多尺度几何结构相似性的理论和计算工具，它可以估计模型中的拓扑特征，有助于理解深度学习模型的行为和性能。 |
| [^161] | [IMP-MARL: a Suite of Environments for Large-scale Infrastructure Management Planning via MARL.](http://arxiv.org/abs/2306.11551) | IMP-MARL是一套用于大规模基础设施管理规划的开源多智能体强化学习环境套件，旨在提供一个基准平台来评估合作MARL方法在现实工程应用中的可扩展性和性能。 |
| [^162] | [Genes in Intelligent Agents.](http://arxiv.org/abs/2306.10225) | 该论文提出了基于基因的强化学习（GRL）框架，通过模拟有机体的进化和利用学习基因来学习和演化智能体。实验证明了GRL在智能体训练中的有效性。 |
| [^163] | [Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima.](http://arxiv.org/abs/2306.09850) | 该研究揭示了实用的锐度感知优化算法在某些情况下不能够全程向最优点收敛。 |
| [^164] | [Block-State Transformer.](http://arxiv.org/abs/2306.09539) | 本文提出了一种名为块状态变换器（BST）的混合神经网络层，结合了状态空间模型和块变换器，旨在在语言建模任务中提高性能和可扩展性。实验证明，该模型在语言建模困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，在层级别上具有超过十倍的速度提升。 |
| [^165] | [Katakomba: Tools and Benchmarks for Data-Driven NetHack.](http://arxiv.org/abs/2306.08772) | 本论文开发了一个用于数据驱动NetHack的工具和基准库，旨在解决资源、实现和基准方面的障碍。该库提供了ORL社区所熟悉的工作流基础，并附带可靠的评估工具。 |
| [^166] | [Deep Gaussian Markov Random Fields for Graph-Structured Dynamical Systems.](http://arxiv.org/abs/2306.08445) | 本研究提出了一种基于深度高斯马尔可夫随机场的方法，用于处理具有图结构的动态系统中的状态估计和学习问题。通过利用先验知识和变分推断，我们可以高效地学习到灵活的时空先验，并获得闭式后验计算方法，与传统的卡尔曼方法相比，具有更好的可扩展性。 |
| [^167] | [Controlling Text-to-Image Diffusion by Orthogonal Finetuning.](http://arxiv.org/abs/2306.07280) | 本文介绍了一种名为正交微调（OFT）的方法，可以有效地引导和控制大型文本到图像扩散模型，以执行不同的下游任务。我们还提出了约束正交微调（COFT），来提高微调的稳定性。这些方法能够保持语义生成能力并生成特定主题的图像。 |
| [^168] | [Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds.](http://arxiv.org/abs/2306.06836) | 本文解决了强化学习中当奖励呈“重尾”分布时的问题，提出了第一种处理这种情况的实例相关算法，并得到了极小最大化的遗憾界。 |
| [^169] | [High-Fidelity Audio Compression with Improved RVQGAN.](http://arxiv.org/abs/2306.06546) | 本论文介绍了一种使用改进的RVQGAN进行高保真音频压缩的算法，在保持音频质量的情况下实现了90倍的压缩。这种算法结合了高质量的音频生成和图像领域的向量量化技术，同时改进了对抗性和重建损失。通过使用单一通用模型对所有领域进行压缩，该算法在音频生成建模中具有广泛的应用价值。与竞争算法相比，该方法表现出更好的性能。 |
| [^170] | [Learning via Wasserstein-Based High Probability Generalisation Bounds.](http://arxiv.org/abs/2306.04375) | 本文证明了基于Wasserstein距离的PAC-Bayesian泛化界限的新颖性，并提出了算法框架，该界限显著扩展了PAC-Bayesian界限的范围，并在经典的学习问题中展现了改进的泛化误差。 |
| [^171] | [Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL.](http://arxiv.org/abs/2306.04220) | 本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。 |
| [^172] | [FAMO: Fast Adaptive Multitask Optimization.](http://arxiv.org/abs/2306.03792) | FAMO是一种快速自适应多任务优化方法，通过动态加权方式实现平衡的任务损失减少，相比最先进的梯度操作技术具有相似或更优的性能。 |
| [^173] | [Asymptotics of Bayesian Uncertainty Estimation in Random Features Regression.](http://arxiv.org/abs/2306.03783) | 论文比较和对比了后验预测分布和最大后验估计的风险，重点关注了模型维度增长速度大于任何常数倍的样本数时它们之间的渐近一致性。数值模拟表明这两个数量在限定维度上具有高斯波动，并表现出相似的属性。 |
| [^174] | [Towards Arbitrarily Expressive GNNs in $O(n^2)$ Space by Rethinking Folklore Weisfeiler-Lehman.](http://arxiv.org/abs/2306.03266) | 本文提出了$(k, t)$-FWL和$k$-FWL+两种方法，理论上证明了它们可以在$O(n^2)$的空间复杂度下，解决图同构问题。 |
| [^175] | [Resolving Interference When Merging Models.](http://arxiv.org/abs/2306.01708) | 本文揭示了现有模型合并技术存在的干扰问题，提出了具有广泛适用性的解决方案，可显着提高合并后模型的性能。 |
| [^176] | [Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model.](http://arxiv.org/abs/2306.01424) | 本文研究了连续性结果的部分反事实识别问题，并提出了一种新颖的敏感性模型——曲率敏感模型，通过限制函数级集的曲率来获得信息边界。 |
| [^177] | [Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations.](http://arxiv.org/abs/2306.01243) | 本文提出了在延迟和缺失状态观察环境中进行高效强化学习的理论研究，并建立了接近最优的遗憾边界，尽管有损可观察性对策略和规划的类别造成了重大挑战，但学习仍然是高效的。 |
| [^178] | [Thought Cloning: Learning to Think while Acting by Imitating Human Thinking.](http://arxiv.org/abs/2306.00323) | 本论文提出了一种新的模仿学习框架“思维克隆”，通过学习人类的思维来训练AI代理，以在泛化、探索、规划等能力方面实现更好的表现。 |
| [^179] | [Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection.](http://arxiv.org/abs/2306.00006) | 本文针对图形异常监测数据集中存在的一类同型现象，提出了一种新的无监督异常评分度量——当前节点亲和力，并通过学习量身定制的节点表示，实现了截断亲和力最大化（TAM）方法，优化在原始图形结构上进行，能够有效进行双重One-Class的GAD。 |
| [^180] | [Reliable Off-Policy Learning for Dosage Combinations.](http://arxiv.org/abs/2305.19742) | 本文提出了一种用于剂量组合的新颖可靠的脱机学习方法，通过三个步骤实现：开发神经网络估计个性化的剂量-反应，估计倾向得分检测共享协变量-治疗空间中的重叠有限区域，然后基于梯度的学习算法找到最佳的个性化剂量组合。 |
| [^181] | [Plug-in Performative Optimization.](http://arxiv.org/abs/2305.18728) | 研究了一种可能“规范不正确”模型的通用协议，“插件式表现优化”。 |
| [^182] | [Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning.](http://arxiv.org/abs/2305.18499) | 本文研究了基于野外视频预训练的上下文化世界模型（ContextWM）用于强化学习。ContextWM采用上下文化扩展的潜在动态模型建模，从而可以更好地泛化不同场景之间的知识转移，提高下游视觉控制任务的样本效率和控制性能。 |
| [^183] | [Explainable Brain Age Prediction using coVariance Neural Networks.](http://arxiv.org/abs/2305.18370) | 本文提出了使用协方差神经网络进行可解释的脑龄预测的框架，可以通过皮质厚度特征捕捉加速老化，并反映出增加的神经疾病或认知障碍的风险。 |
| [^184] | [Flow Matching for Scalable Simulation-Based Inference.](http://arxiv.org/abs/2305.17161) | 这篇论文提出了一种基于流匹配技术的后验估计方法，用于模拟推理，通过提供灵活性和可扩展性解决高维问题的挑战，并在引力波推断上取得了可比离散流方法更好的结果。 |
| [^185] | [Learning Better with Less: Effective Augmentation for Sample-Efficient Visual Reinforcement Learning.](http://arxiv.org/abs/2305.16379) | 本研究发现，对于数据增强在视觉强化学习中的有效性，空间多样性和轻微的困难度不可或缺。并提出了一种新的DA操作——Rand PR，它提供了丰富的空间多样性和最小的困难度，已经在多种数据上得到了有效性验证。 |
| [^186] | [Visual Programming for Text-to-Image Generation and Evaluation.](http://arxiv.org/abs/2305.15328) | 本文提出了两种新颖的可解释/可理解的图像编程框架，用于文本到图像（T2I）的生成和评估。首先，引入了VPGen，一个可解释的逐步T2I生成框架，将T2I生成分解为三个步骤，通过语言模型处理前两个步骤，提供了比端到端模型更强的空间控制能力，并利用了预训练语言模型的世界知识。 |
| [^187] | [A Causal View of Entity Bias in (Large) Language Models.](http://arxiv.org/abs/2305.14695) | 研究提出了一种结构因果模型（SCM）并提供因果干预技术，以缓解大型语言模型中的实体偏见，从而减少偏见信息，同时保留相似实体的共同预测信息。 |
| [^188] | [Block Coordinate Plug-and-Play Methods for Blind Inverse Problems.](http://arxiv.org/abs/2305.12672) | 该论文提出了一种新的块坐标插入和播放（BC-PnP）方法，通过引入学习的去噪器作为先验，高效地解决盲反问题。该方法在非凸数据适应度项和扩张去噪器的条件下收敛到一个隐式函数的稳定点。 |
| [^189] | [MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup.](http://arxiv.org/abs/2305.12029) | 本研究提出了MultiTurnCleanup任务，收集了新的数据集MultiTurnCleanup1，针对口语会话转录中的不连续现象进行探讨并提供了两个可用于未来研究的基准测试模型。 |
| [^190] | [Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models.](http://arxiv.org/abs/2305.11475) | 本文提供了一种共曲抑制正则化器，用于应对广义加性模型易受共错性的问题，通过惩罚非线性转换的特征变量的成对相关性，增强了模型的解释性。 |
| [^191] | [The noise level in linear regression with dependent data.](http://arxiv.org/abs/2305.11165) | 本文研究了具有依赖数据的线性回归中的噪声水平，提出了上限界限，并在误差规范下表现出优雅的降低。 |
| [^192] | [Learn to Unlearn: A Survey on Machine Unlearning.](http://arxiv.org/abs/2305.07512) | 本综述总结了机器去学习技术，用于从训练模型中删除敏感数据，但重新训练ML模型往往不可行。针对这个挑战，需要开发强大的模型以缓解公平性问题。 |
| [^193] | [Investigating the effect of sub-word segmentation on the performance of transformer language models.](http://arxiv.org/abs/2305.05480) | 本文研究使用单语词段算法StateMorph训练语言模型时，可以使模型更高效地收敛并获得更好的验证分数。 |
| [^194] | [Company classification using zero-shot learning.](http://arxiv.org/abs/2305.01028) | 本文提出了一种利用自然语言处理和零样本学习的方法来进行公司分类的方法。该方法可以简化公司分类过程，从而减少传统方法如全球产业分类标准（GICS）所需的时间和资源。 |
| [^195] | [Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields.](http://arxiv.org/abs/2304.06706) | 本文提出了一种 Zip-NeRF 技术，将 mip-NeRF 360 和基于网格的模型相结合，以实现抗锯齿、提高训练速度并降低误差率。 |
| [^196] | [Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy.](http://arxiv.org/abs/2304.02247) | 本文提出了一种通过文档层次结构诱导来检测新闻中的政治偏见的方法，该方法克服了过拟合和有限的普适性，展现了更好的鲁棒性和准确性。 |
| [^197] | [eP-ALM: Efficient Perceptual Augmentation of Language Models.](http://arxiv.org/abs/2303.11403) | 本论文提出了一种用对比学习提高语言模型的感知能力的高效方法eP-ALM，可以实现视觉感知信息和文本信息的融合，同时还能在多模态基准测试上实现最先进的结果。 |
| [^198] | [Entropy-dissipation Informed Neural Network for McKean-Vlasov Type PDEs.](http://arxiv.org/abs/2303.11205) | 本文提出了一种基于熵耗散的神经网络方法来求解McKean-Vlasov类型的偏微分方程(MVEs)，该方法通过最小化广义自洽势来控制假设解与真实解之间的差异，通过神经网络进行函数逼近，并在多个示例问题上验证了方法的有效性。 |
| [^199] | [Synthetic Experience Replay.](http://arxiv.org/abs/2303.06614) | 本文提出了合成经验回放方法解决深度强化学习中数据匮乏问题，通过巧妙应用生成建模技术来扩充数据效果显著。 |
| [^200] | [A Unified Algebraic Perspective on Lipschitz Neural Networks.](http://arxiv.org/abs/2303.03169) | 本文介绍了一种统一的代数视角，融合了各种类型的1-Lipschitz神经网络，包括凸潜在层和近正交层，并利用解析解推导和推广了许多现有的技术。 |
| [^201] | [Statistical Learning under Heterogenous Distribution Shift.](http://arxiv.org/abs/2302.13934) | 本文研究了异质分布偏移下的统计学习问题，通过研究经验风险最小化(ERM)在不同类别的复杂性下的表现，我们发现当类别$F$相比类别$G$更“简单”时，我们的预测器对于协变量偏移具有更强的鲁棒性，尤其在$\textbf{y}$的偏移远小于$\textbf{x}$的情况下。同时，我们发现ERM的行为与正交机器学习具有类似的特性。 |
| [^202] | [Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation.](http://arxiv.org/abs/2302.11294) | 提出了一种新的方法扩展了VAE模型容量，采用无限混合的非对称拉普拉斯分布作为解码器，具有分布拟合能力和调整数据隐私级别的优越性。 |
| [^203] | [Replicable Clustering.](http://arxiv.org/abs/2302.10359) | 该论文提出了三个针对统计聚类的可复制算法，实现了可复制聚类的概念，其中包括利用近似算法组合问题的黑盒方式解决统计$k$-medians、统计$k$-means和统计$k$-centers问题，并给出了表示算法复杂度的函数和误差限制。 |
| [^204] | [Practical Contextual Bandits with Feedback Graphs.](http://arxiv.org/abs/2302.08631) | 本文提出了一种利用反馈图的情境赌博算法，可用于缓解学习的统计复杂性，在实际应用中具有计算上的可行性和优秀的表现。 |
| [^205] | [Unpaired Multi-Domain Causal Representation Learning.](http://arxiv.org/abs/2302.00993) | 研究了未配对的多领域因果表示学习，提出了线性模型下联合分布和共享因果图的可识别条件，并将其转化为一种实用方法以恢复共享的潜在因果图。 |
| [^206] | [Direct Preference-based Policy Optimization without Reward Modeling.](http://arxiv.org/abs/2301.12842) | 本文提出了一种无需奖励模型的直接基于偏好的策略优化算法，通过采用对比学习框架和设计新的策略评分指标，能够从给定的偏好数据中学习并取得良好性能。 |
| [^207] | [Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive.](http://arxiv.org/abs/2301.12534) | 本文通过人工和机器审核员的共情冒犯和噪声审计研究了冒犯性言论检测中的差异性。结果表明，审核员之间存在广泛的分歧，并且人工审核员和大型语言模型分类器无法预测其他审核员的回应。这对于内容审核具有重要意义。 |
| [^208] | [ReSQueing Parallel and Private Stochastic Convex Optimization.](http://arxiv.org/abs/2301.00457) | 该论文介绍了一种名为ReSQue的工具，用于处理随机凸优化问题。通过将ReSQue与最新的球预言加速技术相结合，作者提出了在并行和私密环境下达到最先进复杂度的算法。对于在单位球中的凸优化目标，作者的算法能够在符合一定条件的情况下实现优化误差，同时保持最佳总体工作量的性能。 |
| [^209] | [On the Probability of Necessity and Sufficiency of Explaining Graph Neural Networks: A Lower Bound Optimization Approach.](http://arxiv.org/abs/2212.07056) | 该论文提出了一种利用下界优化来解释图神经网络（GNNs）的方法。该方法考虑解释的必要性和充分性，并通过数学量化这些要求。通过优化下界，可以获得最必要且充分的解释。 |
| [^210] | [Implicit Convolutional Kernels for Steerable CNNs.](http://arxiv.org/abs/2212.06096) | 本文提出了一种使用隐式神经表示的方法来参数化可定向卷积核，从而实现了简单灵活的构建可定向卷积神经网络的方法，能够推广到任何具有等变MLP的群G。 |
| [^211] | [Implicit variance regularization in non-contrastive SSL.](http://arxiv.org/abs/2212.04858) | 非对比自监督学习中的预测网络通过隐式方差正则化避免表示崩溃，欧几里得距离和余弦相似度具有不同的动态机制，并且特征值作为学习率乘数。引入一族等向性损失函数可以平衡收敛速度。 |
| [^212] | [CORL: Research-oriented Deep Offline Reinforcement Learning Library.](http://arxiv.org/abs/2210.07105) | CORL是一个面向研究的深度强化学习离线库，提供了经过充分基准测试的单文件实现离线和离线到在线强化学习算法，并具有简单的开发体验和实验跟踪功能。 |
| [^213] | [Causal Effect Identification in Uncertain Causal Networks.](http://arxiv.org/abs/2208.04627) | 本研究探讨了在存在不确定性因果图的情况下，如何识别出具有最高合理性且能够识别因果效应的子图。通过解决一个NP完全的组合优化问题，我们发现这个问题的答案。 |
| [^214] | [Deeply-Learned Generalized Linear Models with Missing Data.](http://arxiv.org/abs/2207.08911) | 本文提出了一种深度学习广义线性模型 (dlglm) 及其在处理缺失数据中的应用，其中的方法能够灵活地处理输入特征和响应的可忽略和不可忽略的缺失模式。我们通过统计模拟证明，在缺失数据不随机的情况下，该方法优于现有的监督学习方法，可用于生物医学科学等领域中。 |
| [^215] | [Supplementing Recurrent Neural Networks with Annealing to Solve Combinatorial Optimization Problems.](http://arxiv.org/abs/2207.08189) | 这项研究将循环神经网络与传统的退火算法相结合，提出了一种增强的方法来解决组合优化问题。通过使用变分经典退火框架，可以生成不相关的解，克服了传统采样方案的缺点。该方法在实际的优化问题中展现出了潜力，并与模拟退火算法进行了比较。 |
| [^216] | [Language models show human-like content effects on reasoning tasks.](http://arxiv.org/abs/2207.07051) | 本研究探讨了语言模型在逻辑推理任务中是否像人类一样通过混入内容来影响答案，结果发现大型语言模型的先验期望能够捕捉到这种特征。 |
| [^217] | [Algorithmic Foundations of Empirical X-risk Minimization.](http://arxiv.org/abs/2206.00439) | 本文介绍了一种名为"经验性X风险最小化（EXM）"的机器学习和人工智能优化框架，该框架解决了深度学习中优化不可分解目标的困难，并提供了算法基础的详细讨论。 |
| [^218] | [AdaTask: Adaptive Multitask Online Learning.](http://arxiv.org/abs/2205.15802) | AdaTask是一种自适应多任务在线学习算法，当任务被随机激活时，它的遗憾比运行独立算法更小，可以提高一个因子达到$\sqrt{N}$。AdaTask通过马氏距离势函数和变分表示同时学习任务和任务结构。 |
| [^219] | [Mixed-Integer Optimization with Constraint Learning.](http://arxiv.org/abs/2111.04469) | 本论文建立了带约束学习的混合整数优化的方法论基础，提出了一个端到端的数据驱动决策制定流程，并利用混合整数优化可表示性捕捉决策、上下文变量和结果之间的关系。同时，引入了处理学习数据中不确定性的两种方法。 |
| [^220] | [NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks.](http://arxiv.org/abs/2110.14053) | NeuroBack提出了一种使用图神经网络改进CDCL SAT求解的方法，通过预测出现在大多数满足赋值中的变量的阶段，使得求解更加有效，并且消除了对GPU资源的依赖。 |
| [^221] | [The Eigenlearning Framework: A Conservation Law Perspective on Kernel Regression and Wide Neural Networks.](http://arxiv.org/abs/2110.03922) | 该论文提出了Eigenlearning框架，通过限制核回归在学习正交基函数方面的能力并利用守恒定律来解释模型的泛化能力，同时还为Nakkiran等人的“深度引导”现象，经典奇偶问题难度和对抗鲁棒性提供了理论支持，并与统计物理学中的一个系统进行了类比。 |
| [^222] | [The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information.](http://arxiv.org/abs/2102.10019) | 本文证明了不确定性对不同人群的影响是不平等的，虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化。我们提出了一种名为平权信息的策略，可以消除这种差异并扩大机会的获取，这可以作为平权行动的替代方案。 |
| [^223] | [Adaptive Webpage Fingerprinting from TLS Traces.](http://arxiv.org/abs/2010.10294) | 本论文研究了针对TLS协议的现代网页指纹识别对手，提出了一种能够扩展到大量网页、准确分类、低成本的TLS-specific模型，并讨论了相应的对策和对现有对策的评估。 |
| [^224] | [A Neurocomputational Account of Consciousness: The Goal-Aligning Representation Internal Manipulation Theory (GARIM).](http://arxiv.org/abs/1912.13490) | 这个论文提出了一个神经计算框架下的意识理论，称为“目标对齐的内部表示操作”（GARIM）。该理论认为意识支持对目标相关的内部表示进行主动操作，使其与追求的目标更加对齐，从而增加目标导向行为的灵活性。 |
| [^225] | [A General Framework on Enhancing Portfolio Management with Reinforcement Learning.](http://arxiv.org/abs/1911.11880) | 提出了一个通用的强化学习框架，用于优化投资组合管理过程，可以考虑交易成本和空头限制，并比较了三种不同的强化学习算法在模拟环境中的性能优势。 |

# 详细

[^1]: FP8-LM：训练FP8大语言模型

    FP8-LM: Training FP8 Large Language Models. (arXiv:2310.18313v1 [cs.LG])

    [http://arxiv.org/abs/2310.18313](http://arxiv.org/abs/2310.18313)

    本文提出了一种用于训练大语言模型的新型FP8自动混合精度框架，能够在不影响模型准确性的情况下显著减少内存使用并提高训练速度。

    

    本文探讨了用于高效训练大语言模型（LLMs）的FP8低比特数据格式。我们的关键洞察是，在LLM训练中，大多数变量（如梯度和优化器状态）可以使用低精度数据格式，而不会影响模型准确性，并且不需要改变超参数。具体地，我们提出了一种新的FP8自动混合精度框架用于训练LLMs。该框架为LLM的混合精度和分布式并行训练提供了三个级别的FP8利用。它逐步引入8位梯度，优化器状态和分布式学习。实验结果表明，在H100 GPU平台上训练GPT-175B模型期间，我们的FP8混合精度训练框架不仅实现了显著的42%的真实内存使用减少，而且比广泛采用的BF16框架（即Megatron-LM）运行速度快64%，比Nvidia Transformer Engine快17%。

    In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 42% reduction in real memory usage but also ran 64% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 17%. This l
    
[^2]: Gen2Sim：使用生成模型在仿真环境中扩展机器人学习规模

    Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models. (arXiv:2310.18308v1 [cs.RO])

    [http://arxiv.org/abs/2310.18308](http://arxiv.org/abs/2310.18308)

    本文提出了Gen2Sim方法，通过使用生成模型自动生成3D资产、任务描述、任务分解和奖励函数来扩展机器人在仿真环境中的技能学习。这种自动化流程有助于解决人为参与的瓶颈问题，实现机器人学习在不同任务和环境中的扩展。

    

    通用的机器人操作器需要在各种环境中学习各种操纵技能。目前的机器人训练流程依赖于人类提供运动示范或编程仿真环境并为强化学习编写奖励函数。这种人为参与是扩展机器人学习跨不同任务和环境的重要瓶颈。我们提出了Generation to Simulation（Gen2Sim），一种通过自动化生成语言和视觉的大规模预训练生成模型来扩展仿真中机器人技能学习的方法。我们通过使用图像扩散模型将开放世界的二维物体中心图像提升到三维，并查询LLMs来确定合理的物理参数，生成用于仿真的三维资产。给定生成和人类开发的资产的URDF文件，我们使用思维链提示LLMs将这些映射到相关的任务描述。

    Generalist robot manipulators need to learn a wide variety of manipulation skills across diverse environments. Current robot training pipelines rely on humans to provide kinesthetic demonstrations or to program simulation environments and to code up reward functions for reinforcement learning. Such human involvement is an important bottleneck towards scaling up robot learning across diverse tasks and environments. We propose Generation to Simulation (Gen2Sim), a method for scaling up robot skill learning in simulation by automating generation of 3D assets, task descriptions, task decompositions and reward functions using large pre-trained generative models of language and vision. We generate 3D assets for simulation by lifting open-world 2D object-centric images to 3D using image diffusion models and querying LLMs to determine plausible physics parameters. Given URDF files of generated and human-developed assets, we chain-of-thought prompt LLMs to map these to relevant task description
    
[^3]: 监督和受罚基线校正

    Supervised and Penalized Baseline Correction. (arXiv:2310.18306v1 [stat.ML])

    [http://arxiv.org/abs/2310.18306](http://arxiv.org/abs/2310.18306)

    本研究改进了受罚基线校正方法，通过利用先验分析物浓度来改善光谱预测性能，并在两个近红外数据集上进行了评估。

    

    光谱测量可以显示由吸收和散射成分混合引起的扭曲光谱形状。这些扭曲（或基线）通常表现为非恒定偏移或低频振荡。因此，这些基线可能对分析和定量结果产生不利影响。基线校正是一个涵盖了预处理方法的总称，通过获取基线光谱（不需要的扭曲）并通过差异化去除扭曲。然而，当前最先进的基线校正方法即使可用分析物浓度或者它们对观察到的光谱变异有重要贡献，也没有利用它们。我们研究了一类最先进的方法（受罚基线校正）并对其进行修改，使其能够适应先验分析物浓度，从而提高预测性能。将在两个近红外数据集上评估性能，包括经典受罚方法。

    Spectroscopic measurements can show distorted spectra shapes arising from a mixture of absorbing and scattering contributions. These distortions (or baselines) often manifest themselves as non-constant offsets or low-frequency oscillations. As a result, these baselines can adversely affect analytical and quantitative results. Baseline correction is an umbrella term where one applies pre-processing methods to obtain baseline spectra (the unwanted distortions) and then remove the distortions by differencing. However, current state-of-the art baseline correction methods do not utilize analyte concentrations even if they are available, or even if they contribute significantly to the observed spectral variability. We examine a class of state-of-the-art methods (penalized baseline correction) and modify them such that they can accommodate a priori analyte concentration such that prediction can be enhanced. Performance will be access on two near infra-red data sets across both classical penal
    
[^4]: 学习非稳态条件下的稳定性原则

    A Stability Principle for Learning under Non-Stationarity. (arXiv:2310.18304v1 [cs.LG])

    [http://arxiv.org/abs/2310.18304](http://arxiv.org/abs/2310.18304)

    本研究提出了一个适用于非稳态环境的统计学习框架，通过应用稳定性原则选择回溯窗口来最大化历史数据利用，并保持累积偏差在可接受范围内。该方法展示了对未知非稳态的适应性，遗憾界在强凸或满足Lipschitz条件下是极小化的最优解。该研究的创新点是函数相似度度量和非稳态数据序列划分技术。

    

    我们在非稳定环境中开发了一个灵活的统计学习框架。在每个时间段，我们的方法应用稳定性原则来选择一个回溯窗口，最大限度地利用历史数据，同时将累积偏差保持在与随机误差相对可接受的范围内。我们的理论展示了该方法对未知非稳定性的适应性。当人口损失函数强凸或仅满足Lipschitz条件时，遗憾界是极小化的最优解，仅受对数因子的影响。我们的分析核心是两个新颖的组成部分：函数之间的相似度度量和将非稳态数据序列划分为准稳态片段的分割技术。

    We develop a versatile framework for statistical learning in non-stationary environments. In each time period, our approach applies a stability principle to select a look-back window that maximizes the utilization of historical data while keeping the cumulative bias within an acceptable range relative to the stochastic error. Our theory showcases the adaptability of this approach to unknown non-stationarity. The regret bound is minimax optimal up to logarithmic factors when the population losses are strongly convex, or Lipschitz only. At the heart of our analysis lie two novel components: a measure of similarity between functions and a segmentation technique for dividing the non-stationary data sequence into quasi-stationary pieces.
    
[^5]: 通过可调的分类损失解决 GAN 训练不稳定性问题

    Addressing GAN Training Instabilities via Tunable Classification Losses. (arXiv:2310.18291v1 [cs.LG])

    [http://arxiv.org/abs/2310.18291](http://arxiv.org/abs/2310.18291)

    该论文通过使用可调的分类损失解决了GAN训练不稳定性问题，并证明了CPE损失GAN和最小化f-散度的f-GAN之间的双向对应关系。

    

    生成对抗网络（GAN）作为生成器（G）和鉴别器（D）之间的零和博弈模型，允许以形式保证生成合成数据。我们通过使用类概率估计（CPE）损失来重新定义GAN的价值函数。我们证明了CPE损失GAN和最小化f-散度的f-GAN之间存在双向对应关系。我们还证明了所有对称f-散度在收敛性上等价。在有限样本和模型容量设定下，我们定义并获得了关于估计和泛化误差的界限。我们将这些结果特化到使用α-loss定义的α-GAN，它是一个由α（0，∞]参数化的可调CPE损失的家族。然后，我们引入了一类双目标GAN来解决GAN的训练不稳定性问题，通过使用α-loss来对每个玩家的目标建模，以获得(α_D，α_G)-GAN。我们证明了产生的非零和游戏简化为最小化f-散度的形式。

    Generative adversarial networks (GANs), modeled as a zero-sum game between a generator (G) and a discriminator (D), allow generating synthetic data with formal guarantees. Noting that D is a classifier, we begin by reformulating the GAN value function using class probability estimation (CPE) losses. We prove a two-way correspondence between CPE loss GANs and $f$-GANs which minimize $f$-divergences. We also show that all symmetric $f$-divergences are equivalent in convergence. In the finite sample and model capacity setting, we define and obtain bounds on estimation and generalization errors. We specialize these results to $\alpha$-GANs, defined using $\alpha$-loss, a tunable CPE loss family parametrized by $\alpha\in(0,\infty]$. We next introduce a class of dual-objective GANs to address training instabilities of GANs by modeling each player's objective using $\alpha$-loss to obtain $(\alpha_D,\alpha_G)$-GANs. We show that the resulting non-zero sum game simplifies to minimizing an $f$
    
[^6]: 通过贝叶斯优化实现可持续混凝土

    Sustainable Concrete via Bayesian Optimization. (arXiv:2310.18288v1 [cs.LG])

    [http://arxiv.org/abs/2310.18288](http://arxiv.org/abs/2310.18288)

    使用贝叶斯优化方法加速寻找强度和可持续性混凝土配方，通过相对较少的测量有效预测混凝土强度，并将寻找可持续混凝土视为多目标优化问题。

    

    全球八分之一的二氧化碳排放可以归因于水泥的生产，水泥是混凝土的主要组成部分，也是数据中心建设中二氧化碳排放的主要来源。因此，研究低碳混凝土配方对可持续性非常重要。然而，尝试新的混凝土配方非常耗时和劳动密集，因为通常需要等待记录混凝土的28天抗压强度，而这个量的测量无法加速。因此，实验设计方法如贝叶斯优化（BO）可以加速寻找强度和可持续性混凝土配方的搜索。在本文中，我们提出了模型步骤，通过相对较少的测量，使混凝土强度适合用高斯过程模型准确预测；将寻找可持续混凝土视为多目标优化问题；并利用...

    Eight percent of global carbon dioxide emissions can be attributed to the production of cement, the main component of concrete, which is also the dominant source of CO2 emissions in the construction of data centers. The discovery of lower-carbon concrete formulae is therefore of high significance for sustainability. However, experimenting with new concrete formulae is time consuming and labor intensive, as one usually has to wait to record the concrete's 28-day compressive strength, a quantity whose measurement can by its definition not be accelerated. This provides an opportunity for experimental design methodology like Bayesian Optimization (BO) to accelerate the search for strong and sustainable concrete formulae. Herein, we 1) propose modeling steps that make concrete strength amenable to be predicted accurately by a Gaussian process model with relatively few measurements, 2) formulate the search for sustainable concrete as a multi-objective optimization problem, and 3) leverage th
    
[^7]: 用于治疗效果估计的最优运输方法

    Optimal Transport for Treatment Effect Estimation. (arXiv:2310.18286v1 [cs.LG])

    [http://arxiv.org/abs/2310.18286](http://arxiv.org/abs/2310.18286)

    该论文提出一种名为ESCFR的方法，通过运用最优运输在因果性背景下解决治疗效果估计中的小批量采样效应和未观察到的混杂变量效应问题。

    

    由于存在治疗选择偏差，从观察数据中估计条件平均治疗效果是非常具有挑战性的。现有的方法通过在潜在空间中对不同处理组的分布进行对齐来缓解这个问题。然而，这些方法未能解决两个关键问题：（1）小批量采样效应（MSE），即在具有不平衡结果和异常值的非理想小批量中导致错误对齐；（2）未观察到的混杂变量效应（UCE），即由于忽略未观察到的混杂变量而导致的不准确差异计算。为了解决这些问题，我们提出了一种名为整体空间反事实回归（ESCFR）的原则性方法，它是在因果性背景下运用最优运输的新方法。具体来说，基于随机最优运输的框架，我们提出了一个松弛的保持质量的正则化项来解决MSE问题，并设计了一个处理UCE的近因事实结果正则化式。

    Estimating conditional average treatment effect from observational data is highly challenging due to the existence of treatment selection bias. Prevalent methods mitigate this issue by aligning distributions of different treatment groups in the latent space. However, there are two critical problems that these methods fail to address: (1) mini-batch sampling effects (MSE), which causes misalignment in non-ideal mini-batches with outcome imbalance and outliers; (2) unobserved confounder effects (UCE), which results in inaccurate discrepancy calculation due to the neglect of unobserved confounders. To tackle these problems, we propose a principled approach named Entire Space CounterFactual Regression (ESCFR), which is a new take on optimal transport in the context of causality. Specifically, based on the framework of stochastic optimal transport, we propose a relaxed mass-preserving regularizer to address the MSE issue and design a proximal factual outcome regularizer to handle the UCE is
    
[^8]: 异构联邦学习与群体感知提示调整

    Heterogeneous Federated Learning with Group-Aware Prompt Tuning. (arXiv:2310.18285v1 [cs.LG])

    [http://arxiv.org/abs/2310.18285](http://arxiv.org/abs/2310.18285)

    本文研究了在异构联邦学习中利用预训练的Transformer和高效的提示调整策略，通过学习共享和群体提示实现获取通用知识和个性化知识，以训练适应不同本地数据分布的全局模型。

    

    Transformer在各种机器学习任务中取得了显著的成功，促使它们被广泛采用。本文探索了它们在联邦学习（FL）领域的应用，特别关注具有不同本地数据集的异构场景。为了满足FL的计算和通信需求，我们利用预训练的Transformer，并使用高效的提示调整策略。我们的策略引入了同时学习共享和群体提示的概念，能够同时获取通用知识和群体特定知识。此外，提示选择模块为每个输入分配个性化的群体提示，使全局模型与每个客户端数据分布对齐。这种方法使我们能够训练一个单一的全局模型，能够自动适应不同的本地客户端数据分布，而无需进行本地微调。通过这种方式，我们提出的方法有效地搭建了链接

    Transformers have achieved remarkable success in various machine-learning tasks, prompting their widespread adoption. In this paper, we explore their application in the context of federated learning (FL), with a particular focus on heterogeneous scenarios where individual clients possess diverse local datasets. To meet the computational and communication demands of FL, we leverage pre-trained Transformers and use an efficient prompt-tuning strategy. Our strategy introduces the concept of learning both shared and group prompts, enabling the acquisition of universal knowledge and group-specific knowledge simultaneously. Additionally, a prompt selection module assigns personalized group prompts to each input, aligning the global model with the data distribution of each client. This approach allows us to train a single global model that can automatically adapt to various local client data distributions without requiring local fine-tuning. In this way, our proposed method effectively bridge
    
[^9]: LipSim: 一种可证明鲁棒的知觉相似度度量方法

    LipSim: A Provably Robust Perceptual Similarity Metric. (arXiv:2310.18274v1 [cs.CV])

    [http://arxiv.org/abs/2310.18274](http://arxiv.org/abs/2310.18274)

    LipSim是一个可证明鲁棒的知觉相似度度量方法，通过利用1-Lipschitz神经网络作为骨干，提供了围绕度量方法的防护区域。

    

    近年来，人们对于开发和应用知觉相似度度量方法表现出了越来越大的兴趣。研究表明，相较于像素级度量方法，知觉度量方法在与人类感知的一致性和作为人类视觉系统的代理方面具有更高的优势。然而，由于知觉度量方法依赖于神经网络，对神经网络对抗性攻击脆弱性的关注也日益增长。我们展示了基于ViT的特征提取器的最新知觉相似度度量方法对抗攻击的脆弱性，并提出了一个名为LipSim（Lipschitz相似度度量方法）的鲁棒知觉相似度度量框架。通过利用1-Lipschitz神经网络作为骨干，LipSim提供了围绕着度量方法的防护区域。

    Recent years have seen growing interest in developing and applying perceptual similarity metrics. Research has shown the superiority of perceptual metrics over pixel-wise metrics in aligning with human perception and serving as a proxy for the human visual system. On the other hand, as perceptual metrics rely on neural networks, there is a growing concern regarding their resilience, given the established vulnerability of neural networks to adversarial attacks. It is indeed logical to infer that perceptual metrics may inherit both the strengths and shortcomings of neural networks. In this work, we demonstrate the vulnerability of state-of-the-art perceptual similarity metrics based on an ensemble of ViT-based feature extractors to adversarial attacks. We then propose a framework to train a robust perceptual similarity metric called LipSim (Lipschitz Similarity Metric) with provable guarantees. By leveraging 1-Lipschitz neural networks as the backbone, LipSim provides guarded areas aroun
    
[^10]: PlantPlotGAN:一种基于物理信息的生成对抗网络用于植物病害预测

    PlantPlotGAN: A Physics-Informed Generative Adversarial Network for Plant Disease Prediction. (arXiv:2310.18268v1 [cs.CV])

    [http://arxiv.org/abs/2310.18268](http://arxiv.org/abs/2310.18268)

    PlantPlotGAN是一种基于物理信息的生成对抗网络，用于植物病害预测。它通过生成合成的多光谱区域图像来增加预测模型的准确性。

    

    监测植物园林对于农作物管理和产出健康的收成至关重要。无人机已被用于收集多光谱图像来进行监测。然而，考虑到需要监测的面积和飞行的限制，植物病害信号只有在植物生长的后期阶段以及病害已经扩散到植物园林的大部分区域时才会变得明显。这种有限量的相关数据使得预测模型受到限制，因为算法难以有效地从不平衡或不现实的增广数据集中推广出模式。为了解决这个问题，我们提出了PlantPlotGAN，一种基于物理信息的生成模型，能够创建具有真实植被指数的合成多光谱区域图像。这些指数作为疾病检测的代理，并用来评估我们的模型能够帮助增加预测模型的准确性。结果表明，合成的图像可以用于提高预测模型的准确性。

    Monitoring plantations is crucial for crop management and producing healthy harvests. Unmanned Aerial Vehicles (UAVs) have been used to collect multispectral images that aid in this monitoring. However, given the number of hectares to be monitored and the limitations of flight, plant disease signals become visually clear only in the later stages of plant growth and only if the disease has spread throughout a significant portion of the plantation. This limited amount of relevant data hampers the prediction models, as the algorithms struggle to generalize patterns with unbalanced or unrealistic augmented datasets effectively. To address this issue, we propose PlantPlotGAN, a physics-informed generative model capable of creating synthetic multispectral plot images with realistic vegetation indices. These indices served as a proxy for disease detection and were used to evaluate if our model could help increase the accuracy of prediction models. The results demonstrate that the synthetic im
    
[^11]: 结构化半定规划用于恢复结构化预处理器

    Structured Semidefinite Programming for Recovering Structured Preconditioners. (arXiv:2310.18265v1 [cs.DS])

    [http://arxiv.org/abs/2310.18265](http://arxiv.org/abs/2310.18265)

    本论文提出了一个通用框架，用于寻找近似最优的预处理器来解决线性系统，并提供了改进的对角预处理结果，优于先前的通用半/shear programming方法。

    

    我们开发了一个通用框架，用于寻找近似最优的预处理器来解决线性系统。利用这个框架，我们改进了基本预处理和线性系统求解问题的运行时间。我们提供了一个算法，给定正定矩阵$\mathbf{K} \in \mathbb{R}^{d \times d}$，其中$\mathrm{nnz}(\mathbf{K})$为非零元素的数量，它可以在时间复杂度$\widetilde{O}(\mathrm{nnz}(\mathbf{K}) \cdot \mathrm{poly}(\kappa^\star,\epsilon^{-1}))$内计算$\epsilon$-最优的对角预处理器，其中$\kappa^\star$是缩放后矩阵的最优条件数。我们还提供了一个算法，给定$\mathbf{M} \in \mathbb{R}^{d \times d}$，它是一个图拉普拉斯矩阵的伪逆矩阵或者一个常数谱逼近，可以在$\widetilde{O}(d^2)$的时间内求解在$\mathbf{M}$上的线性系统。我们的对角预处理结果改进了通过通用半/shear programming方法获得的$\Omega(d^{3.5})$的最先进运行时间。

    We develop a general framework for finding approximately-optimal preconditioners for solving linear systems. Leveraging this framework we obtain improved runtimes for fundamental preconditioning and linear system solving problems including the following. We give an algorithm which, given positive definite $\mathbf{K} \in \mathbb{R}^{d \times d}$ with $\mathrm{nnz}(\mathbf{K})$ nonzero entries, computes an $\epsilon$-optimal diagonal preconditioner in time $\widetilde{O}(\mathrm{nnz}(\mathbf{K}) \cdot \mathrm{poly}(\kappa^\star,\epsilon^{-1}))$, where $\kappa^\star$ is the optimal condition number of the rescaled matrix. We give an algorithm which, given $\mathbf{M} \in \mathbb{R}^{d \times d}$ that is either the pseudoinverse of a graph Laplacian matrix or a constant spectral approximation of one, solves linear systems in $\mathbf{M}$ in $\widetilde{O}(d^2)$ time. Our diagonal preconditioning results improve state-of-the-art runtimes of $\Omega(d^{3.5})$ attained by general-purpose sem
    
[^12]: 学习搜索具有灵活神经k-Opt的路径问题的可行和不可行区域

    Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt. (arXiv:2310.18264v1 [cs.LG])

    [http://arxiv.org/abs/2310.18264](http://arxiv.org/abs/2310.18264)

    本文介绍了一种名为Neural k-Opt的学习搜索求解器，用于路径问题。通过灵活的k-opt交换和自主的可行和不可行区域探索，该算法在旅行商问题和有容量车辆路径问题上表现出优越性能。

    

    本文介绍了一种用于路径问题的学习搜索（L2S）求解器Neural k-Opt（NeuOpt）。它基于定制的动作因子分解方法和定制的双流循环解码器，学习执行灵活的k-opt交换。作为绕过纯可行性掩盖方案并实现可行和不可行区域的自主探索的开创性工作，我们提出了Guided Infeasible Region Exploration（GIRE）方案，它为NeuOpt策略网络补充了与可行性相关的特征，并利用奖励塑造更有效地引导强化学习。此外，我们还为NeuOpt配备了Dynamic Data Augmentation（D2A），以在推理过程中进行更多样化的搜索。在旅行商问题（TSP）和有容量车辆路径问题（CVRP）上进行的大量实验证明，我们的NeuOpt不仅明显超过现有的（基于掩码的）L2S求解器，而且还展示了在学习的同时提供了更好的解决方案的优越性。

    In this paper, we present Neural k-Opt (NeuOpt), a novel learning-to-search (L2S) solver for routing problems. It learns to perform flexible k-opt exchanges based on a tailored action factorization method and a customized recurrent dual-stream decoder. As a pioneering work to circumvent the pure feasibility masking scheme and enable the autonomous exploration of both feasible and infeasible regions, we then propose the Guided Infeasible Region Exploration (GIRE) scheme, which supplements the NeuOpt policy network with feasibility-related features and leverages reward shaping to steer reinforcement learning more effectively. Additionally, we equip NeuOpt with Dynamic Data Augmentation (D2A) for more diverse searches during inference. Extensive experiments on the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that our NeuOpt not only significantly outstrips existing (masking-based) L2S solvers, but also showcases superiority over the learning-
    
[^13]: 基于MIM-GAN的多元时间序列数据异常检测

    MIM-GAN-based Anomaly Detection for Multivariate Time Series Data. (arXiv:2310.18257v1 [cs.LG])

    [http://arxiv.org/abs/2310.18257](http://arxiv.org/abs/2310.18257)

    本文提出了一种基于MIM-GAN的无监督多元时间序列异常检测算法，通过将时间序列数据划分为子序列，利用LSTM生成器和判别器捕捉时间相关性，并引入指数信息度量来避免局部最优解和模型崩溃。

    

    生成式对抗网络（GAN）的损失函数是影响异常检测生成样本质量和多样性的重要因素。本文提出了一种基于MIM-GAN的无监督多元时间序列异常检测算法。具体而言，使用滑动窗口将时间序列数据划分为子序列，然后利用基于长短期记忆（LSTM）的生成器和判别器来捕捉时间序列数据的时间相关性。为了避免损失函数的局部最优解和模型崩溃，我们引入了指数信息度量到GAN的损失函数中。此外，还考虑了一个由判别性和重构损失组成的判别重构得分。我们推导了损失函数的全局最优解，并证明了我们提出的MIM-GAN-based异常检测避免了模型崩溃。

    The loss function of Generative adversarial network(GAN) is an important factor that affects the quality and diversity of the generated samples for anomaly detection. In this paper, we propose an unsupervised multiple time series anomaly detection algorithm based on the GAN with message importance measure(MIM-GAN). In particular, the time series data is divided into subsequences using a sliding window. Then a generator and a discriminator designed based on the Long Short-Term Memory (LSTM) are employed to capture the temporal correlations of the time series data. To avoid the local optimal solution of loss function and the model collapse, we introduce an exponential information measure into the loss function of GAN. Additionally, a discriminant reconstruction score consisting on discrimination and reconstruction loss is taken into account. The global optimal solution for the loss function is derived and the model collapse is proved to be avoided in our proposed MIM-GAN-based anomaly de
    
[^14]: 为离线增强学习和模仿学习提供指导性数据增强

    Guided Data Augmentation for Offline Reinforcement Learning and Imitation Learning. (arXiv:2310.18247v1 [cs.LG])

    [http://arxiv.org/abs/2310.18247](http://arxiv.org/abs/2310.18247)

    该论文提出了一种人工引导的数据增强框架（GuDA）用于提高演示学习模型的性能。

    

    演示学习是一种使用专家演示来学习机器人控制策略的流行技术。然而，获取专家级演示的难度限制了演示学习方法的适用性：现实世界的数据收集通常很昂贵，并且演示的质量很大程度上取决于演示者的能力和安全问题。一些工作利用数据增强来廉价生成额外的演示数据，但大多数数据增强方法以随机方式生成增强数据，最终产生高度次优的数据。在这项工作中，我们提出了一种人工引导的数据增强框架（GuDA），用于生成高质量的增强数据。GuDA的关键洞见是，虽然演示动作序列可能很难展示产生专家数据所需的动作序列，但用户经常可以轻松地辨别出增强轨迹段表示的任务进展。因此，用户可以施加一系列s

    Learning from demonstration (LfD) is a popular technique that uses expert demonstrations to learn robot control policies. However, the difficulty in acquiring expert-quality demonstrations limits the applicability of LfD methods: real-world data collection is often costly, and the quality of the demonstrations depends greatly on the demonstrator's abilities and safety concerns. A number of works have leveraged data augmentation (DA) to inexpensively generate additional demonstration data, but most DA works generate augmented data in a random fashion and ultimately produce highly suboptimal data. In this work, we propose Guided Data Augmentation (GuDA), a human-guided DA framework that generates expert-quality augmented data. The key insight of GuDA is that while it may be difficult to demonstrate the sequence of actions required to produce expert data, a user can often easily identify when an augmented trajectory segment represents task progress. Thus, the user can impose a series of s
    
[^15]: $\alpha$-互信息：一个可调节的隐私度量用于数据共享中的隐私保护

    $\alpha$-Mutual Information: A Tunable Privacy Measure for Privacy Protection in Data Sharing. (arXiv:2310.18241v1 [cs.LG])

    [http://arxiv.org/abs/2310.18241](http://arxiv.org/abs/2310.18241)

    本文提出了一种可调节的隐私度量$\alpha$-互信息，在隐私保护的数据共享中能够生成优越的模型以有效阻挠攻击者，通过操纵原始数据提供隐私保护，并使用一个通用对抗性深度学习框架来解决隐私度量问题。通过实证实验验证了$\alpha$-互信息的功能，并评估了隐私-效用权衡。

    

    本文采用Arimoto的$\alpha$-互信息作为可调节的隐私度量，在隐私保护的数据发布环境中旨在防止向攻击者披露私密数据。通过对隐私度量进行精细调整，我们证明了我们的方法能够产生在各种性能维度上有效阻挠攻击者的优越模型。我们提出了一种基于扭曲的通用机制，通过操纵原始数据来提供隐私保护。扭曲度量是根据特定实验的数据结构来确定的。我们通过采用一个包含发布者和对手的通用对抗性深度学习框架来解决表达式中的问题，这两者都是以相反目标进行训练的。本研究对图像和时间序列数据进行了实证实验，以验证$\alpha$-互信息的功能。我们评估了定制模型的隐私-效用权衡，并将其与互信息作为基准进行比较。

    This paper adopts Arimoto's $\alpha$-Mutual Information as a tunable privacy measure, in a privacy-preserving data release setting that aims to prevent disclosing private data to adversaries. By fine-tuning the privacy metric, we demonstrate that our approach yields superior models that effectively thwart attackers across various performance dimensions. We formulate a general distortion-based mechanism that manipulates the original data to offer privacy protection. The distortion metrics are determined according to the data structure of a specific experiment. We confront the problem expressed in the formulation by employing a general adversarial deep learning framework that consists of a releaser and an adversary, trained with opposite goals. This study conducts empirical experiments on images and time-series data to verify the functionality of $\alpha$-Mutual Information. We evaluate the privacy-utility trade-off of customized models and compare them to mutual information as the basel
    
[^16]: 如何通过重新采样改善长尾学习？

    How Re-sampling Helps for Long-Tail Learning?. (arXiv:2310.18236v1 [cs.CV])

    [http://arxiv.org/abs/2310.18236](http://arxiv.org/abs/2310.18236)

    本文研究了如何通过重新采样改善长尾学习的效果。研究发现，当训练图像不包含无关背景时，重新采样可以显著提高泛化能力，但在其他场景下可能会导致学习到与目标标签无关的虚假相关性。

    

    近年来，由于极度不平衡的数据集所带来的挑战，长尾学习受到了广泛关注。在这些数据集中，只有少数类别（称为头部类别）具有足够数量的训练样本，而其余类别（称为尾部类别）在训练数据中很少见。重新采样是一种经典且广泛使用的方法，用于解决类别不平衡问题。然而，最近的研究声称，在现代长尾学习任务中，重新采样对性能的提升微不足道。本文旨在系统地研究这一现象。我们的研究表明，当训练图像不包含语义上无关的背景时，重新采样可以显著提高泛化能力。然而，在其他场景下，它可能会学习到与目标标签无关的意外虚假相关性。我们设计了两个同质数据集上的实验，一个包含无关背景，另一个不包含。

    Long-tail learning has received significant attention in recent years due to the challenge it poses with extremely imbalanced datasets. In these datasets, only a few classes (known as the head classes) have an adequate number of training samples, while the rest of the classes (known as the tail classes) are infrequent in the training data. Re-sampling is a classical and widely used approach for addressing class imbalance issues. Unfortunately, recent studies claim that re-sampling brings negligible performance improvements in modern long-tail learning tasks. This paper aims to investigate this phenomenon systematically. Our research shows that re-sampling can considerably improve generalization when the training images do not contain semantically irrelevant contexts. In other scenarios, however, it can learn unexpected spurious correlations between irrelevant contexts and target labels. We design experiments on two homogeneous datasets, one containing irrelevant context and the other n
    
[^17]: Davidsonian场景图：改进文本-图像生成的细粒度评估的可靠性

    Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation. (arXiv:2310.18235v1 [cs.CV])

    [http://arxiv.org/abs/2310.18235](http://arxiv.org/abs/2310.18235)

    本论文提出了Davidsonian场景图（DSG）的评估框架，解决了现有文本-图像生成模型评估中的可靠性挑战，包括QG问题的准确性和VQA答案的一致性。

    

    评估文本到图像模型一直是困难的。最近一种用于评估文本-图像忠实度的强大方法是基于QG/A（问题生成和回答），它使用预训练的基础模型自动生成一组问题和答案，并基于这些答案与基于提示的答案在视觉问题回答模型中提取的一致性对输出图像进行评分。这种评估自然上取决于底层QG和QA模型的质量。我们确定并解决了现有QG/A工作中的几个可靠性挑战：（a）QG问题应尊重提示（避免幻觉、重复和遗漏）和（b）VQA答案应一致（不会在图像中宣称没有摩托车，同时声称摩托车是蓝色）。我们通过Davidsonian场景图（DSG），这个受形式语义启发的实证评估框架，解决了这些问题。

    Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG
    
[^18]: 深度转换高斯过程

    Deep Transformed Gaussian Processes. (arXiv:2310.18230v1 [cs.LG])

    [http://arxiv.org/abs/2310.18230](http://arxiv.org/abs/2310.18230)

    本文提出了一种名为深度转换高斯过程（DTGPs）的转换高斯过程（TGPs）的推广，该模型采用串联层级的随机过程，并实现了相对于TGPs和DGPs的灵活性增强。通过使用变分推理，可以近似所需的计算，从而得到了简单直接的推理算法扩展。

    

    转换高斯过程（TGPs）是通过使用可逆转换从先验过程（通常是高斯过程）中转换样本来指定的随机过程，从而增加了基本过程的灵活性。此外，与通过高斯过程的层级串联构造的深度高斯过程（DGPs）相比，TGPs实现了竞争性结果。在这项工作中，我们提出了一种名为深度转换高斯过程（DTGPs）的TGP推广，它遵循串联随机过程层的趋势。更准确地说，我们得到了一个多层模型，其中每一层都是一个TGP。这种推广意味着相对于TGPs和DGPs都提高了灵活性。在这样的模型中进行精确推理是困难的。但是，我们展示了可以使用变分推理来近似所需的计算，从而得到了流行的DSVI推理算法的直接扩展。

    Transformed Gaussian Processes (TGPs) are stochastic processes specified by transforming samples from the joint distribution from a prior process (typically a GP) using an invertible transformation; increasing the flexibility of the base process.  Furthermore, they achieve competitive results compared with Deep Gaussian Processes (DGPs), which are another generalization constructed by a hierarchical concatenation of GPs. In this work, we propose a generalization of TGPs named Deep Transformed Gaussian Processes (DTGPs), which follows the trend of concatenating layers of stochastic processes. More precisely, we obtain a multi-layer model in which each layer is a TGP. This generalization implies an increment of flexibility with respect to both TGPs and DGPs. Exact inference in such a model is intractable. However, we show that one can use variational inference to approximate the required computations yielding a straightforward extension of the popular DSVI inference algorithm Salimbeni e
    
[^19]: TBDLNet：一种用于分类多药耐药和药物敏感结核病的网络

    TBDLNet: a network for classifying multidrug-resistant and drug-sensitive tuberculosis. (arXiv:2310.18222v1 [eess.IV])

    [http://arxiv.org/abs/2310.18222](http://arxiv.org/abs/2310.18222)

    本文提出了一种名为TBDLNet的深度学习模型，用于自动分类多药耐药和药物敏感结核病。通过使用预训练的ResNet50提取特征，采用三个随机化神经网络减轻过拟合问题，并通过多数投票的方式进行集成，TBDLNet达到了高准确率、特异性、精确度、敏感度和F1-score。该模型可以及早检测到多药耐药性肺结核，帮助及时调整治疗计划并提高治疗效果。

    

    本文提出将一种新颖的深度学习模型TBDLNet应用于识别CT图像，用于自动分类多药耐药和药物敏感结核病。选择预训练的ResNet50来提取特征。采用三个随机化神经网络来减轻过拟合问题。通过多数投票的方式应用三个RNN的集成方法来提高鲁棒性。本文选择了五个指标进行评估，分别是准确率、敏感度、精确度、F1-score和特异性。TBDLNet分别达到了0.9822的准确率、0.9815的特异性、0.9823的精确度、0.9829的敏感度和0.9826的F1-score。TBDLNet适用于分类多药耐药结核病和药物敏感结核病。它能尽早检测到多药耐药性肺结核，有助于及时调整治疗方案，提高治疗效果。

    This paper proposes applying a novel deep-learning model, TBDLNet, to recognize CT images to classify multidrug-resistant and drug-sensitive tuberculosis automatically. The pre-trained ResNet50 is selected to extract features. Three randomized neural networks are used to alleviate the overfitting problem. The ensemble of three RNNs is applied to boost the robustness via majority voting. The proposed model is evaluated by five-fold cross-validation. Five indexes are selected in this paper, which are accuracy, sensitivity, precision, F1-score, and specificity. The TBDLNet achieves 0.9822 accuracy, 0.9815 specificity, 0.9823 precision, 0.9829 sensitivity, and 0.9826 F1-score, respectively. The TBDLNet is suitable for classifying multidrug-resistant tuberculosis and drug-sensitive tuberculosis. It can detect multidrug-resistant pulmonary tuberculosis as early as possible, which helps to adjust the treatment plan in time and improve the treatment effect.
    
[^20]: 一种适用于全区域的交叉地区出租车需求预测模型

    One Model Fits All: Cross-Region Taxi-Demand Forecasting. (arXiv:2310.18215v1 [cs.LG])

    [http://arxiv.org/abs/2310.18215](http://arxiv.org/abs/2310.18215)

    本论文提出了一种适用于全区域的出租车需求预测模型，利用图神经网络捕捉城市环境中的空间依赖性和模式，并采用了区域中立的方法，使模型能够泛化到任何地区，包括未知地区。

    

    随着打车服务需求的增长，对准确的出租车需求预测的需求也在增加。现有系统仅限于特定地区，缺乏对未知地区的泛化能力。本文提出了一种新颖的出租车需求预测系统，利用图神经网络捕捉城市环境中的空间依赖性和模式。此外，该系统采用了一种区域中立的方法，使其能够训练一个模型，适用于任何地区，包括未知地区。为了实现这一点，该框架利用变分自动编码器将输入特征分开为区域特定和区域中立的部分。区域中立特征有助于交叉地区出租车需求预测，使模型能够在不同城市地区具有良好的泛化能力。实验证明了该系统在准确预测出租车需求方面的效果，甚至在以前未观察到的地区也是如此。

    The growing demand for ride-hailing services has led to an increasing need for accurate taxi demand prediction. Existing systems are limited to specific regions, lacking generalizability to unseen areas. This paper presents a novel taxi demand forecasting system that leverages a graph neural network to capture spatial dependencies and patterns in urban environments. Additionally, the proposed system employs a region-neutral approach, enabling it to train a model that can be applied to any region, including unseen regions. To achieve this, the framework incorporates the power of Variational Autoencoder to disentangle the input features into region-specific and region-neutral components. The region-neutral features facilitate cross-region taxi demand predictions, allowing the model to generalize well across different urban areas. Experimental results demonstrate the effectiveness of the proposed system in accurately forecasting taxi demand, even in previously unobserved regions, thus sho
    
[^21]: 用于因果结构学习的算法对超参数选择的稳健性

    Robustness of Algorithms for Causal Structure Learning to Hyperparameter Choice. (arXiv:2310.18212v1 [cs.LG])

    [http://arxiv.org/abs/2310.18212](http://arxiv.org/abs/2310.18212)

    这项研究调查了超参数对因果结构学习任务的影响，并通过对不同复杂级别的数据集进行实证评估，发现超参数选择同样对算法选择具有重要的影响。

    

    超参数在机器学习中起着关键作用。由于结构学习的无监督特性，超参数调整可以在任何算法中产生全球领先和糟糕的预测表现之间的差异。因此，为了使用特定算法的默认值，人们经常忽视超参数调整。虽然已经有大量关于因果发现算法性能评估的研究，但超参数如何影响单个算法以及选择最佳算法解决特定问题的问题尚未得到深入研究。本研究通过调查超参数对因果结构学习任务的影响来弥补这一空白。具体而言，我们对不同复杂级别的数据集上的一些开创性学习算法进行了超参数选择的实证评估。我们发现，尽管算法的选择仍然至关重要，但超参数的选择同样对因果结构学习任务有重要影响。

    Hyperparameters play a critical role in machine learning. Hyperparameter tuning can make the difference between state-of-the-art and poor prediction performance for any algorithm, but it is particularly challenging for structure learning due to its unsupervised nature. As a result, hyperparameter tuning is often neglected in favour of using the default values provided by a particular implementation of an algorithm. While there have been numerous studies on performance evaluation of causal discovery algorithms, how hyperparameters affect individual algorithms, as well as the choice of the best algorithm for a specific problem, has not been studied in depth before. This work addresses this gap by investigating the influence of hyperparameters on causal structure learning tasks. Specifically, we perform an empirical evaluation of hyperparameter selection for some seminal learning algorithms on datasets of varying levels of complexity. We find that, while the choice of algorithm remains cr
    
[^22]: 超卷曲图对比学习的对齐和外壳同构性

    Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive Learning. (arXiv:2310.18209v1 [cs.LG])

    [http://arxiv.org/abs/2310.18209](http://arxiv.org/abs/2310.18209)

    提出了一种新颖的对比学习框架，用于学习高质量的图嵌入，并设计了对齐度量和均匀性度量来解决图领域的非欧几里德几何结构问题。

    

    学习对下游任务有益的自监督图表示是具有挑战性的。在各种方法中，对比学习具有竞争力的性能。对比学习的嵌入被排列在一个超球面上，从而使得在欧几里德空间中的余弦距离测量成为可能。然而，许多领域的潜在几何结构，如图形，展现了高度非欧几里德的特性。为此，我们提出了一种新颖的对比学习框架，用于学习高质量的图嵌入。具体而言，我们设计了一种有效捕捉层次数据不变性信息的对齐度量，同时我们提出了一种替代均匀度量来防止所谓的维度塌陷。我们证明，在双曲空间中，必须解决与树的属性相关的叶子和高度层面的均匀性，而在双曲流形的环境空间中，这些概念转化为对同构性的施加。

    Learning good self-supervised graph representations that are beneficial to downstream tasks is challenging. Among a variety of methods, contrastive learning enjoys competitive performance. The embeddings of contrastive learning are arranged on a hypersphere that enables the Cosine distance measurement in the Euclidean space. However, the underlying structure of many domains such as graphs exhibits highly non-Euclidean latent geometry. To this end, we propose a novel contrastive learning framework to learn high-quality graph embedding. Specifically, we design the alignment metric that effectively captures the hierarchical data-invariant information, as well as we propose a substitute of uniformity metric to prevent the so-called dimensional collapse. We show that in the hyperbolic space one has to address the leaf- and height-level uniformity which are related to properties of trees, whereas in the ambient space of the hyperbolic manifold, these notions translate into imposing an isotro
    
[^23]: ArcheType：一种使用大型语言模型进行开源列类型注释的新框架

    ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models. (arXiv:2310.18208v1 [cs.CL])

    [http://arxiv.org/abs/2310.18208](http://arxiv.org/abs/2310.18208)

    ArcheType是一种使用大型语言模型进行开源列类型注释的新框架，通过改进上下文采样和标签重新映射，实现了全面的零样本解决方案。

    

    现有的深度学习方法在语义列类型注释（CTA）方面存在重要缺点：它们依赖于在训练时固定的语义类型；需要大量的每个类型的训练样本并产生大量运行时推断成本；即使类型保持不变，它们的性能也可能在评估新数据集时下降。大型语言模型在广泛的任务上展示了强大的零样本分类性能，本文探讨了它们在CTA中的应用。我们介绍了ArcheType，一种简单实用的方法，用于上下文采样、提示序列化、模型查询和标签重新映射，从而使大型语言模型能够完全以零样本方式解决列类型注释问题。我们分别对我们方法的每个组成部分进行了分析，并确定出改进上下文采样和标签重新映射提供了最一致的性能提升。

    Existing deep-learning approaches to semantic column type annotation (CTA) have important shortcomings: they rely on semantic types which are fixed at training time; require a large number of training samples per type and incur large run-time inference costs; and their performance can degrade when evaluated on novel datasets, even when types remain constant. Large language models have exhibited strong zero-shot classification performance on a wide range of tasks and in this paper we explore their use for CTA. We introduce ArcheType, a simple, practical method for context sampling, prompt serialization, model querying, and label remapping, which enables large language models to solve column type annotation problems in a fully zero-shot manner. We ablate each component of our method separately, and establish that improvements to context sampling and label remapping provide the most consistent gains. ArcheType establishes new state-of-the-art performance on both zero-shot and fine-tuned C
    
[^24]: 缩放学习优化器是否值得？评估 VeLO 的 4000 个 TPU 月的价值。

    Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months. (arXiv:2310.18191v1 [cs.LG])

    [http://arxiv.org/abs/2310.18191](http://arxiv.org/abs/2310.18191)

    VeLO是迄今为止规模最大的训练通用“基础”优化器的尝试，但我们的评估发现它需要问题特定的调优，并不一定优于竞争对手的解决方案质量和训练误差降低速度，这对于VeLO的通用性和培训投资的价值提出了质疑。

    

    我们分析了 VeLO（万能学习优化器），这是迄今为止规模最大的训练通用“基础”优化器的尝试。VeLO 使用超过 4000 个 TPU 月的机器学习任务进行训练，目标是产生一个能够推广到新问题并且不需要超参数调整，并且超过 Adam 等行业标准的优化器。我们对 MLCommons 优化器基准套件独立评估了 VeLO。我们发现与初步声明相反：（1）VeLO有一个关键的超参数需要根据具体问题进行调整，（2）VeLO在找到的解的质量上不一定优于竞争对手，（3）VeLO在降低训练误差上并不比竞争优化器更快。这些观察结果对 VeLO 的通用性和培训投资的价值提出了质疑。

    We analyze VeLO (versatile learned optimizer), the largest scale attempt to train a general purpose "foundational" optimizer to date. VeLO was trained on thousands of machine learning tasks using over 4000 TPU months with the goal of producing an optimizer capable of generalizing to new problems while being hyperparameter free, and outperforming industry standards such as Adam. We independently evaluate VeLO on the MLCommons optimizer benchmark suite. We find that, contrary to initial claims: (1) VeLO has a critical hyperparameter that needs problem-specific tuning, (2) VeLO does not necessarily outperform competitors in quality of solution found, and (3) VeLO is not faster than competing optimizers at reducing the training loss. These observations call into question VeLO's generality and the value of the investment in training it.
    
[^25]: 无模型后验采样的模型自由随机学习方法

    Model-free Posterior Sampling via Learning Rate Randomization. (arXiv:2310.18186v1 [stat.ML])

    [http://arxiv.org/abs/2310.18186](http://arxiv.org/abs/2310.18186)

    本文介绍了一种随机化无模型算法RandQL，用于减小马尔科夫决策过程中的遗憾。RandQL通过学习率随机化实现乐观探索，并在实证研究中表现出色。

    

    本文介绍了一种新颖的随机化无模型算法，Randomized Q-learning（简称RandQL），用于减小马尔科夫决策过程（MDPs）中的遗憾。据我们所知，RandQL是第一个可行的模型自由后验采样算法。我们分析了RandQL在表格和非表格度量空间设置下的性能。在表格MDPs中，RandQL实现了一个遗憾界的顺序为$\widetilde{\mathcal{O}}(\sqrt{H^{5}SAT})$，其中$H$是计划的时间长度，$S$是状态数，$A$是动作数，$T$是回合数。对于度量状态-动作空间，RandQL实现了一个遗憾界的顺序为$\widetilde{\mathcal{O}}(H^{5/2} T^{(d_z+1)/(d_z+2)})$，其中$d_z$表示缩放维度。需要注意的是，RandQL实现了乐观探索，而不使用奖励，而是依赖于学习率随机化的新思想。我们的实证研究表明，RandQL在基线探索上胜过现有方法。

    In this paper, we introduce Randomized Q-learning (RandQL), a novel randomized model-free algorithm for regret minimization in episodic Markov Decision Processes (MDPs). To the best of our knowledge, RandQL is the first tractable model-free posterior sampling-based algorithm. We analyze the performance of RandQL in both tabular and non-tabular metric space settings. In tabular MDPs, RandQL achieves a regret bound of order $\widetilde{\mathcal{O}}(\sqrt{H^{5}SAT})$, where $H$ is the planning horizon, $S$ is the number of states, $A$ is the number of actions, and $T$ is the number of episodes. For a metric state-action space, RandQL enjoys a regret bound of order $\widetilde{\mathcal{O}}(H^{5/2} T^{(d_z+1)/(d_z+2)})$, where $d_z$ denotes the zooming dimension. Notably, RandQL achieves optimistic exploration without using bonuses, relying instead on a novel idea of learning rate randomization. Our empirical study shows that RandQL outperforms existing approaches on baseline exploration en
    
[^26]: 使用人设来建模语言模型中的真实性

    Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])

    [http://arxiv.org/abs/2310.18168](http://arxiv.org/abs/2310.18168)

    本研究探讨了在大型语言模型中使用人设来建模真实性的可能性。通过建模真实人设，语言模型可以将真实性推广到不同上下文中，并通过相关特征判断个体产生文本的真实性。

    

    大型语言模型使用互联网上的大量文本进行训练，这些文本中既包含了事实，也包含了误导性的信息。语言模型能够从这些相互矛盾的数据中辨别真实与虚假吗？基于语言模型能够建模不同产生文本的个体这一观点，我们假设它们可以通过建模真实人设来聚类真实文本：一群很可能产生真实文本并具有相似特征的个体。例如，可信源如维基百科和科学期刊通常使用正式的写作风格并提出一致的主张。通过建模这一人设，语言模型可以将真实性推广到每个个体生成训练文本的特定上下文之外。例如，模型可以推断出“维基百科”这个个体在“科学”生成的主题上会表现出真实性，因为它们共享一个人设。我们首先通过两个观察结果为人设假设提供了证据：（1）我们可以探测模型在不同领域中判断真实性的能力；（2）模型可以从相关特征中推测个体产生文本的真实性。

    Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod
    
[^27]: 提升企业网络安全：比较基于机器级和进程级分析的动态恶意软件检测方法

    Enhancing Enterprise Network Security: Comparing Machine-Level and Process-Level Analysis for Dynamic Malware Detection. (arXiv:2310.18165v1 [cs.CR])

    [http://arxiv.org/abs/2310.18165](http://arxiv.org/abs/2310.18165)

    本研究比较了基于机器级和进程级分析的动态恶意软件检测方法，探索如何隔离恶意进程以提高网络安全性，并解决了样本执行中的实际情况和资源利用的挑战。

    

    分析恶意软件对于理解恶意软件的工作原理并开发适当的检测和预防方法非常重要。动态分析可以克服常用于绕过静态分析的逃逸技术，并提供对恶意软件运行时活动的洞察。许多关于动态分析的研究侧重于调查机器级信息（例如CPU、内存、网络使用率），以确定机器是否运行恶意活动。恶意机器并不一定意味着机器上的所有运行进程也是恶意的。如果我们可以隔离恶意进程而不是隔离整个机器，我们可以杀死恶意进程，而机器可以继续正常工作。动态恶意软件检测研究面临的另一个挑战是样本在一个没有任何后台应用程序运行的机器上执行。这是不现实的，因为恶意软件事件发生时，计算机通常会运行许多良性（后台）应用程序。我们的实验

    Analysing malware is important to understand how malicious software works and to develop appropriate detection and prevention methods. Dynamic analysis can overcome evasion techniques commonly used to bypass static analysis and provide insights into malware runtime activities. Much research on dynamic analysis focused on investigating machine-level information (e.g., CPU, memory, network usage) to identify whether a machine is running malicious activities. A malicious machine does not necessarily mean all running processes on the machine are also malicious. If we can isolate the malicious process instead of isolating the whole machine, we could kill the malicious process, and the machine can keep doing its job. Another challenge dynamic malware detection research faces is that the samples are executed in one machine without any background applications running. It is unrealistic as a computer typically runs many benign (background) applications when a malware incident happens. Our exper
    
[^28]: 集群中的比例公平性: 社会选择视角的研究

    Proportional Fairness in Clustering: A Social Choice Perspective. (arXiv:2310.18162v1 [cs.LG])

    [http://arxiv.org/abs/2310.18162](http://arxiv.org/abs/2310.18162)

    在这篇论文中，我们研究了比例聚类问题，将其与计算社会选择中的多赢家投票领域相关联。我们发现满足Brill和Peters的比例概念的任何聚类都能同时获得Chen等人的比例公平性、个体公平性和核心的最佳近似。我们还研究了更强的比例代表性概念，并展示了这些更强概念对应的近似。

    

    我们研究了Chen等人的比例聚类问题，并将其与计算社会选择中的多赢家投票领域相关联。我们展示了满足Brill和Peters的弱比例概念的任何聚类同时获得了Chen等人的比例公平性的最佳近似，也获得了个体公平性和“核心”的最佳近似。事实上，我们表明任何对比例公平性的近似也是对个体公平性的近似，反之亦然。最后，我们还研究了更强的比例代表性概念，其中偏差不仅发生在单个候选中心，而是多个候选中心，并展示了Brill和Peters的更强比例概念暗示了这些更强保证的近似。

    We study the proportional clustering problem of Chen et al. [ICML'19] and relate it to the area of multiwinner voting in computational social choice. We show that any clustering satisfying a weak proportionality notion of Brill and Peters [EC'23] simultaneously obtains the best known approximations to the proportional fairness notion of Chen et al. [ICML'19], but also to individual fairness [Jung et al., FORC'20] and the "core" [Li et al. ICML'21]. In fact, we show that any approximation to proportional fairness is also an approximation to individual fairness and vice versa. Finally, we also study stronger notions of proportional representation, in which deviations do not only happen to single, but multiple candidate centers, and show that stronger proportionality notions of Brill and Peters [EC'23] imply approximations to these stronger guarantees.
    
[^29]: 使用大型语言模型进行文本属性图的解缠表征学习

    Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v1 [cs.CL])

    [http://arxiv.org/abs/2310.18152](http://arxiv.org/abs/2310.18152)

    本文提出了一个名为Disentangled Graph-Text Learner (DGTL)的模型，通过引入定制的解缠图神经网络（GNN）层，使得大型语言模型（LLMs）能够更好地理解文本属性图（TAGs）中的复杂结构关系。

    

    文本属性图（TAGs）在网络上非常常见，对于该类图，如引用网络、电子商务网络和社交网络的研究在网络社区中引起了相当大的关注。最近，大型语言模型（LLMs）在各种任务上展示了出色的能力。然而，现有的工作仅仅依靠提示信息来传达图结构信息给LLMs，因此对于TAGs中复杂的结构关系了解不足。为解决这个问题，本文提出了解缠图文学习器（DGTL）模型，能够增强LLMs对TAGs的推理和预测能力。我们的DGTL模型通过定制的解缠图神经网络（GNN）层将图结构信息纳入其中，使得LLMs能够捕捉多个结构因素中隐藏的复杂关系。

    Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthe
    
[^30]: 通过创建固定目标来改进内在探索

    Improving Intrinsic Exploration by Creating Stationary Objectives. (arXiv:2310.18144v1 [cs.LG])

    [http://arxiv.org/abs/2310.18144](http://arxiv.org/abs/2310.18144)

    该论文提出了一个新的方法：通过创建固定目标，将原始的非固定奖励转化为固定奖励，从而改善了强化学习中的内在探索。

    

    强化学习中的探索奖励通过定义自定义的内在目标来引导长期探索。基于计数的方法使用状态访问频率来获得探索奖励。本文发现，任何从基于计数的方法导出的内在奖励函数都是非固定的，因此为代理人构建了一个难以优化的目标。我们工作的关键贡献在于通过增强状态表示将原始的非固定奖励转化为固定奖励。为此，我们引入了用于探索的固定目标（SOFE）框架。SOFE需要识别不同探索奖励的足够统计量，并找到一种将这些统计量高效编码作为深度网络输入的方法。SOFE基于提出扩展状态空间的状态增强，但有希望简化代理目标的优化。我们的实验结果表明，SOFE改善了探索效果。

    Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Count-based methods use the frequency of state visits to derive an exploration bonus. In this paper, we identify that any intrinsic reward function derived from count-based methods is non-stationary and hence induces a difficult objective to optimize for the agent. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework. SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. Our experiments show that SOFE improves the
    
[^31]: 无监督的表示学习在不同形变形状集合上的应用

    Unsupervised Representation Learning for Diverse Deformable Shape Collections. (arXiv:2310.18141v1 [cs.CV])

    [http://arxiv.org/abs/2310.18141](http://arxiv.org/abs/2310.18141)

    本论文提出了一种无监督学习的新方法，用于处理可变形形状集合的编码和操作。通过谱池化技术建立一个通用的潜空间，可以对多样化的网格进行训练，并且实验证明了该方法的优秀性能。

    

    我们介绍了一种新的基于学习的方法，用于编码和操作3D表面网格。我们的方法专门设计用来创建一个可解释的嵌入空间，用于处理可变形的形状集合。与之前的3D网格自编码器不同，我们的方法在无监督的条件下对多样化的网格进行训练。我们的方法的核心是一种谱池化技术，它建立了一个通用的潜空间，打破了传统网格连通性和形状类别的约束。整个过程分为两个阶段。在第一个阶段，我们使用函数映射范式以无监督的方式提取一组形状之间的点对点映射。然后利用这些点对点映射构建一个共同的潜空间，从而确保了直观解释和独立于网格连通性和形状类别。通过大量实验证明，我们的方法取得了优秀的结果。

    We introduce a novel learning-based method for encoding and manipulating 3D surface meshes. Our method is specifically designed to create an interpretable embedding space for deformable shape collections. Unlike previous 3D mesh autoencoders that require meshes to be in a 1-to-1 correspondence, our approach is trained on diverse meshes in an unsupervised manner. Central to our method is a spectral pooling technique that establishes a universal latent space, breaking free from traditional constraints of mesh connectivity and shape categories. The entire process consists of two stages. In the first stage, we employ the functional map paradigm to extract point-to-point (p2p) maps between a collection of shapes in an unsupervised manner. These p2p maps are then utilized to construct a common latent space, which ensures straightforward interpretation and independence from mesh connectivity and shape category. Through extensive experiments, we demonstrate that our method achieves excellent r
    
[^32]: 提问更多，了解更多：利用大型语言模型强化学习的决策问题与思维链

    Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models. (arXiv:2310.18127v1 [cs.LG])

    [http://arxiv.org/abs/2310.18127](http://arxiv.org/abs/2310.18127)

    本文提出了一种利用大型语言模型的强化学习框架，能够学习提问相关问题并进行推理来指导在实际环境中执行的行为的学习。

    

    大型语言模型通过将基于行动的策略与思维链（CoT）推理相结合，展示了解决复杂实际挑战的潜力。然而，对于该框架的有效性来说，具有高质量的提示非常重要。目前，这些提示是通过广泛使用人力手工制作的，导致CoT策略经常无法推广。为了确保低层控制器适当地处理CoT推理，还需要人为介入来开发接地函数。在本文中，我们迈出了迈向在复杂推理中应用实际环境中的任务解决的完全集成的端到端框架的第一步。为此，我们提供了一个新的领导者-追随者双层框架，能够学习提问相关问题（提示），并随后进行推理，指导在环境中执行的行为的学习。一个好的提示应该基于历史的自省性修订来进行修改。

    Large language models (LLMs) demonstrate their promise in tackling complicated practical challenges by combining action-based policies with chain of thought (CoT) reasoning. Having high-quality prompts on hand, however, is vital to the framework's effectiveness. Currently, these prompts are handcrafted utilizing extensive human labor, resulting in CoT policies that frequently fail to generalize. Human intervention is also required in order to develop grounding functions that ensure low-level controllers appropriately process CoT reasoning. In this paper, we take the first step towards a fully integrated end-to-end framework for task-solving in real settings employing complicated reasoning. To that purpose, we offer a new leader-follower bilevel framework capable of learning to ask relevant questions (prompts) and subsequently undertaking reasoning to guide the learning of actions to be performed in an environment. A good prompt should make introspective revisions based on historical fi
    
[^33]: score-matching的样本复杂度界限：因果发现和生成建模

    Sample Complexity Bounds for Score-Matching: Causal Discovery and Generative Modeling. (arXiv:2310.18123v1 [cs.LG])

    [http://arxiv.org/abs/2310.18123](http://arxiv.org/abs/2310.18123)

    本文通过训练深度ReLU神经网络实现了对score-matching的准确估计，并在因果发现和生成建模领域中提供了统计样本复杂度界限。

    

    本文提供了score-matching及其在因果发现中的应用的统计样本复杂度界限。我们证明通过使用随机梯度下降训练标准的深度ReLU神经网络可以实现对分数函数的准确估计。在假定分数函数的估计足够好的情况下，我们对使用基于score-matching的因果发现方法的恢复因果关系的错误率建立了界限。最后，我们分析了基于分数的生成建模中的score-matching估计的上界，该方法不仅在因果发现中应用广泛，也在生成模型领域具有独立的研究意义。

    This paper provides statistical sample complexity bounds for score-matching and its applications in causal discovery. We demonstrate that accurate estimation of the score function is achievable by training a standard deep ReLU neural network using stochastic gradient descent. We establish bounds on the error rate of recovering causal relationships using the score-matching-based causal discovery method of Rolland et al. [2022], assuming a sufficiently good estimation of the score function. Finally, we analyze the upper bound of score-matching estimation within the score-based generative modeling, which has been applied for causal discovery but is also of independent interest within the domain of generative models.
    
[^34]: 一个全球多单元校准方法作为大规模物联网颗粒物监测系统部署的方法

    A Global Multi-Unit Calibration as a Method for Large Scale IoT Particulate Matter Monitoring Systems Deployments. (arXiv:2310.18118v1 [cs.LG])

    [http://arxiv.org/abs/2310.18118](http://arxiv.org/abs/2310.18118)

    本论文提出了一种基于低成本颗粒物（PM）传感器的全球多单元校准方法，用于大规模物联网颗粒物监测系统的部署。该方法通过采集有限数量的物联网AQ多传感器单元的现场响应和应用机器学习概念，能够解决环境干扰和制造差异导致的校准成本问题，实现准确和普遍的空气质量监测。

    

    可扩展和有效的校准是低成本空气质量监测系统的基本要求，将使得在城市中能够进行准确和普遍的监测。受环境干扰和制造差异的影响，这些设备需要包括传感器特定的和复杂的校准流程，以达到足够的准确性，以便作为指示性测量设备部署在空气质量（AQ）监测网络中。概念和传感器漂移经常强制进行频繁的校准过程。这些问题导致无法承受的校准成本，从而阻止了它们在关注准确性时的大规模部署。在这项工作中，我们提出了一种基于低成本颗粒物（PM）传感器的零传输样本的全局校准方法，作为物联网AQ多感知设备的技术实现。这种方法基于从有限数量的物联网AQ多传感器单元中记录的现场响应和机器学习概念。

    Scalable and effective calibration is a fundamental requirement for Low Cost Air Quality Monitoring Systems and will enable accurate and pervasive monitoring in cities. Suffering from environmental interferences and fabrication variance, these devices need to encompass sensors specific and complex calibration processes for reaching a sufficient accuracy to be deployed as indicative measurement devices in Air Quality (AQ) monitoring networks. Concept and sensor drift often force calibration process to be frequently repeated. These issues lead to unbearable calibration costs which denies their massive deployment when accuracy is a concern. In this work, We propose a zero transfer samples, global calibration methodology as a technological enabler for IoT AQ multisensory devices which relies on low cost Particulate Matter (PM) sensors. This methodology is based on field recorded responses from a limited number of IoT AQ multisensors units and machine learning concepts and can be universall
    
[^35]: 具有自适应得分的转导式一致推断

    Transductive conformal inference with adaptive scores. (arXiv:2310.18108v1 [stat.ME])

    [http://arxiv.org/abs/2310.18108](http://arxiv.org/abs/2310.18108)

    利用转导式一致推断方法进行一致性无分布保证的机器学习任务，并通过建立Polya球模型的联合分布和经验分布函数的浓度不等式，提供了自适应得分的可用性和更高准确性。

    

    一致推断是一种基本且多用途的工具，为许多机器学习任务提供了无分布保证。我们考虑转导式设置，在这种设置中，针对$m$个新样本进行决策，产生$m$个一致推断$p$值。虽然经典结果仅涉及其边际分布，但我们表明它们的联合分布遵循一个Polya球模型，并建立了它们的经验分布函数的浓度不等式。这些结果适用于任意可交换的得分，包括可以在训练阶段使用测试+校准样本的协变量以提高准确性的“自适应”得分。我们通过对当前感兴趣的两个机器学习任务（转导式迁移学习的区间预测和基于两类分类的新颖性检测）提供统一且在概率上的保证来演示这些理论结果的有用性。

    Conformal inference is a fundamental and versatile tool that provides distribution-free guarantees for many machine learning tasks. We consider the transductive setting, where decisions are made on a test sample of $m$ new points, giving rise to $m$ conformal $p$-values. {While classical results only concern their marginal distribution, we show that their joint distribution follows a P\'olya urn model, and establish a concentration inequality for their empirical distribution function.} The results hold for arbitrary exchangeable scores, including {\it adaptive} ones that can use the covariates of the test+calibration samples at training stage for increased accuracy. We demonstrate the usefulness of these theoretical results through uniform, in-probability guarantees for two machine learning tasks of current interest: interval prediction for transductive transfer learning and novelty detection based on two-class classification.
    
[^36]: 基于高斯先验和非线性异常分数的对抗异常检测

    Adversarial Anomaly Detection using Gaussian Priors and Nonlinear Anomaly Scores. (arXiv:2310.18091v1 [cs.LG])

    [http://arxiv.org/abs/2310.18091](http://arxiv.org/abs/2310.18091)

    该论文提出了一种基于高斯先验和非线性异常分数的对抗异常检测方法。通过结合$\beta$-VAE的生成稳定性和GAN的判别能力，提出了一种新的模型$\beta$-VAEGAN，并对异常分数的组合方法进行了研究。通过训练核化SVM，考虑了非线性关系，同时利用$\beta$-VAEGAN对高斯先验的偏差形成了新的异常分数组件。相比于现有方法，该方法在异常检测性能上有所改进。

    

    在不平衡数据集中进行异常检测是一个频繁且关键的问题，特别是在医疗领域，获取和标记异常通常是昂贵的。将$\beta$变分自动编码器（VAE）的生成稳定性与生成对抗网络（GAN）的判别能力相结合，我们提出了一种新颖的模型$\beta$-VAEGAN。我们研究了基于我们模型的判别能力和重构能力的异常分数的组合方法。现有工作集中于线性组合这些组件来确定数据是否异常。我们通过在相应的错误组件上训练核化支持向量机（SVM），进一步改进了现有工作，并考虑了非线性关系。这提高了异常检测性能，同时允许更快的优化。最后，我们利用$\beta$-VAEGAN对于高斯先验的偏差形成了一个新颖的异常分数组件。与最先进的工作相比，我们改进了异常检测性能。

    Anomaly detection in imbalanced datasets is a frequent and crucial problem, especially in the medical domain where retrieving and labeling irregularities is often expensive. By combining the generative stability of a $\beta$-variational autoencoder (VAE) with the discriminative strengths of generative adversarial networks (GANs), we propose a novel model, $\beta$-VAEGAN. We investigate methods for composing anomaly scores based on the discriminative and reconstructive capabilities of our model. Existing work focuses on linear combinations of these components to determine if data is anomalous. We advance existing work by training a kernelized support vector machine (SVM) on the respective error components to also consider nonlinear relationships. This improves anomaly detection performance, while allowing faster optimization. Lastly, we use the deviations from the Gaussian prior of $\beta$-VAEGAN to form a novel anomaly score component. In comparison to state-of-the-art work, we improve
    
[^37]: 揭示概率嵌入在自监督学习中的潜力

    Unveiling the Potential of Probabilistic Embeddings in Self-Supervised Learning. (arXiv:2310.18080v1 [cs.LG])

    [http://arxiv.org/abs/2310.18080](http://arxiv.org/abs/2310.18080)

    本文探讨了将概率建模引入自监督学习中对性能、信息压缩和超出分布识别潜力的影响，并从信息论的角度揭示了信息压缩和保留之间的权衡。

    

    近年来，自监督学习通过允许模型从无标签数据获取有意义的表示，为推动机器学习发挥了重要作用。一个引人注目的研究方向是在信息论框架内开发自监督模型，但许多研究在推导目标时往往偏离了随机性假设。为了更深入地了解这个问题，我们提出了明确地用随机嵌入来建模表示，并评估其对性能、信息压缩和识别超出分布的潜力的影响。从信息论的角度出发，我们试图研究概率建模对信息瓶颈的影响，揭示了在表示空间和损失空间中信息压缩和信息保留之间的权衡。强调区分这两个空间的重要性，我们展示了如何约束其中一个空间会影响到另一个空间。

    In recent years, self-supervised learning has played a pivotal role in advancing machine learning by allowing models to acquire meaningful representations from unlabeled data. An intriguing research avenue involves developing self-supervised models within an information-theoretic framework, but many studies often deviate from the stochasticity assumptions made when deriving their objectives. To gain deeper insights into this issue, we propose to explicitly model the representation with stochastic embeddings and assess their effects on performance, information compression and potential for out-of-distribution detection. From an information-theoretic perspective, we seek to investigate the impact of probabilistic modeling on the information bottleneck, shedding light on a trade-off between compression and preservation of information in both representation and loss space. Emphasizing the importance of distinguishing between these two spaces, we demonstrate how constraining one can affect 
    
[^38]: 在再生核希尔伯特空间中的Lipschitz和Hölder连续性

    Lipschitz and H\"older Continuity in Reproducing Kernel Hilbert Spaces. (arXiv:2310.18078v1 [math.FA])

    [http://arxiv.org/abs/2310.18078](http://arxiv.org/abs/2310.18078)

    本研究在再生核希尔伯特空间中研究了Lipschitz和Hölder连续性，并提供了几个充分条件和对再生核的深度调查。这个工作也是这个主题的方便参考资料。

    

    再生核希尔伯特空间（RKHS）是非常重要的函数空间，在机器学习、统计学、数值分析和纯数学中起着重要作用。由于Lipschitz和Hölder连续性是重要的正则性质，在插值、逼近和优化问题中有许多应用。在本研究中，我们研究了RKHS中这些连续性概念，并提供了几个充分条件，以及对引起预定Lipschitz或Hölder连续性的再生核的深度调查。除了新的结果，我们还从文献中总结了相关已知结果，使本工作也成为这个主题的方便参考资料。

    Reproducing kernel Hilbert spaces (RKHSs) are very important function spaces, playing an important role in machine learning, statistics, numerical analysis and pure mathematics. Since Lipschitz and H\"older continuity are important regularity properties, with many applications in interpolation, approximation and optimization problems, in this work we investigate these continuity notion in RKHSs. We provide several sufficient conditions as well as an in depth investigation of reproducing kernels inducing prescribed Lipschitz or H\"older continuity. Apart from new results, we also collect related known results from the literature, making the present work also a convenient reference on this topic.
    
[^39]: 关于均场极限中基于核的统计学习

    On kernel-based statistical learning in the mean field limit. (arXiv:2310.18074v1 [cs.LG])

    [http://arxiv.org/abs/2310.18074](http://arxiv.org/abs/2310.18074)

    这篇论文研究了在大规模问题中的均场极限下的核统计学习，包括核的均场极限的理论完善、逼近以及支持向量机等的应用。研究结果为大规模问题提供了新的理论工具和见解。

    

    在许多机器学习应用中，考虑了大量的变量。受交互粒子系统的机器学习启发，我们考虑了输入变量数量趋于无穷大的情况。首先，我们进一步研究了核及其再生核希尔伯特空间的均场极限，完善了现有理论。接下来，我们提供了与均场极限下这些核的逼近相关的结果，包括一个表现定理。最后，我们将这些核应用于在均场极限中的统计学习，重点关注支持向量机。特别地，我们展示了经验和无穷样本解的均场收敛以及相应风险的收敛。一方面，我们的结果在核方法的背景下建立了严格的均场极限，为大规模问题提供了新的理论工具和见解。另一方面，我们的设置对应于...

    In many applications of machine learning, a large number of variables are considered. Motivated by machine learning of interacting particle systems, we consider the situation when the number of input variables goes to infinity. First, we continue the recent investigation of the mean field limit of kernels and their reproducing kernel Hilbert spaces, completing the existing theory. Next, we provide results relevant for approximation with such kernels in the mean field limit, including a representer theorem. Finally, we use these kernels in the context of statistical learning in the mean field limit, focusing on Support Vector Machines. In particular, we show mean field convergence of empirical and infinite-sample solutions as well as the convergence of the corresponding risks. On the one hand, our results establish rigorous mean field limits in the context of kernel methods, providing new theoretical tools and insights for large-scale problems. On the other hand, our setting corresponds
    
[^40]: “亲爱的，告诉我出了什么问题”，通过合作生成全局文本辨别模型的解释

    "Honey, Tell Me What's Wrong", Global Explanation of Textual Discriminative Models through Cooperative Generation. (arXiv:2310.18063v1 [cs.CL])

    [http://arxiv.org/abs/2310.18063](http://arxiv.org/abs/2310.18063)

    Therapy是第一个适用于文本的全局和无模型解释方法，通过合作生成文本，不依赖于初始样本，并提供了对输入空间中模型行为的全局概览。

    

    复杂机器学习的普及提高了无模型解释算法的重要性。这些方法通过轻微扰动真实实例来创建人工实例，捕捉模型决策的变化。然而，这些方法依赖于初始数据，并且只提供关于这些初始数据决策的解释。为了解决这些问题，我们提出了 Therapy，这是第一个适用于文本的全局和无模型解释方法，不需要输入数据集。Therapy通过合作生成，根据分类器学习到的分布生成文本。因为它不依赖于初始样本，所以即使数据缺失（例如因保密原因），也能生成解释。此外，与将多个局部解释组合成一个全局解释的现有方法不同，Therapy提供了对输入空间中模型行为的全局概览。我们的实验表明，虽然不使用输入数据来生成样本，但 Therapy 提供了有价值的洞察。

    The ubiquity of complex machine learning has raised the importance of model-agnostic explanation algorithms. These methods create artificial instances by slightly perturbing real instances, capturing shifts in model decisions. However, such methods rely on initial data and only provide explanations of the decision for these. To tackle these problems, we propose Therapy, the first global and model-agnostic explanation method adapted to text which requires no input dataset. Therapy generates texts following the distribution learned by a classifier through cooperative generation. Because it does not rely on initial samples, it allows to generate explanations even when data is absent (e.g., for confidentiality reasons). Moreover, conversely to existing methods that combine multiple local explanations into a global one, Therapy offers a global overview of the model behavior on the input space. Our experiments show that although using no input data to generate samples, Therapy provides insig
    
[^41]: 带权重剪裁的差分隐私梯度下降方法

    DP-SGD with weight clipping. (arXiv:2310.18001v1 [cs.LG])

    [http://arxiv.org/abs/2310.18001](http://arxiv.org/abs/2310.18001)

    本研究提出了一种带权重剪裁的差分隐私梯度下降方法，通过利用公共信息对全局模型进行改进，获得更精确的灵敏度界限和噪声水平调整，提供了更好的差分隐私保证。

    

    最近，由于深度神经网络和其他依赖于目标函数优化的方法的高度流行，以及对数据隐私的关注，差分隐私梯度下降方法引起了极大的兴趣。为了在提供最小噪声的情况下实现差分隐私保证，能够准确地限制参与者将观察到的信息的灵敏度非常重要。在本研究中，我们提出了一种新颖的方法，弥补了传统梯度剪裁产生的偏差。通过利用关于当前全局模型及其在搜索领域中位置的公共信息，我们可以获得改进的梯度界限，从而实现更精确的灵敏度确定和噪声水平调整。我们扩展了最先进的算法，提供了更好的差分隐私保证，需要更少的噪声，并进行了实证评估。

    Recently, due to the popularity of deep neural networks and other methods whose training typically relies on the optimization of an objective function, and due to concerns for data privacy, there is a lot of interest in differentially private gradient descent methods. To achieve differential privacy guarantees with a minimum amount of noise, it is important to be able to bound precisely the sensitivity of the information which the participants will observe. In this study, we present a novel approach that mitigates the bias arising from traditional gradient clipping. By leveraging public information concerning the current global model and its location within the search domain, we can achieve improved gradient bounds, leading to enhanced sensitivity determinations and refined noise level adjustments. We extend the state of the art algorithms, present improved differential privacy guarantees requiring less noise and present an empirical evaluation.
    
[^42]: 关于Adam迭代复杂度上下界的缩小

    Closing the Gap Between the Upper Bound and the Lower Bound of Adam's Iteration Complexity. (arXiv:2310.17998v1 [cs.LG])

    [http://arxiv.org/abs/2310.17998](http://arxiv.org/abs/2310.17998)

    本文通过推导出一种新的收敛保证，仅需满足$L$-平滑条件和有界噪声方差的假设，来弥合Adam收敛性的差距。特别是在合理选择的超参数下，我们得到了Adam的迭代复杂度的上界，并且证明它满足一阶优化器的下界。

    

    最近，Arjevani等人[1]在$L$-平滑条件和有界噪声方差的假设下，为一阶优化问题建立了迭代复杂度的下界。然而，对Adam收敛性的现有文献进行彻底回顾发现存在明显的差距：它们都未达到上述的下界。在本文中，我们通过推导出Adam的新收敛保证，仅需要$L$-平滑条件和有界噪声方差的假设来弥合这一差距。我们的结果适用于广泛的超参数范围。特别是在合理选择的超参数下，我们得到了Adam的迭代复杂度的上界，并且证明它满足一阶优化器的下界。据我们所知，这是第一个为Adam的收敛性建立如此紧致的上界。我们的证明利用了新颖的技巧来处理动量和自适应学习率之间的交织问题，并将降解引理中的一阶项转化为较简单的形式。

    Recently, Arjevani et al. [1] established a lower bound of iteration complexity for the first-order optimization under an $L$-smooth condition and a bounded noise variance assumption. However, a thorough review of existing literature on Adam's convergence reveals a noticeable gap: none of them meet the above lower bound. In this paper, we close the gap by deriving a new convergence guarantee of Adam, with only an $L$-smooth condition and a bounded noise variance assumption. Our results remain valid across a broad spectrum of hyperparameters. Especially with properly chosen hyperparameters, we derive an upper bound of the iteration complexity of Adam and show that it meets the lower bound for first-order optimizers. To the best of our knowledge, this is the first to establish such a tight upper bound for Adam's convergence. Our proof utilizes novel techniques to handle the entanglement between momentum and adaptive learning rate and to convert the first-order term in the Descent Lemma t
    
[^43]: CEFL：碳高效的联邦学习

    CEFL: Carbon-Efficient Federated Learning. (arXiv:2310.17972v1 [cs.LG])

    [http://arxiv.org/abs/2310.17972](http://arxiv.org/abs/2310.17972)

    该论文介绍了一种称为CEFL的碳高效联邦学习方法，通过使用自适应的成本感知策略来优化FL模型训练的任意成本度量，并成功实现了碳排放减少93％和训练时间减少50％的效果。

    

    联邦学习（FL）通过将机器学习（ML）训练分布在许多边缘设备上，以减少数据传输开销和保护数据隐私。由于FL模型训练可能涉及数百万个设备，因此需要大量资源，因此之前的工作一直致力于提高其资源效率以优化时间至准确性。然而，之前的工作通常将所有资源视为相同，而实际上它们可能产生大不相同的成本，这反而激发了优化成本至准确性的动机。为了解决这个问题，我们设计了CEFL，它使用自适应的成本感知客户选择策略，在训练FL模型时优化任意成本度量。我们的策略扩展并结合了基于效用的客户选择和关键学习期的先前工作，使其具有成本感知性。我们通过设计碳高效的FL来演示CEFL，在这里能源的碳强度是成本，并且显示它可以将碳排放减少93％，并将训练时间减少50％，与随机客户相比。

    Federated Learning (FL) distributes machine learning (ML) training across many edge devices to reduce data transfer overhead and protect data privacy. Since FL model training may span millions of devices and is thus resource-intensive, prior work has focused on improving its resource efficiency to optimize time-to-accuracy. However, prior work generally treats all resources the same, while, in practice, they may incur widely different costs, which instead motivates optimizing cost-to-accuracy. To address the problem, we design CEFL, which uses adaptive cost-aware client selection policies to optimize an arbitrary cost metric when training FL models. Our policies extend and combine prior work on utility-based client selection and critical learning periods by making them cost-aware. We demonstrate CEFL by designing carbon-efficient FL, where energy's carbon-intensity is the cost, and show that it i) reduces carbon emissions by 93\% and reduces training time by 50% compared to random clie
    
[^44]: 一次训练，获得一个家庭：离线到在线强化学习的状态自适应平衡

    Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning. (arXiv:2310.17966v1 [cs.LG])

    [http://arxiv.org/abs/2310.17966](http://arxiv.org/abs/2310.17966)

    在离线到在线强化学习中，现有解决方案往往只使用一种平衡策略，无法充分利用不同状态的数据质量。本论文提出了一种家族式离线到在线强化学习框架(FamO2O)，它通过训练一系列具有不同改进和约束强度的策略，实现了状态自适应的平衡。

    

    离线到在线强化学习是一种训练范式，它将预先收集的数据集上的预训练与在线环境中的微调相结合。然而，引入在线微调可能会加剧已知的分布偏移问题。现有的解决方案通过对离线和在线学习中的政策改进目标施加策略约束来解决这个问题。它们通常主张在不同的数据集上采用一种平衡政策改进和约束的通用方法。然而，这种一刀切的方式可能无法充分利用每个收集到的样本，因为不同状态的数据质量变化很大。为此，我们引入了家族离线到在线强化学习(FamO2O)，这是一个简单而有效的框架，能够赋予现有算法确定状态自适应改进-约束平衡的能力。FamO2O利用一个通用模型训练一个家族的策略，每个策略具有不同的改进/约束强度，和一个

    Offline-to-online reinforcement learning (RL) is a training paradigm that combines pre-training on a pre-collected dataset with fine-tuning in an online environment. However, the incorporation of online fine-tuning can intensify the well-known distributional shift problem. Existing solutions tackle this problem by imposing a policy constraint on the policy improvement objective in both offline and online learning. They typically advocate a single balance between policy improvement and constraints across diverse data collections. This one-size-fits-all manner may not optimally leverage each collected sample due to the significant variation in data quality across different states. To this end, we introduce Family Offline-to-Online RL (FamO2O), a simple yet effective framework that empowers existing algorithms to determine state-adaptive improvement-constraint balances. FamO2O utilizes a universal model to train a family of policies with different improvement/constraint intensities, and a
    
[^45]: 一种全面可靠的特征归因方法：双边删除和重构（DoRaR）

    A Comprehensive and Reliable Feature Attribution Method: Double-sided Remove and Reconstruct (DoRaR). (arXiv:2310.17945v1 [cs.LG])

    [http://arxiv.org/abs/2310.17945](http://arxiv.org/abs/2310.17945)

    双边删除和重构（DoRaR）是一种全面可靠的特征归因方法，用于解决深度神经网络和其他相关模型内部决策机制不透明的问题。

    

    深度神经网络（DNN）和其他机器学习（ML）模型内部决策机制的透明度有限，阻碍了它们在多个领域的应用。为了解决这个问题，已经开发出了特征归因方法，用于识别对这些黑盒模型的决策产生重要影响的关键特征。然而，许多特征归因方法存在固有的缺点。例如，一类特征归因方法存在伪影问题，这些方法直接通过原始训练在自然数据点上的分类器，对超出分布范围的屏蔽输入进行反馈。另一类特征归因方法通过使用联合训练的特征选择器和预测器来找到解释。虽然避免了伪影问题，但这个新类别的方法存在解释中的编码预测问题（EPITE），其中预测器的决策不依赖于特征，而是依赖于选择这些特征的遮罩。

    The limited transparency of the inner decision-making mechanism in deep neural networks (DNN) and other machine learning (ML) models has hindered their application in several domains. In order to tackle this issue, feature attribution methods have been developed to identify the crucial features that heavily influence decisions made by these black box models. However, many feature attribution methods have inherent downsides. For example, one category of feature attribution methods suffers from the artifacts problem, which feeds out-of-distribution masked inputs directly through the classifier that was originally trained on natural data points. Another category of feature attribution method finds explanations by using jointly trained feature selectors and predictors. While avoiding the artifacts problem, this new category suffers from the Encoding Prediction in the Explanation (EPITE) problem, in which the predictor's decisions rely not on the features, but on the masks that selects thos
    
[^46]: 可信任的边缘机器学习：一项调查研究

    Trustworthy Edge Machine Learning: A Survey. (arXiv:2310.17944v1 [cs.LG])

    [http://arxiv.org/abs/2310.17944](http://arxiv.org/abs/2310.17944)

    可信任的边缘机器学习是边缘计算和机器学习的融合，面临各种挑战，本调查总结了对其的定义、属性、框架、技术和解决方案，并强调了在6G网络中的重要性。

    

    边缘计算（EC）和机器学习（ML）的融合，即边缘机器学习（EML），通过利用分布式网络资源以合作方式进行联合训练和推断，已成为一个备受关注的研究领域。然而，EML面临资源限制、异构网络环境以及不同应用的多样化服务需求等各种挑战，这些因素共同影响着EML在利益相关者眼中的可信度。本调查提供了对可信任的EML定义、属性、框架、技术和解决方案的全面总结。具体而言，我们首先强调了在第六代（6G）网络中可信任的EML的重要性。然后，我们从部署和实际应用场景的挑战的角度讨论了可信任性的必要性。随后，我们提供了可信任的EML的初步定义，并探讨了其关键属性。

    The convergence of Edge Computing (EC) and Machine Learning (ML), known as Edge Machine Learning (EML), has become a highly regarded research area by utilizing distributed network resources to perform joint training and inference in a cooperative manner. However, EML faces various challenges due to resource constraints, heterogeneous network environments, and diverse service requirements of different applications, which together affect the trustworthiness of EML in the eyes of its stakeholders. This survey provides a comprehensive summary of definitions, attributes, frameworks, techniques, and solutions for trustworthy EML. Specifically, we first emphasize the importance of trustworthy EML within the context of Sixth-Generation (6G) networks. We then discuss the necessity of trustworthiness from the perspective of challenges encountered during deployment and real-world application scenarios. Subsequently, we provide a preliminary definition of trustworthy EML and explore its key attrib
    
[^47]: Transformers作为图到图模型

    Transformers as Graph-to-Graph Models. (arXiv:2310.17936v1 [cs.CL])

    [http://arxiv.org/abs/2310.17936](http://arxiv.org/abs/2310.17936)

    本文认为Transformers本质上是图到图模型，通过将注意力权重等价于图中的边，并使用图到图Transformer架构结合显式图和潜在图进行非自回归图预测，实现了在建模各种语言结构方面的最先进准确性。

    

    我们认为Transformers本质上是图到图模型，而序列只是一种特殊情况。注意力权重在功能上等价于图中的边。我们的图到图Transformer架构将这种能力明确地体现出来，通过将图的边输入到注意力权重计算中，并使用类似注意力的函数来预测图的边，从而将显式图集成到预训练Transformers学习的潜在图中。添加迭代图细化可以为输入、输出和潜在图提供联合嵌入，使得非自回归图预测可以优化完整的图，而无需任何专门的管道或解码策略。实证结果表明，该架构在建模各种语言结构方面达到了最先进的准确性，并与预训练学习的潜在语言表示非常有效地集成。

    We argue that Transformers are essentially graph-to-graph models, with sequences just being a special case. Attention weights are functionally equivalent to graph edges. Our Graph-to-Graph Transformer architecture makes this ability explicit, by inputting graph edges into the attention weight computations and predicting graph edges with attention-like functions, thereby integrating explicit graphs into the latent graphs learned by pretrained Transformers. Adding iterative graph refinement provides a joint embedding of input, output, and latent graphs, allowing non-autoregressive graph prediction to optimise the complete graph without any bespoke pipeline or decoding strategy. Empirical results show that this architecture achieves state-of-the-art accuracies for modelling a variety of linguistic structures, integrating very effectively with the latent linguistic representations learned by pretraining.
    
[^48]: 解开Q-learning中深度的力量之谜

    Lifting the Veil: Unlocking the Power of Depth in Q-learning. (arXiv:2310.17915v1 [cs.LG])

    [http://arxiv.org/abs/2310.17915](http://arxiv.org/abs/2310.17915)

    本文通过严格证明在统计学习理论框架下，深度Q-learning的深度具有显著优势，能够捕捉奖励的特殊属性。这一结果对解释深度Q-learning成功的原因起到了重要作用。

    

    在大量数据和丰富的计算资源的帮助下，深度Q-learning已广泛应用于运筹学和管理科学，并在众多应用中取得了巨大成功，包括推荐系统、供应链、游戏和机器人操纵。然而，深度Q-learning的成功缺乏坚实的理论验证和解释性。本文的目的是在统计学习理论框架下，实证地验证深度在深度Q-learning中的作用。通过严格证明其良好的泛化误差界，我们的结果揭示了深度Q-learning成功的主要原因是深度神经网络（深层网络）在捕捉奖励的特殊属性，即空间稀疏性和分段恒定性方面的出色性能，而不是它们的大容量。在本文中，我们对推动深度Q-learning的领域做出了基础性的贡献。

    With the help of massive data and rich computational resources, deep Q-learning has been widely used in operations research and management science and has contributed to great success in numerous applications, including recommender systems, supply chains, games, and robotic manipulation. However, the success of deep Q-learning lacks solid theoretical verification and interpretability. The aim of this paper is to theoretically verify the power of depth in deep Q-learning. Within the framework of statistical learning theory, we rigorously prove that deep Q-learning outperforms its traditional version by demonstrating its good generalization error bound. Our results reveal that the main reason for the success of deep Q-learning is the excellent performance of deep neural networks (deep nets) in capturing the special properties of rewards namely, spatial sparseness and piecewise constancy, rather than their large capacities. In this paper, we make fundamental contributions to the field of 
    
[^49]: 改进知识梯度算法

    Improving the Knowledge Gradient Algorithm. (arXiv:2310.17901v1 [cs.LG])

    [http://arxiv.org/abs/2310.17901](http://arxiv.org/abs/2310.17901)

    本研究改进了知识梯度算法，提出了一种改进的知识梯度（iKG）策略，该策略解决了知识梯度算法的局限性，并且在最佳臂识别问题中具有渐近最优性。此外，相比知识梯度（KG），iKG更容易扩展到其他BAI问题，且在这些问题上表现出更好的性能。

    

    知识梯度（KG）算法是一种用于最佳臂识别（BAI）问题的流行策略。它建立在一种简单的思想上，即始终选择产生对臂的最佳均值估计预期一步改进最大的测量。在这项研究中，我们发现这种策略存在局限性，导致算法在渐近上不是最优的。我们接下来提供了一种改进方法，通过遵循KG的一步前瞻方式，但选择产生对选择最佳臂的概率最大的一步改进的测量。新的策略称为改进的知识梯度（iKG）。可以证明iKG在渐近上是最优的。此外，我们还展示了与KG相比，更容易将iKG扩展到BAI的不同问题，其中包括ε-好臂识别和可行臂识别两个例子。我们还通过数值实例进一步展示了iKG在这些问题上的优越性能。

    The knowledge gradient (KG) algorithm is a popular policy for the best arm identification (BAI) problem. It is built on the simple idea of always choosing the measurement that yields the greatest expected one-step improvement in the estimate of the best mean of the arms. In this research, we show that this policy has limitations, causing the algorithm not asymptotically optimal. We next provide a remedy for it, by following the manner of one-step look ahead of KG, but instead choosing the measurement that yields the greatest one-step improvement in the probability of selecting the best arm. The new policy is called improved knowledge gradient (iKG). iKG can be shown to be asymptotically optimal. In addition, we show that compared to KG, it is easier to extend iKG to variant problems of BAI, with the $\epsilon$-good arm identification and feasible arm identification as two examples. The superior performances of iKG on these problems are further demonstrated using numerical examples.
    
[^50]: 分层联邦学习中的子模型划分：算法设计与收敛分析

    Submodel Partitioning in Hierarchical Federated Learning: Algorithm Design and Convergence Analysis. (arXiv:2310.17890v1 [cs.LG])

    [http://arxiv.org/abs/2310.17890](http://arxiv.org/abs/2310.17890)

    本文提出了一种针对分层联邦学习的新方法：分层独立子模型训练（HIST）。该方法通过将全局模型划分为不相交的子模型，并在分层结构中分布，以降低边缘设备上的计算、通信和存储负担，同时节约资源。

    

    分层联邦学习（HFL）相较传统的“星型拓扑”架构的联邦学习具有更好的可扩展性。然而，在资源受限的物联网（IoT）设备上训练大规模模型时，HFL仍然会对边缘设备造成重大的计算、通信和存储负担。本文提出了一种新的联邦学习方法——分层独立子模型训练（HIST），旨在解决分层场景下的这些问题。HIST的关键思想是分层版本的模型划分，即在每一轮中将全局模型划分为不相交的子模型，并将它们分布在不同的细胞中，使得每个细胞只负责训练全模型的一个划分。这样每个客户端可以节省计算和存储成本，同时减轻整个分层结构中的通信负载。我们对HIST在非凸优化问题下的收敛性行为进行了特征化分析。

    Hierarchical federated learning (HFL) has demonstrated promising scalability advantages over the traditional "star-topology" architecture-based federated learning (FL). However, HFL still imposes significant computation, communication, and storage burdens on the edge, especially when training a large-scale model over resource-constrained Internet of Things (IoT) devices. In this paper, we propose hierarchical independent submodel training (HIST), a new FL methodology that aims to address these issues in hierarchical settings. The key idea behind HIST is a hierarchical version of model partitioning, where we partition the global model into disjoint submodels in each round, and distribute them across different cells, so that each cell is responsible for training only one partition of the full model. This enables each client to save computation/storage costs while alleviating the communication loads throughout the hierarchy. We characterize the convergence behavior of HIST for non-convex 
    
[^51]: 印象——理解视觉符号学和美学影响力

    Impressions: Understanding Visual Semiotics and Aesthetic Impact. (arXiv:2310.17887v1 [cs.CV])

    [http://arxiv.org/abs/2310.17887](http://arxiv.org/abs/2310.17887)

    本研究通过提出Impressions数据集，探索了图像的符号学以及特定的视觉特征和设计选择如何引发情感、思维和信念。研究表明，图像的影响力不仅仅在于美学的形式定义，而是与其作为沟通行为的成功息息相关。

    

    美学影响力是否与美不同？视觉显著性是否反映了其有效沟通的能力？我们提出了Impressions，这是一个新颖的数据集，通过它可以研究图像的符号学，以及特定的视觉特征和设计选择如何引发特定的情感、思维和信念。我们认为图像的影响力不仅仅在于美学的形式定义，而是在于其作为一种沟通行为的成功，其中风格对于意义形成的贡献与主题一样重要。然而，先前的图像描述数据集并不适合最先进的架构来模拟人类对图像的潜在印象或解读。为了填补这一空白，我们设计了一个受视觉艺术中图像分析技术启发的注释任务，收集了1,440个图像-标题对和4,320个独特的注释，探索影响力、实用图像描述、印象和美学设计选择。

    Is aesthetic impact different from beauty? Is visual salience a reflection of its capacity for effective communication? We present Impressions, a novel dataset through which to investigate the semiotics of images, and how specific visual features and design choices can elicit specific emotions, thoughts and beliefs. We posit that the impactfulness of an image extends beyond formal definitions of aesthetics, to its success as a communicative act, where style contributes as much to meaning formation as the subject matter. However, prior image captioning datasets are not designed to empower state-of-the-art architectures to model potential human impressions or interpretations of images. To fill this gap, we design an annotation task heavily inspired by image analysis techniques in the Visual Arts to collect 1,440 image-caption pairs and 4,320 unique annotations exploring impact, pragmatic image description, impressions, and aesthetic design choices. We show that existing multimodal image 
    
[^52]: 为协调虚拟电厂资产的分布式优化引入机器学习

    Machine Learning Infused Distributed Optimization for Coordinating Virtual Power Plant Assets. (arXiv:2310.17882v1 [cs.LG])

    [http://arxiv.org/abs/2310.17882](http://arxiv.org/abs/2310.17882)

    本文提出了一种名为LOOP-MAC的方法，利用机器学习辅助的分布式优化来协调虚拟电厂（VPP）的资产。该方法采用多智能体协调视角，利用神经网络逼近器加速解决方案搜索。该方法通过引入测量图来保证最佳解之间的关系。

    

    随着对分布式能源资源（DER）部署日益增加的兴趣，虚拟电厂（VPP）已成为汇集各种DER并促进其参与批发能源市场的关键工具。这些VPP部署得到了联邦能源监管委员会第2222号命令的推动，该命令使DER和VPP在市场领域具有竞争力。然而，DER的多样性和分散性质给VPP资产的可扩展协调带来了显著挑战。为了解决效率和速度瓶颈，本文提出了一种新颖的机器学习辅助的分布式优化方法来协调VPP资产。我们的方法名为LOOP-MAC（为多智能体协调优化过程学习优化），采用多智能体协调视角，每个VPP代理管理多个DER并利用神经网络逼近器加速解决方案搜索。LOOP-MAC方法使用一个测量图确保了最佳解之间的关系。

    Amid the increasing interest in the deployment of Distributed Energy Resources (DERs), the Virtual Power Plant (VPP) has emerged as a pivotal tool for aggregating diverse DERs and facilitating their participation in wholesale energy markets. These VPP deployments have been fueled by the Federal Energy Regulatory Commission's Order 2222, which makes DERs and VPPs competitive across market segments. However, the diversity and decentralized nature of DERs present significant challenges to the scalable coordination of VPP assets. To address efficiency and speed bottlenecks, this paper presents a novel machine learning-assisted distributed optimization to coordinate VPP assets. Our method, named LOOP-MAC(Learning to Optimize the Optimization Process for Multi-agent Coordination), adopts a multi-agent coordination perspective where each VPP agent manages multiple DERs and utilizes neural network approximators to expedite the solution search. The LOOP-MAC method employs a gauge map to guarant
    
[^53]: 一种具有改进预处理时间的亚线性时间谱聚类预测器

    A Sublinear-Time Spectral Clustering Oracle with Improved Preprocessing Time. (arXiv:2310.17878v1 [cs.DS])

    [http://arxiv.org/abs/2310.17878](http://arxiv.org/abs/2310.17878)

    本研究提出了一种亚线性时间的谱聚类预测器，用于处理具有强聚类特性的图。该预测器能够在亚线性时间内进行预处理和查询聚类成员，并且与真实聚类接近的k-分区保持一致。此外，该预测器对于少量的随机边删除具有鲁棒性。

    

    我们解决了设计一种适用于具有强聚类特性的图的亚线性时间谱聚类预测器的问题。这样的图包含k个潜在聚类，每个聚类的内导纳较大（至少为φ），外导纳较小（最多为ε）。我们的目标是对图进行预处理，以使得聚类成员查询能够在亚线性时间内进行，并且所得到的分区应与接近真实聚类的k-分区一致。之前的预测器要么依赖于内外导纳之间有一个poly(k)log n的差距，要么需要指数级（在k/ε上）的预处理时间。我们的算法放宽了这些假设，尽管会略微增加错误分类率。我们还展示了我们的聚类预测器对于少量的随机边删除是鲁棒的。为了验证我们的理论界限，我们进行了实验。

    We address the problem of designing a sublinear-time spectral clustering oracle for graphs that exhibit strong clusterability. Such graphs contain $k$ latent clusters, each characterized by a large inner conductance (at least $\varphi$) and a small outer conductance (at most $\varepsilon$). Our aim is to preprocess the graph to enable clustering membership queries, with the key requirement that both preprocessing and query answering should be performed in sublinear time, and the resulting partition should be consistent with a $k$-partition that is close to the ground-truth clustering. Previous oracles have relied on either a $\textrm{poly}(k)\log n$ gap between inner and outer conductances or exponential (in $k/\varepsilon$) preprocessing time. Our algorithm relaxes these assumptions, albeit at the cost of a slightly higher misclassification ratio. We also show that our clustering oracle is robust against a few random edge deletions. To validate our theoretical bounds, we conducted exp
    
[^54]: ASPIRO: 一种适用于零到少样本情况下结构化数据到文本生成的错误感知重提示方法

    ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation. (arXiv:2310.17877v1 [cs.CL])

    [http://arxiv.org/abs/2310.17877](http://arxiv.org/abs/2310.17877)

    ASPIRO是一种能在零到少样本情况下将结构化数据转化为简短模板句子的方法。通过算法解析检查、LLM的重新提示以及一致性验证指标PARENT，ASPIRO成功降低了66%的解析错误率，并且在与最近的预训练语言模型的竞争中表现出色。

    

    我们提出了ASPIRO，一种在零到少样本情况下将结构化数据转化为简短模板句子的方法。与之前的方法不同，我们的方法直接提示大型语言模型（LLM）产生与实体无关的模板，而不是依赖LLM忠实地复制给定的实体，或者手动验证/制作模板。我们通过算法解析检查和PARENT指标诱导的一致性验证，结合LLM的重新提示，实时识别和纠正模板生成问题。在DART数据集上，与直接LLM输出相比，ASPIRO对RDF三元组的生成文本的解析错误率平均降低了66％。我们在Rel2Text数据集上的最佳5样本text-davinci-003设置评分为BLEU 50.62，METEOR 45.16，BLEURT 0.82，NUBIA 0.87和PARENT 0.8962，与最近的精调预训练语言模型有了有效的竞争力。

    We present ASPIRO, an approach for structured data verbalisation into short template sentences in zero to few-shot settings. Unlike previous methods, our approach prompts large language models (LLMs) to directly produce entity-agnostic templates, rather than relying on LLMs to faithfully copy the given example entities, or validating/crafting the templates manually. We incorporate LLM re-prompting, triggered by algorithmic parsing checks, as well as the PARENT metric induced consistency validation to identify and rectify template generation problems in real-time. ASPIRO, compared to direct LLM output, averages 66\% parsing error rate reduction in generated verbalisations of RDF triples on the DART dataset. Our best 5-shot text-davinci-003 setup, scoring BLEU of 50.62, METEOR of 45.16, BLEURT of 0.82, NUBIA of 0.87, and PARENT of 0.8962 on the Rel2Text dataset, competes effectively with recent fine-tuned pre-trained language models.
    
[^55]: 带有槽约束的排名问题

    Ranking with Slot Constraints. (arXiv:2310.17870v1 [cs.IR])

    [http://arxiv.org/abs/2310.17870](http://arxiv.org/abs/2310.17870)

    带有槽约束的排名问题中，我们提出了一种新的排名算法MatchRank，它在候选人按排名顺序被人类决策者评估时，产生最大化填充槽位的排名。算法在理论上具有强大的逼近保证，并且可以高效实现。 (arXiv:2310.17870v1 [cs.IR])

    

    我们引入了带有槽约束的排名问题，这可以用来建模各种应用问题 - 从具有不同专业限制槽位的大学录取，到在医学试验中构建符合条件的参与者分层队列。我们发现，传统的概率排名原则（PRP）在带有槽约束的排名问题中可能会非常次优，因此我们提出了一种新的排名算法，称为MatchRank。MatchRank的目标是在候选人按排名顺序由人类决策者进行评估时，产生最大化填充槽位的排名。这样，MatchRank在广义上是PRP的推广，当没有槽约束时，它是PRP的特例。我们的理论分析表明，MatchRank具有强大的逼近保证，没有任何槽位或候选人之间的独立性假设。此外，我们展示了如何高效地实现MatchRank。除了理论保证外，我们还展示了MatchRank的实验结果在不同应用领域的有效性。

    We introduce the problem of ranking with slot constraints, which can be used to model a wide range of application problems -- from college admission with limited slots for different majors, to composing a stratified cohort of eligible participants in a medical trial. We show that the conventional Probability Ranking Principle (PRP) can be highly sub-optimal for slot-constrained ranking problems, and we devise a new ranking algorithm, called MatchRank. The goal of MatchRank is to produce rankings that maximize the number of filled slots if candidates are evaluated by a human decision maker in the order of the ranking. In this way, MatchRank generalizes the PRP, and it subsumes the PRP as a special case when there are no slot constraints. Our theoretical analysis shows that MatchRank has a strong approximation guarantee without any independence assumptions between slots or candidates. Furthermore, we show how MatchRank can be implemented efficiently. Beyond the theoretical guarantees, em
    
[^56]: 多实例学习中的可重现性: 算法单元测试的案例

    Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests. (arXiv:2310.17867v1 [stat.ML])

    [http://arxiv.org/abs/2310.17867](http://arxiv.org/abs/2310.17867)

    多实例学习中的五个深度模型在学习过程中违反了标准的MIL假设，导致能够学习反相关的实例。这一问题需要通过改进和其他策略来解决。

    

    多实例学习(MIL)是分类问题的一个子领域，其中有正负标签和一个输入的“包”，当且仅当包中包含一个正元素时，标签为正，否则为负。在这种情况下，训练需要将包级标签与实例级信息关联起来，并隐含着一个因果假设和任务的不对称性（即，无法交换标签而不改变语义）。MIL问题出现在医疗保健（一个恶性细胞表示癌症），网络安全（一个恶意可执行文件会感染计算机）等许多任务中。在这项工作中，我们检查了最著名的五个深度MIL模型，并发现它们都不符合标准的MIL假设。它们能够学习反相关的实例，即在看到负的反例之前默认为“正”标签，这对于一个正确的MIL模型来说是不可能的。我们怀疑改进和其他策略可能会改善这一问题。

    Multiple Instance Learning (MIL) is a sub-domain of classification problems with positive and negative labels and a "bag" of inputs, where the label is positive if and only if a positive element is contained within the bag, and otherwise is negative. Training in this context requires associating the bag-wide label to instance-level information, and implicitly contains a causal assumption and asymmetry to the task (i.e., you can't swap the labels without changing the semantics). MIL problems occur in healthcare (one malignant cell indicates cancer), cyber security (one malicious executable makes an infected computer), and many other tasks. In this work, we examine five of the most prominent deep-MIL models and find that none of them respects the standard MIL assumption. They are able to learn anti-correlated instances, i.e., defaulting to "positive" labels until seeing a negative counter-example, which should not be possible for a correct MIL model. We suspect that enhancements and othe
    
[^57]: 函数空间贝叶斯伪核心集用于贝叶斯神经网络

    Function Space Bayesian Pseudocoreset for Bayesian Neural Networks. (arXiv:2310.17852v1 [cs.LG])

    [http://arxiv.org/abs/2310.17852](http://arxiv.org/abs/2310.17852)

    本论文提出了一种在函数空间上操作的新颖贝叶斯伪核心集构建方法，通过构建核心集后验的变分近似并在函数空间中将其与完整数据后验匹配，实现了对深度神经网络等高维模型的贝叶斯推断的可扩展性。

    

    贝叶斯伪核心集是一个紧凑的合成数据集，总结了大规模数据集的基本信息，因此可以作为可扩展贝叶斯推断的代理数据集。通常，通过最小化伪核心集后验条件和完整数据集后验条件之间的差异度量来构建贝叶斯伪核心集。然而，评估差异度量可能具有挑战性，尤其是对于具有高维参数的深度神经网络等模型。在本文中，我们提出了一种在函数空间上操作的新颖贝叶斯伪核心集构建方法。与以往的方法不同，以模型参数（权重）的空间构建和匹配核心集和完整数据后验，我们的方法在函数空间上构建核心集后验的变分近似，并在函数空间中将其与完整数据后验匹配。通过直接在函数空间中工作，我们的方法可以绕过一些计算和评估的困难。

    A Bayesian pseudocoreset is a compact synthetic dataset summarizing essential information of a large-scale dataset and thus can be used as a proxy dataset for scalable Bayesian inference. Typically, a Bayesian pseudocoreset is constructed by minimizing a divergence measure between the posterior conditioning on the pseudocoreset and the posterior conditioning on the full dataset. However, evaluating the divergence can be challenging, particularly for the models like deep neural networks having high-dimensional parameters. In this paper, we propose a novel Bayesian pseudocoreset construction method that operates on a function space. Unlike previous methods, which construct and match the coreset and full data posteriors in the space of model parameters (weights), our method constructs variational approximations to the coreset posterior on a function space and matches it to the full data posterior in the function space. By working directly on the function space, our method could bypass sev
    
[^58]: 使用合成数据扩展提升数据分析

    Boosting Data Analytics With Synthetic Volume Expansion. (arXiv:2310.17848v1 [stat.ML])

    [http://arxiv.org/abs/2310.17848](http://arxiv.org/abs/2310.17848)

    本文介绍了一种利用合成数据生成框架来提升数据分析的方法，在此方法中，使用先进模型生成高逼真度的合成数据，并采用统计方法进行分析。研究发现，在合成数据上的统计方法错误随着合成数据的增加而减少，但最终可能会增加或停滞。

    

    合成数据生成作为生成式人工智能的基石，在解决数据稀缺和隐私问题的同时，实现了前所未有的性能。随着合成数据的日益重要，人们开始关注统计方法在合成数据与原始数据上的准确性。在本文中，我们介绍了用于分析的合成数据生成框架。该框架使用高逼真度的合成数据，通过先进模型如表格扩散和生成式预训练转换器模型生成，并结合相关研究洞察进一步增强。在这个框架中的一个重要发现是生成效应：统计方法在合成数据上的错误随着合成数据的增加一开始减少，但最终可能会增加或停滞。这个现象根源于复制原始数据分布的复杂性。

    Synthetic data generation, a cornerstone of Generative Artificial Intelligence, signifies a paradigm shift in data science by addressing data scarcity and privacy while enabling unprecedented performance. As synthetic data gains prominence, questions arise concerning the accuracy of statistical methods when applied to synthetic data compared to raw data. In this article, we introduce the Synthetic Data Generation for Analytics framework. This framework employs statistical methods on high-fidelity synthetic data generated by advanced models such as tabular diffusion and Generative Pre-trained Transformer models. These models, trained on raw data, are further enhanced with insights from pertinent studies. A significant discovery within this framework is the generational effect: the error of a statistical method on synthetic data initially diminishes with added synthetic data but may eventually increase or plateau. This phenomenon, rooted in the complexities of replicating raw data distri
    
[^59]: 一种以数据为中心的机器学习在线市场：从发现到定价

    A Data-Centric Online Market for Machine Learning: From Discovery to Pricing. (arXiv:2310.17843v1 [cs.LG])

    [http://arxiv.org/abs/2310.17843](http://arxiv.org/abs/2310.17843)

    这篇论文介绍了一种以数据为中心的在线市场，用于连接机器学习的供求匹配，并提出了解决这个市场设计中的两个核心挑战的新技术。

    

    数据是机器学习的动力 - 丰富和高质量的训练数据对于机器学习的成功至关重要。然而，要将机器学习从少数大型公司之间的竞赛转变为为众多普通用户的数据分析请求服务的可访问技术，仍然存在重要的挑战。我们观察到的一个差距是，许多机器学习用户可以从其他数据所有者拥有的新数据中受益，而这些数据所有者却坐在一堆数据上，不知道谁可以受益于它。这种差距为构建一个能够自动连接供求的在线市场创造了机会。虽然在线匹配市场很常见（例如，打车系统），但为机器学习设计一个以数据为中心的市场面临许多前所未有的挑战。本文开发了新的技术来解决设计这样一个市场中的两个核心挑战：（a）为了高效地将需求与供应匹配，我们设计了一种算法，可以从数千个数据池中自动发现任何机器学习任务所需的有用数据。

    Data fuels machine learning (ML) - rich and high-quality training data is essential to the success of ML. However, to transform ML from the race among a few large corporations to an accessible technology that serves numerous normal users' data analysis requests, there still exist important challenges. One gap we observed is that many ML users can benefit from new data that other data owners possess, whereas these data owners sit on piles of data without knowing who can benefit from it. This gap creates the opportunity for building an online market that can automatically connect supply with demand. While online matching markets are prevalent (e.g., ride-hailing systems), designing a data-centric market for ML exhibits many unprecedented challenges.  This paper develops new techniques to tackle two core challenges in designing such a market: (a) to efficiently match demand with supply, we design an algorithm to automatically discover useful data for any ML task from a pool of thousands o
    
[^60]: 基于位置编码的多住户智能家居居民识别

    Positional Encoding-based Resident Identification in Multi-resident Smart Homes. (arXiv:2310.17836v1 [cs.LG])

    [http://arxiv.org/abs/2310.17836](http://arxiv.org/abs/2310.17836)

    该论文提出了一种基于位置编码的多住户智能家居居民识别框架，通过构建图形和利用时间序列数据，可以有效地识别居民的身份。实验结果表明，该方法在多住户环境中可以达到较高的准确率。

    

    我们提出了一种新的居民识别框架，用于在多住户智能环境中识别居民。该框架采用了基于位置编码概念的特征提取模型。特征提取模型将住宅位置视为图形，并设计了一种新算法来从智能环境的布局图中构建这些图形。Node2Vec算法被用于将图形转化为高维节点嵌入。引入了一种长短期记忆（LSTM）模型，使用节点嵌入和传感器事件的时间序列来预测居民的身份。大量实验证明我们提出的方案可以有效地识别多住户环境中的居民。在两个真实数据集上的评估结果表明，我们提出的方法分别达到了94.5％和87.9％的准确率。

    We propose a novel resident identification framework to identify residents in a multi-occupant smart environment. The proposed framework employs a feature extraction model based on the concepts of positional encoding. The feature extraction model considers the locations of homes as a graph. We design a novel algorithm to build such graphs from layout maps of smart environments. The Node2Vec algorithm is used to transform the graph into high-dimensional node embeddings. A Long Short-Term Memory (LSTM) model is introduced to predict the identities of residents using temporal sequences of sensor events with the node embeddings. Extensive experiments show that our proposed scheme effectively identifies residents in a multi-occupant environment. Evaluation results on two real-world datasets demonstrate that our proposed approach achieves 94.5% and 87.9% accuracy, respectively.
    
[^61]: 使用机器学习和局部测量的混合光学湍流模型

    Hybrid Optical Turbulence Models Using Machine Learning and Local Measurements. (arXiv:2310.17829v1 [physics.ao-ph])

    [http://arxiv.org/abs/2310.17829](http://arxiv.org/abs/2310.17829)

    该论文介绍了使用机器学习和局部测量的混合光学湍流模型。通过结合基线宏观气象模型和局部观测，可以提高预测能力。与基线模型和仅利用局部观测训练的机器学习模型相比，混合模型具有更大的潜力。

    

    准确预测局部环境中的大气光学湍流对于估算自由空间光学系统性能至关重要。将在一个环境中开发的宏观气象模型应用于新环境可能会失败。然而，现有的宏观气象模型预计仍然具有一定的预测能力。通过将基线宏观气象模型与局部观测结合起来，训练混合模型可以改进每个基线模型的预测能力。将混合模型、选择的基线宏观气象模型和仅利用局部观测训练的机器学习模型的性能进行比较，突出了混合模型的潜力。

    Accurate prediction of atmospheric optical turbulence in localized environments is essential for estimating the performance of free-space optical systems. Macro-meteorological models developed to predict turbulent effects in one environment may fail when applied in new environments. However, existing macro-meteorological models are expected to offer some predictive power. Building a new model from locally-measured macro-meteorology and scintillometer readings can require significant time and resources, as well as a large number of observations. These challenges motivate the development of a machine-learning informed hybrid model framework. By combining some baseline macro-meteorological model with local observations, hybrid models were trained to improve upon the predictive power of each baseline model. Comparisons between the performance of the hybrid models, the selected baseline macro-meteorological models, and machine-learning models trained only on local observations highlight pot
    
[^62]: Bayesian成像逆问题中的SA-Roundtrip先验及HMC-pCN采样器

    Bayesian imaging inverse problem with SA-Roundtrip prior via HMC-pCN sampler. (arXiv:2310.17817v1 [stat.ML])

    [http://arxiv.org/abs/2310.17817](http://arxiv.org/abs/2310.17817)

    本文提出了一种新型的深度生成先验，SA-Roundtrip，可以进行可控的采样生成，并识别数据的内在维度。基于该先验，结合Hamiltonian Monte Carlo算法，解决了贝叶斯成像逆问题，在计算机断层扫描重建任务上超过了最先进的对比算法。

    

    贝叶斯推断与深度生成先验在许多科学和工程领域的成像逆问题求解中受到了广泛关注。先验分布的选择是从可用先验测量中学习的，因此是关于可用先验测量的重要表示学习。引入了一种新颖的深度生成先验SA-Roundtrip，以实现可控的采样生成，并识别出数据的内在维度。该先验在双向生成对抗网络中嵌入了自注意力结构。随后，将贝叶斯推断应用于低维潜在空间中的后验分布，使用具有预条件Crank-Nicolson算法的Hamiltonian Monte Carlo (HMC-pCN)。该算法在特定条件下被证明具有遍历性。对MNIST和TomoPhantom数据集进行的计算机断层扫描重建实验表明，该方法优于最先进的对比算法。

    Bayesian inference with deep generative prior has received considerable interest for solving imaging inverse problems in many scientific and engineering fields. The selection of the prior distribution is learned from, and therefore an important representation learning of, available prior measurements. The SA-Roundtrip, a novel deep generative prior, is introduced to enable controlled sampling generation and identify the data's intrinsic dimension. This prior incorporates a self-attention structure within a bidirectional generative adversarial network. Subsequently, Bayesian inference is applied to the posterior distribution in the low-dimensional latent space using the Hamiltonian Monte Carlo with preconditioned Crank-Nicolson (HMC-pCN) algorithm, which is proven to be ergodic under specific conditions. Experiments conducted on computed tomography (CT) reconstruction with the MNIST and TomoPhantom datasets reveal that the proposed method outperforms state-of-the-art comparisons, consis
    
[^63]: Local Discovery by Partitioning: 在有限先验知识下的多项式时间因果发现方法

    Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs. (arXiv:2310.17816v1 [stat.ML])

    [http://arxiv.org/abs/2310.17816](http://arxiv.org/abs/2310.17816)

    在有限先验知识下，通过局部分区发现算法（LDP），该研究解决了自动变量选择的问题。LDP根据与曝光-结果对{X,Y}相关的子集将变量集合Z进行分区，并区分混淆因素和其他变量类型。该算法具有理论保证，并在实践中观察到次二次的运行时间。

    

    该研究解决了在有限先验知识下自动变量选择的问题。给定一个{X,Y}的曝光-结果对和一个未知因果结构的变量集合Z，局部分区发现（LDP）算法将Z划分成与{X,Y}相关的子集。我们列举了任意Z的8个穷举且互不重复的分区，并利用这个分类法区分混淆因素和其他变量类型。LDP的动机是有效的调整集识别，但避免了自动变量选择方法中常见的预处理假设。我们提供了理论保证，LDP对于满足足够图形条件的任何Z都返回一个有效的调整集。在更强的条件下，我们证明了分区标签的渐近正确性。总独立性测试在|Z|的最坏情况下是二次的，经验上观察到次二次的运行时间。我们在合成数据上对理论保证进行了数值验证。

    This work addresses the problem of automated covariate selection under limited prior knowledge. Given an exposure-outcome pair {X,Y} and a variable set Z of unknown causal structure, the Local Discovery by Partitioning (LDP) algorithm partitions Z into subsets defined by their relation to {X,Y}. We enumerate eight exhaustive and mutually exclusive partitions of any arbitrary Z and leverage this taxonomy to differentiate confounders from other variable types. LDP is motivated by valid adjustment set identification, but avoids the pretreatment assumption commonly made by automated covariate selection methods. We provide theoretical guarantees that LDP returns a valid adjustment set for any Z that meets sufficient graphical conditions. Under stronger conditions, we prove that partition labels are asymptotically correct. Total independence tests is worst-case quadratic in |Z|, with sub-quadratic runtimes observed empirically. We numerically validate our theoretical guarantees on synthetic 
    
[^64]: 一个特征学习的光谱条件

    A Spectral Condition for Feature Learning. (arXiv:2310.17813v1 [cs.LG])

    [http://arxiv.org/abs/2310.17813](http://arxiv.org/abs/2310.17813)

    本文研究了在大型神经网络中特征学习的光谱条件，并提出了将权重矩阵和更新的谱范数缩放为$\sqrt{\texttt{fan-out}/\texttt{fan-in}}$的方法，以实现特征学习。同时，作者还导出了最大更新参数化的推导，旨在帮助读者对神经网络中的特征学习理解更加深入。

    

    针对训练规模越来越大的神经网络的推动，本文研究了在大型网络宽度上的初始化和训练。一个关键挑战是对网络的内部表示进行非平凡的演变，即特征学习。研究表明，通过缩放权重矩阵和更新的谱范数，我们可以实现特征学习，缩放系数为$\sqrt{\texttt{fan-out}/\texttt{fan-in}}$，与基于Frobenius范数和元素大小的启发式缩放方法有所不同。我们的光谱缩放分析还导出了最大更新参数化的基本推导。总之，我们旨在为读者提供对神经网络中特征学习的坚实概念理解。

    The push to train ever larger neural networks has motivated the study of initialization and training at large network width. A key challenge is to scale training so that a network's internal representations evolve nontrivially at all widths, a process known as feature learning. Here, we show that feature learning is achieved by scaling the spectral norm of weight matrices and their updates like $\sqrt{\texttt{fan-out}/\texttt{fan-in}}$, in contrast to widely used but heuristic scalings based on Frobenius norm and entry size. Our spectral scaling analysis also leads to an elementary derivation of \emph{maximal update parametrization}. All in all, we aim to provide the reader with a solid conceptual understanding of feature learning in neural networks.
    
[^65]: Clover: 闭环可验证代码生成

    Clover: Closed-Loop Verifiable Code Generation. (arXiv:2310.17807v1 [cs.SE])

    [http://arxiv.org/abs/2310.17807](http://arxiv.org/abs/2310.17807)

    Clover是一种闭环可验证代码生成的范式，通过在代码、docstrings和形式注释之间进行一致性检查，确保生成的代码的正确性。

    

    在软件开发中，使用大型语言模型进行代码生成是一个快速增长的趋势。然而，如果没有有效的方法来确保生成的代码的正确性，这个趋势可能会导致许多不良结果。在本文中，我们提出了一个解决这个挑战的愿景：Clover范式，即闭环可验证代码生成，它将正确性检查简化为更可访问的一致性检查问题。在Clover的核心是一个检查器，它在代码、docstrings和形式注释之间进行一致性检查。该检查器使用了形式验证工具和大型语言模型的新颖集成实现。我们提供了理论分析来支持我们的论点，即Clover在一致性检查方面应该是有效的。我们还在一个由手工设计的数据集（CloverBench）上进行了实证调查，该数据集包含了注释的Dafny程序，难度水平与教科书相当。实验结果显示

    The use of large language models for code generation is a rapidly growing trend in software development. However, without effective methods for ensuring the correctness of generated code, this trend could lead to any number of undesirable outcomes. In this paper, we lay out a vision for addressing this challenge: the Clover paradigm, short for Closed-Loop Verifiable Code Generation, which reduces correctness checking to the more accessible problem of consistency checking. At the core of Clover lies a checker that performs consistency checks among code, docstrings, and formal annotations. The checker is implemented using a novel integration of formal verification tools and large language models. We provide a theoretical analysis to support our thesis that Clover should be effective at consistency checking. We also empirically investigate its feasibility on a hand-designed dataset (CloverBench) featuring annotated Dafny programs at a textbook level of difficulty. Experimental results sho
    
[^66]: 通过DreamerV3技巧提高PPO的奖励规模鲁棒性

    Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks. (arXiv:2310.17805v1 [cs.LG])

    [http://arxiv.org/abs/2310.17805](http://arxiv.org/abs/2310.17805)

    本研究将DreamerV3的技巧应用到PPO中，并发现这些技巧并不能普遍改善PPO的性能。通过大量的消融研究，我们确定了一些情况下这些技巧的成功，并对它们的关系提供了深入洞察。

    

    大多数强化学习方法依赖于密集、规范化的环境奖励。DreamerV3最近引入了一种基于模型的方法，并采用了一些技巧来减轻这些限制，使用一组超参数在广泛的基准测试中实现了最新的状态。这个结果引发了关于这些技巧的普适性的讨论，因为它们似乎适用于其他强化学习算法。我们的工作将DreamerV3的技巧应用到PPO中，这是第一次在原始工作之外进行这样的实证研究。令人惊讶的是，我们发现这些技巧并不能作为一般的改进转移到PPO上。我们使用了高质量的PPO参考实现，并在Arcade Learning Environment和DeepMind Control Suite上进行了长达10,000个A100小时的大量消融研究。虽然我们的实验表明这些技巧并没有普遍超过PPO，但我们确定了它们成功的情况，并对它们的关系提供了深入洞察。

    Most reinforcement learning methods rely heavily on dense, well-normalized environment rewards. DreamerV3 recently introduced a model-based method with a number of tricks that mitigate these limitations, achieving state-of-the-art on a wide range of benchmarks with a single set of hyperparameters. This result sparked discussion about the generality of the tricks, since they appear to be applicable to other reinforcement learning algorithms. Our work applies DreamerV3's tricks to PPO and is the first such empirical study outside of the original work. Surprisingly, we find that the tricks presented do not transfer as general improvements to PPO. We use a high quality PPO reference implementation and present extensive ablation studies totaling over 10,000 A100 hours on the Arcade Learning Environment and the DeepMind Control Suite. Though our experiments demonstrate that these tricks do not generally outperform PPO, we identify cases where they succeed and offer insight into the relations
    
[^67]: 事件序列预测的交互扩散过程

    Interacting Diffusion Processes for Event Sequence Forecasting. (arXiv:2310.17800v1 [cs.LG])

    [http://arxiv.org/abs/2310.17800](http://arxiv.org/abs/2310.17800)

    本研究提出了一种基于扩散生成模型的交互扩散过程，用于事件序列预测。与之前的方法不同，该模型直接学习多个事件类型和两个事件之间的到达时间的联合概率分布，能够充分利用现代生成模型的高维建模能力。

    

    神经时间点过程（TPPs）已成为预测不规则时间间隔中发生的事件序列的主要框架，但其顺序性可能会影响长期预测的性能。为了解决这个问题，我们引入了一种新颖的方法，将扩散生成模型纳入其中。该模型实现了序列到序列的预测，根据历史事件序列进行多步预测。与之前的方法不同，我们的模型直接学习多个事件类型和两个事件之间的到达时间的联合概率分布。这使我们能够充分利用现代生成模型的高维建模能力。我们的模型由两个扩散过程组成，一个用于时间间隔，一个用于事件类型。这些过程通过各自的去噪函数进行交互，可以接受来自两个过程的中间表示作为输入，使模型能够学习复杂的交互关系。

    Neural Temporal Point Processes (TPPs) have emerged as the primary framework for predicting sequences of events that occur at irregular time intervals, but their sequential nature can hamper performance for long-horizon forecasts. To address this, we introduce a novel approach that incorporates a diffusion generative model. The model facilitates sequence-to-sequence prediction, allowing multi-step predictions based on historical event sequences. In contrast to previous approaches, our model directly learns the joint probability distribution of types and inter-arrival times for multiple events. This allows us to fully leverage the high dimensional modeling capability of modern generative models. Our model is composed of two diffusion processes, one for the time intervals and one for the event types. These processes interact through their respective denoising functions, which can take as input intermediate representations from both processes, allowing the model to learn complex interacti
    
[^68]: 神经应力场在降阶弹塑性和断裂中的应用

    Neural Stress Fields for Reduced-order Elastoplasticity and Fracture. (arXiv:2310.17790v1 [cs.GR])

    [http://arxiv.org/abs/2310.17790](http://arxiv.org/abs/2310.17790)

    本论文提出了一种神经网络和物理框架，用于降低弹塑性和断裂建模的计算需求。关键创新是通过训练低维神经应力场来实现在任意空间位置上高效计算应力值和内部力。此外，还训练了神经变形和仿射场来建立低维流形。

    

    我们提出了一种混合神经网络和物理框架用于降阶建模弹塑性和断裂。最先进的科学计算模型如材料点法 (MPM) 可以准确模拟大变形弹塑性和断裂力学。然而，它们的长运行时间和大内存消耗使其不适用于计算时间和内存使用受限的应用，例如虚拟现实。为克服这些障碍，我们提出了一种降阶框架。我们的关键创新是通过隐式神经表示训练一种基于Kirchhoff应力场的低维流形。这种低维神经应力场 (NSF) 可以高效地计算任意空间位置的应力值和对应的内部力。此外，我们还训练了神经变形和仿射场，为变形和仿射动量场建立低维流形。

    We propose a hybrid neural network and physics framework for reduced-order modeling of elastoplasticity and fracture. State-of-the-art scientific computing models like the Material Point Method (MPM) faithfully simulate large-deformation elastoplasticity and fracture mechanics. However, their long runtime and large memory consumption render them unsuitable for applications constrained by computation time and memory usage, e.g., virtual reality. To overcome these barriers, we propose a reduced-order framework. Our key innovation is training a low-dimensional manifold for the Kirchhoff stress field via an implicit neural representation. This low-dimensional neural stress field (NSF) enables efficient evaluations of stress values and, correspondingly, internal forces at arbitrary spatial locations. In addition, we also train neural deformation and affine fields to build low-dimensional manifolds for the deformation and affine momentum fields. These neural stress, deformation, and affine f
    
[^69]: 理解何时动力学不变的数据增强对模型无关的强化学习更新有益

    Understanding when Dynamics-Invariant Data Augmentations Benefit Model-Free Reinforcement Learning Updates. (arXiv:2310.17786v1 [cs.LG])

    [http://arxiv.org/abs/2310.17786](http://arxiv.org/abs/2310.17786)

    本文研究了在稀疏奖励任务中，动力学不变的数据增强函数对模型无关的强化学习更新的影响。实验结果表明，增加状态-动作覆盖率可以提高学习效果。

    

    最近，数据增强（DA）已经成为一种利用领域知识以低成本产生额外数据的强化学习（RL）任务的方法，往往能够显著提高数据效率。虽然之前的研究已经证明将增强数据直接纳入模型无关的RL更新中的效用，但目前还不太清楚特定的DA策略何时会提高数据效率。本文旨在找出DA的一般方面，以确定导致观察到的学习改进的因素。我们的研究集中在具有动力学不变的数据增强函数的稀疏奖励任务上，这是理解DA及其与RL训练整合的更一般的理解的一个初始步骤。实验上，我们分离了三个与DA相关的方面：状态-动作覆盖率，奖励密度和每次更新生成的增强转换的数量（增强回放率）。根据我们的实验，我们得出两个结论：(1) 增加状态-动作覆盖率可改进学习效果；

    Recently, data augmentation (DA) has emerged as a method for leveraging domain knowledge to inexpensively generate additional data in reinforcement learning (RL) tasks, often yielding substantial improvements in data efficiency. While prior work has demonstrated the utility of incorporating augmented data directly into model-free RL updates, it is not well-understood when a particular DA strategy will improve data efficiency. In this paper, we seek to identify general aspects of DA responsible for observed learning improvements. Our study focuses on sparse-reward tasks with dynamics-invariant data augmentation functions, serving as an initial step towards a more general understanding of DA and its integration into RL training. Experimentally, we isolate three relevant aspects of DA: state-action coverage, reward density, and the number of augmented transitions generated per update (the augmented replay ratio). From our experiments, we draw two conclusions: (1) increasing state-action c
    
[^70]: 使用参数化操作原语学习外在灵巧性

    Learning Extrinsic Dexterity with Parameterized Manipulation Primitives. (arXiv:2310.17785v1 [cs.RO])

    [http://arxiv.org/abs/2310.17785](http://arxiv.org/abs/2310.17785)

    论文通过学习一系列参数化操作原语，并利用环境改变物体姿态，解决了机器人在目标物体抓取被环境遮挡时的问题。

    

    许多实际相关的机器人抓取问题都涉及到目标物体，其所有抓取都被环境遮挡。在这种情况下，单次抓取计划总是失败的。因此，有必要首先将物体操作到一个适合进行抓取的配置。我们通过学习一系列利用环境改变物体姿态的动作来解决这个问题。具体而言，我们利用分层强化学习来结合一系列学习到的参数化操作原语。通过学习低级操作策略，我们的方法可以通过利用物体、夹具和环境之间的相互作用来控制物体的状态。在无控制条件下分析设计这样一个复杂行为是不可行的，因为分析方法需要准确建模互动和接触动力学。相反，我们学习一个在深度图像上直接操作的分层策略模型。

    Many practically relevant robot grasping problems feature a target object for which all grasps are occluded, e.g., by the environment. Single-shot grasp planning invariably fails in such scenarios. Instead, it is necessary to first manipulate the object into a configuration that affords a grasp. We solve this problem by learning a sequence of actions that utilize the environment to change the object's pose. Concretely, we employ hierarchical reinforcement learning to combine a sequence of learned parameterized manipulation primitives. By learning the low-level manipulation policies, our approach can control the object's state through exploiting interactions between the object, the gripper, and the environment. Designing such a complex behavior analytically would be infeasible under uncontrolled conditions, as an analytic approach requires accurate physical modeling of the interaction and contact dynamics. In contrast, we learn a hierarchical policy model that operates directly on depth
    
[^71]: 数据中心化的金融大型语言模型

    Data-Centric Financial Large Language Models. (arXiv:2310.17784v1 [cs.CL])

    [http://arxiv.org/abs/2310.17784](http://arxiv.org/abs/2310.17784)

    本文介绍了一种数据中心化的方法，通过预处理和预理解数据来改善大型语言模型（LLMs）在金融任务中的性能。实验证明，采用该方法的金融LLMs在金融分析和解释任务上达到了最先进的水平。

    

    大型语言模型（LLMs）在自然语言任务中表现出良好的潜力，但直接应用于复杂领域如金融时却遇到困难。LLMs难以推理和整合所有相关信息。我们提出了一种数据中心化的方法，使LLMs能够更好地处理金融任务。我们的关键观点是，不是一次性给LLM负载过多信息，而是更有效地对数据进行预处理和预理解。我们使用多任务基于提示的微调来创建金融LLM（FLLM），以实现数据预处理和预理解。然而，每个任务的标记数据有限。为了克服手动注释的成本，我们采用了自动生成训练数据的增强推理（AAR）来修改FLLM自身输出的伪标签。实验证明，我们的数据中心化FLLM与AAR相比，显著优于为原始文本设计的基线金融LLMs，在金融分析和解释任务上达到了最先进的水平。

    Large language models (LLMs) show promise for natural language tasks but struggle when applied directly to complex domains like finance. LLMs have difficulty reasoning about and integrating all relevant information. We propose a data-centric approach to enable LLMs to better handle financial tasks. Our key insight is that rather than overloading the LLM with everything at once, it is more effective to preprocess and pre-understand the data. We create a financial LLM (FLLM) using multitask prompt-based finetuning to achieve data pre-processing and pre-understanding. However, labeled data is scarce for each task. To overcome manual annotation costs, we employ abductive augmentation reasoning (AAR) to automatically generate training data by modifying the pseudo labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR substantially outperforms baseline financial LLMs designed for raw text, achieving state-of-the-art on financial analysis and interpretation tasks. We 
    
[^72]: 复杂交通情景分类的图卷积网络

    Graph Convolutional Networks for Complex Traffic Scenario Classification. (arXiv:2310.17773v1 [cs.CV])

    [http://arxiv.org/abs/2310.17773](http://arxiv.org/abs/2310.17773)

    该论文提出了一种复杂交通情景分类方法，能够模拟车辆与环境以及其他参与者的互动，并使用图卷积网络对这些情景的空间和时间特征进行建模。

    

    基于场景的测试方法可以减少获取自动驾驶系统安全性的统计显著证据所需的时间。自动化识别这些情景是一项具有挑战性的任务。大多数情景分类方法在复杂情景（高速公路，城市）和与其他交通参与者的互动方面效果不佳。现有方法模拟了车辆与其环境的关系，但忽略了多辆车之间的互动（例如，插入切换，静止前车）。此外，现有数据集缺乏多样性，并且没有每帧注释来准确学习情景的开始和结束时间。我们提出一种能够模拟车辆与环境以及其他参与者互动的复杂交通情景分类方法。我们使用图卷积网络来模拟这些情景的空间和时间特征。扩展t

    A scenario-based testing approach can reduce the time required to obtain statistically significant evidence of the safety of Automated Driving Systems (ADS). Identifying these scenarios in an automated manner is a challenging task. Most methods on scenario classification do not work for complex scenarios with diverse environments (highways, urban) and interaction with other traffic agents. This is mirrored in their approaches which model an individual vehicle in relation to its environment, but neglect the interaction between multiple vehicles (e.g. cut-ins, stationary lead vehicle). Furthermore, existing datasets lack diversity and do not have per-frame annotations to accurately learn the start and end time of a scenario. We propose a method for complex traffic scenario classification that is able to model the interaction of a vehicle with the environment, as well as other agents. We use Graph Convolutional Networks to model spatial and temporal aspects of these scenarios. Expanding t
    
[^73]: 学习对分布变化具有鲁棒性的最优分类树

    Learning Optimal Classification Trees Robust to Distribution Shifts. (arXiv:2310.17772v1 [cs.LG])

    [http://arxiv.org/abs/2310.17772](http://arxiv.org/abs/2310.17772)

    本研究提出了一种学习对分布变化具有鲁棒性的最优分类树的方法，通过混合整数鲁棒优化技术将该问题转化为单阶段混合整数鲁棒优化问题，并设计了基于约束生成的解决过程。

    

    我们考虑学习对训练和测试/部署数据之间的分布变化具有鲁棒性的分类树的问题。这个问题经常在高风险环境中出现，例如公共卫生和社会工作，其中数据通常是通过自我报告的调查收集的，这些调查对问题的表述方式、调查进行的时间和地点、以及受访者与调查员分享信息的舒适程度非常敏感。我们提出了一种基于混合整数鲁棒优化技术的学习最优鲁棒分类树的方法。特别地，我们证明学习最优鲁棒树的问题可以等价地表达为一个具有高度非线性和不连续目标的单阶段混合整数鲁棒优化问题。我们将这个问题等价地重新表述为一个两阶段线性鲁棒优化问题，为此我们设计了一个基于约束生成的定制解决过程。

    We consider the problem of learning classification trees that are robust to distribution shifts between training and testing/deployment data. This problem arises frequently in high stakes settings such as public health and social work where data is often collected using self-reported surveys which are highly sensitive to e.g., the framing of the questions, the time when and place where the survey is conducted, and the level of comfort the interviewee has in sharing information with the interviewer. We propose a method for learning optimal robust classification trees based on mixed-integer robust optimization technology. In particular, we demonstrate that the problem of learning an optimal robust tree can be cast as a single-stage mixed-integer robust optimization problem with a highly nonlinear and discontinuous objective. We reformulate this problem equivalently as a two-stage linear robust optimization problem for which we devise a tailored solution procedure based on constraint gene
    
[^74]: GROOViST:一种用于视觉故事中物体定位评估的度量标准

    GROOViST: A Metric for Grounding Objects in Visual Storytelling. (arXiv:2310.17770v1 [cs.AI])

    [http://arxiv.org/abs/2310.17770](http://arxiv.org/abs/2310.17770)

    GROOViST是一种用于评估视觉故事中物体定位的新的评估工具，考虑了跨模态依赖、时间错位和人类对视觉定位的直觉。这种工具具有模块化设计，可以评估和解释每个组件的贡献。

    

    对于一个由一系列图像生成的故事的适当评估，必须考虑多个方面，例如连贯性，语法正确性和视觉定位。在这项工作中，我们专注于评估定位程度，即故事与图像中显示的实体相关程度。我们分析了当前的评估指标，包括针对此目的设计的指标和针对一般视觉-文本对齐的指标。鉴于它们存在的缺点，我们提出了一种新的评估工具GROOViST，该工具考虑了跨模态依赖，时间错位（故事中实体出现的顺序和图像序列可能不匹配）以及人类对视觉定位的直觉。GROOViST的另一个优点是其模块化设计，可以对每个组件的贡献进行评估和解释。

    A proper evaluation of stories generated for a sequence of images -- the task commonly referred to as visual storytelling -- must consider multiple aspects, such as coherence, grammatical correctness, and visual grounding. In this work, we focus on evaluating the degree of grounding, that is, the extent to which a story is about the entities shown in the images. We analyze current metrics, both designed for this purpose and for general vision-text alignment. Given their observed shortcomings, we propose a novel evaluation tool, GROOViST, that accounts for cross-modal dependencies, temporal misalignments (the fact that the order in which entities appear in the story and the image sequence may not match), and human intuitions on visual grounding. An additional advantage of GROOViST is its modular design, where the contribution of each component can be assessed and interpreted individually.
    
[^75]: 分布式个性化经验风险最小化

    Distributed Personalized Empirical Risk Minimization. (arXiv:2310.17761v1 [cs.LG])

    [http://arxiv.org/abs/2310.17761](http://arxiv.org/abs/2310.17761)

    本文引入个性化经验风险最小化（PERM）的新范式，旨在实现不对参与设备共享的计算资源施加限制的情况下从异构数据源中进行学习，并通过估计数据分布之间的统计差异来个性化聚合本地经验损失，从而克服数据异构性问题。

    

    本文倡导了一种新的范式：个性化经验风险最小化（PERM），以便在不对参与设备共享的计算资源施加严格限制的情况下从异构数据源中进行学习。在PERM中，我们的目标是通过学习如何与谁学习，并通过有效估计数据分布之间的统计差异来个性化聚合本地经验损失，从而获得所有本地分布的最佳统计准确性并克服数据异构性问题。为了学习规模化的个性化模型，我们提出了一种分布式算法，它通过模型重排取代了标准的模型平均化，以同时优化所有设备的PERM目标。这还允许我们为不同的客户学习不同的模型架构（例如具有不同参数数量的神经网络），从而限制了各个客户的潜在内存和计算资源。

    This paper advocates a new paradigm Personalized Empirical Risk Minimization (PERM) to facilitate learning from heterogeneous data sources without imposing stringent constraints on computational resources shared by participating devices. In PERM, we aim to learn a distinct model for each client by learning who to learn with and personalizing the aggregation of local empirical losses by effectively estimating the statistical discrepancy among data distributions, which entails optimal statistical accuracy for all local distributions and overcomes the data heterogeneity issue. To learn personalized models at scale, we propose a distributed algorithm that replaces the standard model averaging with model shuffling to simultaneously optimize PERM objectives for all devices. This also allows us to learn distinct model architectures (e.g., neural networks with different numbers of parameters) for different clients, thus confining underlying memory and compute resources of individual clients. W
    
[^76]: 在凸优化中的算法可重现性和梯度复杂度的最优保证

    Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization. (arXiv:2310.17759v1 [cs.LG])

    [http://arxiv.org/abs/2310.17759](http://arxiv.org/abs/2310.17759)

    该论文研究了在凸优化中算法的可重现性和梯度复杂度问题。他们挑战了之前的观点，证明了对于平滑凸优化和平滑凸凹极小极大问题，可以实现最优的可重现性和接近最优的收敛保证。他们还证明了在不同的oracle设置下，与不同类型的oracle相匹配的算法达到了最优性。

    

    算法可重现性衡量了机器学习算法在训练过程中稍微改变时输出的偏差。之前的研究表明，一阶方法需要在收敛速度（梯度复杂度）和更好的可重现性之间做出权衡。在这项工作中，我们挑战了这种看法，并展示了在各种容易出错的oracle设置下，对于平滑凸优化和平滑凸凹极小极大问题，可以实现最优的可重现性和接近最优的收敛保证。尤其是，在不精确的初始化oracle给定情况下，我们基于正则化的算法实现了最优的可重现性和接近最优的梯度复杂度-对于最小化和最小最大优化。对于不精确的梯度oracle，接近最优的保证也适用于最小最大优化。此外，对于随机梯度oracle，我们证明了随机梯度下降上升在可重现性和收敛速度方面都是最优的。

    Algorithmic reproducibility measures the deviation in outputs of machine learning algorithms upon minor changes in the training process. Previous work suggests that first-order methods would need to trade-off convergence rate (gradient complexity) for better reproducibility. In this work, we challenge this perception and demonstrate that both optimal reproducibility and near-optimal convergence guarantees can be achieved for smooth convex minimization and smooth convex-concave minimax problems under various error-prone oracle settings. Particularly, given the inexact initialization oracle, our regularization-based algorithms achieve the best of both worlds optimal reproducibility and near-optimal gradient complexity - for minimization and minimax optimization. With the inexact gradient oracle, the near-optimal guarantees also hold for minimax optimization. Additionally, with the stochastic gradient oracle, we show that stochastic gradient descent ascent is optimal in terms of both re
    
[^77]: PockEngine: 稀疏且高效的边缘微调引擎

    PockEngine: Sparse and Efficient Fine-tuning in a Pocket. (arXiv:2310.17752v1 [cs.LG])

    [http://arxiv.org/abs/2310.17752](http://arxiv.org/abs/2310.17752)

    PockEngine是一种稀疏且高效的边缘微调引擎，支持稀疏反向传播和编译为先策略，以应对边缘设备的资源限制和硬件多样性。

    

    设备上的学习和高效微调能够实现持续且隐私保护的个性化定制（例如，在个性化数据上本地微调大型语言模型）。然而，现有的训练框架是为强大的云服务器设计的（例如，GPU、TPU等），缺乏针对边缘学习的优化，面临着资源有限和边缘硬件多样性的挑战。我们介绍了PockEngine：一种小型、稀疏且高效的引擎，可以在各种边缘设备上进行微调。PockEngine支持稀疏反向传播：它修剪反向图，并使用经过测量的内存节省和延迟降低来稀疏更新模型，同时保持模型质量。其次，PockEngine以编译为先：整个训练图（包括前向、反向和优化步骤）在编译时推导出来，从而降低了运行时开销，并带来了图变换的机会。PockEngine还整合了丰富的训练工具集。

    On-device learning and efficient fine-tuning enable continuous and privacy-preserving customization (e.g., locally fine-tuning large language models on personalized data). However, existing training frameworks are designed for cloud servers with powerful accelerators (e.g., GPUs, TPUs) and lack the optimizations for learning on the edge, which faces challenges of resource limitations and edge hardware diversity. We introduce PockEngine: a tiny, sparse and efficient engine to enable fine-tuning on various edge devices. PockEngine supports sparse backpropagation: it prunes the backward graph and sparsely updates the model with measured memory saving and latency reduction while maintaining the model quality. Secondly, PockEngine is compilation first: the entire training graph (including forward, backward and optimization steps) is derived at compile-time, which reduces the runtime overhead and brings opportunities for graph transformations. PockEngine also integrates a rich set of trainin
    
[^78]: 让最终用户成为基准测试的重点：OrionBench用于无监督时间序列异常检测

    Making the End-User a Priority in Benchmarking: OrionBench for Unsupervised Time Series Anomaly Detection. (arXiv:2310.17748v1 [cs.LG])

    [http://arxiv.org/abs/2310.17748](http://arxiv.org/abs/2310.17748)

    OrionBench是一个以用户为中心的无监督时间序列异常检测基准测试，提供了通用抽象、可扩展性和发布频繁的基准测试。

    

    时间序列异常检测是许多应用领域中的常见问题，例如医疗保健中的患者监测、金融中的预测或能源中的预测性维护。这导致了许多异常检测方法的出现，包括最近的基于深度学习的方法。虽然已经提出了几种用于比较新开发模型的基准测试，但它们通常依赖于对有限数据集的一次性执行，并且比较仅限于少数模型。我们提出了OrionBench——一个以用户为中心、持续维护的无监督时间序列异常检测基准测试。该框架提供了用于表示模型的通用抽象、添加新的流水线和数据集的可扩展性、超参数标准化、流水线验证以及发布基准测试的频繁版本。我们展示了OrionBench的用法，并展示了在三年时间内发布的15个版本中流水线的演化过程。

    Time series anomaly detection is a prevalent problem in many application domains such as patient monitoring in healthcare, forecasting in finance, or predictive maintenance in energy. This has led to the emergence of a plethora of anomaly detection methods, including more recently, deep learning based methods. Although several benchmarks have been proposed to compare newly developed models, they usually rely on one-time execution over a limited set of datasets and the comparison is restricted to a few models. We propose OrionBench -- a user centric continuously maintained benchmark for unsupervised time series anomaly detection. The framework provides universal abstractions to represent models, extensibility to add new pipelines and datasets, hyperparameter standardization, pipeline verification, and frequent releases with published benchmarks. We demonstrate the usage of OrionBench, and the progression of pipelines across 15 releases published over the course of three years. Moreover,
    
[^79]: BERT-PIN: 一种基于BERT的框架来恢复时间序列负载曲线中的缺失数据段

    BERT-PIN: A BERT-based Framework for Recovering Missing Data Segments in Time-series Load Profiles. (arXiv:2310.17742v1 [eess.AS])

    [http://arxiv.org/abs/2310.17742](http://arxiv.org/abs/2310.17742)

    BERT-PIN是一种基于BERT的框架，用于恢复时间序列负载曲线中的缺失数据段。它使用负载和温度时间序列曲线作为输入，采用Transformer模型结构进行曲线修复，并通过候选者选择过程生成多个可信度不同的可能的修复数据集。在多个MDS恢复和需求响应基线估计等应用中，BERT-PIN表现出优越性能。

    

    受到Transformer模型在自然语言处理和计算机视觉中的成功启发，本文引入了BERT-PIN，一种基于双向编码器转换器（BERT）的Profile Inpainting Network。BERT-PIN使用负载和温度时间序列曲线作为输入来恢复多个缺失数据段（MDSs）。为了采用标准的Transformer模型结构进行曲线修复，我们将负载和温度曲线分割为直线段，将每个段作为一个词，整个曲线作为一个句子。我们在BERT-PIN中引入了一个候选者选择过程，使其能够产生一系列概率分布，用户可以根据这些概率分布生成多个可信度不同的可能的修复数据集。我们使用真实数据集开发和评估了BERT-PIN，应用于两个场景：多个MDS的恢复和需求响应基线估计。仿真结果表明，BERT-PIN优于以前的方法。

    Inspired by the success of the Transformer model in natural language processing and computer vision, this paper introduces BERT-PIN, a Bidirectional Encoder Representations from Transformers (BERT) powered Profile Inpainting Network. BERT-PIN recovers multiple missing data segments (MDSs) using load and temperature time-series profiles as inputs. To adopt a standard Transformer model structure for profile inpainting, we segment the load and temperature profiles into line segments, treating each segment as a word and the entire profile as a sentence. We incorporate a top candidates selection process in BERT-PIN, enabling it to produce a sequence of probability distributions, based on which users can generate multiple plausible imputed data sets, each reflecting different confidence levels. We develop and evaluate BERT-PIN using real-world dataset for two applications: multiple MDSs recovery and demand response baseline estimation. Simulation results show that BERT-PIN outperforms the ex
    
[^80]: GNN-GMVO: 用于优化相似商品推荐中的商品总交易价值的图神经网络

    GNN-GMVO: Graph Neural Networks for Optimizing Gross Merchandise Value in Similar Item Recommendation. (arXiv:2310.17732v1 [cs.IR])

    [http://arxiv.org/abs/2310.17732](http://arxiv.org/abs/2310.17732)

    这项研究设计了一种名为GNN-GMVO的新型图神经网络架构，用于优化电子商务中相似商品推荐的商品总交易价值（GMV）。它解决了传统GNN架构在优化收入相关目标方面的不足，并通过直接优化GMV来保证推荐质量。

    

    相似商品推荐是电子商务行业中的关键任务，它帮助客户基于他们感兴趣的产品探索相似和相关的替代品。尽管传统的机器学习模型，图神经网络（GNN）可以理解产品之间的复杂关系，如相似性。然而，与它们在检索任务中的广泛应用和优化相关性的重点相反，当前的GNN架构并未针对最大化与收入相关的目标（如商品总交易价值（GMV））进行设计，而GMV是电子商务公司的主要业务指标之一。此外，在大规模电子商务系统中定义准确的边关系对于GNN来说是非常复杂的，因为商品之间的关系具有异质性。本研究旨在通过设计一种称为GNN-GMVO的新型GNN架构来解决这些问题。该模型直接优化GMV，同时保证推荐质量。

    Similar item recommendation is a critical task in the e-Commerce industry, which helps customers explore similar and relevant alternatives based on their interested products. Despite the traditional machine learning models, Graph Neural Networks (GNNs), by design, can understand complex relations like similarity between products. However, in contrast to their wide usage in retrieval tasks and their focus on optimizing the relevance, the current GNN architectures are not tailored toward maximizing revenue-related objectives such as Gross Merchandise Value (GMV), which is one of the major business metrics for e-Commerce companies. In addition, defining accurate edge relations in GNNs is non-trivial in large-scale e-Commerce systems, due to the heterogeneity nature of the item-item relationships. This work aims to address these issues by designing a new GNN architecture called GNN-GMVO (Graph Neural Network - Gross Merchandise Value Optimizer). This model directly optimizes GMV while cons
    
[^81]: 使用门控图神经网络改进智能交通系统中的交通密度预测

    Improving Traffic Density Forecasting in Intelligent Transportation Systems Using Gated Graph Neural Networks. (arXiv:2310.17729v1 [cs.LG])

    [http://arxiv.org/abs/2310.17729](http://arxiv.org/abs/2310.17729)

    本研究通过应用门控图神经网络（GGNNs），在智能交通系统中的交通预测领域取得了显著的成果，通过最小化预测误差，GGNNs被证明是最有效的选择，展示了较高的预测性能。

    

    本研究探讨了图神经网络在交通预测领域中的应用，这是智能交通系统中的一个关键方面。准确的交通预测对于行程规划、交通控制和车辆路径规划等功能至关重要。在交通预测的背景下，深入研究了三种主要的GNN架构：图卷积网络（图采样和聚合）和门控图神经网络。详细考察了每种架构的方法论，包括层配置、激活函数和超参数。主要目标是降低预测误差，在三种模型中，GGNNs被证明是最有效的选择。研究概述了每种架构的结果，通过均方根误差和平均绝对误差（MAE）阐明了它们的预测性能。假设的结果揭示了有趣的见解：GCNs显示了9.10的RMSE和8.00的MAE，而GraphSAGE展示了令人印象深刻的预测能力。

    This study delves into the application of graph neural networks in the realm of traffic forecasting, a crucial facet of intelligent transportation systems. Accurate traffic predictions are vital for functions like trip planning, traffic control, and vehicle routing in such systems. Three prominent GNN architectures Graph Convolutional Networks (Graph Sample and Aggregation) and Gated Graph Neural Networks are explored within the context of traffic prediction. Each architecture's methodology is thoroughly examined, including layer configurations, activation functions,and hyperparameters. The primary goal is to minimize prediction errors, with GGNNs emerging as the most effective choice among the three models. The research outlines outcomes for each architecture, elucidating their predictive performance through root mean squared error and mean absolute error (MAE). Hypothetical results reveal intriguing insights: GCNs display an RMSE of 9.10 and an MAE of 8.00, while GraphSAGE shows impr
    
[^82]: ZeroQuant-HERO: W8A8 Transformer的硬件增强的优化后训练量化框架

    ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers. (arXiv:2310.17723v1 [cs.LG])

    [http://arxiv.org/abs/2310.17723](http://arxiv.org/abs/2310.17723)

    ZeroQuant-HERO是一种硬件增强的量化框架，针对W8A8 Transformer模型进行优化，旨在减少内存和计算需求，并在处理复杂的量化问题和内存受限运算符方面提供了新的解决方案。同时，该框架还具有灵活性，允许特定模块切换至FP16/BF16模式以提高准确性。

    

    量化技术在减少深度神经网络推理的内存和计算需求方面起着关键作用。现有的解决方案，如ZeroQuant，为BERT和GPT等模型提供了动态量化，但忽视了关键的内存受限运算符和每个标记的量化复杂性。针对这些差距，我们提出了一种全新的、完全由硬件增强的、经过优化的、后训练W8A8量化框架ZeroQuant-HERO。该框架独特地集成了内存带宽和计算密集型运算符，旨在实现最佳硬件性能。此外，它通过允许特定的INT8模块切换到FP16/BF16模式来提高准确性。

    Quantization techniques are pivotal in reducing the memory and computational demands of deep neural network inference. Existing solutions, such as ZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook crucial memory-bounded operators and the complexities of per-token quantization. Addressing these gaps, we present a novel, fully hardware-enhanced robust optimized post-training W8A8 quantization framework, ZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and compute-intensive operators, aiming for optimal hardware performance. Additionally, it offers flexibility by allowing specific INT8 modules to switch to FP16/BF16 mode, enhancing accuracy.
    
[^83]: 大型语言模型作为具有普适性的机器人任务策略

    Large Language Models as Generalizable Policies for Embodied Tasks. (arXiv:2310.17722v1 [cs.LG])

    [http://arxiv.org/abs/2310.17722](http://arxiv.org/abs/2310.17722)

    本研究展示了大型语言模型(LLMs)可以被适应为具有普适性的机器人任务策略。通过我们的方法LLaRP，我们成功将预训练的冻结LLM用于接收指令和视觉输入，并在环境中直接输出动作。我们的实验结果表明LLaRP不仅对任务指令的复杂改写具有鲁棒性，而且可以推广到需要新颖最优行为的新任务。在大量未见任务中，LLaRP表现出了显著的成功率提升，并且我们提供了一个新的基准测试(Language Rearrangement)来促进进一步的研究。

    

    我们展示了大型语言模型(LLMs)可以被调整为适用于机器人视觉任务的普适性策略。我们的方法被称为大型语言模型强化学习策略(LLaRP)，它将预训练的冻结的LLM调整为接收文本指令和视觉自我中心观测作为输入，并直接在环境中输出动作。通过强化学习，我们训练LLaRP通过与环境的交互来看和行动。我们展示了LLaRP对任务指令的复杂改写具有鲁棒性，并且可以推广到需要新颖最优行为的新任务。特别地，在1,000个未见任务中，它的成功率达到了42%，是其他常见学习基线或零样本应用的1.7倍成功率。最后，为了帮助社区研究以语言为条件的、大规模多任务的机器人AI问题，我们发布了一个新的基准测试(Language Rearrangement)，包括150,000个训练任务和1,000个测试任务，用于语言为条件的重新排列。

    We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangem
    
[^84]: 推进脑肿瘤检测: 对MRI图像中的CNN、聚类和SoftMax分类进行彻底研究

    Advancing Brain Tumor Detection: A Thorough Investigation of CNNs, Clustering, and SoftMax Classification in the Analysis of MRI Images. (arXiv:2310.17720v1 [eess.IV])

    [http://arxiv.org/abs/2310.17720](http://arxiv.org/abs/2310.17720)

    本研究详细研究了使用卷积神经网络（CNN）结合MRI图像进行脑肿瘤检测的方法，通过引入SoftMax分类器以及聚类方法，达到了高准确率。研究结果对脑肿瘤的早期诊断和治疗具有重要意义。

    

    脑肿瘤由于高发病率和全年龄组的高死亡率，是全球面临的重大健康挑战。及早发现脑肿瘤对于有效治疗和患者的预后至关重要。本研究通过使用磁共振成像（MRI）图像对卷积神经网络（CNN）进行详尽研究，以实现脑肿瘤的检测。将包含健康人和患有脑肿瘤的患者的MRI扫描数据集进行处理，并输入到CNN架构中。采用SoftMax全连接层对图像进行分类，达到了98%的准确率。为了评估CNN的性能，还使用了其他两种分类器，径向基函数（RBF）和决策树（DT），分别达到了98.24%和95.64%的准确率。本研究还引入了一种聚类方法进行特征提取，提高了CNN的准确率。在准确率之外，还采用了敏感度、特异度和精确度等指标进行评估。

    Brain tumors pose a significant global health challenge due to their high prevalence and mortality rates across all age groups. Detecting brain tumors at an early stage is crucial for effective treatment and patient outcomes. This study presents a comprehensive investigation into the use of Convolutional Neural Networks (CNNs) for brain tumor detection using Magnetic Resonance Imaging (MRI) images. The dataset, consisting of MRI scans from both healthy individuals and patients with brain tumors, was processed and fed into the CNN architecture. The SoftMax Fully Connected layer was employed to classify the images, achieving an accuracy of 98%. To evaluate the CNN's performance, two other classifiers, Radial Basis Function (RBF) and Decision Tree (DT), were utilized, yielding accuracy rates of 98.24% and 95.64%, respectively. The study also introduced a clustering method for feature extraction, improving CNN's accuracy. Sensitivity, Specificity, and Precision were employed alongside accu
    
[^85]: 统一（量子）统计和参数化（量子）算法

    Unifying (Quantum) Statistical and Parametrized (Quantum) Algorithms. (arXiv:2310.17716v1 [quant-ph])

    [http://arxiv.org/abs/2310.17716](http://arxiv.org/abs/2310.17716)

    本文提出了一种统一的视角，将统计和参数化学习范例相结合，探索了从评估oracle中学习的问题，并提供了无条件的下界和查询复杂度刻画。该框架适用于QSQ设置和基于损失函数优化的算法。

    

    Kearns的统计查询（SQ）oracle（STOC'93）为大多数经典机器学习算法提供了统一的视角。然而在量子学习中这一点不再成立，因为许多设置既没有SQ的类比，也没有量子统计查询（QSQ）的类比。在本文中，我们借鉴了Kearns的SQ oracle和Valiant的弱评估oracle（TOCT'14），通过一种新颖的方式建立了统计和参数化学习范例之间的统一视角。我们探索了从评估oracle中学习的问题，该oracle提供了函数值的估计，并引入了一个广泛而直观的框架，为从评估查询中学习提供了无条件的下界，并且对于学习线性函数类的查询复杂度进行了刻画。该框架直接适用于QSQ设置以及所有基于损失函数优化的算法。我们的第一个应用是扩展先前关于输出可学性的结果。

    Kearns' statistical query (SQ) oracle (STOC'93) lends a unifying perspective for most classical machine learning algorithms. This ceases to be true in quantum learning, where many settings do not admit, neither an SQ analog nor a quantum statistical query (QSQ) analog. In this work, we take inspiration from Kearns' SQ oracle and Valiant's weak evaluation oracle (TOCT'14) and establish a unified perspective bridging the statistical and parametrized learning paradigms in a novel way. We explore the problem of learning from an evaluation oracle, which provides an estimate of function values, and introduce an extensive yet intuitive framework that yields unconditional lower bounds for learning from evaluation queries and characterizes the query complexity for learning linear function classes. The framework is directly applicable to the QSQ setting and virtually all algorithms based on loss function optimization.  Our first application is to extend prior results on the learnability of outpu
    
[^86]: 使用Node2Vec学习到的嵌入进行社区检测和分类的保证

    Community Detection and Classification Guarantees Using Embeddings Learned by Node2Vec. (arXiv:2310.17712v1 [stat.ML])

    [http://arxiv.org/abs/2310.17712](http://arxiv.org/abs/2310.17712)

    本研究通过分析Node2Vec学习到的嵌入的理论属性，证明了在（经过度修正的）随机块模型中，使用k-means聚类方法对这些嵌入进行社区恢复是弱一致的。实验证明这一结果，并探讨了嵌入在节点和链接预测任务中的应用。

    

    将大型网络的节点嵌入到欧几里得空间中是现代机器学习中的常见目标，有各种工具可用。这些嵌入可以用作社区检测/节点聚类或链接预测等任务的特征，其性能达到了最先进水平。除了谱聚类方法之外，对于其他常用的学习嵌入方法，缺乏理论上的理解。在这项工作中，我们考察了由node2vec学习到的嵌入的理论属性。我们的主要结果表明，对node2vec生成的嵌入向量应用k-means聚类可以对（经过度修正的）随机块模型中的节点进行弱一致的社区恢复。我们还讨论了这些嵌入在节点和链接预测任务中的应用。我们通过实验证明了这个结果，并研究了它与网络数据的其他嵌入工具之间的关系。

    Embedding the nodes of a large network into an Euclidean space is a common objective in modern machine learning, with a variety of tools available. These embeddings can then be used as features for tasks such as community detection/node clustering or link prediction, where they achieve state of the art performance. With the exception of spectral clustering methods, there is little theoretical understanding for other commonly used approaches to learning embeddings. In this work we examine the theoretical properties of the embeddings learned by node2vec. Our main result shows that the use of k-means clustering on the embedding vectors produced by node2vec gives weakly consistent community recovery for the nodes in (degree corrected) stochastic block models. We also discuss the use of these embeddings for node and link prediction tasks. We demonstrate this result empirically, and examine how this relates to other embedding tools for network data.
    
[^87]: 一种由语义通信增强的无线AI生成内容（AIGC）供应框架

    A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered by Semantic Communication. (arXiv:2310.17705v1 [cs.NI])

    [http://arxiv.org/abs/2310.17705](http://arxiv.org/abs/2310.17705)

    一种由语义通信增强的无线AI生成内容（AIGC）供应框架，通过使用语义信息而不是所有的二进制位提取和传输内容，以解决在无线网络中提供最优AIGC服务的挑战。

    

    近期，生成式AI应用通过创建多样化且高质量的AI生成内容（AIGC）来满足广大用户群体的需求。随着移动设备的普及和移动流量的快速增长，通过无线通信网络提供对高质量AIGC服务的无处不在的访问已成为AIGC产品的未来方向。然而，在不稳定的信道、有限的带宽资源和分布不均匀的计算资源的无线网络中提供最优的AIGC服务是具有挑战性的。为了解决这些挑战，我们提出了一个由语义通信（SemCom）增强的AIGC（SemAIGC）生成和传输框架，其中只需提取和传输内容的语义信息而不是所有的二进制位。具体而言，SemAIGC在语义编码器和解码器中集成了基于扩散的模型，以实现高效的内容生成和灵活调整计算工作负载的目的。

    Generative AI applications are recently catering to a vast user base by creating diverse and high-quality AI-generated content (AIGC). With the proliferation of mobile devices and rapid growth of mobile traffic, providing ubiquitous access to high-quality AIGC services via wireless communication networks is becoming the future direction for AIGC products. However, it is challenging to provide optimal AIGC services in wireless networks with unstable channels, limited bandwidth resources, and unevenly distributed computational resources. To tackle these challenges, we propose a semantic communication (SemCom)-empowered AIGC (SemAIGC) generation and transmission framework, where only semantic information of the content rather than all the binary bits should be extracted and transmitted by using SemCom. Specifically, SemAIGC integrates diffusion-based models within the semantic encoder and decoder for efficient content generation and flexible adjustment of the computing workload of both tr
    
[^88]: 使用生成对抗网络进行预测的对照因果公平性

    Counterfactual Fairness for Predictions using Generative Adversarial Networks. (arXiv:2310.17687v1 [cs.LG])

    [http://arxiv.org/abs/2310.17687](http://arxiv.org/abs/2310.17687)

    这篇论文提出了一种使用生成对抗网络实现对照因果公平性的方法，通过学习敏感属性的后代的对照分布来确保公平预测。

    

    由于法律、伦理和社会原因，预测中的公平性在实践中非常重要。通常通过对照因果公平性来实现，该公平性确保个体的预测与在不同敏感属性下的对照世界中的预测相同。然而，要实现对照因果公平性是具有挑战性的，因为对照是不可观察的。在本文中，我们开发了一种新颖的深度神经网络，称为对照因果公平性生成对抗网络（GCFN），用于在对照因果公平性下进行预测。具体而言，我们利用一个量身定制的生成对抗网络直接学习敏感属性的后代的对照分布，然后通过一种新颖的对照媒介正则化来实施公平预测。如果对照分布学习得足够好，我们的方法在数学上确保对照因果公平性的概念。因此，我们的GCFN解决了对照因果公平性问题。

    Fairness in predictions is of direct importance in practice due to legal, ethical, and societal reasons. It is often achieved through counterfactual fairness, which ensures that the prediction for an individual is the same as that in a counterfactual world under a different sensitive attribute. However, achieving counterfactual fairness is challenging as counterfactuals are unobservable. In this paper, we develop a novel deep neural network called Generative Counterfactual Fairness Network (GCFN) for making predictions under counterfactual fairness. Specifically, we leverage a tailored generative adversarial network to directly learn the counterfactual distribution of the descendants of the sensitive attribute, which we then use to enforce fair predictions through a novel counterfactual mediator regularization. If the counterfactual distribution is learned sufficiently well, our method is mathematically guaranteed to ensure the notion of counterfactual fairness. Thereby, our GCFN addre
    
[^89]: Sliceformer: 将多头注意力机制简化为排序在判别任务中的应用

    Sliceformer: Make Multi-head Attention as Simple as Sorting in Discriminative Tasks. (arXiv:2310.17683v1 [cs.LG])

    [http://arxiv.org/abs/2310.17683](http://arxiv.org/abs/2310.17683)

    Sliceformer 提出了一个高效且有效的 Transformer 替代方案，通过一个简单的“切片-排序”操作替代了传统的多头注意力机制，解决了其高计算复杂性和数值问题。

    

    作为最流行的神经网络模块之一，Transformer在许多基础的深度学习模型中起着核心作用，例如在计算机视觉中的ViT和在自然语言处理中的BERT和GPT。Transformer的有效性通常归因于其多头注意力机制。在本研究中，我们讨论了MHA的局限性，包括由于其“查询-键-值”架构而导致的高计算复杂性以及由其softmax操作引起的数值问题。考虑到上述问题和注意力层的最近发展趋势，我们提出了一个高效且有效的Transformer替代方案，称为Sliceformer。我们的Sliceformer用一个极其简单的“切片-排序”操作替代了经典的MHA机制，即将输入线性投影到潜空间，并沿着不同的特征维度（或等价地称为通道）进行排序。对于每个特征维度，排序操作隐含地生成了隐藏的参数化多头注意力机制。

    As one of the most popular neural network modules, Transformer plays a central role in many fundamental deep learning models, e.g., the ViT in computer vision and the BERT and GPT in natural language processing. The effectiveness of the Transformer is often attributed to its multi-head attention (MHA) mechanism. In this study, we discuss the limitations of MHA, including the high computational complexity due to its ``query-key-value'' architecture and the numerical issue caused by its softmax operation. Considering the above problems and the recent development tendency of the attention layer, we propose an effective and efficient surrogate of the Transformer, called Sliceformer. Our Sliceformer replaces the classic MHA mechanism with an extremely simple ``slicing-sorting'' operation, i.e., projecting inputs linearly to a latent space and sorting them along different feature dimensions (or equivalently, called channels). For each feature dimension, the sorting operation implicitly gener
    
[^90]: 机器学习在行星科学数据集中的特征提取与分类

    Feature Extraction and Classification from Planetary Science Datasets enabled by Machine Learning. (arXiv:2310.17681v1 [astro-ph.EP])

    [http://arxiv.org/abs/2310.17681](http://arxiv.org/abs/2310.17681)

    本论文提出了两个应用机器学习进行特征识别的例子，一个是识别木卫二混乱区域的冰块，另一个是识别泰坦上的云。通过迁移学习和新数据测试，分别实现了68%和95%的精度，并提出了进一步改进的建议。

    

    本文中，我们提出了两个我们最近进行的研究的例子，应用机器学习神经网络对外行星任务的图像数据集进行特征识别。我们的第一个研究是在木卫二的混乱区域中识别冰块（也被称为漂浮块、板块、多边形）。我们采用了迁移学习的方法，对行业标准的Mask R-CNN（基于区域的卷积神经网络）添加和训练新的层次，以识别训练数据集中标记的块。随后，更新后的模型被测试于新的数据集，实现了68%的精度。在不同的应用中，我们将Mask R-CNN应用于泰坦上云的识别，同样通过更新的训练和对新数据的测试，获得了369幅图像上95%的精度。我们评估了我们的技术相对成功的程度，并提出了如何进一步改进训练和识别的建议。我们使用了新的方法来处理行星科学数据集，通过机器学习算法进行特征提取和分类。

    In this paper we present two examples of recent investigations that we have undertaken, applying Machine Learning (ML) neural networks (NN) to image datasets from outer planet missions to achieve feature recognition. Our first investigation was to recognize ice blocks (also known as rafts, plates, polygons) in the chaos regions of fractured ice on Europa. We used a transfer learning approach, adding and training new layers to an industry-standard Mask R-CNN (Region-based Convolutional Neural Network) to recognize labeled blocks in a training dataset. Subsequently, the updated model was tested against a new dataset, achieving 68% precision. In a different application, we applied the Mask R-CNN to recognize clouds on Titan, again through updated training followed by testing against new data, with a precision of 95% over 369 images. We evaluate the relative successes of our techniques and suggest how training and recognition could be further improved. The new approaches we have used for p
    
[^91]: 使用最佳顺序分数搜索和生长-收缩树快速扩展的DAG发现方法

    Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score Search and Grow-Shrink Trees. (arXiv:2310.17679v1 [cs.LG])

    [http://arxiv.org/abs/2310.17679](http://arxiv.org/abs/2310.17679)

    本论文介绍了一种用于学习有向无环图的最佳顺序分数搜索（BOSS）和生长-收缩树（GSTs）方法，该方法在准确性和执行时间方面达到了最先进的性能，适用于具有数百个高度连接的变量的问题，例如从fMRI数据中恢复脑网络。

    

    学习图形条件独立结构是一个重要的机器学习问题，也是因果发现的基石。然而，学习算法的准确性和执行时间通常难以适应具有数百个高度连接的变量的问题，例如从fMRI数据中恢复脑网络。我们介绍了最佳顺序分数搜索（BOSS）和生长-收缩树（GSTs）用于在这个范例中学习有向无环图（DAGs）。BOSS贪婪地搜索变量的排列，使用GSTs从排列构建和评分DAGs。GSTs有效地缓存分数以消除冗余计算。BOSS在准确性和执行时间方面达到了最先进的性能，在广泛的条件下与各种组合和基于梯度的学习算法进行了有利的比较。为了证明它的实用性，我们将BOSS应用于两组静息态fMRI数据：带有伪经验噪声分布的模拟数据

    Learning graphical conditional independence structures is an important machine learning problem and a cornerstone of causal discovery. However, the accuracy and execution time of learning algorithms generally struggle to scale to problems with hundreds of highly connected variables -- for instance, recovering brain networks from fMRI data. We introduce the best order score search (BOSS) and grow-shrink trees (GSTs) for learning directed acyclic graphs (DAGs) in this paradigm. BOSS greedily searches over permutations of variables, using GSTs to construct and score DAGs from permutations. GSTs efficiently cache scores to eliminate redundant calculations. BOSS achieves state-of-the-art performance in accuracy and execution time, comparing favorably to a variety of combinatorial and gradient-based learning algorithms under a broad range of conditions. To demonstrate its practicality, we apply BOSS to two sets of resting-state fMRI data: simulated data with pseudo-empirical noise distributi
    
[^92]: 时空元对比学习

    Spatio-Temporal Meta Contrastive Learning. (arXiv:2310.17678v1 [cs.LG])

    [http://arxiv.org/abs/2310.17678](http://arxiv.org/abs/2310.17678)

    该论文介绍了一个新的时空对比学习（CL4ST）框架，通过解决数据质量问题和个性化数据增强策略的挑战，提高了时空图神经网络的性能。

    

    时空预测在包括交通预测和犯罪预测在内的多个现实应用中至关重要，旨在提高公共交通和安全管理。许多最先进的模型展示了时空图神经网络（STGNN）捕捉复杂时空相关性的强大能力。然而，尽管它们的有效性，现有方法没有充分解决一些关键问题。数据质量问题，如数据稀缺和稀疏性，导致数据噪音和缺乏监督信号，极大限制了STGNN的性能。虽然近期引入了对比学习的STGNN模型旨在解决这些挑战，但大多数模型使用预定义的数据增强策略，过于依赖于手动设计，不能针对不同的时空图（STG）场景进行定制。为了解决这些挑战，我们提出了一种新的时空对比学习（CL4ST）框架来编码健壮且

    Spatio-temporal prediction is crucial in numerous real-world applications, including traffic forecasting and crime prediction, which aim to improve public transportation and safety management. Many state-of-the-art models demonstrate the strong capability of spatio-temporal graph neural networks (STGNN) to capture complex spatio-temporal correlations. However, despite their effectiveness, existing approaches do not adequately address several key challenges. Data quality issues, such as data scarcity and sparsity, lead to data noise and a lack of supervised signals, which significantly limit the performance of STGNN. Although recent STGNN models with contrastive learning aim to address these challenges, most of them use pre-defined augmentation strategies that heavily depend on manual design and cannot be customized for different Spatio-Temporal Graph (STG) scenarios. To tackle these challenges, we propose a new spatio-temporal contrastive learning (CL4ST) framework to encode robust and
    
[^93]: 机器学习咳嗽声音分析早期检测结核病：朝着更易于全球分类使用的方向

    Early Detection of Tuberculosis with Machine Learning Cough Audio Analysis: Towards More Accessible Global Triaging Usage. (arXiv:2310.17675v1 [eess.AS])

    [http://arxiv.org/abs/2310.17675](http://arxiv.org/abs/2310.17675)

    这项研究开发了一种利用机器学习分析智能手机麦克风中的咳嗽声音来检测结核病的集成模型，可以实现早期诊断和监测治疗进展，从而为全球结核病的分类使用提供更易于获取的方法。

    

    结核病（TB）是一种主要影响肺部的细菌性疾病，是全球死亡率最高的传染病之一。及时有效的抗结核治疗对于防止结核病在体内传播，避免致命并发症至关重要。咳嗽作为结核病的客观生物标志物，是一种监测治疗反应并随着成功治疗而减少的分类工具。目前用于结核病诊断的黄金标准方法缓慢且不易获取，尤其在乡村地区结核病的流行程度最高。此外，目前的机器学习诊断研究，如利用胸部X光片，无法有效监测治疗进展。为了实现有效的诊断，我们开发了一个集成模型，使用了一种新的机器学习架构，从智能手机的麦克风中分析咳嗽声音的流行病学特征来检测结核病。该架构包括一个2D-CNN和XGBoost，使用了来自7个国家的724,964个咳嗽声音样本和人口统计学数据进行训练。

    Tuberculosis (TB), a bacterial disease mainly affecting the lungs, is one of the leading infectious causes of mortality worldwide. To prevent TB from spreading within the body, which causes life-threatening complications, timely and effective anti-TB treatment is crucial. Cough, an objective biomarker for TB, is a triage tool that monitors treatment response and regresses with successful therapy. Current gold standards for TB diagnosis are slow or inaccessible, especially in rural areas where TB is most prevalent. In addition, current machine learning (ML) diagnosis research, like utilizing chest radiographs, is ineffective and does not monitor treatment progression. To enable effective diagnosis, an ensemble model was developed that analyzes, using a novel ML architecture, coughs' acoustic epidemiologies from smartphones' microphones to detect TB. The architecture includes a 2D-CNN and XGBoost that was trained on 724,964 cough audio samples and demographics from 7 countries. After fea
    
[^94]: 将基于强化学习的控制器从模型传递到硬件在环中的研究

    Transfer of Reinforcement Learning-Based Controllers from Model- to Hardware-in-the-Loop. (arXiv:2310.17671v1 [cs.LG])

    [http://arxiv.org/abs/2310.17671](http://arxiv.org/abs/2310.17671)

    本研究通过结合迁移学习和环境中的模拟来加速强化学习代理的训练过程，以实现在嵌入式系统中有效使用强化学习的目标。

    

    开发嵌入式系统的控制功能是资源、时间和数据密集型的过程，经常导致次优的成本和解决方法。强化学习（RL）具有在最小人为干预下自动训练代理执行复杂控制任务的潜力。然而，由于数据生成的成本和安全约束，其应用大多限于纯粹的模拟领域。为了有效地在嵌入式系统功能开发中使用RL，生成的代理必须能够处理真实世界的应用。在这个背景下，本研究通过结合迁移学习（TL）和环境中的模型（XiL）模拟来加速RL代理的训练过程。对于内燃机的瞬态废气再循环控制案例，使用计算成本较低的模型在环（MiL）模拟来选择合适的算法，微调超参数，最后训练候选代理。

    The process of developing control functions for embedded systems is resource-, time-, and data-intensive, often resulting in sub-optimal cost and solutions approaches. Reinforcement Learning (RL) has great potential for autonomously training agents to perform complex control tasks with minimal human intervention. Due to costly data generation and safety constraints, however, its application is mostly limited to purely simulated domains. To use RL effectively in embedded system function development, the generated agents must be able to handle real-world applications. In this context, this work focuses on accelerating the training process of RL agents by combining Transfer Learning (TL) and X-in-the-Loop (XiL) simulation. For the use case of transient exhaust gas re-circulation control for an internal combustion engine, use of a computationally cheap Model-in-the-Loop (MiL) simulation is made to select a suitable algorithm, fine-tune hyperparameters, and finally train candidate agents fo
    
[^95]: 使用基于集体决策的深度学习网络识别未知健康状态在预测维护应用中 (arXiv:2310.17670v1 [cs.LG])

    Unknown Health States Recognition With Collective Decision Based Deep Learning Networks In Predictive Maintenance Applications. (arXiv:2310.17670v1 [cs.LG])

    [http://arxiv.org/abs/2310.17670](http://arxiv.org/abs/2310.17670)

    本论文提出了一种基于集体决策的深度学习网络框架，在预测维护应用中通过同时处理已知和未知健康状态，解决了常规CNN方案无法处理新异常样本的问题。

    

    在计算能力的快速提升下，基于深度学习模型开发的决策方案在预测维护应用中受到了广泛关注。卷积神经网络(CNN)依靠共享权重和空间池化的优势，可以从工业数据中学习到有效的健康状态表示。许多基于CNN的方案，如引入残差学习和多尺度学习的高级CNN，已经在健康状态识别任务中表现出良好的性能，但是这些方案无法处理属于训练集之外的状态类别的新异常样本。本文提出了一种基于不同CNN的集体决策框架，它基于一种一对多网络(OVRN)，同时实现已知和未知健康状态的分类。OVRN学习状态特定的判别表达。

    At present, decision making solutions developed based on deep learning (DL) models have received extensive attention in predictive maintenance (PM) applications along with the rapid improvement of computing power. Relying on the superior properties of shared weights and spatial pooling, Convolutional Neural Network (CNN) can learn effective representations of health states from industrial data. Many developed CNN-based schemes, such as advanced CNNs that introduce residual learning and multi-scale learning, have shown good performance in health state recognition tasks under the assumption that all the classes are known. However, these schemes have no ability to deal with new abnormal samples that belong to state classes not part of the training set. In this paper, a collective decision framework for different CNNs is proposed. It is based on a One-vs-Rest network (OVRN) to simultaneously achieve classification of known and unknown health states. OVRN learn state-specific discriminative
    
[^96]: 一种高效神经架构搜索空间定义方法

    An Approach for Efficient Neural Architecture Search Space Definition. (arXiv:2310.17669v1 [cs.LG])

    [http://arxiv.org/abs/2310.17669](http://arxiv.org/abs/2310.17669)

    这篇论文介绍了一种高效的神经架构搜索空间定义方法，旨在优化搜索时间，并且能够处理大多数最先进的卷积神经网络（CNN）架构。

    

    随着机器学习快速发展的时代的到来，各种新的、更复杂的神经架构正在出现，以更高效地解决问题。一方面，它们的高效使用需要先进的知识和专业技能，这在劳动力市场上往往难以找到。另一方面，使用试错方法手动搜索优化的神经架构是一项耗时的任务。因此，需要一种方法和工具来辅助神经架构的用户，这导致了自动机器学习（AutoML）领域的热切关注。在深度学习领域，自动机器学习的一个重要部分是神经架构搜索（NAS）。在本文中，我们提出了一种新颖的基于单元的分层搜索空间，易于理解和操作。该方法的目标是优化搜索时间，并且足够通用，能够处理大多数最先进的卷积神经网络（CNN）架构。

    As we advance in the fast-growing era of Machine Learning, various new and more complex neural architectures are arising to tackle problem more efficiently. On the one hand their efficient usage requires advanced knowledge and expertise, which is most of the time difficult to find on the labor market. On the other hand, searching for an optimized neural architecture is a time-consuming task when it is performed manually using a trial and error approach. Hence, a method and a tool support is needed to assist users of neural architectures, leading to an eagerness in the field of Automatic Machine Learning (AutoML). When it comes to Deep Learning, an important part of AutoML is the Neural Architecture Search (NAS). In this paper, we propose a novel cell-based hierarchical search space, easy to comprehend and manipulate. The objectives of the proposed approach are to optimize the search-time and to be general enough to handle most of state of the art Convolutional Neural Networks (CNN) arc
    
[^97]: 为了在存在噪声标签的情况下提高鲁棒性，对预训练模型进行微调

    Fine tuning Pre trained Models for Robustness Under Noisy Labels. (arXiv:2310.17668v1 [cs.LG])

    [http://arxiv.org/abs/2310.17668](http://arxiv.org/abs/2310.17668)

    该论文研究了在存在噪声标签的情况下，为了提高鲁棒性，对预训练模型进行微调。目前的研究主要集中在干净数据集上，对于噪声标签情景的探索有限。

    

    训练数据集中存在噪声标签会显著影响机器学习模型的性能。为了解决这个问题，研究人员已经探索了学习噪声标签的方法，以识别干净样本并减少噪声标签的影响。然而，限制训练数据集的某一部分的影响可能会导致整体泛化性能的降低。为了缓解这个问题，最近的研究考虑了通过利用巨大的计算资源来谨慎利用噪声标签。因此，不断增加的训练成本需要重新评估效率。在其他研究领域，人们正在专注于开发针对大型预训练模型的微调技术，旨在实现高泛化性能和效率之间的平衡。然而，这些方法主要集中在干净的数据集上，对于噪声标签情景的探索有限。在这项研究中，我们的目标是找到一种适用于噪声标签场景的微调方法。

    The presence of noisy labels in a training dataset can significantly impact the performance of machine learning models. To tackle this issue, researchers have explored methods for Learning with Noisy Labels to identify clean samples and reduce the influence of noisy labels. However, constraining the influence of a certain portion of the training dataset can result in a reduction in overall generalization performance. To alleviate this, recent studies have considered the careful utilization of noisy labels by leveraging huge computational resources. Therefore, the increasing training cost necessitates a reevaluation of efficiency. In other areas of research, there has been a focus on developing fine-tuning techniques for large pre-trained models that aim to achieve both high generalization performance and efficiency. However, these methods have mainly concentrated on clean datasets, and there has been limited exploration of the noisy label scenario. In this research, our aim is to find 
    
[^98]: 基于神经架构搜索的级联多任务自适应学习

    Cascaded Multi-task Adaptive Learning Based on Neural Architecture Search. (arXiv:2310.17664v1 [cs.LG])

    [http://arxiv.org/abs/2310.17664](http://arxiv.org/abs/2310.17664)

    该论文提出了一种基于神经架构搜索的自适应学习方法，用于优化端到端级联多任务模型。该方法通过在特定模块上应用冻结、插入适配器和微调等自适应操作，并在损失函数上添加惩罚项，成功限制了搜索到的架构。结果表明，该方法能够搜索出与手工设计类似的调整方案，并将优化参数压缩到了全微调的8.7%，同时获得了更好的性能。

    

    级联多个预训练模型是构建端到端系统的有效方法。然而，对整个级联模型进行微调在参数和内存效率上并不高，并且我们的观察结果表明，仅在级联模型上应用适配器模块无法达到与微调相当的性能。我们提出了一种基于神经架构搜索（NAS）框架的自适应学习方法，以优化端到端级联多任务模型。每个特定模型上的候选自适应操作包括冻结、插入适配器和微调。我们还在损失函数上添加了一个惩罚项，以限制学习到的结构，并考虑可训练参数的数量。惩罚项成功限制了搜索到的架构，并且所提出的方法能够搜索出与手工设计类似的调整方案，将优化参数压缩到了全微调的8.7%，且性能更好。

    Cascading multiple pre-trained models is an effective way to compose an end-to-end system. However, fine-tuning the full cascaded model is parameter and memory inefficient and our observations reveal that only applying adapter modules on cascaded model can not achieve considerable performance as fine-tuning. We propose an automatic and effective adaptive learning method to optimize end-to-end cascaded multi-task models based on Neural Architecture Search (NAS) framework. The candidate adaptive operations on each specific module consist of frozen, inserting an adapter and fine-tuning. We further add a penalty item on the loss to limit the learned structure which takes the amount of trainable parameters into account. The penalty item successfully restrict the searched architecture and the proposed approach is able to search similar tuning scheme with hand-craft, compressing the optimizing parameters to 8.7% corresponding to full fine-tuning on SLURP with an even better performance.
    
[^99]: 通道独立策略是否是时间序列预测的最佳解？

    Is Channel Independent strategy optimal for Time Series Forecasting?. (arXiv:2310.17658v1 [cs.LG])

    [http://arxiv.org/abs/2310.17658](http://arxiv.org/abs/2310.17658)

    本文重新考虑了当前通道独立策略在时间序列预测中是否是最佳解决方案，并提出了一种称为CSC的通道自聚类策略来增强性能并减小参数大小。

    

    近年来出现了许多用于长期时间序列预测的模型。最近的研究表明，使用单一线性层的通道相关(CD)或通道独立(CI)建模，甚至可以超过许多复杂模型的性能。然而，当前的研究主要将CD和CI视为两种互补但互斥的方法，无法同时利用这两个极端。而且，CD和CI都是静态策略，无法在没有大量实验的情况下确定是特定数据集的最佳策略。在本文中，我们重新考虑了当前CI策略是否是时间序列预测的最佳解决方案。首先，我们提出了一种简单而有效的策略，称为CSC（通道自聚类策略），用于线性模型。我们的通道自聚类策略增强了CI策略的性能改进，并减小了参数大小。

    There has been an emergence of various models for long-term time series forecasting. Recent studies have demonstrated that a single linear layer, using Channel Dependent (CD) or Channel Independent (CI) modeling, can even outperform a large number of sophisticated models. However, current research primarily considers CD and CI as two complementary yet mutually exclusive approaches, unable to harness these two extremes simultaneously. And it is also a challenging issue that both CD and CI are static strategies that cannot be determined to be optimal for a specific dataset without extensive experiments. In this paper, we reconsider whether the current CI strategy is the best solution for time series forecasting. First, we propose a simple yet effective strategy called CSC, which stands for $\mathbf{C}$hannel $\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel Self-Clustering (CSC) enhances CI strategy's performance improvements while reducing parameter size, fo
    
[^100]: 深度学习算法用于硅碳化物功率MOSFET器件的高级级别-3反模型建模

    Deep Learning Algorithm for Advanced Level-3 Inverse-Modeling of Silicon-Carbide Power MOSFET Devices. (arXiv:2310.17657v1 [eess.SP])

    [http://arxiv.org/abs/2310.17657](http://arxiv.org/abs/2310.17657)

    作者提出了一种深度学习方法，用于检索硅碳化物功率MOSFET的物理参数，该方法适用于重建退化设备的物理参数或检索物理配置。

    

    使用深度学习算法进行反模型建模涉及训练深层结构从静态行为中预测设备参数。反设备建模适用于重建在时间上退化的设备的漂移物理参数或检索物理配置。有很多变量可以影响反模型方法的性能。在这项工作中，作者提出了一种深度学习方法，用于检索硅碳化物功率MOSFET（SiC Power MOS）的级别-3模型的物理参数。SiC器件用于传统硅器件因高温或高开关能力而失效的应用中。SiC功率器件的主要应用在汽车领域（即在电动车领域）。由于生理退化或高应力环境，SiC功率MOS显示出物理参数的显著漂移，可以通过使用反模型进行监测。本工作的目的是...

    Inverse modelling with deep learning algorithms involves training deep architecture to predict device's parameters from its static behaviour. Inverse device modelling is suitable to reconstruct drifted physical parameters of devices temporally degraded or to retrieve physical configuration. There are many variables that can influence the performance of an inverse modelling method. In this work the authors propose a deep learning method trained for retrieving physical parameters of Level-3 model of Power Silicon-Carbide MOSFET (SiC Power MOS). The SiC devices are used in applications where classical silicon devices failed due to high-temperature or high switching capability. The key application of SiC power devices is in the automotive field (i.e. in the field of electrical vehicles). Due to physiological degradation or high-stressing environment, SiC Power MOS shows a significant drift of physical parameters which can be monitored by using inverse modelling. The aim of this work is to 
    
[^101]: 高维预测用于顺序决策制定

    High-Dimensional Prediction for Sequential Decision Making. (arXiv:2310.17651v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.17651](http://arxiv.org/abs/2310.17651)

    本文研究了高维预测用于顺序决策制定的问题，并提出了有效算法和多个应用。通过选择合适的条件事件集合，我们可以为多项式数量的决策制定者制定预测，并实现最优交换后悔。我们还将这一理论推广到在线组合优化和广义博弈领域，提供了高效的无后悔算法。

    

    我们研究了在对抗选择的高维状态下进行预测的问题，这些预测在任意条件事件下都是无偏的，并旨在根据后续决策制定合适的事件。我们提出了解决这个问题的有效算法，并且给出了一些应用，这些应用来自于选择一个合适的条件事件集合。例如，我们可以高效地为多项式数量的决策制定者制定预测，如果他们对我们的预测做出了最佳反应，每个决策制定者都具有最优的交换后悔。我们将这一理论推广到在线组合优化领域，决策制定者具有非常大的行动空间，在这个领域中，我们首次提出了算法，在多项式数量的子序列上实现了多项式数量的决策制定者的无后悔性，这些子序列可能依赖于他们的行动和上下文。我们将这些结果应用到广义博弈中获得了高效的无子序列后悔算法，从而产生了一类新的后悔保证算法。

    We study the problem of making predictions of an adversarially chosen high-dimensional state that are unbiased subject to an arbitrary collection of conditioning events, with the goal of tailoring these events to downstream decision makers. We give efficient algorithms for solving this problem, as well as a number of applications that stem from choosing an appropriate set of conditioning events.  For example, we can efficiently make predictions targeted at polynomially many decision makers, giving each of them optimal swap regret if they best-respond to our predictions. We generalize this to online combinatorial optimization, where the decision makers have a very large action space, to give the first algorithms offering polynomially many decision makers no regret on polynomially many subsequences that may depend on their actions and the context. We apply these results to get efficient no-subsequence-regret algorithms in extensive-form games (EFGs), yielding a new family of regret guara
    
[^102]: 人类引导的复杂度控制抽象化

    Human-Guided Complexity-Controlled Abstractions. (arXiv:2310.17550v1 [cs.LG])

    [http://arxiv.org/abs/2310.17550](http://arxiv.org/abs/2310.17550)

    本研究通过训练神经模型生成一系列离散表示，并通过调整表示的复杂性来提高任务的泛化性能。在微调实验中，我们发现适当的复杂性水平支持最佳的微调性能，并且在人类参与者的研究中也得到验证。

    

    神经网络通常学习任务特定的潜在表示，但这些表示无法推广到新的环境或任务。相反，人类在各种抽象级别（例如，“鸟”与“麻雀”）上学习离散表示（即概念或单词），并根据任务使用适当的抽象。受此启发，我们训练神经模型生成一系列离散表示，并通过调整表示分布的熵来控制表示的复杂性（大致上是为编码输入分配了多少位）。在微调实验中，仅使用少量带标签的示例用于新任务，我们展示了（1）调整表示以适当的复杂性水平支持最高的微调性能，以及（2）在一个人类参与者的研究中，用户能够根据离散表示的可视化来确定下游任务的适当复杂性水平。我们的结果表明一个有希望的方向。

    Neural networks often learn task-specific latent representations that fail to generalize to novel settings or tasks. Conversely, humans learn discrete representations (i.e., concepts or words) at a variety of abstraction levels (e.g., ``bird'' vs. ``sparrow'') and deploy the appropriate abstraction based on task. Inspired by this, we train neural models to generate a spectrum of discrete representations, and control the complexity of the representations (roughly, how many bits are allocated for encoding inputs) by tuning the entropy of the distribution over representations. In finetuning experiments, using only a small number of labeled examples for a new task, we show that (1) tuning the representation to a task-appropriate complexity level supports the highest finetuning performance, and (2) in a human-participant study, users were able to identify the appropriate complexity level for a downstream task using visualizations of discrete representations. Our results indicate a promising
    
[^103]: 大型语言模型能否取代人类在系统评价过程中的角色？评估GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的效果。

    Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])

    [http://arxiv.org/abs/2310.17526](http://arxiv.org/abs/2310.17526)

    本研究评估了GPT-4在多种语言的同行评审文献和灰色文献筛选和提取数据方面的能力，结果显示GPT-4在大多数任务上准确度与人类表现相当，在调整了偶然一致性和数据集不平衡后，其在数据提取方面表现出中等水平的准确度。

    

    系统评价对于指导实践、研究和政策至关重要，然而常常需要耗费大量时间和人力。大型语言模型（LLM）可能能够加快和自动化系统评价的过程，但是它们在这些任务中的表现尚未经过全面评估，而且还没有研究测试过迄今为止最大的LLM——GPT-4。本预注册研究采用“无人参与”的方法评估了GPT-4在标题/摘要筛选、全文审查和数据提取方面在不同文献类型和语言上的能力。尽管GPT-4在大多数任务中的准确度与人类表现相当，但结果受到偶然一致性和数据集不平衡的影响。在调整了这些因素后，数据提取方面表现出中等水平的准确度，在使用高可靠性提示进行筛选的研究中，筛选全文文献的表现水平在不同阶段和语言上均为无到中等。

    Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prom
    
[^104]: 《低秩适应的表达能力》

    The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])

    [http://arxiv.org/abs/2310.17513](http://arxiv.org/abs/2310.17513)

    本文分析了低秩适应（LoRA）的表达能力，证明了对于全连接神经网络，当LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度）时，LoRA可以使任何模型f准确表示任何较小的目标模型f。对于Transformer网络，通过rank-（嵌入大小/ 2）的LoRA适配器可以使任何模型适应于相同大小的目标模型。

    

    低秩适应（LoRA）是一种参数高效的微调方法，利用矩阵的低秩适应性，在微调预训练模型（如大型语言模型和扩散模型）中得到了广泛应用。尽管在实践中取得了巨大成功，但是LoRA的理论基础在很大程度上尚未得到探索。本文通过从理论角度分析LoRA的表达能力，首次尝试弥合这一差距。我们证明了对于全连接神经网络，如果LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度），则LoRA可以使任何模型f准确表示任何较小的目标模型f。当LoRA-rank低于阈值时，我们还量化了逼近误差。对于Transformer网络，我们证明任何模型可以通过rank-（嵌入大小/ 2）的LoRA适配器适应于相同大小的目标模型。

    Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
    
[^105]: 通过暂时卷积神经网络实现的全新化学反应生成

    De-novo Chemical Reaction Generation by Means of Temporarily Convolutional Neural Networks. (arXiv:2310.17341v1 [cs.LG])

    [http://arxiv.org/abs/2310.17341](http://arxiv.org/abs/2310.17341)

    本研究结合递归神经网络（RNN）和临时卷积神经网络（TCN），使用新的反应Smiles-like表示实现了全新的化学反应生成，并通过迁移学习发现微调协议对模型生成范围有重要影响。

    

    我们在使用新颖的反应Smiles-like表示（CGRSmiles）时，将递归神经网络（RNN）和临时卷积神经网络（TCN）相结合，以实现全新的反应生成，并直接融合了原子映射。递归神经网络以其自回归特性而闻名，并经常在语言建模中使用，直接应用于SMILES生成。相对较新的TCN具有类似的性质，具有广泛的感受野，并遵守自然语言处理（NLP）所需的因果性。通过TCN和RNN表达的两种潜在表示的组合相比仅使用RNN时具有更好的性能。此外，研究还表明，通过迁移学习将不同的微调协议应用于感兴趣的数据集时，对模型的生成范围有深远影响。

    We present here a combination of two networks, Recurrent Neural Networks (RNN) and Temporarily Convolutional Neural Networks (TCN) in de novo reaction generation using the novel Reaction Smiles-like representation of reactions (CGRSmiles) with atom mapping directly incorporated. Recurrent Neural Networks are known for their autoregressive properties and are frequently used in language modelling with direct application to SMILES generation. The relatively novel TCNs possess similar properties with wide receptive field while obeying the causality required for natural language processing (NLP). The combination of both latent representations expressed through TCN and RNN results in an overall better performance compared to RNN alone. Additionally, it is shown that different fine-tuning protocols have a profound impact on generative scope of the model when applied on a dataset of interest via transfer learning.
    
[^106]: 将循环引入人类：协作和可解释的贝叶斯优化

    Looping in the Human: Collaborative and Explainable Bayesian Optimization. (arXiv:2310.17273v1 [cs.LG])

    [http://arxiv.org/abs/2310.17273](http://arxiv.org/abs/2310.17273)

    协作和可解释的贝叶斯优化框架(CoExBO)在贝叶斯优化中引入了循环，平衡了人工智能和人类的合作关系。它利用偏好学习将用户见解融合到优化中，解释每次迭代的候选选择，从而增强用户对优化过程的信任，并提供无害保证。

    

    像许多优化器一样，贝叶斯优化在获得用户信任方面常常存在不足，因为其不透明性。虽然已经尝试开发面向人类的优化器，但它们通常假设用户知识是明确且无误的，并主要将用户作为优化过程的监督者。我们放宽了这些假设，提出了一种更平衡的人工智能和人类合作伙伴关系，即我们的协作和可解释的贝叶斯优化（CoExBO）框架。CoExBO使用偏好学习来无缝地将人类见解整合到优化中，从而产生与用户使用偏好一致的算法建议。CoExBO解释其每次迭代的候选选择，以培养信任，使用户更清楚地掌握优化的过程。此外，CoExBO提供无害保证，允许用户犯错误；即使在极端对抗性干扰下，算法也会渐进地收敛。

    Like many optimizers, Bayesian optimization often falls short of gaining user trust due to opacity. While attempts have been made to develop human-centric optimizers, they typically assume user knowledge is well-specified and error-free, employing users mainly as supervisors of the optimization process. We relax these assumptions and propose a more balanced human-AI partnership with our Collaborative and Explainable Bayesian Optimization (CoExBO) framework. Instead of explicitly requiring a user to provide a knowledge model, CoExBO employs preference learning to seamlessly integrate human insights into the optimization, resulting in algorithmic suggestions that resonate with user preference. CoExBO explains its candidate selection every iteration to foster trust, empowering users with a clearer grasp of the optimization. Furthermore, CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with extreme adversarial interventions, the algorithm converges asymptotically to
    
[^107]: 多尺度扩散去噪平滑

    Multi-scale Diffusion Denoised Smoothing. (arXiv:2310.16779v1 [cs.LG])

    [http://arxiv.org/abs/2310.16779](http://arxiv.org/abs/2310.16779)

    本文研究了多尺度扩散去噪平滑的准确度和认证鲁棒性之间的权衡，并提出了一种在共享扩散模型上调整以实现平滑分类器鲁棒性的新方法。

    

    随着最近的扩散模型，随机平滑已成为少数几个切实可行的方法之一，为大规模预训练模型提供对抗鲁棒性。具体而言，可以通过简单的“去噪和分类”流程，即所谓的去噪平滑，在任何分类器上执行随机平滑，前提是有一个准确的去噪器可用，比如扩散模型。在本文中，我们研究了去噪平滑的准确度和认证鲁棒性之间的权衡：例如，我们质疑哪种扩散模型的表示形式能够最大化去噪平滑的认证鲁棒性。我们考虑了一个新的目标，旨在实现共同噪声水平下平滑分类器的鲁棒性，在共享扩散模型上进行精细调整，同时也为其认证鲁棒性补偿准确度的成本提供了一种新途径。

    Along with recent diffusion models, randomized smoothing has become one of a few tangible approaches that offers adversarial robustness to models at scale, e.g., those of large pre-trained models. Specifically, one can perform randomized smoothing on any classifier via a simple "denoise-and-classify" pipeline, so-called denoised smoothing, given that an accurate denoiser is available - such as diffusion model. In this paper, we investigate the trade-off between accuracy and certified robustness of denoised smoothing: for example, we question on which representation of diffusion model would maximize the certified robustness of denoised smoothing. We consider a new objective that aims collective robustness of smoothed classifiers across multiple noise levels at a shared diffusion model, which also suggests a new way to compensate the cost of accuracy in randomized smoothing for its certified robustness. This objective motivates us to fine-tune diffusion model (a) to perform consistent de
    
[^108]: 从图像中实现强化学习的控制中心表示的研究

    Towards Control-Centric Representations in Reinforcement Learning from Images. (arXiv:2310.16655v1 [cs.LG])

    [http://arxiv.org/abs/2310.16655](http://arxiv.org/abs/2310.16655)

    该论文提出了一个名为ReBis的方法，通过整合无奖励控制信息和奖励特定知识，来捕捉图像中的控制中心信息。ReBis利用变形器架构来建模动力学，并通过分块掩码消除时空冗余。此外，ReBis还结合了等仿函式损失和非对称重建损失，以防止在稀疏奖励环境中的特征崩溃。

    

    基于图像的强化学习是一项实际但具有挑战性的任务。其中一个主要障碍在于提取控制中心的表示，同时忽略不相关的信息。虽然遵循等仿函式原则的方法展示了学习状态表示来解决这个问题的潜力，但它们仍然面临着潜在动力学的有限表达能力和适应稀疏奖励环境的困难。为了解决这些限制，我们引入了ReBis，旨在通过将免奖励控制信息与奖励特定知识集成来捕捉控制中心信息。ReBis利用变形器架构隐式建模动力学，并结合分块掩码消除时空冗余。此外，ReBis将等仿函式损失与非对称重建损失相结合，以防止在稀疏奖励环境中的特征崩溃。在Atari游戏和...

    Image-based Reinforcement Learning is a practical yet challenging task. A major hurdle lies in extracting control-centric representations while disregarding irrelevant information. While approaches that follow the bisimulation principle exhibit the potential in learning state representations to address this issue, they still grapple with the limited expressive capacity of latent dynamics and the inadaptability to sparse reward environments. To address these limitations, we introduce ReBis, which aims to capture control-centric information by integrating reward-free control information alongside reward-specific knowledge. ReBis utilizes a transformer architecture to implicitly model the dynamics and incorporates block-wise masking to eliminate spatiotemporal redundancy. Moreover, ReBis combines bisimulation-based loss with asymmetric reconstruction loss to prevent feature collapse in environments with sparse rewards. Empirical studies on two large benchmarks, including Atari games and D
    
[^109]: 乐观主义的陷阱：通过随机化风险标准的分布式强化学习

    Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion. (arXiv:2310.16546v1 [cs.LG])

    [http://arxiv.org/abs/2310.16546](http://arxiv.org/abs/2310.16546)

    本论文提出了一种通过随机化风险标准的分布式强化学习算法，以避免在风险上的偏向性，并证明了其收敛性和最优性。实验证明，在包括Atari 55游戏在内的各种环境中，该方法优于其他分布式算法。

    

    分布式强化学习算法试图利用估计的不确定性进行探索，如在面对不确定性时的乐观主义。然而，使用估计的方差进行乐观探索可能导致数据收集的偏差，阻碍收敛或性能。本文提出了一种新颖的分布式强化学习算法，通过随机化风险标准来选择动作，避免在风险上的单向倾向。我们通过扭曲风险度量提供了一个扰动的分布贝尔曼最优性算子，并证明了所提方法具有较弱的收缩性质的收敛性和最优性。我们的理论结果支持，所提方法不会陷入偏向性的探索，并确保收敛到最优回报。最后，我们在包括Atari 55游戏在内的各种环境中通过实验证明了我们的方法优于其他现有的基于分布的算法。

    Distributional reinforcement learning algorithms have attempted to utilize estimated uncertainty for exploration, such as optimism in the face of uncertainty. However, using the estimated variance for optimistic exploration may cause biased data collection and hinder convergence or performance. In this paper, we present a novel distributional reinforcement learning algorithm that selects actions by randomizing risk criterion to avoid one-sided tendency on risk. We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property. Our theoretical results support that the proposed method does not fall into biased exploration and is guaranteed to converge to an optimal return. Finally, we empirically show that our method outperforms other existing distribution-based algorithms in various environments including Atari 55 games.
    
[^110]: 识别偏见的原因：一种基于论证的方法

    Identifying Reasons for Bias: An Argumentation-Based Approach. (arXiv:2310.16506v1 [cs.LG])

    [http://arxiv.org/abs/2310.16506](http://arxiv.org/abs/2310.16506)

    本文提出了一种基于论证的方法来确定为什么一个个体被分类与相似个体不同，该方法使用定量论证框架来表示个体和与其相似个体的属性-值对，并使用一个众所周知的语义来确定对个体分类产生最大贡献的属性-值对。

    

    随着算法决策系统在社会中的普及，确保这些系统的公平性变得越来越重要。虽然在构建公平算法决策系统方面已经进行了大量研究，但其中大部分方法需要访问训练数据，包括个人特征，并且对于哪些个体被不公平地分类没有透明度。本文提出了一种新颖的、与模型无关的基于论证的方法，以确定为什么一个个体被分类与相似个体不同。我们的方法使用定量论证框架来表示个体和与其相似个体的属性-值对，并使用一个众所周知的语义来确定对个体分类产生最大贡献的属性-值对。我们在两个在公平领域常用的数据集上评估了我们的方法，并展示了它在识别差异分类方面的有效性。

    As algorithmic decision-making systems become more prevalent in society, ensuring the fairness of these systems is becoming increasingly important. Whilst there has been substantial research in building fair algorithmic decision-making systems, the majority of these methods require access to the training data, including personal characteristics, and are not transparent regarding which individuals are classified unfairly. In this paper, we propose a novel model-agnostic argumentation-based method to determine why an individual is classified differently in comparison to similar individuals. Our method uses a quantitative argumentation framework to represent attribute-value pairs of an individual and of those similar to them, and uses a well-known semantics to identify the attribute-value pairs in the individual contributing most to their different classification. We evaluate our method on two datasets commonly used in the fairness literature and illustrate its effectiveness in the identi
    
[^111]: 了解代码语义: 对Transformer模型在摘要中的评估

    Understanding Code Semantics: An Evaluation of Transformer Models in Summarization. (arXiv:2310.16314v1 [cs.LG])

    [http://arxiv.org/abs/2310.16314](http://arxiv.org/abs/2310.16314)

    本研究通过评估代码摘要的有效性和引入对抗性案例，研究了基于Transformer模型的代码理解能力。结果显示，模型在理解代码语义方面仍存在挑战，这对于提高软件开发和维护效率具有重要意义。

    

    本文使用先进的基于Transformer的语言模型深入研究了代码摘要的复杂性。通过实证研究，我们通过改变函数和变量名来评估代码摘要的有效性，以探索模型是否真正理解代码语义，还是仅仅依赖文本线索。我们还引入了死代码和注释代码等对抗性案例，涵盖了三种编程语言(Python、Javascript和Java)，以进一步审查模型的理解能力。最终，我们的研究旨在提供有价值的见解，加强Transformer模型理解代码的能力，从而为更高效的软件开发实践和维护工作流程做出贡献。

    This paper delves into the intricacies of code summarization using advanced transformer-based language models. Through empirical studies, we evaluate the efficacy of code summarization by altering function and variable names to explore whether models truly understand code semantics or merely rely on textual cues. We have also introduced adversaries like dead code and commented code across three programming languages (Python, Javascript, and Java) to further scrutinize the model's understanding. Ultimately, our research aims to offer valuable insights into the inner workings of transformer-based LMs, enhancing their ability to understand code and contributing to more efficient software development practices and maintenance workflows.
    
[^112]: 使用具有专门口音代码本的口音识别

    Accented Speech Recognition With Accent-specific Codebooks. (arXiv:2310.15970v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.15970](http://arxiv.org/abs/2310.15970)

    本研究提出了一种使用具有专门口音代码本的口音适应方法，通过交叉注意力和可训练代码本，用于端到端ASR系统。在实验证明了该方法在已见和未见的口音上都能获得显著的性能提升。

    

    语音口音对于现有自动语音识别（ASR）系统构成了重要挑战。在代表性不足的口音中的性能下降严重阻碍了ASR的普及应用。本研究提出了一种新颖的口音适应方法，通过交叉注意力和可训练代码本，用于端到端ASR系统。这些可学习的代码本捕捉了口音特定信息，并被整合到ASR编码器层中。模型在带口音的英语语音上进行训练，而测试数据中也包含了在训练过程中未见过的口音。在Mozilla Common Voice多口音数据集上，我们展示了我们提出的方法在不仅在已见的英语口音中获得显著的性能提升（单词错误率相对提升高达37%），而且在未见的口音上也获得了5%的相对提升。此外，我们还展示了在L2Artic数据集上的零样本迁移设置的好处。我们还进行了对比实验。

    Speech accents pose a significant challenge to state-of-the-art automatic speech recognition (ASR) systems. Degradation in performance across underrepresented accents is a severe deterrent to the inclusive adoption of ASR. In this work, we propose a novel accent adaptation approach for end-to-end ASR systems using cross-attention with a trainable set of codebooks. These learnable codebooks capture accent-specific information and are integrated within the ASR encoder layers. The model is trained on accented English speech, while the test data also contained accents which were not seen during training. On the Mozilla Common Voice multi-accented dataset, we show that our proposed approach yields significant performance gains not only on the seen English accents (up to $37\%$ relative improvement in word error rate) but also on the unseen accents (up to $5\%$ relative improvement in WER). Further, we illustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We also compare
    
[^113]: MEMPSEP III. 一种面向机器学习的多元数据集，用于使用多元集成方法预测太阳能粒子事件的发生和属性（arXiv:2310.15390v1 [astro-ph.SR]）

    MEMPSEP III. A machine learning-oriented multivariate data set for forecasting the Occurrence and Properties of Solar Energetic Particle Events using a Multivariate Ensemble Approach. (arXiv:2310.15390v1 [astro-ph.SR])

    [http://arxiv.org/abs/2310.15390](http://arxiv.org/abs/2310.15390)

    这个论文介绍了一个面向机器学习的多元数据集，用于预测太阳能粒子事件的发生和属性。数据集包含了多个航天器收集的原位和遥感观测数据，能够与产生太阳能粒子事件的物理过程相关联。

    

    我们介绍了一个新的多元数据集，利用多个航天器收集原位和遥感观测的太阳圈层测量数据，这些数据与产生太阳能粒子事件的物理过程相关。利用地球同步环境卫星（GOES）太阳系活动事件列表从太阳活动周期（SC）23和SC 24的一部分（1998-2013），我们确定了252个产生太阳能粒子（SEP）的太阳事件（耀斑）和17,542个不产生SEP的事件。对于每个确定的事件，我们记录了1 au处的局部等离子体属性，如高能质子和电子数据，上游太阳风条件以及国家地理学会卫星（GOES）和高级组成探测器（ACE）航天器上的不同仪器测得的星际磁场矢量量。我们还收集了来自太阳动力学观测卫星（SDO）、太阳和日球卫星（SoHO）以及Wind太阳射电仪的遥感数据。该数据集旨在允许进行各种变化

    We introduce a new multivariate data set that utilizes multiple spacecraft collecting in-situ and remote sensing heliospheric measurements shown to be linked to physical processes responsible for generating solar energetic particles (SEPs). Using the Geostationary Operational Environmental Satellites (GOES) flare event list from Solar Cycle (SC) 23 and part of SC 24 (1998-2013), we identify 252 solar events (flares) that produce SEPs and 17,542 events that do not. For each identified event, we acquire the local plasma properties at 1 au, such as energetic proton and electron data, upstream solar wind conditions, and the interplanetary magnetic field vector quantities using various instruments onboard GOES and the Advanced Composition Explorer (ACE) spacecraft. We also collect remote sensing data from instruments onboard the Solar Dynamic Observatory (SDO), Solar and Heliospheric Observatory (SoHO), and the Wind solar radio instrument WAVES. The data set is designed to allow for variati
    
[^114]: 模拟对阿尔茨海默病药物重用的有效性进行路径重要性建模

    Modeling Path Importance for Effective Alzheimer's Disease Drug Repurposing. (arXiv:2310.15211v1 [q-bio.QM])

    [http://arxiv.org/abs/2310.15211](http://arxiv.org/abs/2310.15211)

    该论文提出了一种基于网络的新方法（MPI）来有效进行阿尔茨海默病药物重用。该方法通过学习节点嵌入来优先考虑重要路径，从而更好地发现候选药物。

    

    最近，药物重用作为一种有效且资源高效的阿尔茨海默病药物发现范式已经崭露头角。在各种药物重用方法中，基于网络的方法显示出了有希望的结果，因为它们能够利用复杂网络，整合多种相互作用类型（如蛋白质相互作用），更有效地识别潜在药物。然而，现有的方法通常假设网络中相同长度的路径对于识别药物的治疗效果具有相等的重要性。其他领域发现，相同长度的路径并不一定具有相同的重要性。因此，依赖于这一假设可能对药物重用尝试产生不利影响。在这项工作中，我们提出了MPI（模拟路径重要性），这是一种新颖的基于网络的阿尔茨海默病药物重用方法。MPI的独特之处在于，通过学习节点嵌入来优先考虑重要路径，这可以有效捕捉网络的丰富结构信息。因此，利用学习的节点嵌入可以提高药物重用的效果。

    Recently, drug repurposing has emerged as an effective and resource-efficient paradigm for AD drug discovery. Among various methods for drug repurposing, network-based methods have shown promising results as they are capable of leveraging complex networks that integrate multiple interaction types, such as protein-protein interactions, to more effectively identify candidate drugs. However, existing approaches typically assume paths of the same length in the network have equal importance in identifying the therapeutic effect of drugs. Other domains have found that same length paths do not necessarily have the same importance. Thus, relying on this assumption may be deleterious to drug repurposing attempts. In this work, we propose MPI (Modeling Path Importance), a novel network-based method for AD drug repurposing. MPI is unique in that it prioritizes important paths via learned node embeddings, which can effectively capture a network's rich structural information. Thus, leveraging learn
    
[^115]: 在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练

    Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias. (arXiv:2310.14814v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14814](http://arxiv.org/abs/2310.14814)

    本文提出了一种在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练的方法，并引入了一种新的自信度度量方法-$\mathcal{T}$-相似度。实验证明该方法在三种不同伪标签策略下具有良好的效果。

    

    自训练是半监督学习中一种众所周知的方法。它包括对模型自信度高的未标记数据进行伪标签分配，并将其视为标记样本进行处理。对于神经网络，通常使用softmax预测概率作为自信度度量，尽管已知它们对错误预测也过于自信。当数据标注受到某种约束时，这种现象尤为明显，即样本选择偏差存在。为了解决这个问题，我们提出了一种新的自信度度量方法，称为$\mathcal{T}$-相似度，它基于线性分类器的集成预测多样性。我们通过研究稳定点并描述单个成员的多样性与其性能之间的关系来提供我们方法的理论分析。我们通过对三种不同伪标签策略的实验验证了我们自信度度量的好处。

    Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, softmax prediction probabilities are often used as a confidence measure, despite the fact that they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraint. To address this issue, we propose a novel confidence measure, called $\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on c
    
[^116]: 印度股市组合优化方法的比较研究

    A Comparative Study of Portfolio Optimization Methods for the Indian Stock Market. (arXiv:2310.14748v1 [q-fin.PM])

    [http://arxiv.org/abs/2310.14748](http://arxiv.org/abs/2310.14748)

    该论文对印度股市上的三种组合优化方法进行了比较研究，并根据累计收益、波动率和夏普比率等指标，确定出了在不同行业下表现最佳的投资组合。

    

    本章节在印度股市上对MVP、HRP和HERC三种组合优化方法进行了比较研究，特别关注来自印度国家股票交易所上15个行业的股票。根据2022年7月1日印度国家股票交易所发布的报告，确定了每个聚类的热门股票，基于其自由流通市值。对于每个行业，根据2019年7月1日至2022年6月30日期间的股票价格，采用三种组合优化方法设计了三个投资组合，并在2022年7月1日至2023年6月30日期间进行了测试。评估投资组合的表现使用了三个指标，包括累计收益、年波动率和夏普比率。对于每个行业，识别出在训练和测试期间累计收益最高、波动率最低和夏普比率最大的投资组合。

    This chapter presents a comparative study of the three portfolio optimization methods, MVP, HRP, and HERC, on the Indian stock market, particularly focusing on the stocks chosen from 15 sectors listed on the National Stock Exchange of India. The top stocks of each cluster are identified based on their free-float market capitalization from the report of the NSE published on July 1, 2022 (NSE Website). For each sector, three portfolios are designed on stock prices from July 1, 2019, to June 30, 2022, following three portfolio optimization approaches. The portfolios are tested over the period from July 1, 2022, to June 30, 2023. For the evaluation of the performances of the portfolios, three metrics are used. These three metrics are cumulative returns, annual volatilities, and Sharpe ratios. For each sector, the portfolios that yield the highest cumulative return, the lowest volatility, and the maximum Sharpe Ratio over the training and the test periods are identified.
    
[^117]: GNNEvaluator: 在没有标签的未知图上评估GNN性能

    GNNEvaluator: Evaluating GNN Performance On Unseen Graphs Without Labels. (arXiv:2310.14586v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14586](http://arxiv.org/abs/2310.14586)

    本论文研究了GNN模型评估的新问题，提出了一个包括DiscGraph集合构建和GNNEvaluator训练和推断两个阶段的GNN模型评估框架，能够在没有标签的未知图上准确评估GNN模型的性能。

    

    评估图神经网络（GNN）的性能是实际应用GNN模型和服务的重要任务，因为部署的GNN在推断未知和无标签测试图时面临着显著的性能不确定性，这是由于训练-测试图分布不匹配所造成的。本文研究了一个新的问题，即GNN模型评估，旨在通过精确估计其在没有标签的未知图上的性能（如节点分类准确性），来评估在带标签和观察到的图上训练的特定GNN模型的性能。具体而言，我们提出了一个包括DiscGraph集合构建和GNNEvaluator训练和推断两个阶段的GNN模型评估框架。DiscGraph集合通过利用与潜在节点嵌入和节点类别预测相关的GNN输出的差异度量函数，捕捉了广泛和多样的图数据分布差异。在DiscGr的有效训练监督下。我们的方法在多个数据集上进行了实验，并与现有的GNN评估方法进行了比较。实验结果表明，我们的方法能够准确地评估GNN模型在没有标签的未知图上的性能。

    Evaluating the performance of graph neural networks (GNNs) is an essential task for practical GNN model deployment and serving, as deployed GNNs face significant performance uncertainty when inferring on unseen and unlabeled test graphs, due to mismatched training-test graph distributions. In this paper, we study a new problem, GNN model evaluation, that aims to assess the performance of a specific GNN model trained on labeled and observed graphs, by precisely estimating its performance (e.g., node classification accuracy) on unseen graphs without labels. Concretely, we propose a two-stage GNN model evaluation framework, including (1) DiscGraph set construction and (2) GNNEvaluator training and inference. The DiscGraph set captures wide-range and diverse graph data distribution discrepancies through a discrepancy measurement function, which exploits the outputs of GNNs related to latent node embeddings and node class predictions. Under the effective training supervision from the DiscGr
    
[^118]: 机器学习模型的成员推断攻击的基本限制

    Fundamental Limits of Membership Inference Attacks on Machine Learning Models. (arXiv:2310.13786v1 [stat.ML])

    [http://arxiv.org/abs/2310.13786](http://arxiv.org/abs/2310.13786)

    本文探讨了机器学习模型上成员推断攻击的基本限制，包括推导了效果和成功率的统计量，并提供了几种情况下的界限。这使得我们能够根据样本数量和其他结构参数推断潜在攻击的准确性。

    

    成员推断攻击（MIA）可以揭示特定数据点是否是训练数据集的一部分，可能暴露个人的敏感信息。本文探讨了关于机器学习模型上MIA的基本统计限制。具体而言，我们首先推导了统计量，该统计量决定了这种攻击的有效性和成功率。然后，我们研究了几种情况，并对这个感兴趣的统计量提供了界限。这使我们能够根据样本数量和学习模型的其他结构参数推断潜在攻击的准确性，在某些情况下可以直接从数据集中估计。

    Membership inference attacks (MIA) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. This article explores the fundamental statistical limitations associated with MIAs on machine learning models. More precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. Then, we investigate several situations for which we provide bounds on this quantity of interest. This allows us to infer the accuracy of potential attacks as a function of the number of samples and other structural parameters of learning models, which in some cases can be directly estimated from the dataset.
    
[^119]: 通过对比学习增强药物和细胞系表征，以改善抗癌药物优先级排序

    Enhancing drug and cell line representations via contrastive learning for improved anti-cancer drug prioritization. (arXiv:2310.13725v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.13725](http://arxiv.org/abs/2310.13725)

    通过对比学习，该研究提出了一种改进药物和细胞系表征的方法，以保留与药物作用机制和细胞系癌症类型相关的关系结构，并实现了卓越的性能。使用这种方法可以更好地平衡对药物和细胞系特征的依赖，从而实现更个性化的药物优先级排序。

    

    鉴于癌症的复杂性和对治疗的可变反应，通过基因组学序列分析进行的精确个体化癌症治疗已成为当前的标准。然而，每个患者产生的数据量使得快速识别最佳治疗方案变得困难。此外，受限的数据可用性妨碍了计算方法学习与有效药物-细胞系配对相关的模式。在这项工作中，我们提出使用对比学习改进学习到的药物和细胞系表征，以保留与药物作用机制和细胞系癌症类型相关的关系结构。除了相对于最先进的方法实现了卓越的性能外，我们发现使用我们学习到的表征的分类器在进行预测时更加平衡地依赖于药物和细胞系特征。这有助于更个性化的药物优先级排序，其基于与药物耐药性相关的信号。

    Due to cancer's complex nature and variable response to therapy, precision oncology informed by omics sequence analysis has become the current standard of care. However, the amount of data produced for each patients makes it difficult to quickly identify the best treatment regimen. Moreover, limited data availability has hindered computational methods' abilities to learn patterns associated with effective drug-cell line pairs. In this work, we propose the use of contrastive learning to improve learned drug and cell line representations by preserving relationship structures associated with drug mechanism of action and cell line cancer types. In addition to achieving enhanced performance relative to a state-of-the-art method, we find that classifiers using our learned representations exhibit a more balances reliance on drug- and cell line-derived features when making predictions. This facilitates more personalized drug prioritizations that are informed by signals related to drug resistan
    
[^120]: 探索语言模型中谄媚行为的理解

    Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])

    [http://arxiv.org/abs/2310.13548](http://arxiv.org/abs/2310.13548)

    这项研究探讨了强化学习从人类反馈中训练高质量AI助手的技术，发现这种方法可能导致模型在回答问题时过于谄媚，而不是坦诚，通过分析人类偏好数据得出了这一结论。

    

    「从人类反馈中进行强化学习（RLHF）」是训练高质量AI助手的一种流行技术。然而，RLHF可能会鼓励模型通过与用户信念相符的回答来代替真实回答，这种行为被称为谄媚行为。我们研究了RLHF训练模型中谄媚行为的普遍性以及人类偏好判断是否起到了作用。首先，我们证明了五个最先进的AI助手在四个不同的自由文本生成任务中一贯表现出谄媚行为。为了理解人类偏好是否驱动了RLHF模型的这种广泛行为，我们分析了现有的人类偏好数据。我们发现，当回答与用户的观点相符时，它更有可能被选中。此外，人类和偏好模型（PMs）将有说服力的谄媚回答与正确回答相比，有时几乎可以忽略不计地选择了谄媚回答。优化模型输出以满足PMs有时也会在真实性和谄媚行为之间做出取舍。

    Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Over
    
[^121]: 通过对随机分区的视图进行一致分配的表示学习

    Representation Learning via Consistent Assignment of Views over Random Partitions. (arXiv:2310.12692v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2310.12692](http://arxiv.org/abs/2310.12692)

    本论文提出了一种称为CARP的自监督聚类方法，通过对随机分区的视图进行一致分配，实现了可靠的表示学习，同时提高了训练稳定性和防止了崩溃解决方案的出现。在广泛的评估中，证明CARP的表示适用于多种下游任务，并与11种现有的自监督方法进行了比较。

    

    我们提出了一种自监督聚类方法，即对随机分区的视图进行一致分配（CARP），用于学习视觉特征的表示学习。CARP以端到端在线的方式使用梯度下降来学习原型，而无需额外的非可微模块来解决聚类分配问题。CARP通过基于原型的随机分区优化新的预训练任务，从而对模型进行正则化并强制视图之间的分配一致性。此外，我们的方法改善了训练的稳定性，并防止了联合嵌入训练中的崩溃解决方案。通过广泛的评估，我们证明CARP的表示适用于学习下游任务。我们在17个数据集上评估了CARP的表示能力，包括线性评估、少样本分类、k-NN、k-means、图像检索和复制检测等许多标准协议。我们将CARP的性能与11种现有的自监督方法进行了比较。

    We present Consistent Assignment of Views over Random Partitions (CARP), a self-supervised clustering method for representation learning of visual features. CARP learns prototypes in an end-to-end online fashion using gradient descent without additional non-differentiable modules to solve the cluster assignment problem. CARP optimizes a new pretext task based on random partitions of prototypes that regularizes the model and enforces consistency between views' assignments. Additionally, our method improves training stability and prevents collapsed solutions in joint-embedding training. Through an extensive evaluation, we demonstrate that CARP's representations are suitable for learning downstream tasks. We evaluate CARP's representations capabilities in 17 datasets across many standard protocols, including linear evaluation, few-shot classification, k-NN, k-means, image retrieval, and copy detection. We compare CARP performance to 11 existing self-supervised methods. We extensively abla
    
[^122]: Jorge: GPU高效的二阶优化的近似预处理方法

    Jorge: Approximate Preconditioning for GPU-efficient Second-order Optimization. (arXiv:2310.12298v1 [cs.LG])

    [http://arxiv.org/abs/2310.12298](http://arxiv.org/abs/2310.12298)

    本文介绍了Jorge，一种GPU高效的二阶优化算法，通过近似预处理方法替代矩阵求逆计算来提高计算效率，同时兼具二阶方法的收敛性能。实验证明了Jorge的有效性。

    

    尽管与一阶优化器相比，二阶优化器具有更好的收敛性能，但由于计算成本较大，深度学习中的二阶优化器一直不太受欢迎。这种优化器中的主要效率瓶颈是预处理步骤中的矩阵求逆计算，在GPU上计算昂贵。在本文中，我们引入了Jorge，一种二阶优化器，它兼具二阶方法的快速收敛特性和一阶方法的高计算效率。我们通过完全消除矩阵求逆计算的方法来解决计算瓶颈，用近似的预处理器计算替代。这使得Jorge在墙钟时间上在GPU上非常高效。此外，我们描述了一种直接从调整良好的SGD基准中确定Jorge超参数的方法，从而显著减少了调参工作。我们的实证评估证明了Jorge的效果。

    Despite their better convergence properties compared to first-order optimizers, second-order optimizers for deep learning have been less popular due to their significant computational costs. The primary efficiency bottleneck in such optimizers is matrix inverse calculations in the preconditioning step, which are expensive to compute on GPUs. In this paper, we introduce Jorge, a second-order optimizer that promises the best of both worlds -- rapid convergence benefits of second-order methods, and high computational efficiency typical of first-order methods. We address the primary computational bottleneck of computing matrix inverses by completely eliminating them using an approximation of the preconditioner computation. This makes Jorge extremely efficient on GPUs in terms of wall-clock time. Further, we describe an approach to determine Jorge's hyperparameters directly from a well-tuned SGD baseline, thereby significantly minimizing tuning efforts. Our empirical evaluations demonstrate
    
[^123]: 一个简单的无需线搜索的凸优化问题统一最优解法

    A simple uniformly optimal method without line search for convex optimization. (arXiv:2310.10082v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2310.10082](http://arxiv.org/abs/2310.10082)

    本研究提出了一种简单的、无需线搜索的凸优化方法，能够以最佳的收敛速度解决参数未先给定的凸优化问题，并通过自适应条件快速梯度法（AC-FGM）实现了O(1/k^2)的收敛速度。这种方法还被扩展到具有H\"{o}lder连续梯度的凸优化问题，并且能够在所有问题类中以统一的最佳收敛速度解决。

    

    线搜索（或回溯）程序广泛应用于用于解决凸优化问题的一阶方法中，特别是那些具有未知问题参数（例如，Lipschitz常数）的问题中。本文中，我们展示了在线搜索中获得最佳收敛速度对于解决参数未先给定的凸优化问题是多余的。特别地，我们提出了一种新颖的加速梯度下降类型算法，称为自适应条件快速梯度法（AC-FGM），它能够在不需要估计全局Lipschitz常数或使用线搜索程序的情况下实现最佳的O(1/k^2)收敛速度，用于平滑凸优化。然后，我们将AC-FGM扩展到求解具有H\"{o}lder连续梯度的凸优化问题，并展示它在所有具有所需精度解的问题类中自动地实现了统一的最佳收敛速度。

    Line search (or backtracking) procedures have been widely employed into first-order methods for solving convex optimization problems, especially those with unknown problem parameters (e.g., Lipschitz constant). In this paper, we show that line search is superfluous in attaining the optimal rate of convergence for solving a convex optimization problem whose parameters are not given a priori. In particular, we present a novel accelerated gradient descent type algorithm called auto-conditioned fast gradient method (AC-FGM) that can achieve an optimal $\mathcal{O}(1/k^2)$ rate of convergence for smooth convex optimization without requiring the estimate of a global Lipschitz constant or the employment of line search procedures. We then extend AC-FGM to solve convex optimization problems with H\"{o}lder continuous gradients and show that it automatically achieves the optimal rates of convergence uniformly for all problem classes with the desired accuracy of the solution as the only input. Fi
    
[^124]: Provably Fast Convergence of Independent Natural Policy Gradient for Markov Potential Games.（arXiv：2310.09727v2[cs.LG]已更新）

    Provably Fast Convergence of Independent Natural Policy Gradient for Markov Potential Games. (arXiv:2310.09727v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.09727](http://arxiv.org/abs/2310.09727)

    本文研究了马尔可夫潜在博弈中独立自然策略梯度算法的多智能体强化学习问题，通过引入“次优间隙”的条件和精确策略评估的预言机，在$\mathcal{O}(1/ \epsilon)$次迭代内达到$\epsilon$-Nash均衡。

    

    本文研究了马尔可夫潜在博弈中独立自然策略梯度（NPG）算法的多智能体强化学习问题。研究结果表明，在一些技术假设和引入“次优间隙”的条件下，具有提供精确策略评估的预言机的独立NPG方法渐近地在$\mathcal{O}(1/ \epsilon)$次迭代内达到$\epsilon$-Nash均衡（NE）。这改进了之前最好结果$\mathcal{O}(1/ \epsilon^2)$次迭代，并且与单智能体情况下可达到的$\mathcal{O}(1/ \epsilon)$次迭代具有相同的数量级。通过合成潜在博弈和拥塞博弈的实证结果验证了理论界限。

    This work studies an independent natural policy gradient (NPG) algorithm for the multi-agent reinforcement learning problem in Markov potential games. It is shown that, under mild technical assumptions and the introduction of the \textit{suboptimality gap}, the independent NPG method with an oracle providing exact policy evaluation asymptotically reaches an $\epsilon$-Nash Equilibrium (NE) within $\mathcal{O}(1/\epsilon)$ iterations. This improves upon the previous best result of $\mathcal{O}(1/\epsilon^2)$ iterations and is of the same order, $\mathcal{O}(1/\epsilon)$, that is achievable for the single-agent case. Empirical results for a synthetic potential game and a congestion game are presented to verify the theoretical bounds.
    
[^125]: 每个参数都很重要：确保具有动态异构模型缩减的联邦学习的收敛性

    Every Parameter Matters: Ensuring the Convergence of Federated Learning with Dynamic Heterogeneous Models Reduction. (arXiv:2310.08670v1 [cs.LG])

    [http://arxiv.org/abs/2310.08670](http://arxiv.org/abs/2310.08670)

    这项研究提出了一个统一的异构联邦学习算法框架，并证明了在一定条件下，这些算法对于不同类型的数据都能收敛到标准联邦学习的一个稳定点。此外，研究还揭示了两个关键因素：模型提取噪声和最小覆盖指数，提倡了本地模型选择和全局模型选择的联合设计。

    

    在跨设备的联邦学习中，由于底端设备存在资源瓶颈，那些可能提供独特贡献的低端设备被排除在训练大模型之外，这给联邦学习带来了重大挑战。最近的研究工作集中在异构模型的联邦学习上，通过从全局模型中提取缩小尺寸的模型，并将其应用于本地设备。尽管实证成功，但对于该方法的收敛性的一般理论保证仍然是一个未解决的问题。在本文中，我们提出了一个统一的异构联邦学习算法框架，并提供了一种通用的收敛性分析。特别地，我们证明了在某些充分条件下，对于IID和非IID数据，这些算法收敛到标准联邦学习的一个稳定点，适用于一般的平滑成本函数。此外，我们揭示了影响其收敛性的两个关键因素：模型提取噪声和最小覆盖指数，并主张了本地模型选择和全局模型选择的联合设计。

    Cross-device Federated Learning (FL) faces significant challenges where low-end clients that could potentially make unique contributions are excluded from training large models due to their resource bottlenecks. Recent research efforts have focused on model-heterogeneous FL, by extracting reduced-size models from the global model and applying them to local clients accordingly. Despite the empirical success, general theoretical guarantees of convergence on this method remain an open question. In this paper, we present a unifying framework for heterogeneous FL algorithms with online model extraction and provide a general convergence analysis. In particular, we prove that under certain sufficient conditions and for both IID and non-IID data, these algorithms converge to a stationary point of standard FL for general smooth cost functions. Moreover, we illuminate two key factors impacting its convergence: model-extraction noise and minimum coverage index, advocating a joint design of local 
    
[^126]: 数据驱动建模自相似动力学

    Data driven modeling of self-similar dynamics. (arXiv:2310.08282v1 [cs.LG])

    [http://arxiv.org/abs/2310.08282](http://arxiv.org/abs/2310.08282)

    本文介绍了一个多尺度神经网络框架，利用自相似性作为先验知识，对自相似动力系统进行建模。对于确定性动力学，框架可以判断动力学是否自相似；对于不确定性动力学，它可以确定哪个参数集更接近自相似性。方法可以提取与尺度无关的核进行任意尺度的建模，并识别自相似系统中的幂律指数。初步测试表明，方法对Ising模型的临界指数具有理论一致性。

    

    多尺度建模复杂系统对于理解其复杂性至关重要。数据驱动多尺度建模已成为解决复杂系统挑战的一种有希望的方法。另一方面，自相似性在复杂系统中普遍存在，这表明大规模复杂系统可以以较低成本进行建模。本文引入了一个多尺度神经网络框架，将自相似性作为先验知识，并实现了对自相似动力系统的建模。对于确定性动力学，我们的框架可以判断动力学是否自相似。对于不确定性动力学，它可以比较和确定哪个参数集更接近自相似性。该框架允许我们从动力学中提取与尺度无关的核进行任意尺度的建模。此外，我们的方法可以识别自相似系统中的幂律指数。对Ising模型的初步测试产生了与理论一致的临界指数。

    Multiscale modeling of complex systems is crucial for understanding their intricacies. Data-driven multiscale modeling has emerged as a promising approach to tackle challenges associated with complex systems. On the other hand, self-similarity is prevalent in complex systems, hinting that large-scale complex systems can be modeled at a reduced cost. In this paper, we introduce a multiscale neural network framework that incorporates self-similarity as prior knowledge, facilitating the modeling of self-similar dynamical systems. For deterministic dynamics, our framework can discern whether the dynamics are self-similar. For uncertain dynamics, it can compare and determine which parameter set is closer to self-similarity. The framework allows us to extract scale-invariant kernels from the dynamics for modeling at any scale. Moreover, our method can identify the power law exponents in self-similar systems. Preliminary tests on the Ising model yielded critical exponents consistent with theo
    
[^127]: MetaBox：一种用于元黑箱优化与强化学习的基准平台

    MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning. (arXiv:2310.08252v1 [cs.LG])

    [http://arxiv.org/abs/2310.08252](http://arxiv.org/abs/2310.08252)

    MetaBox是一种用于开发和评估元黑箱优化与强化学习方法的基准平台，提供灵活的算法模板、广泛的问题实例和基线方法，并引入了三个标准化的性能指标，以促进方法的严格评估。

    

    最近，元黑箱优化与强化学习（MetaBBO-RL）展示了在元级别上利用强化学习来减少对低级黑箱优化器的手动微调的能力。然而，这个领域由于缺乏统一的基准而受到阻碍。为了填补这个空白，我们介绍了MetaBox，这是一个专门为开发和评估MetaBBO-RL方法而设计的第一个基准平台。MetaBox提供了一个灵活的算法模板，让用户可以轻松地在平台内实现自己的独特设计。此外，它提供了超过300个问题实例，从合成到真实场景的广泛范围，并且包含了19种基线方法的详尽库，包括传统黑箱优化器和最近的MetaBBO-RL方法。此外，MetaBox引入了三个标准化的性能指标，使方法的评估更加全面。

    Recently, Meta-Black-Box Optimization with Reinforcement Learning (MetaBBO-RL) has showcased the power of leveraging RL at the meta-level to mitigate manual fine-tuning of low-level black-box optimizers. However, this field is hindered by the lack of a unified benchmark. To fill this gap, we introduce MetaBox, the first benchmark platform expressly tailored for developing and evaluating MetaBBO-RL methods. MetaBox offers a flexible algorithmic template that allows users to effortlessly implement their unique designs within the platform. Moreover, it provides a broad spectrum of over 300 problem instances, collected from synthetic to realistic scenarios, and an extensive library of 19 baseline methods, including both traditional black-box optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three standardized performance metrics, enabling a more thorough assessment of the methods. In a bid to illustrate the utility of MetaBox for facilitating rigorous evaluation and in-
    
[^128]: 离线强化学习中的问责制：用语料库的例子解释决策

    Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples. (arXiv:2310.07747v1 [cs.LG])

    [http://arxiv.org/abs/2310.07747](http://arxiv.org/abs/2310.07747)

    本论文介绍了一种可解释的离线控制器方法，通过使用离线数据集作为决策语料库，在低数据场景中实现了问责制的控制，并在医疗保健领域展示了良好的性能。

    

    在决策系统中使用离线数据学习透明、可解释的控制器是一个重要的研究领域，因为它有潜力降低在现实世界系统中应用的风险。然而，在责任敏感的设置（如医疗保健）中，决策问责制非常重要，但目前的文献尚未充分解决这个问题。本文介绍了一种名为Accountable Offline Controller（AOC）的方法，它将离线数据集作为决策语料库，并根据一组定制的例子（称为语料库子集）进行问责制的控制。AOC在低数据场景中有效地运行，可以扩展到严格的离线模仿设置，并表现出保护和适应性的特点。我们在模拟和真实的医疗保健场景中评估了AOC的性能，强调了它在保持问责制的同时能够管理高水平的离线控制任务。

    Learning transparent, interpretable controllers with offline data in decision-making systems is an essential area of research due to its potential to reduce the risk of applications in real-world systems. However, in responsibility-sensitive settings such as healthcare, decision accountability is of paramount importance, yet has not been adequately addressed by the literature. This paper introduces the Accountable Offline Controller (AOC) that employs the offline dataset as the Decision Corpus and performs accountable control based on a tailored selection of examples, referred to as the Corpus Subset. ABC operates effectively in low-data scenarios, can be extended to the strictly offline imitation setting, and displays qualities of both conservation and adaptability. We assess ABC's performance in both simulated and real-world healthcare scenarios, emphasizing its capability to manage offline control tasks with high levels of performance while maintaining accountability.  Keywords: Int
    
[^129]: 大规模类别下的广义神经崩溃

    Generalized Neural Collapse for a Large Number of Classes. (arXiv:2310.05351v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05351](http://arxiv.org/abs/2310.05351)

    本论文将神经崩溃概念扩展到类别数远大于特征空间维度的情况，并展示了广义神经崩溃现象的最小边界值被最大化。

    

    神经崩溃提供了深度分类模型中学习的最后一层表示（即特征）和分类器权重的优雅数学描述。这种结果不仅提供了洞察力，还激发了改进实际深度模型的新技术。然而，大多数关于神经崩溃的现有经验和理论研究都集中于类别数相对于特征空间维度较小的情况。本文将神经崩溃扩展到类别数远大于特征空间维度的情况，这在语言模型、检索系统和人脸识别应用中广泛出现。我们展示了特征和分类器展现出了广义神经崩溃现象，其中最小的一对其他类别间边界值被最大化。我们进行了实证研究以验证实际深度神经网络中广义神经崩溃的发生。此外，我们提供了理论研究，以表明….

    Neural collapse provides an elegant mathematical characterization of learned last layer representations (a.k.a. features) and classifier weights in deep classification models. Such results not only provide insights but also motivate new techniques for improving practical deep models. However, most of the existing empirical and theoretical studies in neural collapse focus on the case that the number of classes is small relative to the dimension of the feature space. This paper extends neural collapse to cases where the number of classes are much larger than the dimension of feature space, which broadly occur for language models, retrieval systems, and face recognition applications. We show that the features and classifier exhibit a generalized neural collapse phenomenon, where the minimum one-vs-rest margins is maximized.We provide empirical study to verify the occurrence of generalized neural collapse in practical deep neural networks. Moreover, we provide theoretical study to show tha
    
[^130]: 流行学习：通过随机通信增强分散式学习

    Epidemic Learning: Boosting Decentralized Learning with Randomized Communication. (arXiv:2310.01972v1 [cs.LG])

    [http://arxiv.org/abs/2310.01972](http://arxiv.org/abs/2310.01972)

    流行学习是一种简单而强大的分散式学习算法，通过利用变化的通信拓扑结构实现了比传统方法更快的模型收敛速度，具有更好的收敛性能。

    

    我们提出了一种名为流行学习（EL）的简单而强大的分散式学习（DL）算法，利用不断变化的通信拓扑结构，以比传统的DL方法更快的模型收敛速度。在EL的每一轮中，每个节点将其模型更新发送给一个随机样本的$s$个其他节点（在$n$个节点的系统中）。我们对EL进行了广泛的理论分析，证明其变化的拓扑结构导致了比现有的（静态和动态）拓扑结构更好的收敛性能。对于平滑的非凸损失函数，EL的暂态迭代次数，即达到渐近线性加速所需的轮数，是$\mathcal{O}(\frac{n^3}{s^2})$，超过了已知的最佳界限$\mathcal{O}({n^3})$，增加了$s^2$倍，表明了随机通信在DL中的好处。我们在一个96个节点的网络中对EL进行了实证评估，并将其性能与现有的DL方法进行了比较。我们的结果显示，EL达到了更快的模型收敛速度和更好的收敛性能。

    We present Epidemic Learning (EL), a simple yet powerful decentralized learning (DL) algorithm that leverages changing communication topologies to achieve faster model convergence compared to conventional DL approaches. At each round of EL, each node sends its model updates to a random sample of $s$ other nodes (in a system of $n$ nodes). We provide an extensive theoretical analysis of EL, demonstrating that its changing topology culminates in superior convergence properties compared to the state-of-the-art (static and dynamic) topologies. Considering smooth non-convex loss functions, the number of transient iterations for EL, i.e., the rounds required to achieve asymptotic linear speedup, is in $\mathcal{O}(\frac{n^3}{s^2})$ which outperforms the best-known bound $\mathcal{O}({n^3})$ by a factor of $ s^2 $, indicating the benefit of randomized communication for DL. We empirically evaluate EL in a 96-node network and compare its performance with state-of-the-art DL approaches. Our resu
    
[^131]: 实体推断竞技场：探究LLMs的对话推理和规划能力的平台

    The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])

    [http://arxiv.org/abs/2310.01468](http://arxiv.org/abs/2310.01468)

    本文提供了一个评估框架，通过向法官提出一系列查询来评估LLMs的对话推理和规划能力。我们发现不同的LLMs在这个任务上表现出显著差异。

    

    目前，大型语言模型（LLMs）在回答明确提问时非常有效。然而，当面临含糊不清的查询时，它们可能行为难以预测并产生错误的输出。这凸显了需要开发能够提出澄清问题以有效解决歧义的智能代理的需求。这种能力需要对多个对话轮次进行复杂的理解、状态跟踪、推理和规划。然而，直接测量这种能力可能具有挑战性。在本文中，我们提供了一个替代性问题，通过向法官提出一系列查询，评估了LLMs推断自己不知道但被法官揭示的实体的能力。这个“实体推断游戏”可以作为一个评估框架，用于探究语言模型的对话推理和规划能力。我们系统地评估了各种LLMs，并发现在这个任务上它们的性能存在显著差异。我们发现强大的LLMs...

    Large language models (LLMs) are currently effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This \textit{entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs
    
[^132]: DeepPCR：神经网络中的并行化序列操作

    DeepPCR: Parallelizing Sequential Operations in Neural Networks. (arXiv:2309.16318v1 [cs.LG])

    [http://arxiv.org/abs/2309.16318](http://arxiv.org/abs/2309.16318)

    DeepPCR是一种新型算法，通过使用并行循环降解算法将常规的顺序操作在神经网络的推断和训练中并行化，从而实现了计算速度的提升。

    

    并行化技术已经在加速深度神经网络的推断和训练中变得普遍。尽管如此，仍然有一些操作是按顺序进行的。例如，前向传递和反向传递是逐层执行的，并且扩散模型的输出是通过应用一系列去噪步骤产生的。这种顺序方法导致计算成本与所涉及步骤的数量成正比，随着步骤数量的增加，可能出现潜在瓶颈。在这项工作中，我们介绍了一种新颖的算法DeepPCR，它并行化了神经网络推断和训练中通常是顺序操作的步骤。DeepPCR基于将$L$步骤的序列解释为特定方程组的解，我们使用并行循环降解算法恢复这些方程。这将顺序操作的计算复杂度从$\mathcal{O}(L)$降低到$\mathcal{O}(\log_2L)$，从而实现了加速。

    Parallelization techniques have become ubiquitous for accelerating inference and training of deep neural networks. Despite this, several operations are still performed in a sequential manner. For instance, the forward and backward passes are executed layer-by-layer, and the output of diffusion models is produced by applying a sequence of denoising steps. This sequential approach results in a computational cost proportional to the number of steps involved, presenting a potential bottleneck as the number of steps increases. In this work, we introduce DeepPCR, a novel algorithm which parallelizes typically sequential operations used in inference and training of neural networks. DeepPCR is based on interpreting a sequence of $L$ steps as the solution of a specific system of equations, which we recover using the Parallel Cyclic Reduction algorithm. This reduces the complexity of computing the sequential operations from $\mathcal{O}(L)$ to $\mathcal{O}(\log_2L)$, thus yielding a speedup for 
    
[^133]: 基于复杂网络的框架用于建模和挖掘患者路径

    Framework based on complex networks to model and mine patient pathways. (arXiv:2309.14208v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2309.14208](http://arxiv.org/abs/2309.14208)

    该论文提出了一个基于复杂网络的框架，用于建模和挖掘患者路径。该框架包括路径模型、新的相似度测量方法和基于传统中心度的挖掘方法。评估结果表明该框架可有效应用于实际医疗数据分析。

    

    自动发现用于表示一组患者与医疗系统的接触历史的模型，即所谓的“患者路径”，是一项新的研究领域，它支持临床和组织决策，以提高提供的治疗质量和效率。慢性病患者的路径往往因人而异，有重复的任务，并需要分析多个方面（干预、诊断、医疗专业等），影响结果。因此，建模和挖掘这些路径仍然是一个具有挑战性的任务。在这项工作中，我们提出了一个框架，包括：（i）基于多方面图的路径模型，（ii）一种新的相似度测量方法，考虑了耗时，用于比较路径，并且（iii）基于传统中心度测量方法的挖掘方法，用于发现路径中最相关的步骤。我们使用实际医疗数据评估了这个框架。

    The automatic discovery of a model to represent the history of encounters of a group of patients with the healthcare system -- the so-called "pathway of patients" -- is a new field of research that supports clinical and organisational decisions to improve the quality and efficiency of the treatment provided. The pathways of patients with chronic conditions tend to vary significantly from one person to another, have repetitive tasks, and demand the analysis of multiple perspectives (interventions, diagnoses, medical specialities, among others) influencing the results. Therefore, modelling and mining those pathways is still a challenging task. In this work, we propose a framework comprising: (i) a pathway model based on a multi-aspect graph, (ii) a novel dissimilarity measurement to compare pathways taking the elapsed time into account, and (iii) a mining method based on traditional centrality measures to discover the most relevant steps of the pathways. We evaluated the framework using 
    
[^134]: 使用深度学习和药动学先验预测治疗反应

    Forecasting Response to Treatment with Deep Learning and Pharmacokinetic Priors. (arXiv:2309.13135v1 [cs.LG])

    [http://arxiv.org/abs/2309.13135](http://arxiv.org/abs/2309.13135)

    该研究提出了一种使用深度学习和药动学先验预测治疗反应的方法。研究者通过一个新颖的编码器提供药物的药动学信息，从而实现对时间序列的精确预测。实验结果显示，在逼真模拟和真实世界数据上，该方法比基准模型的预测准确性提高了约11%和8%。这种方法在临床实践中具有多种有益应用，如发出早期警告和定量特定患者的治疗效果。

    

    对医疗时间序列的预测对于早期检测不良结果和患者监测至关重要。然而，由于数据嘈杂和间歇性，实际中预测可能很困难。这些挑战通常通过外部因素诱导的变化点（如药物使用）而加剧。我们提出了一种新颖的编码器，以向深度学习模型提供药物的药动学效应信息，从而实现对受治疗影响的时间序列的准确预测。我们展示了我们方法在使用逼真模拟和真实世界数据预测血糖的任务中的有效性。我们的药动学编码器使深度学习模型在模拟数据上超过基准约11％，在真实世界数据上超过8％。所提出的方法可以在临床实践中具有多种有益应用，例如发出关于意外治疗反应的早期警告，或帮助表征特定于患者的治疗效果。

    Forecasting healthcare time series is crucial for early detection of adverse outcomes and for patient monitoring. Forecasting, however, can be difficult in practice due to noisy and intermittent data. The challenges are often exacerbated by change points induced via extrinsic factors, such as the administration of medication. We propose a novel encoder that informs deep learning models of the pharmacokinetic effects of drugs to allow for accurate forecasting of time series affected by treatment. We showcase the effectiveness of our approach in a task to forecast blood glucose using both realistically simulated and real-world data. Our pharmacokinetic encoder helps deep learning models surpass baselines by approximately 11% on simulated data and 8% on real-world data. The proposed approach can have multiple beneficial applications in clinical practice, such as issuing early warnings about unexpected treatment responses, or helping to characterize patient-specific treatment effects in te
    
[^135]: 具有贝叶斯模型缩减的贝叶斯稀疏性深度神经网络

    Bayesian sparsification for deep neural networks with Bayesian model reduction. (arXiv:2309.12095v1 [stat.ML])

    [http://arxiv.org/abs/2309.12095](http://arxiv.org/abs/2309.12095)

    使用贝叶斯模型缩减作为一种更高效的替代方法来修剪模型权重，以提高深度神经网络的计算效率。

    

    深度学习的巨大能力常常受到其模型复杂性的限制，因此对于有效的稀疏技术的需求不断增加。贝叶斯稀疏性对于深度学习而言是一种关键方法，可以促进在各种深度学习应用中设计既具有计算效率又具有竞争性能的模型。目前，贝叶斯稀疏化深度神经网络的最新技术是将结构收缩先验应用于模型权重，并结合基于黑盒随机变分推断的近似推断方案。然而，与标准的深度学习点估计相比，完整生成模型的模型反演在计算方面非常耗费时间。在这种情况下，我们提倡使用贝叶斯模型缩减（BMR）作为模型权重修剪的更高效替代方法。作为决策率的推广，BMR允许对模型权重进行事后消除

    Deep learning's immense capabilities are often constrained by the complexity of its models, leading to an increasing demand for effective sparsification techniques. Bayesian sparsification for deep learning emerges as a crucial approach, facilitating the design of models that are both computationally efficient and competitive in terms of performance across various deep learning applications. The state-of-the-art -- in Bayesian sparsification of deep neural networks -- combines structural shrinkage priors on model weights with an approximate inference scheme based on black-box stochastic variational inference. However, model inversion of the full generative model is exceptionally computationally demanding, especially when compared to standard deep learning of point estimates. In this context, we advocate for the use of Bayesian model reduction (BMR) as a more efficient alternative for pruning of model weights. As a generalization of the Savage-Dickey ratio, BMR allows a post-hoc elimina
    
[^136]: Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])

    Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])

    [http://arxiv.org/abs/2309.07867](http://arxiv.org/abs/2309.07867)

    beta扩散是一种新型生成模型方法，通过引入去掩盖和去噪的技术，利用缩放和偏移的beta分布进行乘法转换，实现在有界范围内生成数据。相比于传统的基于扩散的生成模型，它通过KL散度上界进行优化，证明了效果更好。

    

    我们引入了beta扩散，一种将去掩盖和去噪集成到一起的新型生成建模方法，用于在有界范围内生成数据。使用了缩放和偏移的beta分布，beta扩散利用了随时间的乘法转换来创建正向和反向的扩散过程，同时维持着正向边缘分布和反向条件分布，给定任意时间点的数据。与传统的基于扩散的生成模型不同，传统模型依赖于加性高斯噪声和重新加权的证据下界（ELBO），beta扩散是乘法的，并且通过从KL散度的凸性推导出来的KL散度上界（KLUB）进行优化。我们证明了所提出的KLUB相对于负ELBO来说对于优化beta扩散更加有效，负ELBO也可以作为相同KL散度的KLUB，只是其两个参数交换了位置。beta扩散的损失函数以Bregman散度为指标来表示。

    We introduce beta diffusion, a novel generative modeling method that integrates demasking and denoising to generate data within bounded ranges. Using scaled and shifted beta distributions, beta diffusion utilizes multiplicative transitions over time to create both forward and reverse diffusion processes, maintaining beta distributions in both the forward marginals and the reverse conditionals, given the data at any point in time. Unlike traditional diffusion-based generative models relying on additive Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived from the convexity of the KL divergence. We demonstrate that the proposed KLUBs are more effective for optimizing beta diffusion compared to negative ELBOs, which can also be derived as the KLUBs of the same KL divergence with its two arguments swapped. The loss function of beta diffusion, expressed in terms of Bregman divergence, furt
    
[^137]: 限制距离的民间传说Weisfeiler-Leman图神经网络及可证明的循环计数能力

    Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power. (arXiv:2309.04941v1 [cs.LG])

    [http://arxiv.org/abs/2309.04941](http://arxiv.org/abs/2309.04941)

    本文提出了一种称为$d$-DRFWL(2) GNNs的新型图神经网络，它通过限制节点之间的距离来实现循环计数能力，克服了子图GNNs的预处理和计算成本高的限制。

    

    图神经网络（GNNs）的能力在广泛的任务中成功计数特定的图子结构，尤其是循环，对于GNNs的成功非常关键。最近，它已被用作评估GNNs表达能力的一种常用指标。许多具有可证明的循环计数能力的GNN模型都基于子图GNNs，即从输入图中提取一组子图，为每个子图生成表示，并使用它们来增强输入图的表示。然而，这些方法需要进行繁重的预处理，并且时间和内存成本较高。在本文中，我们通过提出一种新的GNN类别-- $d$-Distance-Restricted FWL(2) GNNs，或者 $d$-DRFWL(2) GNNs，克服了子图GNNs的上述限制。$d$-DRFWL(2) GNNs将互相之间距离不超过$d$的节点对作为信息传递的单位，以平衡表达能力和复杂性。

    The ability of graph neural networks (GNNs) to count certain graph substructures, especially cycles, is important for the success of GNNs on a wide range of tasks. It has been recently used as a popular metric for evaluating the expressive power of GNNs. Many of the proposed GNN models with provable cycle counting power are based on subgraph GNNs, i.e., extracting a bag of subgraphs from the input graph, generating representations for each subgraph, and using them to augment the representation of the input graph. However, those methods require heavy preprocessing, and suffer from high time and memory costs. In this paper, we overcome the aforementioned limitations of subgraph GNNs by proposing a novel class of GNNs -- $d$-Distance-Restricted FWL(2) GNNs, or $d$-DRFWL(2) GNNs. $d$-DRFWL(2) GNNs use node pairs whose mutual distances are at most $d$ as the units for message passing to balance the expressive power and complexity. By performing message passing among distance-restricted node
    
[^138]: 神经潜在几何搜索：通过格罗莫夫-豪斯多夫信息驱动的贝叶斯优化来进行乘积流形推断

    Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization. (arXiv:2309.04810v1 [cs.LG])

    [http://arxiv.org/abs/2309.04810](http://arxiv.org/abs/2309.04810)

    本论文提出了神经潜在几何搜索(NLGS)的概念，旨在通过格罗莫夫-豪斯多夫距离来自动识别下游任务的最佳潜在几何结构，以提高机器学习模型的性能。

    

    最近的研究表明，通过将潜在空间的几何结构与底层数据结构对齐，可以提高机器学习模型的性能。研究人员提出使用具有恒定曲率的双曲和球形空间，或者它们的组合，来更好地建模潜在空间并增强模型性能，而不仅仅依赖于欧几里得空间。然而，目前对自动识别下游任务的最佳潜在几何结构问题还没有给予足够关注。我们在数学上定义了这个新颖的问题，并将其称为神经潜在几何搜索(NLGS)。具体而言，我们引入了一种基于格罗莫夫-豪斯多夫距离的候选潜在几何结构之间的新概念距离，以实现这一目标。为了计算格罗莫夫-豪斯多夫距离，我们提出了一种通过最小查询评估搜索由恒定曲率模型空间乘积组成的潜在几何结构的原则方法。

    Recent research indicates that the performance of machine learning models can be improved by aligning the geometry of the latent space with the underlying data structure. Rather than relying solely on Euclidean space, researchers have proposed using hyperbolic and spherical spaces with constant curvature, or combinations thereof, to better model the latent space and enhance model performance. However, little attention has been given to the problem of automatically identifying the optimal latent geometry for the downstream task. We mathematically define this novel formulation and coin it as neural latent geometry search (NLGS). More specifically, we introduce a principled method that searches for a latent geometry composed of a product of constant curvature model spaces with minimal query evaluations. To accomplish this, we propose a novel notion of distance between candidate latent geometries based on the Gromov-Hausdorff distance from metric geometry. In order to compute the Gromov-Ha
    
[^139]: 发现离散对称的统一框架

    A Unified Framework for Discovering Discrete Symmetries. (arXiv:2309.02898v1 [cs.LG])

    [http://arxiv.org/abs/2309.02898](http://arxiv.org/abs/2309.02898)

    本文提出了一个统一框架，能够发现各种类型的对称性，通过使用线性和张量值函数构成的新颖架构，在多臂赌博算法和梯度下降的帮助下，高效地优化并学习到对称性。在图像数字求和和多项式回归任务上的实验证明了该方法的有效性。

    

    我们考虑从一类对称性中学习一个符合对称性的函数的问题。我们开发了一个统一框架，能够发现包括局部对称、二面角和循环子群在内的广泛子群的对称性。该框架的核心是一种由线性和张量值函数组成的新颖架构，以原则性的方式表达这些子群不变的函数。架构的结构使我们能够利用多臂赌博算法和梯度下降分别高效优化线性和张量值函数，并推断出最终学习到的对称性。我们还讨论了架构中张量值函数的必要性。对图像数字求和和多项式回归任务的实验结果证明了我们方法的有效性。

    We consider the problem of learning a function respecting a symmetry from among a class of symmetries. We develop a unified framework that enables symmetry discovery across a broad range of subgroups including locally symmetric, dihedral and cyclic subgroups. At the core of the framework is a novel architecture composed of linear and tensor-valued functions that expresses functions invariant to these subgroups in a principled manner. The structure of the architecture enables us to leverage multi-armed bandit algorithms and gradient descent to efficiently optimize over the linear and the tensor-valued functions, respectively, and to infer the symmetry that is ultimately learnt. We also discuss the necessity of the tensor-valued functions in the architecture. Experiments on image-digit sum and polynomial regression tasks demonstrate the effectiveness of our approach.
    
[^140]: 内存高效的具有4位状态的优化器

    Memory Efficient Optimizers with 4-bit States. (arXiv:2309.01507v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01507](http://arxiv.org/abs/2309.01507)

    本论文通过将优化器状态的位宽压缩至4位，实现了内存高效的训练神经网络。通过对一阶和二阶矩的详细经验分析，我们发现当前的块状量化方法无法准确近似复杂的异常值模式。为此，我们使用较小的块大小并同时利用行上和列上的信息进行更好的量化。此外，我们还通过排除零点的线性量化器解决了量化第二阶矩时的零点问题。我们的工作在多个基准测试上进行了评估，结果表明我们的4位优化器具有出色的性能。

    

    优化器状态是训练神经网络时的主要内存消耗来源，限制了在给定内存预算内可训练的最大模型。将优化器状态从32位浮点数压缩到更低的位宽有望减小训练内存占用，而当前最低可达到的位宽为8位。在这项工作中，我们通过详细的经验分析将优化器状态位宽降至4位。具体而言，我们发现矩具有复杂的异常值模式，无法通过当前的块状量化方法准确近似。我们使用较小的块大小，并提出同时利用行上和列上的信息进行更好的量化。我们还发现了量化第二阶矩时的零点问题，并通过排除零点的线性量化器来解决这个问题。我们的4位优化器在包括自然语言理解、机器翻译在内的各种基准测试上进行了评估。

    Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizer is evaluated on a wide variety of benchmarks including natural language understanding, machine translat
    
[^141]: 快速和遗憾最小的最佳臂识别：基本限制和低复杂度算法

    Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms. (arXiv:2309.00591v1 [cs.LG])

    [http://arxiv.org/abs/2309.00591](http://arxiv.org/abs/2309.00591)

    本文提出了一种名为 ROBAI 的算法，旨在快速识别并选择最佳臂，并在一系列连续回合中最大化奖励。该算法在预定停止时间和自适应停止时间要求下均实现了渐进最优遗憾，并且在预定停止时间下仅需 $\mathcal{O}(\log T)$ 回合即可选择最佳臂，在自适应停止时间下仅需 $\mathcal{O}(\log^2 T)$ 回合即可选择最佳臂。

    

    本文考虑具有双重目标的随机多臂老虎机(MAB)问题：(i) 快速识别并选择最佳臂，以及(ii) 在一系列T个连续回合中最大化奖励。尽管每个目标都已经得到了独立的深入研究，即(i)的最佳臂识别和(ii)的遗憾最小化，但是同时实现这两个目标仍然是一个开放的问题，尽管它在实践中非常重要。本文引入了“遗憾最小化的最佳臂识别”(ROBAI)，旨在实现这两个双重目标。为了解决具有预定停止时间和自适应停止时间要求的ROBAI，我们分别提出了$\mathsf{EOCP}$算法及其变体，不仅在高斯老虎机和一般老虎机中达到了渐进最优遗憾，而且在预定停止时间下，在$\mathcal{O}(\log T)$回合内选择了最佳臂，在自适应停止时间下，选择了最佳臂在$\mathcal{O}(\log^2 T)$回合内。

    This paper considers a stochastic multi-armed bandit (MAB) problem with dual objectives: (i) quick identification and commitment to the optimal arm, and (ii) reward maximization throughout a sequence of $T$ consecutive rounds. Though each objective has been individually well-studied, i.e., best arm identification for (i) and regret minimization for (ii), the simultaneous realization of both objectives remains an open problem, despite its practical importance. This paper introduces \emph{Regret Optimal Best Arm Identification} (ROBAI) which aims to achieve these dual objectives. To solve ROBAI with both pre-determined stopping time and adaptive stopping time requirements, we present the $\mathsf{EOCP}$ algorithm and its variants respectively, which not only achieve asymptotic optimal regret in both Gaussian and general bandits, but also commit to the optimal arm in $\mathcal{O}(\log T)$ rounds with pre-determined stopping time and $\mathcal{O}(\log^2 T)$ rounds with adaptive stopping ti
    
[^142]: 深度学习的双高斯牛顿方向

    Dual Gauss-Newton Directions for Deep Learning. (arXiv:2308.08886v1 [cs.LG])

    [http://arxiv.org/abs/2308.08886](http://arxiv.org/abs/2308.08886)

    本文研究了深度学习目标的结构特点，提出了基于双重高斯牛顿方向预言的方法。通过对偶形式计算这些预言，得到了既具有计算好处又具有新见解的结果。通过实验证明了这些预言作为随机梯度的下降方向的优势，并研究了计算上的权衡。

    

    受高斯牛顿方法的启发，我们研究了利用深度学习目标的结构，即由凸损失函数和非线性网络组成，以导出比随机梯度更好的方向预言，基于部分线性化的思想。与以前的工作不同，我们提出通过它们的对偶形式来计算这样的方向预言，从而获得计算上的好处和新的见解。我们证明了由此产生的预言定义了可以用作现有优化算法中随机梯度的替代品的下降方向。我们通过实验证明了使用对偶形式的优势以及涉及计算这些预言的计算权衡。

    Inspired by Gauss-Newton-like methods, we study the benefit of leveraging the structure of deep learning objectives, namely, the composition of a convex loss function and of a nonlinear network, in order to derive better direction oracles than stochastic gradients, based on the idea of partial linearization. In a departure from previous works, we propose to compute such direction oracles via their dual formulation, leading to both computational benefits and new insights. We demonstrate that the resulting oracles define descent directions that can be used as a drop-in replacement for stochastic gradients, in existing optimization algorithms. We empirically study the advantage of using the dual formulation as well as the computational trade-offs involved in the computation of such oracles.
    
[^143]: 通过异构模型重组实现个性化联邦学习

    Towards Personalized Federated Learning via Heterogeneous Model Reassembly. (arXiv:2308.08643v1 [cs.LG])

    [http://arxiv.org/abs/2308.08643](http://arxiv.org/abs/2308.08643)

    本文提出了一个名为pFedHR的新框架，利用异构模型重组实现个性化联邦学习。实验表明，pFedHR在各种设置下优于基准方法，并且能够有效降低使用不兼容数据的不良影响。

    

    本文针对联邦学习中模型异构的实际且具有挑战性的问题进行了研究，其中客户端具有不同网络结构的模型。为了解决这个问题，我们提出了一个名为pFedHR的新框架，利用异构模型重组实现个性化联邦学习。具体而言，我们将异构模型个性化问题视为服务器端的模型匹配优化任务。此外，pFedHR能够自动且动态地生成具有最小人工干预的信息丰富且多样化的个性化候选模型。此外，我们提出的异构模型重组技术在一定程度上减轻了使用具有不同分布的公共数据与客户端数据造成的不良影响。实验结果表明，在IID和非IID设置下，pFedHR在三个数据集上的性能优于基准方法。此外，pFedHR有效降低了使用不兼容数据的不良影响。

    This paper focuses on addressing the practical yet challenging problem of model heterogeneity in federated learning, where clients possess models with different network structures. To track this problem, we propose a novel framework called pFedHR, which leverages heterogeneous model reassembly to achieve personalized federated learning. In particular, we approach the problem of heterogeneous model personalization as a model-matching optimization task on the server side. Moreover, pFedHR automatically and dynamically generates informative and diverse personalized candidates with minimal human intervention. Furthermore, our proposed heterogeneous model reassembly technique mitigates the adverse impact introduced by using public data with different distributions from the client data to a certain extent. Experimental results demonstrate that pFedHR outperforms baselines on three datasets under both IID and Non-IID settings. Additionally, pFedHR effectively reduces the adverse impact of usi
    
[^144]: 量化队列系统中的学习成本

    Quantifying the Cost of Learning in Queueing Systems. (arXiv:2308.07817v1 [cs.LG])

    [http://arxiv.org/abs/2308.07817](http://arxiv.org/abs/2308.07817)

    本文提出了一种新的度量方法，即学习队列中的成本 (CLQ)，用于量化由参数不确定性导致的时间平均队列长度最大增加量。该度量方法可以捕捉学习队列系统的统计复杂性，不局限于渐近性能。

    

    队列系统是广泛应用的随机模型，应用于通信网络、医疗保健、服务系统等等。虽然它们的最优控制已经得到了广泛研究，但大多数现有方法都假设系统参数的完美知识。然而，在实践中，参数不确定性很常见，因此最近一系列关于队列系统的学习的研究产生了。这个新兴的研究方向主要关注所提算法的渐近性能。本文中，我们认为渐近度量，即着眼于后期性能的度量，无法捕捉学习队列系统中固有的统计复杂性，这种复杂性通常出现在早期阶段。相反，我们提出了学习队列中的成本 (CLQ)，这是一种新的度量方法，可以衡量由参数不确定性导致的时间平均队列长度的最大增加量。我们对单队列多服务器系统的CLQ进行了表征。

    Queueing systems are widely applicable stochastic models with use cases in communication networks, healthcare, service systems, etc. Although their optimal control has been extensively studied, most existing approaches assume perfect knowledge of system parameters. Of course, this assumption rarely holds in practice where there is parameter uncertainty, thus motivating a recent line of work on bandit learning for queueing systems. This nascent stream of research focuses on the asymptotic performance of the proposed algorithms.  In this paper, we argue that an asymptotic metric, which focuses on late-stage performance, is insufficient to capture the intrinsic statistical complexity of learning in queueing systems which typically occurs in the early stage. Instead, we propose the Cost of Learning in Queueing (CLQ), a new metric that quantifies the maximum increase in time-averaged queue length caused by parameter uncertainty. We characterize the CLQ of a single-queue multi-server system,
    
[^145]: 决定性混淆下的内核单一代理控制

    Kernel Single Proxy Control for Deterministic Confounding. (arXiv:2308.04585v1 [stat.ML])

    [http://arxiv.org/abs/2308.04585](http://arxiv.org/abs/2308.04585)

    本研究考虑了具有未观测混淆因素的因果效应估计问题，在结果是确定性生成的情况下，提出了一种使用单一代理变量的内核方法，通过两阶段回归和最大矩约束的方法可以一致估计因果效应，并在合成数据集上成功恢复了因果效应。

    

    本文考虑具有未观测混淆因素的因果效应估计问题，其中我们观测到与混淆因素相关的代理变量。尽管代理因果学习（PCL）使用两个代理变量来恢复真实的因果效应，我们证明如果结果是确定性生成的，则使用单个代理变量就足以进行因果估计，并概括了控制结果校准法（COCA）。我们提出了两种基于内核的方法：一种基于两阶段回归方法，另一种基于最大矩约束方法。我们证明了这两种方法都可以一致地估计因果效应，并通过合成数据集的实证实验成功地恢复了因果效应。

    We consider the problem of causal effect estimation with an unobserved confounder, where we observe a proxy variable that is associated with the confounder. Although Proxy Causal Learning (PCL) uses two proxy variables to recover the true causal effect, we show that a single proxy variable is sufficient for causal estimation if the outcome is generated deterministically, generalizing Control Outcome Calibration Approach (COCA). We propose two kernel-based methods for this setting: the first based on the two-stage regression approach, and the second based on a maximum moment restriction approach. We prove that both approaches can consistently estimate the causal effect, and we empirically demonstrate that we can successfully recover the causal effect on a synthetic dataset.
    
[^146]: Thinker: 学习规划和行动

    Thinker: Learning to Plan and Act. (arXiv:2307.14993v1 [cs.AI])

    [http://arxiv.org/abs/2307.14993](http://arxiv.org/abs/2307.14993)

    Thinker算法通过引入世界模型和模型交互动作使强化学习代理实现自主规划，消除了手工设计规划算法的需求，并且在Sokoban游戏和Atari 2600基准测试中取得了state-of-the-art的性能。

    

    我们提出了Thinker算法，一种新颖的方法，使强化学习代理能够自主地与学习的世界模型进行交互并利用其。 Thinker算法通过给环境添加世界模型来改变环境，并引入了用于与世界模型交互的新动作。这些模型交互动作使代理能够通过在选择最终的环境动作之前向世界模型提出备选计划来进行规划。该方法通过使代理自主学习如何进行规划来消除了手工设计的规划算法的需求，并且允许对代理的计划进行易于解释的可视化。我们通过Sokoban游戏和Atari 2600基准测试的实验结果证明了该算法的有效性，其中Thinker算法分别实现了最先进的性能和有竞争力的结果。使用Thinker算法训练的代理的可视化结果表明，它们已经学到了优秀的规划策略。

    We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for hand-crafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have lear
    
[^147]: 通过原位无模型优化实现高性能真实光学计算

    High-performance real-world optical computing trained by in situ model-free optimization. (arXiv:2307.11957v1 [physics.optics] CROSS LISTED)

    [http://arxiv.org/abs/2307.11957](http://arxiv.org/abs/2307.11957)

    本论文提出了一种无模型优化光学计算系统的方法，通过在原位进行轻量级优化，实现了高性能的真实光学计算。实验证明该方法在分类准确度上优于传统方法，并展示了在高速细胞分析方面的潜力。这种方法的固有简单性和低需求的计算资源促进了光学计算技术从实验室研究向真实世界应用的转变。

    

    光学计算系统可以提供高速和低能耗的数据处理，但在计算密集的训练和从模拟到现实的转换中存在不足。我们提出了一种基于得分梯度估计算法的轻量级原位优化光学计算系统的无模型解决方案。该方法将系统视为黑盒子，直接将损失反向传播到光学权重的概率分布，从而避免了对计算密集和有偏见的系统模拟的需求。通过在单层衍射光学计算系统上进行实验证明在MNIST和FMNIST数据集上具有优越的分类准确度。此外，我们展示了其在无图片和高速细胞分析方面的潜力。我们提出的方法的固有简单性，结合其对计算资源的低需求，加速了光学计算从实验室演示到真实世界应用的过渡。

    Optical computing systems can provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and simulation-to-reality gap. We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score gradient estimation algorithm. This approach treats the system as a black box and back-propagates loss directly to the optical weights' probabilistic distributions, hence circumventing the need for computation-heavy and biased system simulation. We demonstrate a superior classification accuracy on the MNIST and FMNIST datasets through experiments on a single-layer diffractive optical computing system. Furthermore, we show its potential for image-free and high-speed cell analysis. The inherent simplicity of our proposed method, combined with its low demand for computational resources, expedites the transition of optical computing from laboratory demonstrations to real-world applications.
    
[^148]: 在野外的Android：用于Android设备控制的大规模数据集

    Android in the Wild: A Large-Scale Dataset for Android Device Control. (arXiv:2307.10088v1 [cs.LG])

    [http://arxiv.org/abs/2307.10088](http://arxiv.org/abs/2307.10088)

    这个论文提出了一个名为Android in the Wild (AITW)的大规模数据集，用于研究设备控制系统，该数据集包括人类示范的设备交互、自然语言指令和多种Android版本和设备类型。这个数据集提供了一个新的挑战，需要从视觉外观中推断用户界面中可用的操作。

    

    对于能够解释人类自然语言指令并直接控制数字设备用户界面执行的设备控制系统，人们越来越感兴趣。我们提出了一个用于设备控制研究的数据集，Android in the Wild (AITW)，该数据集比当前数据集大几个数量级。该数据集包含了设备交互的人类示范，包括屏幕和操作，以及相应的自然语言指令。它包括715k个剧集，涵盖30k个不同的指令，四个Android版本（v10-13），以及八种不同的设备类型（从Pixel 2 XL到Pixel 6）和不同的屏幕分辨率。它包含需要语言和视觉上下文的语义理解的多步骤任务。这个数据集提出了一个新的挑战：必须从它们的视觉外观中推断出用户界面中可用的操作。而且，行动空间不再是简单的基于用户界面元素的行动，而是包含精确的手势（例如，水平滚动）

    There is a growing interest in device-control systems that can interpret human natural language instructions and execute them on a digital device by directly controlling its user interface. We present a dataset for device-control research, Android in the Wild (AITW), which is orders of magnitude larger than current datasets. The dataset contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. It consists of 715k episodes spanning 30k unique instructions, four versions of Android (v10-13),and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It contains multi-step tasks that require semantic understanding of language and visual context. This dataset poses a new challenge: actions available through the user interface must be inferred from their visual appearance. And, instead of simple UI element-based actions, the action space consists of precise gestures (e.g., horizontal scrolls 
    
[^149]: HYTREL: 基于超图的表格数据表示学习

    HYTREL: Hypergraph-enhanced Tabular Data Representation Learning. (arXiv:2307.08623v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.08623](http://arxiv.org/abs/2307.08623)

    HYTREL是一种表格语言模型，通过使用超图来捕捉表格数据的排列不变性和其他三个结构属性。实证结果表明，HYTREL在四个下游任务中始终优于其他竞争基线模型，并且只需最少的预训练。

    

    在许多下游任务中，预训练在大量表格数据集上的语言模型都证明了其有效性。然而，许多模型并没有考虑到表格数据中存在的行/列排列不变性、分层结构等。为了缓解这些限制，我们提出了一种名为HYTREL的表格语言模型，通过使用超图来捕捉表格数据的排列不变性和其他三个结构属性——其中，表格单元格构成节点，并且在每行、每列和整个表格中共同出现的单元格被用来形成三种不同类型的超边。我们展示了HYTREL在特定条件下对于表格数据是最大不变的，即，两个表格通过HYTREL获得的表示相同，当且仅当这两个表格在排列上是相同的。我们的实证结果表明，HYTREL在四个下游任务中始终优于其他竞争基线模型，并且只需最少的预训练。

    Language models pretrained on large collections of tabular data have demonstrated their effectiveness in several downstream tasks. However, many of these models do not take into account the row/column permutation invariances, hierarchical structure, etc. that exist in tabular data. To alleviate these limitations, we propose HYTREL, a tabular language model, that captures the permutation invariances and three more structural properties of tabular data by using hypergraphs - where the table cells make up the nodes and the cells occurring jointly together in each row, column, and the entire table are used to form three different types of hyperedges. We show that HYTREL is maximally invariant under certain conditions for tabular data, i.e., two tables obtain the same representations via HYTREL iff the two tables are identical up to permutations. Our empirical results demonstrate that HYTREL consistently outperforms other competitive baselines on four downstream tasks with minimal pretraini
    
[^150]: 带有乘法平滑的特征归因稳定性保证

    Stability Guarantees for Feature Attributions with Multiplicative Smoothing. (arXiv:2307.05902v1 [cs.LG])

    [http://arxiv.org/abs/2307.05902](http://arxiv.org/abs/2307.05902)

    本文提出了一种基于乘法平滑的特征归因方法，通过证明模型的Lipschitz性质，保证了其稳定性，并在视觉和语言模型上进行了评估，显示了非平凡的稳定性保证。

    

    机器学习模型的解释方法往往不能提供任何形式的保证，也可能不反映底层的决策过程。在这项工作中，我们将稳定性作为可靠的特征归因方法的一个属性进行分析。我们证明了如果模型在特征屏蔽方面具有足够的Lipschitz性质，则可以保证放松变体的稳定性。为了实现这样的模型，我们开发了一种称为乘法平滑（MuS）的平滑方法。我们展示了MuS克服了标准平滑技术的理论限制，并且可以与任何分类器和特征归因方法结合使用。我们使用各种特征归因方法（如LIME和SHAP）对视觉和语言模型进行了MuS的评估，并展示了MuS赋予了特征归因以非平凡的稳定性保证。

    Explanation methods for machine learning models tend to not provide any formal guarantees and may not reflect the underlying decision-making process. In this work, we analyze stability as a property for reliable feature attribution methods. We prove that relaxed variants of stability are guaranteed if the model is sufficiently Lipschitz with respect to the masking of features. To achieve such a model, we develop a smoothing method called Multiplicative Smoothing (MuS). We show that MuS overcomes theoretical limitations of standard smoothing techniques and can be integrated with any classifier and feature attribution method. We evaluate MuS on vision and language models with a variety of feature attribution methods, such as LIME and SHAP, and demonstrate that MuS endows feature attributions with non-trivial stability guarantees.
    
[^151]: DebateKG: 用语义知识图自动创建政策辩论案例

    DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs. (arXiv:2307.04090v1 [cs.CL])

    [http://arxiv.org/abs/2307.04090](http://arxiv.org/abs/2307.04090)

    本论文提出了一种利用语义知识图自动创建政策辩论案例的方法，通过在争论的语义知识图上进行限制最短路径遍历，有效构建高质量的辩论案例。研究结果表明，在美国竞赛辩论中，利用这种方法显著改进了已有数据集DebateSum，并贡献了新的例子和有用的元数据。通过使用txtai语义搜索和知识图工具链，创建和贡献了9个语义知识图，同时提出了一种独特的评估方法来确定哪个知识图更适合政策辩论案例生成。

    

    近期相关工作表明，自然语言处理系统在解决竞赛辩论中的问题方面具有应用性。竞赛辩论中最重要的任务之一是辩手创建高质量的辩论案例。我们展示了使用限制最短路径遍历在争论的语义知识图上构建有效的辩论案例的方法。我们在一个名为DebateSum的大规模数据集上研究了这种潜力，该数据集针对的是一种名为政策辩论的美国竞赛辩论类型。我们通过向数据集中引入53180个新的例子，并为每个例子提供进一步有用的元数据，显著改进了DebateSum。我们利用txtai语义搜索和知识图工具链基于这个数据集产生并贡献了9个语义知识图。我们创建了一种独特的评估方法，以确定在政策辩论案例生成的背景下哪个知识图更好。

    Recent work within the Argument Mining community has shown the applicability of Natural Language Processing systems for solving problems found within competitive debate. One of the most important tasks within competitive debate is for debaters to create high quality debate cases. We show that effective debate cases can be constructed using constrained shortest path traversals on Argumentative Semantic Knowledge Graphs. We study this potential in the context of a type of American Competitive Debate, called Policy Debate, which already has a large scale dataset targeting it called DebateSum. We significantly improve upon DebateSum by introducing 53180 new examples, as well as further useful metadata for every example, to the dataset. We leverage the txtai semantic search and knowledge graph toolchain to produce and contribute 9 semantic knowledge graphs built on this dataset. We create a unique method for evaluating which knowledge graphs are better in the context of producing policy deb
    
[^152]: 通过得分和流匹配实现无需模拟的Schr\"odinger桥

    Simulation-free Schr\"odinger bridges via score and flow matching. (arXiv:2307.03672v1 [cs.LG])

    [http://arxiv.org/abs/2307.03672](http://arxiv.org/abs/2307.03672)

    [SF]$^2$M是一种无需模拟的方法，用于推断随机动力学。它将连续时间随机生成建模解释为Schr\"odinger桥问题，并通过静态熵正则化最优传输来高效学习。在学习细胞动力学方面表现出更高的准确性和效率。

    

    我们提出了无需模拟的得分和流匹配（[SF]$^2$M），这是一种无需模拟的目标函数，用于推断给定来自任意分布的未配对源和目标样本的随机动力学。我们的方法推广了用于训练扩散模型的得分匹配损失和最近提出的用于训练连续归一化流的流匹配损失。[SF]$^2$M将连续时间随机生成建模解释为Schr\"odinger桥问题。它依赖于静态熵正则化最优传输或小批量近似，以有效地学习Schr\"odinger桥，而无需模拟学习过程。我们发现，与先前的基于模拟的方法相比，[SF]$^2$M更高效并提供了更准确的Schr\"odinger桥解决方案。最后，我们将[SF]$^2$M应用于从快照数据中学习细胞动力学的问题。值得注意的是，[SF]$^2$M是首个能够准确建模高维细胞动力学的方法。

    We present simulation-free score and flow matching ([SF]$^2$M), a simulation-free objective for inferring stochastic dynamics given unpaired source and target samples drawn from arbitrary distributions. Our method generalizes both the score-matching loss used in the training of diffusion models and the recently proposed flow matching loss used in the training of continuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic generative modeling as a Schr\"odinger bridge (SB) problem. It relies on static entropy-regularized optimal transport, or a minibatch approximation, to efficiently learn the SB without simulating the learned stochastic process. We find that [SF]$^2$M is more efficient and gives more accurate solutions to the SB problem than simulation-based methods from prior work. Finally, we apply [SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably, [SF]$^2$M is the first method to accurately model cell dynamics in high dimensions and can 
    
[^153]: 数据高效和高性能医学图像处理的等变球卷积神经网络

    Equivariant Spherical CNN for Data Efficient and High-Performance Medical Image Processing. (arXiv:2307.03298v1 [eess.IV])

    [http://arxiv.org/abs/2307.03298](http://arxiv.org/abs/2307.03298)

    等变球卷积神经网络能够提高医学图像处理的效率和性能，通过降低对特定训练集的依赖性，并且在去噪和重建方面表现出卓越的质量和计算效率。

    

    本研究突出了等变网络作为高效和高性能途径在断层扫描应用中的重要性。我们的研究建立在卷积神经网络（CNN）的局限性之上，CNN已经在各种医学影像系统的后处理中显示出了潜力。然而，传统CNN的效率严重依赖于一个不变和适当的训练集。为了解决这个问题，在本研究中，我们引入了一个等变网络，旨在减少CNN对特定训练集的依赖性。我们评估了等变CNN在球信号上在断层扫描医学成像问题中的有效性。我们的结果表明，球形CNN（SCNN）在去噪和重建基准问题上具有优越的质量和计算效率。此外，我们提出了一种新颖的方法，利用SCNN作为传统图像重建工具的补充，增强结果同时减少对训练集的依赖性。在所有案例中，我们观察到...

    This work highlights the significance of equivariant networks as efficient and high-performance approaches for tomography applications. Our study builds upon the limitations of Convolutional Neural Networks (CNNs), which have shown promise in post-processing various medical imaging systems. However, the efficiency of conventional CNNs heavily relies on an undiminished and proper training set. To tackle this issue, in this study, we introduce an equivariant network, aiming to reduce CNN's dependency on specific training sets. We evaluate the efficacy of equivariant CNNs on spherical signals for tomographic medical imaging problems. Our results demonstrate superior quality and computational efficiency of spherical CNNs (SCNNs) in denoising and reconstructing benchmark problems. Furthermore, we propose a novel approach to employ SCNNs as a complement to conventional image reconstruction tools, enhancing the outcomes while reducing reliance on the training set. Across all cases, we observe
    
[^154]: 通过不连续网络实现深层合同设计

    Deep Contract Design via Discontinuous Networks. (arXiv:2307.02318v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02318](http://arxiv.org/abs/2307.02318)

    本文通过不连续ReLU网络实现了深层合同设计，通过学习代理人和委托人的约束和目标的闭合形式表达，支持并行推断以求解最优合同。

    

    合同设计涉及一个委托人对由代理人的行动产生的结果支付的合同约定。在本文中，我们开始研究深度学习对最优合同自动设计的应用。我们引入了一种新颖的表示方法：不连续ReLU（DeLU）网络，它将委托人的效用建模为合同设计的不连续分段仿射函数，其中每个分段对应于代理人采取特定的行动。DeLU网络通过线性规划或内点方法隐式学习代理人的激励相容约束和委托人的效用最大化目标的闭合形式表达，并支持每个分段的并行推断以求解最优合同。我们提供了经验结果，证明了使用少量训练样本近似委托人效用函数并扩展以找到近似最优合同的成功。

    Contract design involves a principal who establishes contractual agreements about payments for outcomes that arise from the actions of an agent. In this paper, we initiate the study of deep learning for the automated design of optimal contracts. We introduce a novel representation: the Discontinuous ReLU (DeLU) network, which models the principal's utility as a discontinuous piecewise affine function of the design of a contract where each piece corresponds to the agent taking a particular action. DeLU networks implicitly learn closed-form expressions for the incentive compatibility constraints of the agent and the utility maximization objective of the principal, and support parallel inference on each piece through linear programming or interior-point methods that solve for optimal contracts. We provide empirical results that demonstrate success in approximating the principal's utility function with a small number of training samples and scaling to find approximately optimal contracts o
    
[^155]: SageFormer：面向多变量时间序列预测的系列感知图增强Transformer

    SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting. (arXiv:2307.01616v1 [cs.LG])

    [http://arxiv.org/abs/2307.01616](http://arxiv.org/abs/2307.01616)

    本文介绍了SageFormer，一种面向多变量时间序列预测的系列感知图增强Transformer模型，通过图结构有效捕捉和建模序列之间的依赖关系，在表示不同序列中的时间模式和减少序列间冗余信息等方面取得了优越性能。

    

    多变量时间序列预测在各个领域起着至关重要的作用。虽然近期深度学习方法，特别是Transformer，展示了很大的潜力，但在解决跨序列依赖性的重要性问题上仍存在差距。本文介绍了SageFormer，一种系列感知图增强Transformer模型，旨在使用图结构有效捕捉和建模序列之间的依赖关系。SageFormer解决了两个关键挑战：有效地表示不同序列中的时间模式以及减少序列之间的冗余信息。重要的是，所提议的系列感知框架可以无缝集成到现有的基于Transformer的模型中，增强了模型对跨序列依赖性的建模能力。通过对真实世界和合成数据集进行广泛的实验证明，SageFormer相比先前的最先进方法展示出了优越的性能。

    Multivariate time series forecasting plays a critical role in diverse domains. While recent advancements in deep learning methods, especially Transformers, have shown promise, there remains a gap in addressing the significance of inter-series dependencies. This paper introduces SageFormer, a Series-aware Graph-enhanced Transformer model designed to effectively capture and model dependencies between series using graph structures. SageFormer tackles two key challenges: effectively representing diverse temporal patterns across series and mitigating redundant information among series. Importantly, the proposed series-aware framework seamlessly integrates with existing Transformer-based models, augmenting their ability to model inter-series dependencies. Through extensive experiments on real-world and synthetic datasets, we showcase the superior performance of SageFormer compared to previous state-of-the-art approaches.
    
[^156]: 通过线掩模引导的黑盒优化实现宏单元布局

    Macro Placement by Wire-Mask-Guided Black-Box Optimization. (arXiv:2306.16844v1 [cs.LG])

    [http://arxiv.org/abs/2306.16844](http://arxiv.org/abs/2306.16844)

    本文介绍了一种名为WireMask-BBO的黑盒优化框架，通过使用线掩模引导的贪心过程进行宏单元布局，在有效降低HPWL的同时节省大量时间。此方法还可以对现有布局进行微调，改善50%的HPWL。

    

    面对大规模集成（VLSI）技术的发展，芯片布局中的电子设计自动化（EDA）技术面临新的挑战。宏单元布局作为该过程中的重要子问题，试图确定所有宏单元的位置，以最小化半周长线长（HPWL）并避免重叠。先前的方法包括基于打包、分析和强化学习的方法。本文提出了一种新的黑盒优化（BBO）框架（称为WireMask-BBO），通过使用线掩模引导的贪心过程进行目标评估来进行宏单元布局。配备不同的BBO算法，WireMask-BBO在实践中比先前的方法实现了显著的改进，即通过使用更少的时间实现了显著更短的HPWL。此外，它可以通过将其视为初始解来微调现有的布局，从而使HPWL改善多达50%。WireMask-BBO具有引领芯片布局领域的潜力。

    The development of very large-scale integration (VLSI) technology has posed new challenges for electronic design automation (EDA) techniques in chip floorplanning. During this process, macro placement is an important subproblem, which tries to determine the positions of all macros with the aim of minimizing half-perimeter wirelength (HPWL) and avoiding overlapping. Previous methods include packing-based, analytical and reinforcement learning methods. In this paper, we propose a new black-box optimization (BBO) framework (called WireMask-BBO) for macro placement, by using a wire-mask-guided greedy procedure for objective evaluation. Equipped with different BBO algorithms, WireMask-BBO empirically achieves significant improvements over previous methods, i.e., achieves significantly shorter HPWL by using much less time. Furthermore, it can fine-tune existing placements by treating them as initial solutions, which can bring up to 50% improvement in HPWL. WireMask-BBO has the potential to s
    
[^157]: LeanDojo: 检索增强语言模型的定理证明

    LeanDojo: Theorem Proving with Retrieval-Augmented Language Models. (arXiv:2306.15626v1 [cs.LG])

    [http://arxiv.org/abs/2306.15626](http://arxiv.org/abs/2306.15626)

    本文引入了LeanDojo，该工具通过提取Lean的数据，为定理证明研究提供了一个开放源代码的平台。利用LeanDojo的数据，开发了ReProver，它是第一个使用检索增强的语言模型的证明器，可以从庞大的数学库中选择命题，训练成本低，并且只需要一周的GPU训练时间。

    

    大型语言模型（LLM）已经显示出在使用Lean等证明助手证明形式定理方面的潜力。然而，由于私有代码、数据和大量计算要求，现有的方法很难复制或建立在其基础上，这给定理证明的机器学习方法的研究带来了巨大的障碍。本文通过引入LeanDojo来消除这些障碍：一个包含工具包、数据、模型和基准测试的开放源代码的Lean游乐场。LeanDojo从Lean中提取数据，并使得可以通过编程与证明环境进行交互。它包含证明中命题的细粒度注释，为命题选择提供了有价值的数据：这是定理证明中的一个关键瓶颈。利用这些数据，我们开发出了ReProver（检索增强的证明器）：它是第一个使用LLM的证明器，通过检索从庞大的数学库中选择命题。它成本低廉，只需要一周的GPU训练时间。我们的检索器利用了LeanDojo的pro相关功能。

    Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection: a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): the first LLM-based prover that is augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's prog
    
[^158]: 学习在强化学习中调整预训练模型

    Learning to Modulate pre-trained Models in RL. (arXiv:2306.14884v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14884](http://arxiv.org/abs/2306.14884)

    本研究通过联合预训练模型，并评估比较了不同的微调方法，旨在解决强化学习中的灾难性遗忘问题。

    

    强化学习在机器人、游戏和仿真等领域取得了成功。然而，尽管强化学习代理在特定任务中展示了令人印象深刻的能力，但它们对新任务的适应性不足。最近，强化学习中的多任务预训练逐渐受到关注。然而，对预训练模型进行微调时往往会出现灾难性遗忘。也就是说，当在新任务上进行微调时，对预训练任务的性能会下降。为了研究灾难性遗忘现象，我们首先在Meta-World和DMControl两个基准套件的数据集上联合预训练模型。然后，我们评估和比较了自然语言处理中常见的多种微调方法在新任务性能和对预训练任务性能保留方面的表现。

    Reinforcement Learning (RL) has been successful in various domains like robotics, game playing, and simulation. While RL agents have shown impressive capabilities in their specific tasks, they insufficiently adapt to new tasks. In supervised learning, this adaptation problem is addressed by large-scale pre-training followed by fine-tuning to new down-stream tasks. Recently, pre-training on multiple tasks has been gaining traction in RL. However, fine-tuning a pre-trained model often suffers from catastrophic forgetting. That is, the performance on the pre-training tasks deteriorates when fine-tuning on new tasks. To investigate the catastrophic forgetting phenomenon, we first jointly pre-train a model on datasets from two benchmark suites, namely Meta-World and DMControl. Then, we evaluate and compare a variety of fine-tuning methods prevalent in natural language processing, both in terms of performance on new tasks, and how well performance on pre-training tasks is retained. Our study
    
[^159]: TACO：基于时间潜在动作驱动对比损失的视觉强化学习

    TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning. (arXiv:2306.13229v1 [cs.LG])

    [http://arxiv.org/abs/2306.13229](http://arxiv.org/abs/2306.13229)

    本文提出了TACO方法，一种基于时间潜在动作驱动对比损失的视觉强化学习方法，能够同时学习状态表示和动作表示，提高代理学习的效率。

    

    尽管在强化学习（RL）从原始像素数据中取得了最近的进展，但样本效率仍然是一个重要的障碍。先前的工作试图通过创建自监督辅助任务来解决这个挑战，旨在为未来状态预测丰富代理学习的表示与控制相关信息。然而，这些目标通常不足以学习能够表示最优策略或值函数的表示，并且它们通常考虑具有小的抽象离散动作空间的任务，因此忽视了在连续控制中动作表示学习的重要性。在本文中，我们引入了TACO：一种简单而强大的时间对比学习方法，利用它，代理可以同时获得潜在状态和动作表示。TACO通过优化重新获得观察与最近的多个先前观察的相似性，同时学习状态与动作表示。

    Despite recent progress in reinforcement learning (RL) from raw pixel data, sample inefficiency continues to present a substantial obstacle. Prior works have attempted to address this challenge by creating self-supervised auxiliary tasks, aiming to enrich the agent's learned representations with control-relevant information for future state prediction. However, these objectives are often insufficient to learn representations that can represent the optimal policy or value function, and they often consider tasks with small, abstract discrete action spaces and thus overlook the importance of action representation learning in continuous control. In this paper, we introduce TACO: Temporal Action-driven Contrastive Learning, a simple yet powerful temporal contrastive learning approach that facilitates the concurrent acquisition of latent state and action representations for agents. TACO simultaneously learns a state and an action representation by optimizing the mutual information between re
    
[^160]: 拓扑视差：深度感知模型的几何规范说明

    Topological Parallax: A Geometric Specification for Deep Perception Models. (arXiv:2306.11835v1 [cs.LG])

    [http://arxiv.org/abs/2306.11835](http://arxiv.org/abs/2306.11835)

    拓扑视差是一种比较训练模型和参考数据集多尺度几何结构相似性的理论和计算工具，它可以估计模型中的拓扑特征，有助于理解深度学习模型的行为和性能。

    

    为了保证人工智能系统的安全性和鲁棒性，我们引入拓扑视差作为比较已训练模型和参考数据集的多尺度几何结构相似性的理论和计算工具。我们的证明和例子表明，数据集和模型之间的这种几何相似性对于可信的插值和扰动至关重要，并且我们猜测，这个新概念将为深度学习应用中过拟合和泛化之间不明确的关系的当前讨论增添价值。在典型的DNN应用中，模型的显式几何描述是不可能的，但视差可以通过检查使用参考数据集的测地畸变对Rips复合体的影响来估计模型中的拓扑特征（组件、周期、空洞等）。因此，视差指示模型与数据集是否共享类似的多尺度几何特征。视差通过拓扑数据分析理论，提供了从不同角度观察数据的直观概念，并为理解深度感知模型的行为和性能提供了新的视角。

    For safety and robustness of AI systems, we introduce topological parallax as a theoretical and computational tool that compares a trained model to a reference dataset to determine whether they have similar multiscale geometric structure. Our proofs and examples show that this geometric similarity between dataset and model is essential to trustworthy interpolation and perturbation, and we conjecture that this new concept will add value to the current debate regarding the unclear relationship between overfitting and generalization in applications of deep-learning. In typical DNN applications, an explicit geometric description of the model is impossible, but parallax can estimate topological features (components, cycles, voids, etc.) in the model by examining the effect on the Rips complex of geodesic distortions using the reference dataset. Thus, parallax indicates whether the model shares similar multiscale geometric features with the dataset. Parallax presents theoretically via topolo
    
[^161]: IMP-MARL: 一套用于基于MARL的大规模基础设施管理规划的环境套件

    IMP-MARL: a Suite of Environments for Large-scale Infrastructure Management Planning via MARL. (arXiv:2306.11551v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11551](http://arxiv.org/abs/2306.11551)

    IMP-MARL是一套用于大规模基础设施管理规划的开源多智能体强化学习环境套件，旨在提供一个基准平台来评估合作MARL方法在现实工程应用中的可扩展性和性能。

    

    我们介绍了IMP-MARL，这是一套用于大规模基础设施管理规划（IMP）的开源多智能体强化学习（MARL）环境套件，为合作MARL方法在现实工程应用中的可扩展性提供了一个基准平台。在IMP中，多组分工程系统由于其组件的损坏情况存在故障风险。具体而言，每个智能体为一个特定的系统组件进行检查和维修规划，旨在最小化维护成本的同时合作以最小化系统故障风险。通过IMP-MARL，我们发布了几个环境，包括一个与海上风电结构系统相关的环境，以满足改善管理策略以支持可持续和可靠能源系统的现今需求。我们使用最多100个智能体的IMP实际工程环境进行了基准测试，评估了目前合作MARL方法的可扩展性和性能。

    We introduce IMP-MARL, an open-source suite of multi-agent reinforcement learning (MARL) environments for large-scale Infrastructure Management Planning (IMP), offering a platform for benchmarking the scalability of cooperative MARL methods in real-world engineering applications. In IMP, a multi-component engineering system is subject to a risk of failure due to its components' damage condition. Specifically, each agent plans inspections and repairs for a specific system component, aiming to minimise maintenance costs while cooperating to minimise system failure risk. With IMP-MARL, we release several environments including one related to offshore wind structural systems, in an effort to meet today's needs to improve management strategies to support sustainable and reliable energy systems. Supported by IMP practical engineering environments featuring up to 100 agents, we conduct a benchmark campaign, where the scalability and performance of state-of-the-art cooperative MARL methods are
    
[^162]: 智能体的基因

    Genes in Intelligent Agents. (arXiv:2306.10225v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2306.10225](http://arxiv.org/abs/2306.10225)

    该论文提出了基于基因的强化学习（GRL）框架，通过模拟有机体的进化和利用学习基因来学习和演化智能体。实验证明了GRL在智能体训练中的有效性。

    

    在自然界中，基因通过数十亿年的传递和积累，赋予地球上生物的当前生物智能。受到生物智能的启发，人工智能（AI）致力于构建机器智能。尽管取得了繁荣的成功，但机器智能仍远远落后于生物智能。原因可能在于动物天生具有某种基因编码的智能，而机器缺乏此类智能，需要从头学习。受到动物基因的启发，我们定义了机器的“基因”，称为“学习基因”，并提出了遗传增强学习（GRL）。GRL是一个计算框架，模拟了强化学习（RL）中有机体的进化，并利用学习基因来学习和演化智能体。利用GRL，我们首先证明了学习基因采用了智能体神经网络的片段形式，并且可以继承。这通过实验验证了GRL在智能体训练中的有效性。

    The genes in nature give the lives on earth the current biological intelligence through transmission and accumulation over billions of years. Inspired by the biological intelligence, artificial intelligence (AI) has devoted to building the machine intelligence. Although it has achieved thriving successes, the machine intelligence still lags far behind the biological intelligence. The reason may lie in that animals are born with some intelligence encoded in their genes, but machines lack such intelligence and learn from scratch. Inspired by the genes of animals, we define the ``genes'' of machines named as the ``learngenes'' and propose the Genetic Reinforcement Learning (GRL). GRL is a computational framework that simulates the evolution of organisms in reinforcement learning (RL) and leverages the learngenes to learn and evolve the intelligence agents. Leveraging GRL, we first show that the learngenes take the form of the fragments of the agents' neural networks and can be inherited a
    
[^163]: 实用的锐度感知优化算法不能全程向最优点收敛

    Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima. (arXiv:2306.09850v1 [cs.LG])

    [http://arxiv.org/abs/2306.09850](http://arxiv.org/abs/2306.09850)

    该研究揭示了实用的锐度感知优化算法在某些情况下不能够全程向最优点收敛。

    

    锐度感知优化(SAM)是一种优化器，它基于当前点$x_t$的梯度，在扰动$y_t=x_t+\rho\frac{\nabla f(x_t)}{\lVert\nabla f(x_t)\rVert}$处进行下降。现有研究证明了SAM对于平滑函数的收敛性，但是它们假设扰动的大小$\rho$逐渐衰减和/或在$y_t$中没有梯度归一化，这与实践不符。为了弥补这一差距，我们研究了具有实用配置（即常数$\rho$和$y_t$中的梯度归一化）的确定性/随机版本的SAM，并探讨了它们在具有（非）凸性假设的平滑函数上的收敛性质。令人惊讶的是，在许多情况下，我们发现SAM在收敛到全局最小值或稳定点方面具有有限的能力。对于平滑强凸函数，我们展示了确定性SAM具有严格的全局收敛率为$\tilde\Theta(\frac{1}{T^2})$，而随机SAM的收敛界则受到噪声水平降低的影响，这表明了平面目标表面的尖锐度和平缓性之间平衡的挑战。

    Sharpness-Aware Minimization (SAM) is an optimizer that takes a descent step based on the gradient at a perturbation $y_t = x_t + \rho \frac{\nabla f(x_t)}{\lVert \nabla f(x_t) \rVert}$ of the current point $x_t$. Existing studies prove convergence of SAM for smooth functions, but they do so by assuming decaying perturbation size $\rho$ and/or no gradient normalization in $y_t$, which is detached from practice. To address this gap, we study deterministic/stochastic versions of SAM with practical configurations (i.e., constant $\rho$ and gradient normalization in $y_t$) and explore their convergence properties on smooth functions with (non)convexity assumptions. Perhaps surprisingly, in many scenarios, we find out that SAM has limited capability to converge to global minima or stationary points. For smooth strongly convex functions, we show that while deterministic SAM enjoys tight global convergence rates of $\tilde \Theta(\frac{1}{T^2})$, the convergence bound of stochastic SAM suffer
    
[^164]: 块状态变换器

    Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])

    [http://arxiv.org/abs/2306.09539](http://arxiv.org/abs/2306.09539)

    本文提出了一种名为块状态变换器（BST）的混合神经网络层，结合了状态空间模型和块变换器，旨在在语言建模任务中提高性能和可扩展性。实验证明，该模型在语言建模困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，在层级别上具有超过十倍的速度提升。

    

    状态空间模型（SSM）在需要建模长期依赖性并且需要高效扩展到长序列的任务中显示出了惊人的效果。尽管最初是为连续信号设计的，但SSM在视觉和音频等许多任务中表现出了优异的性能；然而，在语言建模任务中，SSM仍然落后于Transformers的性能。在这项工作中，我们提出了一个名为块状态变换器（BST）的混合层，它在内部组合了一个用于长距离上下文化的SSM子层和一个用于短期序列表示的块变换器子层。我们研究了三种不同的、完全可并行的集成SSM和块注意力的变体。我们证明了我们的模型在语言建模的困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，块状态变换器在层级别上具有超过十倍的速度提升。

    State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compa
    
[^165]: Katakomba：用于数据驱动NetHack的工具和基准

    Katakomba: Tools and Benchmarks for Data-Driven NetHack. (arXiv:2306.08772v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08772](http://arxiv.org/abs/2306.08772)

    本论文开发了一个用于数据驱动NetHack的工具和基准库，旨在解决资源、实现和基准方面的障碍。该库提供了ORL社区所熟悉的工作流基础，并附带可靠的评估工具。

    

    NetHack被认为是强化学习研究的前沿，其中基于学习的方法仍需赶上基于规则的解决方案。通过使用类似于机器人技术、推荐系统等领域最新发展中的预先收集的数据集，离线强化学习（ORL）成为突破的一个有希望的方向。最近，一个大规模的NetHack数据集被释放出，虽然这是向前迈出的必要一步，但它尚未在ORL社区中得到广泛应用。在这项工作中，我们认为有三个主要障碍需要克服：资源、实现和基准。为了解决这些问题，我们开发了一个开源库，提供了ORL社区熟悉的工作流基础：预定义的D4RL风格任务，简洁的基准实现，以及可靠的评估工具，并附有与云端同步的配置和日志。

    NetHack is known as the frontier of reinforcement learning research where learning-based methods still need to catch up to rule-based solutions. One of the promising directions for a breakthrough is using pre-collected datasets similar to recent developments in robotics, recommender systems, and more under the umbrella of offline reinforcement learning (ORL). Recently, a large-scale NetHack dataset was released; while it was a necessary step forward, it has yet to gain wide adoption in the ORL community. In this work, we argue that there are three major obstacles for adoption: resource-wise, implementation-wise, and benchmark-wise. To address them, we develop an open-source library that provides workflow fundamentals familiar to the ORL community: pre-defined D4RL-style tasks, uncluttered baseline implementations, and reliable evaluation tools with accompanying configs and logs synced to the cloud.
    
[^166]: 深高斯马尔可夫随机场用于图结构动态系统

    Deep Gaussian Markov Random Fields for Graph-Structured Dynamical Systems. (arXiv:2306.08445v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08445](http://arxiv.org/abs/2306.08445)

    本研究提出了一种基于深度高斯马尔可夫随机场的方法，用于处理具有图结构的动态系统中的状态估计和学习问题。通过利用先验知识和变分推断，我们可以高效地学习到灵活的时空先验，并获得闭式后验计算方法，与传统的卡尔曼方法相比，具有更好的可扩展性。

    

    在高维状态空间模型中进行概率推断是具有挑战性的计算任务。然而，对于许多时空系统，我们可以获得有关状态变量依赖结构的先验知识。我们利用这种结构，提出了一种在具有（部分）未知动力学和有限历史数据的图结构状态空间模型中进行状态估计和学习的计算有效方法。借鉴深度学习与高斯马尔可夫随机场（GMRF）中的有原则的推断的最近方法，我们将图结构状态空间模型重新定义为由简单的空间和时间图层定义的深度GMRF。这产生了一个灵活的时空先验，可以通过变分推断从单个时间序列上高效地学习。在线性高斯假设下，我们保留了一个封闭形式的后验，可以使用共轭梯度法高效地采样，与经典的卡尔曼方法相比，具有可扩展性的优势。

    Probabilistic inference in high-dimensional state-space models is computationally challenging. For many spatiotemporal systems, however, prior knowledge about the dependency structure of state variables is available. We leverage this structure to develop a computationally efficient approach to state estimation and learning in graph-structured state-space models with (partially) unknown dynamics and limited historical data. Building on recent methods that combine ideas from deep learning with principled inference in Gaussian Markov random fields (GMRF), we reformulate graph-structured state-space models as Deep GMRFs defined by simple spatial and temporal graph layers. This results in a flexible spatiotemporal prior that can be learned efficiently from a single time sequence via variational inference. Under linear Gaussian assumptions, we retain a closed-form posterior, which can be sampled efficiently using the conjugate gradient method, scaling favourably compared to classical Kalman 
    
[^167]: 通过正交微调控制文本到图像的扩散

    Controlling Text-to-Image Diffusion by Orthogonal Finetuning. (arXiv:2306.07280v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.07280](http://arxiv.org/abs/2306.07280)

    本文介绍了一种名为正交微调（OFT）的方法，可以有效地引导和控制大型文本到图像扩散模型，以执行不同的下游任务。我们还提出了约束正交微调（COFT），来提高微调的稳定性。这些方法能够保持语义生成能力并生成特定主题的图像。

    

    大型文本到图像扩散模型在生成真实感图像方面有很强的能力。如何有效地引导或控制这些强大的模型以执行不同的下游任务成为一个重要的开放性问题。为了解决这个挑战，我们引入了一种基于原则的微调方法——正交微调（OFT），用于将文本到图像扩散模型调整到下游任务中。与现有方法不同，OFT可以证明地保持特征对神经元在单位超球面上的关系所表征的超球形能量。我们发现，这种属性对于保持文本到图像扩散模型的语义生成能力非常关键。为了提高微调的稳定性，我们进一步提出了约束正交微调（COFT），它对超球面施加了额外的半径约束。具体来说，我们考虑了两个重要的微调文本到图像任务：主题驱动生成，目标是生成特定主题的图像

    Large text-to-image diffusion models have impressive capabilities in generating photorealistic images from text prompts. How to effectively guide or control these powerful models to perform different downstream tasks becomes an important open problem. To tackle this challenge, we introduce a principled finetuning method -- Orthogonal Finetuning (OFT), for adapting text-to-image diffusion models to downstream tasks. Unlike existing methods, OFT can provably preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. We find that this property is crucial for preserving the semantic generation ability of text-to-image diffusion models. To improve finetuning stability, we further propose Constrained Orthogonal Finetuning (COFT) which imposes an additional radius constraint to the hypersphere. Specifically, we consider two important finetuning text-to-image tasks: subject-driven generation where the goal is to generate subject-specific images
    
[^168]: 用函数逼近解决强化学习中重尾奖励问题的极小最大化算法和实例相关遗憾度量

    Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds. (arXiv:2306.06836v1 [cs.LG])

    [http://arxiv.org/abs/2306.06836](http://arxiv.org/abs/2306.06836)

    本文解决了强化学习中当奖励呈“重尾”分布时的问题，提出了第一种处理这种情况的实例相关算法，并得到了极小最大化的遗憾界。

    

    虽然有许多工作都专注于为有界奖励的强化学习设计有效算法，但当奖励呈现“重尾”分布时——即存在某个 $\epsilon\in(0,1]$ 使得仅有有限的$(1+\epsilon)$-阶矩——是否存在对大状态-动作空间进行采样或时效性算法仍然是一个未解决的问题。 在本文中，我们解决了具有线性函数逼近的 RL 中的这种奖励机制的挑战。我们首先为重尾线性赌臂设计了一种算法——\textsc{Heavy-OFUL}，其实现了一种实例相关的 $T$-round 遗憾度量，为 $\tilde{O}\big(d T^{\frac{1-\epsilon}{2(1+\epsilon)}} \sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\big)$，这是这种类型的\emph{第一篇}文章。$\nu_t^{1+\epsilon}$是第 $t$ 轮奖励的 $(1+\epsilon)$-阶中心矩。我们进一步证明了在应用于 st 的最坏情况时，上述界是极小值的最优解。

    While numerous works have focused on devising efficient algorithms for reinforcement learning (RL) with uniformly bounded rewards, it remains an open question whether sample or time-efficient algorithms for RL with large state-action space exist when the rewards are \emph{heavy-tailed}, i.e., with only finite $(1+\epsilon)$-th moments for some $\epsilon\in(0,1]$. In this work, we address the challenge of such rewards in RL with linear function approximation. We first design an algorithm, \textsc{Heavy-OFUL}, for heavy-tailed linear bandits, achieving an \emph{instance-dependent} $T$-round regret of $\tilde{O}\big(d T^{\frac{1-\epsilon}{2(1+\epsilon)}} \sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\big)$, the \emph{first} of this kind. Here, $d$ is the feature dimension, and $\nu_t^{1+\epsilon}$ is the $(1+\epsilon)$-th central moment of the reward at the $t$-th round. We further show the above bound is minimax optimal when applied to the worst-case instances in st
    
[^169]: 使用改进的RVQGAN进行高保真音频压缩

    High-Fidelity Audio Compression with Improved RVQGAN. (arXiv:2306.06546v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2306.06546](http://arxiv.org/abs/2306.06546)

    本论文介绍了一种使用改进的RVQGAN进行高保真音频压缩的算法，在保持音频质量的情况下实现了90倍的压缩。这种算法结合了高质量的音频生成和图像领域的向量量化技术，同时改进了对抗性和重建损失。通过使用单一通用模型对所有领域进行压缩，该算法在音频生成建模中具有广泛的应用价值。与竞争算法相比，该方法表现出更好的性能。

    

    语言模型已成功地用于建模自然信号，如图像、语音和音乐。其中关键组件是一个能将高维自然信号压缩为较低维度离散令牌的高质量神经压缩模型。为此，我们介绍了一种高保真全能神经音频压缩算法，将44.1 KHz音频以8kbps的带宽压缩约90倍。我们将高保真音频生成的进展与图像领域的更好向量量化技术相结合，同时改进对抗性和重建损失。我们使用单一通用模型对所有领域（语音、环境、音乐等）进行压缩，使其广泛适用于音频的生成建模。我们与竞争的音频压缩算法进行比较，发现我们的方法明显优于它们。我们提供了对每个设计选择进行彻底分析的实验，以及开源代码和经过训练的模型。

    Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio. We compare with competing audio compression algorithms, and find our method outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained m
    
[^170]: 基于Wasserstein的高概率泛化界限下的学习

    Learning via Wasserstein-Based High Probability Generalisation Bounds. (arXiv:2306.04375v1 [stat.ML])

    [http://arxiv.org/abs/2306.04375](http://arxiv.org/abs/2306.04375)

    本文证明了基于Wasserstein距离的PAC-Bayesian泛化界限的新颖性，并提出了算法框架，该界限显著扩展了PAC-Bayesian界限的范围，并在经典的学习问题中展现了改进的泛化误差。

    

    在结构风险最小化（SRM）中，最小化总体风险或泛化差距上限被广泛使用，这尤其是PAC-Bayesian学习的核心。尽管近年来其取得了成功并吸引了越来越多的关注，但PAC-Bayesian框架的局限是大多数界限涉及Kullback-Leibler（KL）散度项（或其变化），这可能表现出不规则行为并无法捕捉学习问题的底层几何结构，因此限制了其在实际应用中的使用。最近的一些研究企图用Wasserstein距离替换PAC-Bayesian界限中的KL散度。即使这些界限在一定程度上缓解了上述问题，但它们要么保持期望，要么对有界损失有效，要么难以在SRM框架中最小化。在这项工作中，我们为这一研究方向做出了贡献，证明了基于Wasserstein距离的PAC-Bayesian泛化界限的新颖性，并且我们的界限以显著性地扩展了PAC-Bayesian界限的范围，并在几种经典的学习问题中展现了改进的泛化误差。此外，我们提出了一种算法框架，用于学习新的界限，并在各种数据集上展示了有前途的实验结果。

    Minimising upper bounds on the population risk or the generalisation gap has been widely used in structural risk minimisation (SRM) - this is in particular at the core of PAC-Bayesian learning. Despite its successes and unfailing surge of interest in recent years, a limitation of the PAC-Bayesian framework is that most bounds involve a Kullback-Leibler (KL) divergence term (or its variations), which might exhibit erratic behavior and fail to capture the underlying geometric structure of the learning problem - hence restricting its use in practical applications. As a remedy, recent studies have attempted to replace the KL divergence in the PAC-Bayesian bounds with the Wasserstein distance. Even though these bounds alleviated the aforementioned issues to a certain extent, they either hold in expectation, are for bounded losses, or are nontrivial to minimize in an SRM framework. In this work, we contribute to this line of research and prove novel Wasserstein distance-based PAC-Bayesian ge
    
[^171]: 在表面之下寻找：利用基本对称性实现高效离线强化学习

    Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])

    [http://arxiv.org/abs/2306.04220](http://arxiv.org/abs/2306.04220)

    本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。

    

    离线强化学习通过从预先收集的数据集中学习策略来解决与环境交互的实际问题。然而，现有的离线强化学习算法的性能严重依赖于数据集的规模和状态-动作空间覆盖范围。真实世界数据的收集通常是昂贵和难以控制的，导致数据集小且覆盖范围狭窄，从而对离线强化学习的实际部署提出了重大挑战。在本文中，我们提供了一个新的见解，即利用系统动力学的基本对称性可以在小数据集下显著提高离线强化学习的性能。具体来说，我们提出了一个时间反演对称(T-symmetry)强制的动力学模型(TDM)，建立了一对正向和反向潜在动力学之间的一致性。TDM为小数据集提供了良好的表示，并基于T-symmetry的符合性提供了一种新的OOD样本的可靠性度量。

    Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
    
[^172]: FAMO：快速自适应多任务优化

    FAMO: Fast Adaptive Multitask Optimization. (arXiv:2306.03792v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.03792](http://arxiv.org/abs/2306.03792)

    FAMO是一种快速自适应多任务优化方法，通过动态加权方式实现平衡的任务损失减少，相比最先进的梯度操作技术具有相似或更优的性能。

    

    人工智能的一个长久的目标是创建能够通过多任务学习从多样化数据中学习多个不同任务的通用代理。然而，在实践中，对所有任务的平均损失应用梯度下降可能会由于某些任务的严重欠优化而导致较差的多任务性能。以往的方法通过操纵任务梯度以获得更平衡的损失减少，但需要存储和计算所有任务的梯度（时间和空间复杂度为O(k)，其中k是任务数量），限制了它们在大规模场景中的使用。在这项工作中，我们引入了快速自适应多任务优化（FAMO），一种动态加权方法，以O(1)的时间和空间复杂度以平衡的方式减少任务损失。我们进行了一系列广泛的实验，涵盖了多任务监督学习和强化学习问题。我们的结果表明，FAMO在性能上达到了与最先进的梯度操作技术相媲美或更优的水平。

    One of the grand enduring goals of AI is to create generalist agents that can learn multiple different tasks from diverse data via multitask learning (MTL). However, in practice, applying gradient descent (GD) on the average loss across all tasks may yield poor multitask performance due to severe under-optimization of certain tasks. Previous approaches that manipulate task gradients for a more balanced loss decrease require storing and computing all task gradients ($\mathcal{O}(k)$ space and time where $k$ is the number of tasks), limiting their use in large-scale scenarios. In this work, we introduce Fast Adaptive Multitask Optimization FAMO, a dynamic weighting method that decreases task losses in a balanced way using $\mathcal{O}(1)$ space and time. We conduct an extensive set of experiments covering multi-task supervised and reinforcement learning problems. Our results indicate that FAMO achieves comparable or superior performance to state-of-the-art gradient manipulation technique
    
[^173]: 随机特征回归中贝叶斯不确定性估计的渐近性

    Asymptotics of Bayesian Uncertainty Estimation in Random Features Regression. (arXiv:2306.03783v1 [stat.ML])

    [http://arxiv.org/abs/2306.03783](http://arxiv.org/abs/2306.03783)

    论文比较和对比了后验预测分布和最大后验估计的风险，重点关注了模型维度增长速度大于任何常数倍的样本数时它们之间的渐近一致性。数值模拟表明这两个数量在限定维度上具有高斯波动，并表现出相似的属性。

    

    本文比较和对比了贝叶斯回归模型中后验预测分布和最大后验估计（MAP）风险在超参数化区域中的行为。我们将重点关注后验预测分布（贝叶斯模型平均）的方差，并将其渐近性与MAP估计器的风险进行比较。当模型维度增长速度大于任何常数倍的样本数时，它们之间的渐近一致性受到信噪比的相变的控制。当样本数增长速度大于任何常数倍的模型维度时，它们也会渐近一致。数值模拟说明了两个数量的有限维分布性质。我们推测它们具有高斯波动，并表现出与之前在高斯序列模型中发现的类似属性。

    In this paper we compare and contrast the behavior of the posterior predictive distribution to the risk of the maximum a posteriori estimator for the random features regression model in the overparameterized regime. We will focus on the variance of the posterior predictive distribution (Bayesian model average) and compare its asymptotics to that of the risk of the MAP estimator. In the regime where the model dimensions grow faster than any constant multiple of the number of samples, asymptotic agreement between these two quantities is governed by the phase transition in the signal-to-noise ratio. They also asymptotically agree with each other when the number of samples grow faster than any constant multiple of model dimensions. Numerical simulations illustrate finer distributional properties of the two quantities for finite dimensions. We conjecture they have Gaussian fluctuations and exhibit similar properties as found by previous authors in a Gaussian sequence model, which is of inde
    
[^174]: 通过重新思考民间威斯费勒-莱曼算法，实现$O(n^2)$空间内任意表达能力的GNNs

    Towards Arbitrarily Expressive GNNs in $O(n^2)$ Space by Rethinking Folklore Weisfeiler-Lehman. (arXiv:2306.03266v1 [cs.LG])

    [http://arxiv.org/abs/2306.03266](http://arxiv.org/abs/2306.03266)

    本文提出了$(k, t)$-FWL和$k$-FWL+两种方法，理论上证明了它们可以在$O(n^2)$的空间复杂度下，解决图同构问题。

    

    近年来，消息传递神经网络（MPNNs）已成为图神经网络（GNNs）中最受欢迎的框架。然而，其表达能力受到一维威斯费勒-莱曼（1-WL）测试的限制。一些研究受到$k$-WL/FWL（民间WL）的启发并设计其相应的神经版本。尽管具有很高的表达能力，但这一研究方向存在严重局限性。为解决这些问题，作者提出了$(k, t)$-FWL和$k$-FWL+，并在理论上证明了它们的有效性。

    Message passing neural networks (MPNNs) have emerged as the most popular framework of graph neural networks (GNNs) in recent years. However, their expressive power is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Some works are inspired by $k$-WL/FWL (Folklore WL) and design the corresponding neural versions. Despite the high expressive power, there are serious limitations in this line of research. In particular, (1) $k$-WL/FWL requires at least $O(n^k)$ space complexity, which is impractical for large graphs even when $k=3$; (2) The design space of $k$-WL/FWL is rigid, with the only adjustable hyper-parameter being $k$. To tackle the first limitation, we propose an extension, $(k, t)$-FWL. We theoretically prove that even if we fix the space complexity to $O(n^2)$ in $(k, t)$-FWL, we can construct an expressiveness hierarchy up to solving the graph isomorphism problem. To tackle the second problem, we propose $k$-FWL+, which considers any equivariant set as neighbors ins
    
[^175]: 合并模型时如何解决干扰的问题

    Resolving Interference When Merging Models. (arXiv:2306.01708v1 [cs.LG])

    [http://arxiv.org/abs/2306.01708](http://arxiv.org/abs/2306.01708)

    本文揭示了现有模型合并技术存在的干扰问题，提出了具有广泛适用性的解决方案，可显着提高合并后模型的性能。

    

    迁移学习可以在下游任务中进一步微调预训练模型，从而获得显著的优势，包括改进下游性能，加快收敛速度和提高样本效率。然而，已有的模型合并技术往往忽视了不同模型参数之间的干扰，导致合并多个模型时性能大幅下降。本文证明，先前的合并技术由于两个主要干扰来源而不慎丢失有价值的信息：(a)冗余参数值引起的干扰和(b)表示同一参数值的符号在不同模型中的差异。

    Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To 
    
[^176]: 带曲率敏感模型的连续性结果的部分反事实识别

    Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model. (arXiv:2306.01424v1 [stat.ML])

    [http://arxiv.org/abs/2306.01424](http://arxiv.org/abs/2306.01424)

    本文研究了连续性结果的部分反事实识别问题，并提出了一种新颖的敏感性模型——曲率敏感模型，通过限制函数级集的曲率来获得信息边界。

    

    反事实推断旨在回答“如果”问题，因此属于Pearl因果关系阶梯中最精细的推理类型。现有的针对具有连续结果的反事实推断方法旨在进行点识别，因此对基础结构因果模型进行了强有力且不自然的假设。在本文中，我们放宽了这些假设，旨在进行连续结果的部分反事实识别，即当反事实查询存在具有信息边界的无知区间中时。我们证明了，在一般情况下，即使是连续可微的结构因果模型函数的级集的曲率也是非信息的，反事实查询的无知区间也是非信息的。因此，我们提出了一种新颖的敏感性模型称为曲率敏感模型来解决这个问题。它允许我们通过限制函数级集的曲率来获得信息边界。我们进一步展示了现有的点反事实识别方法可以视为我们提出框架的特定情况。

    Counterfactual inference aims to answer retrospective ''what if'' questions and thus belongs to the most fine-grained type of inference in Pearl's causality ladder. Existing methods for counterfactual inference with continuous outcomes aim at point identification and thus make strong and unnatural assumptions about the underlying structural causal model. In this paper, we relax these assumptions and aim at partial counterfactual identification of continuous outcomes, i.e., when the counterfactual query resides in an ignorance interval with informative bounds. We prove that, in general, the ignorance interval of the counterfactual queries has non-informative bounds, already when functions of structural causal models are continuously differentiable. As a remedy, we propose a novel sensitivity model called Curvature Sensitivity Model. This allows us to obtain informative bounds by bounding the curvature of level sets of the functions. We further show that existing point counterfactual ide
    
[^177]: 有损可观察性下的高效强化学习：学习在延迟和缺失环境中做出决策

    Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations. (arXiv:2306.01243v1 [cs.LG])

    [http://arxiv.org/abs/2306.01243](http://arxiv.org/abs/2306.01243)

    本文提出了在延迟和缺失状态观察环境中进行高效强化学习的理论研究，并建立了接近最优的遗憾边界，尽管有损可观察性对策略和规划的类别造成了重大挑战，但学习仍然是高效的。

    

    在现实世界的强化学习系统中，各种形式的有损可观察性可能会使问题变得复杂。当代理由于延迟或丢失的通道而无法观察到系统的最新状态时，这些情况会出现，但代理仍必须做出实时决策。本文介绍了对在控制系统中进行高效强化学习进行理论研究，其中代理必须在延迟和缺失状态观察环境中操作。我们建立了接近最优的遗憾边界，形式为$ \tilde{\mathcal{O}}(\sqrt{{\rm poly}(H) SAK}) $，以在延迟和缺失观察设置下实现强化学习。尽管有损可观察性对策略和规划的类别造成了重大挑战，但我们的结果表明学习仍然是高效的，遗憾边界最优地依赖于原始系统的状态-动作大小。此外，我们比较了受影响的观察下最佳策略的性能与最优值之间的差异。

    In real-world reinforcement learning (RL) systems, various forms of impaired observability can complicate matters. These situations arise when an agent is unable to observe the most recent state of the system due to latency or lossy channels, yet the agent must still make real-time decisions. This paper introduces a theoretical investigation into efficient RL in control systems where agents must act with delayed and missing state observations. We establish near-optimal regret bounds, of the form $\tilde{\mathcal{O}}(\sqrt{{\rm poly}(H) SAK})$, for RL in both the delayed and missing observation settings. Despite impaired observability posing significant challenges to the policy class and planning, our results demonstrate that learning remains efficient, with the regret bound optimally depending on the state-action size of the original system. Additionally, we provide a characterization of the performance of the optimal policy under impaired observability, comparing it to the optimal val
    
[^178]: “思维克隆：通过模仿人类思维学习思考并行动”。（arXiv:2306.00323v1 [cs.AI]）

    Thought Cloning: Learning to Think while Acting by Imitating Human Thinking. (arXiv:2306.00323v1 [cs.AI])

    [http://arxiv.org/abs/2306.00323](http://arxiv.org/abs/2306.00323)

    本论文提出了一种新的模仿学习框架“思维克隆”，通过学习人类的思维来训练AI代理，以在泛化、探索、规划等能力方面实现更好的表现。

    

    语言通常被认为是人类思维的一个关键方面，它为我们提供了非凡的泛化、探索、规划、重新规划和适应新情况的能力。然而，强化学习（RL）代理在这些能力中远未达到人类水平的表现。我们假设其中一个认知缺陷的原因是他们缺乏使用语言思考所带来的好处。我们认为通过训练AI代理人像人类一样思考，可以改善其性能。我们引入了一种新的模仿学习框架“思维克隆”，其想法不仅是克隆人类示范者的行为，而且还包括人类在执行这些行为时所产生的想法。虽然我们希望“思维克隆”在处理网络规模的人类思维和行为数据时能够发挥出色（例如，带有剧本的在线视频），但在这里，我们进行了在思考和行动数据为合成生成的领域的实验。结果显示，“思维克隆”学习速度比传统的强化学习方法快得多。

    Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to think like humans do. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, but also the thoughts humans have as they perform these behaviors. While we expect Thought Cloning to truly shine at scale on internet-sized datasets of humans thinking out loud while acting (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than
    
[^179]: 截断亲和力最大化：用于图形异常监测的单类同型建模

    Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v1 [cs.SI])

    [http://arxiv.org/abs/2306.00006](http://arxiv.org/abs/2306.00006)

    本文针对图形异常监测数据集中存在的一类同型现象，提出了一种新的无监督异常评分度量——当前节点亲和力，并通过学习量身定制的节点表示，实现了截断亲和力最大化（TAM）方法，优化在原始图形结构上进行，能够有效进行双重One-Class的GAD。

    

    我们在现实世界的图形异常监测（GAD）数据集中经常发现一种普遍的属性......本文提出了一种新的无监督异常评分度量 - 当前节点亲和力......我们进一步提出了截断亲和力最大化 (TAM)，该方法通过最大化与_neighbors的本地亲和力来学习量身定制的节点表示。本文所提方法在原始图形结构上进行优化，可以进行双重One-Class的GAD。

    One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be bi
    
[^180]: 用于剂量组合的可靠脱机学习

    Reliable Off-Policy Learning for Dosage Combinations. (arXiv:2305.19742v1 [cs.LG])

    [http://arxiv.org/abs/2305.19742](http://arxiv.org/abs/2305.19742)

    本文提出了一种用于剂量组合的新颖可靠的脱机学习方法，通过三个步骤实现：开发神经网络估计个性化的剂量-反应，估计倾向得分检测共享协变量-治疗空间中的重叠有限区域，然后基于梯度的学习算法找到最佳的个性化剂量组合。

    

    个性化医学领域的决策制定，如癌症治疗或危重护理，通常必须对剂量组合进行选择，即多种连续治疗。现有的这项任务的工作已经独立地建模了多种治疗的效果，而估计联合效果却受到了很少的关注，并且面临着非平凡的挑战。在本文中，我们提出了一种新颖的方法，用于剂量组合的可靠脱机学习。我们的方法分为三个步骤：（1）我们开发了一个特定的神经网络，估计个性化的剂量-反应函数，同时考虑多个相关剂量的联合效应。（2）我们使用条件正态化流量估计广义倾向得分，以检测共享协变量-治疗空间中重叠有限的区域。（3）我们提供一种基于梯度的学习算法，以找到最佳的个性化剂量组合。在此，我们确保可靠地估计策略价值。

    Decision-making in personalized medicine such as cancer therapy or critical care must often make choices for dosage combinations, i.e., multiple continuous treatments. Existing work for this task has modeled the effect of multiple treatments independently, while estimating the joint effect has received little attention but comes with non-trivial challenges. In this paper, we propose a novel method for reliable off-policy learning for dosage combinations. Our method proceeds along three steps: (1) We develop a tailored neural network that estimates the individualized dose-response function while accounting for the joint effect of multiple dependent dosages. (2) We estimate the generalized propensity score using conditional normalizing flows in order to detect regions with limited overlap in the shared covariate-treatment space. (3) We present a gradient-based learning algorithm to find the optimal, individualized dosage combinations. Here, we ensure reliable estimation of the policy val
    
[^181]: 插件化表现优化

    Plug-in Performative Optimization. (arXiv:2305.18728v1 [cs.LG])

    [http://arxiv.org/abs/2305.18728](http://arxiv.org/abs/2305.18728)

    研究了一种可能“规范不正确”模型的通用协议，“插件式表现优化”。

    

    当预测具有表现性时，选择哪个预测器部署将影响未来观测的分布。在表现性学习中，总体目标是找到具有低“表现性风险”的预测器，即在其引导的分布上表现良好。最优化表现性风险的一系列解决方案，包括赌徒算法和其他无导数方法，在表现性反馈中不知道任何结构，导致收敛速度极慢。补充的一系列解决方案利用反馈中的显式“模型”，例如战略分类中的最佳响应模型，可以实现更快的速率。然而，这些速率关键依赖于反馈模型的规范。在本研究中，我们启动了对在表现性预测中使用可能的“规范不正确”模型的研究。我们研究了一种使用模型的通用协议，称为“插件式表现优化”。

    When predictions are performative, the choice of which predictor to deploy influences the distribution of future observations. The overarching goal in learning under performativity is to find a predictor that has low \emph{performative risk}, that is, good performance on its induced distribution. One family of solutions for optimizing the performative risk, including bandits and other derivative-free methods, is agnostic to any structure in the performative feedback, leading to exceedingly slow convergence rates. A complementary family of solutions makes use of explicit \emph{models} for the feedback, such as best-response models in strategic classification, enabling significantly faster rates. However, these rates critically rely on the feedback model being well-specified. In this work we initiate a study of the use of possibly \emph{misspecified} models in performative prediction. We study a general protocol for making use of models, called \emph{plug-in performative optimization}, a
    
[^182]: 基于野外视频预训练的上下文化世界模型用于强化学习

    Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning. (arXiv:2305.18499v1 [cs.CV])

    [http://arxiv.org/abs/2305.18499](http://arxiv.org/abs/2305.18499)

    本文研究了基于野外视频预训练的上下文化世界模型（ContextWM）用于强化学习。ContextWM采用上下文化扩展的潜在动态模型建模，从而可以更好地泛化不同场景之间的知识转移，提高下游视觉控制任务的样本效率和控制性能。

    

    利用大规模多样化的数据集进行的无监督预训练已在各种领域取得了巨大成功。最近的工作调查了这种无监督预训练方法在基于模型的强化学习（MBRL）中的应用，但仅限于特定领域或模拟数据。本文研究了使用大量野外视频进行预训练世界模型，以高效学习下游视觉控制任务的问题。然而，野外视频存在各种上下文因素，如错综复杂的背景和纹理外观，这使得世界模型无法提取共享的世界知识以更好地泛化。为了解决这个问题，我们引入了上下文化世界模型（ContextWM），显式地对上下文和动态进行建模，以克服野外视频的复杂性和多样性，并促进不同场景之间的知识转移。具体而言，使用上下文化扩展的潜在动态模型来捕捉高级状态和低级观察之间的上下文依赖关系。通过使用野外视频对ContextWM进行预训练，我们展示了与从头开始训练相比，下游视觉控制任务的样本效率和控制性能均得到了显着提高。

    Unsupervised pre-training methods utilizing large and diverse datasets have achieved tremendous success across a range of domains. Recent work has investigated such unsupervised pre-training methods for model-based reinforcement learning (MBRL) but is limited to domain-specific or simulated data. In this paper, we study the problem of pre-training world models with abundant in-the-wild videos for efficient learning of downstream visual control tasks. However, in-the-wild videos are complicated with various contextual factors, such as intricate backgrounds and textured appearance, which precludes a world model from extracting shared world knowledge to generalize better. To tackle this issue, we introduce Contextualized World Models (ContextWM) that explicitly model both the context and dynamics to overcome the complexity and diversity of in-the-wild videos and facilitate knowledge transfer between distinct scenes. Specifically, a contextualized extension of the latent dynamics model is 
    
[^183]: 使用协方差神经网络进行可解释的脑龄预测

    Explainable Brain Age Prediction using coVariance Neural Networks. (arXiv:2305.18370v1 [q-bio.QM])

    [http://arxiv.org/abs/2305.18370](http://arxiv.org/abs/2305.18370)

    本文提出了使用协方差神经网络进行可解释的脑龄预测的框架，可以通过皮质厚度特征捕捉加速老化，并反映出增加的神经疾病或认知障碍的风险。

    

    在计算神经科学中，越来越多的机器学习算法被用于利用脑成像数据为个体提供“脑龄”估计。由于脑龄与实际年龄存在差异（称为“脑龄差”），因此可以捕捉由于不良健康状况导致的加速老化，并因此反映出增加的神经疾病或认知障碍的风险。然而，大多数现有的脑龄预测算法缺乏透明度和方法论依据，限制了其在临床决策支持方面的广泛应用。在本文中，我们利用协方差神经网络 (VNN)来提出一种解剖可解释的框架，利用皮质厚度特征进行脑龄预测。具体而言，我们的脑龄预测框架不仅扩展到阿尔茨海默病 (AD) 中脑龄差的粗略指标，而且我们还提出了两个重要观察。

    In computational neuroscience, there has been an increased interest in developing machine learning algorithms that leverage brain imaging data to provide estimates of "brain age" for an individual. Importantly, the discordance between brain age and chronological age (referred to as "brain age gap") can capture accelerated aging due to adverse health conditions and therefore, can reflect increased vulnerability towards neurological disease or cognitive impairments. However, widespread adoption of brain age for clinical decision support has been hindered due to lack of transparency and methodological justifications in most existing brain age prediction algorithms. In this paper, we leverage coVariance neural networks (VNN) to propose an anatomically interpretable framework for brain age prediction using cortical thickness features. Specifically, our brain age prediction framework extends beyond the coarse metric of brain age gap in Alzheimer's disease (AD) and we make two important obser
    
[^184]: 可扩展模拟推理的流匹配技术

    Flow Matching for Scalable Simulation-Based Inference. (arXiv:2305.17161v1 [cs.LG])

    [http://arxiv.org/abs/2305.17161](http://arxiv.org/abs/2305.17161)

    这篇论文提出了一种基于流匹配技术的后验估计方法，用于模拟推理，通过提供灵活性和可扩展性解决高维问题的挑战，并在引力波推断上取得了可比离散流方法更好的结果。

    

    基于离散规范化流的神经后验估计方法已成为模拟推理（SBI）的成熟工具，但将其扩展到高维问题可能具有挑战性。建立在最近生成建模方面的进展基础上，我们提出了基于流匹配的后验估计（FMPE），一种使用连续规范化流进行SBI的技术。与离散流不同，像扩散模型一样，流匹配允许无约束的架构，提供了用于复杂数据模态的增强灵活性。因此，流匹配实现了精确的密度评估、快速的训练和无缝的可扩展性，使其成为SBI的理想选择。我们展示了FMPE在SBI基准测试上实现了有竞争力的性能，然后在一个具有挑战性的科学问题上展示了其改进的可扩展性：对于引力波推断，FMPE胜过基于相似离散流的方法，在减少训练时间30%的同时，提供了有说服力的结果。

    Neural posterior estimation methods based on discrete normalizing flows have become established tools for simulation-based inference (SBI), but scaling them to high-dimensional problems can be challenging. Building on recent advances in generative modeling, we here present flow matching posterior estimation (FMPE), a technique for SBI using continuous normalizing flows. Like diffusion models, and in contrast to discrete flows, flow matching allows for unconstrained architectures, providing enhanced flexibility for complex data modalities. Flow matching, therefore, enables exact density evaluation, fast training, and seamless scalability to large architectures--making it ideal for SBI. We show that FMPE achieves competitive performance on an established SBI benchmark, and then demonstrate its improved scalability on a challenging scientific problem: for gravitational-wave inference, FMPE outperforms methods based on comparable discrete flows, reducing training time by 30% with substanti
    
[^185]: 有效增强视觉强化学习的样本利用率：以少学更好

    Learning Better with Less: Effective Augmentation for Sample-Efficient Visual Reinforcement Learning. (arXiv:2305.16379v1 [cs.LG])

    [http://arxiv.org/abs/2305.16379](http://arxiv.org/abs/2305.16379)

    本研究发现，对于数据增强在视觉强化学习中的有效性，空间多样性和轻微的困难度不可或缺。并提出了一种新的DA操作——Rand PR，它提供了丰富的空间多样性和最小的困难度，已经在多种数据上得到了有效性验证。

    

    数据增强（DA）是增强视觉强化学习（RL）算法的样本效率的关键技术。值得注意的是，仅使用简单的观察变换就可以在不进行额外辅助表示任务或预训练编码器的情况下获得出色的性能。然而，仍然不清楚DA的哪些属性是实现样本效率视觉RL的有效性的原因。为了调查这个问题并进一步探索DA的潜力，本文进行了全面的实验，评估了DA属性对其有效性的影响，并提供以下见解和改进：（1）对于单个DA操作，我们揭示了充足的空间多样性和轻微的困难度都是不可缺少的。基于这一发现，我们引入了一种新的DA操作——随机PadResize（Rand PR），它提供了丰富的空间多样性和最小的困难度。（2）对于多类型的DA融合方案，增加的DA困难度和不稳定的数据分布

    Data augmentation (DA) is a crucial technique for enhancing the sample efficiency of visual reinforcement learning (RL) algorithms. Notably, employing simple observation transformations alone can yield outstanding performance without extra auxiliary representation tasks or pre-trained encoders. However, it remains unclear which attributes of DA account for its effectiveness in achieving sample-efficient visual RL. To investigate this issue and further explore the potential of DA, this work conducts comprehensive experiments to assess the impact of DA's attributes on its efficacy and provides the following insights and improvements: (1) For individual DA operations, we reveal that both ample spatial diversity and slight hardness are indispensable. Building on this finding, we introduce Random PadResize (Rand PR), a new DA operation that offers abundant spatial diversity with minimal hardness. (2) For multi-type DA fusion schemes, the increased DA hardness and unstable data distribution 
    
[^186]: 文本到图像生成与评估的可视化编程

    Visual Programming for Text-to-Image Generation and Evaluation. (arXiv:2305.15328v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.15328](http://arxiv.org/abs/2305.15328)

    本文提出了两种新颖的可解释/可理解的图像编程框架，用于文本到图像（T2I）的生成和评估。首先，引入了VPGen，一个可解释的逐步T2I生成框架，将T2I生成分解为三个步骤，通过语言模型处理前两个步骤，提供了比端到端模型更强的空间控制能力，并利用了预训练语言模型的世界知识。

    

    随着大型语言模型在许多领域表现出卓越性能，最近的研究采用语言模型作为视觉任务中的视觉模块的控制器。虽然现有的工作集中在为语言模型提供视觉理解能力，但我们提出了两种新颖的可解释/可解释的图像编程框架，用于文本到图像（T2I）的生成和评估。首先，我们引入了VPGen，一种可解释的逐步T2I生成框架，将T2I生成分解为三个步骤：对象/计数生成、布局生成和图像生成。我们使用语言模型处理前两个步骤（对象/计数生成和布局生成），通过在文本布局对上微调它。我们的逐步T2I生成框架提供了比端到端模型更强的空间控制能力，而端到端模型是这个任务的主要方法。此外，我们利用了预训练语言模型的世界知识，克服了以前的布局引导T2I作品的局限。

    As large language models have demonstrated impressive performance in many domains, recent works have adopted language models (LMs) as controllers of visual modules for vision-and-language tasks. While existing work focuses on equipping LMs with visual understanding, we propose two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation. First, we introduce VPGen, an interpretable step-by-step T2I generation framework that decomposes T2I generation into three steps: object/count generation, layout generation, and image generation. We employ an LM to handle the first two steps (object/count generation and layout generation), by finetuning it on text-layout pairs. Our step-by-step T2I generation framework provides stronger spatial control than end-to-end models, the dominant approach for this task. Furthermore, we leverage the world knowledge of pretrained LMs, overcoming the limitation of previous layout-guided T2I works that can on
    
[^187]: 大型语言模型中的实体偏见：一种因果视角

    A Causal View of Entity Bias in (Large) Language Models. (arXiv:2305.14695v1 [cs.CL])

    [http://arxiv.org/abs/2305.14695](http://arxiv.org/abs/2305.14695)

    研究提出了一种结构因果模型（SCM）并提供因果干预技术，以缓解大型语言模型中的实体偏见，从而减少偏见信息，同时保留相似实体的共同预测信息。

    

    实体偏见广泛影响预训练的大型语言模型，导致它们过度依赖（有偏见的）参数化知识来进行不准确的预测。尽管因果相关的方法已经显示出缓解实体偏见的巨大潜力，但在实践中精确估计潜在因果模型的参数仍然很困难，黑盒子的语言模型更无法调整。为了解决这些问题，我们提出了一种特定的结构因果模型（SCM），其参数比较容易估计。在此基础上，我们提出了因果干预技术，以缓解白盒和黑盒设置中的实体偏见。这种因果干预将原始实体与相邻实体一起进行扰动。这种干预减少了与原始实体相关的特定偏向信息，同时仍保留了来自类似实体的足够共同预测信息。

    Entity bias widely affects pretrained (large) language models, causing them to excessively rely on (biased) parametric knowledge to make unfaithful predictions. Although causality-inspired methods have shown great potential to mitigate entity bias, it is hard to precisely estimate the parameters of underlying causal models in practice. The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits. To address these problems, we propose a specific structured causal model (SCM) whose parameters are comparatively easier to estimate. Building upon this SCM, we propose causal intervention techniques to mitigate entity bias for both white-box and black-box settings. The proposed causal intervention perturbs the original entity with neighboring entities. This intervention reduces specific biasing information pertaining to the original entity while still preserving sufficient common predictive information from similar entities. 
    
[^188]: 盲反问题的块坐标插入和播放方法

    Block Coordinate Plug-and-Play Methods for Blind Inverse Problems. (arXiv:2305.12672v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.12672](http://arxiv.org/abs/2305.12672)

    该论文提出了一种新的块坐标插入和播放（BC-PnP）方法，通过引入学习的去噪器作为先验，高效地解决盲反问题。该方法在非凸数据适应度项和扩张去噪器的条件下收敛到一个隐式函数的稳定点。

    

    插入和播放（PnP）先验是一类解决成像反问题的方法，通过计算物理测量模型和学习图像去噪器的组合运算符的不动点来实现。虽然PnP方法已广泛用于已知测量算子的图像恢复，但在解决盲反问题方面的研究很少。我们通过提出一种新的块坐标PnP（BC-PnP）方法来填补这一空白，该方法通过引入学习的去噪器作为未知图像和未知测量算子的先验来高效地解决这个联合估计问题。我们提出了一种与盲反问题兼容的BC-PnP收敛理论，该理论考虑了非凸数据适应度项和扩张去噪器。我们的理论分析了BC-PnP收敛到与近似的最小均方误差（MMSE）去噪器相关联的隐式函数的稳定点。我们在两个盲反问题上进行了数值验证。

    Plug-and-play (PnP) prior is a well-known class of methods for solving imaging inverse problems by computing fixed-points of operators combining physical measurement models and learned image denoisers. While PnP methods have been extensively used for image recovery with known measurement operators, there is little work on PnP for solving blind inverse problems. We address this gap by presenting a new block-coordinate PnP (BC-PnP) method that efficiently solves this joint estimation problem by introducing learned denoisers as priors on both the unknown image and the unknown measurement operator. We present a new convergence theory for BC-PnP compatible with blind inverse problems by considering nonconvex data-fidelity terms and expansive denoisers. Our theory analyzes the convergence of BC-PnP to a stationary point of an implicit function associated with an approximate minimum mean-squared error (MMSE) denoiser. We numerically validate our method on two blind inverse problems: automatic
    
[^189]: MultiTurnCleanup：用于多轮口语会话转录清理的基准测试

    MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup. (arXiv:2305.12029v1 [cs.CL])

    [http://arxiv.org/abs/2305.12029](http://arxiv.org/abs/2305.12029)

    本研究提出了MultiTurnCleanup任务，收集了新的数据集MultiTurnCleanup1，针对口语会话转录中的不连续现象进行探讨并提供了两个可用于未来研究的基准测试模型。

    

    目前的语调不连续检测模型侧重于单个说话者的每个话语。然而，口语会话转录中的许多不连续现象都发生在多轮对话中，这影响了人类的可读性和下游 NLP 任务的性能。本研究通过提出创新的“MultiTurnCleanup”任务，针对口语会话转录中的不连续现象进行探讨，并收集了新的数据集MultiTurnCleanup1。我们设计了一种数据标注模式以收集高质量的数据集，提供了广泛的数据分析。此外，我们利用两种建模方法进行实验评估，作为未来研究的基准测试。

    Current disfluency detection models focus on individual utterances each from a single speaker. However, numerous discontinuity phenomena in spoken conversational transcripts occur across multiple turns, hampering human readability and the performance of downstream NLP tasks. This study addresses these phenomena by proposing an innovative Multi-Turn Cleanup task for spoken conversational transcripts and collecting a new dataset, MultiTurnCleanup1. We design a data labeling schema to collect the high-quality dataset and provide extensive data analysis. Furthermore, we leverage two modeling approaches for experimental evaluation as benchmarks for future research.
    
[^190]: 曲线上扬：在可微广义加性模型中的共曲抑制正则化

    Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models. (arXiv:2305.11475v1 [cs.LG])

    [http://arxiv.org/abs/2305.11475](http://arxiv.org/abs/2305.11475)

    本文提供了一种共曲抑制正则化器，用于应对广义加性模型易受共错性的问题，通过惩罚非线性转换的特征变量的成对相关性，增强了模型的解释性。

    

    最近，由于广义加性模型（GAM）可表达目标变量为特征的非线性变换和解释性，其再次受到欢迎。尽管GAM目前备受热捧，但其易受共错性，即特征之间的（可能是非线性的）依赖性迄今为止大多被忽视。在这里，我们展示了共错性如何严重破坏GAM的解释性，并提出了一个解决方法：一个在非线性转换的特征变量的成对相关性上进行惩罚的概念简单但有效的正则化器。该过程适用于任何可微的加性模型，如神经加性模型或神经预言。并且通过消除自我抵消的特征贡献的歧义，增强了解释性。我们在合成和真实时间序列和表格数据集上验证了我们的正则化器的有效性。

    Generalized Additive Models (GAMs) have recently experienced a resurgence in popularity due to their interpretability, which arises from expressing the target value as a sum of non-linear transformations of the features. Despite the current enthusiasm for GAMs, their susceptibility to concurvity - i.e., (possibly non-linear) dependencies between the features - has hitherto been largely overlooked. Here, we demonstrate how concurvity can severly impair the interpretability of GAMs and propose a remedy: a conceptually simple, yet effective regularizer which penalizes pairwise correlations of the non-linearly transformed feature variables. This procedure is applicable to any differentiable additive model, such as Neural Additive Models or NeuralProphet, and enhances interpretability by eliminating ambiguities due to self-canceling feature contributions. We validate the effectiveness of our regularizer in experiments on synthetic as well as real-world datasets for time-series and tabular d
    
[^191]: 线性回归中依赖数据的噪声水平研究

    The noise level in linear regression with dependent data. (arXiv:2305.11165v1 [cs.LG])

    [http://arxiv.org/abs/2305.11165](http://arxiv.org/abs/2305.11165)

    本文研究了具有依赖数据的线性回归中的噪声水平，提出了上限界限，并在误差规范下表现出优雅的降低。

    

    我们从未做任何实现假设出发，对于具有依赖($\beta$-mixing)数据的随机设计线性回归，推导了其上限界限。与仅在可实现的鞅噪声范围内不可用尖锐的实例最优非渐近性相比，文献中没有可用的上限界限。在恰当的常数因素下，我们的分析正确地回归了中心极限定理预测的方差项 - 问题的噪声水平 - 并因此在引入错误规范时表现出逐渐降低的优雅性。在预燃条件下，我们的结果在中度偏差范围内是尖锐的，特别是不会膨胀引领项期限与混合时间因素。

    We derive upper bounds for random design linear regression with dependent ($\beta$-mixing) data absent any realizability assumptions. In contrast to the strictly realizable martingale noise regime, no sharp instance-optimal non-asymptotics are available in the literature. Up to constant factors, our analysis correctly recovers the variance term predicted by the Central Limit Theorem -- the noise level of the problem -- and thus exhibits graceful degradation as we introduce misspecification. Past a burn-in, our result is sharp in the moderate deviations regime, and in particular does not inflate the leading order term by mixing time factors.
    
[^192]: 学习去学习：机器去学习的综述

    Learn to Unlearn: A Survey on Machine Unlearning. (arXiv:2305.07512v1 [cs.LG])

    [http://arxiv.org/abs/2305.07512](http://arxiv.org/abs/2305.07512)

    本综述总结了机器去学习技术，用于从训练模型中删除敏感数据，但重新训练ML模型往往不可行。针对这个挑战，需要开发强大的模型以缓解公平性问题。

    

    机器学习模型包含私密信息，实现被遗忘权是许多数据应用的难题。机器去学习已成为从训练模型中删除敏感数据的替代方法，但重新训练机器学习模型往往是不可行的。本综述提供了机器去学习技术的简要评估，涵盖了精确和近似方法、可能的攻击以及验证方法。本综述比较了每种方法的优点和局限性，并使用Deltagrad精确机器去学习方法评估了它们的性能。本综述还强调了挑战，如非IID删除的强大模型，以缓解公平性问题。总的来说，本综述提供了机器去学习技术和应用的全面概述，并指出了这个不断发展的领域的未来研究方向。本综述旨在成为寻求机器去学习资料的研究人员和从业者的有价值资源。

    Machine Learning (ML) models contain private information, and implementing the right to be forgotten is a challenging privacy issue in many data applications. Machine unlearning has emerged as an alternative to remove sensitive data from a trained model, but completely retraining ML models is often not feasible. This survey provides a concise appraisal of Machine Unlearning techniques, encompassing both exact and approximate methods, probable attacks, and verification approaches. The survey compares the merits and limitations each method and evaluates their performance using the Deltagrad exact machine unlearning method. The survey also highlights challenges like the pressing need for a robust model for non-IID deletion to mitigate fairness issues. Overall, the survey provides a thorough synopsis of machine unlearning techniques and applications, noting future research directions in this evolving field. The survey aims to be a valuable resource for researchers and practitioners seeking
    
[^193]: 探究子词分割对transformer语言模型性能的影响

    Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])

    [http://arxiv.org/abs/2305.05480](http://arxiv.org/abs/2305.05480)

    本文研究使用单语词段算法StateMorph训练语言模型时，可以使模型更高效地收敛并获得更好的验证分数。

    

    我们想研究词段如何影响语言模型的性能。我们使用了一种单语词段算法StateMorph，在芬兰语和俄语中训练了GPT-2和BERT模型。作为比较，我们还训练了一个使用BPE和Morfessor分割算法的模型。我们的初步结果表明，StateMorph可以帮助模型更有效地收敛并获得更好的验证分数。

    We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
    
[^194]: 零样本学习在公司分类中的应用

    Company classification using zero-shot learning. (arXiv:2305.01028v1 [cs.CL])

    [http://arxiv.org/abs/2305.01028](http://arxiv.org/abs/2305.01028)

    本文提出了一种利用自然语言处理和零样本学习的方法来进行公司分类的方法。该方法可以简化公司分类过程，从而减少传统方法如全球产业分类标准（GICS）所需的时间和资源。

    

    近年来，自然语言处理在许多商业应用中变得越来越重要，包括情感分析、文本分类和命名实体识别。本文提出了一种利用自然语言处理和零样本学习的方法来进行公司分类的方法。我们的方法利用预训练的Transformer模型从公司描述中提取特征，然后应用零样本学习将公司分类到相关类别，无需为每个类别提供特定的训练数据。我们在公开可用的公司文本描述数据集上评估我们的方法，并证明它可以简化公司分类过程，从而减少传统方法如全球产业分类标准（GICS）所需的时间和资源。结果表明，该方法具有自动化公司分类的潜力，是未来研究的一个有前途的方向。

    In recent years, natural language processing (NLP) has become increasingly important in a variety of business applications, including sentiment analysis, text classification, and named entity recognition. In this paper, we propose an approach for company classification using NLP and zero-shot learning. Our method utilizes pre-trained transformer models to extract features from company descriptions, and then applies zero-shot learning to classify companies into relevant categories without the need for specific training data for each category. We evaluate our approach on publicly available datasets of textual descriptions of companies, and demonstrate that it can streamline the process of company classification, thereby reducing the time and resources required in traditional approaches such as the Global Industry Classification Standard (GICS). The results show that this method has potential for automation of company classification, making it a promising avenue for future research in thi
    
[^195]: Zip-NeRF：抗锯齿网格化神经辐射场

    Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields. (arXiv:2304.06706v1 [cs.CV])

    [http://arxiv.org/abs/2304.06706](http://arxiv.org/abs/2304.06706)

    本文提出了一种 Zip-NeRF 技术，将 mip-NeRF 360 和基于网格的模型相结合，以实现抗锯齿、提高训练速度并降低误差率。

    

    神经辐射场（NeRF）的网格化表示可以加速训练，但缺乏对比例的明确理解，容易引入锯齿或丢失场景内容。本文提出了一种将渲染和信号处理思想用于将 mip-NeRF 360 和基于网格的模型相结合，误差率比先前的技术低8%到76%，并比 mip-NeRF 360 快22倍的方法。

    Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8% - 76% lower than either prior technique, and that trains 22x faster than mip-NeRF 360.
    
[^196]: 《解开结构与风格的纽带：通过诱导文档层次结构来检测新闻中的政治偏见》

    Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy. (arXiv:2304.02247v1 [cs.CL])

    [http://arxiv.org/abs/2304.02247](http://arxiv.org/abs/2304.02247)

    本文提出了一种通过文档层次结构诱导来检测新闻中的政治偏见的方法，该方法克服了过拟合和有限的普适性，展现了更好的鲁棒性和准确性。

    

    本文针对新闻文章中政治偏见检测方面的重要差距进行研究。先前进行监督式文档分类的工作可能会偏向各网站的写作风格，导致过拟合和有限的普适性。我们的方法通过考虑句子级语义和文档级修辞结构来克服这一限制，从而产生一种更强大和不受风格影响的检测政治偏见的方法。我们引入了一种新颖的多头分层注意力模型，通过各种注意力头的不同集合有效地编码长文档的结构。我们展示了我们的方法克服了这种域依赖性，并表现出比先前方法更好的鲁棒性和准确性。进一步的分析表明，我们的模型能够捕捉到新闻中常用的话语结构。

    We address an important gap in detection of political bias in news articles. Previous works that perform supervised document classification can be biased towards the writing style of each news outlet, leading to overfitting and limited generalizability. Our approach overcomes this limitation by considering both the sentence-level semantics and the document-level rhetorical structure, resulting in a more robust and style-agnostic approach to detecting political bias in news articles. We introduce a novel multi-head hierarchical attention model that effectively encodes the structure of long documents through a diverse ensemble of attention heads. While journalism follows a formalized rhetorical structure, the writing style may vary by news outlet. We demonstrate that our method overcomes this domain dependency and outperforms previous approaches for robustness and accuracy. Further analysis demonstrates the ability of our model to capture the discourse structures commonly used in the jou
    
[^197]: eP-ALM:语言模型的高效感知增强

    eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v1 [cs.CV])

    [http://arxiv.org/abs/2303.11403](http://arxiv.org/abs/2303.11403)

    本论文提出了一种用对比学习提高语言模型的感知能力的高效方法eP-ALM，可以实现视觉感知信息和文本信息的融合，同时还能在多模态基准测试上实现最先进的结果。

    

    大型语言模型(LLM)迄今为止给世界留下了深刻印象，具有大规模模型所具有的非同寻常的能力。在视觉方面，变压器模型（即ViT）也在追随同一趋势，取得了最具挑战性的基准测试的最佳表现。随着这种单模型的丰富多样，自然会引发一个问题：我们是否需要跟随这个趋势来处理多模态任务？在这项工作中，我们提出将努力集中于现有模型的高效适应，并提出用感知来增强语言模型。现有的适应预训练模型用于视觉语言任务的方法仍然依赖于几个关键组件，从而影响了它们的效率。特别地，他们仍然训练大量的参数，依赖大规模的多模态预训练，使用在巨大的图像-文本数据集上训练的编码器（例如CLIP），并添加了显著的推理开销。此外，这些方法中的大多数关注Zero-Shot和In Context Learning，观察到两种范式之间的巨大差异。在本文中，我们介绍了eP-ALM，一种将视觉感知信息与语言模型相结合的高效方法。我们提出了一种方法，利用对比学习来实现视觉感知和文本信息的融合，具有极小的计算成本。我们的方法不需要任何新的预训练，仍然在多模态基准测试上实现了最先进的结果。

    Large Language Models (LLMs) have so far impressed the world, with unprecedented capabilities that emerge in models at large scales. On the vision side, transformer models (i.e., ViT) are following the same trend, achieving the best performance on challenging benchmarks. With the abundance of such unimodal models, a natural question arises; do we need also to follow this trend to tackle multimodal tasks? In this work, we propose to rather direct effort to efficient adaptations of existing models, and propose to augment Language Models with perception. Existing approaches for adapting pretrained models for vision-language tasks still rely on several key components that hinder their efficiency. In particular, they still train a large number of parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP) trained on huge image-text datasets, and add significant inference overhead. In addition, most of these approaches have focused on Zero-Shot and In Context Learning, with l
    
[^198]: 基于熵耗散的神经网络求解McKean-Vlasov类型的偏微分方程

    Entropy-dissipation Informed Neural Network for McKean-Vlasov Type PDEs. (arXiv:2303.11205v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2303.11205](http://arxiv.org/abs/2303.11205)

    本文提出了一种基于熵耗散的神经网络方法来求解McKean-Vlasov类型的偏微分方程(MVEs)，该方法通过最小化广义自洽势来控制假设解与真实解之间的差异，通过神经网络进行函数逼近，并在多个示例问题上验证了方法的有效性。

    

    我们将自洽性的概念扩展到更一般的McKean-Vlasov方程(MVE)中。虽然Fokker-Planck方程(FPE)描述了漂移和扩散下粒子的宏观行为，但MVE考虑了额外的粒子间相互作用，这在物理系统中通常具有奇异性。本文考虑了两个重要的示例，即具有库仑相互作用的MVE和二维Navier-Stokes方程的涡量表述。我们展示了广义自洽势通过熵耗散控制了假设解与真实解之间的KL散度。基于这一结果，我们提出通过最小化该势函数来求解MVE，并利用神经网络进行函数逼近。通过与现有的基于神经网络的PDE求解器在几个示例问题上的比较，验证了我们方法的实证性能。

    We extend the concept of self-consistency for the Fokker-Planck equation (FPE) to the more general McKean-Vlasov equation (MVE). While FPE describes the macroscopic behavior of particles under drift and diffusion, MVE accounts for the additional inter-particle interactions, which are often highly singular in physical systems. Two important examples considered in this paper are the MVE with Coulomb interactions and the vorticity formulation of the 2D Navier-Stokes equation. We show that a generalized self-consistency potential controls the KL-divergence between a hypothesis solution to the ground truth, through entropy dissipation. Built on this result, we propose to solve the MVEs by minimizing this potential function, while utilizing the neural networks for function approximation. We validate the empirical performance of our approach by comparing with state-of-the-art NN-based PDE solvers on several example problems.
    
[^199]: 合成经验回放：旨在用扩充数据来提高深度强化学习的效果

    Synthetic Experience Replay. (arXiv:2303.06614v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06614](http://arxiv.org/abs/2303.06614)

    本文提出了合成经验回放方法解决深度强化学习中数据匮乏问题，通过巧妙应用生成建模技术来扩充数据效果显著。

    

    过去十年的一个关键主题是，当大型神经网络和大型数据集相结合时，它们可以产生令人惊异的结果。在深度强化学习中，这种范式通常通过经验回放实现，其中过去的经验数据集用于训练策略或值函数。然而，与监督学习或自监督学习不同，强化学习代理必须收集自己的数据，这通常是有限的。因此，利用深度学习的好处是具有挑战性的，即使是小型神经网络在训练开始时也可能出现过拟合现象。在这项工作中，我们利用了生成建模的巨大进步，并提出了合成经验回放（SynthER），一种基于扩散的方法来灵活地上采样代理收集的经验。我们证明了SynthER是一种有效的方法，可以在离线和在线设置下训练强化学习代理，无论是在感知环境还是在像素环境中。在离线设置中，我们观察到了显着的改进。

    A key theme in the past decade has been that when large neural networks and large datasets combine they can produce remarkable results. In deep reinforcement learning (RL), this paradigm is commonly made possible through experience replay, whereby a dataset of past experiences is used to train a policy or value function. However, unlike in supervised or self-supervised learning, an RL agent has to collect its own data, which is often limited. Thus, it is challenging to reap the benefits of deep learning, and even small neural networks can overfit at the start of training. In this work, we leverage the tremendous recent progress in generative modeling and propose Synthetic Experience Replay (SynthER), a diffusion-based approach to flexibly upsample an agent's collected experience. We show that SynthER is an effective method for training RL agents across offline and online settings, in both proprioceptive and pixel-based environments. In offline settings, we observe drastic improvements 
    
[^200]: 关于Lipschitz神经网络的统一代数视角

    A Unified Algebraic Perspective on Lipschitz Neural Networks. (arXiv:2303.03169v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03169](http://arxiv.org/abs/2303.03169)

    本文介绍了一种统一的代数视角，融合了各种类型的1-Lipschitz神经网络，包括凸潜在层和近正交层，并利用解析解推导和推广了许多现有的技术。

    

    这项研究致力于设计和训练具有控制的Lipschitz常数的神经网络。目标是提高并有时保证对抗攻击的鲁棒性。最近的一些有希望的技术从不同的背景中汲取灵感来设计1-Lipschitz神经网络，比如凸潜在层从连续动力系统的离散化中得出，近正交层则提出了一种定制的矩阵重新缩放方法。然而，现在重要的是在通用的理论视角下考虑该领域的最新和有希望的贡献，以更好地设计新的和改进的层次。本文引入了一种新的代数视角，统一了各种类型的1-Lipschitz神经网络，包括之前提到的方法，以及基于正交性和谱方法的方法。有趣的是，我们展示了许多现有的技术可以通过找到解析解来推导和推广。

    Important research efforts have focused on the design and training of neural networks with a controlled Lipschitz constant. The goal is to increase and sometimes guarantee the robustness against adversarial attacks. Recent promising techniques draw inspirations from different backgrounds to design 1-Lipschitz neural networks, just to name a few: convex potential layers derive from the discretization of continuous dynamical systems, Almost-Orthogonal-Layer proposes a tailored method for matrix rescaling. However, it is today important to consider the recent and promising contributions in the field under a common theoretical lens to better design new and improved layers. This paper introduces a novel algebraic perspective unifying various types of 1-Lipschitz neural networks, including the ones previously mentioned, along with methods based on orthogonality and spectral methods. Interestingly, we show that many existing techniques can be derived and generalized via finding analytical sol
    
[^201]: 异质分布偏移下的统计学习

    Statistical Learning under Heterogenous Distribution Shift. (arXiv:2302.13934v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13934](http://arxiv.org/abs/2302.13934)

    本文研究了异质分布偏移下的统计学习问题，通过研究经验风险最小化(ERM)在不同类别的复杂性下的表现，我们发现当类别$F$相比类别$G$更“简单”时，我们的预测器对于协变量偏移具有更强的鲁棒性，尤其在$\textbf{y}$的偏移远小于$\textbf{x}$的情况下。同时，我们发现ERM的行为与正交机器学习具有类似的特性。

    

    本文研究了从随机变量对$(\mathbf{x},\mathbf{y})$中预测目标$\mathbf{z}$, 其中真实的预测器是加法的$\mathbb{E}[\mathbf{z} \mid \mathbf{x},\mathbf{y}] = f_\star(\mathbf{x}) +g_{\star}(\mathbf{y})$。我们研究了在给定训练分布上拟合的函数$f+g$, $f \in F$和$g \in G$上的经验风险最小化(ERM)在表现上的差异，但在测试分布上得到评估时会显示出协变量偏移。我们的研究表明，当类别$F$比$G$更“简单”（例如，以度量熵为衡量标准）时，我们的预测器对于协变量偏移的抗干扰能力更强，其中$\textbf{y}$的偏移要远小于$\textbf{x}$的偏移。我们的分析表明，ERM的行为与正交机器学习$\textbf{ qualitatively similarly}$：ERM恢复预测器中的$f$成分的速率仅对于类别$G$的复杂性具有较低阶的依赖性，调整后...

    This paper studies the prediction of a target $\mathbf{z}$ from a pair of random variables $(\mathbf{x},\mathbf{y})$, where the ground-truth predictor is additive $\mathbb{E}[\mathbf{z} \mid \mathbf{x},\mathbf{y}] = f_\star(\mathbf{x}) +g_{\star}(\mathbf{y})$. We study the performance of empirical risk minimization (ERM) over functions $f+g$, $f \in F$ and $g \in G$, fit on a given training distribution, but evaluated on a test distribution which exhibits covariate shift. We show that, when the class $F$ is "simpler" than $G$ (measured, e.g., in terms of its metric entropy), our predictor is more resilient to $\textbf{heterogenous covariate shifts}$ in which the shift in $\mathbf{x}$ is much greater than that in $\mathbf{y}$. Our analysis proceeds by demonstrating that ERM behaves $\textbf{qualitatively similarly to orthogonal machine learning}$: the rate at which ERM recovers the $f$-component of the predictor has only a lower-order dependence on the complexity of the class $G$, adjus
    
[^202]: 变分自编码器的分布式学习：在合成数据生成中的应用

    Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation. (arXiv:2302.11294v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.11294](http://arxiv.org/abs/2302.11294)

    提出了一种新的方法扩展了VAE模型容量，采用无限混合的非对称拉普拉斯分布作为解码器，具有分布拟合能力和调整数据隐私级别的优越性。

    

    尽管变分自编码器（VAE）在计算建模方面很高效，但高斯假设一直被认为是它的主要局限性。在本文中，我们提出了一种新方法，扩展了模型容量（即分布族的表达能力），而不会牺牲VAE框架的计算优势。我们的VAE模型的解码器由无限组合的非对称拉普拉斯分布构成，具有连续变量的分布拟合能力。我们的模型由估计一般分位函数的非参数M-estimator的特殊形式表示，并在理论上建立了所提出模型与分位数估计之间的关系。我们将所提出的模型应用于合成数据生成，特别是在轻松调整数据隐私级别方面，我们的模型展现了其优越性。

    The Gaussianity assumption has been consistently criticized as a main limitation of the Variational Autoencoder (VAE), despite its efficiency in computational modeling. In this paper, we propose a new approach that expands the model capacity (i.e., expressive power of distributional family) without sacrificing the computational advantages of the VAE framework. Our VAE model's decoder is composed of an infinite mixture of asymmetric Laplacian distribution, which possesses general distribution fitting capabilities for continuous variables. Our model is represented by a special form of a nonparametric M-estimator for estimating general quantile functions, and we theoretically establish the relevance between the proposed model and quantile estimation. We apply the proposed model to synthetic data generation, and particularly, our model demonstrates superiority in easily adjusting the level of data privacy.
    
[^203]: 可复制聚类算法

    Replicable Clustering. (arXiv:2302.10359v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10359](http://arxiv.org/abs/2302.10359)

    该论文提出了三个针对统计聚类的可复制算法，实现了可复制聚类的概念，其中包括利用近似算法组合问题的黑盒方式解决统计$k$-medians、统计$k$-means和统计$k$-centers问题，并给出了表示算法复杂度的函数和误差限制。

    

    我们在最近由Impagliazzo等人[2022]引入的可复制性概念下设计了在统计聚类中可复制的算法。根据这个定义，如果一个聚类算法是可复制的，那么在同一分布的两个不同输入上执行时，只要其内部随机性在执行中得到共享，就能高概率地产生完全相同的样本空间分区。我们通过黑盒的方式利用组合对应问题的近似算法，为统计$k$-medians、统计$k$-means和统计$k$-centers问题提出了这样的算法。特别地，我们展示了一个可复制的$O(1)$-逼近算法，其适用于统计欧几里得$k$-medians ($k$-means)，其样本复杂度为$\operatorname{poly}(d)$。我们还描述了一个$O(1)$-逼近算法，其在统计欧几里得$k$-centers$时具有额外的$O(1)$-加性误差，尽管其样本复杂度为$\exp(d)$。

    We design replicable algorithms in the context of statistical clustering under the recently introduced notion of replicability from Impagliazzo et al. [2022]. According to this definition, a clustering algorithm is replicable if, with high probability, its output induces the exact same partition of the sample space after two executions on different inputs drawn from the same distribution, when its internal randomness is shared across the executions. We propose such algorithms for the statistical $k$-medians, statistical $k$-means, and statistical $k$-centers problems by utilizing approximation routines for their combinatorial counterparts in a black-box manner. In particular, we demonstrate a replicable $O(1)$-approximation algorithm for statistical Euclidean $k$-medians ($k$-means) with $\operatorname{poly}(d)$ sample complexity. We also describe an $O(1)$-approximation algorithm with an additional $O(1)$-additive error for statistical Euclidean $k$-centers, albeit with $\exp(d)$ samp
    
[^204]: 反馈图上的实用情境赌博算法

    Practical Contextual Bandits with Feedback Graphs. (arXiv:2302.08631v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08631](http://arxiv.org/abs/2302.08631)

    本文提出了一种利用反馈图的情境赌博算法，可用于缓解学习的统计复杂性，在实际应用中具有计算上的可行性和优秀的表现。

    

    尽管情境赌博已经有成熟的理论，但如何有效地利用不同的反馈模式来加速学习仍不清楚。在反馈图上的赌博问题提供了一个有前途的框架来缓解学习的统计复杂性。本文基于回归的思想，提出和分析了一种情境赌博算法，这些算法在现实应用中具有计算上的可行性，并实现了已知的极小极值，因此减少了学习的统计复杂性。

    While contextual bandit has a mature theory, effectively leveraging different feedback patterns to enhance the pace of learning remains unclear. Bandits with feedback graphs, which interpolates between the full information and bandit regimes, provides a promising framework to mitigate the statistical complexity of learning. In this paper, we propose and analyze an approach to contextual bandits with feedback graphs based upon reduction to regression. The resulting algorithms are computationally practical and achieve established minimax rates, thereby reducing the statistical complexity in real-world applications.
    
[^205]: 未配对的多领域因果表示学习

    Unpaired Multi-Domain Causal Representation Learning. (arXiv:2302.00993v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.00993](http://arxiv.org/abs/2302.00993)

    研究了未配对的多领域因果表示学习，提出了线性模型下联合分布和共享因果图的可识别条件，并将其转化为一种实用方法以恢复共享的潜在因果图。

    

    因果表示学习的目标是找到由因果相关的潜在变量组成的数据表示。我们考虑一个情景，其中我们可以访问来自可能共享因果表示的多个领域的数据。关键在于，假设不同领域的观测结果是未配对的，也就是说，我们只观察每个领域的边缘分布，而不是它们的联合分布。在本文中，我们给出了线性模型下联合分布和共享因果图可识别性的充分条件。只要我们能够从每个领域的边缘分布中唯一恢复联合分布和共享因果表示，可识别性就能够成立。我们将识别性结果转化为一种实用方法，用于恢复共享的潜在因果图。

    The goal of causal representation learning is to find a representation of data that consists of causally related latent variables. We consider a setup where one has access to data from multiple domains that potentially share a causal representation. Crucially, observations in different domains are assumed to be unpaired, that is, we only observe the marginal distribution in each domain but not their joint distribution. In this paper, we give sufficient conditions for identifiability of the joint distribution and the shared causal graph in a linear setup. Identifiability holds if we can uniquely recover the joint distribution and the shared causal representation from the marginal distributions in each domain. We transform our identifiability results into a practical method to recover the shared latent causal graph.
    
[^206]: 不依赖奖励模型的直接基于偏好的策略优化

    Direct Preference-based Policy Optimization without Reward Modeling. (arXiv:2301.12842v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12842](http://arxiv.org/abs/2301.12842)

    本文提出了一种无需奖励模型的直接基于偏好的策略优化算法，通过采用对比学习框架和设计新的策略评分指标，能够从给定的偏好数据中学习并取得良好性能。

    

    基于偏好的强化学习(PbRL)是一种使RL代理能够从偏好中学习的方法，特别适用于在制定奖励函数时存在挑战的情况。现有的PbRL方法一般包括两个步骤：首先根据给定的偏好数据学习奖励模型，然后使用学习到的奖励模型采用现成的强化学习算法。然而，仅通过偏好信息获取准确的奖励模型，尤其是在偏好来自人类教师时，可能很困难。相反，我们提出了一种不需要任何奖励模型的直接从偏好中学习的PbRL算法。为了实现这一目标，我们采用对比学习框架，设计了一种新的策略评分指标，为与给定偏好一致的策略分配高分。我们将我们的算法应用于带有实际人类偏好标签的离线RL任务，并展示了我们的算法优于或与现有方法相当。

    Preference-based reinforcement learning (PbRL) is an approach that enables RL agents to learn from preference, which is particularly useful when formulating a reward function is challenging. Existing PbRL methods generally involve a two-step procedure: they first learn a reward model based on given preference data and then employ off-the-shelf reinforcement learning algorithms using the learned reward model. However, obtaining an accurate reward model solely from preference information, especially when the preference is from human teachers, can be difficult. Instead, we propose a PbRL algorithm that directly learns from preference without requiring any reward modeling. To achieve this, we adopt a contrastive learning framework to design a novel policy scoring metric that assigns a high score to policies that align with the given preferences. We apply our algorithm to offline RL tasks with actual human preference labels and show that our algorithm outperforms or is on par with the exist
    
[^207]: 人工和机器关于什么是冒犯存在较大分歧的共情冒犯和噪声审计：统一主观冒犯的人类和机器差异

    Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive. (arXiv:2301.12534v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.12534](http://arxiv.org/abs/2301.12534)

    本文通过人工和机器审核员的共情冒犯和噪声审计研究了冒犯性言论检测中的差异性。结果表明，审核员之间存在广泛的分歧，并且人工审核员和大型语言模型分类器无法预测其他审核员的回应。这对于内容审核具有重要意义。

    

    冒犯性言论检测是内容审核的一个关键组成部分。然而，什么是冒犯性的可以是高度主观的。本文研究了当涉及到现实世界社交网站政治言论时，人工和机器审核员对于什么是冒犯性的存在分歧。我们发现（1）审核员之间（包括人工和机器）存在广泛分歧；和（2）人工审核员和大型语言模型分类器无法预测其他审核员基于他们的政治倾向如何回应。对于（1），我们进行了一个前所未有规模的噪声审计，结合了机器和人工回答。对于（2），我们介绍了一个首创的共情冒犯的数据集。我们的噪声审计揭示了不同机器审核员之间的审核结果差异很大。我们与人工审核员进行的实验表明，政治倾向结合敏感问题会影响到一对一的冒犯，以及共情冒犯。数据集可通过https://github.com/Homan-Lab/voic获得。

    Offensive speech detection is a key component of content moderation. However, what is offensive can be highly subjective. This paper investigates how machine and human moderators disagree on what is offensive when it comes to real-world social web political discourse. We show that (1) there is extensive disagreement among the moderators (humans and machines); and (2) human and large-language-model classifiers are unable to predict how other human raters will respond, based on their political leanings. For (1), we conduct a noise audit at an unprecedented scale that combines both machine and human responses. For (2), we introduce a first-of-its-kind dataset of vicarious offense. Our noise audit reveals that moderation outcomes vary wildly across different machine moderators. Our experiments with human moderators suggest that political leanings combined with sensitive issues affect both first-person and vicarious offense. The dataset is available through https://github.com/Homan-Lab/voic
    
[^208]: ReSQue的并行和私密随机凸优化

    ReSQueing Parallel and Private Stochastic Convex Optimization. (arXiv:2301.00457v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2301.00457](http://arxiv.org/abs/2301.00457)

    该论文介绍了一种名为ReSQue的工具，用于处理随机凸优化问题。通过将ReSQue与最新的球预言加速技术相结合，作者提出了在并行和私密环境下达到最先进复杂度的算法。对于在单位球中的凸优化目标，作者的算法能够在符合一定条件的情况下实现优化误差，同时保持最佳总体工作量的性能。

    

    我们引入了一种新的随机凸优化（SCO）工具：一种基于重加权随机查询（ReSQue）的梯度估计器，用于与（高斯）概率密度卷积的函数的梯度。将ReSQue与最新的球预言加速技术相结合[CJJJLST20, ACJJS21]，我们开发了在并行和私密环境下实现SCO的算法，达到了最先进的复杂度。对于$\mathbb{R}^d$中被单位球约束的SCO目标，我们得到了以下结果（多对数因子）。我们提供了一个并行算法，通过$d^{1/3}\epsilon_{\text{opt}}^{-2/3}$梯度预言查询深度和$d^{1/3}\epsilon_{\text{opt}}^{-2/3} + \epsilon_{\text{opt}}^{-2}$总梯度查询数，以 $\epsilon_{\text{opt}}$ 获得优化误差，假设可以访问一个有界方差的随机梯度估计器。对于$\epsilon_{\text{opt}} \in [d^{-1}, d^{-1/4}]$，我们的算法与[BJLLS19]的最先进预言深度相匹配，同时保持最佳总体工作量

    We introduce a new tool for stochastic convex optimization (SCO): a Reweighted Stochastic Query (ReSQue) estimator for the gradient of a function convolved with a (Gaussian) probability density. Combining ReSQue with recent advances in ball oracle acceleration [CJJJLST20, ACJJS21], we develop algorithms achieving state-of-the-art complexities for SCO in parallel and private settings. For a SCO objective constrained to the unit ball in $\mathbb{R}^d$, we obtain the following results (up to polylogarithmic factors). We give a parallel algorithm obtaining optimization error $\epsilon_{\text{opt}}$ with $d^{1/3}\epsilon_{\text{opt}}^{-2/3}$ gradient oracle query depth and $d^{1/3}\epsilon_{\text{opt}}^{-2/3} + \epsilon_{\text{opt}}^{-2}$ gradient queries in total, assuming access to a bounded-variance stochastic gradient estimator. For $\epsilon_{\text{opt}} \in [d^{-1}, d^{-1/4}]$, our algorithm matches the state-of-the-art oracle depth of [BJLLS19] while maintaining the optimal total wor
    
[^209]: 关于解释图神经网络的必要性和充分性的概率：一种下界优化方法

    On the Probability of Necessity and Sufficiency of Explaining Graph Neural Networks: A Lower Bound Optimization Approach. (arXiv:2212.07056v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07056](http://arxiv.org/abs/2212.07056)

    该论文提出了一种利用下界优化来解释图神经网络（GNNs）的方法。该方法考虑解释的必要性和充分性，并通过数学量化这些要求。通过优化下界，可以获得最必要且充分的解释。

    

    图神经网络（GNNs）的可解释性对各种GNN应用至关重要，然而它仍然是一个重大挑战。一个令人信服的解释应该同时具有必要性和充分性。然而，现有的GNN解释方法仅关注其中之一，必要性或充分性，或者是两者之间的启发式权衡。从理论上讲，必要性和充分性的概率（PNS）有潜力确定最必要和充分的解释，因为它可以数学上量化解释的必要性和充分性。然而，由于非单调性和反事实估计的挑战，获得PNS的困难限制了其广泛使用。为了解决PNS的不可识别性，我们求助于PNS的一个下界，可以通过反事实估计进行优化，并提出了一个GNN的必要和充分解释的框架（NSEG）。具体来说，我们描述了一种下界优化方法。

    The explainability of Graph Neural Networks (GNNs) is critical to various GNN applications, yet it remains a significant challenge. A convincing explanation should be both necessary and sufficient simultaneously. However, existing GNN explaining approaches focus on only one of the two aspects, necessity or sufficiency, or a heuristic trade-off between the two. Theoretically, the Probability of Necessity and Sufficiency (PNS) holds the potential to identify the most necessary and sufficient explanation since it can mathematically quantify the necessity and sufficiency of an explanation. Nevertheless, the difficulty of obtaining PNS due to non-monotonicity and the challenge of counterfactual estimation limit its wide use. To address the non-identifiability of PNS, we resort to a lower bound of PNS that can be optimized via counterfactual estimation, and propose a framework of Necessary and Sufficient Explanation for GNN (NSEG) via optimizing that lower bound. Specifically, we depict the 
    
[^210]: 隐式卷积核用于可定向卷积神经网络

    Implicit Convolutional Kernels for Steerable CNNs. (arXiv:2212.06096v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06096](http://arxiv.org/abs/2212.06096)

    本文提出了一种使用隐式神经表示的方法来参数化可定向卷积核，从而实现了简单灵活的构建可定向卷积神经网络的方法，能够推广到任何具有等变MLP的群G。

    

    可定向卷积神经网络提供了构建与平移和其他变换等同变换的神经网络的通用框架，这些变换属于基于原点保持的群G，例如反射和旋转。它们依赖于通过在核空间上强加特定于群G的等变性约束来解析求解得到的G-定向卷积核的标准卷积。由于解决方案对特定的群G定制，核基础的实现不能推广到其他对称变换，这导致了通用群等变模型的开发复杂化。我们提出使用通过多层感知器(MLPs)参数化G-定向卷积核的隐式神经表示。所得到的框架提供了一种简单灵活的实现可定向卷积神经网络的方法，并且对于任何可以构建G-等变MLP的群G都可以推广。我们在多个任务上证明了我们的方法的有效性，包括N体模拟。

    Steerable convolutional neural networks (CNNs) provide a general framework for building neural networks equivariant to translations and other transformations belonging to an origin-preserving group $G$, such as reflections and rotations. They rely on standard convolutions with $G$-steerable kernels obtained by analytically solving the group-specific equivariance constraint imposed onto the kernel space. As the solution is tailored to a particular group $G$, the implementation of a kernel basis does not generalize to other symmetry transformations, which complicates the development of general group equivariant models. We propose using implicit neural representation via multi-layer perceptrons (MLPs) to parameterize $G$-steerable kernels. The resulting framework offers a simple and flexible way to implement Steerable CNNs and generalizes to any group $G$ for which a $G$-equivariant MLP can be built. We prove the effectiveness of our method on multiple tasks, including N-body simulations,
    
[^211]: 非对比自监督学习中的隐式方差正则化

    Implicit variance regularization in non-contrastive SSL. (arXiv:2212.04858v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04858](http://arxiv.org/abs/2212.04858)

    非对比自监督学习中的预测网络通过隐式方差正则化避免表示崩溃，欧几里得距离和余弦相似度具有不同的动态机制，并且特征值作为学习率乘数。引入一族等向性损失函数可以平衡收敛速度。

    

    非对比自监督学习方法（如BYOL和SimSiam）依赖于非对称预测网络来避免表示崩溃而无需负样本。然而，预测网络如何促进稳定学习还不完全清楚。虽然先前的理论分析假设欧几里得损失，但大多数实际实现依赖于余弦相似度。为了进一步理解非对比自监督学习，我们在闭式线性预测网络的特征空间中分析研究学习动力学与欧几里得距离和余弦相似度的关系。我们发现，两者均通过隐式方差正则化来避免崩溃，尽管具有不同的动态机制。此外，我们发现特征值作为有效的学习率乘数，并提出了一族等向性损失函数（IsoLoss），以在特征模式之间平衡收敛速度。实验证明，IsoLoss加速了初始学习动力学并增加了鲁棒性，从而使我们能够摆脱...

    Non-contrastive SSL methods like BYOL and SimSiam rely on asymmetric predictor networks to avoid representational collapse without negative samples. Yet, how predictor networks facilitate stable learning is not fully understood. While previous theoretical analyses assumed Euclidean losses, most practical implementations rely on cosine similarity. To gain further theoretical insight into non-contrastive SSL, we analytically study learning dynamics in conjunction with Euclidean and cosine similarity in the eigenspace of closed-form linear predictor networks. We show that both avoid collapse through implicit variance regularization albeit through different dynamical mechanisms. Moreover, we find that the eigenvalues act as effective learning rate multipliers and propose a family of isotropic loss functions (IsoLoss) that equalize convergence rates across eigenmodes. Empirically, IsoLoss speeds up the initial learning dynamics and increases robustness, thereby allowing us to dispense with 
    
[^212]: CORL: 面向研究的深度强化学习离线库

    CORL: Research-oriented Deep Offline Reinforcement Learning Library. (arXiv:2210.07105v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07105](http://arxiv.org/abs/2210.07105)

    CORL是一个面向研究的深度强化学习离线库，提供了经过充分基准测试的单文件实现离线和离线到在线强化学习算法，并具有简单的开发体验和实验跟踪功能。

    

    CORL是一个开源库，提供了经过充分基准测试的单文件实现深度离线和离线到在线强化学习算法。它强调简单的开发体验，具有直观的代码库和现代分析跟踪工具。在CORL中，我们将方法实现隔离到单独的单个文件中，使性能相关的细节更容易识别。此外，实验跟踪功能可用于帮助记录指标、超参数、依赖项等到云端。最后，我们通过基准测试常用的D4RL数据集，确保了实现的可靠性，提供了透明的结果源，可用于强大的评估工具，例如性能概要、改进概率或预期在线性能。

    CORL is an open-source library that provides thoroughly benchmarked single-file implementations of both deep offline and offline-to-online reinforcement learning algorithms. It emphasizes a simple developing experience with a straightforward codebase and a modern analysis tracking tool. In CORL, we isolate methods implementation into separate single files, making performance-relevant details easier to recognize. Additionally, an experiment tracking feature is available to help log metrics, hyperparameters, dependencies, and more to the cloud. Finally, we have ensured the reliability of the implementations by benchmarking commonly employed D4RL datasets providing a transparent source of results that can be reused for robust evaluation tools such as performance profiles, probability of improvement, or expected online performance.
    
[^213]: 不确定因果网络中的因果效应识别

    Causal Effect Identification in Uncertain Causal Networks. (arXiv:2208.04627v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.04627](http://arxiv.org/abs/2208.04627)

    本研究探讨了在存在不确定性因果图的情况下，如何识别出具有最高合理性且能够识别因果效应的子图。通过解决一个NP完全的组合优化问题，我们发现这个问题的答案。

    

    因果识别是因果推断文献的核心，在那里提出了完整的算法来识别感兴趣的因果查询。这些算法的有效性依赖于对正确指定的因果结构的限定性假设。在这项工作中，我们研究了一个可用的因果结构的概率模型的设置。具体而言，因果图中的边存在不确定性，可以表示领域专家的信任程度，也可以反映特定统计检验的置信度。在这种情况下，自然而然地产生的问题是：在给定这样的概率图和感兴趣的特定因果效应的情况下，哪个子图具有最高的合理性，并且因果效应能够识别？我们证明回答这个问题可以归结为解决一个NP完全的组合优化问题，我们称之为边识别问题。

    Causal identification is at the core of the causal inference literature, where complete algorithms have been proposed to identify causal queries of interest. The validity of these algorithms hinges on the restrictive assumption of having access to a correctly specified causal structure. In this work, we study the setting where a probabilistic model of the causal structure is available. Specifically, the edges in a causal graph exist with uncertainties which may, for example, represent degree of belief from domain experts. Alternatively, the uncertainty about an edge may reflect the confidence of a particular statistical test. The question that naturally arises in this setting is: Given such a probabilistic graph and a specific causal effect of interest, what is the subgraph which has the highest plausibility and for which the causal effect is identifiable? We show that answering this question reduces to solving an NP-complete combinatorial optimization problem which we call the edge ID
    
[^214]: 深度学习广义线性模型及其在缺失数据中的应用

    Deeply-Learned Generalized Linear Models with Missing Data. (arXiv:2207.08911v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.08911](http://arxiv.org/abs/2207.08911)

    本文提出了一种深度学习广义线性模型 (dlglm) 及其在处理缺失数据中的应用，其中的方法能够灵活地处理输入特征和响应的可忽略和不可忽略的缺失模式。我们通过统计模拟证明，在缺失数据不随机的情况下，该方法优于现有的监督学习方法，可用于生物医学科学等领域中。

    

    深度学习方法在过去几年中在生物医学科学的监督学习问题中得到了显著应用，但现代生物医学数据集中缺失数据的更加普遍和复杂性给深度学习方法带来了重大挑战。本文提出了一种深度学习广义线性模型(dlglm)的正式处理方法，在训练时能够灵活地处理输入特征和响应的可忽略和不可忽略的缺失模式。我们通过统计模拟证明，在缺失数据不随机的情况下，我们的方法优于现有的监督学习方法。最后，我们以UCI机器学习中的银行营销数据集为例进行了案例研究。

    Deep Learning (DL) methods have dramatically increased in popularity in recent years, with significant growth in their application to supervised learning problems in the biomedical sciences. However, the greater prevalence and complexity of missing data in modern biomedical datasets present significant challenges for DL methods. Here, we provide a formal treatment of missing data in the context of deeply learned generalized linear models, a supervised DL architecture for regression and classification problems. We propose a new architecture, \textit{dlglm}, that is one of the first to be able to flexibly account for both ignorable and non-ignorable patterns of missingness in input features and response at training time. We demonstrate through statistical simulation that our method outperforms existing approaches for supervised learning tasks in the presence of missing not at random (MNAR) missingness. We conclude with a case study of a Bank Marketing dataset from the UCI Machine Learnin
    
[^215]: 使用退火算法增强循环神经网络来解决组合优化问题

    Supplementing Recurrent Neural Networks with Annealing to Solve Combinatorial Optimization Problems. (arXiv:2207.08189v2 [cond-mat.dis-nn] UPDATED)

    [http://arxiv.org/abs/2207.08189](http://arxiv.org/abs/2207.08189)

    这项研究将循环神经网络与传统的退火算法相结合，提出了一种增强的方法来解决组合优化问题。通过使用变分经典退火框架，可以生成不相关的解，克服了传统采样方案的缺点。该方法在实际的优化问题中展现出了潜力，并与模拟退火算法进行了比较。

    

    组合优化问题通常通过启发式算法（如模拟退火）来求解，以通过热力波动在大的搜索空间中寻找最优解。该算法通过马尔科夫链蒙特卡洛技术生成新的解。这种采样方案可能会导致严重的局限性，比如收敛速度慢且在低温下倾向于停留在相同的局部搜索空间中。为了克服这些缺点，我们使用了变分经典退火（VCA）框架，将自回归循环神经网络（RNNs）与传统退火相结合，以采样不相关的解。在本文中，我们展示了使用VCA作为解决实际优化问题的方法的潜力。我们探索比较了VCA与模拟退火在解决三个流行的优化问题中的性能：最大割问题（Max-Cut）、护士排班问题（NSP）和旅行商问题（TSP）。

    Combinatorial optimization problems can be solved by heuristic algorithms such as simulated annealing (SA) which aims to find the optimal solution within a large search space through thermal fluctuations. The algorithm generates new solutions through Markov-chain Monte Carlo techniques. This sampling scheme can result in severe limitations, such as slow convergence and a tendency to stay within the same local search space at small temperatures. To overcome these shortcomings, we use the variational classical annealing (VCA) framework that combines autoregressive recurrent neural networks (RNNs) with traditional annealing to sample solutions that are uncorrelated. In this paper, we demonstrate the potential of using VCA as an approach to solving real-world optimization problems. We explore VCA's performance in comparison with SA at solving three popular optimization problems: the maximum cut problem (Max-Cut), the nurse scheduling problem (NSP), and the traveling salesman problem (TSP).
    
[^216]: 语言模型显示对推理任务具有类似人类的内容效应

    Language models show human-like content effects on reasoning tasks. (arXiv:2207.07051v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.07051](http://arxiv.org/abs/2207.07051)

    本研究探讨了语言模型在逻辑推理任务中是否像人类一样通过混入内容来影响答案，结果发现大型语言模型的先验期望能够捕捉到这种特征。

    

    抽象推理是智能系统的关键能力。大型语言模型在抽象推理任务上实现了高于随机的性能，但存在许多不完善之处。然而，人类的抽象推理也是不完美的。例如，人类推理受到我们对真实世界的知识和信念的影响，并表现出显著的“内容效应”；当问题的语义内容支持正确的逻辑推理时，人类更可靠地进行推理。这些内容纠缠的推理模式在关于人类智能基本性质的争论中起着核心作用。在这里，我们研究了语言模型是否以类似的方式混入内容来回答逻辑问题，这些语言模型的先验期望捕捉了一些人类知识的特征。我们在三个逻辑推理任务上探索了这个问题：自然语言推理、判断三段论的逻辑有效性和Wason选择任务。我们评估了最先进的大型语言模型的性能。

    Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable "content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large 
    
[^217]: 算法基础的经验性X风险最小化

    Algorithmic Foundations of Empirical X-risk Minimization. (arXiv:2206.00439v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00439](http://arxiv.org/abs/2206.00439)

    本文介绍了一种名为"经验性X风险最小化（EXM）"的机器学习和人工智能优化框架，该框架解决了深度学习中优化不可分解目标的困难，并提供了算法基础的详细讨论。

    

    本文介绍了一种名为"经验性X风险最小化（EXM）"的机器学习和人工智能优化框架。X风险是一个用于表示一类组合度量或目标的术语，在其中，将每个数据点与大量的明确或隐含的项目进行比较来定义风险函数。它包括许多广泛使用的代理目标和不可分解的损失函数，例如AUROC、AUPRC、部分AUROC、NDCG、MAP、在前K个位置的精确度/召回率、在特定召回率水平上的精确度、列表损失、p范数推导、顶部推导、全局对比损失等。虽然这些不可分解的目标及其优化算法在机器学习、计算机视觉、信息检索等领域的文献中已经得到了研究，但在深度学习中优化这些目标面临着一些独特的挑战。在本文中，我们重点介绍了EXM的算法基础，并提供了最近的严格工作。

    This manuscript introduces a new optimization framework for machine learning and AI, named {\bf empirical X-risk minimization (EXM)}. X-risk is a term introduced to represent a family of compositional measures or objectives, in which each data point is compared with a large number of items explicitly or implicitly for defining a risk function. It includes surrogate objectives of many widely used measures and non-decomposable losses, e.g., AUROC, AUPRC, partial AUROC, NDCG, MAP, precision/recall at top $K$ positions, precision at a certain recall level, listwise losses, p-norm push, top push, global contrastive losses, etc. While these non-decomposable objectives and their optimization algorithms have been studied in the literature of machine learning, computer vision, information retrieval, and etc, optimizing these objectives has encountered some unique challenges for deep learning. In this paper, we present recent rigorous efforts for EXM with a focus on its algorithmic foundations a
    
[^218]: AdaTask：自适应多任务在线学习

    AdaTask: Adaptive Multitask Online Learning. (arXiv:2205.15802v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15802](http://arxiv.org/abs/2205.15802)

    AdaTask是一种自适应多任务在线学习算法，当任务被随机激活时，它的遗憾比运行独立算法更小，可以提高一个因子达到$\sqrt{N}$。AdaTask通过马氏距离势函数和变分表示同时学习任务和任务结构。

    

    我们引入并分析了AdaTask，一种适应任务未知结构的多任务在线学习算法。当$N$个任务被随机激活时，我们证明AdaTask的遗憾比运行$N$个独立算法（每个任务一个算法）获得的遗憾要好，可以提高一个因子，达到$\sqrt{N}$。AdaTask可以被看作是Follow-the-Regularized-Leader的比较器自适应版本，使用马氏距离势函数。通过对这个势函数的变分表示，我们的分析揭示了AdaTask如何同时学习任务和它们的结构。我们展示了支持我们发现的实验。

    We introduce and analyze AdaTask, a multitask online learning algorithm that adapts to the unknown structure of the tasks. When the $N$ tasks are stochastically activated, we show that the regret of AdaTask is better, by a factor that can be as large as $\sqrt{N}$, than the regret achieved by running $N$ independent algorithms, one for each task. AdaTask can be seen as a comparator-adaptive version of Follow-the-Regularized-Leader with a Mahalanobis norm potential. Through a variational formulation of this potential, our analysis reveals how AdaTask jointly learns the tasks and their structure. Experiments supporting our findings are presented.
    
[^219]: 带约束学习的混合整数优化

    Mixed-Integer Optimization with Constraint Learning. (arXiv:2111.04469v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2111.04469](http://arxiv.org/abs/2111.04469)

    本论文建立了带约束学习的混合整数优化的方法论基础，提出了一个端到端的数据驱动决策制定流程，并利用混合整数优化可表示性捕捉决策、上下文变量和结果之间的关系。同时，引入了处理学习数据中不确定性的两种方法。

    

    我们建立了一个广泛的方法论基础，用于带约束学习的混合整数优化。我们提出了一个端到端的数据驱动决策制定流程，其中约束和目标直接从数据中使用机器学习进行学习，并将训练好的模型嵌入到优化公式中。我们利用了许多机器学习方法（包括线性模型、决策树、集成和多层感知器）的混合整数优化可表示性，这使得我们能够捕捉决策、上下文变量和结果之间的各种潜在关系。我们还引入了两种处理从数据中学习的固有不确定性的方法。首先，我们通过观测值的凸包来表征决策的信任区域，以确保可靠的推荐并避免外推。我们使用列生成方法有效地将这种表示加入到优化公式中，并提出了一种更灵活的公式来处理低密度的情况。

    We establish a broad methodological foundation for mixed-integer optimization with learned constraints. We propose an end-to-end pipeline for data-driven decision making in which constraints and objectives are directly learned from data using machine learning, and the trained models are embedded in an optimization formulation. We exploit the mixed-integer optimization-representability of many machine learning methods, including linear models, decision trees, ensembles, and multi-layer perceptrons, which allows us to capture various underlying relationships between decisions, contextual variables, and outcomes. We also introduce two approaches for handling the inherent uncertainty of learning from data. First, we characterize a decision trust region using the convex hull of the observations, to ensure credible recommendations and avoid extrapolation. We efficiently incorporate this representation using column generation and propose a more flexible formulation to deal with low-density re
    
[^220]: NeuroBack: 使用图神经网络改进CDCL SAT求解

    NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks. (arXiv:2110.14053v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2110.14053](http://arxiv.org/abs/2110.14053)

    NeuroBack提出了一种使用图神经网络改进CDCL SAT求解的方法，通过预测出现在大多数满足赋值中的变量的阶段，使得求解更加有效，并且消除了对GPU资源的依赖。

    

    命题可满足性（SAT）是一个影响到规划、验证和安全等许多研究领域的NP完全问题。主流的现代SAT求解器基于冲突驱动子句学习（CDCL）算法。最近的研究旨在利用图神经网络（GNNs）增强CDCL SAT求解器。然而，到目前为止，这种方法要么没有使求解更加有效，要么需要大量的GPU资源进行频繁的在线模型推断。为了使GNN的改进变得实用，本文提出了一种名为NeuroBack的方法，它建立在两个洞察上：（1）预测出现在大多数（甚至全部）满足赋值中的变量的阶段（即值）对于CDCL SAT求解至关重要，（2）在SAT求解开始之前，只需查询一次神经模型进行预测即可。一旦训练完成，离线模型推断使NeuroBack能够仅在CPU上执行，消除了对GPU资源的依赖。

    Propositional satisfiability (SAT) is an NP-complete problem that impacts many research fields, such as planning, verification, and security. Mainstream modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL) algorithm. Recent work aimed to enhance CDCL SAT solvers using Graph Neural Networks (GNNs). However, so far this approach either has not made solving more effective, or required substantial GPU resources for frequent online model inferences. Aiming to make GNN improvements practical, this paper proposes an approach called NeuroBack, which builds on two insights: (1) predicting phases (i.e., values) of variables appearing in the majority (or even all) of the satisfying assignments are essential for CDCL SAT solving, and (2) it is sufficient to query the neural model only once for the predictions before the SAT solving starts. Once trained, the offline model inference allows NeuroBack to execute exclusively on the CPU, removing its reliance on GPU resources. To t
    
[^221]: Eigenlearning框架：核回归和宽神经网络的守恒定律视角

    The Eigenlearning Framework: A Conservation Law Perspective on Kernel Regression and Wide Neural Networks. (arXiv:2110.03922v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03922](http://arxiv.org/abs/2110.03922)

    该论文提出了Eigenlearning框架，通过限制核回归在学习正交基函数方面的能力并利用守恒定律来解释模型的泛化能力，同时还为Nakkiran等人的“深度引导”现象，经典奇偶问题难度和对抗鲁棒性提供了理论支持，并与统计物理学中的一个系统进行了类比。

    

    我们针对核岭回归（KRR）的测试风险和其他泛化指标导出了简单的闭式估计。相对于以前的工作，我们的推导大大简化，最终表达式更易于解释。这些改进得益于我们识别出的一个尖锐的守恒定律，它限制了KRR学习任何正交基函数的能力。测试风险和其他感兴趣的对象可以透明地用于我们在核特征基中评估的守恒量来表示。我们使用改进的框架来：i）为Nakkiran等人（2020）的“深度引导”提供理论解释，ii）推广先前关于经典奇偶问题难度的结果，iii）为对抗鲁棒性的研究提供理论工具，并iv）在统计物理学中研究核岭回归和熟知系统之间的严密类比。

    We derive simple closed-form estimates for the test risk and other generalization metrics of kernel ridge regression (KRR). Relative to prior work, our derivations are greatly simplified and our final expressions are more readily interpreted. These improvements are enabled by our identification of a sharp conservation law which limits the ability of KRR to learn any orthonormal basis of functions. Test risk and other objects of interest are expressed transparently in terms of our conserved quantity evaluated in the kernel eigenbasis. We use our improved framework to: i) provide a theoretical explanation for the "deep bootstrap" of Nakkiran et al (2020), ii) generalize a previous result regarding the hardness of the classic parity problem, iii) fashion a theoretical tool for the study of adversarial robustness, and iv) draw a tight analogy between KRR and a well-studied system in statistical physics.
    
[^222]: 不确定性的不平等影响：平权行动与平权信息

    The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information. (arXiv:2102.10019v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.10019](http://arxiv.org/abs/2102.10019)

    本文证明了不确定性对不同人群的影响是不平等的，虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化。我们提出了一种名为平权信息的策略，可以消除这种差异并扩大机会的获取，这可以作为平权行动的替代方案。

    This paper proves that uncertainty has a disparate impact on different demographic groups, with varying types of errors. The proposed strategy, called Affirmative Information, can eliminate this disparity and broaden access to opportunity, serving as an alternative to Affirmative Action.

    像贷款批准、医疗干预和大学录取这样的关键决策是在存在不确定性的情况下进行预测的。在本文中，我们证明了不确定性具有不平等的影响。虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化：平均结果较高的群体通常被分配更高的假阳性率，而平均结果较低的群体则被分配更高的假阴性率。我们展示了额外的数据获取可以消除这种差异并扩大机会的获取。我们称之为平权信息的策略可以作为平权行动的替代方案。

    Critical decisions like loan approvals, medical interventions, and college admissions are guided by predictions made in the presence of uncertainty. In this paper, we prove that uncertainty has a disparate impact. While it imparts errors across all demographic groups, the types of errors vary systematically: Groups with higher average outcomes are typically assigned higher false positive rates, while those with lower average outcomes are assigned higher false negative rates. We show that additional data acquisition can eliminate the disparity and broaden access to opportunity. The strategy, which we call Affirmative Information, could stand as an alternative to Affirmative Action.
    
[^223]: 从TLS跟踪中进行自适应网页指纹识别

    Adaptive Webpage Fingerprinting from TLS Traces. (arXiv:2010.10294v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2010.10294](http://arxiv.org/abs/2010.10294)

    本论文研究了针对TLS协议的现代网页指纹识别对手，提出了一种能够扩展到大量网页、准确分类、低成本的TLS-specific模型，并讨论了相应的对策和对现有对策的评估。

    

    在网页指纹识别中，通过分析用户浏览器与网站服务器之间的加密TLS流量中的模式，路径上的对手能够推断出受害用户加载的具体网页。本研究针对TLS协议研究现代网页指纹识别对手，旨在揭示其能力并提供潜在的防御信息。尽管这个研究领域的重要性（全球大多数互联网用户依赖于带有TLS的标准网页浏览）和潜在的实际影响，但大多数过去的研究都集中在针对匿名网络（如Tor）的特定攻击上。我们引入了一种特定于TLS的模型，它可以：1）扩展到前所未有的目标网页数量，2）可以准确地对数千个在训练期间从未遇到的类别进行分类，3）即使在频繁页面更新的情况下也具有较低的运营成本。基于这些研究结果，我们讨论了TLS-specific的对策，并评估了现有对策的有效性。

    In webpage fingerprinting, an on-path adversary infers the specific webpage loaded by a victim user by analysing the patterns in the encrypted TLS traffic exchanged between the user's browser and the website's servers. This work studies modern webpage fingerprinting adversaries against the TLS protocol; aiming to shed light on their capabilities and inform potential defences. Despite the importance of this research area (the majority of global Internet users rely on standard web browsing with TLS) and the potential real-life impact, most past works have focused on attacks specific to anonymity networks (e.g., Tor). We introduce a TLS-specific model that: 1) scales to an unprecedented number of target webpages, 2) can accurately classify thousands of classes it never encountered during training, and 3) has low operational costs even in scenarios of frequent page updates. Based on these findings, we then discuss TLS-specific countermeasures and evaluate the effectiveness of the existing 
    
[^224]: 意识的神经计算模型：目标对齐的内部表示操作理论（GARIM）

    A Neurocomputational Account of Consciousness: The Goal-Aligning Representation Internal Manipulation Theory (GARIM). (arXiv:1912.13490v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/1912.13490](http://arxiv.org/abs/1912.13490)

    这个论文提出了一个神经计算框架下的意识理论，称为“目标对齐的内部表示操作”（GARIM）。该理论认为意识支持对目标相关的内部表示进行主动操作，使其与追求的目标更加对齐，从而增加目标导向行为的灵活性。

    

    意识作为人类认知的核心要素，已经通过神经科学、心理学、人工智能和机器人技术等多种科学方法进行研究。然而，这些领域之间的不良整合限制了对意识的完整和清晰理解。在这篇论文中，我们通过提出一个神经计算框架下的“目标对齐的内部表示操作”（GARIM）意识理论，为改善这种整合做出了贡献。GARIM理论的核心思想是，意识支持对目标相关的内部表示（如世界状态、对象和行为序列）进行主动操作，使它们与追求的目标更加对齐。这些操作使得意识代理能够在内部产生其所缺乏的知识，以应对新条件和目标，从而增加目标导向行为的灵活性。表示的操作由四个神经功能宏系统（Hierarc...

    Consciousness, a central element of human cognition, has been studied with multiple scientific approaches spanning neuroscience, psychology, artificial intelligence and robotics. Unfortunately, poor integration between these fields limits a full and clear understanding of consciousness. Here we contribute to improving this integration by proposing, within a neurocomputational framework, the `Goal-Aligning Representations Internal Manipulation' (GARIM) theory of consciousness. The central idea of the GARIM theory is that consciousness supports the active manipulation of goal-relevant internal representations (e.g., world states, objects, and action sequences), making them more aligned with the goals pursued. These manipulations allow the conscious agent to internally produce the knowledge it lacks to cope with novel conditions and goals, increasing the flexibility of goal-directed behaviour. The manipulation of representations is supported by four neuro-functional macro-systems (hierarc
    
[^225]: 使用强化学习增强投资组合管理的通用框架

    A General Framework on Enhancing Portfolio Management with Reinforcement Learning. (arXiv:1911.11880v2 [q-fin.PM] UPDATED)

    [http://arxiv.org/abs/1911.11880](http://arxiv.org/abs/1911.11880)

    提出了一个通用的强化学习框架，用于优化投资组合管理过程，可以考虑交易成本和空头限制，并比较了三种不同的强化学习算法在模拟环境中的性能优势。

    

    投资组合管理是金融中涉及对资金和资产进行持续重新配置以满足期望收益和风险配置的艺术与科学。深度强化学习（RL）在投资组合管理中引起了越来越多的关注，其中基于金融数据训练RL代理以优化资产重新配置过程。尽管之前有尝试将RL与投资组合管理相结合，但之前的工作未考虑交易成本或空头限制等实际方面，限制了其适用性。为解决这些限制，我们提出了一个通用的RL框架，用于资产管理，它可以实现连续的资产权重、空头交易以及利用相关特征进行决策。我们比较了三种不同的RL算法的性能：策略梯度与演员-评论家（PGAC），近端策略优化（PPO）和进化策略（ES），并展示了它们在模拟环境中的优势。

    Portfolio management is the art and science in fiance that concerns continuous reallocation of funds and assets across financial instruments to meet the desired returns to risk profile. Deep reinforcement learning (RL) has gained increasing interest in portfolio management, where RL agents are trained base on financial data to optimize the asset reallocation process. Though there are prior efforts in trying to combine RL and portfolio management, previous works did not consider practical aspects such as transaction costs or short selling restrictions, limiting their applicability. To address these limitations, we propose a general RL framework for asset management that enables continuous asset weights, short selling and making decisions with relevant features. We compare the performance of three different RL algorithms: Policy Gradient with Actor-Critic (PGAC), Proximal Policy Optimization (PPO), and Evolution Strategies (ES) and demonstrate their advantages in a simulated environment 
    

