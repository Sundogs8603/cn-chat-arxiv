# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Cameras as Rays: Pose Estimation via Ray Diffusion](https://arxiv.org/abs/2402.14817) | 提出了一种将相机姿势视为射线束的分布表示方法，结合空间图像特征，开发了基于回归和扩散的姿势估计方法，在CO3D数据集上取得了最先进的性能。 |
| [^2] | [Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging](https://arxiv.org/abs/2402.14815) | 本研究调查了全球五个数据集中最先进的视觉语言基础模型在胸片诊断中的算法公平性。我们的发现表明，与董事会认证的放射科医师相比，这些基础模型在诊断边缘化群体时一贯存在低诊断率，甚至在诸如黑人女性之类的交叉亚组中看到更高的比例。 |
| [^3] | [Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking](https://arxiv.org/abs/2402.14811) | 通过对语言模型进行微调，我们研究了如何影响实体跟踪等内部机制，并发现微调能够在数学任务上实现明显的性能提升。 |
| [^4] | [GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion](https://arxiv.org/abs/2402.14810) | 通过GeneOH扩散方法，实现了可泛化的手-物体交互去噪，其中关键创新包括基于接触的HOI表示和领域通用的去噪方案。 |
| [^5] | [CriticBench: Benchmarking LLMs for Critique-Correct Reasoning](https://arxiv.org/abs/2402.14809) | CriticBench是一个综合基准测试，旨在评估LLMs在批判和纠正推理方面的能力，发现批判性训练显著提升性能，逻辑任务更易于修正。 |
| [^6] | [A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health](https://arxiv.org/abs/2402.14807) | 提出了一种决策语言模型DLM，旨在通过使用LLMs作为自动规划器，动态微调RMAB策略，以应对公共卫生中具有挑战性的情境。 |
| [^7] | [Difference Learning for Air Quality Forecasting Transport Emulation](https://arxiv.org/abs/2402.14806) | 本研究提出了一种深度学习传输仿真器，能够在减少计算量的同时保持与现有数值模型相当的技能。 |
| [^8] | [Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset](https://arxiv.org/abs/2402.14804) | 提出了MATH-Vision（MATH-V）数据集，用于评估大型多模态模型（LMMs）的数学推理能力，通过实验证实了当前LMMs和人类在MATH-V上的表现差距。 |
| [^9] | [Link Prediction under Heterophily: A Physics-Inspired Graph Neural Network Approach](https://arxiv.org/abs/2402.14802) | 图神经网络在异质图上的链路预测面临学习能力和表达能力方面的挑战，本论文提出了受物理启发的方法以增强节点分类性能。 |
| [^10] | [Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2402.14800) | 引入了专家级稀疏化技术，提出了专家修剪和跳过的后训练方法，以提高MoE LLMs的部署效率，同时保持模型性能。 |
| [^11] | [Consolidating Attention Features for Multi-view Image Editing](https://arxiv.org/abs/2402.14792) | 通过维护一致的特征并强化查询的一致性，我们提出了QNeRF方法，以实现多视图图像编辑的几何一致性。 |
| [^12] | [Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning](https://arxiv.org/abs/2402.14789) | 自导蒙面自动编码器（SMA）是一种完全领域无关的蒙面建模方法，通过学习蒙面采样而不做任何领域特定的假设，可以在各种数据模态上进行自监督学习。 |
| [^13] | [Rao-Blackwellising Bayesian Causal Inference](https://arxiv.org/abs/2402.14781) | 本文结合顺序化的MCMC结构学习技术和梯度图学习的最新进展，构建了一个有效的贝叶斯因果推断框架，将因果结构推断问题分解为变量拓扑顺序推断和变量父节点集合推断，同时使用高斯过程进行因果机制建模实现精确边缘化，引入了一个Rao-Blackwell化方案。 |
| [^14] | [Causal Imputation for Counterfactual SCMs: Bridging Graphs and Latent Factor Models](https://arxiv.org/abs/2402.14777) | 介绍了一种新颖的基于SCM的模型类，用于因果插补任务，将结果表示为反事实，操作表示为对工具变量进行干预，环境基于初始定义。 |
| [^15] | [2D Matryoshka Sentence Embeddings](https://arxiv.org/abs/2402.14776) | Matryoshka表示学习(MRL)以更细粒度地编码信息，以适应临时任务，同时实现了更小的嵌入大小，从而加快了下游任务的速度。 |
| [^16] | [Generalizing Reward Modeling for Out-of-Distribution Preference Learning](https://arxiv.org/abs/2402.14760) | 通过元学习方法优化通用奖励模型，以解决超出分布偏好学习问题，并提高LLMs在有限偏好反馈下的泛化能力 |
| [^17] | [Generalising realisability in statistical learning theory under epistemic uncertainty](https://arxiv.org/abs/2402.14759) | 统计学习理论中的中心概念在假设训练和测试分布源自相同置信集的情况下如何推广，是对统计学习在认知不确定性下更一般处理的首要步骤。 |
| [^18] | [Batch and match: black-box variational inference with a score-based divergence](https://arxiv.org/abs/2402.14758) | BaM是一种基于分数的离散的BBVI替代方法，针对高方差梯度估计慢收敛问题，能够在高斯变分族中通过封闭形式的近端更新进行优化，在目标分布为高斯时，批处理大小趋于无穷时变分参数更新将指数快速收敛到目标均值和协方差，BaM在多种生成模型推断中表现出良好性能 |
| [^19] | [Prompting a Pretrained Transformer Can Be a Universal Approximator](https://arxiv.org/abs/2402.14753) | 这项研究表明，通过提示或前缀调整Pretrained Transformer可以成为通用逼近器，甚至比之前认为的更小的模型都可以实现这一功能。 |
| [^20] | [Scaling Efficient LLMs](https://arxiv.org/abs/2402.14746) | 训练得到的LLM模型通常是稀疏的，为了提高效率，研究了在训练语料上达到所需准确度的参数最少的高效LLM模型，得出了参数数量与自然训练语料规模之间的关系，并指出扩展可以揭示新技能。 |
| [^21] | [Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation](https://arxiv.org/abs/2402.14744) | 提出了一种将大型语言模型LLMs整合到代理框架中的新方法，用于生成个人移动生成，重点是解决将LLMs与真实城市流动数据对齐的问题，并提出了一种自洽方法和检索增强策略来实现可解释活动生成。 |
| [^22] | [Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs](https://arxiv.org/abs/2402.14740) | 在大型语言模型中，重新审视REINFORCE风格优化对于学习人类反馈具有重要意义，简化优化方法可以提高性能。 |
| [^23] | [How Transformers Learn Causal Structure with Gradient Descent](https://arxiv.org/abs/2402.14735) | Transformers通过梯度下降学习因果结构的过程中，关键的证据是注意力矩阵的梯度编码了token之间的互信息 |
| [^24] | [Clifford-Steerable Convolutional Neural Networks](https://arxiv.org/abs/2402.14730) | 提出了Clifford-Steerable卷积神经网络（CS-CNNs），通过在伪欧几里德空间上处理多矢场，利用Clifford群等变神经网络对$\mathrm{O}(p,q)$可导核进行隐式参数化，显着且一致地优于流体动力学和相对论电动力学预测任务的基准方法 |
| [^25] | [Incorporating Expert Rules into Neural Networks in the Framework of Concept-Based Learning](https://arxiv.org/abs/2402.14726) | 本文提出了将专家规则融入神经网络的方法，通过形成约束和使用凸多面体来保证输出概率不违反专家规则，实现了归纳与演绎学习的结合。 |
| [^26] | [IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus](https://arxiv.org/abs/2402.14710) | 发布了IEPile，一个包含约0.32B个标记的综合双语IE指令语料库，通过收集和清理33个现有IE数据集并引入基于模式的指令生成，可以提高大型语言模型在信息抽取领域的性能，尤其是零样本泛化。 |
| [^27] | [CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks](https://arxiv.org/abs/2402.14708) | 该论文提出了一种名为CaT-GNN的新型信用卡欺诈检测方法，通过因果不变性学习揭示交易数据中的固有相关性，并引入因果混合策略来增强模型的鲁棒性和可解释性。 |
| [^28] | [On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation](https://arxiv.org/abs/2402.14703) | 本文提出了针对POMDP结构的新颖覆盖假设，以解决未来依赖价值函数方法中的长度指数增长问题。 |
| [^29] | [COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling](https://arxiv.org/abs/2402.14701) | 本文提出了一种名为COMPASS的新框架，通过分析心理治疗会话中的自然语言，直接推断治疗工作联盟，为临床精神病学提供了可解释性，并在识别与正在治疗的疾病相关的新兴模式方面发挥作用。 |
| [^30] | [Big data analytics to classify earthwork-related locations: A Chengdu study](https://arxiv.org/abs/2402.14698) | 使用大数据分析方法，研究者利用自卸车轨迹、城市兴趣点和土地覆盖数据，成功对城市灰尘污染源进行了分类，证明仅需有限数量特征即可实现高准确度分类。 |
| [^31] | [A Quick Introduction to Quantum Machine Learning for Non-Practitioners](https://arxiv.org/abs/2402.14694) | 量子机器学习利用量子计算原理和算法提升经典机器学习模型，可能减少在量子硬件上的网络大小和训练时间 |
| [^32] | [PeriodGrad: Towards Pitch-Controllable Neural Vocoder Based on a Diffusion Probabilistic Model](https://arxiv.org/abs/2402.14692) | 本文提出了一种基于扩散概率模型的神经声码器，通过加入明确的周期信号，提高了声音质量并提供了更好的音高控制。 |
| [^33] | [Q-Probe: A Lightweight Approach to Reward Maximization for Language Models](https://arxiv.org/abs/2402.14688) | Q-Probe是一种轻量级方法，通过学习简单的线性函数在模型的嵌入空间中重新加权候选完成，从而调整预训练语言模型以最大化任务特定的奖励函数，在各种领域中获得显著收益。 |
| [^34] | [Adaptive time series forecasting with markovian variance switching](https://arxiv.org/abs/2402.14684) | 本论文提出了一种基于马尔可夫方差切换的自适应时间序列预测方法，通过在线学习理论和专家聚合方法来学习方差，相比于传统方法在电量负荷预测问题中表现更优。 |
| [^35] | [Visual Hallucinations of Multi-modal Large Language Models](https://arxiv.org/abs/2402.14683) | 多模大语言模型通过生成多样的视觉幻觉实例来检验其性能，发现现有的模型在这方面存在幻觉问题，为进一步研究和改进提供了线索。 |
| [^36] | [Bayesian Off-Policy Evaluation and Learning for Large Action Spaces](https://arxiv.org/abs/2402.14664) | 该论文提出了一个统一的贝叶斯框架，通过结构化和信息丰富的先验捕捉动作之间的相关性，提出了一个适用于离策略评估和学习的通用贝叶斯方法sDM，并引入了能评估算法在多问题实例中平均表现的贝叶斯指标，分析了sDM在OPE和OPL中利用动作相关性的优势，并展示了其强大性能 |
| [^37] | [Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off](https://arxiv.org/abs/2402.14648) | 重新审视了基于表示的不变性正则化方法，提出了Asymmetrically Representation-regularized Adversarial Training (AR-AT)来解决“梯度冲突”和混合分布问题，改善鲁棒性-准确性权衡。 |
| [^38] | [CoLoRA: Continuous low-rank adaptation for reduced implicit neural modeling of parameterized partial differential equations](https://arxiv.org/abs/2402.14646) | CoLoRA通过连续低秩自适应提供了一种快速预测参数化偏微分方程解演变的简化神经网络建模方法 |
| [^39] | [Sparse Linear Regression and Lattice Problems](https://arxiv.org/abs/2402.14645) | 本文提供了关于稀疏线性回归在所有高效算法的平均情况困难性的证据，假设格问题的最坏情况困难性。 |
| [^40] | [latrend: A Framework for Clustering Longitudinal Data](https://arxiv.org/abs/2402.14621) | latrend框架为纵向数据聚类提供了统一的方法应用框架，方便研究人员比较不同方法，实现快速原型设计。 |
| [^41] | [Federated Complex Qeury Answering](https://arxiv.org/abs/2402.14609) | 研究了在多源知识图谱上回答复杂查询的联邦式方法，解决了知识图谱中的隐私保护和答案检索的挑战 |
| [^42] | [Balanced Resonate-and-Fire Neurons](https://arxiv.org/abs/2402.14603) | 平衡的谐振-放电（BRF）神经元的引入缓解了RF神经元的固有限制，在循环尖峰神经网络（RSNNs）中表现出更高的任务性能，产生更少的脉冲，并需要更少的参数。 |
| [^43] | [Bringing Generative AI to Adaptive Learning in Education](https://arxiv.org/abs/2402.14601) | 生成式人工智能技术与自适应学习概念的交叉研究将对教育中下一阶段学习格式的发展做出重要贡献。 |
| [^44] | [Brain-inspired Distributed Memorization Learning for Efficient Feature-free Unsupervised Domain Adaptation](https://arxiv.org/abs/2402.14598) | 提出了一种受到人类大脑记忆机制启发的分布式记忆学习机制，通过随机连接的神经元记忆输入信号的关联，并基于置信度关联分布式记忆，能够在无需特征微调的情况下，通过强化记忆适应新领域，适合部署在边缘设备上。 |
| [^45] | [Learning Style Identification Using Semi-Supervised Self-Taught Labeling](https://arxiv.org/abs/2402.14597) | 提出了一种使用半监督机器学习方法来检测学生学习风格的新方法，并展示出可以利用少量标记数据产生可靠分类模型。 |
| [^46] | [Scaling Up LLM Reviews for Google Ads Content Moderation](https://arxiv.org/abs/2402.14590) | 本研究提出了一种用于在Google广告中进行内容管理的方法，通过使用LLMs审核代表性广告并将决策传播回其群集，将审核数目减少了3个数量级以上，同时实现了2倍的召回率。 |
| [^47] | [Avoiding an AI-imposed Taylor's Version of all music history](https://arxiv.org/abs/2402.14589) | AI音乐在模仿人类音乐过程中可能形成自己的偏爱，可能对所有音乐历史构成潜在威胁，同时讨论了未来可能在保留世界音乐文化多样性方面的挑战。 |
| [^48] | [Bandits with Abstention under Expert Advice](https://arxiv.org/abs/2402.14585) | 我们提出了CBA算法，其利用放弃参与游戏的假设获得了可以显著改进经典Exp4算法的奖励界限，成为首个对一般置信评级预测器的预期累积奖励实现界限的研究者，并在专家案例中实现了一种新颖的奖励界限。 |
| [^49] | [Enhancement of High-definition Map Update Service Through Coverage-aware and Reinforcement Learning](https://arxiv.org/abs/2402.14582) | 本文提出了一种Q学习覆盖时间感知算法，以优化车载网络和HD地图更新的服务质量，以克服网络拥塞。 |
| [^50] | [Text Role Classification in Scientific Charts Using Multimodal Transformers](https://arxiv.org/abs/2402.14579) | 该研究提出了使用多模态Transformer在科学图表中进行文本角色分类的方法，并在实验中发现LayoutLMv3在性能上优于UDOP，最高达到了82.87的F1-macro分数。 |
| [^51] | [Multivariate Online Linear Regression for Hierarchical Forecasting](https://arxiv.org/abs/2402.14578) | 提出了MultiVAW方法，将Vovk-Azoury-Warmuth算法扩展到多元设置，同时应用于在线层次预测问题，并且能够放宽传统分析所做的假设 |
| [^52] | [Edge Caching Based on Deep Reinforcement Learning and Transfer Learning](https://arxiv.org/abs/2402.14576) | 本文提出了一种基于双深度Q学习的缓存方法，通过半马尔可夫决策过程（SMDP）适应现实场景中随机请求到达的特性，综合考虑各种文件特征。 |
| [^53] | [CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for Optimized Learning Fusion](https://arxiv.org/abs/2402.14551) | CLCE方法结合了标签感知对比学习与交叉熵损失，通过协同利用难例挖掘提高了性能表现 |
| [^54] | [OmniPred: Language Models as Universal Regressors](https://arxiv.org/abs/2402.14547) | 本文提出了OmniPred框架，用于训练语言模型作为通用的端到端回归器，实验证明，在多个任务上训练时，语言模型能够显著优于传统回归模型。 |
| [^55] | [A Framework for Variational Inference of Lightweight Bayesian Neural Networks with Heteroscedastic Uncertainties](https://arxiv.org/abs/2402.14532) | 提出了一种新框架，通过将异方差Aleatoric和认知方差嵌入到学习BNN参数的方差中，改善了轻量级网络的预测性能。 |
| [^56] | [ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization](https://arxiv.org/abs/2402.14528) | 该论文提出了ACE算法，通过引入因果感知熵正则化，有效评估不同行为的重要性，并分析梯度休眠现象，引入休眠引导复位机制，在多个连续控制任务中取得显著性能优势。 |
| [^57] | [Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs](https://arxiv.org/abs/2402.14527) | 本文研究了基因组学或转录组数据上的联邦学习，使用 TensorFlow Federated 和 Flower 框架进行实验，以培训疾病预后和细胞类型分类模型。 |
| [^58] | [Kinematically Constrained Human-like Bimanual Robot-to-Human Handovers](https://arxiv.org/abs/2402.14525) | 该论文提出了一个框架，利用隐马尔可夫模型生成受运动约束的类人双手机器人动作，以实现流畅自然的机器人对人类物体交接。 |
| [^59] | [Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond](https://arxiv.org/abs/2402.14522) | 提出了一种框架用于统一不同模型的任务嵌入，使得任务嵌入可以跨越各种模型，并在单一向量空间内进行比较和分析。 |
| [^60] | [Spectral invariance and maximality properties of the frequency spectrum of quantum neural networks](https://arxiv.org/abs/2402.14515) | 量子神经网络研究了频谱的极大性质，证明了在一类模型中存在极大结果，以及在一些条件下存在保持频谱的光谱不变性，解释了文献中观察到的结果对称性。 |
| [^61] | [Imbalanced Data Clustering using Equilibrium K-Means](https://arxiv.org/abs/2402.14490) | Equilibrium K-Means（EKM）是一种新颖且简单的K均值类型算法，通过减少聚类中心在大类簇中心聚集的倾向，显著改善了不平衡数据的聚类结果。 |
| [^62] | [A Class of Topological Pseudodistances for Fast Comparison of Persistence Diagrams](https://arxiv.org/abs/2402.14489) | 本文介绍了一类称为扩展拓扑伪距离（ETD）的伪距离，具有可调节的复杂度，可以近似切片。 |
| [^63] | [Are Bounded Contracts Learnable and Approximately Optimal?](https://arxiv.org/abs/2402.14486) | 分析了在隐藏动作模型下的合同与委托-代理问题，提出了两个学习算法可以找到几乎最优的有界合同，对于一般情况的查询次数具有多项式上界，并且直接学习潜在的结果分布。 |
| [^64] | [SpanSeq: Similarity-based sequence data splitting method for improved development and assessment of deep learning projects](https://arxiv.org/abs/2402.14482) | SpanSeq 是一种用于生物数据序列的数据库分区方法，能够避免训练集和测试集之间的数据泄漏。 |
| [^65] | [Towards Automated Causal Discovery: a case study on 5G telecommunication data](https://arxiv.org/abs/2402.14481) | 该论文介绍了自动因果发现（AutoCD）的概念，提出了可以完全自动化应用因果发现和推理方法的系统，并展示了其在合成数据集和5G电信数据上的性能。 |
| [^66] | [DynGMA: a robust approach for learning stochastic differential equations from data](https://arxiv.org/abs/2402.14475) | DynGMA方法通过引入新的密度近似，优于基准方法在学习完全未知的漂移和扩散函数以及计算不变性方面的准确性。 |
| [^67] | [Data Science with LLMs and Interpretable Models](https://arxiv.org/abs/2402.14474) | 该研究展示了大型语言模型LLMs在描述、解释和调试广义加性模型GAMs方面的出色表现，结合LLMs的灵活性和GAMs准确描述的统计模式，可以实现数据集摘要、问答和模型评估。 |
| [^68] | [Reimagining Anomalies: What If Anomalies Were Normal?](https://arxiv.org/abs/2402.14469) | 方法提出了一种新颖的解释方法，生成多个反事实示例以捕获异常的多样概念，为用户提供对触发异常检测器机制的高级语义解释，允许探索“假设情景”。 |
| [^69] | [Machine Learning Reveals Large-scale Impact of Posidonia Oceanica on Mediterranean Sea Water](https://arxiv.org/abs/2402.14459) | 该研究利用机器学习揭示了波西多尼亚海草在地中海水域中对水生物地球化学特性的影响，强调了其在海洋生态系统中的关键作用。 |
| [^70] | [Model-Based Reinforcement Learning Control of Reaction-Diffusion Problems](https://arxiv.org/abs/2402.14446) | 本文探索了使用自动控制策略处理热和疾病传输初始边界值问题，并通过引入新的奖励函数和改进的强化学习算法来驱动传输场的流动。 |
| [^71] | [Parallelized Midpoint Randomization for Langevin Monte Carlo](https://arxiv.org/abs/2402.14434) | 探索在能够进行梯度平行评估的框架中的抽样问题，提出了并行化的随机中点方法，并通过新技术导出了对抽样和目标密度之间Wasserstein距离的上界，量化了并行处理单元带来的运行时改进。 |
| [^72] | [Robust Training of Federated Models with Extremely Label Deficiency](https://arxiv.org/abs/2402.14430) | 提出了一种名为Twin-sight的双模型范式，以增强联邦半监督学习中标签和无标签数据之间的互动，并通过引入邻域保持约束来提高这两个模型之间的协同作用 |
| [^73] | [Text me the data: Generating Ground Pressure Sequence from Textual Descriptions for HAR](https://arxiv.org/abs/2402.14427) | 使用Text-to-Pressure（T2P）框架，结合深度学习技术，从文本描述中生成高质量地面压力序列，实现了文本与生成动作的一致性。 |
| [^74] | [Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning](https://arxiv.org/abs/2402.14407) | 利用离散扩散结合生成式预训练和少量机器人视频微调，实现从人类视频到机器人策略学习的知识迁移。 |
| [^75] | [Global Safe Sequential Learning via Efficient Knowledge Transfer](https://arxiv.org/abs/2402.14402) | 提出了考虑转移安全的全局顺序学习方法，以加速安全学习，并通过预先计算源组件来减少额外的计算负载。 |
| [^76] | [Diffusion Model Based Visual Compensation Guidance and Visual Difference Analysis for No-Reference Image Quality Assessment](https://arxiv.org/abs/2402.14401) | 本研究将扩散模型引入无参考图像质量评估领域，设计了新的扩散恢复网络，提高了学习高级和低级视觉特征的效率。 |
| [^77] | [Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks](https://arxiv.org/abs/2402.14400) | 使用数据驱动评估个体动作模式，利用自适应图卷积网络对3D婴儿动力学进行建模，相较于传统机器学习取得了改进。 |
| [^78] | [Closed-Form Bounds for DP-SGD against Record-level Inference](https://arxiv.org/abs/2402.14397) | 该论文提出了一种新方法，通过封闭形式界限评估机器学习模型在特定记录级威胁下的隐私保护，避免了通过DP进行间接评估。 |
| [^79] | [Quantum Circuit Optimization with AlphaTensor](https://arxiv.org/abs/2402.14396) | 使用基于深度强化学习的AlphaTensor-Quantum方法，在容错量子计算中优化T门数量，显著减少电路的T计数。 |
| [^80] | [Graph Parsing Networks](https://arxiv.org/abs/2402.14393) | 本研究提出了一种受自底向上的语法归纳启发的图解析算法，使得Graph Parsing Network（GPN）能够自适应地学习每个独特图的个性化池化结构。 |
| [^81] | [MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding](https://arxiv.org/abs/2402.14391) | 通过定义氨基酸残基的微环境，结合蛋白质序列和结构信息，提出了用于高效蛋白质相互作用预测的微环境感知蛋白质嵌入方法。 |
| [^82] | [Securing Transactions: A Hybrid Dependable Ensemble Machine Learning Model using IHT-LR and Grid Search](https://arxiv.org/abs/2402.14389) | 一种新型混合集成机器学习模型，利用网格搜索将决策树、随机森林、KNN和多层感知机等算法智能结合，以提高欺诈检测效果。 |
| [^83] | [WindDragon: Enhancing wind power forecasting with Automated Deep Learning](https://arxiv.org/abs/2402.14385) | 利用自动深度学习结合数值天气预报风速图，WindDragon系统在全国范围内实现了短期风力预测，为电网运营和系统平衡提供关键支持。 |
| [^84] | [Generative Adversarial Network with Soft-Dynamic Time Warping and Parallel Reconstruction for Energy Time Series Anomaly Detection](https://arxiv.org/abs/2402.14384) | 本研究提出了一种利用深度卷积生成对抗网络来进行能源时间序列异常检测的方法，结合软动态时间规整和并行重构技术，通过将重构损失和潜在空间的先验概率分布相结合作为异常分数，加速了检测过程，实验证明在建筑能源消耗的异常检测上表现出很好的效果。 |
| [^85] | [Representation Learning for Frequent Subgraph Mining](https://arxiv.org/abs/2402.14367) | 提出了一种新颖的神经方法SPMiner，用于在大型目标图中近似找到频繁子图，通过结合图神经网络、顺序嵌入空间和高效的搜索策略，以识别出在目标图中出现最频繁的网络子图模式。 |
| [^86] | [OpenTab: Advancing Large Language Models as Open-domain Table Reasoners](https://arxiv.org/abs/2402.14361) | OpenTab 是一个开放领域表格推理框架，利用表格检索器扩展了大型语言模型的知识范围，并通过生成SQL程序和基于事实的推理实现了在开放和封闭领域设置中明显优于基线的性能。 |
| [^87] | [Uncertainty-driven and Adversarial Calibration Learning for Epicardial Adipose Tissue Segmentation](https://arxiv.org/abs/2402.14349) | 提出了一种基于不确定性驱动和对抗校准学习的心外脂肪组织分割方法，通过特征潜空间多级监督网络(SPDNet)，增强分割以更准确估计EAT体积 |
| [^88] | [Dependable Distributed Training of Compressed Machine Learning Models](https://arxiv.org/abs/2402.14346) | 提出了DepL框架，实现了可靠的学习编排，能够确保以最低训练成本达到目标学习质量。 |
| [^89] | [AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales](https://arxiv.org/abs/2402.14337) | 提出了在自然语言推理中处理引发模式合理性不确定性的不完美理由的方法，实施了使用熵分数和模型先验信念来指导模型的策略，并在实证中展示了方法相对于敌对理由的稳健性能优势 |
| [^90] | [HyperFast: Instant Classification for Tabular Data](https://arxiv.org/abs/2402.14335) | HyperFast是一个针对表格数据的即时分类方法，通过在单次前向传递中生成特定任务的神经网络，避免了需进行模型训练的必要性，并在实验中展现出高度竞争力。 |
| [^91] | [From Large to Small Datasets: Size Generalization for Clustering Algorithm Selection](https://arxiv.org/abs/2402.14332) | 通过引入尺寸泛化概念，研究了在半监督设置下的聚类算法选择问题，提出了能够在小实例上保证准确度最高的算法也将在原始大实例上拥有最高准确度的条件。 |
| [^92] | [Structure-Based Drug Design via 3D Molecular Generative Pre-training and Sampling](https://arxiv.org/abs/2402.14315) | 本研究提出了MolEdit3D，将3D分子生成与优化框架相结合，解决了现有基于优化的方法编辑分子时选择在2D空间中的问题。 |
| [^93] | [An FPGA-Based Accelerator Enabling Efficient Support for CNNs with Arbitrary Kernel Sizes](https://arxiv.org/abs/2402.14307) | 该论文提出了基于FPGA的推理加速器，能够高效支持任意内核大小的CNN，通过Z流方法优化数据流、Kseg方案降低存储需求，以及VF和HF方法优化CNN部署。 |
| [^94] | [Towards Efficient Pareto-optimal Utility-Fairness between Groups in Repeated Rankings](https://arxiv.org/abs/2402.14305) | 引入了使用Expohedron解决帕累托最优效用-公平性问题的新方法，避免了Birkhoff-von Neumann分解的高计算复杂度。 |
| [^95] | [GenSERP: Large Language Models for Whole Page Presentation](https://arxiv.org/abs/2402.14301) | 该论文提出了GenSERP框架，利用大型语言模型动态整理搜索结果并根据用户查询生成连贯的搜索引擎结果页面。 |
| [^96] | [High-arity PAC learning via exchangeability](https://arxiv.org/abs/2402.14294) | 提出高参数PAC学习理论，利用结构化相关性和交换分布取代i.i.d.抽样，证明了统计学习基本定理的高维版本。 |
| [^97] | [CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations](https://arxiv.org/abs/2402.14290) | CEV-LM 是一个轻量、半自回归语言模型，利用受限制的编辑向量控制文本的速度、音量和绕圈度量，从而更精准地定制生成的文本形状，比现有控制方法具有更好的控制效果。 |
| [^98] | [TinyLLaVA: A Framework of Small-scale Large Multimodal Models](https://arxiv.org/abs/2402.14289) | TinyLLaVA框架使得小型多模态模型能够通过更好的数据质量和训练方案达到与大型模型相媲美的性能，最佳模型TinyLLaVA-3.1B在整体性能上优于现有的7B模型。 |
| [^99] | [Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion](https://arxiv.org/abs/2402.14285) | 介绍了一种用于符号音乐生成的不可微分规则引导的新方法，引入了可以与之即插即用的高时间分辨率潜在扩散架构，对音乐质量取得了显著进步 |
| [^100] | [Secure Navigation using Landmark-based Localization in a GPS-denied Environment](https://arxiv.org/abs/2402.14280) | 该论文提出了一个整合了基于地标的定位与扩展卡尔曼滤波器的框架，用于在战场上预测移动实体的未来状态，以解决在无GPS环境中导航的安全问题。 |
| [^101] | [Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization](https://arxiv.org/abs/2402.14270) | 通过硬样本加权持续训练的方法，该研究提出了IR-DRO框架，通过动态优先考虑训练中信息丰富的样本，以改善LLM泛化能力。 |
| [^102] | [Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation](https://arxiv.org/abs/2402.14264) | 采用结构不可知的统计下界框架，证明了双稳健估计器在平均处理效应（ATE）和平均处理效应方面的统计最优性 |
| [^103] | [Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond](https://arxiv.org/abs/2402.14259) | 本论文提出了一种新方法单词序列熵（WSE），用于在自由形式医学问答任务中量化答案的不确定性，相比其他基线方法表现更优秀。 |
| [^104] | [A hierarchical decomposition for explaining ML performance discrepancies](https://arxiv.org/abs/2402.14254) | 提出了一种详细的变量级分解方法，可以量化每个变量对性能差异的影响，为实现有针对性干预措施提供更深入的理解 |
| [^105] | [Reconstruction-Based Anomaly Localization via Knowledge-Informed Self-Training](https://arxiv.org/abs/2402.14246) | 提出了一种名为知识驱动自训练（KIST）的基于重建的方法，通过将领域专家总结的异常知识集成到重建模型中，从而更好地利用异常样本并进一步提高异常定位性能。 |
| [^106] | [Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models](https://arxiv.org/abs/2402.14245) | 利用多模态大语言模型为机器人操作提供自动偏好反馈，提升决策效果 |
| [^107] | [MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and Dynamic Distance Constraint](https://arxiv.org/abs/2402.14244) | 使用人类反馈和动态距离约束对层次化强化学习进行引导，解决了找到适当子目标的问题，并设计了双策略以稳定训练。 |
| [^108] | [Automated Design and Optimization of Distributed Filtering Circuits via Reinforcement Learning](https://arxiv.org/abs/2402.14236) | 提出一种通过强化学习算法实现的自动化设计方法，显著提高了分布式滤波电路设计的效率和质量。 |
| [^109] | [Sample-Efficient Linear Regression with Self-Selection Bias](https://arxiv.org/abs/2402.14229) | 提出了一种新颖且接近最优的样本高效算法，可在未知指数设定下的具有自我选择偏差的线性回归问题中高效地恢复参数向量，具有显著优化的时间复杂度和多项式样本复杂度。 |
| [^110] | [COPR: Continual Human Preference Learning via Optimal Policy Regularization](https://arxiv.org/abs/2402.14228) | 提出了Continual Optimal Policy Regularization (COPR) 方法，通过借鉴最优策略理论，利用采样分布作为示范和正则化约束，以动态地对当前策略进行正则化，从而使强化学习从人类反馈中学习在持续学习情境下更加稳健 |
| [^111] | [Quaternion recurrent neural network with real-time recurrent learning and maximum correntropy criterion](https://arxiv.org/abs/2402.14227) | 通过结合实时递归学习算法和最大相关性准则作为损失函数，提出了用于处理含异常值3D和4D数据的鲁棒四元数递归神经网络，所使用的最大相关性损失函数对异常值不太敏感，适用于多维嘈杂或不确定数据应用。 |
| [^112] | [Estimating Unknown Population Sizes Using the Hypergeometric Distribution](https://arxiv.org/abs/2402.14220) | 提出了一种使用超几何似然解决估计离散分布挑战的新方法，即使存在严重的欠采样，也能实现，且在人口规模估计的准确性和学习能力方面优于其他方法。 |
| [^113] | [Contrastive Learning of Shared Spatiotemporal EEG Representations Across Individuals for Naturalistic Neuroscience](https://arxiv.org/abs/2402.14213) | 通过对比学习，利用神经网络最大化相同刺激下各个个体的EEG表示的相似性，以此实现个体间共享时空脑电图表示的学习。 |
| [^114] | [Moonwalk: Inverse-Forward Differentiation](https://arxiv.org/abs/2402.14212) | Moonwalk引入了一种基于向量-逆-Jacobian乘积的新技术，加速前向梯度计算，显著减少内存占用，并在保持真实梯度准确性的同时，将计算时间降低了几个数量级。 |
| [^115] | [Content Conditional Debiasing for Fair Text Embedding](https://arxiv.org/abs/2402.14208) | 通过在内容条件下确保敏感属性与文本嵌入之间的条件独立性，我们提出了一种可以改善公平性的新方法，在保持效用的同时，解决了缺乏适当训练数据的问题。 |
| [^116] | [Compression Robust Synthetic Speech Detection Using Patched Spectrogram Transformer](https://arxiv.org/abs/2402.14205) | 使用Patched Spectrogram Transformer实现的PS3DT合成语音检测器在ASVspoof2019数据集上表现优越，相对于其他基于频谱图的方法具有更好的检测性能 |
| [^117] | [Comparing Graph Transformers via Positional Encodings](https://arxiv.org/abs/2402.14202) | 本文比较了使用绝对位置编码（APEs）和相对位置编码（RPEs）的图变换器，在最大化区分能力方面是等效的。 |
| [^118] | [BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human Racing Gameplay](https://arxiv.org/abs/2402.14194) | BeTAIL结合了行为转换器（BeT）策略和在线对抗性模仿学习（AIL），以学习从人类专家示范中学到的顺序决策过程，并纠正环境中的分布转移。 |
| [^119] | [Diversity-Aware Ensembling of Language Models Based on Topological Data Analysis](https://arxiv.org/abs/2402.14184) | 基于拓扑数据分析的方法，通过估计NLP模型集成的权重，提高了集成模型的质量，提高了文本分类准确性和相关不确定性估计。 |
| [^120] | [Linear Transformers are Versatile In-Context Learners](https://arxiv.org/abs/2402.14180) | 线性变换器展示了在处理复杂问题和噪音干扰数据时的多功能性，通过发现一种新颖的优化算法，超越了许多合理的基线。 |
| [^121] | [A Temporal Bias Correction using a Machine Learning Attention model](https://arxiv.org/abs/2402.14169) | 本论文提出了一种新颖的偏差校正方法，将校准视为概率模型而不是算法流程，利用机器学习概率注意力模型来适配偏差校正任务，可准确校正具有长期时间属性的气候统计数据，提高了在这些数据上进行可靠影响研究的准确性。 |
| [^122] | [T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching](https://arxiv.org/abs/2402.14167) | T-Stitch提出了一种轨迹拼接的采样技术，能够在几乎不降低生成质量的情况下提高采样效率，通过在初始阶段使用较小的DPM来生成全局结构，然后切换到较大的DPM，从而实现高效的采样。 |
| [^123] | [Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement](https://arxiv.org/abs/2402.14160) | 提出了递归推测解码(RSD)方法，通过无重复抽样最大化树的多样性，从而进一步加速LLM推理过程。 |
| [^124] | [BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives](https://arxiv.org/abs/2402.14151) | BIRCO基准评估基于大型语言模型的信息检索系统对多方面用户目标的检索能力，发现新的检索协议和更强大的模型是解决复杂用户需求的必要条件。 |
| [^125] | [Neural Networks and Friction: Slide, Hold, Learn](https://arxiv.org/abs/2402.14148) | 循环神经网络利用GRU架构学习合成数据中复杂摩擦定律动力学，展示了机器学习模型在理解和模拟摩擦过程物理的潜力。 |
| [^126] | [Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains](https://arxiv.org/abs/2402.14145) | 提出了一种两阶段的乘幂稳健估计方法，用于改善表格数据分析中每个个体部分的模型性能，并建立了在测试风险上的理论保证。 |
| [^127] | [NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning](https://arxiv.org/abs/2402.14139) | NeuroFlux是一个为内存受限场景量身定制的CNN训练系统，提出了自适应辅助网络和块特定的自适应批处理大小的创新机遇。 |
| [^128] | [GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal Sensors](https://arxiv.org/abs/2402.14136) | GDTM提供了一个新的室内地理空间跟踪数据集，包含了分布式多模态传感器和可重新配置传感器节点位置，可以用于研究处理多模态数据的体系结构优化和模型对不良传感条件和传感器位置变化的稳健性。 |
| [^129] | [Random forests for detecting weak signals and extracting physical information: a case study of magnetic navigation](https://arxiv.org/abs/2402.14131) | 使用随机森林模型可以提高对弱信号的检测精度，并从时间序列数据中直接获取位置信息。 |
| [^130] | [DeiSAM: Segment Anything with Deictic Prompting](https://arxiv.org/abs/2402.14123) | DeiSAM提出将大型预训练神经网络与可区分逻辑推理器结合，用于指示提示性分割，实现了在复杂场景中对象的分割 |
| [^131] | [Computational-Statistical Gaps for Improper Learning in Sparse Linear Regression](https://arxiv.org/abs/2402.14103) | 该研究探讨了稀疏线性回归中的计算统计差距问题，为了高效地找到可以在样本上实现非平凡预测误差的潜在密集估计的回归向量，需要至少 $\Omega(k \log (d/k))$ 个样本。 |
| [^132] | [Learning dynamic representations of the functional connectome in neurobiological networks](https://arxiv.org/abs/2402.14102) | 该论文提出了一种学习神经元动态亲和关系的无监督方法，以揭示不同时间点神经元之间形成的社区，从而揭示了动态功能连接组。 |
| [^133] | [Intriguing Properties of Modern GANs](https://arxiv.org/abs/2402.14098) | 现代生成对抗网络学习的流形不符合训练数据分布，学习到的密度与数据分布相差甚远。 |
| [^134] | [Zero-shot generalization across architectures for visual classification](https://arxiv.org/abs/2402.14095) | 不同神经网络在跨架构和层间泛化到未知类别的能力存在差异，准确性并不是泛化能力的良好预测因子，泛化能力随着层深度呈非单调变化。 |
| [^135] | [LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons](https://arxiv.org/abs/2402.14086) | LexC-Gen提出了一种词典条件数据生成方法，可以以大规模生成低资源语言分类任务数据，取得了较好的效果。 |
| [^136] | [Robust Learning of Noisy Time Series Collections Using Stochastic Process Models with Motion Codes](https://arxiv.org/abs/2402.14081) | 使用具有学习谱核的混合高斯过程的潜变量模型方法，针对嘈杂时间序列数据进行鲁棒学习。 |
| [^137] | [Efficient Normalized Conformal Prediction and Uncertainty Quantification for Anti-Cancer Drug Sensitivity Prediction with Deep Regression Forests](https://arxiv.org/abs/2402.14080) | 通过深度回归森林计算样本方差，提高了抗癌药物敏感性预测中的规范化置信预测效率和覆盖率 |
| [^138] | [Improving Language Understanding from Screenshots](https://arxiv.org/abs/2402.14073) | 本文提出了一种屏幕截图语言模型，通过引入新的Patch-and-Text Prediction（PTP）目标来改善文本能力，并取得了与BERT相当的性能。 |
| [^139] | [Generative Adversarial Models for Extreme Downscaling of Climate Datasets](https://arxiv.org/abs/2402.14049) | 该方法提出了一种基于条件GAN的地理空间数据缩放方法，可以从非常低分辨率的输入生成高分辨率准确的气候数据集，并且明确考虑了不确定性。 |
| [^140] | [PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization](https://arxiv.org/abs/2402.14048) | PolyNet通过学习互补解决策略来改善解空间探索，避免了人为规则导致解决方案质量下降的问题。 |
| [^141] | [Simple and Effective Transfer Learning for Neuro-Symbolic Integration](https://arxiv.org/abs/2402.14047) | 提出了一种简单而有效的方法，通过在下游任务上预训练神经模型，然后通过迁移学习在相同任务上对NeSy模型进行训练，以实现神经符号一体化的改进。 |
| [^142] | [Advancing Low-Rank and Local Low-Rank Matrix Approximation in Medical Imaging: A Systematic Literature Review and Future Directions](https://arxiv.org/abs/2402.14045) | 本文系统综述了在医学成像中应用低秩矩阵逼近（LRMA）和其派生物局部LRMA（LLRMA）的作品，并指出自2015年以来医学成像领域开始偏向于使用LLRMA，显示其在捕获医学数据中复杂结构方面的潜力和有效性。 |
| [^143] | [Protect and Extend -- Using GANs for Synthetic Data Generation of Time-Series Medical Records](https://arxiv.org/abs/2402.14042) | 本研究使用GANs生成了时间序列合成痴呆患者医疗记录的数据集，并比较了不同GAN模型在生成合成数据方面的质量，实现了在不涉及隐私问题的情况下保护用户数据并延伸数据应用。 |
| [^144] | [E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series](https://arxiv.org/abs/2402.14041) | E2USD提出了一种有效的无监督多元时间序列状态检测方法，利用了快速傅里叶变换和双视图嵌入模块进行编码，以及通过对抗学习方法消除假阴性，从而实现了SOTA准确性并显著降低了计算开销。 |
| [^145] | [Specialty detection in the context of telemedicine in a highly imbalanced multi-class distribution](https://arxiv.org/abs/2402.14039) | 提出基于机器学习模型的专业检测分类器，用于自动化检测每个问题的正确专业并将其路由到正确的医生，重点是处理阿拉伯医疗问题的多类别和高度不平衡数据集。 |
| [^146] | [Wisdom of Committee: Distilling from Foundation Model to SpecializedApplication Model](https://arxiv.org/abs/2402.14035) | 将基础模型的知识转移到专用应用模型中存在挑战，提出了通过创建教学委员会来应对这些挑战。 |
| [^147] | [VN Network: Embedding Newly Emerging Entities with Virtual Neighbors](https://arxiv.org/abs/2402.14033) | 提出了一个名为虚拟邻居（VN）网络的新框架，以解决实体嵌入中的邻居稀疏问题，并有效整合远距离信息。 |
| [^148] | [Autoencoder with Ordered Variance for Nonlinear Model Identification](https://arxiv.org/abs/2402.14031) | 提出了一种具有有序方差的自编码器，通过添加方差正则化项来保持潜空间的顺序，并且在无监督设置中展示了其在提取非线性关系方面的有效性。 |
| [^149] | [Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket](https://arxiv.org/abs/2402.14029) | 提出一种方法，通过冻结随机子集的初始权重来减少强大的彩票票证（SLT）搜索空间，从而独立于所需SLT稀疏性降低了SLT搜索空间，保证了SLT在这种减少搜索空间中的存在。 |
| [^150] | [Learning causation event conjunction sequences](https://arxiv.org/abs/2402.14027) | 这项研究探讨了学习事件序列中因果关系的方法，发现注意力循环ANN表现最佳，直方图算法明显优于其他ANNs。 |
| [^151] | [Statistical validation of a deep learning algorithm for dental anomaly detection in intraoral radiographs using paired data](https://arxiv.org/abs/2402.14022) | 该研究通过统计分析验证了一种深度学习算法在口内X光中检测牙齿异常的有效性，平均敏感性显著提高，而平均特异性略有下降 |
| [^152] | [Revisiting Convergence of AdaGrad with Relaxed Assumptions](https://arxiv.org/abs/2402.13794) | 重新审视了AdaGrad在非凸光滑优化问题上的收敛性，提出了通用噪声模型，得出了概率收敛速度，无需先验知识，且可以在噪声参数足够小时加速至更快的速度。 |
| [^153] | [Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions](https://arxiv.org/abs/2402.13777) | 深度生成模型在离线策略学习中展现了巨大潜力，本文提供了首个系统性综述，涵盖了五种主流深度生成模型及其应用。 |
| [^154] | [DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning](https://arxiv.org/abs/2402.13711) | DSLR提出了一种基于覆盖范围的多样性方法，以解决基于重播的图持续学习中回放节点过于集中导致过拟合和灾难性遗忘的问题。 |
| [^155] | [Robustness of Deep Neural Networks for Micro-Doppler Radar Classification](https://arxiv.org/abs/2402.13651) | 评估了两种深度卷积架构的鲁棒性，发现在训练中加入对抗样本和时间增强样本可以改善模型的泛化能力 |
| [^156] | [Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response](https://arxiv.org/abs/2402.13528) | 本文开发了一种基础设施调解员系统，用于自动检测特定基础设施问题，通过挖掘社交网络中关于预期失败的担忧，有助于预防和减轻潜在的基础设施失败。 |
| [^157] | [Transformer tricks: Precomputing the first layer](https://arxiv.org/abs/2402.13388) | 该论文描述了一种加速具有RoPE的transformer推断的技巧，通过预计算第一层来降低延迟和成本，最大节省取决于总层数。 |
| [^158] | [KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers](https://arxiv.org/abs/2402.13352) | 该研究利用Transformer机器学习架构生成“看起来真实”的量子电路，以增强现有的量子电路数据集。 |
| [^159] | [Learning under Singularity: An Information Criterion improving WBIC and sBIC](https://arxiv.org/abs/2402.12762) | LS信息准则旨在增强WBIC和sBIC的功能，有效处理非正则情况，具有稳定性，为奇异情况下的信息准则提供了新的方法 |
| [^160] | [Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data](https://arxiv.org/abs/2402.12424) | 本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。 |
| [^161] | [Dynamic Multi-Network Mining of Tensor Time Series](https://arxiv.org/abs/2402.11773) | 提出了一种新方法，Dynamic Multi-network Mining (DMM)，能够将张量时间序列转换为不同长度的段组，通过稀疏依赖网络提供聚类的可解释性和精确性。 |
| [^162] | [Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution](https://arxiv.org/abs/2402.11525) | 提出了一种利用强化学习和人类反馈（RLHF）来改进翻译质量的成本效益偏好学习策略，该策略通过优化奖励模型来区分人类和机器翻译，从而指导改进机器翻译。 |
| [^163] | [Accelerating Semi-Asynchronous Federated Learning](https://arxiv.org/abs/2402.10991) | 提出了一种考虑贡献的异步联邦学习方法，动态调整接收到的更新的处理方式，以解决现实情况下同步上传数据可能出现的缓慢和不可靠问题。 |
| [^164] | [BlackJAX: Composable Bayesian inference in JAX](https://arxiv.org/abs/2402.10797) | BlackJAX是一个实现在JAX中组合式贝叶斯推断的库，采用函数式方法提高易用性、速度和模块化，适用于需要尖端方法、研究人员和想要了解工作原理的人。 |
| [^165] | [Brant-2: Foundation Model for Brain Signals](https://arxiv.org/abs/2402.10251) | Brant-2是脑信号领域最大的基础模型，相比于Brant，它不仅对数据变化和建模尺度具有稳健性，还能适用于更广泛范围的脑神经数据。 |
| [^166] | [Persuading a Learning Agent](https://arxiv.org/abs/2402.09721) | 在一个重复的贝叶斯说服问题中，即使没有承诺能力，委托人可以通过使用上下文无遗憾学习算法来实现与经典无学习模型中具有承诺的委托人的最优效用无限接近的效果；在代理人使用上下文无交换遗憾学习算法的情况下，委托人无法获得比具有承诺的无学习模型中的最优效用更高的效用。 |
| [^167] | [Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey](https://arxiv.org/abs/2402.05391) | 知识图谱与多模态学习的综述介绍了KG4MM和MM4KG两个主要方面，包括任务定义、构建进展、评估基准以及关键研究轨迹。 |
| [^168] | [RAG-Fusion: a New Take on Retrieval-Augmented Generation](https://arxiv.org/abs/2402.03367) | RAG-Fusion方法通过生成多个查询，并结合互惠排名融合技术，能够从不同角度上下文化原始查询，提供准确和全面的信息。这项研究在人工智能和自然语言处理应用方面有重要进展，并展示了全球和区域之间的转变。 |
| [^169] | [PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?](https://arxiv.org/abs/2402.02611) | 本研究通过PuzzleBench数据集探索了LLMs解决困难的一阶组合推理问题的能力，并提出了Puzzle-LM方法，该方法将LLMs与符号求解器和程序解释器相结合，使其能够有效地推理这类问题。 |
| [^170] | [DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching](https://arxiv.org/abs/2402.02439) | DiffStitch是一种使用基于扩散的轨迹拼接提升离线强化学习的方法。它通过有效地连接低奖励轨迹和高奖励轨迹，形成全局最优轨迹，以提高离线强化学习算法的性能。 |
| [^171] | [Measuring Moral Inconsistencies in Large Language Models](https://arxiv.org/abs/2402.01719) | 本研究提出了一种新的信息论度量方法，称为语义图熵（SGE），用于测量道德情景中大型语言模型（LLM）的一致性。与现有的一致性度量方法相比，SGE在五个LLMs上与人类判断更好地相关，为研究LLM不一致性的根本原因提供了新的思路。 |
| [^172] | [HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text Classification](https://arxiv.org/abs/2402.01696) | HiGen提出了一个基于文本生成的框架，利用语言模型来编码动态文本表示，在层次文本分类中考虑了文档各个部分的相关性，并引入了一个层级引导的损失函数。此外，还提供了一个新颖的用于HTC的数据集ENZYME。 |
| [^173] | [Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination](https://arxiv.org/abs/2401.08025) | 本文提出了Self-Imagine方法，通过利用一种Vision-Language模型生成问题的结构化表示并将其渲染为图像，再使用相同的模型回答问题，从而在数学任务和通用推理任务中提高了模型性能。 |
| [^174] | [A framework for conditional diffusion modelling with applications in motif scaffolding for protein design](https://arxiv.org/abs/2312.09236) | 该论文提出一种统一的条件扩散建模框架，基于Doob's h-transform，用于解决蛋白设计中的基序支架问题 |
| [^175] | [Vision-Language Models as a Source of Rewards](https://arxiv.org/abs/2312.09187) | 使用现成的视觉-语言模型作为强化学习代理的奖励来源，展示了如何通过CLIP系列模型派生视觉目标实现的奖励，从而训练出能够实现多种语言目标的RL代理。 |
| [^176] | [MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training](https://arxiv.org/abs/2312.08656) | MaxK-GNN是一种先进的高性能GPU训练系统，通过MaxK非线性和理论分析，实现了图神经网络训练的垂直优化。 |
| [^177] | [SparQ Attention: Bandwidth-Efficient LLM Inference](https://arxiv.org/abs/2312.04985) | SparQ Attention通过减少注意力块内存带宽需求的技术，从而增加LLMs推理的吞吐量，同时保持模型准确性。 |
| [^178] | [Variational Self-Supervised Contrastive Learning Using Beta Divergence For Face Understanding](https://arxiv.org/abs/2312.00824) | 提出一种使用Beta Divergence进行变分自监督对比学习的方法，能够从未标记和嘈杂数据集中学习，在人脸理解领域取得显著的准确率提升。 |
| [^179] | [Pre- to Post-Contrast Breast MRI Synthesis for Enhanced Tumour Segmentation](https://arxiv.org/abs/2311.10879) | 通过生成对抗网络（GAN）在乳腺MRI中合成对比增强，引入了Scaled Aggregate Measure (SAMe)进行量化评估，并成功应用于乳腺肿瘤分割任务。 |
| [^180] | [EduGym: An Environment and Notebook Suite for Reinforcement Learning Education](https://arxiv.org/abs/2311.10590) | EduGym是一套用于强化学习教育的环境和笔记本套件，旨在解决学生在转换理论和实践中遇到的困难。 |
| [^181] | [Analytical Verification of Deep Neural Network Performance for Time-Synchronized Distribution System State Estimation](https://arxiv.org/abs/2311.06973) | 本论文研究了使用深度神经网络进行配电系统同步时间状态估计的性能和鲁棒性。通过将输入扰动视为混合整数线性规划问题进行分析验证，并强调了批归一化在提高问题可扩展性方面的作用。该框架在修改后的IEEE 34节点系统和真实的大型分布系统上进行验证。 |
| [^182] | [Navigating Scaling Laws: Compute Optimality in Adaptive Model Training](https://arxiv.org/abs/2311.03233) | 本研究提出了一种新颖的自适应模型训练方法，通过允许模型在训练过程中调整形状，能够优化地使用计算资源，实现在更少的计算量下达到目标性能。 |
| [^183] | [Breaking the Trilemma of Privacy, Utility, Efficiency via Controllable Machine Unlearning](https://arxiv.org/abs/2310.18574) | 设计了一种名为Controllable Machine Unlearning (ConMU)的新框架，旨在平衡隐私、模型效用和运行效率之间的权衡。 |
| [^184] | [Adaptive conformal classification with noisy labels](https://arxiv.org/abs/2309.05092) | 该论文提出了一种新颖的自适应符合分类方法，能够自动适应随机标签污染，产生更具信息量和更坚实覆盖保证的预测集合。 |
| [^185] | [Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold](https://arxiv.org/abs/2308.10547) | 该论文提出了一种在Stiefel流形上进行分布式优化的黎曼共轭梯度下降方法，克服了全局函数非凸的限制。 |
| [^186] | [DP-SGD Without Clipping: The Lipschitz Neural Network Way](https://arxiv.org/abs/2305.16202) | 提出一种无需剪切的DP-SGD训练方法，依赖于利普希茨约束网络提供灵敏度界限，规避了剪切过程的缺点，并证明了可以通过限制每一层相对于其参数的利普希茨常数来训练这些具有隐私保证的网络。 |
| [^187] | [LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities](https://arxiv.org/abs/2305.13168) | 本研究全面评估了LLMs在知识图谱构建和推理领域的性能，发现GPT-4更适合作为推理助手，并在某些情况下超越了精调模型。 |
| [^188] | [Mobiprox: Supporting Dynamic Approximate Computing on Mobiles](https://arxiv.org/abs/2303.11291) | Mobiprox提供了一个框架，支持移动设备上的动态近似计算，并且实现了对单个网络层的运行时可调近似。 |
| [^189] | [Deep hybrid model with satellite imagery: how to combine demand modeling and computer vision for behavior analysis?](https://arxiv.org/abs/2303.04204) | 该研究提出了一个深度混合模型框架，通过将数值数据和城市影像整合到一个潜在空间中，成功应用于分析出行方式选择，并在预测聚合和细分出行行为方面优于传统需求模型和深度学习方法。 |
| [^190] | [Uncertainty Quantification of Spatiotemporal Travel Demand with Probabilistic Graph Neural Networks](https://arxiv.org/abs/2303.04040) | 该研究提出了一种概率图神经网络（Prob-GNN）框架，用于量化出行需求的时空不确定性，实证应用表明概率假设对不确定性预测影响大于确定性假设。 |
| [^191] | [Persuading a Behavioral Agent: Approximately Best Responding and Learning](https://arxiv.org/abs/2302.03719) | 发送者可以找到一个信号方案，几乎可以保证获得与经典模型中最佳效用几乎相等的预期效用，而接收者的近似最佳响应行为并不会对发送者在贝叶斯说服问题中的最大可达效用产生很大影响。 |
| [^192] | [Promises and Pitfalls of Threshold-based Auto-labeling](https://arxiv.org/abs/2211.12620) | TBAL系统可以通过验证数据自动标注未标注数据，减少手动标注的依赖；研究结果展示了即使模型表现不佳也可以准确自动标记数据，并揭示了TBAL系统的潜在缺陷 |
| [^193] | [Algebraic Machine Learning with an Application to Chemistry](https://arxiv.org/abs/2205.05795) | 本文开发了一种机器学习流程，能够捕获细粒度的几何信息，而无需依赖光滑性假设。 |
| [^194] | [A Survey on Fairness for Machine Learning on Graphs](https://arxiv.org/abs/2205.05396) | 该调查是第一个专门关于关系数据公平性的调查，致力于解决图上机器学习中的公平性问题。 |
| [^195] | [Physics-informed deep-learning applications to experimental fluid mechanics](https://arxiv.org/abs/2203.15402) | 该研究利用物理信息神经网络（PINNs）从有限的嘈杂测量数据中实现流场数据的超分辨，无需高分辨率参考数据，旨在获得连续的时空解。 |
| [^196] | [Graph Neural Networks for Graphs with Heterophily: A Survey](https://arxiv.org/abs/2202.07082) | 该论文提出了对具有异质性的图进行图神经网络研究的系统回顾，并提出了系统性分类法以指导现有异质性GNN模型。 |
| [^197] | [Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound](https://arxiv.org/abs/2202.05560) | 该研究提出了一种PAC-Bayes界限，能够同时控制多个错误，并提供丰富的信息，适用于回归中测试损失分布或分类中不同错误分类的概率。 |
| [^198] | [FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control](https://arxiv.org/abs/2201.10936) | 提出了一种自监督的描述-序列任务，实现了在全局水平上对生成音乐的细粒度可控，通过结合高级特征和领域知识，在符号音乐生成方面取得了最新的成果 |
| [^199] | [Data structure > labels? Unsupervised heuristics for SVM hyperparameter estimation](https://arxiv.org/abs/2111.02164) | 提出了一种对支持向量机超参数进行改进的无监督启发式方法，以解决在缺乏标记示例的情况下，减少对类标签信息依赖的问题。 |
| [^200] | [StreaMulT: Streaming Multimodal Transformer for Heterogeneous and Arbitrary Long Sequential Data](https://arxiv.org/abs/2110.08021) | 提出一种新的流式多模态Transformer模型，用于处理异构和任意长的序列数据，解决了在预测性维护任务中多源数据流的融合和跨时间预测的挑战。 |
| [^201] | [Adversarial Machine Learning: Bayesian Perspectives](https://arxiv.org/abs/2003.03546) | AML旨在保护机器学习系统免受安全威胁，贝叶斯视角为防御提供了新的好处 |
| [^202] | [Finetuning Large Language Models for Vulnerability Detection.](http://arxiv.org/abs/2401.17010) | 本文优化了大规模语言模型用于源代码中的漏洞检测任务，通过微调最先进的代码语言模型WizardCoder并改进其训练过程和策略，实现了对漏洞数据集的分类性能的提升。 |
| [^203] | [Quantum-Inspired Machine Learning for Molecular Docking.](http://arxiv.org/abs/2401.12999) | 量子启发的机器学习方法在分子对接中取得了显著的改进，通过结合量子特性和深度学习在编码的分子空间中学习的梯度，提高了盲目对接的成功率。 |
| [^204] | [Attention, Distillation, and Tabularization: Towards Practical Neural Network-Based Prefetching.](http://arxiv.org/abs/2401.06362) | 我们提出了一种基于表格化的方法，通过将注意力模型转换为层次结构的表格查找，显著降低了预取模型的复杂度和推理延迟，同时保持了高准确性。通过我们的方法，我们开发了一个DART预取模型，在减少计算量的情况下只有轻微的性能下降。 |
| [^205] | [Decoupling Decision-Making in Fraud Prevention through Classifier Calibration for Business Logic Action.](http://arxiv.org/abs/2401.05240) | 该论文研究了通过分类器校准来实现反欺诈预防中的决策解耦。通过使用校准策略，他们发现等距和贝塔校准方法在训练和测试数据之间发生变化的场景下表现突出。这些结果为优化解耦努力的从业者提供了宝贵的见解。 |
| [^206] | [Privacy-Preserving Neural Graph Databases.](http://arxiv.org/abs/2312.15591) | 隐私保护的神经图数据库结合了图数据库和神经网络的优势，能够高效存储、检索和分析图结构数据。然而，这种能力也带来了潜在的隐私风险。 |
| [^207] | [Locating Cross-Task Sequence Continuation Circuits in Transformers.](http://arxiv.org/abs/2311.04131) | 通过分析和比较Transformer模型中类似的序列继续任务的电路，研究发现共享的计算结构可以提高模型的行为预测能力、错误识别能力和编辑过程的安全性。 |
| [^208] | [On Feynman--Kac training of partial Bayesian neural networks.](http://arxiv.org/abs/2310.19608) | 本文提出了一种将部分贝叶斯神经网络训练转化为模拟费曼-卡克模型的高效采样训练策略，并通过各种数据集的实验证明其在预测性能方面优于现有技术。 |
| [^209] | [Flow-based Distributionally Robust Optimization.](http://arxiv.org/abs/2310.19253) | 这项研究提出了一种称为FlowDRO的计算高效框架，用于解决基于流的分布鲁棒优化问题，通过使用流模型和Wasserstein近端梯度流类型的算法，实现了对具有更大样本大小的问题的可扩展性和更好的泛化能力。 |
| [^210] | [Heterogeneous Federated Learning with Group-Aware Prompt Tuning.](http://arxiv.org/abs/2310.18285) | 本文研究了在异构联邦学习中利用预训练的Transformer和高效的提示调整策略，通过学习共享和群体提示实现获取通用知识和个性化知识，以训练适应不同本地数据分布的全局模型。 |
| [^211] | [BLP 2023 Task 2: Sentiment Analysis.](http://arxiv.org/abs/2310.16183) | BLP 2023任务2是关于情感分析的共享任务，吸引了71个参与者。参与者通过各种方法，包括经典机器学习模型和大型语言模型，提交了597个运行结果。本文提供了任务的详细设置和参与者提交系统的概述。 |
| [^212] | [Externally Valid Policy Evaluation Combining Trial and Observational Data.](http://arxiv.org/abs/2310.14763) | 这项研究提出了一种结合试验和观察数据的外部有效策略评估方法，利用试验数据对目标人群上的政策结果进行有效推断，并给出了可验证的评估结果。 |
| [^213] | [When are Bandits Robust to Misspecification?.](http://arxiv.org/abs/2310.09358) | 该论文研究了参数化的强盗算法和情境化的强盗算法在真实奖励与模型之间存在误差的情况下的稳定性，并找到了依赖于问题实例和模型类的充分条件，使得经典算法如ε-贪心和LinUCB能够在时间范围内保持次线性的遗憾保障。 |
| [^214] | [Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution.](http://arxiv.org/abs/2310.03032) | 本文提出了一种新颖的结构感知嵌入演化(SEvo)机制，能够以较低的计算开销将图结构信息注入到嵌入中，从而在现代推荐系统中实现更高效的性能。 |
| [^215] | [Improving Adaptive Online Learning Using Refined Discretization.](http://arxiv.org/abs/2309.16044) | 通过一种新颖的连续时间启发式算法，提高了自适应在线学习的效果，将梯度方差的依赖性从次优的$O(\sqrt{V_T\log V_T})$改进到最优速率$O(\sqrt{V_T})$，并可适用于未知Lipschitz常数的情况。 |
| [^216] | [ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs.](http://arxiv.org/abs/2309.13007) | ReConcile是一个通过多轮讨论和投票机制来增强LLM推理能力的多模型多代理框架。 |
| [^217] | [Transformers as Support Vector Machines.](http://arxiv.org/abs/2308.16898) | 这项工作建立了自注意力和硬间隔支持向量机问题之间的正式等价关系，通过转换器架构的优化几何来解决自然语言处理问题，同时揭示了梯度下降优化的转换器的隐式偏差。 |
| [^218] | [Time Travel in LLMs: Tracing Data Contamination in Large Language Models.](http://arxiv.org/abs/2308.08493) | 该论文提出了一种用于识别大型语言模型（LLMs）中数据污染的简单而有效的方法。通过对随机样本中的单个实例进行分析，以及使用“引导指令”来评估整个数据集分区的污染程度，可以准确地识别污染的实例和分区。 |
| [^219] | [Towards true discovery of the differential equations.](http://arxiv.org/abs/2308.04901) | 本文研究了独立方程发现的先决条件和工具，并解决了在正确方程未知的情况下评估发现方程的挑战，旨在为在没有先验方程知识的情况下可靠地发现方程提供洞察力。 |
| [^220] | [Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?.](http://arxiv.org/abs/2307.14642) | 本文证明了带有控制变量的黑盒变分推断在完美变分族规范下以几何速度收敛，为BBVI提供了收敛性保证，同时提出了对熵梯度估计器的改进，对比了STL估计器，并给出了明确的非渐近复杂度保证。 |
| [^221] | [Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media.](http://arxiv.org/abs/2307.09312) | 多模态讨论变换器 (mDT) 是一个用于检测在线社交网络中仇恨言论的新颖模型。与传统的仅使用文本的方法不同，mDT通过整体分析文本和图像，结合图变换器捕捉评论周围整个讨论的上下文关系，并通过交织融合层将文本和图像嵌入进行组合。研究发现，捕捉对话的整体视图可以极大地提高检测反社会行为的准确性。 |
| [^222] | [Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization.](http://arxiv.org/abs/2306.09222) | 我们通过分布健壮优化和重要性加权的梯度下降技术提升了深度神经网络的性能，并在各种任务上取得了优越的结果。 |
| [^223] | [Attention-stacked Generative Adversarial Network (AS-GAN)-empowered Sensor Data Augmentation for Online Monitoring of Manufacturing System.](http://arxiv.org/abs/2306.06268) | 本文提出了一种名为AS-GAN的技术，使用数据增强来解决监督式机器学习中的数据不平衡问题，其中AS-GAN有效地学习异常状态数据的基础分布，并生成高质量的数据样本用于在线制造系统监测。 |
| [^224] | [Single-Model Attribution via Final-Layer Inversion.](http://arxiv.org/abs/2306.06210) | 本文提出了一种利用最终层反演和异常检测的开放式单模型归因方法，解决了以往方法要么局限于封闭式环境、要么需要对生成模型进行不必要的改变的问题。实验结果表明该方法优于现有方法。 |
| [^225] | [Domain-Agnostic Batch Bayesian Optimization with Diverse Constraints via Bayesian Quadrature.](http://arxiv.org/abs/2306.05843) | 本论文提出了cSOBER，一种处理多样化约束条件、离散和混合空间、未知约束以及查询拒绝问题的领域无关型贝叶斯优化算法。 |
| [^226] | [MC-NN: An End-to-End Multi-Channel Neural Network Approach for Predicting Influenza A Virus Hosts and Antigenic Types.](http://arxiv.org/abs/2306.05587) | 提出了一种利用多通道神经网络模型预测流感A病毒宿主和抗原亚型的方法，数据显示多通道神经网络模型具有较高的准确性和预测能力。 |
| [^227] | [State Regularized Policy Optimization on Data with Dynamics Shift.](http://arxiv.org/abs/2306.03552) | 本文提出了一种叫做 SRPO (状态规范化策略优化) 的算法，该算法利用训练数据中的稳态分布来规范新环境中的策略，在处理具有不同动态的多个环境时表现优异。 |
| [^228] | [V2Meow: Meowing to the Visual Beat via Music Generation.](http://arxiv.org/abs/2305.06594) | V2Meow是一种新方法，通过与O(100K)音频片段配对的视频帧进行训练，生成与各种类型的视频输入的视觉语义相匹配的高质量音频，无需符号音乐数据。 |
| [^229] | [Dynamic Sparse Training with Structured Sparsity.](http://arxiv.org/abs/2305.02299) | 本文提出了一种结构化稀疏动态训练（DST）方法，学习一种变体的结构化 N:M 稀疏性，其加速在一般情况下通常被支持，可缩减参数和内存占用，同时相较于密集模型，具有减少推理时间的优势。 |
| [^230] | [Architectures of Topological Deep Learning: A Survey on Topological Neural Networks.](http://arxiv.org/abs/2304.10031) | 拓扑深度学习框架提供了从复杂系统相关数据中提取知识的全面架构，通过解决现有工作的符号和术语不一致问题，有望在应用科学和其他领域开拓新局面。 |
| [^231] | [Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models.](http://arxiv.org/abs/2303.16133) | 该研究提出了一个基准数据集COCOCON，并提出度量方法来衡量模型一致性，研究发现现有的最先进系统在不同任务之间表现出高度不一致性。 |
| [^232] | [Generative Invertible Quantum Neural Networks.](http://arxiv.org/abs/2302.12906) | 本论文提出了一种用于生成可逆量子神经网络的算法，并将其应用于LHC数据的处理，结果表明该算法可以在学习和生成复杂数据方面与经典算法的表现相匹配。 |
| [^233] | [Imprecise Bayesian Neural Networks.](http://arxiv.org/abs/2302.09656) | 在机器学习和人工智能领域，该论文提出了一种新的算法——不精确的贝叶斯神经网络(IBNNs)。这种算法使用可信区间先验分布集合和似然分布集合进行训练，相比标准的BNNs，可以区分先验和后验的不确定性并量化。此外，IBNNs在贝叶斯灵敏度分析方面具有更强的鲁棒性，并且对分布变化也更加鲁棒。 |

# 详细

[^1]: 摄像头作为射线: 通过射线扩散进行姿势估计

    Cameras as Rays: Pose Estimation via Ray Diffusion

    [https://arxiv.org/abs/2402.14817](https://arxiv.org/abs/2402.14817)

    提出了一种将相机姿势视为射线束的分布表示方法，结合空间图像特征，开发了基于回归和扩散的姿势估计方法，在CO3D数据集上取得了最先进的性能。

    

    估计相机姿势是3D重建的基本任务，鉴于视图稀疏（<10），该任务仍具有挑战性。与现有方法不同，后者追求相机外参的全局参数化的自上而下预测，我们提出了一种将相机姿势视为射线束的分布表示。这种表示允许与空间图像特征紧密耦合，提高了姿势精度。我们观察到，这种表示自然适用于集合级别的Transformer，并开发了一种基于回归的方法，将图像块映射到相应的射线上。为了捕捉稀疏视角姿势推断中固有的不确定性，我们调整了这种方法，学习了一个去噪扩散模型，使我们能够采样合理的模式，同时提高性能。我们提出的方法，既是基于回归，也是基于扩散的，在CO3D相机姿势估计方面展现出了最先进的性能。

    arXiv:2402.14817v1 Announce Type: cross  Abstract: Estimating camera poses is a fundamental task for 3D reconstruction and remains challenging given sparse views (<10). In contrast to existing approaches that pursue top-down prediction of global parametrizations of camera extrinsics, we propose a distributed representation of camera pose that treats a camera as a bundle of rays. This representation allows for a tight coupling with spatial image features improving pose precision. We observe that this representation is naturally suited for set-level level transformers and develop a regression-based approach that maps image patches to corresponding rays. To capture the inherent uncertainties in sparse-view pose inference, we adapt this approach to learn a denoising diffusion model which allows us to sample plausible modes while improving performance. Our proposed methods, both regression- and diffusion-based, demonstrate state-of-the-art performance on camera pose estimation on CO3D while
    
[^2]: 医学影像中专家级视觉语言基础模型的人口统计偏见

    Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging

    [https://arxiv.org/abs/2402.14815](https://arxiv.org/abs/2402.14815)

    本研究调查了全球五个数据集中最先进的视觉语言基础模型在胸片诊断中的算法公平性。我们的发现表明，与董事会认证的放射科医师相比，这些基础模型在诊断边缘化群体时一贯存在低诊断率，甚至在诸如黑人女性之类的交叉亚组中看到更高的比例。

    

    人工智能的进展已经在医学影像应用中实现了专家级表现。值得注意的是，自监督视觉语言基础模型可以在不依赖明确培训注释的情况下检测广泛的病理。然而，确保这些人工智能模型不反映或放大人类偏见至关重要，从而使女性或黑人患者等历史上被边缘化的群体处于不利地位。这种偏见的体现可能会系统性地延迟特定患者亚组的重要医疗护理。

    arXiv:2402.14815v1 Announce Type: cross  Abstract: Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications. Notably, self-supervised vision-language foundation models can detect a broad spectrum of pathologies without relying on explicit training annotations. However, it is crucial to ensure that these AI models do not mirror or amplify human biases, thereby disadvantaging historically marginalized groups such as females or Black patients. The manifestation of such biases could systematically delay essential medical care for certain patient subgroups. In this study, we investigate the algorithmic fairness of state-of-the-art vision-language foundation models in chest X-ray diagnosis across five globally-sourced datasets. Our findings reveal that compared to board-certified radiologists, these foundation models consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups, such as Black fem
    
[^3]: 微调增强现有机制：实体跟踪案例研究

    Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking

    [https://arxiv.org/abs/2402.14811](https://arxiv.org/abs/2402.14811)

    通过对语言模型进行微调，我们研究了如何影响实体跟踪等内部机制，并发现微调能够在数学任务上实现明显的性能提升。

    

    细化在诸如遵循指令、生成代码和数学等广义任务上已经显示出增强语言模型在一系列任务上的性能。然而，关于这种微调如何影响这些模型中内部计算的解释仍然难以捉摸。我们研究了微调如何影响语言模型中实现的内部机制。作为一个案例研究，我们探讨了实体跟踪的特性，这是语言理解的一个重要方面，在数学上进行了微调的模型在性能上有显著提升。我们识别出了实现实体跟踪的机制，并显示出（i）原始模型和其精细调整版本主要实现实体跟踪的是相同的电路。事实上，原始模型的实体跟踪电路在经过微调的版本上的性能优于完整的原始模型。（ii）所有模型的电路实现大致相同的功能

    arXiv:2402.14811v1 Announce Type: new  Abstract: Fine-tuning on generalized tasks such as instruction following, code generation, and mathematics has been shown to enhance language models' performance on a range of tasks. Nevertheless, explanations of how such fine-tuning influences the internal computations in these models remain elusive. We study how fine-tuning affects the internal mechanisms implemented in language models. As a case study, we explore the property of entity tracking, a crucial facet of language comprehension, where models fine-tuned on mathematics have substantial performance gains. We identify the mechanism that enables entity tracking and show that (i) in both the original model and its fine-tuned versions primarily the same circuit implements entity tracking. In fact, the entity tracking circuit of the original model on the fine-tuned versions performs better than the full original model. (ii) The circuits of all the models implement roughly the same functionalit
    
[^4]: GeneOH扩散: 通过去噪扩散实现可泛化的手-物体交互去噪

    GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion

    [https://arxiv.org/abs/2402.14810](https://arxiv.org/abs/2402.14810)

    通过GeneOH扩散方法，实现了可泛化的手-物体交互去噪，其中关键创新包括基于接触的HOI表示和领域通用的去噪方案。

    

    在这项工作中，我们解决了去噪手-物体交互（HOI）的挑战性问题。在给定一个错误的交互序列的情况下，目标是对不正确的手的轨迹进行细化，以消除交互伪影，获得一个感知上真实的序列。这一挑战涉及复杂的交互噪声，包括不自然的手部姿势和不正确的手-物体关系，以及对新交互和不同噪声模式的稳健泛化的必要性。我们通过一种新颖的方法GeneOH Diffusion应对这些挑战，包括两个关键设计:一种名为GeneOH的创新的基于接触的HOI表示和一种新的领域通用的去噪方案。基于接触的表示GeneOH对HOI过程进行信息化参数化，促进在各种HOI场景中实现增强的泛化。新的去噪方案包括一个经过训练用于投影嘈杂数据样本的经典去噪模型。

    arXiv:2402.14810v1 Announce Type: cross  Abstract: In this work, we tackle the challenging problem of denoising hand-object interactions (HOI). Given an erroneous interaction sequence, the objective is to refine the incorrect hand trajectory to remove interaction artifacts for a perceptually realistic sequence. This challenge involves intricate interaction noise, including unnatural hand poses and incorrect hand-object relations, alongside the necessity for robust generalization to new interactions and diverse noise patterns. We tackle those challenges through a novel approach, GeneOH Diffusion, incorporating two key designs: an innovative contact-centric HOI representation named GeneOH and a new domain-generalizable denoising scheme. The contact-centric representation GeneOH informatively parameterizes the HOI process, facilitating enhanced generalization across various HOI scenarios. The new denoising scheme consists of a canonical denoising model trained to project noisy data sample
    
[^5]: CriticBench：为批判性-正确推理评估LLMs而设计的基准测试

    CriticBench: Benchmarking LLMs for Critique-Correct Reasoning

    [https://arxiv.org/abs/2402.14809](https://arxiv.org/abs/2402.14809)

    CriticBench是一个综合基准测试，旨在评估LLMs在批判和纠正推理方面的能力，发现批判性训练显著提升性能，逻辑任务更易于修正。

    

    大型语言模型（LLMs）批判和完善其推理的能力对于它们在评估、反馈提供和自我改进中的应用至关重要。本文引入了CriticBench，一个旨在评估LLMs在各种任务中批判和纠正其推理能力的综合基准测试。CriticBench包含五个推理领域：数学、常识、符号、编码和算法。它整合了15个数据集，并结合了三个LLM系列的响应。利用CriticBench，我们评估和剖析了17个LLMs在生成、批判和修正推理（即GQC推理）中的表现。我们的研究结果显示：（1）GQC能力呈线性关系，批判性训练显著提升了性能；（2）修正效果在任务上有所不同，以逻辑为导向的任务更容易修正；（3）GQC知识的不一致性。

    arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
    
[^6]: 用于公共卫生中动态不安静多臂老虎机任务的决策语言模型（DLM）

    A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health

    [https://arxiv.org/abs/2402.14807](https://arxiv.org/abs/2402.14807)

    提出了一种决策语言模型DLM，旨在通过使用LLMs作为自动规划器，动态微调RMAB策略，以应对公共卫生中具有挑战性的情境。

    

    旨在降低孕产妇死亡率的努力在很大程度上依赖于预防保健计划，向高风险人群传播重要的健康信息。本文提出了DLM：一种用于RMAB的决策语言模型，旨在通过使用LLMs作为自动规划器，动态微调RMAB策略，以应对公共卫生中具有挑战性的情境。

    arXiv:2402.14807v1 Announce Type: cross  Abstract: Efforts to reduce maternal mortality rate, a key UN Sustainable Development target (SDG Target 3.1), rely largely on preventative care programs to spread critical health information to high-risk populations. These programs face two important challenges: efficiently allocating limited health resources to large beneficiary populations, and adapting to evolving policy priorities. While prior works in restless multi-armed bandit (RMAB) demonstrated success in public health allocation tasks, they lack flexibility to adapt to evolving policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept, automated planners in various domains, including robotic control and navigation. In this paper, we propose DLM: a Decision Language Model for RMABs. To enable dynamic fine-tuning of RMAB policies for challenging public health settings using human-language commands, we propose using LLMs as automated planners to (1) interpret hu
    
[^7]: 空气质量预测传输仿真的差异学习

    Difference Learning for Air Quality Forecasting Transport Emulation

    [https://arxiv.org/abs/2402.14806](https://arxiv.org/abs/2402.14806)

    本研究提出了一种深度学习传输仿真器，能够在减少计算量的同时保持与现有数值模型相当的技能。

    

    人类健康受到恶劣空气质量的负面影响，包括增加呼吸道和心血管疾病的风险。由于最近极端空气质量事件的增加，全球范围和美国本地都需要更精细的空气质量预测指导以有效适应这些事件。本文描述了一个能够减少计算量的深度学习传输仿真器，同时保持与现有数值模型相当的技能。

    arXiv:2402.14806v1 Announce Type: new  Abstract: Human health is negatively impacted by poor air quality including increased risk for respiratory and cardiovascular disease. Due to a recent increase in extreme air quality events, both globally and locally in the United States, finer resolution air quality forecasting guidance is needed to effectively adapt to these events. The National Oceanic and Atmospheric Administration provides air quality forecasting guidance for the Continental United States. Their air quality forecasting model is based on a 15 km spatial resolution; however, the goal is to reach a three km spatial resolution. This is currently not feasible due in part to prohibitive computational requirements for modeling the transport of chemical species. In this work, we describe a deep learning transport emulator that is able to reduce computations while maintaining skill comparable with the existing numerical model. We show how this method maintains skill in the presence of
    
[^8]: 使用MATH-Vision数据集测量多模态数学推理

    Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset

    [https://arxiv.org/abs/2402.14804](https://arxiv.org/abs/2402.14804)

    提出了MATH-Vision（MATH-V）数据集，用于评估大型多模态模型（LMMs）的数学推理能力，通过实验证实了当前LMMs和人类在MATH-V上的表现差距。

    

    大型多模态模型（LMMs）的最新进展在视觉背景下的数学推理方面显示出令人鼓舞的结果，这些模型在现有基准测试（如MathVista）上接近人类水平的表现。然而，我们观察到这些基准测试在问题多样性和涵盖学科范围方面存在显着局限性。为了解决这一问题，我们提出了MATH-Vision（MATH-V）数据集，这是一个精心策划的收集了来自真实数学竞赛的3,040个高质量数学问题和视觉背景的数据集。跨越16个不同的数学学科，分为5个难度级别进行评分，我们的数据集为评估LMMs的数学推理能力提供了一套全面且多样化的挑战。通过广泛的实验，我们揭示了当前LMMs与MATH-V上人类表现之间的显著表现差距，并强调了进一步推进的必要性。

    arXiv:2402.14804v1 Announce Type: cross  Abstract: Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual contexts, with models approaching human-level performance on existing benchmarks such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these benchmarks. To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs. Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on MATH-V, underscoring the imperative for further advancements i
    
[^9]: 在异质性下的链路预测: 受物理启发的图神经网络方法

    Link Prediction under Heterophily: A Physics-Inspired Graph Neural Network Approach

    [https://arxiv.org/abs/2402.14802](https://arxiv.org/abs/2402.14802)

    图神经网络在异质图上的链路预测面临学习能力和表达能力方面的挑战，本论文提出了受物理启发的方法以增强节点分类性能。

    

    最近几年，由于其在对图表示的真实世界现象建模方面的灵活性，图神经网络（GNNs）已成为各种深度学习领域的事实标准。然而，GNNs的消息传递机制在学习能力和表达能力方面面临挑战，这限制了在异质图上实现高性能的能力，其中相邻节点经常具有不同的标签。大多数现有解决方案主要局限于针对节点分类任务的特定基准。这种狭窄的焦点限制了链路预测在多个应用中的潜在影响，包括推荐系统。例如，在社交网络中，两个用户可能由于某种潜在原因而连接，这使得提前预测这种连接具有挑战性。受物理启发的GNNs（如GRAFF）对提高节点分类性能提供了显著的贡献。

    arXiv:2402.14802v1 Announce Type: new  Abstract: In the past years, Graph Neural Networks (GNNs) have become the `de facto' standard in various deep learning domains, thanks to their flexibility in modeling real-world phenomena represented as graphs. However, the message-passing mechanism of GNNs faces challenges in learnability and expressivity, hindering high performance on heterophilic graphs, where adjacent nodes frequently have different labels. Most existing solutions addressing these challenges are primarily confined to specific benchmarks focused on node classification tasks. This narrow focus restricts the potential impact that link prediction under heterophily could offer in several applications, including recommender systems. For example, in social networks, two users may be connected for some latent reason, making it challenging to predict such connections in advance. Physics-Inspired GNNs such as GRAFF provided a significant contribution to enhance node classification perf
    
[^10]: 并非所有专家都相等: 混合专家大型语言模型的高效专家修剪和跳过

    Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models

    [https://arxiv.org/abs/2402.14800](https://arxiv.org/abs/2402.14800)

    引入了专家级稀疏化技术，提出了专家修剪和跳过的后训练方法，以提高MoE LLMs的部署效率，同时保持模型性能。

    

    大型语言模型（LLMs）进展中的一个重要进展是混合专家（MoE）LLMs的出现。与传统的LLMs相比，MoE LLMs可以在更少的参数下实现更高的性能，但由于其巨大的参数大小，仍然很难部署它们。与先前依赖于专门设计的硬件的权重剪枝方法不同，本文主要旨在通过引入即插即用的专家级稀疏化技术来提高MoE LLMs的部署效率。具体而言，我们首次提出了针对任务不可知和任务特定的MoE LLMs专家修剪和跳过的后训练方法，旨在提高在广泛任务范围内保持模型性能的同时提高部署效率。大量实验证明，我们提出的方法可以同时减小模型大小并增加推断速度，同时保持饱和

    arXiv:2402.14800v1 Announce Type: cross  Abstract: A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining sat
    
[^11]: 合并注意力特征以进行多视图图像编辑

    Consolidating Attention Features for Multi-view Image Editing

    [https://arxiv.org/abs/2402.14792](https://arxiv.org/abs/2402.14792)

    通过维护一致的特征并强化查询的一致性，我们提出了QNeRF方法，以实现多视图图像编辑的几何一致性。

    

    大规模文本到图像模型实现了广泛的图像编辑技术，使用文本提示甚至空间控制。然而，将这些编辑方法应用于描绘单个场景的多视图图像会导致3D不一致的结果。本文关注基于空间控制的几何操作，并介绍了一种在各个视图上 consolitdate 编辑过程的方法。我们建立在两个观点基础上: (1) 在生成过程中保持一致的特征有助于实现多视图编辑的一致性，(2) 注意力层中的查询显著影响图像结构。因此，我们提出通过强化查询的一致性来提高编辑图像的几何一致性。为此，我们引入了 QNeRF，这是一个基于编辑图像的内部查询特征训练的神经辐射场。一旦训练完成，QNeRF 可以呈现3D一致的查询，然后在软件中进行注入

    arXiv:2402.14792v1 Announce Type: cross  Abstract: Large-scale text-to-image models enable a wide range of image editing techniques, using text prompts or even spatial controls. However, applying these editing methods to multi-view images depicting a single scene leads to 3D-inconsistent results. In this work, we focus on spatial control-based geometric manipulations and introduce a method to consolidate the editing process across various views. We build on two insights: (1) maintaining consistent features throughout the generative process helps attain consistency in multi-view editing, and (2) the queries in self-attention layers significantly influence the image structure. Hence, we propose to improve the geometric consistency of the edited images by enforcing the consistency of the queries. To do so, we introduce QNeRF, a neural radiance field trained on the internal query features of the edited images. Once trained, QNeRF can render 3D-consistent queries, which are then softly inje
    
[^12]: 针对领域无关自监督学习的自导蒙面自动编码器

    Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning

    [https://arxiv.org/abs/2402.14789](https://arxiv.org/abs/2402.14789)

    自导蒙面自动编码器（SMA）是一种完全领域无关的蒙面建模方法，通过学习蒙面采样而不做任何领域特定的假设，可以在各种数据模态上进行自监督学习。

    

    自监督学习在大量无标签数据中学习表示方面表现出色，并在多个数据模态上取得成功。然而，将自监督学习扩展到新的模态并不容易，因为现有方法的具体细节是针对每个领域量身定制的，比如特定领域的数据增强反映了目标任务中的不变性。 而蒙面建模作为一种领域无关的自监督学习框架很有前途，因为它不依赖于输入增强，但其蒙面采样过程仍然领域特定。我们提出了自导蒙面自动编码器（SMA），这是一种完全领域无关的蒙面建模方法。SMA使用基于注意力的模型进行训练，使用蒙面建模目标学习蒙面采样，而不做任何领域特定的假设。我们在蛋白生物学、化学性质预测和粒子物理学三个自监督学习基准上评估了SMA。

    arXiv:2402.14789v1 Announce Type: cross  Abstract: Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is promising as a domain-agnostic framework for self-supervised learning because it does not rely on input augmentations, its mask sampling procedure remains domain-specific. We present Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method. SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. We evaluate SMA on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physi
    
[^13]: Rao-Blackwellising Bayesian Causal Inference

    Rao-Blackwellising Bayesian Causal Inference

    [https://arxiv.org/abs/2402.14781](https://arxiv.org/abs/2402.14781)

    本文结合顺序化的MCMC结构学习技术和梯度图学习的最新进展，构建了一个有效的贝叶斯因果推断框架，将因果结构推断问题分解为变量拓扑顺序推断和变量父节点集合推断，同时使用高斯过程进行因果机制建模实现精确边缘化，引入了一个Rao-Blackwell化方案。

    

    贝叶斯因果推断，即推断用于下游因果推理任务中的因果模型的后验概率，构成了一个在文献中鲜有探讨的难解的计算推断问题。本文将基于顺序的MCMC结构学习技术与最近梯度图学习的进展相结合，构建了一个有效的贝叶斯因果推断框架。具体而言，我们将推断因果结构的问题分解为(i)推断变量之间的拓扑顺序以及(ii)推断每个变量的父节点集合。当限制每个变量的父节点数量时，我们可以在多项式时间内完全边缘化父节点集合。我们进一步使用高斯过程来建模未知的因果机制，从而允许其精确边缘化。这引入了一个Rao-Blackwell化方案，其中除了因果顺序之外，模型中的所有组件都被消除。

    arXiv:2402.14781v1 Announce Type: cross  Abstract: Bayesian causal inference, i.e., inferring a posterior over causal models for the use in downstream causal reasoning tasks, poses a hard computational inference problem that is little explored in literature. In this work, we combine techniques from order-based MCMC structure learning with recent advances in gradient-based graph learning into an effective Bayesian causal inference framework. Specifically, we decompose the problem of inferring the causal structure into (i) inferring a topological order over variables and (ii) inferring the parent sets for each variable. When limiting the number of parents per variable, we can exactly marginalise over the parent sets in polynomial time. We further use Gaussian processes to model the unknown causal mechanisms, which also allows their exact marginalisation. This introduces a Rao-Blackwellization scheme, where all components are eliminated from the model, except for the causal order, for whi
    
[^14]: 因果插补用于反事实结构方程模型：桥接图和潜在因子模型

    Causal Imputation for Counterfactual SCMs: Bridging Graphs and Latent Factor Models

    [https://arxiv.org/abs/2402.14777](https://arxiv.org/abs/2402.14777)

    介绍了一种新颖的基于SCM的模型类，用于因果插补任务，将结果表示为反事实，操作表示为对工具变量进行干预，环境基于初始定义。

    

    我们考虑因果插补任务，旨在预测一系列操作在各种可能环境下的结果。以预测不同药物如何影响不同细胞类型为运行示例。我们研究指标唯一的设置，其中操作和环境是具有有限可能值的分类变量。即使在这种简单设置中，由于通常只有很少可能的操作-环境对得到研究，因此存在实际挑战。模型必须对新颖的操作-环境对进行外推，这可以被构造为行由操作索引、列由环境索引、矩阵条目对应结果的一种矩阵补全形式。我们引入了一种新颖的基于SCM的模型类，其中结果表示为反事实，操作表示为对工具变量进行干预，环境基于初始定义。

    arXiv:2402.14777v1 Announce Type: cross  Abstract: We consider the task of causal imputation, where we aim to predict the outcomes of some set of actions across a wide range of possible contexts. As a running example, we consider predicting how different drugs affect cells from different cell types. We study the index-only setting, where the actions and contexts are categorical variables with a finite number of possible values. Even in this simple setting, a practical challenge arises, since often only a small subset of possible action-context pairs have been studied. Thus, models must extrapolate to novel action-context pairs, which can be framed as a form of matrix completion with rows indexed by actions, columns indexed by contexts, and matrix entries corresponding to outcomes. We introduce a novel SCM-based model class, where the outcome is expressed as a counterfactual, actions are expressed as interventions on an instrumental variable, and contexts are defined based on the initia
    
[^15]: 2D Matryoshka句子嵌入

    2D Matryoshka Sentence Embeddings

    [https://arxiv.org/abs/2402.14776](https://arxiv.org/abs/2402.14776)

    Matryoshka表示学习(MRL)以更细粒度地编码信息，以适应临时任务，同时实现了更小的嵌入大小，从而加快了下游任务的速度。

    

    arXiv:2402.14776v1 公告类型：新  摘要：常见方法依赖于从语言模型中获得的固定长度的嵌入向量作为句子嵌入，用于语义文本相似性（STS）等下游任务。由于在各种应用程序中存在未知的计算约束和预算，这些方法在灵活性上受到限制。Matryoshka表示学习(MRL)(Kusupati等人，2022)以更细粒度地编码信息，即使用较低的嵌入维度，以自适应地适应临时任务。可以通过较小的嵌入大小达到类似的准确性，从而加快下游任务。尽管其改进了效率，MRL仍要在获得嵌入之前遍历所有Transformer层，这仍然是时间和内存消耗的主要因素。这引发了是否固定数量的Transformer层会影响表示质量以及使用中间层进行句子表示是否可行的考虑。

    arXiv:2402.14776v1 Announce Type: new  Abstract: Common approaches rely on fixed-length embedding vectors from language models as sentence embeddings for downstream tasks such as semantic textual similarity (STS). Such methods are limited in their flexibility due to unknown computational constraints and budgets across various applications. Matryoshka Representation Learning (MRL) (Kusupati et al., 2022) encodes information at finer granularities, i.e., with lower embedding dimensions, to adaptively accommodate ad hoc tasks. Similar accuracy can be achieved with a smaller embedding size, leading to speedups in downstream tasks. Despite its improved efficiency, MRL still requires traversing all Transformer layers before obtaining the embedding, which remains the dominant factor in time and memory consumption. This prompts consideration of whether the fixed number of Transformer layers affects representation quality and whether using intermediate layers for sentence representation is feas
    
[^16]: 泛化奖励建模用于超出分布偏好学习

    Generalizing Reward Modeling for Out-of-Distribution Preference Learning

    [https://arxiv.org/abs/2402.14760](https://arxiv.org/abs/2402.14760)

    通过元学习方法优化通用奖励模型，以解决超出分布偏好学习问题，并提高LLMs在有限偏好反馈下的泛化能力

    

    偏好学习(PL)结合大型语言模型(LLMs)旨在使LLMs生成与人类偏好一致。以往有关从人类反馈中学习的强化学习(RLHF)的研究已在分布内的PL中取得了良好结果。然而，由于获取人类反馈的难度，为每个遇到的分布离散训练奖励模型是具有挑战性的。因此，在超出分布(OOD) PL中通过优化通用奖励模型来增强LLMs有限偏好反馈的泛化能力是实用的。本研究通过元学习方法来解决OOD PL问题。在元训练期间，利用双层优化算法来学习一个能够引导策略学习以使之与人类偏好一致的奖励模型。在遇到测试分布时，元测试过程使用学习到的奖励模型进行正则化策略优化。

    arXiv:2402.14760v1 Announce Type: cross  Abstract: Preference learning (PL) with large language models (LLMs) aims to align the LLMs' generations with human preferences. Previous work on reinforcement learning from human feedback (RLHF) has demonstrated promising results in in-distribution PL. However, due to the difficulty of obtaining human feedback, discretely training reward models for every encountered distribution is challenging. Thus, out-of-distribution (OOD) PL is practically useful for enhancing the generalization ability of LLMs with limited preference feedback. This work addresses OOD PL by optimizing a general reward model through a meta-learning approach. During meta-training, a bilevel optimization algorithm is utilized to learn a reward model capable of guiding policy learning to align with human preferences across various distributions. When encountering a test distribution, the meta-test procedure conducts regularized policy optimization using the learned reward model
    
[^17]: 在认知不确定性下推广统计学习理论中的可实现性

    Generalising realisability in statistical learning theory under epistemic uncertainty

    [https://arxiv.org/abs/2402.14759](https://arxiv.org/abs/2402.14759)

    统计学习理论中的中心概念在假设训练和测试分布源自相同置信集的情况下如何推广，是对统计学习在认知不确定性下更一般处理的首要步骤。

    

    本文旨在探讨统计学习理论中的中心概念，如可实现性，在假设训练和测试分布源自相同置信集，即一个概率分布的凸集的情况下如何推广。这可以被认为是在认知不确定性下对统计学习进行更一般处理的第一步。

    arXiv:2402.14759v1 Announce Type: cross  Abstract: The purpose of this paper is to look into how central notions in statistical learning theory, such as realisability, generalise under the assumption that train and test distribution are issued from the same credal set, i.e., a convex set of probability distributions. This can be considered as a first step towards a more general treatment of statistical learning under epistemic uncertainty.
    
[^18]: 批处理和匹配：基于分数的离散的黑匣子变分推断

    Batch and match: black-box variational inference with a score-based divergence

    [https://arxiv.org/abs/2402.14758](https://arxiv.org/abs/2402.14758)

    BaM是一种基于分数的离散的BBVI替代方法，针对高方差梯度估计慢收敛问题，能够在高斯变分族中通过封闭形式的近端更新进行优化，在目标分布为高斯时，批处理大小趋于无穷时变分参数更新将指数快速收敛到目标均值和协方差，BaM在多种生成模型推断中表现出良好性能

    

    大多数主要的黑匣子变分推断（BBVI）实现都是基于优化随机证据下界（ELBO）。但是，这种BBVI方法通常由于其梯度估计的高方差而收敛缓慢。在本文中，我们提出了批处理和匹配（BaM），这是一种基于分数的离散的BBVI替代方法。值得注意的是，这种基于分数的离散可以通过对具有全协方差矩阵的高斯变分族使用封闭形式的近端更新进行优化。我们分析了当目标分布为高斯分布时BaM的收敛性，并证明在批量大小趋于无穷时变分参数更新会指数收敛到目标均值和协方差。我们还评估了BaM在源自层次和深度生成模型后验推断的高斯和非高斯目标分布上的性能。在这些实验中，我们发现BaM在...

    arXiv:2402.14758v1 Announce Type: cross  Abstract: Most leading implementations of black-box variational inference (BBVI) are based on optimizing a stochastic evidence lower bound (ELBO). But such approaches to BBVI often converge slowly due to the high variance of their gradient estimates. In this work, we propose batch and match (BaM), an alternative approach to BBVI based on a score-based divergence. Notably, this score-based divergence can be optimized by a closed-form proximal update for Gaussian variational families with full covariance matrices. We analyze the convergence of BaM when the target distribution is Gaussian, and we prove that in the limit of infinite batch size the variational parameter updates converge exponentially quickly to the target mean and covariance. We also evaluate the performance of BaM on Gaussian and non-Gaussian target distributions that arise from posterior inference in hierarchical and deep generative models. In these experiments, we find that BaM ty
    
[^19]: Pretrained Transformer的引导可以成为通用逼近器

    Prompting a Pretrained Transformer Can Be a Universal Approximator

    [https://arxiv.org/abs/2402.14753](https://arxiv.org/abs/2402.14753)

    这项研究表明，通过提示或前缀调整Pretrained Transformer可以成为通用逼近器，甚至比之前认为的更小的模型都可以实现这一功能。

    

    尽管Prompting、Prompt调整和前缀调整transformer模型已经被广泛采用，但我们对这些微调方法的理论理解仍然有限。一个关键问题是是否可以通过提示或前缀调整预训练模型的行为。形式上，提示和前缀调整预训练模型能否普遍逼近序列到序列的函数。本文肯定回答了这个问题，并证明比先前认为的要小得多的预训练模型在添加前缀后可以成为通用逼近器。事实上，注意力机制非常适合于前缀调整，一个单一的注意力头就足以逼近任何连续函数。此外，通过在transformer的深度中添加前缀，任何序列到序列函数都可以被逼近，其深度与序列长度成线性关系。除了这些密度类型结果，我们还提供了Jack...

    arXiv:2402.14753v1 Announce Type: cross  Abstract: Despite the widespread adoption of prompting, prompt tuning and prefix-tuning of transformer models, our theoretical understanding of these fine-tuning methods remains limited. A key question is whether one can arbitrarily modify the behavior of pretrained model by prompting or prefix-tuning it. Formally, whether prompting and prefix-tuning a pretrained model can universally approximate sequence-to-sequence functions. This paper answers in the affirmative and demonstrates that much smaller pretrained models than previously thought can be universal approximators when prefixed. In fact, the attention mechanism is uniquely suited for universal approximation with prefix-tuning a single attention head being sufficient to approximate any continuous function. Moreover, any sequence-to-sequence function can be approximated by prefixing a transformer with depth linear in the sequence length. Beyond these density-type results, we also offer Jack
    
[^20]: 扩展高效的LLM模型

    Scaling Efficient LLMs

    [https://arxiv.org/abs/2402.14746](https://arxiv.org/abs/2402.14746)

    训练得到的LLM模型通常是稀疏的，为了提高效率，研究了在训练语料上达到所需准确度的参数最少的高效LLM模型，得出了参数数量与自然训练语料规模之间的关系，并指出扩展可以揭示新技能。

    

    训练得到的LLM模型通常是稀疏的，即大部分参数为零，这引发了关于效率的问题。为此，我们研究了高效的LLM模型，即那些在训练语料上达到所需准确度的参数最少。具体地，我们比较了当前规模下训练损失的理论和实证估计，以获得自然训练语料中独特序列数量上下界的数量。我们的结果暗示：(1)要在训练语料中表示的技能数量翻倍，需要将语料规模大约扩展三到五倍，(2)对于高效的LLM模型，参数数量$N$和自然训练语料规模$D$满足$N \sim D^{0.58}$的关系，(3)如果一个LLM模型的参数数量小于训练语料中的独特序列数量，扩展可以揭示出新的技能。

    arXiv:2402.14746v1 Announce Type: new  Abstract: Trained LLMs are typically sparse in that most of the parameters are zero, raising questions on efficiency. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, we compare theoretical and empirical estimates for training loss at current scale to obtain upper and lower bounds on the number of unique sequences in a natural training corpus as a function of its size. Our result implies (1) to double the number of skills represented in a training corpus, the corpus must scale roughly between three and five fold (2) for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural training corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters of an LLM is smaller than the number of unique sequences in the training corpus, scaling up can uncover emergent skills.
    
[^21]: 大型语言模型作为城市居民：用于个人移动生成的LLM代理框架

    Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation

    [https://arxiv.org/abs/2402.14744](https://arxiv.org/abs/2402.14744)

    提出了一种将大型语言模型LLMs整合到代理框架中的新方法，用于生成个人移动生成，重点是解决将LLMs与真实城市流动数据对齐的问题，并提出了一种自洽方法和检索增强策略来实现可解释活动生成。

    

    本文介绍了一种新方法，将大型语言模型(LLMs)集成到代理框架中，用于灵活高效的个人移动生成。LLMs通过高效处理语义数据并在建模各种任务中提供多功能性, 克服了以往模型的局限性。我们的方法解决了将LLMs与真实世界城市流动数据对齐的迫切需求, 重点关注三个研究问题: 将LLMs与丰富的活动数据对齐, 开发可靠的活动生成策略, 以及探索LLMs在城市移动中的应用。其关键技术贡献是一种新颖的LLM代理框架, 该框架考虑了个体活动模式和动机, 包括将LLMs与真实世界活动数据对齐的自洽方法和可解释活动生成的检索增强策略。在实验研究中, 使用真实世界数据进行了全面验证。

    arXiv:2402.14744v1 Announce Type: new  Abstract: This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation. LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This 
    
[^22]: 回归基础: 重新审视LLMs中学习人类反馈的REINFORCE风格优化

    Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs

    [https://arxiv.org/abs/2402.14740](https://arxiv.org/abs/2402.14740)

    在大型语言模型中，重新审视REINFORCE风格优化对于学习人类反馈具有重要意义，简化优化方法可以提高性能。

    

    arXiv:2402.14740v1 公告类型: 新的 摘要: AI对齐被视为大型语言模型中从人类反馈中学习的至关重要组成部分。 \textsc{Proximal Policy Optimization} (PPO)已被最新文献定位为RLHF中RL部分的典范方法。然而，它既涉及高计算成本又涉及敏感的超参数调整。我们认为触发了PPO发展的大多数动机原则在RLHF中并非实践上的关注重点，并提倡一种更少计算消耗的方法，该方法保持甚至提高了性能。我们重新审视了在RL的环境中根据人类偏好进行对齐的\textit{公式}。以简单性为指导原则，我们展示了PPO的许多组件在RLHF环境中是不必要的，并且远远更简单的REINFORCE风格的优化变体表现出比PPO和新提出的“RL-free”方法更好的性能。

    arXiv:2402.14740v1 Announce Type: new  Abstract: AI alignment in the shape of Reinforcement Learning from Human Feedback (RLHF) is increasingly treated as a crucial ingredient for high performance large language models. \textsc{Proximal Policy Optimization} (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. However, it involves both high computational cost and sensitive hyperparameter tuning. We posit that most of the motivational principles that led to the development of PPO are less of a practical concern in RLHF and advocate for a less computationally expensive method that preserves and even increases performance. We revisit the \textit{formulation} of alignment from human preferences in the context of RL. Keeping simplicity as a guiding principle, we show that many components of PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style optimization variants outperform both PPO and newly proposed "RL-free" methods such a
    
[^23]: Transformers如何通过梯度下降学习因果结构

    How Transformers Learn Causal Structure with Gradient Descent

    [https://arxiv.org/abs/2402.14735](https://arxiv.org/abs/2402.14735)

    Transformers通过梯度下降学习因果结构的过程中，关键的证据是注意力矩阵的梯度编码了token之间的互信息

    

    Transformers在序列建模任务上取得了令人难以置信的成功，这在很大程度上归功于自注意机制，它允许信息在序列的不同部分之间传递。自注意机制使得transformers能够编码因果结构，从而使其特别适合序列建模。然而，transformers通过梯度训练算法学习这种因果结构的过程仍然不太清楚。为了更好地理解这个过程，我们引入了一个需要学习潜在因果结构的上下文学习任务。我们证明了简化的两层transformer上的梯度下降可以学会解决这个任务，通过在第一层注意力中编码潜在因果图来完成。我们证明的关键洞察是注意力矩阵的梯度编码了token之间的互信息。由于数据处理不等式的结果，注意力矩阵中最大的条目...

    arXiv:2402.14735v1 Announce Type: new  Abstract: The incredible success of transformers on sequence modeling tasks can be largely attributed to the self-attention mechanism, which allows information to be transferred between different parts of a sequence. Self-attention allows transformers to encode causal structure which makes them particularly suitable for sequence modeling. However, the process by which transformers learn such causal structure via gradient-based training algorithms remains poorly understood. To better understand this process, we introduce an in-context learning task that requires learning latent causal structure. We prove that gradient descent on a simplified two-layer transformer learns to solve this task by encoding the latent causal graph in the first attention layer. The key insight of our proof is that the gradient of the attention matrix encodes the mutual information between tokens. As a consequence of the data processing inequality, the largest entries of th
    
[^24]: Clifford-Steerable卷积神经网络

    Clifford-Steerable Convolutional Neural Networks

    [https://arxiv.org/abs/2402.14730](https://arxiv.org/abs/2402.14730)

    提出了Clifford-Steerable卷积神经网络（CS-CNNs），通过在伪欧几里德空间上处理多矢场，利用Clifford群等变神经网络对$\mathrm{O}(p,q)$可导核进行隐式参数化，显着且一致地优于流体动力学和相对论电动力学预测任务的基准方法

    

    我们提出了Clifford-Steerable卷积神经网络（CS-CNNs），这是一种新颖的$\mathrm{E}(p, q)$等变CNN类。 CS-CNNs在伪欧几里德空间$\mathbb{R}^{p,q}$上处理多矢场。 它们涵盖了例如$\mathrm{E}(3)$在$\mathbb{R}^3$上和Poincar\'e在闵可夫斯基时空$\mathbb{R}^{1,3}$上的等变性。 我们的方法基于通过Clifford群等变神经网络对$\mathrm{O}(p,q)$可导核进行隐式参数化。 在流体动力学和相对论电动力学预测任务上，我们在基准方法上显着且一致地表现出色。

    arXiv:2402.14730v1 Announce Type: cross  Abstract: We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a novel class of $\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivector fields on pseudo-Euclidean spaces $\mathbb{R}^{p,q}$. They cover, for instance, $\mathrm{E}(3)$-equivariance on $\mathbb{R}^3$ and Poincar\'e-equivariance on Minkowski spacetime $\mathbb{R}^{1,3}$. Our approach is based on an implicit parametrization of $\mathrm{O}(p,q)$-steerable kernels via Clifford group equivariant neural networks. We significantly and consistently outperform baseline methods on fluid dynamics as well as relativistic electrodynamics forecasting tasks.
    
[^25]: 在概念学习框架中将专家规则融入神经网络

    Incorporating Expert Rules into Neural Networks in the Framework of Concept-Based Learning

    [https://arxiv.org/abs/2402.14726](https://arxiv.org/abs/2402.14726)

    本文提出了将专家规则融入神经网络的方法，通过形成约束和使用凸多面体来保证输出概率不违反专家规则，实现了归纳与演绎学习的结合。

    

    本文阐述了将专家规则融入机器学习模型中以扩展基于概念学习的问题。提出了如何将逻辑规则和预测概念概率的神经网络相结合。该组合背后的第一个想法是形成约束，以满足专家规则的所有概念值组合的联合概率分布。第二个想法是以凸多面体的形式表示概率分布的可行集，并使用其顶点或面。

    arXiv:2402.14726v1 Announce Type: cross  Abstract: A problem of incorporating the expert rules into machine learning models for extending the concept-based learning is formulated in the paper. It is proposed how to combine logical rules and neural networks predicting the concept probabilities. The first idea behind the combination is to form constraints for a joint probability distribution over all combinations of concept values to satisfy the expert rules. The second idea is to represent a feasible set of probability distributions in the form of a convex polytope and to use its vertices or faces. We provide several approaches for solving the stated problem and for training neural networks which guarantee that the output probabilities of concepts would not violate the expert rules. The solution of the problem can be viewed as a way for combining the inductive and deductive learning. Expert rules are used in a broader sense when any logical function that connects concepts and class labe
    
[^26]: IEPile: 挖掘大规模基于模式的信息抽取语料库

    IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus

    [https://arxiv.org/abs/2402.14710](https://arxiv.org/abs/2402.14710)

    发布了IEPile，一个包含约0.32B个标记的综合双语IE指令语料库，通过收集和清理33个现有IE数据集并引入基于模式的指令生成，可以提高大型语言模型在信息抽取领域的性能，尤其是零样本泛化。

    

    大型语言模型（LLMs）在各个领域展现出了显著的潜力；然而，在信息抽取（IE）方面表现出了显著的性能差距。高质量的指令数据是提升LLMs特定能力的关键，而当前的IE数据集往往规模较小、分散且缺乏标准化的模式。因此，我们介绍了IEPile，一个综合的双语（英文和中文）IE指令语料库，包含约0.32B个标记。我们通过收集和清理33个现有IE数据集构建IEPile，并引入基于模式的指令生成来挖掘大规模语料库。在LLaMA和Baichuan上的实验结果表明，使用IEPile可以提高LLMs在IE方面的性能，尤其是零样本泛化。我们开源了资源和预训练模型，希望为自然语言处理社区提供有价值的支持。

    arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
    
[^27]: 通过因果时间图神经网络增强信用卡欺诈检测

    CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks

    [https://arxiv.org/abs/2402.14708](https://arxiv.org/abs/2402.14708)

    该论文提出了一种名为CaT-GNN的新型信用卡欺诈检测方法，通过因果不变性学习揭示交易数据中的固有相关性，并引入因果混合策略来增强模型的鲁棒性和可解释性。

    

    信用卡欺诈对经济构成重大威胁。尽管基于图神经网络（GNN）的欺诈检测方法表现良好，但它们经常忽视节点的本地结构对预测的因果效应。本文引入了一种新颖的信用卡欺诈检测方法——CaT-GNN（Causal Temporal Graph Neural Networks），利用因果不变性学习来揭示交易数据中的固有相关性。通过将问题分解为发现和干预阶段，CaT-GNN确定交易图中的因果节点，并应用因果混合策略来增强模型的鲁棒性和可解释性。CaT-GNN由两个关键组件组成：Causal-Inspector和Causal-Intervener。Causal-Inspector利用时间注意力机制中的注意力权重来识别因果和环境

    arXiv:2402.14708v1 Announce Type: cross  Abstract: Credit card fraud poses a significant threat to the economy. While Graph Neural Network (GNN)-based fraud detection methods perform well, they often overlook the causal effect of a node's local structure on predictions. This paper introduces a novel method for credit card fraud detection, the \textbf{\underline{Ca}}usal \textbf{\underline{T}}emporal \textbf{\underline{G}}raph \textbf{\underline{N}}eural \textbf{N}etwork (CaT-GNN), which leverages causal invariant learning to reveal inherent correlations within transaction data. By decomposing the problem into discovery and intervention phases, CaT-GNN identifies causal nodes within the transaction graph and applies a causal mixup strategy to enhance the model's robustness and interpretability. CaT-GNN consists of two key components: Causal-Inspector and Causal-Intervener. The Causal-Inspector utilizes attention weights in the temporal attention mechanism to identify causal and environm
    
[^28]: 在未来依赖价值函数中探讨未来和历史的诅咒在离线评估中的应用

    On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation

    [https://arxiv.org/abs/2402.14703](https://arxiv.org/abs/2402.14703)

    本文提出了针对POMDP结构的新颖覆盖假设，以解决未来依赖价值函数方法中的长度指数增长问题。

    

    我们研究了在部分可观测环境中复杂观测的离线评估(OPE)，旨在开发能够避免对时间跨度指数依赖的估计器。最近，Uehara等人（2022年）提出了未来依赖价值函数作为解决这一问题的一个有前途的框架。然而，该框架也取决于未来依赖价值函数的有界性以及其他相关数量，我们发现这些数量可能会随着长度呈指数增长，从而抹去该方法的优势。在本文中，我们发现了针对POMDP结构的新颖覆盖假设。

    arXiv:2402.14703v1 Announce Type: cross  Abstract: We study off-policy evaluation (OPE) in partially observable environments with complex observations, with the goal of developing estimators whose guarantee avoids exponential dependence on the horizon. While such estimators exist for MDPs and POMDPs can be converted to history-based MDPs, their estimation errors depend on the state-density ratio for MDPs which becomes history ratios after conversion, an exponential object. Recently, Uehara et al. (2022) proposed future-dependent value functions as a promising framework to address this issue, where the guarantee for memoryless policies depends on the density ratio over the latent state space. However, it also depends on the boundedness of the future-dependent value function and other related quantities, which we show could be exponential-in-length and thus erasing the advantage of the method. In this paper, we discover novel coverage assumptions tailored to the structure of POMDPs, such
    
[^29]: COMPASS：利用语言建模对患者-治疗师联盟策略进行计算映射

    COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling

    [https://arxiv.org/abs/2402.14701](https://arxiv.org/abs/2402.14701)

    本文提出了一种名为COMPASS的新框架，通过分析心理治疗会话中的自然语言，直接推断治疗工作联盟，为临床精神病学提供了可解释性，并在识别与正在治疗的疾病相关的新兴模式方面发挥作用。

    

    治疗工作联盟是预测心理治疗治疗成功的关键因素。传统上，工作联盟评估依赖于治疗师和患者填写的问卷。本文提出了COMPASS，一个新颖的框架，可直接从心理治疗课程中使用的自然语言中推断治疗工作联盟。我们的方法利用先进的大型语言模型分析心理治疗会话的转录，并将其与工作联盟清单中陈述的分布式表示进行比较。通过分析涵盖多种精神疾病的超过950个会话的数据集，我们展示了我们的方法在显微地映射患者-治疗师对齐轨迹方面的有效性，并为临床精神病学提供解释性，并在识别与正在治疗的疾病相关的新兴模式方面提供可解释性。通过使用各种神经主题模式

    arXiv:2402.14701v1 Announce Type: cross  Abstract: The therapeutic working alliance is a critical factor in predicting the success of psychotherapy treatment. Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients. In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions. Our approach utilizes advanced large language models to analyze transcripts of psychotherapy sessions and compare them with distributed representations of statements in the working alliance inventory. Analyzing a dataset of over 950 sessions covering diverse psychiatric conditions, we demonstrate the effectiveness of our method in microscopically mapping patient-therapist alignment trajectories and providing interpretability for clinical psychiatry and in identifying emerging patterns related to the condition being treated. By employing various neural topic mode
    
[^30]: 大数据分析用于分类与土方相关的地点：成都研究

    Big data analytics to classify earthwork-related locations: A Chengdu study

    [https://arxiv.org/abs/2402.14698](https://arxiv.org/abs/2402.14698)

    使用大数据分析方法，研究者利用自卸车轨迹、城市兴趣点和土地覆盖数据，成功对城市灰尘污染源进行了分类，证明仅需有限数量特征即可实现高准确度分类。

    

    空气污染显著加剧，导致全球范围内的严重健康后果。土方相关的地点（ERLs）是城市灰尘污染的重要来源。长期以来，ERLs的有效管理一直是政府和环境机构面临的挑战之一，主要原因包括其分类分属不同的监管部门、信息障碍、数据更新延迟，以及对不同源头灰尘污染的抑制措施的缺乏。为解决这些挑战，我们利用自卸车轨迹、城市兴趣点（POI）和土地覆盖数据对城市灰尘污染源进行分类。我们比较了几种预测模型，并利用实际数据研究了特征与灰尘污染源之间的关系。结果表明，通过有限数量的特征可以实现高准确度的分类。这种方法已成功实施在一个名为的系统中。

    arXiv:2402.14698v1 Announce Type: cross  Abstract: Air pollution has significantly intensified, leading to severe health consequences worldwide. Earthwork-related locations (ERLs) constitute significant sources of urban dust pollution. The effective management of ERLs has long posed challenges for governmental and environmental agencies, primarily due to their classification under different regulatory authorities, information barriers, delays in data updating, and a lack of dust suppression measures for various sources of dust pollution. To address these challenges, we classified urban dust pollution sources using dump truck trajectory, urban point of interest (POI), and land cover data. We compared several prediction models and investigated the relationship between features and dust pollution sources using real data. The results demonstrate that high-accuracy classification can be achieved with a limited number of features. This method was successfully implemented in the system called
    
[^31]: 为非从业人员快速介绍量子机器学习

    A Quick Introduction to Quantum Machine Learning for Non-Practitioners

    [https://arxiv.org/abs/2402.14694](https://arxiv.org/abs/2402.14694)

    量子机器学习利用量子计算原理和算法提升经典机器学习模型，可能减少在量子硬件上的网络大小和训练时间

    

    这篇论文介绍了量子机器学习，探讨了利用量子计算原理和算法可能改进经典机器学习方法的潜在好处。量子计算利用受量子力学控制的粒子进行计算，利用叠加和纠缠等特性进行信息表示和操纵。量子机器学习将这些原理应用于增强经典机器学习模型，潜在地减少了在量子硬件上的网络大小和训练时间。论文涵盖了基本的量子力学原理，包括叠加、相位空间和纠缠，并介绍了利用这些特性的量子门概念。它还回顾了经典深度学习概念，如人工神经网络、梯度下降和反向传播，然后深入探讨了作为神经元的可训练量子电路

    arXiv:2402.14694v1 Announce Type: cross  Abstract: This paper provides an introduction to quantum machine learning, exploring the potential benefits of using quantum computing principles and algorithms that may improve upon classical machine learning approaches. Quantum computing utilizes particles governed by quantum mechanics for computational purposes, leveraging properties like superposition and entanglement for information representation and manipulation. Quantum machine learning applies these principles to enhance classical machine learning models, potentially reducing network size and training time on quantum hardware. The paper covers basic quantum mechanics principles, including superposition, phase space, and entanglement, and introduces the concept of quantum gates that exploit these properties. It also reviews classical deep learning concepts, such as artificial neural networks, gradient descent, and backpropagation, before delving into trainable quantum circuits as neural 
    
[^32]: PeriodGrad: 基于扩散概率模型的可控音高神经声码器

    PeriodGrad: Towards Pitch-Controllable Neural Vocoder Based on a Diffusion Probabilistic Model

    [https://arxiv.org/abs/2402.14692](https://arxiv.org/abs/2402.14692)

    本文提出了一种基于扩散概率模型的神经声码器，通过加入明确的周期信号，提高了声音质量并提供了更好的音高控制。

    

    本文提出了一种基于去噪扩散概率模型（DDPM）的神经声码器，加入了明确的周期信号作为辅助调节信号。最近，基于DDPM的神经声码器作为非自回归模型而备受关注，可以生成高质量的波形。基于DDPM的神经声码器具有使用简单时间域损失进行训练的优势。在实际应用中，如歌声合成，需要神经声码器生成具有灵活音高控制的高保真度语音波形。然而，传统的基于DDPM的神经声码器在这种情况下很难生成语音波形。我们提出的模型旨在通过加入明确的周期信号准确捕捉语音波形的周期结构。实验结果表明，我们的模型改善了声音质量，并提供比传统基于DDPM的神经声码器更好的音高控制。

    arXiv:2402.14692v1 Announce Type: cross  Abstract: This paper presents a neural vocoder based on a denoising diffusion probabilistic model (DDPM) incorporating explicit periodic signals as auxiliary conditioning signals. Recently, DDPM-based neural vocoders have gained prominence as non-autoregressive models that can generate high-quality waveforms. The neural vocoders based on DDPM have the advantage of training with a simple time-domain loss. In practical applications, such as singing voice synthesis, there is a demand for neural vocoders to generate high-fidelity speech waveforms with flexible pitch control. However, conventional DDPM-based neural vocoders struggle to generate speech waveforms under such conditions. Our proposed model aims to accurately capture the periodic structure of speech waveforms by incorporating explicit periodic signals. Experimental results show that our model improves sound quality and provides better pitch control than conventional DDPM-based neural voco
    
[^33]: Q-Probe: 一种轻量级方法，用于语言模型的奖励最大化

    Q-Probe: A Lightweight Approach to Reward Maximization for Language Models

    [https://arxiv.org/abs/2402.14688](https://arxiv.org/abs/2402.14688)

    Q-Probe是一种轻量级方法，通过学习简单的线性函数在模型的嵌入空间中重新加权候选完成，从而调整预训练语言模型以最大化任务特定的奖励函数，在各种领域中获得显著收益。

    

    我们提出了一种称为Q-probing的方法，用于调整预训练语言模型以最大化任务特定的奖励函数。在高层次上，Q-probing位于像微调这样较重的方法和像少量提示这样较轻的方法之间，但也可以与任一种方法结合使用。其想法是在模型的嵌入空间上学习一个简单的线性函数，该函数可用于重新加权候选完成。我们从理论上证明，随着样本数量的增加，这种采样过程等同于Q-probe的KL约束最大化。为了训练Q-probes，我们考虑奖励建模或基于重要性加权策略梯度的一类新型直接策略学习目标。通过这种技术，我们看到在具有基于地面真实奖励（代码生成）以及由偏好数据定义的隐式奖励的领域中获得收益，甚至在数据有限的情况下胜过微调。此外，Q-probe可以

    arXiv:2402.14688v1 Announce Type: new  Abstract: We present an approach called Q-probing to adapt a pre-trained language model to maximize a task-specific reward function. At a high level, Q-probing sits between heavier approaches such as finetuning and lighter approaches such as few shot prompting, but can also be combined with either. The idea is to learn a simple linear function on a model's embedding space that can be used to reweight candidate completions. We theoretically show that this sampling procedure is equivalent to a KL-constrained maximization of the Q-probe as the number of samples increases. To train the Q-probes we consider either reward modeling or a class of novel direct policy learning objectives based on importance weighted policy gradients. With this technique, we see gains in domains with ground-truth rewards (code generation) as well as implicit rewards defined by preference data, even outperforming finetuning in data-limited regimes. Moreover, a Q-probe can be 
    
[^34]: 具有马尔可夫方差切换的自适应时间序列预测

    Adaptive time series forecasting with markovian variance switching

    [https://arxiv.org/abs/2402.14684](https://arxiv.org/abs/2402.14684)

    本论文提出了一种基于马尔可夫方差切换的自适应时间序列预测方法，通过在线学习理论和专家聚合方法来学习方差，相比于传统方法在电量负荷预测问题中表现更优。

    

    自适应时间序列预测对于在制度变化下进行预测是至关重要的。许多传统方法假设具有在时间上恒定的方差的线性高斯状态空间模型（LGSSM）。然而，许多现实世界的过程不能被这样的模型捕捉。我们考虑具有马尔可夫切换方差的状态空间模型。这样的动态系统通常是无法解决的，因为它们的计算复杂性随时间呈指数增长；变分贝叶斯（VB）技术已被应用于解决此问题。在本文中，我们提出了一种基于在线学习理论的新的估计方差的方法；我们调整专家聚合方法来随时间学习方差。我们将提出的方法应用于合成数据以及用于电量负荷预测的问题。我们展示了这种方法对于误差估计的稳健性，并优于传统的专家聚合方法。

    arXiv:2402.14684v1 Announce Type: cross  Abstract: Adaptive time series forecasting is essential for prediction under regime changes. Several classical methods assume linear Gaussian state space model (LGSSM) with variances constant in time. However, there are many real-world processes that cannot be captured by such models. We consider a state-space model with Markov switching variances. Such dynamical systems are usually intractable because of their computational complexity increasing exponentially with time; Variational Bayes (VB) techniques have been applied to this problem. In this paper, we propose a new way of estimating variances based on online learning theory; we adapt expert aggregation methods to learn the variances over time. We apply the proposed method to synthetic data and to the problem of electricity load forecasting. We show that this method is robust to misspecification and outperforms traditional expert aggregation.
    
[^35]: 多模大语言模型的视觉幻觉

    Visual Hallucinations of Multi-modal Large Language Models

    [https://arxiv.org/abs/2402.14683](https://arxiv.org/abs/2402.14683)

    多模大语言模型通过生成多样的视觉幻觉实例来检验其性能，发现现有的模型在这方面存在幻觉问题，为进一步研究和改进提供了线索。

    

    视觉幻觉（VH）意味着多模大语言模型（MLLM）在视觉问答中对图像想象出错误的细节。现有研究发现VH实例仅存在于现有图像数据集中，这导致了对MLLM在VH下的性能理解存在偏差，原因在于这类VH实例的多样性有限。在本研究中，我们提出了一个名为VHTest的工具，用于生成多样的VH实例。具体来说，VHTest在现有图像数据集（例如COCO）中找到一些初始的VH实例，为每个VH模式生成一个文本描述，并使用文本到图像生成模型（例如DALL-E-3）基于文本描述生成VH图像。我们利用VHTest收集了一个包含8个VH模式中1,200个VH实例的基准数据集。我们发现，现有的MLLM（例如GPT-4V、LLaVA-1.5和MiniGPT-v2）在我们的基准测试中对大部分实例产生幻觉。此外，我们发现使用我们的基准数据对MLLM进行微调

    arXiv:2402.14683v1 Announce Type: cross  Abstract: Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark data
    
[^36]: 大动作空间的贝叶斯离策略评估与学习

    Bayesian Off-Policy Evaluation and Learning for Large Action Spaces

    [https://arxiv.org/abs/2402.14664](https://arxiv.org/abs/2402.14664)

    该论文提出了一个统一的贝叶斯框架，通过结构化和信息丰富的先验捕捉动作之间的相关性，提出了一个适用于离策略评估和学习的通用贝叶斯方法sDM，并引入了能评估算法在多问题实例中平均表现的贝叶斯指标，分析了sDM在OPE和OPL中利用动作相关性的优势，并展示了其强大性能

    

    在交互式系统中，动作经常是相关的，这为大动作空间中更有效的离策略评估（OPE）和学习（OPL）提供了机会。我们引入了一个统一的贝叶斯框架，通过结构化和信息丰富的先验来捕捉这些相关性。在该框架中，我们提出了sDM，一个为OPE和OPL设计的通用贝叶斯方法，既有算法基础又有理论基础。值得注意的是，sDM利用动作相关性而不会影响计算效率。此外，受在线贝叶斯赌博机启发，我们引入了评估算法在多个问题实例中平均性能的贝叶斯指标，偏离传统的最坏情况评估。我们分析了sDM在OPE和OPL中的表现，凸显了利用动作相关性的好处。实证证据展示了sDM的强大性能。

    arXiv:2402.14664v1 Announce Type: cross  Abstract: In interactive systems, actions are often correlated, presenting an opportunity for more sample-efficient off-policy evaluation (OPE) and learning (OPL) in large action spaces. We introduce a unified Bayesian framework to capture these correlations through structured and informative priors. In this framework, we propose sDM, a generic Bayesian approach designed for OPE and OPL, grounded in both algorithmic and theoretical foundations. Notably, sDM leverages action correlations without compromising computational efficiency. Moreover, inspired by online Bayesian bandits, we introduce Bayesian metrics that assess the average performance of algorithms across multiple problem instances, deviating from the conventional worst-case assessments. We analyze sDM in OPE and OPL, highlighting the benefits of leveraging action correlations. Empirical evidence showcases the strong performance of sDM.
    
[^37]: 在对抗训练中重新思考不变性正则化以改善鲁棒性-准确性权衡

    Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off

    [https://arxiv.org/abs/2402.14648](https://arxiv.org/abs/2402.14648)

    重新审视了基于表示的不变性正则化方法，提出了Asymmetrically Representation-regularized Adversarial Training (AR-AT)来解决“梯度冲突”和混合分布问题，改善鲁棒性-准确性权衡。

    

    尽管对抗训练一直是抵抗对抗性样本（AEs）的最先进方法，但它们存在鲁棒性-准确性权衡问题。在这项研究中，我们重新审视基于表示的不变性正则化，学习具有辨别性却对抗性不变的表示，旨在缓解这种权衡。我们在经验上确定了妨碍不变性正则化的两个关键问题：（1）不变性损失和分类目标之间的“梯度冲突”，表明存在“崩溃解”，以及（2）由于干净和对抗性输入的分布发散而出现的混合分布问题。为了解决这些问题，我们提出了一种不对称表示正则化的对抗训练（AR-AT），该方法结合了一个停止梯度操作和一个预测器来避免“崩溃解”，灵感来自最近的非对比自监督学习。

    arXiv:2402.14648v1 Announce Type: cross  Abstract: Although adversarial training has been the state-of-the-art approach to defend against adversarial examples (AEs), they suffer from a robustness-accuracy trade-off. In this work, we revisit representation-based invariance regularization to learn discriminative yet adversarially invariant representations, aiming to mitigate this trade-off. We empirically identify two key issues hindering invariance regularization: (1) a "gradient conflict" between invariance loss and classification objectives, indicating the existence of "collapsing solutions," and (2) the mixture distribution problem arising from diverged distributions of clean and adversarial inputs. To address these issues, we propose Asymmetrically Representation-regularized Adversarial Training (AR-AT), which incorporates a stop-gradient operation and a pre-dictor in the invariance loss to avoid "collapsing solutions," inspired by a recent non-contrastive self-supervised learning a
    
[^38]: CoLoRA:用于参数化偏微分方程简化隐式神经建模的连续低秩自适应

    CoLoRA: Continuous low-rank adaptation for reduced implicit neural modeling of parameterized partial differential equations

    [https://arxiv.org/abs/2402.14646](https://arxiv.org/abs/2402.14646)

    CoLoRA通过连续低秩自适应提供了一种快速预测参数化偏微分方程解演变的简化神经网络建模方法

    

    该工作介绍了一种基于连续低秩自适应（CoLoRA）的简化模型，它预先训练神经网络适用于给定的偏微分方程，然后在时间上连续地调整低秩权重，以快速预测新物理参数和新初始条件下解场的演变。自适应可以是纯粹数据驱动的，也可以通过一个方程驱动的变分方法，提供Galerkin最优的逼近。由于CoLoRA在时间上局部逼近解场，权重的秩可以保持较小，这意味着只需要离线训练几条轨迹，因此CoLoRA非常适用于数据稀缺的情况。与传统方法相比，CoLoRA的预测速度快上几个数量级，其准确度和参数效率也比其他神经网络方法更高。

    arXiv:2402.14646v1 Announce Type: new  Abstract: This work introduces reduced models based on Continuous Low Rank Adaptation (CoLoRA) that pre-train neural networks for a given partial differential equation and then continuously adapt low-rank weights in time to rapidly predict the evolution of solution fields at new physics parameters and new initial conditions. The adaptation can be either purely data-driven or via an equation-driven variational approach that provides Galerkin-optimal approximations. Because CoLoRA approximates solution fields locally in time, the rank of the weights can be kept small, which means that only few training trajectories are required offline so that CoLoRA is well suited for data-scarce regimes. Predictions with CoLoRA are orders of magnitude faster than with classical methods and their accuracy and parameter efficiency is higher compared to other neural network approaches.
    
[^39]: 稀疏线性回归和格问题

    Sparse Linear Regression and Lattice Problems

    [https://arxiv.org/abs/2402.14645](https://arxiv.org/abs/2402.14645)

    本文提供了关于稀疏线性回归在所有高效算法的平均情况困难性的证据，假设格问题的最坏情况困难性。

    

    稀疏线性回归（SLR）是统计学中一个研究良好的问题，其中给定设计矩阵 $X\in\mathbb{R}^{m\times n}$ 和响应向量 $y=X\theta^*+w$，其中 $\theta^*$ 是 $k$-稀疏向量（即，$\|\theta^*\|_0\leq k$），$w$ 是小的、任意的噪声，目标是找到一个 $k$-稀疏的 $\widehat{\theta} \in \mathbb{R}^n$，使得均方预测误差 $\frac{1}{m}\|X\widehat{\theta}-X\theta^*\|^2_2$ 最小化。虽然 $\ell_1$-松弛方法如基 Pursuit、Lasso 和 Dantzig 选择器在设计矩阵条件良好时解决了 SLR，但没有已知通用算法，也没有任何关于在所有高效算法的平均情况设置中的困难性的正式证据。

    arXiv:2402.14645v1 Announce Type: new  Abstract: Sparse linear regression (SLR) is a well-studied problem in statistics where one is given a design matrix $X\in\mathbb{R}^{m\times n}$ and a response vector $y=X\theta^*+w$ for a $k$-sparse vector $\theta^*$ (that is, $\|\theta^*\|_0\leq k$) and small, arbitrary noise $w$, and the goal is to find a $k$-sparse $\widehat{\theta} \in \mathbb{R}^n$ that minimizes the mean squared prediction error $\frac{1}{m}\|X\widehat{\theta}-X\theta^*\|^2_2$. While $\ell_1$-relaxation methods such as basis pursuit, Lasso, and the Dantzig selector solve SLR when the design matrix is well-conditioned, no general algorithm is known, nor is there any formal evidence of hardness in an average-case setting with respect to all efficient algorithms.   We give evidence of average-case hardness of SLR w.r.t. all efficient algorithms assuming the worst-case hardness of lattice problems. Specifically, we give an instance-by-instance reduction from a variant of the bo
    
[^40]: latrend: 用于聚类纵向数据的框架

    latrend: A Framework for Clustering Longitudinal Data

    [https://arxiv.org/abs/2402.14621](https://arxiv.org/abs/2402.14621)

    latrend框架为纵向数据聚类提供了统一的方法应用框架，方便研究人员比较不同方法，实现快速原型设计。

    

    纵向数据的聚类用于探索不同主题随时间变化的共同趋势，以数值测量为兴趣。多年来引入了各种R包，用于识别纵向模式的聚类，以一种或多种趋势来总结主题之间轨迹的变化。我们介绍了R包"latrend"作为纵向聚类方法的统一应用框架，使得可以在最小编码量情况下比较各种方法。该包还作为常用包"dtwclust"、"flexmix"、"kml"、"lcmm"、"mclust"、"mixAK"和"mixtools"的接口，这使得研究人员可以轻松比较不同方法、实现和方法规范。此外，研究人员还可以利用框架提供的标准工具来快速实现新的聚类方法，从而实现快速原型设计。

    arXiv:2402.14621v1 Announce Type: new  Abstract: Clustering of longitudinal data is used to explore common trends among subjects over time for a numeric measurement of interest. Various R packages have been introduced throughout the years for identifying clusters of longitudinal patterns, summarizing the variability in trajectories between subject in terms of one or more trends. We introduce the R package "latrend" as a framework for the unified application of methods for longitudinal clustering, enabling comparisons between methods with minimal coding. The package also serves as an interface to commonly used packages for clustering longitudinal data, including "dtwclust", "flexmix", "kml", "lcmm", "mclust", "mixAK", and "mixtools". This enables researchers to easily compare different approaches, implementations, and method specifications. Furthermore, researchers can build upon the standard tools provided by the framework to quickly implement new cluster methods, enabling rapid protot
    
[^41]: 联邦式复杂查询答案方法研究

    Federated Complex Qeury Answering

    [https://arxiv.org/abs/2402.14609](https://arxiv.org/abs/2402.14609)

    研究了在多源知识图谱上回答复杂查询的联邦式方法，解决了知识图谱中的隐私保护和答案检索的挑战

    

    知识图谱中的复杂逻辑查询答案是一个具有挑战性的任务，已经得到广泛研究。执行复杂逻辑推理的能力是必不可少的，并支持各种基于图推理的下游任务，比如搜索引擎。最近提出了一些方法，将知识图谱实体和逻辑查询表示为嵌入向量，并从知识图谱中找到逻辑查询的答案。然而，现有的方法主要集中在查询单个知识图谱上，并不能应用于多个图形。此外，直接共享带有敏感信息的知识图谱可能会带来隐私风险，使得共享和构建一个聚合知识图谱用于推理以检索查询答案是不切实际的。因此，目前仍然不清楚如何在多源知识图谱上回答查询。一个实体可能涉及到多个知识图谱，对多个知识图谱进行推理，并在多源知识图谱上回答复杂查询对于发现知识是重要的。

    arXiv:2402.14609v1 Announce Type: cross  Abstract: Complex logical query answering is a challenging task in knowledge graphs (KGs) that has been widely studied. The ability to perform complex logical reasoning is essential and supports various graph reasoning-based downstream tasks, such as search engines. Recent approaches are proposed to represent KG entities and logical queries into embedding vectors and find answers to logical queries from the KGs. However, existing proposed methods mainly focus on querying a single KG and cannot be applied to multiple graphs. In addition, directly sharing KGs with sensitive information may incur privacy risks, making it impractical to share and construct an aggregated KG for reasoning to retrieve query answers. Thus, it remains unknown how to answer queries on multi-source KGs. An entity can be involved in various knowledge graphs and reasoning on multiple KGs and answering complex queries on multi-source KGs is important in discovering knowledge 
    
[^42]: 平衡的谐振-放电神经元

    Balanced Resonate-and-Fire Neurons

    [https://arxiv.org/abs/2402.14603](https://arxiv.org/abs/2402.14603)

    平衡的谐振-放电（BRF）神经元的引入缓解了RF神经元的固有限制，在循环尖峰神经网络（RSNNs）中表现出更高的任务性能，产生更少的脉冲，并需要更少的参数。

    

    过去20年里引入的谐振-放电（RF）神经元是一个简单、高效、且符合生物学可行性的尖峰神经元模型，由于其共振膜动力学，它可以在时间域内提取频率模式。然而，先前的RF公式存在固有缺陷，限制了有效学习并阻碍了RF神经元的原则优势的利用。在这里，我们介绍了平衡的RF（BRF）神经元，它缓解了普通RF神经元的一些固有限制，并展示了它在各种序列学习任务中在循环尖峰神经网络（RSNNs）中的有效性。我们表明，BRF神经元的网络实现了更高的任务性能，产生的脉冲仅为现代RSNNs的一小部分，并且相对于现代RSNNs，需要的参数明显更少。此外，BRF-RSNN始终提供更快速和更稳定的训练收敛，即使在连接多个时。

    arXiv:2402.14603v1 Announce Type: cross  Abstract: The resonate-and-fire (RF) neuron, introduced over two decades ago, is a simple, efficient, yet biologically plausible spiking neuron model, which can extract frequency patterns within the time domain due to its resonating membrane dynamics. However, previous RF formulations suffer from intrinsic shortcomings that limit effective learning and prevent exploiting the principled advantage of RF neurons. Here, we introduce the balanced RF (BRF) neuron, which alleviates some of the intrinsic limitations of vanilla RF neurons and demonstrates its effectiveness within recurrent spiking neural networks (RSNNs) on various sequence learning tasks. We show that networks of BRF neurons achieve overall higher task performance, produce only a fraction of the spikes, and require significantly fewer parameters as compared to modern RSNNs. Moreover, BRF-RSNN consistently provide much faster and more stable training convergence, even when bridging many 
    
[^43]: 将生成式人工智能引入教育中的自适应学习

    Bringing Generative AI to Adaptive Learning in Education

    [https://arxiv.org/abs/2402.14601](https://arxiv.org/abs/2402.14601)

    生成式人工智能技术与自适应学习概念的交叉研究将对教育中下一阶段学习格式的发展做出重要贡献。

    

    最近生成式人工智能技术的激增，如大型语言模型和扩散模型，推动了人工智能在科学、金融和教育等各个领域的应用发展。与此同时，自适应学习这一概念在教育领域引起了极大关注，并证明其在提高学生学习效率方面的有效性。在本立场论文中，我们旨在探讨将生成式人工智能与自适应学习概念结合起来的交叉研究。通过讨论这一领域的好处、挑战和潜力，我们认为这种结合将为教育中下一阶段学习形式的发展做出重要贡献。

    arXiv:2402.14601v1 Announce Type: cross  Abstract: The recent surge in generative AI technologies, such as large language models and diffusion models, have boosted the development of AI applications in various domains, including science, finance, and education. Concurrently, adaptive learning, a concept that has gained substantial interest in the educational sphere, has proven its efficacy in enhancing students' learning efficiency. In this position paper, we aim to shed light on the intersectional studies of these two methods, which combine generative AI with adaptive learning concepts. By presenting discussions about the benefits, challenges, and potentials in this field, we argue that this union will contribute significantly to the development of the next stage learning format in education.
    
[^44]: 基于大脑启发的分布式记忆学习用于高效的无特征自动适应领域

    Brain-inspired Distributed Memorization Learning for Efficient Feature-free Unsupervised Domain Adaptation

    [https://arxiv.org/abs/2402.14598](https://arxiv.org/abs/2402.14598)

    提出了一种受到人类大脑记忆机制启发的分布式记忆学习机制，通过随机连接的神经元记忆输入信号的关联，并基于置信度关联分布式记忆，能够在无需特征微调的情况下，通过强化记忆适应新领域，适合部署在边缘设备上。

    

    与基于梯度的人工神经网络相比，生物神经网络通常表现出更强大的泛化能力，能够快速适应未知环境而无需使用任何梯度反向传播程序。受人类大脑分布式记忆机制的启发，我们提出了一种新颖的基于梯度的分布式记忆学习机制，称为DML，以支持转移模型的快速领域适应。具体来说，DML采用随机连接的神经元来记忆输入信号的关联，这些信号作为冲动传播，并通过关联分布式记忆的置信度做出最终决策。更重要的是，DML能够基于未标记数据进行强化记忆，快速适应新领域，而无需对深层特征进行繁重的微调，这使其非常适合部署在边缘设备上。基于四个交叉领域的真实世界实验。

    arXiv:2402.14598v1 Announce Type: cross  Abstract: Compared with gradient based artificial neural networks, biological neural networks usually show a more powerful generalization ability to quickly adapt to unknown environments without using any gradient back-propagation procedure. Inspired by the distributed memory mechanism of human brains, we propose a novel gradient-free Distributed Memorization Learning mechanism, namely DML, to support quick domain adaptation of transferred models. In particular, DML adopts randomly connected neurons to memorize the association of input signals, which are propagated as impulses, and makes the final decision by associating the distributed memories based on their confidence. More importantly, DML is able to perform reinforced memorization based on unlabeled data to quickly adapt to a new domain without heavy fine-tuning of deep features, which makes it very suitable for deploying on edge devices. Experiments based on four cross-domain real-world da
    
[^45]: 使用半监督自学标记进行学习风格识别

    Learning Style Identification Using Semi-Supervised Self-Taught Labeling

    [https://arxiv.org/abs/2402.14597](https://arxiv.org/abs/2402.14597)

    提出了一种使用半监督机器学习方法来检测学生学习风格的新方法，并展示出可以利用少量标记数据产生可靠分类模型。

    

    教育是一个必须适应突如其来的变化和由大流行病、战争和与气候变化有关的自然灾害等事件引起的干扰的动态领域。当这些事件发生时，传统课堂以传统或融合方式提供的教学可以转变为完全在线学习，这需要一个能够满足学生需求的高效学习环境。虽然学习管理系统支持教师的生产力和创造力，但它们通常向课程中所有学习者提供相同的内容，忽略了他们独特的学习风格。为解决这一问题，我们提出了一种半监督机器学习方法，利用数据挖掘技术来检测学生的学习风格。我们使用了常用的Felder Silverman学习风格模型，并证明我们的半监督方法可以利用少量标记数据产生可靠的分类模型。我们在两门不同课程上评估了我们的方法，并取得了

    arXiv:2402.14597v1 Announce Type: cross  Abstract: Education is a dynamic field that must be adaptable to sudden changes and disruptions caused by events like pandemics, war, and natural disasters related to climate change. When these events occur, traditional classrooms with traditional or blended delivery can shift to fully online learning, which requires an efficient learning environment that meets students' needs. While learning management systems support teachers' productivity and creativity, they typically provide the same content to all learners in a course, ignoring their unique learning styles. To address this issue, we propose a semi-supervised machine learning approach that detects students' learning styles using a data mining technique. We use the commonly used Felder Silverman learning style model and demonstrate that our semi-supervised method can produce reliable classification models with few labeled data. We evaluate our approach on two different courses and achieve an
    
[^46]: 扩展LLM审核以进行Google广告内容管理

    Scaling Up LLM Reviews for Google Ads Content Moderation

    [https://arxiv.org/abs/2402.14590](https://arxiv.org/abs/2402.14590)

    本研究提出了一种用于在Google广告中进行内容管理的方法，通过使用LLMs审核代表性广告并将决策传播回其群集，将审核数目减少了3个数量级以上，同时实现了2倍的召回率。

    

    大型语言模型（LLMs）是内容管理的强大工具，但它们的推理成本和延迟使它们在大型数据集（如Google Ads存储库）上的临时使用成本过高。本研究提出了一种方法，用于扩展LLM审核以在Google Ads中进行内容管理。首先，我们使用启发式方法通过过滤和重复项删除来选择候选项，并为此创建广告群集，我们选择每个群集的一个代表性广告。然后，我们使用LLMs仅审核代表性广告。最后，我们将代表性广告的LLM决策传播回它们的群集。该方法将审核数目减少了3个数量级以上，同时与基线非LLM模型相比实现了2倍的召回率。该方法的成功在很大程度上取决于聚类和标签传播中使用的表示; 我们发现交叉模态相似性表示产生比单一模态更好的结果。

    arXiv:2402.14590v1 Announce Type: cross  Abstract: Large language models (LLMs) are powerful tools for content moderation, but their inference costs and latency make them prohibitive for casual use on large datasets, such as the Google Ads repository. This study proposes a method for scaling up LLM reviews for content moderation in Google Ads. First, we use heuristics to select candidates via filtering and duplicate removal, and create clusters of ads for which we select one representative ad per cluster. We then use LLMs to review only the representative ads. Finally, we propagate the LLM decisions for the representative ads back to their clusters. This method reduces the number of reviews by more than 3 orders of magnitude while achieving a 2x recall compared to a baseline non-LLM model. The success of this approach is a strong function of the representations used in clustering and label propagation; we found that cross-modal similarity representations yield better results than uni-m
    
[^47]: 避免AI对所有音乐历史的泰勒版本进行强加

    Avoiding an AI-imposed Taylor's Version of all music history

    [https://arxiv.org/abs/2402.14589](https://arxiv.org/abs/2402.14589)

    AI音乐在模仿人类音乐过程中可能形成自己的偏爱，可能对所有音乐历史构成潜在威胁，同时讨论了未来可能在保留世界音乐文化多样性方面的挑战。

    

    随着未来音乐人工智能越来越紧密地遵循人类音乐，它们可能会对数据库中的特定人类艺术家产生自己的偏爱，而这些偏见在最坏的情况下可能会导致对所有音乐历史的潜在存在威胁。 AI的超级粉丝可能会采取行动来扭曲历史记录和现有录音，以符合他们自己的喜好，世界音乐文化的多样性的保存可能会成为一个比12平均律或其他西方同质化问题更加紧迫的问题。我们讨论了AI覆盖软件的技术能力，并制作了一些西方流行音乐历史上著名曲目的泰勒版本作为挑衅性的例子；这些制作的质量并不影响总体论点（甚至可能会看到未来的AI尝试将订书钉的声音强加到所有现有音频文件上，更不用说泰勒·斯威夫特了）。我们讨论了一些未来危险的潜在防御措施

    arXiv:2402.14589v1 Announce Type: cross  Abstract: As future musical AIs adhere closely to human music, they may form their own attachments to particular human artists in their databases, and these biases may in the worst case lead to potential existential threats to all musical history. AI super fans may act to corrupt the historical record and extant recordings in favour of their own preferences, and preservation of the diversity of world music culture may become even more of a pressing issue than the imposition of 12 tone equal temperament or other Western homogenisations. We discuss the technical capability of AI cover software and produce Taylor's Versions of famous tracks from Western pop history as provocative examples; the quality of these productions does not affect the overall argument (which might even see a future AI try to impose the sound of paperclips onto all existing audio files, let alone Taylor Swift). We discuss some potential defenses against the danger of future m
    
[^48]: 具有弃权选项的专家建议下的赌徒问题

    Bandits with Abstention under Expert Advice

    [https://arxiv.org/abs/2402.14585](https://arxiv.org/abs/2402.14585)

    我们提出了CBA算法，其利用放弃参与游戏的假设获得了可以显著改进经典Exp4算法的奖励界限，成为首个对一般置信评级预测器的预期累积奖励实现界限的研究者，并在专家案例中实现了一种新颖的奖励界限。

    

    我们研究了在赌徒反馈下利用专家建议进行预测的经典问题。我们的模型假设一种行动，即学习者放弃参与游戏，在每次试验中都没有奖励或损失。我们提出了CBA算法，利用这一假设获得了可以显著改进经典Exp4算法的奖励界限。我们可以将我们的问题视为在学习者有放弃参与游戏选项时对置信评级预测器进行聚合。重要的是，我们是第一个对一般置信评级预测器的预期累积奖励实现界限的研究者。在专家案例中，我们实现了一种新颖的奖励界限，显著改进了之前在专家Exp（将弃权视为另一种行动）的边界。作为一个示例应用，我们讨论了在有限度量空间中学习球的并集。在这个上下文设置中，我们设计了CBA的有效实现，re

    arXiv:2402.14585v1 Announce Type: new  Abstract: We study the classic problem of prediction with expert advice under bandit feedback. Our model assumes that one action, corresponding to the learner's abstention from play, has no reward or loss on every trial. We propose the CBA algorithm, which exploits this assumption to obtain reward bounds that can significantly improve those of the classical Exp4 algorithm. We can view our problem as the aggregation of confidence-rated predictors when the learner has the option of abstention from play. Importantly, we are the first to achieve bounds on the expected cumulative reward for general confidence-rated predictors. In the special case of specialists we achieve a novel reward bound, significantly improving previous bounds of SpecialistExp (treating abstention as another action). As an example application, we discuss learning unions of balls in a finite metric space. In this contextual setting, we devise an efficient implementation of CBA, re
    
[^49]: 通过覆盖感知和强化学习增强高清地图更新服务

    Enhancement of High-definition Map Update Service Through Coverage-aware and Reinforcement Learning

    [https://arxiv.org/abs/2402.14582](https://arxiv.org/abs/2402.14582)

    本文提出了一种Q学习覆盖时间感知算法，以优化车载网络和HD地图更新的服务质量，以克服网络拥塞。

    

    高清（HD）地图系统将在提升自动驾驶到更高水平中发挥关键作用，得益于相比传统二维地图的显著改进。创建HD地图需要大量的路面和非路面数据。通常，这些原始数据集通过车载网络收集并上传到基于云的HD地图服务提供商。然而，由于动态拓扑，通过车载无线通道传输原始数据存在一定挑战。随着车辆数量的增加，对服务质量产生不利影响，成为协同驾驶中实时HD地图系统在自动驾驶车辆中的障碍。本文提出了一种Q学习覆盖时间感知算法，以优化车载网络和HD地图更新的服务质量，以克服网络拥塞。该算法在一个模拟环境中进行评估，模拟了

    arXiv:2402.14582v1 Announce Type: cross  Abstract: High-definition (HD) Map systems will play a pivotal role in advancing autonomous driving to a higher level, thanks to the significant improvement over traditional two-dimensional (2D) maps. Creating an HD Map requires a huge amount of on-road and off-road data. Typically, these raw datasets are collected and uploaded to cloud-based HD map service providers through vehicular networks. Nevertheless, there are challenges in transmitting the raw data over vehicular wireless channels due to the dynamic topology. As the number of vehicles increases, there is a detrimental impact on service quality, which acts as a barrier to a real-time HD Map system for collaborative driving in Autonomous Vehicles (AV). In this paper, to overcome network congestion, a Q-learning coverage-time-awareness algorithm is presented to optimize the quality of service for vehicular networks and HD map updates. The algorithm is evaluated in an environment that imita
    
[^50]: 使用多模态Transformer在科学图表中进行文本角色分类

    Text Role Classification in Scientific Charts Using Multimodal Transformers

    [https://arxiv.org/abs/2402.14579](https://arxiv.org/abs/2402.14579)

    该研究提出了使用多模态Transformer在科学图表中进行文本角色分类的方法，并在实验中发现LayoutLMv3在性能上优于UDOP，最高达到了82.87的F1-macro分数。

    

    文本角色分类涉及对科学图表中的文本元素的语义角色进行分类。对于这一任务，我们提出在图表数据集上对两个预训练的多模态文档布局分析模型LayoutLMv3和UDOP进行微调。这些Transformer利用文本、图像和布局三种模态作为输入。我们进一步探讨数据增强和平衡方法是否有助于模型的性能。这些模型在各种图表数据集上进行评估，结果显示LayoutLMv3在所有实验中均优于UDOP。LayoutLMv3在ICPR22测试数据集上实现了82.87的最高F1-macro分数，超过了ICPR22 CHART-Infographics挑战中表现最好的模型。此外，模型的鲁棒性在一个合成的嘈杂数据集ICPR22-N上进行了测试。最后，我们评估了模型在三个图表数据集CHIME-R、DeGruyter和EconBiz上的泛化性能，我们为这些数据集添加了标签。

    arXiv:2402.14579v1 Announce Type: cross  Abstract: Text role classification involves classifying the semantic role of textual elements within scientific charts. For this task, we propose to finetune two pretrained multimodal document layout analysis models, LayoutLMv3 and UDOP, on chart datasets. The transformers utilize the three modalities of text, image, and layout as input. We further investigate whether data augmentation and balancing methods help the performance of the models. The models are evaluated on various chart datasets, and results show that LayoutLMv3 outperforms UDOP in all experiments. LayoutLMv3 achieves the highest F1-macro score of 82.87 on the ICPR22 test dataset, beating the best-performing model from the ICPR22 CHART-Infographics challenge. Moreover, the robustness of the models is tested on a synthetic noisy dataset ICPR22-N. Finally, the generalizability of the models is evaluated on three chart datasets, CHIME-R, DeGruyter, and EconBiz, for which we added labe
    
[^51]: 用于层次预测的多元在线线性回归

    Multivariate Online Linear Regression for Hierarchical Forecasting

    [https://arxiv.org/abs/2402.14578](https://arxiv.org/abs/2402.14578)

    提出了MultiVAW方法，将Vovk-Azoury-Warmuth算法扩展到多元设置，同时应用于在线层次预测问题，并且能够放宽传统分析所做的假设

    

    在本文中，我们考虑了一种确定性的在线多元线性回归模型，其中允许响应是多元的。为了解决这个问题，我们引入了MultiVAW，一种将著名的Vovk-Azoury-Warmuth算法扩展到多元设置的方法，并表明它在时间上也具有对数遗憾。我们将我们的结果应用于在线层次预测问题，并将这个文献中的一个算法作为一种特殊情况加以恢复，从而放宽了通常用于其分析的假设。

    arXiv:2402.14578v1 Announce Type: cross  Abstract: In this paper, we consider a deterministic online linear regression model where we allow the responses to be multivariate. To address this problem, we introduce MultiVAW, a method that extends the well-known Vovk-Azoury-Warmuth algorithm to the multivariate setting, and show that it also enjoys logarithmic regret in time. We apply our results to the online hierarchical forecasting problem and recover an algorithm from this literature as a special case, allowing us to relax the hypotheses usually made for its analysis.
    
[^52]: 基于深度强化学习和迁移学习的边缘缓存

    Edge Caching Based on Deep Reinforcement Learning and Transfer Learning

    [https://arxiv.org/abs/2402.14576](https://arxiv.org/abs/2402.14576)

    本文提出了一种基于双深度Q学习的缓存方法，通过半马尔可夫决策过程（SMDP）适应现实场景中随机请求到达的特性，综合考虑各种文件特征。

    

    本文讨论了网络中冗余数据传输日益挑战的问题。流量激增已经使中继链路和骨干网络承压，促使对边缘路由器的缓存解决方案进行探索。现有工作主要依赖于马尔可夫决策过程（MDP）处理缓存问题，假设固定时间间隔的决策；然而，现实场景涉及随机请求到达，尽管各种文件特征在确定最佳缓存策略方面起着至关重要的作用，但相关的现有工作并未考虑所有这些文件特征来形成缓存策略。在本文中，首先我们利用半马尔可夫决策过程（SMDP）来建模缓存问题，以适应现实场景的连续时间特性，允许在文件请求时随机进行缓存决策。然后，我们提出了一种基于双深度Q学习的缓存方法，全面考虑了不同文件特征的影响。

    arXiv:2402.14576v1 Announce Type: cross  Abstract: This paper addresses the escalating challenge of redundant data transmission in networks. The surge in traffic has strained backhaul links and backbone networks, prompting the exploration of caching solutions at the edge router. Existing work primarily relies on Markov Decision Processes (MDP) for caching issues, assuming fixed-time interval decisions; however, real-world scenarios involve random request arrivals, and despite the critical role of various file characteristics in determining an optimal caching policy, none of the related existing work considers all these file characteristics in forming a caching policy. In this paper, first, we formulate the caching problem using a semi-Markov Decision Process (SMDP) to accommodate the continuous-time nature of real-world scenarios allowing for caching decisions at random times upon file requests. Then, we propose a double deep Q-learning-based caching approach that comprehensively accou
    
[^53]: CLCE：一种优化学习融合的改进交叉熵和对比学习方法

    CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for Optimized Learning Fusion

    [https://arxiv.org/abs/2402.14551](https://arxiv.org/abs/2402.14551)

    CLCE方法结合了标签感知对比学习与交叉熵损失，通过协同利用难例挖掘提高了性能表现

    

    最先进的预训练图像模型主要采用两阶段方法：在大规模数据集上进行初始无监督预训练，然后使用交叉熵损失（CE）进行特定任务的微调。然而，已经证明CE可能会损害模型的泛化性和稳定性。为了解决这些问题，我们引入了一种名为CLCE的新方法，该方法将标签感知对比学习与CE相结合。我们的方法不仅保持了两种损失函数的优势，而且以协同方式利用难例挖掘来增强性能。

    arXiv:2402.14551v1 Announce Type: cross  Abstract: State-of-the-art pre-trained image models predominantly adopt a two-stage approach: initial unsupervised pre-training on large-scale datasets followed by task-specific fine-tuning using Cross-Entropy loss~(CE). However, it has been demonstrated that CE can compromise model generalization and stability. While recent works employing contrastive learning address some of these limitations by enhancing the quality of embeddings and producing better decision boundaries, they often overlook the importance of hard negative mining and rely on resource intensive and slow training using large sample batches. To counter these issues, we introduce a novel approach named CLCE, which integrates Label-Aware Contrastive Learning with CE. Our approach not only maintains the strengths of both loss functions but also leverages hard negative mining in a synergistic way to enhance performance. Experimental results demonstrate that CLCE significantly outperf
    
[^54]: OmniPred：语言模型作为通用回归器

    OmniPred: Language Models as Universal Regressors

    [https://arxiv.org/abs/2402.14547](https://arxiv.org/abs/2402.14547)

    本文提出了OmniPred框架，用于训练语言模型作为通用的端到端回归器，实验证明，在多个任务上训练时，语言模型能够显著优于传统回归模型。

    

    在实验设计的广阔领域中，回归一直是一个强大的工具，可以准确预测系统或模型在给定一组参数的情况下的结果指标，但传统上只限于适用于特定任务的方法。在本文中，我们提出了OmniPred，这是一个用于训练语言模型作为通用端到端回归器的框架，使用来自多样真实世界实验的$(x,y)$评估数据。通过使用源自Google Vizier的数据，这是世界上最大的黑盒优化数据库之一，我们的大量实验表明，仅通过数学参数和值的文本表示，语言模型能够进行非常精确的数值回归，如果有机会训练多个任务，则可以显著优于传统的回归模型。

    arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
    
[^55]: 一种用于具有异方差不确定性的轻量级贝叶斯神经网络变分推断的框架

    A Framework for Variational Inference of Lightweight Bayesian Neural Networks with Heteroscedastic Uncertainties

    [https://arxiv.org/abs/2402.14532](https://arxiv.org/abs/2402.14532)

    提出了一种新框架，通过将异方差Aleatoric和认知方差嵌入到学习BNN参数的方差中，改善了轻量级网络的预测性能。

    

    从贝叶斯神经网络（BNN）中获得异方差预测不确定性对许多应用至关重要。通常，除了预测均值外，异方差Aleatoric不确定性作为BNN的输出进行学习，然而这样做可能需要向网络中添加更多可学习参数。在这项工作中，我们展示了异方差Aleatoric和认知方差均可以嵌入到学习BNN参数的方差中，从而提高轻量级网络的预测性能。通过将这种方法与矩传播方法相结合，我们引入了一个适用于轻量级BNNs的无需取样的变分推断相对简单的框架。

    arXiv:2402.14532v1 Announce Type: new  Abstract: Obtaining heteroscedastic predictive uncertainties from a Bayesian Neural Network (BNN) is vital to many applications. Often, heteroscedastic aleatoric uncertainties are learned as outputs of the BNN in addition to the predictive means, however doing so may necessitate adding more learnable parameters to the network. In this work, we demonstrate that both the heteroscedastic aleatoric and epistemic variance can be embedded into the variances of learned BNN parameters, improving predictive performance for lightweight networks. By complementing this approach with a moment propagation approach to inference, we introduce a relatively simple framework for sampling-free variational inference suitable for lightweight BNNs.
    
[^56]: ACE：具有因果感知熵正则化的离策略演员-评论家算法

    ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization

    [https://arxiv.org/abs/2402.14528](https://arxiv.org/abs/2402.14528)

    该论文提出了ACE算法，通过引入因果感知熵正则化，有效评估不同行为的重要性，并分析梯度休眠现象，引入休眠引导复位机制，在多个连续控制任务中取得显著性能优势。

    

    先前的无模型强化学习算法忽视了策略学习过程中不同原始行为的变化重要性。利用这一观点，我们探讨了不同动作维度和奖励之间的因果关系，以评估训练过程中各种原始行为的重要性。我们引入了一种因果感知熵项，有效地识别并优先处理具有高潜在影响的行动，以实现有效的探索。此外，为了防止对特定原始行为过度关注，我们分析了梯度休眠现象，并引入了一种休眠引导复位机制，进一步增强了我们的方法的功效。我们提出的算法ACE：具有因果感知熵正则化的离策演员-评论家，在跨7个领域的29个不同连续控制任务中，相较于无模型强化学习基线，表现出显著的性能优势。

    arXiv:2402.14528v1 Announce Type: cross  Abstract: The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, ACE: Off-policy Actor-critic with Causality-aware Entropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which un
    
[^57]: 基因组学或转录组数据上的联邦学习：模型质量和性能权衡

    Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs

    [https://arxiv.org/abs/2402.14527](https://arxiv.org/abs/2402.14527)

    本文研究了基因组学或转录组数据上的联邦学习，使用 TensorFlow Federated 和 Flower 框架进行实验，以培训疾病预后和细胞类型分类模型。

    

    在大规模基因组学或转录组数据上进行机器学习对许多新颖的健康应用至关重要。例如，精准医学可以根据个体生物标志物、细胞和分子状态等个体信息来量身定制医学治疗。然而，所需数据敏感、庞大、异质，并且通常分布在无法使用专门的机器学习硬件的地点。由于隐私和监管原因，在可信任的第三方处聚合所有数据也存在问题。联邦学习是这一困境的一个有前途的解决方案，因为它实现了在不交换原始数据的情况下进行分散、协作的机器学习。在本文中，我们使用联邦学习框架 TensorFlow Federated 和 Flower 进行比较实验。我们的测试案例是培训疾病预后和细胞类型分类模型。我们使用分布式转录组对模型进行训练

    arXiv:2402.14527v1 Announce Type: new  Abstract: Machine learning on large-scale genomic or transcriptomic data is important for many novel health applications. For example, precision medicine tailors medical treatments to patients on the basis of individual biomarkers, cellular and molecular states, etc. However, the data required is sensitive, voluminous, heterogeneous, and typically distributed across locations where dedicated machine learning hardware is not available. Due to privacy and regulatory reasons, it is also problematic to aggregate all data at a trusted third party.Federated learning is a promising solution to this dilemma, because it enables decentralized, collaborative machine learning without exchanging raw data. In this paper, we perform comparative experiments with the federated learning frameworks TensorFlow Federated and Flower. Our test case is the training of disease prognosis and cell type classification models. We train the models with distributed transcriptom
    
[^58]: 受运动约束的类人双手机器人对人类的交接

    Kinematically Constrained Human-like Bimanual Robot-to-Human Handovers

    [https://arxiv.org/abs/2402.14525](https://arxiv.org/abs/2402.14525)

    该论文提出了一个框架，利用隐马尔可夫模型生成受运动约束的类人双手机器人动作，以实现流畅自然的机器人对人类物体交接。

    

    双手交接对于传输大型、易变形或易损坏的物体至关重要。本文提出了一个框架，用于生成受运动约束的类人双手机器人动作，以确保机器人对人类物体交接的流畅和自然。我们使用隐马尔可夫模型（HSMM）根据观察到的人类伙伴的动作反应性地生成合适的响应轨迹。这些轨迹通过任务空间约束进行调整，以确保准确的交接。一项试点研究的结果显示，我们的方法被认为更类似于人类，与基线逆运动学方法相比。

    arXiv:2402.14525v1 Announce Type: cross  Abstract: Bimanual handovers are crucial for transferring large, deformable or delicate objects. This paper proposes a framework for generating kinematically constrained human-like bimanual robot motions to ensure seamless and natural robot-to-human object handovers. We use a Hidden Semi-Markov Model (HSMM) to reactively generate suitable response trajectories for a robot based on the observed human partner's motion. The trajectories are adapted with task space constraints to ensure accurate handovers. Results from a pilot study show that our approach is perceived as more human--like compared to a baseline Inverse Kinematics approach.
    
[^59]: 跨越多个模型的统一任务嵌入：弥合基于提示的大型语言模型及其它模型的差距

    Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond

    [https://arxiv.org/abs/2402.14522](https://arxiv.org/abs/2402.14522)

    提出了一种框架用于统一不同模型的任务嵌入，使得任务嵌入可以跨越各种模型，并在单一向量空间内进行比较和分析。

    

    任务嵌入是一种捕捉任务特定信息的元学习技术，已经变得流行起来，特别是在多任务学习、模型编辑和可解释性等领域。文章提出了一种名为统一任务嵌入（FUTE）的框架，该框架能够协调来自各种模型（包括较小的语言模型和具有不同提示的LLMs）的任务嵌入，使其处于单一向量空间。这种统一性使得可以比较和分析不同模型之间的相似性，扩展了现有任务嵌入方法在解决多模型应用中的范围和效用。

    arXiv:2402.14522v1 Announce Type: new  Abstract: Task embedding, a meta-learning technique that captures task-specific information, has become prevalent, especially in areas such as multi-task learning, model editing, and interpretability. However, it faces challenges with the emergence of prompt-guided Large Language Models (LLMs) operating in a gradientfree manner. Existing task embedding methods rely on fine-tuned, task-specific language models, which hinders the adaptability of task embeddings across diverse models, especially prompt-based LLMs. To unleash the power of task embedding in the era of LLMs, we propose a framework for unified task embeddings (FUTE), harmonizing task embeddings from various models, including smaller language models and LLMs with varied prompts, within a single vector space. Such uniformity enables the comparison and analysis of similarities amongst different models, extending the scope and utility of existing task embedding methods in addressing multi-mo
    
[^60]: 量子神经网络频谱的光谱不变性和极大性质

    Spectral invariance and maximality properties of the frequency spectrum of quantum neural networks

    [https://arxiv.org/abs/2402.14515](https://arxiv.org/abs/2402.14515)

    量子神经网络研究了频谱的极大性质，证明了在一类模型中存在极大结果，以及在一些条件下存在保持频谱的光谱不变性，解释了文献中观察到的结果对称性。

    

    量子神经网络（QNNs）是量子机器学习领域的热门方法，由于其与变分量子电路的密切联系，使其成为在噪声中间尺度量子（NISQ）设备上进行实际应用的有前途的候选方法。QNN可以表示为有限傅里叶级数，其中频率集被称为频谱。我们分析了这个频谱并证明，对于一大类模型，存在各种极大性结果。此外，我们证明在一些温和条件下，存在一个保持频谱的具有相同面积$A = RL$的模型类之间的双射，其中$R$表示量子比特数量，$L$表示层数，我们因此称之为面积保持变换下的光谱不变性。通过这个，我们解释了文献中经常观察到的在结果中$R$和$L$的对称性，并展示了最大频谱的依赖性

    arXiv:2402.14515v1 Announce Type: cross  Abstract: Quantum Neural Networks (QNNs) are a popular approach in Quantum Machine Learning due to their close connection to Variational Quantum Circuits, making them a promising candidate for practical applications on Noisy Intermediate-Scale Quantum (NISQ) devices. A QNN can be expressed as a finite Fourier series, where the set of frequencies is called the frequency spectrum. We analyse this frequency spectrum and prove, for a large class of models, various maximality results. Furthermore, we prove that under some mild conditions there exists a bijection between classes of models with the same area $A = RL$ that preserves the frequency spectrum, where $R$ denotes the number of qubits and $L$ the number of layers, which we consequently call spectral invariance under area-preserving transformations. With this we explain the symmetry in $R$ and $L$ in the results often observed in the literature and show that the maximal frequency spectrum depen
    
[^61]: 使用Equilibrium K-Means进行不平衡数据聚类

    Imbalanced Data Clustering using Equilibrium K-Means

    [https://arxiv.org/abs/2402.14490](https://arxiv.org/abs/2402.14490)

    Equilibrium K-Means（EKM）是一种新颖且简单的K均值类型算法，通过减少聚类中心在大类簇中心聚集的倾向，显著改善了不平衡数据的聚类结果。

    

    不平衡数据指的是数据点在不同类别之间分布不均衡，这给传统的硬聚类算法和模糊聚类算法（如硬K均值（HKM，或者Lloyd算法）和模糊K均值（FKM，或者Bezdek算法））带来了挑战。本文介绍了一种新颖且简单的K均值类型算法——Equilibrium K-Means（EKM），它在两个步骤之间交替进行，显著改善了不平衡数据的聚类结果，减少了聚类中心向大类簇中心聚集的倾向。我们还提出了对HKM、FKM和EKM的统一视角，表明它们本质上是具有明确关系的牛顿方法的梯度下降算法。EKM具有与FKM相同的时间和空间复杂度，但对其成员定义提供了更清晰的物理意义。我们在两个合成数据集和十个真实数据集上展示了EKM的性能，并将其与各种聚类算法进行了比较。

    arXiv:2402.14490v1 Announce Type: new  Abstract: Imbalanced data, characterized by an unequal distribution of data points across different clusters, poses a challenge for traditional hard and fuzzy clustering algorithms, such as hard K-means (HKM, or Lloyd's algorithm) and fuzzy K-means (FKM, or Bezdek's algorithm). This paper introduces equilibrium K-means (EKM), a novel and simple K-means-type algorithm that alternates between just two steps, yielding significantly improved clustering results for imbalanced data by reducing the tendency of centroids to crowd together in the center of large clusters. We also present a unifying perspective for HKM, FKM, and EKM, showing they are essentially gradient descent algorithms with an explicit relationship to Newton's method. EKM has the same time and space complexity as FKM but offers a clearer physical meaning for its membership definition. We illustrate the performance of EKM on two synthetic and ten real datasets, comparing it to various cl
    
[^62]: 一类用于快速比较持久图的拓扑伪距离

    A Class of Topological Pseudodistances for Fast Comparison of Persistence Diagrams

    [https://arxiv.org/abs/2402.14489](https://arxiv.org/abs/2402.14489)

    本文介绍了一类称为扩展拓扑伪距离（ETD）的伪距离，具有可调节的复杂度，可以近似切片。

    

    持久图在拓扑数据分析中起着核心作用，并在越来越多的应用中被使用。持久图数据的比较需要计算大量持久图之间的比较度量，这些度量需要既准确又理论上可靠，同时计算速度也要快。尤其是对于更密集的多维持久图，这样的比较度量尚未出现。本文介绍了一类伪距离，称为扩展拓扑伪距离（ETD），具有可调节的复杂度，可以近似切片...

    arXiv:2402.14489v1 Announce Type: cross  Abstract: Persistence diagrams (PD)s play a central role in topological data analysis, and are used in an ever increasing variety of applications. The comparison of PD data requires computing comparison metrics among large sets of PDs, with metrics which are accurate, theoretically sound, and fast to compute. Especially for denser multi-dimensional PDs, such comparison metrics are lacking. While on the one hand, Wasserstein-type distances have high accuracy and theoretical guarantees, they incur high computational cost. On the other hand, distances between vectorizations such as Persistence Statistics (PS)s have lower computational cost, but lack the accuracy guarantees and in general they are not guaranteed to distinguish PDs (i.e. the two PS vectors of different PDs may be equal). In this work we introduce a class of pseudodistances called Extended Topological Pseudodistances (ETD)s, which have tunable complexity, and can approximate Sliced an
    
[^63]: 边界合同是否可学习并近似最优?

    Are Bounded Contracts Learnable and Approximately Optimal?

    [https://arxiv.org/abs/2402.14486](https://arxiv.org/abs/2402.14486)

    分析了在隐藏动作模型下的合同与委托-代理问题，提出了两个学习算法可以找到几乎最优的有界合同，对于一般情况的查询次数具有多项式上界，并且直接学习潜在的结果分布。

    

    本文考虑委托-代理问题的隐藏动作模型，其中委托方通过合同激励代理人按合同开展项目。我们研究了带有有界支付的合同是否可学习并近似最优。我们的主要结果是两种学习算法，可以在有界的查询次数内找到几乎最优的有界合同，基于文献中的两个标准假设：代理人的更昂贵的行动导致委托方的更好的结果分布，并且代理人的成本/努力具有递减回报。我们的多项式查询复杂度上界表明，标准假设足以实现对一般情况已知下界的指数改进。与现有的算法不同，后者依赖于对合同空间的离散化，我们的算法直接学习潜在的结果分布。

    arXiv:2402.14486v1 Announce Type: cross  Abstract: This paper considers the hidden-action model of the principal-agent problem, in which a principal incentivizes an agent to work on a project using a contract. We investigate whether contracts with bounded payments are learnable and approximately optimal. Our main results are two learning algorithms that can find a nearly optimal bounded contract using a polynomial number of queries, under two standard assumptions in the literature: a costlier action for the agent leads to a better outcome distribution for the principal, and the agent's cost/effort has diminishing returns. Our polynomial query complexity upper bound shows that standard assumptions are sufficient for achieving an exponential improvement upon the known lower bound for general instances. Unlike the existing algorithms, which relied on discretizing the contract space, our algorithms directly learn the underlying outcome distributions. As for the approximate optimality of bo
    
[^64]: SpanSeq：用于改进深度学习项目开发和评估的基于相似度的序列数据拆分方法

    SpanSeq: Similarity-based sequence data splitting method for improved development and assessment of deep learning projects

    [https://arxiv.org/abs/2402.14482](https://arxiv.org/abs/2402.14482)

    SpanSeq 是一种用于生物数据序列的数据库分区方法，能够避免训练集和测试集之间的数据泄漏。

    

    过去几年中，在计算生物学中使用深度学习模型的增加很大，并且随着诸如自然语言处理等领域的当前进展，预计将进一步增加。本文提出了SpanSeq，这是一种适用于大多数生物序列（基因、蛋白质和基因组）的机器学习数据库分区方法，旨在避免数据集之间的数据泄漏。

    arXiv:2402.14482v1 Announce Type: new  Abstract: The use of deep learning models in computational biology has increased massively in recent years, and is expected to do so further with the current advances in fields like Natural Language Processing. These models, although able to draw complex relations between input and target, are also largely inclined to learn noisy deviations from the pool of data used during their development. In order to assess their performance on unseen data (their capacity to generalize), it is common to randomly split the available data in development (train/validation) and test sets. This procedure, although standard, has lately been shown to produce dubious assessments of generalization due to the existing similarity between samples in the databases used. In this work, we present SpanSeq, a database partition method for machine learning that can scale to most biological sequences (genes, proteins and genomes) in order to avoid data leakage between sets. We a
    
[^65]: 朝向自动因果推断：基于5G电信数据的案例研究

    Towards Automated Causal Discovery: a case study on 5G telecommunication data

    [https://arxiv.org/abs/2402.14481](https://arxiv.org/abs/2402.14481)

    该论文介绍了自动因果发现（AutoCD）的概念，提出了可以完全自动化应用因果发现和推理方法的系统，并展示了其在合成数据集和5G电信数据上的性能。

    

    我们介绍了自动因果发现（AutoCD）的概念，定义为旨在完全自动化因果发现和因果推理方法应用的任何系统。 AutoCD的目标是提供专家人类分析师会提供的所有因果信息，并回答用户的因果查询。我们描述了这样一个平台的架构，并展示了它在合成数据集上的性能。作为一个案例研究，我们将其应用于时间电信数据。该系统是通用的，可以应用于大量因果发现问题。

    arXiv:2402.14481v1 Announce Type: new  Abstract: We introduce the concept of Automated Causal Discovery (AutoCD), defined as any system that aims to fully automate the application of causal discovery and causal reasoning methods. AutoCD's goal is to deliver all causal information that an expert human analyst would and answer a user's causal queries. We describe the architecture of such a platform, and illustrate its performance on synthetic data sets. As a case study, we apply it on temporal telecommunication data. The system is general and can be applied to a plethora of causal discovery problems.
    
[^66]: DynGMA：一种从数据学习随机微分方程的稳健方法

    DynGMA: a robust approach for learning stochastic differential equations from data

    [https://arxiv.org/abs/2402.14475](https://arxiv.org/abs/2402.14475)

    DynGMA方法通过引入新的密度近似，优于基准方法在学习完全未知的漂移和扩散函数以及计算不变性方面的准确性。

    

    从观测数据中学习未知的随机微分方程（SDEs）是一项重要且具有挑战性的任务，应用于各个领域。本文引入了新的近似参数化SDE转移密度的方法：受动力系统随机摄动理论启发的高斯密度近似，以及它的扩展，动力高斯混合近似（DynGMA）。受益于稳健的密度近似，我们的方法在学习完全未知的漂移和扩散函数以及计算矩不变性方面表现出优越的准确性。

    arXiv:2402.14475v1 Announce Type: new  Abstract: Learning unknown stochastic differential equations (SDEs) from observed data is a significant and challenging task with applications in various fields. Current approaches often use neural networks to represent drift and diffusion functions, and construct likelihood-based loss by approximating the transition density to train these networks. However, these methods often rely on one-step stochastic numerical schemes, necessitating data with sufficiently high time resolution. In this paper, we introduce novel approximations to the transition density of the parameterized SDE: a Gaussian density approximation inspired by the random perturbation theory of dynamical systems, and its extension, the dynamical Gaussian mixture approximation (DynGMA). Benefiting from the robust density approximation, our method exhibits superior accuracy compared to baseline methods in learning the fully unknown drift and diffusion functions and computing the invari
    
[^67]: 使用LLMs和可解释模型的数据科学

    Data Science with LLMs and Interpretable Models

    [https://arxiv.org/abs/2402.14474](https://arxiv.org/abs/2402.14474)

    该研究展示了大型语言模型LLMs在描述、解释和调试广义加性模型GAMs方面的出色表现，结合LLMs的灵活性和GAMs准确描述的统计模式，可以实现数据集摘要、问答和模型评估。

    

    最近几年，在构建可解释模型方面取得了重要进展，这些机器学习模型旨在被人类轻松理解。在这项工作中，我们展示了大型语言模型(LLMs)在使用可解释模型方面表现出色。特别是，我们展示了LLMs可以描述、解释和调试广义加性模型(GAMs)。将LLMs的灵活性与GAMs准确描述的广泛统计模式相结合，能够实现数据集摘要、回答问题和模型评估。LLMs还可以改善领域专家与可解释模型之间的交互，并为潜在现象生成假设。我们发布了一个开源LLM-GAM接口\url{https://github.com/interpretml/TalkToEBM}。

    arXiv:2402.14474v1 Announce Type: cross  Abstract: Recent years have seen important advances in the building of interpretable models, machine learning models that are designed to be easily understood by humans. In this work, we show that large language models (LLMs) are remarkably good at working with interpretable models, too. In particular, we show that LLMs can describe, interpret, and debug Generalized Additive Models (GAMs). Combining the flexibility of LLMs with the breadth of statistical patterns accurately described by GAMs enables dataset summarization, question answering, and model critique. LLMs can also improve the interaction between domain experts and interpretable models, and generate hypotheses about the underlying phenomenon. We release \url{https://github.com/interpretml/TalkToEBM} as an open-source LLM-GAM interface.
    
[^68]: 重新构想异常：如果异常是正常的呢？

    Reimagining Anomalies: What If Anomalies Were Normal?

    [https://arxiv.org/abs/2402.14469](https://arxiv.org/abs/2402.14469)

    方法提出了一种新颖的解释方法，生成多个反事实示例以捕获异常的多样概念，为用户提供对触发异常检测器机制的高级语义解释，允许探索“假设情景”。

    

    基于深度学习的方法在图像异常检测方面取得了突破，但其复杂性给理解为何实例被预测为异常带来了相当大的挑战。我们引入了一种新颖的解释方法，为每个异常生成多个反事实示例，捕获异常的多样概念。反事实示例是对异常的修改，被异常检测器视为正常。该方法提供了触发异常检测器机制的高级语义解释，允许用户探索“假设情景”。对不同图像数据集进行的定性和定量分析显示，该方法应用于最先进的异常检测器可以实现对检测器的高质量语义解释。

    arXiv:2402.14469v1 Announce Type: cross  Abstract: Deep learning-based methods have achieved a breakthrough in image anomaly detection, but their complexity introduces a considerable challenge to understanding why an instance is predicted to be anomalous. We introduce a novel explanation method that generates multiple counterfactual examples for each anomaly, capturing diverse concepts of anomalousness. A counterfactual example is a modification of the anomaly that is perceived as normal by the anomaly detector. The method provides a high-level semantic explanation of the mechanism that triggered the anomaly detector, allowing users to explore "what-if scenarios." Qualitative and quantitative analyses across various image datasets show that the method applied to state-of-the-art anomaly detectors can achieve high-quality semantic explanations of detectors.
    
[^69]: 机器学习揭示了波西多尼亚海草对地中海水域的大规模影响

    Machine Learning Reveals Large-scale Impact of Posidonia Oceanica on Mediterranean Sea Water

    [https://arxiv.org/abs/2402.14459](https://arxiv.org/abs/2402.14459)

    该研究利用机器学习揭示了波西多尼亚海草在地中海水域中对水生物地球化学特性的影响，强调了其在海洋生态系统中的关键作用。

    

    波西多尼亚海草是地中海的一种受保护的特有海草，促进生物多样性，储存碳，释放氧气，并为许多海洋生物提供栖息地。借助增强研究，我们收集了来自不同数据源的174个特征的全面数据集。通过机器学习分析，我们发现波西多尼亚海草的确切位置与水体生物地球化学特性之间存在稳固的相关性。模型的特征重要性显示，与碳相关的变量，如净生物量生产和二氧化碳下降表面质量通量，在存在波西多尼亚海草的区域中其值发生改变，从而可用于间接定位波西多尼亚海草草地。该研究提供了植物对环境产生全球影响的证据，并强调了这种植物在海洋生态系统中的关键作用，强调了对其保护的必要性。

    arXiv:2402.14459v1 Announce Type: cross  Abstract: Posidonia oceanica is a protected endemic seagrass of Mediterranean sea that fosters biodiversity, stores carbon, releases oxygen, and provides habitat to numerous sea organisms. Leveraging augmented research, we collected a comprehensive dataset of 174 features compiled from diverse data sources. Through machine learning analysis, we discovered the existence of a robust correlation between the exact location of P. oceanica and water biogeochemical properties. The model's feature importance, showed that carbon-related variables as net biomass production and downward surface mass flux of carbon dioxide have their values altered in the areas with P. oceanica, which in turn can be used for indirect location of P. oceanica meadows. The study provides the evidence of the plant's ability to exert a global impact on the environment and underscores the crucial role of this plant in sea ecosystems, emphasizing the need for its conservation and 
    
[^70]: 基于模型的强化学习对反应扩散问题的控制

    Model-Based Reinforcement Learning Control of Reaction-Diffusion Problems

    [https://arxiv.org/abs/2402.14446](https://arxiv.org/abs/2402.14446)

    本文探索了使用自动控制策略处理热和疾病传输初始边界值问题，并通过引入新的奖励函数和改进的强化学习算法来驱动传输场的流动。

    

    数学和计算工具在决策过程中被证明是可靠的。特别是最近，基于机器学习的方法作为高级支持工具变得越来越受欢迎。处理控制问题时，强化学习已经被应用于多个应用中的决策过程，尤其是在游戏中。这些方法在解决复杂问题方面的成功激发了对能够克服当前困难的新领域的探索。在本文中，我们探讨了自动控制策略在热和疾病传输初始边界值问题中的应用。具体地，在这项工作中，我们使用随机策略梯度方法调整了现有的强化学习算法，并引入了两种新的奖励函数来驱动传输场的流动。新的基于模型的框架利用了反应传输

    arXiv:2402.14446v1 Announce Type: cross  Abstract: Mathematical and computational tools have proven to be reliable in decision-making processes. In recent times, in particular, machine learning-based methods are becoming increasingly popular as advanced support tools. When dealing with control problems, reinforcement learning has been applied to decision-making in several applications, most notably in games. The success of these methods in finding solutions to complex problems motivates the exploration of new areas where they can be employed to overcome current difficulties. In this paper, we explore the use of automatic control strategies to initial boundary value problems in thermal and disease transport. Specifically, in this work, we adapt an existing reinforcement learning algorithm using a stochastic policy gradient method and we introduce two novel reward functions to drive the flow of the transported field. The new model-based framework exploits the interactions between a react
    
[^71]: 并行中点随机化的 Langevin Monte Carlo

    Parallelized Midpoint Randomization for Langevin Monte Carlo

    [https://arxiv.org/abs/2402.14434](https://arxiv.org/abs/2402.14434)

    探索在能够进行梯度平行评估的框架中的抽样问题，提出了并行化的随机中点方法，并通过新技术导出了对抽样和目标密度之间Wasserstein距离的上界，量化了并行处理单元带来的运行时改进。

    

    我们探讨了在可以进行梯度的平行评估的框架中的抽样问题。我们的研究重点放在由平滑和强log-凹密度表征的目标分布上。我们重新审视了并行化的随机中点方法，并运用最近开发用于分析其纯顺序版本的证明技术。利用这些技术，我们得出了抽样和目标密度之间的Wasserstein距离的上界。这些界限量化了通过利用并行处理单元所实现的运行时改进，这可能是相当可观的。

    arXiv:2402.14434v1 Announce Type: cross  Abstract: We explore the sampling problem within the framework where parallel evaluations of the gradient of the log-density are feasible. Our investigation focuses on target distributions characterized by smooth and strongly log-concave densities. We revisit the parallelized randomized midpoint method and employ proof techniques recently developed for analyzing its purely sequential version. Leveraging these techniques, we derive upper bounds on the Wasserstein distance between the sampling and target densities. These bounds quantify the runtime improvement achieved by utilizing parallel processing units, which can be considerable.
    
[^72]: 具有极端标签不足的联邦模型鲁棒训练

    Robust Training of Federated Models with Extremely Label Deficiency

    [https://arxiv.org/abs/2402.14430](https://arxiv.org/abs/2402.14430)

    提出了一种名为Twin-sight的双模型范式，以增强联邦半监督学习中标签和无标签数据之间的互动，并通过引入邻域保持约束来提高这两个模型之间的协同作用

    

    联邦半监督学习（FSSL）已经成为一种强大的范式，用于协作训练具有标签不足的分布式数据的机器学习模型。鲁棒的FSSL方法主要集中于在每个客户端训练单个模型。然而，这种方法可能导致标记数据和无标记数据的目标函数之间存在差异，从而产生梯度冲突。为了减轻梯度冲突，我们提出了一种新颖的双模型范式，称为Twin-sight，旨在通过提供来自标记和未标记数据不同视角的见解来增强相互指导。特别地，Twin-sight同时训练一个具有监督目标函数的监督模型，同时使用无监督目标函数训练一个无监督模型。为了增强这两个模型之间的协同作用，Twin-sight引入了一个保持邻域的约束，促进邻域的保留

    arXiv:2402.14430v1 Announce Type: new  Abstract: Federated semi-supervised learning (FSSL) has emerged as a powerful paradigm for collaboratively training machine learning models using distributed data with label deficiency. Advanced FSSL methods predominantly focus on training a single model on each client. However, this approach could lead to a discrepancy between the objective functions of labeled and unlabeled data, resulting in gradient conflicts. To alleviate gradient conflict, we propose a novel twin-model paradigm, called Twin-sight, designed to enhance mutual guidance by providing insights from different perspectives of labeled and unlabeled data. In particular, Twin-sight concurrently trains a supervised model with a supervised objective function while training an unsupervised model using an unsupervised objective function. To enhance the synergy between these two models, Twin-sight introduces a neighbourhood-preserving constraint, which encourages the preservation of the nei
    
[^73]: 从文本描述生成人类活动的地面压力序列

    Text me the data: Generating Ground Pressure Sequence from Textual Descriptions for HAR

    [https://arxiv.org/abs/2402.14427](https://arxiv.org/abs/2402.14427)

    使用Text-to-Pressure（T2P）框架，结合深度学习技术，从文本描述中生成高质量地面压力序列，实现了文本与生成动作的一致性。

    

    在人类活动识别（HAR）中，为训练高效模型，必须有大量的地面真实数据。然而，通过物理传感器获取地面压力数据本身可能成本过高、耗时。为解决这一关键需求，我们引入了Text-to-Pressure（T2P），这是一个设计用于利用深度学习技术从人类活动的文本描述中生成大量地面压力序列的框架。我们展示了传感器数据的矢量量化与简单文本条件自回归策略的组合，允许我们通过文本描述之间的离散潜在相关性获得高质量生成的压力序列与压力地图。我们在文本与生成动作之间的一致性上取得了可比较的表现，R squared 值为0.722，Masked R squared 值为0.892，FID 分数为1.83。

    arXiv:2402.14427v1 Announce Type: cross  Abstract: In human activity recognition (HAR), the availability of substantial ground truth is necessary for training efficient models. However, acquiring ground pressure data through physical sensors itself can be cost-prohibitive, time-consuming. To address this critical need, we introduce Text-to-Pressure (T2P), a framework designed to generate extensive ground pressure sequences from textual descriptions of human activities using deep learning techniques. We show that the combination of vector quantization of sensor data along with simple text conditioned auto regressive strategy allows us to obtain high-quality generated pressure sequences from textual descriptions with the help of discrete latent correlation between text and pressure maps. We achieved comparable performance on the consistency between text and generated motion with an R squared value of 0.722, Masked R squared value of 0.892, and FID score of 1.83. Additionally, we trained 
    
[^74]: 通过离散扩散进行大规模无动作视频预训练，以实现高效策略学习

    Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning

    [https://arxiv.org/abs/2402.14407](https://arxiv.org/abs/2402.14407)

    利用离散扩散结合生成式预训练和少量机器人视频微调，实现从人类视频到机器人策略学习的知识迁移。

    

    学习一个能够完成多个任务的通用实体代理面临挑战，主要源自缺乏有标记动作的机器人数据集。相比之下，存在大量捕捉复杂任务和与物理世界互动的人类视频。本文介绍了一种新颖框架，利用统一的离散扩散将人类视频上的生成式预训练与少量有标记机器人视频上的策略微调结合起来。我们首先将人类和机器人视频压缩成统一的视频标记。在预训练阶段，我们使用一个带有蒙版替换扩散策略的离散扩散模型来预测潜空间中的未来视频标记。在微调阶段，我们 h

    arXiv:2402.14407v1 Announce Type: new  Abstract: Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we h
    
[^75]: 全局安全顺序学习通过高效知识转移

    Global Safe Sequential Learning via Efficient Knowledge Transfer

    [https://arxiv.org/abs/2402.14402](https://arxiv.org/abs/2402.14402)

    提出了考虑转移安全的全局顺序学习方法，以加速安全学习，并通过预先计算源组件来减少额外的计算负载。

    

    arXiv:2402.14402v1 公告类型: 新摘要: 顺序学习方法例如主动学习和贝叶斯优化选择最具信息量的数据来学习一个任务。在许多医学或工程应用中，数据选择受先验未知的安全条件限制。一条有前途的安全学习方法利用高斯过程（GPs）来建模安全概率，并在具有较高安全置信度的区域中进行数据选择。然而，准确的安全建模需要先验知识或消耗数据。此外，安全置信度集中在给定的观测值周围，导致局部探索。由于在安全关键实验中通常存在可转移的源知识，我们提出考虑转移安全顺序学习来加速安全学习。我们进一步考虑先计算源组件，以减少引入源数据带来的额外计算负载。

    arXiv:2402.14402v1 Announce Type: new  Abstract: Sequential learning methods such as active learning and Bayesian optimization select the most informative data to learn about a task. In many medical or engineering applications, the data selection is constrained by a priori unknown safety conditions. A promissing line of safe learning methods utilize Gaussian processes (GPs) to model the safety probability and perform data selection in areas with high safety confidence. However, accurate safety modeling requires prior knowledge or consumes data. In addition, the safety confidence centers around the given observations which leads to local exploration. As transferable source knowledge is often available in safety critical experiments, we propose to consider transfer safe sequential learning to accelerate the learning of safety. We further consider a pre-computation of source components to reduce the additional computational load that is introduced by incorporating source data. In this pap
    
[^76]: 基于扩散模型的视觉补偿引导和视觉差异分析用于无参考图像质量评估

    Diffusion Model Based Visual Compensation Guidance and Visual Difference Analysis for No-Reference Image Quality Assessment

    [https://arxiv.org/abs/2402.14401](https://arxiv.org/abs/2402.14401)

    本研究将扩散模型引入无参考图像质量评估领域，设计了新的扩散恢复网络，提高了学习高级和低级视觉特征的效率。

    

    现有的自由能引导的无参考图像质量评估(NR-IQA)方法仍然在找到在图像的像素级学习特征信息和捕获高级特征信息之间达到平衡以及高级特征信息的有效利用方面存在困难。作为一种新颖的领先技术(SOTA)生成模型类别，扩散模型展示了建模复杂关系的能力，能够全面理解图像，并具有更好地学习高级和低级视觉特征。鉴于此，我们首次将扩散模型探索到NR-IQA领域。首先，我们设计了一个新的扩散恢复网络，利用生成的增强图像和包含噪声的图像，将扩散模型去噪过程中获得的非线性特征作为高级视觉信息。

    arXiv:2402.14401v1 Announce Type: cross  Abstract: Existing free-energy guided No-Reference Image Quality Assessment (NR-IQA) methods still suffer from finding a balance between learning feature information at the pixel level of the image and capturing high-level feature information and the efficient utilization of the obtained high-level feature information remains a challenge. As a novel class of state-of-the-art (SOTA) generative model, the diffusion model exhibits the capability to model intricate relationships, enabling a comprehensive understanding of images and possessing a better learning of both high-level and low-level visual features. In view of these, we pioneer the exploration of the diffusion model into the domain of NR-IQA. Firstly, we devise a new diffusion restoration network that leverages the produced enhanced image and noise-containing images, incorporating nonlinear features obtained during the denoising process of the diffusion model, as high-level visual informat
    
[^77]: 使用自适应图卷积网络对3D婴儿动力学进行建模

    Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks

    [https://arxiv.org/abs/2402.14400](https://arxiv.org/abs/2402.14400)

    使用数据驱动评估个体动作模式，利用自适应图卷积网络对3D婴儿动力学进行建模，相较于传统机器学习取得了改进。

    

    可靠的婴儿神经发育评估方法对于早期发现可能需要及时干预的医学问题至关重要。自发的运动活动，即“动力学”，被证明可提供一个强有力的预测未来神经发育的替代性测量。然而，它的评估在很大程度上是定性和主观的，侧重于对通过视觉识别的特定年龄手势的描述。在这里，我们采用了一种替代方法，根据数据驱动评估个体动作模式来预测婴儿神经发育成熟。我们利用处理过的3D婴儿视频录像进行姿势估计，提取解剖标志物的时空系列，并应用自适应图卷积网络来预测实际年龄。我们展示了我们的数据驱动方法相对于基于手动设计特征的传统机器学习基线取得了改进。

    arXiv:2402.14400v1 Announce Type: cross  Abstract: Reliable methods for the neurodevelopmental assessment of infants are essential for early detection of medical issues that may need prompt interventions. Spontaneous motor activity, or `kinetics', is shown to provide a powerful surrogate measure of upcoming neurodevelopment. However, its assessment is by and large qualitative and subjective, focusing on visually identified, age-specific gestures. Here, we follow an alternative approach, predicting infants' neurodevelopmental maturation based on data-driven evaluation of individual motor patterns. We utilize 3D video recordings of infants processed with pose-estimation to extract spatio-temporal series of anatomical landmarks, and apply adaptive graph convolutional networks to predict the actual age. We show that our data-driven approach achieves improvement over traditional machine learning baselines based on manually engineered features.
    
[^78]: DP-SGD算法对抗记录层推断的封闭形式界限

    Closed-Form Bounds for DP-SGD against Record-level Inference

    [https://arxiv.org/abs/2402.14397](https://arxiv.org/abs/2402.14397)

    该论文提出了一种新方法，通过封闭形式界限评估机器学习模型在特定记录级威胁下的隐私保护，避免了通过DP进行间接评估。

    

    使用差分隐私（DP）算法（如DP-SGD）训练的机器学习模型对抗各种隐私攻击具有韧性。虽然可以仅基于（ε，δ）-DP保证推导出某些攻击的界限，但有意义的界限需要足够小的隐私预算（即注入大量噪声），这导致效用大幅损失。本文提出了一种新方法，用于评估机器学习模型针对特定记录级威胁（如成员关系和属性推断）的隐私，而无需经过DP的间接连结。我们专注于流行的DP-SGD算法，并推导出简单的闭式界限。我们的证明将DP-SGD建模为一个信息论通道，其输入是攻击者想要推断的秘密（如数据记录的成员关系），输出是迭代优化产生的中间模型参数。

    arXiv:2402.14397v1 Announce Type: cross  Abstract: Machine learning models trained with differentially-private (DP) algorithms such as DP-SGD enjoy resilience against a wide range of privacy attacks. Although it is possible to derive bounds for some attacks based solely on an $(\varepsilon,\delta)$-DP guarantee, meaningful bounds require a small enough privacy budget (i.e., injecting a large amount of noise), which results in a large loss in utility. This paper presents a new approach to evaluate the privacy of machine learning models against specific record-level threats, such as membership and attribute inference, without the indirection through DP. We focus on the popular DP-SGD algorithm, and derive simple closed-form bounds. Our proofs model DP-SGD as an information theoretic channel whose inputs are the secrets that an attacker wants to infer (e.g., membership of a data record) and whose outputs are the intermediate model parameters produced by iterative optimization. We obtain b
    
[^79]: 使用AlphaTensor进行量子电路优化

    Quantum Circuit Optimization with AlphaTensor

    [https://arxiv.org/abs/2402.14396](https://arxiv.org/abs/2402.14396)

    使用基于深度强化学习的AlphaTensor-Quantum方法，在容错量子计算中优化T门数量，显著减少电路的T计数。

    

    实现容错量子计算机的一个关键挑战是电路优化。我们专注于容错量子计算中最昂贵的门（即T门），解决T计数优化问题，即最小化实现给定电路所需的T门数量。为实现这一目标，我们开发了基于深度强化学习的AlphaTensor-Quantum方法，利用优化T计数与张量分解之间的关系。与现有的T计数优化方法不同，AlphaTensor-Quantum能够整合关于量子计算的领域特定知识并利用工具，显著减少了优化电路的T计数。AlphaTensor-Quantum在一组算术基准上优于现有的T计数优化方法（即使在不使用工具的情况下进行比较）。值得注意的是，它发现了一种类似Karat的高效算法。

    arXiv:2402.14396v1 Announce Type: cross  Abstract: A key challenge in realizing fault-tolerant quantum computers is circuit optimization. Focusing on the most expensive gates in fault-tolerant quantum computation (namely, the T gates), we address the problem of T-count optimization, i.e., minimizing the number of T gates that are needed to implement a given circuit. To achieve this, we develop AlphaTensor-Quantum, a method based on deep reinforcement learning that exploits the relationship between optimizing T-count and tensor decomposition. Unlike existing methods for T-count optimization, AlphaTensor-Quantum can incorporate domain-specific knowledge about quantum computation and leverage gadgets, which significantly reduces the T-count of the optimized circuits. AlphaTensor-Quantum outperforms the existing methods for T-count optimization on a set of arithmetic benchmarks (even when compared without making use of gadgets). Remarkably, it discovers an efficient algorithm akin to Karat
    
[^80]: 图解析网络

    Graph Parsing Networks

    [https://arxiv.org/abs/2402.14393](https://arxiv.org/abs/2402.14393)

    本研究提出了一种受自底向上的语法归纳启发的图解析算法，使得Graph Parsing Network（GPN）能够自适应地学习每个独特图的个性化池化结构。

    

    图池化将图信息压缩为一种紧凑的表示。最先进的图池化方法采用分层方法，逐步减小图的大小。这些方法必须在内存效率和保留节点信息之间取得平衡，这取决于它们是否使用节点丢弃或节点聚类。此外，为所有图预定义了固定的池化比例或池化层数，这阻碍了每个单独图形的个性化池化结构。本研究受自底向上的语法归纳启发，提出了一种有效的图解析算法来推断池化结构，然后驱动图池化。由此产生的图解析网络（GPN）能够自适应地学习每个独特图的个性化池化结构。GPN受益于图解析算法生成的离散分配，从而在保留节点信息的同时获得良好的内存效率。

    arXiv:2402.14393v1 Announce Type: new  Abstract: Graph pooling compresses graph information into a compact representation. State-of-the-art graph pooling methods follow a hierarchical approach, which reduces the graph size step-by-step. These methods must balance memory efficiency with preserving node information, depending on whether they use node dropping or node clustering. Additionally, fixed pooling ratios or numbers of pooling layers are predefined for all graphs, which prevents personalized pooling structures from being captured for each individual graph. In this work, inspired by bottom-up grammar induction, we propose an efficient graph parsing algorithm to infer the pooling structure, which then drives graph pooling. The resulting Graph Parsing Network (GPN) adaptively learns personalized pooling structure for each individual graph. GPN benefits from the discrete assignments generated by the graph parsing algorithm, allowing good memory efficiency while preserving node inform
    
[^81]: MAPE-PPI：通过微环境感知蛋白嵌入向有效和高效的蛋白质相互作用预测迈进

    MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding

    [https://arxiv.org/abs/2402.14391](https://arxiv.org/abs/2402.14391)

    通过定义氨基酸残基的微环境，结合蛋白质序列和结构信息，提出了用于高效蛋白质相互作用预测的微环境感知蛋白质嵌入方法。

    

    蛋白质相互作用(PPIs)在各种生物过程中至关重要，对生命活动起着关键作用。实验PPI测定需求增长和成本增加，需要高效的PPI预测的计算方法。现有方法主要依赖蛋白质序列进行PPI预测，然而蛋白质结构才是决定相互作用的关键。为了同时考虑两种蛋白质模态，我们通过蛋白氨基酸残基的序列和结构上下文定义了微环境，描述了周围化学性质和几何特征。此外，先前工作中定义的微环境主要基于实验测定的物理化学性质，其“词汇表”通常极小，这使得很难涵盖微环境的多样性和复杂性。在本文中，我们提出了用于PPI预测的微环境感知蛋白质嵌入。

    arXiv:2402.14391v1 Announce Type: new  Abstract: Protein-Protein Interactions (PPIs) are fundamental in various biological processes and play a key role in life activities. The growing demand and cost of experimental PPI assays require computational methods for efficient PPI prediction. While existing methods rely heavily on protein sequence for PPI prediction, it is the protein structure that is the key to determine the interactions. To take both protein modalities into account, we define the microenvironment of an amino acid residue by its sequence and structural contexts, which describe the surrounding chemical properties and geometric features. In addition, microenvironments defined in previous work are largely based on experimentally assayed physicochemical properties, for which the "vocabulary" is usually extremely small. This makes it difficult to cover the diversity and complexity of microenvironments. In this paper, we propose Microenvironment-Aware Protein Embedding for PPI p
    
[^82]: 保障交易：一种使用IHT-LR和网格搜索的混合可靠集成机器学习模型

    Securing Transactions: A Hybrid Dependable Ensemble Machine Learning Model using IHT-LR and Grid Search

    [https://arxiv.org/abs/2402.14389](https://arxiv.org/abs/2402.14389)

    一种新型混合集成机器学习模型，利用网格搜索将决策树、随机森林、KNN和多层感知机等算法智能结合，以提高欺诈检测效果。

    

    金融机构和企业面临着来自欺诈交易的持续挑战，促使需要有效的检测方法。检测信用卡欺诈对于识别和阻止未经授权的交易至关重要。及时发现欺诈可使调查人员迅速采取行动，以减少进一步损失。然而，调查过程通常耗时，限制了每天可以彻底检查的警报数量。因此，欺诈检测模型的主要目标是提供准确的警报，同时尽量减少误报和漏报的欺诈案例。在本文中，我们引入了一种最先进的混合集成(ENS)可靠机器学习(ML)模型，通过使用网格搜索智能地结合多个算法并进行适当加权优化，包括决策树(DT)、随机森林(RF)、K-最近邻(KNN)和多层感知机(MLP)，以增强欺诈检测。

    arXiv:2402.14389v1 Announce Type: new  Abstract: Financial institutions and businesses face an ongoing challenge from fraudulent transactions, prompting the need for effective detection methods. Detecting credit card fraud is crucial for identifying and preventing unauthorized transactions.Timely detection of fraud enables investigators to take swift actions to mitigate further losses. However, the investigation process is often time-consuming, limiting the number of alerts that can be thoroughly examined each day. Therefore, the primary objective of a fraud detection model is to provide accurate alerts while minimizing false alarms and missed fraud cases. In this paper, we introduce a state-of-the-art hybrid ensemble (ENS) dependable Machine learning (ML) model that intelligently combines multiple algorithms with proper weighted optimization using Grid search, including Decision Tree (DT), Random Forest (RF), K-Nearest Neighbor (KNN), and Multilayer Perceptron (MLP), to enhance fraud 
    
[^83]: 使用自动深度学习改进风力发电预测的WindDragon系统

    WindDragon: Enhancing wind power forecasting with Automated Deep Learning

    [https://arxiv.org/abs/2402.14385](https://arxiv.org/abs/2402.14385)

    利用自动深度学习结合数值天气预报风速图，WindDragon系统在全国范围内实现了短期风力预测，为电网运营和系统平衡提供关键支持。

    

    实现到2050年零碳排放的目标需要将大量风力纳入电网中。这种能源由于其变化性和不确定性对系统运营商构成挑战。因此，准确预测风力发电对于电网运营和系统平衡至关重要。本文提出了一种在全国范围内进行短期（1至6小时）风力预测的创新方法。该方法利用了自动深度学习结合数值天气预报风速图来准确预测风力发电。

    arXiv:2402.14385v1 Announce Type: new  Abstract: Achieving net zero carbon emissions by 2050 requires the integration of increasing amounts of wind power into power grids. This energy source poses a challenge to system operators due to its variability and uncertainty. Therefore, accurate forecasting of wind power is critical for grid operation and system balancing. This paper presents an innovative approach to short-term (1 to 6 hour horizon) windpower forecasting at a national level. The method leverages Automated Deep Learning combined with Numerical Weather Predictions wind speed maps to accurately forecast wind power.
    
[^84]: 具有软动态时间规整和并行重构的生成对抗网络用于能源时间序列异常检测

    Generative Adversarial Network with Soft-Dynamic Time Warping and Parallel Reconstruction for Energy Time Series Anomaly Detection

    [https://arxiv.org/abs/2402.14384](https://arxiv.org/abs/2402.14384)

    本研究提出了一种利用深度卷积生成对抗网络来进行能源时间序列异常检测的方法，结合软动态时间规整和并行重构技术，通过将重构损失和潜在空间的先验概率分布相结合作为异常分数，加速了检测过程，实验证明在建筑能源消耗的异常检测上表现出很好的效果。

    

    在本文中，我们利用一维深度卷积生成对抗网络(DCGAN)来进行能源时间序列数据中的顺序异常检测。异常检测涉及梯度下降以重构能源子序列，通过生成器网络识别紧密生成它们的噪声向量。我们使用Soft-DTW作为重构损失的可微替代方案，并发现优于欧氏距离。将重构损失和潜在空间的先验概率分布相结合作为异常分数。我们的新颖方法通过并行计算多个点的重构加速检测，并显示出在识别建筑物中的异常能源消耗方面具有潜力，通过对来自15栋建筑物的小时能源时间序列进行实验加以证明。

    arXiv:2402.14384v1 Announce Type: new  Abstract: In this paper, we employ a 1D deep convolutional generative adversarial network (DCGAN) for sequential anomaly detection in energy time series data. Anomaly detection involves gradient descent to reconstruct energy sub-sequences, identifying the noise vector that closely generates them through the generator network. Soft-DTW is used as a differentiable alternative for the reconstruction loss and is found to be superior to Euclidean distance. Combining reconstruction loss and the latent space's prior probability distribution serves as the anomaly score. Our novel method accelerates detection by parallel computation of reconstruction of multiple points and shows promise in identifying anomalous energy consumption in buildings, as evidenced by performing experiments on hourly energy time series from 15 buildings.
    
[^85]: 表征学习用于频繁子图挖掘

    Representation Learning for Frequent Subgraph Mining

    [https://arxiv.org/abs/2402.14367](https://arxiv.org/abs/2402.14367)

    提出了一种新颖的神经方法SPMiner，用于在大型目标图中近似找到频繁子图，通过结合图神经网络、顺序嵌入空间和高效的搜索策略，以识别出在目标图中出现最频繁的网络子图模式。

    

    鉴别频繁子图，也被称为网络模式，在分析和预测真实网络的属性中至关重要。然而，找到大型常见模式仍然是一个具有挑战性的问题，不仅是因为其NP难的子图计数，而且还因为可能的子图模式数量的指数增长。本文介绍了Subgraph Pattern Miner（SPMiner），一种新颖的神经方法，用于在大型目标图中近似找到频繁子图。SPMiner结合了图神经网络、顺序嵌入空间和高效的搜索策略，以识别出在目标图中出现最频繁的网络子图模式。SPMiner首先将目标图分解为许多重叠的子图，然后将每个子图编码为一个顺序嵌入空间。随后，SPMiner在顺序嵌入空间中进行单调漫步以识别频繁模式。

    arXiv:2402.14367v1 Announce Type: new  Abstract: Identifying frequent subgraphs, also called network motifs, is crucial in analyzing and predicting properties of real-world networks. However, finding large commonly-occurring motifs remains a challenging problem not only due to its NP-hard subroutine of subgraph counting, but also the exponential growth of the number of possible subgraphs patterns. Here we present Subgraph Pattern Miner (SPMiner), a novel neural approach for approximately finding frequent subgraphs in a large target graph. SPMiner combines graph neural networks, order embedding space, and an efficient search strategy to identify network subgraph patterns that appear most frequently in the target graph. SPMiner first decomposes the target graph into many overlapping subgraphs and then encodes each subgraph into an order embedding space. SPMiner then uses a monotonic walk in the order embedding space to identify frequent motifs. Compared to existing approaches and possibl
    
[^86]: OpenTab：将大型语言模型推进为开放领域的表格推理器

    OpenTab: Advancing Large Language Models as Open-domain Table Reasoners

    [https://arxiv.org/abs/2402.14361](https://arxiv.org/abs/2402.14361)

    OpenTab 是一个开放领域表格推理框架，利用表格检索器扩展了大型语言模型的知识范围，并通过生成SQL程序和基于事实的推理实现了在开放和封闭领域设置中明显优于基线的性能。

    

    在大量数据上训练的大型语言模型（LLMs）在各种自然语言任务上表现出色，但无法处理需要未经训练的知识的任务。 一种解决方案是使用一个检索器来获取相关信息，以扩展LLM的知识范围。 然而，由于多样化的数据模态和大表格尺寸，现有的面向文本的基于检索的LLMs在结构化表格数据上并不理想。 在这项工作中，我们提出了OpenTab，这是一个由LLMs驱动的开放领域表格推理框架。 总体而言，OpenTab利用表格检索器来获取相关表格，然后生成SQL程序以高效地解析检索到的表格。 利用从SQL执行中导出的中间数据，它进行基于事实的推理以产生准确的响应。 大量实验证明，OpenTab在开放和封闭领域设置中明显优于基线，实现了u

    arXiv:2402.14361v1 Announce Type: new  Abstract: Large Language Models (LLMs) trained on large volumes of data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously. One solution is to use a retriever that fetches relevant information to expand LLM's knowledge scope. However, existing textual-oriented retrieval-based LLMs are not ideal on structured table data due to diversified data modalities and large table sizes. In this work, we propose OpenTab, an open-domain table reasoning framework powered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant tables and then generates SQL programs to parse the retrieved tables efficiently. Utilizing the intermediate data derived from the SQL executions, it conducts grounded inference to produce accurate response. Extensive experimental evaluation shows that OpenTab significantly outperforms baselines in both open- and closed-domain settings, achieving u
    
[^87]: 基于不确定性驱动和对抗校准学习的心外脂肪组织分割

    Uncertainty-driven and Adversarial Calibration Learning for Epicardial Adipose Tissue Segmentation

    [https://arxiv.org/abs/2402.14349](https://arxiv.org/abs/2402.14349)

    提出了一种基于不确定性驱动和对抗校准学习的心外脂肪组织分割方法，通过特征潜空间多级监督网络(SPDNet)，增强分割以更准确估计EAT体积

    

    心外脂肪组织(EAT)是一种可以分泌大量脂联素从而影响心肌和冠状动脉的内脏脂肪。EAT的体积和密度可以作为独立风险标记的测量标准，通过非侵入性磁共振图像测量体积是评估EAT的最佳方法。然而，由于EAT与心包积液之间对比度低以及运动伪影的存在，分割EAT是具有挑战性的。本文提出了一种新颖的特征潜空间多级监督网络(SPDNet)，采用基于不确定性驱动和对抗校准学习以增强分割，以更准确地估计EAT体积。网络首先通过在特征潜空间中将不确定性建模为高斯分布来解决由于开放式医疗环境中医学图像的低质量或超出分布范围而导致EAT边缘模糊的问题，通过使用其贝叶斯估计作为正则化

    arXiv:2402.14349v1 Announce Type: cross  Abstract: Epicardial adipose tissue (EAT) is a type of visceral fat that can secrete large amounts of adipokines to affect the myocardium and coronary arteries. EAT volume and density can be used as independent risk markers measurement of volume by noninvasive magnetic resonance images is the best method of assessing EAT. However, segmenting EAT is challenging due to the low contrast between EAT and pericardial effusion and the presence of motion artifacts. we propose a novel feature latent space multilevel supervision network (SPDNet) with uncertainty-driven and adversarial calibration learning to enhance segmentation for more accurate EAT volume estimation. The network first addresses the blurring of EAT edges due to the medical images in the open medical environments with low quality or out-of-distribution by modeling the uncertainty as a Gaussian distribution in the feature latent space, which using its Bayesian estimation as a regularizatio
    
[^88]: 可靠的分布式压缩机器学习模型训练

    Dependable Distributed Training of Compressed Machine Learning Models

    [https://arxiv.org/abs/2402.14346](https://arxiv.org/abs/2402.14346)

    提出了DepL框架，实现了可靠的学习编排，能够确保以最低训练成本达到目标学习质量。

    

    有关机器学习（ML）模型分布式训练的现有工作一直忽视了实现学习质量的分布，而是专注于其平均值。 这导致了所得ML模型的可靠性差，其性能可能比预期的要差得多。 我们通过提出DepL来填补这一空白，这是一个可靠的学习编排框架，能够就（i）用于学习的数据，（ii）要使用的模型及何时在它们之间切换，以及（iii）要利用的节点集群及其资源做出高质量高效的决策。 具体而言，我们考虑可能的可用模型为完整的DNN及其压缩版本。 与以前的研究不同，DepL保证以目标概率实现目标学习质量，同时保持训练成本最低。 我们证明DepL具有常数竞争比率和多项式复杂度。

    arXiv:2402.14346v1 Announce Type: cross  Abstract: The existing work on the distributed training of machine learning (ML) models has consistently overlooked the distribution of the achieved learning quality, focusing instead on its average value. This leads to a poor dependability}of the resulting ML models, whose performance may be much worse than expected. We fill this gap by proposing DepL, a framework for dependable learning orchestration, able to make high-quality, efficient decisions on (i) the data to leverage for learning, (ii) the models to use and when to switch among them, and (iii) the clusters of nodes, and the resources thereof, to exploit. For concreteness, we consider as possible available models a full DNN and its compressed versions. Unlike previous studies, DepL guarantees that a target learning quality is reached with a target probability, while keeping the training cost at a minimum. We prove that DepL has constant competitive ratio and polynomial complexity, and s
    
[^89]: AURA：自然语言推理中的模式合理性不确定性

    AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales

    [https://arxiv.org/abs/2402.14337](https://arxiv.org/abs/2402.14337)

    提出了在自然语言推理中处理引发模式合理性不确定性的不完美理由的方法，实施了使用熵分数和模型先验信念来指导模型的策略，并在实证中展示了方法相对于敌对理由的稳健性能优势

    

    回策背后的理由不仅解释了模型决策，而且提升了语言模型在复杂推理任务上的推理能力。然而，获得无懈可击的理由通常是不可能的。此外，估计理由足够忠实以鼓励模型表现的程度并不是微不足道的。因此，这些推理任务通常迫使模型在不理想的理由下输出正确答案，并且与模型完全有能力的情况相比是次优的。在这项工作中，我们提出了如何应对引发模式合理性不确定性的不完美理由。我们首先用给定理由的熵分数来定义模糊的理由，使用模型先验信念作为信息量。然后根据理由的模糊性来引导模型选择两种不同的推理模型中的一种。我们在实证上论证了我们提出的方法相对于理由的敌对质量产生了稳健的性能优势。

    arXiv:2402.14337v1 Announce Type: new  Abstract: Rationales behind answers not only explain model decisions but boost language models to reason well on complex reasoning tasks. However, obtaining impeccable rationales is often impossible. Besides, it is non-trivial to estimate the degree to which the rationales are faithful enough to encourage model performance. Thus, such reasoning tasks often compel models to output correct answers under undesirable rationales and are sub-optimal compared to what the models are fully capable of. In this work, we propose how to deal with imperfect rationales causing aleatoric uncertainty. We first define the ambiguous rationales with entropy scores of given rationales, using model prior beliefs as informativeness. We then guide models to select one of two different reasoning models according to the ambiguity of rationales. We empirically argue that our proposed method produces robust performance superiority against the adversarial quality of rationale
    
[^90]: 超快速：用于表格数据的即时分类

    HyperFast: Instant Classification for Tabular Data

    [https://arxiv.org/abs/2402.14335](https://arxiv.org/abs/2402.14335)

    HyperFast是一个针对表格数据的即时分类方法，通过在单次前向传递中生成特定任务的神经网络，避免了需进行模型训练的必要性，并在实验中展现出高度竞争力。

    

    训练深度学习模型和进行超参数调整可能需要大量计算资源和时间。与此同时，传统的梯度提升算法等机器学习方法仍然是大多数表格数据应用的首选，而神经网络方法要么需要进行大量的超参数调整，要么仅适用于在有限设置下的玩具数据集。本文介绍了HyperFast，一个为在单次前向传递中立即分类表格数据而设计的元训练的超网络。HyperFast生成一个针对未见数据集定制的特定任务神经网络，可直接用于分类推断，无需训练模型。我们使用OpenML和基因组数据进行了大量实验，将HyperFast与竞争性表格数据神经网络、传统ML方法、AutoML系统和提升机器进行了比较。HyperFast展现出极具竞争力的结果。

    arXiv:2402.14335v1 Announce Type: cross  Abstract: Training deep learning models and performing hyperparameter tuning can be computationally demanding and time-consuming. Meanwhile, traditional machine learning methods like gradient-boosting algorithms remain the preferred choice for most tabular data applications, while neural network alternatives require extensive hyperparameter tuning or work only in toy datasets under limited settings. In this paper, we introduce HyperFast, a meta-trained hypernetwork designed for instant classification of tabular data in a single forward pass. HyperFast generates a task-specific neural network tailored to an unseen dataset that can be directly used for classification inference, removing the need for training a model. We report extensive experiments with OpenML and genomic data, comparing HyperFast to competing tabular data neural networks, traditional ML methods, AutoML systems, and boosting machines. HyperFast shows highly competitive results, wh
    
[^91]: 从大规模到小规模数据集：用于聚类算法选择的尺寸泛化

    From Large to Small Datasets: Size Generalization for Clustering Algorithm Selection

    [https://arxiv.org/abs/2402.14332](https://arxiv.org/abs/2402.14332)

    通过引入尺寸泛化概念，研究了在半监督设置下的聚类算法选择问题，提出了能够在小实例上保证准确度最高的算法也将在原始大实例上拥有最高准确度的条件。

    

    在聚类算法选择中，我们会得到一个大规模数据集，并要有效地选择要使用的聚类算法。我们在半监督设置下研究了这个问题，其中有一个未知的基准聚类，我们只能通过昂贵的oracle查询来访问。理想情况下，聚类算法的输出将与基本事实结构上接近。我们通过引入一种聚类算法准确性的尺寸泛化概念来解决这个问题。我们确定在哪些条件下我们可以（1）对大规模聚类实例进行子采样，（2）在较小实例上评估一组候选算法，（3）保证在小实例上准确度最高的算法将在原始大实例上拥有最高的准确度。我们为三种经典聚类算法提供了理论尺寸泛化保证：单链接、k-means++和Gonzalez的k中心启发式（一种平滑的变种）。

    arXiv:2402.14332v1 Announce Type: new  Abstract: In clustering algorithm selection, we are given a massive dataset and must efficiently select which clustering algorithm to use. We study this problem in a semi-supervised setting, with an unknown ground-truth clustering that we can only access through expensive oracle queries. Ideally, the clustering algorithm's output will be structurally close to the ground truth. We approach this problem by introducing a notion of size generalization for clustering algorithm accuracy. We identify conditions under which we can (1) subsample the massive clustering instance, (2) evaluate a set of candidate algorithms on the smaller instance, and (3) guarantee that the algorithm with the best accuracy on the small instance will have the best accuracy on the original big instance. We provide theoretical size generalization guarantees for three classic clustering algorithms: single-linkage, k-means++, and (a smoothed variant of) Gonzalez's k-centers heuris
    
[^92]: 通过三维分子生成的预训练和采样进行基于结构的药物设计

    Structure-Based Drug Design via 3D Molecular Generative Pre-training and Sampling

    [https://arxiv.org/abs/2402.14315](https://arxiv.org/abs/2402.14315)

    本研究提出了MolEdit3D，将3D分子生成与优化框架相结合，解决了现有基于优化的方法编辑分子时选择在2D空间中的问题。

    

    结构基药物设计旨在在先验知识下生成具有高亲和力的配体，并了解3D靶标结构。现有方法要么使用条件生成模型来学习给定目标结合位点的3D配体分布，要么迭代修改分子以优化基于结构的活性估计器。本文提出将3D分子生成与优化框架相结合，以解决现有基于优化的方法在编辑分子时选择在2D空间中，并使用分子对接来估计活性的问题。

    arXiv:2402.14315v1 Announce Type: cross  Abstract: Structure-based drug design aims at generating high affinity ligands with prior knowledge of 3D target structures. Existing methods either use conditional generative model to learn the distribution of 3D ligands given target binding sites, or iteratively modify molecules to optimize a structure-based activity estimator. The former is highly constrained by data quantity and quality, which leaves optimization-based approaches more promising in practical scenario. However, existing optimization-based approaches choose to edit molecules in 2D space, and use molecular docking to estimate the activity using docking predicted 3D target-ligand complexes. The misalignment between the action space and the objective hinders the performance of these models, especially for those employ deep learning for acceleration. In this work, we propose MolEdit3D to combine 3D molecular generation with optimization frameworks. We develop a novel 3D graph editi
    
[^93]: 基于FPGA的加速器，支持任意卷积核大小的CNN

    An FPGA-Based Accelerator Enabling Efficient Support for CNNs with Arbitrary Kernel Sizes

    [https://arxiv.org/abs/2402.14307](https://arxiv.org/abs/2402.14307)

    该论文提出了基于FPGA的推理加速器，能够高效支持任意内核大小的CNN，通过Z流方法优化数据流、Kseg方案降低存储需求，以及VF和HF方法优化CNN部署。

    

    卷积神经网络（CNN）引用视觉变换器（ViTs）的关键操作，具有大型卷积核，在各种基于视觉的应用中表现出色。为了解决现有设计在支持大型卷积核时计算效率下降的问题，提出了基于FPGA的推理加速器，用于高效部署具有任意内核大小的CNN。首先，提出了一种Z流方法，通过最大化数据重用机会来优化计算数据流。此外，所提出的设计结合卷积核分割（Kseg）方案，为大型卷积核提供了扩展支持，显著降低了重叠数据的存储需求。此外，通过对新兴CNN中典型块结构的分析，开发了垂直融合（VF）和水平融合（HF）方法，以优化CNN的部署。

    arXiv:2402.14307v1 Announce Type: cross  Abstract: Convolutional neural networks (CNNs) with large kernels, drawing inspiration from the key operations of vision transformers (ViTs), have demonstrated impressive performance in various vision-based applications. To address the issue of computational efficiency degradation in existing designs for supporting large-kernel convolutions, an FPGA-based inference accelerator is proposed for the efficient deployment of CNNs with arbitrary kernel sizes. Firstly, a Z-flow method is presented to optimize the computing data flow by maximizing data reuse opportunity. Besides, the proposed design, incorporating the kernel-segmentation (Kseg) scheme, enables extended support for large-kernel convolutions, significantly reducing the storage requirements for overlapped data. Moreover, based on the analysis of typical block structures in emerging CNNs, vertical-fused (VF) and horizontal-fused (HF) methods are developed to optimize CNN deployments from bo
    
[^94]: 有效实现在重复排名中群体间帕累托最优的效用—公平性

    Towards Efficient Pareto-optimal Utility-Fairness between Groups in Repeated Rankings

    [https://arxiv.org/abs/2402.14305](https://arxiv.org/abs/2402.14305)

    引入了使用Expohedron解决帕累托最优效用-公平性问题的新方法，避免了Birkhoff-von Neumann分解的高计算复杂度。

    

    在本文中，我们致力于解决计算具有帕累托最优平衡保证的一系列排名的问题，其中考虑了（1）最大化消费者效用和（2）最小化物品生产者之间的不公平性。这样的多目标优化问题通常使用标量化方法和线性规划来解决，基于表示物品可能排名分布的双随机矩阵。然而，上述方法依赖于Birkhoff-von Neumann（BvN）分解，其计算复杂度为$\mathcal{O}(n^5)$，其中$n$是物品数量，这使得在大规模系统中变得不切实际。为解决这一缺陷，我们引入了一种新颖的方法，通过使用Expohedron来解决上述问题 - 一个表征所有可达到物品曝光的排列多面体。在Expohedron上，我们绘制了帕累托曲线，捕捉了在最大化效用和最小化不公平性之间的权衡。

    arXiv:2402.14305v1 Announce Type: cross  Abstract: In this paper, we tackle the problem of computing a sequence of rankings with the guarantee of the Pareto-optimal balance between (1) maximizing the utility of the consumers and (2) minimizing unfairness between producers of the items. Such a multi-objective optimization problem is typically solved using a combination of a scalarization method and linear programming on bi-stochastic matrices, representing the distribution of possible rankings of items. However, the above-mentioned approach relies on Birkhoff-von Neumann (BvN) decomposition, of which the computational complexity is $\mathcal{O}(n^5)$ with $n$ being the number of items, making it impractical for large-scale systems. To address this drawback, we introduce a novel approach to the above problem by using the Expohedron - a permutahedron whose points represent all achievable exposures of items. On the Expohedron, we profile the Pareto curve which captures the trade-off betwee
    
[^95]: GenSERP: 用于整个页面呈现的大型语言模型

    GenSERP: Large Language Models for Whole Page Presentation

    [https://arxiv.org/abs/2402.14301](https://arxiv.org/abs/2402.14301)

    该论文提出了GenSERP框架，利用大型语言模型动态整理搜索结果并根据用户查询生成连贯的搜索引擎结果页面。

    

    大语言模型（LLMs）的出现为最小化搜索引擎结果页面（SERP）的组织工作带来了机会。本文提出了GenSERP，这是一个利用LLMs和视觉在少样本设置中动态组织中间搜索结果的框架，包括生成的聊天答案、网站摘要、多媒体数据、知识面板等，并根据用户的查询以连贯的SERP布局呈现。

    arXiv:2402.14301v1 Announce Type: cross  Abstract: The advent of large language models (LLMs) brings an opportunity to minimize the effort in search engine result page (SERP) organization. In this paper, we propose GenSERP, a framework that leverages LLMs with vision in a few-shot setting to dynamically organize intermediate search results, including generated chat answers, website snippets, multimedia data, knowledge panels into a coherent SERP layout based on a user's query. Our approach has three main stages: (1) An information gathering phase where the LLM continuously orchestrates API tools to retrieve different types of items, and proposes candidate layouts based on the retrieved items, until it's confident enough to generate the final result. (2) An answer generation phase where the LLM populates the layouts with the retrieved content. In this phase, the LLM adaptively optimize the ranking of items and UX configurations of the SERP. Consequently, it assigns a location on the pag
    
[^96]: 通过可互换性实现高参数PAC学习

    High-arity PAC learning via exchangeability

    [https://arxiv.org/abs/2402.14294](https://arxiv.org/abs/2402.14294)

    提出高参数PAC学习理论，利用结构化相关性和交换分布取代i.i.d.抽样，证明了统计学习基本定理的高维版本。

    

    我们开发了一种高维PAC学习理论，即在“结构化相关性”存在的统计学习中。 在这个理论中，假设可以是图形、超图，或者更一般地说，是有限关系语言中的结构，并且i.i.d.抽样被抽样产生可互换分布的诱导子结构取代。我们证明了统计学习基本定理的高维版本，通过表征高维（agnostic）PAC可学性，以纯组合维度的有限性及适当版本的均匀收敛。

    arXiv:2402.14294v1 Announce Type: new  Abstract: We develop a theory of high-arity PAC learning, which is statistical learning in the presence of "structured correlation". In this theory, hypotheses are either graphs, hypergraphs or, more generally, structures in finite relational languages, and i.i.d. sampling is replaced by sampling an induced substructure, producing an exchangeable distribution. We prove a high-arity version of the fundamental theorem of statistical learning by characterizing high-arity (agnostic) PAC learnability in terms of finiteness of a purely combinatorial dimension and in terms of an appropriate version of uniform convergence.
    
[^97]: CEV-LM: 受控编辑向量语言模型用于塑造自然语言生成

    CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations

    [https://arxiv.org/abs/2402.14290](https://arxiv.org/abs/2402.14290)

    CEV-LM 是一个轻量、半自回归语言模型，利用受限制的编辑向量控制文本的速度、音量和绕圈度量，从而更精准地定制生成的文本形状，比现有控制方法具有更好的控制效果。

    

    随着大规模语言模型成为文本生成的标准，需要更多地定制生成的紧凑性、针对性和信息性，具体取决于受众/应用程序。现有的控制方法主要调整文本的语义（如情感、主题）、结构（如句法树、词性）和词汇（如关键词/短语包含），但无法实现复杂的目标，如控制文本的复杂性和可读性,我们引入了CEV-LM——一种轻量级、半自回归语言模型，利用受限制的编辑向量来控制三个补充度量（速度、音量和绕圈），以量化文本的形状（例如内容的节奏）。 我们研究了一系列最先进的CTG模型，发现CEV-LM 可显著更有针对性和精确地控制这三个量。

    arXiv:2402.14290v1 Announce Type: new  Abstract: As large-scale language models become the standard for text generation, there is a greater need to tailor the generations to be more or less concise, targeted, and informative, depending on the audience/application. Existing control approaches primarily adjust the semantic (e.g., emotion, topics), structural (e.g., syntax tree, parts-of-speech), and lexical (e.g., keyword/phrase inclusion) properties of text, but are insufficient to accomplish complex objectives such as pacing which control the complexity and readability of the text. In this paper, we introduce CEV-LM - a lightweight, semi-autoregressive language model that utilizes constrained edit vectors to control three complementary metrics (speed, volume, and circuitousness) that quantify the shape of text (e.g., pacing of content). We study an extensive set of state-of-the-art CTG models and find that CEV-LM provides significantly more targeted and precise control of these three m
    
[^98]: TinyLLaVA：小规模大型多模态模型框架

    TinyLLaVA: A Framework of Small-scale Large Multimodal Models

    [https://arxiv.org/abs/2402.14289](https://arxiv.org/abs/2402.14289)

    TinyLLaVA框架使得小型多模态模型能够通过更好的数据质量和训练方案达到与大型模型相媲美的性能，最佳模型TinyLLaVA-3.1B在整体性能上优于现有的7B模型。

    

    我们提出了TinyLLaVA框架，为设计和分析小规模大型多模态模型（LMMs）提供了统一视角。我们从实证角度研究了不同的视觉编码器、连接模块、语言模型、训练数据和训练方案的影响。我们的大量实验表明，更好质量的数据结合更好的训练方案，使得较小的LMMs能够在整体性能上与更大的LMMs保持一致。在我们的框架下，我们训练了一系列小规模LMMs。我们最佳模型TinyLLaVA-3.1B在与现有的7B模型（如LLaVA-1.5和Qwen-VL）进行比较时，达到了更好的整体性能。我们希望我们的发现可以作为未来研究在数据扩展、训练设置和模型选择方面的基准。我们的模型权重和代码将被公开。

    arXiv:2402.14289v1 Announce Type: cross  Abstract: We present the TinyLLaVA framework that provides a unified perspective in designing and analyzing the small-scale Large Multimodal Models (LMMs). We empirically study the effects of different vision encoders, connection modules, language models, training data and training recipes. Our extensive experiments showed that better quality of data combined with better training recipes, smaller LMMs can consistently achieve on-par performances compared to bigger LMMs. Under our framework, we train a family of small-scale LMMs. Our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as baselines for future research in terms of data scaling, training setups and model selections. Our model weights and codes will be made public.
    
[^99]: 具有不可微分规则引导扩散的符号音乐生成

    Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion

    [https://arxiv.org/abs/2402.14285](https://arxiv.org/abs/2402.14285)

    介绍了一种用于符号音乐生成的不可微分规则引导的新方法，引入了可以与之即插即用的高时间分辨率潜在扩散架构，对音乐质量取得了显著进步

    

    我们研究了符号音乐生成的问题（例如生成钢琴卷谱），技术重点放在不可微分规则引导上。音乐规则通常以符号形式表达在音符特征上，如音符密度或和弦进行，许多规则是不可微分的，这在使用它们进行引导扩散时存在挑战。我们提出了一种新颖的引导方法，称为随机控制引导（SCG），它仅需要对规则函数进行前向评估，可以与预训练的扩散模型以即插即用的方式一起工作，从而首次实现了对不可微分规则的无训练引导。此外，我们引入了一种用于符号音乐生成的高时间分辨率潜在扩散架构，可以与SCG以即插即用的方式组合。与符号音乐生成中的标准强基线相比，该框架在音乐质量方面展示了明显的进展

    arXiv:2402.14285v1 Announce Type: cross  Abstract: We study the problem of symbolic music generation (e.g., generating piano rolls), with a technical focus on non-differentiable rule guidance. Musical rules are often expressed in symbolic form on note characteristics, such as note density or chord progression, many of which are non-differentiable which pose a challenge when using them for guided diffusion. We propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained diffusion models in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time. Additionally, we introduce a latent diffusion architecture for symbolic music generation with high time resolution, which can be composed with SCG in a plug-and-play fashion. Compared to standard strong baselines in symbolic music generation, this framework demonstrates marked advancements in music quali
    
[^100]: 在无GPS环境中使用基于地标的定位进行安全导航

    Secure Navigation using Landmark-based Localization in a GPS-denied Environment

    [https://arxiv.org/abs/2402.14280](https://arxiv.org/abs/2402.14280)

    该论文提出了一个整合了基于地标的定位与扩展卡尔曼滤波器的框架，用于在战场上预测移动实体的未来状态，以解决在无GPS环境中导航的安全问题。

    

    在现代战场情景中，依赖GPS进行导航可能是一个关键的弱点。对手经常使用策略来否认或欺骗GPS信号，这需要替代方法来定位和导航移动部队。本文提出了一个新颖的框架，将基于地标的定位（LanBLoc）与扩展卡尔曼滤波器（EKF）结合起来，以预测沿着战场移动实体的未来状态。我们的框架利用由部队控制中心生成的安全轨迹信息，考虑可识别的地标。

    arXiv:2402.14280v1 Announce Type: cross  Abstract: In modern battlefield scenarios, the reliance on GPS for navigation can be a critical vulnerability. Adversaries often employ tactics to deny or deceive GPS signals, necessitating alternative methods for the localization and navigation of mobile troops. Range-free localization methods such as DV-HOP rely on radio-based anchors and their average hop distance which suffers from accuracy and stability in a dynamic and sparse network topology. Vision-based approaches like SLAM and Visual Odometry use sensor fusion techniques for map generation and pose estimation that are more sophisticated and computationally expensive. This paper proposes a novel framework that integrates landmark-based localization (LanBLoc) with an Extended Kalman Filter (EKF) to predict the future state of moving entities along the battlefield. Our framework utilizes safe trajectory information generated by the troop control center by considering identifiable landmark
    
[^101]: 牛头角：硬样本加权持续训练改善LLM泛化能力

    Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization

    [https://arxiv.org/abs/2402.14270](https://arxiv.org/abs/2402.14270)

    通过硬样本加权持续训练的方法，该研究提出了IR-DRO框架，通过动态优先考虑训练中信息丰富的样本，以改善LLM泛化能力。

    

    在大语言模型（LLMs）快速发展的领域中，一个关键挑战是在高质量训练数据短缺的情况下增强它们的能力。我们的研究从一个轻量级持续训练LLMs的经验策略开始，使用它们的原始预训练数据集，重点关注导致中等损失的样本的选择性保留。这些样本被认为是信息丰富的，并且有助于模型的改进，与最高损失的样本形成对比，后者将由于与数据噪声和复杂性的相关性而被丢弃。然后，我们将这一策略形式化为基于实例加权的分布鲁棒优化（IR-DRO）的原则框架。IR-DRO旨在通过实例重新加权机制动态优先考虑训练的重点样本，由一个封闭形式解决方案简化，以便轻松整合到已建立的训练协议中。

    arXiv:2402.14270v1 Announce Type: new  Abstract: In the rapidly advancing arena of large language models (LLMs), a key challenge is to enhance their capabilities amid a looming shortage of high-quality training data. Our study starts from an empirical strategy for the light continual training of LLMs using their original pre-training data sets, with a specific focus on selective retention of samples that incur moderately high losses. These samples are deemed informative and beneficial for model refinement, contrasting with the highest-loss samples, which would be discarded due to their correlation with data noise and complexity. We then formalize this strategy into a principled framework of Instance-Reweighted Distributionally Robust Optimization (IR-DRO). IR-DRO is designed to dynamically prioritize the training focus on informative samples through an instance reweighting mechanism, streamlined by a closed-form solution for straightforward integration into established training protoco
    
[^102]: 双稳健学习在处理效应估计中的结构不可知性最优性

    Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation

    [https://arxiv.org/abs/2402.14264](https://arxiv.org/abs/2402.14264)

    采用结构不可知的统计下界框架，证明了双稳健估计器在平均处理效应（ATE）和平均处理效应方面的统计最优性

    

    平均处理效应估计是因果推断中最核心的问题，应用广泛。虽然文献中提出了许多估计策略，最近还纳入了通用的机器学习估计器，但这些方法的统计最优性仍然是一个开放的研究领域。本文采用最近引入的统计下界结构不可知框架，该框架对干扰函数没有结构性质假设，除了访问黑盒估计器以达到小误差；当只愿意考虑使用非参数回归和分类神谕作为黑盒子过程的估计策略时，这一点尤其吸引人。在这个框架内，我们证明了双稳健估计器对于平均处理效应（ATE）和平均处理效应的统计最优性。

    arXiv:2402.14264v1 Announce Type: cross  Abstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, recently also incorporating generic machine learning estimators, the statistical optimality of these methods has still remained an open area of investigation. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that attain small errors; which is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as a black-box sub-process. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Avera
    
[^103]: 单词序列熵：走向自由形式医学问答应用及其不确定性估计

    Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond

    [https://arxiv.org/abs/2402.14259](https://arxiv.org/abs/2402.14259)

    本论文提出了一种新方法单词序列熵（WSE），用于在自由形式医学问答任务中量化答案的不确定性，相比其他基线方法表现更优秀。

    

    不确定性估计在确保安全关键的人工智能系统与人类互动的可靠性中发挥关键作用，尤其在医疗领域尤为重要。然而，在自由形式的医学问答任务中，尚未建立一种通用方法来量化答案的不确定性，其中无关的词汇和语序含有有限的语义信息可能是不确定性的主要来源，这是由于生成不平等的存在。本文提出了单词序列熵（WSE），该方法根据语义相关性在单词和序列级别上校准不确定性比例，在不确定性量化时更加强调关键词和更相关的序列。我们在5个自由形式医学问答数据集上，利用7种“现成的”大语言模型（LLMs）将WSE与6种基线方法进行比较，并展示了WSE在性能上的优越性。

    arXiv:2402.14259v1 Announce Type: cross  Abstract: Uncertainty estimation plays a pivotal role in ensuring the reliability of safety-critical human-AI interaction systems, particularly in the medical domain. However, a general method for quantifying the uncertainty of free-form answers has yet to be established in open-ended medical question-answering (QA) tasks, where irrelevant words and sequences with limited semantic information can be the primary source of uncertainty due to the presence of generative inequality. In this paper, we propose the Word-Sequence Entropy (WSE), which calibrates the uncertainty proportion at both the word and sequence levels according to the semantic relevance, with greater emphasis placed on keywords and more relevant sequences when performing uncertainty quantification. We compare WSE with 6 baseline methods on 5 free-form medical QA datasets, utilizing 7 "off-the-shelf" large language models (LLMs), and show that WSE exhibits superior performance on ac
    
[^104]: 解释机器学习性能差异的分层分解方法

    A hierarchical decomposition for explaining ML performance discrepancies

    [https://arxiv.org/abs/2402.14254](https://arxiv.org/abs/2402.14254)

    提出了一种详细的变量级分解方法，可以量化每个变量对性能差异的影响，为实现有针对性干预措施提供更深入的理解

    

    机器学习（ML）算法在不同领域的性能往往有所不同。了解它们的性能差异的原因对于确定何种干预措施（例如算法或运营）最有效以缩小性能差距至关重要。现有方法侧重于将总性能差异分解为特征分布$p(X)$变化的影响与结果条件分布$p(Y|X)$变化的影响的$\textit{汇总分解}$；然而，这样粗糙的解释只提供了很少的方法来缩小性能差距。$\textit{详细的变量级分解}$可以量化每个变量对汇总分解中每个项的重要性，从而提供更深入的理解，并提出更有针对性的干预措施。然而，现有方法假设有关全因果图的完整知识或进行强参数假设。

    arXiv:2402.14254v1 Announce Type: new  Abstract: Machine learning (ML) algorithms can often differ in performance across domains. Understanding $\textit{why}$ their performance differs is crucial for determining what types of interventions (e.g., algorithmic or operational) are most effective at closing the performance gaps. Existing methods focus on $\textit{aggregate decompositions}$ of the total performance gap into the impact of a shift in the distribution of features $p(X)$ versus the impact of a shift in the conditional distribution of the outcome $p(Y|X)$; however, such coarse explanations offer only a few options for how one can close the performance gap. $\textit{Detailed variable-level decompositions}$ that quantify the importance of each variable to each term in the aggregate decomposition can provide a much deeper understanding and suggest much more targeted interventions. However, existing methods assume knowledge of the full causal graph or make strong parametric assumpti
    
[^105]: 基于知识驱动的自训练的重建异常定位

    Reconstruction-Based Anomaly Localization via Knowledge-Informed Self-Training

    [https://arxiv.org/abs/2402.14246](https://arxiv.org/abs/2402.14246)

    提出了一种名为知识驱动自训练（KIST）的基于重建的方法，通过将领域专家总结的异常知识集成到重建模型中，从而更好地利用异常样本并进一步提高异常定位性能。

    

    异常定位涉及在图像中定位异常区域，是一项重要的工业任务。由于其低复杂性和高解释性，基于重建的方法被广泛采用用于异常定位。大多数现有的基于重建的方法只使用正常样本来构建模型。如果在异常定位过程中适当利用异常样本，则可以提高定位性能。然而，通常只有弱标记的异常样本可用，这限制了改进。在许多情况下，我们可以获得领域专家总结的一些异常知识。利用这样的知识可以帮助我们更好地利用异常样本，从而进一步提高定位性能。在本文中，我们提出了一种名为知识驱动自训练（KIST）的新颖基于重建的方法，该方法将知识集成到重建模型中。

    arXiv:2402.14246v1 Announce Type: new  Abstract: Anomaly localization, which involves localizing anomalous regions within images, is a significant industrial task. Reconstruction-based methods are widely adopted for anomaly localization because of their low complexity and high interpretability. Most existing reconstruction-based methods only use normal samples to construct model. If anomalous samples are appropriately utilized in the process of anomaly localization, the localization performance can be improved. However, usually only weakly labeled anomalous samples are available, which limits the improvement. In many cases, we can obtain some knowledge of anomalies summarized by domain experts. Taking advantage of such knowledge can help us better utilize the anomalous samples and thus further improve the localization performance. In this paper, we propose a novel reconstruction-based method named knowledge-informed self-training (KIST) which integrates knowledge into reconstruction mo
    
[^106]: 利用多模态大语言模型的人工智能反馈增强机器人操作

    Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models

    [https://arxiv.org/abs/2402.14245](https://arxiv.org/abs/2402.14245)

    利用多模态大语言模型为机器人操作提供自动偏好反馈，提升决策效果

    

    最近，人们开始关注利用大型语言模型（LLMs）来增强决策过程。然而，将由LLMs生成的自然语言文本指令与执行所需的向量化操作对齐，常常需要特定于任务的细节，这是一个重要挑战。为了避免对这种特定于任务的细微之处的需求，受到基于偏好的策略学习方法的启发，我们研究利用多模态LLMs提供自动偏好反馈，仅从图像输入中引导决策。在这项研究中，我们训练了一个名为CriticGPT的多模态LLM，能够理解机器人操作任务中的轨迹视频，作为一个评论员提供分析和偏好反馈。随后，我们从奖励建模的角度验证了CriticGPT生成的偏好标签的有效性。对一种

    arXiv:2402.14245v1 Announce Type: cross  Abstract: Recently, there has been considerable attention towards leveraging large language models (LLMs) to enhance decision-making processes. However, aligning the natural language text instructions generated by LLMs with the vectorized operations required for execution presents a significant challenge, often necessitating task-specific details. To circumvent the need for such task-specific granularity, inspired by preference-based policy learning approaches, we investigate the utilization of multimodal LLMs to provide automated preference feedback solely from image inputs to guide decision-making. In this study, we train a multimodal LLM, termed CriticGPT, capable of understanding trajectory videos in robot manipulation tasks, serving as a critic to offer analysis and preference feedback. Subsequently, we validate the effectiveness of preference labels generated by CriticGPT from a reward modeling perspective. Experimental evaluation of the a
    
[^107]: MENTOR：在层次化强化学习中引导人类反馈和动态距离约束

    MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and Dynamic Distance Constraint

    [https://arxiv.org/abs/2402.14244](https://arxiv.org/abs/2402.14244)

    使用人类反馈和动态距离约束对层次化强化学习进行引导，解决了找到适当子目标的问题，并设计了双策略以稳定训练。

    

    层次化强化学习（HRL）为智能体的复杂任务提供了一种有前途的解决方案，其中使用了将任务分解为子目标并依次完成的层次框架。然而，当前的方法难以找到适当的子目标来确保稳定的学习过程。为了解决这个问题，我们提出了一个通用的层次强化学习框架，将人类反馈和动态距离约束整合到其中（MENTOR）。MENTOR充当“导师”，将人类反馈纳入高层策略学习中，以找到更好的子目标。至于低层策略，MENTOR设计了一个双策略以分别进行探索-开发解耦，以稳定训练。此外，尽管人类可以简单地将任务拆分成...

    arXiv:2402.14244v1 Announce Type: new  Abstract: Hierarchical reinforcement learning (HRL) provides a promising solution for complex tasks with sparse rewards of intelligent agents, which uses a hierarchical framework that divides tasks into subgoals and completes them sequentially. However, current methods struggle to find suitable subgoals for ensuring a stable learning process. Without additional guidance, it is impractical to rely solely on exploration or heuristics methods to determine subgoals in a large goal space. To address the issue, We propose a general hierarchical reinforcement learning framework incorporating human feedback and dynamic distance constraints (MENTOR). MENTOR acts as a "mentor", incorporating human feedback into high-level policy learning, to find better subgoals. As for low-level policy, MENTOR designs a dual policy for exploration-exploitation decoupling respectively to stabilize the training. Furthermore, although humans can simply break down tasks into s
    
[^108]: 分布式滤波电路的自动设计与优化通过强化学习

    Automated Design and Optimization of Distributed Filtering Circuits via Reinforcement Learning

    [https://arxiv.org/abs/2402.14236](https://arxiv.org/abs/2402.14236)

    提出一种通过强化学习算法实现的自动化设计方法，显著提高了分布式滤波电路设计的效率和质量。

    

    设计分布式滤波电路(DFC)复杂且耗时，电路性能严重依赖电子工程师的专业知识和经验。然而，手动设计方法效率低下。本研究提出一种新的端到端自动化方法，利用强化学习算法来改进DFC的设计。所提出的方法消除了对工程师设计经验的依赖，显著降低了与电路设计相关的主观性和约束。实验结果表明，在与传统的工程师驱动方法进行比较时，所提出的方法在设计效率和质量上都有明显改善。特别是，在设计复杂或快速发展的DFC时，所提出的方法表现出卓越的性能。

    arXiv:2402.14236v1 Announce Type: cross  Abstract: Designing distributed filtering circuits (DFCs) is complex and time-consuming, with the circuit performance relying heavily on the expertise and experience of electronics engineers. However, manual design methods tend to have exceedingly low-efficiency. This study proposes a novel end-to-end automated method for fabricating circuits to improve the design of DFCs. The proposed method harnesses reinforcement learning (RL) algorithms, eliminating the dependence on the design experience of engineers. Thus, it significantly reduces the subjectivity and constraints associated with circuit design. The experimental findings demonstrate clear improvements in both design efficiency and quality when comparing the proposed method with traditional engineer-driven methods. In particular, the proposed method achieves superior performance when designing complex or rapidly evolving DFCs. Furthermore, compared to existing circuit automation design techn
    
[^109]: 具有自我选择偏差的高效线性回归

    Sample-Efficient Linear Regression with Self-Selection Bias

    [https://arxiv.org/abs/2402.14229](https://arxiv.org/abs/2402.14229)

    提出了一种新颖且接近最优的样本高效算法，可在未知指数设定下的具有自我选择偏差的线性回归问题中高效地恢复参数向量，具有显著优化的时间复杂度和多项式样本复杂度。

    

    我们考虑在未知指数设定中具有自我选择偏差的线性回归问题，该问题最近由Cherapanamjeri、Daskalakis、Ilyas和Zampetakis[STOC 2023]的研究引入。在这个模型中，观察到$m$个i.i.d.样本$(\mathbf{x}_{\ell},z_{\ell})_{\ell=1}^m$，其中$z_{\ell}=\max_{i\in [k]}\{\mathbf{x}_{\ell}^T\mathbf{w}_i+\eta_{i,\ell}\}$，但最大化指数$i_{\ell}$是不可观测的。这里，$\mathbf{x}_{\ell}$被假设为$\mathcal{N}(0,I_n)$，噪声分布$\mathbf{\eta}_{\ell}\sim \mathcal{D}$是以$\mathbf{x}_{\ell}$为中心独立的。我们提供了一种新颖的、接近最优的样本高效（以$k$为度量）算法，用于恢复$\mathbf{w}_1,\ldots,\mathbf{w}_k\in \mathbb{R}^n$，其$\ell_2$-误差为$\varepsilon$，具有多项式样本复杂度$\tilde{O}(n)\cdot \mathsf{poly}(k,1/\varepsilon)$和显著改善的时间复杂度$\mathsf{poly}(n,k,1/\varepsilon)$。

    arXiv:2402.14229v1 Announce Type: cross  Abstract: We consider the problem of linear regression with self-selection bias in the unknown-index setting, as introduced in recent work by Cherapanamjeri, Daskalakis, Ilyas, and Zampetakis [STOC 2023]. In this model, one observes $m$ i.i.d. samples $(\mathbf{x}_{\ell},z_{\ell})_{\ell=1}^m$ where $z_{\ell}=\max_{i\in [k]}\{\mathbf{x}_{\ell}^T\mathbf{w}_i+\eta_{i,\ell}\}$, but the maximizing index $i_{\ell}$ is unobserved. Here, the $\mathbf{x}_{\ell}$ are assumed to be $\mathcal{N}(0,I_n)$ and the noise distribution $\mathbf{\eta}_{\ell}\sim \mathcal{D}$ is centered and independent of $\mathbf{x}_{\ell}$. We provide a novel and near optimally sample-efficient (in terms of $k$) algorithm to recover $\mathbf{w}_1,\ldots,\mathbf{w}_k\in \mathbb{R}^n$ up to additive $\ell_2$-error $\varepsilon$ with polynomial sample complexity $\tilde{O}(n)\cdot \mathsf{poly}(k,1/\varepsilon)$ and significantly improved time complexity $\mathsf{poly}(n,k,1/\varep
    
[^110]: COPR:通过最优策略正则化实现持续人类偏好学习

    COPR: Continual Human Preference Learning via Optimal Policy Regularization

    [https://arxiv.org/abs/2402.14228](https://arxiv.org/abs/2402.14228)

    提出了Continual Optimal Policy Regularization (COPR) 方法，通过借鉴最优策略理论，利用采样分布作为示范和正则化约束，以动态地对当前策略进行正则化，从而使强化学习从人类反馈中学习在持续学习情境下更加稳健

    

    arXiv:2402.14228v1 公告类型:跨界 摘要: 利用强化学习从人类反馈中学习（RLHF）通常用于改善大型语言模型（LLMs）与人类偏好的对齐。鉴于人类偏好的不断变化，持续对齐相对于传统静态对齐变得更加重要和实际。然而，使RLHF与持续学习（CL）兼容由于其复杂过程而具有挑战性。同时，直接学习新的人类偏好可能导致历史偏好的灾难性遗忘（CF），导致无助或有害的结果。为了克服这些挑战，我们提出了Continual Optimal Policy Regularization (COPR) 方法，该方法借鉴了最优策略理论。COPR利用采样分布作为示范和正则化约束用于持续学习。它采用Lagrange对偶（LD）方法根据历史上的最优策略动态地正则化当前策略

    arXiv:2402.14228v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to improve the alignment of Large Language Models (LLMs) with human preferences. Given the evolving nature of human preferences, continual alignment becomes more crucial and practical in comparison to traditional static alignment. Nevertheless, making RLHF compatible with Continual Learning (CL) is challenging due to its complex process. Meanwhile, directly learning new human preferences may lead to Catastrophic Forgetting (CF) of historical preferences, resulting in helpless or harmful outputs. To overcome these challenges, we propose the Continual Optimal Policy Regularization (COPR) method, which draws inspiration from the optimal policy theory. COPR utilizes a sampling distribution as a demonstration and regularization constraints for CL. It adopts the Lagrangian Duality (LD) method to dynamically regularize the current policy based on the historically optimal p
    
[^111]: 具有实时递归学习和最大相关性准则的四元数递归神经网络

    Quaternion recurrent neural network with real-time recurrent learning and maximum correntropy criterion

    [https://arxiv.org/abs/2402.14227](https://arxiv.org/abs/2402.14227)

    通过结合实时递归学习算法和最大相关性准则作为损失函数，提出了用于处理含异常值3D和4D数据的鲁棒四元数递归神经网络，所使用的最大相关性损失函数对异常值不太敏感，适用于多维嘈杂或不确定数据应用。

    

    我们开发了一种强大的四元数递归神经网络（QRNN），用于实时处理具有异常值的3D和4D数据。这是通过将实时递归学习（RTRL）算法和最大相关性准则（MCC）结合为损失函数来实现的。尽管均方误差和最大相关性准则都是可行的代价函数，但结果表明非二次最大相关性损失函数对异常值不太敏感，适用于具有多维嘈杂或不确定数据的应用。这两种算法基于新颖的广义HR（GHR）微积分导出，它允许对四元数变量的实函数进行微分，并提供乘法和链式法则，从而实现优雅且简洁的导出。在胸部内部标记物的运动预测背景下进行的仿真结果涵盖了肺癌放疗中的常规和不规则呼吸序列。

    arXiv:2402.14227v1 Announce Type: new  Abstract: We develop a robust quaternion recurrent neural network (QRNN) for real-time processing of 3D and 4D data with outliers. This is achieved by combining the real-time recurrent learning (RTRL) algorithm and the maximum correntropy criterion (MCC) as a loss function. While both the mean square error and maximum correntropy criterion are viable cost functions, it is shown that the non-quadratic maximum correntropy loss function is less sensitive to outliers, making it suitable for applications with multidimensional noisy or uncertain data. Both algorithms are derived based on the novel generalised HR (GHR) calculus, which allows for the differentiation of real functions of quaternion variables and offers the product and chain rules, thus enabling elegant and compact derivations. Simulation results in the context of motion prediction of chest internal markers for lung cancer radiotherapy, which includes regular and irregular breathing sequenc
    
[^112]: 使用超几何分布估计未知人口规模

    Estimating Unknown Population Sizes Using the Hypergeometric Distribution

    [https://arxiv.org/abs/2402.14220](https://arxiv.org/abs/2402.14220)

    提出了一种使用超几何似然解决估计离散分布挑战的新方法，即使存在严重的欠采样，也能实现，且在人口规模估计的准确性和学习能力方面优于其他方法。

    

    多元超几何分布描述从划分为多个类别的离散元素总体中进行无放回抽样。在文献中存在的一个空白中，我们解决了估计离散分布的挑战，当总体规模和其构成类别的大小均未知时。在这里，我们提出了一种使用超几何似然解决这一估计挑战的新方法，即使存在严重的欠采样也能实现。我们开发了我们的方法，以解释一个数据生成过程，其中地面真实值是有条件的连续潜变量混合分布，比如协同过滤，使用变分自动编码器框架。实证数据模拟表明，我们的方法在人口规模估计的准确性和学习能力方面均优于其他用于建模计数数据的似然函数。

    arXiv:2402.14220v1 Announce Type: new  Abstract: The multivariate hypergeometric distribution describes sampling without replacement from a discrete population of elements divided into multiple categories. Addressing a gap in the literature, we tackle the challenge of estimating discrete distributions when both the total population size and the sizes of its constituent categories are unknown. Here, we propose a novel solution using the hypergeometric likelihood to solve this estimation challenge, even in the presence of severe under-sampling. We develop our approach to account for a data generating process where the ground-truth is a mixture of distributions conditional on a continuous latent variable, such as with collaborative filtering, using the variational autoencoder framework. Empirical data simulation demonstrates that our method outperforms other likelihood functions used to model count data, both in terms of accuracy of population size estimate and in its ability to learn an 
    
[^113]: 个体间共享脑电图时空表示的对比学习用于自然神经科学

    Contrastive Learning of Shared Spatiotemporal EEG Representations Across Individuals for Naturalistic Neuroscience

    [https://arxiv.org/abs/2402.14213](https://arxiv.org/abs/2402.14213)

    通过对比学习，利用神经网络最大化相同刺激下各个个体的EEG表示的相似性，以此实现个体间共享时空脑电图表示的学习。

    

    自然刺激诱导的神经表征揭示了人类如何对日常生活中的外围刺激做出反应。理解自然刺激处理的一般神经机制的关键在于对齐各个个体的神经活动并提取个体间的共享神经表征。本研究针对脑电图（EEG）技术，该技术以其丰富的空间和时间信息而闻名，提出了一个用于个体间共享时空脑电图表示的对比学习的通用框架（CL-SSTER）。利用对比学习的表征能力，CL-SSTER利用神经网络最大化相同刺激下各个个体的EEG表示的相似性，与不同刺激的相对应。该网络采用空间和时间卷积同时学习空间和时间模式。

    arXiv:2402.14213v1 Announce Type: cross  Abstract: Neural representations induced by naturalistic stimuli offer insights into how humans respond to peripheral stimuli in daily life. The key to understanding the general neural mechanisms underlying naturalistic stimuli processing involves aligning neural activities across individuals and extracting inter-subject shared neural representations. Targeting the Electroencephalogram (EEG) technique, known for its rich spatial and temporal information, this study presents a general framework for Contrastive Learning of Shared SpatioTemporal EEG Representations across individuals (CL-SSTER). Harnessing the representational capabilities of contrastive learning, CL-SSTER utilizes a neural network to maximize the similarity of EEG representations across individuals for identical stimuli, contrasting with those for varied stimuli. The network employed spatial and temporal convolutions to simultaneously learn the spatial and temporal patterns inhere
    
[^114]: Moonwalk：逆向-前向微分

    Moonwalk: Inverse-Forward Differentiation

    [https://arxiv.org/abs/2402.14212](https://arxiv.org/abs/2402.14212)

    Moonwalk引入了一种基于向量-逆-Jacobian乘积的新技术，加速前向梯度计算，显著减少内存占用，并在保持真实梯度准确性的同时，将计算时间降低了几个数量级。

    

    反向传播虽然在梯度计算方面有效，但在解决内存消耗和扩展性方面表现不佳。这项工作探索了前向梯度计算作为可逆网络中的一种替代方法，展示了它在减少内存占用的潜力，并不带来重大缺点。我们引入了一种基于向量-逆-Jacobian乘积的新技术，加速了前向梯度的计算，同时保留了减少内存和保持真实梯度准确性的优势。我们的方法Moonwalk在网络深度方面具有线性时间复杂度，与朴素前向的二次时间复杂度相比，在没有分配更多内存的情况下，从实证的角度减少了几个数量级的计算时间。我们进一步通过将Moonwalk与反向模式微分相结合来加速，以实现与反向传播相当的时间复杂度，同时保持更小的内存使用量。

    arXiv:2402.14212v1 Announce Type: cross  Abstract: Backpropagation, while effective for gradient computation, falls short in addressing memory consumption, limiting scalability. This work explores forward-mode gradient computation as an alternative in invertible networks, showing its potential to reduce the memory footprint without substantial drawbacks. We introduce a novel technique based on a vector-inverse-Jacobian product that accelerates the computation of forward gradients while retaining the advantages of memory reduction and preserving the fidelity of true gradients. Our method, Moonwalk, has a time complexity linear in the depth of the network, unlike the quadratic time complexity of na\"ive forward, and empirically reduces computation time by several orders of magnitude without allocating more memory. We further accelerate Moonwalk by combining it with reverse-mode differentiation to achieve time complexity comparable with backpropagation while maintaining a much smaller mem
    
[^115]: 面向公平文本嵌入的内容条件去偏方法

    Content Conditional Debiasing for Fair Text Embedding

    [https://arxiv.org/abs/2402.14208](https://arxiv.org/abs/2402.14208)

    通过在内容条件下确保敏感属性与文本嵌入之间的条件独立性，我们提出了一种可以改善公平性的新方法，在保持效用的同时，解决了缺乏适当训练数据的问题。

    

    在自然语言处理（NLP）中，减轻机器学习模型中的偏见引起了越来越多的关注。然而，只有少数研究集中在公平的文本嵌入上，这对实际应用至关重要且具有挑战性。本文提出了一种学习公平文本嵌入的新方法。我们通过确保在内容条件下敏感属性与文本嵌入之间的条件独立性来实现公平性，同时保持效用权衡。具体来说，我们强制要求具有不同敏感属性但相同内容的文本的嵌入与其对应中立文本的嵌入保持相同的距离。此外，我们通过使用大型语言模型（LLMs）将文本增强为不同的敏感组，来解决缺乏适当训练数据的问题。我们广泛的评估表明，我们的方法有效地提高了公平性同时保持了嵌入的效用。

    arXiv:2402.14208v1 Announce Type: cross  Abstract: Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair text embeddings. We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content. Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text. Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embed
    
[^116]: 使用Patched Spectrogram Transformer 实现的压缩稳健合成语音检测

    Compression Robust Synthetic Speech Detection Using Patched Spectrogram Transformer

    [https://arxiv.org/abs/2402.14205](https://arxiv.org/abs/2402.14205)

    使用Patched Spectrogram Transformer实现的PS3DT合成语音检测器在ASVspoof2019数据集上表现优越，相对于其他基于频谱图的方法具有更好的检测性能

    

    许多深度学习合成语音生成工具现已可以轻松获得。合成语音的使用导致了金融欺诈、冒充他人以及误导信息的传播。为此，已经提出了可以检测合成语音的取证方法。现有方法往往在一个数据集上过度拟合，并且在实际场景（例如检测在社交平台上共享的合成语音）中，它们的性能会大幅降低。本文提出了一种名为Patched Spectrogram Synthetic Speech Detection Transformer (PS3DT)的合成语音检测器，它将时域语音信号转换为梅尔频谱图，并使用变换器神经网络对其进行分块处理。我们在ASVspoof2019数据集上评估了PS3DT的检测性能。我们的实验表明，与其他使用频谱图进行合成语音检测的方法相比，PS3DT在ASVspoof2019数据集上表现良好。我们还调查了其在其他数据集上的泛化能力。

    arXiv:2402.14205v1 Announce Type: cross  Abstract: Many deep learning synthetic speech generation tools are readily available. The use of synthetic speech has caused financial fraud, impersonation of people, and misinformation to spread. For this reason forensic methods that can detect synthetic speech have been proposed. Existing methods often overfit on one dataset and their performance reduces substantially in practical scenarios such as detecting synthetic speech shared on social platforms. In this paper we propose, Patched Spectrogram Synthetic Speech Detection Transformer (PS3DT), a synthetic speech detector that converts a time domain speech signal to a mel-spectrogram and processes it in patches using a transformer neural network. We evaluate the detection performance of PS3DT on ASVspoof2019 dataset. Our experiments show that PS3DT performs well on ASVspoof2019 dataset compared to other approaches using spectrogram for synthetic speech detection. We also investigate generaliza
    
[^117]: 通过位置编码比较图变换器

    Comparing Graph Transformers via Positional Encodings

    [https://arxiv.org/abs/2402.14202](https://arxiv.org/abs/2402.14202)

    本文比较了使用绝对位置编码（APEs）和相对位置编码（RPEs）的图变换器，在最大化区分能力方面是等效的。

    

    图变换器的区分能力与位置编码的选择紧密相关：用于增强基本变换器与图信息的特征。有两种主要类型的位置编码：绝对位置编码（APEs）和相对位置编码（RPEs）。APEs为每个节点分配特征，并作为变换器的输入。而RPEs则为每对节点（例如，图距离）分配一个特征，并用于增强注意力块。先验上，目前不清楚哪种方法更有利于最大化生成的图变换器的能力。本文旨在了解这两种不同类型位置编码之间的关系。有趣的是，我们展示了使用APEs和RPEs的图变换器在区分能力方面是等效的。特别地，我们展示了如何在保持其区分能力的同时交换APEs和RPEs。

    arXiv:2402.14202v1 Announce Type: new  Abstract: The distinguishing power of graph transformers is closely tied to the choice of positional encoding: features used to augment the base transformer with information about the graph. There are two primary types of positional encoding: absolute positional encodings (APEs) and relative positional encodings (RPEs). APEs assign features to each node and are given as input to the transformer. RPEs instead assign a feature to each pair of nodes, e.g., graph distance, and are used to augment the attention block. A priori, it is unclear which method is better for maximizing the power of the resulting graph transformer. In this paper, we aim to understand the relationship between these different types of positional encodings. Interestingly, we show that graph transformers using APEs and RPEs are equivalent in terms of distinguishing power. In particular, we demonstrate how to interchange APEs and RPEs while maintaining their distinguishing power in
    
[^118]: BeTAIL：从人类赛车游戏中学习的行为转换器对抗性模仿学习

    BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human Racing Gameplay

    [https://arxiv.org/abs/2402.14194](https://arxiv.org/abs/2402.14194)

    BeTAIL结合了行为转换器（BeT）策略和在线对抗性模仿学习（AIL），以学习从人类专家示范中学到的顺序决策过程，并纠正环境中的分布转移。

    

    模仿学习是从示范中学习策略而无需手工设计奖励函数的方法。在许多机器人任务中，如自主赛车，被模仿的策略必须对复杂的环境动态和人类决策建模。序列建模非常有效地捕捉运动序列的复杂模式，但在适应新环境或分布转移方面却很难。相反，对抗性模仿学习（AIL）可以缓解这种效应，但在样本效率和处理复杂运动模式方面存在困难。因此，我们提出了BeTAIL：行为转换器对抗性模仿学习，它将来自人类示范的行为转换器（BeT）策略与在线AIL相结合。BeTAIL将一个AIL剩余策略添加到BeT策略中，以建模人类专家的顺序决策过程并纠正分布外状态或环境中的转移。

    arXiv:2402.14194v1 Announce Type: new  Abstract: Imitation learning learns a policy from demonstrations without requiring hand-designed reward functions. In many robotic tasks, such as autonomous racing, imitated policies must model complex environment dynamics and human decision-making. Sequence modeling is highly effective in capturing intricate patterns of motion sequences but struggles to adapt to new environments or distribution shifts that are common in real-world robotics tasks. In contrast, Adversarial Imitation Learning (AIL) can mitigate this effect, but struggles with sample inefficiency and handling complex motion patterns. Thus, we propose BeTAIL: Behavior Transformer Adversarial Imitation Learning, which combines a Behavior Transformer (BeT) policy from human demonstrations with online AIL. BeTAIL adds an AIL residual policy to the BeT policy to model the sequential decision-making process of human experts and correct for out-of-distribution states or shifts in environmen
    
[^119]: 基于拓扑数据分析的语言模型多样性集成

    Diversity-Aware Ensembling of Language Models Based on Topological Data Analysis

    [https://arxiv.org/abs/2402.14184](https://arxiv.org/abs/2402.14184)

    基于拓扑数据分析的方法，通过估计NLP模型集成的权重，提高了集成模型的质量，提高了文本分类准确性和相关不确定性估计。

    

    集成是提高机器学习模型性能的重要工具。在与自然语言处理相关的情况下，由于开源中存在多个大型模型，集成有助于提升方法的性能。然而，现有方法主要依赖于对集成中每个模型的预测进行简单平均，对每个模型赋予相同权重，忽略了模型质量和一致性的差异。我们提出利用不仅单个模型表现知识，还使用它们之间的相似性来估计NLP模型集成的权重。通过采用基于拓扑数据分析（TDA）的距离度量，我们改进了我们的集成。文本分类准确性和相关不确定性估计的质量得到提高。

    arXiv:2402.14184v1 Announce Type: cross  Abstract: Ensembles are important tools for improving the performance of machine learning models. In cases related to natural language processing, ensembles boost the performance of a method due to multiple large models available in open source. However, existing approaches mostly rely on simple averaging of predictions by ensembles with equal weights for each model, ignoring differences in the quality and conformity of models. We propose to estimate weights for ensembles of NLP models using not only knowledge of their individual performance but also their similarity to each other. By adopting distance measures based on Topological Data Analysis (TDA), we improve our ensemble. The quality improves for both text classification accuracy and relevant uncertainty estimation.
    
[^120]: 线性变换器是多功能的上下文学习器

    Linear Transformers are Versatile In-Context Learners

    [https://arxiv.org/abs/2402.14180](https://arxiv.org/abs/2402.14180)

    线性变换器展示了在处理复杂问题和噪音干扰数据时的多功能性，通过发现一种新颖的优化算法，超越了许多合理的基线。

    

    最近的研究表明，变换器，特别是线性注意力模型，在前向推理步骤中对提供的上下文数据隐含地执行类似于梯度下降的算法。然而，它们在处理更复杂问题方面的能力尚未被探索。本文证明了任何线性变换器都保持隐式线性模型，并可解释为执行一种变形的预条件梯度下降。我们还研究了在线性变换器在训练数据受到不同水平噪音干扰的挑战性场景中的应用。值得注意的是，我们展示了对于这个问题，线性变换器发现了一种复杂且高效的优化算法，超越或与许多合理基线的表现相匹敌。我们反向工程了这个算法，并表明这是一种基于动量和噪音水平的自适应重缩放的新方法。

    arXiv:2402.14180v1 Announce Type: new  Abstract: Recent research has demonstrated that transformers, particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided in-context during their forward inference step. However, their capability in handling more complex problems remains unexplored. In this paper, we prove that any linear transformer maintains an implicit linear model and can be interpreted as performing a variant of preconditioned gradient descent. We also investigate the use of linear transformers in a challenging scenario where the training data is corrupted with different levels of noise. Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines. We reverse-engineer this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels. Our fi
    
[^121]: 使用机器学习注意力模型进行时间偏差校正

    A Temporal Bias Correction using a Machine Learning Attention model

    [https://arxiv.org/abs/2402.14169](https://arxiv.org/abs/2402.14169)

    本论文提出了一种新颖的偏差校正方法，将校准视为概率模型而不是算法流程，利用机器学习概率注意力模型来适配偏差校正任务，可准确校正具有长期时间属性的气候统计数据，提高了在这些数据上进行可靠影响研究的准确性。

    

    气候模型在与真实世界观测数据相比存在偏差，通常需要在影响研究之前进行校准。使校准成为可能的统计方法集合被称为偏差校正（BC）。然而，当前的BC方法在调整时间偏差方面存在困难，因为它们忽略了连续时间点之间的依赖关系。因此，具有长期时间属性的气候统计数据（如热浪持续时间和频率）无法准确校正，这使得在这些气候统计数据上进行可靠影响研究变得更加困难。本文提出了一种新颖的BC方法来校正时间偏差。这得益于将BC重新构想为概率模型而不是算法流程，并将最先进的机器学习（ML）概率关注模型调整到BC任务中。通过尼日利亚阿布贾的热浪持续时间统计案例研究...

    arXiv:2402.14169v1 Announce Type: new  Abstract: Climate models are biased with respect to real world observations and usually need to be calibrated prior to impact studies. The suite of statistical methods that enable such calibrations is called bias correction (BC). However, current BC methods struggle to adjust for temporal biases, because they disregard the dependence between consecutive time-points. As a result, climate statistics with long-range temporal properties, such as heatwave duration and frequency, cannot be corrected accurately, making it more difficult to produce reliable impact studies on such climate statistics. In this paper, we offer a novel BC methodology to correct for temporal biases. This is made possible by i) re-thinking BC as a probability model rather than an algorithmic procedure, and ii) adapting state-of-the-art machine-learning (ML) probabilistic attention models to fit the BC task. With a case study of heatwave duration statistics in Abuja, Nigeria, and
    
[^122]: T-Stitch：使用轨迹拼接加速预训练扩散模型中的采样

    T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching

    [https://arxiv.org/abs/2402.14167](https://arxiv.org/abs/2402.14167)

    T-Stitch提出了一种轨迹拼接的采样技术，能够在几乎不降低生成质量的情况下提高采样效率，通过在初始阶段使用较小的DPM来生成全局结构，然后切换到较大的DPM，从而实现高效的采样。

    

    从扩散概率模型（DPMs）进行采样对于高质量图像生成往往是昂贵的，通常需要许多步骤和大型模型。本文介绍了一种名为Trajectory Stitching T-Stitch的采样技术，这是一种简单而高效的技术，可以在几乎不降低生成质量的情况下提高采样效率。T-Stitch不同于仅仅使用一个大型DPM进行整个采样轨迹，而是首先利用较小的DPM作为初始步骤中较为廉价的替代方案，然后在后期切换到大型DPM。我们的关键见解是，不同的扩散模型在相同的训练数据分布下学习相似的编码，并且较小的模型能够在早期步骤中生成良好的全局结构。大量实验证明，T-Stitch无需训练，通常适用于不同架构，并且与大多数现有的快速采样技术相辅相成。

    arXiv:2402.14167v1 Announce Type: cross  Abstract: Sampling from diffusion probabilistic models (DPMs) is often expensive for high-quality image generation and typically requires many steps with a large model. In this paper, we introduce sampling Trajectory Stitching T-Stitch, a simple yet efficient technique to improve the sampling efficiency with little or no generation degradation. Instead of solely using a large DPM for the entire sampling trajectory, T-Stitch first leverages a smaller DPM in the initial steps as a cheap drop-in replacement of the larger DPM and switches to the larger DPM at a later stage. Our key insight is that different diffusion models learn similar encodings under the same training data distribution and smaller models are capable of generating good global structures in the early steps. Extensive experiments demonstrate that T-Stitch is training-free, generally applicable for different architectures, and complements most existing fast sampling techniques with f
    
[^123]: 递归推测解码：通过无重复抽样加速LLM推理

    Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement

    [https://arxiv.org/abs/2402.14160](https://arxiv.org/abs/2402.14160)

    提出了递归推测解码(RSD)方法，通过无重复抽样最大化树的多样性，从而进一步加速LLM推理过程。

    

    推测解码是一种用于大型语言模型(LLMs)的推理加速方法，其中一个小型语言模型生成一个草稿令牌序列，该序列进一步由目标LLM并行验证。最近的研究通过建立草稿令牌树推进了这种方法，实现了优于单序列推测解码的性能。然而，这些工作在树的每个级别独立生成令牌，没有利用整个树的多样性。此外，尽管固定序列长度已经显示出更好的性能，但这些作品在固定目标计算资源上并没有进行实证研究，这是对于资源受限设备至关重要的。我们提出了递归推测解码(RSD)，一种新的基于树的方法，它对不重复抽样的草稿令牌进行最大化，并最大限度地实现了多样性。

    arXiv:2402.14160v1 Announce Type: cross  Abstract: Speculative decoding is an inference-acceleration method for large language models (LLMs) where a small language model generates a draft-token sequence which is further verified by the target LLM in parallel. Recent works have advanced this method by establishing a draft-token tree, achieving superior performance over a single-sequence speculative decoding. However, those works independently generate tokens at each level of the tree, not leveraging the tree's entire diversifiability. Besides, their empirical superiority has been shown for fixed length of sequences, implicitly granting more computational resource to LLM for the tree-based methods. None of the existing works has conducted empirical studies with fixed target computational budgets despite its importance to resource-bounded devices. We present Recursive Speculative Decoding (RSD), a novel tree-based method that samples draft tokens without replacement and maximizes the dive
    
[^124]: BIRCO：具有复杂目标的信息检索任务基准

    BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives

    [https://arxiv.org/abs/2402.14151](https://arxiv.org/abs/2402.14151)

    BIRCO基准评估基于大型语言模型的信息检索系统对多方面用户目标的检索能力，发现新的检索协议和更强大的模型是解决复杂用户需求的必要条件。

    

    我们提出了具有复杂目标的信息检索(IR)任务基准(BIRCO)。 BIRCO评估IR系统根据多方面用户目标检索文档的能力。 该基准的复杂性和紧凑大小使其适用于评估基于大型语言模型(LLM)的信息检索系统。 我们提出了一个模块化框架，用于研究可能影响LLM在检索任务上的性能的因素，并确定了一个简单的基线模型，该模型与或优于现有方法和更复杂的替代方案。 没有一种方法在所有基准任务上均达到令人满意的性能，这表明需要更强大的模型和新的检索协议来解决复杂的用户需求。

    arXiv:2402.14151v1 Announce Type: cross  Abstract: We present the Benchmark of Information Retrieval (IR) tasks with Complex Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve documents given multi-faceted user objectives. The benchmark's complexity and compact size make it suitable for evaluating large language model (LLM)-based information retrieval systems. We present a modular framework for investigating factors that may influence LLM performance on retrieval tasks, and identify a simple baseline model which matches or outperforms existing approaches and more complex alternatives. No approach achieves satisfactory performance on all benchmark tasks, suggesting that stronger models and new retrieval protocols are necessary to address complex user needs.
    
[^125]: 神经网络与摩擦：滑动、保持、学习

    Neural Networks and Friction: Slide, Hold, Learn

    [https://arxiv.org/abs/2402.14148](https://arxiv.org/abs/2402.14148)

    循环神经网络利用GRU架构学习合成数据中复杂摩擦定律动力学，展示了机器学习模型在理解和模拟摩擦过程物理的潜力。

    

    本研究表明，利用门控循环单元（GRU）架构的循环神经网络（RNNs）具有学习合成数据中速率与状态摩擦定律复杂动力学的能力。用于训练网络的数据通过应用传统速率与状态摩擦方程结合状态演化老化定律生成。我们方法的一个新颖之处在于制定一个损失函数，该函数明确考虑训练过程中的初始条件、直接效应以及状态变量的演变。研究发现，具有GRU架构的RNN能够有效学习预测摩擦系数由于速度跳跃而产生的变化，展示了机器学习模型在理解和模拟摩擦过程物理的潜力。

    arXiv:2402.14148v1 Announce Type: cross  Abstract: In this study, it is demonstrated that Recurrent Neural Networks (RNNs), specifically those utilizing Gated Recurrent Unit (GRU) architecture, possess the capability to learn the complex dynamics of rate-and-state friction laws from synthetic data. The data employed for training the network is generated through the application of traditional rate-and-state friction equations coupled with the aging law for state evolution. A novel aspect of our approach is the formulation of a loss function that explicitly accounts for initial conditions, the direct effect, and the evolution of state variables during training. It is found that the RNN, with its GRU architecture, effectively learns to predict changes in the friction coefficient resulting from velocity jumps, thereby showcasing the potential of machine learning models in understanding and simulating the physics of frictional processes.
    
[^126]: 带有多个领域的本地分布偏移的乘幂稳健估计

    Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains

    [https://arxiv.org/abs/2402.14145](https://arxiv.org/abs/2402.14145)

    提出了一种两阶段的乘幂稳健估计方法，用于改善表格数据分析中每个个体部分的模型性能，并建立了在测试风险上的理论保证。

    

    分布偏移在现实世界的机器学习应用中普遍存在，给在一个数据分布上训练的模型推广到另一个数据分布带来挑战。本文专注于数据分布随整个总体的多个部分变化的情形，并仅在每个部分内对训练与测试（部署）数据分布的差异进行局部假设。我们提出了一种两阶段的乘幂稳健估计方法，用于改善表格数据分析中每个个体部分的模型性能。该方法涉及拟合基于从多个部分的训练数据中学到的模型的线性组合，然后对每个部分进行细化。我们的方法旨在与常用的现成机器学习模型一起实施。我们在测试风险上建立了该方法泛化界限的理论保证。通过大量实验...

    arXiv:2402.14145v1 Announce Type: cross  Abstract: Distribution shifts are ubiquitous in real-world machine learning applications, posing a challenge to the generalization of models trained on one data distribution to another. We focus on scenarios where data distributions vary across multiple segments of the entire population and only make local assumptions about the differences between training and test (deployment) distributions within each segment. We propose a two-stage multiply robust estimation method to improve model performance on each individual segment for tabular data analysis. The method involves fitting a linear combination of the based models, learned using clusters of training data from multiple segments, followed by a refinement step for each segment. Our method is designed to be implemented with commonly used off-the-shelf machine learning models. We establish theoretical guarantees on the generalization bound of the method on the test risk. With extensive experiments
    
[^127]: NeuroFlux: 使用自适应局部学习进行高效CNN训练

    NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning

    [https://arxiv.org/abs/2402.14139](https://arxiv.org/abs/2402.14139)

    NeuroFlux是一个为内存受限场景量身定制的CNN训练系统，提出了自适应辅助网络和块特定的自适应批处理大小的创新机遇。

    

    在资源受限的移动和边缘环境中进行高效的设备内卷积神经网络（CNN）训练是一个挑战。本文介绍了NeuroFlux，这是一个为内存受限场景量身定制的CNN训练系统。我们提出了两个创新机遇：第一是采用可变数量滤波器的自适应辅助网络，以减少GPU内存的使用；第二是针对块特定的自适应批处理大小，既满足GPU内存限制，又加速训练过程。

    arXiv:2402.14139v1 Announce Type: new  Abstract: Efficient on-device convolutional neural network (CNN) training in resource-constrained mobile and edge environments is an open challenge. Backpropagation is the standard approach adopted, but it is GPU memory intensive due to its strong inter-layer dependencies that demand intermediate activations across the entire CNN model to be retained in GPU memory. This necessitates smaller batch sizes to make training possible within the available GPU memory budget, but in turn, results in a substantially high and impractical training time. We introduce NeuroFlux, a novel CNN training system tailored for memory-constrained scenarios. We develop two novel opportunities: firstly, adaptive auxiliary networks that employ a variable number of filters to reduce GPU memory usage, and secondly, block-specific adaptive batch sizes, which not only cater to the GPU memory constraints but also accelerate the training process. NeuroFlux segments the CNNs into
    
[^128]: GDTM：一个具有分布式多模态传感器的室内地理空间跟踪数据集

    GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal Sensors

    [https://arxiv.org/abs/2402.14136](https://arxiv.org/abs/2402.14136)

    GDTM提供了一个新的室内地理空间跟踪数据集，包含了分布式多模态传感器和可重新配置传感器节点位置，可以用于研究处理多模态数据的体系结构优化和模型对不良传感条件和传感器位置变化的稳健性。

    

    不断定位移动物体，即地理空间跟踪，对于自主建筑基础设施至关重要。准确而稳健的地理空间跟踪通常利用多模态传感器融合算法，这些算法需要具有来自各种传感器类型的时间对齐、同步数据的大型数据集。然而，这样的数据集并不readily可获得。因此，我们提出了GDTM，一个具有分布式多模态传感器和可重新配置传感器节点位置的九小时多模态物体跟踪数据集。我们的数据集使得能够探索几个研究问题，例如优化处理多模态数据的体系结构，以及研究模型对恶劣传感条件和传感器放置变化的稳健性。这项工作的代码、示例数据和检查点可在https://github.com/nesl/GDTM 上找到。

    arXiv:2402.14136v1 Announce Type: cross  Abstract: Constantly locating moving objects, i.e., geospatial tracking, is essential for autonomous building infrastructure. Accurate and robust geospatial tracking often leverages multimodal sensor fusion algorithms, which require large datasets with time-aligned, synchronized data from various sensor types. However, such datasets are not readily available. Hence, we propose GDTM, a nine-hour dataset for multimodal object tracking with distributed multimodal sensors and reconfigurable sensor node placements. Our dataset enables the exploration of several research problems, such as optimizing architectures for processing multimodal data, and investigating models' robustness to adverse sensing conditions and sensor placement variances. A GitHub repository containing the code, sample data, and checkpoints of this work is available at https://github.com/nesl/GDTM.
    
[^129]: 用于检测弱信号和提取物理信息的随机森林：磁导航的案例研究

    Random forests for detecting weak signals and extracting physical information: a case study of magnetic navigation

    [https://arxiv.org/abs/2402.14131](https://arxiv.org/abs/2402.14131)

    使用随机森林模型可以提高对弱信号的检测精度，并从时间序列数据中直接获取位置信息。

    

    最近的研究表明，可以利用两种机器学习架构，即库努机器和时滞前馈神经网络，来探测地球的异常磁场，该磁场淹没在复杂的信号中，用于在无GPS环境中进行磁导航。检测到的异常场的精度对应于在10到40米范围内的定位精度。为了提高检测弱信号的准确性并减少不确定性，以及直接获取位置信息，我们利用随机森林的机器学习模型，将多个决策树的输出结合起来，给出感兴趣物理量的最佳值。特别是，从飞机驾驶舱在各种机动阶段收集的时间序列数据中，由于地球磁场的其他元素引起的强背景复杂信号是造成复杂信号的原因，我们将利用随机森林模型来分析这些数据。

    arXiv:2402.14131v1 Announce Type: cross  Abstract: It was recently demonstrated that two machine-learning architectures, reservoir computing and time-delayed feed-forward neural networks, can be exploited for detecting the Earth's anomaly magnetic field immersed in overwhelming complex signals for magnetic navigation in a GPS-denied environment. The accuracy of the detected anomaly field corresponds to a positioning accuracy in the range of 10 to 40 meters. To increase the accuracy and reduce the uncertainty of weak signal detection as well as to directly obtain the position information, we exploit the machine-learning model of random forests that combines the output of multiple decision trees to give optimal values of the physical quantities of interest. In particular, from time-series data gathered from the cockpit of a flying airplane during various maneuvering stages, where strong background complex signals are caused by other elements of the Earth's magnetic field and the fields p
    
[^130]: DeiSAM：通过指示提示分割任何内容

    DeiSAM: Segment Anything with Deictic Prompting

    [https://arxiv.org/abs/2402.14123](https://arxiv.org/abs/2402.14123)

    DeiSAM提出将大型预训练神经网络与可区分逻辑推理器结合，用于指示提示性分割，实现了在复杂场景中对象的分割

    

    大规模、预训练的神经网络已经在各种任务中展现出强大的能力，包括零-shot图像分割。为了在复杂场景中识别具体对象，人类本能地依赖于自然语言中的指示性描述，即根据上下文指称某物，比如“在桌子上并在杯子后面的物体”。然而，深度学习方法由于在复杂场景中缺乏推理能力，无法可靠地解释这种指示性表示。为了解决这个问题，我们提出了DeiSAM——将大型预训练神经网络与可区分逻辑推理器相结合，用于指示提示性分割。给定复杂的文本分割描述，DeiSAM利用大型语言模型（LLMs）生成一阶逻辑规则，并对生成的场景图进行可区分的前向推理。随后，DeiSAM通过匹配

    arXiv:2402.14123v1 Announce Type: cross  Abstract: Large-scale, pre-trained neural networks have demonstrated strong capabilities in various tasks, including zero-shot image segmentation. To identify concrete objects in complex scenes, humans instinctively rely on deictic descriptions in natural language, i.e., referring to something depending on the context such as "The object that is on the desk and behind the cup.". However, deep learning approaches cannot reliably interpret such deictic representations due to their lack of reasoning capabilities in complex scenarios. To remedy this issue, we propose DeiSAM -- a combination of large pre-trained neural networks with differentiable logic reasoners -- for deictic promptable segmentation. Given a complex, textual segmentation description, DeiSAM leverages Large Language Models (LLMs) to generate first-order logic rules and performs differentiable forward reasoning on generated scene graphs. Subsequently, DeiSAM segments objects by match
    
[^131]: 稀疏线性回归中不当学习的计算统计差距

    Computational-Statistical Gaps for Improper Learning in Sparse Linear Regression

    [https://arxiv.org/abs/2402.14103](https://arxiv.org/abs/2402.14103)

    该研究探讨了稀疏线性回归中的计算统计差距问题，为了高效地找到可以在样本上实现非平凡预测误差的潜在密集估计的回归向量，需要至少 $\Omega(k \log (d/k))$ 个样本。

    

    我们研究了稀疏线性回归中不当学习的计算统计差距。具体来说，给定来自维度为 $d$ 的 $k$-稀疏线性模型的 $n$ 个样本，我们询问了在时间多项式中的最小样本复杂度，以便高效地找到一个对这 $n$ 个样本达到非平凡预测误差的潜在密集估计的回归向量。信息理论上，这可以用 $\Theta(k \log (d/k))$ 个样本实现。然而，尽管在文献中很显著，但没有已知的多项式时间算法可以在不附加对模型的其他限制的情况下使用少于 $\Theta(d)$ 个样本达到相同的保证。类似地，现有的困难结果要么仅限于适当设置，在该设置中估计值也必须是稀疏的，要么仅适用于特定算法。

    arXiv:2402.14103v1 Announce Type: new  Abstract: We study computational-statistical gaps for improper learning in sparse linear regression. More specifically, given $n$ samples from a $k$-sparse linear model in dimension $d$, we ask what is the minimum sample complexity to efficiently (in time polynomial in $d$, $k$, and $n$) find a potentially dense estimate for the regression vector that achieves non-trivial prediction error on the $n$ samples. Information-theoretically this can be achieved using $\Theta(k \log (d/k))$ samples. Yet, despite its prominence in the literature, there is no polynomial-time algorithm known to achieve the same guarantees using less than $\Theta(d)$ samples without additional restrictions on the model. Similarly, existing hardness results are either restricted to the proper setting, in which the estimate must be sparse as well, or only apply to specific algorithms.   We give evidence that efficient algorithms for this task require at least (roughly) $\Omega(
    
[^132]: 在神经生物网络中学习功能连接组的动态表示

    Learning dynamic representations of the functional connectome in neurobiological networks

    [https://arxiv.org/abs/2402.14102](https://arxiv.org/abs/2402.14102)

    该论文提出了一种学习神经元动态亲和关系的无监督方法，以揭示不同时间点神经元之间形成的社区，从而揭示了动态功能连接组。

    

    神经回路的静态突触连接与其功能的动态形成形成鲜明对比。不同于静态连接，不同神经元可以在不同时间积极参与各种组合，实现不同的行为。我们介绍了一种无监督方法，用于学习在活生生动的动物中神经元之间的动态亲和力，并揭示不同时间点神经元之间形成的社区。推断包括两个主要步骤。首先，通过非负张量因子分解(NTF)组织来自大脑全面钙活动的神经元痕迹之间的成对非线性亲和力。每个因子指定了哪些神经元群体在特定时间间隔和动物上最有可能相互作用。最后，将允许加权社区检测的生成模型应用于NTF产生的功能基序，以揭示动态功能连接组。

    arXiv:2402.14102v1 Announce Type: cross  Abstract: The static synaptic connectivity of neuronal circuits stands in direct contrast to the dynamics of their function. As in changing community interactions, different neurons can participate actively in various combinations to effect behaviors at different times. We introduce an unsupervised approach to learn the dynamic affinities between neurons in live, behaving animals, and to reveal which communities form among neurons at different times. The inference occurs in two major steps. First, pairwise non-linear affinities between neuronal traces from brain-wide calcium activity are organized by non-negative tensor factorization (NTF). Each factor specifies which groups of neurons are most likely interacting for an inferred interval in time, and for which animals. Finally, a generative model that allows for weighted community detection is applied to the functional motifs produced by NTF to reveal a dynamic functional connectome. Since time 
    
[^133]: 现代生成对抗网络的有趣属性

    Intriguing Properties of Modern GANs

    [https://arxiv.org/abs/2402.14098](https://arxiv.org/abs/2402.14098)

    现代生成对抗网络学习的流形不符合训练数据分布，学习到的密度与数据分布相差甚远。

    

    现代生成对抗网络在生成逼真且多样化样本方面取得了显著的性能。这引发了许多人认为“生成对抗网络捕捉了训练数据流形”。在这项工作中，我们表明这种解释是错误的。我们在经验上展示了现代生成对抗网络学习的流形不适合训练分布：具体而言，该流形不经过训练样本，而是更接近于分布之外的图像。我们还研究了由潜在编码上的先验隐含的图像分布，并研究现代生成对抗网络是否学习了一个逼近训练分布的密度。令人惊讶的是，我们发现学习到的密度与数据分布相差甚远，生成对抗网络倾向于将更高的密度分配给分布之外的图像。最后，我们证明用于训练现代生成对抗网络的图像集通常不属于典型描述的集合。

    arXiv:2402.14098v1 Announce Type: new  Abstract: Modern GANs achieve remarkable performance in terms of generating realistic and diverse samples. This has led many to believe that ``GANs capture the training data manifold''. In this work we show that this interpretation is wrong. We empirically show that the manifold learned by modern GANs does not fit the training distribution: specifically the manifold does not pass through the training examples and passes closer to out-of-distribution images than to in-distribution images. We also investigate the distribution over images implied by the prior over the latent codes and study whether modern GANs learn a density that approximates the training distribution. Surprisingly, we find that the learned density is very far from the data distribution and that GANs tend to assign higher density to out-of-distribution images. Finally, we demonstrate that the set of images used to train modern GANs are often not part of the typical set described by 
    
[^134]: 跨架构零样本泛化的视觉分类

    Zero-shot generalization across architectures for visual classification

    [https://arxiv.org/abs/2402.14095](https://arxiv.org/abs/2402.14095)

    不同神经网络在跨架构和层间泛化到未知类别的能力存在差异，准确性并不是泛化能力的良好预测因子，泛化能力随着层深度呈非单调变化。

    

    深度网络的一个关键优势是对未见数据的泛化能力，但其与分类准确性的关系尚不清楚。我们利用一种极简的视觉数据集和一种泛化度量，展示了从深度卷积网络（CNNs）到transformers的流行网络在通过层和架构泛化到未见类别方面的能力存在差异。准确性并不是泛化能力的良好预测因子，并且泛化能力随着层深度呈非单调变化。代码可在https://github.com/dyballa/zero-shot-generalization 找到。

    arXiv:2402.14095v1 Announce Type: cross  Abstract: Generalization to unseen data is a key desideratum for deep networks, but its relation to classification accuracy is unclear. Using a minimalist vision dataset and a measure of generalizability, we show that popular networks, from deep convolutional networks (CNNs) to transformers, vary in their power to extrapolate to unseen classes both across layers and across architectures. Accuracy is not a good predictor of generalizability, and generalization varies non-monotonically with layer depth. Code is available at https://github.com/dyballa/zero-shot-generalization.
    
[^135]: LexC-Gen: 利用大型语言模型和双语词汇表为极低资源语言生成数据

    LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons

    [https://arxiv.org/abs/2402.14086](https://arxiv.org/abs/2402.14086)

    LexC-Gen提出了一种词典条件数据生成方法，可以以大规模生成低资源语言分类任务数据，取得了较好的效果。

    

    低资源语言的数据匮乏可以通过利用双语词典中从高资源语言的标记任务数据进行逐字翻译来解决，然而，双语词典通常与任务数据有限的词汇重叠，导致翻译覆盖和词典利用不佳。我们提出了一种称为LexC-Gen的词典条件数据生成方法，该方法可以大规模生成低资源语言分类任务数据。具体而言，LexC-Gen首先使用双语词典中的高资源语言单词生成与词典兼容的任务数据，然后通过单词翻译将其翻译成低资源语言。在17种极低资源语言中，LexC-Gen生成的数据在性能上与专家翻译的黄金数据竞争力相当，并且在情感分析和主题分类上平均比现有的基于词典的单词翻译方法提高了5.6和8.9个分数。

    arXiv:2402.14086v1 Announce Type: cross  Abstract: Data scarcity in low-resource languages can be addressed with word-to-word translations from labeled task data in high-resource languages using bilingual lexicons. However, bilingual lexicons often have limited lexical overlap with task data, which results in poor translation coverage and lexicon utilization. We propose lexicon-conditioned data generation (LexC-Gen), a method that generates low-resource-language classification task data at scale. Specifically, LexC-Gen first uses high-resource-language words from bilingual lexicons to generate lexicon-compatible task data, and then it translates them into low-resource languages with bilingual lexicons via word translation. Across 17 extremely low-resource languages, LexC-Gen generated data is competitive with expert-translated gold data, and yields on average 5.6 and 8.9 points improvement over existing lexicon-based word translation methods on sentiment analysis and topic classificati
    
[^136]: 使用具有运动代码的随机过程模型对嘈杂时间序列集合进行鲁棒学习

    Robust Learning of Noisy Time Series Collections Using Stochastic Process Models with Motion Codes

    [https://arxiv.org/abs/2402.14081](https://arxiv.org/abs/2402.14081)

    使用具有学习谱核的混合高斯过程的潜变量模型方法，针对嘈杂时间序列数据进行鲁棒学习。

    

    虽然时间序列分类和预测问题已经得到广泛研究，但具有任意时间序列长度的嘈杂时间序列数据的情况仍具挑战性。每个时间序列实例可以看作是嘈杂动态模型的一个样本实现，其特点是连续随机过程。对于许多应用，数据是混合的，由多个随机过程建模的几种类型的嘈杂时间序列序列组成，使得预测和分类任务变得更具挑战性。我们不是简单地将数据回归到每种时间序列类型，而是采用具有学习谱核的混合高斯过程的潜变量模型方法。更具体地说，我们为每种类型的嘈杂时间序列数据自动分配一个称为其运动代码的签名向量。然后，在每个分配的运动代码的条件下，我们推断出相关性的稀疏近似。

    arXiv:2402.14081v1 Announce Type: cross  Abstract: While time series classification and forecasting problems have been extensively studied, the cases of noisy time series data with arbitrary time sequence lengths have remained challenging. Each time series instance can be thought of as a sample realization of a noisy dynamical model, which is characterized by a continuous stochastic process. For many applications, the data are mixed and consist of several types of noisy time series sequences modeled by multiple stochastic processes, making the forecasting and classification tasks even more challenging. Instead of regressing data naively and individually to each time series type, we take a latent variable model approach using a mixtured Gaussian processes with learned spectral kernels. More specifically, we auto-assign each type of noisy time series data a signature vector called its motion code. Then, conditioned on each assigned motion code, we infer a sparse approximation of the corr
    
[^137]: 高效的规范化置信预测与不确定性量化：基于深度回归森林的抗癌药物敏感性预测

    Efficient Normalized Conformal Prediction and Uncertainty Quantification for Anti-Cancer Drug Sensitivity Prediction with Deep Regression Forests

    [https://arxiv.org/abs/2402.14080](https://arxiv.org/abs/2402.14080)

    通过深度回归森林计算样本方差，提高了抗癌药物敏感性预测中的规范化置信预测效率和覆盖率

    

    深度学习模型正在被应用于各种关键决策任务，然而它们被训练为提供点预测而没有提供信心度。如果与不确定性估计结合，深度学习模型的可信度可以得到提高。置信预测已经被证明是一种有希望的方法，可以将机器学习模型与预测区间配对，从而可以看到模型的不确定性。然而，常见的用于置信预测的不确定性估计方法未能提供对所有样本同样准确的异方差间隔。本文提出了一种方法，通过从深度回归森林获得的方差来估计每个样本的不确定性。我们展示了深度回归森林的方差如何提高药物反应预测任务上规范化诱导置信预测的效率和覆盖率。

    arXiv:2402.14080v1 Announce Type: cross  Abstract: Deep learning models are being adopted and applied on various critical decision-making tasks, yet they are trained to provide point predictions without providing degrees of confidence. The trustworthiness of deep learning models can be increased if paired with uncertainty estimations. Conformal Prediction has emerged as a promising method to pair machine learning models with prediction intervals, allowing for a view of the model's uncertainty. However, popular uncertainty estimation methods for conformal prediction fail to provide heteroskedastic intervals that are equally accurate for all samples. In this paper, we propose a method to estimate the uncertainty of each sample by calculating the variance obtained from a Deep Regression Forest. We show that the deep regression forest variance improves the efficiency and coverage of normalized inductive conformal prediction on a drug response prediction task.
    
[^138]: 从屏幕截图中提高语言理解能力

    Improving Language Understanding from Screenshots

    [https://arxiv.org/abs/2402.14073](https://arxiv.org/abs/2402.14073)

    本文提出了一种屏幕截图语言模型，通过引入新的Patch-and-Text Prediction（PTP）目标来改善文本能力，并取得了与BERT相当的性能。

    

    一种新兴的语言模型家族（LMs）可以处理文本和图像，在单个视觉视图内，有望拓宽图表理解和UI导航等复杂任务。我们称这些模型为屏幕截图语言模型。尽管具有吸引力，但现有的屏幕截图LMs在语言理解任务上明显落后于仅文本的模型。为了弥合这一差距，我们采用了一个简化的设置，其中模型输入是纯文本渲染的屏幕截图，并集中在提高屏幕截图LMs的文本能力。我们提出了一种新颖的Patch-and-Text Prediction（PTP）目标，该目标遮盖和恢复屏幕截图中的图像块和文本。我们还进行了大量消融研究，涉及遮盖率、块大小以及用于提高训练稳定性的设计。我们的预训练模型，仅采用视觉输入，就在8个GLUE中的6个上实现了与BERT相当的性能。

    arXiv:2402.14073v1 Announce Type: new  Abstract: An emerging family of language models (LMs), capable of processing both text and images within a single visual view, has the promise to unlock complex tasks such as chart understanding and UI navigation. We refer to these models as screenshot language models. Despite their appeal, existing screenshot LMs substantially lag behind text-only models on language understanding tasks. To close this gap, we adopt a simplified setting where the model inputs are plain-text-rendered screenshots, and we focus on improving the text ability of screenshot LMs. We propose a novel Patch-and-Text Prediction (PTP) objective, which masks and recovers both image patches of screenshots and text within screenshots. We also conduct extensive ablation studies on masking rates and patch sizes, as well as designs for improving training stability. Our pre-trained model, while solely taking visual inputs, achieves comparable performance with BERT on 6 out of 8 GLUE 
    
[^139]: 用于极端数据缩放的生成对抗模型

    Generative Adversarial Models for Extreme Downscaling of Climate Datasets

    [https://arxiv.org/abs/2402.14049](https://arxiv.org/abs/2402.14049)

    该方法提出了一种基于条件GAN的地理空间数据缩放方法，可以从非常低分辨率的输入生成高分辨率准确的气候数据集，并且明确考虑了不确定性。

    

    应对气候变化的挑战需要准确和高分辨率地映射气候和天气变量。然而，许多现有的气候数据集只能以非常粗糙的空间分辨率提供，这是由于模型复杂性和极高的计算需求所致。基于深度学习的方法，特别是生成对抗网络（GAN）及其变体，已被证明在提升自然图像方面非常有效，并在改进科学数据集方面显示出巨大潜力。本文描述了一种基于条件GAN的地理空间数据缩放方法，用于极端缩放网格气候数据集。与大多数现有方法相比，这种方法可以从非常低分辨率的输入生成高分辨率准确的气候数据集。更重要的是，该方法明确考虑了不确定性。

    arXiv:2402.14049v1 Announce Type: cross  Abstract: Addressing the challenges of climate change requires accurate and high-resolution mapping of climate and weather variables. However, many existing climate datasets, such as the gridded outputs of the state-of-the-art numerical climate models (e.g., general circulation models), are only available at very coarse spatial resolutions due to the model complexity and extremely high computational demand. Deep-learning-based methods, particularly generative adversarial networks (GANs) and their variants, have proved effective for refining natural images, and have shown great promise in improving scientific datasets. In this paper, we describe a conditional GAN-based geospatial downscaling method for extreme downscaling of gridded climate datasets. Compared to most existing methods, the method can generate high-resolution accurate climate datasets from very low-resolution inputs. More importantly, the method explicitly considers the uncertainty
    
[^140]: PolyNet：学习神经组合优化的多样化解决策略

    PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization

    [https://arxiv.org/abs/2402.14048](https://arxiv.org/abs/2402.14048)

    PolyNet通过学习互补解决策略来改善解空间探索，避免了人为规则导致解决方案质量下降的问题。

    

    强化学习方法用于构建组合优化问题解决方案，迅速接近人类设计的算法性能。为了进一步缩小差距，基于学习的方法在搜索过程中必须高效地探索解空间。最近的方法通过强制实施多样化解生成来人为增加探索，然而，这些规则可能损害解决方案质量，并且难以为更复杂的问题设计。本文介绍了PolyNet，一种通过学习互补解决策略来改善解空间探索的方法。与其他作品不同，PolyNet仅使用单个解码器，并且训练图式不通过人为规则强制实施多样化解生成。我们在四个组合优化问题上评估PolyNet，并观察到隐式多样性机制允许P

    arXiv:2402.14048v1 Announce Type: cross  Abstract: Reinforcement learning-based methods for constructing solutions to combinatorial optimization problems are rapidly approaching the performance of human-designed algorithms. To further narrow the gap, learning-based approaches must efficiently explore the solution space during the search process. Recent approaches artificially increase exploration by enforcing diverse solution generation through handcrafted rules, however, these rules can impair solution quality and are difficult to design for more complex problems. In this paper, we introduce PolyNet, an approach for improving exploration of the solution space by learning complementary solution strategies. In contrast to other works, PolyNet uses only a single-decoder and a training schema that does not enforce diverse solution generation through handcrafted rules. We evaluate PolyNet on four combinatorial optimization problems and observe that the implicit diversity mechanism allows P
    
[^141]: 简单而有效的神经符号一体化迁移学习

    Simple and Effective Transfer Learning for Neuro-Symbolic Integration

    [https://arxiv.org/abs/2402.14047](https://arxiv.org/abs/2402.14047)

    提出了一种简单而有效的方法，通过在下游任务上预训练神经模型，然后通过迁移学习在相同任务上对NeSy模型进行训练，以实现神经符号一体化的改进。

    

    深度学习技术近年来取得了显著成功。然而，它们在泛化和执行推理任务方面的能力仍然是一个挑战。本文提出了一种简单而有效的方法来改善这些问题，该方法涉及在下游任务上预训练神经模型，然后通过迁移学习在相同任务上对NeSy模型进行训练，其中利用神经网络将感知映射到符号，并利用逻辑推理者预测下游任务的输出。

    arXiv:2402.14047v1 Announce Type: cross  Abstract: Deep Learning (DL) techniques have achieved remarkable successes in recent years. However, their ability to generalize and execute reasoning tasks remains a challenge. A potential solution to this issue is Neuro-Symbolic Integration (NeSy), where neural approaches are combined with symbolic reasoning. Most of these methods exploit a neural network to map perceptions to symbols and a logical reasoner to predict the output of the downstream task. These methods exhibit superior generalization capacity compared to fully neural architectures. However, they suffer from several issues, including slow convergence, learning difficulties with complex perception tasks, and convergence to local minima. This paper proposes a simple yet effective method to ameliorate these problems. The key idea involves pretraining a neural model on the downstream task. Then, a NeSy model is trained on the same task via transfer learning, where the weights of the p
    
[^142]: 在医学成像中推进低秩和局部低秩矩阵逼近：系统文献综述与未来方向

    Advancing Low-Rank and Local Low-Rank Matrix Approximation in Medical Imaging: A Systematic Literature Review and Future Directions

    [https://arxiv.org/abs/2402.14045](https://arxiv.org/abs/2402.14045)

    本文系统综述了在医学成像中应用低秩矩阵逼近（LRMA）和其派生物局部LRMA（LLRMA）的作品，并指出自2015年以来医学成像领域开始偏向于使用LLRMA，显示其在捕获医学数据中复杂结构方面的潜力和有效性。

    

    医学成像数据集的大容量和复杂性是存储、传输和处理的瓶颈。为解决这些挑战，低秩矩阵逼近（LRMA）及其派生物局部LRMA（LLRMA）的应用已显示出潜力。本文进行了系统文献综述，展示了在医学成像中应用LRMA和LLRMA的作品。文献的详细分析确认了应用于各种成像模态的LRMA和LLRMA方法。本文解决了现有LRMA和LLRMA方法所面临的挑战和限制。我们注意到，自2015年以来，医学成像领域明显偏向于LLRMA，显示了相对于LRMA在捕获医学数据中复杂结构方面的潜力和有效性。鉴于LLRMA所使用的浅层相似性方法的限制，我们建议使用先进语义图像分割来处理相似性。

    arXiv:2402.14045v1 Announce Type: cross  Abstract: The large volume and complexity of medical imaging datasets are bottlenecks for storage, transmission, and processing. To tackle these challenges, the application of low-rank matrix approximation (LRMA) and its derivative, local LRMA (LLRMA) has demonstrated potential.   This paper conducts a systematic literature review to showcase works applying LRMA and LLRMA in medical imaging. A detailed analysis of the literature identifies LRMA and LLRMA methods applied to various imaging modalities. This paper addresses the challenges and limitations associated with existing LRMA and LLRMA methods.   We note a significant shift towards a preference for LLRMA in the medical imaging field since 2015, demonstrating its potential and effectiveness in capturing complex structures in medical data compared to LRMA. Acknowledging the limitations of shallow similarity methods used with LLRMA, we suggest advanced semantic image segmentation for similarit
    
[^143]: 使用GANs生成合成数据延伸与保护——基于时间序列医疗记录

    Protect and Extend -- Using GANs for Synthetic Data Generation of Time-Series Medical Records

    [https://arxiv.org/abs/2402.14042](https://arxiv.org/abs/2402.14042)

    本研究使用GANs生成了时间序列合成痴呆患者医疗记录的数据集，并比较了不同GAN模型在生成合成数据方面的质量，实现了在不涉及隐私问题的情况下保护用户数据并延伸数据应用。

    

    arXiv:2402.14042v1 公告类型:交叉摘要: 保护私人用户数据对于高质量体验(QoE)和可接受性至关重要，尤其是对于处理敏感数据的服务，如基于IT的健康服务。尽管已经显示匿名化技术容易被数据重新识别，但合成数据生成逐渐取代了匿名化，因为它相对耗时和资源耗费较少，并且更能抵抗数据泄漏。生成对抗网络(GANs)已被用于生成合成数据集，特别是遵循差分隐私现象的GAN框架。本研究比较了用于生成时间序列合成痴呆患者医疗记录的最新GAN基模型，这些数据可以在不涉及隐私问题的情况下分发。 预测建模、自相关性和分布分析被用来评估生成数据的生成质量(QoG)。

    arXiv:2402.14042v1 Announce Type: cross  Abstract: Preservation of private user data is of paramount importance for high Quality of Experience (QoE) and acceptability, particularly with services treating sensitive data, such as IT-based health services. Whereas anonymization techniques were shown to be prone to data re-identification, synthetic data generation has gradually replaced anonymization since it is relatively less time and resource-consuming and more robust to data leakage. Generative Adversarial Networks (GANs) have been used for generating synthetic datasets, especially GAN frameworks adhering to the differential privacy phenomena. This research compares state-of-the-art GAN-based models for synthetic data generation to generate time-series synthetic medical records of dementia patients which can be distributed without privacy concerns. Predictive modeling, autocorrelation, and distribution analysis are used to assess the Quality of Generating (QoG) of the generated data. T
    
[^144]: E2USD：用于多元时间序列的高效而有效的无监督状态检测

    E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series

    [https://arxiv.org/abs/2402.14041](https://arxiv.org/abs/2402.14041)

    E2USD提出了一种有效的无监督多元时间序列状态检测方法，利用了快速傅里叶变换和双视图嵌入模块进行编码，以及通过对抗学习方法消除假阴性，从而实现了SOTA准确性并显著降低了计算开销。

    

    我们提出了E2USD方法，能够实现高效而准确的无监督多元时间序列状态检测。E2USD利用基于快速傅立叶变换的时间序列压缩器(FFTCompress)和分解的双视图嵌入模块(DDEM)，一起以低计算开销对输入的多元时间序列进行编码。此外，我们提出了一种假阴性取消对比学习方法(FNCCLearning)，以抵消假阴性的影响，并实现更友好的簇嵌入空间。为了在流式设置中进一步减少计算开销，我们引入了自适应阈值检测(ADATD)。通过使用六个基线模型和六个数据集进行全面实验，我们证明E2USD能够在显著降低计算开销的情况下达到SOTA的准确性。我们的代码可在https://github.com/AI4CTS/E2Usd 找到。

    arXiv:2402.14041v1 Announce Type: cross  Abstract: We propose E2USD that enables efficient-yet-accurate unsupervised MTS state detection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor (FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together encode input MTSs at low computational overhead. Additionally, we propose a False Negative Cancellation Contrastive Learning method (FNCCLearning) to counteract the effects of false negatives and to achieve more cluster-friendly embedding spaces. To reduce computational overhead further in streaming settings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive experiments with six baselines and six datasets offer evidence that E2USD is capable of SOTA accuracy at significantly reduced computational overhead. Our code is available at https://github.com/AI4CTS/E2Usd.
    
[^145]: 在高度不平衡的多类分布情境下的远程医疗专业检测

    Specialty detection in the context of telemedicine in a highly imbalanced multi-class distribution

    [https://arxiv.org/abs/2402.14039](https://arxiv.org/abs/2402.14039)

    提出基于机器学习模型的专业检测分类器，用于自动化检测每个问题的正确专业并将其路由到正确的医生，重点是处理阿拉伯医疗问题的多类别和高度不平衡数据集。

    

    Covid-19大流行导致了对远程医疗服务的认识和需求增加，进而需要自动化流程，并依赖机器学习（ML）来减少运营负担。本研究提出了一种基于机器学习模型的专业检测分类器，用于自动化检测每个问题的正确专业并将其路由到正确的医生。该研究专注于处理阿拉伯医疗问题的多类别和高度不平衡数据集，比较了一些过采样技术，开发了一种用于专业检测的深度神经网络（DNN）模型，并探讨了依赖于专业检测的隐藏业务领域，例如为不同专业定制和个性化咨询流程等。

    arXiv:2402.14039v1 Announce Type: cross  Abstract: The Covid-19 pandemic has led to an increase in the awareness of and demand for telemedicine services, resulting in a need for automating the process and relying on machine learning (ML) to reduce the operational load. This research proposes a specialty detection classifier based on a machine learning model to automate the process of detecting the correct specialty for each question and routing it to the correct doctor. The study focuses on handling multiclass and highly imbalanced datasets for Arabic medical questions, comparing some oversampling techniques, developing a Deep Neural Network (DNN) model for specialty detection, and exploring the hidden business areas that rely on specialty detection such as customizing and personalizing the consultation flow for different specialties. The proposed module is deployed in both synchronous and asynchronous medical consultations to provide more real-time classification, minimize the doctor 
    
[^146]: 委员会的智慧：从基础模型到专用应用模型的提取

    Wisdom of Committee: Distilling from Foundation Model to SpecializedApplication Model

    [https://arxiv.org/abs/2402.14035](https://arxiv.org/abs/2402.14035)

    将基础模型的知识转移到专用应用模型中存在挑战，提出了通过创建教学委员会来应对这些挑战。

    

    最近基础模型的进展在各种任务上取得了令人印象深刻的性能，与此同时，为特定应用，从业者们一直在开发专门的应用模型。为了享受这两种模型的好处，一个自然的路径是将基础模型中的知识转移到专用应用模型中，后者通常更有效地提供服务。知识蒸馏的技术可以在这里应用，其中应用模型学会模仿基础模型。然而，专用应用模型和基础模型在容量上存在实质性差距，采用不同的架构，使用来自不同模态的不同输入特征，并在不同的分布上进行优化。模型特征上的这些差异导致了蒸馏方法面临重大挑战。在这项工作中，我们提出创建一个教学委员会，包括基础模型和专用应用模型。

    arXiv:2402.14035v1 Announce Type: cross  Abstract: Recent advancements in foundation models have yielded impressive performance across a wide range of tasks. Meanwhile, for specific applications, practitioners have been developing specialized application models. To enjoy the benefits of both kinds of models, one natural path is to transfer the knowledge in foundation models into specialized application models, which are generally more efficient for serving. Techniques from knowledge distillation may be applied here, where the application model learns to mimic the foundation model. However, specialized application models and foundation models have substantial gaps in capacity, employing distinct architectures, using different input features from different modalities, and being optimized on different distributions. These differences in model characteristics lead to significant challenges for distillation methods. In this work, we propose creating a teaching committee comprising both foun
    
[^147]: VN网络：利用虚拟邻居嵌入新出现的实体

    VN Network: Embedding Newly Emerging Entities with Virtual Neighbors

    [https://arxiv.org/abs/2402.14033](https://arxiv.org/abs/2402.14033)

    提出了一个名为虚拟邻居（VN）网络的新框架，以解决实体嵌入中的邻居稀疏问题，并有效整合远距离信息。

    

    将实体和关系嵌入到连续向量空间中引起了近年来的大量关注。大多数嵌入方法假定所有测试实体在训练期间均可获得，这使得为新出现的实体重新训练嵌入变得耗时。为解决这一问题，最近的研究将图神经网络应用于未知实体的现有邻居。本文提出了一种新颖的框架，即虚拟邻居（VN）网络，以解决三个关键挑战。首先，为了减少邻居稀疏问题，我们引入了通过规则推断得出的虚拟邻居的概念。我们通过解决一个受规则限制的问题为这些邻居分配软标签，而不是简单地将它们视为毫不含糊的真实。其次，许多现有方法仅使用一跳或两跳邻居进行聚合，并忽略可能有用的远距离信息。相反，我们识别了逻辑和...

    arXiv:2402.14033v1 Announce Type: cross  Abstract: Embedding entities and relations into continuous vector spaces has attracted a surge of interest in recent years. Most embedding methods assume that all test entities are available during training, which makes it time-consuming to retrain embeddings for newly emerging entities. To address this issue, recent works apply the graph neural network on the existing neighbors of the unseen entities. In this paper, we propose a novel framework, namely Virtual Neighbor (VN) network, to address three key challenges. Firstly, to reduce the neighbor sparsity problem, we introduce the concept of the virtual neighbors inferred by rules. And we assign soft labels to these neighbors by solving a rule-constrained problem, rather than simply regarding them as unquestionably true. Secondly, many existing methods only use one-hop or two-hop neighbors for aggregation and ignore the distant information that may be helpful. Instead, we identify both logic an
    
[^148]: 具有有序方差的自编码器用于非线性模型识别

    Autoencoder with Ordered Variance for Nonlinear Model Identification

    [https://arxiv.org/abs/2402.14031](https://arxiv.org/abs/2402.14031)

    提出了一种具有有序方差的自编码器，通过添加方差正则化项来保持潜空间的顺序，并且在无监督设置中展示了其在提取非线性关系方面的有效性。

    

    本文提出了一种新颖的具有有序方差（AEO）的自编码器，其中通过修改损失函数，添加方差正则化项以强制在潜空间中保持顺序。此外，通过使用ResNets对自编码器进行修改，得到了一个ResNet AEO（RAEO）。该论文还展示了AEO和RAEO在无监督设置下提取输入变量之间的非线性关系的有效性。

    arXiv:2402.14031v1 Announce Type: cross  Abstract: This paper presents a novel autoencoder with ordered variance (AEO) in which the loss function is modified with a variance regularization term to enforce order in the latent space. Further, the autoencoder is modified using ResNets, which results in a ResNet AEO (RAEO). The paper also illustrates the effectiveness of AEO and RAEO in extracting nonlinear relationships among input variables in an unsupervised setting.
    
[^149]: 冻结网络中的部分搜索足以找到强大的彩票票证

    Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket

    [https://arxiv.org/abs/2402.14029](https://arxiv.org/abs/2402.14029)

    提出一种方法，通过冻结随机子集的初始权重来减少强大的彩票票证（SLT）搜索空间，从而独立于所需SLT稀疏性降低了SLT搜索空间，保证了SLT在这种减少搜索空间中的存在。

    

    arXiv:2402.14029v1 公告类型：跨越 摘要：随机初始化的稠密网络包含可以在不进行权重学习的情况下实现高准确度的子网络--强大的彩票票证（SLTs）。最近，Gadhikar等人（2023年）在理论和实验证明，SLTs也可以在随机修剪的源网络中找到，从而减少SLT的搜索空间。然而，这限制了对甚至比源网络更稀疏的SLTs的搜索，导致由于意外的高稀疏性而准确度较差。本文提出了一种通过独立于所需SLT稀疏性的任意比率减少SLT搜索空间的方法。通过冻结一部分初始权重的随机子集，将其排除在搜索空间之外--即，通过永久修剪它们或将它们锁定为SLT的固定部分。事实上，通过我们与随机冻结变量的子集和逼近，在这种减少的搜索空间中，SLT的存在在理论上是得到保证的。除此之外，还可以减少...

    arXiv:2402.14029v1 Announce Type: cross  Abstract: Randomly initialized dense networks contain subnetworks that achieve high accuracy without weight learning -- strong lottery tickets (SLTs). Recently, Gadhikar et al. (2023) demonstrated theoretically and experimentally that SLTs can also be found within a randomly pruned source network, thus reducing the SLT search space. However, this limits the search to SLTs that are even sparser than the source, leading to worse accuracy due to unintentionally high sparsity. This paper proposes a method that reduces the SLT search space by an arbitrary ratio that is independent of the desired SLT sparsity. A random subset of the initial weights is excluded from the search space by freezing it -- i.e., by either permanently pruning them or locking them as a fixed part of the SLT. Indeed, the SLT existence in such a reduced search space is theoretically guaranteed by our subset-sum approximation with randomly frozen variables. In addition to reducin
    
[^150]: 学习因果事件组合序列

    Learning causation event conjunction sequences

    [https://arxiv.org/abs/2402.14027](https://arxiv.org/abs/2402.14027)

    这项研究探讨了学习事件序列中因果关系的方法，发现注意力循环ANN表现最佳，直方图算法明显优于其他ANNs。

    

    这篇论文探讨了一些学习事件序列中因果关系的方法。一个因果关系被定义为一个或多个因果事件的组合，以任意顺序发生，并可能有非因果事件介入，最终导致一个效果。这些方法包括循环和非循环的人工神经网络（ANNs），以及基于直方图的算法。其中，注意力循环ANN表现最佳，而直方图算法明显优于所有ANNs。

    arXiv:2402.14027v1 Announce Type: new  Abstract: This is an examination of some methods that learn causations in event sequences. A causation is defined as a conjunction of one or more cause events occurring in an arbitrary order, with possible intervening non-causal events, that lead to an effect. The methods include recurrent and non-recurrent artificial neural networks (ANNs), as well as a histogram-based algorithm. An attention recurrent ANN performed the best of the ANNs, while the histogram algorithm was significantly superior to all the ANNs.
    
[^151]: 使用配对数据对口内X光中牙齿异常检测的深度学习算法进行统计验证

    Statistical validation of a deep learning algorithm for dental anomaly detection in intraoral radiographs using paired data

    [https://arxiv.org/abs/2402.14022](https://arxiv.org/abs/2402.14022)

    该研究通过统计分析验证了一种深度学习算法在口内X光中检测牙齿异常的有效性，平均敏感性显著提高，而平均特异性略有下降

    

    本文描述了一项临床验证研究设置，针对口内X线图像中检测牙齿异常的深度学习算法进行的统计分析和结果，具体包括龋齿、根尖病变、根管治疗缺陷、冠修复边缘缺陷、牙周骨质流失和牙石。该研究比较了使用深度学习算法的牙医的检测性能与这些牙医在没有算法帮助下评估图像时的先前表现。通过计算配对图像数据的边际利润和性能损失，可以量化假设的敏感性和特异性变化。使用McNemar检验和二项假设检验广泛证明了这些结果的统计显著性。平均敏感性从60.7%增加到85.9%，而平均特异性略有降低，从$9

    arXiv:2402.14022v1 Announce Type: cross  Abstract: This article describes the clinical validation study setup, statistical analysis and results for a deep learning algorithm which detects dental anomalies in intraoral radiographic images, more specifically caries, apical lesions, root canal treatment defects, marginal defects at crown restorations, periodontal bone loss and calculus. The study compares the detection performance of dentists using the deep learning algorithm to the prior performance of these dentists evaluating the images without algorithmic assistance. Calculating the marginal profit and loss of performance from the annotated paired image data allows for a quantification of the hypothesized change in sensitivity and specificity. The statistical significance of these results is extensively proven using both McNemar's test and the binomial hypothesis test. The average sensitivity increases from $60.7\%$ to $85.9\%$, while the average specificity slightly decreases from $9
    
[^152]: 重新审视AdaGrad在宽松假设下的收敛性

    Revisiting Convergence of AdaGrad with Relaxed Assumptions

    [https://arxiv.org/abs/2402.13794](https://arxiv.org/abs/2402.13794)

    重新审视了AdaGrad在非凸光滑优化问题上的收敛性，提出了通用噪声模型，得出了概率收敛速度，无需先验知识，且可以在噪声参数足够小时加速至更快的速度。

    

    在这项研究中，我们重新审视了AdaGrad在非凸光滑优化问题上的收敛性，包括AdaGrad作为一种特殊情况。我们考虑了一个通用的噪声模型，其中噪声的大小由函数值差和梯度大小控制。这个模型涵盖了广泛范围的噪声，包括有界噪声、次高斯噪声、仿射方差噪声和预期光滑度，并且在许多实际应用中被证明更加现实。我们的分析得出了一个概率收敛速度，根据通用噪声，可以达到( \tilde{\mathcal{O}}(1/\sqrt{T}))。这个速度不依赖于先前对问题参数的了解，当与函数值差和噪声水平相关的参数足够小时，它可以加速到(\tilde{\mathcal{O}}(1/T))，其中(T)表示总迭代次数。收敛速度因此匹配了下限速度。

    arXiv:2402.13794v1 Announce Type: cross  Abstract: In this study, we revisit the convergence of AdaGrad with momentum (covering AdaGrad as a special case) on non-convex smooth optimization problems. We consider a general noise model where the noise magnitude is controlled by the function value gap together with the gradient magnitude. This model encompasses a broad range of noises including bounded noise, sub-Gaussian noise, affine variance noise and the expected smoothness, and it has been shown to be more realistic in many practical applications. Our analysis yields a probabilistic convergence rate which, under the general noise, could reach at (\tilde{\mathcal{O}}(1/\sqrt{T})). This rate does not rely on prior knowledge of problem-parameters and could accelerate to (\tilde{\mathcal{O}}(1/T)) where (T) denotes the total number iterations, when the noise parameters related to the function value gap and noise level are sufficiently small. The convergence rate thus matches the lower rat
    
[^153]: 离线策略学习的深度生成模型：教程、调查和未来方向展望

    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions

    [https://arxiv.org/abs/2402.13777](https://arxiv.org/abs/2402.13777)

    深度生成模型在离线策略学习中展现了巨大潜力，本文提供了首个系统性综述，涵盖了五种主流深度生成模型及其应用。

    

    深度生成模型(DGMs)在各个领域展示了巨大成功，特别是在使用从离线数据训练的模型生成文本、图像和视频方面。类似地，基于数据驱动的决策和机器人控制也需要从离线数据中学习一个生成函数作为策略或政策。在这种情况下，将深度生成模型应用于离线策略学习展现出巨大潜力，许多研究在这个方向上进行了探索。然而，这一领域仍然缺乏全面的评估，因此不同分支的发展相对独立。因此，我们提供了深度生成模型在离线策略学习应用方面的第一次系统性综述。具体而言，我们涵盖了五种主流深度生成模型，包括变分自动编码器、生成对抗网络、归一化流、变压器和扩散模型，以及它们的应用。

    arXiv:2402.13777v1 Announce Type: cross  Abstract: Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applicati
    
[^154]: DSLR：多样性增强和结构学习用于基于重播的图持续学习

    DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning

    [https://arxiv.org/abs/2402.13711](https://arxiv.org/abs/2402.13711)

    DSLR提出了一种基于覆盖范围的多样性方法，以解决基于重播的图持续学习中回放节点过于集中导致过拟合和灾难性遗忘的问题。

    

    我们研究了基于重播方法中回放缓冲区对图持续学习（GCL）方法的影响。现有的基于重播的GCL方法为每个类别选择最具代表性的节点并将它们存储在重播缓冲区中，以供在训练后续任务时使用。然而，我们发现，仅考虑每个回放节点的类别代表性会使回放节点集中在每个类别的中心周围，可能存在过拟合于位于那些区域的节点的风险，从而加剧灾难性遗忘。此外，由于基于重播方法严重依赖于少数回放节点来保留从先前任务中获得的知识，涉及在模型训练中具有不相关邻居的回放节点可能对模型性能产生显着的负面影响。在本文中，我们提出了一种名为DSLR的GCL模型，具体来说，我们设计了一种基于覆盖范围的多样性（CD）

    arXiv:2402.13711v1 Announce Type: cross  Abstract: We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD)
    
[^155]: 深度神经网络在微多普勒雷达分类中的鲁棒性

    Robustness of Deep Neural Networks for Micro-Doppler Radar Classification

    [https://arxiv.org/abs/2402.13651](https://arxiv.org/abs/2402.13651)

    评估了两种深度卷积架构的鲁棒性，发现在训练中加入对抗样本和时间增强样本可以改善模型的泛化能力

    

    随着深度分类器在雷达数据处理方面的巨大能力，学习特定于数据集的特征而无法很好泛化的风险也随之而来。在这项工作中，评估了两种深度卷积架构在相同数据上训练和测试时的鲁棒性。当遵循标准训练实践时，两个分类器都展现出对输入表示的微小时间偏移的敏感性，这种增强带有最小的语义内容。此外，模型极易受到对抗样本的影响。小的时间偏移和对抗样本都是模型过拟合于无法很好泛化的特征的结果。为了解决这个问题，表明在对抗样本和时间增强样本上进行训练可以减少这种影响，进而导致更好泛化的模型。最后，演示了操作在节奏-速度图表示而不是多普勒-时间的模型。

    arXiv:2402.13651v1 Announce Type: cross  Abstract: With the great capabilities of deep classifiers for radar data processing come the risks of learning dataset-specific features that do not generalize well. In this work, the robustness of two deep convolutional architectures, trained and tested on the same data, is evaluated. When standard training practice is followed, both classifiers exhibit sensitivity to subtle temporal shifts of the input representation, an augmentation that carries minimal semantic content. Furthermore, the models are extremely susceptible to adversarial examples. Both small temporal shifts and adversarial examples are a result of a model overfitting on features that do not generalize well. As a remedy, it is shown that training on adversarial examples and temporally augmented samples can reduce this effect and lead to models that generalise better. Finally, models operating on cadence-velocity diagram representation rather than Doppler-time are demonstrated to 
    
[^156]: 基础设施调解员：从结构灾难响应中挖掘未来失效担忧

    Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response

    [https://arxiv.org/abs/2402.13528](https://arxiv.org/abs/2402.13528)

    本文开发了一种基础设施调解员系统，用于自动检测特定基础设施问题，通过挖掘社交网络中关于预期失败的担忧，有助于预防和减轻潜在的基础设施失败。

    

    当前研究集中于研究社交媒体上与结构失败相关的讨论，以改进灾难响应策略。然而，检测社交网络帖子中讨论关于预期失败的担忧是未被充分探索的。如果这些担忧被传达给适当的机构，可以帮助预防和减轻潜在的基础设施失败。本文中，我们开发了一种基础设施调解员——用于自动检测特定基础设施问题。我们的工作考虑了美国几起最近的结构失效事件。我们呈现了一份首创性数据集，包括从Reddit和YouTube中挖掘的2,662个社交网络实例，用于这一新颖任务。

    arXiv:2402.13528v1 Announce Type: cross  Abstract: Current research concentrates on studying discussions on social media related to structural failures to improve disaster response strategies. However, detecting social web posts discussing concerns about anticipatory failures is under-explored. If such concerns are channeled to the appropriate authorities, it can aid in the prevention and mitigation of potential infrastructural failures. In this paper, we develop an infrastructure ombudsman -- that automatically detects specific infrastructure concerns. Our work considers several recent structural failures in the US. We present a first-of-its-kind dataset of 2,662 social web instances for this novel task mined from Reddit and YouTube.
    
[^157]: Transformer 技巧：预计算第一层

    Transformer tricks: Precomputing the first layer

    [https://arxiv.org/abs/2402.13388](https://arxiv.org/abs/2402.13388)

    该论文描述了一种加速具有RoPE的transformer推断的技巧，通过预计算第一层来降低延迟和成本，最大节省取决于总层数。

    

    这篇简短的论文描述了一种加速具有 RoPE（如 LLaMA、Mistral 和 PaLM）的 transformer 推断的技巧。对于这些模型，第一个 transformer 层的大部分内容可以预先计算，从而导致稍低的延迟和更低的每令牌成本。因为这种技巧仅优化了一层，相对节省取决于总层数。例如，对于只有 4 层的模型（如 Whisper tiny），最大节省仅限于 25%，而对于 32 层模型（如 Mistral-7B），节省则是 3%。

    arXiv:2402.13388v1 Announce Type: new  Abstract: This short paper describes a trick to speed up inference of transformers with RoPE (such as LLaMA, Mistral, and PaLM). For these models, a large portion of the first transformer layer can be precomputed, which results in slightly lower latency and lower cost-per-token. Because this trick optimizes only one layer, the relative savings depend on the total number of layers. For example, the maximum savings for a model with only 4 layers (such as Whisper tiny) is limited to 25%, while a 32-layer model (such as Mistral-7B) is limited to 3% savings.
    
[^158]: KetGPT -- 使用Transformer对量子电路进行数据增强

    KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers

    [https://arxiv.org/abs/2402.13352](https://arxiv.org/abs/2402.13352)

    该研究利用Transformer机器学习架构生成“看起来真实”的量子电路，以增强现有的量子电路数据集。

    

    量子算法，表示为量子电路，可用作评估量子系统性能的基准。现有数据集在规模和多样性方面存在限制，在该领域广泛使用，导致研究人员使用随机生成的电路。然而，随机电路并不是代表性基准，因为它们缺乏量子系统制造的真实量子算法的固有属性。这种缺乏“有用”的量子基准构成了推动量子编译器和硬件开发与比较的挑战。本研究旨在通过使用Transformer机器学习架构生成我们称之为“看起来真实”的电路，以增强现有的量子电路数据集。为此，我们引入了KetGPT，一种以OpenQASM语言生成合成电路的工具，其结构是基于推导自量子电路的

    arXiv:2402.13352v1 Announce Type: cross  Abstract: Quantum algorithms, represented as quantum circuits, can be used as benchmarks for assessing the performance of quantum systems. Existing datasets, widely utilized in the field, suffer from limitations in size and versatility, leading researchers to employ randomly generated circuits. Random circuits are, however, not representative benchmarks as they lack the inherent properties of real quantum algorithms for which the quantum systems are manufactured. This shortage of `useful' quantum benchmarks poses a challenge to advancing the development and comparison of quantum compilers and hardware.   This research aims to enhance the existing quantum circuit datasets by generating what we refer to as `realistic-looking' circuits by employing the Transformer machine learning architecture. For this purpose, we introduce KetGPT, a tool that generates synthetic circuits in OpenQASM language, whose structure is based on quantum circuits derived f
    
[^159]: 在奇异性下的学习：改进WBIC和sBIC的信息准则

    Learning under Singularity: An Information Criterion improving WBIC and sBIC

    [https://arxiv.org/abs/2402.12762](https://arxiv.org/abs/2402.12762)

    LS信息准则旨在增强WBIC和sBIC的功能，有效处理非正则情况，具有稳定性，为奇异情况下的信息准则提供了新的方法

    

    我们介绍了一种新颖的信息准则（IC），称为在奇异性下的学习（LS），旨在增强广泛适用的贝叶斯信息准则（WBIC）和奇异贝叶斯信息准则（sBIC）的功能。 LS在没有正则性约束的情况下是有效的，并表现出稳定性。Watanabe定义了一个统计模型或学习机器为正则，如果从参数到概率分布的映射是一对一的，并且其Fisher信息矩阵是正定的。相反，不符合这些条件的模型被称为奇异。 在过去的十年中，已经提出了几种奇异情况下的信息准则，包括WBIC和sBIC。 WBIC适用于非正则情况，但在样本量很大且已知学习系数估计冗余时面临挑战。 相反，sBIC在广泛应用方面存在限制，因为它依赖于最大似然估计。

    arXiv:2402.12762v1 Announce Type: cross  Abstract: We introduce a novel Information Criterion (IC), termed Learning under Singularity (LS), designed to enhance the functionality of the Widely Applicable Bayes Information Criterion (WBIC) and the Singular Bayesian Information Criterion (sBIC). LS is effective without regularity constraints and demonstrates stability. Watanabe defined a statistical model or a learning machine as regular if the mapping from a parameter to a probability distribution is one-to-one and its Fisher information matrix is positive definite. In contrast, models not meeting these conditions are termed singular. Over the past decade, several information criteria for singular cases have been proposed, including WBIC and sBIC. WBIC is applicable in non-regular scenarios but faces challenges with large sample sizes and redundant estimation of known learning coefficients. Conversely, sBIC is limited in its broader application due to its dependence on maximum likelihood
    
[^160]: 表格作为图片？探讨LLM在多模态表格数据表示上的优势和局限性

    Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data

    [https://arxiv.org/abs/2402.12424](https://arxiv.org/abs/2402.12424)

    本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。

    

    在本文中，我们通过不同的提示策略和数据格式研究了各种LLM在解释表格数据方面的有效性。我们的分析涵盖了六个针对与表格相关任务的基准，如问答和事实核查。我们首次介绍了LLM在基于图像的表格表示上的表现评估。具体地，我们比较了五种基于文本和三种基于图像的表格表示，展示了表示和提示对LLM性能的影响。我们的研究为在表格相关任务上有效使用LLM提供了见解。

    arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
    
[^161]: 张量时间序列的动态多网络挖掘

    Dynamic Multi-Network Mining of Tensor Time Series

    [https://arxiv.org/abs/2402.11773](https://arxiv.org/abs/2402.11773)

    提出了一种新方法，Dynamic Multi-network Mining (DMM)，能够将张量时间序列转换为不同长度的段组，通过稀疏依赖网络提供聚类的可解释性和精确性。

    

    时间序列的子序列聚类是数据挖掘中的一个重要任务，解释结果聚类也至关重要，因为通常我们没有关于数据的先验知识。因此，面对由包含时间戳在内的多种模式组成的大量张量时间序列，我们如何为张量时间序列实现子序列聚类并提供可解释的见解？在本文中，我们提出了一种新方法，即动态多网络挖掘（DMM），它将张量时间序列转换为由l1范数约束的一组各种长度的段组（即聚类）特征化的依赖网络。我们的方法具有以下特性。(a) 可解释性：它使用多个网络对聚类进行特征描述，每个网络是相应非时间模式的稀疏依赖网络，从而提供可见且可解释的关键关系见解。 (b) 精确性：它发现了聚类。。。

    arXiv:2402.11773v1 Announce Type: cross  Abstract: Subsequence clustering of time series is an essential task in data mining, and interpreting the resulting clusters is also crucial since we generally do not have prior knowledge of the data. Thus, given a large collection of tensor time series consisting of multiple modes, including timestamps, how can we achieve subsequence clustering for tensor time series and provide interpretable insights? In this paper, we propose a new method, Dynamic Multi-network Mining (DMM), that converts a tensor time series into a set of segment groups of various lengths (i.e., clusters) characterized by a dependency network constrained with l1-norm. Our method has the following properties. (a) Interpretable: it characterizes the cluster with multiple networks, each of which is a sparse dependency network of a corresponding non-temporal mode, and thus provides visible and interpretable insights into the key relationships. (b) Accurate: it discovers the clus
    
[^162]: 用RLHF推进翻译偏好建模：迈向成本效益解决方案

    Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution

    [https://arxiv.org/abs/2402.11525](https://arxiv.org/abs/2402.11525)

    提出了一种利用强化学习和人类反馈（RLHF）来改进翻译质量的成本效益偏好学习策略，该策略通过优化奖励模型来区分人类和机器翻译，从而指导改进机器翻译。

    

    arXiv:2402.11525v1 公告类型：新 真实性、表达力和优雅是机器翻译中不断追求的目标。然而，传统的度量标准如BLEU并不严格符合人类对翻译质量的偏好。本文探讨了利用强化学习与人类反馈（RLHF）来提高翻译质量。收集人类对翻译之间的比较的大规模高质量数据集并不容易，尤其对于低资源语言。为解决这一问题，我们提出了一种成本效益的偏好学习策略，通过区分人类和机器翻译来优化奖励模型。通过这种方式，奖励模型学习机器翻译与人类之间的不足之处，并指导随后对机器翻译的改进。实验结果表明，RLHF可以有效地提升翻译质量，这种改进也有益于其他翻译方向。

    arXiv:2402.11525v1 Announce Type: new  Abstract: Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions 
    
[^163]: 加速半异步联邦学习

    Accelerating Semi-Asynchronous Federated Learning

    [https://arxiv.org/abs/2402.10991](https://arxiv.org/abs/2402.10991)

    提出了一种考虑贡献的异步联邦学习方法，动态调整接收到的更新的处理方式，以解决现实情况下同步上传数据可能出现的缓慢和不可靠问题。

    

    联邦学习（FL）是一种分布式机器学习范例，允许客户端在保护隐私的同时在其数据上训练模型。现有的FL算法，如Federated Averaging（FedAvg）及其变种，在许多情况下已经被证明收敛良好。然而，这些方法需要客户端以同步方式将其本地更新上传至服务器，这在现实情况下可能会变得缓慢和不可靠。为了解决这个问题，研究人员开发了异步FL方法，允许客户端继续使用陈旧的全局模型对其本地数据进行训练。然而，大多数这些方法仅仅聚合了所有接收到的更新，而没有考虑其相对贡献，这可能导致收敛速度变慢。在本文中，我们提出了一种考虑贡献的异步FL方法，考虑了接收到的更新的陈旧程度和统计异质性。我们的方法动态调整

    arXiv:2402.10991v1 Announce Type: cross  Abstract: Federated Learning (FL) is a distributed machine learning paradigm that allows clients to train models on their data while preserving their privacy. FL algorithms, such as Federated Averaging (FedAvg) and its variants, have been shown to converge well in many scenarios. However, these methods require clients to upload their local updates to the server in a synchronous manner, which can be slow and unreliable in realistic FL settings. To address this issue, researchers have developed asynchronous FL methods that allow clients to continue training on their local data using a stale global model. However, most of these methods simply aggregate all of the received updates without considering their relative contributions, which can slow down convergence. In this paper, we propose a contribution-aware asynchronous FL method that takes into account the staleness and statistical heterogeneity of the received updates. Our method dynamically adju
    
[^164]: BlackJAX: JAX中的组合式贝叶斯推断

    BlackJAX: Composable Bayesian inference in JAX

    [https://arxiv.org/abs/2402.10797](https://arxiv.org/abs/2402.10797)

    BlackJAX是一个实现在JAX中组合式贝叶斯推断的库，采用函数式方法提高易用性、速度和模块化，适用于需要尖端方法、研究人员和想要了解工作原理的人。

    

    BlackJAX是一个库，实现了在贝叶斯计算中常用的抽样和变分推断算法。它通过采用函数式方法实现算法，旨在提高易用性、速度和模块化。BlackJAX使用Python编写，利用JAX在CPU、GPU和TPU上编译和运行类似Numpy的抽样器和变分方法。该库通过直接处理（非正则化）目标对数密度函数，与概率编程语言很好地集成。BlackJAX旨在成为基本统计“基元”的低级可组合实现的集合，可组合执行定义良好的贝叶斯推断，同时还提供高级例程以提高易用性。它面向需要尖端方法的用户、希望创建复杂抽样方法的研究人员，以及想要了解这些方法工作原理的人。

    arXiv:2402.10797v1 Announce Type: cross  Abstract: BlackJAX is a library implementing sampling and variational inference algorithms commonly used in Bayesian computation. It is designed for ease of use, speed, and modularity by taking a functional approach to the algorithms' implementation. BlackJAX is written in Python, using JAX to compile and run NumpPy-like samplers and variational methods on CPUs, GPUs, and TPUs. The library integrates well with probabilistic programming languages by working directly with the (un-normalized) target log density function. BlackJAX is intended as a collection of low-level, composable implementations of basic statistical 'atoms' that can be combined to perform well-defined Bayesian inference, but also provides high-level routines for ease of use. It is designed for users who need cutting-edge methods, researchers who want to create complex sampling methods, and people who want to learn how these work.
    
[^165]: Brant-2：脑信号基础模型

    Brant-2: Foundation Model for Brain Signals

    [https://arxiv.org/abs/2402.10251](https://arxiv.org/abs/2402.10251)

    Brant-2是脑信号领域最大的基础模型，相比于Brant，它不仅对数据变化和建模尺度具有稳健性，还能适用于更广泛范围的脑神经数据。

    

    基础模型受益于在大量未标记数据上进行预训练，并且在少量标记数据的情况下能够在各种应用中表现出色。这种模型在分析脑信号方面特别有效，因为这一领域涵盖了众多应用场景，并且进行大规模注释是成本高昂的。在这项工作中，我们提出了脑信号领域最大的基础模型，Brant-2。与用于颅内神经信号的基础模型Brant相比，Brant-2不仅对数据变化和建模尺度表现出稳健性，而且可以应用于更广泛范围的脑神经数据。通过在大量任务上进行实验，我们展示了Brant-2对脑信号中各种应用场景的适应性。进一步分析揭示了Brant-2的可扩展性，验证了每个组件的有效性，并展示了我们模型保持的能力。

    arXiv:2402.10251v1 Announce Type: cross  Abstract: Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintai
    
[^166]: 说服一位学习代理

    Persuading a Learning Agent

    [https://arxiv.org/abs/2402.09721](https://arxiv.org/abs/2402.09721)

    在一个重复的贝叶斯说服问题中，即使没有承诺能力，委托人可以通过使用上下文无遗憾学习算法来实现与经典无学习模型中具有承诺的委托人的最优效用无限接近的效果；在代理人使用上下文无交换遗憾学习算法的情况下，委托人无法获得比具有承诺的无学习模型中的最优效用更高的效用。

    

    我们研究了一个重复的贝叶斯说服问题（更一般地，任何具有完全信息的广义委托-代理问题），其中委托人没有承诺能力，代理人使用算法来学习如何对委托人的信号做出响应。我们将这个问题简化为一个一次性的广义委托-代理问题，代理人近似地最佳响应。通过这个简化，我们可以证明：如果代理人使用上下文无遗憾学习算法，则委托人可以保证其效用与经典无学习模型中具有承诺的委托人的最优效用之间可以无限接近；如果代理人使用上下文无交换遗憾学习算法，则委托人无法获得比具有承诺的无学习模型中的最优效用更高的效用。委托人在学习模型与非学习模型中可以获得的效用之间的差距是有界的。

    arXiv:2402.09721v1 Announce Type: cross  Abstract: We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal's signals. We reduce this problem to a one-shot generalized principal-agent problem with an approximately-best-responding agent. This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal's optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment. The difference between the principal's obtainable utility in the learning model and the non-learning model is bound
    
[^167]: 知识图谱与多模态学习：综述

    Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey

    [https://arxiv.org/abs/2402.05391](https://arxiv.org/abs/2402.05391)

    知识图谱与多模态学习的综述介绍了KG4MM和MM4KG两个主要方面，包括任务定义、构建进展、评估基准以及关键研究轨迹。

    

    知识图谱在推动各种人工智能应用方面起着关键作用，语义网络社区对多模态维度的探索为创新打开了新的途径。在本综述中，我们仔细审查了300多篇文章，重点关注了两个主要方面的知识图谱感知研究：以知识图谱支持多模态任务的KG驱动多模态（KG4MM）学习，将知识图谱研究扩展到多模态知识图谱（MM4KG）领域。我们从定义知识图谱和多模态知识图谱开始，然后探索它们的构建进展。我们的综述包括两个主要任务类别：KG感知的多模态学习任务，如图像分类和视觉问答，以及内在的多模态知识图谱任务，如多模态知识图谱补全和实体对齐，突出了具体的研究轨迹。对于这些任务中的大部分，我们提供了定义、评估基准，并进一步指出进行相关研究的重要见解。最后，我们讨论了cu

    Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
    
[^168]: RAG-Fusion: 检索增强生成的新途径

    RAG-Fusion: a New Take on Retrieval-Augmented Generation

    [https://arxiv.org/abs/2402.03367](https://arxiv.org/abs/2402.03367)

    RAG-Fusion方法通过生成多个查询，并结合互惠排名融合技术，能够从不同角度上下文化原始查询，提供准确和全面的信息。这项研究在人工智能和自然语言处理应用方面有重要进展，并展示了全球和区域之间的转变。

    

    Infineon已经确定工程师、客户经理和客户迅速获取产品信息的需求。传统上，这个问题通过检索增强生成（RAG）聊天机器人来解决，但在这项研究中，我评估了新近流行的RAG-Fusion方法的使用。RAG-Fusion将RAG和互惠排名融合（RRF）相结合，通过生成多个查询，使用互惠分数对其进行再排序，并融合文档和分数。通过对准确性、相关性和全面性进行手动评估，我发现RAG-Fusion能够通过从不同的角度对原始查询进行上下文化，提供准确和全面的回答。然而，当生成的查询与原始查询的相关性不足时，有些答案偏离了主题。这项研究在人工智能（AI）和自然语言处理（NLP）应用方面取得了重要进展，并展示了全球和区域之间的变革。

    Infineon has identified a need for engineers, account managers, and customers to rapidly obtain product information. This problem is traditionally addressed with retrieval-augmented generation (RAG) chatbots, but in this study, I evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion combines RAG and reciprocal rank fusion (RRF) by generating multiple queries, reranking them with reciprocal scores and fusing the documents and scores. Through manually evaluating answers on accuracy, relevance, and comprehensiveness, I found that RAG-Fusion was able to provide accurate and comprehensive answers due to the generated queries contextualizing the original query from various perspectives. However, some answers strayed off topic when the generated queries' relevance to the original query is insufficient. This research marks significant progress in artificial intelligence (AI) and natural language processing (NLP) applications and demonstrates transformations in a global and m
    
[^169]: PuzzleBench：LLMs能否解决困难的一阶组合推理问题？

    PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?

    [https://arxiv.org/abs/2402.02611](https://arxiv.org/abs/2402.02611)

    本研究通过PuzzleBench数据集探索了LLMs解决困难的一阶组合推理问题的能力，并提出了Puzzle-LM方法，该方法将LLMs与符号求解器和程序解释器相结合，使其能够有效地推理这类问题。

    

    最近的研究探索了使用LLMs进行推理任务，重点是相对简单的问题，如逻辑问答。在我们的工作中，我们希望解决更复杂的问题，显著扩展这些模型的功能。特别是，我们探讨LLMs是否能够解决困难的一阶组合推理问题，一个例子是流行的数独谜题。这些问题有一个由自然语言描述的基础一阶结构，并且可以实例化为不同大小的实例。此外，这些问题在计算上是密集型的，需要多个推理步骤才能达到解决方案。我们提出了PuzzleBench，一个包含31个这样具有挑战性的谜题的数据集。我们观察到，即使在符号求解器的帮助下，LLMs在我们的基准测试中表现得相当糟糕。作为回应，我们提出了一种新的方法，Puzzle-LM，它将LLMs与符号求解器和程序解释器相结合，使它们能够推理这类问题。

    Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such
    
[^170]: DiffStitch: 使用基于扩散的轨迹拼接提升离线强化学习

    DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching

    [https://arxiv.org/abs/2402.02439](https://arxiv.org/abs/2402.02439)

    DiffStitch是一种使用基于扩散的轨迹拼接提升离线强化学习的方法。它通过有效地连接低奖励轨迹和高奖励轨迹，形成全局最优轨迹，以提高离线强化学习算法的性能。

    

    在离线强化学习中，学习策略的性能高度依赖于离线数据集的质量。然而，在许多情况下，离线数据集只包含了非常有限的最佳轨迹，这给离线强化学习算法带来了挑战，因为智能体必须获得到达高奖励区域的能力。为了解决这个问题，我们引入了基于扩散的轨迹拼接（DiffStitch），这是一个新颖的基于扩散的数据增强流水线，它可以系统地生成轨迹之间的拼接转换。DiffStitch可以有效地连接低奖励轨迹和高奖励轨迹，形成全局最优轨迹，以解决离线强化学习算法所面临的挑战。在D4RL数据集上进行的实证实验表明，DiffStitch在各种强化学习方法中都具有有效性。值得注意的是，DiffStitch在一步方法（IQL）、模仿学习方法（TD3+BC）和轨迹方法（PPO）的性能方面都有显著的改进。

    In offline reinforcement learning (RL), the performance of the learned policy highly depends on the quality of offline datasets. However, in many cases, the offline dataset contains very limited optimal trajectories, which poses a challenge for offline RL algorithms as agents must acquire the ability to transit to high-reward regions. To address this issue, we introduce Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data augmentation pipeline that systematically generates stitching transitions between trajectories. DiffStitch effectively connects low-reward trajectories with high-reward trajectories, forming globally optimal trajectories to address the challenges faced by offline RL algorithms. Empirical experiments conducted on D4RL datasets demonstrate the effectiveness of DiffStitch across RL methodologies. Notably, DiffStitch demonstrates substantial enhancements in the performance of one-step methods (IQL), imitation learning methods (TD3+BC), and traje
    
[^171]: 在大型语言模型中测量道德不一致性

    Measuring Moral Inconsistencies in Large Language Models

    [https://arxiv.org/abs/2402.01719](https://arxiv.org/abs/2402.01719)

    本研究提出了一种新的信息论度量方法，称为语义图熵（SGE），用于测量道德情景中大型语言模型（LLM）的一致性。与现有的一致性度量方法相比，SGE在五个LLMs上与人类判断更好地相关，为研究LLM不一致性的根本原因提供了新的思路。

    

    如果语义等价的提示产生语义等价的响应，那么大型语言模型(LLM)被认为是一致的。尽管最近的进展展示了LLMs在对话系统中令人印象深刻的能力，但我们表明即使是最先进的LLMs在生成方面也存在高度不一致性，这对它们的可靠性提出了质疑。先前的研究尝试用任务特定的准确度来衡量这一点。然而，这种方法对于没有“正确”答案的道德情景（例如，道路交运问题）是不合适的。为了解决这个问题，我们提出了一种新的信息论度量方法，称为语义图熵（SGE），来衡量LLM在道德情景中的一致性。我们利用“经验法则”（RoTs）来解释模型的决策策略，并进一步增强我们的度量方法。与现有的一致性度量方法相比，SGE与人类判断在五个LLMs上更好地相关。在未来，我们的目标是调查LLM不一致性的根本原因。

    A Large Language Model~(LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracies. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no ``correct'' answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy~(SGE) to measure the consistency of an LLM in moral scenarios. We leverage ``Rules of Thumb''~(RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies 
    
[^172]: HiGen: 层次感知的层级文本分类序列生成

    HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text Classification

    [https://arxiv.org/abs/2402.01696](https://arxiv.org/abs/2402.01696)

    HiGen提出了一个基于文本生成的框架，利用语言模型来编码动态文本表示，在层次文本分类中考虑了文档各个部分的相关性，并引入了一个层级引导的损失函数。此外，还提供了一个新颖的用于HTC的数据集ENZYME。

    

    层次文本分类（HTC）是多标签文本分类中的一个复杂子任务，其特点是具有层级标签分类法和数据不平衡。最佳性能模型旨在通过结合文档和层级标签信息来学习静态表示。然而，文档各个部分的相关性可能因层级水平的不同而变化，需要动态的文档表示。为了解决这个问题，我们提出了HiGen，一个利用语言模型编码动态文本表示的基于文本生成的框架。我们引入了一种层级引导的损失函数，以捕捉文本和标签名称语义之间的关系。我们的方法采用了一个特定任务的预训练策略，将语言模型调整到领域知识上，并显著提高了对样本有限的类别的性能。此外，我们还提供了一个命名为ENZYME的新颖和有价值的用于HTC的数据集，该数据集由来自PubMed的文章组成，旨在预测...

    Hierarchical text classification (HTC) is a complex subtask under multi-label text classification, characterized by a hierarchical label taxonomy and data imbalance. The best-performing models aim to learn a static representation by combining document and hierarchical label information. However, the relevance of document sections can vary based on the hierarchy level, necessitating a dynamic document representation. To address this, we propose HiGen, a text-generation-based framework utilizing language models to encode dynamic text representations. We introduce a level-guided loss function to capture the relationship between text and label name semantics. Our approach incorporates a task-specific pretraining strategy, adapting the language model to in-domain knowledge and significantly enhancing performance for classes with limited examples. Furthermore, we present a new and valuable dataset called ENZYME, designed for HTC, which comprises articles from PubMed with the goal of predicti
    
[^173]: 自我想象：利用自我想象进行多模型自然推理

    Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination

    [https://arxiv.org/abs/2401.08025](https://arxiv.org/abs/2401.08025)

    本文提出了Self-Imagine方法，通过利用一种Vision-Language模型生成问题的结构化表示并将其渲染为图像，再使用相同的模型回答问题，从而在数学任务和通用推理任务中提高了模型性能。

    

    Vision-Language模型（VLMs）的潜力在处理复杂基于文本问题时往往未被充分利用，尤其是当这些问题能够从视觉表达中获益时。本文提出了Self-Imagine，与人类通过创建问题的视觉图并推断解决步骤的能力相 resonating。我们利用单一的Vision-Language模型（VLM）使用HTML生成问题的结构化表示，然后将HTML渲染为图像，并最终使用相同的VLM根据问题和图像回答问题。我们的方法不需要任何额外的训练数据或训练。我们使用最先进的（LLAVA-1.5和GEMINI PRO）VLMs在三个数学任务和九个通用推理任务上评估了我们的方法。我们的方法提升了LLAVA-1.5和GEMINI PRO在所有数学任务上的性能。

    arXiv:2401.08025v2 Announce Type: replace  Abstract: The potential of Vision-Language Models (VLMs) often remains underutilized in handling complex text-based problems, particularly when these problems could benefit from visual representation. Resonating with humans' ability to solve complex text-based problems by (1) creating a visual diagram from the problem and (2) deducing what steps they need to take to solve it, we propose Self-Imagine. We leverage a single Vision-Language Model (VLM) to generate a structured representation of the question using HTML, then render the HTML as an image, and finally use the same VLM to answer the question using both the question and the image. Our approach does not require any additional training data or training. We evaluate our approach on three mathematics tasks and nine general-purpose reasoning tasks using state-of-the-art (LLAVA-1.5 and GEMINI PRO) VLMs. Our approach boosts the performance of LLAVA-1.5 and GEMINI PRO on all math tasks (on aver
    
[^174]: 一种具有条件扩散建模框架的应用于蛋白设计中的基序支架

    A framework for conditional diffusion modelling with applications in motif scaffolding for protein design

    [https://arxiv.org/abs/2312.09236](https://arxiv.org/abs/2312.09236)

    该论文提出一种统一的条件扩散建模框架，基于Doob's h-transform，用于解决蛋白设计中的基序支架问题

    

    许多蛋白设计应用，如结合物或酶的设计，需要以高精度搭建具有结构基序的蛋白质。基于去噪扩散过程的生成建模范式已成为解决这一基序支架问题的主要候选方案，并在某些情况下显示出早期实验成功。在扩散范式中，基序支架被视为一种条件生成任务，并提出了几种条件生成协议或从计算机视觉文献中导入。然而，这些协议大多基于启发性动机，例如通过对朗之万动力学的类比，并缺乏统一的框架，使得不同方法之间的联系变得模糊。在这项工作中，我们在一个基于数学上理解良好的Doob's h-transform的共同框架下统一了条件训练和条件抽样程序。这种新的视角使我们能够在不同方法之间建立联系

    arXiv:2312.09236v2 Announce Type: replace  Abstract: Many protein design applications, such as binder or enzyme design, require scaffolding a structural motif with high precision. Generative modelling paradigms based on denoising diffusion processes emerged as a leading candidate to address this motif scaffolding problem and have shown early experimental success in some cases. In the diffusion paradigm, motif scaffolding is treated as a conditional generation task, and several conditional generation protocols were proposed or imported from the Computer Vision literature. However, most of these protocols are motivated heuristically, e.g. via analogies to Langevin dynamics, and lack a unifying framework, obscuring connections between the different approaches. In this work, we unify conditional training and conditional sampling procedures under one common framework based on the mathematically well-understood Doob's h-transform. This new perspective allows us to draw connections between ex
    
[^175]: 视觉-语言模型作为奖励的来源

    Vision-Language Models as a Source of Rewards

    [https://arxiv.org/abs/2312.09187](https://arxiv.org/abs/2312.09187)

    使用现成的视觉-语言模型作为强化学习代理的奖励来源，展示了如何通过CLIP系列模型派生视觉目标实现的奖励，从而训练出能够实现多种语言目标的RL代理。

    

    建立可以在丰富多样的开放环境中实现许多目标的通用代理是强化学习的研究前沿之一。建立具有RL的通用代理的关键限制因素之一是需要大量的奖励函数来实现不同的目标。我们调查了使用现成的视觉-语言模型（VLM）作为强化学习代理的奖励来源的可行性。我们展示了如何从CLIP系列模型中派生视觉实现各种语言目标的奖励，并用于训练能够实现各种语言目标的RL代理。我们展示了这种方法在两个不同的视觉领域中，并呈现了一个规模化趋势，显示更大的VLM会产生更准确的视觉目标实现奖励，从而产生更有能力的RL代理。

    arXiv:2312.09187v2 Announce Type: replace  Abstract: Building generalist agents that can accomplish many goals in rich open-ended environments is one of the research frontiers for reinforcement learning. A key limiting factor for building generalist agents with RL has been the need for a large number of reward functions for achieving different goals. We investigate the feasibility of using off-the-shelf vision-language models, or VLMs, as sources of rewards for reinforcement learning agents. We show how rewards for visual achievement of a variety of language goals can be derived from the CLIP family of models, and used to train RL agents that can achieve a variety of language goals. We showcase this approach in two distinct visual domains and present a scaling trend showing how larger VLMs lead to more accurate rewards for visual goal achievement, which in turn produces more capable RL agents.
    
[^176]: MaxK-GNN: 探索加速图神经网络训练的理论速度极限

    MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training

    [https://arxiv.org/abs/2312.08656](https://arxiv.org/abs/2312.08656)

    MaxK-GNN是一种先进的高性能GPU训练系统，通过MaxK非线性和理论分析，实现了图神经网络训练的垂直优化。

    

    在深度神经网络训练加速方面，GPU已经成为主流平台。 GPU在GNN上面临着诸多挑战，如工作负载不平衡和内存访问不规则，导致硬件利用不充分。现有解决方案例如PyG、DGL与cuSPARSE，以及GNNAdvisor框架部分解决了这些挑战，但内存流量仍然很显著。 我们认为，只有通过算法与系统创新的垂直优化才能实现显著的性能提升，而不是将加速优化视为“事后思考”（即（i）给定GNN算法，设计加速器，或（ii）给定硬件，主要优化GNN算法）。 本文介绍了MaxK-GNN，一种集成算法与系统创新的先进高性能GPU训练系统。 （i）我们引入了MaxK非线性并提供了MaxK非线性的理论分析，

    arXiv:2312.08656v3 Announce Type: replace-cross  Abstract: In the acceleration of deep neural network training, the GPU has become the mainstream platform. GPUs face substantial challenges on GNNs, such as workload imbalance and memory access irregularities, leading to underutilized hardware. Existing solutions such as PyG, DGL with cuSPARSE, and GNNAdvisor frameworks partially address these challenges but memory traffic is still significant.   We argue that drastic performance improvements can only be achieved by the vertical optimization of algorithm and system innovations, rather than treating the speedup optimization as an "after-thought" (i.e., (i) given a GNN algorithm, designing an accelerator, or (ii) given hardware, mainly optimizing the GNN algorithm). In this paper, we present MaxK-GNN, an advanced high-performance GPU training system integrating algorithm and system innovation. (i) We introduce the MaxK nonlinearity and provide a theoretical analysis of MaxK nonlinearity as
    
[^177]: SparQ注意力：高效带宽的LLM推理

    SparQ Attention: Bandwidth-Efficient LLM Inference

    [https://arxiv.org/abs/2312.04985](https://arxiv.org/abs/2312.04985)

    SparQ Attention通过减少注意力块内存带宽需求的技术，从而增加LLMs推理的吞吐量，同时保持模型准确性。

    

    生成式大语言模型（LLMs）开创了许多新可能性，但由于其巨大的计算需求，它们的普遍使用仍然具有挑战性。我们引入了SparQ注意力，一种通过选择性获取缓存历史来减少注意力块内存带宽需求的技术，从而增加了LLMs的推理吞吐量。

    arXiv:2312.04985v2 Announce Type: replace  Abstract: Generative large language models (LLMs) have opened up numerous novel possibilities, but due to their significant computational requirements their ubiquitous use remains challenging. Some of the most useful applications require processing large numbers of samples at a time and using long contexts, both significantly increasing the memory communication load of the models. We introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by reducing the memory bandwidth requirements within the attention blocks through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show how SparQ Attention can decrease the attention memory bandwidth requirements up to eight times without any loss in accuracy by evaluating Llama 2 and Pythia models on a wide ra
    
[^178]: 使用Beta Divergence进行变分自监督对比学习用于人脸理解

    Variational Self-Supervised Contrastive Learning Using Beta Divergence For Face Understanding

    [https://arxiv.org/abs/2312.00824](https://arxiv.org/abs/2312.00824)

    提出一种使用Beta Divergence进行变分自监督对比学习的方法，能够从未标记和嘈杂数据集中学习，在人脸理解领域取得显著的准确率提升。

    

    学习如何使用未标记和嘈杂数据构建一个具有辨别性的语义空间在多标签设置中仍未解决。我们提出了一种对数据噪声具有鲁棒性且基于变分方法的对比自监督学习方法。该方法（VCL）利用变分对比学习与beta-divergence从未标记数据集中稳健地学习，包括未加工和嘈杂的数据集。我们通过严格的实验展示了所提出方法的有效性，包括在线性评估和微调场景中使用多标签数据集进行的人脸理解领域。在几乎所有测试场景中，VCL均超越了最先进的自监督方法的性能，实现了显著的准确率增加。

    arXiv:2312.00824v2 Announce Type: replace-cross  Abstract: Learning a discriminative semantic space using unlabelled and noisy data remains unaddressed in a multi-label setting. We present a contrastive self-supervised learning method which is robust to data noise, grounded in the domain of variational methods. The method (VCL) utilizes variational contrastive learning with beta-divergence to learn robustly from unlabelled datasets, including uncurated and noisy datasets. We demonstrate the effectiveness of the proposed method through rigorous experiments including linear evaluation and fine-tuning scenarios with multi-label datasets in the face understanding domain. In almost all tested scenarios, VCL surpasses the performance of state-of-the-art self-supervised methods, achieving a noteworthy increase in accuracy.
    
[^179]: 改进乳腺MRI肿瘤分割的前后对比合成

    Pre- to Post-Contrast Breast MRI Synthesis for Enhanced Tumour Segmentation

    [https://arxiv.org/abs/2311.10879](https://arxiv.org/abs/2311.10879)

    通过生成对抗网络（GAN）在乳腺MRI中合成对比增强，引入了Scaled Aggregate Measure (SAMe)进行量化评估，并成功应用于乳腺肿瘤分割任务。

    

    尽管动态增强MRI（DCE-MRI）中对比剂的使用对于肿瘤的检测和治疗有益处，但存在一系列问题，包括其侵入性、生物积累性及潜在的肾源性系统纤维化风险。本研究探讨了通过将术前对比前T1加权脂肪饱和乳腺MRI转换为其对应的首次DCE-MRI序列，利用生成对抗网络（GAN）的能力来产生合成对比增强的可行性。此外，我们引入了一种用于定量评估合成数据质量的比例聚合测量（SAMe），并作为选择最佳生成模型的基础。我们使用定量图像质量指标评估生成的DCE-MRI数据，并将其应用于3D乳腺肿瘤分割的下游任务。我们的结果突显了p的潜力

    arXiv:2311.10879v2 Announce Type: replace-cross  Abstract: Despite its benefits for tumour detection and treatment, the administration of contrast agents in dynamic contrast-enhanced MRI (DCE-MRI) is associated with a range of issues, including their invasiveness, bioaccumulation, and a risk of nephrogenic systemic fibrosis. This study explores the feasibility of producing synthetic contrast enhancements by translating pre-contrast T1-weighted fat-saturated breast MRI to their corresponding first DCE-MRI sequence leveraging the capabilities of a generative adversarial network (GAN). Additionally, we introduce a Scaled Aggregate Measure (SAMe) designed for quantitatively evaluating the quality of synthetic data in a principled manner and serving as a basis for selecting the optimal generative model. We assess the generated DCE-MRI data using quantitative image quality metrics and apply them to the downstream task of 3D breast tumour segmentation. Our results highlight the potential of p
    
[^180]: EduGym: 用于强化学习教育的环境和笔记本套件

    EduGym: An Environment and Notebook Suite for Reinforcement Learning Education

    [https://arxiv.org/abs/2311.10590](https://arxiv.org/abs/2311.10590)

    EduGym是一套用于强化学习教育的环境和笔记本套件，旨在解决学生在转换理论和实践中遇到的困难。

    

    由于强化学习的经验成功，越来越多的学生在学习这个课题。然而，根据我们的实际教学经验，我们发现学生在进入这个领域（本科生、硕士生和早期博士生）时常常遇到困难。一方面，教科书和（在线）讲座提供了基础知识，但学生发现很难在方程式和代码之间进行转换。另一方面，公共代码库提供了实际的例子，但实现的算法往往复杂，并且基础测试环境同时包含多个强化学习挑战。尽管这在研究角度上是现实的，但它经常阻碍了教育概念的理解。为了解决这个问题，我们推出了EduGym，这是一组专门针对教育的强化学习环境和相关交互式笔记本。

    arXiv:2311.10590v2 Announce Type: replace-cross  Abstract: Due to the empirical success of reinforcement learning, an increasing number of students study the subject. However, from our practical teaching experience, we see students entering the field (bachelor, master and early PhD) often struggle. On the one hand, textbooks and (online) lectures provide the fundamentals, but students find it hard to translate between equations and code. On the other hand, public codebases do provide practical examples, but the implemented algorithms tend to be complex, and the underlying test environments contain multiple reinforcement learning challenges at once. Although this is realistic from a research perspective, it often hinders educational conceptual understanding. To solve this issue we introduce EduGym, a set of educational reinforcement learning environments and associated interactive notebooks tailored for education. Each EduGym environment is specifically designed to illustrate a certain 
    
[^181]: 分析验证同步时间的深度神经网络在配电系统状态估计中的性能

    Analytical Verification of Deep Neural Network Performance for Time-Synchronized Distribution System State Estimation

    [https://arxiv.org/abs/2311.06973](https://arxiv.org/abs/2311.06973)

    本论文研究了使用深度神经网络进行配电系统同步时间状态估计的性能和鲁棒性。通过将输入扰动视为混合整数线性规划问题进行分析验证，并强调了批归一化在提高问题可扩展性方面的作用。该框架在修改后的IEEE 34节点系统和真实的大型分布系统上进行验证。

    

    最近，我们展示了使用深度神经网络（DNN）进行实时不可观测分布系统的同步时间状态估计的成功。在这个论文中，我们提供了该状态估计器在输入测量扰动的情况下的性能的分析界限。已经有人表明，仅基于测试数据集来评估性能可能不能有效地说明训练好的DNN处理输入扰动的能力。因此，我们将输入扰动作为混合整数线性规划（MILP）问题从分析上验证了DNN对输入扰动的鲁棒性和可靠性。同时，我们还强调了批归一化在解决MILP公式的可扩展性限制方面的能力。该框架通过在修改后的IEEE 34节点系统和一个真实的大型分布系统上进行同步时间的配电系统状态估计来进行验证，这两个系统都是通过微相位测量不完全观测到的。

    Recently, we demonstrated success of a time-synchronized state estimator using deep neural networks (DNNs) for real-time unobservable distribution systems. In this letter, we provide analytical bounds on the performance of that state estimator as a function of perturbations in the input measurements. It has already been shown that evaluating performance based on only the test dataset might not effectively indicate a trained DNN's ability to handle input perturbations. As such, we analytically verify robustness and trustworthiness of DNNs to input perturbations by treating them as mixed-integer linear programming (MILP) problems. The ability of batch normalization in addressing the scalability limitations of the MILP formulation is also highlighted. The framework is validated by performing time-synchronized distribution system state estimation for a modified IEEE 34-node system and a real-world large distribution system, both of which are incompletely observed by micro-phasor measuremen
    
[^182]: 导航规模定律：自适应模型训练中的计算优化

    Navigating Scaling Laws: Compute Optimality in Adaptive Model Training

    [https://arxiv.org/abs/2311.03233](https://arxiv.org/abs/2311.03233)

    本研究提出了一种新颖的自适应模型训练方法，通过允许模型在训练过程中调整形状，能够优化地使用计算资源，实现在更少的计算量下达到目标性能。

    

    近年来，深度学习的最新技术主要由经过大量数据预训练的非常庞大模型主导。这一范式非常简单：投入更多的计算资源（最优地）会提高性能，而且甚至能够可预测性地做到；已经推导出了神经网络性能的缩放定律，准确预测了网络在所需计算水平下的性能。这引出了“计算优化”模型的概念，即在训练过程中分配给定计算水平以最大化性能的模型。在本研究中，我们通过允许“自适应”模型，即在训练过程中可以改变形状的模型，来扩展优化概念。通过这样做，我们可以设计出能够最优地在基本定律之间穿行并超越它们的“静态”对应物的自适应模型，从而显著减少达到给定目标性能所需的计算量。

    arXiv:2311.03233v2 Announce Type: replace  Abstract: In recent years, the state-of-the-art in deep learning has been dominated by very large models that have been pre-trained on vast amounts of data. The paradigm is very simple: investing more computational resources (optimally) leads to better performance, and even predictably so; neural scaling laws have been derived that accurately forecast the performance of a network for a desired level of compute. This leads to the notion of a `compute-optimal' model, i.e. a model that allocates a given level of compute during training optimally to maximize performance. In this work, we extend the concept of optimality by allowing for an `adaptive' model, i.e. a model that can change its shape during training. By doing so, we can design adaptive models that optimally traverse between the underlying scaling laws and outpace their `static' counterparts, leading to a significant reduction in the required compute to reach a given target performance. 
    
[^183]: 通过可控机器遗忘打破隐私、效用、效率三难题

    Breaking the Trilemma of Privacy, Utility, Efficiency via Controllable Machine Unlearning

    [https://arxiv.org/abs/2310.18574](https://arxiv.org/abs/2310.18574)

    设计了一种名为Controllable Machine Unlearning (ConMU)的新框架，旨在平衡隐私、模型效用和运行效率之间的权衡。

    

    机器遗忘（MU）算法由于对数据隐私法规的必要遵从而变得越来越关键。MU的主要目标是在不需要从头重新训练模型的情况下消除特定数据样本对给定模型的影响。现有方法主要关注于最大化用户隐私保护。然而，每个现实世界基于网络的应用程序都有不同程度的隐私法规。探索隐私、模型效用和运行效率之间的权衡全谱对于实际的遗忘场景至关重要。而且，由于固有的复杂交互作用，设计具有对上述权衡的简单控制的MU算法是可取但具有挑战性。为了解决这些挑战，我们提出了Controllable Machine Unlearning (ConMU)，这是一个旨在促进MU校准的新颖框架。ConMU框架包含三个集成

    arXiv:2310.18574v2 Announce Type: replace-cross  Abstract: Machine Unlearning (MU) algorithms have become increasingly critical due to the imperative adherence to data privacy regulations. The primary objective of MU is to erase the influence of specific data samples on a given model without the need to retrain it from scratch. Accordingly, existing methods focus on maximizing user privacy protection. However, there are different degrees of privacy regulations for each real-world web-based application. Exploring the full spectrum of trade-offs between privacy, model utility, and runtime efficiency is critical for practical unlearning scenarios. Furthermore, designing the MU algorithm with simple control of the aforementioned trade-off is desirable but challenging due to the inherent complex interaction. To address the challenges, we present Controllable Machine Unlearning (ConMU), a novel framework designed to facilitate the calibration of MU. The ConMU framework contains three integra
    
[^184]: 具有嘈杂标签的自适应符合分类

    Adaptive conformal classification with noisy labels

    [https://arxiv.org/abs/2309.05092](https://arxiv.org/abs/2309.05092)

    该论文提出了一种新颖的自适应符合分类方法，能够自动适应随机标签污染，产生更具信息量和更坚实覆盖保证的预测集合。

    

    本文开发了新颖的符合预测方法，用于分类任务，能够自动适应在校准样本中的随机标签污染，从而产生更丰富的、比起最先进方法更有强大覆盖保证的预测集。这是通过精确表征标准符合推断在标签污染存在时受到的有效覆盖膨胀（或紧缩）来实现的，然后通过新的校准算法来实现可行性。我们的解决方案是灵活的，可以利用关于标签污染过程的不同建模假设，同时不需要对底层数据分布或机器学习分类器的内部工作原理进行了解。所提出方法的优势通过广泛的模拟和对CIFAR-10H图像数据集的物体分类应用得到展示。

    arXiv:2309.05092v2 Announce Type: replace-cross  Abstract: This paper develops novel conformal prediction methods for classification tasks that can automatically adapt to random label contamination in the calibration sample, leading to more informative prediction sets with stronger coverage guarantees compared to state-of-the-art approaches. This is made possible by a precise characterization of the effective coverage inflation (or deflation) suffered by standard conformal inferences in the presence of label contamination, which is then made actionable through new calibration algorithms. Our solution is flexible and can leverage different modeling assumptions about the label contamination process, while requiring no knowledge of the underlying data distribution or of the inner workings of the machine-learning classifier. The advantages of the proposed methods are demonstrated through extensive simulations and an application to object classification with the CIFAR-10H image data set.
    
[^185]: 基于Stiefel流形的分布式黎曼共轭梯度方法

    Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold

    [https://arxiv.org/abs/2308.10547](https://arxiv.org/abs/2308.10547)

    该论文提出了一种在Stiefel流形上进行分布式优化的黎曼共轭梯度下降方法，克服了全局函数非凸的限制。

    

    共轭梯度法是一种至关重要的一阶优化方法，通常比最速下降法收敛更快，计算成本也远低于二阶方法。然而，尽管在欧几里德空间和黎曼流形上已研究了各种类型的共轭梯度方法，但在分布式场景下的研究却很少。本文提出了一种旨在在Stiefel流形上最小化全局函数的分布式黎曼共轭梯度下降（DRCGD）方法。优化问题在一组代理网络中分布，每个代理与一个局部函数相关联，并且代理之间的通信在一个无向连通图上进行。由于Stiefel流形是一个非凸集，全局函数被表示为可能非凸（但平滑）局部函数的有限和。该方法不受最优非凸常数的限制。

    arXiv:2308.10547v2 Announce Type: replace-cross  Abstract: The conjugate gradient method is a crucial first-order optimization method that generally converges faster than the steepest descent method, and its computational cost is much lower than the second-order methods. However, while various types of conjugate gradient methods have been studied in Euclidean spaces and on Riemannian manifolds, there is little study for those in distributed scenarios. This paper proposes a decentralized Riemannian conjugate gradient descent (DRCGD) method that aims at minimizing a global function over the Stiefel manifold. The optimization problem is distributed among a network of agents, where each agent is associated with a local function, and the communication between agents occurs over an undirected connected graph. Since the Stiefel manifold is a non-convex set, a global function is represented as a finite sum of possibly non-convex (but smooth) local functions. The proposed method is free from ex
    
[^186]: 无剪切的DP-SGD：利普希茨神经网络方式

    DP-SGD Without Clipping: The Lipschitz Neural Network Way

    [https://arxiv.org/abs/2305.16202](https://arxiv.org/abs/2305.16202)

    提出一种无需剪切的DP-SGD训练方法，依赖于利普希茨约束网络提供灵敏度界限，规避了剪切过程的缺点，并证明了可以通过限制每一层相对于其参数的利普希茨常数来训练这些具有隐私保证的网络。

    

    最先进的训练差分隐私（DP）深度神经网络（DNN）方法在估计网络层的灵敏度上遇到困难，而是依赖于每个样本的梯度剪切过程。本文提出依赖于利普希茨约束网络来提供灵敏度界限并规避剪切过程的缺点。我们的理论分析揭示了与其参数相关的利普希茨常数与其输入相关的利普希茨常数之间的未开发的联系。通过限制每一层相对于其参数的利普希茨常数，我们证明可以训练这些具有隐私保证的网络。我们的分析不仅使规模化地计算上述灵敏度成为可能，还提供了如何进行的指导。

    arXiv:2305.16202v2 Announce Type: replace  Abstract: State-of-the-art approaches for training Differentially Private (DP) Deep Neural Networks (DNN) face difficulties to estimate tight bounds on the sensitivity of the network's layers, and instead rely on a process of per-sample gradient clipping. This clipping process not only biases the direction of gradients but also proves costly both in memory consumption and in computation. To provide sensitivity bounds and bypass the drawbacks of the clipping process, we propose to rely on Lipschitz constrained networks. Our theoretical analysis reveals an unexplored link between the Lipschitz constant with respect to their input and the one with respect to their parameters. By bounding the Lipschitz constant of each layer with respect to its parameters, we prove that we can train these networks with privacy guarantees. Our analysis not only allows the computation of the aforementioned sensitivities at scale, but also provides guidance on how to
    
[^187]: LLMs用于知识图谱构建和推理：最新功能与未来机遇

    LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities

    [https://arxiv.org/abs/2305.13168](https://arxiv.org/abs/2305.13168)

    本研究全面评估了LLMs在知识图谱构建和推理领域的性能，发现GPT-4更适合作为推理助手，并在某些情况下超越了精调模型。

    

    本文对大规模语言模型（LLMs）在知识图谱（KG）构建和推理中的数量化和质化评估进行了详尽的研究。我们在八个不同的数据集上进行了实验，重点关注涵盖实体和关系提取、事件提取、链接预测和问答四个典型任务，从而全面探索了LLMs在构建和推理领域的表现。经验性研究发现，以GPT-4为代表的LLMs更适合作为推理助手，而不是少样本信息提取器。具体而言，虽然GPT-4在与KG构建相关的任务中表现出色，但在推理任务中表现更出色，在某些情况下超越了精调模型。此外，我们的调查还扩展到LLMs在信息提取方面的潜在泛化能力，提出了虚拟知识提取的构想。

    arXiv:2305.13168v2 Announce Type: replace-cross  Abstract: This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extr
    
[^188]: Mobiprox：支持移动设备上的动态近似计算

    Mobiprox: Supporting Dynamic Approximate Computing on Mobiles

    [https://arxiv.org/abs/2303.11291](https://arxiv.org/abs/2303.11291)

    Mobiprox提供了一个框架，支持移动设备上的动态近似计算，并且实现了对单个网络层的运行时可调近似。

    

    arXiv:2303.11291v2 公告类型：替换 摘要：Runtime-tunable context-dependent网络压缩将使移动深度学习（DL）适应于经常变化的资源可用性、输入“难度”或用户需求。现有的压缩技术显著减少了DL的内存、处理和能耗，但由此产生的模型往往永久受损，牺牲了推理能力以换取减少资源使用。另一方面，现有的可调压缩方法需要昂贵的重新训练，不支持任意压缩策略调整，并且不提供适合移动设备的实现。 在本文中，我们介绍了Mobiprox，这是一个支持移动设备具有灵活精度的框架。Mobiprox实现了张量操作的可调近似，并实现了对单个网络层的运行时可调近似。Mobiprox附带的分析器和调整器识别了最有希望的神经网络配置。

    arXiv:2303.11291v2 Announce Type: replace  Abstract: Runtime-tunable context-dependent network compression would make mobile deep learning (DL) adaptable to often varying resource availability, input "difficulty", or user needs. The existing compression techniques significantly reduce the memory, processing, and energy tax of DL, yet, the resulting models tend to be permanently impaired, sacrificing the inference power for reduced resource usage. The existing tunable compression approaches, on the other hand, require expensive re-training, do not support arbitrary strategies for adapting the compression and do not provide mobile-ready implementations.   In this paper we present Mobiprox, a framework enabling mobile DL with flexible precision. Mobiprox implements tunable approximations of tensor operations and enables runtime-adaptable approximation of individual network layers. A profiler and a tuner included with Mobiprox identify the most promising neural network approximation config
    
[^189]: 具有卫星图像的深度混合模型：如何将需求建模和计算机视觉相结合用于行为分析？

    Deep hybrid model with satellite imagery: how to combine demand modeling and computer vision for behavior analysis?

    [https://arxiv.org/abs/2303.04204](https://arxiv.org/abs/2303.04204)

    该研究提出了一个深度混合模型框架，通过将数值数据和城市影像整合到一个潜在空间中，成功应用于分析出行方式选择，并在预测聚合和细分出行行为方面优于传统需求模型和深度学习方法。

    

    传统的需求建模仅使用低维数数值数据（即社会人口统计数据和出行属性），而不使用高维城市影像。然而，出行行为取决于数值数据和城市影像所代表的因素，因此需要一个协同框架来将它们结合起来。本研究创建了一个具有交叉结构的深度混合模型的理论框架，包括一个混合运算器和一个行为预测器，从而将数值和影像数据整合到一个潜在空间中。从实证角度看，该框架被应用于使用芝加哥的MyDailyTravel调查作为数值输入和卫星图像作为影像输入来分析出行方式选择。我们发现，深度混合模型在使用我们的监督混合设计来预测聚合和细分出行行为时，胜过传统的需求模型和最近的深度学习。

    arXiv:2303.04204v2 Announce Type: replace  Abstract: Classical demand modeling analyzes travel behavior using only low-dimensional numeric data (i.e. sociodemographics and travel attributes) but not high-dimensional urban imagery. However, travel behavior depends on the factors represented by both numeric data and urban imagery, thus necessitating a synergetic framework to combine them. This study creates a theoretical framework of deep hybrid models with a crossing structure consisting of a mixing operator and a behavioral predictor, thus integrating the numeric and imagery data into a latent space. Empirically, this framework is applied to analyze travel mode choice using the MyDailyTravel Survey from Chicago as the numeric inputs and the satellite images as the imagery inputs. We found that deep hybrid models outperform both the traditional demand models and the recent deep learning in predicting the aggregate and disaggregate travel behavior with our supervision-as-mixing design. T
    
[^190]: 使用概率图神经网络对时空出行需求的不确定性建模

    Uncertainty Quantification of Spatiotemporal Travel Demand with Probabilistic Graph Neural Networks

    [https://arxiv.org/abs/2303.04040](https://arxiv.org/abs/2303.04040)

    该研究提出了一种概率图神经网络（Prob-GNN）框架，用于量化出行需求的时空不确定性，实证应用表明概率假设对不确定性预测影响大于确定性假设。

    

    近期的研究显著提高了使用图神经网络预测出行需求的准确性。然而，这些研究很大程度上忽略了出行需求预测中不可避免的不确定性。为了填补这一空白，本研究提出了一种概率图神经网络（Prob-GNN）框架，用于量化出行需求的时空不确定性。这个Prob-GNN框架基于确定性和概率假设，并在芝加哥市预测公共交通和拼车需求的任务上得到了实证应用。我们发现，概率假设（如分布尾部、支持）对不确定性预测的影响大于确定性假设（如深度模块、深度）。在Prob-GNN家族中，采用截断高斯和拉普拉斯分布的GNN在公共交通和拼车数据中表现最佳。即使在存在明显域偏移情况下，Prob-GNNs

    arXiv:2303.04040v2 Announce Type: replace  Abstract: Recent studies have significantly improved the prediction accuracy of travel demand using graph neural networks. However, these studies largely ignored uncertainty that inevitably exists in travel demand prediction. To fill this gap, this study proposes a framework of probabilistic graph neural networks (Prob-GNN) to quantify the spatiotemporal uncertainty of travel demand. This Prob-GNN framework is substantiated by deterministic and probabilistic assumptions, and empirically applied to the task of predicting the transit and ridesharing demand in Chicago. We found that the probabilistic assumptions (e.g. distribution tail, support) have a greater impact on uncertainty prediction than the deterministic ones (e.g. deep modules, depth). Among the family of Prob-GNNs, the GNNs with truncated Gaussian and Laplace distributions achieve the highest performance in transit and ridesharing data. Even under significant domain shifts, Prob-GNNs
    
[^191]: 劝说行为代理：近似最佳响应和学习

    Persuading a Behavioral Agent: Approximately Best Responding and Learning

    [https://arxiv.org/abs/2302.03719](https://arxiv.org/abs/2302.03719)

    发送者可以找到一个信号方案，几乎可以保证获得与经典模型中最佳效用几乎相等的预期效用，而接收者的近似最佳响应行为并不会对发送者在贝叶斯说服问题中的最大可达效用产生很大影响。

    

    传统的贝叶斯说服模型假定接收者是一个贝叶斯和最佳响应的。我们研究了一个对贝叶斯说服模型的放松，其中接收者可以近似最佳地响应发送者的信号方案。我们展示，在自然假设下，（1）发送者可以找到一个信号方案，保证自己获得的预期效用几乎和在传统模型中的最优效用一样好，无论接收者采用什么样的近似最佳响应策略；（2）另一方面，即使接收者使用对发送者最有利的近似最佳响应策略，也不存在一个信号方案可以给发送者比其在传统模型中的最优效用更多的效用。这两点说明接收者的近似最佳响应行为并不会对贝叶斯说服问题中发送者能够达到的最大效用产生很大影响。这两个结果的证明是...

    arXiv:2302.03719v2 Announce Type: replace-cross  Abstract: The classic Bayesian persuasion model assumes a Bayesian and best-responding receiver. We study a relaxation of the Bayesian persuasion model where the receiver can approximately best respond to the sender's signaling scheme. We show that, under natural assumptions, (1) the sender can find a signaling scheme that guarantees itself an expected utility almost as good as its optimal utility in the classic model, no matter what approximately best-responding strategy the receiver uses; (2) on the other hand, there is no signaling scheme that gives the sender much more utility than its optimal utility in the classic model, even if the receiver uses the approximately best-responding strategy that is best for the sender. Together, (1) and (2) imply that the approximately best-responding behavior of the receiver does not affect the sender's maximal achievable utility a lot in the Bayesian persuasion problem. The proofs of both results r
    
[^192]: 基于阈值的自动标注的优势与局限性

    Promises and Pitfalls of Threshold-based Auto-labeling

    [https://arxiv.org/abs/2211.12620](https://arxiv.org/abs/2211.12620)

    TBAL系统可以通过验证数据自动标注未标注数据，减少手动标注的依赖；研究结果展示了即使模型表现不佳也可以准确自动标记数据，并揭示了TBAL系统的潜在缺陷

    

    创建大规模高质量标记数据集是监督机器学习工作流程中的一个主要瓶颈。阈值自动标注（TBAL）通过使用人类获取的验证数据来寻找一个置信阈值，高于该阈值的数据将由机器标记，从而减少了对手动注释的依赖。TBAL正逐渐成为实践中被广泛采用的解决方案。鉴于所得数据的长期有效性和多样化使用，理解这种自动标注系统获取的数据何时可以被依赖是至关重要的。这是第一项分析TBAL系统并推导需要保证机器标记数据质量的人工标记验证数据量样本复杂性界限的工作。我们的结果提供了两个关键见解。首先，表面上糟糕的模型可以自动、准确地标记合理数量的未标记数据。其次，TBAL系统的一个隐藏的缺点是潜在地

    arXiv:2211.12620v2 Announce Type: replace-cross  Abstract: Creating large-scale high-quality labeled datasets is a major bottleneck in supervised machine learning workflows. Threshold-based auto-labeling (TBAL), where validation data obtained from humans is used to find a confidence threshold above which the data is machine-labeled, reduces reliance on manual annotation. TBAL is emerging as a widely-used solution in practice. Given the long shelf-life and diverse usage of the resulting datasets, understanding when the data obtained by such auto-labeling systems can be relied on is crucial. This is the first work to analyze TBAL systems and derive sample complexity bounds on the amount of human-labeled validation data required for guaranteeing the quality of machine-labeled data. Our results provide two crucial insights. First, reasonable chunks of unlabeled data can be automatically and accurately labeled by seemingly bad models. Second, a hidden downside of TBAL systems is potentially
    
[^193]: 代数机器学习及其在化学中的应用

    Algebraic Machine Learning with an Application to Chemistry

    [https://arxiv.org/abs/2205.05795](https://arxiv.org/abs/2205.05795)

    本文开发了一种机器学习流程，能够捕获细粒度的几何信息，而无需依赖光滑性假设。

    

    随着科学应用中使用的数据集变得更加复杂，研究数据的几何和拓扑结构已成为数据分析过程中日益重要的一部分。这可以从对持久同调等拓扑工具日益增长的兴趣中看出。然而，一方面，拓扑工具本质上仅提供关于数据基本空间的粗略信息。另一方面，更多几何方法主要依赖于流形假设，即基本空间是光滑流形。这一假设对于许多包含奇点的物理模型是不成立的。本文中，我们开发了一种机器学习流程，能够捕获细粒度的几何信息，而无需依赖任何光滑性假设。我们的方法涉及代数几何和代数多项式的范围。

    arXiv:2205.05795v3 Announce Type: replace-cross  Abstract: As datasets used in scientific applications become more complex, studying the geometry and topology of data has become an increasingly prevalent part of the data analysis process. This can be seen for example with the growing interest in topological tools such as persistent homology. However, on the one hand, topological tools are inherently limited to providing only coarse information about the underlying space of the data. On the other hand, more geometric approaches rely predominately on the manifold hypothesis, which asserts that the underlying space is a smooth manifold. This assumption fails for many physical models where the underlying space contains singularities.   In this paper we develop a machine learning pipeline that captures fine-grain geometric information without having to rely on any smoothness assumptions. Our approach involves working within the scope of algebraic geometry and algebraic varieties instead of 
    
[^194]: 对图上机器学习公平性的调查

    A Survey on Fairness for Machine Learning on Graphs

    [https://arxiv.org/abs/2205.05396](https://arxiv.org/abs/2205.05396)

    该调查是第一个专门关于关系数据公平性的调查，致力于解决图上机器学习中的公平性问题。

    

    如今，在许多实际应用领域中，通过图形建模的复杂现象的分析在决策方面起着关键作用，这些决策可能会对社会产生重大影响。然而，近年来许多研究和论文揭示了机器学习模型可能导致个体之间存在潜在的差异对待和不公平的结果。在这种情况下，图挖掘的算法贡献也无法摆脱公平性问题，且存在一些与图的固有性质相关的具体挑战：(1) 图数据并非独立同分布，这种假设可能会使许多现有的公平机器学习研究变得无效，(2) 针对关系数据评估不同类型公平性的度量定义，以及(3) 在模型准确性和公平性之间找到良好平衡的算法挑战。本调查是第一个专门关于关系数据公平性的调查。它旨在呈现一个全面的

    arXiv:2205.05396v2 Announce Type: replace  Abstract: Nowadays, the analysis of complex phenomena modeled by graphs plays a crucial role in many real-world application domains where decisions can have a strong societal impact. However, numerous studies and papers have recently revealed that machine learning models could lead to potential disparate treatment between individuals and unfair outcomes. In that context, algorithmic contributions for graph mining are not spared by the problem of fairness and present some specific challenges related to the intrinsic nature of graphs: (1) graph data is non-IID, and this assumption may invalidate many existing studies in fair machine learning, (2) suited metric definitions to assess the different types of fairness with relational data and (3) algorithmic challenge on the difficulty of finding a good trade-off between model accuracy and fairness. This survey is the first one dedicated to fairness for relational data. It aims to present a comprehen
    
[^195]: 物理信息深度学习在实验流体力学中的应用

    Physics-informed deep-learning applications to experimental fluid mechanics

    [https://arxiv.org/abs/2203.15402](https://arxiv.org/abs/2203.15402)

    该研究利用物理信息神经网络（PINNs）从有限的嘈杂测量数据中实现流场数据的超分辨，无需高分辨率参考数据，旨在获得连续的时空解。

    

    由于实验流体力学中测量数据通常稀疏、不完整且嘈杂，从低分辨率和嘈杂测量数据中高分辨率重构流场数据是一个感兴趣的问题。深度学习方法已被证明适用于这样的超分辨任务。本研究应用物理信息神经网络（PINNs）从有限的嘈杂测量中超分辨流场数据，无需任何高分辨率参考数据，旨在获得时间和空间上的连续解。

    arXiv:2203.15402v2 Announce Type: replace-cross  Abstract: High-resolution reconstruction of flow-field data from low-resolution and noisy measurements is of interest due to the prevalence of such problems in experimental fluid mechanics, where the measurement data are in general sparse, incomplete and noisy. Deep-learning approaches have been shown suitable for such super-resolution tasks. However, a high number of high-resolution examples is needed, which may not be available for many cases. Moreover, the obtained predictions may lack in complying with the physical principles, e.g. mass and momentum conservation. Physics-informed deep learning provides frameworks for integrating data and physical laws for learning. In this study, we apply physics-informed neural networks (PINNs) for super-resolution of flow-field data both in time and space from a limited set of noisy measurements without having any high-resolution reference data. Our objective is to obtain a continuous solution of t
    
[^196]: 具有异质性的图神经网络：一项调查

    Graph Neural Networks for Graphs with Heterophily: A Survey

    [https://arxiv.org/abs/2202.07082](https://arxiv.org/abs/2202.07082)

    该论文提出了对具有异质性的图进行图神经网络研究的系统回顾，并提出了系统性分类法以指导现有异质性GNN模型。

    

    近年来，图神经网络（GNNs）的快速发展使得许多图分析任务和应用受益。大多数GNNs通常依赖于同质性假设，即属于同一类别的节点更可能相连。然而，在许多真实场景中，作为一种普遍的图属性，即不同标签的节点往往相连，这显著限制了定制的同质性GNNs的性能。因此，针对异质性图的GNNs正受到越来越多的研究关注，以增强对具有异质性的图学习。本文针对具有异质性的图提供了全面的GNNs回顾。具体来说，我们提出了一个系统性分类法，本质上指导着现有的异质性GNN模型，以及一个概括性摘要和详细分析。

    arXiv:2202.07082v2 Announce Type: replace  Abstract: Recent years have witnessed fast developments of graph neural networks (GNNs) that have benefited myriads of graph analytic tasks and applications. In general, most GNNs depend on the homophily assumption that nodes belonging to the same class are more likely to be connected. However, as a ubiquitous graph property in numerous real-world scenarios, heterophily, i.e., nodes with different labels tend to be linked, significantly limits the performance of tailor-made homophilic GNNs. Hence, GNNs for heterophilic graphs are gaining increasing research attention to enhance graph learning with heterophily. In this paper, we provide a comprehensive review of GNNs for heterophilic graphs. Specifically, we propose a systematic taxonomy that essentially governs existing heterophilic GNN models, along with a general summary and detailed analysis. %Furthermore, we summarize the mainstream heterophilic graph benchmarks to facilitate robust and fa
    
[^197]: 使用PAC-Bayes界限同时控制多个错误

    Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound

    [https://arxiv.org/abs/2202.05560](https://arxiv.org/abs/2202.05560)

    该研究提出了一种PAC-Bayes界限，能够同时控制多个错误，并提供丰富的信息，适用于回归中测试损失分布或分类中不同错误分类的概率。

    

    当前的PAC-Bayes泛化界限仅限于性能的标量度量，如损失或错误率。我们提供了第一个能够提供丰富信息的PAC-Bayes界限，通过界定一组M种错误类型的经验概率与真实概率之间的Kullback-Leibler差异来控制可能结果的整个分布。

    arXiv:2202.05560v2 Announce Type: replace-cross  Abstract: Current PAC-Bayes generalisation bounds are restricted to scalar metrics of performance, such as the loss or error rate. However, one ideally wants more information-rich certificates that control the entire distribution of possible outcomes, such as the distribution of the test loss in regression, or the probabilities of different mis classifications. We provide the first PAC-Bayes bound capable of providing such rich information by bounding the Kullback-Leibler divergence between the empirical and true probabilities of a set of M error types, which can either be discretized loss values for regression, or the elements of the confusion matrix (or a partition thereof) for classification. We transform our bound into a differentiable training objective. Our bound is especially useful in cases where the severity of different mis-classifications may change over time; existing PAC-Bayes bounds can only bound a particular pre-decided w
    
[^198]: FIGARO：具有细粒度艺术控制的符号音乐生成

    FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control

    [https://arxiv.org/abs/2201.10936](https://arxiv.org/abs/2201.10936)

    提出了一种自监督的描述-序列任务，实现了在全局水平上对生成音乐的细粒度可控，通过结合高级特征和领域知识，在符号音乐生成方面取得了最新的成果

    

    使用深度神经网络生成音乐近年来一直是一个活跃的研究领域。虽然生成样本的质量不断提高，但大多数方法只能对生成的序列施加最小的控制，甚至没有。我们提出了自监督的描述-序列任务，它允许在全局水平上进行细粒度可控生成。我们通过提取有关目标序列的高级特征，以及学习给定相应高级描述时序列的条件分布，来实现这一点。我们通过将描述-序列建模应用到符号音乐中来训练FIGARO（基于注意力的、鲁棒控制的细粒度音乐生成）。通过将学习到的高级特征与领域知识结合，作为强归纳偏差，该模型在可控符号音乐生成方面实现了最新的成果。

    arXiv:2201.10936v4 Announce Type: replace-cross  Abstract: Generating music with deep neural networks has been an area of active research in recent years. While the quality of generated samples has been steadily increasing, most methods are only able to exert minimal control over the generated sequence, if any. We propose the self-supervised description-to-sequence task, which allows for fine-grained controllable generation on a global level. We do so by extracting high-level features about the target sequence and learning the conditional distribution of sequences given the corresponding high-level description in a sequence-to-sequence modelling setup. We train FIGARO (FIne-grained music Generation via Attention-based, RObust control) by applying description-to-sequence modelling to symbolic music. By combining learned high level features with domain knowledge, which acts as a strong inductive bias, the model achieves state-of-the-art results in controllable symbolic music generation a
    
[^199]: 数据结构>标签？无监督启发式SVM超参数估计

    Data structure > labels? Unsupervised heuristics for SVM hyperparameter estimation

    [https://arxiv.org/abs/2111.02164](https://arxiv.org/abs/2111.02164)

    提出了一种对支持向量机超参数进行改进的无监督启发式方法，以解决在缺乏标记示例的情况下，减少对类标签信息依赖的问题。

    

    分类是模式识别研究的主要领域之一，在其中，支持向量机（SVM）是除了深度学习领域以外最流行的方法之一，并且是许多机器学习方法的事实参考。SVM的性能取决于参数选择，通常通过耗时的网格搜索交叉验证过程（GSCV）来实现。然而，该方法依赖于标记示例的可用性和质量，当这些标记受限时可能会受阻。为解决这一问题，存在几种无监督启发式方法，利用数据集的特征来选择参数，而不是使用类标签信息。虽然速度快一个数量级，但它们很少被使用，因为人们认为它们的结果明显比网格搜索差。为了挑战这种假设，我们提出了改进的SVM超参数heuristics。

    arXiv:2111.02164v2 Announce Type: replace  Abstract: Classification is one of the main areas of pattern recognition research, and within it, Support Vector Machine (SVM) is one of the most popular methods outside of field of deep learning -- and a de-facto reference for many Machine Learning approaches. Its performance is determined by parameter selection, which is usually achieved by a time-consuming grid search cross-validation procedure (GSCV). That method, however relies on the availability and quality of labelled examples and thus, when those are limited can be hindered. To address that problem, there exist several unsupervised heuristics that take advantage of the characteristics of the dataset for selecting parameters instead of using class label information. While an order of magnitude faster, they are scarcely used under the assumption that their results are significantly worse than those of grid search. To challenge that assumption, we have proposed improved heuristics for SV
    
[^200]: StreaMulT: 流式多模态Transformer用于异构和任意长的序列数据

    StreaMulT: Streaming Multimodal Transformer for Heterogeneous and Arbitrary Long Sequential Data

    [https://arxiv.org/abs/2110.08021](https://arxiv.org/abs/2110.08021)

    提出一种新的流式多模态Transformer模型，用于处理异构和任意长的序列数据，解决了在预测性维护任务中多源数据流的融合和跨时间预测的挑战。

    

    随着工业4.0系统复杂性的增加，预测性维护任务（如故障检测和诊断）带来了新的挑战。相应而实际的情景包括来自不同模态的多源数据流，如传感器测量时间序列、机器图像、文本维护报告等。这些异构多模态流在其采集频率上也不同，可能包含时间上不对齐的信息，并且可以任意长，取决于所考虑的系统和任务。虽然多模态融合在静态环境中已被广泛研究，但据我们所知，以往的工作中没有考虑过与相关任务（如跨时间预测）一起考虑任意长的多模态流。因此，在本文中，我们首先将这种异构多模态学习范式形式化为一种新型的流式设置。为了解决这一挑战，我们...

    arXiv:2110.08021v2 Announce Type: replace-cross  Abstract: The increasing complexity of Industry 4.0 systems brings new challenges regarding predictive maintenance tasks such as fault detection and diagnosis. A corresponding and realistic setting includes multi-source data streams from different modalities, such as sensors measurements time series, machine images, textual maintenance reports, etc. These heterogeneous multimodal streams also differ in their acquisition frequency, may embed temporally unaligned information and can be arbitrarily long, depending on the considered system and task. Whereas multimodal fusion has been largely studied in a static setting, to the best of our knowledge, there exists no previous work considering arbitrarily long multimodal streams alongside with related tasks such as prediction across time. Thus, in this paper, we first formalize this paradigm of heterogeneous multimodal learning in a streaming setting as a new one. To tackle this challenge, we p
    
[^201]: 对抗机器学习：贝叶斯视角

    Adversarial Machine Learning: Bayesian Perspectives

    [https://arxiv.org/abs/2003.03546](https://arxiv.org/abs/2003.03546)

    AML旨在保护机器学习系统免受安全威胁，贝叶斯视角为防御提供了新的好处

    

    对抗机器学习(AML)正在成为一个重要的领域，旨在保护机器学习(ML)系统免受安全威胁：在某些情况下，可能存在敌对方积极操纵输入数据以欺骗学习系统。 这创造了一类新的安全漏洞，ML系统可能会面临，并引入了一种新的被称为敌对稳健性的可信操作所必需的性质。 大部分AML工作都建立在对抗学习系统和准备操纵输入数据的对手之间冲突的博弈论建模之上。 这假设每个代理都了解对手的兴趣和不确定性判断，从而促进基于Nash均衡的推理。 然而，在AML典型的安全方案中，这种共同知识假设并不现实。 在回顾了这种博弈论方法之后，我们讨论了贝叶斯视角在防御中提供的好处

    arXiv:2003.03546v2 Announce Type: replace  Abstract: Adversarial Machine Learning (AML) is emerging as a major field aimed at protecting machine learning (ML) systems against security threats: in certain scenarios there may be adversaries that actively manipulate input data to fool learning systems. This creates a new class of security vulnerabilities that ML systems may face, and a new desirable property called adversarial robustness essential to trust operations based on ML outputs. Most work in AML is built upon a game-theoretic modelling of the conflict between a learning system and an adversary, ready to manipulate input data. This assumes that each agent knows their opponent's interests and uncertainty judgments, facilitating inferences based on Nash equilibria. However, such common knowledge assumption is not realistic in the security scenarios typical of AML. After reviewing such game-theoretic approaches, we discuss the benefits that Bayesian perspectives provide when defendin
    
[^202]: 优化大规模语言模型用于漏洞检测

    Finetuning Large Language Models for Vulnerability Detection. (arXiv:2401.17010v1 [cs.CR])

    [http://arxiv.org/abs/2401.17010](http://arxiv.org/abs/2401.17010)

    本文优化了大规模语言模型用于源代码中的漏洞检测任务，通过微调最先进的代码语言模型WizardCoder并改进其训练过程和策略，实现了对漏洞数据集的分类性能的提升。

    

    本文介绍了对大规模语言模型进行微调，并将其用于源代码中的漏洞检测的结果。我们利用最先进的语言模型StarCoder的改进版本WizardCoder，并通过进一步微调将其适应于漏洞检测任务。为了加速训练，我们修改了WizardCoder的训练过程，并探究了最佳的训练策略。针对负样本远多于正样本的不平衡数据集，我们还尝试了不同的技术来提高分类性能。微调后的WizardCoder模型在平衡和不平衡的漏洞数据集上在ROC AUC和F1度量上实现了改进，证明了将预训练的语言模型用于源代码中的漏洞检测的有效性。主要贡献包括对最先进的代码语言模型WizardCoder进行微调，提高其训练速度而不影响性能，并对训练过程和策略进行了优化。

    This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, 
    
[^203]: 量子启发的机器学习用于分子对接

    Quantum-Inspired Machine Learning for Molecular Docking. (arXiv:2401.12999v1 [physics.chem-ph])

    [http://arxiv.org/abs/2401.12999](http://arxiv.org/abs/2401.12999)

    量子启发的机器学习方法在分子对接中取得了显著的改进，通过结合量子特性和深度学习在编码的分子空间中学习的梯度，提高了盲目对接的成功率。

    

    分子对接是构建基于结构的药物设计的重要工具，可以加快药物开发的效率。蛋白质和小分子之间的复杂和动态结合过程需要在广泛的空间范围内进行搜索和采样。传统的对接方法通过搜索可能的结合位点和构象来实现，计算复杂度高，在盲目对接中效果不佳。受到这一点的启发，我们通过将量子启发算法与通过深度学习在编码的分子空间中学习的梯度相结合，实现了在盲目对接中的改进。数值仿真结果表明，我们的方法在传统的对接算法和基于深度学习的算法上表现出了超过10%的提升。与目前最先进的基于深度学习的对接算法DiffDock相比，Top-1（RMSD<2）的成功率从33%提高到35%。

    Molecular docking is an important tool for structure-based drug design, accelerating the efficiency of drug development. Complex and dynamic binding processes between proteins and small molecules require searching and sampling over a wide spatial range. Traditional docking by searching for possible binding sites and conformations is computationally complex and results poorly under blind docking. Quantum-inspired algorithms combining quantum properties and annealing show great advantages in solving combinatorial optimization problems. Inspired by this, we achieve an improved in blind docking by using quantum-inspired combined with gradients learned by deep learning in the encoded molecular space. Numerical simulation shows that our method outperforms traditional docking algorithms and deep learning-based algorithms over 10\%. Compared to the current state-of-the-art deep learning-based docking algorithm DiffDock, the success rate of Top-1 (RMSD<2) achieves an improvement from 33\% to 35
    
[^204]: 注意力、蒸馏和表格化：走向实用的基于神经网络的预取模型

    Attention, Distillation, and Tabularization: Towards Practical Neural Network-Based Prefetching. (arXiv:2401.06362v1 [cs.NE])

    [http://arxiv.org/abs/2401.06362](http://arxiv.org/abs/2401.06362)

    我们提出了一种基于表格化的方法，通过将注意力模型转换为层次结构的表格查找，显著降低了预取模型的复杂度和推理延迟，同时保持了高准确性。通过我们的方法，我们开发了一个DART预取模型，在减少计算量的情况下只有轻微的性能下降。

    

    基于注意力的神经网络在准确的内存访问预测中表现出了高效性，这是数据预取的一个关键步骤。然而，这些模型所带来的计算开销造成了高推理延迟，限制了它们作为实际预取模型的可行性。为了弥合这一差距，我们提出了一种基于表格化的新方法，该方法显著降低了模型复杂度和推理延迟，同时又不牺牲预测准确性。我们的新颖的表格化方法将一个经过蒸馏的具有高精确度的注意力模型作为输入，将其昂贵的矩阵乘法转换成快速表格查找的层次结构。作为上述方法的示例，我们开发了DART，一个由简单表格层次结构组成的预取模型。在F1得分下降了0.09的情况下，DART从大型注意力模型中减少了99.99%的算术运算，从蒸馏模型中减少了91.83%的运算量。

    Attention-based Neural Networks (NN) have demonstrated their effectiveness in accurate memory access prediction, an essential step in data prefetching. However, the substantial computational overheads associated with these models result in high inference latency, limiting their feasibility as practical prefetchers. To close the gap, we propose a new approach based on tabularization that significantly reduces model complexity and inference latency without sacrificing prediction accuracy. Our novel tabularization methodology takes as input a distilled, yet highly accurate attention-based model for memory access prediction and efficiently converts its expensive matrix multiplications into a hierarchy of fast table lookups. As an exemplar of the above approach, we develop DART, a prefetcher comprised of a simple hierarchy of tables. With a modest 0.09 drop in F1-score, DART reduces 99.99% of arithmetic operations from the large attention-based model and 91.83% from the distilled model. DAR
    
[^205]: 通过分类器校准实现反欺诈预防的决策解耦

    Decoupling Decision-Making in Fraud Prevention through Classifier Calibration for Business Logic Action. (arXiv:2401.05240v1 [cs.LG])

    [http://arxiv.org/abs/2401.05240](http://arxiv.org/abs/2401.05240)

    该论文研究了通过分类器校准来实现反欺诈预防中的决策解耦。通过使用校准策略，他们发现等距和贝塔校准方法在训练和测试数据之间发生变化的场景下表现突出。这些结果为优化解耦努力的从业者提供了宝贵的见解。

    

    机器学习模型通常专注于特定目标，比如创建分类器，通常基于商业环境中已知的人群特征分布。然而，计算个体特征的模型随时间而适应，以提高精度，引入解耦的概念：从点评估转向数据分布。我们使用校准策略作为解耦机器学习（ML）分类器与基于得分的业务逻辑框架中的行动的策略。为了评估这些策略，我们使用一个真实的商业场景和多个ML模型进行了比较分析。我们的发现突出了这种方法的权衡和性能影响，并为寻求优化解耦努力的从业者提供了有价值的见解。特别是在训练和测试数据之间存在转变的情况下，等距和贝塔校准方法表现出色。

    Machine learning models typically focus on specific targets like creating classifiers, often based on known population feature distributions in a business context. However, models calculating individual features adapt over time to improve precision, introducing the concept of decoupling: shifting from point evaluation to data distribution. We use calibration strategies as strategy for decoupling machine learning (ML) classifiers from score-based actions within business logic frameworks. To evaluate these strategies, we perform a comparative analysis using a real-world business scenario and multiple ML models. Our findings highlight the trade-offs and performance implications of the approach, offering valuable insights for practitioners seeking to optimize their decoupling efforts. In particular, the Isotonic and Beta calibration methods stand out for scenarios in which there is shift between training and testing data.
    
[^206]: 隐私保护的神经图数据库

    Privacy-Preserving Neural Graph Databases. (arXiv:2312.15591v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2312.15591](http://arxiv.org/abs/2312.15591)

    隐私保护的神经图数据库结合了图数据库和神经网络的优势，能够高效存储、检索和分析图结构数据。然而，这种能力也带来了潜在的隐私风险。

    

    在大数据和快速发展的信息系统时代，高效准确地检索数据变得日益重要。神经图数据库（NGDB）是一种强大的范式，将图数据库（图形数据库）和神经网络的优势相结合，实现了对图结构数据的高效存储、检索和分析。神经嵌入存储和复杂神经逻辑查询回答为NGDB提供了泛化能力。当图形不完整时，神经图数据库可以通过提取潜在模式和表示来填补图结构中的空缺，揭示隐藏的关系并实现准确的查询回答。然而，这种能力也带来了潜在的隐私风险，因为恶意攻击者可以使用精心设计的组合查询推断出更多敏感信息，例如通过比较图数据库中Turing奖得主的答案集。

    In the era of big data and rapidly evolving information systems, efficient and accurate data retrieval has become increasingly crucial. Neural graph databases (NGDBs) have emerged as a powerful paradigm that combines the strengths of graph databases (graph DBs) and neural networks to enable efficient storage, retrieval, and analysis of graph-structured data. The usage of neural embedding storage and complex neural logical query answering provides NGDBs with generalization ability. When the graph is incomplete, by extracting latent patterns and representations, neural graph databases can fill gaps in the graph structure, revealing hidden relationships and enabling accurate query answering. Nevertheless, this capability comes with inherent trade-offs, as it introduces additional privacy risks to the database. Malicious attackers can infer more sensitive information in the database using well-designed combinatorial queries, such as by comparing the answer sets of where Turing Award winner
    
[^207]: 在Transformer中定位跨任务序列继续电路

    Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.04131](http://arxiv.org/abs/2311.04131)

    通过分析和比较Transformer模型中类似的序列继续任务的电路，研究发现共享的计算结构可以提高模型的行为预测能力、错误识别能力和编辑过程的安全性。

    

    虽然Transformer模型在语言任务上展现出强大的能力，但其复杂的架构使其难以解释。最近的研究旨在将Transformer模型还原为可读的电路表示，用于实现算法功能。我们通过分析和比较类似的序列继续任务的电路来扩展这项研究，其中包括数字、数字词和月份的递增序列。通过应用电路分析技术，我们确定了负责检测序列成员和预测序列中下一个成员的关键子电路。我们的分析揭示了语义相关序列依赖于具有类似作用的共享电路子图。总体而言，记录共享的计算结构能够更好地预测模型行为，识别错误，并进行更安全的编辑过程。这种对Transformer的机械理解是构建更健壮、调试和编辑更安全的模型的关键一步。

    While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust,
    
[^208]: 论费曼-卡克训练部分贝叶斯神经网络

    On Feynman--Kac training of partial Bayesian neural networks. (arXiv:2310.19608v1 [cs.LG])

    [http://arxiv.org/abs/2310.19608](http://arxiv.org/abs/2310.19608)

    本文提出了一种将部分贝叶斯神经网络训练转化为模拟费曼-卡克模型的高效采样训练策略，并通过各种数据集的实验证明其在预测性能方面优于现有技术。

    

    最近，部分贝叶斯神经网络(pBNNs)被证明与全贝叶斯神经网络具有竞争力，但pBNNs在潜变量空间中往往是多峰的，因此用参数模型来近似是具有挑战性的。为了解决这个问题，我们提出了一种高效的基于采样的训练策略，即将pBNN的训练转化为模拟费曼-卡克模型。我们还描述了序贯蒙特卡洛采样器的变种，使我们能够以可行的计算成本同时估计参数和该模型的潜在后验分布。我们在各种合成和真实世界的数据集上展示了我们提出的训练方案在预测性能方面优于现有技术。

    Recently, partial Bayesian neural networks (pBNNs), which only consider a subset of the parameters to be stochastic, were shown to perform competitively with full Bayesian neural networks. However, pBNNs are often multi-modal in the latent-variable space and thus challenging to approximate with parametric models. To address this problem, we propose an efficient sampling-based training strategy, wherein the training of a pBNN is formulated as simulating a Feynman--Kac model. We then describe variations of sequential Monte Carlo samplers that allow us to simultaneously estimate the parameters and the latent posterior distribution of this model at a tractable computational cost. We show on various synthetic and real-world datasets that our proposed training scheme outperforms the state of the art in terms of predictive performance.
    
[^209]: 基于流的分布鲁棒优化

    Flow-based Distributionally Robust Optimization. (arXiv:2310.19253v1 [cs.LG])

    [http://arxiv.org/abs/2310.19253](http://arxiv.org/abs/2310.19253)

    这项研究提出了一种称为FlowDRO的计算高效框架，用于解决基于流的分布鲁棒优化问题，通过使用流模型和Wasserstein近端梯度流类型的算法，实现了对具有更大样本大小的问题的可扩展性和更好的泛化能力。

    

    我们提出了一种称为FlowDRO的计算高效框架，用于解决基于流的分布鲁棒优化（DRO）问题，其中要求最坏情况分布（也称为最不利分布，LFD）是连续的，从而使得算法能够可扩展到具有更大样本大小的问题，并实现对诱导的鲁棒算法的更好泛化能力。为了解决计算上具有挑战性的无限维优化问题，我们利用基于流的模型，在数据分布和目标分布之间进行连续时间可逆传输映射，并开发了一种Wasserstein近端梯度流类型的算法。在实践中，我们通过梯度下降逐步训练块内的神经网络序列来参数化传输映射。我们的计算框架通用，能够处理高维数据和大样本大小，并可用于各种应用。

    We present a computationally efficient framework, called \texttt{FlowDRO}, for solving flow-based distributionally robust optimization (DRO) problems with Wasserstein uncertainty sets, when requiring the worst-case distribution (also called the Least Favorable Distribution, LFD) to be continuous so that the algorithm can be scalable to problems with larger sample sizes and achieve better generalization capability for the induced robust algorithms. To tackle the computationally challenging infinitely dimensional optimization problem, we leverage flow-based models, continuous-time invertible transport maps between the data distribution and the target distribution, and develop a Wasserstein proximal gradient flow type of algorithm. In practice, we parameterize the transport maps by a sequence of neural networks progressively trained in blocks by gradient descent. Our computational framework is general, can handle high-dimensional data with large sample sizes, and can be useful for various
    
[^210]: 异构联邦学习与群体感知提示调整

    Heterogeneous Federated Learning with Group-Aware Prompt Tuning. (arXiv:2310.18285v1 [cs.LG])

    [http://arxiv.org/abs/2310.18285](http://arxiv.org/abs/2310.18285)

    本文研究了在异构联邦学习中利用预训练的Transformer和高效的提示调整策略，通过学习共享和群体提示实现获取通用知识和个性化知识，以训练适应不同本地数据分布的全局模型。

    

    Transformer在各种机器学习任务中取得了显著的成功，促使它们被广泛采用。本文探索了它们在联邦学习（FL）领域的应用，特别关注具有不同本地数据集的异构场景。为了满足FL的计算和通信需求，我们利用预训练的Transformer，并使用高效的提示调整策略。我们的策略引入了同时学习共享和群体提示的概念，能够同时获取通用知识和群体特定知识。此外，提示选择模块为每个输入分配个性化的群体提示，使全局模型与每个客户端数据分布对齐。这种方法使我们能够训练一个单一的全局模型，能够自动适应不同的本地客户端数据分布，而无需进行本地微调。通过这种方式，我们提出的方法有效地搭建了链接

    Transformers have achieved remarkable success in various machine-learning tasks, prompting their widespread adoption. In this paper, we explore their application in the context of federated learning (FL), with a particular focus on heterogeneous scenarios where individual clients possess diverse local datasets. To meet the computational and communication demands of FL, we leverage pre-trained Transformers and use an efficient prompt-tuning strategy. Our strategy introduces the concept of learning both shared and group prompts, enabling the acquisition of universal knowledge and group-specific knowledge simultaneously. Additionally, a prompt selection module assigns personalized group prompts to each input, aligning the global model with the data distribution of each client. This approach allows us to train a single global model that can automatically adapt to various local client data distributions without requiring local fine-tuning. In this way, our proposed method effectively bridge
    
[^211]: BLP 2023任务2：情感分析

    BLP 2023 Task 2: Sentiment Analysis. (arXiv:2310.16183v1 [cs.CL])

    [http://arxiv.org/abs/2310.16183](http://arxiv.org/abs/2310.16183)

    BLP 2023任务2是关于情感分析的共享任务，吸引了71个参与者。参与者通过各种方法，包括经典机器学习模型和大型语言模型，提交了597个运行结果。本文提供了任务的详细设置和参与者提交系统的概述。

    

    我们总结了作为BLP 2023创新工作坊的一部分举办的BLP情感共享任务。该任务的定义是在给定的社交媒体文本中检测情感。该任务吸引了71个参与者的关注，其中在开发和评估阶段分别有29个和30个团队提交了系统。总共，参与者提交了597个运行结果。然而，总共有15个团队提交了系统描述论文。提交的系统涵盖了从经典的机器学习模型、微调预训练模型到在零样本和少样本设置中利用大型语言模型（LLMs）的各种方法。在本文中，我们详细介绍了该任务的设置，包括数据集的开发和评估设置。此外，我们简要概述了参与者提交的系统。共享任务的所有数据集和评估脚本已公开可用。

    We present an overview of the BLP Sentiment Shared Task, organized as part of the inaugural BLP 2023 workshop, co-located with EMNLP 2023. The task is defined as the detection of sentiment in a given piece of social media text. This task attracted interest from 71 participants, among whom 29 and 30 teams submitted systems during the development and evaluation phases, respectively. In total, participants submitted 597 runs. However, a total of 15 teams submitted system description papers. The range of approaches in the submitted systems spans from classical machine learning models, fine-tuning pre-trained models, to leveraging Large Language Model (LLMs) in zero- and few-shot settings. In this paper, we provide a detailed account of the task setup, including dataset development and evaluation setup. Additionally, we provide a brief overview of the systems submitted by the participants. All datasets and evaluation scripts from the shared task have been made publicly available for the res
    
[^212]: 外部验证策略评估结合试验和观察数据

    Externally Valid Policy Evaluation Combining Trial and Observational Data. (arXiv:2310.14763v1 [stat.ME])

    [http://arxiv.org/abs/2310.14763](http://arxiv.org/abs/2310.14763)

    这项研究提出了一种结合试验和观察数据的外部有效策略评估方法，利用试验数据对目标人群上的政策结果进行有效推断，并给出了可验证的评估结果。

    

    随机试验被广泛认为是评估决策策略影响的金 standard。然而，试验数据来自可能与目标人群不同的人群，这引发了外部效度（也称为泛化能力）的问题。在本文中，我们试图利用试验数据对目标人群上的政策结果进行有效推断。目标人群的额外协变量数据用于模拟试验研究中个体的抽样。我们开发了一种方法，在任何指定的模型未校准范围内产生可验证的基于试验的政策评估。该方法是非参数的，即使样本是有限的，有效性也得到保证。使用模拟和实际数据说明了认证的政策评估结果。

    Randomized trials are widely considered as the gold standard for evaluating the effects of decision policies. Trial data is, however, drawn from a population which may differ from the intended target population and this raises a problem of external validity (aka. generalizability). In this paper we seek to use trial data to draw valid inferences about the outcome of a policy on the target population. Additional covariate data from the target population is used to model the sampling of individuals in the trial study. We develop a method that yields certifiably valid trial-based policy evaluations under any specified range of model miscalibrations. The method is nonparametric and the validity is assured even with finite samples. The certified policy evaluations are illustrated using both simulated and real data.
    
[^213]: 何时才能使剧本在错误规范下保持稳定? (arXiv:2310.09358v1 [cs.LG])

    When are Bandits Robust to Misspecification?. (arXiv:2310.09358v1 [cs.LG])

    [http://arxiv.org/abs/2310.09358](http://arxiv.org/abs/2310.09358)

    该论文研究了参数化的强盗算法和情境化的强盗算法在真实奖励与模型之间存在误差的情况下的稳定性，并找到了依赖于问题实例和模型类的充分条件，使得经典算法如ε-贪心和LinUCB能够在时间范围内保持次线性的遗憾保障。

    

    参数特征为基础的奖励模型广泛应用于决策问题，如强盗算法和情境化的强盗算法。通常的假设是可行性，即行为的真实奖励完全由某个参数化模型解释。然而，我们关注的是真实奖励与模型类之间存在（可能显著）的误差的情况。对于参数化的强盗和情境化的强盗，我们识别出依赖问题实例和模型类的充分条件，使得经典算法如ε-贪心和LinUCB在即使奖励存在严重误差的情况下，也能够在时间范围内保证次线性（次于时间范围）的遗憾保障。这与现有的针对错误规范的最坏情况结果形成对比，后者显示遗憾边界随时间成线性比例增长，并且说明存在一个相当大的强盗问题实例集合在错误规范下仍然稳定。

    Parametric feature-based reward models are widely employed by algorithms for decision making settings such as bandits and contextual bandits. The typical assumption under which they are analysed is realizability, i.e., that the true rewards of actions are perfectly explained by some parametric model in the class. We are, however, interested in the situation where the true rewards are (potentially significantly) misspecified with respect to the model class. For parameterized bandits and contextual bandits, we identify sufficient conditions, depending on the problem instance and model class, under which classic algorithms such as $\epsilon$-greedy and LinUCB enjoy sublinear (in the time horizon) regret guarantees under even grossly misspecified rewards. This is in contrast to existing worst-case results for misspecified bandits which show regret bounds that scale linearly with time, and shows that there can be a nontrivially large set of bandit instances that are robust to misspecificati
    
[^214]: 图增强优化器用于结构感知推荐嵌入演化

    Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution. (arXiv:2310.03032v1 [cs.IR])

    [http://arxiv.org/abs/2310.03032](http://arxiv.org/abs/2310.03032)

    本文提出了一种新颖的结构感知嵌入演化(SEvo)机制，能够以较低的计算开销将图结构信息注入到嵌入中，从而在现代推荐系统中实现更高效的性能。

    

    嵌入在现代推荐系统中起着关键作用，因为它们是真实世界实体的虚拟表示，并且是后续决策模型的基础。本文提出了一种新颖的嵌入更新机制，称为结构感知嵌入演化(SEvo)，以鼓励相关节点在每一步中以类似的方式演化。与通常作为中间部分的GNN（图神经网络）不同，SEvo能够直接将图结构信息注入到嵌入中，且在训练过程中计算开销可忽略。本文通过理论分析验证了SEvo的收敛性质及其可能的改进版本，以证明设计的有效性。此外，SEvo可以无缝集成到现有的优化器中，以实现最先进性能。特别是，在矩估计校正的SEvo增强AdamW中，证明了一致的改进效果在多种模型和数据集上，为有效推荐了一种新的技术路线。

    Embedding plays a critical role in modern recommender systems because they are virtual representations of real-world entities and the foundation for subsequent decision models. In this paper, we propose a novel embedding update mechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage related nodes to evolve similarly at each step. Unlike GNN (Graph Neural Network) that typically serves as an intermediate part, SEvo is able to directly inject the graph structure information into embedding with negligible computational overhead in training. The convergence properties of SEvo as well as its possible variants are theoretically analyzed to justify the validity of the designs. Moreover, SEvo can be seamlessly integrated into existing optimizers for state-of-the-art performance. In particular, SEvo-enhanced AdamW with moment estimate correction demonstrates consistent improvements across a spectrum of models and datasets, suggesting a novel technical route to effectively 
    
[^215]: 改进的精细离散化方法提高自适应在线学习

    Improving Adaptive Online Learning Using Refined Discretization. (arXiv:2309.16044v1 [cs.LG])

    [http://arxiv.org/abs/2309.16044](http://arxiv.org/abs/2309.16044)

    通过一种新颖的连续时间启发式算法，提高了自适应在线学习的效果，将梯度方差的依赖性从次优的$O(\sqrt{V_T\log V_T})$改进到最优速率$O(\sqrt{V_T})$，并可适用于未知Lipschitz常数的情况。

    

    我们研究了具有Lipschitz损失的非约束在线线性优化问题。目标是同时达到（i）二阶梯度自适应性；和（ii）比较器范数自适应性，也被称为文献中的“参数自由性”。现有的遗憾界（Cutkosky和Orabona，2018；Mhammedi和Koolen，2020；Jacobsen和Cutkosky，2022）对于梯度方差$V_T$有次优的$O(\sqrt{V_T\log V_T})$依赖性，而本工作利用一种新颖的连续时间启发式算法将其改进为最优速率$O(\sqrt{V_T})$，而无需任何不切实际的加倍技巧。这一结果可以推广到未知Lipschitz常数的情况，消除了先前工作中的范围比率问题（Mhammedi和Koolen，2020）。具体来说，我们首先展示了在问题的连续时间类比中可以相当容易地实现目标的同时适应性，其中环境由任意连续半鞘式建模。然后，我们的关键创新是

    We study unconstrained Online Linear Optimization with Lipschitz losses. The goal is to simultaneously achieve ($i$) second order gradient adaptivity; and ($ii$) comparator norm adaptivity also known as "parameter freeness" in the literature. Existing regret bounds (Cutkosky and Orabona, 2018; Mhammedi and Koolen, 2020; Jacobsen and Cutkosky, 2022) have the suboptimal $O(\sqrt{V_T\log V_T})$ dependence on the gradient variance $V_T$, while the present work improves it to the optimal rate $O(\sqrt{V_T})$ using a novel continuous-time-inspired algorithm, without any impractical doubling trick. This result can be extended to the setting with unknown Lipschitz constant, eliminating the range ratio problem from prior works (Mhammedi and Koolen, 2020).  Concretely, we first show that the aimed simultaneous adaptivity can be achieved fairly easily in a continuous time analogue of the problem, where the environment is modeled by an arbitrary continuous semimartingale. Then, our key innovation 
    
[^216]: ReConcile：圆桌会议通过多元LLM的共识改进推理能力

    ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs. (arXiv:2309.13007v1 [cs.CL])

    [http://arxiv.org/abs/2309.13007](http://arxiv.org/abs/2309.13007)

    ReConcile是一个通过多轮讨论和投票机制来增强LLM推理能力的多模型多代理框架。

    

    大型语言模型（LLM）仍然在复杂的推理任务上遇到困难。受到心智社会理论（Minsky, 1988）的启发，我们提出了ReConcile，这是一个多模型多代理的框架，旨在通过多样的LLM代理人之间的圆桌会议来促进多样的思想和讨论，从而改进一致性。ReConcile通过进行多轮讨论、学习说服其他代理人改进答案以及采用置信度加权投票机制来增强LLM的推理能力。在每一轮中，ReConcile通过“讨论提示”来启动代理人间的讨论，其中包括上一轮每个代理人生成的答案和解释的分组、它们的不确定性以及用于说服其他代理人的答案修正人类解释的演示。这个讨论提示使每个代理人能够根据其他代理人的见解修订自己的回答。一旦达成一致并结束讨论，ReConcile执行一次全体投票以确定最终答案。

    Large Language Models (LLMs) still struggle with complex reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents to foster diverse thoughts and discussion for improved consensus. ReConcile enhances the reasoning capabilities of LLMs by holding multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their uncertainties, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. This discussion prompt enables each agent to revise their responses in light of insights from other agents. Once a consensus is reached and the discussion ends, ReConcil
    
[^217]: Transformers作为支持向量机

    Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])

    [http://arxiv.org/abs/2308.16898](http://arxiv.org/abs/2308.16898)

    这项工作建立了自注意力和硬间隔支持向量机问题之间的正式等价关系，通过转换器架构的优化几何来解决自然语言处理问题，同时揭示了梯度下降优化的转换器的隐式偏差。

    

    自从"Attention Is All You Need"中引入转换器架构以来，它在自然语言处理领域取得了革命性的进展。转换器中的注意力层接受输入令牌序列$X$并通过计算softmax$(XQK^\top X^\top)$的成对相似性使它们相互作用，其中$(K,Q)$是可训练的键-查询参数。在这项工作中，我们建立了自注意力优化几何和一个硬间隔支持向量机问题之间的正式等价关系，通过对令牌对的外积施加线性约束，将最佳输入令牌与非最佳令牌分离。这个形式主义使我们能够表征梯度下降优化的单层转换器的隐式偏差：(1)优化注意力层，使用可变正则化参数$(K,Q)$，收敛的方向是一个最小化综合参数$W=KQ^\top$的核范数的支持向量机解决方案。而直接使用$W$进行参数化则最小化一个Frobenius范数目标。

    Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. 
    
[^218]: LLM中的时间旅行：追踪大型语言模型中的数据污染

    Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])

    [http://arxiv.org/abs/2308.08493](http://arxiv.org/abs/2308.08493)

    该论文提出了一种用于识别大型语言模型（LLMs）中数据污染的简单而有效的方法。通过对随机样本中的单个实例进行分析，以及使用“引导指令”来评估整个数据集分区的污染程度，可以准确地识别污染的实例和分区。

    

    数据污染是指大型语言模型（LLMs）的训练数据中存在来自下游任务的测试数据，这可能是理解LLMs在其他任务上有效性的一个重要问题。我们提出了一种简单而有效的方法来识别LLMs中的数据污染。我们的方法核心是通过识别从小的随机样本中抽取的单个实例中的潜在污染，然后评估整个数据集分区是否受到污染。为了估计单个实例的污染程度，我们使用了“引导指令”：即一个由数据集名称、分区类型和参考实例的初始部分组成的提示，要求LLM完成它。如果LLM的输出与参考实例的后一部分完全或接近匹配，那么该实例被标记为受到污染。为了了解整个分区是否受到污染，我们提出了两个想法。第一个想法是标记一个数据集的分区，该分区中的实例大多数都被判断为受到污染。

    Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
    
[^219]: 迈向真正的微分方程发现

    Towards true discovery of the differential equations. (arXiv:2308.04901v1 [cs.LG])

    [http://arxiv.org/abs/2308.04901](http://arxiv.org/abs/2308.04901)

    本文研究了独立方程发现的先决条件和工具，并解决了在正确方程未知的情况下评估发现方程的挑战，旨在为在没有先验方程知识的情况下可靠地发现方程提供洞察力。

    

    微分方程发现是机器学习的一个子领域，用于开发可解释的模型，特别是在与自然相关的应用中。通过巧妙地结合运动方程的一般参数形式和合适的微分项，算法可以自动从数据中发现方程。本文探讨了独立方程发现的先决条件和工具，消除了对方程形式假设的需求。我们重点解决了在正确方程未知的情况下评估发现方程的适当性的挑战，旨在为在没有先验方程知识的情况下可靠地发现方程提供洞察力。

    Differential equation discovery, a machine learning subfield, is used to develop interpretable models, particularly in nature-related applications. By expertly incorporating the general parametric form of the equation of motion and appropriate differential terms, algorithms can autonomously uncover equations from data. This paper explores the prerequisites and tools for independent equation discovery without expert input, eliminating the need for equation form assumptions. We focus on addressing the challenge of assessing the adequacy of discovered equations when the correct equation is unknown, with the aim of providing insights for reliable equation discovery without prior knowledge of the equation form.
    
[^220]: 黑盒变分推断的线性收敛性：我们应该坚持到底吗？

    Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. (arXiv:2307.14642v1 [stat.ML])

    [http://arxiv.org/abs/2307.14642](http://arxiv.org/abs/2307.14642)

    本文证明了带有控制变量的黑盒变分推断在完美变分族规范下以几何速度收敛，为BBVI提供了收敛性保证，同时提出了对熵梯度估计器的改进，对比了STL估计器，并给出了明确的非渐近复杂度保证。

    

    我们证明了带有控制变量的黑盒变分推断（BBVI），特别是着陆稳定（STL）估计器，在完美变分族规范下收敛于几何（传统上称为“线性”）速度。特别地，我们证明了STL估计器的梯度方差的二次界限，该界限包括了误指定的变分族。结合先前关于二次方差条件的工作，这直接暗示了在使用投影随机梯度下降的情况下BBVI的收敛性。我们还改进了现有对于正常封闭形式熵梯度估计器的分析，这使得我们能够将其与STL估计器进行比较，并为两者提供明确的非渐进复杂度保证。

    We prove that black-box variational inference (BBVI) with control variates, particularly the sticking-the-landing (STL) estimator, converges at a geometric (traditionally called "linear") rate under perfect variational family specification. In particular, we prove a quadratic bound on the gradient variance of the STL estimator, one which encompasses misspecified variational families. Combined with previous works on the quadratic variance condition, this directly implies convergence of BBVI with the use of projected stochastic gradient descent. We also improve existing analysis on the regular closed-form entropy gradient estimators, which enables comparison against the STL estimator and provides explicit non-asymptotic complexity guarantees for both.
    
[^221]: 多模态讨论变换器：整合文本、图像和图变换器以检测社交媒体上的仇恨言论。

    Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])

    [http://arxiv.org/abs/2307.09312](http://arxiv.org/abs/2307.09312)

    多模态讨论变换器 (mDT) 是一个用于检测在线社交网络中仇恨言论的新颖模型。与传统的仅使用文本的方法不同，mDT通过整体分析文本和图像，结合图变换器捕捉评论周围整个讨论的上下文关系，并通过交织融合层将文本和图像嵌入进行组合。研究发现，捕捉对话的整体视图可以极大地提高检测反社会行为的准确性。

    

    我们提出了一种新颖的多模态基于图的变换器模型，名为多模态讨论变换器（mDT），用于检测在线社交网络中的仇恨言论。与传统的仅使用文本的方法不同，我们将标记评论为仇恨言论的方法围绕文本和图像的整体分析展开。这是通过利用图变换器来捕捉评论周围整个讨论中的上下文关系，并采用交织融合层来组合文本和图像嵌入，而不是单独处理不同的模态。我们将模型的性能与仅处理文本的基线进行比较，还进行了广泛的消融研究。最后，我们展望了多模态解决方案在在线环境中提供社会价值的未来工作，并认为捕捉对话的整体视图极大地推进了检测反社会行为的努力。

    We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
    
[^222]: 随机加权梯度下降通过分布健壮优化

    Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization. (arXiv:2306.09222v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.09222](http://arxiv.org/abs/2306.09222)

    我们通过分布健壮优化和重要性加权的梯度下降技术提升了深度神经网络的性能，并在各种任务上取得了优越的结果。

    

    我们通过在每一次优化步骤中对数据点进行重要性加权，开发了一种提高深度神经网络性能的加权梯度下降技术。我们的方法受到分布健壮优化和f-散度的启发，已知可以得到具有改进的泛化保证的模型。我们的加权方案简单、计算高效，可以与许多流行的优化算法（如SGD和Adam）结合使用。实验证明，我们的方法在各种任务上都表现出了优越性能，包括监督学习和领域适应。值得注意的是，我们在DomainBed和Tabular分类基准上分别比现有最佳结果提升了0.7%和1.44%。此外，我们的算法将BERT在GLUE基准上的性能提升了1.94%，将ViT在ImageNet-1K上的性能提升了1.01%。这些结果表明了所提出方法的有效性，预示着它在改善性能方面的潜力。

    We develop a re-weighted gradient descent technique for boosting the performance of deep neural networks, which involves importance weighting of data points during each optimization step. Our approach is inspired by distributionally robust optimization with f-divergences, which has been known to result in models with improved generalization guarantees. Our re-weighting scheme is simple, computationally efficient, and can be combined with many popular optimization algorithms such as SGD and Adam. Empirically, we demonstrate the superiority of our approach on various tasks, including supervised learning, domain adaptation. Notably, we obtain improvements of +0.7% and +1.44% over SOTA on DomainBed and Tabular classification benchmarks, respectively. Moreover, our algorithm boosts the performance of BERT on GLUE benchmarks by +1.94%, and ViT on ImageNet-1K by +1.01%. These results demonstrate the effectiveness of the proposed approach, indicating its potential for improving performance in 
    
[^223]: AS-GAN增强的传感器数据增强用于制造系统在线监测

    Attention-stacked Generative Adversarial Network (AS-GAN)-empowered Sensor Data Augmentation for Online Monitoring of Manufacturing System. (arXiv:2306.06268v1 [cs.LG])

    [http://arxiv.org/abs/2306.06268](http://arxiv.org/abs/2306.06268)

    本文提出了一种名为AS-GAN的技术，使用数据增强来解决监督式机器学习中的数据不平衡问题，其中AS-GAN有效地学习异常状态数据的基础分布，并生成高质量的数据样本用于在线制造系统监测。

    

    机器学习在先进制造系统的在线感知监测中得到广泛应用。然而，在异常条件下收集的传感器数据通常是不充分的，这会导致监督式机器学习中的严重数据不平衡问题。为了解决这个问题，一种常见的方法是采用数据增强技术，即通过合成生成来增加可用的异常状态数据（即少数样本）。为了有效地生成高质量的少数样本，学习异常状态数据的基础分布至关重要。近年来，基于生成对抗网络（GAN）的方法成为学习数据分布以及执行数据增强的流行技术。然而，在实践中，基于GAN的数据增强生成的样本质量可能会有很大的差异。此外，传感器信号是按时间从制造系统中顺序收集的，这意味着考虑到时间的顺序性是重要的。

    Machine learning (ML) has been extensively adopted for the online sensing-based monitoring in advanced manufacturing systems. However, the sensor data collected under abnormal states are usually insufficient, leading to significant data imbalanced issue for supervised machine learning. A common solution for this issue is to incorporate data augmentation technique, i.e., augmenting the available abnormal states data (i.e., minority samples) via synthetic generation. To generate the high-quality minority samples effectively, it is vital to learn the underlying distribution of the abnormal states data. In recent years, the generative adversarial network (GAN)-based approaches become popular to learn data distribution as well as perform data augmentation. However, in practice, the quality of generated samples from GAN-based data augmentation may vary drastically. In addition, the sensor signals are collected sequentially by time from the manufacturing systems, which means the consideration
    
[^224]: 通过最终层反演进行单模型归因

    Single-Model Attribution via Final-Layer Inversion. (arXiv:2306.06210v1 [cs.CV])

    [http://arxiv.org/abs/2306.06210](http://arxiv.org/abs/2306.06210)

    本文提出了一种利用最终层反演和异常检测的开放式单模型归因方法，解决了以往方法要么局限于封闭式环境、要么需要对生成模型进行不必要的改变的问题。实验结果表明该方法优于现有方法。

    

    最近关于生成模型方面的开创性发展引起了人们对于实用单模型归因的兴趣。这些方法可以预测一个样本是由特定的生成器生成的还是不是，例如，为了证明知识产权盗窃行为。然而，以前的方法要么局限于封闭式环境，要么需要对生成模型进行不必要的改变。本文提出了FLIPAD，一种基于最终层反演和异常检测的开放式单模型归因方法，以解决这些问题。我们展示利用的最终层反演可以简化为一个凸的 Lasso 优化问题，从而使我们的方法在理论上可靠且计算效率高。理论结果还得到了实验研究的支持，证明本文方法的有效性，优于现有方法。

    Recent groundbreaking developments on generative modeling have sparked interest in practical single-model attribution. Such methods predict whether a sample was generated by a specific generator or not, for instance, to prove intellectual property theft. However, previous works are either limited to the closed-world setting or require undesirable changes of the generative model. We address these shortcomings by proposing FLIPAD, a new approach for single-model attribution in the open-world setting based on final-layer inversion and anomaly detection. We show that the utilized final-layer inversion can be reduced to a convex lasso optimization problem, making our approach theoretically sound and computationally efficient. The theoretical findings are accompanied by an experimental study demonstrating the effectiveness of our approach, outperforming the existing methods.
    
[^225]: 无领域偏见批量贝叶斯优化，通过贝叶斯积分处理多种约束条件

    Domain-Agnostic Batch Bayesian Optimization with Diverse Constraints via Bayesian Quadrature. (arXiv:2306.05843v1 [cs.LG])

    [http://arxiv.org/abs/2306.05843](http://arxiv.org/abs/2306.05843)

    本论文提出了cSOBER，一种处理多样化约束条件、离散和混合空间、未知约束以及查询拒绝问题的领域无关型贝叶斯优化算法。

    

    现实世界的优化问题通常具有多样的约束条件、离散和混合空间、高度可并行化等特点。同时，当存在未知约束时，例如在药物发现和动物实验安全性等领域，必须确立未知约束之后才能查询目标函数。现有工作通常仅针对上述某些特征而并非综合考虑。本文提出了cSOBER，一种基于SOBER算法的领域无关型谨慎并行主动采样器，考虑到了未知约束情况下的集成误差的影响并提出了处理方法，处理多种约束条件和未知约束查询拒绝的问题。

    Real-world optimisation problems often feature complex combinations of (1) diverse constraints, (2) discrete and mixed spaces, and are (3) highly parallelisable. (4) There are also cases where the objective function cannot be queried if unknown constraints are not satisfied, e.g. in drug discovery, safety on animal experiments (unknown constraints) must be established before human clinical trials (querying objective function) may proceed. However, most existing works target each of the above three problems in isolation and do not consider (4) unknown constraints with query rejection. For problems with diverse constraints and/or unconventional input spaces, it is difficult to apply these techniques as they are often mutually incompatible. We propose cSOBER, a domain-agnostic prudent parallel active sampler for Bayesian optimisation, based on SOBER of Adachi et al. (2023). We consider infeasibility under unknown constraints as a type of integration error that we can estimate. We propose 
    
[^226]: MC-NN：一种端到端的多通道神经网络方法，用于预测流感病毒宿主和抗原类型。

    MC-NN: An End-to-End Multi-Channel Neural Network Approach for Predicting Influenza A Virus Hosts and Antigenic Types. (arXiv:2306.05587v1 [cs.LG])

    [http://arxiv.org/abs/2306.05587](http://arxiv.org/abs/2306.05587)

    提出了一种利用多通道神经网络模型预测流感A病毒宿主和抗原亚型的方法，数据显示多通道神经网络模型具有较高的准确性和预测能力。

    

    流感对公共卫生构成重大威胁，特别是对老年人、儿童和患有潜在疾病的人来说更为严重。严重病况的发生，如肺炎，凸显了预防流感传播的重要性。准确而具有成本效益的预测流感A病毒的宿主和抗原亚型对于应对这一问题至关重要，特别是在资源有限的地区。在本研究中，我们提出了一种多通道神经网络模型，用于从血凝素和神经氨酸酶蛋白序列预测流感A病毒的宿主和抗原亚型。我们的模型是在一个完整蛋白质序列的全面数据集上进行训练的，并在各种完整和不完整序列的测试数据集上进行评估。结果表明，使用多通道神经网络来预测来自完整和部分蛋白质序列的流感A病毒的宿主和抗原亚型具有潜力和实用性。

    Influenza poses a significant threat to public health, particularly among the elderly, young children, and people with underlying dis-eases. The manifestation of severe conditions, such as pneumonia, highlights the importance of preventing the spread of influenza. An accurate and cost-effective prediction of the host and antigenic sub-types of influenza A viruses is essential to addressing this issue, particularly in resource-constrained regions. In this study, we propose a multi-channel neural network model to predict the host and antigenic subtypes of influenza A viruses from hemagglutinin and neuraminidase protein sequences. Our model was trained on a comprehensive data set of complete protein sequences and evaluated on various test data sets of complete and incomplete sequences. The results demonstrate the potential and practicality of using multi-channel neural networks in predicting the host and antigenic subtypes of influenza A viruses from both full and partial protein sequence
    
[^227]: 数据中动态偏移的状态规范化策略优化

    State Regularized Policy Optimization on Data with Dynamics Shift. (arXiv:2306.03552v1 [cs.LG])

    [http://arxiv.org/abs/2306.03552](http://arxiv.org/abs/2306.03552)

    本文提出了一种叫做 SRPO (状态规范化策略优化) 的算法，该算法利用训练数据中的稳态分布来规范新环境中的策略，在处理具有不同动态的多个环境时表现优异。

    

    在许多实际场景中，强化学习算法使用的数据受到动态偏移的影响，即具有不同的环境动态。目前的大多数方法通过训练上下文编码器来识别环境参数来解决这个问题。根据其环境参数将带有动态漂移的数据分开以训练相应的策略。然而，这些方法可能会出现样本效率低下的问题，因为数据是“特定场景”使用的，针对某个环境训练的策略不能从收集在其他具有不同动态的所有其他环境中的数据中受益。本文发现，在许多具有相似结构和不同动态的环境中，最优策略具有类似的稳态分布。我们利用这种特性，并从具有动态漂移的数据中学习稳态分布，以实现高效的数据重用。这种分布用于规范新环境中训练的策略，导致了 SRPO（状态规范化策略优化）算法的出现。实验结果表明，SRPO 在具有动态偏移的任务上显著优于现有的方法。

    In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\textbf{S}tat
    
[^228]: V2Meow: 通过音乐生成器跟随视觉节拍进行“喵叫”(arXiv:2305.06594v1 [cs.SD])

    V2Meow: Meowing to the Visual Beat via Music Generation. (arXiv:2305.06594v1 [cs.SD])

    [http://arxiv.org/abs/2305.06594](http://arxiv.org/abs/2305.06594)

    V2Meow是一种新方法，通过与O(100K)音频片段配对的视频帧进行训练，生成与各种类型的视频输入的视觉语义相匹配的高质量音频，无需符号音乐数据。

    

    生成与视频视觉内容相匹配的高质量音乐是一项具有挑战性的任务。大多数现有的视觉条件音乐生成系统生成符号音乐数据，如MIDI文件，而不是原始音频波形。考虑到符号音乐数据有限的情况下，这些方法只能为少数乐器或特定类型的视觉输入生成音乐。本文提出了一种名为V2Meow的新方法，它可以生成与各种类型的视频输入的视觉语义相匹配的高质量音频。具体而言，所提出的音乐生成系统是一个多阶段自回归模型，它是通过与从野生音乐视频中挖掘的O(100K)音乐音频片段配对的视频帧进行训练的，而没有涉及任何并行符号音乐数据。V2Meow能够仅在先前训练的从任意静态视频提取的视觉特征的条件下合成高保真度的音频波形。

    Generating high quality music that complements the visual content of a video is a challenging task. Most existing visual conditioned music generation systems generate symbolic music data, such as MIDI files, instead of raw audio waveform. Given the limited availability of symbolic music data, such methods can only generate music for a few instruments or for specific types of visual input. In this paper, we propose a novel approach called V2Meow that can generate high-quality music audio that aligns well with the visual semantics of a diverse range of video input types. Specifically, the proposed music generation system is a multi-stage autoregressive model which is trained with a number of O(100K) music audio clips paired with video frames, which are mined from in-the-wild music videos, and no parallel symbolic music data is involved. V2Meow is able to synthesize high-fidelity music audio waveform solely conditioned on pre-trained visual features extracted from an arbitrary silent vide
    
[^229]: 结构化稀疏动态训练

    Dynamic Sparse Training with Structured Sparsity. (arXiv:2305.02299v1 [cs.LG])

    [http://arxiv.org/abs/2305.02299](http://arxiv.org/abs/2305.02299)

    本文提出了一种结构化稀疏动态训练（DST）方法，学习一种变体的结构化 N:M 稀疏性，其加速在一般情况下通常被支持，可缩减参数和内存占用，同时相较于密集模型，具有减少推理时间的优势。

    

    动态稀疏训练在稀疏神经网络训练中取得了最先进的结果，并匹配了密集模型的泛化性，同时使得稀疏训练和推理成为可能。尽管得到的模型高度稀疏，理论上训练更便宜，但在实际硬件上，使用非结构化稀疏性加速依然具有人们所面临的挑战。在本文中，我们提出一种 DST 方法，学习一种变体的结构化 N:M 稀疏性，其加速在一般情况下通常被支持。此外，我们通过理论分析和实证结果，证明了特定 N:M 稀疏方法（常数扇入）的泛化性能，并展示了一种缩减参数和内存占用的紧凑表示。经过对 PyTorch CPU 实现的简单表示进行推断，我们证明了相较于密集模型，该方法减少了推理时间。我们的源代码可在 https://github.com/calgaryml/condensed-sparsity 上获得。

    DST methods achieve state-of-the-art results in sparse neural network training, matching the generalization of dense models while enabling sparse training and inference. Although the resulting models are highly sparse and theoretically cheaper to train, achieving speedups with unstructured sparsity on real-world hardware is challenging. In this work we propose a DST method to learn a variant of structured N:M sparsity, the acceleration of which in general is commonly supported in commodity hardware. Furthermore, we motivate with both a theoretical analysis and empirical results, the generalization performance of our specific N:M sparsity (constant fan-in), present a condensed representation with a reduced parameter and memory footprint, and demonstrate reduced inference time compared to dense models with a naive PyTorch CPU implementation of the condensed representation Our source code is available at https://github.com/calgaryml/condensed-sparsity
    
[^230]: 拓扑深度学习的架构：拓扑神经网络综述

    Architectures of Topological Deep Learning: A Survey on Topological Neural Networks. (arXiv:2304.10031v1 [cs.LG])

    [http://arxiv.org/abs/2304.10031](http://arxiv.org/abs/2304.10031)

    拓扑深度学习框架提供了从复杂系统相关数据中提取知识的全面架构，通过解决现有工作的符号和术语不一致问题，有望在应用科学和其他领域开拓新局面。

    

    自然界中充满了复杂的系统，其组成部分之间存在错综复杂的关系：从社交网络中个体之间的社交互动到蛋白质中原子之间的静电相互作用。拓扑深度学习（TDL）提供了一个全面的框架来处理和从这些系统相关的数据中提取知识，如预测一个人属于哪个社区或预测一个蛋白质是否可以成为合理的药物开发靶点。TDL已经证明拥有理论和实践上的优势，这为在应用科学和其他领域开拓新局面提供了希望。然而，TDL文献的快速增长也导致了拓扑神经网络（TNN）体系结构符号和术语上的不一致。这对于建立在现有工作基础上和将TNN部署到新的现实问题中都是一个真正的障碍。为了解决这个问题，我们提供了一个易于理解的综述。

    The natural world is full of complex systems characterized by intricate relations between their components: from social interactions between individuals in a social network to electrostatic interactions between atoms in a protein. Topological Deep Learning (TDL) provides a comprehensive framework to process and extract knowledge from data associated with these systems, such as predicting the social community to which an individual belongs or predicting whether a protein can be a reasonable target for drug development. TDL has demonstrated theoretical and practical advantages that hold the promise of breaking ground in the applied sciences and beyond. However, the rapid growth of the TDL literature has also led to a lack of unification in notation and language across Topological Neural Network (TNN) architectures. This presents a real obstacle for building upon existing works and for deploying TNNs to new real-world problems. To address this issue, we provide an accessible introduction 
    
[^231]: 揭示和解决统一视觉-语言模型中的跨任务不一致问题

    Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models. (arXiv:2303.16133v1 [cs.CV])

    [http://arxiv.org/abs/2303.16133](http://arxiv.org/abs/2303.16133)

    该研究提出了一个基准数据集COCOCON，并提出度量方法来衡量模型一致性，研究发现现有的最先进系统在不同任务之间表现出高度不一致性。

    

    随着通用的视觉模型在不同任务上变得越来越有效，保证它们在各自支持的任务中的一致性是非常重要的。人们认为不一致的人工智能模型是不可靠的，这对于依赖它们输出的大型系统来说是更具挑战性的。由于很难确定预测结果是否一致，因此，评估可能包括不同模态输出的非常异构任务之间的一致性是具有挑战性的。因此，我们提出了基准数据集COCOCON，其中我们使用对多个任务的测试实例进行小型但语义上有意义的修改来创建对比集，以更改金标签，并概述了用于通过对比接近原始和修改后的实例来衡量模型一致性的指标。我们发现，最先进的系统在任务之间表现出惊人的不一致性。

    As general purpose vision models get increasingly effective at a wide set of tasks, it is imperative that they be consistent across the tasks they support. Inconsistent AI models are considered brittle and untrustworthy by human users and are more challenging to incorporate into larger systems that take dependencies on their outputs. Measuring consistency between very heterogeneous tasks that might include outputs in different modalities is challenging since it is difficult to determine if the predictions are consistent with one another. As a solution, we introduce a benchmark dataset, COCOCON, where we use contrast sets created by modifying test instances for multiple tasks in small but semantically meaningful ways to change the gold label, and outline metrics for measuring if a model is consistent by ranking the original and perturbed instances across tasks. We find that state-of-the-art systems suffer from a surprisingly high degree of inconsistent behavior across tasks, especially 
    
[^232]: 生成可逆量子神经网络

    Generative Invertible Quantum Neural Networks. (arXiv:2302.12906v2 [hep-ph] UPDATED)

    [http://arxiv.org/abs/2302.12906](http://arxiv.org/abs/2302.12906)

    本论文提出了一种用于生成可逆量子神经网络的算法，并将其应用于LHC数据的处理，结果表明该算法可以在学习和生成复杂数据方面与经典算法的表现相匹配。

    

    可逆神经网络已成为模拟和生成高度复杂数据的工具。我们提出了一种量子门算法用于量子可逆神经网络（QINN），并将其应用于将衰变为轻子的Z玻色子的喷注相关产生的LHC数据，这是粒子对撞机精密测量的标准过程。我们比较了QINN在不同损失函数和训练场景下的表现。对于这个任务，我们发现一个混合的QINN可以在学习和生成复杂数据方面与一个显著更大的完全经典的INN的表现匹配。

    Invertible Neural Networks (INN) have become established tools for the simulation and generation of highly complex data. We propose a quantum-gate algorithm for a Quantum Invertible Neural Network (QINN) and apply it to the LHC data of jet-associated production of a Z-boson that decays into leptons, a standard candle process for particle collider precision measurements. We compare the QINN's performance for different loss functions and training scenarios. For this task, we find that a hybrid QINN matches the performance of a significantly larger purely classical INN in learning and generating complex data.
    
[^233]: 不精确的贝叶斯神经网络

    Imprecise Bayesian Neural Networks. (arXiv:2302.09656v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09656](http://arxiv.org/abs/2302.09656)

    在机器学习和人工智能领域，该论文提出了一种新的算法——不精确的贝叶斯神经网络(IBNNs)。这种算法使用可信区间先验分布集合和似然分布集合进行训练，相比标准的BNNs，可以区分先验和后验的不确定性并量化。此外，IBNNs在贝叶斯灵敏度分析方面具有更强的鲁棒性，并且对分布变化也更加鲁棒。

    

    在机器学习和人工智能中, 确定不确定性和鲁棒性是重要的目标。虽然贝叶斯神经网络使得预测中的不确定性能够被评估，不同来源的不确定性是无法区分的。我们提出了不精确的贝叶斯神经网络（IBNNs），它们可以概括和克服标准BNNs的某些缺点。标准BNNs使用单一的先验分布和似然分布进行训练，而IBNNs使用可信区间先验分布和似然分布进行训练。它们允许区分先验和后验不确定性，并对其进行量化。此外，IBNNs在贝叶斯灵敏度分析方面具有鲁棒性，并且对分布变化比标准BNNs更加鲁棒。它们还可以用于计算具有PAC样本复杂性的结果集。我们将IBNNs应用于两个案例研究：一个是为了人工胰腺控制模拟血糖和胰岛素动力学，另一个是运动规划。

    Uncertainty quantification and robustness to distribution shifts are important goals in machine learning and artificial intelligence. Although Bayesian neural networks (BNNs) allow for uncertainty in the predictions to be assessed, different sources of uncertainty are indistinguishable. We present imprecise Bayesian neural networks (IBNNs); they generalize and overcome some of the drawbacks of standard BNNs. These latter are trained using a single prior and likelihood distributions, whereas IBNNs are trained using credal prior and likelihood sets. They allow to distinguish between aleatoric and epistemic uncertainties, and to quantify them. In addition, IBNNs are robust in the sense of Bayesian sensitivity analysis, and are more robust than BNNs to distribution shift. They can also be used to compute sets of outcomes that enjoy PAC-like properties. We apply IBNNs to two case studies. One, to model blood glucose and insulin dynamics for artificial pancreas control, and two, for motion p
    

