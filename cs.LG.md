# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models](https://arxiv.org/abs/2403.20331) | 本文提出了一个新颖且重要的挑战，即Unsolvable Problem Detection（UPD），用于评估视觉语言模型在视觉问答任务中能否在面对不可解问题时保持答案的能力，并通过广泛实验发现大多数模型存在改进的空间。 |
| [^2] | [ReALM: Reference Resolution As Language Modeling](https://arxiv.org/abs/2403.20329) | 本论文展示了如何利用LLMs创建一个极其有效的系统来解决各种类型的引用问题，通过将参考解析转化为语言建模问题，尽管涉及到屏幕上的实体等不易约简为纯文本形式的实体。 |
| [^3] | [Learning Visual Quadrupedal Loco-Manipulation from Demonstrations](https://arxiv.org/abs/2403.20328) | 通过将走行操纵过程分解为低层强化学习控制器和高层行为克隆规划器，使四足机器人能够执行真实世界的操纵任务。 |
| [^4] | [Localising the Seizure Onset Zone from Single-Pulse Electrical Stimulation Responses with a Transformer](https://arxiv.org/abs/2403.20324) | 本研究通过引入Transformer模型结合跨通道注意力，推动了使用深度学习进行单脉冲电刺激响应的SOZ本地化，在评估中展示了模型对未见患者和电极放置的泛化能力 |
| [^5] | [MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning](https://arxiv.org/abs/2403.20320) | MTLoRA提出了一种新颖的多任务学习模型参数高效训练框架，通过任务不可知和任务特定的低秩适应模块有效地实现了参数空间的分离，使得模型能够灵活处理任务专业化和在多任务学习环境中的相互作用 |
| [^6] | [Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement](https://arxiv.org/abs/2403.20298) | 本文基于评论文本提出了一种双曲CDR方法，以应对推荐系统中的数据稀疏性挑战，避免传统基于距离的领域对齐技术可能引发的问题。 |
| [^7] | [Benchmarking Counterfactual Image Generation](https://arxiv.org/abs/2403.20287) | 提出了一个针对对照图像生成方法的基准测试框架，包含评估对照多个方面的度量标准以及评估三种不同类型的条件图像生成模型性能。 |
| [^8] | [LayerNorm: A key component in parameter-efficient fine-tuning](https://arxiv.org/abs/2403.20284) | LayerNorm在参数高效微调中扮演关键角色，微调output LayerNorm而保持其他部分不变足以在多个GLUE任务中取得竞争力 |
| [^9] | [Sparse multimodal fusion with modal channel attention](https://arxiv.org/abs/2403.20280) | 研究了蒙特卡罗多模态变换器架构在模态样本稀疏对齐时学习稳健嵌入空间的能力，并提出了模态通道注意力（MCA）机制，可以改善生成的嵌入空间质量和下游任务性能。 |
| [^10] | [Latxa: An Open Language Model and Evaluation Suite for Basque](https://arxiv.org/abs/2403.20266) | Latxa是一种用于巴斯克语的大型语言模型系列，在语言熟练度和理解能力方面表现出色，优于所有以前的开放模型，并具有多个评估数据集，填补了巴斯克语高质量基准的不足。 |
| [^11] | [ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models](https://arxiv.org/abs/2403.20262) | 该论文提出了一个新的基准 ELITR-Bench，专注于长上下文语言模型的实际会议助理场景，通过在现有 ELITR 语料库的转录中添加手工制作的问题和真实答案，揭示了开源模型和专有模型之间的差距。 |
| [^12] | [FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation](https://arxiv.org/abs/2403.20261) | FABind+通过改进口袋预测和姿态生成，提升分子对接表现 |
| [^13] | [MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation](https://arxiv.org/abs/2403.20253) | 提出了MedCLIP-SAM框架，结合了CLIP和SAM模型，实现使用少量标记数据的临床图像分割 |
| [^14] | [Using LLMs to Model the Beliefs and Preferences of Targeted Populations](https://arxiv.org/abs/2403.20252) | 使用LLMs建模目标人群的信念和偏好，旨在实现各种应用，评估不同微调方法的效果，并检验其在匹配真实人类受访者偏好方面的能力。 |
| [^15] | [Optimal Policy Learning with Observational Data in Multi-Action Scenarios: Estimation, Risk Preference, and Potential Failures](https://arxiv.org/abs/2403.20250) | 本文讨论了多动作场景中利用观测数据进行最优策略学习的方法，着重探讨了估计、风险偏好和潜在故障三个方面。 |
| [^16] | [Enhancing Dimension-Reduced Scatter Plots with Class and Feature Centroids](https://arxiv.org/abs/2403.20246) | 使用类和特征质心增强降维散点图，提高散点图的可解释性。 |
| [^17] | [Functional Bilevel Optimization for Machine Learning](https://arxiv.org/abs/2403.20233) | 介绍了机器学习中的函数双层优化问题，提出了不依赖于强凸假设的方法，并展示了在仪表回归和强化学习任务中使用神经网络的优势。 |
| [^18] | [An FPGA-Based Reconfigurable Accelerator for Convolution-Transformer Hybrid EfficientViT](https://arxiv.org/abs/2403.20230) | 提出了一种基于FPGA的可重构加速器，用于EfficientViT，在硬件效率方面取得了重要进展 |
| [^19] | [Graph Neural Aggregation-diffusion with Metastability](https://arxiv.org/abs/2403.20221) | 提出了GRADE模型，通过图聚合-扩散方程实现了节点表示的亚稳定性，能够缓解图神经网络中过度平滑的问题。 |
| [^20] | [On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem](https://arxiv.org/abs/2403.20212) | 研究了无监督学习在解决旅行商问题中的泛化能力，结果表明，使用更大的实例大小进行训练并增加嵌入维度可以构建更有效的表示，增强模型解决TSP问题的能力。 |
| [^21] | [Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science](https://arxiv.org/abs/2403.20208) | 本研究旨在利用大型语言模型解决数据科学中表格数据预测任务，通过在丰富的数据集上训练Llama-2模型并进行实际应用，取得显著的改进。 |
| [^22] | [Voice Signal Processing for Machine Learning. The Case of Speaker Isolation](https://arxiv.org/abs/2403.20202) | 本文旨在提供对最常用作音频处理任务中信号分解方法的傅里叶和小波变换的简洁比较分析，以及用于评估语音可 intelligibility 的指标。 |
| [^23] | [Dual Simplex Volume Maximization for Simplex-Structured Matrix Factorization](https://arxiv.org/abs/2403.20197) | 通过使用对偶/极性概念，提出了一种双对偶体积最大化方法，用于解决单纯结构矩阵分解问题，填补了现有SSMF算法家族之间的差距。 |
| [^24] | [Enhancing Lithological Mapping with Spatially Constrained Bayesian Network (SCB-Net): An Approach for Field Data-Constrained Predictions with Uncertainty Evaluation](https://arxiv.org/abs/2403.20195) | 基于空间约束贝叶斯网络（SCB-Net）的新架构旨在有效利用辅助变量信息，以增强岩性制图和解决传统地质统计技术在处理多变量和空间相关性方面的限制。 |
| [^25] | [Homomorphic WiSARDs: Efficient Weightless Neural Network training over encrypted data](https://arxiv.org/abs/2403.20190) | 本文提出了对Wilkie, Stonham和Aleksander的Recognition Device (WiSARD)进行同态评估，然后用于加密数据的训练和推断Weightless Neural Networks (WNNs)的方法，相比较传统的CNNs，WNNs在性能上提供了更好的表现。 |
| [^26] | [Distributed Swarm Learning for Edge Internet of Things](https://arxiv.org/abs/2403.20188) | 本文探讨了一种名为分布式群智能学习（DSL）的新型框架，将人工智能和生物群智能结合在一起，为大规模IoT在无线网络边缘提供高效的解决方案和稳健的工具。 |
| [^27] | [Exploring Pathological Speech Quality Assessment with ASR-Powered Wav2Vec2 in Data-Scarce Context](https://arxiv.org/abs/2403.20184) | 本文提出了一种新颖的方法，在数据稀缺情况下系统在整体音频级别进行学习，同时利用ASR驱动的Wav2Vec2架构作为特征提取器，在语音评估中取得了显著的结果。 |
| [^28] | [CAESAR: Enhancing Federated RL in Heterogeneous MDPs through Convergence-Aware Sampling with Screening](https://arxiv.org/abs/2403.20156) | CAESAR算法通过结合收敛感知采样和筛选机制，有效增强了个体代理在不同MDPs上的学习。 |
| [^29] | [TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods](https://arxiv.org/abs/2403.20150) | TFB通过解决数据领域覆盖不足、对传统方法的刻板印象以及不一致、不灵活的流程等问题，推动了时间序列预测方法基准比较的最新技术发展。 |
| [^30] | [Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets](https://arxiv.org/abs/2403.20149) | 论文研究了在电力市场中使用一致性预测对光伏电力日前预测进行决策的方法，并发现结合特定出价策略可以在保持能量平衡最小的情况下获得高利润。 |
| [^31] | [Designing Poisson Integrators Through Machine Learning](https://arxiv.org/abs/2403.20139) | 本文通过机器学习技术，将泊松积分器的设计理解为解决特定PDE问题的优化过程，实现了对哈密顿-雅可比PDE的简单近似，结合了物理建模和数据的设计趋势。 |
| [^32] | [Sound event localization and classification using WASN in Outdoor Environment](https://arxiv.org/abs/2403.20130) | 本文提出了一种基于深度学习的方法，利用多种特征和注意机制来估计声源的位置和类别，包括引入"声音地图"特征、使用Gammatone滤波器生成更适合室外环境的声学特征，以及集成注意机制来学习通道间的关系。 |
| [^33] | [Application of Machine Learning Algorithms in Classifying Postoperative Success in Metabolic Bariatric Surgery: A Comprehensive Study](https://arxiv.org/abs/2403.20124) | 该研究提出了一种新颖的机器学习方法，用于代谢性减重手术患者的分类，通过对比不同模型和变量类型的有效性，为优化治疗策略提供了重要见解。 |
| [^34] | [Learning using granularity statistical invariants for classification](https://arxiv.org/abs/2403.20122) | LUGSI是第一次引入粒度统计不变量，相较于LUSI，它通过强弱两种收敛机制并最小化预期风险，提高了数据结构信息并成功降维。 |
| [^35] | [Segmentation, Classification and Interpretation of Breast Cancer Medical Images using Human-in-the-Loop Machine Learning](https://arxiv.org/abs/2403.20112) | 本文研究了在医学领域使用人机协同策略训练机器学习模型，具体涉及乳腺癌基因组数据和全切片成像分析，虽然病理学家的参与增强了模型的解释能力，但分类结果不理想，指出了该方法的局限性。 |
| [^36] | [Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation](https://arxiv.org/abs/2403.20109) | Mol-AIR 提出了使用自适应内在奖励的分子强化学习框架，通过利用历史和学习的内在奖励优势，在生成具有期望性质的分子方面表现出卓越性能。 |
| [^37] | [Aggregating Local and Global Features via Selective State Spaces Model for Efficient Image Deblurring](https://arxiv.org/abs/2403.20106) | 我们提出了一种高效的图像去模糊网络，通过选择性状态空间模型聚合局部和全局特征，解决了消除长距离模糊降噪扰动和保持计算效率之间的难题 |
| [^38] | [RealKIE: Five Novel Datasets for Enterprise Key Information Extraction](https://arxiv.org/abs/2403.20101) | RealKIE提供了五个具有挑战性的企业关键信息提取数据集，为投资分析和法律数据处理等任务提供了现实的测试基地，并为NLP模型的发展做出了贡献。 |
| [^39] | [Negative Label Guided OOD Detection with Pretrained Vision-Language Models](https://arxiv.org/abs/2403.20078) | 本文提出了一种新颖的后验OOD检测方法NegLabel，通过利用大量负标签信息，设计了与负标签协作的OOD分数方案，并在多个VLM架构上取得了最先进的性能。 |
| [^40] | [Adaptive Decentralized Federated Learning in Energy and Latency Constrained Wireless Networks](https://arxiv.org/abs/2403.20075) | 本文提出了在考虑能源和延迟约束的情况下，通过优化不同资源预算的设备之间的本地训练轮次数量，以最小化分散式联邦学习（DFL）的损失函数。 |
| [^41] | [Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks](https://arxiv.org/abs/2403.20058) | 提出了MX-ARM，一种基于AI的疾病诊断模型，利用同时功能PET/MR技术，能够在推理过程中同时接受单模态和多模态输入，具有创新的模态分离和重构功能。 |
| [^42] | [Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World](https://arxiv.org/abs/2403.20047) | 本研究探讨了稀疏训练在未知分布数据检测中的可靠性问题，提出了一种新的面向未知的稀疏训练方法，解决了稀疏训练加剧未知数据不可靠性的挑战。 |
| [^43] | [A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO](https://arxiv.org/abs/2403.20033) | 该研究提出了一种新型的决策融合方法，通过选择信息性变量来提高价格预测的准确性和效果 |
| [^44] | [Nonparametric Bellman Mappings for Reinforcement Learning: Application to Robust Adaptive Filtering](https://arxiv.org/abs/2403.20020) | 该论文设计了在再现核希尔伯特空间中的无参数贝尔曼映射，具有丰富的逼近特性，无需对数据的统计特性做任何假设，也不需要了解马尔可夫决策过程的转移概率，可以在没有训练数据的情况下运行，并支持多种操作和应用。 |
| [^45] | [EnCoMP: Enhanced Covert Maneuver Planning using Offline Reinforcement Learning](https://arxiv.org/abs/2403.20016) | 提出了一种增强的导航系统，利用LiDAR数据生成高质量的掩护地图和潜在威胁地图，在复杂环境中通过离线强化学习训练模型，以最大化掩护利用、最小化暴露于威胁并高效到达目标。 |
| [^46] | [On Large Language Models' Hallucination with Regard to Known Facts](https://arxiv.org/abs/2403.20009) | 通过推理动态的角度研究大型语言模型对已知事实的幻觉现象，通过对事实性问题和输出 token 概率动态的分析，揭示了幻觉发生的模式。 |
| [^47] | [DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data](https://arxiv.org/abs/2403.19996) | 提出了一个新颖的深度学习模型，结合了卷积神经网络和双向门控循环单元，以端到端的方式学习局部和全局特征，有效解决了异构IoT传感器数据分类的挑战。 |
| [^48] | [Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer](https://arxiv.org/abs/2403.19979) | 适配器调整方法在持续学习中展现出较优性能，提出了增量调整共享适配器和利用存储原型进行特征采样和更新的方法来增强模型学习能力。 |
| [^49] | [Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output Channel Pruning on Computer Vision Tasks](https://arxiv.org/abs/2403.19969) | SMART剪枝器引入了独立的、可学习的概率掩码、可微的Top k运算符和动态温度参数技巧，在块和输出通道剪枝任务中显示出优越性。 |
| [^50] | [FairRAG: Fair Human Generation via Fair Retrieval Augmentation](https://arxiv.org/abs/2403.19964) | FairRAG框架通过在外部图像数据库检索到的参考图像来提高人类生成中的公平性，并应用简单但有效的去偏策略，从而为生成过程提供来自不同人口统计组的图像。 |
| [^51] | [Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning](https://arxiv.org/abs/2403.19962) | 通过构建特定于代理的数据并细调模型以及设计能够有效激活LLMs推理能力的提示，提出了一种综合方法来增强低参数LLMs的通用代理功能。 |
| [^52] | [Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data](https://arxiv.org/abs/2403.19950) | 研究了OOD泛化设置中的置信集预测问题，提出了一种方法来形成有信心的预测集，并在理论上证明了方法的有效性 |
| [^53] | [TDANet: A Novel Temporal Denoise Convolutional Neural Network With Attention for Fault Diagnosis](https://arxiv.org/abs/2403.19943) | TDANet是一种新颖的具有注意力机制的时间去噪卷积神经网络，旨在改善噪声环境下的故障诊断性能。 |
| [^54] | [DiJiang: Efficient Large Language Models through Compact Kernelization](https://arxiv.org/abs/2403.19928) | DiJiang提出了一种新颖的频域核方法，可以将预训练的基本Transformer模型转化为具有线性复杂度的模型，大大减少训练成本，并在理论上提供更好的逼近效率。 |
| [^55] | [Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2403.19925) | 本研究将Mamba框架整合到Decision Transformer架构中，提出了Decision Mamba，通过在不同决策环境中进行一系列实验，表明了神经网络的架构和训练方法对性能的重要影响 |
| [^56] | [CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning](https://arxiv.org/abs/2403.19918) | CtRL-Sim提出了一种利用离线强化学习生成反应性和可控交通代理的方法，通过在Nocturne模拟器中处理真实世界的驾驶数据来实现这一目标。 |
| [^57] | [MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models](https://arxiv.org/abs/2403.19913) | 提出了用于评估大型语言模型执行文本映射和导航能力的MANGO基准，发现即使是迄今为止最好的语言模型GPT-4在回答涉及映射和导航的问题时表现不佳。 |
| [^58] | [Beyond the Known: Novel Class Discovery for Open-world Graph Learning](https://arxiv.org/abs/2403.19907) | 本文提出了一种名为ORAL的新方法，用于在开放世界图学习中发现新类别，通过半监督原型学习和原型注意力网络解决了新类别和已知类别节点之间的相关性问题。 |
| [^59] | [Disentangling Racial Phenotypes: Fine-Grained Control of Race-related Facial Phenotype Characteristics](https://arxiv.org/abs/2403.19897) | 本文提出了一种新颖的GAN框架，能够实现对面部图像个体种族相关表型属性的细粒度控制，从而分离种族相关的表型特征，为数据驱动的种族偏见缓解策略提供关键支持。 |
| [^60] | [Nonlinearity Enhanced Adaptive Activation Function](https://arxiv.org/abs/2403.19896) | 引入了一种带有甚至立方非线性的简单实现激活函数，通过引入可优化参数使得激活函数具有更大的自由度，可以提高神经网络的准确性，同时不需要太多额外的计算资源。 |
| [^61] | [An Information-Theoretic Framework for Out-of-Distribution Generalization](https://arxiv.org/abs/2403.19895) | 提出了一个信息论框架用于机器学习中的超出分布泛化，可以自由插值并产生新的泛化界限，同时具有最优输运解释。 |
| [^62] | [Towards a Robust Retrieval-Based Summarization System](https://arxiv.org/abs/2403.19889) | 该论文对大型语言模型在检索增强生成-基础摘要任务中的健壮性进行了调查，并提出了一个创新的评估框架和一个全面的系统来增强模型在特定场景下的健壮性。 |
| [^63] | [MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection](https://arxiv.org/abs/2403.19888) | MambaMixer是一种新的架构，提出了具有数据依赖权重的双重选择机制，称为选择性标记和通道混合器，对长序列建模具有潜在优势。 |
| [^64] | [Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/abs/2403.19887) | Jamba是一个基于混合Transformer-Mamba架构的语言模型，在单个80GB GPU上实现了强大的性能，对标准语言模型基准和长上下文评估具有state-of-the-art的表现。 |
| [^65] | [Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights](https://arxiv.org/abs/2403.19882) | 本文综述了Vision Transformer（ViT）网络中重新设计的注意力机制的技术和见解，旨在提升效率。 |
| [^66] | [Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences](https://arxiv.org/abs/2403.19871) | 通过混合整数优化算法，以保持一致的分析洞见为重点，在重新训练机器学习模型中实现比贪婪训练更强稳定性，同时在模型性能上有小幅、可控的牺牲。 |
| [^67] | [Finding Decision Tree Splits in Streaming and Massively Parallel Models](https://arxiv.org/abs/2403.19867) | 提出了在数据流学习中计算决策树最佳分割点的算法，能够在流式计算和大规模并行模型中高效运行 |
| [^68] | [DeNetDM: Debiasing by Network Depth Modulation](https://arxiv.org/abs/2403.19863) | DeNetDM 是一种基于网络深度调制的新型去偏见方法，通过使用来自专家乘积的训练范式，在创建深浅架构的偏见和去偏见分支后，将知识提炼产生目标去偏见模型，相比当前去偏见技术取得更优异的效果。 |
| [^69] | [A Review of Graph Neural Networks in Epidemic Modeling](https://arxiv.org/abs/2403.19852) | 图神经网络在流行病建模中作为一种新工具备受关注，本文全面回顾了GNN在流行病研究中的应用，并提出了未来发展方向。 |
| [^70] | [Localizing Paragraph Memorization in Language Models](https://arxiv.org/abs/2403.19851) | 论文展示了语言模型中段落记忆的梯度具有可区分的空间模式，通过微调高梯度权重可以取消学习，定位了特别参与段落记忆的低层注意头，并研究了记忆在前缀中的本地化程度。 |
| [^71] | [Biased Over-the-Air Federated Learning under Wireless Heterogeneity](https://arxiv.org/abs/2403.19849) | 研究了在无线异构环境下的有偏Over-the-Air联邦学习，提出了优化偏差和方差之间权衡的方法 |
| [^72] | [Generalized Gradient Descent is a Hypergraph Functor](https://arxiv.org/abs/2403.19845) | 广义梯度下降相对于Cartesian reverse derivative categories (CRDCs)的通用客观函数诱导出一个超图函子，将优化问题映射到动力系统，为分布式优化算法提供了新途径。 |
| [^73] | [Expanding Chemical Representation with k-mers and Fragment-based Fingerprints for Molecular Fingerprinting](https://arxiv.org/abs/2403.19844) | 该方法通过结合亚结构计数、k-mers和Daylight-like指纹，扩展了化学结构的表示，提升了分子指纹的信息内容和区分能力，在化学信息学任务中表现出优越性，为分子相似性分析和化学结构设计提供更加信息丰富的表示。 |
| [^74] | [The New Agronomists: Language Models are Experts in Crop Management](https://arxiv.org/abs/2403.19839) | 本文介绍了一个更先进的智能作物管理系统，利用深度强化学习和语言模型结合决策支持系统来优化作物管理实践。 |
| [^75] | [Concept-based Analysis of Neural Networks via Vision-Language Models](https://arxiv.org/abs/2403.19837) | 本文提出利用视觉语言模型作为透镜，通过其隐含的高层次概念来进行对视觉模型的分析。 |
| [^76] | [Evaluating Explanatory Capabilities of Machine Learning Models in Medical Diagnostics: A Human-in-the-Loop Approach](https://arxiv.org/abs/2403.19820) | 通过人机协作方法，评估医疗诊断中机器学习模型的解释能力，提出了使用医学指南和特征重要性确定胰腺癌治疗关键特征的方法，同时探索了使用相似度衡量的解释结果的方法。 |
| [^77] | [The State of Lithium-Ion Battery Health Prognostics in the CPS Era](https://arxiv.org/abs/2403.19816) | 本论文探讨了在锂离子电池中整合预测和健康管理的方法，着重于剩余有效寿命（RUL）的概念以及向深度学习架构的范式转变。 |
| [^78] | [Feature-Based Echo-State Networks: A Step Towards Interpretability and Minimalism in Reservoir Computer](https://arxiv.org/abs/2403.19806) | 本文提出了一种基于特征的回声状态网络架构，通过使用较小并行储水池和非线性组合，提高了时间序列预测的性能，并展示了在动态系统预测方面的潜力。 |
| [^79] | [Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction](https://arxiv.org/abs/2403.19800) | 提出了一种新颖的Gegenbauer-based graph convolutional (GegenConv)算子，用于提高时变信号重构的准确性 |
| [^80] | [MAPL: Model Agnostic Peer-to-peer Learning](https://arxiv.org/abs/2403.19792) | MAPL提出了一种新颖的方法，即Model Agnostic Peer-to-peer Learning，通过点对点通信在邻近客户端之间同时学习异质个性化模型和协作图，在去中心化环境中实现了有效的协作，并且实验证明了MAPL在性能上具有竞争力。 |
| [^81] | [AlloyBERT: Alloy Property Prediction with Large Language Models](https://arxiv.org/abs/2403.19783) | AlloyBERT是一种基于Transformer编码器的模型，利用文本输入预测合金的弹性模量和屈服强度，结合预训练的RoBERTa编码器和训练有素的分词器，实现了在MPEA数据集上的低均方误差。 |
| [^82] | [Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior](https://arxiv.org/abs/2403.19781) | 通过使用强化学习代理，这项研究展示了在代理模拟中观察到的真实市场风格化事实，同时揭示了RL代理在面对外部市场冲击时的行为反应。 |
| [^83] | [CLoRA: A Contrastive Approach to Compose Multiple LoRA Models](https://arxiv.org/abs/2403.19776) | CLoRA提出了一种对比方法，用于组合多个LoRA模型，解决了将不同概念LoRA模型无缝混合到一个图像中的挑战。 |
| [^84] | [Hierarchical Deep Learning for Intention Estimation of Teleoperation Manipulation in Assembly Tasks](https://arxiv.org/abs/2403.19770) | 该论文提出了一种分层深度学习框架，将多尺度层次信息整合到神经网络中，通过采用分层依赖损失来提高意图估计的准确性，在装配任务中实现了较优的意图识别和预测性能。 |
| [^85] | [Physics-Informed Neural Networks for Satellite State Estimation](https://arxiv.org/abs/2403.19736) | 物理信息神经网络与深度神经网络相结合，为卫星中一些难以建模的异常加速度情况提供了强大的工具 |
| [^86] | [EmoScan: Automatic Screening of Depression Symptoms in Romanized Sinhala Tweets](https://arxiv.org/abs/2403.19728) | 通过分析罗马化僧伽罗语社交媒体数据，提出了一种基于神经网络的框架，利用语言模式、情绪和行为线索可高准确率自动筛查抑郁症症状，显著超越了当前方法，有助于确定需积极干预和支持的个体。 |
| [^87] | [MUGC: Machine Generated versus User Generated Content Detection](https://arxiv.org/abs/2403.19725) | 本研究对八种传统机器学习算法在识别机器生成和人类生成数据方面进行了比较评估，发现这些方法在识别机器生成数据方面具有较高的准确性。另外，机器生成的文本往往更短、词汇变化更少，而人类生成内容则使用了一些特定领域相关关键词被当前的大型语言模型忽略了。 |
| [^88] | [Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data](https://arxiv.org/abs/2403.19721) | 本研究通过利用稳健主成分分析(RPCA)进行噪声降低和异常值排除，以及优化传感器放置(OSP)进行有效数据压缩和存储，提出了一种同时优化计算和内存效率的大数据预测分析方法。 |
| [^89] | [Meta-Learning with Generalized Ridge Regression: High-dimensional Asymptotics, Optimality and Hyper-covariance Estimation](https://arxiv.org/abs/2403.19720) | 本研究在高维多元随机效应线性模型框架下研究了元学习，在使用广义岭回归进行预测时发现，利用随机回归系数的协方差结构可以在新任务上做出更好的预测，并提出了一种最优的权重矩阵选择方法。 |
| [^90] | [A Python library for efficient computation of molecular fingerprints](https://arxiv.org/abs/2403.19718) | 创建了一个Python库，可以高效计算分子指纹，并提供了全面的接口，使用户能够轻松将该库整合到其现有的机器学习工作流程中。 |
| [^91] | [A Picture is Worth 500 Labels: A Case Study of Demographic Disparities in Local Machine Learning Models for Instagram and TikTok](https://arxiv.org/abs/2403.19717) | 本研究分析了Instagram和TikTok两个流行社交媒体应用中视觉模型推断用户信息的能力，揭示了该模型在不同人口统计数据上的性能差异，为确保用户获得公平准确的服务提供了重要参考。 |
| [^92] | [NJUST-KMG at TRAC-2024 Tasks 1 and 2: Offline Harm Potential Identification](https://arxiv.org/abs/2403.19713) | 该研究提出了在TRAC-2024离线危害潜在性识别任务中的方法，利用专家标注的社交媒体评论数据集，成功设计了能够准确评估危害可能性并识别目标的算法，在两个赛道中取得第二名的成绩。 |
| [^93] | [STRUM-LLM: Attributed and Structured Contrastive Summarization](https://arxiv.org/abs/2403.19710) | STRUM-LLM提出了一种生成属性化、结构化和有帮助的对比摘要的方法，识别并突出两个选项之间的关键差异，不需要人工标记的数据或固定属性列表，具有高吞吐量和小体积。 |
| [^94] | [Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models](https://arxiv.org/abs/2403.19709) | 提出了一种分层递归适配器模块，能够在大规模多任务适配场景下降低每个任务的参数开销，同时保持在下游任务中的性能表现，优于先前的适配器方法和完整模型微调基线 |
| [^95] | [AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving](https://arxiv.org/abs/2403.19708) | AttentionStore提出了一种新的注意力机制，通过实现KV缓存的复用，在大型语言模型服务中显著降低了多轮对话中的重复计算成本。 |
| [^96] | [First path component power based NLOS mitigation in UWB positioning system](https://arxiv.org/abs/2403.19706) | 该论文介绍了一种基于第一个路径信号成分功率的NLOS抑制方法，通过纠正NLOS条件下的测量结果并降低其在标签位置估计中的重要性来实现。 |
| [^97] | [Analyzing the Roles of Language and Vision in Learning from Limited Data](https://arxiv.org/abs/2403.19669) | 研究人工智能中的复杂视觉-语言模型，发现即使缺乏视觉输入，利用所有组件的语言模型能够恢复大部分VLM的性能，表明语言通过提供对先前知识和推理的访问来对学习新任务有贡献 |
| [^98] | [Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers](https://arxiv.org/abs/2403.19591) | 本文提出的GQA-LUT算法在变压器中的非线性操作中具有量化感知性，并实现了对INT8-based LUT逼近的应用，节约了大量硬件和功耗 |
| [^99] | [Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers](https://arxiv.org/abs/2403.19060) | 本文提出了一种人类中心的建筑机器人方法，通过强化学习驱动的助手机器人为木工劳动者提供环境上下文协助，推进了机器人在建筑中的应用。 |
| [^100] | [On permutation-invariant neural networks](https://arxiv.org/abs/2403.17410) | 神经网络如Deep Sets和Transformers的出现显著推动了基于集合的数据处理的进展 |
| [^101] | [Language Models are Free Boosters for Biomedical Imaging Tasks](https://arxiv.org/abs/2403.17343) | 本研究揭示了基于残差的大型语言模型在生物医学成像任务中作为编码器的意想不到的有效性，利用冻结的变压器块进行直接处理视觉令牌，从而提高各种生物医学成像应用的性能。 |
| [^102] | [Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability](https://arxiv.org/abs/2403.16970) | 该论文引入了一种新的深度学习框架，用于联合疾病诊断和胸部X光扫描对应视觉显著性图的预测，通过设计新颖的双编码器多任务UNet并利用多尺度特征融合分类器来提高计算辅助诊断的可解释性和质量。 |
| [^103] | [FedAC: A Adaptive Clustered Federated Learning Framework for Heterogeneous Data](https://arxiv.org/abs/2403.16460) | FedAC框架通过解耦神经网络，使用不同聚合方法为每个子模块提供全局知识，引入经济高效的在线模型相似度度量，及集群数量微调模块，显著提高了性能。 |
| [^104] | [Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices](https://arxiv.org/abs/2403.15905) | 提出了面向资源受限设备的低能耗自适应个性化框架目标块微调，根据数据漂移类型微调不同模块以实现最佳性能和降低能源消耗。 |
| [^105] | [Audio-Visual Compound Expression Recognition Method based on Late Modality Fusion and Rule-based Decision](https://arxiv.org/abs/2403.12687) | 该研究提出了一种新颖的音视频复合表达识别方法，通过情绪识别模型融合不同模态的情绪概率，并基于预定义规则进行决策，无需特定训练数据，具有潜力为人类基本和复合情感情境下的音视频数据注释提供智能工具。 |
| [^106] | [Improving Generalization via Meta-Learning on Hard Samples](https://arxiv.org/abs/2403.12236) | 在学习的重加权(LRW)方法中，通过元学习中使用难以分类的实例作为验证集，来改善分类器的泛化性能，并提出了一个高效的算法来训练这个模型。 |
| [^107] | [Dual-Channel Multiplex Graph Neural Networks for Recommendation](https://arxiv.org/abs/2403.11624) | 该研究提出了一种名为双通道多重图神经网络（DCMGNN）的新型推荐框架，能够有效解决现有推荐方法中存在的多通路关系行为模式建模和对目标关系影响忽略的问题。 |
| [^108] | [Structured Evaluation of Synthetic Tabular Data](https://arxiv.org/abs/2403.10424) | 提出了一个具有单一数学目标的评估框架，用于确定合成数据应该从与观测数据相同的分布中提取，并且推理了任何一组指标的完整性，统一了现有的指标，并鼓励新的模型无关基线和指标。 |
| [^109] | [Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems](https://arxiv.org/abs/2403.08585) | 通过预条件化，研究了SGD在最小二乘问题中的泛化性能，对比了预条件化SGD和（标准和预条件化）岭回归，为改善SGD理解和应用提供了关键贡献。 |
| [^110] | [Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models](https://arxiv.org/abs/2402.10189) | 本文研究了大型语言模型（LLM）上下文学习中的不确定性，并提出了一种新的方法来量化这种不确定性，包括演示产生的不确定性和模型配置的模糊性。 |
| [^111] | [Lens: A Foundation Model for Network Traffic](https://arxiv.org/abs/2402.03646) | "Lens"是一个基于T5架构的基础网络流量模型，通过学习大规模无标签数据的预训练表示，能够在流量理解和生成任务中取得精确的预测和生成。 |
| [^112] | [Variational Flow Models: Flowing in Your Style](https://arxiv.org/abs/2402.02977) | 我们引入了一种变分流模型的方法，并提出了一种系统的无需训练的转换方法，使得快速采样成为可能，同时保持了采样的准确性和效率。 |
| [^113] | [CroissantLLM: A Truly Bilingual French-English Language Model](https://arxiv.org/abs/2402.00786) | CroissantLLM是一个1.3B的双语语言模型，通过使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集进行训练，实现了高性能和开源。模型还发布了训练数据集和多个检查点，以及一个法语基准测试 FrenchBench。 |
| [^114] | [DXAI: Explaining Classification by Image Decomposition](https://arxiv.org/abs/2401.00320) | 提出一种新的可解释AI方法DXAI，通过将图像分解为类别不可知和类别明显部分来解释和可视化神经网络分类，为解释分类提供了一种根本不同的方式 |
| [^115] | [New Classes of the Greedy-Applicable Arm Feature Distributions in the Sparse Linear Bandit Problem](https://arxiv.org/abs/2312.12400) | 本文展示了贪婪算法适用于更广泛范围的臂特征分布，提出了新的适用类别和混合分布概念。 |
| [^116] | [GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning](https://arxiv.org/abs/2312.11461) | 该论文提出了一种基于高斯喷洒的方法，用于从文本描述中生成逼真且可动态的化身，解决了基于网格或NeRF表示的灵活性和效率方面的局限性，通过原始的3D高斯表示和神经隐式场来稳定和改进学习过程。 |
| [^117] | [Conformer-Based Speech Recognition On Extreme Edge-Computing Devices](https://arxiv.org/abs/2312.10359) | 提出了在资源受限设备上实现基于Conformer的端到端流式ASR系统的一系列模型架构适配、神经网络图变换和数值优化方法，实现了超过5.26倍实时速度的语音识别，同时最大限度减少能耗并实现了最先进的准确性。 |
| [^118] | [LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos](https://arxiv.org/abs/2312.05269) | 终身记忆是一个新的框架，通过自然语言问答和检索方式访问长篇自我中心视频，利用零-shot能力进行推理，使用置信度和解释模块产生自信、高质量和可解释的答案，在EgoSchema问题回答基准上达到最先进性能，在Ego4D的自然语言查询挑战中具有很强的竞争力 |
| [^119] | [Rapid Motor Adaptation for Robotic Manipulator Arms](https://arxiv.org/abs/2312.04670) | 快速电机适应性（RMA）为机器人操作技能的泛化提出了解决方案，利用深度感知开发了针对各种操纵任务的代理。 |
| [^120] | [Toward a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using Reinforcement and Imitation Learning](https://arxiv.org/abs/2311.17693) | 利用模拟图像引导的以外科医生为中心的自主机器人学徒系统，通过强化学习和模仿学习代理在眼科白内障手术中适应外科医生的技能水平和外科手术偏好。 |
| [^121] | [Compositional Chain-of-Thought Prompting for Large Multimodal Models](https://arxiv.org/abs/2311.17076) | 提出了一种新颖的零样式思维提示（CCoT），以克服大型多模态模型难以捕捉到组合视觉推理方面的细节的问题。 |
| [^122] | [Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values](https://arxiv.org/abs/2311.04855) | 本文提出了两种算法，Shift-NMF和Nearly-NMF，可以处理带有负值的嘈杂数据，在不引入正的偏移量的情况下正确恢复非负信号。 |
| [^123] | [Hybrid quantum image classification and federated learning for hepatic steatosis diagnosis](https://arxiv.org/abs/2311.02402) | 该研究旨在开发一种混合量子图像分类和联邦学习算法，以提高肝脂肪变性的诊断准确性，同时解决因数据隐私问题带来的挑战。 |
| [^124] | [Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and Binding Site Design](https://arxiv.org/abs/2310.05764) | 该研究开发了一种谐波自调流匹配方法，在多配体对接和结合位点设计中表现出比现有方法更好的生成过程和设计效果。 |
| [^125] | [DFWLayer: Differentiable Frank-Wolfe Optimization Layer](https://arxiv.org/abs/2308.10806) | 本文提出了一种名为Differentiable Frank-Wolfe Layer（DFWLayer）的可微层，通过推出Frank-Wolfe方法，有效处理具有范数约束的大规模凸优化问题，并在解和梯度准确性方面表现竞争性。 |
| [^126] | [Data-efficient, Explainable and Safe Box Manipulation: Illustrating the Advantages of Physical Priors in Model-Predictive Control](https://arxiv.org/abs/2303.01563) | 本论文旨在通过一个真实机器人系统案例研究，展示在模型预测控制中利用环境动态先验知识可以提高可解释性、安全性和数据效率的优点 |
| [^127] | [Attending to Graph Transformers](https://arxiv.org/abs/2302.04181) | 提出了一种图转换器架构的分类法，概述了其理论性质，调查了结构和位置编码，并探讨了对重要图类的扩展，如3D分子图。 |
| [^128] | [Complete Neural Networks for Complete Euclidean Graphs](https://arxiv.org/abs/2301.13821) | 神经网络模型可以通过应用3-WL图同构测试于点云的Gram矩阵，或者应用欧几里得2-WL测试，在多项式复杂度内实现对完全欧几里得图的完全确定性 |
| [^129] | [Restricting to the chip architecture maintains the quantum neural network accuracy](https://arxiv.org/abs/2212.14426) | 该研究旨在探讨量子神经网络在量子芯片架构上表现出的准确性，并发现成本函数往往会收敛到一个平均值。 |
| [^130] | [Strong Transferable Adversarial Attacks via Ensembled Asymptotically Normal Distribution Learning](https://arxiv.org/abs/2209.11964) | 本文提出了一种名为多渐进正态分布攻击（MultiANDA）的方法，通过学习的分布明确刻画对抗性扰动，利用随机梯度上升的渐进正态性质来近似扰动的后验分布，并采用深度集成策略作为贝叶斯模型的有效代理。 |
| [^131] | [Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach](https://arxiv.org/abs/2206.10716) | 通过密度估计技术直接学习任务分布，从而对元强化学习中所需训练任务数提出了新的界限分析方法 |
| [^132] | [QAGCN: Answering Multi-Relation Questions via Single-Step Implicit Reasoning over Knowledge Graphs](https://arxiv.org/abs/2206.01818) | 本文提出了 QAGCN 方法，通过对问题进行感知来实现单步隐式推理，从而回答多关系问题，相比于显式多步推理方法，该方法更简单、高效且易于采用。 |
| [^133] | [Generalization bounds for learning under graph-dependence: A survey](https://arxiv.org/abs/2203.13534) | 通过收集各种图依赖性的集中界限，该研究推导出了用于基于图依赖性数据学习的Rademacher复杂度和稳定性的学习泛化界限。 |
| [^134] | [Doubly Optimal No-Regret Online Learning in Strongly Monotone Games with Bandit Feedback](https://arxiv.org/abs/2112.02856) | 该论文提出了一种在强单调博弈中具有摇臂反馈的双重最优无悔在线学习方法，并展示了在特定条件下的最优后悔和联合动作收敛到纳什均衡点的速率的结果。 |
| [^135] | [Two-sample Test using Projected Wasserstein Distance](https://arxiv.org/abs/2010.11970) | 提出了一种使用投影Wasserstein距离进行双样本检验的方法，可以避免高维度情况下的测试能力减弱问题，并通过最优投影和低维线性映射最大化Wasserstein距离来实现。 |
| [^136] | [Multi-Agent Diagnostics for Robustness via Illuminated Diversity.](http://arxiv.org/abs/2401.13460) | MADRID是一种新方法，通过生成多样化的对抗场景来揭示预训练多Agent策略的战略漏洞，并通过遗憾值衡量漏洞的程度。 |
| [^137] | [Data-Efficient Multimodal Fusion on a Single GPU.](http://arxiv.org/abs/2312.10144) | 本论文提出了一种在单一GPU上进行数据高效多模态融合的方法，通过使用预训练的单模态编码器的潜在空间，我们在多模态对齐中取得了有竞争力的性能，且计算和数据量减少了数个数量级。 |
| [^138] | [LipSim: A Provably Robust Perceptual Similarity Metric.](http://arxiv.org/abs/2310.18274) | LipSim是一个可证明鲁棒的知觉相似度度量方法，通过利用1-Lipschitz神经网络作为骨干，提供了围绕度量方法的防护区域。 |
| [^139] | [Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and Exp-Concave Games with Gradient Feedback.](http://arxiv.org/abs/2310.14085) | 本文提出了一个自适应的OGD算法\textsf{AdaOGD}，在强凸性下实现了$ O(\log^2(T)) $的后悔，并且在强单调博弈中使得联合行动最后一次收敛到唯一的纳什均衡。 |
| [^140] | [Transparency challenges in policy evaluation with causal machine learning -- improving usability and accountability.](http://arxiv.org/abs/2310.13240) | 透明度问题是因果机器学习在政策评估中的挑战，因为黑盒子模型难以理解和问责。本文提出了通过可解释的AI工具和简化模型来解决这些问题的方法。 |
| [^141] | [Conversational Financial Information Retrieval Model (ConFIRM).](http://arxiv.org/abs/2310.13001) | ConFIRM是一种会话式金融信息检索模型，通过合成金融领域特定问答对和评估参数微调方法，实现了超过90%的准确性，为金融对话系统提供了数据高效的解决方案。 |
| [^142] | [Gromov-Wassertein-like Distances in the Gaussian Mixture Models Space.](http://arxiv.org/abs/2310.11256) | 本文介绍了两种在高斯混合模型空间中的Gromov-Wasserstein类型距离，分别用于评估分布之间的距离和推导最优的点分配方案。 |
| [^143] | [GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers.](http://arxiv.org/abs/2310.10375) | 提出了一种面向几何的注意力机制（GTA），用于将几何结构编码为相对变换，从而改进了多视图Transformer的学习效率和性能。 |
| [^144] | [TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design.](http://arxiv.org/abs/2310.03223) | 该论文提出了一种名为TacoGFN的目标条件GFlowNet模型，用于自动化生成符合特定蛋白质口袋目标的类药物化合物。该模型通过强化学习框架，鼓励生成具有期望属性的分子，并利用转换器和对接神经网络进行高效的分子空间探索和对接得分预测，以实现较高的结合改善效果。 |
| [^145] | [Multi-Resolution Active Learning of Fourier Neural Operators.](http://arxiv.org/abs/2309.16971) | 提出了一种多分辨率主动学习的傅里叶神经算子（MRA-FNO），通过动态选择输入函数和分辨率来降低数据成本并优化学习效率。 |
| [^146] | [RLSynC: Offline-Online Reinforcement Learning for Synthon Completion.](http://arxiv.org/abs/2309.02671) | RLSynC是一种离线-在线强化学习方法，用于半模板化逆向合成中的合成物补全。它使用多个代理同时完成合成物的补全，并通过正向合成模型评估反应物的合成能力来指导行动搜索。 |
| [^147] | [A multiobjective continuation method to compute the regularization path of deep neural networks.](http://arxiv.org/abs/2308.12044) | 本文提出了一种多目标延续方法，用于计算深度神经网络的正则化路径，以解决DNNs中稀疏性和数值效率之间的冲突。 |
| [^148] | [Gradient strikes back: How filtering out high frequencies improves explanations.](http://arxiv.org/abs/2307.09591) | 本研究发现，基于预测的属性方法与基于梯度的方法产生的属性图具有不同的高频内容，滤除高频率可以提高解释性。 |
| [^149] | [Towards Sustainable Deep Learning for Multi-Label Classification on NILM.](http://arxiv.org/abs/2307.09244) | 本研究提出了一种面向NILM的新型深度学习模型，通过改进计算和能源效率，实现了对NILM的多标签分类的增强。同时，还提出了一种测试方法，可以比较不同模型在虚拟数据集上的性能。 |
| [^150] | [Temporal Difference Learning for High-Dimensional PIDEs with Jumps.](http://arxiv.org/abs/2307.02766) | 本文提出了一种用于解决高维度跳跃偏微分方程的时差学习深度学习框架，该方法具有低计算成本和稳健性的优势，可以有效地处理具有不同形式和强度跳跃的问题。 |
| [^151] | [Are We There Yet? Product Quantization and its Hardware Acceleration.](http://arxiv.org/abs/2305.18334) | 本文研究了产品量化（PQ）在深度神经网络中替代传统乘加（MAC）运算的效果。作者发现FLOP和参数数量等指标可能具有误导性，并设计了第一个PQ定制硬件加速器评估其性能和效率。 |
| [^152] | [MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation.](http://arxiv.org/abs/2305.08396) | MaxViT-UNet是基于编码器-解码器混合视觉变压器的医学图像分割模型，多轴自我关注机制在每个解码器阶段有助于更有效地区分对象和背景区域。 |
| [^153] | [SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks.](http://arxiv.org/abs/2305.03039) | 通过分析159个交互式可视化工具及其用户反馈，本论文提出了在计算笔记本中设计可视分析工具的独特机会和考虑因素。 |
| [^154] | [mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality.](http://arxiv.org/abs/2304.14178) | 本文介绍了一种名为mPLUG-Owl的训练范式，它通过模块化学习基础LLM、视觉知识模块和视觉抽象器模块，赋予LLMs多模态的能力。实验结果表明，mPLUG-Owl在图像字幕和视觉问答任务中表现优于基线模型，并在某些情况下达到了最先进的性能水平。 |
| [^155] | [Sample-Efficient and Surrogate-Based Design Optimization of Underwater Vehicle Hulls.](http://arxiv.org/abs/2304.12420) | 该论文使用了高效的样本集优化和基于代理的方法来设计水下航行器船体，其中代理模型显著提高了计算效率，使优化更加快速准确。 |
| [^156] | [TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed Training.](http://arxiv.org/abs/2304.05301) | TACOS 是一个能够自动合成任意输入网络拓扑的面向拓扑结构的集合合成器。与基准算法相比，TACOS 合成的 All-Reduce 算法速度提高了 3.73 倍，为 512-NPU 系统合成集体算法只需 6.1 分钟。 |
| [^157] | [Making Batch Normalization Great in Federated Deep Learning.](http://arxiv.org/abs/2303.06530) | 本文研究了在联邦学习中使用批标准化和群组归一化的效果，发现在适当的处理下，批标准化可以在广泛的联邦学习设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。 |
| [^158] | [Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review.](http://arxiv.org/abs/2303.06471) | 本文综述了深度神经网络在多模态数据整合方面的应用，以提高癌症诊断和治疗的准确性和可靠性。 |
| [^159] | [Training neural networks with structured noise improves classification and generalization.](http://arxiv.org/abs/2302.13417) | 通过在训练数据中添加结构化噪声，可以显著提高神经网络的分类和泛化能力，并提出了一种采样策略来优于传统的训练和赫布生规则方法。 |
| [^160] | [Federated contrastive learning models for prostate cancer diagnosis and Gleason grading.](http://arxiv.org/abs/2302.06089) | 该研究提出了一个面向大规模病理图像和异质性挑战的联邦对比学习模型（FCL），通过最大化本地客户端和服务器模型间的注意力一致性来增强模型的泛化能力。 在前列腺癌诊断和格里森分级任务中，FCL表现出优异的性能。 |
| [^161] | [CPPF++: Uncertainty-Aware Sim2Real Object Pose Estimation by Vote Aggregation.](http://arxiv.org/abs/2211.13398) | 本文针对Sim2Real物体姿态估计问题，提出了一种新颖的CPPF++方法，通过投票聚合和概率建模来考虑投票不确定性，并通过迭代噪声过滤来提高姿态估计的准确性。 |
| [^162] | [Positive Unlabeled Contrastive Learning.](http://arxiv.org/abs/2206.01206) | 我们提出了一种正样本未标记对比学习的新方法，通过扩展对比损失和使用PU特定聚类方案，该方法在PU任务中学习到了优秀的表示，并在多个标准数据集上明显优于现有方法。 |

# 详细

[^1]: 不可解问题检测：评估视觉语言模型的可信度

    Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models

    [https://arxiv.org/abs/2403.20331](https://arxiv.org/abs/2403.20331)

    本文提出了一个新颖且重要的挑战，即Unsolvable Problem Detection（UPD），用于评估视觉语言模型在视觉问答任务中能否在面对不可解问题时保持答案的能力，并通过广泛实验发现大多数模型存在改进的空间。

    

    本文介绍了一个新颖而重要的挑战，即Unsolvable Problem Detection（UPD），用于评估视觉语言模型（VLMs）在视觉问答（VQA）任务中面对不可解问题时保持答案的能力。UPD包括三个不同的设置：缺失答案检测（AAD）、不兼容答案集检测（IASD）和不兼容视觉问题检测（IVQD）。通过广泛的实验深入研究UPD问题表明，大多数VLMs，包括GPT-4V和LLaVA-Next-34B，在各种程度上都很难应对我们的基准测试，突显了改进的重要空间。为了解决UPD，我们探索了无需训练和基于训练的解决方案，提供了对其有效性和局限性的新见解。我们希望我们的见解，以及在提议的UPD设置内的未来努力，将增强对VLMs的更广泛理解和发展。

    arXiv:2403.20331v1 Announce Type: cross  Abstract: This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of
    
[^2]: ReALM: 参考解析作为语言建模

    ReALM: Reference Resolution As Language Modeling

    [https://arxiv.org/abs/2403.20329](https://arxiv.org/abs/2403.20329)

    本论文展示了如何利用LLMs创建一个极其有效的系统来解决各种类型的引用问题，通过将参考解析转化为语言建模问题，尽管涉及到屏幕上的实体等不易约简为纯文本形式的实体。

    

    参考解析是一个重要的问题，对于理解和成功处理各种上下文至关重要。 这种上下文既包括先前的对话，也包括与非对话实体相关的上下文，例如用户屏幕上的实体或后台运行的实体。 尽管已经证明了LLMs在各种任务中非常强大，但它们在参考解析中的运用，特别是对于非对话实体，仍未充分利用。 本文展示了LLMs如何被用来创建一个极其有效的系统来解决各种类型的引用问题，通过展示如何将参考解析转化为语言建模问题，尽管其中涉及屏幕上的这种实体等传统上不易约简为纯文本形式的实体。 我们展示了在不同类型的参考解析中相对于已有类似功能的系统的显着改进。

    arXiv:2403.20329v1 Announce Type: cross  Abstract: Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user's screen or those running in the background. While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of re
    
[^3]: 从示范学习视觉四足走行操纵

    Learning Visual Quadrupedal Loco-Manipulation from Demonstrations

    [https://arxiv.org/abs/2403.20328](https://arxiv.org/abs/2403.20328)

    通过将走行操纵过程分解为低层强化学习控制器和高层行为克隆规划器，使四足机器人能够执行真实世界的操纵任务。

    

    四足机器人逐渐被整合进人类环境。尽管四足机器人的行走能力不断增强，但它们在现实场景中与物体的互动仍然有限。为了让四足机器人能够执行真实世界的操纵任务，我们将走行操纵过程分解为基于强化学习（RL）的低层控制器和基于行为克隆（BC）的高层规划器。通过参数化操纵轨迹，我们同步上层和下层的努力，从而充分利用RL和BC的优势。我们的方法通过模拟和现实世界实验得到验证。

    arXiv:2403.20328v1 Announce Type: cross  Abstract: Quadruped robots are progressively being integrated into human environments. Despite the growing locomotion capabilities of quadrupedal robots, their interaction with objects in realistic scenes is still limited. While additional robotic arms on quadrupedal robots enable manipulating objects, they are sometimes redundant given that a quadruped robot is essentially a mobile unit equipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence, we aim to empower a quadruped robot to execute real-world manipulation tasks using only its legs. We decompose the loco-manipulation process into a low-level reinforcement learning (RL)-based controller and a high-level Behavior Cloning (BC)-based planner. By parameterizing the manipulation trajectory, we synchronize the efforts of the upper and lower layers, thereby leveraging the advantages of both RL and BC. Our approach is validated through simulations and real-world experiments, d
    
[^4]: 使用变压器从单脉冲电刺激响应中定位癫痫发作起始区

    Localising the Seizure Onset Zone from Single-Pulse Electrical Stimulation Responses with a Transformer

    [https://arxiv.org/abs/2403.20324](https://arxiv.org/abs/2403.20324)

    本研究通过引入Transformer模型结合跨通道注意力，推动了使用深度学习进行单脉冲电刺激响应的SOZ本地化，在评估中展示了模型对未见患者和电极放置的泛化能力

    

    癫痫是最常见的神经疾病之一，许多患者在药物无法控制癫痫发作时需要手术干预。为了取得有效的手术结果，准确定位癫痫发作起始区 - 通常近似为癫痫发作起始区 (SOZ) - 至关重要但仍然具有挑战性。通过电刺激进行主动探测已经成为识别癫痫发作区域的标准临床实践。本文推动了深度学习在使用单脉冲电刺激 (SPES) 响应进行 SOZ 定位的应用。我们通过引入包含跨通道注意力的Transformer模型来实现这一点。我们在保留的患者测试集上评估这些模型，以评估它们对未见患者和电极放置的泛化能力。

    arXiv:2403.20324v1 Announce Type: new  Abstract: Epilepsy is one of the most common neurological disorders, and many patients require surgical intervention when medication fails to control seizures. For effective surgical outcomes, precise localisation of the epileptogenic focus - often approximated through the Seizure Onset Zone (SOZ) - is critical yet remains a challenge. Active probing through electrical stimulation is already standard clinical practice for identifying epileptogenic areas. This paper advances the application of deep learning for SOZ localisation using Single Pulse Electrical Stimulation (SPES) responses. We achieve this by introducing Transformer models that incorporate cross-channel attention. We evaluate these models on held-out patient test sets to assess their generalisability to unseen patients and electrode placements.   Our study makes three key contributions: Firstly, we implement an existing deep learning model to compare two SPES analysis paradigms - namel
    
[^5]: MTLoRA: 一种用于高效多任务学习的低秩适应方法

    MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning

    [https://arxiv.org/abs/2403.20320](https://arxiv.org/abs/2403.20320)

    MTLoRA提出了一种新颖的多任务学习模型参数高效训练框架，通过任务不可知和任务特定的低秩适应模块有效地实现了参数空间的分离，使得模型能够灵活处理任务专业化和在多任务学习环境中的相互作用

    

    适应在大规模数据集上预训练的模型到各种下游任务是深度学习中常见的策略。因此，参数高效的微调方法已经成为将预训练模型适应到不同任务的一种有前景的方式，同时仅训练少量参数。虽然大多数这些方法是为单任务适应而设计的，但在多任务学习（MTL）架构中进行参数高效训练仍未被探索。在本文中，我们介绍了MTLoRA，一种用于多任务学习模型参数高效训练的新框架。MTLoRA采用任务不可知和任务特定的低秩适应模块，有效地分开了MTL微调中的参数空间，从而使模型能够熟练处理MTL上下文中的任务专门化和交互。我们将MTLoRA应用于基于分层变压器的MTL架构，将它们调整到多个下游密集预测

    arXiv:2403.20320v1 Announce Type: cross  Abstract: Adapting models pre-trained on large-scale datasets to a variety of downstream tasks is a common strategy in deep learning. Consequently, parameter-efficient fine-tuning methods have emerged as a promising way to adapt pre-trained models to different tasks while training only a minimal number of parameters. While most of these methods are designed for single-task adaptation, parameter-efficient training in Multi-Task Learning (MTL) architectures is still unexplored. In this paper, we introduce MTLoRA, a novel framework for parameter-efficient training of MTL models. MTLoRA employs Task-Agnostic and Task-Specific Low-Rank Adaptation modules, which effectively disentangle the parameter space in MTL fine-tuning, thereby enabling the model to adeptly handle both task specialization and interaction within MTL contexts. We applied MTLoRA to hierarchical-transformer-based MTL architectures, adapting them to multiple downstream dense predictio
    
[^6]: 基于双曲嵌入和层次感知域解耦的基于评论的跨领域推荐

    Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement

    [https://arxiv.org/abs/2403.20298](https://arxiv.org/abs/2403.20298)

    本文基于评论文本提出了一种双曲CDR方法，以应对推荐系统中的数据稀疏性挑战，避免传统基于距离的领域对齐技术可能引发的问题。

    

    数据稀疏性问题对推荐系统构成了重要挑战。本文提出了一种基于评论文本的算法，以应对这一问题。此外，跨领域推荐（CDR）吸引了广泛关注，它捕捉可在领域间共享的知识，并将其从更丰富的领域（源领域）转移到更稀疏的领域（目标领域）。然而，现有大多数方法假设欧几里德嵌入空间，在准确表示更丰富的文本信息和处理用户和物品之间的复杂交互方面遇到困难。本文倡导一种基于评论文本的双曲CDR方法来建模用户-物品关系。首先强调了传统的基于距离的领域对齐技术可能会导致问题，因为在双曲几何中对小修改造成的干扰会被放大，最终导致层次性崩溃。

    arXiv:2403.20298v1 Announce Type: cross  Abstract: The issue of data sparsity poses a significant challenge to recommender systems. In response to this, algorithms that leverage side information such as review texts have been proposed. Furthermore, Cross-Domain Recommendation (CDR), which captures domain-shareable knowledge and transfers it from a richer domain (source) to a sparser one (target), has received notable attention. Nevertheless, the majority of existing methodologies assume a Euclidean embedding space, encountering difficulties in accurately representing richer text information and managing complex interactions between users and items. This paper advocates a hyperbolic CDR approach based on review texts for modeling user-item relationships. We first emphasize that conventional distance-based domain alignment techniques may cause problems because small modifications in hyperbolic geometry result in magnified perturbations, ultimately leading to the collapse of hierarchical 
    
[^7]: 基准对照图像生成

    Benchmarking Counterfactual Image Generation

    [https://arxiv.org/abs/2403.20287](https://arxiv.org/abs/2403.20287)

    提出了一个针对对照图像生成方法的基准测试框架，包含评估对照多个方面的度量标准以及评估三种不同类型的条件图像生成模型性能。

    

    对照图像生成在理解变量因果关系方面具有关键作用，在解释性和生成无偏合成数据方面有应用。然而，评估图像生成本身就是一个长期存在的挑战。对于评估对照生成的需求进一步加剧了这一挑战，因为根据定义，对照情景是没有可观测基准事实的假设情况。本文提出了一个旨在对照图像生成方法进行基准测试的新颖综合框架。我们结合了侧重于评估对照的不同方面的度量标准，例如组成、有效性、干预的最小性和图像逼真度。我们评估了基于结构因果模型范式的三种不同条件图像生成模型类型的性能。我们的工作还配备了一个用户友好的Python软件包，可以进一步评估。

    arXiv:2403.20287v1 Announce Type: cross  Abstract: Counterfactual image generation is pivotal for understanding the causal relations of variables, with applications in interpretability and generation of unbiased synthetic data. However, evaluating image generation is a long-standing challenge in itself. The need to evaluate counterfactual generation compounds on this challenge, precisely because counterfactuals, by definition, are hypothetical scenarios without observable ground truths. In this paper, we present a novel comprehensive framework aimed at benchmarking counterfactual image generation methods. We incorporate metrics that focus on evaluating diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism. We assess the performance of three distinct conditional image generation model types, based on the Structural Causal Model paradigm. Our work is accompanied by a user-friendly Python package which allows to further eval
    
[^8]: LayerNorm：参数高效微调中的关键组件

    LayerNorm: A key component in parameter-efficient fine-tuning

    [https://arxiv.org/abs/2403.20284](https://arxiv.org/abs/2403.20284)

    LayerNorm在参数高效微调中扮演关键角色，微调output LayerNorm而保持其他部分不变足以在多个GLUE任务中取得竞争力

    

    细调预训练模型，如双向编码器表示来自转换器（BERT），已被证明是解决许多自然语言处理（NLP）任务的有效方法。然而，由于许多最先进的NLP模型（包括BERT）中的参数数量庞大，微调过程耗费了大量计算资源。解决这一问题的一种吸引人方法是参数高效微调，即仅修改模型的最小部分，同时保持其余部分不变。然而，目前仍不清楚BERT模型的哪个部分对微调至关重要。在本文中，我们首先分析BERT模型中的不同组件，以查明在微调后哪些组件发生了最显著的变化。我们发现，在针对不同General Language Understanding Evaluation（GLUE）任务进行微调时，输出的LayerNorm发生的变化比其他组件都要大。然后我们展示仅微调output LayerNorm而保持其他部分不变就足以在多个GLUE任务中取得竞争力。

    arXiv:2403.20284v1 Announce Type: new  Abstract: Fine-tuning a pre-trained model, such as Bidirectional Encoder Representations from Transformers (BERT), has been proven to be an effective method for solving many natural language processing (NLP) tasks. However, due to the large number of parameters in many state-of-the-art NLP models, including BERT, the process of fine-tuning is computationally expensive. One attractive solution to this issue is parameter-efficient fine-tuning, which involves modifying only a minimal segment of the model while keeping the remainder unchanged. Yet, it remains unclear which segment of the BERT model is crucial for fine-tuning. In this paper, we first analyze different components in the BERT model to pinpoint which one undergoes the most significant changes after fine-tuning. We find that output LayerNorm changes more than any other components when fine-tuned for different General Language Understanding Evaluation (GLUE) tasks. Then we show that only fi
    
[^9]: 带有模态通道注意力的稀疏多模态融合

    Sparse multimodal fusion with modal channel attention

    [https://arxiv.org/abs/2403.20280](https://arxiv.org/abs/2403.20280)

    研究了蒙特卡罗多模态变换器架构在模态样本稀疏对齐时学习稳健嵌入空间的能力，并提出了模态通道注意力（MCA）机制，可以改善生成的嵌入空间质量和下游任务性能。

    

    通过测量生成的嵌入空间质量作为模态稀疏度函数的能力，研究了蒙特卡罗多模态变换器架构在模态样本稀疏对齐时学习稳健嵌入空间的能力。提出了一种扩展的蒙特卡罗多模态变压器模型，该模型在多头注意机制中引入了模态不完全通道，称为模态通道注意力（MCA）。使用了包含4种模态的两个数据集，CMU-MOSEI用于多模态情感识别，TCGA用于多组学。模型显示在大多数样本中只用了四种模态中的两种就学习出统一且对齐的嵌入空间。发现，即使没有模态稀疏，所提出的MCA机制也能提高生成的嵌入空间质量，召回指标，并提高下游任务的性能。

    arXiv:2403.20280v1 Announce Type: cross  Abstract: The ability of masked multimodal transformer architectures to learn a robust embedding space when modality samples are sparsely aligned is studied by measuring the quality of generated embedding spaces as a function of modal sparsity. An extension to the masked multimodal transformer model is proposed which incorporates modal-incomplete channels in the multihead attention mechanism called modal channel attention (MCA). Two datasets with 4 modalities are used, CMU-MOSEI for multimodal sentiment recognition and TCGA for multiomics. Models are shown to learn uniform and aligned embedding spaces with only two out of four modalities in most samples. It was found that, even with no modal sparsity, the proposed MCA mechanism improves the quality of generated embedding spaces, recall metrics, and subsequent performance on downstream tasks.
    
[^10]: Latxa: 一种用于巴斯克语的开放语言模型和评估套件

    Latxa: An Open Language Model and Evaluation Suite for Basque

    [https://arxiv.org/abs/2403.20266](https://arxiv.org/abs/2403.20266)

    Latxa是一种用于巴斯克语的大型语言模型系列，在语言熟练度和理解能力方面表现出色，优于所有以前的开放模型，并具有多个评估数据集，填补了巴斯克语高质量基准的不足。

    

    我们介绍了Latxa，这是一个基于Llama 2的大型巴斯克语言模型系列，参数范围从7到700亿。Latxa基于新的巴斯克语语料库预训练，包括430万个文档和42亿个标记。针对巴斯克语高质量基准的稀缺性，我们进一步提出了4个多项选择评估数据集：EusProficiency，包括来自官方语言能力考试的5169个问题；EusReading，包括352个阅读理解问题；EusTrivia，包括来自5个知识领域的1715个琐事问题；以及EusExams，包括来自公共考试的16774个问题。在我们的广泛评估中，Latxa在与我们比较的所有先前开放模型中表现出色。此外，尽管在阅读理解和知识密集型任务方面落后，但在语言熟练度和理解能力方面，它与GPT-4 Turbo具有竞争力。Latxa模型系列，以及

    arXiv:2403.20266v1 Announce Type: cross  Abstract: We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well
    
[^11]: ELITR-Bench: 面向长上下文语言模型的会议助理基准

    ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models

    [https://arxiv.org/abs/2403.20262](https://arxiv.org/abs/2403.20262)

    该论文提出了一个新的基准 ELITR-Bench，专注于长上下文语言模型的实际会议助理场景，通过在现有 ELITR 语料库的转录中添加手工制作的问题和真实答案，揭示了开源模型和专有模型之间的差距。

    

    最近，对大型语言模型（LLMs）的研究越来越受到关注，主要致力于扩展模型的上下文大小，以更好地捕捉长文档内部的依赖关系。尽管已经提出了用于评估长距离能力的基准，但现有的努力主要考虑的是不一定与现实应用相关的通用任务。相反，我们的工作提出了一个针对实际会议助理场景的长上下文LLMs的新基准。在这种情景下，长上下文由自动语音识别获得的转录组成，由于这些数据的固有嘈杂性和口语特性，这为LLMs提出了独特的挑战。我们的基准，名为ELITR-Bench，通过271个手工制作的问题及其真实答案来增强现有的ELITR语料库的转录。我们在ELITR-Bench上对最新的长上下文LLMs进行的实验凸显了开源模型和专有模型之间的差距。

    arXiv:2403.20262v1 Announce Type: cross  Abstract: Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities, existing efforts primarily considered generic tasks that are not necessarily aligned with real-world applications. In contrast, our work proposes a new benchmark for long-context LLMs focused on a practical meeting assistant scenario. In this scenario, the long contexts consist of transcripts obtained by automatic speech recognition, presenting unique challenges for LLMs due to the inherent noisiness and oral nature of such data. Our benchmark, named ELITR-Bench, augments the existing ELITR corpus' transcripts with 271 manually crafted questions and their ground-truth answers. Our experiments with recent long-context LLMs on ELITR-Bench highlight a gap between open-source and proprietary models, e
    
[^12]: FABind+: 通过改进口袋预测和姿态生成增强分子对接

    FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation

    [https://arxiv.org/abs/2403.20261](https://arxiv.org/abs/2403.20261)

    FABind+通过改进口袋预测和姿态生成，提升分子对接表现

    

    分子对接是药物发现中至关重要的过程。传统技术依赖于受物理原理支配的广泛采样和模拟，但这些方法往往速度慢且昂贵。基于深度学习的方法的出现显示出显著的前景，提供了精确性和效率的增长。建立在FABind的基础工作之上，这是一个专注于速度和准确性的模型，我们提出了FABind+，这是一个大大提升其前身性能的增强版。我们确定口袋预测是分子对接中的一个关键瓶颈，并提出了一种显著改进口袋预测的新方法，从而简化了对接过程。此外，我们对对接模块进行了修改，以增强其姿态生成能力。为了缩小与传统采样/生成方法之间的差距，我们结合了一个简单而有效的s

    arXiv:2403.20261v1 Announce Type: cross  Abstract: Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective s
    
[^13]: MedCLIP-SAM：将文本和图像进行桥接，实现通用医学图像分割

    MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation

    [https://arxiv.org/abs/2403.20253](https://arxiv.org/abs/2403.20253)

    提出了MedCLIP-SAM框架，结合了CLIP和SAM模型，实现使用少量标记数据的临床图像分割

    

    医学图像中解剖结构和病变的分割在现代临床诊断、疾病研究和治疗规划中至关重要。本文提出了一个新颖的框架，称为MedCLIP-SAM，结合了CLIP和SAM模型，以生成使用少量标记数据的临床扫描的分割。

    arXiv:2403.20253v1 Announce Type: cross  Abstract: Medical image segmentation of anatomical structures and pathology is crucial in modern clinical diagnosis, disease study, and treatment planning. To date, great progress has been made in deep learning-based segmentation techniques, but most methods still lack data efficiency, generalizability, and interactability. Consequently, the development of new, precise segmentation methods that demand fewer labeled datasets is of utmost importance in medical image analysis. Recently, the emergence of foundation models, such as CLIP and Segment-Anything-Model (SAM), with comprehensive cross-domain representation opened the door for interactive and universal image segmentation. However, exploration of these models for data-efficient medical image segmentation is still limited, but is highly necessary. In this paper, we propose a novel framework, called MedCLIP-SAM that combines CLIP and SAM models to generate segmentation of clinical scans using t
    
[^14]: 使用LLMs建模目标人群的信念和偏好

    Using LLMs to Model the Beliefs and Preferences of Targeted Populations

    [https://arxiv.org/abs/2403.20252](https://arxiv.org/abs/2403.20252)

    使用LLMs建模目标人群的信念和偏好，旨在实现各种应用，评估不同微调方法的效果，并检验其在匹配真实人类受访者偏好方面的能力。

    

    我们考虑了将大型语言模型(LLM)与人群的偏好进行对齐的问题。建模特定人群的信念、偏好和行为对于各种不同应用可能很有用，比如为新产品开展模拟焦点小组、进行虚拟调查以及测试行为干预，特别是对于昂贵、不切实际或不道德的干预。现有研究在不同情境下使用LLMs准确建模人类行为方面取得了不同程度的成功。我们对两种众所周知的微调方法进行基准测试和评估，评估得到的人群在匹配对电池电动汽车(BEVs)偏好调查中真实人类受访者偏好方面的能力。我们评估我们的模型是否能与整体人口统计数据以及个体回应相匹配，并研究LLMs在建模人群信念和偏好方面的作用。

    arXiv:2403.20252v1 Announce Type: cross  Abstract: We consider the problem of aligning a large language model (LLM) to model the preferences of a human population. Modeling the beliefs, preferences, and behaviors of a specific population can be useful for a variety of different applications, such as conducting simulated focus groups for new products, conducting virtual surveys, and testing behavioral interventions, especially for interventions that are expensive, impractical, or unethical. Existing work has had mixed success using LLMs to accurately model human behavior in different contexts. We benchmark and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs). We evaluate our models against their ability to match population-wide statistics as well as their ability to match individual responses, and we investigate the role
    
[^15]: 多动作场景中利用观测数据进行最优策略学习：估计、风险偏好和潜在故障

    Optimal Policy Learning with Observational Data in Multi-Action Scenarios: Estimation, Risk Preference, and Potential Failures

    [https://arxiv.org/abs/2403.20250](https://arxiv.org/abs/2403.20250)

    本文讨论了多动作场景中利用观测数据进行最优策略学习的方法，着重探讨了估计、风险偏好和潜在故障三个方面。

    

    本文讨论了利用观测数据进行最优策略学习（OPL），即数据驱动的最优决策，在多动作（或多臂）设置中，有限的决策选项可供选择。文章分为三个部分，分别讨论：估计、风险偏好和潜在故障。第一部分简要回顾了在这种分析背景下估计奖励（或值）函数和最优策略的关键方法。第二部分深入分析了决策风险。分析表明，决策者对风险的态度可以影响最优选择，具体体现在奖励条件均值与条件方差之间的权衡。在这里，作者将所提出的模型应用于...

    arXiv:2403.20250v1 Announce Type: cross  Abstract: This paper deals with optimal policy learning (OPL) with observational data, i.e. data-driven optimal decision-making, in multi-action (or multi-arm) settings, where a finite set of decision options is available. It is organized in three parts, where I discuss respectively: estimation, risk preference, and potential failures. The first part provides a brief review of the key approaches to estimating the reward (or value) function and optimal policy within this context of analysis. Here, I delineate the identification assumptions and statistical properties related to offline optimal policy learning estimators. In the second part, I delve into the analysis of decision risk. This analysis reveals that the optimal choice can be influenced by the decision maker's attitude towards risks, specifically in terms of the trade-off between reward conditional mean and conditional variance. Here, I present an application of the proposed model to rea
    
[^16]: 借助类和特征质心增强降维散点图

    Enhancing Dimension-Reduced Scatter Plots with Class and Feature Centroids

    [https://arxiv.org/abs/2403.20246](https://arxiv.org/abs/2403.20246)

    使用类和特征质心增强降维散点图，提高散点图的可解释性。

    

    arXiv:2403.20246v1 公告类型: 新的 摘要: 在高维生物医学数据中，降维越来越多地被应用以提高其可解释性。当数据集被降低到两个维度时，每个观测值被分配一个x和y坐标，并表示为散点图上的一个点。一个重要的挑战在于解释x和y轴的含义，这是由于降维本身固有的复杂性所致。本研究通过使用从降维得到的x和y坐标来计算类和特征质心来解决这一挑战，这些质心可以叠加到散点图上。这种方法将低维空间与原始高维空间连接起来。我们利用来自三种神经遗传疾病表型的数据阐明了这种方法的实用性，并展示了添加类和特征质心如何增加散点图的可解释性。

    arXiv:2403.20246v1 Announce Type: new  Abstract: Dimension reduction is increasingly applied to high-dimensional biomedical data to improve its interpretability. When datasets are reduced to two dimensions, each observation is assigned an x and y coordinates and is represented as a point on a scatter plot. A significant challenge lies in interpreting the meaning of the x and y axes due to the complexities inherent in dimension reduction. This study addresses this challenge by using the x and y coordinates derived from dimension reduction to calculate class and feature centroids, which can be overlaid onto the scatter plots. This method connects the low-dimension space to the original high-dimensional space. We illustrate the utility of this approach with data derived from the phenotypes of three neurogenetic diseases and demonstrate how the addition of class and feature centroids increases the interpretability of scatter plots.
    
[^17]: 机器学习中的函数双层优化

    Functional Bilevel Optimization for Machine Learning

    [https://arxiv.org/abs/2403.20233](https://arxiv.org/abs/2403.20233)

    介绍了机器学习中的函数双层优化问题，提出了不依赖于强凸假设的方法，并展示了在仪表回归和强化学习任务中使用神经网络的优势。

    

    在本文中，我们介绍了针对机器学习中的双层优化问题的一种新的函数视角，其中内部目标在函数空间上被最小化。这些类型的问题通常通过在参数设置下开发的方法来解决，其中内部目标对于预测函数的参数强凸。函数视角不依赖于此假设，特别允许使用超参数化的神经网络作为内部预测函数。我们提出了可扩展和高效的算法来解决函数双层优化问题，并展示了我们方法在适合自然函数双层结构的仪表回归和强化学习任务上的优势。

    arXiv:2403.20233v1 Announce Type: cross  Abstract: In this paper, we introduce a new functional point of view on bilevel optimization problems for machine learning, where the inner objective is minimized over a function space. These types of problems are most often solved by using methods developed in the parametric setting, where the inner objective is strongly convex with respect to the parameters of the prediction function. The functional point of view does not rely on this assumption and notably allows using over-parameterized neural networks as the inner prediction function. We propose scalable and efficient algorithms for the functional bilevel optimization problem and illustrate the benefits of our approach on instrumental regression and reinforcement learning tasks, which admit natural functional bilevel structures.
    
[^18]: 一种基于FPGA的可重构加速器，用于卷积-Transformer混合EfficientViT

    An FPGA-Based Reconfigurable Accelerator for Convolution-Transformer Hybrid EfficientViT

    [https://arxiv.org/abs/2403.20230](https://arxiv.org/abs/2403.20230)

    提出了一种基于FPGA的可重构加速器，用于EfficientViT，在硬件效率方面取得了重要进展

    

    视觉Transformer（ViT）在计算机视觉方面取得了显著的成功。然而，它们密集的计算和巨大的内存占用挑战了ViTs在嵌入式设备上的部署，需要高效的ViTs。在其中，EfficientViT是最先进的一种，具有卷积-Transformer混合架构，提高了准确性和硬件效率。不幸的是，由于其独特的架构，现有的加速器无法充分利用EfficientViT的硬件优势。在本文中，我们提出了一种基于FPGA的EfficientViT加速器，以推进ViTs的硬件效率前沿。具体而言，我们设计了一个可重构架构，有效支持各种操作类型，包括轻量级卷积和注意力，提高了硬件利用率。此外，我们提出了一种时分复用和流水线数据流，促进了层内和层间融合，减少了片外

    arXiv:2403.20230v1 Announce Type: cross  Abstract: Vision Transformers (ViTs) have achieved significant success in computer vision. However, their intensive computations and massive memory footprint challenge ViTs' deployment on embedded devices, calling for efficient ViTs. Among them, EfficientViT, the state-of-the-art one, features a Convolution-Transformer hybrid architecture, enhancing both accuracy and hardware efficiency. Unfortunately, existing accelerators cannot fully exploit the hardware benefits of EfficientViT due to its unique architecture. In this paper, we propose an FPGA-based accelerator for EfficientViT to advance the hardware efficiency frontier of ViTs. Specifically, we design a reconfigurable architecture to efficiently support various operation types, including lightweight convolutions and attention, boosting hardware utilization. Additionally, we present a time-multiplexed and pipelined dataflow to facilitate both intra- and inter-layer fusions, reducing off-chip
    
[^19]: 带隐蔽状态的图神经聚合-扩散

    Graph Neural Aggregation-diffusion with Metastability

    [https://arxiv.org/abs/2403.20221](https://arxiv.org/abs/2403.20221)

    提出了GRADE模型，通过图聚合-扩散方程实现了节点表示的亚稳定性，能够缓解图神经网络中过度平滑的问题。

    

    基于微分方程的连续图神经模型扩展了图神经网络（GNN）的架构。由于图扩散与信息传递之间的联系，基于扩散的模型得到了广泛研究。然而，扩散自然地将系统推向平衡状态，导致问题，如过度平滑。为此，我们提出了受图聚合-扩散方程启发的GRADE，其中包括非线性扩散和相互作用势引起的聚合之间的微妙平衡。通过聚合-扩散方程获得的节点表示表现出亚稳态，表明特征可以聚合成多个簇。此外，这些簇内的动态可以持续很长时间，有望缓解过度平滑的影响。我们模型中的非线性扩散推广了现有基于扩散的模型并确立了

    arXiv:2403.20221v1 Announce Type: cross  Abstract: Continuous graph neural models based on differential equations have expanded the architecture of graph neural networks (GNNs). Due to the connection between graph diffusion and message passing, diffusion-based models have been widely studied. However, diffusion naturally drives the system towards an equilibrium state, leading to issues like over-smoothing. To this end, we propose GRADE inspired by graph aggregation-diffusion equations, which includes the delicate balance between nonlinear diffusion and aggregation induced by interaction potentials. The node representations obtained through aggregation-diffusion equations exhibit metastability, indicating that features can aggregate into multiple clusters. In addition, the dynamics within these clusters can persist for long time periods, offering the potential to alleviate over-smoothing effects. This nonlinear diffusion in our model generalizes existing diffusion-based models and estab
    
[^20]: 无监督学习中旅行商问题中尺寸和难度的泛化

    On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem

    [https://arxiv.org/abs/2403.20212](https://arxiv.org/abs/2403.20212)

    研究了无监督学习在解决旅行商问题中的泛化能力，结果表明，使用更大的实例大小进行训练并增加嵌入维度可以构建更有效的表示，增强模型解决TSP问题的能力。

    

    我们研究了无监督学习在解决旅行商问题（TSP）中的泛化能力。我们使用一个用替代损失函数训练的图神经网络（GNN）来为每个节点生成嵌入。我们使用这些嵌入来构建一个热图，指示每条边成为最佳路径的可能性。然后我们应用局部搜索生成最终的预测。我们的研究探讨了不同训练实例大小、嵌入维数和分布如何影响无监督学习方法的结果。我们的结果表明，使用更大的实例大小进行训练并增加嵌入维度可以构建更有效的表示，增强模型解决TSP问题的能力。此外，在评估不同分布下的泛化能力时，我们首先确定了各种分布的难度，并探讨了不同难度如何影响最终结果。

    arXiv:2403.20212v1 Announce Type: new  Abstract: We study the generalization capability of Unsupervised Learning in solving the Travelling Salesman Problem (TSP). We use a Graph Neural Network (GNN) trained with a surrogate loss function to generate an embedding for each node. We use these embeddings to construct a heat map that indicates the likelihood of each edge being part of the optimal route. We then apply local search to generate our final predictions. Our investigation explores how different training instance sizes, embedding dimensions, and distributions influence the outcomes of Unsupervised Learning methods. Our results show that training with larger instance sizes and increasing embedding dimensions can build a more effective representation, enhancing the model's ability to solve TSP. Furthermore, in evaluating generalization across different distributions, we first determine the hardness of various distributions and explore how different hardnesses affect the final results
    
[^21]: 发挥大型语言模型在数据科学中预测表格任务的潜力

    Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science

    [https://arxiv.org/abs/2403.20208](https://arxiv.org/abs/2403.20208)

    本研究旨在利用大型语言模型解决数据科学中表格数据预测任务，通过在丰富的数据集上训练Llama-2模型并进行实际应用，取得显著的改进。

    

    在数据科学领域，分类、回归和缺失值填充等预测任务是与表格数据相关的常见挑战。这项研究旨在应用大型语言模型(LLMs)来解决这些预测任务。尽管LLMs擅长理解自然语言，但在处理结构化表格数据方面表现不佳。我们的研究旨在通过收集带有指令注释的表格语料库，并在这一丰富的数据集上对Llama-2进行大规模训练，以弥合这一差距。此外，我们研究了将训练模型应用于零-shot预测、少-shot预测和上下文学习场景的实际应用。通过广泛实验，我们的方法论显示了显著的改进。

    arXiv:2403.20208v1 Announce Type: new  Abstract: In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improv
    
[^22]: 用于机器学习的语音信号处理。以说话者隔离为例

    Voice Signal Processing for Machine Learning. The Case of Speaker Isolation

    [https://arxiv.org/abs/2403.20202](https://arxiv.org/abs/2403.20202)

    本文旨在提供对最常用作音频处理任务中信号分解方法的傅里叶和小波变换的简洁比较分析，以及用于评估语音可 intelligibility 的指标。

    

    自动语音助手的广泛使用以及其他最近的技术发展增加了对处理音频信号和特别是人类语音的应用的需求。语音识别任务通常使用人工智能和机器学习模型来执行。尽管存在端到端模型，但适当预处理信号可以大大减少任务的复杂性，使其可以用简单的ML模型和更少的计算资源来解决。然而，从事这类任务的ML工程师可能没有信号处理背景，这是一个完全不同的专业领域。本工作的目标是对傅里叶和小波变换进行简洁的比较分析，这是用于音频处理任务的最常用的信号分解方法。还讨论了用于评估语音可 intelligibility 的指标，即不变尺度的信号到

    arXiv:2403.20202v1 Announce Type: cross  Abstract: The widespread use of automated voice assistants along with other recent technological developments have increased the demand for applications that process audio signals and human voice in particular. Voice recognition tasks are typically performed using artificial intelligence and machine learning models. Even though end-to-end models exist, properly pre-processing the signal can greatly reduce the complexity of the task and allow it to be solved with a simpler ML model and fewer computational resources. However, ML engineers who work on such tasks might not have a background in signal processing which is an entirely different area of expertise.   The objective of this work is to provide a concise comparative analysis of Fourier and Wavelet transforms that are most commonly used as signal decomposition methods for audio processing tasks. Metrics for evaluating speech intelligibility are also discussed, namely Scale-Invariant Signal-to
    
[^23]: 双对偶体积最大化用于单纯结构矩阵分解

    Dual Simplex Volume Maximization for Simplex-Structured Matrix Factorization

    [https://arxiv.org/abs/2403.20197](https://arxiv.org/abs/2403.20197)

    通过使用对偶/极性概念，提出了一种双对偶体积最大化方法，用于解决单纯结构矩阵分解问题，填补了现有SSMF算法家族之间的差距。

    

    Simplex-structured matrix factorization（SSMF）是非负矩阵分解的泛化，是一种基础的可解释数据分析模型，在高光谱解混和和主题建模中有应用。为了获得可识别的解，标准方法是寻找最小体积解。通过利用多面体的对偶/极性概念，我们将原始空间中的最小体积SSMF转换为对偶空间中的最大体积问题。我们首先证明了这个最大体积对偶问题的可识别性。然后，我们使用这个对偶公式提供一种新颖的优化方法，以填补SSMF的两个现有算法家族之间的差距，即体积最小化和面识别。数值实验表明，所提出的方法相对于最先进的SSMF算法表现更好。

    arXiv:2403.20197v1 Announce Type: cross  Abstract: Simplex-structured matrix factorization (SSMF) is a generalization of nonnegative matrix factorization, a fundamental interpretable data analysis model, and has applications in hyperspectral unmixing and topic modeling. To obtain identifiable solutions, a standard approach is to find minimum-volume solutions. By taking advantage of the duality/polarity concept for polytopes, we convert minimum-volume SSMF in the primal space to a maximum-volume problem in the dual space. We first prove the identifiability of this maximum-volume dual problem. Then, we use this dual formulation to provide a novel optimization approach which bridges the gap between two existing families of algorithms for SSMF, namely volume minimization and facet identification. Numerical experiments show that the proposed approach performs favorably compared to the state-of-the-art SSMF algorithms.
    
[^24]: 利用空间约束贝叶斯网络（SCB-Net）增强岩性制图：一种基于现场数据约束预测的方法与不确定性评估

    Enhancing Lithological Mapping with Spatially Constrained Bayesian Network (SCB-Net): An Approach for Field Data-Constrained Predictions with Uncertainty Evaluation

    [https://arxiv.org/abs/2403.20195](https://arxiv.org/abs/2403.20195)

    基于空间约束贝叶斯网络（SCB-Net）的新架构旨在有效利用辅助变量信息，以增强岩性制图和解决传统地质统计技术在处理多变量和空间相关性方面的限制。

    

    地质地图是地球科学中一份极有价值的信息来源，为矿产勘探、自然灾害易损性等提供见解。这些地图使用地质观测数据来推断数据的数值或概念模型创建而成。传统地质统计技术被用于生成可靠的预测，考虑了数据中固有的空间模式。然而，随着辅助变量数量的增加，这些方法变得更加繁重。此外，传统机器学习方法常常难以处理空间相关数据，难以从地学数据集中提取有价值的非线性信息。为了解决这些限制，开发了一种新的架构称为空间约束贝叶斯网络（SCB-Net）。SCB-Net旨在有效利用辅助变量的信息。

    arXiv:2403.20195v1 Announce Type: cross  Abstract: Geological maps are an extremely valuable source of information for the Earth sciences. They provide insights into mineral exploration, vulnerability to natural hazards, and many other applications. These maps are created using numerical or conceptual models that use geological observations to extrapolate data. Geostatistical techniques have traditionally been used to generate reliable predictions that take into account the spatial patterns inherent in the data. However, as the number of auxiliary variables increases, these methods become more labor-intensive. Additionally, traditional machine learning methods often struggle with spatially correlated data and extracting valuable non-linear information from geoscientific datasets. To address these limitations, a new architecture called the Spatially Constrained Bayesian Network (SCB-Net) has been developed. The SCB-Net aims to effectively exploit the information from auxiliary variables
    
[^25]: Homomorphic WiSARDs: Efficient Weightless Neural Network training over encrypted data

    Homomorphic WiSARDs: Efficient Weightless Neural Network training over encrypted data

    [https://arxiv.org/abs/2403.20190](https://arxiv.org/abs/2403.20190)

    本文提出了对Wilkie, Stonham和Aleksander的Recognition Device (WiSARD)进行同态评估，然后用于加密数据的训练和推断Weightless Neural Networks (WNNs)的方法，相比较传统的CNNs，WNNs在性能上提供了更好的表现。

    

    机器学习算法的广泛应用越来越受到数据隐私研究界的关注，许多人试图为其开发保护隐私的技术。在现有方法中，通过对加密数据直接执行操作的同态评估具有突出优点，可以提供强大的保密保障。推断算法的同态评估即使对于相对深的卷积神经网络(CNNs)也是实用的。然而, 训练仍然是一个主要挑战，目前的解决方案通常倾向于使用轻量级算法，这可能不适合解决更复杂的问题，如图像识别。本文介绍了对Wilkie, Stonham和Aleksander的Recognition Device (WiSARD)和随后的Weightless Neural Networks (WNNs)进行同态评估，用于加密数据的训练和推断。与CNNs相比，WNNs在性能上提供了更好的表现。

    arXiv:2403.20190v1 Announce Type: cross  Abstract: The widespread application of machine learning algorithms is a matter of increasing concern for the data privacy research community, and many have sought to develop privacy-preserving techniques for it. Among existing approaches, the homomorphic evaluation of ML algorithms stands out by performing operations directly over encrypted data, enabling strong guarantees of confidentiality. The homomorphic evaluation of inference algorithms is practical even for relatively deep Convolution Neural Networks (CNNs). However, training is still a major challenge, with current solutions often resorting to lightweight algorithms that can be unfit for solving more complex problems, such as image recognition. This work introduces the homomorphic evaluation of Wilkie, Stonham, and Aleksander's Recognition Device (WiSARD) and subsequent Weightless Neural Networks (WNNs) for training and inference on encrypted data. Compared to CNNs, WNNs offer better pe
    
[^26]: 分布式群智能学习用于边缘物联网

    Distributed Swarm Learning for Edge Internet of Things

    [https://arxiv.org/abs/2403.20188](https://arxiv.org/abs/2403.20188)

    本文探讨了一种名为分布式群智能学习（DSL）的新型框架，将人工智能和生物群智能结合在一起，为大规模IoT在无线网络边缘提供高效的解决方案和稳健的工具。

    

    arXiv:2403.20188v1 公告类型: 跨领域  摘要: 物联网（IoT）的快速增长导致智能IoT设备在无线边缘进行协作机器学习任务，开启了边缘学习的新时代。面对在资源有限的无线网络中运行的大量硬件受限制的IoT设备，边缘学习面临着通信和计算瓶颈、设备和数据异构性、安全风险、隐私泄漏、非凸优化和复杂的无线环境等巨大挑战。为了解决这些问题，本文探讨了一种称为分布式群智能学习（DSL）的新型框架，将人工智能和生物群智能在整体上结合起来。通过运用先进的信号处理和通信技术，DSL为无线网络边缘的大规模IoT提供了高效的解决方案和稳健的工具。

    arXiv:2403.20188v1 Announce Type: cross  Abstract: The rapid growth of Internet of Things (IoT) has led to the widespread deployment of smart IoT devices at wireless edge for collaborative machine learning tasks, ushering in a new era of edge learning. With a huge number of hardware-constrained IoT devices operating in resource-limited wireless networks, edge learning encounters substantial challenges, including communication and computation bottlenecks, device and data heterogeneity, security risks, privacy leakages, non-convex optimization, and complex wireless environments. To address these issues, this article explores a novel framework known as distributed swarm learning (DSL), which combines artificial intelligence and biological swarm intelligence in a holistic manner. By harnessing advanced signal processing and communications, DSL provides efficient solutions and robust tools for large-scale IoT at the edge of wireless networks.
    
[^27]: 利用ASR驱动的Wav2Vec2在数据稀缺环境中探究病态语音质量评估

    Exploring Pathological Speech Quality Assessment with ASR-Powered Wav2Vec2 in Data-Scarce Context

    [https://arxiv.org/abs/2403.20184](https://arxiv.org/abs/2403.20184)

    本文提出了一种新颖的方法，在数据稀缺情况下系统在整体音频级别进行学习，同时利用ASR驱动的Wav2Vec2架构作为特征提取器，在语音评估中取得了显著的结果。

    

    自动语音质量评估作为传统感知临床评估的替代或支持近年来备受关注。然而，迄今为止大部分研究仅在简单任务（如二元分类）上取得了良好的结果，这主要是由于数据稀缺造成的。为了应对这一挑战，目前的研究倾向于将患者的音频文件分割成多个样本以增加数据集。然而，这种方法存在局限性，因为它将整体音频得分间接地与个别段相关联。本文提出了一种新颖的方法，即系统在整个音频级别学习，而不是在片段级别学习，尽管存在数据稀缺情况。本文建议在语音评估中使用预训练的Wav2Vec2架构进行自监督学习（SSL）和ASR作为特征提取器。在HNC数据集上进行了实验，我们的ASR驱动方法与其他方法相比建立了新的基线，实现了平均$MSE=0.73$和$MSE=1.15$来预测智商。

    arXiv:2403.20184v1 Announce Type: cross  Abstract: Automatic speech quality assessment has raised more attention as an alternative or support to traditional perceptual clinical evaluation. However, most research so far only gains good results on simple tasks such as binary classification, largely due to data scarcity. To deal with this challenge, current works tend to segment patients' audio files into many samples to augment the datasets. Nevertheless, this approach has limitations, as it indirectly relates overall audio scores to individual segments. This paper introduces a novel approach where the system learns at the audio level instead of segments despite data scarcity. This paper proposes to use the pre-trained Wav2Vec2 architecture for both SSL, and ASR as feature extractor in speech assessment. Carried out on the HNC dataset, our ASR-driven approach established a new baseline compared with other approaches, obtaining average $MSE=0.73$ and $MSE=1.15$ for the prediction of intel
    
[^28]: 在异构MDPs中通过收敛感知采样与筛选增强联邦强化学习的CAESAR算法

    CAESAR: Enhancing Federated RL in Heterogeneous MDPs through Convergence-Aware Sampling with Screening

    [https://arxiv.org/abs/2403.20156](https://arxiv.org/abs/2403.20156)

    CAESAR算法通过结合收敛感知采样和筛选机制，有效增强了个体代理在不同MDPs上的学习。

    

    本研究探讨了在值为基础代理在不同马尔可夫决策过程（MDPs）之间运行时的联邦强化学习（FedRL）。现有的FedRL方法通常通过对代理的值函数进行平均来改善它们的表现。然而，在异构环境中，这种聚合策略在代理收敛到不同的最优值函数时是次优的。为了解决这个问题，我们引入了设计用于增强个体代理跨各种MDPs学习的Convergence-AwarE SAmpling with scReening（CAESAR）聚合方案。CAESAR是服务器使用的一种聚合策略，结合了收敛感知采样和筛选机制。通过利用学习相同MDP中代理收敛到相同最优值函数的事实，CAESAR使得能够从更熟练的同行那里有选择地吸收知识。

    arXiv:2403.20156v1 Announce Type: cross  Abstract: In this study, we delve into Federated Reinforcement Learning (FedRL) in the context of value-based agents operating across diverse Markov Decision Processes (MDPs). Existing FedRL methods typically aggregate agents' learning by averaging the value functions across them to improve their performance. However, this aggregation strategy is suboptimal in heterogeneous environments where agents converge to diverse optimal value functions. To address this problem, we introduce the Convergence-AwarE SAmpling with scReening (CAESAR) aggregation scheme designed to enhance the learning of individual agents across varied MDPs. CAESAR is an aggregation strategy used by the server that combines convergence-aware sampling with a screening mechanism. By exploiting the fact that agents learning in identical MDPs are converging to the same optimal value function, CAESAR enables the selective assimilation of knowledge from more proficient counterparts, 
    
[^29]: TFB：面向时间序列预测方法全面且公平的基准比较

    TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods

    [https://arxiv.org/abs/2403.20150](https://arxiv.org/abs/2403.20150)

    TFB通过解决数据领域覆盖不足、对传统方法的刻板印象以及不一致、不灵活的流程等问题，推动了时间序列预测方法基准比较的最新技术发展。

    

    时间序列会在经济、交通、健康和能源等不同领域中产生，对未来数值的预测在许多重要应用中起着关键作用。不出所料，许多预测方法被提出。为了确保进展，有必要能够以全面且可靠的方式经验性地研究和比较这些方法。为了实现这一目标，我们提出了TFB，一个自动化的时间序列预测（TSF）方法基准测试。TFB通过解决与数据集、比较方法和评估管道相关的缺点，推动了最新技术的发展：1）数据领域覆盖不足，2）对传统方法的刻板印象，3）不一致和不灵活的流程。为了获得更好的领域覆盖率，我们包括了来自10个不同领域的数据集：交通、电力、能源、环境、自然、经济、股票市场、银行、健康和网络。我们还提供了一个时间序列特性

    arXiv:2403.20150v1 Announce Type: cross  Abstract: Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series char
    
[^30]: 电力市场中光伏电力随机决策的一致性预测

    Conformal Prediction for Stochastic Decision-Making of PV Power in Electricity Markets

    [https://arxiv.org/abs/2403.20149](https://arxiv.org/abs/2403.20149)

    论文研究了在电力市场中使用一致性预测对光伏电力日前预测进行决策的方法，并发现结合特定出价策略可以在保持能量平衡最小的情况下获得高利润。

    

    这篇论文研究了使用一致性预测（CP），一种新兴的概率预测方法，用于加强在电力市场中日前光伏电力预测的参与。首先，使用机器学习模型构建点预测。然后，实施了几种CP的变体，通过创建CP区间和累积分布函数来量化这些预测的不确定性。在不确定性下估计了多种出价策略下的电力市场的最优数量出价，即：信任预测、最坏情况、Newsvendor和期望效用最大化（EUM）。结果显示，结合k最近邻和/或Mondrian分箱的CP胜过其对应的线性分位数回归器。使用CP结合某些出价策略可以在保持能量平衡最小的情况下获得高利润。具体来说，使用具有k近

    arXiv:2403.20149v1 Announce Type: new  Abstract: This paper studies the use of conformal prediction (CP), an emerging probabilistic forecasting method, for day-ahead photovoltaic power predictions to enhance participation in electricity markets. First, machine learning models are used to construct point predictions. Thereafter, several variants of CP are implemented to quantify the uncertainty of those predictions by creating CP intervals and cumulative distribution functions. Optimal quantity bids for the electricity market are estimated using several bidding strategies under uncertainty, namely: trust-the-forecast, worst-case, Newsvendor and expected utility maximization (EUM). Results show that CP in combination with k-nearest neighbors and/or Mondrian binning outperforms its corresponding linear quantile regressors. Using CP in combination with certain bidding strategies can yield high profit with minimal energy imbalance. In concrete, using conformal predictive systems with k-near
    
[^31]: 通过机器学习设计泊松积分器

    Designing Poisson Integrators Through Machine Learning

    [https://arxiv.org/abs/2403.20139](https://arxiv.org/abs/2403.20139)

    本文通过机器学习技术，将泊松积分器的设计理解为解决特定PDE问题的优化过程，实现了对哈密顿-雅可比PDE的简单近似，结合了物理建模和数据的设计趋势。

    

    本文提出了一种构建泊松积分器的通用方法，即保留基础泊松几何的积分器。我们假设泊松流形是可积的，这意味着存在已知的局部辛群体，泊松流形作为单位的集合。我们的构造建立在泊松微分同胚和拉格朗日切分之间的对应关系之上，这使得我们可以将泊松积分器的设计重新表述为对某个特定PDE（哈密顿-雅可比）的解。这项工作的主要创新在于将哈密顿-雅可比PDE理解为一个优化问题，其解可以通过机器学习相关技术轻松近似。这个研究方向与当前PDE和机器学习社区的趋势相一致，如由物理启发的神经网络所发起，倡导结合物理建模（哈密顿-雅可比PDE）和数据的设计。

    arXiv:2403.20139v1 Announce Type: cross  Abstract: This paper presents a general method to construct Poisson integrators, i.e., integrators that preserve the underlying Poisson geometry. We assume the Poisson manifold is integrable, meaning there is a known local symplectic groupoid for which the Poisson manifold serves as the set of units. Our constructions build upon the correspondence between Poisson diffeomorphisms and Lagrangian bisections, which allows us to reformulate the design of Poisson integrators as solutions to a certain PDE (Hamilton-Jacobi). The main novelty of this work is to understand the Hamilton-Jacobi PDE as an optimization problem, whose solution can be easily approximated using machine learning related techniques. This research direction aligns with the current trend in the PDE and machine learning communities, as initiated by Physics- Informed Neural Networks, advocating for designs that combine both physical modeling (the Hamilton-Jacobi PDE) and data.
    
[^32]: 在室外环境中使用WASN进行声事件定位和分类

    Sound event localization and classification using WASN in Outdoor Environment

    [https://arxiv.org/abs/2403.20130](https://arxiv.org/abs/2403.20130)

    本文提出了一种基于深度学习的方法，利用多种特征和注意机制来估计声源的位置和类别，包括引入"声音地图"特征、使用Gammatone滤波器生成更适合室外环境的声学特征，以及集成注意机制来学习通道间的关系。

    

    基于深度学习的声事件定位和分类是无线声学传感器网络中的新兴研究领域。然而，当前的声事件定位和分类方法通常依赖于单个麦克风阵列，容易受到信号衰减和环境噪音的影响，这限制了它们的监测范围。此外，使用多个麦克风阵列的方法通常只关注源定位，忽略了声事件分类方面。在本文中，我们提出了一种基于深度学习的方法，利用多种特征和注意机制来估计声源的位置和类别。我们引入了"声音地图"特征，以捕获多个频段的空间信息。我们还使用Gammatone滤波器生成更适合室外环境的声学特征。此外，我们集成了注意机制来学习通道间的关系。

    arXiv:2403.20130v1 Announce Type: cross  Abstract: Deep learning-based sound event localization and classification is an emerging research area within wireless acoustic sensor networks. However, current methods for sound event localization and classification typically rely on a single microphone array, making them susceptible to signal attenuation and environmental noise, which limits their monitoring range. Moreover, methods using multiple microphone arrays often focus solely on source localization, neglecting the aspect of sound event classification. In this paper, we propose a deep learning-based method that employs multiple features and attention mechanisms to estimate the location and class of sound source. We introduce a Soundmap feature to capture spatial information across multiple frequency bands. We also use the Gammatone filter to generate acoustic features more suitable for outdoor environments. Furthermore, we integrate attention mechanisms to learn channel-wise relationsh
    
[^33]: 机器学习算法在代谢性减重手术术后成功分类中的应用：一项全面研究

    Application of Machine Learning Algorithms in Classifying Postoperative Success in Metabolic Bariatric Surgery: A Comprehensive Study

    [https://arxiv.org/abs/2403.20124](https://arxiv.org/abs/2403.20124)

    该研究提出了一种新颖的机器学习方法，用于代谢性减重手术患者的分类，通过对比不同模型和变量类型的有效性，为优化治疗策略提供了重要见解。

    

    目标：代谢性减重手术(MBS)是对患有肥胖及相关健康问题的患者进行重要干预。精确分类和预测患者结果对优化治疗策略至关重要。本研究提出了一种新颖的机器学习方法，用于在代谢性减重手术环境中对患者进行分类，为不同模型和变量类型的有效性提供见解。方法：对73名患者的数据集应用了各种机器学习模型，包括GaussianNB、ComplementNB、KNN、决策树、带有RandomOverSampler的KNN和带有SMOTE的KNN。数据集包括心理测量、社会经济和分析变量，通过分析确定最有效的预测模型。研究还探讨了不同变量分组和过采样技术的影响。结果：实验结果显示，平均准确率高达66.7%，表明了一些模型在代谢性减重手术中具有良好预测能力。

    arXiv:2403.20124v1 Announce Type: new  Abstract: Objectives: Metabolic Bariatric Surgery (MBS) is a critical intervention for patients living with obesity and related health issues. Accurate classification and prediction of patient outcomes are vital for optimizing treatment strategies. This study presents a novel machine learning approach to classify patients in the context of metabolic bariatric surgery, providing insights into the efficacy of different models and variable types. Methods: Various machine learning models, including GaussianNB, ComplementNB, KNN, Decision Tree, KNN with RandomOverSampler, and KNN with SMOTE, were applied to a dataset of 73 patients. The dataset, comprising psychometric, socioeconomic, and analytical variables, was analyzed to determine the most efficient predictive model. The study also explored the impact of different variable groupings and oversampling techniques. Results: Experimental results indicate average accuracy values as high as 66.7% for the
    
[^34]: 使用粒度统计不变量进行分类的学习

    Learning using granularity statistical invariants for classification

    [https://arxiv.org/abs/2403.20122](https://arxiv.org/abs/2403.20122)

    LUGSI是第一次引入粒度统计不变量，相较于LUSI，它通过强弱两种收敛机制并最小化预期风险，提高了数据结构信息并成功降维。

    

    arXiv:2403.20122v1 公告类型：新摘要：学习使用统计不变量（Learning using statistical invariants，LUSI）是一种新的学习范式，采用了弱收敛机制，可以应用于更广泛的分类问题。然而，LUSI中不变矩阵的计算成本在训练过程中对于大规模数据集来说过高。为解决这一问题，本文引入了一种粒度统计不变量用于LUSI，并开发了一种名为学习使用粒度统计不变量（Learning using granularity statistical invariants，LUGSI）的新学习范式。LUGSI同时采用了强和弱收敛机制，以最小化预期风险的视角。据我们所知，这是第一次构建粒度统计不变量。与LUSI相比，引入这种新的统计不变量带来两个优势。首先，它增强了数据的结构信息。其次，LUGSI通过最大化类别之间的距离，将大的不变性矩阵转化为小矩阵，达到了降维的效果。

    arXiv:2403.20122v1 Announce Type: new  Abstract: Learning using statistical invariants (LUSI) is a new learning paradigm, which adopts weak convergence mechanism, and can be applied to a wider range of classification problems. However, the computation cost of invariant matrices in LUSI is high for large-scale datasets during training. To settle this issue, this paper introduces a granularity statistical invariant for LUSI, and develops a new learning paradigm called learning using granularity statistical invariants (LUGSI). LUGSI employs both strong and weak convergence mechanisms, taking a perspective of minimizing expected risk. As far as we know, it is the first time to construct granularity statistical invariants. Compared to LUSI, the introduction of this new statistical invariant brings two advantages. Firstly, it enhances the structural information of the data. Secondly, LUGSI transforms a large invariant matrix into a smaller one by maximizing the distance between classes, achi
    
[^35]: 利用人机协同机器学习进行乳腺癌医学图像的分割、分类和解释

    Segmentation, Classification and Interpretation of Breast Cancer Medical Images using Human-in-the-Loop Machine Learning

    [https://arxiv.org/abs/2403.20112](https://arxiv.org/abs/2403.20112)

    本文研究了在医学领域使用人机协同策略训练机器学习模型，具体涉及乳腺癌基因组数据和全切片成像分析，虽然病理学家的参与增强了模型的解释能力，但分类结果不理想，指出了该方法的局限性。

    

    本文探讨了在医学领域训练机器学习模型中应用人机协同（HITL）策略。在本案例中，提出了一种医生参与的方法，以利用人类专家处理大规模和复杂数据。具体而言，本文涉及基因组数据的整合以及乳腺癌的全切片成像（WSI）分析。开发了三个不同的任务：组织病理图像的分割，针对癌症基因组亚型的图像分类，最后是对机器学习结果的解释。病理学家的参与帮助我们开发了更好的分割模型，并增强了模型的解释能力，但分类结果不理想，突出了这种方法的局限性：尽管涉及了人类专家，但复杂领域仍然可能带来挑战，人机协同方法并不总是有效的。

    arXiv:2403.20112v1 Announce Type: cross  Abstract: This paper explores the application of Human-in-the-Loop (HITL) strategies in training machine learning models in the medical domain. In this case a doctor-in-the-loop approach is proposed to leverage human expertise in dealing with large and complex data. Specifically, the paper deals with the integration of genomic data and Whole Slide Imaging (WSI) analysis of breast cancer. Three different tasks were developed: segmentation of histopathological images, classification of this images regarding the genomic subtype of the cancer and, finally, interpretation of the machine learning results. The involvement of a pathologist helped us to develop a better segmentation model and to enhance the explainatory capabilities of the models, but the classification results were suboptimal, highlighting the limitations of this approach: despite involving human experts, complex domains can still pose challenges, and a HITL approach may not always be e
    
[^36]: Mol-AIR：使用自适应内在奖励的分子强化学习用于目标导向的分子生成

    Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation

    [https://arxiv.org/abs/2403.20109](https://arxiv.org/abs/2403.20109)

    Mol-AIR 提出了使用自适应内在奖励的分子强化学习框架，通过利用历史和学习的内在奖励优势，在生成具有期望性质的分子方面表现出卓越性能。

    

    优化发现具有期望性质的分子结构的技术在基于人工智能(AI)的药物发现中至关重要。将深度生成模型与强化学习相结合已经成为生成具有特定性质的分子的有效策略。尽管具有潜力，但这种方法在探索庞大的化学空间和优化特定化学性质方面效果不佳。为了克服这些局限性，我们提出了Mol-AIR，这是一个基于强化学习的框架，使用自适应内在奖励进行有效的目标导向分子生成。Mol-AIR利用基于历史的和基于学习的内在奖励的优势，通过利用随机蒸馏网络和基于计数的策略。在基准测试中，Mol-AIR展现了超出现有方法的性能，在生成具有期望性质的分子方面，无需任何先验知识，包括pena

    arXiv:2403.20109v1 Announce Type: cross  Abstract: Optimizing techniques for discovering molecular structures with desired properties is crucial in artificial intelligence(AI)-based drug discovery. Combining deep generative models with reinforcement learning has emerged as an effective strategy for generating molecules with specific properties. Despite its potential, this approach is ineffective in exploring the vast chemical space and optimizing particular chemical properties. To overcome these limitations, we present Mol-AIR, a reinforcement learning-based framework using adaptive intrinsic rewards for effective goal-directed molecular generation. Mol-AIR leverages the strengths of both history-based and learning-based intrinsic rewards by exploiting random distillation network and counting-based strategies. In benchmark tests, Mol-AIR demonstrates superior performance over existing approaches in generating molecules with desired properties without any prior knowledge, including pena
    
[^37]: 通过选择性状态空间模型聚合局部和全局特征以实现高效图像去模糊

    Aggregating Local and Global Features via Selective State Spaces Model for Efficient Image Deblurring

    [https://arxiv.org/abs/2403.20106](https://arxiv.org/abs/2403.20106)

    我们提出了一种高效的图像去模糊网络，通过选择性状态空间模型聚合局部和全局特征，解决了消除长距离模糊降噪扰动和保持计算效率之间的难题

    

    图像去模糊是指从对应的模糊图像中恢复高质量图像的过程。近年来，深度学习模型如CNN和Transformer的出现使得在这一领域取得了重大进展。然而，这些方法往往面临着在消除远程模糊降解扰动和保持计算效率之间的两难境地，这制约了它们的实际应用。为解决这一问题，我们提出了一种有效的图像去模糊网络，利用选择性结构化状态空间模型来聚合丰富准确的特征。具体而言，我们设计了一个聚合局部和全局块（ALGBlock）来捕获和融合局部不变特性和非局部信息。ALGBlock由两个块组成：（1）局部块使用简化的通道注意力模型局部连通性。（2）全局块捕捉长距离依赖特征。

    arXiv:2403.20106v1 Announce Type: cross  Abstract: Image deblurring is a process of restoring a high quality image from the corresponding blurred image. Significant progress in this field has been made possible by the emergence of various effective deep learning models, including CNNs and Transformers. However, these methods often face the dilemma between eliminating long-range blur degradation perturbations and maintaining computational efficiency, which hinders their practical application. To address this issue, we propose an efficient image deblurring network that leverages selective structured state spaces model to aggregate enriched and accurate features. Specifically, we design an aggregate local and global block (ALGBlock) to capture and fuse both local invariant properties and non-local information. The ALGBlock consists of two blocks: (1) The local block models local connectivity using simplified channel attention. (2) The global block captures long-range dependency features w
    
[^38]: RealKIE: 五个新颖的企业关键信息提取数据集

    RealKIE: Five Novel Datasets for Enterprise Key Information Extraction

    [https://arxiv.org/abs/2403.20101](https://arxiv.org/abs/2403.20101)

    RealKIE提供了五个具有挑战性的企业关键信息提取数据集，为投资分析和法律数据处理等任务提供了现实的测试基地，并为NLP模型的发展做出了贡献。

    

    我们介绍了RealKIE，这是一个旨在推动关键信息提取方法发展的五个具有挑战性的数据集基准，重点是企业应用。这些数据集包括美国SEC S1文件、美国保密协议、英国慈善报告、FCC发票和资源合同等各种类型的文档。每个数据集都具有独特的挑战：文本序列化不佳、长文档中稀疏的注释和复杂的表格布局。这些数据集为关键信息提取任务（如投资分析和法律数据处理）提供了一个现实的测试基地。除了介绍这些数据集外，我们还提供了对注释过程、文档处理技术和基线建模方法的深入描述。这一贡献促进了能够处理实际挑战的NLP模型的发展，并支持进一步研究可应用于工业的信息提取技术。

    arXiv:2403.20101v1 Announce Type: new  Abstract: We introduce RealKIE, a benchmark of five challenging datasets aimed at advancing key information extraction methods, with an emphasis on enterprise applications. The datasets include a diverse range of documents including SEC S1 Filings, US Non-disclosure Agreements, UK Charity Reports, FCC Invoices, and Resource Contracts. Each presents unique challenges: poor text serialization, sparse annotations in long documents, and complex tabular layouts. These datasets provide a realistic testing ground for key information extraction tasks like investment analysis and legal data processing.   In addition to presenting these datasets, we offer an in-depth description of the annotation process, document processing techniques, and baseline modeling approaches. This contribution facilitates the development of NLP models capable of handling practical challenges and supports further research into information extraction technologies applicable to indu
    
[^39]: 通过预训练视觉-语言模型进行负标签引导的OOD检测

    Negative Label Guided OOD Detection with Pretrained Vision-Language Models

    [https://arxiv.org/abs/2403.20078](https://arxiv.org/abs/2403.20078)

    本文提出了一种新颖的后验OOD检测方法NegLabel，通过利用大量负标签信息，设计了与负标签协作的OOD分数方案，并在多个VLM架构上取得了最先进的性能。

    

    OOD检测旨在识别来自未知类别的样本，在信任模型抵御意外输入错误方面发挥关键作用。本文提出了一种新颖的后验OOD检测方法NegLabel，该方法从大量语料库数据库中提取了大量负标签。我们设计了一个与负标签协作的OOD分数的新方案。理论分析有助于理解负标签的机制。广泛的实验表明，我们的方法NegLabel在各种OOD检测基准上实现了最先进的性能，并在多个VLM架构上具有良好的泛化性能。

    arXiv:2403.20078v1 Announce Type: cross  Abstract: Out-of-distribution (OOD) detection aims at identifying samples from unknown classes, playing a crucial role in trustworthy models against errors on unexpected inputs. Extensive research has been dedicated to exploring OOD detection in the vision modality. Vision-language models (VLMs) can leverage both textual and visual information for various multi-modal applications, whereas few OOD detection methods take into account information from the text modality. In this paper, we propose a novel post hoc OOD detection method, called NegLabel, which takes a vast number of negative labels from extensive corpus databases. We design a novel scheme for the OOD score collaborated with negative labels. Theoretical analysis helps to understand the mechanism of negative labels. Extensive experiments demonstrate that our method NegLabel achieves state-of-the-art performance on various OOD detection benchmarks and generalizes well on multiple VLM arch
    
[^40]: 能源和延迟受限的无线网络中的自适应分散式联邦学习

    Adaptive Decentralized Federated Learning in Energy and Latency Constrained Wireless Networks

    [https://arxiv.org/abs/2403.20075](https://arxiv.org/abs/2403.20075)

    本文提出了在考虑能源和延迟约束的情况下，通过优化不同资源预算的设备之间的本地训练轮次数量，以最小化分散式联邦学习（DFL）的损失函数。

    

    在联邦学习（FL）中，由中心节点聚合参数，通信开销是一个重要的问题。为了规避这一限制并减轻FL框架内的单点故障，最近的研究引入了分散式联邦学习（DFL）作为一种可行的替代方案。考虑到设备的异构性，以及与参数聚合相关的能量成本，本文研究了如何高效利用有限资源来提高模型性能的问题。具体而言，我们提出了一个问题，即在考虑能源和延迟约束的情况下最小化DFL的损失函数。所提出的解决方案涉及优化在不同资源预算的多样化设备之间进行的本地训练轮次的数量。为了使这个问题可行，我们首先分析了具有不同本地训练轮次的边缘设备的DFL的收敛性。

    arXiv:2403.20075v1 Announce Type: new  Abstract: In Federated Learning (FL), with parameter aggregated by a central node, the communication overhead is a substantial concern. To circumvent this limitation and alleviate the single point of failure within the FL framework, recent studies have introduced Decentralized Federated Learning (DFL) as a viable alternative. Considering the device heterogeneity, and energy cost associated with parameter aggregation, in this paper, the problem on how to efficiently leverage the limited resources available to enhance the model performance is investigated. Specifically, we formulate a problem that minimizes the loss function of DFL while considering energy and latency constraints. The proposed solution involves optimizing the number of local training rounds across diverse devices with varying resource budgets. To make this problem tractable, we first analyze the convergence of DFL with edge devices with different rounds of local training. The derive
    
[^41]: 利用同时功能PET/MR和深度整合的脑代谢、血液动力学和灌注网络彻底改变疾病诊断

    Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks

    [https://arxiv.org/abs/2403.20058](https://arxiv.org/abs/2403.20058)

    提出了MX-ARM，一种基于AI的疾病诊断模型，利用同时功能PET/MR技术，能够在推理过程中同时接受单模态和多模态输入，具有创新的模态分离和重构功能。

    

    同时功能PET/MR（sf-PET/MR）是一种尖端的多模式神经影像技术。它提供了一个前所未有的机会，可以同时监测和整合由时空协变代谢活动、神经活动和脑血流（灌注）构建的多方面大脑网络。虽然在科学/临床价值上很高，但PET/MR硬件的可及性不足阻碍了其应用，更不用说现代基于AI的PET/MR融合模型。我们的目标是开发一个基于AI的临床可行疾病诊断模型，该模型基于全面的sf-PET/MR数据进行训练，在推理过程中具有允许单模态输入（例如，仅PET）以及强制多模态准确性的能力。为此，我们提出了MX-ARM，一种多模态专家混合对齐和重构模型。它是模态可分离和可交换的，动态分配不同的多层感知器（"混合）

    arXiv:2403.20058v1 Announce Type: cross  Abstract: Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge multimodal neuroimaging technique. It provides an unprecedented opportunity for concurrently monitoring and integrating multifaceted brain networks built by spatiotemporally covaried metabolic activity, neural activity, and cerebral blood flow (perfusion). Albeit high scientific/clinical values, short in hardware accessibility of PET/MR hinders its applications, let alone modern AI-based PET/MR fusion models. Our objective is to develop a clinically feasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR data with the power of, during inferencing, allowing single modality input (e.g., PET only) as well as enforcing multimodal-based accuracy. To this end, we propose MX-ARM, a multimodal MiXture-of-experts Alignment and Reconstruction Model. It is modality detachable and exchangeable, allocating different multi-layer perceptrons dynamically ("mixture 
    
[^42]: 一步步拥抱未知：走向真实世界中可靠的稀疏训练

    Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World

    [https://arxiv.org/abs/2403.20047](https://arxiv.org/abs/2403.20047)

    本研究探讨了稀疏训练在未知分布数据检测中的可靠性问题，提出了一种新的面向未知的稀疏训练方法，解决了稀疏训练加剧未知数据不可靠性的挑战。

    

    稀疏训练已经成为一种在现实世界应用中资源高效的深度神经网络（DNN）方法，然而稀疏模型的可靠性仍然是一个关键问题，特别是在检测未知的超出分布（OOD）数据时。本研究从OOD的角度研究了稀疏训练的可靠性，并揭示了稀疏训练加剧了OOD的不可靠性。缺乏未知信息和稀疏约束阻碍了对权重空间的有效探索以及准确区分已知和未知知识。为了解决这些挑战，我们提出了一种新的面向未知的稀疏训练方法，该方法结合了损失修改、自动调整策略和投票方案，以引导权重空间的探索，并减轻已知和未知信息之间的混淆，而不会增加显著的额外成本或需要额外访问。

    arXiv:2403.20047v1 Announce Type: new  Abstract: Sparse training has emerged as a promising method for resource-efficient deep neural networks (DNNs) in real-world applications. However, the reliability of sparse models remains a crucial concern, particularly in detecting unknown out-of-distribution (OOD) data. This study addresses the knowledge gap by investigating the reliability of sparse training from an OOD perspective and reveals that sparse training exacerbates OOD unreliability. The lack of unknown information and the sparse constraints hinder the effective exploration of weight space and accurate differentiation between known and unknown knowledge. To tackle these challenges, we propose a new unknown-aware sparse training method, which incorporates a loss modification, auto-tuning strategy, and a voting scheme to guide weight space exploration and mitigate confusion between known and unknown information without incurring significant additional costs or requiring access to addi
    
[^43]: 使用弹性网络和MOPSO进行销售价格预测的新型决策融合方法

    A novel decision fusion approach for sale price prediction using Elastic Net and MOPSO

    [https://arxiv.org/abs/2403.20033](https://arxiv.org/abs/2403.20033)

    该研究提出了一种新型的决策融合方法，通过选择信息性变量来提高价格预测的准确性和效果

    

    价格预测算法根据市场趋势、预期需求以及其他特征（包括政府规定、国际交易、投机和期望）为每种产品或服务提出价格。作为价格预测中的因变量，价格受到多个独立和相关变量的影响，这可能对价格预测构成挑战。为了克服这一挑战，机器学习算法允许更准确地进行价格预测，而无需明确对变量之间的关联性建模。然而，随着输入变量的增加，这挑战了现有的机器学习方法在计算效率和预测效果上。因此，本研究介绍了一种新颖的决策级融合方法来选择价格预测中的信息性变量。建议的元启发式算法平衡了两个竞争的目标函数，旨在改善利用变量进行预测的效果。

    arXiv:2403.20033v1 Announce Type: cross  Abstract: Price prediction algorithms propose prices for every product or service according to market trends, projected demand, and other characteristics, including government rules, international transactions, and speculation and expectation. As the dependent variable in price prediction, it is affected by several independent and correlated variables which may challenge the price prediction. To overcome this challenge, machine learning algorithms allow more accurate price prediction without explicitly modeling the relatedness between variables. However, as inputs increase, it challenges the existing machine learning approaches regarding computing efficiency and prediction effectiveness. Hence, this study introduces a novel decision level fusion approach to select informative variables in price prediction. The suggested metaheuristic algorithm balances two competitive objective functions, which are defined to improve the prediction utilized vari
    
[^44]: 无参数贝尔曼映射用于强化学习：在鲁棒自适应滤波中的应用

    Nonparametric Bellman Mappings for Reinforcement Learning: Application to Robust Adaptive Filtering

    [https://arxiv.org/abs/2403.20020](https://arxiv.org/abs/2403.20020)

    该论文设计了在再现核希尔伯特空间中的无参数贝尔曼映射，具有丰富的逼近特性，无需对数据的统计特性做任何假设，也不需要了解马尔可夫决策过程的转移概率，可以在没有训练数据的情况下运行，并支持多种操作和应用。

    

    本文针对强化学习设计了新颖的在再现核希尔伯特空间（RKHSs）中的无参数贝尔曼映射。所提出的映射利用RKHSs的丰富逼近特性，由于它们的无参数特性，不对数据的统计性质作任何假设，也不需要对马尔可夫决策过程的转移概率有任何了解，并且可以在没有任何训练数据的情况下操作。此外，它们允许通过设计轨迹样本进行即时抽样，通过经验重放重复使用过去的测试数据，通过随机傅立叶特征实现维度降低，并实现计算轻量级操作以适应高效的在线或时间自适应学习。本文还提供了一个变分框架来设计所提出的贝尔曼映射的自由参数，并展示适当选择这些参数可以产生几种流行的贝尔曼映射设计。作为应用，

    arXiv:2403.20020v1 Announce Type: cross  Abstract: This paper designs novel nonparametric Bellman mappings in reproducing kernel Hilbert spaces (RKHSs) for reinforcement learning (RL). The proposed mappings benefit from the rich approximating properties of RKHSs, adopt no assumptions on the statistics of the data owing to their nonparametric nature, require no knowledge on transition probabilities of Markov decision processes, and may operate without any training data. Moreover, they allow for sampling on-the-fly via the design of trajectory samples, re-use past test data via experience replay, effect dimensionality reduction by random Fourier features, and enable computationally lightweight operations to fit into efficient online or time-adaptive learning. The paper offers also a variational framework to design the free parameters of the proposed Bellman mappings, and shows that appropriate choices of those parameters yield several popular Bellman-mapping designs. As an application, t
    
[^45]: EnCoMP: 使用离线强化学习增强的隐蔽机动规划

    EnCoMP: Enhanced Covert Maneuver Planning using Offline Reinforcement Learning

    [https://arxiv.org/abs/2403.20016](https://arxiv.org/abs/2403.20016)

    提出了一种增强的导航系统，利用LiDAR数据生成高质量的掩护地图和潜在威胁地图，在复杂环境中通过离线强化学习训练模型，以最大化掩护利用、最小化暴露于威胁并高效到达目标。

    

    在复杂环境中进行隐蔽导航是自主机器人面临的关键挑战，需要识别和利用环境掩护同时保持有效导航。我们提出了一种增强的导航系统，使机器人能够识别和利用自然和人工环境特征作为掩护，从而最大程度地减少暴露于潜在威胁之下。我们的感知管道利用激光雷达数据生成高保真度的掩护地图和潜在威胁地图，提供对周围环境的全面理解。我们使用从实际环境中收集的多样化数据集训练一个离线强化学习模型，学习一个评估候选行动质量的强健策略，基于它们最大化掩护利用、最小化暴露于威胁和高效到达目标的能力。大量的实际实验证明了我们方法的优越性。

    arXiv:2403.20016v1 Announce Type: cross  Abstract: Cover navigation in complex environments is a critical challenge for autonomous robots, requiring the identification and utilization of environmental cover while maintaining efficient navigation. We propose an enhanced navigation system that enables robots to identify and utilize natural and artificial environmental features as cover, thereby minimizing exposure to potential threats. Our perception pipeline leverages LiDAR data to generate high-fidelity cover maps and potential threat maps, providing a comprehensive understanding of the surrounding environment. We train an offline reinforcement learning model using a diverse dataset collected from real-world environments, learning a robust policy that evaluates the quality of candidate actions based on their ability to maximize cover utilization, minimize exposure to threats, and reach the goal efficiently. Extensive real-world experiments demonstrate the superiority of our approach in
    
[^46]: 关于大型语言模型对已知事实的幻觉现象

    On Large Language Models' Hallucination with Regard to Known Facts

    [https://arxiv.org/abs/2403.20009](https://arxiv.org/abs/2403.20009)

    通过推理动态的角度研究大型语言模型对已知事实的幻觉现象，通过对事实性问题和输出 token 概率动态的分析，揭示了幻觉发生的模式。

    

    大型语言模型在回答事实类问题方面取得成功，但也容易出现幻觉。我们通过推理动态的角度研究LLMs具有正确答案知识却仍然产生幻觉的现象，这是以往关于幻觉研究尚未涵盖的领域。我们通过两个关键思路进行分析。首先，我们确定了查询相同三元知识但导致不同答案的事实性问题。模型在正确和不正确输出上的行为差异因此暗示了幻觉发生的模式。其次，为了衡量这种模式，我们利用了剩余流到词汇空间的映射。我们揭示了输出令牌概率在正确和幻觉情况下在层深度上的不同动态。在幻觉案例中，输出令牌的信息很少表现出突增和持续的情况。

    arXiv:2403.20009v1 Announce Type: new  Abstract: Large language models are successful in answering factoid questions but are also prone to hallucination.We investigate the phenomenon of LLMs possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics, an area not previously covered in studies on hallucinations.We are able to conduct this analysis via two key ideas.First, we identify the factual questions that query the same triplet knowledge but result in different answers. The difference between the model behaviors on the correct and incorrect outputs hence suggests the patterns when hallucinations happen. Second, to measure the pattern, we utilize mappings from the residual streams to vocabulary space. We reveal the different dynamics of the output token probabilities along the depths of layers between the correct and hallucinated cases. In hallucinated cases, the output token's information rarely demonstrates abrupt increases and consistent
    
[^47]: DeepHeteroIoT：基于异构IoT传感器数据的深度局部和全局学习

    DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data

    [https://arxiv.org/abs/2403.19996](https://arxiv.org/abs/2403.19996)

    提出了一个新颖的深度学习模型，结合了卷积神经网络和双向门控循环单元，以端到端的方式学习局部和全局特征，有效解决了异构IoT传感器数据分类的挑战。

    

    互联网物联网（IoT）传感器数据或读数在时间戳范围、采样频率、地理位置、测量单位等方面表现出差异。这种呈现的序列数据异质性使传统时间序列分类算法难以表现良好。因此，解决异质性挑战需要学习不仅子模式（局部特征）而且总体模式（全局特征）。为了解决分类异构IoT传感器数据（例如，将传感器数据类型如温度和湿度进行分类）的挑战，我们提出了一种新颖的深度学习模型，该模型融合了卷积神经网络和双向门控循环单元，分别以端到端的方式学习局部和全局特征。通过对异构IoT传感器数据集的严格实验，我们验证了我们提出的模型的有效性，该模型优于最近的最先进技术。

    arXiv:2403.19996v1 Announce Type: new  Abstract: Internet of Things (IoT) sensor data or readings evince variations in timestamp range, sampling frequency, geographical location, unit of measurement, etc. Such presented sequence data heterogeneity makes it difficult for traditional time series classification algorithms to perform well. Therefore, addressing the heterogeneity challenge demands learning not only the sub-patterns (local features) but also the overall pattern (global feature). To address the challenge of classifying heterogeneous IoT sensor data (e.g., categorizing sensor data types like temperature and humidity), we propose a novel deep learning model that incorporates both Convolutional Neural Network and Bi-directional Gated Recurrent Unit to learn local and global features respectively, in an end-to-end manner. Through rigorous experimentation on heterogeneous IoT sensor datasets, we validate the effectiveness of our proposed model, which outperforms recent state-of-th
    
[^48]: 语义转移增量适配器调整是一种持续的 ViTransformer

    Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer

    [https://arxiv.org/abs/2403.19979](https://arxiv.org/abs/2403.19979)

    适配器调整方法在持续学习中展现出较优性能，提出了增量调整共享适配器和利用存储原型进行特征采样和更新的方法来增强模型学习能力。

    

    类增量学习（CIL）旨在使模型能够在克服灾难性遗忘的同时持续学习新的类别。本文重新审视了在持续学习背景下的不同参数高效调整（PET）方法。我们观察到适配器调整表现优于基于提示的方法，甚至在每个学习会话中没有参数扩展的情况下也如此。受此启发，我们提出了增量调整共享适配器而不施加参数更新约束，增强骨干的学习能力。此外，我们从存储的原型中抽取特征样本来重新训练统一的分类器，进一步提高其性能。我们估计旧原型的语义转移，而无法访问过去的样本，并逐个会话更新存储的原型。我们提出的方法消除了模型的扩展和...

    arXiv:2403.19979v1 Announce Type: cross  Abstract: Class-incremental learning (CIL) aims to enable models to continuously learn new classes while overcoming catastrophic forgetting. The introduction of pre-trained models has brought new tuning paradigms to CIL. In this paper, we revisit different parameter-efficient tuning (PET) methods within the context of continual learning. We observe that adapter tuning demonstrates superiority over prompt-based methods, even without parameter expansion in each learning session. Motivated by this, we propose incrementally tuning the shared adapter without imposing parameter update constraints, enhancing the learning capacity of the backbone. Additionally, we employ feature sampling from stored prototypes to retrain a unified classifier, further improving its performance. We estimate the semantic shift of old prototypes without access to past samples and update stored prototypes session by session. Our proposed method eliminates model expansion and
    
[^49]: 用于计算机视觉任务的分离、动态和可微（SMART）剪枝器

    Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output Channel Pruning on Computer Vision Tasks

    [https://arxiv.org/abs/2403.19969](https://arxiv.org/abs/2403.19969)

    SMART剪枝器引入了独立的、可学习的概率掩码、可微的Top k运算符和动态温度参数技巧，在块和输出通道剪枝任务中显示出优越性。

    

    深度神经网络（DNN）剪枝已被视为减小模型大小、改善推理延迟以及降低DNN加速器功耗的关键策略。在各种剪枝技术中，块和输出通道剪枝在加速硬件性能方面展现出了显著潜力。然而，它们的准确性通常需要进一步改进。为了应对这一挑战，我们介绍了一种名为分离、动态和可微（SMART）剪枝器。该剪枝器利用一个独立的、可学习的概率掩码进行权重重要性排序，采用可微的Top k运算符来实现目标稀疏性，并利用动态温度参数技巧来逃离非稀疏局部极小值。在我们的实验中，SMART剪枝器在块和输出通道剪枝的各种任务和模型上一直表现出优越性。

    arXiv:2403.19969v1 Announce Type: cross  Abstract: Deep Neural Network (DNN) pruning has emerged as a key strategy to reduce model size, improve inference latency, and lower power consumption on DNN accelerators. Among various pruning techniques, block and output channel pruning have shown significant potential in accelerating hardware performance. However, their accuracy often requires further improvement. In response to this challenge, we introduce a separate, dynamic and differentiable (SMART) pruner. This pruner stands out by utilizing a separate, learnable probability mask for weight importance ranking, employing a differentiable Top k operator to achieve target sparsity, and leveraging a dynamic temperature parameter trick to escape from non-sparse local minima. In our experiments, the SMART pruner consistently demonstrated its superiority over existing pruning methods across a wide range of tasks and models on block and output channel pruning. Additionally, we extend our testing
    
[^50]: FairRAG: 公平人类生成的公平检索增强

    FairRAG: Fair Human Generation via Fair Retrieval Augmentation

    [https://arxiv.org/abs/2403.19964](https://arxiv.org/abs/2403.19964)

    FairRAG框架通过在外部图像数据库检索到的参考图像来提高人类生成中的公平性，并应用简单但有效的去偏策略，从而为生成过程提供来自不同人口统计组的图像。

    

    存在的文本到图像生成模型反映甚至放大了其训练数据中根深蒂固的社会偏见。这对人类图像生成尤为令人担忧，因为模型偏向某些人口统计组。现有的纠正此问题的尝试受到预训练模型固有限制的影响，并未能在根本上改善人口多样性。在这项工作中，我们引入了公平检索增强生成（FairRAG），这是一个新颖的框架，通过在来自外部图像数据库的参考图像上进行条件化来提高人类生成中的公平性。FairRAG通过一个轻量级线性模块实现条件化，将参考图像投射到文本空间中。为了增强公平性，FairRAG应用了简单而有效的去偏方法，在生成过程中提供来自不同人口统计组的图像。大量实验展示

    arXiv:2403.19964v1 Announce Type: cross  Abstract: Existing text-to-image generative models reflect or even amplify societal biases ingrained in their training data. This is especially concerning for human image generation where models are biased against certain demographic groups. Existing attempts to rectify this issue are hindered by the inherent limitations of the pre-trained models and fail to substantially improve demographic diversity. In this work, we introduce Fair Retrieval Augmented Generation (FairRAG), a novel framework that conditions pre-trained generative models on reference images retrieved from an external image database to improve fairness in human generation. FairRAG enables conditioning through a lightweight linear module that projects reference images into the textual space. To enhance fairness, FairRAG applies simple-yet-effective debiasing strategies, providing images from diverse demographic groups during the generative process. Extensive experiments demonstrat
    
[^51]: 通过调整和多支路推理增强低参数LLMs的通用代理功能

    Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning

    [https://arxiv.org/abs/2403.19962](https://arxiv.org/abs/2403.19962)

    通过构建特定于代理的数据并细调模型以及设计能够有效激活LLMs推理能力的提示，提出了一种综合方法来增强低参数LLMs的通用代理功能。

    

    arXiv:2403.19962v1 声明类型: 跨领域 摘要: 开源预训练的大型语言模型（LLM）表现出强大的语言理解和生成能力，使它们在各种任务中非常成功。然而，当将它们用作处理现实世界复杂问题的代理时，它们的性能远远不及ChatGPT和GPT-4等大型商用模型。作为智能代理，LLMs需要具备任务规划、长期记忆以及利用外部工具实现令人满意的性能的能力。各种方法已被提出来增强LLMs的代理能力。一方面，有些方法涉及构建特定于代理的数据和微调模型。另一方面，一些方法集中于设计能有效激活LLMs推理能力的提示。我们在7B和13B模型上同时探讨了这两种策略。我们提出了一种使用GPT-4构建特定于代理数据的全面方法。

    arXiv:2403.19962v1 Announce Type: cross  Abstract: Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. T
    
[^52]: 保证覆盖率的预测集合用于OOD数据

    Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data

    [https://arxiv.org/abs/2403.19950](https://arxiv.org/abs/2403.19950)

    研究了OOD泛化设置中的置信集预测问题，提出了一种方法来形成有信心的预测集，并在理论上证明了方法的有效性

    

    异分布（OOD）泛化近年来在真实世界应用中表现出有希望的实验结果，因此越来越受到研究关注。本文研究了OOD泛化设置中的置信集预测问题。 分割符合性预测（SCP）是处理置信集预测问题的有效框架。我们展示了在看不见的目标领域与源领域不同时，简单应用SCP导致无法维持边际覆盖的失败。为解决这个问题，我们开发了一种在OOD设置中形成有信心的预测集的方法，并在理论上证明了我们方法的有效性。最后，我们在模拟数据上进行实验证明了我们理论的正确性和我们提出方法的有效性。

    arXiv:2403.19950v1 Announce Type: new  Abstract: Out-of-distribution (OOD) generalization has attracted increasing research attention in recent years, due to its promising experimental results in real-world applications. In this paper,we study the confidence set prediction problem in the OOD generalization setting. Split conformal prediction (SCP) is an efficient framework for handling the confidence set prediction problem. However, the validity of SCP requires the examples to be exchangeable, which is violated in the OOD setting. Empirically, we show that trivially applying SCP results in a failure to maintain the marginal coverage when the unseen target domain is different from the source domain. To address this issue, we develop a method for forming confident prediction sets in the OOD setting and theoretically prove the validity of our method. Finally, we conduct experiments on simulated data to empirically verify the correctness of our theory and the validity of our proposed metho
    
[^53]: TDANet：一种新颖的具有注意力机制的时间去噪卷积神经网络在故障诊断中的应用

    TDANet: A Novel Temporal Denoise Convolutional Neural Network With Attention for Fault Diagnosis

    [https://arxiv.org/abs/2403.19943](https://arxiv.org/abs/2403.19943)

    TDANet是一种新颖的具有注意力机制的时间去噪卷积神经网络，旨在改善噪声环境下的故障诊断性能。

    

    故障诊断在维护机械系统的运行完整性，避免由于意外故障造成的重大损失方面起着至关重要的作用。本文提出了一种名为TDANet的时间去噪卷积神经网络，旨在改善噪声环境下的故障诊断性能。该模型基于信号的周期性质将一维信号转换为二维张量，采用多尺度2D卷积核从周期内部和跨周期提取信号信息。这种方法能有效地识别信号特性。

    arXiv:2403.19943v1 Announce Type: cross  Abstract: Fault diagnosis plays a crucial role in maintaining the operational integrity of mechanical systems, preventing significant losses due to unexpected failures. As intelligent manufacturing and data-driven approaches evolve, Deep Learning (DL) has emerged as a pivotal technique in fault diagnosis research, recognized for its ability to autonomously extract complex features. However, the practical application of current fault diagnosis methods is challenged by the complexity of industrial environments. This paper proposed the Temporal Denoise Convolutional Neural Network With Attention (TDANet), designed to improve fault diagnosis performance in noise environments. This model transforms one-dimensional signals into two-dimensional tensors based on their periodic properties, employing multi-scale 2D convolution kernels to extract signal information both within and across periods. This method enables effective identification of signal chara
    
[^54]: DiJiang：通过紧凑的核方法实现高效的大型语言模型

    DiJiang: Efficient Large Language Models through Compact Kernelization

    [https://arxiv.org/abs/2403.19928](https://arxiv.org/abs/2403.19928)

    DiJiang提出了一种新颖的频域核方法，可以将预训练的基本Transformer模型转化为具有线性复杂度的模型，大大减少训练成本，并在理论上提供更好的逼近效率。

    

    为了减少Transformers的计算负荷，线性注意力的研究已经取得了显著的进展。然而，注意机制的改进策略通常需要经过大量的重新训练，在具有大量参数的大型语言模型上是不切实际的。本文介绍了DiJiang，一种新颖的频域核方法，可将预训练的基本Transformer转化为具有较小训练成本的线性复杂度模型。通过采用加权拟随机采样法，所提出的方法在理论上提供了更好的逼近效率。为了进一步降低训练的计算复杂度，我们的核方法基于离散余弦变换（DCT）操作。大量实验证明，所提出的方法达到了与原始Transformer相当的性能，但训练时间大大减少。

    arXiv:2403.19928v1 Announce Type: new  Abstract: In an effort to reduce the computational load of Transformers, research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original Transformer, but with significantly reduced trainin
    
[^55]: 决策巨蟒：通过选择性状态空间进行序列建模的强化学习

    Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces

    [https://arxiv.org/abs/2403.19925](https://arxiv.org/abs/2403.19925)

    本研究将Mamba框架整合到Decision Transformer架构中，提出了Decision Mamba，通过在不同决策环境中进行一系列实验，表明了神经网络的架构和训练方法对性能的重要影响

    

    Decision Transformer是一种将Transformer架构应用于强化学习的有前途的方法，它依赖因果自注意力来模拟状态、动作和奖励序列。本文研究了Mamba框架的整合，该框架以有效和高效的序列建模能力而闻名，将其整合到Decision Transformer架构中，关注在顺序决策任务中潜在的性能增强。我们通过在各种决策环境中进行一系列实验来系统评估这种整合，将修改后的Decision Transformer，Decision Mamba，与传统对应物进行比较。这项工作促进了顺序决策模型的发展，表明神经网络的架构和训练方法可以显著影响其性能

    arXiv:2403.19925v1 Announce Type: cross  Abstract: Decision Transformer, a promising approach that applies Transformer architectures to reinforcement learning, relies on causal self-attention to model sequences of states, actions, and rewards. While this method has shown competitive results, this paper investigates the integration of the Mamba framework, known for its advanced capabilities in efficient and effective sequence modeling, into the Decision Transformer architecture, focusing on the potential performance enhancements in sequential decision-making tasks. Our study systematically evaluates this integration by conducting a series of experiments across various decision-making environments, comparing the modified Decision Transformer, Decision Mamba, with its traditional counterpart. This work contributes to the advancement of sequential decision-making models, suggesting that the architecture and training methodology of neural networks can significantly impact their performance 
    
[^56]: CtRL-Sim：使用离线强化学习的反应性可控驾驶代理

    CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning

    [https://arxiv.org/abs/2403.19918](https://arxiv.org/abs/2403.19918)

    CtRL-Sim提出了一种利用离线强化学习生成反应性和可控交通代理的方法，通过在Nocturne模拟器中处理真实世界的驾驶数据来实现这一目标。

    

    在这项工作中，我们提出了CtRL-Sim，一种利用物理增强的Nocturne模拟器中的回报条件化离线强化学习来高效生成反应性和可控交通代理的方法。具体来说，我们通过Nocturne模拟器处理真实世界的驾驶数据，以生成多样化的离线数据。

    arXiv:2403.19918v1 Announce Type: cross  Abstract: Evaluating autonomous vehicle stacks (AVs) in simulation typically involves replaying driving logs from real-world recorded traffic. However, agents replayed from offline data do not react to the actions of the AV, and their behaviour cannot be easily controlled to simulate counterfactual scenarios. Existing approaches have attempted to address these shortcomings by proposing methods that rely on heuristics or learned generative models of real-world data but these approaches either lack realism or necessitate costly iterative sampling procedures to control the generated behaviours. In this work, we take an alternative approach and propose CtRL-Sim, a method that leverages return-conditioned offline reinforcement learning within a physics-enhanced Nocturne simulator to efficiently generate reactive and controllable traffic agents. Specifically, we process real-world driving data through the Nocturne simulator to generate a diverse offli
    
[^57]: MANGO：用于评估大型语言模型映射和导航能力的基准

    MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models

    [https://arxiv.org/abs/2403.19913](https://arxiv.org/abs/2403.19913)

    提出了用于评估大型语言模型执行文本映射和导航能力的MANGO基准，发现即使是迄今为止最好的语言模型GPT-4在回答涉及映射和导航的问题时表现不佳。

    

    如ChatGPT和GPT-4等大型语言模型最近在各种自然语言处理任务上取得了惊人的性能。本文提出了MANGO，这是一个用于评估它们执行基于文本映射和导航能力的基准。我们的基准包括来自一套文本游戏的53个迷宫：每个迷宫都与一个游览说明配对，其中包含每个位置的访问但不涵盖所有可能的路径。任务是问答：对于每个迷宫，大型语言模型读取游览说明并回答数百个映射和导航问题，例如“你应该从房子西部如何去阁楼？”和“如果我们从地下室向北和东走，我们会在哪里？”。尽管这些问题对人类来说很容易，但事实证明，迄今为止最好的语言模型GPT-4甚至在回答这些问题时表现不佳。此外，我们的实验表明，强大的映射和导航能力将有利于大型语言模型。

    arXiv:2403.19913v1 Announce Type: cross  Abstract: Large language models such as ChatGPT and GPT-4 have recently achieved astonishing performance on a variety of natural language processing tasks. In this paper, we propose MANGO, a benchmark to evaluate their capabilities to perform text-based mapping and navigation. Our benchmark includes 53 mazes taken from a suite of textgames: each maze is paired with a walkthrough that visits every location but does not cover all possible paths. The task is question-answering: for each maze, a large language model reads the walkthrough and answers hundreds of mapping and navigation questions such as "How should you go to Attic from West of House?" and "Where are we if we go north and east from Cellar?". Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them. Further, our experiments suggest that a strong mapping and navigation ability would benefit large languag
    
[^58]: 超越已知：开放世界图学习中的新类别发现

    Beyond the Known: Novel Class Discovery for Open-world Graph Learning

    [https://arxiv.org/abs/2403.19907](https://arxiv.org/abs/2403.19907)

    本文提出了一种名为ORAL的新方法，用于在开放世界图学习中发现新类别，通过半监督原型学习和原型注意力网络解决了新类别和已知类别节点之间的相关性问题。

    

    图上的节点分类在许多应用中至关重要。由于现实世界开放场景中标记能力有限且发展迅速，未标记测试节点上可能会出现新类别。然而，目前对图上的新类别发现关注较少。由于图中的新类别和已知类别节点由边相关联，因此当应用消息传递GNN时它们的表示是不可区分的。此外，新类别缺乏标签信息来指导学习过程。本文提出了一种新方法Open-world gRAph neuraL network (ORAL)来解决这些挑战。ORAL首先通过半监督原型学习检测类别之间的相关性。然后通过原型注意力网络消除类间相关性，从而为不同类别提供独特的表示。此外，为了充分探索mu

    arXiv:2403.19907v1 Announce Type: cross  Abstract: Node classification on graphs is of great importance in many applications. Due to the limited labeling capability and evolution in real-world open scenarios, novel classes can emerge on unlabeled testing nodes. However, little attention has been paid to novel class discovery on graphs. Discovering novel classes is challenging as novel and known class nodes are correlated by edges, which makes their representations indistinguishable when applying message passing GNNs. Furthermore, the novel classes lack labeling information to guide the learning process. In this paper, we propose a novel method Open-world gRAph neuraL network (ORAL) to tackle these challenges. ORAL first detects correlations between classes through semi-supervised prototypical learning. Inter-class correlations are subsequently eliminated by the prototypical attention network, leading to distinctive representations for different classes. Furthermore, to fully explore mu
    
[^59]: 分离种族表型：对种族相关面部表型特征进行细粒度控制

    Disentangling Racial Phenotypes: Fine-Grained Control of Race-related Facial Phenotype Characteristics

    [https://arxiv.org/abs/2403.19897](https://arxiv.org/abs/2403.19897)

    本文提出了一种新颖的GAN框架，能够实现对面部图像个体种族相关表型属性的细粒度控制，从而分离种族相关的表型特征，为数据驱动的种族偏见缓解策略提供关键支持。

    

    通过实现对2D面部图像的有效的细粒度外观变化，同时保留面部身份，是一项具有挑战性的任务，因为常见的2D面部特征编码空间具有很高的复杂性和纠缠性。尽管存在这些挑战，通过分解来实现这种细粒度控制是数据驱动的种族偏见缓解策略的关键因素，因为它允许分析、表征和合成人类面孔的多样性。本文提出了一个新颖的GAN框架，以实现对面部图像个体种族相关表型属性的细粒度控制。我们的框架将潜在（特征）空间分解为相应于种族相关面部表型表示的元素，从而分离表型方面（例如皮肤、头发颜色、鼻子、眼睛、嘴巴形状），这在真实世界的面部数据中是极其难以鲁棒地标注的。

    arXiv:2403.19897v1 Announce Type: cross  Abstract: Achieving an effective fine-grained appearance variation over 2D facial images, whilst preserving facial identity, is a challenging task due to the high complexity and entanglement of common 2D facial feature encoding spaces. Despite these challenges, such fine-grained control, by way of disentanglement is a crucial enabler for data-driven racial bias mitigation strategies across multiple automated facial analysis tasks, as it allows to analyse, characterise and synthesise human facial diversity. In this paper, we propose a novel GAN framework to enable fine-grained control over individual race-related phenotype attributes of the facial images. Our framework factors the latent (feature) space into elements that correspond to race-related facial phenotype representations, thereby separating phenotype aspects (e.g. skin, hair colour, nose, eye, mouth shapes), which are notoriously difficult to annotate robustly in real-world facial data.
    
[^60]: 非线性增强自适应激活函数

    Nonlinearity Enhanced Adaptive Activation Function

    [https://arxiv.org/abs/2403.19896](https://arxiv.org/abs/2403.19896)

    引入了一种带有甚至立方非线性的简单实现激活函数，通过引入可优化参数使得激活函数具有更大的自由度，可以提高神经网络的准确性，同时不需要太多额外的计算资源。

    

    引入一种简单实现的激活函数，具有甚至立方非线性，可以提高神经网络的准确性，而不需要太多额外的计算资源。通过一种明显的收敛与准确性之间的权衡来实现。该激活函数通过引入可优化参数来泛化标准RELU函数，从而增加了额外的自由度，使得非线性程度可以被调整。通过与标准技术进行比较，将在MNIST数字数据集的背景下量化相关准确性的提升。

    arXiv:2403.19896v1 Announce Type: new  Abstract: A simply implemented activation function with even cubic nonlinearity is introduced that increases the accuracy of neural networks without substantial additional computational resources. This is partially enabled through an apparent tradeoff between convergence and accuracy. The activation function generalizes the standard RELU function by introducing additional degrees of freedom through optimizable parameters that enable the degree of nonlinearity to be adjusted. The associated accuracy enhancement is quantified in the context of the MNIST digit data set through a comparison with standard techniques.
    
[^61]: 一种信息论框架用于超出分布泛化

    An Information-Theoretic Framework for Out-of-Distribution Generalization

    [https://arxiv.org/abs/2403.19895](https://arxiv.org/abs/2403.19895)

    提出了一个信息论框架用于机器学习中的超出分布泛化，可以自由插值并产生新的泛化界限，同时具有最优输运解释。

    

    我们研究了机器学习中的超出分布（OOD）泛化，并提出了一个通用框架，提供了信息论泛化界限。我们的框架在Integral Probability Metric（IPM）和$f$-divergence之间自由插值，自然地恢复了一些已知结果（包括Wasserstein和KL-bound），并产生了新的泛化界限。此外，我们展示了我们的框架具有最优输运解释。在两个具体示例中评估时，所提出的界限在某些情况下严格改进了现有界限，或者恢复了现有OOD泛化界限中的最佳者。

    arXiv:2403.19895v1 Announce Type: cross  Abstract: We study the Out-of-Distribution (OOD) generalization in machine learning and propose a general framework that provides information-theoretic generalization bounds. Our framework interpolates freely between Integral Probability Metric (IPM) and $f$-divergence, which naturally recovers some known results (including Wasserstein- and KL-bounds), as well as yields new generalization bounds. Moreover, we show that our framework admits an optimal transport interpretation. When evaluated in two concrete examples, the proposed bounds either strictly improve upon existing bounds in some cases or recover the best among existing OOD generalization bounds.
    
[^62]: 朝向一个强大的基于检索的摘要系统

    Towards a Robust Retrieval-Based Summarization System

    [https://arxiv.org/abs/2403.19889](https://arxiv.org/abs/2403.19889)

    该论文对大型语言模型在检索增强生成-基础摘要任务中的健壮性进行了调查，并提出了一个创新的评估框架和一个全面的系统来增强模型在特定场景下的健壮性。

    

    本文描述了对大型语言模型（LLMs）在检索增强生成（RAG）-基础摘要任务中的健壮性进行的调查。虽然LLMs提供了摘要能力，但它们在复杂的实际场景中的表现仍未得到充分探讨。我们的第一个贡献是LogicSumm，这是一个创新的评估框架，结合了现实场景，用来评估LLMs在RAG基础摘要过程中的健壮性。根据LogicSumm识别出的局限性，我们开发了SummRAG，这是一个全面的系统，用于创建训练对话并微调模型，以增强在LogicSumm场景中的健壮性。SummRAG是我们定义结构化方法来测试LLM能力的目标的一个示例，而不是一劳永逸地解决问题。实验结果证实了SummRAG的强大，展示了逻辑连贯性和摘要质量的提升。

    arXiv:2403.19889v1 Announce Type: cross  Abstract: This paper describes an investigation of the robustness of large language models (LLMs) for retrieval augmented generation (RAG)-based summarization tasks. While LLMs provide summarization capabilities, their performance in complex, real-world scenarios remains under-explored. Our first contribution is LogicSumm, an innovative evaluation framework incorporating realistic scenarios to assess LLM robustness during RAG-based summarization. Based on limitations identified by LogiSumm, we then developed SummRAG, a comprehensive system to create training dialogues and fine-tune a model to enhance robustness within LogicSumm's scenarios. SummRAG is an example of our goal of defining structured methods to test the capabilities of an LLM, rather than addressing issues in a one-off fashion. Experimental results confirm the power of SummRAG, showcasing improved logical coherence and summarization quality. Data, corresponding model weights, and Py
    
[^63]: MambaMixer：具有双重标记和通道选择的高效选择性状态空间模型

    MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection

    [https://arxiv.org/abs/2403.19888](https://arxiv.org/abs/2403.19888)

    MambaMixer是一种新的架构，提出了具有数据依赖权重的双重选择机制，称为选择性标记和通道混合器，对长序列建模具有潜在优势。

    

    深度学习的最新进展主要依赖于Transformers，因为它们具有数据依赖性并且能够实现大规模学习。然而，这些架构中的注意力模块展现出输入大小的二次时间和空间，限制了它们用于长序列建模的可扩展性。尽管最近有尝试为多维数据设计高效有效的架构主干，例如图像和多变量时间序列，但现有模型要么是数据独立的，要么无法允许跨维度和内部维度之间的通信。最近，状态空间模型（SSMs），尤其是具有高效硬件感知实现的选择性状态空间模型，展现出了用于长序列建模的潜在优势。受到SSMs成功的启发，我们提出了MambaMixer，一种新的具有数据依赖权重的架构，使用跨标记和通道的双重选择机制，称为选择性标记和通道混合器。

    arXiv:2403.19888v1 Announce Type: cross  Abstract: Recent advances in deep learning have mainly relied on Transformers due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models, with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the success of SSMs, we present MambaMixer, a new architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels, called Selective Token and Channel Mixer. M
    
[^64]: Jamba: 一个混合Transformer-Mamba语言模型

    Jamba: A Hybrid Transformer-Mamba Language Model

    [https://arxiv.org/abs/2403.19887](https://arxiv.org/abs/2403.19887)

    Jamba是一个基于混合Transformer-Mamba架构的语言模型，在单个80GB GPU上实现了强大的性能，对标准语言模型基准和长上下文评估具有state-of-the-art的表现。

    

    我们提出了Jamba，这是一个基于新颖的混合Transformer-Mamba混合专家(MoE)架构的新基础大型语言模型。具体来说，Jamba交错使用Transformer和Mamba层，从两种模型家族中获益。MoE被添加在其中一些层中，以增加模型容量，同时保持活跃参数的可控性。这种灵活的架构允许特定资源和目标的配置。

    arXiv:2403.19887v1 Announce Type: new  Abstract: We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transfor
    
[^65]: 提升视觉Transformer网络效率的设计技术和见解

    Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights

    [https://arxiv.org/abs/2403.19882](https://arxiv.org/abs/2403.19882)

    本文综述了Vision Transformer（ViT）网络中重新设计的注意力机制的技术和见解，旨在提升效率。

    

    受到人类视觉系统在复杂场景中识别显著区域的固有能力的启发，注意力机制已经无缝地整合到各种计算机视觉（CV）任务中。构建在这一范式之上，Vision Transformer（ViT）网络利用注意力机制提高效率。本综述探讨了ViTs中重新设计的注意力机制的领域，旨在提升它们的性能。本文全面探讨了设计注意力机制的技术和见解，系统地审查了计算机视觉领域的最新文献。该调查从介绍注意力机制的理论基础和基本概念开始。我们随后提出了ViTs内各种注意力机制的系统分类法，采用了重新设计的方法。基于它们的应用、目标和

    arXiv:2403.19882v1 Announce Type: cross  Abstract: Intrigued by the inherent ability of the human visual system to identify salient regions in complex scenes, attention mechanisms have been seamlessly integrated into various Computer Vision (CV) tasks. Building upon this paradigm, Vision Transformer (ViT) networks exploit attention mechanisms for improved efficiency. This review navigates the landscape of redesigned attention mechanisms within ViTs, aiming to enhance their performance. This paper provides a comprehensive exploration of techniques and insights for designing attention mechanisms, systematically reviewing recent literature in the field of CV. This survey begins with an introduction to the theoretical foundations and fundamental concepts underlying attention mechanisms. We then present a systematic taxonomy of various attention mechanisms within ViTs, employing redesigned approaches. A multi-perspective categorization is proposed based on their application, objectives, and
    
[^66]: 通过缓慢变化的序列实现稳定的机器学习模型重新训练

    Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences

    [https://arxiv.org/abs/2403.19871](https://arxiv.org/abs/2403.19871)

    通过混合整数优化算法，以保持一致的分析洞见为重点，在重新训练机器学习模型中实现比贪婪训练更强稳定性，同时在模型性能上有小幅、可控的牺牲。

    

    重新训练机器学习模型仍然是实际机器学习模型部署的重要任务。现有方法主要关注贪婪方法，以找到表现最佳的模型，而不考虑通过不同的重新训练演变来保持训练模型结构的稳定性。在这项研究中，我们开发了一种混合整数优化算法，全面考虑了通过不同的数据批次更新重新训练机器学习模型的问题。我们的方法侧重于保留一致的分析洞见 - 这对于模型可解释性、实施简易性和与用户建立信任至关重要 - 通过使用可以直接纳入优化问题的自定义定义的距离度量。重要的是，我们的方法在真实的生产案例研究中表现出比贪婪训练模型更强的稳定性，同时在模型性能上有小幅、可控的牺牲。

    arXiv:2403.19871v1 Announce Type: cross  Abstract: Retraining machine learning models remains an important task for real-world machine learning model deployment. Existing methods focus largely on greedy approaches to find the best-performing model without considering the stability of trained model structures across different retraining evolutions. In this study, we develop a mixed integer optimization algorithm that holistically considers the problem of retraining machine learning models across different data batch updates. Our method focuses on retaining consistent analytical insights - which is important to model interpretability, ease of implementation, and fostering trust with users - by using custom-defined distance metrics that can be directly incorporated into the optimization problem. Importantly, our method shows stronger stability than greedily trained models with a small, controllable sacrifice in model performance in a real-world production case study. Finally, important an
    
[^67]: 在流式和大规模并行模型中找到决策树分割点

    Finding Decision Tree Splits in Streaming and Massively Parallel Models

    [https://arxiv.org/abs/2403.19867](https://arxiv.org/abs/2403.19867)

    提出了在数据流学习中计算决策树最佳分割点的算法，能够在流式计算和大规模并行模型中高效运行

    

    在这项工作中，我们提出了一种数据流算法，用于计算决策树学习中的最优分割点。具体而言，给定观测数据流$x_i$及其标签$y_i$，目标是找到将数据分为两组的最佳分割点$j$，使得均方误差（回归问题）或误分类率（分类问题）最小化。我们提供了多种快速的数据流算法，这些算法在这些问题中使用亚线性空间和少量次数的遍历。这些算法还可以扩展到大规模并行计算模型中。尽管不能直接比较，但我们的工作与Domingos和Hulten的开创性工作（KDD 2000）相互补充。

    arXiv:2403.19867v1 Announce Type: cross  Abstract: In this work, we provide data stream algorithms that compute optimal splits in decision tree learning. In particular, given a data stream of observations $x_i$ and their labels $y_i$, the goal is to find the optimal split point $j$ that divides the data into two sets such that the mean squared error (for regression) or misclassification rate (for classification) is minimized. We provide various fast streaming algorithms that use sublinear space and a small number of passes for these problems. These algorithms can also be extended to the massively parallel computation model. Our work, while not directly comparable, complements the seminal work of Domingos and Hulten (KDD 2000).
    
[^68]: DeNetDM: 通过网络深度调制来消除偏见

    DeNetDM: Debiasing by Network Depth Modulation

    [https://arxiv.org/abs/2403.19863](https://arxiv.org/abs/2403.19863)

    DeNetDM 是一种基于网络深度调制的新型去偏见方法，通过使用来自专家乘积的训练范式，在创建深浅架构的偏见和去偏见分支后，将知识提炼产生目标去偏见模型，相比当前去偏见技术取得更优异的效果。

    

    当神经网络在偏见数据集上训练时，它们往往会无意间学习到虚假的相关性，从而导致在实现强大的泛化性和鲁棒性方面面临挑战。目前解决这种偏见的方法通常包括利用偏见注释、根据伪偏见标签进行加权重、或通过增强技术增加偏见冲突数据点的多样性。我们引入了DeNetDM，这是一种基于观察结果的新型去偏见方法，浅层神经网络优先学习核心属性，而更深层次的神经网络在获取不同信息时强调偏见。我们利用从专家乘积中推导出的训练范式，创建了深浅架构的偏见和去偏见分支，然后用知识提炼产生目标的去偏见模型。大量实验证明，我们的方法优于当前的去偏见技术，实现了一个...

    arXiv:2403.19863v1 Announce Type: new  Abstract: When neural networks are trained on biased datasets, they tend to inadvertently learn spurious correlations, leading to challenges in achieving strong generalization and robustness. Current approaches to address such biases typically involve utilizing bias annotations, reweighting based on pseudo-bias labels, or enhancing diversity within bias-conflicting data points through augmentation techniques. We introduce DeNetDM, a novel debiasing method based on the observation that shallow neural networks prioritize learning core attributes, while deeper ones emphasize biases when tasked with acquiring distinct information. Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then distill knowledge to produce the target debiased model. Extensive experiments and analyses demonstrate that our approach outperforms current debiasing techniques, achieving a not
    
[^69]: 流行病建模中图神经网络的综述

    A Review of Graph Neural Networks in Epidemic Modeling

    [https://arxiv.org/abs/2403.19852](https://arxiv.org/abs/2403.19852)

    图神经网络在流行病建模中作为一种新工具备受关注，本文全面回顾了GNN在流行病研究中的应用，并提出了未来发展方向。

    

    自新冠疫情爆发以来，人们对流行病学模型的研究越来越感兴趣。传统的机械模型数学描述了传染病的传播机制，但在面对当今不断增长的挑战时往往力不从心。因此，图神经网络（GNNs）已经成为流行病研究中越来越流行的工具。本文试图全面回顾GNN在流行病任务中的应用，并强调潜在的未来发展方向。为实现这一目标，我们为流行病任务和方法论各引入了分层分类法，为该领域内的发展轨迹提供了一个框架。对于流行病任务，我们建立了一个类似于流行病领域通常应用的分类体系。对于方法论，我们将现有研究分为“神经模型”和“混合模型”。

    arXiv:2403.19852v1 Announce Type: new  Abstract: Since the onset of the COVID-19 pandemic, there has been a growing interest in studying epidemiological models. Traditional mechanistic models mathematically describe the transmission mechanisms of infectious diseases. However, they often fall short when confronted with the growing challenges of today. Consequently, Graph Neural Networks (GNNs) have emerged as a progressively popular tool in epidemic research. In this paper, we endeavor to furnish a comprehensive review of GNNs in epidemic tasks and highlight potential future directions. To accomplish this objective, we introduce hierarchical taxonomies for both epidemic tasks and methodologies, offering a trajectory of development within this domain. For epidemic tasks, we establish a taxonomy akin to those typically employed within the epidemic domain. For methodology, we categorize existing work into \textit{Neural Models} and \textit{Hybrid Models}. Following this, we perform an exha
    
[^70]: 将语言模型中的段落记忆本地化

    Localizing Paragraph Memorization in Language Models

    [https://arxiv.org/abs/2403.19851](https://arxiv.org/abs/2403.19851)

    论文展示了语言模型中段落记忆的梯度具有可区分的空间模式，通过微调高梯度权重可以取消学习，定位了特别参与段落记忆的低层注意头，并研究了记忆在前缀中的本地化程度。

    

    我们能否将语言模型用于记忆和背诵整个训练数据段的权重和机制本地化？本文表明，虽然记忆分布在多个层次和模型组件中，但记忆段落的梯度具有可区分的空间模式，较低模型层次中的梯度比非记忆示例的梯度更大。此外，这些记忆示例可以通过仅微调高梯度权重来取消学习。我们定位了一个似乎特别参与段落记忆的低层注意头。这个头部主要将注意力集中在在语料库级别的单语分布中最不频繁的独特、罕见的令牌上。接下来，我们通过扰动令牌并测量对解码造成的改变来研究记忆在前缀中的本地化程度。前缀中的一些独特令牌经常会使整个内容受损。

    arXiv:2403.19851v1 Announce Type: new  Abstract: Can we localize the weights and mechanisms used by a language model to memorize and recite entire paragraphs of its training data? In this paper, we show that while memorization is spread across multiple layers and model components, gradients of memorized paragraphs have a distinguishable spatial pattern, being larger in lower model layers than gradients of non-memorized examples. Moreover, the memorized examples can be unlearned by fine-tuning only the high-gradient weights. We localize a low-layer attention head that appears to be especially involved in paragraph memorization. This head is predominantly focusing its attention on distinctive, rare tokens that are least frequent in a corpus-level unigram distribution. Next, we study how localized memorization is across the tokens in the prefix by perturbing tokens and measuring the caused change in the decoding. A few distinctive tokens early in a prefix can often corrupt the entire cont
    
[^71]: 在无线异构环境下的有偏空中联邦学习

    Biased Over-the-Air Federated Learning under Wireless Heterogeneity

    [https://arxiv.org/abs/2403.19849](https://arxiv.org/abs/2403.19849)

    研究了在无线异构环境下的有偏Over-the-Air联邦学习，提出了优化偏差和方差之间权衡的方法

    

    最近，基于OTA（Over-the-Air）计算的联邦学习（FL）范式崭露头角，利用无线信道的波形叠加特性实现快速模型更新。之前的研究主要集中在\emph{同质}无线条件下的OTA设备“预缩放器”设计，即设备经历相同的平均路径损耗，导致零偏差解决方案。然而，零偏差设计受到路径损耗最严重的设备的限制，因此在\emph{异构}无线环境中可能表现不佳。在这种情况下，设计\emph{有偏}解决方案可能有益，以换取模型更新的方差降低。为了优化这种权衡，我们研究了OTA设备预缩放器的设计，重点关注OTA-FL的收敛性。我们推导了模型“最优性误差”的上界，明确捕捉了偏差和方差对于选择的影响

    arXiv:2403.19849v1 Announce Type: new  Abstract: Recently, Over-the-Air (OTA) computation has emerged as a promising federated learning (FL) paradigm that leverages the waveform superposition properties of the wireless channel to realize fast model updates. Prior work focused on the OTA device ``pre-scaler" design under \emph{homogeneous} wireless conditions, in which devices experience the same average path loss, resulting in zero-bias solutions. Yet, zero-bias designs are limited by the device with the worst average path loss and hence may perform poorly in \emph{heterogeneous} wireless settings. In this scenario, there may be a benefit in designing \emph{biased} solutions, in exchange for a lower variance in the model updates. To optimize this trade-off, we study the design of OTA device pre-scalers by focusing on the OTA-FL convergence. We derive an upper bound on the model ``optimality error", which explicitly captures the effect of bias and variance in terms of the choice of the 
    
[^72]: 广义梯度下降是一个超图函子

    Generalized Gradient Descent is a Hypergraph Functor

    [https://arxiv.org/abs/2403.19845](https://arxiv.org/abs/2403.19845)

    广义梯度下降相对于Cartesian reverse derivative categories (CRDCs)的通用客观函数诱导出一个超图函子，将优化问题映射到动力系统，为分布式优化算法提供了新途径。

    

    Cartesian reverse derivative categories (CRDCs)提供了对反向导数的公理化泛化，这使得可以将相对于广泛类问题的梯度下降的广义类比应用于经典优化算法。本文展示了相对于给定CRDC的广义梯度下降诱导出一个从优化问题的超图范畴到动力系统的超图函子。该函子的定义域由客观函数组成，这些客观函数在任意CRDC下都是通用的，并且是开放的，可以通过变量共享与其他这样的客观函数组合。对映域类似地被指定为基础CRDC的通用和开放动态系统类别。我们描述了超图函子如何诱导出一个针对任意问题的分布式优化算法。

    arXiv:2403.19845v1 Announce Type: cross  Abstract: Cartesian reverse derivative categories (CRDCs) provide an axiomatic generalization of the reverse derivative, which allows generalized analogues of classic optimization algorithms such as gradient descent to be applied to a broad class of problems. In this paper, we show that generalized gradient descent with respect to a given CRDC induces a hypergraph functor from a hypergraph category of optimization problems to a hypergraph category of dynamical systems. The domain of this functor consists of objective functions that are 1) general in the sense that they are defined with respect to an arbitrary CRDC, and 2) open in that they are decorated spans that can be composed with other such objective functions via variable sharing. The codomain is specified analogously as a category of general and open dynamical systems for the underlying CRDC. We describe how the hypergraph functor induces a distributed optimization algorithm for arbitrary
    
[^73]: 使用k-mers和基于片段的指纹扩展化学表示进行分子指纹

    Expanding Chemical Representation with k-mers and Fragment-based Fingerprints for Molecular Fingerprinting

    [https://arxiv.org/abs/2403.19844](https://arxiv.org/abs/2403.19844)

    该方法通过结合亚结构计数、k-mers和Daylight-like指纹，扩展了化学结构的表示，提升了分子指纹的信息内容和区分能力，在化学信息学任务中表现出优越性，为分子相似性分析和化学结构设计提供更加信息丰富的表示。

    

    该研究引入了一种新颖的方法，将亚结构计数、k-mers和类似Daylight的指纹结合在一起，以扩展SMILES字符串中化学结构的表示。集成方法生成了增强区分能力和信息内容的综合分子嵌入。实验评估证明了其优于传统Morgan指纹、MACCS和单独Daylight指纹的方法，改善了药物分类等化学信息学任务。所提出的方法提供了更具信息的化学结构表示，推进了分子相似性分析，并促进了在分子设计和药物发现中的应用。它为分子结构分析和设计提供了一个有前景的途径，具有实际实施的重要潜力。

    arXiv:2403.19844v1 Announce Type: cross  Abstract: This study introduces a novel approach, combining substruct counting, $k$-mers, and Daylight-like fingerprints, to expand the representation of chemical structures in SMILES strings. The integrated method generates comprehensive molecular embeddings that enhance discriminative power and information content. Experimental evaluations demonstrate its superiority over traditional Morgan fingerprinting, MACCS, and Daylight fingerprint alone, improving chemoinformatics tasks such as drug classification. The proposed method offers a more informative representation of chemical structures, advancing molecular similarity analysis and facilitating applications in molecular design and drug discovery. It presents a promising avenue for molecular structure analysis and design, with significant potential for practical implementation.
    
[^74]: 新的农学家：语言模型是作物管理专家

    The New Agronomists: Language Models are Experts in Crop Management

    [https://arxiv.org/abs/2403.19839](https://arxiv.org/abs/2403.19839)

    本文介绍了一个更先进的智能作物管理系统，利用深度强化学习和语言模型结合决策支持系统来优化作物管理实践。

    

    作物管理在决定作物产量、经济盈利和环境可持续性方面起着至关重要的作用。本文在以往研究基础上引入了一个更先进的智能作物管理系统，该系统独特地将强化学习、语言模型（LM）和由决策支持系统为农业技术转移（DSSAT）实现的作物模拟相结合。我们利用深度强化学习，特别是深度Q网络，来训练处理模拟器中众多状态变量作为观测的管理策略。

    arXiv:2403.19839v1 Announce Type: cross  Abstract: Crop management plays a crucial role in determining crop yield, economic profitability, and environmental sustainability. Despite the availability of management guidelines, optimizing these practices remains a complex and multifaceted challenge. In response, previous studies have explored using reinforcement learning with crop simulators, typically employing simple neural-network-based reinforcement learning (RL) agents. Building on this foundation, this paper introduces a more advanced intelligent crop management system. This system uniquely combines RL, a language model (LM), and crop simulations facilitated by the Decision Support System for Agrotechnology Transfer (DSSAT). We utilize deep RL, specifically a deep Q-network, to train management policies that process numerous state variables from the simulator as observations. A novel aspect of our approach is the conversion of these state variables into more informative language, fac
    
[^75]: 通过视觉语言模型对神经网络进行基于概念的分析

    Concept-based Analysis of Neural Networks via Vision-Language Models

    [https://arxiv.org/abs/2403.19837](https://arxiv.org/abs/2403.19837)

    本文提出利用视觉语言模型作为透镜，通过其隐含的高层次概念来进行对视觉模型的分析。

    

    视觉深度神经网络（DNNs）的形式化分析非常可取，但由于难以表达视觉任务的形式化规范以及缺乏高效的验证程序，这是非常具有挑战性的。在本文中，我们提出利用新兴的多模态、视觉语言、基础模型（VLMs）作为一种通过其可以推理视觉模型的透镜。VLMs已经在大量图像及其文本描述上进行了训练，因此隐式地了解描述这些图像的高层次、人类可理解的概念。我们描述了一种名为$\texttt{Con}_{\texttt{spec}}$的逻辑规范语言，旨在便于按照这些概念编写规范。为了定义和形式化检查$\texttt{Con}_{\texttt{spec}}$规范，我们利用了一个VLM，它提供了一种编码和高效检查视觉模型的自然语言属性的方法。我们展示了我们的te

    arXiv:2403.19837v1 Announce Type: cross  Abstract: Formal analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\texttt{Con}_{\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\texttt{Con}_{\texttt{spec}}$ specifications, we leverage a VLM, which provides a means to encode and efficiently check natural-language properties of vision models. We demonstrate our te
    
[^76]: 评估医疗诊断中机器学习模型的解释能力：一种人机协作方法

    Evaluating Explanatory Capabilities of Machine Learning Models in Medical Diagnostics: A Human-in-the-Loop Approach

    [https://arxiv.org/abs/2403.19820](https://arxiv.org/abs/2403.19820)

    通过人机协作方法，评估医疗诊断中机器学习模型的解释能力，提出了使用医学指南和特征重要性确定胰腺癌治疗关键特征的方法，同时探索了使用相似度衡量的解释结果的方法。

    

    本文针对决策树、随机森林和XGBoost模型在胰腺癌数据集上的解释能力进行了全面研究。我们利用人机协作技术和医学指南作为领域知识来源，以确定与制定胰腺癌治疗相关的不同特征的重要性。这些特征不仅用作机器学习模型的降维方法，还用作评估不同模型的可解释性能力的方式，使用无知和非无知的解释性技术。为了便于解释结果的解释，我们提出使用诸如加权杰卡相似系数之类的相似度衡量。我们的目标不仅是选择性能最佳的模型，还要选择能够最好解释其结论并与之保持一致的模型。

    arXiv:2403.19820v1 Announce Type: cross  Abstract: This paper presents a comprehensive study on the evaluation of explanatory capabilities of machine learning models, with a focus on Decision Trees, Random Forest and XGBoost models using a pancreatic cancer dataset. We use Human-in-the-Loop related techniques and medical guidelines as a source of domain knowledge to establish the importance of the different features that are relevant to establish a pancreatic cancer treatment. These features are not only used as a dimensionality reduction approach for the machine learning models, but also as way to evaluate the explainability capabilities of the different models using agnostic and non-agnostic explainability techniques. To facilitate interpretation of explanatory results, we propose the use of similarity measures such as the Weighted Jaccard Similarity coefficient. The goal is to not only select the best performing model but also the one that can best explain its conclusions and aligns
    
[^77]: 锂离子电池健康预测在CPS时代的状况

    The State of Lithium-Ion Battery Health Prognostics in the CPS Era

    [https://arxiv.org/abs/2403.19816](https://arxiv.org/abs/2403.19816)

    本论文探讨了在锂离子电池中整合预测和健康管理的方法，着重于剩余有效寿命（RUL）的概念以及向深度学习架构的范式转变。

    

    锂离子电池已经彻底改变了能源存储技术，通过为各种设备和应用提供动力，已经成为我们日常生活中不可或缺的一部分。这篇论文探讨了在电池内实现预测和健康管理的无缝整合，提出了一种跨学科的方法，可以提高这些动力源的可靠性、安全性和性能。剩余有效寿命（RUL），在预测中的一个关键概念，被深入研究，强调了它在预测组件故障之前的作用。该论文审查了各种RUL预测方法，从传统模型到尖端的数据驱动技术。此外，它还突出了锂离子电池健康预测领域向深度学习架构转变的范式转变。

    arXiv:2403.19816v1 Announce Type: new  Abstract: Lithium-ion batteries (Li-ion) have revolutionized energy storage technology, becoming integral to our daily lives by powering a diverse range of devices and applications. Their high energy density, fast power response, recyclability, and mobility advantages have made them the preferred choice for numerous sectors. This paper explores the seamless integration of Prognostics and Health Management within batteries, presenting a multidisciplinary approach that enhances the reliability, safety, and performance of these powerhouses. Remaining useful life (RUL), a critical concept in prognostics, is examined in depth, emphasizing its role in predicting component failure before it occurs. The paper reviews various RUL prediction methods, from traditional models to cutting-edge data-driven techniques. Furthermore, it highlights the paradigm shift toward deep learning architectures within the field of Li-ion battery health prognostics, elucidatin
    
[^78]: 基于特征的回声状态网络：在储水池计算中迈向可解释性和极简主义的一步

    Feature-Based Echo-State Networks: A Step Towards Interpretability and Minimalism in Reservoir Computer

    [https://arxiv.org/abs/2403.19806](https://arxiv.org/abs/2403.19806)

    本文提出了一种基于特征的回声状态网络架构，通过使用较小并行储水池和非线性组合，提高了时间序列预测的性能，并展示了在动态系统预测方面的潜力。

    

    这篇论文提出了一种使用回声状态网络（ESN）范式进行时间序列预测的新颖且可解释的循环神经网络结构。传统的ESN在动态系统预测方面表现良好，但需要一个具有增加计算复杂性的大动态储盆。它也缺乏解释性，难以区分不同输入组合对输出的贡献。在这里，通过使用由不同输入组合驱动的较小并行储水池开发了一个系统化的储水池架构，称为特征ESN（Feat-ESN），然后将它们进行非线性组合以产生输出。所得的基于特征的ESN在少量储水池节点的情况下胜过传统的单一储水池ESN。提出的架构的预测能力在三个系统上得到了展示：两个来自混沌动力系统的合成数据集和一组实时交通数据。

    arXiv:2403.19806v1 Announce Type: new  Abstract: This paper proposes a novel and interpretable recurrent neural-network structure using the echo-state network (ESN) paradigm for time-series prediction. While the traditional ESNs perform well for dynamical systems prediction, it needs a large dynamic reservoir with increased computational complexity. It also lacks interpretability to discern contributions from different input combinations to the output. Here, a systematic reservoir architecture is developed using smaller parallel reservoirs driven by different input combinations, known as features, and then they are nonlinearly combined to produce the output. The resultant feature-based ESN (Feat-ESN) outperforms the traditional single-reservoir ESN with less reservoir nodes. The predictive capability of the proposed architecture is demonstrated on three systems: two synthetic datasets from chaotic dynamical systems and a set of real-time traffic data.
    
[^79]: Gegenbauer图神经网络用于时变信号重构

    Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction

    [https://arxiv.org/abs/2403.19800](https://arxiv.org/abs/2403.19800)

    提出了一种新颖的Gegenbauer-based graph convolutional (GegenConv)算子，用于提高时变信号重构的准确性

    

    重构时变图信号（或图时间序列插补）是机器学习和信号处理中的一个关键问题，具有广泛的应用，从传感器网络中的缺失数据插补到时间序列预测。准确捕捉这些信号固有的时空信息对于有效解决这些任务至关重要。然而，现有方法依赖于时间差的平滑性假设和简单的凸优化技术，存在固有限制。为了解决这些挑战，我们提出了一种结合学习模块以增强下游任务准确性的新方法。为此，我们引入基于Gegenbauer多项式理论的Gegenbauer-based graph convolutional（GegenConv）算子，这是传统切比雪夫图卷积的推 generalization。

    arXiv:2403.19800v1 Announce Type: cross  Abstract: Reconstructing time-varying graph signals (or graph time-series imputation) is a critical problem in machine learning and signal processing with broad applications, ranging from missing data imputation in sensor networks to time-series forecasting. Accurately capturing the spatio-temporal information inherent in these signals is crucial for effectively addressing these tasks. However, existing approaches relying on smoothness assumptions of temporal differences and simple convex optimization techniques have inherent limitations. To address these challenges, we propose a novel approach that incorporates a learning module to enhance the accuracy of the downstream task. To this end, we introduce the Gegenbauer-based graph convolutional (GegenConv) operator, which is a generalization of the conventional Chebyshev graph convolution by leveraging the theory of Gegenbauer polynomials. By deviating from traditional convex problems, we expand t
    
[^80]: MAPL: 模型无关的点对点学习

    MAPL: Model Agnostic Peer-to-peer Learning

    [https://arxiv.org/abs/2403.19792](https://arxiv.org/abs/2403.19792)

    MAPL提出了一种新颖的方法，即Model Agnostic Peer-to-peer Learning，通过点对点通信在邻近客户端之间同时学习异质个性化模型和协作图，在去中心化环境中实现了有效的协作，并且实验证明了MAPL在性能上具有竞争力。

    

    在去中心化环境中，异质客户端之间的有效协作在文献中是一个相当未被探索的领域。为了从结构上解决这个问题，我们引入了模型无关的点对点学习（简称MAPL），这是一种通过邻近客户端之间的点对点通信同时学习异质个性化模型和协作图的新方法。MAPL由两个主要模块组成：（i）本地级别的个性化模型学习（PML），利用客户端内部和客户端间对比损失的组合；（ii）网络范围的去中心化协作图学习（CGL），根据本地任务相似性以隐私保护的方式动态地优化协作权重。我们广泛的实验证明了MAPL的有效性，并且与其集中式模型无关的对应物相比，MAPL表现出竞争力（或者在大多数情况下更优），而且不依赖于任何中心服务器。

    arXiv:2403.19792v1 Announce Type: cross  Abstract: Effective collaboration among heterogeneous clients in a decentralized setting is a rather unexplored avenue in the literature. To structurally address this, we introduce Model Agnostic Peer-to-peer Learning (coined as MAPL) a novel approach to simultaneously learn heterogeneous personalized models as well as a collaboration graph through peer-to-peer communication among neighboring clients. MAPL is comprised of two main modules: (i) local-level Personalized Model Learning (PML), leveraging a combination of intra- and inter-client contrastive losses; (ii) network-wide decentralized Collaborative Graph Learning (CGL) dynamically refining collaboration weights in a privacy-preserving manner based on local task similarities. Our extensive experimentation demonstrates the efficacy of MAPL and its competitive (or, in most cases, superior) performance compared to its centralized model-agnostic counterparts, without relying on any central ser
    
[^81]: 使用大型语言模型的合金性能预测：AlloyBERT

    AlloyBERT: Alloy Property Prediction with Large Language Models

    [https://arxiv.org/abs/2403.19783](https://arxiv.org/abs/2403.19783)

    AlloyBERT是一种基于Transformer编码器的模型，利用文本输入预测合金的弹性模量和屈服强度，结合预训练的RoBERTa编码器和训练有素的分词器，实现了在MPEA数据集上的低均方误差。

    

    通过化学成分和处理参数，基于大型语言模型设计AlloyBERT模型来预测合金的弹性模量和屈服强度等性能，该模型利用预训练的RoBERTa编码器模型作为基础，通过自注意机制建立单词之间的有意义关系，从而解释可读的输入并预测合金性能。

    arXiv:2403.19783v1 Announce Type: cross  Abstract: The pursuit of novel alloys tailored to specific requirements poses significant challenges for researchers in the field. This underscores the importance of developing predictive techniques for essential physical properties of alloys based on their chemical composition and processing parameters. This study introduces AlloyBERT, a transformer encoder-based model designed to predict properties such as elastic modulus and yield strength of alloys using textual inputs. Leveraging the pre-trained RoBERTa encoder model as its foundation, AlloyBERT employs self-attention mechanisms to establish meaningful relationships between words, enabling it to interpret human-readable input and predict target alloy properties. By combining a tokenizer trained on our textual data and a RoBERTa encoder pre-trained and fine-tuned for this specific task, we achieved a mean squared error (MSE) of 0.00015 on the Multi Principal Elemental Alloys (MPEA) data set 
    
[^82]: 强化学习在基于代理的市场模拟中的应用：揭示真实的风格化事实和行为

    Reinforcement Learning in Agent-Based Market Simulation: Unveiling Realistic Stylized Facts and Behavior

    [https://arxiv.org/abs/2403.19781](https://arxiv.org/abs/2403.19781)

    通过使用强化学习代理，这项研究展示了在代理模拟中观察到的真实市场风格化事实，同时揭示了RL代理在面对外部市场冲击时的行为反应。

    

    投资者和监管机构可以从一个现实的市场模拟器中受益，这个模拟器让他们能够预测他们在真实市场中的决策后果。然而，传统的基于规则的市场模拟器往往难以准确捕捉市场参与者的动态行为，特别是在外部市场影响事件或其他参与者行为变化时。在这项研究中，我们探索了一个采用强化学习代理的代理模拟框架。我们介绍了这些强化学习代理的实现细节，并证明了模拟市场展示了观察到的真实世界市场的风格化事实。此外，我们研究了当RL代理面临外部市场冲击时的行为，比如闪崩。我们的研究结果揭示了RL代理在模拟中的有效性和适应性，提供了关于它们应对外部影响的见解。

    arXiv:2403.19781v1 Announce Type: cross  Abstract: Investors and regulators can greatly benefit from a realistic market simulator that enables them to anticipate the consequences of their decisions in real markets. However, traditional rule-based market simulators often fall short in accurately capturing the dynamic behavior of market participants, particularly in response to external market impact events or changes in the behavior of other participants. In this study, we explore an agent-based simulation framework employing reinforcement learning (RL) agents. We present the implementation details of these RL agents and demonstrate that the simulated market exhibits realistic stylized facts observed in real-world markets. Furthermore, we investigate the behavior of RL agents when confronted with external market impacts, such as a flash crash. Our findings shed light on the effectiveness and adaptability of RL-based agents within the simulation, offering insights into their response to 
    
[^83]: CLoRA: 一种对比方法来组合多个 LoRA 模型

    CLoRA: A Contrastive Approach to Compose Multiple LoRA Models

    [https://arxiv.org/abs/2403.19776](https://arxiv.org/abs/2403.19776)

    CLoRA提出了一种对比方法，用于组合多个LoRA模型，解决了将不同概念LoRA模型无缝混合到一个图像中的挑战。

    

    低秩调整（LoRA）已经成为图像生成领域中一种强大且受欢迎的技术，提供了一种高效的方式来调整和改进预训练的深度学习模型，而无需全面地重新训练。通过使用预训练的 LoRA 模型，例如代表特定猫和特定狗的模型，我们的目标是生成一个图像，该图像真实地体现了 LoRA 所定义的两种动物。然而，无缝地混合多个概念 LoRA 模型以捕获一个图像中的各种概念的任务被证明是一个重大挑战。常见方法往往表现不佳，主要是因为不同 LoRA 模型内的注意机制重叠，导致一个概念可能被完全忽略（例如漏掉了狗），或者概念被错误地组合在一起（例如生成两只猫的图像而不是一只猫和一只狗）。为了克服这一挑战，

    arXiv:2403.19776v1 Announce Type: cross  Abstract: Low-Rank Adaptations (LoRAs) have emerged as a powerful and popular technique in the field of image generation, offering a highly effective way to adapt and refine pre-trained deep learning models for specific tasks without the need for comprehensive retraining. By employing pre-trained LoRA models, such as those representing a specific cat and a particular dog, the objective is to generate an image that faithfully embodies both animals as defined by the LoRAs. However, the task of seamlessly blending multiple concept LoRAs to capture a variety of concepts in one image proves to be a significant challenge. Common approaches often fall short, primarily because the attention mechanisms within different LoRA models overlap, leading to scenarios where one concept may be completely ignored (e.g., omitting the dog) or where concepts are incorrectly combined (e.g., producing an image of two cats instead of one cat and one dog). To overcome th
    
[^84]: 分层深度学习用于装配任务中远程操纵操作意图估计

    Hierarchical Deep Learning for Intention Estimation of Teleoperation Manipulation in Assembly Tasks

    [https://arxiv.org/abs/2403.19770](https://arxiv.org/abs/2403.19770)

    该论文提出了一种分层深度学习框架，将多尺度层次信息整合到神经网络中，通过采用分层依赖损失来提高意图估计的准确性，在装配任务中实现了较优的意图识别和预测性能。

    

    在人机协作中，共享控制为远程操作机器人操纵提供了提升制造和装配工艺效率的机会。在执行用户意图上，机器人需要辅助。为此，需要鲁棒和即时的意图估计，依赖于行为观察。该框架提出了一种分层级别的意图估计技术，即低级动作和高级任务，通过在神经网络中整合多尺度层次信息。从技术上讲，我们采用分层依赖损失来提高整体准确率。此外，我们提出了一种多窗口方法，为输入数据分配适当的分层预测窗口。通过对各种输入的预测能力进行分析，证明了深度分层模型在预测准确度和提前意图识别方面的优越性。我们将该算法实现在一

    arXiv:2403.19770v1 Announce Type: cross  Abstract: In human-robot collaboration, shared control presents an opportunity to teleoperate robotic manipulation to improve the efficiency of manufacturing and assembly processes. Robots are expected to assist in executing the user's intentions. To this end, robust and prompt intention estimation is needed, relying on behavioral observations. The framework presents an intention estimation technique at hierarchical levels i.e., low-level actions and high-level tasks, by incorporating multi-scale hierarchical information in neural networks. Technically, we employ hierarchical dependency loss to boost overall accuracy. Furthermore, we propose a multi-window method that assigns proper hierarchical prediction windows of input data. An analysis of the predictive power with various inputs demonstrates the predominance of the deep hierarchical model in the sense of prediction accuracy and early intention identification. We implement the algorithm on a
    
[^85]: 物理信息神经网络用于卫星状态估计

    Physics-Informed Neural Networks for Satellite State Estimation

    [https://arxiv.org/abs/2403.19736](https://arxiv.org/abs/2403.19736)

    物理信息神经网络与深度神经网络相结合，为卫星中一些难以建模的异常加速度情况提供了强大的工具

    

    《物理信息神经网络用于卫星状态估计》论文翻译摘要：太空领域意识（SDA）群体通过将轨道状态拟合到太空监视网络（SSN）观测到的卫星来定期跟踪发射卫星。为了拟合这样的轨道，需要准确的作用于卫星的力模型。过去几十年，为卫星状态估计和传播开发了高质量的基于物理的模型。这些模型在估算和传播非机动卫星的轨道状态方面表现非常出色；然而，卫星可能遇到几类不太好建模的异常加速度，比如使用低推力电推进来修改轨道的卫星。对于这些类别的卫星，物理信息神经网络（PINNs）是一个宝贵的工具，因为它们将物理模型与深度神经网络（DNNs）结合起来，DNN是高度表现力和多功能的函数逼近器。

    arXiv:2403.19736v1 Announce Type: cross  Abstract: The Space Domain Awareness (SDA) community routinely tracks satellites in orbit by fitting an orbital state to observations made by the Space Surveillance Network (SSN). In order to fit such orbits, an accurate model of the forces that are acting on the satellite is required. Over the past several decades, high-quality, physics-based models have been developed for satellite state estimation and propagation. These models are exceedingly good at estimating and propagating orbital states for non-maneuvering satellites; however, there are several classes of anomalous accelerations that a satellite might experience which are not well-modeled, such as satellites that use low-thrust electric propulsion to modify their orbit. Physics-Informed Neural Networks (PINNs) are a valuable tool for these classes of satellites as they combine physics models with Deep Neural Networks (DNNs), which are highly expressive and versatile function approximator
    
[^86]: EmoScan：罗马化僧伽罗语推文中抑郁症症状的自动筛查

    EmoScan: Automatic Screening of Depression Symptoms in Romanized Sinhala Tweets

    [https://arxiv.org/abs/2403.19728](https://arxiv.org/abs/2403.19728)

    通过分析罗马化僧伽罗语社交媒体数据，提出了一种基于神经网络的框架，利用语言模式、情绪和行为线索可高准确率自动筛查抑郁症症状，显著超越了当前方法，有助于确定需积极干预和支持的个体。

    

    本文探讨了利用罗马化僧伽罗语社交媒体数据识别存在抑郁风险的个体。提出了基于机器学习的框架，通过分析社交媒体帖子中的语言模式、情绪和行为线索，在一套全面的数据集中自动筛查抑郁症症状。研究比较了神经网络与传统机器学习技术的适用性。所提出的神经网络具有注意力层，能处理长序列数据，在检测抑郁症状方面达到了93.25%的显著准确率，超越了当前的最先进方法。这些发现强调了该方法在确定需要积极干预和支持的个体方面的功效。通过提出的模型，心理健康专业人士、决策者和社交媒体公司可以获得有价值的见解。

    arXiv:2403.19728v1 Announce Type: new  Abstract: This work explores the utilization of Romanized Sinhala social media data to identify individuals at risk of depression. A machine learning-based framework is presented for the automatic screening of depression symptoms by analyzing language patterns, sentiment, and behavioural cues within a comprehensive dataset of social media posts. The research has been carried out to compare the suitability of Neural Networks over the classical machine learning techniques. The proposed Neural Network with an attention layer which is capable of handling long sequence data, attains a remarkable accuracy of 93.25% in detecting depression symptoms, surpassing current state-of-the-art methods. These findings underscore the efficacy of this approach in pinpointing individuals in need of proactive interventions and support. Mental health professionals, policymakers, and social media companies can gain valuable insights through the proposed model. Leveragin
    
[^87]: MUGC：机器生成与用户生成内容的检测

    MUGC: Machine Generated versus User Generated Content Detection

    [https://arxiv.org/abs/2403.19725](https://arxiv.org/abs/2403.19725)

    本研究对八种传统机器学习算法在识别机器生成和人类生成数据方面进行了比较评估，发现这些方法在识别机器生成数据方面具有较高的准确性。另外，机器生成的文本往往更短、词汇变化更少，而人类生成内容则使用了一些特定领域相关关键词被当前的大型语言模型忽略了。

    

    随着先进的现代系统如深度神经网络（DNNs）和生成式人工智能不断增强其产生令人信服和逼真内容的能力，区分用户生成与机器生成内容的需求变得越来越明显。在这项研究中，我们对八种传统机器学习算法进行了比较评估，以区分跨三个不同数据集（诗歌、摘要和论文）中的机器生成和人类生成数据。我们的结果表明，传统方法在识别机器生成数据方面表现出很高的准确性，反映了像RoBERT这样的热门预训练模型的有效性。我们注意到，与人类生成内容相比，机器生成的文本往往更短，词汇变化更少。虽然人类常用的特定领域相关关键词被当前的大型语言模型（LLMs）忽略了。

    arXiv:2403.19725v1 Announce Type: cross  Abstract: As advanced modern systems like deep neural networks (DNNs) and generative AI continue to enhance their capabilities in producing convincing and realistic content, the need to distinguish between user-generated and machine generated content is becoming increasingly evident. In this research, we undertake a comparative evaluation of eight traditional machine-learning algorithms to distinguish between machine-generated and human-generated data across three diverse datasets: Poems, Abstracts, and Essays. Our results indicate that traditional methods demonstrate a high level of accuracy in identifying machine-generated data, reflecting the documented effectiveness of popular pre-trained models like RoBERT. We note that machine-generated texts tend to be shorter and exhibit less word variety compared to human-generated content. While specific domain-related keywords commonly utilized by humans, albeit disregarded by current LLMs (Large Lang
    
[^88]: 利用大数据进行高效稳健预测分析的计算和内存优化

    Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data

    [https://arxiv.org/abs/2403.19721](https://arxiv.org/abs/2403.19721)

    本研究通过利用稳健主成分分析(RPCA)进行噪声降低和异常值排除，以及优化传感器放置(OSP)进行有效数据压缩和存储，提出了一种同时优化计算和内存效率的大数据预测分析方法。

    

    在当前数据密集的时代，大数据已经成为人工智能（AI）的重要资产，为开发基于数据的模型并深入探索各种未知领域奠定了基础。本研究探讨了使用大数据进行数据不确定性、存储限制和预测数据驱动建模的挑战。我们利用稳健主成分分析（RPCA）有效降噪和去除离群值，以及优化传感器放置（OSP）实现高效数据压缩和存储。所提出的OSP技术能够实现数据压缩，同时减少存储需求，且信息损失不大。虽然RPCA为高维数据管理提供了比传统主成分分析（PCA）更好的选择，但本研究的范围则扩展了其应用范围，专注于适用于实时海量数据集的稳健数据驱动建模。

    arXiv:2403.19721v1 Announce Type: cross  Abstract: In the current data-intensive era, big data has become a significant asset for Artificial Intelligence (AI), serving as a foundation for developing data-driven models and providing insight into various unknown fields. This study navigates through the challenges of data uncertainties, storage limitations, and predictive data-driven modeling using big data. We utilize Robust Principal Component Analysis (RPCA) for effective noise reduction and outlier elimination, and Optimal Sensor Placement (OSP) for efficient data compression and storage. The proposed OSP technique enables data compression without substantial information loss while simultaneously reducing storage needs. While RPCA offers an enhanced alternative to traditional Principal Component Analysis (PCA) for high-dimensional data management, the scope of this work extends its utilization, focusing on robust, data-driven modeling applicable to huge data sets in real-time. For tha
    
[^89]: 具有广义岭回归的元学习：高维渐近性、最优性和超协方差估计

    Meta-Learning with Generalized Ridge Regression: High-dimensional Asymptotics, Optimality and Hyper-covariance Estimation

    [https://arxiv.org/abs/2403.19720](https://arxiv.org/abs/2403.19720)

    本研究在高维多元随机效应线性模型框架下研究了元学习，在使用广义岭回归进行预测时发现，利用随机回归系数的协方差结构可以在新任务上做出更好的预测，并提出了一种最优的权重矩阵选择方法。

    

    Meta-learning即元学习，指的是以一种方式在多个训练任务上训练模型，使之能够在新的、未见过的测试任务上有很好的泛化能力。本文将元学习纳入高维多元随机效应线性模型框架中，并研究基于广义岭回归的预测。在该设定下使用广义岭回归的统计直觉是，随机回归系数的协方差结构可以被利用来在新任务上做出更好的预测。我们首先详细描述了在数据维度与每个任务样本数成比例增长时，对于新测试任务的预测风险的精确渐近行为。接着我们证明了当广义岭回归中的权重矩阵选择为随机系数的协方差矩阵的逆时，这种预测风险是最优的。最后，我们提出并分析了一种估计方法。

    arXiv:2403.19720v1 Announce Type: cross  Abstract: Meta-learning involves training models on a variety of training tasks in a way that enables them to generalize well on new, unseen test tasks. In this work, we consider meta-learning within the framework of high-dimensional multivariate random-effects linear models and study generalized ridge-regression based predictions. The statistical intuition of using generalized ridge regression in this setting is that the covariance structure of the random regression coefficients could be leveraged to make better predictions on new tasks. Accordingly, we first characterize the precise asymptotic behavior of the predictive risk for a new test task when the data dimension grows proportionally to the number of samples per task. We next show that this predictive risk is optimal when the weight matrix in generalized ridge regression is chosen to be the inverse of the covariance matrix of random coefficients. Finally, we propose and analyze an estimat
    
[^90]: 用于高效计算分子指纹的Python库

    A Python library for efficient computation of molecular fingerprints

    [https://arxiv.org/abs/2403.19718](https://arxiv.org/abs/2403.19718)

    创建了一个Python库，可以高效计算分子指纹，并提供了全面的接口，使用户能够轻松将该库整合到其现有的机器学习工作流程中。

    

    机器学习解决方案在化学信息学领域非常流行，具有诸多应用，如新药发现或分子性质预测。分子指纹是一种常用算法，用于将化学分子向量化，作为预处理解决方案的一部分。然而，尽管它们很受欢迎，但没有一个库能够高效地为大型数据集实现它们，利用现代的多核架构。此外，大部分库并没有提供用户友好的接口，或者与其他机器学习工具兼容的接口。

    arXiv:2403.19718v1 Announce Type: cross  Abstract: Machine learning solutions are very popular in the field of chemoinformatics, where they have numerous applications, such as novel drug discovery or molecular property prediction. Molecular fingerprints are algorithms commonly used for vectorizing chemical molecules as a part of preprocessing in this kind of solution. However, despite their popularity, there are no libraries that implement them efficiently for large datasets, utilizing modern, multicore architectures. On top of that, most of them do not provide the user with an intuitive interface, or one that would be compatible with other machine learning tools.   In this project, we created a Python library that computes molecular fingerprints efficiently and delivers an interface that is comprehensive and enables the user to easily incorporate the library into their existing machine learning workflow. The library enables the user to perform computation on large datasets using paral
    
[^91]: 一张图片价值500个标签：Instagram和TikTok本地机器学习模型中的人口统计差异案例研究

    A Picture is Worth 500 Labels: A Case Study of Demographic Disparities in Local Machine Learning Models for Instagram and TikTok

    [https://arxiv.org/abs/2403.19717](https://arxiv.org/abs/2403.19717)

    本研究分析了Instagram和TikTok两个流行社交媒体应用中视觉模型推断用户信息的能力，揭示了该模型在不同人口统计数据上的性能差异，为确保用户获得公平准确的服务提供了重要参考。

    

    移动应用通过将数据处理移至用户的智能手机来支持用户隐私。现在，诸如视觉模型之类的先进机器学习（ML）模型能够在本地分析用户图像，以提取驱动多种功能的见解。利用这种新的处理模型，我们分析了两个流行的社交媒体应用TikTok和Instagram，揭示这两个应用中的视觉模型从用户的图像和视频数据中推断出的见解，以及这些模型在不同人口统计数据方面是否表现出性能差异。由于视觉模型为诸如年龄验证和人脸识别等敏感技术提供信号，了解这些模型中潜在的偏见对于确保用户获得公平准确的服务至关重要。我们开发了一种捕获和评估移动应用中ML任务的新方法，克服了代码混淆、原生应用等挑战。

    arXiv:2403.19717v1 Announce Type: new  Abstract: Mobile apps have embraced user privacy by moving their data processing to the user's smartphone. Advanced machine learning (ML) models, such as vision models, can now locally analyze user images to extract insights that drive several functionalities. Capitalizing on this new processing model of locally analyzing user images, we analyze two popular social media apps, TikTok and Instagram, to reveal (1) what insights vision models in both apps infer about users from their image and video data and (2) whether these models exhibit performance disparities with respect to demographics. As vision models provide signals for sensitive technologies like age verification and facial recognition, understanding potential biases in these models is crucial for ensuring that users receive equitable and accurate services.   We develop a novel method for capturing and evaluating ML tasks in mobile apps, overcoming challenges like code obfuscation, native c
    
[^92]: NJUST-KMG参加TRAC-2024任务1和任务2：离线危害潜在性识别

    NJUST-KMG at TRAC-2024 Tasks 1 and 2: Offline Harm Potential Identification

    [https://arxiv.org/abs/2403.19713](https://arxiv.org/abs/2403.19713)

    该研究提出了在TRAC-2024离线危害潜在性识别任务中的方法，利用专家标注的社交媒体评论数据集，成功设计了能够准确评估危害可能性并识别目标的算法，在两个赛道中取得第二名的成绩。

    

    这份报告详细描述了我们在TRAC-2024离线危害潜在性识别中提出的方法，该比赛包含两个子任务。研究利用了一个包含多种印度语言社交媒体评论的丰富数据集，由专家评分标注，以捕捉离线环境危害的微妙含义。参与者的任务是设计能够准确评估特定情况下危害可能性并识别离线危害最可能的目标的算法。我们的方法在两个不同的赛道中排名第二，F1值分别为0.73和0.96。我们的方法主要涉及选择预训练模型进行微调，整合对比学习技术，并通过集成方法应用于测试集。

    arXiv:2403.19713v1 Announce Type: new  Abstract: This report provide a detailed description of the method that we proposed in the TRAC-2024 Offline Harm Potential dentification which encloses two sub-tasks. The investigation utilized a rich dataset comprised of social media comments in several Indian languages, annotated with precision by expert judges to capture the nuanced implications for offline context harm. The objective assigned to the participants was to design algorithms capable of accurately assessing the likelihood of harm in given situations and identifying the most likely target(s) of offline harm. Our approach ranked second in two separate tracks, with F1 values of 0.73 and 0.96 respectively. Our method principally involved selecting pretrained models for finetuning, incorporating contrastive learning techniques, and culminating in an ensemble approach for the test set.
    
[^93]: STRUM-LLM: 属性化和结构化对比摘要

    STRUM-LLM: Attributed and Structured Contrastive Summarization

    [https://arxiv.org/abs/2403.19710](https://arxiv.org/abs/2403.19710)

    STRUM-LLM提出了一种生成属性化、结构化和有帮助的对比摘要的方法，识别并突出两个选项之间的关键差异，不需要人工标记的数据或固定属性列表，具有高吞吐量和小体积。

    

    用户经常在两个选项（A vs B）之间做决策时感到困难，因为这通常需要在多个网页上进行耗时的研究。我们提出了STRUM-LLM，通过生成带属性、结构化和有帮助的对比摘要，突出两个选项之间的关键差异，来解决这一挑战。STRUM-LLM识别了有帮助的对比：两个选项在哪些特定属性上有显著差异，以及最有可能影响用户决策。我们的技术是与领域无关的，并不需要任何人工标记的数据或固定属性列表作为监督。STRUM-LLM将所有提取的内容属性化，以及文本证据，且不限制其处理的输入来源的长度。STRUM-LLM Distilled的吞吐量比具有相似性能的模型高100倍，同时体积小10倍。在本文中，我们进行了广泛的评估。

    arXiv:2403.19710v1 Announce Type: cross  Abstract: Users often struggle with decision-making between two options (A vs B), as it usually requires time-consuming research across multiple web pages. We propose STRUM-LLM that addresses this challenge by generating attributed, structured, and helpful contrastive summaries that highlight key differences between the two options. STRUM-LLM identifies helpful contrast: the specific attributes along which the two options differ significantly and which are most likely to influence the user's decision. Our technique is domain-agnostic, and does not require any human-labeled data or fixed attribute list as supervision. STRUM-LLM attributes all extractions back to the input sources along with textual evidence, and it does not have a limit on the length of input sources that it can process. STRUM-LLM Distilled has 100x more throughput than the models with comparable performance while being 10x smaller. In this paper, we provide extensive evaluations
    
[^94]: 高效多任务调整大型语音模型的分层递归适配器

    Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models

    [https://arxiv.org/abs/2403.19709](https://arxiv.org/abs/2403.19709)

    提出了一种分层递归适配器模块，能够在大规模多任务适配场景下降低每个任务的参数开销，同时保持在下游任务中的性能表现，优于先前的适配器方法和完整模型微调基线

    

    参数高效的适配方法已经成为训练大型预训练模型用于下游任务的关键机制。我们引入了一种适配器模块，在大规模多任务适配场景下具有更好的效率。我们的适配器在适配器参数分配方面是分层的。适配器由一个共享的控制网络和多个任务级适配器头组成，以减少每个任务的参数开销，而不会影响下游任务的性能。适配器还是递归的，因此整个适配器参数在预训练模型的不同层之间被重用。我们的分层递归适配器（HRA）在单任务和多任务适配设置中都优于先前的基于适配器的方法以及完整模型微调基线。

    arXiv:2403.19709v1 Announce Type: cross  Abstract: Parameter efficient adaptation methods have become a key mechanism to train large pre-trained models for downstream tasks. However, their per-task parameter overhead is considered still high when the number of downstream tasks to adapt for is large. We introduce an adapter module that has a better efficiency in large scale multi-task adaptation scenario. Our adapter is hierarchical in terms of how the adapter parameters are allocated. The adapter consists of a single shared controller network and multiple task-level adapter heads to reduce the per-task parameter overhead without performance regression on downstream tasks. The adapter is also recurrent so the entire adapter parameters are reused across different layers of the pre-trained model. Our Hierarchical Recurrent Adapter (HRA) outperforms the previous adapter-based approaches as well as full model fine-tuning baseline in both single and multi-task adaptation settings when evalua
    
[^95]: AttentionStore: 在大型语言模型服务中实现多轮对话中的注意力成本效益复用

    AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving

    [https://arxiv.org/abs/2403.19708](https://arxiv.org/abs/2403.19708)

    AttentionStore提出了一种新的注意力机制，通过实现KV缓存的复用，在大型语言模型服务中显著降低了多轮对话中的重复计算成本。

    

    通过多轮对话与人类进行交互是大型语言模型（LLMs）的基本特征。然而，由于需要重复计算历史记号的键值（KV）缓存，导致现有用于执行多轮对话的LLM服务引擎效率低下，产生高昂的服务成本。为解决这一问题，本文提出了AttentionStore，一种新的注意力机制，实现了跨多轮对话的KV缓存复用（即 注意力复用），显著降低了重复计算开销。AttentionStore维护了一个层次结构的KV缓存系统，利用成本效益的内存/存储介质为所有请求保存KV缓存。为了减少慢速介质的KV缓存访问开销，AttentionStore采用逐层预加载和异步保存方案，将KV缓存访问与GPU计算重叠。为确保要访问的KV缓存…

    arXiv:2403.19708v1 Announce Type: new  Abstract: Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines for executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes AttentionStore, a new attention mechanism that enables the reuse of KV caches (i.e., attention reuse) across multi-turn conversations, significantly reducing the repetitive computation overheads. AttentionStore maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, AttentionStore employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are pl
    
[^96]: UWB定位系统中基于第一个路径成分功率的NLOS抑制

    First path component power based NLOS mitigation in UWB positioning system

    [https://arxiv.org/abs/2403.19706](https://arxiv.org/abs/2403.19706)

    该论文介绍了一种基于第一个路径信号成分功率的NLOS抑制方法，通过纠正NLOS条件下的测量结果并降低其在标签位置估计中的重要性来实现。

    

    该论文描述了一种用于UWB定位系统的NLOS（非视距）抑制方法。该方法将局部对象与形成系统基础设施的锚点之间的传播条件分类为LOS（视距）、NLOS和严重NLOS中的一种。根据第一个路径信号成分功率测量进行非视距检测。针对每个类别，基于在配备齐全的公寓进行的测量活动期间收集到的结果，估计了平均NLOS引入到达时间偏差和偏差标准差。为了定位标签，使用了基于EKF（扩展卡尔曼滤波器）的算法。提出的NLOS抑制方法在纠正在NLOS条件下获得的测量结果并降低其在标签位置估计过程中的重要性方面。论文包括该方法和r

    arXiv:2403.19706v1 Announce Type: cross  Abstract: The paper describes an NLOS (Non-Line-of-Sight) mitigation method intended for use in a UWB positioning system. In the proposed method propagation conditions between the localized objects and the anchors forming system infrastructure are classified into one of three categories: LOS (Line-of-Sight), NLOS and severe NLOS. Non-Line-of-Sight detection is conducted based on first path signal component power measurements. For each of the categories, average NLOS inducted time of arrival bias and bias standard deviation have been estimated based on results gathered during a measurement campaign conducted in a fully furnished apartment. To locate a tag, an EKF (Extended Kalman Filter) based algorithm is used. The proposed method of NLOS mitigation consists in correcting measurement results obtained in NLOS conditions and lowering their significance in a tag position estimation process. The paper includes the description of the method and the r
    
[^97]: 分析语言和视觉在从有限数据中学习中的作用

    Analyzing the Roles of Language and Vision in Learning from Limited Data

    [https://arxiv.org/abs/2403.19669](https://arxiv.org/abs/2403.19669)

    研究人工智能中的复杂视觉-语言模型，发现即使缺乏视觉输入，利用所有组件的语言模型能够恢复大部分VLM的性能，表明语言通过提供对先前知识和推理的访问来对学习新任务有贡献

    

    arXiv:2403.19669v1 公告类型：交叉摘要：语言是否有助于理解视觉世界？实际观察世界需要看到实际情况，而不是用文字描述吗？关于智能本质的这些基本问题很难回答，因为我们只有一个智能系统的例子——人类——以及有限的独立语言或视觉的案例。然而，人工智能研究人员开发出复杂的视觉-语言模型（VLMs）为我们提供了新的机会，探索语言和视觉对于学习世界的贡献。我们从这些模型的认知架构中切除组件，以确定它们对从有限数据中学习新任务的贡献。我们发现，利用所有组件的语言模型恢复了大部分VLM的性能，尽管它缺乏视觉输入，而语言似乎可以通过提供对先前知识和推理的访问来实现这一点。

    arXiv:2403.19669v1 Announce Type: cross  Abstract: Does language help make sense of the visual world? How important is it to actually see the world rather than having it described with words? These basic questions about the nature of intelligence have been difficult to answer because we only had one example of an intelligent system -- humans -- and limited access to cases that isolated language or vision. However, the development of sophisticated Vision-Language Models (VLMs) by artificial intelligence researchers offers us new opportunities to explore the contributions that language and vision make to learning about the world. We ablate components from the cognitive architecture of these models to identify their contributions to learning new tasks from limited data. We find that a language model leveraging all components recovers a majority of a VLM's performance, despite its lack of visual input, and that language seems to allow this by providing access to prior knowledge and reasoni
    
[^98]: 基因量化感知逼近用于变压器中的非线性操作

    Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers

    [https://arxiv.org/abs/2403.19591](https://arxiv.org/abs/2403.19591)

    本文提出的GQA-LUT算法在变压器中的非线性操作中具有量化感知性，并实现了对INT8-based LUT逼近的应用，节约了大量硬件和功耗

    

    在变压器及其轻量级变体中，非线性函数普遍存在，导致硬件成本显著且经常被低估。先前的最先进作品通过分段线性逼近来优化这些操作，并将参数存储在查找表（LUT）中，但大多数需要不友好的高精度算术，如FP/INT 32，并且缺乏对纯整数INT量化的考虑。本文提出了一种遗传LUT-逼近算法，即GQA-LUT，它可以自动确定具有量化意识的参数。结果表明，对于普通和线性Transformer模型的挑战性语义分割任务，GQA-LUT实现了可忽略的性能降级。此外，提出的GQA-LUT使得能够使用基于INT8的LUT逼近，相比高精度FP/INT 32，可以实现81.3~81.7%的面积节约和79.3~80.2%的功耗降低。

    arXiv:2403.19591v1 Announce Type: new  Abstract: Non-linear functions are prevalent in Transformers and their lightweight variants, incurring substantial and frequently underestimated hardware costs. Previous state-of-the-art works optimize these operations by piece-wise linear approximation and store the parameters in look-up tables (LUT), but most of them require unfriendly high-precision arithmetics such as FP/INT 32 and lack consideration of integer-only INT quantization. This paper proposed a genetic LUT-Approximation algorithm namely GQA-LUT that can automatically determine the parameters with quantization awareness. The results demonstrate that GQA-LUT achieves negligible degradation on the challenging semantic segmentation task for both vanilla and linear Transformer models. Besides, proposed GQA-LUT enables the employment of INT8-based LUT-Approximation that achieves an area savings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to the high-precision FP/INT 32 alte
    
[^99]: 人类中心施工机器人：基于强化学习的助手机器人为木工劳动者提供环境上下文协助

    Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers

    [https://arxiv.org/abs/2403.19060](https://arxiv.org/abs/2403.19060)

    本文提出了一种人类中心的建筑机器人方法，通过强化学习驱动的助手机器人为木工劳动者提供环境上下文协助，推进了机器人在建筑中的应用。

    

    在这个充满活力的建筑行业中，传统的机器人集成主要集中在自动化特定任务，通常忽略了建筑工作流程中人类因素的复杂性和变化性。本文提出了一种以人为本的方法，设计了一个“工作伴侣漫游器”，旨在协助建筑工人完成其现有实践，旨在增强安全性和工作流程的流畅性，同时尊重建筑劳动的技术性质。我们对在木工模板工程中部署机器人系统进行了深入研究，展示了一个原型，通过环境相关的强化学习（RL）驱动模块化框架，重点强调了在动态环境中的机动性、安全性和舒适的工人-机器人协作。我们的研究推进了机器人在建筑中的应用，倡导协作模型，其中自适应机器人支持而不是取代人类，强调了交互式的潜力。

    arXiv:2403.19060v1 Announce Type: cross  Abstract: In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows. This paper introduces a human-centered approach with a ``work companion rover" designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor's skilled nature. We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual Reinforcement Learning (RL)-driven modular framework. Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans, underscoring the potential for an interactive a
    
[^100]: 论排列不变神经网络

    On permutation-invariant neural networks

    [https://arxiv.org/abs/2403.17410](https://arxiv.org/abs/2403.17410)

    神经网络如Deep Sets和Transformers的出现显著推动了基于集合的数据处理的进展

    

    传统机器学习算法通常在假设输入数据遵循基于向量的格式的前提下设计，着重于基于向量的范式。然而，随着需求涉及基于集合的任务的增长，研究界对解决这些挑战的兴趣发生了范式转变。近年来，Deep Sets和Transformers等神经网络架构的出现在处理基于集合的数据方面取得了重大进展。这些架构专门设计为自然容纳集合作为输入，从而更有效地表示和处理集合结构。因此，近年来出现了大量致力于探索和利用这些架构能力的研究努力，以逼近集合函数的各种任务。这项综合调查旨在概述th

    arXiv:2403.17410v1 Announce Type: cross  Abstract: Conventional machine learning algorithms have traditionally been designed under the assumption that input data follows a vector-based format, with an emphasis on vector-centric paradigms. However, as the demand for tasks involving set-based inputs has grown, there has been a paradigm shift in the research community towards addressing these challenges. In recent years, the emergence of neural network architectures such as Deep Sets and Transformers has presented a significant advancement in the treatment of set-based data. These architectures are specifically engineered to naturally accommodate sets as input, enabling more effective representation and processing of set structures. Consequently, there has been a surge of research endeavors dedicated to exploring and harnessing the capabilities of these architectures for various tasks involving the approximation of set functions. This comprehensive survey aims to provide an overview of th
    
[^101]: 语言模型是生物医学成像任务的免费助推器

    Language Models are Free Boosters for Biomedical Imaging Tasks

    [https://arxiv.org/abs/2403.17343](https://arxiv.org/abs/2403.17343)

    本研究揭示了基于残差的大型语言模型在生物医学成像任务中作为编码器的意想不到的有效性，利用冻结的变压器块进行直接处理视觉令牌，从而提高各种生物医学成像应用的性能。

    

    在这项研究中，我们揭示了基于残差的大型语言模型（LLMs）在生物医学成像任务中作为编码器的意想不到的有效性，这是传统上缺乏语言或文本数据的领域。该方法不同于已建立的方法，通过利用从预训练的LLMs中提取的冻结变压器块作为创新的编码器层，直接处理视觉令牌。这种策略与通常依赖于语言驱动提示和输入的标准多模态视觉语言框架有着显著的不同。我们发现这些LLMs能够提升各种生物医学成像应用的性能，包括2D和3D视觉分类任务，充当即插即用的助推器。更有趣的是，作为副产品，我们发现所提出的框架实现了卓越的性能，在M的广泛、标准化数据集中取得了新的最先进结果。

    arXiv:2403.17343v1 Announce Type: cross  Abstract: In this study, we uncover the unexpected efficacy of residual-based large language models (LLMs) as part of encoders for biomedical imaging tasks, a domain traditionally devoid of language or textual data. The approach diverges from established methodologies by utilizing a frozen transformer block, extracted from pre-trained LLMs, as an innovative encoder layer for the direct processing of visual tokens. This strategy represents a significant departure from the standard multi-modal vision-language frameworks, which typically hinge on language-driven prompts and inputs. We found that these LLMs could boost performance across a spectrum of biomedical imaging applications, including both 2D and 3D visual classification tasks, serving as plug-and-play boosters. More interestingly, as a byproduct, we found that the proposed framework achieved superior performance, setting new state-of-the-art results on extensive, standardized datasets in M
    
[^102]: 联合胸部X光诊断和临床视觉注意力预测的多阶段协作学习：增强可解释性

    Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability

    [https://arxiv.org/abs/2403.16970](https://arxiv.org/abs/2403.16970)

    该论文引入了一种新的深度学习框架，用于联合疾病诊断和胸部X光扫描对应视觉显著性图的预测，通过设计新颖的双编码器多任务UNet并利用多尺度特征融合分类器来提高计算辅助诊断的可解释性和质量。

    

    随着深度学习成为计算辅助诊断的最新技术，自动决策的可解释性对临床部署至关重要。尽管在这一领域提出了各种方法，但在放射学筛查过程中临床医生的视觉注意力图为提供重要洞察提供了独特的资产，并有可能提高计算辅助诊断的质量。通过这篇论文，我们引入了一种新颖的深度学习框架，用于联合疾病诊断和胸部X光扫描对应视觉显著性图的预测。具体来说，我们设计了一种新颖的双编码器多任务UNet，利用了DenseNet201主干和基于残差和膨胀激励块的编码器来提取用于显著性图预测的多样特征，并使用多尺度特征融合分类器进行疾病分类。

    arXiv:2403.16970v1 Announce Type: cross  Abstract: As deep learning has become the state-of-the-art for computer-assisted diagnosis, interpretability of the automatic decisions is crucial for clinical deployment. While various methods were proposed in this domain, visual attention maps of clinicians during radiological screening offer a unique asset to provide important insights and can potentially enhance the quality of computer-assisted diagnosis. With this paper, we introduce a novel deep-learning framework for joint disease diagnosis and prediction of corresponding visual saliency maps for chest X-ray scans. Specifically, we designed a novel dual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based encoder to extract diverse features for saliency map prediction, and a multi-scale feature-fusion classifier to perform disease classification. To tackle the issue of asynchronous training schedules of individual tasks
    
[^103]: FedAC：一种用于异构数据的自适应分簇联邦学习框架

    FedAC: A Adaptive Clustered Federated Learning Framework for Heterogeneous Data

    [https://arxiv.org/abs/2403.16460](https://arxiv.org/abs/2403.16460)

    FedAC框架通过解耦神经网络，使用不同聚合方法为每个子模块提供全局知识，引入经济高效的在线模型相似度度量，及集群数量微调模块，显著提高了性能。

    

    本文提出了一种自适应的分簇联邦学习框架FedAC，该框架通过解耦神经网络并利用不同的聚合方法为每个子模块有效地将全局知识整合到簇内学习中，显著提高了性能；引入了一种基于降维的经济高效的在线模型相似度度量；并且结合了一个用于改进复杂性的集群数量微调模块，从而提高了适应性和可伸缩性。

    arXiv:2403.16460v1 Announce Type: cross  Abstract: Clustered federated learning (CFL) is proposed to mitigate the performance deterioration stemming from data heterogeneity in federated learning (FL) by grouping similar clients for cluster-wise model training. However, current CFL methods struggle due to inadequate integration of global and intra-cluster knowledge and the absence of an efficient online model similarity metric, while treating the cluster count as a fixed hyperparameter limits flexibility and robustness. In this paper, we propose an adaptive CFL framework, named FedAC, which (1) efficiently integrates global knowledge into intra-cluster learning by decoupling neural networks and utilizing distinct aggregation methods for each submodule, significantly enhancing performance; (2) includes a costeffective online model similarity metric based on dimensionality reduction; (3) incorporates a cluster number fine-tuning module for improved adaptability and scalability in complex,
    
[^104]: 面向资源受限设备的低能量自适应个性化研究

    Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices

    [https://arxiv.org/abs/2403.15905](https://arxiv.org/abs/2403.15905)

    提出了面向资源受限设备的低能耗自适应个性化框架目标块微调，根据数据漂移类型微调不同模块以实现最佳性能和降低能源消耗。

    

    机器学习（ML）模型个性化以解决数据漂移问题在物联网（IoT）应用中是一个重要挑战。目前，大多数方法侧重于微调完整基础模型或其最后几层以适应新数据，但往往忽视能源成本。我们提出了一种面向资源受限设备的低能耗自适应个性化框架——目标块微调（TBFT）。我们将数据漂移和个性化分为三种类型：输入级别、特征级别和输出级别。针对每种类型，我们微调不同模型块以实现在降低能源成本的情况下达到最佳性能。具体而言，输入级、特征级和输出级对应于微调模型的前端、中段和后端。

    arXiv:2403.15905v1 Announce Type: new  Abstract: The personalization of machine learning (ML) models to address data drift is a significant challenge in the context of Internet of Things (IoT) applications. Presently, most approaches focus on fine-tuning either the full base model or its last few layers to adapt to new data, while often neglecting energy costs. However, various types of data drift exist, and fine-tuning the full base model or the last few layers may not result in optimal performance in certain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy adaptive personalization framework designed for resource-constrained devices. We categorize data drift and personalization into three types: input-level, feature-level, and output-level. For each type, we fine-tune different blocks of the model to achieve optimal performance with reduced energy costs. Specifically, input-, feature-, and output-level correspond to fine-tuning the front, middle, and rear blocks of 
    
[^105]: 基于迟来的模态融合和基于规则的决策的音视频复合表达识别方法

    Audio-Visual Compound Expression Recognition Method based on Late Modality Fusion and Rule-based Decision

    [https://arxiv.org/abs/2403.12687](https://arxiv.org/abs/2403.12687)

    该研究提出了一种新颖的音视频复合表达识别方法，通过情绪识别模型融合不同模态的情绪概率，并基于预定义规则进行决策，无需特定训练数据，具有潜力为人类基本和复合情感情境下的音视频数据注释提供智能工具。

    

    本文介绍了第六届ABAW比赛的SUN团队针对复合表达识别挑战的结果。我们提出了一种新颖的音视频复合表达识别方法。我们的方法依赖于融合情绪概率水平的情绪识别模型，而关于复合表达预测的决策基于预定义规则。值得注意的是，我们的方法不使用任何特定于目标任务的训练数据。该方法在多语料训练和跨语料验证设置中进行评估。从挑战中的研究结果表明，所提出的方法有可能为在人类基本和复合情感情境下注释音视频数据的智能工具开发奠定基础。源代码已公开。

    arXiv:2403.12687v1 Announce Type: cross  Abstract: This paper presents the results of the SUN team for the Compound Expressions Recognition Challenge of the 6th ABAW Competition. We propose a novel audio-visual method for compound expression recognition. Our method relies on emotion recognition models that fuse modalities at the emotion probability level, while decisions regarding the prediction of compound expressions are based on predefined rules. Notably, our method does not use any training data specific to the target task. The method is evaluated in multi-corpus training and cross-corpus validation setups. Our findings from the challenge demonstrate that the proposed method can potentially form a basis for development of intelligent tools for annotating audio-visual data in the context of human's basic and compound emotions. The source code is publicly available.
    
[^106]: 通过元学习在困难样本上改善泛化性能

    Improving Generalization via Meta-Learning on Hard Samples

    [https://arxiv.org/abs/2403.12236](https://arxiv.org/abs/2403.12236)

    在学习的重加权(LRW)方法中，通过元学习中使用难以分类的实例作为验证集，来改善分类器的泛化性能，并提出了一个高效的算法来训练这个模型。

    

    学习的重加权(LRW)方法用一个优化准则为训练实例分配权重，以便在一个代表性验证数据集上最大化性能。我们提出并形式化了LRW训练中的优化验证集选择的问题，以改善分类器的泛化性能。特别地，我们展示了在验证集中使用难以分类的实例既与理论相关，又有强有力的实证证据支持泛化。我们提供了一个高效的算法来训练这个元优化模型，以及一个简单的两次训练启发式方法以进行谨慎的比较研究。我们证明，与易验证数据一起的LRW表现始终比难验证数据一起的LRW表现差，从而确立了我们的元优化问题的有效性。我们提出的算法在各种数据集和领域转移挑战上胜过了各种基线。

    arXiv:2403.12236v1 Announce Type: new  Abstract: Learned reweighting (LRW) approaches to supervised learning use an optimization criterion to assign weights for training instances, in order to maximize performance on a representative validation dataset. We pose and formalize the problem of optimized selection of the validation set used in LRW training, to improve classifier generalization. In particular, we show that using hard-to-classify instances in the validation set has both a theoretical connection to, and strong empirical evidence of generalization. We provide an efficient algorithm for training this meta-optimized model, as well as a simple train-twice heuristic for careful comparative study. We demonstrate that LRW with easy validation data performs consistently worse than LRW with hard validation data, establishing the validity of our meta-optimization problem. Our proposed algorithm outperforms a wide range of baselines on a range of datasets and domain shift challenges (Ima
    
[^107]: 双通道多重图神经网络用于推荐

    Dual-Channel Multiplex Graph Neural Networks for Recommendation

    [https://arxiv.org/abs/2403.11624](https://arxiv.org/abs/2403.11624)

    该研究提出了一种名为双通道多重图神经网络（DCMGNN）的新型推荐框架，能够有效解决现有推荐方法中存在的多通路关系行为模式建模和对目标关系影响忽略的问题。

    

    高效的推荐系统在准确捕捉反映个人偏好的用户和项目属性方面发挥着至关重要的作用。一些现有的推荐技术已经开始将重点转向在真实世界的推荐场景中对用户和项目之间的各种类型交互关系进行建模，例如在线购物平台上的点击、标记收藏和购买。然而，这些方法仍然面临两个重要的缺点：(1) 不足的建模和利用用户和项目之间多通路关系形成的各种行为模式对表示学习的影响，以及(2) 忽略了行为模式中不同关系对推荐系统场景中目标关系的影响。在本研究中，我们介绍了一种新颖的推荐框架，即双通道多重图神经网络（DCMGNN），该框架解决了上述挑战。

    arXiv:2403.11624v1 Announce Type: cross  Abstract: Efficient recommender systems play a crucial role in accurately capturing user and item attributes that mirror individual preferences. Some existing recommendation techniques have started to shift their focus towards modeling various types of interaction relations between users and items in real-world recommendation scenarios, such as clicks, marking favorites, and purchases on online shopping platforms. Nevertheless, these approaches still grapple with two significant shortcomings: (1) Insufficient modeling and exploitation of the impact of various behavior patterns formed by multiplex relations between users and items on representation learning, and (2) ignoring the effect of different relations in the behavior patterns on the target relation in recommender system scenarios. In this study, we introduce a novel recommendation framework, Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the aforementioned challenges
    
[^108]: 结构化评估合成表格数据

    Structured Evaluation of Synthetic Tabular Data

    [https://arxiv.org/abs/2403.10424](https://arxiv.org/abs/2403.10424)

    提出了一个具有单一数学目标的评估框架，用于确定合成数据应该从与观测数据相同的分布中提取，并且推理了任何一组指标的完整性，统一了现有的指标，并鼓励新的模型无关基线和指标。

    

    表格数据通常存在但往往不完整，数据量较小，并且由于隐私原因受限于访问。合成数据生成提供了潜在解决方案。存在许多用于评估合成表格式数据质量的指标；然而，我们缺乏对这些指标的客观、连贯的解释。为解决这一问题，我们提出了一个具有单一数学目标的评估框架，认为合成数据应该从与观测数据相同的分布中提取。通过对目标的各种结构分解，该框架首次允许我们推理任何一组指标的完整性，并统一现有的指标，包括源自忠实性考虑、下游应用和基于模型方法的指标。此外，该框架激励了无模型基线和一系列新的指标。我们评估了结构化信息合成器和合成器。

    arXiv:2403.10424v1 Announce Type: new  Abstract: Tabular data is common yet typically incomplete, small in volume, and access-restricted due to privacy concerns. Synthetic data generation offers potential solutions. Many metrics exist for evaluating the quality of synthetic tabular data; however, we lack an objective, coherent interpretation of the many metrics. To address this issue, we propose an evaluation framework with a single, mathematical objective that posits that the synthetic data should be drawn from the same distribution as the observed data. Through various structural decomposition of the objective, this framework allows us to reason for the first time the completeness of any set of metrics, as well as unifies existing metrics, including those that stem from fidelity considerations, downstream application, and model-based approaches. Moreover, the framework motivates model-free baselines and a new spectrum of metrics. We evaluate structurally informed synthesizers and syn
    
[^109]: 通过预条件化改善最小二乘问题随机梯度下降的隐式正则化

    Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems

    [https://arxiv.org/abs/2403.08585](https://arxiv.org/abs/2403.08585)

    通过预条件化，研究了SGD在最小二乘问题中的泛化性能，对比了预条件化SGD和（标准和预条件化）岭回归，为改善SGD理解和应用提供了关键贡献。

    

    随机梯度下降（SGD）在实践中表现出强大的算法正则化效果，在现代机器学习的泛化中起着重要作用。然而，先前的研究发现，SGD的泛化性能有时会比岭回归差，这是由于沿不同维度的优化不均匀造成的。预条件化通过重新平衡沿不同方向的优化来提供解决这一问题的自然方法。然而，预条件化能够提升SGD的泛化性能的程度以及它是否能够填补现有与岭回归之间的差距仍不确定。本文研究了预条件化对最小二乘问题中SGD的泛化性能的影响。我们全面比较了预条件化SGD与（标准和预条件化）岭回归。我们的研究对了解和改善SGD做出了几项关键贡献。

    arXiv:2403.08585v1 Announce Type: new  Abstract: Stochastic gradient descent (SGD) exhibits strong algorithmic regularization effects in practice and plays an important role in the generalization of modern machine learning. However, prior research has revealed instances where the generalization performance of SGD is worse than ridge regression due to uneven optimization along different dimensions. Preconditioning offers a natural solution to this issue by rebalancing optimization across different directions. Yet, the extent to which preconditioning can enhance the generalization performance of SGD and whether it can bridge the existing gap with ridge regression remains uncertain. In this paper, we study the generalization performance of SGD with preconditioning for the least squared problem. We make a comprehensive comparison between preconditioned SGD and (standard \& preconditioned) ridge regression. Our study makes several key contributions toward understanding and improving SGD wit
    
[^110]: 大型语言模型的上下文学习中的不确定性分解和量化

    Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models

    [https://arxiv.org/abs/2402.10189](https://arxiv.org/abs/2402.10189)

    本文研究了大型语言模型（LLM）上下文学习中的不确定性，并提出了一种新的方法来量化这种不确定性，包括演示产生的不确定性和模型配置的模糊性。

    

    上下文学习已经成为大型语言模型（LLM）的突破性能力，并通过在提示中提供一些与任务相关的演示来彻底改变了各个领域。然而，LLM响应中的可信问题，如幻觉，也被积极讨论。现有工作致力于量化LLM响应中的不确定性，但往往忽视LLM的复杂性和上下文学习的独特性。在这项工作中，我们深入研究了与上下文学习相关的LLM预测不确定性，并强调这种不确定性可能来自于提供的演示（aleatoric不确定性）和与模型配置相关的模糊性（epistemic不确定性）。我们提出了一种新的公式和相应的估计方法来量化这两种类型的不确定性。该方法为理解上下文学习里的预测提供了一种无监督的方式。

    arXiv:2402.10189v1 Announce Type: new  Abstract: In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-cont
    
[^111]: Lens: 网络流量的基础模型

    Lens: A Foundation Model for Network Traffic

    [https://arxiv.org/abs/2402.03646](https://arxiv.org/abs/2402.03646)

    "Lens"是一个基于T5架构的基础网络流量模型，通过学习大规模无标签数据的预训练表示，能够在流量理解和生成任务中取得精确的预测和生成。

    

    网络流量是指通过互联网或连接计算机的任何系统发送和接收的信息量。分析和理解网络流量对于提高网络安全和管理至关重要。然而，由于数据包的特殊特性，如异构标头和缺乏语义的加密负载，网络流量的分析带来了巨大的挑战。为了捕捉流量的潜在语义，一些研究采用了基于Transformer编码器或解码器的预训练技术，从大规模的流量数据中学习表示。然而，这些方法通常只在流量理解（分类）或流量生成任务中表现出色。为了解决这个问题，我们开发了Lens，这是一个基础的网络流量模型，利用T5架构从大规模的无标签数据中学习预训练表示。借助编码器-解码器框架的优势，该模型能够捕捉全局和局部特征，实现精确的流量预测和生成。

    Network traffic refers to the amount of information being sent and received over the internet or any system that connects computers. Analyzing and understanding network traffic is vital for improving network security and management. However, the analysis of network traffic poses great challenges due to the unique characteristics of data packets, such as heterogeneous headers and encrypted payload lacking semantics. To capture the latent semantics of traffic, a few studies have adopted pre-training techniques based on the Transformer encoder or decoder to learn the representations from large-scale traffic data. However, these methods typically excel only in traffic understanding (classification) or traffic generation tasks. To address this issue, we develop Lens, a foundational network traffic model that leverages the T5 architecture to learn the pre-trained representations from large-scale unlabeled data. Harnessing the strength of the encoder-decoder framework, which captures the glob
    
[^112]: 变分流模型：以你的风格流动

    Variational Flow Models: Flowing in Your Style

    [https://arxiv.org/abs/2402.02977](https://arxiv.org/abs/2402.02977)

    我们引入了一种变分流模型的方法，并提出了一种系统的无需训练的转换方法，使得快速采样成为可能，同时保持了采样的准确性和效率。

    

    我们引入了一种对"后验流"模型进行变分推理解释的方法——用以将"概率流"推广到更广泛的随机过程类别，不必局限于扩散过程。我们将这种结果称为"变分流模型"。此外，我们提出了一种无需训练的系统方法，将由方程Xt = at * X0 + st * X1所描述的"线性"随机过程的后验流转化为直线恒速(SC)流，类似于矫正流。这种转化使得可以快速沿着原始的后验流进行采样，而无需训练一个新的SC流模型。我们的方法的灵活性使我们能够将转换扩展到两个不同"线性"随机过程的后验流之间进行互相转化。此外，我们还可以将高阶数值解法轻松集成到转换后的SC流中，进一步提高采样的准确性和效率。我们进行了严格的理论分析和大量实验结果的验证。

    We introduce a variational inference interpretation for models of "posterior flows" - generalizations of "probability flows" to a broader class of stochastic processes not necessarily diffusion processes. We coin the resulting models as "Variational Flow Models". Additionally, we propose a systematic training-free method to transform the posterior flow of a "linear" stochastic process characterized by the equation Xt = at * X0 + st * X1 into a straight constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation facilitates fast sampling along the original posterior flow without training a new model of the SC flow. The flexibility of our approach allows us to extend our transformation to inter-convert two posterior flows from distinct "linear" stochastic processes. Moreover, we can easily integrate high-order numerical solvers into the transformed SC flow, further enhancing sampling accuracy and efficiency. Rigorous theoretical analysis and extensive experimental result
    
[^113]: CroissantLLM: 一个真正的双语法语-英语语言模型

    CroissantLLM: A Truly Bilingual French-English Language Model

    [https://arxiv.org/abs/2402.00786](https://arxiv.org/abs/2402.00786)

    CroissantLLM是一个1.3B的双语语言模型，通过使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集进行训练，实现了高性能和开源。模型还发布了训练数据集和多个检查点，以及一个法语基准测试 FrenchBench。

    

    我们介绍了CroissantLLM，这是一个在3T个英语和法语标记上预训练的13亿语言模型，为研究和工业社区带来了一种高性能的、完全开源的双语模型，可以在消费级本地硬件上快速运行。为此，我们首次尝试使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集来训练一种内在双语的模型。我们发布了训练数据集，其中包含了一个法语分割，其中包含了手工策划、高质量和多样化的数据源。为了评估在英语以外的性能，我们创建了一个新的基准测试 FrenchBench，包括一系列分类和生成任务，涵盖了模型在法语语言中性能的各个方面。此外，为了保持透明度并促进进一步的大规模语言模型研究，我们发布了代码库和各种模型规模、训练数据分布上的几十个检查点。

    We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, 
    
[^114]: DXAI：通过图像分解解释分类

    DXAI: Explaining Classification by Image Decomposition

    [https://arxiv.org/abs/2401.00320](https://arxiv.org/abs/2401.00320)

    提出一种新的可解释AI方法DXAI，通过将图像分解为类别不可知和类别明显部分来解释和可视化神经网络分类，为解释分类提供了一种根本不同的方式

    

    我们提出了一种通过基于分解的可解释AI（DXAI）来解释和可视化神经网络分类的新方法。我们的方法不是提供解释热图，而是将图像分解为与数据和所选分类器相关的类别不可知和类别明显部分。遵循分析和综合的基本信号处理范式，原始图像是分解部分的总和。因此，我们获得了一种根本不同的解释分类方式。类别不可知部分理想地由不具有类别信息的所有图像特征组成，而类别明显部分则是其补充。在某些情况下，尤其是当属性密集、全局且具有累积性质时，此新可视化可能更有帮助和信息性，例如，当颜色或纹理对于类别区分至关重要时。代码可在https://gith上找到

    arXiv:2401.00320v2 Announce Type: replace-cross  Abstract: We propose a new way to explain and to visualize neural network classification through a decomposition-based explainable AI (DXAI). Instead of providing an explanation heatmap, our method yields a decomposition of the image into class-agnostic and class-distinct parts, with respect to the data and chosen classifier. Following a fundamental signal processing paradigm of analysis and synthesis, the original image is the sum of the decomposed parts. We thus obtain a radically different way of explaining classification. The class-agnostic part ideally is composed of all image features which do not posses class information, where the class-distinct part is its complementary. This new visualization can be more helpful and informative in certain scenarios, especially when the attributes are dense, global and additive in nature, for instance, when colors or textures are essential for class distinction. Code is available at https://gith
    
[^115]: 稀疏线性赌博问题中贪婪适用臂特征分布的新类别

    New Classes of the Greedy-Applicable Arm Feature Distributions in the Sparse Linear Bandit Problem

    [https://arxiv.org/abs/2312.12400](https://arxiv.org/abs/2312.12400)

    本文展示了贪婪算法适用于更广泛范围的臂特征分布，提出了新的适用类别和混合分布概念。

    

    我们考虑稀疏情境赌博问题，其中臂特征通过稀疏参数的内积影响奖励。最近的研究开发了基于贪婪臂选择策略的不考虑稀疏性的算法。然而，对这些算法的分析需要对臂特征分布做出强假设，以确保贪婪选择的样本足够多样化；其中最常见的假设之一是放松对称性，对分布施加了近似原点对称性，这不允许具有原点不对称支持的分布。本文表明，贪婪算法适用于更广泛范围的臂特征分布有两个方面。首先，我们表明具有一个贪婪适用组件的混合分布也是贪婪适用的。其次，我们提出了新的分布类别，与高斯混合、离散和径向分布相关，适用于该类问题。

    arXiv:2312.12400v2 Announce Type: replace  Abstract: We consider the sparse contextual bandit problem where arm feature affects reward through the inner product of sparse parameters. Recent studies have developed sparsity-agnostic algorithms based on the greedy arm selection policy. However, the analysis of these algorithms requires strong assumptions on the arm feature distribution to ensure that the greedily selected samples are sufficiently diverse; One of the most common assumptions, relaxed symmetry, imposes approximate origin-symmetry on the distribution, which cannot allow distributions that has origin-asymmetric support. In this paper, we show that the greedy algorithm is applicable to a wider range of the arm feature distributions from two aspects. Firstly, we show that a mixture distribution that has a greedy-applicable component is also greedy-applicable. Second, we propose new distribution classes, related to Gaussian mixture, discrete, and radial distribution, for which th
    
[^116]: GAvatar：具有隐式网格学习的可动态三维高斯化身

    GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning

    [https://arxiv.org/abs/2312.11461](https://arxiv.org/abs/2312.11461)

    该论文提出了一种基于高斯喷洒的方法，用于从文本描述中生成逼真且可动态的化身，解决了基于网格或NeRF表示的灵活性和效率方面的局限性，通过原始的3D高斯表示和神经隐式场来稳定和改进学习过程。

    

    高斯喷洒已经形成了一种强大的3D表示，充分利用了显式（网格）和隐式（NeRF）3D表示的优势。本文旨在利用高斯喷洒从文本描述中生成逼真且可动态的化身，解决了基于网格或NeRF表示所施加的灵活性和效率等局限性。然而，简单地应用高斯喷洒不能生成高质量的可动态化身，容易产生学习不稳定性；它也不能捕捉到精细的化身几何形状并经常导致退化的身体部分。为解决这些问题，我们首先提出基于原始的3D高斯表示，其中高斯定义在姿势驱动的基本元素内以便促进动画。其次，为了稳定和摊销数百万高斯的学习，我们提出使用神经隐式场来预测高斯属性（例如，...

    arXiv:2312.11461v2 Announce Type: replace-cross  Abstract: Gaussian splatting has emerged as a powerful 3D representation that harnesses the advantages of both explicit (mesh) and implicit (NeRF) 3D representations. In this paper, we seek to leverage Gaussian splatting to generate realistic animatable avatars from textual descriptions, addressing the limitations (e.g., flexibility and efficiency) imposed by mesh or NeRF-based representations. However, a naive application of Gaussian splatting cannot generate high-quality animatable avatars and suffers from learning instability; it also cannot capture fine avatar geometries and often leads to degenerate body parts. To tackle these problems, we first propose a primitive-based 3D Gaussian representation where Gaussians are defined inside pose-driven primitives to facilitate animation. Second, to stabilize and amortize the learning of millions of Gaussians, we propose to use neural implicit fields to predict the Gaussian attributes (e.g., 
    
[^117]: 基于Conformer的极端边缘计算设备上的语音识别

    Conformer-Based Speech Recognition On Extreme Edge-Computing Devices

    [https://arxiv.org/abs/2312.10359](https://arxiv.org/abs/2312.10359)

    提出了在资源受限设备上实现基于Conformer的端到端流式ASR系统的一系列模型架构适配、神经网络图变换和数值优化方法，实现了超过5.26倍实时速度的语音识别，同时最大限度减少能耗并实现了最先进的准确性。

    

    随着今天设备中越来越强大的计算能力和资源，传统上在云端执行的计算密集型自动语音识别(ASR)正从云端转移到设备上以更好地保护用户隐私。然而，在资源受限的设备上实现本地ASR仍然具有挑战性，例如智能手机、智能可穿戴设备和其他小型家居自动化设备。本文提出了一系列模型架构调整、神经网络图变换和数值优化，以在资源受限设备上适配先进的基于Conformer的端到端流式ASR系统，而不降低准确性。我们在小型可穿戴设备上实现了超过实时5.26倍快（0.19 RTF）的语音识别，同时最大限度地减少能耗并实现了最先进的准确性。所提出的方法广泛适用于其他基于变换器的无服务器AI应用。

    arXiv:2312.10359v2 Announce Type: replace  Abstract: With increasingly more powerful compute capabilities and resources in today's devices, traditionally compute-intensive automatic speech recognition (ASR) has been moving from the cloud to devices to better protect user privacy. However, it is still challenging to implement on-device ASR on resource-constrained devices, such as smartphones, smart wearables, and other small home automation devices. In this paper, we propose a series of model architecture adaptions, neural network graph transformations, and numerical optimizations to fit an advanced Conformer based end-to-end streaming ASR system on resource-constrained devices without accuracy degradation. We achieve over 5.26 times faster than realtime (0.19 RTF) speech recognition on small wearables while minimizing energy consumption and achieving state-of-the-art accuracy. The proposed methods are widely applicable to other transformer-based server-free AI applications. In addition
    
[^118]: 终身记忆：利用LLMs回答长篇自我中心视频中的查询

    LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos

    [https://arxiv.org/abs/2312.05269](https://arxiv.org/abs/2312.05269)

    终身记忆是一个新的框架，通过自然语言问答和检索方式访问长篇自我中心视频，利用零-shot能力进行推理，使用置信度和解释模块产生自信、高质量和可解释的答案，在EgoSchema问题回答基准上达到最先进性能，在Ego4D的自然语言查询挑战中具有很强的竞争力

    

    在本文中，我们介绍了终身记忆(LifelongMemory)，这是一个新的框架，通过自然语言问答和检索来访问长篇自我中心视频存储。终身记忆生成摄像机佩戴者的简洁视频活动描述，并利用预训练的大型语言模型的零-shot能力来推理长篇视频内容。此外，终身记忆使用置信度和解释模块来产生自信、高质量和可解释的答案。我们的方法在EgoSchema问题回答基准上实现了最先进的性能，并在Ego4D的自然语言查询(NLQ)挑战中具有很强的竞争力。代码可在https://github.com/Agentic-Learning-AI-Lab/lifelong-memory 中找到。

    arXiv:2312.05269v2 Announce Type: replace-cross  Abstract: In this paper we introduce LifelongMemory, a new framework for accessing long-form egocentric videographic memory through natural language question answering and retrieval. LifelongMemory generates concise video activity descriptions of the camera wearer and leverages the zero-shot capabilities of pretrained large language models to perform reasoning over long-form video context. Furthermore, Lifelong Memory uses a confidence and explanation module to produce confident, high-quality, and interpretable answers. Our approach achieves state-of-the-art performance on the EgoSchema benchmark for question answering and is highly competitive on the natural language query (NLQ) challenge of Ego4D. Code is available at https://github.com/Agentic-Learning-AI-Lab/lifelong-memory.
    
[^119]: 机器人操纵臂的快速电机适应性

    Rapid Motor Adaptation for Robotic Manipulator Arms

    [https://arxiv.org/abs/2312.04670](https://arxiv.org/abs/2312.04670)

    快速电机适应性（RMA）为机器人操作技能的泛化提出了解决方案，利用深度感知开发了针对各种操纵任务的代理。

    

    开发通用机器人操作技能是体现人工智能的核心挑战。这包括跨越各种任务配置的泛化，涵盖了对象形状、密度、摩擦系数的变化以及外部干扰，如施加在机器人身上的力。快速电机适应性（Rapid Motor Adaptation，RMA）为这一挑战提供了一个有前景的解决方案。它认为，影响智能体任务表现的基本隐变量，如对象质量和形状，可以从智能体的动作和本体感性历史中有效地推断出来。我们借鉴了在移动和手内旋转中的RMA，利用深度感知来开发针对各种操纵任务的快速电机适应性代理。我们在Maniskill2基准测试中对我们的代理进行了评估，这些任务包括从YCB和EGAD数据集中数百个对象的取放操作，以及精确定位和

    arXiv:2312.04670v2 Announce Type: replace-cross  Abstract: Developing generalizable manipulation skills is a core challenge in embodied AI. This includes generalization across diverse task configurations, encompassing variations in object shape, density, friction coefficient, and external disturbances such as forces applied to the robot. Rapid Motor Adaptation (RMA) offers a promising solution to this challenge. It posits that essential hidden variables influencing an agent's task performance, such as object mass and shape, can be effectively inferred from the agent's action and proprioceptive history. Drawing inspiration from RMA in locomotion and in-hand rotation, we use depth perception to develop agents tailored for rapid motor adaptation in a variety of manipulation tasks. We evaluated our agents on four challenging tasks from the Maniskill2 benchmark, namely pick-and-place operations with hundreds of objects from the YCB and EGAD datasets, peg insertion with precise position and 
    
[^120]: 利用强化学习和模仿学习的外科医生参与眼科机器人学徒系统研究

    Toward a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using Reinforcement and Imitation Learning

    [https://arxiv.org/abs/2311.17693](https://arxiv.org/abs/2311.17693)

    利用模拟图像引导的以外科医生为中心的自主机器人学徒系统，通过强化学习和模仿学习代理在眼科白内障手术中适应外科医生的技能水平和外科手术偏好。

    

    机器人辅助手术系统在提高手术精确度和减少人为错误方面展示了显著潜力。然而，现有系统缺乏适应个别外科医生的独特偏好和要求的能力。此外，它们主要集中在普通手术（如腹腔镜手术），不适用于非常精密的微创手术，如眼科手术。因此，我们提出了一种基于模拟图像引导的以外科医生为中心的自主机器人学徒系统，可在眼科白内障手术过程中适应个别外科医生的技能水平和首选外科手术技术。我们的方法利用模拟环境来训练以图像数据为指导的强化学习和模仿学习代理，以执行白内障手术的切口阶段所有任务。通过将外科医生的动作和偏好整合到训练过程中，让外科医生参与其中，我们的方法可以达到更好的效果。

    arXiv:2311.17693v2 Announce Type: replace-cross  Abstract: Robotic-assisted surgical systems have demonstrated significant potential in enhancing surgical precision and minimizing human errors. However, existing systems lack the ability to accommodate the unique preferences and requirements of individual surgeons. Additionally, they primarily focus on general surgeries (e.g., laparoscopy) and are not suitable for highly precise microsurgeries, such as ophthalmic procedures. Thus, we propose a simulation-based image-guided approach for surgeon-centered autonomous agents that can adapt to the individual surgeon's skill level and preferred surgical techniques during ophthalmic cataract surgery. Our approach utilizes a simulated environment to train reinforcement and imitation learning agents guided by image data to perform all tasks of the incision phase of cataract surgery. By integrating the surgeon's actions and preferences into the training process with the surgeon-in-the-loop, our ap
    
[^121]: 大型多模态模型的组合式思维提示

    Compositional Chain-of-Thought Prompting for Large Multimodal Models

    [https://arxiv.org/abs/2311.17076](https://arxiv.org/abs/2311.17076)

    提出了一种新颖的零样式思维提示（CCoT），以克服大型多模态模型难以捕捉到组合视觉推理方面的细节的问题。

    

    强大的视觉骨干和大规模语言模型(LLM)推理的结合已经导致大型多模态模型(LMM)成为当前广泛视觉和语言(VL)任务的标准。然而，最近的研究表明，即使是最先进的LMM仍然难以捕捉到组合视觉推理方面的细节，比如对象之间的属性和关系。一种解决方案是利用场景图(SGs)——对象及其关系和属性的形式化表达，它已被广泛用作视觉和文本领域之间的桥梁。然而，场景图数据需要场景图注释，这种数据收集成本高昂，因此难以扩展。此外，基于场景图数据微调LMM可能导致预训练目标的灾难性遗忘。为了克服这一问题，受到思维链方法的启发，我们提出了一种新颖的零样式思维提示（CCoT）。

    arXiv:2311.17076v2 Announce Type: replace-cross  Abstract: The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-sho
    
[^122]: 用于处理带有负值的嘈杂数据的非负矩阵分解算法

    Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values

    [https://arxiv.org/abs/2311.04855](https://arxiv.org/abs/2311.04855)

    本文提出了两种算法，Shift-NMF和Nearly-NMF，可以处理带有负值的嘈杂数据，在不引入正的偏移量的情况下正确恢复非负信号。

    

    非负矩阵分解（NMF）是一种降维技术，在分析嘈杂数据，特别是天文数据方面表现出了潜力。在这些数据集中，由于噪声，观测数据可能包含负值，即使真实的物理信号严格为正。以往的NMF工作未以统计一致的方式处理负数据，这在低信噪比数据中出现许多负值时会变得棘手。在本文中，我们提出了两种算法，Shift-NMF和Nearly-NMF，可以处理输入数据的嘈杂性，并消除任何引入的负值。这两种算法都使用负数据空间而不进行截取，并且在消除负数据时不会引入正的偏移量。我们在简单和更现实的示例上进行了数值演示，并证明了这两种算法具有单调性。

    arXiv:2311.04855v2 Announce Type: replace-cross  Abstract: Non-negative matrix factorization (NMF) is a dimensionality reduction technique that has shown promise for analyzing noisy data, especially astronomical data. For these datasets, the observed data may contain negative values due to noise even when the true underlying physical signal is strictly positive. Prior NMF work has not treated negative data in a statistically consistent manner, which becomes problematic for low signal-to-noise data with many negative values. In this paper we present two algorithms, Shift-NMF and Nearly-NMF, that can handle both the noisiness of the input data and also any introduced negativity. Both of these algorithms use the negative data space without clipping, and correctly recover non-negative signals without any introduced positive offset that occurs when clipping negative data. We demonstrate this numerically on both simple and more realistic examples, and prove that both algorithms have monotoni
    
[^123]: 混合量子图像分类和联邦学习用于肝脂肪变性诊断

    Hybrid quantum image classification and federated learning for hepatic steatosis diagnosis

    [https://arxiv.org/abs/2311.02402](https://arxiv.org/abs/2311.02402)

    该研究旨在开发一种混合量子图像分类和联邦学习算法，以提高肝脂肪变性的诊断准确性，同时解决因数据隐私问题带来的挑战。

    

    在肝移植领域中，准确确定肝脂肪变性水平至关重要。为了提高诊断精度，特别是通过迅速处理易于解决的病例，让专家有更多时间关注复杂病例，本研究旨在开发优化肝活检图像分类的尖端算法。此外，当创建自动化算法解决方案时，保护数据隐私变成了一个挑战，因为医院之间共享患者数据受到限制，进一步复杂化了开发和验证过程。本研究借助于从快速发展的量子机器学习领域引入的新技术来提高诊断准确性，这些技术以其优越的泛化能力而闻名。同时，通过实施隐私意识协作机器学习，还解决了隐私问题。

    arXiv:2311.02402v2 Announce Type: replace  Abstract: In the realm of liver transplantation, accurately determining hepatic steatosis levels is crucial. Recognizing the essential need for improved diagnostic precision, particularly for optimizing diagnosis time by swiftly handling easy-to-solve cases and allowing the expert time to focus on more complex cases, this study aims to develop cutting-edge algorithms that enhance the classification of liver biopsy images. Additionally, the challenge of maintaining data privacy arises when creating automated algorithmic solutions, as sharing patient data between hospitals is restricted, further complicating the development and validation process. This research tackles diagnostic accuracy by leveraging novel techniques from the rapidly evolving field of quantum machine learning, known for their superior generalization abilities. Concurrently, it addresses privacy concerns through the implementation of privacy-conscious collaborative machine lear
    
[^124]: 多配体对接和结合位点设计的谐波自调流匹配

    Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and Binding Site Design

    [https://arxiv.org/abs/2310.05764](https://arxiv.org/abs/2310.05764)

    该研究开发了一种谐波自调流匹配方法，在多配体对接和结合位点设计中表现出比现有方法更好的生成过程和设计效果。

    

    大量蛋白质功能需要与小分子结合，包括酶催化。因此，为小分子设计结合口袋具有从药物合成到能量存储等多种影响深远的应用。为实现这一目标，我们首先开发了HarmonicFlow，这是一个改进的基于自调流匹配目标的3D蛋白质-配体结合结构生成过程。FlowSite将这种流模型扩展到联合生成蛋白质口袋的离散残基类型和分子的结合3D结构。我们展示了HarmonicFlow在口袋级对接中在简单性、普适性和平均样本质量上优于最先进的生成过程。借助于这种结构建模，FlowSite设计的结合位点明显优于基线方法。

    arXiv:2310.05764v3 Announce Type: replace-cross  Abstract: A significant amount of protein function requires binding small molecules, including enzymatic catalysis. As such, designing binding pockets for small molecules has several impactful applications ranging from drug synthesis to energy storage. Towards this goal, we first develop HarmonicFlow, an improved generative process over 3D protein-ligand binding structures based on our self-conditioned flow matching objective. FlowSite extends this flow model to jointly generate a protein pocket's discrete residue types and the molecule's binding 3D structure. We show that HarmonicFlow improves upon state-of-the-art generative processes for docking in simplicity, generality, and average sample quality in pocket-level docking. Enabled by this structure modeling, FlowSite designs binding sites substantially better than baseline approaches.
    
[^125]: DFWLayer: 可微化的Frank-Wolfe优化层

    DFWLayer: Differentiable Frank-Wolfe Optimization Layer

    [https://arxiv.org/abs/2308.10806](https://arxiv.org/abs/2308.10806)

    本文提出了一种名为Differentiable Frank-Wolfe Layer（DFWLayer）的可微层，通过推出Frank-Wolfe方法，有效处理具有范数约束的大规模凸优化问题，并在解和梯度准确性方面表现竞争性。

    

    不同iable优化因其在基于神经网络的机器学习领域中的基础作用而受到广泛关注。本文通过推出Frank-Wolfe方法提出了一种可微层，命名为Differentiable Frank-Wolfe Layer（DFWLayer），该方法是一种可以在不进行投影和Hessian矩阵计算的情况下解决约束优化问题的著名优化算法，从而有效地处理具有范数约束的大规模凸优化问题。实验结果表明，DFWLayer不仅在解和梯度精度上表现竞争性，而且始终遵守约束条件。

    arXiv:2308.10806v2 Announce Type: replace-cross  Abstract: Differentiable optimization has received a significant amount of attention due to its foundational role in the domain of machine learning based on neural networks. This paper proposes a differentiable layer, named Differentiable Frank-Wolfe Layer (DFWLayer), by rolling out the Frank-Wolfe method, a well-known optimization algorithm which can solve constrained optimization problems without projections and Hessian matrix computations, thus leading to an efficient way of dealing with large-scale convex optimization problems with norm constraints. Experimental results demonstrate that the DFWLayer not only attains competitive accuracy in solutions and gradients but also consistently adheres to constraints.
    
[^126]: 高效、可解释和安全的箱子操纵：展示模型预测控制中物理先验的优势

    Data-efficient, Explainable and Safe Box Manipulation: Illustrating the Advantages of Physical Priors in Model-Predictive Control

    [https://arxiv.org/abs/2303.01563](https://arxiv.org/abs/2303.01563)

    本论文旨在通过一个真实机器人系统案例研究，展示在模型预测控制中利用环境动态先验知识可以提高可解释性、安全性和数据效率的优点

    

    基于模型的强化学习/控制在机器人领域获得了显著的关注。然而，这些方法通常在数据效率和缺乏手工设计解决方案的可解释性方面存在问题。这使得它们在安全关键环境中难以调试/集成。然而，在许多系统中，环境运动学/动力学的先验知识是可用的。通过整合这些先验知识，可以通过减少问题复杂性和探索需求，同时促进代理所做决策以物理上有意义的实体来表达，有助于解决上述问题。我们的目标是通过一个案例研究来阐明和支持这一观点。我们基于一个真实机器人系统模拟了一个载荷操纵问题，并展示了在 MPC 框架中利用关于环境动态的先验知识可以改善可解释性、安全性和数据效率，从而实现更安全、更高效的操纵。

    arXiv:2303.01563v2 Announce Type: replace-cross  Abstract: Model-based RL/control have gained significant traction in robotics. Yet, these approaches often remain data-inefficient and lack the explainability of hand-engineered solutions. This makes them difficult to debug/integrate in safety-critical settings. However, in many systems, prior knowledge of environment kinematics/dynamics is available. Incorporating such priors can help address the aforementioned problems by reducing problem complexity and the need for exploration, while also facilitating the expression of the decisions taken by the agent in terms of physically meaningful entities. Our aim with this paper is to illustrate and support this point of view via a case-study. We model a payload manipulation problem based on a real robotic system, and show that leveraging prior knowledge about the dynamics of the environment in an MPC framework can lead to improvements in explainability, safety and data-efficiency, leading to sa
    
[^127]: 关注图转换器

    Attending to Graph Transformers

    [https://arxiv.org/abs/2302.04181](https://arxiv.org/abs/2302.04181)

    提出了一种图转换器架构的分类法，概述了其理论性质，调查了结构和位置编码，并探讨了对重要图类的扩展，如3D分子图。

    

    最近，用于图形的转换器架构作为机器学习的替代技术出现，例如（消息传递）图神经网络。到目前为止，它们已经展示出有希望的实证结果，例如在分子预测数据集上，通常归因于其绕过图神经网络的缺点，如过度平滑和过度压缩。在这里，我们提出了一种图转换器架构的分类法，为这个新兴领域带来了一些秩序。我们概述它们的理论性质，调查结构和位置编码，并讨论了对重要图类的扩展，例如3D分子图。在实证方面，我们研究图转换器如何恢复各种图属性，如何处理异性图，以及它们在多大程度上可以防止过度压缩。此外，我们概述了未解决的挑战和研究方向，以促进未来工作。

    arXiv:2302.04181v3 Announce Type: replace-cross  Abstract: Recently, transformer architectures for graphs emerged as an alternative to established techniques for machine learning with graphs, such as (message-passing) graph neural networks. So far, they have shown promising empirical results, e.g., on molecular prediction datasets, often attributed to their ability to circumvent graph neural networks' shortcomings, such as over-smoothing and over-squashing. Here, we derive a taxonomy of graph transformer architectures, bringing some order to this emerging field. We overview their theoretical properties, survey structural and positional encodings, and discuss extensions for important graph classes, e.g., 3D molecular graphs. Empirically, we probe how well graph transformers can recover various graph properties, how well they can deal with heterophilic graphs, and to what extent they prevent over-squashing. Further, we outline open challenges and research direction to stimulate future wo
    
[^128]: 完全欧几里得图的完全神经网络

    Complete Neural Networks for Complete Euclidean Graphs

    [https://arxiv.org/abs/2301.13821](https://arxiv.org/abs/2301.13821)

    神经网络模型可以通过应用3-WL图同构测试于点云的Gram矩阵，或者应用欧几里得2-WL测试，在多项式复杂度内实现对完全欧几里得图的完全确定性

    

    最近，在对点云建模时，神经网络已经取得了成功，因为它们尊重排列和刚性运动的自然不变性，从分子动力学到推荐系统，从几何现象到推荐系统。然而，迄今未知具有多项式复杂性的模型是完备的，即能够区分任意一对非同构点云。我们通过展示可以将点云完全确定，直到排列和刚性运动，通过将3-WL图同构测试应用于点云的集中Gram矩阵来填补这一理论空白。此外，我们制定了2-WL测试的欧几里得变体，并展示它也足以实现完整性。然后，我们展示了如何通过中等大小的欧几里得图神经网络模拟我们的完全欧几里得WL测试，并展示了它们在高度对称的点云上的分离能力。

    arXiv:2301.13821v3 Announce Type: replace  Abstract: Neural networks for point clouds, which respect their natural invariance to permutation and rigid motion, have enjoyed recent success in modeling geometric phenomena, from molecular dynamics to recommender systems. Yet, to date, no model with polynomial complexity is known to be complete, that is, able to distinguish between any pair of non-isomorphic point clouds. We fill this theoretical gap by showing that point clouds can be completely determined, up to permutation and rigid motion, by applying the 3-WL graph isomorphism test to the point cloud's centralized Gram matrix. Moreover, we formulate an Euclidean variant of the 2-WL test and show that it is also sufficient to achieve completeness. We then show how our complete Euclidean WL tests can be simulated by an Euclidean graph neural network of moderate size and demonstrate their separation capability on highly symmetrical point clouds.
    
[^129]: 限制在芯片架构上保持量子神经网络的准确性

    Restricting to the chip architecture maintains the quantum neural network accuracy

    [https://arxiv.org/abs/2212.14426](https://arxiv.org/abs/2212.14426)

    该研究旨在探讨量子神经网络在量子芯片架构上表现出的准确性，并发现成本函数往往会收敛到一个平均值。

    

    在噪声中等规模量子设备的时代，变分量子算法（VQAs）作为构建量子机器学习模型的显著策略。这些模型包括量子部分和经典部分。量子部分通过参数化 $U$ 来表征，通常由各种量子门的组合得到。另一方面，经典部分涉及一个优化器，调节 $U$ 的参数以最小化成本函数 $C$。尽管VQAs有广泛的应用，但仍然存在一些关键问题，比如确定最佳门序列、设计高效的参数优化策略、选择合适的成本函数，以及了解量子芯片架构对最终结果的影响。本文旨在解决最后一个问题，并强调，一般情况下，成本函数倾向于收敛到一个平均值。

    arXiv:2212.14426v2 Announce Type: replace-cross  Abstract: In the era of noisy intermediate-scale quantum devices, variational quantum algorithms (VQAs) stand as a prominent strategy for constructing quantum machine learning models. These models comprise both a quantum and a classical component. The quantum facet is characterized by a parametrization $U$, typically derived from the composition of various quantum gates. On the other hand, the classical component involves an optimizer that adjusts the parameters of $U$ to minimize a cost function $C$. Despite the extensive applications of VQAs, several critical questions persist, such as determining the optimal gate sequence, devising efficient parameter optimization strategies, selecting appropriate cost functions, and understanding the influence of quantum chip architectures on the final results. This article aims to address the last question, emphasizing that, in general, the cost function tends to converge towards an average value as
    
[^130]: 通过集成渐进正态分布学习进行强可转移对抗攻击

    Strong Transferable Adversarial Attacks via Ensembled Asymptotically Normal Distribution Learning

    [https://arxiv.org/abs/2209.11964](https://arxiv.org/abs/2209.11964)

    本文提出了一种名为多渐进正态分布攻击（MultiANDA）的方法，通过学习的分布明确刻画对抗性扰动，利用随机梯度上升的渐进正态性质来近似扰动的后验分布，并采用深度集成策略作为贝叶斯模型的有效代理。

    

    强可转移对抗样本对于评估和增强深度神经网络的鲁棒性至关重要。然而，流行攻击的性能通常对带有限信息的微小图像转换敏感，典型地只有一个输入示例、几个白盒源模型和未定义的防御策略。因此，精心设计的对抗样本很容易过度拟合源模型，从而阻碍它们对未知架构的可转移性。在本文中，我们提出了一种称为多渐进正态分布攻击（MultiANDA）的方法，它明确地从一个学习到的分布中表征对抗性扰动。具体地，我们利用随机梯度上升（SGA）的渐进正态性质来近似涉及扰动的后验分布，然后使用深度集成策略作为贝叶斯模型的有效代理。

    arXiv:2209.11964v2 Announce Type: replace  Abstract: Strong adversarial examples are crucial for evaluating and enhancing the robustness of deep neural networks. However, the performance of popular attacks is usually sensitive, for instance, to minor image transformations, stemming from limited information -- typically only one input example, a handful of white-box source models, and undefined defense strategies. Hence, the crafted adversarial examples are prone to overfit the source model, which hampers their transferability to unknown architectures. In this paper, we propose an approach named Multiple Asymptotically Normal Distribution Attacks (MultiANDA) which explicitly characterize adversarial perturbations from a learned distribution. Specifically, we approximate the posterior distribution over the perturbations by taking advantage of the asymptotic normality property of stochastic gradient ascent (SGA), then employ the deep ensemble strategy as an effective proxy for Bayesian ma
    
[^131]: 具有有限训练任务的元强化学习--一种密度估计方法

    Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach

    [https://arxiv.org/abs/2206.10716](https://arxiv.org/abs/2206.10716)

    通过密度估计技术直接学习任务分布，从而对元强化学习中所需训练任务数提出了新的界限分析方法

    

    在元强化学习(meta RL)中，一个智能体通过一组训练任务学习如何快速解决一个新任务，新任务来自相同的任务分布。优化的元RL策略，即贝叶斯最优行为，是明确定义的，并保证期望下相对于任务分布的最优奖励。我们在这项工作中探讨的问题是需要多少个训练任务才能保证以高概率近似地获得最优行为。最近的工作为模型无关设置提供了第一个这样的PAC分析，其中从训练任务中学习了一种历史相关的策略。在这项工作中，我们提出了一种不同的方法：直接学习任务分布，使用密度估计技术，然后在学习的任务分布上训练策略。我们展示了我们的方法导致依赖于任务分布维度的界限。特别是，在任务分布维度较大的情况下。

    arXiv:2206.10716v2 Announce Type: replace-cross  Abstract: In meta reinforcement learning (meta RL), an agent learns from a set of training tasks how to quickly solve a new task, drawn from the same task distribution. The optimal meta RL policy, a.k.a. the Bayes-optimal behavior, is well defined, and guarantees optimal reward in expectation, taken with respect to the task distribution. The question we explore in this work is how many training tasks are required to guarantee approximately optimal behavior with high probability. Recent work provided the first such PAC analysis for a model-free setting, where a history-dependent policy was learned from the training tasks. In this work, we propose a different approach: directly learn the task distribution, using density estimation techniques, and then train a policy on the learned task distribution. We show that our approach leads to bounds that depend on the dimension of the task distribution. In particular, in settings where the task dis
    
[^132]: QAGCN：通过对知识图谱进行单步隐式推理回答多关系问题

    QAGCN: Answering Multi-Relation Questions via Single-Step Implicit Reasoning over Knowledge Graphs

    [https://arxiv.org/abs/2206.01818](https://arxiv.org/abs/2206.01818)

    本文提出了 QAGCN 方法，通过对问题进行感知来实现单步隐式推理，从而回答多关系问题，相比于显式多步推理方法，该方法更简单、高效且易于采用。

    

    多关系问题回答（QA）是一项具有挑战性的任务，通常需要在由多个关系组成的知识图谱中进行长时间推理链的问题。最近，在这一任务中明显使用了基于知识图谱的显式多步推理方法，并展现出了良好的性能。这些方法包括通过知识图谱三元组逐步标签传播的方法以及基于强化学习浏览知识图谱三元组的方法。这些方法的一个主要弱点是它们的推理机制通常复杂且难以实现或训练。在本文中，我们认为可以通过端到端单步隐式推理实现多关系QA，这种方法更简单、更高效且更易于采用。我们提出了 QAGCN -- 一种基于问题意识的图卷积网络（GCN）方法，其中包括一种新颖的具有受控问题相关信息传播的GCN架构。

    arXiv:2206.01818v3 Announce Type: replace  Abstract: Multi-relation question answering (QA) is a challenging task, where given questions usually require long reasoning chains in KGs that consist of multiple relations. Recently, methods with explicit multi-step reasoning over KGs have been prominently used in this task and have demonstrated promising performance. Examples include methods that perform stepwise label propagation through KG triples and methods that navigate over KG triples based on reinforcement learning. A main weakness of these methods is that their reasoning mechanisms are usually complex and difficult to implement or train. In this paper, we argue that multi-relation QA can be achieved via end-to-end single-step implicit reasoning, which is simpler, more efficient, and easier to adopt. We propose QAGCN -- a Question-Aware Graph Convolutional Network (GCN)-based method that includes a novel GCN architecture with controlled question-dependent message propagation for the 
    
[^133]: 基于图依赖性的学习泛化界限：一份调查

    Generalization bounds for learning under graph-dependence: A survey

    [https://arxiv.org/abs/2203.13534](https://arxiv.org/abs/2203.13534)

    通过收集各种图依赖性的集中界限，该研究推导出了用于基于图依赖性数据学习的Rademacher复杂度和稳定性的学习泛化界限。

    

    传统统计学习理论依赖于数据独立同分布（i.i.d.）的假设。然而，在许多现实应用中，这种假设往往不成立。本调查中，我们探讨了学习场景中示例之间存在依赖关系，并且这种依赖关系由依赖图描述，这是概率论和组合数学中常用的模型。我们收集了各种图依赖性的集中界限，然后用它们来推导基于Rademacher复杂度和稳定性的学习泛化界限，以适用于基于图依赖性数据的学习。我们通过实际学习任务来说明这一范式，并为未来的研究方向提供了一些建议。据我们所知，这份调查是该主题上第一份此类调查。

    arXiv:2203.13534v2 Announce Type: replace  Abstract: Traditional statistical learning theory relies on the assumption that data are identically and independently distributed (i.i.d.). However, this assumption often does not hold in many real-life applications. In this survey, we explore learning scenarios where examples are dependent and their dependence relationship is described by a dependency graph, a commonly utilized model in probability and combinatorics. We collect various graph-dependent concentration bounds, which are then used to derive Rademacher complexity and stability generalization bounds for learning from graph-dependent data. We illustrate this paradigm through practical learning tasks and provide some research directions for future work. To our knowledge, this survey is the first of this kind on this subject.
    
[^134]: 在具有摇臂反馈的强单调博弈中的双重最优无悔在线学习

    Doubly Optimal No-Regret Online Learning in Strongly Monotone Games with Bandit Feedback

    [https://arxiv.org/abs/2112.02856](https://arxiv.org/abs/2112.02856)

    该论文提出了一种在强单调博弈中具有摇臂反馈的双重最优无悔在线学习方法，并展示了在特定条件下的最优后悔和联合动作收敛到纳什均衡点的速率的结果。

    

    我们研究了在具有摇臂反馈的未知博弈中的在线无悔学习，其中每个玩家只能观察到每个时间点的奖励 -- 由所有玩家当前的联合动作确定 -- 而不是梯度。我们专注于\textit{光滑且强单调}博弈类，并研究其中的最优无悔学习。利用自共轭障碍函数，我们首先构建了一个新的摇臂学习算法，并展示它在光滑和强凹性奖励函数下实现了$\tilde{\Theta}(n\sqrt{T})$的单一代理最优后悔（$n \geq 1$是问题维度）。然后我们展示，如果每个玩家在强单调博弈中应用这个无悔学习算法，联合动作在\textit{最后迭代}中以$\tilde{\Theta}(nT^{-1/2})$的速率收敛到唯一的纳什均衡点。在我们的工作之前，在相同类别的博弈中，最佳已知的收敛速率为$\tilde{O}(n^{2/3}T^{-1/3})。

    arXiv:2112.02856v4 Announce Type: replace  Abstract: We consider online no-regret learning in unknown games with bandit feedback, where each player can only observe its reward at each time -- determined by all players' current joint action -- rather than its gradient. We focus on the class of \textit{smooth and strongly monotone} games and study optimal no-regret learning therein. Leveraging self-concordant barrier functions, we first construct a new bandit learning algorithm and show that it achieves the single-agent optimal regret of $\tilde{\Theta}(n\sqrt{T})$ under smooth and strongly concave reward functions ($n \geq 1$ is the problem dimension). We then show that if each player applies this no-regret learning algorithm in strongly monotone games, the joint action converges in the \textit{last iterate} to the unique Nash equilibrium at a rate of $\tilde{\Theta}(nT^{-1/2})$. Prior to our work, the best-known convergence rate in the same class of games is $\tilde{O}(n^{2/3}T^{-1/3})
    
[^135]: 使用投影Wasserstein距离的双样本检验

    Two-sample Test using Projected Wasserstein Distance

    [https://arxiv.org/abs/2010.11970](https://arxiv.org/abs/2010.11970)

    提出了一种使用投影Wasserstein距离进行双样本检验的方法，可以避免高维度情况下的测试能力减弱问题，并通过最优投影和低维线性映射最大化Wasserstein距离来实现。

    

    我们开发了一种使用投影Wasserstein距离进行双样本检验的方法，这是统计学和机器学习中的一个基本问题：给定两组样本，确定它们是否来自同一分布。我们的目标是避免Wasserstein距离中的维度灾难：当维度很高时，它的测试能力会显著减弱，这主要是由于高维空间中Wasserstein度量的缓慢集中特性所致。一个关键贡献是将最优投影耦合在一起，找到低维线性映射以最大化投影概率分布之间的Wasserstein距离。我们刻画了IPM中有限样本收敛速率的理论特性，并提出了用于计算该度量的实际算法。数值实例验证了我们的理论结果。

    arXiv:2010.11970v4 Announce Type: replace-cross  Abstract: We develop a projected Wasserstein distance for the two-sample test, a fundamental problem in statistics and machine learning: given two sets of samples, to determine whether they are from the same distribution. In particular, we aim to circumvent the curse of dimensionality in Wasserstein distance: when the dimension is high, it has diminishing testing power, which is inherently due to the slow concentration property of Wasserstein metrics in the high dimension space. A key contribution is to couple optimal projection to find the low dimensional linear mapping to maximize the Wasserstein distance between projected probability distributions. We characterize the theoretical property of the finite-sample convergence rate on IPMs and present practical algorithms for computing this metric. Numerical examples validate our theoretical results.
    
[^136]: 通过多样性启示的多Agent诊断方法用于稳健性

    Multi-Agent Diagnostics for Robustness via Illuminated Diversity. (arXiv:2401.13460v1 [cs.LG])

    [http://arxiv.org/abs/2401.13460](http://arxiv.org/abs/2401.13460)

    MADRID是一种新方法，通过生成多样化的对抗场景来揭示预训练多Agent策略的战略漏洞，并通过遗憾值衡量漏洞的程度。

    

    在快速发展的多Agent系统领域中，确保在陌生和敌对环境中的稳健性至关重要。尽管这些系统在熟悉环境中表现出色，但在新情况下往往会因为训练阶段的过拟合而失败。在既包含合作又包含竞争行为的环境中，这一问题尤为突出，体现了过拟合和泛化挑战的双重性质。为了解决这个问题，我们提出了通过多样性启示的多Agent稳健性诊断（MADRID），这是一种生成多Agent策略中暴露战略漏洞的多样化对抗场景的新方法。MADRID利用开放式学习的概念，导航对抗环境的广阔空间，使用目标策略的遗憾值来衡量这些环境的漏洞。我们在11vs11版的Google Research Football上评估了MADRID的有效性。

    In the rapidly advancing field of multi-agent systems, ensuring robustness in unfamiliar and adversarial settings is crucial. Notwithstanding their outstanding performance in familiar environments, these systems often falter in new situations due to overfitting during the training phase. This is especially pronounced in settings where both cooperative and competitive behaviours are present, encapsulating a dual nature of overfitting and generalisation challenges. To address this issue, we present Multi-Agent Diagnostics for Robustness via Illuminated Diversity (MADRID), a novel approach for generating diverse adversarial scenarios that expose strategic vulnerabilities in pre-trained multi-agent policies. Leveraging the concepts from open-ended learning, MADRID navigates the vast space of adversarial settings, employing a target policy's regret to gauge the vulnerabilities of these settings. We evaluate the effectiveness of MADRID on the 11vs11 version of Google Research Football, one o
    
[^137]: 单一GPU上的数据高效多模态融合

    Data-Efficient Multimodal Fusion on a Single GPU. (arXiv:2312.10144v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.10144](http://arxiv.org/abs/2312.10144)

    本论文提出了一种在单一GPU上进行数据高效多模态融合的方法，通过使用预训练的单模态编码器的潜在空间，我们在多模态对齐中取得了有竞争力的性能，且计算和数据量减少了数个数量级。

    

    多模态对齐的目标是学习共享多模态输入之间的单一潜在空间。在这个领域中，最强大的模型通常是使用大规模数据集和大规模计算资源进行训练的，因此在许多实际场景中训练这些模型的成本非常高昂。我们推测，现有的在大量单模态数据上预训练的单模态编码器应该能够以更低的成本从单模态模型中创建多模态模型。因此，我们提出了FuseMix，一种多模态增强方案，该方案在任意预训练的单模态编码器的潜在空间中操作。通过使用FuseMix进行多模态对齐，我们在图像-文本和音频-文本检索任务中取得了有竞争力的性能，并在某些情况下超越了最先进的方法，而计算和数据量减少了数个数量级：例如，我们在Flickr30K的文本-图像检索任务中比CLIP的性能提高了约600倍，而计算和数据量减少了数个数量级。

    The goal of multimodal alignment is to learn a single latent space that is shared between multimodal inputs. The most powerful models in this space have been trained using massive datasets of paired inputs and large-scale computational resources, making them prohibitively expensive to train in many practical scenarios. We surmise that existing unimodal encoders pre-trained on large amounts of unimodal data should provide an effective bootstrap to create multimodal models from unimodal ones at much lower costs. We therefore propose FuseMix, a multimodal augmentation scheme that operates on the latent spaces of arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal alignment, we achieve competitive performance -- and in certain cases outperform state-of-the art methods -- in both image-text and audio-text retrieval, with orders of magnitude less compute and data: for example, we outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \! 600\times$ fewer GP
    
[^138]: LipSim: 一种可证明鲁棒的知觉相似度度量方法

    LipSim: A Provably Robust Perceptual Similarity Metric. (arXiv:2310.18274v1 [cs.CV])

    [http://arxiv.org/abs/2310.18274](http://arxiv.org/abs/2310.18274)

    LipSim是一个可证明鲁棒的知觉相似度度量方法，通过利用1-Lipschitz神经网络作为骨干，提供了围绕度量方法的防护区域。

    

    近年来，人们对于开发和应用知觉相似度度量方法表现出了越来越大的兴趣。研究表明，相较于像素级度量方法，知觉度量方法在与人类感知的一致性和作为人类视觉系统的代理方面具有更高的优势。然而，由于知觉度量方法依赖于神经网络，对神经网络对抗性攻击脆弱性的关注也日益增长。我们展示了基于ViT的特征提取器的最新知觉相似度度量方法对抗攻击的脆弱性，并提出了一个名为LipSim（Lipschitz相似度度量方法）的鲁棒知觉相似度度量框架。通过利用1-Lipschitz神经网络作为骨干，LipSim提供了围绕着度量方法的防护区域。

    Recent years have seen growing interest in developing and applying perceptual similarity metrics. Research has shown the superiority of perceptual metrics over pixel-wise metrics in aligning with human perception and serving as a proxy for the human visual system. On the other hand, as perceptual metrics rely on neural networks, there is a growing concern regarding their resilience, given the established vulnerability of neural networks to adversarial attacks. It is indeed logical to infer that perceptual metrics may inherit both the strengths and shortcomings of neural networks. In this work, we demonstrate the vulnerability of state-of-the-art perceptual similarity metrics based on an ensemble of ViT-based feature extractors to adversarial attacks. We then propose a framework to train a robust perceptual similarity metric called LipSim (Lipschitz Similarity Metric) with provable guarantees. By leveraging 1-Lipschitz neural networks as the backbone, LipSim provides guarded areas aroun
    
[^139]: 在具有梯度反馈的强单调和指数凸博弈中的自适应、双重最优无悔学习

    Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and Exp-Concave Games with Gradient Feedback. (arXiv:2310.14085v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2310.14085](http://arxiv.org/abs/2310.14085)

    本文提出了一个自适应的OGD算法\textsf{AdaOGD}，在强凸性下实现了$ O(\log^2(T)) $的后悔，并且在强单调博弈中使得联合行动最后一次收敛到唯一的纳什均衡。

    

    在强凸性或单调性假设下，网上梯度下降（OGD）被广泛认为是双重最优的：（1）在单个代理设置中，对于强凸成本函数，它实现了$ \Theta(\log T) $的最优后悔；（2）在具有强单调性的多代理博弈的情况下，每个代理使用OGD，我们获得了关于联合行动的最后一次收敛到唯一纳什均衡的最优速率$ \Theta(\frac{1}{T}) $。尽管这些有限时间的保证突出了其优点，但OGD的缺点是需要知道强凸性/单调性的参数。在本文中，我们设计了一个完全自适应的OGD算法\textsf{AdaOGD}，它不需要先验的知识这些参数。在单个代理设置中，我们的算法在强凸性下实现了$ O(\log^2(T)) $的后悔，这是最优的除了一个对数因子。此外，如果在强单调博弈中每个代理都使用\textsf{AdaOGD}，则联合行动收敛到最后一个迭代时的一次。

    Online gradient descent (OGD) is well known to be doubly optimal under strong convexity or monotonicity assumptions: (1) in the single-agent setting, it achieves an optimal regret of $\Theta(\log T)$ for strongly convex cost functions; and (2) in the multi-agent setting of strongly monotone games, with each agent employing OGD, we obtain last-iterate convergence of the joint action to a unique Nash equilibrium at an optimal rate of $\Theta(\frac{1}{T})$. While these finite-time guarantees highlight its merits, OGD has the drawback that it requires knowing the strong convexity/monotonicity parameters. In this paper, we design a fully adaptive OGD algorithm, \textsf{AdaOGD}, that does not require a priori knowledge of these parameters. In the single-agent setting, our algorithm achieves $O(\log^2(T))$ regret under strong convexity, which is optimal up to a log factor. Further, if each agent employs \textsf{AdaOGD} in strongly monotone games, the joint action converges in a last-iterate s
    
[^140]: 透明度挑战与因果机器学习中的政策评估 - 提高可用性和问责性

    Transparency challenges in policy evaluation with causal machine learning -- improving usability and accountability. (arXiv:2310.13240v1 [cs.LG])

    [http://arxiv.org/abs/2310.13240](http://arxiv.org/abs/2310.13240)

    透明度问题是因果机器学习在政策评估中的挑战，因为黑盒子模型难以理解和问责。本文提出了通过可解释的AI工具和简化模型来解决这些问题的方法。

    

    因果机器学习工具开始在实际政策评估任务中使用，灵活估计治疗效果。这些方法的一个问题是所使用的机器学习模型通常是黑盒子，即没有全局可解释的方式来理解模型如何进行估计。这在政策评估应用中是一个明显的问题，特别是在政府领域，因为很难理解这些模型是否按照公正的方式运行，基于正确的证据解释，并且透明到足以允许在出现问题时进行问责。然而，在因果机器学习文献中很少讨论透明度问题以及如何解决这些问题。本文探讨了为什么透明度问题是因果机器学习在公共政策评估应用中的问题，并考虑通过可解释的AI工具和简化模型来解决这些问题的方法。

    Causal machine learning tools are beginning to see use in real-world policy evaluation tasks to flexibly estimate treatment effects. One issue with these methods is that the machine learning models used are generally black boxes, i.e., there is no globally interpretable way to understand how a model makes estimates. This is a clear problem in policy evaluation applications, particularly in government, because it is difficult to understand whether such models are functioning in ways that are fair, based on the correct interpretation of evidence and transparent enough to allow for accountability if things go wrong. However, there has been little discussion of transparency problems in the causal machine learning literature and how these might be overcome. This paper explores why transparency issues are a problem for causal machine learning in public policy evaluation applications and considers ways these problems might be addressed through explainable AI tools and by simplifying models in
    
[^141]: 会话式金融信息检索模型（ConFIRM）

    Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])

    [http://arxiv.org/abs/2310.13001](http://arxiv.org/abs/2310.13001)

    ConFIRM是一种会话式金融信息检索模型，通过合成金融领域特定问答对和评估参数微调方法，实现了超过90%的准确性，为金融对话系统提供了数据高效的解决方案。

    

    随着大型语言模型（LLM）的指数级增长，利用它们在金融等专门领域的新兴特性具有探索的价值。然而，金融等受监管领域具有独特的约束条件，需要具备针对该领域的优化框架。我们提出了ConFIRM，一种基于LLM的会话式金融信息检索模型，用于查询意图分类和知识库标记。ConFIRM包括两个模块：1）一种合成金融领域特定问答对的方法，以及2）评估参数高效的微调方法来进行查询分类任务。我们生成了一个包含4000多个样本的数据集，并在单独的测试集上评估了准确性。ConFIRM实现了超过90%的准确性，这对于符合监管要求至关重要。ConFIRM提供了一种数据高效的解决方案，用于提取金融对话系统的精确查询意图。

    With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
    
[^142]: 在高斯混合模型空间中引入了类似于Gromov-Wassertein的距离

    Gromov-Wassertein-like Distances in the Gaussian Mixture Models Space. (arXiv:2310.11256v1 [stat.ML])

    [http://arxiv.org/abs/2310.11256](http://arxiv.org/abs/2310.11256)

    本文介绍了两种在高斯混合模型空间中的Gromov-Wasserstein类型距离，分别用于评估分布之间的距离和推导最优的点分配方案。

    

    本文介绍了两种在高斯混合模型集合上的Gromov-Wasserstein类型距离。第一种距离是在高斯测度空间上两个离散分布的Gromov-Wasserstein距离。该距离可以作为Gromov-Wasserstein的替代，用于评估分布之间的距离，但不能直接推导出最优的运输方案。为了设计出这样的运输方案，我们引入了另一种在不可比较的空间中的测度之间的距离，该距离与Gromov-Wasserstein密切相关。当将允许的运输耦合限制为高斯混合模型时，这定义了另一种高斯混合模型之间的距离，可以作为Gromov-Wasserstein的另一种替代，并允许推导出最优的点分配方案。

    In this paper, we introduce two Gromov-Wasserstein-type distances on the set of Gaussian mixture models. The first one takes the form of a Gromov-Wasserstein distance between two discrete distributionson the space of Gaussian measures. This distance can be used as an alternative to Gromov-Wasserstein for applications which only require to evaluate how far the distributions are from each other but does not allow to derive directly an optimal transportation plan between clouds of points. To design a way to define such a transportation plan, we introduce another distance between measures living in incomparable spaces that turns out to be closely related to Gromov-Wasserstein. When restricting the set of admissible transportation couplings to be themselves Gaussian mixture models in this latter, this defines another distance between Gaussian mixture models that can be used as another alternative to Gromov-Wasserstein and which allows to derive an optimal assignment between points. Finally,
    
[^143]: GTA：一种面向几何的多视图Transformer的注意力机制

    GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers. (arXiv:2310.10375v1 [cs.CV])

    [http://arxiv.org/abs/2310.10375](http://arxiv.org/abs/2310.10375)

    提出了一种面向几何的注意力机制（GTA），用于将几何结构编码为相对变换，从而改进了多视图Transformer的学习效率和性能。

    

    随着transformers对输入标记的排列具有等变性，对标记的位置信息进行编码对许多任务是必要的。然而，由于现有的位置编码方案最初是为自然语言处理任务设计的，对于通常在其数据中表现出不同结构特性的视觉任务来说，它们的适用性值得怀疑。我们认为现有的位置编码方案对于3D视觉任务来说是次优的，因为它们不尊重其底层的3D几何结构。基于这个假设，我们提出了一种面向几何的注意力机制，它将标记的几何结构编码为由查询和键值对之间的几何关系所确定的相对变换。通过在稀疏宽基线多视图设置中评估多个新颖视图合成（NVS）数据集，我们展示了我们的注意力机制——几何变换注意力（GTA）如何提高了最先进的Transformer的学习效率和性能。

    As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks. However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit different structural properties in their data, is questionable. We argue that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure. Based on this hypothesis, we propose a geometry-aware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs. By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-b
    
[^144]: TacoGFN: 针对基于结构的药物设计的目标条件GFlowNet

    TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design. (arXiv:2310.03223v1 [cs.LG])

    [http://arxiv.org/abs/2310.03223](http://arxiv.org/abs/2310.03223)

    该论文提出了一种名为TacoGFN的目标条件GFlowNet模型，用于自动化生成符合特定蛋白质口袋目标的类药物化合物。该模型通过强化学习框架，鼓励生成具有期望属性的分子，并利用转换器和对接神经网络进行高效的分子空间探索和对接得分预测，以实现较高的结合改善效果。

    

    我们旨在自动化生成符合特定蛋白质口袋目标的类药物化合物。大多数当前方法是对有限数据集中的蛋白质-分子分布进行近似，因此在生成的分子中很难实现与训练数据集相比具有显著结合改善的分子。我们将口袋条件下的分子生成任务定义为强化学习问题，并开发了TacoGFN，一种目标条件下的生成流网络模型。我们的方法明确鼓励生成具有期望属性的分子，而不是适应预先存在的数据分布。为此，我们开发了基于转换器的对接得分预测方法来加快对接得分计算，并提出了TacoGFN来高效地探索分子空间。此外，我们还结合了几轮主动学习，使用对接神经网络对生成的样本进行查询，以改善对接得分预测。这种方法使我们能够准确地探索更多的分子空间。

    We seek to automate the generation of drug-like compounds conditioned to specific protein pocket targets. Most current methods approximate the protein-molecule distribution of a finite dataset and, therefore struggle to generate molecules with significant binding improvement over the training dataset. We instead frame the pocket-conditioned molecular generation task as an RL problem and develop TacoGFN, a target conditional Generative Flow Network model. Our method is explicitly encouraged to generate molecules with desired properties as opposed to fitting on a pre-existing data distribution. To this end, we develop transformer-based docking score prediction to speed up docking score computation and propose TacoGFN to explore molecule space efficiently. Furthermore, we incorporate several rounds of active learning where generated samples are queried using a docking oracle to improve the docking score prediction. This approach allows us to accurately explore as much of the molecule land
    
[^145]: 多分辨率主动学习的傅里叶神经算子

    Multi-Resolution Active Learning of Fourier Neural Operators. (arXiv:2309.16971v1 [cs.LG])

    [http://arxiv.org/abs/2309.16971](http://arxiv.org/abs/2309.16971)

    提出了一种多分辨率主动学习的傅里叶神经算子（MRA-FNO），通过动态选择输入函数和分辨率来降低数据成本并优化学习效率。

    

    傅里叶神经算子（FNO）是一种流行的算子学习框架，不仅在许多任务中实现了最先进的性能，而且在训练和预测方面高效。然而，在实践中，为FNO收集训练数据是一个昂贵的瓶颈，因为它经常需要进行昂贵的物理模拟。为了解决这个问题，我们提出了多分辨率主动学习的FNO（MRA-FNO），它能够动态选择输入函数和分辨率，尽量降低数据成本，同时优化学习效率。具体而言，我们提出了概率多分辨率FNO，并使用集成蒙特卡洛方法开发了一种有效的后验推理算法。为了进行主动学习，我们最大化效用成本比作为获取函数，在每一步获取新的样本和分辨率。我们使用矩匹配和矩阵行列式引理实现了可行，高效的效用计算。此外，我们还开发了一种方法来。

    Fourier Neural Operator (FNO) is a popular operator learning framework, which not only achieves the state-of-the-art performance in many tasks, but also is highly efficient in training and prediction. However, collecting training data for the FNO is a costly bottleneck in practice, because it often demands expensive physical simulations. To overcome this problem, we propose Multi-Resolution Active learning of FNO (MRA-FNO), which can dynamically select the input functions and resolutions to lower the data cost as much as possible while optimizing the learning efficiency. Specifically, we propose a probabilistic multi-resolution FNO and use ensemble Monte-Carlo to develop an effective posterior inference algorithm. To conduct active learning, we maximize a utility-cost ratio as the acquisition function to acquire new examples and resolutions at each step. We use moment matching and the matrix determinant lemma to enable tractable, efficient utility computation. Furthermore, we develop a
    
[^146]: RLSynC: 离线-在线强化学习用于合成方法的合成物补全

    RLSynC: Offline-Online Reinforcement Learning for Synthon Completion. (arXiv:2309.02671v1 [cs.LG])

    [http://arxiv.org/abs/2309.02671](http://arxiv.org/abs/2309.02671)

    RLSynC是一种离线-在线强化学习方法，用于半模板化逆向合成中的合成物补全。它使用多个代理同时完成合成物的补全，并通过正向合成模型评估反应物的合成能力来指导行动搜索。

    

    逆向合成是确定能够反应形成所需产物的一组反应物分子的过程。半模板化逆向合成方法首先预测产物中的反应中心，然后将生成的合成物重新补全成反应物。这些方法能够提供必要的可解释性和高实用性，以指导合成规划。我们开发了一种新的离线-在线强化学习方法RLSynC，用于半模板化方法中的合成物补全。RLSynC为每个合成物分配一个代理，所有代理都通过同步进行逐步行动，完成合成物的补全。RLSynC通过同时进行离线训练和在线交互来学习策略，从而可以探索新的反应空间。RLSynC使用正向合成模型来评估预测的反应物在合成产物时的可能性，从而指导行动搜索。

    Retrosynthesis is the process of determining the set of reactant molecules that can react to form a desired product. Semi-template-based retrosynthesis methods, which imitate the reverse logic of synthesis reactions, first predict the reaction centers in the products, and then complete the resulting synthons back into reactants. These methods enable necessary interpretability and high practical utility to inform synthesis planning. We develop a new offline-online reinforcement learning method RLSynC for synthon completion in semi-template-based methods. RLSynC assigns one agent to each synthon, all of which complete the synthons by conducting actions step by step in a synchronized fashion. RLSynC learns the policy from both offline training episodes and online interactions which allow RLSynC to explore new reaction spaces. RLSynC uses a forward synthesis model to evaluate the likelihood of the predicted reactants in synthesizing a product, and thus guides the action search. We compare 
    
[^147]: 用于计算深度神经网络正则化路径的多目标延续方法

    A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])

    [http://arxiv.org/abs/2308.12044](http://arxiv.org/abs/2308.12044)

    本文提出了一种多目标延续方法，用于计算深度神经网络的正则化路径，以解决DNNs中稀疏性和数值效率之间的冲突。

    

    稀疏性是深度神经网络(DNNs)中非常理想的特征，因为它确保了数值效率，提高了模型的可解释性(由于相关特征的数量较少)和鲁棒性。在基于线性模型的机器学习方法中，众所周知在$\ell^1$范数(即零权重)的最稀疏解和非正则化解之间存在一条连接路径，这条路径被称为正则化路径。最近，通过将经验损失和稀疏性($\ell^1$范数)作为两个冲突的标准，并解决由此产生的多目标优化问题，首次尝试将正则化路径的概念扩展到DNNs。然而，由于$\ell^1$范数的不光滑性和参数数量的高度，从计算的角度来看，这种方法并不是很有效。为了克服这个限制，我们提出了一种算法，可以近似计算整个帕累托曲线

    Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
    
[^148]: 梯度反击：如何滤除高频率提高解释性

    Gradient strikes back: How filtering out high frequencies improves explanations. (arXiv:2307.09591v1 [cs.AI])

    [http://arxiv.org/abs/2307.09591](http://arxiv.org/abs/2307.09591)

    本研究发现，基于预测的属性方法与基于梯度的方法产生的属性图具有不同的高频内容，滤除高频率可以提高解释性。

    

    近年来，新型基于预测的属性方法的发展迅猛，逐渐取代了旧的基于梯度的方法来解释深度神经网络的决策。然而，预测型方法为何优于梯度型方法仍不清楚。本文从经验观察开始：这两种方法产生的属性图具有非常不同的功率谱，梯度型方法揭示了比预测型方法更多的高频内容。这一观察引发了多个问题：这种高频信息的来源是什么，它是否真正反映了系统所作出的决策？最后，为什么在多个评价指标下，预测型方法中缺乏高频信息将产生更好的可解释性分数？我们分析了三个代表性的视觉分类模型的梯度，并观察到它包含来自高频的噪声信息。

    Recent years have witnessed an explosion in the development of novel prediction-based attribution methods, which have slowly been supplanting older gradient-based methods to explain the decisions of deep neural networks. However, it is still not clear why prediction-based methods outperform gradient-based ones. Here, we start with an empirical observation: these two approaches yield attribution maps with very different power spectra, with gradient-based methods revealing more high-frequency content than prediction-based methods. This observation raises multiple questions: What is the source of this high-frequency information, and does it truly reflect decisions made by the system? Lastly, why would the absence of high-frequency information in prediction-based methods yield better explainability scores along multiple metrics? We analyze the gradient of three representative visual classification models and observe that it contains noisy information emanating from high-frequencies. Furthe
    
[^149]: 面向NILM的多标签分类的可持续深度学习

    Towards Sustainable Deep Learning for Multi-Label Classification on NILM. (arXiv:2307.09244v1 [cs.LG])

    [http://arxiv.org/abs/2307.09244](http://arxiv.org/abs/2307.09244)

    本研究提出了一种面向NILM的新型深度学习模型，通过改进计算和能源效率，实现了对NILM的多标签分类的增强。同时，还提出了一种测试方法，可以比较不同模型在虚拟数据集上的性能。

    

    非侵入式负载监测（NILM）是从单个计量点获取家庭或企业总电力消耗的电器级数据的过程。电器级数据可以直接用于需求响应应用、能源管理系统以及提高能效和减少碳足迹的意识提高和激励。最近，经典机器学习和深度学习（DL）技术在NILM分类中变得非常流行，并证明在增长的复杂性下对NILM分类非常有效，但随着复杂度的增加，这些方法在训练和操作过程中面临着显著的计算和能源需求。在本文中，我们引入了一种新的DL模型，旨在通过提高计算和能源效率来增强NILM的多标签分类。我们还提出了一种用于使用从测量数据集合成的数据比较不同模型的测试方法。

    Non-intrusive load monitoring (NILM) is the process of obtaining appliance-level data from a single metering point, measuring total electricity consumption of a household or a business. Appliance-level data can be directly used for demand response applications and energy management systems as well as for awareness raising and motivation for improvements in energy efficiency and reduction in the carbon footprint. Recently, classical machine learning and deep learning (DL) techniques became very popular and proved as highly effective for NILM classification, but with the growing complexity these methods are faced with significant computational and energy demands during both their training and operation. In this paper, we introduce a novel DL model aimed at enhanced multi-label classification of NILM with improved computation and energy efficiency. We also propose a testing methodology for comparison of different models using data synthesized from the measurement datasets so as to better 
    
[^150]: 高维度跳跃偏微分方程的时差学习

    Temporal Difference Learning for High-Dimensional PIDEs with Jumps. (arXiv:2307.02766v1 [math.NA])

    [http://arxiv.org/abs/2307.02766](http://arxiv.org/abs/2307.02766)

    本文提出了一种用于解决高维度跳跃偏微分方程的时差学习深度学习框架，该方法具有低计算成本和稳健性的优势，可以有效地处理具有不同形式和强度跳跃的问题。

    

    在本文中，我们提出了基于时差学习的深度学习框架，用于解决高维度的偏积分微分方程（PIDEs）。我们引入了一组Levy过程，并构建了一个相应的强化学习模型。为了模拟整个过程，我们使用深度神经网络来表示方程的解和非局部项。随后，我们使用时差误差、终止条件和非局部项的属性作为损失函数来训练网络。该方法在100维的实验中相对误差达到了O(10^{-3})，在一维纯跳跃问题中达到了O(10^{-4})。此外，我们的方法具有低计算成本和稳健性的优势，适用于解决具有不同形式和强度跳跃的问题。

    In this paper, we propose a deep learning framework for solving high-dimensional partial integro-differential equations (PIDEs) based on the temporal difference learning. We introduce a set of Levy processes and construct a corresponding reinforcement learning model. To simulate the entire process, we use deep neural networks to represent the solutions and non-local terms of the equations. Subsequently, we train the networks using the temporal difference error, termination condition, and properties of the non-local terms as the loss function. The relative error of the method reaches O(10^{-3}) in 100-dimensional experiments and O(10^{-4}) in one-dimensional pure jump problems. Additionally, our method demonstrates the advantages of low computational cost and robustness, making it well-suited for addressing problems with different forms and intensities of jumps.
    
[^151]: 这里是翻译过的论文标题：我们到了吗？产品量化及其硬件加速。

    Are We There Yet? Product Quantization and its Hardware Acceleration. (arXiv:2305.18334v1 [cs.AR])

    [http://arxiv.org/abs/2305.18334](http://arxiv.org/abs/2305.18334)

    本文研究了产品量化（PQ）在深度神经网络中替代传统乘加（MAC）运算的效果。作者发现FLOP和参数数量等指标可能具有误导性，并设计了第一个PQ定制硬件加速器评估其性能和效率。

    

    传统的乘加（MAC）运算长期以来一直主导深度神经网络（DNN）的计算时间。最近，产品量化（PQ）已成功地应用于这些工作负载，用预先计算的点积的内存查找替换了MAC。虽然这个属性使PQ成为模型加速的一个有吸引力的解决方案，但人们很少了解与计算和存储器占用相关的权衡，以及对准确性的影响。我们的实证研究调查了不同PQ设置和训练方法对逐层重建误差和端到端模型准确性的影响。在研究部署PQ DNN的效率时，我们发现FLOP、参数数量甚至CPU/GPU性能等指标可能具有误导性。为了解决这个问题，更公平地评估PQ的硬件效率，我们设计了第一个定制的硬件加速器，用于评估运行PQ模型的速度和效率。我们确定了PQ配置的硬件性能和存储要求之间的权衡。

    Conventional multiply-accumulate (MAC) operations have long dominated computation time for deep neural networks (DNNs). Recently, product quantization (PQ) has been successfully applied to these workloads, replacing MACs with memory lookups to pre-computed dot products. While this property makes PQ an attractive solution for model acceleration, little is understood about the associated trade-offs in terms of compute and memory footprint, and the impact on accuracy. Our empirical study investigates the impact of different PQ settings and training methods on layerwise reconstruction error and end-to-end model accuracy. When studying the efficiency of deploying PQ DNNs, we find that metrics such as FLOPs, number of parameters, and even CPU/GPU performance, can be misleading. To address this issue, and to more fairly assess PQ in terms of hardware efficiency, we design the first custom hardware accelerator to evaluate the speed and efficiency of running PQ models. We identify PQ configurat
    
[^152]: MaxViT-UNet: 多轴注意力用于医学图像分割

    MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation. (arXiv:2305.08396v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.08396](http://arxiv.org/abs/2305.08396)

    MaxViT-UNet是基于编码器-解码器混合视觉变压器的医学图像分割模型，多轴自我关注机制在每个解码器阶段有助于更有效地区分对象和背景区域。

    

    近年来，卷积神经网络在医学图像分析方面取得了重大进展。然而，卷积算子的局部性质抑制了CNNs捕捉全局和长程交互。最近，Transformer在计算机视觉社区和医学图像分割中变得流行。但是，自我注意机制的可扩展性问题和缺乏CNN类归纳偏差限制了它们的应用。在本文中，我们提出了MaxViT-UNet，一种基于编码器-解码器混合视觉变压器的医学图像分割模型。提出的混合解码器，还基于MaxViT-block，旨在在每个解码阶段最小化计算负担下利用卷积和自我注意机制的力量。每个解码器阶段的多轴自我关注有助于更有效地区分对象和背景区域。混合解码器块最初通过上采样传输低层特征。

    Convolutional neural networks have made significant strides in medical image analysis in recent years. However, the local nature of the convolution operator inhibits the CNNs from capturing global and long-range interactions. Recently, Transformers have gained popularity in the computer vision community and also medical image segmentation. But scalability issues of self-attention mechanism and lack of the CNN like inductive bias have limited their adoption. In this work, we present MaxViT-UNet, an Encoder-Decoder based hybrid vision transformer for medical image segmentation. The proposed hybrid decoder, also based on MaxViT-block, is designed to harness the power of convolution and self-attention mechanism at each decoding stage with minimal computational burden. The multi-axis self-attention in each decoder stage helps in differentiating between the object and background regions much more efficiently. The hybrid decoder block initially fuses the lower level features upsampled via tra
    
[^153]: SuperNOVA: 计算笔记本中互动可视化的设计策略与机会

    SuperNOVA: Design Strategies and Opportunities for Interactive Visualization in Computational Notebooks. (arXiv:2305.03039v1 [cs.HC])

    [http://arxiv.org/abs/2305.03039](http://arxiv.org/abs/2305.03039)

    通过分析159个交互式可视化工具及其用户反馈，本论文提出了在计算笔记本中设计可视分析工具的独特机会和考虑因素。

    

    计算笔记本，如 Jupyter Notebook，已成为数据科学家的事实编程环境。许多可视化研究者和实践者已开发出支持笔记本的交互式可视化工具。然而，关于在笔记本中设计 VA 工具的适当方法的了解甚少。为了填补这一关键的研究空白，我们通过分析 159 个笔记本 VA 工具及其用户反馈来研究这一领域的设计策略。我们的分析包括来自学术论文的 62 个系统和来自通过在 GitHub 上抓取 860 万个笔记本而获得的包含交互式可视化的 55k 个笔记本池中的 103 个系统。我们还研究了 15 个用户研究的发现和 379 个 GitHub 问题中的用户反馈。通过这项工作，我们确定了未来笔记本 VA 工具的独特设计机会和考虑因素，例如在笔记本中使用和操作多模态数据以及平衡可视化笔记本的程度。

    Computational notebooks such as Jupyter Notebook have become data scientists' de facto programming environments. Many visualization researchers and practitioners have developed interactive visualization tools that support notebooks. However, little is known about the appropriate design of visual analytics (VA) tools in notebooks. To bridge this critical research gap, we investigate the design strategies in this space by analyzing 159 notebook VA tools and their users' feedback. Our analysis encompasses 62 systems from academic papers and 103 systems sourced from a pool of 55k notebooks containing interactive visualizations that we obtain via scraping 8.6 million notebooks on GitHub. We also examine findings from 15 user studies and user feedback in 379 GitHub issues. Through this work, we identify unique design opportunities and considerations for future notebook VA tools, such as using and manipulating multimodal data in notebooks as well as balancing the degree of visualization-noteb
    
[^154]: mPLUG-Owl: 模块化增强了大型语言模型的多模态能力

    mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality. (arXiv:2304.14178v1 [cs.CL])

    [http://arxiv.org/abs/2304.14178](http://arxiv.org/abs/2304.14178)

    本文介绍了一种名为mPLUG-Owl的训练范式，它通过模块化学习基础LLM、视觉知识模块和视觉抽象器模块，赋予LLMs多模态的能力。实验结果表明，mPLUG-Owl在图像字幕和视觉问答任务中表现优于基线模型，并在某些情况下达到了最先进的性能水平。

    

    大型语言模型(LLMs)已经在各种开放式任务中展示出了令人印象深刻的零-shot表现，而最近的研究还探讨了将LLMs用于多模态生成的应用。在本研究中，我们引入了一种新的训练范式mPLUG-Owl，通过基础LLM、视觉知识模块和视觉抽象器模块的模块化学习，使LLMs具备了多模态的能力。该方法可以支持多种模态，并通过模态协作促进了多单模态和多模态的能力。mPLUG-Owl的训练范式包括用于对齐图像和文本的两阶段方法，该方法利用LLM的辅助学习视觉知识，同时保持甚至改进了LLM的生成能力。在第一阶段中，使用冻结的LLM模块对视觉知识模块和抽象器模块进行训练以对齐图像和文本。在第二阶段中，使用仅语言和多模态监督数据集共同对模型进行微调。对于图像字幕和视觉问答任务的实验结果表明，mPLUG-Owl优于基线模型，在某些情况下达到了最先进的性能水平。

    Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tu
    
[^155]: 水下航行器船体的样本高效和基于代理的设计优化

    Sample-Efficient and Surrogate-Based Design Optimization of Underwater Vehicle Hulls. (arXiv:2304.12420v1 [cs.LG])

    [http://arxiv.org/abs/2304.12420](http://arxiv.org/abs/2304.12420)

    该论文使用了高效的样本集优化和基于代理的方法来设计水下航行器船体，其中代理模型显著提高了计算效率，使优化更加快速准确。

    

    物理模拟是计算机辅助设计(CAD)优化过程中的一个计算瓶颈。因此，为了使精确(计算昂贵)的模拟可用于设计优化中，需要一个高样本效率的优化框架或快速的数据驱动代理(代理模型)来代替长时间运行的模拟。在这项工作中，我们利用最近优化和人工智能(AI)的进展来解决这两个潜在的解决方案，以设计一个最佳的无人水下航行器(UUV)。我们首先研究并比较了不同优化技术在优化循环中与标准计算流体力学(CFD)求解器相结合时的样本效率和收敛行为。然后，我们开发了一个基于深度神经网络(DNN)的代理模型来逼近否则通过CFD求解器进行计算的阻力。代理模型进而用于样本高效的优化框架中，该框架在不使用代理模型的情况下优于标准优化方法。

    Physics simulations are a computational bottleneck in computer-aided design (CAD) optimization processes. Hence, in order to make accurate (computationally expensive) simulations feasible for use in design optimization, one requires either an optimization framework that is highly sample-efficient or fast data-driven proxies (surrogate models) for long running simulations. In this work, we leverage recent advances in optimization and artificial intelligence (AI) to address both of these potential solutions, in the context of designing an optimal unmanned underwater vehicle (UUV). We first investigate and compare the sample efficiency and convergence behavior of different optimization techniques with a standard computational fluid dynamics (CFD) solver in the optimization loop. We then develop a deep neural network (DNN) based surrogate model to approximate drag forces that would otherwise be computed via direct numerical simulation with the CFD solver. The surrogate model is in turn use
    
[^156]: TACOS: 面向拓扑结构的分布式训练集合算法合成器

    TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed Training. (arXiv:2304.05301v1 [cs.DC])

    [http://arxiv.org/abs/2304.05301](http://arxiv.org/abs/2304.05301)

    TACOS 是一个能够自动合成任意输入网络拓扑的面向拓扑结构的集合合成器。与基准算法相比，TACOS 合成的 All-Reduce 算法速度提高了 3.73 倍，为 512-NPU 系统合成集体算法只需 6.1 分钟。

    

    集群之间的集合通讯是分布式训练不可或缺的一部分。运行面向拓扑结构的集合算法对于优化通讯性能以最小化拥塞是至关重要的。目前，此类算法仅适用于一小部分简单拓扑结构，限制了训练集群中采用的拓扑结构并处理由于网络故障而产生的不规则拓扑结构。 本文提出了 TACOS，这是一个可自动合成任意输入网络拓扑的面向拓扑结构的集合合成器。TACOS 合成的 All-Reduce 算法比基线算法快了 3.73 倍，并为 512-NPU 系统合成集体算法仅需 6.1 分钟。

    Collective communications are an indispensable part of distributed training. Running a topology-aware collective algorithm is crucial for optimizing communication performance by minimizing congestion. Today such algorithms only exist for a small set of simple topologies, limiting the topologies employed in training clusters and handling irregular topologies due to network failures. In this paper, we propose TACOS, an automated topology-aware collective synthesizer for arbitrary input network topologies. TACOS synthesized 3.73x faster All-Reduce algorithm over baselines, and synthesized collective algorithms for 512-NPU system in just 6.1 minutes.
    
[^157]: 在联邦深度学习中优化批标准化

    Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])

    [http://arxiv.org/abs/2303.06530](http://arxiv.org/abs/2303.06530)

    本文研究了在联邦学习中使用批标准化和群组归一化的效果，发现在适当的处理下，批标准化可以在广泛的联邦学习设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。

    This paper studies the use of batch normalization and group normalization in federated learning, and finds that with proper treatments, batch normalization can be highly competitive across a wide range of federated learning settings, and this requires no additional training or communication costs.

    批标准化（BN）通常用于现代深度神经网络（DNN）中，以提高稳定性并加速集中式训练的收敛速度。在具有非IID分散数据的联邦学习（FL）中，先前的研究观察到使用BN进行训练可能会由于训练和测试之间的BN统计不匹配而阻碍性能。因此，群组归一化（GN）更常用于FL作为BN的替代方法。然而，通过我们在各种FL设置下的实证研究，我们发现BN和GN之间没有一致的优胜者。这促使我们重新审视FL中归一化层的使用。我们发现，在适当的处理下，BN可以在广泛的FL设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。我们希望我们的研究可以成为FL未来实际使用和理论分析的有价值参考。

    Batch Normalization (BN) is commonly used in modern deep neural networks (DNNs) to improve stability and speed up convergence during centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. However, from our empirical study across various FL settings, we see no consistent winner between BN and GN. This leads us to revisit the use of normalization layers in FL. We find that with proper treatments, BN can be highly competitive across a wide range of FL settings, and this requires no additional training or communication costs. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL.
    
[^158]: 深度神经网络时代的肿瘤多模态数据整合：一篇综述

    Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review. (arXiv:2303.06471v1 [cs.LG])

    [http://arxiv.org/abs/2303.06471](http://arxiv.org/abs/2303.06471)

    本文综述了深度神经网络在多模态数据整合方面的应用，以提高癌症诊断和治疗的准确性和可靠性。

    This review article analyzes the application of deep neural networks in multimodal data integration to improve the accuracy and reliability of cancer diagnosis and treatment.

    癌症在不同尺度、模态和分辨率的获取数据中具有关系信息，例如放射学、病理学、基因组学、蛋白质组学和临床记录。整合多种数据类型可以提高癌症诊断和治疗的准确性和可靠性。可能存在人类或现有技术工具无法视觉上区分的与疾病相关的信息。传统方法通常关注单个尺度的生物系统的部分或单一模态信息，并未涵盖数据异质性的完整光谱。深度神经网络促进了复杂的多模态数据融合方法的发展，可以从多个来源提取和整合相关信息。最近的深度学习框架，如图形神经网络（GNN）和变压器，在多模态学习方面取得了显着的成功。本综述文章提供了对当前多模态数据整合方法的深入分析。

    Cancer has relational information residing at varying scales, modalities, and resolutions of the acquired data, such as radiology, pathology, genomics, proteomics, and clinical records. Integrating diverse data types can improve the accuracy and reliability of cancer diagnosis and treatment. There can be disease-related information that is too subtle for humans or existing technological tools to discern visually. Traditional methods typically focus on partial or unimodal information about biological systems at individual scales and fail to encapsulate the complete spectrum of the heterogeneous nature of data. Deep neural networks have facilitated the development of sophisticated multimodal data fusion approaches that can extract and integrate relevant information from multiple sources. Recent deep learning frameworks such as Graph Neural Networks (GNNs) and Transformers have shown remarkable success in multimodal learning. This review article provides an in-depth analysis of the state-
    
[^159]: 通过结构化噪声训练神经网络提高分类和泛化能力。

    Training neural networks with structured noise improves classification and generalization. (arXiv:2302.13417v3 [cond-mat.dis-nn] UPDATED)

    [http://arxiv.org/abs/2302.13417](http://arxiv.org/abs/2302.13417)

    通过在训练数据中添加结构化噪声，可以显著提高神经网络的分类和泛化能力，并提出了一种采样策略来优于传统的训练和赫布生规则方法。

    

    噪声在学习中的积极作用是人工神经网络领域中一个已经被确认的概念，这表明甚至生物系统可能利用类似的机制来最大化性能。Gardner和合作者提出的噪声训练算法是在循环网络中注入噪声的典型示例，循环网络通常用于建模真实神经系统。我们展示了在噪声训练数据中添加结构可以显着提高算法性能，使得可以接近完美分类和最大吸引域。我们还证明了所谓的赫布生规则在噪声达到最大且数据是网络动力学的固定点时与噪声训练算法一致。最后，我们提出并实施了一种用于最佳噪声数据的采样策略，来超越噪声训练和赫布生规则的性能。

    The beneficial role of noise in learning is nowadays a consolidated concept in the field of artificial neural networks, suggesting that even biological systems might take advantage of similar mechanisms to maximize their performance. The training-with-noise algorithm proposed by Gardner and collaborators is an emblematic example of a noise injection procedure in recurrent networks, which are usually employed to model real neural systems. We show how adding structure into noisy training data can substantially improve the algorithm performance, allowing to approach perfect classification and maximal basins of attraction. We also prove that the so-called Hebbian unlearning rule coincides with the training-with-noise algorithm when noise is maximal and data are fixed points of the network dynamics. A sampling scheme for optimal noisy data is eventually proposed and implemented to outperform both the training-with-noise and the Hebbian unlearning procedures.
    
[^160]: 面向前列腺癌诊断和格里森分级的联邦对比学习模型

    Federated contrastive learning models for prostate cancer diagnosis and Gleason grading. (arXiv:2302.06089v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.06089](http://arxiv.org/abs/2302.06089)

    该研究提出了一个面向大规模病理图像和异质性挑战的联邦对比学习模型（FCL），通过最大化本地客户端和服务器模型间的注意力一致性来增强模型的泛化能力。 在前列腺癌诊断和格里森分级任务中，FCL表现出优异的性能。

    

    人工智能在医学影像领域的应用效果显著。然而，稳健的人工智能模型训练需要大规模的数据集，但数据收集面临沟通、伦理和隐私保护等限制。联邦学习可以通过协调多个客户端训练模型而不共享原始数据来解决上述问题。本研究设计了一个面向大规模病理图像和异质性挑战的联邦对比学习框架（FCL），通过最大化本地客户端和服务器模型间的注意力一致性来增强模型的泛化能力。为了缓解参数传输中的隐私泄露问题并验证FCL的稳健性，我们使用差分隐私通过添加噪音进一步保护模型。我们在19,635个来自多个客户端的前列腺癌WSI上评估了FCL在癌症诊断任务和格里森分级任务中的有效性。在诊断任务中，我们实现了区分癌性和非癌性WSI的0.99的曲线下面积（AUC）。在格里森分级任务中，我们的FCL模型实现了一个均方误差（MSE）为0.143，相比最先进的方法降低了21.7％。

    The application effect of artificial intelligence (AI) in the field of medical imaging is remarkable. Robust AI model training requires large datasets, but data collection faces communication, ethics, and privacy protection constraints. Fortunately, federated learning can solve the above problems by coordinating multiple clients to train the model without sharing the original data. In this study, we design a federated contrastive learning framework (FCL) for large-scale pathology images and the heterogeneity challenges. It enhances the model's generalization ability by maximizing the attention consistency between the local client and server models. To alleviate the privacy leakage problem when transferring parameters and verify the robustness of FCL, we use differential privacy to further protect the model by adding noise. We evaluate the effectiveness of FCL on the cancer diagnosis task and Gleason grading task on 19,635 prostate cancer WSIs from multiple clients. In the diagnosis tas
    
[^161]: CPPF++：基于投票聚合的考虑不确定性的Sim2Real物体姿态估计

    CPPF++: Uncertainty-Aware Sim2Real Object Pose Estimation by Vote Aggregation. (arXiv:2211.13398v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.13398](http://arxiv.org/abs/2211.13398)

    本文针对Sim2Real物体姿态估计问题，提出了一种新颖的CPPF++方法，通过投票聚合和概率建模来考虑投票不确定性，并通过迭代噪声过滤来提高姿态估计的准确性。

    

    物体姿态估计是三维视觉领域中的关键技术，本文针对只利用三维CAD模型作为先验知识的情况，提出了一种新颖的CPPF++方法，用于Sim2Real物体姿态估计。该方法通过概率性视角重新构思了CPPF的基础点对投票方案，以解决投票碰撞的挑战，并通过估计规范空间中每个点对的概率分布来建模投票不确定性。此外，该方法还通过迭代噪声过滤来消除与背景或杂波有关的投票，并增强了每个投票所提供的上下文信息。

    Object pose estimation constitutes a critical area within the domain of 3D vision. While contemporary state-of-the-art methods that leverage real-world pose annotations have demonstrated commendable performance, the procurement of such real-world training data incurs substantial costs. This paper focuses on a specific setting wherein only 3D CAD models are utilized as a priori knowledge, devoid of any background or clutter information. We introduce a novel method, CPPF++, designed for sim-to-real pose estimation. This method builds upon the foundational point-pair voting scheme of CPPF, reconceptualizing it through a probabilistic lens. To address the challenge of voting collision, we model voting uncertainty by estimating the probabilistic distribution of each point pair within the canonical space. This approach is further augmented by iterative noise filtering, employed to eradicate votes associated with backgrounds or clutters. Additionally, we enhance the context provided by each v
    
[^162]: 正样本未标记对比学习

    Positive Unlabeled Contrastive Learning. (arXiv:2206.01206v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.01206](http://arxiv.org/abs/2206.01206)

    我们提出了一种正样本未标记对比学习的新方法，通过扩展对比损失和使用PU特定聚类方案，该方法在PU任务中学习到了优秀的表示，并在多个标准数据集上明显优于现有方法。

    

    自我监督预训练无标签数据，然后在标记数据上进行监督微调是一种常见的从有限标记样本中学习的方法。我们将这个方法扩展到经典的正样本未标记（PU）设置，其中的任务是仅通过一些标记为正样本和（通常）大量的未标记样本（可以是正样本或负样本）来学习二分类器。我们首先对标准infoNCE对比损失的家族提出了一个简单的扩展，适用于PU设置；并且证明相比于现有的无监督和有监督方法，这种方法学习到了更好的表示。然后，我们开发了一种简单的方法，使用新的PU特定聚类方案为未标记样本构建伪标签；这些伪标签可以用来训练最终的（正样本 vs. 负样本）分类器。我们的方法在几个标准PU基准数据集上明显优于现有的PU方法，并且不需要任何类别的先验知识。

    Self-supervised pretraining on unlabeled data followed by supervised fine-tuning on labeled data is a popular paradigm for learning from limited labeled examples. We extend this paradigm to the classical positive unlabeled (PU) setting, where the task is to learn a binary classifier given only a few labeled positive samples, and (often) a large amount of unlabeled samples (which could be positive or negative).  We first propose a simple extension of standard infoNCE family of contrastive losses, to the PU setting; and show that this learns superior representations, as compared to existing unsupervised and supervised approaches. We then develop a simple methodology to pseudo-label the unlabeled samples using a new PU-specific clustering scheme; these pseudo-labels can then be used to train the final (positive vs. negative) classifier. Our method handily outperforms state-of-the-art PU methods over several standard PU benchmark datasets, while not requiring a-priori knowledge of any clas
    

