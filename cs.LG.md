# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Unveiling Delay Effects in Traffic Forecasting: A Perspective from Spatial-Temporal Delay Differential Equations](https://rss.arxiv.org/abs/2402.01231) | 本研究提出了一个基于神经网络的空间-时间延迟微分方程模型，用于捕捉交通预测中的延迟效应。 |
| [^2] | [Label Learning Method Based on Tensor Projection](https://arxiv.org/abs/2402.16544) | 我们提出了一种基于张量投影的标签学习方法，通过将锚图投影到标签空间，并将矩阵投影扩展到张量投影，以充分利用多视图数据的空间结构信息。 |
| [^3] | [Model-based deep reinforcement learning for accelerated learning from flow simulations](https://arxiv.org/abs/2402.16543) | 本文展示了基于模型的强化学习在流体控制应用中的优势，通过优化策略来减少流体模拟的计算成本和运行时间。 |
| [^4] | [Integrating Large Language Models with Graphical Session-Based Recommendation](https://arxiv.org/abs/2402.16539) | 本文提出了一种名为LLMGR的框架，将大型语言模型与图形会话推荐相结合，有效地弥合了推荐系统中大型语言模型在会话推荐领域的适应性差距 |
| [^5] | [Discovering Artificial Viscosity Models for Discontinuous Galerkin Approximation of Conservation Laws using Physics-Informed Machine Learning](https://arxiv.org/abs/2402.16517) | 使用物理信息机器学习算法自动发现人工粘性模型，无需数据集训练，成功应用于高阶守恒定律求解器中。 |
| [^6] | [Generative Pretrained Hierarchical Transformer for Time Series Forecasting](https://arxiv.org/abs/2402.16516) | 提出一种名为GPHT的新型生成式预训练分层Transformer架构，通过构建混合数据集来预训练模型，解决了时间序列预测中数据集限制和时间依赖性忽视的问题 |
| [^7] | [Learning to Schedule Online Tasks with Bandit Feedback](https://arxiv.org/abs/2402.16463) | 通过提出一种基于双乐观学习的Robbins-Monro算法，解决了在线任务调度中奖励和成本难以建模、任务到达分布不确定等挑战 |
| [^8] | [On Distributed Larger-Than-Memory Subset Selection With Pairwise Submodular Functions](https://arxiv.org/abs/2402.16442) | 本文提出了一种新颖的分布式约束算法，通过迭代绑定最小和最大效用值来选择高质量的点并丢弃不重要的点。 |
| [^9] | [Training Implicit Generative Models via an Invariant Statistical Loss](https://arxiv.org/abs/2402.16435) | 提出了一种通过不变统计损失训练隐式生成模型的方法，解决了训练不稳定和模式缺失问题 |
| [^10] | [TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis](https://arxiv.org/abs/2402.16412) | TOTEM提出了一种简单的令牌化架构，通过自监督学习的离散矢量化表示嵌入不同领域的时间序列数据，能够实现通用、跨领域训练，在多个任务和领域上进行广泛评估。 |
| [^11] | [Stable Training of Normalizing Flows for High-dimensional Variational Inference](https://arxiv.org/abs/2402.16408) | 提出了稳定训练高维变分推断中正规化流的方法 |
| [^12] | [Graph Learning with Distributional Edge Layouts](https://arxiv.org/abs/2402.16402) | 图神经网络中提出了一种新的全局布局采样方法，Distributional Edge Layouts（DELs），通过Langevin动力学和玻尔兹曼分布，能够捕获广泛的能量分布，提供额外的表达能力，有助于简化下游任务。 |
| [^13] | [Uncertainty Quantification in Anomaly Detection with Cross-Conformal $p$-Values](https://arxiv.org/abs/2402.16388) | 针对异常检测系统中不确定性量化的需求，提出了一种新颖的框架，称为交叉一致异常检测，通过校准模型的不确定性提供统计保证。 |
| [^14] | [On the Generalization Capability of Temporal Graph Learning Algorithms: Theoretical Insights and a Simpler Method](https://arxiv.org/abs/2402.16387) | 本文探讨了时间图学习算法的泛化能力，并提出了一种更简单的方法Simplified-Temporal-Graph-Network。 |
| [^15] | [Self Supervised Correlation-based Permutations for Multi-View Clustering](https://arxiv.org/abs/2402.16383) | 提出了一种基于深度学习的多视图聚类框架，利用新颖的基于置换的规范相关性目标学习融合数据表示，并通过识别多个视图的一致伪标签来学习聚类分配，实验结果表明模型有效性，理论上证明逼近监督线性判别分析（LDA）表示，提供了由错误伪标签注释引起的误差界限。 |
| [^16] | [An Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation](https://arxiv.org/abs/2402.16380) | 该论文介绍了一种端到端工具，用于生成高质量的文本转语音（TTS）模型数据集，实现了语言特定的语音分布整合、自动化录制过程、自动化和人机协作的录音质量保证以及录音格式处理。 |
| [^17] | [Graph Learning under Distribution Shifts: A Comprehensive Survey on Domain Adaptation, Out-of-distribution, and Continual Learning](https://arxiv.org/abs/2402.16374) | 该综述调查了解决图学习中分布变化问题的最新方法、策略和见解。 |
| [^18] | [Generative AI in Vision: A Survey on Models, Metrics and Applications](https://arxiv.org/abs/2402.16369) | 扩散模型作为一种强大的方法正在生成高质量图像、文本和音频，而此调查论文旨在全面概述生成AI扩散和传统模型的基本技术、应用和挑战。 |
| [^19] | [Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions](https://arxiv.org/abs/2402.16364) | 论文探讨了基于自然空间描述进行多尺度空间关系推理的方法，发现通过获知地图知识得到的描述能够提供环境的整体结构。 |
| [^20] | [Feedback Efficient Online Fine-Tuning of Diffusion Models](https://arxiv.org/abs/2402.16359) | 提出了一种反馈高效的在线微调扩散模型的强化学习程序 |
| [^21] | [An Integrated Data Processing Framework for Pretraining Foundation Models](https://arxiv.org/abs/2402.16358) | 提出了一个集成了处理模块和分析模块的数据处理框架，旨在改善数据质量并展示其有效性。 |
| [^22] | [Language-guided Skill Learning with Temporal Variational Inference](https://arxiv.org/abs/2402.16354) | 该论文提出了一种语言引导的技能学习算法，通过整合大型语言模型生成的分割信息来发现可重用的技能，并引入最小描述长度原则来引导这一过程，实现了在不同环境中加速学习并超越基线方法的效果。 |
| [^23] | [An optimal tradeoff between entanglement and copy complexity for state tomography](https://arxiv.org/abs/2402.16353) | 本研究在量子态重构中研究了可以同时测量多个副本的情况，发现为了学习一个未知的$d$维态到跟踪距离$\epsilon$，需要并且足够的拷贝为$\widetilde{\Theta}(\frac{d^3}{\sqrt{t}\epsilon^2})$。 |
| [^24] | [C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory](https://arxiv.org/abs/2402.16349) | 该论文利用控制理论改进了生成对抗模仿学习（GAIL），提出了一种名为“Controlled-GAIL”（C-GAIL）的算法，能够解决GAIL训练不稳定性的问题，并在MuJoCo任务中取得了较快的收敛速度。 |
| [^25] | [Boosting Graph Pooling with Persistent Homology](https://arxiv.org/abs/2402.16346) | 通过PH向池化层注入全局拓扑不变性的机制显著提升了图神经网络的性能。 |
| [^26] | [A Provably Accurate Randomized Sampling Algorithm for Logistic Regression](https://arxiv.org/abs/2402.16326) | 提出了一种逻辑回归问题的简单随机抽样算法，通过随机矩阵乘法实现高质量逼近估计概率和模型整体差异性。 |
| [^27] | [Achieving $\tilde{O}(1/\epsilon)$ Sample Complexity for Constrained Markov Decision Process](https://arxiv.org/abs/2402.16324) | 该论文提出了一种算法，在约束马尔可夫决策过程中实现了约$O(1/\epsilon)$的样本复杂度，相比先前文献中已有的$O(1/\epsilon^2)$样本复杂度有所提升。 |
| [^28] | [Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech](https://arxiv.org/abs/2402.16321) | 提出了VQScore，一种基于矢量量化可变分编码器的自监督度量，用于评估语音质量并进行自监督语音增强训练，通过引入领域知识和新颖的自蒸馏机制提高模型的相关性和鲁棒性。 |
| [^29] | [Federated Contextual Cascading Bandits with Asynchronous Communication and Heterogeneous Users](https://arxiv.org/abs/2402.16312) | 本研究提出了一种解决联邦上下文级联多臂老虎机问题的算法，通过异步通信和考虑异质用户行为，实现了对具有不同偏好的用户提供定制化推荐，并给出了次线性的遗憾界限。 |
| [^30] | [REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories](https://arxiv.org/abs/2402.16310) | 该论文提出了REPLAY模型，利用一般RNN架构来学习捕捉人类移动的时间变化规律，用于位置预测。 |
| [^31] | [Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion](https://arxiv.org/abs/2402.16305) | 通过模型反演提出了一种训练自由的方法，可以绕过传统的采样过程，直接优化图像并获得更好的文本图像对齐，为改进图像生成提供了关键设计。 |
| [^32] | [Graph Diffusion Policy Optimization](https://arxiv.org/abs/2402.16302) | 本文引入了图扩散策略优化（GDPO），通过强化学习为任意目标优化图扩散模型，实现了在各种图生成任务中的最先进性能。 |
| [^33] | [Conformalized Selective Regression](https://arxiv.org/abs/2402.16300) | 通过利用一致性预测，提供基于模型特定偏差的置信度量，以解决选择性回归中不确定性测量的方法。 |
| [^34] | [Against Filter Bubbles: Diversified Music Recommendation via Weighted Hypergraph Embedding Learning](https://arxiv.org/abs/2402.16299) | 引入了DWHRec算法来解决音乐推荐中准确性和多样性之间的平衡问题，通过加权超图嵌入学习来提高推荐系统的多样性。 |
| [^35] | [Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics](https://arxiv.org/abs/2402.16297) | 提出了一种具有非平稳转移动态的泊松-伽马动力系统，通过采用Dirichlet Markov链和数据增广技术来解决原有模型捕捉时变转移动态的不足。 |
| [^36] | [A Comparison of Deep Learning Models for Proton Background Rejection with the AMS Electromagnetic Calorimeter](https://arxiv.org/abs/2402.16285) | 提出一种使用AMS电磁量能器进行质子背景拒绝的新模型，以解决高能正电子测量的挑战。 |
| [^37] | [A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction](https://arxiv.org/abs/2402.16278) | 提出了一种自匹配训练方法，通过两种本体嵌入模型捕获全局和局部信息，提高了概念子类预测的稳健性 |
| [^38] | [From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto](https://arxiv.org/abs/2402.16269) | 运用大型语言模型与优化相结合，创建决策优化CoPilot（DOCP），帮助决策者通过自然语言交互理解并解决业务问题。 |
| [^39] | [Foundation Model Transparency Reports](https://arxiv.org/abs/2402.16268) | 提出了基础模型透明度报告，借鉴社交媒体的透明度报告实践，目的在于在基础模型行业尚未成熟时制定透明度报告。 |
| [^40] | [Watch Your Head: Assembling Projection Heads to Save the Reliability of Federated Models](https://arxiv.org/abs/2402.16255) | 联邦学习中发现当面对异构数据时，因存在有偏的投影头导致的联邦模型不可靠性，提出了“组装投影头”（APH）方法以提高模型可靠性。 |
| [^41] | [Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition](https://arxiv.org/abs/2402.16247) | 提出了一个名为合作语言习得问题（CLAP）的新颖人工智能挑战，通过允许代理在目标社区中从互动数据集中学习，放宽了Zero-Shot Coordination假设。 |
| [^42] | [Active Level Set Estimation for Continuous Search Space with Theoretical Guarantee](https://arxiv.org/abs/2402.16237) | 提出了一种不需要任何离散化直接在连续搜索空间中工作的具有理论保证的活跃水平集估计算法 |
| [^43] | [GARNN: An Interpretable Graph Attentive Recurrent Neural Network for Predicting Blood Glucose Levels via Multivariate Time Series](https://arxiv.org/abs/2402.16230) | 提出了一种可解释的图注意力循环神经网络（GARNNs），用于通过多元时间序列预测血糖水平，并实现了更具解释性的变量贡献总结和特征图生成。 |
| [^44] | [IR2: Information Regularization for Information Retrieval](https://arxiv.org/abs/2402.16200) | 介绍了IR2，一种用于在合成数据生成过程中减少过拟合的信息正则化技术，在复杂查询的信息检索任务中表现出优越性能，同时将成本降低高达50%。 |
| [^45] | [Language Models for Code Completion: A Practical Evaluation](https://arxiv.org/abs/2402.16197) | 这项研究对完成真实世界代码时的三种公共代码语言模型进行了定量和定性评估，在线和离线环境中进行了比较分析，为自动代码补全的语言模型评估提供了有益的发现。 |
| [^46] | [Combining Machine Learning with Computational Fluid Dynamics using OpenFOAM and SmartSim](https://arxiv.org/abs/2402.16196) | 使用OpenFOAM和SmartSim，我们提供了一个有效且可伸缩的解决方案来开发CFD+ML算法，通过SmartSim将OpenFOAM的不同部分有效地与ML耦合，包括预处理/后处理应用程序、求解器、函数对象和网格运动求解器。 |
| [^47] | [Attacking LLM Watermarks by Exploiting Their Strengths](https://arxiv.org/abs/2402.16187) | 现有的LLM水印系统虽然具有质量保留、鲁棒性和公开检测API等优点，但也因此容易受到各种攻击，研究者提出了一套实用指南以缓解这些攻击。 |
| [^48] | [Deep Neural Network Initialization with Sparsity Inducing Activations](https://arxiv.org/abs/2402.16184) | 本文利用大宽高斯过程极限分析随机初始化时诱导隐藏输出稀疏行为的激活函数，克服了训练不稳定性。 |
| [^49] | [How Can LLM Guide RL? A Value-Based Approach](https://arxiv.org/abs/2402.16181) | 本文研究了如何利用大型语言模型（LLM）提供的策略先验来增强强化学习（RL）算法的样本效率。 |
| [^50] | [Distribution-Free Fair Federated Learning with Small Samples](https://arxiv.org/abs/2402.16158) | 本文介绍了一种用于分布无关公平学习的后处理算法FedFaiREE，适用于去中心化具有小样本的环境。 |
| [^51] | [Consensus learning: A novel decentralised ensemble learning paradigm](https://arxiv.org/abs/2402.16157) | 引入共识学习这一全新的分布式机器学习范式，结合经典集成方法与共识协议，在保证用户数据隐私的同时抵御拜占庭攻击，具有高效性和可扩展性。 |
| [^52] | [ChatMusician: Understanding and Generating Music Intrinsically with LLM](https://arxiv.org/abs/2402.16153) | ChatMusician 是一个集成了内在音乐能力的开源LLM，通过对文本兼容的音乐表示法进行持续预训练和微调，能够理解和生成音乐，表现优于GPT-4基准模型。 |
| [^53] | [A VAE-based Framework for Learning Multi-Level Neural Granger-Causal Connectivity](https://arxiv.org/abs/2402.16131) | 该论文提出了一种基于VAE的框架，可联合学习一组相关但异构动态系统中的Granger因果关系，并以原则性方式处理提取共享结构和识别个体特性的任务。 |
| [^54] | [InstructEdit: Instruction-based Knowledge Editing for Large Language Models](https://arxiv.org/abs/2402.16123) | InstructEdit提出了一种基于指令的知识编辑技术，通过简单指令使编辑器适应不同任务的表现，显著提高了多任务编辑中的可靠性。 |
| [^55] | [DeepForge: Leveraging AI for Microstructural Control in Metal Forming via Model Predictive Control](https://arxiv.org/abs/2402.16119) | 该研究提出了一种结合模型预测控制和名为DeepForge的机器学习模型的方法，用于在闭模热锻造过程中通过使用表面温度测量预测微观组织的变化。研究结果显示，DeepForge能够以极低的平均绝对误差预测微观组织，并探索了使用MPC调整等待时间来实现在特定工件区域内达到目标晶粒尺寸的可能性。 |
| [^56] | [Informed Meta-Learning](https://arxiv.org/abs/2402.16105) | 该研究提出了通知元学习这一新范式，旨在通过人类和机器之间的跨任务知识共享，提高数据效率和抵御观测噪声。 |
| [^57] | [Bayesian Neural Network For Personalized Federated Learning Parameter Selection](https://arxiv.org/abs/2402.16091) | 通过引入贝叶斯神经网络，本研究提出在元素级别而非传统的层级上进行个性化，以选择个性化参数。 |
| [^58] | [Key Design Choices in Source-Free Unsupervised Domain Adaptation: An In-depth Empirical Analysis](https://arxiv.org/abs/2402.16090) | 无源无监督领域自适应中的关键设计选择研究了多种技术，评估了它们在数据集上的表现、对超参数的敏感性，以及在不同主干架构上的适用性，强调了主干架构和预训练数据集选择对性能的重要性 |
| [^59] | [Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs](https://arxiv.org/abs/2402.16078) | 这里是中文总结出的一句话要点：提出了Evolution Graph Fourier Transform(EFT)，首次实现在时间图上捕捉演化表示的可逆谱变换，通过优化连续时间动态图的拉普拉斯，同时提出了伪谱松弛来高效计算转换过程。 |
| [^60] | [Equivariant Frames and the Impossibility of Continuous Canonicalization](https://arxiv.org/abs/2402.16077) | 对于常用的群，研究揭示出没有有效的可计算的框架选择能够保持被平均函数的连续性，但本研究提出了加权框架这一解决方案。 |
| [^61] | [Behavioral Refinement via Interpolant-based Policy Diffusion](https://arxiv.org/abs/2402.16075) | 使用比高斯更具信息量的源头启动扩散方法有助于克服模仿学习任务中的限制。 |
| [^62] | [Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities](https://arxiv.org/abs/2402.16073) | 使用预计算的嵌入相似性生成个性化信息流，提高了电子商务平台上的客户参与度和体验，转化率提升4.9％。 |
| [^63] | [Training a Bilingual Language Model by Mapping Tokens onto a Shared Character Space](https://arxiv.org/abs/2402.16065) | 通过将标记映射到共享字符空间，研究了阿拉伯-希伯来双语语言模型训练。结果表明，使用同时表示两种语言的统一脚本的语言模型在机器翻译上表现出色，相比于保持原有脚本的模型有着更好的性能表现。 |
| [^64] | [Gradient-enhanced deep Gaussian processes for multifidelity modelling](https://arxiv.org/abs/2402.16059) | 这项工作将深高斯过程扩展到包含梯度数据，用于多保真建模，能够捕获不同保真数据之间的非线性和输入相关关系。 |
| [^65] | [LLMs with Chain-of-Thought Are Non-Causal Reasoners](https://arxiv.org/abs/2402.16048) | 本文探讨了大型语言模型在推理过程中思维链条（CoT）的作用，发现LLMs在答案生成过程中与人类推理存在差异，相关因素包括语境学习、有监督微调以及对人类反馈的强化学习。 |
| [^66] | [Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy](https://arxiv.org/abs/2402.16041) | 通过多种群意识优化检测机器生成文本的最大均值离差，解决了机器生成文本与人工编写文本之间微妙的分布差异挑战。 |
| [^67] | [Deep Learning Approaches for Improving Question Answering Systems in Hepatocellular Carcinoma Research](https://arxiv.org/abs/2402.16038) | 深度学习技术在问答系统领域取得的成就，尤其是在肝细胞癌研究中，极大地推动了自然语言处理的发展。 |
| [^68] | [Machine Learning-Based Vehicle Intention Trajectory Recognition and Prediction for Autonomous Driving](https://arxiv.org/abs/2402.16036) | 该论文主要解决了自动驾驶技术中关于车辆意图轨迹识别和预测的挑战。 |
| [^69] | [Feature Selection Based on Orthogonal Constraints and Polygon Area](https://arxiv.org/abs/2402.16026) | 该研究提出了一种结合多边形面积的新型正交回归模型，用于捕捉特征与标签之间的区分性依赖关系，并采用混合非单调线性搜索方法处理正交约束带来的非凸优化挑战，实验证明该方法在降维和提升分类性能方面优于传统方法。 |
| [^70] | [HiGPT: Heterogeneous Graph Language Model](https://arxiv.org/abs/2402.16024) | 该论文提出了HiGPT模型，致力于解决异质图学习中存在的泛化限制和分布不稳定性问题。 |
| [^71] | [A Step-by-step Introduction to the Implementation of Automatic Differentiation](https://arxiv.org/abs/2402.16020) | 本文通过逐步介绍实现简单自动微分系统，填补了教学中的空白，并简化了数学概念和实现。 |
| [^72] | [Spectrum Extraction and Clipping for Implicitly Linear Layers](https://arxiv.org/abs/2402.16017) | 展示自动微分在计算和控制隐式线性算子频谱中的有效性；提供第一个适用于一般卷积层的裁剪方法；研究了批归一化层与卷积层组合的效果；通过比较算法与最先进方法的精度和性能表明更精确和高效。 |
| [^73] | [Building Flexible Machine Learning Models for Scientific Computing at Scale](https://arxiv.org/abs/2402.16014) | OmniArch通过多物理学时空数据处理、可扩展的自回归任务和物理信息增强学习技术，在科学计算领域构建灵活的基础模型，并在性能、适应性和逆问题求解方面取得突破，展现了AI对科学计算的潜力。 |
| [^74] | [Deep Contrastive Graph Learning with Clustering-Oriented Guidance](https://arxiv.org/abs/2402.16012) | 该论文提出了一种深度对比图学习（DCGL）模型，通过结合自编码器和GCN，在处理一般数据聚类时强调了图结构和原始特征。 |
| [^75] | [Unmasking Dementia Detection by Masking Input Gradients: A JSM Approach to Model Interpretability and Precision](https://arxiv.org/abs/2402.16008) | 通过Jacobson显著性地图（JSM）方法，本文提出了一种创新的模型调试方法，用于揭示深度学习模型中可能存在的偏见和不合理的认知，以提高其在医学领域（以阿尔茨海默病诊断为例）的可解释性和精确性。 |
| [^76] | [Adversarial-Robust Transfer Learning for Medical Imaging via Domain Assimilation](https://arxiv.org/abs/2402.16005) | 该论文着力于解决医学图像领域中机器学习模型的敌对攻击问题与训练数据稀缺问题，并利用领域同化的方法来增强模型的稳健性。 |
| [^77] | [Post-Quantum Cryptography Neural Network](https://arxiv.org/abs/2402.16002) | 提出了一种将基于编码的后量子密码方法映射到神经网络结构的PQC方法，通过非线性激活函数、随机扰动的密文和密文的均匀分布增强密文安全性。 |
| [^78] | [Cieran: Designing Sequential Colormaps via In-Situ Active Preference Learning](https://arxiv.org/abs/2402.15997) | Cieran是一个允许数据分析师在Jupyter笔记本中设计图表时快速找到质量配色方案的工具，通过主动偏好学习范式进行排序和创建新的配色方案，帮助新手分析师定制配色方案以适应其数据背景。 |
| [^79] | [Improved Hardness Results for Learning Intersections of Halfspaces](https://arxiv.org/abs/2402.15995) | 我们通过展示学习在维度N中的$\omega(\log \log N)$个半空间甚至需要超多项式时间的标准假设，显著缩小了这一差距 |
| [^80] | [Optimizing Portfolio Management and Risk Assessment in Digital Assets Using Deep Learning for Predictive Analysis](https://arxiv.org/abs/2402.15994) | 通过将DQN算法引入资产管理投资组合中，本研究证明了DRL算法在投资组合管理中的有效性 |
| [^81] | [Learning method for S4 with Diagonal State Space Layers using Balanced Truncation](https://arxiv.org/abs/2402.15993) | 一种用于处理长序列数据的边缘智能应用的S4模型的学习方法，利用平衡截断技术降低计算成本，并通过改进初始化过程和优化准确度和效率指标来超越传统训练模型。 |
| [^82] | [A Machine Learning Approach to Detect Customer Satisfaction From Multiple Tweet Parameters](https://arxiv.org/abs/2402.15992) | 使用机器学习方法分析推文以确定客户满意水平，有助于简化研究成千上万条推文并改进航空公司服务的繁琐过程。 |
| [^83] | [Towards Fair Graph Anomaly Detection: Problem, New Datasets, and Evaluation](https://arxiv.org/abs/2402.15988) | 这项研究提出了公平图异常检测（FairGAD）问题，介绍了来自Reddit和Twitter的两个新图数据集，填补了当前文献在此问题上的空白。 |
| [^84] | [Phonetic and Lexical Discovery of a Canine Language using HuBERT](https://arxiv.org/abs/2402.15985) | 使用HuBERT实现了对犬叫声的声素标签分类和词汇识别，发现了具有显著声学一致性的犬词汇，还开发了一个Web系统标记狗叫声中的声素n-gram。 |
| [^85] | [A unified Fourier slice method to derive ridgelet transform for a variety of depth-2 neural networks](https://arxiv.org/abs/2402.15984) | 通过使用傅里叶表达式导出尖峰变换，实现了对各种现代神经网络的描述和分析。 |
| [^86] | [Shaving Weights with Occam's Razor: Bayesian Sparsification for Neural Networks Using the Marginal Likelihood](https://arxiv.org/abs/2402.15978) | 提出了一种基于边缘似然的贝叶斯稀疏化神经网络的方法，通过有效利用贝叶斯边缘似然和稀疏诱导先验，使神经网络更易稀疏化，并采用自动奥卡姆剃刀选择最适合的模型，以实现高效的权重削减。 |
| [^87] | [Structural Knowledge-Driven Meta-Learning for Task Offloading in Vehicular Networks with Integrated Communications, Sensing and Computing](https://arxiv.org/abs/2402.15972) | 研究了一种基于集成通信、感知和计算的任务卸载范式，旨在优化计算模式和网络资源，降低资源消耗成本，并保证任务的延迟。 |
| [^88] | [CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models](https://arxiv.org/abs/2402.15968) | CoDream提出了一种通过在输入数据空间中协作优化数据来交换知识的框架，实现了模型之间的合作学习，实现了模型架构无关、通信不受模型大小影响、兼容安全聚合的优点。 |
| [^89] | [Hierarchical energy signatures using machine learning for operational visibility and diagnostics in automotive manufacturing](https://arxiv.org/abs/2402.15962) | 使用层次化机器学习方法对汽车制造过程的能量消耗数据进行分析，可实现更好的操作可见性和识别节能机会。 |
| [^90] | [On the dynamics of three-layer neural networks: initial condensation](https://arxiv.org/abs/2402.15958) | 深入研究三层神经网络训练中的凝聚现象和梯度下降方法自发减少神经网络复杂性的机制，提出有效动力学的爆炸性质和凝聚发生的充分条件，并通过实验证实了这些发现。 |
| [^91] | [DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement Learning](https://arxiv.org/abs/2402.15957) | DynaMITE-RL 提出了一种动态模型元强化学习方法，通过一致性的潜在信息、会话掩码和先验潜在条件等关键修改，在各种领域中实现了比最先进基线更优异的样本效率和推理结果。 |
| [^92] | [GreenLLaMA: A Framework for Detoxification with Explanations](https://arxiv.org/abs/2402.15951) | GreenLLaMA是一种全面的端到端解毒框架，通过跨平台语料库训练出的模型优于当前最先进的模型。 |
| [^93] | [Implementing Recycling Methods for Linear Systems in Python with an Application to Multiple Objective Optimization](https://arxiv.org/abs/2402.15941) | 优化多目标优化问题中线性系统的计算效率，实现了RMINRES方法在Python和PyTorch中的应用 |
| [^94] | [Deep Separable Spatiotemporal Learning for Fast Dynamic Cardiac MRI](https://arxiv.org/abs/2402.15939) | 提出了一种使用深度可分离时空学习网络（DeepSSL）的方法，结合时空先验开发了一个高效的图像重建方案，即使在训练数据有限的情况下也表现出色。 |
| [^95] | [Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models](https://arxiv.org/abs/2402.15938) | 本文提出了一种通过LLMs输出分布进行污染检测的方法CDD，以及一种基于LLMs输出修正的可信评估方法TED，以应对大语言模型在数据污染和可信评估方面面临的挑战。 |
| [^96] | [Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA](https://arxiv.org/abs/2402.15933) | 通过问题条件的2D视图选择过程和双分支Transformer结构，将2D知识整合到3D-VQA系统中，从而弥补了当前方法在3D视觉问答中遇到的挑战。 |
| [^97] | [Scalable Volt-VAR Optimization using RLlib-IMPALA Framework: A Reinforcement Learning Approach](https://arxiv.org/abs/2402.15932) | 使用RLlib-IMPALA框架的可扩展Volt-VAR优化方法利用深度强化学习和分布式计算加速了电力系统中的VVO解决方案搜索。 |
| [^98] | [QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs](https://arxiv.org/abs/2402.15929) | 本文提出了一种新颖的认证框架QuaCer-C，用于正式认证大型语言模型中知识理解的能力，证书定量化且包含高置信度的概率界限，研究发现，随着参数数量的增加，知识理解能力提高，Mistral模型在这一评估中表现不如其他模型。 |
| [^99] | [Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency](https://arxiv.org/abs/2402.15926) | 该研究表明对于具有线性可分数据的逻辑回归问题，设置一个恒定但较大的步长，在初始震荡后可以实现较快的收敛，并且在一定步骤后可以达到加速的收敛速率，这种方法无需动量或变步长调度器。 |
| [^100] | [Predicting Outcomes in Video Games with Long Short Term Memory Networks](https://arxiv.org/abs/2402.15923) | 使用长短期记忆网络在实时分析中预测电子竞技比赛结果，仅利用玩家生命值指标作为时间序列，较之传统方法和Transformer模型，取得了更高效的预测性能。 |
| [^101] | [Pretraining Strategy for Neural Potentials](https://arxiv.org/abs/2402.15921) | 通过提出的面向图神经网络的掩码预训练方法，改善了GNN在拟合水系统势能表面方面的性能，并在精度和收敛速度上优于从头开始训练或使用其他预训练技术。 |
| [^102] | [Explainable Contrastive and Cost-Sensitive Learning for Cervical Cancer Classification](https://arxiv.org/abs/2402.15905) | 本文提出了一个高效的宫颈癌细胞分类系统，通过使用预训练的CNNs进行微调并结合监督对比学习，最小化误分类成本，达到了97.29%的准确率，并引入了可解释的人工智能技术来解释模型的决策过程。 |
| [^103] | [ESFL: Efficient Split Federated Learning over Resource-Constrained Heterogeneous Wireless Devices](https://arxiv.org/abs/2402.15903) | 该论文提出了一种高效的分裂联邦学习算法（ESFL），能够充分利用中央服务器和端设备的计算资源，通过将模型分为不同的子模型并考虑用户的异质性，共同优化用户端工作量和服务器端计算资源分配。 |
| [^104] | [Information-based Transductive Active Learning](https://arxiv.org/abs/2402.15898) | ITL是一种基于信息的转导式学习方法，可以在现实世界设置中自适应采样，以最大化关于指定预测目标的信息获取，并在少样本微调和安全贝叶斯优化应用中显著优于最先进技术。 |
| [^105] | [Concurrent Learning of Policy and Unknown Safety Constraints in Reinforcement Learning](https://arxiv.org/abs/2402.15893) | 提出了一种同时学习安全RL控制策略和识别未知安全约束参数的新方法。 |
| [^106] | [Statistical Games](https://arxiv.org/abs/2402.15892) | 本研究将Bayesian统计嵌入到更广泛的决策框架中，提出了统计游戏作为统一框架，涵盖了频率派和贝叶斯统计，并提出了最小后悔准则作为决策的一般方法。 |
| [^107] | [Fusion Encoder Networks](https://arxiv.org/abs/2402.15883) | FENs是一种神经网络算法，具有对数深度且可以在线性时间内处理序列，关键创新在于通过训练大致线性数量的常深度神经网络并行学习。 |
| [^108] | [Field-based Molecule Generation](https://arxiv.org/abs/2402.15864) | 介绍了一种基于场的模型用于生成拟药分子，相比基于点云的方法具有灵活性和竞争性，能够解决对映异构体问题，进而考虑所有分子几何方面。 |
| [^109] | [Prompt Perturbation Consistency Learning for Robust Language Models](https://arxiv.org/abs/2402.15833) | 微调大型语言模型可以产生与判别模型相当的性能，在分析和解决LLMs对输入提示中不同类型扰动的鲁棒性方面取得了重要进展 |
| [^110] | [Reward Design for Justifiable Sequential Decision-Making](https://arxiv.org/abs/2402.15826) | 代理通过辩论型奖励模型学习可辩明策略，以支持证据证明决策的合理性。 |
| [^111] | [Debiased Model-based Interactive Recommendation](https://arxiv.org/abs/2402.15819) | 该论文提出了一种名为iDMIR的模型，通过设计基于因果机制的偏见消除因果世界模型来克服传统基于模型的交互式推荐系统中存在的流行度和抽样偏见的问题。 |
| [^112] | [A Generative Machine Learning Model for Material Microstructure 3D Reconstruction and Performance Evaluation](https://arxiv.org/abs/2402.15815) | 提出了一种将 U-net 和 GAN 结合的生成模型，具有多尺度特性和生成能力，创新性地构建了多尺度通道聚合模块、多尺度分层特征聚合模块和卷积块注意机制 |
| [^113] | [A Theoretical Result on the Inductive Bias of RNN Language Models](https://arxiv.org/abs/2402.15814) | RNN语言模型能够有效表示更大类别的语言模型，具有有界堆栈和广义堆栈更新函数，类似于保留固定数量符号记忆并使用简单更新机制的自动机。 |
| [^114] | [OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining](https://arxiv.org/abs/2402.15810) | OAG-Bench是一个基于开放学术图的全面、多方面和精细化人工筛选基准，涵盖了多个任务、数据集、基准和实验结果，旨在促进学术图挖掘。 |
| [^115] | [Optimal Zero-Shot Detector for Multi-Armed Attacks](https://arxiv.org/abs/2402.15808) | 本文提出了一种创新的信息论防御方法，通过最优地汇总现有探测器做出的决策，消除了对训练数据的需求。 |
| [^116] | [Analysis of Off-Policy Multi-Step TD-Learning with Linear Function Approximation](https://arxiv.org/abs/2402.15781) | 该论文分析了在具有线性函数逼近、离策略学习和自举的“致命三连”场景中的多步TD学习算法，并证明了当采样时间跨度 n 足够大时这些算法会收敛到有意义的解。 |
| [^117] | [Truly No-Regret Learning in Constrained MDPs](https://arxiv.org/abs/2402.15776) | 本文首次肯定回答了一个开放问题，即是否可以在不允许错误抵消的情况下，通过将一种常见的安全约束模型扩展到具有多个约束的CMDPs，提出了一种可以实现次线性后悔的新方法。 |
| [^118] | [Batch Active Learning of Reward Functions from Human Preferences](https://arxiv.org/abs/2402.15757) | 本文提出了一种批量主动基于偏好的学习方法，通过少量数据样本有效学习奖励函数，同时保持查询生成时间短并可并行化。 |
| [^119] | [Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning](https://arxiv.org/abs/2402.15751) | 提出了一种稀疏MeZO方法，通过仅对精心选择的参数子集应用零阶优化，实现了在零阶LLM微调中减少参数以获得更好性能的目标 |
| [^120] | [Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery](https://arxiv.org/abs/2402.15739) | 该论文介绍了一种解决低秩环境中具有上下文信息的赌徒问题的高效算法，其中包括策略评估、最佳策略识别和遗憾最小化，并且在最佳策略识别和策略评估方面的算法几乎是极小极大最优的。 |
| [^121] | [Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning](https://arxiv.org/abs/2402.15734) | 该论文提出了一种通过无监督预训练和上下文学习方法实现PDE运算符学习的高效方式，以提高数据效率并改善模型的外域性能。 |
| [^122] | [ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters](https://arxiv.org/abs/2402.15733) | 该论文介绍了一种用于阿拉伯字符的EEG数据集ArEEG_Chars，通过深度学习实现97%的准确率，在脑机接口中具有重要意义。 |
| [^123] | [Clustering in Dynamic Environments: A Framework for Benchmark Dataset Generation With Heterogeneous Changes](https://arxiv.org/abs/2402.15731) | 本论文引入了Dynamic Dataset Generator（DDG）来解决在动态环境中进行聚类时缺乏多样性、可控性和现实性动态数据集的问题，从而帮助设计更有效的聚类算法。 |
| [^124] | [Understanding Missingness in Time-series Electronic Health Records for Individualized Representation](https://arxiv.org/abs/2402.15730) | 该研究围绕在时间序列电子健康记录中对缺失性进行个性化表示展开，为真正个性化的预测建模提供了新见解。 |
| [^125] | [Operator Learning: Algorithms and Analysis](https://arxiv.org/abs/2402.15715) | 神经算子是近似非线性算子映射的机器学习方法，可作为高效替代模型应用于多查询任务，尤其在没有数学描述的情况下进行模型发现。 |
| [^126] | [A Statistical Analysis of Wasserstein Autoencoders for Intrinsically Low-dimensional Data](https://arxiv.org/abs/2402.15710) | 本文从统计分析的角度探讨了Wasserstein自编码器用于内在低维数据的特性与局限。 |
| [^127] | [Is Offline Decision Making Possible with Only Few Samples? Reliable Decisions in Data-Starved Bandits via Trust Region Enhancement](https://arxiv.org/abs/2402.15703) | 本文展示了即使在数据稀缺的情况下，仍然可能找到一个与最优策略竞争的随机策略，为在仅有少量样本下进行可靠决策铺平了道路。 |
| [^128] | [CoRelation: Boosting Automatic ICD Coding Through Contextualized Code Relation Learning](https://arxiv.org/abs/2402.15700) | 通过上下文化的编码关系学习，提出了一种新的框架来增强ICD编码表示的学习，实验结果表明其相比最先进基线方法的有效性。 |
| [^129] | [Orthogonal Gradient Boosting for Simpler Additive Rule Ensembles](https://arxiv.org/abs/2402.15691) | 提出了一种正交梯度提升方法，通过新的目标函数促进生成更加简化的加法规则集合，提高了模型的解释性和准确性。 |
| [^130] | [Anchor-free Clustering based on Anchor Graph Factorization](https://arxiv.org/abs/2402.15688) | 无锚聚类方法AFCAGF通过学习锚图并优化成对样本距离，避免了锚点选择和初始化的需要，提升了聚类算法性能。 |
| [^131] | [Overcoming Pitfalls in Graph Contrastive Learning Evaluation: Toward Comprehensive Benchmarks](https://arxiv.org/abs/2402.15680) | 本文深入研究了图对比学习方法评估中的缺陷，并提出了对超参数选择和下游任务选择影响的新观点，同时引入了一个增强型评估框架。 |
| [^132] | [Scalable Density-based Clustering with Random Projections](https://arxiv.org/abs/2402.15679) | sDBSCAN是一种利用随机投影进行高维密度聚类的算法，在速度和准确性上显著优于其他算法。 |
| [^133] | [Universal Model in Online Customer Service](https://arxiv.org/abs/2402.15666) | 本文介绍了一种在电子商务中改进在线客户服务的解决方案，即提出了一种基于客户问题预测标签的通用模型，无需进行训练，通过消除个别模型训练和维护的需求，减少了模型开发周期和成本。 |
| [^134] | [Teacher-Student Learning on Complexity in Intelligent Routing](https://arxiv.org/abs/2402.15665) | 通过机器学习框架中的师生模型，成功预测客户联系的复杂性并将其引导到合适的代理商，提高了客户体验，并提出了复杂性AUC度量标准。 |
| [^135] | [Learning Semilinear Neural Operators : A Unified Recursive Framework For Prediction And Data Assimilation](https://arxiv.org/abs/2402.15656) | 提出了一种学习半线性神经算子的方法，通过结合预测和校正操作实现了对长时间尺度上时空PDE的解进行处理与数据同化。 |
| [^136] | [Contact Complexity in Customer Service](https://arxiv.org/abs/2402.15655) | 开发了一种新颖的机器学习方法来定义联系复杂性，通过训练AI专家模型来评估客户问题复杂性，避免了人工标注的时间和金钱成本。 |
| [^137] | [Multi-Constraint Safe RL with Objective Suppression for Safety-Critical Applications](https://arxiv.org/abs/2402.15650) | 提出了一种目标抑制的新方法，可以在多约束安全领域中改进安全强化学习任务表现，实验证明此方法结合现有算法能够在减少约束违规的情况下实现与基准线相当的任务奖励水平。 |
| [^138] | [Fair Resource Allocation in Multi-Task Learning](https://arxiv.org/abs/2402.15638) | 通过将多任务学习的优化问题形式化为效用最大化问题，并提出FairGrad方法，实现了对不同任务损失递减的公平优化，同时具有理论收敛保证。 |
| [^139] | [Smooth and Sparse Latent Dynamics in Operator Learning with Jerk Regularization](https://arxiv.org/abs/2402.15636) | 引入了连续的操作员学习框架，将Jerk正则化纳入压缩潜在空间的学习中，促进了潜在空间动态的平滑性和稀疏性 |
| [^140] | [Bagged Deep Image Prior for Recovering Images in the Presence of Speckle Noise](https://arxiv.org/abs/2402.15635) | 该论文在斑点噪声存在情况下提出了裹袋式深度图像先验（Bagged-DIP）的概念，并将其与投影梯度下降算法集成，以及通过在迭代中使用Newton-Schulz算法来减少算法的计算复杂度。 |
| [^141] | [MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs](https://arxiv.org/abs/2402.15627) | MegaScale项目介绍了一个用于在超过10,000个GPU规模上训练大型语言模型的生产系统，通过全栈方法协同设计算法和系统组件来解决训练效率和稳定性挑战。 |
| [^142] | [Learning Cyclic Causal Models from Incomplete Data](https://arxiv.org/abs/2402.15625) | 提出了一个名为MissNODAGS的框架，可以从部分缺失数据中学习循环因果图，通过交替替补缺失数据和最大化可见数据部分的预期对数似然来学习因果图。 |
| [^143] | [Language-Based User Profiles for Recommendation](https://arxiv.org/abs/2402.15623) | 通过使用以人类可读文本表示的用户偏好，提出了Language-based Factorization Model (LFM)，在冷启动环境中与传统方法相比取得了更好的表现 |
| [^144] | [Towards Efficient Active Learning in NLP via Pretrained Representations](https://arxiv.org/abs/2402.15613) | 通过在主动学习循环中使用预训练LLMs的表示，可以显著加快标记数据获取的过程，并通过微调获得最佳性能，同时大大降低计算开销。 |
| [^145] | [Data/moment-driven approaches for fast predictive control of collective dynamics](https://arxiv.org/abs/2402.15611) | 提出了基于数据/瞬间驱动的两种替代MPC的方法，实现大规模粒子系统的快速、实时反馈合成。 |
| [^146] | [Machine Learning-Based Completions Sequencing for Well Performance Optimization](https://arxiv.org/abs/2402.15608) | 该研究旨在开发能够准确预测12个月累计产量的有效机器学习模型，以优化油井性能。 |
| [^147] | [Training Nonlinear Transformers for Efficient In-Context Learning: A Theoretical Learning and Generalization Analysis](https://arxiv.org/abs/2402.15607) | 本文首次提供了对具有非线性自注意力和非线性MLP的Transformers的训练动态以及由此产生的模型的ICL泛化能力的理论分析。 |
| [^148] | [Differentially Private Fair Binary Classifications](https://arxiv.org/abs/2402.15603) | 该论文提出了一种差分隐私与公平性约束下的二元分类算法，通过解耦技术和差分隐私的引入，实现了在保证公平性的同时提升了隐私性能和效用保证。 |
| [^149] | [Minimax Optimality of Score-based Diffusion Models: Beyond the Density Lower Bound Assumptions](https://arxiv.org/abs/2402.15602) | 该研究展示了基于分数的扩散模型的采样具有极小均方误差，可以获得扩散模型生成样本的总变差误差的上界，这突破了仅做次高斯假设的限制。 |
| [^150] | [Neural optimal controller for stochastic systems via pathwise HJB operator](https://arxiv.org/abs/2402.15592) | 本文提出了一种基于路径HJB算子的随机系统神经最优控制器的算法，通过引入基于物理信息学习的方法解决高维随机控制问题，展示了其在各种应用程序上的性能。 |
| [^151] | [Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts](https://arxiv.org/abs/2402.15589) | 本文研究了使用不同类型/级别的提示来激发三种流行LLM，GPT-3.5、LLaMA2和PaLM2，在学术同行评审过程中自动生成元评论，并进行了详细的定性研究。 |
| [^152] | [State Space Models for Event Cameras](https://arxiv.org/abs/2402.15584) | 通过引入具有可学习时间尺度参数的状态空间模型（SSMs），以适应不同频率而无需重新训练网络，并研究了两种对抗混叠效应的策略，该方法训练速度快33%。 |
| [^153] | [Cohere3D: Exploiting Temporal Coherence for Unsupervised Representation Learning of Vision-based Autonomous Driving](https://arxiv.org/abs/2402.15583) | 提出了Cohere3D，利用对比学习算法在长期输入序列中学习一致实例表征，有助于视觉自动驾驶中的多帧实例级对应关系。 |
| [^154] | [Toward Fully Self-Supervised Multi-Pitch Estimation](https://arxiv.org/abs/2402.15569) | 提出一套自监督学习目标，用于多音高估计，训练完全卷积自动编码器生成多音高显著图，无需微调 |
| [^155] | [Foundation Policies with Hilbert Representations](https://arxiv.org/abs/2402.15567) | 该研究提出了一个新颖的无监督框架，用于从未标记的离线数据中预训练通用政策，以捕获多样化、最优、长时域行为。 |
| [^156] | [Closing the AI generalization gap by adjusting for dermatology condition distribution differences across clinical settings](https://arxiv.org/abs/2402.15566) | 本研究表明，在AI算法从未见数据源上评估时出现错误的主要原因是皮肤病分布的差异，而不是人口统计数据或图像捕捉模式，提出了一系列步骤来解决这一泛化差距。 |
| [^157] | [Fair Multivariate Adaptive Regression Splines for Ensuring Equity and Transparency](https://arxiv.org/abs/2402.15561) | 提出了一种基于多元自适应回归样条的公平预测模型，该模型在学习过程中融入了公平性评估。 |
| [^158] | [Deep Networks Always Grok and Here is Why](https://arxiv.org/abs/2402.15555) | 深度神经网络存在延迟泛化和延迟鲁棒性现象，在各种实际环境中普遍存在，并基于新的局部复杂度度量提供了解释。 |
| [^159] | [HiMAP: Learning Heuristics-Informed Policies for Large-Scale Multi-Agent Pathfinding](https://arxiv.org/abs/2402.15546) | HiMAP 是一种新颖的可伸缩方法，使用启发式引导的模仿学习在分散式训练中对用户智体路径规划进行了改进 |
| [^160] | [Streaming IoT Data and the Quantum Edge: A Classic/Quantum Machine Learning Use Case](https://arxiv.org/abs/2402.15542) | 量子机器学习的集成面临挑战，本研究通过边缘计算探索了将量子机器学习应用于分布式计算的可能性。 |
| [^161] | [Evaluating the Performance of ChatGPT for Spam Email Detection](https://arxiv.org/abs/2402.15537) | 该研究评估了ChatGPT在英文和中文电子邮件数据集中用于垃圾邮件检测的性能，并探讨了其在这一领域的潜力。 |
| [^162] | [DiCoM -- Diverse Concept Modeling towards Enhancing Generalizability in Chest X-Ray Studies](https://arxiv.org/abs/2402.15534) | DiCoM是一种新颖的自监督训练范式，通过学习多元概念，有效表示胸部X射线数据，以应对医学成像预训练中与自然图像不同的挑战。 |
| [^163] | [Chain-of-Specificity: An Iteratively Refining Method for Eliciting Knowledge from Large Language Models](https://arxiv.org/abs/2402.15526) | Chain-of-Specificity (CoS)是一种从大型语言模型中提取知识的逐步精化方法，能够在输入指令中迭代强调特定约束，解锁知识并改进回应。 |
| [^164] | [Graph Pruning for Enumeration of Minimal Unsatisfiable Subsets](https://arxiv.org/abs/2402.15524) | 提出了一种图剪枝算法，通过基于图的学习模型预测修剪公式的部分，加速枚举最小不可满足子集，无需数据标记，也无需来自目标应用的训练数据，实验结果表明在各种基准测试中的有效性。 |
| [^165] | [HKD-SHO: A hybrid smart home system based on knowledge-based and data-driven services](https://arxiv.org/abs/2402.15521) | 提出了一个名为HKD-SHO的混合智能家居系统，将基于知识和基于机器学习的数据驱动服务有益地融合在一起，解决了智能家居系统中基于知识和数据驱动方法的问题。 |
| [^166] | [GLA-Grad: A Griffin-Lim Extended Waveform Generation Diffusion Model](https://arxiv.org/abs/2402.15516) | GLA-Grad提出了一种新方案，利用格里芬-林算法（GLA）在波形生成扩散模型的每一步中来最小化条件错误，提高噪声扩散过程的效率。 |
| [^167] | [Investigating the Generalizability of Physiological Characteristics of Anxiety](https://arxiv.org/abs/2402.15513) | 评估了焦虑和压力相关的生理特征在其他高激活情绪中的泛化能力。 |
| [^168] | [AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning](https://arxiv.org/abs/2402.15506) | AgentOhana提供了一种统一数据和训练流水线的综合解决方案，有助于克服使用大型语言模型（LLMs）进行智能体任务时的挑战。 |
| [^169] | [ArabianGPT: Native Arabic GPT-based Large Language](https://arxiv.org/abs/2402.15313) | 提出了ArabianGPT，这是一系列专门为阿拉伯语设计的基于Transformer的模型，包括大小和复杂性不同的ArabianGPT-0.1B和ArabianGPT-0.3B，帮助弥补了本土阿拉伯语大型语言模型的不足。 |
| [^170] | [Neural Implicit Swept Volume Models for Fast Collision Detection](https://arxiv.org/abs/2402.15281) | 提出了一种新颖的神经隐式扫描体模型，能够连续表示任意运动，并结合了深度学习速度和几何碰撞检查的准确性保证。 |
| [^171] | [Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition](https://arxiv.org/abs/2402.15175) | 提供了一个全面框架，统一解释了Grokking、双重下降和新兴能力这三种现象，着重探讨了记忆和泛化电路之间的竞争。 |
| [^172] | [Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach](https://arxiv.org/abs/2402.14948) | 提出了基于课程的正无标记学习 CuPUL 方法，能够显著降低嘈杂标签的影响，胜过现有方法 |
| [^173] | [Vygotsky Distance: Measure for Benchmark Task Similarity](https://arxiv.org/abs/2402.14890) | 论文提出了一种基于相对性能而非任务属性的相似性度量方法，即“维果茨基距离”，可帮助减少评估任务数量并保持高验证质量。 |
| [^174] | [A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health](https://arxiv.org/abs/2402.14807) | 提出了一种决策语言模型DLM，旨在通过使用LLMs作为自动规划器，动态微调RMAB策略，以应对公共卫生中具有挑战性的情境。 |
| [^175] | [Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs](https://arxiv.org/abs/2402.14740) | 在大型语言模型中，重新审视REINFORCE风格优化对于学习人类反馈具有重要意义，简化优化方法可以提高性能。 |
| [^176] | [Federated Complex Qeury Answering](https://arxiv.org/abs/2402.14609) | 研究了在多源知识图谱上回答复杂查询的联邦式方法，解决了知识图谱中的隐私保护和答案检索的挑战 |
| [^177] | [From Large to Small Datasets: Size Generalization for Clustering Algorithm Selection](https://arxiv.org/abs/2402.14332) | 通过引入尺寸泛化概念，研究了在半监督设置下的聚类算法选择问题，提出了能够在小实例上保证准确度最高的算法也将在原始大实例上拥有最高准确度的条件。 |
| [^178] | [A Temporal Bias Correction using a Machine Learning Attention model](https://arxiv.org/abs/2402.14169) | 本论文提出了一种新颖的偏差校正方法，将校准视为概率模型而不是算法流程，利用机器学习概率注意力模型来适配偏差校正任务，可准确校正具有长期时间属性的气候统计数据，提高了在这些数据上进行可靠影响研究的准确性。 |
| [^179] | [SDXL-Lightning: Progressive Adversarial Diffusion Distillation](https://arxiv.org/abs/2402.13929) | 提出了一种结合渐进和对抗性蒸馏的扩散蒸馏方法，在文本到图像生成任务中取得了新的最先进结果，并开源了相应模型。 |
| [^180] | [BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery](https://arxiv.org/abs/2402.13918) | 本文通过对遥感图像中的云进行分割，旨在提高卫星图像分析的精度和效率，应用于环境监测、资源管理和灾害响应。 |
| [^181] | [Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions](https://arxiv.org/abs/2402.13777) | 深度生成模型在离线策略学习中展现了巨大潜力，本文提供了首个系统性综述，涵盖了五种主流深度生成模型及其应用。 |
| [^182] | [HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts](https://arxiv.org/abs/2402.12656) | HyperMoE通过Hypernetworks框架整合知识传递的概念，解决了在专家选择过程中专家知识稀疏性和可用性之间的矛盾。 |
| [^183] | [Hierarchical Bayes Approach to Personalized Federated Unsupervised Learning](https://arxiv.org/abs/2402.12537) | 该论文提出了基于分层贝叶斯统计框架的算法，用于个性化无监督学习，其中开发了适应性算法来平衡利用有限本地数据和协作信息。 |
| [^184] | [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/abs/2402.12226) | AnyGPT是一个统一的多模态语言模型，通过离散表示实现各种模态的统一处理，能够在不改变大型语言模型架构或训练方式的情况下稳定训练，为新模态的无缝整合提供了可能。 |
| [^185] | [Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning](https://arxiv.org/abs/2402.12177) | Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。 |
| [^186] | [The effect of Leaky ReLUs on the training and generalization of overparameterized networks](https://arxiv.org/abs/2402.11942) | Leaky ReLU参数$\alpha=-1$在训练误差和泛化误差界方面是最优的选择。 |
| [^187] | [Generative Kaleidoscopic Networks](https://arxiv.org/abs/2402.11793) | 发现深层ReLU网络表现出过度泛化现象，利用这一特性设计了“生成万花筒网络”，通过递归映射随机输入噪声生成样本。 |
| [^188] | [Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark](https://arxiv.org/abs/2402.11592) | 本研究提出了一种不使用反向传播的零阶优化方法，用于降低LLM微调中的内存成本，通过全面的基准研究扩展了对不同的ZO优化技术的探索。 |
| [^189] | [Evaluating the Stability of Deep Learning Latent Feature Spaces](https://arxiv.org/abs/2402.11404) | 评估深度学习潜在特征空间稳定性的新方法，引入了可以确保一致性和可靠性的稳定性评估工作流程，包括了三种稳定性类型和一套全面评估的度量标准。 |
| [^190] | [Physics-based material parameters extraction from perovskite experiments via Bayesian optimization](https://arxiv.org/abs/2402.11101) | 使用贝叶斯优化开发了一个分析平台，可以从钙钛矿实验中提取多个基本材料参数，加速材料发现和半导体优化 |
| [^191] | [Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model](https://arxiv.org/abs/2402.10965) | 大型语言模型在医疗健康领域有着重要作用，然而它们的泛化效果取决于在不同临床环境和人群中的表现，对于泛化能力不足的原因进行了分析，发现在样本较少的医院和特定人群中存在挑战。 |
| [^192] | [Robust agents learn causal world models](https://arxiv.org/abs/2402.10877) | 智能体必须学习因果模型才能在广泛的分布转变下达到后悔界限，这对迁移学习和因果推断等研究领域有重要影响。 |
| [^193] | [TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly Detection Models](https://arxiv.org/abs/2402.10802) | 时间序列异常检测模型的工业级基准TimeSeriesBench填补了当前算法在训练范式、在线检测范式和评估标准方面与实际需求之间的差距。 |
| [^194] | [Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment](https://arxiv.org/abs/2402.10207) | 本文介绍了Rewards-in-Context（RiC）方法，该方法通过多个奖励条件控制基础模型的响应，并应用有监督的微调进行对齐。它具有简单性和适应性，并支持在推理时动态调整用户偏好。 |
| [^195] | [Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion](https://arxiv.org/abs/2402.10009) | 本文研究了使用DDPM反转进行音频信号的零样本编辑技术，包括基于文本的编辑和无监督发现编辑方向。这些方法在音乐信号中展现了多样的音乐兴趣修改。 |
| [^196] | [Why are Sensitive Functions Hard for Transformers?](https://arxiv.org/abs/2402.09963) | 本文证明了在Transformer架构下，损失函数的空间受到输入敏感性的限制，从而解释了Transformer对敏感函数的困难。这一理论统一了关于Transformer学习能力和偏见的广泛观察。 |
| [^197] | [Optimistic Thompson Sampling for No-Regret Learning in Unknown Games](https://arxiv.org/abs/2402.09456) | 该论文提出了一种在未知博弈中进行无遗憾学习的乐观的汤普森抽样方法，通过利用对手的行动和奖励结构信息，显著减少了实验预算，成功地缓解了多机构问题。此外，研究还引入了乐观-无遗憾框架，将现有算法与提出的方法相结合。 |
| [^198] | [Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283) | 这篇调查提供了LLM对话安全性的全面概述，涵盖了攻击、防御和评估三个关键方面，旨在提高对该主题的理解并促进进一步的研究。 |
| [^199] | [Homomorphism Counts for Graph Neural Networks: All About That Basis](https://arxiv.org/abs/2402.08595) | 本研究展示了基于图神经网络的同态计数对于增强其表达能力的重要性，并提出了一种更细致的方法来融合目标模式的同态计数。这种方法比现有方法更具表达力且没有额外的计算复杂度开销。 |
| [^200] | [Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering](https://arxiv.org/abs/2402.08277) | 这项工作探索了如何鲁棒地微调大型语言模型以提高答案的来源质量和答案归因能力，引入了数据生成流水线和四个测试集来评估模型的性能，并展示了在合成数据上微调可以改善内部和外部分布的性能。 |
| [^201] | [Policy Improvement using Language Feedback Models](https://arxiv.org/abs/2402.07876) | 本文介绍了一种使用语言反馈模型（LFMs）改进政策的方法，通过识别期望的行为并进行模仿学习，我们在任务完成率、泛化性能和人类可解释性方面取得了显著改进。 |
| [^202] | [Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study](https://arxiv.org/abs/2402.07281) | 本文通过一项基准研究评估了多种基于机器学习的异常检测算法，包括树结构方法和深度学习方法，并揭示了深度学习神话的真相。 |
| [^203] | [Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey](https://arxiv.org/abs/2402.05391) | 知识图谱与多模态学习的综述介绍了KG4MM和MM4KG两个主要方面，包括任务定义、构建进展、评估基准以及关键研究轨迹。 |
| [^204] | [Combining Cloud and Mobile Computing for Machine Learning](https://arxiv.org/abs/2402.04880) | 这项研究将模型分割为移动设备和云之间的计算，以减轻移动设备的负担，并优化云端的工作负载。 |
| [^205] | [On Provable Length and Compositional Generalization](https://arxiv.org/abs/2402.04875) | 本研究针对包括深度集合、变压器、状态空间模型和简单递归神经网络等多种架构，探索了可证明的长度和组合泛化，认为对于长度和组合泛化，不同架构需要不同程度的表示识别。 |
| [^206] | [Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming](https://arxiv.org/abs/2402.04830) | 本研究介绍了dSGP4，一种使用PyTorch实现的可微版本的SGP4。通过可微化，dSGP4实现了轨道传播的高精度，并且适用于各种与太空相关的应用，包括卫星轨道确定、状态转换、协方差传播等。 |
| [^207] | [PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks](https://arxiv.org/abs/2402.04284) | 这篇论文研究了如何实现可扩展的基于内存的动态图神经网络，并解决了训练中的时间间断问题。通过使用内存模块提取和记忆长期的时间依赖关系，MDGNNs在处理大的时间批量时表现出更好的性能和灵活性。 |
| [^208] | [RevOrder: A Novel Method for Enhanced Arithmetic in Language Models](https://arxiv.org/abs/2402.03822) | 本文提出了一种名为RevOrder的新方法，通过翻转加法、减法和nD乘以1D的输出数字，显著改善了语言模型中的算术运算。经过全面测试，RevOrder在基本算术运算中达到了完美准确度，并在除法任务中提升了语言模型性能，特别是在处理大数时。在GSM8K数学任务中应用RevOrder进行微调，有效降低了错误率并提高了总体得分。 |
| [^209] | [Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification](https://arxiv.org/abs/2402.03780) | 本文通过分析文体线索比较人类注释和机器分类的方法，揭示了宣传语言的特征，并提出了一个多源、多语言、多模态的数据集PPN。结果表明，人类注释者能够可靠地区分宣传新闻和常规新闻。研究还比较了不同的自然语言处理技术，并提供了一些有关文体线索的发现。 |
| [^210] | [Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity](https://arxiv.org/abs/2402.03167) | 本文提出了一种单循环的去中心化双级优化算法（D-SOBA），首次阐明了网络拓扑和数据异构性对去中心化双级算法的共同影响。D-SOBA在渐近速率、渐近梯度/海森复杂性和瞬态梯度/海森复杂性方面达到了最先进水平。 |
| [^211] | [Spin: An Efficient Secure Computation Framework with GPU Acceleration](https://arxiv.org/abs/2402.02320) | Spin是一个GPU加速的多方计算(MPC)框架，支持多个计算方和不诚实多数对抗设置。该框架提出了针对机器学习关键的非线性函数的优化协议，并进行了针对Transformer模型的注意力的新颖优化，以实现高效且安全的计算。 |
| [^212] | [Vanilla Bayesian Optimization Performs Great in High Dimension](https://arxiv.org/abs/2402.02229) | 本文研究了高维情况下贝叶斯优化算法的问题，并提出了一种改进方法，通过对先验假设进行简单的缩放，使普通贝叶斯优化在高维任务中表现出色。 |
| [^213] | [Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic Forecasting](https://arxiv.org/abs/2402.00397) | 我们提出了一种跨城市少样本交通预测的解决方案，利用多尺度交通模式库从数据丰富的源城市学习并预测其他城市的交通情况。 |
| [^214] | [Freely Long-Thinking Transformer (FraiLT)](https://arxiv.org/abs/2401.11626) | FraiLT是一个改进的变压器模型，通过递归方法和迭代编码，实现了在紧凑形式下达到较大模型的解释深度，在性能表现上优于较大模型，旨在实现更高效和可访问的语言模型。 |
| [^215] | [GD-CAF: Graph Dual-stream Convolutional Attention Fusion for Precipitation Nowcasting](https://arxiv.org/abs/2401.07958) | GD-CAF提出了一种新颖的方法，将降水预报作为一个时空图序列预报问题，利用图形双流卷积注意力融合来学习历史降水图并在不同空间位置上预测未来的降水。 |
| [^216] | [MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts](https://arxiv.org/abs/2401.04081) | 结合混合专家模型的MoE-Mamba在性能上优于Mamba和基准Transformer-MoE，达到了与Mamba相同性能的同时，训练步骤减少了2.35倍。 |
| [^217] | [AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with Thirteen Modalities](https://arxiv.org/abs/2401.00546) | 提出了一个名为AllSpark的多模态时空智能通用人工智能模型，集成了十三种不同的模态，旨在解决多模态时空数据联合解释的挑战。 |
| [^218] | [Learning to Embed Time Series Patches Independently](https://arxiv.org/abs/2312.16427) | 学习独立嵌入时间序列片段可以产生更好的时间序列表示，通过简单的块重构任务和独立嵌入每个块的MLP模型以及互补对比学习来实现。 |
| [^219] | [Soft Contrastive Learning for Time Series](https://arxiv.org/abs/2312.16424) | 提出了一种名为SoftCLT的方法，通过引入实例级和时间级软对比损失，解决了在时间序列中忽略固有相关性所导致的学习表示质量下降的问题。 |
| [^220] | [RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair](https://arxiv.org/abs/2312.15698) | 高效表示和微调适配器相结合的新型程序修复方法RepairLLaMA可为语言模型修复错误产生高效的适配器。 |
| [^221] | [Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP](https://arxiv.org/abs/2312.12430) | 引入了通过广播查询编码器实现的高效标题重新排序器和为标题重新排序定制的Sigmoid Trick损失函数，相结合在KILT知识基准测试的数据集上取得了最先进的结果。 |
| [^222] | [Policy Optimization in RLHF: The Impact of Out-of-preference Data](https://arxiv.org/abs/2312.10584) | 本研究比较了直接偏好优化与基于奖励模型的政策优化方法，研究表明超出偏好数据对政策优化的影响，发现在实验证实中，RMB-PO+表现最佳。 |
| [^223] | [MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training](https://arxiv.org/abs/2312.08656) | MaxK-GNN是一种先进的高性能GPU训练系统，通过MaxK非线性和理论分析，实现了图神经网络训练的垂直优化。 |
| [^224] | [How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation](https://arxiv.org/abs/2312.07424) | 该研究对GPT-4V(ision)在动态环境中的适应能力和泛化能力进行了评估，对比了其与CLIP、LLaVA和Gemini等知名模型。 |
| [^225] | [Low-Cost High-Power Membership Inference Attacks](https://arxiv.org/abs/2312.03262) | 提出了一种新颖、高效且强大的成员推断攻击（RMIA），具有更准确的建模和更高的测试能力，适用于隐私风险评估。 |
| [^226] | [Learning Multi-graph Structure for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2312.03004) | 提出了一种关注学习多图结构的创新推理方法，用于解决时间知识图推理中存在的历史依赖和未来趋势反映不充分的问题。 |
| [^227] | [FRAPP\'E: A Group Fairness Framework for Post-Processing Everything](https://arxiv.org/abs/2312.02592) | 提出了一个将任何正则化的处理中方法转化为后处理方法的框架，适用于更广泛的问题设置，保持了良好的公平错误权衡，并且可能提高之前方法的效力 |
| [^228] | [ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference](https://arxiv.org/abs/2312.02554) | 提出了一种逐点直接偏好优化方法，用于统一语言模型对齐，通过将人类演示和逐点偏好相结合，解决了偏好学习中存在的信息丢失和性能次优问题。 |
| [^229] | [xTrimoGene: An Efficient and Scalable Representation Learner for Single-Cell RNA-Seq Data](https://arxiv.org/abs/2311.15156) | xTrimoGene是一种针对scRNA-seq数据的新型非对称编码器-解码器Transformer，利用数据的稀疏特性降低了计算复杂度，使得在保持高准确性的同时能够训练最大的转移学习模型。 |
| [^230] | [Tube-NeRF: Efficient Imitation Learning of Visuomotor Policies from MPC using Tube-Guided Data Augmentation and NeRFs](https://arxiv.org/abs/2311.14153) | 结合IL和鲁棒MPC，设计了一种名为Tube-NeRF的数据增强方法，利用NeRFs生成合成图像，通过管的特性选择相关视图，高效计算对应的动作，从而实现了视觉导向策略的高效学习。 |
| [^231] | [Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2311.12351) | 本论文对基于Transformer的大型语言模型架构的最新进展进行了全面调查，旨在增强其处理长上下文能力，从预训练到推断过程中进行了分类和分析。 |
| [^232] | [Designing monitoring strategies for deployed machine learning algorithms: navigating performativity through a causal lens](https://arxiv.org/abs/2311.11463) | 监控部署的机器学习算法的性能是重要的，该研究探讨了通过因果镜头导航解决有效性问题的方法。 |
| [^233] | [Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based Inference](https://arxiv.org/abs/2311.10671) | 提出了多模态神经后验估计 (MultiNPE) 方法，利用深度融合学习整合不同来源的异构数据，在模拟推理中提高了对复杂数学模型参数的准确推断能力。 |
| [^234] | [Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification](https://arxiv.org/abs/2311.09114) | 通过实时验证和纠正的策略，文章提出了一种名为Ever的方法，用于减轻大型语言模型生成中的虚构问题。 |
| [^235] | [Towards A Unified View of Answer Calibration for Multi-Step Reasoning](https://arxiv.org/abs/2311.09101) | 本文总结了最近答案校准技术的分类法，从统一视角对步级和路径级答案校准进行了彻底评估，结果显示整合两种策略的优势倾向于产生最佳结果。 |
| [^236] | [Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?](https://arxiv.org/abs/2311.07564) | 本文研究了作者归属模型在演讲文本中区分发言人的能力，并提出了以会话演讲文本为重点的发言人归属基准。 |
| [^237] | [Resource-Aware Hierarchical Federated Learning for Video Caching in Wireless Networks](https://arxiv.org/abs/2311.06918) | 本文提出了一种资源感知分层联邦学习方法(RawHFL)，用于无线网络视频缓存，通过预测用户未来的内容请求，以改善回传流量拥塞问题。该方法考虑了部分客户端参与，通过优化客户端的选择、本地训练轮次和CPU频率，以最小化一个加权效用函数，实现了RawHFL的收敛。 |
| [^238] | [The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models](https://arxiv.org/abs/2311.05928) | 本研究揭示了Transformer解码器中的各向异性呈钟状曲线，最高各向异性浓度在中间层，与编码器中更均匀分布的各向异性不同，并发现嵌入的内在维度在训练初期增加，随后在训练末期出现压缩，表明更紧凑的表示形式。 |
| [^239] | [Learning to Learn for Few-shot Continual Active Learning](https://arxiv.org/abs/2311.03732) | 提出了一种简单而高效的方法，Meta-Continual Active Learning，通过元学习和经验重播解决少样本持续主动学习中的任务混淆和灾难性遗忘，进一步结合文本增强来确保泛化。 |
| [^240] | [Explainable Modeling for Wind Power Forecasting: A Glass-Box Approach with High Accuracy](https://arxiv.org/abs/2310.18629) | 提出了一种结合高准确性与透明性的玻璃箱方法，通过构建形状函数总结特征效应，有效映射风力输出与输入特征之间的非线性关系，并丰富了预测模型，以捕获输入特征之间的相互依赖性和协同作用。 |
| [^241] | [Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model](https://arxiv.org/abs/2310.17653) | 通过研究预训练模型之间的知识转移，并尝试在不降低性能的情况下实现“互补”知识的传递，本研究探讨了如何提高模型之间的通用知识转移。 |
| [^242] | [Pre-Training LiDAR-Based 3D Object Detectors Through Colorization](https://arxiv.org/abs/2310.14592) | 本研究通过引入基于地面点颜色化（GPC）的创新预训练方法，教导模型着色LiDAR点云，提供宝贵的语义线索，显著改善了3D物体检测的效果。 |
| [^243] | [On Bilingual Lexicon Induction with Large Language Models](https://arxiv.org/abs/2310.13995) | 本文研究了利用大型语言模型进行双语词汇识别的潜力，通过研究零次提示和少量上下文提示等方法，探讨了这种方法如何与当前BLI方法相比，并如何进行补充。 |
| [^244] | [Federated Learning with Convex Global and Local Constraints](https://arxiv.org/abs/2310.10117) | 该论文提出了一种针对具有约束的机器学习问题的新联邦学习算法，建立了基于凸目标和凸约束的最坏情况复杂度，并通过数值实验证明了算法的有效性 |
| [^245] | [Ask Again, Then Fail: Large Language Models' Vacillations in Judgement](https://arxiv.org/abs/2310.02174) | 目前的语言模型在面对后续问题时常常摇摆不定，研究者提出了一个后续问题机制和两个度量标准来量化这种不一致性，并开发出Unwavering-FQ框架来教导模型保持最初的正确判断，实验证明其有效性。 |
| [^246] | [Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View](https://arxiv.org/abs/2310.02124) | 通过实践实验和理论洞察，探究当代NLP系统之间的协作机制，发现某些协作策略优于先前的方法，并且优化了效率。 |
| [^247] | [Improving Automatic Parallel Training via Balanced Memory Workload Optimization](https://arxiv.org/abs/2307.02031) | 本文提出了Galvatron-BMW，一个新的系统框架，通过平衡内存工作负载优化，集成多个并行维度并自动识别最有效的混合并行策略，通过决策树方法和动态规划搜索算法来有效地处理复杂的训练挑战。 |
| [^248] | [Fixing confirmation bias in feature attribution methods via semantic match](https://arxiv.org/abs/2307.00897) | 提出了通过语义匹配修复特征归因方法中的确认偏见问题，引入了人类概念与（亚符号）解释之间的概念框架，并提出了一种结构化方法来评估语义匹配。 |
| [^249] | [DOS: Diverse Outlier Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2306.02031) | 在外域检测性能中，多样性对于异常样本的采样至关重要，因此提出了一种名为DOS的多样化异常样本采样策略。 |
| [^250] | [CaloClouds: Fast Geometry-Independent Highly-Granular Calorimeter Simulation](https://arxiv.org/abs/2305.04847) | 通过生成模型和点云技术，本研究实现了在高粒度探测器中准确快速地模拟粒子簇，为将机器学习应用于粒子物理提供了重要突破。 |
| [^251] | [Learning Hidden Markov Models Using Conditional Samples](https://arxiv.org/abs/2302.14753) | 本文提出了一种使用交互方式访问隐马尔可夫模型的条件分布样本的学习方法，实现了对HMM的高效学习算法，从而绕过了其密码学困难性。 |
| [^252] | [Efficient Ensemble for Multimodal Punctuation Restoration using Time-Delay Neural Network](https://arxiv.org/abs/2302.13376) | 使用时间延迟神经网络实现了多模态标点修复的高效集成方法，相比当前最佳模型提高了1.0个F1分数，并且使用了更少的推理网络参数。 |
| [^253] | [Fourier series weight in quantum machine learning](https://arxiv.org/abs/2302.00105) | 本研究确认了傅里叶级数对量子机器学习模型的影响，并提出了基于哈密顿编码的量子机器学习模型以及傅里叶系数的确定方法。 |
| [^254] | [Don't Play Favorites: Minority Guidance for Diffusion Models](https://arxiv.org/abs/2301.12334) | 本研究提出了一个可以使扩散模型生成过程专注于少数样本的新颖框架。 |
| [^255] | [CEDAS: A Compressed Decentralized Stochastic Gradient Method with Improved Convergence](https://arxiv.org/abs/2301.05872) | CEDAS提出了一种压缩分布式随机梯度方法，在无偏压缩运算符下具有与集中式随机梯度下降相当的收敛速度，实现了最短的瞬态时间，对光滑强凸和非凸目标函数都适用。 |
| [^256] | [Non-invasive Liver Fibrosis Screening on CT Images using Radiomics](https://arxiv.org/abs/2211.14396) | 开发并评估了一种在CT肝脏上检测肝纤维化的放射组学机器学习模型，结果发现非增强CT在平均AUC值上优于增强CT。 |
| [^257] | [Spherical convolutional neural networks can improve brain microstructure estimation from diffusion MRI data](https://arxiv.org/abs/2211.09887) | 球面卷积神经网络在估计脑微结构参数方面具有比常规技术更高的准确性和更少的旋转方差。 |
| [^258] | [Multimodal Generative Models for Bankruptcy Prediction Using Textual Data](https://arxiv.org/abs/2211.08405) | 该研究介绍了一种条件多模态判别（CMMD）模型，通过学习多模态表示来预测破产风险，弥补了传统破产模型中缺少MDA文本数据的限制。 |
| [^259] | [Localized Randomized Smoothing for Collective Robustness Certification](https://arxiv.org/abs/2210.16140) | 提出了面向所有类型模型的集体鲁棒性证书，适用于软局部模型；证书基于新颖的局部随机平滑方法。 |
| [^260] | [DynaConF: Dynamic Forecasting of Non-Stationary Time Series](https://arxiv.org/abs/2209.08411) | 本研究提出了一种新方法，通过将平稳条件分布建模与非平稳动态建模解耦，有效地建模时间上的非平稳条件分布，能更好地适应非平稳时间序列。 |
| [^261] | [Model-Based Reinforcement Learning for Offline Zero-Sum Markov Games](https://arxiv.org/abs/2206.04044) | 本文提出了一种基于模型的悲观算法 VI-LCB-Game，在离线数据中找到了两人零和马尔可夫博弈的纳什均衡，加强了先前研究。 |
| [^262] | [Complex behavior from intrinsic motivation to occupy action-state path space](https://arxiv.org/abs/2205.10316) | 行为的目标是最大化未来行动和状态路径的占用，根据最大占用原则，奖励是占用路径空间的手段，而不是目标本身，并提供了与最优策略和状态值函数相关的解析表达式，证明了值迭代算法的收敛性 |
| [^263] | [Detecting data-driven robust statistical arbitrage strategies with deep neural networks](https://arxiv.org/abs/2203.03179) | 这项研究提出了一种基于深度神经网络的方法，能够识别金融市场中的稳健统计套利策略，无需依赖资产协整对的识别，可在高维金融市场中实现盈利交易。 |
| [^264] | [Learning Dynamic Mechanisms in Unknown Environments: A Reinforcement Learning Approach](https://arxiv.org/abs/2202.12797) | 通过将无奖励在线强化学习引入到在线机制设计问题中，我们提出了能够在未知环境中学习动态VCG机制且具有上界为$\tilde{\mathcal{O}}(T^{2/3})$的遗憾保证的新颖学习算法。 |
| [^265] | [Graph Neural Networks for Graphs with Heterophily: A Survey](https://arxiv.org/abs/2202.07082) | 该论文提出了对具有异质性的图进行图神经网络研究的系统回顾，并提出了系统性分类法以指导现有异质性GNN模型。 |
| [^266] | [The Klarna Product Page Dataset: Web Element Nomination with Graph Neural Networks and Large Language Models](https://arxiv.org/abs/2111.02168) | Klarna产品页面数据集是一个全面多样的网页集合，为解决网络自动化算法设计中数据集稀缺性问题提供了新的资源。 |
| [^267] | [The Role of Contextual Information in Best Arm Identification](https://arxiv.org/abs/2106.14077) | 通过在固定置信度下利用上下文信息，在识别最佳臂时提出了一种上下文感知的“跟踪停止”策略，实现了比之前方法更高效的效果。 |
| [^268] | [Max-Linear Regression by Convex Programming](https://arxiv.org/abs/2103.07020) | 本文提出并分析了一种基于锚定回归（AR）的可扩展凸规划方案，用于解决最大线性回归问题。 |
| [^269] | [Mixed Policy Gradient: off-policy reinforcement learning driven jointly by data and model](https://arxiv.org/abs/2102.11513) | 该论文提出了混合策略梯度（MPG）算法，通过融合数据驱动和模型驱动的策略梯度，加速强化学习的收敛速度而不降低性能。 |
| [^270] | [The MSR-Video to Text Dataset with Clean Annotations](https://arxiv.org/abs/2102.06448) | 清洗了MSR-VTT数据集中的标注，提高了视频字幕模型的性能。 |
| [^271] | [Scaling the Convex Barrier with Sparse Dual Algorithms](https://arxiv.org/abs/2101.05844) | 提出了两种新颖的对偶算法，通过操作小的对偶变量活跃集上的次梯度方法和利用Frank-Wolfe类型优化器的稀疏性质，来解决神经网络边界验证系统中常见的松弛性问题。 |
| [^272] | [On the representation and learning of monotone triangular transport maps](https://arxiv.org/abs/2009.10303) | 提出了通过光滑函数的可逆变换表示单调三角形映射的通用框架，使得相关的无穷维最小化问题具有全局最小值。 |
| [^273] | [On the generalization of Tanimoto-type kernels to real valued functions](https://arxiv.org/abs/2007.05943) | 本论文介绍了一种更一般的Tanimoto核公式，允许衡量任意实值函数的相似性，并提供了一种光滑逼近的方法。 |
| [^274] | [Sustainable Recreational Fishing Using a Novel Electrical Muscle Stimulation (EMS) Lure and Ensemble Network Algorithm to Maximize Catch and Release Survivability](https://arxiv.org/abs/2006.10125) | 该论文提出了一种新型电肌肉刺激钓鱼饵和集成网络算法，以解决垂钓损害和释放死亡率较高的问题，对于可持续开展休闲钓鱼具有重要意义。 |
| [^275] | [Sparse Orthogonal Variational Inference for Gaussian Processes](https://arxiv.org/abs/1910.10596) | 介绍了一种使用感应点进行稀疏正交变分推断的新方法，可以得到更具可扩展性的算法，实现了更紧的边缘似然下界和新的随机变分推断算法 |
| [^276] | [Online Causal Inference for Advertising in Real-Time Bidding Auctions](https://arxiv.org/abs/1908.08600) | 该论文提出了一种新的在线因果推断方法，利用一价和二价拍卖的经济结构，通过引入改进的汤普森抽样算法来有效识别实时竞价广告的效果，最小化实验成本，并获得了顺序最优的遗憾上界。 |
| [^277] | [HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy.](http://arxiv.org/abs/2401.15207) | HiFT是一种分层全参数微调策略，通过仅在每个训练步骤中更新参数的子集，可以显著减少GPU内存使用，并实现与参数高效微调和标准全参数微调相当的性能。 |
| [^278] | [Ricci flow-guided autoencoders in learning time-dependent dynamics.](http://arxiv.org/abs/2401.14591) | 利用Ricci流引导的自编码器方法能够学习非线性动力学，尤其是偏微分方程。该方法通过在训练中学习流形，并使用Ricci流使流形潜空间逐步适应动力学的变化，从而获得更好的表示能力。在实验中，我们展示了该方法在具有周期性和随机性的PDE上的应用，并评估了在分布内和外推场景中的误差。 |
| [^279] | [Communication-Efficient Federated Learning through Adaptive Weight Clustering and Server-Side Distillation.](http://arxiv.org/abs/2401.14211) | 本论文提出了一种名为FedCompress的新方法，通过动态权重聚类和服务器端知识蒸馏的结合，实现了高效通信的联邦学习。该方法在降低通信成本的同时，能够学习到高度可泛化的模型。 |
| [^280] | [Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations.](http://arxiv.org/abs/2401.14142) | 基于能量的概念瓶颈模型统一了预测、概念干预和条件解释的功能，解决了现有方法在高阶非线性相互作用和复杂条件依赖关系上的限制。 |
| [^281] | [Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation.](http://arxiv.org/abs/2401.11648) | 通过医学代码中心的多模态对比EHR建模预测下次就诊诊断，并通过分层正则化提高性能。 |
| [^282] | [QuasiNet: a neural network with trainable product layers.](http://arxiv.org/abs/2401.06137) | QuasiNet是一种新的神经网络模型，通过可训练的乘积层解决了小规模隐藏神经元下传统神经网络在难问题上的有限收敛问题，具有更高的成功率。 |
| [^283] | [A Closer Look at AUROC and AUPRC under Class Imbalance.](http://arxiv.org/abs/2401.06091) | 通过数学分析，研究发现AUROC和AUPRC在类别不平衡情况下可以以概率术语简洁地相关联。相比于人们普遍认为的AUPRC优越性，结果表明AUPRC并不如人们预期的有优势，并且可能是一种有害的指标。研究还通过分析大量文献验证了这一结论。 |
| [^284] | [Quantifying Marketing Performance at Channel-Partner Level by Using Marketing Mix Modeling (MMM) and Shapley Value Regression.](http://arxiv.org/abs/2401.05653) | 本文研究了使用Shapley值回归对渠道合作伙伴层面的营销绩效进行量化，并通过与营销组合建模进行比较，证明了Shapley值回归的实用性。同时提出了一种简单的方法来计算调整系数。 |
| [^285] | [Diffusion Model with Perceptual Loss.](http://arxiv.org/abs/2401.00110) | 本研究介绍了一种使用感知损失的扩散模型，通过无分类器指导实现了生成更真实样本的目的。 |
| [^286] | [M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy.](http://arxiv.org/abs/2312.15927) | 该论文提出了一种名为M3D的新型基于分布匹配的数据集压缩方法，通过最小化最大均值差异来提高压缩效率，克服了优化过程在实际和较大数据集上的应用难题。 |
| [^287] | [Privacy-Preserving Neural Graph Databases.](http://arxiv.org/abs/2312.15591) | 隐私保护的神经图数据库结合了图数据库和神经网络的优势，能够高效存储、检索和分析图结构数据。然而，这种能力也带来了潜在的隐私风险。 |
| [^288] | [TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training.](http://arxiv.org/abs/2312.08846) | TiMix是一种将文本感知的图像混合技术用于视觉语言预训练的方法，通过集成混合数据增强技术，并从互信息的角度进行理论分析，提高了数据效率并取得了可比较的性能。 |
| [^289] | [Convergence Analysis of Fractional Gradient Descent.](http://arxiv.org/abs/2311.18426) | 本论文通过分析不同环境下的分数梯度下降方法，建立了分数导数与整数导数之间的新界限，并证明了在平滑且凸、平滑且强凸以及平滑且非凸环境下的收敛性，为分数梯度下降的收敛性分析提供了新的理论支持。 |
| [^290] | [Linear Log-Normal Attention with Unbiased Concentration.](http://arxiv.org/abs/2311.13541) | 本论文研究了自注意机制，并分析了注意力矩阵的分布和集中能力。通过引入线性对数正态注意力来模拟原始自注意力的分布和集中行为，提高了Transformer模型的可扩展性。 |
| [^291] | [A Study of Continual Learning Under Language Shift.](http://arxiv.org/abs/2311.01200) | 本文研究了持续学习在语言转换中的应用，发现在更新语言模型时，前向转移效果较好且与语言顺序无关，但后向转移效果可能取决于新语言的顺序和特征。 |
| [^292] | [Contrastive Difference Predictive Coding.](http://arxiv.org/abs/2310.20141) | 本文介绍了一种时间差异版本的对比预测编码，通过将不同时间序列数据的片段组合在一起，来减少学习预测未来事件所需的数据量。实验证明，与先前的方法相比，我们的方法在成功率上提高了2倍，并且对于随机环境有更好的适应能力。 |
| [^293] | [Flow-based Distributionally Robust Optimization.](http://arxiv.org/abs/2310.19253) | 这项研究提出了一种称为FlowDRO的计算高效框架，用于解决基于流的分布鲁棒优化问题，通过使用流模型和Wasserstein近端梯度流类型的算法，实现了对具有更大样本大小的问题的可扩展性和更好的泛化能力。 |
| [^294] | [Simple and Asymmetric Graph Contrastive Learning without Augmentations.](http://arxiv.org/abs/2310.18884) | 本文提出了一种无需增强的简单非对称图对比学习方法GraphACL，通过考虑邻居节点的非对称视图，该方法能够有效地在同类和异类图上进行对比学习，对于建模异类图非常重要。 |
| [^295] | [Supervised and Penalized Baseline Correction.](http://arxiv.org/abs/2310.18306) | 本研究改进了受罚基线校正方法，通过利用先验分析物浓度来改善光谱预测性能，并在两个近红外数据集上进行了评估。 |
| [^296] | [Heterogeneous Federated Learning with Group-Aware Prompt Tuning.](http://arxiv.org/abs/2310.18285) | 本文研究了在异构联邦学习中利用预训练的Transformer和高效的提示调整策略，通过学习共享和群体提示实现获取通用知识和个性化知识，以训练适应不同本地数据分布的全局模型。 |
| [^297] | [Over-the-air Federated Policy Gradient.](http://arxiv.org/abs/2310.16592) | 本文提出了一种过空中联合策略梯度算法，通过无线信道广播携带本地信息的模拟信号实现更新策略参数，研究了噪声和信道失真对算法收敛性的影响，并通过仿真结果验证了算法的有效性。 |
| [^298] | [Function Vectors in Large Language Models.](http://arxiv.org/abs/2310.15213) | 大型语言模型中存在一种简单的神经机制，将输入-输出函数表示为向量。这些函数向量在不同的上下文中具有鲁棒性，并且具有强大的因果效应。同时，它们还具有将语义向量进行组合的能力。 |
| [^299] | [Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain.](http://arxiv.org/abs/2310.14053) | 这篇论文提出了一种评估大型代码语言模型自一致性的方法，并指出目前的模型在自一致性方面存在问题。 |
| [^300] | [Optimal Transport for Measures with Noisy Tree Metric.](http://arxiv.org/abs/2310.13653) | 本文提出了一种针对树度量有噪声的优化传输方法，通过引入新的不确定性集合，解决了实际应用中树结构扰动的问题。 |
| [^301] | [Generative Flow Networks as Entropy-Regularized RL.](http://arxiv.org/abs/2310.12934) | 本研究将生成流网络的学习任务重新定义为具有特定奖励和正则化器结构的熵正则化强化学习问题，并证明熵正则化强化学习方法在生成流网络训练中具有实际效率和竞争力。 |
| [^302] | [Hierarchical Forecasting at Scale.](http://arxiv.org/abs/2310.12809) | 提出了一种大规模层次预测的方法，使用稀疏损失函数直接优化层次产品和/或时间结构，从而为数百万个时间序列提供一致的预测。在实验中，该方法在M5数据集上表现出10%的性能提升。 |
| [^303] | [A Quasi-Wasserstein Loss for Learning Graph Neural Networks.](http://arxiv.org/abs/2310.11762) | 这篇论文提出了一种新的准瓦狄斯坦损失函数，通过利用图上的最优传输来学习图神经网络，消除了现有损失函数在节点级别预测中可能存在的不一致性。 |
| [^304] | [User Inference Attacks on Large Language Models.](http://arxiv.org/abs/2310.09266) | 本论文研究了在大型语言模型上的用户推理攻击，发现LLMs对于各种微调数据集都很容易受到攻击，尤其是对于离群用户和贡献大量数据的用户。这对保护用户隐私具有重要意义。 |
| [^305] | [Neural Diffusion Models.](http://arxiv.org/abs/2310.08337) | 本文提出了神经扩散模型（NDMs），它是传统扩散模型的推广，可以定义和学习数据的时间依赖非线性变换。我们展示了如何在无需模拟的设置中使用变分界对NDMs进行优化，并通过在标准图像生成任务上的实验证明了可学习变换的NDMs的实用性。 |
| [^306] | [Suppressing Overestimation in Q-Learning through Adversarial Behaviors.](http://arxiv.org/abs/2310.06286) | 本文提出了一种新的Q学习算法，通过引入虚拟对抗性玩家，有效调节了标准Q学习中的过高估计偏差，提出的算法简单而有效，能够轻松应用于强化学习算法并提高性能。 |
| [^307] | [Let Models Speak Ciphers: Multiagent Debate through Embeddings.](http://arxiv.org/abs/2310.06272) | 本文引入了一种名为CIPHER的通信机制，通过去除LLMs中的标记采样步骤，让模型可以通过期望的原始Transformer输出嵌入来传达其信念，从而解决了在自然语言生成中可能存在的信息丢失风险，并提供了编码更广泛信息的优势。 |
| [^308] | [GeoLLM: Extracting Geospatial Knowledge from Large Language Models.](http://arxiv.org/abs/2310.06213) | GeoLLM是一种能够从大型语言模型中提取地理空间知识的新方法，结合来自OpenStreetMap的辅助地图数据，可以有效应用于测量人口密度和经济生计等任务。 |
| [^309] | [Label-free Node Classification on Graphs with Large Language Models (LLMS).](http://arxiv.org/abs/2310.04668) | 本文介绍了一种使用大型语言模型（LLMs）对图中节点进行无标签分类的方法，即LLM-GNN。它利用LLMs对一小部分节点进行注释，然后通过对LLMs的注释进行训练，使得GNN能够对其余大部分节点进行预测。这种方法充分发挥了GNNs和LLMs的优势，同时解决了它们在处理结构化数据方面的限制。 |
| [^310] | [Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference.](http://arxiv.org/abs/2310.04395) | 该论文提出了一种利用自一致性改进数据有效的摊余贝叶斯推理方法，通过反转贝叶斯定理并利用近似表示的联合模型估计边际似然，加速条件神经密度估计器的学习动力学。 |
| [^311] | [Exploiting Transformer Activation Sparsity with Dynamic Inference.](http://arxiv.org/abs/2310.04361) | 本文提出了一种名为DSTI的方法，通过强制激活稀疏性并将Transformer模型转换为稀疏的专家混合版本来极大地降低推理成本。此方法可以应用于任何Transformer模型，并对准确性影响微乎其微。 |
| [^312] | [Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation.](http://arxiv.org/abs/2310.03986) | 通过低秩适应和中间特征的调制，我们提出了针对预训练多模态网络的参数高效适应程序，以实现对缺失模态的鲁棒性，并在某些情况下胜过独立的专门网络。 |
| [^313] | [Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models.](http://arxiv.org/abs/2310.01107) | 本论文提出了一种名为 Ground-A-Video 的基于引导的视频到视频转换框架，用于多属性视频编辑。该方法在没有训练的情况下实现了输入视频的时间一致的多属性编辑，并且解决了其他方法存在的问题。 |
| [^314] | [Going Beyond Familiar Features for Deep Anomaly Detection.](http://arxiv.org/abs/2310.00797) | 该论文提出了一种超越熟悉特征的深度异常检测方法，通过利用可解释性在输入空间中捕捉新颖特征来避免假阴性。该方法在广泛的异常基准测试中取得了强大的性能，并且消除了昂贵的背景模型和密集匹配的需求。 |
| [^315] | [Pre-training with Synthetic Data Helps Offline Reinforcement Learning.](http://arxiv.org/abs/2310.00771) | 本文研究表明，在离线深度强化学习中，使用合成数据进行预训练可以提高性能，而不一定需要语言预训练。此外，使用一步马尔科夫链生成的数据进行预训练可进一步改善性能。在一个流行的离线DRL算法中，使用简单的预训练方案也能获得性能提升。 |
| [^316] | [Order-Preserving GFlowNets.](http://arxiv.org/abs/2310.00386) | 本研究提出了保序GFlowNets（OP-GFNs），通过学习奖励函数与候选者的排序相一致的概率进行采样，解决了使用预定义标量奖励的局限性，同时提供了证明训练过程稀疏奖励景观的理论支持。 |
| [^317] | [A Foundation Model for General Moving Object Segmentation in Medical Images.](http://arxiv.org/abs/2309.17264) | 本文提出了一种用于医学图像中移动目标分割的基础模型iMOS，通过对序列中只有少量图像进行注释，即可实现高精度的分割效果 |
| [^318] | [Transformer-VQ: Linear-Time Transformers via Vector Quantization.](http://arxiv.org/abs/2309.16354) | Transformer-VQ是一种基于向量量化实现线性时间的Transformer模型，能够高效计算自注意力，在大规模实验中表现出色。 |
| [^319] | [Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts.](http://arxiv.org/abs/2309.13850) | 该论文研究前K稀疏softmax门控混合专家在密度和参数估计方面的作用，通过定义新的损失函数，探讨了输入区域的不同行为。研究发现，在真实专家数量已知的情况下，密度和参数估计的收敛速度与样本量成正比，但当真实模式未知时 |
| [^320] | [Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis.](http://arxiv.org/abs/2309.12368) | 本研究探索了在复杂医疗决策中重新思考人工智能与人类合作的设计要求，以脓毒症诊断为例。研究发现，在人工智能系统中，支持临床专家在决策过程的中间阶段发挥作用（如生成假设或收集数据）是至关重要的，而不仅仅关注最终决策。 |
| [^321] | [Convolutional Deep Kernel Machines.](http://arxiv.org/abs/2309.09814) | 这项研究介绍了一种称为卷积深度核机器的新型核方法，该方法纯粹使用核而不使用特征，通过高效的跨域诱导点近似方案和多种模型变体的设计，达到了在MNIST、CIFAR-10和CIFAR-100上接近甚至超过其他方法的测试准确率。 |
| [^322] | [Contrastive Initial State Buffer for Reinforcement Learning.](http://arxiv.org/abs/2309.09752) | 本论文提出了对比初始状态缓冲区的概念，它通过选择过去的经验中的状态来初始化环境中的代理，以引导其进入更有信息量的状态。实验证明，该方法在两个复杂机器人任务上取得了更高的任务性能并加速了训练过程。 |
| [^323] | [RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud.](http://arxiv.org/abs/2309.09737) | RaTrack是一种针对雷达跟踪的创新解决方案，通过运动分割和聚类以及运动估计模块，实现了对移动物体的精确跟踪，优于最先进性能。 |
| [^324] | [Quantitative and Qualitative Evaluation of Reinforcement Learning Policies for Autonomous Vehicles.](http://arxiv.org/abs/2309.08254) | 本文使用强化学习算法（PPO）针对自主驾驶车辆的选择进行了优化，通过最小化时间和污染来缓解交通阻塞问题，经实证分析和定性评估证明了方法的有效性和实用性。 |
| [^325] | [CaloClouds II: Ultra-Fast Geometry-Independent Highly-Granular Calorimeter Simulation.](http://arxiv.org/abs/2309.05704) | CaloClouds II是一个超快速几何独立的高分辨率量能器模拟，通过连续时间得分建模，相比传统模拟更快且具有可比的保真度。 |
| [^326] | [A Gentle Introduction to Gradient-Based Optimization and Variational Inequalities for Machine Learning.](http://arxiv.org/abs/2309.04877) | 这篇论文介绍了渐变优化和变分不等式在机器学习中的应用，强调了从模式识别到决策和多智能体问题的转变，以及涉及均衡和博弈论的数学挑战，提供了一些算法的收敛性证明，但主要关注于提供动机和直观理解。 |
| [^327] | [Graph Neural Networks Use Graphs When They Shouldn't.](http://arxiv.org/abs/2309.04332) | 在图形预测问题中，GNNs倾向于过拟合图结构，即使在忽略图结构的情况下可以获得更好的解决方案。常规图对于这种过拟合更具鲁棒性。 |
| [^328] | [A hybrid quantum-classical fusion neural network to improve protein-ligand binding affinity predictions for drug discovery.](http://arxiv.org/abs/2309.03919) | 提出了一种用于药物发现的混合量子-经典融合神经网络模型，通过优化的量子架构将3D和空间图卷积神经网络相互整合，提高了结合亲和力预测的准确性。 |
| [^329] | [Mixed Variational Flows for Discrete Variables.](http://arxiv.org/abs/2308.15613) | 本文提出了一种混合方差流方法，用于近似离散分布，通过开发一个离散且保持度量的映射，而不需要连续嵌入。实验证明，与连续嵌入流相比，该方法产生更可靠的近似。 |
| [^330] | [A Benchmark Study on Calibration.](http://arxiv.org/abs/2308.11838) | 这项研究提出了一个模型校准的基准研究，利用神经架构搜索空间探索了模型校准属性。研究结果显示，模型校准可以在不同任务中泛化，并可以同时兼顾模型的准确性和校准性能。 |
| [^331] | [When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making.](http://arxiv.org/abs/2308.11721) | 这项研究分析了一种特定类型的人工和算法合作，对于多个噪音模型来说，将选择项目的子集大小$k$设置在$[2, n-1]$范围内能够最大化最终选择最佳项目的概率。 |
| [^332] | [Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning.](http://arxiv.org/abs/2308.11464) | 提出了一种基于内部跨层梯度的联邦学习方法，通过混合浅层和深层的梯度，增强了深层的相似性，从而扩展了在处理系统异质性方面的能力。 |
| [^333] | [ALI-DPFL: Differentially Private Federated Learning with Adaptive Local Iterations.](http://arxiv.org/abs/2308.10457) | ALI-DPFL是一种进行差分隐私联邦学习的算法，通过自适应本地迭代来优化性能，并在实验中展示了显著的改进。 |
| [^334] | [Rigid Transformations for Stabilized Lower Dimensional Space to Support Subsurface Uncertainty Quantification and Interpretation.](http://arxiv.org/abs/2308.08079) | 该论文介绍了一种用于地下数据集的稳定降维方法，通过刚性变换实现了欧几里德不变表示，能够量化地下数据的不确定性并且适应外样本点的扩展。 |
| [^335] | [Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability.](http://arxiv.org/abs/2308.07728) | 本文提出了领域感知微调（DAFT）方法，通过批归一化转换和线性探测与微调的集成，有效减轻微调过程中的特征畸变问题。 |
| [^336] | [Large Language Models for Telecom: Forthcoming Impact on the Industry.](http://arxiv.org/abs/2308.06013) | 大型语言模型在电信行业将产生重要的影响。它们可以提高运营效率，简化任务，并需要解决使用中的挑战。 |
| [^337] | [Language models as master equation solvers.](http://arxiv.org/abs/2308.02514) | 本研究将语言模型用作求解主方程的机器学习方法，通过设计提示网络和使用强化学习算法训练，实现了对多模组和高维系统的高精度求解。 |
| [^338] | [Explainable Equivariant Neural Networks for Particle Physics: PELICAN.](http://arxiv.org/abs/2307.16506) | PELICAN是一种可解释的等变神经网络，应用于粒子物理问题中。相比于其他方法，PELICAN的优势在于它采用了基于对称群的架构，具有降低复杂性、增加可解释性和提高性能的特点。它在标记和重构动量增强的顶夸克，并在密集环境中特别识别和测量W玻色子，以及识别不同类型的喷注等任务方面展示了出色的表现。 |
| [^339] | [A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis.](http://arxiv.org/abs/2307.12856) | 这篇论文介绍了一种名为WebAgent的LLM驱动代理，通过自我经验学习，在真实网站上完成任务。该方法通过规划、总结和生成代码来提高在真实网站上的成功率。 |
| [^340] | [Graph Neural Network-enabled Terahertz-based Flow-guided Nanoscale Localization.](http://arxiv.org/abs/2307.05551) | 本研究提出了一种基于图神经网络的太赫兹流导向纳米尺度定位方法，可以提高定位精度和覆盖范围，解决了现有方法的定位精度低和无法全局定位的问题。 |
| [^341] | [On the power of graph neural networks and the role of the activation function.](http://arxiv.org/abs/2307.04661) | 本文通过对称多项式代数的工具证明了对于具有分段多项式激活函数且体系结构大小不变的GNNs，存在一对非同构根树在任意迭代次数内无法被区分，与此同时，具有不同大小的GNNs只需两次迭代即可区分。此外，我们还证明了如果允许非分段多项式激活函数，则在两次迭代内，单个神经元感知器可以区分任意一对非同构树的根节点。 |
| [^342] | [Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed Tomography Images.](http://arxiv.org/abs/2307.03137) | 本文介绍了一种新的拓扑感知损失函数，通过持久同调来惩罚计算机断层扫描图像中主动脉和大血管分割结果与真实值之间的拓扑差异。这种方法能够改善分割任务的性能，尤其是针对具有固有几何特征的对象。 |
| [^343] | [Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion.](http://arxiv.org/abs/2307.02496) | 本研究利用可逆神经网络和误差扩散方法，通过测量气泡引起的磁场波动，重建电解过程中的气泡分布和电导率图，并实现了比传统方法更优异的性能。 |
| [^344] | [Inertial Navigation Meets Deep Learning: A Survey of Current Trends and Future Directions.](http://arxiv.org/abs/2307.00014) | 本文综述了惯性导航领域中当前的深度学习方法，包括对不同车辆操作领域的研究、滤波参数学习的改进以及惯性传感器的校准和去噪方法。翻译过的论文标题: 惯性导航与深度学习：当前趋势与未来方向的综述 |
| [^345] | [Solving Kernel Ridge Regression with Gradient-Based Optimization Methods.](http://arxiv.org/abs/2306.16838) | 本研究提出了一种新的方法来解决核岭回归问题，通过等价的目标函数形式和基于梯度的优化方法，我们不仅可以使用其他惩罚方法，还能够从梯度下降的角度研究核岭回归。通过提前停止的正则化，我们推导出了一个闭合解，即核梯度流（KGF），并证明了KGF和KRR之间的差异。我们还将KRR泛化，使用$\ell_1$和$\ell_\infty$惩罚方法，并发现使用这些方法得到的解与前向分步回归和符号梯度下降结合提前停止得到的解非常相似。因此，我们减少了计算复杂度重的近端梯度下降算法的需求。 |
| [^346] | [Momentum Benefits Non-IID Federated Learning Simply and Provably.](http://arxiv.org/abs/2306.16504) | 本论文研究了在非独立同分布联邦学习中利用动量来提升FedAvg和SCAFFOLD算法的性能，证明了引入动量可以使FedAvg在不依赖于数据异质性的假设下收敛。 |
| [^347] | [Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork.](http://arxiv.org/abs/2306.10698) | 人工智能领域，一个新算法利用基于任务条件化超网络的检索网络，根据任务调整网络参数，以解决深度强化学习中选择最相关的过去经验并将其融合到既有决策网络中的问题。 |
| [^348] | [Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization.](http://arxiv.org/abs/2306.09222) | 我们通过分布健壮优化和重要性加权的梯度下降技术提升了深度神经网络的性能，并在各种任务上取得了优越的结果。 |
| [^349] | [BackpropTools: A Fast, Portable Deep Reinforcement Learning Library for Continuous Control.](http://arxiv.org/abs/2306.03530) | BackpropTools是一款快速、可移植的连续控制深度强化学习库，它通过模板元编程提供紧密集成的可组合组件，并在异构平台集合上无缝使用，同时在连续控制问题的深度RL代理高效可扩展训练方面具有优势。由于其可移植性和实时保证，它成为了在嵌入式设备上部署学来的策略的有价值的工具。 |
| [^350] | [Stabilizing Contrastive RL: Techniques for Offline Goal Reaching.](http://arxiv.org/abs/2306.03346) | 本文提出了一种稳定的对比强化学习方法，通过浅而宽的结构，结合谨慎的权重初始化和数据增强等实验方法，在具有挑战性的仿真基准测试中显著提高了性能，并演示了对比方法可以解决现实世界的机器人任务。 |
| [^351] | [Broadcasting in random recursive dags.](http://arxiv.org/abs/2306.01727) | 该论文研究了一个均匀的$k$-dag广播模型，确定了与$p$和$k$有关的阈值，并讨论了大多数规则的误差率。 |
| [^352] | [Going Deeper with Spectral Embeddings.](http://arxiv.org/abs/2306.00742) | 本文提出两种新的谱嵌入方法，一种基于函数分析原理和核方法，另一种基于深度网络优化损失，提供理论保证和实际有效的算法，并提供新的采样算法。 |
| [^353] | [Feature-aligned N-BEATS with Sinkhorn divergence.](http://arxiv.org/abs/2305.15196) | 这是一个基于Sinkhorn距离的特征对其N-BEATS模型，它通过对齐堆栈中的边际特征概率测度来进行领域广义的时间序列预测，同时保留了N-BEATS的可解释性和预测能力。 |
| [^354] | [Revisit and Outstrip Entity Alignment: A Perspective of Generative Models.](http://arxiv.org/abs/2305.14651) | 本文重新从生成模型的角度研究了基于嵌入的实体对齐（EEA）问题，引入基于生成对抗网络的EEA方法及提出的生成的EEA（GEEA）框架，通过互相变分自动编码器（M-VAE）实现实体从一个KG转换到另一个KG，并且从随机噪声向量生成新的实体，具有较好的效果。 |
| [^355] | [Multimodal Web Navigation with Instruction-Finetuned Foundation Models.](http://arxiv.org/abs/2305.11854) | 本文研究使用视觉语言基础模型进行数据驱动离线训练的 Web 代理，提出了一个指令跟随多模态代理WebGUM，将微调指令微调语言模型和视觉转换器，能够有效提高代理的基于视觉感知、HTML 理解和多步推理的能力。 |
| [^356] | [Smaller Language Models are Better Black-box Machine-Generated Text Detectors.](http://arxiv.org/abs/2305.09859) | 本文研究发现，小型语言模型更适用于作为通用文本检测器，可以更加精确地检测出机器生成的文本，而检测器和生成模型是否具有相同的架构或语料库并不会对检测性能产生显著影响。 |
| [^357] | [Can Large Language Models Transform Computational Social Science?.](http://arxiv.org/abs/2305.03514) | 本文研究了大型语言模型作为计算社会科学工具的潜力。虽然在分类任务上没有优势，但在自由形式编码任务上表现优异，今后可以作为零-shot检测工具进行使用， |
| [^358] | [Efficient Convex Algorithms for Universal Kernel Learning.](http://arxiv.org/abs/2304.07472) | 本文提出了一种基于SVD-QCQP原始对偶算法的高效内核学习实现，用于学习半分离核，大大降低了计算复杂度，并在几个基准数据集上展示了其准确性和速度。 |
| [^359] | [Homogenizing Non-IID datasets via In-Distribution Knowledge Distillation for Decentralized Learning.](http://arxiv.org/abs/2304.04326) | 本文提出了基于分布中知识蒸馏（IDKD）的方法来解决节点间同质化数据分布的问题，该方法使用公共数据集来从每个节点中提取知识，并通过生成的标签将其传递给其邻居，以实现相同的目标，同时保持隐私约束。在非i.i.d.数据集上，IDKD的性能显着优于现有方法，同时保持隐私和通信效率。 |
| [^360] | [Domain Generalization In Robust Invariant Representation.](http://arxiv.org/abs/2304.03431) | 本文研究了不变表示的泛化性能，证明具有不变表示的模型可以学习到具有鲁棒性的非结构化潜在表示，因此使不变性成为域泛化的一个关键方面。 |
| [^361] | [Distributed Graph Embedding with Information-Oriented Random Walks.](http://arxiv.org/abs/2303.15702) | 本文提出了一种名为DistGER的分布式图嵌入方法，基于信息导向随机游走策略，利用多种优化技术实现了高效的十亿级别图嵌入。 |
| [^362] | [Uni-RXN: A Unified Framework Bridging the Gap between Chemical Reaction Pretraining and Conditional Molecule Generation.](http://arxiv.org/abs/2303.06965) | 本文提出了Uni-RXN框架，在化学反应Pretraining和分子生成任务中都取得了最先进的结果。通过具备化学知识，克服了当前分子生成模型仅依赖少量反应模板的限制，生成质量高、可合成的药物类分子结构。 |
| [^363] | [Flooding with Absorption: An Efficient Protocol for Heterogeneous Bandits over Complex Networks.](http://arxiv.org/abs/2303.05445) | 该论文提出了一种名为带吸收的泛洪（FwA）的新协议，用于解决复杂网络上的异构赌博机问题。通过严格的遗憾分析，证明了该协议的有效性。 |
| [^364] | [Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays.](http://arxiv.org/abs/2302.13991) | 通过内容感知的风格不变模型，我们提出了一种解决深度学习医学图像分析中源领域不匹配挑战的方法。我们采用了风格随机化模块来提取既是风格不变又是内容偏好的领域不变特征，在胸部X射线疾病检测中取得了良好的性能。 |
| [^365] | [Online Instrumental Variable Regression: Regret Analysis and Bandit Feedback.](http://arxiv.org/abs/2302.09357) | 该论文研究了在线学习中内生性问题的解决方法，提出了使用Two-Stage Least Squares方法的在线变体O2SLS来处理内生性，取得了较好的识别率和预测遗憾率。 |
| [^366] | [Label-efficient Time Series Representation Learning: A Review.](http://arxiv.org/abs/2302.06433) | 这篇综述介绍了针对时间序列数据中标记数据稀缺性问题的现有方法，并提供了一个新颖的分类系统来归纳这些方法。该综述总结了每种方法的最新进展并提出了未来的研究方向。 |
| [^367] | [The Sample Complexity of Approximate Rejection Sampling with Applications to Smoothed Online Learning.](http://arxiv.org/abs/2302.04658) | 本研究展示了在有界f-散度约束下，近似拒绝采样的样本复杂度可以通过Θ(~(D/f'(n)))函数来表示，并且应用于平滑在线学习中的相关算法的性能依然成立。 |
| [^368] | [The Certification Paradox: Certifications Admit Better Attacks.](http://arxiv.org/abs/2302.04379) | 本文指出了一个"认证悖论"，认证虽然可以展示模型的稳健性，但额外揭示了有关认证模型的信息也成为新的攻击面，导致更好的攻击效果。 |
| [^369] | [Riemannian Flow Matching on General Geometries.](http://arxiv.org/abs/2302.03660) | 本文提出了一种名为黎曼流匹配的方法，可以在一般几何上训练连续标准化流，并在高维度数据上具有优势。 |
| [^370] | [Quality at the Tail.](http://arxiv.org/abs/2212.13925) | 本研究发现深度学习推理质量存在波动，引入了“尾部质量”的概念来描述这一现象。 |
| [^371] | [TREE-G: Decision Trees Contesting Graph Neural Networks.](http://arxiv.org/abs/2207.02760) | TREE-G引入了一种专门针对图数据的修改决策树方法，通过新颖的分裂函数和指针机制，使得决策树能够更好地应用于带有拓扑信息的图结构化数据。 |
| [^372] | [On the Identifiability of Nonlinear ICA: Sparsity and Beyond.](http://arxiv.org/abs/2206.07751) | 本文提出一个新的方法，考虑混合过程的假设，即结构稀疏性，来实现非线性ICA的可识别性，无需辅助变量。 |
| [^373] | [Optimization with access to auxiliary information.](http://arxiv.org/abs/2206.00395) | 本文研究了在计算目标函数梯度很昂贵或有限的情况下，给定一些辅助函数的情况下，如何最小化目标函数。作者提出了两种通用的新算法，并证明了这个框架可以受益于目标和辅助信息之间的Hessian相似性假设。 |
| [^374] | [Broad Recommender System: An Efficient Nonlinear Collaborative Filtering Approach.](http://arxiv.org/abs/2204.11602) | 本文提出了一种新的宽泛推荐系统(BroadCF)，使用宽泛学习系统(BLS)作为映射函数来学习用户和项目之间的复杂非线性关系，同时通过用户-项评级协同向量预处理程序将原始数据转换为更适合BLS学习的格式。BroadCF的实验结果表明，在用户推荐准确性和效率方面都优于几种最先进的CF方法。 |
| [^375] | [Kernel Two-Sample Tests for Manifold Data.](http://arxiv.org/abs/2105.03425) | 本文研究了与最大均值差异（MMD）相关的基于核的双样本检验统计量在测量流形数据时的应用。文章展示了检验水平和功率与核带宽、样本数量和流形内在维度之间的关系，并在特定条件下建立了测试功率下界。 |

# 详细

[^1]: 交通预测中的延迟效应揭示：基于时空延迟微分方程的视角

    Unveiling Delay Effects in Traffic Forecasting: A Perspective from Spatial-Temporal Delay Differential Equations

    [https://rss.arxiv.org/abs/2402.01231](https://rss.arxiv.org/abs/2402.01231)

    本研究提出了一个基于神经网络的空间-时间延迟微分方程模型，用于捕捉交通预测中的延迟效应。

    

    交通流量预测是交通规划和管理中的一个基本研究问题，也是空间-时间预测的一个典型例子。最近，图神经网络（GNNs）和循环神经网络（RNNs）在捕捉交通流量预测的空间-时间相关性方面取得了巨大成功。然而，目前仍有两个不可忽视的问题没有得到很好地解决：1）GNNs中的消息传递是即时的，而在现实中，邻居节点之间的空间消息交互可能存在延迟。交通流量在一个节点上的变化需要几分钟的时间延迟，才能影响其相连的邻居节点。2）交通状况不断变化。交通流量预测的预测频率可能根据具体场景要求而变化。目前大多数现有的离散模型需要针对每个预测时间段进行重新训练，限制了它们的适用性。为了解决以上问题，我们提出了一个基于神经网络的空间-时间延迟微分方程模型，来捕捉交通预测中的延迟效应。

    Traffic flow forecasting is a fundamental research issue for transportation planning and management, which serves as a canonical and typical example of spatial-temporal predictions. In recent years, Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) have achieved great success in capturing spatial-temporal correlations for traffic flow forecasting. Yet, two non-ignorable issues haven't been well solved: 1) The message passing in GNNs is immediate, while in reality the spatial message interactions among neighboring nodes can be delayed. The change of traffic flow at one node will take several minutes, i.e., time delay, to influence its connected neighbors. 2) Traffic conditions undergo continuous changes. The prediction frequency for traffic flow forecasting may vary based on specific scenario requirements. Most existing discretized models require retraining for each prediction horizon, restricting their applicability. To tackle the above issues, we propose a neural Spati
    
[^2]: 基于张量投影的标签学习方法

    Label Learning Method Based on Tensor Projection

    [https://arxiv.org/abs/2402.16544](https://arxiv.org/abs/2402.16544)

    我们提出了一种基于张量投影的标签学习方法，通过将锚图投影到标签空间，并将矩阵投影扩展到张量投影，以充分利用多视图数据的空间结构信息。

    

    基于锚图的多视图聚类方法因其高效性和有效性而受到广泛关注。为了避免后处理，大多数现有的基于锚图的方法学习具有连接组件的二部图。然而，这种方法对参数有很高的要求，在某些情况下可能无法获得具有清晰连接组件的二部图。为了解决这个问题，我们提出了一种基于张量投影的标签学习方法（LLMTP）。具体地，我们通过正交投影矩阵将锚图投影到标签空间，直接获得聚类标签。考虑到在不同视图中分别投影时，多视图数据的空间结构信息可能会在一定程度上被忽略，我们将矩阵投影变换扩展到张量投影，以便充分利用视图之间的空间结构信息。

    arXiv:2402.16544v1 Announce Type: new  Abstract: Multi-view clustering method based on anchor graph has been widely concerned due to its high efficiency and effectiveness. In order to avoid post-processing, most of the existing anchor graph-based methods learn bipartite graphs with connected components. However, such methods have high requirements on parameters, and in some cases it may not be possible to obtain bipartite graphs with clear connected components. To end this, we propose a label learning method based on tensor projection (LLMTP). Specifically, we project anchor graph into the label space through an orthogonal projection matrix to obtain cluster labels directly. Considering that the spatial structure information of multi-view data may be ignored to a certain extent when projected in different views separately, we extend the matrix projection transformation to tensor projection, so that the spatial structure information between views can be fully utilized. In addition, we i
    
[^3]: 基于模型的深度强化学习用于加速流体模拟中的学习

    Model-based deep reinforcement learning for accelerated learning from flow simulations

    [https://arxiv.org/abs/2402.16543](https://arxiv.org/abs/2402.16543)

    本文展示了基于模型的强化学习在流体控制应用中的优势，通过优化策略来减少流体模拟的计算成本和运行时间。

    

    近年来，深度强化学习已经成为解决闭环流控问题的技术。在强化学习中使用基于模拟的环境可以事先端到端地优化控制系统，为安全关键的控制应用提供虚拟试验平台，并且可以深入理解控制机制。本文展示了基于模型的强化学习在流控应用中的优势，通过在从流模拟中采样的轨迹和从环境模型集合中采样的轨迹之间交替优化策略。模型为基础学习降低了整体

    arXiv:2402.16543v1 Announce Type: cross  Abstract: In recent years, deep reinforcement learning has emerged as a technique to solve closed-loop flow control problems. Employing simulation-based environments in reinforcement learning enables a priori end-to-end optimization of the control system, provides a virtual testbed for safety-critical control applications, and allows to gain a deep understanding of the control mechanisms. While reinforcement learning has been applied successfully in a number of rather simple flow control benchmarks, a major bottleneck toward real-world applications is the high computational cost and turnaround time of flow simulations. In this contribution, we demonstrate the benefits of model-based reinforcement learning for flow control applications. Specifically, we optimize the policy by alternating between trajectories sampled from flow simulations and trajectories sampled from an ensemble of environment models. The model-based learning reduces the overall 
    
[^4]: 将大型语言模型与图形会话推荐相结合

    Integrating Large Language Models with Graphical Session-Based Recommendation

    [https://arxiv.org/abs/2402.16539](https://arxiv.org/abs/2402.16539)

    本文提出了一种名为LLMGR的框架，将大型语言模型与图形会话推荐相结合，有效地弥合了推荐系统中大型语言模型在会话推荐领域的适应性差距

    

    随着大型语言模型（LLMs）的快速发展，出现了各种探索，利用LLMs在推荐系统上的上下文理解能力。虽然开创性的策略主要是将传统推荐任务转变为自然语言生成挑战，但在会话推荐（SBR）领域的探索相对较少，因为其具体性。SBR主要由图神经网络主导，由于其捕获相邻行为之间的内在和显性关系的能力，取得了许多成功结果。图的结构性质与自然语言的本质形成对比，为LLMs提出了重大的适应性差距。本文介绍了将大型语言模型与图形会话推荐相结合的框架LLMGR，这是一个有效的框架，通过和谐地弥合上述差距

    arXiv:2402.16539v1 Announce Type: cross  Abstract: With the rapid development of Large Language Models (LLMs), various explorations have arisen to utilize LLMs capability of context understanding on recommender systems. While pioneering strategies have primarily transformed traditional recommendation tasks into challenges of natural language generation, there has been a relative scarcity of exploration in the domain of session-based recommendation (SBR) due to its specificity. SBR has been primarily dominated by Graph Neural Networks, which have achieved many successful outcomes due to their ability to capture both the implicit and explicit relationships between adjacent behaviors. The structural nature of graphs contrasts with the essence of natural language, posing a significant adaptation gap for LLMs. In this paper, we introduce large language models with graphical Session-Based recommendation, named LLMGR, an effective framework that bridges the aforementioned gap by harmoniously 
    
[^5]: 使用物理信息机器学习发现不连续Galerkin逼近守恒定律的人工粘性模型

    Discovering Artificial Viscosity Models for Discontinuous Galerkin Approximation of Conservation Laws using Physics-Informed Machine Learning

    [https://arxiv.org/abs/2402.16517](https://arxiv.org/abs/2402.16517)

    使用物理信息机器学习算法自动发现人工粘性模型，无需数据集训练，成功应用于高阶守恒定律求解器中。

    

    基于有限元的高阶守恒定律求解器提供了较高的准确性，但在不连续处面临Gibbs现象挑战。人工粘性是基于物理见解的解决方案。本文提出了一种物理信息机器学习算法，用于自动发现非监督范式下的人工粘性模型。该算法受强化学习启发，通过最小化定义为相对参考解的差异的损失来训练神经网络，单元格逐个单元格操作（粘性模型）。这使得能够进行无数据集的训练过程。我们证明了该算法通过将其整合到最先进的Runge-Kutta不连续Galerkin求解器中是有效的。我们在标量和矢量问题上展示了几个数值测试，如Burgers'和Euler的方程在一维和二维的情况。

    arXiv:2402.16517v1 Announce Type: cross  Abstract: Finite element-based high-order solvers of conservation laws offer large accuracy but face challenges near discontinuities due to the Gibbs phenomenon. Artificial viscosity is a popular and effective solution to this problem based on physical insight. In this work, we present a physics-informed machine learning algorithm to automate the discovery of artificial viscosity models in a non-supervised paradigm. The algorithm is inspired by reinforcement learning and trains a neural network acting cell-by-cell (the viscosity model) by minimizing a loss defined as the difference with respect to a reference solution thanks to automatic differentiation. This enables a dataset-free training procedure. We prove that the algorithm is effective by integrating it into a state-of-the-art Runge-Kutta discontinuous Galerkin solver. We showcase several numerical tests on scalar and vectorial problems, such as Burgers' and Euler's equations in one and tw
    
[^6]: 针对时间序列预测的生成式预训练分层Transformer

    Generative Pretrained Hierarchical Transformer for Time Series Forecasting

    [https://arxiv.org/abs/2402.16516](https://arxiv.org/abs/2402.16516)

    提出一种名为GPHT的新型生成式预训练分层Transformer架构，通过构建混合数据集来预训练模型，解决了时间序列预测中数据集限制和时间依赖性忽视的问题

    

    最近的研究致力于通过引入先进的网络架构和自监督预训练策略来提高时间序列预测的准确性。然而，现有方法仍存在两个关键缺陷。首先，这些方法通常依赖于单一数据集进行训练，由于训练数据的规模受限，限制了模型的泛化能力。其次，广泛采用一步生成模式，这需要定制化的预测头部，忽略了输出序列中的时间依赖性，同时在不同的时间跨度设置下会导致增加的训练成本。为了解决这些问题，我们提出了一种新颖的用于预测的生成式预训练分层Transformer架构，命名为GPHT。GPHT中有两个关键设计方面。

    arXiv:2402.16516v1 Announce Type: new  Abstract: Recent efforts have been dedicated to enhancing time series forecasting accuracy by introducing advanced network architectures and self-supervised pretraining strategies. Nevertheless, existing approaches still exhibit two critical drawbacks. Firstly, these methods often rely on a single dataset for training, limiting the model's generalizability due to the restricted scale of the training data. Secondly, the one-step generation schema is widely followed, which necessitates a customized forecasting head and overlooks the temporal dependencies in the output series, and also leads to increased training costs under different horizon length settings.   To address these issues, we propose a novel generative pretrained hierarchical transformer architecture for forecasting, named GPHT. There are two aspects of key designs in GPHT. On the one hand, we advocate for constructing a mixed dataset for pretraining our model, comprising various dataset
    
[^7]: 使用赌博反馈学习在线任务调度

    Learning to Schedule Online Tasks with Bandit Feedback

    [https://arxiv.org/abs/2402.16463](https://arxiv.org/abs/2402.16463)

    通过提出一种基于双乐观学习的Robbins-Monro算法，解决了在线任务调度中奖励和成本难以建模、任务到达分布不确定等挑战

    

    在云计算和众包中，在线任务调度对于任务密集型应用起着至关重要的作用。优化的调度可以在某些任务到达分布下增强系统性能，通常通过奖励成本比来衡量。然而，奖励和成本都依赖于任务上下文（例如，评价指标）且在实践中保持黑盒状态。这使得奖励和成本难以建模，因此在决策之前未知。另一方面，任务到达行为对于诸如系统波动等因素保持敏感，先前的估计或常规到达分布假设（例如，泊松分布）可能失效。这意味着另一个实际但常被忽视的挑战，即不确定的任务到达分布。为了解决在各种不确定性的静态环境下有效调度的问题，我们提出了一种基于双乐观学习的Robbins-Monro（DOL-RM）算法。

    arXiv:2402.16463v1 Announce Type: new  Abstract: Online task scheduling serves an integral role for task-intensive applications in cloud computing and crowdsourcing. Optimal scheduling can enhance system performance, typically measured by the reward-to-cost ratio, under some task arrival distribution. On one hand, both reward and cost are dependent on task context (e.g., evaluation metric) and remain black-box in practice. These render reward and cost hard to model thus unknown before decision making. On the other hand, task arrival behaviors remain sensitive to factors like unpredictable system fluctuation whereby a prior estimation or the conventional assumption of arrival distribution (e.g., Poisson) may fail. This implies another practical yet often neglected challenge, i.e., uncertain task arrival distribution. Towards effective scheduling under a stationary environment with various uncertainties, we propose a double-optimistic learning based Robbins-Monro (DOL-RM) algorithm. Spec
    
[^8]: 在具有配对次模模函数的分布式大于内存的子集选择问题研究

    On Distributed Larger-Than-Memory Subset Selection With Pairwise Submodular Functions

    [https://arxiv.org/abs/2402.16442](https://arxiv.org/abs/2402.16442)

    本文提出了一种新颖的分布式约束算法，通过迭代绑定最小和最大效用值来选择高质量的点并丢弃不重要的点。

    

    许多学习问题取决于子集选择的基本问题，即确定一组重要和代表性的点。本文提出了一种具有可证估计近似保证的新颖分布式约束算法，它通过迭代绑定最小和最大效用值来选择高质量的点并丢弃不重要的点。

    arXiv:2402.16442v1 Announce Type: cross  Abstract: Many learning problems hinge on the fundamental problem of subset selection, i.e., identifying a subset of important and representative points. For example, selecting the most significant samples in ML training cannot only reduce training costs but also enhance model quality. Submodularity, a discrete analogue of convexity, is commonly used for solving subset selection problems. However, existing algorithms for optimizing submodular functions are sequential, and the prior distributed methods require at least one central machine to fit the target subset. In this paper, we relax the requirement of having a central machine for the target subset by proposing a novel distributed bounding algorithm with provable approximation guarantees. The algorithm iteratively bounds the minimum and maximum utility values to select high quality points and discard the unimportant ones. When bounding does not find the complete subset, we use a multi-round, 
    
[^9]: 通过不变统计损失训练隐式生成模型

    Training Implicit Generative Models via an Invariant Statistical Loss

    [https://arxiv.org/abs/2402.16435](https://arxiv.org/abs/2402.16435)

    提出了一种通过不变统计损失训练隐式生成模型的方法，解决了训练不稳定和模式缺失问题

    

    隐式生成模型具有学习任意复杂数据分布的能力。然而，训练需要通过对抗性鉴别器区分真实数据和人工生成的数据，导致训练不稳定和模式缺失问题。在这项工作中，我们提出了一种无需鉴别器的方法用于训练一维（1D）隐式生成模型，随后将该方法扩展以适应多变量情况。我们的损失函数是模型样本经过适当选择的变换与均匀分布之间的差异度量；因此，它对数据的真实分布保持不变。我们首先为一维随机变量制定我们的方法，为近似重参数化提供了有效的解决方案。

    arXiv:2402.16435v1 Announce Type: cross  Abstract: Implicit generative models have the capability to learn arbitrary complex data distributions. On the downside, training requires telling apart real data from artificially-generated ones using adversarial discriminators, leading to unstable training and mode-dropping issues. As reported by Zahee et al. (2017), even in the one-dimensional (1D) case, training a generative adversarial network (GAN) is challenging and often suboptimal. In this work, we develop a discriminator-free method for training one-dimensional (1D) generative implicit models and subsequently expand this method to accommodate multivariate cases. Our loss function is a discrepancy measure between a suitably chosen transformation of the model samples and a uniform distribution; hence, it is invariant with respect to the true distribution of the data. We first formulate our method for 1D random variables, providing an effective solution for approximate reparameterization 
    
[^10]: TOTEM：用于一般时间序列分析的令牌化时间序列嵌入

    TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis

    [https://arxiv.org/abs/2402.16412](https://arxiv.org/abs/2402.16412)

    TOTEM提出了一种简单的令牌化架构，通过自监督学习的离散矢量化表示嵌入不同领域的时间序列数据，能够实现通用、跨领域训练，在多个任务和领域上进行广泛评估。

    

    最近，一般时间序列分析领域开始探索统一建模，其中一个通用的架构可以在特定任务和特定数据集上进行重新训练。本文从一个互补的角度接近统一化：跨任务和领域的统一化。为此，我们探讨了离散、学习到的时间序列数据表示对启用通用、跨领域训练的影响。我们的方法TOTEM，即TOkenized Time Series EMbeddings，提出了一种简单的标记器架构，通过以自监督方式学习的离散矢量化表示嵌入来自不同领域的时间序列数据。TOTEM可以跨多个任务和领域工作，几乎不需要调整。我们通过对3个任务上的17个真实世界时间序列数据集进行广泛评估来研究TOTEM的有效性。我们评估了专家（即在每个领域训练模型）和通用（即训练）的TOTEM。

    arXiv:2402.16412v1 Announce Type: new  Abstract: The field of general time series analysis has recently begun to explore unified modeling, where a common architectural backbone can be retrained on a specific task for a specific dataset. In this work, we approach unification from a complementary vantage point: unification across tasks and domains. To this end, we explore the impact of discrete, learnt, time series data representations that enable generalist, cross-domain training. Our method, TOTEM, or TOkenized Time Series EMbeddings, proposes a simple tokenizer architecture that embeds time series data from varying domains using a discrete vectorized representation learned in a self-supervised manner. TOTEM works across multiple tasks and domains with minimal to no tuning. We study the efficacy of TOTEM with an extensive evaluation on 17 real world time series datasets across 3 tasks. We evaluate both the specialist (i.e., training a model on each domain) and generalist (i.e., trainin
    
[^11]: 高维变分推断中正规化流的稳定训练

    Stable Training of Normalizing Flows for High-dimensional Variational Inference

    [https://arxiv.org/abs/2402.16408](https://arxiv.org/abs/2402.16408)

    提出了稳定训练高维变分推断中正规化流的方法

    

    使用正规化流进行变分推断在取代MCMC方法方面越来越受欢迎。特别是基于耦合层（Real NVPs）的正规化流由于其良好的经验性能而经常使用。然而，在实践中，训练用于逼近高维后验分布的深层正规化流通常是不可行的，因为随机梯度的高方差。在这项工作中，我们展示了先前用于稳定随机梯度下降方差的方法可能不足以实现Real NVPs的稳定训练。我们确定问题的根源是，在训练期间，样本通常呈现异常高的值。为了解决这个问题，我们提出了两种方法的组合：（1）对Real NVPs中的尺度进行软阈值处理，以及（2）对样本进行双射软对数变换。

    arXiv:2402.16408v1 Announce Type: cross  Abstract: Variational inference with normalizing flows (NFs) is an increasingly popular alternative to MCMC methods. In particular, NFs based on coupling layers (Real NVPs) are frequently used due to their good empirical performance. In theory, increasing the depth of normalizing flows should lead to more accurate posterior approximations. However, in practice, training deep normalizing flows for approximating high-dimensional posterior distributions is often infeasible due to the high variance of the stochastic gradients. In this work, we show that previous methods for stabilizing the variance of stochastic gradient descent can be insufficient to achieve stable training of Real NVPs. As the source of the problem, we identify that, during training, samples often exhibit unusual high values. As a remedy, we propose a combination of two methods: (1) soft-thresholding of the scale in Real NVPs, and (2) a bijective soft log transformation of the sam
    
[^12]: 基于分布式边布局的图学习

    Graph Learning with Distributional Edge Layouts

    [https://arxiv.org/abs/2402.16402](https://arxiv.org/abs/2402.16402)

    图神经网络中提出了一种新的全局布局采样方法，Distributional Edge Layouts（DELs），通过Langevin动力学和玻尔兹曼分布，能够捕获广泛的能量分布，提供额外的表达能力，有助于简化下游任务。

    

    图神经网络（GNNs）通过在某些拓扑布局上沿着边在相邻节点之间传递局部信息来学习图结构数据。在现代GNNs中，这些拓扑布局通常是按照确定性计算（例如，基于注意力的GNNs）或在启发性假设下本地采样（例如，GraphSage）的。本文首次提出这些布局可以通过Langevin动力学全局采样，遵循配备明确物理能量的玻尔兹曼分布，从而在物理世界中更具可行性。我们认为这样一组采样/优化的布局可以捕获广泛的能量分布，并在WL-test之上带来额外的表达能力，因此有助于简化下游任务。因此，我们提出了分布式边布局（DELs）作为各种GNNs的补充。DEL是一个与后续GNN变种无关的预处理策略，因此非常灵活。

    arXiv:2402.16402v1 Announce Type: cross  Abstract: Graph Neural Networks (GNNs) learn from graph-structured data by passing local messages between neighboring nodes along edges on certain topological layouts. Typically, these topological layouts in modern GNNs are deterministically computed (e.g., attention-based GNNs) or locally sampled (e.g., GraphSage) under heuristic assumptions. In this paper, we for the first time pose that these layouts can be globally sampled via Langevin dynamics following Boltzmann distribution equipped with explicit physical energy, leading to higher feasibility in the physical world. We argue that such a collection of sampled/optimized layouts can capture the wide energy distribution and bring extra expressivity on top of WL-test, therefore easing downstream tasks. As such, we propose Distributional Edge Layouts (DELs) to serve as a complement to a variety of GNNs. DEL is a pre-processing strategy independent of subsequent GNN variants, thus being highly fl
    
[^13]: 具有交叉一致$p$-值的异常检测中的不确定性量化

    Uncertainty Quantification in Anomaly Detection with Cross-Conformal $p$-Values

    [https://arxiv.org/abs/2402.16388](https://arxiv.org/abs/2402.16388)

    针对异常检测系统中不确定性量化的需求，提出了一种新颖的框架，称为交叉一致异常检测，通过校准模型的不确定性提供统计保证。

    

    随着可靠、可信和可解释机器学习的重要性日益增加，对异常检测系统进行不确定性量化的要求变得愈发重要。在这种情况下，有效控制类型I错误率($\alpha$)而又不损害系统的统计功率($1-\beta$)可以建立信任，并减少与假发现相关的成本，特别是当后续程序昂贵时。利用符合预测原则的方法有望通过校准模型的不确定性为异常检测提供相应的统计保证。该工作引入了一个新颖的异常检测框架，称为交叉一致异常检测，建立在为预测任务设计的著名交叉一致方法之上。通过这种方法，他填补了在归纳一致异常检测环境中扩展先前研究的自然研究空白

    arXiv:2402.16388v1 Announce Type: cross  Abstract: Given the growing significance of reliable, trustworthy, and explainable machine learning, the requirement of uncertainty quantification for anomaly detection systems has become increasingly important. In this context, effectively controlling Type I error rates ($\alpha$) without compromising the statistical power ($1-\beta$) of these systems can build trust and reduce costs related to false discoveries, particularly when follow-up procedures are expensive. Leveraging the principles of conformal prediction emerges as a promising approach for providing respective statistical guarantees by calibrating a model's uncertainty. This work introduces a novel framework for anomaly detection, termed cross-conformal anomaly detection, building upon well-known cross-conformal methods designed for prediction tasks. With that, it addresses a natural research gap by extending previous works in the context of inductive conformal anomaly detection, rel
    
[^14]: 关于时间图学习算法的泛化能力：理论见解与一种更简单的方法

    On the Generalization Capability of Temporal Graph Learning Algorithms: Theoretical Insights and a Simpler Method

    [https://arxiv.org/abs/2402.16387](https://arxiv.org/abs/2402.16387)

    本文探讨了时间图学习算法的泛化能力，并提出了一种更简单的方法Simplified-Temporal-Graph-Network。

    

    时间图学习（TGL）已成为不同真实应用中普遍采用的技术，尤其是在数据可以表示为随时间演变的图的领域。尽管TGL在算法解决方案方面最近取得了显着进展，但其理论基础仍然大部分未被探索。本文旨在通过研究有限宽超参数化条件下不同TGL算法（如基于GNN、基于RNN和基于内存的方法）的泛化能力，来弥合这一差距。我们建立了TGL算法的泛化误差与GNN-/RNN- TGL方法中“层数/步数”以及特征-标签对齐（FLA）分数之间的关系，其中FLA可用作表达能力的代理，并解释了基于内存的方法的性能。在我们的理论分析的指导下，我们提出了简化时间图网络，该方法具有较小的泛化误差。

    arXiv:2402.16387v1 Announce Type: cross  Abstract: Temporal Graph Learning (TGL) has become a prevalent technique across diverse real-world applications, especially in domains where data can be represented as a graph and evolves over time. Although TGL has recently seen notable progress in algorithmic solutions, its theoretical foundations remain largely unexplored. This paper aims at bridging this gap by investigating the generalization ability of different TGL algorithms (e.g., GNN-based, RNN-based, and memory-based methods) under the finite-wide over-parameterized regime. We establish the connection between the generalization error of TGL algorithms and "the number of layers/steps" in the GNN-/RNN-based TGL methods and "the feature-label alignment (FLA) score", where FLA can be used as a proxy for the expressive power and explains the performance of memory-based methods. Guided by our theoretical analysis, we propose Simplified-Temporal-Graph-Network, which enjoys a small generaliza
    
[^15]: 自监督基于相关性的多视图聚类排序

    Self Supervised Correlation-based Permutations for Multi-View Clustering

    [https://arxiv.org/abs/2402.16383](https://arxiv.org/abs/2402.16383)

    提出了一种基于深度学习的多视图聚类框架，利用新颖的基于置换的规范相关性目标学习融合数据表示，并通过识别多个视图的一致伪标签来学习聚类分配，实验结果表明模型有效性，理论上证明逼近监督线性判别分析（LDA）表示，提供了由错误伪标签注释引起的误差界限。

    

    融合来自不同模态的信息可以增强数据分析任务，包括聚类。然而，现有的多视图聚类（MVC）解决方案仅限于特定领域，或者依赖于次优的且计算需求高的表示和聚类两阶段程序。我们提出了一个基于端到端深度学习的通用数据（图像、表格等）的MVC框架。我们的方法涉及使用基于新颖置换的规范相关性目标来学习有意义的融合数据表示。同时，我们通过识别跨多个视图的一致伪标签来学习聚类分配。我们使用十个MVC基准数据集展示了我们模型的有效性。在理论上，我们证明了我们的模型逼近了监督线性判别分析（LDA）表示。另外，我们提供了由错误伪标签注释引起的误差界限。

    arXiv:2402.16383v1 Announce Type: new  Abstract: Fusing information from different modalities can enhance data analysis tasks, including clustering. However, existing multi-view clustering (MVC) solutions are limited to specific domains or rely on a suboptimal and computationally demanding two-stage procedure of representation and clustering. We propose an end-to-end deep learning-based MVC framework for general data (image, tabular, etc.). Our approach involves learning meaningful fused data representations with a novel permutation-based canonical correlation objective. Concurrently, we learn cluster assignments by identifying consistent pseudo-labels across multiple views. We demonstrate the effectiveness of our model using ten MVC benchmark datasets. Theoretically, we show that our model approximates the supervised linear discrimination analysis (LDA) representation. Additionally, we provide an error bound induced by false-pseudo label annotations.
    
[^16]: 一种用于生成高质量文本转语音数据集的自动化端到端开源软件

    An Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation

    [https://arxiv.org/abs/2402.16380](https://arxiv.org/abs/2402.16380)

    该论文介绍了一种端到端工具，用于生成高质量的文本转语音（TTS）模型数据集，实现了语言特定的语音分布整合、自动化录制过程、自动化和人机协作的录音质量保证以及录音格式处理。

    

    数据的可用性对推动人工智能应用至关重要，包括基于语音的技术。随着内容创作的需求增加，尤其是社交媒体上的内容，翻译和文本转语音（TTS）技术已经成为必不可少的工具。本文介绍了一种端到端工具，用于生成高质量的文本转语音（TTS）模型数据集，以解决对高质量数据的急切需求。本作品的贡献多方面，包括：将语言特定的语音分布整合到样本选择中，自动化录制过程，自动化和人机协作的录音质量保证，以及处理录音以满足指定格式。

    arXiv:2402.16380v1 Announce Type: cross  Abstract: Data availability is crucial for advancing artificial intelligence applications, including voice-based technologies. As content creation, particularly in social media, experiences increasing demand, translation and text-to-speech (TTS) technologies have become essential tools. Notably, the performance of these TTS technologies is highly dependent on the quality of the training data, emphasizing the mutual dependence of data availability and technological progress. This paper introduces an end-to-end tool to generate high-quality datasets for text-to-speech (TTS) models to address this critical need for high-quality data. The contributions of this work are manifold and include: the integration of language-specific phoneme distribution into sample selection, automation of the recording process, automated and human-in-the-loop quality assurance of recordings, and processing of recordings to meet specified formats. The proposed application
    
[^17]: 图学习在分布变化下的研究：领域自适应、外部分布和持续学习的综述

    Graph Learning under Distribution Shifts: A Comprehensive Survey on Domain Adaptation, Out-of-distribution, and Continual Learning

    [https://arxiv.org/abs/2402.16374](https://arxiv.org/abs/2402.16374)

    该综述调查了解决图学习中分布变化问题的最新方法、策略和见解。

    

    图学习在各种应用场景中发挥着关键作用，并且因其在建模由图结构数据表示的复杂数据关系方面的有效性而引起了广泛关注，从社交网络分析到推荐系统。现实中，真实世界的图数据通常随时间动态变化，节点属性和边结构也会发生变化，导致严重的图数据分布转移问题。这个问题受到分布转移多样性和复杂性的影响，这些转移可以显著影响图学习方法在降低泛化和适应能力方面的性能，给它们的有效性带来了重大挑战。在这项调查中，我们对解决图学习背景下的分布变化的最新方法、策略和见解进行了全面回顾和总结。

    arXiv:2402.16374v1 Announce Type: new  Abstract: Graph learning plays a pivotal role and has gained significant attention in various application scenarios, from social network analysis to recommendation systems, for its effectiveness in modeling complex data relations represented by graph structural data. In reality, the real-world graph data typically show dynamics over time, with changing node attributes and edge structure, leading to the severe graph data distribution shift issue. This issue is compounded by the diverse and complex nature of distribution shifts, which can significantly impact the performance of graph learning methods in degraded generalization and adaptation capabilities, posing a substantial challenge to their effectiveness. In this survey, we provide a comprehensive review and summary of the latest approaches, strategies, and insights that address distribution shifts within the context of graph learning. Concretely, according to the observability of distributions 
    
[^18]: 视觉中的生成人工智能：模型、度量和应用的调查

    Generative AI in Vision: A Survey on Models, Metrics and Applications

    [https://arxiv.org/abs/2402.16369](https://arxiv.org/abs/2402.16369)

    扩散模型作为一种强大的方法正在生成高质量图像、文本和音频，而此调查论文旨在全面概述生成AI扩散和传统模型的基本技术、应用和挑战。

    

    生成AI模型通过实现逼真和多样化的数据样本的创建，已经在各个领域引起了革命。在这些模型中，扩散模型已经成为生成高质量图像、文本和音频的强大方法。这篇调查论文全面概述了生成AI扩散和传统模型，重点关注它们的基本技术、在不同领域的应用以及它们所面临的挑战。我们深入探讨了扩散模型的理论基础，包括去噪扩散概率模型（DDPM）和基于得分的生成建模等概念。此外，我们探讨了这些模型在文本到图像、图像修补和图像超分辨率等多样化应用中的潜力，展示了它们在创造性任务和数据增强中的潜力。通过综合现有研究并突出这一领域的关键进展，本调查旨在提供一份关于生成AI在视觉中的综述。

    arXiv:2402.16369v1 Announce Type: cross  Abstract: Generative AI models have revolutionized various fields by enabling the creation of realistic and diverse data samples. Among these models, diffusion models have emerged as a powerful approach for generating high-quality images, text, and audio. This survey paper provides a comprehensive overview of generative AI diffusion and legacy models, focusing on their underlying techniques, applications across different domains, and their challenges. We delve into the theoretical foundations of diffusion models, including concepts such as denoising diffusion probabilistic models (DDPM) and score-based generative modeling. Furthermore, we explore the diverse applications of these models in text-to-image, image inpainting, and image super-resolution, along with others, showcasing their potential in creative tasks and data augmentation. By synthesizing existing research and highlighting critical advancements in this field, this survey aims to prov
    
[^19]: 从哪里出发？来自自然空间描述中的多尺度空间关系推理

    Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions

    [https://arxiv.org/abs/2402.16364](https://arxiv.org/abs/2402.16364)

    论文探讨了基于自然空间描述进行多尺度空间关系推理的方法，发现通过获知地图知识得到的描述能够提供环境的整体结构。

    

    当用自然语言传达路线时，“获得的空间知识”概念对地理信息检索（GIR）和空间认知研究至关重要。然而，自然语言处理导航研究经常忽视这种获得知识对文本描述的影响。当前导航研究集中在以自我为中心的本地描述（例如，“它将在您的右边”），这些描述需要对代理人的本地知觉进行推理。在地图获得的知识基础上的描述提供了环境的整体视图，并捕捉了其总体结构。

    arXiv:2402.16364v1 Announce Type: new  Abstract: When communicating routes in natural language, the concept of {\em acquired spatial knowledge} is crucial for geographic information retrieval (GIR) and in spatial cognitive research. However, NLP navigation studies often overlook the impact of such acquired knowledge on textual descriptions. Current navigation studies concentrate on egocentric local descriptions (e.g., `it will be on your right') that require reasoning over the agent's local perception. These instructions are typically given as a sequence of steps, with each action-step explicitly mentioning and being followed by a landmark that the agent can use to verify they are on the right path (e.g., `turn right and then you will see...'). In contrast, descriptions based on knowledge acquired through a map provide a complete view of the environment and capture its overall structure. These instructions (e.g., `it is south of Central Park and a block north of a police station') are 
    
[^20]: 反馈高效在线微调扩散模型

    Feedback Efficient Online Fine-Tuning of Diffusion Models

    [https://arxiv.org/abs/2402.16359](https://arxiv.org/abs/2402.16359)

    提出了一种反馈高效的在线微调扩散模型的强化学习程序

    

    扩散模型在建模复杂数据分布方面表现出色，包括图像，蛋白质和小分子的分布。然而，在许多情况下，我们的目标是模拟最大化某些属性的分布的部分：例如，我们可能希望生成具有高审美质量的图像，或具有高生物活性的分子。自然地，我们可以将这视为一个强化学习（RL）问题，其目标是微调扩散模型以最大化与某些属性对应的奖励函数。即使可以访问地面真实奖励函数的在线查询，有效地发现高奖励样本也可能具有挑战性：它们在初始分布中的概率可能很低，并且可能存在许多不可行的样本，甚至没有定义良好的奖励（例如，不自然的图像或物理上不可能的分子）。在这项工作中，我们提出了一种新颖的强化学习程序，可以高效地发现高奖励样本。

    arXiv:2402.16359v1 Announce Type: cross  Abstract: Diffusion models excel at modeling complex data distributions, including those of images, proteins, and small molecules. However, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity. It is natural to frame this as a reinforcement learning (RL) problem, in which the objective is to fine-tune a diffusion model to maximize a reward function that corresponds to some property. Even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules). In this work, we propose a novel reinforcement learning procedure that effi
    
[^21]: 一个整合的数据处理框架用于预训练基础模型

    An Integrated Data Processing Framework for Pretraining Foundation Models

    [https://arxiv.org/abs/2402.16358](https://arxiv.org/abs/2402.16358)

    提出了一个集成了处理模块和分析模块的数据处理框架，旨在改善数据质量并展示其有效性。

    

    基础模型的能力在很大程度上依赖于大规模、多样化和高质量的预训练数据。为了提高数据质量，研究人员和从业者经常需要手动从不同来源策划数据集，并为每个数据存储库开发专门的数据清洗流程。缺乏统一的数据处理框架，这一过程重复而繁琐。为了缓解这一问题，我们提出了一个集成了处理模块和分析模块的数据处理框架，处理模块包括一系列不同粒度水平的操作符，而分析模块支持对精炼数据进行探查和评估。所提出的框架易于使用且高度灵活。在这篇演示论文中，我们首先介绍如何使用这个框架并展示它在改善数据质量方面的有效性，通过与ChatGPT的自动评估和端到端评估。

    arXiv:2402.16358v1 Announce Type: cross  Abstract: The ability of the foundation models heavily relies on large-scale, diverse, and high-quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and develop dedicated data cleansing pipeline for each data repository. Lacking a unified data processing framework, this process is repetitive and cumbersome. To mitigate this issue, we propose a data processing framework that integrates a Processing Module which consists of a series of operators at different granularity levels, and an Analyzing Module which supports probing and evaluation of the refined data. The proposed framework is easy to use and highly flexible. In this demo paper, we first introduce how to use this framework with some example use cases and then demonstrate its effectiveness in improving the data quality with an automated evaluation with ChatGPT and an end-to-end evaluation in 
    
[^22]: 通过时间变分推断进行语言引导的技能学习

    Language-guided Skill Learning with Temporal Variational Inference

    [https://arxiv.org/abs/2402.16354](https://arxiv.org/abs/2402.16354)

    该论文提出了一种语言引导的技能学习算法，通过整合大型语言模型生成的分割信息来发现可重用的技能，并引入最小描述长度原则来引导这一过程，实现了在不同环境中加速学习并超越基线方法的效果。

    

    我们提出了一种从专家演示中发现技能的算法。该算法首先利用大型语言模型（LLMs）来提出轨迹的初始分割。随后，一个分层变分推断框架将LLM生成的分割信息纳入其中，通过合并轨迹段来发现可重用的技能。为了进一步控制压缩和可重用性之间的权衡，我们引入了一个基于最小描述长度原则的新辅助目标，帮助引导这种技能发现过程。我们的结果表明，使用我们方法的Agent能够发现有助于加速学习的技能，在BabyAI（一个网格世界导航环境）以及ALFRED（一个家庭模拟环境）的新长期任务中胜过基线技能学习方法。

    arXiv:2402.16354v1 Announce Type: cross  Abstract: We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.
    
[^23]: 在量子态重构中量子纠缠和复制复杂性之间的最优权衡

    An optimal tradeoff between entanglement and copy complexity for state tomography

    [https://arxiv.org/abs/2402.16353](https://arxiv.org/abs/2402.16353)

    本研究在量子态重构中研究了可以同时测量多个副本的情况，发现为了学习一个未知的$d$维态到跟踪距离$\epsilon$，需要并且足够的拷贝为$\widetilde{\Theta}(\frac{d^3}{\sqrt{t}\epsilon^2})$。

    

    对于当代量子设备的实际约束如何影响量子学习的复杂性存在着相当大的兴趣。对于经典问题——态重构，最近的研究严格刻画了只能同时测量未知态的一个副本的任何协议的复制复杂性，表明这种复杂性在多项式时间内比可以进行完全纠缠测量的情况更糟。虽然我们现在已经比较全面地了解了在近期和容错区域进行此类任务的速率，但目前我们还不太了解其中的景象是什么样的。在这项工作中，我们研究了可以同时对$t$个副本进行测量的自然情境中的态重构。对于足够小的$\epsilon$，我们证明了对于任意$t \le d^2$，学习一个未知的$d$维态$\rho$到跟踪距离$\epsilon$需要并且足够的拷贝为$\widetilde{\Theta}(\frac{d^3}{\sqrt{t}\epsilon^2})$。

    arXiv:2402.16353v1 Announce Type: cross  Abstract: There has been significant interest in understanding how practical constraints on contemporary quantum devices impact the complexity of quantum learning. For the classic question of tomography, recent work tightly characterized the copy complexity for any protocol that can only measure one copy of the unknown state at a time, showing it is polynomially worse than if one can make fully-entangled measurements. While we now have a fairly complete picture of the rates for such tasks in the near-term and fault-tolerant regimes, it remains poorly understood what the landscape in between looks like.   In this work, we study tomography in the natural setting where one can make measurements of $t$ copies at a time. For sufficiently small $\epsilon$, we show that for any $t \le d^2$, $\widetilde{\Theta}(\frac{d^3}{\sqrt{t}\epsilon^2})$ copies are necessary and sufficient to learn an unknown $d$-dimensional state $\rho$ to trace distance $\epsilo
    
[^24]: C-GAIL: 利用控制理论稳定生成对抗模仿学习

    C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory

    [https://arxiv.org/abs/2402.16349](https://arxiv.org/abs/2402.16349)

    该论文利用控制理论改进了生成对抗模仿学习（GAIL），提出了一种名为“Controlled-GAIL”（C-GAIL）的算法，能够解决GAIL训练不稳定性的问题，并在MuJoCo任务中取得了较快的收敛速度。

    

    生成对抗模仿学习（GAIL）训练一个生成策略来模仿一个演示者。它使用基于策略的强化学习（RL）来优化从类似GAN的鉴别器中导出的奖励信号。GAIL的一个主要缺点是其训练不稳定性 - 它继承了GAN的复杂训练动态，以及RL引入的分布转移。这可能导致训练过程中的振荡，从而影响其样本效率和最终策略性能。最近的工作表明，控制理论可以帮助GAN的训练收敛。本文延伸了这一线路的工作，对GAIL进行了控制理论分析，并导出了一种新颖的控制器，该控制器不仅将GAIL推向期望的均衡点，还在“单步”设置中实现了渐近稳定性。基于此，我们提出了一个实用算法“Controlled-GAIL”（C-GAIL）。在MuJoCo任务中，我们的受控变体能够加速收敛速度。

    arXiv:2402.16349v1 Announce Type: new  Abstract: Generative Adversarial Imitation Learning (GAIL) trains a generative policy to mimic a demonstrator. It uses on-policy Reinforcement Learning (RL) to optimize a reward signal derived from a GAN-like discriminator. A major drawback of GAIL is its training instability - it inherits the complex training dynamics of GANs, and the distribution shift introduced by RL. This can cause oscillations during training, harming its sample efficiency and final policy performance. Recent work has shown that control theory can help with the convergence of a GAN's training. This paper extends this line of work, conducting a control-theoretic analysis of GAIL and deriving a novel controller that not only pushes GAIL to the desired equilibrium but also achieves asymptotic stability in a 'one-step' setting. Based on this, we propose a practical algorithm 'Controlled-GAIL' (C-GAIL). On MuJoCo tasks, our controlled variant is able to speed up the rate of conve
    
[^25]: 用持久同调增强图池化

    Boosting Graph Pooling with Persistent Homology

    [https://arxiv.org/abs/2402.16346](https://arxiv.org/abs/2402.16346)

    通过PH向池化层注入全局拓扑不变性的机制显著提升了图神经网络的性能。

    

    最近，将持久同调（PH）纳入图神经网络（GNN）以丰富表达能力的趋势越来越明显。然而，简单地将PH特征插入GNN层总是带来较低可解释性的边际改进。本文研究了一种新颖的机制，通过PH向池化层注入全局拓扑不变性，灵感来自PH中的过滤操作自然地使图池化以截断方式对齐。这种方式下，粗化图中的消息传递沿着持久池化拓扑进行，从而提升性能。在实验中，我们将该机制应用于一系列图池化方法，并观察到在几个常见数据集上持续且显著的性能提升，展示了其广泛适用性和灵活性。

    arXiv:2402.16346v1 Announce Type: new  Abstract: Recently, there has been an emerging trend to integrate persistent homology (PH) into graph neural networks (GNNs) to enrich expressive power. However, naively plugging PH features into GNN layers always results in marginal improvement with low interpretability. In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns graph pooling in a cut-off manner. In this fashion, message passing in the coarsened graph acts along persistent pooled topology, leading to improved performance. Experimentally, we apply our mechanism to a collection of graph pooling methods and observe consistent and substantial performance gain over several popular datasets, demonstrating its wide applicability and flexibility.
    
[^26]: 逻辑回归的可证实准确性随机抽样算法

    A Provably Accurate Randomized Sampling Algorithm for Logistic Regression

    [https://arxiv.org/abs/2402.16326](https://arxiv.org/abs/2402.16326)

    提出了一种逻辑回归问题的简单随机抽样算法，通过随机矩阵乘法实现高质量逼近估计概率和模型整体差异性。

    

    在统计学和机器学习中，逻辑回归是一种广泛应用于二分类任务的监督学习技术。当观测数量远远超过预测变量数量时，我们提出了一种简单的基于随机抽样的逻辑回归问题算法，保证高质量逼近估计概率和模型整体差异性。我们的分析建立在两个简单的结构条件基础上，这两个条件可归结为随机矩阵乘法，是随机化数值线性代数的基本且深入理解的基元。当利用杠杆分数对观测进行抽样时，我们分析了逻辑回归的估计概率属性，并证明准确逼近可以通过远小于总观测数的样本实现。为了进一步验证我们的理论发现，

    arXiv:2402.16326v1 Announce Type: cross  Abstract: In statistics and machine learning, logistic regression is a widely-used supervised learning technique primarily employed for binary classification tasks. When the number of observations greatly exceeds the number of predictor variables, we present a simple, randomized sampling-based algorithm for logistic regression problem that guarantees high-quality approximations to both the estimated probabilities and the overall discrepancy of the model. Our analysis builds upon two simple structural conditions that boil down to randomized matrix multiplication, a fundamental and well-understood primitive of randomized numerical linear algebra. We analyze the properties of estimated probabilities of logistic regression when leverage scores are used to sample observations, and prove that accurate approximations can be achieved with a sample whose size is much smaller than the total number of observations. To further validate our theoretical findi
    
[^27]: 实现约$O(1/\epsilon)$的样本复杂度用于约束马尔可夫决策过程

    Achieving $\tilde{O}(1/\epsilon)$ Sample Complexity for Constrained Markov Decision Process

    [https://arxiv.org/abs/2402.16324](https://arxiv.org/abs/2402.16324)

    该论文提出了一种算法，在约束马尔可夫决策过程中实现了约$O(1/\epsilon)$的样本复杂度，相比先前文献中已有的$O(1/\epsilon^2)$样本复杂度有所提升。

    

    我们考虑约束马尔可夫决策过程（CMDP）的强化学习问题，在顺序学习和决策中满足安全性或资源约束方面起着关键作用。在这个问题中，我们拥有有限资源和未知转移概率的MDP。在每个阶段，我们采取一个行动，收集奖励并消耗一些资源，所有假设都是未知的，并且需要随着时间学习。在这项工作中，我们迈出了为CMDP问题推导出最优的问题相关保证的第一步。我们得出了一个对数遗憾界限，这转化为$O(\frac{\kappa}{\epsilon}\cdot\log^2(1/\epsilon))$的样本复杂度界限，其中$\kappa$是一个与问题相关的参数，但与$\epsilon$无关。我们的样本复杂度界限改进了先前文献中针对CMDP问题建立的$O(1/\epsilon^2)$样本复杂度。

    arXiv:2402.16324v1 Announce Type: new  Abstract: We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a $O(\frac{\kappa}{\epsilon}\cdot\log^2(1/\epsilon))$ sample complexity bound, with $\kappa$ being a problem-dependent parameter, yet independent of $\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms
    
[^28]: 仅使用清晰语音的自监督语音质量评估与增强

    Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech

    [https://arxiv.org/abs/2402.16321](https://arxiv.org/abs/2402.16321)

    提出了VQScore，一种基于矢量量化可变分编码器的自监督度量，用于评估语音质量并进行自监督语音增强训练，通过引入领域知识和新颖的自蒸馏机制提高模型的相关性和鲁棒性。

    

    最近，语音质量评估从人类听觉专家设计转变为机器学习模型。为了解决目前大多数模型依赖于监督学习而导致标签收集耗时昂贵的问题，本文提出了VQScore，一种基于矢量量化可变分编码器（VQ-VAE）的量化误差评估语音的自监督度量。VQ-VAE的训练依赖于清晰语音，因此当语音受到扭曲时可以预期到较大的量化误差。为了进一步提高与真实质量分数的相关性，将语音处理领域知识融入模型设计中。发现矢量量化机制也可以用于自监督语音增强（SE）模型训练。为了提高编码器对SE的鲁棒性，引入了一种新颖的自蒸馏机制结合对抗训练。

    arXiv:2402.16321v1 Announce Type: cross  Abstract: Speech quality estimation has recently undergone a paradigm shift from human-hearing expert designs to machine-learning models. However, current models rely mainly on supervised learning, which is time-consuming and expensive for label collection. To solve this problem, we propose VQScore, a self-supervised metric for evaluating speech based on the quantization error of a vector-quantized-variational autoencoder (VQ-VAE). The training of VQ-VAE relies on clean speech; hence, large quantization errors can be expected when the speech is distorted. To further improve correlation with real quality scores, domain knowledge of speech processing is incorporated into the model design. We found that the vector quantization mechanism could also be used for self-supervised speech enhancement (SE) model training. To improve the robustness of the encoder for SE, a novel self-distillation mechanism combined with adversarial training is introduced. I
    
[^29]: 具有异步通信和异质用户的联邦上下文级联多臂老虎机问题

    Federated Contextual Cascading Bandits with Asynchronous Communication and Heterogeneous Users

    [https://arxiv.org/abs/2402.16312](https://arxiv.org/abs/2402.16312)

    本研究提出了一种解决联邦上下文级联多臂老虎机问题的算法，通过异步通信和考虑异质用户行为，实现了对具有不同偏好的用户提供定制化推荐，并给出了次线性的遗憾界限。

    

    我们研究了联邦上下文组合级联多臂老虎机问题，其中$|\mathcal{U}|$个代理在一个中央服务器的协调下合作，为$|\mathcal{U}|$个对应的用户提供定制推荐。我们考虑了异步通信范式下的联邦代理，无需强制同步，并且所有代理都独立于服务器通信，以及异质用户行为，其中用户可以被分为$J\le |\mathcal{U}|$个潜在用户集群，每个集群展现出不同的偏好。针对这种情况，我们提出了一种带有精心通信协议的UCB类型算法。通过理论分析，我们给出了对于p的次线性遗憾界限。

    arXiv:2402.16312v1 Announce Type: cross  Abstract: We study the problem of federated contextual combinatorial cascading bandits, where $|\mathcal{U}|$ agents collaborate under the coordination of a central server to provide tailored recommendations to the $|\mathcal{U}|$ corresponding users. Existing works consider either a synchronous framework, necessitating full agent participation and global synchronization, or assume user homogeneity with identical behaviors. We overcome these limitations by considering (1) federated agents operating in an asynchronous communication paradigm, where no mandatory synchronization is required and all agents communicate independently with the server, (2) heterogeneous user behaviors, where users can be stratified into $J \le |\mathcal{U}|$ latent user clusters, each exhibiting distinct preferences. For this setting, we propose a UCB-type algorithm with delicate communication protocols. Through theoretical analysis, we give sub-linear regret bounds on p
    
[^30]: REPLAY: 对稀疏轨迹进行位置预测的人类移动时间变化规律建模

    REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories

    [https://arxiv.org/abs/2402.16310](https://arxiv.org/abs/2402.16310)

    该论文提出了REPLAY模型，利用一般RNN架构来学习捕捉人类移动的时间变化规律，用于位置预测。

    

    位置预测是根据历史用户移动轨迹来预测用户位置的技术。为了应对真实世界用户移动轨迹的固有稀疏问题，时空上下文被证明是非常有用的。现有的解决方案主要是将位置之间的时空距离纳入到移动轨迹中，要么通过将其作为附加输入提供给递归神经网络（RNNs），要么通过利用它们来寻找有信息的过去隐藏状态进行预测。然而，这种基于距离的方法未能捕捉人类移动的时间变化规律，例如，人类移动在早晨通常比其他时间更有规律；这暗示了实际时间戳的有用性。基于这一背景，我们提出了REPLAY，是一种通用的RNN架构，旨在捕捉时间变化的人类移动时间规律以进行位置预测。

    arXiv:2402.16310v1 Announce Type: cross  Abstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, 
    
[^31]: 审稿员也可以参与：通过模型反演进行条件生成的替代方法

    Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion

    [https://arxiv.org/abs/2402.16305](https://arxiv.org/abs/2402.16305)

    通过模型反演提出了一种训练自由的方法，可以绕过传统的采样过程，直接优化图像并获得更好的文本图像对齐，为改进图像生成提供了关键设计。

    

    作为文本到图像生成任务中占据主导地位的扩散概率模型（DPMs）面临着一个关键性挑战，即可控性方面的问题，难以严格遵守复杂、多方面的指令。在这项工作中，我们旨在解决条件生成任务中的这一协调挑战。首先，我们提供了一个替代观点，将现有最先进的DPMs视为反转先进视觉-语言模型（VLMs）的一种方式。通过这种表述，我们自然地提出了一种无需训练的方法，绕过与DPMs相关的传统采样过程。通过直接优化图像，并在有辨别力的VLMs的监督下，所提出的方法有潜力实现更好的文本图像对齐。作为概念的证明，我们演示了预训练的BLIP-2模型的流程，并确定了几个用于改进图像生成的关键设计。为进一步增强图像的保真度，我们提出了一个稳定的得分蒸馏采样模块。

    arXiv:2402.16305v1 Announce Type: cross  Abstract: As a dominant force in text-to-image generation tasks, Diffusion Probabilistic Models (DPMs) face a critical challenge in controllability, struggling to adhere strictly to complex, multi-faceted instructions. In this work, we aim to address this alignment challenge for conditional generation tasks. First, we provide an alternative view of state-of-the-art DPMs as a way of inverting advanced Vision-Language Models (VLMs). With this formulation, we naturally propose a training-free approach that bypasses the conventional sampling process associated with DPMs. By directly optimizing images with the supervision of discriminative VLMs, the proposed method can potentially achieve a better text-image alignment. As proof of concept, we demonstrate the pipeline with the pre-trained BLIP-2 model and identify several key designs for improved image generation. To further enhance the image fidelity, a Score Distillation Sampling module of Stable Di
    
[^32]: 图扩散策略优化

    Graph Diffusion Policy Optimization

    [https://arxiv.org/abs/2402.16302](https://arxiv.org/abs/2402.16302)

    本文引入了图扩散策略优化（GDPO），通过强化学习为任意目标优化图扩散模型，实现了在各种图生成任务中的最先进性能。

    

    最近的研究在优化扩散模型以实现特定下游目标方面取得了重要进展，这对于领域如药物设计中的图生成是一个重要的追求。然而，直接将这些模型应用于图扩散存在挑战，导致性能不佳。本文介绍了一种名为图扩散策略优化（GDPO）的新方法，该方法通过强化学习为任意（如非可微分）目标优化图扩散模型。GDPO基于针对图扩散模型量身定制的急切策略梯度，通过认真分析开发，有望提高性能。实验结果表明，GDPO在具有复杂和多样化目标的各种图生成任务中实现了最先进的性能。代码可在https://github.com/sail-sg/GDPO上找到。

    arXiv:2402.16302v1 Announce Type: cross  Abstract: Recent research has made significant progress in optimizing diffusion models for specific downstream objectives, which is an important pursuit in fields such as graph generation for drug design. However, directly applying these models to graph diffusion presents challenges, resulting in suboptimal performance. This paper introduces graph diffusion policy optimization (GDPO), a novel approach to optimize graph diffusion models for arbitrary (e.g., non-differentiable) objectives using reinforcement learning. GDPO is based on an eager policy gradient tailored for graph diffusion models, developed through meticulous analysis and promising improved performance. Experimental results show that GDPO achieves state-of-the-art performance in various graph generation tasks with complex and diverse objectives. Code is available at https://github.com/sail-sg/GDPO.
    
[^33]: Conformalized Selective Regression

    Conformalized Selective Regression

    [https://arxiv.org/abs/2402.16300](https://arxiv.org/abs/2402.16300)

    通过利用一致性预测，提供基于模型特定偏差的置信度量，以解决选择性回归中不确定性测量的方法。

    

    预测模型是否总是要提供预测？在追求最大预测性能的过程中，可靠性和公平性往往被忽视，尤其是关于不确定性的作用。选择性回归，也称为“拒绝选项”，允许模型在存在相当大的不确定性情况下放弃预测。尽管7十年前就最初提出了选择性回归的方法，但大多数方法主要集中在用于测量不确定性的基于分布的代理，尤其是条件方差。但这种关注忽视了模型特定偏差对模型性能的显著影响。本文提出了一种新的选择性回归方法，通过利用一致性预测，为基于模型特定偏差的个别预测提供有根据的置信度度量。此外，我们提出了一个标准化的评估框架，以便进行恰当的比较。

    arXiv:2402.16300v1 Announce Type: new  Abstract: Should prediction models always deliver a prediction? In the pursuit of maximum predictive performance, critical considerations of reliability and fairness are often overshadowed, particularly when it comes to the role of uncertainty. Selective regression, also known as the "reject option," allows models to abstain from predictions in cases of considerable uncertainty. Initially proposed seven decades ago, approaches to selective regression have mostly focused on distribution-based proxies for measuring uncertainty, particularly conditional variance. However, this focus neglects the significant influence of model-specific biases on a model's performance. In this paper, we propose a novel approach to selective regression by leveraging conformal prediction, which provides grounded confidence measures for individual predictions based on model-specific biases. In addition, we propose a standardized evaluation framework to allow proper compar
    
[^34]: 对抗筛选气泡：基于加权超图嵌入学习的音乐推荐多样化

    Against Filter Bubbles: Diversified Music Recommendation via Weighted Hypergraph Embedding Learning

    [https://arxiv.org/abs/2402.16299](https://arxiv.org/abs/2402.16299)

    引入了DWHRec算法来解决音乐推荐中准确性和多样性之间的平衡问题，通过加权超图嵌入学习来提高推荐系统的多样性。

    

    推荐系统对用户有着双重作用：过滤不合适或不匹配的信息，同时准确识别符合其偏好的项目。许多推荐算法旨在为用户提供个性化的信息阵列，以满足其偏好。然而，过度个性化可能会将用户限制在“筛选气泡”中。因此，在推荐中获得准确性和多样性之间的平衡是一项迫切关注的问题。为了应对这一挑战，以音乐推荐为例，我们介绍了多样化加权超图音乐推荐算法（DWHRec）。在DWHRec算法中，用户和已听曲目之间的初始连接由加权超图表示。同时，还将艺术家、专辑和标记与曲目的关联也附加到超图中。为了探索用户的潜在偏好，一个超图

    arXiv:2402.16299v1 Announce Type: cross  Abstract: Recommender systems serve a dual purpose for users: sifting out inappropriate or mismatched information while accurately identifying items that align with their preferences. Numerous recommendation algorithms are designed to provide users with a personalized array of information tailored to their preferences. Nevertheless, excessive personalization can confine users within a "filter bubble". Consequently, achieving the right balance between accuracy and diversity in recommendations is a pressing concern. To address this challenge, exemplified by music recommendation, we introduce the Diversified Weighted Hypergraph music Recommendation algorithm (DWHRec). In the DWHRec algorithm, the initial connections between users and listened tracks are represented by a weighted hypergraph. Simultaneously, associations between artists, albums and tags with tracks are also appended to the hypergraph. To explore users' latent preferences, a hypergrap
    
[^35]: 具有非平稳转移动态的泊松-伽马动力系统

    Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics

    [https://arxiv.org/abs/2402.16297](https://arxiv.org/abs/2402.16297)

    提出了一种具有非平稳转移动态的泊松-伽马动力系统，通过采用Dirichlet Markov链和数据增广技术来解决原有模型捕捉时变转移动态的不足。

    

    处理计数值时间序列的贝叶斯方法因其能够推断可解释的潜在结构和估计不确定性而备受重视，尤其适用于处理嘈杂和不完整的计数数据。在这些贝叶斯模型中，泊松-伽马动力系统（PGDSs）被证明能够有效捕捉观察到的计数序列底层动态的演变特征。然而，最新的PGDS在捕捉常见于实际计数时间序列中的时变转移动态方面仍有不足。为了克服这一限制，提出了一种非平稳PGDS，允许基础转移矩阵随时间演变，演变的转移矩阵由精心设计的Dirichlet Markov链建模。利用Dirichlet-Multinomial-Beta数据增广技术，开发了一个完全共轭且高效的Gibbs采样器。

    arXiv:2402.16297v1 Announce Type: cross  Abstract: Bayesian methodologies for handling count-valued time series have gained prominence due to their ability to infer interpretable latent structures and to estimate uncertainties, and thus are especially suitable for dealing with noisy and incomplete count data. Among these Bayesian models, Poisson-Gamma Dynamical Systems (PGDSs) are proven to be effective in capturing the evolving dynamics underlying observed count sequences. However, the state-of-the-art PGDS still falls short in capturing the time-varying transition dynamics that are commonly observed in real-world count time series. To mitigate this limitation, a non-stationary PGDS is proposed to allow the underlying transition matrices to evolve over time, and the evolving transition matrices are modeled by sophisticatedly-designed Dirichlet Markov chains. Leveraging Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed t
    
[^36]: 用AMS电磁量能器比较探测质子背景的深度学习模型

    A Comparison of Deep Learning Models for Proton Background Rejection with the AMS Electromagnetic Calorimeter

    [https://arxiv.org/abs/2402.16285](https://arxiv.org/abs/2402.16285)

    提出一种使用AMS电磁量能器进行质子背景拒绝的新模型，以解决高能正电子测量的挑战。

    

    Alpha Magnetic Spectrometer (AMS)是一种高精度粒子探测器，安装在国际空间站上，包含六种不同的次探测器。过渡辐射探测器和电磁量能器(ECAL)用于将电子/正电子与丰富的宇宙射线质子背景分开。 AMS在空间中测量到的正电子通量服从幂律，在25 GeV以上意外变软，然后在280 GeV以上变硬。有几种理论模型试图解释这些现象，需要更纯净的高能正电子测量来帮助测试它们。目前用于在高能量下拒绝质子背景的方法涉及从ECAL中外推淋浴特征，用作增强决策树和似然分类器的输入。我们提出一种使用AMS ECAL进行粒子识别的新方法，即深度学习(DL)。

    arXiv:2402.16285v1 Announce Type: cross  Abstract: The Alpha Magnetic Spectrometer (AMS) is a high-precision particle detector onboard the International Space Station containing six different subdetectors. The Transition Radiation Detector and Electromagnetic Calorimeter (ECAL) are used to separate electrons/positrons from the abundant cosmic-ray proton background.   The positron flux measured in space by AMS falls with a power law which unexpectedly softens above 25 GeV and then hardens above 280 GeV. Several theoretical models try to explain these phenomena, and a purer measurement of positrons at higher energies is needed to help test them. The currently used methods to reject the proton background at high energies involve extrapolating shower features from the ECAL to use as inputs for boosted decision tree and likelihood classifiers. We present a new approach for particle identification with the AMS ECAL using deep learning (DL). By taking the energy deposition within all the ECAL
    
[^37]: 一种使用注释嵌入模型的本体包含关系预测自匹配训练方法

    A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction

    [https://arxiv.org/abs/2402.16278](https://arxiv.org/abs/2402.16278)

    提出了一种自匹配训练方法，通过两种本体嵌入模型捕获全局和局部信息，提高了概念子类预测的稳健性

    

    最近，提出了一种在低维空间中表示实体的本体嵌入，用于本体完成。然而，用于概念子类预测的本体嵌入未解决类似和孤立实体的困难，并且未提取本体中注释公理的全局信息。本文提出了一种针对两种本体嵌入模型的自匹配训练方法：Inverted-index Matrix Embedding (InME) 和 Co-occurrence Matrix Embedding (CoME)。这两种嵌入通过每个单词在一组公理中出现的位置以及每个公理中单词的共现来捕获注释公理中的全局和局部信息。自匹配训练方法提高了概念子类预测的稳健性，当预测的超类与子类相似且孤立于本体中的其他实体时。

    arXiv:2402.16278v1 Announce Type: new  Abstract: Recently, ontology embeddings representing entities in a low-dimensional space have been proposed for ontology completion. However, the ontology embeddings for concept subsumption prediction do not address the difficulties of similar and isolated entities and fail to extract the global information of annotation axioms from an ontology. In this paper, we propose a self-matching training method for the two ontology embedding models: Inverted-index Matrix Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom. The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology. Our evaluation experiments show that
    
[^38]: 从大规模语言模型和优化到决策优化CoPilot：一项研究宣言

    From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto

    [https://arxiv.org/abs/2402.16269](https://arxiv.org/abs/2402.16269)

    运用大型语言模型与优化相结合，创建决策优化CoPilot（DOCP），帮助决策者通过自然语言交互理解并解决业务问题。

    

    大大简化为 real-world business problems 创建优化模型一直是将数学优化更广泛地应用于重要商业和社会决策的主要目标。最近大型语言模型（LLMs）的能力提供了一个及时的机会来实现这一目标。因此，我们提出在LLMs和优化的交叉点上开展研究，创建一个决策优化CoPilot（DOCP）- 一个旨在帮助任何决策者的AI工具，通过自然语言交互来理解业务问题，随后制定和解决相应优化模型。本文概述了我们的DOCP愿景，并确定了其实施的几个基本要求。我们通过文献调查和使用ChatGPT进行实验描述了现状。我们展示了a）LLMs已经提供了与DOCP相关的重大新功能，b）主要研究 c

    arXiv:2402.16269v1 Announce Type: new  Abstract: Significantly simplifying the creation of optimization models for real-world business problems has long been a major goal in applying mathematical optimization more widely to important business and societal decisions. The recent capabilities of Large Language Models (LLMs) present a timely opportunity to achieve this goal. Therefore, we propose research at the intersection of LLMs and optimization to create a Decision Optimization CoPilot (DOCP) - an AI tool designed to assist any decision maker, interacting in natural language to grasp the business problem, subsequently formulating and solving the corresponding optimization model. This paper outlines our DOCP vision and identifies several fundamental requirements for its implementation. We describe the state of the art through a literature survey and experiments using ChatGPT. We show that a) LLMs already provide substantial novel capabilities relevant to a DOCP, and b) major research c
    
[^39]: 基础模型透明度报告

    Foundation Model Transparency Reports

    [https://arxiv.org/abs/2402.16268](https://arxiv.org/abs/2402.16268)

    提出了基础模型透明度报告，借鉴社交媒体的透明度报告实践，目的在于在基础模型行业尚未成熟时制定透明度报告。

    

    基础模型是具有广泛社会影响的关键数字技术，需要透明度。为了规范基础模型开发者应如何提供有关其模型开发和部署的透明度，我们提出了基础模型透明度报告，借鉴社交媒体的透明度报告实践。尽管社交媒体透明度报告是由外部对社会伤害的文档促成的，我们的目标是在行业仍处于萌芽阶段时为基础模型制定透明度报告。为设计我们的报告，我们确定了6条设计原则，考虑了社交媒体透明度报告的成功和不足。为了进一步使我们的报告系统化，我们借鉴了基础模型透明度指数的100个透明度指标。根据这些指标，我们测量它们与六个主要透明度要求中包含的透明度要求的重叠程度。

    arXiv:2402.16268v1 Announce Type: cross  Abstract: Foundation models are critical digital technologies with sweeping societal impact that necessitates transparency. To codify how foundation model developers should provide transparency about the development and deployment of their models, we propose Foundation Model Transparency Reports, drawing upon the transparency reporting practices in social media. While external documentation of societal harms prompted social media transparency reports, our objective is to institutionalize transparency reporting for foundation models while the industry is still nascent. To design our reports, we identify 6 design principles given the successes and shortcomings of social media transparency reporting. To further schematize our reports, we draw upon the 100 transparency indicators from the Foundation Model Transparency Index. Given these indicators, we measure the extent to which they overlap with the transparency requirements included in six promine
    
[^40]: 留意你的头部：组装投影头以提高联邦模型的可靠性

    Watch Your Head: Assembling Projection Heads to Save the Reliability of Federated Models

    [https://arxiv.org/abs/2402.16255](https://arxiv.org/abs/2402.16255)

    联邦学习中发现当面对异构数据时，因存在有偏的投影头导致的联邦模型不可靠性，提出了“组装投影头”（APH）方法以提高模型可靠性。

    

    联邦学习在面对异构数据时遇到重大挑战，导致性能下降和收敛问题。尽管在减轻此类影响方面取得了相当大的进展，但联邦模型的可靠性方面却被大多忽视。在本研究中，我们进行了大量实验，研究了通用和个性化联邦模型的可靠性。我们的探索揭示了一个重要发现：\textbf{当面对异构数据时，联邦模型表现出不可靠性}，表现为对分布测试数据的较差校准和对分布外数据的低不确定性水平。这种不可靠性主要归因于存在有偏的投影头，这些投影头为联邦模型引入了错误校准。受到这一发现的启发，我们提出了用于增强联邦模型可靠性的“组装投影头”（APH）方法。

    arXiv:2402.16255v1 Announce Type: cross  Abstract: Federated learning encounters substantial challenges with heterogeneous data, leading to performance degradation and convergence issues. While considerable progress has been achieved in mitigating such an impact, the reliability aspect of federated models has been largely disregarded. In this study, we conduct extensive experiments to investigate the reliability of both generic and personalized federated models. Our exploration uncovers a significant finding: \textbf{federated models exhibit unreliability when faced with heterogeneous data}, demonstrating poor calibration on in-distribution test data and low uncertainty levels on out-of-distribution data. This unreliability is primarily attributed to the presence of biased projection heads, which introduce miscalibration into the federated models. Inspired by this observation, we propose the "Assembled Projection Heads" (APH) method for enhancing the reliability of federated models. By
    
[^41]: 学习翻译：应对合作语言习得的新兴沟通预训练

    Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition

    [https://arxiv.org/abs/2402.16247](https://arxiv.org/abs/2402.16247)

    提出了一个名为合作语言习得问题（CLAP）的新颖人工智能挑战，通过允许代理在目标社区中从互动数据集中学习，放宽了Zero-Shot Coordination假设。

    

    在新兴沟通中，代理学习彼此进行沟通，但他们制定的协议是针对他们的训练群体的。这一观察结果导致了对于学习对未在训练中遇到的代理稳健的沟通策略的Zero-Shot Coordination（ZSC）的研究。但是，ZSC通常假设关于在零-shot设置中会遇到的代理的先前数据是无法获得的。在许多情况下，这提出了一个不必要的棘手问题，并排除了通过预先建立的约定进行沟通。我们提出了一个名为合作语言习得问题（CLAP）的新颖人工智能挑战，在其中通过允许“加入者”代理从目标社区内代理之间的互动数据集中学习来放宽了ZSC假设。我们提出并比较了解决CLAPs的两种方法：模仿学习（IL）和新兴沟通的预训练和翻译学习。

    arXiv:2402.16247v1 Announce Type: cross  Abstract: In Emergent Communication (EC) agents learn to communicate with one another, but the protocols that they develop are specialised to their training community. This observation led to research into Zero-Shot Coordination (ZSC) for learning communication strategies that are robust to agents not encountered during training. However, ZSC typically assumes that no prior data is available about the agents that will be encountered in the zero-shot setting. In many cases, this presents an unnecessarily hard problem and rules out communication via preestablished conventions. We propose a novel AI challenge called a Cooperative Language Acquisition Problem (CLAP) in which the ZSC assumptions are relaxed by allowing a 'joiner' agent to learn from a dataset of interactions between agents in a target community. We propose and compare two methods for solving CLAPs: Imitation Learning (IL), and Emergent Communication pretraining and Translation Learni
    
[^42]: 具有理论保证的连续搜索空间中活跃水平集估计

    Active Level Set Estimation for Continuous Search Space with Theoretical Guarantee

    [https://arxiv.org/abs/2402.16237](https://arxiv.org/abs/2402.16237)

    提出了一种不需要任何离散化直接在连续搜索空间中工作的具有理论保证的活跃水平集估计算法

    

    许多现实世界应用中经常遇到的一个常见问题是水平集估计，其目标是确定函数域中函数高于或低于给定阈值的区域。 当函数是黑盒且评估成本高时，需要在最小的函数评估集中找到水平集。 现有方法通常假定为具有有限数据点集的离散搜索空间，用于函数评估和估计水平集。 当应用于连续搜索空间时，这些方法通常需要首先对空间进行离散化，这会导致结果不佳，同时需要高计算时间。 虽然一些方法适用于连续设定，但它们仍然缺乏对理论收敛的适当保证。 为了解决这一问题，我们提出了一种新颖算法，它不需要任何离散化，可以直接在连续搜索空间中工作。 我们的方法通过构建

    arXiv:2402.16237v1 Announce Type: cross  Abstract: A common problem encountered in many real-world applications is level set estimation where the goal is to determine the region in the function domain where the function is above or below a given threshold. When the function is black-box and expensive to evaluate, the level sets need to be found in a minimum set of function evaluations. Existing methods often assume a discrete search space with a finite set of data points for function evaluations and estimating the level sets. When applied to a continuous search space, these methods often need to first discretize the space which leads to poor results while needing high computational time. While some methods cater for the continuous setting, they still lack a proper guarantee for theoretical convergence. To address this problem, we propose a novel algorithm that does not need any discretization and can directly work in continuous search spaces. Our method suggests points by constructing 
    
[^43]: GARNN: 一种可解释的图注意力循环神经网络，用于通过多元时间序列预测血糖水平

    GARNN: An Interpretable Graph Attentive Recurrent Neural Network for Predicting Blood Glucose Levels via Multivariate Time Series

    [https://arxiv.org/abs/2402.16230](https://arxiv.org/abs/2402.16230)

    提出了一种可解释的图注意力循环神经网络（GARNNs），用于通过多元时间序列预测血糖水平，并实现了更具解释性的变量贡献总结和特征图生成。

    

    精确预测未来血糖（BG）水平可以有效改善糖尿病患者的血糖管理，从而减少并发症，提高生活质量。本文提出了可解释的图注意力循环神经网络（GARNNs），用于建模多元时间序列（MTS），通过总结变量重要性解释变量贡献，并通过图注意机制生成特征图，而不是进行事后分析。我们在代表不同临床场景的四个数据集上评估了GARNNs。与十二种公认的基准方法相比，GARNNs不仅实现了BG预测的最新技术水平，还更具解释性。

    arXiv:2402.16230v1 Announce Type: cross  Abstract: Accurate prediction of future blood glucose (BG) levels can effectively improve BG management for people living with diabetes, thereby reducing complications and improving quality of life. The state of the art of BG prediction has been achieved by leveraging advanced deep learning methods to model multi-modal data, i.e., sensor data and self-reported event data, organised as multi-variate time series (MTS). However, these methods are mostly regarded as ``black boxes'' and not entirely trusted by clinicians and patients. In this paper, we propose interpretable graph attentive recurrent neural networks (GARNNs) to model MTS, explaining variable contributions via summarizing variable importance and generating feature maps by graph attention mechanisms instead of post-hoc analysis. We evaluate GARNNs on four datasets, representing diverse clinical scenarios. Upon comparison with twelve well-established baseline methods, GARNNs not only ach
    
[^44]: IR2：信息正则化用于信息检索

    IR2: Information Regularization for Information Retrieval

    [https://arxiv.org/abs/2402.16200](https://arxiv.org/abs/2402.16200)

    介绍了IR2，一种用于在合成数据生成过程中减少过拟合的信息正则化技术，在复杂查询的信息检索任务中表现出优越性能，同时将成本降低高达50%。

    

    有效地在训练数据有限的情况下进行信息检索（IR），特别是对于复杂查询，仍然是一项具有挑战性的任务。本文介绍了IR2，即信息检索的信息正则化，一种用于在合成数据生成过程中减少过拟合的技术。该方法在具有复杂查询特征的三个最近的IR任务上进行了测试：DORIS-MAE、ArguAna和WhatsThatBook。实验结果表明，我们的正则化技术不仅在所考虑的任务上优于先前的合成查询生成方法，而且还能将成本降低高达50％。此外，本文将不同阶段的三种正则化方法——输入、提示和输出进行了分类和探索，每种方法相对于没有正则化的模型均提供了不同程度的性能改进。

    arXiv:2402.16200v1 Announce Type: cross  Abstract: Effective information retrieval (IR) in settings with limited training data, particularly for complex queries, remains a challenging task. This paper introduces IR2, Information Regularization for Information Retrieval, a technique for reducing overfitting during synthetic data generation. This approach, representing a novel application of regularization techniques in synthetic data creation for IR, is tested on three recent IR tasks characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook. Experimental results indicate that our regularization techniques not only outperform previous synthetic query generation methods on the tasks considered but also reduce cost by up to 50%. Furthermore, this paper categorizes and explores three regularization methods at different stages of the query synthesis pipeline-input, prompt, and output-each offering varying degrees of performance improvement compared to models where no regulariz
    
[^45]: 用于代码自动补全的语言模型：实践评估

    Language Models for Code Completion: A Practical Evaluation

    [https://arxiv.org/abs/2402.16197](https://arxiv.org/abs/2402.16197)

    这项研究对完成真实世界代码时的三种公共代码语言模型进行了定量和定性评估，在线和离线环境中进行了比较分析，为自动代码补全的语言模型评估提供了有益的发现。

    

    基于Transformer的语言模型在自动代码补全方面表现出很大的潜力，然而这些模型的评估很少使用真实数据。本研究对完成真实世界代码时的三种公共代码语言模型进行了定量和定性评估。我们首先开发了一个开源的IDE扩展工具，Code4Me，用于在线评估这些模型。我们从1200多名用户那里收集了一年多的真实自动补全使用数据，产生了超过60万个有效的完成结果。然后，我们使用六个标准指标对这些模型进行评估，涵盖了十二种编程语言。接下来，我们进行了一个定性研究，通过对1690个真实世界的完成请求进行分析，以确定模型表现不佳背后的原因。还对模型在在线和离线环境中的表现进行了比较分析，使用两种遮蔽策略和基准合成数据集。我们的研究结果表明，虽然

    arXiv:2402.16197v1 Announce Type: cross  Abstract: Transformer-based language models for automatic code completion have shown great promise so far, yet the evaluation of these models rarely uses real data. This study provides both quantitative and qualitative assessments of three public code language models when completing real-world code. We first developed an open-source IDE extension, Code4Me, for the online evaluation of the models. We collected real auto-completion usage data for over a year from more than 1200 users, resulting in over 600K valid completions. These models were then evaluated using six standard metrics across twelve programming languages. Next, we conducted a qualitative study of 1690 real-world completion requests to identify the reasons behind the poor model performance. A comparative analysis of the models' performance in online and offline settings was also performed, using benchmark synthetic datasets and two masking strategies. Our findings suggest that while
    
[^46]: 结合 OpenFOAM 和 SmartSim 的机器学习与计算流体动力学

    Combining Machine Learning with Computational Fluid Dynamics using OpenFOAM and SmartSim

    [https://arxiv.org/abs/2402.16196](https://arxiv.org/abs/2402.16196)

    使用OpenFOAM和SmartSim，我们提供了一个有效且可伸缩的解决方案来开发CFD+ML算法，通过SmartSim将OpenFOAM的不同部分有效地与ML耦合，包括预处理/后处理应用程序、求解器、函数对象和网格运动求解器。

    

    将机器学习（ML）与计算流体动力学（CFD）结合起来，为改进技术和自然系统的模拟打开了许多可能性。然而，CFD+ML算法需要在异构硬件上交换数据、同步和计算，使得它们在大规模问题上的实现异常具有挑战性。我们提供了一个有效且可伸缩的解决方案，使用开源软件OpenFOAM和SmartSim开发CFD+ML算法。SmartSim提供了一个编排器，大大简化了编程CFD+ML算法的过程，以及一个Redis数据库，确保ML和CFD客户端之间高度可伸缩的数据交换。我们展示了如何利用SmartSim将OpenFOAM的不同部分有效地与ML耦合，包括预处理/后处理应用程序、求解器、函数对象和网格运动求解器。此外，我们还提供了一个OpenFOAM子模块，其中包含可用作起始点的示例。

    arXiv:2402.16196v1 Announce Type: new  Abstract: Combining machine learning (ML) with computational fluid dynamics (CFD) opens many possibilities for improving simulations of technical and natural systems. However, CFD+ML algorithms require exchange of data, synchronization, and calculation on heterogeneous hardware, making their implementation for large-scale problems exceptionally challenging.   We provide an effective and scalable solution to developing CFD+ML algorithms using open source software OpenFOAM and SmartSim. SmartSim provides an Orchestrator that significantly simplifies the programming of CFD+ML algorithms and a Redis database that ensures highly scalable data exchange between ML and CFD clients. We show how to leverage SmartSim to effectively couple different segments of OpenFOAM with ML, including pre/post-processing applications, solvers, function objects, and mesh motion solvers. We additionally provide an OpenFOAM sub-module with examples that can be used as starti
    
[^47]: 利用其优势攻击LLM水印

    Attacking LLM Watermarks by Exploiting Their Strengths

    [https://arxiv.org/abs/2402.16187](https://arxiv.org/abs/2402.16187)

    现有的LLM水印系统虽然具有质量保留、鲁棒性和公开检测API等优点，但也因此容易受到各种攻击，研究者提出了一套实用指南以缓解这些攻击。

    

    生成模型的进展使得人工智能生成的文本、代码和图片能够在许多应用中模仿人类生成的内容。水印技术旨在将信息嵌入模型的输出中以验证其来源，对于减少对这些人工智能生成内容的滥用非常有用。然而，现有的水印方案仍然令人意外地容易受到攻击。具体而言，我们展示了现有的LLM水印系统共享的可取特性，例如质量保留、鲁棒性和公开检测API，反过来却使这些系统容易遭受各种攻击。我们在常见水印设计选择方面严格研究潜在攻击，并提出了缓解攻击的最佳实践和防御措施——建立了一套嵌入和检测LLM水印的实用指南。

    arXiv:2402.16187v1 Announce Type: cross  Abstract: Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating misuse of such AI-generated content. However, existing watermarking schemes remain surprisingly susceptible to attack. In particular, we show that desirable properties shared by existing LLM watermarking systems such as quality preservation, robustness, and public detection APIs can in turn make these systems vulnerable to various attacks. We rigorously study potential attacks in terms of common watermark design choices, and propose best practices and defenses for mitigation -- establishing a set of practical guidelines for embedding and detection of LLM watermarks.
    
[^48]: 用稀疏诱导激活初始化深度神经网络

    Deep Neural Network Initialization with Sparsity Inducing Activations

    [https://arxiv.org/abs/2402.16184](https://arxiv.org/abs/2402.16184)

    本文利用大宽高斯过程极限分析随机初始化时诱导隐藏输出稀疏行为的激活函数，克服了训练不稳定性。

    

    在训练和推理过程中诱导和利用稀疏激活是改善深度网络计算效率的一个有前途的途径，随着网络规模的不断增长和应用范围的扩大，这一点变得越来越重要。本文利用大宽高斯过程极限来分析非线性激活函数在随机初始化时诱导隐藏输出稀疏的行为。我们证明了先前未报道的训练不稳定形式，针对隐藏层稀疏化的两个最自然的候选者：移位ReLU（$\phi(x)=\max(0, x-\tau)$，其中$\tau\ge 0$）和软阈值（$\phi(x)=0$，当$|x|\le\tau$时，$x-\text{sign}(x)\tau$，当$|x|>\tau$时）。我们展示了这种不稳定性通过将非线性激活幅度修剪到由相关高斯过程方差图的形状规定的水平上被克服。数值实验证明

    arXiv:2402.16184v1 Announce Type: new  Abstract: Inducing and leveraging sparse activations during training and inference is a promising avenue for improving the computational efficiency of deep networks, which is increasingly important as network sizes continue to grow and their application becomes more widespread. Here we use the large width Gaussian process limit to analyze the behaviour, at random initialization, of nonlinear activations that induce sparsity in the hidden outputs. A previously unreported form of training instability is proven for arguably two of the most natural candidates for hidden layer sparsification; those being a shifted ReLU ($\phi(x)=\max(0, x-\tau)$ for $\tau\ge 0$) and soft thresholding ($\phi(x)=0$ for $|x|\le\tau$ and $x-\text{sign}(x)\tau$ for $|x|>\tau$). We show that this instability is overcome by clipping the nonlinear activation magnitude, at a level prescribed by the shape of the associated Gaussian process variance map. Numerical experiments ver
    
[^49]: LLM如何指导强化学习？一种基于价值的方法

    How Can LLM Guide RL? A Value-Based Approach

    [https://arxiv.org/abs/2402.16181](https://arxiv.org/abs/2402.16181)

    本文研究了如何利用大型语言模型（LLM）提供的策略先验来增强强化学习（RL）算法的样本效率。

    

    强化学习（RL）已经成为通过改进未来的行动策略来解决序贯决策问题的事实标准实践，但是RL算法可能需要大量的试错交互来收集有用的反馈以进行改进。与此同时，最近大型语言模型（LLMs）的发展展示了在语言理解和生成方面令人印象深刻的能力，然而它们在探索和自我改进规划任务的能力上仍存在不足，缺乏基于反馈自主改进响应的能力。因此，在本文中，我们研究了LLM提供的策略先验如何增强RL算法的样本效率。具体而言，我们开发了一种名为LINVIT的算法，该算法将LLM引导作为价值型RL中的正则化因子，可以显著减少学习所需的数据量，特别是当……

    arXiv:2402.16181v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has become the de facto standard practice for sequential decision-making problems by improving future acting policies with feedback. However, RL algorithms may require extensive trial-and-error interactions to collect useful feedback for improvement. On the other hand, recent developments in large language models (LLMs) have showcased impressive capabilities in language understanding and generation, yet they fall short in exploration and self-improvement capabilities for planning tasks, lacking the ability to autonomously refine their responses based on feedback. Therefore, in this paper, we study how the policy prior provided by the LLM can enhance the sample efficiency of RL algorithms. Specifically, we develop an algorithm named LINVIT that incorporates LLM guidance as a regularization factor in value-based RL, leading to significant reductions in the amount of data needed for learning, particularly when 
    
[^50]: 分布无关公平联邦学习与小样本

    Distribution-Free Fair Federated Learning with Small Samples

    [https://arxiv.org/abs/2402.16158](https://arxiv.org/abs/2402.16158)

    本文介绍了一种用于分布无关公平学习的后处理算法FedFaiREE，适用于去中心化具有小样本的环境。

    

    随着联邦学习在实际应用中变得越来越重要，因为它具有去中心化数据训练的能力，解决跨群体的公平性问题变得至关重要。然而，大多数现有的用于确保公平性的机器学习算法是为集中化数据环境设计的，通常需要大样本和分布假设，强调了迫切需要针对具有有限样本和分布无关保证的去中心化和异构系统进行公平性技术的调整。为了解决这个问题，本文介绍了FedFaiREE，这是一种专门用于去中心化环境中小样本的分布无关公平学习的后处理算法。我们的方法考虑到了去中心化环境中的独特挑战，例如客户异质性、通信成本和小样本大小。我们为bot提供严格的理论保证

    arXiv:2402.16158v1 Announce Type: cross  Abstract: As federated learning gains increasing importance in real-world applications due to its capacity for decentralized data training, addressing fairness concerns across demographic groups becomes critically important. However, most existing machine learning algorithms for ensuring fairness are designed for centralized data environments and generally require large-sample and distributional assumptions, underscoring the urgent need for fairness techniques adapted for decentralized and heterogeneous systems with finite-sample and distribution-free guarantees. To address this issue, this paper introduces FedFaiREE, a post-processing algorithm developed specifically for distribution-free fair learning in decentralized settings with small samples. Our approach accounts for unique challenges in decentralized environments, such as client heterogeneity, communication costs, and small sample sizes. We provide rigorous theoretical guarantees for bot
    
[^51]: 共识学习：一种全新的分散式集成学习范式

    Consensus learning: A novel decentralised ensemble learning paradigm

    [https://arxiv.org/abs/2402.16157](https://arxiv.org/abs/2402.16157)

    引入共识学习这一全新的分布式机器学习范式，结合经典集成方法与共识协议，在保证用户数据隐私的同时抵御拜占庭攻击，具有高效性和可扩展性。

    

    最近大规模机器学习模型的广泛采用突显了分布式计算在效率和可扩展性上的需求。本文引入了一种新颖的分布式机器学习范式--共识学习--将经典集成方法与部署在对等系统中的共识协议相结合。这些算法包括两个阶段：首先，参与者开发他们的模型并为任何新数据输入提交预测；其次，个体预测被用作通信阶段的输入，该阶段由共识协议控制。共识学习确保用户数据隐私，同时也继承了基础共识机制对拜占庭攻击的安全措施。我们为特定的共识协议提供了详细的理论分析，并将共识学习集成的性能与中心化集成学习算法进行了比较。

    arXiv:2402.16157v1 Announce Type: new  Abstract: The widespread adoption of large-scale machine learning models in recent years highlights the need for distributed computing for efficiency and scalability. This work introduces a novel distributed machine learning paradigm -- \emph{consensus learning} -- which combines classical ensemble methods with consensus protocols deployed in peer-to-peer systems. These algorithms consist of two phases: first, participants develop their models and submit predictions for any new data inputs; second, the individual predictions are used as inputs for a communication phase, which is governed by a consensus protocol. Consensus learning ensures user data privacy, while also inheriting the safety measures against Byzantine attacks from the underlying consensus mechanism. We provide a detailed theoretical analysis for a particular consensus protocol and compare the performance of the consensus learning ensemble with centralised ensemble learning algorithm
    
[^52]: ChatMusician：理解和生成具有LLM的音乐内在

    ChatMusician: Understanding and Generating Music Intrinsically with LLM

    [https://arxiv.org/abs/2402.16153](https://arxiv.org/abs/2402.16153)

    ChatMusician 是一个集成了内在音乐能力的开源LLM，通过对文本兼容的音乐表示法进行持续预训练和微调，能够理解和生成音乐，表现优于GPT-4基准模型。

    

    虽然大型语言模型（LLMs）在文本生成方面展现出令人印象深刻的能力，但我们发现它们的能力尚未推广到音乐，也就是人类的创造性语言。我们介绍了ChatMusician，这是一个开源的LLM，集成了内在的音乐能力。它基于对文本兼容的音乐表示法ABC记谱的持续预训练和微调LLaMA2，并且将音乐视为第二语言。ChatMusician可以使用纯文本标记器理解和生成音乐，而无需任何外部多模态神经结构或标记器。有趣的是，赋予音乐能力并不会损害语言能力，甚至可以达到略高的MMLU分数。我们的模型能够根据文本、和弦、旋律、主题、音乐形式等创作结构良好、完整长度的音乐，超越了GPT-4的基线。在我们精心策划的大学级音乐理解基准上，MusicTheory

    arXiv:2402.16153v1 Announce Type: cross  Abstract: While Large Language Models (LLMs) demonstrate impressive capabilities in text generation, we find that their ability has yet to be generalized to music, humanity's creative language. We introduce ChatMusician, an open-source LLM that integrates intrinsic musical abilities. It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music representation, ABC notation, and the music is treated as a second language. ChatMusician can understand and generate music with a pure text tokenizer without any external multi-modal neural structures or tokenizers. Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher MMLU score. Our model is capable of composing well-structured, full-length music, conditioned on texts, chords, melodies, motifs, musical forms, etc, surpassing GPT-4 baseline. On our meticulously curated college-level music understanding benchmark, MusicTheory
    
[^53]: 基于VAE的多层神经Granger-因果连接学习框架

    A VAE-based Framework for Learning Multi-Level Neural Granger-Causal Connectivity

    [https://arxiv.org/abs/2402.16131](https://arxiv.org/abs/2402.16131)

    该论文提出了一种基于VAE的框架，可联合学习一组相关但异构动态系统中的Granger因果关系，并以原则性方式处理提取共享结构和识别个体特性的任务。

    

    Granger因果关系在各种应用领域中被广泛使用，用于捕捉复杂动态系统中组件之间的先导-滞后关系，现有文献的重点集中在单一动态系统上。在宏观经济学和神经科学的某些应用中，人们可以访问来自一组相关系统的数据，在这些系统中，感兴趣的建模任务是提取嵌入这些系统中的共享公共结构，以及识别各自系统中的特质。本文介绍了一种基于变分自动编码器（VAE）的框架，它联合学习了一组相关但异构动态系统中的Granger因果关系，并以原则性方式处理上述任务。所提出的框架在几种合成数据设置上进行了评估，并与为单个动态系统设计的现有方法进行了基准测试。

    arXiv:2402.16131v1 Announce Type: new  Abstract: Granger causality has been widely used in various application domains to capture lead-lag relationships amongst the components of complex dynamical systems, and the focus in extant literature has been on a single dynamical system. In certain applications in macroeconomics and neuroscience, one has access to data from a collection of related such systems, wherein the modeling task of interest is to extract the shared common structure that is embedded across them, as well as to identify the idiosyncrasies within individual ones. This paper introduces a Variational Autoencoder (VAE) based framework that jointly learns Granger-causal relationships amongst components in a collection of related-yet-heterogeneous dynamical systems, and handles the aforementioned task in a principled way. The performance of the proposed framework is evaluated on several synthetic data settings and benchmarked against existing approaches designed for individual s
    
[^54]: InstructEdit：针对大型语言模型的基于指令的知识编辑

    InstructEdit: Instruction-based Knowledge Editing for Large Language Models

    [https://arxiv.org/abs/2402.16123](https://arxiv.org/abs/2402.16123)

    InstructEdit提出了一种基于指令的知识编辑技术，通过简单指令使编辑器适应不同任务的表现，显著提高了多任务编辑中的可靠性。

    

    大型语言模型的知识编辑可以提供一种有效的解决方案，以改变模型的行为而不会对整体性能产生消极影响。然而，当前的方法在跨任务的通用性方面存在问题，需要为每个任务设计一个独特的编辑器，这显著阻碍了更广泛的应用。为了解决这一问题，我们首先分析了知识编辑中的多任务泛化问题。具体地，我们开发了一种基于指令的编辑技术，称为InstructEdit，通过简单的指令促进编辑器同时适应各种任务的表现。通过为每个LLM只使用一个统一的编辑器，我们在实证方面表明，InstructEdit可以提高编辑器的控制能力，从而在多任务编辑设置中平均提高可靠性14.86%。此外，涉及保留未见任务的实验说明，InstructEdi

    arXiv:2402.16123v1 Announce Type: cross  Abstract: Knowledge editing for large language models can offer an efficient solution to alter a model's behavior without negatively impacting the overall performance. However, the current approach encounters issues with limited generalizability across tasks, necessitating one distinct editor for each task, which significantly hinders the broader applications. To address this, we take the first step to analyze the multi-task generalization issue in knowledge editing. Specifically, we develop an instruction-based editing technique, termed InstructEdit, which facilitates the editor's adaptation to various task performances simultaneously using simple instructions. With only one unified editor for each LLM, we empirically demonstrate that InstructEdit can improve the editor's control, leading to an average 14.86% increase in Reliability in multi-task editing setting. Furthermore, experiments involving holdout unseen task illustrate that InstructEdi
    
[^55]: 利用模型预测控制的深度锻造：在金属成形中利用人工智能实现微观组织控制

    DeepForge: Leveraging AI for Microstructural Control in Metal Forming via Model Predictive Control

    [https://arxiv.org/abs/2402.16119](https://arxiv.org/abs/2402.16119)

    该研究提出了一种结合模型预测控制和名为DeepForge的机器学习模型的方法，用于在闭模热锻造过程中通过使用表面温度测量预测微观组织的变化。研究结果显示，DeepForge能够以极低的平均绝对误差预测微观组织，并探索了使用MPC调整等待时间来实现在特定工件区域内达到目标晶粒尺寸的可能性。

    

    这项研究提出了一种新颖的方法，用于在闭模热锻造中实现微观组织控制，将模型预测控制（MPC）与一种名为DeepForge的开发的机器学习模型相结合。DeepForge使用结合了1D卷积神经网络和门控循环单元的架构。它使用工件表面温度测量作为输入，以预测锻造过程中的微观组织变化。论文还详细介绍了DeepForge的架构以及用于生成数据集的有限元模拟模型，使用了三行程锻造工艺。研究结果表明，DeepForge能够以0.4±0.3%的平均绝对误差预测微观组织。此外，该研究探讨了使用MPC调整行程之间的等待时间，有效地抵消温度扰动，实现在工件的特定2D区域内将晶粒尺寸控制在35微米以下的目标。然后，这些结果经过验证。

    arXiv:2402.16119v1 Announce Type: new  Abstract: This study presents a novel method for microstructure control in closed die hot forging that combines Model Predictive Control (MPC) with a developed machine learning model called DeepForge. DeepForge uses an architecture that combines 1D convolutional neural networks and gated recurrent units. It uses surface temperature measurements of a workpiece as input to predict microstructure changes during forging. The paper also details DeepForge's architecture and the finite element simulation model used to generate the data set, using a three-stroke forging process. The results demonstrate DeepForge's ability to predict microstructure with a mean absolute error of 0.4$\pm$0.3%. In addition, the study explores the use of MPC to adjust inter-stroke wait times, effectively counteracting temperature disturbances to achieve a target grain size of less than 35 microns within a specific 2D region of the workpiece. These results are then verified exp
    
[^56]: 通知元学习

    Informed Meta-Learning

    [https://arxiv.org/abs/2402.16105](https://arxiv.org/abs/2402.16105)

    该研究提出了通知元学习这一新范式，旨在通过人类和机器之间的跨任务知识共享，提高数据效率和抵御观测噪声。

    

    在真实应用中盛行的嘈杂和低数据情况下，机器学习中一个突出的挑战在于有效地融合促进数据效率和稳健性的归纳偏差。元学习和通知机器学习是两种将先验知识纳入机器学习流程的方法。前者依赖于一种纯数据驱动的先验来源，而后者受专家知识的形式化表示引导。本文介绍了一种新颖的混合范式，通知元学习，旨在实现人类和机器之间跨任务知识共享的互补性。我们建立了通知元学习的基本组成部分，并提出了这一框架的具体实例--通知神经过程。通过一系列说明性和更大规模的实验，我们展示了通知元学习在提高数据效率和抵御观测噪声方面的潜在优势。

    arXiv:2402.16105v1 Announce Type: new  Abstract: In noisy and low-data regimes prevalent in real-world applications, an outstanding challenge of machine learning lies in effectively incorporating inductive biases that promote data efficiency and robustness. Meta-learning and informed ML stand out as two approaches for incorporating prior knowledge into the ML pipeline. While the former relies on a purely data-driven source of priors, the latter is guided by a formal representation of expert knowledge. This paper introduces a novel hybrid paradigm, informed meta-learning, seeking complementarity in cross-task knowledge sharing of humans and machines. We establish the foundational components of informed meta-learning and present a concrete instantiation of this framework--the Informed Neural Process. Through a series of illustrative and larger-scale experiments, we demonstrate the potential benefits of informed meta-learning in improving data efficiency and robustness to observational no
    
[^57]: 个性化联邦学习参数选择的贝叶斯神经网络

    Bayesian Neural Network For Personalized Federated Learning Parameter Selection

    [https://arxiv.org/abs/2402.16091](https://arxiv.org/abs/2402.16091)

    通过引入贝叶斯神经网络，本研究提出在元素级别而非传统的层级上进行个性化，以选择个性化参数。

    

    联邦学习在存在异构数据时性能不佳仍然是该领域最为紧迫的问题之一。个性化联邦学习偏离传统范式，其中所有客户端使用相同模型，而是努力为每个客户端发现一个个性化模型，以解决数据中的异质性。一种方法涉及个性化神经网络的特定层。然而，先前的努力并没有提供可靠的理由，有些选择了完全不同且相互冲突的个性化层。在这项工作中，我们更进一步，提议在元素级别进行个性化，而不是传统的层级个性化。为了选择个性化参数，我们引入了贝叶斯神经网络，并依赖它们提供的不确定性来指导我们选择个性化参数。

    arXiv:2402.16091v1 Announce Type: cross  Abstract: Federated learning's poor performance in the presence of heterogeneous data remains one of the most pressing issues in the field. Personalized federated learning departs from the conventional paradigm in which all clients employ the same model, instead striving to discover an individualized model for each client to address the heterogeneity in the data. One of such approach involves personalizing specific layers of neural networks. However, prior endeavors have not provided a dependable rationale, and some have selected personalized layers that are entirely distinct and conflicting. In this work, we take a step further by proposing personalization at the elemental level, rather than the traditional layer-level personalization. To select personalized parameters, we introduce Bayesian neural networks and rely on the uncertainty they offer to guide our selection of personalized parameters. Finally, we validate our algorithm's efficacy on 
    
[^58]: 无源无监督领域自适应中的关键设计选择：深入实证分析

    Key Design Choices in Source-Free Unsupervised Domain Adaptation: An In-depth Empirical Analysis

    [https://arxiv.org/abs/2402.16090](https://arxiv.org/abs/2402.16090)

    无源无监督领域自适应中的关键设计选择研究了多种技术，评估了它们在数据集上的表现、对超参数的敏感性，以及在不同主干架构上的适用性，强调了主干架构和预训练数据集选择对性能的重要性

    

    本研究为图像分类中的无源无监督领域自适应（SF-UDA）提供了全面的基准框架，旨在通过严谨的实证理解SF-UDA方法中多个关键设计因素之间复杂关系。研究从实证角度检验了多种SF-UDA技术，评估其在数据集上的一致性、对特定超参数的敏感性以及在不同主干架构系列上的适用性。此外，它全面评估了预训练数据集和策略，特别关注监督和自监督方法，以及微调对源领域的影响。我们的分析还强调了现有基准实践中的差距，引导SF-UDA研究朝着更有效和通用的方法发展。它强调了主干架构和预训练数据集选择对SF-UDA性能的重要性。

    arXiv:2402.16090v1 Announce Type: cross  Abstract: This study provides a comprehensive benchmark framework for Source-Free Unsupervised Domain Adaptation (SF-UDA) in image classification, aiming to achieve a rigorous empirical understanding of the complex relationships between multiple key design factors in SF-UDA methods. The study empirically examines a diverse set of SF-UDA techniques, assessing their consistency across datasets, sensitivity to specific hyperparameters, and applicability across different families of backbone architectures. Moreover, it exhaustively evaluates pre-training datasets and strategies, particularly focusing on both supervised and self-supervised methods, as well as the impact of fine-tuning on the source domain. Our analysis also highlights gaps in existing benchmark practices, guiding SF-UDA research towards more effective and general approaches. It emphasizes the importance of backbone architecture and pre-training dataset selection on SF-UDA performance
    
[^59]: 超越时空表示：演化Fourier变换用于时间图

    Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs

    [https://arxiv.org/abs/2402.16078](https://arxiv.org/abs/2402.16078)

    这里是中文总结出的一句话要点：提出了Evolution Graph Fourier Transform(EFT)，首次实现在时间图上捕捉演化表示的可逆谱变换，通过优化连续时间动态图的拉普拉斯，同时提出了伪谱松弛来高效计算转换过程。

    

    我们提出了Evolving Graph Fourier Transform (EFT)，这是第一个捕捉时间图演变表示的可逆谱变换。我们通过现有方法无法捕捉演变图谱的不足来激发我们的工作，由于时间因素以及图顶点域的计算成本较高。我们将问题视为对连续时间动态图的拉普拉斯进行优化。此外，我们提出了伪谱松弛，使变换过程分解，从而使其高度计算有效。EFT方法熟练地捕捉了演变图的结构和位置属性，使其可以有效用于演变图上的下游任务。因此，作为参考实施，我们开发了一个简单的神经模型，用EFT来捕捉演变图谱。我们在大规模数据集上通过实验证实了我们的理论发现。

    arXiv:2402.16078v1 Announce Type: new  Abstract: We present the Evolving Graph Fourier Transform (EFT), the first invertible spectral transform that captures evolving representations on temporal graphs. We motivate our work by the inadequacy of existing methods for capturing the evolving graph spectra, which are also computationally expensive due to the temporal aspect along with the graph vertex domain. We view the problem as an optimization over the Laplacian of the continuous time dynamic graph. Additionally, we propose pseudo-spectrum relaxations that decompose the transformation process, making it highly computationally efficient. The EFT method adeptly captures the evolving graph's structural and positional properties, making it effective for downstream tasks on evolving graphs. Hence, as a reference implementation, we develop a simple neural model induced with EFT for capturing evolving graph spectra. We empirically validate our theoretical findings on a number of large-scale an
    
[^60]: 等变框架与连续规范化的不可能性

    Equivariant Frames and the Impossibility of Continuous Canonicalization

    [https://arxiv.org/abs/2402.16077](https://arxiv.org/abs/2402.16077)

    对于常用的群，研究揭示出没有有效的可计算的框架选择能够保持被平均函数的连续性，但本研究提出了加权框架这一解决方案。

    

    规范化提供了一种与架构无关的方法来强制保持等变性，近期广受关注的泛化方法如框架平均化成为一种轻量且灵活的等变架构替代方案。 最近的研究发现，使用概率框架能够带来实证效益，这些框架学习群元素上的加权分布。 本文提供了关于这一现象的强有力理论证据：对于常用的群，没有有效的可计算的框架选择能够保持被平均函数的连续性。 换句话说，非加权的框架平均可以将一个平滑的、非对称的函数转变为一个不连续、对称的函数。 为了解决这一基本的鲁棒性问题，我们正式定义并构建了\emph{加权}框架，据证明能够保持连续性，并通过构建高效连续的加权框架展示了它们的实用性。

    arXiv:2402.16077v1 Announce Type: new  Abstract: Canonicalization provides an architecture-agnostic method for enforcing equivariance, with generalizations such as frame-averaging recently gaining prominence as a lightweight and flexible alternative to equivariant architectures. Recent works have found an empirical benefit to using probabilistic frames instead, which learn weighted distributions over group elements. In this work, we provide strong theoretical justification for this phenomenon: for commonly-used groups, there is no efficiently computable choice of frame that preserves continuity of the function being averaged. In other words, unweighted frame-averaging can turn a smooth, non-symmetric function into a discontinuous, symmetric function. To address this fundamental robustness problem, we formally define and construct \emph{weighted} frames, which provably preserve continuity, and demonstrate their utility by constructing efficient and continuous weighted frames for the act
    
[^61]: 基于插值的策略扩散的行为细化

    Behavioral Refinement via Interpolant-based Policy Diffusion

    [https://arxiv.org/abs/2402.16075](https://arxiv.org/abs/2402.16075)

    使用比高斯更具信息量的源头启动扩散方法有助于克服模仿学习任务中的限制。

    

    模仿学习使人工智能代理通过从演示中学习来模仿行为。最近，拥有建模高维度和多模态分布能力的扩散模型在模仿学习任务上表现出色。这些模型通过将动作（或状态）从标准高斯噪声中扩散来塑造策略。然而，要学习的目标策略通常与高斯分布显著不同，这种不匹配可能导致在使用少量扩散步骤（以提高推理速度）和有限数据下性能不佳。这项工作的关键思想是，从比高斯更具信息量的源头开始，可以使扩散方法克服上述限制。我们提供了理论结果、一种新方法和实证发现，展示了使用信息量丰富的源策略的好处。我们的方法，称为BRIDGER，利用了随机性。

    arXiv:2402.16075v1 Announce Type: cross  Abstract: Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to overcome the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGER, leverages the stochast
    
[^62]: 使用预计算的嵌入相似性生成几乎实时个性化信息流

    Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities

    [https://arxiv.org/abs/2402.16073](https://arxiv.org/abs/2402.16073)

    使用预计算的嵌入相似性生成个性化信息流，提高了电子商务平台上的客户参与度和体验，转化率提升4.9％。

    

    在个性化推荐系统中，通常使用嵌入来编码用户动作和项目，然后在嵌入空间中进行检索，使用近似最近邻搜索。然而，这种方法可能会导致两个挑战：1）用户嵌入可能限制所捕获的兴趣多样性，2）保持它们最新需要昂贵的实时基础设施。本文提出了一种在实际工业环境中克服这些挑战的方法。该方法动态更新客户配置文件，并每两分钟组成一个信息流，利用预计算的嵌入及其各自的相似性。我们在荷兰和比利时最大的电子商务平台之一Bol上测试并部署了这种方法，该方法提高了客户参与度和体验，导致转化率显著提高了4.9％。

    arXiv:2402.16073v1 Announce Type: cross  Abstract: In personalized recommender systems, embeddings are often used to encode customer actions and items, and retrieval is then performed in the embedding space using approximate nearest neighbor search. However, this approach can lead to two challenges: 1) user embeddings can restrict the diversity of interests captured and 2) the need to keep them up-to-date requires an expensive, real-time infrastructure. In this paper, we propose a method that overcomes these challenges in a practical, industrial setting. The method dynamically updates customer profiles and composes a feed every two minutes, employing precomputed embeddings and their respective similarities. We tested and deployed this method to personalise promotional items at Bol, one of the largest e-commerce platforms of the Netherlands and Belgium. The method enhanced customer engagement and experience, leading to a significant 4.9% uplift in conversions.
    
[^63]: 通过将标记映射到共享字符空间来训练双语语言模型

    Training a Bilingual Language Model by Mapping Tokens onto a Shared Character Space

    [https://arxiv.org/abs/2402.16065](https://arxiv.org/abs/2402.16065)

    通过将标记映射到共享字符空间，研究了阿拉伯-希伯来双语语言模型训练。结果表明，使用同时表示两种语言的统一脚本的语言模型在机器翻译上表现出色，相比于保持原有脚本的模型有着更好的性能表现。

    

    我们使用阿拉伯文文本的音译版本在希伯来语中训练了一个双语语言模型，以确保两种语言在同一脚本中表示。鉴于阿拉伯语和希伯来语之间的形态学、结构相似性以及大量共同词源词，我们评估了使用统一脚本表示两种语言的语言模型在需要跨语言知识的机器翻译上的表现。结果表明：我们的模型优于保持阿拉伯文本在阿拉伯脚本中的对比模型，展示了音译步骤的有效性。尽管我们的模型在数据集方面训练集大小约为其他现有语言模型的60％，但在机器翻译的两个方向上似乎提供了可比较的性能。

    arXiv:2402.16065v1 Announce Type: new  Abstract: We train a bilingual Arabic-Hebrew language model using a transliterated version of Arabic texts in Hebrew, to ensure both languages are represented in the same script. Given the morphological, structural similarities, and the extensive number of cognates shared among Arabic and Hebrew, we assess the performance of a language model that employs a unified script for both languages, on machine translation which requires cross-lingual knowledge. The results are promising: our model outperforms a contrasting model which keeps the Arabic texts in the Arabic script, demonstrating the efficacy of the transliteration step. Despite being trained on a dataset approximately 60% smaller than that of other existing language models, our model appears to deliver comparable performance in machine translation across both translation directions.
    
[^64]: 梯度增强深高斯过程用于多保真建模

    Gradient-enhanced deep Gaussian processes for multifidelity modelling

    [https://arxiv.org/abs/2402.16059](https://arxiv.org/abs/2402.16059)

    这项工作将深高斯过程扩展到包含梯度数据，用于多保真建模，能够捕获不同保真数据之间的非线性和输入相关关系。

    

    多保真模型整合来自多个来源的数据，生成底层过程的单一逼近器。密集的低保真样本用于减少插值误差，稀疏的高保真样本用于弥补低保真样本中的偏差或噪音。梯度增强的深高斯过程对多保真建模具有吸引力，因为它们是非参数的，不容易过拟合，在小数据集上表现良好，并且关键是能够捕获不同保真数据之间非线性和输入相关的关系。许多数据集自然包含梯度数据，特别是当它们由与自动微分兼容或具有共轭解的计算模型生成时。本工作主要是将深高斯过程扩展为包含梯度数据。我们在一个分析测试问题和一个现实的偏微分方程问题上演示了这种方法，我们在这两个问题中进行了气动预测

    arXiv:2402.16059v1 Announce Type: cross  Abstract: Multifidelity models integrate data from multiple sources to produce a single approximator for the underlying process. Dense low-fidelity samples are used to reduce interpolation error, while sparse high-fidelity samples are used to compensate for bias or noise in the low-fidelity samples. Deep Gaussian processes (GPs) are attractive for multifidelity modelling as they are non-parametric, robust to overfitting, perform well for small datasets, and, critically, can capture nonlinear and input-dependent relationships between data of different fidelities. Many datasets naturally contain gradient data, especially when they are generated by computational models that are compatible with automatic differentiation or have adjoint solutions. Principally, this work extends deep GPs to incorporate gradient data. We demonstrate this method on an analytical test problem and a realistic partial differential equation problem, where we predict the aer
    
[^65]: LLMs带有思维链条是非因果推理者

    LLMs with Chain-of-Thought Are Non-Causal Reasoners

    [https://arxiv.org/abs/2402.16048](https://arxiv.org/abs/2402.16048)

    本文探讨了大型语言模型在推理过程中思维链条（CoT）的作用，发现LLMs在答案生成过程中与人类推理存在差异，相关因素包括语境学习、有监督微调以及对人类反馈的强化学习。

    

    本文探讨了大型语言模型（LLMs）推理中思维链条（CoT）的作用。尽管它有改善任务性能的潜力，但我们的分析揭示了在LLMs中正确答案跟随不正确CoTs的频率及反之。我们采用因果分析来评估CoTs/指令与LLMs答案之间的因果关系，揭示LLMs近似的结构因果模型（SCM）。通过比较暗示SCM与人类推理的SCM，我们突显了LLM和人类推理过程之间的差异。我们进一步研究了影响暗示SCM因果结构的因素，揭示了语境学习、有监督微调以及对人类反馈的强化学习显著影响因果关系。我们在https://github.com/StevenZHB/CoT_Causal_Analysis发布了代码和结果。

    arXiv:2402.16048v1 Announce Type: cross  Abstract: This paper explores the role of the Chain of Thought (CoT) in Large Language Models (LLMs) reasoning. Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa. We employ causal analysis to assess the cause-effect relationship between CoTs/instructions and answers in LLMs, uncovering the Structural Causal Model (SCM) that LLMs approximate. By comparing the implied SCM with that of human reasoning, we highlight discrepancies between LLM and human reasoning processes. We further examine the factors influencing the causal structure of the implied SCM, revealing that in-context learning, supervised fine-tuning, and reinforcement learning on human feedback significantly impact the causal relations. We release the code and results at https://github.com/StevenZHB/CoT_Causal_Analysis.
    
[^66]: 通过多种群意识优化检测机器生成文本的最大均值离差

    Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy

    [https://arxiv.org/abs/2402.16041](https://arxiv.org/abs/2402.16041)

    通过多种群意识优化检测机器生成文本的最大均值离差，解决了机器生成文本与人工编写文本之间微妙的分布差异挑战。

    

    大型语言模型（LLMs）如ChatGPT在生成类人文本方面表现出色。然而，机器生成文本可能存在严重风险，如抄袭问题、误导性信息或幻觉问题。因此，在许多情况下，检测机器生成文本是非常紧迫和重要的。不幸的是，由于LLMs的出色表现，区分机器生成文本和人工编写文本之间的分布差异常常非常微妙，这是具有挑战性的。在这篇论文中，我们试图利用\textit{最大均值离差}（MMD）来解决这个问题，因为MMD可以很好地识别分布差异。然而，直接使用各种机器生成文本对MMD进行训练将导致MMD的方差显著增加，因为不同LLMs的机器生成文本可能包含\textit{多个文本群体}。这将严重损害MMD测量分布差异的能力。

    arXiv:2402.16041v1 Announce Type: new  Abstract: Large language models (LLMs) such as ChatGPT have exhibited remarkable performance in generating human-like texts. However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues, misleading information, or hallucination issues. Therefore, it is very urgent and important to detect MGTs in many situations. Unfortunately, it is challenging to distinguish MGTs and human-written texts because the distributional discrepancy between them is often very subtle due to the remarkable performance of LLMs. In this paper, we seek to exploit \textit{maximum mean discrepancy} (MMD) to address this issue in the sense that MMD can well identify distributional discrepancies. However, directly training a detector with MMD using diverse MGTs will incur a significantly increased variance of MMD since MGTs may contain \textit{multiple text populations} due to various LLMs. This will severely impair MMD's ability to measure the diff
    
[^67]: 深度学习方法用于改进肝细胞癌研究中的问答系统

    Deep Learning Approaches for Improving Question Answering Systems in Hepatocellular Carcinoma Research

    [https://arxiv.org/abs/2402.16038](https://arxiv.org/abs/2402.16038)

    深度学习技术在问答系统领域取得的成就，尤其是在肝细胞癌研究中，极大地推动了自然语言处理的发展。

    

    近年来，自然语言处理（NLP）领域的进展受益于深度学习技术的发展，特别是通过利用诸如GPU和TPU等强大的计算资源。像BERT和GPT-3这样的模型，在大量数据的训练下，彻底改变了语言理解和生成。这些预训练模型为各种任务提供了坚实的基础，包括语义理解、智能写作和推理，为更通用的人工智能铺平了道路。作为人工智能的一个重要应用，NLP旨在通过自然语言交互来弥合人与计算机之间的差距。本文深入探讨了基于大规模模型的NLP的当前格局和未来展望，重点放在这一领域内的问答系统上。分析了人工智能驱动的问答系统的实际案例和发展，以促进进一步的探索。

    arXiv:2402.16038v1 Announce Type: cross  Abstract: In recent years, advancements in natural language processing (NLP) have been fueled by deep learning techniques, particularly through the utilization of powerful computing resources like GPUs and TPUs. Models such as BERT and GPT-3, trained on vast amounts of data, have revolutionized language understanding and generation. These pre-trained models serve as robust bases for various tasks including semantic understanding, intelligent writing, and reasoning, paving the way for a more generalized form of artificial intelligence. NLP, as a vital application of AI, aims to bridge the gap between humans and computers through natural language interaction. This paper delves into the current landscape and future prospects of large-scale model-based NLP, focusing on the question-answering systems within this domain. Practical cases and developments in artificial intelligence-driven question-answering systems are analyzed to foster further explora
    
[^68]: 基于机器学习的自动驾驶车辆意图轨迹识别和预测

    Machine Learning-Based Vehicle Intention Trajectory Recognition and Prediction for Autonomous Driving

    [https://arxiv.org/abs/2402.16036](https://arxiv.org/abs/2402.16036)

    该论文主要解决了自动驾驶技术中关于车辆意图轨迹识别和预测的挑战。

    

    近年来，互联网技术的扩张和自动化的进步引起了自动驾驶技术的广泛关注。包括沃尔沃、奔驰和特斯拉在内的主要汽车制造商逐步推出了从辅助驾驶车辆到半自动驾驶车辆的产品。然而，这一时期也发生了几起涉及自动驾驶车辆的交通安全事故。例如，2016年3月，一辆谷歌自动驾驶汽车与一辆公交车发生了轻微碰撞。事故发生时，自动驾驶汽车试图并入右车道，但未能动态响应车道变更过程中的实时环境信息。它错误地假设靠近的公交车会减速避让，导致与公交车发生低速碰撞。这一事件凸显出当前技术的不足和安全顾虑。

    arXiv:2402.16036v1 Announce Type: cross  Abstract: In recent years, the expansion of internet technology and advancements in automation have brought significant attention to autonomous driving technology. Major automobile manufacturers, including Volvo, Mercedes-Benz, and Tesla, have progressively introduced products ranging from assisted-driving vehicles to semi-autonomous vehicles. However, this period has also witnessed several traffic safety incidents involving self-driving vehicles. For instance, in March 2016, a Google self-driving car was involved in a minor collision with a bus. At the time of the accident, the autonomous vehicle was attempting to merge into the right lane but failed to dynamically respond to the real-time environmental information during the lane change. It incorrectly assumed that the approaching bus would slow down to avoid it, leading to a low-speed collision with the bus. This incident highlights the current technological shortcomings and safety concerns a
    
[^69]: 基于正交约束和多边形面积的特征选择方法

    Feature Selection Based on Orthogonal Constraints and Polygon Area

    [https://arxiv.org/abs/2402.16026](https://arxiv.org/abs/2402.16026)

    该研究提出了一种结合多边形面积的新型正交回归模型，用于捕捉特征与标签之间的区分性依赖关系，并采用混合非单调线性搜索方法处理正交约束带来的非凸优化挑战，实验证明该方法在降维和提升分类性能方面优于传统方法。

    

    特征选择的目标是通过评估每个特征的重要性，为识别任务选择最佳特征子集，从而实现有效的降维。目前，现有的特征选择方法常常忽视特征与标签之间的区分性依赖关系。为了解决这一问题，本文引入了一种结合多边形面积的新型正交回归模型。该模型能直观捕捉特征与标签之间的区分性依赖关系。此外，本文采用混合非单调线性搜索方法来高效处理正交约束带来的非凸优化挑战。实验结果表明，我们的方法不仅有效捕捉区分性依赖信息，而且在降低特征维度和提升分类性能方面超越传统方法。

    arXiv:2402.16026v1 Announce Type: new  Abstract: The goal of feature selection is to choose the optimal subset of features for a recognition task by evaluating the importance of each feature, thereby achieving effective dimensionality reduction. Currently, proposed feature selection methods often overlook the discriminative dependencies between features and labels. To address this problem, this paper introduces a novel orthogonal regression model incorporating the area of a polygon. The model can intuitively capture the discriminative dependencies between features and labels. Additionally, this paper employs a hybrid non-monotone linear search method to efficiently tackle the non-convex optimization challenge posed by orthogonal constraints. Experimental results demonstrate that our approach not only effectively captures discriminative dependency information but also surpasses traditional methods in reducing feature dimensions and enhancing classification performance.
    
[^70]: HiGPT：异质图语言模型

    HiGPT: Heterogeneous Graph Language Model

    [https://arxiv.org/abs/2402.16024](https://arxiv.org/abs/2402.16024)

    该论文提出了HiGPT模型，致力于解决异质图学习中存在的泛化限制和分布不稳定性问题。

    

    异构图学习旨在捕捉异构图中实体之间的复杂关系和多样化关系语义，以获得节点和边的有意义表示。最近在异构图神经网络（HGNNs）领域取得了最先进的性能，通过考虑关系的异质性并使用专门的消息函数和聚合规则。然而，现有的异构图学习框架在泛化到不同的异构图数据集方面存在局限。大多数这些框架都遵循同一数据集上的“预训练”和“微调”范式，这限制了它们适应新的和看不见的数据的能力。这引出了一个问题：“我们是否能够将异质图模型泛化为适应具有节点令牌集和关系类型异质性分布变化的不同下游学习任务？”为了解决这些挑战，我们p

    arXiv:2402.16024v1 Announce Type: new  Abstract: Heterogeneous graph learning aims to capture complex relationships and diverse relational semantics among entities in a heterogeneous graph to obtain meaningful representations for nodes and edges. Recent advancements in heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art performance by considering relation heterogeneity and using specialized message functions and aggregation rules. However, existing frameworks for heterogeneous graph learning have limitations in generalizing across diverse heterogeneous graph datasets. Most of these frameworks follow the "pre-train" and "fine-tune" paradigm on the same dataset, which restricts their capacity to adapt to new and unseen data. This raises the question: "Can we generalize heterogeneous graph models to be well-adapted to diverse downstream learning tasks with distribution shifts in both node token sets and relation type heterogeneity?'' To tackle those challenges, we p
    
[^71]: 自动微分实现的分步介绍

    A Step-by-step Introduction to the Implementation of Automatic Differentiation

    [https://arxiv.org/abs/2402.16020](https://arxiv.org/abs/2402.16020)

    本文通过逐步介绍实现简单自动微分系统，填补了教学中的空白，并简化了数学概念和实现。

    

    自动微分是深度学习中的关键组成部分。这个主题得到了深入研究，优秀的调查（如Baydin等，2018年）已经可以清晰地描述基本概念。此外，自动微分的复杂实现现在是流行的深度学习框架的重要部分。然而，直接教授学生现有系统的实现是困难的，如果不是不可能的，因为它太复杂了。另一方面，如果教学止步于基本概念，学生将无法感受到实现的实现。例如，我们在教学自动微分时经常提及计算图，但学生们想知道如何实现和使用它。在本文中，我们通过逐步介绍实现简单自动微分系统来填补部分空白。我们简化了数学概念和实现。此外，我们还提供了

    arXiv:2402.16020v1 Announce Type: new  Abstract: Automatic differentiation is a key component in deep learning. This topic is well studied and excellent surveys such as Baydin et al. (2018) have been available to clearly describe the basic concepts. Further, sophisticated implementations of automatic differentiation are now an important part of popular deep learning frameworks. However, it is difficult, if not impossible, to directly teach students the implementation of existing systems due to the complexity. On the other hand, if the teaching stops at the basic concept, students fail to sense the realization of an implementation. For example, we often mention the computational graph in teaching automatic differentiation, but students wonder how to implement and use it. In this document, we partially fill the gap by giving a step by step introduction of implementing a simple automatic differentiation system. We streamline the mathematical concepts and the implementation. Further, we gi
    
[^72]: 隐式线性层的频谱提取和裁剪

    Spectrum Extraction and Clipping for Implicitly Linear Layers

    [https://arxiv.org/abs/2402.16017](https://arxiv.org/abs/2402.16017)

    展示自动微分在计算和控制隐式线性算子频谱中的有效性；提供第一个适用于一般卷积层的裁剪方法；研究了批归一化层与卷积层组合的效果；通过比较算法与最先进方法的精度和性能表明更精确和高效。

    

    我们展示了自动微分在高效准确计算和控制隐式线性算子的频谱上的有效性，这是一类包括所有标准卷积和全连接层的丰富层类型。我们提供了第一个适用于一般卷积层的裁剪方法，并阐明了导致之前工作中正确性问题的表示限制。我们研究了批归一化层与卷积层串联时的效果，并展示了我们的裁剪方法如何应用于它们的组合。通过对我们的算法与最先进方法的精度和性能进行比较，我们使用各种实验表明它们更精确和高效，能够实现更好的泛化和对抗鲁棒性。我们提供了使用我们方法的代码链接https://github.com/Ali-E/FastClip。

    arXiv:2402.16017v1 Announce Type: new  Abstract: We show the effectiveness of automatic differentiation in efficiently and correctly computing and controlling the spectrum of implicitly linear operators, a rich family of layer types including all standard convolutional and dense layers. We provide the first clipping method which is correct for general convolution layers, and illuminate the representational limitation that caused correctness issues in prior work. We study the effect of the batch normalization layers when concatenated with convolutional layers and show how our clipping method can be applied to their composition. By comparing the accuracy and performance of our algorithms to the state-of-the-art methods, using various experiments, we show they are more precise and efficient and lead to better generalization and adversarial robustness. We provide the code for using our methods at https://github.com/Ali-E/FastClip.
    
[^73]: 在科学计算规模上构建灵活的机器学习模型

    Building Flexible Machine Learning Models for Scientific Computing at Scale

    [https://arxiv.org/abs/2402.16014](https://arxiv.org/abs/2402.16014)

    OmniArch通过多物理学时空数据处理、可扩展的自回归任务和物理信息增强学习技术，在科学计算领域构建灵活的基础模型，并在性能、适应性和逆问题求解方面取得突破，展现了AI对科学计算的潜力。

    

    arXiv:2402.16014v1

    arXiv:2402.16014v1 Announce Type: cross  Abstract: Foundation models have revolutionized knowledge acquisition across domains, and our study introduces OmniArch, a paradigm-shifting approach designed for building foundation models in multi-physics scientific computing. OmniArch's pre-training involves a versatile pipeline that processes multi-physics spatio-temporal data, casting forward problem learning into scalable auto-regressive tasks, while our novel Physics-Informed Reinforcement Learning (PIRL) technique during fine-tuning ensures alignment with physical laws. Pre-trained on the comprehensive PDEBench dataset, OmniArch not only sets new performance benchmarks for 1D, 2D and 3D PDEs but also demonstrates exceptional adaptability to new physics via few-shot and zero-shot learning approaches. The model's representations further extend to inverse problem-solving, highlighting the transformative potential of AI-enabled Scientific Computing(AI4SC) foundation models for engineering ap
    
[^74]: 具有面向聚类的引导的深度对比图学习

    Deep Contrastive Graph Learning with Clustering-Oriented Guidance

    [https://arxiv.org/abs/2402.16012](https://arxiv.org/abs/2402.16012)

    该论文提出了一种深度对比图学习（DCGL）模型，通过结合自编码器和GCN，在处理一般数据聚类时强调了图结构和原始特征。

    

    图卷积网络（GCN）在改善基于图的聚类中展现出了显著潜力。为了处理没有先验图的一般聚类场景，这些模型先估计一个初始图，然后应用GCN。文献中显示，大多数模型关注于初始图而忽略了原始特征。因此，学到的表示的可辨识性可能会受到低质量初始图的破坏；训练过程缺乏有效的聚类引导，这可能导致将与聚类无关的信息合并到学到的图中。为了解决这些问题，提出了用于一般数据聚类的深度对比图学习（DCGL）模型。具体来说，我们建立了一个伪孪生网络，将自编码器与GCN相结合，以强调图结构和原始特征。基于此，特征级对比。

    arXiv:2402.16012v1 Announce Type: new  Abstract: Graph Convolutional Network (GCN) has exhibited remarkable potential in improving graph-based clustering. To handle the general clustering scenario without a prior graph, these models estimate an initial graph beforehand to apply GCN. Throughout the literature, we have witnessed that 1) most models focus on the initial graph while neglecting the original features. Therefore, the discriminability of the learned representation may be corrupted by a low-quality initial graph; 2) the training procedure lacks effective clustering guidance, which may lead to the incorporation of clustering-irrelevant information into the learned graph. To tackle these problems, the Deep Contrastive Graph Learning (DCGL) model is proposed for general data clustering. Specifically, we establish a pseudo-siamese network, which incorporates auto-encoder with GCN to emphasize both the graph structure and the original features. On this basis, feature-level contrasti
    
[^75]: 通过掩盖输入梯度揭示痴呆症检测：一种用于模型可解释性和精确性的JSM方法

    Unmasking Dementia Detection by Masking Input Gradients: A JSM Approach to Model Interpretability and Precision

    [https://arxiv.org/abs/2402.16008](https://arxiv.org/abs/2402.16008)

    通过Jacobson显著性地图（JSM）方法，本文提出了一种创新的模型调试方法，用于揭示深度学习模型中可能存在的偏见和不合理的认知，以提高其在医学领域（以阿尔茨海默病诊断为例）的可解释性和精确性。

    

    深度学习和人工智能的发展显著改变了技术格局。然而，在医学等关键领域中有效应用它们需要更多，不仅要有出色的性能，还要有可信赖性。虽然可解释性起着关键作用，但现有的可解释人工智能（XAI）方法经常无法揭示模型以数据中的偏见或偶然关联做出（不能泛化的）正确预测的“机智汉斯”行为。同样，当前的事后XAI方法容易生成不合理的反事实示例。在本文中，我们采用一种创新的“模型调试”方法来处理XAI，该方法通过雅各比显著性图（JSM）实现。为了使问题具体化，我们选择阿尔茨海默病（AD）诊断作为用例，这源于其对人类生活的重大影响以及对早期检测的巨大挑战。

    arXiv:2402.16008v1 Announce Type: cross  Abstract: The evolution of deep learning and artificial intelligence has significantly reshaped technological landscapes. However, their effective application in crucial sectors such as medicine demands more than just superior performance, but trustworthiness as well. While interpretability plays a pivotal role, existing explainable AI (XAI) approaches often do not reveal {\em Clever Hans} behavior where a model makes (ungeneralizable) correct predictions using spurious correlations or biases in data. Likewise, current post-hoc XAI methods are susceptible to generating unjustified counterfactual examples. In this paper, we approach XAI with an innovative {\em model debugging} methodology realized through Jacobian Saliency Map (JSM). To cast the problem into a concrete context, we employ Alzheimer's disease (AD) diagnosis as the use case, motivated by its significant impact on human lives and the formidable challenge in its early detection, stemm
    
[^76]: 通过领域同化实现医学图像的敌对稳健迁移学习

    Adversarial-Robust Transfer Learning for Medical Imaging via Domain Assimilation

    [https://arxiv.org/abs/2402.16005](https://arxiv.org/abs/2402.16005)

    该论文着力于解决医学图像领域中机器学习模型的敌对攻击问题与训练数据稀缺问题，并利用领域同化的方法来增强模型的稳健性。

    

    在医学图像领域，人们致力于利用其潜力来揭示患者的关键诊断特征。人工智能驱动的医学诊断依赖于复杂的机器学习和深度学习模型，用于分析、检测和识别医学图像中的疾病。尽管这些模型表现出高准确性的显著性能，但它们仍然面临着信任问题。对原始图像引入微小扰动使对手能够操纵预测输出，将其重定向到其他目标或非目标类别。此外，公开可用的医学图像稀缺，构成可靠训练的瓶颈，这导致当代算法依赖于基于大量自然图像的预训练模型 -- 一种称为迁移学习的实践。

    arXiv:2402.16005v1 Announce Type: cross  Abstract: In the field of Medical Imaging, extensive research has been dedicated to leveraging its potential in uncovering critical diagnostic features in patients. Artificial Intelligence (AI)-driven medical diagnosis relies on sophisticated machine learning and deep learning models to analyze, detect, and identify diseases from medical images. Despite the remarkable performance of these models, characterized by high accuracy, they grapple with trustworthiness issues. The introduction of a subtle perturbation to the original image empowers adversaries to manipulate the prediction output, redirecting it to other targeted or untargeted classes. Furthermore, the scarcity of publicly available medical images, constituting a bottleneck for reliable training, has led contemporary algorithms to depend on pretrained models grounded on a large set of natural images -- a practice referred to as transfer learning. However, a significant {\em domain discre
    
[^77]: 后量子密码神经网络

    Post-Quantum Cryptography Neural Network

    [https://arxiv.org/abs/2402.16002](https://arxiv.org/abs/2402.16002)

    提出了一种将基于编码的后量子密码方法映射到神经网络结构的PQC方法，通过非线性激活函数、随机扰动的密文和密文的均匀分布增强密文安全性。

    

    近年来，量子计算机和Shor量子算法对当前主流的非对称加密方法（例如RSA和椭圆曲线加密（ECC））构成威胁。因此，有必要构建一种后量子密码（PQC）方法来抵抗量子计算攻击。本研究提出了一种基于PQC的神经网络，将基于编码的PQC方法映射到神经网络结构，并通过非线性激活函数、随机扰动的密文以及密文的均匀分布增强了密文的安全性。在实际实验中，本研究以蜂窝网络信号为案例研究，证明了建议的基于PQC的神经网络可以通过密文的均匀分布进行加密和解密。未来，这种提出的基于PQC的神经网络可以应用于各种应用中。

    arXiv:2402.16002v1 Announce Type: cross  Abstract: In recent years, quantum computers and Shor quantum algorithm have posed a threat to current mainstream asymmetric cryptography methods (e.g. RSA and Elliptic Curve Cryptography (ECC)). Therefore, it is necessary to construct a Post-Quantum Cryptography (PQC) method to resist quantum computing attacks. Therefore, this study proposes a PQC-based neural network that maps a code-based PQC method to a neural network structure and enhances the security of ciphertexts with non-linear activation functions, random perturbation of ciphertexts, and uniform distribution of ciphertexts. In practical experiments, this study uses cellular network signals as a case study to demonstrate that encryption and decryption can be performed by the proposed PQC-based neural network with the uniform distribution of ciphertexts. In the future, the proposed PQC-based neural network could be applied to various applications.
    
[^78]: 通过基于现场主动偏好学习设计顺序配色方案的Cieran

    Cieran: Designing Sequential Colormaps via In-Situ Active Preference Learning

    [https://arxiv.org/abs/2402.15997](https://arxiv.org/abs/2402.15997)

    Cieran是一个允许数据分析师在Jupyter笔记本中设计图表时快速找到质量配色方案的工具，通过主动偏好学习范式进行排序和创建新的配色方案，帮助新手分析师定制配色方案以适应其数据背景。

    

    优质的配色方案可以帮助传达重要的数据模式。然而，要为特定情景找到看起来“恰到好处”的美观配色方案，需要相当多的设计和技术专业知识。我们引入了Cieran，这是一个工具，允许任何数据分析师在设计Jupyter笔记本中的图表时迅速找到优质的配色方案。我们的系统采用了一种主动的偏好学习范式，通过两两比较来对专家设计的配色方案进行排序，允许在色彩设计方面是新手的分析师根据其数据背景定制配色方案。我们通过将配色方案的设计视为在CIELAB颜色空间中的路径规划问题，并使用上下文特定的奖励模型来实现这一目标。在与十二名科学家的评估中，我们发现Cieran有效地模拟了用户的偏好来排名配色方案，并利用这一模型创建了新的优质设计。我们的工作展示了主动偏好学习的潜力。

    arXiv:2402.15997v1 Announce Type: cross  Abstract: Quality colormaps can help communicate important data patterns. However, finding an aesthetically pleasing colormap that looks "just right" for a given scenario requires significant design and technical expertise. We introduce Cieran, a tool that allows any data analyst to rapidly find quality colormaps while designing charts within Jupyter Notebooks. Our system employs an active preference learning paradigm to rank expert-designed colormaps and create new ones from pairwise comparisons, allowing analysts who are novices in color design to tailor colormaps to their data context. We accomplish this by treating colormap design as a path planning problem through the CIELAB colorspace with a context-specific reward model. In an evaluation with twelve scientists, we found that Cieran effectively modeled user preferences to rank colormaps and leveraged this model to create new quality designs. Our work shows the potential of active preferenc
    
[^79]: 改进学习半空间交集的困难性结果

    Improved Hardness Results for Learning Intersections of Halfspaces

    [https://arxiv.org/abs/2402.15995](https://arxiv.org/abs/2402.15995)

    我们通过展示学习在维度N中的$\omega(\log \log N)$个半空间甚至需要超多项式时间的标准假设，显著缩小了这一差距

    

    我们展示了在不正确设置中学习半空间交集的弱学习下界，这些下界非常强大（并且令人惊讶地简单）。关于这个问题知之甚少。例如，甚至不知道是否存在一个多项式时间算法来学习仅两个半空间的交集。另一方面，基于良好建立的假设（如近似最坏情况的格问题或Feige的3SAT假设的变体）的下界仅对超对数个半空间的交集已知（或者由已有结果暗示）。

    arXiv:2402.15995v1 Announce Type: cross  Abstract: We show strong (and surprisingly simple) lower bounds for weakly learning intersections of halfspaces in the improper setting. Strikingly little is known about this problem. For instance, it is not even known if there is a polynomial-time algorithm for learning the intersection of only two halfspaces. On the other hand, lower bounds based on well-established assumptions (such as approximating worst-case lattice problems or variants of Feige's 3SAT hypothesis) are only known (or are implied by existing results) for the intersection of super-logarithmically many halfspaces [KS09,KS06,DSS16]. With intersections of fewer halfspaces being only ruled out under less standard assumptions [DV21] (such as the existence of local pseudo-random generators with large stretch). We significantly narrow this gap by showing that even learning $\omega(\log \log N)$ halfspaces in dimension $N$ takes super-polynomial time under standard assumptions on wors
    
[^80]: 利用深度学习优化数字资产的投资组合管理和风险评估

    Optimizing Portfolio Management and Risk Assessment in Digital Assets Using Deep Learning for Predictive Analysis

    [https://arxiv.org/abs/2402.15994](https://arxiv.org/abs/2402.15994)

    通过将DQN算法引入资产管理投资组合中，本研究证明了DRL算法在投资组合管理中的有效性

    

    近年来，投资组合管理问题已在人工智能领域得到广泛研究，但现有基于深度学习的量化交易方法仍存在一些可以改进的方面。本文将DQN算法以一种全新而直接的方式引入资产管理投资组合中，性能大大超过基准，充分证明了DRL算法在投资组合管理中的有效性。

    arXiv:2402.15994v1 Announce Type: cross  Abstract: Portfolio management issues have been extensively studied in the field of artificial intelligence in recent years, but existing deep learning-based quantitative trading methods have some areas where they could be improved. First of all, the prediction mode of stocks is singular; often, only one trading expert is trained by a model, and the trading decision is solely based on the prediction results of the model. Secondly, the data source used by the model is relatively simple, and only considers the data of the stock itself, ignoring the impact of the whole market risk on the stock. In this paper, the DQN algorithm is introduced into asset management portfolios in a novel and straightforward way, and the performance greatly exceeds the benchmark, which fully proves the effectiveness of the DRL algorithm in portfolio management. This also inspires us to consider the complexity of financial problems, and the use of algorithms should be fu
    
[^81]: 使用平衡截断技术学习带有对角状态空间层的S4模型的方法

    Learning method for S4 with Diagonal State Space Layers using Balanced Truncation

    [https://arxiv.org/abs/2402.15993](https://arxiv.org/abs/2402.15993)

    一种用于处理长序列数据的边缘智能应用的S4模型的学习方法，利用平衡截断技术降低计算成本，并通过改进初始化过程和优化准确度和效率指标来超越传统训练模型。

    

    我们引入了一种新颖的学习方法，用于结构化状态空间序列（S4）模型并且加入了对角状态空间（DSS）层，这种方法专门设计用于处理边缘智能应用中的长序列数据，包括传感器数据分析和实时分析。该方法利用平衡截断技术，在控制理论中很常见，特别应用于DSS层以降低推断过程中的计算成本。通过利用减少模型的参数，我们改进了S4模型的初始化过程，在性能方面优于广泛使用的Skew-HiPPo初始化。数值实验表明，我们训练的带有DSS层的S4模型在准确度和效率指标上超越了传统训练的模型。此外，我们的观察结果显示一个积极的相关性：原始模型中的更高准确度一致导致使用我们方法训练的模型的准确度增加，这表明

    arXiv:2402.15993v1 Announce Type: new  Abstract: We introduce a novel learning method for Structured State Space Sequence (S4) models incorporating Diagonal State Space (DSS) layers, tailored for processing long-sequence data in edge intelligence applications, including sensor data analysis and real-time analytics. This method utilizes the balanced truncation technique, prevalent in control theory, applied specifically to DSS layers to reduce computational costs during inference. By leveraging parameters from the reduced model, we refine the initialization process of S4 models, outperforming the widely used Skew-HiPPo initialization in terms of performance. Numerical experiments demonstrate that our trained S4 models with DSS layers surpass conventionally trained models in accuracy and efficiency metrics. Furthermore, our observations reveal a positive correlation: higher accuracy in the original model consistently leads to increased accuracy in models trained using our method, suggest
    
[^82]: 从多个推文参数中检测客户满意度的机器学习方法

    A Machine Learning Approach to Detect Customer Satisfaction From Multiple Tweet Parameters

    [https://arxiv.org/abs/2402.15992](https://arxiv.org/abs/2402.15992)

    使用机器学习方法分析推文以确定客户满意水平，有助于简化研究成千上万条推文并改进航空公司服务的繁琐过程。

    

    自互联网技术得以发展以来，客户满意度已成为企业发展的主要因素之一。在线平台已成为分享评论的主要场所之一。Twitter是其中之一，客户经常在该平台上发布他们的想法。在这些平台上对航班的评论已成为航空公司关注的焦点。积极的评论可以帮助公司增长，而消极的评论则可能迅速破坏其收入和声誉。因此，对航空公司来说，审查客户的反馈和体验、改进其服务以保持竞争力至关重要。然而，研究成千上万条推文并分析它们以确定客户满意度是一项相当困难的任务。利用机器学习方法分析推文以确定客户满意水平可以简化这一费时的过程。已经有一些关于这种策略的工作可自动化该过程使用机器学

    arXiv:2402.15992v1 Announce Type: cross  Abstract: Since internet technologies have advanced, one of the primary factors in company development is customer happiness. Online platforms have become prominent places for sharing reviews. Twitter is one of these platforms where customers frequently post their thoughts. Reviews of flights on these platforms have become a concern for the airline business. A positive review can help the company grow, while a negative one can quickly ruin its revenue and reputation. So it's vital for airline businesses to examine the feedback and experiences of their customers and enhance their services to remain competitive. But studying thousands of tweets and analyzing them to find the satisfaction of the customer is quite a difficult task. This tedious process can be made easier by using a machine learning approach to analyze tweets to determine client satisfaction levels. Some work has already been done on this strategy to automate the procedure using mach
    
[^83]: 朝向公平的图异常检测：问题、新数据集和评估

    Towards Fair Graph Anomaly Detection: Problem, New Datasets, and Evaluation

    [https://arxiv.org/abs/2402.15988](https://arxiv.org/abs/2402.15988)

    这项研究提出了公平图异常检测（FairGAD）问题，介绍了来自Reddit和Twitter的两个新图数据集，填补了当前文献在此问题上的空白。

    

    公平图异常检测（FairGAD）问题旨在在输入图中准确检测异常节点，同时确保公平性，避免针对来自敏感亚组如性别或政治倾向的个体的偏见预测。图中的公平性在异常检测领域尤为关键，比如在搜索/排名系统中进行的误信息检测，决策结果可能会极大地影响个人。然而，当前文献没有全面讨论这个问题，也没有提供贴近实际图结构、异常标签和敏感属性的数据集，以用于FairGAD研究。为弥补这一不足，我们介绍了FairGAD问题的正式定义，并提出了两个新颖的图数据集，这些数据集来自于全球知名社交媒体平台Reddit和Twitter。

    arXiv:2402.15988v1 Announce Type: cross  Abstract: The Fair Graph Anomaly Detection (FairGAD) problem aims to accurately detect anomalous nodes in an input graph while ensuring fairness and avoiding biased predictions against individuals from sensitive subgroups such as gender or political leanings. Fairness in graphs is particularly crucial in anomaly detection areas such as misinformation detection in search/ranking systems, where decision outcomes can significantly affect individuals. However, the current literature does not comprehensively discuss this problem, nor does it provide realistic datasets that encompass actual graph structures, anomaly labels, and sensitive attributes for research in FairGAD. To bridge this gap, we introduce a formal definition of the FairGAD problem and present two novel graph datasets constructed from the globally prominent social media platforms Reddit and Twitter. These datasets comprise 1.2 million and 400,000 edges associated with 9,000 and 47,000 
    
[^84]: 使用HuBERT进行对犬语言的语音和词汇发现

    Phonetic and Lexical Discovery of a Canine Language using HuBERT

    [https://arxiv.org/abs/2402.15985](https://arxiv.org/abs/2402.15985)

    使用HuBERT实现了对犬叫声的声素标签分类和词汇识别，发现了具有显著声学一致性的犬词汇，还开发了一个Web系统标记狗叫声中的声素n-gram。

    

    本文深入探讨了在犬叫声中潜在沟通模式的开创性探索，并超越了传统的语言分析障碍，大量依赖于人类先验知识和有限数据集来发现狗的叫声中的声音单元。我们提出了一种自监督方法，利用HuBERT实现了声素标签的准确分类，并识别了暗示犬叫声中一种基础词汇的声音模式。我们的研究结果表明，这些确定的犬词汇中存在显著的声学一致性，覆盖了所有观察到的犬叫声序列。我们进一步开发了一个基于Web的犬叫声标记系统。该系统可以在用户上传的狗叫声中突出显示词汇中存在的声素n-gram。

    arXiv:2402.15985v1 Announce Type: cross  Abstract: This paper delves into the pioneering exploration of potential communication patterns within dog vocalizations and transcends traditional linguistic analysis barriers, which heavily relies on human priori knowledge on limited datasets to find sound units in dog vocalization. We present a self-supervised approach with HuBERT, enabling the accurate classification of phoneme labels and the identification of vocal patterns that suggest a rudimentary vocabulary within dog vocalizations. Our findings indicate a significant acoustic consistency in these identified canine vocabulary, covering the entirety of observed dog vocalization sequences. We further develop a web-based dog vocalization labeling system. This system can highlight phoneme n-grams, present in the vocabulary, in the dog audio uploaded by users.
    
[^85]: 用统一的傅里叶切片方法导出一种适用于多种深度-2神经网络的尖峰变换

    A unified Fourier slice method to derive ridgelet transform for a variety of depth-2 neural networks

    [https://arxiv.org/abs/2402.15984](https://arxiv.org/abs/2402.15984)

    通过使用傅里叶表达式导出尖峰变换，实现了对各种现代神经网络的描述和分析。

    

    研究神经网络参数时，研究参数分布比研究每个神经元的参数更容易。尖峰变换是一个伪逆算子，将给定函数 $f$ 映射到参数分布 $\gamma$，使得网络 $\mathtt{NN}[\gamma]$ 能够重现 $f$，即 $\mathtt{NN}[\gamma]=f$。在欧氏空间上的深度-2全连接网络中，已发现了尖峰变换的闭合形式表达式，因此我们可以描述参数的分布。然而，对于多种现代神经网络架构，尚不知道闭合形式表达式。本文介绍了一种使用傅里叶表达式的系统方法，用于推导各种现代网络的尖峰变换，例如有限域 $\mathbb{F}_p$ 上的网络、抽象希尔伯特空间 $\mathcal{H}$ 上的群卷积网络，以及非紧致对称的全连接网络。

    arXiv:2402.15984v1 Announce Type: new  Abstract: To investigate neural network parameters, it is easier to study the distribution of parameters than to study the parameters in each neuron. The ridgelet transform is a pseudo-inverse operator that maps a given function $f$ to the parameter distribution $\gamma$ so that a network $\mathtt{NN}[\gamma]$ reproduces $f$, i.e. $\mathtt{NN}[\gamma]=f$. For depth-2 fully-connected networks on a Euclidean space, the ridgelet transform has been discovered up to the closed-form expression, thus we could describe how the parameters are distributed. However, for a variety of modern neural network architectures, the closed-form expression has not been known. In this paper, we explain a systematic method using Fourier expressions to derive ridgelet transforms for a variety of modern networks such as networks on finite fields $\mathbb{F}_p$, group convolutional networks on abstract Hilbert space $\mathcal{H}$, fully-connected networks on noncompact symm
    
[^86]: 使用奥卡姆剃刀削减权重：使用边缘似然的贝叶斯稀疏化神经网络

    Shaving Weights with Occam's Razor: Bayesian Sparsification for Neural Networks Using the Marginal Likelihood

    [https://arxiv.org/abs/2402.15978](https://arxiv.org/abs/2402.15978)

    提出了一种基于边缘似然的贝叶斯稀疏化神经网络的方法，通过有效利用贝叶斯边缘似然和稀疏诱导先验，使神经网络更易稀疏化，并采用自动奥卡姆剃刀选择最适合的模型，以实现高效的权重削减。

    

    神经网络稀疏化是一个有前途的途径，可以节省计算时间和内存成本，特别是在许多成功的人工智能模型变得过大以至无法直接部署在消费类硬件的时代。虽然很多工作都集中在不同的权重剪枝准则上，但网络的总体稀疏性，即可以在不损失质量的情况下剪枝的能力，经常被忽视。我们提出了通过边缘似然量（Marginal likelihood）的稀疏性（SpaM），一个稀疏化框架，重点强调使用贝叶斯边缘似然与稀疏诱导先验相结合，使神经网络更易稀疏化的有效性。我们的方法实现了一个自动的奥卡姆剃刀，选择最想要削减的模型，以依然能够很好地解释数据，无论是对于结构化还是非结构化的稀疏化。此外，我们展示了拉普拉斯近似中使用的预计算后验黑塞近似的效果。

    arXiv:2402.15978v1 Announce Type: new  Abstract: Neural network sparsification is a promising avenue to save computational time and memory costs, especially in an age where many successful AI models are becoming too large to na\"ively deploy on consumer hardware. While much work has focused on different weight pruning criteria, the overall sparsifiability of the network, i.e., its capacity to be pruned without quality loss, has often been overlooked. We present Sparsifiability via the Marginal likelihood (SpaM), a pruning framework that highlights the effectiveness of using the Bayesian marginal likelihood in conjunction with sparsity-inducing priors for making neural networks more sparsifiable. Our approach implements an automatic Occam's razor that selects the most sparsifiable model that still explains the data well, both for structured and unstructured sparsification. In addition, we demonstrate that the pre-computed posterior Hessian approximation used in the Laplace approximation
    
[^87]: 结构知识驱动的元学习用于车联网中集成通信、感知和计算的任务卸载

    Structural Knowledge-Driven Meta-Learning for Task Offloading in Vehicular Networks with Integrated Communications, Sensing and Computing

    [https://arxiv.org/abs/2402.15972](https://arxiv.org/abs/2402.15972)

    研究了一种基于集成通信、感知和计算的任务卸载范式，旨在优化计算模式和网络资源，降低资源消耗成本，并保证任务的延迟。

    

    任务卸载是满足车载应用的严格计算需求和对延迟敏感的潜在解决方案，因为车载计算资源有限。本文提出了一种新颖的任务卸载范式，即集成通信、感知和计算（I-CSC），通过路侧单元（RSU）感知的环境数据直接用于计算，车辆可以选择将感知数据上传到RSU或在卸载过程中向RSU发送计算指令。通过优化计算模式和网络资源，本文研究了基于I-CSC的任务卸载问题，以减少资源消耗带来的成本，同时保证每个任务的延迟。

    arXiv:2402.15972v1 Announce Type: new  Abstract: Task offloading is a potential solution to satisfy the strict requirements of computation-intensive and latency-sensitive vehicular applications due to the limited onboard computing resources. However, the overwhelming upload traffic may lead to unacceptable uploading time. To tackle this issue, for tasks taking environmental data as input, the data perceived by roadside units (RSU) equipped with several sensors can be directly exploited for computation, resulting in a novel task offloading paradigm with integrated communications, sensing and computing (I-CSC). With this paradigm, vehicles can select to upload their sensed data to RSUs or transmit computing instructions to RSUs during the offloading. By optimizing the computation mode and network resources, in this paper, we investigate an I-CSC-based task offloading problem to reduce the cost caused by resource consumption while guaranteeing the latency of each task. Although this non-c
    
[^88]: CoDream：使用异构模型交换梦想而不是模型进行联合聚合

    CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models

    [https://arxiv.org/abs/2402.15968](https://arxiv.org/abs/2402.15968)

    CoDream提出了一种通过在输入数据空间中协作优化数据来交换知识的框架，实现了模型之间的合作学习，实现了模型架构无关、通信不受模型大小影响、兼容安全聚合的优点。

    

    联邦学习（FL）通过聚合模型参数实现机器学习模型在分散数据上的协作优化。我们的方法通过聚合模型产生的“知识”，而不是模型参数来扩展这一概念。我们提出了一个名为 \codream 的新颖框架，在这个框架中，客户端通过在输入数据空间中使用联合优化来协作优化随机初始化的数据，类似于在FL中优化随机初始化的模型参数。我们的关键见解是，联合优化这些数据可以有效捕获全局数据分布的特性。在数据空间共享知识具有许多好处：（1）与模型无关的协作学习，即不同客户端可以具有不同的模型架构；（2）通信不受模型大小影响，消除了模型参数的可伸缩性问题；（3）与安全聚合兼容，因此可预

    arXiv:2402.15968v1 Announce Type: cross  Abstract: Federated Learning (FL) enables collaborative optimization of machine learning models across decentralized data by aggregating model parameters. Our approach extends this concept by aggregating "knowledge" derived from models, instead of model parameters. We present a novel framework called \codream, where clients collaboratively optimize randomly initialized data using federated optimization in the input data space, similar to how randomly initialized model parameters are optimized in FL. Our key insight is that jointly optimizing this data can effectively capture the properties of the global data distribution. Sharing knowledge in data space offers numerous benefits: (1) model-agnostic collaborative learning, i.e., different clients can have different model architectures; (2) communication that is independent of the model size, eliminating scalability concerns with model parameters; (3) compatibility with secure aggregation, thus pre
    
[^89]: 使用机器学习构建的层次化能量特征对汽车制造过程进行操作可见性和诊断

    Hierarchical energy signatures using machine learning for operational visibility and diagnostics in automotive manufacturing

    [https://arxiv.org/abs/2402.15962](https://arxiv.org/abs/2402.15962)

    使用层次化机器学习方法对汽车制造过程的能量消耗数据进行分析，可实现更好的操作可见性和识别节能机会。

    

    制造业能源消耗数据包含了操作可见性和诊断所需的重要过程特征。这些特征可能具有不同的时间尺度，从月度到亚秒级分辨率不等。我们引入了一种层次化的机器学习方法，以识别汽车制造过程的特征从油漆车间电力消耗数据中，涵盖不同时间尺度（周度和日度）。多层感知器（MLP）、卷积神经网络（CNN）以及主成分分析（PCA）结合逻辑回归（LR）用于分析。我们通过专业领域专家验证了所开发算法的实用性，以实现更好的操作可见性和识别节能机会。

    arXiv:2402.15962v1 Announce Type: new  Abstract: Manufacturing energy consumption data contains important process signatures required for operational visibility and diagnostics. These signatures may be of different temporal scales, ranging from monthly to sub-second resolutions. We introduce a hierarchical machine learning approach to identify automotive process signatures from paint shop electricity consumption data at varying temporal scales (weekly and daily). A Multi-Layer Perceptron (MLP), a Convolutional Neural Network (CNN), and Principal Component Analysis (PCA) combined with Logistic Regression (LR) are used for the analysis. We validate the utility of the developed algorithms with subject matter experts for (i) better operational visibility, and (ii) identifying energy saving opportunities.
    
[^90]: 论三层神经网络动力学：初始凝聚

    On the dynamics of three-layer neural networks: initial condensation

    [https://arxiv.org/abs/2402.15958](https://arxiv.org/abs/2402.15958)

    深入研究三层神经网络训练中的凝聚现象和梯度下降方法自发减少神经网络复杂性的机制，提出有效动力学的爆炸性质和凝聚发生的充分条件，并通过实验证实了这些发现。

    

    经验和理论研究显示，当初始化为小值时，两层神经网络的输入权重会收敛到孤立的方向。这种现象被称为凝聚，表明梯度下降方法在训练过程中往往会自发地减少神经网络的复杂性。 在这项研究中，我们阐明了三层神经网络训练中出现的凝聚现象背后的机制，并将其与两层神经网络的训练进行区分。 通过严格的理论分析，我们建立了有效动力学的爆炸性质，并提出了发生凝聚的充分条件，这些发现得到了实验结果的证实。此外，我们还探讨了凝聚与深度矩阵因子分解中观察到的低秩偏差之间的关联。

    arXiv:2402.15958v1 Announce Type: new  Abstract: Empirical and theoretical works show that the input weights of two-layer neural networks, when initialized with small values, converge towards isolated orientations. This phenomenon, referred to as condensation, indicates that the gradient descent methods tend to spontaneously reduce the complexity of neural networks during the training process. In this work, we elucidate the mechanisms behind the condensation phenomena occurring in the training of three-layer neural networks and distinguish it from the training of two-layer neural networks. Through rigorous theoretical analysis, we establish the blow-up property of effective dynamics and present a sufficient condition for the occurrence of condensation, findings that are substantiated by experimental results. Additionally, we explore the association between condensation and the low-rank bias observed in deep matrix factorization.
    
[^91]: DynaMITE-RL: 一种动态模型用于改进时间元元强化学习

    DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement Learning

    [https://arxiv.org/abs/2402.15957](https://arxiv.org/abs/2402.15957)

    DynaMITE-RL 提出了一种动态模型元强化学习方法，通过一致性的潜在信息、会话掩码和先验潜在条件等关键修改，在各种领域中实现了比最先进基线更优异的样本效率和推理结果。

    

    我们引入了DynaMITE-RL，这是一种元强化学习(meta-RL)方法，用于在潜在状态以不同速率演变的环境中进行近似推理。我们建模剧集会话 - 剧集的部分，在这些部分中，潜在状态是固定的 - 并对现有的meta-RL方法提出了三个关键修改：剧集内部潜在信息的一致性，会话掩码和先验潜在条件。我们在各种领域展示了这些修改的重要性，从离散Gridworld环境到连续控制和模拟机器人辅助任务，证明了DynaMITE-RL在样本效率和推理结果方面显着优于最先进的基线。

    arXiv:2402.15957v1 Announce Type: new  Abstract: We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach to approximate inference in environments where the latent state evolves at varying rates. We model episode sessions - parts of the episode where the latent state is fixed - and propose three key modifications to existing meta-RL methods: consistency of latent information within sessions, session masking, and prior latent conditioning. We demonstrate the importance of these modifications in various domains, ranging from discrete Gridworld environments to continuous-control and simulated robot assistive tasks, demonstrating that DynaMITE-RL significantly outperforms state-of-the-art baselines in sample efficiency and inference returns.
    
[^92]: GreenLLaMA: 一种带有解释的解毒框架

    GreenLLaMA: A Framework for Detoxification with Explanations

    [https://arxiv.org/abs/2402.15951](https://arxiv.org/abs/2402.15951)

    GreenLLaMA是一种全面的端到端解毒框架，通过跨平台语料库训练出的模型优于当前最先进的模型。

    

    先前关于解毒的研究工作分散在某种程度上，因为它们并没有涵盖到真实场景中所需的所有解毒方面。值得注意的是，先前的研究将开发解毒模型的任务局限在仅见过的平台子集上，没有探讨模型在未知平台上的表现如何。此外，这些工作没有解决不可解毒性这一现象，即毒性文本无法在不改变含义的情况下进行解毒。我们提出了GreenLLaMA，这是第一个全面的端到端解毒框架，旨在减轻上述限制。我们首先介绍了一个跨平台伪并行语料库，应用多步数据处理和生成策略利用ChatGPT。然后，我们使用跨平台语料库训练一套解毒模型。我们展示了我们的解毒模型优于使用人工注释的最先进模型的表现。

    arXiv:2402.15951v1 Announce Type: cross  Abstract: Prior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario. Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning. We propose GreenLLaMA, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations. We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT. We then train a suite of detoxification models with our cross-platform corpus. We show that our detoxification models outperform the SoTA model trained with human-annotated par
    
[^93]: 在Python中实现用于线性系统的回收方法，并应用于多目标优化

    Implementing Recycling Methods for Linear Systems in Python with an Application to Multiple Objective Optimization

    [https://arxiv.org/abs/2402.15941](https://arxiv.org/abs/2402.15941)

    优化多目标优化问题中线性系统的计算效率，实现了RMINRES方法在Python和PyTorch中的应用

    

    在计算多目标优化的帕紗图前沿时，线性系统的序列出现在预测校正方法中。与丢弃解决一个系统时生成的信息不同，重新利用这些信息可能对后续系统有利。为了实现这一目标，我们试图通过常见的回收方法来减少解决线性系统时的总计算成本。在这项工作中，我们评估了回收最小残差（RMINRES）方法以及系数矩阵之间的映射的性能。为了将这些方法完全整合到Enouen等人（2022年）使用的软件中，必须在Python和PyTorch中都有每个方法的可用版本。在这里，我们讨论了在计算这些回收策略的高效Python实现时遇到的挑战和解决方案（以及一些正在进行中的）。该项目的目标是在Python和PyTorch中实现RMINRES并将其添加

    arXiv:2402.15941v1 Announce Type: cross  Abstract: Sequences of linear systems arise in the predictor-corrector method when computing the Pareto front for multi-objective optimization. Rather than discarding information generated when solving one system, it may be advantageous to recycle information for subsequent systems. To accomplish this, we seek to reduce the overall cost of computation when solving linear systems using common recycling methods. In this work, we assessed the performance of recycling minimum residual (RMINRES) method along with a map between coefficient matrices. For these methods to be fully integrated into the software used in Enouen et al. (2022), there must be working version of each in both Python and PyTorch. Herein, we discuss the challenges we encountered and solutions undertaken (and some ongoing) when computing efficient Python implementations of these recycling strategies. The goal of this project was to implement RMINRES in Python and PyTorch and add it
    
[^94]: 深度可分离时空学习用于快速动态心脏磁共振成像

    Deep Separable Spatiotemporal Learning for Fast Dynamic Cardiac MRI

    [https://arxiv.org/abs/2402.15939](https://arxiv.org/abs/2402.15939)

    提出了一种使用深度可分离时空学习网络（DeepSSL）的方法，结合时空先验开发了一个高效的图像重建方案，即使在训练数据有限的情况下也表现出色。

    

    动态磁共振成像（MRI）在心脏诊断中起着不可或缺的作用。为了实现快速成像，k-空间数据可以进行欠采样，但图像重建面临着高维处理的巨大挑战。为了解决这一挑战，许多深度学习重建方法需要大量的训练数据。本文提出了一种新颖高效的方法，利用降维可分离学习方案，即使在训练数据极为有限的情况下也能表现卓越。我们进一步将其与时空先验相结合，开发了一个深度可分离时空学习网络（DeepSSL），该网络展开了一个具有时间低秩性和空间稀疏性的重建模型的迭代过程。中间输出被可视化，以提供有关网络行为的见解并增强其可解释性。对心脏短片数据集的广泛结果表明，所提出的DeepSSL优于其他方法。

    arXiv:2402.15939v1 Announce Type: cross  Abstract: Dynamic magnetic resonance imaging (MRI) plays an indispensable role in cardiac diagnosis. To enable fast imaging, the k-space data can be undersampled but the image reconstruction poses a great challenge of high-dimensional processing. This challenge leads to necessitate extensive training data in many deep learning reconstruction methods. This work proposes a novel and efficient approach, leveraging a dimension-reduced separable learning scheme that excels even with highly limited training data. We further integrate it with spatiotemporal priors to develop a Deep Separable Spatiotemporal Learning network (DeepSSL), which unrolls an iteration process of a reconstruction model with both temporal low-rankness and spatial sparsity. Intermediate outputs are visualized to provide insights into the network's behavior and enhance its interpretability. Extensive results on cardiac cine datasets show that the proposed DeepSSL is superior to th
    
[^95]: 大语言模型的泛化或记忆：数据污染与可信评估

    Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models

    [https://arxiv.org/abs/2402.15938](https://arxiv.org/abs/2402.15938)

    本文提出了一种通过LLMs输出分布进行污染检测的方法CDD，以及一种基于LLMs输出修正的可信评估方法TED，以应对大语言模型在数据污染和可信评估方面面临的挑战。

    

    最近关于大语言模型（LLMs）令人印象深刻能力的说法通常是通过在开放获取的基准上进行评估来支持的。考虑到LLMs的训练数据的庞大规模和广泛来源，它可能明确或隐含地包含测试数据，导致LLMs更容易受到数据污染的影响。然而，由于训练数据的不透明性、模型的黑盒访问以及合成训练数据的快速增长，对于LLMs来说检测和减轻数据污染面临着重大挑战。在本文中，我们提出了CDD，即通过LLMs输出分布进行污染检测的CDD。CDD仅需要采样文本来检测数据污染，通过识别LLMs输出分布的峰值来进行检测。为了减轻评估中数据污染的影响，我们还提出了TED：基于LLMs输出修正的可信评估。

    arXiv:2402.15938v1 Announce Type: cross  Abstract: Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's outp
    
[^96]: 跨越2D和3D视觉问答之间的鸿沟：一种用于3D VQA的融合方法

    Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA

    [https://arxiv.org/abs/2402.15933](https://arxiv.org/abs/2402.15933)

    通过问题条件的2D视图选择过程和双分支Transformer结构，将2D知识整合到3D-VQA系统中，从而弥补了当前方法在3D视觉问答中遇到的挑战。

    

    在3D视觉问答（3D VQA）中，充分注释数据的稀缺性和有限的视觉内容多样性阻碍了对新颖场景和3D概念的泛化（如ScanQA和SQA数据集仅利用了约800个场景）。目前的方法通过补充2D信息来辅助3D推理。然而，这些方法面临挑战：它们要么使用引入过于复杂且有时与问题无关的视觉线索的自上而下的2D视图，要么依靠来自2D VLM的全局聚合场景/图像级表示，从而丢失了细粒度的视觉语言相关性。为了克服这些局限性，我们的方法利用了问题条件下的2D视图选择过程，准确地指出了关键视觉线索的语义相关2D输入。然后，我们通过双分支Transformer结构将这种2D知识整合到3D-VQA系统中。这种结构采用了双Transformer设计，紧凑地结合

    arXiv:2402.15933v1 Announce Type: cross  Abstract: In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset). Current approaches resort supplement 3D reasoning with 2D information. However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations. To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues. We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure. This structure, featuring a Twin-Transformer design, compactly combine
    
[^97]: 使用RLlib-IMPALA框架的可扩展Volt-VAR优化：一种强化学习方法

    Scalable Volt-VAR Optimization using RLlib-IMPALA Framework: A Reinforcement Learning Approach

    [https://arxiv.org/abs/2402.15932](https://arxiv.org/abs/2402.15932)

    使用RLlib-IMPALA框架的可扩展Volt-VAR优化方法利用深度强化学习和分布式计算加速了电力系统中的VVO解决方案搜索。

    

    在电力系统不断发展的领域中，Volt-VAR优化(VVO)变得越来越关键，特别是随着可再生能源不断融入其中。传统的基于学习的VVO方法在庞大且动态变化的电力系统中常常受到计算复杂性的限制。为了解决这一挑战，我们的研究提出了一个利用深度强化学习(DRL)潜力的新框架，具体利用了重要性加权Actor-Learner架构(IMPALA)算法，并在RAY平台上执行。这个框架建立在RLlib之上，RLlib是强化学习中的行业标准，巧妙地利用了RAY提供的分布式计算能力和先进的超参数调整功能。这个设计显著加快了VVO解空间中的探索和利用阶段。我们的实验结果表明，我们的方法不但可以

    arXiv:2402.15932v1 Announce Type: new  Abstract: In the rapidly evolving domain of electrical power systems, the Volt-VAR optimization (VVO) is increasingly critical, especially with the burgeoning integration of renewable energy sources. Traditional approaches to learning-based VVO in expansive and dynamically changing power systems are often hindered by computational complexities. To address this challenge, our research presents a novel framework that harnesses the potential of Deep Reinforcement Learning (DRL), specifically utilizing the Importance Weighted Actor-Learner Architecture (IMPALA) algorithm, executed on the RAY platform. This framework, built upon RLlib-an industry-standard in Reinforcement Learning-ingeniously capitalizes on the distributed computing capabilities and advanced hyperparameter tuning offered by RAY. This design significantly expedites the exploration and exploitation phases in the VVO solution space. Our empirical results demonstrate that our approach not 
    
[^98]: QuaCer-C：大型语言模型中知识理解的定量认证

    QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs

    [https://arxiv.org/abs/2402.15929](https://arxiv.org/abs/2402.15929)

    本文提出了一种新颖的认证框架QuaCer-C，用于正式认证大型语言模型中知识理解的能力，证书定量化且包含高置信度的概率界限，研究发现，随着参数数量的增加，知识理解能力提高，Mistral模型在这一评估中表现不如其他模型。

    

    大型语言模型（LLMs）在多个基准测试中展现出令人印象深刻的表现。然而，传统研究并未对LLMs的表现提供正式的保证。本文提出了一种新颖的LLM认证框架QuaCer-C，我们在此对知名LLMs的知识理解能力进行正式认证。我们的证书是定量的 - 它们包括对目标LLM在任何相关知识理解提示上给出正确答案的概率的高置信度紧密界限。我们针对Llama、Vicuna和Mistral LLMs的证书表明，知识理解能力随参数数量的增加而提高，并且Mistral模型在这一评估中表现不如其他模型。

    arXiv:2402.15929v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive performance on several benchmarks. However, traditional studies do not provide formal guarantees on the performance of LLMs. In this work, we propose a novel certification framework for LLM, QuaCer-C, wherein we formally certify the knowledge-comprehension capabilities of popular LLMs. Our certificates are quantitative - they consist of high-confidence, tight bounds on the probability that the target LLM gives the correct answer on any relevant knowledge comprehension prompt. Our certificates for the Llama, Vicuna, and Mistral LLMs indicate that the knowledge comprehension capability improves with an increase in the number of parameters and that the Mistral model is less performant than the rest in this evaluation.
    
[^99]: 逻辑回归的大步梯度下降：损失的非单调性提高了优化效率

    Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency

    [https://arxiv.org/abs/2402.15926](https://arxiv.org/abs/2402.15926)

    该研究表明对于具有线性可分数据的逻辑回归问题，设置一个恒定但较大的步长，在初始震荡后可以实现较快的收敛，并且在一定步骤后可以达到加速的收敛速率，这种方法无需动量或变步长调度器。

    

    我们考虑了梯度下降（GD）与具有线性可分数据的逻辑回归结合使用的恒定步长情况，其中恒定步长$\eta$非常大，以至于损失在初始阶段会震荡。我们展示了GD在$\mathcal{O}(\eta)$步内迅速退出这种初始震荡阶段，并在额外的$t$步之后实现了一个$\tilde{\mathcal{O}}(1 / (\eta t) )$的收敛速率。我们的结果意味着，给定$T$步的预算，使用积极的步长$\eta:= \Theta( T)$，无需使用任何动量或变步长调度器，GD可以实现一个$\tilde{\mathcal{O}}(1/T^2)$的加速损失。我们的证明技术多才多艺，还可以处理一般分类损失函数（其中需要指数尾部来实现$\tilde{\mathcal{O}}(1/T^2)$的加速）、神经切线核区域的非线性预测器，以及具有大步长的在线随机梯度下降（SGD）。

    arXiv:2402.15926v1 Announce Type: new  Abstract: We consider gradient descent (GD) with a constant stepsize applied to logistic regression with linearly separable data, where the constant stepsize $\eta$ is so large that the loss initially oscillates. We show that GD exits this initial oscillatory phase rapidly -- in $\mathcal{O}(\eta)$ steps -- and subsequently achieves an $\tilde{\mathcal{O}}(1 / (\eta t) )$ convergence rate after $t$ additional steps. Our results imply that, given a budget of $T$ steps, GD can achieve an accelerated loss of $\tilde{\mathcal{O}}(1/T^2)$ with an aggressive stepsize $\eta:= \Theta( T)$, without any use of momentum or variable stepsize schedulers. Our proof technique is versatile and also handles general classification loss functions (where exponential tails are needed for the $\tilde{\mathcal{O}}(1/T^2)$ acceleration), nonlinear predictors in the neural tangent kernel regime, and online stochastic gradient descent (SGD) with a large stepsize, under sui
    
[^100]: 使用长短期记忆网络在电子竞技中预测结果

    Predicting Outcomes in Video Games with Long Short Term Memory Networks

    [https://arxiv.org/abs/2402.15923](https://arxiv.org/abs/2402.15923)

    使用长短期记忆网络在实时分析中预测电子竞技比赛结果，仅利用玩家生命值指标作为时间序列，较之传统方法和Transformer模型，取得了更高效的预测性能。

    

    预测电子竞技中的胜者，在实时分析中具有潜力进一步吸引观看主要锦标赛的观众。然而，由于游戏中涉及不同玩家策略和决策的不可预测变量，进行这种实时预测是具有挑战性的。我们的工作试图通过引入一种实时预测胜利的方法来增强视频游戏比赛中观众的参与度。我们基于长短期记忆网络（LSTMs）的方法能够通过仅使用每个玩家的生命值指标作为时间序列来高效预测胜负结果。作为概念验证，我们在经典的两人街机游戏《超级街霸II Turbo》中评估了我们模型的性能。我们还将我们的方法与用于时间序列预测的最新方法进行基准测试；即在大型语言模型（LLMs）中找到的Transformer模型。最后，我们开源了我们的数据集和代码，希望可以

    arXiv:2402.15923v1 Announce Type: cross  Abstract: Forecasting winners in E-sports with real-time analytics has the potential to further engage audiences watching major tournament events. However, making such real-time predictions is challenging due to unpredictable variables within the game involving diverse player strategies and decision-making. Our work attempts to enhance audience engagement within video game tournaments by introducing a real-time method of predicting wins. Our Long Short Term Memory Network (LSTMs) based approach enables efficient predictions of win-lose outcomes by only using the health indicator of each player as a time series. As a proof of concept, we evaluate our model's performance within a classic, two-player arcade game, Super Street Fighter II Turbo. We also benchmark our method against state of the art methods for time series forecasting; i.e. Transformer models found in large language models (LLMs). Finally, we open-source our data set and code in hopes
    
[^101]: 神经势预训练策略

    Pretraining Strategy for Neural Potentials

    [https://arxiv.org/abs/2402.15921](https://arxiv.org/abs/2402.15921)

    通过提出的面向图神经网络的掩码预训练方法，改善了GNN在拟合水系统势能表面方面的性能，并在精度和收敛速度上优于从头开始训练或使用其他预训练技术。

    

    我们提出了一种面向图神经网络(GNNs)的掩码预训练方法，以提高其在拟合势能表面方面的性能，特别是在水系统中。GNNs通过从分子中恢复与掩码的原子相关的空间信息进行预训练，然后在原子力场上进行转移和微调。通过这种预训练，GNNs学习了关于分子系统的结构和潜在物理信息的有意义先验，对于下游任务是有用的。通过全面的实验和消融研究，我们展示了所提出的方法相对于从头开始训练或使用其他预训练技术（如去噪）的GNNs，在精度和收敛速度上的提高。另一方面，我们的预训练方法适用于以能量为中心和以力为中心的GNNs。这种方法展示了它在拟合分子力场方面提高了GNNs的性能和数据效率的潜力。

    arXiv:2402.15921v1 Announce Type: new  Abstract: We propose a mask pretraining method for Graph Neural Networks (GNNs) to improve their performance on fitting potential energy surfaces, particularly in water systems. GNNs are pretrained by recovering spatial information related to masked-out atoms from molecules, then transferred and finetuned on atomic forcefields. Through such pretraining, GNNs learn meaningful prior about structural and underlying physical information of molecule systems that are useful for downstream tasks. From comprehensive experiments and ablation studies, we show that the proposed method improves the accuracy and convergence speed compared to GNNs trained from scratch or using other pretraining techniques such as denoising. On the other hand, our pretraining method is suitable for both energy-centric and force-centric GNNs. This approach showcases its potential to enhance the performance and data efficiency of GNNs in fitting molecular force fields.
    
[^102]: 可解释的对比和成本敏感学习用于宫颈癌分类

    Explainable Contrastive and Cost-Sensitive Learning for Cervical Cancer Classification

    [https://arxiv.org/abs/2402.15905](https://arxiv.org/abs/2402.15905)

    本文提出了一个高效的宫颈癌细胞分类系统，通过使用预训练的CNNs进行微调并结合监督对比学习，最小化误分类成本，达到了97.29%的准确率，并引入了可解释的人工智能技术来解释模型的决策过程。

    

    本文提出了一个高效的系统，使用预训练的卷积神经网络（CNNs）对宫颈癌细胞进行分类。我们首先微调了五个预训练的CNNs，并通过优先考虑具有更高相关成本或重要性的类别的准确性来最小化总的误分类成本。为了进一步提高模型的性能，我们加入了监督对比学习，使模型更擅长捕捉重要特征和模式。对提出的系统在SIPaKMeD数据集上进行了大量实验评估。实验结果表明，该系统的有效性，达到了97.29%的准确率。为了使我们的系统更加可信，我们采用了几种可解释的人工智能技术来解释模型是如何做出具体决策的。系统的实现可在以下网址找到 - https://github.com/isha-67/CervicalCancerStudy.

    arXiv:2402.15905v1 Announce Type: cross  Abstract: This paper proposes an efficient system for classifying cervical cancer cells using pre-trained convolutional neural networks (CNNs). We first fine-tune five pre-trained CNNs and minimize the overall cost of misclassification by prioritizing accuracy for certain classes that have higher associated costs or importance. To further enhance the performance of the models, supervised contrastive learning is included to make the models more adept at capturing important features and patterns. Extensive experimentation are conducted to evaluate the proposed system on the SIPaKMeD dataset. The experimental results demonstrate the effectiveness of the developed system, achieving an accuracy of 97.29%. To make our system more trustworthy, we have employed several explainable AI techniques to interpret how the models reached a specific decision. The implementation of the system can be found at - https://github.com/isha-67/CervicalCancerStudy.
    
[^103]: 高效分裂联邦学习在资源受限的异构无线设备上

    ESFL: Efficient Split Federated Learning over Resource-Constrained Heterogeneous Wireless Devices

    [https://arxiv.org/abs/2402.15903](https://arxiv.org/abs/2402.15903)

    该论文提出了一种高效的分裂联邦学习算法（ESFL），能够充分利用中央服务器和端设备的计算资源，通过将模型分为不同的子模型并考虑用户的异质性，共同优化用户端工作量和服务器端计算资源分配。

    

    联邦学习（FL）允许多个参与方（分布式设备）在不共享原始数据的情况下训练机器学习模型。如何有效地利用设备和中央服务器上的资源是一个极具吸引力但具有挑战性的问题。本文提出了一种高效的分裂联邦学习算法（ESFL），以充分利用中央服务器在具有异构端设备（EDs）的分裂联邦学习框架下的强大计算能力。通过在服务器和EDs之间将模型分为不同的子模型，我们的方法通过考虑用户的异质性共同优化用户端工作量和服务器端计算资源分配。我们将整个优化问题建模为一个混合整数非线性规划，这是一个NP难问题，并开发了一个迭代方法来有效地获得近似解决方案。进行了大量的模拟实验。

    arXiv:2402.15903v1 Announce Type: cross  Abstract: Federated learning (FL) allows multiple parties (distributed devices) to train a machine learning model without sharing raw data. How to effectively and efficiently utilize the resources on devices and the central server is a highly interesting yet challenging problem. In this paper, we propose an efficient split federated learning algorithm (ESFL) to take full advantage of the powerful computing capabilities at a central server under a split federated learning framework with heterogeneous end devices (EDs). By splitting the model into different submodels between the server and EDs, our approach jointly optimizes user-side workload and server-side computing resource allocation by considering users' heterogeneity. We formulate the whole optimization problem as a mixed-integer non-linear program, which is an NP-hard problem, and develop an iterative approach to obtain an approximate solution efficiently. Extensive simulations have been c
    
[^104]: 基于信息的转导式主动学习

    Information-based Transductive Active Learning

    [https://arxiv.org/abs/2402.15898](https://arxiv.org/abs/2402.15898)

    ITL是一种基于信息的转导式学习方法，可以在现实世界设置中自适应采样，以最大化关于指定预测目标的信息获取，并在少样本微调和安全贝叶斯优化应用中显著优于最先进技术。

    

    我们将主动学习推广到解决现实世界中采样受限于可访问域的情况，而预测目标可能位于这个域之外。为此，我们提出了ITL，即基于信息的转导式学习，一种自适应采样的方法，旨在最大化关于指定预测目标的信息获取。在一般正则性假设下，我们展示了ITL收敛到可从可访问数据中获得的最小可能不确定性。我们在两个关键应用中展示了ITL：大型神经网络的少样本微调和安全贝叶斯优化，在两种情况下，ITL明显优于最先进技术。

    arXiv:2402.15898v1 Announce Type: cross  Abstract: We generalize active learning to address real-world settings where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region. To this end, we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified prediction targets. We show, under general regularity assumptions, that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. We demonstrate ITL in two key applications: Few-shot fine-tuning of large neural networks and safe Bayesian optimization, and in both cases, ITL significantly outperforms the state-of-the-art.
    
[^105]: 强化学习中并行学习策略和未知安全约束

    Concurrent Learning of Policy and Unknown Safety Constraints in Reinforcement Learning

    [https://arxiv.org/abs/2402.15893](https://arxiv.org/abs/2402.15893)

    提出了一种同时学习安全RL控制策略和识别未知安全约束参数的新方法。

    

    强化学习（RL）在过去几十年中已经彻底改变了跨多个领域的决策制定。然而，在实际场景中部署RL策略面临着确保安全的关键挑战。传统的安全RL方法主要集中于将预定义的安全约束纳入到策略学习过程中。然而，在动态和不可预测的实际环境中，这种对预定义安全约束的依赖在安全控制RL任务中具有限制，因为这些约束可能无法得到或不够适应。为弥补这一差距，我们提出了一种新颖的方法，同时学习安全的RL控制策略并确定给定环境的未知安全约束参数。通过使用一个参数化的信号时间逻辑（pSTL）安全规范和一个小的初始标记数据集进行初始化，我们将问题构建为一个双层优化任务，巧妙地将受限策略op

    arXiv:2402.15893v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has revolutionized decision-making across a wide range of domains over the past few decades. Yet, deploying RL policies in real-world scenarios presents the crucial challenge of ensuring safety. Traditional safe RL approaches have predominantly focused on incorporating predefined safety constraints into the policy learning process. However, this reliance on predefined safety constraints poses limitations in dynamic and unpredictable real-world settings where such constraints may not be available or sufficiently adaptable. Bridging this gap, we propose a novel approach that concurrently learns a safe RL control policy and identifies the unknown safety constraint parameters of a given environment. Initializing with a parametric signal temporal logic (pSTL) safety specification and a small initial labeled dataset, we frame the problem as a bilevel optimization task, intricately integrating constrained policy op
    
[^106]: 统计游戏

    Statistical Games

    [https://arxiv.org/abs/2402.15892](https://arxiv.org/abs/2402.15892)

    本研究将Bayesian统计嵌入到更广泛的决策框架中，提出了统计游戏作为统一框架，涵盖了频率派和贝叶斯统计，并提出了最小后悔准则作为决策的一般方法。

    

    这项工作对几种典型的游戏进行了数学探索，其中自然涌现了统计学和概率论的核心概念。这些游戏包括费舍尔游戏和贝叶斯游戏，它们分别与频率派统计学和贝叶斯统计学相关。随后引入了一个更一般类型的游戏，称为统计游戏，在其中可以设置一个进一步的参数，即玩家的相对风险厌恶。本研究表明，费舍尔游戏和贝叶斯游戏可以被视为统计游戏的极限情况。因此，统计游戏可以被视为一个统一的框架，融合了频率派和贝叶斯统计。此外，还提出了一种哲学框架，通常被称为最小后悔准则，作为决策的一般方法。

    arXiv:2402.15892v1 Announce Type: cross  Abstract: This work contains the mathematical exploration of a few prototypical games in which central concepts from statistics and probability theory naturally emerge. The first two kinds of games are termed Fisher and Bayesian games, which are connected to Frequentist and Bayesian statistics, respectively. Later, a more general type of game is introduced, termed Statistical game, in which a further parameter, the players' relative risk aversion, can be set. In this work, we show that Fisher and Bayesian games can be viewed as limiting cases of Statistical games. Therefore, Statistical games can be viewed as a unified framework, incorporating both Frequentist and Bayesian statistics. Furthermore, a philosophical framework is (re-)presented -- often referred to as minimax regret criterion -- as a general approach to decision making.   The main motivation for this work was to embed Bayesian statistics into a broader decision-making framework, whe
    
[^107]: 融合编码器网络

    Fusion Encoder Networks

    [https://arxiv.org/abs/2402.15883](https://arxiv.org/abs/2402.15883)

    FENs是一种神经网络算法，具有对数深度且可以在线性时间内处理序列，关键创新在于通过训练大致线性数量的常深度神经网络并行学习。

    

    在本文中，我们提出了一种名为融合编码器网络（FENs）的算法类：用于创建将固定长度序列映射到输出的神经网络。生成的神经网络仅具有对数深度（减轻数据在网络中传播时的退化），可以在线性时间内处理序列（或者在具有线性处理器数量的对数时间内）。FENs的关键属性是它们通过训练大致线性数量的常深度神经网络并行学习。这些网络具有常深度意味着反向传播效果良好。需要注意的是，目前FENs的性能仅仅是推测，因为我们尚未实现它们。

    arXiv:2402.15883v1 Announce Type: new  Abstract: In this paper we present fusion encoder networks (FENs): a class of algorithms for creating neural networks that map fixed-length sequences to outputs. The resulting neural network has only logarithmic depth (alleviating the degradation of data as it propagates through the network) and can process sequences in linear time (or in logarithmic time with a linear number of processors). The crucial property of FENs is that they learn by training a quasi-linear number of constant-depth neural networks in parallel. The fact that these networks are constant depth means that backpropagation works well. We note that currently the performance of FENs is only conjectured as we are yet to implement them.
    
[^108]: 基于场的分子生成

    Field-based Molecule Generation

    [https://arxiv.org/abs/2402.15864](https://arxiv.org/abs/2402.15864)

    介绍了一种基于场的模型用于生成拟药分子，相比基于点云的方法具有灵活性和竞争性，能够解决对映异构体问题，进而考虑所有分子几何方面。

    

    这项工作介绍了FMG，一种用于生成拟药分子的基于场的模型。我们展示了这种方法的灵活性如何相比普遍使用的基于点云的方法提供了重要优势，并实现了有竞争力的分子稳定性生成。我们解决了光学异构体（对映异构体），这是一个先前被忽略的对于药物安全性和有效性至关重要的分子属性，并因此考虑了所有分子几何方面。我们展示了先前的方法是对一组变换不变的，其中包括对映异构体成对存在，导致它们对分子的R和S构型保持不变，而我们的基于场的生成模型捕捉了这一属性。

    arXiv:2402.15864v1 Announce Type: new  Abstract: This work introduces FMG, a field-based model for drug-like molecule generation. We show how the flexibility of this method provides crucial advantages over the prevalent, point-cloud based methods, and achieves competitive molecular stability generation. We tackle optical isomerism (enantiomers), a previously omitted molecular property that is crucial for drug safety and effectiveness, and thus account for all molecular geometry aspects. We demonstrate how previous methods are invariant to a group of transformations that includes enantiomer pairs, leading them invariant to the molecular R and S configurations, while our field-based generative model captures this property.
    
[^109]: 针对鲁棒性语言模型的提示扰动一致性学习

    Prompt Perturbation Consistency Learning for Robust Language Models

    [https://arxiv.org/abs/2402.15833](https://arxiv.org/abs/2402.15833)

    微调大型语言模型可以产生与判别模型相当的性能，在分析和解决LLMs对输入提示中不同类型扰动的鲁棒性方面取得了重要进展

    

    大型语言模型（LLMs）在诸如问答和文本总结等多个自然语言处理任务中展现出令人印象深刻的性能。然而，在意图分类和槽填充（IC-SF）等序列标注任务上，它们的性能显著落后于判别模型。本文的贡献有三个方面。首先，我们展示了对足够大的LLMs进行微调可以产生与判别模型相媲美的IC-SF性能。接下来，我们系统分析了这些经过微调的模型由于三种不同而相关的输入扰动 - 同音词、同义词和释义 - 导致的性能恶化。最后，我们提出了一种高效的缓解方法，即提示扰动一致性学习。

    arXiv:2402.15833v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated impressive performance on a number of natural language processing tasks, such as question answering and text summarization. However, their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on the robustness of LLMs to various perturbations in the input prompts. The contributions of this paper are three-fold. First, we show that fine-tuning sufficiently large LLMs can produce IC-SF performance comparable to discriminative models. Next, we systematically analyze the performance deterioration of those fine-tuned models due to three distinct yet relevant types of input perturbations - oronyms, synonyms, and paraphrasing. Finally, we propose an efficient mitigation approach, Prompt Perturbatio
    
[^110]: 用于合理序贯决策的奖励设计

    Reward Design for Justifiable Sequential Decision-Making

    [https://arxiv.org/abs/2402.15826](https://arxiv.org/abs/2402.15826)

    代理通过辩论型奖励模型学习可辩明策略，以支持证据证明决策的合理性。

    

    将代理赋予能够使用支持证据来证明决策的能力是负责任决策的基石。此外，确保这些证明与人类期望和社会规范一致至关重要，特别是在高风险情况下，比如医疗保健。在这项工作中，我们提出了一个辩论型奖励模型，用于强化学习代理，其中零和辩论游戏的结果量化了在特定状态下的决策的合理性。然后使用该奖励模型来训练一个可以更容易地与支持证据相印证的可辩明策略。在辩论游戏中，两个辩护性代理轮流提供支持证据，支持两个竞争性决策。在提供的证据条件下，一个人类法官的代理评估哪个决策更合理。我们展示了我们的方法在学习处方策略方面的潜力。

    arXiv:2402.15826v1 Announce Type: cross  Abstract: Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making. Furthermore, ensuring that justifications are in line with human expectations and societal norms is vital, especially in high-stakes situations such as healthcare. In this work, we propose the use of a debate-based reward model for reinforcement learning agents, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state. This reward model is then used to train a justifiable policy, whose decisions can be more easily corroborated with supporting evidence. In the debate game, two argumentative agents take turns providing supporting evidence for two competing decisions. Given the proposed evidence, a proxy of a human judge evaluates which decision is better justified. We demonstrate the potential of our approach in learning policies for prescribin
    
[^111]: 消除偏见的基于模型的交互式推荐

    Debiased Model-based Interactive Recommendation

    [https://arxiv.org/abs/2402.15819](https://arxiv.org/abs/2402.15819)

    该论文提出了一种名为iDMIR的模型，通过设计基于因果机制的偏见消除因果世界模型来克服传统基于模型的交互式推荐系统中存在的流行度和抽样偏见的问题。

    

    现有的基于模型的交互式推荐系统通过查询世界模型来捕捉用户偏好，但从历史记录数据中学习世界模型很容易受到偏见问题的困扰，如流行度偏见和抽样偏见。最近提出了一些消除偏见的方法。然而，仍然存在两个主要缺点：1）忽略时间变化流行度的动态会导致对项目的错误重新加权。 2）将未知样本视为负样本在负采样中会导致抽样偏见。为了克服这两个缺点，我们开发了一种称为可识别去偏见模型的交互式推荐模型（简称iDMIR）。在iDMIR中，针对第一个缺点，我们设计了一个基于因果机制的偏见消除因果世界模型，保证了鉴别性。

    arXiv:2402.15819v1 Announce Type: cross  Abstract: Existing model-based interactive recommendation systems are trained by querying a world model to capture the user preference, but learning the world model from historical logged data will easily suffer from bias issues such as popularity bias and sampling bias. This is why some debiased methods have been proposed recently. However, two essential drawbacks still remain: 1) ignoring the dynamics of the time-varying popularity results in a false reweighting of items. 2) taking the unknown samples as negative samples in negative sampling results in the sampling bias. To overcome these two drawbacks, we develop a model called \textbf{i}dentifiable \textbf{D}ebiased \textbf{M}odel-based \textbf{I}nteractive \textbf{R}ecommendation (\textbf{iDMIR} in short). In iDMIR, for the first drawback, we devise a debiased causal world model based on the causal mechanism of the time-varying recommendation generation process with identification guarantee
    
[^112]: 一种用于材料微观结构3D重建和性能评估的生成式机器学习模型

    A Generative Machine Learning Model for Material Microstructure 3D Reconstruction and Performance Evaluation

    [https://arxiv.org/abs/2402.15815](https://arxiv.org/abs/2402.15815)

    提出了一种将 U-net 和 GAN 结合的生成模型，具有多尺度特性和生成能力，创新性地构建了多尺度通道聚合模块、多尺度分层特征聚合模块和卷积块注意机制

    

    从2D切片重建3D微观结构被认为在预测材料的空间结构和物理性质方面具有重要价值。从2D到3D的维度扩展从当前技术角度看被视为一个极具挑战性的逆问题。最近，基于生成对抗网络的方法引起了广泛关注。然而，它们仍然受到许多限制，包括模型过于简化、需要大量训练样本以及在训练过程中难以实现模型收敛。鉴此，提出了一种将 U-net 的多尺度特性和 GAN 的生成能力结合的新型生成模型。基于此，创新性地构建了一种多尺度通道聚合模块、多尺度分层特征聚合模块和卷积块注意机制。

    arXiv:2402.15815v1 Announce Type: new  Abstract: The reconstruction of 3D microstructures from 2D slices is considered to hold significant value in predicting the spatial structure and physical properties of materials.The dimensional extension from 2D to 3D is viewed as a highly challenging inverse problem from the current technological perspective.Recently,methods based on generative adversarial networks have garnered widespread attention.However,they are still hampered by numerous limitations,including oversimplified models,a requirement for a substantial number of training samples,and difficulties in achieving model convergence during training.In light of this,a novel generative model that integrates the multiscale properties of U-net with and the generative capabilities of GAN has been proposed.Based on this,the innovative construction of a multi-scale channel aggregation module,a multi-scale hierarchical feature aggregation module and a convolutional block attention mechanism can 
    
[^113]: RNN语言模型归纳偏差的一个理论结果

    A Theoretical Result on the Inductive Bias of RNN Language Models

    [https://arxiv.org/abs/2402.15814](https://arxiv.org/abs/2402.15814)

    RNN语言模型能够有效表示更大类别的语言模型，具有有界堆栈和广义堆栈更新函数，类似于保留固定数量符号记忆并使用简单更新机制的自动机。

    

    最近Hewitt等人（2020）的工作提出了对循环神经网络（RNNs）作为语言模型（LMs）的经验成功可能性的一个解释。 它显示RNNs可以有效地表示在人类语言中普遍存在的有界分层结构。 这表明RNNs的成功可能与它们建模层次结构的能力有关。 然而，对Hewitt等人（2020）构造的更详细检查表明，它不限于分层LMs，这引出了RNNs可以有效表示哪些\emph{其他类型} LMs的问题。 为此，我们概括他们的构造以展示RNNs可以有效表示更大类别的LMs：可以通过带有有界堆栈和广义堆栈更新函数的下推自动机表示的那些。 这类似于一个保留固定数量符号记忆并使用简单更新机制更新记忆的自动机。

    arXiv:2402.15814v1 Announce Type: new  Abstract: Recent work by Hewitt et al. (2020) provides a possible interpretation of the empirical success of recurrent neural networks (RNNs) as language models (LMs).   It shows that RNNs can efficiently represent bounded hierarchical structures that are prevalent in human language.   This suggests that RNNs' success might be linked to their ability to model hierarchy.   However, a closer inspection of Hewitt et al.'s (2020) construction shows that it is not limited to hierarchical LMs, posing the question of what \emph{other classes} of LMs can be efficiently represented by RNNs.   To this end, we generalize their construction to show that RNNs can efficiently represent a larger class of LMs: Those that can be represented by a pushdown automaton with a bounded stack and a generalized stack update function.   This is analogous to an automaton that keeps a memory of a fixed number of symbols and updates the memory with a simple update mechanism.  
    
[^114]: OAG-Bench：面向学术图挖掘的人工筛选基准

    OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining

    [https://arxiv.org/abs/2402.15810](https://arxiv.org/abs/2402.15810)

    OAG-Bench是一个基于开放学术图的全面、多方面和精细化人工筛选基准，涵盖了多个任务、数据集、基准和实验结果，旨在促进学术图挖掘。

    

    随着科学文献的迅速增长，多功能的学术知识服务越来越依赖全面的学术图挖掘。尽管公开学术图、基准和数据集已经有了，但这些资源通常在多方面和细粒度注释方面存在不足，受限于特定任务类型和领域，或者缺乏真实学术图。本文提出了基于开放学术图（OAG）的全面、多方面和精细化人工筛选基准OAG-Bench。OAG-Bench涵盖了10个任务，20个数据集，70+个基准和120+个截至目前的实验结果。我们针对某些任务提出了新的数据注释策略，并提供一套数据预处理代码、算法实现和标准化评估协议，以促进学术图挖掘。大量实验表明，即使是大型语言模型（LLMs）这样的先进算法也会在某些任务上受限。

    arXiv:2402.15810v1 Announce Type: cross  Abstract: With the rapid proliferation of scientific literature, versatile academic knowledge services increasingly rely on comprehensive academic graph mining. Despite the availability of public academic graphs, benchmarks, and datasets, these resources often fall short in multi-aspect and fine-grained annotations, are constrained to specific task types and domains, or lack underlying real academic graphs. In this paper, we present OAG-Bench, a comprehensive, multi-aspect, and fine-grained human-curated benchmark based on the Open Academic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines, and 120+ experimental results to date. We propose new data annotation strategies for certain tasks and offer a suite of data pre-processing codes, algorithm implementations, and standardized evaluation protocols to facilitate academic graph mining. Extensive experiments reveal that even advanced algorithms like large language models (LLMs) en
    
[^115]: 多臂攻击的最佳零射击探测器

    Optimal Zero-Shot Detector for Multi-Armed Attacks

    [https://arxiv.org/abs/2402.15808](https://arxiv.org/abs/2402.15808)

    本文提出了一种创新的信息论防御方法，通过最优地汇总现有探测器做出的决策，消除了对训练数据的需求。

    

    本文探讨了恶意参与者采用多臂攻击策略操纵数据样本的情况，为其提供了各种方式向数据集中引入噪音。我们的主要目标是通过检测任何对输入的更改来保护数据。我们在防御策略中极度谨慎，操作在防守者拥有信息明显少于攻击者的环境中。具体而言，防守者无法利用任何数据样本来训练防御模型或验证信道的完整性。相反，防守者完全依赖一组现成的“即插即用”探测器。为了解决这一挑战，我们提出了一种创新的信息论防御方法，通过最优地汇总这些探测器做出的决策，从而消除了对任何训练数据的需求。我们进一步探讨了一个实际的使用案例场景。

    arXiv:2402.15808v1 Announce Type: cross  Abstract: This paper explores a scenario in which a malicious actor employs a multi-armed attack strategy to manipulate data samples, offering them various avenues to introduce noise into the dataset. Our central objective is to protect the data by detecting any alterations to the input. We approach this defensive strategy with utmost caution, operating in an environment where the defender possesses significantly less information compared to the attacker. Specifically, the defender is unable to utilize any data samples for training a defense model or verifying the integrity of the channel. Instead, the defender relies exclusively on a set of pre-existing detectors readily available ``off the shelf''. To tackle this challenge, we derive an innovative information-theoretic defense approach that optimally aggregates the decisions made by these detectors, eliminating the need for any training data. We further explore a practical use-case scenario fo
    
[^116]: 对具有线性函数逼近的离策略多步TD学习算法的分析

    Analysis of Off-Policy Multi-Step TD-Learning with Linear Function Approximation

    [https://arxiv.org/abs/2402.15781](https://arxiv.org/abs/2402.15781)

    该论文分析了在具有线性函数逼近、离策略学习和自举的“致命三连”场景中的多步TD学习算法，并证明了当采样时间跨度 n 足够大时这些算法会收敛到有意义的解。

    

    本文分析了在具有线性函数逼近、离策略学习和自举的“致命三连”场景中的多步TD学习算法。特别地，我们证明了当采样时间跨度 n 足够大时，n步TD学习算法会收敛到一个解。该论文分为两部分。第一部分全面研究了模型基础确定性算法的基本性质，包括投影值迭代、梯度下降算法和控制理论方法，它们可以被视为原型确定性算法，对于理解和发展无模型增强学习算法起着关键作用。特别地，我们证明了当 n 足够大时，这些算法会收敛到有意义的解。根据这些发现，提出并分析了两种n步TD学习算法。

    arXiv:2402.15781v1 Announce Type: cross  Abstract: This paper analyzes multi-step TD-learning algorithms within the `deadly triad' scenario, characterized by linear function approximation, off-policy learning, and bootstrapping. In particular, we prove that n-step TD-learning algorithms converge to a solution as the sampling horizon n increases sufficiently. The paper is divided into two parts. In the first part, we comprehensively examine the fundamental properties of their model-based deterministic counterparts, including projected value iteration, gradient descent algorithms, and the control theoretic approach, which can be viewed as prototype deterministic algorithms whose analysis plays a pivotal role in understanding and developing their model-free reinforcement learning counterparts. In particular, we prove that these algorithms converge to meaningful solutions when n is sufficiently large. Based on these findings, two n-step TD-learning algorithms are proposed and analyzed, whi
    
[^117]: 受限制MDP中的真正无悔学习

    Truly No-Regret Learning in Constrained MDPs

    [https://arxiv.org/abs/2402.15776](https://arxiv.org/abs/2402.15776)

    本文首次肯定回答了一个开放问题，即是否可以在不允许错误抵消的情况下，通过将一种常见的安全约束模型扩展到具有多个约束的CMDPs，提出了一种可以实现次线性后悔的新方法。

    

    受约束的马尔可夫决策过程（CMDPs）是在强化学习中建模安全约束的常见方式。目前用于高效解决CMDPs的最先进方法基于原始-对偶算法。对于这些算法，所有当前已知的后悔界都允许错误抵消——可以通过在一个回合中的约束违反来用严格的约束满足在另一个回合中。这使得在线学习过程不安全，因为它仅保证最终（混合）策略的安全性，而在学习过程中不保证安全。正如Efroni等人（2020年）指出的，原始-对偶算法是否可以在不允许错误抵消的情况下可证明地实现次线性后悔是一个开放问题。在本文中，我们给出了第一个肯定的答案。我们首先将关于正则化原始-对偶方案的最后迭代收敛性通用化到具有多个约束的CMDPs上。基于这一见解，我们提出了一种基于模型的原始

    arXiv:2402.15776v1 Announce Type: new  Abstract: Constrained Markov decision processes (CMDPs) are a common way to model safety constraints in reinforcement learning. State-of-the-art methods for efficiently solving CMDPs are based on primal-dual algorithms. For these algorithms, all currently known regret bounds allow for error cancellations -- one can compensate for a constraint violation in one round with a strict constraint satisfaction in another. This makes the online learning process unsafe since it only guarantees safety for the final (mixture) policy but not during learning. As Efroni et al. (2020) pointed out, it is an open question whether primal-dual algorithms can provably achieve sublinear regret if we do not allow error cancellations. In this paper, we give the first affirmative answer. We first generalize a result on last-iterate convergence of regularized primal-dual schemes to CMDPs with multiple constraints. Building upon this insight, we propose a model-based primal
    
[^118]: 从人类偏好中批量主动学习奖励函数

    Batch Active Learning of Reward Functions from Human Preferences

    [https://arxiv.org/abs/2402.15757](https://arxiv.org/abs/2402.15757)

    本文提出了一种批量主动基于偏好的学习方法，通过少量数据样本有效学习奖励函数，同时保持查询生成时间短并可并行化。

    

    数据生成和标记在机器人学习中往往成本高昂。基于偏好的学习是一个概念，通过向用户提出偏好问题来实现可靠的标记。本文中，我们开发了一组新算法，批量主动基于偏好的学习方法，能够使用尽可能少的数据样本有效学习奖励函数，同时具有较短的查询生成时间，并保持可并行化。我们介绍了一种基于确定性点过程（DPP）的方法，用于批量生成和几种基于启发式的替代方法。最后，我们在模拟中介绍了一些机器人学任务的实验结果。我们的结果表明，我们的批量主动学习算法仅需要少量查询。

    arXiv:2402.15757v1 Announce Type: cross  Abstract: Data generation and labeling are often expensive in robot learning. Preference-based learning is a concept that enables reliable labeling by querying users with preference questions. Active querying methods are commonly employed in preference-based learning to generate more informative data at the expense of parallelization and computation time. In this paper, we develop a set of novel algorithms, batch active preference-based learning methods, that enable efficient learning of reward functions using as few data samples as possible while still having short query generation times and also retaining parallelizability. We introduce a method based on determinantal point processes (DPP) for active batch generation and several heuristic-based alternatives. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that ar
    
[^119]: 稀疏MeZO：在零阶LLM微调中减少参数以获得更好性能

    Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning

    [https://arxiv.org/abs/2402.15751](https://arxiv.org/abs/2402.15751)

    提出了一种稀疏MeZO方法，通过仅对精心选择的参数子集应用零阶优化，实现了在零阶LLM微调中减少参数以获得更好性能的目标

    

    在针对特定任务进行大型语言模型（LLMs）微调通常会产生令人印象深刻的结果，但由于基于梯度的训练中的反向传播而导致内存效率低下。最近提出的高效利用存储器的零阶（MeZO）优化器旨在解决这个问题，在训练过程中只需要前向传递，使其更符合内存友好性。然而，零阶优化中梯度估计的质量往往取决于数据的维数，这可能解释了为什么与各种任务中的标准微调相比，MeZO仍然表现出显著的性能下降。受到参数高效微调（PEFT）成功的启发，本文介绍了稀疏MeZO，这是一种新颖的内存高效的零阶优化方法，仅将ZO应用于精心选择的参数子集。我们提出了一种简单而有效的参数选择方案，获得了显著的性能提升。

    arXiv:2402.15751v1 Announce Type: cross  Abstract: While fine-tuning large language models (LLMs) for specific tasks often yields impressive results, it comes at the cost of memory inefficiency due to back-propagation in gradient-based training. Memory-efficient Zeroth-order (MeZO) optimizers, recently proposed to address this issue, only require forward passes during training, making them more memory-friendly. However, the quality of gradient estimates in zeroth order optimization often depends on the data dimensionality, potentially explaining why MeZO still exhibits significant performance drops compared to standard fine-tuning across various tasks. Inspired by the success of Parameter-Efficient Fine-Tuning (PEFT), this paper introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization approach that applies ZO only to a carefully chosen subset of parameters. We propose a simple yet effective parameter selection scheme that yields significant performance gains with Spar
    
[^120]: 低秩赌徒通过紧绝对到无穷奇异子空间恢复

    Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery

    [https://arxiv.org/abs/2402.15739](https://arxiv.org/abs/2402.15739)

    该论文介绍了一种解决低秩环境中具有上下文信息的赌徒问题的高效算法，其中包括策略评估、最佳策略识别和遗憾最小化，并且在最佳策略识别和策略评估方面的算法几乎是极小极大最优的。

    

    我们研究了具有低秩结构的情境赌徒问题，在每一轮中，如果选择了(情境，动作)对$(i,j)\in [m]\times [n]$，学习者会观察一个未知低秩奖励矩阵的$(i,j)$-th入口的嘈杂样本。连续的情境以独立同分布的方式随机生成并透露给学习者。对于这样的赌徒问题，我们提出了高效的算法用于策略评估、最佳策略识别和遗憾最小化。对于策略评估和最佳策略识别，我们展示了我们的算法几乎是极小极大最优的。例如，为了以至少$1-\delta$的概率返回一个$\varepsilon$-最佳策略，通常需要的样本数大致按照${m+n\over \varepsilon^2}\log(1/\delta)$来衡量。我们的遗憾最小化算法享有的极小极大保证按照$r^{7/4}(m+n)^{3/4}\sqrt{T}$缩放，这优于现有算法。所有提出的算法包括两个阶段：

    arXiv:2402.15739v1 Announce Type: new  Abstract: We study contextual bandits with low-rank structure where, in each round, if the (context, arm) pair $(i,j)\in [m]\times [n]$ is selected, the learner observes a noisy sample of the $(i,j)$-th entry of an unknown low-rank reward matrix. Successive contexts are generated randomly in an i.i.d. manner and are revealed to the learner. For such bandits, we present efficient algorithms for policy evaluation, best policy identification and regret minimization. For policy evaluation and best policy identification, we show that our algorithms are nearly minimax optimal. For instance, the number of samples required to return an $\varepsilon$-optimal policy with probability at least $1-\delta$ typically scales as ${m+n\over \varepsilon^2}\log(1/\delta)$. Our regret minimization algorithm enjoys minimax guarantees scaling as $r^{7/4}(m+n)^{3/4}\sqrt{T}$, which improves over existing algorithms. All the proposed algorithms consist of two phases: they
    
[^121]: 通过无监督预训练和上下文学习实现高效的运算符学习

    Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning

    [https://arxiv.org/abs/2402.15734](https://arxiv.org/abs/2402.15734)

    该论文提出了一种通过无监督预训练和上下文学习方法实现PDE运算符学习的高效方式，以提高数据效率并改善模型的外域性能。

    

    近年来，人们见证了将机器学习方法与物理领域特定洞察力相结合，以解决基于偏微分方程（PDEs）的科学问题的潜力。然而，由于数据密集，这些方法仍然需要大量PDE数据。 这重新引入了对昂贵的数值PDE解决方案的需求，部分削弱了避免这些昂贵模拟的原始目标。 在这项工作中，为了寻求数据效率，我们设计了用于PDE运算符学习的无监督预训练和上下文学习方法。 为了减少对带有模拟解的训练数据的需求，我们使用基于重构的代理任务在未标记的PDE数据上预训练神经运算符。 为了提高超出分布性能，我们进一步帮助神经运算符灵活地利用上下文学习方法，而无需额外的训练成本或设计。 在各种PD上进行了大量实证评估

    arXiv:2402.15734v1 Announce Type: new  Abstract: Recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insight for solving scientific problems based on partial differential equations (PDEs). However, being data-intensive, these methods still require a large amount of PDE data. This reintroduces the need for expensive numerical PDE solutions, partially undermining the original goal of avoiding these expensive simulations. In this work, seeking data efficiency, we design unsupervised pretraining and in-context learning methods for PDE operator learning. To reduce the need for training data with simulated solutions, we pretrain neural operators on unlabeled PDE data using reconstruction-based proxy tasks. To improve out-of-distribution performance, we further assist neural operators in flexibly leveraging in-context learning methods, without incurring extra training costs or designs. Extensive empirical evaluations on a diverse set of PD
    
[^122]: ArEEG_Chars: 用于基于脑电图的设想语音识别的阿拉伯字符数据集

    ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters

    [https://arxiv.org/abs/2402.15733](https://arxiv.org/abs/2402.15733)

    该论文介绍了一种用于阿拉伯字符的EEG数据集ArEEG_Chars，通过深度学习实现97%的准确率，在脑机接口中具有重要意义。

    

    脑机接口（BCI）是近年来热门的研究课题，可以帮助瘫痪患者改善生活。有几项研究自动将脑电图（EEG）信号分类为英文字符和单词。阿拉伯语是世界上使用最广泛的语言之一。然而据我们所知，目前没有针对阿拉伯字符的脑电图信号数据集。在本文中，我们创建了一个用于阿拉伯字符的EEG数据集，并命名为ArEEG_Chars。此外，我们使用深度学习对ArEEG_Chars进行了多项实验。在使用LSTM时获得了最佳结果，准确率达到97%。ArEEG_Chars数据集将对研究人员公开。

    arXiv:2402.15733v1 Announce Type: cross  Abstract: Brain-Computer-Interface (BCI) has been a hot research topic in the last few years that could help paralyzed people in their lives. Several researches were done to classify electroencephalography (EEG) signals automatically into English characters and words. Arabic language is one of the most used languages around the world. However, to the best of our knowledge, there is no dataset for Arabic characters EEG signals. In this paper, we have created an EEG dataset for Arabic characters and named it ArEEG_Chars. Moreover, several experiments were done on ArEEG_Chars using deep learning. Best results were achieved using LSTM and reached an accuracy of 97%. ArEEG_Chars dataset will be public for researchers.
    
[^123]: 在动态环境中的聚类：具有异质性变化的基准数据集生成框架

    Clustering in Dynamic Environments: A Framework for Benchmark Dataset Generation With Heterogeneous Changes

    [https://arxiv.org/abs/2402.15731](https://arxiv.org/abs/2402.15731)

    本论文引入了Dynamic Dataset Generator（DDG）来解决在动态环境中进行聚类时缺乏多样性、可控性和现实性动态数据集的问题，从而帮助设计更有效的聚类算法。

    

    在动态环境中进行聚类是越来越重要的，具有广泛的应用范围，从实时数据分析和在线无监督学习到动态设施定位问题。尽管元启发式在静态聚类任务中表现出有希望的有效性，但它们在动态环境中跟踪最佳聚类解决方案或在时间上稳健地进行聚类的应用仍然很少探讨。这在一定程度上是由于缺乏具有多样性、可控性和现实性动态特征的动态数据集，阻碍了在各种动态场景中对聚类算法进行系统性性能评估。这种缺陷导致我们对在动态环境中设计聚类算法的理解和能力存在差距。为了弥补这一差距，本文引入了动态数据集生成器（DDG）。DDG具有多个动态高斯组件，集成了一系列异质性特征。

    arXiv:2402.15731v1 Announce Type: new  Abstract: Clustering in dynamic environments is of increasing importance, with broad applications ranging from real-time data analysis and online unsupervised learning to dynamic facility location problems. While meta-heuristics have shown promising effectiveness in static clustering tasks, their application for tracking optimal clustering solutions or robust clustering over time in dynamic environments remains largely underexplored. This is partly due to a lack of dynamic datasets with diverse, controllable, and realistic dynamic characteristics, hindering systematic performance evaluations of clustering algorithms in various dynamic scenarios. This deficiency leads to a gap in our understanding and capability to effectively design algorithms for clustering in dynamic environments. To bridge this gap, this paper introduces the Dynamic Dataset Generator (DDG). DDG features multiple dynamic Gaussian components integrated with a range of heterogeneo
    
[^124]: 了解时间序列电子健康记录中的缺失性，用于个性化表示

    Understanding Missingness in Time-series Electronic Health Records for Individualized Representation

    [https://arxiv.org/abs/2402.15730](https://arxiv.org/abs/2402.15730)

    该研究围绕在时间序列电子健康记录中对缺失性进行个性化表示展开，为真正个性化的预测建模提供了新见解。

    

    随着机器学习模型在医疗应用中的广泛应用，建立个性化医学应用的兴趣逐渐增加。尽管已经有大量关于个性化医学的研究提出，但很少有研究关注如何从时间序列电子健康记录（EHR）数据中表示缺失性并学习缺失性模式。对缺失性表示缺乏个性化关注，限制了机器学习应用在真正个性化方向上的充分利用。本简要交流突出了缺失性模式的新见解，提供了真实案例和EHR中缺失性的影响。本工作的见解旨在弥合理论假设与实际观察之间在实际EHR中的差距。我们希望这项工作能为探索更好的预测建模表示方向打开新的大门。

    arXiv:2402.15730v1 Announce Type: new  Abstract: With the widespread of machine learning models for healthcare applications, there is increased interest in building applications for personalized medicine. Despite the plethora of proposed research for personalized medicine, very few focus on representing missingness and learning from the missingness patterns in time-series Electronic Health Records (EHR) data. The lack of focus on missingness representation in an individualized way limits the full utilization of machine learning applications towards true personalization. In this brief communication, we highlight new insights into patterns of missingness with real-world examples and implications of missingness in EHRs. The insights in this work aim to bridge the gap between theoretical assumptions and practical observations in real-world EHRs. We hope this work will open new doors for exploring directions for better representation in predictive modelling for true personalization.
    
[^125]: 操作符学习：算法与分析

    Operator Learning: Algorithms and Analysis

    [https://arxiv.org/abs/2402.15715](https://arxiv.org/abs/2402.15715)

    神经算子是近似非线性算子映射的机器学习方法，可作为高效替代模型应用于多查询任务，尤其在没有数学描述的情况下进行模型发现。

    

    操作符学习指的是将机器学习的思想应用于近似（通常是非线性的）在范数空间中映射的算子，这些算子通常来自于用偏微分方程（PDEs）表达的物理模型。在这种情况下，这种近似算子作为高效的替代模型在许多查询任务中具有巨大潜力，以补充传统的数值方法。由于是数据驱动的，它们还可以在数学上无法描述的情况下进行模型发现。这篇综述主要关注神经算子，建立在深度神经网络在有限维欧几里得空间上定义函数近似方面的成功基础上。从经验上看，神经算子在各种应用中已经显示出成功，但我们的理论理解仍然不完整。本综述文章总结了最近的进展和我们理论的现状。

    arXiv:2402.15715v1 Announce Type: new  Abstract: Operator learning refers to the application of ideas from machine learning to approximate (typically nonlinear) operators mapping between Banach spaces of functions. Such operators often arise from physical models expressed in terms of partial differential equations (PDEs). In this context, such approximate operators hold great potential as efficient surrogate models to complement traditional numerical methods in many-query tasks. Being data-driven, they also enable model discovery when a mathematical description in terms of a PDE is not available. This review focuses primarily on neural operators, built on the success of deep neural networks in the approximation of functions defined on finite dimensional Euclidean spaces. Empirically, neural operators have shown success in a variety of applications, but our theoretical understanding remains incomplete. This review article summarizes recent progress and the current state of our theoretic
    
[^126]: 对内在低维数据的Wasserstein自编码器的统计分析

    A Statistical Analysis of Wasserstein Autoencoders for Intrinsically Low-dimensional Data

    [https://arxiv.org/abs/2402.15710](https://arxiv.org/abs/2402.15710)

    本文从统计分析的角度探讨了Wasserstein自编码器用于内在低维数据的特性与局限。

    

    变分自编码器(Variational Autoencoders, VAEs)在研究人员中广受欢迎，被认为是理解基于有限样本的未知分布的强大工具。这种受欢迎程度部分源于其出色的性能，部分源于其能够在潜在空间中提供有意义的特征表示。Wasserstein自编码器(WAEs)是VAEs的一种变体，旨在不仅提高模型效率，而且提高可解释性。然而，对其统计保证的分析受到了限制。由于WAEs所应用的数据分布（例如自然图像）通常被认为在高维特征空间中具有低维结构，而当前的理论并未充分考虑这一点，导致已知的界限效率低下。为弥合WAEs理论与实践之间的差距，在本文中，我们展示了WAEs...

    arXiv:2402.15710v1 Announce Type: new  Abstract: Variational Autoencoders (VAEs) have gained significant popularity among researchers as a powerful tool for understanding unknown distributions based on limited samples. This popularity stems partly from their impressive performance and partly from their ability to provide meaningful feature representations in the latent space. Wasserstein Autoencoders (WAEs), a variant of VAEs, aim to not only improve model efficiency but also interpretability. However, there has been limited focus on analyzing their statistical guarantees. The matter is further complicated by the fact that the data distributions to which WAEs are applied - such as natural images - are often presumed to possess an underlying low-dimensional structure within a high-dimensional feature space, which current theory does not adequately account for, rendering known bounds inefficient. To bridge the gap between the theory and practice of WAEs, in this paper, we show that WAEs 
    
[^127]: 是否可以仅凭有限样本进行离线决策？通过信任区域增强在数据稀缺赌博机中可靠决策

    Is Offline Decision Making Possible with Only Few Samples? Reliable Decisions in Data-Starved Bandits via Trust Region Enhancement

    [https://arxiv.org/abs/2402.15703](https://arxiv.org/abs/2402.15703)

    本文展示了即使在数据稀缺的情况下，仍然可能找到一个与最优策略竞争的随机策略，为在仅有少量样本下进行可靠决策铺平了道路。

    

    在一个只包含每个臂的单个样本数据集中，一个智能体能从随机多臂老虎机（MAB）问题中学到什么？令人惊讶的是，在这项工作中，我们证明即使在这种数据稀缺的环境中，仍然可能找到一个与最优策略竞争的策略。这为在必须仅依靠少数样本做出关键决策的环境中进行可靠的决策铺平了道路。我们的分析揭示了\emph{随机策略对于离线决策能够显著优于确定性策略}。专注于离线多臂老虎机，我们设计了一种名为基于不确定性信任区域的随机策略增强（TRUST）的算法，这与主导性价值为基础的较低置信下界方法有很大不同。其设计得益于定位规律、临界半径和相对悲观主义。我们证明其样本复杂度与L的相当。

    arXiv:2402.15703v1 Announce Type: cross  Abstract: What can an agent learn in a stochastic Multi-Armed Bandit (MAB) problem from a dataset that contains just a single sample for each arm? Surprisingly, in this work, we demonstrate that even in such a data-starved setting it may still be possible to find a policy competitive with the optimal one. This paves the way to reliable decision-making in settings where critical decisions must be made by relying only on a handful of samples.   Our analysis reveals that \emph{stochastic policies can be substantially better} than deterministic ones for offline decision-making. Focusing on offline multi-armed bandits, we design an algorithm called Trust Region of Uncertainty for Stochastic policy enhancemenT (TRUST) which is quite different from the predominant value-based lower confidence bound approach. Its design is enabled by localization laws, critical radii, and relative pessimism. We prove that its sample complexity is comparable to that of L
    
[^128]: 《CoRelation: 通过上下文化的编码关系学习提升自动ICD编码》

    CoRelation: Boosting Automatic ICD Coding Through Contextualized Code Relation Learning

    [https://arxiv.org/abs/2402.15700](https://arxiv.org/abs/2402.15700)

    通过上下文化的编码关系学习，提出了一种新的框架来增强ICD编码表示的学习，实验结果表明其相比最先进基线方法的有效性。

    

    自动国际疾病分类（ICD）编码在从临床记录中提取相关信息以便正确记录和计费方面起着至关重要的作用。提升自动ICD编码性能的一个重要方向是对ICD编码关系进行建模。然而，当前方法对ICD编码之间错综复杂的关系建模不足，通常忽视了临床记录中上下文的重要性。本文提出了一种新颖方法，即一种上下文化和灵活的框架，以增强ICD编码表示的学习。与现有方法不同，我们的方法采用了考虑临床记录上下文的依赖学习范式，对建模所有可能的编码关系。我们在六个公共ICD编码数据集上评估了我们的方法，实验结果表明与最先进基线方法相比，我们的方法的有效性。

    arXiv:2402.15700v1 Announce Type: cross  Abstract: Automatic International Classification of Diseases (ICD) coding plays a crucial role in the extraction of relevant information from clinical notes for proper recording and billing. One of the most important directions for boosting the performance of automatic ICD coding is modeling ICD code relations. However, current methods insufficiently model the intricate relationships among ICD codes and often overlook the importance of context in clinical notes. In this paper, we propose a novel approach, a contextualized and flexible framework, to enhance the learning of ICD code representations. Our approach, unlike existing methods, employs a dependent learning paradigm that considers the context of clinical notes in modeling all possible code relations. We evaluate our approach on six public ICD coding datasets and the experimental results demonstrate the effectiveness of our approach compared to state-of-the-art baselines.
    
[^129]: 正交梯度提升用于简化的加法规则集合

    Orthogonal Gradient Boosting for Simpler Additive Rule Ensembles

    [https://arxiv.org/abs/2402.15691](https://arxiv.org/abs/2402.15691)

    提出了一种正交梯度提升方法，通过新的目标函数促进生成更加简化的加法规则集合，提高了模型的解释性和准确性。

    

    预测规则的梯度提升是一种学习潜在可解释且准确的概率模型的高效方法。然而，实际的可解释性需要限制生成的规则数量和大小，现有的提升变体并非为此目的而设计。本文通过一个新的目标函数来解决这个问题，该目标函数衡量了风险梯度向量与条件输出向量在已选择条件的正交补上的投影的夹角，从而正确逼近将风险梯度本身添加到模型的理想更新，并倾向于包括更一般且更短的规则。

    arXiv:2402.15691v1 Announce Type: new  Abstract: Gradient boosting of prediction rules is an efficient approach to learn potentially interpretable yet accurate probabilistic models. However, actual interpretability requires to limit the number and size of the generated rules, and existing boosting variants are not designed for this purpose. Though corrective boosting refits all rule weights in each iteration to minimise prediction risk, the included rule conditions tend to be sub-optimal, because commonly used objective functions fail to anticipate this refitting. Here, we address this issue by a new objective function that measures the angle between the risk gradient vector and the projection of the condition output vector onto the orthogonal complement of the already selected conditions. This approach correctly approximate the ideal update of adding the risk gradient itself to the model and favours the inclusion of more general and thus shorter rules. As we demonstrate using a wide r
    
[^130]: 基于锚图因子分解的无锚聚类

    Anchor-free Clustering based on Anchor Graph Factorization

    [https://arxiv.org/abs/2402.15688](https://arxiv.org/abs/2402.15688)

    无锚聚类方法AFCAGF通过学习锚图并优化成对样本距离，避免了锚点选择和初始化的需要，提升了聚类算法性能。

    

    锚点方法是处理大规模数据聚类的一个关键方法。然而，这些方法通常包括两个不同阶段：选择锚点和构建锚图。这种二分以及锚点的初始化显著影响算法的整体性能。为了缓解这些问题，我们引入了一种名为基于锚图因子分解的无锚聚类（AFCAGF）的新方法。AFCAGF在学习锚图方面具有创新性，只需要计算样本之间的成对距离。通过简单的优化过程，可以实现这一过程，避免了需要明确选择锚点的必要性。具体而言，我们的方法增强了模糊k均值聚类算法（FKM），引入了一种新的流形学习技术，无需初始化聚类中心。此外，我们发展了记忆的概念。

    arXiv:2402.15688v1 Announce Type: new  Abstract: Anchor-based methods are a pivotal approach in handling clustering of large-scale data. However, these methods typically entail two distinct stages: selecting anchor points and constructing an anchor graph. This bifurcation, along with the initialization of anchor points, significantly influences the overall performance of the algorithm. To mitigate these issues, we introduce a novel method termed Anchor-free Clustering based on Anchor Graph Factorization (AFCAGF). AFCAGF innovates in learning the anchor graph, requiring only the computation of pairwise distances between samples. This process, achievable through straightforward optimization, circumvents the necessity for explicit selection of anchor points. More concretely, our approach enhances the Fuzzy k-means clustering algorithm (FKM), introducing a new manifold learning technique that obviates the need for initializing cluster centers. Additionally, we evolve the concept of the mem
    
[^131]: 克服图对比学习评估中的陷阱：走向全面基准

    Overcoming Pitfalls in Graph Contrastive Learning Evaluation: Toward Comprehensive Benchmarks

    [https://arxiv.org/abs/2402.15680](https://arxiv.org/abs/2402.15680)

    本文深入研究了图对比学习方法评估中的缺陷，并提出了对超参数选择和下游任务选择影响的新观点，同时引入了一个增强型评估框架。

    

    arXiv:2402.15680v1 公告类型：新摘要：自监督学习的兴起使得图学习社区对其产生了极大的兴趣，而自监督学习无需标记数据。这种热情导致了许多图对比学习（GCL）技术的发展，其目标都是创建一个多功能的图编码器，利用大量未标记数据进行各种下游任务。然而，目前用于评估GCL方法的评估标准存在缺陷，因为在预训练期间需要进行大量超参数调整，并且依赖单一的下游任务进行评估。这些缺陷可能使评估偏离预期目标，潜在导致误导性结论。在我们的论文中，我们彻底研究了这些缺陷，并提出了新的观点，即GCL方法如何受超参数选择和选择用于评估的下游任务的影响。此外，我们引入了一个增强型评估框架。

    arXiv:2402.15680v1 Announce Type: new  Abstract: The rise of self-supervised learning, which operates without the need for labeled data, has garnered significant interest within the graph learning community. This enthusiasm has led to the development of numerous Graph Contrastive Learning (GCL) techniques, all aiming to create a versatile graph encoder that leverages the wealth of unlabeled data for various downstream tasks. However, the current evaluation standards for GCL approaches are flawed due to the need for extensive hyper-parameter tuning during pre-training and the reliance on a single downstream task for assessment. These flaws can skew the evaluation away from the intended goals, potentially leading to misleading conclusions. In our paper, we thoroughly examine these shortcomings and offer fresh perspectives on how GCL methods are affected by hyper-parameter choices and the choice of downstream tasks for their evaluation. Additionally, we introduce an enhanced evaluation fr
    
[^132]: 使用随机投影的可扩展密度聚类

    Scalable Density-based Clustering with Random Projections

    [https://arxiv.org/abs/2402.15679](https://arxiv.org/abs/2402.15679)

    sDBSCAN是一种利用随机投影进行高维密度聚类的算法，在速度和准确性上显著优于其他算法。

    

    我们提出了一种名为sDBSCAN的算法，在高维空间中使用余弦距离进行可扩展的密度聚类。通过利用随机投影的保邻特性，sDBSCAN能够快速识别核心点及其邻域，这是密度聚类的主要障碍。在理论上，sDBSCAN在较轻的条件下以高概率输出类似于DBSCAN的聚类结构。为了进一步促进sDBSCAN，我们提出了sOPTICS，这是一种用于交互式探索内在聚类结构的可扩展OPTICS。我们还通过随机核特征将sDBSCAN和sOPTICS扩展到L2、L1、$\chi^2$和Jensen-Shannon距离。在实证方面，sDBSCAN在真实世界的百万数据集上比许多其他聚类算法显著更快，并提供更高的准确性。在这些数据集上，sDBSCAN和sOPTICS在几分钟内运行，而scikit-learn的对应算法需要数小时或由于内存不足而无法运行。

    arXiv:2402.15679v1 Announce Type: new  Abstract: We present sDBSCAN, a scalable density-based clustering algorithm in high dimensions with cosine distance. Utilizing the neighborhood-preserving property of random projections, sDBSCAN can quickly identify core points and their neighborhoods, the primary hurdle of density-based clustering. Theoretically, sDBSCAN outputs a clustering structure similar to DBSCAN under mild conditions with high probability. To further facilitate sDBSCAN, we present sOPTICS, a scalable OPTICS for interactive exploration of the intrinsic clustering structure. We also extend sDBSCAN and sOPTICS to L2, L1, $\chi^2$, and Jensen-Shannon distances via random kernel features. Empirically, sDBSCAN is significantly faster and provides higher accuracy than many other clustering algorithms on real-world million-point data sets. On these data sets, sDBSCAN and sOPTICS run in a few minutes, while the scikit-learn's counterparts demand several hours or cannot run due to m
    
[^133]: 在线客户服务中的通用模型

    Universal Model in Online Customer Service

    [https://arxiv.org/abs/2402.15666](https://arxiv.org/abs/2402.15666)

    本文介绍了一种在电子商务中改进在线客户服务的解决方案，即提出了一种基于客户问题预测标签的通用模型，无需进行训练，通过消除个别模型训练和维护的需求，减少了模型开发周期和成本。

    

    建立机器学习模型可能是一个耗时的过程，在 typcial 商业场景中常常需要数月来实现。为了确保模型性能的一致性并考虑到数据分布的变化，定期的重新训练是必需的。本文介绍了一种改进电子商务在线客户服务的解决方案，即提出了一种基于客户问题预测标签的通用模型，无需进行训练。我们的新方法涉及使用机器学习技术在对话中标记客户问题，并创建问题及相应标签的存储库。当客户请求帮助时，一个信息检索模型在存储库中搜索相似问题，并使用统计分析来预测相应的标签。通过消除个别模型训练和维护的需求，我们的方法减少了模型开发周期和成本。

    arXiv:2402.15666v1 Announce Type: cross  Abstract: Building machine learning models can be a time-consuming process that often takes several months to implement in typical business scenarios. To ensure consistent model performance and account for variations in data distribution, regular retraining is necessary. This paper introduces a solution for improving online customer service in e-commerce by presenting a universal model for predict-ing labels based on customer questions, without requiring training. Our novel approach involves using machine learning techniques to tag customer questions in transcripts and create a repository of questions and corresponding labels. When a customer requests assistance, an information retrieval model searches the repository for similar questions, and statistical analysis is used to predict the corresponding label. By eliminating the need for individual model training and maintenance, our approach reduces both the model development cycle and costs. The 
    
[^134]: 在智能路由中的师生学习复杂性

    Teacher-Student Learning on Complexity in Intelligent Routing

    [https://arxiv.org/abs/2402.15665](https://arxiv.org/abs/2402.15665)

    通过机器学习框架中的师生模型，成功预测客户联系的复杂性并将其引导到合适的代理商，提高了客户体验，并提出了复杂性AUC度量标准。

    

    客户服务通常是电子商务网站中最耗时的环节，每次联系通常需要花费10-15分钟。有效地将客户引导到合适的代理商，避免转接是电子商务成功的关键。为此，我们开发了一个机器学习框架，用于预测客户联系的复杂性并相应地将其引导到合适的代理商。该框架分为两部分。首先，我们训练一个师傅模型，根据联系后的对话文本评分来评估联系的复杂性。然后，我们使用师傅模型作为数据标注者，为训练一个只根据联系前数据预测复杂性的学生模型提供标签。我们的实验表明，这样的框架是成功的，并且可以显著改善客户体验。我们还提出了一个称为复杂性AUC的有用度量标准，用于在统计水平上评估客户服务的有效性。

    arXiv:2402.15665v1 Announce Type: cross  Abstract: Customer service is often the most time-consuming aspect for e-commerce websites, with each contact typically taking 10-15 minutes. Effectively routing customers to appropriate agents without transfers is therefore crucial for e-commerce success. To this end, we have developed a machine learning framework that predicts the complexity of customer contacts and routes them to appropriate agents accordingly. The framework consists of two parts. First, we train a teacher model to score the complexity of a contact based on the post-contact transcripts. Then, we use the teacher model as a data annotator to provide labels to train a student model that predicts the complexity based on pre-contact data only. Our experiments show that such a framework is successful and can significantly improve customer experience. We also propose a useful metric called complexity AUC that evaluates the effectiveness of customer service at a statistical level.
    
[^135]: 学习半线性神经算子：预测和数据同化的统一递归框架

    Learning Semilinear Neural Operators : A Unified Recursive Framework For Prediction And Data Assimilation

    [https://arxiv.org/abs/2402.15656](https://arxiv.org/abs/2402.15656)

    提出了一种学习半线性神经算子的方法，通过结合预测和校正操作实现了对长时间尺度上时空PDE的解进行处理与数据同化。

    

    最近神经算子（NOs）理论的进展使得能够快速而准确地计算由偏微分方程（PDEs）描述的复杂系统的解成为可能。尽管取得了巨大成功，但当前基于NO的解决方案在处理长时间尺度上的时空PDE时面临重要挑战。具体而言，当前的NO理论没有提出一个系统框架，以便根据稀疏采样的嘈杂测量有效地纠正PDE解的演化。本文提出了一种基于学习的状态空间方法来计算无限维半线性PDE的解算子。利用半线性PDE的结构和函数空间中的非线性观测者理论，我们开发了一种灵活的递归方法，通过结合预测和校正操作，允许同时进行预测和数据同化。

    arXiv:2402.15656v1 Announce Type: cross  Abstract: Recent advances in the theory of Neural Operators (NOs) have enabled fast and accurate computation of the solutions to complex systems described by partial differential equations (PDEs). Despite their great success, current NO-based solutions face important challenges when dealing with spatio-temporal PDEs over long time scales. Specifically, the current theory of NOs does not present a systematic framework to perform data assimilation and efficiently correct the evolution of PDE solutions over time based on sparsely sampled noisy measurements. In this paper, we propose a learning-based state-space approach to compute the solution operators to infinite-dimensional semilinear PDEs. Exploiting the structure of semilinear PDEs and the theory of nonlinear observers in function spaces, we develop a flexible recursive method that allows for both prediction and data assimilation by combining prediction and correction operations. The proposed 
    
[^136]: 客户服务中的联系复杂性

    Contact Complexity in Customer Service

    [https://arxiv.org/abs/2402.15655](https://arxiv.org/abs/2402.15655)

    开发了一种新颖的机器学习方法来定义联系复杂性，通过训练AI专家模型来评估客户问题复杂性，避免了人工标注的时间和金钱成本。

    

    联系客户服务支持的客户可能面临各种复杂程度不同的问题。将高复杂性的联系路由到初级代理可能导致多次转接或重复联系，而将低复杂性的联系路由到高级代理可能会使他们的专业帮助容量受到压力。为了解决这个问题，一种可以准确预测客户问题复杂性的机器学习模型至关重要。然而，定义联系的复杂性是一项困难的任务，因为它是一个高度抽象的概念。虽然经验代理进行基于共识的数据注释是一个可能的解决方案，但这需要耗费时间和金钱。为了克服这些障碍，我们开发了一种新颖的机器学习方法来定义联系复杂性。我们的方法不依赖于人工注释，而是训练一种AI专家模型来模拟代理的行为，并根据联系的行为评估每个联系的复杂性。

    arXiv:2402.15655v1 Announce Type: cross  Abstract: Customers who reach out for customer service support may face a range of issues that vary in complexity. Routing high-complexity contacts to junior agents can lead to multiple transfers or repeated contacts, while directing low-complexity contacts to senior agents can strain their capacity to assist customers who need professional help. To tackle this, a machine learning model that accurately predicts the complexity of customer issues is highly desirable. However, defining the complexity of a contact is a difficult task as it is a highly abstract concept. While consensus-based data annotation by experienced agents is a possible solution, it is time-consuming and costly. To overcome these challenges, we have developed a novel machine learning approach to define contact complexity. Instead of relying on human annotation, we trained an AI expert model to mimic the behavior of agents and evaluate each contact's complexity based on how the 
    
[^137]: 具有目标抑制的多约束安全强化学习用于安全关键应用

    Multi-Constraint Safe RL with Objective Suppression for Safety-Critical Applications

    [https://arxiv.org/abs/2402.15650](https://arxiv.org/abs/2402.15650)

    提出了一种目标抑制的新方法，可以在多约束安全领域中改进安全强化学习任务表现，实验证明此方法结合现有算法能够在减少约束违规的情况下实现与基准线相当的任务奖励水平。

    

    尽管在现实世界中非常常见，但具有多个约束条件的安全强化学习任务仍然是一个具有挑战性的领域。为了解决这一挑战，我们提出了一种新方法，即目标抑制，根据安全评判器自适应地抑制任务奖励最大化目标。我们在两个多约束安全领域中对目标抑制进行了基准测试，包括一个自动驾驶领域，在这个领域中任何错误的行为都可能导致灾难性后果。实证结果表明，我们提出的方法与现有的安全强化学习算法相结合，可以在显著减少约束违规的情况下匹配我们的基准线所达到的任务奖励。

    arXiv:2402.15650v1 Announce Type: cross  Abstract: Safe reinforcement learning tasks with multiple constraints are a challenging domain despite being very common in the real world. To address this challenge, we propose Objective Suppression, a novel method that adaptively suppresses the task reward maximizing objectives according to a safety critic. We benchmark Objective Suppression in two multi-constraint safety domains, including an autonomous driving domain where any incorrect behavior can lead to disastrous consequences. Empirically, we demonstrate that our proposed method, when combined with existing safe RL algorithms, can match the task reward achieved by our baselines with significantly fewer constraint violations.
    
[^138]: 多任务学习中的公平资源分配

    Fair Resource Allocation in Multi-Task Learning

    [https://arxiv.org/abs/2402.15638](https://arxiv.org/abs/2402.15638)

    通过将多任务学习的优化问题形式化为效用最大化问题，并提出FairGrad方法，实现了对不同任务损失递减的公平优化，同时具有理论收敛保证。

    

    通过联合学习多个任务，多任务学习（MTL）可以利用任务之间的共享知识，提高数据效率和泛化性能。然而，在MTL中的一个主要挑战在于存在冲突的梯度，这可能阻碍某些任务的公平优化，从而妨碍MTL实现更好的整体性能。受通信网络中公平资源分配的启发，我们将MTL的优化定式为一个效用最大化问题，其中在不同的公平度量下最大化跨任务的损失递减。为了解决这个问题，我们提出了一种新颖的MTL优化方法FairGrad。FairGrad不仅可以灵活地强调某些任务，而且可以实现理论收敛保证。大量实验证明，我们的方法在一系列梯度调整方法中可以实现最先进的性能。

    arXiv:2402.15638v1 Announce Type: new  Abstract: By jointly learning multiple tasks, multi-task learning (MTL) can leverage the shared knowledge across tasks, resulting in improved data efficiency and generalization performance. However, a major challenge in MTL lies in the presence of conflicting gradients, which can hinder the fair optimization of some tasks and subsequently impede MTL's ability to achieve better overall performance. Inspired by fair resource allocation in communication networks, we formulate the optimization of MTL as a utility maximization problem, where the loss decreases across tasks are maximized under different fairness measurements. To solve this problem, we propose FairGrad, a novel MTL optimization method. FairGrad not only enables flexible emphasis on certain tasks but also achieves a theoretical convergence guarantee. Extensive experiments demonstrate that our method can achieve state-of-the-art performance among gradient manipulation methods on a suite of
    
[^139]: 操作员学习中光滑稀疏的潜在动态与Jerk正则化

    Smooth and Sparse Latent Dynamics in Operator Learning with Jerk Regularization

    [https://arxiv.org/abs/2402.15636](https://arxiv.org/abs/2402.15636)

    引入了连续的操作员学习框架，将Jerk正则化纳入压缩潜在空间的学习中，促进了潜在空间动态的平滑性和稀疏性

    

    arXiv:2402.15636v1 Announce Type: new  Abstract: 时空建模对于理解跨越各种科学和工程学科的复杂系统至关重要，但由于系统固有的复杂性，控制方程通常不完全已知或在计算上难以处理。基于数据驱动的降阶模型（ROMs）通过在压缩的潜在空间中计算解决方案来提供一种快速准确的时空预测方法。然而，这些模型在构建潜在空间时通常忽略相邻快照之间的时间相关性，这导致压缩次优、锯齿状的潜在轨迹以及随时间有限的外推能力。为了解决这些问题，本文引入了一个连续的操作员学习框架，将Jerk正则化纳入到压缩潜在空间的学习中。这种Jerk正则化促进了潜在空间动态的平滑性和稀疏性，不仅提高了准确性和收敛性

    arXiv:2402.15636v1 Announce Type: new  Abstract: Spatiotemporal modeling is critical for understanding complex systems across various scientific and engineering disciplines, but governing equations are often not fully known or computationally intractable due to inherent system complexity. Data-driven reduced-order models (ROMs) offer a promising approach for fast and accurate spatiotemporal forecasting by computing solutions in a compressed latent space. However, these models often neglect temporal correlations between consecutive snapshots when constructing the latent space, leading to suboptimal compression, jagged latent trajectories, and limited extrapolation ability over time. To address these issues, this paper introduces a continuous operator learning framework that incorporates jerk regularization into the learning of the compressed latent space. This jerk regularization promotes smoothness and sparsity of latent space dynamics, which not only yields enhanced accuracy and conve
    
[^140]: 用于在斑点噪声存在情况下恢复图像的裹袋式深度图像先验

    Bagged Deep Image Prior for Recovering Images in the Presence of Speckle Noise

    [https://arxiv.org/abs/2402.15635](https://arxiv.org/abs/2402.15635)

    该论文在斑点噪声存在情况下提出了裹袋式深度图像先验（Bagged-DIP）的概念，并将其与投影梯度下降算法集成，以及通过在迭代中使用Newton-Schulz算法来减少算法的计算复杂度。

    

    我们研究基于似然的方法在斑点（乘性）噪声影响下从多组测量中恢复复杂值信号的理论和算法方面。我们的理论贡献包括建立在深度图像先验假设下，最大似然估计器的均方误差（MSE）的第一个理论上界。我们的理论结果捕捉了MSE与深度图像先验的参数数量、观测次数、信号维度和每次观测的测量次数之间的依赖关系。在算法方面，我们引入了裹袋式深度图像先验（Bagged-DIP）的概念，并将其与投影梯度下降算法集成。此外，我们展示了如何在PGD的迭代中使用Newton-Schulz算法计算矩阵逆，从而降低算法的计算复杂度。

    arXiv:2402.15635v1 Announce Type: cross  Abstract: We investigate both the theoretical and algorithmic aspects of likelihood-based methods for recovering a complex-valued signal from multiple sets of measurements, referred to as looks, affected by speckle (multiplicative) noise. Our theoretical contributions include establishing the first existing theoretical upper bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under the deep image prior hypothesis. Our theoretical results capture the dependence of MSE upon the number of parameters in the deep image prior, the number of looks, the signal dimension, and the number of measurements per look. On the algorithmic side, we introduce the concept of bagged Deep Image Priors (Bagged-DIP) and integrate them with projected gradient descent. Furthermore, we show how employing Newton-Schulz algorithm for calculating matrix inverses within the iterations of PGD reduces the computational complexity of the algorithm. We will 
    
[^141]: MegaScale: 将大型语言模型训练扩展到超过10,000个GPU

    MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs

    [https://arxiv.org/abs/2402.15627](https://arxiv.org/abs/2402.15627)

    MegaScale项目介绍了一个用于在超过10,000个GPU规模上训练大型语言模型的生产系统，通过全栈方法协同设计算法和系统组件来解决训练效率和稳定性挑战。

    

    我们介绍了设计、实施和工程经验，构建和部署了MegaScale，一个用于在超过10,000个GPU规模上训练大型语言模型（LLM）的生产系统。在这种规模上训练LLMs会给训练效率和稳定性带来前所未有的挑战。我们采取了一种全栈方法，跨模型块和优化器设计、计算和通信重叠、算子优化、数据管道和网络性能调优，协同设计算法和系统组件。在训练过程中保持高效率（即稳定性）是在生产中的一个重要考虑，考虑到LLM训练作业的长时间跨度。许多艰难的稳定性问题只有在大规模下才会出现，深入的可观察性是解决这些问题的关键。我们开发了一套诊断工具，用于监控系统组件和深层堆栈中的事件，识别根本原因，并得出有效的

    arXiv:2402.15627v1 Announce Type: new  Abstract: We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effectiv
    
[^142]: 从不完整数据中学习循环因果模型

    Learning Cyclic Causal Models from Incomplete Data

    [https://arxiv.org/abs/2402.15625](https://arxiv.org/abs/2402.15625)

    提出了一个名为MissNODAGS的框架，可以从部分缺失数据中学习循环因果图，通过交替替补缺失数据和最大化可见数据部分的预期对数似然来学习因果图。

    

    因果学习是统计学和科学中的一个基本问题，它可以帮助预测未见治疗对系统的影响。尽管最近在这个领域取得了进展，但大多数现有的因果发现算法都基于两个关键假设：(i) 潜在图是无环的，(ii) 可用数据是完整的。这些假设可能存在问题，因为许多现实世界中的系统包含反馈环路（例如生物系统），实际情况经常涉及缺失数据。在这项工作中，我们提出了一个名为MissNODAGS的新框架，用于从部分缺失数据中学习循环因果图。在加性噪声模型下，MissNODAGS通过在每个训练步骤中在替补缺失数据与最大化数据可见部分的预期对数似然之间交替学习因果图，遵循期望最大化（EM）框架的原则。

    arXiv:2402.15625v1 Announce Type: cross  Abstract: Causal learning is a fundamental problem in statistics and science, offering insights into predicting the effects of unseen treatments on a system. Despite recent advances in this topic, most existing causal discovery algorithms operate under two key assumptions: (i) the underlying graph is acyclic, and (ii) the available data is complete. These assumptions can be problematic as many real-world systems contain feedback loops (e.g., biological systems), and practical scenarios frequently involve missing data. In this work, we propose a novel framework, named MissNODAGS, for learning cyclic causal graphs from partially missing data. Under the additive noise model, MissNODAGS learns the causal graph by alternating between imputing the missing data and maximizing the expected log-likelihood of the visible part of the data in each training step, following the principles of the expectation-maximization (EM) framework. Through synthetic exper
    
[^143]: 基于语言的用户偏好推荐方法

    Language-Based User Profiles for Recommendation

    [https://arxiv.org/abs/2402.15623](https://arxiv.org/abs/2402.15623)

    通过使用以人类可读文本表示的用户偏好，提出了Language-based Factorization Model (LFM)，在冷启动环境中与传统方法相比取得了更好的表现

    

    大多数传统的推荐方法（如矩阵分解）将用户偏好表示为高维向量。不幸的是，这些向量缺乏可解释性和可控性，在冷启动环境下往往表现不佳。为了解决这些缺点，我们探索了使用以人类可读文本表示的用户偏好。我们提出了基于语言的因子分解模型（LFM），它本质上是一个编码器/解码器模型，其中编码器和解码器均为大型语言模型（LLM）。编码器LLM从用户的评分历史生成用户兴趣的简洁自然语言描述。解码器LLM使用这个简要描述来完成预测性的下游任务。我们在MovieLens数据集上评估了LFM方法，将其与矩阵分解和直接从用户评分历史预测的LLM模型进行了比较。在冷启动环境下，我们发现我们的方法能够...

    arXiv:2402.15623v1 Announce Type: new  Abstract: Most conventional recommendation methods (e.g., matrix factorization) represent user profiles as high-dimensional vectors. Unfortunately, these vectors lack interpretability and steerability, and often perform poorly in cold-start settings. To address these shortcomings, we explore the use of user profiles that are represented as human-readable text. We propose the Language-based Factorization Model (LFM), which is essentially an encoder/decoder model where both the encoder and the decoder are large language models (LLMs). The encoder LLM generates a compact natural-language profile of the user's interests from the user's rating history. The decoder LLM uses this summary profile to complete predictive downstream tasks. We evaluate our LFM approach on the MovieLens dataset, comparing it against matrix factorization and an LLM model that directly predicts from the user's rating history. In cold-start settings, we find that our method can h
    
[^144]: 通过预训练表示实现在NLP中的高效主动学习

    Towards Efficient Active Learning in NLP via Pretrained Representations

    [https://arxiv.org/abs/2402.15613](https://arxiv.org/abs/2402.15613)

    通过在主动学习循环中使用预训练LLMs的表示，可以显著加快标记数据获取的过程，并通过微调获得最佳性能，同时大大降低计算开销。

    

    大型语言模型（LLMs）的微调现在是文本分类中常用的方法，在许多应用中都能看到。当标记文档稀缺时，主动学习有助于节省注释工作，但需要在每次获取迭代时重新训练大规模模型。我们通过在主动学习循环中使用LLMs的预训练表示，显著加快了这一过程，一旦获得所需数量的标记数据，就可以对LLMs进行微调，以获得最佳性能。通过在常见的文本分类基准上验证，以预训练的BERT和RoBERTa作为基础，我们的策略产生了与在整个主动学习循环中微调相似的性能，但计算开销降低了数个数量级。使用我们的程序获取的数据可以跨预训练网络进行泛化，从而可以灵活选择最终模型或更新。

    arXiv:2402.15613v1 Announce Type: cross  Abstract: Fine-tuning Large Language Models (LLMs) is now a common approach for text classification in a wide range of applications. When labeled documents are scarce, active learning helps save annotation efforts but requires retraining of massive models on each acquisition iteration. We drastically expedite this process by using pretrained representations of LLMs within the active learning loop and, once the desired amount of labeled data is acquired, fine-tuning that or even a different pretrained LLM on this labeled data to achieve the best performance. As verified on common text classification benchmarks with pretrained BERT and RoBERTa as the backbone, our strategy yields similar performance to fine-tuning all the way through the active learning loop but is orders of magnitude less computationally expensive. The data acquired with our procedure generalizes across pretrained networks, allowing flexibility in choosing the final model or upda
    
[^145]: 基于数据/瞬间驱动的集体动态快速预测控制方法

    Data/moment-driven approaches for fast predictive control of collective dynamics

    [https://arxiv.org/abs/2402.15611](https://arxiv.org/abs/2402.15611)

    提出了基于数据/瞬间驱动的两种替代MPC的方法，实现大规模粒子系统的快速、实时反馈合成。

    

    在模型预测控制（MPC）框架中审查了针对大规模粒子系统的反馈控制合成。集体动态的高维特性阻碍了传统MPC算法的性能，这些算法基于每个时间步的快速在线动态优化。提出了两种替代方案。首先，讨论了使用监督学习技术离线近似最优反馈定律的方法。然后，审查了一种基于粒子集合的宏观量的顺序线性化动态的过程。这两种方法都避开了在线求解最优控制问题，实现了大规模粒子系统的快速、实时的反馈合成。数值实验评估了所提出算法的性能。

    arXiv:2402.15611v1 Announce Type: cross  Abstract: Feedback control synthesis for large-scale particle systems is reviewed in the framework of model predictive control (MPC). The high-dimensional character of collective dynamics hampers the performance of traditional MPC algorithms based on fast online dynamic optimization at every time step. Two alternatives to MPC are proposed. First, the use of supervised learning techniques for the offline approximation of optimal feedback laws is discussed. Then, a procedure based on sequential linearization of the dynamics based on macroscopic quantities of the particle ensemble is reviewed. Both approaches circumvent the online solution of optimal control problems enabling fast, real-time, feedback synthesis for large-scale particle systems. Numerical experiments assess the performance of the proposed algorithms.
    
[^146]: 基于机器学习的油井性能优化完井顺序

    Machine Learning-Based Completions Sequencing for Well Performance Optimization

    [https://arxiv.org/abs/2402.15608](https://arxiv.org/abs/2402.15608)

    该研究旨在开发能够准确预测12个月累计产量的有效机器学习模型，以优化油井性能。

    

    建立准确的田野发展参数以优化长期油田产量需要时间和精力，因为油井开发的复杂性，以及估算长期油井产量的不确定性。传统上，石油和天然气公司使用固有计算昂贵的模拟软件来预测产量。因此，机器学习方法最近被文献中利用作为一种有效的优化完井发展的替代方案，通过增强完井环境。该项目的主要目标是开发能够集成多维预测变量效应（即完井环境）以准确预测12个月累计产量的有效机器学习模型。

    arXiv:2402.15608v1 Announce Type: new  Abstract: Establishing accurate field development parameters to optimize long-term oil production takes time and effort due to the complexity of oil well development, and the uncertainty in estimating long-term well production. Traditionally, oil and gas companies use simulation software that are inherently computationally expensive to forecast production. Thus, machine learning approaches are recently utilized in literature as an efficient alternative to optimize well developments by enhancing completion conditions. The primary goal of this project is to develop effective machine-learning models that can integrate the effects of multidimensional predictive variables (i.e., completion conditions) to predict 12-Month Cumulative Production accurately.   Three predictive regression machine learning models are implemented for predicting 12-month cumulative oil production: Random Forest, Gradient Boosting, and Long Short-Term Memory Models. All three m
    
[^147]: 训练非线性Transformer进行高效上下文学习：理论学习和泛化分析

    Training Nonlinear Transformers for Efficient In-Context Learning: A Theoretical Learning and Generalization Analysis

    [https://arxiv.org/abs/2402.15607](https://arxiv.org/abs/2402.15607)

    本文首次提供了对具有非线性自注意力和非线性MLP的Transformers的训练动态以及由此产生的模型的ICL泛化能力的理论分析。

    

    基于Transformer的大型语言模型展现出了令人印象深刻的上下文学习能力，其中预训练模型可以处理新任务，而无需通过简单地增加查询与来自该任务的一些输入-输出示例来微调。尽管在实证上取得了成功，但由于分析Transformers中非凸训练问题的技术挑战，如非线性自注意力和非线性激活，训练Transformer以实现ICL及相应的ICL容量的机制大多不为人知。据我们所知，本文首次提供了对具有非线性自注意力和非线性MLP的Transformers的训练动态以及由此产生的模型的ICL泛化能力的理论分析。我们专注于一组二分类任务，通过使用来自这些任务子集的数据来训练Transformers，并量化各种因素的影响。

    arXiv:2402.15607v1 Announce Type: new  Abstract: Transformer-based large language models have displayed impressive in-context learning capabilities, where a pre-trained model can handle new tasks without fine-tuning by simply augmenting the query with some input-output examples from that task. Despite the empirical success, the mechanics of how to train a Transformer to achieve ICL and the corresponding ICL capacity is mostly elusive due to the technical challenges of analyzing the nonconvex training problems resulting from the nonlinear self-attention and nonlinear activation in Transformers. To the best of our knowledge, this paper provides the first theoretical analysis of the training dynamics of Transformers with nonlinear self-attention and nonlinear MLP, together with the ICL generalization capability of the resulting model. Focusing on a group of binary classification tasks, we train Transformers using data from a subset of these tasks and quantify the impact of various factors
    
[^148]: 差分隐私公平二元分类

    Differentially Private Fair Binary Classifications

    [https://arxiv.org/abs/2402.15603](https://arxiv.org/abs/2402.15603)

    该论文提出了一种差分隐私与公平性约束下的二元分类算法，通过解耦技术和差分隐私的引入，实现了在保证公平性的同时提升了隐私性能和效用保证。

    

    在本工作中，我们研究了在差分隐私和公平性约束下的二元分类。我们首先提出了一种基于解耦技术的算法，用于学习一个仅具有公平性保证的分类器。该算法接受针对不同人口群体训练的分类器，并生成一个满足统计平衡的单一分类器。然后，我们改进了该算法以纳入差分隐私。最终算法的性能在隐私、公平性和效用保证方面得到了严格检验。对Adult和信用卡数据集进行的实证评估表明，我们的算法在公平性保证方面优于现有技术，同时保持了相同水平的隐私和效用。

    arXiv:2402.15603v1 Announce Type: new  Abstract: In this work, we investigate binary classification under the constraints of both differential privacy and fairness. We first propose an algorithm based on the decoupling technique for learning a classifier with only fairness guarantee. This algorithm takes in classifiers trained on different demographic groups and generates a single classifier satisfying statistical parity. We then refine this algorithm to incorporate differential privacy. The performance of the final algorithm is rigorously examined in terms of privacy, fairness, and utility guarantees. Empirical evaluations conducted on the Adult and Credit Card datasets illustrate that our algorithm outperforms the state-of-the-art in terms of fairness guarantees, while maintaining the same level of privacy and utility.
    
[^149]: 基于分数的扩散模型的极小化最优性：超越密度下界假设

    Minimax Optimality of Score-based Diffusion Models: Beyond the Density Lower Bound Assumptions

    [https://arxiv.org/abs/2402.15602](https://arxiv.org/abs/2402.15602)

    该研究展示了基于分数的扩散模型的采样具有极小均方误差，可以获得扩散模型生成样本的总变差误差的上界，这突破了仅做次高斯假设的限制。

    

    我们从非参数统计的角度研究了在大样本场景下得分扩散模型抽样的渐近误差。我们展示了基于核的得分估计器可以实现对 $p_0*\mathcal{N}(0,t\boldsymbol{I}_d)$ 的得分函数的最优均方误差为 $\widetilde{O}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$，其中 $n$ 和 $d$ 分别代表样本大小和维度，$t$ 在上下受到 $n$ 的多项式的限制，并且 $p_0$ 是任意次亚高斯分布。因此，这导致在仅进行次高斯假设时，扩散模型生成的样本分布的总变差误差的上界为 $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$。如果此外，$p_0$ 属于 $\beta\le 2$ 的 $\beta$-Sobolev空间的非参数族，通过采用早停策略，我们得到该扩散模型的样本的分布的总变差误差的上界为 $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$。

    arXiv:2402.15602v1 Announce Type: cross  Abstract: We study the asymptotic error of score-based diffusion model sampling in large-sample scenarios from a non-parametric statistics perspective. We show that a kernel-based score estimator achieves an optimal mean square error of $\widetilde{O}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$ for the score function of $p_0*\mathcal{N}(0,t\boldsymbol{I}_d)$, where $n$ and $d$ represent the sample size and the dimension, $t$ is bounded above and below by polynomials of $n$, and $p_0$ is an arbitrary sub-Gaussian distribution. As a consequence, this yields an $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$ upper bound for the total variation error of the distribution of the sample generated by the diffusion model under a mere sub-Gaussian assumption. If in addition, $p_0$ belongs to the nonparametric family of the $\beta$-Sobolev space with $\beta\le 2$, by adopting an early stopping strategy, we obtain that the diffusion
    
[^150]: 基于路径HJB算子的随机系统神经最优控制器

    Neural optimal controller for stochastic systems via pathwise HJB operator

    [https://arxiv.org/abs/2402.15592](https://arxiv.org/abs/2402.15592)

    本文提出了一种基于路径HJB算子的随机系统神经最优控制器的算法，通过引入基于物理信息学习的方法解决高维随机控制问题，展示了其在各种应用程序上的性能。

    

    本文旨在基于基于物理信息学习和动态规划，为高维随机控制问题开发基于深度学习的算法。与依赖于Hamilton-Jacobi-Bellman（HJB）方程解的概率表示的经典深度学习方法不同，我们引入了与HJB方程相关的路径算子，从而可以定义一个基于物理信息学习的问题。根据最优控制是否具有显式表示，提出了两种数值方法来解决基于物理信息学习的问题。我们对截断、近似和优化误差如何影响这些方法的准确性进行了误差分析。通过在各种应用程序上展示数值结果，说明了所提算法的性能。

    arXiv:2402.15592v1 Announce Type: cross  Abstract: The aim of this work is to develop deep learning-based algorithms for high-dimensional stochastic control problems based on physics-informed learning and dynamic programming. Unlike classical deep learning-based methods relying on a probabilistic representation of the solution to the Hamilton--Jacobi--Bellman (HJB) equation, we introduce a pathwise operator associated with the HJB equation so that we can define a problem of physics-informed learning. According to whether the optimal control has an explicit representation, two numerical methods are proposed to solve the physics-informed learning problem. We provide an error analysis on how the truncation, approximation and optimization errors affect the accuracy of these methods. Numerical results on various applications are presented to illustrate the performance of the proposed algorithms.
    
[^151]: 从学术手稿的同行评审叙事中要求LLMs撰写元评论草案

    Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts

    [https://arxiv.org/abs/2402.15589](https://arxiv.org/abs/2402.15589)

    本文研究了使用不同类型/级别的提示来激发三种流行LLM，GPT-3.5、LLaMA2和PaLM2，在学术同行评审过程中自动生成元评论，并进行了详细的定性研究。

    

    学术同行评审过程中最重要但也最繁重的任务之一是撰写元评论，这涉及根据多位专家的同行评审叙事理解学术手稿的核心贡献、优点和缺点，然后将这些专家多视角的看法总结为简洁的整体概述。鉴于生成型AI，尤其是大型语言模型（LLMs）的最新重大发展，我们有充分的理由深入研究LLMs在学术同行评审环境中生成这种元评论的实用性。本文通过使用三种流行的LLM，即GPT-3.5、LLaMA2和PaLM2，执行案例研究，通过基于最近提出的TELeR分类法以不同类型/级别的提示促使它们自动生成元评论。最后，我们对LLM生成的元评论进行了详细的定性研究，并总结了我们的发现。

    arXiv:2402.15589v1 Announce Type: cross  Abstract: One of the most important yet onerous tasks in the academic peer-reviewing process is composing meta-reviews, which involves understanding the core contributions, strengths, and weaknesses of a scholarly manuscript based on peer-review narratives from multiple experts and then summarizing those multiple experts' perspectives into a concise holistic overview. Given the latest major developments in generative AI, especially Large Language Models (LLMs), it is very compelling to rigorously study the utility of LLMs in generating such meta-reviews in an academic peer-review setting. In this paper, we perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and PaLM2, to automatically generate meta-reviews by prompting them with different types/levels of prompts based on the recently proposed TELeR taxonomy. Finally, we perform a detailed qualitative study of the meta-reviews generated by the LLMs and summarize our findings and 
    
[^152]: 事件相机的状态空间模型

    State Space Models for Event Cameras

    [https://arxiv.org/abs/2402.15584](https://arxiv.org/abs/2402.15584)

    通过引入具有可学习时间尺度参数的状态空间模型（SSMs），以适应不同频率而无需重新训练网络，并研究了两种对抗混叠效应的策略，该方法训练速度快33%。

    

    如今，处理事件相机数据的最先进的深度神经网络首先将一段时间内的事件转换为稠密的网格状输入表示。因此，在部署推断频率高于它们训练时的频率（即时间窗口较小）时，它们表现出较差的泛化能力。我们通过引入具有可学习时间尺度参数的状态空间模型（SSMs）来应对这一挑战。这种设计适应不同频率而无需在不同频率下重新训练网络。此外，我们研究了两种对抗混叠效应的策略，当在更高频率下部署模型时。我们对我们的方法进行了全面评估，对抗基于RNN和Transformer架构的现有方法，包括Gen1和1 Mpx事件相机数据集。我们的结果表明，基于SSM的模型训练速度快33%，同时也表现出最小值。

    arXiv:2402.15584v1 Announce Type: cross  Abstract: Today, state-of-the-art deep neural networks that process event-camera data first convert a temporal window of events into dense, grid-like input representations. As such, they exhibit poor generalizability when deployed at higher inference frequencies (i.e., smaller temporal windows) than the ones they were trained on. We address this challenge by introducing state-space models (SSMs) with learnable timescale parameters to event-based vision. This design adapts to varying frequencies without the need to retrain the network at different frequencies. Additionally, we investigate two strategies to counteract aliasing effects when deploying the model at higher frequencies. We comprehensively evaluate our approach against existing methods based on RNN and Transformer architectures across various benchmarks, including Gen1 and 1 Mpx event camera datasets. Our results demonstrate that SSM-based models train 33% faster and also exhibit minima
    
[^153]: Cohere3D：利用时间一致性进行视觉自动驾驶无监督表征学习

    Cohere3D: Exploiting Temporal Coherence for Unsupervised Representation Learning of Vision-based Autonomous Driving

    [https://arxiv.org/abs/2402.15583](https://arxiv.org/abs/2402.15583)

    提出了Cohere3D，利用对比学习算法在长期输入序列中学习一致实例表征，有助于视觉自动驾驶中的多帧实例级对应关系。

    

    由于图像缺乏深度线索，对于基于视觉的自动驾驶中的感知、预测和规划成功，多帧输入非常重要。不同角度的观察使得能够从不同输入帧中识别相同实例来恢复3D对象状态。然而，自动驾驶场景的动态性导致相机在不同时间步长捕获的每个实例的外观和形状发生显着变化。因此，我们提出了一种新颖的对比学习算法Cohere3D，用于学习长期输入序列中的一致实例表征，能够抵抗距离和视角变化。所学的表征有助于下游任务中跨多个输入帧的实例级对应关系。在预训练阶段，利用来自LiDAR传感器的原始点云来构建长期时间对应关系。

    arXiv:2402.15583v1 Announce Type: cross  Abstract: Due to the lack of depth cues in images, multi-frame inputs are important for the success of vision-based perception, prediction, and planning in autonomous driving. Observations from different angles enable the recovery of 3D object states from 2D image inputs if we can identify the same instance in different input frames. However, the dynamic nature of autonomous driving scenes leads to significant changes in the appearance and shape of each instance captured by the camera at different time steps. To this end, we propose a novel contrastive learning algorithm, Cohere3D, to learn coherent instance representations in a long-term input sequence robust to the change in distance and perspective. The learned representation aids in instance-level correspondence across multiple input frames in downstream tasks. In the pretraining stage, the raw point clouds from LiDAR sensors are utilized to construct the long-term temporal correspondence fo
    
[^154]: 实现完全自监督的多音高估计

    Toward Fully Self-Supervised Multi-Pitch Estimation

    [https://arxiv.org/abs/2402.15569](https://arxiv.org/abs/2402.15569)

    提出一套自监督学习目标，用于多音高估计，训练完全卷积自动编码器生成多音高显著图，无需微调

    

    多音高估计是一个持续几十年的研究问题，涉及检测与多乐器混音中同时发生的音乐事件相关的音高活动。监督学习技术已经在任务的较窄特征描述上表现出色，但受到缺乏大规模和多样化的带有多音高标注的复音音乐数据集的限制。我们提出了一套自监督学习目标，用于多音高估计，这些目标鼓励支持围绕谐波中心、对音色转换不变以及对几何转换等变换等特性。这些目标足以训练完全基于卷积自动编码器，直接生成多音高显著图，无需任何微调。尽管仅在一组合成单音频样本上训练，我们的完全自监督框架具有泛化到 po

    arXiv:2402.15569v1 Announce Type: cross  Abstract: Multi-pitch estimation is a decades-long research problem involving the detection of pitch activity associated with concurrent musical events within multi-instrument mixtures. Supervised learning techniques have demonstrated solid performance on more narrow characterizations of the task, but suffer from limitations concerning the shortage of large-scale and diverse polyphonic music datasets with multi-pitch annotations. We present a suite of self-supervised learning objectives for multi-pitch estimation, which encourage the concentration of support around harmonics, invariance to timbral transformations, and equivariance to geometric transformations. These objectives are sufficient to train an entirely convolutional autoencoder to produce multi-pitch salience-grams directly, without any fine-tuning. Despite training exclusively on a collection of synthetic single-note audio samples, our fully self-supervised framework generalizes to po
    
[^155]: 具有希尔伯特表示的基础政策

    Foundation Policies with Hilbert Representations

    [https://arxiv.org/abs/2402.15567](https://arxiv.org/abs/2402.15567)

    该研究提出了一个新颖的无监督框架，用于从未标记的离线数据中预训练通用政策，以捕获多样化、最优、长时域行为。

    

    arXiv:2402.15567v1 进行类型：交叉 摘要：无监督和自监督目标，例如下一个令牌预测，已经使得可以从大量未标记的数据中预训练通用模型。然而，在强化学习（RL）中，从离线数据中找到一个真正通用且可扩展的无监督预训练目标以获取通用政策仍然是一个主要的开放问题。尽管已经提出了许多方法来实现通用的自监督RL，基于诸如基于目标的RL、行为克隆和无监督技能学习等原则，但这些方法在发现的行为多样性、需要高质量演示数据或缺乏明确的提示或适应机制以用于下游任务方面仍然存在局限性。在这项工作中，我们提出了一个新颖的无监督框架，用于预训练能够捕捉多样化、最优、长时域行为的通用政策，从未标记的离线数据中获取这些行为，以便它们可以快速适应

    arXiv:2402.15567v1 Announce Type: cross  Abstract: Unsupervised and self-supervised objectives, such as next token prediction, have enabled pre-training generalist models from large amounts of unlabeled data. In reinforcement learning (RL), however, finding a truly general and scalable unsupervised pre-training objective for generalist policies from offline data remains a major open question. While a number of methods have been proposed to enable generic self-supervised RL, based on principles such as goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such methods remain limited in terms of either the diversity of the discovered behaviors, the need for high-quality demonstration data, or the lack of a clear prompting or adaptation mechanism for downstream tasks. In this work, we propose a novel unsupervised framework to pre-train generalist policies that capture diverse, optimal, long-horizon behaviors from unlabeled offline data such that they can be quickly ada
    
[^156]: 通过调整临床设置中皮肤病分布差异来缩小人工智能泛化差距

    Closing the AI generalization gap by adjusting for dermatology condition distribution differences across clinical settings

    [https://arxiv.org/abs/2402.15566](https://arxiv.org/abs/2402.15566)

    本研究表明，在AI算法从未见数据源上评估时出现错误的主要原因是皮肤病分布的差异，而不是人口统计数据或图像捕捉模式，提出了一系列步骤来解决这一泛化差距。

    

    最近，人工智能（AI）算法在从临床照片中对皮肤病症进行分类方面取得了巨大进展。然而，在现实世界中，存在许多因素可能导致算法的泛化能力不足，关于这些算法在不同临床环境中的鲁棒性了解甚少。理解并克服这些限制将有助于开发出可以在各种临床环境中辅助诊断皮肤病症的通用型AI。在这项回顾性研究中，我们证明了皮肤病分布的差异才是导致当一个AI算法在来自先前未见数据的源上评估时出现错误的主要原因，而不是人口统计数据或图像捕捉模式。我们展示了一系列步骤来缩小这一泛化差距，需要关于新源数据的更多信息，范围从疾病的分布到富集数据的训练数据。

    arXiv:2402.15566v1 Announce Type: cross  Abstract: Recently, there has been great progress in the ability of artificial intelligence (AI) algorithms to classify dermatological conditions from clinical photographs. However, little is known about the robustness of these algorithms in real-world settings where several factors can lead to a loss of generalizability. Understanding and overcoming these limitations will permit the development of generalizable AI that can aid in the diagnosis of skin conditions across a variety of clinical settings. In this retrospective study, we demonstrate that differences in skin condition distribution, rather than in demographics or image capture mode are the main source of errors when an AI algorithm is evaluated on data from a previously unseen source. We demonstrate a series of steps to close this generalization gap, requiring progressively more information about the new source, ranging from the condition distribution to training data enriched for data
    
[^157]: 保证公平和透明性的公平多元自适应回归样条

    Fair Multivariate Adaptive Regression Splines for Ensuring Equity and Transparency

    [https://arxiv.org/abs/2402.15561](https://arxiv.org/abs/2402.15561)

    提出了一种基于多元自适应回归样条的公平预测模型，该模型在学习过程中融入了公平性评估。

    

    预测分析广泛应用于各个领域，包括教育，以指导决策并改善结果。然而，许多预测模型是专有的，研究人员和从业人员无法评估或修改，从而限制了它们的问责制和道德设计。此外，预测模型通常对使用它们的官员而言是不透明和难以理解的，降低了它们的信任度和实用性。此外，预测模型可能引入或加剧偏见和不公平，就像在社会的许多领域一样。因此，有必要提出一种透明、可解释和公平的预测模型，可以轻松被不同利益相关者采纳和调整。在本文中，我们提出了一种基于多元自适应回归样条（MARS）的公平预测模型，该模型在学习过程中融入了公平性评估。

    arXiv:2402.15561v1 Announce Type: new  Abstract: Predictive analytics is widely used in various domains, including education, to inform decision-making and improve outcomes. However, many predictive models are proprietary and inaccessible for evaluation or modification by researchers and practitioners, limiting their accountability and ethical design. Moreover, predictive models are often opaque and incomprehensible to the officials who use them, reducing their trust and utility. Furthermore, predictive models may introduce or exacerbate bias and inequity, as they have done in many sectors of society. Therefore, there is a need for transparent, interpretable, and fair predictive models that can be easily adopted and adapted by different stakeholders. In this paper, we propose a fair predictive model based on multivariate adaptive regression splines(MARS) that incorporates fairness measures in the learning process. MARS is a non-parametric regression model that performs feature selectio
    
[^158]: 深度神经网络总是理解并且这就是原因

    Deep Networks Always Grok and Here is Why

    [https://arxiv.org/abs/2402.15555](https://arxiv.org/abs/2402.15555)

    深度神经网络存在延迟泛化和延迟鲁棒性现象，在各种实际环境中普遍存在，并基于新的局部复杂度度量提供了解释。

    

    Grokking，或者延迟泛化，是指深度神经网络（DNN）在达到接近于零的训练误差后很长时间内才发生泛化的现象。先前的研究报告了在特定受控环境中出现grokking的情况，例如使用大范数参数初始化的DNN或者在算法数据集上训练的transformers。我们展示了grokking实际上更加普遍，并且在广泛的实际环境中呈现，例如在CIFAR10上训练的卷积神经网络（CNN）或者在Imagenette上训练的Resnet。我们引入了延迟鲁棒性的新概念，即DNN在插值和/或泛化之后对抗示例进行理解并变得鲁棒。我们针对DNN的输入-输出映射的局部复杂度提出了出现延迟泛化和延迟鲁棒性的解释。我们的局部复杂度衡量了DNN输入-输出映射的复杂程度。

    arXiv:2402.15555v1 Announce Type: cross  Abstract: Grokking, or delayed generalization, is a phenomenon where generalization in a deep neural network (DNN) occurs long after achieving near zero training error. Previous studies have reported the occurrence of grokking in specific controlled settings, such as DNNs initialized with large-norm parameters or transformers trained on algorithmic datasets. We demonstrate that grokking is actually much more widespread and materializes in a wide range of practical settings, such as training of a convolutional neural network (CNN) on CIFAR10 or a Resnet on Imagenette. We introduce the new concept of delayed robustness, whereby a DNN groks adversarial examples and becomes robust, long after interpolation and/or generalization. We develop an analytical explanation for the emergence of both delayed generalization and delayed robustness based on a new measure of the local complexity of a DNN's input-output mapping. Our local complexity measures the d
    
[^159]: HiMAP：用于大规模多智体路径规划的学习启发式策略

    HiMAP: Learning Heuristics-Informed Policies for Large-Scale Multi-Agent Pathfinding

    [https://arxiv.org/abs/2402.15546](https://arxiv.org/abs/2402.15546)

    HiMAP 是一种新颖的可伸缩方法，使用启发式引导的模仿学习在分散式训练中对用户智体路径规划进行了改进

    

    大规模多智体路径规划（MAPF）在多个领域都存在着重大挑战。随着系统的复杂性随着大量自治体同时操作而增长，高效和避免碰撞的协调变得至关重要。传统算法在可伸缩性方面通常表现不佳，特别是在复杂场景中。强化学习（RL）已经显示出潜力来解决MAPF的复杂性；然而，它也被发现在可伸缩性方面遇到困难，需要精细的实施、漫长的训练，而且通常表现出不稳定的收敛性，限制了其实际应用。在本文中，我们介绍了Heuristics-Informed Multi-Agent Pathfinding（HiMAP），这是一种新颖的可伸缩方法，采用以启发式引导的模仿学习方式来进行分散式训练。我们在小规模实例上进行训练，使用一个启发式策略作为教师，将每个单个智体观测信息映射到行动概率

    arXiv:2402.15546v1 Announce Type: cross  Abstract: Large-scale multi-agent pathfinding (MAPF) presents significant challenges in several areas. As systems grow in complexity with a multitude of autonomous agents operating simultaneously, efficient and collision-free coordination becomes paramount. Traditional algorithms often fall short in scalability, especially in intricate scenarios. Reinforcement Learning (RL) has shown potential to address the intricacies of MAPF; however, it has also been shown to struggle with scalability, demanding intricate implementation, lengthy training, and often exhibiting unstable convergence, limiting its practical application. In this paper, we introduce Heuristics-Informed Multi-Agent Pathfinding (HiMAP), a novel scalable approach that employs imitation learning with heuristic guidance in a decentralized manner. We train on small-scale instances using a heuristic policy as a teacher that maps each single agent observation information to an action prob
    
[^160]: 流式物联网数据与量子边缘：一个经典/量子机器学习应用案例

    Streaming IoT Data and the Quantum Edge: A Classic/Quantum Machine Learning Use Case

    [https://arxiv.org/abs/2402.15542](https://arxiv.org/abs/2402.15542)

    量子机器学习的集成面临挑战，本研究通过边缘计算探索了将量子机器学习应用于分布式计算的可能性。

    

    随着后摩尔时代的来临，科学界面临着解决当前数据密集型机器学习应用的需求挑战，这些应用是分布式计算中紧急数据分析的基石。量子机器学习可能是满足紧急数据分析需求增加的解决方案，提供潜在的理论加速和增加的空间效率。然而，诸如(1)从经典到量子领域的数据编码、(2)超参数调整，以及(3)量子硬件集成到分布式计算中的连续体等挑战限制了量子机器学习在紧急数据分析中的采用。在这项工作中，我们研究了边缘计算用于将量子机器学习集成到分布式计算连续体中的可能性，并确定了主要挑战和可能的解决方案。此外，我们还探讨了数据编码和超参数调整等方面。

    arXiv:2402.15542v1 Announce Type: cross  Abstract: With the advent of the Post-Moore era, the scientific community is faced with the challenge of addressing the demands of current data-intensive machine learning applications, which are the cornerstone of urgent analytics in distributed computing. Quantum machine learning could be a solution for the increasing demand of urgent analytics, providing potential theoretical speedups and increased space efficiency. However, challenges such as (1) the encoding of data from the classical to the quantum domain, (2) hyperparameter tuning, and (3) the integration of quantum hardware into a distributed computing continuum limit the adoption of quantum machine learning for urgent analytics. In this work, we investigate the use of Edge computing for the integration of quantum machine learning into a distributed computing continuum, identifying the main challenges and possible solutions. Furthermore, exploring the data encoding and hyperparameter tuni
    
[^161]: 评估ChatGPT用于垃圾邮件检测的性能

    Evaluating the Performance of ChatGPT for Spam Email Detection

    [https://arxiv.org/abs/2402.15537](https://arxiv.org/abs/2402.15537)

    该研究评估了ChatGPT在英文和中文电子邮件数据集中用于垃圾邮件检测的性能，并探讨了其在这一领域的潜力。

    

    电子邮件继续是专业和商业领域中至关重要且广泛使用的通信媒介。然而，垃圾邮件的普及给用户带来了重大挑战，扰乱了他们的日常工作并降低了生产率。因此，基于内容准确地识别和过滤垃圾邮件对网络安全至关重要。最近自然语言处理领域的发展，特别是大型语言模型如ChatGPT，在诸如问答和文本生成等任务中表现出色。然而，其在垃圾邮件识别方面的潜力尚未得到充分探索。为了填补这一空白，本研究尝试评估ChatGPT在英文和中文电子邮件数据集中用于垃圾邮件识别的能力。我们利用ChatGPT进行垃圾邮件检测，采用上下文学习，需要提示说明和少量示范。

    arXiv:2402.15537v1 Announce Type: cross  Abstract: Email continues to be a pivotal and extensively utilized communication medium within professional and commercial domains. Nonetheless, the prevalence of spam emails poses a significant challenge for users, disrupting their daily routines and diminishing productivity. Consequently, accurately identifying and filtering spam based on content has become crucial for cybersecurity. Recent advancements in natural language processing, particularly with large language models like ChatGPT, have shown remarkable performance in tasks such as question answering and text generation. However, its potential in spam identification remains underexplored. To fill in the gap, this study attempts to evaluate ChatGPT's capabilities for spam identification in both English and Chinese email datasets. We employ ChatGPT for spam email detection using in-context learning, which requires a prompt instruction and a few demonstrations. We also investigate how the t
    
[^162]: DiCoM -- 多元概念建模以增强胸部X射线研究的普适性

    DiCoM -- Diverse Concept Modeling towards Enhancing Generalizability in Chest X-Ray Studies

    [https://arxiv.org/abs/2402.15534](https://arxiv.org/abs/2402.15534)

    DiCoM是一种新颖的自监督训练范式，通过学习多元概念，有效表示胸部X射线数据，以应对医学成像预训练中与自然图像不同的挑战。

    

    胸部X线（CXR）是一种广泛应用的临床成像模态，在各种肺部和心脏相关疾病的诊断和预后中起着关键作用。传统的依赖放射学读片和监督学习的自动化临床诊断工具设计策略需要高质量注释训练数据，为了解决这一挑战，自监督预训练已被证明在许多下游视觉任务中胜过监督预训练，代表了该领域的重大突破。然而，医学成像预训练与自然图像（例如ImageNet）的预训练在很大程度上不同，因为临床图像具有独特属性。在这种背景下，我们介绍了多元概念建模（DiCoM），这是一种新颖的自监督训练范式，利用了学生教师框架来学习多元概念，从而有效表示CXR数据。

    arXiv:2402.15534v1 Announce Type: cross  Abstract: Chest X-Ray (CXR) is a widely used clinical imaging modality and has a pivotal role in the diagnosis and prognosis of various lung and heart related conditions. Conventional automated clinical diagnostic tool design strategies relying on radiology reads and supervised learning, entail the cumbersome requirement of high quality annotated training data. To address this challenge, self-supervised pre-training has proven to outperform supervised pre-training in numerous downstream vision tasks, representing a significant breakthrough in the field. However, medical imaging pre-training significantly differs from pre-training with natural images (e.g., ImageNet) due to unique attributes of clinical images. In this context, we introduce Diverse Concept Modeling (DiCoM), a novel self-supervised training paradigm that leverages a student teacher framework for learning diverse concepts and hence effective representation of the CXR data. Hence, e
    
[^163]: Chain-of-Specificity: 一种从大型语言模型中提取知识的逐步精化方法

    Chain-of-Specificity: An Iteratively Refining Method for Eliciting Knowledge from Large Language Models

    [https://arxiv.org/abs/2402.15526](https://arxiv.org/abs/2402.15526)

    Chain-of-Specificity (CoS)是一种从大型语言模型中提取知识的逐步精化方法，能够在输入指令中迭代强调特定约束，解锁知识并改进回应。

    

    大型语言模型(LLMs)展现出引人瞩目的生成能力，能够生成有价值的信息。然而，先前的研究发现，LLMs有时难以遵循具体的约束(如在特定地点或特定时间)，甚至有时会忽略这些约束，导致回应要么太笼统，要么不够满意。本文提出一种简单而有效的方法，名为Chain-of-Specificity (CoS)。具体而言，CoS迭代地强调输入指令中的具体约束，解锁LLMs内部的知识，并精化回应。

    arXiv:2402.15526v1 Announce Type: new  Abstract: Large Language Models (LLMs) exhibit remarkable generative capabilities, enabling the generation of valuable information. Despite these advancements, previous research found that LLMs sometimes struggle with adhering to specific constraints (e.g., in specific place or at specific time), at times even overlooking them, which leads to responses that are either too generic or not fully satisfactory. Existing approaches attempted to address this issue by decomposing or rewriting input instructions, yet they fall short in adequately emphasizing specific constraints and in unlocking the underlying knowledge (e.g., programming within the context of software development). In response, this paper proposes a simple yet effective method named Chain-of-Specificity (CoS). Specifically, CoS iteratively emphasizes the specific constraints in the input instructions, unlocks knowledge within LLMs, and refines responses. Experiments conducted on publicly 
    
[^164]: 图剪枝用于枚举最小不可满足子集

    Graph Pruning for Enumeration of Minimal Unsatisfiable Subsets

    [https://arxiv.org/abs/2402.15524](https://arxiv.org/abs/2402.15524)

    提出了一种图剪枝算法，通过基于图的学习模型预测修剪公式的部分，加速枚举最小不可满足子集，无需数据标记，也无需来自目标应用的训练数据，实验结果表明在各种基准测试中的有效性。

    

    寻找二进制约束的最小不可满足子集（MUSes）是超约束系统不可行性分析中的常见问题。然而，由于问题的指数搜索空间，枚举MUSes在实际应用中非常耗时。本文提出使用学习模型对公式进行修剪以加速MUS枚举。我们将公式表示为图，然后开发基于图的学习模型来预测应该修剪公式的哪一部分。重要的是，我们的算法不需要通过仅检查修剪后的公式的可满足性来进行数据标记。它甚至不需要来自目标应用的训练数据，因为它对具有不同分布的数据进行外推。在我们的实验中，我们将我们的算法与现有的MUS枚举器结合，并验证其在包括一组超出我们训练分布范围的实际问题在内的多个基准测试中的有效性。

    arXiv:2402.15524v1 Announce Type: new  Abstract: Finding Minimal Unsatisfiable Subsets (MUSes) of binary constraints is a common problem in infeasibility analysis of over-constrained systems. However, because of the exponential search space of the problem, enumerating MUSes is extremely time-consuming in real applications. In this work, we propose to prune formulas using a learned model to speed up MUS enumeration. We represent formulas as graphs and then develop a graph-based learning model to predict which part of the formula should be pruned. Importantly, our algorithm does not require data labeling by only checking the satisfiability of pruned formulas. It does not even require training data from the target application because it extrapolates to data with different distributions. In our experiments we combine our algorithm with existing MUS enumerators and validate its effectiveness in multiple benchmarks including a set of real-world problems outside our training distribution. The
    
[^165]: 基于知识和数据驱动服务的混合智能家居系统 - HKD-SHO

    HKD-SHO: A hybrid smart home system based on knowledge-based and data-driven services

    [https://arxiv.org/abs/2402.15521](https://arxiv.org/abs/2402.15521)

    提出了一个名为HKD-SHO的混合智能家居系统，将基于知识和基于机器学习的数据驱动服务有益地融合在一起，解决了智能家居系统中基于知识和数据驱动方法的问题。

    

    一个智能家居通过设置各种服务来实现。已经提出了几种用于创建智能家居服务的方法，可以分为基于知识和数据驱动的方法。然而，基于知识的方法通常需要居民的手动输入，如果所关注环境状态的物理现象复杂，而且居民不知道如何调整相关执行器以实现服务监视的状态的目标值，则可能会变得复杂。此外，我们感兴趣的基于机器学习的数据驱动方法就像黑匣子一样，无法向居民展示在哪些情况下某些服务提出了某些执行器状态。为了解决这些问题，我们提出了一个名为HKD-SHO（基于混合知识和数据驱动服务的智能家居系统）的混合系统。

    arXiv:2402.15521v1 Announce Type: new  Abstract: A smart home is realized by setting up various services. Several methods have been proposed to create smart home services, which can be divided into knowledge-based and data-driven approaches. However, knowledge-based approaches usually require manual input from the inhabitant, which can be complicated if the physical phenomena of the concerned environment states are complex, and the inhabitant does not know how to adjust related actuators to achieve the target values of the states monitored by services. Moreover, machine learning-based data-driven approaches that we are interested in are like black boxes and cannot show the inhabitant in which situations certain services proposed certain actuators' states. To solve these problems, we propose a hybrid system called HKD-SHO (Hybrid Knowledge-based and Data-driven services based Smart HOme system), where knowledge-based and machine learning-based data-driven services are profitably integra
    
[^166]: GLA-Grad：一种格里芬-林扩展波形生成扩散模型

    GLA-Grad: A Griffin-Lim Extended Waveform Generation Diffusion Model

    [https://arxiv.org/abs/2402.15516](https://arxiv.org/abs/2402.15516)

    GLA-Grad提出了一种新方案，利用格里芬-林算法（GLA）在波形生成扩散模型的每一步中来最小化条件错误，提高噪声扩散过程的效率。

    

    扩散模型越来越受到人们关注，用于各种信号生成任务，如语音或音乐合成。WaveGrad是一个成功的扩散模型，它有条件地使用梅尔声谱来指导扩散过程，以生成高保真音频。本文提出了一种新方案GLA-Grad，旨在最小化条件错误，提高噪声扩散过程的效率，该方案在每一步普通扩散过程中引入格里芬-林算法（GLA）等相位恢复算法。

    arXiv:2402.15516v1 Announce Type: cross  Abstract: Diffusion models are receiving a growing interest for a variety of signal generation tasks such as speech or music synthesis. WaveGrad, for example, is a successful diffusion model that conditionally uses the mel spectrogram to guide a diffusion process for the generation of high-fidelity audio. However, such models face important challenges concerning the noise diffusion process for training and inference, and they have difficulty generating high-quality speech for speakers that were not seen during training. With the aim of minimizing the conditioning error and increasing the efficiency of the noise diffusion process, we propose in this paper a new scheme called GLA-Grad, which consists in introducing a phase recovery algorithm such as the Griffin-Lim algorithm (GLA) at each step of the regular diffusion process. Furthermore, it can be directly applied to an already-trained waveform generation model, without additional training or fi
    
[^167]: 研究焦虑的生理特征的泛化能力

    Investigating the Generalizability of Physiological Characteristics of Anxiety

    [https://arxiv.org/abs/2402.15513](https://arxiv.org/abs/2402.15513)

    评估了焦虑和压力相关的生理特征在其他高激活情绪中的泛化能力。

    

    最近的研究表明，利用生理信号检测焦虑和压力的机器学习（ML）技术的有效性，但目前尚不清楚ML模型是否正在学习与压力特定相关的生理特征。为了解决这一歧义，我们评估了已被证明与焦虑和压力相关的生理特征的泛化能力，以适应高激活情绪。具体来说，我们检查了从以下三个数据集提取的特征：焦虑阶段数据集（APD），可穿戴压力和情感检测（WESAD），以及情绪连续标注信号（CASE）数据集的心电图（ECG）和皮电（EDA）信号。我们旨在通过统计回归分析，以及在语料库内部、跨语料库和留一语料库外的交叉验证，了解这些特征是特定于焦虑还是普遍于其他高激活情绪。

    arXiv:2402.15513v1 Announce Type: cross  Abstract: Recent works have demonstrated the effectiveness of machine learning (ML) techniques in detecting anxiety and stress using physiological signals, but it is unclear whether ML models are learning physiological features specific to stress. To address this ambiguity, we evaluated the generalizability of physiological features that have been shown to be correlated with anxiety and stress to high-arousal emotions. Specifically, we examine features extracted from electrocardiogram (ECG) and electrodermal (EDA) signals from the following three datasets: Anxiety Phases Dataset (APD), Wearable Stress and Affect Detection (WESAD), and the Continuously Annotated Signals of Emotion (CASE) dataset. We aim to understand whether these features are specific to anxiety or general to other high-arousal emotions through a statistical regression analysis, in addition to a within-corpus, cross-corpus, and leave-one-corpus-out cross-validation across instan
    
[^168]: AgentOhana：为有效智能体学习设计统一数据和训练流水线

    AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning

    [https://arxiv.org/abs/2402.15506](https://arxiv.org/abs/2402.15506)

    AgentOhana提供了一种统一数据和训练流水线的综合解决方案，有助于克服使用大型语言模型（LLMs）进行智能体任务时的挑战。

    

    由大型语言模型（LLMs）提供支持的自主智能体引起了重大研究关注。然而，充分利用LLMs的潜力进行基于智能体的任务面临困难，这是由于具有多轮轨迹的多样化数据源的异构性。在本文中，我们介绍AgentOhana作为解决这些挑战的综合解决方案。AgentOhana从不同环境中聚合智能体轨迹，涵盖了各种情景。它精心地将这些轨迹标准化和统一到一致的格式中，简化了为智能体训练优化的通用数据加载器的创建。通过数据统一，我们的训练流水线在不同数据源之间保持平衡，并在数据集划分和模型训练过程中保持设备之间的独立随机性。此外，我们还介绍了xLAM-v0.1，一个大动作模式

    arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action mode
    
[^169]: ArabianGPT：基于原生阿拉伯语的大型语言模型

    ArabianGPT: Native Arabic GPT-based Large Language

    [https://arxiv.org/abs/2402.15313](https://arxiv.org/abs/2402.15313)

    提出了ArabianGPT，这是一系列专门为阿拉伯语设计的基于Transformer的模型，包括大小和复杂性不同的ArabianGPT-0.1B和ArabianGPT-0.3B，帮助弥补了本土阿拉伯语大型语言模型的不足。

    

    英语和拉丁语为主导的大型语言模型（LLMs）的主导地位导致了本土阿拉伯语LLMs的显著不足。本文提出ArabianGPT，这是一系列基于Transformer的模型，专门为阿拉伯语设计而成。这些模型包括ArabianGPT-0.1B和ArabianGPT-0.3B，大小和复杂性不同，与阿拉伯语的微妙语言特征相契合。

    arXiv:2402.15313v1 Announce Type: cross  Abstract: The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax. Consequently, there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements. To address this gap, this paper proposes ArabianGPT, a series of transformer-based models within the ArabianLLM suite designed explicitly for Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing. Empirical results from fine-tuning t
    
[^170]: 神经隐式扫描体模型用于快速碰撞检测

    Neural Implicit Swept Volume Models for Fast Collision Detection

    [https://arxiv.org/abs/2402.15281](https://arxiv.org/abs/2402.15281)

    提出了一种新颖的神经隐式扫描体模型，能够连续表示任意运动，并结合了深度学习速度和几何碰撞检查的准确性保证。

    

    碰撞检测是运动规划中最耗时的操作之一。因此，越来越多的人开始探索利用机器学习技术加速碰撞检测和基于采样的运动规划。最近的一系列研究侧重于利用机器人几何体或机器人运动的扫描体的神经有符号距离函数。在此基础上，我们提出了一种新颖的神经隐式扫描体模型，首次连续表示由起始和目标配置参数化的任意运动。这使得可以快速计算任务空间中任意点到机器人运动的有符号距离。此外，我们提出了一种算法，将基于深度学习的有符号距离计算的速度与几何碰撞检查的强大准确性保证相结合。我们在模拟和真实世界的机器人实验中验证了我们的方法，并证明了…

    arXiv:2402.15281v1 Announce Type: cross  Abstract: Collision detection is one of the most time-consuming operations during motion planning. Thus, there is an increasing interest in exploring machine learning techniques to speed up collision detection and sampling-based motion planning. A recent line of research focuses on utilizing neural signed distance functions of either the robot geometry or the swept volume of the robot motion. Building on this, we present a novel neural implicit swept volume model that is the first to continuously represent arbitrary motions parameterized by their start and goal configurations. This allows to quickly compute signed distances for any point in the task space to the robot motion. Further, we present an algorithm combining the speed of the deep learning-based signed distance computations with the strong accuracy guarantees of geometric collision checkers. We validate our approach in simulated and real-world robotic experiments, and demonstrate that i
    
[^171]: 统一理解Grokking、双降和新兴能力：来自电路竞争视角的观点

    Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition

    [https://arxiv.org/abs/2402.15175](https://arxiv.org/abs/2402.15175)

    提供了一个全面框架，统一解释了Grokking、双重下降和新兴能力这三种现象，着重探讨了记忆和泛化电路之间的竞争。

    

    最近的研究揭示了深度学习中一些有趣的现象，如Grokking、双重下降以及大型语言模型中的新兴能力，这些挑战了人类的直觉，并对神经模型的更深入理解至关重要。在本文中，我们提出了一个全面的框架，提供了对这三种现象的统一观点，重点关注记忆和泛化电路之间的竞争。这种方法最初用于解释Grokking，我们在工作中将其扩展到了更广泛的模型大小和训练数据量。我们的框架勾勒出了四种不同的训练动态，每种都取决于模型大小和训练数据数量的不同组合。利用这一框架，我们对双重下降现象进行了详细分析，并提出了两个关于其发生的可验证预测，均得到我们实验结果的支持。此外，我们扩展了我们的框架

    arXiv:2402.15175v1 Announce Type: new  Abstract: Recent studies have uncovered intriguing phenomena in deep learning, such as grokking, double descent, and emergent abilities in large language models, which challenge human intuition and are crucial for a deeper understanding of neural models. In this paper, we present a comprehensive framework that provides a unified view of these three phenomena, focusing on the competition between memorization and generalization circuits. This approach, initially employed to explain grokking, is extended in our work to encompass a wider range of model sizes and training data volumes. Our framework delineates four distinct training dynamics, each depending on varying combinations of model size and training data quantity. Utilizing this framework, we provide a detailed analysis of the double descent phenomenon and propose two verifiable predictions regarding its occurrence, both substantiated by our experimental results. Moreover, we expand our framewo
    
[^172]: 重新审视远程监督命名实体识别：一个新的基准和简单方法

    Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach

    [https://arxiv.org/abs/2402.14948](https://arxiv.org/abs/2402.14948)

    提出了基于课程的正无标记学习 CuPUL 方法，能够显著降低嘈杂标签的影响，胜过现有方法

    

    本文深入探讨了在远程监督（DS-NER）框架下的命名实体识别（NER），主要挑战在于标签质量受到误差的影响，如假阳性、假阴性和正向类型错误。我们批判性地评估了当前DS-NER方法的有效性，使用了一个名为QTL的真实世界基准数据集，揭示它们的性能往往不符合预期。为了解决标签噪声普遍问题，我们引入了一种简单而有效的方法，基于课程的正无标记学习（CuPUL），在训练过程中策略性地从“易”和更清洁的样本开始，以增强模型对嘈杂样本的韧性。我们的实证结果突出了CuPUL减少嘈杂标签影响并胜过现有方法的能力。

    arXiv:2402.14948v1 Announce Type: new  Abstract: This paper delves into Named Entity Recognition (NER) under the framework of Distant Supervision (DS-NER), where the main challenge lies in the compromised quality of labels due to inherent errors such as false positives, false negatives, and positive type errors. We critically assess the efficacy of current DS-NER methodologies using a real-world benchmark dataset named QTL, revealing that their performance often does not meet expectations. To tackle the prevalent issue of label noise, we introduce a simple yet effective approach, Curriculum-based Positive-Unlabeled Learning CuPUL, which strategically starts on "easy" and cleaner samples during the training process to enhance model resilience to noisy samples. Our empirical results highlight the capability of CuPUL to significantly reduce the impact of noisy labels and outperform existing methods.
    
[^173]: Vygotsky Distance: 用于基准任务相似性的度量方法

    Vygotsky Distance: Measure for Benchmark Task Similarity

    [https://arxiv.org/abs/2402.14890](https://arxiv.org/abs/2402.14890)

    论文提出了一种基于相对性能而非任务属性的相似性度量方法，即“维果茨基距离”，可帮助减少评估任务数量并保持高验证质量。

    

    论文介绍了一种理论工具和实践算法来计算基准任务之间的相似性，称之为"维果茨基距离"。这种相似性度量的核心思想是基于“学生”在给定任务上的相对表现，而不是基于任务本身的属性。如果两个任务在维果茨基距离上彼此接近，模型在这些任务上 tend to have similar relative performance。因此，通过了解任务之间的维果茨基距离，可以显著减少评估任务数量，同时保持高验证质量。

    arXiv:2402.14890v1 Announce Type: cross  Abstract: Evaluation plays a significant role in modern natural language processing. Most modern NLP benchmarks consist of arbitrary sets of tasks that neither guarantee any generalization potential for the model once applied outside the test set nor try to minimize the resource consumption needed for model evaluation. This paper presents a theoretical instrument and a practical algorithm to calculate similarity between benchmark tasks, we call this similarity measure "Vygotsky distance". The core idea of this similarity measure is that it is based on relative performance of the "students" on a given task, rather that on the properties of the task itself. If two tasks are close to each other in terms of Vygotsky distance the models tend to have similar relative performance on them. Thus knowing Vygotsky distance between tasks one can significantly reduce the number of evaluation tasks while maintaining a high validation quality. Experiments on v
    
[^174]: 用于公共卫生中动态不安静多臂老虎机任务的决策语言模型（DLM）

    A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health

    [https://arxiv.org/abs/2402.14807](https://arxiv.org/abs/2402.14807)

    提出了一种决策语言模型DLM，旨在通过使用LLMs作为自动规划器，动态微调RMAB策略，以应对公共卫生中具有挑战性的情境。

    

    旨在降低孕产妇死亡率的努力在很大程度上依赖于预防保健计划，向高风险人群传播重要的健康信息。本文提出了DLM：一种用于RMAB的决策语言模型，旨在通过使用LLMs作为自动规划器，动态微调RMAB策略，以应对公共卫生中具有挑战性的情境。

    arXiv:2402.14807v1 Announce Type: cross  Abstract: Efforts to reduce maternal mortality rate, a key UN Sustainable Development target (SDG Target 3.1), rely largely on preventative care programs to spread critical health information to high-risk populations. These programs face two important challenges: efficiently allocating limited health resources to large beneficiary populations, and adapting to evolving policy priorities. While prior works in restless multi-armed bandit (RMAB) demonstrated success in public health allocation tasks, they lack flexibility to adapt to evolving policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept, automated planners in various domains, including robotic control and navigation. In this paper, we propose DLM: a Decision Language Model for RMABs. To enable dynamic fine-tuning of RMAB policies for challenging public health settings using human-language commands, we propose using LLMs as automated planners to (1) interpret hu
    
[^175]: 回归基础: 重新审视LLMs中学习人类反馈的REINFORCE风格优化

    Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs

    [https://arxiv.org/abs/2402.14740](https://arxiv.org/abs/2402.14740)

    在大型语言模型中，重新审视REINFORCE风格优化对于学习人类反馈具有重要意义，简化优化方法可以提高性能。

    

    arXiv:2402.14740v1 公告类型: 新的 摘要: AI对齐被视为大型语言模型中从人类反馈中学习的至关重要组成部分。 \textsc{Proximal Policy Optimization} (PPO)已被最新文献定位为RLHF中RL部分的典范方法。然而，它既涉及高计算成本又涉及敏感的超参数调整。我们认为触发了PPO发展的大多数动机原则在RLHF中并非实践上的关注重点，并提倡一种更少计算消耗的方法，该方法保持甚至提高了性能。我们重新审视了在RL的环境中根据人类偏好进行对齐的\textit{公式}。以简单性为指导原则，我们展示了PPO的许多组件在RLHF环境中是不必要的，并且远远更简单的REINFORCE风格的优化变体表现出比PPO和新提出的“RL-free”方法更好的性能。

    arXiv:2402.14740v1 Announce Type: new  Abstract: AI alignment in the shape of Reinforcement Learning from Human Feedback (RLHF) is increasingly treated as a crucial ingredient for high performance large language models. \textsc{Proximal Policy Optimization} (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. However, it involves both high computational cost and sensitive hyperparameter tuning. We posit that most of the motivational principles that led to the development of PPO are less of a practical concern in RLHF and advocate for a less computationally expensive method that preserves and even increases performance. We revisit the \textit{formulation} of alignment from human preferences in the context of RL. Keeping simplicity as a guiding principle, we show that many components of PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style optimization variants outperform both PPO and newly proposed "RL-free" methods such a
    
[^176]: 联邦式复杂查询答案方法研究

    Federated Complex Qeury Answering

    [https://arxiv.org/abs/2402.14609](https://arxiv.org/abs/2402.14609)

    研究了在多源知识图谱上回答复杂查询的联邦式方法，解决了知识图谱中的隐私保护和答案检索的挑战

    

    知识图谱中的复杂逻辑查询答案是一个具有挑战性的任务，已经得到广泛研究。执行复杂逻辑推理的能力是必不可少的，并支持各种基于图推理的下游任务，比如搜索引擎。最近提出了一些方法，将知识图谱实体和逻辑查询表示为嵌入向量，并从知识图谱中找到逻辑查询的答案。然而，现有的方法主要集中在查询单个知识图谱上，并不能应用于多个图形。此外，直接共享带有敏感信息的知识图谱可能会带来隐私风险，使得共享和构建一个聚合知识图谱用于推理以检索查询答案是不切实际的。因此，目前仍然不清楚如何在多源知识图谱上回答查询。一个实体可能涉及到多个知识图谱，对多个知识图谱进行推理，并在多源知识图谱上回答复杂查询对于发现知识是重要的。

    arXiv:2402.14609v1 Announce Type: cross  Abstract: Complex logical query answering is a challenging task in knowledge graphs (KGs) that has been widely studied. The ability to perform complex logical reasoning is essential and supports various graph reasoning-based downstream tasks, such as search engines. Recent approaches are proposed to represent KG entities and logical queries into embedding vectors and find answers to logical queries from the KGs. However, existing proposed methods mainly focus on querying a single KG and cannot be applied to multiple graphs. In addition, directly sharing KGs with sensitive information may incur privacy risks, making it impractical to share and construct an aggregated KG for reasoning to retrieve query answers. Thus, it remains unknown how to answer queries on multi-source KGs. An entity can be involved in various knowledge graphs and reasoning on multiple KGs and answering complex queries on multi-source KGs is important in discovering knowledge 
    
[^177]: 从大规模到小规模数据集：用于聚类算法选择的尺寸泛化

    From Large to Small Datasets: Size Generalization for Clustering Algorithm Selection

    [https://arxiv.org/abs/2402.14332](https://arxiv.org/abs/2402.14332)

    通过引入尺寸泛化概念，研究了在半监督设置下的聚类算法选择问题，提出了能够在小实例上保证准确度最高的算法也将在原始大实例上拥有最高准确度的条件。

    

    在聚类算法选择中，我们会得到一个大规模数据集，并要有效地选择要使用的聚类算法。我们在半监督设置下研究了这个问题，其中有一个未知的基准聚类，我们只能通过昂贵的oracle查询来访问。理想情况下，聚类算法的输出将与基本事实结构上接近。我们通过引入一种聚类算法准确性的尺寸泛化概念来解决这个问题。我们确定在哪些条件下我们可以（1）对大规模聚类实例进行子采样，（2）在较小实例上评估一组候选算法，（3）保证在小实例上准确度最高的算法将在原始大实例上拥有最高的准确度。我们为三种经典聚类算法提供了理论尺寸泛化保证：单链接、k-means++和Gonzalez的k中心启发式（一种平滑的变种）。

    arXiv:2402.14332v1 Announce Type: new  Abstract: In clustering algorithm selection, we are given a massive dataset and must efficiently select which clustering algorithm to use. We study this problem in a semi-supervised setting, with an unknown ground-truth clustering that we can only access through expensive oracle queries. Ideally, the clustering algorithm's output will be structurally close to the ground truth. We approach this problem by introducing a notion of size generalization for clustering algorithm accuracy. We identify conditions under which we can (1) subsample the massive clustering instance, (2) evaluate a set of candidate algorithms on the smaller instance, and (3) guarantee that the algorithm with the best accuracy on the small instance will have the best accuracy on the original big instance. We provide theoretical size generalization guarantees for three classic clustering algorithms: single-linkage, k-means++, and (a smoothed variant of) Gonzalez's k-centers heuris
    
[^178]: 使用机器学习注意力模型进行时间偏差校正

    A Temporal Bias Correction using a Machine Learning Attention model

    [https://arxiv.org/abs/2402.14169](https://arxiv.org/abs/2402.14169)

    本论文提出了一种新颖的偏差校正方法，将校准视为概率模型而不是算法流程，利用机器学习概率注意力模型来适配偏差校正任务，可准确校正具有长期时间属性的气候统计数据，提高了在这些数据上进行可靠影响研究的准确性。

    

    气候模型在与真实世界观测数据相比存在偏差，通常需要在影响研究之前进行校准。使校准成为可能的统计方法集合被称为偏差校正（BC）。然而，当前的BC方法在调整时间偏差方面存在困难，因为它们忽略了连续时间点之间的依赖关系。因此，具有长期时间属性的气候统计数据（如热浪持续时间和频率）无法准确校正，这使得在这些气候统计数据上进行可靠影响研究变得更加困难。本文提出了一种新颖的BC方法来校正时间偏差。这得益于将BC重新构想为概率模型而不是算法流程，并将最先进的机器学习（ML）概率关注模型调整到BC任务中。通过尼日利亚阿布贾的热浪持续时间统计案例研究...

    arXiv:2402.14169v1 Announce Type: new  Abstract: Climate models are biased with respect to real world observations and usually need to be calibrated prior to impact studies. The suite of statistical methods that enable such calibrations is called bias correction (BC). However, current BC methods struggle to adjust for temporal biases, because they disregard the dependence between consecutive time-points. As a result, climate statistics with long-range temporal properties, such as heatwave duration and frequency, cannot be corrected accurately, making it more difficult to produce reliable impact studies on such climate statistics. In this paper, we offer a novel BC methodology to correct for temporal biases. This is made possible by i) re-thinking BC as a probability model rather than an algorithmic procedure, and ii) adapting state-of-the-art machine-learning (ML) probabilistic attention models to fit the BC task. With a case study of heatwave duration statistics in Abuja, Nigeria, and
    
[^179]: SDXL-Lightning: 渐进式对抗性扩散蒸馏

    SDXL-Lightning: Progressive Adversarial Diffusion Distillation

    [https://arxiv.org/abs/2402.13929](https://arxiv.org/abs/2402.13929)

    提出了一种结合渐进和对抗性蒸馏的扩散蒸馏方法，在文本到图像生成任务中取得了新的最先进结果，并开源了相应模型。

    

    我们提出了一种扩散蒸馏方法，在基于SDXL的一步/几步1024像素文本到图像生成任务中实现了全新的最先进水平。我们的方法结合了渐进和对抗性蒸馏，实现了质量和模式覆盖之间的平衡。本文讨论了理论分析、判别器设计、模型公式和训练技巧。我们以LoRA和完整UNet权重的形式开源了我们的蒸馏SDXL-Lightning模型。

    arXiv:2402.13929v1 Announce Type: cross  Abstract: We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.
    
[^180]: BenchCloudVision: 云检测和分割中深度学习方法的基准分析

    BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery

    [https://arxiv.org/abs/2402.13918](https://arxiv.org/abs/2402.13918)

    本文通过对遥感图像中的云进行分割，旨在提高卫星图像分析的精度和效率，应用于环境监测、资源管理和灾害响应。

    

    配备光学传感器的卫星捕获高分辨率图像，为各种环境现象提供了宝贵的见解。近年来，针对遥感中一些挑战的研究激增，从不同景观中的水检测到山地和地形的分割。正在进行的研究旨在提高卫星图像分析的精度和效率。尤其是，越来越多的重点放在开发准确的水体检测、雪和云的方法上，这些对环境监测、资源管理和灾害响应至关重要。在这个背景下，本文专注于从遥感图像中分割云。由于光学传感器应用中云的存在，准确的遥感数据分析可能具有挑战性。由此产生的产品（如应用和研究）的质量直接受到影响。

    arXiv:2402.13918v1 Announce Type: cross  Abstract: Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena. In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains. Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis. Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response. Within this context, this paper focus on the cloud segmentation from remote sensing imagery. Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications. The quality of resulting products such as applications and research is directl
    
[^181]: 离线策略学习的深度生成模型：教程、调查和未来方向展望

    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions

    [https://arxiv.org/abs/2402.13777](https://arxiv.org/abs/2402.13777)

    深度生成模型在离线策略学习中展现了巨大潜力，本文提供了首个系统性综述，涵盖了五种主流深度生成模型及其应用。

    

    深度生成模型(DGMs)在各个领域展示了巨大成功，特别是在使用从离线数据训练的模型生成文本、图像和视频方面。类似地，基于数据驱动的决策和机器人控制也需要从离线数据中学习一个生成函数作为策略或政策。在这种情况下，将深度生成模型应用于离线策略学习展现出巨大潜力，许多研究在这个方向上进行了探索。然而，这一领域仍然缺乏全面的评估，因此不同分支的发展相对独立。因此，我们提供了深度生成模型在离线策略学习应用方面的第一次系统性综述。具体而言，我们涵盖了五种主流深度生成模型，包括变分自动编码器、生成对抗网络、归一化流、变压器和扩散模型，以及它们的应用。

    arXiv:2402.13777v1 Announce Type: cross  Abstract: Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applicati
    
[^182]: HyperMoE: 通过专家之间的知识传递实现更好的专家混合

    HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts

    [https://arxiv.org/abs/2402.12656](https://arxiv.org/abs/2402.12656)

    HyperMoE通过Hypernetworks框架整合知识传递的概念，解决了在专家选择过程中专家知识稀疏性和可用性之间的矛盾。

    

    混合专家(MoE)在语言模型中被证明有效地增强了模型的能力，通过动态地将每个输入标记路由到特定的专家子集进行处理。尽管取得了成功，但大多数现有方法在专家知识的稀疏性和可用性之间面临挑战：通过增加对专家知识的使用来增强性能，往往会导致在专家选择过程中稀疏度减少。为了缓解这一矛盾，我们提出了HyperMoE，这是一个建立在Hypernetworks之上的新颖MoE框架。该框架将MoE的计算过程与多任务学习中的知识传递概念进行了集成。基于未选择专家信息生成的特定模块作为补充信息，允许未被选中的专家的知识在保持选择稀疏性的同时被使用。

    arXiv:2402.12656v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multi
    
[^183]: 针对个性化联邦无监督学习的分层贝叶斯方法

    Hierarchical Bayes Approach to Personalized Federated Unsupervised Learning

    [https://arxiv.org/abs/2402.12537](https://arxiv.org/abs/2402.12537)

    该论文提出了基于分层贝叶斯统计框架的算法，用于个性化无监督学习，其中开发了适应性算法来平衡利用有限本地数据和协作信息。

    

    客户本地数据的统计异质性是联邦学习中的重要特征，其促使个性化算法针对本地数据统计量进行定制。尽管已经提出了大量针对个性化监督学习的算法，但通过个性化无监督学习发现本地数据的结构却很少被探索。我们通过基于层次贝叶斯统计框架启动了对这种个性化无监督学习的系统研究。我们开发了基于优化标准的算法，这些算法受启发于层次贝叶斯统计框架。我们开发了适应性算法，发现了利用有限本地数据和协作信息之间的平衡。我们在两个无监督学习任务的背景下进行了这项工作：个性化降维和个性化扩散模型。我们为我们的自适应算法开发了收敛分析，这些分析展示了对问题参数（例如，异质性）的依赖性。

    arXiv:2402.12537v1 Announce Type: new  Abstract: Statistical heterogeneity of clients' local data is an important characteristic in federated learning, motivating personalized algorithms tailored to the local data statistics. Though there has been a plethora of algorithms proposed for personalized supervised learning, discovering the structure of local data through personalized unsupervised learning is less explored. We initiate a systematic study of such personalized unsupervised learning by developing algorithms based on optimization criteria inspired by a hierarchical Bayesian statistical framework. We develop adaptive algorithms that discover the balance between using limited local data and collaborative information. We do this in the context of two unsupervised learning tasks: personalized dimensionality reduction and personalized diffusion models. We develop convergence analyses for our adaptive algorithms which illustrate the dependence on problem parameters (e.g., heterogeneity
    
[^184]: AnyGPT：统一的多模式离散序列建模语言模型

    AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling

    [https://arxiv.org/abs/2402.12226](https://arxiv.org/abs/2402.12226)

    AnyGPT是一个统一的多模态语言模型，通过离散表示实现各种模态的统一处理，能够在不改变大型语言模型架构或训练方式的情况下稳定训练，为新模态的无缝整合提供了可能。

    

    我们介绍了 AnyGPT，这是一个任意多模式语言模型，利用离散表示统一处理各种模态，包括语音、文本、图像和音乐。AnyGPT 可以稳定训练，无需对当前大型语言模型（LLM）架构或训练范式进行任何改动。相反，它仅依赖于数据级预处理，促进了新模态的无缝集成到LLM中，类似于新语言的整合。我们构建了一个多模式文本中心的数据集，用于多模式对齐预训练。利用生成模型，我们合成了第一个大规模任意多模式指令数据集。它包括108k个多轮对话示例，精细地交织各种模态，从而使模型能够处理多模态输入和输出的任意组合。实验结果表明，AnyGPT能够促进...

    arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
    
[^185]: Mafin: 用模型增强微调来增强黑盒嵌入

    Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning

    [https://arxiv.org/abs/2402.12177](https://arxiv.org/abs/2402.12177)

    Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。

    

    检索增强生成（RAG）已经成为缓解大型语言模型（LLMs）中幻觉的有效解决方案。RAG中的检索阶段通常涉及预训练的嵌入模型，将查询和段落转换为向量以捕获它们的语义。然而，当应用于特定领域知识时，标准的预训练嵌入模型可能表现出次优性能，需要进行微调。本文解决了仅能从黑盒模型获取嵌入的情况。我们引入了模型增强微调（Mafin）--一种通过用可训练的嵌入模型增强黑盒嵌入模型来进行微调的新方法。我们的结果表明，Mafin仅需要训练一个小的增强模型就可以显著提高黑盒嵌入的性能。我们验证了我们的方法在有标签和无标签数据集上的有效性。

    arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
    
[^186]: Leaky ReLU对超参数网络的训练和泛化的影响

    The effect of Leaky ReLUs on the training and generalization of overparameterized networks

    [https://arxiv.org/abs/2402.11942](https://arxiv.org/abs/2402.11942)

    Leaky ReLU参数$\alpha=-1$在训练误差和泛化误差界方面是最优的选择。

    

    我们研究了具有各种泄漏修正线性单元（ReLU）函数的超参数神经网络（NNs）的训练和泛化误差。更具体地，我们仔细地对这些NNs的训练误差的收敛速率和泛化误差进行了上界估计，并研究了这些界限对Leaky ReLU参数$\alpha$的依赖性。我们表明$\alpha=-1$，对应于绝对值激活函数，对于训练误差界是最优的。此外，在特定设置中，这也是泛化误差界的最优选择。数值实验在实践中支持了理论引导的实际选择。

    arXiv:2402.11942v1 Announce Type: new  Abstract: We investigate the training and generalization errors of overparameterized neural networks (NNs) with a wide class of leaky rectified linear unit (ReLU) functions. More specifically, we carefully upper bound both the convergence rate of the training error and the generalization error of such NNs and investigate the dependence of these bounds on the Leaky ReLU parameter, $\alpha$. We show that $\alpha =-1$, which corresponds to the absolute value activation function, is optimal for the training error bound. Furthermore, in special settings, it is also optimal for the generalization error bound. Numerical experiments empirically support the practical choices guided by the theory.
    
[^187]: 生成万花筒网络

    Generative Kaleidoscopic Networks

    [https://arxiv.org/abs/2402.11793](https://arxiv.org/abs/2402.11793)

    发现深层ReLU网络表现出过度泛化现象，利用这一特性设计了“生成万花筒网络”，通过递归映射随机输入噪声生成样本。

    

    发现深层ReLU网络（或多层感知器架构）表现出“过度泛化”现象。也就是说，那些在训练过程中没有看到的输入的输出值被映射到了在学习过程中观察到的输出范围附近。换句话说，多层感知器学习了一对多的映射，这种效应在增加层数或多层感知器的深度时更为明显。我们利用了深层ReLU网络的这一特性来设计一个数据集万花筒，称为“生成万花筒网络”。简而言之，如果我们学习一个多层感知器将输入 $x\in\mathbb{R}^D$ 映射到自身 $f_\mathcal{N}(x)\rightarrow x$，那么“万花筒采样”过程将从随机输入噪声 $z\in\mathbb{R}^D$ 开始，并递归地应用 $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$。经过燃烧期后，我们开始观察来自输入分布的样本，我们发现更深的

    arXiv:2402.11793v1 Announce Type: cross  Abstract: We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from input $x\in\mathbb{R}^D$ to itself $f_\mathcal{N}(x)\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\in\mathbb{R}^D$ and recursively applies $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper 
    
[^188]: 重新探讨零阶优化在内存高效LLM微调中的应用：一个基准研究

    Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark

    [https://arxiv.org/abs/2402.11592](https://arxiv.org/abs/2402.11592)

    本研究提出了一种不使用反向传播的零阶优化方法，用于降低LLM微调中的内存成本，通过全面的基准研究扩展了对不同的ZO优化技术的探索。

    

    在自然语言处理（NLP）领域的不断发展中，使用SGD和Adam等一阶（FO）优化器微调预训练的大型语言模型（LLMs）已成为标准。然而，随着LLMs体积的增长，由于FO梯度计算的反向传播（BP）带来的巨大内存开销构成了一个重大挑战。解决这个问题至关重要，尤其对于内存效率至关重要的设备端训练等应用。本文提出了一种转向不使用BP的零阶（ZO）优化的方法，用于在LLM微调过程中降低内存成本，构建在MeZO提出的概念基础上。与传统的ZO梯度下降方法不同，我们的工作将探索扩展到更广泛的ZO优化技术，通过全面的、首次推出的基准研究跨越五个LLM系列（Roberta，OPT，LLaMA，Vicuna，Mistral），三种任务复杂性和五种微调方案。

    arXiv:2402.11592v1 Announce Type: new  Abstract: In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes
    
[^189]: 评估深度学习潜在特征空间的稳定性

    Evaluating the Stability of Deep Learning Latent Feature Spaces

    [https://arxiv.org/abs/2402.11404](https://arxiv.org/abs/2402.11404)

    评估深度学习潜在特征空间稳定性的新方法，引入了可以确保一致性和可靠性的稳定性评估工作流程，包括了三种稳定性类型和一套全面评估的度量标准。

    

    高维数据集在各个学科的统计建模中提出了重大挑战，需要有效的降维方法。深度学习方法以从复杂数据中提炼关键特征的能力而著称，通过降维的潜在特征空间实现建模、可视化和压缩，在生物信息学到地球科学等领域有广泛应用。本研究引入了一种新颖工作流程，用于评估这些潜在空间的稳定性，确保后续分析的一致性和可靠性。稳定性被定义为潜在空间对于微小数据、训练实现和参数扰动的不变性，是至关重要却经常被忽视的。我们提出的方法论界定了潜在空间中的三种稳定性类型，样本稳定性、结构稳定性和推断稳定性，并引入了一套用于全面评估的度量标准。我们实现了这个工作流程。

    arXiv:2402.11404v1 Announce Type: new  Abstract: High-dimensional datasets present substantial challenges in statistical modeling across various disciplines, necessitating effective dimensionality reduction methods. Deep learning approaches, notable for their capacity to distill essential features from complex data, facilitate modeling, visualization, and compression through reduced dimensionality latent feature spaces, have wide applications from bioinformatics to earth sciences. This study introduces a novel workflow to evaluate the stability of these latent spaces, ensuring consistency and reliability in subsequent analyses. Stability, defined as the invariance of latent spaces to minor data, training realizations, and parameter perturbations, is crucial yet often overlooked.   Our proposed methodology delineates three stability types, sample, structural, and inferential, within latent spaces, and introduces a suite of metrics for comprehensive evaluation. We implement this workflow
    
[^190]: 通过贝叶斯优化从钙钛矿实验中提取基于物理的材料参数

    Physics-based material parameters extraction from perovskite experiments via Bayesian optimization

    [https://arxiv.org/abs/2402.11101](https://arxiv.org/abs/2402.11101)

    使用贝叶斯优化开发了一个分析平台，可以从钙钛矿实验中提取多个基本材料参数，加速材料发现和半导体优化

    

    从定量实验分析中提取材料参数的能力对于合理设计和理论进步至关重要。然而，随着理论模型的复杂性和材料参数数量的增加，这种分析的难度显着增加。在这里，我们使用贝叶斯优化开发了一个分析平台，可以从瞬态光致发光实验中提取一个有机金属钙钛矿半导体的8个基本材料参数，基于一个包括载流子漂移扩散和动态缺陷占据的复杂全物理模型。热降解的一个示例研究表明，掺杂浓度和载流子迁移率的变化主导，而缺陷能级几乎保持不变。这个平台可以方便地应用于其他实验或实验组合，加速材料发现和半导体优化。

    arXiv:2402.11101v1 Announce Type: cross  Abstract: The ability to extract material parameters from quantitative experimental analysis is essential for rational design and theory advancement. However, the difficulty of this analysis increases significantly with the complexity of the theoretical model and the number of material parameters. Here we use Bayesian optimization to develop an analysis platform that can extract up to 8 fundamental material parameters of an organometallic perovskite semiconductor from a transient photoluminescence experiment, based on a complex full physics model that includes drift-diffusion of carriers and dynamic defect occupation. An example study of thermal degradation reveals that changes in doping concentration and carrier mobility dominate, while the defect energy level remains nearly unchanged. This platform can be conveniently applied to other experiments or to combinations of experiments, accelerating materials discovery and optimization of semiconduc
    
[^191]: 医疗AI中的泛化性能：临床大型语言模型的评估

    Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model

    [https://arxiv.org/abs/2402.10965](https://arxiv.org/abs/2402.10965)

    大型语言模型在医疗健康领域有着重要作用，然而它们的泛化效果取决于在不同临床环境和人群中的表现，对于泛化能力不足的原因进行了分析，发现在样本较少的医院和特定人群中存在挑战。

    

    大型语言模型（LLMs）的进展为医疗健康领域提供了新机遇，可以改善患者护理、临床决策以及提升医师和管理人员的工作流程。然而，这些模型的潜力重要取决于它们在临床环境和人群中有效泛化的能力，这是在早期开发中经常被低估的挑战。为了更好地理解这些挑战的原因并制定缓解方法，我们评估了ClinicLLM，这是一个在 [HOSPITAL] 的临床笔记上训练的LLM模型，对其在30天全因素再入院预测中的表现进行分析，关注跨医院和患者特征的变异性。我们发现在样本较少的医院、政府和未指定保险的患者、老年人以及高共病性患者中，泛化效果较差。为了了解泛化不彰的原因，我们调查了样本量

    arXiv:2402.10965v1 Announce Type: new  Abstract: Advances in large language models (LLMs) provide new opportunities in healthcare for improved patient care, clinical decision-making, and enhancement of physician and administrator workflows. However, the potential of these models importantly depends on their ability to generalize effectively across clinical environments and populations, a challenge often underestimated in early development. To better understand reasons for these challenges and inform mitigation approaches, we evaluated ClinicLLM, an LLM trained on [HOSPITAL]'s clinical notes, analyzing its performance on 30-day all-cause readmission prediction focusing on variability across hospitals and patient characteristics. We found poorer generalization particularly in hospitals with fewer samples, among patients with government and unspecified insurance, the elderly, and those with high comorbidities. To understand reasons for lack of generalization, we investigated sample sizes 
    
[^192]: 强健的智能体学习因果世界模型

    Robust agents learn causal world models

    [https://arxiv.org/abs/2402.10877](https://arxiv.org/abs/2402.10877)

    智能体必须学习因果模型才能在广泛的分布转变下达到后悔界限，这对迁移学习和因果推断等研究领域有重要影响。

    

    一直有人假设因果推理在强健且具有通用智能中起着基础作用，然而不清楚智能体是否必须学习因果模型才能推广到新的领域，或者其他归纳偏差是否足够。我们回答了这个问题，表明任何能够在大量分布转变下满足后悔界限的智能体必须学习数据生成过程的近似因果模型，对于优化智能体来说，该近似模型会收敛到真实的因果模型。我们讨论了这一结果对于多个研究领域，包括迁移学习和因果推断的影响。

    arXiv:2402.10877v1 Announce Type: new  Abstract: It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.
    
[^193]: TimeSeriesBench：面向时间序列异常检测模型的工业级基准

    TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly Detection Models

    [https://arxiv.org/abs/2402.10802](https://arxiv.org/abs/2402.10802)

    时间序列异常检测模型的工业级基准TimeSeriesBench填补了当前算法在训练范式、在线检测范式和评估标准方面与实际需求之间的差距。

    

    由于实际应用场景和规模的蔓延，时间序列异常检测（TSAD）引起了学术界和工业界的广泛兴趣。然而，与实际工业系统的需求相比，现有算法在训练范式、在线检测范式和评估标准方面存在差距。当前算法通常为每个单独的时间序列训练一个特定模型，然而在具有数以万计曲线的大规模在线系统中，维护这么多模型是不切实际的。仅使用一个统一模型来检测异常的性能尚不明确。大多数TSAD模型都是在时间序列的历史部分上进行训练，并在其未来部分上进行测试。然而，在分布式系统中，经常部署和升级系统，每天都会出现新的、以前没有见过的时间序列。使用历史数据所训练模型直接应用于新时间序列的性能也不明确。

    arXiv:2402.10802v1 Announce Type: new  Abstract: Driven by the proliferation of real-world application scenarios and scales, time series anomaly detection (TSAD) has attracted considerable scholarly and industrial interest. However, existing algorithms exhibit a gap in terms of training paradigm, online detection paradigm, and evaluation criteria when compared to the actual needs of real-world industrial systems. Firstly, current algorithms typically train a specific model for each individual time series. In a large-scale online system with tens of thousands of curves, maintaining such a multitude of models is impractical. The performance of using merely one single unified model to detect anomalies remains unknown. Secondly, most TSAD models are trained on the historical part of a time series and are tested on its future segment. In distributed systems, however, there are frequent system deployments and upgrades, with new, previously unseen time series emerging daily. The performance o
    
[^194]: 基于上下文的奖励：基于动态偏好调整的多目标基础模型对齐

    Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment

    [https://arxiv.org/abs/2402.10207](https://arxiv.org/abs/2402.10207)

    本文介绍了Rewards-in-Context（RiC）方法，该方法通过多个奖励条件控制基础模型的响应，并应用有监督的微调进行对齐。它具有简单性和适应性，并支持在推理时动态调整用户偏好。

    

    我们考虑了基于人类偏好的基础模型多目标对齐问题，这是实现有益和无害的人工智能系统的关键步骤。然而，使用强化学习（RL）对大型基础模型进行微调通常是昂贵且不稳定的，并且人类偏好的多维度、异质性和冲突性进一步复杂化了对齐过程。在本文中，我们引入了Rewards-in-Context（RiC）方法，它使得基础模型的响应取决于其提示上下文中的多个奖励，并应用有监督的微调来进行对齐。RiC的显著特点是简单性和适应性，因为它只需要对单个基础模型进行有监督的微调，并支持在推理时动态调整用户偏好。受到抽象的凸优化问题的解析解的启发，我们提出了一种动态推理时调整方法。

    arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
    
[^195]: 使用DDPM反转进行零样本无监督和基于文本的音频编辑

    Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion

    [https://arxiv.org/abs/2402.10009](https://arxiv.org/abs/2402.10009)

    本文研究了使用DDPM反转进行音频信号的零样本编辑技术，包括基于文本的编辑和无监督发现编辑方向。这些方法在音乐信号中展现了多样的音乐兴趣修改。

    

    使用大型预训练模型进行零样本编辑已经在图像领域取得了迅猛的发展，但在音频领域尚未出现。本文中，我们探索了两种基于DDPM反转的音频信号零样本编辑技术。第一种是从图像领域采用的方法，允许基于文本进行编辑。第二种是一种新颖的方法，可以在无监督情况下发现语义上有意义的编辑方向。当应用于音乐信号时，这种方法可以展现出一系列具有音乐兴趣的修改，从控制特定乐器的参与到对旋律进行即兴演奏。示例可以在我们的例子页面中找到：https://hilamanor.github.io/AudioEditing/ ，代码可以在 https://github.com/hilamanor/AudioEditing/ 找到。

    arXiv:2402.10009v1 Announce Type: cross  Abstract: Editing signals using large pre-trained models, in a zero-shot manner, has recently seen rapid advancements in the image domain. However, this wave has yet to reach the audio domain. In this paper, we explore two zero-shot editing techniques for audio signals, which use DDPM inversion on pre-trained diffusion models. The first, adopted from the image domain, allows text-based editing. The second, is a novel approach for discovering semantically meaningful editing directions without supervision. When applied to music signals, this method exposes a range of musically interesting modifications, from controlling the participation of specific instruments to improvisations on the melody. Samples can be found on our examples page in https://hilamanor.github.io/AudioEditing/ and code can be found in https://github.com/hilamanor/AudioEditing/ .
    
[^196]: 为什么Transformer对敏感函数困难?

    Why are Sensitive Functions Hard for Transformers?

    [https://arxiv.org/abs/2402.09963](https://arxiv.org/abs/2402.09963)

    本文证明了在Transformer架构下，损失函数的空间受到输入敏感性的限制，从而解释了Transformer对敏感函数的困难。这一理论统一了关于Transformer学习能力和偏见的广泛观察。

    

    经验研究发现，Transformer存在一系列的学习偏见和限制，如在学习计算简单形式语言（如PARITY）时的持久困难，以及对低阶函数的偏好。然而，现有的表达能力理论要么过度预测，要么低估了实际的学习能力。我们证明，在Transformer架构下，损失函数的空间受到输入敏感性的限制：输出对输入字符串的多个部分敏感的Transformer存在于参数空间中的孤立点，导致泛化中的低敏感性偏差。我们理论上和实证上证明了该理论统一了关于Transformer学习能力和偏见的广泛观察，如它们对低敏感性和低阶的泛化偏差，以及在长度泛化上的困难。

    arXiv:2402.09963v1 Announce Type: new  Abstract: Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the transformer architecture, the loss landscape is constrained by the input-space sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalizati
    
[^197]: 未知博弈中乐观的汤普森抽样方法用于无遗憾学习

    Optimistic Thompson Sampling for No-Regret Learning in Unknown Games

    [https://arxiv.org/abs/2402.09456](https://arxiv.org/abs/2402.09456)

    该论文提出了一种在未知博弈中进行无遗憾学习的乐观的汤普森抽样方法，通过利用对手的行动和奖励结构信息，显著减少了实验预算，成功地缓解了多机构问题。此外，研究还引入了乐观-无遗憾框架，将现有算法与提出的方法相结合。

    

    许多涉及多个决策者的真实世界问题可以建模为一个具有部分观测的未知博弈。为了解决部分信息和多机构的挑战，我们开发了汤普森抽样类型的算法，利用对手的行动和奖励结构的信息。我们的方法在实际应用中，如交通路由和雷达感知中，显著减少了实验预算，与基准算法相比，减少了十倍以上。我们证明，在对奖励结构有一定假设的情况下，遗憾界限仅对总行动空间大小呈对数依赖，有效缓解了多机构问题。此外，本研究引入了乐观-无遗憾框架，该框架将我们提出的方法和领域内现有的算法相结合，是一项新的贡献。

    arXiv:2402.09456v1 Announce Type: cross  Abstract: Many real-world problems involving multiple decision-makers can be modeled as an unknown game characterized by partial observations. Addressing the challenges posed by partial information and the curse of multi-agency, we developed Thompson sampling-type algorithms, leveraging information about opponent's action and reward structures. Our approach significantly reduces experimental budgets, achieving a more than tenfold reduction compared to baseline algorithms in practical applications like traffic routing and radar sensing. We demonstrate that, under certain assumptions about the reward structure, the regret bound exhibits merely a logarithmic dependence on the total action space size, effectively mitigating the curse of multi-agency. Additionally, this research introduces the Optimism-then-NoRegret framework, a novel contribution that integrates both our proposed methodologies and existing algorithms in the field.
    
[^198]: 攻击、防御和评估LLM对话安全性的调查

    Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey

    [https://arxiv.org/abs/2402.09283](https://arxiv.org/abs/2402.09283)

    这篇调查提供了LLM对话安全性的全面概述，涵盖了攻击、防御和评估三个关键方面，旨在提高对该主题的理解并促进进一步的研究。

    

    arXiv:2402.09283v1 公告类型: 新的摘要: 大型语言模型（LLMs）在对话应用中已经很常见。然而，它们可能被误用生成有害回复的风险引起了严重的社会关切，并激发了LLM对话安全性的最新研究。因此，在此调查中，我们提供了最近研究的全面概述，涵盖了LLM对话安全性的三个关键方面：攻击、防御和评估。我们的目标是提供一个结构化的摘要，增进对LLM对话安全性的理解，并鼓励进一步研究这一重要课题。为了方便参考，我们根据我们的分类法对所有在此调查中提到的研究进行了分类，可在以下网址找到：https://github.com/niconi19/LLM-conversation-safety。

    arXiv:2402.09283v1 Announce Type: new Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.
    
[^199]: 图神经网络的同态计数：关于基础的一切

    Homomorphism Counts for Graph Neural Networks: All About That Basis

    [https://arxiv.org/abs/2402.08595](https://arxiv.org/abs/2402.08595)

    本研究展示了基于图神经网络的同态计数对于增强其表达能力的重要性，并提出了一种更细致的方法来融合目标模式的同态计数。这种方法比现有方法更具表达力且没有额外的计算复杂度开销。

    

    图神经网络是用于学习图上不变函数的架构。大量研究已经探讨了图神经网络的性质，并确定了一些限制，特别是与其表达能力相关的限制。它们无法计数图中的某些模式（例如循环）是这些限制的核心，因为许多需要学习的函数依赖于计数这些模式的能力。两种突出的范例旨在通过丰富图特征的子图或同态模式计数来解决这个限制。在这项工作中，我们展示了这两种方法在某种意义上都是次优的，并主张采用一种更细致的方法，将目标模式的“基础”中的同态计数纳入考虑。与现有方法相比，这产生了更加表达力的架构，而不会带来任何额外的计算复杂度开销。我们证明了一系列理论结论。

    Graph neural networks are architectures for learning invariant functions over graphs. A large body of work has investigated the properties of graph neural networks and identified several limitations, particularly pertaining to their expressive power. Their inability to count certain patterns (e.g., cycles) in a graph lies at the heart of such limitations, since many functions to be learned rely on the ability of counting such patterns. Two prominent paradigms aim to address this limitation by enriching the graph features with subgraph or homomorphism pattern counts. In this work, we show that both of these approaches are sub-optimal in a certain sense and argue for a more fine-grained approach, which incorporates the homomorphism counts of all structures in the "basis" of the target pattern. This yields strictly more expressive architectures without incurring any additional overhead in terms of computational complexity compared to existing approaches. We prove a series of theoretical r
    
[^200]: 朝着忠实和强大的基于证据的问答专家的方向前进

    Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering

    [https://arxiv.org/abs/2402.08277](https://arxiv.org/abs/2402.08277)

    这项工作探索了如何鲁棒地微调大型语言模型以提高答案的来源质量和答案归因能力，引入了数据生成流水线和四个测试集来评估模型的性能，并展示了在合成数据上微调可以改善内部和外部分布的性能。

    

    对大型语言模型（LLM）更忠实和可追踪的答案的进步对于各种研究和实践活动至关重要。其中一种达到这个目标的方法是基于可靠的来源提供答案。然而，这种基于证据的问答在使用LLM时已经证明在引用正确的来源（来源质量）和准确地表示来源中的信息（答案归因能力）方面工作不足。在这项工作中，我们系统地研究了如何鲁棒地微调LLM，以提高来源质量和答案归因能力。具体而言，我们引入了一个数据生成流水线，其中包括自动数据质量过滤器，可以大规模合成多样化的高质量训练和测试数据。我们还引入了四个测试集，以对微调后的专家模型的鲁棒性进行基准测试。广泛的评估结果表明，在合成数据上进行微调可以提高在内部和外部分布的性能。%基于证据的问答案例。此外，我们展示了用于评估的四个测试集，以评估微调后的专家模型的鲁棒性。

    Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we sho
    
[^201]: 使用语言反馈模型来改进政策

    Policy Improvement using Language Feedback Models

    [https://arxiv.org/abs/2402.07876](https://arxiv.org/abs/2402.07876)

    本文介绍了一种使用语言反馈模型（LFMs）改进政策的方法，通过识别期望的行为并进行模仿学习，我们在任务完成率、泛化性能和人类可解释性方面取得了显著改进。

    

    我们引入了语言反馈模型（LFMs），用于在指令遵循中识别期望的行为-有助于实现指令中指定任务的行动-以进行模仿学习。为了训练LFMs，我们从大型语言模型（LLMs）获取对视觉轨迹进行语言描述的反馈。首先，通过使用LFMs识别期望模仿的行为，我们在三种不同的语言基础环境（Touchdown，ScienceWorld和ALFWorld）上，在任务完成率上改善了强行为克隆的基线方法。其次，与LLMs直接预测行动相比，使用LFMs在LLM输出标记的数量相同的情况下表现更好。第三，LFMs适应未见环境，通过一轮适应使任务完成率提高了3.5-12.0％。最后，可以修改LFM以提供人类可解释的反馈，无需性能损失，从而允许人类验证模仿学习的期望行为。

    We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
    
[^202]: 《基于树结构方法的异常检测能否超越深度学习？一项基准研究》

    Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study

    [https://arxiv.org/abs/2402.07281](https://arxiv.org/abs/2402.07281)

    本文通过一项基准研究评估了多种基于机器学习的异常检测算法，包括树结构方法和深度学习方法，并揭示了深度学习神话的真相。

    

    在确保服务连续性时，复杂的关键任务系统中检测异常情况至关重要。由于异常事件被认为是罕见事件，因此从操作数据中检测异常情况面临着类别分布不平衡问题的挑战。本文通过全面的基准研究评估了多种基于机器学习的异常检测算法。论文通过对各种异常检测算法的公正比较做出了重大贡献，包括经典机器学习方法、各种基于树结构的方法、深度学习和异常点检测方法。论文使用了104个公开可用的和少数专有的工业系统数据集，增强了研究的多样性，使算法性能的评估更加真实，并强调了对实际场景的适应性的重要性。论文揭示了深度学习神话的真相。

    Detection of anomalous situations for complex mission-critical systems holds paramount importance when their service continuity needs to be ensured. A major challenge in detecting anomalies from the operational data arises due to the imbalanced class distribution problem since the anomalies are supposed to be rare events. This paper evaluates a diverse array of machine learning-based anomaly detection algorithms through a comprehensive benchmark study. The paper contributes significantly by conducting an unbiased comparison of various anomaly detection algorithms, spanning classical machine learning including various tree-based approaches to deep learning and outlier detection methods. The inclusion of 104 publicly available and a few proprietary industrial systems datasets enhances the diversity of the study, allowing for a more realistic evaluation of algorithm performance and emphasizing the importance of adaptability to real-world scenarios. The paper dispels the deep learning myth
    
[^203]: 知识图谱与多模态学习：综述

    Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey

    [https://arxiv.org/abs/2402.05391](https://arxiv.org/abs/2402.05391)

    知识图谱与多模态学习的综述介绍了KG4MM和MM4KG两个主要方面，包括任务定义、构建进展、评估基准以及关键研究轨迹。

    

    知识图谱在推动各种人工智能应用方面起着关键作用，语义网络社区对多模态维度的探索为创新打开了新的途径。在本综述中，我们仔细审查了300多篇文章，重点关注了两个主要方面的知识图谱感知研究：以知识图谱支持多模态任务的KG驱动多模态（KG4MM）学习，将知识图谱研究扩展到多模态知识图谱（MM4KG）领域。我们从定义知识图谱和多模态知识图谱开始，然后探索它们的构建进展。我们的综述包括两个主要任务类别：KG感知的多模态学习任务，如图像分类和视觉问答，以及内在的多模态知识图谱任务，如多模态知识图谱补全和实体对齐，突出了具体的研究轨迹。对于这些任务中的大部分，我们提供了定义、评估基准，并进一步指出进行相关研究的重要见解。最后，我们讨论了cu

    Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
    
[^204]: 结合云计算与移动计算的机器学习研究

    Combining Cloud and Mobile Computing for Machine Learning

    [https://arxiv.org/abs/2402.04880](https://arxiv.org/abs/2402.04880)

    这项研究将模型分割为移动设备和云之间的计算，以减轻移动设备的负担，并优化云端的工作负载。

    

    尽管移动设备的计算能力正在增加，但机器学习模型的大小也在增长。这种趋势给移动设备带来了问题，如内存容量和电池寿命的限制。虽然许多服务（如ChatGPT和Midjourney）在云中运行所有的推理，但我们认为灵活性和细粒度的任务分配更可取。在这项工作中，我们将模型分割视为改善用户体验的解决方案，将计算分割在移动设备和云之间，以减轻模型的计算密集部分，同时尽量减少数据传输的需求。我们展示了这种分割不仅减少了用户等待时间，还可以通过细粒度调整来优化云端的工作负载。为了实现这一目标，我们设计了一个调度器，收集网络质量、客户端设备能力和作业要求的信息，做出决策以实现在各种设备上的一致性性能。

    Although the computing power of mobile devices is increasing, machine learning models are also growing in size. This trend creates problems for mobile devices due to limitations like their memory capacity and battery life. While many services, like ChatGPT and Midjourney, run all the inferences in the cloud, we believe a flexible and fine-grained task distribution is more desirable. In this work, we consider model segmentation as a solution to improving the user experience, dividing the computation between mobile devices and the cloud in a way that offloads the compute-heavy portion of the model while minimizing the data transfer required. We show that the division not only reduces the wait time for users but can also be fine-tuned to optimize the workloads of the cloud. To achieve that, we design a scheduler that collects information about network quality, client device capability, and job requirements, making decisions to achieve consistent performance across a range of devices while
    
[^205]: 关于可证明的长度和组合泛化

    On Provable Length and Compositional Generalization

    [https://arxiv.org/abs/2402.04875](https://arxiv.org/abs/2402.04875)

    本研究针对包括深度集合、变压器、状态空间模型和简单递归神经网络等多种架构，探索了可证明的长度和组合泛化，认为对于长度和组合泛化，不同架构需要不同程度的表示识别。

    

    长度泛化——对训练时未见到的更长序列的泛化能力，以及组合泛化——对训练时未见到的令牌组合的泛化能力，在序列到序列模型中是重要的非分布化泛化形式。在这项工作中，我们在包括深度集合、变压器、状态空间模型和简单递归神经网络在内的一系列架构中，朝着可证明的长度和组合泛化迈出了第一步。根据架构的不同，我们证明了不同程度的表示识别的必要性，例如与真实表示具有线性或排列关系。

    Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.
    
[^206]: 缩小SGP4和高精度传播之间的差距：通过可微编程

    Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming

    [https://arxiv.org/abs/2402.04830](https://arxiv.org/abs/2402.04830)

    本研究介绍了dSGP4，一种使用PyTorch实现的可微版本的SGP4。通过可微化，dSGP4实现了轨道传播的高精度，并且适用于各种与太空相关的应用，包括卫星轨道确定、状态转换、协方差传播等。

    

    简化的第四级摄动(SGP4)轨道传播方法被广泛用于快速可靠地预测地球轨道物体的位置和速度。尽管不断改进，SGP模型仍然缺乏数值传播器的精度，后者的误差显著较小。本研究提出了dSGP4，一种使用PyTorch实现的新型可微版本的SGP4。通过使SGP4可微化，dSGP4便于进行各种与太空相关的应用，包括航天器轨道确定、状态转换、协方差转换、状态转移矩阵计算和协方差传播。此外，dSGP4的PyTorch实现允许在批量的TLE（两行参数）集上进行尴尬的并行轨道传播，利用CPU、GPU和分布式预测卫星位置的高级硬件的计算能力。此外，dSGP4的可微性使其能与模式集成。

    The Simplified General Perturbations 4 (SGP4) orbital propagation method is widely used for predicting the positions and velocities of Earth-orbiting objects rapidly and reliably. Despite continuous refinement, SGP models still lack the precision of numerical propagators, which offer significantly smaller errors. This study presents dSGP4, a novel differentiable version of SGP4 implemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates various space-related applications, including spacecraft orbit determination, state conversion, covariance transformation, state transition matrix computation, and covariance propagation. Additionally, dSGP4's PyTorch implementation allows for embarrassingly parallel orbital propagation across batches of Two-Line Element Sets (TLEs), leveraging the computational power of CPUs, GPUs, and advanced hardware for distributed prediction of satellite positions at future times. Furthermore, dSGP4's differentiability enables integration with mode
    
[^207]: PRES: 实现可扩展的基于内存的动态图神经网络

    PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks

    [https://arxiv.org/abs/2402.04284](https://arxiv.org/abs/2402.04284)

    这篇论文研究了如何实现可扩展的基于内存的动态图神经网络，并解决了训练中的时间间断问题。通过使用内存模块提取和记忆长期的时间依赖关系，MDGNNs在处理大的时间批量时表现出更好的性能和灵活性。

    

    基于内存的动态图神经网络（MDGNNs）是一类动态图神经网络，利用内存模块提取、提炼和记忆长期的时间依赖关系，相比于无内存的对应物，表现出更卓越的性能。然而，训练MDGNNs面临着处理纠结的时间和结构依赖关系的挑战，需要对数据序列进行顺序和时间顺序的处理，以捕捉准确的时间模式。在批量训练中，同一批次内的时间数据点将被并行处理，而它们的时间依赖关系将被忽视。这个问题被称为时间间断，限制了有效的时间批量大小，限制了数据的并行性，并降低了MDGNNs在工业应用中的灵活性。本文研究了MDGNNs的大规模高效训练，重点关注在大的时间批量大小下训练MDGNNs时的时间间断问题。我们首先进行了理论工作。

    Memory-based Dynamic Graph Neural Networks (MDGNNs) are a family of dynamic graph neural networks that leverage a memory module to extract, distill, and memorize long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, training MDGNNs faces the challenge of handling entangled temporal and structural dependencies, requiring sequential and chronological processing of data sequences to capture accurate temporal patterns. During the batch training, the temporal data points within the same batch will be processed in parallel, while their temporal dependencies are neglected. This issue is referred to as temporal discontinuity and restricts the effective temporal batch size, limiting data parallelism and reducing MDGNNs' flexibility in industrial applications. This paper studies the efficient training of MDGNNs at scale, focusing on the temporal discontinuity in training MDGNNs with large temporal batch sizes. We first conduct a theoretic
    
[^208]: RevOrder：一种增强语言模型中算术运算的新方法

    RevOrder: A Novel Method for Enhanced Arithmetic in Language Models

    [https://arxiv.org/abs/2402.03822](https://arxiv.org/abs/2402.03822)

    本文提出了一种名为RevOrder的新方法，通过翻转加法、减法和nD乘以1D的输出数字，显著改善了语言模型中的算术运算。经过全面测试，RevOrder在基本算术运算中达到了完美准确度，并在除法任务中提升了语言模型性能，特别是在处理大数时。在GSM8K数学任务中应用RevOrder进行微调，有效降低了错误率并提高了总体得分。

    

    本文提出了RevOrder，一种旨在改善大型语言模型中算术运算的新技术。该方法通过翻转加法、减法和n位数乘以1位数（nD乘以1D）的输出数字，显著降低了顺序中间数字的数量 (CSID)，这是我们引入的一种评估方程复杂性的新度量。通过全面的测试，RevOrder不仅在基本的算术运算中达到了完美的准确度，而且在除法任务中显著提升了语言模型的性能，特别是在传统模型难以处理的大数情况下。RevOrder的实现对于训练和推理阶段都具有成本效益。此外，将RevOrder应用于对GSM8K数学任务进行微调的LLaMA2-7B模型中，取得了显著的改善，将方程计算错误率降低了46%，将总体得分从41.6提升到44.4。

    This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks. Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\mathcal{O}(1)$, a new metric we introduce to assess equation complexity. Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle. Implementation of RevOrder is cost-effective for both training and inference phases. Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4.
    
[^209]: 揭示宣传：基于人类注释和机器分类的文体线索比较分析

    Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification

    [https://arxiv.org/abs/2402.03780](https://arxiv.org/abs/2402.03780)

    本文通过分析文体线索比较人类注释和机器分类的方法，揭示了宣传语言的特征，并提出了一个多源、多语言、多模态的数据集PPN。结果表明，人类注释者能够可靠地区分宣传新闻和常规新闻。研究还比较了不同的自然语言处理技术，并提供了一些有关文体线索的发现。

    

    本文研究了宣传语言及其文体特征。提出了PPN数据集，即宣传性伪新闻数据集，它是一种多源、多语言、多模态的数据集，由专家机构确定的宣传来源网站上的新闻文章组成。从该数据集中随机选择了一部分样本与来自常规法国新闻的文章混合，并对它们的URL进行了掩盖，以进行人类注释实验，使用11个不同的标签。结果显示，人类注释者能够可靠地区分两种类型的新闻纸对每个标签。我们提出了不同的自然语言处理技术来识别注释者使用的线索，并与机器分类进行比较。其中包括使用VAGO分析器进行辞述模糊和主观性的测量，使用TF-IDF作为基准，以及四种不同的分类器：两个基于RoBERTa模型的模型，使用句法的CATS，以及结合句法和语义特征的一个XGBoost模型。

    This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
    
[^210]: 图上的去中心化双级优化: 无环算法更新和瞬态迭代复杂性

    Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity

    [https://arxiv.org/abs/2402.03167](https://arxiv.org/abs/2402.03167)

    本文提出了一种单循环的去中心化双级优化算法（D-SOBA），首次阐明了网络拓扑和数据异构性对去中心化双级算法的共同影响。D-SOBA在渐近速率、渐近梯度/海森复杂性和瞬态梯度/海森复杂性方面达到了最先进水平。

    

    随机双级优化（SBO）在处理嵌套结构方面的多样性使其在机器学习中变得越来越重要。为了解决大规模SBO，去中心化方法作为有效的范例出现，其中节点与直接相邻节点进行通信，无需中央服务器，从而提高通信效率和增强算法的稳健性。然而，当前的去中心化SBO算法面临挑战，包括昂贵的内部循环更新和对网络拓扑、数据异构性和嵌套双级算法结构的影响不明确。在本文中，我们引入了一种单循环的去中心化SBO（D-SOBA）算法，并建立了其瞬态迭代复杂性，首次澄清了网络拓扑和数据异构性对去中心化双级算法的共同影响。D-SOBA实现了最先进的渐近速率、渐近梯度/海森复杂性和瞬态梯度/海森复杂性。

    Stochastic bilevel optimization (SBO) is becoming increasingly essential in machine learning due to its versatility in handling nested structures. To address large-scale SBO, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness. However, current decentralized SBO algorithms face challenges, including expensive inner-loop updates and unclear understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures. In this paper, we introduce a single-loop decentralized SBO (D-SOBA) algorithm and establish its transient iteration complexity, which, for the first time, clarifies the joint influence of network topology and data heterogeneity on decentralized bilevel algorithms. D-SOBA achieves the state-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, and transien
    
[^211]: Spin: 一种具备GPU加速的高效安全计算框架

    Spin: An Efficient Secure Computation Framework with GPU Acceleration

    [https://arxiv.org/abs/2402.02320](https://arxiv.org/abs/2402.02320)

    Spin是一个GPU加速的多方计算(MPC)框架，支持多个计算方和不诚实多数对抗设置。该框架提出了针对机器学习关键的非线性函数的优化协议，并进行了针对Transformer模型的注意力的新颖优化，以实现高效且安全的计算。

    

    准确性和效率对于多方计算（MPC）框架仍然是挑战。Spin是一个支持多个计算方和不诚实多数对抗设置的GPU加速的MPC框架。我们提出了针对机器学习关键的非线性函数的优化协议，以及针对Transformer模型的基本单元注意力的几种新颖优化，使Spin能够在不牺牲安全性的情况下进行非常规CNN训练和Transformer推断。在后端层面，Spin利用GPU、CPU和RDMA启用的智能网络卡进行加速。全面的评估表明，Spin在深度神经网络训练方面比最先进技术快两倍。对于具有1890万参数的Transformer模型的推断，我们的注意力特定优化使Spin能够实现更好的效率、更少的通信和更好的准确性。

    Accuracy and efficiency remain challenges for multi-party computation (MPC) frameworks. Spin is a GPU-accelerated MPC framework that supports multiple computation parties and a dishonest majority adversarial setup. We propose optimized protocols for non-linear functions that are critical for machine learning, as well as several novel optimizations specific to attention that is the fundamental unit of Transformer models, allowing Spin to perform non-trivial CNNs training and Transformer inference without sacrificing security. At the backend level, Spin leverages GPU, CPU, and RDMA-enabled smart network cards for acceleration. Comprehensive evaluations demonstrate that Spin can be up to $2\times$ faster than the state-of-the-art for deep neural network training. For inference on a Transformer model with 18.9 million parameters, our attention-specific optimizations enable Spin to achieve better efficiency, less communication, and better accuracy.
    
[^212]: 高维情况下，普通贝叶斯优化算法表现出色

    Vanilla Bayesian Optimization Performs Great in High Dimension

    [https://arxiv.org/abs/2402.02229](https://arxiv.org/abs/2402.02229)

    本文研究了高维情况下贝叶斯优化算法的问题，并提出了一种改进方法，通过对先验假设进行简单的缩放，使普通贝叶斯优化在高维任务中表现出色。

    

    长期以来，高维问题一直被认为是贝叶斯优化算法的软肋。受到维度噪音的刺激，许多算法旨在通过对目标应用各种简化假设来提高其性能。本文通过识别导致普通贝叶斯优化在高维任务中不适用的退化现象，并进一步展示了现有算法如何通过降低模型复杂度来应对这些退化现象。此外，我们还提出了一种对普通贝叶斯优化算法中典型先验假设的改进方法，该方法在不对目标施加结构性限制的情况下将复杂性降低到可管理的水平。我们的修改方法——通过维度对高斯过程长度先验进行简单的缩放——揭示了标准贝叶斯优化在高维情况下的显著改进，明确表明其效果远远超出以往的预期。

    High-dimensional problems have long been considered the Achilles' heel of Bayesian optimization algorithms. Spurred by the curse of dimensionality, a large collection of algorithms aim to make it more performant in this setting, commonly by imposing various simplifying assumptions on the objective. In this paper, we identify the degeneracies that make vanilla Bayesian optimization poorly suited to high-dimensional tasks, and further show how existing algorithms address these degeneracies through the lens of lowering the model complexity. Moreover, we propose an enhancement to the prior assumptions that are typical to vanilla Bayesian optimization algorithms, which reduces the complexity to manageable levels without imposing structural restrictions on the objective. Our modification - a simple scaling of the Gaussian process lengthscale prior with the dimensionality - reveals that standard Bayesian optimization works drastically better than previously thought in high dimensions, clearly
    
[^213]: 跨城市少样本交通预测的多尺度交通模式库

    Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic Forecasting

    [https://arxiv.org/abs/2402.00397](https://arxiv.org/abs/2402.00397)

    我们提出了一种跨城市少样本交通预测的解决方案，利用多尺度交通模式库从数据丰富的源城市学习并预测其他城市的交通情况。

    

    交通预测对智能交通系统具有重要意义，可以帮助高效分配资源和有效控制交通。然而，其有效性往往严重依赖于丰富的交通数据，而许多城市由于设备支持有限而缺乏足够的数据，这对交通预测构成了重大挑战。鉴于这一挑战，我们做出了一个显著的观察：交通模式在不同城市之间存在相似性。基于这一关键洞察，我们提出了一种解决跨城市少样本交通预测问题的方法，称为多尺度交通模式库（MTPB）。主要上，MTPB通过利用数据丰富的源城市启动其学习过程，通过空间-时间感知的预训练过程有效获取全面的交通知识。随后，该框架采用先进的聚类技术从学习到的知识中系统生成一个多尺度交通模式库。接下来，该框架使用准确的交通模式检索机制进行跨城市的少样本交通预测。

    Traffic forecasting is crucial for intelligent transportation systems (ITS), aiding in efficient resource allocation and effective traffic control. However, its effectiveness often relies heavily on abundant traffic data, while many cities lack sufficient data due to limited device support, posing a significant challenge for traffic forecasting. Recognizing this challenge, we have made a noteworthy observation: traffic patterns exhibit similarities across diverse cities. Building on this key insight, we propose a solution for the cross-city few-shot traffic forecasting problem called Multi-scale Traffic Pattern Bank (MTPB). Primarily, MTPB initiates its learning process by leveraging data-rich source cities, effectively acquiring comprehensive traffic knowledge through a spatial-temporal-aware pre-training process. Subsequently, the framework employs advanced clustering techniques to systematically generate a multi-scale traffic pattern bank derived from the learned knowledge. Next, th
    
[^214]: 自由长思变压器（FraiLT）

    Freely Long-Thinking Transformer (FraiLT)

    [https://arxiv.org/abs/2401.11626](https://arxiv.org/abs/2401.11626)

    FraiLT是一个改进的变压器模型，通过递归方法和迭代编码，实现了在紧凑形式下达到较大模型的解释深度，在性能表现上优于较大模型，旨在实现更高效和可访问的语言模型。

    

    自由长思变压器（FraiLT）是一个改进的变压器模型，旨在增强处理能力而不增加规模。它采用递归方法，多次迭代子层，并引入迭代编码以在这些周期中保持意识。迭代编码使FraiLT能够以紧凑的形式实现较大模型的解释深度。在合成故事数据集上进行评估时，FraiLT优于较大的模型，展示了其在减少内存需求的同时提供高质量性能的能力。该模型代表了更高效和可访问的语言模型的一步。

    arXiv:2401.11626v2 Announce Type: replace-cross  Abstract: Freely Long-Thinking Transformer (FraiLT) is an improved transformer model designed to enhance processing capabilities without scaling up size. It utilizes a recursive approach, iterating over a subset of layers multiple times, and introduces iteration encodings to maintain awareness across these cycles. Iteration encoding allows FraiLT to achieve the interpretive depth of larger models in a compact form. When evaluated on a synthetic story dataset, FraiLT outperformed larger models, showcasing its ability to deliver high-quality performance while reducing memory demands. This model represents a step forward towards more efficient and accessible language models.
    
[^215]: GD-CAF：用于降水预报的图形双流卷积注意力融合

    GD-CAF: Graph Dual-stream Convolutional Attention Fusion for Precipitation Nowcasting

    [https://arxiv.org/abs/2401.07958](https://arxiv.org/abs/2401.07958)

    GD-CAF提出了一种新颖的方法，将降水预报作为一个时空图序列预报问题，利用图形双流卷积注意力融合来学习历史降水图并在不同空间位置上预测未来的降水。

    

    精确的降水预报对于各种应用至关重要，包括洪水预测、灾害管理、优化农业活动、管理交通路线和可再生能源。本文将降水预报形式化为时空图序列预报问题，提出了一种名为图形双流卷积注意力融合（GD-CAF）的新方法，旨在从历史降水图的时空图中学习，并预测未来不同空间位置的降水。

    arXiv:2401.07958v2 Announce Type: replace  Abstract: Accurate precipitation nowcasting is essential for various applications, including flood prediction, disaster management, optimizing agricultural activities, managing transportation routes and renewable energy. While several studies have addressed this challenging task from a sequence-to-sequence perspective, most of them have focused on a single area without considering the existing correlation between multiple disjoint regions. In this paper, we formulate precipitation nowcasting as a spatiotemporal graph sequence nowcasting problem. In particular, we introduce Graph Dual-stream Convolutional Attention Fusion (GD-CAF), a novel approach designed to learn from historical spatiotemporal graph of precipitation maps and nowcast future time step ahead precipitation at different spatial locations. GD-CAF consists of spatio-temporal convolutional attention as well as gated fusion modules which are equipped with depthwise-separable convolut
    
[^216]: MoE-Mamba: 混合专家模型的高效选择性状态空间模型

    MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts

    [https://arxiv.org/abs/2401.04081](https://arxiv.org/abs/2401.04081)

    结合混合专家模型的MoE-Mamba在性能上优于Mamba和基准Transformer-MoE，达到了与Mamba相同性能的同时，训练步骤减少了2.35倍。

    

    状态空间模型（SSMs）已经成为顺序建模领域的严肃竞争者，挑战了Transformer的主导地位。与此同时，混合专家（MoE）显著改进了基于Transformer的大型语言模型，包括最近的最先进开放模型。我们提出要发掘SSMs在扩展方面的潜力，它们应该与MoE相结合。我们在Mamba上展示了这一点，这是一个最近基于SSM的模型，取得了显著的性能。我们的模型MoE-Mamba在性能方面表现优异，优于Mamba和基准Transformer-MoE。特别地，MoE-Mamba在更少的训练步骤中达到与Mamba相同的性能，同时保持Mamba相对于Transformer的推理性能增益。

    arXiv:2401.04081v2 Announce Type: replace-cross  Abstract: State Space Models (SSMs) have become serious contenders in the field of sequential modeling, challenging the dominance of Transformers. At the same time, Mixture of Experts (MoE) has significantly improved Transformer-based Large Language Models, including recent state-of-the-art open models. We propose that to unlock the potential of SSMs for scaling, they should be combined with MoE. We showcase this on Mamba, a recent SSM-based model that achieves remarkable performance. Our model, MoE-Mamba, outperforms both Mamba and baseline Transformer-MoE. In particular, MoE-Mamba reaches the same performance as Mamba in $2.35\times$ fewer training steps while preserving the inference performance gains of Mamba against Transformer.
    
[^217]: AllSpark: 一个具有十三种模态的多模态时空智能模型

    AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with Thirteen Modalities

    [https://arxiv.org/abs/2401.00546](https://arxiv.org/abs/2401.00546)

    提出了一个名为AllSpark的多模态时空智能通用人工智能模型，集成了十三种不同的模态，旨在解决多模态时空数据联合解释的挑战。

    

    长期以来，由于各种时空模态数据之间结构和语义的高度异质性，多模态时空数据的联合解释一直是一个极具挑战性的问题。主要挑战在于在不同模态之间的凝聚力和自治性之间取得平衡，而随着模态数量的增加，这种平衡表现出逐渐非线性的特性。我们引入了语言作为参考框架（LaRF），这是构建多模态统一模型的基本原则，旨在在不同模态之间取得凝聚力和自治性之间的平衡。我们提出了一个名为AllSpark的多模态时空智能通用人工智能模型。我们的模型将十三种不同的模态集成到一个统一框架中，包括1D（文本，代码），2D（RGB，红外线，SAR，多光谱，高光谱，表格，图表，轨迹，斜角摄影）。

    arXiv:2401.00546v2 Announce Type: replace  Abstract: For a long time, due to the high heterogeneity in structure and semantics among various spatiotemporal modal data, the joint interpretation of multimodal spatiotemporal data has been an extremely challenging problem. The primary challenge resides in striking a trade-off between the cohesion and autonomy of diverse modalities, and this trade-off exhibits a progressively nonlinear nature as the number of modalities expands. We introduce the Language as Reference Framework (LaRF), a fundamental principle for constructing a multimodal unified model, aiming to strike a trade-off between the cohesion and autonomy among different modalities. We propose a multimodal spatiotemporal general artificial intelligence model, called AllSpark. Our model integrates thirteen different modalities into a unified framework, including 1D (text, code), 2D (RGB, infrared, SAR, multispectral, hyperspectral, tables, graphs, trajectory, oblique photography), a
    
[^218]: 独立学习将时间序列片段嵌入

    Learning to Embed Time Series Patches Independently

    [https://arxiv.org/abs/2312.16427](https://arxiv.org/abs/2312.16427)

    学习独立嵌入时间序列片段可以产生更好的时间序列表示，通过简单的块重构任务和独立嵌入每个块的MLP模型以及互补对比学习来实现。

    

    最近，掩码时间序列建模作为一种自监督表示学习策略引起了广泛关注。受计算机视觉中的掩码图像建模启发，最近的研究首先将时间序列进行分块处理并部分掩盖，然后训练Transformer模型通过从未掩盖的块预测被掩盖块来捕捉块之间的依赖关系。然而，我们认为捕捉这种块之间的依赖关系可能不是时间序列表示学习的最佳策略；相反，独立学习嵌入片段会产生更好的时间序列表示。具体而言，我们建议使用1）简单的块重构任务，自动将每个块进行编码而不查看其他块，以及2）独自嵌入每个块的简单块式MLP。此外，我们引入互补对比学习来有效地分层捕获相邻时间序列信息。

    arXiv:2312.16427v2 Announce Type: replace-cross  Abstract: Masked time series modeling has recently gained much attention as a self-supervised representation learning strategy for time series. Inspired by masked image modeling in computer vision, recent works first patchify and partially mask out time series, and then train Transformers to capture the dependencies between patches by predicting masked patches from unmasked patches. However, we argue that capturing such patch dependencies might not be an optimal strategy for time series representation learning; rather, learning to embed patches independently results in better time series representations. Specifically, we propose to use 1) the simple patch reconstruction task, which autoencode each patch without looking at other patches, and 2) the simple patch-wise MLP that embeds each patch independently. In addition, we introduce complementary contrastive learning to hierarchically capture adjacent time series information efficiently. 
    
[^219]: 时间序列的软对比学习

    Soft Contrastive Learning for Time Series

    [https://arxiv.org/abs/2312.16424](https://arxiv.org/abs/2312.16424)

    提出了一种名为SoftCLT的方法，通过引入实例级和时间级软对比损失，解决了在时间序列中忽略固有相关性所导致的学习表示质量下降的问题。

    

    对比学习已经被证明在自监督学习中对于从时间序列中学习表示是有效的。然而，将时间序列中相似的实例或相邻时间戳的值进行对比会忽略它们固有的相关性，从而导致学习表示的质量下降。为了解决这个问题，我们提出了SoftCLT，一种简单而有效的时间序列软对比学习策略。这是通过引入从零到一的软赋值的实例级和时间级对比损失来实现的。具体来说，我们为1)基于数据空间上的时间序列之间的距离定义了实例级对比损失的软赋值，并为2)基于时间戳之间的差异定义了时间级对比损失。SoftCLT是一种即插即用的时间序列对比学习方法，可以提高学习表示的质量，没有过多复杂的设计。

    arXiv:2312.16424v2 Announce Type: replace-cross  Abstract: Contrastive learning has shown to be effective to learn representations from time series in a self-supervised way. However, contrasting similar time series instances or values from adjacent timestamps within a time series leads to ignore their inherent correlations, which results in deteriorating the quality of learned representations. To address this issue, we propose SoftCLT, a simple yet effective soft contrastive learning strategy for time series. This is achieved by introducing instance-wise and temporal contrastive loss with soft assignments ranging from zero to one. Specifically, we define soft assignments for 1) instance-wise contrastive loss by the distance between time series on the data space, and 2) temporal contrastive loss by the difference of timestamps. SoftCLT is a plug-and-play method for time series contrastive learning that improves the quality of learned representations without bells and whistles. In experi
    
[^220]: RepairLLaMA：高效表示和微调适配器用于程序修复

    RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair

    [https://arxiv.org/abs/2312.15698](https://arxiv.org/abs/2312.15698)

    高效表示和微调适配器相结合的新型程序修复方法RepairLLaMA可为语言模型修复错误产生高效的适配器。

    

    自动程序修复（APR）随着大型语言模型（LLMs）的出现已有了显著发展。对于程序修复进行LLMs的微调是最近研究的一个新领域，有许多未被探索的维度。现有工作大多使用简单的代码表示对LLMs进行微调，并在能够微调更大型LLMs的能力方面存在根本性局限。为解决这个问题，我们提出了RepairLLaMA，一个结合了1）用于APR的代码表示和2）最先进的参数高效的LLM微调技术LoRA的新型程序修复方法。这使得RepairLLaMA产生了一个高效的“程序修复适配器”，用于使用语言模型修复错误。我们的实验证明了这两个概念的有效性。首先，使用具有程序修复特定代码表示的微调适配器使模型能够使用有意义的修复信号。其次，参数高效的微调有助于微调...

    arXiv:2312.15698v2 Announce Type: replace-cross  Abstract: Automated Program Repair (APR) has evolved significantly with the advent of Large Language Models (LLMs). Fine-tuning LLMs for program repair is a recent avenue of research, with many dimensions which have not been explored. Existing work mostly fine-tunes LLMs with naive code representations and is fundamentally limited in its ability to fine-tune larger LLMs. To address this problem, we propose RepairLLaMA, a novel program repair approach that combines 1) code representations for APR and 2) the state-of-the-art parameter-efficient LLM fine-tuning technique called LoRA. This results in RepairLLaMA producing a highly effective `program repair adapter' for fixing bugs with language models. Our experiments demonstrate the validity of both concepts. First, fine-tuning adapters with program repair specific code representations enables the model to use meaningful repair signals. Second, parameter-efficient fine-tuning helps fine-tun
    
[^221]: 高效的标题重新排序器，用于快速和改进的知识密集型自然语言处理

    Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP

    [https://arxiv.org/abs/2312.12430](https://arxiv.org/abs/2312.12430)

    引入了通过广播查询编码器实现的高效标题重新排序器和为标题重新排序定制的Sigmoid Trick损失函数，相结合在KILT知识基准测试的数据集上取得了最先进的结果。

    

    在最近的RAG方法中，重新排序器在提升检索准确性方面发挥着关键作用，能够揭示每对查询和文本之间的逻辑关系。然而，现有的重新排序器需要反复对查询和大量长文本进行编码。这导致了较高的计算成本，并限制了检索文本的数量，从而影响了准确性。作为问题的解决方案，我们引入了通过广播查询编码器实现的高效标题重新排序器，这是一种用于标题重新排序的新技术，可以使速度提高20倍至40倍，超过基准通道重新排序器。此外，我们还引入了Sigmoid Trick，一种为标题重新排序定制的新损失函数。将这两种技术结合起来，我们在从KILT知识基准测试中实验的四个数据集上都经验验证了它们的有效性，实现了最先进的结果。

    arXiv:2312.12430v3 Announce Type: replace-cross  Abstract: In recent RAG approaches, rerankers play a pivotal role in refining retrieval accuracy with the ability of revealing logical relations for each pair of query and text. However, existing rerankers are required to repeatedly encode the query and a large number of long retrieved text. This results in high computational costs and limits the number of retrieved text, hindering accuracy. As a remedy of the problem, we introduce the Efficient Title Reranker via Broadcasting Query Encoder, a novel technique for title reranking that achieves a 20x-40x speedup over the vanilla passage reranker. Furthermore, we introduce Sigmoid Trick, a novel loss function customized for title reranking. Combining both techniques, we empirically validated their effectiveness, achieving state-of-the-art results on all four datasets we experimented with from the KILT knowledge benchmark.
    
[^222]: RLHF中的政策优化：超出偏好数据的影响

    Policy Optimization in RLHF: The Impact of Out-of-preference Data

    [https://arxiv.org/abs/2312.10584](https://arxiv.org/abs/2312.10584)

    本研究比较了直接偏好优化与基于奖励模型的政策优化方法，研究表明超出偏好数据对政策优化的影响，发现在实验证实中，RMB-PO+表现最佳。

    

    本文研究了两种流行的智能体与人类偏好和价值观对齐的方法：直接偏好优化 (DPO) 和基于奖励模型的政策优化 (RMB-PO)。还考虑了RMB-PO的变体，称为RMB-PO+。这些方法通过偏好数据明确或隐式地学习奖励模型，在用于政策优化的数据中有所不同，以解锁奖励模型的泛化能力。具体来说，与DPO相比，RMB-PO还使用策略生成的数据，而RMB-PO+进一步利用新的无偏好数据。我们研究了这种超出偏好数据的影响。通过受控和合成实验进行的研究表明，DPO表现不佳，而RMB-PO+表现最佳。特别是，即使为策略模型提供了良好的特征表示，我们发现用充分的政策优化进行时

    arXiv:2312.10584v2 Announce Type: replace  Abstract: Aligning intelligent agents with human preferences and values is important. This paper examines two popular alignment methods: Direct Preference Optimization (DPO) and Reward-Model-Based Policy Optimization (RMB-PO). A variant of RMB-PO, referred to as RMB-PO+ is also considered. These methods, either explicitly or implicitly, learn a reward model from preference data and differ in the data used for policy optimization to unlock the generalization ability of the reward model. In particular, compared with DPO, RMB-PO additionally uses policy-generated data, and RMB-PO+ further leverages new, preference-free data. We examine the impact of such out-of-preference data. Our study, conducted through controlled and synthetic experiments, demonstrates that DPO performs poorly, whereas RMB-PO+ performs the best. In particular, even when providing the policy model with a good feature representation, we find that policy optimization with adequa
    
[^223]: MaxK-GNN: 探索加速图神经网络训练的理论速度极限

    MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training

    [https://arxiv.org/abs/2312.08656](https://arxiv.org/abs/2312.08656)

    MaxK-GNN是一种先进的高性能GPU训练系统，通过MaxK非线性和理论分析，实现了图神经网络训练的垂直优化。

    

    在深度神经网络训练加速方面，GPU已经成为主流平台。 GPU在GNN上面临着诸多挑战，如工作负载不平衡和内存访问不规则，导致硬件利用不充分。现有解决方案例如PyG、DGL与cuSPARSE，以及GNNAdvisor框架部分解决了这些挑战，但内存流量仍然很显著。 我们认为，只有通过算法与系统创新的垂直优化才能实现显著的性能提升，而不是将加速优化视为“事后思考”（即（i）给定GNN算法，设计加速器，或（ii）给定硬件，主要优化GNN算法）。 本文介绍了MaxK-GNN，一种集成算法与系统创新的先进高性能GPU训练系统。 （i）我们引入了MaxK非线性并提供了MaxK非线性的理论分析，

    arXiv:2312.08656v3 Announce Type: replace-cross  Abstract: In the acceleration of deep neural network training, the GPU has become the mainstream platform. GPUs face substantial challenges on GNNs, such as workload imbalance and memory access irregularities, leading to underutilized hardware. Existing solutions such as PyG, DGL with cuSPARSE, and GNNAdvisor frameworks partially address these challenges but memory traffic is still significant.   We argue that drastic performance improvements can only be achieved by the vertical optimization of algorithm and system innovations, rather than treating the speedup optimization as an "after-thought" (i.e., (i) given a GNN algorithm, designing an accelerator, or (ii) given hardware, mainly optimizing the GNN algorithm). In this paper, we present MaxK-GNN, an advanced high-performance GPU training system integrating algorithm and system innovation. (i) We introduce the MaxK nonlinearity and provide a theoretical analysis of MaxK nonlinearity as
    
[^224]: GPT-4V(ision)对分布转移的适应性如何？初步调查

    How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation

    [https://arxiv.org/abs/2312.07424](https://arxiv.org/abs/2312.07424)

    该研究对GPT-4V(ision)在动态环境中的适应能力和泛化能力进行了评估，对比了其与CLIP、LLaVA和Gemini等知名模型。

    

    在机器学习领域，针对分布转移的泛化能力——即部署条件与训练场景不一致的情况——至关重要，特别是在诸如气候建模、生物医学和自动驾驶等领域。基于广泛预训练和任务多样性而区别于其他模型的基础模型的出现，引发了对它们对分布转移的适应能力的增加兴趣。GPT-4V(ision)作为最先进的公开获取的多模式基础模型，在各个领域，包括异常检测、视频理解、图像生成和医学诊断等方面有广泛应用。然而，它对数据分布的稳健性仍然较少被探究。针对这一空白，本研究对GPT-4V在动态环境中的适应能力和泛化能力进行了严格评估，与CLIP、LLaVA和Gemini等知名模型进行了对比。

    arXiv:2312.07424v3 Announce Type: replace-cross  Abstract: In machine learning, generalization against distribution shifts -- where deployment conditions diverge from the training scenarios -- is crucial, particularly in fields like climate modeling, biomedicine, and autonomous driving. The emergence of foundation models, distinguished by their extensive pretraining and task versatility, has led to an increased interest in their adaptability to distribution shifts. GPT-4V(ision) acts as the most advanced publicly accessible multimodal foundation model, with extensive applications across various domains, including anomaly detection, video understanding, image generation, and medical diagnosis. However, its robustness against data distributions remains largely underexplored. Addressing this gap, this study rigorously evaluates GPT-4V's adaptability and generalization capabilities in dynamic environments, benchmarking against prominent models like CLIP, LLaVA, and Gemini. We delve into GP
    
[^225]: 低成本高功率成员推断攻击

    Low-Cost High-Power Membership Inference Attacks

    [https://arxiv.org/abs/2312.03262](https://arxiv.org/abs/2312.03262)

    提出了一种新颖、高效且强大的成员推断攻击（RMIA），具有更准确的建模和更高的测试能力，适用于隐私风险评估。

    

    成员推断攻击（MIA）旨在检测特定数据点是否在训练机器学习模型时使用。最近一些强大的攻击具有较高的计算成本，并在不同条件下表现不一致，使它们对于实际的隐私风险评估不可靠。我们设计了一种新颖、高效且强大的成员推断攻击（RMIA），能够准确区分模型的总体数据和训练数据，同时计算开销最小。我们通过在似然比检验中更准确地建模零假设设置，并有效地利用来自总体的参考模型和参考数据样本，实现了这一目标。我们的算法在真正率（true-positive rate）方面表现出比先前方法更高的测试能力，整个TPR-FPR曲线都具备这种优势，即使在极低的误报率下（低至0）也是如此。在计算约束条件下，只有有限数量的情况下，

    arXiv:2312.03262v2 Announce Type: replace-cross  Abstract: Membership inference attacks (MIA) aim to detect if a particular data point was used in training a machine learning model. Recent strong attacks have high computational costs and inconsistent performance under varying conditions, rendering them unreliable for practical privacy risk assessment. We design a novel, efficient, and robust membership inference attack (RMIA) which accurately differentiates between population data and training data of a model, with minimal computational overhead. We achieve this by a more accurate modeling of the null hypothesis setting in our likelihood ratio tests, and effectively leveraging both reference models and reference data samples from the population. Our algorithm exhibits superior test power (true-positive rate) compared to prior methods, throughout the TPR-FPR curve including at extremely low false-positive rates (as low as 0). Under computation constraints, where only a limited number of
    
[^226]: 学习多图结构用于时间知识图推理

    Learning Multi-graph Structure for Temporal Knowledge Graph Reasoning

    [https://arxiv.org/abs/2312.03004](https://arxiv.org/abs/2312.03004)

    提出了一种关注学习多图结构的创新推理方法，用于解决时间知识图推理中存在的历史依赖和未来趋势反映不充分的问题。

    

    抽象：将基于历史快照的未来事件预测的时间知识图（TKG）推理称为外推，已引起广泛关注。由于其极端的多样性和空间与时间相关性的变化，TKG推理呈现出一项具有挑战性的任务，要求有效捕获事实之间的并发结构和演变交互作用。虽然现有方法在这方面已经取得了进展，但它们仍然无法充分利用TKG的多种内在表达语义形式，其中包括跨多个时间戳的实体相关性和时间信息的周期性。这种限制限制了它们充分反映历史依赖关系和未来趋势的能力。为了应对这些缺点，本文提出了一种关注学习多图结构（LMS）的创新推理方法。

    arXiv:2312.03004v2 Announce Type: replace-cross  Abstract: Temporal Knowledge Graph (TKG) reasoning that forecasts future events based on historical snapshots distributed over timestamps is denoted as extrapolation and has gained significant attention. Owing to its extreme versatility and variation in spatial and temporal correlations, TKG reasoning presents a challenging task, demanding efficient capture of concurrent structures and evolutional interactions among facts. While existing methods have made strides in this direction, they still fall short of harnessing the diverse forms of intrinsic expressive semantics of TKGs, which encompass entity correlations across multiple timestamps and periodicity of temporal information. This limitation constrains their ability to thoroughly reflect historical dependencies and future trends. In response to these drawbacks, this paper proposes an innovative reasoning approach that focuses on Learning Multi-graph Structure (LMS). Concretely, it com
    
[^227]: FRAPP'E：一个用于后处理的群体公平性框架

    FRAPP\'E: A Group Fairness Framework for Post-Processing Everything

    [https://arxiv.org/abs/2312.02592](https://arxiv.org/abs/2312.02592)

    提出了一个将任何正则化的处理中方法转化为后处理方法的框架，适用于更广泛的问题设置，保持了良好的公平错误权衡，并且可能提高之前方法的效力

    

    尽管在处理中实现了有前途的公平错误权衡，但群体公平性的处理方法无法在许多计算资源有限或无法访问预测模型训练管道的实际应用中使用。在这些情况下，后处理是一个可行的替代方法。然而，当前方法专为特定问题设置和公平性定义而设计，因此不如处理中方法广泛适用。在这项工作中，我们提出了一个框架，将任何正则化的处理中方法转化为后处理方法。这一方法提供了一种获得更广泛问题设置的后处理技术的途径，远超过以前的后处理文献。我们理论上并通过大量实验展示，我们的框架保留了处理中实现的良好公平错误权衡，并且能够提高先前方法的效力。

    arXiv:2312.02592v2 Announce Type: replace  Abstract: Despite achieving promising fairness-error trade-offs, in-processing mitigation techniques for group fairness cannot be employed in numerous practical applications with limited computation resources or no access to the training pipeline of the prediction model. In these situations, post-processing is a viable alternative. However, current methods are tailored to specific problem settings and fairness definitions and hence, are not as broadly applicable as in-processing. In this work, we propose a framework that turns any regularized in-processing method into a post-processing approach. This procedure prescribes a way to obtain post-processing techniques for a much broader range of problem settings than the prior post-processing literature. We show theoretically and through extensive experiments that our framework preserves the good fairness-error trade-offs achieved with in-processing and can improve over the effectiveness of prior p
    
[^228]: ULMA：人类演示和逐点偏好统一语言模型对齐

    ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference

    [https://arxiv.org/abs/2312.02554](https://arxiv.org/abs/2312.02554)

    提出了一种逐点直接偏好优化方法，用于统一语言模型对齐，通过将人类演示和逐点偏好相结合，解决了偏好学习中存在的信息丢失和性能次优问题。

    

    将语言模型与人类期望（例如，有益和无害）对齐已成为大型语言模型的迫切挑战。典型的对齐过程包括监督微调和偏好学习。大多数偏好学习方法（如RLHF和DPO）依赖于成对偏好数据，这并不充分地解决人类反馈是逐点的情况，导致潜在信息丢失和性能次优。为了解决这一问题，我们引入了逐点直接偏好优化，一种旨在有效利用逐点反馈的新颖偏好学习方法。我们的工作还揭示了监督微调和逐点偏好学习之间的新颖联系，最终形成了统一语言模型对齐，这是一种将对齐与人类演示和逐点偏好统一的单步方法。在逐点偏好数据集上进行了大量实验。

    arXiv:2312.02554v2 Announce Type: replace-cross  Abstract: Aligning language models to human expectations, e.g., being helpful and harmless, has become a pressing challenge for large language models. A typical alignment procedure consists of supervised fine-tuning and preference learning. Most preference learning methods, such as RLHF and DPO, depend on pairwise preference data, which inadequately address scenarios where human feedback is point-wise, leading to potential information loss and suboptimal performance. Addressing this gap, we introduce Point-wise Direct Preference Optimization, a novel preference learning method designed to harness point-wise feedback effectively. Our work also uncovers a novel connection between supervised fine-tuning and point-wise preference learning, culminating in Unified Language Model Alignment, a single-step method that unifies the alignment with human demonstrations and point-wise preferences. Extensive experiments on point-wise preference dataset
    
[^229]: xTrimoGene：用于单细胞RNA-Seq数据的高效可扩展表示学习器

    xTrimoGene: An Efficient and Scalable Representation Learner for Single-Cell RNA-Seq Data

    [https://arxiv.org/abs/2311.15156](https://arxiv.org/abs/2311.15156)

    xTrimoGene是一种针对scRNA-seq数据的新型非对称编码器-解码器Transformer，利用数据的稀疏特性降低了计算复杂度，使得在保持高准确性的同时能够训练最大的转移学习模型。

    

    高通量测序技术的进步已经在单细胞水平上测量基因表达方面取得了显著进展。可公开获取的单细胞RNA测序（scRNA-seq）数据量已经超过了5000万条人类记录，每条记录测量了2万个基因。这突出了对无监督表示学习的需求，然而传统的Transformer架构在这些数据上进行训练在计算和内存方面都是不可行的。为了解决这一挑战，我们提出了一种新颖的用于scRNA-seq数据的非对称编码器-解码器Transformer，称为xTrimoGene$^\alpha$（或简称为xTrimoGene），它利用了数据的稀疏特性来扩展预训练。xTrimoGene的可扩展设计将FLOPs降低了一个到两个数量级，而与传统Transformer相比仍保持高准确性，使我们能够训练最大的转移学习模型。

    arXiv:2311.15156v2 Announce Type: replace-cross  Abstract: Advances in high-throughput sequencing technology have led to significant progress in measuring gene expressions at the single-cell level. The amount of publicly available single-cell RNA-seq (scRNA-seq) data is already surpassing 50M records for humans with each record measuring 20,000 genes. This highlights the need for unsupervised representation learning to fully ingest these data, yet classical transformer architectures are prohibitive to train on such data in terms of both computation and memory. To address this challenge, we propose a novel asymmetric encoder-decoder transformer for scRNA-seq data, called xTrimoGene$^\alpha$ (or xTrimoGene for short), which leverages the sparse characteristic of the data to scale up the pre-training. This scalable design of xTrimoGene reduces FLOPs by one to two orders of magnitude compared to classical transformers while maintaining high accuracy, enabling us to train the largest transf
    
[^230]: Tube-NeRF：使用Tube-Guided数据增强和NeRFs从MPC进行视运动策略的高效模仿学习

    Tube-NeRF: Efficient Imitation Learning of Visuomotor Policies from MPC using Tube-Guided Data Augmentation and NeRFs

    [https://arxiv.org/abs/2311.14153](https://arxiv.org/abs/2311.14153)

    结合IL和鲁棒MPC，设计了一种名为Tube-NeRF的数据增强方法，利用NeRFs生成合成图像，通过管的特性选择相关视图，高效计算对应的动作，从而实现了视觉导向策略的高效学习。

    

    模仿学习（IL）可以从资源密集型的模型预测控制器（MPC）中训练计算效率高的感知动作策略，但通常需要大量样本，导致训练时间长或鲁棒性有限。为了解决这些问题，我们将IL与一种考虑过程和传感不确定性的鲁棒MPC的变体相结合，设计了一种数据增强（DA）策略，实现了基于视觉的策略的高效学习。提出的DA方法名为Tube-NeRF，利用神经辐射场（NeRFs）生成新颖的合成图像，并利用鲁棒MPC的性质（管）选择相关视图，并有效计算相应的动作。我们将方法量身定制为多旋翼上的定位和轨迹跟踪任务，通过学习一个仅使用机载摄像机图像作为水平位置唯一来源的视动作策略来生成控制动作。

    arXiv:2311.14153v2 Announce Type: replace-cross  Abstract: Imitation learning (IL) can train computationally-efficient sensorimotor policies from a resource-intensive Model Predictive Controller (MPC), but it often requires many samples, leading to long training times or limited robustness. To address these issues, we combine IL with a variant of robust MPC that accounts for process and sensing uncertainties, and we design a data augmentation (DA) strategy that enables efficient learning of vision-based policies. The proposed DA method, named Tube-NeRF, leverages Neural Radiance Fields (NeRFs) to generate novel synthetic images, and uses properties of the robust MPC (the tube) to select relevant views and to efficiently compute the corresponding actions. We tailor our approach to the task of localization and trajectory tracking on a multirotor, by learning a visuomotor policy that generates control actions using images from the onboard camera as only source of horizontal position. Nume
    
[^231]: 在长上下文大语言模型中推进Transformer架构：一项全面调查

    Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey

    [https://arxiv.org/abs/2311.12351](https://arxiv.org/abs/2311.12351)

    本论文对基于Transformer的大型语言模型架构的最新进展进行了全面调查，旨在增强其处理长上下文能力，从预训练到推断过程中进行了分类和分析。

    

    基于Transformer的大型语言模型（LLMs）已应用于知识库、人机界面和动态代理等多个领域，标志着迈向达到人工通用智能(AGI)的一大步。然而，当前的LLMs主要是在短文本片段上进行预训练，这危及了它们在处理在实际场景中频繁遇到的长上下文提示时的有效性。本文对最近在旨在增强LLMs长上下文能力的基于Transformer的LLM架构的进展进行了全面调查，涵盖了整个模型生命周期，从预训练到推断。首先，我们阐述并分析了当前基于Transformer模型处理长上下文输入和输出的问题。然后，我们提供了一个解决这些问题的Transformer架构升级的分类和景观。随后，我们进行了一项调查

    arXiv:2311.12351v2 Announce Type: replace  Abstract: Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investi
    
[^232]: 设计部署的机器学习算法监控策略：通过因果镜头导航有效性

    Designing monitoring strategies for deployed machine learning algorithms: navigating performativity through a causal lens

    [https://arxiv.org/abs/2311.11463](https://arxiv.org/abs/2311.11463)

    监控部署的机器学习算法的性能是重要的，该研究探讨了通过因果镜头导航解决有效性问题的方法。

    

    机器学习(ML)系统部署后，监控其性能对于确保算法长期安全有效至关重要。当ML算法与其环境互动时，算法可能影响数据生成机制，并在评估其独立性能时成为主要偏见源，这一问题被称为有效性问题。先前的工作已经展示了如何使用因果推断技术在有效性存在的情况下验证模型，但在有效性存在的环境中监控模型的工作却很少。与模型验证设置不同，对于要监控哪些性能指标没有很多一致性。不同的监控标准会影响结果的可解释性，可辨识性所需的假设，以及检测速度。当这一选择进一步与使用观察性与不平等性的决定相结合时

    arXiv:2311.11463v2 Announce Type: replace  Abstract: After a machine learning (ML)-based system is deployed, monitoring its performance is important to ensure the safety and effectiveness of the algorithm over time. When an ML algorithm interacts with its environment, the algorithm can affect the data-generating mechanism and be a major source of bias when evaluating its standalone performance, an issue known as performativity. Although prior work has shown how to validate models in the presence of performativity using causal inference techniques, there has been little work on how to monitor models in the presence of performativity. Unlike the setting of model validation, there is much less agreement on which performance metrics to monitor. Different monitoring criteria impact how interpretable the resulting test statistic is, what assumptions are needed for identifiability, and the speed of detection. When this choice is further coupled with the decision to use observational versus in
    
[^233]: 多模拟推理的深度融合：深度融合用于多模态模拟推理

    Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based Inference

    [https://arxiv.org/abs/2311.10671](https://arxiv.org/abs/2311.10671)

    提出了多模态神经后验估计 (MultiNPE) 方法，利用深度融合学习整合不同来源的异构数据，在模拟推理中提高了对复杂数学模型参数的准确推断能力。

    

    我们提出多模态神经后验估计(MultiNPE)，这是一种利用神经网络在模拟推理中整合来自不同来源的异构数据的方法。受深度融合学习的进展启发，它赋予研究人员分析来自不同领域的数据并推断复杂数学模型参数的能力，提高了准确性。我们针对MultiNPE制定了多模态融合方法（早期、后期、混合），并在三项具有挑战性的实验中评估它们的性能。MultiNPE不仅在参考任务上优于单一数据源基线，还在神经科学和心脏病学的科学模型推理上取得了卓越成绩。我们系统地研究了部分缺失数据对不同融合策略的影响。在我们的实验中，后期和混合融合技术成为多模态模拟推理实际应用的首选方法。

    arXiv:2311.10671v2 Announce Type: replace-cross  Abstract: We present multimodal neural posterior estimation (MultiNPE), a method to integrate heterogeneous data from different sources in simulation-based inference with neural networks. Inspired by advances in deep fusion learning, it empowers researchers to analyze data from different domains and infer the parameters of complex mathematical models with increased accuracy. We formulate multimodal fusion approaches for \hbox{MultiNPE} (early, late, hybrid) and evaluate their performance in three challenging experiments. MultiNPE not only outperforms single-source baselines on a reference task, but also achieves superior inference on scientific models from neuroscience and cardiology. We systematically investigate the impact of partially missing data on the different fusion strategies. Across our experiments, late and hybrid fusion techniques emerge as the methods of choice for practical applications of multimodal simulation-based infere
    
[^234]: 通过实时验证和纠正减轻大型语言模型中的虚构问题

    Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification

    [https://arxiv.org/abs/2311.09114](https://arxiv.org/abs/2311.09114)

    通过实时验证和纠正的策略，文章提出了一种名为Ever的方法，用于减轻大型语言模型生成中的虚构问题。

    

    大型语言模型(LLMs)在生成流畅文本方面表现出色。然而，它们经常遇到生成不准确或虚构内容的挑战。这个问题普遍存在于非基于检索的生成和检索增强生成方法中，现有的事后纠正方法可能无法解决“滚雪球”问题导致的累积虚构错误，特别是在推理任务中。为了解决这些挑战，我们提出了一种名为“Ever”的新方法。Ever采用实时、逐步的生成和虚构纠正策略，而不是等到生成过程结束才纠正虚构。其主要目标是在文本生成过程中检测和纠正虚构。与基于检索和非基于检索的基线模型相比，

    arXiv:2311.09114v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the "snowballing" issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (Ever). Instead of waiting until the end of the generation process to rectify hallucinations, Ever employs a real-time, step-wise generation and hallucination rectification strategy. The primary objective is to detect and rectify hallucinations as they occur during the text generation process. When compared to both retrieval-based and non-retrieval-based basel
    
[^235]: 朝向多步推理的答案校准统一视图

    Towards A Unified View of Answer Calibration for Multi-Step Reasoning

    [https://arxiv.org/abs/2311.09101](https://arxiv.org/abs/2311.09101)

    本文总结了最近答案校准技术的分类法，从统一视角对步级和路径级答案校准进行了彻底评估，结果显示整合两种策略的优势倾向于产生最佳结果。

    

    大型语言模型（LLMs）使用“思维链”提示扩展了改进多步推理能力的范围。我们通常将多步推理分为两个阶段：路径生成以生成推理路径；和答案校准后处理推理路径以获得最终答案。然而，现有文献缺乏对不同答案校准方法的系统分析。本文总结了最近答案校准技术的分类法，并将其分解为步级和路径级策略。然后，我们从统一视角对这些策略进行了彻底评估，系统地审查了多路径上的步级和路径级答案校准。实验结果表明，整合两种策略的优势倾向于产生最佳结果。我们的研究有可能启示优化多步推理系统的关键见解。

    arXiv:2311.09101v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. We generally divide multi-step reasoning into two phases: path generation to generate the reasoning path(s); and answer calibration post-processing the reasoning path(s) to obtain a final answer. However, the existing literature lacks systematic analysis on different answer calibration approaches. In this paper, we summarize the taxonomy of recent answer calibration techniques and break them down into step-level and path-level strategies. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Experimental results reveal that integrating the dominance of both strategies tends to derive optimal outcomes. Our study holds the potential to illuminate key insights for opti
    
[^236]: 作者归属模型能否区分演讲文本中的发言人？

    Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?

    [https://arxiv.org/abs/2311.07564](https://arxiv.org/abs/2311.07564)

    本文研究了作者归属模型在演讲文本中区分发言人的能力，并提出了以会话演讲文本为重点的发言人归属基准。

    

    作者归属验证是确定两个不同书面样本是否同属一作者的任务，通常涉及对书面文本的归因。本文探讨了转录演讲的归属问题，这带来了新的挑战。其中一个主要挑战是，许多文体特征，如标点和大写，在这种情境下并不具备信息量。另一方面，转录的演讲呈现其他模式，如填充词和回应性声音（例如“嗯”，“嗯，嗯”），这些可能是不同发言人的特征性表现。我们提出了一个新的以会话演讲文本为重点的发言人归属基准。为了限制发言人与话题之间的虚假关联，我们使用会话提示和参与同一对话的发言人构建不同难度的验证试验。通过比较一系列方法，在这一新基准上建立了最新技术水平。

    arXiv:2311.07564v2 Announce Type: replace  Abstract: Authorship verification is the task of determining if two distinct writing samples share the same author and is typically concerned with the attribution of written text. In this paper, we explore the attribution of transcribed speech, which poses novel challenges. The main challenge is that many stylistic features, such as punctuation and capitalization, are not informative in this setting. On the other hand, transcribed speech exhibits other patterns, such as filler words and backchannels (e.g., 'um', 'uh-huh'), which may be characteristic of different speakers. We propose a new benchmark for speaker attribution focused on conversational speech transcripts. To limit spurious associations of speakers with topic, we employ both conversation prompts and speakers participating in the same conversation to construct verification trials of varying difficulties. We establish the state of the art on this new benchmark by comparing a suite of
    
[^237]: 无线网络视频缓存的资源感知分层联邦学习

    Resource-Aware Hierarchical Federated Learning for Video Caching in Wireless Networks

    [https://arxiv.org/abs/2311.06918](https://arxiv.org/abs/2311.06918)

    本文提出了一种资源感知分层联邦学习方法(RawHFL)，用于无线网络视频缓存，通过预测用户未来的内容请求，以改善回传流量拥塞问题。该方法考虑了部分客户端参与，通过优化客户端的选择、本地训练轮次和CPU频率，以最小化一个加权效用函数，实现了RawHFL的收敛。

    

    视频缓存通过本地存储用户频繁请求的热门内容，可以显著改善回传流量拥塞问题。为了学习用户需求随时间的变化而保护隐私，本文提出了一种新颖的资源感知分层联邦学习(RawHFL)方法，用于预测用户未来的内容请求。考虑到部分客户端参与的情况，我们首先推导出全局梯度范数的上界，该范数取决于客户端的本地训练轮次和无线链路上累积梯度的成功接收。在延迟、能耗和无线资源约束条件下，我们优化客户端的选择、本地训练轮次和中央处理单元(CPU)频率，以最小化一个加权效用函数，从而促进RawHFL的收敛。

    Video caching can significantly improve backhaul traffic congestion by locally storing the popular content that users frequently request. A privacy-preserving method is desirable to learn how users' demands change over time. As such, this paper proposes a novel resource-aware hierarchical federated learning (RawHFL) solution to predict users' future content requests under the realistic assumptions that content requests are sporadic and users' datasets can only be updated based on the requested content's information. Considering a partial client participation case, we first derive the upper bound of the global gradient norm that depends on the clients' local training rounds and the successful reception of their accumulated gradients over the wireless links. Under delay, energy and radio resource constraints, we then optimize client selection and their local rounds and central processing unit (CPU) frequencies to minimize a weighted utility function that facilitates RawHFL's convergence 
    
[^238]: 学习的形状：基于Transformer模型的各向异性和内在维度研究

    The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models

    [https://arxiv.org/abs/2311.05928](https://arxiv.org/abs/2311.05928)

    本研究揭示了Transformer解码器中的各向异性呈钟状曲线，最高各向异性浓度在中间层，与编码器中更均匀分布的各向异性不同，并发现嵌入的内在维度在训练初期增加，随后在训练末期出现压缩，表明更紧凑的表示形式。

    

    在这项研究中，我们针对Transformer架构中嵌入的各向异性动态和内在维度展开调查，重点关注编码器和解码器之间的二分法。我们的研究结果显示，Transformer解码器中的各向异性配置呈现出明显的钟状曲线，具有最高的各向异性浓度在中间层。这种模式与编码器中观察到的更均匀分布的各向异性有所不同。此外，我们发现嵌入的内在维度在训练的初始阶段增加，表明向更高维空间的扩展。然后在训练末尾出现向更低维度的压缩阶段，暗示着对更紧凑表示的改进。我们的结果为理解编码器和解码器嵌入属性提供了新的见解。

    arXiv:2311.05928v2 Announce Type: replace-cross  Abstract: In this study, we present an investigation into the anisotropy dynamics and intrinsic dimension of embeddings in transformer architectures, focusing on the dichotomy between encoders and decoders. Our findings reveal that the anisotropy profile in transformer decoders exhibits a distinct bell-shaped curve, with the highest anisotropy concentrations in the middle layers. This pattern diverges from the more uniformly distributed anisotropy observed in encoders. In addition, we found that the intrinsic dimension of embeddings increases in the initial phases of training, indicating an expansion into higher-dimensional space. Which is then followed by a compression phase towards the end of training with dimensionality decrease, suggesting a refinement into more compact representations. Our results provide fresh insights to the understanding of encoders and decoders embedding properties.
    
[^239]: 学习学习以进行少样本持久主动学习

    Learning to Learn for Few-shot Continual Active Learning

    [https://arxiv.org/abs/2311.03732](https://arxiv.org/abs/2311.03732)

    提出了一种简单而高效的方法，Meta-Continual Active Learning，通过元学习和经验重播解决少样本持续主动学习中的任务混淆和灾难性遗忘，进一步结合文本增强来确保泛化。

    

    持续学习旨在确保解决先前见过的任务的稳定性，同时展示对新领域的可塑性。最近在持续学习方面的进展主要局限于监督学习设置，尤其是在自然语言处理领域。在这项工作中，我们考虑了一种少样本持续主动学习（CAL）设置，其中标记数据不足，但未标记数据充足，但有限的注释预算。我们提出了一种简单而高效的方法，称为元持续主动学习。具体地，我们采用元学习和经验重播来解决任务之间的混淆和灾难性遗忘。我们进一步结合文本增强来确保泛化性能。我们在基准文本分类数据集上进行了大量实验，以验证所提方法的有效性，并分析了在少样本CAL设置中不同主动学习策略的影响。我们的实验结果表明

    arXiv:2311.03732v2 Announce Type: replace-cross  Abstract: Continual learning strives to ensure stability in solving previously seen tasks while demonstrating plasticity in a novel domain. Recent advances in CL are mostly confined to a supervised learning setting, especially in NLP domain. In this work, we consider a few-shot continual active learning (CAL) setting where labeled data are inadequate, and unlabeled data are abundant but with a limited annotation budget. We propose a simple but efficient method, called Meta-Continual Active Learning. Specifically, we employ meta-learning and experience replay to address inter-task confusion and catastrophic forgetting. We further incorporate textual augmentations to ensure generalization. We conduct extensive experiments on benchmark text classification datasets to validate the effectiveness of the proposed method and analyze the effect of different active learning strategies in few-shot CAL setting. Our experimental results demonstrate t
    
[^240]: 可解释的风力发电预测建模：具有高准确性的玻璃箱方法

    Explainable Modeling for Wind Power Forecasting: A Glass-Box Approach with High Accuracy

    [https://arxiv.org/abs/2310.18629](https://arxiv.org/abs/2310.18629)

    提出了一种结合高准确性与透明性的玻璃箱方法，通过构建形状函数总结特征效应，有效映射风力输出与输入特征之间的非线性关系，并丰富了预测模型，以捕获输入特征之间的相互依赖性和协同作用。

    

    机器学习模型（例如神经网络）在风力发电预测中取得高准确性，但通常被视为缺乏可解释性的黑盒。为解决这一问题，本文提出了一种结合高准确性和透明性的玻璃箱方法，用于风力发电预测。具体而言，通过构建形状函数总结特征效应，有效地映射风力输出与输入特征之间复杂的非线性关系。此外，通过合并交互项，丰富了预测模型，巧妙地捕获了输入特征之间的相互依赖性和协同作用。所提出的玻璃箱方法的加法性质确保了其可解释性。模拟结果表明，所提出的玻璃箱方法能有效地从全局和实例角度解释风力发电预测的结果。此外，它的表现优于大多数基

    arXiv:2310.18629v2 Announce Type: replace  Abstract: Machine learning models (e.g., neural networks) achieve high accuracy in wind power forecasting, but they are usually regarded as black boxes that lack interpretability. To address this issue, the paper proposes a glass-box approach that combines high accuracy with transparency for wind power forecasting. Specifically, the core is to sum up the feature effects by constructing shape functions, which effectively map the intricate non-linear relationships between wind power output and input features. Furthermore, the forecasting model is enriched by incorporating interaction terms that adeptly capture interdependencies and synergies among the input features. The additive nature of the proposed glass-box approach ensures its interpretability. Simulation results show that the proposed glass-box approach effectively interprets the results of wind power forecasting from both global and instance perspectives. Besides, it outperforms most ben
    
[^241]: 关于任何预训练模型之间存在和前景的通用知识转移

    Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model

    [https://arxiv.org/abs/2310.17653](https://arxiv.org/abs/2310.17653)

    通过研究预训练模型之间的知识转移，并尝试在不降低性能的情况下实现“互补”知识的传递，本研究探讨了如何提高模型之间的通用知识转移。

    

    训练深度网络需要关于架构、数据增强或优化等各种设计决策。在这项工作中，我们发现这些训练变化导致网络从数据中学习出独特的特征集。利用包括数千个模型在诸如ImageNet之类的经典数据集上训练的公共模型库，我们观察到对于任意预训练模型的配对，一个模型提取出另一个模型中不可用的重要数据背景--而这与整体性能无关。鉴于任何任意预训练模型的配对以及没有外部排名（如独立测试集，例如由于数据隐私），我们探讨是否可能在不降低性能的情况下从一个模型转移这种“互补”知识到另一个模型--这一任务尤其困难，因为更强大、性能相同或更弱的模型中可能包含额外知识。然而，促进强大的转移

    arXiv:2310.17653v2 Announce Type: replace  Abstract: Training deep networks requires various design decisions regarding for instance their architecture, data augmentation, or optimization. In this work, we find these training variations to result in networks learning unique feature sets from the data. Using public model libraries comprising thousands of models trained on canonical datasets like ImageNet, we observe that for arbitrary pairings of pretrained models, one model extracts significant data context unavailable in the other -- independent of overall performance. Given any arbitrary pairing of pretrained models and no external rankings (such as separate test sets, e.g. due to data privacy), we investigate if it is possible to transfer such "complementary" knowledge from one model to another without performance degradation -- a task made particularly difficult as additional knowledge can be contained in stronger, equiperformant or weaker models. Yet facilitating robust transfer i
    
[^242]: 通过颜色化进行预训练的基于LiDAR的三维物体检测器

    Pre-Training LiDAR-Based 3D Object Detectors Through Colorization

    [https://arxiv.org/abs/2310.14592](https://arxiv.org/abs/2310.14592)

    本研究通过引入基于地面点颜色化（GPC）的创新预训练方法，教导模型着色LiDAR点云，提供宝贵的语义线索，显著改善了3D物体检测的效果。

    

    准确的自动驾驶汽车的三维物体检测和理解在很大程度上依赖于LiDAR点云，需要大量标记数据进行训练。本文介绍一种创新的预训练方法，Grounded Point Colorization (GPC)，通过教导模型着色LiDAR点云来弥合数据和标签之间的差距，为其提供宝贵的语义线索。为了解决由颜色变化和选择偏差引起的挑战，我们通过在着色过程中提供地面真实颜色作为提示，将颜色作为“上下文”来加以考虑。在KITTI和Waymo数据集上的实验结果展示了GPC的显著有效性。即使是有限标记数据，GPC也显着提高了微调性能；值得注意的是，在仅使用KITTI数据集的20%进行训练时，GPC的表现优于使用整个数据集从头开始训练。总之，我们为三维物体检测的预训练引入了新的视角，

    arXiv:2310.14592v2 Announce Type: replace-cross  Abstract: Accurate 3D object detection and understanding for self-driving cars heavily relies on LiDAR point clouds, necessitating large amounts of labeled data to train. In this work, we introduce an innovative pre-training approach, Grounded Point Colorization (GPC), to bridge the gap between data and labels by teaching the model to colorize LiDAR point clouds, equipping it with valuable semantic cues. To tackle challenges arising from color variations and selection bias, we incorporate color as "context" by providing ground-truth colors as hints during colorization. Experimental results on the KITTI and Waymo datasets demonstrate GPC's remarkable effectiveness. Even with limited labeled data, GPC significantly improves fine-tuning performance; notably, on just 20% of the KITTI dataset, GPC outperforms training from scratch with the entire dataset. In sum, we introduce a fresh perspective on pre-training for 3D object detection, aligni
    
[^243]: 关于利用大型语言模型进行双语词汇识别

    On Bilingual Lexicon Induction with Large Language Models

    [https://arxiv.org/abs/2310.13995](https://arxiv.org/abs/2310.13995)

    本文研究了利用大型语言模型进行双语词汇识别的潜力，通过研究零次提示和少量上下文提示等方法，探讨了这种方法如何与当前BLI方法相比，并如何进行补充。

    

    双语词汇识别（BLI）是多语言自然语言处理中的核心任务，目前在很大程度上仍然依赖于计算跨语言单词表示。受自然语言处理领域向大型语言模型（LLMs）的全球范式转变的启发，我们探讨了最新一代LLMs在双语词汇开发中的潜力。我们提出了以下研究问题：是否可能促使和微调多语言LLMs（mLLMs）以进行BLI，并且这种方法与当前BLI方法相比如何以及如何补充？为此，我们系统地研究了1）用于无监督BLI的零次提示和2）使用一组种子翻译对进行少量上下文提示，均无需进行任何LLM微调，以及3）对较小LLMs进行标准BLI导向微调。我们在涵盖不同大小（从0.3B到13B参数）的18个开源文本对文本mLLMs上进行实验，涵盖两个标准BLI基准测试。

    arXiv:2310.13995v2 Announce Type: replace-cross  Abstract: Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a rang
    
[^244]: 具有凸全局和局部约束的联邦学习

    Federated Learning with Convex Global and Local Constraints

    [https://arxiv.org/abs/2310.10117](https://arxiv.org/abs/2310.10117)

    该论文提出了一种针对具有约束的机器学习问题的新联邦学习算法，建立了基于凸目标和凸约束的最坏情况复杂度，并通过数值实验证明了算法的有效性

    

    在实践中，许多机器学习（ML）问题都带有约束，其应用领域涉及无法与他人共享的分布式敏感数据，例如在医疗保健中。在这种实际场景中的协作学习涉及联邦学习（FL）用于带约束的ML问题，或简称带约束的FL。尽管近年来FL技术得到了广泛发展，但这些技术仅处理无约束的FL问题。为了填补这一空白，我们迈出了解决带约束FL问题的通用算法框架的第一步。具体而言，我们基于近端增广拉格朗日（AL）方法，提出了一种针对受限制ML问题的新FL算法。假设凸目标和凸约束以及其他一些温和条件，我们确定了所提算法的最坏情况复杂度。我们的数值实验证明了我们的算法的有效性。

    arXiv:2310.10117v2 Announce Type: replace  Abstract: In practice, many machine learning (ML) problems come with constraints, and their applied domains involve distributed sensitive data that cannot be shared with others, e.g., in healthcare. Collaborative learning in such practical scenarios entails federated learning (FL) for ML problems with constraints, or FL with constraints for short. Despite the extensive developments of FL techniques in recent years, these techniques only deal with unconstrained FL problems. To fill this gap, we take the first step toward building a general algorithmic framework for solving FL problems with constraints. In particular, we propose a new FL algorithm for constrained ML problems based on the proximal augmented Lagrangian (AL) method. Assuming convex objective and convex constraints plus other mild conditions, we establish the worst-case complexity of the proposed algorithm. Our numerical experiments show the effectiveness of our algorithm in perform
    
[^245]: 让循环的询问: 大型语言模型在判断中的摇摆

    Ask Again, Then Fail: Large Language Models' Vacillations in Judgement

    [https://arxiv.org/abs/2310.02174](https://arxiv.org/abs/2310.02174)

    目前的语言模型在面对后续问题时常常摇摆不定，研究者提出了一个后续问题机制和两个度量标准来量化这种不一致性，并开发出Unwavering-FQ框架来教导模型保持最初的正确判断，实验证明其有效性。

    

    我们观察到目前的会话式语言模型在面对后续问题时往往在其判断上摇摆不定，即使原始判断是正确的。这种摇摆对于生成可靠回复和建立用户信任构成了重要挑战。为了全面评估这一问题，我们引入了一个后续问题机制以及两个度量标准来量化这种不一致性，确认了当前语言模型普遍存在这种情况。为了缓解这一问题，我们探讨了各种提示策略用于闭源模型；此外，我们开发了一个基于训练的框架Unwavering-FQ，通过合成高质量的偏好数据来教导语言模型保持其最初的正确判断。我们的实验结果验证了我们框架的有效性以及其增强模型通用能力的能力。

    arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
    
[^246]: 探索LLM代理的协作机制：社会心理学视角

    Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View

    [https://arxiv.org/abs/2310.02124](https://arxiv.org/abs/2310.02124)

    通过实践实验和理论洞察，探究当代NLP系统之间的协作机制，发现某些协作策略优于先前的方法，并且优化了效率。

    

    随着自然语言处理（NLP）系统越来越多地应用于复杂的社会环境中，一个迫切的问题出现了：这些NLP系统能否模仿类人类的协作智能，在由多个大型语言模型（LLMs）组成的多代理社会中？本文通过将实践实验与理论观点相结合，探究当代NLP系统之间的协作机制。我们构建了四个由LLM代理组成的独特“社会”，每个代理以特定的“特质”（随和或过于自信）为特征，并与不同的“思维模式”（辩论或反思）展开协作。通过在三个基准数据集上评估这些多代理社会，我们发现某些协作策略不仅胜过先前顶尖方法，而且优化了效率（使用更少的API令牌）。此外，我们的结果进一步说明LLM代理可以

    arXiv:2310.02124v2 Announce Type: replace-cross  Abstract: As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents mani
    
[^247]: 通过平衡内存工作负载优化改进自动并行训练

    Improving Automatic Parallel Training via Balanced Memory Workload Optimization

    [https://arxiv.org/abs/2307.02031](https://arxiv.org/abs/2307.02031)

    本文提出了Galvatron-BMW，一个新的系统框架，通过平衡内存工作负载优化，集成多个并行维度并自动识别最有效的混合并行策略，通过决策树方法和动态规划搜索算法来有效地处理复杂的训练挑战。

    

    Transformer模型已成为实现各种应用领域最先进性能的领先方法，为高级大规模深度学习(DL)模型奠定了基础。然而，由于并行选项的丰富性，跨多个GPU有效训练这些模型仍然是一个复杂的挑战。现有的DL系统要么需要手动设计分布式训练计划，要么将并行组合限制在约束的搜索空间中。在本文中，我们提出了Galvatron-BMW，一个集成多个主流并行维度并自动确定最有效混合并行策略的新型系统框架。为了有效地遍历这个庞大的搜索空间，我们采用了一个基于直观见解的决策树方法进行分解和修剪。我们进一步利用动态规划搜索算法推出最优计划。

    arXiv:2307.02031v2 Announce Type: replace  Abstract: Transformer models have emerged as the leading approach for achieving state-of-the-art performance across various application domains, serving as the foundation for advanced large-scale deep learning (DL) models. However, efficiently training these models across multiple GPUs remains a complex challenge due to the abundance of parallelism options. Existing DL systems either require manual efforts to design distributed training plans or limit parallelism combinations to a constrained search space. In this paper, we present Galvatron-BMW, a novel system framework that integrates multiple prevalent parallelism dimensions and automatically identifies the most efficient hybrid parallelism strategy. To effectively navigate this vast search space, we employ a decision tree approach for decomposition and pruning based on intuitive insights. We further utilize a dynamic programming search algorithm to derive the optimal plan. Moreover, to imp
    
[^248]: 通过语义匹配修复特征归因方法中的确认偏见

    Fixing confirmation bias in feature attribution methods via semantic match

    [https://arxiv.org/abs/2307.00897](https://arxiv.org/abs/2307.00897)

    提出了通过语义匹配修复特征归因方法中的确认偏见问题，引入了人类概念与（亚符号）解释之间的概念框架，并提出了一种结构化方法来评估语义匹配。

    

    特征归因方法已经成为解析黑盒模型复杂行为的重要方法。尽管取得了成功，一些学者指出这类方法存在严重缺陷：它们不能可靠地用人类概念进行解释。简而言之，仅仅可视化一系列特征贡献对于人类来说无法得出关于模型内部表示的结论，而确认偏见可能会让用户产生关于模型行为的错误信念。我们认为需要一种结构化方法来验证我们对模型的假设是否得到了特征归因的确认。这就是我们所说的人类概念与（亚符号）解释之间的“语义匹配”。在 Cin\`a等人[2023]提出的概念框架基础上，我们提出了一种结构化方法来在实践中评估语义匹配。我们在一系列实验中展示了这一过程。

    arXiv:2307.00897v2 Announce Type: replace-cross  Abstract: Feature attribution methods have become a staple method to disentangle the complex behavior of black box models. Despite their success, some scholars have argued that such methods suffer from a serious flaw: they do not allow a reliable interpretation in terms of human concepts. Simply put, visualizing an array of feature contributions is not enough for humans to conclude something about a model's internal representations, and confirmation bias can trick users into false beliefs about model behavior. We argue that a structured approach is required to test whether our hypotheses on the model are confirmed by the feature attributions. This is what we call the "semantic match" between human concepts and (sub-symbolic) explanations. Building on the conceptual framework put forward in Cin\`a et al. [2023], we propose a structured approach to evaluate semantic match in practice. We showcase the procedure in a suite of experiments spa
    
[^249]: DOS：多样化异常样本采样用于外域检测

    DOS: Diverse Outlier Sampling for Out-of-Distribution Detection

    [https://arxiv.org/abs/2306.02031](https://arxiv.org/abs/2306.02031)

    在外域检测性能中，多样性对于异常样本的采样至关重要，因此提出了一种名为DOS的多样化异常样本采样策略。

    

    现代神经网络在部署在开放世界时，对外域输入给出过于自信的预测，通常会利用替代异常数据集在训练期间对模型进行规范化，最近的研究强调了在设计异常数据集采样策略时的不确定性的作用。然而，仅基于预测不确定性选择的外域样本可能偏向于某些类型，无法完全捕捉整个异常分布。在这项工作中，我们经验证明多样性对于采样外域样本以提升性能至关重要。受到这一观察的启发，我们提出了一种名为DOS（多样化异常样本采样）的简单而新颖的采样策略，以选择多样化和信息丰富的异常样本。具体来说，我们在每次迭代中对标准化特征进行聚类，然后从每个簇中选择最具信息量的异常样本用于模型训练。

    arXiv:2306.02031v2 Announce Type: replace  Abstract: Modern neural networks are known to give overconfident prediction for out-of-distribution inputs when deployed in the open world. It is common practice to leverage a surrogate outlier dataset to regularize the model during training, and recent studies emphasize the role of uncertainty in designing the sampling strategy for outlier dataset. However, the OOD samples selected solely based on predictive uncertainty can be biased towards certain types, which may fail to capture the full outlier distribution. In this work, we empirically show that diversity is critical in sampling outliers for OOD detection performance. Motivated by the observation, we propose a straightforward and novel sampling strategy named DOS (Diverse Outlier Sampling) to select diverse and informative outliers. Specifically, we cluster the normalized features at each iteration, and the most informative outlier from each cluster is selected for model training with ab
    
[^250]: CaloClouds: 快速几何独立、高粒度量能器模拟

    CaloClouds: Fast Geometry-Independent Highly-Granular Calorimeter Simulation

    [https://arxiv.org/abs/2305.04847](https://arxiv.org/abs/2305.04847)

    通过生成模型和点云技术，本研究实现了在高粒度探测器中准确快速地模拟粒子簇，为将机器学习应用于粒子物理提供了重要突破。

    

    在高粒度探测器中模拟粒子簇是将机器学习应用于粒子物理的一个关键前沿。使用生成机器学习模型实现高准确度和速度将使其能够增强传统模拟，缓解主要的计算约束。本文通过首次直接在3D空间中生成少量能量沉积的空间点的点云，而无需依赖固定网格结构，在这一任务中取得了重大突破。这要归功于两个关键创新：i)利用生成模型的最新改进，我们应用扩散模型生成光子簇作为高基数的点云。ii)这些高达6000个空间点的点云在很大程度上与几何无关，因为它们是从最初最高分辨率高达40000个Geant4步骤的点云中进行下采样的。

    arXiv:2305.04847v2 Announce Type: replace-cross  Abstract: Simulating showers of particles in highly-granular detectors is a key frontier in the application of machine learning to particle physics. Achieving high accuracy and speed with generative machine learning models would enable them to augment traditional simulations and alleviate a major computing constraint. This work achieves a major breakthrough in this task by, for the first time, directly generating a point cloud of a few thousand space points with energy depositions in the detector in 3D space without relying on a fixed-grid structure. This is made possible by two key innovations: i) Using recent improvements in generative modeling we apply a diffusion model to generate photon showers as high-cardinality point clouds. ii) These point clouds of up to $6,000$ space points are largely geometry-independent as they are down-sampled from initial even higher-resolution point clouds of up to $40,000$ so-called Geant4 steps. We sho
    
[^251]: 使用条件样本学习隐马尔可夫模型

    Learning Hidden Markov Models Using Conditional Samples

    [https://arxiv.org/abs/2302.14753](https://arxiv.org/abs/2302.14753)

    本文提出了一种使用交互方式访问隐马尔可夫模型的条件分布样本的学习方法，实现了对HMM的高效学习算法，从而绕过了其密码学困难性。

    

    本文关注学习隐马尔可夫模型（HMM）的计算复杂性。虽然HMM是顺序和时间序列建模中最广泛使用的工具之一，但在标准设置下，即对观测序列的独立同分布（i.i.d.）样本具有访问权限的情况下，学习起来是具有密码学困难性的。本文偏离了这一设定，考虑了一种交互访问模型，在这种模型中，算法可以查询HMM的条件分布的样本。我们展示了对HMM的交互访问可以实现计算高效的学习算法，从而绕过密码学困难性。具体来说，我们设计了在两种情况下学习HMM的高效算法：（a）一种更容易的设置，我们可以查询准确条件概率。在这里，我们的算法在多项式时间内运行，并进行了多项式次查询，以在总变差距离中近似任何HMM。

    arXiv:2302.14753v2 Announce Type: replace-cross  Abstract: This paper is concerned with the computational complexity of learning the Hidden Markov Model (HMM). Although HMMs are some of the most widely used tools in sequential and time series modeling, they are cryptographically hard to learn in the standard setting where one has access to i.i.d. samples of observation sequences. In this paper, we depart from this setup and consider an interactive access model, in which the algorithm can query for samples from the conditional distributions of the HMMs. We show that interactive access to the HMM enables computationally efficient learning algorithms, thereby bypassing cryptographic hardness. Specifically, we obtain efficient algorithms for learning HMMs in two settings:   (a) An easier setting where we have query access to the exact conditional probabilities. Here our algorithm runs in polynomial time and makes polynomially many queries to approximate any HMM in total variation distance.
    
[^252]: 使用时间延迟神经网络实现多模态标点修复的高效集成

    Efficient Ensemble for Multimodal Punctuation Restoration using Time-Delay Neural Network

    [https://arxiv.org/abs/2302.13376](https://arxiv.org/abs/2302.13376)

    使用时间延迟神经网络实现了多模态标点修复的高效集成方法，相比当前最佳模型提高了1.0个F1分数，并且使用了更少的推理网络参数。

    

    标点修复在自动语音识别的后处理过程中起着至关重要的作用，但对模型效率的要求是关键。为此，我们提出了EfficientPunct，这是一种使用多模态时间延迟神经网络的集成方法，通过使用不到十分之一的推理网络参数，优于当前最佳模型1.0个F1分数。我们简化了语音识别器，以有效地输出用于标点修复的隐藏层声学嵌入，同时使用BERT提取有意义的文本嵌入。通过使用强制对齐和时间卷积，我们消除了注意力融合的必要性，极大提高了计算效率并提高了性能。EfficientPunct通过一个集成方法，对BERT纯粹基于语言的预测的权重略高于多模态网络的预测，树立了一个新的技术水平。我们的代码可在 https://githu

    arXiv:2302.13376v2 Announce Type: replace  Abstract: Punctuation restoration plays an essential role in the post-processing procedure of automatic speech recognition, but model efficiency is a key requirement for this task. To that end, we present EfficientPunct, an ensemble method with a multimodal time-delay neural network that outperforms the current best model by 1.0 F1 points, using less than a tenth of its inference network parameters. We streamline a speech recognizer to efficiently output hidden layer acoustic embeddings for punctuation restoration, as well as BERT to extract meaningful text embeddings. By using forced alignment and temporal convolutions, we eliminate the need for attention-based fusion, greatly increasing computational efficiency and raising performance. EfficientPunct sets a new state of the art with an ensemble that weights BERT's purely language-based predictions slightly more than the multimodal network's predictions. Our code is available at https://githu
    
[^253]: 量子机器学习中的傅里叶级数权重

    Fourier series weight in quantum machine learning

    [https://arxiv.org/abs/2302.00105](https://arxiv.org/abs/2302.00105)

    本研究确认了傅里叶级数对量子机器学习模型的影响，并提出了基于哈密顿编码的量子机器学习模型以及傅里叶系数的确定方法。

    

    在这项工作中，我们旨在确认傅里叶级数对量子机器学习模型的影响。我们将提出模型、测试和演示来实现这一目标。我们设计了一个基于哈密顿编码的量子机器学习模型。通过微妙的变化，我们进行了三角插值、二进制和多类分类器以及量子信号处理应用。我们还提出了一个基于量子机器学习确定傅里叶系数的方块图。我们使用Pennylane框架执行和测试了所有提出的模型。

    arXiv:2302.00105v2 Announce Type: replace-cross  Abstract: In this work, we aim to confirm the impact of the Fourier series on the quantum machine learning model. We will propose models, tests, and demonstrations to achieve this objective. We designed a quantum machine learning leveraged on the Hamiltonian encoding. With a subtle change, we performed the trigonometric interpolation, binary and multiclass classifier, and a quantum signal processing application. We also proposed a block diagram of determining approximately the Fourier coefficient based on quantum machine learning. We performed and tested all the proposed models using the Pennylane framework.
    
[^254]: 不偏不倚：少数族群指导扩散模型

    Don't Play Favorites: Minority Guidance for Diffusion Models

    [https://arxiv.org/abs/2301.12334](https://arxiv.org/abs/2301.12334)

    本研究提出了一个可以使扩散模型生成过程专注于少数样本的新颖框架。

    

    我们探讨使用扩散模型生成少数样本的问题。少数样本是位于数据流形低密度区域的实例。生成足够数量的这种少数样本很重要，因为它们通常包含数据的一些独特属性。然而，由于高似然性，扩散模型的传统生成过程主要产生大多数样本（位于流形高密度区域），使自身对少数生成任务无效且耗时。本研究提出了一个新颖的框架，可以使扩散模型的生成过程专注于少数样本。首先强调 Tweedie 的降噪公式对大多数样本产生有利结果。这一观察激励我们引入描述给定样本独特性的度量。为了解决扩散模型固有的偏好，我们...

    arXiv:2301.12334v2 Announce Type: replace-cross  Abstract: We explore the problem of generating minority samples using diffusion models. The minority samples are instances that lie on low-density regions of a data manifold. Generating a sufficient number of such minority instances is important, since they often contain some unique attributes of the data. However, the conventional generation process of the diffusion models mostly yields majority samples (that lie on high-density regions of the manifold) due to their high likelihoods, making themselves ineffective and time-consuming for the minority generating task. In this work, we present a novel framework that can make the generation process of the diffusion models focus on the minority samples. We first highlight that Tweedie's denoising formula yields favorable results for majority samples. The observation motivates us to introduce a metric that describes the uniqueness of a given sample. To address the inherent preference of the di
    
[^255]: CEDAS：一种具有改进收敛性的压缩分布式随机梯度法

    CEDAS: A Compressed Decentralized Stochastic Gradient Method with Improved Convergence

    [https://arxiv.org/abs/2301.05872](https://arxiv.org/abs/2301.05872)

    CEDAS提出了一种压缩分布式随机梯度方法，在无偏压缩运算符下具有与集中式随机梯度下降相当的收敛速度，实现了最短的瞬态时间，对光滑强凸和非凸目标函数都适用。

    

    在本文中，我们考虑在通信受限环境下解决多代理网络上的分布式优化问题。我们研究了一种称为“具有自适应步长的压缩精确扩散（CEDAS）”的压缩分布式随机梯度方法，并证明该方法在无偏压缩运算符下渐近地实现了与集中式随机梯度下降（SGD）相当的收敛速度，适用于光滑强凸目标函数和光滑非凸目标函数。特别地，据我们所知，CEDAS迄今为止以其最短的瞬态时间（关于图的特性）实现了与集中式SGD相同的收敛速度，其在光滑强凸目标函数下表现为$\mathcal{O}(n{C^3}/(1-\lambda_2)^{2})$，在光滑非凸目标函数下表现为$\mathcal{O}(n^3{C^6}/(1-\lambda_2)^4)$，其中$(1-\lambda_2)$表示谱...

    arXiv:2301.05872v2 Announce Type: replace-cross  Abstract: In this paper, we consider solving the distributed optimization problem over a multi-agent network under the communication restricted setting. We study a compressed decentralized stochastic gradient method, termed ``compressed exact diffusion with adaptive stepsizes (CEDAS)", and show the method asymptotically achieves comparable convergence rate as centralized { stochastic gradient descent (SGD)} for both smooth strongly convex objective functions and smooth nonconvex objective functions under unbiased compression operators. In particular, to our knowledge, CEDAS enjoys so far the shortest transient time (with respect to the graph specifics) for achieving the convergence rate of centralized SGD, which behaves as $\mathcal{O}(n{C^3}/(1-\lambda_2)^{2})$ under smooth strongly convex objective functions, and $\mathcal{O}(n^3{C^6}/(1-\lambda_2)^4)$ under smooth nonconvex objective functions, where $(1-\lambda_2)$ denotes the spectr
    
[^256]: 使用放射组学在CT图像上进行非侵入性肝纤维化筛查

    Non-invasive Liver Fibrosis Screening on CT Images using Radiomics

    [https://arxiv.org/abs/2211.14396](https://arxiv.org/abs/2211.14396)

    开发并评估了一种在CT肝脏上检测肝纤维化的放射组学机器学习模型，结果发现非增强CT在平均AUC值上优于增强CT。

    

    目的：开发并评估一种用于在CT肝脏上检测肝纤维化的放射组学机器学习模型。方法：针对这项回顾性的单中心研究，从接受同时肝活检和CT检查的患者的CT图像上提取放射组学特征。基于随机放置的ROI的平均测试下ROC曲线下面积（AUC），确定了对比度、归一化、机器学习模型和特征选择方法的组合。具有最高AUC的组合和选定的特征被用来开发最终的肝纤维化筛查模型。结果：研究包括101名男性和68名女性患者（平均年龄=51.2岁，±14.7[标准差]）。在所有组合的AUC平均值中，非增强CT（AUC，0.6100；95% CI：0.5897，0.6303）优于增强CT（AUC，0.5680；95% CI

    arXiv:2211.14396v2 Announce Type: replace-cross  Abstract: Objectives: To develop and evaluate a radiomics machine learning model for detecting liver fibrosis on CT of the liver.   Methods: For this retrospective, single-centre study, radiomic features were extracted from Regions of Interest (ROIs) on CT images of patients who underwent simultaneous liver biopsy and CT examinations. Combinations of contrast, normalization, machine learning model, and feature selection method were determined based on their mean test Area Under the Receiver Operating Characteristic curve (AUC) on randomly placed ROIs. The combination and selected features with the highest AUC were used to develop a final liver fibrosis screening model.   Results: The study included 101 male and 68 female patients (mean age = 51.2 years $\pm$ 14.7 [SD]). When averaging the AUC across all combinations, non-contrast enhanced (NC) CT (AUC, 0.6100; 95% CI: 0.5897, 0.6303) outperformed contrast-enhanced CT (AUC, 0.5680; 95% CI
    
[^257]: 球面卷积神经网络可以改善扩散MRI数据中对脑微结构的估计

    Spherical convolutional neural networks can improve brain microstructure estimation from diffusion MRI data

    [https://arxiv.org/abs/2211.09887](https://arxiv.org/abs/2211.09887)

    球面卷积神经网络在估计脑微结构参数方面具有比常规技术更高的准确性和更少的旋转方差。

    

    扩散磁共振成像对大脑组织的微结构特性有很高的敏感性。然而，从测量信号中估计临床和科学相关的微结构特性仍然是一个极具挑战性的逆问题，机器学习可能有助于解决。本研究调查了最近发展的旋转不变球面卷积神经网络是否可以改善微结构参数的估计。我们训练了一个球面卷积神经网络，以从高效模拟的带噪数据中预测地面实况参数数值，并将训练好的网络应用于在临床环境中获取的成像数据，生成微结构参数图。我们的网络表现优于球面平均技术和多层感知器，比球面平均技术具有更高的预测准确性，比多层感知器具有更少的旋转方差。

    arXiv:2211.09887v3 Announce Type: replace-cross  Abstract: Diffusion magnetic resonance imaging is sensitive to the microstructural properties of brain tissue. However, estimating clinically and scientifically relevant microstructural properties from the measured signals remains a highly challenging inverse problem that machine learning may help solve. This study investigated if recently developed rotationally invariant spherical convolutional neural networks can improve microstructural parameter estimation. We trained a spherical convolutional neural network to predict the ground-truth parameter values from efficiently simulated noisy data and applied the trained network to imaging data acquired in a clinical setting to generate microstructural parameter maps. Our network performed better than the spherical mean technique and multi-layer perceptron, achieving higher prediction accuracy than the spherical mean technique with less rotational variance than the multi-layer perceptron. Alt
    
[^258]: 利用文本数据进行破产预测的多模态生成模型

    Multimodal Generative Models for Bankruptcy Prediction Using Textual Data

    [https://arxiv.org/abs/2211.08405](https://arxiv.org/abs/2211.08405)

    该研究介绍了一种条件多模态判别（CMMD）模型，通过学习多模态表示来预测破产风险，弥补了传统破产模型中缺少MDA文本数据的限制。

    

    来自财务报告的文本数据，例如10-K表中的“管理讨论与分析”（MDA）部分，已被用于提高破产模型的预测准确性。然而，在实践中，我们无法为所有上市公司获取MDA部分，这限制了MDA数据在传统破产模型中的使用，因为它们需要完整数据来进行预测。缺少MDA数据的两个主要原因是：（i）并非所有公司都有义务提交MDA，（ii）当爬取和抓取MDA部分时会出现技术问题。为了解决这一限制，该研究引入了条件多模态判别（CMMD）模型，该模型学习多模态表示，嵌入了会计、市场和文本数据模态的信息。CMMD模型需要一组所有数据模态进行模型训练。在测试时，CMMD模型只需要访问会计和市场模态来生成

    arXiv:2211.08405v5 Announce Type: replace-cross  Abstract: Textual data from financial filings, e.g., the Management's Discussion & Analysis (MDA) section in Form 10-K, has been used to improve the prediction accuracy of bankruptcy models. In practice, however, we cannot obtain the MDA section for all public companies, which limits the use of MDA data in traditional bankruptcy models, as they need complete data to make predictions. The two main reasons for the lack of MDA are: (i) not all companies are obliged to submit the MDA and (ii) technical problems arise when crawling and scrapping the MDA section. To solve this limitation, this research introduces the Conditional Multimodal Discriminative (CMMD) model that learns multimodal representations that embed information from accounting, market, and textual data modalities. The CMMD model needs a sample with all data modalities for model training. At test time, the CMMD model only needs access to accounting and market modalities to gene
    
[^259]: 面向集体鲁棒性认证的本地化随机平滑

    Localized Randomized Smoothing for Collective Robustness Certification

    [https://arxiv.org/abs/2210.16140](https://arxiv.org/abs/2210.16140)

    提出了面向所有类型模型的集体鲁棒性证书，适用于软局部模型；证书基于新颖的局部随机平滑方法。

    

    图像分割、节点分类等任务的模型将单个输入映射到多个标签。通过扰动这个单一共享的输入（例如图像），对手可以操纵多个预测（例如错误分类多个像素）。集体鲁棒性认证是在这种威胁模型下，着眼于证明界定鲁棒预测数量的任务。我们提出了一种更通用的所有类型模型的集体鲁棒性证书。我们进一步展示了这种方法对于较大的一类软局部模型是有益的，其中每个输出依赖于整个输入，但对不同的输入区域赋予不同的重要性水平（例如基于它们在图像中的接近程度）。该证书基于我们的新颖的局部随机平滑方法。

    arXiv:2210.16140v3 Announce Type: replace  Abstract: Models for image segmentation, node classification and many other tasks map a single input to multiple labels. By perturbing this single shared input (e.g. the image) an adversary can manipulate several predictions (e.g. misclassify several pixels). Collective robustness certification is the task of provably bounding the number of robust predictions under this threat model. The only dedicated method that goes beyond certifying each output independently is limited to strictly local models, where each prediction is associated with a small receptive field. We propose a more general collective robustness certificate for all types of models. We further show that this approach is beneficial for the larger class of softly local models, where each output is dependent on the entire input but assigns different levels of importance to different input regions (e.g. based on their proximity in the image). The certificate is based on our novel loc
    
[^260]: DynaConF：非平稳时间序列的动态预测

    DynaConF: Dynamic Forecasting of Non-Stationary Time Series

    [https://arxiv.org/abs/2209.08411](https://arxiv.org/abs/2209.08411)

    本研究提出了一种新方法，通过将平稳条件分布建模与非平稳动态建模解耦，有效地建模时间上的非平稳条件分布，能更好地适应非平稳时间序列。

    

    深度学习在各种时间序列预测任务中展现出令人印象深刻的结果，其中建模未来给定过去的条件分布是其核心。然而，当这种条件分布是非平稳的时，对于这些模型来说，要一致学习和准确预测会带来挑战。在本研究中，我们提出了一种新的方法，通过明确地将平稳条件分布建模与非平稳动态建模解耦，来对时间上的非平稳条件分布进行建模。我们的方法基于贝叶斯动态模型，可以适应条件分布变化，以及使用分解的输出空间处理多变量时间序列的深度条件分布模型。我们在合成和真实数据集上的实验结果表明，我们的模型可以比最先进的深度学习解决方案更好地适应非平稳时间序列。

    arXiv:2209.08411v3 Announce Type: replace  Abstract: Deep learning has shown impressive results in a variety of time series forecasting tasks, where modeling the conditional distribution of the future given the past is the essence. However, when this conditional distribution is non-stationary, it poses challenges for these models to learn consistently and to predict accurately. In this work, we propose a new method to model non-stationary conditional distributions over time by clearly decoupling stationary conditional distribution modeling from non-stationary dynamics modeling. Our method is based on a Bayesian dynamic model that can adapt to conditional distribution changes and a deep conditional distribution model that handles multivariate time series using a factorized output space. Our experimental results on synthetic and real-world datasets show that our model can adapt to non-stationary time series better than state-of-the-art deep learning solutions.
    
[^261]: 基于模型的离线零和马尔可夫博弈强化学习

    Model-Based Reinforcement Learning for Offline Zero-Sum Markov Games

    [https://arxiv.org/abs/2206.04044](https://arxiv.org/abs/2206.04044)

    本文提出了一种基于模型的悲观算法 VI-LCB-Game，在离线数据中找到了两人零和马尔可夫博弈的纳什均衡，加强了先前研究。

    

    本文在从离线数据中学习两人零和马尔可夫博弈的纳什均衡方面取得进展。我们提出了一种基于模型的悲观算法，即具有Bernstein风格的下限置信界的VI-LCB-Game算法，可以证明以样本复杂度不大于$\frac{C_{\mathsf{clipped}}^{\star}S(A+B)}{(1-\gamma)^{3}\varepsilon^{2}}$（带有一些对数因子）找到一个$\varepsilon$-近似的纳什均衡。在这里，$C_{\mathsf{clipped}}^{\star}$ 是反映可用数据（关于目标数据）的覆盖率和分布转变的某种单侧剪切的集中度系数，目标精度$\varepsilon$ 可以是$\big(0,\frac{1}{1-\gamma}\big]$范围内的任何值。我们的样本复杂度界限加强了先前的研究。

    arXiv:2206.04044v2 Announce Type: replace  Abstract: This paper makes progress towards learning Nash equilibria in two-player zero-sum Markov games from offline data. Specifically, consider a $\gamma$-discounted infinite-horizon Markov game with $S$ states, where the max-player has $A$ actions and the min-player has $B$ actions. We propose a pessimistic model-based algorithm with Bernstein-style lower confidence bounds -- called VI-LCB-Game -- that provably finds an $\varepsilon$-approximate Nash equilibrium with a sample complexity no larger than $\frac{C_{\mathsf{clipped}}^{\star}S(A+B)}{(1-\gamma)^{3}\varepsilon^{2}}$ (up to some log factor). Here, $C_{\mathsf{clipped}}^{\star}$ is some unilateral clipped concentrability coefficient that reflects the coverage and distribution shift of the available data (vis-\`a-vis the target data), and the target accuracy $\varepsilon$ can be any value within $\big(0,\frac{1}{1-\gamma}\big]$. Our sample complexity bound strengthens prior art by a 
    
[^262]: 从内在动机到占据行动-状态路径空间的复杂行为

    Complex behavior from intrinsic motivation to occupy action-state path space

    [https://arxiv.org/abs/2205.10316](https://arxiv.org/abs/2205.10316)

    行为的目标是最大化未来行动和状态路径的占用，根据最大占用原则，奖励是占用路径空间的手段，而不是目标本身，并提供了与最优策略和状态值函数相关的解析表达式，证明了值迭代算法的收敛性

    

    大多数行为理论认为，代理人倾向于最大化某种形式的奖励或效用。然而，动物经常出于好奇心移动，并且似乎在没有奖励的情况下受到激励。在这里，我们放弃了奖励最大化的概念，提出行为的目标是最大化未来行动和状态路径的占用。根据最大占用原则，奖励是占用路径空间的手段，而不是目标本身；目标导向性简单地作为搜索资源的理性方式而出现，以使运动从广义上理解永不结束。我们发现，行动-状态路径熵是唯一与可加性和其他直观的预期未来行动-状态路径占用性质一致的度量。我们提供了关于最优策略和状态值函数的解析表达式，并证明了我们的值迭代算法的收敛性。使用离散和连续状态任务，包括一个高

    arXiv:2205.10316v2 Announce Type: replace  Abstract: Most theories of behavior posit that agents tend to maximize some form of reward or utility. However, animals very often move with curiosity and seem to be motivated in a reward-free manner. Here we abandon the idea of reward maximization, and propose that the goal of behavior is maximizing occupancy of future paths of actions and states. According to this maximum occupancy principle, rewards are the means to occupy path space, not the goal per se; goal-directedness simply emerges as rational ways of searching for resources so that movement, understood amply, never ends. We find that action-state path entropy is the only measure consistent with additivity and other intuitive properties of expected future action-state path occupancy. We provide analytical expressions that relate the optimal policy and state-value function, and prove convergence of our value iteration algorithm. Using discrete and continuous state tasks, including a hi
    
[^263]: 用深度神经网络检测数据驱动的稳健统计套利策略

    Detecting data-driven robust statistical arbitrage strategies with deep neural networks

    [https://arxiv.org/abs/2203.03179](https://arxiv.org/abs/2203.03179)

    这项研究提出了一种基于深度神经网络的方法，能够识别金融市场中的稳健统计套利策略，无需依赖资产协整对的识别，可在高维金融市场中实现盈利交易。

    

    我们提出了一种基于深度神经网络的方法，可以在金融市场中识别出稳健的统计套利策略。稳健的统计套利策略指的是在模型不确定性下实现盈利交易的交易策略。这一创新方法允许同时考虑大量基础证券，并不依赖于资产协整对的识别，因此适用于高维金融市场或在传统对冲交易方法失败的市场。此外，我们提供了一种从观察市场数据中得到的可容许概率测度的模糊集构建方法。因此，该方法可被认为是无模型且完全数据驱动的。我们通过提供具有高盈利交易表现的实证研究展示了我们方法的适用性，甚至在50维度的情况下也表现出色。

    arXiv:2203.03179v4 Announce Type: replace-cross  Abstract: We present an approach, based on deep neural networks, that allows identifying robust statistical arbitrage strategies in financial markets. Robust statistical arbitrage strategies refer to trading strategies that enable profitable trading under model ambiguity. The presented novel methodology allows to consider a large amount of underlying securities simultaneously and does not depend on the identification of cointegrated pairs of assets, hence it is applicable on high-dimensional financial markets or in markets where classical pairs trading approaches fail. Moreover, we provide a method to build an ambiguity set of admissible probability measures that can be derived from observed market data. Thus, the approach can be considered as being model-free and entirely data-driven. We showcase the applicability of our method by providing empirical investigations with highly profitable trading performances even in 50 dimensions, durin
    
[^264]: 在未知环境中学习动态机制：一种强化学习方法

    Learning Dynamic Mechanisms in Unknown Environments: A Reinforcement Learning Approach

    [https://arxiv.org/abs/2202.12797](https://arxiv.org/abs/2202.12797)

    通过将无奖励在线强化学习引入到在线机制设计问题中，我们提出了能够在未知环境中学习动态VCG机制且具有上界为$\tilde{\mathcal{O}}(T^{2/3})$的遗憾保证的新颖学习算法。

    

    动态机制设计研究了机制设计者在时变环境中应该如何在代理之间分配资源。我们考虑了一种问题，即代理根据未知的马尔可夫决策过程(MDP)与机制设计者互动，在这个过程中代理的奖励和机制设计者的状态根据一个带有未知奖励函数和转移核的情节MDP演化。我们关注在线设置下的线性函数近似，并提出了新颖的学习算法，在多轮互动中恢复动态Vickrey-Clarke-Grove(VCG)机制。我们方法的一个关键贡献是将无奖励在线强化学习(RL)结合进来，以帮助在丰富的策略空间中进行探索，从而估计动态VCG机制中的价格。我们展示了我们提出的方法的遗憾上界为$\tilde{\mathcal{O}}(T^{2/3})$，并进一步设计了一个下界，以展示我们方法的...

    arXiv:2202.12797v2 Announce Type: replace  Abstract: Dynamic mechanism design studies how mechanism designers should allocate resources among agents in a time-varying environment. We consider the problem where the agents interact with the mechanism designer according to an unknown Markov Decision Process (MDP), where agent rewards and the mechanism designer's state evolve according to an episodic MDP with unknown reward functions and transition kernels. We focus on the online setting with linear function approximation and propose novel learning algorithms to recover the dynamic Vickrey-Clarke-Grove (VCG) mechanism over multiple rounds of interaction. A key contribution of our approach is incorporating reward-free online Reinforcement Learning (RL) to aid exploration over a rich policy space to estimate prices in the dynamic VCG mechanism. We show that the regret of our proposed method is upper bounded by $\tilde{\mathcal{O}}(T^{2/3})$ and further devise a lower bound to show that our a
    
[^265]: 具有异质性的图神经网络：一项调查

    Graph Neural Networks for Graphs with Heterophily: A Survey

    [https://arxiv.org/abs/2202.07082](https://arxiv.org/abs/2202.07082)

    该论文提出了对具有异质性的图进行图神经网络研究的系统回顾，并提出了系统性分类法以指导现有异质性GNN模型。

    

    近年来，图神经网络（GNNs）的快速发展使得许多图分析任务和应用受益。大多数GNNs通常依赖于同质性假设，即属于同一类别的节点更可能相连。然而，在许多真实场景中，作为一种普遍的图属性，即不同标签的节点往往相连，这显著限制了定制的同质性GNNs的性能。因此，针对异质性图的GNNs正受到越来越多的研究关注，以增强对具有异质性的图学习。本文针对具有异质性的图提供了全面的GNNs回顾。具体来说，我们提出了一个系统性分类法，本质上指导着现有的异质性GNN模型，以及一个概括性摘要和详细分析。

    arXiv:2202.07082v2 Announce Type: replace  Abstract: Recent years have witnessed fast developments of graph neural networks (GNNs) that have benefited myriads of graph analytic tasks and applications. In general, most GNNs depend on the homophily assumption that nodes belonging to the same class are more likely to be connected. However, as a ubiquitous graph property in numerous real-world scenarios, heterophily, i.e., nodes with different labels tend to be linked, significantly limits the performance of tailor-made homophilic GNNs. Hence, GNNs for heterophilic graphs are gaining increasing research attention to enhance graph learning with heterophily. In this paper, we provide a comprehensive review of GNNs for heterophilic graphs. Specifically, we propose a systematic taxonomy that essentially governs existing heterophilic GNN models, along with a general summary and detailed analysis. %Furthermore, we summarize the mainstream heterophilic graph benchmarks to facilitate robust and fa
    
[^266]: Klarna产品页面数据集：利用图神经网络和大型语言模型进行网络元素提名

    The Klarna Product Page Dataset: Web Element Nomination with Graph Neural Networks and Large Language Models

    [https://arxiv.org/abs/2111.02168](https://arxiv.org/abs/2111.02168)

    Klarna产品页面数据集是一个全面多样的网页集合，为解决网络自动化算法设计中数据集稀缺性问题提供了新的资源。

    

    Web自动化有可能彻底改变用户与数字世界的互动方式，通过复杂的计算方法提供无与伦比的帮助，简化任务。在这一演进过程中，网络元素提名任务至关重要，它涉及识别网页上的独特元素。不幸的是，网络自动化算法设计的发展受到全面和真实反映网络应用程序复杂性的数据集的稀缺性的阻碍。为了解决这一问题，我们推出了Klarna产品页面数据集，这是一个全面多样的网页集合，超越了现有数据集的丰富性和多样性。该数据集包含来自8,175个电子商务网站的51,701个手动标记的产品页面，覆盖了八个地理区域，并附带了一组渲染页面截图数据集。为了开始研究Klarna产品页面数据集，我们进行了实证研究

    arXiv:2111.02168v4 Announce Type: replace-cross  Abstract: Web automation holds the potential to revolutionize how users interact with the digital world, offering unparalleled assistance and simplifying tasks via sophisticated computational methods. Central to this evolution is the web element nomination task, which entails identifying unique elements on webpages. Unfortunately, the development of algorithmic designs for web automation is hampered by the scarcity of comprehensive and realistic datasets that reflect the complexity faced by real-world applications on the Web. To address this, we introduce the Klarna Product Page Dataset, a comprehensive and diverse collection of webpages that surpasses existing datasets in richness and variety. The dataset features 51,701 manually labeled product pages from 8,175 e-commerce websites across eight geographic regions, accompanied by a dataset of rendered page screenshots. To initiate research on the Klarna Product Page Dataset, we empirical
    
[^267]: 在最佳臂识别中上下文信息的作用

    The Role of Contextual Information in Best Arm Identification

    [https://arxiv.org/abs/2106.14077](https://arxiv.org/abs/2106.14077)

    通过在固定置信度下利用上下文信息，在识别最佳臂时提出了一种上下文感知的“跟踪停止”策略，实现了比之前方法更高效的效果。

    

    我们研究了在随机赌博机中当有上下文（协变量）信息可用时的固定置信度下的最佳臂识别问题。虽然我们可以在每一轮中使用上下文信息，但我们对上下文分布的边际化均值重视。我们的目标是在给定错误率值的情况下以最小数量的抽样识别最佳臂。我们为该问题展示了特定实例的样本复杂性下界。然后，我们提出了一个“跟踪停止”策略的上下文感知版本，其中臂抽取的比例跟踪最优分配集，并证明了预期的臂抽取次数渐近地与下界匹配。我们证明了相对于Garivier & Kaufmann（2016）的结果，上下文信息可以用来改善对最佳边际化均值奖励的识别效率。我们实验证实了 cont

    arXiv:2106.14077v3 Announce Type: replace  Abstract: We study the best-arm identification problem with fixed confidence when contextual (covariate) information is available in stochastic bandits. Although we can use contextual information in each round, we are interested in the marginalized mean reward over the contextual distribution. Our goal is to identify the best arm with a minimal number of samplings under a given value of the error rate. We show the instance-specific sample complexity lower bounds for the problem. Then, we propose a context-aware version of the "Track-and-Stop" strategy, wherein the proportion of the arm draws tracks the set of optimal allocations and prove that the expected number of arm draws matches the lower bound asymptotically. We demonstrate that contextual information can be used to improve the efficiency of the identification of the best marginalized mean reward compared with the results of Garivier & Kaufmann (2016). We experimentally confirm that cont
    
[^268]: 利用凸规划的最大线性回归

    Max-Linear Regression by Convex Programming

    [https://arxiv.org/abs/2103.07020](https://arxiv.org/abs/2103.07020)

    本文提出并分析了一种基于锚定回归（AR）的可扩展凸规划方案，用于解决最大线性回归问题。

    

    我们考虑多元最大线性回归问题，其中需要从$n$个独立样本中估计模型参数$\boldsymbol{\beta}_{1},\dotsc,\boldsymbol{\beta}_{k}\in\mathbb{R}^{p}$，这些样本是（噪声的）观测$y = \max_{1\leq j \leq k} \boldsymbol{\beta}_{j}^{\mathsf{T}} \boldsymbol{x} + \mathrm{noise}$。最大线性模型广泛地推广了传统的线性模型，当线性模型的数量$k$足够大时，它可以以任意精度逼近任何凸函数。然而，最大线性模型固有的非线性使得回归参数的估计在计算上具有挑战性。特别地，文献中没有基于凸规划的估计方法。我们提出并分析了一种可扩展的凸规划程序，即锚定回归（AR），作为最大线性回归问题的估计器。

    arXiv:2103.07020v2 Announce Type: replace-cross  Abstract: We consider the multivariate max-linear regression problem where the model parameters $\boldsymbol{\beta}_{1},\dotsc,\boldsymbol{\beta}_{k}\in\mathbb{R}^{p}$ need to be estimated from $n$ independent samples of the (noisy) observations $y = \max_{1\leq j \leq k} \boldsymbol{\beta}_{j}^{\mathsf{T}} \boldsymbol{x} + \mathrm{noise}$. The max-linear model vastly generalizes the conventional linear model, and it can approximate any convex function to an arbitrary accuracy when the number of linear models $k$ is large enough. However, the inherent nonlinearity of the max-linear model renders the estimation of the regression parameters computationally challenging. Particularly, no estimator based on convex programming is known in the literature. We formulate and analyze a scalable convex program given by anchored regression (AR) as the estimator for the max-linear regression problem. Under the standard Gaussian observation setting, we
    
[^269]: 混合策略梯度：数据与模型共同驱动的脱机强化学习

    Mixed Policy Gradient: off-policy reinforcement learning driven jointly by data and model

    [https://arxiv.org/abs/2102.11513](https://arxiv.org/abs/2102.11513)

    该论文提出了混合策略梯度（MPG）算法，通过融合数据驱动和模型驱动的策略梯度，加速强化学习的收敛速度而不降低性能。

    

    强化学习在顺序决策中显示出巨大潜力。目前，主流的强化学习算法是数据驱动的，通常在渐近性能方面表现更好，但与模型驱动方法相比收敛速度要慢得多。本文提出了混合策略梯度（MPG）算法，该算法将经验数据和策略梯度（PG）中的转移模型融合在一起，以加速收敛而不会降低性能。形式上，MPG被构建为数据驱动和模型驱动PG的加权平均，其中前者是学习的Q值函数的导数，而后者是模型预测回报的导数。为指导权重设计，我们分析并比较每个PG误差的上限。依靠这一点，采用了基于规则的方法来启发性地调整权重。特别地，要获得更好的PG，数据驱动PG的权重被设计成沿着学习过程增长。

    arXiv:2102.11513v2 Announce Type: replace  Abstract: Reinforcement learning (RL) shows great potential in sequential decision-making. At present, mainstream RL algorithms are data-driven, which usually yield better asymptotic performance but much slower convergence compared with model-driven methods. This paper proposes mixed policy gradient (MPG) algorithm, which fuses the empirical data and the transition model in policy gradient (PG) to accelerate convergence without performance degradation. Formally, MPG is constructed as a weighted average of the data-driven and model-driven PGs, where the former is the derivative of the learned Q-value function, and the latter is that of the model-predictive return. To guide the weight design, we analyze and compare the upper bound of each PG error. Relying on that, a rule-based method is employed to heuristically adjust the weights. In particular, to get a better PG, the weight of the data-driven PG is designed to grow along the learning process
    
[^270]: 具有干净标注的MSR-Video to Text数据集

    The MSR-Video to Text Dataset with Clean Annotations

    [https://arxiv.org/abs/2102.06448](https://arxiv.org/abs/2102.06448)

    清洗了MSR-VTT数据集中的标注，提高了视频字幕模型的性能。

    

    视频字幕自动生成视频内容的简短描述，通常是一句话的形式。已经提出了许多解决这一任务的方法。一个名为MSR Video to Text（MSR-VTT）的大型数据集经常被用作测试这些方法性能的基准数据集。然而，我们发现数据集中的人工标注，即视频内容描述，存在相当多的噪音，例如许多重复字幕和许多字幕包含语法问题。这些问题可能会给视频字幕模型学习底层模式带来困难。我们通过消除这些问题来清理MSR-VTT的标注，然后在清理后的数据集上测试了几种典型的视频字幕模型。实验结果显示，数据清洗提升了模型的性能，以流行的定量指标衡量。我们招募了受试者来评估模型的结果。

    arXiv:2102.06448v4 Announce Type: replace-cross  Abstract: Video captioning automatically generates short descriptions of the video content, usually in form of a single sentence. Many methods have been proposed for solving this task. A large dataset called MSR Video to Text (MSR-VTT) is often used as the benchmark dataset for testing the performance of the methods. However, we found that the human annotations, i.e., the descriptions of video contents in the dataset are quite noisy, e.g., there are many duplicate captions and many captions contain grammatical problems. These problems may pose difficulties to video captioning models for learning underlying patterns. We cleaned the MSR-VTT annotations by removing these problems, then tested several typical video captioning models on the cleaned dataset. Experimental results showed that data cleaning boosted the performances of the models measured by popular quantitative metrics. We recruited subjects to evaluate the results of a model tra
    
[^271]: 使用稀疏对偶算法扩展凸障碍

    Scaling the Convex Barrier with Sparse Dual Algorithms

    [https://arxiv.org/abs/2101.05844](https://arxiv.org/abs/2101.05844)

    提出了两种新颖的对偶算法，通过操作小的对偶变量活跃集上的次梯度方法和利用Frank-Wolfe类型优化器的稀疏性质，来解决神经网络边界验证系统中常见的松弛性问题。

    

    紧凑而高效的神经网络边界对神经网络验证系统的扩展至关重要。最近提出了许多高效的边界算法，但通常太松以验证更具挑战性的属性。这是因为所使用的松弛性质较弱，通常是与神经元数量成线性关系的线性规划。虽然存在一种更紧致的分段线性激活松弛，但代价是指数级的约束，当前缺乏高效的定制求解器。我们通过提出两种新颖的对偶算法来解决这一不足：一种在一个小的对偶变量活跃集上运行次梯度方法，另一种利用Frank-Wolfe类型优化器的稀疏性质仅产生线性内存开销。这两种方法恢复了新松弛的优势：紧凑性和线性分离预言。同时，它们也分享了pr的好处

    arXiv:2101.05844v3 Announce Type: replace  Abstract: Tight and efficient neural network bounding is crucial to the scaling of neural network verification systems. Many efficient bounding algorithms have been presented recently, but they are often too loose to verify more challenging properties. This is due to the weakness of the employed relaxation, which is usually a linear program of size linear in the number of neurons. While a tighter linear relaxation for piecewise-linear activations exists, it comes at the cost of exponentially many constraints and currently lacks an efficient customized solver. We alleviate this deficiency by presenting two novel dual algorithms: one operates a subgradient method on a small active set of dual variables, the other exploits the sparsity of Frank-Wolfe type optimizers to incur only a linear memory cost. Both methods recover the strengths of the new relaxation: tightness and a linear separation oracle. At the same time, they share the benefits of pr
    
[^272]: 关于单调三角形输运映射的表示和学习

    On the representation and learning of monotone triangular transport maps

    [https://arxiv.org/abs/2009.10303](https://arxiv.org/abs/2009.10303)

    提出了通过光滑函数的可逆变换表示单调三角形映射的通用框架，使得相关的无穷维最小化问题具有全局最小值。

    

    测度的输运提供了对建模复杂概率分布的多功能方法，在密度估计、贝叶斯推断、生成建模等方面有应用。单调三角形输运映射——Knothe-Rosenblatt (KR)排列的近似，在这些任务中是一个经典选择。然而，这些映射的表示和参数化对其通用性、表达能力以及从数据中学习映射所引起的优化问题的性质（例如通过最大似然估计）有重要影响。我们提出了一个表示单调三角形映射的通用框架，通过光滑函数的可逆变换。我们建立了变换的条件，使得相关的无穷维最小化问题没有虚假局部极小值，即所有局部极小值都是全局极小值。

    arXiv:2009.10303v3 Announce Type: replace-cross  Abstract: Transportation of measure provides a versatile approach for modeling complex probability distributions, with applications in density estimation, Bayesian inference, generative modeling, and beyond. Monotone triangular transport maps$\unicode{x2014}$approximations of the Knothe$\unicode{x2013}$Rosenblatt (KR) rearrangement$\unicode{x2014}$are a canonical choice for these tasks. Yet the representation and parameterization of such maps have a significant impact on their generality and expressiveness, and on properties of the optimization problem that arises in learning a map from data (e.g., via maximum likelihood estimation). We present a general framework for representing monotone triangular maps via invertible transformations of smooth functions. We establish conditions on the transformation such that the associated infinite-dimensional minimization problem has no spurious local minima, i.e., all local minima are global minima;
    
[^273]: 将Tanimoto类型核泛化到实值函数

    On the generalization of Tanimoto-type kernels to real valued functions

    [https://arxiv.org/abs/2007.05943](https://arxiv.org/abs/2007.05943)

    本论文介绍了一种更一般的Tanimoto核公式，允许衡量任意实值函数的相似性，并提供了一种光滑逼近的方法。

    

    Tanimoto核（Jaccard指数）是描述二值属性集相似性的知名工具。已将其扩展到属性为非负实数值的情况。本文介绍了一种更一般的Tanimoto核公式，允许衡量任意实值函数的相似性。通过通过适当选择的集合统一属性表示来构建此扩展。在推导核的一般形式后，从核函数中提取了显式特征表示，并展示了将一般核包含到Tanimoto核中的简单方法。最后，核也表示为分段线性函数的商，并提供了光滑逼近。

    arXiv:2007.05943v2 Announce Type: replace  Abstract: The Tanimoto kernel (Jaccard index) is a well known tool to describe the similarity between sets of binary attributes. It has been extended to the case when the attributes are nonnegative real values. This paper introduces a more general Tanimoto kernel formulation which allows to measure the similarity of arbitrary real-valued functions. This extension is constructed by unifying the representation of the attributes via properly chosen sets. After deriving the general form of the kernel, explicit feature representation is extracted from the kernel function, and a simply way of including general kernels into the Tanimoto kernel is shown. Finally, the kernel is also expressed as a quotient of piecewise linear functions, and a smooth approximation is provided.
    
[^274]: 利用新型电肌肉刺激（EMS）鱼饵和集成网络算法进行可持续休闲钓鱼以最大限度提高捕获和释放的可生存性

    Sustainable Recreational Fishing Using a Novel Electrical Muscle Stimulation (EMS) Lure and Ensemble Network Algorithm to Maximize Catch and Release Survivability

    [https://arxiv.org/abs/2006.10125](https://arxiv.org/abs/2006.10125)

    该论文提出了一种新型电肌肉刺激钓鱼饵和集成网络算法，以解决垂钓损害和释放死亡率较高的问题，对于可持续开展休闲钓鱼具有重要意义。

    

    有20亿至7亿垂钓者，垂钓比商业拖网渔业普遍多出近五倍。全球范围内，数十万个工作与垂钓产业相关，为水边社区和渔业保护区带来数十亿美元的收入。然而，垂钓活动的广泛流行对水生生物多样性构成难以规范的威胁。例如，最多25%的过度捕捞种群可以追溯到垂钓者。这一令人担忧的统计数据可通过43%的平均捕捞和释放死亡率来解释，主要是由于钩相关的伤害和手法不当地处理带来的。本文提出的临时专利设计分别解决了这两个问题，首先，提出了一种新型的基于电肌肉刺激的钓鱼饵，作为尖钩的无害和低成本替代方案。早期原型显示了一种持恒的电。。。

    arXiv:2006.10125v3 Announce Type: replace-cross  Abstract: With 200-700 million anglers in the world, sportfishing is nearly five times more common than commercial trawling. Worldwide, hundreds of thousands of jobs are linked to the sportfishing industry, which generates billions of dollars for water-side communities and fisheries conservatories alike. However, the sheer popularity of recreational fishing poses threats to aquatic biodiversity that are hard to regulate. For example, as much as 25% of overfished populations can be traced to anglers. This alarming statistic is explained by the average catch and release mortality rate of 43%, which primarily results from hook-related injuries and careless out-of-water handling. The provisional-patented design proposed in this paper addresses both these problems separately First, a novel, electrical muscle stimulation based fishing lure is proposed as a harmless and low cost alternative to sharp hooks. Early prototypes show a constant elect
    
[^275]: 稀疏正交变分推断用于高斯过程

    Sparse Orthogonal Variational Inference for Gaussian Processes

    [https://arxiv.org/abs/1910.10596](https://arxiv.org/abs/1910.10596)

    介绍了一种使用感应点进行稀疏正交变分推断的新方法，可以得到更具可扩展性的算法，实现了更紧的边缘似然下界和新的随机变分推断算法

    

    我们介绍了一种新的稀疏变分逼近高斯过程的解释，使用感应点，这可以导致比先前方法更具可扩展性的算法。它基于将高斯过程分解为两个独立过程之和：一个由有限基感应点展开，另一个捕获剩余变化。我们展示了这种形式可恢复现有逼近，并同时允许获得更紧的边缘似然下界和新的随机变分推断算法。我们展示了这些算法在几种高斯过程模型中的效率，从标准回归到多类分类，使用(深度)卷积高斯过程，并在CIFAR-10上报告了纯GP模型中的最新结果。

    arXiv:1910.10596v5 Announce Type: replace-cross  Abstract: We introduce a new interpretation of sparse variational approximations for Gaussian processes using inducing points, which can lead to more scalable algorithms than previous methods. It is based on decomposing a Gaussian process as a sum of two independent processes: one spanned by a finite basis of inducing points and the other capturing the remaining variation. We show that this formulation recovers existing approximations and at the same time allows to obtain tighter lower bounds on the marginal likelihood and new stochastic variational inference algorithms. We demonstrate the efficiency of these algorithms in several Gaussian process models ranging from standard regression to multi-class classification using (deep) convolutional Gaussian processes and report state-of-the-art results on CIFAR-10 among purely GP-based models.
    
[^276]: 实时竞价广告中的在线因果推断

    Online Causal Inference for Advertising in Real-Time Bidding Auctions

    [https://arxiv.org/abs/1908.08600](https://arxiv.org/abs/1908.08600)

    该论文提出了一种新的在线因果推断方法，利用一价和二价拍卖的经济结构，通过引入改进的汤普森抽样算法来有效识别实时竞价广告的效果，最小化实验成本，并获得了顺序最优的遗憾上界。

    

    实时竞价（RTB）系统通过拍卖将用户曝光分配给竞争对手的广告商，继续在数字广告领域取得成功。评估这种广告的有效性在研究和实践中仍然是一个挑战。本文提出了一种新的方法来对通过这种机制购买的广告进行因果推断。利用一价和二价拍卖的经济结构，我们首先展示了广告效果由最佳出价确定。因此，由于这些最佳出价是唯一需要恢复的对象，我们引入了一种改进的汤普森抽样（TS）算法，来解决一个多臂老虎机问题，成功恢复这些出价和因此广告效果，同时最小化实验成本。我们推导了算法的遗憾上界，该上界是顺序最优的，并利用RTB拍卖数据展示了它的性能表现。

    arXiv:1908.08600v4 Announce Type: replace  Abstract: Real-time bidding (RTB) systems, which utilize auctions to allocate user impressions to competing advertisers, continue to enjoy success in digital advertising. Assessing the effectiveness of such advertising remains a challenge in research and practice. This paper proposes a new approach to perform causal inference on advertising bought through such mechanisms. Leveraging the economic structure of first- and second-price auctions, we first show that the effects of advertising are identified by the optimal bids. Hence, since these optimal bids are the only objects that need to be recovered, we introduce an adapted Thompson sampling (TS) algorithm to solve a multi-armed bandit problem that succeeds in recovering such bids and, consequently, the effects of advertising while minimizing the costs of experimentation. We derive a regret bound for our algorithm which is order optimal and use data from RTB auctions to show that it outperform
    
[^277]: HiFT:一种分层全参数微调策略

    HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy. (arXiv:2401.15207v1 [cs.LG])

    [http://arxiv.org/abs/2401.15207](http://arxiv.org/abs/2401.15207)

    HiFT是一种分层全参数微调策略，通过仅在每个训练步骤中更新参数的子集，可以显著减少GPU内存使用，并实现与参数高效微调和标准全参数微调相当的性能。

    

    随着语言模型的增长，在下游任务中微调语言模型的全参数需要占用大量GPU内存。现有方法利用零阶优化器以节省GPU内存，但这可能会影响语言模型的性能，因为非零阶优化器在大多数下游任务上更容易收敛。本文提出了一种新颖的独立于优化器的端到端分层微调策略HiFT，它仅在每个训练步骤中更新参数的子集。 HiFT可以显著减少存储在GPU内存中的梯度和优化器状态参数的量，从而减少GPU内存的使用。我们的结果表明：（1）HiFT实现了与参数高效微调和标准全参数微调相当的性能。（2）HiFT支持包括在内的各种优化器

    Full-parameter fine-tuning has become the go-to choice for adapting language models (LMs) to downstream tasks due to its excellent performance. As LMs grow in size, fine-tuning the full parameters of LMs requires a prohibitively large amount of GPU memory. Existing approaches utilize zeroth-order optimizer to conserve GPU memory, which can potentially compromise the performance of LMs as non-zero order optimizers tend to converge more readily on most downstream tasks. In this paper, we propose a novel optimizer-independent end-to-end hierarchical fine-tuning strategy, HiFT, which only updates a subset of parameters at each training step. HiFT can significantly reduce the amount of gradients and optimizer state parameters residing in GPU memory at the same time, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFT achieves comparable performance to parameter-efficient fine-tuning and standard full parameter fine-tuning. (2) HiFT supports various optimizers including
    
[^278]: 利用Ricci流引导的自编码器学习时变动力学

    Ricci flow-guided autoencoders in learning time-dependent dynamics. (arXiv:2401.14591v1 [cs.LG])

    [http://arxiv.org/abs/2401.14591](http://arxiv.org/abs/2401.14591)

    利用Ricci流引导的自编码器方法能够学习非线性动力学，尤其是偏微分方程。该方法通过在训练中学习流形，并使用Ricci流使流形潜空间逐步适应动力学的变化，从而获得更好的表示能力。在实验中，我们展示了该方法在具有周期性和随机性的PDE上的应用，并评估了在分布内和外推场景中的误差。

    

    我们提出了一种基于流形的自编码器方法，用于学习时间上的非线性动力学，尤其是偏微分方程（PDE），其中流形潜空间根据Ricci流发展。这可以通过在物理信息设置中模拟Ricci流来实现，并且可以匹配流形量，以便实现Ricci流。使用我们的方法，流形是作为训练过程的一部分学习的，因此可以识别出理想的几何形状，同时演变也能在静态方法上引起更宽容的潜在表示。我们在一系列数值实验中展示了我们的方法，包括具有周期性和随机性等理想特征的PDE，并在分布内和外推场景中进行误差评估。

    We present a manifold-based autoencoder method for learning nonlinear dynamics in time, notably partial differential equations (PDEs), in which the manifold latent space evolves according to Ricci flow. This can be accomplished by simulating Ricci flow in a physics-informed setting, and manifold quantities can be matched so that Ricci flow is empirically achieved. With our methodology, the manifold is learned as part of the training procedure, so ideal geometries may be discerned, while the evolution simultaneously induces a more accommodating latent representation over static methods. We present our method on a range of numerical experiments consisting of PDEs that encompass desirable characteristics such as periodicity and randomness, remarking error on in-distribution and extrapolation scenarios.
    
[^279]: 通过自适应权重聚类和服务器端蒸馏实现高效通信的联邦学习

    Communication-Efficient Federated Learning through Adaptive Weight Clustering and Server-Side Distillation. (arXiv:2401.14211v1 [cs.LG])

    [http://arxiv.org/abs/2401.14211](http://arxiv.org/abs/2401.14211)

    本论文提出了一种名为FedCompress的新方法，通过动态权重聚类和服务器端知识蒸馏的结合，实现了高效通信的联邦学习。该方法在降低通信成本的同时，能够学习到高度可泛化的模型。

    

    联邦学习是一种有望在保护数据隐私的同时，通过多个设备共同训练深度神经网络的技术。然而，由于训练过程中重复的服务器-客户端通信导致了过多的通信成本，这给联邦学习带来了困难。为了解决这个挑战，我们应用了模型压缩技术，例如稀疏化和权重聚类，然而这些技术通常需要修改底层的模型聚合方案或者涉及繁琐的超参数调整，后者不仅调整了模型的压缩率，还限制了模型在不断增长的数据上的持续改进潜力。在本文中，我们提出了一种新颖的方法FedCompress，它结合了动态权重聚类和服务器端知识蒸馏，以降低通信成本同时学习高度可泛化的模型。通过对多个公共数据集进行全面评估，我们证明了我们的方法相比于其他方法的有效性。

    Federated Learning (FL) is a promising technique for the collaborative training of deep neural networks across multiple devices while preserving data privacy. Despite its potential benefits, FL is hindered by excessive communication costs due to repeated server-client communication during training. To address this challenge, model compression techniques, such as sparsification and weight clustering are applied, which often require modifying the underlying model aggregation schemes or involve cumbersome hyperparameter tuning, with the latter not only adjusts the model's compression rate but also limits model's potential for continuous improvement over growing data. In this paper, we propose FedCompress, a novel approach that combines dynamic weight clustering and server-side knowledge distillation to reduce communication costs while learning highly generalizable models. Through a comprehensive evaluation on diverse public datasets, we demonstrate the efficacy of our approach compared to
    
[^280]: 基于能量的概念瓶颈模型：统一预测、概念干预和条件解释

    Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])

    [http://arxiv.org/abs/2401.14142](http://arxiv.org/abs/2401.14142)

    基于能量的概念瓶颈模型统一了预测、概念干预和条件解释的功能，解决了现有方法在高阶非线性相互作用和复杂条件依赖关系上的限制。

    

    现有方法，如概念瓶颈模型 (CBM)，在为黑盒深度学习模型提供基于概念的解释方面取得了成功。它们通常通过在给定输入的情况下预测概念，然后在给定预测的概念的情况下预测最终的类别标签。然而，它们经常无法捕捉到概念之间的高阶非线性相互作用，例如纠正一个预测的概念（例如“黄色胸部”）无法帮助纠正高度相关的概念（例如“黄色腹部”），导致最终准确率不理想；它们无法自然地量化不同概念和类别标签之间的复杂条件依赖关系（例如对于一个带有类别标签“Kentucky Warbler”和概念“黑色嘴巴”的图像，模型能够正确预测另一个概念“黑色冠”的概率是多少），因此无法提供关于黑盒模型工作原理更深层次的洞察。针对这些限制，我们提出了基于能量的概念瓶颈模型（Energy-based Concept Bottleneck Models）。

    Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
    
[^281]: 通过具有分层正则化的医学代码中心的多模态对比EHR建模预测下次就诊诊断

    Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.11648](http://arxiv.org/abs/2401.11648)

    通过医学代码中心的多模态对比EHR建模预测下次就诊诊断，并通过分层正则化提高性能。

    

    在医疗保健中，利用电子健康记录（EHR）预测下次就诊的诊断是一项必要的任务，对于制定医疗保健提供者和患者的主动未来计划至关重要。然而，之前的许多研究并没有充分解决EHR数据固有的异构和分层特征，必然导致次优的性能。为此，我们提出了NECHO，一种新颖的医学代码中心的多模态对比EHR学习框架，其中包括分层正则化。首先，我们使用定制的网络设计和一对双模态对比损失融合涵盖医学代码、人口统计数据和临床笔记的多方面信息，所有这些都围绕着医学代码表现。我们还使用医学本体中的父级信息来规范特定模态的编码器，以学习EHR数据的层次结构。对MIMIC-III数据进行的一系列实验证明了我们方法的有效性。

    Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
    
[^282]: QuasiNet: 一种具有可训练乘积层的神经网络

    QuasiNet: a neural network with trainable product layers. (arXiv:2401.06137v1 [cs.NE])

    [http://arxiv.org/abs/2401.06137](http://arxiv.org/abs/2401.06137)

    QuasiNet是一种新的神经网络模型，通过可训练的乘积层解决了小规模隐藏神经元下传统神经网络在难问题上的有限收敛问题，具有更高的成功率。

    

    传统神经网络在类似XOR或奇偶校验等难题的小规模隐藏神经元下只能实现有限的收敛。为了提高神经网络在这些问题上的成功率，我们提出了一种新的神经网络模型，受现有具有所谓乘积神经元和由经典误差反向传播推导出的学习规则启发，优雅地解决了互斥情况的问题。与现有的具有预设且不可调节权重的乘积神经元不同，我们的神经元乘积层也能够学习。我们测试了该模型，并将其成功率与传统的多层感知机在前述问题和其他难题（如两个螺旋）中进行了比较。我们的结果表明，我们的模型比传统的多层感知机更成功，并且在许多任务和应用中具有潜力。

    Classical neural networks achieve only limited convergence in hard problems such as XOR or parity when the number of hidden neurons is small. With the motivation to improve the success rate of neural networks in these problems, we propose a new neural network model inspired by existing neural network models with so called product neurons and a learning rule derived from classical error backpropagation, which elegantly solves the problem of mutually exclusive situations. Unlike existing product neurons, which have weights that are preset and not adaptable, our product layers of neurons also do learn. We tested the model and compared its success rate to a classical multilayer perceptron in the aforementioned problems as well as in other hard problems such as the two spirals. Our results indicate that our model is clearly more successful than the classical MLP and has the potential to be used in many tasks and applications.
    
[^283]: AUROC和AUPRC在类不平衡下的深入研究

    A Closer Look at AUROC and AUPRC under Class Imbalance. (arXiv:2401.06091v1 [cs.LG])

    [http://arxiv.org/abs/2401.06091](http://arxiv.org/abs/2401.06091)

    通过数学分析，研究发现AUROC和AUPRC在类别不平衡情况下可以以概率术语简洁地相关联。相比于人们普遍认为的AUPRC优越性，结果表明AUPRC并不如人们预期的有优势，并且可能是一种有害的指标。研究还通过分析大量文献验证了这一结论。

    

    在机器学习中，一个广泛的观点是，在二分类任务中，面积受限制的准确率曲线（AUPRC）比受试者工作特征曲线下的面积（AUROC）更好地用于模型比较，尤其是在存在类别不平衡的情况下。本文通过新颖的数学分析挑战了这一观点，并说明了AUROC和AUPRC可以以概率术语简洁地相关联。我们证明了AUPRC并不如人们普遍认为的在类别不平衡的情况下更优，甚至可能是一种有害的指标，因为它倾向于过分偏向于在正样本较为频繁的子群中改善模型。这种偏差可能会无意中增加算法的差异。在这些洞见的推动下，我们对现有的机器学习文献进行了彻底的回顾，并利用大型语言模型对arXiv上的150多万篇论文进行了分析。我们的调查重点是验证和证明声称的AUPRC优越性的普遍性。

    In machine learning (ML), a widespread adage is that the area under the precision-recall curve (AUPRC) is a superior metric for model comparison to the area under the receiver operating characteristic (AUROC) for binary classification tasks with class imbalance. This paper challenges this notion through novel mathematical analysis, illustrating that AUROC and AUPRC can be concisely related in probabilistic terms. We demonstrate that AUPRC, contrary to popular belief, is not superior in cases of class imbalance and might even be a harmful metric, given its inclination to unduly favor model improvements in subpopulations with more frequent positive labels. This bias can inadvertently heighten algorithmic disparities. Prompted by these insights, a thorough review of existing ML literature was conducted, utilizing large language models to analyze over 1.5 million papers from arXiv. Our investigation focused on the prevalence and substantiation of the purported AUPRC superiority. The result
    
[^284]: 使用营销组合建模（MMM）和Shapley值回归量化渠道合作伙伴层面的营销绩效

    Quantifying Marketing Performance at Channel-Partner Level by Using Marketing Mix Modeling (MMM) and Shapley Value Regression. (arXiv:2401.05653v1 [cs.LG])

    [http://arxiv.org/abs/2401.05653](http://arxiv.org/abs/2401.05653)

    本文研究了使用Shapley值回归对渠道合作伙伴层面的营销绩效进行量化，并通过与营销组合建模进行比较，证明了Shapley值回归的实用性。同时提出了一种简单的方法来计算调整系数。

    

    本文探索了在渠道合作伙伴层面利用Shapley值回归来解析营销绩效的应用，补充了渠道层面的营销组合建模（MMM）。利用来自金融服务行业的真实数据，我们展示了Shapley值回归在评估个别合作伙伴贡献方面的实用性。尽管结构化的现场测试以及合作博弈理论最为准确，但经常会非常复杂和昂贵。因此，Shapley值回归是一种更可行的方法，可以分离营销渠道中每个营销合作伙伴的影响。我们还提出了一种简单的方法来推导调整系数，将其与其他方法进行比较。

    This paper explores the application of Shapley Value Regression in dissecting marketing performance at channel-partner level, complementing channel-level Marketing Mix Modeling (MMM). Utilizing real-world data from the financial services industry, we demonstrate the practicality of Shapley Value Regression in evaluating individual partner contributions. Although structured in-field testing along with cooperative game theory is most accurate, it can often be highly complex and expensive to conduct. Shapley Value Regression is thus a more feasible approach to disentangle the influence of each marketing partner within a marketing channel. We also propose a simple method to derive adjusted coefficients of Shapley Value Regression and compares it with alternative approaches.
    
[^285]: 使用感知损失的扩散模型

    Diffusion Model with Perceptual Loss. (arXiv:2401.00110v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.00110](http://arxiv.org/abs/2401.00110)

    本研究介绍了一种使用感知损失的扩散模型，通过无分类器指导实现了生成更真实样本的目的。

    

    使用均方误差损失训练的扩散模型倾向于生成不真实的样本。目前的最先进模型依靠无分类器指导来改善样本质量，然而其惊人的效果尚未完全理解。本文中，我们展示了无分类器指导的有效性在一定程度上源自其作为一种隐式感知指导的形式。因此，我们可以直接在扩散训练中加入感知损失来提高样本质量。由于扩散训练中使用的分数匹配目标与无监督训练感知网络时使用的去噪自动编码器目标非常相似，因此扩散模型本身就是一个感知网络，并可以用于生成有意义的感知损失。我们提出了一种新颖的自感知目标，其结果是扩散模型能够生成更真实的样本。对于条件生成，我们的方法仅改善样本质量，而不与条件绑定。

    Diffusion models trained with mean squared error loss tend to generate unrealistic samples. Current state-of-the-art models rely on classifier-free guidance to improve sample quality, yet its surprising effectiveness is not fully understood. In this paper, We show that the effectiveness of classifier-free guidance partly originates from it being a form of implicit perceptual guidance. As a result, we can directly incorporate perceptual loss in diffusion training to improve sample quality. Since the score matching objective used in diffusion training strongly resembles the denoising autoencoder objective used in unsupervised training of perceptual networks, the diffusion model itself is a perceptual network and can be used to generate meaningful perceptual loss. We propose a novel self-perceptual objective that results in diffusion models capable of generating more realistic samples. For conditional generation, our method only improves sample quality without entanglement with the condit
    
[^286]: M3D：通过最小化最大均值差异来进行数据集压缩

    M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy. (arXiv:2312.15927v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.15927](http://arxiv.org/abs/2312.15927)

    该论文提出了一种名为M3D的新型基于分布匹配的数据集压缩方法，通过最小化最大均值差异来提高压缩效率，克服了优化过程在实际和较大数据集上的应用难题。

    

    训练最先进的深度模型通常需要大量的数据，导致训练和存储成本高昂。为了解决这些挑战，提出了数据集压缩方法，通过学习一个小的合成集合来保留原始大规模数据集的关键信息。目前，以优化为导向的方法是数据集压缩领域中实现最先进结果的主要方法。然而，双层优化过程阻碍了这些方法在实际和较大的数据集上的实际应用。为了提高压缩效率，先前的工作提出了分布匹配（DM）作为替代方法，显著减少了压缩成本。然而，由于专注于对齐分布的一阶矩，目前的基于DM的方法与以优化为导向的方法相比，结果不太可比。在本文中，我们介绍了一种名为M3D的新型基于DM的数据集压缩方法。

    Training state-of-the-art (SOTA) deep models often requires extensive data, resulting in substantial training and storage costs. To address these challenges, dataset condensation has been developed to learn a small synthetic set that preserves essential information from the original large-scale dataset. Nowadays, optimization-oriented methods have been the primary method in the field of dataset condensation for achieving SOTA results. However, the bi-level optimization process hinders the practical application of such methods to realistic and larger datasets. To enhance condensation efficiency, previous works proposed Distribution-Matching (DM) as an alternative, which significantly reduces the condensation cost. Nonetheless, current DM-based methods have yielded less comparable results to optimization-oriented methods due to their focus on aligning only the first moment of the distributions. In this paper, we present a novel DM-based method named M3D for dataset condensation by Minimi
    
[^287]: 隐私保护的神经图数据库

    Privacy-Preserving Neural Graph Databases. (arXiv:2312.15591v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2312.15591](http://arxiv.org/abs/2312.15591)

    隐私保护的神经图数据库结合了图数据库和神经网络的优势，能够高效存储、检索和分析图结构数据。然而，这种能力也带来了潜在的隐私风险。

    

    在大数据和快速发展的信息系统时代，高效准确地检索数据变得日益重要。神经图数据库（NGDB）是一种强大的范式，将图数据库（图形数据库）和神经网络的优势相结合，实现了对图结构数据的高效存储、检索和分析。神经嵌入存储和复杂神经逻辑查询回答为NGDB提供了泛化能力。当图形不完整时，神经图数据库可以通过提取潜在模式和表示来填补图结构中的空缺，揭示隐藏的关系并实现准确的查询回答。然而，这种能力也带来了潜在的隐私风险，因为恶意攻击者可以使用精心设计的组合查询推断出更多敏感信息，例如通过比较图数据库中Turing奖得主的答案集。

    In the era of big data and rapidly evolving information systems, efficient and accurate data retrieval has become increasingly crucial. Neural graph databases (NGDBs) have emerged as a powerful paradigm that combines the strengths of graph databases (graph DBs) and neural networks to enable efficient storage, retrieval, and analysis of graph-structured data. The usage of neural embedding storage and complex neural logical query answering provides NGDBs with generalization ability. When the graph is incomplete, by extracting latent patterns and representations, neural graph databases can fill gaps in the graph structure, revealing hidden relationships and enabling accurate query answering. Nevertheless, this capability comes with inherent trade-offs, as it introduces additional privacy risks to the database. Malicious attackers can infer more sensitive information in the database using well-designed combinatorial queries, such as by comparing the answer sets of where Turing Award winner
    
[^288]: TiMix: 文本感知图像混合用于有效的视觉语言预训练

    TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training. (arXiv:2312.08846v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.08846](http://arxiv.org/abs/2312.08846)

    TiMix是一种将文本感知的图像混合技术用于视觉语言预训练的方法，通过集成混合数据增强技术，并从互信息的角度进行理论分析，提高了数据效率并取得了可比较的性能。

    

    自监督的多模态对比学习（SMCL）通过对齐视觉和语言模态，显著推进了现代视觉语言预训练（VLP）模型的发展。然而，由于网络收集的文本-图像对中存在噪声，扩大SMCL的训练数据量在计算成本和数据效率方面面临着相当大的障碍。为了提高VLP的数据效率，我们提出了文本感知图像混合（TiMix），将基于混合的数据增强技术集成到SMCL中，显著提升了性能，而不会显著增加计算开销。我们从互信息（MI）的角度对TiMix进行了理论分析，表明跨模态对比学习的混合数据样本隐式地作为对比损失的正则化器。实验结果表明，即使使用较少的训练数据和较短的训练时间，TiMix在下游任务上表现出可比较的性能。

    Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances modern Vision-Language Pre-training (VLP) models by aligning visual and linguistic modalities. Due to noises in web-harvested text-image pairs, however, scaling up training data volume in SMCL presents considerable obstacles in terms of computational cost and data inefficiency. To improve data efficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates mix-based data augmentation techniques into SMCL, yielding significant performance improvements without significantly increasing computational overhead. We provide a theoretical analysis of TiMixfrom a mutual information (MI) perspective, showing that mixed data samples for cross-modal contrastive learning implicitly serve as a regularizer for the contrastive loss. The experimental results demonstrate that TiMix exhibits a comparable performance on downstream tasks, even with a reduced amount of training data and shorter training time, when be
    
[^289]: 分数梯度下降法的收敛性分析

    Convergence Analysis of Fractional Gradient Descent. (arXiv:2311.18426v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2311.18426](http://arxiv.org/abs/2311.18426)

    本论文通过分析不同环境下的分数梯度下降方法，建立了分数导数与整数导数之间的新界限，并证明了在平滑且凸、平滑且强凸以及平滑且非凸环境下的收敛性，为分数梯度下降的收敛性分析提供了新的理论支持。

    

    分数导数是整数阶导数的一种广义推广。对于优化问题，研究使用分数导数的梯度下降方法的收敛性是非常有意义的。目前，对于分数梯度下降的收敛性分析在研究方法和研究环境方面都存在限制。本文旨在填补这些空白，分析了平滑且凸、平滑且强凸以及平滑且非凸环境下的分数梯度下降的变种。首先，我们将建立将分数导数与整数导数联系起来的新界限。然后，利用这些界限证明了对于平滑且强凸函数的线性收敛性和对于平滑且凸函数的O(1/T)收敛性。此外，通过使用一种更适合分数导数的扩展平滑度概念-Holder平滑度，我们还证明了对于平滑且非凸函数的O(1/T)收敛性。

    Fractional derivatives are a well-studied generalization of integer order derivatives. Naturally, for optimization, it is of interest to understand the convergence properties of gradient descent using fractional derivatives. Convergence analysis of fractional gradient descent is currently limited both in the methods analyzed and the settings analyzed. This paper aims to fill in these gaps by analyzing variations of fractional gradient descent in smooth and convex, smooth and strongly convex, and smooth and non-convex settings. First, novel bounds will be established bridging fractional and integer derivatives. Then, these bounds will be applied to the aforementioned settings to prove linear convergence for smooth and strongly convex functions and $O(1/T)$ convergence for smooth and convex functions. Additionally, we prove $O(1/T)$ convergence for smooth and non-convex functions using an extended notion of smoothness - H\"older smoothness - that is more natural for fractional derivative
    
[^290]: 线性对数正态注意力与无偏集中力

    Linear Log-Normal Attention with Unbiased Concentration. (arXiv:2311.13541v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.13541](http://arxiv.org/abs/2311.13541)

    本论文研究了自注意机制，并分析了注意力矩阵的分布和集中能力。通过引入线性对数正态注意力来模拟原始自注意力的分布和集中行为，提高了Transformer模型的可扩展性。

    

    Transformer模型在各种应用中取得了显著的成果。然而，由于自注意机制的时间和内存复杂度与序列长度的二次关系，其可扩展性受到限制。当处理长文档或高分辨率图像时，这一限制构成了重大障碍。本研究通过分析注意力矩阵的分布和集中能力，对自注意机制进行了研究。此外，我们提出了衡量这些数量的工具，并引入了一种新的自注意机制，即线性对数正态注意力，旨在模拟原始自注意力的分布和集中行为。我们在常用的自然语言基准测试上的实验证明，我们提出的线性对数正态注意力优于其他线性化注意力替代方法，为增强Transformer模型的可扩展性提供了一个有前途的途径。我们的代码附在补充材料中。

    Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models. Our code is available in supplementary
    
[^291]: 持续学习在语言转换中的研究

    A Study of Continual Learning Under Language Shift. (arXiv:2311.01200v1 [cs.CL])

    [http://arxiv.org/abs/2311.01200](http://arxiv.org/abs/2311.01200)

    本文研究了持续学习在语言转换中的应用，发现在更新语言模型时，前向转移效果较好且与语言顺序无关，但后向转移效果可能取决于新语言的顺序和特征。

    

    最近语言模型预训练的数据和模型规模的增加导致了巨大的训练成本。在随时间推移而出现新数据的情况下，更新模型而不是完全重新训练可以带来显著的收益。在本文中，我们研究了在新语言出现时更新语言模型时的好处和弊端，即在语言转换中持续学习的情况。从单语英语语言模型出发，我们逐步添加了来自挪威语和冰岛语的数据，以研究前向和后向转移效果如何取决于预训练顺序和语言特征，对于不同的模型大小和学习率调度器。我们的结果表明，尽管前向转移主要是正向的，不受语言顺序的影响，但后向转移则可能是正向的或负向的，具体取决于新语言的顺序和特征。为了解释这些模式，我们探索了几种语言相似度度量方法。

    The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. In this paper, we study the benefits and downsides of updating a language model when new data comes from new languages - the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Norwegian and Icelandic to investigate how forward and backward transfer effects depend on the pre-training order and characteristics of languages, for different model sizes and learning rate schedulers. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be either positive or negative depending on the order and characteristics of new languages. To explain these patterns we explore several language similarity metr
    
[^292]: 对比差异性预测编码

    Contrastive Difference Predictive Coding. (arXiv:2310.20141v1 [cs.LG])

    [http://arxiv.org/abs/2310.20141](http://arxiv.org/abs/2310.20141)

    本文介绍了一种时间差异版本的对比预测编码，通过将不同时间序列数据的片段组合在一起，来减少学习预测未来事件所需的数据量。实验证明，与先前的方法相比，我们的方法在成功率上提高了2倍，并且对于随机环境有更好的适应能力。

    

    预测和推理未来是许多时间序列问题的核心。例如，目标导向的强化学习可以被看作是学习表示以预测未来可能访问的状态。虽然先前的方法已经使用对比性预测编码来建模时间序列数据，但学习编码长期依赖通常需要大量的数据。在本文中，我们引入了一种时间差异版本的对比预测编码，将不同时间序列数据的片段组合在一起，以减少学习未来事件预测所需的数据量。我们将这种表示学习方法应用于导出目标导向的强化学习的离策略算法。实验证明，与先前的强化学习方法相比，我们的方法在成功率上实现了中位数提高2倍，并且可以更好地应对随机环境。在表格设置中，我们展示了我们的方法约为20倍。

    Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20
    
[^293]: 基于流的分布鲁棒优化

    Flow-based Distributionally Robust Optimization. (arXiv:2310.19253v1 [cs.LG])

    [http://arxiv.org/abs/2310.19253](http://arxiv.org/abs/2310.19253)

    这项研究提出了一种称为FlowDRO的计算高效框架，用于解决基于流的分布鲁棒优化问题，通过使用流模型和Wasserstein近端梯度流类型的算法，实现了对具有更大样本大小的问题的可扩展性和更好的泛化能力。

    

    我们提出了一种称为FlowDRO的计算高效框架，用于解决基于流的分布鲁棒优化（DRO）问题，其中要求最坏情况分布（也称为最不利分布，LFD）是连续的，从而使得算法能够可扩展到具有更大样本大小的问题，并实现对诱导的鲁棒算法的更好泛化能力。为了解决计算上具有挑战性的无限维优化问题，我们利用基于流的模型，在数据分布和目标分布之间进行连续时间可逆传输映射，并开发了一种Wasserstein近端梯度流类型的算法。在实践中，我们通过梯度下降逐步训练块内的神经网络序列来参数化传输映射。我们的计算框架通用，能够处理高维数据和大样本大小，并可用于各种应用。

    We present a computationally efficient framework, called \texttt{FlowDRO}, for solving flow-based distributionally robust optimization (DRO) problems with Wasserstein uncertainty sets, when requiring the worst-case distribution (also called the Least Favorable Distribution, LFD) to be continuous so that the algorithm can be scalable to problems with larger sample sizes and achieve better generalization capability for the induced robust algorithms. To tackle the computationally challenging infinitely dimensional optimization problem, we leverage flow-based models, continuous-time invertible transport maps between the data distribution and the target distribution, and develop a Wasserstein proximal gradient flow type of algorithm. In practice, we parameterize the transport maps by a sequence of neural networks progressively trained in blocks by gradient descent. Our computational framework is general, can handle high-dimensional data with large sample sizes, and can be useful for various
    
[^294]: 无需增强的简单非对称图对比学习

    Simple and Asymmetric Graph Contrastive Learning without Augmentations. (arXiv:2310.18884v1 [cs.LG])

    [http://arxiv.org/abs/2310.18884](http://arxiv.org/abs/2310.18884)

    本文提出了一种无需增强的简单非对称图对比学习方法GraphACL，通过考虑邻居节点的非对称视图，该方法能够有效地在同类和异类图上进行对比学习，对于建模异类图非常重要。

    

    图对比学习（GCL）在图结构数据的表示学习中显示出了优越的性能。尽管取得了成功，但大多数现有的GCL方法依赖于预制的图增强和同类假设。因此，它们在连通节点可能具有不同类标签和不相似特征的异类图上无法很好地推广。在本文中，我们研究了在同类和异类图上进行对比学习的问题。我们发现，通过考虑邻居节点的非对称视图，我们可以实现有希望的性能。由此产生的简单算法，称为图的非对称对比学习(GraphACL)，易于实现，不依赖于图增强和同类假设。我们提供了理论和实证证据，证明GraphACL能够捕捉单跳本地邻域信息和双跳单一相似性，这两者对于建模异类图非常重要。

    Graph Contrastive Learning (GCL) has shown superior performance in representation learning in graph-structured data. Despite their success, most existing GCL methods rely on prefabricated graph augmentation and homophily assumptions. Thus, they fail to generalize well to heterophilic graphs where connected nodes may have different class labels and dissimilar features. In this paper, we study the problem of conducting contrastive learning on homophilic and heterophilic graphs. We find that we can achieve promising performance simply by considering an asymmetric view of the neighboring nodes. The resulting simple algorithm, Asymmetric Contrastive Learning for Graphs (GraphACL), is easy to implement and does not rely on graph augmentations and homophily assumptions. We provide theoretical and empirical evidence that GraphACL can capture one-hop local neighborhood information and two-hop monophily similarity, which are both important for modeling heterophilic graphs. Experimental results s
    
[^295]: 监督和受罚基线校正

    Supervised and Penalized Baseline Correction. (arXiv:2310.18306v1 [stat.ML])

    [http://arxiv.org/abs/2310.18306](http://arxiv.org/abs/2310.18306)

    本研究改进了受罚基线校正方法，通过利用先验分析物浓度来改善光谱预测性能，并在两个近红外数据集上进行了评估。

    

    光谱测量可以显示由吸收和散射成分混合引起的扭曲光谱形状。这些扭曲（或基线）通常表现为非恒定偏移或低频振荡。因此，这些基线可能对分析和定量结果产生不利影响。基线校正是一个涵盖了预处理方法的总称，通过获取基线光谱（不需要的扭曲）并通过差异化去除扭曲。然而，当前最先进的基线校正方法即使可用分析物浓度或者它们对观察到的光谱变异有重要贡献，也没有利用它们。我们研究了一类最先进的方法（受罚基线校正）并对其进行修改，使其能够适应先验分析物浓度，从而提高预测性能。将在两个近红外数据集上评估性能，包括经典受罚方法。

    Spectroscopic measurements can show distorted spectra shapes arising from a mixture of absorbing and scattering contributions. These distortions (or baselines) often manifest themselves as non-constant offsets or low-frequency oscillations. As a result, these baselines can adversely affect analytical and quantitative results. Baseline correction is an umbrella term where one applies pre-processing methods to obtain baseline spectra (the unwanted distortions) and then remove the distortions by differencing. However, current state-of-the art baseline correction methods do not utilize analyte concentrations even if they are available, or even if they contribute significantly to the observed spectral variability. We examine a class of state-of-the-art methods (penalized baseline correction) and modify them such that they can accommodate a priori analyte concentration such that prediction can be enhanced. Performance will be access on two near infra-red data sets across both classical penal
    
[^296]: 异构联邦学习与群体感知提示调整

    Heterogeneous Federated Learning with Group-Aware Prompt Tuning. (arXiv:2310.18285v1 [cs.LG])

    [http://arxiv.org/abs/2310.18285](http://arxiv.org/abs/2310.18285)

    本文研究了在异构联邦学习中利用预训练的Transformer和高效的提示调整策略，通过学习共享和群体提示实现获取通用知识和个性化知识，以训练适应不同本地数据分布的全局模型。

    

    Transformer在各种机器学习任务中取得了显著的成功，促使它们被广泛采用。本文探索了它们在联邦学习（FL）领域的应用，特别关注具有不同本地数据集的异构场景。为了满足FL的计算和通信需求，我们利用预训练的Transformer，并使用高效的提示调整策略。我们的策略引入了同时学习共享和群体提示的概念，能够同时获取通用知识和群体特定知识。此外，提示选择模块为每个输入分配个性化的群体提示，使全局模型与每个客户端数据分布对齐。这种方法使我们能够训练一个单一的全局模型，能够自动适应不同的本地客户端数据分布，而无需进行本地微调。通过这种方式，我们提出的方法有效地搭建了链接

    Transformers have achieved remarkable success in various machine-learning tasks, prompting their widespread adoption. In this paper, we explore their application in the context of federated learning (FL), with a particular focus on heterogeneous scenarios where individual clients possess diverse local datasets. To meet the computational and communication demands of FL, we leverage pre-trained Transformers and use an efficient prompt-tuning strategy. Our strategy introduces the concept of learning both shared and group prompts, enabling the acquisition of universal knowledge and group-specific knowledge simultaneously. Additionally, a prompt selection module assigns personalized group prompts to each input, aligning the global model with the data distribution of each client. This approach allows us to train a single global model that can automatically adapt to various local client data distributions without requiring local fine-tuning. In this way, our proposed method effectively bridge
    
[^297]: 无线联合策略梯度的过空中聚合方法

    Over-the-air Federated Policy Gradient. (arXiv:2310.16592v1 [cs.LG])

    [http://arxiv.org/abs/2310.16592](http://arxiv.org/abs/2310.16592)

    本文提出了一种过空中联合策略梯度算法，通过无线信道广播携带本地信息的模拟信号实现更新策略参数，研究了噪声和信道失真对算法收敛性的影响，并通过仿真结果验证了算法的有效性。

    

    近年来，过空中聚合在大规模分布式学习、优化和感知中得到了广泛关注。本文提出了一种过空中联合策略梯度算法，其中所有的智能体同时向共享的无线信道广播携带本地信息的模拟信号，中央控制器使用接收到的汇总波形来更新策略参数。我们研究了噪声和信道失真对所提出算法收敛性的影响，并建立了通信和采样的复杂度来找到一个$\epsilon$-近似的稳定点。最后，我们通过一些仿真结果展示了该算法的有效性。

    In recent years, over-the-air aggregation has been widely considered in large-scale distributed learning, optimization, and sensing. In this paper, we propose the over-the-air federated policy gradient algorithm, where all agents simultaneously broadcast an analog signal carrying local information to a common wireless channel, and a central controller uses the received aggregated waveform to update the policy parameters. We investigate the effect of noise and channel distortion on the convergence of the proposed algorithm, and establish the complexities of communication and sampling for finding an $\epsilon$-approximate stationary point. Finally, we present some simulation results to show the effectiveness of the algorithm.
    
[^298]: 大型语言模型中的函数向量

    Function Vectors in Large Language Models. (arXiv:2310.15213v1 [cs.CL])

    [http://arxiv.org/abs/2310.15213](http://arxiv.org/abs/2310.15213)

    大型语言模型中存在一种简单的神经机制，将输入-输出函数表示为向量。这些函数向量在不同的上下文中具有鲁棒性，并且具有强大的因果效应。同时，它们还具有将语义向量进行组合的能力。

    

    我们报告了一个简单的神经机制，将输入-输出函数表示为自回归变换语言模型（LMs）中的向量。通过在各种上下文学习（ICL）任务上使用因果中介分析，我们发现少数注意力头传输了展示任务的紧凑表示，我们称之为函数向量（FV）。FV对上下文的变化具有鲁棒性，即它们在不类似于其收集时的ICL上下文的情况下触发对输入的任务执行，例如零样本和自然文本设置。我们在各种任务、模型和层上测试了FV，并在中层发现强大的因果效应。我们研究了FV的内部结构，并发现虽然它们通常包含编码函数的输出空间的信息，但仅此信息无法重构FV。最后，我们测试了FV中的语义向量组合，并发现在某种程度上存在组合的能力。

    We report the presence of a simple neural mechanism that represents an input-output function as a vector within autoregressive transformer language models (LMs). Using causal mediation analysis on a diverse range of in-context-learning (ICL) tasks, we find that a small number attention heads transport a compact representation of the demonstrated task, which we call a function vector (FV). FVs are robust to changes in context, i.e., they trigger execution of the task on inputs such as zero-shot and natural text settings that do not resemble the ICL contexts from which they are collected. We test FVs across a range of tasks, models, and layers and find strong causal effects across settings in middle layers. We investigate the internal structure of FVs and find while that they often contain information that encodes the output space of the function, this information alone is not sufficient to reconstruct an FV. Finally, we test semantic vector composition in FVs, and find that to some exte
    
[^299]: 超越准确性：用IdentityChain评估大型代码语言模型的自一致性

    Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain. (arXiv:2310.14053v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14053](http://arxiv.org/abs/2310.14053)

    这篇论文提出了一种评估大型代码语言模型自一致性的方法，并指出目前的模型在自一致性方面存在问题。

    

    代码语言模型(Code LLMs)在实际应用中的使用越来越多，因此对它们进行评估至关重要。传统的准确性评估方法评估Code LLMs在一系列独立任务上的性能，但忽视了其在不同任务上的自一致性。直观来讲，一个可信赖的模型在为其自身的代码生成自然语言规范以及为其自身的规范生成代码时应该是自一致的。未能保持自一致性揭示了对自然语言和编程语言共享语义的理解的不足，从而削弱了模型的可信度。本文首先正式定义了Code LLMs的自一致性，然后设计了一个名为IdentityChain的框架，可以同时有效且高效地评估模型的自一致性和传统准确性。我们研究了11个Code LLMs，并表明它们未能保持自一致性。

    Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the conventional accuracy evaluates the performance of Code LLMs on a set of individual tasks, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and conventional accuracy of a model at the same time. We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indee
    
[^300]: 采用有噪声树度量的优化传输方法

    Optimal Transport for Measures with Noisy Tree Metric. (arXiv:2310.13653v1 [stat.ML])

    [http://arxiv.org/abs/2310.13653](http://arxiv.org/abs/2310.13653)

    本文提出了一种针对树度量有噪声的优化传输方法，通过引入新的不确定性集合，解决了实际应用中树结构扰动的问题。

    

    本研究探讨了在树度量空间上支持的概率测度的优化传输（OT）问题。已知这种OT问题（即树-瓦瓦斯坦（TW））具有闭合形式表达式，但基本上取决于输入测度支持上的底层树结构。然而，在实际操作中，由于噪声或对抗性测量，给定的树结构可能会被扰动。为了缓解这个问题，我们采取了最大-最小鲁棒OT方法，该方法考虑了在一个树度量的不确定性集合上两个输入测度之间的最大可能距离。总体上说，由于其非凸性和非光滑性，这种方法很难计算，即便是在支持为1维空间的测度情况下，这妨碍了它的实际应用，特别是在大规模情景下。在本文中，我们从边缘删除/添加的角度提出了一种新颖的树度量的不确定性集合，这个集合在一个优雅的框架下涵盖了多样的树结构。

    We study optimal transport (OT) problem for probability measures supported on a tree metric space. It is known that such OT problem (i.e., tree-Wasserstein (TW)) admits a closed-form expression, but depends fundamentally on the underlying tree structure over supports of input measures. In practice, the given tree structure may be, however, perturbed due to noisy or adversarial measurements. In order to mitigate this issue, we follow the max-min robust OT approach which considers the maximal possible distances between two input measures over an uncertainty set of tree metrics. In general, this approach is hard to compute, even for measures supported in $1$-dimensional space, due to its non-convexity and non-smoothness which hinders its practical applications, especially for large-scale settings. In this work, we propose \emph{novel uncertainty sets of tree metrics} from the lens of edge deletion/addition which covers a diversity of tree structures in an elegant framework. Consequently, 
    
[^301]: 生成流网络作为熵正则化强化学习

    Generative Flow Networks as Entropy-Regularized RL. (arXiv:2310.12934v1 [cs.LG])

    [http://arxiv.org/abs/2310.12934](http://arxiv.org/abs/2310.12934)

    本研究将生成流网络的学习任务重新定义为具有特定奖励和正则化器结构的熵正则化强化学习问题，并证明熵正则化强化学习方法在生成流网络训练中具有实际效率和竞争力。

    

    最近提出的生成流网络(GFlowNets)是一种训练策略以便样本具有与给定奖励成比例的组合离散对象的概率的方法，通过一系列的动作。 GFlowNets利用问题的序列性质，与强化学习(RL)进行类比。我们的工作将RL和GFlowNets之间的联系扩展到了一般情况。我们演示了如何将学习生成流网络的任务高效地重新定义为具有特定奖励和正则化器结构的熵正则化RL问题。此外，我们通过将标准的软RL算法应用于几个概率建模任务的GFlowNet训练，来说明这种重定义的实际效率。与先前报道的结果相反，我们表明熵正则化强化学习方法在与已有的GFlowNet训练方法竞争中具有竞争力。这个观点为将强化学习原则融入实际问题提供了直接途径。

    The recently proposed generative flow networks (GFlowNets) are a method of training a policy to sample compositional discrete objects with probabilities proportional to a given reward via a sequence of actions. GFlowNets exploit the sequential nature of the problem, drawing parallels with reinforcement learning (RL). Our work extends the connection between RL and GFlowNets to a general case. We demonstrate how the task of learning a generative flow network can be efficiently redefined as an entropy-regularized RL problem with a specific reward and regularizer structure. Furthermore, we illustrate the practical efficiency of this reformulation by applying standard soft RL algorithms to GFlowNet training across several probabilistic modeling tasks. Contrary to previously reported results, we show that entropic RL approaches can be competitive against established GFlowNet training methods. This perspective opens a direct path for integrating reinforcement learning principles into the real
    
[^302]: 大规模层次预测

    Hierarchical Forecasting at Scale. (arXiv:2310.12809v1 [cs.LG])

    [http://arxiv.org/abs/2310.12809](http://arxiv.org/abs/2310.12809)

    提出了一种大规模层次预测的方法，使用稀疏损失函数直接优化层次产品和/或时间结构，从而为数百万个时间序列提供一致的预测。在实验中，该方法在M5数据集上表现出10%的性能提升。

    

    当时间序列的数量增加时，现有的层次预测技术的扩展性较差。我们提出使用稀疏损失函数来学习千万个时间序列的一致预测，该损失函数直接优化层次产品和/或时间结构。我们稀疏层次损失函数的优点是提供了一种方法，使实践者能够产生与任何选择的横向或时间层次一致的底层预测。此外，消除传统层次预测技术中需要的后处理步骤，减少了预测流程中的计算成本。在公开的M5数据集上，与基准损失函数相比，我们的稀疏层次损失函数性能提高了10%（RMSE）。我们将稀疏层次损失函数实现在bol这个大型欧洲电子商务平台上现存的预测模型中。

    Existing hierarchical forecasting techniques scale poorly when the number of time series increases. We propose to learn a coherent forecast for millions of time series with a single bottom-level forecast model by using a sparse loss function that directly optimizes the hierarchical product and/or temporal structure. The benefit of our sparse hierarchical loss function is that it provides practitioners a method of producing bottom-level forecasts that are coherent to any chosen cross-sectional or temporal hierarchy. In addition, removing the need for a post-processing step as required in traditional hierarchical forecasting techniques reduces the computational cost of the prediction phase in the forecasting pipeline. On the public M5 dataset, our sparse hierarchical loss function performs up to 10% (RMSE) better compared to the baseline loss function. We implement our sparse hierarchical loss function within an existing forecasting model at bol, a large European e-commerce platform, res
    
[^303]: 用于学习图神经网络的准瓦狄斯坦损失

    A Quasi-Wasserstein Loss for Learning Graph Neural Networks. (arXiv:2310.11762v1 [cs.LG])

    [http://arxiv.org/abs/2310.11762](http://arxiv.org/abs/2310.11762)

    这篇论文提出了一种新的准瓦狄斯坦损失函数，通过利用图上的最优传输来学习图神经网络，消除了现有损失函数在节点级别预测中可能存在的不一致性。

    

    当在节点级别预测任务中学习图神经网络（GNNs）时，大多数现有的损失函数是独立地应用于每个节点的，即使节点嵌入和它们的标签由于图结构的存在而不是独立同分布的。为了消除这种不一致性，本研究提出了一种新的准瓦狄斯坦（QW）损失函数，借助于在图上定义的最优传输，从而引导GNN的新学习和预测范式。特别地，我们设计了一种“准瓦狄斯坦”距离，用于观测到的多维节点标签和它们的估计之间，通过优化在图边上定义的标签传输。这些估计是由一个GNN参数化的，其中最优标签传输可以选择性地确定图边的权重。通过将标签传输的严格约束重新表达为基于Bregman散度的正则化项，我们得到了所提出的准瓦狄斯坦损失，关联两个高效求解器来学习GNN以及最优标签传输。

    When learning graph neural networks (GNNs) in node-level prediction tasks, most existing loss functions are applied for each node independently, even if node embeddings and their labels are non-i.i.d. because of their graph structures. To eliminate such inconsistency, in this study we propose a novel Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on graphs, leading to new learning and prediction paradigms of GNNs. In particular, we design a "Quasi-Wasserstein" distance between the observed multi-dimensional node labels and their estimations, optimizing the label transport defined on graph edges. The estimations are parameterized by a GNN in which the optimal label transport may determine the graph edge weights optionally. By reformulating the strict constraint of the label transport to a Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein loss associated with two efficient solvers learning the GNN together with optimal label transp
    
[^304]: 大型语言模型上的用户推理攻击

    User Inference Attacks on Large Language Models. (arXiv:2310.09266v1 [cs.CR])

    [http://arxiv.org/abs/2310.09266](http://arxiv.org/abs/2310.09266)

    本论文研究了在大型语言模型上的用户推理攻击，发现LLMs对于各种微调数据集都很容易受到攻击，尤其是对于离群用户和贡献大量数据的用户。这对保护用户隐私具有重要意义。

    

    微调是将大型语言模型（LLMs）定制为专业任务和应用的常见有效方法。本文研究了在用户数据上微调LLMs的隐私问题。为此，我们定义了一个称为用户推理的现实威胁模型，其中攻击者推断出用户的数据是否被用于微调。我们实现了这种威胁模型的攻击，只需要从用户那里获取一小组样本（可能与用于训练的样本不同）和对微调LLM的黑盒访问权限。我们发现，LLMs在各种微调数据集上易受用户推理攻击的影响，有时攻击成功率接近完美。此外，我们调查了哪些特性使用户容易受到用户推理的攻击，发现离群用户（即数据分布与其他用户明显不同）和贡献大量数据的用户更容易受到攻击。最后，我们探索了解决这种攻击的方案。

    Fine-tuning is a common and effective method for tailoring large language models (LLMs) to specialized tasks and applications. In this paper, we study the privacy implications of fine-tuning LLMs on user data. To this end, we define a realistic threat model, called user inference, wherein an attacker infers whether or not a user's data was used for fine-tuning. We implement attacks for this threat model that require only a small set of samples from a user (possibly different from the samples used for training) and black-box access to the fine-tuned LLM. We find that LLMs are susceptible to user inference attacks across a variety of fine-tuning datasets, at times with near perfect attack success rates. Further, we investigate which properties make users vulnerable to user inference, finding that outlier users (i.e. those with data distributions sufficiently different from other users) and users who contribute large quantities of data are most susceptible to attack. Finally, we explore s
    
[^305]: 神经扩散模型

    Neural Diffusion Models. (arXiv:2310.08337v1 [cs.LG])

    [http://arxiv.org/abs/2310.08337](http://arxiv.org/abs/2310.08337)

    本文提出了神经扩散模型（NDMs），它是传统扩散模型的推广，可以定义和学习数据的时间依赖非线性变换。我们展示了如何在无需模拟的设置中使用变分界对NDMs进行优化，并通过在标准图像生成任务上的实验证明了可学习变换的NDMs的实用性。

    

    扩散模型在许多生成任务上表现出色。然而，尽管最近取得了一些成功，大多数扩散模型只允许对数据分布进行线性转换，受到了一定的限制。相比之下，更广泛的变换家族可能有助于更有效地训练生成分布，简化逆过程并缩小真实负对数似然和变分近似之间的差距。本文介绍了神经扩散模型（NDMs），它是传统扩散模型的推广，可以定义和学习数据的时间依赖非线性变换。我们展示了如何在一个无需模拟的设置中使用变分界对NDMs进行优化。此外，我们导出了NDMs的时间连续形式，通过使用现成的数值ODE和SDE求解器，可以快速可靠地进行推理。最后，我们通过在标准图像生成任务上的实验展示了可学习变换的NDMs的实用性。

    Diffusion models have shown remarkable performance on many generative tasks. Despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. In contrast, broader family of transformations can potentially help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. In this paper, we present Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimise NDMs using a variational bound in a simulation-free setting. Moreover, we derive a time-continuous formulation of NDMs, which allows fast and reliable inference using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the utility of NDMs with learnable transformations through experiments on standard image ge
    
[^306]: 通过对抗行为抑制Q学习中的过高估计

    Suppressing Overestimation in Q-Learning through Adversarial Behaviors. (arXiv:2310.06286v1 [cs.LG])

    [http://arxiv.org/abs/2310.06286](http://arxiv.org/abs/2310.06286)

    本文提出了一种新的Q学习算法，通过引入虚拟对抗性玩家，有效调节了标准Q学习中的过高估计偏差，提出的算法简单而有效，能够轻松应用于强化学习算法并提高性能。

    

    本文旨在提出一种新的Q学习算法，使用一个虚拟对抗性玩家，称为虚拟对抗性Q学习（DAQ），以有效地调节标准Q学习中的过高估计偏差。通过虚拟玩家，学习可以被表述为一个双人零和博弈。所提出的DAQ将几种Q学习的变体统一到一个单一的框架中，以控制过高估计偏差，例如maxmin Q学习和minmax Q学习（本文提出）。通过虚拟对抗性行为，所提出的DAQ是一种简单而有效的方式，可以轻松应用于现成的强化学习算法，以提高性能。通过调整对抗性Q学习，从综合的角度分析了DAQ的有限时间收敛性。在各种基准环境下，实证验证了所提出DAQ的性能。

    The goal of this paper is to propose a new Q-learning algorithm with a dummy adversarial player, which is called dummy adversarial Q-learning (DAQ), that can effectively regulate the overestimation bias in standard Q-learning. With the dummy player, the learning can be formulated as a two-player zero-sum game. The proposed DAQ unifies several Q-learning variations to control overestimation biases, such as maxmin Q-learning and minmax Q-learning (proposed in this paper) in a single framework. The proposed DAQ is a simple but effective way to suppress the overestimation bias thourgh dummy adversarial behaviors and can be easily applied to off-the-shelf reinforcement learning algorithms to improve the performances. A finite-time convergence of DAQ is analyzed from an integrated perspective by adapting an adversarial Q-learning. The performance of the suggested DAQ is empirically demonstrated under various benchmark environments.
    
[^307]: 让模型说密文: 通过嵌入进行多智能体辩论

    Let Models Speak Ciphers: Multiagent Debate through Embeddings. (arXiv:2310.06272v1 [cs.CL])

    [http://arxiv.org/abs/2310.06272](http://arxiv.org/abs/2310.06272)

    本文引入了一种名为CIPHER的通信机制，通过去除LLMs中的标记采样步骤，让模型可以通过期望的原始Transformer输出嵌入来传达其信念，从而解决了在自然语言生成中可能存在的信息丢失风险，并提供了编码更广泛信息的优势。

    

    最近，对大型语言模型（LLMs）之间的讨论和辩论引起了广泛关注，因为它们有潜力增强LLMs的推理能力。尽管自然语言由于LLMs的语言理解能力而成为明显的交流选择，但生成自然语言时需要进行的标记采样步骤可能存在信息丢失的潜在风险，因为它仅使用一个标记来代表模型在整个词汇表中的信念。在本文中，我们介绍了一种名为CIPHER（通过嵌入表示进行交流的网络模型协议）的通信机制来解决这个问题。具体来说，我们从LLMs中去除了标记采样步骤，让它们通过原始Transformer输出嵌入的期望来传达它们的信念。值得注意的是，通过偏离自然语言，CIPHER在不对模型权重进行任何修改的情况下，提供了编码更广泛信息的优势。

    Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights. While the state-of-the-a
    
[^308]: GeoLLM: 从大型语言模型中提取地理空间知识

    GeoLLM: Extracting Geospatial Knowledge from Large Language Models. (arXiv:2310.06213v1 [cs.CL])

    [http://arxiv.org/abs/2310.06213](http://arxiv.org/abs/2310.06213)

    GeoLLM是一种能够从大型语言模型中提取地理空间知识的新方法，结合来自OpenStreetMap的辅助地图数据，可以有效应用于测量人口密度和经济生计等任务。

    

    机器学习在各种地理空间任务中的应用越来越普遍，但常常依赖于全球范围可用的卫星图像等预测变量，这可能要么很昂贵，要么缺乏预测能力。本文探讨了一个问题，即互联网语言语料库中包含的大量知识是否可以利用大型语言模型（LLM）进行地理空间预测任务。我们首先证明了LLM中嵌入了有关位置的显著空间信息，但仅使用地理坐标来查询LLM对于预测人口密度等关键指标是无效的。然后，我们提出了一种名为GeoLLM的新方法，可以有效地从LLM中提取地理空间知识，并结合来自OpenStreetMap的辅助地图数据。我们展示了我们的方法在多个国际社区关心的任务中的实用性，包括人口密度和经济生计的测量。在这些任务中

    The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap. We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods. Across these task
    
[^309]: 使用大型语言模型（LLMS）对图中的节点进行无标签分类

    Label-free Node Classification on Graphs with Large Language Models (LLMS). (arXiv:2310.04668v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04668](http://arxiv.org/abs/2310.04668)

    本文介绍了一种使用大型语言模型（LLMs）对图中节点进行无标签分类的方法，即LLM-GNN。它利用LLMs对一小部分节点进行注释，然后通过对LLMs的注释进行训练，使得GNN能够对其余大部分节点进行预测。这种方法充分发挥了GNNs和LLMs的优势，同时解决了它们在处理结构化数据方面的限制。

    

    近年来，图神经网络（Graph Neural Networks，GNNs）在节点分类方面取得了显著的进展。然而，为了确保良好的性能，它们需要大量高质量的标签。相比之下，大型语言模型（Large Language Models，LLMs）在文本属性图上展现出了令人印象深刻的零样学习能力。然而，它们在高效处理结构化数据方面面临挑战，并且推理成本较高。鉴于这些观察结果，本文引入了一种基于LLMs的无标签图节点分类方法，命名为LLM-GNN。它集成了GNNs和LLMs的优势，同时减轻了它们的限制。具体而言，LLMs被用来注释一小部分节点，然后通过对LLMs的注释进行训练，使GNNs能够预测其余大部分节点。LLM-GNN的实现面临一个独特的挑战：我们如何主动选择要由LLMs注释的节点，从而增强GNN的训练？我们如何利用LLMs来优化结构化数据的处理？

    In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to ob
    
[^310]: 利用自一致性提高数据有效的摊余贝叶斯推理方法

    Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference. (arXiv:2310.04395v1 [cs.LG])

    [http://arxiv.org/abs/2310.04395](http://arxiv.org/abs/2310.04395)

    该论文提出了一种利用自一致性改进数据有效的摊余贝叶斯推理方法，通过反转贝叶斯定理并利用近似表示的联合模型估计边际似然，加速条件神经密度估计器的学习动力学。

    

    我们提出了一种方法，通过利用参数$\theta$和数据$y$的概率联合模型$p(\theta, y)$中的通用对称性，改进了摊余贝叶斯推理（ABI）的效率和准确性。简言之，我们反转贝叶斯定理，并基于近似表示的联合模型估计边际似然。在完美近似情况下，边际似然在所有参数值上都是常数定义的。然而，近似误差导致不同参数值的边际似然估计中存在不可取的方差。我们将这种对称性的违反形式化为损失函数，加速条件神经密度估计器的学习动力学。我们将我们的方法应用于具有显式似然（基于似然）的双峰玩具问题和具有隐式似然（基于模拟）的现实模型。

    We propose a method to improve the efficiency and accuracy of amortized Bayesian inference (ABI) by leveraging universal symmetries in the probabilistic joint model $p(\theta, y)$ of parameters $\theta$ and data $y$. In a nutshell, we invert Bayes' theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators. We apply our method to a bimodal toy problem with an explicit likelihood (likelihood-based) and a realistic model with an implicit likelihood (simulation-based).
    
[^311]: 利用动态推理来利用Transformer激活稀疏性

    Exploiting Transformer Activation Sparsity with Dynamic Inference. (arXiv:2310.04361v1 [cs.LG])

    [http://arxiv.org/abs/2310.04361](http://arxiv.org/abs/2310.04361)

    本文提出了一种名为DSTI的方法，通过强制激活稀疏性并将Transformer模型转换为稀疏的专家混合版本来极大地降低推理成本。此方法可以应用于任何Transformer模型，并对准确性影响微乎其微。

    

    Transformer模型尽管表现出色，但由于其高计算需求，常面临实际限制。与此同时，先前的研究揭示了这些模型中的显著激活稀疏性，表明存在冗余计算。本文提出了一种称为动态稀疏化Transformer推理（DSTI）的方法，通过强制激活稀疏性并将密集模型转换为其稀疏的专家混合（MoE）版本，从而极大地降低Transformer模型的推理成本。我们证明，在推理过程中可以训练出成功预测每个专家相对贡献的小型门控网络。此外，我们引入了一种动态确定每个令牌执行的专家数量的机制。DSTI可以应用于任何基于Transformer的体系结构，并对准确性影响微乎其微。针对BERT-base分类模型，我们降低了推理成本。

    Transformer models, despite their impressive performance, often face practical limitations due to their high computational requirements. At the same time, previous studies have revealed significant activation sparsity in these models, indicating the presence of redundant computations. In this paper, we propose Dynamic Sparsified Transformer Inference (DSTI), a method that radically reduces the inference cost of Transformer models by enforcing activation sparsity and subsequently transforming a dense model into its sparse Mixture of Experts (MoE) version. We demonstrate that it is possible to train small gating networks that successfully predict the relative contribution of each expert during inference. Furthermore, we introduce a mechanism that dynamically determines the number of executed experts individually for each token. DSTI can be applied to any Transformer-based architecture and has negligible impact on the accuracy. For the BERT-base classification model, we reduce inference c
    
[^312]: 通过参数高效适应，实现对缺失模态的鲁棒多模态学习

    Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation. (arXiv:2310.03986v1 [cs.CV])

    [http://arxiv.org/abs/2310.03986](http://arxiv.org/abs/2310.03986)

    通过低秩适应和中间特征的调制，我们提出了针对预训练多模态网络的参数高效适应程序，以实现对缺失模态的鲁棒性，并在某些情况下胜过独立的专门网络。

    

    多模态学习旨在利用多个数据源来提高下游任务的整体性能。在一些相关的模态中观察到，如果在测试时间缺少一个或多个模态，现有的多模态网络的性能会显著下降。为了实现对缺失模态的鲁棒性，我们提出了预训练的多模态网络的简单和参数高效的适应程序。特别地，我们利用低秩适应和中间特征的调制来补偿缺失的模态。我们证明，这种适应可以部分弥补由于缺失模态而导致的性能下降，并在某些情况下胜过针对可用模态组合进行训练的独立的、专门的网络。所提出的适应所需的参数非常少（例如，少于）

    Multimodal learning seeks to utilize data from multiple sources to improve the overall performance of downstream tasks. It is desirable for redundancies in the data to make multimodal systems robust to missing or corrupted observations in some correlated modalities. However, we observe that the performance of several existing multimodal networks significantly deteriorates if one or multiple modalities are absent at test time. To enable robustness to missing modalities, we propose simple and parameter-efficient adaptation procedures for pretrained multimodal networks. In particular, we exploit low-rank adaptation and modulation of intermediate features to compensate for the missing modalities. We demonstrate that such adaptation can partially bridge performance drop due to missing modalities and outperform independent, dedicated networks trained for the available modality combinations in some cases. The proposed adaptation requires extremely small number of parameters (e.g., fewer than 
    
[^313]: Ground-A-Video: 使用文本到图像扩散模型的零样本视频编辑

    Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models. (arXiv:2310.01107v1 [cs.CV])

    [http://arxiv.org/abs/2310.01107](http://arxiv.org/abs/2310.01107)

    本论文提出了一种名为 Ground-A-Video 的基于引导的视频到视频转换框架，用于多属性视频编辑。该方法在没有训练的情况下实现了输入视频的时间一致的多属性编辑，并且解决了其他方法存在的问题。

    

    最近在视频编辑领域取得了令人期待的成果，实现了单属性编辑或风格传递的任务，不论通过在文本-视频数据上训练文本到视频（T2V）模型还是采用无需训练的方法。然而，当面对多属性编辑情景的复杂性时，它们存在一些缺点，比如忽略或忽视所期望的属性变化，修改输入视频的错误元素，以及无法保留应该保持原样的输入视频区域。为解决这个问题，我们提出了一种新颖的基于引导的视频到视频转换框架，名为 Ground-A-Video，用于多属性视频编辑。Ground-A-Video以无需训练的方式实现了输入视频的时间一致的多属性编辑，并且没有上述缺点。我们方法的核心是引入了交叉帧门控注意力，以一种时间上一致的方式将定位信息融入到潜在表示中。

    Recent endeavors in video editing have showcased promising results in single-attribute editing or style transfer tasks, either by training text-to-video (T2V) models on text-video data or adopting training-free methods. However, when confronted with the complexities of multi-attribute editing scenarios, they exhibit shortcomings such as omitting or overlooking intended attribute changes, modifying the wrong elements of the input video, and failing to preserve regions of the input video that should remain intact. To address this, here we present a novel grounding-guided video-to-video translation framework called Ground-A-Video for multi-attribute video editing. Ground-A-Video attains temporally consistent multi-attribute editing of input videos in a training-free manner without aforementioned shortcomings. Central to our method is the introduction of Cross-Frame Gated Attention which incorporates groundings information into the latent representations in a temporally consistent fashion,
    
[^314]: 超越熟悉特征的深度异常检测

    Going Beyond Familiar Features for Deep Anomaly Detection. (arXiv:2310.00797v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00797](http://arxiv.org/abs/2310.00797)

    该论文提出了一种超越熟悉特征的深度异常检测方法，通过利用可解释性在输入空间中捕捉新颖特征来避免假阴性。该方法在广泛的异常基准测试中取得了强大的性能，并且消除了昂贵的背景模型和密集匹配的需求。

    

    异常检测（AD）是一项重要任务，涉及识别不符合已学习的正常模型的观察结果。之前的深度AD工作主要基于熟悉性假设，在预训练的嵌入空间中使用熟悉特征作为参考。虽然这种策略已被证明非常成功，但实际上，在异常包含未被预训练编码很好捕捉到的全新特征时，它会导致持续的假阴性。我们提出了一种新颖的AD方法，利用可解释性在输入空间中捕捉新颖特征作为未解释的观察结果。通过在混合方法中结合相似性和新颖性，我们在广泛的异常基准测试中取得了很强的性能。我们的方法在多个基准测试中建立了新的最新技术，处理多样化的异常类型，同时消除了昂贵的背景模型和密集匹配的需求。特别地，我们展示了通过考虑新颖特征，我们的方法能够解决先前的假阴性问题。

    Anomaly Detection (AD) is a critical task that involves identifying observations that do not conform to a learned model of normality. Prior work in deep AD is predominantly based on a familiarity hypothesis, where familiar features serve as the reference in a pre-trained embedding space. While this strategy has proven highly successful, it turns out that it causes consistent false negatives when anomalies consist of truly novel features that are not well captured by the pre-trained encoding. We propose a novel approach to AD using explainability to capture novel features as unexplained observations in the input space. We achieve strong performance across a wide range of anomaly benchmarks by combining similarity and novelty in a hybrid approach. Our approach establishes a new state-of-the-art across multiple benchmarks, handling diverse anomaly types while eliminating the need for expensive background models and dense matching. In particular, we show that by taking account of novel fea
    
[^315]: 使用合成数据进行预训练有助于离线强化学习

    Pre-training with Synthetic Data Helps Offline Reinforcement Learning. (arXiv:2310.00771v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.00771](http://arxiv.org/abs/2310.00771)

    本文研究表明，在离线深度强化学习中，使用合成数据进行预训练可以提高性能，而不一定需要语言预训练。此外，使用一步马尔科夫链生成的数据进行预训练可进一步改善性能。在一个流行的离线DRL算法中，使用简单的预训练方案也能获得性能提升。

    

    最近的研究表明，对于离线深度强化学习(DRL)，使用大型语言语料库预训练Decision Transformer可以提高下游性能。一个自然的问题是，这种性能提升是否只能通过语言预训练实现，还是可以通过不涉及语言的更简单的预训练方案实现。在本文中，我们首先证明了语言对于改善性能并不是必要的，实际上，使用合成的IID数据进行少量更新的预训练可以达到与使用大型语言语料库预训练相匹配的性能提升；此外，使用一步马尔科夫链生成的数据进行预训练可以进一步提高性能。受到这些实验结果的启发，我们进一步考虑了预训练Conservative Q-Learning(CQL)，这是一种流行的离线DRL算法，它基于Q-learning，并通常使用多层感知器(MLP)骨干。令人惊讶的是，使用简单的预训练方案也能在CQL算法中取得性能提升。

    Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with sim
    
[^316]: 保序GFlowNets

    Order-Preserving GFlowNets. (arXiv:2310.00386v1 [cs.LG])

    [http://arxiv.org/abs/2310.00386](http://arxiv.org/abs/2310.00386)

    本研究提出了保序GFlowNets（OP-GFNs），通过学习奖励函数与候选者的排序相一致的概率进行采样，解决了使用预定义标量奖励的局限性，同时提供了证明训练过程稀疏奖励景观的理论支持。

    

    生成流网络（GFlowNets）被引入作为一种根据给定奖励概率采样多样化的候选集的方法。然而，GFlowNets只能与预定义的标量奖励一起使用，在多目标优化（MOO）任务中，这可能是计算昂贵的或者直接不可访问的。此外，为了优先识别高奖励候选者，传统做法是将奖励提高到更高的指数，而这个最优选择在不同环境下可能会有所不同。为了解决这些问题，我们提出了保序GFlowNets（OP-GFNs），它们以与提供的（部分）候选者排序一致的学习奖励函数的概率进行采样，从而消除了对奖励函数的显式表达的需求。我们在理论上证明了OP-GFNs的训练过程逐渐稀疏了学习到的奖励景观。

    Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates with probabilities proportional to a given reward. However, GFlowNets can only be used with a predefined scalar reward, which can be either computationally expensive or not directly accessible, in the case of multi-objective optimization (MOO) tasks for example. Moreover, to prioritize identifying high-reward candidates, the conventional practice is to raise the reward to a higher exponent, the optimal choice of which may vary across different environments. To address these issues, we propose Order-Preserving GFlowNets (OP-GFNs), which sample with probabilities in proportion to a learned reward function that is consistent with a provided (partial) order on the candidates, thus eliminating the need for an explicit formulation of the reward function. We theoretically prove that the training process of OP-GFNs gradually sparsifies the learned reward landscape in single-objective max
    
[^317]: 一种用于医学图像中一般移动目标分割的基础模型

    A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])

    [http://arxiv.org/abs/2309.17264](http://arxiv.org/abs/2309.17264)

    本文提出了一种用于医学图像中移动目标分割的基础模型iMOS，通过对序列中只有少量图像进行注释，即可实现高精度的分割效果

    

    医学图像分割旨在描绘感兴趣的解剖或病理结构，在临床诊断中起着关键作用。构建高精度的深度分割模型需要大量高质量的注释数据。然而，医学注释非常繁琐耗时，特别是对于医学视频或3D体积，由于巨大的标签空间和差的帧间一致性。最近，在自然图像中，一个名为Moving Object Segmentation (MOS)的基本任务在技术上取得了重大进展。它的目标是在图像序列中从背景中描绘移动物体，只需要最小的注释。在本文中，我们提出了第一个用于医学图像中MOS的基础模型，名为iMOS。对一个大规模多模态医学数据集进行的大量实验验证了所提出的iMOS的有效性。具体而言，只需对序列中少量的图像进行注释，iMOS就可以实现了

    Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve sati
    
[^318]: Transformer-VQ: 基于向量量化实现线性时间的Transformer

    Transformer-VQ: Linear-Time Transformers via Vector Quantization. (arXiv:2309.16354v1 [cs.LG])

    [http://arxiv.org/abs/2309.16354](http://arxiv.org/abs/2309.16354)

    Transformer-VQ是一种基于向量量化实现线性时间的Transformer模型，能够高效计算自注意力，在大规模实验中表现出色。

    

    我们引入了Transformer-VQ，一种仅编码器的Transformer，能够在线性时间内计算基于softmax的密集自注意力。Transformer-VQ的高效注意力是通过向量量化键和一种新颖的缓存机制实现的。在大规模实验中，Transformer-VQ在质量上表现出色，Enwik8(0.99 bpb)，PG-19(26.6 ppl)和ImageNet64(3.16 bpb)都取得了很好的结果。

    We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer_vq
    
[^319]: 统计角度下的前K稀疏Softmax门控混合专家

    Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts. (arXiv:2309.13850v1 [stat.ML])

    [http://arxiv.org/abs/2309.13850](http://arxiv.org/abs/2309.13850)

    该论文研究前K稀疏softmax门控混合专家在密度和参数估计方面的作用，通过定义新的损失函数，探讨了输入区域的不同行为。研究发现，在真实专家数量已知的情况下，密度和参数估计的收敛速度与样本量成正比，但当真实模式未知时

    

    前K稀疏softmax门控混合专家被广泛用于在不增加计算成本的情况下扩展大规模深度学习架构。尽管在现实应用中非常受欢迎，但对该门控函数的理论理解仍然是一个未解决的问题。主要挑战来自于前K稀疏softmax门控函数的结构，它将输入空间划分为具有不同行为的多个区域。通过专注于高斯混合专家，我们对前K稀疏softmax门控函数对密度和参数估计的影响建立了理论结果。我们的结果依赖于定义参数之间的新损失函数，以捕捉输入区域的不同行为。当真实专家数量$k_{\ast}$已知时，我们证明了密度和参数估计的收敛速度都与样本量成正比。然而，当$k_{\ast}$变为未知且真实模式时

    Top-K sparse softmax gating mixture of experts has been widely used for scaling up massive deep-learning architectures without increasing the computational cost. Despite its popularity in real-world applications, the theoretical understanding of that gating function has remained an open problem. The main challenge comes from the structure of the top-K sparse softmax gating function, which partitions the input space into multiple regions with distinct behaviors. By focusing on a Gaussian mixture of experts, we establish theoretical results on the effects of the top-K sparse softmax gating function on both density and parameter estimations. Our results hinge upon defining novel loss functions among parameters to capture different behaviors of the input regions. When the true number of experts $k_{\ast}$ is known, we demonstrate that the convergence rates of density and parameter estimations are both parametric on the sample size. However, when $k_{\ast}$ becomes unknown and the true mode
    
[^320]: 在复杂医疗决策中重新思考人工智能与人类的合作：以脓毒症诊断为案例研究

    Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis. (arXiv:2309.12368v1 [cs.HC])

    [http://arxiv.org/abs/2309.12368](http://arxiv.org/abs/2309.12368)

    本研究探索了在复杂医疗决策中重新思考人工智能与人类合作的设计要求，以脓毒症诊断为例。研究发现，在人工智能系统中，支持临床专家在决策过程的中间阶段发挥作用（如生成假设或收集数据）是至关重要的，而不仅仅关注最终决策。

    

    如今的医疗决策支持人工智能系统在研究论文中取得了成功，但在实际部署中却面临失败的问题。本研究聚焦于脓毒症的决策过程，这是一种需要临床医生早期高度不确定性诊断的急性致命全身性感染。我们的目标是探索能够支持临床专家做出更好脓毒症早期诊断决策的人工智能系统的设计要求。研究从一个形成性研究开始，调查为什么临床专家在电子病历系统中放弃了一个现有的脓毒症预测模块。我们认为，一个以人为中心的人工智能系统需要在医疗决策过程的中间阶段（如生成假设或收集数据）支持人类专家，而不仅仅关注最终决策。因此，我们基于先进的人工智能算法构建了SepsisLab，并将其扩展到预测未来趋势。

    Today's AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module in their electrical health record (EHR) system. We argue that a human-centered AI system needs to support human experts in the intermediate stages of a medical decision-making process (e.g., generating hypotheses or gathering data), instead of focusing only on the final decision. Therefore, we build SepsisLab based on a state-of-the-art AI algorithm and extend it to predict the future projection o
    
[^321]: 卷积深度核机器

    Convolutional Deep Kernel Machines. (arXiv:2309.09814v1 [stat.ML])

    [http://arxiv.org/abs/2309.09814](http://arxiv.org/abs/2309.09814)

    这项研究介绍了一种称为卷积深度核机器的新型核方法，该方法纯粹使用核而不使用特征，通过高效的跨域诱导点近似方案和多种模型变体的设计，达到了在MNIST、CIFAR-10和CIFAR-100上接近甚至超过其他方法的测试准确率。

    

    深度核机器(DKMs)是一种最近引入的具有其他深度模型灵活性的核方法，包括深度神经网络和深度高斯过程。DKMs纯粹使用核，而不使用特征，因此与其他方法（从神经网络到深度核学习甚至深度高斯过程）不同，后者都使用特征作为基本组成部分。在这里，我们引入了卷积DKMs，并配以一种高效的跨域诱导点近似方案。此外，我们还开发并实验评估了许多模型变体，包括9种不同类型的为卷积DKMs设计的归一化方法，两种似然函数和两种不同类型的顶层。尽管只在约28个GPU小时内训练（比完全的NNGP / NTK / Myrtle kernel快1-2个数量级），但得到的模型在MNIST上实现了约99％的测试准确性，在CIFAR-10上为92％，在CIFAR-100上为71％，同时达到可比较的性能。

    Deep kernel machines (DKMs) are a recently introduced kernel method with the flexibility of other deep models including deep NNs and deep Gaussian processes. DKMs work purely with kernels, never with features, and are therefore different from other methods ranging from NNs to deep kernel learning and even deep Gaussian processes, which all use features as a fundamental component. Here, we introduce convolutional DKMs, along with an efficient inter-domain inducing point approximation scheme. Further, we develop and experimentally assess a number of model variants, including 9 different types of normalisation designed for the convolutional DKMs, two likelihoods, and two different types of top-layer. The resulting models achieve around 99% test accuracy on MNIST, 92% on CIFAR-10 and 71% on CIFAR-100, despite training in only around 28 GPU hours, 1-2 orders of magnitude faster than full NNGP / NTK / Myrtle kernels, whilst achieving comparable performance.
    
[^322]: 对比初始状态缓冲区在强化学习中的应用

    Contrastive Initial State Buffer for Reinforcement Learning. (arXiv:2309.09752v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.09752](http://arxiv.org/abs/2309.09752)

    本论文提出了对比初始状态缓冲区的概念，它通过选择过去的经验中的状态来初始化环境中的代理，以引导其进入更有信息量的状态。实验证明，该方法在两个复杂机器人任务上取得了更高的任务性能并加速了训练过程。

    

    在强化学习中，勘探与利用之间的平衡给从有限的样本中实现高效学习带来了复杂的挑战。虽然最近的工作在利用过去的经验进行策略更新方面是有效的，但它们常常忽视了重新利用过去经验进行数据收集的潜力。独立于基本强化学习算法，我们引入了对比初始状态缓冲区的概念，它从过去的经验中选择状态，并用这些状态初始化环境中的代理，以引导它走向更具信息量的状态。我们在两个复杂的机器人任务上验证了我们的方法，而不依赖任何关于环境的先验信息：（i）四足机器人穿越具有挑战性的地形和（ii）四旋翼无人机在赛道上飞行。实验结果表明，我们的初始状态缓冲区在任务性能上优于基准线，同时还加速了训练过程。

    In Reinforcement Learning, the trade-off between exploration and exploitation poses a complex challenge for achieving efficient learning from limited samples. While recent works have been effective in leveraging past experiences for policy updates, they often overlook the potential of reusing past experiences for data collection. Independent of the underlying RL algorithm, we introduce the concept of a Contrastive Initial State Buffer, which strategically selects states from past experiences and uses them to initialize the agent in the environment in order to guide it toward more informative states. We validate our approach on two complex robotic tasks without relying on any prior information about the environment: (i) locomotion of a quadruped robot traversing challenging terrains and (ii) a quadcopter drone racing through a track. The experimental results show that our initial state buffer achieves higher task performance than the nominal baseline while also speeding up training conv
    
[^323]: RaTrack: 带有4D雷达点云的运动物体检测与跟踪

    RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud. (arXiv:2309.09737v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.09737](http://arxiv.org/abs/2309.09737)

    RaTrack是一种针对雷达跟踪的创新解决方案，通过运动分割和聚类以及运动估计模块，实现了对移动物体的精确跟踪，优于最先进性能。

    

    移动自主性依赖于对动态环境的精确感知。在3D世界中稳定地跟踪移动物体因此对于轨迹预测、避障和路径规划等应用起着关键作用。虽然大多数现有方法利用LiDAR或相机进行多目标跟踪（MOT），但4D成像雷达的能力仍然很少被探索。认识到4D雷达数据中的雷达噪声和点稀疏性所带来的挑战，我们介绍了RaTrack，这是一种专门针对基于雷达的跟踪的创新解决方案。我们的方法摒弃了对特定对象类型和3D边界框的依赖，而是专注于运动分割和聚类，并配以运动估计模块。在View-of-Delft数据集上进行评估时，RaTrack展示出了优于最先进性能的运动物体跟踪精度。

    Mobile autonomy relies on the precise perception of dynamic environments. Robustly tracking moving objects in 3D world thus plays a pivotal role for applications like trajectory prediction, obstacle avoidance, and path planning. While most current methods utilize LiDARs or cameras for Multiple Object Tracking (MOT), the capabilities of 4D imaging radars remain largely unexplored. Recognizing the challenges posed by radar noise and point sparsity in 4D radar data, we introduce RaTrack, an innovative solution tailored for radar-based tracking. Bypassing the typical reliance on specific object types and 3D bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. Evaluated on the View-of-Delft dataset, RaTrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art.
    
[^324]: 自主驾驶车辆的强化学习策略的定量和定性评估

    Quantitative and Qualitative Evaluation of Reinforcement Learning Policies for Autonomous Vehicles. (arXiv:2309.08254v1 [cs.AI])

    [http://arxiv.org/abs/2309.08254](http://arxiv.org/abs/2309.08254)

    本文使用强化学习算法（PPO）针对自主驾驶车辆的选择进行了优化，通过最小化时间和污染来缓解交通阻塞问题，经实证分析和定性评估证明了方法的有效性和实用性。

    

    在不断变化的交通环境中优化交通动力学非常重要，特别是在自动驾驶车辆（AVs）与人驾驶车辆并存的情况下。本文提出了一种使用近端策略优化（PPO）强化学习算法来优化AVs选择的新方法。我们通过学习一种策略来最小化交通阻塞（即最小化横过米兰的环形道的时间）并减少污染。通过经验分析，我们证明了我们的方法可以减少时间和污染水平。此外，我们使用先进的驾驶舱定性评估了学到的策略，以评估其在接近真实世界条件下的性能。为了评估策略的实用性和可接受性，我们通过模拟器进行了人类参与者的评估，重点关注交通平稳性和安全感等一系列指标。总的来说，我们的研究结果表明，人驾驶车辆的感知和行车平滑性方面，我们的方法非常实用。

    Optimizing traffic dynamics in an evolving transportation landscape is crucial, particularly in scenarios where autonomous vehicles (AVs) with varying levels of autonomy coexist with human-driven cars. This paper presents a novel approach to optimizing choices of AVs using Proximal Policy Optimization (PPO), a reinforcement learning algorithm. We learned a policy to minimize traffic jams (i.e., minimize the time to cross the scenario) and to minimize pollution in a roundabout in Milan, Italy. Through empirical analysis, we demonstrate that our approach can reduce time and pollution levels. Furthermore, we qualitatively evaluate the learned policy using a cutting-edge cockpit to assess its performance in near-real-world conditions. To gauge the practicality and acceptability of the policy, we conducted evaluations with human participants using the simulator, focusing on a range of metrics like traffic smoothness and safety perception. In general, our findings show that human-driven vehi
    
[^325]: CaloClouds II: 超快速几何独立的高分辨率量能器模拟

    CaloClouds II: Ultra-Fast Geometry-Independent Highly-Granular Calorimeter Simulation. (arXiv:2309.05704v1 [physics.ins-det])

    [http://arxiv.org/abs/2309.05704](http://arxiv.org/abs/2309.05704)

    CaloClouds II是一个超快速几何独立的高分辨率量能器模拟，通过连续时间得分建模，相比传统模拟更快且具有可比的保真度。

    

    高分辨率探测器的能量沉积的快速模拟对于未来具有不断增加亮度的对撞机实验至关重要。生成式机器学习（ML）模型已经被证明可以加快和增强物理分析中的传统模拟链。然而，大多数以前的努力局限于依赖于固定、规则的探测器读出几何的模型。最近提出的CaloClouds模型是一个几何独立的扩散模型，为设想中的国际大型探测器（ILD）的电磁量能器生成块状点云。在这项工作中，我们介绍了CaloClouds II，它具有一些关键的改进。这包括基于连续时间得分的建模，它允许进行25步采样，与CaloClouds的保真度相当，同时在单个CPU上比Geant4快6倍（比CaloClouds快5倍）。我们进一步将扩散模型提炼成一种新的模型。

    Fast simulation of the energy depositions in high-granular detectors is needed for future collider experiments with ever increasing luminosities. Generative machine learning (ML) models have been shown to speed up and augment the traditional simulation chain in physics analysis. However, the majority of previous efforts were limited to models relying on fixed, regular detector readout geometries. A major advancement is the recently introduced CaloClouds model, a geometry-independent diffusion model, which generates calorimeter showers as point clouds for the electromagnetic calorimeter of the envisioned International Large Detector (ILD).  In this work, we introduce CaloClouds II which features a number of key improvements. This includes continuous time score-based modelling, which allows for a 25 step sampling with comparable fidelity to CaloClouds while yielding a $6\times$ speed-up over Geant4 on a single CPU ($5\times$ over CaloClouds). We further distill the diffusion model into a
    
[^326]: 渐变优化和变分不等式在机器学习中的温和介绍

    A Gentle Introduction to Gradient-Based Optimization and Variational Inequalities for Machine Learning. (arXiv:2309.04877v1 [cs.LG])

    [http://arxiv.org/abs/2309.04877](http://arxiv.org/abs/2309.04877)

    这篇论文介绍了渐变优化和变分不等式在机器学习中的应用，强调了从模式识别到决策和多智能体问题的转变，以及涉及均衡和博弈论的数学挑战，提供了一些算法的收敛性证明，但主要关注于提供动机和直观理解。

    

    近年来机器学习的快速发展基于与渐变优化的紧密联系。进一步的进展部分取决于从模式识别到决策和多智能体问题的转变。在这些更广泛的背景下，涉及均衡和博弈论而不是极值的新的数学挑战出现了。基于梯度的方法仍然至关重要--考虑到机器学习问题的高维度和大规模--但简单的梯度下降不再是算法设计的出发点。我们提供了一个对机器学习中基于梯度的算法的更广泛框架的温和介绍，从鞍点和单调博弈开始，然后到一般的变分不等式。虽然我们对所提出的几个算法进行了收敛性证明，但我们的主要关注点是提供动机和直观理解。

    The rapid progress in machine learning in recent years has been based on a highly productive connection to gradient-based optimization. Further progress hinges in part on a shift in focus from pattern recognition to decision-making and multi-agent problems. In these broader settings, new mathematical challenges emerge that involve equilibria and game theory instead of optima. Gradient-based methods remain essential -- given the high dimensionality and large scale of machine-learning problems -- but simple gradient descent is no longer the point of departure for algorithm design. We provide a gentle introduction to a broader framework for gradient-based algorithms in machine learning, beginning with saddle points and monotone games, and proceeding to general variational inequalities. While we provide convergence proofs for several of the algorithms that we present, our main focus is that of providing motivation and intuition.
    
[^327]: 图神经网络在不需要的时候仍然使用图形信息

    Graph Neural Networks Use Graphs When They Shouldn't. (arXiv:2309.04332v1 [cs.LG])

    [http://arxiv.org/abs/2309.04332](http://arxiv.org/abs/2309.04332)

    在图形预测问题中，GNNs倾向于过拟合图结构，即使在忽略图结构的情况下可以获得更好的解决方案。常规图对于这种过拟合更具鲁棒性。

    

    在各个领域中，包括社交网络、分子生物学、医学等，对图形进行预测起着至关重要的作用。图神经网络(GNNs)已成为学习图数据的主要方法。图形标注问题的实例包括图结构(即邻接矩阵)和节点特定的特征向量。在某些情况下，这种图结构对于预测任务来说并不具有信息量。例如，分子性质如摩尔质量仅依赖于组成原子(节点特征)，而与分子结构无关。尽管GNNs有能力在这种情况下忽略图结构，但不清楚它们是否会这样做。在这项工作中，我们展示了GNNs实际上倾向于在过拟合图结构，即在忽略它可以获得更好解决方案的情况下仍在使用。我们根据不同的图分布来研究这种现象，发现常规图对这种过拟合更加稳健。

    Predictions over graphs play a crucial role in various domains, including social networks, molecular biology, medicine, and more. Graph Neural Networks (GNNs) have emerged as the dominant approach for learning on graph data. Instances of graph labeling problems consist of the graph-structure (i.e., the adjacency matrix), along with node-specific feature vectors. In some cases, this graph-structure is non-informative for the predictive task. For instance, molecular properties such as molar mass depend solely on the constituent atoms (node features), and not on the molecular structure. While GNNs have the ability to ignore the graph-structure in such cases, it is not clear that they will. In this work, we show that GNNs actually tend to overfit the graph-structure in the sense that they use it even when a better solution can be obtained by ignoring it. We examine this phenomenon with respect to different graph distributions and find that regular graphs are more robust to this overfitting
    
[^328]: 一种用于药物发现的混合量子-经典融合神经网络以提高蛋白质-配体结合亲和力预测的准确性

    A hybrid quantum-classical fusion neural network to improve protein-ligand binding affinity predictions for drug discovery. (arXiv:2309.03919v1 [quant-ph])

    [http://arxiv.org/abs/2309.03919](http://arxiv.org/abs/2309.03919)

    提出了一种用于药物发现的混合量子-经典融合神经网络模型，通过优化的量子架构将3D和空间图卷积神经网络相互整合，提高了结合亲和力预测的准确性。

    

    药物发现领域关键在于准确预测潜在药物分子与靶蛋白之间的结合亲和力，特别是当这些蛋白直接影响疾病的进展时。然而，估计结合亲和力需要显著的财务和计算资源。虽然最先进的方法使用经典机器学习技术，但新兴的混合量子机器学习模型显示出更好的性能，这归功于它们固有的并行性和管理数据维度指数级增加的能力。尽管有这些进展，现有模型在收敛稳定性和预测准确性方面存在问题。本文介绍了一种新颖的混合量子-经典深度学习模型，用于药物发现中的结合亲和力预测。

    The field of drug discovery hinges on the accurate prediction of binding affinity between prospective drug molecules and target proteins, especially when such proteins directly influence disease progression. However, estimating binding affinity demands significant financial and computational resources. While state-of-the-art methodologies employ classical machine learning (ML) techniques, emerging hybrid quantum machine learning (QML) models have shown promise for enhanced performance, owing to their inherent parallelism and capacity to manage exponential increases in data dimensionality. Despite these advances, existing models encounter issues related to convergence stability and prediction accuracy. This paper introduces a novel hybrid quantum-classical deep learning model tailored for binding affinity prediction in drug discovery. Specifically, the proposed model synergistically integrates 3D and spatial graph convolutional neural networks within an optimized quantum architecture. S
    
[^329]: 混合方差流用于离散变量

    Mixed Variational Flows for Discrete Variables. (arXiv:2308.15613v1 [stat.CO])

    [http://arxiv.org/abs/2308.15613](http://arxiv.org/abs/2308.15613)

    本文提出了一种混合方差流方法，用于近似离散分布，通过开发一个离散且保持度量的映射，而不需要连续嵌入。实验证明，与连续嵌入流相比，该方法产生更可靠的近似。

    

    变分流允许从事者学习复杂的连续分布，但是近似离散分布仍然是一个挑战。目前的方法通常将离散目标嵌入连续空间中-通常是通过连续松弛或去量化-然后应用连续流动。这些方法涉及一个可能无法捕捉到原始离散目标的替代目标，可能具有偏倚或不稳定的梯度，并且可能会创建一个困难的优化问题。在这项工作中，我们开发了一种针对离散分布的变分流族，而不需要任何连续嵌入。首先，我们开发了一个保持度量的离散可逆映射，使离散目标保持不变，然后基于该映射创建了一个混合变分流(MAD Mix)。我们还开发了一个扩展，用于处理联合离散和连续模型。我们的实验表明，MAD Mix产生了比连续嵌入流更可靠的近似。

    Variational flows allow practitioners to learn complex continuous distributions, but approximating discrete distributions remains a challenge. Current methodologies typically embed the discrete target in a continuous space - usually via continuous relaxation or dequantization - and then apply a continuous flow. These approaches involve a surrogate target that may not capture the original discrete target, might have biased or unstable gradients, and can create a difficult optimization problem. In this work, we develop a variational flow family for discrete distributions without any continuous embedding. First, we develop a measure-preserving and discrete (MAD) invertible map that leaves the discrete target invariant, and then create a mixed variational flow (MAD Mix) based on that map. We also develop an extension to MAD Mix that handles joint discrete and continuous models. Our experiments suggest that MAD Mix produces more reliable approximations than continuous-embedding flows while 
    
[^330]: 一个关于校准的基准研究

    A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])

    [http://arxiv.org/abs/2308.11838](http://arxiv.org/abs/2308.11838)

    这项研究提出了一个模型校准的基准研究，利用神经架构搜索空间探索了模型校准属性。研究结果显示，模型校准可以在不同任务中泛化，并可以同时兼顾模型的准确性和校准性能。

    

    深度神经网络在各种机器学习任务中的应用越来越广泛。然而，随着这些模型复杂性的增加，它们往往面临校准问题，尽管预测准确性有所提高。许多研究通过数据预处理、使用特定损失函数和训练框架来改善校准性能。然而，对校准属性的研究有点被忽视了。我们的研究利用神经架构搜索（NAS）搜索空间，在全面探索校准属性的模型架构空间中提供了一个详尽的模型架构空间。我们特别创建了一个模型校准数据集。该数据集在广泛使用的NATS-Bench搜索空间中评估了90个基于区间的校准度量和12个其他校准度量，涵盖了117,702个独特的神经网络。我们的分析旨在通过我们提出的数据集回答该领域一些长期存在的问题：（i）模型校准能否在不同任务中泛化？（ii）能否同时兼顾模型的准确性和校准性能？

    Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
    
[^331]: 两个列表什么时候比一个列表更好？合作决策中的益处和伤害

    When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making. (arXiv:2308.11721v1 [cs.LG])

    [http://arxiv.org/abs/2308.11721](http://arxiv.org/abs/2308.11721)

    这项研究分析了一种特定类型的人工和算法合作，对于多个噪音模型来说，将选择项目的子集大小$k$设置在$[2, n-1]$范围内能够最大化最终选择最佳项目的概率。

    

    在过去的机器学习研究中，很大一部分关注的是算法的性能，但最近更多地关注于优化人工和算法的联合性能。在这里，我们分析了一种特定类型的人工和算法合作，在这种合作中，算法可以访问一组n个项目，并将大小为k的一个子集呈现给人类，然后人类从这些k个项目中选择一个最终项目。这种情况可以模拟内容推荐、路径规划或任何类型的标注任务。由于人类和算法都对项目的真实排序有着不完美、有噪音的信息，关键问题是：哪个$k$值能最大化最终选择最佳项目的概率？对于$k=1$，算法单独行动时性能最优，而对于$k=n$，人类单独行动时性能最优。令人惊讶的是，我们发现对于多个噪音模型，将$k$设置在$[2, n-1]$范围内是最优的，也就是说，合作有明显的益处。

    Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of $n$ items, and presents a subset of size $k$ to the human, who selects a final item from among those $k$. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of $k$ maximizes the probability that the best item will be ultimately selected? For $k=1$, performance is optimized by the algorithm acting alone, and for $k=n$ it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set $k \in [2, n-1]$ - that is, there are strict benefits to collaborating,
    
[^332]: 基于内部跨层梯度的联邦学习中的同质性到异质性的扩展

    Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning. (arXiv:2308.11464v1 [cs.LG])

    [http://arxiv.org/abs/2308.11464](http://arxiv.org/abs/2308.11464)

    提出了一种基于内部跨层梯度的联邦学习方法，通过混合浅层和深层的梯度，增强了深层的相似性，从而扩展了在处理系统异质性方面的能力。

    

    联邦学习（FL）在实际场景中不可避免地面临系统异质性的挑战。为了增强大多数模型同质性FL方法处理系统异质性的能力，我们提出了一种训练方案，可以扩展它们应对这一挑战的能力。我们在本文中从详细探索同质性和异质性FL设置开始，发现了三个关键观察结果：（1）客户端性能与层之间的相似性呈正相关，（2）浅层比深层具有更高的相似性，（3）较为平滑的梯度分布指示了更高的层相似性。基于这些观察结果，我们提出了InCo Aggregation方法，利用内部跨层梯度，即服务器模型中来自浅层和深层的梯度混合，以增强深层的相似性，而无需额外的客户端通信。

    Federated learning (FL) inevitably confronts the challenge of system heterogeneity in practical scenarios. To enhance the capabilities of most model-homogeneous FL methods in handling system heterogeneity, we propose a training scheme that can extend their capabilities to cope with this challenge. In this paper, we commence our study with a detailed exploration of homogeneous and heterogeneous FL settings and discover three key observations: (1) a positive correlation between client performance and layer similarities, (2) higher similarities in the shallow layers in contrast to the deep layers, and (3) the smoother gradients distributions indicate the higher layer similarities. Building upon these observations, we propose InCo Aggregation that leverags internal cross-layer gradients, a mixture of gradients from shallow and deep layers within a server model, to augment the similarity in the deep layers without requiring additional communication between clients. Furthermore, our methods 
    
[^333]: ALI-DPFL: 具有自适应本地迭代的差分隐私联邦学习

    ALI-DPFL: Differentially Private Federated Learning with Adaptive Local Iterations. (arXiv:2308.10457v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10457](http://arxiv.org/abs/2308.10457)

    ALI-DPFL是一种进行差分隐私联邦学习的算法，通过自适应本地迭代来优化性能，并在实验中展示了显著的改进。

    

    联邦学习是一种分布式机器学习技术，通过共享训练参数而不是原始数据，允许多个设备或组织之间进行模型训练。然而，攻击者仍然可以通过对这些训练参数的推理攻击（例如差分攻击）来推断个体信息。因此，差分隐私被广泛应用于联邦学习中以防止此类攻击。我们在资源受限的场景中考虑差分隐私联邦学习，其中既有隐私预算受限，又有通信轮次受限。通过理论分析收敛性，我们可以找到在任意两个顺序全局更新之间的客户机之间的最佳差分隐私本地迭代次数。基于此，我们设计了一种具有自适应本地迭代的差分隐私联邦学习算法（ALI-DPFL）。我们在FashionMNIST和CIFAR10数据集上对我们的算法进行实验，并展示了显著更好的性能。

    Federated Learning (FL) is a distributed machine learning technique that allows model training among multiple devices or organizations by sharing training parameters instead of raw data. However, adversaries can still infer individual information through inference attacks (e.g. differential attacks) on these training parameters. As a result, Differential Privacy (DP) has been widely used in FL to prevent such attacks. We consider differentially private federated learning in a resource-constrained scenario, where both privacy budget and communication round are constrained. By theoretically analyzing the convergence, we can find the optimal number of differentially private local iterations for clients between any two sequential global updates. Based on this, we design an algorithm of differentially private federated learning with adaptive local iterations (ALI-DPFL). We experiment our algorithm on the FashionMNIST and CIFAR10 datasets, and demonstrate significantly better performances th
    
[^334]: 用于支持地下不确定性量化和解释的稳定低维空间刚性变换

    Rigid Transformations for Stabilized Lower Dimensional Space to Support Subsurface Uncertainty Quantification and Interpretation. (arXiv:2308.08079v1 [cs.LG])

    [http://arxiv.org/abs/2308.08079](http://arxiv.org/abs/2308.08079)

    该论文介绍了一种用于地下数据集的稳定降维方法，通过刚性变换实现了欧几里德不变表示，能够量化地下数据的不确定性并且适应外样本点的扩展。

    

    地下数据集天然地具有大数据特征，如庞大的体积、多样的特征和高速采样速度，受到各种物理、工程和地质输入引起的维数诅咒的影响。在现有的降维方法中，非线性降维方法，特别是度量多维缩放（MDS），是地下数据集中首选的方法，因为它们具有固有的复杂性。虽然MDS保留了内在的数据结构和不确定性的量化，但其局限性包括不稳定的唯一解，不变于欧几里德变换，并且没有外样本点（OOSP）扩展。为了增强地下推理和机器学习工作流程，必须将数据集转化为稳定的、降维的表示，以容纳OOSP。我们的解决方案利用刚性变换实现了LDS的稳定欧几里德不变表示。通过计算MDS输入的不相似度，

    Subsurface datasets inherently possess big data characteristics such as vast volume, diverse features, and high sampling speeds, further compounded by the curse of dimensionality from various physical, engineering, and geological inputs. Among the existing dimensionality reduction (DR) methods, nonlinear dimensionality reduction (NDR) methods, especially Metric-multidimensional scaling (MDS), are preferred for subsurface datasets due to their inherent complexity. While MDS retains intrinsic data structure and quantifies uncertainty, its limitations include unstabilized unique solutions invariant to Euclidean transformations and an absence of out-of-sample points (OOSP) extension. To enhance subsurface inferential and machine learning workflows, datasets must be transformed into stable, reduced-dimension representations that accommodate OOSP.  Our solution employs rigid transformations for a stabilized Euclidean invariant representation for LDS. By computing an MDS input dissimilarity m
    
[^335]: 领域感知微调：增强神经网络的适应性能力

    Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability. (arXiv:2308.07728v1 [cs.LG])

    [http://arxiv.org/abs/2308.07728](http://arxiv.org/abs/2308.07728)

    本文提出了领域感知微调（DAFT）方法，通过批归一化转换和线性探测与微调的集成，有效减轻微调过程中的特征畸变问题。

    

    微调预训练神经网络模型已成为各个领域广泛采用的方法。然而，微调可能导致已具备强大泛化能力的预训练特征提取器发生畸变。在适应新目标领域时减轻特征畸变至关重要。最近的研究表明，在进行微调之前，在分布数据集上对头层进行对齐处理可以处理特征畸变问题取得有希望的结果。然而，在微调过程中，批归一化层的处理存在显著局限性，导致性能不佳。在本文中，我们提出了领域感知微调（DAFT），一种新的方法，它结合了批归一化转换、线性探测和微调的特性。我们的批归一化转换方法通过减少对神经网络的修改来有效减轻特征畸变。此外，我们还引入了线性探测和微调的集成方法。

    Fine-tuning pre-trained neural network models has become a widely adopted approach across various domains. However, it can lead to the distortion of pre-trained feature extractors that already possess strong generalization capabilities. Mitigating feature distortion during adaptation to new target domains is crucial. Recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. Nonetheless, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integrati
    
[^336]: 大型语言模型在电信行业的未来影响

    Large Language Models for Telecom: Forthcoming Impact on the Industry. (arXiv:2308.06013v1 [cs.IT])

    [http://arxiv.org/abs/2308.06013](http://arxiv.org/abs/2308.06013)

    大型语言模型在电信行业将产生重要的影响。它们可以提高运营效率，简化任务，并需要解决使用中的挑战。

    

    大型语言模型（LLMs）已经成为一股变革的力量，不仅在自然语言处理（NLP）的传统领域之外，还在许多领域引起了革命性的关注。随着LLM技术的不断发展，电信行业面临着潜在影响的前景。为了阐明这些影响，我们深入研究LLMs的内部机制，提供了关于它们目前的能力和局限性的见解。我们还研究了在电信行业可以方便实施的使用案例，简化了目前妨碍运营效率并需要大量人力和工程专业知识的任务。此外，我们还揭示了在电信领域利用LLMs所面临的独特挑战的重要研究方向。解决这些挑战是充分利用LLMs潜力和发挥其能力的重要进展。

    Large Language Models (LLMs) have emerged as a transformative force, revolutionizing numerous fields well beyond the conventional domain of Natural Language Processing (NLP) and garnering unprecedented attention. As LLM technology continues to progress, the telecom industry is facing the prospect of its potential impact on its landscape. To elucidate these implications, we delve into the inner workings of LLMs, providing insights into their current capabilities and limitations. We also examine the use cases that can be readily implemented in the telecom industry, streamlining numerous tasks that currently hinder operational efficiency and demand significant manpower and engineering expertise. Furthermore, we uncover essential research directions that deal with the distinctive challenges of utilizing the LLMs within the telecom domain. Addressing these challenges represents a significant stride towards fully harnessing the potential of LLMs and unlocking their capabilities to the fulles
    
[^337]: 语言模型作为主方程求解器

    Language models as master equation solvers. (arXiv:2308.02514v1 [cs.LG])

    [http://arxiv.org/abs/2308.02514](http://arxiv.org/abs/2308.02514)

    本研究将语言模型用作求解主方程的机器学习方法，通过设计提示网络和使用强化学习算法训练，实现了对多模组和高维系统的高精度求解。

    

    主方程在建模随机动力系统中具有基本重要性，然而由于状态空间维度的增加，解决主方程是具有挑战性的。本研究提出将语言模型重新应用为机器学习方法来解决主方程。我们设计了一个基于提示的神经网络，将速率参数、初始条件和时间值直接映射到与输入上下文完全匹配的状态联合概率分布。通过这种方式，我们近似地求解了主方程的最一般形式。我们使用强化学习框架中的策略梯度算法对网络进行训练，反馈奖励由一组变分自回归模型提供。通过将该方法应用于代表性示例，我们观察到对于多模组和高维系统，准确性很高。训练后的网络还展示了...

    Master equations are of fundamental importance in modeling stochastic dynamical systems.However, solving master equations is challenging due to the exponential increase in the number of possible states or trajectories with the dimension of the state space. In this study, we propose repurposing language models as a machine learning approach to solve master equations. We design a prompt-based neural network to map rate parameters, initial conditions, and time values directly to the state joint probability distribution that exactly matches the input contexts. In this way, we approximate the solution of the master equation in its most general form. We train the network using the policy gradient algorithm within the reinforcement learning framework, with feedback rewards provided by a set of variational autoregressive models. By applying this approach to representative examples, we observe high accuracy for both multi-module and high-dimensional systems. The trained network also exhibits ex
    
[^338]: 可解释的等变神经网络在粒子物理中的应用：PELICAN

    Explainable Equivariant Neural Networks for Particle Physics: PELICAN. (arXiv:2307.16506v2 [hep-ph] UPDATED)

    [http://arxiv.org/abs/2307.16506](http://arxiv.org/abs/2307.16506)

    PELICAN是一种可解释的等变神经网络，应用于粒子物理问题中。相比于其他方法，PELICAN的优势在于它采用了基于对称群的架构，具有降低复杂性、增加可解释性和提高性能的特点。它在标记和重构动量增强的顶夸克，并在密集环境中特别识别和测量W玻色子，以及识别不同类型的喷注等任务方面展示了出色的表现。

    

    PELICAN是一种新颖的置换等变且洛伦兹不变或协变的聚合网络，旨在克服应用于粒子物理问题的常见限制。与许多使用非专用架构的方法相比，PELICAN采用基于对称群的架构，体现了复杂度降低、可解释性增强和性能提升等优势，而非以庞大的参数为代价。我们在标记（分类）和重构（回归）动量增强的顶夸克的背景下对PELICAN算法架构进行全面研究，包括在洛伦兹增强的顶夸克强子末态的密集环境中特别识别和测量W玻色子的困难任务。我们还将PELICAN应用于识别夸克-引发与胶子-引发喷注以及多类别分类任务。

    PELICAN is a novel permutation equivariant and Lorentz invariant or covariant aggregator network designed to overcome common limitations found in architectures applied to particle physics problems. Compared to many approaches that use non-specialized architectures that neglect underlying physics principles and require very large numbers of parameters, PELICAN employs a fundamentally symmetry group-based architecture that demonstrates benefits in terms of reduced complexity, increased interpretability, and raw performance. We present a comprehensive study of the PELICAN algorithm architecture in the context of both tagging (classification) and reconstructing (regression) Lorentz-boosted top quarks, including the difficult task of specifically identifying and measuring the $W$-boson inside the dense environment of the Lorentz-boosted top-quark hadronic final state. We also extend the application of PELICAN to the tasks of identifying quark-initiated vs.~gluon-initiated jets, and a multi-
    
[^339]: 一种具有规划、长期上下文理解和程序合成能力的现实世界WebAgent

    A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. (arXiv:2307.12856v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12856](http://arxiv.org/abs/2307.12856)

    这篇论文介绍了一种名为WebAgent的LLM驱动代理，通过自我经验学习，在真实网站上完成任务。该方法通过规划、总结和生成代码来提高在真实网站上的成功率。

    

    最近，预训练的大型语言模型（LLMs）在自主Web自动化方面取得了更好的泛化性能和样本效率。然而，在真实世界的网站上，性能仍然受到三个方面的限制：开放领域性、有限的上下文长度和对HTML的归纳偏差的缺乏。我们介绍了一种名为WebAgent的LLM驱动代理，它通过自我经验学习，在遵循自然语言指令的前提下，在真实网站上完成任务。WebAgent通过将指令分解为规范的子指令，将长HTML文档总结为与任务相关的片段，并通过从中生成的Python程序对网站进行操作来提前进行规划。我们使用Flan-U-PaLM设计了WebAgent，用于生成有根代码，并使用HTML-T5进行预训练LLMs，利用局部和全局注意机制以及混合长跨度去噪目标来进行规划和总结。我们通过实验证明，我们的模块化方法提高了在真实网站上的成功率。

    Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by ov
    
[^340]: 基于图神经网络的太赫兹流导向纳米尺度定位

    Graph Neural Network-enabled Terahertz-based Flow-guided Nanoscale Localization. (arXiv:2307.05551v1 [cs.LG])

    [http://arxiv.org/abs/2307.05551](http://arxiv.org/abs/2307.05551)

    本研究提出了一种基于图神经网络的太赫兹流导向纳米尺度定位方法，可以提高定位精度和覆盖范围，解决了现有方法的定位精度低和无法全局定位的问题。

    

    纳米技术和先进材料的科学进展为体内精准医学的纳米尺度装置铺平了道路，其中包括集成感应、计算、通信、数据和能量存储能力。在人体心血管系统中，这些装置被设想为被动流动并持续感知以便检测诊断感兴趣的事件。通过将这些事件的物理位置（如身体区域）分配给它们，可以提高检测到这些事件的诊断价值，这是流导向定位的主要命题。当前的流导向定位方法存在定位精度低和无法在整个心血管系统内本地化事件的问题。为了解决这个问题，我们提出利用图神经网络（GNNs）来进行定位，并证明我们的方法在定位精度和覆盖范围上优于现有的最先进方法。

    Scientific advancements in nanotechnology and advanced materials are paving the way toward nanoscale devices for in-body precision medicine; comprising integrated sensing, computing, communication, data and energy storage capabilities. In the human cardiovascular system, such devices are envisioned to be passively flowing and continuously sensing for detecting events of diagnostic interest. The diagnostic value of detecting such events can be enhanced by assigning to them their physical locations (e.g., body region), which is the main proposition of flow-guided localization. Current flow-guided localization approaches suffer from low localization accuracy and they are by-design unable to localize events within the entire cardiovascular system. Toward addressing this issue, we propose the utilization of Graph Neural Networks (GNNs) for this purpose, and demonstrate localization accuracy and coverage enhancements of our proposal over the existing State of the Art (SotA) approaches. Based
    
[^341]: 关于图神经网络的能力和激活函数的作用

    On the power of graph neural networks and the role of the activation function. (arXiv:2307.04661v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04661](http://arxiv.org/abs/2307.04661)

    本文通过对称多项式代数的工具证明了对于具有分段多项式激活函数且体系结构大小不变的GNNs，存在一对非同构根树在任意迭代次数内无法被区分，与此同时，具有不同大小的GNNs只需两次迭代即可区分。此外，我们还证明了如果允许非分段多项式激活函数，则在两次迭代内，单个神经元感知器可以区分任意一对非同构树的根节点。

    

    在这篇文章中，我们提出了关于图神经网络（GNNs）表达能力的新结果。我们证明了对于任何具有分段多项式激活函数、其体系结构大小不随图输入大小增长的GNNs，存在一对深度为二的非同构根树，使得GNNs在任意迭代次数内无法区分它们的根节点。证明依赖于对称多项式代数的工具。相比之下，已经知道具有分段多项式激活函数的无界GNNs（其大小允许随图大小改变）只需两次迭代即可区分这些顶点。我们的结果对于有界大小和无界大小的GNNs之间存在严格的分离，回答了 [Grohe, 2021] 提出的一个开放性问题。接下来，我们证明如果允许非分段多项式激活函数，则在两次迭代中，单个神经元感知器可以区分任意一对非同构树的根节点。

    In this article we present new results about the expressivity of Graph Neural Networks (GNNs). We prove that for any GNN with piecewise polynomial activations, whose architecture size does not grow with the graph input sizes, there exists a pair of non-isomorphic rooted trees of depth two such that the GNN cannot distinguish their root vertex up to an arbitrary number of iterations. The proof relies on tools from the algebra of symmetric polynomials. In contrast, it was already known that unbounded GNNs (those whose size is allowed to change with the graph sizes) with piecewise polynomial activations can distinguish these vertices in only two iterations. Our results imply a strict separation between bounded and unbounded size GNNs, answering an open question formulated by [Grohe, 2021]. We next prove that if one allows activations that are not piecewise polynomial, then in two iterations a single neuron perceptron can distinguish the root vertices of any pair of nonisomorphic trees of 
    
[^342]: 计算机断层扫描图像中主动脉和大血管分割的拓扑感知损失

    Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed Tomography Images. (arXiv:2307.03137v1 [eess.IV])

    [http://arxiv.org/abs/2307.03137](http://arxiv.org/abs/2307.03137)

    本文介绍了一种新的拓扑感知损失函数，通过持久同调来惩罚计算机断层扫描图像中主动脉和大血管分割结果与真实值之间的拓扑差异。这种方法能够改善分割任务的性能，尤其是针对具有固有几何特征的对象。

    

    当使用标准损失函数训练分割网络时，网络并没有明确被要求学习图像的全局不变性，如对象的形状和多个对象之间的几何关系。然而，将这些不变性纳入网络训练中可能有助于改善各种分割任务的性能，尤其是当它们是需要分割的对象的固有特性时。本文以计算机断层扫描（CT）图像中主动脉和大血管的分割为例，这些血管由于人体解剖学，通常在身体中以特定的几何形状出现，并在2D CT图像上主要呈现为圆形对象。本文通过引入一种新的拓扑感知损失函数，通过持久同调惩罚地面真实值和预测之间的拓扑差异来解决这个问题。这与先前提出的分割网络设计不同，先前的设计是将阈值滤波应用于预测图像的似然函数。

    Segmentation networks are not explicitly imposed to learn global invariants of an image, such as the shape of an object and the geometry between multiple objects, when they are trained with a standard loss function. On the other hand, incorporating such invariants into network training may help improve performance for various segmentation tasks when they are the intrinsic characteristics of the objects to be segmented. One example is segmentation of aorta and great vessels in computed tomography (CT) images where vessels are found in a particular geometry in the body due to the human anatomy and they mostly seem as round objects on a 2D CT image. This paper addresses this issue by introducing a new topology-aware loss function that penalizes topology dissimilarities between the ground truth and prediction through persistent homology. Different from the previously suggested segmentation network designs, which apply the threshold filtration on a likelihood function of the prediction map 
    
[^343]: 使用可逆神经网络和误差扩散学习利用导电图重建气泡分布

    Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion. (arXiv:2307.02496v1 [eess.IV])

    [http://arxiv.org/abs/2307.02496](http://arxiv.org/abs/2307.02496)

    本研究利用可逆神经网络和误差扩散方法，通过测量气泡引起的磁场波动，重建电解过程中的气泡分布和电导率图，并实现了比传统方法更优异的性能。

    

    电解是环保的氢气生产过程中的关键，但是过程中产生的气泡会阻碍反应，降低电池效率，增加能量消耗。此外，这些气泡会导致电池内部的电导率发生变化，从而导致周围产生感应磁场的变化。因此，利用外部磁传感器测量这些气泡引起的磁场波动，并求解Biot-Savart定律的反问题，可以估计电池内的电导率，从而得到气泡的大小和位置。然而，仅凭几个感应磁场测量值确定高分辨率电导率图是一个病态反问题。为了克服这个问题，我们利用可逆神经网络（INN）重建电导率场。我们的定性结果和使用随机误差扩散的定量评估表明，与Tikhonov正则化相比，INN具有更优异的性能。

    Electrolysis is crucial for eco-friendly hydrogen production, but gas bubbles generated during the process hinder reactions, reduce cell efficiency, and increase energy consumption. Additionally, these gas bubbles cause changes in the conductivity inside the cell, resulting in corresponding variations in the induced magnetic field around the cell. Therefore, measuring these gas bubble-induced magnetic field fluctuations using external magnetic sensors and solving the inverse problem of Biot-Savart Law allows for estimating the conductivity in the cell and, thus, bubble size and location. However, determining high-resolution conductivity maps from only a few induced magnetic field measurements is an ill-posed inverse problem. To overcome this, we exploit Invertible Neural Networks (INNs) to reconstruct the conductivity field. Our qualitative results and quantitative evaluation using random error diffusion show that INN achieves far superior performance compared to Tikhonov regularizatio
    
[^344]: 惯性导航与深度学习：当前趋势与未来方向的综述

    Inertial Navigation Meets Deep Learning: A Survey of Current Trends and Future Directions. (arXiv:2307.00014v1 [cs.RO])

    [http://arxiv.org/abs/2307.00014](http://arxiv.org/abs/2307.00014)

    本文综述了惯性导航领域中当前的深度学习方法，包括对不同车辆操作领域的研究、滤波参数学习的改进以及惯性传感器的校准和去噪方法。翻译过的论文标题: 惯性导航与深度学习：当前趋势与未来方向的综述

    

    惯性传感在许多应用和平台中被使用，从智能手机等日常设备到自动驾驶车辆等复杂设备。近年来，机器学习和深度学习技术在惯性传感领域取得了显著发展。这是由于高效的计算硬件的发展和公开可用的传感器数据的可获得性。这些数据驱动的方法被用于强化基于模型的导航和传感器融合算法。本文提供了对这些深度学习方法的深入综述。我们分别考察了每个车辆操作领域，包括陆地、空中和海洋。每个领域分为纯惯性进展和基于滤波参数学习的改进。此外，我们还回顾了用于校准和去噪惯性传感器的深度学习方法。在整篇论文中，我们讨论了这些趋势和未来方向。我们还提供了常用的统计数据。

    Inertial sensing is used in many applications and platforms, ranging from day-to-day devices such as smartphones to very complex ones such as autonomous vehicles. In recent years, the development of machine learning and deep learning techniques has increased significantly in the field of inertial sensing. This is due to the development of efficient computing hardware and the accessibility of publicly available sensor data. These data-driven approaches are used to empower model-based navigation and sensor fusion algorithms. This paper provides an in-depth review of those deep learning methods. We examine separately, each vehicle operation domain including land, air, and sea. Each domain is divided into pure inertial advances and improvements based on filter parameters learning. In addition, we review deep learning approaches for calibrating and denoising inertial sensors. Throughout the paper, we discuss these trends and future directions. We also provide statistics on the commonly used
    
[^345]: 用基于梯度的优化方法解决核岭回归问题

    Solving Kernel Ridge Regression with Gradient-Based Optimization Methods. (arXiv:2306.16838v1 [stat.ML])

    [http://arxiv.org/abs/2306.16838](http://arxiv.org/abs/2306.16838)

    本研究提出了一种新的方法来解决核岭回归问题，通过等价的目标函数形式和基于梯度的优化方法，我们不仅可以使用其他惩罚方法，还能够从梯度下降的角度研究核岭回归。通过提前停止的正则化，我们推导出了一个闭合解，即核梯度流（KGF），并证明了KGF和KRR之间的差异。我们还将KRR泛化，使用$\ell_1$和$\ell_\infty$惩罚方法，并发现使用这些方法得到的解与前向分步回归和符号梯度下降结合提前停止得到的解非常相似。因此，我们减少了计算复杂度重的近端梯度下降算法的需求。

    

    核岭回归（KRR）是线性岭回归的非线性推广。在这里，我们引入了KRR目标函数的等价形式，为使用其他惩罚方法和从梯度下降的角度研究核岭回归打开了可能。通过连续时间的视角，我们推导出了一个闭合解——核梯度流（KGF），通过提前停止的正则化，让我们能够在KGF和KRR之间理论上界定差异。我们用$\ell_1$和$\ell_\infty$惩罚方法将KRR泛化，并利用类似KGF和KRR之间的相似性，使用这些惩罚方法得到的解与使用前向分步回归（也称为坐标下降）和符号梯度下降结合提前停止得到的解非常相似。因此，减少了计算复杂度重的近端梯度下降算法的需求。

    Kernel ridge regression, KRR, is a non-linear generalization of linear ridge regression. Here, we introduce an equivalent formulation of the objective function of KRR, opening up both for using other penalties than the ridge penalty and for studying kernel ridge regression from the perspective of gradient descent. Using a continuous-time perspective, we derive a closed-form solution, kernel gradient flow, KGF, with regularization through early stopping, which allows us to theoretically bound the differences between KGF and KRR. We generalize KRR by replacing the ridge penalty with the $\ell_1$ and $\ell_\infty$ penalties and utilize the fact that analogously to the similarities between KGF and KRR, the solutions obtained when using these penalties are very similar to those obtained from forward stagewise regression (also known as coordinate descent) and sign gradient descent in combination with early stopping. Thus the need for computationally heavy proximal gradient descent algorithms
    
[^346]: 动量简单而可证实地增强非独立同分布联邦学习的效果

    Momentum Benefits Non-IID Federated Learning Simply and Provably. (arXiv:2306.16504v1 [cs.LG])

    [http://arxiv.org/abs/2306.16504](http://arxiv.org/abs/2306.16504)

    本论文研究了在非独立同分布联邦学习中利用动量来提升FedAvg和SCAFFOLD算法的性能，证明了引入动量可以使FedAvg在不依赖于数据异质性的假设下收敛。

    

    联邦学习是一种用于大规模机器学习的强大范例，但由于不可靠的网络连接、缓慢的通信以及客户端之间存在的数据异质性，它面临着重大挑战。FedAvg和SCAFFOLD是两种解决这些挑战的基本算法。特别地，FedAvg在与中央服务器进行通信之前采用多个本地更新，而SCAFFOLD在其本地更新中维护每个客户端上的控制变量以补偿“客户端漂移”。文献中提出了各种方法来增强这两种算法的收敛性，但它们要么对算法结构进行不切实际的调整，要么依赖于有界数据异质性的假设。本文探讨了利用动量来增强FedAvg和SCAFFOLD性能的方法。当所有客户端参与训练过程时，我们证明了引入动量可以使FedAvg在不依赖于数据异质性的假设下收敛。

    Federated learning is a powerful paradigm for large-scale machine learning, but it faces significant challenges due to unreliable network connections, slow communication, and substantial data heterogeneity across clients. FedAvg and SCAFFOLD are two fundamental algorithms to address these challenges. In particular, FedAvg employs multiple local updates before communicating with a central server, while SCAFFOLD maintains a control variable on each client to compensate for "client drift" in its local updates. Various methods have been proposed in literature to enhance the convergence of these two algorithms, but they either make impractical adjustments to algorithmic structure, or rely on the assumption of bounded data heterogeneity.  This paper explores the utilization of momentum to enhance the performance of FedAvg and SCAFFOLD. When all clients participate in the training process, we demonstrate that incorporating momentum allows FedAvg to converge without relying on the assumption o
    
[^347]: 基于任务条件化超网络的多任务记忆深度强化学习

    Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork. (arXiv:2306.10698v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10698](http://arxiv.org/abs/2306.10698)

    人工智能领域，一个新算法利用基于任务条件化超网络的检索网络，根据任务调整网络参数，以解决深度强化学习中选择最相关的过去经验并将其融合到既有决策网络中的问题。

    

    深度强化学习算法通常受到采样效率低下的限制，严重依赖与环境的多次交互才能获得准确的决策能力。相比之下，人类似乎依赖海马体从过去有关任务的经历中检索相关信息，在学习新任务时指导其决策，而不是仅仅依赖于环境交互。然而，为代理设计类似海马体的模块以将过去的经历融入既有的强化学习算法面临两个挑战。第一个挑战涉及选择当前任务最相关的过去经验，第二个是将这些经验与决策网络相结合。为了解决这些问题，我们提出了一种新算法，利用基于任务条件化超网络的检索网络，根据任务调整检索网络的参数。

    Deep reinforcement learning algorithms are usually impeded by sampling inefficiency, heavily depending on multiple interactions with the environment to acquire accurate decision-making capabilities. In contrast, humans seem to rely on their hippocampus to retrieve relevant information from past experiences of relevant tasks, which guides their decision-making when learning a new task, rather than exclusively depending on environmental interactions. Nevertheless, designing a hippocampus-like module for an agent to incorporate past experiences into established reinforcement learning algorithms presents two challenges. The first challenge involves selecting the most relevant past experiences for the current task, and the second is integrating such experiences into the decision network. To address these challenges, we propose a novel algorithm that utilizes a retrieval network based on a task-conditioned hypernetwork, which adapts the retrieval network's parameters depending on the task. A
    
[^348]: 随机加权梯度下降通过分布健壮优化

    Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization. (arXiv:2306.09222v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.09222](http://arxiv.org/abs/2306.09222)

    我们通过分布健壮优化和重要性加权的梯度下降技术提升了深度神经网络的性能，并在各种任务上取得了优越的结果。

    

    我们通过在每一次优化步骤中对数据点进行重要性加权，开发了一种提高深度神经网络性能的加权梯度下降技术。我们的方法受到分布健壮优化和f-散度的启发，已知可以得到具有改进的泛化保证的模型。我们的加权方案简单、计算高效，可以与许多流行的优化算法（如SGD和Adam）结合使用。实验证明，我们的方法在各种任务上都表现出了优越性能，包括监督学习和领域适应。值得注意的是，我们在DomainBed和Tabular分类基准上分别比现有最佳结果提升了0.7%和1.44%。此外，我们的算法将BERT在GLUE基准上的性能提升了1.94%，将ViT在ImageNet-1K上的性能提升了1.01%。这些结果表明了所提出方法的有效性，预示着它在改善性能方面的潜力。

    We develop a re-weighted gradient descent technique for boosting the performance of deep neural networks, which involves importance weighting of data points during each optimization step. Our approach is inspired by distributionally robust optimization with f-divergences, which has been known to result in models with improved generalization guarantees. Our re-weighting scheme is simple, computationally efficient, and can be combined with many popular optimization algorithms such as SGD and Adam. Empirically, we demonstrate the superiority of our approach on various tasks, including supervised learning, domain adaptation. Notably, we obtain improvements of +0.7% and +1.44% over SOTA on DomainBed and Tabular classification benchmarks, respectively. Moreover, our algorithm boosts the performance of BERT on GLUE benchmarks by +1.94%, and ViT on ImageNet-1K by +1.01%. These results demonstrate the effectiveness of the proposed approach, indicating its potential for improving performance in 
    
[^349]: BackpropTools: 一款快速、可移植的连续控制深度强化学习库

    BackpropTools: A Fast, Portable Deep Reinforcement Learning Library for Continuous Control. (arXiv:2306.03530v1 [cs.LG])

    [http://arxiv.org/abs/2306.03530](http://arxiv.org/abs/2306.03530)

    BackpropTools是一款快速、可移植的连续控制深度强化学习库，它通过模板元编程提供紧密集成的可组合组件，并在异构平台集合上无缝使用，同时在连续控制问题的深度RL代理高效可扩展训练方面具有优势。由于其可移植性和实时保证，它成为了在嵌入式设备上部署学来的策略的有价值的工具。

    

    深度强化学习在许多领域中已被证明可以产生出具有能力的代理和控制策略，但常常受到训练时间过长的困扰。此外，在连续控制问题的情况下，现有深度学习库的实时性和可移植性的缺乏限制了学习策略在实际嵌入式设备上的应用。为了解决这些问题，我们提出了BackpropTools，一种依赖性-free、header-only、pure C++的深度监督和强化学习库。利用最近C++标准的模板元编程能力，我们提供了可以由编译器紧密集成的可组合组件。其新颖的架构允许BackpropTools在异构平台集合上无缝使用，从HPC集群、工作站和笔记本电脑到智能手机、智能手表和微控制器。具体来说，由于RL算法与模拟环境的紧密集成，BackpropTools在连续控制问题的深度RL代理的高效可扩展训练方面具有优势。此外，它的可移植性和实时保证使其成为在嵌入式设备上部署学来的策略的有价值的工具。

    Deep Reinforcement Learning (RL) has been demonstrated to yield capable agents and control policies in several domains but is commonly plagued by prohibitively long training times. Additionally, in the case of continuous control problems, the applicability of learned policies on real-world embedded devices is limited due to the lack of real-time guarantees and portability of existing deep learning libraries. To address these challenges, we present BackpropTools, a dependency-free, header-only, pure C++ library for deep supervised and reinforcement learning. Leveraging the template meta-programming capabilities of recent C++ standards, we provide composable components that can be tightly integrated by the compiler. Its novel architecture allows BackpropTools to be used seamlessly on a heterogeneous set of platforms, from HPC clusters over workstations and laptops to smartphones, smartwatches, and microcontrollers. Specifically, due to the tight integration of the RL algorithms with simu
    
[^350]: 稳定对比强化学习: 离线目标达成的技术

    Stabilizing Contrastive RL: Techniques for Offline Goal Reaching. (arXiv:2306.03346v1 [cs.LG])

    [http://arxiv.org/abs/2306.03346](http://arxiv.org/abs/2306.03346)

    本文提出了一种稳定的对比强化学习方法，通过浅而宽的结构，结合谨慎的权重初始化和数据增强等实验方法，在具有挑战性的仿真基准测试中显著提高了性能，并演示了对比方法可以解决现实世界的机器人任务。

    

    计算机视觉和自然语言处理领域已经开发了自监督方法，强化学习也可以被视为自监督问题：学习达到任何目标，而不需要人类指定的奖励或标签。然而，为强化学习建立自监督基础实际上面临着一些重要的挑战。基于此前对比学习方法，我们进行了细致的剖析实验，并发现一个浅而宽的结构，结合谨慎的权重初始化和数据增强，可以显着提高与对比强化学习方法的性能，特别是在具有挑战性的仿真基准测试中。此外，我们还演示了通过这些设计决策，对比方法可以解决现实世界的机器人操作任务，其中任务由训练后提供的单个目标图像指定。

    In the same way that the computer vision (CV) and natural language processing (NLP) communities have developed self-supervised methods, reinforcement learning (RL) can be cast as a self-supervised problem: learning to reach any goal, without requiring human-specified rewards or labels. However, actually building a self-supervised foundation for RL faces some important challenges. Building on prior contrastive approaches to this RL problem, we conduct careful ablation experiments and discover that a shallow and wide architecture, combined with careful weight initialization and data augmentation, can significantly boost the performance of these contrastive RL approaches on challenging simulated benchmarks. Additionally, we demonstrate that, with these design decisions, contrastive approaches can solve real-world robotic manipulation tasks, with tasks being specified by a single goal image provided after training.
    
[^351]: 在随机递归有向无环图中的广播

    Broadcasting in random recursive dags. (arXiv:2306.01727v1 [stat.ML])

    [http://arxiv.org/abs/2306.01727](http://arxiv.org/abs/2306.01727)

    该论文研究了一个均匀的$k$-dag广播模型，确定了与$p$和$k$有关的阈值，并讨论了大多数规则的误差率。

    

    一个均匀的$k$-dag通过从现有节点中均匀随机选择$k$个父节点来推广均匀的随机递归树。它以$k$个“根”开始。每个$k$个根节点都被分配一个位。这些位通过一个嘈杂的信道传播。每个父节点的位都以概率$p$发生变化，并进行大多数表决。当所有节点都接收到它们的位后，$k$-dag被显示，不识别根节点。目标是估计所有根节点中的大多数位。我们确定了$p$的阈值，作为一个关于$k$的函数，使得所有节点的大多数规则产生错误$c+o(1)$的概率小于$1/2$。在阈值以上，大多数规则的错误概率为$1/2+o(1)$。

    A uniform $k$-{\sc dag} generalizes the uniform random recursive tree by picking $k$ parents uniformly at random from the existing nodes. It starts with $k$ ''roots''. Each of the $k$ roots is assigned a bit. These bits are propagated by a noisy channel. The parents' bits are flipped with probability $p$, and a majority vote is taken. When all nodes have received their bits, the $k$-{\sc dag} is shown without identifying the roots. The goal is to estimate the majority bit among the roots. We identify the threshold for $p$ as a function of $k$ below which the majority rule among all nodes yields an error $c+o(1)$ with $c<1/2$. Above the threshold the majority rule errs with probability $1/2+o(1)$.
    
[^352]: 基于谱嵌入的深度学习研究

    Going Deeper with Spectral Embeddings. (arXiv:2306.00742v1 [cs.LG])

    [http://arxiv.org/abs/2306.00742](http://arxiv.org/abs/2306.00742)

    本文提出两种新的谱嵌入方法，一种基于函数分析原理和核方法，另一种基于深度网络优化损失，提供理论保证和实际有效的算法，并提供新的采样算法。

    

    为了有效地处理海量的数据，从而更好地对其进行表征，科学家们采用表示学习。最近，这些方法与一些底层运算的谱分解之间展现出明显的联系。在历史上，是通过在数据的顶部构建图形来建立明确的谱嵌入，而我们提出了两种新的方法：一种基于函数分析原理和核方法构建的，这将导致具有理论保证的算法，另一种基于深度网络训练以优化基本变分损失的算法，它们产生了实际有效的算法。此外，我们提供了一种新的采样算法，利用学习到的表征来在一步中生成新的样本。

    To make sense of millions of raw data and represent them efficiently, practitioners rely on representation learning. Recently, deep connections have been shown between these approaches and the spectral decompositions of some underlying operators. Historically, explicit spectral embeddings were built from graphs constructed on top of the data. In contrast, we propose two new methods to build spectral embeddings: one based on functional analysis principles and kernel methods, which leads to algorithms with theoretical guarantees, and the other based on deep networks trained to optimize principled variational losses, which yield practically efficient algorithms. Furthermore, we provide a new sampling algorithm that leverages learned representations to generate new samples in a single step.
    
[^353]: 基于Sinkhorn距离的特征对其N-BEATS

    Feature-aligned N-BEATS with Sinkhorn divergence. (arXiv:2305.15196v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15196](http://arxiv.org/abs/2305.15196)

    这是一个基于Sinkhorn距离的特征对其N-BEATS模型，它通过对齐堆栈中的边际特征概率测度来进行领域广义的时间序列预测，同时保留了N-BEATS的可解释性和预测能力。

    

    我们提出了基于Sinkhorn距离的特征对其N-BEATS作为一个领域广义时间序列预测模型。它是N-BEATS的非平凡扩展，采用了双重残差叠加原则（Oreshkin等人[42]）并将其转化为一个表示学习框架。具体而言，它围绕着由N-BEATS每个堆栈的残差和特征提取算子的复杂组合产生的边际特征概率测度，并通过一种近似最优传输距离（Sinkhorn距离）将它们堆叠地对齐。训练损失由来自多个源域的经验风险最小化（即预测损失）和Sinkhorn距离计算的对齐损失组成，使得模型能够在多个源数据序列中堆叠地学习不变特征，同时保留N-BEATS的可解释设计和预测能力。我们提供了全面的实验评估和消融研究，并展示了相应的结果。

    We propose Feature-aligned N-BEATS as a domain-generalized time series forecasting model. It is a nontrivial extension of N-BEATS with doubly residual stacking principle (Oreshkin et al.[42]) into a representation learning framework. In particular, it revolves around marginal feature probability measures induced by the intricate composition of residual and feature extracting operators of N-BEATS in each stack and aligns them stack-wisely via an approximate of an optimal transport distance referred to as the Sinkhorn divergence. The training loss consists of an empirical risk minimization from multiple source domains, i.e., forecasting loss, and an alignment loss calculated with the Sinkhorn divergence, which allows the model to learn invariant features stack-wisely across multiple source data sequences while retaining N-BEATS's interpretable design and forecasting power. Comprehensive experimental evaluations with ablation studies are provided and the corresponding results demonstrate 
    
[^354]: 从生成模型的角度重新审视实体对齐及超越：一个视角

    Revisit and Outstrip Entity Alignment: A Perspective of Generative Models. (arXiv:2305.14651v1 [cs.CL])

    [http://arxiv.org/abs/2305.14651](http://arxiv.org/abs/2305.14651)

    本文重新从生成模型的角度研究了基于嵌入的实体对齐（EEA）问题，引入基于生成对抗网络的EEA方法及提出的生成的EEA（GEEA）框架，通过互相变分自动编码器（M-VAE）实现实体从一个KG转换到另一个KG，并且从随机噪声向量生成新的实体，具有较好的效果。

    

    最近，基于嵌入的方法在利用多模态知识图谱（KG）嵌入的实体对齐方面取得了巨大成功。在本文中，我们从生成模型的角度研究了基于嵌入的实体对齐（EEA）。我们表明EEA是一个特殊的问题，其主要目标类似于典型生成模型中的目标，基于这个目标，我们从理论上证明了最近发展的基于生成对抗网络（GAN）的EEA方法的有效性。然后，我们揭示了他们不完整的目标限制了实体对齐和实体合成（即生成新实体）的能力。我们通过引入生成的EEA（abbr.，GEEA）框架和提出的互相变分自动编码器（M-VAE）作为生成模型来缓解这个问题。M-VAE可以将一个实体从一个KG转换到另一个KG，并从随机噪声向量生成新实体。我们通过理论分析和实证实验展示了GEEA的优势。

    Recent embedding-based methods have achieved great successes on exploiting entity alignment from knowledge graph (KG) embeddings of multiple modals. In this paper, we study embedding-based entity alignment (EEA) from a perspective of generative models. We show that EEA is a special problem where the main objective is analogous to that in a typical generative model, based on which we theoretically prove the effectiveness of the recently developed generative adversarial network (GAN)-based EEA methods. We then reveal that their incomplete objective limits the capacity on both entity alignment and entity synthesis (i.e., generating new entities). We mitigate this problem by introducing a generative EEA (abbr., GEEA) framework with the proposed mutual variational autoencoder (M-VAE) as the generative model. M-VAE can convert an entity from one KG to another and generate new entities from random noise vectors. We demonstrate the power of GEEA with theoretical analysis and empirical experime
    
[^355]: 使用指令微调基础模型的多模态 Web 导航。

    Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])

    [http://arxiv.org/abs/2305.11854](http://arxiv.org/abs/2305.11854)

    本文研究使用视觉语言基础模型进行数据驱动离线训练的 Web 代理，提出了一个指令跟随多模态代理WebGUM，将微调指令微调语言模型和视觉转换器，能够有效提高代理的基于视觉感知、HTML 理解和多步推理的能力。

    

    自主 Web 导航的进展受到了依赖数十亿次在线强化学习的探索性交互和具有领域特定模型设计的影响，这使得难以利用来自丰富领域外数据的泛化。在本工作中，我们研究了基于数据驱动的脱机训练，用于使用视觉语言基础模型的 Web 代理。我们提出了一个指令跟随多模态代理， WebGUM，它观察了网页截图和 HTML 页面，并输出 Web 导航操作，如单击和输入。WebGUM 是通过联合微调指令微调语言模型和视觉转换器在大量的演示语料库上训练的。我们凭经验证明，这种方法可以提高代理的基于视觉感知、HTML 理解和多步推理的能力，明显优于之前的工作。在 MiniWoB 基准测试中，我们超过之前最佳脱机方法 31.9% 以上，接近实现在线交互的表现。

    The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision transformer on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded visual perception, HTML comprehension and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB benchmark, we improve over the previous best offline methods by more than 31.9%, being close to re
    
[^356]: 小型语言模型更适合作为黑匣子机器生成文本检测器

    Smaller Language Models are Better Black-box Machine-Generated Text Detectors. (arXiv:2305.09859v1 [cs.CL])

    [http://arxiv.org/abs/2305.09859](http://arxiv.org/abs/2305.09859)

    本文研究发现，小型语言模型更适用于作为通用文本检测器，可以更加精确地检测出机器生成的文本，而检测器和生成模型是否具有相同的架构或语料库并不会对检测性能产生显著影响。

    

    随着流畅的生成语言模型的出现，它们可以生成与人类写作的非常相似的令人信服的话语，因此区分一段文本是由机器生成的还是人类写作的变得更加具有挑战性和重要性，因为这样的模型可以用于传播错误信息、虚假新闻、虚假评论并模仿某些作者和人物。为此，已经提出了许多检测机器生成文本的方法。其中大部分方法需要访问目标模型的 logits，或需要可以从目标模型中进行采样的能力。其中一种黑匣子检测方法依赖于观察到生成文本在生成器的似然函数下是局部最优的，而人类写作的文本则不是。我们发现，总体而言，较小且部分训练的模型更适合作为通用文本检测器：它们可以更精确地检测来自小型和大型模型的生成文本。有趣的是，我们发现检测器和生成模型是否具有相同的架构或相同的语料库对检测性能没有显著影响。

    With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generat
    
[^357]: 大型语言模型能否改变计算社会科学？

    Can Large Language Models Transform Computational Social Science?. (arXiv:2305.03514v1 [cs.CL])

    [http://arxiv.org/abs/2305.03514](http://arxiv.org/abs/2305.03514)

    本文研究了大型语言模型作为计算社会科学工具的潜力。虽然在分类任务上没有优势，但在自由形式编码任务上表现优异，今后可以作为零-shot检测工具进行使用，

    

    ChatGPT等大型语言模型(LLMs)能够成功地在许多语言处理任务中进行零-shot操作（无需训练数据）。如果这种能力也适用于对说服力和政治意识形态等社会现象的编码，那么LLMs就可以有效地改变计算社会科学(CSS)。本研究提供了使用LLMs作为CSS工具的路线图。为此，我们提供了一组优秀的提示实践以及一个广泛的评估流程，以测量13种语言模型在24个代表性的CSS基准测试上的零-shot性能。在分类任务上，LLMs无法超越最佳微调模型，但仍然与人类达成了公平的协议水平。在自由形式的编码任务（生成）上，LLMs生成的解释常常超过了工作者的黄金参考的质量。我们得出结论，今天的LLMs可以通过两种方式从根本上增强CSS研究流程：(1)作为零-shot检测工具进行无缝工作。

    Large Language Models (LLMs) like ChatGPT are capable of successfully performing many language processing tasks zero-shot (without the need for training data). If this capacity also applies to the coding of social phenomena like persuasiveness and political ideology, then LLMs could effectively transform Computational Social Science (CSS). This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 24 representative CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that today's LLMs can radically augment the CSS research pipeline in two ways: (1) serving as zero-shot d
    
[^358]: 通用核学习的高效凸优化算法

    Efficient Convex Algorithms for Universal Kernel Learning. (arXiv:2304.07472v1 [stat.ML])

    [http://arxiv.org/abs/2304.07472](http://arxiv.org/abs/2304.07472)

    本文提出了一种基于SVD-QCQP原始对偶算法的高效内核学习实现，用于学习半分离核，大大降低了计算复杂度，并在几个基准数据集上展示了其准确性和速度。

    

    基于核优化的机器学习算法的准确性和复杂性取决于它们能够优化的核集。理想的核集应该：具有线性参数化（以便于可处理性）；在所有核集中密集（以便于鲁棒性）；是通用的（以便于准确性）。最近，提出了一种框架，使用正定矩阵来参数化一类正半分离核。尽管此类核能够满足所有三个标准，但之前用于优化此类核的算法仅限于分类，并且还依赖于计算复杂的半定规划（SDP）算法。在本文中，我们将学习半分离核的问题作为极小化极大化优化问题，并提出了一种SVD-QCQP原始对偶算法，其与之前基于SDP的方法相比，大大降低了计算复杂度。此外，我们提供了一种高效的内核学习实现，并在几个基准数据集上展示了其准确性和速度。

    The accuracy and complexity of machine learning algorithms based on kernel optimization are determined by the set of kernels over which they are able to optimize. An ideal set of kernels should: admit a linear parameterization (for tractability); be dense in the set of all kernels (for robustness); be universal (for accuracy). Recently, a framework was proposed for using positive matrices to parameterize a class of positive semi-separable kernels. Although this class can be shown to meet all three criteria, previous algorithms for optimization of such kernels were limited to classification and furthermore relied on computationally complex Semidefinite Programming (SDP) algorithms. In this paper, we pose the problem of learning semiseparable kernels as a minimax optimization problem and propose a SVD-QCQP primal-dual algorithm which dramatically reduces the computational complexity as compared with previous SDP-based approaches. Furthermore, we provide an efficient implementation of thi
    
[^359]: 基于分布中知识蒸馏的非IID数据同质化方法来进行分散式学习

    Homogenizing Non-IID datasets via In-Distribution Knowledge Distillation for Decentralized Learning. (arXiv:2304.04326v1 [cs.LG])

    [http://arxiv.org/abs/2304.04326](http://arxiv.org/abs/2304.04326)

    本文提出了基于分布中知识蒸馏（IDKD）的方法来解决节点间同质化数据分布的问题，该方法使用公共数据集来从每个节点中提取知识，并通过生成的标签将其传递给其邻居，以实现相同的目标，同时保持隐私约束。在非i.i.d.数据集上，IDKD的性能显着优于现有方法，同时保持隐私和通信效率。

    

    分散式学习允许在多个节点上以分散式方式训练深度神经网络（DNN），并使用各种数据源进行训练。然而，分散式学习的一个主要挑战是节点间数据分布的异质性。本文提出了基于分布中知识蒸馏（IDKD）的方法来解决此挑战。IDKD的目标是同质化节点之间的数据分布。虽然这种数据同质化可以通过在节点之间交换数据来实现，但这样会牺牲隐私。IDKD使用公共数据集来从每个节点中提取知识，并通过生成的标签将其传递给其邻居，以实现相同的目标，同时保持隐私约束。我们评估了IDKD在非i.i.d.数据集上的性能，并显示它在同质数据分布方面显着优于现有方法，同时保持隐私和通信效率。

    Decentralized learning enables serverless training of deep neural networks (DNNs) in a distributed manner on multiple nodes. This allows for the use of large datasets, as well as the ability to train with a wide variety of data sources. However, one of the key challenges with decentralized learning is heterogeneity in the data distribution across the nodes. In this paper, we propose In-Distribution Knowledge Distillation (IDKD) to address the challenge of heterogeneous data distribution. The goal of IDKD is to homogenize the data distribution across the nodes. While such data homogenization can be achieved by exchanging data among the nodes sacrificing privacy, IDKD achieves the same objective using a common public dataset across nodes without breaking the privacy constraint. This public dataset is different from the training dataset and is used to distill the knowledge from each node and communicate it to its neighbors through the generated labels. With traditional knowledge distillat
    
[^360]: 鲁棒不变表示中的域泛化

    Domain Generalization In Robust Invariant Representation. (arXiv:2304.03431v1 [cs.LG])

    [http://arxiv.org/abs/2304.03431](http://arxiv.org/abs/2304.03431)

    本文研究了不变表示的泛化性能，证明具有不变表示的模型可以学习到具有鲁棒性的非结构化潜在表示，因此使不变性成为域泛化的一个关键方面。

    

    无监督学习常见变换的不变表示方法常用于目标识别。学习不变性使得模型更加鲁棒，并在实际场景中更容易应用。由于不改变对象固有属性的数据变换是识别任务中主要的复杂性来源，对这些变换具有不变性的模型有助于减少所需的训练数据。这进一步提高了模型的效率并简化了训练过程。本文研究了不变表示的泛化性能，并试图回答一个问题：具有某些变换不变性的模型在先前未见域中是否仍具有不变性？通过广泛的实验，我们证明了具有不变表示的模型可以学习到具有鲁棒性的非结构化潜在表示，因此使不变性成为域泛化的一个关键方面。

    Unsupervised approaches for learning representations invariant to common transformations are used quite often for object recognition. Learning invariances makes models more robust and practical to use in real-world scenarios. Since data transformations that do not change the intrinsic properties of the object cause the majority of the complexity in recognition tasks, models that are invariant to these transformations help reduce the amount of training data required. This further increases the model's efficiency and simplifies training. In this paper, we investigate the generalization of invariant representations on out-of-distribution data and try to answer the question: Do model representations invariant to some transformations in a particular seen domain also remain invariant in previously unseen domains? Through extensive experiments, we demonstrate that the invariant model learns unstructured latent representations that are robust to distribution shifts, thus making invariance a de
    
[^361]: 基于信息导向随机游走的分布式图嵌入方法

    Distributed Graph Embedding with Information-Oriented Random Walks. (arXiv:2303.15702v1 [cs.DC])

    [http://arxiv.org/abs/2303.15702](http://arxiv.org/abs/2303.15702)

    本文提出了一种名为DistGER的分布式图嵌入方法，基于信息导向随机游走策略，利用多种优化技术实现了高效的十亿级别图嵌入。

    

    图嵌入将图节点映射到低维向量中，被广泛应用于机器学习任务中。本文提出了一种分布式的、基于信息导向随机游走的通用图嵌入框架DistGER，可用于处理十亿级别的图嵌入。该方法通过增量式地计算信息导向的随机游走，并利用了多种邻近感知、流式、并行的图划分策略，实现了在不同机器上既高质量的本地划分，又出色的负载均衡。同时DistGER改进了分布式Skip-Gram学习模型以生成节点嵌入，并优化了在分布式环境中的访问局部性、CPU吞吐量和同步效率。

    Graph embedding maps graph nodes to low-dimensional vectors, and is widely adopted in machine learning tasks. The increasing availability of billion-edge graphs underscores the importance of learning efficient and effective embeddings on large graphs, such as link prediction on Twitter with over one billion edges. Most existing graph embedding methods fall short of reaching high data scalability. In this paper, we present a general-purpose, distributed, information-centric random walk-based graph embedding framework, DistGER, which can scale to embed billion-edge graphs. DistGER incrementally computes information-centric random walks. It further leverages a multi-proximity-aware, streaming, parallel graph partitioning strategy, simultaneously achieving high local partition quality and excellent workload balancing across machines. DistGER also improves the distributed Skip-Gram learning model to generate node embeddings by optimizing the access locality, CPU throughput, and synchronizat
    
[^362]: Uni-RXN: 一种统一的框架，弥合化学反应Pretraining和条件分子生成之间的差距

    Uni-RXN: A Unified Framework Bridging the Gap between Chemical Reaction Pretraining and Conditional Molecule Generation. (arXiv:2303.06965v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06965](http://arxiv.org/abs/2303.06965)

    本文提出了Uni-RXN框架，在化学反应Pretraining和分子生成任务中都取得了最先进的结果。通过具备化学知识，克服了当前分子生成模型仅依赖少量反应模板的限制，生成质量高、可合成的药物类分子结构。

    

    化学反应是药物设计和有机化学研究的基本构建模块。近年来，对于一个可以有效捕捉化学反应基本规则的大规模深度学习框架的需求不断增长。在本文中，我们提出了一个统一的框架，解决了反应表示学习和分子生成任务，允许更整体的方法。受有机化学机制的启发，我们开发了一种新的预训练框架，使我们能够将归纳偏见纳入模型。我们的框架在具有挑战性的下游任务上取得了最先进的结果。通过具备化学知识，该框架可应用于基于反应的生成模型，克服了当前分子生成模型仅依赖少量反应模板的限制。在广泛的实验中，我们的模型生成了高质量的可合成药物类分子结构。

    Chemical reactions are the fundamental building blocks of drug design and organic chemistry research. In recent years, there has been a growing need for a large-scale deep-learning framework that can efficiently capture the basic rules of chemical reactions. In this paper, we have proposed a unified framework that addresses both the reaction representation learning and molecule generation tasks, which allows for a more holistic approach. Inspired by the organic chemistry mechanism, we develop a novel pretraining framework that enables us to incorporate inductive biases into the model. Our framework achieves state-of-the-art results on challenging downstream tasks. By possessing chemical knowledge, this framework can be applied to reaction-based generative models, overcoming the limitations of current molecule generation models that rely on a small number of reaction templates. In the extensive experiments, our model generates synthesizable drug-like structures of high quality. Overall,
    
[^363]: 带吸收的泛洪：复杂网络上异构赌博机的高效协议

    Flooding with Absorption: An Efficient Protocol for Heterogeneous Bandits over Complex Networks. (arXiv:2303.05445v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05445](http://arxiv.org/abs/2303.05445)

    该论文提出了一种名为带吸收的泛洪（FwA）的新协议，用于解决复杂网络上的异构赌博机问题。通过严格的遗憾分析，证明了该协议的有效性。

    

    多臂赌博机广泛用于建模顺序决策，在许多现实应用中如在线推荐系统和无线网络中无处不在。我们考虑一个多代理的场景，每个代理解决自己的赌博机问题，赌博机拥有不同的臂。他们的目标是在通过给定网络的通信协议协作的同时最小化他们的集体遗憾。先前关于此问题的文献只考虑了臂的异质性和网络化代理问题。在这项工作中，我们引入了一个同时包含这两个特性的设置。针对这一新颖的设置，我们首先对标准泛洪协议结合经典的上置信界策略提供了严格的遗憾分析。然后，为了减轻在复杂网络中泛洪造成的高通信成本问题，我们提出了一种新的协议，称为带吸收的泛洪（FwA）。我们对由此产生的遗憾上界进行了理论分析，并讨论了该协议的优点。

    Multi-armed bandits are extensively used to model sequential decision-making, making them ubiquitous in many real-life applications such as online recommender systems and wireless networking. We consider a multi-agent setting where each agent solves their own bandit instance endowed with a different set of arms. Their goal is to minimize their group regret while collaborating via some communication protocol over a given network. Previous literature on this problem only considered arm heterogeneity and networked agents separately. In this work, we introduce a setting that encompasses both features. For this novel setting, we first provide a rigorous regret analysis for a standard flooding protocol combined with the classic UCB policy. Then, to mitigate the issue of high communication costs incurred by flooding in complex networks, we propose a new protocol called Flooding with Absorption (FwA). We provide a theoretical analysis of the resulting regret bound and discuss the advantages of
    
[^364]: 通过内容感知的风格不变模型学习对未知领域进行泛化：用于胸部X射线疾病检测的翻译摘要

    Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays. (arXiv:2302.13991v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.13991](http://arxiv.org/abs/2302.13991)

    通过内容感知的风格不变模型，我们提出了一种解决深度学习医学图像分析中源领域不匹配挑战的方法。我们采用了风格随机化模块来提取既是风格不变又是内容偏好的领域不变特征，在胸部X射线疾病检测中取得了良好的性能。

    

    在基于深度学习的医学图像分析中，由于源领域不匹配而导致性能降低一直是一个长期存在的挑战，特别是在胸部X射线（CXR）领域。为了解决这种领域转移问题，已经提出了一些方法（如对抗训练，多领域混合），用于提取领域不变的高级特征。然而，这些方法并没有明确规范提取的领域不变特征的内容和风格特征。最近的研究表明，CNN模型对风格（例如，无信息的纹理）有很强的偏好，而不是对内容（例如，形状）的偏好，这与人类视觉系统形成鲜明对比。放射科医师倾向于从CXR图像中学习视觉线索，并因此在多个领域中表现良好。因此，在从CXR图像进行病理诊断的医学成像中，模型应该提取既是风格不变又是内容偏好的领域不变特征。受此启发，我们在实验中使用了新颖的风格随机化模块（SRMs）。

    Performance degradation due to source domain mismatch is a longstanding challenge in deep learning-based medical image analysis, particularly for chest X-rays (CXRs). Several methods (e.g., adversarial training, multi-domain mixups) have been proposed to extract domain-invariant high-level features to address this domain shift. However, these methods do not explicitly regularize the content and style characteristics of the extracted domain-invariant features. Recent studies have demonstrated that CNN models exhibit a strong bias toward styles (e.g., uninformative textures) rather than content (e.g., shape), in stark contrast to the human-vision system. Radiologists tend to learn visual cues from CXRs and thus perform well across multiple domains. Therefore, in medical imaging for pathology diagnosis from CXR images, models should extract domain-invariant features that are style-invariant and content-biased. Motivated by this, we employ the novel style randomization modules (SRMs) at bo
    
[^365]: 在线工具变量回归: 遗憾分析和Bandit反馈

    Online Instrumental Variable Regression: Regret Analysis and Bandit Feedback. (arXiv:2302.09357v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09357](http://arxiv.org/abs/2302.09357)

    该论文研究了在线学习中内生性问题的解决方法，提出了使用Two-Stage Least Squares方法的在线变体O2SLS来处理内生性，取得了较好的识别率和预测遗憾率。

    

    内生性是实际数据中常见的现象，因为遗漏变量、战略行为、测量误差等原因导致噪声和协变量之间的依赖性。与之相反，现有的无界噪声和线性Bandit随机在线线性回归分析严重依赖外生性，即噪声和协变量之间的独立性。鉴于这一差距，我们研究了工具变量（IV）回归在随机在线学习中的超识别和恰好识别情况。我们提出使用Two-Stage Least Squares方法的在线变体（即O2SLS）来处理内生性。我们的分析表明，O2SLS实现了$ \mathcal{O} \left(d_x d_z \log ^ 2 T \right)$的识别率和$ \tilde {\mathcal {O}} \left(\gamma \sqrt {d_x T} \right)$的预测遗憾率。

    Endogeneity, i.e. the dependence between noise and covariates, is a common phenomenon in real data due to omitted variables, strategic behaviours, measurement errors etc. In contrast, the existing analyses of stochastic online linear regression with unbounded noise and linear bandits depend heavily on exogeneity, i.e. the independence between noise and covariates. Motivated by this gap, we study the over-and just-identified Instrumental Variable (IV) regression for stochastic online learning. IV regression and the Two-Stage Least Squares approach to it are widely deployed in economics and causal inference to identify the underlying model from an endogenous dataset. Thus, we propose to use an online variant of Two-Stage Least Squares approach, namely O2SLS, to tackle endogeneity in stochastic online learning. Our analysis shows that O2SLS achieves $\mathcal{O}\left(d_x d_z \log ^2 T\right)$ identification and $\tilde{\mathcal{O}}\left(\gamma \sqrt{d_x T}\right)$ oracle regret after $T$ 
    
[^366]: 标签效率的时间序列表示学习：一项综述

    Label-efficient Time Series Representation Learning: A Review. (arXiv:2302.06433v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06433](http://arxiv.org/abs/2302.06433)

    这篇综述介绍了针对时间序列数据中标记数据稀缺性问题的现有方法，并提供了一个新颖的分类系统来归纳这些方法。该综述总结了每种方法的最新进展并提出了未来的研究方向。

    

    标记数据的稀缺性是在现实世界中应用深度学习模型于时间序列数据时的主要挑战之一。因此，最近已经开发了几种方法，例如迁移学习，自监督学习和半监督学习，以促进深度学习模型从有限的时间序列标签中获取学习能力。在本综述中，我们首次提供了一个新颖的分类系统，用来根据它们对外部数据源的依赖，对解决时间序列数据中标记数据稀缺性问题的现有方法进行分类。此外，我们对每种方法的最新进展进行了审查，并总结了当前工作的局限性，并提供了可能在这一领域取得更好进展的未来方向。

    The scarcity of labeled data is one of the main challenges of applying deep learning models on time series data in the real world. Therefore, several approaches, e.g., transfer learning, self-supervised learning, and semi-supervised learning, have been recently developed to promote the learning capability of deep learning models from the limited time series labels. In this survey, for the first time, we provide a novel taxonomy to categorize existing approaches that address the scarcity of labeled data problem in time series data based on their dependency on external data sources. Moreover, we present a review of the recent advances in each approach and conclude the limitations of the current works and provide future directions that could yield better progress in the field.
    
[^367]: 近似拒绝采样的样本复杂度及其在平滑在线学习中的应用

    The Sample Complexity of Approximate Rejection Sampling with Applications to Smoothed Online Learning. (arXiv:2302.04658v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.04658](http://arxiv.org/abs/2302.04658)

    本研究展示了在有界f-散度约束下，近似拒绝采样的样本复杂度可以通过Θ(~(D/f'(n)))函数来表示，并且应用于平滑在线学习中的相关算法的性能依然成立。

    

    假设我们可以访问来自分布μ的n个独立样本，并且我们希望输出其中一个样本，使得输出的分布尽可能接近目标分布ν。在这项工作中，我们展示了在所有具有有界f-散度Df(ν|μ)≤D的ν,μ对中，关于n的最优总变差距离由Θ(~(D/f'(n)))给出。之前，这个问题只研究了ν相对于μ的Radon-Nikodym导数一致有界的情况。我们还考虑了似乎非常不同的平滑在线学习领域的一个应用，我们展示了最小化遗憾和具有oracle效率的算法的遗憾即使在对手有边界f-散度（而不是有界Radon-Nikodym导数）的松弛约束下，仍然成立。最后，我们还研究了在均匀估计中用于平均估计的重要性采样的效果。

    Suppose we are given access to $n$ independent samples from distribution $\mu$ and we wish to output one of them with the goal of making the output distributed as close as possible to a target distribution $\nu$. In this work we show that the optimal total variation distance as a function of $n$ is given by $\tilde\Theta(\frac{D}{f'(n)})$ over the class of all pairs $\nu,\mu$ with a bounded $f$-divergence $D_f(\nu\|\mu)\leq D$. Previously, this question was studied only for the case when the Radon-Nikodym derivative of $\nu$ with respect to $\mu$ is uniformly bounded. We then consider an application in the seemingly very different field of smoothed online learning, where we show that recent results on the minimax regret and the regret of oracle-efficient algorithms still hold even under relaxed constraints on the adversary (to have bounded $f$-divergence, as opposed to bounded Radon-Nikodym derivative). Finally, we also study efficacy of importance sampling for mean estimates uniform o
    
[^368]: 认证悖论: 认证会揭示更好的攻击

    The Certification Paradox: Certifications Admit Better Attacks. (arXiv:2302.04379v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04379](http://arxiv.org/abs/2302.04379)

    本文指出了一个"认证悖论"，认证虽然可以展示模型的稳健性，但额外揭示了有关认证模型的信息也成为新的攻击面，导致更好的攻击效果。

    

    在保证有一个有界区域内不存在对抗样本的情况下，认证机制在展示神经网络的稳健性方面扮演着重要角色。本文提出了一个问题: 认证是否会有任何意想不到的后果，通过揭示有关认证模型的额外信息？我们以肯定的答案回答了这个问题，证明了认证不仅测量模型的稳健性，而且展现了新的攻击面。我们提出了"认证感知攻击"，在针对经过认证的模型进行攻击时，这种攻击会比以前的任何方法更频繁地产生更小的对抗性扰动。我们的攻击实现了最多34%的扰动规范中位数的减小(比较目标和攻击实例)，同时需要的计算时间比PGD等方法少了90%。我们的攻击实现了如此显着的扰动大小和计算成本的降低，突显了以认证作为对抗攻击防御的一种悖论。具体来说，认证不仅揭示了稳健模型的属性，而且还可以用来发起更有效的攻击。

    In guaranteeing that no adversarial examples exist within a bounded region, certification mechanisms play an important role in demonstrating the robustness of neural networks. In this work we ask: Could certifications have any unintended consequences, through exposing additional information about certified models? We answer this question in the affirmative, demonstrating that certifications not only measure model robustness but also present a new attack surface. We propose \emph{Certification Aware Attacks}, that produce smaller adversarial perturbations more than twice as frequently as any prior approach, when launched against certified models. Our attacks achieve an up to $34\%$ reduction in the median perturbation norm (comparing target and attack instances), while requiring $90 \%$ less computational time than approaches like PGD. That our attacks achieve such significant reductions in perturbation size and computational cost highlights an apparent paradox in deploying certificatio
    
[^369]: 一般几何上的黎曼流匹配

    Riemannian Flow Matching on General Geometries. (arXiv:2302.03660v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03660](http://arxiv.org/abs/2302.03660)

    本文提出了一种名为黎曼流匹配的方法，可以在一般几何上训练连续标准化流，并在高维度数据上具有优势。

    

    我们提出了一种名为黎曼流匹配（RFM）的框架，用于在流形上训练连续标准化流。现有的流形生成建模方法要么需要昂贵的模拟，要么无法本质上扩展到高维度，要么使用限制量的近似来产生有偏的训练目标。黎曼流匹配绕过了这些限制，并提供了比以前方法更多的优势：它在简单几何上无需模拟，不需要散度计算，并以闭合形式计算其目标向量场。 RFM的关键因素是构建一个相对简单的前度量，以定义目标向量场，其中包括现有的欧几里得情况。为了扩展到一般几何，我们依靠使用谱分解来有效地即兴计算前度量。我们的方法在现实世界的非欧几里得数据集上实现了最先进的性能，并通过在3D网格和双曲空间上训练标准化流来证明其功效。

    We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on real-world non-Euclidean datasets, and we demonstr
    
[^370]: 质量的尾部

    Quality at the Tail. (arXiv:2212.13925v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13925](http://arxiv.org/abs/2212.13925)

    本研究发现深度学习推理质量存在波动，引入了“尾部质量”的概念来描述这一现象。

    

    对深度学习模型和系统进行基准测试和评估需要一种细致入微的方法，以确保全面评估。在实际应用中，考虑到推理质量和推理时间是至关重要的，特别是在严苛的环境下，要求同时满足两个指标的要求。忽视其中任何一个方面都可能导致严重和不可逆的后果，包括人员伤亡和财产损失。不幸的是，许多研究缺乏对这些指标的全面考虑，通常在理想或宽松条件下进行，从而导致评估方法不完整或不直观。本研究揭示了深度学习推理质量的波动，进一步给基准测试和评估带来了复杂性和挑战。为了更好地描述这一现象，引入了“尾部质量”的概念，表示分布尾部的质量。

    Benchmarking and evaluating deep learning models and systems necessitate a meticulous approach to ensure comprehensive assessment. In practical applications, it is paramount to consider both the inference quality and the inference time, particularly within critical contexts, where stringent requirements demand the simultaneous satisfaction of both metrics. Neglecting either aspect can result in severe and irreversible consequences, including loss of human life and property damage. Unfortunately, many studies lack a comprehensive consideration of these metrics, often conducted under ideal or permissive conditions, thereby leading to incomplete or non-intuitive evaluation methodologies.  This study reveals that deep learning inference quality exhibits fluctuations, which further introduces complications and challenges to the benchmarking and evaluation. To better characterize the phenomenon, the concept of "tail quality" is introduced, which indicates the quality at the tail of distribut
    
[^371]: TREE-G:决策树对抗图神经网络

    TREE-G: Decision Trees Contesting Graph Neural Networks. (arXiv:2207.02760v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.02760](http://arxiv.org/abs/2207.02760)

    TREE-G引入了一种专门针对图数据的修改决策树方法，通过新颖的分裂函数和指针机制，使得决策树能够更好地应用于带有拓扑信息的图结构化数据。

    

    在处理表格数据时，基于决策树的模型是一个流行的选择，因为它们在这些数据类型上具有高准确性、易于应用和可解释性的特点。然而，在处理图结构化数据时，如何有效地应用决策树并将拓扑信息与图的顶点上的表格数据相结合仍不清楚。为了解决这个挑战，我们引入了TREE-G。TREE-G修改了标准决策树，引入了一种专门针对图数据的新型分裂函数。这个分裂函数不仅包括节点特征和拓扑信息，还使用了一种新颖的指针机制，允许分裂节点使用在先前分裂中计算得到的信息。因此，分裂函数能够适应预测任务和当前的图。我们对TREE-G的理论性质进行了分析，并在多个图和顶点预测基准测试上从经验上证明了它的好处。

    When dealing with tabular data, models based on decision trees are a popular choice due to their high accuracy on these data types, their ease of application, and explainability properties. However, when it comes to graph-structured data, it is not clear how to apply them effectively, in a way that incorporates the topological information with the tabular data available on the vertices of the graph. To address this challenge, we introduce TREE-G. TREE-G modifies standard decision trees, by introducing a novel split function that is specialized for graph data. Not only does this split function incorporate the node features and the topological information, but it also uses a novel pointer mechanism that allows split nodes to use information computed in previous splits. Therefore, the split function adapts to the predictive task and the graph at hand. We analyze the theoretical properties of TREE-G and demonstrate its benefits empirically on multiple graph and vertex prediction benchmarks
    
[^372]: 非线性独立分量分析的可辨识性：稀疏性及其它

    On the Identifiability of Nonlinear ICA: Sparsity and Beyond. (arXiv:2206.07751v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.07751](http://arxiv.org/abs/2206.07751)

    本文提出一个新的方法，考虑混合过程的假设，即结构稀疏性，来实现非线性ICA的可识别性，无需辅助变量。

    

    非线性独立分量分析旨在从其可观测的非线性混合中恢复出潜在独立分量。如何使非线性ICA模型可辨识直到某些平凡不确定性是无监督学习中的一个长期问题。最近的突破是将源的标准独立性假设重新定义为在某些辅助变量（例如类标签和/或域/时间索引）给定的条件独立性，作为弱监督或归纳偏置。然而，具有无条件先验的非线性ICA无法从这些发展中受益。我们探索了一条替代路径，并仅考虑混合过程的假设，例如结构稀疏性。我们展示了在这些约束的具体实例下，独立的潜在分量可以从其非线性混合中辨识出来，达到非平凡的非线性ICA可识别性，而无需辅助变量。

    Nonlinear independent component analysis (ICA) aims to recover the underlying independent latent sources from their observable nonlinear mixtures. How to make the nonlinear ICA model identifiable up to certain trivial indeterminacies is a long-standing problem in unsupervised learning. Recent breakthroughs reformulate the standard independence assumption of sources as conditional independence given some auxiliary variables (e.g., class labels and/or domain/time indexes) as weak supervision or inductive bias. However, nonlinear ICA with unconditional priors cannot benefit from such developments. We explore an alternative path and consider only assumptions on the mixing process, such as Structural Sparsity. We show that under specific instantiations of such constraints, the independent latent sources can be identified from their nonlinear mixtures up to a permutation and a component-wise transformation, thus achieving nontrivial identifiability of nonlinear ICA without auxiliary variable
    
[^373]: 具备辅助信息的优化

    Optimization with access to auxiliary information. (arXiv:2206.00395v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00395](http://arxiv.org/abs/2206.00395)

    本文研究了在计算目标函数梯度很昂贵或有限的情况下，给定一些辅助函数的情况下，如何最小化目标函数。作者提出了两种通用的新算法，并证明了这个框架可以受益于目标和辅助信息之间的Hessian相似性假设。

    This paper investigates the fundamental optimization question of minimizing a target function with expensive or limited gradient computation, given access to some auxiliary side function with cheaper or more available gradients. The authors propose two generic new algorithms and prove that this framework can benefit from the Hessian similarity assumption between the target and side information.

    我们研究了基本的优化问题，即在计算目标函数$f(x)$的梯度很昂贵或有限的情况下，给定一些辅助函数$h(x)$的情况下，如何最小化目标函数。这个公式涵盖了许多实际相关的设置，如i）在SGD中重复使用批次，ii）迁移学习，iii）联邦学习，iv）使用压缩模型/丢弃等进行训练。我们提出了两种通用的新算法，适用于所有这些设置，并证明仅使用目标和辅助信息之间的Hessian相似性假设，我们可以从这个框架中受益。

    We investigate the fundamental optimization question of minimizing a target function $f(x)$ whose gradients are expensive to compute or have limited availability, given access to some auxiliary side function $h(x)$ whose gradients are cheap or more available. This formulation captures many settings of practical relevance such as i) re-using batches in SGD, ii) transfer learning, iii) federated learning, iv) training with compressed models/dropout, etc. We propose two generic new algorithms which are applicable in all these settings and prove using only an assumption on the Hessian similarity between the target and side information that we can benefit from this framework.
    
[^374]: 宽泛推荐系统：一种高效的非线性协同过滤方法

    Broad Recommender System: An Efficient Nonlinear Collaborative Filtering Approach. (arXiv:2204.11602v4 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2204.11602](http://arxiv.org/abs/2204.11602)

    本文提出了一种新的宽泛推荐系统(BroadCF)，使用宽泛学习系统(BLS)作为映射函数来学习用户和项目之间的复杂非线性关系，同时通过用户-项评级协同向量预处理程序将原始数据转换为更适合BLS学习的格式。BroadCF的实验结果表明，在用户推荐准确性和效率方面都优于几种最先进的CF方法。

    

    最近，深度神经网络（DNN）被广泛引入到协同过滤（CF）中，以产生更准确的推荐结果，因为它们具有捕获项目和用户之间复杂非线性关系的能力。然而，基于DNN的模型通常遭受高计算复杂性的问题，即消耗非常长的训练时间并存储大量可训练参数。为了解决这些问题，我们提出了一种新的宽泛推荐系统，称为宽泛协同过滤（BroadCF），它是一种高效的非线性协同过滤方法。宽泛学习系统（BLS）被用作映射函数，以学习用户和项目之间的复杂非线性关系，可以避免上述问题，同时实现非常令人满意的推荐性能。但是，将原始评分数据直接馈送到BLS中并不可行。为此，我们提出了一种用户-项评级协同向量预处理程序，将原始数据转换为更适合BLS学习的格式。三个公共数据集上的实验结果表明，我们提出的BroadCF在推荐准确性和效率方面均优于几种最先进的CF方法。

    Recently, Deep Neural Networks (DNNs) have been widely introduced into Collaborative Filtering (CF) to produce more accurate recommendation results due to their capability of capturing the complex nonlinear relationships between items and users.However, the DNNs-based models usually suffer from high computational complexity, i.e., consuming very long training time and storing huge amount of trainable parameters. To address these problems, we propose a new broad recommender system called Broad Collaborative Filtering (BroadCF), which is an efficient nonlinear collaborative filtering approach. Instead of DNNs, Broad Learning System (BLS) is used as a mapping function to learn the complex nonlinear relationships between users and items, which can avoid the above issues while achieving very satisfactory recommendation performance. However, it is not feasible to directly feed the original rating data into BLS. To this end, we propose a user-item rating collaborative vector preprocessing pro
    
[^375]: 测量流形数据的核双样本检验

    Kernel Two-Sample Tests for Manifold Data. (arXiv:2105.03425v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2105.03425](http://arxiv.org/abs/2105.03425)

    本文研究了与最大均值差异（MMD）相关的基于核的双样本检验统计量在测量流形数据时的应用。文章展示了检验水平和功率与核带宽、样本数量和流形内在维度之间的关系，并在特定条件下建立了测试功率下界。

    

    我们在流形数据设置下研究了与最大均值差异（MMD）相关的基于核的双样本检验统计量，假设高维观测数据接近于低维流形。我们表征了测试水平和功率与核带宽、样本数量和流形的内在维度之间的关系。具体地，我们表明，当数据密度支持在一个嵌入到$m$维空间中的$d$维子流形$\mathcal{M}$上时，从服从于一对分布$p$和$q$抽取的数据进行核双样本检验，这对分布$ p $和$q$是具有H\"older阶$\beta$（最高2），样本数量$n$足够大，使得$\Delta_2\gtrsim n^{- {2\beta/(d+4\beta)}}$，其中$\Delta_2$是流形上$p$和$q$之间的平方$L^2$-差异。我们建立了一个足够大且有限$n$的测试功率下界，其中核带宽参数$\gamma$的比例尺度为$n^ {-1/(d+4\beta)}$。

    We present a study of a kernel-based two-sample test statistic related to the Maximum Mean Discrepancy (MMD) in the manifold data setting, assuming that high-dimensional observations are close to a low-dimensional manifold. We characterize the test level and power in relation to the kernel bandwidth, the number of samples, and the intrinsic dimensionality of the manifold. Specifically, we show that when data densities are supported on a $d$-dimensional sub-manifold $\mathcal{M}$ embedded in an $m$-dimensional space, the kernel two-sample test for data sampled from a pair of distributions $p$ and $q$ that are H\"older with order $\beta$ (up to 2) is powerful when the number of samples $n$ is large such that $\Delta_2 \gtrsim n^{- { 2 \beta/( d + 4 \beta ) }}$, where $\Delta_2$ is the squared $L^2$-divergence between $p$ and $q$ on manifold. We establish a lower bound on the test power for finite $n$ that is sufficiently large, where the kernel bandwidth parameter $\gamma$ scales as $n^{
    

