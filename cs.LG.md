# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Are Normalizing Flows the Key to Unlocking the Exponential Mechanism? A Path through the Accuracy-Privacy Ceiling Constraining Differentially Private ML](https://rss.arxiv.org/abs/2311.09200) | 通过使用正则流模型，解决了差分隐私机器学习中历史难题，提高了准确性和隐私保护。 |
| [^2] | [Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning](https://arxiv.org/abs/2403.11996) | 利用生成式人工智能和图算法加速科学发现，揭示论文之间的深入跨学科关系，并提出了新颖的材料设计。 |
| [^3] | [Distinguishing Neighborhood Representations Through Reverse Process of GNNs for Heterophilic Graphs](https://arxiv.org/abs/2403.10543) | 通过GNN的反向传播过程，可以显著改善异质图中节点表示的区分度，并在许多情况下提高分类性能。 |
| [^4] | [Group Privacy Amplification and Unified Amplification by Subsampling for R\'enyi Differential Privacy](https://arxiv.org/abs/2403.04867) | 该论文提出了一个统一的框架，用于为Rényi-DP推导通过子抽样的放大保证，这是首个针对隐私核算方法的框架，也具有独立的重要性。 |
| [^5] | [Effectiveness Assessment of Recent Large Vision-Language Models](https://arxiv.org/abs/2403.04306) | 本文评估了最近出现的大型视觉-语言模型在专业和通用任务中的表现，旨在全面了解这些创新方法的能力。 |
| [^6] | [Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications](https://arxiv.org/abs/2403.03905) | 我们的主要贡献是对于$k$-PCA算法中近似参数退化的边界得到了显著更为精确的界限 |
| [^7] | [ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation](https://arxiv.org/abs/2403.01306) | 提出一种新的度量标准，图像描述具体性，用于评估标题文本的具体性和相关性，以帮助在多模态学习中隔离提供最强信号的最具体样本。 |
| [^8] | [Log Neural Controlled Differential Equations: The Lie Brackets Make a Difference](https://arxiv.org/abs/2402.18512) | Log-NCDEs是一种新颖而有效的训练NCDEs的方法，通过引入Log-ODE方法从粗糙路径研究中近似CDE的解，并在多变量时间序列分类基准上表现出比其他模型更高的准确率。 |
| [^9] | [Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes](https://arxiv.org/abs/2402.18477) | 本文在随机过程中开发了一种基于签名核的条件独立性测试，实现了对因果关系的推断，以及开发了约束条件的因果发现算法用于恢复整个有向图。 |
| [^10] | [reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use](https://arxiv.org/abs/2402.17739) | reBandit是一种在线RL算法，利用随机效应和贝叶斯先验快速高效地学习，在移动健康环境中通过个性化干预来减少新兴成年人的大麻使用 |
| [^11] | [Batched Nonparametric Contextual Bandits](https://arxiv.org/abs/2402.17732) | 该论文研究了批处理约束下的非参数上下文臂问题，提出了一种名为BaSEDB的方案，在动态分割协变量空间的同时，实现了最优的后悔。 |
| [^12] | [Hybrid Square Neural ODE Causal Modeling](https://arxiv.org/abs/2402.17233) | 混合模型将基于ODE的机械动力学与神经网络组件结合，在解释性和因果基础的同时，利用领域知识对治疗效果进行排名，从而解决灵活性增加带来的因果基础丢失问题。 |
| [^13] | [Open Ad Hoc Teamwork with Cooperative Game Theory](https://arxiv.org/abs/2402.15259) | 提出了采用合作博弈论解释开放式即兴团队合作中联合Q值表示的新理论，为进一步发展这一研究方向和应用提供了新思路 |
| [^14] | [Clifford-Steerable Convolutional Neural Networks](https://arxiv.org/abs/2402.14730) | 提出了Clifford-Steerable卷积神经网络（CS-CNNs），通过在伪欧几里德空间上处理多矢场，利用Clifford群等变神经网络对$\mathrm{O}(p,q)$可导核进行隐式参数化，显着且一致地优于流体动力学和相对论电动力学预测任务的基准方法 |
| [^15] | [Proving membership in LLM pretraining data via data watermarks](https://arxiv.org/abs/2402.10892) | 使用数据水印在LLM预训练中检测版权持有人作品的方法，可以进行合理检测且提供误检率保证，研究了水印设计对假设检验能力的影响以及在模型和数据集缩放下的检测强度变化。 |
| [^16] | [Persuading a Learning Agent](https://arxiv.org/abs/2402.09721) | 在一个重复的贝叶斯说服问题中，即使没有承诺能力，委托人可以通过使用上下文无遗憾学习算法来实现与经典无学习模型中具有承诺的委托人的最优效用无限接近的效果；在代理人使用上下文无交换遗憾学习算法的情况下，委托人无法获得比具有承诺的无学习模型中的最优效用更高的效用。 |
| [^17] | [Reconstructing the Geometry of Random Geometric Graphs](https://arxiv.org/abs/2402.09591) | 该论文通过在底层空间中采样的图来有效地重构随机几何图的几何形状。该方法基于流形假设，即底层空间是低维流形，并且连接概率是嵌入在$\mathbb{R}^N$中的流形中点之间欧几里德距离的严格递减函数。 |
| [^18] | [Online Structured Prediction with Fenchel--Young Losses and Improved Surrogate Regret for Online Multiclass Classification with Logistic Loss](https://arxiv.org/abs/2402.08180) | 这项研究扩展了在线结构化预测的替代后悔度界限，通过引入Fenchel-Young损失和随机解码方案，使得在在线多类分类和逻辑损失下获得了更好的结果。 |
| [^19] | [Refined Sample Complexity for Markov Games with Independent Linear Function Approximation](https://arxiv.org/abs/2402.07082) | 本文在独立线性函数逼近的马尔科夫博弈中，通过改进AVLPR框架，提出了基于数据依赖的悲观估计方法，解决了多智能体的诅咒问题。 |
| [^20] | [Large Language Model Meets Graph Neural Network in Knowledge Distillation](https://arxiv.org/abs/2402.05894) | 本论文提出了一种新颖的图知识蒸馏框架，使用大规模语言模型作为教师模型、图神经网络作为学生模型，解决了在理解文本-属性图中的节点分类问题中的限制。 |
| [^21] | [Room transfer function reconstruction using complex-valued neural networks and irregularly distributed microphones](https://arxiv.org/abs/2402.04866) | 本研究使用复值神经网络在有限的麦克风数据中估计房间传递函数，实现了房间传递函数的重建，创新之处在于首次使用了复值神经网络来估计房间传递函数。 |
| [^22] | [How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning](https://arxiv.org/abs/2402.02872) | 本论文探索了大型语言模型如何进行上下文学习的机制，提出了一个使用定位和投影方法的假设。通过查询和键矩阵来计算输入文本与每个演示之间的注意力权重，以学习它们之间的相似度度量。实验证明了我们的分析。 |
| [^23] | [A Latent Space Metric for Enhancing Prediction Confidence in Earth Observation Data](https://arxiv.org/abs/2401.17342) | 这项研究提出了一种新的方法来估计利用地球观测数据进行回归任务时机器学习模型预测的置信度。通过利用潜在空间表示来推导置信度度量，建立了潜在表示中的欧几里得距离与个体蚊子种群预测的绝对误差之间的相关性。该方法在意大利威尼托地区和德国上莱茵河谷的地区得到了验证，并表现出较高的可靠性和可信度。 |
| [^24] | [On the convergence of loss and uncertainty-based active learning algorithms](https://arxiv.org/abs/2312.13927) | 论文考虑了损失和不确定性基础的主动学习算法在线性分类器和线性可分数据集上的收敛速度，提出了一种新算法并展示了其效率。 |
| [^25] | [diff History for Neural Language Agents](https://arxiv.org/abs/2312.07540) | 本文介绍了一种名为diff历史的简单且高效的解决方案，用于将环境中的观测转换为文本提示，以便对于长期推理决策的任务中的Neural Language Models进行优化。 |
| [^26] | [Efficient Reinforcement Learning from Partial Observability](https://arxiv.org/abs/2311.12244) | 该论文提出了一种基于表示的方法，用于从部分观测中进行有效的强化学习。该方法能够处理部分可观测性带来的计算和统计挑战，并在各种基准测试中展现出优于先进算法的性能。 |
| [^27] | [Ask Again, Then Fail: Large Language Models' Vacillations in Judgement](https://arxiv.org/abs/2310.02174) | 目前的语言模型在面对后续问题时常常摇摆不定，研究者提出了一个后续问题机制和两个度量标准来量化这种不一致性，并开发出Unwavering-FQ框架来教导模型保持最初的正确判断，实验证明其有效性。 |
| [^28] | [UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming](https://arxiv.org/abs/2307.16375) | UniAP是一种新型的自动并行化方法，通过混合整数二次规划统一跨层和内层的自动并行化。与现有方法相比，UniAP在吞吐量方面表现更好，并且减少了策略优化时间。 |
| [^29] | [Noise-adaptive (Accelerated) Stochastic Heavy-Ball Momentum.](http://arxiv.org/abs/2401.06738) | 本研究分析了在光滑、强凸环境中随机重力球动量的收敛性，并证明了当批量大小大于某个阈值时，该方法可以实现加速收敛速度。针对强凸二次函数，我们建议了一种噪声自适应的多阶段方法，可以使收敛速度进一步提高。实验结果验证了该方法的有效性。 |
| [^30] | [RudolfV: A Foundation Model by Pathologists for Pathologists.](http://arxiv.org/abs/2401.04079) | 本文提出了一种由病理学家为病理学家构建的基础模型，通过数据整理和融入病理学领域知识，扩展了数字病理学全玻片图像基础模型，解决了人工智能在处理罕见疾病方面的挑战。 |
| [^31] | [Fun with Flags: Robust Principal Directions via Flag Manifolds.](http://arxiv.org/abs/2401.04071) | 本研究提出了一种统一的PCA和其变种框架，该框架基于线性子空间旗帜，并引入了对异常值和数据流形的考虑。通过在旗帜流形上进行优化问题的求解，结合主测地线近似，提出了一系列新的降维算法。 |
| [^32] | [Tailoring Mixup to Data using Kernel Warping functions.](http://arxiv.org/abs/2311.01434) | 本研究提出了一种利用核扭曲函数对Mixup数据进行个性化处理的方法，通过动态改变插值系数的概率分布来实现更频繁和更强烈的混合相似数据点。实验证明这种方法不仅提高了模型性能，还提高了模型的校准性。 |
| [^33] | [Unreading Race: Purging Protected Features from Chest X-ray Embeddings.](http://arxiv.org/abs/2311.01349) | 论文的目的是利用正交化方法消除胸部X射线嵌入中的保护特征影响，并证明其有效性。研究结果表明保护特征对病理预测有显著影响，而应用正交化方法可以消除这些影响。 |
| [^34] | [The statistical thermodynamics of generative diffusion models.](http://arxiv.org/abs/2310.17467) | 本文通过在生成性扩散模型中应用平衡统计力学的工具，揭示了这些模型中的二阶相变现象，并且认为这种稳定性形式是生成能力的关键。 |
| [^35] | [SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding.](http://arxiv.org/abs/2310.15308) | 该论文提出了一种将视觉基础模型合并为一个统一模型的方法，通过集成多任务学习、持续学习技术和师生蒸馏，实现了显著较少的计算成本和较少的预训练数据需求。通过应用该方法于SAM和CLIP，得到了一个统一模型SAM-CLIP，将两者的优势融合在一起。 |
| [^36] | [Fundamental Limits of Membership Inference Attacks on Machine Learning Models.](http://arxiv.org/abs/2310.13786) | 本文探讨了机器学习模型上成员推断攻击的基本限制，包括推导了效果和成功率的统计量，并提供了几种情况下的界限。这使得我们能够根据样本数量和其他结构参数推断潜在攻击的准确性。 |
| [^37] | [On the Evaluation of Generative Models in Distributed Learning Tasks.](http://arxiv.org/abs/2310.11714) | 本文研究了在具有异构数据分布的分布式学习任务中评估生成模型。通过研究Fr\'echet inception距离（FID），并考虑不同聚合分数，发现FID-all和FID-avg分数的模型排名可能不一致。 |
| [^38] | [MAD Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems.](http://arxiv.org/abs/2310.02784) | 该研究提出了一个性能建模框架，在分布式系统上实现了大规模机器学习模型的加速，获得了2.24倍和5.27倍的吞吐量提升潜力。 |
| [^39] | [Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes.](http://arxiv.org/abs/2309.17207) | 本研究提出了记忆健身房，一种用于测试利用记忆为基础的深度强化学习智能体能力的基准。它包括部分可观察的二维环境和离散控制，并通过无尽任务对记忆能力、噪声抗性和泛化能力进行评估。研究还提供了一个使用Transformer-XL和Proximal Policy Optimization驱动的实现。 |
| [^40] | [From Classification to Segmentation with Explainable AI: A Study on Crack Detection and Growth Monitoring.](http://arxiv.org/abs/2309.11267) | 本研究提出了一种将可解释人工智能应用于裂缝分割和监测的方法，通过从分类器的解释中导出分割结果，无需像素级注释。实验结果表明，该方法能有效进行裂缝分割和生长监测。 |
| [^41] | [Learning End-to-End Channel Coding with Diffusion Models.](http://arxiv.org/abs/2309.10505) | 本文提出了使用扩散模型学习端到端信道编码的框架，并通过模拟实验证明了扩散模型能够准确学习信道分布从而实现接近最优的端到端符号误码率。 |
| [^42] | [Toward efficient resource utilization at edge nodes in federated learning.](http://arxiv.org/abs/2309.10367) | 本论文提出了一种受迁移学习启发的策略，旨在通过减少设备上的资源利用、以及减少服务器和网络的负载，提高联邦学习中边缘节点的效率。 |
| [^43] | [Exploring Meta Information for Audio-based Zero-shot Bird Classification.](http://arxiv.org/abs/2309.08398) | 该研究探索了如何利用元信息来改善基于音频的零样本鸟类分类，并通过连接不同的元数据和音频特征获得最佳结果。 |
| [^44] | [A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining.](http://arxiv.org/abs/2309.04761) | 本调研综合审查了在教育数据挖掘中深度学习技术的最新研究进展，包括对知识跟踪、学生不良行为检测、性能预测和个性化推荐等典型教育场景的应用。同时提供了公共数据集和处理工具的综合概述，并指出了未来的研究方向。 |
| [^45] | [Occlusion-Aware Deep Convolutional Neural Network via Homogeneous Tanh-transforms for Face Parsing.](http://arxiv.org/abs/2308.15323) | 本论文提出了一种通过均匀的Tanh变换进行面部解析的遮挡感知深度卷积神经网络。该方法解决了面部遮挡问题，并且能够融合更多上下文信息。同时，引入了遮挡感知损失，提高了边界的识别能力。 |
| [^46] | [Spectral Estimators for Structured Generalized Linear Models via Approximate Message Passing.](http://arxiv.org/abs/2308.14507) | 本论文研究了针对广义线性模型的参数估计问题，提出了一种通过谱估计器进行预处理的方法。通过对测量进行特征协方差矩阵Σ表示，分析了谱估计器在结构化设计中的性能，并确定了最优预处理以最小化样本数量。 |
| [^47] | [Unlocking Carbon Reduction Potential with Reinforcement Learning for the Three-Dimensional Loading Capacitated Vehicle Routing Problem.](http://arxiv.org/abs/2307.12136) | 该论文提出了一个利用强化学习方法解决三维装载有限车辆路径问题的模型，该方法能够在线性时间内有效计算共载和路径的可行解决方案，从而释放了碳减排的潜力。 |
| [^48] | [TinyTrain: Deep Neural Network Training at the Extreme Edge.](http://arxiv.org/abs/2307.09988) | TinyTrain是一种在设备上进行训练的方法，通过选择性更新模型的部分并处理数据稀缺问题，大大缩短了训练时间。通过任务自适应的稀疏更新方法，TinyTrain能够在高准确性的同时减小计算和内存占用，对未知任务表现出色。 |
| [^49] | [Graph Positional and Structural Encoder.](http://arxiv.org/abs/2307.07107) | 这是一个关于图位置和结构编码器的研究，提出了图位置和结构编码器（GPSE），它能有效地捕捉多个PSE的共同潜在表示，并在各种图预测任务中取得显著的性能提升。 |
| [^50] | [S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction.](http://arxiv.org/abs/2307.06701) | S-HR-VQVAE是一种序列分层残差学习向量量化变分自编码器，通过结合分层残差向量量化变分自编码器（HR-VQVAE）和时空PixelCNN（ST-PixelCNN）的能力，解决了视频预测中的主要挑战，并在KTH人体动作和Moving-MNIST任务上取得了较好的实验结果。 |
| [^51] | [FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods.](http://arxiv.org/abs/2306.09468) | 本文提出了针对处理中组公平方法的公平公正基准框架（FFB），并进行了全面分析。该工作的关键贡献包括提供灵活、可扩展、极简和面向研究的开源代码；建立统一的公平方法基准测试流水线；进行广泛的基准测试，从 $\mathbf{45,079}$ 个实验中获取关键见解。 |
| [^52] | [ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer.](http://arxiv.org/abs/2306.06446) | ShiftAddViT通过使用位移和加法等多种乘法原语对ViT进行重新参数化，实现了减少乘法操作的高效视觉变换器模型，可以在GPU上实现端到端的推理加速，无需从头训练。 |
| [^53] | [Analysis, Identification and Prediction of Parkinson's disease sub-types and progression through Machine Learning.](http://arxiv.org/abs/2306.04748) | 本文使用监督和无监督机器学习方法鉴别出三种不同的帕金森病亚型，并生成患者个性化的症状进展预测，有助于制定针对性干预和改善患者预后。 |
| [^54] | [Sparsity in neural networks can improve their privacy.](http://arxiv.org/abs/2304.10553) | 稀疏性能够提高神经网络的隐私，并且能够保持网络的表现 |
| [^55] | [Contrastive Learning under Heterophily.](http://arxiv.org/abs/2303.06344) | 本文提出了第一个图形对比学习方法，以解决现有图形对比学习方法在异质性下无法学习高质量表示的问题。 |
| [^56] | [Data-dependent Generalization Bounds via Variable-Size Compressibility.](http://arxiv.org/abs/2303.05369) | 本文通过引入可变大小压缩性框架，建立了一种新的数据相关的泛化误差上界。该方法将算法的泛化误差与其输入数据的可变大小压缩率相关联，并提供了依赖于经验分布而非未知分布的界限。此外，该方法还可以推导出输入数据和输出假设随机变量的任何函数的泛化界限，并包含并可能优于现有的基于PAC-Bayes和数据相关内在维度的界限。 |
| [^57] | [FairShap: A Data Re-weighting Approach for Algorithmic Fairness based on Shapley Values.](http://arxiv.org/abs/2303.01928) | FairShap是一种通过数据重估计算法决策公平性的预处理方法，使用Shapley值估值，能够提高模型的公平性和精确性，并且易于解释。 |
| [^58] | [Label Alignment Regularization for Distribution Shift.](http://arxiv.org/abs/2211.14960) | 这篇论文提出了一种用于无监督领域自适应的正则化方法，通过鼓励目标域中的预测与其前几个奇异向量对齐来实现。与传统方法不同的是，这个方法通过正则化分类器与无监督目标数据对齐，而不是正则化表示。通过消除对最优联合风险假设的依赖，该方法展示了很好的效果。 |
| [^59] | [A Survey on Graph Counterfactual Explanations: Definitions, Methods, Evaluation.](http://arxiv.org/abs/2210.12089) | 这篇综述研究了图形反事实解释的概念、方法、评估及其应用于图神经网络的情况，提供了分类法、统一的符号表示、基准数据集和评估指标，并对十四种方法、二十二个数据集和十九个指标进行了讨论和整合。未来的工作主要集中在解决开放的挑战上。 |

# 详细

[^1]: 正则流是否是解锁指数机制的关键？经过准确性和隐私双重约束的差分隐私机器学习的一条路径

    Are Normalizing Flows the Key to Unlocking the Exponential Mechanism? A Path through the Accuracy-Privacy Ceiling Constraining Differentially Private ML

    [https://rss.arxiv.org/abs/2311.09200](https://rss.arxiv.org/abs/2311.09200)

    通过使用正则流模型，解决了差分隐私机器学习中历史难题，提高了准确性和隐私保护。

    

    当前的差分隐私机器学习（ML）的最先进且事实标准是差分隐私随机梯度下降（DPSGD）。然而，这种方法本质上是浪费的。通过向每个梯度添加噪声，它会在每个梯度步骤中降低整体隐私。尽管经过15年的丰富研究，推进了组合定理、子采样方法和实现技术，但当前的隐私机器学习方法往往无法达到足够的准确性和隐私保护。与此同时，为了私下优化而设计的指数机制（ExpM）历来被排除在现代机器学习算法的私下训练之外，主要是因为ExpM需要从一种历来难以处理的密度中进行采样。尽管最近发现了正则流模型（NFs），这是一种用于逼近难以处理分布的表达深度网络，但ExpM仍然处于背景中。我们的观点是利用正则流来绕过ExpM的历史障碍是一个潜在的方法。

    The state of the art and de facto standard for differentially private machine learning (ML) is differentially private stochastic gradient descent (DPSGD). Yet, the method is inherently wasteful. By adding noise to every gradient, it diminishes the overall privacy with every gradient step. Despite 15 years of fruitful research advancing the composition theorems, sub-sampling methods, and implementation techniques, adequate accuracy and privacy is often unattainable with current private ML methods. Meanwhile, the Exponential Mechanism (ExpM), designed for private optimization, has been historically sidelined from privately training modern ML algorithms primarily because ExpM requires sampling from a historically intractable density. Despite the recent discovery of Normalizing Flow models (NFs), expressive deep networks for approximating intractable distributions, ExpM remains in the background. Our position is that leveraging NFs to circumvent historic obstructions of ExpM is a potential
    
[^2]: 利用生成式知识提取、基于图的表示和多模态智能图推理加速科学发现

    Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning

    [https://arxiv.org/abs/2403.11996](https://arxiv.org/abs/2403.11996)

    利用生成式人工智能和图算法加速科学发现，揭示论文之间的深入跨学科关系，并提出了新颖的材料设计。

    

    利用生成式人工智能，我们将一组涉及生物材料领域的1,000篇科学论文转化为详细的本体知识图，揭示了它们固有的无标度特性。通过基于节点相似性和介数中心性的组合排名，探测不同概念之间的图遍历路径，我们揭示了深入的跨学科关系，可用于回答查询，识别知识中的空白，并提出前所未见的材料设计及其行为。一项比较揭示了生物材料和贝多芬第九交响曲之间的详细结构相似之处，突显了通过同构映射共享复杂性模式。该算法进一步创建了一种创新的基于分级菌丝体的复合材料，将图采样的联合合成原理与康定斯基《第七组成》中提取的原则相结合

    arXiv:2403.11996v1 Announce Type: cross  Abstract: Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature. Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII p
    
[^3]: 通过GNN的反向过程区分异质图中的邻居表示

    Distinguishing Neighborhood Representations Through Reverse Process of GNNs for Heterophilic Graphs

    [https://arxiv.org/abs/2403.10543](https://arxiv.org/abs/2403.10543)

    通过GNN的反向传播过程，可以显著改善异质图中节点表示的区分度，并在许多情况下提高分类性能。

    

    Graph Neural Network（GNN）类似于扩散过程，在堆叠许多层时导致学习表示过度平滑。因此，消息传递的反向过程可以通过反转正向消息传播来锐化节点表示。锐化后的表示可以帮助我们更好地区分具有不同标签的邻居节点，例如在异质图中。在这项工作中，我们将反向过程的设计原则应用于GNN的三个变体。通过在异质图数据上的实验，其中相邻节点需要具有不同表示才能成功分类，我们展示了反向过程在许多情况下显着提高了预测性能。进一步分析表明，反向机制可以减轻数百层上的过度平滑。

    arXiv:2403.10543v1 Announce Type: cross  Abstract: Graph Neural Network (GNN) resembles the diffusion process, leading to the over-smoothing of learned representations when stacking many layers. Hence, the reverse process of message passing can sharpen the node representations by inverting the forward message propagation. The sharpened representations can help us to better distinguish neighboring nodes with different labels, such as in heterophilic graphs. In this work, we apply the design principle of the reverse process to the three variants of the GNNs. Through the experiments on heterophilic graph data, where adjacent nodes need to have different representations for successful classification, we show that the reverse process significantly improves the prediction performance in many cases. Additional analysis reveals that the reverse mechanism can mitigate the over-smoothing over hundreds of layers.
    
[^4]: 组隐私放大和子抽样的Rényi差分隐私统一放大

    Group Privacy Amplification and Unified Amplification by Subsampling for R\'enyi Differential Privacy

    [https://arxiv.org/abs/2403.04867](https://arxiv.org/abs/2403.04867)

    该论文提出了一个统一的框架，用于为Rényi-DP推导通过子抽样的放大保证，这是首个针对隐私核算方法的框架，也具有独立的重要性。

    

    差分隐私(DP)具有多种理想属性，如对后处理的鲁棒性、组隐私和通过子抽样放大，这些属性可以相互独立推导。我们的目标是确定是否通过联合考虑这些属性中的多个可以获得更强的隐私保证。为此，我们专注于组隐私和通过子抽样放大的组合。为了提供适合机器学习算法的保证，我们在Rényi-DP框架中进行了分析，这比$(\epsilon,\delta)$-DP具有更有利的组合属性。作为这个分析的一部分，我们开发了一个统一的框架，用于为Rényi-DP推导通过子抽样的放大保证，这是首个针对隐私核算方法的框架，也具有独立的重要性。我们发现，它不仅让我们改进和泛化现有的放大结果。

    arXiv:2403.04867v1 Announce Type: cross  Abstract: Differential privacy (DP) has various desirable properties, such as robustness to post-processing, group privacy, and amplification by subsampling, which can be derived independently of each other. Our goal is to determine whether stronger privacy guarantees can be obtained by considering multiple of these properties jointly. To this end, we focus on the combination of group privacy and amplification by subsampling. To provide guarantees that are amenable to machine learning algorithms, we conduct our analysis in the framework of R\'enyi-DP, which has more favorable composition properties than $(\epsilon,\delta)$-DP. As part of this analysis, we develop a unified framework for deriving amplification by subsampling guarantees for R\'enyi-DP, which represents the first such framework for a privacy accounting method and is of independent interest. We find that it not only lets us improve upon and generalize existing amplification results 
    
[^5]: 最近大型视觉-语言模型的有效性评估

    Effectiveness Assessment of Recent Large Vision-Language Models

    [https://arxiv.org/abs/2403.04306](https://arxiv.org/abs/2403.04306)

    本文评估了最近出现的大型视觉-语言模型在专业和通用任务中的表现，旨在全面了解这些创新方法的能力。

    

    大型视觉-语言模型(LVLMs)的出现代表着迈向人工通用智能的重要进步。然而，它们在专业和通用任务中的有效性程度需要进一步调查。本文旨在评估流行的LVLMs在专业和通用任务中的能力，旨在提供对这些创新方法的全面理解。为了评估它们在专业任务中的有效性，我们量身定制了一个包含自然、医疗和工业三种不同场景的全面测试平台，涵盖六项具有挑战性的任务。这些任务包括显著、伪装和透明物体检测，以及息肉和皮肤病变检测，以及工业异常检测。我们检验了最近三种开源LVLMs--MiniGPT-v2、LLaVA-1.5和Shikra--在视觉识别和定位领域的表现。

    arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
    
[^6]: 黑盒$k$-to-$1$-PCA降维：理论与应用

    Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications

    [https://arxiv.org/abs/2403.03905](https://arxiv.org/abs/2403.03905)

    我们的主要贡献是对于$k$-PCA算法中近似参数退化的边界得到了显著更为精确的界限

    

    $k$-主成分分析（$k$-PCA）问题是一种基本的算法原语，在数据分析和降维应用中被广泛使用。在统计环境中，$k$-PCA的目标是识别一个分布的协方差矩阵的顶部特征空间，我们只能通过样本隐式访问这个矩阵。受这些隐式设置的启发，我们分析黑盒缩减方法作为设计$k$-PCA算法的框架，其中我们通过黑盒$1$-PCA预言模拟对未知目标矩阵的访问，该预言返回一个近似的顶部特征向量，根据两个流行的近似概念。尽管这种黑盒方法可能是设计$k$-PCA算法中最自然的基于降维的方法，这种方法，即通过递归调用$1$-PCA预言调用了$k$次，以前很难理解。

    arXiv:2403.03905v1 Announce Type: cross  Abstract: The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples. Motivated by these implicit settings, we analyze black-box deflation methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a black-box $1$-PCA oracle which returns an approximate top eigenvector, under two popular notions of approximation. Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such black-box methods, which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood.   Our main contribution is significantly sharper bounds on the approximation parameter degradation of
    
[^7]: ICC：用于多模态数据集筛选的图像描述具体性量化

    ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation

    [https://arxiv.org/abs/2403.01306](https://arxiv.org/abs/2403.01306)

    提出一种新的度量标准，图像描述具体性，用于评估标题文本的具体性和相关性，以帮助在多模态学习中隔离提供最强信号的最具体样本。

    

    arXiv:2403.01306v1 公告类型：新摘要：针对配对文本-图像数据的Web规模训练在多模态学习中变得越来越重要，但挑战在野外数据集的高噪声特性。标准数据过滤方法成功去除了不匹配的文本-图像对，但允许语义相关但非常抽象或主观的文本。这些方法缺乏细粒度的能力来隔离提供在嘈杂数据集中学习最强信号的最具体样本。在这项工作中，我们提出了一种新的度量标准，图像描述具体性，评估没有图像参考的标题文本以衡量其具体性和相关性，以供在多模态学习中使用。我们的方法利用了衡量视觉-语义信息损失的强基础模型来进行评估。我们证明了这与人类对单词和句子级文本具体性的评估高度相关。此外，我们展示了...

    arXiv:2403.01306v1 Announce Type: new  Abstract: Web-scale training on paired text-image data is becoming increasingly central to multimodal learning, but is challenged by the highly noisy nature of datasets in the wild. Standard data filtering approaches succeed in removing mismatched text-image pairs, but permit semantically related but highly abstract or subjective text. These approaches lack the fine-grained ability to isolate the most concrete samples that provide the strongest signal for learning in a noisy dataset. In this work, we propose a new metric, image caption concreteness, that evaluates caption text without an image reference to measure its concreteness and relevancy for use in multimodal learning. Our approach leverages strong foundation models for measuring visual-semantic information loss in multimodal representations. We demonstrate that this strongly correlates with human evaluation of concreteness in both single-word and sentence-level texts. Moreover, we show tha
    
[^8]: Log神经控制微分方程：李括号的差异

    Log Neural Controlled Differential Equations: The Lie Brackets Make a Difference

    [https://arxiv.org/abs/2402.18512](https://arxiv.org/abs/2402.18512)

    Log-NCDEs是一种新颖而有效的训练NCDEs的方法，通过引入Log-ODE方法从粗糙路径研究中近似CDE的解，并在多变量时间序列分类基准上表现出比其他模型更高的准确率。

    

    受控微分方程（CDE）的矢量场描述了控制路径与解路径演化之间的关系。神经CDE（NCDE）将时间序列数据视为对控制路径的观测，使用神经网络对CDE的矢量场进行参数化，并将解路径作为持续演化的隐藏状态。由于其构造使其能够抵抗不规则采样率，NCDE是建模现实世界数据的强大方法。在神经粗糙微分方程（NRDE）的基础上，我们引入了Log-NCDE，这是一种训练NCDE的新颖且有效的方法。Log-NCDE的核心组件是Log-ODE方法，这是从粗糙路径研究中的一种用于近似CDE解的工具。在一系列多变量时间序列分类基准上，展示了Log-NCDE比NCDE，NRDE和两种最先进模型S5和线性递归模型具有更高的平均测试集准确率。

    arXiv:2402.18512v1 Announce Type: new  Abstract: The vector field of a controlled differential equation (CDE) describes the relationship between a control path and the evolution of a solution path. Neural CDEs (NCDEs) treat time series data as observations from a control path, parameterise a CDE's vector field using a neural network, and use the solution path as a continuously evolving hidden state. As their formulation makes them robust to irregular sampling rates, NCDEs are a powerful approach for modelling real-world data. Building on neural rough differential equations (NRDEs), we introduce Log-NCDEs, a novel and effective method for training NCDEs. The core component of Log-NCDEs is the Log-ODE method, a tool from the study of rough paths for approximating a CDE's solution. On a range of multivariate time series classification benchmarks, Log-NCDEs are shown to achieve a higher average test set accuracy than NCDEs, NRDEs, and two state-of-the-art models, S5 and the linear recurren
    
[^9]: 在因果发现中的签名核条件独立性测试用于随机过程

    Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes

    [https://arxiv.org/abs/2402.18477](https://arxiv.org/abs/2402.18477)

    本文在随机过程中开发了一种基于签名核的条件独立性测试，实现了对因果关系的推断，以及开发了约束条件的因果发现算法用于恢复整个有向图。

    

    从观测数据中推断随机动力系统背后的因果结构在科学、健康和金融等领域具有巨大潜力。本文通过利用最近签名核技术的进展，开发了一种基于内核的“路径空间”上条件独立性（CI）测试，用于随机微分方程的解。我们展示了相较于现有方法，在路径空间上，我们提出的CI测试表现出严格更好的性能。此外，我们还为非循环随机动力系统开发了基于约束的因果发现算法，利用时间信息来恢复整个有向图。在假设忠实性和CI预言机的情况下，我们的算法是完备且正确的。

    arXiv:2402.18477v1 Announce Type: cross  Abstract: Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via "which variables enter the differential of which other variables". In this paper, we develop a kernel-based test of conditional independence (CI) on "path-space" -- solutions to SDEs -- by leveraging recent advances in signature kernels. We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space. Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed graph. Assuming faithfulness and a CI oracle, our algorithm is sound and complete. We empirical
    
[^10]: reBandit：基于随机效应的在线RL算法用于减少大麻使用

    reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use

    [https://arxiv.org/abs/2402.17739](https://arxiv.org/abs/2402.17739)

    reBandit是一种在线RL算法，利用随机效应和贝叶斯先验快速高效地学习，在移动健康环境中通过个性化干预来减少新兴成年人的大麻使用

    

    大麻使用及相关的大麻使用障碍（CUD）的不断增加在全球范围内构成了一个重大的公共卫生挑战。尤其是在新兴成年人（18-25岁）中，存在明显的治疗缺口，因此解决大麻使用和CUD仍然是2030年联合国可持续发展目标（SDG）中的一个关键目标。在这项工作中，我们开发了一种名为reBandit的在线强化学习（RL）算法，将其应用于移动健康研究中，旨在通过提供个性化移动健康干预来减少新兴成年人的大麻使用。reBandit利用随机效应和信息丰富的贝叶斯先验以在嘈杂的移动健康环境中快速而有效地学习。此外，reBandit采用经验贝叶斯和优化技术来在线自主更新其超参数。为了评估我们算法的性能，我们利用数据构建了一个模拟测试平台

    arXiv:2402.17739v1 Announce Type: new  Abstract: The escalating prevalence of cannabis use, and associated cannabis-use disorder (CUD), poses a significant public health challenge globally. With a notably wide treatment gap, especially among emerging adults (EAs; ages 18-25), addressing cannabis use and CUD remains a pivotal objective within the 2030 United Nations Agenda for Sustainable Development Goals (SDG). In this work, we develop an online reinforcement learning (RL) algorithm called reBandit which will be utilized in a mobile health study to deliver personalized mobile health interventions aimed at reducing cannabis use among EAs. reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments. Moreover, reBandit employs Empirical Bayes and optimization techniques to autonomously update its hyper-parameters online. To evaluate the performance of our algorithm, we construct a simulation testbed using data from
    
[^11]: 批处理非参数上下文臂

    Batched Nonparametric Contextual Bandits

    [https://arxiv.org/abs/2402.17732](https://arxiv.org/abs/2402.17732)

    该论文研究了批处理约束下的非参数上下文臂问题，提出了一种名为BaSEDB的方案，在动态分割协变量空间的同时，实现了最优的后悔。

    

    我们研究了在批处理约束下的非参数上下文臂问题，在这种情况下，每个动作的期望奖励被建模为协变量的平滑函数，并且策略更新是在每个Observations批次结束时进行的。我们为这种设置建立了一个最小化后悔的下限，并提出了一种名为Batched Successive Elimination with Dynamic Binning（BaSEDB）的方案，可以实现最优的后悔（达到对数因子）。实质上，BaSEDB动态地将协变量空间分割成更小的箱子，并仔细调整它们的宽度以符合批次大小。我们还展示了在批处理约束下静态分箱的非最优性，突出了动态分箱的必要性。另外，我们的结果表明，在完全在线设置中，几乎恒定数量的策略更新可以达到最佳后悔。

    arXiv:2402.17732v1 Announce Type: cross  Abstract: We study nonparametric contextual bandits under batch constraints, where the expected reward for each action is modeled as a smooth function of covariates, and the policy updates are made at the end of each batch of observations. We establish a minimax regret lower bound for this setting and propose Batched Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal regret (up to logarithmic factors). In essence, BaSEDB dynamically splits the covariate space into smaller bins, carefully aligning their widths with the batch size. We also show the suboptimality of static binning under batch constraints, highlighting the necessity of dynamic binning. Additionally, our results suggest that a nearly constant number of policy updates can attain optimal regret in the fully online setting.
    
[^12]: 混合方块神经ODE因果建模

    Hybrid Square Neural ODE Causal Modeling

    [https://arxiv.org/abs/2402.17233](https://arxiv.org/abs/2402.17233)

    混合模型将基于ODE的机械动力学与神经网络组件结合，在解释性和因果基础的同时，利用领域知识对治疗效果进行排名，从而解决灵活性增加带来的因果基础丢失问题。

    

    混合模型将基于ODE的机械动力学与灵活且表达力强的神经网络组件结合起来。这种模型在科学领域越来越受欢迎，特别是在ODE-based建模提供重要解释性和经过验证的因果基础（例如，用于反事实推理）的领域。将机械模型纳入也为标准黑箱建模方法提供了归纳偏差，这在从小型数据集或部分观察到的复杂系统中学习时至关重要。不幸的是，随着混合模型变得更加灵活，机械模型提供的因果基础很快会丢失。我们通过利用另一个常见的领域知识来源来解决这个问题：对一组干预的治疗效果进行排名，即使准确的治疗效果不知道。我们在因果损失中编码这些信息，将其与标准预测损失相结合，得出混合损失。

    arXiv:2402.17233v1 Announce Type: new  Abstract: Hybrid models combine mechanistic ODE-based dynamics with flexible and expressive neural network components. Such models have grown rapidly in popularity, especially in scientific domains where such ODE-based modeling offers important interpretability and validated causal grounding (e.g., for counterfactual reasoning). The incorporation of mechanistic models also provides inductive bias in standard blackbox modeling approaches, critical when learning from small datasets or partially observed, complex systems. Unfortunately, as hybrid models become more flexible, the causal grounding provided by the mechanistic model can quickly be lost. We address this problem by leveraging another common source of domain knowledge: ranking of treatment effects for a set of interventions, even if the precise treatment effect is unknown. We encode this information in a causal loss that we combine with the standard predictive loss to arrive at a hybrid los
    
[^13]: 采用合作博弈论的开放式即兴团队合作

    Open Ad Hoc Teamwork with Cooperative Game Theory

    [https://arxiv.org/abs/2402.15259](https://arxiv.org/abs/2402.15259)

    提出了采用合作博弈论解释开放式即兴团队合作中联合Q值表示的新理论，为进一步发展这一研究方向和应用提供了新思路

    

    即兴团队合作面临着一个具有挑战性的问题，需要设计一个能够与队友协作但没有先前协调或联合训练的智能体。开放式即兴团队合作进一步复杂化了这一挑战，考虑了具有不断变化的队友数量的环境，即开放式团队。现有解决这一问题的最先进方法是基于图神经网络的策略学习（GPL），利用了图神经网络的泛化能力来处理无限数量的智能体，有效应对开放式团队。GPL的性能优于其他方法，但其联合Q值表示对解释造成了挑战，阻碍了进一步发展这一研究方向和应用。本文建立了一种新的理论，从合作博弈论的角度为GPL中采用的联合Q值表示提供了一种解释。基于我们的理论，我们提出了一种基于

    arXiv:2402.15259v1 Announce Type: cross  Abstract: Ad hoc teamwork poses a challenging problem, requiring the design of an agent to collaborate with teammates without prior coordination or joint training. Open ad hoc teamwork further complicates this challenge by considering environments with a changing number of teammates, referred to as open teams. The state-of-the-art solution to this problem is graph-based policy learning (GPL), leveraging the generalizability of graph neural networks to handle an unrestricted number of agents and effectively address open teams. GPL's performance is superior to other methods, but its joint Q-value representation presents challenges for interpretation, hindering further development of this research line and applicability. In this paper, we establish a new theory to give an interpretation for the joint Q-value representation employed in GPL, from the perspective of cooperative game theory. Building on our theory, we propose a novel algorithm based on
    
[^14]: Clifford-Steerable卷积神经网络

    Clifford-Steerable Convolutional Neural Networks

    [https://arxiv.org/abs/2402.14730](https://arxiv.org/abs/2402.14730)

    提出了Clifford-Steerable卷积神经网络（CS-CNNs），通过在伪欧几里德空间上处理多矢场，利用Clifford群等变神经网络对$\mathrm{O}(p,q)$可导核进行隐式参数化，显着且一致地优于流体动力学和相对论电动力学预测任务的基准方法

    

    我们提出了Clifford-Steerable卷积神经网络（CS-CNNs），这是一种新颖的$\mathrm{E}(p, q)$等变CNN类。 CS-CNNs在伪欧几里德空间$\mathbb{R}^{p,q}$上处理多矢场。 它们涵盖了例如$\mathrm{E}(3)$在$\mathbb{R}^3$上和Poincar\'e在闵可夫斯基时空$\mathbb{R}^{1,3}$上的等变性。 我们的方法基于通过Clifford群等变神经网络对$\mathrm{O}(p,q)$可导核进行隐式参数化。 在流体动力学和相对论电动力学预测任务上，我们在基准方法上显着且一致地表现出色。

    arXiv:2402.14730v1 Announce Type: cross  Abstract: We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a novel class of $\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivector fields on pseudo-Euclidean spaces $\mathbb{R}^{p,q}$. They cover, for instance, $\mathrm{E}(3)$-equivariance on $\mathbb{R}^3$ and Poincar\'e-equivariance on Minkowski spacetime $\mathbb{R}^{1,3}$. Our approach is based on an implicit parametrization of $\mathrm{O}(p,q)$-steerable kernels via Clifford group equivariant neural networks. We significantly and consistently outperform baseline methods on fluid dynamics as well as relativistic electrodynamics forecasting tasks.
    
[^15]: 通过数据水印证明LLM预训练数据的成员资格

    Proving membership in LLM pretraining data via data watermarks

    [https://arxiv.org/abs/2402.10892](https://arxiv.org/abs/2402.10892)

    使用数据水印在LLM预训练中检测版权持有人作品的方法，可以进行合理检测且提供误检率保证，研究了水印设计对假设检验能力的影响以及在模型和数据集缩放下的检测强度变化。

    

    检测版权持有人的作品是否在LLM预训练中使用是一个重要问题，本文提出使用数据水印实现基于黑盒模型访问的合理检测，前提是版权持有人在公开发布之前贡献了多个训练文档并对其进行了水印处理。通过应用随机采样的数据水印，检测可以被构造为假设检验，从而提供对误检率的保证。研究了两种水印：一种插入随机序列，另一种随机用Unicode类似字符替换字符。首先展示了水印设计的三个方面--水印长度、复制次数和干扰--如何影响假设检验的能力。接着研究了水印在模型和数据集缩放下的检测强度如何变化：增加数据集大小会降低水印的强度，水印...

    arXiv:2402.10892v1 Announce Type: cross  Abstract: Detecting whether copyright holders' works were used in LLM pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design -- watermark length, number of duplications, and interference -- affect the power of the hypothesis test. Next, we study how a watermark's detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks
    
[^16]: 说服一位学习代理

    Persuading a Learning Agent

    [https://arxiv.org/abs/2402.09721](https://arxiv.org/abs/2402.09721)

    在一个重复的贝叶斯说服问题中，即使没有承诺能力，委托人可以通过使用上下文无遗憾学习算法来实现与经典无学习模型中具有承诺的委托人的最优效用无限接近的效果；在代理人使用上下文无交换遗憾学习算法的情况下，委托人无法获得比具有承诺的无学习模型中的最优效用更高的效用。

    

    我们研究了一个重复的贝叶斯说服问题（更一般地，任何具有完全信息的广义委托-代理问题），其中委托人没有承诺能力，代理人使用算法来学习如何对委托人的信号做出响应。我们将这个问题简化为一个一次性的广义委托-代理问题，代理人近似地最佳响应。通过这个简化，我们可以证明：如果代理人使用上下文无遗憾学习算法，则委托人可以保证其效用与经典无学习模型中具有承诺的委托人的最优效用之间可以无限接近；如果代理人使用上下文无交换遗憾学习算法，则委托人无法获得比具有承诺的无学习模型中的最优效用更高的效用。委托人在学习模型与非学习模型中可以获得的效用之间的差距是有界的。

    arXiv:2402.09721v1 Announce Type: cross  Abstract: We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal's signals. We reduce this problem to a one-shot generalized principal-agent problem with an approximately-best-responding agent. This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal's optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment. The difference between the principal's obtainable utility in the learning model and the non-learning model is bound
    
[^17]: 重构随机几何图的几何形状

    Reconstructing the Geometry of Random Geometric Graphs

    [https://arxiv.org/abs/2402.09591](https://arxiv.org/abs/2402.09591)

    该论文通过在底层空间中采样的图来有效地重构随机几何图的几何形状。该方法基于流形假设，即底层空间是低维流形，并且连接概率是嵌入在$\mathbb{R}^N$中的流形中点之间欧几里德距离的严格递减函数。

    

    随机几何图是在度量空间上定义的随机图模型。该模型首先从度量空间中采样点，然后以依赖于它们之间距离的概率独立地连接每对采样点。在本工作中，我们展示了如何在流形假设下有效地从采样的图中重构底层空间的几何形状，即假设底层空间是低维流形，并且连接概率是嵌入在$\mathbb{R}^N$中的流形中点之间欧几里德距离的严格递减函数。我们的工作补充了大量关于流形学习的工作，其目标是从在流形中采样的点及其（近似的）距离中恢复出流形。

    arXiv:2402.09591v1 Announce Type: new  Abstract: Random geometric graphs are random graph models defined on metric spaces. Such a model is defined by first sampling points from a metric space and then connecting each pair of sampled points with probability that depends on their distance, independently among pairs. In this work, we show how to efficiently reconstruct the geometry of the underlying space from the sampled graph under the manifold assumption, i.e., assuming that the underlying space is a low dimensional manifold and that the connection probability is a strictly decreasing function of the Euclidean distance between the points in a given embedding of the manifold in $\mathbb{R}^N$. Our work complements a large body of work on manifold learning, where the goal is to recover a manifold from sampled points sampled in the manifold along with their (approximate) distances.
    
[^18]: 在线结构化预测与Fenchel-Young损失和改进的替代后悔度用于在线多类分类与逻辑损失的研究

    Online Structured Prediction with Fenchel--Young Losses and Improved Surrogate Regret for Online Multiclass Classification with Logistic Loss

    [https://arxiv.org/abs/2402.08180](https://arxiv.org/abs/2402.08180)

    这项研究扩展了在线结构化预测的替代后悔度界限，通过引入Fenchel-Young损失和随机解码方案，使得在在线多类分类和逻辑损失下获得了更好的结果。

    

    本文研究了具有完全信息反馈的在线结构化预测。对于在线多类分类，van der Hoeven(2020)通过引入一个优美的“利用替代间隙”的框架，获得了与时间范围无关的替代后悔度界限，即有限的界限。然而，这个框架主要限于多类分类，因为它依赖于一种特定于分类的过程，将估计得分转化为输出。我们将“利用替代间隙”框架扩展到具有“Fenchel-Young损失”的在线结构化预测中，这是一大类包括多类分类的逻辑损失在内的替代损失，获得了在各种结构化预测问题上的有限替代后悔度界限。为此，我们提出并分析了随机解码，将估计得分转化为一般的结构化输出。此外，通过将我们的解码应用于在线多类分类与逻辑损失，我们获得了改进的替代后悔度界限。

    This paper studies online structured prediction with full-information feedback. For online multiclass classification, van der Hoeven (2020) has obtained surrogate regret bounds independent of the time horizon, or \emph{finite}, by introducing an elegant \emph{exploit-the-surrogate-gap} framework. However, this framework has been limited to multiclass classification primarily because it relies on a classification-specific procedure for converting estimated scores to outputs. We extend the exploit-the-surrogate-gap framework to online structured prediction with \emph{Fenchel--Young losses}, a large family of surrogate losses including the logistic loss for multiclass classification, obtaining finite surrogate regret bounds in various structured prediction problems. To this end, we propose and analyze \emph{randomized decoding}, which converts estimated scores to general structured outputs. Moreover, by applying our decoding to online multiclass classification with the logistic loss, we o
    
[^19]: 基于独立线性函数逼近的马尔科夫博弈的样本复杂度改进

    Refined Sample Complexity for Markov Games with Independent Linear Function Approximation

    [https://arxiv.org/abs/2402.07082](https://arxiv.org/abs/2402.07082)

    本文在独立线性函数逼近的马尔科夫博弈中，通过改进AVLPR框架，提出了基于数据依赖的悲观估计方法，解决了多智能体的诅咒问题。

    

    马尔科夫博弈（MG）是多智能体强化学习（MARL）中的重要模型。长期以来人们一直认为“多智能体的诅咒”（即算法性能随着智能体数量指数级下降）是不可避免的，直到最近几篇作品（Daskalakis等人，2023年；Cui等人，2023年；Wang等人，2023年）。这些作品确实解决了多智能体的诅咒，当状态空间极大且（线性）函数逼近被应用时，它们要么具有更慢的收敛速度$O(T^{-1/4})$，要么在行动数$A_{\max}$上带来多项式依赖——尽管在单智能体情况下即使损失函数可以随时间任意变化（Dai等人，2023年），也可避免这种依赖。本文首先通过Wang等人（2023年）的“AVLPR”框架精化，洞察了基于数据的（即随机的）悲观估计子优化差距，从而允许更广泛的插件算法选择。当专门应用于MGs时，这一方法能够处理独立的情况。

    Markov Games (MG) is an important model for Multi-Agent Reinforcement Learning (MARL). It was long believed that the "curse of multi-agents" (i.e., the algorithmic performance drops exponentially with the number of agents) is unavoidable until several recent works (Daskalakis et al., 2023; Cui et al., 2023; Wang et al., 2023. While these works did resolve the curse of multi-agents, when the state spaces are prohibitively large and (linear) function approximations are deployed, they either had a slower convergence rate of $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions $A_{\max}$ -- which is avoidable in single-agent cases even when the loss functions can arbitrarily vary with time (Dai et al., 2023). This paper first refines the `AVLPR` framework by Wang et al. (2023), with an insight of *data-dependent* (i.e., stochastic) pessimistic estimation of the sub-optimality gap, allowing a broader choice of plug-in algorithms. When specialized to MGs with independent
    
[^20]: 大规模语言模型在知识蒸馏中遇见图神经网络

    Large Language Model Meets Graph Neural Network in Knowledge Distillation

    [https://arxiv.org/abs/2402.05894](https://arxiv.org/abs/2402.05894)

    本论文提出了一种新颖的图知识蒸馏框架，使用大规模语言模型作为教师模型、图神经网络作为学生模型，解决了在理解文本-属性图中的节点分类问题中的限制。

    

    尽管近期学术界对于大规模语言模型（LLMs）在理解文本-属性图（TAG）方面的进展和潜力有所披露，但LLMs在实际应用中的部署受到了计算和存储需求高，推理过程中延迟长的限制。同时，传统的图神经网络（GNNs）虽然轻量且擅长学习图的结构特征，但对于真实应用中TAG复杂语义的把握有所限制。为了解决这些限制，我们聚焦于TAG中节点分类的下游任务，提出了一种新颖的图知识蒸馏框架，称为语言图知识蒸馏（LinguGKD），使用LLMs作为教师模型，GNNs作为学生模型进行知识蒸馏。其中包括对LLM进行TAG定向指导调整以应对设计的节点分类提示，然后对层次化学习的节点特征进行对齐。

    Despite recent community revelations about the advancements and potential of Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the deployment of LLMs for production is hindered by their high computational and storage requirements, as well as long latencies during inference. Simultaneously, although traditional Graph Neural Networks (GNNs) are light weight and adept at learning structural features of graphs, their ability to grasp the complex semantics in TAGs is somewhat constrained for real applications. To address these limitations, we concentrate on the downstream task of node classification in TAG and propose a novel graph knowledge distillation framework, termed Linguistic Graph Knowledge Distillation (LinguGKD), using LLMs as teacher models and GNNs as student models for knowledge distillation. It involves TAG-oriented instruction tuning of LLM on designed node classification prompts, followed by aligning the hierarchically learned node features of the t
    
[^21]: 使用复值神经网络和不规则分布的麦克风重建房间传递函数

    Room transfer function reconstruction using complex-valued neural networks and irregularly distributed microphones

    [https://arxiv.org/abs/2402.04866](https://arxiv.org/abs/2402.04866)

    本研究使用复值神经网络在有限的麦克风数据中估计房间传递函数，实现了房间传递函数的重建，创新之处在于首次使用了复值神经网络来估计房间传递函数。

    

    重建房间传递函数用于计算房间内的复杂声场具有重要的实际应用。然而，通常需要使用大量的麦克风，这是不现实的。最近，除了传统的信号处理方法，深度学习技术已经被应用于从房间内零散点测量得到的有限的房间传递函数来重建房间传递函数。在本研究中，我们使用复值神经网络估计房间传递函数在第一个声学共振频率范围内，使用少量不规则分布的麦克风。据我们所知，这是首次使用复值神经网络来估计房间传递函数。为了分析将复值优化应用于所考虑任务的好处，我们将所提出的技术与最先进的实值神经网络方法和基于核的最先进方法进行比较。

    Reconstructing the room transfer functions needed to calculate the complex sound field in a room has several important real-world applications. However, an unpractical number of microphones is often required. Recently, in addition to classical signal processing methods, deep learning techniques have been applied to reconstruct the room transfer function starting from a very limited set of room transfer functions measured at scattered points in the room. In this study, we employ complex-valued neural networks to estimate room transfer functions in the frequency range of the first room resonances, using a few irregularly distributed microphones. To the best of our knowledge, this is the first time complex-valued neural networks are used to estimate room transfer functions. To analyze the benefits of applying complex-valued optimization to the considered task, we compare the proposed technique with a state-of-the-art real-valued neural network method and a state-of-the-art kernel-based si
    
[^22]: 大型语言模型如何进行上下文学习？查询和键矩阵是上下文头部进行度量学习的两个关键组成部分

    How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning

    [https://arxiv.org/abs/2402.02872](https://arxiv.org/abs/2402.02872)

    本论文探索了大型语言模型如何进行上下文学习的机制，提出了一个使用定位和投影方法的假设。通过查询和键矩阵来计算输入文本与每个演示之间的注意力权重，以学习它们之间的相似度度量。实验证明了我们的分析。

    

    我们探索了上下文学习的机制，并提出了使用定位和投影方法的假设。在浅层中，演示的特征被合并到相应的标签中，输入文本的特征被聚合到最后一个标记中。在深层中，上下文头部发挥了重要作用。在每个上下文头部中，值-输出矩阵提取了标签的特征。查询和键矩阵计算了输入文本与每个演示之间的注意力权重。注意力权重越大，越多的标签信息被传输到最后一个标记中，用于预测下一个单词。查询和键矩阵可以被视为学习输入文本与每个演示之间相似度度量的两个关键组成部分。基于这个假设，我们解释了为什么不平衡的标签和演示顺序会影响预测。我们在GPT2大型、Llama 7B、13B和30B上进行了实验。结果支持我们的分析。总体而言，我们的研究提供了一个关于大型语言模型如何进行上下文学习的理论解释和验证。

    We explore the mechanism of in-context learning and propose a hypothesis using locate-and-project method. In shallow layers, the features of demonstrations are merged into their corresponding labels, and the features of the input text are aggregated into the last token. In deep layers, in-context heads make great contributions. In each in-context head, the value-output matrix extracts the labels' features. Query and key matrices compute the attention weights between the input text and each demonstration. The larger the attention weight is, the more label information is transferred into the last token for predicting the next word. Query and key matrices can be regarded as two towers for learning the similarity metric between the input text and each demonstration. Based on this hypothesis, we explain why imbalanced labels and demonstration order affect predictions. We conduct experiments on GPT2 large, Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study provid
    
[^23]: 提高地球观测数据预测置信度的潜在空间度量

    A Latent Space Metric for Enhancing Prediction Confidence in Earth Observation Data

    [https://arxiv.org/abs/2401.17342](https://arxiv.org/abs/2401.17342)

    这项研究提出了一种新的方法来估计利用地球观测数据进行回归任务时机器学习模型预测的置信度。通过利用潜在空间表示来推导置信度度量，建立了潜在表示中的欧几里得距离与个体蚊子种群预测的绝对误差之间的相关性。该方法在意大利威尼托地区和德国上莱茵河谷的地区得到了验证，并表现出较高的可靠性和可信度。

    

    本研究提出了一种新的方法，用于估计机器学习模型预测的置信度，特别是在利用地球观测数据进行回归任务时，重点关注蚊子种群（MA）估计。我们利用变分自动编码器的架构，通过EO数据的潜在空间表示来推导置信度度量。这种方法对于建立潜在表示中的欧几里得距离与个体MA预测的绝对误差（AE）之间的相关性至关重要。我们的研究重点关注了意大利威尼托地区和德国上莱茵河谷的EO数据集，这些地区受蚊子种群的影响显著。一个重要的发现是MA预测的AE与所提出的置信度度量之间存在0.46的显著相关性。这个相关性意味着这是一个稳健的、新的度量方法，用于量化AI模型在该背景下的预测可靠性和提高可信度。

    This study presents a new approach for estimating confidence in machine learning model predictions, specifically in regression tasks utilizing Earth Observation (EO) data, with a particular focus on mosquito abundance (MA) estimation. We take advantage of a Variational AutoEncoder architecture, to derive a confidence metric by the latent space representations of EO datasets. This methodology is pivotal in establishing a correlation between the Euclidean distance in latent representations and the Absolute Error (AE) in individual MA predictions. Our research focuses on EO datasets from the Veneto region in Italy and the Upper Rhine Valley in Germany, targeting areas significantly affected by mosquito populations. A key finding is a notable correlation of 0.46 between the AE of MA predictions and the proposed confidence metric. This correlation signifies a robust, new metric for quantifying the reliability and enhancing the trustworthiness of the AI model's predictions in the context of 
    
[^24]: 关于损失和基于不确定性的主动学习算法的收敛性

    On the convergence of loss and uncertainty-based active learning algorithms

    [https://arxiv.org/abs/2312.13927](https://arxiv.org/abs/2312.13927)

    论文考虑了损失和不确定性基础的主动学习算法在线性分类器和线性可分数据集上的收敛速度，提出了一种新算法并展示了其效率。

    

    我们考虑了在不同假设下损失和基于不确定性的主动学习算法的收敛速度。首先，我们建立了一组条件，确保在应用于线性分类器和线性可分数据集时的收敛速度。这包括证明各种损失函数的基于损失的采样的收敛速度保证。其次，我们引入了一个框架，通过利用已知的随机梯度下降算法的收敛速率界限，使我们能够导出损失采样的收敛速率界限。最后，我们提出了一种新算法，将点采样和随机Polyak步长相结合。我们建立了一个关于采样过程的条件，确保该算法的收敛速度保证，特别是在光滑凸损失函数的情况下。我们的数值结果展示了所提出算法的效率。

    arXiv:2312.13927v2 Announce Type: replace-cross  Abstract: We consider the convergence rates of loss and uncertainty-based active learning algorithms under various assumptions. Firstly, we establish a set of conditions that ensure convergence rates when applied to linear classifiers and linearly separable datasets. This includes demonstrating convergence rate guarantees for loss-based sampling with various loss functions. Secondly, we introduce a framework that allows us to derive convergence rate bounds for loss-based sampling by leveraging known convergence rate bounds for stochastic gradient descent algorithms. Lastly, we propose a new algorithm that combines point sampling and stochastic Polyak's step size. We establish a condition on the sampling process, ensuring a convergence rate guarantee for this algorithm, particularly in the case of smooth convex loss functions. Our numerical results showcase the efficiency of the proposed algorithm.
    
[^25]: Neural Language Agents的diff历史

    diff History for Neural Language Agents

    [https://arxiv.org/abs/2312.07540](https://arxiv.org/abs/2312.07540)

    本文介绍了一种名为diff历史的简单且高效的解决方案，用于将环境中的观测转换为文本提示，以便对于长期推理决策的任务中的Neural Language Models进行优化。

    

    Neural Language Models (LMs)为通用的具体控制提供了令人兴奋的解决方案。然而，当使用基于LM的控制器时，会出现一个关键的技术问题：环境观测必须转换为文本，这与历史耦合在一起，导致冗长而冗余的文本提示。因此，LM代理的先前工作局限于具有小观测大小以及对交互历史或指示调优需求较小的限制领域。在本文中，我们引入了diff历史，这是一个简单且非常有效的解决方案。通过在用于提示LM策略的交互历史中的连续文本观测上应用Unix diff命令，我们既可以摘除冗余信息，又可以将文本输入的内容集中在环境中显著变化的方面。在需要长期推理进行决策的未解决的视频游戏NetHack中，使用diff历史调优的LM与状态匹配。

    arXiv:2312.07540v2 Announce Type: replace Abstract: Neural Language Models (LMs) offer an exciting solution for general-purpose embodied control. However, a key technical issue arises when using an LM-based controller: environment observations must be converted to text, which coupled with history, results in long and verbose textual prompts. As a result, prior work in LM agents is limited to restricted domains with small observation size as well as minimal needs for interaction history or instruction tuning. In this paper, we introduce diff history, a simple and highly effective solution to these issues. By applying the Unix diff command on consecutive text observations in the interaction histories used to prompt LM policies, we can both abstract away redundant information and focus the content of textual inputs on the salient changes in the environment. On NetHack, an unsolved video game that requires long-horizon reasoning for decision-making, LMs tuned with diff history match state-
    
[^26]: 高效强化学习在部分可观察性下的应用

    Efficient Reinforcement Learning from Partial Observability

    [https://arxiv.org/abs/2311.12244](https://arxiv.org/abs/2311.12244)

    该论文提出了一种基于表示的方法，用于从部分观测中进行有效的强化学习。该方法能够处理部分可观测性带来的计算和统计挑战，并在各种基准测试中展现出优于先进算法的性能。

    

    在大多数实际应用中，状态信息只能部分观测到，这破坏了马尔科夫决策过程的假设，导致将观测与状态相混淆的算法表现不佳。而部分可观测马尔科夫决策过程（POMDP）提供了一个允许在学习、探索和规划中考虑部分可观测性的通用框架，但也带来了显著的计算和统计挑战。为解决这些困难，我们提出了一个基于表示的视角，提供了一个统一的框架和可行的算法方法，用于从部分观测中进行实际的强化学习。我们提供了理论分析来证明所提出算法的统计效率，并经验性地证明了在各种基准测试中，所提出的算法在部分观测下能够超越最先进性能，推动了可靠的强化学习。

    In most real-world reinforcement learning applications, state information is only partially observable, which breaks the Markov decision process assumption and leads to inferior performance for algorithms that conflate observations with state. Partially Observable Markov Decision Processes (POMDPs), on the other hand, provide a general framework that allows for partial observability to be accounted for in learning, exploration and planning, but presents significant computational and statistical challenges. To address these difficulties, we develop a representation-based perspective that leads to a coherent framework and tractable algorithmic approach for practical reinforcement learning from partial observations. We provide a theoretical analysis for justifying the statistical efficiency of the proposed algorithm, and also empirically demonstrate the proposed algorithm can surpass state-of-the-art performance with partial observations across various benchmarks, advancing reliable reinf
    
[^27]: 让循环的询问: 大型语言模型在判断中的摇摆

    Ask Again, Then Fail: Large Language Models' Vacillations in Judgement

    [https://arxiv.org/abs/2310.02174](https://arxiv.org/abs/2310.02174)

    目前的语言模型在面对后续问题时常常摇摆不定，研究者提出了一个后续问题机制和两个度量标准来量化这种不一致性，并开发出Unwavering-FQ框架来教导模型保持最初的正确判断，实验证明其有效性。

    

    我们观察到目前的会话式语言模型在面对后续问题时往往在其判断上摇摆不定，即使原始判断是正确的。这种摇摆对于生成可靠回复和建立用户信任构成了重要挑战。为了全面评估这一问题，我们引入了一个后续问题机制以及两个度量标准来量化这种不一致性，确认了当前语言模型普遍存在这种情况。为了缓解这一问题，我们探讨了各种提示策略用于闭源模型；此外，我们开发了一个基于训练的框架Unwavering-FQ，通过合成高质量的偏好数据来教导语言模型保持其最初的正确判断。我们的实验结果验证了我们框架的有效性以及其增强模型通用能力的能力。

    arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
    
[^28]: UniAP: 通过混合整数二次规划统一跨层和内层自动并行化

    UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming

    [https://arxiv.org/abs/2307.16375](https://arxiv.org/abs/2307.16375)

    UniAP是一种新型的自动并行化方法，通过混合整数二次规划统一跨层和内层的自动并行化。与现有方法相比，UniAP在吞吐量方面表现更好，并且减少了策略优化时间。

    

    分布式学习常用于训练深度学习模型，特别是大型模型。在分布式学习中，手动并行化方法需要大量人力，并且灵活性有限。因此，最近提出了自动并行化方法来自动化并行策略优化过程。现有的自动并行化方法存在次优解的问题，因为它们不会同时优化跨层并行化和内层并行化这两个类别的并行策略。在本文中，我们提出了一种名为UniAP的新型自动并行化方法，通过混合整数二次规划统一跨层和内层的自动并行化。据我们所知，UniAP是第一种能够同时优化这两个类别的并行策略以求得最优解的并行化方法。实验结果表明，UniAP在吞吐量方面胜过了最先进的方法，提高了最多1.71倍，并减少了策略优化的时间。

    Distributed learning is commonly used for training deep learning models, especially large models. In distributed learning, manual parallelism (MP) methods demand considerable human effort and have limited flexibility. Hence, automatic parallelism (AP) methods have recently been proposed for automating the parallel strategy optimization process. Existing AP methods suffer from sub-optimal solutions because they do not jointly optimize the two categories of parallel strategies (i.e., inter-layer parallelism and intra-layer parallelism). In this paper, we propose a novel AP method called UniAP, which unifies inter- and intra-layer automatic parallelism by mixed integer quadratic programming. To the best of our knowledge, UniAP is the first parallel method that can jointly optimize the two categories of parallel strategies to find an optimal solution. Experimental results show that UniAP outperforms state-of-the-art methods by up to 1.71$\times$ in throughput and reduces strategy optimizat
    
[^29]: 噪声自适应（加速）随机重力球动量

    Noise-adaptive (Accelerated) Stochastic Heavy-Ball Momentum. (arXiv:2401.06738v1 [math.OC])

    [http://arxiv.org/abs/2401.06738](http://arxiv.org/abs/2401.06738)

    本研究分析了在光滑、强凸环境中随机重力球动量的收敛性，并证明了当批量大小大于某个阈值时，该方法可以实现加速收敛速度。针对强凸二次函数，我们建议了一种噪声自适应的多阶段方法，可以使收敛速度进一步提高。实验结果验证了该方法的有效性。

    

    我们分析了在光滑，强凸环境中随机重力球动量（SHB）的收敛性。Kidambi等人（2018）表明，对于二次函数，SHB（带有小批量）无法达到加速的收敛速度，并猜想SHB的实际收益是小批量的副产品。我们通过展示当批量大小大于一定阈值时，SHB可以获得加速的收敛速度来证实这一观点。特别地，对于条件数为$\kappa$的强凸二次函数，我们证明了使用标准步长和动量参数的SHB具有$O\left(\exp(-\frac{T}{\sqrt{\kappa}}) + \sigma \right)$的收敛速度，其中$T$为迭代次数，$\sigma^2$为随机梯度的方差。为确保收敛到极小值，我们提出了一种多阶段方法，结果是噪声自适应的$O\left(\exp\left(-\frac{T}{\sqrt{\kappa}} \right) + \frac{\sigma}{T}\right)$速度。对于一般的强凸函数，我们在实验中展示了所提方法的有效性。

    We analyze the convergence of stochastic heavy ball (SHB) momentum in the smooth, strongly-convex setting. Kidambi et al. (2018) show that SHB (with small mini-batches) cannot attain an accelerated rate of convergence even for quadratics, and conjecture that the practical gain of SHB is a by-product of mini-batching. We substantiate this claim by showing that SHB can obtain an accelerated rate when the mini-batch size is larger than some threshold. In particular, for strongly-convex quadratics with condition number $\kappa$, we prove that SHB with the standard step-size and momentum parameters results in an $O\left(\exp(-\frac{T}{\sqrt{\kappa}}) + \sigma \right)$ convergence rate, where $T$ is the number of iterations and $\sigma^2$ is the variance in the stochastic gradients. To ensure convergence to the minimizer, we propose a multi-stage approach that results in a noise-adaptive $O\left(\exp\left(-\frac{T}{\sqrt{\kappa}} \right) + \frac{\sigma}{T}\right)$ rate. For general strongly-
    
[^30]: RudolfV：一种由病理学家为病理学家构建的基础模型

    RudolfV: A Foundation Model by Pathologists for Pathologists. (arXiv:2401.04079v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2401.04079](http://arxiv.org/abs/2401.04079)

    本文提出了一种由病理学家为病理学家构建的基础模型，通过数据整理和融入病理学领域知识，扩展了数字病理学全玻片图像基础模型，解决了人工智能在处理罕见疾病方面的挑战。

    

    组织病理学在临床医学和生物医学研究中起着核心作用。虽然人工智能在许多病理学任务上显示出有希望的结果，但在泛化和处理训练数据稀缺的罕见疾病方面仍然是一个挑战。在学习来自有限标记数据之前，从无标记数据中提取知识到基础模型可以解决这些挑战。在这项工作中，我们通过半自动数据整理和融入病理学领域知识，扩展了数字病理学全玻片图像基础模型的最新技术。具体而言，我们结合计算和病理学领域知识(1)整理了一个多样化的数据集，包括10.3万个玻片图像对应的7.5亿个图像块，涵盖了来自欧美不同修复、染色和扫描协议以及不同指示和实验室的数据，(2)用于对语义上相似的玻片和组织块进行分组。

    Histopathology plays a central role in clinical medicine and biomedical research. While artificial intelligence shows promising results on many pathological tasks, generalization and dealing with rare diseases, where training data is scarce, remains a challenge. Distilling knowledge from unlabeled data into a foundation model before learning from, potentially limited, labeled data provides a viable path to address these challenges. In this work, we extend the state of the art of foundation models for digital pathology whole slide images by semi-automated data curation and incorporating pathologist domain knowledge. Specifically, we combine computational and pathologist domain knowledge (1) to curate a diverse dataset of 103k slides corresponding to 750 million image patches covering data from different fixation, staining, and scanning protocols as well as data from different indications and labs across the EU and US, (2) for grouping semantically similar slides and tissue patches, and 
    
[^31]: 旗帜游戏：通过旗帜流形来获得鲁棒的主方向

    Fun with Flags: Robust Principal Directions via Flag Manifolds. (arXiv:2401.04071v1 [cs.CV])

    [http://arxiv.org/abs/2401.04071](http://arxiv.org/abs/2401.04071)

    本研究提出了一种统一的PCA和其变种框架，该框架基于线性子空间旗帜，并引入了对异常值和数据流形的考虑。通过在旗帜流形上进行优化问题的求解，结合主测地线近似，提出了一系列新的降维算法。

    

    主成分分析（PCA）及其对流形和异常数据的扩展，在计算机视觉和机器学习中是不可或缺的。本研究提出了PCA及其变种的统一形式，引入了基于线性子空间旗帜的框架，即逐渐增加维度的嵌套线性子空间的层次结构，不仅允许共同实现，还产生了新的未曾探索的变种。我们从广义化传统的PCA方法开始，这些方法要么最大化方差，要么最小化重构误差。我们扩展这些解释，通过考虑异常值和数据流形，开发出了大量新的降维算法。为了设计一种通用的计算方法，我们将鲁棒和对偶形式的PCA重新构建为在旗帜流形上的优化问题。然后，我们将主测地线近似（切线PCA）整合到这个基于旗帜的框架中，创造出一种新的方法。

    Principal component analysis (PCA), along with its extensions to manifolds and outlier contaminated data, have been indispensable in computer vision and machine learning. In this work, we present a unifying formalism for PCA and its variants, and introduce a framework based on the flags of linear subspaces, \ie a hierarchy of nested linear subspaces of increasing dimension, which not only allows for a common implementation but also yields novel variants, not explored previously. We begin by generalizing traditional PCA methods that either maximize variance or minimize reconstruction error. We expand these interpretations to develop a wide array of new dimensionality reduction algorithms by accounting for outliers and the data manifold. To devise a common computational approach, we recast robust and dual forms of PCA as optimization problems on flag manifolds. We then integrate tangent space approximations of principal geodesic analysis (tangent-PCA) into this flag-based framework, crea
    
[^32]: 通过核扭曲函数定制Mixup数据

    Tailoring Mixup to Data using Kernel Warping functions. (arXiv:2311.01434v1 [cs.LG])

    [http://arxiv.org/abs/2311.01434](http://arxiv.org/abs/2311.01434)

    本研究提出了一种利用核扭曲函数对Mixup数据进行个性化处理的方法，通过动态改变插值系数的概率分布来实现更频繁和更强烈的混合相似数据点。实验证明这种方法不仅提高了模型性能，还提高了模型的校准性。

    

    数据增强是学习高效深度学习模型的重要基础。在所有提出的增强技术中，线性插值训练数据点（也称为Mixup）已被证明在许多应用中非常有效。然而，大多数研究都集中在选择合适的点进行混合，或者应用复杂的非线性插值，而我们则对更相似的点进行更频繁和更强烈的混合感兴趣。为此，我们提出了通过扭曲函数动态改变插值系数的概率分布的方法，取决于要组合的数据点之间的相似性。我们定义了一个高效而灵活的框架来实现这一点，以避免多样性的损失。我们进行了广泛的分类和回归任务实验，结果显示我们提出的方法既提高了模型的性能，又提高了模型的校准性。代码可在https://github.com/ENSTA-U2IS/torch-uncertainty上找到。

    Data augmentation is an essential building block for learning efficient deep learning models. Among all augmentation techniques proposed so far, linear interpolation of training data points, also called mixup, has found to be effective for a large panel of applications. While the majority of works have focused on selecting the right points to mix, or applying complex non-linear interpolation, we are interested in mixing similar points more frequently and strongly than less similar ones. To this end, we propose to dynamically change the underlying distribution of interpolation coefficients through warping functions, depending on the similarity between data points to combine. We define an efficient and flexible framework to do so without losing in diversity. We provide extensive experiments for classification and regression tasks, showing that our proposed method improves both performance and calibration of models. Code available in https://github.com/ENSTA-U2IS/torch-uncertainty
    
[^33]: 取消保护特征：从胸部X射线嵌入中消除保护特征

    Unreading Race: Purging Protected Features from Chest X-ray Embeddings. (arXiv:2311.01349v1 [cs.LG])

    [http://arxiv.org/abs/2311.01349](http://arxiv.org/abs/2311.01349)

    论文的目的是利用正交化方法消除胸部X射线嵌入中的保护特征影响，并证明其有效性。研究结果表明保护特征对病理预测有显著影响，而应用正交化方法可以消除这些影响。

    

    目的：分析并消除深度学习模型中胸部X射线嵌入的保护特征影响。方法：使用正交化方法消除胸部X射线嵌入中的保护特征（如年龄、性别、种族）的影响，确保特征独立的结果。为了验证该方法的有效性，我们使用三个预训练模型（有监督对比、自监督对比和基线分类器模型）对MIMIC和CheXpert数据集进行了回顾性研究。我们的统计分析涉及通过估计保护特征影响和评估使用两种类型嵌入的能力来预测种族、年龄或性别的原始与正交嵌入的比较。结果：我们的实验揭示了保护特征对病理预测的显着影响。应用正交化方法可以消除这些特征影响。除了消除对病理分类的影响之外，

    Purpose: To analyze and remove protected feature effects in chest radiograph embeddings of deep learning models.  Materials and Methods: An orthogonalization is utilized to remove the influence of protected features (e.g., age, sex, race) in chest radiograph embeddings, ensuring feature-independent results. To validate the efficacy of the approach, we retrospectively study the MIMIC and CheXpert datasets using three pre-trained models, namely a supervised contrastive, a self-supervised contrastive, and a baseline classifier model. Our statistical analysis involves comparing the original versus the orthogonalized embeddings by estimating protected feature influences and evaluating the ability to predict race, age, or sex using the two types of embeddings.  Results: Our experiments reveal a significant influence of protected features on predictions of pathologies. Applying orthogonalization removes these feature effects. Apart from removing any influence on pathology classification, whil
    
[^34]: 生成性扩散模型的统计热力学

    The statistical thermodynamics of generative diffusion models. (arXiv:2310.17467v1 [stat.ML])

    [http://arxiv.org/abs/2310.17467](http://arxiv.org/abs/2310.17467)

    本文通过在生成性扩散模型中应用平衡统计力学的工具，揭示了这些模型中的二阶相变现象，并且认为这种稳定性形式是生成能力的关键。

    

    生成性扩散模型在生成建模的许多领域取得了惊人的表现。虽然这些模型的基本思想来自非平衡物理学，但本文中我们表明，可以用平衡统计力学的工具来理解这些模型的许多方面。利用这种重构，我们展示了生成性扩散模型经历了与对称性破缺现象相对应的二阶相变。我们认为，这导致了一种稳定性形式，它是生成能力的核心，并可以用一组平均场临界指数来描述。最后，我们根据热力学的公式分析了将扩散模型与关联记忆网络连接的最近研究。

    Generative diffusion models have achieved spectacular performance in many areas of generative modeling. While the fundamental ideas behind these models come from non-equilibrium physics, in this paper we show that many aspects of these models can be understood using the tools of equilibrium statistical mechanics. Using this reformulation, we show that generative diffusion models undergo second-order phase transitions corresponding to symmetry breaking phenomena. We argue that this lead to a form of instability that lies at the heart of their generative capabilities and that can be described by a set of mean field critical exponents. We conclude by analyzing recent work connecting diffusion models and associative memory networks in view of the thermodynamic formulations.
    
[^35]: SAM-CLIP: 将视觉基础模型合并为语义和空间理解

    SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding. (arXiv:2310.15308v1 [cs.CV])

    [http://arxiv.org/abs/2310.15308](http://arxiv.org/abs/2310.15308)

    该论文提出了一种将视觉基础模型合并为一个统一模型的方法，通过集成多任务学习、持续学习技术和师生蒸馏，实现了显著较少的计算成本和较少的预训练数据需求。通过应用该方法于SAM和CLIP，得到了一个统一模型SAM-CLIP，将两者的优势融合在一起。

    

    公开可用的视觉基础模型（VFMs）的领域，如CLIP和Segment Anything Model（SAM），正在迅速扩大。VFMs具有源自它们的预训练目标的不同能力。例如，CLIP在语义理解方面表现出色，而SAM专注于分割的空间理解。在这项工作中，我们介绍了一种将VFMs高效合并为一个统一模型的简单方法，以吸收它们的专业知识。我们提出的方法集成了多任务学习、持续学习技术和师生蒸馏。与传统的从头开始进行多任务训练相比，这种策略具有显著较少的计算成本。此外，它只需要最初用于训练单个模型的预训练数据集的一小部分。通过将我们的方法应用于SAM和CLIP，我们得到了SAM-CLIP：将SAM和CLIP的优势融合为单一主干的统一模型，使其适用于...

    The landscape of publicly available vision foundation models (VFMs), such as CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed with distinct capabilities stemming from their pre-training objectives. For instance, CLIP excels in semantic understanding, while SAM specializes in spatial understanding for segmentation. In this work, we introduce a simple recipe to efficiently merge VFMs into a unified model that assimilates their expertise. Our proposed method integrates multi-task learning, continual learning techniques, and teacher-student distillation. This strategy entails significantly less computational cost compared to traditional multi-task training from scratch. Additionally, it only demands a small fraction of the pre-training datasets that were initially used to train individual models. By applying our method to SAM and CLIP, we derive SAM-CLIP: a unified model that amalgamates the strengths of SAM and CLIP into a single backbone, making it apt for ed
    
[^36]: 机器学习模型的成员推断攻击的基本限制

    Fundamental Limits of Membership Inference Attacks on Machine Learning Models. (arXiv:2310.13786v1 [stat.ML])

    [http://arxiv.org/abs/2310.13786](http://arxiv.org/abs/2310.13786)

    本文探讨了机器学习模型上成员推断攻击的基本限制，包括推导了效果和成功率的统计量，并提供了几种情况下的界限。这使得我们能够根据样本数量和其他结构参数推断潜在攻击的准确性。

    

    成员推断攻击（MIA）可以揭示特定数据点是否是训练数据集的一部分，可能暴露个人的敏感信息。本文探讨了关于机器学习模型上MIA的基本统计限制。具体而言，我们首先推导了统计量，该统计量决定了这种攻击的有效性和成功率。然后，我们研究了几种情况，并对这个感兴趣的统计量提供了界限。这使我们能够根据样本数量和学习模型的其他结构参数推断潜在攻击的准确性，在某些情况下可以直接从数据集中估计。

    Membership inference attacks (MIA) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. This article explores the fundamental statistical limitations associated with MIAs on machine learning models. More precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. Then, we investigate several situations for which we provide bounds on this quantity of interest. This allows us to infer the accuracy of potential attacks as a function of the number of samples and other structural parameters of learning models, which in some cases can be directly estimated from the dataset.
    
[^37]: 在分布式学习任务中评估生成模型

    On the Evaluation of Generative Models in Distributed Learning Tasks. (arXiv:2310.11714v1 [cs.LG])

    [http://arxiv.org/abs/2310.11714](http://arxiv.org/abs/2310.11714)

    本文研究了在具有异构数据分布的分布式学习任务中评估生成模型。通过研究Fr\'echet inception距离（FID），并考虑不同聚合分数，发现FID-all和FID-avg分数的模型排名可能不一致。

    

    在文献中已经广泛研究了对包括生成对抗网络（GAN）和扩散模型在内的深度生成模型的评估。然而，现有的评估方法主要针对单个客户端存储的训练数据的集中式学习问题，而生成模型的许多应用涉及到分布式学习环境，例如联邦学习场景，其中训练数据由多个客户端收集并分发。本文研究了在具有异构数据分布的分布式学习任务中评估生成模型。首先，我们关注Fr\'echet inception距离（FID），并考虑以下基于FID的聚合分数：1）FID-avg作为客户端个体FID分数的平均值，2）FID-all作为训练模型与包含所有客户端数据的集体数据集之间的FID距离。我们证明了根据FID-all和FID-avg分数的模型排名可能不一致。

    The evaluation of deep generative models including generative adversarial networks (GANs) and diffusion models has been extensively studied in the literature. While the existing evaluation methods mainly target a centralized learning problem with training data stored by a single client, many applications of generative models concern distributed learning settings, e.g. the federated learning scenario, where training data are collected by and distributed among several clients. In this paper, we study the evaluation of generative models in distributed learning tasks with heterogeneous data distributions. First, we focus on the Fr\'echet inception distance (FID) and consider the following FID-based aggregate scores over the clients: 1) FID-avg as the mean of clients' individual FID scores, 2) FID-all as the FID distance of the trained model to the collective dataset containing all clients' data. We prove that the model rankings according to the FID-all and FID-avg scores could be inconsist
    
[^38]: 超越单节点：在分布式系统上实现大规模机器学习模型加速

    MAD Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems. (arXiv:2310.02784v1 [cs.DC])

    [http://arxiv.org/abs/2310.02784](http://arxiv.org/abs/2310.02784)

    该研究提出了一个性能建模框架，在分布式系统上实现了大规模机器学习模型的加速，获得了2.24倍和5.27倍的吞吐量提升潜力。

    

    训练和部署大规模机器学习（ML）模型是耗时且需要大量分布式计算基础设施。根据实际情况在数据中心规模基础设施上进行大模型训练，我们发现14~32%的GPU小时用于通信，没有重叠计算。为了尽量减少等待通信延迟，本研究开发了一个灵活的性能建模框架，指导并行化和硬件软件共同设计策略。利用最先进的GPU训练硬件上的一套实际大规模ML模型，我们展示了预训练和推断场景分别可以提高2.24倍和5.27倍的吞吐量。

    Training and deploying large machine learning (ML) models is time-consuming and requires significant distributed computing infrastructures. Based on real-world large model training on datacenter-scale infrastructures, we show 14~32% of all GPU hours are spent on communication with no overlapping computation. To minimize the outstanding communication latency, in this work, we develop an agile performance modeling framework to guide parallelization and hardware-software co-design strategies. Using the suite of real-world large ML models on state-of-the-art GPU training hardware, we demonstrate 2.24x and 5.27x throughput improvement potential for pre-training and inference scenarios, respectively.
    
[^39]: 记忆健身房：对内存为基础的智能体在无尽任务中的部分可观察挑战

    Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes. (arXiv:2309.17207v1 [cs.LG])

    [http://arxiv.org/abs/2309.17207](http://arxiv.org/abs/2309.17207)

    本研究提出了记忆健身房，一种用于测试利用记忆为基础的深度强化学习智能体能力的基准。它包括部分可观察的二维环境和离散控制，并通过无尽任务对记忆能力、噪声抗性和泛化能力进行评估。研究还提供了一个使用Transformer-XL和Proximal Policy Optimization驱动的实现。

    

    记忆健身房介绍了一个独特的基准测试，旨在测试深度强化学习智能体，特别是将门循环单元(GRU)与Transformer-XL(TrXL)相比，它们对于记忆长序列的能力、抗噪声和泛化能力。它采用了部分可观察的二维环境和离散控制，即Mortar Mayhem、Mystery Path和Searing Spotlights。这些最初是有限的环境被推广为新颖的无尽任务，作为一种自动课程，从车游戏"I packed my bag"中汲取灵感。这些无尽任务不仅有助于评估效率，而且有趣地评估了记忆为基础的方法的有效性。鉴于现有公开可用的记忆基准的稀缺性，我们提供了一个由TrXL和Proximal Policy Optimization驱动的实现。本实现利用TrXL作为以滑动窗口方法使用的情节性记忆。在有限环境的实验中，我们发现...

    Memory Gym introduces a unique benchmark designed to test Deep Reinforcement Learning agents, specifically comparing Gated Recurrent Unit (GRU) against Transformer-XL (TrXL), on their ability to memorize long sequences, withstand noise, and generalize. It features partially observable 2D environments with discrete controls, namely Mortar Mayhem, Mystery Path, and Searing Spotlights. These originally finite environments are extrapolated to novel endless tasks that act as an automatic curriculum, drawing inspiration from the car game ``I packed my bag". These endless tasks are not only beneficial for evaluating efficiency but also intriguingly valuable for assessing the effectiveness of approaches in memory-based agents. Given the scarcity of publicly available memory baselines, we contribute an implementation driven by TrXL and Proximal Policy Optimization. This implementation leverages TrXL as episodic memory using a sliding window approach. In our experiments on the finite environment
    
[^40]: 从分类到分割与可解释人工智能：关于裂纹检测和生长监测的研究

    From Classification to Segmentation with Explainable AI: A Study on Crack Detection and Growth Monitoring. (arXiv:2309.11267v1 [cs.CV])

    [http://arxiv.org/abs/2309.11267](http://arxiv.org/abs/2309.11267)

    本研究提出了一种将可解释人工智能应用于裂缝分割和监测的方法，通过从分类器的解释中导出分割结果，无需像素级注释。实验结果表明，该方法能有效进行裂缝分割和生长监测。

    

    监测基础设施的表面裂缝对于结构健康监测至关重要。自动视觉检测提供了一种有效的解决方案，特别是在难以到达的区域。机器学习方法已经证明了它们的有效性，但通常需要大量的注释数据集进行监督训练。一旦检测到裂缝，监测其严重程度通常需要对损害进行精确的分割。然而，对于分割而言，图像的像素级注释是一项劳动密集型的工作。为了减少这种成本，可以利用可解释人工智能（XAI）从分类器的解释中导出分割结果，仅需要弱图像级别的监督。本文提出将这种方法应用于分割和监测表面裂缝。我们评估了各种XAI方法的性能，并研究了这种方法如何促进严重程度量化和生长监测。结果表明，虽然得到的分割掩模可能较低质量，但采用这种方法仍能有效地进行裂纹分割。

    Monitoring surface cracks in infrastructure is crucial for structural health monitoring. Automatic visual inspection offers an effective solution, especially in hard-to-reach areas. Machine learning approaches have proven their effectiveness but typically require large annotated datasets for supervised training. Once a crack is detected, monitoring its severity often demands precise segmentation of the damage. However, pixel-level annotation of images for segmentation is labor-intensive. To mitigate this cost, one can leverage explainable artificial intelligence (XAI) to derive segmentations from the explanations of a classifier, requiring only weak image-level supervision. This paper proposes applying this methodology to segment and monitor surface cracks. We evaluate the performance of various XAI methods and examine how this approach facilitates severity quantification and growth monitoring. Results reveal that while the resulting segmentation masks may exhibit lower quality than th
    
[^41]: 使用扩散模型学习端到端信道编码

    Learning End-to-End Channel Coding with Diffusion Models. (arXiv:2309.10505v1 [cs.IT])

    [http://arxiv.org/abs/2309.10505](http://arxiv.org/abs/2309.10505)

    本文提出了使用扩散模型学习端到端信道编码的框架，并通过模拟实验证明了扩散模型能够准确学习信道分布从而实现接近最优的端到端符号误码率。

    

    通过扩散模型近似信道分布，本文提出了一个基于扩散模型的端到端信道编码框架，并提出了一种高效的训练算法。通过与各种信道模型的模拟实验，验证了扩散模型精确学习信道分布的能力，从而实现了接近最优的端到端符号误码率（SER）。

    The training of neural encoders via deep learning necessitates a differentiable channel model due to the backpropagation algorithm. This requirement can be sidestepped by approximating either the channel distribution or its gradient through pilot signals in real-world scenarios. The initial approach draws upon the latest advancements in image generation, utilizing generative adversarial networks (GANs) or their enhanced variants to generate channel distributions. In this paper, we address this channel approximation challenge with diffusion models, which have demonstrated high sample quality in image generation. We offer an end-to-end channel coding framework underpinned by diffusion models and propose an efficient training algorithm. Our simulations with various channel models establish that our diffusion models learn the channel distribution accurately, thereby achieving near-optimal end-to-end symbol error rates (SERs). We also note a significant advantage of diffusion models: A robu
    
[^42]: 边缘节点在联邦学习中的资源利用效率

    Toward efficient resource utilization at edge nodes in federated learning. (arXiv:2309.10367v1 [cs.LG])

    [http://arxiv.org/abs/2309.10367](http://arxiv.org/abs/2309.10367)

    本论文提出了一种受迁移学习启发的策略，旨在通过减少设备上的资源利用、以及减少服务器和网络的负载，提高联邦学习中边缘节点的效率。

    

    联邦学习（FL）使得边缘节点能够共同构建全局模型，而无需共享他们的数据。这是通过设备计算本地私有模型更新，然后由服务器进行聚合来实现的。然而，计算资源限制和网络通信对于大型深度学习应用中的较大模型大小可能成为严重瓶颈。边缘节点往往具有有限的硬件资源（RAM、CPU），而边缘的网络带宽和可靠性对于扩展联邦车队应用来说是一个问题。在本文中，我们提出并评估了一种受迁移学习启发的FL策略，以减少设备上的资源利用，以及每个全局训练轮次中服务器和网络的负载。对于每个本地模型更新，我们随机选择要训练的层，冻结模型的其余部分。通过这样做，我们可以通过排除所有未训练的部分来减少每轮的服务器负载和通信成本。

    Federated learning (FL) enables edge nodes to collaboratively contribute to constructing a global model without sharing their data. This is accomplished by devices computing local, private model updates that are then aggregated by a server. However, computational resource constraints and network communication can become a severe bottleneck for larger model sizes typical for deep learning applications. Edge nodes tend to have limited hardware resources (RAM, CPU), and the network bandwidth and reliability at the edge is a concern for scaling federated fleet applications. In this paper, we propose and evaluate a FL strategy inspired by transfer learning in order to reduce resource utilization on devices, as well as the load on the server and network in each global training round. For each local model update, we randomly select layers to train, freezing the remaining part of the model. In doing so, we can reduce both server load and communication costs per round by excluding all untrained
    
[^43]: 探索基于元信息的基于音频的零样本鸟类分类

    Exploring Meta Information for Audio-based Zero-shot Bird Classification. (arXiv:2309.08398v1 [cs.SD])

    [http://arxiv.org/abs/2309.08398](http://arxiv.org/abs/2309.08398)

    该研究探索了如何利用元信息来改善基于音频的零样本鸟类分类，并通过连接不同的元数据和音频特征获得最佳结果。

    

    被动声学监测和机器学习的进步已经为计算生物声学研究提供了大量数据集。然而，对于稀有和代表性不足的物种来说，数据稀缺仍然是一个问题。本研究通过使用丰富和多样的元数据，以鸟类物种为例进行了探索，研究了如何利用元信息来改善零样本音频分类。我们研究了三种不同的元数据来源：通过(S)BERT编码的文本鸟鸣描述，功能特性(AVONET)和鸟类生活史(BLH)特征。作为音频特征，我们提取音频频谱图变换器(AST)嵌入，并通过采用单个线性层将其投影到辅助信息的维度上。然后，我们采用点积作为兼容性函数，并使用标准的零样本学习排名铰链损失确定正确的类别。通过连接AVONET和BLH特征，我们获得了最佳结果。

    Advances in passive acoustic monitoring and machine learning have led to the procurement of vast datasets for computational bioacoustic research. Nevertheless, data scarcity is still an issue for rare and underrepresented species. This study investigates how meta-information can improve zero-shot audio classification, utilising bird species as an example case study due to the availability of rich and diverse metadata. We investigate three different sources of metadata: textual bird sound descriptions encoded via (S)BERT, functional traits (AVONET), and bird life-history (BLH) characteristics. As audio features, we extract audio spectrogram transformer (AST) embeddings and project them to the dimension of the auxiliary information by adopting a single linear layer. Then, we employ the dot product as compatibility function and a standard zero-shot learning ranking hinge loss to determine the correct class. The best results are achieved by concatenating the AVONET and BLH features attaini
    
[^44]: 在教育数据挖掘中深度学习技术的综合调研

    A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining. (arXiv:2309.04761v1 [cs.LG])

    [http://arxiv.org/abs/2309.04761](http://arxiv.org/abs/2309.04761)

    本调研综合审查了在教育数据挖掘中深度学习技术的最新研究进展，包括对知识跟踪、学生不良行为检测、性能预测和个性化推荐等典型教育场景的应用。同时提供了公共数据集和处理工具的综合概述，并指出了未来的研究方向。

    

    教育数据挖掘(EDM)作为研究的重要领域，利用计算技术来分析教育数据。随着教育数据的复杂性和多样性增加，深度学习技术在解决分析和建模这些数据所面临的挑战方面表现出了显著的优势。本调研旨在系统地审查深度学习在EDM领域的最新研究进展。我们首先提供了关于EDM和深度学习的简要介绍，强调了它们在现代教育环境中的重要性。接下来，我们详细回顾了在四个典型教育场景中应用的深度学习技术，包括知识跟踪、学生不良行为检测、性能预测和个性化推荐。此外，我们还提供了EDM的公共数据集和处理工具的综合概述。最后，我们指出了该研究领域的新兴趋势和未来方向。

    Educational Data Mining (EDM) has emerged as a vital field of research, which harnesses the power of computational techniques to analyze educational data. With the increasing complexity and diversity of educational data, Deep Learning techniques have shown significant advantages in addressing the challenges associated with analyzing and modeling this data. This survey aims to systematically review the state-of-the-art in EDM with Deep Learning. We begin by providing a brief introduction to EDM and Deep Learning, highlighting their relevance in the context of modern education. Next, we present a detailed review of Deep Learning techniques applied in four typical educational scenarios, including knowledge tracing, undesirable student detecting, performance prediction, and personalized recommendation. Furthermore, a comprehensive overview of public datasets and processing tools for EDM is provided. Finally, we point out emerging trends and future directions in this research area.
    
[^45]: 通过均匀的Tanh变换的面部解析的遮挡感知深度卷积神经网络

    Occlusion-Aware Deep Convolutional Neural Network via Homogeneous Tanh-transforms for Face Parsing. (arXiv:2308.15323v1 [cs.CV])

    [http://arxiv.org/abs/2308.15323](http://arxiv.org/abs/2308.15323)

    本论文提出了一种通过均匀的Tanh变换进行面部解析的遮挡感知深度卷积神经网络。该方法解决了面部遮挡问题，并且能够融合更多上下文信息。同时，引入了遮挡感知损失，提高了边界的识别能力。

    

    面部解析是为每个语义面部组件推断像素级标签图的过程。以前的方法通常对无遮挡的面部效果良好，但在面部遮挡下忽略了遮挡和忽视了单个面部外一些上下文区域，尤其是在COVID-19流行期间，面部遮挡已经成为一种常见情况。受图像照明理论的启发，我们提出了一种新颖的图像预处理方法，即由四个Tanh变换组成的均匀Tanh变换，将中央视觉和周边视觉融合在一起。我们的方法解决了遮挡下面部解析的困境，并压缩了更多的周围上下文信息。基于均匀的Tanh变换，我们提出了一种适用于遮挡面部解析的遮挡感知卷积神经网络。它结合了Tanh极坐标空间和Tanh笛卡尔空间中的信息，能够增强感受野。此外，我们引入了一种遮挡感知损失，专注于遮挡边界。

    Face parsing infers a pixel-wise label map for each semantic facial component. Previous methods generally work well for uncovered faces, however overlook the facial occlusion and ignore some contextual area outside a single face, especially when facial occlusion has become a common situation during the COVID-19 epidemic. Inspired by the illumination theory of image, we propose a novel homogeneous tanh-transforms for image preprocessing, which made up of four tanh-transforms, that fuse the central vision and the peripheral vision together. Our proposed method addresses the dilemma of face parsing under occlusion and compresses more information of surrounding context. Based on homogeneous tanh-transforms, we propose an occlusion-aware convolutional neural network for occluded face parsing. It combines the information both in Tanh-polar space and Tanh-Cartesian space, capable of enhancing receptive fields. Furthermore, we introduce an occlusion-aware loss to focus on the boundaries of occ
    
[^46]: 通过近似传递消息实现结构化广义线性模型的谱估计器

    Spectral Estimators for Structured Generalized Linear Models via Approximate Message Passing. (arXiv:2308.14507v1 [math.ST])

    [http://arxiv.org/abs/2308.14507](http://arxiv.org/abs/2308.14507)

    本论文研究了针对广义线性模型的参数估计问题，提出了一种通过谱估计器进行预处理的方法。通过对测量进行特征协方差矩阵Σ表示，分析了谱估计器在结构化设计中的性能，并确定了最优预处理以最小化样本数量。

    

    我们考虑从广义线性模型中的观测中进行参数估计的问题。谱方法是一种简单而有效的估计方法：它通过对观测进行适当预处理得到的矩阵的主特征向量来估计参数。尽管谱估计器被广泛使用，但对于结构化（即独立同分布的高斯和哈尔）设计，目前仅有对谱估计器的严格性能表征以及对数据进行预处理的基本方法可用。相反，实际的设计矩阵具有高度结构化并且表现出非平凡的相关性。为解决这个问题，我们考虑了捕捉测量的非各向同性特性的相关高斯设计，通过特征协方差矩阵Σ进行表示。我们的主要结果是对于这种情况下谱估计器性能的精确渐近分析。然后，可以通过这一结果来确定最优预处理，从而最小化所需样本的数量。

    We consider the problem of parameter estimation from observations given by a generalized linear model. Spectral methods are a simple yet effective approach for estimation: they estimate the parameter via the principal eigenvector of a matrix obtained by suitably preprocessing the observations. Despite their wide use, a rigorous performance characterization of spectral estimators, as well as a principled way to preprocess the data, is available only for unstructured (i.e., i.i.d. Gaussian and Haar) designs. In contrast, real-world design matrices are highly structured and exhibit non-trivial correlations. To address this problem, we consider correlated Gaussian designs which capture the anisotropic nature of the measurements via a feature covariance matrix $\Sigma$. Our main result is a precise asymptotic characterization of the performance of spectral estimators in this setting. This then allows to identify the optimal preprocessing that minimizes the number of samples needed to meanin
    
[^47]: 利用强化学习释放三维装载有限车辆路径问题的碳减排潜力

    Unlocking Carbon Reduction Potential with Reinforcement Learning for the Three-Dimensional Loading Capacitated Vehicle Routing Problem. (arXiv:2307.12136v1 [cs.LG])

    [http://arxiv.org/abs/2307.12136](http://arxiv.org/abs/2307.12136)

    该论文提出了一个利用强化学习方法解决三维装载有限车辆路径问题的模型，该方法能够在线性时间内有效计算共载和路径的可行解决方案，从而释放了碳减排的潜力。

    

    重型货车是供应链交付系统的重要支柱，但在英国仅具有60％的装载效率，对碳排放有显著贡献。协同车辆路径规划被提出作为提高效率的解决方案，但仍面临挑战。一个关键挑战是有效计算共载和路径的可行解决方案。当前的运筹学方法在问题规模增大时存在非线性扩展，因此只能限于在有限的地理范围内计算结果以满足日常运营的时间要求。这仅允许在路径规划上寻找局部最优解，无法实现全局优化潜力。我们开发了一个强化学习模型来近线性时间解决三维装载有限车辆路径问题。虽然此问题在运筹学中得到了广泛研究，但没有关于用强化学习解决该问题的出版物存在。

    Heavy goods vehicles are vital backbones of the supply chain delivery system but also contribute significantly to carbon emissions with only 60% loading efficiency in the United Kingdom. Collaborative vehicle routing has been proposed as a solution to increase efficiency, but challenges remain to make this a possibility. One key challenge is the efficient computation of viable solutions for co-loading and routing. Current operations research methods suffer from non-linear scaling with increasing problem size and are therefore bound to limited geographic areas to compute results in time for day-to-day operations. This only allows for local optima in routing and leaves global optimisation potential untouched. We develop a reinforcement learning model to solve the three-dimensional loading capacitated vehicle routing problem in approximately linear time. While this problem has been studied extensively in operations research, no publications on solving it with reinforcement learning exist.
    
[^48]: TinyTrain：在极端边缘进行深度神经网络训练

    TinyTrain: Deep Neural Network Training at the Extreme Edge. (arXiv:2307.09988v1 [cs.LG])

    [http://arxiv.org/abs/2307.09988](http://arxiv.org/abs/2307.09988)

    TinyTrain是一种在设备上进行训练的方法，通过选择性更新模型的部分并处理数据稀缺问题，大大缩短了训练时间。通过任务自适应的稀疏更新方法，TinyTrain能够在高准确性的同时减小计算和内存占用，对未知任务表现出色。

    

    设备上的训练对于用户个性化和隐私至关重要。随着物联网设备和微控制器单元（MCU）的普及，由于受限的内存和计算资源以及标注的用户数据的有限可用性，这项任务变得更加具有挑战性。尽管如此，先前的研究忽视了数据稀缺问题，需要过长的训练时间（例如几个小时），或者导致重大的准确性损失（≥10%）。我们提出了TinyTrain，一种设备上的训练方法，通过选择性更新模型的部分，并明确处理数据稀缺问题，大幅缩短了训练时间。TinyTrain引入了一种任务自适应的稀疏更新方法，根据多目标准则动态选择层/通道，同时捕捉用户数据、内存和目标设备的计算能力，从而在未知任务上获得高准确性，并减小计算和内存占用。TinyTrain在整体微调的基础上表现出色。

    On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCU), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss ($\geq$10\%). We propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the enti
    
[^49]: 图位置和结构编码器

    Graph Positional and Structural Encoder. (arXiv:2307.07107v1 [cs.LG])

    [http://arxiv.org/abs/2307.07107](http://arxiv.org/abs/2307.07107)

    这是一个关于图位置和结构编码器的研究，提出了图位置和结构编码器（GPSE），它能有效地捕捉多个PSE的共同潜在表示，并在各种图预测任务中取得显著的性能提升。

    

    位置和结构编码（PSE）可以更好地在图中识别节点，因为一般图缺乏规范的节点顺序。这使得PSE成为赋予现代图神经网络（GNN）和特别是图变换器重要功能的工具。然而，设计适用于各种图预测任务的PSE是一个具有挑战性且未解决的问题。在这里，我们提出了图位置和结构编码器（GPSE），这是首次尝试训练一个能够捕捉丰富的PSE表示以增强任何GNN的图编码器。GPSE可以有效地学习多个PSE的共同潜在表示，并且具有高度可传输性。在特定图数据集上训练的编码器可以在从显著不同分布甚至模态的数据集上有效地使用。我们显示，在广泛的基准测试中，经过GPSE增强的模型在某些任务中显著提高性能，同时与明确使用PSE的模型性能相当。

    Positional and structural encodings (PSE) enable better identifiability of nodes within a graph, as in general graphs lack a canonical node ordering. This renders PSEs essential tools for empowering modern GNNs, and in particular graph Transformers. However, designing PSEs that work optimally for a variety of graph prediction tasks is a challenging and unsolved problem. Here, we present the graph positional and structural encoder (GPSE), a first-ever attempt to train a graph encoder that captures rich PSE representations for augmenting any GNN. GPSE can effectively learn a common latent representation for multiple PSEs, and is highly transferable. The encoder trained on a particular graph dataset can be used effectively on datasets drawn from significantly different distributions and even modalities. We show that across a wide range of benchmarks, GPSE-enhanced models can significantly improve the performance in certain tasks, while performing on par with those that employ explicitly c
    
[^50]: S-HR-VQVAE: 序列分层残差学习向量量化变分自编码器用于视频预测

    S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction. (arXiv:2307.06701v1 [cs.CV])

    [http://arxiv.org/abs/2307.06701](http://arxiv.org/abs/2307.06701)

    S-HR-VQVAE是一种序列分层残差学习向量量化变分自编码器，通过结合分层残差向量量化变分自编码器（HR-VQVAE）和时空PixelCNN（ST-PixelCNN）的能力，解决了视频预测中的主要挑战，并在KTH人体动作和Moving-MNIST任务上取得了较好的实验结果。

    

    我们提出了一种新的模型，将我们最近提出的分层残差向量量化变分自编码器（HR-VQVAE）与一种新颖的时空PixelCNN（ST-PixelCNN）相结合，用来解决视频预测任务。我们将这种方法称为序列分层残差学习向量量化变分自编码器（S-HR-VQVAE）。通过利用HR-VQVAE在对静止图像进行建模时的内在能力和紧凑表示，以及ST-PixelCNN处理时空信息的能力， S-HR-VQVAE能够更好地应对视频预测中的主要挑战，包括学习时空信息、处理高维数据、消除模糊预测和隐式建模物理特性。对KTH人体动作和Moving-MNIST任务的大量实验证明，我们的模型在定量和定性评估方面与顶级视频预测技术相比具有优势。

    We address the video prediction task by putting forth a novel model that combines (i) our recently proposed hierarchical residual vector quantized variational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN (ST-PixelCNN). We refer to this approach as a sequential hierarchical residual learning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging the intrinsic capabilities of HR-VQVAE at modeling still images with a parsimonious representation, combined with the ST-PixelCNN's ability at handling spatiotemporal information, S-HR-VQVAE can better deal with chief challenges in video prediction. These include learning spatiotemporal information, handling high dimensional data, combating blurry prediction, and implicit modeling of physical characteristics. Extensive experimental results on the KTH Human Action and Moving-MNIST tasks demonstrate that our model compares favorably against top video prediction techniques both in quantitative and qualitative evalu
    
[^51]: FFB:面向处理组公平方法的公平公正基准

    FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods. (arXiv:2306.09468v1 [cs.LG])

    [http://arxiv.org/abs/2306.09468](http://arxiv.org/abs/2306.09468)

    本文提出了针对处理中组公平方法的公平公正基准框架（FFB），并进行了全面分析。该工作的关键贡献包括提供灵活、可扩展、极简和面向研究的开源代码；建立统一的公平方法基准测试流水线；进行广泛的基准测试，从 $\mathbf{45,079}$ 个实验中获取关键见解。

    

    本文介绍了公平公正基准（FFB），这是一种针对处理中组公平方法的基准框架。确保机器学习的公平性对于符合道德和法律要求至关重要。然而，由于实验设置的不一致，缺乏易于访问的算法实现以及当前公平度量工具的有限可扩展性，存在比较和开发公平度量方法的挑战。为了解决这些问题，我们介绍了一个开源、标准化的基准，用于评估处理中的组公平方法，并提供了对确保不同民族/种族群体公平的最先进方法的全面分析。该工作提供了以下关键贡献：提供灵活、可扩展、极简和面向研究的开源代码；建立统一的公平方法基准测试流水线；进行广泛的基准测试，从 $\mathbf{45,079}$ 个实验中获取关键见解。

    This paper introduces the Fair Fairness Benchmark (\textsf{FFB}), a benchmarking framework for in-processing group fairness methods. Ensuring fairness in machine learning is critical for ethical and legal compliance. However, there exist challenges in comparing and developing of fairness methods due to inconsistencies in experimental settings, lack of accessible algorithmic implementations, and limited extensibility of current fairness packages and tools. To address these issues, we introduce an open-source, standardized benchmark for evaluating in-processing group fairness methods and provide a comprehensive analysis of state-of-the-art methods to ensure different notions of group fairness. This work offers the following key contributions: the provision of flexible, extensible, minimalistic, and research-oriented open-source code; the establishment of unified fairness method benchmarking pipelines; and extensive benchmarking, which yields key insights from $\mathbf{45,079}$ experiment
    
[^52]: ShiftAddViT：多种乘法原语混合实现高效的视觉变换器

    ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. (arXiv:2306.06446v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06446](http://arxiv.org/abs/2306.06446)

    ShiftAddViT通过使用位移和加法等多种乘法原语对ViT进行重新参数化，实现了减少乘法操作的高效视觉变换器模型，可以在GPU上实现端到端的推理加速，无需从头训练。

    

    视觉变换器（ViT）展示了令人印象深刻的性能，并成为多个视觉任务的统一骨干。但是，ViTs中的注意力和多层感知器（MLPs）由于密集的乘法而不够高效，导致训练和推理代价高昂。为此，我们提出了一种将预训练的ViT以多种乘法原语（例如位移和加法）重新参数化的方法，以实现全新类型的减少乘法的模型，称为ShiftAddViT，旨在实现GPU上的端到端推理加速，无需从头开始训练。具体而言，我们将查询和键映射为汉明空间中的二进制码之后，采用加法核对查询、键和值之间的MatMul进行重新参数化。剩余的MLPs或线性层则采用位移核进行重新参数化。我们利用TVM在GPU上实施并优化这些定制核，以实现实际硬件部署。我们发现，这种重新参数化方法可以显著提高推理速度，而无需从头开始训练。

    Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. But both attention and multi-layer perceptions (MLPs) in ViTs are not efficient enough due to dense multiplications, resulting in costly training and inference. To this end, we propose to reparameterize the pre-trained ViT with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\textbf{ShiftAddViT}$, which aims for end-to-end inference speedups on GPUs without the need of training from scratch. Specifically, all $\texttt{MatMuls}$ among queries, keys, and values are reparameterized by additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized by shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameter
    
[^53]: 机器学习分析、识别和预测帕金森病亚型和进展

    Analysis, Identification and Prediction of Parkinson's disease sub-types and progression through Machine Learning. (arXiv:2306.04748v1 [cs.LG])

    [http://arxiv.org/abs/2306.04748](http://arxiv.org/abs/2306.04748)

    本文使用监督和无监督机器学习方法鉴别出三种不同的帕金森病亚型，并生成患者个性化的症状进展预测，有助于制定针对性干预和改善患者预后。

    

    帕金森病（PD）是一种常见的神经退行性疾病，患者表现多种多样，但对其潜在原因和症状进展的了解还很有限。帕金森进展标志物计划（PPMI）收集了多个患者群的详细纵向数据以确定生物标志物并辅助干预方法的制定。尽管已经有超过110个机器学习研究使用PPMI数据库，但大部分研究仅聚焦于监督模型用于诊断预测，限制了对患者多样性和进展的认识。本文通过结合监督和无监督的机器学习方法来鉴别亚型，准确预测帕金森患者疾病进程。本研究利用PPMI数据库中5年的纵向数据并在之前研究的基础上，整合无监督的病人聚类和病人当前和未来症状的预测。我们的方法鉴别出三个不同的帕金森病亚型，并为个体患者生成个性化的症状进展预测。这种方法提高了预测疾病进展的准确性，帮助制定个性化干预方法并改善患者的预后。

    Parkinson's disease (PD) is a prevalent neurodegenerative disorder with varying patient trajectories, yet little is understood about the underlying causes and symptom progression. The Parkinson's Progression Markers Initiative (PPMI) has collected comprehensive longitudinal data from diverse patient cohorts to identify biomarkers and aid in the development of interventions. Despite over 110 machine learning studies using the PPMI database, the majority have focused on supervised models for diagnosis prediction, which has limited impact on understanding patient variability and progression. This paper addresses this gap by combining supervised and unsupervised machine learning methods to identify subtypes that accurately predict disease progression in Parkinson's patients. Building upon previous work, we replicate and extend the study by integrating unsupervised patient clustering and prediction of present and future symptoms using 5 additional years of longitudinal data from the Progres
    
[^54]: 稀疏性能够提高神经网络的隐私

    Sparsity in neural networks can improve their privacy. (arXiv:2304.10553v1 [cs.LG])

    [http://arxiv.org/abs/2304.10553](http://arxiv.org/abs/2304.10553)

    稀疏性能够提高神经网络的隐私，并且能够保持网络的表现

    

    本文研究稀疏性如何提高神经网络对成员推理攻击的鲁棒性。实验结果表明，稀疏性能够提高网络的隐私，同时保持其在任务上的相似表现。这个实证研究完善和扩展了现有文献。

    This article measures how sparsity can make neural networks more robust to membership inference attacks. The obtained empirical results show that sparsity improves the privacy of the network, while preserving comparable performances on the task at hand. This empirical study completes and extends existing literature.
    
[^55]: 异质性下的对比学习

    Contrastive Learning under Heterophily. (arXiv:2303.06344v1 [cs.LG])

    [http://arxiv.org/abs/2303.06344](http://arxiv.org/abs/2303.06344)

    本文提出了第一个图形对比学习方法，以解决现有图形对比学习方法在异质性下无法学习高质量表示的问题。

    This paper proposes the first graph contrastive learning method to address the problem that existing graph contrastive learning methods cannot learn high-quality representations under heterophily.

    图神经网络是一种强大的工具，可以在具有特定任务节点标签的情况下学习节点表示。然而，在许多应用中，为图形获取标签是昂贵的。这在大型图形的情况下尤其如此。为了解决这个问题，已经有一些工作在没有标签的情况下以自监督的方式学习节点表示。对比学习（CL）在以自监督的方式学习表示方面特别受欢迎。一般来说，CL方法通过最大化相同示例的增强视图的表示之间的相似性，并最小化不同示例的增强视图之间的相似性来工作。然而，现有的图形CL方法不能在异质性下学习高质量的表示，其中连接的节点倾向于属于不同的类。这是因为在异质性下，同一示例的增强可能彼此不相似。在这项工作中，我们通过提出第一个图形对比学习方法来解决上述问题。

    Graph Neural Networks are powerful tools for learning node representations when task-specific node labels are available. However, obtaining labels for graphs is expensive in many applications. This is particularly the case for large graphs. To address this, there has been a body of work to learn node representations in a self-supervised manner without labels. Contrastive learning (CL), has been particularly popular to learn representations in a self-supervised manner. In general, CL methods work by maximizing the similarity between representations of augmented views of the same example, and minimizing the similarity between augmented views of different examples. However, existing graph CL methods cannot learn high-quality representations under heterophily, where connected nodes tend to belong to different classes. This is because under heterophily, augmentations of the same example may not be similar to each other. In this work, we address the above problem by proposing the first graph
    
[^56]: 通过可变大小的压缩性建立数据相关的泛化界限

    Data-dependent Generalization Bounds via Variable-Size Compressibility. (arXiv:2303.05369v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.05369](http://arxiv.org/abs/2303.05369)

    本文通过引入可变大小压缩性框架，建立了一种新的数据相关的泛化误差上界。该方法将算法的泛化误差与其输入数据的可变大小压缩率相关联，并提供了依赖于经验分布而非未知分布的界限。此外，该方法还可以推导出输入数据和输出假设随机变量的任何函数的泛化界限，并包含并可能优于现有的基于PAC-Bayes和数据相关内在维度的界限。

    

    本文通过引入“可变大小压缩性”框架，建立了一种新的数据相关泛化误差的上界。在这个框架中，算法的泛化误差与其输入数据的可变大小“压缩率”相关联。通过这种方式，我们得到的界限依赖于手头给定输入数据的经验分布，而不是其未知分布。我们建立的新的泛化界限包括尾部界限、期望值的尾部界限和期望界限。此外，我们的框架还可以推导出对输入数据和输出假设随机变量的任何函数的泛化界限。特别是，这些泛化界限包含并可能优于几种现有的基于PAC-Bayes和数据相关内在维度的界限，这些界限作为特殊情况得到复原，从而揭示出我们方法的统一特性。

    In this paper, we establish novel data-dependent upper bounds on the generalization error through the lens of a "variable-size compressibility" framework that we introduce newly here. In this framework, the generalization error of an algorithm is linked to a variable-size 'compression rate' of its input data. This is shown to yield bounds that depend on the empirical measure of the given input data at hand, rather than its unknown distribution. Our new generalization bounds that we establish are tail bounds, tail bounds on the expectation, and in-expectations bounds. Moreover, it is shown that our framework also allows to derive general bounds on any function of the input data and output hypothesis random variables. In particular, these general bounds are shown to subsume and possibly improve over several existing PAC-Bayes and data-dependent intrinsic dimension-based bounds that are recovered as special cases, thus unveiling a unifying character of our approach. For instance, a new da
    
[^57]: 基于Shapley值的算法公平性数据再加权方法FairShap

    FairShap: A Data Re-weighting Approach for Algorithmic Fairness based on Shapley Values. (arXiv:2303.01928v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01928](http://arxiv.org/abs/2303.01928)

    FairShap是一种通过数据重估计算法决策公平性的预处理方法，使用Shapley值估值，能够提高模型的公平性和精确性，并且易于解释。

    

    算法公平性是极其重要的社会问题，然而当前大规模机器学习模型的趋势要求使用通常存在偏差的海量数据进行训练。在这种情况下，专注于建模和纠正数据偏差的预处理方法成为有价值的方法。在本文中，我们提出了一种新颖的基于Shapley值进行数据估值的预处理（再加权）方法FairShap，用于公平的算法决策制定。我们的方法是模型无关且易于解释，因为它衡量每个训练数据点对预定义的公平指标的贡献。我们在多个最先进的数据集上进行了实证验证，这些数据集具有不同的性质，有各种培训场景和模型，并展示了它如何优于其他方法，产生更公平的模型并且准确度更高或相似。我们还通过直方图和潜空间可视化来说明FairShap的解释性。我们认为，这对于在大数据时代确保算法决策公平性是重要的一步。

    Algorithmic fairness is of utmost societal importance, yet the current trend in large-scale machine learning models requires training with massive datasets that are typically biased. In this context, pre-processing methods that focus on modeling and correcting bias in the data emerge as valuable approaches. In this paper, we propose FairShap, a novel pre-processing (re-weighting) method for fair algorithmic decision-making through data valuation by means of Shapley Values. Our approach is model agnostic and easily interpretable, as it measures the contribution of each training data point to a predefined fairness metric. We empirically validate FairShap on several state-of-the-art datasets of different nature, with a variety of training scenarios and models and show how it outperforms other methods, yielding fairer models with higher or similar levels of accuracy. We also illustrate FairShap's interpretability by means of histograms and latent space visualizations. We believe that this 
    
[^58]: 分布偏移的标签对齐正则化

    Label Alignment Regularization for Distribution Shift. (arXiv:2211.14960v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14960](http://arxiv.org/abs/2211.14960)

    这篇论文提出了一种用于无监督领域自适应的正则化方法，通过鼓励目标域中的预测与其前几个奇异向量对齐来实现。与传统方法不同的是，这个方法通过正则化分类器与无监督目标数据对齐，而不是正则化表示。通过消除对最优联合风险假设的依赖，该方法展示了很好的效果。

    

    最近的研究强调了监督学习中的标签对齐属性（LAP），即数据集中所有标签的向量大部分在数据矩阵的前几个奇异向量的张成空间内。受到这一观察的启发，我们提出了一种无监督领域自适应的正则化方法，鼓励目标域中的预测与其前几个奇异向量对齐。与传统的领域适应方法专注于正则化表示不同，我们相反，通过在源域和目标域中使用LAP，用正则化分类器与无监督目标数据对齐。理论分析表明，在一定的假设下，我们的解决方案位于目标域数据的前几个右奇异向量的张成空间内，并与最优解对齐。通过消除经典领域适应理论中常见的最优联合风险假设的依赖，我们展示了该方法的有效性。

    Recent work has highlighted the label alignment property (LAP) in supervised learning, where the vector of all labels in the dataset is mostly in the span of the top few singular vectors of the data matrix. Drawing inspiration from this observation, we propose a regularization method for unsupervised domain adaptation that encourages alignment between the predictions in the target domain and its top singular vectors. Unlike conventional domain adaptation approaches that focus on regularizing representations, we instead regularize the classifier to align with the unsupervised target data, guided by the LAP in both the source and target domains. Theoretical analysis demonstrates that, under certain assumptions, our solution resides within the span of the top right singular vectors of the target domain data and aligns with the optimal solution. By removing the reliance on the commonly used optimal joint risk assumption found in classic domain adaptation theory, we showcase the effectivene
    
[^59]: 图形反事实解释的综述: 定义, 方法, 评估

    A Survey on Graph Counterfactual Explanations: Definitions, Methods, Evaluation. (arXiv:2210.12089v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12089](http://arxiv.org/abs/2210.12089)

    这篇综述研究了图形反事实解释的概念、方法、评估及其应用于图神经网络的情况，提供了分类法、统一的符号表示、基准数据集和评估指标，并对十四种方法、二十二个数据集和十九个指标进行了讨论和整合。未来的工作主要集中在解决开放的挑战上。

    

    图神经网络 (GNNs) 在社区检测和分子分类方面表现出色。反事实解释 (CE) 提供反例来克服黑盒模型的透明度限制。由于对图学习的关注不断增长，我们将重点关注 GNNs 的 CE 概念。我们分析了非常规的手段，提供了分类法，统一的符号表示，以及基准数据集和评估指标。我们讨论了十四种方法，它们的评估协议，二十二个数据集和十九个指标。我们整合了大多数方法到 GRETEL 库中，进行了实证评估，以了解它们的优势和缺点。我们强调了开放的挑战和未来的工作。

    Graph Neural Networks (GNNs) perform well in community detection and molecule classification. Counterfactual Explanations (CE) provide counter-examples to overcome the transparency limitations of black-box models. Due to the growing attention in graph learning, we focus on the concepts of CE for GNNs. We analysed the SoA to provide a taxonomy, a uniform notation, and the benchmarking datasets and evaluation metrics. We discuss fourteen methods, their evaluation protocols, twenty-two datasets, and nineteen metrics. We integrated the majority of methods into the GRETEL library to conduct an empirical evaluation to understand their strengths and pitfalls. We highlight open challenges and future work.
    

