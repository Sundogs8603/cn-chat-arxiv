# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification](https://rss.arxiv.org/abs/2402.01274) | 本研究评估了大规模自监督模型在少样本音频分类中的性能，并发现在一些少样本问题中取得了最先进的性能，同时发现语音为基础的少样本问题与多个下游音频任务之间存在较强的相关性。 |
| [^2] | [Efficient Causal Graph Discovery Using Large Language Models](https://rss.arxiv.org/abs/2402.01207) | 提出了一个新的框架，利用大型语言模型进行高效的因果图发现，采用了广度优先搜索方法，只需要线性数量的查询，同时能轻松结合观察数据以提高性能，具有高效性和数据效率，并在真实因果图上取得了最先进的结果，展示了其在不同领域的广泛适用性潜力。 |
| [^3] | [Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning](https://rss.arxiv.org/abs/2402.00910) | 本文提出了一种通过集成学习和正则化微调的方法，解决AI模型中的偏见问题。该方法可通过在小数据集和有偏的预训练模型上训练多个对抗偏见的模型，并使用集成学习得到无偏的预测结果。通过实验证明了该方法在CIFAR10和HAM10000数据集上的有效性。 |
| [^4] | [AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability](https://arxiv.org/abs/2402.09404) | AQA-Bench是一个交互式基准测试，用于评估大型语言模型在算法上下文中的顺序推理能力。研究发现闭源模型表现出更强的顺序推理能力，显著优于开源模型。 |
| [^5] | [Reinforcement Learning from Human Feedback with Active Queries](https://arxiv.org/abs/2402.09401) | 本文提出了一种基于主动查询的强化学习方法，用于解决与人类反馈的对齐问题。通过在强化学习过程中减少人工标注偏好数据的需求，该方法具有较低的代价，并在实验中表现出较好的性能。 |
| [^6] | [Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference](https://arxiv.org/abs/2402.09398) | 这项研究提出了一个称为LESS的方法，通过集成一个固定尺寸的缓存和基于驱逐的缓存方法，可以在大型语言模型中减小内存占用的问题，同时保持全部标记的可查询能力，并在多种任务上显示出良好的性能。 |
| [^7] | [Active Disruption Avoidance and Trajectory Design for Tokamak Ramp-downs with Neural Differential Equations and Reinforcement Learning](https://arxiv.org/abs/2402.09387) | 本研究利用神经微分方程和强化学习方法，开发了一个能够安全降低等离子体电流并避免非正常放电的策略。通过对SPARC主要参考放电的模拟进行训练，成功将该策略转移到高准确度的模拟器上并取得了良好的效果。 |
| [^8] | [GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in Metagenomic Assembly](https://arxiv.org/abs/2402.09381) | GraSSRep是一种基于图的自监督学习方法，用于元基因组组装中的重复检测。它利用组装图的结构通过图神经网络将DNA序列分类为重复和非重复类别。通过自监督学习的方式，使用伪标签进行训练。 |
| [^9] | [Loss Shaping Constraints for Long-Term Time Series Forecasting](https://arxiv.org/abs/2402.09373) | 该论文提出了一种用于长期时间序列预测的受限学习方法，通过在每个时间步骤上设置损失上限来寻找最佳模型，以解决平均性能优化导致特定时间步骤上误差过大的问题。 |
| [^10] | [Transformers Can Achieve Length Generalization But Not Robustly](https://arxiv.org/abs/2402.09371) | Transformers在特定组合的数据格式和位置编码的情况下可以实现长度的泛化，但仍然存在脆弱性和大量方差。 |
| [^11] | [Pseudorandom Error-Correcting Codes](https://arxiv.org/abs/2402.09370) | 我们构建了一种伪随机纠错码，它对替换和删除错误具有鲁棒性，并且可以高效解码。我们还使用伪随机码提出了一种对语言模型输出进行水印处理的方案，该方案对裁剪和随机替换、删除具有恒定的鲁棒性。 |
| [^12] | [HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference](https://arxiv.org/abs/2402.09360) | HiRE是一种用于高效的LLM推理的高召回率的近似Top-k估计方法，通过压缩方案减少了模型参数传输和延迟。 |
| [^13] | [Integrating ChatGPT into Secure Hospital Networks: A Case Study on Improving Radiology Report Analysis](https://arxiv.org/abs/2402.09358) | 本研究首次将类似于ChatGPT的基于云的人工智能应用于安全的医院模型中，用于分析放射学报告，通过对比学习方法实现了95%以上的准确率，同时提高了预测可靠性和可解释性，对于医生来说具有重要意义。 |
| [^14] | [Mitigating Reward Hacking via Information-Theoretic Reward Modeling](https://arxiv.org/abs/2402.09345) | 本文提出了一种名为InfoRM的奖励建模框架，通过引入变分信息瓶颈目标和模型复杂度调节机制，解决了奖励作弊问题，并利用集成聚类偏差得分（ICDS）来检测奖励过度优化。 |
| [^15] | [3D-based RNA function prediction tools in rnaglib](https://arxiv.org/abs/2402.09330) | rnaglib是一个基于3D的RNA功能预测工具，可以用于在RNA 3D结构数据集上训练监督和非监督机器学习模型。 |
| [^16] | [Connecting Algorithmic Fairness to Quality Dimensions in Machine Learning in Official Statistics and Survey Production](https://arxiv.org/abs/2402.09328) | 这项研究将算法公平性与机器学习在官方统计和调查生产中的质量维度联系起来，扩展了质量框架，并调查了公平性与其他质量维度的互动。 |
| [^17] | [Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization](https://arxiv.org/abs/2402.09327) | 本论文研究了随机凸优化中记忆和学习之间的相互作用。通过量化学习算法对训练数据点揭示的信息来定义记忆，并准确定义了学习算法准确性与条件互信息（CMI）之间的权衡关系。在特定条件下，我们证明了学习算法的准确性与CMI之间的最佳边界。通过设计对手，我们进一步展示了记忆在随机凸优化中学习问题中的重要性。 |
| [^18] | [Stability and Multigroup Fairness in Ranking with Uncertain Predictions](https://arxiv.org/abs/2402.09326) | 本研究考虑了在不确定预测中如何表示稳定性和公平性的问题，并提出了一种新的不确定性感知排名函数。 |
| [^19] | [Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models](https://arxiv.org/abs/2402.09316) | 这项开创性研究提出了一种隐私保护方法，通过生成可被人类感知的图像，在经过授权的模型中保持准确的推理，同时规避其他未经授权的黑盒模型的推理，填补了之前的研究空白。 |
| [^20] | [Embracing the black box: Heading towards foundation models for causal discovery from time series data](https://arxiv.org/abs/2402.09305) | 本文研究了基于时间序列数据进行因果发现的问题，提出了一种称为因果预训练的方法，通过以监督方式学习从多变量时间序列到潜在因果图的映射，实现了在共享动力学的情况下的监督式因果发现。 |
| [^21] | [Immediate generalisation in humans but a generalisation lag in deep neural networks$\unicode{x2014}$evidence for representational divergence?](https://arxiv.org/abs/2402.09303) | 研究对比了人类和深度神经网络在图像分类中的行为差异，发现人类具有即时概括能力，而DNNs存在滞后概括现象，这表明了表示分歧的存在。 |
| [^22] | [Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code](https://arxiv.org/abs/2402.09299) | 这项研究关注如何在训练代码的语言模型中检测代码包含，以解决使用这些模型进行代码审计时的版权侵权问题。 |
| [^23] | [Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially Supervised Reinforcement Learning](https://arxiv.org/abs/2402.09290) | 该论文提出了部分监督强化学习（PSRL）框架，通过融合监督和非监督学习来生成更可解释的策略，同时利用状态估计器提取出监督语义状态信息，以及捕捉潜在状态信息。 |
| [^24] | [EcoVal: An Efficient Data Valuation Framework for Machine Learning](https://arxiv.org/abs/2402.09288) | EcoVal是一种高效的机器学习数据估值框架，通过估计每个数据的内在和外在价值，实现了快速实用地估算机器学习模型数据的价值。 |
| [^25] | [Nutrition Facts, Drug Facts, and Model Facts: Putting AI Ethics into Practice in Gun Violence Research](https://arxiv.org/abs/2402.09286) | 通过提供一个模型事实模板，该研究旨在在枪支暴力研究中建立AI的信任和透明度，并减少对易受剥削人群数据的不信任。此模板使一般用户能够评估模型的有效性和偏见。 |
| [^26] | [Synergistic eigenanalysis of covariance and Hessian matrices for enhanced binary classification](https://arxiv.org/abs/2402.09281) | 本论文提出了一种新颖的方法，通过将训练集上计算的协方差矩阵的特征分析与深度学习模型上计算的Hessian矩阵相结合，实现了二分类任务中的最优类别可分性。该方法通过最大化类间平均距离和最小化类内方差，以及将数据投影到两个矩阵的最相关特征方向的组合空间来实现最优类别可分性。实证验证显示了该方法的有效性。 |
| [^27] | [Hybrid Machine Learning techniques in the management of harmful algal blooms impact](https://arxiv.org/abs/2402.09271) | 本论文提出了一种将混合机器学习技术应用于有害藻类水华管理的方法，通过预测生产区域的活动状态来替代常规的毒素浓度检测方法，以解决采样不可能的情况下的风险预测问题。 |
| [^28] | [Transformers, parallel computation, and logarithmic depth](https://arxiv.org/abs/2402.09268) | transformers的关键区别性质是并行性，它们通过使用固定数量的自注意层实现对基本计算任务的高效解决，而这些任务无法被其他神经序列模型和亚二次变压器逼近高效解决。 |
| [^29] | [UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers](https://arxiv.org/abs/2402.09264) | UR2M是一个新颖的不确定性和资源感知的事件检测框架，针对微控制器上的应用，通过评估模型输出的可靠性来解决传统机器学习技术在数据分布变化时产生不准确预测的问题。 |
| [^30] | [Exploring the Relationship: Transformative Adaptive Activation Functions in Comparison to Other Activation Functions](https://arxiv.org/abs/2402.09249) | 变革性自适应激活函数(TAAF)被发现可以广义地推广现有激活函数，并利用了其他激活函数的相似概念，这使得TAAF成为神经网络中有潜力和适应性的重要组成部分。 |
| [^31] | [Momentum Approximation in Asynchronous Private Federated Learning](https://arxiv.org/abs/2402.09247) | 本文提出了动量近似方法，在异步私有联邦学习（FL）中有效结合了动量和异步协议的技术，通过最小化动量更新的偏差来改进模型性能。实证研究证明了动量近似在基准FL数据集上的有效性。 |
| [^32] | [Overview of the L3DAS23 Challenge on Audio-Visual Extended Reality](https://arxiv.org/abs/2402.09245) | L3DAS23挑战赛旨在推进机器学习在3D音频信号处理方面的合作研究，并提供了更新的数据集和基准模型，用于音频-视觉扩展现实应用中的3D语音增强和3D声音事件定位和检测。 |
| [^33] | [Switch EMA: A Free Lunch for Better Flatness and Sharpness](https://arxiv.org/abs/2402.09240) | 本研究提出了一种称为Switch EMA（SEMA）的方法，通过简单的修改指数移动平均（EMA）参数，可以帮助深度神经网络（DNN）达到更好的平坦性和锐度的概括最优解。通过在多个任务和数据集上进行实验证明了SEMA的有效性。 |
| [^34] | [Robust Training of Temporal GNNs using Nearest Neighbours based Hard Negatives](https://arxiv.org/abs/2402.09239) | 本研究提出了使用基于重要性的负样本抽样训练TGNNS的方法，并通过实证评估证明了其优越性能。 |
| [^35] | [Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models](https://arxiv.org/abs/2402.09236) | 本研究将因果表示学习和基础模型相结合，研究了如何从数据中学习人类可解释的概念。实验证明了这一统一方法的实用性。 |
| [^36] | [Multi-Hierarchical Surrogate Learning for Structural Dynamics of Automotive Crashworthiness Using Graph Convolutional Neural Networks](https://arxiv.org/abs/2402.09234) | 该论文提出了使用图卷积神经网络的多层次代理学习框架，用于汽车碰撞安全结构动力学研究。该框架能够通过创建一系列适应不同计算环境和准确度要求的代理模型，从而提高碰撞仿真的效率和精确度。 |
| [^37] | [Context Composing for Full Line Code Completion](https://arxiv.org/abs/2402.09230) | 本论文介绍了用于全行代码自动完成功能的Transformer模型的上下文组合方法，该功能已在PyCharm Pro IDE上得到应用，并在实际的Python用户A/B测试中证明了其有用性。 |
| [^38] | [Directional Convergence Near Small Initializations and Saddles in Two-Homogeneous Neural Networks](https://arxiv.org/abs/2402.09226) | 本文研究了两次齐次神经网络在小初值附近的梯度流动，发现权重会在方向上收敛到神经相关函数的KKT点和某些鞍点附近。 |
| [^39] | [Better-than-KL PAC-Bayes Bounds](https://arxiv.org/abs/2402.09201) | 本文提出了一种更好的比KL PAC-Bayes界限方法来估计序列均值，应用于预测器泛化误差的估计。 |
| [^40] | [Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling](https://arxiv.org/abs/2402.09199) | 本文提出了一种利用代理引导的高效重采样方法来改进黑盒AI生成文本检测。通过估计单词生成概率作为伪白盒特征，选择少量代表性词汇进行多次重采样，在包含人类文本和LLM生成文本的数据集上进行了实验，取得了出色的结果。 |
| [^41] | [Implementing local-explainability in Gradient Boosting Trees: Feature Contribution](https://arxiv.org/abs/2402.09197) | 本文提出了一个针对GBDT的特征贡献方法，通过计算每个节点的残差来实现局部可解释性。这种方法是GBDT算法的局部可解释性模型，也是一种独特的选择。 |
| [^42] | [Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization](https://arxiv.org/abs/2402.09179) | 本文介绍了针对不可信定制语言模型的指令后门攻击，通过在定制语言模型中设计带有后门指令的提示，实现攻击者预期的结果。攻击包括三个级别，不需要对后端语言模型进行任何修改。 |
| [^43] | [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks](https://arxiv.org/abs/2402.09177) | 本研究提出了一种新的攻击形式——上下文交互攻击，通过交互式与大型语言模型（LLMs）进行问答来引出有害信息。实验结果表明该方法的有效性。 |
| [^44] | [Nearly Optimal Regret for Decentralized Online Convex Optimization](https://arxiv.org/abs/2402.09173) | 本论文研究了分布式在线凸优化，开发了新的算法来分别降低凸函数和强凸函数的后悔边界，并填补了现有下界之间的差距。 |
| [^45] | [Evolving Restricted Boltzmann Machine-Kohonen Network for Online Clustering](https://arxiv.org/abs/2402.09167) | 本文提出一种将进化限制玻尔兹曼机和Kohonen网络相结合的在线聚类算法，通过使用ERBM处理流式数据和使用KNet更新聚类中心，有效地克服了聚类算法的常见挑战，提高了聚类性能。 |
| [^46] | [Deinterleaving of Discrete Renewal Process Mixtures with Application to Electronic Support Measures](https://arxiv.org/abs/2402.09166) | 本文提出了一种用于解缠织混合离散更新Markov链的新方法，通过最大化惩罚似然分数来恢复真实的符号分组，并在电子支援措施中的应用中展示了较好的性能。 |
| [^47] | [Unifying Invariance and Spuriousity for Graph Out-of-Distribution via Probability of Necessity and Sufficiency](https://arxiv.org/abs/2402.09165) | 该论文提出了一种统一框架 PNSIS，通过统一必要性和充分性概率，提取不变的子结构，并利用伪现象子图提升推广性能和鲁棒性。 |
| [^48] | [Less is More: Fewer Interpretable Region via Submodular Subset Selection](https://arxiv.org/abs/2402.09164) | 本论文将图像归属问题重新建模为次模子集选择问题，通过使用更少的区域来增强模型的解释性，解决了现有归属解决方案面临的不准确区域和预测错误样本的问题。 |
| [^49] | [Attacking Large Language Models with Projected Gradient Descent](https://arxiv.org/abs/2402.09154) | 本研究通过使用投影梯度下降方法，以连续松弛的输入提示来攻击大型语言模型，取得了比离散优化更快的速度，实现了相同的毁灭性攻击效果。 |
| [^50] | [Improved Regret for Bandit Convex Optimization with Delayed Feedback](https://arxiv.org/abs/2402.09152) | 本文针对具有延迟反馈的强化学习问题提出了一种改进后悔率的算法，通过精确利用延迟的强化学习反馈，成功将后悔界限从$O(T^{3/4}+d^{1/3}T^{2/3})$改进为$O(T^{3/4}+\sqrt{dT})$，并在更大的延迟量$d=O(\sqrt{T})$情况下与非延迟设置下的强化学习梯度下降算法相匹配。 |
| [^51] | [Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis](https://arxiv.org/abs/2402.09151) | 本文介绍了一种领域自适应预训练模型Chinese MentalBERT，该模型针对中国社交媒体上心理健康文本分析进行了优化，在预训练过程中加入心理学词典，提高了模型的适用性。 |
| [^52] | [ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks](https://arxiv.org/abs/2402.09146) | 本文介绍了一种增强量子卷积神经网络性能的新框架ResQuNNs，在quanvolutional层中引入可训练性，通过残差学习的概念解决了跨层梯度访问的问题。 |
| [^53] | [When Representations Align: Universality in Representation Learning Dynamics](https://arxiv.org/abs/2402.09142) | 本研究提出了一个有效的表示学习理论，该理论假设了编码映射和解码映射为任意光滑函数，并且能够描述复杂且大型架构中的表示学习动力学。 |
| [^54] | [Unconventional Computing based on Four Wave Mixing in Highly Nonlinear Waveguides](https://arxiv.org/abs/2402.09135) | 本论文通过研究基于高非线性波导中的四波混频效应的光学非常规加速器，提出了一个可以直接在光学领域内进行非线性信号处理的完全模拟系统。通过利用丰富的Kerr诱导非线性效应，可以生成和用于解决复杂的非线性任务的多个非线性变换，并在光通信场景中提供优于强大的机器学习算法的全光非线性补偿结果，同时降低功耗和计算复杂性。 |
| [^55] | [Exploring the Adversarial Capabilities of Large Language Models](https://arxiv.org/abs/2402.09132) | 本研究探索了大型语言模型的对抗能力，并发现其能够成功地制造对抗性示例以愚弄安全措施，特别是在仇恨言论检测方面具有重大影响。 |
| [^56] | [MPIrigen: MPI Code Generation through Domain-Specific Language Models](https://arxiv.org/abs/2402.09126) | 本文研究了使用领域特定语言模型生成MPI代码的性能，并提出了使用预训练模型MonoCoder进行MPI-based程序生成的方法。 |
| [^57] | [Mixed-Output Gaussian Process Latent Variable Models](https://arxiv.org/abs/2402.09122) | 本文提出了一种基于高斯过程潜变量模型的贝叶斯非参数方法，可以用于信号分离，并且能够处理包含纯组分信号加权和的情况，适用于光谱学和其他领域的多种应用。 |
| [^58] | [Measuring Exploration in Reinforcement Learning via Optimal Transport in Policy Space](https://arxiv.org/abs/2402.09113) | 本论文提出了一种通过比较强化学习和监督学习算法在数据分布空间中路径长度来量化探索的方法，并在各种环境和多种算法上进行了实证分析。 |
| [^59] | [Stochastic Spiking Attention: Accelerating Attention with Stochastic Computing in Spiking Networks](https://arxiv.org/abs/2402.09109) | 本文提出了一种利用随机计算加速尖峰网络中的关注机制的新框架，证明该方法在CIFAR-10上能够达到高分类准确率，同时大大降低了计算能量和内存访问成本。 |
| [^60] | [Scheduling for On-Board Federated Learning with Satellite Clusters](https://arxiv.org/abs/2402.09105) | 本文提出了一种用于卫星集群上的机载联邦学习调度方案，利用可预测的可见性模式来解决间歇性连接问题，实现高效的数据管理和机器学习模型协同训练。 |
| [^61] | [FedSiKD: Clients Similarity and Knowledge Distillation: Addressing Non-i.i.d. and Constraints in Federated Learning](https://arxiv.org/abs/2402.09095) | FedSiKD是一种结合了知识蒸馏的相似性联邦学习框架，旨在解决非独立同分布和联邦学习中的约束问题，通过促进簇内同质性来提高优化效率和加速学习过程。 |
| [^62] | [Three Decades of Activations: A Comprehensive Survey of 400 Activation Functions for Neural Networks](https://arxiv.org/abs/2402.09092) | 这篇论文是关于对400个神经网络激活函数进行全面调查，为研究人员提供一个详尽的概述和分类的方法，以便更好地选择适当的激活函数。 |
| [^63] | [Sobolev Training for Operator Learning](https://arxiv.org/abs/2402.09084) | 本研究通过将导数信息融入损失函数来改善算子学习框架的性能，并提出了一种在不规则网格上近似导数的新框架，从而表明Sobolev训练在近似解算子方面的有效性。 |
| [^64] | [Detection Latencies of Anomaly Detectors: An Overlooked Perspective ?](https://arxiv.org/abs/2402.09082) | 本文提出了检测延迟的重要性，并提出了一种评估方法，以确保准确和及时的检测之间的权衡。研究通过将误报率与攻击和错误的时间延迟相关联，并提供了配置检测器的指导原则。 |
| [^65] | [Low-Rank Extragradient Methods for Scalable Semidefinite Optimization](https://arxiv.org/abs/2402.09081) | 本文研究了低秩外推梯度方法在可扩展半定规划问题上的应用，通过使用低秩奇异值分解来投影到半正定锥，取得了收敛于约束优化问题解的理论结果。 |
| [^66] | [Exploiting Estimation Bias in Deep Double Q-Learning for Actor-Critic Methods](https://arxiv.org/abs/2402.09078) | 本文提出了两种创新方法，ExpD3 和 BE-TD3，用于解决和利用 Actor-Critic 方法中的估计偏差问题。实验证明这些算法在连续控制任务中比现有方法更高效。 |
| [^67] | [DisGNet: A Distance Graph Neural Network for Forward Kinematics Learning of Gough-Stewart Platform](https://arxiv.org/abs/2402.09077) | 本文提出了一种名为DisGNet的距离图神经网络，用于解决Gough-Stewart平台的前向运动学问题。通过使用k-FWL算法进行消息传递和引入适用于GPU的Newton-Raphson方法，DisGNet可以实现超高精度的输出姿势，同时满足实时计算的要求。 |
| [^68] | [Steady-State Error Compensation for Reinforcement Learning with Quadratic Rewards](https://arxiv.org/abs/2402.09075) | 该论文研究提出了一种使用积分项补偿二次奖励函数稳态误差的方法，通过增强长期奖励的考虑，有效降低了系统性能（如自适应巡航控制和变道模型）中的稳态误差问题。 |
| [^69] | [Solid Waste Detection in Remote Sensing Images: A Survey](https://arxiv.org/abs/2402.09066) | 本文调查了固体废物在遥感图像中的检测方法。研究者利用地球观测卫星提供的高分辨率数据，通过遥感图像实现了固体废物处置场地的识别、监测和评估。 |
| [^70] | [Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space](https://arxiv.org/abs/2402.09063) | 该论文提出了一种新的嵌入空间攻击方法，针对开源LLMs进行攻击，绕过模型对齐并在遗忘的情况下提取信息，比传统的离散攻击更高效。 |
| [^71] | [I can't see it but I can Fine-tune it: On Encrypted Fine-tuning of Transformers using Fully Homomorphic Encryption](https://arxiv.org/abs/2402.09059) | 本文介绍了一种使用全同态加密进行隐私保护微调的系统BlindTuner，它可以在图像分类任务中实现对Transformer模型的隐私保护训练，并在准确性上与非加密模型相当。 |
| [^72] | [Distributed Sensing Along Fibres for Smart Clothing](https://arxiv.org/abs/2402.09057) | 该论文介绍了一种分布式纤维传感技术，用于智能服装中的运动跟踪和生物信号监测。通过使用螺旋辅助弹性纱线传感器和特定算法，能够在服装中高效地实现多区域的局部应变测量。 |
| [^73] | [Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?](https://arxiv.org/abs/2402.09056) | 本论文提出了关于证据深度学习的新理论洞见, 高亮了在优化二阶损失函数和解释得出的认识不确定性度量上的困难性 |
| [^74] | [End-to-End Training Induces Information Bottleneck through Layer-Role Differentiation: A Comparative Analysis with Layer-wise Training](https://arxiv.org/abs/2402.09050) | 本文通过与逐层训练的比较，重新考虑了为什么端到端训练表现出优越的性能，分析了信息传播方面的优势，并对训练模型的性质差异进行了深入理解。 |
| [^75] | [Inference of Abstraction for a Unified Account of Reasoning and Learning](https://arxiv.org/abs/2402.09046) | 本文提出了一个以贝叶斯方法为基础的概率推断理论，用于推理和学习的统一理论。该理论通过将数据转化为符号知识，从而实现推理过程。我们对逻辑推理关系和MNIST数据集进行了讨论和实验证明。 |
| [^76] | [Under manipulations, are some AI models harder to audit?](https://arxiv.org/abs/2402.09043) | 本论文研究在操纵下，是否有些人工智能模型更难以审计，在模型可以适应任何数据的情况下，无论是主动还是非主动的审计策略都无法超越随机抽样。研究发现审计的可操纵性与目标模型的容量有关。 |
| [^77] | [Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST) Activation Under Data Constraints](https://arxiv.org/abs/2402.09034) | 该论文提出了一种平方Sigmoid TanH（SST）激活函数，用于增强在数据限制下的顺序模型学习能力。通过数学平方放大强激活和弱激活之间的差异，改善梯度流和信息过滤。在多个应用中评估了SST驱动的LSTM和GRU模型的性能。 |
| [^78] | [SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks](https://arxiv.org/abs/2402.09025) | SLEB是一种通过消除冗余的Transformer块来优化LLM流程的新方法，它成功加速了LLM的推理过程。 |
| [^79] | [Neural Operators Meet Energy-based Theory: Operator Learning for Hamiltonian and Dissipative PDEs](https://arxiv.org/abs/2402.09018) | 本文提出了Energy-consistent Neural Operators (ENOs)框架，用于学习遵循能量守恒或耗散定律的PDE解算子。通过引入受物理学能量理论启发的惩罚函数，并通过另一个DNN模型建模能量函数，可以确保DNN解算子的输出满足能量一致性，而无需显式的PDEs。实验证明ENO在多个物理系统上的表现优于以往方法。 |
| [^80] | [Gradient Alignment with Prototype Feature for Fully Test-time Adaptation](https://arxiv.org/abs/2402.09004) | 在测试时间自适应中，我们提出了一种名为GAP的正则化方法，通过梯度对齐和原型特征，减轻了来自于错误分类伪标签熵最小化损失的不适当引导，显著改善了TTA方法。 |
| [^81] | [Exploring Federated Deep Learning for Standardising Naming Conventions in Radiotherapy Data](https://arxiv.org/abs/2402.08999) | 本文介绍了一种利用联合深度学习在放射治疗数据中标准化命名约定的方法，通过集成去中心化实时数据和联邦学习来实现。这是一个自动化和高效的方法，可以解决多机构中心的数据标准化问题。 |
| [^82] | [Nearly Minimax Optimal Regret for Learning Linear Mixture Stochastic Shortest Path](https://arxiv.org/abs/2402.08998) | 本文研究了具有线性混合转移核函数的随机最短路径问题，并提出了一种无需限制性假设的新算法。该算法基于带有方差感知置信区间的扩展值迭代，并通过高阶矩的递归估计实现了近似最小最大遗憾界。 |
| [^83] | [Variance Reduction and Low Sample Complexity in Stochastic Optimization via Proximal Point Method](https://arxiv.org/abs/2402.08992) | 本文提出了一种通过近端点方法进行随机优化的方法，能够在弱条件下获得低样本复杂度，并实现方差减少的目标。 |
| [^84] | [Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption](https://arxiv.org/abs/2402.08991) | 本研究通过引入对抗性健壮的乐观MLE（CR-OMLE）算法，解决了模型驱动强化学习中对抗性破坏的挑战，实现了对转移模型的健壮估计。 |
| [^85] | [MEL: Efficient Multi-Task Evolutionary Learning for High-Dimensional Feature Selection](https://arxiv.org/abs/2402.08982) | MEL是一种高效的多任务进化学习方法，通过多任务学习中的信息共享来增强特征选择的能力和效率，解决了高维特征选择中的挑战。 |
| [^86] | [Learning-enabled Flexible Job-shop Scheduling for Scalable Smart Manufacturing](https://arxiv.org/abs/2402.08979) | 本研究提出了一种基于图的深度强化学习方法，名为异构图调度器（HGS），用于解决灵活作业车间调度问题。该方法利用局部关系知识进行调度，并采用图结构的决策框架，提高了规模泛化能力。 |
| [^87] | [Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks](https://arxiv.org/abs/2402.08978) | Prismatic是一个集成历史数据分析和业务关系知识的交互式多视角概念股集群分析系统，通过多视角集群分析方法，丰富了数据驱动的集群，并提供了细致的业务相关性理解。 |
| [^88] | [Research and application of Transformer based anomaly detection model: A literature review](https://arxiv.org/abs/2402.08975) | 本文综述了基于Transformer的异常检测模型在自然语言处理领域的研究与应用，概述了Transformer及其变种在异常检测任务中的工作原理和应用场景，并提出了该领域的未来研究趋势。 |
| [^89] | [Predicting User Experience on Laptops from Hardware Specifications](https://arxiv.org/abs/2402.08964) | 本文研究了如何从笔记本电脑的硬件规格预测实际生活中的用户体验，通过对54台笔记本电脑上的9个用户体验指标进行研究，针对Chromebook上的Web应用进行聚合。通过这项研究，可以更准确地评估设备的整体用户体验。 |
| [^90] | [DUEL: Duplicate Elimination on Active Memory for Self-Supervised Class-Imbalanced Learning](https://arxiv.org/abs/2402.08963) | DUEL是一个用于解决类别不平衡问题的自监督学习框架，通过主动内存去重策略来增强数据的多样性，有效缓解了过拟合的问题。英文总结目前暂无法给出。 |
| [^91] | [Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers](https://arxiv.org/abs/2402.08958) | 本文提出了一种新颖的后训练量化算法，名为aespa，它在保持完整的注意力得分的同时，通过逐层量化来提高效率，解决了当前后训练量化方案的瓶颈问题。 |
| [^92] | [MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data](https://arxiv.org/abs/2402.08957) | 这项工作介绍了MUSTARD，一种掌握定理和证明数据统一合成的数据生成框架，通过三个阶段的合成，实现了高质量和多样化的问题和推理步骤的生成。 |
| [^93] | [Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input](https://arxiv.org/abs/2402.08948) | 本文研究了使用随机梯度下降和双层神经网络学习子空间稀疏多项式的均场流动。我们提出了合并阶梯属性的无基础推广，并建立了SGD可学习性的必要条件。此外，我们证明了稍强的条件可以保证损失函数的指数衰减至零。 |
| [^94] | [Measuring Sharpness in Grokking](https://arxiv.org/abs/2402.08946) | 本研究介绍了一种基于适当函数形式的技术来测量理解现象，并研究了在不同设置下训练和验证精度变化的锐度。在两个设置中，相对理解差距和理解锐度之间的趋势相似。 |
| [^95] | [Evaluating DTW Measures via a Synthesis Framework for Time-Series Data](https://arxiv.org/abs/2402.08943) | 本论文提出了一个合成框架，用于评估时间序列数据的DTW度量。这个框架可以模拟比较两个时间序列数据序列之间的变化，有助于解释DTW度量在不同类型的时间序列数据上表现良好的原因。 |
| [^96] | [Second Order Methods for Bandit Optimization and Control](https://arxiv.org/abs/2402.08929) | 本文提出了一种简单实用的二阶赌徒凸优化算法，并证明了其对于一类称之为$\kappa$-凸的凸函数实现了最优的后期损失界限。该算法在多个应用中表现出高效性能，包括赌徒逻辑回归。 |
| [^97] | [MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences](https://arxiv.org/abs/2402.08925) | 这项工作提出了一种公平对齐大型语言模型与多样的人类偏好的方法，通过学习混合偏好分布并使用MaxMin对齐目标来更好地表示人类偏好。 |
| [^98] | [IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture](https://arxiv.org/abs/2402.08923) | 本文提出了一种基于数据驱动的方法，用于使用Transformer架构进行人体姿态估计的最佳IMU放置。研究结果表明，该方法在姿态重建准确性方面优于传统的双向RNN模型，并且在使用只有6个IMU时，Transformer架构将24个IMU位置获取的数据与双向RNN具有相当的性能。这一优化选择的IMU放置策略结合了Transformer的并行性和性能优势，对基于IMU的姿态估计领域带来了显著的改进。 |
| [^99] | [The Mirrored Influence Hypothesis: Efficient Data Influence Estimation by Harnessing Forward Passes](https://arxiv.org/abs/2402.08922) | 本文介绍和探讨了镜像影响假设，突出了训练和测试数据之间影响的相互性。具体而言，它指出，评估训练数据对测试预测的影响可以重新表述为一个等效但相反的问题：评估如果模型在特定的测试样本上进行训练，对训练样本的预测将如何改变。通过实证和理论验证，我们演示了这一假设的正确性。 |
| [^100] | [Interpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding](https://arxiv.org/abs/2402.08919) | 这篇论文提出了一种可解释的限制复杂性描绘自动编码方法，用于衡量图像之间的概念相似度，以便解决基于图像的机器学习中的版权问题。 |
| [^101] | [Graph Inference Acceleration by Learning MLPs on Graphs without Supervision](https://arxiv.org/abs/2402.08918) | 该论文提出了一个简单而有效的框架SimMLP，通过在图上无监督学习MLPs，提高了在延迟敏感的应用中的泛化能力。 |
| [^102] | [Learning-based Bone Quality Classification Method for Spinal Metastasis](https://arxiv.org/abs/2402.08910) | 本文提出了一种基于CT图像的学习型自动脊柱转移骨质量分类方法，并利用多任务学习技术同时考虑横纵脊柱受累分类任务，以提高性能。 |
| [^103] | [Tackling Negative Transfer on Graphs](https://arxiv.org/abs/2402.08907) | 图迁移学习中的负迁移现象尚未得到充分研究，本文发现在图结构数据中负迁移普遍存在，即使源图和目标图在语义上相似。我们提出了一个新的观点，对于语义相似的图，结构差异对子图嵌入的影响较小。 |
| [^104] | [Auto-Encoding Bayesian Inverse Games](https://arxiv.org/abs/2402.08902) | 本文研究了多个智能体在共同环境中的交互问题，并提出了一种贝叶斯方法来推断游戏参数。采用变分自动编码器（VAE）来构建游戏参数的后验分布，从而解决了现有方法中无法对不确定性进行定量化的问题。 |
| [^105] | [Weakly Supervised Segmentation of Vertebral Bodies with Iterative Slice-propagation](https://arxiv.org/abs/2402.08892) | 本文提出了一种利用弱监督方法进行椎体分割的迭代切片传播方法，仅需四个角标记点作为标签，实现了从CT图像中自动识别椎体的体积分割。 |
| [^106] | [Predicting the Emergence of Solar Active Regions Using Machine Learning](https://arxiv.org/abs/2402.08890) | 该论文通过使用机器学习模型，利用声功率密度演化特征以及多普勒频移和连续强度观测数据预测太阳活动区出现的连续强度变化。 |
| [^107] | [Moving Object Proposals with Deep Learned Optical Flow for Video Object Segmentation](https://arxiv.org/abs/2402.08882) | 本研究提出了一种使用深度学习的光流进行视频物体分割的最新方法，并通过微调预训练的光流模型和使用完全卷积神经网络实现了准确和高效的移动物体提案生成。 |
| [^108] | [Inference for an Algorithmic Fairness-Accuracy Frontier](https://arxiv.org/abs/2402.08879) | 本文提供了一个算法公平性和准确性推理的方法。我们提出了一种一致的估计器，并进行了一些检验假设的推理。同时，我们还给出了一个估计器来计算一个给定算法与前沿上最公平点之间的距离，并描述了它的渐近性质。 |
| [^109] | [Position Paper: Challenges and Opportunities in Topological Deep Learning](https://arxiv.org/abs/2402.08871) | 拓扑深度学习将拓扑特征引入深度学习模型，可作为图表示学习和几何深度学习的补充，给各种机器学习环境提供了自然选择。本文讨论了拓扑深度学习中的开放问题，并提出了未来的研究机会。 |
| [^110] | [DeepPolar: Inventing Nonlinear Large-Kernel Polar Codes via Deep Learning](https://arxiv.org/abs/2402.08864) | 本文提出了一种通过深度学习发明的非线性大核心极化码，称为DeepPolar码。与现有的神经码和传统的极化码相比，DeepPolar码通过有效利用更大的核心尺寸提高了可靠性。 |
| [^111] | [Approximation of relation functions and attention mechanisms](https://arxiv.org/abs/2402.08856) | 研究了多层感知机内积的近似性质，揭示了它们作为通用逼近器的能力。得到了对称和非对称关系函数逼近所需神经元数量的界限。 |
| [^112] | [Hybrid Inverse Reinforcement Learning](https://arxiv.org/abs/2402.08848) | 本文提出使用混合强化学习的方法来减少逆强化学习中的不必要探索，通过在在线数据和专家数据的混合上进行训练，从而提高学习效率。 |
| [^113] | [Space-Time Bridge-Diffusion](https://arxiv.org/abs/2402.08847) | 介绍了一种利用时空混合策略生成独立同分布合成样本的方法，并通过线性和非线性随机过程实现最佳转运，进一步细化通过分数匹配技术训练方法 |
| [^114] | [Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation](https://arxiv.org/abs/2402.08845) | 这篇论文提出了一种使用双阶段扰动测试来进行特征归因的方法，通过计算扰动一个特征对预测变化的必要性和充分性作用的概率来衡量特征重要性。该方法能够增强特征归因方法在区分不同特征贡献方面的能力。 |
| [^115] | [Intelligent Agricultural Management Considering N$_2$O Emission and Climate Variability with Uncertainties](https://arxiv.org/abs/2402.08832) | 本研究利用人工智能和机器学习技术，特别是强化学习和递归神经网络，以及概率性建模方法，开发了一种智能农业管理系统，可以提高作物产量，优化肥料和水的使用，同时减少氮肥流失和温室气体排放，特别关注氧化亚氮（N2O）排放。 |
| [^116] | [Disambiguated Node Classification with Graph Neural Networks](https://arxiv.org/abs/2402.08824) | 这篇论文研究了图神经网络中歧义问题对表示学习的影响，并提出了一种新的方法{\method}来消除节点嵌入中的歧义。 |
| [^117] | [RanDumb: A Simple Approach that Questions the Efficacy of Continual Representation Learning](https://arxiv.org/abs/2402.08823) | RanDumb是一种简单的方法，通过固定的随机变换嵌入原始像素并学习简单的线性分类器，质疑了持续表示学习的效果。 实验结果显示，RanDumb在众多持续学习基准测试中明显优于使用深度网络进行持续学习的表示学习。 |
| [^118] | [Corridor Geometry in Gradient-Based Optimization](https://arxiv.org/abs/2402.08818) | 本文研究了基于梯度优化中的走廊几何，发现走廊可以提供有关梯度下降优化的洞见，并提出了一种适用于梯度下降的学习率自适应策略CLR，该策略与凸优化中的Polyak步长特例一致。 |
| [^119] | [Model approximation in MDPs with unbounded per-step cost](https://arxiv.org/abs/2402.08813) | 本文考虑了在MDP中具有无界每步成本的模型逼近问题，并通过限定模型变换和距离来提供上界。 |
| [^120] | [Deep and shallow data science for multi-scale optical neuroscience](https://arxiv.org/abs/2402.08811) | 这项研究讨论了多尺度光学神经科学中深度和浅层数据科学的应用。由于不同尺度的数据差异和共性，需要针对每个尺度开发专门的算法，并结合最新的机器学习技术和图信号处理方法来优化数据处理流程。 |
| [^121] | [Depth Separation in Norm-Bounded Infinite-Width Neural Networks](https://arxiv.org/abs/2402.08808) | 本研究探讨了在无限宽度神经网络中的深度分隔问题，并发现在特定条件下，用深度为3的ReLU网络学习比用深度为2的ReLU网络学习要更高效。 |
| [^122] | [Projection-Free Online Convex Optimization with Time-Varying Constraints](https://arxiv.org/abs/2402.08799) | 这个论文介绍了在线凸优化中的投影-free算法，使用线性优化预测访问固定可行集，并满足时变约束。算法在序列上实现了$\tilde{O}(T^{3/4})$的遗憾和$O(T^{7/8})$的约束违反。 |
| [^123] | [Improving Molecule Generation and Drug Discovery with a Knowledge-enhanced Generative Model](https://arxiv.org/abs/2402.08790) | 本文介绍了一种名为K-DReAM的知识增强生成模型框架，该框架将知识图谱嵌入到生成模型中，以产生具有特定特征的新药候选物。 |
| [^124] | [Leveraging cough sounds to optimize chest x-ray usage in low-resource settings](https://arxiv.org/abs/2402.08789) | 通过分析咳嗽声音，我们开发了三种模型，可以在胸部X射线中预测异常结果，从而优化资源使用并提高医疗效率。 |
| [^125] | [Rethinking Machine Unlearning for Large Language Models](https://arxiv.org/abs/2402.08787) | 这篇论文研究了大型语言模型中的机器消除技术，旨在消除不良数据的影响并保持基本知识生成的完整性，为开发安全、可靠和资源高效的生成式人工智能提供基础。 |
| [^126] | [Preconditioners for the Stochastic Training of Implicit Neural Representations](https://arxiv.org/abs/2402.08784) | 本论文提出了一种新的随机训练方法，通过使用曲率感知对角预处理器，在不损失准确性的情况下加速了隐式神经表示的训练过程，适用于多个信号模态。 |
| [^127] | [FLASH: Federated Learning Across Simultaneous Heterogeneities](https://arxiv.org/abs/2402.08769) | FLASH是一个跨同时异质性的联邦学习方法，通过综合考虑数据质量、数据分布和延迟等因素，优于其他现有的方法。 |
| [^128] | [Adversarially Robust Feature Learning for Breast Cancer Diagnosis](https://arxiv.org/abs/2402.08768) | 提出了一种乳腺癌诊断中的对抗性鲁棒特征学习方法(ARFL)，通过对抗性训练和特征相关性测量来学习鲁棒特征，提供更准确和安全的乳腺癌诊断。 |
| [^129] | [Bayesian Strategic Classification](https://arxiv.org/abs/2402.08758) | 本研究从学习者部分信息公开的角度研究了战略分类，不再假设代理者完全了解分类器的参数，而是考虑代理者对学习者所使用的分类器有一个共同的分布先验。 |
| [^130] | [Forecasting for Swap Regret for All Downstream Agents](https://arxiv.org/abs/2402.08753) | 本研究提出了一种可以在保证缩减换位后悔的同时显著提高速度的预测算法，通过进行非校准预测，但在精心选择的事件集合下保持无偏的预测，能够适用于任何下游代理。 |
| [^131] | [Nearest Neighbor Representations of Neural Circuits](https://arxiv.org/abs/2402.08751) | 该论文介绍了一种受大脑结构启发的新的计算方法——最近邻表示法，通过建立与神经网络的对应关系和明确的构造，可以表示深度为2的阈值电路等不同函数。 |
| [^132] | [Automated detection of motion artifacts in brain MR images using deep learning and explainable artificial intelligence](https://arxiv.org/abs/2402.08749) | 本研究使用深度学习模型和可解释人工智能，实现了对脑MR图像中刚性运动的自动检测。通过对公开数据集的测试，模型展示了良好的性能，能够识别运动伪影并提供解释。该模型的结果在实验数据集上具有高的准确率和召回率，并与图像质量指标呈现强烈的负相关性。 |
| [^133] | [Nearest Neighbor Representations of Neurons](https://arxiv.org/abs/2402.08748) | 这个论文研究了最近邻（NN）表示用于表示神经元的复杂性，并证明了著名的阈值函数可以用多项式规模的锚点和对数分辨率实现。 |
| [^134] | [ADS: Approximate Densest Subgraph for Novel Image Discovery](https://arxiv.org/abs/2402.08743) | 本文提出了一种快速且无需训练的算法，用于从大量图像中发现具有独特特征的图像。通过将图像集合形式化为感知距离加权图，并定位最独特图像子集的最密子图，我们解决了一个NP难问题，并通过随机梯度下降高效求解。在合成和真实数据集上的实验证明了算法的优越性能。 |
| [^135] | [Unveiling Hidden Energy Anomalies: Harnessing Deep Learning to Optimize Energy Management in Sports Facilities](https://arxiv.org/abs/2402.08742) | 利用深度学习和阈值估计技术，提出了一种在体育设施中检测能源异常的方法，旨在优化能源管理并提高运营效率。 |
| [^136] | [Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs](https://arxiv.org/abs/2402.08733) | 通过预测对偶的方法，我们提出了一种教导模型逼近真实条件分布并估计模型的不确定性的通用策略。 |
| [^137] | [Trained quantum neural networks are Gaussian processes](https://arxiv.org/abs/2402.08726) | 本文研究了训练的量子神经网络，证明了当每个测量的比特仅与少数其他测量的比特相关时，未训练的网络生成的函数的概率分布收敛于高斯过程；通过分析梯度下降训练网络，证明了训练后的网络能够完美拟合训练集，并且训练后生成的函数的概率分布仍然收敛于高斯过程；同时考虑了统计噪声的影响。 |
| [^138] | [PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models](https://arxiv.org/abs/2402.08714) | 本研究提出了PRDP方法，通过近端奖励差异预测实现了稳定的黑盒奖励微调扩散模型，能够在大规模提示数据集上进行训练，并且具有更好的训练稳定性。 |
| [^139] | [BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation](https://arxiv.org/abs/2402.08712) | BECoTTA是一种基于输入的高效CTTA框架，通过采用MoDE（Mixture-of-Domain Low-rank Experts）模型，它包含领域自适应路由和领域专家协同损失两个核心组件，能够在持续的测试时间中自适应不断变化的领域，同时只需较少的可训练参数。 |
| [^140] | [Correction to "Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations"](https://arxiv.org/abs/2402.08711) | 修正了《对于数值逼近遍历SDE的分布的Wasserstein距离估计》中的错误局部误差估计，提出了一种方法来分析数值离散遍历SDE的Wasserstein-2距离的非渐近保证，并解决了实践中维度依赖性的问题。 |
| [^141] | [Zero Shot Molecular Generation via Similarity Kernels](https://arxiv.org/abs/2402.08708) | 通过研究基于能量的扩散模型分数行为，提出了一种新的零-shot分子生成方法SiMGen，该方法使用基于相似性的分子生成技术，能够构建大分子。 |
| [^142] | [A Survey of Generative AI for De Novo Drug Design: New Frontiers in Molecule and Protein Generation](https://arxiv.org/abs/2402.08703) | 这项综述提出了一个广义方法来驱动AI药物设计，重点关注小分子和蛋白质生成两个主要主题，介绍了各种子任务和应用，并比较了顶级模型的性能。 |
| [^143] | [Primal-Dual Algorithms with Predictions for Online Bounded Allocation and Ad-Auctions Problems](https://arxiv.org/abs/2402.08701) | 该论文提出了应用机器学习预测的在线有界分配和在线广告拍卖问题的原始-对偶算法，通过构建根据预测质量调整性能的算法，在预测准确时超过了先前性能界限。 |
| [^144] | [Unsupervised Evaluation of Code LLMs with Round-Trip Correctness](https://arxiv.org/abs/2402.08699) | 无需人工策划，我们提出了往返正确性（RTC）作为评估代码大型语言模型（LLMs）的替代方法，RTC可以在更广泛的真实世界软件领域对代码进行评估，并且与现有基准具有强相关性。 |
| [^145] | [AMEND: A Mixture of Experts Framework for Long-tailed Trajectory Prediction](https://arxiv.org/abs/2402.08698) | 本研究提出了一种模块化的模型无关的轨迹预测框架，使用专家混合来解决长尾效应问题，提高对于包含挑战性场景的数据的预测性能。 |
| [^146] | [Game of Trojans: Adaptive Adversaries Against Output-based Trojaned-Model Detectors](https://arxiv.org/abs/2402.08695) | 该论文提出了一种自适应的对抗者，可以重新训练带特洛伊的深度神经网络，并且可以规避最先进的基于输出的特洛伊模型检测器，同时确保高准确率。 |
| [^147] | [Inference Stage Denoising for Undersampled MRI Reconstruction](https://arxiv.org/abs/2402.08692) | 本文提出了一种在稀疏采样MRI重构过程中利用推理阶段降噪的方法，通过采用条件超参数网络，消除了数据增强的需求，并在各种噪声水平下保持稳健性能。实验表明该方法在测试阶段产生高清晰度重构，并在准确性和图像质量方面达到最佳效果。 |
| [^148] | [If Turing played piano with an artificial partner](https://arxiv.org/abs/2402.08690) | 本研究调查了通过训练生成模型来生成音乐乐谱是否可以实现令人信服的社交体验，而无需优化其同步和延续能力。 |
| [^149] | [Context-Aware Automated Passenger Counting Data Denoising](https://arxiv.org/abs/2402.08688) | 本论文提出了一种上下文感知的自动乘客计数数据降噪算法，通过约束整数线性优化结合票务数据和历史乘车人数数据，有效地提高了数据鲁棒性和分析的准确性。 |
| [^150] | [Fuzzy clustering of circular time series based on a new dependence measure with applications to wind data](https://arxiv.org/abs/2402.08687) | 本文提出了一种基于新的依赖测度的模糊圆形时间序列聚类方法，能够有效地将取值在单位圆上的时间序列进行聚类分析，并考虑了序列的动态特性。 |
| [^151] | [Prompted Contextual Vectors for Spear-Phishing Detection](https://arxiv.org/abs/2402.08309) | 通过新的文档向量化方法，我们的方法使用大型语言模型来检测钓鱼网络攻击的电子邮件，并在实验证明具有高效性能。 |
| [^152] | [ChatCell: Facilitating Single-Cell Analysis with Natural Language](https://arxiv.org/abs/2402.08303) | ChatCell是一个利用自然语言促进单细胞分析的工具，通过词汇适应和统一序列生成，它具备深厚的专业知识和适应各种分析任务的能力。 |
| [^153] | [Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective](https://arxiv.org/abs/2402.08228) | 这项研究从架构的角度全面调查了图的超分布推广，揭示了图自我注意机制和其他常见构建模块在超分布问题上的影响。 |
| [^154] | [On the Resurgence of Recurrent Models for Long Sequences: Survey and Research Opportunities in the Transformer Era](https://arxiv.org/abs/2402.08132) | 这项调研总结了在处理长序列数据方面，循环模型的复兴和与Transformer模型相结合的新型神经模型的发展，以及深度空间状态模型作为时间函数逼近的方法的出现。 |
| [^155] | [Convergence Analysis of Discrete Diffusion Model: Exact Implementation through Uniformization](https://arxiv.org/abs/2402.08095) | 本文通过均匀化的方式确切实现了离散扩散模型，研究了其理论性质，并提供了关于采样的总变差距离和KL散度保证。这一方法在建模离散数据方面具有重要的应用价值。 |
| [^156] | [Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian probability distributions](https://arxiv.org/abs/2402.08082) | 基于分数的生成模型在学习一个子高斯概率分布族中突破了维数灾难，通过分数匹配生成的分布以维度无关的速率逼近目标分布的总变差。 |
| [^157] | [Towards Unified Alignment Between Agents, Humans, and Environment](https://arxiv.org/abs/2402.07744) | 本文介绍了统一对齐原则 ($\mathbf{UA}^2$)，旨在实现智能体与人类意图、环境动态和自我约束的统一对齐，提出了引入实际特性进行概念验证研究的方法。 |
| [^158] | [Online Sequential Decision-Making with Unknown Delays](https://arxiv.org/abs/2402.07703) | 本文提出了在在线顺序决策中处理未知延迟问题的三个延迟算法族，并提供了相应的遗憾界限。 |
| [^159] | [Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT](https://arxiv.org/abs/2402.07440) | 该论文介绍了LoCoV1，一个用于评估长上下文检索性能的新型基准测试，并提出了M2-BERT检索编码器，用于处理长上下文检索，解决了如何评估性能、预训练语言模型以及如何进行微调的挑战。 |
| [^160] | [Monitored Markov Decision Processes](https://arxiv.org/abs/2402.06819) | 这篇论文介绍了一种新颖且通用的强化学习框架——监控马尔可夫决策过程(Monitored MDPs)。在这个框架中，代理不能始终观察到奖励，提出了算法来解决这个新颖的场景。 |
| [^161] | [Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss](https://arxiv.org/abs/2402.06187) | Premier-TACO是一种多任务特征表示学习方法，通过预训练通用特征表示，并引入负例抽样策略来提高时序行动对比学习的计算效率，从而显著增强了对新颖动作的少样本模仿学习的效果。 |
| [^162] | [Federated Learning Can Find Friends That Are Beneficial](https://arxiv.org/abs/2402.05050) | 本研究介绍了一种新颖的算法，在Federated Learning中使用自适应聚合权重来识别对特定学习目标最有益的客户，证明了该方法的收敛性，并经过实证评估发现，使用该算法引导的合作优于传统方法，这为更加简化和有效的Federated Learning实现奠定了基础。 |
| [^163] | [Neural Networks Learn Statistics of Increasing Complexity](https://arxiv.org/abs/2402.04362) | 本文证明了分布简单性倾向（DSB）的神经网络学习规律，即在训练早期自动学习低阶矩的最大熵分布特征，然后在训练后期失去这种能力。此外，研究还利用最优传输方法进行了低阶统计数据的编辑，证明了早期训练的网络会将编辑的样本视为目标类别的样本。 |
| [^164] | [On dimensionality of feature vectors in MPNNs](https://arxiv.org/abs/2402.03966) | 这篇论文重新考察了消息传递图神经网络（MPNN）特征向量的维度问题，发现实际使用的架构与理论保证存在差距。 |
| [^165] | [MolTC: Towards Molecular Relational Modeling In Language Models](https://arxiv.org/abs/2402.03781) | 本研究提出了一种基于语言模型的多模态框架MolTC，用于分子相互作用预测，该框架能够高效地整合分子对的丰富图形信息，并通过思维链理论实现统一的分子关系学习。 |
| [^166] | [Efficient Numerical Wave Propagation Enhanced by an End-to-End Deep Learning Model](https://arxiv.org/abs/2402.02304) | 本文提出了一个由端到端深度学习模型加强的高效数值波传播方法，通过结合数值求解器和深度学习组件，优化算法架构、数据生成和并行时间算法，实现了在保持速度的同时显著提高性能。 |
| [^167] | [A Bayesian cluster validity index](https://arxiv.org/abs/2402.02162) | 该论文提出了一个基于贝叶斯方法的聚类有效性指数，该指数根据现有的基础指数定义，并用于检测次优聚类数，通过与其他指数进行比较，验证了其有效性。 |
| [^168] | [Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling](https://arxiv.org/abs/2402.00522) | 本研究系统地探讨了Transformer在长序列建模中的近似性质，并研究了其关键组件对表达能力的影响机制。这些发现揭示了关键参数对Transformer的作用，并为替代架构提供了自然建议。 |
| [^169] | [Large Language Models are Null-Shot Learners](https://arxiv.org/abs/2401.08273) | 本文提出了零射击提示方法，通过利用大规模语言模型中的错误信息来指导模型进行任务，以提高任务表现。实验结果表明，在不同数据集上，包括阅读理解、算术推理和闭卷问答，模型性能有所提升。这些结果也显示出不同模型之间存在不同程度的错误信息。 |
| [^170] | [Incentive-Aware Synthetic Control: Accurate Counterfactual Estimation via Incentivized Exploration](https://arxiv.org/abs/2312.16307) | 本论文提出了一种为了解决合成对照方法中"重叠"假设的问题的激励感知合成对照方法。该方法通过激励单位采取通常不会考虑的干预措施，提供与激励相容的干预建议，从而实现在面板数据环境中准确估计反事实效果。 |
| [^171] | [Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks](https://arxiv.org/abs/2312.14440) | 本文对文本到图像生成中的对抗攻击进行了实证研究，发现了攻击成功率的不对称性，并提出了用于探测模型对抗攻击信号的指标。 |
| [^172] | [POND: Multi-Source Time Series Domain Adaptation with Information-Aware Prompt Tuning](https://arxiv.org/abs/2312.12276) | POND是第一个可以利用领域特定信息进行多源时间序列领域适应的方法。 |
| [^173] | [diff History for Neural Language Agents](https://arxiv.org/abs/2312.07540) | 本文介绍了一种名为diff历史的简单且高效的解决方案，用于将环境中的观测转换为文本提示，以便对于长期推理决策的任务中的Neural Language Models进行优化。 |
| [^174] | [Characterization of Locality in Spin States and Forced Moves for Optimizations](https://arxiv.org/abs/2312.02544) | 该论文提出了一种优化算法，利用特定硬件较容易获取的局部性特征，可以高效地脱离局部最小值，从而实现对全局最小值的优化 |
| [^175] | [Learning High-Order Relationships of Brain Regions](https://arxiv.org/abs/2312.02203) | 从fMRI数据中学习脑区的高阶关系对于脑区之间的相互作用的准确表征具有重要意义。我们提出了一种名为HYBRID的新方法，通过识别超边结构和计算超边的权重，提取了最具信息量和最小冗余的高阶关系。 |
| [^176] | [(Ir)rationality in AI: State of the Art, Research Challenges and Open Questions](https://arxiv.org/abs/2311.17165) | 这篇论文调查了人工智能中理性与非理性的概念，提出了未解问题。重点讨论了行为在某些情况下的非理性行为可能是最优的情况。已经提出了一些方法来处理非理性代理，但仍存在挑战和问题。 |
| [^177] | [Attentional Graph Neural Networks for Robust Massive Network Localization](https://arxiv.org/abs/2311.16856) | 本文通过将图神经网络与注意机制相结合，提出了一种用于网络定位的新方法。该方法具有出色的精确度，甚至在严重非直视视线条件下也能表现出良好的效果。通过提出的关注图神经网络模型，我们进一步改善了现有方法的灵活性和对超参数的敏感性。 |
| [^178] | [On the Communication Complexity of Decentralized Bilevel Optimization](https://arxiv.org/abs/2311.11342) | 本研究针对去中心化双层优化的通信复杂度问题，提出了一种新颖的去中心化随机双层梯度下降算法，在异构设置下具有较小的通信成本和轮次，并实现了比现有算法更好的通信复杂度。 |
| [^179] | [Discrete Nonparametric Causal Discovery Under Latent Class Confounding](https://arxiv.org/abs/2311.07454) | 本论文研究了离散非参数隐性类别混淆下的因果发现问题，证明了在有限的潜在类别下，因果发现仍然是可识别的。 |
| [^180] | [In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering](https://arxiv.org/abs/2311.06668) | 通过潜空间操控，使用上下文向量作为替代方法来进行上下文学习，以使语言模型更有效地遵循示例演示，并通过调整向量的量级来轻松控制学习过程。 |
| [^181] | [Contrastive Deep Nonnegative Matrix Factorization for Community Detection](https://arxiv.org/abs/2311.02357) | 对比深度非负矩阵分解算法（CDNMF）通过加深NMF的信息提取能力，采用对比学习的思想构建了网络拓扑和节点属性作为对比视图，并利用去偏方法来优化社区探测的全局结构信息的学习。 |
| [^182] | [Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision](https://arxiv.org/abs/2311.02333) | 本文提出了一种基于字节级精度的编码器-解码器模型，用于理解DNA的自然语言。该模型可以在字节级精度上分析DNA序列，使得能够用于识别DNA序列中的各种功能和变异。 |
| [^183] | [DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization](https://arxiv.org/abs/2310.19668) | 通过最小化休眠比率，本文揭示了当前视觉强化学习方法的一个主要缺点是智能体在早期训练阶段展现出持续的不活动状态，限制了其有效探索能力，并提出了使用休眠比率作为度量指标的方法。 |
| [^184] | [AttributionLab: Faithfulness of Feature Attribution Under Controllable Environments](https://arxiv.org/abs/2310.06514) | 本文提出了AttributionLab，一个可控环境下测试特征归因忠实性的实验室。通过在设计数据上拟合模型并与真实输入特征进行比较，该实验室可以用于分析归因方法的性能，并提出改进建议。 |
| [^185] | [Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic Distance Enhances Open World Object Detection](https://arxiv.org/abs/2306.14291) | Hyp-OW是一种利用超几何距离的层次结构学习增强开放世界目标检测的方法，通过超类正则化器学习和建模已知项目的层次表示，通过基于相似度距离的重新标记模块有效地检测未知对象。 |
| [^186] | [LongForm: Effective Instruction Tuning with Reverse Instructions](https://arxiv.org/abs/2304.08460) | 使用反向指令进行有效的指令调优，通过生成一组自然的、适用于长文本生成的指令调优数据集，我们的模型在故事/菜谱生成和长篇问答等任务上优于10倍规模更大的语言模型，而无需指令调优。 |
| [^187] | [Causal Deep Learning](https://arxiv.org/abs/2303.02186) | 因果深度学习是一种新的关于因果性的思考方式，它综合了可测试的因果知识、参数形式和时间维度，能够帮助我们解决实际问题。 |
| [^188] | [On the Statistical Benefits of Temporal Difference Learning](https://arxiv.org/abs/2301.13289) | 时序差异学习方法通过最小化连续时间步骤中的估计时序不一致度来拟合值函数，具有统计优势，可以显著减少值估计的均方误差，并且可以在两个状态的值差估计中获得显著改进。 |
| [^189] | [Optimal Decision Tree Policies for Markov Decision Processes](https://arxiv.org/abs/2301.13185) | 本研究研究了有限大小决策树在Markov决策过程（MDPs）中的优化，并提出了OMDTs：最优MDP决策树。通过混合整数线性规划直接最大化决策树的期望折扣回报。研究发现现有模仿学习技术的最优性差距，并发现它们表现为次优。 |
| [^190] | [Optimistically Tempered Online Learning](https://arxiv.org/abs/2301.07530) | 本文提出了一种乐观调节的在线学习框架和适应算法，挑战了对专家的信心假设，并通过动态遗憾界限的理论保证和实验证明了该方法的有效性。 |
| [^191] | [A Faster $k$-means++ Algorithm](https://arxiv.org/abs/2211.15118) | 本文提出了一种更快的$k$-means++算法，能够在接近最优的运行时间内解决初始聚类中心选择的问题。 |
| [^192] | [High-Dimensional Undirected Graphical Models for Arbitrary Mixed Data](https://arxiv.org/abs/2211.11700) | 本文提出了在高维非定向图模型中处理任意混合数据的方法，通过在潜变量高斯copula框架中应用经典的多项和多序相关的思想。 |
| [^193] | [Deep Reinforcement Learning Guided Improvement Heuristic for Job Shop Scheduling](https://arxiv.org/abs/2211.10936) | 本文提出了一种基于深度强化学习的指导下的作业车间调度改进启发式方法，采用图表示法来编码完整的解决方案，并设计了基于图神经网络的表示方案和新颖的消息传递机制，以提高性能和加快解决方案评估。 |
| [^194] | [Theoretical Guarantees for Permutation-Equivariant Quantum Neural Networks](https://arxiv.org/abs/2210.09974) | 这篇论文主要研究了置换等变量量子神经网络在量子机器学习中的应用。研究发现，这种架构能够解决传统 QNNs 遇到的局部最小值和贫瘠的高原问题，并能够在少量数据上进行良好的泛化。 |
| [^195] | [FedMT: Federated Learning with Mixed-type Labels](https://arxiv.org/abs/2210.02042) | 本文提出了一种概念新颖的联邦学习设置，即具有混合类型标签的联邦学习，在其中不同的中心可以使用不同的标签准则。为了有效地训练具有混合类型标签的模型，作者提出了一种理论指导和模型无关的方法。 |
| [^196] | [Variance Covariance Regularization Enforces Pairwise Independence in Self-Supervised Representations](https://arxiv.org/abs/2209.14905) | 本研究发现，方差协方差正则化与MLP投影器结合，在学习表示的特征之间实现了成对独立性，并对成对独立性在自监督学习和域外泛化方面的价值进行了验证。 |
| [^197] | [Large-scale unsupervised spatio-temporal semantic analysis of vast regions from satellite images sequences](https://arxiv.org/abs/2208.13504) | 通过无监督深度嵌入和时间序列聚类的结合，本研究提出一种全面理解卫星图像序列的大规模空间-时间语义分析方法，从而解决了缺乏标注数据、地形变异性和图像复杂性等挑战。通过利用底层的空间-时间模式，实现了对感兴趣区域的全面理解。 |
| [^198] | [Dynamic Maintenance of Kernel Density Estimation Data Structure: From Practice to Theory](https://arxiv.org/abs/2208.03915) | 本文研究了具有对抗性查询鲁棒性的动态维护核密度估计数据结构，并提供了一个亚二次空间复杂度和亚线性更新时间的理论框架。 |
| [^199] | [FakeNews: GAN-based generation of realistic 3D volumetric data -- A systematic review and taxonomy](https://arxiv.org/abs/2207.01390) | 这项研究综述了基于GAN的虚假新闻的生成方法和技术，探讨了其在医疗保健领域中帮助解决数据不足问题的潜力。 |
| [^200] | [Enhancing Distributional Stability among Sub-populations](https://arxiv.org/abs/2206.02990) | 本文提出了一种称为“分布稳定性”的概念，旨在提高机器学习算法在分布变化条件下的稳定性。通过量化预测机制子群体之间的稳定性，并基于此提出了学习假设和泛化误差上界。通过稳定风险最小化（SRM）算法，我们增强了模型在预测机制变化时的稳定性。 |
| [^201] | [Compression-aware Training of Neural Networks using Frank-Wolfe](https://arxiv.org/abs/2205.11921) | 本论文提出了一种使用Frank-Wolfe算法进行神经网络压缩感知训练的框架，通过使用多功能的范数约束和SFW算法，实现了在单次密集训练中获得最先进的密集模型，并具有对压缩比鲁棒性和处理卷积滤波器剪枝和低秩矩阵分解的能力。这种方法优于现有的压缩感知方法，并且在低秩矩阵分解的情况下需要更少的计算资源。 |
| [^202] | [Provably Efficient Representation Selection in Low-rank Markov Decision Processes: From Online to Offline RL](https://arxiv.org/abs/2106.11935) | 本论文研究了在低秩马尔可夫决策过程中表示选择对于提高强化学习效率的影响。提出了ReLEX算法，可以在在线和离线强化学习中实现高效表示学习。实验证明，在线版本ReLEX-UCB总是不比没有表示选择的最先进算法差，并在表示函数类具有“覆盖度”性质时，实现了更好的常数遗憾。对于离线版本ReLEX-LCB，可以找到最优策略。 |
| [^203] | [Central Limit Theorem for Two-Timescale Stochastic Approximation with Markovian Noise: Theory and Applications.](http://arxiv.org/abs/2401.09339) | 本文通过对两时间尺度随机逼近（TTSA）的广义分析，利用中心极限定理（CLT）揭示了TTSA受马尔可夫噪声影响的耦合动力学，从而拓展了传统SGD的高效采样策略在分布式学习中的应用范围，同时研究了具有非线性函数逼近的GTD算法的统计特性。 |
| [^204] | [Transportation Market Rate Forecast Using Signature Transform.](http://arxiv.org/abs/2401.04857) | 本论文提出了一种基于特征变换的新型统计方法，用于解决交通市场利率的预测挑战。该方法具有通用的非线性属性和特征变换核函数，能够高效生成特征，并在预测过程中准确识别季节性和制度转换。 |
| [^205] | [SoK: Pitfalls in Evaluating Black-Box Attacks.](http://arxiv.org/abs/2310.17534) | 提出了一个评估黑盒攻击的分类法，揭示了未开发的威胁空间，并展示了在某些设置上已有技术的局限性。 |
| [^206] | [General Identifiability and Achievability for Causal Representation Learning.](http://arxiv.org/abs/2310.15450) | 本文在通用非参数因果潜变量模型和通用转换模型下，通过非耦合干预建立了因果表达学习的可识别性和可实现性结果。在不知道具体干预对应的节点的情况下，这些结果保证了潜在的因果模型和变量的完美恢复，并设计了一个算法来实现这一目标。 |
| [^207] | [The Fundamental Dilemma of Bayesian Active Meta-learning.](http://arxiv.org/abs/2310.14968) | 在贝叶斯主动元学习中，贪婪追求可转移知识可能会损害对可转移参数的估计，学习者面临任务识别和可转移知识获取之间的困境。 |
| [^208] | [Open-Set Multivariate Time-Series Anomaly Detection.](http://arxiv.org/abs/2310.12294) | 本论文提出了一种开放式时间序列异常检测方法，能够在训练阶段识别有限异常类别的少量标记异常，并在测试阶段检测到见过和未见过的异常类别。 |
| [^209] | [MMD-based Variable Importance for Distributional Random Forest.](http://arxiv.org/abs/2310.12115) | 本文介绍了基于MMD距离和经典的drop and relearn原理的变量重要性算法，可以在分布随机森林中检测影响输出分布的变量，并且在实证性能上超越了竞争对手。 |
| [^210] | [DPZero: Dimension-Independent and Differentially Private Zeroth-Order Optimization.](http://arxiv.org/abs/2310.09639) | 该论文提出了DPZero算法，这是一种与维度无关且具有差分隐私的零阶优化算法，用于解决在细调大型语言模型时面临的内存和隐私挑战。 |
| [^211] | [A 4-approximation algorithm for min max correlation clustering.](http://arxiv.org/abs/2310.09196) | 本论文引入了一种新的下界技术，并提出了一个组合算法来求解最小最大相关聚类问题，该算法能够在完全图上达到4的近似结果。通过贪婪联合启发式算法的扩展和实验证明，在多个基准数据集上，该算法在解决方案质量和运行时间上均有显著改进。 |
| [^212] | [Evolutionary Dynamic Optimization and Machine Learning.](http://arxiv.org/abs/2310.08748) | 进化计算和机器学习的结合为优化复杂的机器学习任务提供了有价值的机会，并通过利用进化计算算法生成的数据来提供对搜索空间和种群动态的洞察。 |
| [^213] | [Regret Analysis of Repeated Delegated Choice.](http://arxiv.org/abs/2310.04884) | 本研究针对重复委托选择问题展开研究，通过动态宣布合格集合以学习解决方案分布来减轻代理人的自私行为，进而最小化与最优合格集合之间的后悔。 |
| [^214] | [Learning Quantum Processes with Quantum Statistical Queries.](http://arxiv.org/abs/2310.02075) | 本文提出了第一个在量子统计查询模型内学习量子过程的框架，并提供了一个高效的学习器和可证明的性能保证。通过在密码分析中的应用，揭示了经典读出量子物理不可克隆函数的脆弱性，这是量子硬件安全领域一个重要的开放问题的解决方法。 |
| [^215] | [Intriguing properties of generative classifiers.](http://arxiv.org/abs/2309.16779) | 生成分类器展示了记录破纪录的人类形状偏好、接近人类级别的超出分布准确性、与人类分类错误的最先进对齐以及理解某些知觉幻象的新兴特性，揭示了零样本生成模型出奇地接近人类物体识别数据。 |
| [^216] | [Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces.](http://arxiv.org/abs/2309.16597) | 本文提出了MPHD方法，通过模型预训练和神经网络在异质搜索空间上实现贝叶斯优化的迁移学习。实验证明了MPHD的有效性和在黑盒函数优化任务中的优越性能。 |
| [^217] | [Deep Learning-based Analysis of Basins of Attraction.](http://arxiv.org/abs/2309.15732) | 本研究展示了基于深度学习的卷积神经网络方法在表征各种动力系统的吸引盆的复杂性和不可预测性方面的有效性，相比传统方法，该方法具有更低的计算成本且表现更好。 |
| [^218] | [Reduced Simulations for High-Energy Physics, a Middle Ground for Data-Driven Physics Research.](http://arxiv.org/abs/2309.03780) | 该论文提出了一种简化模拟的方法，通过使用红外探测器模型和粒子碰撞事件模拟器来生成简化数据，以便于高能物理研究和教育中的机器学习模型设计和解决方案探索。 |
| [^219] | [Training Data Protection with Compositional Diffusion Models.](http://arxiv.org/abs/2308.01937) | 使用分区扩散模型（CDM）训练不同的扩散模型，并在推断时任意组合它们，实现了训练数据保护和选择性遗忘，同时还可以根据用户访问权限提供定制模型。 |
| [^220] | [Implicitly Normalized Explicitly Regularized Density Estimation.](http://arxiv.org/abs/2307.13763) | 我们提出了一种新的非参数密度估计方法，基于正则化密度的 Sobolev 范数，通过适当的初始化和使用自然梯度，可以得到性能良好的解，该方法在 Anomaly Detection benchmark 中排名第二。 |
| [^221] | [Metal Oxide-based Gas Sensor Array for the VOCs Analysis in Complex Mixtures using Machine Learning.](http://arxiv.org/abs/2307.06556) | 本研究提出了一种基于金属氧化物传感器阵列和机器学习的方法，可以在复杂混合物中识别出不同的挥发性有机化合物（VOCs），且取得了很高的准确率和回归分析结果。 |
| [^222] | [Self-Supervised Learning with Lie Symmetries for Partial Differential Equations.](http://arxiv.org/abs/2307.05432) | 本研究通过自监督学习的方法，利用李对称将异构数据中的PDEs表示进行优化，提高了不变任务的性能并改进了神经求解器的时间推进性能。 |
| [^223] | [Understanding Pathologies of Deep Heteroskedastic Regression.](http://arxiv.org/abs/2306.16717) | 该论文研究了利用异方差神经回归模型对真实世界数据进行建模时的困难，并从统计物理的角度提供了解释。作者证明了这些不稳定性不仅适用于神经网络结构，而且已经在过参数化条件高斯似然模型的场论中存在。数值求解结果与实证模型拟合的定性一致性证明了相变的存在。 |
| [^224] | [Optimal Differentially Private Learning with Public Data.](http://arxiv.org/abs/2306.15056) | 本论文研究了具有公共数据的最优差分隐私学习，并解决了在训练差分隐私模型时如何利用公共数据提高准确性的问题。 |
| [^225] | [More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime-validity.](http://arxiv.org/abs/2306.12214) | 本文提出了一种新的高概率PAC-Bayes界限，在有界和一般尾部行为的损失中均适用。此外，这些界限还能够保持随时有效性。 |
| [^226] | [$\texttt{causalAssembly}$: Generating Realistic Production Data for Benchmarking Causal Discovery.](http://arxiv.org/abs/2306.10816) | 该论文提出了一种生成基于装配线数据的半合成制造数据集的方法，以支持因果发现方法的基准测试。 |
| [^227] | [Fast and Effective GNN Training with Linearized Random Spanning Trees.](http://arxiv.org/abs/2306.04828) | 本文提出了一种基于线性化随机生成树的GNN训练框架，在多个真实世界的图形基准测试中表现得比其他经典算法更快且更准确。 |
| [^228] | [Optimal transport for automatic alignment of untargeted metabolomic data.](http://arxiv.org/abs/2306.03218) | 本文提出了一种名为GromovMatcher的算法，通过使用最优输运自动合并LC-MS数据集，可提高数据对齐的准确性和鲁棒性，有效解决代谢组学数据合并的挑战。 |
| [^229] | [Improved Stability and Generalization Analysis of the Decentralized SGD Algorithm.](http://arxiv.org/abs/2306.02939) | 本文提出了新的算法稳定性理论来改进分布式SGD算法的泛化性能分析，推翻了现有技术对通信图负面影响的观点，并展示了D-SGD在凸设置中与经典SGD算法泛化界相同。 |
| [^230] | [Evading Black-box Classifiers Without Breaking Eggs.](http://arxiv.org/abs/2306.02895) | 本文提出了一种基于实际代价的黑盒攻击，通过设计新的攻击方式，成功减少了“有害”查询的数量，提高了黑盒攻击效率。 |
| [^231] | [Input gradient diversity for neural network ensembles.](http://arxiv.org/abs/2306.02775) | 本文提出了一阶斥力深度集成 (FoRDE) 算法，它使用输入梯度来增强多样性以提高神经网络集成的表现。 |
| [^232] | [A Uniform Confidence Phenomenon in Deep Learning and its Implications for Calibration.](http://arxiv.org/abs/2306.00740) | 深度神经网络在训练点周围有大的几乎确定的置信邻域，这导致现代模型校准面临重要障碍。 |
| [^233] | [Deep Stochastic Mechanics.](http://arxiv.org/abs/2305.19685) | 本文提出了一种基于深度学习的方法，用于数值模拟时间演化薛定谔方程，利用马尔可夫扩散采样来适应波函数的潜在低维结构，并提出了新的随机量子力学方程，具有线性的计算复杂度。数值模拟显示出显着的优势。 |
| [^234] | [Perturbation-Assisted Sample Synthesis: A Novel Approach for Uncertainty Quantification.](http://arxiv.org/abs/2305.18671) | 本文提出了扰动辅助样本合成（PASS）方法，可从复杂数据中绘制可靠结论，并通过估计数据生成分布和蒙特卡罗实验证明任何统计数据的估计分布。进一步推出扰动辅助推理（PAI）框架，可以提供有效性的统计保证。 |
| [^235] | [Neural Fourier Transform: A General Approach to Equivariant Representation Learning.](http://arxiv.org/abs/2305.18484) | 神经傅里叶变换是一种通用的等变表示学习方法，它可以在不需要显式知识的情况下学习组的潜在线性作用，实现对数据隐藏结构的提取。 |
| [^236] | [The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs).](http://arxiv.org/abs/2305.17033) | 这个论文介绍了CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023挑战，该挑战是首个专注于儿童脑肿瘤的BraTS挑战，旨在评估儿童脑胶质瘤的体积分割算法的发展。儿童中枢神经系统肿瘤是儿童癌症相关死亡的主要原因，并且对这些实体的诊断和治疗存在一些挑战。 |
| [^237] | [Counterfactual Generative Models for Time-Varying Treatments.](http://arxiv.org/abs/2305.15742) | 本文研究了时间变量处理情况下的反事实生成模型，能够捕捉整个反事实分布，并且能够有效推断反事实分布的某些统计量，适用于医疗保健和公共政策制定领域。 |
| [^238] | [Conditional Generative Modeling is All You Need for Marked Temporal Point Processes.](http://arxiv.org/abs/2305.12569) | 本文提出了一种从标记时间点过程中提取其统计直觉的事件生成模型，通过条件生成器以历史观察作为输入，生成可能发生的高质量随后事件。该模型具有高效、灵活和表示能力等方面的优势。 |
| [^239] | [Non-stationary Online Convex Optimization with Arbitrary Delays.](http://arxiv.org/abs/2305.12131) | 本文研究了任意时延的非稳态在线凸优化，提出了一种简单的算法DOGD，并证明它能在最坏情况下获得$O(\sqrt{dT}(P_T+1))$的动态遗憾界，同时当延迟不改变梯度到达顺序时，自动将动态遗憾减少到$O(\sqrt{S}(1+P_T))$。 |
| [^240] | [Inductive CaloFlow.](http://arxiv.org/abs/2305.11934) | iCaloFlow是一个基于归纳流的快速探测器模拟框架，可以以高达以往10-100倍的分辨率进行快速、高保真度模拟。 |
| [^241] | [Transfer operators on graphs: Spectral clustering and beyond.](http://arxiv.org/abs/2305.11766) | 本文介绍了在图上定义的转移算子及其谱特性，提出了基于广义转移算子的有向图聚类算法，并证明了算法有效性。 |
| [^242] | [Bi-level Latent Variable Model for Sample-Efficient Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2304.06011) | 提出了一种新颖的基于模型的MARL算法，BiLL，它学习一个双层潜变量模型，以提高样本效率。该模型在顶层学习全局状态的潜在表示，底层学习每个智能体的潜在表示，并生成潜在轨迹用于策略学习。在SMAC和Flatland环境中，我们的算法在样本效率方面优于现有的无模型和基于模型的基准算法，包括在Super Hard SMAC地图上。 |
| [^243] | [Reinforcement learning for optimization of energy trading strategy.](http://arxiv.org/abs/2303.16266) | 本文使用强化学习算法优化了一种黑盒交易策略，该策略通过在马尔可夫决策过程中使用真实数据进行优化，在 DA 能源市场上由中型生产者自动进行交易。 |
| [^244] | [Boosting Semi-Supervised Few-Shot Object Detection with SoftER Teacher.](http://arxiv.org/abs/2303.05739) | 本文研究了半监督Few-Shot目标检测任务，发现未标记数据对提高半监督FSOD有好处。受此发现启发，我们引入了SoftER Teacher，一种强大的检测器，用于改进FSOD，不需要丰富的标签，并能在性能方面超越强有力的监督检测器，而且不会出现灾难性遗忘。 |
| [^245] | [Reinforcement Learning in Non-Markovian Environments.](http://arxiv.org/abs/2211.01595) | 本文通过递归计算近似充分统计量，提出了一种基于自编码器的代理设计方案，实现了在非马尔可夫环境中进行强化学习。 |

# 详细

[^1]: 关于大规模自监督学习在少样本音频分类中的可迁移性

    On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification

    [https://rss.arxiv.org/abs/2402.01274](https://rss.arxiv.org/abs/2402.01274)

    本研究评估了大规模自监督模型在少样本音频分类中的性能，并发现在一些少样本问题中取得了最先进的性能，同时发现语音为基础的少样本问题与多个下游音频任务之间存在较强的相关性。

    

    近年来，自监督学习因其能够从无标签数据中学习到稳健的特征表示而表现出色。经过自监督预训练的网络可作为下游任务（包括少样本学习）中有效的特征提取器。尽管对于图像的无监督学习方法在少样本学习中的评估已经有了良好的基础，但在声学领域却明显缺失。本研究通过评估大规模自监督模型在少样本音频分类中的性能，弥补了这一空白。此外，我们还探讨了模型的少样本学习能力与其他下游任务基准之间的关系。我们的研究结果表明，在一些少样本问题（如SpeechCommandsv2）中，我们取得了最先进的性能，并且语音为基础的少样本问题与多个下游音频任务之间存在着较强的相关性。

    In recent years, self-supervised learning has excelled for its capacity to learn robust feature representations from unlabelled data. Networks pretrained through self-supervision serve as effective feature extractors for downstream tasks, including Few-Shot Learning. While the evaluation of unsupervised approaches for few-shot learning is well-established in imagery, it is notably absent in acoustics. This study addresses this gap by assessing large-scale self-supervised models' performance in few-shot audio classification. Additionally, we explore the relationship between a model's few-shot learning capability and other downstream task benchmarks. Our findings reveal state-of-the-art performance in some few-shot problems such as SpeechCommandsv2, as well as strong correlations between speech-based few-shot problems and various downstream audio tasks.
    
[^2]: 使用大型语言模型的高效因果图发现

    Efficient Causal Graph Discovery Using Large Language Models

    [https://rss.arxiv.org/abs/2402.01207](https://rss.arxiv.org/abs/2402.01207)

    提出了一个新的框架，利用大型语言模型进行高效的因果图发现，采用了广度优先搜索方法，只需要线性数量的查询，同时能轻松结合观察数据以提高性能，具有高效性和数据效率，并在真实因果图上取得了最先进的结果，展示了其在不同领域的广泛适用性潜力。

    

    我们提出了一个新的框架，利用LLMs进行完整的因果图发现。之前基于LLM的方法采用了成对查询的方法，但这需要二次查询的数量，对于较大的因果图来说很快变得不可行。相反，提出的框架采用了广度优先搜索（BFS）的方法，只需要线性数量的查询。我们还展示了当有所观察数据可用时，提出的方法可以轻松地进行结合以提高性能。除了更具时间和数据效率外，提出的框架在不同大小的真实因果图上取得了最先进的结果。结果证明了提出方法在发现因果关系方面的有效性和效率，展示了其在不同领域的因果图发现任务中的广泛适用性潜力。

    We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.
    
[^3]: 通过集成学习和正则化微调应对偏见

    Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning

    [https://rss.arxiv.org/abs/2402.00910](https://rss.arxiv.org/abs/2402.00910)

    本文提出了一种通过集成学习和正则化微调的方法，解决AI模型中的偏见问题。该方法可通过在小数据集和有偏的预训练模型上训练多个对抗偏见的模型，并使用集成学习得到无偏的预测结果。通过实验证明了该方法在CIFAR10和HAM10000数据集上的有效性。

    

    解决AI模型中的偏见对于确保公平和准确的预测至关重要。然而，获取大规模、无偏的训练数据集可能具有挑战性。本文提出了一种全面的方法，使用多种方法消除AI模型中的偏见，其中仅使用小型数据集和潜在有偏的预训练模型。我们通过数据分离、局部训练和正则化微调训练多个模型，以对抗预训练模型的偏见，得到潜在的对抗偏见模型。然后，我们采用集成学习来对所有模型进行预测，以达到无偏的预测结果。为了进一步加速我们的集成模型的推理时间，我们使用知识蒸馏来获得一个单一无偏的神经网络。通过在CIFAR10和HAM10000数据集上的实验证明了我们方法的有效性，展示了有希望的结果。这项工作为创建更无偏、可靠的AI模型的持续努力做出了贡献。

    Addressing biases in AI models is crucial for ensuring fair and accurate predictions. However, obtaining large, unbiased datasets for training can be challenging. This paper proposes a comprehensive approach using multiple methods to remove bias in AI models, with only a small dataset and a potentially biased pretrained model. We train multiple models with the counter-bias of the pre-trained model through data splitting, local training, and regularized fine-tuning, gaining potentially counter-biased models. Then, we employ ensemble learning for all models to reach unbiased predictions. To further accelerate the inference time of our ensemble model, we conclude our solution with knowledge distillation that results in a single unbiased neural network. We demonstrate the effectiveness of our approach through experiments on the CIFAR10 and HAM10000 datasets, showcasing promising results. This work contributes to the ongoing effort to create more unbiased and reliable AI models, even with l
    
[^4]: AQA-Bench：评估LLM顺序推理能力的交互式基准测试

    AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability

    [https://arxiv.org/abs/2402.09404](https://arxiv.org/abs/2402.09404)

    AQA-Bench是一个交互式基准测试，用于评估大型语言模型在算法上下文中的顺序推理能力。研究发现闭源模型表现出更强的顺序推理能力，显著优于开源模型。

    

    该论文介绍了一种新的基准测试AQA-Bench，用于评估大型语言模型（LLMs）在算法上下文中，如深度优先搜索（DFS）等的顺序推理能力。我们评估基准测试的关键特点在于其交互式评估协议-例如，在DFS中，每个节点的可用连接边取决于模型对该节点的遍历，因此需要LLM有效地记住已访问节点并策划后续移动的能力。我们使用三种不同的算法构建了AQA-Bench，分别是二分搜索，深度优先搜索和广度优先搜索，并评估了12种不同的LLMs的顺序推理能力。我们的调查揭示了一些有趣的发现：（1）类似GPT-4和Gemini等闭源模型通常显示出强大的顺序推理能力，明显优于开源LLMs。（2）天真地提供互操作性

    arXiv:2402.09404v1 Announce Type: cross Abstract: This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol -- for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 12 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show strong sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing inter
    
[^5]: 使用主动查询的人类反馈强化学习

    Reinforcement Learning from Human Feedback with Active Queries

    [https://arxiv.org/abs/2402.09401](https://arxiv.org/abs/2402.09401)

    本文提出了一种基于主动查询的强化学习方法，用于解决与人类反馈的对齐问题。通过在强化学习过程中减少人工标注偏好数据的需求，该方法具有较低的代价，并在实验中表现出较好的性能。

    

    将大型语言模型（LLM）与人类偏好进行对齐，在构建现代生成模型中发挥重要作用，这可以通过从人类反馈中进行强化学习来实现。然而，尽管当前的强化学习方法表现出优越性能，但往往需要大量的人工标注偏好数据，而这种数据收集费时费力。本文受到主动学习的成功启发，通过提出查询效率高的强化学习方法来解决这个问题。我们首先将对齐问题形式化为上下文竞争二臂强盗问题，并设计了基于主动查询的近端策略优化（APPO）算法，具有$\tilde{O}(d^2/\Delta)$的遗憾界和$\tilde{O}(d^2/\Delta^2)$的查询复杂度，其中$d$是特征空间的维度，$\Delta$是所有上下文中的次优差距。然后，我们提出了ADPO，这是我们算法的实际版本，基于直接偏好优化（DPO）并将其应用于...

    arXiv:2402.09401v1 Announce Type: cross Abstract: Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF). Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect. In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods. We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$ regret bound and an $\tilde{O}(d^2/\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\Delta$ is the sub-optimality gap over all the contexts. We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to 
    
[^6]: 使用KV缓存压缩合成循环以提高LLM推断的效率

    Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference

    [https://arxiv.org/abs/2402.09398](https://arxiv.org/abs/2402.09398)

    这项研究提出了一个称为LESS的方法，通过集成一个固定尺寸的缓存和基于驱逐的缓存方法，可以在大型语言模型中减小内存占用的问题，同时保持全部标记的可查询能力，并在多种任务上显示出良好的性能。

    

    许多计算因素限制了大型语言模型的广泛部署。本文关注于由键值(KV)缓存引起的内存瓶颈，这是一种计算快捷方式，在解码过程中需要存储先前的KV对。现有的KV缓存方法通过修剪或驱逐相对不重要的KV对的大片区域，显著减少缓存的内存占用，但在需要重新收集大多数前一个标记的任务中，它们的成功有限。为了缓解这个问题，我们提出了LESS，它将一个（几乎免费的）固定尺寸的缓存与基于驱逐的缓存方法简单地集成在一起，以便所有的标记可以在后续的解码步骤中查询。它能够在时间上保留信息，在多种任务上展现出合理性，我们展示了LESS可以帮助减小缓存所有内容的性能差距，有时甚至可以与其相匹配，同时具有高效性。

    arXiv:2402.09398v1 Announce Type: cross Abstract: Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient.
    
[^7]: 使用神经微分方程和强化学习的托卡马克非正常放电避免和轨迹设计

    Active Disruption Avoidance and Trajectory Design for Tokamak Ramp-downs with Neural Differential Equations and Reinforcement Learning

    [https://arxiv.org/abs/2402.09387](https://arxiv.org/abs/2402.09387)

    本研究利用神经微分方程和强化学习方法，开发了一个能够安全降低等离子体电流并避免非正常放电的策略。通过对SPARC主要参考放电的模拟进行训练，成功将该策略转移到高准确度的模拟器上并取得了良好的效果。

    

    托卡马克提供了一条实现聚变能源的有希望的途径，但是等离子体非正常放电会带来重大的经济风险，因此对非正常放电的避免进行了相当多的研究。本研究通过训练一个策略，使其能够在避免与非正常放电相关的多个量的限制的同时安全地降低等离子体电流。策略训练环境是一个混合物理学和机器学习模型，该模型基于SPARC主要参考放电(PRlD)降放过程的模拟进行训练，这是一个即将到来且具有燃烧等离子体的实验性情景，我们将其用作测试平台。为了应对物理不确定性和模型的不准确性，在策略训练期间，模拟环境使用GPU进行大规模并行处理，并采用随机化的物理参数。经过训练的策略随后成功地转移到高准确度的模拟器中，在避免用户指定的破坏性限制的同时，成功地降低等离子体电流。

    arXiv:2402.09387v1 Announce Type: cross Abstract: The tokamak offers a promising path to fusion energy, but plasma disruptions pose a major economic risk, motivating considerable advances in disruption avoidance. This work develops a reinforcement learning approach to this problem by training a policy to safely ramp-down the plasma current while avoiding limits on a number of quantities correlated with disruptions. The policy training environment is a hybrid physics and machine learning model trained on simulations of the SPARC primary reference discharge (PRD) ramp-down, an upcoming burning plasma scenario which we use as a testbed. To address physics uncertainty and model inaccuracies, the simulation environment is massively parallelized on GPU with randomized physics parameters during policy training. The trained policy is then successfully transferred to a higher fidelity simulator where it successfully ramps down the plasma while avoiding user-specified disruptive limits. We also 
    
[^8]: GraSSRep: 基于图的元基因组组装中重复检测的自监督学习

    GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in Metagenomic Assembly

    [https://arxiv.org/abs/2402.09381](https://arxiv.org/abs/2402.09381)

    GraSSRep是一种基于图的自监督学习方法，用于元基因组组装中的重复检测。它利用组装图的结构通过图神经网络将DNA序列分类为重复和非重复类别。通过自监督学习的方式，使用伪标签进行训练。

    

    重复DNA（重复）对于准确高效地进行基因组组装和序列比对提出了重要挑战。对于元基因组数据而言，这一挑战尤为突出，因为基因组动态性，如水平基因转移、基因复制以及基因损失/获得，使得从元基因组群体中准确组装基因组变得复杂。检测重复是克服这些挑战的关键第一步。为了解决这个问题，我们提出了一种名为GraSSRep的新方法，该方法利用组装图的结构，通过图神经网络（GNNs）在自监督学习框架中将DNA序列分类为重复和非重复类别。具体来说，我们将这个问题构建为在元基因组组装图中的节点分类任务。我们依靠一个高精度（但低召回率）的启发式方法以自监督的方式为一小部分节点生成伪标签。然后，我们使用这些伪标签进行训练。

    arXiv:2402.09381v1 Announce Type: new Abstract: Repetitive DNA (repeats) poses significant challenges for accurate and efficient genome assembly and sequence alignment. This is particularly true for metagenomic data, where genome dynamics such as horizontal gene transfer, gene duplication, and gene loss/gain complicate accurate genome assembly from metagenomic communities. Detecting repeats is a crucial first step in overcoming these challenges. To address this issue, we propose GraSSRep, a novel approach that leverages the assembly graph's structure through graph neural networks (GNNs) within a self-supervised learning framework to classify DNA sequences into repetitive and non-repetitive categories. Specifically, we frame this problem as a node classification task within a metagenomic assembly graph. In a self-supervised fashion, we rely on a high-precision (but low-recall) heuristic to generate pseudo-labels for a small proportion of the nodes. We then use those pseudo-labels to tra
    
[^9]: 长期时间序列预测的损失塑造约束

    Loss Shaping Constraints for Long-Term Time Series Forecasting

    [https://arxiv.org/abs/2402.09373](https://arxiv.org/abs/2402.09373)

    该论文提出了一种用于长期时间序列预测的受限学习方法，通过在每个时间步骤上设置损失上限来寻找最佳模型，以解决平均性能优化导致特定时间步骤上误差过大的问题。

    

    许多时间序列预测应用程序需要预测多个步骤。尽管在这个主题上有大量的文献，但经典和最近的基于深度学习的方法主要集中在最小化预测窗口上的性能平均值。我们观察到，这可能导致在预测步骤之间存在不同的错误分布，尤其是对于在常见预测基准上训练的最近的变换器架构。也就是说，平均性能优化可能导致特定时间步骤上的错误过大。在这项工作中，我们提出了一种长期时间序列预测的受限学习方法，旨在找到在平均性能上最好的模型，并且在每个时间步骤上保持用户定义的损失上限。我们称这种方法为损失塑造约束，因为它对每个时间步骤的损失施加约束，并利用最近的对偶性结果展示了...

    arXiv:2402.09373v1 Announce Type: new Abstract: Several applications in time series forecasting require predicting multiple steps ahead. Despite the vast amount of literature in the topic, both classical and recent deep learning based approaches have mostly focused on minimising performance averaged over the predicted window. We observe that this can lead to disparate distributions of errors across forecasting steps, especially for recent transformer architectures trained on popular forecasting benchmarks. That is, optimising performance on average can lead to undesirably large errors at specific time-steps. In this work, we present a Constrained Learning approach for long-term time series forecasting that aims to find the best model in terms of average performance that respects a user-defined upper bound on the loss at each time-step. We call our approach loss shaping constraints because it imposes constraints on the loss at each time step, and leverage recent duality results to show 
    
[^10]: Transformers可以实现长度的泛化，但并不稳健

    Transformers Can Achieve Length Generalization But Not Robustly

    [https://arxiv.org/abs/2402.09371](https://arxiv.org/abs/2402.09371)

    Transformers在特定组合的数据格式和位置编码的情况下可以实现长度的泛化，但仍然存在脆弱性和大量方差。

    

    长度的泛化，即从较短的训练序列推广到较长的测试序列的能力，对于语言模型来说是一个重要的挑战。即使是处理相对简单任务的大规模Transformer也存在这个问题。在本文中，我们使用两个整数相加的任务来测试Transformer的长度泛化能力。我们展示了长度泛化的成功与数据格式和位置编码的类型密切相关。通过正确组合数据格式和位置编码，我们首次展示出标准的Transformer可以推广到输入长度的2.5倍的序列长度。然而，与内分布泛化不同，长度泛化仍然很脆弱，受到随机权重初始化和训练数据顺序等因素的显著影响，导致不同随机种子之间存在较大差异。

    arXiv:2402.09371v1 Announce Type: cross Abstract: Length generalization, defined as the ability to extrapolate from shorter training sequences to longer test ones, is a significant challenge for language models. This issue persists even with large-scale Transformers handling relatively straightforward tasks. In this paper, we test the Transformer's ability of length generalization using the task of addition of two integers. We show that the success of length generalization is intricately linked to the data format and the type of position encoding. Using the right combination of data format and position encodings, we show for the first time that standard Transformers can extrapolate to a sequence length that is 2.5x the input length. Nevertheless, unlike in-distribution generalization, length generalization remains fragile, significantly influenced by factors like random weight initialization and training data order, leading to large variances across different random seeds.
    
[^11]: 伪随机纠错码

    Pseudorandom Error-Correcting Codes

    [https://arxiv.org/abs/2402.09370](https://arxiv.org/abs/2402.09370)

    我们构建了一种伪随机纠错码，它对替换和删除错误具有鲁棒性，并且可以高效解码。我们还使用伪随机码提出了一种对语言模型输出进行水印处理的方案，该方案对裁剪和随机替换、删除具有恒定的鲁棒性。

    

    我们构建了伪随机纠错码（或简称为伪随机码），它们是具有以下特性的纠错码：对于任何计算受限的对手来说，任意多个编码词都是伪随机的。通过解码密钥，可以高效地纠正有错误的编码词。我们构建了对替换错误和删除错误具有强鲁棒性的伪随机码，其中伪随机性基于标准密码学假设。具体而言，伪随机性基于LPN问题的$2^{O(\sqrt{n})}$困难程度，或者基于LPN问题和低密度下的插入异或问题的多项式困难程度。

    arXiv:2402.09370v1 Announce Type: cross Abstract: We construct pseudorandom error-correcting codes (or simply pseudorandom codes), which are error-correcting codes with the property that any polynomial number of codewords are pseudorandom to any computationally-bounded adversary. Efficient decoding of corrupted codewords is possible with the help of a decoding key.   We build pseudorandom codes that are robust to substitution and deletion errors, where pseudorandomness rests on standard cryptographic assumptions. Specifically, pseudorandomness is based on either $2^{O(\sqrt{n})}$-hardness of LPN, or polynomial hardness of LPN and the planted XOR problem at low density.   As our primary application of pseudorandom codes, we present an undetectable watermarking scheme for outputs of language models that is robust to cropping and a constant rate of random substitutions and deletions. The watermark is undetectable in the sense that any number of samples of watermarked text are computationa
    
[^12]: HiRE:高召回率的高效LLM推理的近似Top-$k$估计

    HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference

    [https://arxiv.org/abs/2402.09360](https://arxiv.org/abs/2402.09360)

    HiRE是一种用于高效的LLM推理的高召回率的近似Top-k估计方法，通过压缩方案减少了模型参数传输和延迟。

    

    在加速器（GPU/TPU）上使用生成式大型语言模型（LLM）进行自回归解码往往受到内存限制，大部分时间用于将模型参数从高带宽内存（HBM）传输到缓存中。与此同时，最近的研究表明，通过适当训练模型，在前馈（FFN）层中维持质量的同时具有显著的稀疏性/冗余性（其中$k \approx 0.05$），从而提出了减少模型参数传输和延迟的方法。然而，利用这种稀疏性来改善延迟的过程受到数据依赖性的限制，通常需要进行完整的矩阵操作来识别前$k$个行/列，严重限制了潜在的收益。为了解决这些问题，我们引入了HiRE（高召回率的近似Top-k估计）。HiRE包括两个新颖的组件：（i）一种压缩方案以便廉价地估计前$k$个行/列。

    arXiv:2402.09360v1 Announce Type: cross Abstract: Autoregressive decoding with generative Large Language Models (LLMs) on accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent on transferring model parameters from high bandwidth memory (HBM) to cache. On the other hand, recent works show that LLMs can maintain quality with significant sparsity/redundancy in the feedforward (FFN) layers by appropriately training the model to operate on a top-$k$ fraction of rows/columns (where $k \approx 0.05$), there by suggesting a way to reduce the transfer of model parameters, and hence latency. However, exploiting this sparsity for improving latency is hindered by the fact that identifying top rows/columns is data-dependent and is usually performed using full matrix operations, severely limiting potential gains. To address these issues, we introduce HiRE (High Recall Approximate Top-k Estimation). HiRE comprises of two novel components: (i) a compression scheme to cheaply p
    
[^13]: 将ChatGPT集成到安全医院网络中：一个关于改进放射学报告分析的案例研究

    Integrating ChatGPT into Secure Hospital Networks: A Case Study on Improving Radiology Report Analysis

    [https://arxiv.org/abs/2402.09358](https://arxiv.org/abs/2402.09358)

    本研究首次将类似于ChatGPT的基于云的人工智能应用于安全的医院模型中，用于分析放射学报告，通过对比学习方法实现了95%以上的准确率，同时提高了预测可靠性和可解释性，对于医生来说具有重要意义。

    

    本研究展示了将类似于ChatGPT的基于云的人工智能首次应用于安全模型中，用于分析放射学报告，重视患者数据隐私。通过使用独特的句子级知识蒸馏方法，通过对比学习，我们在检测异常方面达到了超过95%的准确率。该模型还准确地标记其预测中的不确定性，增强了对医生的可靠性和可解释性，并带有确定性指示器。这些进展在开发安全高效的医疗人工智能工具方面代表了重大进步，为监管最少的医院内部人工智能应用提供了有希望的未来。

    arXiv:2402.09358v1 Announce Type: new Abstract: This study demonstrates the first in-hospital adaptation of a cloud-based AI, similar to ChatGPT, into a secure model for analyzing radiology reports, prioritizing patient data privacy. By employing a unique sentence-level knowledge distillation method through contrastive learning, we achieve over 95% accuracy in detecting anomalies. The model also accurately flags uncertainties in its predictions, enhancing its reliability and interpretability for physicians with certainty indicators. These advancements represent significant progress in developing secure and efficient AI tools for healthcare, suggesting a promising future for in-hospital AI applications with minimal supervision.
    
[^14]: 通过信息论奖励建模来减轻奖励作弊问题

    Mitigating Reward Hacking via Information-Theoretic Reward Modeling

    [https://arxiv.org/abs/2402.09345](https://arxiv.org/abs/2402.09345)

    本文提出了一种名为InfoRM的奖励建模框架，通过引入变分信息瓶颈目标和模型复杂度调节机制，解决了奖励作弊问题，并利用集成聚类偏差得分（ICDS）来检测奖励过度优化。

    

    尽管强化学习从人类反馈（RLHF）中的成功在与人类价值观的语言模型的对齐方面，奖励作弊问题，也被称为奖励过度优化，仍然是一个关键挑战，主要源于奖励建模的局限性，即奖励模型的泛化能力和偏好数据集的不一致性。在这项工作中，我们从信息论的视角来解决这个问题，并提出了一种可推广和鲁棒的奖励建模框架，称为InfoRM，通过引入变分信息瓶颈目标来过滤出不相关的信息，并开发一种模型复杂度调节机制。值得注意的是，我们进一步发现了过度优化与潜变量空间的异常值之间的相关性，将InfoRM作为检测奖励过度优化的一种有前途的工具。受到这一发现的启发，我们提出了集成聚类偏差得分（ICDS），用于量化过优化问题。

    arXiv:2402.09345v1 Announce Type: cross Abstract: Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset. In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation. Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quant
    
[^15]: rnaglib中基于3D的RNA功能预测工具

    3D-based RNA function prediction tools in rnaglib

    [https://arxiv.org/abs/2402.09330](https://arxiv.org/abs/2402.09330)

    rnaglib是一个基于3D的RNA功能预测工具，可以用于在RNA 3D结构数据集上训练监督和非监督机器学习模型。

    

    理解RNA的复杂结构特征与生物功能之间的联系是进化研究和RNA设计中的一个基本挑战。然而，构建RNA 3D结构的数据集并进行适当的建模选择仍然耗时且缺乏标准化。在本章中，我们描述了在RNA 3D结构数据集上训练监督和非监督机器学习功能预测模型的rnaglib的使用方法。

    arXiv:2402.09330v1 Announce Type: cross Abstract: Understanding the connection between complex structural features of RNA and biological function is a fundamental challenge in evolutionary studies and in RNA design. However, building datasets of RNA 3D structures and making appropriate modeling choices remains time-consuming and lacks standardization. In this chapter, we describe the use of rnaglib, to train supervised and unsupervised machine learning-based function prediction models on datasets of RNA 3D structures.
    
[^16]: 将算法公平性与机器学习在官方统计和调查生产中的质量维度联系起来

    Connecting Algorithmic Fairness to Quality Dimensions in Machine Learning in Official Statistics and Survey Production

    [https://arxiv.org/abs/2402.09328](https://arxiv.org/abs/2402.09328)

    这项研究将算法公平性与机器学习在官方统计和调查生产中的质量维度联系起来，扩展了质量框架，并调查了公平性与其他质量维度的互动。

    

    国家统计机构（NSOs）越来越多地利用机器学习（ML）来提高产品的时效性和成本效益性。引入ML解决方案时，NSOs必须确保在统计算法的质量框架（QF4SA; Yung等,2022）中明确规定的健壮性、可重复性和准确性等高标准得到保持。与此同时，越来越多的研究关注公平性作为安全部署ML的前提，以防止实践中不同社会影响的出现。然而，在NSOs应用ML的背景下，公平性尚未明确讨论为质量方面。我们使用Yung等人 (2022)的QF4SA质量框架，并将其质量维度映射到算法公平性。这样，我们在几个方面扩展了QF4SA框架：我们主张公平性作为其独立的质量维度，我们调查了公平性与其他质量维度的互动。

    arXiv:2402.09328v1 Announce Type: cross Abstract: National Statistical Organizations (NSOs) increasingly draw on Machine Learning (ML) to improve the timeliness and cost-effectiveness of their products. When introducing ML solutions, NSOs must ensure that high standards with respect to robustness, reproducibility, and accuracy are upheld as codified, e.g., in the Quality Framework for Statistical Algorithms (QF4SA; Yung et al. 2022). At the same time, a growing body of research focuses on fairness as a pre-condition of a safe deployment of ML to prevent disparate social impacts in practice. However, fairness has not yet been explicitly discussed as a quality aspect in the context of the application of ML at NSOs. We employ Yung et al. (2022)'s QF4SA quality framework and present a mapping of its quality dimensions to algorithmic fairness. We thereby extend the QF4SA framework in several ways: we argue for fairness as its own quality dimension, we investigate the interaction of fairness
    
[^17]: 随机凸优化的信息复杂度：泛化和记忆的应用

    Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization

    [https://arxiv.org/abs/2402.09327](https://arxiv.org/abs/2402.09327)

    本论文研究了随机凸优化中记忆和学习之间的相互作用。通过量化学习算法对训练数据点揭示的信息来定义记忆，并准确定义了学习算法准确性与条件互信息（CMI）之间的权衡关系。在特定条件下，我们证明了学习算法的准确性与CMI之间的最佳边界。通过设计对手，我们进一步展示了记忆在随机凸优化中学习问题中的重要性。

    

    在这项工作中，我们研究了在随机凸优化（SCO）的背景下记忆和学习的相互作用。我们通过学习算法对其训练数据点揭示的信息来定义记忆。然后，我们使用Steinke和Zakynthinou（2020）提出的条件互信息（CMI）框架来量化这些信息。我们的主要结果是对学习算法的准确性和它的CMI之间的权衡的精确描述，回答了Livni（2023）提出的一个未解之问。我们证明，在$L^2$ Lipschitz-有界的设置和强凸性下，每个具有超额错误$\varepsilon$的学习算法的CMI下界分别被$\Omega(1/\varepsilon^2)$和$\Omega(1/\varepsilon)$所限制。我们进一步通过设计一个能够准确识别出大部分训练数据的对手来展示记忆在SCO中学习问题中的重要作用。

    arXiv:2402.09327v1 Announce Type: new Abstract: In this work, we investigate the interplay between memorization and learning in the context of \emph{stochastic convex optimization} (SCO). We define memorization via the information a learning algorithm reveals about its training data points. We then quantify this information using the framework of conditional mutual information (CMI) proposed by Steinke and Zakynthinou (2020). Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering an open question posed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded setting and under strong convexity, every learner with an excess error $\varepsilon$ has CMI bounded below by $\Omega(1/\varepsilon^2)$ and $\Omega(1/\varepsilon)$, respectively. We further demonstrate the essential role of memorization in learning problems in SCO by designing an adversary capable of accurately identifying a significant fraction of the
    
[^18]: 不确定预测中的稳定性和多组公平性在排名中的应用

    Stability and Multigroup Fairness in Ranking with Uncertain Predictions

    [https://arxiv.org/abs/2402.09326](https://arxiv.org/abs/2402.09326)

    本研究考虑了在不确定预测中如何表示稳定性和公平性的问题，并提出了一种新的不确定性感知排名函数。

    

    排名在许多应用中都是普遍存在的，从搜索引擎到招聘委员会。然而，在预测器训练用于分类任务时，当存在内在的不确定性时，不明确如何在派生排名中表示这种不确定性。我们的工作考虑排名函数：从分类任务的个体预测到排名分布的映射。我们关注排名函数的两个方面：对预测扰动的稳定性和对个体和子群体的公平性。稳定性不仅是其本身的重要要求，而且正如我们所显示的，它与Dwork等人（2012）的个体公平性在意义上是和谐的。尽管确定性排名函数除了平凡的情况外不能稳定，但我们显示了最近由Singh等人提出的考虑不确定性的（UA）排名函数

    arXiv:2402.09326v1 Announce Type: new Abstract: Rankings are ubiquitous across many applications, from search engines to hiring committees. In practice, many rankings are derived from the output of predictors. However, when predictors trained for classification tasks have intrinsic uncertainty, it is not obvious how this uncertainty should be represented in the derived rankings. Our work considers ranking functions: maps from individual predictions for a classification task to distributions over rankings. We focus on two aspects of ranking functions: stability to perturbations in predictions and fairness towards both individuals and subgroups. Not only is stability an important requirement for its own sake, but -- as we show -- it composes harmoniously with individual fairness in the sense of Dwork et al. (2012). While deterministic ranking functions cannot be stable aside from trivial scenarios, we show that the recently proposed uncertainty aware (UA) ranking functions of Singh et al
    
[^19]: 只有我的模型在我的数据上：一种保护模型和欺骗未经授权的黑盒模型的隐私保护方法

    Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models

    [https://arxiv.org/abs/2402.09316](https://arxiv.org/abs/2402.09316)

    这项开创性研究提出了一种隐私保护方法，通过生成可被人类感知的图像，在经过授权的模型中保持准确的推理，同时规避其他未经授权的黑盒模型的推理，填补了之前的研究空白。

    

    深度神经网络广泛应用于面部识别和医学图像分类等现实世界任务中，隐私和数据保护至关重要。如果不保护图像数据，可能会被利用来推断个人或环境信息。现有的隐私保护方法，如加密，生成的扰动图像即使对人类来说也无法识别。对抗性攻击方法禁止授权利益相关者进行自动推理，限制了商业和广泛适用的实际激励。这项开创性研究通过生成可被人类感知的图像，在经过授权的模型中保持准确的推理，同时规避其他类似或不同目标的未经授权的黑盒模型，并解决了之前的研究空白。所使用的数据集包括ImageNet（用于图像分类）和Celeba-HQ数据集（用于身份识别）。

    arXiv:2402.09316v1 Announce Type: cross Abstract: Deep neural networks are extensively applied to real-world tasks, such as face recognition and medical image classification, where privacy and data protection are critical. Image data, if not protected, can be exploited to infer personal or contextual information. Existing privacy preservation methods, like encryption, generate perturbed images that are unrecognizable to even humans. Adversarial attack approaches prohibit automated inference even for authorized stakeholders, limiting practical incentives for commercial and widespread adaptation. This pioneering study tackles an unexplored practical privacy preservation use case by generating human-perceivable images that maintain accurate inference by an authorized model while evading other unauthorized black-box models of similar or dissimilar objectives, and addresses the previous research gaps. The datasets employed are ImageNet, for image classification, Celeba-HQ dataset, for ident
    
[^20]: embracing the black box: 朝向基于时间序列数据进行因果发现的基础模型

    Embracing the black box: Heading towards foundation models for causal discovery from time series data

    [https://arxiv.org/abs/2402.09305](https://arxiv.org/abs/2402.09305)

    本文研究了基于时间序列数据进行因果发现的问题，提出了一种称为因果预训练的方法，通过以监督方式学习从多变量时间序列到潜在因果图的映射，实现了在共享动力学的情况下的监督式因果发现。

    

    来自时间序列数据的因果发现涵盖了许多现有的解决方案，包括基于深度学习技术的方法。然而，这些方法通常不支持深度学习中最常见的范式之一：端到端学习。为了弥补这一差距，我们研究了我们所称之为因果预训练的方法。这种方法旨在以监督的方式学习从多变量时间序列到潜在因果图的直接映射。我们的实证结果表明，在训练和测试时间序列样本共享大部分动力学的情况下，监督式因果发现是可能的。更重要的是，我们发现，即使额外的数据不共享相同的动力学，因果预训练的性能也随着数据和模型规模的增加而增加。此外，我们提供了实例，证明了基于因果预训练神经网络的真实世界数据的因果发现在一定范围内是可能的。

    arXiv:2402.09305v1 Announce Type: cross Abstract: Causal discovery from time series data encompasses many existing solutions, including those based on deep learning techniques. However, these methods typically do not endorse one of the most prevalent paradigms in deep learning: End-to-end learning. To address this gap, we explore what we call Causal Pretraining. A methodology that aims to learn a direct mapping from multivariate time series to the underlying causal graphs in a supervised manner. Our empirical findings suggest that causal discovery in a supervised manner is possible, assuming that the training and test time series samples share most of their dynamics. More importantly, we found evidence that the performance of Causal Pretraining can increase with data and model size, even if the additional data do not share the same dynamics. Further, we provide examples where causal discovery for real-world data with causally pretrained neural networks is possible within limits. We arg
    
[^21]: 人类中的即时概括与深度神经网络中的滞后概括——表示分歧的证据？

    Immediate generalisation in humans but a generalisation lag in deep neural networks$\unicode{x2014}$evidence for representational divergence?

    [https://arxiv.org/abs/2402.09303](https://arxiv.org/abs/2402.09303)

    研究对比了人类和深度神经网络在图像分类中的行为差异，发现人类具有即时概括能力，而DNNs存在滞后概括现象，这表明了表示分歧的存在。

    

    近期的研究在图像分类领域中对比了人类与深度神经网络（DNNs）的许多行为比较。通常，比较研究关注的是学习过程的最终结果，通过测量和比较目标类别表示的相似性。然而，这些表示如何形成即其过程——即在获取过程中观察到的行为变化和中间阶段——往往少有直接和实证的比较。在这里，我们报告了对人类观察者和不同经典与最新技术的DNNs中可转移表示是如何被获取的的详细调查。我们开发了一个受限的监督学习环境，该环境中我们对齐了学习相关的参数，如起始点、输入模式、可用输入数据以及提供的反馈。在整个学习过程中我们评估...

    arXiv:2402.09303v1 Announce Type: cross Abstract: Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification. Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed. However, the process of how these representations emerge$\unicode{x2014}$that is, the behavioral changes and intermediate stages observed during the acquisition$\unicode{x2014}$is less often directly and empirically compared.   Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs. We develop a constrained supervised learning environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided. Across the whole learning process we evaluate 
    
[^22]: 未经本人同意的训练：在训练代码的语言模型中检测代码包含

    Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code

    [https://arxiv.org/abs/2402.09299](https://arxiv.org/abs/2402.09299)

    这项研究关注如何在训练代码的语言模型中检测代码包含，以解决使用这些模型进行代码审计时的版权侵权问题。

    

    代码审计通过验证开发的代码是否符合标准、法规和版权保护，确保其不包含来自受保护来源的代码。在软件开发过程中，大型语言模型(LLMs)作为编码助手的出现给代码审计带来了新的挑战。训练这些模型的数据集主要来自公开可用的来源。这引发了知识产权侵权问题，因为开发者的代码已包含在数据集中。因此，使用LLMs开发的代码审计具有挑战性，因为我们无法准确确定开发过程中使用的LLM是否已经在特定的受版权保护的代码上进行了训练，因为我们无法获得这些模型的训练数据集。鉴于训练数据集的保密性，传统的代码克隆检测等方法无法确保版权侵权。

    arXiv:2402.09299v1 Announce Type: cross Abstract: Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers' codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To add
    
[^23]: 通过部分监督强化学习学习后验可观察POMDP中的可解释策略

    Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially Supervised Reinforcement Learning

    [https://arxiv.org/abs/2402.09290](https://arxiv.org/abs/2402.09290)

    该论文提出了部分监督强化学习（PSRL）框架，通过融合监督和非监督学习来生成更可解释的策略，同时利用状态估计器提取出监督语义状态信息，以及捕捉潜在状态信息。

    

    深度强化学习在视频游戏、机器人控制、自主驾驶和药物发现等各个领域取得了显着的成就。但在部分可观察的领域中，常见的方法主要依赖于从高维观察（如图像）进行端到端学习，而没有明确推理真实状态。我们提出了一个替代方向，引入了部分监督强化学习（PSRL）框架。PSRL框架的核心是将监督学习和非监督学习进行融合。该方法利用状态估计器从高维观察中提取出有时在训练时完全观察到的监督语义状态信息。这样可以得到更可解释的策略，将状态预测与控制结合起来。同时，它还捕捉到一个非监督潜在表示。这两者-语义状态和潜在状态然后被融合和应用。

    arXiv:2402.09290v1 Announce Type: cross Abstract: Deep reinforcement learning has demonstrated remarkable achievements across diverse domains such as video games, robotic control, autonomous driving, and drug discovery. Common methodologies in partially-observable domains largely lean on end-to-end learning from high-dimensional observations, such as images, without explicitly reasoning about true state. We suggest an alternative direction, introducing the Partially Supervised Reinforcement Learning (PSRL) framework. At the heart of PSRL is the fusion of both supervised and unsupervised learning. The approach leverages a state estimator to distill supervised semantic state information from high-dimensional observations which are often fully observable at training time. This yields more interpretable policies that compose state predictions with control. In parallel, it captures an unsupervised latent representation. These two-the semantic state and the latent state-are then fused and ut
    
[^24]: EcoVal:一种高效的机器学习数据估值框架

    EcoVal: An Efficient Data Valuation Framework for Machine Learning

    [https://arxiv.org/abs/2402.09288](https://arxiv.org/abs/2402.09288)

    EcoVal是一种高效的机器学习数据估值框架，通过估计每个数据的内在和外在价值，实现了快速实用地估算机器学习模型数据的价值。

    

    在机器学习工作流中量化数据的价值可以在机器学习倡议中做出更具战略意义的决策中起到关键作用。现有的基于Shapley值的机器学习数据估值框架在计算方面非常昂贵，因为需要大量重复训练模型才能获得Shapley值。在本文中，我们介绍了一种高效的数据估值框架EcoVal，以快速实用的方式估算机器学习模型数据的价值。我们不直接处理独立的数据样本，而是确定类似的数据点簇的价值。这个价值进一步在所有成员簇点之间传播。我们展示了可以通过估计每个数据的内在和外在价值来确定整体数据价值。这是通过将模型的性能建模为“生产函数”来实现的，这是一个非常重要的概念。

    arXiv:2402.09288v1 Announce Type: new Abstract: Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall data value can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as a \textit{production function}, a concept which is po
    
[^25]: 营养事实、药物事实和模型事实：将AI伦理应用于枪支暴力研究

    Nutrition Facts, Drug Facts, and Model Facts: Putting AI Ethics into Practice in Gun Violence Research

    [https://arxiv.org/abs/2402.09286](https://arxiv.org/abs/2402.09286)

    通过提供一个模型事实模板，该研究旨在在枪支暴力研究中建立AI的信任和透明度，并减少对易受剥削人群数据的不信任。此模板使一般用户能够评估模型的有效性和偏见。

    

    论文介绍了一个在枪支暴力研究中应用AI伦理的框架，该框架旨在减少对黑人和有色人种等易受剥削的弱势人群数据的不信任。通过提出一个模型事实模板，将准确度和人口统计学分解为标准化和最小复杂值，使一般用户能够评估模型的有效性和偏见，而不需要深入研究技术模型文档。

    arXiv:2402.09286v1 Announce Type: new Abstract: Objective: Firearm injury research necessitates using data from often-exploited vulnerable populations of Black and Brown Americans. In order to minimize distrust, this study provides a framework for establishing AI trust and transparency with the general population. Methods: We propose a Model Facts template that is easily extendable and decomposes accuracy and demographics into standardized and minimally complex values. This framework allows general users to assess the validity and biases of a model without diving into technical model documentation. Examples: We apply the Model Facts template on two previously published models, a violence risk identification model and a suicide risk prediction model. We demonstrate the ease of accessing the appropriate information when the data is structured appropriately. Discussion: The Model Facts template is limited in its current form to human based data and biases. Like nutrition facts, it also wi
    
[^26]: 提升二分类问题的协方差和Hessian矩阵的协同特征分析

    Synergistic eigenanalysis of covariance and Hessian matrices for enhanced binary classification

    [https://arxiv.org/abs/2402.09281](https://arxiv.org/abs/2402.09281)

    本论文提出了一种新颖的方法，通过将训练集上计算的协方差矩阵的特征分析与深度学习模型上计算的Hessian矩阵相结合，实现了二分类任务中的最优类别可分性。该方法通过最大化类间平均距离和最小化类内方差，以及将数据投影到两个矩阵的最相关特征方向的组合空间来实现最优类别可分性。实证验证显示了该方法的有效性。

    

    在分类问题中，协方差和Hessian矩阵分别被单独分析，但是将这些矩阵集成起来可以增强它们在提高分类性能方面的综合能力。我们提出了一种新颖的方法，将训练集上计算的协方差矩阵的特征分析与深度学习模型上计算的Hessian矩阵相结合，以实现二分类任务中的最优类别可分性。我们的方法通过形式化证明证明了它可以最大化类间平均距离并最小化类内方差。通过将数据投影到两个矩阵的最相关特征方向的组合空间中，我们按照线性判别分析（LDA）的标准实现了最优类别可分性。对神经网络和健康数据集的实证验证始终支持我们的理论框架，并且证明了我们的方法的有效性。

    arXiv:2402.09281v1 Announce Type: cross Abstract: Covariance and Hessian matrices have been analyzed separately in the literature for classification problems. However, integrating these matrices has the potential to enhance their combined power in improving classification performance. We present a novel approach that combines the eigenanalysis of a covariance matrix evaluated on a training set with a Hessian matrix evaluated on a deep learning model to achieve optimal class separability in binary classification tasks. Our approach is substantiated by formal proofs that establish its capability to maximize between-class mean distance and minimize within-class variances. By projecting data into the combined space of the most relevant eigendirections from both matrices, we achieve optimal class separability as per the linear discriminant analysis (LDA) criteria. Empirical validation across neural and health datasets consistently supports our theoretical framework and demonstrates that our
    
[^27]: 混合机器学习技术在有害藻类水华管理中的应用

    Hybrid Machine Learning techniques in the management of harmful algal blooms impact

    [https://arxiv.org/abs/2402.09271](https://arxiv.org/abs/2402.09271)

    本论文提出了一种将混合机器学习技术应用于有害藻类水华管理的方法，通过预测生产区域的活动状态来替代常规的毒素浓度检测方法，以解决采样不可能的情况下的风险预测问题。

    

    有害藻类水华是高浓度藻类的突发事件，对人类的食用有潜在的毒性。贝类养殖业可能受到有害藻类水华的影响，因为贝类作为过滤食物的动物，能够在其体内积累高浓度的海洋生物毒素。为了避免人类食用的风险，当检测到毒性时会禁止收割。目前，生产区域的关闭是基于专家知识，并且当条件复杂且无法进行采样时，预测模型的存在将有所帮助。虽然肉类中毒素的浓度是控制贝类生产区域的专家最常用的方法，但很少被自动预测模型用作目标。这在很大程度上是由于建立的采样计划导致数据的不规则性。作为替代方案，提出了生产区域的活动状态作为基于该区域是否存在有害藻类水华的目标变量。

    arXiv:2402.09271v1 Announce Type: new Abstract: Harmful algal blooms (HABs) are episodes of high concentrations of algae that are potentially toxic for human consumption. Mollusc farming can be affected by HABs because, as filter feeders, they can accumulate high concentrations of marine biotoxins in their tissues. To avoid the risk to human consumption, harvesting is prohibited when toxicity is detected. At present, the closure of production areas is based on expert knowledge and the existence of a predictive model would help when conditions are complex and sampling is not possible. Although the concentration of toxin in meat is the method most commonly used by experts in the control of shellfish production areas, it is rarely used as a target by automatic prediction models. This is largely due to the irregularity of the data due to the established sampling programs. As an alternative, the activity status of production areas has been proposed as a target variable based on whether moll
    
[^28]: Transformers，并行计算和对数深度

    Transformers, parallel computation, and logarithmic depth

    [https://arxiv.org/abs/2402.09268](https://arxiv.org/abs/2402.09268)

    transformers的关键区别性质是并行性，它们通过使用固定数量的自注意层实现对基本计算任务的高效解决，而这些任务无法被其他神经序列模型和亚二次变压器逼近高效解决。

    

    我们展示了一个固定数量的自注意层可以有效地模拟和被Massively Parallel Computation的通信轮数所模拟。因此，我们证明了对数深度对于transformers来解决一些其他神经序列模型和亚二次变压器逼近无法高效解决的基本计算任务是足够的。这样，我们将并行性确定为transformers的一个关键区别性质。

    arXiv:2402.09268v1 Announce Type: new Abstract: We show that a constant number of self-attention layers can efficiently simulate, and be simulated by, a constant number of communication rounds of Massively Parallel Computation. As a consequence, we show that logarithmic depth is sufficient for transformers to solve basic computational tasks that cannot be efficiently solved by several other neural sequence models and sub-quadratic transformer approximations. We thus establish parallelism as a key distinguishing property of transformers.
    
[^29]: UR2M: 微控制器上的不确定性和资源感知事件检测

    UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers

    [https://arxiv.org/abs/2402.09264](https://arxiv.org/abs/2402.09264)

    UR2M是一个新颖的不确定性和资源感知的事件检测框架，针对微控制器上的应用，通过评估模型输出的可靠性来解决传统机器学习技术在数据分布变化时产生不准确预测的问题。

    

    传统的机器学习技术在训练和测试阶段的数据分布发生变化时容易产生不准确的预测。这种脆弱性可能导致严重后果，特别是在移动医疗等应用中。不确定性估计有潜力通过评估模型输出的可靠性来缓解这个问题。然而，现有的不确定性估计技术通常需要大量的计算资源和内存，使它们在微控制器（MCU）上的实施变得不切实际。这个限制阻碍了许多重要的设备上可穿戴事件检测（WED）应用的可行性，如心脏病发作检测。在本文中，我们提出了UR2M，一个针对MCU的新颖的不确定性和资源感知事件检测框架。具体地，我们（i）基于证据理论开发了一种不确定性感知的WED，用于准确的事件检测。

    arXiv:2402.09264v1 Announce Type: new Abstract: Traditional machine learning techniques are prone to generating inaccurate predictions when confronted with shifts in the distribution of data between the training and testing phases. This vulnerability can lead to severe consequences, especially in applications such as mobile healthcare. Uncertainty estimation has the potential to mitigate this issue by assessing the reliability of a model's output. However, existing uncertainty estimation techniques often require substantial computational resources and memory, making them impractical for implementation on microcontrollers (MCUs). This limitation hinders the feasibility of many important on-device wearable event detection (WED) applications, such as heart attack detection.   In this paper, we present UR2M, a novel Uncertainty and Resource-aware event detection framework for MCUs. Specifically, we (i) develop an uncertainty-aware WED based on evidential theory for accurate event detection
    
[^30]: 探索关系：变革性自适应激活函数与其他激活函数的比较

    Exploring the Relationship: Transformative Adaptive Activation Functions in Comparison to Other Activation Functions

    [https://arxiv.org/abs/2402.09249](https://arxiv.org/abs/2402.09249)

    变革性自适应激活函数(TAAF)被发现可以广义地推广现有激活函数，并利用了其他激活函数的相似概念，这使得TAAF成为神经网络中有潜力和适应性的重要组成部分。

    

    神经网络是许多任务的最先进方法，而激活函数是实现这种性能的主要构建模块之一。最近，提出了一种新型的变革性自适应激活函数(TAAF)，它允许任何垂直和水平平移和缩放。本研究将TAAF置于其他激活函数的背景下。研究表明，TAAFs广义地推广了50种现有激活函数，并利用了超过70种其他激活函数的类似概念，突出了TAAFs的多功能性。这项全面的探索将TAAFs定位为神经网络中具有潜力和适应性的重要组成部分。

    arXiv:2402.09249v1 Announce Type: new Abstract: Neural networks are the state-of-the-art approach for many tasks and the activation function is one of the main building blocks that allow such performance. Recently, a novel transformative adaptive activation function (TAAF) allowing for any vertical and horizontal translation and scaling was proposed. This work sets the TAAF into the context of other activation functions. It shows that the TAAFs generalize over 50 existing activation functions and utilize similar concepts as over 70 other activation functions, underscoring the versatility of TAAFs. This comprehensive exploration positions TAAFs as a promising and adaptable addition to neural networks.
    
[^31]: 异步私有联邦学习中的动量近似

    Momentum Approximation in Asynchronous Private Federated Learning

    [https://arxiv.org/abs/2402.09247](https://arxiv.org/abs/2402.09247)

    本文提出了动量近似方法，在异步私有联邦学习（FL）中有效结合了动量和异步协议的技术，通过最小化动量更新的偏差来改进模型性能。实证研究证明了动量近似在基准FL数据集上的有效性。

    

    异步协议已被证明能够提高大规模客户端联邦学习（FL）的可扩展性。同时，基于动量的方法可以在同步FL中实现最佳模型质量。然而，在异步FL算法中简单地应用动量会导致收敛速度变慢和模型性能下降。如何有效地结合这两种技术以实现双赢目前尚不清楚。在本文中，我们发现异步性引入了对动量更新的隐含偏差。为了解决这个问题，我们提出了动量近似，通过找到所有历史模型更新的最佳加权平均值来最小化偏差。动量近似与安全聚合和差分隐私是兼容的，并且可以在生产的FL系统中很容易地集成，只需较小的通信和存储成本。我们在基准FL数据集上进行了实证研究，证明了动量近似在性能上的改进效果。

    arXiv:2402.09247v1 Announce Type: new Abstract: Asynchronous protocols have been shown to improve the scalability of federated learning (FL) with a massive number of clients. Meanwhile, momentum-based methods can achieve the best model quality in synchronous FL. However, naively applying momentum in asynchronous FL algorithms leads to slower convergence and degraded model performance. It is still unclear how to effective combinie these two techniques together to achieve a win-win. In this paper, we find that asynchrony introduces implicit bias to momentum updates. In order to address this problem, we propose momentum approximation that minimizes the bias by finding an optimal weighted average of all historical model updates. Momentum approximation is compatible with secure aggregation as well as differential privacy, and can be easily integrated in production FL systems with a minor communication and storage cost. We empirically demonstrate that on benchmark FL datasets, momentum appro
    
[^32]: L3DAS23挑战赛关于音频-视觉扩展现实的综述

    Overview of the L3DAS23 Challenge on Audio-Visual Extended Reality

    [https://arxiv.org/abs/2402.09245](https://arxiv.org/abs/2402.09245)

    L3DAS23挑战赛旨在推进机器学习在3D音频信号处理方面的合作研究，并提供了更新的数据集和基准模型，用于音频-视觉扩展现实应用中的3D语音增强和3D声音事件定位和检测。

    

    L3DAS23信号处理大赛的主要目标是促进和支持机器学习在3D音频信号处理方面的合作研究，特别关注扩展现实应用中的3D语音增强以及3D声音事件定位和检测。作为最新一届比赛的一部分，我们提供了全新的数据集，该数据集与L3DAS21和L3DAS22的数据集具有相同的一般特性，但是使用了多个混响模拟环境中的一阶Ambisonics录音。此外，我们开始探索音频-视觉场景，通过提供不同麦克风位置和方向所感知的这些环境的图像。我们还提出了更新的两个任务的基准模型，可以支持音频-图像对作为输入，并提供了一个支持的API来复制我们的结果。最后，我们呈现了参与者的结果。更多详情请参见

    arXiv:2402.09245v1 Announce Type: cross Abstract: The primary goal of the L3DAS23 Signal Processing Grand Challenge at ICASSP 2023 is to promote and support collaborative research on machine learning for 3D audio signal processing, with a specific emphasis on 3D speech enhancement and 3D Sound Event Localization and Detection in Extended Reality applications. As part of our latest competition, we provide a brand-new dataset, which maintains the same general characteristics of the L3DAS21 and L3DAS22 datasets, but with first-order Ambisonics recordings from multiple reverberant simulated environments. Moreover, we start exploring an audio-visual scenario by providing images of these environments, as perceived by the different microphone positions and orientations. We also propose updated baseline models for both tasks that can now support audio-image couples as input and a supporting API to replicate our results. Finally, we present the results of the participants. Further details about
    
[^33]: Switch EMA: 更好的平坦性和锐度的免费午餐

    Switch EMA: A Free Lunch for Better Flatness and Sharpness

    [https://arxiv.org/abs/2402.09240](https://arxiv.org/abs/2402.09240)

    本研究提出了一种称为Switch EMA（SEMA）的方法，通过简单的修改指数移动平均（EMA）参数，可以帮助深度神经网络（DNN）达到更好的平坦性和锐度的概括最优解。通过在多个任务和数据集上进行实验证明了SEMA的有效性。

    

    指数移动平均（EMA）是一种广泛使用的权重平均正则化方法，在深度神经网络（DNN）优化中学习平坦的最优解，以获得更好的概括能力而不增加额外的成本。尽管可以获得更好的平坦性，但现有的平均方法可能陷入更差的最终性能或需要额外的测试时间计算。本文通过一个简单的修改，即在每个周期后将EMA参数切换回原始模型，揭示了EMA的充分潜力，被称为Switch EMA（SEMA）。从理论和实证方面都证明了SEMA可以帮助DNNs达到更好的平坦性和锐度之间平衡的概括最优解。为了验证SEMA的效果，我们在视觉和语言数据集上进行了辨别性、生成性和回归任务的比较实验，包括图像分类、自监督学习、目标检测和分割、图像生成等。

    arXiv:2402.09240v1 Announce Type: new Abstract: Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization. Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations. This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA). From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness. To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generati
    
[^34]: 使用基于最近邻的硬负样本的鲁棒性训练时间GNN

    Robust Training of Temporal GNNs using Nearest Neighbours based Hard Negatives

    [https://arxiv.org/abs/2402.09239](https://arxiv.org/abs/2402.09239)

    本研究提出了使用基于重要性的负样本抽样训练TGNNS的方法，并通过实证评估证明了其优越性能。

    

    时间图神经网络(TGNNS)在未来链接预测任务中表现出最先进的性能。这些TGNNS的训练是通过均匀随机抽样的无监督损失进行列举的。在训练过程中，对于正例情况，损失是在无信息的负样本上计算的，这引入了冗余和次优的性能。在本文中，我们提出了改进的TGNNS无监督学习，通过使用基于重要性的负样本抽样来替换均匀负样本抽样。我们从理论上对负例采样的动态计算分布进行了理论验证和定义。最后，通过对三个真实世界数据集进行实证评估，我们展示了使用基于提出的负样本抽样的损失训练的TGNNS提供了一致的优越性能。

    arXiv:2402.09239v1 Announce Type: new Abstract: Temporal graph neural networks Tgnn have exhibited state-of-art performance in future-link prediction tasks. Training of these TGNNs is enumerated by uniform random sampling based unsupervised loss. During training, in the context of a positive example, the loss is computed over uninformative negatives, which introduces redundancy and sub-optimal performance. In this paper, we propose modified unsupervised learning of Tgnn, by replacing the uniform negative sampling with importance-based negative sampling. We theoretically motivate and define the dynamically computed distribution for a sampling of negative examples. Finally, using empirical evaluations over three real-world datasets, we show that Tgnn trained using loss based on proposed negative sampling provides consistent superior performance.
    
[^35]: 学习可解释概念：统一因果表示学习与基础模型

    Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models

    [https://arxiv.org/abs/2402.09236](https://arxiv.org/abs/2402.09236)

    本研究将因果表示学习和基础模型相结合，研究了如何从数据中学习人类可解释的概念。实验证明了这一统一方法的实用性。

    

    构建智能机器学习系统有两种广泛的方法。一种方法是构建天生可解释的模型，这是因果表示学习领域的努力方向。另一种方法是构建高性能的基础模型，然后投入努力去理解它们的工作原理。本研究将这两种方法联系起来，研究如何从数据中学习人类可解释的概念。通过结合这两个领域的思想，我们正式定义了概念的概念，并展示了它们可以从多样的数据中被可靠地恢复出来。对于合成数据和大型语言模型的实验证明了我们统一方法的实用性。

    arXiv:2402.09236v1 Announce Type: cross Abstract: To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.
    
[^36]: 使用图卷积神经网络的汽车碰撞安全结构动力学的多层次代理学习

    Multi-Hierarchical Surrogate Learning for Structural Dynamics of Automotive Crashworthiness Using Graph Convolutional Neural Networks

    [https://arxiv.org/abs/2402.09234](https://arxiv.org/abs/2402.09234)

    该论文提出了使用图卷积神经网络的多层次代理学习框架，用于汽车碰撞安全结构动力学研究。该框架能够通过创建一系列适应不同计算环境和准确度要求的代理模型，从而提高碰撞仿真的效率和精确度。

    

    碰撞仿真在提高车辆安全性、设计优化和伤害风险估计方面发挥着重要作用。然而，使用最先进的高保真模型进行这类问题的数值解需要大量的计算工作。传统的数据驱动代理建模方法通过创建低维嵌入来演化动力学，以规避这种计算工作。大多数方法直接在从数值离散化获取的高分辨率数据上操作，这既昂贵又复杂，无法在大范围的空间距离上映射信息流动。此外，使用固定分辨率的方法阻止了代理模型对计算能力环境、不同的可视化分辨率和不同的精确度要求进行自适应。因此，我们提出了一个多层次框架，用于结构化地创建一系列用于卡丁车碰撞安全性的代理模型。

    arXiv:2402.09234v1 Announce Type: new Abstract: Crash simulations play an essential role in improving vehicle safety, design optimization, and injury risk estimation. Unfortunately, numerical solutions of such problems using state-of-the-art high-fidelity models require significant computational effort. Conventional data-driven surrogate modeling approaches create low-dimensional embeddings for evolving the dynamics in order to circumvent this computational effort. Most approaches directly operate on high-resolution data obtained from numerical discretization, which is both costly and complicated for mapping the flow of information over large spatial distances. Furthermore, working with a fixed resolution prevents the adaptation of surrogate models to environments with variable computing capacities, different visualization resolutions, and different accuracy requirements. We thus propose a multi-hierarchical framework for structurally creating a series of surrogate models for a kart fr
    
[^37]: 全行代码自动完成的上下文组合

    Context Composing for Full Line Code Completion

    [https://arxiv.org/abs/2402.09230](https://arxiv.org/abs/2402.09230)

    本论文介绍了用于全行代码自动完成功能的Transformer模型的上下文组合方法，该功能已在PyCharm Pro IDE上得到应用，并在实际的Python用户A/B测试中证明了其有用性。

    

    代码自动完成是最常用的集成开发环境（IDE）功能之一，它影响着软件开发人员的日常工作。现代代码自动完成方法已从基于静态分析的几个贡献者的组合转向涉及神经网络的流水线。这种改变允许在生成自身所花费的相对短时间内提出更长的代码建议。在JetBrains，我们投入了大量的精力来完善代码自动完成工作流程，以便对程序员既有帮助又不分散注意力。我们成功将全行代码自动完成功能推出到PyCharm Pro IDE，并在数百名真实Python用户的A/B测试中证明了其有用性。本文描述了我们在Transformer模型上下文组合方面的方法，该模型是该功能实现的核心。此外，我们还分享了改进该功能的下一步工作，并强调了其重要性。

    arXiv:2402.09230v1 Announce Type: cross Abstract: Code Completion is one of the most used Integrated Development Environment (IDE) features, which affects the everyday life of a software developer. Modern code completion approaches moved from the composition of several static analysis-based contributors to pipelines that involve neural networks. This change allows the proposal of longer code suggestions while maintaining the relatively short time spent on generation itself. At JetBrains, we put a lot of effort into perfecting the code completion workflow so it can be both helpful and non-distracting for a programmer. We managed to ship the Full Line Code Completion feature to PyCharm Pro IDE and proved its usefulness in A/B testing on hundreds of real Python users. The paper describes our approach to context composing for the Transformer model that is a core of the feature's implementation. In addition to that, we share our next steps to improve the feature and emphasize the importance
    
[^38]: 在两次齐次神经网络的小初值和鞍点附近的方向收敛

    Directional Convergence Near Small Initializations and Saddles in Two-Homogeneous Neural Networks

    [https://arxiv.org/abs/2402.09226](https://arxiv.org/abs/2402.09226)

    本文研究了两次齐次神经网络在小初值附近的梯度流动，发现权重会在方向上收敛到神经相关函数的KKT点和某些鞍点附近。

    

    本文研究了两次齐次神经网络在小初值附近的梯度流动力学，其中所有权重都初始化在原点附近。针对平方误差和逻辑损失，论文证明，对于足够小的初始值，梯度流动动态在原点附近花费足够的时间，使得神经网络的权重可以近似地在方向上收敛到神经相关函数的Karush-Kuhn-Tucker（KKT）点，该函数量化了神经网络输出与训练数据集中相应标签之间的关联性。

    arXiv:2402.09226v1 Announce Type: new Abstract: This paper examines gradient flow dynamics of two-homogeneous neural networks for small initializations, where all weights are initialized near the origin. For both square and logistic losses, it is shown that for sufficiently small initializations, the gradient flow dynamics spend sufficient time in the neighborhood of the origin to allow the weights of the neural network to approximately converge in direction to the Karush-Kuhn-Tucker (KKT) points of a neural correlation function that quantifies the correlation between the output of the neural network and corresponding labels in the training data set. For square loss, it has been observed that neural networks undergo saddle-to-saddle dynamics when initialized close to the origin. Motivated by this, this paper also shows a similar directional convergence among weights of small magnitude in the neighborhood of certain saddle points.
    
[^39]: 更好的比KL PAC-Bayes界限

    Better-than-KL PAC-Bayes Bounds

    [https://arxiv.org/abs/2402.09201](https://arxiv.org/abs/2402.09201)

    本文提出了一种更好的比KL PAC-Bayes界限方法来估计序列均值，应用于预测器泛化误差的估计。

    

    让$f(\theta, X_1),$ $ \dots,$ $ f(\theta, X_n)$成为一个随机元素序列，其中$f$是一个固定的标量函数，$X_1, \dots, X_n$是独立的随机变量（数据），而$\theta$是根据一些数据相关的后验分布$P_n$分布的随机参数。本文考虑了证明浓度不等式来估计序列均值的问题。这样一个问题的一个例子是对某些通过随机算法训练的预测器的泛化误差的估计，比如神经网络，其中$f$是一个损失函数。传统上，这个问题是通过PAC-Bayes分析来解决的，在这个分析中，除了后验分布，我们还选择一个能够捕捉到学习问题归纳偏差的先验分布。然后，PAC-Bayes浓度界限中的关键数量是一个能够捕捉到学习问题复杂性的分歧。

    arXiv:2402.09201v1 Announce Type: new Abstract: Let $f(\theta, X_1),$ $ \dots,$ $ f(\theta, X_n)$ be a sequence of random elements, where $f$ is a fixed scalar function, $X_1, \dots, X_n$ are independent random variables (data), and $\theta$ is a random parameter distributed according to some data-dependent posterior distribution $P_n$. In this paper, we consider the problem of proving concentration inequalities to estimate the mean of the sequence. An example of such a problem is the estimation of the generalization error of some predictor trained by a stochastic algorithm, such as a neural network where $f$ is a loss function. Classically, this problem is approached through a PAC-Bayes analysis where, in addition to the posterior, we choose a prior distribution which captures our belief about the inductive bias of the learning problem. Then, the key quantity in PAC-Bayes concentration bounds is a divergence that captures the complexity of the learning problem where the de facto stand
    
[^40]: 十个关键词仍然有用：通过代理引导的高效重采样改进黑盒AI生成文本检测

    Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling

    [https://arxiv.org/abs/2402.09199](https://arxiv.org/abs/2402.09199)

    本文提出了一种利用代理引导的高效重采样方法来改进黑盒AI生成文本检测。通过估计单词生成概率作为伪白盒特征，选择少量代表性词汇进行多次重采样，在包含人类文本和LLM生成文本的数据集上进行了实验，取得了出色的结果。

    

    随着大型语言模型（LLM）的应用日益增多，它们的滥用引发了许多不受欢迎的社会问题，如假新闻、学术不诚实和信息污染。这使得AI生成文本（AIGT）的检测非常重要。在现有方法中，白盒方法在性能和泛化性方面通常优于黑盒方法，但它们需要访问LLM的内部状态，不适用于黑盒设置。本文提出了一种通过多次重采样来估计单词生成概率作为伪白盒特征以帮助改进黑盒AIGT检测的方法。具体而言，我们设计了POGER，一种代理引导的高效重采样方法，在黑盒AIGT检测中选择一个小的代表性词汇子集（例如10个词），进行多次重采样。实验证明，这种方法在包含人类文本和七个LLM生成文本的数据集上表现出色。

    arXiv:2402.09199v1 Announce Type: cross Abstract: With the rapidly increasing application of large language models (LLMs), their abuse has caused many undesirable societal problems such as fake news, academic dishonesty, and information pollution. This makes AI-generated text (AIGT) detection of great importance. Among existing methods, white-box methods are generally superior to black-box methods in terms of performance and generalizability, but they require access to LLMs' internal states and are not applicable to black-box settings. In this paper, we propose to estimate word generation probabilities as pseudo white-box features via multiple re-sampling to help improve AIGT detection under the black-box setting. Specifically, we design POGER, a proxy-guided efficient re-sampling method, which selects a small subset of representative words (e.g., 10 words) for performing multiple re-sampling in black-box AIGT detection. Experiments on datasets containing texts from humans and seven LL
    
[^41]: 在梯度提升决策树中实现局部可解释性：特征贡献

    Implementing local-explainability in Gradient Boosting Trees: Feature Contribution

    [https://arxiv.org/abs/2402.09197](https://arxiv.org/abs/2402.09197)

    本文提出了一个针对GBDT的特征贡献方法，通过计算每个节点的残差来实现局部可解释性。这种方法是GBDT算法的局部可解释性模型，也是一种独特的选择。

    

    Gradient Boosting Decision Trees (GBDT)是基于树集成的强大的加法模型。尽管有多个可解释的人工智能(XAI)模型通过重新解释全局和局部的模型来获取信息，但是GBDT的本质让它成为一个黑盒模型。集成中的每棵树都是一个透明的模型，但最终的结果是这些树的总和，很难澄清。本文提出了一种针对GBDT的特征贡献方法。该方法利用GBDT的架构，通过计算每个节点的残差来计算每个特征的贡献。该算法可以计算给定预测的节点决策序列。通过理论证明和多个实验证明了我们方法的性能，它不仅是GBDT算法的局部可解释性模型，也是反映GBDT的独特选择。

    arXiv:2402.09197v1 Announce Type: new Abstract: Gradient Boost Decision Trees (GBDT) is a powerful additive model based on tree ensembles. Its nature makes GBDT a black-box model even though there are multiple explainable artificial intelligence (XAI) models obtaining information by reinterpreting the model globally and locally. Each tree of the ensemble is a transparent model itself but the final outcome is the result of a sum of these trees and it is not easy to clarify.   In this paper, a feature contribution method for GBDT is developed. The proposed method takes advantage of the GBDT architecture to calculate the contribution of each feature using the residue of each node. This algorithm allows to calculate the sequence of node decisions given a prediction.   Theoretical proofs and multiple experiments have been carried out to demonstrate the performance of our method which is not only a local explicability model for the GBDT algorithm but also a unique option that reflects GBDTs 
    
[^42]: 快速采用，隐藏风险：大型语言模型定制的双重影响

    Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization

    [https://arxiv.org/abs/2402.09179](https://arxiv.org/abs/2402.09179)

    本文介绍了针对不可信定制语言模型的指令后门攻击，通过在定制语言模型中设计带有后门指令的提示，实现攻击者预期的结果。攻击包括三个级别，不需要对后端语言模型进行任何修改。

    

    自然语言生成模型的定制化需求不断增加，导致了像GPT这样的解决方案的开发。这些解决方案通过自然语言提示来促进定制的语言模型的创建，无需编码。然而，第三方定制语言模型的可信度仍然是一个重要的问题。在本文中，我们提出了针对与不可信定制语言模型（例如GPT）集成的应用的首个指令后门攻击。具体来说，这些攻击通过设计带有后门指令的提示，将后门嵌入到定制语言模型的版本中，当输入包含预定义的触发器时，输出攻击者期望的结果。我们的攻击包括三个级别：单词级别、语法级别和语义级别，采用不同类型的触发器，并具有逐步隐蔽性。我们强调，我们的攻击不需要对后端语言模型进行微调或任何修改，严格遵循GPT的开发。

    arXiv:2402.09179v1 Announce Type: cross Abstract: The increasing demand for customized Large Language Models (LLMs) has led to the development of solutions like GPTs. These solutions facilitate tailored LLM creation via natural language prompts without coding. However, the trustworthiness of third-party custom versions of LLMs remains an essential concern. In this paper, we propose the first instruction backdoor attacks against applications integrated with untrusted customized LLMs (e.g., GPTs). Specifically, these attacks embed the backdoor into the custom version of LLMs by designing prompts with backdoor instructions, outputting the attacker's desired result when inputs contain the pre-defined triggers. Our attack includes 3 levels of attacks: word-level, syntax-level, and semantic-level, which adopt different types of triggers with progressive stealthiness. We stress that our attacks do not require fine-tuning or any modification to the backend LLMs, adhering strictly to GPTs devel
    
[^43]: 借助多轮交互利用上下文进行越狱攻击

    Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks

    [https://arxiv.org/abs/2402.09177](https://arxiv.org/abs/2402.09177)

    本研究提出了一种新的攻击形式——上下文交互攻击，通过交互式与大型语言模型（LLMs）进行问答来引出有害信息。实验结果表明该方法的有效性。

    

    大型语言模型（LLMs）容易受到越狱攻击的影响，越狱攻击通过微妙地修改攻击查询来提取有害信息。随着防御机制的进化，越狱攻击直接获取有害信息变得越来越具有挑战性。本研究受到人类间接引出有害信息的实践启发，针对一种新的攻击形式——上下文交互攻击。该方法依赖于LLMs生成过程中的自回归性质。我们认为先前的上下文——攻击查询之前的信息在实现强大的越狱攻击方面起着关键作用。具体而言，我们提出了一种利用初步问答对与LLMs交互的方法。通过这样做，我们引导模型的回答朝着揭示“期望的”有害信息的方向发展。我们在四种不同的LLMs上进行了实验证明了方法的有效性。

    arXiv:2402.09177v1 Announce Type: cross Abstract: Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which aim to extract harmful information by subtly modifying the attack query. As defense mechanisms evolve, directly obtaining harmful information becomes increasingly challenging for Jailbreaking attacks. In this work, inspired by human practices of indirect context to elicit harmful information, we focus on a new attack form called Contextual Interaction Attack. The idea relies on the autoregressive nature of the generation process in LLMs. We contend that the prior context--the information preceding the attack query--plays a pivotal role in enabling potent Jailbreaking attacks. Specifically, we propose an approach that leverages preliminary question-answer pairs to interact with the LLM. By doing so, we guide the responses of the model toward revealing the 'desired' harmful information. We conduct experiments on four different LLMs and demonstrate the efficacy of 
    
[^44]: 基于分布式在线凸优化的近似最优后悔算法

    Nearly Optimal Regret for Decentralized Online Convex Optimization

    [https://arxiv.org/abs/2402.09173](https://arxiv.org/abs/2402.09173)

    本论文研究了分布式在线凸优化，开发了新的算法来分别降低凸函数和强凸函数的后悔边界，并填补了现有下界之间的差距。

    

    我们研究了分布式在线凸优化(D-OCO)，其中一组本地学习器需要使用仅限于本地计算和通信的方法来最小化一系列全局损失函数。以前的研究已经确定了针对凸函数和强凸函数的后悔界限分别为$O(n^{5/4}\rho^{-1/2}\sqrt{T})$和${O}(n^{3/2}\rho^{-1}\log T)$，其中$n$是本地学习器的数量，$\rho<1$是通信矩阵的谱间隙，$T$是时间段。然而，对于凸函数存在着较大的间隙，即凸函数的下界为$\Omega(n\sqrt{T})$，强凸函数的下界为$\Omega(n)$。为了填补这些间隙，本文首先开发了新的D-OCO算法，将凸函数和强凸函数的后悔边界分别降低到$\tilde{O}(n\rho^{-1/4}\sqrt{T})$和$\tilde{O}(n\rho^{-1/2}\log T)$。主要技术是设计一种在线可进取的算法。

    arXiv:2402.09173v1 Announce Type: new Abstract: We investigate decentralized online convex optimization (D-OCO), in which a set of local learners are required to minimize a sequence of global loss functions using only local computations and communications. Previous studies have established $O(n^{5/4}\rho^{-1/2}\sqrt{T})$ and ${O}(n^{3/2}\rho^{-1}\log T)$ regret bounds for convex and strongly convex functions respectively, where $n$ is the number of local learners, $\rho<1$ is the spectral gap of the communication matrix, and $T$ is the time horizon. However, there exist large gaps from the existing lower bounds, i.e., $\Omega(n\sqrt{T})$ for convex functions and $\Omega(n)$ for strongly convex functions. To fill these gaps, in this paper, we first develop novel D-OCO algorithms that can respectively reduce the regret bounds for convex and strongly convex functions to $\tilde{O}(n\rho^{-1/4}\sqrt{T})$ and $\tilde{O}(n\rho^{-1/2}\log T)$. The primary technique is to design an online acce
    
[^45]: 在线聚类的进化限制玻尔兹曼机-Kohonen网络

    Evolving Restricted Boltzmann Machine-Kohonen Network for Online Clustering

    [https://arxiv.org/abs/2402.09167](https://arxiv.org/abs/2402.09167)

    本文提出一种将进化限制玻尔兹曼机和Kohonen网络相结合的在线聚类算法，通过使用ERBM处理流式数据和使用KNet更新聚类中心，有效地克服了聚类算法的常见挑战，提高了聚类性能。

    

    本文提出了一种新颖的在线聚类算法，其中将进化限制玻尔兹曼机（ERBM）与Kohonen网络（ERBM-KNet）相结合。所提出的ERBM-KNet利用ERBM以单次传递模式高效处理流式数据，利用偏差-方差策略进行神经元的生长和修剪，以及基于聚类更新策略进行在线聚类和使用KNet进行聚类中心更新。初始阶段，ERBM在处理未标记的图像数据时演化其体系结构，有效地解开了潜在空间中的数据分布。随后，KNet利用从ERBM提取的特征来预测聚类数量并更新聚类中心。通过克服聚类算法常见的挑战，如聚类数量的先验初始化和聚类准确度低下，所提出的ERBM-KNet能够显著提升性能。

    arXiv:2402.09167v1 Announce Type: new Abstract: A novel online clustering algorithm is presented where an Evolving Restricted Boltzmann Machine (ERBM) is embedded with a Kohonen Network called ERBM-KNet. The proposed ERBM-KNet efficiently handles streaming data in a single-pass mode using the ERBM, employing a bias-variance strategy for neuron growing and pruning, as well as online clustering based on a cluster update strategy for cluster prediction and cluster center update using KNet. Initially, ERBM evolves its architecture while processing unlabeled image data, effectively disentangling the data distribution in the latent space. Subsequently, the KNet utilizes the feature extracted from ERBM to predict the number of clusters and updates the cluster centers. By overcoming the common challenges associated with clustering algorithms, such as prior initialization of the number of clusters and subpar clustering accuracy, the proposed ERBM-KNet offers significant improvements. Extensive 
    
[^46]: 混合离散更新过程的解缠织方法及其在电子支援措施中的应用

    Deinterleaving of Discrete Renewal Process Mixtures with Application to Electronic Support Measures

    [https://arxiv.org/abs/2402.09166](https://arxiv.org/abs/2402.09166)

    本文提出了一种用于解缠织混合离散更新Markov链的新方法，通过最大化惩罚似然分数来恢复真实的符号分组，并在电子支援措施中的应用中展示了较好的性能。

    

    在本文中，我们提出了一种用于混合离散更新Markov链的新的解缠织方法。该方法依赖于对惩罚似然分数的最大化。它利用了有关不同符号序列及其到达时间的所有可用信息。通过理论分析证明，在组成过程上满足一定条件的情况下，最小化该分数能够在大样本限制下恢复出真实的符号分组。然后，通过对合成数据的实验验证了该理论分析。最后，将该方法应用于从RESM（雷达电子支援测量）环境中接收到的不同发射机的脉冲串解缠织，并且我们展示了该方法在模拟战数据集上与最先进的方法相比具有竞争优势。

    arXiv:2402.09166v1 Announce Type: new Abstract: In this paper, we propose a new deinterleaving method for mixtures of discrete renewal Markov chains. This method relies on the maximization of a penalized likelihood score. It exploits all available information about both the sequence of the different symbols and their arrival times. A theoretical analysis is carried out to prove that minimizing this score allows to recover the true partition of symbols in the large sample limit, under mild conditions on the component processes. This theoretical analysis is then validated by experiments on synthetic data. Finally, the method is applied to deinterleave pulse trains received from different emitters in a RESM (Radar Electronic Support Measurements) context and we show that the proposed method competes favorably with state-of-the-art methods on simulated warfare datasets.
    
[^47]: 通过必要性和充分性概率统一不变性和伪现象，用于图形离散分析

    Unifying Invariance and Spuriousity for Graph Out-of-Distribution via Probability of Necessity and Sufficiency

    [https://arxiv.org/abs/2402.09165](https://arxiv.org/abs/2402.09165)

    该论文提出了一种统一框架 PNSIS，通过统一必要性和充分性概率，提取不变的子结构，并利用伪现象子图提升推广性能和鲁棒性。

    

    图形离散分析（Graph Out-of-Distribution，OOD）要求在偏倚数据上训练的模型能够推广到未见的测试数据，并具有大量的实际应用。其中一种主流方法是通过环境增强来对齐原始数据和增强数据，从而提取不变的子图。然而，这些方法可能会导致语义子图的丢失或冗余，进而导致子优化的推广。为了解决这个挑战，我们提出了一种统一框架，利用必要性和充分性概率来提取不变的子结构（PNSIS）。除此之外，该框架还以集成的方式利用伪现象子图来提升推广性能，以增强对噪声数据的鲁棒性。具体而言，我们首先考虑了图形数据的数据生成过程。在温和条件下，我们证明了通过最小化一个

    arXiv:2402.09165v1 Announce Type: new Abstract: Graph Out-of-Distribution (OOD), requiring that models trained on biased data generalize to the unseen test data, has a massive of real-world applications. One of the most mainstream methods is to extract the invariant subgraph by aligning the original and augmented data with the help of environment augmentation. However, these solutions might lead to the loss or redundancy of semantic subgraph and further result in suboptimal generalization. To address this challenge, we propose a unified framework to exploit the Probability of Necessity and Sufficiency to extract the Invariant Substructure (PNSIS). Beyond that, this framework further leverages the spurious subgraph to boost the generalization performance in an ensemble manner to enhance the robustness on the noise data. Specificially, we first consider the data generation process for graph data. Under mild conditions, we show that the invariant subgraph can be extracted by minimizing an
    
[^48]: 简约即是美：通过次模子集选择减少可解释区域

    Less is More: Fewer Interpretable Region via Submodular Subset Selection

    [https://arxiv.org/abs/2402.09164](https://arxiv.org/abs/2402.09164)

    本论文将图像归属问题重新建模为次模子集选择问题，通过使用更少的区域来增强模型的解释性，解决了现有归属解决方案面临的不准确区域和预测错误样本的问题。

    

    图像归属算法旨在确定与模型决策高度相关的重要区域。尽管现有的归属解决方案可以有效地给目标元素分配重要性，但仍面临以下挑战：1）现有的归属方法生成不准确的小区域，从而误导正确归属的方向；2）模型无法为预测错误的样本产生良好的归属结果。为了解决上述挑战，本文将上述图像归属问题重新建模为次模子集选择问题，旨在使用更少的区域增强模型的可解释性。为了解决对局部区域的关注不足，我们构造了一个新的次模函数来发现更准确的精细解释区域。为了增强所有样本的归属效果，我们还对子区域选择施加了四个不同的约束，即置信度，

    arXiv:2402.09164v1 Announce Type: cross Abstract: Image attribution algorithms aim to identify important regions that are highly relevant to model decisions. Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate small regions thus misleading the direction of correct attribution, and 2) the model cannot produce good attribution results for samples with wrong predictions. To address the above challenges, this paper re-models the above image attribution problem as a submodular subset selection problem, aiming to enhance model interpretability using fewer regions. To address the lack of attention to local regions, we construct a novel submodular function to discover more accurate fine-grained interpretation regions. To enhance the attribution effect for all samples, we also impose four different constraints on the selection of sub-regions, i.e., confidence, 
    
[^49]: 用投影梯度下降攻击大型语言模型

    Attacking Large Language Models with Projected Gradient Descent

    [https://arxiv.org/abs/2402.09154](https://arxiv.org/abs/2402.09154)

    本研究通过使用投影梯度下降方法，以连续松弛的输入提示来攻击大型语言模型，取得了比离散优化更快的速度，实现了相同的毁灭性攻击效果。

    

    当前的大型语言模型对特定设计的对抗性提示很容易被破解。虽然使用离散优化制作对抗性提示非常有效，但这种攻击通常需要超过100,000次的语言模型调用。这种高计算成本使得它们不适用于定量分析和对抗性训练。为了解决这个问题，我们重新考虑了对连续松弛的输入提示使用投影梯度下降（PGD）的方法。尽管先前使用普通梯度攻击的尝试基本失败，但我们表明，仔细控制连续松弛引入的误差极大地提升了它们的效力。我们的LLMs的PGD速度比最先进的离散优化快一个数量级，以达到相同的毁灭性攻击结果。

    arXiv:2402.09154v1 Announce Type: new Abstract: Current LLM alignment methods are readily broken through specifically crafted adversarial prompts. While crafting adversarial prompts using discrete optimization is highly effective, such attacks typically use more than 100,000 LLM calls. This high computational cost makes them unsuitable for, e.g., quantitative analyses and adversarial training. To remedy this, we revisit Projected Gradient Descent (PGD) on the continuously relaxed input prompt. Although previous attempts with ordinary gradient-based attacks largely failed, we show that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one order of magnitude faster than state-of-the-art discrete optimization to achieve the same devastating attack results.
    
[^50]: 改进了具有延迟反馈的强化学习问题的后悔率

    Improved Regret for Bandit Convex Optimization with Delayed Feedback

    [https://arxiv.org/abs/2402.09152](https://arxiv.org/abs/2402.09152)

    本文针对具有延迟反馈的强化学习问题提出了一种改进后悔率的算法，通过精确利用延迟的强化学习反馈，成功将后悔界限从$O(T^{3/4}+d^{1/3}T^{2/3})$改进为$O(T^{3/4}+\sqrt{dT})$，并在更大的延迟量$d=O(\sqrt{T})$情况下与非延迟设置下的强化学习梯度下降算法相匹配。

    

    我们研究了具有延迟反馈的强化学习问题，其中只有在任意延迟下才会显示动作的损失值。先前的研究通过简单地将延迟的损失值输入到经典的强化学习梯度下降算法，建立了该问题的后悔界限为$O(T^{3/4}+d^{1/3}T^{2/3})$。在本文中，我们开发了一种新的算法来提高后悔率，通过一个阻塞更新机制精确利用延迟的强化学习反馈。我们的分析首先揭示了所提出的算法可以分离延迟和强化学习反馈对后悔率的联合影响，并将凸函数的后悔界限改进为$O(T^{3/4}+\sqrt{dT})$。与先前的结果相比，在更大的延迟量$d=O(\sqrt{T})$，而不是$d=O(T^{1/4})$的情况下，我们的后悔与非延迟设置下强化学习梯度下降算法的$O(T^{3/4})$后悔相匹配。

    arXiv:2402.09152v1 Announce Type: new Abstract: We investigate bandit convex optimization (BCO) with delayed feedback, where only the loss value of the action is revealed under an arbitrary delay. Previous studies have established a regret bound of $O(T^{3/4}+d^{1/3}T^{2/3})$ for this problem, where $d$ is the maximum delay, by simply feeding delayed loss values to the classical bandit gradient descent (BGD) algorithm. In this paper, we develop a novel algorithm to enhance the regret, which carefully exploits the delayed bandit feedback via a blocking update mechanism. Our analysis first reveals that the proposed algorithm can decouple the joint effect of the delays and bandit feedback on the regret, and improve the regret bound to $O(T^{3/4}+\sqrt{dT})$ for convex functions. Compared with the previous result, our regret matches the $O(T^{3/4})$ regret of BGD in the non-delayed setting for a larger amount of delay, i.e., $d=O(\sqrt{T})$, instead of $d=O(T^{1/4})$. Furthermore, we consi
    
[^51]: Chinese MentalBERT: 在社交媒体上针对中国心理健康文本分析的领域自适应预训练

    Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis

    [https://arxiv.org/abs/2402.09151](https://arxiv.org/abs/2402.09151)

    本文介绍了一种领域自适应预训练模型Chinese MentalBERT，该模型针对中国社交媒体上心理健康文本分析进行了优化，在预训练过程中加入心理学词典，提高了模型的适用性。

    

    受到社交媒体的影响，心理问题在当前环境中普遍存在，并且社交媒体成为个人分享感受的重要出口。这导致每天产生大量数据，其中负面情绪有潜力引发危机。因此需要开发出能够高效分析这些数据的模型。虽然预训练语言模型广泛显示出效果，但针对心理学等特定领域的预训练模型存在明显缺失。为解决这一问题，我们从中国社交媒体平台收集了大量数据，并丰富了公开可用数据集，创建了一个包含336万条文本条目的综合数据库。为提高模型在心理文本分析上的适用性，我们将心理学词典融入预训练的掩码机制。在现有的中文语言模型基础上进行构建。

    arXiv:2402.09151v1 Announce Type: new Abstract: In the current environment, psychological issues are prevalent and widespread, with social media serving as a key outlet for individuals to share their feelings. This results in the generation of vast quantities of data daily, where negative emotions have the potential to precipitate crisis situations. There is a recognized need for models capable of efficient analysis. While pre-trained language models have demonstrated their effectiveness broadly, there's a noticeable gap in pre-trained models tailored for specialized domains like psychology. To address this, we have collected a huge dataset from Chinese social media platforms and enriched it with publicly available datasets to create a comprehensive database encompassing 3.36 million text entries. To enhance the model's applicability to psychological text analysis, we integrated psychological lexicons into the pre-training masking mechanism. Building on an existing Chinese language mod
    
[^52]: ResQuNNs: 实现量子卷积神经网络中深度学习的新框架

    ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks

    [https://arxiv.org/abs/2402.09146](https://arxiv.org/abs/2402.09146)

    本文介绍了一种增强量子卷积神经网络性能的新框架ResQuNNs，在quanvolutional层中引入可训练性，通过残差学习的概念解决了跨层梯度访问的问题。

    

    本文提出了一种增强量子卷积神经网络（QuNNs）性能的新框架，通过引入可训练的quanvolutional层并解决与其相关的关键挑战。传统的quanvolutional层虽然有助于特征提取，但往往是静态的，适应性有限。与最先进的研究不同，我们的研究通过在这些层内部进行训练，显著提高了QuNNs的灵活性和潜力。然而，多个可训练的quanvolutional层的引入给基于梯度的优化带来了复杂性，主要是由于难以在这些层之间访问梯度。为了解决这个问题，我们提出了一种新的架构，Residual Quanvolutional Neural Networks (ResQuNNs)，利用残差学习的概念，在这些层之间添加跳过连接以促进梯度的流动。

    arXiv:2402.09146v1 Announce Type: new Abstract: In this paper, we present a novel framework for enhancing the performance of Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional layers and addressing the critical challenges associated with them. Traditional quanvolutional layers, although beneficial for feature extraction, have largely been static, offering limited adaptability. Unlike state-of-the-art, our research overcomes this limitation by enabling training within these layers, significantly increasing the flexibility and potential of QuNNs. However, the introduction of multiple trainable quanvolutional layers induces complexities in gradient-based optimization, primarily due to the difficulty in accessing gradients across these layers. To resolve this, we propose a novel architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging the concept of residual learning, which facilitates the flow of gradients by adding skip connections between 
    
[^53]: 当表示对齐时：表示学习动力学中的普遍性

    When Representations Align: Universality in Representation Learning Dynamics

    [https://arxiv.org/abs/2402.09142](https://arxiv.org/abs/2402.09142)

    本研究提出了一个有效的表示学习理论，该理论假设了编码映射和解码映射为任意光滑函数，并且能够描述复杂且大型架构中的表示学习动力学。

    

    深度神经网络有许多不同的大小和结构。架构的选择，结合数据集和学习算法，普遍认为影响了学习到的神经表示。然而，最近的研究结果显示，不同的架构学习到的表示具有惊人的定性相似性。在这里，我们在将输入到隐藏表示的编码映射和从表示到输出的解码映射都是任意光滑函数的假设下，推导了表示学习的有效理论。在复杂和大型架构的情况下，隐藏表示没有被参数化强约束，该理论概括了表示学习动力学。我们通过实验证明，这个有效理论描述了具有不同激活函数和架构的深度网络中表示学习动力学的一些方面。

    arXiv:2402.09142v1 Announce Type: new Abstract: Deep neural networks come in many sizes and architectures. The choice of architecture, in conjunction with the dataset and learning algorithm, is commonly understood to affect the learned neural representations. Yet, recent results have shown that different architectures learn representations with striking qualitative similarities. Here we derive an effective theory of representation learning under the assumption that the encoding map from input to hidden representation and the decoding map from representation to output are arbitrary smooth functions. This theory schematizes representation learning dynamics in the regime of complex, large architectures, where hidden representations are not strongly constrained by the parametrization. We show through experiments that the effective theory describes aspects of representation learning dynamics across a range of deep networks with different activation functions and architectures, and exhibits 
    
[^54]: 基于高非线性波导中的四波混频的非常规计算

    Unconventional Computing based on Four Wave Mixing in Highly Nonlinear Waveguides

    [https://arxiv.org/abs/2402.09135](https://arxiv.org/abs/2402.09135)

    本论文通过研究基于高非线性波导中的四波混频效应的光学非常规加速器，提出了一个可以直接在光学领域内进行非线性信号处理的完全模拟系统。通过利用丰富的Kerr诱导非线性效应，可以生成和用于解决复杂的非线性任务的多个非线性变换，并在光通信场景中提供优于强大的机器学习算法的全光非线性补偿结果，同时降低功耗和计算复杂性。

    

    在这项工作中，我们通过数值分析了一种基于高非线性波导中的四波混频效应的光学非常规加速器。该方案可以作为一个完全模拟的系统，直接在光学领域内进行非线性信号处理。通过利用富含的Kerr诱导非线性效应，可以生成并用于解决复杂的非线性任务的输入信号的多个非线性变换。我们首先评估了我们的方案在Santa-Fe混沌时间序列预测中的性能。这种处理器的真正威力在于在光通信场景中提供优于强大的机器学习算法的结果的全光非线性补偿，同时降低功耗和计算复杂性。最后，我们展示了如何使用FWM模块作为可重构的非线性激活模块，能够复现特征等

    arXiv:2402.09135v1 Announce Type: cross Abstract: In this work we numerically analyze a photonic unconventional accelerator based on the four-wave mixing effect in highly nonlinear waveguides. The proposed scheme can act as a fully analogue system for nonlinear signal processing directly in the optical domain. By exploiting the rich Kerr-induced nonlinearities, multiple nonlinear transformations of an input signal can be generated and used for solving complex nonlinear tasks. We first evaluate the performance of our scheme in the Santa-Fe chaotic time-series prediction. The true power of this processor is revealed in the all-optical nonlinearity compensation in an optical communication scenario where we provide results superior to those offered by strong machine learning algorithms with reduced power consumption and computational complexity. Finally, we showcase how the FWM module can be used as a reconfigurable nonlinear activation module being capable of reproducing characteristic fu
    
[^55]: 探索大型语言模型的对抗能力

    Exploring the Adversarial Capabilities of Large Language Models

    [https://arxiv.org/abs/2402.09132](https://arxiv.org/abs/2402.09132)

    本研究探索了大型语言模型的对抗能力，并发现其能够成功地制造对抗性示例以愚弄安全措施，特别是在仇恨言论检测方面具有重大影响。

    

    大型语言模型（LLMs）的普及引发了广泛和普遍的兴趣，因为它们具有强大的语言生成能力，为行业和研究提供了巨大的潜力。尽管以前的研究探讨了LLMs的安全性和隐私问题，但这些模型能否表现出对抗行为的程度仍然尚未完全探索。为了填补这一空白，我们研究常见的公开可用LLMs是否具有能力扰乱文本样本以愚弄安全措施，即所谓的对抗示例或攻击。更具体地说，我们调查LLMs是否本质上能够从良性样本中制造对抗性示例以愚弄现有的安全防线。我们的实验重点关注仇恨言论检测，发现LLMs成功地找到了对抗性扰动，有效地破坏了对仇恨言论检测系统的防御。我们的发现对（半）自动化安全评估和防御具有重要影响。

    arXiv:2402.09132v1 Announce Type: new Abstract: The proliferation of large language models (LLMs) has sparked widespread and general interest due to their strong language generation capabilities, offering great potential for both industry and research. While previous research delved into the security and privacy issues of LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored. Addressing this gap, we investigate whether common publicly available LLMs have inherent capabilities to perturb text samples to fool safety measures, so-called adversarial examples resp.~attacks. More specifically, we investigate whether LLMs are inherently able to craft adversarial examples out of benign samples to fool existing safe rails. Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively undermining hate speech detection systems. Our findings carry significant implications for (semi-)aut
    
[^56]: MPIrigen: 通过领域特定语言模型生成MPI代码

    MPIrigen: MPI Code Generation through Domain-Specific Language Models

    [https://arxiv.org/abs/2402.09126](https://arxiv.org/abs/2402.09126)

    本文研究了使用领域特定语言模型生成MPI代码的性能，并提出了使用预训练模型MonoCoder进行MPI-based程序生成的方法。

    

    在大规模并行计算中，高效的并行计算尤为重要，特别是在消息传递接口（MPI）集成领域。生成基于MPI的并行程序是一个具有挑战性的并行编程任务，尚未被探索。本研究首先探讨了先进的语言模型在生成基于MPI的并行程序方面的性能。发现广泛使用的模型，如GPT-3.5和PolyCoder（专门的多语言代码模型），在生成基于MPI的程序时表现出明显的性能下降，相比通用程序。相比之下，基于MPI相关编程语言C和C++预训练的领域特定模型MonoCoder的性能更好。随后，我们通过在HPCorpusMPI上对MonoCoder进行微调，引入了一个专门的MPI-based程序生成任务。

    arXiv:2402.09126v1 Announce Type: cross Abstract: The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration. The challenging parallel programming task of generating MPI-based parallel programs has remained unexplored. This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs. Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation, when generating MPI-based programs compared to general-purpose programs. In contrast, domain-specific models such as MonoCoder, which are pretrained on MPI-related programming languages of C and C++, outperform larger models. Subsequently, we introduce a dedicated downstream task of MPI-based program generation by fine-tuning MonoCoder on HPCorpusMPI. We call the r
    
[^57]: 混合输出高斯过程潜变量模型

    Mixed-Output Gaussian Process Latent Variable Models

    [https://arxiv.org/abs/2402.09122](https://arxiv.org/abs/2402.09122)

    本文提出了一种基于高斯过程潜变量模型的贝叶斯非参数方法，可以用于信号分离，并且能够处理包含纯组分信号加权和的情况，适用于光谱学和其他领域的多种应用。

    

    本文提出了一种贝叶斯非参数的信号分离方法，其中信号可以根据潜变量变化。我们的主要贡献是增加了高斯过程潜变量模型（GPLVMs），以包括每个数据点由已知数量的纯组分信号的加权和组成的情况，并观察多个输入位置。我们的框架允许使用各种关于每个观测权重的先验。这种灵活性使我们能够表示包括用于估计分数组成的总和为一约束和用于分类的二进制权重的用例。我们的贡献对于光谱学尤其相关，因为改变条件可能导致基础纯组分信号在样本之间变化。为了展示对光谱学和其他领域的适用性，我们考虑了几个应用：一个具有不同温度的近红外光谱数据集。

    arXiv:2402.09122v1 Announce Type: cross Abstract: This work develops a Bayesian non-parametric approach to signal separation where the signals may vary according to latent variables. Our key contribution is to augment Gaussian Process Latent Variable Models (GPLVMs) to incorporate the case where each data point comprises the weighted sum of a known number of pure component signals, observed across several input locations. Our framework allows the use of a range of priors for the weights of each observation. This flexibility enables us to represent use cases including sum-to-one constraints for estimating fractional makeup, and binary weights for classification. Our contributions are particularly relevant to spectroscopy, where changing conditions may cause the underlying pure component signals to vary from sample to sample. To demonstrate the applicability to both spectroscopy and other domains, we consider several applications: a near-infrared spectroscopy data set with varying temper
    
[^58]: 通过策略空间中的最优输运测量强化学习的探索

    Measuring Exploration in Reinforcement Learning via Optimal Transport in Policy Space

    [https://arxiv.org/abs/2402.09113](https://arxiv.org/abs/2402.09113)

    本论文提出了一种通过比较强化学习和监督学习算法在数据分布空间中路径长度来量化探索的方法，并在各种环境和多种算法上进行了实证分析。

    

    探索是决定强化学习（RL）学习速度和成功的关键因素。我们在这里量化和比较强化学习算法所完成的探索和学习的数量。具体来说，我们提出了一种新的度量指标，称为探索指数，用于比较强化学习算法相对于监督学习算法在知识传输（可转移性）方面所付出的相对努力，以及将 RL 的初始数据分布转化为对应的最终数据分布。我们将强化学习中的学习建模为一系列监督学习任务，并使用基于最优输运的度量标准来比较 RL 和 SL 算法在数据分布空间中的总路径。我们在各种环境和多种算法上进行了广泛的实证分析，证明探索指数可以揭示探索行为的特点。

    arXiv:2402.09113v1 Announce Type: new Abstract: Exploration is the key ingredient of reinforcement learning (RL) that determines the speed and success of learning. Here, we quantify and compare the amount of exploration and learning accomplished by a Reinforcement Learning (RL) algorithm. Specifically, we propose a novel measure, named Exploration Index, that quantifies the relative effort of knowledge transfer (transferability) by an RL algorithm in comparison to supervised learning (SL) that transforms the initial data distribution of RL to the corresponding final data distribution. The comparison is established by formulating learning in RL as a sequence of SL tasks, and using optimal transport based metrics to compare the total path traversed by the RL and SL algorithms in the data distribution space. We perform extensive empirical analysis on various environments and with multiple algorithms to demonstrate that the exploration index yields insights about the exploration behaviour 
    
[^59]: 随机尖峰关注：在尖峰网络中利用随机计算加速关注机制

    Stochastic Spiking Attention: Accelerating Attention with Stochastic Computing in Spiking Networks

    [https://arxiv.org/abs/2402.09109](https://arxiv.org/abs/2402.09109)

    本文提出了一种利用随机计算加速尖峰网络中的关注机制的新框架，证明该方法在CIFAR-10上能够达到高分类准确率，同时大大降低了计算能量和内存访问成本。

    

    最近，由于尖峰神经网络（SNN）在减少计算需求和提高功耗效率方面的潜力，已将其整合到Transformer体系结构中。然而，使用尖峰信号在通用计算平台上实现关注机制仍然低效。在本文中，我们提出了一种利用随机计算（SC）来有效执行基于SNN的Transformer的点积注意力的新框架。我们证明，我们的方法能够在10个时间步内在CIFAR-10上达到高分类准确率（83.53%），这与基线人工神经网络实现的性能（83.66%）相当。我们估计，该SC方法可以导致CMOS数字ASIC设计中计算能量减少6.3倍，内存访问成本减少1.7倍。我们通过实验证实了我们的随机注意力块设计。

    arXiv:2402.09109v1 Announce Type: cross Abstract: Spiking Neural Networks (SNNs) have been recently integrated into Transformer architectures due to their potential to reduce computational demands and to improve power efficiency. Yet, the implementation of the attention mechanism using spiking signals on general-purpose computing platforms remains inefficient. In this paper, we propose a novel framework leveraging stochastic computing (SC) to effectively execute the dot-product attention for SNN-based Transformers. We demonstrate that our approach can achieve high classification accuracy ($83.53\%$) on CIFAR-10 within 10 time steps, which is comparable to the performance of a baseline artificial neural network implementation ($83.66\%$). We estimate that the proposed SC approach can lead to over $6.3\times$ reduction in computing energy and $1.7\times$ reduction in memory access costs for a digital CMOS-based ASIC design. We experimentally validate our stochastic attention block design
    
[^60]: 卫星集群上的机载联邦学习调度

    Scheduling for On-Board Federated Learning with Satellite Clusters

    [https://arxiv.org/abs/2402.09105](https://arxiv.org/abs/2402.09105)

    本文提出了一种用于卫星集群上的机载联邦学习调度方案，利用可预测的可见性模式来解决间歇性连接问题，实现高效的数据管理和机器学习模型协同训练。

    

    小卫星的大规模星座已经成为大量宝贵数据的来源。为了高效管理这些数据，机载联邦学习（FL）允许卫星在不共享原始数据的情况下进行机器学习模型的协同训练。本文介绍了一种用于连接星座内部轨道间卫星的机载FL调度方案。所提方案利用了卫星与地面站之间的可预测可见性模式，包括个别卫星级别和整个轨道范围内的模式，以减轻间歇性连接问题，最大限度地利用可用时间。为此，本文采用了两个不同的调度程序：一个用于协调轨道之间的FL过程，另一个用于控制每个轨道内部的FL过程。这两个调度程序共同确定在地面站执行全局更新的适当时间，并分配合适的时间进行本地更新。

    arXiv:2402.09105v1 Announce Type: cross Abstract: Mega-constellations of small satellites have evolved into a source of massive amount of valuable data. To manage this data efficiently, on-board federated learning (FL) enables satellites to train a machine learning (ML) model collaboratively without having to share the raw data. This paper introduces a scheme for scheduling on-board FL for constellations connected with intra-orbit inter-satellite links. The proposed scheme utilizes the predictable visibility pattern between satellites and ground station (GS), both at the individual satellite level and cumulatively within the entire orbit, to mitigate intermittent connectivity and best use of available time. To this end, two distinct schedulers are employed: one for coordinating the FL procedures among orbits, and the other for controlling those within each orbit. These two schedulers cooperatively determine the appropriate time to perform global updates in GS and then allocate suitable
    
[^61]: FedSiKD: 客户相似性和知识蒸馏：解决非独立同分布和联邦学习中的约束问题

    FedSiKD: Clients Similarity and Knowledge Distillation: Addressing Non-i.i.d. and Constraints in Federated Learning

    [https://arxiv.org/abs/2402.09095](https://arxiv.org/abs/2402.09095)

    FedSiKD是一种结合了知识蒸馏的相似性联邦学习框架，旨在解决非独立同分布和联邦学习中的约束问题，通过促进簇内同质性来提高优化效率和加速学习过程。

    

    近年来，联邦学习（FL）作为一种在分散方式下训练机器学习模型并保护数据隐私的有前途的技术出现了。客户数据的非独立同分布（non-i.i.d.）特性以及对客户或边缘设备的限制，在FL中提出了重大挑战。此外，跨许多通信轮次的学习可能对模型的利用具有风险和潜在的不安全性。传统FL方法可能会受到这些挑战的影响。因此，我们引入了FedSiKD，它在基于相似性的联邦学习框架中融入了知识蒸馏（KD）。当客户端加入系统时，他们安全地共享有关其数据分布的相关统计信息，促进簇内同质性。这提高了优化效率并加速了学习过程，有效地在教师和学生模型之间进行知识传输。

    arXiv:2402.09095v1 Announce Type: new Abstract: In recent years, federated learning (FL) has emerged as a promising technique for training machine learning models in a decentralized manner while also preserving data privacy. The non-independent and identically distributed (non-i.i.d.) nature of client data, coupled with constraints on client or edge devices, presents significant challenges in FL. Furthermore, learning across a high number of communication rounds can be risky and potentially unsafe for model exploitation. Traditional FL approaches may suffer from these challenges. Therefore, we introduce FedSiKD, which incorporates knowledge distillation (KD) within a similarity-based federated learning framework. As clients join the system, they securely share relevant statistics about their data distribution, promoting intra-cluster homogeneity. This enhances optimization efficiency and accelerates the learning process, effectively transferring knowledge between teacher and student mo
    
[^62]: 三十年的激活函数：400个神经网络激活函数综合调查

    Three Decades of Activations: A Comprehensive Survey of 400 Activation Functions for Neural Networks

    [https://arxiv.org/abs/2402.09092](https://arxiv.org/abs/2402.09092)

    这篇论文是关于对400个神经网络激活函数进行全面调查，为研究人员提供一个详尽的概述和分类的方法，以便更好地选择适当的激活函数。

    

    神经网络在许多领域中已被证明是解决复杂问题的高效工具。深度学习的出现进一步加强了它们的重要性和实际可用性。神经网络成功的重要条件之一是选择适当的激活函数引入非线性模型。过去文献中提出了许多类型的激活函数，但没有单一的综合来源包含它们的详尽概述。即使在我们的经验中，缺乏这个概述也会导致冗余和无意中重新发现已有的激活函数。为了弥补这个差距，我们的论文提供了一个涉及400个激活函数的广泛调查，比之前的调查规模大几倍。我们的综合汇编还引用了这些调查；然而，它的主要目标是提供一个详尽的概述和分类的方法，以便研究人员能够更好地了解和选择适当的激活函数。

    arXiv:2402.09092v1 Announce Type: new Abstract: Neural networks have proven to be a highly effective tool for solving complex problems in many areas of life. Recently, their importance and practical usability have further been reinforced with the advent of deep learning. One of the important conditions for the success of neural networks is the choice of an appropriate activation function introducing non-linearity into the model. Many types of these functions have been proposed in the literature in the past, but there is no single comprehensive source containing their exhaustive overview. The absence of this overview, even in our experience, leads to redundancy and the unintentional rediscovery of already existing activation functions. To bridge this gap, our paper presents an extensive survey involving 400 activation functions, which is several times larger in scale than previous surveys. Our comprehensive compilation also references these surveys; however, its main goal is to provide 
    
[^63]: Sobolev训练用于算子学习

    Sobolev Training for Operator Learning

    [https://arxiv.org/abs/2402.09084](https://arxiv.org/abs/2402.09084)

    本研究通过将导数信息融入损失函数来改善算子学习框架的性能，并提出了一种在不规则网格上近似导数的新框架，从而表明Sobolev训练在近似解算子方面的有效性。

    

    本研究调查了Sobolev训练对于改善模型性能的算子学习框架的影响。我们的研究发现，将导数信息融入损失函数可以改善训练过程，并提出了一种在算子学习中近似不规则网格上的导数的新框架。我们的发现得到了实验和理论分析的支持。这表明了Sobolev训练在近似无穷维空间中的解算子方面的有效性。

    arXiv:2402.09084v1 Announce Type: cross Abstract: This study investigates the impact of Sobolev Training on operator learning frameworks for improving model performance. Our research reveals that integrating derivative information into the loss function enhances the training process, and we propose a novel framework to approximate derivatives on irregular meshes in operator learning. Our findings are supported by both experimental evidence and theoretical analysis. This demonstrates the effectiveness of Sobolev Training in approximating the solution operators between infinite-dimensional spaces.
    
[^64]: 异常检测器的检测延迟：一个被忽视的视角？

    Detection Latencies of Anomaly Detectors: An Overlooked Perspective ?

    [https://arxiv.org/abs/2402.09082](https://arxiv.org/abs/2402.09082)

    本文提出了检测延迟的重要性，并提出了一种评估方法，以确保准确和及时的检测之间的权衡。研究通过将误报率与攻击和错误的时间延迟相关联，并提供了配置检测器的指导原则。

    

    攻击方式不断变化，与此同时ICT系统的复杂性也在增加，这使得构建基于异常的入侵检测器(ID)和错误检测器(ED)变得困难：它们必须准确地检测攻击，并且应该及时进行检测。尽管大多数研究工作侧重于改进和比较检测能力，但检测的及时性往往不被考虑或不够充分地进行评估或讨论。本文论述了测量攻击和错误的时间延迟的相关性，并提出了对检测器进行评估的方法，以确保正确和及时检测之间的实用性权衡。简要地说，该方法将误报率与攻击和错误的时间延迟相关联，最终提供了配置检测器的指导原则。我们通过评估不同的ED和ID解决方案在两个工业案例中应用了我们的方法：i) 一个案例是...

    arXiv:2402.09082v1 Announce Type: cross Abstract: The ever-evolving landscape of attacks, coupled with the growing complexity of ICT systems, makes crafting anomaly-based intrusion detectors (ID) and error detectors (ED) a difficult task: they must accurately detect attacks, and they should promptly perform detections. Although improving and comparing the detection capability is the focus of most research works, the timeliness of the detection is less considered and often insufficiently evaluated or discussed. In this paper, we argue the relevance of measuring the temporal latency of attacks and errors, and we propose an evaluation approach for detectors to ensure a pragmatic trade-off between correct and in-time detection. Briefly, the approach relates the false positive rate with the temporal latency of attacks and errors, and this ultimately leads to guidelines for configuring a detector. We apply our approach by evaluating different ED and ID solutions in two industrial cases: i) a
    
[^65]: 低秩外推梯度方法用于可扩展的半定规划问题

    Low-Rank Extragradient Methods for Scalable Semidefinite Optimization

    [https://arxiv.org/abs/2402.09081](https://arxiv.org/abs/2402.09081)

    本文研究了低秩外推梯度方法在可扩展半定规划问题上的应用，通过使用低秩奇异值分解来投影到半正定锥，取得了收敛于约束优化问题解的理论结果。

    

    我们考虑了几类非常重要的半定规划问题，这些问题既包括凸目标函数（平滑或非平滑），又包括额外的线性或非线性平滑凸约束，这些问题在统计学、机器学习、组合优化等领域都很常见。我们关注高维和可能的情境，其中问题具有低秩解，并满足低秩互补条件。我们提供了几个理论结果，证明在这些情况下，已知的外推梯度方法，在接近最优原始对偶解的情况下初始化时，收敛于带有标准收敛速度保证的约束优化问题的解，仅使用低秩奇异值分解（SVD）来投影到半正定锥，而不是计算上限的全秩SVD所需的操作。

    arXiv:2402.09081v1 Announce Type: cross Abstract: We consider several classes of highly important semidefinite optimization problems that involve both a convex objective function (smooth or nonsmooth) and additional linear or nonlinear smooth and convex constraints, which are ubiquitous in statistics, machine learning, combinatorial optimization, and other domains. We focus on high-dimensional and plausible settings in which the problem admits a low-rank solution which also satisfies a low-rank complementarity condition. We provide several theoretical results proving that, under these circumstances, the well-known Extragradient method, when initialized in the proximity of an optimal primal-dual solution, converges to a solution of the constrained optimization problem with its standard convergence rates guarantees, using only low-rank singular value decompositions (SVD) to project onto the positive semidefinite cone, as opposed to computationally-prohibitive full-rank SVDs required in w
    
[^66]: 在 Actor-Critic 方法中利用估计偏差的深度双 Q-Learning 的探索

    Exploiting Estimation Bias in Deep Double Q-Learning for Actor-Critic Methods

    [https://arxiv.org/abs/2402.09078](https://arxiv.org/abs/2402.09078)

    本文提出了两种创新方法，ExpD3 和 BE-TD3，用于解决和利用 Actor-Critic 方法中的估计偏差问题。实验证明这些算法在连续控制任务中比现有方法更高效。

    

    本文介绍了在强化学习领域中创新的方法，重点是解决和利用连续控制任务中 Actor-Critic 方法中的估计偏差问题，使用了深度双 Q-Learning。我们提出了两种新的算法：Expectile Delayed Deep Deterministic Policy Gradient (ExpD3) 和 Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3)。ExpD3 旨在通过单一的 Q 估计来减少过度估计偏差，并在计算效率和性能之间提供平衡，而 BE-TD3 则旨在在训练过程中动态选择最有利的估计偏差。我们进行了大量的实验，在各种连续控制任务中展示了我们方法的有效性。我们展示了这些算法在与 TD3 等现有方法相比，尤其是在估计偏差显著影响学习的环境中，可以匹敌或超越它们。这些结果凸显了估计偏差对学习的重要性。

    arXiv:2402.09078v1 Announce Type: cross Abstract: This paper introduces innovative methods in Reinforcement Learning (RL), focusing on addressing and exploiting estimation biases in Actor-Critic methods for continuous control tasks, using Deep Double Q-Learning. We propose two novel algorithms: Expectile Delayed Deep Deterministic Policy Gradient (ExpD3) and Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3). ExpD3 aims to reduce overestimation bias with a single $Q$ estimate, offering a balance between computational efficiency and performance, while BE-TD3 is designed to dynamically select the most advantageous estimation bias during training. Our extensive experiments across various continuous control tasks demonstrate the effectiveness of our approaches. We show that these algorithms can either match or surpass existing methods like TD3, particularly in environments where estimation biases significantly impact learning. The results underline the importance of
    
[^67]: DisGNet: 一种用于Gough-Stewart平台前向运动学学习的距离图神经网络

    DisGNet: A Distance Graph Neural Network for Forward Kinematics Learning of Gough-Stewart Platform

    [https://arxiv.org/abs/2402.09077](https://arxiv.org/abs/2402.09077)

    本文提出了一种名为DisGNet的距离图神经网络，用于解决Gough-Stewart平台的前向运动学问题。通过使用k-FWL算法进行消息传递和引入适用于GPU的Newton-Raphson方法，DisGNet可以实现超高精度的输出姿势，同时满足实时计算的要求。

    

    本文提出了一种名为DisGNet的图神经网络，用于学习图距离矩阵，以解决Gough-Stewart平台的前向运动学问题。 DisGNet采用k-FWL算法进行消息传递，具有较小的参数数量，提供高表达能力，适合实际部署。此外，我们引入了适用于GPU的Newton-Raphson方法，一种在GPU上执行的高效并行优化方法，用于优化DisGNet的输出姿势，实现了超高精度姿势。该新颖的两阶段方法在满足实时要求的同时提供了超高精度的输出。我们的结果显示，在我们的数据集上，DisGNet可以实现错误精度低于1mm和1度，在79.8\%和98.2\%以上。作为在GPU上执行的两阶段方法，我们可以确保满足实时计算的要求。代码已在https://github.com/FLAMEZZ5201/DisGNet上发布。

    arXiv:2402.09077v1 Announce Type: cross Abstract: In this paper, we propose a graph neural network, DisGNet, for learning the graph distance matrix to address the forward kinematics problem of the Gough-Stewart platform. DisGNet employs the k-FWL algorithm for message-passing, providing high expressiveness with a small parameter count, making it suitable for practical deployment. Additionally, we introduce the GPU-friendly Newton-Raphson method, an efficient parallelized optimization method executed on the GPU to refine DisGNet's output poses, achieving ultra-high-precision pose. This novel two-stage approach delivers ultra-high precision output while meeting real-time requirements. Our results indicate that on our dataset, DisGNet can achieves error accuracys below 1mm and 1deg at 79.8\% and 98.2\%, respectively. As executed on a GPU, our two-stage method can ensure the requirement for real-time computation. Codes are released at https://github.com/FLAMEZZ5201/DisGNet.
    
[^68]: 用于具有二次奖励的强化学习的稳态误差补偿

    Steady-State Error Compensation for Reinforcement Learning with Quadratic Rewards

    [https://arxiv.org/abs/2402.09075](https://arxiv.org/abs/2402.09075)

    该论文研究提出了一种使用积分项补偿二次奖励函数稳态误差的方法，通过增强长期奖励的考虑，有效降低了系统性能（如自适应巡航控制和变道模型）中的稳态误差问题。

    

    强化学习中的奖励函数选择对系统性能的影响引起了广泛关注。当使用二次奖励函数时，经常会出现稳态误差的问题。尽管已有的使用绝对值类型奖励函数的解决方案在一定程度上解决了这个问题，但往往会在特定系统状态下引起较大波动，导致突然的变化。为了应对这一挑战，本研究提出了一种引入积分项的方法。通过将这个项积分进二次奖励函数中，对RL算法进行精确调整，增强系统对长期奖励的考虑，从而缓解稳态误差的问题。通过在自适应巡航控制（ACC）模型和变道模型上的实验和性能评估，我们验证了提出的方法不仅有效地减小了稳态误差，还提高了系统的性能表现。

    arXiv:2402.09075v1 Announce Type: cross Abstract: The selection of a reward function in Reinforcement Learning (RL) has garnered significant attention because of its impact on system performance. Issues of steady-state error often manifest when quadratic reward functions are employed. Although existing solutions using absolute-value-type reward functions partially address this problem, they tend to induce substantial fluctuations in specific system states, leading to abrupt changes. In response to this challenge, this study proposes an approach that introduces an integral term. By integrating this term into quadratic-type reward functions, the RL algorithm is adeptly tuned, augmenting the system's consideration of long-term rewards and, consequently, alleviating concerns related to steady-state errors. Through experiments and performance evaluations on the Adaptive Cruise Control (ACC) model and lane change models, we validate that the proposed method not only effectively diminishes st
    
[^69]: 遥感图像中的固体废物检测：一项调查

    Solid Waste Detection in Remote Sensing Images: A Survey

    [https://arxiv.org/abs/2402.09066](https://arxiv.org/abs/2402.09066)

    本文调查了固体废物在遥感图像中的检测方法。研究者利用地球观测卫星提供的高分辨率数据，通过遥感图像实现了固体废物处置场地的识别、监测和评估。

    

    识别和表征非法固体废物处置场地对环境保护至关重要，特别是应对污染和健康危害。不当管理的垃圾填埋场通过雨水渗透污染土壤和地下水，对动物和人类构成威胁。传统的填埋场辨识方法，如现场检查，耗时且昂贵。遥感技术是用于识别和监测固体废物处置场地的一种经济有效的解决方案，可以实现广泛覆盖和多次获取。地球观测（EO）卫星配备了一系列传感器和成像能力，几十年来一直提供高分辨率的数据。研究人员提出了专门的技术，利用遥感图像执行一系列任务，如废物场地检测、倾倒场监测和适宜位置评估。

    arXiv:2402.09066v1 Announce Type: cross Abstract: The detection and characterization of illegal solid waste disposal sites are essential for environmental protection, particularly for mitigating pollution and health hazards. Improperly managed landfills contaminate soil and groundwater via rainwater infiltration, posing threats to both animals and humans. Traditional landfill identification approaches, such as on-site inspections, are time-consuming and expensive. Remote sensing is a cost-effective solution for the identification and monitoring of solid waste disposal sites that enables broad coverage and repeated acquisitions over time. Earth Observation (EO) satellites, equipped with an array of sensors and imaging capabilities, have been providing high-resolution data for several decades. Researchers proposed specialized techniques that leverage remote sensing imagery to perform a range of tasks such as waste site detection, dumping site monitoring, and assessment of suitable locati
    
[^70]: 软提示威胁：通过嵌入空间对开源LLMs进行安全对齐攻击和遗忘

    Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space

    [https://arxiv.org/abs/2402.09063](https://arxiv.org/abs/2402.09063)

    该论文提出了一种新的嵌入空间攻击方法，针对开源LLMs进行攻击，绕过模型对齐并在遗忘的情况下提取信息，比传统的离散攻击更高效。

    

    当前对LLMs的敌对鲁棒性研究专注于自然语言空间中的离散输入操纵，这些操纵可以直接转移到闭源模型中。然而，这种方法忽视了开源模型的持续进展。随着开源模型能力的提升，确保其安全性也变得越来越重要。然而，针对开源LLMs的攻击，利用完全模型访问权限的方式仍然很少被探索。我们填补了这一研究空白，并提出了嵌入空间攻击，直接攻击输入令牌的连续嵌入表示。我们发现，嵌入空间攻击比离散攻击或模型微调更有效地绕过模型对齐并触发有害行为。此外，我们在遗忘的背景下提出了一种新的威胁模型，并展示了嵌入空间攻击在从未经学习的LLMs中提取应该删除的信息方面的能力。

    arXiv:2402.09063v1 Announce Type: new Abstract: Current research in adversarial robustness of LLMs focuses on discrete input manipulations in the natural language space, which can be directly transferred to closed-source models. However, this approach neglects the steady progression of open-source models. As open-source models advance in capability, ensuring their safety also becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose the embedding space attack, which directly attacks the continuous embedding representation of input tokens. We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Furthermore, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across m
    
[^71]: 我看不见它，但我可以微调它：使用全同态加密对Transformer进行加密微调

    I can't see it but I can Fine-tune it: On Encrypted Fine-tuning of Transformers using Fully Homomorphic Encryption

    [https://arxiv.org/abs/2402.09059](https://arxiv.org/abs/2402.09059)

    本文介绍了一种使用全同态加密进行隐私保护微调的系统BlindTuner，它可以在图像分类任务中实现对Transformer模型的隐私保护训练，并在准确性上与非加密模型相当。

    

    在当前的机器学习环境中，微调预训练的Transformer模型已经成为一种重要的技术，特别是在训练数据有限的情况下。然而，当数据共享遇到障碍时，如严格的隐私法规或用户对个人信息披露的担忧，就会出现挑战。早期基于安全多方计算（SMC）和全同态加密（FHE）的隐私保护机器学习（PPML）的工作更注重隐私保护的推理而不是隐私保护的训练。为此，我们引入了BlindTuner，一种隐私保护的微调系统，可实现对Transformer模型基于全同态加密数据的训练，用于图像分类。我们进行了大量实验验证了BlindTuner的有效性，并证明其与非加密模型相比准确性相当。值得注意的是，我们的研究结果突出了一些重要的创新和贡献。

    arXiv:2402.09059v1 Announce Type: cross Abstract: In today's machine learning landscape, fine-tuning pretrained transformer models has emerged as an essential technique, particularly in scenarios where access to task-aligned training data is limited. However, challenges surface when data sharing encounters obstacles due to stringent privacy regulations or user apprehension regarding personal information disclosure. Earlier works based on secure multiparty computation (SMC) and fully homomorphic encryption (FHE) for privacy-preserving machine learning (PPML) focused more on privacy-preserving inference than privacy-preserving training. In response, we introduce BlindTuner, a privacy-preserving fine-tuning system that enables transformer training exclusively on homomorphically encrypted data for image classification. Our extensive experimentation validates BlindTuner's effectiveness by demonstrating comparable accuracy to non-encrypted models. Notably, our findings highlight a substantia
    
[^72]: 智能服装中的分布式纤维传感

    Distributed Sensing Along Fibres for Smart Clothing

    [https://arxiv.org/abs/2402.09057](https://arxiv.org/abs/2402.09057)

    该论文介绍了一种分布式纤维传感技术，用于智能服装中的运动跟踪和生物信号监测。通过使用螺旋辅助弹性纱线传感器和特定算法，能够在服装中高效地实现多区域的局部应变测量。

    

    纺织传感器将我们日常穿着的衣物完全无干扰地转化为追踪运动和生物信号的手段。但是，当传感器数量扩大时，连接和空间成为采用“智能”服装的主要障碍。现有研究中存在一个关键限制：刚性和纺织元素之间的连接常常不可靠，并且需要以与纺织品大规模生产方法不兼容的方式与传感器进行接口。我们引入了一个原型服装、紧凑读出电路和算法，以测量纤维多个区域的局部应变。我们使用具有可调灵敏度的螺旋辅助弹性纱线传感器，以选择性地对应变信号作出反应。我们在服装上展示了分布式感应，在单个连续纤维上监测手臂关节角度。与光学运动捕捉相比，我们实现了约5倍的性能提升。

    arXiv:2402.09057v1 Announce Type: cross Abstract: Textile sensors transform our everyday clothing into a means to track movement and bio-signals in a completely unobtrusive way. One major hindrance to the adoption of "smart" clothing is the difficulty encountered with connections and space when scaling up the number of sensors. There is a lack of research addressing a key limitation in wearable electronics: connections between rigid and textile elements are often unreliable and they require interfacing sensors in a way incompatible with textile mass production methods. We introduce a prototype garment, compact readout circuit, and algorithm to measure localized strain along multiple regions of a fibre. We employ a helical auxetic yarn sensor with tunable sensitivity along its length to selectively respond to strain signals. We demonstrate distributed sensing in clothing, monitoring arm joint angles from a single continuous fibre. Compared to optical motion capture, we achieve around 5{
    
[^73]: 证据深度学习方法是否准确地表示认识不确定性？

    Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?

    [https://arxiv.org/abs/2402.09056](https://arxiv.org/abs/2402.09056)

    本论文提出了关于证据深度学习的新理论洞见, 高亮了在优化二阶损失函数和解释得出的认识不确定性度量上的困难性

    

    可信的机器学习系统不仅应返回准确的预测结果，还应提供可靠的不确定性表示。贝叶斯方法常用于量化不确定性，但近年来，证据深度学习方法等替代方法也变得流行起来。后者本质上扩展了经验风险最小化（ERM），用于预测结果的二阶概率分布，从中可以提取认识（和随机）不确定性的度量。本文提供了证据深度学习的新理论洞见，强调了优化二阶损失函数以及解释结果认识不确定性度量的困难性。通过系统化的设置，涵盖了分类、回归和计数的广泛方法，这篇论文为可辨识性和收敛性问题提供了新的洞察。

    arXiv:2402.09056v1 Announce Type: new Abstract: Trustworthy ML systems should not only return accurate predictions, but also a reliable representation of their uncertainty. Bayesian methods are commonly used to quantify both aleatoric and epistemic uncertainty, but alternative approaches, such as evidential deep learning methods, have become popular in recent years. The latter group of methods in essence extends empirical risk minimization (ERM) for predicting second-order probability distributions over outcomes, from which measures of epistemic (and aleatoric) uncertainty can be extracted. This paper presents novel theoretical insights of evidential deep learning, highlighting the difficulties in optimizing second-order loss functions and interpreting the resulting epistemic uncertainty measures. With a systematic setup that covers a wide range of approaches for classification, regression and counts, it provides novel insights into issues of identifiability and convergence in second-o
    
[^74]: 通过层角色差异化，端到端训练引发信息瓶颈：与逐层训练的比较分析

    End-to-End Training Induces Information Bottleneck through Layer-Role Differentiation: A Comparative Analysis with Layer-wise Training

    [https://arxiv.org/abs/2402.09050](https://arxiv.org/abs/2402.09050)

    本文通过与逐层训练的比较，重新考虑了为什么端到端训练表现出优越的性能，分析了信息传播方面的优势，并对训练模型的性质差异进行了深入理解。

    

    端到端（E2E）训练通过误差反向传播优化整个模型，从根本上支持深度学习的进展。尽管其性能很高，但E2E训练面临内存消耗、并行计算和与实际大脑功能的不一致等问题。已经提出了各种替代方法来克服这些困难，然而目前还没有一种能够与E2E训练的性能匹配的方法，因此在实用性上存在不足。此外，对于训练模型性质的差异在性能差距之外缺乏深入的理解。在本文中，我们通过与逐层训练进行比较，重新考虑了为什么E2E训练表现出优越的性能，逐层训练是一种局部设置错误的非E2E方法。在观察到E2E训练在传播输入信息方面具有优势的基础上，我们分析了中间表示的信息平面动态。

    arXiv:2402.09050v1 Announce Type: new Abstract: End-to-end (E2E) training, optimizing the entire model through error backpropagation, fundamentally supports the advancements of deep learning. Despite its high performance, E2E training faces the problems of memory consumption, parallel computing, and discrepancy with the functionalities of the actual brain. Various alternative methods have been proposed to overcome these difficulties; however, no one can yet match the performance of E2E training, thereby falling short in practicality. Furthermore, there is no deep understanding regarding differences in the trained model properties beyond the performance gap. In this paper, we reconsider why E2E training demonstrates a superior performance through a comparison with layer-wise training, a non-E2E method that locally sets errors. On the basis of the observation that E2E training has an advantage in propagating input information, we analyze the information plane dynamics of intermediate rep
    
[^75]: 推理和学习统一理论的抽象推断

    Inference of Abstraction for a Unified Account of Reasoning and Learning

    [https://arxiv.org/abs/2402.09046](https://arxiv.org/abs/2402.09046)

    本文提出了一个以贝叶斯方法为基础的概率推断理论，用于推理和学习的统一理论。该理论通过将数据转化为符号知识，从而实现推理过程。我们对逻辑推理关系和MNIST数据集进行了讨论和实验证明。

    

    在神经科学中受到贝叶斯方法的启发，我们提出了一个简单的概率推断理论，用于推理和学习的统一理论。我们通过形式逻辑中的可满足性来建模数据如何导致符号知识。推理是通过抽象，即选择性无知，从数据中推导出符号知识的过程。我们讨论了逻辑推理关系的证明基础的理论正确性。我们还讨论了基于实验证据的MNIST数据集的实验正确性。

    arXiv:2402.09046v1 Announce Type: new Abstract: Inspired by Bayesian approaches to brain function in neuroscience, we give a simple theory of probabilistic inference for a unified account of reasoning and learning. We simply model how data cause symbolic knowledge in terms of its satisfiability in formal logic. The underlying idea is that reasoning is a process of deriving symbolic knowledge from data via abstraction, i.e., selective ignorance. The logical consequence relation is discussed for its proof-based theoretical correctness. The MNIST dataset is discussed for its experiment-based empirical correctness.
    
[^76]: 人工智能模型的可审计性：在操纵下，是否有些模型更难以审计？

    Under manipulations, are some AI models harder to audit?

    [https://arxiv.org/abs/2402.09043](https://arxiv.org/abs/2402.09043)

    本论文研究在操纵下，是否有些人工智能模型更难以审计，在模型可以适应任何数据的情况下，无论是主动还是非主动的审计策略都无法超越随机抽样。研究发现审计的可操纵性与目标模型的容量有关。

    

    审计员需要稳健的方法来评估网络平台是否符合法律规定。然而，由于他们很难获得平台使用的算法、实现或训练数据，这个问题比简单的度量估计更难。在最近的防操纵审计框架下，我们研究了在现实情况下进行稳健审计的可行性，其中模型具有较大的容量。我们首先证明了一个约束性结果：如果一个网络平台使用的模型可以适应任何数据，那么无论是主动还是非主动的审计策略，在估计诸如人口平衡性等性质时都无法超越随机抽样。为了更好地理解最先进的审计技术在何种条件下仍然具有竞争力，我们然后通过使用Rademacher复杂度将审计的可操纵性与目标模型的容量相关联。我们通过实证验证这些结果在流行的增加模型上的有效性。

    arXiv:2402.09043v1 Announce Type: new Abstract: Auditors need robust methods to assess the compliance of web platforms with the law. However, since they hardly ever have access to the algorithm, implementation, or training data used by a platform, the problem is harder than a simple metric estimation. Within the recent framework of manipulation-proof auditing, we study in this paper the feasibility of robust audits in realistic settings, in which models exhibit large capacities. We first prove a constraining result: if a web platform uses models that may fit any data, no audit strategy -- whether active or not -- can outperform random sampling when estimating properties such as demographic parity. To better understand the conditions under which state-of-the-art auditing techniques may remain competitive, we then relate the manipulability of audits to the capacity of the targeted models, using the Rademacher complexity. We empirically validate these results on popular models of increasi
    
[^77]: 使用平方Sigmoid TanH (SST)激活在数据限制下提高顺序模型性能

    Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST) Activation Under Data Constraints

    [https://arxiv.org/abs/2402.09034](https://arxiv.org/abs/2402.09034)

    该论文提出了一种平方Sigmoid TanH（SST）激活函数，用于增强在数据限制下的顺序模型学习能力。通过数学平方放大强激活和弱激活之间的差异，改善梯度流和信息过滤。在多个应用中评估了SST驱动的LSTM和GRU模型的性能。

    

    激活函数通过引入非线性来使神经网络能够学习复杂的表示。虽然前馈模型通常使用修正线性单元，但是顺序模型如递归神经网络、长短时记忆（LSTM）和门控循环单元（GRU）仍然依赖于Sigmoid和TanH激活函数。然而，这些传统的激活函数常常在训练在小顺序数据集上时难以建模稀疏模式以有效捕获时间依赖性。为了解决这个限制，我们提出了特别针对在数据限制下增强顺序模型学习能力的平方Sigmoid TanH（SST）激活。SST通过数学平方来放大强激活和弱激活之间的差异，随着信号随时间传播，有助于改善梯度流和信息过滤。我们评估了使用SST的LSTM和GRU模型在不同应用中的性能。

    arXiv:2402.09034v1 Announce Type: cross Abstract: Activation functions enable neural networks to learn complex representations by introducing non-linearities. While feedforward models commonly use rectified linear units, sequential models like recurrent neural networks, long short-term memory (LSTMs) and gated recurrent units (GRUs) still rely on Sigmoid and TanH activation functions. However, these classical activation functions often struggle to model sparse patterns when trained on small sequential datasets to effectively capture temporal dependencies. To address this limitation, we propose squared Sigmoid TanH (SST) activation specifically tailored to enhance the learning capability of sequential models under data constraints. SST applies mathematical squaring to amplify differences between strong and weak activations as signals propagate over time, facilitating improved gradient flow and information filtering. We evaluate SST-powered LSTMs and GRUs for diverse applications, such a
    
[^78]: SLEB: 通过冗余验证和消除Transformer块优化LLM的流程

    SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks

    [https://arxiv.org/abs/2402.09025](https://arxiv.org/abs/2402.09025)

    SLEB是一种通过消除冗余的Transformer块来优化LLM流程的新方法，它成功加速了LLM的推理过程。

    

    大型语言模型（LLM）在各种自然语言处理任务中证明了其高效性。然而，它们庞大的参数数量给实际部署带来了重大挑战。精简，一种旨在减小LLM大小和复杂度的技术，通过从网络中删除冗余组件提供了潜在解决方案。尽管精简有希望，但现有方法往往难以实现显著的端到端LLM推理加速。本文中，我们引入了SLEB，一种通过消除冗余的Transformer块来优化LLM流程的新方法。我们选择Transformer块作为精简的基本单位，因为LLM在相邻块的输出之间具有块级别的冗余和高相似性。这个选择使我们能够有效地增强LLM的处理速度。我们的实验证明，SLEB成功加速了LLM的推理过程。

    arXiv:2402.09025v1 Announce Type: new Abstract: Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB successfully accelerates LLM inference without
    
[^79]: 神经算子遇上能量理论: 哈密顿和耗散型偏微分方程的算子学习

    Neural Operators Meet Energy-based Theory: Operator Learning for Hamiltonian and Dissipative PDEs

    [https://arxiv.org/abs/2402.09018](https://arxiv.org/abs/2402.09018)

    本文提出了Energy-consistent Neural Operators (ENOs)框架，用于学习遵循能量守恒或耗散定律的PDE解算子。通过引入受物理学能量理论启发的惩罚函数，并通过另一个DNN模型建模能量函数，可以确保DNN解算子的输出满足能量一致性，而无需显式的PDEs。实验证明ENO在多个物理系统上的表现优于以往方法。

    

    近年来，算子学习引起了广泛关注，旨在学习函数空间之间的映射关系。之前的研究提出了利用深度神经网络(DNNs)学习这种映射的方法，实现对偏微分方程(PDEs)的解算子的学习。然而，这些方法仍然难以学习遵守物理规律的动力学。本文提出了能量一致神经算子(ENOs)，这是一个通用的框架，用于学习遵循能量守恒或耗散定律的PDE解算子。我们引入了一种受物理学能量理论启发的惩罚函数用于训练，其中能量函数由另一个DNN来建模，使得基于DNN的解算子的输出能够保证能量一致性，而不需要显式的PDEs。在多个物理系统上的实验证明ENO的性能明显优于以往方法。

    arXiv:2402.09018v1 Announce Type: cross Abstract: The operator learning has received significant attention in recent years, with the aim of learning a mapping between function spaces. Prior works have proposed deep neural networks (DNNs) for learning such a mapping, enabling the learning of solution operators of partial differential equations (PDEs). However, these works still struggle to learn dynamics that obeys the laws of physics. This paper proposes Energy-consistent Neural Operators (ENOs), a general framework for learning solution operators of PDEs that follows the energy conservation or dissipation law from observed solution trajectories. We introduce a novel penalty function inspired by the energy-based theory of physics for training, in which the energy functional is modeled by another DNN, allowing one to bias the outputs of the DNN-based solution operators to ensure energetic consistency without explicit PDEs. Experiments on multiple physical systems show that ENO outperfor
    
[^80]: 使用原型特征进行梯度对齐的完全测试时间自适应

    Gradient Alignment with Prototype Feature for Fully Test-time Adaptation

    [https://arxiv.org/abs/2402.09004](https://arxiv.org/abs/2402.09004)

    在测试时间自适应中，我们提出了一种名为GAP的正则化方法，通过梯度对齐和原型特征，减轻了来自于错误分类伪标签熵最小化损失的不适当引导，显著改善了TTA方法。

    

    在测试时间自适应（TTA）的背景下，我们提出了一种正则化方法，称为梯度对齐与原型特征（GAP），它可以减轻由于错误分类伪标签的熵最小化损失而导致的不适当引导。我们开发了一个梯度对齐损失，以精确地管理适应过程，确保对某些数据进行的更改不会对模型在其他数据上的性能产生负面影响。我们引入了一个类的原型特征作为负面影响的代理测量。为了使在TTA约束下GAP正则化方法可行，即模型只能访问没有标签的测试数据，我们通过两种方式修改了其公式：用分类器的权重向量近似原型特征，不使用反向传播计算梯度。我们证明了GAP在各种数据集上显著改善了TTA方法，证明了其多功能性和有效性。

    arXiv:2402.09004v1 Announce Type: cross Abstract: In context of Test-time Adaptation(TTA), we propose a regularizer, dubbed Gradient Alignment with Prototype feature (GAP), which alleviates the inappropriate guidance from entropy minimization loss from misclassified pseudo label. We developed a gradient alignment loss to precisely manage the adaptation process, ensuring that changes made for some data don't negatively impact the model's performance on other data. We introduce a prototype feature of a class as a proxy measure of the negative impact. To make GAP regularizer feasible under the TTA constraints, where model can only access test data without labels, we tailored its formula in two ways: approximating prototype features with weight vectors of the classifier, calculating gradient without back-propagation. We demonstrate GAP significantly improves TTA methods across various datasets, which proves its versatility and effectiveness.
    
[^81]: 在放射治疗数据中探索联合深度学习以标准化命名约定

    Exploring Federated Deep Learning for Standardising Naming Conventions in Radiotherapy Data

    [https://arxiv.org/abs/2402.08999](https://arxiv.org/abs/2402.08999)

    本文介绍了一种利用联合深度学习在放射治疗数据中标准化命名约定的方法，通过集成去中心化实时数据和联邦学习来实现。这是一个自动化和高效的方法，可以解决多机构中心的数据标准化问题。

    

    标准化放射治疗（RT）数据中的结构体积名称是必要的，以实现数据挖掘和分析，特别是跨机构中心。这个过程需要大量时间和资源，这突显了处理任务的新的自动化和高效方法的需求。已经提出和评估了几种基于机器学习的方法来标准化命名约定。然而，没有研究考虑到RT患者记录分布在多个数据中心。本文介绍了一种模拟真实环境的方法，建立标准化命名约定。通过集成去中心化实时数据和联邦学习（FL）来实现这一目标。提出了一种多模态深度人工神经网络，以在联邦设置中标准化RT数据。从结构中提取了三种可能的属性来训练深度学习模型：表格、视觉和体积。

    arXiv:2402.08999v1 Announce Type: new Abstract: Standardising structure volume names in radiotherapy (RT) data is necessary to enable data mining and analyses, especially across multi-institutional centres. This process is time and resource intensive, which highlights the need for new automated and efficient approaches to handle the task. Several machine learning-based methods have been proposed and evaluated to standardise nomenclature. However, no studies have considered that RT patient records are distributed across multiple data centres. This paper introduces a method that emulates real-world environments to establish standardised nomenclature. This is achieved by integrating decentralised real-time data and federated learning (FL). A multimodal deep artificial neural network was proposed to standardise RT data in federated settings. Three types of possible attributes were extracted from the structures to train the deep learning models: tabular, visual, and volumetric. Simulated ex
    
[^82]: 学习线性混合随机最短路径的近似最小最大遗憾

    Nearly Minimax Optimal Regret for Learning Linear Mixture Stochastic Shortest Path

    [https://arxiv.org/abs/2402.08998](https://arxiv.org/abs/2402.08998)

    本文研究了具有线性混合转移核函数的随机最短路径问题，并提出了一种无需限制性假设的新算法。该算法基于带有方差感知置信区间的扩展值迭代，并通过高阶矩的递归估计实现了近似最小最大遗憾界。

    

    我们研究了具有线性混合转移核函数的随机最短路径（SSP）问题，其中一个代理重复与随机环境互动，并寻求达到特定目标状态同时最小化累积成本。现有的工作通常假设成本函数具有严格的正下界，或者期望长度的最优策略具有上界。在本文中，我们提出了一种新的算法来消除这些限制性假设。我们的算法基于带有精细化方差感知置信区间的扩展值迭代，其中方差从高阶矩递归估计得到。我们的算法实现了$\tilde{\mathcal O}(dB_*\sqrt{K})$ 的遗憾界，其中$d$ 是线性转移核函数中特征映射的维度，$B_*$ 是最优策略的总累积成本的上界，$K$ 是剧集的数量。我们的遗憾上界与$\Omega(.

    arXiv:2402.08998v1 Announce Type: new Abstract: We study the Stochastic Shortest Path (SSP) problem with a linear mixture transition kernel, where an agent repeatedly interacts with a stochastic environment and seeks to reach certain goal state while minimizing the cumulative cost. Existing works often assume a strictly positive lower bound of the cost function or an upper bound of the expected length for the optimal policy. In this paper, we propose a new algorithm to eliminate these restrictive assumptions. Our algorithm is based on extended value iteration with a fine-grained variance-aware confidence set, where the variance is estimated recursively from high-order moments. Our algorithm achieves an $\tilde{\mathcal O}(dB_*\sqrt{K})$ regret bound, where $d$ is the dimension of the feature mapping in the linear transition kernel, $B_*$ is the upper bound of the total cumulative cost for the optimal policy, and $K$ is the number of episodes. Our regret upper bound matches the $\Omega(
    
[^83]: 通过近端点方法进行随机优化中的方差减少和低样本复杂性

    Variance Reduction and Low Sample Complexity in Stochastic Optimization via Proximal Point Method

    [https://arxiv.org/abs/2402.08992](https://arxiv.org/abs/2402.08992)

    本文提出了一种通过近端点方法进行随机优化的方法，能够在弱条件下获得低样本复杂度，并实现方差减少的目标。

    

    本文提出了一种随机近端点法来解决随机凸复合优化问题。随机优化中的高概率结果通常依赖于对随机梯度噪声的限制性假设，例如子高斯分布。本文只假设了随机梯度的有界方差等弱条件，建立了一种低样本复杂度以获得关于所提方法收敛的高概率保证。此外，本工作的一个显著方面是发展了一个用于解决近端子问题的子程序，它同时也是一种用于减少方差的新技术。

    arXiv:2402.08992v1 Announce Type: cross Abstract: This paper proposes a stochastic proximal point method to solve a stochastic convex composite optimization problem. High probability results in stochastic optimization typically hinge on restrictive assumptions on the stochastic gradient noise, for example, sub-Gaussian distributions. Assuming only weak conditions such as bounded variance of the stochastic gradient, this paper establishes a low sample complexity to obtain a high probability guarantee on the convergence of the proposed method. Additionally, a notable aspect of this work is the development of a subroutine to solve the proximal subproblem, which also serves as a novel technique for variance reduction.
    
[^84]: 面向对抗性破坏的健壮模型驱动强化学习

    Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption

    [https://arxiv.org/abs/2402.08991](https://arxiv.org/abs/2402.08991)

    本研究通过引入对抗性健壮的乐观MLE（CR-OMLE）算法，解决了模型驱动强化学习中对抗性破坏的挑战，实现了对转移模型的健壮估计。

    

    本研究解决了模型驱动强化学习中对抗性破坏的挑战，其中转移动力学可以被对手破坏。现有研究主要集中在模型无关强化学习的情景下，通常采用健壮的最小二乘回归来进行值函数估计。然而，这些技术不能直接应用于模型驱动的强化学习。在本文中，我们专注于模型驱动的强化学习，并采用最大似然估计（MLE）方法来学习转移模型。我们的工作涵盖了在线和离线两种情况。在在线情况下，我们引入了一种名为对抗性健壮的乐观MLE（CR-OMLE）的算法，它利用基于总变差（TV）的信息比率作为MLE的不确定权重。我们证明了CR-OMLE的遗憾度为$ \tilde {\mathcal {O}}（\sqrt {T} + C）$，其中$ C $表示经过$ T $个回合后的累计破坏水平。

    arXiv:2402.08991v1 Announce Type: cross Abstract: This study tackles the challenges of adversarial corruption in model-based reinforcement learning (RL), where the transition dynamics can be corrupted by an adversary. Existing studies on corruption-robust RL mostly focus on the setting of model-free RL, where robust least-square regression is often employed for value function estimation. However, these techniques cannot be directly applied to model-based RL. In this paper, we focus on model-based RL and take the maximum likelihood estimation (MLE) approach to learn transition model. Our work encompasses both online and offline settings. In the online setting, we introduce an algorithm called corruption-robust optimistic MLE (CR-OMLE), which leverages total-variation (TV)-based information ratios as uncertainty weights for MLE. We prove that CR-OMLE achieves a regret of $\tilde{\mathcal{O}}(\sqrt{T} + C)$, where $C$ denotes the cumulative corruption level after $T$ episodes. We also pro
    
[^85]: MEL: 高维特征选择的高效多任务进化学习

    MEL: Efficient Multi-Task Evolutionary Learning for High-Dimensional Feature Selection

    [https://arxiv.org/abs/2402.08982](https://arxiv.org/abs/2402.08982)

    MEL是一种高效的多任务进化学习方法，通过多任务学习中的信息共享来增强特征选择的能力和效率，解决了高维特征选择中的挑战。

    

    特征选择是数据挖掘中的关键步骤，通过降低数据维度来提高模型性能。然而，收集到的数据的维度越来越高，使得计算复杂度与维度数量呈指数增长。为了解决这个问题，进化计算方法因其简单性和适用性而越来越受欢迎。不幸的是，不同的进化计算方法的多样设计导致其处理不同数据的能力不尽相同，经常无法有效地利用和共享信息。在本文中，我们提出了一种名为PSO-based Multi-task Evolutionary Learning (MEL)的新方法，它利用多任务学习来解决这些挑战。通过在不同特征选择任务之间共享信息，MEL实现了增强的学习能力和效率。我们通过广泛的实验评估了MEL的有效性。

    arXiv:2402.08982v1 Announce Type: cross Abstract: Feature selection is a crucial step in data mining to enhance model performance by reducing data dimensionality. However, the increasing dimensionality of collected data exacerbates the challenge known as the "curse of dimensionality", where computation grows exponentially with the number of dimensions. To tackle this issue, evolutionary computational (EC) approaches have gained popularity due to their simplicity and applicability. Unfortunately, the diverse designs of EC methods result in varying abilities to handle different data, often underutilizing and not sharing information effectively. In this paper, we propose a novel approach called PSO-based Multi-task Evolutionary Learning (MEL) that leverages multi-task learning to address these challenges. By incorporating information sharing between different feature selection tasks, MEL achieves enhanced learning ability and efficiency. We evaluate the effectiveness of MEL through extens
    
[^86]: 学习驱动的灵活作业车间调度用于可扩展智能制造

    Learning-enabled Flexible Job-shop Scheduling for Scalable Smart Manufacturing

    [https://arxiv.org/abs/2402.08979](https://arxiv.org/abs/2402.08979)

    本研究提出了一种基于图的深度强化学习方法，名为异构图调度器（HGS），用于解决灵活作业车间调度问题。该方法利用局部关系知识进行调度，并采用图结构的决策框架，提高了规模泛化能力。

    

    在智能制造系统中，考虑到基于自动导引车辆（AGV）的生产灵活性，具有运输约束的灵活作业车间调度（FJSPT）是优化最大化生产力的解决方案的关键。最近，基于深度强化学习（DRL）的FJSPT方法在规模泛化方面遇到了挑战。这些方法在与其训练集不同规模的环境中表现不佳，导致低质量的解决方案。为了解决这个问题，我们引入了一种新颖的基于图的DRL方法，命名为异构图调度器（HGS）。我们的方法利用在操作、机器和车辆节点之间提取的局部关系知识进行调度，采用图结构的决策框架来降低编码复杂性并增强规模泛化能力。我们进行了基准数据集的性能评估，结果显示该方法在规模泛化上具有良好的表现。

    arXiv:2402.08979v1 Announce Type: cross Abstract: In smart manufacturing systems (SMSs), flexible job-shop scheduling with transportation constraints (FJSPT) is essential to optimize solutions for maximizing productivity, considering production flexibility based on automated guided vehicles (AGVs). Recent developments in deep reinforcement learning (DRL)-based methods for FJSPT have encountered a scale generalization challenge. These methods underperform when applied to environment at scales different from their training set, resulting in low-quality solutions. To address this, we introduce a novel graph-based DRL method, named the Heterogeneous Graph Scheduler (HGS). Our method leverages locally extracted relational knowledge among operations, machines, and vehicle nodes for scheduling, with a graph-structured decision-making framework that reduces encoding complexity and enhances scale generalization. Our performance evaluation, conducted with benchmark datasets, reveals that the pro
    
[^87]: Prismatic:交互式多视角概念股集群分析

    Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks

    [https://arxiv.org/abs/2402.08978](https://arxiv.org/abs/2402.08978)

    Prismatic是一个集成历史数据分析和业务关系知识的交互式多视角概念股集群分析系统，通过多视角集群分析方法，丰富了数据驱动的集群，并提供了细致的业务相关性理解。

    

    arXiv:2402.08978v1 公告类型:跨领域 摘要:金融集群分析使投资者能够发现投资替代品，并避免承担过高的风险。然而，这种分析任务面临许多挑战，如大量的两两比较、时间跨度的动态相关性以及从业务关系知识中得出推论的模糊性。我们提出了Prismatic，一种可视化分析系统，它整合了历史性能的定量分析和业务关系知识的定性分析，以交互方式对相关业务进行集群分析。Prismatic具有三个集群生成过程：动态集群生成、基于知识的集群探索和基于相关性的集群验证。利用多视角集群分析方法，它通过知识驱动的相似性丰富了数据驱动的集群，提供了对业务相关性的细致理解。通过良好协调的可视化视图，Prismatic便于了解企业的关联性。

    arXiv:2402.08978v1 Announce Type: cross Abstract: Financial cluster analysis allows investors to discover investment alternatives and avoid undertaking excessive risks. However, this analytical task faces substantial challenges arising from many pairwise comparisons, the dynamic correlations across time spans, and the ambiguity in deriving implications from business relational knowledge. We propose Prismatic, a visual analytics system that integrates quantitative analysis of historical performance and qualitative analysis of business relational knowledge to cluster correlated businesses interactively. Prismatic features three clustering processes: dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation. Utilizing a multi-view clustering approach, it enriches data-driven clusters with knowledge-driven similarity, providing a nuanced understanding of business correlations. Through well-coordinated visual views, Prismatic facilitates a com
    
[^88]: 基于Transformer的异常检测模型的研究与应用: 文献综述

    Research and application of Transformer based anomaly detection model: A literature review

    [https://arxiv.org/abs/2402.08975](https://arxiv.org/abs/2402.08975)

    本文综述了基于Transformer的异常检测模型在自然语言处理领域的研究与应用，概述了Transformer及其变种在异常检测任务中的工作原理和应用场景，并提出了该领域的未来研究趋势。

    

    Transformer作为一种在自然语言处理领域中应用最广泛的先进神经网络模型之一，在异常检测领域展示了多样的应用。为了激发对基于Transformer的异常检测的研究，本综述从新的角度探索了异常检测的概念，探讨了当前异常检测面临的挑战，并详细介绍了Transformer及其变种在异常检测任务中的工作原理。此外，我们还勾勒了基于Transformer的异常检测模型的各种应用场景，并讨论了所采用的数据集和评估指标。此外，本综述强调了基于Transformer的异常检测研究中的关键挑战，并对该领域的未来研究趋势进行了全面分析。综述中收录了100多个与基于Transformer的异常检测相关的核心参考文献。

    arXiv:2402.08975v1 Announce Type: cross Abstract: Transformer, as one of the most advanced neural network models in Natural Language Processing (NLP), exhibits diverse applications in the field of anomaly detection. To inspire research on Transformer-based anomaly detection, this review offers a fresh perspective on the concept of anomaly detection. We explore the current challenges of anomaly detection and provide detailed insights into the operating principles of Transformer and its variants in anomaly detection tasks. Additionally, we delineate various application scenarios for Transformer-based anomaly detection models and discuss the datasets and evaluation metrics employed. Furthermore, this review highlights the key challenges in Transformer-based anomaly detection research and conducts a comprehensive analysis of future research trends in this domain. The review includes an extensive compilation of over 100 core references related to Transformer-based anomaly detection. To the 
    
[^89]: 从硬件规格预测笔记本电脑的用户体验

    Predicting User Experience on Laptops from Hardware Specifications

    [https://arxiv.org/abs/2402.08964](https://arxiv.org/abs/2402.08964)

    本文研究了如何从笔记本电脑的硬件规格预测实际生活中的用户体验，通过对54台笔记本电脑上的9个用户体验指标进行研究，针对Chromebook上的Web应用进行聚合。通过这项研究，可以更准确地评估设备的整体用户体验。

    

    估计设备的整体用户体验是制造商所面临的常见挑战。目前，设备制造商主要依赖于微基准分数，如Geekbench，这些分数会对特定硬件组件进行压力测试，如CPU或RAM，但并不能很好地捕捉消费者的工作负载。系统设计师通常依赖于特定领域的启发式算法和对原型的大量测试来达到期望的用户体验目标，但制造商的性能声称往往与消费者的实际体验不匹配。我们在笔记本电脑的硬件规格上预测实际生活中的用户体验的初步结果。我们针对在Chromebook上运行的Web应用进行研究，以便对应用和工作负载的体验进行简单而公平的聚合。在54台笔记本电脑上，我们跟踪了常见的终端用户工作负载上的9个用户体验指标：网络浏览、视频播放和音视频通话。我们专注于高负载工作负载的子集。

    arXiv:2402.08964v1 Announce Type: new Abstract: Estimating the overall user experience (UX) on a device is a common challenge faced by manufacturers. Today, device makers primarily rely on microbenchmark scores, such as Geekbench, that stress test specific hardware components, such as CPU or RAM, but do not satisfactorily capture consumer workloads. System designers often rely on domain-specific heuristics and extensive testing of prototypes to reach a desired UX goal, and yet there is often a mismatch between the manufacturers' performance claims and the consumers' experience.   We present our initial results on predicting real-life experience on laptops from their hardware specifications. We target web applications that run on Chromebooks (ChromeOS laptops) for a simple and fair aggregation of experience across applications and workloads. On 54 laptops, we track 9 UX metrics on common end-user workloads: web browsing, video playback and audio/video calls. We focus on a subset of high
    
[^90]: DUEL: 用于自监督类别不平衡学习的主动内存去重方法

    DUEL: Duplicate Elimination on Active Memory for Self-Supervised Class-Imbalanced Learning

    [https://arxiv.org/abs/2402.08963](https://arxiv.org/abs/2402.08963)

    DUEL是一个用于解决类别不平衡问题的自监督学习框架，通过主动内存去重策略来增强数据的多样性，有效缓解了过拟合的问题。英文总结目前暂无法给出。

    

    最近的机器学习算法通常使用经过精心筛选的数据集进行开发，这往往需要大量成本和资源。另一方面，直接使用原始数据往往会导致过拟合，偏向于频繁出现的类别信息。为了高效解决类别不平衡问题，我们提出了一种在自监督预训练过程中进行主动数据过滤的新框架，命名为Duplicate Elimination (DUEL)。该框架结合了受人类工作内存启发的主动内存，并引入了区分性信息，用于衡量内存中数据的多样性，以优化特征提取器和内存。DUEL策略通过替换最常重复的数据样本来增强内存中的区分性信息，从而缓解类别不平衡问题。我们在类别不平衡环境中验证了DUEL框架的有效性，并展示了其优越的性能。

    arXiv:2402.08963v1 Announce Type: cross Abstract: Recent machine learning algorithms have been developed using well-curated datasets, which often require substantial cost and resources. On the other hand, the direct use of raw data often leads to overfitting towards frequently occurring class information. To address class imbalances cost-efficiently, we propose an active data filtering process during self-supervised pre-training in our novel framework, Duplicate Elimination (DUEL). This framework integrates an active memory inspired by human working memory and introduces distinctiveness information, which measures the diversity of the data in the memory, to optimize both the feature extractor and the memory. The DUEL policy, which replaces the most duplicated data with new samples, aims to enhance the distinctiveness information in the memory and thereby mitigate class imbalances. We validate the effectiveness of the DUEL framework in class-imbalanced environments, demonstrating its ro
    
[^91]: 迈向超大规模Transformer的下一级后训练量化

    Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers

    [https://arxiv.org/abs/2402.08958](https://arxiv.org/abs/2402.08958)

    本文提出了一种新颖的后训练量化算法，名为aespa，它在保持完整的注意力得分的同时，通过逐层量化来提高效率，解决了当前后训练量化方案的瓶颈问题。

    

    随着生成AI模型的复杂性增加，后训练量化（PTQ）已成为在移动设备和电视等边缘设备上部署超大规模模型的有希望的解决方案。然而，现有的PTQ方案耗费大量时间和资源，这可能成为实际情况中频繁模型更新和多种超参数调整的瓶颈。作为一种成本效益的替代方案，已经提出了一次性PTQ方案。然而，它们的性能有些受限，因为它们无法考虑到Transformer中注意力模块内部层间的依赖关系，而这是一个非常重要的特性。因此，在本文中，我们提出了一种新颖的PTQ算法，它在精度和效率之间取得了平衡。所提出的算法的关键思想叫做aespa，通过在效率上进行逐层量化，同时考虑到跨层依赖以保留注意力得分。

    arXiv:2402.08958v1 Announce Type: cross Abstract: With the increasing complexity of generative AI models, post-training quantization (PTQ) has emerged as a promising solution for deploying hyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ schemes, however, consume considerable time and resources, which could be a bottleneck in real situations where frequent model updates and multiple hyper-parameter tunings are required. As a cost-effective alternative, one-shot PTQ schemes have been proposed. Still, the performance is somewhat limited because they cannot consider the inter-layer dependency within the attention module, which is a very important feature of Transformers. In this paper, we thus propose a novel PTQ algorithm that balances accuracy and efficiency. The key idea of the proposed algorithm called aespa is to perform quantization layer-wise for efficiency while considering cross-layer dependency to preserve the attention score. Through extensive exp
    
[^92]: MUSTARD：掌握定理和证明数据的统一合成

    MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data

    [https://arxiv.org/abs/2402.08957](https://arxiv.org/abs/2402.08957)

    这项工作介绍了MUSTARD，一种掌握定理和证明数据统一合成的数据生成框架，通过三个阶段的合成，实现了高质量和多样化的问题和推理步骤的生成。

    

    最近，大型语言模型（LLMs）在各种任务中取得了显著进展，包括数学推理和定理证明。由于这两个任务需要严格和形式化的多步推理，它们是探索LLMs推理能力的吸引领域，但仍面临重要挑战。以前的研究如Chain-of-Thought（CoT）揭示了中间步骤指导的有效性。然而，这种逐步注释需要大量的劳动力，导致当前基准测试的训练步骤不足。为了填补这一空白，本研究引入了MUSTARD，一种数据生成框架，可以主导高质量和多样化的定理和证明数据的统一合成。MUSTARD通过三个阶段合成数据：（1）它随机选择几个数学概念作为问题的类别。（2）然后，它使用选定的概念提示生成性语言模型，以获得问题和它们的推理步骤。

    arXiv:2402.08957v1 Announce Type: new Abstract: Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-w
    
[^93]: 使用高斯输入学习子空间稀疏多项式的均场分析

    Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input

    [https://arxiv.org/abs/2402.08948](https://arxiv.org/abs/2402.08948)

    本文研究了使用随机梯度下降和双层神经网络学习子空间稀疏多项式的均场流动。我们提出了合并阶梯属性的无基础推广，并建立了SGD可学习性的必要条件。此外，我们证明了稍强的条件可以保证损失函数的指数衰减至零。

    

    在这项工作中，我们研究了使用随机梯度下降和双层神经网络学习子空间稀疏多项式的均场流动，其中输入分布是标准高斯分布，输出仅依赖于输入在低维子空间上的投影。我们提出了Abbe等人(2022年)中合并阶梯属性的无基础推广，并建立了SGD可学习性的必要条件。此外，我们证明了此条件几乎是充分的，即比必要条件稍强的条件可以保证损失函数的指数衰减至零。

    arXiv:2402.08948v1 Announce Type: new Abstract: In this work, we study the mean-field flow for learning subspace-sparse polynomials using stochastic gradient descent and two-layer neural networks, where the input distribution is standard Gaussian and the output only depends on the projection of the input onto a low-dimensional subspace. We propose a basis-free generalization of the merged-staircase property in Abbe et al. (2022) and establish a necessary condition for the SGD-learnability. In addition, we prove that the condition is almost sufficient, in the sense that a condition slightly stronger than the necessary condition can guarantee the exponential decay of the loss functional to zero.
    
[^94]: 在理解中测量锐度

    Measuring Sharpness in Grokking

    [https://arxiv.org/abs/2402.08946](https://arxiv.org/abs/2402.08946)

    本研究介绍了一种基于适当函数形式的技术来测量理解现象，并研究了在不同设置下训练和验证精度变化的锐度。在两个设置中，相对理解差距和理解锐度之间的趋势相似。

    

    神经网络有时会出现理解现象，即在验证集上达到完美或接近完美的性能，而在相应的训练集上已经获得了相同的性能。在本研讨会论文中，我们引入了一种基于拟合适当的函数形式的强大技术来测量理解，并利用此技术在两个设置下研究训练和验证精度的变化的锐度。第一个设置是由Levi等人（2023）开发的理论框架，其中可以方便地得到闭合形式的表达式。第二个设置是训练一个两层MLP来预测位的奇偶性，通过Miller等人（2023）的隐藏策略引发理解。我们发现，在使用绝对和相对锐度测量时，相对理解差距和理解锐度之间的趋势在这两个设置中是相似的。在此基础上，我们对此进行了思考，为解决该问题取得了进展。

    arXiv:2402.08946v1 Announce Type: new Abstract: Neural networks sometimes exhibit grokking, a phenomenon where perfect or near-perfect performance is achieved on a validation set well after the same performance has been obtained on the corresponding training set. In this workshop paper, we introduce a robust technique for measuring grokking, based on fitting an appropriate functional form. We then use this to investigate the sharpness of transitions in training and validation accuracy under two settings. The first setting is the theoretical framework developed by Levi et al. (2023) where closed form expressions are readily accessible. The second setting is a two-layer MLP trained to predict the parity of bits, with grokking induced by the concealment strategy of Miller et al. (2023). We find that trends between relative grokking gap and grokking sharpness are similar in both settings when using absolute and relative measures of sharpness. Reflecting on this, we make progress toward exp
    
[^95]: 通过合成框架评估时间序列数据的DTW度量

    Evaluating DTW Measures via a Synthesis Framework for Time-Series Data

    [https://arxiv.org/abs/2402.08943](https://arxiv.org/abs/2402.08943)

    本论文提出了一个合成框架，用于评估时间序列数据的DTW度量。这个框架可以模拟比较两个时间序列数据序列之间的变化，有助于解释DTW度量在不同类型的时间序列数据上表现良好的原因。

    

    时间序列数据源自描述特定观察或感兴趣量随时间变化的应用程序。其分析经常涉及对不同时间序列数据序列的比较，这要求对这些序列进行对齐。动态时间规整（DTW）是实现两个时间信号之间最佳对齐的标准方法。已经提出了不同变体的DTW来满足信号对齐或分类的各种需求。然而，缺乏对它们在这些时间序列数据处理任务中性能的综合评估。大多数DTW度量在某些类型的时间序列数据上表现良好，但缺乏明确的解释原因。为了解决这个问题，我们提出了一个合成框架，用于模拟比较两个时间序列数据序列之间的变化。我们的合成框架可以产生一个逼真的初始信号，并以可控的变化进行变形。

    arXiv:2402.08943v1 Announce Type: new Abstract: Time-series data originate from various applications that describe specific observations or quantities of interest over time. Their analysis often involves the comparison across different time-series data sequences, which in turn requires the alignment of these sequences. Dynamic Time Warping (DTW) is the standard approach to achieve an optimal alignment between two temporal signals. Different variations of DTW have been proposed to address various needs for signal alignment or classifications. However, a comprehensive evaluation of their performance in these time-series data processing tasks is lacking. Most DTW measures perform well on certain types of time-series data without a clear explanation of the reason. To address that, we propose a synthesis framework to model the variation between two time-series data sequences for comparison. Our synthesis framework can produce a realistic initial signal and deform it with controllable variat
    
[^96]: 二阶方法用于赌徒优化与控制

    Second Order Methods for Bandit Optimization and Control

    [https://arxiv.org/abs/2402.08929](https://arxiv.org/abs/2402.08929)

    本文提出了一种简单实用的二阶赌徒凸优化算法，并证明了其对于一类称之为$\kappa$-凸的凸函数实现了最优的后期损失界限。该算法在多个应用中表现出高效性能，包括赌徒逻辑回归。

    

    Bandit凸优化(BCO)是一种在不确定性下进行在线决策的通用框架。尽管已经建立了一般凸损失的紧束后期界限，但现有算法在高维数据上具有难以忍受的计算成本。在本文中，我们提出了一种受在线牛顿步骤算法启发的简单实用的BCO算法。我们证明了我们的算法对于一类我们称之为$\kappa$-凸的凸函数实现了最优(从层面上讲)的后期界限。这个类包含了一系列实际相关的损失函数，包括线性、二次和广义线性模型。除了最优的后期损失，这种方法也是一些经过深入研究的应用中已知的最高效的算法，包括赌徒逻辑回归。

    arXiv:2402.08929v1 Announce Type: new Abstract: Bandit convex optimization (BCO) is a general framework for online decision making under uncertainty. While tight regret bounds for general convex losses have been established, existing algorithms achieving these bounds have prohibitive computational costs for high dimensional data.   In this paper, we propose a simple and practical BCO algorithm inspired by the online Newton step algorithm. We show that our algorithm achieves optimal (in terms of horizon) regret bounds for a large class of convex functions that we call $\kappa$-convex. This class contains a wide range of practically relevant loss functions including linear, quadratic, and generalized linear models. In addition to optimal regret, this method is the most efficient known algorithm for several well-studied applications including bandit logistic regression.   Furthermore, we investigate the adaptation of our second-order bandit algorithm to online convex optimization with mem
    
[^97]: MaxMin-RLHF:面向具有多样的人类偏好的大型语言模型的公平对齐

    MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences

    [https://arxiv.org/abs/2402.08925](https://arxiv.org/abs/2402.08925)

    这项工作提出了一种公平对齐大型语言模型与多样的人类偏好的方法，通过学习混合偏好分布并使用MaxMin对齐目标来更好地表示人类偏好。

    

    强化学习从人类反馈中学习(RLHF)通过使用从偏好数据中派生的单一奖励模型来对齐语言模型与人类偏好一致。然而，这种方法忽视了从多个用户收集的数据中固有的人类偏好的丰富多样性。在这项工作中，我们首先推导出了使用单一奖励RLHF进行对齐的不可能性结果，从而凸显了其无法表示多样的人类偏好。为了提供一个公平的解决方案，我们通过期望最大化算法学习了一种混合偏好分布，并提出了一种受社会选择理论中的平等原则启发的MaxMin对齐目标来更好地表示多样的人类偏好。我们阐明了我们提出的方法与分布稳健优化和通用效用RL的联系，从而突显了我们提出的方法的普适性和鲁棒性。

    arXiv:2402.08925v1 Announce Type: cross Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, such an approach overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. To provide an equitable solution to the problem, we learn a mixture of preference distributions via an expectation-maximization algorithm and propose a MaxMin alignment objective for policy learning inspired by the Egalitarian principle in social choice theory to better represent diverse human preferences. We elucidate the connection of our proposed approach to distributionally robust optimization and general utility RL, thereby highlighting the generality and robustness of our proposed
    
[^98]: IMUOptimize: 一种基于数据驱动的方法，用于使用Transformer架构进行人体姿态估计的最佳IMU放置

    IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture

    [https://arxiv.org/abs/2402.08923](https://arxiv.org/abs/2402.08923)

    本文提出了一种基于数据驱动的方法，用于使用Transformer架构进行人体姿态估计的最佳IMU放置。研究结果表明，该方法在姿态重建准确性方面优于传统的双向RNN模型，并且在使用只有6个IMU时，Transformer架构将24个IMU位置获取的数据与双向RNN具有相当的性能。这一优化选择的IMU放置策略结合了Transformer的并行性和性能优势，对基于IMU的姿态估计领域带来了显著的改进。

    

    本文提出了一种使用IMU数据预测人体姿态的新方法，不同于之前的研究DIP-IMU、IMUPoser和TransPose，它们使用最多6个IMU与双向RNN结合。我们引入了两个主要创新：一种基于数据驱动的最佳IMU放置策略和基于Transformer的时序分析模型架构。我们的研究结果表明，我们的方法不仅在传统的以6个IMU为基础的双向RNN模型上表现优越，而且使用只有6个IMU时，与双向RNN相比，Transformer架构显著提高了从24个IMU位置获取的数据的姿态重建准确性。我们优化选择的位置提供了显著的IMU姿态估计领域的改进，结合Transformer的并行性和性能优势。

    arXiv:2402.08923v1 Announce Type: new Abstract: This paper presents a novel approach for predicting human poses using IMU data, diverging from previous studies such as DIP-IMU, IMUPoser, and TransPose, which use up to 6 IMUs in conjunction with bidirectional RNNs. We introduce two main innovations: a data-driven strategy for optimal IMU placement and a transformer-based model architecture for time series analysis. Our findings indicate that our approach not only outperforms traditional 6 IMU-based biRNN models but also that the transformer architecture significantly enhances pose reconstruction from data obtained from 24 IMU locations, with equivalent performance to biRNNs when using only 6 IMUs. The enhanced accuracy provided by our optimally chosen locations, when coupled with the parallelizability and performance of transformers, provides significant improvements to the field of IMU-based pose estimation.
    
[^99]: 镜像影响假设：通过利用前向传递实现高效的数据影响估计

    The Mirrored Influence Hypothesis: Efficient Data Influence Estimation by Harnessing Forward Passes

    [https://arxiv.org/abs/2402.08922](https://arxiv.org/abs/2402.08922)

    本文介绍和探讨了镜像影响假设，突出了训练和测试数据之间影响的相互性。具体而言，它指出，评估训练数据对测试预测的影响可以重新表述为一个等效但相反的问题：评估如果模型在特定的测试样本上进行训练，对训练样本的预测将如何改变。通过实证和理论验证，我们演示了这一假设的正确性。

    

    大规模黑盒模型已经在许多应用中变得无处不在。了解个别训练数据源对这些模型所做预测的影响对于改善其可信性至关重要。当前的影响评估技术涉及计算每个训练点的梯度或在不同子集上重复训练。当扩展到大规模数据集和模型时，这些方法面临明显的计算挑战。

    arXiv:2402.08922v1 Announce Type: new Abstract: Large-scale black-box models have become ubiquitous across numerous applications. Understanding the influence of individual training data sources on predictions made by these models is crucial for improving their trustworthiness. Current influence estimation techniques involve computing gradients for every training point or repeated training on different subsets. These approaches face obvious computational challenges when scaled up to large datasets and models.   In this paper, we introduce and explore the Mirrored Influence Hypothesis, highlighting a reciprocal nature of influence between training and test data. Specifically, it suggests that evaluating the influence of training data on test predictions can be reformulated as an equivalent, yet inverse problem: assessing how the predictions for training samples would be altered if the model were trained on specific test samples. Through both empirical and theoretical validations, we demo
    
[^100]: 可解释的限制复杂性描绘自动编码的概念相似度度量

    Interpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding

    [https://arxiv.org/abs/2402.08919](https://arxiv.org/abs/2402.08919)

    这篇论文提出了一种可解释的限制复杂性描绘自动编码方法，用于衡量图像之间的概念相似度，以便解决基于图像的机器学习中的版权问题。

    

    对于基于图像的机器学习而言，衡量图像之间相似度的程度是一个关键的版权问题。然而，在法律原则中，确定作品之间的相似度需要主观分析，事实裁决者（法官和陪审团）在这些主观判断中可能存在相当大的变异性。在结构上相似的图像可能被认为是不相似的，而完全不同的场景图像可能被认为足够相似以支持剽窃的指控。我们希望定义和计算图像之间的“概念相似度”，即使这些图像没有重复元素或视觉相似组件也能捕捉到高层次的关系。这个想法是使用一个基本的多模态模型生成对视觉数据的“解释”（标题），并在逐渐增加的复杂性水平上进行计算。然后，可以通过需要区分这两个图像的标题长度来衡量相似度。

    arXiv:2402.08919v1 Announce Type: cross Abstract: Quantifying the degree of similarity between images is a key copyright issue for image-based machine learning. In legal doctrine however, determining the degree of similarity between works requires subjective analysis, and fact-finders (judges and juries) can demonstrate considerable variability in these subjective judgement calls. Images that are structurally similar can be deemed dissimilar, whereas images of completely different scenes can be deemed similar enough to support a claim of copying. We seek to define and compute a notion of "conceptual similarity" among images that captures high-level relations even among images that do not share repeated elements or visually similar components. The idea is to use a base multi-modal model to generate "explanations" (captions) of visual data at increasing levels of complexity. Then, similarity can be measured by the length of the caption needed to discriminate between the two images: Two h
    
[^101]: 通过无监督在图上学习多层感知机（MLP）加速图推理

    Graph Inference Acceleration by Learning MLPs on Graphs without Supervision

    [https://arxiv.org/abs/2402.08918](https://arxiv.org/abs/2402.08918)

    该论文提出了一个简单而有效的框架SimMLP，通过在图上无监督学习MLPs，提高了在延迟敏感的应用中的泛化能力。

    

    图神经网络（GNNs）已经在各种图学习任务中展示出了有效性，但是它们对消息传递的依赖限制了它们在延迟敏感的应用中的部署，比如金融欺诈检测。最近的研究探索了从GNNs中提取知识到多层感知机（MLPs）来加速推理。然而，这种任务特定的有监督蒸馏限制了对未见节点的泛化，而在延迟敏感的应用中这种情况很常见。为此，我们提出了一种简单而有效的框架SimMLP，用于在图上无监督学习MLPs，以增强泛化能力。SimMLP利用自监督对齐GNNs和MLPs之间的节点特征和图结构之间的精细和泛化的相关性，并提出了两种策略来减轻平凡解的风险。从理论上讲，

    arXiv:2402.08918v1 Announce Type: cross Abstract: Graph Neural Networks (GNNs) have demonstrated effectiveness in various graph learning tasks, yet their reliance on message-passing constraints their deployment in latency-sensitive applications such as financial fraud detection. Recent works have explored distilling knowledge from GNNs to Multi-Layer Perceptrons (MLPs) to accelerate inference. However, this task-specific supervised distillation limits generalization to unseen nodes, which are prevalent in latency-sensitive applications. To this end, we present \textbf{\textsc{SimMLP}}, a \textbf{\textsc{Sim}}ple yet effective framework for learning \textbf{\textsc{MLP}}s on graphs without supervision, to enhance generalization. \textsc{SimMLP} employs self-supervised alignment between GNNs and MLPs to capture the fine-grained and generalizable correlation between node features and graph structures, and proposes two strategies to alleviate the risk of trivial solutions. Theoretically, w
    
[^102]: 基于学习的脊柱转移骨质量分类方法

    Learning-based Bone Quality Classification Method for Spinal Metastasis

    [https://arxiv.org/abs/2402.08910](https://arxiv.org/abs/2402.08910)

    本文提出了一种基于CT图像的学习型自动脊柱转移骨质量分类方法，并利用多任务学习技术同时考虑横纵脊柱受累分类任务，以提高性能。

    

    脊柱转移是骨转移中最常见的疾病，可能导致疼痛、不稳定性和神经损伤。早期检测脊柱转移对准确分期和最佳治疗至关重要。通常通过计算机断层扫描（CT）来辅助诊断，但这需要经验丰富的放射科医生付出大量努力。本文提出了一种基于CT图像的学习型自动脊柱转移骨质量分类方法。我们同时考虑了横纵脊柱受累分类任务，并利用多任务学习（MTL）技术来提高性能。MTL作为归纳偏置形式，通过在相关任务之间共享表示来帮助模型更好地泛化。基于混合型可被视为增生性和溶骨性的先验知识，我们将骨质量分类任务建模为两个二分类问题。

    arXiv:2402.08910v1 Announce Type: cross Abstract: Spinal metastasis is the most common disease in bone metastasis and may cause pain, instability and neurological injuries. Early detection of spinal metastasis is critical for accurate staging and optimal treatment. The diagnosis is usually facilitated with Computed Tomography (CT) scans, which requires considerable efforts from well-trained radiologists. In this paper, we explore a learning-based automatic bone quality classification method for spinal metastasis based on CT images. We simultaneously take the posterolateral spine involvement classification task into account, and employ multi-task learning (MTL) technique to improve the performance. MTL acts as a form of inductive bias which helps the model generalize better on each task by sharing representations between related tasks. Based on the prior knowledge that the mixed type can be viewed as both blastic and lytic, we model the task of bone quality classification as two binary 
    
[^103]: 解决图上的负迁移问题

    Tackling Negative Transfer on Graphs

    [https://arxiv.org/abs/2402.08907](https://arxiv.org/abs/2402.08907)

    图迁移学习中的负迁移现象尚未得到充分研究，本文发现在图结构数据中负迁移普遍存在，即使源图和目标图在语义上相似。我们提出了一个新的观点，对于语义相似的图，结构差异对子图嵌入的影响较小。

    

    迁移学习旨在通过利用从其他相关任务中学到的知识来提高目标任务上的学习效果。然而，当源任务和目标任务之间关系不密切时，学习性能可能会受到不利影响，这种现象被称为负迁移。本文研究了在图迁移学习中的负迁移问题，这是一个重要但尚未深入研究的领域。我们发现，在图结构数据中，与图像或文本不同，负迁移经常发生，即使源图和目标图在语义上有相似之处。具体来说，我们发现结构差异会大大增强图中节点嵌入之间的差异。为了缓解这个问题，我们带来了一个新的观点：对于语义相似的图，尽管结构差异会导致节点嵌入的分布差异，但它们对子图嵌入的影响可能较小。基于这个观点，我们引入了tw

    arXiv:2402.08907v1 Announce Type: cross Abstract: Transfer learning aims to boost the learning on the target task leveraging knowledge learned from other relevant tasks. However, when the source and target are not closely related, the learning performance may be adversely affected, a phenomenon known as negative transfer. In this paper, we investigate the negative transfer in graph transfer learning, which is important yet underexplored. We reveal that, unlike image or text, negative transfer commonly occurs in graph-structured data, even when source and target graphs share semantic similarities. Specifically, we identify that structural differences significantly amplify the dissimilarities in the node embeddings across graphs. To mitigate this, we bring a new insight: for semantically similar graphs, although structural differences lead to significant distribution shift in node embeddings, their impact on subgraph embeddings could be marginal. Building on this insight, we introduce tw
    
[^104]: 自动编码贝叶斯反向游戏

    Auto-Encoding Bayesian Inverse Games

    [https://arxiv.org/abs/2402.08902](https://arxiv.org/abs/2402.08902)

    本文研究了多个智能体在共同环境中的交互问题，并提出了一种贝叶斯方法来推断游戏参数。采用变分自动编码器（VAE）来构建游戏参数的后验分布，从而解决了现有方法中无法对不确定性进行定量化的问题。

    

    当多个智能体在共同环境中互动时，每个智能体的行动会影响其他智能体未来的决策，而非合作动态游戏自然地捕捉到了这种耦合。然而，在交互式运动规划中，智能体通常没有完整的游戏模型，例如由于其他玩家的目标是未知的。因此，我们考虑了逆向游戏问题，其中游戏的某些属性是先验未知的，必须根据观测结果进行推断。现有的最大似然估计（MLE）方法解决逆向游戏问题时仅提供未知参数的点估计而不对不确定性进行定量化，并且在许多参数值能解释观测行为时表现不佳。为了解决这些限制，我们采用贝叶斯观点构建了游戏参数的后验分布。为了使推断可行，我们使用了一个具有内嵌可微分游戏的变分自动编码器（VAE）。

    arXiv:2402.08902v1 Announce Type: cross Abstract: When multiple agents interact in a common environment, each agent's actions impact others' future decisions, and noncooperative dynamic games naturally capture this coupling. In interactive motion planning, however, agents typically do not have access to a complete model of the game, e.g., due to unknown objectives of other players. Therefore, we consider the inverse game problem, in which some properties of the game are unknown a priori and must be inferred from observations. Existing maximum likelihood estimation (MLE) approaches to solve inverse games provide only point estimates of unknown parameters without quantifying uncertainty, and perform poorly when many parameter values explain the observed behavior. To address these limitations, we take a Bayesian perspective and construct posterior distributions of game parameters. To render inference tractable, we employ a variational autoencoder (VAE) with an embedded differentiable game
    
[^105]: 弱监督下的椎体分割：迭代切片传播

    Weakly Supervised Segmentation of Vertebral Bodies with Iterative Slice-propagation

    [https://arxiv.org/abs/2402.08892](https://arxiv.org/abs/2402.08892)

    本文提出了一种利用弱监督方法进行椎体分割的迭代切片传播方法，仅需四个角标记点作为标签，实现了从CT图像中自动识别椎体的体积分割。

    

    椎体（VB）分割是脊柱疾病医学视觉诊断的重要预处理步骤。然而，大多数先前的研究需要像素/体素级别的强监督，这对于专家来说是昂贵、繁琐和耗时的。在本文中，我们提出了一种弱监督迭代脊柱分割（WISS）方法，仅利用单个矢状切片上的四个角标记点来从CT图像中实现VB的自动体积分割。WISS首先采用自我训练的方式，在有注释的矢状切片上分割VB。这种自我训练的方法在训练集中交替进行训练和细化标签。然后，WISS使用切片传播方法逐层切片地进行整个VB的分割，获得体积分割结果。

    arXiv:2402.08892v1 Announce Type: cross Abstract: Vertebral body (VB) segmentation is an important preliminary step towards medical visual diagnosis for spinal diseases. However, most previous works require pixel/voxel-wise strong supervisions, which is expensive, tedious and time-consuming for experts to annotate. In this paper, we propose a Weakly supervised Iterative Spinal Segmentation (WISS) method leveraging only four corner landmark weak labels on a single sagittal slice to achieve automatic volumetric segmentation from CT images for VBs. WISS first segments VBs on an annotated sagittal slice in an iterative self-training manner. This self-training method alternates between training and refining labels in the training set. Then WISS proceeds to segment the whole VBs slice by slice with a slice-propagation method to obtain volumetric segmentations. We evaluate the performance of WISS on a private spinal metastases CT dataset and the public lumbar CT dataset. On the first dataset,
    
[^106]: 使用机器学习预测太阳活动区的出现

    Predicting the Emergence of Solar Active Regions Using Machine Learning

    [https://arxiv.org/abs/2402.08890](https://arxiv.org/abs/2402.08890)

    该论文通过使用机器学习模型，利用声功率密度演化特征以及多普勒频移和连续强度观测数据预测太阳活动区出现的连续强度变化。

    

    为了创建即将发生的太空天气干扰的早期警报能力，我们选择了一个包含61个新出现的活动区的数据集，这使我们能够识别声功率密度演化中的特征，以预测连续强度的出现。为了进行我们的研究，我们利用了太阳动力学观测卫星（SDO）上的地震声学和磁场成像仪（HMI）观测的多普勒频移和连续强度数据。通过在活动区附近跟踪30.66 x 30.66度的区域，我们可以追踪活动区从预出现状态开始的演化。我们开发了一个机器学习模型来捕捉即将出现的磁通量变化所关联的声功率流密度变化。经过训练的长短期记忆（LSTM）模型能够提前5个小时预测太阳表面某个区域的连续强度值是否会下降。

    arXiv:2402.08890v1 Announce Type: cross Abstract: To create early warning capabilities for upcoming Space Weather disturbances, we have selected a dataset of 61 emerging active regions, which allows us to identify characteristic features in the evolution of acoustic power density to predict continuum intensity emergence. For our study, we have utilized Doppler shift and continuum intensity observations from the Helioseismic and Magnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO). The local tracking of 30.66 x 30.66-degree patches in the vicinity of active regions allowed us to trace the evolution of active regions starting from the pre-emergence state. We have developed a machine learning model to capture the acoustic power flux density variations associated with upcoming magnetic flux emergence. The trained Long Short-Term Memory (LSTM) model is able to predict 5 hours ahead whether, in a given area of the solar surface, continuum intensity values will decrease. The per
    
[^107]: 使用深度学习的光流来进行移动物体提案，用于视频物体分割

    Moving Object Proposals with Deep Learned Optical Flow for Video Object Segmentation

    [https://arxiv.org/abs/2402.08882](https://arxiv.org/abs/2402.08882)

    本研究提出了一种使用深度学习的光流进行视频物体分割的最新方法，并通过微调预训练的光流模型和使用完全卷积神经网络实现了准确和高效的移动物体提案生成。

    

    动态场景理解是计算机视觉领域中最引人注目的研究领域之一。为了增强动态场景理解，广泛应用了基于神经网络的像素级分割。最近的研究将语义信息和运动信息相结合，取得了良好的性能。在本研究中，我们提出了一种最先进的神经网络架构，能够准确高效地获取移动物体提案(MOP)。我们首先训练一个无监督的卷积神经网络(UnFlow)来生成光流估计。然后我们将光流网络的输出渲染到一个完全卷积的SegNet模型中。我们的工作主要贡献有：（1）在全新的DAVIS数据集上对预训练的光流模型进行微调；（2）利用编码器-解码器架构的完全卷积神经网络来进行物体分割。我们使用TensorFlow开发了相关代码，并进行了实验验证。

    arXiv:2402.08882v1 Announce Type: cross Abstract: Dynamic scene understanding is one of the most conspicuous field of interest among computer vision community. In order to enhance dynamic scene understanding, pixel-wise segmentation with neural networks is widely accepted. The latest researches on pixel-wise segmentation combined semantic and motion information and produced good performance. In this work, we propose a state of art architecture of neural networks to accurately and efficiently get the moving object proposals (MOP). We first train an unsupervised convolutional neural network (UnFlow) to generate optical flow estimation. Then we render the output of optical flow net to a fully convolutional SegNet model. The main contribution of our work is (1) Fine-tuning the pretrained optical flow model on the brand new DAVIS Dataset; (2) Leveraging fully convolutional neural networks with Encoder-Decoder architecture to segment objects. We developed the codes with TensorFlow, and execu
    
[^108]: 一个算法公平性和准确性的推理

    Inference for an Algorithmic Fairness-Accuracy Frontier

    [https://arxiv.org/abs/2402.08879](https://arxiv.org/abs/2402.08879)

    本文提供了一个算法公平性和准确性推理的方法。我们提出了一种一致的估计器，并进行了一些检验假设的推理。同时，我们还给出了一个估计器来计算一个给定算法与前沿上最公平点之间的距离，并描述了它的渐近性质。

    

    决策过程越来越依赖于算法的使用。然而，算法的预测能力在人口的不同子群体中经常出现系统性变化。虽然公平性和准确性都是算法的期望特性，但它们常常是相互牺牲的。那么，当面对有限的数据时，一个注重公平性的决策者应该怎么做呢?在本文中，我们为Liang，Lu和Mu（2023）提出的一个理论公平性-准确性前沿提供了一致的估计器，并提出了检验假设的推理方法。这些假设在公平性文献中引起了很多关注，例如(i)全面排除在算法训练中使用一个协变量是否是最优的，(ii)是否存在对现有算法更少歧视性的替代方案。我们还为给定算法与前沿上最公平点之间的距离提供了一个估计器，并描述了它的渐近性质。

    arXiv:2402.08879v1 Announce Type: cross Abstract: Decision-making processes increasingly rely on the use of algorithms. Yet, algorithms' predictive ability frequently exhibit systematic variation across subgroups of the population. While both fairness and accuracy are desirable properties of an algorithm, they often come at the cost of one another. What should a fairness-minded policymaker do then, when confronted with finite data? In this paper, we provide a consistent estimator for a theoretical fairness-accuracy frontier put forward by Liang, Lu and Mu (2023) and propose inference methods to test hypotheses that have received much attention in the fairness literature, such as (i) whether fully excluding a covariate from use in training the algorithm is optimal and (ii) whether there are less discriminatory alternatives to an existing algorithm. We also provide an estimator for the distance between a given algorithm and the fairest point on the frontier, and characterize its asymptot
    
[^109]: 位置论文：拓扑深度学习中的挑战与机遇

    Position Paper: Challenges and Opportunities in Topological Deep Learning

    [https://arxiv.org/abs/2402.08871](https://arxiv.org/abs/2402.08871)

    拓扑深度学习将拓扑特征引入深度学习模型，可作为图表示学习和几何深度学习的补充，给各种机器学习环境提供了自然选择。本文讨论了拓扑深度学习中的开放问题，并提出了未来的研究机会。

    

    拓扑深度学习是一个快速发展的领域，它利用拓扑特征来理解和设计深度学习模型。本文认为，通过融入拓扑概念，拓扑深度学习可以补充图表示学习和几何深度学习，并成为各种机器学习环境下的自然选择。为此，本文讨论了拓扑深度学习中的开放问题，涵盖了从实用益处到理论基础的各个方面。针对每个问题，它概述了潜在的解决方案和未来的研究机会。同时，本文也是对科学界的邀请，希望积极参与拓扑深度学习研究，开发这个新兴领域的潜力。

    arXiv:2402.08871v1 Announce Type: new Abstract: Topological deep learning (TDL) is a rapidly evolving field that uses topological features to understand and design deep learning models. This paper posits that TDL may complement graph representation learning and geometric deep learning by incorporating topological concepts, and can thus provide a natural choice for various machine learning settings. To this end, this paper discusses open problems in TDL, ranging from practical benefits to theoretical foundations. For each problem, it outlines potential solutions and future research opportunities. At the same time, this paper serves as an invitation to the scientific community to actively participate in TDL research to unlock the potential of this emerging field.
    
[^110]: DeepPolar：通过深度学习发明非线性大核心极化码

    DeepPolar: Inventing Nonlinear Large-Kernel Polar Codes via Deep Learning

    [https://arxiv.org/abs/2402.08864](https://arxiv.org/abs/2402.08864)

    本文提出了一种通过深度学习发明的非线性大核心极化码，称为DeepPolar码。与现有的神经码和传统的极化码相比，DeepPolar码通过有效利用更大的核心尺寸提高了可靠性。

    

    极化码以Arikan的极化核心为基础发展而来，代表了编码理论的一个突破，并且已经成为短到中等块长度区域的最先进纠错码。最近的研究表明，通过将Arikan的核心替换为更大的核心，可以进一步提高极化码的可靠性，从而实现更快的极化。然而，对于短到中等块长度区域来说，尚未实现有效利用大核心尺寸的极化码的发展。本文探索了一种新颖的、非线性推广的极化码，它具有扩展的核心尺寸，我们称之为DeepPolar码。我们的结果表明，DeepPolar码有效地利用了更大的核心尺寸的优势，相比现有的神经码和传统的极化码，提供了更高的可靠性。

    arXiv:2402.08864v1 Announce Type: cross Abstract: Polar codes, developed on the foundation of Arikan's polarization kernel, represent a breakthrough in coding theory and have emerged as the state-of-the-art error-correction-code in short-to-medium block length regimes. Importantly, recent research has indicated that the reliability of polar codes can be further enhanced by substituting Arikan's kernel with a larger one, leading to a faster polarization. However, for short-to-medium block length regimes, the development of polar codes that effectively employ large kernel sizes has not yet been realized. In this paper, we explore a novel, non-linear generalization of polar codes with an expanded kernel size, which we call DeepPolar codes. Our results show that DeepPolar codes effectively utilize the benefits of larger kernel size, resulting in enhanced reliability compared to both the existing neural codes and conventional polar codes.
    
[^111]: 关于关系函数和注意力机制的近似方法

    Approximation of relation functions and attention mechanisms

    [https://arxiv.org/abs/2402.08856](https://arxiv.org/abs/2402.08856)

    研究了多层感知机内积的近似性质，揭示了它们作为通用逼近器的能力。得到了对称和非对称关系函数逼近所需神经元数量的界限。

    

    神经网络特征映射的内积在各种机器学习框架中被用于建模输入之间的关系。本研究探讨了神经网络内积的近似性质。研究结果表明，多层感知机自身的内积是对称正定关系函数的通用逼近器。对于非对称关系函数，不同的多层感知机的内积是一个通用逼近器。在两种情况下，都得到了达到给定逼近精度所需的神经元数量的界限。对称情况下，函数类可以被认为是再生核希尔伯特空间中的核函数，而对称情况下函数类可以被认为是再生核巴拿赫空间中的核函数。最后，这些逼近结果被应用于分析...

    arXiv:2402.08856v1 Announce Type: new Abstract: Inner products of neural network feature maps arises in a wide variety of machine learning frameworks as a method of modeling relations between inputs. This work studies the approximation properties of inner products of neural networks. It is shown that the inner product of a multi-layer perceptron with itself is a universal approximator for symmetric positive-definite relation functions. In the case of asymmetric relation functions, it is shown that the inner product of two different multi-layer perceptrons is a universal approximator. In both cases, a bound is obtained on the number of neurons required to achieve a given accuracy of approximation. In the symmetric case, the function class can be identified with kernels of reproducing kernel Hilbert spaces, whereas in the asymmetric case the function class can be identified with kernels of reproducing kernel Banach spaces. Finally, these approximation results are applied to analyzing the
    
[^112]: 混合逆强化学习

    Hybrid Inverse Reinforcement Learning

    [https://arxiv.org/abs/2402.08848](https://arxiv.org/abs/2402.08848)

    本文提出使用混合强化学习的方法来减少逆强化学习中的不必要探索，通过在在线数据和专家数据的混合上进行训练，从而提高学习效率。

    

    对于模仿学习来说，逆强化学习方法是一把双刃剑。一方面，它可以通过较少的专家演示来进行学习，并且能够比行为克隆方法更具鲁棒性地处理错误累积。另一方面，它要求学习者反复解决计算代价高昂的强化学习问题。通常情况下，这种计算往往会浪费在搜索非常不相似于专家策略的策略上。在这项工作中，我们提出使用混合强化学习-在在线数据和专家数据的混合上进行训练-以减少不必要的探索。直观上，专家数据在训练过程中将学习者专注于良好的状态，从而减少了计算强策略所需的探索量。值得注意的是，这种方法不需要将学习者重置到环境中的任意状态，这是以前在高效逆强化学习中的要求。

    arXiv:2402.08848v1 Announce Type: cross Abstract: The inverse reinforcement learning approach to imitation learning is a double-edged sword. On the one hand, it can enable learning from a smaller number of expert demonstrations with more robustness to error compounding than behavioral cloning approaches. On the other hand, it requires that the learner repeatedly solve a computationally expensive reinforcement learning (RL) problem. Often, much of this computation is wasted searching over policies very dissimilar to the expert's. In this work, we propose using hybrid RL -- training on a mixture of online and expert data -- to curtail unnecessary exploration. Intuitively, the expert data focuses the learner on good states during training, which reduces the amount of exploration required to compute a strong policy. Notably, such an approach doesn't need the ability to reset the learner to arbitrary states in the environment, a requirement of prior work in efficient inverse RL. More formal
    
[^113]: 时空桥扩散方法

    Space-Time Bridge-Diffusion

    [https://arxiv.org/abs/2402.08847](https://arxiv.org/abs/2402.08847)

    介绍了一种利用时空混合策略生成独立同分布合成样本的方法，并通过线性和非线性随机过程实现最佳转运，进一步细化通过分数匹配技术训练方法

    

    在这项研究中，我们介绍了一种新的方法，用于从由一组地面真实样本（GT样本）隐式定义的高维实值概率分布中生成独立同分布（i.i.d.）的新合成样本。我们的方法的核心是通过时空混合策略在时间和空间维度上进行扩展。我们的方法基于三个相互关联的随机过程，旨在实现从容易处理的初始概率分布到由GT样本表示的目标分布的最佳转运：（a）包含时空混合的线性过程产生高斯条件概率密度，（b）其桥扩散模拟，条件为初始和最终状态向量，以及（c）通过分数匹配技术进行细化的非线性随机过程。我们训练方法的关键在于精调

    arXiv:2402.08847v1 Announce Type: cross Abstract: In this study, we introduce a novel method for generating new synthetic samples that are independent and identically distributed (i.i.d.) from high-dimensional real-valued probability distributions, as defined implicitly by a set of Ground Truth (GT) samples. Central to our method is the integration of space-time mixing strategies that extend across temporal and spatial dimensions. Our methodology is underpinned by three interrelated stochastic processes designed to enable optimal transport from an easily tractable initial probability distribution to the target distribution represented by the GT samples: (a) linear processes incorporating space-time mixing that yield Gaussian conditional probability densities, (b) their bridge-diffusion analogs that are conditioned to the initial and final state vectors, and (c) nonlinear stochastic processes refined through score-matching techniques. The crux of our training regime involves fine-tuning
    
[^114]: 使用双阶段扰动测试通过必要性和充分性进行特征归因，以进行因果解释

    Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation

    [https://arxiv.org/abs/2402.08845](https://arxiv.org/abs/2402.08845)

    这篇论文提出了一种使用双阶段扰动测试来进行特征归因的方法，通过计算扰动一个特征对预测变化的必要性和充分性作用的概率来衡量特征重要性。该方法能够增强特征归因方法在区分不同特征贡献方面的能力。

    

    我们研究了机器学习中的可解释性问题。为了解决这个问题，特征归因方法（FAMs）通过扰动测试来测量每个特征的贡献，其中在不同扰动下的预测差异进行比较。然而，在特征的预测变化相同的情况下，这种扰动测试可能无法准确区分不同特征的贡献。为了增强FAMs在这种具有挑战性的情况下区分不同特征贡献的能力，我们提出利用扰动一个特征对预测变化起到必要性和充分性作用的概率（PNS）作为特征重要性的度量。我们的方法，利用必要性和充分性进行特征归因（FANS），通过涉及两个阶段（事实性和干预性）的扰动测试计算PNS。在实践中，为了生成反事实样本，我们使用了一个重新...

    arXiv:2402.08845v1 Announce Type: new Abstract: We investigate the problem of explainability in machine learning.To address this problem, Feature Attribution Methods (FAMs) measure the contribution of each feature through a perturbation test, where the difference in prediction is compared under different perturbations.However, such perturbation tests may not accurately distinguish the contributions of different features, when their change in prediction is the same after perturbation.In order to enhance the ability of FAMs to distinguish different features' contributions in this challenging setting, we propose to utilize the probability (PNS) that perturbing a feature is a necessary and sufficient cause for the prediction to change as a measure of feature importance.Our approach, Feature Attribution with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test involving two stages (factual and interventional).In practice, to generate counterfactual samples, we use a re
    
[^115]: 考虑N2O排放和气候变异的智能农业管理

    Intelligent Agricultural Management Considering N$_2$O Emission and Climate Variability with Uncertainties

    [https://arxiv.org/abs/2402.08832](https://arxiv.org/abs/2402.08832)

    本研究利用人工智能和机器学习技术，特别是强化学习和递归神经网络，以及概率性建模方法，开发了一种智能农业管理系统，可以提高作物产量，优化肥料和水的使用，同时减少氮肥流失和温室气体排放，特别关注氧化亚氮（N2O）排放。

    

    本研究探讨了如何利用人工智能（AI），特别是强化学习（RL），在农业中提高作物产量，优化氮肥使用和浇水，并减少硝酸盐流失和温室气体排放，重点关注土壤中的氧化亚氮（N2O）排放。面对气候变化和有限的农业知识，我们使用部分可观察的马尔可夫决策过程（POMDPs）和作物模拟器来模拟AI代理与农业环境的交互。我们采用基于递归神经网络（RNN）的Q网络对智能代理进行深度Q学习训练。同时，我们开发机器学习（ML）模型来预测N2O排放，并将这些预测整合到模拟器中。我们的研究通过概率性机器学习方法处理N2O排放估计的不确定性，并通过随机天气模型处理气候变异，提供一系列排放结果来提高预测可靠性。

    arXiv:2402.08832v1 Announce Type: cross Abstract: This study examines how artificial intelligence (AI), especially Reinforcement Learning (RL), can be used in farming to boost crop yields, fine-tune nitrogen use and watering, and reduce nitrate runoff and greenhouse gases, focusing on Nitrous Oxide (N$_2$O) emissions from soil. Facing climate change and limited agricultural knowledge, we use Partially Observable Markov Decision Processes (POMDPs) with a crop simulator to model AI agents' interactions with farming environments. We apply deep Q-learning with Recurrent Neural Network (RNN)-based Q networks for training agents on optimal actions. Also, we develop Machine Learning (ML) models to predict N$_2$O emissions, integrating these predictions into the simulator. Our research tackles uncertainties in N$_2$O emission estimates with a probabilistic ML approach and climate variability through a stochastic weather model, offering a range of emission outcomes to improve forecast reliabili
    
[^116]: 带有图神经网络的消除歧义节点分类

    Disambiguated Node Classification with Graph Neural Networks

    [https://arxiv.org/abs/2402.08824](https://arxiv.org/abs/2402.08824)

    这篇论文研究了图神经网络中歧义问题对表示学习的影响，并提出了一种新的方法{\method}来消除节点嵌入中的歧义。

    

    图神经网络（GNNs）在学习各种领域的图结构数据方面取得了显著的成功。然而，现有的研究往往忽视了一个关键挑战，即如何学习能够有效推广到代表性不足的图区域的消息传播。这些少数区域常常表现出不规则的同质/异质性模式和多样化的邻域类分布，导致了歧义。在本研究中，我们调查了GNN中的歧义问题，它对表示学习的影响以及对抗这个问题的更丰富的监督信号的发展。我们对GNN进行了细粒度评估，分析了不同图区域中是否存在歧义及其与节点位置的关系。为了消除节点嵌入的歧义，我们提出了一种新的方法{\method}，它利用额外的优化指导来增强表示。

    arXiv:2402.08824v1 Announce Type: new Abstract: Graph Neural Networks (GNNs) have demonstrated significant success in learning from graph-structured data across various domains. Despite their great successful, one critical challenge is often overlooked by existing works, i.e., the learning of message propagation that can generalize effectively to underrepresented graph regions. These minority regions often exhibit irregular homophily/heterophily patterns and diverse neighborhood class distributions, resulting in ambiguity. In this work, we investigate the ambiguity problem within GNNs, its impact on representation learning, and the development of richer supervision signals to fight against this problem. We conduct a fine-grained evaluation of GNN, analyzing the existence of ambiguity in different graph regions and its relation with node positions. To disambiguate node embeddings, we propose a novel method, {\method}, which exploits additional optimization guidance to enhance representa
    
[^117]: RanDumb: 一种质疑持续表示学习效果的简单方法

    RanDumb: A Simple Approach that Questions the Efficacy of Continual Representation Learning

    [https://arxiv.org/abs/2402.08823](https://arxiv.org/abs/2402.08823)

    RanDumb是一种简单的方法，通过固定的随机变换嵌入原始像素并学习简单的线性分类器，质疑了持续表示学习的效果。 实验结果显示，RanDumb在众多持续学习基准测试中明显优于使用深度网络进行持续学习的表示学习。

    

    我们提出了RanDumb来检验持续表示学习的效果。RanDumb将原始像素使用一个固定的随机变换嵌入，这个变换近似了RBF-Kernel，在看到任何数据之前初始化，并学习一个简单的线性分类器。我们提出了一个令人惊讶且一致的发现：在众多持续学习基准测试中，RanDumb在性能上明显优于使用深度网络进行持续学习的表示学习，这表明在这些情景下表示学习的性能较差。RanDumb不存储样本，并在数据上进行单次遍历，一次处理一个样本。它与GDumb相辅相成，在GDumb性能特别差的低样本情况下运行。当将RanDumb扩展到使用预训练模型替换随机变换的情景时，我们得出相同一致的结论。我们的调查结果既令人惊讶又令人担忧，因为表示学习在这些情况下表现糟糕。

    arXiv:2402.08823v1 Announce Type: cross Abstract: We propose RanDumb to examine the efficacy of continual representation learning. RanDumb embeds raw pixels using a fixed random transform which approximates an RBF-Kernel, initialized before seeing any data, and learns a simple linear classifier on top. We present a surprising and consistent finding: RanDumb significantly outperforms the continually learned representations using deep networks across numerous continual learning benchmarks, demonstrating the poor performance of representation learning in these scenarios. RanDumb stores no exemplars and performs a single pass over the data, processing one sample at a time. It complements GDumb, operating in a low-exemplar regime where GDumb has especially poor performance. We reach the same consistent conclusions when RanDumb is extended to scenarios with pretrained models replacing the random transform with pretrained feature extractor. Our investigation is both surprising and alarming as
    
[^118]: 基于梯度优化中的走廊几何

    Corridor Geometry in Gradient-Based Optimization

    [https://arxiv.org/abs/2402.08818](https://arxiv.org/abs/2402.08818)

    本文研究了基于梯度优化中的走廊几何，发现走廊可以提供有关梯度下降优化的洞见，并提出了一种适用于梯度下降的学习率自适应策略CLR，该策略与凸优化中的Polyak步长特例一致。

    

    本文通过将最陡下降的连续曲线，即梯度流的解，变成直线，将损失曲面的区域划分为走廊。我们表明走廊能够提供关于梯度下降优化的洞见，因为走廊正是梯度下降和梯度流遵循相同轨迹且损失线性下降的区域。因此，在走廊内部，不存在因梯度下降和梯度流之间的漂移而导致的隐式正则化效应或训练不稳定性。基于走廊上损失的线性下降，我们设计了一种适用于梯度下降的学习率自适应策略，我们称之为走廊学习率(CLR)。CLR的形式与凸优化上最近发现的Polyak步长特例一致。Polyak步长近期已被证明具有良好的收敛性质。

    arXiv:2402.08818v1 Announce Type: cross Abstract: We characterize regions of a loss surface as corridors when the continuous curves of steepest descent -- the solutions of the gradient flow -- become straight lines. We show that corridors provide insights into gradient-based optimization, since corridors are exactly the regions where gradient descent and the gradient flow follow the same trajectory, while the loss decreases linearly. As a result, inside corridors there are no implicit regularization effects or training instabilities that have been shown to occur due to the drift between gradient descent and the gradient flow. Using the loss linear decrease on corridors, we devise a learning rate adaptation scheme for gradient descent; we call this scheme Corridor Learning Rate (CLR). The CLR formulation coincides with a special case of Polyak step-size, discovered in the context of convex optimization. The Polyak step-size has been shown recently to have also good convergence propertie
    
[^119]: MDP中具有无界每步成本的模型逼近

    Model approximation in MDPs with unbounded per-step cost

    [https://arxiv.org/abs/2402.08813](https://arxiv.org/abs/2402.08813)

    本文考虑了在MDP中具有无界每步成本的模型逼近问题，并通过限定模型变换和距离来提供上界。

    

    本文考虑在只有对近似模型$\hat{\mathcal{M}}$进行访问的情况下，设计无限时间折扣成本MDP$\mathcal{M}$的控制策略的问题。当在原始模型$\mathcal{M}$中使用近似模型的最优策略$\hat{\pi}^{\star}$时，其性能如何？我们通过限定在$\mathcal{M}$中使用$\hat{\pi}^\star$的价值函数与$\mathcal{M}$的最优价值函数之间的加权范数来回答这个问题。然后通过考虑每步成本的仿射变换，扩展我们的结果并获得潜在更紧的上界。我们进一步提供了上界，明确取决于原始模型和近似模型之间的成本函数加权距离和转移核函数加权距离。我们通过实例来说明我们的结果。

    arXiv:2402.08813v1 Announce Type: cross Abstract: We consider the problem of designing a control policy for an infinite-horizon discounted cost Markov decision process $\mathcal{M}$ when we only have access to an approximate model $\hat{\mathcal{M}}$. How well does an optimal policy $\hat{\pi}^{\star}$ of the approximate model perform when used in the original model $\mathcal{M}$? We answer this question by bounding a weighted norm of the difference between the value function of $\hat{\pi}^\star $ when used in $\mathcal{M}$ and the optimal value function of $\mathcal{M}$. We then extend our results and obtain potentially tighter upper bounds by considering affine transformations of the per-step cost. We further provide upper bounds that explicitly depend on the weighted distance between cost functions and weighted distance between transition kernels of the original and approximate models. We present examples to illustrate our results.
    
[^120]: 多尺度光学神经科学的深度和浅层数据科学

    Deep and shallow data science for multi-scale optical neuroscience

    [https://arxiv.org/abs/2402.08811](https://arxiv.org/abs/2402.08811)

    这项研究讨论了多尺度光学神经科学中深度和浅层数据科学的应用。由于不同尺度的数据差异和共性，需要针对每个尺度开发专门的算法，并结合最新的机器学习技术和图信号处理方法来优化数据处理流程。

    

    大脑的光学成像在过去二十年里得到了大幅扩展。新的光学技术、指示物和实验范式现在能够从突触到整个大脑皮层的尺度上进行体内成像。为了处理不同尺度产生的海量数据，连续不断地开发了计算方法来提取与生物学相关的信息。在这个努力中，面临一些领域的挑战（例如微米级数据中的信噪比和分辨率限制），需要专门的算法来解决。这些算法可以利用最先进的机器学习方法，最大程度地学习给定尺度的细节，以优化处理流程。相反，其他方法，如图信号处理，则试图抽象出某些特定尺度的细节，以提供解决神经成像各个尺度通用子问题的解决方案。在这里，我们讨论了限制和问题。

    arXiv:2402.08811v1 Announce Type: cross Abstract: Optical imaging of the brain has expanded dramatically in the past two decades. New optics, indicators, and experimental paradigms are now enabling in-vivo imaging from the synaptic to the cortex-wide scales. To match the resulting flood of data across scales, computational methods are continuously being developed to meet the need of extracting biologically relevant information. In this pursuit, challenges arise in some domains (e.g., SNR and resolution limits in micron-scale data) that require specialized algorithms. These algorithms can, for example, make use of state-of-the-art machine learning to maximally learn the details of a given scale to optimize the processing pipeline. In contrast, other methods, however, such as graph signal processing, seek to abstract away from some of the details that are scale-specific to provide solutions to specific sub-problems common across scales of neuroimaging. Here we discuss limitations and tra
    
[^121]: 在规范有界的无限宽度神经网络中的深度分隔问题

    Depth Separation in Norm-Bounded Infinite-Width Neural Networks

    [https://arxiv.org/abs/2402.08808](https://arxiv.org/abs/2402.08808)

    本研究探讨了在无限宽度神经网络中的深度分隔问题，并发现在特定条件下，用深度为3的ReLU网络学习比用深度为2的ReLU网络学习要更高效。

    

    我们研究了在无限宽度神经网络中的深度分隔问题，其中复杂性由权重的整体二次$\ell_2$范数控制（网络中所有权重的平方和）。之前的深度分隔结果主要关注宽度方面的分隔，这些结果无法说明深度是否决定了在宽度无限制的情况下能否学习出适用于广义上的好泛化性能的网络。在这里，我们研究学习可行性的样本复杂度方面的分隔。具体来说，我们表明有些函数可以通过控制范数的深度3 ReLU网络以多项式复杂度的样本量进行学习，但不能通过控制范数的深度2 ReLU网络（任何范数值）以亚指数复杂度进行学习。同时我们还表明类似的逆向说法是不可能成立的：任何可以通过多项式样本复杂度进行学习的函数，并不能通过亚指数样本复杂度进行学习。

    arXiv:2402.08808v1 Announce Type: new Abstract: We study depth separation in infinite-width neural networks, where complexity is controlled by the overall squared $\ell_2$-norm of the weights (sum of squares of all weights in the network). Whereas previous depth separation results focused on separation in terms of width, such results do not give insight into whether depth determines if it is possible to learn a network that generalizes well even when the network width is unbounded. Here, we study separation in terms of the sample complexity required for learnability. Specifically, we show that there are functions that are learnable with sample complexity polynomial in the input dimension by norm-controlled depth-3 ReLU networks, yet are not learnable with sub-exponential sample complexity by norm-controlled depth-2 ReLU networks (with any value for the norm). We also show that a similar statement in the reverse direction is not possible: any function learnable with polynomial sample co
    
[^122]: 具有时变约束的无投影在线凸优化

    Projection-Free Online Convex Optimization with Time-Varying Constraints

    [https://arxiv.org/abs/2402.08799](https://arxiv.org/abs/2402.08799)

    这个论文介绍了在线凸优化中的投影-free算法，使用线性优化预测访问固定可行集，并满足时变约束。算法在序列上实现了$\tilde{O}(T^{3/4})$的遗憾和$O(T^{7/8})$的约束违反。

    

    我们考虑在线凸优化中的对抗性时变约束设置，其中的操作必须是相对于固定约束集可行的，并且还要平均地满足额外的时变约束。受到固定可行集（硬约束）在投影方面困难的情景的启发，我们考虑只通过线性优化预测（LOO）访问该集合的无投影算法。我们提出了一种算法，在长度为$T$的序列上，并使用总共$T$次LOO调用，保证与损失相关的$\tilde{O}(T^{3/4})$的遗憾和$O(T^{7/8})$的约束违反（忽略所有除$T$之外的量）。特别地，这些界限对于序列的任意区间都成立。我们还提出了一种更高效的算法，它只需要对软约束进行一阶预测访问，并实现了与整个序列相关的类似界限。

    arXiv:2402.08799v1 Announce Type: new Abstract: We consider the setting of online convex optimization with adversarial time-varying constraints in which actions must be feasible w.r.t. a fixed constraint set, and are also required on average to approximately satisfy additional time-varying constraints. Motivated by scenarios in which the fixed feasible set (hard constraint) is difficult to project on, we consider projection-free algorithms that access this set only through a linear optimization oracle (LOO). We present an algorithm that, on a sequence of length $T$ and using overall $T$ calls to the LOO, guarantees $\tilde{O}(T^{3/4})$ regret w.r.t. the losses and $O(T^{7/8})$ constraints violation (ignoring all quantities except for $T$) . In particular, these bounds hold w.r.t. any interval of the sequence. We also present a more efficient algorithm that requires only first-order oracle access to the soft constraints and achieves similar bounds w.r.t. the entire sequence. We extend t
    
[^123]: 用知识增强的生成模型改进分子生成和药物发现

    Improving Molecule Generation and Drug Discovery with a Knowledge-enhanced Generative Model

    [https://arxiv.org/abs/2402.08790](https://arxiv.org/abs/2402.08790)

    本文介绍了一种名为K-DReAM的知识增强生成模型框架，该框架将知识图谱嵌入到生成模型中，以产生具有特定特征的新药候选物。

    

    生成模型在生成分子和新药候选物方面取得了最先进的成果。尽管取得了这些成功，但是生成模型与广泛的生物医学知识之间仍存在重大差距，这些知识通常被系统化在知识图谱中，而这些知识对于信息和增强生成过程的潜力尚未实现。本文提出了一种新方法，通过开发一个名为K-DReAM的知识增强生成模型框架来弥合这个差距。我们开发了一种可扩展的方法，扩展了知识图谱的功能，同时保持语义完整性，并将这些上下文信息结合到一个生成框架中，以指导扩散模型。知识图谱嵌入与我们的生成模型相结合，为产生具有特定特征的新药候选物提供了强大的机制。

    arXiv:2402.08790v1 Announce Type: new Abstract: Recent advancements in generative models have established state-of-the-art benchmarks in generating molecules and novel drug candidates. Despite these successes, a significant gap persists between generative models and the utilization of extensive biomedical knowledge, often systematized within knowledge graphs, whose potential to inform and enhance generative processes has not been realized. In this paper, we present a novel approach that bridges this divide by developing a framework for knowledge-enhanced generative models called K-DReAM. We develop a scalable methodology to extend the functionality of knowledge graphs while preserving semantic integrity and incorporate this contextual information into a generative framework to guide a diffusion-based model. The integration of knowledge graph embeddings with our generative model furnishes a robust mechanism for producing novel drug candidates possessing specific characteristics while en
    
[^124]: 在资源有限的环境中利用咳嗽声音优化胸部X射线的使用

    Leveraging cough sounds to optimize chest x-ray usage in low-resource settings

    [https://arxiv.org/abs/2402.08789](https://arxiv.org/abs/2402.08789)

    通过分析咳嗽声音，我们开发了三种模型，可以在胸部X射线中预测异常结果，从而优化资源使用并提高医疗效率。

    

    胸部X射线在急诊、诊断和呼吸系统疾病管理中是常用的工具。在资源受限的环境下，优化这一资源可以为医疗保健系统和患者节省宝贵的费用，同时改善咨询时间。我们使用印度比哈尔邦普尔尼亚基督医学中心和医院（CMCH）的137名患者的前瞻性收集的数据。每个患者在等待进行放射照相时提供了至少五个咳嗽声。我们使用声学人工智能方法分析了收集到的咳嗽声音。对每个患者的咳嗽声音进行了时间和频谱特征的交叉验证。使用标准统计方法对特征进行了总结。我们开发、测试和比较了三种模型，以预测胸部X射线的异常结果。这三种方法均能在一定程度上区分正常和异常结果。

    arXiv:2402.08789v1 Announce Type: cross Abstract: Chest X-ray is a commonly used tool during triage, diagnosis and management of respiratory diseases. In resource-constricted settings, optimizing this resource can lead to valuable cost savings for the health care system and the patients as well as to and improvement in consult time. We used prospectively-collected data from 137 patients referred for chest X-ray at the Christian Medical Center and Hospital (CMCH) in Purnia, Bihar, India. Each patient provided at least five coughs while awaiting radiography. Collected cough sounds were analyzed using acoustic AI methods. Cross-validation was done on temporal and spectral features on the cough sounds of each patient. Features were summarized using standard statistical approaches. Three models were developed, tested and compared in their capacity to predict an abnormal result in the chest X-ray. All three methods yielded models that could discriminate to some extent between normal and abno
    
[^125]: 重新思考大型语言模型的机器消除技术

    Rethinking Machine Unlearning for Large Language Models

    [https://arxiv.org/abs/2402.08787](https://arxiv.org/abs/2402.08787)

    这篇论文研究了大型语言模型中的机器消除技术，旨在消除不良数据的影响并保持基本知识生成的完整性，为开发安全、可靠和资源高效的生成式人工智能提供基础。

    

    我们研究了大型语言模型（LLM）领域的机器消除技术（MU），称为LLM消除技术。这个研究旨在消除不良数据的影响（例如敏感或非法信息）以及相关模型的能力，同时保持基本的知识生成的完整性，并不影响因果无关的信息。我们设想LLM消除技术将成为LLM生命周期管理中的关键要素，可能成为开发既安全、可靠又资源高效的生成式人工智能的基础，而无需进行完全重训练。我们从概念、方法、评估指标和应用等方面探索了LLM消除技术的研究领域。特别是，我们突出了现有LLM消除技术研究中经常被忽视的方面，例如消除范围、数据模型交互和多方面的有效性评估。

    arXiv:2402.08787v1 Announce Type: cross Abstract: We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We
    
[^126]: 隐式神经表示的随机训练的预处理器

    Preconditioners for the Stochastic Training of Implicit Neural Representations

    [https://arxiv.org/abs/2402.08784](https://arxiv.org/abs/2402.08784)

    本论文提出了一种新的随机训练方法，通过使用曲率感知对角预处理器，在不损失准确性的情况下加速了隐式神经表示的训练过程，适用于多个信号模态。

    

    隐式神经表示已经成为一种强大的技术，用于将复杂连续多维信号编码为神经网络，从而实现计算机视觉、机器人学和几何学等广泛应用。尽管Adam由于其随机的高效性而被广泛应用于训练中，但其训练时间往往较长。为了解决这个问题，我们探索了在加速训练的同时不损失准确性的替代优化技术。传统的二阶优化器如L-BFGS在随机环境中效果不佳，因此不适用于大规模数据集。相反，我们提出了使用曲率感知对角预处理器进行随机训练，展示了它们在图像、形状重建和神经辐射场等各种信号模态中的有效性。

    arXiv:2402.08784v1 Announce Type: cross Abstract: Implicit neural representations have emerged as a powerful technique for encoding complex continuous multidimensional signals as neural networks, enabling a wide range of applications in computer vision, robotics, and geometry. While Adam is commonly used for training due to its stochastic proficiency, it entails lengthy training durations. To address this, we explore alternative optimization techniques for accelerated training without sacrificing accuracy. Traditional second-order optimizers like L-BFGS are suboptimal in stochastic settings, making them unsuitable for large-scale data sets. Instead, we propose stochastic training using curvature-aware diagonal preconditioners, showcasing their effectiveness across various signal modalities such as images, shape reconstruction, and Neural Radiance Fields (NeRF).
    
[^127]: FLASH: 跨同时异质性的联邦学习

    FLASH: Federated Learning Across Simultaneous Heterogeneities

    [https://arxiv.org/abs/2402.08769](https://arxiv.org/abs/2402.08769)

    FLASH是一个跨同时异质性的联邦学习方法，通过综合考虑数据质量、数据分布和延迟等因素，优于其他现有的方法。

    

    联邦学习（FL）的关键前提是在不交换本地数据的情况下，训练机器学习模型在不同的数据所有者（客户端）之间。迄今为止，这个方法面临的一个重要挑战是客户端的异质性，这可能不仅来自数据分布的变化，还来自数据质量以及计算/通信延迟方面的差异。对这些不同且同时存在的异质性的综合视图至关重要；例如，延迟较低的客户端可能具有较差的数据质量，反之亦然。在这项工作中，我们提出了FLASH（跨同时异质性的联邦学习），一个轻量且灵活的客户端选择算法，通过权衡与客户端数据质量、数据分布和延迟相关的统计信息，优于最先进的FL框架。据我们所知，FLASH是第一个能够在统一的方法中处理所有这些异质性的方法。

    arXiv:2402.08769v1 Announce Type: new Abstract: The key premise of federated learning (FL) is to train ML models across a diverse set of data-owners (clients), without exchanging local data. An overarching challenge to this date is client heterogeneity, which may arise not only from variations in data distribution, but also in data quality, as well as compute/communication latency. An integrated view of these diverse and concurrent sources of heterogeneity is critical; for instance, low-latency clients may have poor data quality, and vice versa. In this work, we propose FLASH(Federated Learning Across Simultaneous Heterogeneities), a lightweight and flexible client selection algorithm that outperforms state-of-the-art FL frameworks under extensive sources of heterogeneity, by trading-off the statistical information associated with the client's data quality, data distribution, and latency. FLASH is the first method, to our knowledge, for handling all these heterogeneities in a unified m
    
[^128]: 乳腺癌诊断中的对抗性鲁棒特征学习

    Adversarially Robust Feature Learning for Breast Cancer Diagnosis

    [https://arxiv.org/abs/2402.08768](https://arxiv.org/abs/2402.08768)

    提出了一种乳腺癌诊断中的对抗性鲁棒特征学习方法(ARFL)，通过对抗性训练和特征相关性测量来学习鲁棒特征，提供更准确和安全的乳腺癌诊断。

    

    对抗性数据可能导致深度学习应用的故障。对于标准、干净数据准确的深度学习模型而言，开发对抗性数据具有鲁棒性的模型是至关重要的。在这项研究中，我们提出了一种新颖的对抗性鲁棒特征学习(ARFL)方法，用于乳腺癌诊断的真实世界应用。ARFL利用标准数据和对抗性数据进行对抗性训练，其中将特征相关性测量作为目标函数，以鼓励学习鲁棒特征和抑制虚假特征。为了展示ARFL在乳腺癌诊断中的效果，我们使用两个独立临床收集的乳腺影像数据集建立并评估了诊断模型，总共包括9,548张乳腺X光片图像。我们进行了大量实验证明我们的方法优于几种最先进的方法，并且我们的方法可以提高更安全的乳腺癌诊断。

    arXiv:2402.08768v1 Announce Type: cross Abstract: Adversarial data can lead to malfunction of deep learning applications. It is essential to develop deep learning models that are robust to adversarial data while accurate on standard, clean data. In this study, we proposed a novel adversarially robust feature learning (ARFL) method for a real-world application of breast cancer diagnosis. ARFL facilitates adversarial training using both standard data and adversarial data, where a feature correlation measure is incorporated as an objective function to encourage learning of robust features and restrain spurious features. To show the effects of ARFL in breast cancer diagnosis, we built and evaluated diagnosis models using two independent clinically collected breast imaging datasets, comprising a total of 9,548 mammogram images. We performed extensive experiments showing that our method outperformed several state-of-the-art methods and that our method can enhance safer breast cancer diagnosi
    
[^129]: 贝叶斯战略分类

    Bayesian Strategic Classification

    [https://arxiv.org/abs/2402.08758](https://arxiv.org/abs/2402.08758)

    本研究从学习者部分信息公开的角度研究了战略分类，不再假设代理者完全了解分类器的参数，而是考虑代理者对学习者所使用的分类器有一个共同的分布先验。

    

    在战略分类中，代理者通过修改特征，在一定成本下，希望从学习者的分类器中得到正面分类。学习者通常会小心地修改他们的分类器，以便对代理者的战略行为具有鲁棒性。大多数研究战略分类的论文在考虑代理者操纵时依赖于以下强假设：代理者完全了解学习者所使用的分类器的确切参数。当在真实预测任务中使用复杂或专有的机器学习技术时，这通常是不现实的假设。

    arXiv:2402.08758v1 Announce Type: new Abstract: In strategic classification, agents modify their features, at a cost, to ideally obtain a positive classification from the learner's classifier. The typical response of the learner is to carefully modify their classifier to be robust to such strategic behavior. When reasoning about agent manipulations, most papers that study strategic classification rely on the following strong assumption: agents fully know the exact parameters of the deployed classifier by the learner. This often is an unrealistic assumption when using complex or proprietary machine learning techniques in real-world prediction tasks.   We initiate the study of partial information release by the learner in strategic classification. We move away from the traditional assumption that agents have full knowledge of the classifier. Instead, we consider agents that have a common distributional prior on which classifier the learner is using. The learner in our model can reveal tr
    
[^130]: 面向所有下游代理的换位后悔预测

    Forecasting for Swap Regret for All Downstream Agents

    [https://arxiv.org/abs/2402.08753](https://arxiv.org/abs/2402.08753)

    本研究提出了一种可以在保证缩减换位后悔的同时显著提高速度的预测算法，通过进行非校准预测，但在精心选择的事件集合下保持无偏的预测，能够适用于任何下游代理。

    

    我们研究了如何进行预测，以确保最佳对策的下游代理在任何效用函数下都能保证缩减换位后悔。自从Foster和Vohra（1997）以来，已经知道最佳对策于校准的预测没有换位后悔。然而，目前已知的在顺序对抗环境中保证校准预测的算法，其速度在预测空间维度增加时呈指数级下降。在本文中，我们展示了通过进行非校准预测，但在精心选择的事件集合下保持无偏的预测，我们能够保证任意下游代理缩减换位后悔的速度显著提高，同时保持我们的预测为任何下游代理提供保证的吸引力特性，而无需我们的预测算法的参与。

    arXiv:2402.08753v1 Announce Type: cross Abstract: We study the problem of making predictions so that downstream agents who best respond to them will be guaranteed diminishing swap regret, no matter what their utility functions are. It has been known since Foster and Vohra (1997) that agents who best-respond to calibrated forecasts have no swap regret. Unfortunately, the best known algorithms for guaranteeing calibrated forecasts in sequential adversarial environments do so at rates that degrade exponentially with the dimension of the prediction space. In this work, we show that by making predictions that are not calibrated, but are unbiased subject to a carefully selected collection of events, we can guarantee arbitrary downstream agents diminishing swap regret at rates that substantially improve over the rates that result from calibrated forecasts -- while maintaining the appealing property that our forecasts give guarantees for any downstream agent, without our forecasting algorithm 
    
[^131]: 神经电路的最近邻表示法

    Nearest Neighbor Representations of Neural Circuits

    [https://arxiv.org/abs/2402.08751](https://arxiv.org/abs/2402.08751)

    该论文介绍了一种受大脑结构启发的新的计算方法——最近邻表示法，通过建立与神经网络的对应关系和明确的构造，可以表示深度为2的阈值电路等不同函数。

    

    神经网络成功地捕捉到了人脑在许多任务中的计算能力。受到大脑结构的启发，最近邻（NN）表示法是一种新的计算方法。我们在NN表示法与神经网络之间建立了更牢固的对应关系。虽然已知如何使用NN表示法表示单个神经元，但对于小规模深度神经网络尚无结果。具体地，针对深度为2的阈值电路，我们提供了其NN表示的明确构造，并给出了表示所需位数的明确界限。示例函数包括凸多面体的NN表示（阈值门的AND）、IP2、阈值门的OR以及线性或精确决策列表。

    arXiv:2402.08751v1 Announce Type: cross Abstract: Neural networks successfully capture the computational power of the human brain for many tasks. Similarly inspired by the brain architecture, Nearest Neighbor (NN) representations is a novel approach of computation. We establish a firmer correspondence between NN representations and neural networks. Although it was known how to represent a single neuron using NN representations, there were no results even for small depth neural networks. Specifically, for depth-2 threshold circuits, we provide explicit constructions for their NN representation with an explicit bound on the number of bits to represent it. Example functions include NN representations of convex polytopes (AND of threshold gates), IP2, OR of threshold gates, and linear or exact decision lists.
    
[^132]: 使用深度学习和可解释人工智能自动检测脑MR图像中的运动伪影

    Automated detection of motion artifacts in brain MR images using deep learning and explainable artificial intelligence

    [https://arxiv.org/abs/2402.08749](https://arxiv.org/abs/2402.08749)

    本研究使用深度学习模型和可解释人工智能，实现了对脑MR图像中刚性运动的自动检测。通过对公开数据集的测试，模型展示了良好的性能，能够识别运动伪影并提供解释。该模型的结果在实验数据集上具有高的准确率和召回率，并与图像质量指标呈现强烈的负相关性。

    

    质量评估，包括对图像进行伪影检查，是MRI数据采集过程中的关键步骤，以确保数据质量和下游分析或解释的成功。本研究展示了一种利用深度学习模型检测T1加权脑图像中的刚性运动的方法。我们利用2D CNN进行三类分类，并在公开可获取的回顾性和前瞻性数据集上进行了测试。Grad-CAM热图可以识别失败模式，并对模型结果提供解释。该模型在六个运动模拟回顾性数据集上实现了平均精度和召回率分别为85%和80%的指标。此外，与平均边缘强度相关的图像质量指标相比，该模型在前瞻性数据集上的分类表现显示了强烈的负相关性（-0.84），该指标可以表示运动伪影。该模型是ArtifactID工具的一部分，旨在内联自动检测Gibbs环状伪影、包围伪影等等。

    arXiv:2402.08749v1 Announce Type: cross Abstract: Quality assessment, including inspecting the images for artifacts, is a critical step during MRI data acquisition to ensure data quality and downstream analysis or interpretation success. This study demonstrates a deep learning model to detect rigid motion in T1-weighted brain images. We leveraged a 2D CNN for three-class classification and tested it on publicly available retrospective and prospective datasets. Grad-CAM heatmaps enabled the identification of failure modes and provided an interpretation of the model's results. The model achieved average precision and recall metrics of 85% and 80% on six motion-simulated retrospective datasets. Additionally, the model's classifications on the prospective dataset showed a strong inverse correlation (-0.84) compared to average edge strength, an image quality metric indicative of motion. This model is part of the ArtifactID tool, aimed at inline automatic detection of Gibbs ringing, wrap-aro
    
[^133]: 神经元的最近邻表示

    Nearest Neighbor Representations of Neurons

    [https://arxiv.org/abs/2402.08748](https://arxiv.org/abs/2402.08748)

    这个论文研究了最近邻（NN）表示用于表示神经元的复杂性，并证明了著名的阈值函数可以用多项式规模的锚点和对数分辨率实现。

    

    最近邻（NN）表示是一种受到大脑启发的新兴计算模型。我们研究了使用NN表示来表示神经元（阈值函数）的复杂性。已知，两个锚点（NN计算的点）足以表示阈值函数的NN表示，然而分辨率（锚点条目所需的最大比特数）是$O(n\log{n})$。在这项研究中，我们调查了锚点数和阈值函数的NN表示的分辨率之间的权衡。我们证明了著名的阈值函数EQUALITY，COMPARISON和ODD-MAX-BIT可以通过$n$和$O(\log{n})$的分辨率以多项式规模的锚点表示，这些函数需要2或3个锚点。

    arXiv:2402.08748v1 Announce Type: cross Abstract: The Nearest Neighbor (NN) Representation is an emerging computational model that is inspired by the brain. We study the complexity of representing a neuron (threshold function) using the NN representations. It is known that two anchors (the points to which NN is computed) are sufficient for a NN representation of a threshold function, however, the resolution (the maximum number of bits required for the entries of an anchor) is $O(n\log{n})$. In this work, the trade-off between the number of anchors and the resolution of a NN representation of threshold functions is investigated. We prove that the well-known threshold functions EQUALITY, COMPARISON, and ODD-MAX-BIT, which require 2 or 3 anchors and resolution of $O(n)$, can be represented by polynomially large number of anchors in $n$ and $O(\log{n})$ resolution. We conjecture that for all threshold functions, there are NN representations with polynomially large size and logarithmic reso
    
[^134]: ADS：近似最密子图用于新颖图像的发现

    ADS: Approximate Densest Subgraph for Novel Image Discovery

    [https://arxiv.org/abs/2402.08743](https://arxiv.org/abs/2402.08743)

    本文提出了一种快速且无需训练的算法，用于从大量图像中发现具有独特特征的图像。通过将图像集合形式化为感知距离加权图，并定位最独特图像子集的最密子图，我们解决了一个NP难问题，并通过随机梯度下降高效求解。在合成和真实数据集上的实验证明了算法的优越性能。

    

    随着图像库的增长，尽管有内容检索的功能，我们仍然缺乏一个轻量级工具，能够从大量图像中发现具有独特特征的图像。本文提出了一种快速且无需训练的算法，用于新颖图像的发现。我们的算法的关键在于将一组图像形式化为一个感知距离加权图，其中我们的任务是定位对应于最独特图像子集的K-最密子图。虽然解决这个问题不仅是NP难问题，而且要求完全计算可能巨大的距离矩阵，但我们提出了将其松弛为K-稀疏特征向量问题，通过随机梯度下降（SGD）来高效解决，而无需显式计算距离矩阵。我们在合成和真实数据集上将我们的算法与现有算法进行了比较，表明它具有很高的性能。

    arXiv:2402.08743v1 Announce Type: cross Abstract: The volume of image repositories continues to grow. Despite the availability of content-based addressing, we still lack a lightweight tool that allows us to discover images of distinct characteristics from a large collection. In this paper, we propose a fast and training-free algorithm for novel image discovery. The key of our algorithm is formulating a collection of images as a perceptual distance-weighted graph, within which our task is to locate the K-densest subgraph that corresponds to a subset of the most unique images. While solving this problem is not just NP-hard but also requires a full computation of the potentially huge distance matrix, we propose to relax it into a K-sparse eigenvector problem that we can efficiently solve using stochastic gradient descent (SGD) without explicitly computing the distance matrix. We compare our algorithm against state-of-the-arts on both synthetic and real datasets, showing that it is conside
    
[^135]: 揭示隐藏的能源异常：利用深度学习优化体育设施能源管理

    Unveiling Hidden Energy Anomalies: Harnessing Deep Learning to Optimize Energy Management in Sports Facilities

    [https://arxiv.org/abs/2402.08742](https://arxiv.org/abs/2402.08742)

    利用深度学习和阈值估计技术，提出了一种在体育设施中检测能源异常的方法，旨在优化能源管理并提高运营效率。

    

    体育设施中的异常检测因其潜力在促进节能和优化运营效率方面而受到了重视。在这篇研究文章中，我们探讨了机器学习，特别是深度学习，在体育设施的异常检测中的作用。我们探索了利用深度学习方法解决传统方法的局限性和问题的挑战和前景。我们提出的方法涉及从收集的数据中提取特征。我们使用深度前馈神经网络（DFNN）来进行问题建模，并引入阈值估计技术来有效识别异常。此外，我们还提出了减少错误警报的方法，确保异常检测的可靠性和准确性。为了评估我们的方法的有效性，我们在卡塔尔大学的水中心数据集上进行了实验。

    arXiv:2402.08742v1 Announce Type: cross Abstract: Anomaly detection in sport facilities has gained significant attention due to its potential to promote energy saving and optimizing operational efficiency. In this research article, we investigate the role of machine learning, particularly deep learning, in anomaly detection for sport facilities. We explore the challenges and perspectives of utilizing deep learning methods for this task, aiming to address the drawbacks and limitations of conventional approaches. Our proposed approach involves feature extraction from the data collected in sport facilities. We present a problem formulation using Deep Feedforward Neural Networks (DFNN) and introduce threshold estimation techniques to identify anomalies effectively. Furthermore, we propose methods to reduce false alarms, ensuring the reliability and accuracy of anomaly detection. To evaluate the effectiveness of our approach, we conduct experiments on aquatic center dataset at Qatar Univers
    
[^136]: 专家不作弊: 通过预测对偶来学习未知信息

    Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs

    [https://arxiv.org/abs/2402.08733](https://arxiv.org/abs/2402.08733)

    通过预测对偶的方法，我们提出了一种教导模型逼近真实条件分布并估计模型的不确定性的通用策略。

    

    模型${\widehat{p}}_{\theta}(Y|X)$对于其训练数据$p(Y|X)$的了解程度的准确评估对于避免产生错误或"虚构"的答案或采取不安全的行为非常重要，然而这对于生成模型来说是困难的，因为概率预测不能区分每个响应的噪声（广义不确定性）和对过程的不了解（专题不确定性），而现有的专题不确定性量化技术往往在模型欠拟合时过于自信。我们提出了一种通用策略，可以教导模型同时逼近$p(Y|X)$并估计${\widehat{p}}_{\theta}(Y|X)$与$p(Y|X)$之间的差距：训练模型预测来自真实条件分布的独立响应对，允许它在预测一个响应时观察另一个响应，然后测量它的作弊程度。令人惊讶的是，我们证明了这种策略可以准确估计模型的专题不确定性。

    arXiv:2402.08733v1 Announce Type: new Abstract: Identifying how much a model ${\widehat{p}}_{\theta}(Y|X)$ knows about the stochastic real-world process $p(Y|X)$ it was trained on is important to ensure it avoids producing incorrect or "hallucinated" answers or taking unsafe actions. But this is difficult for generative models because probabilistic predictions do not distinguish between per-response noise (aleatoric uncertainty) and lack of knowledge about the process (epistemic uncertainty), and existing epistemic uncertainty quantification techniques tend to be overconfident when the model underfits. We propose a general strategy for teaching a model to both approximate $p(Y|X)$ and also estimate the remaining gaps between ${\widehat{p}}_{\theta}(Y|X)$ and $p(Y|X)$: train it to predict pairs of independent responses drawn from the true conditional distribution, allow it to "cheat" by observing one response while predicting the other, then measure how much it cheats. Remarkably, we pr
    
[^137]: 训练的量子神经网络是高斯过程

    Trained quantum neural networks are Gaussian processes

    [https://arxiv.org/abs/2402.08726](https://arxiv.org/abs/2402.08726)

    本文研究了训练的量子神经网络，证明了当每个测量的比特仅与少数其他测量的比特相关时，未训练的网络生成的函数的概率分布收敛于高斯过程；通过分析梯度下降训练网络，证明了训练后的网络能够完美拟合训练集，并且训练后生成的函数的概率分布仍然收敛于高斯过程；同时考虑了统计噪声的影响。

    

    我们研究了由参数化的一比特门和固定的两比特门构成的量子神经网络，在无穷宽度的极限下，生成的函数是所有比特上单比特可观测量的期望值的和。首先，我们证明了具有随机初始化参数的未训练网络所生成的函数的概率分布在每个测量比特仅与少数其他测量比特相关的情况下收敛于高斯过程。然后，我们通过梯度下降和平方损失函数对网络的训练进行了分析。我们证明，只要网络没有受到贫瘠高原的影响，训练后的网络可以完美地拟合训练集，训练后所生成的函数的概率分布仍然收敛于高斯过程。最后，我们考虑了统计噪声的影响。

    arXiv:2402.08726v1 Announce Type: cross Abstract: We study quantum neural networks made by parametric one-qubit gates and fixed two-qubit gates in the limit of infinite width, where the generated function is the expectation value of the sum of single-qubit observables over all the qubits. First, we prove that the probability distribution of the function generated by the untrained network with randomly initialized parameters converges in distribution to a Gaussian process whenever each measured qubit is correlated only with few other measured qubits. Then, we analytically characterize the training of the network via gradient descent with square loss on supervised learning problems. We prove that, as long as the network is not affected by barren plateaus, the trained network can perfectly fit the training set and that the probability distribution of the function generated after training still converges in distribution to a Gaussian process. Finally, we consider the statistical noise of t
    
[^138]: PRDP：大规模扩散模型的近端奖励差异预测用于奖励微调

    PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models

    [https://arxiv.org/abs/2402.08714](https://arxiv.org/abs/2402.08714)

    本研究提出了PRDP方法，通过近端奖励差异预测实现了稳定的黑盒奖励微调扩散模型，能够在大规模提示数据集上进行训练，并且具有更好的训练稳定性。

    

    奖励微调已成为将基础模型与下游目标对齐的一种有前途的方法。在语言领域，使用强化学习（RL）来最大化反映人类偏好的奖励已取得了显著的成功。然而，在视觉领域，现有的基于RL的奖励微调方法在大规模训练中存在不稳定性，使它们无法推广到复杂的、未知的提示。在本文中，我们提出了近端奖励差异预测（PRDP），首次在超过100K个提示的大规模提示数据集上实现了稳定的黑盒奖励微调扩散模型。我们的主要创新是奖励差异预测（RDP）目标，该目标与RL目标具有相同的最优解，同时享受更好的训练稳定性。

    arXiv:2402.08714v1 Announce Type: cross Abstract: Reward finetuning has emerged as a promising approach to aligning foundation models with downstream objectives. Remarkable success has been achieved in the language domain by using reinforcement learning (RL) to maximize rewards that reflect human preference. However, in the vision domain, existing RL-based reward finetuning methods are limited by their instability in large-scale training, rendering them incapable of generalizing to complex, unseen prompts. In this paper, we propose Proximal Reward Difference Prediction (PRDP), enabling stable black-box reward finetuning for diffusion models for the first time on large-scale prompt datasets with over 100K prompts. Our key innovation is the Reward Difference Prediction (RDP) objective that has the same optimal solution as the RL objective while enjoying better training stability. Specifically, the RDP objective is a supervised regression objective that tasks the diffusion model with pred
    
[^139]: BECoTTA: 基于输入的在线专家混合模型用于持续的测试时间自适应

    BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation

    [https://arxiv.org/abs/2402.08712](https://arxiv.org/abs/2402.08712)

    BECoTTA是一种基于输入的高效CTTA框架，通过采用MoDE（Mixture-of-Domain Low-rank Experts）模型，它包含领域自适应路由和领域专家协同损失两个核心组件，能够在持续的测试时间中自适应不断变化的领域，同时只需较少的可训练参数。

    

    持续的测试时间自适应（CTTA）要求在适应不断变化的未知领域的同时保留先前学到的知识。然而，尽管CTTA取得了一些进展，但忘记适应权衡和效率仍未得到探索。此外，当前的CTTA场景仅假设存在不相交的情况，而实际世界的领域是无缝变化的。为了解决这些挑战，本文提出了一种名为BECoTTA的基于输入的高效CTTA框架。我们提出了MoDE（Mixture-of-Domain Low-rank Experts），它包含两个核心组件：i）领域自适应路由，通过多个领域路由器有选择地捕捉领域自适应知识，和ii）领域专家协同损失，以增加每个领域和专家之间的依赖性。我们验证了我们的方法优于多个CTTA场景，包括不相交和渐变领域切换，同时只需要大约98％更少的可训练参数。

    arXiv:2402.08712v1 Announce Type: new Abstract: Continual Test Time Adaptation (CTTA) is required to adapt efficiently to continuous unseen domains while retaining previously learned knowledge. However, despite the progress of CTTA, forgetting-adaptation trade-offs and efficiency are still unexplored. Moreover, current CTTA scenarios assume only the disjoint situation, even though real-world domains are seamlessly changed. To tackle these challenges, this paper proposes BECoTTA, an input-dependent yet efficient framework for CTTA. We propose Mixture-of-Domain Low-rank Experts (MoDE) that contains two core components: i) Domain-Adaptive Routing, which aids in selectively capturing the domain-adaptive knowledge with multiple domain routers, and (ii) Domain-Expert Synergy Loss to maximize the dependency between each domain and expert. We validate our method outperforms multiple CTTA scenarios including disjoint and gradual domain shits, while only requiring ~98% fewer trainable parameters
    
[^140]: 《对于数值逼近遍历SDE的分布的Wasserstein距离估计》修正

    Correction to "Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations"

    [https://arxiv.org/abs/2402.08711](https://arxiv.org/abs/2402.08711)

    修正了《对于数值逼近遍历SDE的分布的Wasserstein距离估计》中的错误局部误差估计，提出了一种方法来分析数值离散遍历SDE的Wasserstein-2距离的非渐近保证，并解决了实践中维度依赖性的问题。

    

    本文对San-Serna和Zygalakis的《对于数值逼近遍历SDE的分布的Wasserstein距离估计》中的非渐近保证数值离散分析方法进行了修正。他们分析了UBU积分器，该积分器是二阶强型的，并且每个步骤只需要一次梯度评估，从而得到了理想的非渐近保证，特别是在Wasserstein-2距离中到达离目标分布 $\epsilon > 0$ 的距离仅需 $\mathcal{O}(d^{1/4}\epsilon^{-1/2})$ 步。然而，Sanz-Serna和Zygalakis (2021)中的局部误差估计存在错误，在实践中需要更强的假设才能实现这些复杂度估计。本文解决了理论与实践中观察到的许多应用场景中的维度依赖性。

    arXiv:2402.08711v1 Announce Type: cross Abstract: A method for analyzing non-asymptotic guarantees of numerical discretizations of ergodic SDEs in Wasserstein-2 distance is presented by Sanz-Serna and Zygalakis in ``Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations". They analyze the UBU integrator which is strong order two and only requires one gradient evaluation per step, resulting in desirable non-asymptotic guarantees, in particular $\mathcal{O}(d^{1/4}\epsilon^{-1/2})$ steps to reach a distance of $\epsilon > 0$ in Wasserstein-2 distance away from the target distribution. However, there is a mistake in the local error estimates in Sanz-Serna and Zygalakis (2021), in particular, a stronger assumption is needed to achieve these complexity estimates. This note reconciles the theory with the dimension dependence observed in practice in many applications of interest.
    
[^141]: 通过相似性核实现零-shot分子生成

    Zero Shot Molecular Generation via Similarity Kernels

    [https://arxiv.org/abs/2402.08708](https://arxiv.org/abs/2402.08708)

    通过研究基于能量的扩散模型分数行为，提出了一种新的零-shot分子生成方法SiMGen，该方法使用基于相似性的分子生成技术，能够构建大分子。

    

    生成建模旨在通过直接提出具有理想属性的结构加速新化学物质的发现。最近，基于分数或扩散的生成模型明显优于之前的方法。其成功的关键在于分数与物理力之间的密切关系，可以使用强大的等变神经网络。然而，学到的分数的行为尚不完全理解。在这里，我们通过训练一个基于能量的扩散模型来分析分数以进行分子生成。我们发现在生成过程中，分数一开始类似于恢复性能势，并在最后类似于量子力学力。在这两个端点之间，它表现出能够构建大分子的特殊属性。利用从训练模型中获得的见解，我们提出了基于相似性的分子生成（SiMGen），这是一种新的零-shot分子生成方法。

    arXiv:2402.08708v1 Announce Type: cross Abstract: Generative modelling aims to accelerate the discovery of novel chemicals by directly proposing structures with desirable properties. Recently, score-based, or diffusion, generative models have significantly outperformed previous approaches. Key to their success is the close relationship between the score and physical force, allowing the use of powerful equivariant neural networks. However, the behaviour of the learnt score is not yet well understood. Here, we analyse the score by training an energy-based diffusion model for molecular generation. We find that during the generation the score resembles a restorative potential initially and a quantum-mechanical force at the end. In between the two endpoints, it exhibits special properties that enable the building of large molecules. Using insights from the trained model, we present Similarity-based Molecular Generation (SiMGen), a new method for zero shot molecular generation. SiMGen combin
    
[^142]: 生成式人工智能在全新药物设计中的应用概述：分子和蛋白质生成的新领域

    A Survey of Generative AI for De Novo Drug Design: New Frontiers in Molecule and Protein Generation

    [https://arxiv.org/abs/2402.08703](https://arxiv.org/abs/2402.08703)

    这项综述提出了一个广义方法来驱动AI药物设计，重点关注小分子和蛋白质生成两个主要主题，介绍了各种子任务和应用，并比较了顶级模型的性能。

    

    人工智能（AI）驱动的方法可以极大地改进历来代价高昂的药物设计过程，各种生成模型已经在广泛使用中。特别是针对全新药物设计的生成模型，专注于从零开始创建新的生物化合物，展示了一个有前景的未来方向。该领域的快速发展，加上药物设计过程的固有复杂性，为新研究人员进入创造了一个困难的局面。在这份综述中，我们将全新药物设计分为两个主要主题：小分子和蛋白质生成。在每个主题中，我们确定了各种子任务和应用，重点介绍了重要的数据集、基准和模型架构，并对顶级模型的性能进行了比较。我们采用了广义的方法来驱动AI药物设计，允许在每个子任务中进行各种方法的微观比较和宏观比较。

    arXiv:2402.08703v1 Announce Type: cross Abstract: Artificial intelligence (AI)-driven methods can vastly improve the historically costly drug design process, with various generative models already in widespread use. Generative models for de novo drug design, in particular, focus on the creation of novel biological compounds entirely from scratch, representing a promising future direction. Rapid development in the field, combined with the inherent complexity of the drug design process, creates a difficult landscape for new researchers to enter. In this survey, we organize de novo drug design into two overarching themes: small molecule and protein generation. Within each theme, we identify a variety of subtasks and applications, highlighting important datasets, benchmarks, and model architectures and comparing the performance of top models. We take a broad approach to AI-driven drug design, allowing for both micro-level comparisons of various methods within each subtask and macro-level o
    
[^143]: 具有预测的原始-对偶算法用于在线有界分配和广告拍卖问题

    Primal-Dual Algorithms with Predictions for Online Bounded Allocation and Ad-Auctions Problems

    [https://arxiv.org/abs/2402.08701](https://arxiv.org/abs/2402.08701)

    该论文提出了应用机器学习预测的在线有界分配和在线广告拍卖问题的原始-对偶算法，通过构建根据预测质量调整性能的算法，在预测准确时超过了先前性能界限。

    

    匹配问题在研究界广泛研究，尤其是具有众多应用的广告拍卖。随着机器学习的发展，一个自然的问题是经典算法是否能从机器学习中受益并获得更好的解决方案。即使在匹配问题中获得一小部分性能改进，也可能为研究的使用案例带来显著收益。本文提出了应用机器学习预测的在线有界分配和在线广告拍卖问题的原始-对偶算法。我们构建了基于预测质量的原始-对偶算法，其性能竞争力强。当预测准确时，算法的性能超过了先前的性能界限；而当预测不准确时，算法的性能仍然可接受。

    arXiv:2402.08701v1 Announce Type: cross Abstract: Matching problems have been widely studied in the research community, especially Ad-Auctions with many applications ranging from network design to advertising. Following the various advancements in machine learning, one natural question is whether classical algorithms can benefit from machine learning and obtain better-quality solutions. Even a small percentage of performance improvement in matching problems could result in significant gains for the studied use cases. For example, the network throughput or the revenue of Ad-Auctions can increase remarkably. This paper presents algorithms with machine learning predictions for the Online Bounded Allocation and the Online Ad-Auctions problems. We constructed primal-dual algorithms that achieve competitive performance depending on the quality of the predictions. When the predictions are accurate, the algorithms' performance surpasses previous performance bounds, while when the predictions a
    
[^144]: 无监督评估具有往返正确性的代码LLMs

    Unsupervised Evaluation of Code LLMs with Round-Trip Correctness

    [https://arxiv.org/abs/2402.08699](https://arxiv.org/abs/2402.08699)

    无需人工策划，我们提出了往返正确性（RTC）作为评估代码大型语言模型（LLMs）的替代方法，RTC可以在更广泛的真实世界软件领域对代码进行评估，并且与现有基准具有强相关性。

    

    为了评估代码大型语言模型（LLMs），研究一直依赖于一些小的手动策划的基准，如HumanEval和MBPP，这些基准只代表了真实世界软件领域的一个狭窄部分。在这项工作中，我们引入了往返正确性（RTC）作为一种替代评估方法。RTC允许在更广泛的真实世界软件领域对代码LLM进行评估，而无需昂贵的人工策划。RTC的基本思想是我们可以要求模型做出预测（例如用自然语言描述一些代码），将该预测返回（例如从预测的描述中合成代码），并检查这个往返过程是否导致与原始输入语义等效的代码。我们展示了如何利用RTC来评估代码合成和编辑。我们发现RTC与现有狭窄领域代码合成基准上的模型性能强相关，同时也允许我们扩展到更广阔的领域。

    arXiv:2402.08699v1 Announce Type: cross Abstract: To evaluate code large language models (LLMs), research has relied on a few small manually curated benchmarks, such as HumanEval and MBPP, which represent a narrow part of the real-world software domains. In this work, we introduce round-trip correctness (RTC) as an alternative evaluation method. RTC allows Code LLM evaluation on a broader spectrum of real-world software domains without the need for costly human curation. RTC rests on the idea that we can ask a model to make a prediction (e.g., describe some code using natural language), feed that prediction back (e.g., synthesize code from the predicted description), and check if this round-trip leads to code that is semantically equivalent to the original input. We show how to employ RTC to evaluate code synthesis and editing. We find that RTC strongly correlates with model performance on existing narrow-domain code synthesis benchmarks while allowing us to expand to a much broader se
    
[^145]: AMEND：一种用于长尾轨迹预测的专家混合框架

    AMEND: A Mixture of Experts Framework for Long-tailed Trajectory Prediction

    [https://arxiv.org/abs/2402.08698](https://arxiv.org/abs/2402.08698)

    本研究提出了一种模块化的模型无关的轨迹预测框架，使用专家混合来解决长尾效应问题，提高对于包含挑战性场景的数据的预测性能。

    

    准确地预测行人未来的动向对于智能驾驶系统至关重要。开发这个任务的模型需要包含多样样本的丰富数据集。然而，现有的自然轨迹预测数据集通常对简单样本偏重，并缺乏具有挑战性的场景。这种长尾效应导致预测模型在包含安全关键场景的数据分布的尾部部分表现不佳。以前的方法使用对比学习和类条件超网络等方法解决了长尾问题。然而，这些方法不是模块化的，不能应用于许多机器学习架构。在这项工作中，我们提出了一种模块化的模型无关的轨迹预测框架，利用专门的专家混合。在我们的方法中，每个专家都通过针对特定部分的特殊技能进行训练。

    arXiv:2402.08698v1 Announce Type: cross Abstract: Accurate prediction of pedestrians' future motions is critical for intelligent driving systems. Developing models for this task requires rich datasets containing diverse sets of samples. However, the existing naturalistic trajectory prediction datasets are generally imbalanced in favor of simpler samples and lack challenging scenarios. Such a long-tail effect causes prediction models to underperform on the tail portion of the data distribution containing safety-critical scenarios. Previous methods tackle the long-tail problem using methods such as contrastive learning and class-conditioned hypernetworks. These approaches, however, are not modular and cannot be applied to many machine learning architectures. In this work, we propose a modular model-agnostic framework for trajectory prediction that leverages a specialized mixture of experts. In our approach, each expert is trained with a specialized skill with respect to a particular part
    
[^146]: Game of Trojans: 自适应的对抗者对抗基于输出的特洛伊模型检测器

    Game of Trojans: Adaptive Adversaries Against Output-based Trojaned-Model Detectors

    [https://arxiv.org/abs/2402.08695](https://arxiv.org/abs/2402.08695)

    该论文提出了一种自适应的对抗者，可以重新训练带特洛伊的深度神经网络，并且可以规避最先进的基于输出的特洛伊模型检测器，同时确保高准确率。

    

    我们提出并分析了一种自适应对手，可以重新训练带有特洛伊的深度神经网络，并且了解最先进的基于输出的特洛伊模型检测器。我们展示了这样的对手可以确保（1）对于嵌入触发器和干净样本都具有高准确率，并且（2）可以规避检测。我们的方法基于一个观察：深度神经网络参数的高维度提供足够的自由度来同时实现这些目标。我们还通过允许重新训练来使最先进的检测器适应性，以重新校准其参数，从而模拟特洛伊模型和检测器参数的共同演化。然后我们展示了这种共同进化可以被建模为一个迭代博弈，并证明了得到的（最优）解决方案会使对手成功地实现上述目标。此外，我们为对手提供了一个贪婪算法来选择最少的输入样本进行嵌入。

    arXiv:2402.08695v1 Announce Type: cross Abstract: We propose and analyze an adaptive adversary that can retrain a Trojaned DNN and is also aware of SOTA output-based Trojaned model detectors. We show that such an adversary can ensure (1) high accuracy on both trigger-embedded and clean samples and (2) bypass detection. Our approach is based on an observation that the high dimensionality of the DNN parameters provides sufficient degrees of freedom to simultaneously achieve these objectives. We also enable SOTA detectors to be adaptive by allowing retraining to recalibrate their parameters, thus modeling a co-evolution of parameters of a Trojaned model and detectors. We then show that this co-evolution can be modeled as an iterative game, and prove that the resulting (optimal) solution of this interactive game leads to the adversary successfully achieving the above objectives. In addition, we provide a greedy algorithm for the adversary to select a minimum number of input samples for emb
    
[^147]: 稀疏采样MRI重构的推理阶段降噪

    Inference Stage Denoising for Undersampled MRI Reconstruction

    [https://arxiv.org/abs/2402.08692](https://arxiv.org/abs/2402.08692)

    本文提出了一种在稀疏采样MRI重构过程中利用推理阶段降噪的方法，通过采用条件超参数网络，消除了数据增强的需求，并在各种噪声水平下保持稳健性能。实验表明该方法在测试阶段产生高清晰度重构，并在准确性和图像质量方面达到最佳效果。

    

    磁共振成像（MRI）数据的重构受到深度学习的积极影响。然而，一个关键的挑战是改善对训练和测试数据之间分布变化的泛化能力。大多数方法通过归纳设计或数据增强来解决这个问题。然而，它们可能受到误导性数据（如随机噪声）和推理阶段数据与模型中假设的变化不匹配的影响。在本研究中，通过采用条件超参数网络，我们消除了对增强的需求，在各种高斯噪声水平下仍能保持稳健性能。我们证明了我们的模型能够经受各种输入噪声水平的考验，并在测试阶段产生高清晰度重构。此外，我们提出了一种超参数采样策略，加快了训练的收敛速度。我们提出的方法在所有设置中实现了最高的准确性和图像质量。

    arXiv:2402.08692v1 Announce Type: cross Abstract: Reconstruction of magnetic resonance imaging (MRI) data has been positively affected by deep learning. A key challenge remains: to improve generalisation to distribution shifts between the training and testing data. Most approaches aim to address this via inductive design or data augmentation. However, they can be affected by misleading data, e.g. random noise, and cases where the inference stage data do not match assumptions in the modelled shifts. In this work, by employing a conditional hyperparameter network, we eliminate the need of augmentation, yet maintain robust performance under various levels of Gaussian noise. We demonstrate that our model withstands various input noise levels while producing high-definition reconstructions during the test stage. Moreover, we present a hyperparameter sampling strategy that accelerates the convergence of training. Our proposed method achieves the highest accuracy and image quality in all sett
    
[^148]: 如果图灵和一个人工伙伴一起弹钢琴

    If Turing played piano with an artificial partner

    [https://arxiv.org/abs/2402.08690](https://arxiv.org/abs/2402.08690)

    本研究调查了通过训练生成模型来生成音乐乐谱是否可以实现令人信服的社交体验，而无需优化其同步和延续能力。

    

    音乐是一种天生的社交活动，允许人们分享体验并与彼此产生连接。设计实现与与另一个人一起演奏类似的社交体验的人工伙伴方面的进展很小。实现生成模型的神经网络架构，如大型语言模型，适合生成音乐乐谱。然而，社交性的音乐演奏不仅仅是演奏一个乐谱；它必须与其他音乐家的想法相协调，并正确保持时间。我们探讨了一个问题，即通过训练用于生成音乐乐谱的生成模型是否能够产生令人信服的社交体验，而不必优化其同步和延续能力。该网络是在大量数字乐谱语料库上训练的变分自动编码器，在与人类伙伴进行定时的问答任务中进行了适应。参与者与人类或人工伙伴一起弹奏钢琴。

    arXiv:2402.08690v1 Announce Type: cross Abstract: Music is an inherently social activity that allows people to share experiences and feel connected with one another. There has been little progress in designing artificial partners exhibiting a similar social experience as playing with another person. Neural network architectures that implement generative models, such as large language models, are suited for producing musical scores. Playing music socially, however, involves more than playing a score; it must complement the other musicians' ideas and keep time correctly. We addressed the question of whether a convincing social experience is made possible by a generative model trained to produce musical scores, not necessarily optimized for synchronization and continuation. The network, a variational autoencoder trained on a large corpus of digital scores, was adapted for a timed call-and-response task with a human partner. Participants played piano with a human or artificial partner-in v
    
[^149]: 上下文感知的自动乘客计数数据降噪

    Context-Aware Automated Passenger Counting Data Denoising

    [https://arxiv.org/abs/2402.08688](https://arxiv.org/abs/2402.08688)

    本论文提出了一种上下文感知的自动乘客计数数据降噪算法，通过约束整数线性优化结合票务数据和历史乘车人数数据，有效地提高了数据鲁棒性和分析的准确性。

    

    准确可靠地了解公共交通网络中的乘客数量对于公共交通运营商和公共部门至关重要，以便了解网络的使用情况并优化运输服务。目前有几种估计乘客数量的技术，其中一些是自动化的。其中，自动乘客计数（APC）系统在车辆每个车站检测乘客的进出。然而，这些系统产生的数据常常是嘈杂的，甚至有偏差，导致乘车人数被低估或高估。在这项工作中，我们提出了一种用于APC数据的降噪算法，以提高其鲁棒性并便于分析。所提出的方法是通过一个约束整数线性优化来实现的，利用票务数据和历史乘车人数数据进一步约束和指导优化过程。通过对性能的评估和与其他降噪方法进行比较，证明了该方法的有效性。

    arXiv:2402.08688v1 Announce Type: cross Abstract: A reliable and accurate knowledge of the ridership in public transportation networks is crucial for public transport operators and public authorities to be aware of their network's use and optimize transport offering. Several techniques to estimate ridership exist nowadays, some of them in an automated manner. Among them, Automatic Passenger Counting (APC) systems detect passengers entering and leaving the vehicle at each station of its course. However, data resulting from these systems are often noisy or even biased, resulting in under or overestimation of onboard occupancy. In this work, we propose a denoising algorithm for APC data to improve their robustness and ease their analyzes. The proposed approach consists in a constrained integer linear optimization, taking advantage of ticketing data and historical ridership data to further constrain and guide the optimization. The performances are assessed and compared to other denoising m
    
[^150]: 基于新的依赖测度的模糊圆形时间序列聚类及其在风数据中的应用

    Fuzzy clustering of circular time series based on a new dependence measure with applications to wind data

    [https://arxiv.org/abs/2402.08687](https://arxiv.org/abs/2402.08687)

    本文提出了一种基于新的依赖测度的模糊圆形时间序列聚类方法，能够有效地将取值在单位圆上的时间序列进行聚类分析，并考虑了序列的动态特性。

    

    时间序列聚类是一项机器学习任务，广泛应用于许多领域。尽管大多数方法都集中在取值于实数线上的时间序列，但很少有方法考虑到取值在单位圆上的时间序列，尽管后者在许多应用中经常出现。本文提出了对圆形时间序列进行聚类的问题。为此，引入了一种圆形序列之间的距离，用于构建聚类过程。该度量依赖于一种考虑圆弧的新的序列依赖测度，从而利用了序列范围固有的方向特性。由于序列的动态特性可能随时间变化，我们采用了模糊方法，使该过程能够将每个序列分配到具有不同成员度的多个聚类中。得到的聚类算法能够将从相似随机生成的序列分组在一起。

    arXiv:2402.08687v1 Announce Type: cross Abstract: Time series clustering is an essential machine learning task with applications in many disciplines. While the majority of the methods focus on time series taking values on the real line, very few works consider time series defined on the unit circle, although the latter objects frequently arise in many applications. In this paper, the problem of clustering circular time series is addressed. To this aim, a distance between circular series is introduced and used to construct a clustering procedure. The metric relies on a new measure of serial dependence considering circular arcs, thus taking advantage of the directional character inherent to the series range. Since the dynamics of the series may vary over the time, we adopt a fuzzy approach, which enables the procedure to locate each series into several clusters with different membership degrees. The resulting clustering algorithm is able to group series generated from similar stochastic 
    
[^151]: 通过提示的上下文向量检测钓鱼网络攻击

    Prompted Contextual Vectors for Spear-Phishing Detection

    [https://arxiv.org/abs/2402.08309](https://arxiv.org/abs/2402.08309)

    通过新的文档向量化方法，我们的方法使用大型语言模型来检测钓鱼网络攻击的电子邮件，并在实验证明具有高效性能。

    

    钓鱼网络攻击是一个重大的安全挑战，而大型语言模型（LLMs）通过生成令人信服的电子邮件并方便目标侦察来升级了威胁。为了解决这个问题，我们提出了一种基于新颖文档向量化方法的检测方法，该方法利用LLMs的集合来创建表示向量。通过提示LLMs来推理和回答人工制定的问题，我们量化电子邮件内容中常见说服原则的存在，为下游监督机器学习模型生成提示上下文文档向量。我们使用一个专有系统生成的独特数据集来评估我们的方法，该系统自动化目标侦察和钓鱼电子邮件的创建。我们的方法在仅包含传统钓鱼和良性电子邮件的训练集中实现了91%的F1得分，其中关键贡献包括一种创新的文档向量化方法。

    Spear-phishing attacks present a significant security challenge, with large language models (LLMs) escalating the threat by generating convincing emails and facilitating target reconnaissance. To address this, we propose a detection approach based on a novel document vectorization method that utilizes an ensemble of LLMs to create representation vectors. By prompting LLMs to reason and respond to human-crafted questions, we quantify the presence of common persuasion principles in the email's content, producing prompted contextual document vectors for a downstream supervised machine learning model. We evaluate our method using a unique dataset generated by a proprietary system that automates target reconnaissance and spear-phishing email creation. Our method achieves a 91% F1 score in identifying LLM-generated spear-phishing emails, with the training set comprising only traditional phishing and benign emails. Key contributions include an innovative document vectorization method utilizin
    
[^152]: ChatCell: 利用自然语言促进单细胞分析

    ChatCell: Facilitating Single-Cell Analysis with Natural Language

    [https://arxiv.org/abs/2402.08303](https://arxiv.org/abs/2402.08303)

    ChatCell是一个利用自然语言促进单细胞分析的工具，通过词汇适应和统一序列生成，它具备深厚的专业知识和适应各种分析任务的能力。

    

    随着大型语言模型(LLMs)的快速发展，它们在科学中的影响日益突出。LLMs在任务泛化和自由对话方面的新兴能力可以极大地推进化学和生物学等领域。然而，单细胞生物学这个构成生物体基础构件的领域仍面临一些挑战。当前方法在知识门槛和可扩展性方面存在限制，阻碍了LLMs在掌握单细胞数据方面的充分利用，影响了直接可访问和快速迭代的能力。为此，我们引入了ChatCell，通过利用词汇适应和统一序列生成，它在单细胞生物学领域获得了深厚的专业知识和适应各种分析任务的能力，标志着一种范式转变。

    As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
    
[^153]: 研究GNN的超分布推广：从架构角度的视角

    Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective

    [https://arxiv.org/abs/2402.08228](https://arxiv.org/abs/2402.08228)

    这项研究从架构的角度全面调查了图的超分布推广，揭示了图自我注意机制和其他常见构建模块在超分布问题上的影响。

    

    图神经网络（GNN）在测试数据来自于训练数据相同分布的假设下表现出了出色的性能。然而，在真实场景中，这个假设可能并不总是成立。因此，在图的上下文中，对超分布（OOD）问题的探索日益受到关注。大部分现有的研究主要集中在改进图的OOD推广的两个“模型无关”角度上：数据驱动方法和策略学习。然而，对于已知的GNN模型架构对图的OOD推广的影响的研究相对较少，这与现有的研究相互独立。在这项工作中，我们从架构的角度首次全面调查了图的OOD推广，并对现代GNN的常见构建模块进行了考察。通过大量实验，我们揭示了图自我注意机制和...

    Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs. Most existing efforts have primarily concentrated on improving graph OOD generalization from two \textbf{model-agnostic} perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known \textbf{GNN model architectures} on graph OOD generalization, which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs. Through extensive experiments, we reveal that both the graph self-attention mechanism an
    
[^154]: 《关于循环模型在长序列中的复兴：在Transformer时代的调研和研究机会》

    On the Resurgence of Recurrent Models for Long Sequences: Survey and Research Opportunities in the Transformer Era

    [https://arxiv.org/abs/2402.08132](https://arxiv.org/abs/2402.08132)

    这项调研总结了在处理长序列数据方面，循环模型的复兴和与Transformer模型相结合的新型神经模型的发展，以及深度空间状态模型作为时间函数逼近的方法的出现。

    

    机器学习领域长期以来的一个挑战是开发可以处理和学习非常长的数据序列的模型。基于Transformer的网络（例如大型语言模型）的出色结果推动了并行注意力的概念，将经典的顺序处理的循环模型的作用掩盖起来。然而，过去几年中，一些研究人员对自注意力的二次复杂度表示关注，提出了一系列兼顾Transformer和循环网络两个世界优势的新型神经模型。同时，深度空间状态模型作为时间函数逼近的强大方法出现，从而为从序列数据中学习开辟了新的视角，许多领域的研究者对此感兴趣并利用它来实现一类特殊的（线性）循环神经网络。本调研旨在概述这些趋势。

    A longstanding challenge for the Machine Learning community is the one of developing models that are capable of processing and learning from very long sequences of data. The outstanding results of Transformers-based networks (e.g., Large Language Models) promotes the idea of parallel attention as the key to succeed in such a challenge, obfuscating the role of classic sequential processing of Recurrent Models. However, in the last few years, researchers who were concerned by the quadratic complexity of self-attention have been proposing a novel wave of neural models, which gets the best from the two worlds, i.e., Transformers and Recurrent Nets. Meanwhile, Deep Space-State Models emerged as robust approaches to function approximation over time, thus opening a new perspective in learning from sequential data, followed by many people in the field and exploited to implement a special class of (linear) Recurrent Neural Networks. This survey is aimed at providing an overview of these trends 
    
[^155]: 离散扩散模型的收敛分析：通过均匀化的确切实现

    Convergence Analysis of Discrete Diffusion Model: Exact Implementation through Uniformization

    [https://arxiv.org/abs/2402.08095](https://arxiv.org/abs/2402.08095)

    本文通过均匀化的方式确切实现了离散扩散模型，研究了其理论性质，并提供了关于采样的总变差距离和KL散度保证。这一方法在建模离散数据方面具有重要的应用价值。

    

    扩散模型在数据生成任务中取得了巨大的经验成功。最近，一些努力已经被做出来，将扩散模型的框架适应到离散状态空间，为建模本质上是离散数据（如语言和图形）提供了一种更自然的方法。这通过将前向噪声过程和相应的逆过程都构建为连续时间马尔可夫链（CTMC）来实现。在本文中，我们研究了离散扩散模型的理论性质。具体而言，我们介绍了一种利用连续马尔可夫链均匀化的算法，在随机时间点上实现转移。在关于离散得分函数学习的合理假设下，我们得到了从超立方体上的任何分布进行采样所需的总变差距离和KL散度保证。我们的结果与在$\mathbb{R}^d$中的扩散模型的最新成就相一致，并进一步强调了d的优势。

    Diffusion models have achieved huge empirical success in data generation tasks. Recently, some efforts have been made to adapt the framework of diffusion models to discrete state space, providing a more natural approach for modeling intrinsically discrete data, such as language and graphs. This is achieved by formulating both the forward noising process and the corresponding reversed process as Continuous Time Markov Chains (CTMCs). In this paper, we investigate the theoretical properties of the discrete diffusion model. Specifically, we introduce an algorithm leveraging the uniformization of continuous Markov chains, implementing transitions on random time points. Under reasonable assumptions on the learning of the discrete score function, we derive Total Variation distance and KL divergence guarantees for sampling from any distribution on a hypercube. Our results align with state-of-the-art achievements for diffusion models in $\mathbb{R}^d$ and further underscore the advantages of d
    
[^156]: 基于分数的生成模型在学习一个子高斯概率分布族中突破了维数灾难

    Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian probability distributions

    [https://arxiv.org/abs/2402.08082](https://arxiv.org/abs/2402.08082)

    基于分数的生成模型在学习一个子高斯概率分布族中突破了维数灾难，通过分数匹配生成的分布以维度无关的速率逼近目标分布的总变差。

    

    尽管基于分数的生成模型（SGMs）在巨大的图像生成任务中取得了显著的成功，但它们的数学基础仍然有限。在本文中，我们分析了SGMs在学习一个子高斯概率分布族中的近似和泛化。我们引入了一种关于概率分布复杂性的概念，即相对密度与标准高斯测度的相对密度。我们证明，如果对数相对密度可以通过神经网络进行局部逼近，并且网络参数可以适当地受限，那么通过经验分数匹配生成的分布以维度无关的速率逼近目标分布的总变差。我们通过示例说明了我们的理论，其中包括某些高斯混合分布。我们证明的一个关键点是推导出与正向过程相关的真实得分函数的维度无关的深度神经网络逼近速率。

    While score-based generative models (SGMs) have achieved remarkable success in enormous image generation tasks, their mathematical foundations are still limited. In this paper, we analyze the approximation and generalization of SGMs in learning a family of sub-Gaussian probability distributions. We introduce a notion of complexity for probability distributions in terms of their relative density with respect to the standard Gaussian measure. We prove that if the log-relative density can be locally approximated by a neural network whose parameters can be suitably bounded, then the distribution generated by empirical score matching approximates the target distribution in total variation with a dimension-independent rate. We illustrate our theory through examples, which include certain mixtures of Gaussians. An essential ingredient of our proof is to derive a dimension-free deep neural network approximation rate for the true score function associated with the forward process, which is inte
    
[^157]: 实现智能体、人类和环境之间的统一对齐

    Towards Unified Alignment Between Agents, Humans, and Environment

    [https://arxiv.org/abs/2402.07744](https://arxiv.org/abs/2402.07744)

    本文介绍了统一对齐原则 ($\mathbf{UA}^2$)，旨在实现智能体与人类意图、环境动态和自我约束的统一对齐，提出了引入实际特性进行概念验证研究的方法。

    

    基于基础模型的快速进展导致了自主智能体的繁荣，这些智能体利用基础模型的通用能力进行推理、决策和环境交互。然而，当在复杂、现实的环境中运行时，智能体的效能仍然有限。在本研究中，我们引入了统一对齐原则，即同时对齐智能体与人类意图、环境动态和自我约束（如货币预算限制）。从统一对齐 ($\mathbf{UA}^2$) 的视角出发，我们回顾了当前智能体研究的现状，并指出了现有智能体基准和方法候选中被忽视的因素。我们还通过为WebShop引入实际特性进行了概念验证研究，包括使用用户配置文件来展示意图、个性化重新排名以应对复杂的环境动态和运行时成本统计。

    The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of $\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents ($\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of $\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost stat
    
[^158]: 在线顺序决策中的未知延迟问题

    Online Sequential Decision-Making with Unknown Delays

    [https://arxiv.org/abs/2402.07703](https://arxiv.org/abs/2402.07703)

    本文提出了在在线顺序决策中处理未知延迟问题的三个延迟算法族，并提供了相应的遗憾界限。

    

    在在线顺序决策领域，我们利用在线凸优化（OCO）框架解决了具有延迟的问题，其中决策的反馈可能以未知延迟到达。与之前仅限于欧几里得范数和梯度信息的研究不同，我们提出了三个基于近似解的延迟算法族，处理不同类型的接收反馈。我们提出的算法是多功能且适用于通用范数。具体地，我们引入了一系列针对具有完整损失函数信息反馈的延迟规范化领导算法族，一系列针对具有梯度信息反馈的延迟镜像下降算法族，以及一系列针对相应决策点损失函数梯度值信息反馈的简化延迟镜像下降算法族。对于每种类型的算法，我们提供了相应的遗憾界限。

    In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay. Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback. Our proposed algorithms are versatile and applicable to universal norms. Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points. For each type of algorithm, we provide corresponding regret bounds under cases of 
    
[^159]: 使用LoCo和M2-BERT进行基准测试和构建长上下文检索模型

    Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT

    [https://arxiv.org/abs/2402.07440](https://arxiv.org/abs/2402.07440)

    该论文介绍了LoCoV1，一个用于评估长上下文检索性能的新型基准测试，并提出了M2-BERT检索编码器，用于处理长上下文检索，解决了如何评估性能、预训练语言模型以及如何进行微调的挑战。

    

    检索管道是许多机器学习系统中的重要组成部分，在文档很长（例如10K个标记或更多）且需要在整个文本中合成信息来确定相关文档的领域中表现不佳。开发适用于这些领域的长上下文检索编码器面临三个挑战：（1）如何评估长上下文检索性能，（2）如何预训练基本语言模型以表示短上下文（对应查询）和长上下文（对应文档），以及（3）如何根据GPU内存限制下的批量大小限制对该模型进行微调。为了解决这些挑战，我们首先介绍了LoCoV1，这是一个新颖的12个任务基准测试，用于测量在不可分块或不有效的情况下的长上下文检索。接下来，我们提出了M2-BERT检索编码器，这是一个80M参数状态空间编码器模型，采用Monarch Mixer架构构建，能够进行可扩展的检索。

    Retrieval pipelines-an integral component of many machine learning systems-perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text. Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to fine-tune this model for retrieval under the batch size limitations imposed by GPU memory constraints. To address these challenges, we first introduce LoCoV1, a novel 12 task benchmark constructed to measure long-context retrieval where chunking is not possible or not effective. We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scali
    
[^160]: 监控马尔可夫决策过程

    Monitored Markov Decision Processes

    [https://arxiv.org/abs/2402.06819](https://arxiv.org/abs/2402.06819)

    这篇论文介绍了一种新颖且通用的强化学习框架——监控马尔可夫决策过程(Monitored MDPs)。在这个框架中，代理不能始终观察到奖励，提出了算法来解决这个新颖的场景。

    

    在强化学习中，代理通过与环境的交互和接收反馈（数值奖励）来学习执行任务。然而，奖励始终可观察的假设在现实世界的问题中通常不适用。例如，代理可能需要要求人类监督其行为或激活监控系统以接收反馈。甚至可能存在奖励在可观察之前一段时间或在不再给予奖励之后的时间。换句话说，有些情况下，环境根据代理的行为生成奖励，但代理无法观察到这些奖励。在本文中，我们正式定义了一个新颖但通用的强化学习框架 - 监控马尔可夫决策过程(Monitored MDPs)，在此框架中代理并非总是能够观察到奖励。我们讨论了这种设置可能带来的理论和实践上的后果，展示了即使在玩具环境中也会出现的挑战，并提出了算法来开始解决这个新颖的场景。

    In reinforcement learning (RL), an agent learns to perform a task by interacting with an environment and receiving feedback (a numerical reward) for its actions. However, the assumption that rewards are always observable is often not applicable in real-world problems. For example, the agent may need to ask a human to supervise its actions or activate a monitoring system to receive feedback. There may even be a period of time before rewards become observable, or a period of time after which rewards are no longer given. In other words, there are cases where the environment generates rewards in response to the agent's actions but the agent cannot observe them. In this paper, we formalize a novel but general RL framework - Monitored MDPs - where the agent cannot always observe rewards. We discuss the theoretical and practical consequences of this setting, show challenges raised even in toy environments, and propose algorithms to begin to tackle this novel setting. This paper introduces a p
    
[^161]: Premier-TACO: 通过时间驱动的对比损失进行多任务表示预训练

    Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

    [https://arxiv.org/abs/2402.06187](https://arxiv.org/abs/2402.06187)

    Premier-TACO是一种多任务特征表示学习方法，通过预训练通用特征表示，并引入负例抽样策略来提高时序行动对比学习的计算效率，从而显著增强了对新颖动作的少样本模仿学习的效果。

    

    我们提出了Premier-TACO，这是一种多任务特征表示学习方法，旨在提高顺序决策任务中少样本策略学习的效率。Premier-TACO利用一部分多任务离线数据集进行预训练通用特征表示，该特征表示捕捉了关键的环境动力学，并使用最少的专家演示进行微调。它通过引入一种新的负例抽样策略推动了时序行动对比学习（TACO）目标的发展，TACO在视觉控制任务中具有最先进的结果。这种策略在显著提高TACO的计算效率方面非常重要，使大规模多任务离线预训练成为可能。我们在包括Deepmind Control Suite、MetaWorld和LIBERO在内的各种连续控制基准测试中进行了广泛的实证评估，证明了Premier-TACO在预训练视觉表示方面的有效性，显著增强了对新颖动作的少样本模仿学习。

    We present Premier-TACO, a multitask feature representation learning approach designed to improve few-shot policy learning efficiency in sequential decision-making tasks. Premier-TACO leverages a subset of multitask offline datasets for pretraining a general feature representation, which captures critical environmental dynamics and is fine-tuned using minimal expert demonstrations. It advances the temporal action contrastive learning (TACO) objective, known for state-of-the-art results in visual control tasks, by incorporating a novel negative example sampling strategy. This strategy is crucial in significantly boosting TACO's computational efficiency, making large-scale multitask offline pretraining feasible. Our extensive empirical evaluation in a diverse set of continuous control benchmarks including Deepmind Control Suite, MetaWorld, and LIBERO demonstrate Premier-TACO's effectiveness in pretraining visual representations, significantly enhancing few-shot imitation learning of nove
    
[^162]: Federated Learning能够找到有益的好友

    Federated Learning Can Find Friends That Are Beneficial

    [https://arxiv.org/abs/2402.05050](https://arxiv.org/abs/2402.05050)

    本研究介绍了一种新颖的算法，在Federated Learning中使用自适应聚合权重来识别对特定学习目标最有益的客户，证明了该方法的收敛性，并经过实证评估发现，使用该算法引导的合作优于传统方法，这为更加简化和有效的Federated Learning实现奠定了基础。

    

    在Federated Learning (FL)中，分布式性质和客户数据的异质性既带来了机会，也带来了挑战。虽然客户之间的合作可以显著增强学习过程，但并不是所有的合作都是有益的；有些甚至可能是有害的。在这项研究中，我们引入了一种新颖的算法，为参与FL训练的客户分配自适应的聚合权重，识别出数据分布对特定学习目标最有益的客户。我们证明了我们的聚合方法的收敛性与仅聚合具有相同数据分布的客户接收的更新的方法不相上下。此外，经验证明，由我们的算法引导的合作优于传统的FL方法。这强调了审慎选择客户的关键作用，并为未来更简化和有效的FL实现奠定了基础。

    In Federated Learning (FL), the distributed nature and heterogeneity of client data present both opportunities and challenges. While collaboration among clients can significantly enhance the learning process, not all collaborations are beneficial; some may even be detrimental. In this study, we introduce a novel algorithm that assigns adaptive aggregation weights to clients participating in FL training, identifying those with data distributions most conducive to a specific learning objective. We demonstrate that our aggregation method converges no worse than the method that aggregates only the updates received from clients with the same data distribution. Furthermore, empirical evaluations consistently reveal that collaborations guided by our algorithm outperform traditional FL approaches. This underscores the critical role of judicious client selection and lays the foundation for more streamlined and effective FL implementations in the coming years.
    
[^163]: 神经网络学习逐渐复杂的统计学

    Neural Networks Learn Statistics of Increasing Complexity

    [https://arxiv.org/abs/2402.04362](https://arxiv.org/abs/2402.04362)

    本文证明了分布简单性倾向（DSB）的神经网络学习规律，即在训练早期自动学习低阶矩的最大熵分布特征，然后在训练后期失去这种能力。此外，研究还利用最优传输方法进行了低阶统计数据的编辑，证明了早期训练的网络会将编辑的样本视为目标类别的样本。

    

    分布简单性倾向（DSB）假设神经网络首先学习低阶矩，然后再转向高阶相关性。在这项工作中，我们通过展示网络在训练早期自动学习在最大熵分布上表现良好，而在训练后期失去这种能力的有力新证据，给出了令人信服的新证据来支持DSB。我们还通过证明令牌$n$-gram频率与嵌入向量的矩之间的等价关系，并在LLM中找到倾向的实证证据，将DSB扩展到离散领域。最后，我们使用最优传输方法将一类的低阶统计数据手术性地编辑成与另一类相匹配，然后展示早期训练的网络将编辑的样本视为来自目标类别的样本。代码可在 https://github.com/EleutherAI/features-across-time 上获取。

    The distributional simplicity bias (DSB) posits that neural networks learn low-order moments of the data distribution first, before moving on to higher-order correlations. In this work, we present compelling new evidence for the DSB by showing that networks automatically learn to perform well on maximum-entropy distributions whose low-order statistics match those of the training set early in training, then lose this ability later. We also extend the DSB to discrete domains by proving an equivalence between token $n$-gram frequencies and the moments of embedding vectors, and by finding empirical evidence for the bias in LLMs. Finally we use optimal transport methods to surgically edit the low-order statistics of one class to match those of another, and show that early-training networks treat the edited samples as if they were drawn from the target class. Code is available at https://github.com/EleutherAI/features-across-time.
    
[^164]: 关于MPNN中特征向量的维度

    On dimensionality of feature vectors in MPNNs

    [https://arxiv.org/abs/2402.03966](https://arxiv.org/abs/2402.03966)

    这篇论文重新考察了消息传递图神经网络（MPNN）特征向量的维度问题，发现实际使用的架构与理论保证存在差距。

    

    我们重新考察了Morris等人（AAAI'19）关于消息传递图神经网络（MPNNs）与Weisfeiler-Leman（WL）同构测试在区分能力上相等的经典结果。Morris等人展示了使用ReLU激活函数和$O(n)$维特征向量的仿真结果，其中$n$是图的节点数。最近，通过将随机性引入到架构中，Aamand等人（NeurIPS'22）能够将特征向量的维度提高到$O(\log n)$，尽管以高概率保证完全模拟的开销也增加了。在所有这些构造中，为了保证与WL测试的等价性，MPNN中的特征向量维度必须随着图的大小增加。然而，实际使用的架构具有恒定维度的特征向量。因此，这些结果提供的保证与实际使用的架构特性之间存在差距。

    We revisit the classical result of Morris et al.~(AAAI'19) that message-passing graphs neural networks (MPNNs) are equal in their distinguishing power to the Weisfeiler--Leman (WL) isomorphism test.   Morris et al.~show their simulation result with ReLU activation function and $O(n)$-dimensional feature vectors, where $n$ is the number of nodes of the graph. Recently, by introducing randomness into the architecture, Aamand et al.~(NeurIPS'22) were able to improve this bound to $O(\log n)$-dimensional feature vectors, although at the expense of guaranteeing perfect simulation only with high probability.   In all these constructions, to guarantee equivalence to the WL test, the dimension of feature vectors in the MPNN has to increase with the size of the graphs. However, architectures used in practice have feature vectors of constant dimension. Thus, there is a gap between the guarantees provided by these results and the actual characteristics of architectures used in practice. In this p
    
[^165]: MolTC: 在语言模型中进行分子关系建模

    MolTC: Towards Molecular Relational Modeling In Language Models

    [https://arxiv.org/abs/2402.03781](https://arxiv.org/abs/2402.03781)

    本研究提出了一种基于语言模型的多模态框架MolTC，用于分子相互作用预测，该框架能够高效地整合分子对的丰富图形信息，并通过思维链理论实现统一的分子关系学习。

    

    分子关系学习（MRL）旨在理解分子之间的相互作用，在推进生物化学研究方面起到了关键作用。最近，大型语言模型（LLMs）的采用已成为一种有效和高效的MRL方法，这些模型以其庞大的知识存储库和先进的逻辑推理能力而闻名。尽管具有潜力，但这些方法主要依赖于文本数据，因此没有充分利用分子图中固有的丰富结构信息。此外，缺乏统一的框架加剧了信息的浪费，因为它阻碍了在不同数据集之间共享学习到的相互作用理由。为了解决这些挑战，本研究提出了一种基于LLM的多模态框架，用于根据思维链（CoT）理论对分子相互作用进行预测，称为MolTC，它可以高效地整合分子对的丰富图形信息。

    Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
    
[^166]: 由端到端深度学习模型加强的高效数值波传播

    Efficient Numerical Wave Propagation Enhanced by an End-to-End Deep Learning Model

    [https://arxiv.org/abs/2402.02304](https://arxiv.org/abs/2402.02304)

    本文提出了一个由端到端深度学习模型加强的高效数值波传播方法，通过结合数值求解器和深度学习组件，优化算法架构、数据生成和并行时间算法，实现了在保持速度的同时显著提高性能。

    

    在多个科学和工程领域，从地震建模到医学成像，对于高频波传播的高保真和高效解决方案的需求非常重要。最近在波传播模型中的一项进展利用足够准确的细求解器输出来训练神经网络，以提高快速但不准确的粗求解器的准确性。稳定且快速的求解器还允许使用并行时间算法Parareal来提取和纠正高频波组成部分。在本文中，我们在Nguyen和Tsai（2023）的工作基础上，提出了一个新颖的统一系统，将数值求解器与深度学习组件整合到端到端框架中。在提出的设置中，我们研究了神经网络架构、数据生成算法和Parareal方案的改进。我们的结果表明，这种协调的结构在不牺牲速度的情况下显著提高了性能，并且证明了

    In a variety of scientific and engineering domains, ranging from seismic modeling to medical imaging, the need for high-fidelity and efficient solutions for high-frequency wave propagation holds great significance. Recent advances in wave modeling use sufficiently accurate fine solver outputs to train neural networks that enhance the accuracy of a fast but inaccurate coarse solver. A stable and fast solver further allows the use of Parareal, a parallel-in-time algorithm to retrieve and correct high-frequency wave components. In this paper we build upon the work of Nguyen and Tsai (2023) and present a novel unified system that integrates a numerical solver with deep learning components into an end-to-end framework. In the proposed setting, we investigate refinements to the neural network architecture, data generation algorithm and Parareal scheme. Our results show that the cohesive structure significantly improves performance without sacrificing speed, and demonstrate the importance of 
    
[^167]: 一个贝叶斯聚类有效性指数

    A Bayesian cluster validity index

    [https://arxiv.org/abs/2402.02162](https://arxiv.org/abs/2402.02162)

    该论文提出了一个基于贝叶斯方法的聚类有效性指数，该指数根据现有的基础指数定义，并用于检测次优聚类数，通过与其他指数进行比较，验证了其有效性。

    

    在应用聚类算法时，选择聚类数是关键步骤之一。为了完成这个任务，引入了各种聚类有效性指数（CVIs）。大多数聚类有效性指数都被定义为检测数据集中隐藏的最优聚类数。然而，用户有时并不期望获得最优聚类数，而是更适合他们应用的次优聚类数。这促使我们引入了一种基于现有基础指数的贝叶斯聚类有效性指数（BCVI）。该指数基于狄利克雷或广义狄利克雷先验定义，得到相同的后验分布。然后我们基于Wiroonsri指数（WI）和Wiroonsri-Preedasawakul指数（WP）作为硬聚类和软聚类的基础指数来测试我们的BCVI。我们将它们的结果与原始的基础指数以及一些其他存在的CVIs（包括Davies and Bouldin (DB)，Starczewski (STR)）进行比较。

    Selecting the number of clusters is one of the key processes when applying clustering algorithms. To fulfill this task, various cluster validity indices (CVIs) have been introduced. Most of the cluster validity indices are defined to detect the optimal number of clusters hidden in a dataset. However, users sometimes do not expect to get the optimal number of groups but a secondary one which is more reasonable for their applications. This has motivated us to introduce a Bayesian cluster validity index (BCVI) based on existing underlying indices. This index is defined based on either Dirichlet or Generalized Dirichlet priors which result in the same posterior distribution. Our BCVI is then tested based on the Wiroonsri index (WI), and the Wiroonsri-Preedasawakul index (WP) as underlying indices for hard and soft clustering, respectively. We compare their outcomes with the original underlying indices, as well as a few more existing CVIs including Davies and Bouldin (DB), Starczewski (STR)
    
[^168]: 理解Transformer在序列建模中的表达能力和机制

    Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling

    [https://arxiv.org/abs/2402.00522](https://arxiv.org/abs/2402.00522)

    本研究系统地探讨了Transformer在长序列建模中的近似性质，并研究了其关键组件对表达能力的影响机制。这些发现揭示了关键参数对Transformer的作用，并为替代架构提供了自然建议。

    

    我们对Transformer在长、稀疏和复杂记忆的序列建模中的近似性质进行了系统研究。我们调查了Transformer的不同组件（如点积自注意力、位置编码和前馈层）是如何影响其表达能力的机制，并通过建立明确的近似率来研究它们的综合影响。我们的研究揭示了Transformer中关键参数（如层数和注意力头数）的作用，并且这些洞察还为替代架构提供了自然建议。

    We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates. Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads, and these insights also provide natural suggestions for alternative architectures.
    
[^169]: 大规模语言模型是零射击学习器

    Large Language Models are Null-Shot Learners

    [https://arxiv.org/abs/2401.08273](https://arxiv.org/abs/2401.08273)

    本文提出了零射击提示方法，通过利用大规模语言模型中的错误信息来指导模型进行任务，以提高任务表现。实验结果表明，在不同数据集上，包括阅读理解、算术推理和闭卷问答，模型性能有所提升。这些结果也显示出不同模型之间存在不同程度的错误信息。

    

    本文提出了零射击提示方法。零射击提示利用大规模语言模型（LLMs）中的错误信息，通过指示LLMs利用从“示例”部分中获取的信息（该信息在所提供的上下文中不存在）来完成任务。虽然减少错误信息对于LLMs的日常和重要用途至关重要，但我们提出在目前的环境中，这些LLMs仍然具有错误信息，实际上可以利用错误信息来提高与标准零射击提示相比的任务表现。对八个LLMs进行实验，结果显示在大多数八个数据集（包括阅读理解、算术推理和闭卷问答）中，性能有所提升。观察到的不一致性增加相对性能在LLMs之间的差异，也可能表示每个模型中存在不同程度的错误信息。

    arXiv:2401.08273v2 Announce Type: replace-cross Abstract: This paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the "Examples" section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with eight LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across the LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show 
    
[^170]: 激励感知合成对照方法：通过激励探索进行准确的反事实估计

    Incentive-Aware Synthetic Control: Accurate Counterfactual Estimation via Incentivized Exploration

    [https://arxiv.org/abs/2312.16307](https://arxiv.org/abs/2312.16307)

    本论文提出了一种为了解决合成对照方法中"重叠"假设的问题的激励感知合成对照方法。该方法通过激励单位采取通常不会考虑的干预措施，提供与激励相容的干预建议，从而实现在面板数据环境中准确估计反事实效果。

    

    我们考虑合成对照方法（SCMs）的设定，这是一种在面板数据环境中估计被治疗对象的治疗效应的经典方法。我们揭示了SCMs中经常被忽视但普遍存在的“重叠”假设：一个被治疗的单位可以被写成保持控制的单位的某种组合（通常是凸或线性组合）。我们展示了如果单位选择自己的干预措施，并且单位之间的异质性足够大，以至于他们偏好不同的干预措施，重叠将不成立。为了解决这个问题，我们提出了一个框架，通过激励具有不同偏好的单位来采取他们通常不会考虑的干预措施。具体来说，我们利用信息设计和在线学习的工具，提出了一种SCM，通过为单位提供与激励相容的干预建议，在面板数据环境中激励探索。

    arXiv:2312.16307v2 Announce Type: replace-cross Abstract: We consider the setting of synthetic control methods (SCMs), a canonical approach used to estimate the treatment effect on the treated in a panel data setting. We shed light on a frequently overlooked but ubiquitous assumption made in SCMs of "overlap": a treated unit can be written as some combination -- typically, convex or linear combination -- of the units that remain under control. We show that if units select their own interventions, and there is sufficiently large heterogeneity between units that prefer different interventions, overlap will not hold. We address this issue by proposing a framework which incentivizes units with different preferences to take interventions they would not normally consider. Specifically, leveraging tools from information design and online learning, we propose a SCM that incentivizes exploration in panel data settings by providing incentive-compatible intervention recommendations to units. We e
    
[^171]: 对文本到图像生成中的不对称偏差的对抗攻击研究

    Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks

    [https://arxiv.org/abs/2312.14440](https://arxiv.org/abs/2312.14440)

    本文对文本到图像生成中的对抗攻击进行了实证研究，发现了攻击成功率的不对称性，并提出了用于探测模型对抗攻击信号的指标。

    

    文本到图像（T2I）模型在内容生成中的广泛应用需要仔细研究它们的安全性，包括它们对抗攻击的鲁棒性。尽管对抗攻击的研究已经很广泛，但其有效性的原因仍然未被深入探索。本文对T2I模型的对抗攻击进行了实证研究，重点分析了与攻击成功率（ASR）相关的因素。我们引入了一种新的攻击目标 - 使用对抗性后缀进行实体替换，以及两种基于梯度的攻击算法。人工和自动评估揭示了实体交换中ASR的不对称性质：例如，对于在提示“在雨中跳舞的人类”中替换“人类”为“机器人”的对抗性后缀，较容易实现，而反向替换则明显困难得多。我们进一步提出了探测指标，以确定模型对抗ASR的信号。我们发现了：

    arXiv:2312.14440v2 Announce Type: replace Abstract: The widespread use of Text-to-Image (T2I) models in content generation requires careful examination of their safety, including their robustness to adversarial attacks. Despite extensive research on adversarial attacks, the reasons for their effectiveness remain underexplored. This paper presents an empirical study on adversarial attacks against T2I models, focusing on analyzing factors associated with attack success rates (ASR). We introduce a new attack objective - entity swapping using adversarial suffixes and two gradient-based attack algorithms. Human and automatic evaluations reveal the asymmetric nature of ASRs on entity swap: for example, it is easier to replace "human" with "robot" in the prompt "a human dancing in the rain." with an adversarial suffix, but the reverse replacement is significantly harder. We further propose probing metrics to establish indicative signals from the model's beliefs to the adversarial ASR. We iden
    
[^172]: POND: 带有信息感知提示调整的多源时间序列领域适应

    POND: Multi-Source Time Series Domain Adaptation with Information-Aware Prompt Tuning

    [https://arxiv.org/abs/2312.12276](https://arxiv.org/abs/2312.12276)

    POND是第一个可以利用领域特定信息进行多源时间序列领域适应的方法。

    

    时间序列领域适应是一个关键而复杂的挑战，具有各种应用，包括人体活动识别、睡眠阶段分类和机器故障诊断等。尽管已经提出了许多领域适应技术来应对这个复杂问题，但它们主要集中在单个源领域的领域适应上。然而，探索来自多个领域的领域适应更为关键，因为它有潜力带来更大的改进。为了同时解决这些挑战，本文提出了基于提示的领域判别（POND），这是第一个可以利用领域特定信息进行领域适应的方法。

    arXiv:2312.12276v2 Announce Type: replace Abstract: Time series domain adaptation stands as a pivotal and intricate challenge with diverse applications, including but not limited to human activity recognition, sleep stage classification, and machine fault diagnosis. Despite the numerous domain adaptation techniques proposed to tackle this complex problem, they primarily focus on domain adaptation from a single source domain. Yet, it is more crucial to investigate domain adaptation from multiple domains due to the potential for greater improvements. To address this, three important challenges need to be overcome: 1). The lack of exploration to utilize domain-specific information for domain adaptation, 2). The difficulty to learn domain-specific information that changes over time, and 3). The difficulty to evaluate learned domain-specific information. In order to tackle these challenges simultaneously, in this paper, we introduce PrOmpt-based domaiN Discrimination (POND), the first frame
    
[^173]: Neural Language Agents的diff历史

    diff History for Neural Language Agents

    [https://arxiv.org/abs/2312.07540](https://arxiv.org/abs/2312.07540)

    本文介绍了一种名为diff历史的简单且高效的解决方案，用于将环境中的观测转换为文本提示，以便对于长期推理决策的任务中的Neural Language Models进行优化。

    

    Neural Language Models (LMs)为通用的具体控制提供了令人兴奋的解决方案。然而，当使用基于LM的控制器时，会出现一个关键的技术问题：环境观测必须转换为文本，这与历史耦合在一起，导致冗长而冗余的文本提示。因此，LM代理的先前工作局限于具有小观测大小以及对交互历史或指示调优需求较小的限制领域。在本文中，我们引入了diff历史，这是一个简单且非常有效的解决方案。通过在用于提示LM策略的交互历史中的连续文本观测上应用Unix diff命令，我们既可以摘除冗余信息，又可以将文本输入的内容集中在环境中显著变化的方面。在需要长期推理进行决策的未解决的视频游戏NetHack中，使用diff历史调优的LM与状态匹配。

    arXiv:2312.07540v2 Announce Type: replace Abstract: Neural Language Models (LMs) offer an exciting solution for general-purpose embodied control. However, a key technical issue arises when using an LM-based controller: environment observations must be converted to text, which coupled with history, results in long and verbose textual prompts. As a result, prior work in LM agents is limited to restricted domains with small observation size as well as minimal needs for interaction history or instruction tuning. In this paper, we introduce diff history, a simple and highly effective solution to these issues. By applying the Unix diff command on consecutive text observations in the interaction histories used to prompt LM policies, we can both abstract away redundant information and focus the content of textual inputs on the salient changes in the environment. On NetHack, an unsolved video game that requires long-horizon reasoning for decision-making, LMs tuned with diff history match state-
    
[^174]: 表征自旋状态和强制移动优化中的局部性

    Characterization of Locality in Spin States and Forced Moves for Optimizations

    [https://arxiv.org/abs/2312.02544](https://arxiv.org/abs/2312.02544)

    该论文提出了一种优化算法，利用特定硬件较容易获取的局部性特征，可以高效地脱离局部最小值，从而实现对全局最小值的优化

    

    Ising公式被广泛应用于解决组合优化问题，最近也提供了各种量子或基于半导体的硬件。在组合优化问题中，能量景观中存在局部极小值对于寻找全局极小值是有问题的。我们注意到优化的目标不是从玻尔兹曼分布中获得精确的采样，因此不需要满足细致平衡条件。基于这一事实，我们开发了一种算法，可以高效地从局部极小值中脱离出来，而不会产生精确的采样。为此，我们利用了一种描述当前状态局部性的特征，这种特征可以利用某种专用硬件轻松获取。此外，由于提出的算法基于无拒绝的算法，计算成本很低。在这项工作中，我们将介绍所提算法的细节

    arXiv:2312.02544v2 Announce Type: replace-cross Abstract: Ising formulations are widely utilized to solve combinatorial optimization problems, and a variety of quantum or semiconductor-based hardware has recently been made available. In combinatorial optimization problems, the existence of local minima in energy landscapes is problematic to use to seek the global minimum. We note that the aim of the optimization is not to obtain exact samplings from the Boltzmann distribution, and there is thus no need to satisfy detailed balance conditions. In light of this fact, we develop an algorithm to get out of the local minima efficiently while it does not yield the exact samplings. For this purpose, we utilize a feature that characterizes locality in the current state, which is easy to obtain with a type of specialized hardware. Furthermore, as the proposed algorithm is based on a rejection-free algorithm, the computational cost is low. In this work, after presenting the details of the propose
    
[^175]: 学习脑区的高阶关系

    Learning High-Order Relationships of Brain Regions

    [https://arxiv.org/abs/2312.02203](https://arxiv.org/abs/2312.02203)

    从fMRI数据中学习脑区的高阶关系对于脑区之间的相互作用的准确表征具有重要意义。我们提出了一种名为HYBRID的新方法，通过识别超边结构和计算超边的权重，提取了最具信息量和最小冗余的高阶关系。

    

    从功能性磁共振成像（fMRI）信号中发现脑区之间可靠且具有信息量的关系对于表型预测至关重要。大多数现有方法无法准确地表征这些相互作用，因为它们只关注配对连接并忽视了脑区的高阶关系。我们提出这些高阶关系应该具有最大的信息量和最小的冗余（MIMR）。然而，由于指数级的搜索空间和可解性目标的缺乏，发现这种高阶关系是具有挑战性且未经探索的。针对这一间隙，我们提出了一种名为HYBRID的新方法，旨在从fMRI数据中提取MIMR高阶关系。HYBRID利用CONSTRUCTOR来识别超边结构，并利用WEIGHTER来计算每个超边的权重，从而避免在指数空间中进行搜索。HYBRID取得了...

    arXiv:2312.02203v2 Announce Type: replace-cross Abstract: Discovering reliable and informative relationships among brain regions from functional magnetic resonance imaging (fMRI) signals is essential in phenotypic predictions. Most of the current methods fail to accurately characterize those interactions because they only focus on pairwise connections and overlook the high-order relationships of brain regions. We propose that these high-order relationships should be maximally informative and minimally redundant (MIMR). However, identifying such high-order relationships is challenging and under-explored due to the exponential search space and the absence of a tractable objective. In response to this gap, we propose a novel method named HYBRID which aims to extract MIMR high-order relationships from fMRI data. HYBRID employs a CONSTRUCTOR to identify hyperedge structures, and a WEIGHTER to compute a weight for each hyperedge, which avoids searching in exponential space. HYBRID achieves t
    
[^176]: (非)理性在人工智能中的应用：现状、研究挑战和未解之问

    (Ir)rationality in AI: State of the Art, Research Challenges and Open Questions

    [https://arxiv.org/abs/2311.17165](https://arxiv.org/abs/2311.17165)

    这篇论文调查了人工智能中理性与非理性的概念，提出了未解问题。重点讨论了行为在某些情况下的非理性行为可能是最优的情况。已经提出了一些方法来处理非理性代理，但仍存在挑战和问题。

    

    理性概念在人工智能领域中占据着重要地位。无论是模拟人类推理还是追求有限最优性，我们通常希望使人工智能代理尽可能理性。尽管这个概念在人工智能中非常核心，但对于什么构成理性代理并没有统一的定义。本文调查了人工智能中的理性与非理性，并提出了这个领域的未解问题。在其他领域对理性的理解对其在人工智能中的概念产生了影响，特别是经济学、哲学和心理学方面的研究。着重考虑人工智能代理的行为，我们探讨了在某些情境中非理性行为可能是最优的情况。关于处理非理性代理的方法已经得到了一些发展，包括识别和交互等方面的研究，然而，在这个领域的工作仍然存在一些挑战和问题。

    arXiv:2311.17165v2 Announce Type: replace Abstract: The concept of rationality is central to the field of artificial intelligence. Whether we are seeking to simulate human reasoning, or the goal is to achieve bounded optimality, we generally seek to make artificial agents as rational as possible. Despite the centrality of the concept within AI, there is no unified definition of what constitutes a rational agent. This article provides a survey of rationality and irrationality in artificial intelligence, and sets out the open questions in this area. The understanding of rationality in other fields has influenced its conception within artificial intelligence, in particular work in economics, philosophy and psychology. Focusing on the behaviour of artificial agents, we consider irrational behaviours that can prove to be optimal in certain scenarios. Some methods have been developed to deal with irrational agents, both in terms of identification and interaction, however work in this area re
    
[^177]: 关注图神经网络用于稳健的大规模网络定位

    Attentional Graph Neural Networks for Robust Massive Network Localization

    [https://arxiv.org/abs/2311.16856](https://arxiv.org/abs/2311.16856)

    本文通过将图神经网络与注意机制相结合，提出了一种用于网络定位的新方法。该方法具有出色的精确度，甚至在严重非直视视线条件下也能表现出良好的效果。通过提出的关注图神经网络模型，我们进一步改善了现有方法的灵活性和对超参数的敏感性。

    

    近年来，图神经网络(GNNs)已成为机器学习分类任务中的重要工具。然而，它们在回归任务中的应用仍然未被充分探索。为了发掘GNNs在回归中的潜力，本文将GNNs与注意机制相结合，这是一种通过其适应性和鲁棒性彻底改变了序列学习任务的技术，以解决一个具有挑战性的非线性回归问题：网络定位。我们首先介绍了一种基于图卷积网络(GCN)的新型网络定位方法，即使在严重非直视视线(NLOS)条件下也表现出卓越的精度，从而减少了繁琐的离线校准或NLOS识别的需求。我们进一步提出了一种关注图神经网络(AGNN)模型，旨在改善基于GCN方法的有限灵活性和对超参数的高敏感性。

    arXiv:2311.16856v2 Announce Type: replace Abstract: In recent years, Graph neural networks (GNNs) have emerged as a prominent tool for classification tasks in machine learning. However, their application in regression tasks remains underexplored. To tap the potential of GNNs in regression, this paper integrates GNNs with attention mechanism, a technique that revolutionized sequential learning tasks with its adaptability and robustness, to tackle a challenging nonlinear regression problem: network localization. We first introduce a novel network localization method based on graph convolutional network (GCN), which exhibits exceptional precision even under severe non-line-of-sight (NLOS) conditions, thereby diminishing the need for laborious offline calibration or NLOS identification. We further propose an attentional graph neural network (AGNN) model, aimed at improving the limited flexibility and mitigating the high sensitivity to the hyperparameter of the GCN-based method. The AGNN co
    
[^178]: 关于去中心化双层优化的通信复杂度

    On the Communication Complexity of Decentralized Bilevel Optimization

    [https://arxiv.org/abs/2311.11342](https://arxiv.org/abs/2311.11342)

    本研究针对去中心化双层优化的通信复杂度问题，提出了一种新颖的去中心化随机双层梯度下降算法，在异构设置下具有较小的通信成本和轮次，并实现了比现有算法更好的通信复杂度。

    

    过去几年中，去中心化双层优化在机器学习中得到了广泛研究，因为它在现实世界任务中有着广泛的应用。然而，现有算法由于估计随机超梯度而导致通信复杂度较大，限制了它们在实际任务中的应用。为了解决这个问题，我们在异构设置下开发了一种新颖的去中心化随机双层梯度下降算法，每轮中具有较小的通信成本和较少的通信轮次。因此，它可以在没有任何关于异构性的强假设的情况下实现比现有算法更好的通信复杂度。据我们所知，这是第一个在异构设置下实现这些理论结果的随机算法。最后，实验结果证实了我们算法的有效性。

    arXiv:2311.11342v2 Announce Type: replace Abstract: Decentralized bilevel optimization has been actively studied in the past few years since it has widespread applications in machine learning. However, existing algorithms suffer from large communication complexity caused by the estimation of stochastic hypergradient, limiting their application to real-world tasks. To address this issue, we develop a novel decentralized stochastic bilevel gradient descent algorithm under the heterogeneous setting, which enjoys a small communication cost in each round and a small number of communication rounds. As such, it can achieve a much better communication complexity than existing algorithms without any strong assumptions regarding heterogeneity. To the best of our knowledge, this is the first stochastic algorithm achieving these theoretical results under the heterogeneous setting. At last, the experimental results confirm the efficacy of our algorithm.
    
[^179]: 离散非参数隐性类别混淆下的因果发现

    Discrete Nonparametric Causal Discovery Under Latent Class Confounding

    [https://arxiv.org/abs/2311.07454](https://arxiv.org/abs/2311.07454)

    本论文研究了离散非参数隐性类别混淆下的因果发现问题，证明了在有限的潜在类别下，因果发现仍然是可识别的。

    

    有向无环图用于建模系统的因果结构。"因果发现"描述了从数据中学习这种结构的问题。当数据是来自多个源（群体或环境）的聚合物时，全局混淆使驱动许多因果发现算法的条件独立性特性变得模糊。这种情况有时被称为混合模型或潜在类别。虽然一些现代因果发现方法能够在特定情况下处理未观察到的混淆，但是目前所知的处理全局混淆的方法都涉及不适用于离散分布的参数假设。以离散和非参数观察变量为重点，我们证明了在有限的潜在类别下，因果发现仍然是可识别的。这个问题的可行性由全局混淆的基数、观察变量的基数等决定。

    arXiv:2311.07454v2 Announce Type: replace Abstract: Directed acyclic graphs are used to model the causal structure of a system. ``Causal discovery'' describes the problem of learning this structure from data. When data is an aggregate from multiple sources (populations or environments), global confounding obscures conditional independence properties that drive many causal discovery algorithms. This setting is sometimes known as a mixture model or a latent class. While some modern methods for causal discovery are able to work around unobserved confounding in specific cases, the only known ways to deal with a global confounder involve parametric assumptions. that are unsuitable for discrete distributions.Focusing on discrete and non-parametric observed variables, we demonstrate that causal discovery can still be identifiable under bounded latent classes. The feasibility of this problem is governed by a trade-off between the cardinality of the global confounder, the cardinalities of the o
    
[^180]: 在上下文向量中，通过潜空间操控使上下文学习更有效和可控

    In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering

    [https://arxiv.org/abs/2311.06668](https://arxiv.org/abs/2311.06668)

    通过潜空间操控，使用上下文向量作为替代方法来进行上下文学习，以使语言模型更有效地遵循示例演示，并通过调整向量的量级来轻松控制学习过程。

    

    大型语言模型（LLM）展示了新任务适应能力，并通过示例演示来进行上下文学习。然而，在许多情况下，上下文学习的效果有限，难以定量控制，并占用上下文窗口空间。为了克服这些限制，我们提出了一种替代方法，将上下文学习重新定义为上下文向量（ICV）。使用ICV有两个步骤。首先，我们对演示示例进行正向传递，从LLM的潜在嵌入中创建上下文向量。这个向量捕捉了关于预期任务的关键信息。对于一个新的查询，我们不是将示例添加到提示中，而是使用ICV来改变LLM的潜在状态。ICV方法有几个好处：1）它使LLM能够更有效地遵循示例演示；2）通过调整ICV的量级，它易于控制；3）

    arXiv:2311.06668v3 Announce Type: replace-cross Abstract: Large language models (LLMs) demonstrate emergent in-context learning capabilities, where they adapt to new tasks based on example demonstrations. However, in-context learning has seen limited effectiveness in many settings, is difficult to quantitatively control and takes up context window space. To overcome these limitations, we propose an alternative approach that recasts in-context learning as in-context vectors (ICV). Using ICV has two steps. We first use a forward pass on demonstration examples to create the in-context vector from the latent embedding of the LLM. This vector captures essential information about the intended task. On a new query, instead of adding demonstrations to the prompt, we shift the latent states of the LLM using the ICV. The ICV approach has several benefits: 1) it enables the LLM to more effectively follow the demonstration examples; 2) it's easy to control by adjusting the magnitude of the ICV; 3)
    
[^181]: 对比深度非负矩阵分解用于社区发现

    Contrastive Deep Nonnegative Matrix Factorization for Community Detection

    [https://arxiv.org/abs/2311.02357](https://arxiv.org/abs/2311.02357)

    对比深度非负矩阵分解算法（CDNMF）通过加深NMF的信息提取能力，采用对比学习的思想构建了网络拓扑和节点属性作为对比视图，并利用去偏方法来优化社区探测的全局结构信息的学习。

    

    最近，非负矩阵分解（NMF）被广泛应用于社区发现，因为其具有更好的可解释性。然而，现有的基于NMF的方法存在以下三个问题：1）它们直接将原始网络转换为社区成员空间，因此很难捕捉层次信息；2）它们通常只关注网络的拓扑结构，忽视了节点属性；3）它们很难学习到社区发现所需的全局结构信息。因此，我们提出了一种新的社区发现算法，名为对比深度非负矩阵分解（CDNMF）。首先，我们深化NMF以增强其信息提取能力。随后，受到对比学习的启发，我们的算法创造性地将网络拓扑和节点属性构建为两种对比视图。此外，我们使用一种去偏的方法...

    arXiv:2311.02357v2 Announce Type: replace-cross Abstract: Recently, nonnegative matrix factorization (NMF) has been widely adopted for community detection, because of its better interpretability. However, the existing NMF-based methods have the following three problems: 1) they directly transform the original network into community membership space, so it is difficult for them to capture the hierarchical information; 2) they often only pay attention to the topology of the network and ignore its node attributes; 3) it is hard for them to learn the global structure information necessary for community detection. Therefore, we propose a new community detection algorithm, named Contrastive Deep Nonnegative Matrix Factorization (CDNMF). Firstly, we deepen NMF to strengthen its capacity for information extraction. Subsequently, inspired by contrastive learning, our algorithm creatively constructs network topology and node attributes as two contrasting views. Furthermore, we utilize a debiased
    
[^182]: 使用字节级精确度的编码器-解码器基于Transformer模型理解DNA的自然语言

    Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision

    [https://arxiv.org/abs/2311.02333](https://arxiv.org/abs/2311.02333)

    本文提出了一种基于字节级精度的编码器-解码器模型，用于理解DNA的自然语言。该模型可以在字节级精度上分析DNA序列，使得能够用于识别DNA序列中的各种功能和变异。

    

    本文提出了一种基于字节级编码器-解码器Transformer架构的集合核苷酸字节级编码器-解码器(ENBED)基础模型，可以在字节级精度上分析DNA序列。ENBED使用次二次的注意力实现了一个高效的模型，能够进行序列到序列的转换，泛化先前基因组模型只采用编码器或者解码器体系结构的限制。我们使用遮蔽语言建模来预训练这个基础模型，使用参考基因组序列并将其应用到以下下游任务上：(1)识别增强子、启动子和剪切位点，(2)识别包含碱基调用不匹配和插入/缺失错误的序列，这是对多个碱基对进行标记化的方案的优势，丢失了字节级精度分析的能力，(3)识别基因组序列的生物功能注释，以及(4)生成突变基因组序列。

    arXiv:2311.02333v2 Announce Type: replace Abstract: This paper presents the Ensemble Nucleotide Byte-level Encoder-Decoder (ENBED) foundation model, analyzing DNA sequences at byte-level precision with an encoder-decoder Transformer architecture. ENBED uses a sub-quadratic implementation of attention to develop an efficient model capable of sequence-to-sequence transformations, generalizing previous genomic models with encoder-only or decoder-only architectures. We use Masked Language Modeling to pre-train the foundation model using reference genome sequences and apply it in the following downstream tasks: (1) identification of enhancers, promotors and splice sites, (2) recognition of sequences containing base call mismatches and insertion/deletion errors, an advantage over tokenization schemes involving multiple base pairs, which lose the ability to analyze with byte-level precision, (3) identification of biological function annotations of genomic sequences, and (4) generating mutatio
    
[^183]: 通过最小化休眠比率掌握视觉强化学习

    DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization

    [https://arxiv.org/abs/2310.19668](https://arxiv.org/abs/2310.19668)

    通过最小化休眠比率，本文揭示了当前视觉强化学习方法的一个主要缺点是智能体在早期训练阶段展现出持续的不活动状态，限制了其有效探索能力，并提出了使用休眠比率作为度量指标的方法。

    

    视觉强化学习在连续控制任务中显示出了潜力。尽管取得了一定的进展，但当前的算法在样本效率、渐近性能和对随机种子选择的鲁棒性等方面仍然不尽人意。在本文中，我们发现了现有视觉强化学习方法的一个主要缺点，即在早期训练阶段，智能体经常展现出持续的不活动状态，从而限制了它们有效探索的能力。基于这一关键观察，我们进一步揭示了智能体倾向于运动不活跃探索与其策略网络中的神经活动缺失之间的显著相关性。为了量化这种不活跃状态，我们采用休眠比率作为衡量强化学习智能体网络中不活跃的度量标准。实证上，我们还认识到休眠比率可以作为智能体匹配能力的独立指标。

    arXiv:2310.19668v2 Announce Type: replace Abstract: Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent'
    
[^184]: AttributionLab:在可控环境下的特征归因忠实性

    AttributionLab: Faithfulness of Feature Attribution Under Controllable Environments

    [https://arxiv.org/abs/2310.06514](https://arxiv.org/abs/2310.06514)

    本文提出了AttributionLab，一个可控环境下测试特征归因忠实性的实验室。通过在设计数据上拟合模型并与真实输入特征进行比较，该实验室可以用于分析归因方法的性能，并提出改进建议。

    

    特征归因通过识别相关的输入特征来解释神经网络的输出。这种归因必须是忠实的，意味着被归因的特征必须反映影响输出的输入特征。最近的一个趋势是通过在设计数据上拟合模型并将归因与真实的输入特征进行比较来测试忠实性。这个想法假设模型学会了仅使用这些设计特征，但并没有保证。本文通过设计网络并手动设置其权重以及设计数据来解决这个问题。这个设定称为“AttributionLab”，它可以作为忠实性的合理性检查：如果一个归因方法在一个可控环境中并不忠实，那么它在实际应用中可能是不可靠的。这个环境也是一个用于控制实验的实验室，我们可以通过这些实验来分析归因方法并提出改进的建议。

    arXiv:2310.06514v2 Announce Type: replace Abstract: Feature attribution explains neural network outputs by identifying relevant input features. The attribution has to be faithful, meaning that the attributed features must mirror the input features that influence the output. One recent trend to test faithfulness is to fit a model on designed data with known relevant features and then compare attributions with ground truth input features.This idea assumes that the model learns to use all and only these designed features, for which there is no guarantee. In this paper, we solve this issue by designing the network and manually setting its weights, along with designing data. The setup, AttributionLab, serves as a sanity check for faithfulness: If an attribution method is not faithful in a controlled environment, it can be unreliable in the wild. The environment is also a laboratory for controlled experiments by which we can analyze attribution methods and suggest improvements.
    
[^185]: Hyp-OW: 利用超几何距离的层次结构学习增强开放世界目标检测

    Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic Distance Enhances Open World Object Detection

    [https://arxiv.org/abs/2306.14291](https://arxiv.org/abs/2306.14291)

    Hyp-OW是一种利用超几何距离的层次结构学习增强开放世界目标检测的方法，通过超类正则化器学习和建模已知项目的层次表示，通过基于相似度距离的重新标记模块有效地检测未知对象。

    

    开放世界目标检测(OWOD)是一项具有挑战性且现实的任务，超越了标准目标检测任务的范围。它需要在检测已知和未知对象的同时，整合学习到的知识用于未来的任务。然而，“未知性”在不同上下文中有很大的变化。例如，在自动驾驶场景中，树通常被认为是背景的一部分，但在家庭环境中可能具有重要性。我们认为这种上下文信息应该已经嵌入到已知类别中。换句话说，已知和未知项之间应该存在语义或潜在的结构关系等待发现。受到这一观察的启发，我们提出了Hyp-OW，一种通过超类正则化器来学习和建模已知项目的层次表示的方法。利用这种表示，我们可以通过基于相似度距离的重新标记模块有效地检测未知对象。大量实验证明了我们方法的有效性。

    Open World Object Detection (OWOD) is a challenging and realistic task that extends beyond the scope of standard Object Detection task. It involves detecting both known and unknown objects while integrating learned knowledge for future tasks. However, the level of "unknownness" varies significantly depending on the context. For example, a tree is typically considered part of the background in a self-driving scene, but it may be significant in a household context. We argue that this contextual information should already be embedded within the known classes. In other words, there should be a semantic or latent structure relationship between the known and unknown items to be discovered. Motivated by this observation, we propose Hyp-OW, a method that learns and models hierarchical representation of known items through a SuperClass Regularizer. Leveraging this representation allows us to effectively detect unknown objects using a similarity distance-based relabeling module. Extensive experi
    
[^186]: LongForm: 使用反向指令进行有效的指令调优

    LongForm: Effective Instruction Tuning with Reverse Instructions

    [https://arxiv.org/abs/2304.08460](https://arxiv.org/abs/2304.08460)

    使用反向指令进行有效的指令调优，通过生成一组自然的、适用于长文本生成的指令调优数据集，我们的模型在故事/菜谱生成和长篇问答等任务上优于10倍规模更大的语言模型，而无需指令调优。

    

    Instruction tuning使得语言模型能够更有效地泛化，并更好地遵循用户意图。然而，获取指令数据成本高且具有挑战性。先前的工作采用了诸如昂贵的人工注释、存在对齐问题的众包数据集、以及通过LLMs生成噪声示例的方法。我们引入了LongForm-C数据集，该数据集由反向指令创建。我们使用LLMs为人类写作语料库示例使用反向指令生成指令。首先，我们从诸如C4和Wikipedia的语料库中选择了一组多样性的人类撰写文档；然后，我们使用LLMs为这些文档生成指令。这种方法提供了一个更便宜、更干净、输出自然以及适用于长文本生成的指令调优数据集。我们的模型在故事/菜谱生成和长篇问答等任务上优于10倍规模更大的语言模型，而无需指令调优。

    arXiv:2304.08460v2 Announce Type: replace-cross Abstract: Instruction tuning enables language models to more effectively generalize and better follow user intent. However, obtaining instruction data is costly and challenging. Prior work employs methods such as expensive human annotation, crowd-sourced datasets with alignment issues, and generating noisy examples via LLMs. We introduce the LongForm-C dataset, which is created by reverse instructions. We generate instructions via LLMs for human-written corpus examples using reverse instructions. First we select a diverse set of human-written documents from corpora such as C4 and Wikipedia; then we generate instructions for these documents via LLMs. This approach provides a cheaper and cleaner instruction-tuning dataset with natural output and one suitable for long text generation. Our models outperform 10x larger language models without instruction tuning on tasks such as story/recipe generation and long-form question answering. Moreover
    
[^187]: 因果深度学习

    Causal Deep Learning

    [https://arxiv.org/abs/2303.02186](https://arxiv.org/abs/2303.02186)

    因果深度学习是一种新的关于因果性的思考方式，它综合了可测试的因果知识、参数形式和时间维度，能够帮助我们解决实际问题。

    

    因果性有望真正改变我们解决许多实际问题的方式。然而，迄今为止，因果性的潜力在很大程度上仍未得到充分发挥，因为因果性常常需要无法在实践中进行测试的关键假设。为了应对这一挑战，我们提出了一种全新的关于因果性的思考方式——我们称之为因果深度学习。我们的因果深度学习框架涵盖了三个方面：（1）结构维度，它将部分可以测试的因果知识融入，而不是假设感兴趣的变量之间的因果知识是完整的或完全不存在的；（2）参数维度，它涵盖了捕捉感兴趣的变量之间关系类型的参数形式；（3）时间维度，它捕捉感兴趣的变量如何在时间上相互作用（可能有因果关系）。因果深度学习使我们能够在各种实际问题上取得进展。

    arXiv:2303.02186v2 Announce Type: replace-cross Abstract: Causality has the potential to truly transform the way we solve a large number of real-world problems. Yet, so far, its potential largely remains to be unlocked as causality often requires crucial assumptions which cannot be tested in practice. To address this challenge, we propose a new way of thinking about causality -- we call this causal deep learning. Our causal deep learning framework spans three dimensions: (1) a structural dimension, which incorporates partial yet testable causal knowledge rather than assuming either complete or no causal knowledge among the variables of interest; (2) a parametric dimension, which encompasses parametric forms that capture the type of relationships among the variables of interest; and (3) a temporal dimension, which captures exposure times or how the variables of interest interact (possibly causally) over time. Causal deep learning enables us to make progress on a variety of real-world pr
    
[^188]: 关于时序差异学习的统计优势

    On the Statistical Benefits of Temporal Difference Learning

    [https://arxiv.org/abs/2301.13289](https://arxiv.org/abs/2301.13289)

    时序差异学习方法通过最小化连续时间步骤中的估计时序不一致度来拟合值函数，具有统计优势，可以显著减少值估计的均方误差，并且可以在两个状态的值差估计中获得显著改进。

    

    给定一个关于动作和长期奖励的数据集，直接估计方法通过将值函数与训练数据的预测误差最小化来拟合。而时序差异学习(TD)方法则通过最小化在连续时间步骤中进行的估计之间的时序不一致程度来拟合值函数。针对有限状态Markov链，我们提供了关于这种方法的统计优势的清晰渐进理论。首先，我们证明了一个直观的逆轨迹汇集系数完全刻画了值估计均方误差的百分比减少。根据问题结构的不同，这种减少可以是巨大的或不存在的。接下来，我们证明了两个状态的值差估计可以有巨大的改进：TD的误差受到问题轨迹交叉时间的界限，而这个界限可能远小于概率。

    arXiv:2301.13289v3 Announce Type: replace Abstract: Given a dataset on actions and resulting long-term rewards, a direct estimation approach fits value functions that minimize prediction error on the training data. Temporal difference learning (TD) methods instead fit value functions by minimizing the degree of temporal inconsistency between estimates made at successive time-steps. Focusing on finite state Markov chains, we provide a crisp asymptotic theory of the statistical advantages of this approach. First, we show that an intuitive inverse trajectory pooling coefficient completely characterizes the percent reduction in mean-squared error of value estimates. Depending on problem structure, the reduction could be enormous or nonexistent. Next, we prove that there can be dramatic improvements in estimates of the difference in value-to-go for two states: TD's errors are bounded in terms of a novel measure - the problem's trajectory crossing time - which can be much smaller than the pr
    
[^189]: Markov决策过程的最优决策树策略

    Optimal Decision Tree Policies for Markov Decision Processes

    [https://arxiv.org/abs/2301.13185](https://arxiv.org/abs/2301.13185)

    本研究研究了有限大小决策树在Markov决策过程（MDPs）中的优化，并提出了OMDTs：最优MDP决策树。通过混合整数线性规划直接最大化决策树的期望折扣回报。研究发现现有模仿学习技术的最优性差距，并发现它们表现为次优。

    

    强化学习策略的解释性对于许多实际任务至关重要，但学习这种可解释的策略是一个困难的问题。特别是像决策树和规则列表这样的基于规则的策略，由于其不可微性，很难进行优化。尽管现有技术可以学习可验证的决策树策略，但不能保证学习者生成的决策是最优的。在这项工作中，我们研究了有限大小决策树在Markov决策过程（MDPs）中的优化，并提出了OMDTs：最优MDP决策树。给定用户定义的大小限制和MDP形式，OMDT通过混合整数线性规划直接最大化决策树的期望折扣回报。通过为不同的MDP训练最优决策树策略，我们在实证上研究了现有模仿学习技术的最优性差距，并发现它们表现为次优。

    arXiv:2301.13185v2 Announce Type: replace Abstract: Interpretability of reinforcement learning policies is essential for many real-world tasks but learning such interpretable policies is a hard problem. Particularly rule-based policies such as decision trees and rules lists are difficult to optimize due to their non-differentiability. While existing techniques can learn verifiable decision tree policies there is no guarantee that the learners generate a decision that performs optimally. In this work, we study the optimization of size-limited decision trees for Markov Decision Processes (MPDs) and propose OMDTs: Optimal MDP Decision Trees. Given a user-defined size limit and MDP formulation OMDT directly maximizes the expected discounted return for the decision tree using Mixed-Integer Linear Programming. By training optimal decision tree policies for different MDPs we empirically study the optimality gap for existing imitation learning techniques and find that they perform sub-optimall
    
[^190]: 乐观调节的在线学习

    Optimistically Tempered Online Learning

    [https://arxiv.org/abs/2301.07530](https://arxiv.org/abs/2301.07530)

    本文提出了一种乐观调节的在线学习框架和适应算法，挑战了对专家的信心假设，并通过动态遗憾界限的理论保证和实验证明了该方法的有效性。

    

    乐观在线学习算法已经被开发出来，以利用专家意见，假设专家意见总是有用的。然而，我们可以合理地对这些意见与基于梯度的在线算法提供的学习信息的相关性提出质疑。在这项工作中，我们质疑对专家的信心假设，并开发了乐观调节（OT）在线学习框架以及在线算法的OT适应性。我们的算法具有动态遗憾界限的稳固理论保证，并最终验证了OT方法的有用性。

    arXiv:2301.07530v2 Announce Type: replace Abstract: Optimistic Online Learning algorithms have been developed to exploit expert advices, assumed optimistically to be always useful. However, it is legitimate to question the relevance of such advices \emph{w.r.t.} the learning information provided by gradient-based online algorithms. In this work, we challenge the confidence assumption on the expert and develop the \emph{optimistically tempered} (OT) online learning framework as well as OT adaptations of online algorithms. Our algorithms come with sound theoretical guarantees in the form of dynamic regret bounds, and we eventually provide experimental validation of the usefulness of the OT approach.
    
[^191]: 一个更快的$k$-means++算法

    A Faster $k$-means++ Algorithm

    [https://arxiv.org/abs/2211.15118](https://arxiv.org/abs/2211.15118)

    本文提出了一种更快的$k$-means++算法，能够在接近最优的运行时间内解决初始聚类中心选择的问题。

    

    $k$-means++是选择$k$-means聚类算法初始聚类中心的重要算法。在这项工作中，我们提出了一种新算法，可以以接近最优的运行时间解决$k$-means++问题。给定$\mathbb{R}^d$中的$n$个数据点，目前最先进的算法需要$\widetilde{O}(k)$次迭代，而每次迭代需要$\widetilde{O}(nd k)$的时间。因此，总运行时间为$\widetilde{O}(n d k^2)$。我们提出了一种新算法\textsc{FastKmeans++}，它只需要$\widetilde{O}(nd + nk^2)$的时间。

    arXiv:2211.15118v2 Announce Type: replace-cross Abstract: $k$-means++ is an important algorithm for choosing initial cluster centers for the $k$-means clustering algorithm. In this work, we present a new algorithm that can solve the $k$-means++ problem with nearly optimal running time. Given $n$ data points in $\mathbb{R}^d$, the current state-of-the-art algorithm runs in $\widetilde{O}(k )$ iterations, and each iteration takes $\widetilde{O}(nd k)$ time. The overall running time is thus $\widetilde{O}(n d k^2)$. We propose a new algorithm \textsc{FastKmeans++} that only takes in $\widetilde{O}(nd + nk^2)$ time, in total.
    
[^192]: 高维非定向图模型中的任意混合数据

    High-Dimensional Undirected Graphical Models for Arbitrary Mixed Data

    [https://arxiv.org/abs/2211.11700](https://arxiv.org/abs/2211.11700)

    本文提出了在高维非定向图模型中处理任意混合数据的方法，通过在潜变量高斯copula框架中应用经典的多项和多序相关的思想。

    

    图模型是探索复杂多变量数据中变量之间关系的重要工具。学习这些图模型的方法在所有变量都是连续或离散的情况下已经很成熟，包括高维情况。然而，在许多应用中，数据涉及不同类型的变量（例如连续、计数、二值、有序等），其联合分析是非平凡的。最近的研究进展已经展示了如何处理二值-连续情况，但是一般的混合变量类型仍然具有挑战性。在这项工作中，我们简单而有用地观察到，关于多项与多序相关的经典思想可以在潜变量高斯copula框架中得到应用。

    arXiv:2211.11700v2 Announce Type: replace-cross Abstract: Graphical models are an important tool in exploring relationships between variables in complex, multivariate data. Methods for learning such graphical models are well developed in the case where all variables are either continuous or discrete, including in high-dimensions. However, in many applications data span variables of different types (e.g. continuous, count, binary, ordinal, etc.), whose principled joint analysis is nontrivial. Latent Gaussian copula models, in which all variables are modeled as transformations of underlying jointly Gaussian variables, represent a useful approach. Recent advances have shown how the binary-continuous case can be tackled, but the general mixed variable type regime remains challenging. In this work, we make the simple yet useful observation that classical ideas concerning polychoric and polyserial correlations can be leveraged in a latent Gaussian copula framework. Building on this observati
    
[^193]: 深度强化学习指导下的作业车间调度改进启发式方法

    Deep Reinforcement Learning Guided Improvement Heuristic for Job Shop Scheduling

    [https://arxiv.org/abs/2211.10936](https://arxiv.org/abs/2211.10936)

    本文提出了一种基于深度强化学习的指导下的作业车间调度改进启发式方法，采用图表示法来编码完整的解决方案，并设计了基于图神经网络的表示方案和新颖的消息传递机制，以提高性能和加快解决方案评估。

    

    近期研究将深度强化学习（DRL）应用于解决作业车间调度问题时，主要关注的是构建启发式方法。然而，他们的性能仍远离最优，主要是因为底层图表示方案不适合对每个构建步骤中的部分解进行建模。本文提出了一种新颖的DRL指导下的作业车间调度改进启发式方法，采用图表示法来编码完整的解决方案。我们设计了一个基于图神经网络的表示方案，包含两个模块，可以有效地捕捉改进过程中遇到的动态拓扑和不同类型的节点的信息。为了在改进过程中加快解决方案评估，我们提出了一种新颖的消息传递机制，可以同时评估多个解决方案。我们证明了我们的方法的计算复杂性与问题规模呈线性关系。

    arXiv:2211.10936v3 Announce Type: replace-cross Abstract: Recent studies in using deep reinforcement learning (DRL) to solve Job-shop scheduling problems (JSSP) focus on construction heuristics. However, their performance is still far from optimality, mainly because the underlying graph representation scheme is unsuitable for modelling partial solutions at each construction step. This paper proposes a novel DRL-guided improvement heuristic for solving JSSP, where graph representation is employed to encode complete solutions. We design a Graph Neural-Network-based representation scheme, consisting of two modules to effectively capture the information of dynamic topology and different types of nodes in graphs encountered during the improvement process. To speed up solution evaluation during improvement, we present a novel message-passing mechanism that can evaluate multiple solutions simultaneously. We prove that the computational complexity of our method scales linearly with problem siz
    
[^194]: 《置换等变量量子神经网络的理论保证》

    Theoretical Guarantees for Permutation-Equivariant Quantum Neural Networks

    [https://arxiv.org/abs/2210.09974](https://arxiv.org/abs/2210.09974)

    这篇论文主要研究了置换等变量量子神经网络在量子机器学习中的应用。研究发现，这种架构能够解决传统 QNNs 遇到的局部最小值和贫瘠的高原问题，并能够在少量数据上进行良好的泛化。

    

    尽管量子机器学习模型有着巨大的潜力，但在释放其全部潜力之前，我们必须克服一些挑战。例如，基于量子神经网络（QNN）的模型可能在训练过程中遇到过多的局部最小值和贫瘠的高原问题。最近，几何量子机器学习（GQML）这一新兴领域已被提出作为其中一些问题的潜在解决方案。GQML 的关键见解是，我们应该设计编码问题的对称性的架构，例如等变 QNNs。在这里，我们专注于具有置换对称性（即对称群 $S_n$）的问题，并展示了如何构建 $S_n$-equivariant QNNs。我们对它们的性能进行了分析研究，证明它们不会遭遇贫瘠的高原问题，能够快速实现过参数化，并且能够从少量的数据中进行良好的泛化。为了验证我们的结果，我们进行了数值仿真实验。

    arXiv:2210.09974v3 Announce Type: replace-cross Abstract: Despite the great promise of quantum machine learning models, there are several challenges one must overcome before unlocking their full potential. For instance, models based on quantum neural networks (QNNs) can suffer from excessive local minima and barren plateaus in their training landscapes. Recently, the nascent field of geometric quantum machine learning (GQML) has emerged as a potential solution to some of those issues. The key insight of GQML is that one should design architectures, such as equivariant QNNs, encoding the symmetries of the problem at hand. Here, we focus on problems with permutation symmetry (i.e., the group of symmetry $S_n$), and show how to build $S_n$-equivariant QNNs. We provide an analytical study of their performance, proving that they do not suffer from barren plateaus, quickly reach overparametrization, and generalize well from small amounts of data. To verify our results, we perform numerical s
    
[^195]: FedMT: 混合类型标签的联邦学习

    FedMT: Federated Learning with Mixed-type Labels

    [https://arxiv.org/abs/2210.02042](https://arxiv.org/abs/2210.02042)

    本文提出了一种概念新颖的联邦学习设置，即具有混合类型标签的联邦学习，在其中不同的中心可以使用不同的标签准则。为了有效地训练具有混合类型标签的模型，作者提出了一种理论指导和模型无关的方法。

    

    在联邦学习（FL）中，分类器（例如深度网络）在多个中心的数据集上进行训练，而无需在这些中心之间交换数据，从而提高了样本效率。在传统的FL设置中，通常在所有参与训练的中心中采用相同的标签准则。这个限制极大地限制了FL的适用性。例如，在疾病诊断中使用的标准很可能在临床中心之间存在差异，这与传统FL的设置不匹配。在本文中，我们考虑了一个重要但尚未充分探索的FL设置，即具有混合类型标签的FL，其中各个中心可以使用不同的标签准则，从而导致中心间标签空间的差异，并对为传统设置设计的现有FL方法提出了挑战。为了有效而高效地训练具有混合类型标签的模型，我们提出了一种基于理论指导和模型无关的方法

    arXiv:2210.02042v3 Announce Type: replace-cross Abstract: In federated learning (FL), classifiers (e.g., deep networks) are trained on datasets from multiple centers without exchanging data across them, and thus improves sample efficiency. In the classical setting of FL, the same labeling criterion is usually employed across all centers being involved in training. This constraint greatly limits the applicability of FL. For example, standards used for disease diagnosis are more likely to be different across clinical centers, which mismatches the classical FL setting. In this paper, we consider an important yet under-explored setting of FL, namely FL with mixed-type labels where different labeling criteria can be employed by various centers, leading to inter-center label space differences and challenging existing FL methods designed for the classical setting. To effectively and efficiently train models with mixed-type labels, we propose a theory-guided and model-agnostic approach that ca
    
[^196]: 方差协方差正则化在自监督表示中强制实现成对独立性

    Variance Covariance Regularization Enforces Pairwise Independence in Self-Supervised Representations

    [https://arxiv.org/abs/2209.14905](https://arxiv.org/abs/2209.14905)

    本研究发现，方差协方差正则化与MLP投影器结合，在学习表示的特征之间实现了成对独立性，并对成对独立性在自监督学习和域外泛化方面的价值进行了验证。

    

    自监督学习方法（如VICReg、Barlow Twins或W-MSE）通过约束或正则化它们的投影器输出的协方差矩阵来避免联合嵌入结构的坍塌。本研究突出了这种策略的重要性质，我们称之为方差协方差正则化（VCReg）。更具体地说，我们证明了“VCReg与MLP投影器结合，在学习表示的特征之间实现了成对独立性”。这个结果通过将应用于投影器输入的核独立性标准与应用于投影器输出的VCReg相结合来得出。我们在实证上验证了我们的发现：（i）我们突出了哪些投影器的特性有利于成对独立性，（ii）我们证明成对独立性对于域外泛化具有益处，（iii）我们证明VCReg的范围超出了自监督学习，我们可以用它来解决独立成分问题。

    arXiv:2209.14905v2 Announce Type: replace Abstract: Self-Supervised Learning (SSL) methods such as VICReg, Barlow Twins or W-MSE avoid collapse of their joint embedding architectures by constraining or regularizing the covariance matrix of their projector's output. This study highlights important properties of such strategy, which we coin Variance-Covariance regularization (VCReg). More precisely, we show that {\em VCReg combined to a MLP projector enforces pairwise independence between the features of the learned representation}. This result emerges by bridging VCReg applied on the projector's output to kernel independence criteria applied on the projector's input. We empirically validate our findings where (i) we put in evidence which projector's characteristics favor pairwise independence, (ii) we demonstrate pairwise independence to be beneficial for out-of-domain generalization, (iii) we demonstrate that the scope of VCReg goes beyond SSL by using it to solve Independent Component
    
[^197]: 大规模无监督的卫星图像序列空间-时间语义分析

    Large-scale unsupervised spatio-temporal semantic analysis of vast regions from satellite images sequences

    [https://arxiv.org/abs/2208.13504](https://arxiv.org/abs/2208.13504)

    通过无监督深度嵌入和时间序列聚类的结合，本研究提出一种全面理解卫星图像序列的大规模空间-时间语义分析方法，从而解决了缺乏标注数据、地形变异性和图像复杂性等挑战。通过利用底层的空间-时间模式，实现了对感兴趣区域的全面理解。

    

    卫星图像的时间序列是分析感兴趣区域的一种极其宝贵和丰富的资源。然而，由于各种因素，如缺乏精确的标注数据、地形实体的定义和变异性，以及图像及其融合的固有复杂性，实现大规模知识的自动获取仍然是一项具有挑战性的任务。在这种背景下，我们提出了一种完全无监督和通用的方法，用于从卫星图像序列中进行大区域的空间-时间分类学。我们的方法基于深度嵌入和时间序列聚类的结合，以捕捉地面的语义特性及其随时间的演变，从而全面了解感兴趣区域。所提出的方法通过一种专门设计的新型过程来改进嵌入并利用底层的空间-时间模式。

    arXiv:2208.13504v3 Announce Type: replace-cross Abstract: Temporal sequences of satellite images constitute a highly valuable and abundant resource for analyzing regions of interest. However, the automatic acquisition of knowledge on a large scale is a challenging task due to different factors such as the lack of precise labeled data, the definition and variability of the terrain entities, or the inherent complexity of the images and their fusion. In this context, we present a fully unsupervised and general methodology to conduct spatio-temporal taxonomies of large regions from sequences of satellite images. Our approach relies on a combination of deep embeddings and time series clustering to capture the semantic properties of the ground and its evolution over time, providing a comprehensive understanding of the region of interest. The proposed method is enhanced by a novel procedure specifically devised to refine the embedding and exploit the underlying spatio-temporal patterns. We us
    
[^198]: 动态维护核密度估计数据结构：从实践到理论

    Dynamic Maintenance of Kernel Density Estimation Data Structure: From Practice to Theory

    [https://arxiv.org/abs/2208.03915](https://arxiv.org/abs/2208.03915)

    本文研究了具有对抗性查询鲁棒性的动态维护核密度估计数据结构，并提供了一个亚二次空间复杂度和亚线性更新时间的理论框架。

    

    Kernel density estimation (KDE)在机器学习中是一项具有挑战性的任务。该问题定义如下：给定一个核函数$f(x,y)$和一组点$\{x_1, x_2, \cdots, x_n \} \subset \mathbb{R}^d$，我们希望计算任意查询点$y \in \mathbb{R}^d$的$\frac{1}{n}\sum_{i=1}^{n} f(x_i,y)$。近年来，使用数据结构来高效计算KDE的趋势日益增加。然而，现有的KDE数据结构提供的是静态设置下的解决方案，对于动态变化的数据分布的鲁棒性问题并未得到解决。本文旨在研究具有对抗性查询鲁棒性的动态维护KDE数据结构。特别地，我们提供了一个KDE数据结构的理论框架。在我们的框架中，KDE数据结构只需要亚二次空间复杂度。此外，我们的数据结构支持数据集的动态更新，且更新时间为亚线性时间。

    arXiv:2208.03915v2 Announce Type: replace Abstract: Kernel density estimation (KDE) stands out as a challenging task in machine learning. The problem is defined in the following way: given a kernel function $f(x,y)$ and a set of points $\{x_1, x_2, \cdots, x_n \} \subset \mathbb{R}^d$, we would like to compute $\frac{1}{n}\sum_{i=1}^{n} f(x_i,y)$ for any query point $y \in \mathbb{R}^d$. Recently, there has been a growing trend of using data structures for efficient KDE. However, the proposed KDE data structures focus on static settings. The robustness of KDE data structures over dynamic changing data distributions is not addressed. In this work, we focus on the dynamic maintenance of KDE data structures with robustness to adversarial queries. Especially, we provide a theoretical framework of KDE data structures. In our framework, the KDE data structures only require subquadratic spaces. Moreover, our data structure supports the dynamic update of the dataset in sublinear time. Furtherm
    
[^199]: 虚假新闻: 基于GAN的逼真3D体积数据生成--系统综述和分类

    FakeNews: GAN-based generation of realistic 3D volumetric data -- A systematic review and taxonomy

    [https://arxiv.org/abs/2207.01390](https://arxiv.org/abs/2207.01390)

    这项研究综述了基于GAN的虚假新闻的生成方法和技术，探讨了其在医疗保健领域中帮助解决数据不足问题的潜力。

    

    随着数据驱动算法（如基于深度学习的方法）的大规模普及，高质量的数据的可用性变得非常重要。在医学领域，体积数据非常重要，涵盖了疾病诊断和治疗监测等方面。当数据集足够时，可以训练模型来帮助医生进行这些任务。然而，有些情况下大量数据是不可用的。例如，罕见疾病和隐私问题可能导致数据受限。在非医学领域，获取足够高质量的数据的高成本也可能是一个问题。解决这些问题的方法可以是使用生成对抗网络（GANs）生成逼真的合成数据。这些机制的存在是一项重要资源，特别是在医疗保健领域，因为数据必须具有良好的质量、逼真，并且没有隐私问题。因此，关于体积数据的大部分出版物侧重于生成高质量逼真数据的方法和技术。

    arXiv:2207.01390v2 Announce Type: replace-cross Abstract: With the massive proliferation of data-driven algorithms, such as deep learning-based approaches, the availability of high-quality data is of great interest. Volumetric data is very important in medicine, as it ranges from disease diagnoses to therapy monitoring. When the dataset is sufficient, models can be trained to help doctors with these tasks. Unfortunately, there are scenarios where large amounts of data is unavailable. For example, rare diseases and privacy issues can lead to restricted data availability. In non-medical fields, the high cost of obtaining enough high-quality data can also be a concern. A solution to these problems can be the generation of realistic synthetic data using Generative Adversarial Networks (GANs). The existence of these mechanisms is a good asset, especially in healthcare, as the data must be of good quality, realistic, and without privacy issues. Therefore, most of the publications on volumetr
    
[^200]: 提高子群体间分布稳定性的方法

    Enhancing Distributional Stability among Sub-populations

    [https://arxiv.org/abs/2206.02990](https://arxiv.org/abs/2206.02990)

    本文提出了一种称为“分布稳定性”的概念，旨在提高机器学习算法在分布变化条件下的稳定性。通过量化预测机制子群体之间的稳定性，并基于此提出了学习假设和泛化误差上界。通过稳定风险最小化（SRM）算法，我们增强了模型在预测机制变化时的稳定性。

    

    强调机器学习算法在分布变化条件下的稳定性是解决超出分布（Out-of-Distribution，OOD）泛化问题的关键。从因果学习中衍生出来的不变学习方法追求对于多个训练环境的严格不变性。虽然直观上是合理的，但是要学习严格的不变性性质需要对环境的可用性和质量做出强假设。本研究提出了“分布稳定性”概念来缓解这些限制。它在预测机制的子群体之间的特定尺度上量化了其稳定性。基于此，我们提出了学习假设，并推导了在分布变化下的泛化误差上界。在理论分析的基础上，我们提出了我们的新型稳定风险最小化（SRM）算法，以增强模型在预测机制（$Y|X$-shifts）变化时的稳定性。

    arXiv:2206.02990v2 Announce Type: replace Abstract: Enhancing the stability of machine learning algorithms under distributional shifts is at the heart of the Out-of-Distribution (OOD) Generalization problem. Derived from causal learning, recent works of invariant learning pursue strict invariance with multiple training environments. Although intuitively reasonable, strong assumptions on the availability and quality of environments are made to learn the strict invariance property. In this work, we come up with the ``distributional stability" notion to mitigate such limitations. It quantifies the stability of prediction mechanisms among sub-populations down to a prescribed scale. Based on this, we propose the learnability assumption and derive the generalization error bound under distribution shifts. Inspired by theoretical analyses, we propose our novel stable risk minimization (SRM) algorithm to enhance the model's stability w.r.t. shifts in prediction mechanisms ($Y|X$-shifts). Experi
    
[^201]: 使用Frank-Wolfe算法进行神经网络的压缩感知训练

    Compression-aware Training of Neural Networks using Frank-Wolfe

    [https://arxiv.org/abs/2205.11921](https://arxiv.org/abs/2205.11921)

    本论文提出了一种使用Frank-Wolfe算法进行神经网络压缩感知训练的框架，通过使用多功能的范数约束和SFW算法，实现了在单次密集训练中获得最先进的密集模型，并具有对压缩比鲁棒性和处理卷积滤波器剪枝和低秩矩阵分解的能力。这种方法优于现有的压缩感知方法，并且在低秩矩阵分解的情况下需要更少的计算资源。

    

    许多现有的神经网络剪枝方法要么依赖重新训练，要么在训练过程中引入强大的偏差，以便收敛到一个稀疏解。而第三种范式，“压缩感知”训练旨在通过单个密集训练运行获得最先进的密集模型，这些模型对多种压缩比都具有鲁棒性，同时避免重新训练。我们提出了一个以多功能的范数约束和随机Frank-Wolfe(SFW)算法为中心的框架，促使收敛到表现良好的解，同时在卷积滤波器剪枝和低秩矩阵分解方面具有鲁棒性。我们的方法能够优于现有的压缩感知方法，并且在低秩矩阵分解的情况下，相对于基于核范数正则化的方法，它还需要显著更少的计算资源。我们的研究结果表明，动态调整学习率直到压缩过程比较好是非常重要的。

    arXiv:2205.11921v2 Announce Type: replace Abstract: Many existing Neural Network pruning approaches rely on either retraining or inducing a strong bias in order to converge to a sparse solution throughout training. A third paradigm, 'compression-aware' training, aims to obtain state-of-the-art dense models that are robust to a wide range of compression ratios using a single dense training run while also avoiding retraining. We propose a framework centered around a versatile family of norm constraints and the Stochastic Frank-Wolfe (SFW) algorithm that encourage convergence to well-performing solutions while inducing robustness towards convolutional filter pruning and low-rank matrix decomposition. Our method is able to outperform existing compression-aware approaches and, in the case of low-rank matrix decomposition, it also requires significantly less computational resources than approaches based on nuclear-norm regularization. Our findings indicate that dynamically adjusting the lear
    
[^202]: 低秩马尔可夫决策过程中可证明高效的表示选择：从在线到离线强化学习

    Provably Efficient Representation Selection in Low-rank Markov Decision Processes: From Online to Offline RL

    [https://arxiv.org/abs/2106.11935](https://arxiv.org/abs/2106.11935)

    本论文研究了在低秩马尔可夫决策过程中表示选择对于提高强化学习效率的影响。提出了ReLEX算法，可以在在线和离线强化学习中实现高效表示学习。实验证明，在线版本ReLEX-UCB总是不比没有表示选择的最先进算法差，并在表示函数类具有“覆盖度”性质时，实现了更好的常数遗憾。对于离线版本ReLEX-LCB，可以找到最优策略。

    

    深度强化学习(DRL)的成功在于其学习适合探索和利用任务的表示。为了理解表示选择如何提高强化学习的效率，我们研究了一类低秩马尔可夫决策过程(MDP)，其中转移核能够以双线性形式表示。我们提出了一个称为ReLEX的高效算法，用于在线和离线强化学习中的表示学习。具体来说，我们展示了ReLEX的在线版本ReLEX-UCB总是不比没有表示选择的最先进算法差，并在表示函数类在整个状态-动作空间上具有“覆盖度”性质时，实现了更好的常数遗憾。对于其离线对应物ReLEX-LCB，我们展示了该算法可以找到最优策略，如果表示函数类具有“覆盖度”性质。

    arXiv:2106.11935v2 Announce Type: replace Abstract: The success of deep reinforcement learning (DRL) lies in its ability to learn a representation that is well-suited for the exploration and exploitation task. To understand how the choice of representation can improve the efficiency of reinforcement learning (RL), we study representation selection for a class of low-rank Markov Decision Processes (MDPs) where the transition kernel can be represented in a bilinear form. We propose an efficient algorithm, called ReLEX, for representation learning in both online and offline RL. Specifically, we show that the online version of ReLEX, called ReLEX-UCB, always performs no worse than the state-of-the-art algorithm without representation selection, and achieves a strictly better constant regret if the representation function class has a "coverage" property over the entire state-action space. For the offline counterpart, ReLEX-LCB, we show that the algorithm can find the optimal policy if the r
    
[^203]: 两时间尺度带马尔可夫噪声的随机逼近中心极限定理：理论和应用

    Central Limit Theorem for Two-Timescale Stochastic Approximation with Markovian Noise: Theory and Applications. (arXiv:2401.09339v1 [stat.ML])

    [http://arxiv.org/abs/2401.09339](http://arxiv.org/abs/2401.09339)

    本文通过对两时间尺度随机逼近（TTSA）的广义分析，利用中心极限定理（CLT）揭示了TTSA受马尔可夫噪声影响的耦合动力学，从而拓展了传统SGD的高效采样策略在分布式学习中的应用范围，同时研究了具有非线性函数逼近的GTD算法的统计特性。

    

    两时间尺度随机逼近（TTSA）是最通用的迭代随机算法框架之一。这包括了众所周知的随机优化方法，如SGD变种和用于双层或极小化问题的方法，以及类似梯度-based时序差异（GTD）算法的强化学习方法。本文通过中心极限定理（CLT）对带控制马尔可夫噪声的TTSA进行了深入的渐近分析，揭示了TTSA受底层马尔可夫链影响的耦合动力学，这在以前仅考虑鞅差异噪声的TTSA的CLT结果中没有得到解决。基于我们的CLT，我们将高效采样策略的应用范围从传统SGD扩展到了更广泛的TTSA背景下的分布式学习，从而扩大了胡等人（2022）的研究范围。此外，我们利用我们的CLT结果推导了具有非线性函数逼近的GTD算法的统计特性。

    Two-timescale stochastic approximation (TTSA) is among the most general frameworks for iterative stochastic algorithms. This includes well-known stochastic optimization methods such as SGD variants and those designed for bilevel or minimax problems, as well as reinforcement learning like the family of gradient-based temporal difference (GTD) algorithms. In this paper, we conduct an in-depth asymptotic analysis of TTSA under controlled Markovian noise via central limit theorem (CLT), uncovering the coupled dynamics of TTSA influenced by the underlying Markov chain, which has not been addressed by previous CLT results of TTSA only with Martingale difference noise. Building upon our CLT, we expand its application horizon of efficient sampling strategies from vanilla SGD to a wider TTSA context in distributed learning, thus broadening the scope of Hu et al. (2022). In addition, we leverage our CLT result to deduce the statistical properties of GTD algorithms with nonlinear function approxi
    
[^204]: 使用特征变换进行交通市场利率预测

    Transportation Market Rate Forecast Using Signature Transform. (arXiv:2401.04857v1 [cs.LG])

    [http://arxiv.org/abs/2401.04857](http://arxiv.org/abs/2401.04857)

    本论文提出了一种基于特征变换的新型统计方法，用于解决交通市场利率的预测挑战。该方法具有通用的非线性属性和特征变换核函数，能够高效生成特征，并在预测过程中准确识别季节性和制度转换。

    

    目前，亚马逊在交通市场利率预测上依赖第三方，尽管这些预测质量差且缺乏可解释性。虽然交通市场利率通常很难准确预测，但我们开发了一种基于特征变换的新型统计技术来解决这些挑战，并构建了一个预测和自适应模型来预测市场利率。这种新技术基于特征变换的两个关键属性。第一个是其通用的非线性，它线性化特征空间，从而将预测问题转化为线性回归分析；第二个是特征变换核函数，它允许在时间序列数据之间进行计算有效的相似性比较。结合起来，这些属性允许进行高效的特征生成，并在预测过程中更精确地识别季节性和制度转换。模型的初步结果显示，这种新方法可以改善市场利率的预测性能。

    Currently, Amazon relies on third parties for transportation marketplace rate forecasts, despite the poor quality and lack of interpretability of these forecasts. While transportation marketplace rates are typically very challenging to forecast accurately, we have developed a novel signature-based statistical technique to address these challenges and built a predictive and adaptive model to forecast marketplace rates. This novel technique is based on two key properties of the signature transform. The first is its universal nonlinearity which linearizes the feature space and hence translates the forecasting problem into a linear regression analysis; the second is the signature kernel which allows for comparing computationally efficiently similarities between time series data. Combined, these properties allow for efficient feature generation and more precise identification of seasonality and regime switching in the forecasting process. Preliminary result by the model shows that this new 
    
[^205]: SoK：评估黑盒攻击中的陷阱

    SoK: Pitfalls in Evaluating Black-Box Attacks. (arXiv:2310.17534v1 [cs.CR])

    [http://arxiv.org/abs/2310.17534](http://arxiv.org/abs/2310.17534)

    提出了一个评估黑盒攻击的分类法，揭示了未开发的威胁空间，并展示了在某些设置上已有技术的局限性。

    

    许多研究涉及对图像分类器的黑盒攻击。然而，这些研究对对手的知识假设不同，目前的文献缺乏围绕威胁模型进行组织的连贯性。为了系统化该领域的知识，我们提出了一个关于威胁空间的分类法，涵盖了反馈粒度、交互式查询的访问和攻击者可用的辅助数据的质量和数量三个维度。我们的新分类法提供了三个关键见解。1) 尽管有广泛文献，仍存在许多未开发的威胁空间，无法通过从已知领域的技术简单地改进来解决。我们通过将基于已知领域从完整置信向量访问的技术适应到访问前k个置信得分的较少研究的设置中来证明这一点，但同时也展示了它在仅获得预测标签的更严格设置中仍然不足。

    Numerous works study black-box attacks on image classifiers. However, these works make different assumptions on the adversary's knowledge and current literature lacks a cohesive organization centered around the threat model. To systematize knowledge in this area, we propose a taxonomy over the threat space spanning the axes of feedback granularity, the access of interactive queries, and the quality and quantity of the auxiliary data available to the attacker. Our new taxonomy provides three key insights. 1) Despite extensive literature, numerous under-explored threat spaces exist, which cannot be trivially solved by adapting techniques from well-explored settings. We demonstrate this by establishing a new state-of-the-art in the less-studied setting of access to top-k confidence scores by adapting techniques from well-explored settings of accessing the complete confidence vector, but show how it still falls short of the more restrictive setting that only obtains the prediction label, h
    
[^206]: 通用因果表达学习的可识别性和可实现性

    General Identifiability and Achievability for Causal Representation Learning. (arXiv:2310.15450v1 [cs.LG])

    [http://arxiv.org/abs/2310.15450](http://arxiv.org/abs/2310.15450)

    本文在通用非参数因果潜变量模型和通用转换模型下，通过非耦合干预建立了因果表达学习的可识别性和可实现性结果。在不知道具体干预对应的节点的情况下，这些结果保证了潜在的因果模型和变量的完美恢复，并设计了一个算法来实现这一目标。

    

    本文关注通用非参数因果潜变量模型和将潜变量数据映射到观测数据的通用转换模型下的因果表达学习。通过在潜在因果图中每个节点进行两个硬性非耦合干预来建立可识别性和可实现性结果。值得注意的是，人们不知道哪个干预环境对应的节点是相同的（因此是非耦合环境）。在可识别性方面，本文确保在非耦合干预下能够完美恢复潜在的因果模型和变量。在可实现性方面，设计了一个算法，利用观测和干预数据，并提供了对该算法的可验证的保证，以恢复潜在的因果模型和变量。该算法利用不同环境中的得分变化来估计转换器的逆和随后的潜变量。该分析还...

    This paper focuses on causal representation learning (CRL) under a general nonparametric causal latent model and a general transformation model that maps the latent data to the observational data. It establishes \textbf{identifiability} and \textbf{achievability} results using two hard \textbf{uncoupled} interventions per node in the latent causal graph. Notably, one does not know which pair of intervention environments have the same node intervened (hence, uncoupled environments). For identifiability, the paper establishes that perfect recovery of the latent causal model and variables is guaranteed under uncoupled interventions. For achievability, an algorithm is designed that uses observational and interventional data and recovers the latent causal model and variables with provable guarantees for the algorithm. This algorithm leverages score variations across different environments to estimate the inverse of the transformer and, subsequently, the latent variables. The analysis, addit
    
[^207]: 贝叶斯主动元学习的基本困境

    The Fundamental Dilemma of Bayesian Active Meta-learning. (arXiv:2310.14968v1 [cs.LG])

    [http://arxiv.org/abs/2310.14968](http://arxiv.org/abs/2310.14968)

    在贝叶斯主动元学习中，贪婪追求可转移知识可能会损害对可转移参数的估计，学习者面临任务识别和可转移知识获取之间的困境。

    

    许多应用需要估计在多个不同但相关的数据稀缺任务环境中推广的参数。贝叶斯主动元学习是一种顺序最优实验设计的形式，为解决这类问题提供了一个框架。主动元学习者的目标是在当前任务的特殊特征（任务特定参数）的情况下获得可转移的知识（估计可转移的参数）。我们证明，在这种情况下，贪婪追求这个目标实际上可能会损害对可转移参数的估计（引起所谓的负迁移）。学习者面临着一个类似但不同于勘探-利用困境的困境：他们应该花费他们的获取预算来追求可转移的知识，还是用来确定当前任务特定的参数？我们理论上证明，一些任务存在不可避免且任意大的负迁移威胁，任务的识别对于重新寻找可迁移参数至关重要。

    Many applications involve estimation of parameters that generalize across multiple diverse, but related, data-scarce task environments. Bayesian active meta-learning, a form of sequential optimal experimental design, provides a framework for solving such problems. The active meta-learner's goal is to gain transferable knowledge (estimate the transferable parameters) in the presence of idiosyncratic characteristics of the current task (task-specific parameters). We show that in such a setting, greedy pursuit of this goal can actually hurt estimation of the transferable parameters (induce so-called negative transfer). The learner faces a dilemma akin to but distinct from the exploration--exploitation dilemma: should they spend their acquisition budget pursuing transferable knowledge, or identifying the current task-specific parameters? We show theoretically that some tasks pose an inevitable and arbitrarily large threat of negative transfer, and that task identification is critical to re
    
[^208]: 开放式多元时间序列异常检测

    Open-Set Multivariate Time-Series Anomaly Detection. (arXiv:2310.12294v1 [cs.LG])

    [http://arxiv.org/abs/2310.12294](http://arxiv.org/abs/2310.12294)

    本论文提出了一种开放式时间序列异常检测方法，能够在训练阶段识别有限异常类别的少量标记异常，并在测试阶段检测到见过和未见过的异常类别。

    

    近年来出现了许多时间序列异常检测方法。大多数现有方法是无监督的，仅假设有正常的训练样本，而一些监督方法通过在训练阶段加入标记的异常样本来提高性能。然而，某些异常类型对无监督方法来说在区分正常数据时具有挑战性，而监督方法仅能检测类似于训练期间存在的异常，无法推广到未见异常类别。本文首次提出了一种针对开放式时间序列异常检测问题的新方法，在训练阶段可以看到来自有限异常类别的少量标记异常，并旨在在测试阶段检测到见过和未见过的异常类别。所提出的方法被称为多变量开放式时间序列异常检测（MOSAD），包括三个主要步骤。

    Numerous methods for time series anomaly detection (TSAD) methods have emerged in recent years. Most existing methods are unsupervised and assume the availability of normal training samples only, while few supervised methods have shown superior performance by incorporating labeled anomalous samples in the training phase. However, certain anomaly types are inherently challenging for unsupervised methods to differentiate from normal data, while supervised methods are constrained to detecting anomalies resembling those present during training, failing to generalize to unseen anomaly classes. This paper is the first attempt in providing a novel approach for the open-set TSAD problem, in which a small number of labeled anomalies from a limited class of anomalies are visible in the training phase, with the objective of detecting both seen and unseen anomaly classes in the test phase. The proposed method, called Multivariate Open-Set timeseries Anomaly Detection (MOSAD) consists of three prim
    
[^209]: 基于MMD的分布随机森林的变量重要性

    MMD-based Variable Importance for Distributional Random Forest. (arXiv:2310.12115v1 [stat.ML])

    [http://arxiv.org/abs/2310.12115](http://arxiv.org/abs/2310.12115)

    本文介绍了基于MMD距离和经典的drop and relearn原理的变量重要性算法，可以在分布随机森林中检测影响输出分布的变量，并且在实证性能上超越了竞争对手。

    

    分布随机森林（DRF）是一种灵活的基于森林的方法，用于估计给定输入变量的多元输出的全条件分布。在本文中，我们介绍了一种基于经典的drop and relearn原理和MMD距离的DRF变量重要性算法。传统的重要性度量只能发现对输出均值有影响的变量，而我们的算法可以更普遍地发现影响输出分布的变量。我们展示了引入的重要性度量是一致的，在真实数据和模拟数据上具有较高的实证性能，并且超越了竞争对手。特别地，我们的算法通过递归特征消除高效地选择变量，因此可以提供小型变量集合来构建准确的条件输出分布估计。

    Distributional Random Forest (DRF) is a flexible forest-based method to estimate the full conditional distribution of a multivariate output of interest given input variables. In this article, we introduce a variable importance algorithm for DRFs, based on the well-established drop and relearn principle and MMD distance. While traditional importance measures only detect variables with an influence on the output mean, our algorithm detects variables impacting the output distribution more generally. We show that the introduced importance measure is consistent, exhibits high empirical performance on both real and simulated data, and outperforms competitors. In particular, our algorithm is highly efficient to select variables through recursive feature elimination, and can therefore provide small sets of variables to build accurate estimates of conditional output distributions.
    
[^210]: DPZero：与维度无关且具有差分隐私的零阶优化算法

    DPZero: Dimension-Independent and Differentially Private Zeroth-Order Optimization. (arXiv:2310.09639v1 [cs.LG])

    [http://arxiv.org/abs/2310.09639](http://arxiv.org/abs/2310.09639)

    该论文提出了DPZero算法，这是一种与维度无关且具有差分隐私的零阶优化算法，用于解决在细调大型语言模型时面临的内存和隐私挑战。

    

    在细调预训练的大型语言模型（LLM）以适应特定领域数据的广泛实践中，面临着内存和隐私两个主要挑战。首先，随着LLM的规模不断增长，达到数十亿个参数，基于梯度的反向传播训练方法所需的内存消耗变得难以承受。其次，考虑到LLM倾向于记忆和泄露敏感的训练数据，必须保护细调数据的隐私。为此，我们探索了将零阶方法与差分隐私优化相结合用于LLM的细调的潜力。零阶方法仅依赖前向传递，大大减少了训练过程中的内存消耗。然而，直接将它们与标准的差分隐私机制结合在一起会导致维度相关的复杂性。为了弥合这一差距，我们引入了DPZero，一种具有近乎维度无关率的新型差分隐私零阶算法。我们的理论分析揭示出了

    The widespread practice of fine-tuning pretrained large language models (LLMs) on domain-specific data faces two major challenges in memory and privacy. First, as the size of LLMs continue to grow, encompassing billions of parameters, the memory demands of gradient-based training methods via backpropagation become prohibitively high. Second, given the tendency of LLMs to memorize and disclose sensitive training data, the privacy of fine-tuning data must be respected. To this end, we explore the potential of zeroth-order methods in differentially private optimization for fine-tuning LLMs. Zeroth-order methods, which rely solely on forward passes, substantially reduce memory consumption during training. However, directly combining them with standard differential privacy mechanism poses dimension-dependent complexity. To bridge the gap, we introduce DPZero, a novel differentially private zeroth-order algorithm with nearly dimension-independent rates. Our theoretical analysis reveals that 
    
[^211]: 一个求解最小最大相关聚类问题的4近似算法

    A 4-approximation algorithm for min max correlation clustering. (arXiv:2310.09196v1 [cs.DS])

    [http://arxiv.org/abs/2310.09196](http://arxiv.org/abs/2310.09196)

    本论文引入了一种新的下界技术，并提出了一个组合算法来求解最小最大相关聚类问题，该算法能够在完全图上达到4的近似结果。通过贪婪联合启发式算法的扩展和实验证明，在多个基准数据集上，该算法在解决方案质量和运行时间上均有显著改进。

    

    我们引入了一种求解最小最大相关聚类问题的下界技术，并基于此技术，提出了一个针对完全图的组合4近似算法。这改进了之前使用线性规划公式（Kalhan等，2019）获得的近似保证为5和使用组合算法（Davies等，2023）获得的近似保证为4的最佳已知结果。我们通过贪婪联合启发式算法扩展了该算法，并通过实验证明，在几个基准数据集上，它提高了解决方案质量和运行时间的技术水平。

    We introduce a lower bounding technique for the min max correlation clustering problem and, based on this technique, a combinatorial 4-approximation algorithm for complete graphs. This improves upon the previous best known approximation guarantees of 5, using a linear program formulation (Kalhan et al., 2019), and 4, for a combinatorial algorithm (Davies et al., 2023). We extend this algorithm by a greedy joining heuristic and show empirically that it improves the state of the art in solution quality and runtime on several benchmark datasets.
    
[^212]: 进化动态优化与机器学习

    Evolutionary Dynamic Optimization and Machine Learning. (arXiv:2310.08748v1 [cs.NE])

    [http://arxiv.org/abs/2310.08748](http://arxiv.org/abs/2310.08748)

    进化计算和机器学习的结合为优化复杂的机器学习任务提供了有价值的机会，并通过利用进化计算算法生成的数据来提供对搜索空间和种群动态的洞察。

    

    进化计算(EC)作为一种受自然渐进发展机制启发的强大的人工智能领域已经出现。然而，EC方法常常面临停滞、多样性损失、计算复杂性、种群初始化和过早收敛等挑战。为了克服这些限制，研究人员将学习算法与进化技术相结合。这种集成利用了EC算法在迭代搜索过程中生成的有价值的数据，提供了对搜索空间和种群动态的洞察。类似地，进化算法与机器学习(ML)之间的关系是相互的，因为EC方法为优化噪声、不准确和动态目标函数所描述的复杂ML任务提供了极好的机会。这些混合技术被称为进化机器学习(EML)，已在ML过程的各个阶段应用。

    Evolutionary Computation (EC) has emerged as a powerful field of Artificial Intelligence, inspired by nature's mechanisms of gradual development. However, EC approaches often face challenges such as stagnation, diversity loss, computational complexity, population initialization, and premature convergence. To overcome these limitations, researchers have integrated learning algorithms with evolutionary techniques. This integration harnesses the valuable data generated by EC algorithms during iterative searches, providing insights into the search space and population dynamics. Similarly, the relationship between evolutionary algorithms and Machine Learning (ML) is reciprocal, as EC methods offer exceptional opportunities for optimizing complex ML tasks characterized by noisy, inaccurate, and dynamic objective functions. These hybrid techniques, known as Evolutionary Machine Learning (EML), have been applied at various stages of the ML process. EC techniques play a vital role in tasks such
    
[^213]: 重复委托选择的后悔分析

    Regret Analysis of Repeated Delegated Choice. (arXiv:2310.04884v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2310.04884](http://arxiv.org/abs/2310.04884)

    本研究针对重复委托选择问题展开研究，通过动态宣布合格集合以学习解决方案分布来减轻代理人的自私行为，进而最小化与最优合格集合之间的后悔。

    

    我们对重复委托选择问题进行了研究，这是第一个考虑克莱因伯格和克莱因伯格(EC'18)在线学习变种的模型。在该模型中，一个委托人与一个拥有外生解集的代理人进行重复互动，以寻找高效的解决方案。每个解决方案对委托人和代理人都可以产生不同的效用，代理人可能以自私的方式提出解决方案，以最大化自身的效用。为了减轻这种行为，委托人宣布一个合格集合，筛掉一定的解决方案。然而，委托人事先并不了解解决方案的分布情况。因此，委托人动态地宣布各种合格集合，以有效地学习分布。委托人的目标是最小化与事后最优合格集合相比的累积后悔。我们探讨了问题设置的两个维度，即代理人是根据眼前利益行事还是在回合之间制定策略。

    We present a study on a repeated delegated choice problem, which is the first to consider an online learning variant of Kleinberg and Kleinberg, EC'18. In this model, a principal interacts repeatedly with an agent who possesses an exogenous set of solutions to search for efficient ones. Each solution can yield varying utility for both the principal and the agent, and the agent may propose a solution to maximize its own utility in a selfish manner. To mitigate this behavior, the principal announces an eligible set which screens out a certain set of solutions. The principal, however, does not have any information on the distribution of solutions in advance. Therefore, the principal dynamically announces various eligible sets to efficiently learn the distribution. The principal's objective is to minimize cumulative regret compared to the optimal eligible set in hindsight. We explore two dimensions of the problem setup, whether the agent behaves myopically or strategizes across the rounds,
    
[^214]: 使用量子统计查询学习量子过程

    Learning Quantum Processes with Quantum Statistical Queries. (arXiv:2310.02075v1 [quant-ph] CROSS LISTED)

    [http://arxiv.org/abs/2310.02075](http://arxiv.org/abs/2310.02075)

    本文提出了第一个在量子统计查询模型内学习量子过程的框架，并提供了一个高效的学习器和可证明的性能保证。通过在密码分析中的应用，揭示了经典读出量子物理不可克隆函数的脆弱性，这是量子硬件安全领域一个重要的开放问题的解决方法。

    

    学习复杂的量子过程是量子计算和量子机器学习的许多领域的一个核心挑战，应用于量子基准测试、密码分析和变分量子算法。本文引入了第一个学习框架，用于在量子统计查询（QSQ）模型内研究量子过程学习，提供了对量子过程（QPSQs）进行统计查询的第一个正式定义。该框架使我们能够提出一种高效的QPSQ学习器，适用于任意量子过程，并附带可证明的性能保证。我们还提供了数值模拟来展示该算法的有效性。通过在密码分析中应用该框架，突出了经典读出量子物理不可克隆函数（CR-QPUFs）的脆弱性，解决了量子硬件安全领域中的一个重要开放问题。这项工作是朝着深入理解量子过程学习迈出的重要一步。

    Learning complex quantum processes is a central challenge in many areas of quantum computing and quantum machine learning, with applications in quantum benchmarking, cryptanalysis, and variational quantum algorithms. This paper introduces the first learning framework for studying quantum process learning within the Quantum Statistical Query (QSQ) model, providing the first formal definition of statistical queries to quantum processes (QPSQs). The framework allows us to propose an efficient QPSQ learner for arbitrary quantum processes accompanied by a provable performance guarantee. We also provide numerical simulations to demonstrate the efficacy of this algorithm. The practical relevance of this framework is exemplified through application in cryptanalysis, highlighting vulnerabilities of Classical-Readout Quantum Physical Unclonable Functions (CR-QPUFs), addressing an important open question in the field of quantum hardware security. This work marks a significant step towards underst
    
[^215]: 生成分类器的有趣属性

    Intriguing properties of generative classifiers. (arXiv:2309.16779v1 [cs.CV])

    [http://arxiv.org/abs/2309.16779](http://arxiv.org/abs/2309.16779)

    生成分类器展示了记录破纪录的人类形状偏好、接近人类级别的超出分布准确性、与人类分类错误的最先进对齐以及理解某些知觉幻象的新兴特性，揭示了零样本生成模型出奇地接近人类物体识别数据。

    

    识别对象的最佳范式是判别式推理（快速但潜在容易出现快捷学习）还是使用生成模型（较慢但潜在更稳健）？我们借鉴了最新的生成模型进展，将文本到图像模型转化为分类器。这使得我们能够研究其行为，并将其与判别模型和人类心理物理数据进行比较。我们报道了生成分类器的四个有趣的新兴特性：它们显示出破纪录的人类形状偏好（对于Imagen达到99%），接近人类级别的超出分布准确性，与人类分类错误的最先进对齐以及它们理解某些知觉幻象。我们的结果表明，尽管目前模拟人类物体识别的主导范式是判别式推理，零样本生成模型出奇地接近人类物体识别数据。

    What is the best paradigm to recognize objects -- discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data. We report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.
    
[^216]: 异质搜索空间上的贝叶斯优化的迁移学习

    Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces. (arXiv:2309.16597v1 [cs.LG])

    [http://arxiv.org/abs/2309.16597](http://arxiv.org/abs/2309.16597)

    本文提出了MPHD方法，通过模型预训练和神经网络在异质搜索空间上实现贝叶斯优化的迁移学习。实验证明了MPHD的有效性和在黑盒函数优化任务中的优越性能。

    

    贝叶斯优化是一种流行的黑盒函数优化方法，它基于贝叶斯模型（通常是高斯过程）进行顺序决策。为了确保模型的质量，我们开发了迁移学习方法，通过学习来自“训练”函数的观察结果来自动设计高斯过程先验。这些训练函数通常需要与“测试”函数（待优化的黑盒函数）具有相同的定义域。在本文中，我们介绍了一种名为MPHD的模型预训练方法，它使用神经网络将特定于领域的上下文映射到分层高斯过程的规范。MPHD可以与贝叶斯优化无缝集成，实现异质搜索空间的知识迁移。我们的理论和实证结果证明了MPHD的有效性，并展示了它在具有挑战性的黑盒函数优化任务中的优越性能。

    Bayesian optimization (BO) is a popular black-box function optimization method, which makes sequential decisions based on a Bayesian model, typically a Gaussian process (GP), of the function. To ensure the quality of the model, transfer learning approaches have been developed to automatically design GP priors by learning from observations on "training" functions. These training functions are typically required to have the same domain as the "test" function (black-box function to be optimized). In this paper, we introduce MPHD, a model pre-training method on heterogeneous domains, which uses a neural net mapping from domain-specific contexts to specifications of hierarchical GPs. MPHD can be seamlessly integrated with BO to transfer knowledge across heterogeneous search spaces. Our theoretical and empirical results demonstrate the validity of MPHD and its superior performance on challenging black-box function optimization tasks.
    
[^217]: 基于深度学习的吸引盆分析

    Deep Learning-based Analysis of Basins of Attraction. (arXiv:2309.15732v1 [cs.LG])

    [http://arxiv.org/abs/2309.15732](http://arxiv.org/abs/2309.15732)

    本研究展示了基于深度学习的卷积神经网络方法在表征各种动力系统的吸引盆的复杂性和不可预测性方面的有效性，相比传统方法，该方法具有更低的计算成本且表现更好。

    

    本研究展示了卷积神经网络（CNN）在表征不同动力系统吸引盆的复杂性和不可预测性方面的有效性。这种新颖的方法在探索动力系统的不同参数方面是最优的，因为传统方法在表征多个吸引盆时计算成本很高。此外，我们的研究还比较了不同CNN体系结构在这个任务中的表现，表明我们提出的特征提取方法相比传统方法具有优势，即使使用过时的体系结构也是如此。

    This study showcases the effectiveness of convolutional neural networks (CNNs) in characterizing the complexity and unpredictability of basins of attraction for diverse dynamical systems. This novel method is optimal for exploring different parameters of dynamical systems since the conventional methods are computationally expensive for characterizing multiple basins of attraction. Additionally, our research includes a comparison of different CNN architectures for this task showing the superiority of our proposed characterization method over the conventional methods, even with obsolete architectures.
    
[^218]: 高能物理中的简化模拟：数据驱动物理研究的折中方法

    Reduced Simulations for High-Energy Physics, a Middle Ground for Data-Driven Physics Research. (arXiv:2309.03780v1 [hep-ex])

    [http://arxiv.org/abs/2309.03780](http://arxiv.org/abs/2309.03780)

    该论文提出了一种简化模拟的方法，通过使用红外探测器模型和粒子碰撞事件模拟器来生成简化数据，以便于高能物理研究和教育中的机器学习模型设计和解决方案探索。

    

    亚原子粒子轨迹重建（追踪）是高能物理实验中的关键任务。追踪任务在计算上极具挑战性，目前使用的传统算法解决方案不能线性扩展。机器学习辅助的解决方案是一个有前途的答案。我们认为，简化了的问题描述和所代表的数据将有助于解决方案的探索工作流程。我们提供了经过简化的虚拟探测器（REDVID）作为复杂度简化探测器模型和粒子碰撞事件模拟器的组合。REDVID旨在作为一个模拟-循环来高效生成合成数据，并简化机器学习模型设计的挑战。与物理精确模拟相比，我们工具的完全参数化特性允许生成不同层次的研究和教育简化数据。由于简化的复杂性，我们展示了

    Subatomic particle track reconstruction (tracking) is a vital task in High-Energy Physics experiments. Tracking is exceptionally computationally challenging and fielded solutions, relying on traditional algorithms, do not scale linearly. Machine Learning (ML) assisted solutions are a promising answer. We argue that a complexity-reduced problem description and the data representing it, will facilitate the solution exploration workflow. We provide the REDuced VIrtual Detector (REDVID) as a complexity-reduced detector model and particle collision event simulator combo. REDVID is intended as a simulation-in-the-loop, to both generate synthetic data efficiently and to simplify the challenge of ML model design. The fully parametric nature of our tool, with regards to system-level configuration, while in contrast to physics-accurate simulations, allows for the generation of simplified data for research and education, at different levels. Resulting from the reduced complexity, we showcase the 
    
[^219]: 使用组合扩散模型实现训练数据保护

    Training Data Protection with Compositional Diffusion Models. (arXiv:2308.01937v1 [cs.LG])

    [http://arxiv.org/abs/2308.01937](http://arxiv.org/abs/2308.01937)

    使用分区扩散模型（CDM）训练不同的扩散模型，并在推断时任意组合它们，实现了训练数据保护和选择性遗忘，同时还可以根据用户访问权限提供定制模型。

    

    我们引入了分区扩散模型（CDM），一种在不同数据源上训练不同扩散模型（或提示）并在推断时任意组合它们的方法。这些单独的模型可以在孤立状态下、在不同时间、在不同分布和领域上进行训练，并可以后续组合以达到与同时训练所有数据的理想模型相当的性能。此外，每个模型只包含其在训练期间接触到的数据子集的信息，可以实现多种形式的训练数据保护。特别是，CDM是第一种可以实现大规模扩散模型的选择性遗忘和持续学习的方法，并且允许根据用户访问权限提供定制模型。CDM还可以确定生成特定样本的数据子集的重要性。

    We introduce Compartmentalized Diffusion Models (CDM), a method to train different diffusion models (or prompts) on distinct data sources and arbitrarily compose them at inference time. The individual models can be trained in isolation, at different times, and on different distributions and domains and can be later composed to achieve performance comparable to a paragon model trained on all data simultaneously. Furthermore, each model only contains information about the subset of the data it was exposed to during training, enabling several forms of training data protection. In particular, CDMs are the first method to enable both selective forgetting and continual learning for large-scale diffusion models, as well as allowing serving customized models based on the user's access rights. CDMs also allow determining the importance of a subset of the data in generating particular samples.
    
[^220]: 隐式归一化显式正则化密度估计

    Implicitly Normalized Explicitly Regularized Density Estimation. (arXiv:2307.13763v1 [stat.ML])

    [http://arxiv.org/abs/2307.13763](http://arxiv.org/abs/2307.13763)

    我们提出了一种新的非参数密度估计方法，基于正则化密度的 Sobolev 范数，通过适当的初始化和使用自然梯度，可以得到性能良好的解，该方法在 Anomaly Detection benchmark 中排名第二。

    

    我们提出了一种新的非参数密度估计方法，该方法是基于正则化密度的 Sobolev 范数。这种方法与核密度估计有明显差异，可以清晰解释模型的偏差。虽然我们无法得到相关核函数的闭合解析形式，但我们证明可以通过采样进行近似。决定密度的优化问题是非凸的，标准的梯度方法效果不好。然而，我们证明在适当的初始化和使用自然梯度的情况下，可以得到性能良好的解。最后，虽然该方法提供的是非归一化的密度，无法使用对数似然进行交叉验证，但我们证明可以采用基于 Fisher 散度的分数匹配方法来解决这个问题。我们在最近的异常检测基准套件 ADBench 上评估了得到的方法，并发现它在超过15个算法中排名第二。

    We propose a new approach to non-parametric density estimation, that is based on regularizing a Sobolev norm of the density. This method is provably different from Kernel Density Estimation, and makes the bias of the model clear and interpretable. While there is no closed analytic form for the associated kernel, we show that one can approximate it using sampling. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, we show that with an appropriate initialization and using natural gradients, one can obtain well performing solutions. Finally, while the approach provides unnormalized densities, which prevents the use of log-likelihood for cross validation, we show that one can instead adapt Fisher Divergence based Score Matching methods for this task. We evaluate the resulting method on the comprehensive recent Anomaly Detection benchmark suite, ADBench, and find that it ranks second best, among more than 15 al
    
[^221]: 基于金属氧化物的气体传感器阵列在复杂混合物中利用机器学习进行VOCs分析

    Metal Oxide-based Gas Sensor Array for the VOCs Analysis in Complex Mixtures using Machine Learning. (arXiv:2307.06556v1 [physics.app-ph])

    [http://arxiv.org/abs/2307.06556](http://arxiv.org/abs/2307.06556)

    本研究提出了一种基于金属氧化物传感器阵列和机器学习的方法，可以在复杂混合物中识别出不同的挥发性有机化合物（VOCs），且取得了很高的准确率和回归分析结果。

    

    呼吸中挥发性有机化合物（VOCs）的检测正在成为一种非侵入性疾病早期检测的可行途径。本文提出了一种具有三个金属氧化物电极的传感器阵列，可以使用机器学习方法在混合物中识别出四种不同的VOCs。金属氧化物传感器阵列经过不同VOC浓度的测试，包括乙醇、丙酮、甲苯和氯仿。从单一气体和混合物获得的数据集使用多个机器学习算法进行分析，如随机森林（RF）、K最近邻（KNN）、决策树、线性回归、逻辑回归、朴素贝叶斯、线性判别分析、人工神经网络和支持向量机。KNN和RF在对气体混合物中的不同变化化学品进行分类方面的准确率超过99%。在回归分析中，KNN的结果最好，R2值超过0.99，LOD值为0.012、0.015、0.014和0.025 PPM。

    Detection of Volatile Organic Compounds (VOCs) from the breath is becoming a viable route for the early detection of diseases non-invasively. This paper presents a sensor array with three metal oxide electrodes that can use machine learning methods to identify four distinct VOCs in a mixture. The metal oxide sensor array was subjected to various VOC concentrations, including ethanol, acetone, toluene and chloroform. The dataset obtained from individual gases and their mixtures were analyzed using multiple machine learning algorithms, such as Random Forest (RF), K-Nearest Neighbor (KNN), Decision Tree, Linear Regression, Logistic Regression, Naive Bayes, Linear Discriminant Analysis, Artificial Neural Network, and Support Vector Machine. KNN and RF have shown more than 99% accuracy in classifying different varying chemicals in the gas mixtures. In regression analysis, KNN has delivered the best results with R2 value of more than 0.99 and LOD of 0.012, 0.015, 0.014 and 0.025 PPM for pred
    
[^222]: 利用李对称的自监督学习解决偏微分方程问题

    Self-Supervised Learning with Lie Symmetries for Partial Differential Equations. (arXiv:2307.05432v1 [cs.LG])

    [http://arxiv.org/abs/2307.05432](http://arxiv.org/abs/2307.05432)

    本研究通过自监督学习的方法，利用李对称将异构数据中的PDEs表示进行优化，提高了不变任务的性能并改进了神经求解器的时间推进性能。

    

    差分方程的机器学习为数值求解器提供了计算效率高的替代方法，可能在科学和工程领域产生广泛影响。本研究通过实施联合嵌入方法的自监督学习（SSL）框架，从异构数据中学习PDEs的通用表示，该框架是一种无监督表示学习方法，在计算机视觉领域取得了显著的成功。我们的表示优于基线方法在不变任务（如回归PDE的系数）上的表现，同时提高神经求解器的时间推进性能。我们希望我们提出的方法将在未来的通用基础模型的发展中发挥作用。

    Machine learning for differential equations paves the way for computationally efficient alternatives to numerical solvers, with potentially broad impacts in science and engineering. Though current algorithms typically require simulated training data tailored to a given setting, one may instead wish to learn useful information from heterogeneous sources, or from real dynamical systems observations that are messy or incomplete. In this work, we learn general-purpose representations of PDEs from heterogeneous data by implementing joint embedding methods for self-supervised learning (SSL), a framework for unsupervised representation learning that has had notable success in computer vision. Our representation outperforms baseline approaches to invariant tasks, such as regressing the coefficients of a PDE, while also improving the time-stepping performance of neural solvers. We hope that our proposed methodology will prove useful in the eventual development of general-purpose foundation mode
    
[^223]: 理解深度异方差回归的病态

    Understanding Pathologies of Deep Heteroskedastic Regression. (arXiv:2306.16717v1 [stat.ML])

    [http://arxiv.org/abs/2306.16717](http://arxiv.org/abs/2306.16717)

    该论文研究了利用异方差神经回归模型对真实世界数据进行建模时的困难，并从统计物理的角度提供了解释。作者证明了这些不稳定性不仅适用于神经网络结构，而且已经在过参数化条件高斯似然模型的场论中存在。数值求解结果与实证模型拟合的定性一致性证明了相变的存在。

    

    近期的研究报告了在使用异方差神经回归模型对真实世界数据建模时出现的负面结果。特别是，对于过参数化模型，均值网络和方差网络足够强大，可以拟合每个数据点（同时将预测的方差收缩到零），或者学习一个恒定的预测，输出方差恰好匹配每个预测残差（即将目标解释为纯噪声）。本文从统计物理的角度研究了这些困难。我们证明了观察到的不稳定性不特定于任何神经网络结构，而是已经存在于过参数化条件高斯似然模型的场论中。在轻微的假设下，我们推导出一个可以通过数值求解的非参数自由能。得到的解与真实世界数据上的实证模型拟合具有良好的定性一致性，并且特别证明了相变的存在。

    Several recent studies have reported negative results when using heteroskedastic neural regression models to model real-world data. In particular, for overparameterized models, the mean and variance networks are powerful enough to either fit every single data point (while shrinking the predicted variances to zero), or to learn a constant prediction with an output variance exactly matching every predicted residual (i.e., explaining the targets as pure noise). This paper studies these difficulties from the perspective of statistical physics. We show that the observed instabilities are not specific to any neural network architecture but are already present in a field theory of an overparameterized conditional Gaussian likelihood model. Under light assumptions, we derive a nonparametric free energy that can be solved numerically. The resulting solutions show excellent qualitative agreement with empirical model fits on real-world data and, in particular, prove the existence of phase transit
    
[^224]: 具有公共数据的最优差分隐私学习

    Optimal Differentially Private Learning with Public Data. (arXiv:2306.15056v1 [cs.LG])

    [http://arxiv.org/abs/2306.15056](http://arxiv.org/abs/2306.15056)

    本论文研究了具有公共数据的最优差分隐私学习，并解决了在训练差分隐私模型时如何利用公共数据提高准确性的问题。

    

    差分隐私能够确保训练机器学习模型不泄漏私密数据。然而，差分隐私的代价是模型的准确性降低或样本复杂度增加。在实践中，我们可能可以访问不涉及隐私问题的辅助公共数据。这促使了最近研究公共数据在提高差分隐私模型准确性方面的作用。在本研究中，我们假设有一定数量的公共数据，并解决以下基本开放问题：1.在有公共数据的情况下，训练基于私有数据集的差分隐私模型的最优（最坏情况）误差是多少？哪些算法是最优的？2.如何利用公共数据在实践中改进差分隐私模型训练？我们在本地模型和中心模型的差分隐私问题下考虑这些问题。为了回答第一个问题，我们证明了对三个基本问题的最优误差率的紧密（最高常数因子）下界和上界。这三个问题是：均值估计，经验风险最小化和凸奇化。

    Differential Privacy (DP) ensures that training a machine learning model does not leak private data. However, the cost of DP is lower model accuracy or higher sample complexity. In practice, we may have access to auxiliary public data that is free of privacy concerns. This has motivated the recent study of what role public data might play in improving the accuracy of DP models. In this work, we assume access to a given amount of public data and settle the following fundamental open questions: 1. What is the optimal (worst-case) error of a DP model trained over a private data set while having access to side public data? What algorithms are optimal? 2. How can we harness public data to improve DP model training in practice? We consider these questions in both the local and central models of DP. To answer the first question, we prove tight (up to constant factors) lower and upper bounds that characterize the optimal error rates of three fundamental problems: mean estimation, empirical ris
    
[^225]: 更多的PAC-Bayes Bounds：从有界损失到具有一般性尾部行为的损失，到任何时间均有效的损失。

    More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime-validity. (arXiv:2306.12214v1 [stat.ML])

    [http://arxiv.org/abs/2306.12214](http://arxiv.org/abs/2306.12214)

    本文提出了一种新的高概率PAC-Bayes界限，在有界和一般尾部行为的损失中均适用。此外，这些界限还能够保持随时有效性。

    

    本文针对不同类型的损失提出了新的高概率PAC-Bayes界限。首先，针对有界范围的损失，我们提出了Catoni界的加强版本，适用于所有参数值的统一界。这导致了新的快速速率和混合速率上限，这些上限可解释性强且比文献中先前界限更紧。其次，针对更一般的尾部行为的损失，我们引入了两个新的无参数上限：当损失的累积生成函数有界时，我们引入了一个PAC-Bayes Chernoff类比，另一个上限是损失的二阶矩有界。这两个上限是利用一种基于可能事件空间的离散化的新技术获得的，“在概率”参数优化问题。最后，我们使用一种适用于任何现有界限的简单技术将所有先前结果扩展到任何时间有效的上限。

    In this paper, we present new high-probability PAC-Bayes bounds for different types of losses. Firstly, for losses with a bounded range, we present a strengthened version of Catoni's bound that holds uniformly for all parameter values. This leads to new fast rate and mixed rate bounds that are interpretable and tighter than previous bounds in the literature. Secondly, for losses with more general tail behaviors, we introduce two new parameter-free bounds: a PAC-Bayes Chernoff analogue when the loss' cumulative generating function is bounded, and a bound when the loss' second moment is bounded. These two bounds are obtained using a new technique based on a discretization of the space of possible events for the "in probability" parameter optimization problem. Finally, we extend all previous results to anytime-valid bounds using a simple technique applicable to any existing bound.
    
[^226]: $\texttt{causalAssembly}$: 用于基准因果发现的生成真实生产数据

    $\texttt{causalAssembly}$: Generating Realistic Production Data for Benchmarking Causal Discovery. (arXiv:2306.10816v1 [stat.ML])

    [http://arxiv.org/abs/2306.10816](http://arxiv.org/abs/2306.10816)

    该论文提出了一种生成基于装配线数据的半合成制造数据集的方法，以支持因果发现方法的基准测试。

    

    因果发现算法近年来取得了快速进展并越来越多地依靠灵活的非参数方法来处理复杂数据。然而，由于大多数真实数据源中真正的因果关系仍不为人所知，因此这些算法需要充分的经验验证。这个问题进一步加剧了环绕合适高质量数据发布的隐私问题。为了解决这些挑战，我们收集了一组复杂数据集，包括制造过程中装配线的测量数据。借助于对物理学的深入研究，这个数据集能够提供地面的因果关系对照。我们使用这个装配线数据和相关的地面真实信息来构建一个系统，生成半合成造数据来支持基准因果发现方法。为了实现这个目标，我们采用了最先进的仿真技术来生成数据集，模仿原始装配线数据集的特征，保留了过程变量之间的因果关系和它们的非线性依赖关系。

    Algorithms for causal discovery have recently undergone rapid advances and increasingly draw on flexible nonparametric methods to process complex data. With these advances comes a need for adequate empirical validation of the causal relationships learned by different algorithms. However, for most real data sources true causal relations remain unknown. This issue is further compounded by privacy concerns surrounding the release of suitable high-quality data. To help address these challenges, we gather a complex dataset comprising measurements from an assembly line in a manufacturing context. This line consists of numerous physical processes for which we are able to provide ground truth causal relationships on the basis of a detailed study of the underlying physics. We use the assembly line data and associated ground truth information to build a system for generation of semisynthetic manufacturing data that supports benchmarking of causal discovery methods. To accomplish this, we employ 
    
[^227]: 线性化随机生成树与GNN的快速高效训练

    Fast and Effective GNN Training with Linearized Random Spanning Trees. (arXiv:2306.04828v1 [cs.LG])

    [http://arxiv.org/abs/2306.04828](http://arxiv.org/abs/2306.04828)

    本文提出了一种基于线性化随机生成树的GNN训练框架，在多个真实世界的图形基准测试中表现得比其他经典算法更快且更准确。

    

    我们提出了一种新的有效和可扩展的框架，用于在给定图形结构数据的监督节点分类任务中训练GNN。我们的方法通过线性化从输入网络中提取的随机生成树得到一系列路径图来逐步精细化权重更新操作。路径图被设计为保留原始图的基本拓扑和节点信息。同时，路径图的稀疏性使得GNN训练更轻便，除了可扩展性外，还有助于缓解过度压缩和过度平滑等经典训练问题。我们在多个真实世界的图形基准测试上进行了广泛的实验研究，并将我们的框架应用于图形卷积网络，与众所周知的基线相比，同时提高了训练速度和测试精度。

    We present a new effective and scalable framework for training GNNs in supervised node classification tasks, given graph-structured data. Our approach increasingly refines the weight update operations on a sequence of path graphs obtained by linearizing random spanning trees extracted from the input network. The path graphs are designed to retain essential topological and node information of the original graph. At the same time, the sparsity of path graphs enables a much lighter GNN training which, besides scalability, helps in mitigating classical training issues, like over-squashing and over-smoothing. We carry out an extensive experimental investigation on a number of real-world graph benchmarks, where we apply our framework to graph convolutional networks, showing simultaneous improvement of both training speed and test accuracy, as compared to well-known baselines.
    
[^228]: 用于无目标代谢组学数据自动对齐的最优输运

    Optimal transport for automatic alignment of untargeted metabolomic data. (arXiv:2306.03218v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.03218](http://arxiv.org/abs/2306.03218)

    本文提出了一种名为GromovMatcher的算法，通过使用最优输运自动合并LC-MS数据集，可提高数据对齐的准确性和鲁棒性，有效解决代谢组学数据合并的挑战。

    

    液相色谱-质谱（LC-MS）通过测量生物标本中的大量代谢物推动药物研发，疾病诊断和风险预测的进展。然而，LC-MS的低通量对于生物标记物发现，注释和实验比较构成了主要挑战，需要合并多个数据集。当前的数据池化方法由于对数据变化和超参数依赖性的脆弱性而遇到实际限制。本文介绍了GromovMatcher，一种灵活且用户友好的算法，使用最优输运自动结合LC-MS数据集。通过利用特征强度相关结构，GromovMatcher提供了比现有方法更高的对齐准确性和鲁棒性。该算法可扩展到需要最小超参数调整的数千个特征。将我们的方法应用于肝癌和胰腺癌的实验患者研究

    Untargeted metabolomic profiling through liquid chromatography-mass spectrometry (LC-MS) measures a vast array of metabolites within biospecimens, advancing drug development, disease diagnosis, and risk prediction. However, the low throughput of LC-MS poses a major challenge for biomarker discovery, annotation, and experimental comparison, necessitating the merging of multiple datasets. Current data pooling methods encounter practical limitations due to their vulnerability to data variations and hyperparameter dependence. Here we introduce GromovMatcher, a flexible and user-friendly algorithm that automatically combines LC-MS datasets using optimal transport. By capitalizing on feature intensity correlation structures, GromovMatcher delivers superior alignment accuracy and robustness compared to existing approaches. This algorithm scales to thousands of features requiring minimal hyperparameter tuning. Applying our method to experimental patient studies of liver and pancreatic cancer, 
    
[^229]: 分布式SGD算法的稳定性与泛化分析改进

    Improved Stability and Generalization Analysis of the Decentralized SGD Algorithm. (arXiv:2306.02939v1 [cs.LG])

    [http://arxiv.org/abs/2306.02939](http://arxiv.org/abs/2306.02939)

    本文提出了新的算法稳定性理论来改进分布式SGD算法的泛化性能分析，推翻了现有技术对通信图负面影响的观点，并展示了D-SGD在凸设置中与经典SGD算法泛化界相同。

    

    本文基于算法稳定性，提出了分布式随机梯度下降(D-SGD)算法的新的泛化误差分析方法。得到的结果大大改进了现有技术，并推翻了它们关于通信图对泛化的负面影响的观点。例如，在凸设置中，无论图的选择如何，D-SGD具有与经典SGD算法相同的泛化界。我们发现这种反直觉的结果来自于考虑本地参数的平均值，这会隐藏一个与分布式场景不兼容的最终全局平均化步骤。考虑到这一观察结果，我们倡导分析本地参数的上确界，并展示了在这种情况下，图确实对泛化产生影响。与之前的结果不同，我们的分析即使对于非连接图也能产生非平凡边界。

    This paper presents a new generalization error analysis for the Decentralized Stochastic Gradient Descent (D-SGD) algorithm based on algorithmic stability. The obtained results largely improve upon state-of-the-art results, and even invalidate their claims that the communication graph has a detrimental effect on generalization. For instance, we show that in convex settings, D-SGD has the same generalization bounds as the classical SGD algorithm, no matter the choice of graph. We exhibit that this counter-intuitive result comes from considering the average of local parameters, which hides a final global averaging step incompatible with the decentralized scenario. In light of this observation, we advocate to analyze the supremum over local parameters and show that in this case, the graph does have an impact on the generalization. Unlike prior results, our analysis yields non-vacuous bounds even for non-connected graphs.
    
[^230]: 不破坏黑盒分类器的情况下规避它的分类——基于实际代价的黑盒攻击

    Evading Black-box Classifiers Without Breaking Eggs. (arXiv:2306.02895v1 [cs.CR])

    [http://arxiv.org/abs/2306.02895](http://arxiv.org/abs/2306.02895)

    本文提出了一种基于实际代价的黑盒攻击，通过设计新的攻击方式，成功减少了“有害”查询的数量，提高了黑盒攻击效率。

    

    基于决策的规避攻击是通过不断查询黑盒分类器来生成对抗性样本。本文认为现有的攻击方式在处理对安全性敏感的机器学习系统时有缺陷。因为这些系统主要目的是过滤出有害数据（例如恶意软件、有害内容等），所以查询的代价是不对等的，一旦查询被检测出是有害的，就会触发额外的安全过滤，例如使用限制或账户暂停。然而，现有的基于决策的攻击产生了大量的“有害”查询，导致它们很可能对安全关键系统无效。因此，本文提出新的攻击方式，通过减少“有害”查询的数量（最多可以减少 $1.5$ 倍到 $7.3$ 倍），以实现更加有效的黑盒攻击。但这些攻击的正常查询数量大大增加，因此提出了在实际代价度量下构建更有效的黑盒攻击的开放性问题。

    Decision-based evasion attacks repeatedly query a black-box classifier to generate adversarial examples. Prior work measures the cost of such attacks by the total number of queries made to the classifier. We argue this metric is flawed. Most security-critical machine learning systems aim to weed out "bad" data (e.g., malware, harmful content, etc). Queries to such systems carry a fundamentally asymmetric cost: queries detected as "bad" come at a higher cost because they trigger additional security filters, e.g., usage throttling or account suspension. Yet, we find that existing decision-based attacks issue a large number of "bad" queries, which likely renders them ineffective against security-critical systems. We then design new attacks that reduce the number of bad queries by $1.5$-$7.3\times$, but often at a significant increase in total (non-bad) queries. We thus pose it as an open problem to build black-box attacks that are more effective under realistic cost metrics.
    
[^231]: 神经网络集合的输入梯度多样性

    Input gradient diversity for neural network ensembles. (arXiv:2306.02775v1 [stat.ML])

    [http://arxiv.org/abs/2306.02775](http://arxiv.org/abs/2306.02775)

    本文提出了一阶斥力深度集成 (FoRDE) 算法，它使用输入梯度来增强多样性以提高神经网络集成的表现。

    

    深度集成 (DE) 通过它们的功能多样性在准确性、校准性和抵抗干扰方面表现出比单个神经网络更好的表现。基于粒子的变分推断 (ParVI) 方法通过基于网络相似性内核的排斥项来增强多样性。然而，由于过度参数化，权重空间排斥是低效的，而直接功能空间排斥被发现对 DE 的改进很小。为了避免这些困难，我们提出了基于 ParVI 的一阶斥力深度集成 (FoRDE)，这是一种基于输入梯度的集成学习方法。由于输入梯度唯一地确定了一个函数并且比权重小得多，所以这种方法保证了集合成员在功能上是不同的。直观地说，多样化输入梯度鼓励每个网络学习不同的特征，这有望改善神经网络集成的表现。

    Deep Ensembles (DEs) demonstrate improved accuracy, calibration and robustness to perturbations over single neural networks partly due to their functional diversity. Particle-based variational inference (ParVI) methods enhance diversity by formalizing a repulsion term based on a network similarity kernel. However, weight-space repulsion is inefficient due to over-parameterization, while direct function-space repulsion has been found to produce little improvement over DEs. To sidestep these difficulties, we propose First-order Repulsive Deep Ensemble (FoRDE), an ensemble learning method based on ParVI, which performs repulsion in the space of first-order input gradients. As input gradients uniquely characterize a function up to translation and are much smaller in dimension than the weights, this method guarantees that ensemble members are functionally different. Intuitively, diversifying the input gradients encourages each network to learn different features, which is expected to improv
    
[^232]: 深度学习中的一致置信现象及其对校准的影响

    A Uniform Confidence Phenomenon in Deep Learning and its Implications for Calibration. (arXiv:2306.00740v1 [cs.LG])

    [http://arxiv.org/abs/2306.00740](http://arxiv.org/abs/2306.00740)

    深度神经网络在训练点周围有大的几乎确定的置信邻域，这导致现代模型校准面临重要障碍。

    

    尽管深度神经网络具有惊人的泛化能力，但它们屡次表现出在预测不确定性方面估计不佳的情况——换句话说，它们在错误时经常过度自信。解决这个问题被称为模型校准，并以修改训练方案和训练后校准程序的形式受到了广泛关注。在本文中，我们提出了一个现代模型校准的重要障碍：深度神经网络在它们的训练点周围有大的几乎确定的置信邻域。我们在实验中证明了这种现象在很多模型和数据集对中都会出现（在图像分类的背景下）。此外，我们证明了当这种现象出现时，在类别之间存在重叠的大类数据分布中，即使在应用校准后也不能获得比随机更好的渐近校准模型（在渐近意义下）。

    Despite the impressive generalization capabilities of deep neural networks, they have been repeatedly shown to poorly estimate their predictive uncertainty - in other words, they are frequently overconfident when they are wrong. Fixing this issue is known as model calibration, and has consequently received much attention in the form of modified training schemes and post-training calibration procedures. In this work, we present a significant hurdle to the calibration of modern models: deep neural networks have large neighborhoods of almost certain confidence around their training points. We demonstrate in our experiments that this phenomenon consistently arises (in the context of image classification) across many model and dataset pairs. Furthermore, we prove that when this phenomenon holds, for a large class of data distributions with overlaps between classes, it is not possible to obtain a model that is asymptotically better than random (with respect to calibration) even after applyin
    
[^233]: 深度随机力学

    Deep Stochastic Mechanics. (arXiv:2305.19685v1 [cs.LG])

    [http://arxiv.org/abs/2305.19685](http://arxiv.org/abs/2305.19685)

    本文提出了一种基于深度学习的方法，用于数值模拟时间演化薛定谔方程，利用马尔可夫扩散采样来适应波函数的潜在低维结构，并提出了新的随机量子力学方程，具有线性的计算复杂度。数值模拟显示出显着的优势。

    

    本文引入了一种基于深度学习的方法，用于数值模拟时间演化薛定谔方程，受随机力学和生成性扩散模型的启发。与现有方法不同的是，我们的方法允许我们通过从马尔可夫扩散中采样来适应波函数潜在的低维结构，因此可以在更高的维度上降低计算复杂度。此外，我们提出了新的随机量子力学方程，结果具有与维数数量线性的计算复杂度。数值模拟验证了我们的理论发现，并显示出我们的方法与其他用于量子力学的基于深度学习的方法相比具有显着优势。

    This paper introduces a novel deep-learning-based approach for numerical simulation of a time-evolving Schr\"odinger equation inspired by stochastic mechanics and generative diffusion models. Unlike existing approaches, which exhibit computational complexity that scales exponentially in the problem dimension, our method allows us to adapt to the latent low-dimensional structure of the wave function by sampling from the Markovian diffusion. Depending on the latent dimension, our method may have far lower computational complexity in higher dimensions. Moreover, we propose novel equations for stochastic quantum mechanics, resulting in linear computational complexity with respect to the number of dimensions. Numerical simulations verify our theoretical findings and show a significant advantage of our method compared to other deep-learning-based approaches used for quantum mechanics.
    
[^234]: 扰动辅助样本合成：一种新的不确定性量化方法

    Perturbation-Assisted Sample Synthesis: A Novel Approach for Uncertainty Quantification. (arXiv:2305.18671v1 [stat.ML])

    [http://arxiv.org/abs/2305.18671](http://arxiv.org/abs/2305.18671)

    本文提出了扰动辅助样本合成（PASS）方法，可从复杂数据中绘制可靠结论，并通过估计数据生成分布和蒙特卡罗实验证明任何统计数据的估计分布。进一步推出扰动辅助推理（PAI）框架，可以提供有效性的统计保证。

    

    本文介绍了一种名为“扰动辅助样本合成（PASS）”的新型生成器，旨在从复杂数据中绘制可靠的结论，特别是在使用深度神经网络等高级建模技术时。 PASS利用扰动生成靠近原始数据分布的合成数据，包括数字和非结构化数据类型，如基因表达、图像和文本。通过估计数据生成分布并利用大型预训练生成模型，PASS提高了估计精度，并通过蒙特卡罗实验证明了任何统计数据的估计分布。基于PASS，我们提出了一种生成推理框架称为“扰动辅助推理（PAI）”，它提供了有效性的统计保证。在关键推理中，PAI使得在不知道引导分布（如模拟中）的情况下能够得出准确的结论，即使只有有限的数据。在非关键情况下，我们训练PASS使用中间变量插补策略来提高准确性。实验结果表明，在各种情况下，包括时间序列预测、图像分类和文本生成，PASS和PAI都优于现有最先进的替代品。

    This paper introduces a novel generator called Perturbation-Assisted Sample Synthesis (PASS), designed for drawing reliable conclusions from complex data, especially when using advanced modeling techniques like deep neural networks. PASS utilizes perturbation to generate synthetic data that closely mirrors the distribution of raw data, encompassing numerical and unstructured data types such as gene expression, images, and text. By estimating the data-generating distribution and leveraging large pre-trained generative models, PASS enhances estimation accuracy, providing an estimated distribution of any statistic through Monte Carlo experiments. Building on PASS, we propose a generative inference framework called Perturbation-Assisted Inference (PAI), which offers a statistical guarantee of validity. In pivotal inference, PAI enables accurate conclusions without knowing a pivotal's distribution as in simulations, even with limited data. In non-pivotal situations, we train PASS using an i
    
[^235]: 神经傅里叶变换：等变表示学习的通用方法

    Neural Fourier Transform: A General Approach to Equivariant Representation Learning. (arXiv:2305.18484v1 [stat.ML])

    [http://arxiv.org/abs/2305.18484](http://arxiv.org/abs/2305.18484)

    神经傅里叶变换是一种通用的等变表示学习方法，它可以在不需要显式知识的情况下学习组的潜在线性作用，实现对数据隐藏结构的提取。

    

    对称学习已被证明是提取数据隐藏结构的有效方法，其中等变关系概念起着中心作用。然而，大多数当前研究都建立在建筑理论和对数据形式的相应假设之上。我们提出了神经傅里叶变换（NFT），这是一种学习组的潜在线性作用的通用框架，而无需假设关于组如何作用于数据的显式知识。我们展示了NFT的理论基础，并表明等变特征的存在，即在等变性学习中普遍假定的，等价于数据空间中存在一组不变核。我们还提供实验结果，演示了在具有不同程度的关于操作组的知识的典型场景中应用NFT的应用。

    Symmetry learning has proven to be an effective approach for extracting the hidden structure of data, with the concept of equivariance relation playing the central role. However, most of the current studies are built on architectural theory and corresponding assumptions on the form of data. We propose Neural Fourier Transform (NFT), a general framework of learning the latent linear action of the group without assuming explicit knowledge of how the group acts on data. We present the theoretical foundations of NFT and show that the existence of a linear equivariant feature, which has been assumed ubiquitously in equivariance learning, is equivalent to the existence of a group invariant kernel on the dataspace. We also provide experimental results to demonstrate the application of NFT in typical scenarios with varying levels of knowledge about the acting group.
    
[^236]: 《大脑肿瘤分割（BraTS）挑战2023：关注儿科（CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs）》

    The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs). (arXiv:2305.17033v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.17033](http://arxiv.org/abs/2305.17033)

    这个论文介绍了CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023挑战，该挑战是首个专注于儿童脑肿瘤的BraTS挑战，旨在评估儿童脑胶质瘤的体积分割算法的发展。儿童中枢神经系统肿瘤是儿童癌症相关死亡的主要原因，并且对这些实体的诊断和治疗存在一些挑战。

    

    儿童中枢神经系统肿瘤是儿童癌症相关死亡的最常见原因。儿童高级别胶质瘤的五年生存率不到20％。由于罕见，对这些实体的诊断通常会延迟，其治疗主要基于历史治疗理念，并且临床试验需要多机构合作。MICCAI大脑肿瘤分割（BraTS）挑战是一个里程碑式的社区基准事件，已经成功创建资源12年，用于成人胶质瘤的分割和分析。在这里，我们介绍了CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023挑战，这是首个专注于儿童脑肿瘤的BraTS挑战，其中包括多个国际合作组织专注于儿科神经肿瘤和临床试验的数据。BraTS-PEDs 2023挑战侧重于评估用于儿童脑胶质瘤的体积分割算法的发展。

    Pediatric tumors of the central nervous system are the most common cause of cancer-related death in children. The five-year survival rate for high-grade gliomas in children is less than 20\%. Due to their rarity, the diagnosis of these entities is often delayed, their treatment is mainly based on historic treatment concepts, and clinical trials require multi-institutional collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a landmark community benchmark event with a successful history of 12 years of resource creation for the segmentation and analysis of adult glioma. Here we present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which represents the first BraTS challenge focused on pediatric brain tumors with data acquired across multiple international consortia dedicated to pediatric neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on benchmarking the development of volumentric segmentation algorithms for pediatric brain gli
    
[^237]: 时间变化处理的反事实生成模型

    Counterfactual Generative Models for Time-Varying Treatments. (arXiv:2305.15742v1 [stat.ML])

    [http://arxiv.org/abs/2305.15742](http://arxiv.org/abs/2305.15742)

    本文研究了时间变量处理情况下的反事实生成模型，能够捕捉整个反事实分布，并且能够有效推断反事实分布的某些统计量，适用于医疗保健和公共政策制定领域。

    

    估计平均因果效应是测试新疗法的常用做法。然而，平均效应会掩盖反事实分布中重要的个体特征，可能会引起安全、公平和道德方面的担忧。这个问题在时间设置中更加严重，因为处理是时序的和时变的，对反事实分布产生了错综复杂的影响。本文提出了一种新的条件生成建模方法，以捕获整个反事实分布，允许对反事实分布的某些统计量进行有效推断。这使得所提出的方法尤其适用于医疗保健和公共政策制定领域。我们的生成建模方法通过边际结构模型谨慎地解决了观察数据和目标反事实分布之间的分布不匹配。在合成和真实数据上，我们的方法优于现有的基线方法。

    Estimating average causal effects is a common practice to test new treatments. However, the average effect ''masks'' important individual characteristics in the counterfactual distribution, which may lead to safety, fairness, and ethical concerns. This issue is exacerbated in the temporal setting, where the treatment is sequential and time-varying, leading to an intricate influence on the counterfactual distribution. In this paper, we propose a novel conditional generative modeling approach to capture the whole counterfactual distribution, allowing efficient inference on certain statistics of the counterfactual distribution. This makes the proposed approach particularly suitable for healthcare and public policy making. Our generative modeling approach carefully tackles the distribution mismatch in the observed data and the targeted counterfactual distribution via a marginal structural model. Our method outperforms state-of-the-art baselines on both synthetic and real data.
    
[^238]: 有条件生成模型是标记时间点过程的必备工具。

    Conditional Generative Modeling is All You Need for Marked Temporal Point Processes. (arXiv:2305.12569v1 [stat.ML])

    [http://arxiv.org/abs/2305.12569](http://arxiv.org/abs/2305.12569)

    本文提出了一种从标记时间点过程中提取其统计直觉的事件生成模型，通过条件生成器以历史观察作为输入，生成可能发生的高质量随后事件。该模型具有高效、灵活和表示能力等方面的优势。

    

    近年来，生成建模的进步使得从上下文信息中生成高质量内容成为可能，但一个关键问题仍然存在：如何教模型知道何时生成内容？为了回答这个问题，本研究提出了一种新的事件生成模型，从标记时间点过程中提取其统计直觉，并提供了一个干净、灵活和计算效率高的解决方案，适用于涉及多维标记的各种应用。我们旨在捕捉点过程的分布而不需明确指定条件强度或概率密度。我们使用一个条件生成器，以事件历史为输入并生成在先前观察到的事件下，可能发生的高质量随后事件。所提出的框架提供了一系列利益，包括在学习模型和生成样本方面的异常效率以及相当大的表示能力来捕捉。

    Recent advancements in generative modeling have made it possible to generate high-quality content from context information, but a key question remains: how to teach models to know when to generate content? To answer this question, this study proposes a novel event generative model that draws its statistical intuition from marked temporal point processes, and offers a clean, flexible, and computationally efficient solution for a wide range of applications involving multi-dimensional marks. We aim to capture the distribution of the point process without explicitly specifying the conditional intensity or probability density. Instead, we use a conditional generator that takes the history of events as input and generates the high-quality subsequent event that is likely to occur given the prior observations. The proposed framework offers a host of benefits, including exceptional efficiency in learning the model and generating samples, as well as considerable representational power to capture
    
[^239]: 任意时延的非稳态在线凸优化

    Non-stationary Online Convex Optimization with Arbitrary Delays. (arXiv:2305.12131v1 [cs.LG])

    [http://arxiv.org/abs/2305.12131](http://arxiv.org/abs/2305.12131)

    本文研究了任意时延的非稳态在线凸优化，提出了一种简单的算法DOGD，并证明它能在最坏情况下获得$O(\sqrt{dT}(P_T+1))$的动态遗憾界，同时当延迟不改变梯度到达顺序时，自动将动态遗憾减少到$O(\sqrt{S}(1+P_T))$。

    

    最近，以梯度或其他函数信息可以任意延迟为特点的在线凸优化（OCO）引起了越来越多的关注。与之前研究稳态环境的研究不同，本文研究了非稳态环境下的延迟OCO，并旨在最小化与任何比较器序列相关的动态遗憾。为此，我们首先提出了一个简单的算法，即DOGD，该算法根据其到达顺序为每个延迟梯度执行渐变下降步骤。尽管它很简单，但我们的新型分析表明，DOGD可以在最坏情况下获得$O(\sqrt{dT}(P_T+1))$的动态遗憾界，其中$d$是最大延迟，$T$是时间跨度，$P_T$是比较器的路径长度。更重要的是，在延迟不改变渐变的到达顺序的情况下，它可以自动将动态遗憾减少到$O(\sqrt{S}(1+P_T))$，其中$S$是延迟之和。此外，我们将DOGD扩展为更通用的算法，并证明它实现了与DOGD相同的遗憾界。广泛的模拟表明了所提出算法的有效性和效率。

    Online convex optimization (OCO) with arbitrary delays, in which gradients or other information of functions could be arbitrarily delayed, has received increasing attention recently. Different from previous studies that focus on stationary environments, this paper investigates the delayed OCO in non-stationary environments, and aims to minimize the dynamic regret with respect to any sequence of comparators. To this end, we first propose a simple algorithm, namely DOGD, which performs a gradient descent step for each delayed gradient according to their arrival order. Despite its simplicity, our novel analysis shows that DOGD can attain an $O(\sqrt{dT}(P_T+1)$ dynamic regret bound in the worst case, where $d$ is the maximum delay, $T$ is the time horizon, and $P_T$ is the path length of comparators. More importantly, in case delays do not change the arrival order of gradients, it can automatically reduce the dynamic regret to $O(\sqrt{S}(1+P_T))$, where $S$ is the sum of delays. Furtherm
    
[^240]: 基于归纳流的快速粒子探测器模拟框架

    Inductive CaloFlow. (arXiv:2305.11934v1 [physics.ins-det])

    [http://arxiv.org/abs/2305.11934](http://arxiv.org/abs/2305.11934)

    iCaloFlow是一个基于归纳流的快速探测器模拟框架，可以以高达以往10-100倍的分辨率进行快速、高保真度模拟。

    

    模拟粒子探测器响应是大型强子对撞机计算流程中最昂贵的步骤。最近的研究表明，归一化流可以加快此过程，并实现前所未有的精度水平，但将此方法扩展到与未来探测器升级相关的更高分辨率时会导致限制性的内存约束。为了克服这个问题，我们介绍了基于归纳系列归一化流的快速探测器模拟框架iCaloFlow，它是在成对的连续能量沉积层中训练的。为了增加采样速度而不失表现力，我们进一步使用师生蒸馏。正如我们在CaloChallenge2022的数据集2和数据集3中展示的那样，iCaloFlow可以实现归一化流在进行快速、高保真度模拟时的潜力，这些模拟对应的探测器几何约比以前考虑的高10-100倍。

    Simulating particle detector response is the single most expensive step in the Large Hadron Collider computational pipeline. Recently it was shown that normalizing flows can accelerate this process while achieving unprecedented levels of accuracy, but scaling this approach up to higher resolutions relevant for future detector upgrades leads to prohibitive memory constraints. To overcome this problem, we introduce Inductive CaloFlow (iCaloFlow), a framework for fast detector simulation based on an inductive series of normalizing flows trained on the pattern of energy depositions in pairs of consecutive calorimeter layers. We further use a teacher-student distillation to increase sampling speed without loss of expressivity. As we demonstrate with Datasets 2 and 3 of the CaloChallenge2022, iCaloFlow can realize the potential of normalizing flows in performing fast, high-fidelity simulation on detector geometries that are ~ 10 - 100 times higher granularity than previously considered.
    
[^241]: 图上的转移算子：谱聚类及其扩展

    Transfer operators on graphs: Spectral clustering and beyond. (arXiv:2305.11766v1 [stat.ML])

    [http://arxiv.org/abs/2305.11766](http://arxiv.org/abs/2305.11766)

    本文介绍了在图上定义的转移算子及其谱特性，提出了基于广义转移算子的有向图聚类算法，并证明了算法有效性。

    

    图和网络在建模和分析复杂的相关系统中发挥着重要作用，例如交通网络，集成电路，电力网格，引文图以及生物和人工神经网络。本文在图上定义了转移算子，如Koopman算子和Perron-Frobenius算子，研究了它们的谱特性，引入了这些算子的Galerkin投影，并说明了如何从数据中估计降低表示。特别地，我们展示了无向图谱聚类可以被解释为Koopman算子的特征函数，并提出了基于广义转移算子的有向图聚类算法。我们在几个基准问题上证明了所得算法的有效性，并提供了不同聚类的解释。

    Graphs and networks play an important role in modeling and analyzing complex interconnected systems such as transportation networks, integrated circuits, power grids, citation graphs, and biological and artificial neural networks. Graph clustering algorithms can be used to detect groups of strongly connected vertices and to derive coarse-grained models. We define transfer operators such as the Koopman operator and the Perron-Frobenius operator on graphs, study their spectral properties, introduce Galerkin projections of these operators, and illustrate how reduced representations can be estimated from data. In particular, we show that spectral clustering of undirected graphs can be interpreted in terms of eigenfunctions of the Koopman operator and propose novel clustering algorithms for directed graphs based on generalized transfer operators. We demonstrate the efficacy of the resulting algorithms on several benchmark problems and provide different interpretations of clusters.
    
[^242]: 多智能体强化学习的双层潜变量模型，提高样本效率

    Bi-level Latent Variable Model for Sample-Efficient Multi-Agent Reinforcement Learning. (arXiv:2304.06011v1 [cs.LG])

    [http://arxiv.org/abs/2304.06011](http://arxiv.org/abs/2304.06011)

    提出了一种新颖的基于模型的MARL算法，BiLL，它学习一个双层潜变量模型，以提高样本效率。该模型在顶层学习全局状态的潜在表示，底层学习每个智能体的潜在表示，并生成潜在轨迹用于策略学习。在SMAC和Flatland环境中，我们的算法在样本效率方面优于现有的无模型和基于模型的基准算法，包括在Super Hard SMAC地图上。

    

    多智能体强化学习(MARL)算法在实际应用中具有很大的潜力，但通常具有较高的样本复杂度。为了解决这个问题，我们提出了一种新颖的基于模型的MARL算法：BiLL(双层潜变量模型学习)，该算法从高维度输入中学习双层潜在变量模型。在顶层，该模型学习全局状态的潜在表示，该表示编码与行为学习相关的全局信息。在底层，模型学习每个智能体的潜在表示，给定来自顶层的全局潜在表示。模型生成潜在轨迹以用于策略学习。我们在具有挑战性的SMAC和Flatland环境中评估了我们的算法，结果表明我们的算法在样本效率方面优于现有的无模型和基于模型的基准算法，包括在两个极具挑战性的Super Hard SMAC地图上。

    Despite their potential in real-world applications, multi-agent reinforcement learning (MARL) algorithms often suffer from high sample complexity. To address this issue, we present a novel model-based MARL algorithm, BiLL (Bi-Level Latent Variable Model-based Learning), that learns a bi-level latent variable model from high-dimensional inputs. At the top level, the model learns latent representations of the global state, which encode global information relevant to behavior learning. At the bottom level, it learns latent representations for each agent, given the global latent representations from the top level. The model generates latent trajectories to use for policy learning. We evaluate our algorithm on complex multi-agent tasks in the challenging SMAC and Flatland environments. Our algorithm outperforms state-of-the-art model-free and model-based baselines in sample efficiency, including on two extremely challenging Super Hard SMAC maps.
    
[^243]: 强化学习用于能源交易策略的优化

    Reinforcement learning for optimization of energy trading strategy. (arXiv:2303.16266v1 [cs.LG])

    [http://arxiv.org/abs/2303.16266](http://arxiv.org/abs/2303.16266)

    本文使用强化学习算法优化了一种黑盒交易策略，该策略通过在马尔可夫决策过程中使用真实数据进行优化，在 DA 能源市场上由中型生产者自动进行交易。

    

    越来越多的能源来自大量小型生产者的可再生能源，这些来源的效率是不稳定的，在某种程度上也是随机的，加剧了能源市场平衡问题。在许多国家，这种平衡是在预测日（DA）能源市场上完成的。本文考虑由中型生产者在DA能源市场上的自动化交易。我们将此活动建模为马尔可夫决策过程，并规范了一个框架，其中可以使用现实数据优化即用策略。我们合成参数化交易策略，并使用进化算法优化它们。我们还使用最先进的强化学习算法优化一个黑盒交易策略，该策略利用来自环境的可用信息来影响未来价格。

    An increasing part of energy is produced from renewable sources by a large number of small producers. The efficiency of these sources is volatile and, to some extent, random, exacerbating the energy market balance problem. In many countries, that balancing is performed on day-ahead (DA) energy markets. In this paper, we consider automated trading on a DA energy market by a medium size prosumer. We model this activity as a Markov Decision Process and formalize a framework in which a ready-to-use strategy can be optimized with real-life data. We synthesize parametric trading strategies and optimize them with an evolutionary algorithm. We also use state-of-the-art reinforcement learning algorithms to optimize a black-box trading strategy fed with available information from the environment that can impact future prices.
    
[^244]: 使用SoftER Teacher增强半监督Few-Shot目标检测

    Boosting Semi-Supervised Few-Shot Object Detection with SoftER Teacher. (arXiv:2303.05739v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05739](http://arxiv.org/abs/2303.05739)

    本文研究了半监督Few-Shot目标检测任务，发现未标记数据对提高半监督FSOD有好处。受此发现启发，我们引入了SoftER Teacher，一种强大的检测器，用于改进FSOD，不需要丰富的标签，并能在性能方面超越强有力的监督检测器，而且不会出现灾难性遗忘。

    

    Few-shot目标检测（FSOD）是一个新兴的问题，旨在从少量样本中检测新概念。现有的FSOD方法假定有丰富的基础标签来适应新对象。本文研究了半监督FSOD任务，考虑到基础和新标签同时很少的现实情况。我们探索了未标记数据的实用性，并发现其通过区域提议的方式提高了半监督FSOD的显着能力。受此发现的启发，我们引入了SoftER Teacher，一种结合区域提议上的伪标记和表示学习的强大检测器，以利用未标记数据改进FSOD，而无需依赖丰富的标签。广泛的实验表明，SoftER Teacher超越了强有力的监督检测器的新性能，仅使用所需基础标签的10％，而不会出现之前方法中观察到的灾难性遗忘。我们的工作还揭示了半监督学习和Few-Shot目标检测之间潜在关系的可能性。

    Few-shot object detection (FSOD) is an emerging problem aimed at detecting novel concepts from few exemplars. Existing approaches to FSOD assume abundant base labels to adapt to novel objects. This paper studies the task of semi-supervised FSOD by considering a realistic scenario in which both base and novel labels are simultaneously scarce. We explore the utility of unlabeled data and discover its remarkable ability to boost semi-supervised FSOD by way of region proposals. Motivated by this finding, we introduce SoftER Teacher, a robust detector combining pseudo-labeling with representation learning on region proposals, to harness unlabeled data for improved FSOD without relying on abundant labels. Extensive experiments show that SoftER Teacher surpasses the novel performance of a strong supervised detector using only 10% of required base labels, without experiencing catastrophic forgetting observed in prior approaches. Our work also sheds light on a potential relationship between sem
    
[^245]: 非马尔可夫环境下的强化学习

    Reinforcement Learning in Non-Markovian Environments. (arXiv:2211.01595v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2211.01595](http://arxiv.org/abs/2211.01595)

    本文通过递归计算近似充分统计量，提出了一种基于自编码器的代理设计方案，实现了在非马尔可夫环境中进行强化学习。

    

    本文以Van Roy及其合作者为基础，提出了在任意非马尔可夫环境中进行强化学习的新范式，并明确了当在该范式上应用Q学习算法时，由于非马尔可夫性质引起的错误。基于此观察，我们建议代理设计的标准应是寻找某些条件规律的良好近似。受经典随机控制的启发，我们证明了我们的问题归结为递归计算近似充分统计量的问题。这导致了一种基于自编码器的代理设计方案，我们在部分观察到的强化学习环境中进行了数值测试。

    Motivated by the novel paradigm developed by Van Roy and coauthors for reinforcement learning in arbitrary non-Markovian environments, we propose a related formulation and explicitly pin down the error caused by non-Markovianity of observations when the Q-learning algorithm is applied on this formulation. Based on this observation, we propose that the criterion for agent design should be to seek good approximations for certain conditional laws. Inspired by classical stochastic control, we show that our problem reduces to that of recursive computation of approximate sufficient statistics. This leads to an autoencoder-based scheme for agent design which is then numerically tested on partially observed reinforcement learning environments.
    

